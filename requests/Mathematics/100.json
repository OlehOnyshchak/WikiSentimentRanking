{
  "pages": [
    {
      "title": "Mathematics",
      "url": "https://en.wikipedia.org/wiki/Mathematics",
      "text": "thumb|Euclid (holding calipers), Greek mathematician, 3rd century BC, as imagined by Raphael in this detail from The School of Athens.\n\nMathematics (from Greek μάθημα máthēma, \"knowledge, study, learning\") includes the study of such topics as quantity, structure, space, and change. It has no generally accepted definition.\n\nMathematicians seek and use patterns to formulate new conjectures; they resolve the truth or falsity of conjectures by mathematical proof. When mathematical structures are good models of real phenomena, then mathematical reasoning can provide insight or predictions about nature. Through the use of abstraction and logic, mathematics developed from counting, calculation, measurement, and the systematic study of the shapes and motions of physical objects. Practical mathematics has been a human activity from as far back as written records exist. The research required to solve mathematical problems can take years or even centuries of sustained inquiry.\n\nRigorous arguments first appeared in Greek mathematics, most notably in Euclid's Elements. Since the pioneering work of Giuseppe Peano (1858–1932), David Hilbert (1862–1943), and others on axiomatic systems in the late 19th century, it has become customary to view mathematical research as establishing truth by rigorous deduction from appropriately chosen axioms and definitions. Mathematics developed at a relatively slow pace until the Renaissance, when mathematical innovations interacting with new scientific discoveries led to a rapid increase in the rate of mathematical discovery that has continued to the present day.Eves, p. 306\n\nMathematics is essential in many fields, including natural science, engineering, medicine, finance, and the social sciences. Applied mathematics has led to entirely new mathematical disciplines, such as statistics and game theory. Mathematicians engage in pure mathematics (mathematics for its own sake) without having any application in mind, but practical applications for what began as pure mathematics are often discovered later.Peterson, p. 12\n\nHistory\n\nthumb|The Babylonian mathematical tablet Plimpton 322, dated to 1800 BC.\nthumb|right|Archimedes used the method of exhaustion to approximate the value of pi.\nthumb|right|350px|The numerals used in the Bakhshali manuscript, dated between the 2nd century BCE and the 2nd century CE.\nThe history of mathematics can be seen as an ever-increasing series of abstractions. The first abstraction, which is shared by many animals, was probably that of numbers: the realization that a collection of two apples and a collection of two oranges (for example) have something in common, namely quantity of their members.\n\nAs evidenced by tallies found on bone, in addition to recognizing how to count physical objects, prehistoric peoples may have also recognized how to count abstract quantities, like time – days, seasons, years.See, for example, Raymond L. Wilder, Evolution of Mathematical Concepts; an Elementary Study, passim\n\nEvidence for more complex mathematics does not appear until around 3000 BC, when the Babylonians and Egyptians began using arithmetic, algebra and geometry for taxation and other financial calculations, for building and construction, and for astronomy.Kline 1990, Chapter 1. The most ancient mathematical texts from Mesopotamia and Egypt are from 2000–1800 BC. Many early texts mention Pythagorean triples and so, by inference, the Pythagorean theorem seems to be the most ancient and widespread mathematical development after basic arithmetic and geometry. It is in Babylonian mathematics that elementary arithmetic (addition, subtraction, multiplication and division) first appear in the archaeological record. The Babylonians also possessed a place-value system, and used a sexagesimal numeral system, still in use today for measuring angles and time.\n\nBeginning in the 6th century BC with the Pythagoreans, the Ancient Greeks began a systematic study of mathematics as a subject in its own right with Greek mathematics. Around 300 BC, Euclid introduced the axiomatic method still used in mathematics today, consisting of definition, axiom, theorem, and proof. His textbook Elements is widely considered the most successful and influential textbook of all time. The greatest mathematician of antiquity is often held to be Archimedes (c. 287–212 BC) of Syracuse. He developed formulas for calculating the surface area and volume of solids of revolution and used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, in a manner not too dissimilar from modern calculus. Other notable achievements of Greek mathematics are conic sections (Apollonius of Perga, 3rd century BC), trigonometry (Hipparchus of Nicaea (2nd century BC), and the beginnings of algebra (Diophantus, 3rd century AD).\n\nThe Hindu–Arabic numeral system and the rules for the use of its operations, in use throughout the world today, evolved over the course of the first millennium AD in India and were transmitted to the Western world via Islamic mathematics. Other notable developments of Indian mathematics include the modern definition of sine and cosine, and an early form of infinite series.\n\nleft|thumb|200px|A page from al-Khwārizmī's Algebra\nDuring the Golden Age of Islam, especially during the 9th and 10th centuries, mathematics saw many important innovations building on Greek mathematics. The most notable achievement of Islamic mathematics was the development of algebra. Other notable achievements of the Islamic period are advances in spherical trigonometry and the addition of the decimal point to the Arabic numeral system. Many notable mathematicians from this period were Persian, such as Al-Khwarismi, Omar Khayyam and Sharaf al-Dīn al-Ṭūsī.\n\nDuring the early modern period, mathematics began to develop at an accelerating pace in Western Europe. The development of calculus by Newton and Leibniz in the 17th century revolutionized mathematics. Leonhard Euler was the most notable mathematician of the 18th century, contributing numerous theorems and discoveries. Perhaps the foremost mathematician of the 19th century was the German mathematician Carl Friedrich Gauss, who made numerous contributions to fields such as algebra, analysis, differential geometry, matrix theory, number theory, and statistics. In the early 20th century, Kurt Gödel transformed mathematics by publishing his incompleteness theorems, which show that any axiomatic system that is consistent will contain unprovable propositions.\n\nMathematics has since been greatly extended, and there has been a fruitful interaction between mathematics and science, to the benefit of both. Mathematical discoveries continue to be made today. According to Mikhail B. Sevryuk, in the January 2006 issue of the Bulletin of the American Mathematical Society, \"The number of papers and books included in the Mathematical Reviews database since 1940 (the first year of operation of MR) is now more than 1.9 million, and more than 75 thousand items are added to the database each year. The overwhelming majority of works in this ocean contain new mathematical theorems and their proofs.\"\n\nEtymology\nThe word mathematics comes from Ancient Greek μάθημα (máthēma), meaning \"that which is learnt\", \"what one gets to know\", hence also \"study\" and \"science\". The word for \"mathematics\" came to have the narrower and more technical meaning \"mathematical study\" even in Classical times.Both meanings can be found in Plato, the narrower in Republic 510c, but Plato did not use a math- word; Aristotle did, commenting on it. . OED Online , \"Mathematics\". Its adjective is  (mathēmatikós), meaning \"related to learning\" or \"studious\", which likewise further came to mean \"mathematical\". In particular,  (mathēmatikḗ tékhnē), , meant \"the mathematical art\".\n\nSimilarly, one of the two main schools of thought in Pythagoreanism was known as the mathēmatikoi (μαθηματικοί)—which at the time meant \"teachers\" rather than \"mathematicians\" in the modern sense.\n\nIn Latin, and in English until around 1700, the term mathematics more commonly meant \"astrology\" (or sometimes \"astronomy\") rather than \"mathematics\"; the meaning gradually changed to its present one from about 1500 to 1800. This has resulted in several mistranslations. For example, Saint Augustine's warning that Christians should beware of mathematici, meaning astrologers, is sometimes mistranslated as a condemnation of mathematicians.\n\nThe apparent plural form in English, like the French plural form  (and the less commonly used singular derivative ), goes back to the Latin neuter plural  (Cicero), based on the Greek plural  (ta mathēmatiká), used by Aristotle (384–322 BC), and meaning roughly \"all things mathematical\"; although it is plausible that English borrowed only the adjective mathematic(al) and formed the noun mathematics anew, after the pattern of physics and metaphysics, which were inherited from Greek.The Oxford Dictionary of English Etymology, Oxford English Dictionary, sub \"mathematics\", \"mathematic\", \"mathematics\" In English, the noun mathematics takes a singular verb. It is often shortened to maths or, in North America, math.\"maths, n.\" and \"math, n.3\". Oxford English Dictionary, on-line version (2012).\n\nDefinitions of mathematics\n\nthumb|upright|Leonardo Fibonacci, the Italian mathematician who introduced the Hindu–Arabic numeral system invented between the 1st and 4th centuries by Indian mathematicians, to the Western World\nMathematics has no generally accepted definition. Aristotle defined mathematics as \"the science of quantity\" and this definition prevailed until the 18th century. In the 19th century, when the study of mathematics increased in rigor and began to address abstract topics such as group theory and projective geometry, which have no clear-cut relation to quantity and measurement, mathematicians and philosophers began to propose a variety of new definitions. Three leading types of definition of mathematics today are called logicist, intuitionist, and formalist, each reflecting a different philosophical school of thought. All have severe problems, none has widespread acceptance, and no reconciliation seems possible.\n\nAn early definition of mathematics in terms of logic was Benjamin Peirce's \"the science that draws necessary conclusions\" (1870). In the Principia Mathematica, Bertrand Russell and Alfred North Whitehead advanced the philosophical program known as logicism, and attempted to prove that all mathematical concepts, statements, and principles can be defined and proved entirely in terms of symbolic logic. A logicist definition of mathematics is Russell's \"All Mathematics is Symbolic Logic\" (1903).\n\nIntuitionist definitions, developing from the philosophy of mathematician L. E. J. Brouwer, identify mathematics with certain mental phenomena. An example of an intuitionist definition is \"Mathematics is the mental activity which consists in carrying out constructs one after the other.\" A peculiarity of intuitionism is that it rejects some mathematical ideas considered valid according to other definitions. In particular, while other philosophies of mathematics allow objects that can be proved to exist even though they cannot be constructed, intuitionism allows only mathematical objects that one can actually construct.\n\nFormalist definitions identify mathematics with its symbols and the rules for operating on them. Haskell Curry defined mathematics simply as \"the science of formal systems\". A formal system is a set of symbols, or tokens, and some rules telling how the tokens may be combined into formulas. In formal systems, the word axiom has a special meaning, different from the ordinary meaning of \"a self-evident truth\". In formal systems, an axiom is a combination of tokens that is included in a given formal system without needing to be derived using the rules of the system.\n\nA great many professional mathematicians take no interest in a definition of mathematics, or consider it undefinable. There is not even consensus on whether mathematics is an art or a science. Some just say, \"Mathematics is what mathematicians do.\"\n\nMathematics as science\nupright|thumb|left|Carl Friedrich Gauss, known as the prince of mathematicians\nThe German mathematician Carl Friedrich Gauss referred to mathematics as \"the Queen of the Sciences\".Waltershausen, p. 79 More recently, Marcus du Sautoy has called mathematics \"the Queen of Science ... the main driving force behind scientific discovery\". In the original Latin Regina Scientiarum, as well as in German Königin der Wissenschaften, the word corresponding to science means a \"field of knowledge\", and this was the original meaning of \"science\" in English, also; mathematics is in this sense a field of knowledge. The specialization restricting the meaning of \"science\" to natural science follows the rise of Baconian science, which contrasted \"natural science\" to scholasticism, the Aristotelean method of inquiring from first principles. The role of empirical experimentation and observation is negligible in mathematics, compared to natural sciences such as biology, chemistry, or physics. Albert Einstein stated that \"as far as the laws of mathematics refer to reality, they are not certain; and as far as they are certain, they do not refer to reality.\"Einstein, p. 28. The quote is Einstein's answer to the question: \"How can it be that mathematics, being after all a product of human thought which is independent of experience, is so admirably appropriate to the objects of reality?\" This question was inspired by Eugene Wigner's paper \"The Unreasonable Effectiveness of Mathematics in the Natural Sciences\".\n\nMany philosophers believe that mathematics is not experimentally falsifiable, and thus not a science according to the definition of Karl Popper. However, in the 1930s Gödel's incompleteness theorems convinced many mathematicians that mathematics cannot be reduced to logic alone, and Karl Popper concluded that \"most mathematical theories are, like those of physics and biology, hypothetico-deductive: pure mathematics therefore turns out to be much closer to the natural sciences whose hypotheses are conjectures, than it seemed even recently.\"Popper 1995, p. 56 Other thinkers, notably Imre Lakatos, have applied a version of falsificationism to mathematics itself.Imre Lakatos (1976), Proofs and Refutations. Cambridge: Cambridge University Press.\n\nAn alternative view is that certain scientific fields (such as theoretical physics) are mathematics with axioms that are intended to correspond to reality. Mathematics shares much in common with many fields in the physical sciences, notably the exploration of the logical consequences of assumptions. Intuition and experimentation also play a role in the formulation of conjectures in both mathematics and the (other) sciences. Experimental mathematics continues to grow in importance within mathematics, and computation and simulation are playing an increasing role in both the sciences and mathematics.\n\nThe opinions of mathematicians on this matter are varied. Many mathematiciansSee, for example Bertrand Russell's statement \"Mathematics, rightly viewed, possesses not only truth, but supreme beauty ...\" in his History of Western Philosophy feel that to call their area a science is to downplay the importance of its aesthetic side, and its history in the traditional seven liberal arts; others feel that to ignore its connection to the sciences is to turn a blind eye to the fact that the interface between mathematics and its applications in science and engineering has driven much development in mathematics. One way this difference of viewpoint plays out is in the philosophical debate as to whether mathematics is created (as in art) or discovered (as in science). It is common to see universities divided into sections that include a division of Science and Mathematics, indicating that the fields are seen as being allied but that they do not coincide. In practice, mathematicians are typically grouped with scientists at the gross level but separated at finer levels. This is one of many issues considered in the philosophy of mathematics.\n\nInspiration, pure and applied mathematics, and aesthetics\n\nMathematics arises from many different kinds of problems. At first these were found in commerce, land measurement, architecture and later astronomy; today, all sciences suggest problems studied by mathematicians, and many problems arise within mathematics itself. For example, the physicist Richard Feynman invented the path integral formulation of quantum mechanics using a combination of mathematical reasoning and physical insight, and today's string theory, a still-developing scientific theory which attempts to unify the four fundamental forces of nature, continues to inspire new mathematics.\n\nSome mathematics is relevant only in the area that inspired it, and is applied to solve further problems in that area. But often mathematics inspired by one area proves useful in many areas, and joins the general stock of mathematical concepts. A distinction is often made between pure mathematics and applied mathematics. However pure mathematics topics often turn out to have applications, e.g. number theory in cryptography. This remarkable fact, that even the \"purest\" mathematics often turns out to have practical applications, is what Eugene Wigner has called \"the unreasonable effectiveness of mathematics\". As in most areas of study, the explosion of knowledge in the scientific age has led to specialization: there are now hundreds of specialized areas in mathematics and the latest Mathematics Subject Classification runs to 46 pages. Several areas of applied mathematics have merged with related traditions outside of mathematics and become disciplines in their own right, including statistics, operations research, and computer science.\n\nFor those who are mathematically inclined, there is often a definite aesthetic aspect to much of mathematics. Many mathematicians talk about the elegance of mathematics, its intrinsic aesthetics and inner beauty. Simplicity and generality are valued. There is beauty in a simple and elegant proof, such as Euclid's proof that there are infinitely many prime numbers, and in an elegant numerical method that speeds calculation, such as the fast Fourier transform. G. H. Hardy in A Mathematician's Apology expressed the belief that these aesthetic considerations are, in themselves, sufficient to justify the study of pure mathematics. He identified criteria such as significance, unexpectedness, inevitability, and economy as factors that contribute to a mathematical aesthetic. Mathematical research often seeks critical features of a mathematical object. A theorem expressed as a characterization of the object by these features is the prize. Examples of particularly succinct and revelatory mathematical arguments has been published in Proofs from THE BOOK.\n\nThe popularity of recreational mathematics is another sign of the pleasure many find in solving mathematical questions. And at the other social extreme, philosophers continue to find problems in philosophy of mathematics, such as the nature of mathematical proof.\n\nNotation, language, and rigor\n\nupright|thumb|Leonhard Euler created and popularized much of the mathematical notation used today.\n\nMost of the mathematical notation in use today was not invented until the 16th century. Before that, mathematics was written out in words, limiting mathematical discovery.Kline, p. 140, on Diophantus; p. 261, on Vieta. Euler (1707–1783) was responsible for many of the notations in use today. Modern notation makes mathematics much easier for the professional, but beginners often find it daunting. According to Barbara Oakley, this can be attributed to the fact that mathematical ideas are both more abstract and more encrypted than those of natural language.Oakley 2014, p. 16: \"Focused problem solving in math and science is often more effortful than focused-mode thinking involving language and people. This may be because humans haven't evolved over the millennia to manipulate mathematical ideas, which are frequently more abstractly encrypted than those of conventional language.\" Unlike natural language, where people can often equate a word (such as cow) with the physical object it corresponds to, mathematical symbols are abstract, lacking any physical analog.Oakley 2014, p. 16: \"What do I mean by abstractness? You can point to a real live cow chewing its cud in a pasture and equate it with the letters c–o–w on the page. But you can't point to a real live plus sign that the symbol '+' is modeled after – the idea underlying the plus sign is more abstract.\" Mathematical symbols are also more highly encrypted than regular words, meaning a single symbol can encode a number of different operations or ideas.Oakley 2014, p. 16: \"By encryptedness, I mean that one symbol can stand for a number of different operations or ideas, just as the multiplication sign symbolizes repeated addition.\"\n\nMathematical language can be difficult to understand for beginners because even common terms, such as or and only, have a more precise meaning than they have in everyday speech, and other terms such as open and field refer to specific mathematical ideas, not covered by their laymen's meanings. Mathematical language also includes many technical terms such as homeomorphism and integrable that have no meaning outside of mathematics. Additionally, shorthand phrases such as iff for \"if and only if\" belong to mathematical jargon. There is a reason for special notation and technical vocabulary: mathematics requires more precision than everyday speech. Mathematicians refer to this precision of language and logic as \"rigor\".\n\nMathematical proof is fundamentally a matter of rigor. Mathematicians want their theorems to follow from axioms by means of systematic reasoning. This is to avoid mistaken \"theorems\", based on fallible intuitions, of which many instances have occurred in the history of the subject. The level of rigor expected in mathematics has varied over time: the Greeks expected detailed arguments, but at the time of Isaac Newton the methods employed were less rigorous. Problems inherent in the definitions used by Newton would lead to a resurgence of careful analysis and formal proof in the 19th century. Misunderstanding the rigor is a cause for some of the common misconceptions of mathematics. Today, mathematicians continue to argue among themselves about computer-assisted proofs. Since large computations are hard to verify, such proofs may not be sufficiently rigorous.Ivars Peterson, The Mathematical Tourist, Freeman, 1988, . p. 4 \"A few complain that the computer program can't be verified properly\", (in reference to the Haken–Apple proof of the Four Color Theorem).\n\nAxioms in traditional thought were \"self-evident truths\", but that conception is problematic.\"The method of 'postulating' what we want has many advantages; they are the same as the advantages of theft over honest toil.\" Bertrand Russell (1919), Introduction to Mathematical Philosophy, New York and London, p. 71.  At a formal level, an axiom is just a string of symbols, which has an intrinsic meaning only in the context of all derivable formulas of an axiomatic system. It was the goal of Hilbert's program to put all of mathematics on a firm axiomatic basis, but according to Gödel's incompleteness theorem every (sufficiently powerful) axiomatic system has undecidable formulas; and so a final axiomatization of mathematics is impossible. Nonetheless mathematics is often imagined to be (as far as its formal content) nothing but set theory in some axiomatization, in the sense that every mathematical statement or proof could be cast into formulas within set theory.Patrick Suppes, Axiomatic Set Theory, Dover, 1972, . p. 1, \"Among the many branches of modern mathematics set theory occupies a unique place: with a few rare exceptions the entities which are studied and analyzed in mathematics may be regarded as certain particular sets or classes of objects.\"\n\nFields of mathematics\n\nright|thumb|The abacus is a simple calculating tool used since ancient times.\nMathematics can, broadly speaking, be subdivided into the study of quantity, structure, space, and change (i.e. arithmetic, algebra, geometry, and analysis). In addition to these main concerns, there are also subdivisions dedicated to exploring links from the heart of mathematics to other fields: to logic, to set theory (foundations), to the empirical mathematics of the various sciences (applied mathematics), and more recently to the rigorous study of uncertainty. While some areas might seem unrelated, the Langlands program has found connections between areas previously thought unconnected, such as Galois groups, Riemann surfaces and number theory.\n\nFoundations and philosophy\nIn order to clarify the foundations of mathematics, the fields of mathematical logic and set theory were developed. Mathematical logic includes the mathematical study of logic and the applications of formal logic to other areas of mathematics; set theory is the branch of mathematics that studies sets or collections of objects. Category theory, which deals in an abstract way with mathematical structures and relationships between them, is still in development. The phrase \"crisis of foundations\" describes the search for a rigorous foundation for mathematics that took place from approximately 1900 to 1930.Luke Howard Hodgkin & Luke Hodgkin, A History of Mathematics, Oxford University Press, 2005. Some disagreement about the foundations of mathematics continues to the present day. The crisis of foundations was stimulated by a number of controversies at the time, including the controversy over Cantor's set theory and the Brouwer–Hilbert controversy.\n\nMathematical logic is concerned with setting mathematics within a rigorous axiomatic framework, and studying the implications of such a framework. As such, it is home to Gödel's incompleteness theorems which (informally) imply that any effective formal system that contains basic arithmetic, if sound (meaning that all theorems that can be proved are true), is necessarily incomplete (meaning that there are true theorems which cannot be proved in that system). Whatever finite collection of number-theoretical axioms is taken as a foundation, Gödel showed how to construct a formal statement that is a true number-theoretical fact, but which does not follow from those axioms. Therefore, no formal system is a complete axiomatization of full number theory. Modern logic is divided into recursion theory, model theory, and proof theory, and is closely linked to theoretical computer science, as well as to category theory. In the context of recursion theory, the impossibility of a full axiomatization of number theory can also be formally demonstrated as a consequence of the MRDP theorem.\n\nTheoretical computer science includes computability theory, computational complexity theory, and information theory. Computability theory examines the limitations of various theoretical models of the computer, including the most well-known model – the Turing machine. Complexity theory is the study of tractability by computer; some problems, although theoretically solvable by computer, are so expensive in terms of time or space that solving them is likely to remain practically unfeasible, even with the rapid advancement of computer hardware. A famous problem is the \"\" problem, one of the Millennium Prize Problems.Clay Mathematics Institute, P=NP, claymath.org Finally, information theory is concerned with the amount of data that can be stored on a given medium, and hence deals with concepts such as compression and entropy.\n\n{|style=\"border:1px solid #ddd; text-align:center; margin:auto\" cellspacing=\"15\"\n||| 128px || 96px || 96px\n|-\n|Mathematical logic || Set theory || Category theory || Theory of computation\n|}\n\nPure mathematics\nQuantity\n\nThe study of quantity starts with numbers, first the familiar natural numbers and integers (\"whole numbers\") and arithmetical operations on them, which are characterized in arithmetic. The deeper properties of integers are studied in number theory, from which come such popular results as Fermat's Last Theorem. The twin prime conjecture and Goldbach's conjecture are two unsolved problems in number theory.\n\nAs the number system is further developed, the integers are recognized as a subset of the rational numbers (\"fractions\"). These, in turn, are contained within the real numbers, which are used to represent continuous quantities. Real numbers are generalized to complex numbers. These are the first steps of a hierarchy of numbers that goes on to include quaternions and octonions. Consideration of the natural numbers also leads to the transfinite numbers, which formalize the concept of \"infinity\". According to the fundamental theorem of algebra all solutions of equations in one unknown with complex coefficients are complex numbers, regardless of degree. Another area of study is the size of sets, which is described with the cardinal numbers. These include the aleph numbers, which allow meaningful comparison of the size of infinitely large sets.\n\n{|style=\"border:1px solid #ddd; text-align:center; margin:auto\" cellspacing=\"20\"\n| ||  ||  ||  || \n|-\n|Natural numbers|| Integers || Rational numbers || Real numbers || Complex numbers\n|}\n\nStructure\n\nMany mathematical objects, such as sets of numbers and functions, exhibit internal structure as a consequence of operations or relations that are defined on the set. Mathematics then studies properties of those sets that can be expressed in terms of that structure; for instance number theory studies properties of the set of integers that can be expressed in terms of arithmetic operations. Moreover, it frequently happens that different such structured sets (or structures) exhibit similar properties, which makes it possible, by a further step of abstraction, to state axioms for a class of structures, and then study at once the whole class of structures satisfying these axioms. Thus one can study groups, rings, fields and other abstract systems; together such studies (for structures defined by algebraic operations) constitute the domain of abstract algebra.\n\nBy its great generality, abstract algebra can often be applied to seemingly unrelated problems; for instance a number of ancient problems concerning compass and straightedge constructions were finally solved using Galois theory, which involves field theory and group theory. Another example of an algebraic theory is linear algebra, which is the general study of vector spaces, whose elements called vectors have both quantity and direction, and can be used to model (relations between) points in space. This is one example of the phenomenon that the originally unrelated areas of geometry and algebra have very strong interactions in modern mathematics. Combinatorics studies ways of enumerating the number of objects that fit a given structure.\n\n{|style=\"border:1px solid #ddd; text-align:center; margin:auto\" cellspacing=\"15\"\n| || 96px || 96px || 96px || 96px || 96px\n|-\n|| Number theory || Algebra || Group theory || Combinatorics || Graph theory || Order theory \n|}\n\nSpace\n\nThe study of space originates with geometry – in particular, Euclidean geometry, which combines space and numbers, and encompasses the well-known Pythagorean theorem. Trigonometry is the branch of mathematics that deals with relationships between the sides and the angles of triangles and with the trigonometric functions. The modern study of space generalizes these ideas to include higher-dimensional geometry, non-Euclidean geometries (which play a central role in general relativity) and topology. Quantity and space both play a role in analytic geometry, differential geometry, and algebraic geometry. Convex and discrete geometry were developed to solve problems in number theory and functional analysis but now are pursued with an eye on applications in optimization and computer science. Within differential geometry are the concepts of fiber bundles and calculus on manifolds, in particular, vector and tensor calculus. Within algebraic geometry is the description of geometric objects as solution sets of polynomial equations, combining the concepts of quantity and space, and also the study of topological groups, which combine structure and space. Lie groups are used to study space, structure, and change. Topology in all its many ramifications may have been the greatest growth area in 20th-century mathematics; it includes point-set topology, set-theoretic topology, algebraic topology and differential topology. In particular, instances of modern-day topology are metrizability theory, axiomatic set theory, homotopy theory, and Morse theory. Topology also includes the now solved Poincaré conjecture, and the still unsolved areas of the Hodge conjecture. Other results in geometry and topology, including the four color theorem and Kepler conjecture, have been proved only with the help of computers.\n\n{|style=\"border:1px solid #ddd; text-align:center; margin:auto\" cellspacing=\"15\"\n|96px || 96px || 96px || 96px || 96px || 70px\n|-\n|Geometry || Trigonometry || Differential geometry || Topology || Fractal geometry || Measure theory\n|}\n\nChange\n\nUnderstanding and describing change is a common theme in the natural sciences, and calculus was developed as a tool to investigate it. Functions arise here, as a central concept describing a changing quantity. The rigorous study of real numbers and functions of a real variable is known as real analysis, with complex analysis the equivalent field for the complex numbers. Functional analysis focuses attention on (typically infinite-dimensional) spaces of functions. One of many applications of functional analysis is quantum mechanics. Many problems lead naturally to relationships between a quantity and its rate of change, and these are studied as differential equations. Many phenomena in nature can be described by dynamical systems; chaos theory makes precise the ways in which many of these systems exhibit unpredictable yet still deterministic behavior.\n\n96px  96px  96px  96px  96px  96pxCalculus  Vector calculus Differential equations  Complex analysis  Dynamical systems  Chaos theory\n\nApplied mathematics\n\nApplied mathematics concerns itself with mathematical methods that are typically used in science, engineering, business, and industry. Thus, \"applied mathematics\" is a mathematical science with specialized knowledge. The term applied mathematics also describes the professional specialty in which mathematicians work on practical problems; as a profession focused on practical problems, applied mathematics focuses on the \"formulation, study, and use of mathematical models\" in science, engineering, and other areas of mathematical practice.\n\nIn the past, practical applications have motivated the development of mathematical theories, which then became the subject of study in pure mathematics, where mathematics is developed primarily for its own sake. Thus, the activity of applied mathematics is vitally connected with research in pure mathematics.\n\nStatistics and other decision sciences\n\nApplied mathematics has significant overlap with the discipline of statistics, whose theory is formulated mathematically, especially with probability theory. Statisticians (working as part of a research project) \"create data that makes sense\" with random sampling and with randomized experiments;Rao, C.R. (1997) Statistics and Truth: Putting Chance to Work, World Scientific.  the design of a statistical sample or experiment specifies the analysis of the data (before the data be available). When reconsidering data from experiments and samples or when analyzing data from observational studies, statisticians \"make sense of the data\" using the art of modelling and the theory of inference – with model selection and estimation; the estimated models and consequential predictions should be tested on new data.\n\nStatistical theory studies decision problems such as minimizing the risk (expected loss) of a statistical action, such as using a procedure in, for example, parameter estimation, hypothesis testing, and selecting the best. In these traditional areas of mathematical statistics, a statistical-decision problem is formulated by minimizing an objective function, like expected loss or cost, under specific constraints: For example, designing a survey often involves minimizing the cost of estimating a population mean with a given level of confidence. Because of its use of optimization, the mathematical theory of statistics shares concerns with other decision sciences, such as operations research, control theory, and mathematical economics.: \n\nComputational mathematics\nComputational mathematics proposes and studies methods for solving mathematical problems that are typically too large for human numerical capacity. Numerical analysis studies methods for problems in analysis using functional analysis and approximation theory; numerical analysis includes the study of approximation and discretization broadly with special concern for rounding errors. Numerical analysis and, more broadly, scientific computing also study non-analytic topics of mathematical science, especially algorithmic matrix and graph theory. Other areas of computational mathematics include computer algebra and symbolic computation.\n\n96px 96px  96px  96px  96px  96px  96pxGame theory  Fluid dynamics  Numerical analysis  Optimization  Probability theory  Statistics  Cryptography96px  96px  96px  96px  96px  96pxMathematical finance  Mathematical physics  Mathematical chemistry  Mathematical biology Mathematical economics  Control theory\n\nMathematical awards\nArguably the most prestigious award in mathematics is the Fields Medal, established in 1936 and awarded every four years (except around World War II) to as many as four individuals. The Fields Medal is often considered a mathematical equivalent to the Nobel Prize.\n\nThe Wolf Prize in Mathematics, instituted in 1978, recognizes lifetime achievement, and another major international award, the Abel Prize, was instituted in 2003. The Chern Medal was introduced in 2010 to recognize lifetime achievement. These accolades are awarded in recognition of a particular body of work, which may be innovational, or provide a solution to an outstanding problem in an established field.\n\nA famous list of 23 open problems, called \"Hilbert's problems\", was compiled in 1900 by German mathematician David Hilbert. This list achieved great celebrity among mathematicians, and at least nine of the problems have now been solved. A new list of seven important problems, titled the \"Millennium Prize Problems\", was published in 2000. Only one of them, the Riemann hypothesis, duplicates one of Hilbert's problems. A solution to any of these problems carries a $1 million reward.\n\nSee also\n\n International Mathematical Olympiad\n Lists of mathematics topics\n Mathematical sciences\n Mathematics and art\n Mathematics education\n National Museum of Mathematics\n Philosophy of mathematics\n Relationship between mathematics and physics\n Science, Technology, Engineering, and Mathematics\n\nNotes\n\nReferences\n\nBibliography\n\n \n \n \n \n \n \n \n \n \n .\n \n \n \n \n \n\nFurther reading\n\n \n \n \n  – A translated and expanded version of a Soviet mathematics encyclopedia, in ten volumes. Also in paperback and on CD-ROM, and online.\n \n \n\n \nCategory:Formal sciences\n \nCategory:Main topic articles"
    },
    {
      "title": "Outline of mathematics",
      "url": "https://en.wikipedia.org/wiki/Outline_of_mathematics",
      "text": "Mathematics is a field of study that investigates topics including number, space, structure, and change.  For more on the relationship between mathematics and science, refer to the article on science.\n\nNature of mathematics\nDefinitions of mathematics – Mathematics has no generally accepted definition. Different schools of thought, particularly in philosophy, have put forth radically different definitions, all of which are controversial.\nPhilosophy of mathematics – its aim is to provide an account of the nature and methodology of mathematics and to understand the place of mathematics in people's lives.\n\nMathematics is\nan academic discipline – branch of knowledge that is taught at all levels of education and researched typically at the college or university level. Disciplines are defined (in part), and recognized by the academic journals in which research is published, and the learned societies and academic departments or faculties to which their practitioners belong.\na formal science – branch of knowledge concerned with the properties of formal systems based on definitions and rules of inference. Unlike other sciences, the formal sciences are not concerned with the validity of theories based on observations in the physical world.\n\nGeneral reference\n\nClassification systems\nMathematics in the Dewey Decimal Classification system\n\nMathematics Subject Classification – alphanumerical classification scheme collaboratively produced by staff of and based on the coverage of the two major mathematical reviewing databases, Mathematical Reviews and Zentralblatt MATH.\n\nReference databases\nMathematical Reviews – journal and online database published by the American Mathematical Society (AMS) that contains brief synopses (and occasionally evaluations) of many articles in mathematics, statistics and theoretical computer science.\nZentralblatt MATH – service providing reviews and abstracts for articles in pure and applied mathematics, published by Springer Science+Business Media. It is a major international reviewing service which covers the entire field of mathematics. It uses the Mathematics Subject Classification codes for organizing their reviews by topic.\n\nSubjects\n\nQuantity\nQuantity\nArithmetic\nNatural numbers\nIntegers\nRational numbers\nReal numbers\nComplex numbers\nHypercomplex numbers\nInfinity\n\nStructure\nStructure\nAbstract algebra\nLinear algebra\nNumber theory\nOrder theory\nFunction (mathematics)\n\nSpace\nSpace\nGeometry\nAlgebraic geometry\nTrigonometry\nDifferential geometry\nTopology\nFractal geometry\n\nChange\nChange\nCalculus\nVector calculus\nDifferential equations\nDynamical systems\nChaos theory\nAnalysis\n\nFoundations and philosophy\nFoundations of mathematics\nPhilosophy of mathematics\nCategory theory\nSet theory\nType theory\n\nMathematical logic\n\nMathematical logic\nModel theory\nProof theory\nSet theory\nType theory\nRecursion theory\nTheory of Computation\n\nDiscrete mathematics\nDiscrete mathematics\nCombinatorics\nCryptography\nGraph theory\n\nApplied mathematics\nApplied mathematics\nMathematical physics\nAnalytical mechanics\nMathematical fluid dynamics\nNumerical analysis\nControl theory\nDynamical systems\nMathematical optimization\nOperations research\nProbability\nStatistics\nGame theory\nMathematical economics\nFinancial mathematics\nInformation theory\nCryptography\nMathematical biology\n\nHistory\n\nHistory of mathematics\nBabylonian mathematics\nEgyptian mathematics\nIndian mathematics\nGreek mathematics\nChinese mathematics\nHistory of the Hindu–Arabic numeral system\nIslamic mathematics\nJapanese mathematics\nHistory of algebra\nHistory of geometry\nHistory of mathematical notation\nHistory of trigonometry\nHistory of writing numbers\n\nPsychology\nMathematics education\nNumeracy\nNumerical Cognition\nSubitizing\nMathematical anxiety\nDyscalculia\nAcalculia\nAgeometresia\nNumber sense\nNumerosity adaptation effect\nApproximate number system\nMathematical maturity\n\nInfluential mathematicians\nSee Lists of mathematicians.\n\nMathematical notation\n\nMathematical notation\n List of mathematical abbreviations\n List of mathematical symbols\n List of mathematical symbols by subject\n Table of mathematical symbols by introduction date\n Notation in probability and statistics\n Table of logic symbols\n Physical constants\n Greek letters used in mathematics, science, and engineering\n Latin letters used in mathematics\n Mathematical alphanumeric symbols\n Mathematical operators and symbols in Unicode\n ISO 31-11 (Mathematical signs and symbols for use in physical sciences and technology)\n\nSee also\n\nLists of mathematics topics\nAreas of mathematics\nGlossary of areas of mathematics\n\n External links \n\nMAA Reviews – The Basic Library List – Mathematical Association of America\nNaoki's Recommended Books, compiled by Naoki Saito, U. C. Davis\nA List of Recommended Books in Topology, compiled by Allen Hatcher, Cornell U.\nBooks in algebraic geometry in nLab\n\n+\n+\nMathematics\nMathematics\nCategory:Mathematics-related lists"
    },
    {
      "title": "Analysis of Boolean functions",
      "url": "https://en.wikipedia.org/wiki/Analysis_of_Boolean_functions",
      "text": "In mathematics and theoretical computer science, analysis of Boolean functions is the study of real-valued functions on  or  from a spectral perspective (such functions are sometimes known as pseudo-Boolean functions). The functions studied are often, but not always, Boolean-valued, making them Boolean functions. The area has found many applications in combinatorics, social choice theory, random graphs, and theoretical computer science, especially in hardness of approximation, property testing and PAC learning.\n\nBasic concepts\n\nWe will mostly consider functions defined on the domain . Sometimes it is more convenient to work with the domain  instead. If  is defined on , then the corresponding function defined on  is\n\nSimilarly, for us a Boolean function is a -valued function, though often it is more convenient to consider -valued functions instead.\n\nFourier expansion\n\nEvery real-valued function  has a unique expansion as a multilinear polynomial:\n\nThis is the Hadamard transform of the function , which is the Fourier transform in the group . The coefficients  are known as Fourier coefficients, and the entire sum is known as the Fourier expansion of . The functions  are known as Fourier characters, and they form an orthonormal basis for the space of all functions over , with respect to the inner product .\n\nThe Fourier coefficients can be calculated using an inner product:\n\nIn particular, this shows that  Parseval's identity states that\n\nIf we skip , then we get the variance of :\n\n Fourier degree and Fourier levels \n\nThe degree of a function  is the maximum  such that  for some set  of size . In other words, the degree of  is its degree as a multilinear polynomial.\n\nIt is convenient to decompose the Fourier expansion into levels: the Fourier coefficient  is on level .\n\nThe degree  part of  is\n\nIt is obtained from  by zeroing out all Fourier coefficients not on level .\n\nWe similarly define .\n\nInfluence\n\nThe 'th influence of a function  can be defined in two equivalent ways:\n\n \n\nIf  is Boolean then  is the probability that flipping the 'th coordinate flips the value of the function:\n\nIf  then  doesn't depend on the 'th coordinate.\n\nThe total influence of  is the sum of all of its influences:\n\nThe total influence of a Boolean function is also the average sensitivity of the function. The sensitivity of a Boolean function  at a given point is the number of coordinates  such that if we flip the 'th coordinate, the value of the function changes. The average value of this quantity is exactly the total influence.\n\nThe total influence can also be defined using the discrete Laplacian of the Hamming graph, suitably normalized:  .\n\nNoise stability\n\nGiven , we say that two random vectors  are -correlated if the marginal distributions of  are uniform, and . Concretely, we can generate a pair of -correlated random variables by first choosing  uniformly at random, and then choosing  according to one of the following two equivalent rules, applied independently to each coordinate:\n\nWe denote this distribution by .\n\nThe noise stability of a function  at  can be defined in two equivalent ways:\n\nFor , the noise sensitivity of  at  is\n\nIf  is Boolean, then this is the probability that the value of  changes if we flip each coordinate with probability , independently.\n\nNoise operator\n\nThe noise operator  is an operator taking a function  and returning another function  given by\n\nWhen , the noise operator can also be defined using a continuous-time Markov chain in which each bit is flipped independently with rate 1. The operator  corresponds to running this Markov chain for  steps starting at , and taking the average value of  at the final state. This Markov chain is generated by the Laplacian of the Hamming graph, and this relates total influence to the noise operator.\n\nNoise stability can be defined in terms of the noise operator: .\n\nHypercontractivity\n\nFor , the -norm of a function  is defined by\n\nWe also define \n\nThe hypercontractivity theorem states that for any  and ,\n\nHypercontractivity is closely related to the logarithmic Sobolev inequalities of functional analysis.\n\nA similar result for  is known as reverse hypercontractivity.\n\np-Biased analysis\n\nIn many situations the input to the function is not uniformly distributed over , but instead has a bias toward  or . In these situations it is customary to consider functions over the domain . For , the p-biased measure  is given by\n\nThis measure can be generated by choosing each coordinate independently to be 1 with probability  and 0 with probability .\n\nThe classical Fourier characters are no longer orthogonal with respect to this measure. Instead, we use the following characters:\n\nThe p-biased Fourier expansion of  is the expansion of  as a linear combination of p-biased characters:\n\nWe can extend the definitions of influence and the noise operator to the p-biased setting by using their spectral definitions.\n\nInfluence\n\nThe 's influence is given by\n\nThe total influence is the sum of the individual influences:\n\nNoise operator\n\nA pair of -correlated random variables can be obtained by choosing  independently and , where  is given by\n\nThe noise operator is then given by\n\nUsing this we can define the noise stability and the noise sensitivity, as before.\n\nRusso–Margulis formula\n\nThe Russo–Margulis formula states that for monotone Boolean functions ,\n\nBoth the influence and the probabilities are taken with respect to , and on the right-hand side we have the average sensitivity of . If we think of  as a property, then the formula states that as  varies, the derivative of the probability that  occurs at  equals the average sensitivity at .\n\nThe Russo–Margulis formula is key for proving sharp threshold theorems such as Friedgut's.\n\nGaussian space\n\nOne of the deepest results in the area, the invariance principle, connects the distribution of functions on the Boolean cube  to their distribution on Gaussian space, which is the space  endowed with the standard -dimensional Gaussian measure.\n\nMany of the basic concepts of Fourier analysis on the Boolean cube have counterparts in Gaussian space:\n\n The counterpart of the Fourier expansion in Gaussian space is the Hermite expansion, which is an expansion to an infinite sum (converging in ) of multivariate Hermite polynomials.\n The counterpart of total influence or average sensitivity for the indicator function of a set is Gaussian surface area, which is the Minkowski content of the boundary of the set.\n The counterpart of the noise operator is the Ornstein–Uhlenbeck operator (related to the Mehler transform), given by , or alternatively by , where  is a pair of -correlated standard Gaussians.\n Hypercontractivity holds (with appropriate parameters) in Gaussian space as well.\n\nGaussian space is more symmetric than the Boolean cube (for example, it is rotation invariant), and supports continuous arguments which may be harder to get through in the discrete setting of the Boolean cube. The invariance principle links the two settings, and allows deducing results on the Boolean cube from results on Gaussian space.\n\nBasic results\n\nFriedgut–Kalai–Naor theorem\n\nIf  has degree at most 1, then  is either constant, equal to a coordinate, or equal to the negation of a coordinate. In particular,  is a dictatorship: a function depending on at most one coordinate.\n\nThe Friedgut–Kalai–Naor theorem, also known as the FKN theorem, states that if  almost has degree 1 then it is close to a dictatorship. Quantitatively, if  and , then  is -close to a dictatorship, that is,  for some Boolean dictatorship , or equivalently,  for some Boolean dictatorship .\n\nSimilarly, a Boolean function of degree at most  depends on at most  coordinates, making it a junta (a function depending on a constant number of coordinates), where  is an absolute constant equal to at least 1.5, and at most 4.41, as shown by Wellens. The Kindler–Safra theorem generalizes the Friedgut–Kalai–Naor theorem to this setting. It states that if  satisfies  then  is -close to a Boolean function of degree at most .\n\nKahn–Kalai–Linial theorem\n\nThe Poincaré inequality for the Boolean cube (which follows from formulas appearing above) states that for a function ,\n\nThis implies that .\n\nThe Kahn–Kalai–Linial theorem, also known as the KKL theorem, states that if  is Boolean then .\n\nThe bound given by the Kahn–Kalai–Linial theorem is tight, and is achieved by the Tribes function of Ben-Or and Linial:\n\nThe Kahn–Kalai–Linial theorem was one of the first results in the area, and was the one introducing hypercontractivity into the context of Boolean functions.\n\nFriedgut's junta theorem\n\nIf  is an -junta (a function depending on at most  coordinates) then  according to the Poincaré inequality.\n\nFriedgut's theorem is a converse to this result. It states that for any , the function  is -close to a Boolean junta depending on  coordinates.\n\nCombined with the Russo–Margulis lemma, Friedgut's junta theorem implies that for every , every monotone function is close to a junta with respect to  for some .\n\nInvariance principle\n\nThe invariance principle generalizes the Berry–Esseen theorem to non-linear functions.\n\nThe Berry–Esseen theorem states (among else) that if  and no  is too large compared to the rest, then the distribution of  over  is close to a normal distribution with the same mean and variance.\n\nThe invariance principle (in a special case) informally states that if  is a multilinear polynomial of bounded degree over  and all influences of  are small, then the distribution of  under the uniform measure over  is close to its distribution in Gaussian space.\n\nMore formally, let  be a univariate Lipschitz function, let , let , and let\n. Suppose that . Then\n\nBy choosing appropriate , this implies that the distributions of  under both measures are close in CDF distance, which is given by .\n\nThe invariance principle was the key ingredient in the original proof of the Majority is Stablest theorem.\n\nSome applications\n\nLinearity testing\n\nA Boolean function  is linear if it satisfies , where . It is not hard to show that the Boolean linear functions are exactly the characters .\n\nIn property testing we want to test whether a given function is linear. It is natural to try the following test: choose  uniformly at random, and check that . If  is linear then it always passes the test. Blum, Luby and Rubinfeld showed that if the test passes with probability  then  is -close to a Fourier character. Their proof was combinatorial.\n\nBellare et al. gave an extremely simple Fourier-analytic proof, that also shows that if the test succeeds with probability , then  is correlated with a Fourier character. Their proof relies on the following formula for the success probability of the test:\n\nArrow's theorem\n\nArrow's impossibility theorem states that for three and more candidates, the only unanimous voting rule for which there is always a Condorcet winner is a dictatorship.\n\nThe usual proof of Arrow's theorem is combinatorial. Kalai gave an alternative proof of this result in the case of three candidates using Fourier analysis. If  is the rule that assigns a winner among two candidates given their relative orders in the votes, then the probability that there is a Condorcet winner given a uniformly random vote is , from which the theorem easily follows.\n\nThe FKN theorem implies that if  is a rule for which there is almost always a Condorcet winner, then  is close to a dictatorship.\n\nSharp thresholds\n\nA classical result in the theory of random graphs states that the probability that a  random graph is connected tends to  if . This is an example of a sharp threshold: the width of the \"threshold window\", which is , is asymptotically smaller than the threshold itself, which is roughly . In contrast, the probability that a  graph contains a triangle tends to  when . Here both the threshold window and the threshold itself are , and so this is a coarse threshold.\n\nFriedgut's sharp threshold theorem states, roughly speaking, that a monotone graph property (a graph property is a property which doesn't depend on the names of the vertices) has a sharp threshold unless it is correlated with the appearance of small subgraphs. This theorem has been widely applied to analyze random graphs and percolation.\n\nOn a related note, the KKL theorem implies that the width of threshold window is always at most .\n\nMajority is stablest\n\nLet  denote the majority function on  coordinates. Sheppard's formula gives the asymptotic noise stability of majority:\n\nThis is related to the probability that if we choose  uniformly at random and form  by flipping each bit of  with probability , then the majority stays the same:\n.\n\nThere are Boolean functions with larger noise stability. For example, a dictatorship  has noise stability .\n\nThe Majority is Stablest theorem states, informally, then the only functions having noise stability larger than majority have influential coordinates. Formally, for every  there exists  such that if  has expectation zero and , then .\n\nThe first proof of this theorem used the invariance principle in conjunction with an isoperimetric theorem of Borell in Gaussian space; since then more direct proofs were devised.\n\nMajority is Stablest implies that the Goemans–Williamson approximation algorithm for MAX-CUT is optimal, assuming the unique games conjecture. This implication, due to Khot et al., was the impetus behind proving the theorem.\n\nReferences\n\nCategory:Boolean algebra\nCategory:Mathematical optimization\nCategory:Mathematics\nCategory:Theoretical computer science"
    },
    {
      "title": "The Archimedeans",
      "url": "https://en.wikipedia.org/wiki/The_Archimedeans",
      "text": "The Archimedeans are the mathematical society of the University of Cambridge, founded in 1935. It currently has over 2000 active members, many of them alumni, making it one of the largest student societies in Cambridge. The society hosts regular talks at the Centre for Mathematical Sciences, including in the past by many well-known speakers in the field of mathematics. It publishes two magazines, Eureka and QARCH.\n\nOne of several aims of the society, as laid down in its constitution, is to encourage co-operation between the existing mathematical societies of individual Cambridge colleges, which at present are just the Adam's society of St John's College and the Trinity Mathematical Society, but in the past have included many more.\n\nThe society is mentioned in G. H. Hardy's essay A Mathematician's Apology.\n\nPast presidents of The Archimedeans include Michael Atiyah and Richard Taylor.\n\n Activity \nThe main focus of the society's activities are the regular talks, which generally concern topics from mathematics or theoretical physics, and are accessible to students on an undergraduate level. Among the list of recent speakers are Fields medalists Michael Atiyah, Wendelin Werner and Alain Connes, as well as authors Ian Stewart and Simon Singh. Many of the speakers are international, and are hosted by The Archimedeans during their visit.\n\nAfter exams and University-wide project deadlines, the society is also known to organise social events, which have shown to be highly popular.\n\n Publications \nEureka is a mathematical journal that is published annually by The Archimedeans. It includes articles on a variety of topics in mathematics, written by students and academics from all over the world, as well as a short summary of the activities of the society, problem sets, puzzles, artwork and book reviews. The magazine has been published 60 times since 1939, and authors include many famous mathematicians and scientists such as Paul Erdős, Martin Gardner, Douglas Hofstadter, Godfrey Hardy, Béla Bollobás, John Conway, Stephen Hawking, Roger Penrose, Ian Stewart, Fields Medallist Timothy Gowers and Nobel laureate Paul Dirac.\n\nThe journal is distributed free of charge to all current members of the Archimedeans. In addition, there are many subscriptions by other students, alumni and libraries. Subscriptions to Eureka are the society's main source of income.\n\nThe Archimedeans also publish QARCH, a magazine containing problem sets and solutions or partial solutions submitted by readers. It is published on an irregular basis and distributed free of charge.\n\n References \n\nCategory:British mathematicians\nCategory:Mathematics\nCategory:Clubs and societies of the University of Cambridge\nCategory:Student organizations established in 1935"
    },
    {
      "title": "Archives of American Mathematics",
      "url": "https://en.wikipedia.org/wiki/Archives_of_American_Mathematics",
      "text": "The Archives of American Mathematics, located at the University of Texas at Austin,  aims to collect, preserve, and provide access to the papers principally of American mathematicians and the records of American mathematical organizations.\n\n  History  \n\nThe Archives  began in 1975 at the University of Texas at Austin with the preservation of the papers of Texas mathematicians R.L. Moore and H.S. Vandiver.\n\nIn 1978, the Mathematical Association of America established the university as the official repository for its archival records and the name \"Archives of American Mathematics\" was adopted to encompass all of the mathematical archival collections at the university.The Minutes of the Board of Regents of The University of Texas System, 8–9 June 1978. https://www.utsystem.edu/sites/default/files/offices/board-of-regents/board-meetings/board-minutes/6-78meeting754.pdf pp. 3307-3312, accessed 17 July 2017.  Originally a part of the Harry Ransom Center, in 1984, the Archives was added to the special collections of the Briscoe Center for American History at the University of Texas at Austin.\n\n Collections \n\nThe AAM includes approximately 120 collections.\n\n Notable Examples \n\nMathematical Association of America Records.\nThomas F. Banchoff Papers  document a career of teaching, writing, and making mathematical films.\nMarion Walter Photograph Collection  includes photographs of A.A.  Albert, H.S.M.  Coxeter, Paul Erdős, Fritz John, D.H. Lehmer, Alexander Ostrowski, George Polya, Mina Rees, and Olga Taussky-Todd.\nSchool Mathematics Study Group Records  document the history of the \"New Math\" movement of the 1960s, and includes the files of the director, Edward G.  Begle.\nDorothy L. Bernstein Papers  reflect both her professional and personal life.\nPaul R. Halmos Photograph Collection  consists of 14,000 photographs Halmos and others took from the 1930s to 2006.\nIvor Grattan-Guinness Papers  reflect the career of a mathematics historian.\nPaul Erdős and Carl Pomerance Correspondence Collection  consists of 435 letters between Erdős and Pomerance.\n\n Related Collections Elsewhere \nSignificant archives of American mathematicians and their organizations are held by other repositories. The following are examples which include a few Canadian collections with substantial United States connections. For the complete holdings, the catalogs of the individual repositories would need to be consulted. In addition, the archives of academic institutions will typically include administrative records of mathematics departments and clubs as well as the papers of faculty.\n\n John Hay Library, Brown University   -- American Mathematical Society Records (1888- ); Raymond Clare Archibald (1875-1957); James Glaisher (1848-1928); R. G. D. Richardson (1878-1949); Marshall Harvey Stone (1903-1989); James Joseph Sylvester (1814-1897) also at St. John's College (Cambridge).\n American Philosophical Society -- Robert Patterson  (1743-1824); David Rittenhouse  (1732-1796); Robert Adrain  (1775-1843); Samuel Stanley Wilks  (1906-1964).\n Amherst College—Ebenezer Strong Snell  (1801-1876).\n Boston Public Library -- Nathaniel Bowditch  (1773-1838); Nicholas Pike  (1743-1819).\n College of Charleston Library—Lewis Reeves Gibbes  (1810-1894), also Library of Congress;\n Columbia University --  Arthur Korn  (1870-1945);  Cassius Jackson Keyser  (1862-1947); Christine Franklin (1847-1930) and Fabian Franklin  (1853-1939); F. A. P. Barnard  (1809-1889);  Harold Hotelling  (1895-); Henry Seely White  (1861-1943);  John Howard Van Amringe  (1835-1915); Thomas Scott Fiske  (1865-1944); David Eugene Smith  (1860-1944).\n Dartmouth College -- George Robert Stibitz  (1904-1995).\n Duke University—Edward Henry Courtenay  (1803-1853).\n Hampshire College—Herman Goldstine  (1913-2004).\n Harvard University --  Benjamin Peirce  (1809-1880);  Charles Sanders Peirce  (1839-1914); Damodar Dharmanand Kosambi  (1907–1966); Isaac Greenwood  (1702-1745);  John Farrar  (1779-1853); John Winthrop  (1714–1779);  Maxime Bôcher  (1867-1918); Richard Von Mises  (1883-1953);  Thomas Hill  (1818-1891); George David Birkhoff  (1884-1944).\n Iowa State University --  American Statistical Association (1839-); Herbert Solomon  (1919-2004); Edward J. Wegman  (1943-);  Eugene Lukacs  (1906-1987); Ingram Olkin  (1924-).\n Hope College—Albert Eugene Lampen  (1887-1963); Jay Erenst Folkert  (1916-).\n Library of Congress --  Andrew Ellicott  (1754-1820); George F. Becker  (1847-1919); Oswald Veblen  (1880-1960); Lewis Reeves Gibbes  (1810-1894), John von Neumann (1903-1957), also College of Charleston.\n Maryland Historical Society --  John Henry Alexander  (1812-1867).\n McMaster University (Canada) -- Bertrand Russell  (1872-1970).\n Massachusetts Institute of Technology --  John Daniel Runkle  (1822-1902); Norbert Wiener  (1894-1964).\n New Jersey Historical Society --  Francis Robbins Upton  (1852-1921).\n New York Public Library -- Ferdinand Rudolph Hassler  (1770-1843).\n New York University --  Richard Courant  (1888-1972).\n Northwestern University --  Ernst D. Hellinger  (1883-1950); Helen M. Clark  (1908-1974); Lois W. Griffiths  (1899-1981).\n Ohio History Connection --  Jared Mansfield  (1759-1830), also  U.S. Military Academy.\n Princeton University --  Alfred James Lotka  (1880-1949);  Walter Minto (1753-1796);  Eugene Paul Wigner  (1902-1995); Henry Dallas Thompson  (1864-1927); Kurt Gödel  (1906-1978); Nicola Fergola  (1757-1824); Sylvester Robins  (files: 1880-1899).\n Rice University—Fred Terry Rogers  (1914-1956);  Salomon Bochner  (1899-1982).\n Rockefeller University --  Mark Kac  (1914-1984).\n Rutgers University—Edward Albert Bowser  (1837-1910);  George Washington Coakley  (1814-1893).\n Smith College, Sophia Smith Collection --  Dorothy Maud Wrinch  (1895-1976).\n Stanford University --  Georg Pólya  (1887-1985).\n University of Chicago --  A. Adrian Albert  (1905-1972 ); Saunders Mac Lane (1909-2005); E. H. Moore (1862-1932); Alfred L. Putnam (1916-2004); Nicolas Rashevsky (1899-1972); Ernest Julius Wilczynski (1876-1932).\n University of Illinois at Urbana-Champaign --  Arnold Emch  (1871-1959); Arthur Byron Coble  (1878-1966); George Abram Miller  (1863-1951);  George William Meyers  (1864-1931); Leonard L. Steimley  (1890-1975);  Olive C. Hazlett  (1890-1974); Robert Daniel Carmichael  (1879-1967).\n University of Michigan—Wooster Woodruff Beman (1850-1922);  Louis Allen Hopkins  (1881-).\n State Historical Society of Missouri—Joseph Ficklin  (1833-1887).\n University of North Carolina; U. of Texas --  Charles Scott Venable  (1827-1900).\n University of Oklahoma Library -- Nathan Altshiller Court  (1881-1968).\n University of Toronto --  Kenneth O. May  (1915-1977).\n University of Virginia -- G. T. Whyburn (1904-1969)\n University of Washington Libraries --  Carl B. Allendoerfer  (1911-1974).\n University of Wisconsin --  Albert C. Schaeffer  (files: 1954-1956); Bronson Barlow (Mathematics of Design)   (b. 1924); Charles S. Slichter  (files: 1891-1941); Cyrus C. MacDuffee  (1895-1961); Edward Burr Van Vleck  (1863-1943); Ernest B. Skinner  (files: 1892-1935); Isaac Schoenberg  (files: 1930-1980); Ivan Sokolnikoff  (1901-); J. Barkley Rosser  (1907-1989); John D. Mayor  (?);  Mark H. Ingraham  (1896-1982); Military Training Programs, WW II  (1943-1945); U.S. Naval Research Office  (1951-1955); Rudolph E. Langer  (1894-1968); Stephen Kleene  (1909-1994); Warren Weaver  (1894-1978).\n Virginia Military Institute --  Claudius Crozet  (1789-1864).\n Virginia Polytechnic Institute and State U. --  Irving John Good  (1916-2009); John Edward Williams  (1867-1943).\n Wake Forest University—John Wesley Sawyer  (1916-);  Roland L. Gay  (1905-1979)\n Western Reserve Historical Society (Cleveland) --  Elisha Scott Loomis  (1852-1940).\n Yale University Library --  Abraham Robinson  (1918-1974);  Elias Loomis  (1811-1889); Josiah Willard Gibbs  (1839-1903).\n Yeshiva University -- Jekuthiel Ginsburg (1889-1957)\n\n References \n\n External links \n Archives of American Mathematics\n\nCategory:Archives in the United States\nCategory:Mathematics\nCategory:University of Texas at Austin"
    },
    {
      "title": "Cayley–Menger determinant",
      "url": "https://en.wikipedia.org/wiki/Cayley%E2%80%93Menger_determinant",
      "text": "In linear algebra, geometry, and trigonometry, the Cayley–Menger determinant is a formula for the content, i.e. the higher-dimensional volume, of a -dimensional simplex in terms of the squares of all of the distances between pairs of its vertices.\n\n Definition \nLet  be  points in -dimensional Euclidean space, often with . These points are the vertices of an n-dimensional simplex: a triangle when ; a tetrahedron when , and so on. Let  be the distances between  and , for . The content, i.e. the n-dimensional volume of this simplex, denoted by , can be expressed as a function of determinants of certain matrices, as follows:\n\nThis is the Cayley–Menger determinant. It is a symmetric polynomial in the 's and is thus invariant under permutation of these quantities.\n\nA proof of the second equation can be found . From the second equation, the first can be derived by elementary row and colomn operations:\n\nthen exchange the first and last column, gaining a , and multiply each of its  inner rows by .\n\n Generalization to hyperbolic and spherical geometry \nThere are spherical and hyperbolic generalizations. A proof can be found here .\n\nIn a spherical space of dimension  and constant curvature , any  points satisfy\n\nwhere , and  is the spherical distance between points .\n\nIn a hyperbolic space of dimension  and constant curvature , any  points satisfy\n\nwhere , and  is the hyperbolic distance between points .\n\n Example \nIn the case of , we have that  is the area of a triangle and thus we will denote this by . By the Cayley–Menger determinant, where the triangle has side lengths ,  and ,\n\n \n\nThe result in the third line is due to the Fibonacci identity. The final line can be rewritten to obtain Heron's formula for the area of a triangle given three sides, which was known to Archimedes prior.\n\nIn the case of , the quantity  gives the volume of a tetrahedron, which we will denote by . For distances between  and  given by , the Cayley–Menger determinant gives us\n\n \n\n Finding the circumradius of a simplex \nGiven a nondegenerate n-simplex, it has a circumscribed n-sphere, with radius . Then the (n+1)-simplex made of the vertices of the n-simplex and the center of the n-sphere is degenerate. Thus, we have \n\nIn particular, when , this gives the circumradius of a triangle in terms of its edge lengths.\n\n See also \n\n Distance geometry\n\n References \n\nCategory:Mathematics"
    },
    {
      "title": "Data-driven control system",
      "url": "https://en.wikipedia.org/wiki/Data-driven_control_system",
      "text": "Data-driven control systems are a broad family of control systems, in which the identification of the process model and/or the design of the controller are based entirely on experimental data collected from the plant Bazanella, A.S., Campestrini, L., Eckhard, D. (2012). Data-driven controller design: the  approach. Springer, , 208 pages..\n\nIn many control applications, trying to write a mathematical model of the plant is considered a hard task, requiring efforts and time to the process and control engineers. This problem is overcome by data-driven methods, which allow to fit a system model to the experimental data collected, choosing it in a specific models class. The control engineer can then exploit this model to design a proper controller for the system. However, it is still difficult to find a simple yet reliable model for a physical system, that includes only those dynamics of the system that are of interest for the control specifications. The direct data-driven methods allow to tune a controller, belonging to a given class, without the need of an identified model of the system. In this way, one can also simply weight process dynamics of interest inside the control cost function, and exclude those dynamics that are out of interest.\n\n Overview \n\nThe standard approach to control systems design is organized in two-steps: \n Model identification aims at estimating a nominal model of the system , where  is the unit-delay operator (for discrete-time transfer functions representation) and  is the vector of parameters of  identified on a set of  data. Then, validation consists in constructing the uncertainty set  that contains the true system  at a certain probability level.\n Controller design aims at finding a controller  achieving closed-loop stability and meeting the required performance with .\nTypical objectives of system identification are to have  as close as possible to , and to have  as small as possible. However, from an identification for control perspective, what really matters is the performance achieved by the controller, not the intrinsic quality of the model.\n\nOne way to deal with uncertainty is to design a controller that has an acceptable performance with all models in , including . This is the main idea behind robust control design procedure, that aims at building frequency domain uncertainty descriptions of the process. However, being based on worst-case assumptions rather than on the idea of averaging out the noise, this approach typically leads to conservative uncertainty sets. Rather, data-driven techniques deal with uncertainty by working on experimental data, and avoiding excessive conservativism.\n\nIn the following, the main classifications of data-driven control systems are presented.\n\n Indirect and direct methods \nThere are many methods available to control the systems. \nThe fundamental distinction is between indirect and direct controller design methods. The former group of techniques is still retaining the standard two-step approach, i.e. first a model is identified, then a controller is tuned based on such model. The main issue in doing so is that the controller is computed from the estimated model  (according to the certainty equivalence principle), but in practice . To overcome this problem, the idea behind the latter group of techniques is to map the experimental data directly onto the controller, without any model to be identified in between.\n\n Iterative and noniterative methods \n\nAnother important distinction is between iterative and noniterative (or one-shot) methods. In the former group, repeated iterations are needed to estimate the controller parameters, during which the optimization problem is performed based on the results of the previous iteration, and the estimation is expected to become more and more accurate at each iteration. This approach is also prone to on-line implementations (see below). In the latter group, the (optimal) controller parametrization is provided with a single optimization problem. This is particularly important for those systems in which iterations or repetitions of data collection experiments are limited or even not allowed (for example, due to economic aspects). In such cases, one should select a design technique capable of delivering a controller on a single data set. This approach is often implemented off-line (see below).\n\n On-line and off-line methods \n\nSince, on practical industrial applications, open-loop or closed-loop data are often available continuously, on-line data-driven techniques use those data to improve the quality of the identified model and/or the performance of the controller each time new information is collected on the plant. Instead, off-line approaches work on batch of data, which may be collected only once, or multiple times at a regular (but rather long) interval of time.\n\n Iterative feedback tuning \n\nThe iterative feedback tuning (IFT) method was introduced in 1994 Hjalmarsson, H., Gevers, M., Gunnarsson, S., & Lequin, O. (1998). Iterative feedback tuning: theory and applications. IEEE control systems, 18(4), 26–41., starting from the observation that, in identification for control, each iteration is based on the (wrong) certainty equivalence principle.\n\nIFT is a model-free technique for the direct iterative optimization of the parameters of a fixed-order controller; such parameters can be successively updated using information coming from standard (closed-loop) system operation.\n\nLet  be a desired output to the reference signal ; the error between the achieved and desired response is . The control design objective can be formulated as the minimization of the objective function:\n\nGiven the objective function to minimize, the quasi-Newton method can be applied, i.e. a gradient-based minimization using a gradient search of the type:\n\nThe value  is the step size,  is an appropriate positive definite matrix and  is an approximation of the gradient; the true value of the gradient is given by the following:\n\n \n\nThe value of  is obtained through the following three-step methodology:\n\n Normal Experiment: Perform an experiment on the closed loop system with  as controller and  as reference; collect N measurements of the output , denoted as .\n Gradient Experiment: Perform an experiment on the closed loop system with  as controller and 0 as reference ; inject the signal  such that it is summed to the control variable output by , going as input into the plant. Collect the output, denoted as .\n Take the following as gradient approximation: .\n\nA crucial factor for the convergence speed of the algorithm is the choice of ; when  is small, a good choice is the approximation given by the Gauss–Newton direction:\n\n \n\n Noniterative correlation-based tuning \n\nNoniterative correlation-based tuning (nCbT) is a noniterative method for data-driven tuning of a fixed-structure controllervan Heusden, K., Karimi, A. and Bonvin, D. (2011), Data-driven model reference control with asymptotically guaranteed stability. Int. J. Adapt. Control Signal Process., 25: 331–351. doi:10.1002/acs.1212. It provides a one-shot method to directly synthesize a controller based on a single dataset.\n\nSuppose that  denotes an unknown LTI stable SISO plant,  a user-defined reference model and  a user-defined weighting function. An LTI fixed-order controller is indicated as , where , and  is a vector of LTI basis functions. Finally,  is an ideal LTI controller of any structure, guaranteeing a closed-loop function  when applied to .\n\nThe goal is to minimize the following objective function:\n\n \n\n is a convex approximation of the objective function obtained from a model reference problem, supposing that .\n\nWhen  is stable and minimum-phase, the approximated model reference problem is equivalent to the minimization of the norm of  in the scheme in figure.\n\nthumb|483x180px|The idea is that, when G is stable and minimum phase, the approximated model reference problem is equivalent to the minimization of the norm of .\n\nThe input signal  is supposed to be a persistently exciting input signal and  to be generated by a stable data-generation mechanism. The two signals are thus uncorrelated in an open-loop experiment; hence, the ideal error  is uncorrelated with . The control objective thus consists in finding  such that  and  are uncorrelated.\n\nThe vector of instrumental variables  is defined as:\n\n \n\nwhere  is large enough and , where  is an appropriate filter.\n\nThe correlation function is:\n\n \n\nand the optimization problem becomes:\n\nDenoting with  the spectrum of , it can be demonstrated that, under some assumptions, if  is selected as:\n\nthen, the following holds:\n\n Stability constraint \n\nThere is no guarantee that the controller  that minimizes  is stable. Instability may occur in the following cases:\n\n If  is non-minimum phase,  may lead to cancellations in the right-half complex plane.\n If  (even if stabilizing) is not achievable,  may not be stabilizing.\n Due to measurement noise, even if  is stabilizing, data-estimated  may not be so.\n\nConsider a stabilizing controller  and the closed loop transfer function .\nDefine:\n\nTheorem \nThe controller  stabilizes the plant  if\n\n  is stable\n  s.t. \n\nCondition 1. is enforced when:\n\n  is stable\n  contains an integrator (it is canceled).\n\nThe model reference design with stability constraint becomes:\n\nA convex data-driven estimation of  can be obtained through the discrete Fourier transform. \n\nDefine the following:\n\n \n\nFor stable minimum phase plants, the following convex data-driven oprimization problem is given:\n\n Virtual reference feedback tuning \n\nVirtual Reference Feedback Tuning (VRFT) is a noniterative method for data-driven tuning of a fixed-structure controller. It provides a one-shot method to directly synthesize a controller based on a single dataset.\n\nVRFT was first proposed in Campi, Marco C., Andrea Lecchini, and Sergio M. Savaresi. \"Virtual reference feedback tuning: a direct method for the design of feedback controllers.\" Automatica 38.8 (2002): 1337–1346. and then extended to LPV systems Formentin, S., Piga, D., Tóth, R., & Savaresi, S. M. (2016). Direct learning of LPV controllers from data. Automatica, 65, 98–110.. VRFT also builds on ideas given in Guardabassi, Guido O., and Sergio M. Savaresi. \"Approximate feedback linearization of discrete-time non-linear systems using virtual input direct design.\" Systems & Control Letters 32.2 (1997): 63–74. as . \n\nThe main idea is to define a desired closed loop model  and to use its inverse dynamics to obtain a virtual reference  from the measured output signal .\n\nthumb|483x180px|The main idea is to define a desired closed loop model M and to use its inverse dynamics to obtain a virtual reference from the measured output signal y.\n\nThe virtual signals are  and \n\nThe optimal controller is obtained from noiseless data by solving the following optimization problem:\n\n \n\nwhere the optimization function is given as follows:\n\n References \n\nCategory:Mathematics\nCategory:Robotics\nCategory:Systems\nCategory:Control theory\nCategory:Control engineering\nCategory:Computational mathematics"
    },
    {
      "title": "Discrete Logarithm Problem (DLP)",
      "url": "https://en.wikipedia.org/wiki/Discrete_Logarithm_Problem_%28DLP%29",
      "text": "REDIRECT Discrete_logarithm#Algorithms\nCategory:Mathematics\nCategory:Cryptography\nCategory:Computer science"
    },
    {
      "title": "Intersection",
      "url": "https://en.wikipedia.org/wiki/Intersection",
      "text": "thumb|right|The circle (black) intersects the line (purple) in two points (red). The disk (yellow) intersects the line in the line segment between the two red points.\n220px|thumb|The intersection (red) of two disks (white and red with black boundaries).\n220px|thumb|The intersection of D and E is shown in grayish purple. The intersection of A with any of B, C, D, or E is the empty set.\n\nIn mathematics, the intersection of two or more objects is another, usually \"smaller\" object. All objects are presumed to lie in a certain common space except in set theory, where the intersection of arbitrary sets is defined. The intersection is one of basic concepts of geometry. Intuitively, the intersection of two or more objects is a new object that lies in each of original objects. An intersection can have various geometric shapes, but a point is the most common in a plane geometry.\n\nDefinitions vary in different contexts: set theory formalizes the idea that a smaller object lies in a larger object with inclusion, and the intersection of sets is formed of elements that belong to all intersecting sets. It is always defined, but may be empty. Incidence geometry defines an intersection (usually, of flats) as an object of lower dimension that is incident to each of original objects. In this approach an intersection can be sometimes undefined, such as for parallel lines. In both cases the concept of intersection relies on logical conjunction.\n\nAlgebraic geometry defines intersections in its own way with intersection theory.\n\nEuclidean geometry deals with the intersections of planar and solid shapes.\n\nUniqueness\nThere can be more than one primitive object, such as points (pictured above), that form an intersection. The intersection can be viewed collectively as all of the shared objects (i.e., the intersection operation results in a set, possibly empty), or as several intersection objects (possibly zero).\n\n In set theory \n\nThe intersection of two sets A and B is the set of elements which are in both A and B.  In symbols,\n\n.\n\nFor example, if A = {1, 3, 5, 7} and B = {1, 2, 4, 6} then A ∩ B = {1}.  A more elaborate example (involving infinite sets) is:\n A = {x is an even integer}\n B = {x is an integer divisible by 3}\n \n\nAs another example, the number 9 is not contained in the intersection of the set of prime numbers {2, 3, 5, 7, 11, …} and the set of even numbers {2, 4, 6, 8, 10, …}, because 9 is neither prime nor even.\n\nIn Euclidean geometry\n\n Line–line intersection\n Line–plane intersection\n Line–sphere intersection\n Intersection of a polyhedron with a line\n Line segment intersection\n Intersection curve\n\nNotation\nIntersection is denoted by the  from Unicode Mathematical Operators. \n\nSee also\n Constructive solid geometry, Boolean Intersection is one of the ways of combining 2D/3D shapes\n Meet (lattice theory)\n\nReferences\n\nExternal links\n\nCategory:Mathematics\nCategory:Broad-concept articles"
    },
    {
      "title": "Peano kernel theorem",
      "url": "https://en.wikipedia.org/wiki/Peano_kernel_theorem",
      "text": "In numerical analysis, the Peano kernel theorem is a general result on error bounds for a wide class of numerical approximations (such as numerical quadratures), defined in terms of linear functionals. It is attributed to Giuseppe Peano.\n\n Statement of theorem \nLet   be the space of all differentiable functions  defined for  that are of bounded variation on , and let  be a linear functional on . Assume that  is  times continuously differentiable and that  annihilates all polynomials of degree , i.e.Suppose further that for any bivariate function  with , the following is valid:and define the Peano kernel of  asintroducing notationThe Peano kernel theorem then states that provided .\n\n Bounds \nSeveral bounds on the value of  follow from this result:\n\nwhere ,  and are the taxicab, Euclidean and maximum norms respectively.\n\n Application \nIn practice, the main application of the Peano kernel theorem is to bound the error of an approximation that is exact for all . The theorem above follows from the Taylor polynomial for  with integral remainder:\n\n \n\ndefining  as the error of the approximation, using the linearity of  together with exactness for  to annihilate all but the final term on the right-hand side, and using the  notation to remove the -dependence from the integral limits.\n\n See also \n\n Divided differences\n\n References \n\nCategory:Numerical analysis\nCategory:Mathematics"
    },
    {
      "title": "Permutation category",
      "url": "https://en.wikipedia.org/wiki/Permutation_category",
      "text": "In mathematics, the permutation category is a category where\n an object is a natural number,\n a morphism  is an element of the symmetric group  when  and is none otherwise.\n\nIt is equivalent as an category to the category of finite sets and bijections between them.\n\n References \n\nTodd Trimble, Notes on operads and the Lie operad\n\nCategory:Mathematics"
    },
    {
      "title": "Phase reduction",
      "url": "https://en.wikipedia.org/wiki/Phase_reduction",
      "text": "Phase reduction is a method used to reduce a multi-dimensional dynamical equation describing a nonlinear limit cycle oscillator into a one-dimensional phase equation. Many phenomena in our world such as chemical reactions, electric circuits, mechanical vibrations, cardiac cells, and spiking neurons are examples of rhythmic phenomena, and can be considered as nonlinear limit cycle oscillators.\n\nHistory\nThe theory of phase reduction method was first introduced in the 1950s, the existence of periodic solutions to nonlinear oscillators under perturbation, has been discussed by Malkin in, in the 1960s, Winfree illustrated the importance of the notion of phase and formulated the phase model for a population of nonlinear oscillators in his studies on biological synchronization. Since then, many researchers have discovered different rhythmic phenomena related to phase reduction theory.\n\n Phase model of reduction\nConsider the dynamical system of the form\n\nwhere  is the oscillator state variable,  is the baseline vector field. Let  be the flow induced by the system, that is,  is the solution of the system for the initial condition . This system of differential equations can describe for a neuron model for conductance with , where  represents the voltage difference across the membrane and  represents the -dimensional vector that defines gating variables.  When a neuron is perturbed by a stimulus current, the dynamics of the perturbed system will no longer be the same with the dynamics of the baseline neural oscillator. \n\nthumb|upright=2.1|Isochrons and a stable limit cycle of the planar system . The system has a unique stable limit cycle (solid circle). Only isochrons corresponding to phases , where  is the period of the orbit, are shown (dotted lines). Neighbouring trajectories (blue dotted curves) with different initial conditions are attracted to the cycle (except the origin).\n\nThe target here is to reduce the system by defining a phase for each point in some neighbourhood of the limit cycle. The allowance of sufficiently small perturbations (e.g. external forcing or stimulus effect to the system) might cause a large deviation of the phase, but the amplitude is perturbed slightly because of the attracting of the limit cycle. Hence we need to extend the definition of the phase to points in the neighborhood of the cycle by introducing the definition of asymptotic phase (or latent phase). This helps us to assign a phase to each point in the basin of attraction of a periodic orbit. The set of points in the basin of attraction of  that share the same asymptotic phase  is called the isochron (e.g. see Figure 1), which were first introduced by Winfree. Isochrons can be shown to exist for such a stable hyperbolic limit cycle .  So for all point  in some neighbourhood of the cycle, the evolution of the phase  can be given by the relation , where  is the natural frequency of the oscillation. By the chain rule we then obtain an equation that govern the evolution of the phase of the neuron model is given by the phase model:\n\nwhere  is the gradient of the phase function  with respect to the vector of the neuron's state vector , for the derivation of this result, see  This means that the -dimensional system describing the oscillating neuron dynamics is then reduced to a simple one-dimensional phase equation. One can notice that, it is impossible to retrieve the full information of the oscillator  from the phase  because \n is not one-to-one mapping.\n\nPhase model with external forcing\nConsider now a weakly perturbed system of the form\n\nwhere  is the baseline vector field,  is a weak periodic external forcing (or stimulus effect) of period , which can be different from  (in general), and frequency , which might depend on the oscillator state . Assuming that the baseline neural oscillator (that is, when )  has an exponentially stable limit cycle  with period  (example, see Figure 1)  that is normally hyperbolic,  it can be shown that  persists under small perturbations.  This implies that for a small perturbation, the perturbed system will remain close to the limit cycle. Hence we assume that such a limit cycle always exists for each neuron.\n\nThe evolution of the perturbed system in terms of the isochrons is \n\nwhere  is the gradient of the phase  with respect to the vector of the neuron's state vector , and  is the stimulus effect driving the firing of the neuron as a function of time . This phase equation is a partial differential equation (PDE).\n\nFor a sufficiently small , a reduced phase model evaluated on the limit cycle  of the unperturbed system can be given by, up to the first order of ,\n\nwhere function  measures the normalized phase shift due to a small perturbation delivered at any point  on the limit cycle , and is called the phase sensitivity function or infinitesimal phase response curve.  \n\nIn order to analyze the reduced phase equation corresponding to the perturbed nonlinear system, we need to solve a PDE, which is not a trivial one. So we need to simplify it into an autonomous phase equation for , which can more easily be analyzed.  Assuming that the frequencies  and  are sufficiently small so that\n, where  is , we can introduce a new phase function .\n\nBy the method of averaging, assuming that  does not vary within , we obtain an approximated phase equation\n\nwhere , and  is a -periodic function representing the effect of the periodic external forcing on the oscillator phase, defined by\n \nThe graph of this function  can be shown to exhibit the dynamics of the approximated phase model, for more illustrations see .\n\nExamples of phase reduction\nFor a sufficiently small perturbation of a certain nonlinear oscillator or a network of coupled oscillators, we can compute the corresponding phase sensitivity function or infinitesimal PRC .\n\n References \n\nCategory:Mathematics"
    },
    {
      "title": "Priority heuristic",
      "url": "https://en.wikipedia.org/wiki/Priority_heuristic",
      "text": "The priority heuristic is a simple, lexicographic decision strategy that correctly predicts classic violations of expected utility theory such as the Allais paradox, the four-fold pattern, the certainty effect, the possibility effect, or intransitivities.Brandstätter, E., Gigerenzer, G., & Hertwig, R. (2006). The priority heuristic: Making choices without trade-offs. Psychological Review, 113, 409–432. \n\nThe heuristic maps onto Rubinstein’s three-step-model, according to which people first check dominance and stop if it is present, otherwise they check for dissimilarity.Rubinstein, A. (1988). Similarity and decision making under risk (Is there a utility resolution to the Allais-paradox?). Journal of Economic Theory, 46, 145–153. To highlight Rubinstein’s model consider the following choice problem:\n\nI: 50% chance to win 2,000\n50% chance to win nothing\n\nII: 52% chance to win 1,000\n48% chance to win nothing\n\nDominance is absent, and while chances are similar monetary outcomes are not. Rubinstein’s model predicts that people check for dissimilarity and consequently choose Gamble I. Unfortunately, dissimilarity checks are often not decisive, and Rubinstein suggested that people, proceed to a third step that he left unspecified. The priority heuristic elaborates on Rubinstein’s framework by specifying this Step 3.\n\n Priority heuristic \nFor illustrative purposes consider a choice between two simple gambles of the type “a chance c of winning monetary amount x; a chance (100 - c) of winning amount y.” A choice between two such gambles contains four reasons for choosing: the maximum gain, the minimum gain, and their respective chances; because chances are complementary, three reasons remain: the minimum gain, the chance of the minimum gain, and the maximum gain.\n\nFor choices between gambles in which all outcomes are positive or 0, the priority heuristic consists of the following three steps (for all other choices see Brandstätter et al. 2006):\n\nPriority rule: Go through reasons in the order of minimum gain, chance of minimum gain, and maximum gain.\n\nStopping rule: Stop examination if the minimum gains differ by 1/10 (or more) of the maximum gain; otherwise, stop examination if chances differ by 10% (or more).\n\nDecision rule: Choose the gamble with the more attractive gain (chance). The term “attractive” refers to the gamble with the higher (minimum or maximum) gain and to the lower chance of the minimum gain.\n\n Examples \nConsider the following two choice problems, which were developed to support prospect theory, not the priority heuristic.Kahneman, D., & Tversky, A. (1979). Prospect theory: An analysis of decision under risk. Econometrica, 47, 263–291.\n\nProblem 1\nA: 80% chance to win 4,000\n20% chance to win nothing\nB: 100% chance to win 3,000\n\nMost people chose B (80%). The priority heuristic starts by comparing the minimum gains of the Gambles A (0) and B (3,000). The difference is 3,000, which is larger than 400 (10% of the maximum gain), examination is stopped; and the heuristic predicts that people prefer the sure gain B, which is in fact the majority choice.A\n\nProblem 2\nC: 45% chance to win 6,000\n55% chance to win nothing\nD: 90% chance to win 3,000 \n10% chance to win nothing\n\nMost people (86%) chose Gamble D. The priority heuristic starts by comparing the minimum gains (0 and 0). Because they do not differ, the probabilities (.45 and .90 or their logical complements .55 and .10) are compared. This difference is larger than 10%, examination stops and people are correctly predicted to choose D because of its higher probability of winning.\n\n Empirical support \nThe priority heuristic correctly predicted the majority choice in all (one-stage) gambles in Kahneman and Tversky (1979). Across four different data sets with a total of 260 problems, the heuristic predicted the majority choice better than (a) cumulative prospect theory, (b) two other modifications of expected utility theory, and (c) ten well-known heuristics (such as minimax or equal-weight) did. However, the priority heuristic has no free parameters, which triggered criticism,Birnbaum, M. H. (2008). Evaluation of the priority heuristic as a descriptive model of risky decision making: Comment on Brandstaätter, Gigerenzer, and Hertwig (2006). Psychological Review, 115, 253–262.Johnson, E. J., Schulte-Mecklenbeck, M., & Willemsen, M. C. (2008). Process models deserve process data: A comment on Brandstätter, Gigerenzer, and Hertwig (2006). Psychological Review, 115, 263–273.\nand countercriticism.Brandstätter, E., Gigerenzer, G., & Hertwig, R. (2008). Risky choice with heuristics: Reply to Birnbaum (2008), Johnson, Schulte-Mecklenbeck, and Willemsen (2008), and Rieger and Wang (2008). Psychological Review, 115, 281–289.Brandstätter, E., & Gussmack, M. (2013). The cognitive processes underlying risky choice. Journal of Behavioral Decision Making, 26, 185–197.Su, Y., Rao, L. L., Sun, H. Y., Du, X. L., Li, X., & Li, S. (2013). Is making a risky choice based on a weighting and adding process? An eye-tracking investigation. Journal of Experimental Psychology: Learning, Memory, and Cognition, 39, 1765–1780.\n\n Footnotes \nA For ease of exposure prominent numbers are neglected.\n\n References \n\n Weblinks \n http://library.mpib-berlin.mpg.de/ft/eb/EB_Priority_2006.pdf\n\nCategory:Mathematics\nCategory:Heuristics"
    },
    {
      "title": "Proof School",
      "url": "https://en.wikipedia.org/wiki/Proof_School",
      "text": "Proof School is a school in San Francisco for pupils who love mathematics. Their slogan is \"For kids who love math.\" The school opened in the fall of 2015 with 45 students in grades 6–10. Currently, 94.4 students in grades 6–12 are enrolled in Proof School for the academic year (2018–2019).\n\nAcademics\n\nProof School is a full-curriculum day school that emphasizes communication, collaboration, and problem solving. The school is accredited by Western Association of Schools and Colleges.\n\nThe school year is divided into 5 'blocks', each of which consist of 6 normal academic weeks and a build week.\n\nEach student has 5 courses: 4 'morning' courses that vary across grades, and a math class. The morning courses meet twice a week for 80 minutes per class. The math courses meet for two hours and ten minutes every day in the afternoon.  \n\nThe (non-post-calculus) math classes focus on a different subject each block: Block 1 is Combinatorics, Block 2 is Algebra, Block 3 is Geometry, Block 4 is Algebra and Pre-Calculus, and Block 5 is Number Theory.\n\n Teams and Clubs \nProof School currently has a number of internal clubs, as well as a Zero Robotics team called Proof Robotics. The team qualified for the competition finals, and is the leading member of the alliance Hit or Miss with the following teams: Crab Nebula from Liveo Cecioni in Livorno, Italy and Rock Rovers from Council Rock High School South in Holland, PA, USA. Hit or Miss placed 2nd place internationally, and performed one of the first satellite hookings aboard the ISS.\n\nReferences\n\nCategory:Private schools in California\nCategory:Schools in San Francisco\nCategory:Mathematics"
    },
    {
      "title": "Quota rule",
      "url": "https://en.wikipedia.org/wiki/Quota_rule",
      "text": "In mathematics and political science, the quota rule governs the way a state should apportion representative seats. It states that the number of seats that should be allocated to a given party should be no more than the upper or lower roundings (called upper and lower quotas) of the proportional representation;Michael J. Caulfield. \"Apportioning Representatives in the United States Congress - The Quota Rule\". MAA Publications. Retrieved October 22, 2018 e.g., if a party deserves 10.56 seats out of 15, the quota rule states that when the seats are allotted, the party may get 10 or 11 seats, but not lower or higher. The use of this rule is important when allocating seats for elected chambers such as the U.S. House of Representatives, which use proportional representation.\n\nMathematics\nIf  is the population of the party,  is the total population, and  is the number of available seats, then the natural quota for that party (the number of seats the party would ideally get) is\n\nThe lower quota is then the natural quota rounded down to the nearest integer while the upper quota is the natural quota rounded up. The quota rule states that the only two allocations that a party can receive should be either the lower or upper quota. If at any time an allocation gives a party a greater or lesser number of seats than the upper or lower quota, that allocation (and by extension, the method used to allocate it) is said to be in violation of the quota rule. Another way to state this is to say that a given method only satisfies the quota rule if each party's allocation differs from its natural quota by less than one, where each party's allocation is an integer value.Alan Stein. Apportionment Methods Retrieved December 9, 2018\n\nExample\nIf there are 5 available seats in the council of a club with 300 members, and party A has 106 members, then the natural quota for party A is . The lower quota for party A is 1, because 1.8 rounded down equal 1. The upper quota, 1.8 rounded up, is 2. Therefore, the quota rule states that the only two allocations allowed for party A are 1 or 2 seats on the council. If there is a second party, B, that has 137 members, then the quota rule states that party B gets , rounded up and down equals either 2 or 3 seats. Finally, a party C with the remaining 57 members of the club has a natural quota of , which means its allocated seats should be either 0 or 1. In all cases, the method for actually allocating the seats determines whether an allocation violates the quota rule, which in this case would mean giving party A any seats other than 1 or 2, giving party B any other than 2 or 3, or giving party C any other than 0 or 1 seat.\n\nRelation to other apportionment paradoxes\nThe Balinski–Young theorem proved in 1980 that if an apportionment method satisfies the quota rule, it must fail to satisfy some other apportionment paradox.Beth-Allyn Osikiewicz, Ph.D.  Impossibilities of Apportionment Retrieved October 23, 2018. For instance, although Hamilton's method satisfies the quota rule, it violates the Alabama paradox and the population paradox.Warren D. Smith. (2007).Apportionment and rounding schemes Retrieved October 23, 2018 The theorem itself is broken up into several different proofs that cover a wide number of circumstances.M.L. Balinski and H.P. Young. (1980). \"The Theory of Apportionment\". Retrieved October 23 2018\n\nSpecifically, there are two main statements that apply to the quota rule:\nAny method that follows the quota rule must fail the population paradox.\nAny method that is free of both the Alabama paradox and the population paradox must necessarily fail the quota rule for some circumstances.\n\nUse in apportionment methods\nDifferent methods for allocating seats may or may not satisfy the quota rule. While many methods do violate the quota rule, it is sometimes preferable to violate the rule very rarely than to violate some other apportionment paradox; some sophisticated methods violate the rule so rarely that it has not ever happened in a real apportionment, while some methods that never violate the quota rule violate other paradoxes in much more serious fashions.\n\nHamilton's method does satisfy the quota rule. The method works by proportioning seats equally until a fractional value is reached; the surplus seats are then given to the state with the largest fractional parts until there are no more surplus seats. Because it is impossible to give more than one surplus seat to a state, every state will always get either its lower or upper quota.Hilary Freeman. \"Apportionment\". Retrieved October 22 2018 \n\nJefferson's method, which was one of the first used by the United States,\"Apportionment 2\" Retrieved October 22, 2018. sometimes violated the quota rule by allocating more seats than the upper quota allowed. Jefferson’s Method Retrieved October 22, 2018. This violation led to a growing problem where larger states receive more representatives than smaller states, which was not corrected until Webster's method was implemented in 1842; even though Webster's method does violate the quota rule, it happens extremely rarely.Ghidewon Abay Asmerom. Apportionment. Lecture 4. Retrieved October 23, 2018.\n\nSee also\nApportionment in the European Parliament\nHighest averages method\nHuntington–Hill method\nMalapportionment\n\nReferences\n\nCategory:Mathematics\nCategory:Politics"
    },
    {
      "title": "Radial basis function interpolation",
      "url": "https://en.wikipedia.org/wiki/Radial_basis_function_interpolation",
      "text": "Radial basis function (RBF) interpolation is an advanced method in approximation theory for constructing high-order accurate interpolants of unstructured data, possibly in high-dimensional spaces. The interpolant takes the form of a weighted sum of radial basis functions. RBF interpolation is a mesh-free method, meaning the nodes (points in the domain) need not lie on a structured grid, and does not require the formation of a mesh. It is often spectrally accurate and stable for large numbers of nodes even in high dimensions.\n\nMany interpolation methods can be used as the theoretical foundation of algorithms for approximating linear operators, and RBF interpolation is no exception. RBF interpolation has been used to approximate differential operators, integral operators, and surface differential operators. These algorithms have been used to find highly accurate solutions of many differential equations including Navier–Stokes equations, Cahn–Hilliard equation, and the shallow water equations.\n\nExamples\nLet  and let  be 15 equally spaced points on the interval . We will form  where  is a radial basis function, and choose  such that  ( interpolates  at the chosen points). In matrix notation this can be written as\n \n\nChoosing , the Gaussian, with a shape parameter of , we can then solve the matrix equation for the weights and plot the interpolant. Plotting the interpolating function below, we see that it is visually the same everywhere except near the left boundary (an example of Runge's phenomenon), where it is still a very close approximation. More precisely the maximum error is roughly .\n\nframe|center|The function  sampled at  equally spaced nodes in the unit interval, and interpolated using the Gaussian RBF with a shape parameter of .\n\nMotivation\nThe Mairhuber–Curtis theorem says that for any vector space  with dimension higher than 2, and  linearly independent functions on , there exists a set of  points in the domain such that the interpolation matrix\n \nis not singular.\n\nThis means that if one wishes to have a general interpolation algorithm, one must choose the basis functions to depend on the interpolation points. In 1971, Rolland Hardy developed a method of interpolating scattered data using interpolants of the form . This is interpolation using a basis of shifted multiquadric functions, now more commonly written as , and is the first instance of radial basis function interpolation. It has been shown that the resulting interpolation matrix will always be non-singular. This does not violate the Mairhuber–Curtis theorem since the basis functions depend on the points of interpolation. Choosing a radial kernel such that the interpolation matrix is non-singular is exactly the definition of a radial basis function. It has been shown that any function that is completely monotone will have this property, including the Gaussian, inverse quadratic, and inverse multiquadric functions.\n\n Shape-parameter tuning \nMany radial basis functions have a parameter that controls their relative flatness or peakedness. This parameter is usually represented by the symbol  with the function becoming increasingly flat as . For example, Rolland Hardy used the formula  for the multiquadric, however nowadays the formula  is used instead. These formulas are equivalent up to a scale factor. This factor is inconsequential since the basis vectors have the same span and the interpolation weights will compensate. By convention, the basis function is scaled such that  as seen in the plots of the Gaussian functions and the Bump functions.\n\nthumb|An RBF interpolant of the function f(x)=e^(x*cos(3*pi*x))-1 sampled at 15 points, using Gaussians, with a very large shape parameter e=100. The \"bed-of-nails interpolant.\"\nA consequence of this choice, is that the interpolation matrix approaches the identity matrix as  leading to stability when solving the matrix system. The resulting interpolant will in general be a poor approximation to the function since it will be near zero everywhere, except near the interpolation points where it will sharply peak − the so-called \"bed-of-nails interpolant\" (as seen in the plot to the right).\n\nthumb|A plot of the condition number by the shape parameter for a 15x15 radial basis function interpolation matrix using the Gaussian.\nOn the opposite side of the spectrum, the condition number of the interpolation matrix will diverge to infinity as  leading to ill-conditioning of the system. In practice, one chooses a shape parameter so that the interpolation matrix is \"on the edge of ill-conditioning\" (eg. with a condition number of roughly  for double-precision floating point).\n\nThere are sometimes other factors to consider when choosing a shape-parameter. For example the bump function\n\nhas a compact support (it is zero everywhere except when ) leading to a sparse interpolation matrix.\n\nSome radial basis functions such as the polyharmonic splines have no shape-parameter.\n\n References \n\nCategory:Mathematics\nCategory:Physics"
    },
    {
      "title": "Areas of mathematics",
      "url": "https://en.wikipedia.org/wiki/Areas_of_mathematics",
      "text": "Mathematics encompasses a growing variety and depth of subjects over history, and comprehension requires a system to categorize and organize the many subjects into more general areas of mathematics. A number of different classification schemes have arisen, and though they share some similarities, there are differences due in part to the different purposes they serve. In addition, as mathematics continues to be developed, these classification schemes must change as well to account for newly created areas or newly discovered links between different areas. Classification is made more difficult by some subjects, often the most active, which straddle the boundary between different areas.\n\nA traditional division of mathematics is into pure mathematics, mathematics studied for its intrinsic interest, and applied mathematics, mathematics which can be directly applied to real world problems.For example the Encyclopædia Britannica Eleventh Edition groups its mathematics articles as Pure, Applied, and Biographies.\nThis division is not always clear and many subjects have been developed as pure mathematics to find unexpected applications later on. Broad divisions, such as discrete mathematics and computational mathematics, have emerged more recently.\n\nAn ideal system of classification permits adding new areas into the organization of previous knowledge, and fitting surprising discoveries and unexpected interactions into the outline.\nFor example, the Langlands program has found unexpected connections between areas previously thought unconnected, at least Galois groups, Riemann surfaces and number theory.\n\nClassification systems\nThe Mathematics Subject Classification (MSC) is produced by the staff of the review databases Mathematical Reviews and Zentralblatt MATH. Many mathematics journals ask authors to label their papers with MSC subject codes. The MSC divides mathematics into over 60 areas, with further subdivisions within each area.\nIn the Library of Congress Classification, mathematics is assigned the subclass QA within the class Q (Science). The LCC defines broad divisions, and individual subjects are assigned specific numerical values.\nThe Dewey Decimal Classification assigns mathematics to division 510, with subdivisions for Algebra & number theory, Arithmetic, Topology, Analysis, Geometry, Numerical analysis, and Probabilities & applied mathematics.\nThe Categories within Mathematics list is used by the Arxiv for categorizing preprints. It differs from MSC; for example, it includes things like quantum algebra.\nThe IMU uses its programme structure for organizing the lectures at its four-yearly ICM. One of its top-level sections that MSC doesn't have is Lie theory.\nThe ACM Computing Classification System includes a couple of mathematical categories F. Theory of Computation and G. Mathematics of Computing.\nMathOverflow has a tag system.\nMathematics book publishers such as Springer (subdisciplines), Cambridge (Browse Mathematics and statistics) and the AMS (subject area) use their own subject lists on their websites to enable customers to browse books or filter searches by subdiscipline, including topics such as mathematical biology and mathematical finance as top-level headings.\nSchools and other educational bodies have syllabuses.\nResearch institutes and university mathematics departments often have sub-departments or study groups. e.g. SIAM has activity groups for its members.\nWikipedia uses a :Category: Mathematics system on its articles, and also has a list of mathematics lists.\n\nMajor divisions of mathematics\n\nPure mathematics\n\nFoundations\nFoundations, including set theory and mathematical logic\n Mathematicians have always worked with logic and symbols, but for centuries the underlying laws of logic were taken for granted, and never expressed symbolically. Mathematical logic, also known as symbolic logic, was developed when people finally realized that the tools of mathematics can be used to study the structure of logic itself. Areas of research in this field have expanded rapidly, and are usually subdivided into several distinct departments.\nProof theory and constructive mathematics\n Proof theory grew out of David Hilbert's ambitious program to formalize all the proofs in mathematics. The most famous result in the field is encapsulated in Gödel's incompleteness theorems. A closely related and now quite popular concept is the idea of Turing machines. Constructivism is the outgrowth of Brouwer's unorthodox view of the nature of logic itself; constructively speaking, mathematicians cannot assert \"Either a circle is round, or it is not\" until they have actually exhibited a circle and measured its roundness.\nModel theory\n Model theory studies mathematical structures in a general framework. Its main tool is first-order logic.\nSet theory\n A set can be thought of as a collection of distinct things united by some common feature. Set theory is subdivided into three main areas. Naive set theory is the original set theory developed by mathematicians at the end of the 19th century. Axiomatic set theory is a rigorous axiomatic theory developed in response to the discovery of serious flaws (such as Russell's paradox) in naive set theory.  It treats sets as \"whatever satisfies the axioms\", and the notion of collections of things serves only as motivation for the axioms. Internal set theory is an axiomatic extension of set theory that supports a logically consistent identification of illimited (enormously large) and infinitesimal (unimaginably small) elements within the real numbers. See also List of set theory topics.\nHistory and biography\n The history of mathematics is inextricably intertwined with the subject itself. This is perfectly natural: mathematics has an internal organic structure, deriving new theorems from those that have come before. As each new generation of mathematicians builds upon the achievements of our ancestors, the subject itself expands and grows new layers, like an onion.\nRecreational mathematics\n From magic squares to the Mandelbrot set, numbers have been a source of amusement and delight for millions of people throughout the ages. Many important branches of \"serious\" mathematics have their roots in what was once a mere puzzle and/or game.\n\nNumber Theory\nNumber theory is the study of numbers and the properties of operations between them. Number theory is traditionally concerned with the properties of integers, but more recently, it has come to be concerned with wider classes of problems that have arisen naturally from the study of integers. \n Arithmetic  \n An elementary part of number theory that primarily focuses upon the study of natural numbers, integers, fractions, and decimals, as well as the properties of the traditional operations on them: addition, subtraction, multiplication and division. Up until the 19th century, arithmetic and number theory were synonyms, but the evolution and growth of the field has resulted in arithmetic referring only to the elementary branch of number theory. \n Elementary number theory\n The study of integers at a higher level than arithmetic, where the term 'elementary' here refers to the fact that no techniques from other mathematical fields are used.\n Analytic number theory \n Calculus and complex analysis are used as tools to study the integers.\n Algebraic number theory\n The study of algebraic numbers, the roots of polynomials with integer coefficients.\n Other number theory subfields\n Geometric number theory; combinatorial number theory; transcendental number theory; and computational number theory. See also the list of number theory topics\n\nAlgebra\nThe study of structure begins with numbers, first the familiar natural numbers and integers and their arithmetical operations, which are recorded in elementary algebra. The deeper properties of these numbers are studied in number theory. The investigation of methods to solve equations leads to the field of abstract algebra, which, among other things, studies rings and fields, structures that generalize the properties possessed by everyday numbers. Long standing questions about compass and straightedge construction were finally settled by Galois theory. The physically important concept of vectors, generalized to vector spaces, is studied in linear algebra.\n\n Order theory\n For any two distinct  real numbers, one must be greater than the other. Order Theory extends this idea to sets in general. It includes notions like lattices and ordered algebraic structures. See also the order theory glossary and the list of order topics.\n General algebraic systems\n Given a set, different ways of combining or relating members of that set can be defined. If these obey certain rules, then a particular algebraic structure is formed. Universal algebra is the more formal study of these structures and systems.\n Field theory and polynomials\n Field theory studies the properties of fields. A field is a mathematical entity for which addition, subtraction, multiplication and division are well-defined.  A polynomial is an expression in which constants and variables are combined using only addition, subtraction, and multiplication.\n Commutative rings and algebras\n In ring theory, a branch of abstract algebra, a commutative ring is a ring in which the multiplication operation obeys the commutative law. This means that if a and b are any elements of the ring, then a×b=b×a.  Commutative algebra is the field of study of commutative rings and their ideals, modules and algebras.  It is foundational both for algebraic geometry and for algebraic number theory. The most prominent examples of commutative rings are rings of polynomials.\n\nCombinatorics\nCombinatorics is the study of finite or discrete collections of objects that satisfy specified criteria. In particular, it is concerned with \"counting\" the objects in those collections (enumerative combinatorics) and with deciding whether certain \"optimal\" objects exist (extremal combinatorics). It includes graph theory, used to describe inter-connected objects (a graph in this sense is a network, or collection of connected points). See also the list of combinatorics topics, list of graph theory topics and glossary of graph theory.  A combinatorial flavour is present in many parts of problem-solving.\n\nGeometry and topology\nGeometry deals with spatial relationships, using fundamental qualities or axioms. Such axioms can be used in conjunction with mathematical definitions for points, straight lines, curves, surfaces, and solids to draw logical conclusions.  See also List of geometry topics\n\nConvex geometry and discrete geometry\n Includes the study of objects such as polytopes and polyhedra. See also List of convexity topics\nDiscrete or combinatorial geometry\n The study of geometrical objects and properties that are discrete or combinatorial, either by their nature or by their representation. It includes the study of shapes such as the Platonic solids and the notion of tessellation.\nDifferential geometry\n The study of geometry using calculus. It is very closely related to differential topology. Covers such areas as Riemannian geometry, curvature and differential geometry of curves. See also the glossary of differential geometry and topology.\nAlgebraic geometry\n Given a polynomial of two real variables, then the points on a plane where that function is zero will form a curve. An algebraic curve extends this notion to polynomials over a field in a given number of variables. Algebraic geometry may be viewed as the study of these curves. See also the list of algebraic geometry topics and list of algebraic surfaces.\nArithmetic geometry\n The study of schemes of finite type over the spectrum of the ring of integers. Alternatively defined as the application of the techniques of algebraic geometry to problems in number theory.\nDiophantine geometry\n The study of the points of algebraic varieties with coordinates in fields that are not algebraically closed and occur in algebraic number theory, such as the field of rational numbers, number fields, finite fields, function fields, and p-adic fields, but not including the real numbers.\nReal algebraic geometry\n The study of semialgebraic sets, i.e. real-number solutions to algebraic inequalities with-real number coefficients, and mappings between them.\nTopology\n Deals with the properties of a figure that do not change when the figure is continuously deformed. The main areas are point set topology (or general topology), algebraic topology, and the topology of manifolds, defined below.\nGeneral topology\n Also called point set topology. Properties of topological spaces. Includes such notions as open and closed sets, compact spaces, continuous functions, convergence, separation axioms, metric spaces, dimension theory.  See also the glossary of general topology and the list of general topology topics.\nAlgebraic topology\n Properties of algebraic objects associated with a topological space and how these algebraic objects capture properties of such spaces.  Contains areas like homology theory, cohomology theory, homotopy theory, and homological algebra, some of them examples of functors.  Homotopy deals with homotopy groups (including the fundamental group) as well as simplicial complexes and CW complexes (also called cell complexes).  See also the list of algebraic topology topics.\nDifferential topology\n The field dealing with differentiable functions on differentiable manifolds, which can be thought of as an n-dimensional generalization of a surface in the usual 3-dimensional Euclidean space.\n\nAnalysis \nWithin the world of mathematics, analysis is the branch that focuses on change: rates of change, accumulated change, and multiple things changing relative to (or independently of) one another.\n\nModern analysis is a vast and rapidly expanding branch of mathematics that touches almost every other subdivision of the discipline, finding direct and indirect applications in topics as diverse as number theory, cryptography, and abstract algebra. It is also the language of science itself and is used across chemistry, biology, and physics, from astrophysics to X-ray crystallography.\n\nApplied mathematics\n\nProbability and statistics\n\nProbability theory: The mathematical theory of random phenomena. Probability theory studies random variables and events, which are mathematical abstractions of non-deterministic events or measured quantities. See also :Category:probability theory, and the list of probability topics.\nStochastic processes: An extension of probability theory that studies collections of random variables, such as time series or spatial processes. See also List of stochastic processes topics, and :Category:Stochastic processes.\nStatistics: The science of making effective use of numerical data from experiments or from populations of individuals. Statistics includes not only the collection, analysis and interpretation of such data, but also the planning of the collection of data, in terms of the design of surveys and experiments. See also the list of statistical topics.\n\nComputational sciences\nNumerical analysis\n Many problems in mathematics cannot in general be solved exactly. Numerical analysis is the study of iterative methods and algorithms for approximately solving problems to a specified error bound. Includes numerical differentiation, numerical integration and numerical methods; c.f. scientific computing. See also List of numerical analysis topics\nComputer algebra\n This area is also called symbolic computation or algebraic computation. It deals with exact computation, for example with integers of arbitrary size, polynomials or elements of finite fields. It includes also the computation with non numeric mathematical objects like polynomial ideals or series.\n\nPhysical sciences\n Mechanics\n Addresses what happens when a real physical object is subjected to forces. This divides naturally into the study of rigid solids, deformable solids, and fluids, detailed below.\n \n Mechanics of structures\n Mechanics of structures is a field of study within applied mechanics that investigates the behavior of structures under mechanical loads, such as bending of a beam, buckling of a column, torsion of a shaft, deflection of a thin shell, and vibration of a bridge.\n \n Mechanics of deformable solids\n Most real-world objects are not point-like nor perfectly rigid. More importantly, objects change shape when subjected to forces. This subject has a very strong overlap with continuum mechanics, which is concerned with continuous matter. It deals with such notions as stress, strain and elasticity.\n \n Fluid mechanics\n Fluids in this sense includes not just liquids, but flowing gases, and even solids under certain situations. (For example, dry sand can behave like a fluid). It includes such notions as viscosity, turbulent flow and laminar flow (its opposite).\n \n Particle mechanics\n In mathematics, a particle is a point-like, perfectly rigid, solid object. Particle mechanics deals with the results of subjecting particles to forces. It includes celestial mechanics—the study of the motion of celestial objects.\n\nOther applied mathematics\nOperations research (OR), also known as operational research, provides optimal or near-optimal solutions to complex problems. OR uses mathematical modeling, statistical analysis, and mathematical optimization.\nMathematical programming (or mathematical optimization) minimizes (or maximizes) a real-valued function over a domain that is often specified by constraints on the variables. Mathematical programming studies these problems and develops iterative methods and algorithms for their solution.\n\n See also \n \nMathematics Subject Classification\nGlossary of areas of mathematics\nOutline of mathematics\n\nNotes\n\nExternal links\nThe Divisions of Mathematics [from the Web Archive; Last modified 2006/01/25]\n\n "
    },
    {
      "title": "Glossary of areas of mathematics",
      "url": "https://en.wikipedia.org/wiki/Glossary_of_areas_of_mathematics",
      "text": "This is a glossary of terms that are or have been considered areas of study in mathematics.\n\nA\n\n Absolute differential calculus: the original name for tensor calculus developed around 1890.\n Absolute geometry: an extension of ordered geometry that is sometimes referred to as neutral geometry because its axiom system is neutral to the parallel postulate.\n Abstract algebra: the study of algebraic structures and their properties. Originally it was known as modern algebra.\n Abstract analytic number theory: a branch of mathematics that takes ideas from classical analytic number theory and applies them to various other areas of mathematics.\n Abstract differential geometry: a form of differential geometry without the notion of smoothness from calculus. Instead it is built using sheaf theory and sheaf cohomology.\n Abstract harmonic analysis: a modern branch of harmonic analysis that extends upon the generalized Fourier transforms that can be defined on locally compact groups.\n Abstract homotopy theory: a part of topology that deals with homotopic functions, i.e. functions from one topological space to another which are homotopic (the functions can be deformed into one another).\n Additive combinatorics: the part of arithmetic combinatorics devoted to the operations of addition and subtraction.\n Additive number theory: a part of number theory that studies subsets of integers and their behaviour under addition.\n Affine geometry: a branch of geometry that is centered on the study of geometric properties that remain unchanged by affine transformations. It can be described as a generalization of Euclidean geometry.\n Affine geometry of curves: the study of curves in affine space.\n Affine differential geometry: a type of differential geometry dedicated to differential invariants under volume-preserving affine transformations.\n Ahlfors theory: a part of complex analysis being the geometric counterpart of Nevanlinna theory. It was invented by Lars Ahlfors\n Algebra: a major part of pure mathematics centered on operations and relations. Beginning with elementary algebra, it introduces the concept of variables and how these can be manipulated towards problem solving; known as equation solving. Generalizations of operations and relations defined on sets have led to the idea of an algebraic structure which are studied in abstract algebra. Other branches of algebra include universal algebra, linear algebra and multilinear algebra.\n Algebraic analysis: motivated by systems of linear partial differential equations, it is a branch of algebraic geometry and algebraic topology that uses methods from sheaf theory and complex analysis, to study the properties and generalizations of functions. It was started by Mikio Sato.\n Algebraic combinatorics: an area that employs methods of abstract algebra to problems of combinatorics. It also refers to the application of methods from combinatorics to problems in abstract algebra.\n Algebraic computation: see symbolic computation.\n Algebraic geometry: a branch that combines techniques from abstract algebra with the language and problems of geometry. Fundamentally, it studies algebraic varieties.\n Algebraic graph theory: a branch of graph theory in which methods are taken from algebra and employed to problems about graphs. The methods are commonly taken from group theory and linear algebra.\n Algebraic K-theory: an important part of homological algebra concerned with defining and applying a certain sequence of functors from rings to abelian groups.\n Algebraic number theory: a part of algebraic geometry devoted to the study of the points of the algebraic varieties whose coordinates belong to an algebraic number field. It is a major branch of number theory and is also said to study algebraic structures related to algebraic integers.\n Algebraic statistics: the use of algebra to advance statistics, although the term is sometimes restricted to label the use of algebraic geometry and commutative algebra in statistics.\n Algebraic topology: a branch that uses tools from abstract algebra for topology to study topological spaces.\n Algorithmic number theory: also known as computational number theory, it is the study of algorithms for performing number theoretic computations.\n Anabelian geometry: an area of study based on the theory proposed by Alexander Grothendieck in the 1980s that describes the way a geometric object of an algebraic variety (such as an algebraic fundamental group) can be mapped into another object, without it being an abelian group.\n Analysis: a rigorous branch of pure mathematics that had its beginnings in the formulation of infinitesimal calculus. (Then it was known as infinitesimal analysis.) The classical forms of analysis are real analysis and its extension complex analysis, whilst more modern forms are those such as functional analysis.\n Analytic combinatorics: part of enumerative combinatorics where methods of complex analysis are applied to generating functions.\n Analytic geometry: usually this refer to the study of geometry using a coordinate system (also known as Cartesian geometry). Alternatively it can refer to the geometry of analytic varieties. In this respect it is essentially equivalent to real and complex algebraic geometry.\n Analytic number theory: part of number theory using methods of analysis (as opposed to algebraic number theory)\n Applied mathematics: a combination of various parts of mathematics that concern a variety of mathematical methods that can be applied to practical and theoretical problems. Typically the methods used are for science, engineering, finance, economics and logistics.\n Approximation theory: part of analysis that studies how well functions can be approximated by simpler ones (such as polynomials or trigonometric polynomials)\n Arakelov geometry: also known as Arakelov theory\n Arakelov theory: an approach to Diophantine geometry used to study Diophantine equations in higher dimensions (using techniques from algebraic geometry). It is named after Suren Arakelov.\n Arithmetic: to most people this refers to the branch known as elementary arithmetic dedicated to the usage of addition, subtraction, multiplication and division. However arithmetic also includes higher arithmetic referring to advanced results from number theory.\n Arithmetic algebraic geometry: see arithmetic geometry\n Arithmetic combinatorics: the study of the estimates from combinatorics that are associated with arithmetic operations such as addition, subtraction, multiplication and division.\n Arithmetic dynamics:Arithmetic dynamics is the study of the number-theoretic properties of integer, rational, -adic, and/or algebraic points under repeated application of a polynomial or rational function. A fundamental goal is to describe arithmetic properties in terms of underlying geometric structures.\n Arithmetic geometry: the study of schemes of finite type over the spectrum of the ring of integers\n Arithmetic topology: a combination of algebraic number theory and topology studying analogies between prime ideals and knots\n Arithmetical algebraic geometry: an alternative name for arithmetic algebraic geometry\n Asymptotic combinatorics:It uses the internal structure of the objects to derive formulas for their generating functions and then complex analysis techniques to get asymptotics.\n Asymptotic geometric analysis\n Asymptotic theory: the study of asymptotic expansions\n Auslander–Reiten theory: the study of the representation theory of Artinian rings\n Axiomatic geometry: also known as synthetic geometry: it is a branch of geometry that uses axioms and logical arguments to draw conclusions as opposed to analytic and algebraic methods.\n Axiomatic homology theory\n Axiomatic set theory: the study of systems of axioms in a context relevant to set theory and mathematical logic.\n\nB\n\n Bifurcation theory: the study of changes in the qualitative or topological structure of a given family. It is a part of dynamical systems theory\n Birational geometry: a part of algebraic geometry that deals with the geometry (of an algebraic variety) that is dependent only on its function field.\n Bolyai–Lobachevskian geometry: see hyperbolic geometry.\n Bivariate data: a data comparison that deals with two independent variables.\n\nC\n\n C*-algebra theory: a complex algebra A of continuous linear operators on a complex Hilbert space with two additional properties-(i) A is a topologically closed set in the norm topology of operators.(ii)A is closed under the operation of taking adjoints of operators.\n Cartesian geometry: see analytic geometry\n Calculus: a branch usually associated with limits, functions, derivatives, integrals and infinite series. It forms the basis of classical analysis, and historically was called the calculus of infinitesimals or infinitesimal calculus. Now it can refer to a system of calculation guided by symbolic manipulation.\n Calculus of infinitesimals: also known as infinitesimal calculus. It is a branch of calculus built upon the concepts of infinitesimals.\n Calculus of moving surfaces: an extension of the theory of tensor calculus to include deforming manifolds.\n Calculus of variations: the field dedicated to maximizing or minimizing functionals. It used to be called functional calculus.\n Catastrophe theory: a branch of bifurcation theory from dynamical systems theory, and also a special case of the more general singularity theory from geometry. It analyses the germs of the catastrophe geometries.\n Categorical logic: a branch of category theory adjacent to the mathematical logic. It is based on type theory for intuitionistic logics.\n Category theory: the study of the properties of particular mathematical concepts by formalising them as collections of objects and arrows.\n Chaos theory: the study of the behaviour of dynamical systems that are highly sensitive to their initial conditions.\n Character theory: a branch of group theory that studies the characters of group representations or modular representations.\n Class field theory: a branch of algebraic number theory that studies abelian extensions of number fields.\n Classical differential geometry: also known as Euclidean differential geometry. see Euclidean differential geometry.\n Classical algebraic topology\n Classical analysis: usually refers to the more traditional topics of analysis such as real analysis and complex analysis. It includes any work that does not use techniques from functional analysis and is sometimes called hard analysis. However it may also refer to mathematical analysis done according to the principles of classical mathematics.\n Classical analytic number theory\n Classical differential calculus\n Classical Diophantine geometry\n Classical Euclidean geometry: see Euclidean geometry\n Classical geometry: may refer to solid geometry or classical Euclidean geometry. See geometry\n Classical invariant theory: the form of invariant theory that deals with describing polynomial functions that are invariant under transformations from a given linear group.\n Classical mathematics: the standard approach to mathematics based on classical logic and ZFC set theory.\n Classical projective geometry\n Classical tensor calculus\n Clifford analysis: the study of Dirac operators and Dirac type operators from geometry and analysis using clifford algebras.\n Clifford theory is a branch of representation theory spawned from Cliffords theorem.\n Cobordism theory\n Cohomology theory\n Combinatorial analysis\n Combinatorial commutative algebra: a discipline viewed as the intersection between commutative algebra and combinatorics. It frequently employs methods from one to address problems arising in the other. Polyhedral geometry also plays a significant role.\n Combinatorial design theory: a part of combinatorial mathematics that deals with the existence and construction of systems of finite sets whose intersections have certain properties.\n Combinatorial game theory\n Combinatorial geometry: see discrete geometry\n Combinatorial group theory: the theory of free groups and the presentation of a group. It is closely related to geometric group theory and is applied in geometric topology.\n Combinatorial mathematics\n Combinatorial number theory\n Combinatorial set theory: also known as Infinitary combinatorics. see infinitary combinatorics\n Combinatorial theory\n Combinatorial topology: an old name for algebraic topology, when topological invariants of spaces were regarded as derived from combinatorial decompositions.\n Combinatorics: a branch of discrete mathematics concerned with countable structures. Branches of it include enumerative combinatorics, combinatorial design theory, matroid theory, extremal combinatorics and algebraic combinatorics, as well as many more.\n Commutative algebra: a branch of abstract algebra studying commutative rings.\n Complex algebra\n Complex algebraic geometry:  the mainstream of algebraic geometry devoted to the study of the complex points of algebraic varieties.\n Complex analysis: a part of analysis that deals with functions of a complex variable.\n Complex analytic dynamics: a subdivision of complex dynamics being the study of the dynamic systems defined by analytic functions.\n Complex analytic geometry: the application of complex numbers to plane geometry.\n Complex differential geometry: a branch of differential geometry that studies complex manifolds.\n Complex dynamics: the study of dynamical systems defined by iterated functions on complex number spaces.\n Complex geometry: the study of complex manifolds and functions of complex variables. It includes complex algebraic geometry and complex analytic geometry.\n Complexity theory: the study of complex systems with the inclusion of the theory of complex systems.\n Computable analysis: the study of which parts of real analysis and functional analysis can be carried out in a computable manner. It is closely related to constructive analysis.\n Computable model theory: a branch of model theory dealing with the relevant questions computability.\n Computability theory: a branch of mathematical logic originating in the 1930s with the study of computable functions and Turing degrees, but now includes the study of generalized computability and definability. It overlaps with proof theory and effective descriptive set theory.\n Computational algebraic geometry\n Computational complexity theory: a branch of mathematics and theoretical computer science that focuses on classifying computational problems according to their inherent difficulty, and relating those classes to each other.\n Computational geometry\n Computational group theory: the study of groups by means of computers.\n Computational mathematics: the mathematical research in areas of science where computing plays an essential role.\n Computational number theory: also known as algorithmic number theory, it is the study of algorithms for performing number theoretic computations.\n Computational real algebraic geometry\n Computational synthetic geometry\n Computational topology\n Computer algebra: see symbolic computation\n Conformal geometry: the study of conformal transformations on a space.\n Constructive analysis: mathematical analysis done according to the principles of constructive mathematics. This differs from classical analysis.\n Constructive function theory: a branch of analysis that is closely related to approximation theory, studying the connection between the smoothness of a function and its degree of approximation\n Constructive mathematics: mathematics which tends to use intuitionistic logic. Essentially that is classical logic but without the assumption that the law of the excluded middle is an axiom.\n Constructive quantum field theory: a branch of mathematical physics that is devoted to showing that quantum theory is mathematically compatible with special relativity.\n Constructive set theory\n Contact geometry: a branch of differential geometry and topology, closely related to and considered the odd-dimensional counterpart of symplectic geometry. It is the study of a geometric structure called a contact structure on a differentiable manifold.\n Convex analysis: the study of properties of convex functions and convex sets.\n Convex geometry: part of geometry devoted to the study of convex sets.\n Coordinate geometry: see analytic geometry\n CR geometry: a branch of differential geometry, being the study of CR manifolds.\n\nD\n\n Derived noncommutative algebraic geometry\n Descriptive set theory: a part of mathematical logic, more specifically a part of set theory dedicated to the study of Polish spaces.\n Differential algebraic geometry: the adaption of methods and concepts from algebraic geometry to systems of algebraic differential equations.\n Differential calculus: a subfield of calculus concerned with derivatives or the rates that quantities change. It is one of two traditional divisions of calculus, the other being integral calculus.\n Differential Galois theory: the study of the Galois groups of differential fields.\n Differential geometry: a form of geometry that uses techniques from integral and differential calculus as well as linear and multilinear algebra to study problems in geometry. Classically, these were problems of Euclidean geometry, although now it has been expanded. It is generally concerned with geometric structures on differentiable manifolds. It is closely related to differential topology.\n Differential geometry of curves: the study of smooth curves in Euclidean space by using techniques from differential geometry\n Differential geometry of surfaces: the study of smooth surfaces with various additional structures using the techniques of differential geometry.\n Differential topology: a branch of topology that deals with differentiable functions on differentiable manifolds.\n Diffiety theory\n Diophantine geometry: in general the study of algebraic varieties over fields that are finitely generated over their prime fields.\n Discrepancy theory\n Discrete computational geometry\n Discrete differential geometry\n Discrete dynamics\n Discrete exterior calculus\n Discrete geometry\n Discrete mathematics\n Discrete Morse theory: a combinatorial adaption of Morse theory.\n Distance geometry\n Domain theory\n Donaldson theory: the study of smooth 4-manifolds using gauge theory.\n Dynamical systems theory\n\nE\n\n Econometrics: the application of mathematical and statistical methods to economic data.\n Effective descriptive set theory: a branch of descriptive set theory dealing with set of real numbers that have lightface definitions. It uses aspects of computability theory.\n Elementary algebra: a fundamental form of algebra extending on elementary arithmetic to include the concept of variables.\n Elementary arithmetic: the simplified portion of arithmetic considered necessary for primary education. It includes the usage addition, subtraction, multiplication and division of the natural numbers. It also includes the concept of fractions and negative numbers.\n Elementary mathematics: parts of mathematics frequently taught at the primary and secondary school levels. This includes elementary arithmetic, geometry, probability and statistics, elementary algebra and trigonometry. (calculus is not usually considered a part)\n Elementary group theory: the study of the basics of group theory\n Elimination theory: the classical name for algorithmic approaches to eliminating between polynomials of several variables. It is a part of commutative algebra and algebraic geometry.\n Elliptic geometry: a type of non-Euclidean geometry (it violates Euclid's parallel postulate) and is based on spherical geometry. It is constructed in elliptic space.\n Enumerative combinatorics: an area of combinatorics that deals with the number of ways that certain patterns can be formed.\n Enumerative geometry: a branch of algebraic geometry concerned with counting the number of solutions to geometric questions. This is usually done by means of intersection theory.\n Equivariant noncommutative algebraic geometry\n Ergodic Ramsey theory: a branch where problems are motivated by additive combinatorics and solved using ergodic theory.\n Ergodic theory: the study of dynamical systems with an invariant measure, and related problems.\n Euclidean geometry\n Euclidean differential geometry: also known as classical differential geometry. See differential geometry.\n Euler calculus\n Experimental mathematics\n Extraordinary cohomology theory\n Extremal combinatorics: a branch of combinatorics, it is the study of the possible sizes of a collection of finite objects given certain restrictions.\n Extremal graph theory\n\nF\n\n Field theory: branch of abstract algebra studying fields.\n Finite geometry\n Finite model theory\n Finsler geometry: a branch of differential geometry whose main object of study is the Finsler manifold (a generalisation of a Riemannian manifold).\n First order arithmetic\n Fourier analysis\n Fractional calculus: a branch of analysis that studies the possibility of taking real or complex powers of the differentiation operator.\n Fractional dynamics: investigates the behaviour of objects and systems that are described by differentiation and integration of fractional orders using methods of fractional calculus.\n Fredholm theory: part of spectral theory studying integral equations.\n Function theory: part of analysis devoted to properties of functions, especially functions of a complex variable (see complex analysis).\n Functional analysis\n Functional calculus: historically the term was used synonymously with calculus of variations, but now refers to a branch of functional analysis connected with spectral theory\n Fuzzy arithmetic\n Fuzzy geometry\n Fuzzy Galois theory\n Fuzzy mathematics: a branch of mathematics based on fuzzy set theory and fuzzy logic.\n Fuzzy measure theory\n Fuzzy qualitative trigonometry\n Fuzzy set theory: a form of set theory that studies fuzzy sets, that is sets that have degrees of membership.\n Fuzzy topology\n\nG\n\n Galois cohomology: an application of homological algebra, it is the study of group cohomology of Galois modules.\n Galois theory: named after Évariste Galois, it is a branch of abstract algebra providing a connection between field theory and group theory.\n Galois geometry: a branch of finite geometry concerned with algebraic and analytic geometry over a Galois field.\n Game theory\n Gauge theory\n General topology: also known as point-set topology, it is a branch of topology studying the properties of topological spaces and structures defined on them. It differs from other branches of topology as the topological spaces do not have to be similar to manifolds.\n Generalized trigonometry: developments of trigonometric methods from the application to real numbers of Euclidean geometry to any geometry or space. This includes spherical trigonometry, hyperbolic trigonometry, gyrotrigonometry, rational trigonometry, universal hyperbolic trigonometry, fuzzy qualitative trigonometry, operator trigonometry and lattice trigonometry.\n Geometric algebra: an alternative approach to classical, computational and relativistic geometry. It shows a natural correspondence between geometric entities and elements of algebra.\n Geometric analysis: a discipline that uses methods from differential geometry to study partial differential equations as well as the applications to geometry.\n Geometric calculus\n Geometric combinatorics\n Geometric function theory: the study of geometric properties of analytic functions.\n Geometric homology theory\n Geometric invariant theory\n Geometric graph theory\n Geometric group theory\n Geometric measure theory\n Geometric topology: a branch of topology studying manifolds and mappings between them; in particular the embedding of one manifold into another.\n Geometry: a branch of mathematics concerned with shape and the properties of space. Classically it arose as what is now known as solid geometry; this was concerning practical knowledge of length, area and volume. It was then put into an axiomatic form by Euclid, giving rise to what is now known as classical Euclidean geometry. The use of coordinates by René Descartes gave rise to Cartesian geometry enabling a more analytical approach to geometric entities. Since then many other branches have appeared including projective geometry, differential geometry, non-Euclidean geometry, Fractal geometry and algebraic geometry. Geometry also gave rise to the modern discipline of topology.\n Geometry of numbers: initiated by Hermann Minkowski, it is a branch of number theory studying convex bodies and integer vectors.\n Global analysis: the study of differential equations on manifolds and the relationship between differential equations and topology.\n Global arithmetic dynamics\n Graph theory: a branch of discrete mathematics devoted to the study of graphs. It has many applications in physical, biological and social systems.\n Group-character theory: the part of character theory dedicated to the study of characters of group representations.\n Group representation theory\n Group theory\n Gyrotrigonometry: a form of trigonometry used in gyrovector space for hyperbolic geometry. (An analogy of the vector space in Euclidean geometry.)\n\nH\n\n Hard analysis: see classical analysis\n Harmonic analysis: part of analysis concerned with the representations of functions in terms of waves. It generalizes the notions of Fourier series and Fourier transforms from the Fourier analysis.\n High-dimensional topology\n Higher arithmetic\n Higher category theory\n Higher-dimensional algebra\n Hodge theory\n Holomorphic functional calculus: a branch of functional calculus starting with holomorphic functions.\n Homological algebra: the study of homology in general algebraic settings.\n Homology theory\n Homotopy theory\n Hyperbolic geometry: also known as Lobachevskian geometry or Bolyai-Lobachevskian geometry. It is a non-Euclidean geometry looking at hyperbolic space.\n hyperbolic trigonometry: the study of hyperbolic triangles in hyperbolic geometry, or hyperbolic functions in Euclidean geometry. Other forms include gyrotrigonometry and universal hyperbolic trigonometry.\n Hypercomplex analysis\n Hyperfunction theory\n\nI\n\n Ideal theory: once the precursor name for what is now known as commutative algebra; it is the theory of ideals in commutative rings.\n Idempotent analysis\n Incidence geometry: the study of relations of incidence between various geometric objects, like curves and lines.\n Inconsistent mathematics: see paraconsistent mathematics.\n Infinitary combinatorics: an expansion of ideas in combinatorics to account for infinite sets.\n Infinitesimal analysis: once a synonym for infinitesimal calculus\n Infinitesimal calculus: see calculus of infinitesimals\n Information geometry\n Integral calculus\n Integral geometry\n Intersection theory: a branch of algebraic geometry and algebraic topology\n Intuitionistic type theory\n Invariant theory: studies how group actions on algebraic varieties affect functions.\n Inversive geometry: the study of invariants preserved by a type of transformation known as inversion\n Inversive plane geometry: inversive geometry that is limited to two dimensions\n Inversive ring geometry\n Itō calculus\n Iwasawa theory\n\nJ\n\nK\n\n K-theory: originated as the study of a ring generated by vector bundles over a topological space or scheme. In algebraic topology it is an extraordinary cohomology theory known as topological K-theory. In algebra and algebraic geometry it is referred to as algebraic K-theory. In physics, K-theory has appeared in type II string theory. (In particular twisted K-theory.)\n K-homology\n Kähler geometry: a branch of differential geometry, more specifically a union of Riemannian geometry, complex differential geometry and symplectic geometry. It is the study of Kähler manifolds. (named after Erich Kähler)\n KK-theory\n Klein geometry: More specifically, it is a homogeneous space X together with a transitive action on X by a Lie group G, which acts as the symmetry group of the geometry.\n Knot theory: part of topology dealing with knots\n Kummer theory: provides a description of certain types of field extensions involving the adjunction of nth roots of elements of the base field\n\nL\n\n L-theory\n Large deviations theory: part of probability theory studying events of small probability (tail events).\n Large sample theory: also known as asymptotic theory\n Lattice theory: the study of lattices, being important in order theory and universal algebra\n Lattice trigonometry\n Lie algebra theory\n Lie group theory\n Lie sphere geometry\n Lie theory\n Line geometry\n Linear algebra – a branch of algebra studying linear spaces and linear maps. It has applications in fields such as abstract algebra and functional analysis; it can be represented in analytic geometry and it is generalized in operator theory and in module theory. Sometimes matrix theory is considered a branch, although linear algebra is restricted to only finite dimensions. Extensions of the methods used belong to multilinear algebra.\n Linear functional analysis\n List of graphical methods Included are diagram techniques, chart techniques, plot techniques, and other forms of visualization.\n Local algebra: a term sometimes applied to the theory of local rings.\n Local arithmetic dynamics: also known as p-adic dynamics or nonarchimedean dynamics.\n Local class field theory\n Low-dimensional topology\n\nM\n\n Malliavin calculus\n Mathematical finance\n Mathematical logic\n Mathematical optimization\n Mathematical physics: a part of mathematics that develops mathematical methods motivated by problems in physics.\n Mathematical sciences: refers to academic disciplines that are mathematical in nature, but are not considered proper subfields of mathematics. Examples include statistics, cryptography, game theory and actuarial science.\n Matrix algebra\n Matrix calculus\n Matrix theory\n Matroid theory\n Measure theory\n Metric geometry\n Microlocal analysis\n Model theory\n Modern algebra: see abstract algebra\n Modern algebraic geometry: the form of algebraic geometry given by Alexander Grothendieck and Jean-Pierre Serre drawing on sheaf theory.\n Modern invariant theory: the form of invariant theory that analyses the decomposition of representations into irreducibles.\n Modular representation theory\n Module theory\n Molecular geometry\n Morse theory: a part of differential topology, it analyzes the topological space of a manifold by studying differentiable functions on that manifold.\n Motivic cohomology\n Multilinear algebra: an extension of linear algebra building upon concepts of p-vectors and multivectors with Grassmann algebra.\n Multiplicative calculus: multiplicative alternatives to the classical additive calculus of Newton and Leibniz.\n Multiplicative number theory: a subfield of analytic number theory that deals with prime numbers, factorization and divisors.\n Multivariable calculus\n Multiple-scale analysis\n\nN\n\n Neutral geometry: see absolute geometry\n Nevanlinna theory: part of complex analysis studying the value distribution of meromorphic functions. It is named after Rolf Nevanlinna\n Nielsen theory: an area of mathematical research with its origins in fixed point topology, developed by Jakob Nielsen\n Non-abelian class field theory\n Non-classical analysis\n Non-Euclidean geometry\n Non-Newtonian calculus: alternatives to the classical calculus of Newton and Leibniz\n Non-standard analysis\n Non-standard calculus\n Nonarchimedean dynamics: also known as p-adic analysis or local arithmetic dynamics\n Noncommutative algebraic geometry: a direction in noncommutative geometry studying the geometric properties of formal duals of non-commutative algebraic objects.\n Noncommutative geometry\n Noncommutative harmonic analysis: see representation theory\n Noncommutative topology\n Nonlinear analysis\n Nonlinear functional analysis\n Number theory: a branch of pure mathematics primarily devoted to the study of the integers. Originally it was known as arithmetic or higher arithmetic.\n Numerical analysis\n Numerical geometry\n Numerical linear algebra\n\nO\n\n Operad theory: a type of abstract algebra concerned with prototypical algebras.\n Operator geometry\n Operator K-theory\n Operator theory: part of functional analysis studying operators.\n Operator trigonometry\n Optimal control theory: a generalization of the calculus of variations.\n Orbifold theory\n Order theory: a branch that investigates the intuitive notion of order using binary relations.\n Ordered geometry: a form of geometry  omitting the notion of measurement but featuring the concept of intermediacy. It is a fundamental geometry forming a common framework for affine geometry, Euclidean geometry, absolute geometry and hyperbolic geometry.\n Oriented elliptic geometry\n Oriented spherical geometry\n Oscillation theory\n\nP\n\n p-adic analysis: a branch of number theory that deals with the analysis of functions of p-adic numbers.\n p-adic dynamics: an application of p-adic analysis looking at p-adic differential equations.\n p-adic Hodge theory\n Parabolic geometry\n Paraconsistent mathematics: sometimes called inconsistent mathematics, it is an attempt to develop the classical infrastructure of mathematics based on a foundation of paraconsistent logic instead of classical logic.\n Partition theory\n Perturbation theory\n Picard–Vessiot theory\n Plane geometry\n Point-set topology: see general topology\n Pointless topology\n Poisson geometry\n Polyhedral combinatorics: a branch within combinatorics and discrete geometry that studies the problems of  describing convex polytopes.\n Polyhedral geometry\n Possibility theory\n Potential theory\n Precalculus\n Predicative mathematics\n Probability theory\n Probabilistic combinatorics\n Probabilistic graph theory\n Probabilistic number theory\n Projective geometry: a form of geometry that studies geometric properties that are invariant under a projective transformation.\n Projective differential geometry\n Proof theory\n Pseudo-Riemannian geometry: generalizes Riemannian geometry to the study of pseudo-Riemannian manifolds.\n Pure mathematics: the part of mathematics that studies entirely abstract concepts.\n\nQ\n\n Quantum calculus: a form of calculus without the notion of limits. There are 2 forms known as q-calculus and h-calculus\n Quantum geometry: the generalization of concepts of geometry used to describe the physical phenomena of quantum physics\n Quaternionic analysis\n Quick maths: The rapid application of arithmetic in the form of a rap.\n\nR\n\n Ramsey theory: the study of the conditions in which order must appear. It is named after Frank P. Ramsey.\n Rational geometry\n Rational trigonometry: a reformulation of trigonometry in terms of spread and quadrance instead of angle and length.\n Real algebra: the study of the part of algebra relevant to real algebraic geometry.\n Real algebraic geometry: the part of algebraic geometry that studies real points of the algebraic varieties.\n Real analysis: a branch of mathematical analysis; in particular hard analysis, that is the study of real numbers and functions of Real values. It provides a rigorous formulation of the calculus of real numbers in terms of continuity and smoothness, whilst the theory is extended to the complex numbers in complex analysis.\n Real analytic geometry\n Real K-theory\n Recreational mathematics: the area dedicated to mathematical puzzles and mathematical games.\n Recursion theory: see computability theory\n Representation theory: a subfield of abstract algebra; it studies algebraic structures by representing their elements as linear transformations of vector spaces. It also studies modules over these algebraic structures, providing a way of reducing problems in abstract algebra to problems in linear algebra.\n Representation theory of algebraic groups\n Representation theory of algebras\n Representation theory of diffeomorphism groups\n Representation theory of finite groups\n Representation theory of groups\n Representation theory of Hopf algebras\n Representation theory of Lie algebras\n Representation theory of Lie groups\n Representation theory of the Galilean group\n Representation theory of the Lorentz group\n Representation theory of the Poincaré group\n Representation theory of the symmetric group\n Ribbon theory: a branch of topology studying ribbons.\n Riemannian geometry: a branch of differential geometry that is more specifically, the study of Riemannian manifolds. It is named after Bernhard Riemann and it features many generalizations of concepts from Euclidean geometry, analysis and calculus.\n Rough set theory: the a form of set theory based on rough sets.\n\nS\n\n Scheme theory: the study of schemes introduced by Alexander Grothendieck. It allows the use of sheaf theory to study algebraic varieties and is considered the central part of modern algebraic geometry.\n Secondary calculus\n Self-similarity an object is exactly or approximately similar to a part of itself (i.e. the whole has the same shape as one or more of the parts).\n Semialgebraic geometry: a part of algebraic geometry; more specifically a branch of real algebraic geometry that studies semialgebraic sets.\n Set-theoretic topology\n Set theory\n Sheaf theory\n Sheaf cohomology\n Sieve theory\n Single operator theory: deals with the properties and classifications of single operators.\n Singularity theory: a branch, notably of geometry; that studies the failure of manifold structure.\n Smooth infinitesimal analysis: a rigorous reformation of infinitesimal calculus employing methods of category theory. As a theory, it is a subset of synthetic differential geometry.\n Solid geometry\n Spatial geometry\n Spectral geometry: a field that concerns the relationships between geometric structures of manifolds and spectra of canonically defined differential operators.\n Spectral graph theory: the study of properties of a graph using methods from matrix theory.\n Spectral theory: part of operator theory extending the concepts of eigenvalues and eigenvectors from linear algebra and matrix theory.\n Spectral theory of ordinary differential equations: part of spectral theory concerned with the spectrum and eigenfunction expansion associated with linear ordinary differential equations.\n Spectrum continuation analysis: generalizes the concept of a Fourier series to non-periodic functions.\n Spherical geometry: a branch of non-Euclidean geometry, studying the 2-dimensional surface of a sphere.\n Spherical trigonometry: a branch of spherical geometry that studies polygons on the surface of a sphere. Usually the polygons are triangles.\n Statistics: although the term may refer to the more general study of statistics, the term is used in mathematics to refer to the mathematical study of statistics and related fields. This includes probability theory.\n Stochastic calculus\n Stochastic calculus of variations\n Stochastic geometry: the study of random patterns of points\n Stratified Morse theory\n Super category theory\n Super linear algebra\n Surgery theory: a part of geometric topology referring to methods used to produce one manifold from another (in a controlled way.)\n Symbolic computation: also known as algebraic computation and computer algebra. It refers to the techniques used to manipulate mathematical expressions and equations in symbolic form as opposed to manipulating them by the numerical quantities represented by them.\n Symbolic dynamics\n Symmetric function theory\n Symplectic geometry: a branch of differential geometry and topology whose main object of study is the symplectic manifold.\n Symplectic topology\n Synthetic differential geometry: a reformulation of differential geometry in the language of topos theory and in the context of an intuitionistic logic.\n Synthetic geometry: also known as axiomatic geometry, it is a branch of geometry that uses axioms and logical arguments to draw conclusions as opposed to analytic and algebraic methods.\n Systolic geometry: a branch of differential geometry studying systolic invariants of manifolds and polyhedra.\n Systolic hyperbolic geometry: the study of systoles in hyperbolic geometry.\n\nT\n\n Tensor analysis: the study of tensors, which play a role in subjects such as differential geometry, mathematical physics, algebraic topology, multilinear algebra, homological algebra and representation theory.\n Tensor calculus: an older term for tensor analysis.\n Tensor theory: an alternative name for tensor analysis.\n Tessellation: when periodic tiling has a repeating pattern.\n Theoretical physics: a branch primarily of the science physics that uses mathematical models and abstraction of physics to rationalize and predict phenomena.\n Time-scale calculus\n Topology\n Topological combinatorics: the application of methods from algebraic topology to solve problems in combinatorics.\n Topological degree theory\n Topological fixed point theory\n Topological graph theory\n Topological K-theory\n Topos theory\n Toric geometry\n Transcendental number theory: a branch of number theory that revolves around the transcendental numbers.\n Transfinite order theory\n Transformation geometry\n Trigonometry: the study of triangles and the relationships between the length of their sides, and the angles between them. It is essential to many parts of applied mathematics.\n Tropical analysis: see idempotent analysis\n Tropical geometry\n Twisted K-theory: a variation on K-theory, spanning abstract algebra, algebraic topology and operator theory.\n Type theory\n\nU\n\n Umbral calculus: the study of Sheffer sequences\n Uncertainty theory: a new branch of mathematics based on normality, monotonicity, self-duality, countable subadditivity, and product measure axioms.\n Unitary representation theory\n Universal algebra: a field studying the formalization of algebraic structures itself.\n Universal hyperbolic trigonometry: an approach to hyperbolic trigonometry based on rational geometry.\n\nV\n\n Valuation theory\n Variational analysis\n Vector algebra: a part of linear algebra concerned with the operations of vector addition and scalar multiplication, although it may also refer to vector operations of vector calculus, including the dot and cross product. In this case it can be contrasted with geometric algebra which generalizes into higher dimensions.\n Vector analysis: also known as vector calculus, see vector calculus.\n Vector calculus: a branch of multivariable calculus concerned with differentiation and integration of vector fields. Primarily it is concerned with 3-dimensional Euclidean space.\n\nW\n Wavelets\n Windowed Fourier transform\n Window functions\n\nX\n\nY\n\nZ\n\nSee also\n Glossary of astronomy\n Glossary of biology\n Glossary of calculus\n Glossary of chemistry\n Glossary of engineering\n Glossary of physics\n Glossary of probability and statistics\n\nareas of mathematics\n* \nA\nMathematics"
    },
    {
      "title": "Glossary of calculus",
      "url": "https://en.wikipedia.org/wiki/Glossary_of_calculus",
      "text": "Most of the terms listed in Wikipedia glossaries are already defined and explained within Wikipedia itself.  However, glossaries like this one are useful for looking up, comparing and reviewing large numbers of terms together.  You can help enhance this page by adding new terms or writing definitions for existing ones.\n\nThis glossary of calculus is a list of definitions about calculus, its sub-disciplines, and related fields.\n\n A \n\nB\n\nC\n\n \n\nD\n\nE\n\nF\n\nG\n\nH\n\nI\n\n \n\n \n\nJ\n\nK\n\nL\n\nM\n\n N \n\nO\n\nP\n\nQ\n\nR\n\nS\n\nT\n\nU\n\nV\n\nW\n\nX\n\nY\n\nZ\n\n See also \nCalculus\nOutline of calculus\nGlossary of areas of mathematics\nGlossary of astronomy\nGlossary of biology\nGlossary of botany\nGlossary of chemistry\nGlossary of ecology\nGlossary of engineering\nGlossary of physics\nGlossary of probability and statistics\n\nReferences\n\nNotes\n\n* \nC\nCalculus"
    },
    {
      "title": "Algebraic geometry",
      "url": "https://en.wikipedia.org/wiki/Algebraic_geometry",
      "text": "thumb|This Togliatti surface is an algebraic surface of degree five. The picture represents a portion of its real locus.\n\nAlgebraic geometry is a branch of mathematics, classically studying zeros of multivariate polynomials. Modern algebraic geometry is based on the use of abstract algebraic techniques, mainly from commutative algebra, for solving  geometrical problems about these sets of zeros.\n\nThe fundamental objects of study in algebraic geometry are algebraic varieties, which are geometric manifestations of solutions of systems of polynomial equations. Examples of the most studied classes of algebraic varieties are: plane algebraic curves, which include lines, circles, parabolas, ellipses, hyperbolas, cubic curves like elliptic curves, and quartic curves like lemniscates and Cassini ovals. A point of the plane belongs to an algebraic curve if its coordinates satisfy a given polynomial equation. Basic questions involve the study of the points of special interest like the singular points, the inflection points and the points at infinity. More advanced questions involve the topology of the curve and relations between the curves given by different equations.\n\nAlgebraic geometry occupies a central place in modern mathematics and has multiple conceptual connections with such diverse fields as complex analysis, topology and number theory. Initially a study of systems of polynomial equations in several variables, the subject of algebraic geometry starts where equation solving leaves off, and it becomes even more important to understand the intrinsic properties of the totality of solutions of a system of equations, than to find a specific solution; this leads into some of the deepest areas in all of mathematics, both conceptually and in terms of technique.\n\nIn the 20th century, algebraic geometry split into several subareas.\n The mainstream of algebraic geometry is devoted to the study of the complex points of the algebraic varieties and more generally to the points with coordinates in an algebraically closed field.\n Real algebraic geometry is the study of the real points of an algebraic variety.\n Diophantine geometry and, more generally, arithmetic geometry is the study of the points of an algebraic variety with coordinates in fields that are not algebraically closed and occur in algebraic number theory, such as the field of rational numbers, number fields, finite fields, function fields, and p-adic fields.\n A large part of singularity theory is devoted to the singularities of algebraic varieties.\n Computational algebraic geometry is an area that has emerged at the intersection of algebraic geometry and computer algebra, with the rise of computers. It consists mainly of algorithm design and software development for the study of properties of explicitly given algebraic varieties.\n\nMuch of the development of the mainstream of algebraic geometry in the 20th century occurred within an abstract algebraic framework, with increasing emphasis being placed on \"intrinsic\" properties of algebraic varieties not dependent on any particular way of embedding the variety in an ambient coordinate space; this parallels developments in topology, differential and complex geometry. One key achievement of this abstract algebraic geometry is Grothendieck's scheme theory which allows one to use sheaf theory to study algebraic varieties in a way which is very similar to its use in the study of differential and analytic manifolds. This is obtained by extending the notion of point: In classical algebraic geometry, a point of an affine variety may be identified, through Hilbert's Nullstellensatz, with a maximal ideal of the coordinate ring, while the points of the corresponding affine scheme are all prime ideals of this ring. This means that a point of such a scheme may be either a usual point or a subvariety. This approach also enables a unification of the language and the tools of classical algebraic geometry, mainly concerned with complex points, and of algebraic number theory. Wiles' proof of the longstanding conjecture called Fermat's last theorem is an example of the power of this approach.\n\nBasic notions\n\n Zeros of simultaneous polynomials \nthumb|right|Sphere and slanted circle\nIn classical algebraic geometry, the main objects of interest are the vanishing sets of collections of polynomials, meaning the set of all points that simultaneously satisfy one or more polynomial equations. For instance, the two-dimensional sphere of radius 1 in three-dimensional Euclidean space R3 could be defined as the set of all points (x,y,z) with\n\nA \"slanted\" circle in R3 can be defined as the set of all points (x,y,z) which satisfy the two polynomial equations\n\n Affine varieties \n\nFirst we start with a field k. In classical algebraic geometry, this field was always the complex numbers C, but many of the same results are true if we assume only that k is algebraically closed. We consider the affine space of dimension n over k, denoted An(k) (or more simply An, when k is clear from the context). When one fixes a coordinate system, one may identify An(k) with kn. The purpose of not working with kn is to emphasize that one \"forgets\" the vector space structure that kn carries.\n\nA function f : An → A1 is said to be polynomial (or regular) if it can be written as a polynomial, that is, if there is a polynomial p in k[x1,...,xn] such that f(M) = p(t1,...,tn) for every point M with coordinates (t1,...,tn) in An. The property of a function to be polynomial (or regular) does not depend on the choice of a coordinate system in An.\n\nWhen a coordinate system is chosen, the regular functions on the affine n-space may be identified with the ring of polynomial functions in n variables over k. Therefore, the set of the regular functions on An is a ring, which is denoted k[An].\n\nWe say that a polynomial vanishes at a point if evaluating it at that point gives zero. Let S be a set of polynomials in k[An]. The vanishing set of S (or vanishing locus or zero set) is the set V(S) of all points in An where every polynomial in S vanishes. Symbolically,\n\nA subset of An which is V(S), for some S, is called an algebraic set. The V stands for variety (a specific type of algebraic set to be defined below).\n\nGiven a subset U of An, can one recover the set of polynomials which generate it? If U is any subset of An, define I(U) to be the set of all polynomials whose vanishing set contains U. The I stands for ideal: if two polynomials f and g both vanish on U, then f+g vanishes on U, and if h is any polynomial, then hf vanishes on U, so I(U) is always an ideal of the polynomial ring k[An].\n\nTwo natural questions to ask are:\n Given a subset U of An, when is U = V(I(U))?\n Given a set S of polynomials, when is S = I(V(S))?\n\nThe answer to the first question is provided by introducing the Zariski topology, a topology on An whose closed sets are the algebraic sets, and which directly reflects the algebraic structure of k[An]. Then U = V(I(U)) if and only if U is an algebraic set or equivalently a Zariski-closed set. The answer to the second question is given by Hilbert's Nullstellensatz. In one of its forms, it says that I(V(S)) is the radical of the ideal generated by S. In more abstract language, there is a Galois connection, giving rise to two closure operators; they can be identified, and naturally play a basic role in the theory; the example is elaborated at Galois connection.\n\nFor various reasons we may not always want to work with the entire ideal corresponding to an algebraic set U. Hilbert's basis theorem implies that ideals in k[An] are always finitely generated.\n\nAn algebraic set is called irreducible if it cannot be written as the union of two smaller algebraic sets. Any algebraic set is a finite union of irreducible algebraic sets and this decomposition is unique. Thus its elements are called the irreducible components of the algebraic set. An irreducible algebraic set is also called a variety. It turns out that an algebraic set is a variety if and only if it may be defined as the vanishing set of a prime ideal of the polynomial ring.\n\nSome authors do not make a clear distinction between algebraic sets and varieties and use irreducible variety to make the distinction when needed.\n\n Regular functions \n\nJust as continuous functions are the natural maps on topological spaces and smooth functions are the natural maps on differentiable manifolds, there is a natural class of functions on an algebraic set, called regular functions or polynomial functions. A regular function on an algebraic set V contained in An is the restriction to V of a regular function on An. For an algebraic set defined on the field of the complex numbers, the regular functions are smooth and even analytic.\n\nIt may seem unnaturally restrictive to require that a regular function always extend to the ambient space, but it is very similar to the situation in a normal topological space, where the Tietze extension theorem guarantees that a continuous function on a closed subset always extends to the ambient topological space.\n\nJust as with the regular functions on affine space, the regular functions on V form a ring, which we denote by k[V]. This ring is called the coordinate ring of V.\n\nSince regular functions on V come from regular functions on An, there is a relationship between the coordinate rings. Specifically, if a regular function on V is the restriction of two functions f and g in k[An], then f − g is a polynomial function which is null on V and thus belongs to I(V). Thus k[V] may be identified with k[An]/I(V).\n\n Morphism of affine varieties \nUsing regular functions from an affine variety to A1, we can define regular maps from one affine variety to another. First we will define a regular map from a variety into affine space: Let V be a variety contained in An. Choose m regular functions on V, and call them f1, ..., fm. We define a regular map f from V to Am by letting . In other words, each fi determines one coordinate of the range of f.\n\nIf V′ is a variety contained in Am, we say that f is a regular map from V to V′ if the range of f is contained in V′.\n\nThe definition of the regular maps apply also to algebraic sets.\nThe regular maps are also called morphisms, as they make the collection of all affine algebraic sets into a category, where the objects are the affine algebraic sets and the morphisms are the regular maps. The affine varieties is a subcategory of the category of the algebraic sets.\n\nGiven a regular map g from V to V′ and a regular function f of k[V′], then . The map  is a ring homomorphism from k[V′] to k[V]. Conversely, every ring homomorphism from k[V′] to k[V] defines a regular map from V to V′. This defines an equivalence of categories between the category of algebraic sets and the opposite category of the finitely generated reduced k-algebras. This equivalence is one of the starting points of scheme theory.\n\n Rational function and birational equivalence \n\nIn contrast to the preceding sections, this section concerns only varieties and not algebraic sets. On the other hand, the definitions extend naturally to projective varieties (next section), as an affine variety and its projective completion have the same field of functions.\n\nIf V is an affine variety, its coordinate ring is an integral domain and has thus a field of fractions which is denoted k(V) and called the field of the rational functions on V or, shortly, the function field of V. Its elements are the restrictions to V of the rational functions over the affine space containing V. The domain of a rational function f is not V but the complement of the subvariety (a hypersurface) where the denominator of f vanishes.\n\nAs with regular maps, one may define a rational map from a variety V to a variety V'. As with the regular maps, the rational maps from V to V' may be identified to the field homomorphisms from k(V') to k(V).\n\nTwo affine varieties are birationally equivalent if there are two rational functions between them which are inverse one to the other in the regions where both are defined. Equivalently, they are birationally equivalent if their function fields are isomorphic.\n\nAn affine variety is a rational variety if it is birationally equivalent to an affine space. This means that the variety admits a rational parameterization. For example, the circle of equation  is a rational curve, as it has the parameterization\n\nwhich may also be viewed as a rational map from the line to the circle.\n\nThe problem of resolution of singularities is to know if every algebraic variety is birationally equivalent to a variety whose projective completion is nonsingular (see also smooth completion). It was solved in the affirmative in characteristic 0 by Heisuke Hironaka in 1964 and is yet unsolved in finite characteristic.\n\n Projective variety \n\nthumb|Parabola (, red) and cubic (, blue) in projective space\nJust as the formulas for the roots of second, third, and fourth degree polynomials suggest extending real numbers to the more algebraically complete setting of the complex numbers, many properties of algebraic varieties suggest extending affine space to a more geometrically complete projective space. Whereas the complex numbers are obtained by adding the number i, a root of the polynomial , projective space is obtained by adding in appropriate points \"at infinity\", points where parallel lines may meet.\n\nTo see how this might come about, consider the variety . If we draw it, we get a parabola. As x goes to positive infinity, the slope of the line from the origin to the point (x, x2) also goes to positive infinity. As x goes to negative infinity, the slope of the same line goes to negative infinity.\n\nCompare this to the variety V(y − x3). This is a cubic curve. As x goes to positive infinity, the slope of the line from the origin to the point (x, x3) goes to positive infinity just as before. But unlike before, as x goes to negative infinity, the slope of the same line goes to positive infinity as well; the exact opposite of the parabola. So the behavior \"at infinity\" of V(y − x3) is different from the behavior \"at infinity\" of V(y − x2).\n\nThe consideration of the projective completion of the two curves, which is their prolongation \"at infinity\" in the projective plane, allows us to quantify this difference: the point at infinity of the parabola is a regular point, whose tangent is the line at infinity, while the point at infinity of the cubic curve is a cusp. Also, both curves are rational, as they are parameterized by x, and the Riemann-Roch theorem implies that the cubic curve must have a singularity, which must be at infinity, as all its points in the affine space are regular.\n\nThus many of the properties of algebraic varieties, including birational equivalence and all the topological properties, depend on the behavior \"at infinity\" and so it is natural to study the varieties in projective space. Furthermore, the introduction of projective techniques made many theorems in algebraic geometry simpler and sharper: For example, Bézout's theorem on the number of intersection points between two varieties can be stated in its sharpest form only in projective space. For these reasons, projective space plays a fundamental role in algebraic geometry.\n\nNowadays, the projective space Pn of dimension n is usually defined as the set of the lines passing through a point, considered as the origin, in the affine space of dimension , or equivalently to the set of the vector lines in a vector space of dimension . When a coordinate system has been chosen in the space of dimension , all the points of a line have the same set of coordinates, up to the multiplication by an element of k. This defines the homogeneous coordinates of a point of Pn as a sequence of  elements of the base field k, defined up to the multiplication by a nonzero element of k (the same for the whole sequence).\n\nA polynomial in  variables vanishes at all points of a line passing through the origin if and only if it is homogeneous. In this case, one says that the polynomial vanishes at the corresponding point of Pn. This allows us to define a projective algebraic set in Pn as the set , where a finite set of homogeneous polynomials  vanishes. Like for affine algebraic sets, there is a bijection between the projective algebraic sets and the reduced homogeneous ideals which define them. The projective varieties are the projective algebraic sets whose defining ideal is prime. In other words, a projective variety is a projective algebraic set, whose homogeneous coordinate ring is an integral domain, the projective coordinates ring being defined as the quotient of the graded ring or the polynomials in  variables by the homogeneous (reduced) ideal defining the variety. Every projective algebraic set may be uniquely decomposed into a finite union of projective varieties.\n\nThe only regular functions which may be defined properly on a projective variety are the constant functions. Thus this notion is not used in projective situations. On the other hand, the field of the rational functions or function field  is a useful notion, which, similarly to the affine case, is defined as the set of the quotients of two homogeneous elements of the same degree in the homogeneous coordinate ring.\n\nReal algebraic geometry\n\nReal algebraic geometry is the study of the real points of algebraic varieties.\n\nThe fact that the field of the real numbers is an ordered field cannot be ignored in such a study. For example, the curve of equation  is a circle if , but does not have any real point if . It follows that real algebraic geometry is not only the study of the real algebraic varieties, but has been generalized to the study of the semi-algebraic sets, which are the solutions of systems of polynomial equations and polynomial inequalities. For example, a branch of the hyperbola of equation  is not an algebraic variety, but is a semi-algebraic set defined by  and  or by  and .\n\nOne of the challenging problems of real algebraic geometry is the unsolved Hilbert's sixteenth problem: Decide which respective positions are possible for the ovals of a nonsingular plane curve of degree 8.\n\n Computational algebraic geometry \nOne may date the origin of computational algebraic geometry to meeting EUROSAM'79 (International Symposium on Symbolic and Algebraic Manipulation) held at Marseille, France in June 1979. At this meeting,\n Dennis S. Arnon showed that George E. Collins's Cylindrical algebraic decomposition (CAD) allows the computation of the topology of semi-algebraic sets,\n Bruno Buchberger presented the Gröbner bases and his algorithm to compute them,\n Daniel Lazard presented a new algorithm for solving systems of homogeneous polynomial equations with a computational complexity which is essentially polynomial in the expected number of solutions and thus simply exponential in the number of the unknowns. This algorithm is strongly related with Macaulay's multivariate resultant.\n\nSince then, most results in this area are related to one or several of these items either by using or improving one of these algorithms, or by finding algorithms whose complexity is simply exponential in the number of the variables.\n\nA body of mathematical theory complementary to symbolic methods called numerical algebraic geometry has been developed over the last several decades.  The main computational method is homotopy continuation.  This supports, for example, a model of floating point computation for solving problems of algebraic geometry.\n\nGröbner basis\n\nA Gröbner basis is a system of generators of a polynomial ideal whose computation allows the deduction of many properties of the affine algebraic variety defined by the ideal.\n\nGiven an ideal I defining an algebraic set V:\n V is empty (over an algebraically closed extension of the basis field), if and only if the Gröbner basis for any monomial ordering is reduced to {1}.\n By means of the Hilbert series one may compute the dimension and the degree of V from any Gröbner basis of I for a monomial ordering refining the total degree.\n If the dimension of V is 0, one may compute the points (finite in number) of V from any Gröbner basis of I (see Systems of polynomial equations).\n A Gröbner basis computation allows one to remove from V all irreducible components which are contained in a given hypersurface.\n A Gröbner basis computation allows one to compute the Zariski closure of the image of V by the projection on the k first coordinates, and the subset of the image where the projection is not proper.\n More generally Gröbner basis computations allow one to compute the Zariski closure of the image and the critical points of a rational function of V into another affine variety.\n\nGröbner basis computations do not allow one to compute directly the primary decomposition of I nor the prime ideals defining the irreducible components of V, but most algorithms for this involve Gröbner basis computation. The algorithms which are not based on Gröbner bases use regular chains but may need Gröbner bases in some exceptional situations.\n\nGröbner bases are deemed to be difficult to compute. In fact they may contain, in the worst case, polynomials whose degree is doubly exponential in the number of variables and a number of polynomials which is also doubly exponential. However, this is only a worst case complexity, and the complexity bound of Lazard's algorithm of 1979 may frequently apply. Faugère F5 algorithm realizes this complexity, as it may be viewed as an improvement of Lazard's 1979 algorithm. It follows that the best implementations allow one to compute almost routinely with algebraic sets of degree more than 100. This means that, presently, the difficulty of computing a Gröbner basis is strongly related to the intrinsic difficulty of the problem.\n\nCylindrical algebraic decomposition (CAD)\nCAD is an algorithm which was introduced in 1973 by G. Collins to implement with an acceptable complexity the Tarski–Seidenberg theorem on quantifier elimination over the real numbers.\n\nThis theorem concerns the formulas of the first-order logic whose atomic formulas are polynomial equalities or inequalities between polynomials with real coefficients. These formulas are thus the formulas which may be constructed from the atomic formulas by the logical operators and (∧), or (∨), not (¬), for all (∀) and exists (∃). Tarski's theorem asserts that, from such a formula, one may compute an equivalent formula without quantifier (∀, ∃).\n\nThe complexity of CAD is doubly exponential in the number of variables. This means that CAD allows, in theory, to solve every problem of real algebraic geometry which may be expressed by such a formula, that is almost every problem concerning explicitly given varieties and semi-algebraic sets.\n\nWhile Gröbner basis computation has doubly exponential complexity only in rare cases, CAD has almost always this high complexity. This implies that, unless if most polynomials appearing in the input are linear, it may not solve problems with more than four variables.\n\nSince 1973, most of the research on this subject is devoted either to improve CAD or to find alternative algorithms in special cases of general interest.\n\nAs an example of the state of art, there are efficient algorithms to find at least a point in every connected component of a semi-algebraic set, and thus to test if a semi-algebraic set is empty. On the other hand, CAD is yet, in practice, the best algorithm to count the number of connected components.\n\n Asymptotic complexity vs. practical efficiency \nThe basic general algorithms of computational geometry have a double exponential worst case complexity. More precisely, if d is the maximal degree of the input polynomials and n the number of variables, their complexity is at most  for some constant c, and, for some inputs, the complexity is at least  for another constant c′.\n\nDuring the last 20 years of 20th century, various algorithms have been introduced to solve specific subproblems with a better complexity. Most of these algorithms have a complexity .\n\nAmong these algorithms which solve a sub problem of the problems solved by Gröbner bases, one may cite testing if an affine variety is empty and solving nonhomogeneous polynomial systems which have a finite number of solutions. Such algorithms are rarely implemented because, on most entries Faugère's F4 and F5 algorithms have a better practical efficiency and probably a similar or better complexity (probably because the evaluation of the complexity of Gröbner basis algorithms on a particular class of entries is a difficult task which has been done only in a few special cases).\n\nThe main algorithms of real algebraic geometry which solve a problem solved by CAD are related to the topology of semi-algebraic sets. One may cite counting the number of connected components, testing if two points are in the same components or computing a Whitney stratification of a real algebraic set. They have a complexity of\n, but the constant involved by O notation is so high that using them to solve any nontrivial problem effectively solved by CAD, is impossible even if one could use all the existing computing power in the world. Therefore, these algorithms have never been implemented and this is an active research area to search for algorithms with have together a good asymptotic complexity and a good practical efficiency.\n\n Abstract modern viewpoint \nThe modern approaches to algebraic geometry redefine and effectively extend the range of basic objects in various levels of generality to schemes, formal schemes, ind-schemes, algebraic spaces, algebraic stacks and so on. The need for this arises already from the useful ideas within theory of varieties, e.g. the formal functions of Zariski can be accommodated by introducing nilpotent elements in structure rings; considering spaces of loops and arcs, constructing quotients by group actions and developing formal grounds for natural intersection theory and deformation theory lead to some of the further extensions.\n\nMost remarkably, in late 1950s, algebraic varieties were subsumed into Alexander Grothendieck's concept of a scheme. Their local objects are affine schemes or prime spectra which are locally ringed spaces which form a category which is antiequivalent to the category of commutative unital rings, extending the duality between the category of affine algebraic varieties over a field k, and the category of finitely generated reduced k-algebras. The gluing is along Zariski topology; one can glue within the category of locally ringed spaces, but also, using the Yoneda embedding, within the more abstract category of presheaves of sets over the category of affine schemes. The Zariski topology in the set theoretic sense is then replaced by a Grothendieck topology. Grothendieck introduced Grothendieck topologies having in mind more exotic but geometrically finer and more sensitive examples than the crude Zariski topology, namely the étale topology, and the two flat Grothendieck topologies: fppf and fpqc; nowadays some other examples became prominent including Nisnevich topology. Sheaves can be furthermore generalized to stacks in the sense of Grothendieck, usually with some additional representability conditions leading to Artin stacks and, even finer, Deligne-Mumford stacks, both often called algebraic stacks.\n\nSometimes other algebraic sites replace the category of affine schemes. For example, Nikolai Durov has introduced commutative algebraic monads as a generalization of local objects in a generalized algebraic geometry. Versions of a tropical geometry, of an absolute geometry over a field of one element and an algebraic analogue of Arakelov's geometry were realized in this setup.\n\nAnother formal generalization is possible to universal algebraic geometry in which every variety of algebras has its own algebraic geometry. The term variety of algebras should not be confused with algebraic variety.\n\nThe language of schemes, stacks and generalizations has proved to be a valuable way of dealing with geometric concepts and became cornerstones of modern algebraic geometry.\n\nAlgebraic stacks can be further generalized and for many practical questions like deformation theory and intersection theory, this is often the most natural approach. One can extend the Grothendieck site of affine schemes to a higher categorical site of derived affine schemes, by replacing the commutative rings with an infinity category of differential graded commutative algebras, or of simplicial commutative rings or a similar category with an appropriate variant of a Grothendieck topology. One can also replace presheaves of sets by presheaves of simplicial sets (or of infinity groupoids). Then, in presence of an appropriate homotopic machinery one can develop a notion of derived stack as such a presheaf on the infinity category of derived affine schemes, which is satisfying certain infinite categorical version of a sheaf axiom (and to be algebraic, inductively a sequence of representability conditions). Quillen model categories, Segal categories and quasicategories are some of the most often used tools to formalize this yielding the derived algebraic geometry, introduced by the school of Carlos Simpson, including Andre Hirschowitz, Bertrand Toën, Gabrielle Vezzosi, Michel Vaquié and others; and developed further by Jacob Lurie, Bertrand Toën, and Gabrielle Vezzosi. Another (noncommutative) version of derived algebraic geometry, using A-infinity categories has been developed from early 1990s by Maxim Kontsevich and followers.\n\n History \n\nBefore the 16th century\nSome of the roots of algebraic geometry date back to the work of the Hellenistic Greeks from the 5th century BC. The Delian problem, for instance, was to construct a length x so that the cube of side x contained the same volume as the rectangular box a2b for given sides a and b. Menaechmus (circa 350 BC) considered the problem geometrically by intersecting the pair of plane conics ay = x2 and xy = ab. The later work, in the 3rd century BC, of Archimedes and Apollonius studied more systematically problems on conic sections,Kline, M. (1972) Mathematical Thought from Ancient to Modern Times (Volume 1). Oxford University Press. pp. 108, 90. and also involved the use of coordinates. The Arab mathematicians were able to solve by purely algebraic means certain cubic equations, and then to interpret the results geometrically. This was done, for instance, by Ibn al-Haytham in the 10th century AD.Kline, M. (1972) Mathematical Thought from Ancient to Modern Times (Volume 1). Oxford University Press. p. 193. Subsequently, Persian mathematician Omar Khayyám (born 1048 A.D.) discovered a method for solving cubic equations by intersecting a parabola with a circleKline, M. (1972) Mathematical Thought from Ancient to Modern Times (Volume 1). Oxford University Press. pp. 193–195. and seems to have been the first to conceive a general theory of cubic equations.St Andrews  \"Khayyam himself seems to have been the first to conceive a general theory of cubic equations.\" A few years after Omar Khayyám, Sharaf al-Din al-Tusi's Treatise on equations has been described as \"inaugurating the beginning of algebraic geometry\".Rashed (1994, pp.102-3)\n\nRenaissance\n\nSuch techniques of applying geometrical constructions to algebraic problems were also adopted by a number of Renaissance mathematicians such as Gerolamo Cardano and Niccolò Fontana \"Tartaglia\" on their studies of the cubic equation. The geometrical approach to construction problems, rather than the algebraic one, was favored by most 16th and 17th century mathematicians, notably Blaise Pascal who argued against the use of algebraic and analytical methods in geometry.Kline, M. (1972) Mathematical Thought from Ancient to Modern Times (Volume 1). Oxford University Press. p. 279. The French mathematicians Franciscus Vieta and later René Descartes and Pierre de Fermat revolutionized the conventional way of thinking about construction problems through the introduction of coordinate geometry. They were interested primarily in the properties of algebraic curves, such as those defined by Diophantine equations (in the case of Fermat), and the algebraic reformulation of the classical Greek works on conics and cubics (in the case of Descartes).\n\nDuring the same period, Blaise Pascal and Gérard Desargues approached geometry from a different perspective, developing the synthetic notions of projective geometry. Pascal and Desargues also studied curves, but from the purely geometrical point of view: the analog of the Greek ruler and compass construction. Ultimately, the analytic geometry of Descartes and Fermat won out, for it supplied the 18th century mathematicians with concrete quantitative tools needed to study physical problems using the new calculus of Newton and Leibniz. However, by the end of the 18th century, most of the algebraic character of coordinate geometry was subsumed by the calculus of infinitesimals of Lagrange and Euler.\n\n19th and early 20th century\nIt took the simultaneous 19th century developments of non-Euclidean geometry and Abelian integrals in order to bring the old algebraic ideas back into the geometrical fold. The first of these new developments was seized up by Edmond Laguerre and Arthur Cayley, who attempted to ascertain the generalized metric properties of projective space. Cayley introduced the idea of homogeneous polynomial forms, and more specifically quadratic forms, on projective space. Subsequently, Felix Klein studied projective geometry (along with other types of geometry) from the viewpoint that the geometry on a space is encoded in a certain class of transformations on the space. By the end of the 19th century, projective geometers were studying more general kinds of transformations on figures in projective space. Rather than the projective linear transformations which were normally regarded as giving the fundamental Kleinian geometry on projective space, they concerned themselves also with the higher degree birational transformations. This weaker notion of congruence would later lead members of the 20th century Italian school of algebraic geometry to classify algebraic surfaces up to birational isomorphism.\n\nThe second early 19th century development, that of Abelian integrals, would lead Bernhard Riemann to the development of Riemann surfaces.\n\nIn the same period began the algebraization of the algebraic geometry through commutative algebra. The prominent results in this direction are Hilbert's basis theorem and Hilbert's Nullstellensatz, which are the basis of the connexion between algebraic geometry and commutative algebra, and Macaulay's multivariate resultant, which is the basis of elimination theory. Probably because of the size of the computation which is implied by multivariate resultants, elimination theory was forgotten during the middle of the 20th century until it was renewed by singularity theory and computational algebraic geometry.A witness of this oblivion is the fact that Van der Waerden removed the chapter on elimination theory from the third edition (and all the subsequent ones) of his treatise Moderne algebra (in German).\n\n20th century\nB. L. van der Waerden, Oscar Zariski and André Weil developed a foundation for algebraic geometry based on contemporary commutative algebra, including valuation theory and the theory of ideals. One of the goals was to give a rigorous framework for proving the results of Italian school of algebraic geometry. In particular, this school used systematically the notion of generic point without any precise definition, which was first given by these authors during the 1930s.\n\nIn the 1950s and 1960s, Jean-Pierre Serre and Alexander Grothendieck recast the foundations making use of sheaf theory. Later, from about 1960, and largely led by Grothendieck, the idea of schemes was worked out, in conjunction with a very refined apparatus of homological techniques. After a decade of rapid development the field stabilized in the 1970s, and new applications were made, both to number theory and to more classical geometric questions on algebraic varieties, singularities, moduli, and formal moduli.\n\nAn important class of varieties, not easily understood directly from their defining equations, are the abelian varieties, which are the projective varieties whose points form an abelian group. The prototypical examples are the elliptic curves, which have a rich theory. They were instrumental in the proof of Fermat's last theorem and are also used in elliptic-curve cryptography.\n\nIn parallel with the abstract trend of the algebraic geometry, which is concerned with general statements about varieties, methods for effective computation with concretely-given varieties have also been developed, which lead to the new area of computational algebraic geometry. One of the founding methods of this area is the theory of Gröbner bases, introduced by Bruno Buchberger in 1965. Another founding method, more specially devoted to real algebraic geometry, is the cylindrical algebraic decomposition, introduced by George E. Collins in 1973.\n\nSee also: derived algebraic geometry.\n\nAnalytic geometry\nAn analytic variety is defined locally as the set of common solutions of several equations involving analytic functions. It is analogous to the included concept of real or complex algebraic variety. Any complex manifold is an analytic variety. Since analytic varieties may have singular points, not all analytic varieties are manifolds.\n\nModern analytic geometry is essentially equivalent to real and complex algebraic geometry, as has been shown by Jean-Pierre Serre in his paper GAGA, the name of which is French for Algebraic geometry and analytic geometry. Nevertheless, the two fields remain distinct, as the methods of proof are quite different and algebraic geometry includes also geometry in finite characteristic.\n\nApplications\nAlgebraic geometry now finds applications in statistics, control theory,Allen Tannenbaum (1982), Invariance and Systems Theory: Algebraic and Geometric Aspects, Lecture Notes in Mathematics, volume 845, Springer-Verlag,  robotics, error-correcting codes, phylogeneticsBarry Arthur Cipra (2007), Algebraic Geometers See Ideal Approach to Biology , SIAM News, Volume 40, Number 6 and geometric modelling. There are also connections to string theory, game theory, graph matchings, solitons and integer programming.\n\nSee also\n\n Algebraic statistics\n Differential geometry\n Geometric algebra\n Glossary of classical algebraic geometry\n Intersection theory\n Important publications in algebraic geometry\n List of algebraic surfaces\n Noncommutative algebraic geometry\n Diffiety theory\n Differential algebraic geometry\n Real algebraic geometry\n\nNotes\n\nFurther reading\nSome classic textbooks that predate schemes\n \n \n \n \n\nModern textbooks that do not use the language of schemes\n \n \n \n \n \n \n\nTextbooks in computational algebraic geometry\n \n \n \n \n \n \n \n\nTextbooks and references for schemes\n \n \n \n \n \n \n\nExternal links\n\n Foundations of Algebraic Geometry by Ravi Vakil, 764 pp.\n Algebraic geometry entry on PlanetMath\n English translation of the van der Waerden textbook\n  \n The Stacks Project, an open source textbook and reference work on algebraic stacks and algebraic geometry\n\n \nCategory:Fields of mathematics"
    },
    {
      "title": "Algebraic number theory",
      "url": "https://en.wikipedia.org/wiki/Algebraic_number_theory",
      "text": "thumb|Title page of the first edition of Disquisitiones Arithmeticae, one of the founding works of modern algebraic number theory.\n\nAlgebraic number theory is a branch of number theory that uses the techniques of abstract algebra to study the integers, rational numbers, and their generalizations. Number-theoretic questions are expressed in terms of properties of algebraic objects such as algebraic number fields and their rings of integers, finite fields, and function fields. These properties, such as whether a ring admits unique factorization, the behavior of ideals, and the Galois groups of fields, can resolve questions of primary importance in number theory, like the existence of solutions to Diophantine equations.\n\nHistory of algebraic number theory\n\nDiophantus\nThe beginnings of algebraic number theory can be traced to Diophantine equations,Stark, pp. 145–146. named after the 3rd-century Alexandrian mathematician, Diophantus, who studied them and developed methods for the solution of some kinds of Diophantine equations. A typical Diophantine problem is to find two integers x and y such that their sum, and the sum of their squares, equal two given numbers A and B, respectively:\n\nDiophantine equations have been studied for thousands of years.  For example, the solutions to the quadratic Diophantine equation x2 + y2 = z2 are given by the Pythagorean triples, originally solved by the Babylonians (c. 1800 BC).Aczel, pp. 14–15. Solutions to linear Diophantine equations, such as 26x + 65y = 13, may be found using the Euclidean algorithm (c. 5th century BC).Stark, pp. 44–47.\n\nDiophantus' major work was the Arithmetica, of which only a portion has survived.\n\nFermat\nFermat's last theorem was first conjectured by Pierre de Fermat in 1637, famously in the margin of a copy of Arithmetica where he claimed he had a proof that was too large to fit in the margin. No successful proof was published until 1995 despite the efforts of countless mathematicians during the 358 intervening years. The unsolved problem stimulated the development of algebraic number theory in the 19th century and the proof of the modularity theorem in the 20th century.\n\nGauss\nOne of the founding works of algebraic number theory, the Disquisitiones Arithmeticae (Latin: Arithmetical Investigations) is a textbook of number theory written in LatinDisquisitiones Arithmeticae at Yalepress.yale.edu by Carl Friedrich Gauss in 1798 when Gauss was 21 and first published in 1801 when he was 24. In this book Gauss brings together results in number theory obtained by mathematicians such as Fermat, Euler, Lagrange and Legendre and adds important new results of his own. Before the Disquisitiones was published, number theory consisted of a collection of isolated theorems and conjectures. Gauss brought the work of his predecessors together with his own original work into a systematic framework, filled in gaps, corrected unsound proofs, and extended the subject in numerous ways.\n\nThe Disquisitiones was the starting point for the work of other nineteenth century European mathematicians including Ernst Kummer, Peter Gustav Lejeune Dirichlet and Richard Dedekind. Many of the annotations given by Gauss are in effect announcements of further research of his own, some of which remained unpublished. They must have appeared particularly cryptic to his contemporaries; we can now read them as containing the germs of the theories of L-functions and complex multiplication, in particular.\n\nDirichlet\nIn a couple of papers in 1838 and 1839 Peter Gustav Lejeune Dirichlet proved the first class number formula, for quadratic forms (later refined by his student Leopold Kronecker). The formula, which Jacobi called a result \"touching the utmost of human acumen\", opened the way for similar results regarding more general number fields. Based on his research of the structure of the unit group of quadratic fields, he proved the Dirichlet unit theorem, a fundamental result in algebraic number theory.\n\nHe first used the pigeonhole principle, a basic counting argument, in the proof of a theorem in diophantine approximation, later named after him Dirichlet's approximation theorem. He published important contributions to Fermat's last theorem, for which he proved the cases n = 5 and n = 14, and to the biquadratic reciprocity law. The Dirichlet divisor problem, for which he found the first results, is still an unsolved problem in number theory despite later contributions by other researchers.\n\nDedekind\nRichard Dedekind's study of Lejeune Dirichlet's work was what led him to his later study of algebraic number fields and ideals. In 1863, he published Lejeune Dirichlet's lectures on number theory as Vorlesungen über Zahlentheorie (\"Lectures on Number Theory\") about which it has been written that:\n\"Although the book is assuredly based on Dirichlet's lectures, and although Dedekind himself referred to the book throughout his life as Dirichlet's, the book itself was entirely written by Dedekind, for the most part after Dirichlet's death.\" (Edwards 1983)\n\n1879 and 1894 editions of the Vorlesungen included supplements introducing the notion of an ideal, fundamental to ring theory. (The word \"Ring\", introduced later by Hilbert, does not appear in Dedekind's work.) Dedekind defined an ideal as a subset of a set of numbers, composed of algebraic integers that satisfy polynomial equations with integer coefficients. The concept underwent further development in the hands of Hilbert and, especially, of Emmy Noether. Ideals generalize Ernst Eduard Kummer's ideal numbers, devised as part of Kummer's 1843 attempt to prove Fermat's Last Theorem.\n\nHilbert\nDavid Hilbert unified the field of algebraic number theory with his 1897 treatise Zahlbericht (literally \"report on numbers\"). He also resolved a significant number-theory problem formulated by Waring in 1770.  As with the finiteness theorem, he used an existence proof that shows there must be solutions for the problem rather than providing a mechanism to produce the answers.Reid, Constance, 1996. Hilbert, Springer, .  He then had little more to publish on the subject; but the emergence of Hilbert modular forms in the dissertation of a student means his name is further attached to a major area.\n\nHe made a series of conjectures on class field theory. The concepts were highly influential, and his own contribution lives on in the names of the Hilbert class field and of the Hilbert symbol of local class field theory. Results were mostly proved by 1930, after work by Teiji Takagi.This work established Takagi as Japan's first mathematician of international stature.\n\nArtin\nEmil Artin established the Artin reciprocity law in a series of papers (1924; 1927; 1930). This law is a general theorem in number theory that forms a central part of global class field theory.Helmut Hasse, History of Class Field Theory, in Algebraic Number Theory, edited by Cassels and Frölich, Academic Press, 1967, pp. 266–279 The term \"reciprocity law\" refers to a long line of more concrete number theoretic statements which it generalized, from the quadratic reciprocity law and the reciprocity laws of Eisenstein and Kummer to Hilbert's product formula for the norm symbol. Artin's result provided a partial solution to Hilbert's ninth problem.\n\nModern theory\nAround 1955, Japanese mathematicians Goro Shimura and Yutaka Taniyama observed a possible link between two apparently completely distinct, branches of mathematics, elliptic curves and modular forms. The resulting modularity theorem (at the time known as the Taniyama–Shimura conjecture) states that every elliptic curve is modular, meaning that it can be associated with a unique modular form.\n\nIt was initially dismissed as unlikely or highly speculative, and was taken more seriously when number theorist André Weil found evidence supporting it, but no proof; as a result the \"astounding\"Fermat's Last Theorem, Simon Singh, 1997, > conjecture was often known as the Taniyama–Shimura-Weil conjecture. It became a part of the Langlands program, a list of important conjectures needing proof or disproof.\n\nFrom 1993 to 1994, Andrew Wiles provided a proof of the modularity theorem for semistable elliptic curves, which, together with Ribet's theorem, provided a proof for Fermat's Last Theorem. Almost every mathematician at the time had previously considered both Fermat's Last Theorem and the Modularity Theorem either impossible or virtually impossible to prove, even given the most cutting edge developments. Wiles first announced his proof in June 1993 in a version that was soon recognized as having a serious gap at a key point. The proof was corrected by Wiles, partly in collaboration with Richard Taylor, and the final, widely accepted version was released in September 1994, and formally published in 1995. The proof uses many techniques from algebraic geometry and number theory, and has many ramifications in these branches of mathematics. It also uses standard constructions of modern algebraic geometry, such as the category of schemes and Iwasawa theory, and other 20th-century techniques not available to Fermat.\n\nBasic notions\n\nFailure of unique factorization\nAn important property of the ring of integers is that it satisfies the fundamental theorem of arithmetic, that every (positive) integer has a factorization into a product of prime numbers, and this factorization is unique up to the ordering of the factors. This may no longer be true in the ring of integers  of an algebraic number field .\n\nA prime element is an element  of  such that if  divides a product , then it divides one of the factors  or . This property is closely related to primality in the integers, because any positive integer satisfying this property is either  or a prime number. However, it is strictly weaker.  For example,  is not a prime number because it is negative, but it is a prime element. If factorizations into prime elements are permitted, then, even in the integers, there are alternative factorizations such as\n\nIn general, if  is a unit, meaning a number with a multiplicative inverse in , and if  is a prime element, then  is also a prime element. Numbers such as  and  are said to be associate. In the integers, the primes  and  are associate, but only one of these is positive. Requiring that prime numbers be positive selects a unique element from among a set of associated prime elements. When K is not the rational numbers, however, there is no analog of positivity. For example, in the Gaussian integers , the numbers  and  are associate because the latter is the product of the former by , but there is no way to single out one as being more canonical than the other. This leads to equations such as\n\nwhich prove that in , it is not true that factorizations are unique up to the order of the factors. For this reason, one adopts the definition of unique factorization used in unique factorization domains (UFDs). In a UFD, the prime elements occurring in a factorization are only expected to be unique up to units and their ordering.\n\nHowever, even with this weaker definition, many rings of integers in algebraic number fields do not admit unique factorization. There is an algebraic obstruction called the ideal class group. When the ideal class group is trivial, the ring is a UFD. When it is not, there is a distinction between a prime element and an irreducible element. An irreducible element  is an element such that if , then either  or  is a unit. These are the elements that cannot be factored any further. Every element in O admits a factorization into irreducible elements, but it may admit more than one. This is because, while all prime elements are irreducible, some irreducible elements may not be prime. For example, consider the ring . In this ring, the numbers ,  and  are irreducible. This means that the number  has two factorizations into irreducible elements,\n\nThis equation shows that  divides the product . If  were a prime element, then it would divide  or , but it does not, because all elements divisible by  are of the form .  Similarly,  and  divide the product , but neither of these elements divides  itself, so neither of them are prime. As there is no sense in which the elements ,  and  can be made equivalent, unique factorization fails in . Unlike the situation with units, where uniqueness could be repaired by weakening the definition, overcoming this failure requires a new perspective.\n\nFactorization into prime ideals\nIf  is an ideal in , then there is always a factorization\n\nwhere each  is a prime ideal, and where this expression is unique up to the order of the factors. In particular, this is true if  is the principal ideal generated by a single element. This is the strongest sense in which the ring of integers of a general number field admits unique factorization. In the language of ring theory, it says that rings of integers are Dedekind domains.\n\nWhen  is a UFD, every prime ideal is generated by a prime element. Otherwise, there are prime ideals which are not generated by prime elements. In , for instance, the ideal  is a prime ideal which cannot be generated by a single element.\n\nHistorically, the idea of factoring ideals into prime ideals was preceded by Ernst Kummer's introduction of ideal numbers. These are numbers lying in an extension field  of . This extension field is now known as the Hilbert class field. By the principal ideal theorem, every prime ideal of  generates a principal ideal of the ring of integers of . A generator of this principal ideal is called an ideal number. Kummer used these as a substitute for the failure of unique factorization in cyclotomic fields. These eventually led Richard Dedekind to introduce a forerunner of ideals and to prove unique factorization of ideals.\n\nAn ideal which is prime in the ring of integers in one number field may fail to be prime when extended to a larger number field. Consider, for example, the prime numbers. The corresponding ideals  are prime ideals of the ring . However, when this ideal is extended to the Gaussian integers to get , it may or may not be prime. For example, the factorization  implies that\n\nnote that because , the ideals generated by  and  are the same. A complete answer to the question of which ideals remain prime in the Gaussian integers is provided by Fermat's theorem on sums of two squares.  It implies that for an odd prime number ,  is a prime ideal if  and is not a prime ideal if . This, together with the observation that the ideal  is prime, provides a complete description of the prime ideals in the Gaussian integers. Generalizing this simple result to more general rings of integers is a basic problem in algebraic number theory. Class field theory accomplishes this goal when K is an abelian extension of Q (i.e. a Galois extension with abelian Galois group).\n\nIdeal class group\nUnique factorization fails if and only if there are prime ideals that fail to be principal. The object which measures the failure of prime ideals to be principal is called the ideal class group. Defining the ideal class group requires enlarging the set of ideals in a ring of algebraic integers so that they admit a group structure. This is done by generalizing ideals to fractional ideals. A fractional ideal is an additive subgroup  of  which is closed under multiplication by elements of , meaning that  if . All ideals of  are also fractional ideals. If  and  are fractional ideals, then the set  of all products of an element in  and an element in  is also a fractional ideal. This operation makes the set of non-zero fractional ideals into a group. The group identity is the ideal , and the inverse of  is a (generalized) ideal quotient, .\n\nThe principal fractional ideals, meaning the ones of the form  where , form a subgroup of the group of all non-zero fractional ideals. The quotient of the group of non-zero fractional ideals by this subgroup is the ideal class group. Two fractional ideals  and  represent the same element of the ideal class group if and only if there exists an element  such that . Therefore, the ideal class group makes two fractional ideals equivalent if one is as close to being principal as the other is. The ideal class group is generally denoted , , or  (with the last notation identifying it with the Picard group in algebraic geometry).\n\nThe number of elements in the class group is called the class number of K. The class number of  is 2.  This means that there are only two ideal classes, the class of principal fractional ideals, and the class of a non-principal fractional ideal such as .\n\nThe ideal class group has another description in terms of divisors. These are formal objects which represent possible factorizations of numbers. The divisor group  is defined to be the free abelian group generated by the prime ideals of . There is a group homomorphism from , the non-zero elements of  up to multiplication, to . Suppose that  satisfies\n\nThen  is defined to be the divisor\n\nThe kernel of  is the group of units in , while the cokernel is the ideal class group. In the language of homological algebra, this says that there is an exact sequence of abelian groups (written multiplicatively),\n\nReal and complex embeddings\nSome number fields, such as , can be specified as subfields of the real numbers. Others, such as , cannot. Abstractly, such a specification corresponds to a field homomorphism  or . These are called real embeddings and complex embeddings, respectively.\n\nA real quadratic field , with , and  not a perfect square, is so-called because it admits two real embeddings but no complex embeddings. These are the field homomorphisms which send  to  and to , respectively. Dually, an imaginary quadratic field  admits no real embeddings but admits a conjugate pair of complex embeddings. One of these embeddings sends  to , while the other sends it to its complex conjugate, .\n\nConventionally, the number of real embeddings of  is denoted , while the number of conjugate pairs of complex embeddings is denoted .  The signature of K is the pair . It is a theorem that , where  is the degree of .\n\nConsidering all embeddings at once determines a function\n\nThis is called the Minkowski embedding. The subspace of the codomain fixed by complex conjugation is a real vector space of dimension  called Minkowski space. Because the Minkowski embedding is defined by field homomorphisms, multiplication of elements of  by an element  corresponds to multiplication by a diagonal matrix in the Minkowski embedding. The dot product on Minkowski space corresponds to the trace form .\n\nThe image of  under the Minkowski embedding is a -dimensional lattice. If  is a basis for this lattice, then  is the discriminant of . The discriminant is denoted  or . The covolume of the image of  is .\n\nPlaces\nReal and complex embeddings can be put on the same footing as prime ideals by adopting a perspective based on valuations. Consider, for example, the integers. In addition to the usual absolute value function |·| : Q → R, there are p-adic absolute value functions |·|p : Q → R, defined for each prime number p, which measure divisibility by p. Ostrowski's theorem states that these are all possible absolute value functions on Q (up to equivalence). Therefore, absolute values are a common language to describe both the real embedding of Q and the prime numbers.\n\nA place of an algebraic number field is an equivalence class of absolute value functions on K. There are two types of places. There is a -adic absolute value for each prime ideal  of O, and, like the p-adic absolute values, it measures divisibility. These are called finite places.  The other type of place is specified using a real or complex embedding of K and the standard absolute value function on R or C. These are infinite places. Because absolute values are unable to distinguish between a complex embedding and its conjugate, a complex embedding and its conjugate determine the same place. Therefore, there are  real places and  complex places. Because places encompass the primes, places are sometimes referred to as primes.  When this is done, finite places are called finite primes and infinite places are called infinite primes. If  is a valuation corresponding to an absolute value, then one frequently writes  to mean that  is an infinite place and  to mean that it is a finite place.\n\nConsidering all the places of the field together produces the adele ring of the number field. The adele ring allows one to simultaneously track all the data available using absolute values. This produces significant advantages in situations where the behavior at one place can affect the behavior at other places, as in the Artin reciprocity law.\n\nUnits\nThe integers have only two units,  and . Other rings of integers may admit more units. The Gaussian integers have four units, the previous two as well as . The Eisenstein integers  have six units. The integers in real quadratic number fields have infinitely many units. For example, in , every power of  is a unit, and all these powers are distinct.\n\nIn general, the group of units of , denoted , is a finitely generated abelian group. The fundamental theorem of finitely generated abelian groups therefore implies that it is a direct sum of a torsion part and a free part. Reinterpreting this in the context of a number field, the torsion part consists of the roots of unity that lie in . This group is cyclic. The free part is described by Dirichlet's unit theorem. This theorem says that rank of the free part is . Thus, for example, the only fields for which the rank of the free part is zero are  and the imaginary quadratic fields. A more precise statement giving the structure of O× ⊗Z Q as a Galois module for the Galois group of K/Q is also possible.See proposition VIII.8.6.11 of \n\nThe free part of the unit group can be studied using the infinite places of . Consider the function\n\ndefined by\n\nwhere  varies over the infinite places of  and |·|v is the absolute value associated with . The function  is a homomorphism from  to a real vector space. It can be shown that the image of  is a lattice that spans the hyperplane defined by . The covolume of this lattice is the regulator of the number field. One of the simplifications made possible by working with the adele ring is that there is a single object, the idele class group, that describes both the quotient by this lattice and the ideal class group.\n\nZeta function\nThe Dedekind zeta function of a number field, analogous to the Riemann zeta function is an analytic object which describes the behavior of prime ideals in . When  is an abelian extension of , Dedekind zeta functions are products of Dirichlet L-functions, with there being one factor for each Dirichlet character.  The trivial character corresponds to the Riemann zeta function. When  is a Galois extension, the Dedekind zeta function is the Artin L-function of the regular representation of the Galois group of , and it has a factorization in terms of irreducible Artin representations of the Galois group.\n\nThe zeta function is related to the other invariants described above by the class number formula.\n\nLocal fields\n\nCompleting a number field K at a place w gives a complete field. If the valuation is archimedean, one gets R or C, if it is non-archimedean and lies over a prime p of the rationals, one gets a finite extension Kw / Qp: a complete, discrete valued field with finite residue field. This process simplifies the arithmetic of the field and allows the local study of problems. For example, the Kronecker–Weber theorem can be deduced easily from the analogous local statement. The philosophy behind the study of local fields is largely motivated by geometric methods. In algebraic geometry, it is common to study varieties locally at a point by localizing to a maximal ideal. Global information can then be recovered by gluing together local data. This spirit is adopted in algebraic number theory. Given a prime in the ring of algebraic integers in a number field, it is desirable to study the field locally at that prime. Therefore, one localizes the ring of algebraic integers to that prime and then completes the fraction field much in the spirit of geometry.\n\nMajor results\n\nFiniteness of the class group\nOne of the classical results in algebraic number theory is that the ideal class group of an algebraic number field K is finite. The order of the class group is called the class number, and is often denoted by the letter h.\n\nDirichlet's unit theorem\n\nDirichlet's unit theorem provides a description of the structure of the multiplicative group of units O× of the ring of integers O. Specifically, it states that O× is isomorphic to G × Zr, where G is the finite cyclic group consisting of all the roots of unity in O, and r = r1 + r2 − 1 (where r1 (respectively, r2) denotes the number of real embeddings (respectively, pairs of conjugate non-real embeddings) of K). In other words, O× is a finitely generated abelian group of rank r1 + r2 − 1 whose torsion consists of the roots of unity in O.\n\nReciprocity laws\n\nIn terms of the Legendre symbol, the law of quadratic reciprocity for positive odd primes states\n\nA reciprocity law is a generalization of the law of quadratic reciprocity.\n\nThere are several different ways to express reciprocity laws. The early reciprocity laws found in the 19th century were usually expressed in terms of a power residue symbol (p/q) generalizing the quadratic reciprocity symbol,  that describes when a prime number is an nth power residue modulo another prime, and gave a relation between (p/q) and (q/p).  Hilbert reformulated the reciprocity laws as saying that a product over p of Hilbert symbols (a,b/p), taking values in roots of unity, is equal to 1. Artin's reformulated reciprocity law states that the Artin symbol from ideals (or ideles) to elements of a Galois group is trivial on a certain subgroup. Several more recent generalizations express reciprocity laws using cohomology of groups or representations of adelic groups or algebraic K-groups, and their relationship with the original quadratic reciprocity law can be hard to see.\n\nClass number formula\n\nThe class number formula relates many important invariants of a number field to a special value of its Dedekind zeta function.\n\nRelated areas\nAlgebraic number theory interacts with many other mathematical disciplines. It uses tools from homological algebra. Via the analogy of function fields vs. number fields, it relies on techniques and ideas from algebraic geometry. Moreover, the study of higher-dimensional schemes over Z instead of number rings is referred to as arithmetic geometry. Algebraic number theory is also used in the study of arithmetic hyperbolic 3-manifolds.\n\nSee also\nTamagawa number\nKummer theory\n\nNotes\n\n \n\nFurther reading\n\nIntroductory texts\n Stein, William (2012). Algebraic Number Theory, A Computational Approach. Retrieved from https://wstein.org/books/ant/ant.pdf\n Ireland, Kenneth and Rosen, Michael (2013). A classical introduction to modern number theory (Vol. 84). Springer Science & Business Media. \n Stewart, Ian and Tall, David (2015). Algebraic number theory and Fermat's last theorem. CRC Press.\n\nIntermediate texts\n Marcus, Daniel A. (1977). Number fields (Vol. 8). New York: Springer.\n\nGraduate level texts\n\nExternal links\n\n \nCategory:Fields of mathematics"
    },
    {
      "title": "Arithmetic geometry",
      "url": "https://en.wikipedia.org/wiki/Arithmetic_geometry",
      "text": "thumb|The hyperelliptic curve defined by  has only finitely many rational points (such as the points  and ) by Faltings's theorem.\n\nIn mathematics, arithmetic geometry is roughly the application of techniques from algebraic geometry to problems in number theory. Arithmetic geometry is centered around Diophantine geometry, the study of rational points of algebraic varieties.\n\nIn more abstract terms, arithmetic geometry can be defined as the study of schemes of finite type over the spectrum of the ring of integers.\n\nOverview\nThe classical objects of interest in arithmetic geometry are rational points: sets of solutions of a system of polynomial equations over number fields, finite fields, p-adic fields, or function fields, i.e. fields that are not algebraically closed excluding the real numbers. Rational points can be directly characterized by height functions which measure their arithmetic complexity.\n\nThe structure of algebraic varieties defined over non-algebraically-closed fields has become a central area of interest that arose with the modern abstract development of algebraic geometry. Over finite fields, étale cohomology provides topological invariants associated to algebraic varieties. p-adic Hodge theory gives tools to examine when cohomological properties of varieties over the complex numbers extend to those over p-adic fields.\n\nHistory\n19th century: early arithmetic geometry\nIn the early 19th century, Carl Friedrich Gauss observed that non-zero integer solutions to homogeneous polynomial equations with rational coefficients exist if non-zero rational solutions exist.\n\nIn the 1850s, Leopold Kronecker formulated the Kronecker–Weber theorem, introduced the theory of divisors, and made numerous other connections between number theory and algebra. He then conjectured his \"liebster Jugendtraum\" (\"dearest dream of youth\"), a generalization that was later put forward by Hilbert in a modified form as his twelfth problem, which outlines a goal to have number theory operate only with rings that are quotients of polynomial rings over the integers.\n\nEarly-to-mid 20th century: algebraic developments and the Weil conjectures\nIn the late 1920s, André Weil demonstrated profound connections between algebraic geometry and number theory with his doctoral work leading to the Mordell–Weil theorem which demonstrates that the set of rational points of an abelian variety is a finitely generated abelian group.A. Weil, L'arithmétique sur les courbes algébriques, Acta Math 52, (1929) p. 281-315, reprinted in vol 1 of his collected papers .\n\nModern foundations of algebraic geometry were developed based on contemporary commutative algebra, including valuation theory and the theory of ideals by Oscar Zariski and others in the 1930s and 1940s.\n\nIn 1949, André Weil posed the landmark Weil conjectures about the local zeta-functions of algebraic varieties over finite fields. Reprinted in Oeuvres Scientifiques/Collected Papers by André Weil  These conjectures offered a framework between algebraic geometry and number theory that propelled Alexander Grothendieck to recast the foundations making use of sheaf theory (together with Jean-Pierre Serre), and later scheme theory, in the 1950s and 1960s. Bernard Dwork proved one of the four Weil conjectures (rationality of the local zeta function) in 1960. Grothendieck developed étale cohomology theory to prove two of the Weil conjectures (together with Michael Artin and Jean-Louis Verdier) by 1965. The last of the Weil conjectures (an analogue of the Riemann hypothesis) would be finally proven in 1974 by Pierre Deligne.\n\nMid-to-late 20th century: developments in modularity, p-adic methods, and beyond\nBetween 1956 and 1957, Yutaka Taniyama and Goro Shimura posed the Taniyama–Shimura conjecture (now known as the modularity theorem) relating elliptic curves to modular forms. This connection would ultimately lead to the first proof of Fermat's Last Theorem in number theory through algebraic geometry techniques of modularity lifting developed by Andrew Wiles in 1995.\n\nIn the 1960s, Goro Shimura introduced Shimura varieties as generalizations of modular curves. Since the 1979, Shimura varieties have played a crucial role in the Langlands program as a natural realm of examples for testing conjectures.\n\nIn papers in 1977 and 1978, Barry Mazur proved the torsion conjecture giving a complete list of the possible torsion subgroups of elliptic curves over the rational numbers. Mazur's first proof of this theorem depended upon a complete analysis of the rational points on certain modular curves. In 1996, the proof of the torsion conjecture was extended to all number fields by Loïc Merel.\n\nIn 1983, Gerd Faltings proved the Mordell conjecture, demonstrating that a curve of genus greater than 1 has only finitely many rational points (where the Mordell–Weil theorem only demonstrates finite generation of the set of rational points as opposed to finiteness).\n\nIn 2001, the proof of the local Langlands conjectures for GLn was based on the geometry of certain Shimura varieties.\n\nIn the 2010s, Peter Scholze developed perfectoid spaces and new cohomology theories in arithmetic geometry over p-adic fields with application to Galois representations and certain cases of the weight-monodromy conjecture.\n\nSee also\nArithmetic of abelian varieties\nSiegel's theorem on integral points\n\nReferences\n\nCategory:Algebraic geometry\nCategory:Fields of mathematics"
    },
    {
      "title": "Diophantine geometry",
      "url": "https://en.wikipedia.org/wiki/Diophantine_geometry",
      "text": "In mathematics, Diophantine geometry is the study of points of algebraic varieties with coordinates in the integers, rational numbers, and their generalizations. These generalizations typically are fields that are not algebraically closed, such as number fields, finite fields, function fields, and p-adic fields (but not the real numbers which are used in real algebraic geometry). It is a sub-branch of arithmetic geometry and is one approach to the theory of Diophantine equations, formulating questions about such equations in terms of algebraic geometry.\n\nA single equation defines a hypersurface, and simultaneous Diophantine equations give rise to a general algebraic variety V over K; the typical question is about the nature of the set V(K) of points on V with co-ordinates in K, and by means of height functions quantitative questions about the \"size\" of these solutions may be posed, as well as the qualitative issues of whether any points exist, and if so whether there are an infinite number. Given the geometric approach, the consideration of homogeneous equations and homogeneous co-ordinates is fundamental, for the same reasons that projective geometry is the dominant approach in algebraic geometry. Rational number solutions therefore are the primary consideration; but integral solutions (i.e. lattice points) can be treated in the same way as an affine variety may be considered inside a projective variety that has extra points at infinity.\n\nThe general approach of Diophantine geometry is illustrated by Faltings's theorem (a conjecture of L. J. Mordell) stating that an algebraic curve C of genus g > 1 over the rational numbers has only finitely many rational points. The first result of this kind may have been the theorem of Hilbert and Hurwitz dealing with the case g = 0. The theory consists both of theorems and many conjectures and open questions.\n\nBackground\nSerge Lang published a book Diophantine Geometry in the area, in 1962. The traditional arrangement of material on Diophantine equations was by degree and number of variables, as in Mordell's Diophantine Equations (1969). Mordell's book starts with a remark on homogeneous equations f = 0 over the rational field, attributed to C. F. Gauss, that non-zero solutions in integers (even primitive lattice points) exist if non-zero rational solutions do, and notes a caveat of L. E. Dickson, which is about parametric solutions. The Hilbert–Hurwitz result from 1890 reducing the Diophantine geometry of curves of genus 0 to degrees 1 and 2 (conic sections) occurs in Chapter 17, as does Mordell's conjecture. Siegel's theorem on integral points occurs in Chapter 28. Mordell's theorem on the finite generation of the group of rational points on an elliptic curve is in Chapter 16, and integer points on the Mordell curve in Chapter 26.\n\nIn a hostile review of Lang's book, Mordell wrote\n\nHe notes that the content of the book is largely versions of the Mordell–Weil theorem, Thue–Siegel–Roth theorem, Siegel's theorem, with a treatment of Hilbert's irreducibility theorem and applications (in the style of Siegel). Leaving aside issues of generality, and a completely different style, the major mathematical difference between the two books is that Lang used abelian varieties and offered a proof of Siegel's theorem, while Mordell noted that the proof \"is of a very advanced character\" (p. 263).\n\nDespite a bad press initially, Lang's conception has been sufficiently widely accepted for a 2006 tribute to call the book \"visionary\". A larger field sometimes called arithmetic of abelian varieties now includes Diophantine geometry along with class field theory, complex multiplication, local zeta-functions and L-functions. Paul Vojta wrote:\n\nWhile others at the time shared this viewpoint (e.g., Weil, Tate, Serre), it is easy to forget that others did not, as Mordell's review of Diophantine Geometry attests.\n\nSee also\nGlossary of arithmetic and Diophantine geometry\n\nReferences\n\nNotes\n\nFurther reading\n \n \n \n \n\nExternal links\nLang's review of Mordell's Diophantine Equations\nMordell's review of Lang's Diophantine Geometry\n\nCategory:Diophantine geometry\nCategory:Fields of mathematics"
    },
    {
      "title": "Mathematics Subject Classification",
      "url": "https://en.wikipedia.org/wiki/Mathematics_Subject_Classification",
      "text": "The Mathematics Subject Classification (MSC) is an alphanumerical classification scheme collaboratively produced by staff of, and based on the coverage of, the two major mathematical reviewing databases, Mathematical Reviews and Zentralblatt MATH. The MSC is used by many mathematics journals, which ask authors of research papers and expository articles to list subject codes from the Mathematics Subject Classification in their papers. The current version is MSC2010.\n\n Structure \n\nThe MSC is a hierarchical scheme, with three levels of structure. A classification can be two, three or five digits long, depending on how many levels of the classification scheme are used.\n\nThe first level is represented by a two-digit number, the second by a letter, and the third by another two-digit number. For example:\n\n 53 is the classification for differential geometry\n 53A is the classification for classical differential geometry\n 53A45 is the classification for vector and tensor analysis\n\n First level \nAt the top level, 64 mathematical disciplines are labeled with a unique two-digit number. As well as the typical areas of mathematical research, there are top-level categories for \"History and Biography\", \"Mathematics Education\", and for the overlap with different sciences. Physics (i.e. mathematical physics) is particularly well represented in the classification scheme with a number of different categories including:\n Fluid mechanics\n Quantum mechanics\n Geophysics\n Optics and electromagnetic theory\n\nAll valid MSC classification codes must have at least the first-level identifier.\n\n Second level \n\nThe second-level codes are a single letter from the Latin alphabet. These represent specific areas covered by the first-level discipline. The second-level codes vary from discipline to discipline.\n\nFor example, for differential geometry, the top-level code is 53, and the second-level codes are:\n A for classical differential geometry\n B for local differential geometry\n C for global differential geometry\n D for symplectic geometry and contact geometry\n\nIn addition, the special second-level code \"-\" is used for specific kinds of materials. These codes are of the form:\n\n 53-00 General reference works (handbooks, dictionaries, bibliographies, etc.)\n 53-01 Instructional exposition (textbooks, tutorial papers, etc.)\n 53-02 Research exposition (monographs, survey articles)\n 53-03 Historical (must also be assigned at least one classification number from Section 01)\n 53-04 Explicit machine computation and programs (not the theory of computation or programming)\n 53-06 Proceedings, conferences, collections, etc.\n\nThe second and third level of these codes are always the same - only the first level changes. For example, it is not valid to use 53- as a classification. Either 53 on its own or, better yet, a more specific code should be used.\n\n Third level \n\nThird-level codes are the most specific, usually corresponding to a specific kind of mathematical object or a well-known problem or research area.\n\nThe third-level code 99 exists in every category and means none of the above, but in this section.\n\n Using the scheme \n\nThe AMS recommends that papers submitted to its journals for publication have one primary classification and one or more optional secondary classifications. A typical MSC subject class line on a research paper looks like\n\nMSC Primary 03C90; Secondary 03-02;\n\nHistory\n\nAccording to the American Mathematical Society (AMS) help page about MSC, the MSC has been revised a number of times since 1940. Based on a scheme to organize AMS's Mathematical Offprint Service (MOS scheme), the AMS Classification was established for the classification of reviews in Mathematical Reviews in the 1960s. It saw various ad-hoc changes. Despite its shortcomings, Zentralblatt für Mathematik started to use it as well in the 1970s. In the late 1980s, a jointly revised scheme with more formal rules was agreed upon by Mathematical Reviews and Zentralblatt für Mathematik under the new name Mathematics Subject Classification. It saw various revisions as MSC1990, MSC2000 and MSC2010.Bernd Wegner. Indexierung mathematischer Literatur Die Revision der Mathematics Subject Classification MSC. Institute of Mathematics, TU Berlin. http://fidmath.de/fileadmin/download/graz_wegner.ppt In July 2016, Mathematical Reviews and zbMATH started collecting input from the mathematical community on the next revision of MSC, which is due to be released in 2020.Announcement of the plan to revise the Mathematics Subject Classification The original classification of older items has not been changed. This can sometimes make it difficult to search for older works dealing with particular topics. Changes at the first level involved the subjects with (present) codes 03, 08, 12-20, 28, 37, 51, 58, 74, 90, 91, 92.\n\n Relation to other classification schemes \n\nFor physics papers the Physics and Astronomy Classification Scheme (PACS) is often used. Due to the large overlap between mathematics and physics research it is quite common to see both PACS and MSC codes on research papers, particularly for multidisciplinary journals and repositories such as the arXiv.\n\nThe ACM Computing Classification System (CCS) is a similar hierarchical classification scheme for computer science. There is some overlap between the AMS and ACM classification schemes, in subjects related to both mathematics and computer science, however the two schemes differ in the details of their organization of those topics.\n\nThe classification scheme used on the arXiv is chosen to reflect the papers submitted. As arXiv is multidisciplinary its classification scheme does not fit entirely with the MSC, ACM or PACS classification schemes. It is common to see codes from one or more of these schemes on individual papers.\n\nFirst-level areas\nThe top-level subjects under the MSC are, grouped here by common area names that are not part of the MSC:\n\nGeneral/foundations [Study of foundations of mathematics and logic]\n00: General (Includes topics such as recreational mathematics, philosophy of mathematics and mathematical modeling.)\n01: History and biography\n03: Mathematical logic and foundations (including model theory, computability theory, set theory, proof theory, and algebraic logic)\n\nDiscrete mathematics/algebra [Study of structure of mathematical abstractions]\n05: Combinatorics\n06: Order, lattices, ordered algebraic structures\n08: General algebraic systems\n11: Number theory\n12: Field theory and polynomials \n13: Commutative algebra (Commutative rings and algebras)\n14: Algebraic geometry\n15: Linear and multilinear algebra; matrix theory \n16: Associative rings and (associative) algebras\n17: Non-associative rings and (non-associative) algebras\n18: Category theory; homological algebra\n19: -theory\n20: Group theory and generalizations \n22: Topological groups, Lie groups (and analysis upon them)\n\nAnalysis [Study of change and quantity]\n26: Real functions (including derivatives and integrals)\n28: Measure and integration\n30: Functions of a complex variable (including approximation theory in the complex domain)\n31: Potential theory \n32: Several complex variables and analytic spaces\n33: Special functions\n34: Ordinary differential equations \n35: Partial differential equations \n37: Dynamical systems and ergodic theory \n39: Difference (equations) and functional equations \n40: Sequences, series, summability \n41: Approximations and expansions\n42: Harmonic analysis on Euclidean spaces (including Fourier analysis, Fourier transforms, trigonometric approximation, trigonometric interpolation, and orthogonal functions)\n43: Abstract harmonic analysis \n44: Integral transforms, operational calculus \n45: Integral equations   \n46: Functional analysis (including infinite-dimensional holomorphy, integral transforms in distribution spaces)\n47: Operator theory \n49: Calculus of variations and optimal control; optimization (including geometric integration theory)\n\nGeometry and topology [Study of space]\n51: Geometry \n52: Convex (geometry) and discrete geometry\n53: Differential geometry \n54: General topology \n55: Algebraic topology \n57: Manifolds and cell complexes\n58: Global analysis, analysis on manifolds (including infinite-dimensional holomorphy)\n\nApplied mathematics / other [Study of applications of mathematical abstractions]\n60: Probability theory and stochastic processes\n62: Statistics\n65: Numerical analysis\n68: Computer science\n70: Mechanics of particles and systems (including particle mechanics)\n74: Mechanics of deformable solids \n76: Fluid mechanics\n78: Optics, electromagnetic theory \n80: Classical thermodynamics, heat transfer\n81: Quantum theory\n82: Statistical mechanics, structure of matter\n83: Relativity and gravitational theory (including relativistic mechanics) \n85: Astronomy and astrophysics\n86: Geophysics\n90: Operations research, mathematical programming \n91: Game theory, economics, social and behavioral sciences\n92: Biology and other natural sciences\n93: Systems theory; control (including optimal control)\n94: Information and communication, circuits\n97: Mathematics education\n\nSee also\n\n Areas of mathematics\n Mathematical knowledge management\n MathSciNet\n\nReferences\n\nExternal links\nMathematics Subject Classification 2010 The site where the MSC2010 revision was carried out publicly in an MSCwiki.  A view of the whole scheme and the changes made from MSC2000, as well as PDF files of the MSC and ancillary documents are there.  A personal copy of the MSC in TiddlyWiki form can be had also.\nThe American Mathematical Society page on the Mathematics Subject Classification.\nThe Zentralblatt MATH page on the Mathematics Subject Classification.\n\nCategory:Fields of mathematics\nCategory:Classification systems"
    },
    {
      "title": "Physical mathematics",
      "url": "https://en.wikipedia.org/wiki/Physical_mathematics",
      "text": "The subject of physical mathematics is concerned with physically motivated mathematics and is different from mathematical physics. The Journal of Physical Mathematics is an important journal in the field.\n\nString theorist Greg Moore said this about physical mathematics in his vision talk at Strings 2014.\n\nSee also\n Theoretical physics\n\nReferences\n\nEric Zaslow, Physmatics, \nArthur Jaffe, Frank Quinn, ``Theoretical mathematics``: Toward a cultural synthesis of mathematics and theoretical physics, Bull. Am. Math. Soc. 30: 178-207, 1994, \nMichael Atiyah et al., Responses to ``Theoretical Mathematics: Toward a cultural synthesis of mathematics and theoretical physics``, by A. Jaffe and F. Quinn, Bull. Am. Math. Soc. 30: 178-207, 1994, \nMichael Stöltzner, Theoretical Mathematics: On the Philosophical Significance of the Jaffe-Quinn Debate, in: The Role of Mathematics in Physical Sciences pp 197-222, \n\nCategory:Fields of mathematics\nCategory:Mathematical physics"
    },
    {
      "title": "Pure mathematics",
      "url": "https://en.wikipedia.org/wiki/Pure_mathematics",
      "text": "thumb|251x251px|Pure mathematics studies the properties and structure of abstract objects, such as the E8 group, in group theory. This may be done without focusing on concrete applications of the concepts in the physical world\nPure mathematics is the study of mathematical concepts independently of any application outside mathematics. These concepts may originate in real-world concerns, and the results obtained may later turn out to be useful for practical applications, but the pure mathematicians are not primarily motivated by such applications. Instead, the appeal is attributed to the intellectual challenge and aesthetic beauty of working out the logical consequences of basic principles.\n\nWhile pure mathematics has existed as an activity since at least Ancient Greece, the concept was elaborated upon around the year 1900, after the introduction of theories with counter-intuitive properties (such as non-Euclidean geometries and Cantor's theory of infinite sets), and the discovery of apparent paradoxes (such as continuous functions that are nowhere differentiable, and Russell's paradox). This introduced the need of renewing the concept of mathematical rigor and rewriting all mathematics accordingly, with a systematic use of axiomatic methods. This led many mathematicians to focus on mathematics for its own sake, that is, pure mathematics.\n\nNevertheless, almost all mathematical theories remained motivated by problems coming from the real world or from less abstract mathematical theories. Also, many mathematical theories, which had seemed to be totally pure mathematics, were eventually used in applied areas, mainly physics and computer science. A famous early example is Isaac Newton's demonstration that his law of universal gravitation implied that planets move in orbits that are conic sections, geometrical curves that had been studied in antiquity by Apollonius. Another example is the problem of factoring large integers, which is the basis of the RSA cryptosystem, widely used to secure internet communications.\n\nIt follows that, presently, the distinction between pure and applied mathematics is more a philosophical point of view or a mathematician's preference than a rigid subdivision of mathematics. In particular, it is not uncommon that some members of a department of applied mathematics describe themselves as pure mathematicians.\n\nHistory\n\nAncient Greece\nAncient Greek mathematicians were among the earliest to make a distinction between pure and applied mathematics. Plato helped to create the gap between \"arithmetic\", now called number theory, and \"logistic\", now called arithmetic. Plato regarded logistic (arithmetic) as appropriate for businessmen and men of war who \"must learn the art of numbers or [they] will not know how to array [their] troops\" and arithmetic (number theory) as appropriate for philosophers \"because [they have] to arise out of the sea of change and lay hold of true being.\" Euclid of Alexandria, when asked by one of his students of what use was the study of geometry, asked his slave to give the student threepence, \"since he must make gain of what he learns.\" The Greek mathematician Apollonius of Perga was asked about the usefulness of some of his theorems in Book IV of Conics to which he proudly asserted,\nThey are worthy of acceptance for the sake of the demonstrations themselves, in the same way as we accept many other things in mathematics for this and for no other reason.\nAnd since many of his results were not applicable to the science or engineering of his day, Apollonius further argued in the preface of the fifth book of Conics that the subject is one of those that \"...seem worthy of study for their own sake.\"\n\n19th century\nThe term itself is enshrined in the full title of the Sadleirian Chair, Sadleirian Professor of Pure Mathematics, founded (as a professorship) in the mid-nineteenth century. The idea of a separate discipline of pure mathematics may have emerged at that time. The generation of Gauss made no sweeping distinction of the kind, between pure and applied. In the following years, specialisation and professionalisation (particularly in the Weierstrass approach to mathematical analysis) started to make a rift more apparent.\n\n20th century\nAt the start of the twentieth century mathematicians took up the axiomatic method, strongly influenced by David Hilbert's example. The logical formulation of pure mathematics suggested by Bertrand Russell in terms of a quantifier structure of propositions seemed more and more plausible, as large parts of mathematics became axiomatised and thus subject to the simple criteria of rigorous proof.\n\nPure mathematics, according to a view that can be ascribed to the Bourbaki group, is what is proved. Pure mathematician became a recognized vocation, achievable through training.\n\nThe case was made that pure mathematics is useful in engineering education:A. S. Hathaway (1901) \"Pure mathematics for engineering students\", Bulletin of the American Mathematical Society 7(6):266–71.\n\nThere is a training in habits of thought, points of view, and intellectual comprehension of ordinary engineering problems, which only the study of higher mathematics can give.\n\nGenerality and abstraction\nthumbnail|right|350px|An illustration of the Banach–Tarski paradox, a famous result in pure mathematics. Although it is proven that it is possible to convert one sphere into two using nothing but cuts and rotations, the transformation involves objects that cannot exist in the physical world.\nOne central concept in pure mathematics is the idea of generality; pure mathematics often exhibits a trend towards increased generality. Uses and advantages of generality include the following:\n\n Generalizing theorems or mathematical structures can lead to deeper understanding of the original theorems or structures\n Generality can simplify the presentation of material, resulting in shorter proofs or arguments that are easier to follow.\n One can use generality to avoid duplication of effort, proving a general result instead of having to prove separate cases independently, or using results from other areas of mathematics.\n Generality can facilitate connections between different branches of mathematics. Category theory is one area of mathematics dedicated to exploring this commonality of structure as it plays out in some areas of math.\n\nGenerality's impact on intuition is both dependent on the subject and a matter of personal preference or learning style.  Often generality is seen as a hindrance to intuition, although it can certainly function as an aid to it, especially when it provides analogies to material for which one already has good intuition.\n\nAs a prime example of generality, the Erlangen program involved an expansion of geometry to accommodate non-Euclidean geometries as well as the field of topology, and other forms of geometry, by viewing geometry as the study of a space together with a group of transformations.  The study of numbers, called algebra at the beginning undergraduate level, extends to abstract algebra at a more advanced level; and the study of functions, called calculus at the college freshman level becomes mathematical analysis and functional analysis at a more advanced level.  Each of these branches of more abstract mathematics have many sub-specialties, and there are in fact many connections between pure mathematics and applied mathematics disciplines. A steep rise in abstraction was seen mid 20th century.\n\nIn practice, however, these developments led to a sharp divergence from physics, particularly from 1950 to 1983. Later this was criticised, for example by Vladimir Arnold, as too much Hilbert, not enough Poincaré. The point does not yet seem to be settled, in that string theory pulls one way, while discrete mathematics pulls back towards proof as central.\n\nPurism\nMathematicians have always had differing opinions regarding the distinction between pure and applied mathematics. \nOne of the most famous (but perhaps misunderstood) modern examples of this debate can be found in G.H. Hardy's A Mathematician's Apology.\n\nIt is widely believed that Hardy considered applied mathematics to be ugly and dull. Although it is true that Hardy preferred pure mathematics, which he often compared to painting and poetry, Hardy saw the distinction between pure and applied mathematics to be simply that applied mathematics sought to express physical truth in a mathematical framework, whereas pure mathematics expressed truths that were independent of the physical world. Hardy made a separate distinction in mathematics between what he called \"real\" mathematics, \"which has permanent aesthetic value\", and \"the dull and elementary parts of mathematics\" that have practical use.\n\nHardy considered some physicists, such as Einstein, and Dirac, to be among the \"real\" mathematicians, but at the time that he was writing the Apology he also considered general relativity and quantum mechanics to be \"useless\", which allowed him to hold the opinion that only \"dull\" mathematics was useful. Moreover, Hardy briefly admitted that—just as the application of matrix theory and group theory to physics had come unexpectedly—the time may come where some kinds of beautiful, \"real\" mathematics may be useful as well.\n\nAnother insightful view is offered by Magid:\n\nSee also\nApplied mathematics\nLogic\nMetalogic\nMetamathematics\n\nReferences\n\nExternal links\n\nWhat is Pure Mathematics? – Department of Pure Mathematics, University of Waterloo\n What is Pure Mathematics? by Professor P. J. Giblin The University of Liverpool\nThe Principles of Mathematics by Bertrand Russell\nHow to Become a Pure Mathematician (or Statistician), a list of undergraduate and basic graduate textbooks and lecture notes, with several comments and links to solutions, companion sites, data sets, errata pages, etc.\n\nCategory:Fields of mathematics\nCategory:Abstraction"
    },
    {
      "title": "Algebra",
      "url": "https://en.wikipedia.org/wiki/Algebra",
      "text": "thumb|The quadratic formula expresses the solution of the equation , where  is not zero, in terms of its coefficients  and .\n\nAlgebra (from Arabic \"al-jabr\", literally meaning \"reunion of broken parts\") is one of the broad parts of mathematics, together with number theory, geometry and analysis. In its most general form, algebra is the study of mathematical symbols and the rules for manipulating these symbols;See , page 1: \"An algebraic system can be described as a set of objects together with some operations for combining them\". it is a unifying thread of almost all of mathematics.See  , page 1: \"...it also serves as the unifying thread which interlaces almost all of mathematics\". It includes everything from elementary equation solving to the study of abstractions such as groups, rings, and fields. The more basic parts of algebra are called elementary algebra; the more abstract parts are called abstract algebra or modern algebra. Elementary algebra is generally considered to be essential for any study of mathematics, science, or engineering, as well as such applications as medicine and economics. Abstract algebra is a major area in advanced mathematics, studied primarily by professional mathematicians.\n\nElementary algebra differs from arithmetic in the use of abstractions, such as using letters to stand for numbers that are either unknown or allowed to take on many values. For example, in  the letter  is unknown, but applying additive inverses can reveal its value: . In , the letters  and  are variables, and the letter  is a constant, the speed of light in a vacuum. Algebra gives methods for writing formulas and solving equations that are much clearer and easier than the older method of writing everything out in words.\n\nThe word algebra is also used in certain specialized ways. A special kind of mathematical object in abstract algebra is called an \"algebra\", and the word is used, for example, in the phrases linear algebra and algebraic topology.\n\nA mathematician who does research in algebra is called an algebraist.\n\n Etymology \nthumb|180px|The name of algebra comes from the title of a book by Muhammad ibn Musa al-KhwarizmiEsposito, John L. (2000-04-06). The Oxford History of Islam. Oxford University Press. p. 188. .\nThe word algebra comes from the Arabic  ( lit. \"the reunion of broken parts\") from the title of the book Ilm al-jabr wa'l-muḳābala by the Persian mathematician and astronomer al-Khwarizmi. The word entered the English language during the fifteenth century, from either Spanish, Italian, or Medieval Latin. It originally referred to the surgical procedure of setting broken or dislocated bones. The mathematical meaning was first recorded in the sixteenth century.\n\n Different meanings of \"algebra\" \nThe word \"algebra\" has several related meanings in mathematics, as a single word or with qualifiers.\n As a single word without an article, \"algebra\" names a broad part of mathematics.\n As a single word with an article or in plural, \"an algebra\" or \"algebras\" denotes a specific mathematical structure, whose precise definition depends on the author. Usually, the structure has an addition, multiplication, and a scalar multiplication (see Algebra over a field). When some authors use the term \"algebra\", they make a subset of the following additional assumptions: associative, commutative, unital, and/or finite-dimensional. In universal algebra, the word \"algebra\" refers to a generalization of the above concept, which allows for n-ary operations.\n With a qualifier, there is the same distinction:\n Without an article, it means a part of algebra, such as linear algebra, elementary algebra (the symbol-manipulation rules taught in elementary courses of mathematics as part of primary and secondary education), or abstract algebra (the study of the algebraic structures for themselves).\n With an article, it means an instance of some abstract structure, like a Lie algebra, an associative algebra, or a vertex operator algebra.\n Sometimes both meanings exist for the same qualifier, as in the sentence: Commutative algebra is the study of commutative rings, which are commutative algebras over the integers.\n\n Algebra as a branch of mathematics \n\nAlgebra began with computations similar to those of arithmetic, with letters standing for numbers. This allowed proofs of properties that are true no matter which numbers are involved. For example, in the quadratic equation\n\n can be any numbers whatsoever (except that  cannot be ), and the quadratic formula can be used to quickly and easily find the values of the unknown quantity  which satisfy the equation. That is to say, to find all the solutions of the equation.\n\nHistorically, and in current teaching, the study of algebra starts with the solving of equations such as the quadratic equation above. Then more general questions, such as \"does an equation have a solution?\", \"how many solutions does an equation have?\", \"what can be said about the nature of the solutions?\" are considered. These questions led extending algebra to non-numerical objects, such as permutations, vectors, matrices, and polynomials. The structural properties of these non-numerical objects were then abstracted into algebraic structures such as groups, rings, and fields.\n\nBefore the 16th century, mathematics was divided into only two subfields, arithmetic and geometry. Even though some methods, which had been developed much earlier, may be considered nowadays as algebra, the emergence of algebra and, soon thereafter, of infinitesimal calculus as subfields of mathematics only dates from the 16th or 17th century. From the second half of 19th century on, many new fields of mathematics appeared, most of which made use of both arithmetic and geometry, and almost all of which used algebra.\n\nToday, algebra has grown until it includes many branches of mathematics, as can be seen in the Mathematics Subject Classification\nwhere none of the first level areas (two digit entries) is called algebra. Today algebra includes section 08-General algebraic systems, 12-Field theory and polynomials, 13-Commutative algebra, 15-Linear and multilinear algebra; matrix theory, 16-Associative rings and algebras, 17-Nonassociative rings and algebras, 18-Category theory; homological algebra, 19-K-theory and 20-Group theory. Algebra is also used extensively in 11-Number theory and 14-Algebraic geometry.\n\n History \n\n Early history of algebra \n\nthumb|A page from Al-Khwārizmī's al-Kitāb al-muḫtaṣar fī ḥisāb al-ğabr wa-l-muqābala\n\nThe roots of algebra can be traced to the ancient Babylonians, who developed an advanced arithmetical system with which they were able to do calculations in an algorithmic fashion. The Babylonians developed formulas to calculate solutions for problems typically solved today by using linear equations, quadratic equations, and indeterminate linear equations. By contrast, most Egyptians of this era, as well as Greek and Chinese mathematics in the 1st millennium BC, usually solved such equations by geometric methods, such as those described in the Rhind Mathematical Papyrus, Euclid's Elements, and The Nine Chapters on the Mathematical Art. The geometric work of the Greeks, typified in the Elements, provided the framework for generalizing formulae beyond the solution of particular problems into more general systems of stating and solving equations, although this would not be realized until mathematics developed in medieval Islam.See .\n\nBy the time of Plato, Greek mathematics had undergone a drastic change. The Greeks created a geometric algebra where terms were represented by sides of geometric objects, usually lines, that had letters associated with them.See , Europe in the Middle Ages, p. 258: \"In the arithmetical theorems in Euclid's Elements VII–IX, numbers had been represented by line segments to which letters had been attached, and the geometric proofs in al-Khwarizmi's Algebra made use of lettered diagrams; but all coefficients in the equations used in the Algebra are specific numbers, whether represented by numerals or written out in words. The idea of generality is implied in al-Khwarizmi's exposition, but he had no scheme for expressing algebraically the general propositions that are so readily available in geometry.\" Diophantus (3rd century AD) was an Alexandrian Greek mathematician and the author of a series of books called Arithmetica. These texts deal with solving algebraic equations, and have led, in number theory to the modern notion of Diophantine equation.\n\nEarlier traditions discussed above had a direct influence on the Persian mathematician Muḥammad ibn Mūsā al-Khwārizmī (c. 780–850). He later wrote The Compendious Book on Calculation by Completion and Balancing, which established algebra as a mathematical discipline that is independent of geometry and arithmetic.\n\nThe Hellenistic mathematicians Hero of Alexandria and Diophantus as well as Indian mathematicians such as Brahmagupta continued the traditions of Egypt and Babylon, though Diophantus' Arithmetica and Brahmagupta's Brāhmasphuṭasiddhānta are on a higher level. For example, the first complete arithmetic solution (including zero and negative solutions) to quadratic equations was described by Brahmagupta in his book Brahmasphutasiddhanta. Later, Persian and Arabic mathematicians developed algebraic methods to a much higher degree of sophistication. Although Diophantus and the Babylonians used mostly special ad hoc methods to solve equations, Al-Khwarizmi's contribution was fundamental. He solved linear and quadratic equations without algebraic symbolism, negative numbers or zero, thus he had to distinguish several types of equations.\n\nIn the context where algebra is identified with the theory of equations, the Greek mathematician Diophantus has traditionally been known as the \"father of algebra\" and in context where it is identified with rules for manipulating and solving equations, Persian mathematician al-Khwarizmi is regarded as \"the father of algebra\".See , page 181: \"If we think primarily of matter of notations, Diophantus has good claim to be known as the 'father of algebra', but in terms of motivation and concept, the claim is less appropriate. The Arithmetica is not a systematic exposition of the algebraic operations, or of algebraic functions or of the solution of algebraic equations\".See , page 230: \"The six cases of equations given above exhaust all possiblities for linear and quadratic equations...In this sense, then, al-Khwarizmi is entitled to be known as 'the father of algebra'\".See , page 228: \"Diophantus sometimes is called the father of algebra, but this title more appropriately belongs to al-Khowarizmi\".See , page 263–277: \"In a sense, al-Khwarizmi is more entitled to be called \"the father of algebra\" than Diophantus because al-Khwarizmi is the first to teach algebra in an elementary form and for its own sake, Diophantus is primarily concerned with the theory of numbers\". A debate now exists whether who (in general sense) is more entitled to be known as \"the father of algebra\". Those who support Diophantus point to the fact that the algebra found in Al-Jabr is slightly more elementary than the algebra found in Arithmetica and that Arithmetica is syncopated while Al-Jabr is fully rhetorical.See , page 228. Those who support Al-Khwarizmi point to the fact that he introduced the methods of \"reduction\" and \"balancing\" (the transposition of subtracted terms to the other side of an equation, that is, the cancellation of like terms on opposite sides of the equation) which the term al-jabr originally referred to,See , The Arabic Hegemony, p. 229: \"It is not certain just what the terms al-jabr and muqabalah mean, but the usual interpretation is similar to that implied in the translation above. The word al-jabr presumably meant something like \"restoration\" or \"completion\" and seems to refer to the transposition of subtracted terms to the other side of an equation; the word muqabalah is said to refer to \"reduction\" or \"balancing\" – that is, the cancellation of like terms on opposite sides of the equation\". and that he gave an exhaustive explanation of solving quadratic equations,See , The Arabic Hegemony, p. 230: \"The six cases of equations given above exhaust all possibilities for linear and quadratic equations having positive root. So systematic and exhaustive was al-Khwarizmi's exposition that his readers must have had little difficulty in mastering the solutions\". supported by geometric proofs, while treating algebra as an independent discipline in its own right. His algebra was also no longer concerned \"with a series of problems to be resolved, but an exposition which starts with primitive terms in which the combinations must give all possible prototypes for equations, which henceforward explicitly constitute the true object of study\". He also studied an equation for its own sake and \"in a generic manner, insofar as it does not simply emerge in the course of solving a problem, but is specifically called on to define an infinite class of problems\".\n\nAnother Persian mathematician Omar Khayyam is credited with identifying the foundations of algebraic geometry and found the general geometric solution of the cubic equation. His book Treatise on Demonstrations of Problems of Algebra (1070), which laid down the principles of algebra, is part of the body of Persian mathematics that was eventually transmitted to Europe. Yet another Persian mathematician, Sharaf al-Dīn al-Tūsī, found algebraic and numerical solutions to various cases of cubic equations. He also developed the concept of a function. The Indian mathematicians Mahavira and Bhaskara II, the Persian mathematician Al-Karaji,See , The Arabic Hegemony, p. 239: \"Abu'l Wefa was a capable algebraist as well as a trigonometer. ... His successor al-Karkhi evidently used this translation to become an Arabic disciple of Diophantus – but without Diophantine analysis! ... In particular, to al-Karkhi is attributed the first numerical solution of equations of the form ax2n + bxn = c (only equations with positive roots were considered),\" and the Chinese mathematician Zhu Shijie, solved various cases of cubic, quartic, quintic and higher-order polynomial equations using numerical methods. In the 13th century, the solution of a cubic equation by Fibonacci is representative of the beginning of a revival in European algebra. Abū al-Ḥasan ibn ʿAlī al-Qalaṣādī (1412–1486) took \"the first steps toward the introduction of algebraic symbolism\". He also computed ∑n2, ∑n3 and used the method of successive approximation to determine square roots.\n\n Modern history of algebra \nthumb|200px|Italian mathematician Girolamo Cardano published the solutions to the cubic and quartic equations in his 1545 book Ars magna.\n\nFrançois Viète's work on new algebra at the close of the 16th century was an important step towards modern algebra. In 1637, René Descartes published La Géométrie, inventing analytic geometry and introducing modern algebraic notation. Another key event in the further development of algebra was the general algebraic solution of the cubic and quartic equations, developed in the mid-16th century. The idea of a determinant was developed by Japanese mathematician Seki Kōwa in the 17th century, followed independently by Gottfried Leibniz ten years later, for the purpose of solving systems of simultaneous linear equations using matrices. Gabriel Cramer also did some work on matrices and determinants in the 18th century. Permutations were studied by Joseph-Louis Lagrange in his 1770 paper Réflexions sur la résolution algébrique des équations devoted to solutions of algebraic equations, in which he introduced Lagrange resolvents. Paolo Ruffini was the first person to develop the theory of permutation groups, and like his predecessors, also in the context of solving algebraic equations.\n\nAbstract algebra was developed in the 19th century, deriving from the interest in solving equations, initially focusing on what is now called Galois theory, and on constructibility issues.\"The Origins of Abstract Algebra\". University of Hawaii Mathematics Department. George Peacock was the founder of axiomatic thinking in arithmetic and algebra. Augustus De Morgan discovered relation algebra in his Syllabus of a Proposed System of Logic. Josiah Willard Gibbs developed an algebra of vectors in three-dimensional space, and Arthur Cayley developed an algebra of matrices (this is a noncommutative algebra).\"The Collected Mathematical Papers\".Cambridge University Press.\n\n Areas of mathematics with the word algebra in their name \n\nSome areas of mathematics that fall under the classification abstract algebra have the word algebra in their name; linear algebra is one example. Others do not: group theory, ring theory, and field theory are examples. In this section, we list some areas of mathematics with the word \"algebra\" in the name.\n Elementary algebra, the part of algebra that is usually taught in elementary courses of mathematics.\n Abstract algebra, in which algebraic structures such as groups, rings and fields are axiomatically defined and investigated.\n Linear algebra, in which the specific properties of linear equations, vector spaces and matrices are studied.\n Boolean algebra, a branch of algebra abstracting the computation with the truth values false and true.\n Commutative algebra, the study of commutative rings.\n Computer algebra, the implementation of algebraic methods as algorithms and computer programs.\n Homological algebra, the study of algebraic structures that are fundamental to study topological spaces.\n Universal algebra, in which properties common to all algebraic structures are studied.\n Algebraic number theory, in which the properties of numbers are studied from an algebraic point of view.\n Algebraic geometry, a branch of geometry, in its primitive form specifying curves and surfaces as solutions of polynomial equations.\n Algebraic combinatorics, in which algebraic methods are used to study combinatorial questions.\n Relational algebra: a set of finitary relations that is closed under certain operators.\n\nMany mathematical structures are called algebras:\n Algebra over a field or more generally algebra over a ring.Many classes of algebras over a field or over a ring have a specific name:\n Associative algebra\n Non-associative algebra\n Lie algebra\n Hopf algebra\n C*-algebra\n Symmetric algebra\n Exterior algebra\n Tensor algebra\n In measure theory,\n Sigma-algebra\n Algebra over a set\n In category theory\n F-algebra and F-coalgebra\n T-algebra\n In logic,\n Relation algebra, a residuated Boolean algebra expanded with an involution called converse.\n Boolean algebra, a complemented distributive lattice.\n Heyting algebra\n\n Elementary algebra \n\nthumb|right|Algebraic expression notation:  1 – power (exponent)  2 – coefficient  3 – term  4 – operator  5 – constant term  x y c – variables/constants\nElementary algebra is the most basic form of algebra. It is taught to students who are presumed to have no knowledge of mathematics beyond the basic principles of arithmetic. In arithmetic, only numbers and their arithmetical operations (such as +, −, ×, ÷) occur. In algebra, numbers are often represented by symbols called variables (such as a, n, x, y or z). This is useful because:\n It allows the general formulation of arithmetical laws (such as a + b = b + a for all a and b), and thus is the first step to a systematic exploration of the properties of the real number system.\n It allows the reference to \"unknown\" numbers, the formulation of equations and the study of how to solve these. (For instance, \"Find a number x such that 3x + 1 = 10\" or going a bit further \"Find a number x such that ax + b = c\". This step leads to the conclusion that it is not the nature of the specific numbers that allows us to solve it, but that of the operations involved.)\n It allows the formulation of functional relationships. (For instance, \"If you sell x tickets, then your profit will be 3x − 10 dollars, or f(x) = 3x − 10, where f is the function, and x is the number to which the function is applied\".)\n\n Polynomials \nThe graph of a polynomial function of degree 3.|thumb|upright\n\nA polynomial is an expression that is the sum of a finite number of non-zero terms, each term consisting of the product of a constant and a finite number of variables raised to whole number powers. For example, x2 + 2x − 3 is a polynomial in the single variable x. A polynomial expression is an expression that may be rewritten as a polynomial, by using commutativity, associativity and distributivity of addition and multiplication. For example, (x − 1)(x + 3) is a polynomial expression, that, properly speaking, is not a polynomial. A polynomial function is a function that is defined by a polynomial, or, equivalently, by a polynomial expression. The two preceding examples define the same polynomial function.\n\nTwo important and related problems in algebra are the factorization of polynomials, that is, expressing a given polynomial as a product of other polynomials that can not be factored any further, and the computation of polynomial greatest common divisors. The example polynomial above can be factored as (x − 1)(x + 3). A related class of problems is finding algebraic expressions for the roots of a polynomial in a single variable.\n\n Education \n\nIt has been suggested that elementary algebra should be taught to students as young as eleven years old, though in recent years it is more common for public lessons to begin at the eighth grade level (≈ 13 y.o. ±) in the United States. However, in some US schools, algebra is started in ninth grade.\n\n Abstract algebra \n\nAbstract algebra extends the familiar concepts found in elementary algebra and arithmetic of numbers to more general concepts. Here are listed fundamental concepts in abstract algebra.\n\nSets: Rather than just considering the different types of numbers, abstract algebra deals with the more general concept of sets: a collection of all objects (called elements) selected by property specific for the set. All collections of the familiar types of numbers are sets. Other examples of sets include the set of all two-by-two matrices, the set of all second-degree polynomials (ax2 + bx + c), the set of all two dimensional vectors in the plane, and the various finite groups such as the cyclic groups, which are the groups of integers modulo n. Set theory is a branch of logic and not technically a branch of algebra.\n\nBinary operations: The notion of addition (+) is abstracted to give a binary operation, ∗ say. The notion of binary operation is meaningless without the set on which the operation is defined. For two elements a and b in a set S, a ∗ b is another element in the set; this condition is called closure. Addition (+), subtraction (−), multiplication (×), and division (÷) can be binary operations when defined on different sets, as are addition and multiplication of matrices, vectors, and polynomials.\n\nIdentity elements: The numbers zero and one are abstracted to give the notion of an identity element for an operation. Zero is the identity element for addition and one is the identity element for multiplication. For a general binary operator ∗ the identity element e must satisfy a ∗ e = a and e ∗ a = a, and is necessarily unique, if it exists. This holds for addition as a + 0 = a and 0 + a = a and multiplication a × 1 = a and 1 × a = a. Not all sets and operator combinations have an identity element; for example, the set of positive natural numbers (1, 2, 3, ...) has no identity element for addition.\n\nInverse elements: The negative numbers give rise to the concept of inverse elements. For addition, the inverse of a is written −a, and for multiplication the inverse is written a−1. A general two-sided inverse element a−1 satisfies the property that a ∗ a−1 = e and a−1 ∗ a = e, where e is the identity element.\n\nAssociativity: Addition of integers has a property called associativity. That is, the grouping of the numbers to be added does not affect the sum. For example: . In general, this becomes (a ∗ b) ∗ c = a ∗ (b ∗ c). This property is shared by most binary operations, but not subtraction or division or octonion multiplication.\n\nCommutativity: Addition and multiplication of real numbers are both commutative. That is, the order of the numbers does not affect the result. For example: 2 + 3 = 3 + 2. In general, this becomes a ∗ b = b ∗ a. This property does not hold for all binary operations. For example, matrix multiplication and quaternion multiplication are both non-commutative.\n\n Groups \n\nCombining the above concepts gives one of the most important structures in mathematics: a group. A group is a combination of a set S and a single binary operation ∗, defined in any way you choose, but with the following properties:\n An identity element e exists, such that for every member a of S, e ∗ a and a ∗ e are both identical to a.\n Every element has an inverse: for every member a of S, there exists a member a−1 such that a ∗ a−1 and a−1 ∗ a are both identical to the identity element.\n The operation is associative: if a, b and c are members of S, then (a ∗ b) ∗ c is identical to a ∗ (b ∗ c).\n\nIf a group is also commutative – that is, for any two members a and b of S, a ∗ b is identical to b ∗ a – then the group is said to be abelian.\n\nFor example, the set of integers under the operation of addition is a group. In this group, the identity element is 0 and the inverse of any element a is its negation, −a. The associativity requirement is met, because for any integers a, b and c, (a + b) + c = a + (b + c)\n\nThe non-zero rational numbers form a group under multiplication. Here, the identity element is 1, since 1 × a = a × 1 = a for any rational number a. The inverse of a is 1/a, since a × 1/a = 1.\n\nThe integers under the multiplication operation, however, do not form a group. This is because, in general, the multiplicative inverse of an integer is not an integer. For example, 4 is an integer, but its multiplicative inverse is ¼, which is not an integer.\n\nThe theory of groups is studied in group theory. A major result in this theory is the classification of finite simple groups, mostly published between about 1955 and 1983, which separates the finite simple groups into roughly 30 basic types.\n\nSemi-groups, quasi-groups, and monoids are structures similar to groups, but more general. They comprise a set and a closed binary operation, but do not necessarily satisfy the other conditions. A semi-group has an associative binary operation, but might not have an identity element. A monoid is a semi-group which does have an identity but might not have an inverse for every element. A quasi-group satisfies a requirement that any element can be turned into any other by either a unique left-multiplication or right-multiplication; however the binary operation might not be associative.\n\nAll groups are monoids, and all monoids are semi-groups.\n\n+ExamplesSetNatural numbers NIntegers ZRational numbers Q (also real R and complex C numbers)Integers modulo 3: Z3 = {0, 1, 2}Operation + × (w/o zero) + × (w/o zero) + − × (w/o zero) ÷ (w/o zero) + × (w/o zero)Closed Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Identity 0 1 0 1 0 N/A 1 N/A 0 1 Inverse N/A N/A −a N/A −a N/A 1/a N/A 0, 2, 1, respectively N/A, 1, 2, respectively Associative Yes Yes Yes Yes Yes No Yes No Yes Yes Commutative Yes Yes Yes Yes Yes No Yes No Yes Yes Structure monoid monoid abelian group monoid abelian group quasi-group abelian group quasi-group abelian group abelian group (Z2)\n\n Rings and fields \n\nGroups just have one binary operation. To fully explain the behaviour of the different types of numbers, structures with two operators need to be studied. The most important of these are rings, and fields.\n\nA ring has two binary operations (+) and (×), with × distributive over +. Under the first operator (+) it forms an abelian group. Under the second operator (×) it is associative, but it does not need to have identity, or inverse, so division is not required. The additive (+) identity element is written as 0 and the additive inverse of a is written as −a.\n\nDistributivity generalises the distributive law for numbers. For the integers  and  and × is said to be distributive over +.\n\nThe integers are an example of a ring. The integers have additional properties which make it an integral domain.\n\nA field is a ring with the additional property that all the elements excluding 0 form an abelian group under ×. The multiplicative (×) identity is written as 1 and the multiplicative inverse of a is written as a−1.\n\nThe rational numbers, the real numbers and the complex numbers are all examples of fields.\n\n See also \n \n Outline of algebra\n Outline of linear algebra\n Algebra tile\n\n References \n\n Works cited \n \n \n \n\n Further reading \n \n \n \n \n \n \n \n \n \n\n External links \n\n Khan Academy: Conceptual videos and worked examples\n Khan Academy: Origins of Algebra, free online micro lectures\n Algebrarules.com: An open source resource for learning the fundamentals of Algebra\n 4000 Years of Algebra, lecture by Robin Wilson, at Gresham College, October 17, 2007 (available for MP3 and MP4 download, as well as a text file).\n \n\n "
    },
    {
      "title": "Dialgebra",
      "url": "https://en.wikipedia.org/wiki/Dialgebra",
      "text": "Dialgebra is the generalization of both algebra and coalgebra. Many algebraic notions have previously been generalized to dialgebras. Dialgebra also attempts to obtain Lie algebras from associated algebras.\n\nReferences\n\n "
    },
    {
      "title": "Outline of algebra",
      "url": "https://en.wikipedia.org/wiki/Outline_of_algebra",
      "text": "Algebra is one of the main branches of mathematics, covering the study of structure, relation  and quantity. Algebra studies the effects of adding and multiplying numbers, variables, and polynomials, along with their factorization and determining their roots. In addition to working directly with numbers, algebra also covers symbols, variables, and set elements. Addition and multiplication are general operations, but their precise definitions lead to structures such as groups, rings, and fields.\n\nBranches\n Pre-algebra\n Elementary algebra\n Abstract algebra\n Linear algebra\n Universal algebra\n\n Algebraic equations \n\nAn algebraic equation is an equation involving only algebraic expressions in the unknowns. These are further classified by degree.\n\n Linear equation – algebraic equation of degree one.\n Polynomial equation – equation in which a polynomial is set equal to another polynomial.\n Transcendental equation – equation involving a transcendental function of one of its  variables.\n Functional equation – equation in which the unknowns are functions rather than simple quantities.\n Differential equation – equation involving derivatives.\n Integral equation – equation involving integrals.\n Diophantine equation – equation where the unknowns are required to be integers.\n Polynomials\n Variables\n\nHistory\n\n History of algebra\n\nGeneral algebra concepts\n Algebra – \n Fundamental theorem of algebra – states that every non-constant single-variable polynomial with complex coefficients has at least one complex root. This includes polynomials with real coefficients, since every real number is a complex number with an imaginary part equal to zero.\n Linear equation – an algebraic equation with a degree of one\n Quadratic equation – an algebraic equation with a degree of two\n Cubic equation – an algebraic equation with a degree of  three\n Quartic equation – an algebraic equation with a degree of four\n Quintic equation – an algebraic equation with a degree of five\n Polynomial – \n\nSee also\n\n Table of mathematical symbols\n\nExternal links\n'4000 Years of Algebra', lecture by Robin Wilson, at Gresham College, 17 October 2007 (available for MP3 and MP4 download, as well as a text file).\n ExampleProblems.com Example problems and solutions from basic and abstract algebra.\n\n List\nAlgebra\nAlgebra\nAlgebra"
    },
    {
      "title": "Timeline of geometry",
      "url": "https://en.wikipedia.org/wiki/Timeline_of_geometry",
      "text": "A timeline of algebra and geometry\n\nBefore 1000 BC\n ca. 2000 BC — Scotland, Carved Stone Balls exhibit a variety of symmetries including all of the symmetries of Platonic solids.\n 1800 BC — Moscow Mathematical Papyrus, findings volume of a frustum\n 1650 BC — Rhind Mathematical Papyrus, copy of a lost scroll from around 1850 BC, the scribe Ahmes presents one of the first known approximate values of π at 3.16, the first attempt at squaring the circle, earliest known use of a sort of cotangent, and knowledge of solving first order linear equations\n\n1st millennium BC\n 800 BC — Baudhayana, author of the Baudhayana Sulba Sutra, a Vedic Sanskrit geometric text, contains quadratic equations, and calculates the square root of 2 correct to five decimal places\n ca. 600 BC — the other Vedic “Sulba Sutras” (“rule of chords” in Sanskrit) use Pythagorean triples, contain of a number of geometrical proofs, and approximate π at 3.16\n 5th century BC — Hippocrates of Chios utilizes lunes in an attempt to square the circle\n 5th century BC — Apastamba, author of the Apastamba Sulba Sutra, another Vedic Sanskrit geometric text, makes an attempt at squaring the circle and also calculates the square root of 2 correct to five decimal places\n 530 BC — Pythagoras studies propositional geometry and vibrating lyre strings; his group also discover the irrationality of the square root of two,\n 370 BC — Eudoxus states the method of exhaustion for area determination\n 300 BC — Euclid in his Elements studies geometry as an axiomatic system, proves the infinitude of prime numbers and presents the Euclidean algorithm; he states the law of reflection in  Catoptrics, and he proves the fundamental theorem of arithmetic\n 260 BC — Archimedes proved that the value of π lies between 3 + 1/7 (approx. 3.1429) and 3 + 10/71 (approx. 3.1408), that the area of a circle was equal to π multiplied by the square of the radius of the circle and that the area enclosed by a parabola and a straight line is 4/3 multiplied by the area of a triangle with equal base and height. He also gave a very accurate estimate of the value of the square root of 3.\n 225 BC — Apollonius of Perga writes  On Conic Sections and names the ellipse, parabola, and hyperbola,\n 150 BC — Jain mathematicians in India write the “Sthananga Sutra”, which contains work on the theory of numbers, arithmetical operations, geometry, operations with fractions, simple equations, cubic equations, quartic equations, and permutations and combinations\n 140 BC — Hipparchus develops the bases of trigonometry.\n\n1st millennium\n ca. 340 — Pappus of Alexandria states his hexagon theorem and his centroid theorem\n 500 — Aryabhata writes the “Aryabhata-Siddhanta”, which first introduces the trigonometric functions and methods of calculating their approximate numerical values. It defines the concepts of sine and cosine, and also contains the earliest tables of sine and cosine values (in 3.75-degree intervals from 0 to 90 degrees)\n 7th century — Bhaskara I gives a rational approximation of the sine function\n 8th century — Virasena gives explicit rules for the Fibonacci sequence, gives the derivation of the volume of a frustum using an infinite procedure, and also deals with the logarithm to base 2 and knows its laws\n 8th century — Shridhara gives the rule for finding the volume of a sphere and also the formula for solving quadratic equations\n 820 — Al-Mahani conceived the idea of reducing geometrical problems such as doubling the cube to problems in algebra.\n ca. 900 — Abu Kamil of Egypt had begun to understand what we would write in symbols as \n 975 — Al-Batani — Extended the Indian concepts of sine and cosine to other trigonometrical ratios, like tangent, secant and their inverse functions. Derived the formula:  and .\n\n1000–1500\nca. 1000 — Law of sines is discovered by Muslim mathematicians, but it is uncertain who discovers it first between Abu-Mahmud al-Khujandi, Abu Nasr Mansur, and Abu al-Wafa.\n ca. 1100 — Omar Khayyám “gave a complete classification of cubic equations with geometric solutions found by means of intersecting conic sections.” He became the first to find general geometric solutions of cubic equations and laid the foundations for the development of analytic geometry and non-Euclidean geometry. He also extracted roots using the decimal system (Hindu-Arabic numeral system).\n 1135 — Sharafeddin Tusi followed al-Khayyam's application of algebra to geometry, and wrote a treatise on cubic equations which “represents an essential contribution to another algebra which aimed to study curves by means of equations, thus inaugurating the beginning of algebraic geometry.”Arabic mathematics, MacTutor History of Mathematics archive, University of St Andrews, Scotland\n ca. 1250 — Nasir Al-Din Al-Tusi attempts to develop a form of non-Euclidean geometry.\n 15th century — Nilakantha Somayaji, a Kerala school mathematician, writes the “Aryabhatiya Bhasya”, which contains work on infinite-series expansions, problems of algebra, and spherical geometry\n\n17th century\n 17th century – Putumana Somayaji writes the \"Paddhati\", which presents a detailed discussion of various trigonometric series\n 1619 –  Johannes Kepler discovers two of the Kepler-Poinsot polyhedra.\n\n18th century\n 1722 –  Abraham de Moivre states de Moivre's formula connecting trigonometric functions and complex numbers,\n 1733 –  Giovanni Gerolamo Saccheri studies what geometry would be like if Euclid's fifth postulate were false,\n 1796 –  Carl Friedrich Gauss proves that the regular 17-gon can be constructed using only a compass and straightedge\n 1797 –  Caspar Wessel associates vectors with complex numbers and studies complex number operations in geometrical terms,\n 1799 –  Gaspard Monge publishes Géométrie descriptive, in which he introduces descriptive geometry.\n\n19th century\n 1806 –  Louis Poinsot discovers the two remaining Kepler-Poinsot polyhedra.\n 1829 –  Bolyai, Gauss, and Lobachevsky invent hyperbolic non-Euclidean geometry,\n 1837 –  Pierre Wantzel proves that doubling the cube and trisecting the angle are impossible with only a compass and straightedge, as well as the full completion of the problem of constructibility of regular polygons\n 1843 –  William Hamilton discovers the calculus of quaternions and deduces that they are non-commutative,\n 1854 –  Bernhard Riemann introduces Riemannian geometry,\n 1854 –  Arthur Cayley shows that quaternions can be used to represent rotations in four-dimensional space,\n 1858 –  August Ferdinand Möbius invents the Möbius strip,\n 1870 –  Felix Klein constructs an analytic geometry for Lobachevski's geometry thereby establishing its self-consistency and the logical independence of Euclid's fifth postulate,\n 1873 –  Charles Hermite proves that e is transcendental,\n 1878 – Charles Hermite solves the general quintic equation by means of elliptic and modular functions\n 1882 –  Ferdinand von Lindemann proves that π is transcendental and that therefore the circle cannot be squared with a compass and straightedge,\n 1882 –  Felix Klein invents the Klein bottle,\n 1899 –  David Hilbert presents a set of self-consistent geometric axioms in Foundations of Geometry\n\n20th century\n 1901 –  Élie Cartan develops the exterior derivative,\n 1912 –  Luitzen Egbertus Jan Brouwer presents the Brouwer fixed-point theorem,\n 1916 – Einstein's theory of general relativity.\n 1930 –  Casimir Kuratowski shows that the three-cottage problem has no solution,\n 1931 –  Georges de Rham develops theorems in cohomology and characteristic classes,\n 1933 –  Karol Borsuk and Stanislaw Ulam present the Borsuk-Ulam antipodal-point theorem,\n 1955 –  H. S. M. Coxeter et al. publish the complete list of uniform polyhedron,\n 1975 –  Benoit Mandelbrot, fractals theory,\n 1981 – Mikhail Gromov develops the theory of hyperbolic groups, revolutionizing both infinite group theory and global differential geometry,\n 1983 –  the classification of finite simple groups, a collaborative work involving some hundred mathematicians and spanning thirty years, is completed,\n 1991 –  Alain Connes and John Lott develop non-commutative geometry,\n 1998 –  Thomas Callister Hales proves the Kepler conjecture,\n\n21st century\n 2003 – Grigori Perelman proves the Poincaré conjecture,\n 2007 – a team of researches throughout North America and Europe used networks of computers to map E8 (mathematics).Elizabeth A. Thompson, MIT News Office, Math research team maps E8 http://www.huliq.com/15695/mathematicians-map-e8\n\nReferences\n\nAlgebra and geometry\n Timeline\n "
    },
    {
      "title": "Akivis algebra",
      "url": "https://en.wikipedia.org/wiki/Akivis_algebra",
      "text": "In mathematics, and in particular the study of algebra, an Akivis algebra is a nonassociative algebra equipped with a binary operator, the commutator  and a ternary operator, the associator  that satisfy a particular relationship known as the Akivis identity. They are named in honour of Russian mathematician Maks A. Akivis.\n\nFormally, if  is a vector space over a field  of characteristic zero, we say  is an Akivis algebra if the operation  is bilinear and anticommutative; and the trilinear operator  satisfies the Akivis identity:\n\nAn Akivis algebra with  is a Lie algebra, for the Akivis identity reduces to the Jacobi identity. Note that the terms on the right hand side have positive sign for even permutations and negative sign for odd permutations of .\n\nAny nonassociative algebra is an Akivis algebra if we define  and . It is known that all Akivis algebras may be represented as a subalgebra of a nonassociative algebra in this way (for associative algebras, the associator is identically zero, and the Akivis identity reduces to the Jacobi identity).\n\n References \n M. R. Bremner, I. R. Hentzel, and L. A. Peresi 2005. \"Dimension formulas for the free nonassociative algebra\". Communications in Algebra 33:4063-4081.\n\nCategory:Algebra"
    },
    {
      "title": "Rational difference equation",
      "url": "https://en.wikipedia.org/wiki/Rational_difference_equation",
      "text": "A rational difference equation is a nonlinear difference equation of the formSkellam,  J.G. (1951).  “Random dispersal in theoretical populations”, Biometrika 38 196−218, eqns (41,42)Dynamics of third-order rational difference equations with open problems and ConjecturesDynamics of Second-order rational difference equations with open problems and ConjecturesNewth, Gerald, \"World order from chaotic beginnings\", Mathematical Gazette 88, March 2004, 39-45 gives a trigonometric approach.\n \nwhere the initial conditions  are such that the denominator  never vanishes for any .\n\nFirst-order rational difference equation\nA first-order rational difference equation is a nonlinear difference equation of the form\n\n \n\nWhen  and the initial condition  are real numbers, this difference equation is called a Riccati difference equation.\n\nSuch an equation can be solved by writing  as a nonlinear transformation of another variable  which itself evolves linearly.  Then standard methods can be used to solve the linear difference equation in .\n\n Solving a first-order equation\n\nFirst approach\n\nOne approach Brand, Louis, \"A sequence defined by a difference equation,\" American Mathematical Monthly 62, September 1955, 489–492.    online to developing the transformed variable , when , is to write\n \nwhere   and  and where .\n\nFurther writing   can be shown to yield\n \n\nSecond approach\n\nThis approach Mitchell, Douglas W., \"An analytic Riccati solution for two-target discrete-time control,\" Journal of Economic Dynamics and Control 24, 2000, 615–622.  gives a first-order difference equation for  instead of a second-order one, for the case in which  is non-negative.  Write    implying , where  is given by  and where .  Then it can be shown that  evolves according to\n\n \n\nThird approach\n\nThe equation\n\n \n\ncan also be solved by treating it as a special case of the more general matrix equation\n\nwhere all of A, B, C, E, and X are n×n matrices (in this case n=1); the  solution of this isMartin, C. F., and Ammar, G., \"The geometry of the matrix Riccati equation and associated eigenvalue method,\" in Bittani, Laub, and Willems (eds.), The Riccati Equation, Springer-Verlag, 1991.\n\nwhere\n\nApplication\n\nIt was shown in Balvers, Ronald J., and Mitchell, Douglas W., \"Reducing the dimensionality of linear quadratic control problems,\" Journal of Economic Dynamics and Control 31, 2007, 141–159. that a dynamic matrix Riccati equation of the form\n\n \n\nwhich can arise in some discrete-time optimal control problems, can be solved using the second approach above if the matrix C has only one more row than column.\n\nReferences\n\nFurther reading\n\n Simons, Stuart, \"A non-linear difference equation,\" Mathematical Gazette 93, November 2009, 500-504.\n\nAlgebra\nRecurrence relations\nCategory:Dynamical systems"
    },
    {
      "title": "List of algebraic constructions",
      "url": "https://en.wikipedia.org/wiki/List_of_algebraic_constructions",
      "text": "An algebraic construction is a method by which an algebraic entity is defined or derived from another.\n\nInstances include:\n\n Cayley–Dickson construction\n Proj construction\n Grothendieck group\n Gelfand–Naimark–Segal construction\n Ultraproduct\n ADHM construction\n Burnside ring\n Simplicial set\n Fox derivative\n Mapping cone (homological algebra)\n Prym variety\n Todd class\n Adjunction (field theory)\n Vaughan Jones construction\n Strähle construction\n Coset construction\n Plus construction\n Algebraic K-theory\n Gelfand–Naimark–Segal construction\n Stanley–Reisner ring construction\n Quotient ring construction\n Ward's twistor construction\n Hilbert symbol\n Hilbert's arithmetic of ends\n Colombeau's construction\n Vector bundle\n Integral monoid ring construction\n Integral group ring construction\n Category of Eilenberg–Moore algebras\n Kleisli category\n Adjunction (field theory)\n Lindenbaum–Tarski algebra construction\n Freudenthal magic square\n Stone–Čech compactification\n\nCategory:Mathematics-related lists\nCategory:Algebra"
    },
    {
      "title": "Algebraic signal processing",
      "url": "https://en.wikipedia.org/wiki/Algebraic_signal_processing",
      "text": "In the algebraic theory of linear signal processing, a set of filters is treated as an algebra and a set of signals is treated as a module and the z-transform is generalized to linear maps.\n\nReferences\n .\n\nExternal links\nSmart Project: Algebraic Theory of Signal Processing at the Department of Electrical and Computer Engineering at Carnegie Mellon University.\n\nCategory:Algebra\nCategory:Signal processing"
    },
    {
      "title": "Algebraic solution",
      "url": "https://en.wikipedia.org/wiki/Algebraic_solution",
      "text": "An algebraic solution or solution in radicals is a closed-form expression, and more specifically a closed-form algebraic expression, that is the solution of an algebraic equation in terms of the coefficients, relying only on addition, subtraction, multiplication, division, raising to integer powers, and the extraction of nth roots (square roots, cube roots, and other integer roots). \n\nThe most well-known example is the solution \n\nintroduced in secondary school, of the quadratic equation\n\n(where a ≠ 0).\n\nThere exist more complicated algebraic solutions for the general cubic equationNickalls, R. W. D., \"A new approach to solving the cubic: Cardano's solution revealed,\" Mathematical Gazette 77, November 1993, 354-359. and quartic equation.Carpenter, William, \"On the solution of the real quartic,\" Mathematics Magazine 39, 1966, 28-30.  The Abel–Ruffini theoremJacobson, Nathan (2009), Basic Algebra 1 (2nd ed.), Dover,  states that the general quintic equation lacks an algebraic solution, and this directly implies that the general polynomial equation of degree n, for n ≥ 5, cannot be solved algebraically. However, for n  ≥ 5, some polynomial equations have algebraic solutions; for example, the equation  can be solved as  See  for various other examples in degree 5.\n\nÉvariste Galois introduced a criterion allowing one to decide which equations are solvable in radicals. See Radical extension for the precise formulation of his result.\n\nAlgebraic solutions form a subset of closed-form expressions, because the latter permit transcendental functions (non-algebraic functions) such as the exponential function, the logarithmic function, and the trigonometric functions and their inverses.\n\nSee also\nSolvable quintics\nSolvable sextics\nSolvable septics\n\nReferences\n\nCategory:Algebra\nCategory:Equations"
    },
    {
      "title": "Amitsur complex",
      "url": "https://en.wikipedia.org/wiki/Amitsur_complex",
      "text": "In algebra, the Amitsur complex is a natural complex associated to a ring homomorphism that, when the homomorphism is a faithfully flat, is exact (thus determining a resolution). It was introduced in .\n\nThe notion should be thought of as a mechanism to go beyond the conventional localization of rings and modules: see .\n\n Definition \nLet  be a homomorphism of (not-necessary-commutative) rings. First define the cosimplicial set  as follows. Define the face maps  by inserting 1 at the i-th spot:Note the reference (M. Artin) seems to have a typo, and this should be the correct formula; see the calculation of s0 and d2 in the note.\n\nDefine the degeneracies  by multiplying out the i-th and (i + 1)-th spots:\n\nThey satisfy the \"obvious\" cosimplicial identities and thus  is a cosimplicial set. It then determines the complex with the augumentation , the Amitsur complex:\n\nwhere \n\n A theorem of Grothendieck \nIn the notations of #Definition, if  is right faithfully flat, then a theorem of Grothendieck states that the (augmented) complex  is exact and thus is a resolution. More generally,\n\nProof:\n\nStep 1: The statement is true if  splits as a ring homomorphism.\n\nThat \" splits\" is to say  for some homomorphism  ( is a retraction and  a section). Given such a , define\n\nby\n\nAn easy computation shows the following identity: with ,\n.\nThis is to say that h is a homotopy operator and so  determines the zero map on cohomology: i.e., the complex is exact.\n\nStep 2: The statement is true in general.\n\nWe remark that  is a section of . Thus, Step 1 applied to the split ring homomorphism  implies:\n\nwhere , is exact. Since , etc., by \"faithfully flat\", the original sequence is exact. \n\n References \n\nM. Artin, noncommutative rings\nShimshon Amitsur, “Simple algebras and cohomology groups of arbitrary fields,” Transactions of the American Mathematical Society Vol. 90, No. 1 (Jan., 1959), pp. 73–112\n\n \n\nCategory:Algebra"
    },
    {
      "title": "Antiisomorphism",
      "url": "https://en.wikipedia.org/wiki/Antiisomorphism",
      "text": "In category theory, a branch of mathematics, an antiisomorphism (or anti-isomorphism) between structured sets A and B is an isomorphism from A to the opposite of B (or equivalently from the opposite of A to B). If there exists an antiisomorphism between two structures, they are said to be antiisomorphic.\n\nIntuitively, to say that two mathematical structures are antiisomorphic is to say that they are basically opposites of one another.\n\nThe concept is particularly useful in an algebraic setting, as, for instance, when applied to rings.\n\nSimple example\nLet A be the binary relation (or directed graph) consisting of elements {1,2,3} and binary relation  defined as follows:\n \n \n \n\nLet B be the binary relation set consisting of elements {a,b,c} and binary relation  defined as follows:\n \n \n \n\nNote that the opposite of B (denoted Bop) is the same set of elements with the opposite binary relation  (that is, reverse all the arcs of the directed graph):\n \n \n \n\nIf we replace a, b, and c with 1, 2, and 3 respectively, we see that each rule in Bop is the same as some rule in A. That is, we can define an isomorphism  from A to Bop by .  is then an antiisomorphism between A and B.\n\nRing anti-isomorphisms\nSpecializing the general language of category theory to the algebraic topic of rings, we have:\nLet R and S be rings and f: R → S be a bijection. Then f is a ring anti-isomorphism if\n\nIf R = S then f is a ring anti-automorphism.\n\nAn example of a ring anti-automorphism is given by the conjugate mapping of quaternions:\n\nNotes\n\nReferences\n \n \n \n\nCategory:Morphisms\nCategory:Ring theory\nCategory:Algebra"
    },
    {
      "title": "Antivector",
      "url": "https://en.wikipedia.org/wiki/Antivector",
      "text": "An antivector is an element of grade  in an n-dimensional exterior algebra. An antivector is always a  blade, and it gets its name from the fact that its components each involve a combination of all except one basis vector, thus being the opposite of a vector whose components each involve exactly one basis vector. Like a vector, an antivector has n components in n-dimensional space, and this sometimes leads to an inadequate distinction being made between the two types of entities. However, antivectors transform differently with a change of basis than vectors do, which shows that they are different kinds of quantities. \n\nIn physics, the names pseudovector and axial vector are used to describe vectors that transform in the same way that an antivector transforms. These typically arise as the result of cross products between two vectors.\n\nSee also\nExterior algebra\nGeometric algebra\n\n References \n\nCategory:Algebra"
    },
    {
      "title": "Basic element",
      "url": "https://en.wikipedia.org/wiki/Basic_element",
      "text": "In algebra, a basic element x with respect to an element y is an element of a cochain complex  (e.g., complex of differential forms on a manifold) that is closed:  and the contraction of x by y is zero.\n\nCategory:Algebra"
    },
    {
      "title": "Binomial (polynomial)",
      "url": "https://en.wikipedia.org/wiki/Binomial_%28polynomial%29",
      "text": "In algebra, a binomial is a polynomial that is the sum of two terms, each of which is a monomial. It is the simplest kind of polynomial after the monomials.\n\nDefinition\nA binomial is a polynomial which is the sum of two monomials. A binomial in a single indeterminate (also known as a univariate binomial) can be written in the form\n\nwhere  and  are numbers, and  and  are distinct nonnegative integers and  is a symbol which is called an indeterminate or, for historical reasons, a variable. In the context of Laurent polynomials, a Laurent binomial, often simply called a binomial, is similarly defined, but the exponents  and  may be negative.\n\nMore generally, a binomial may be written as:\n\nSome examples of binomials are:\n\nOperations on simple binomials\nThe binomial  can be factored as the product of two other binomials:\n\nThis is a special case of the more general formula:\n\nWhen working over the complex numbers, this can also be extended to:\n\nThe product of a pair of linear binomials  and  is a trinomial:\n\nA binomial raised to the th power, represented as  can be expanded by means of the binomial theorem or, equivalently, using Pascal's triangle. For example, the square  of the binomial  is equal to the sum of the squares of the two terms and twice the product of the terms, that is:\n\nThe numbers (1, 2, 1) appearing as multipliers for the terms in this expansion are binomial coefficients two rows down from the top of Pascal's triangle. The expansion of the th power uses the numbers  rows down from the top of the triangle.\nAn application of above formula for the square of a binomial is the \"-formula\" for generating Pythagorean triples:\nFor , let , , and ; then .\n Binomials that are sums or differences of cubes can be factored into lower-order polynomials as follows:\n\nSee also\nCompleting the square\nBinomial distribution\nList of factorial and binomial topics (which contains a large number of related links)\n\n Notes \n\nReferences\n \n\nCategory:Algebra\nCategory:Factorial and binomial topics"
    },
    {
      "title": "Board puzzles with algebra of binary variables",
      "url": "https://en.wikipedia.org/wiki/Board_puzzles_with_algebra_of_binary_variables",
      "text": "Board puzzles with algebra of binary variables ask players to locate the hidden objects based on a set of clue cells and their neighbors marked as variables (unknowns). A variable with value of 1 corresponds to a cell with an object. Contrary, a variable with value of 0 corresponds to an empty cell—no hidden object.\n\nOverview\nThese puzzles are based on algebra with binary variables taking a pair of values, for example, (no, yes), (false, true), (not exists, exists), (0, 1). It invites the player quickly establish some equations, and  inequalities for the solution. The partitioning can be used to reduce the complexity of the problem. Moreover, if the puzzle is prepared in a way that there exists a unique solution only, this fact can be used to eliminate some variables without calculation.\n\nThe problem can be modeled as binary integer linear programming which is a special case of integer linear programming.Schrijver 1986\n\nHistory\nMinesweeper, along with its variants, is the most notable example of this type of puzzle.\n\nAlgebra with binary variables\n\nBelow the letters in the mathematical statements are used as variables where each can take the value either 0 or 1 only. A simple example of an equation with binary variables is given below:\n\na + b = 0\n\nHere there are two variables a and b but one equation. The solution is constrained by the fact that a and b can take only values 0 or 1. There is only one solution here, both a = 0, and b = 0. Another simple example is given below:\n\na + b = 2\n\nThe solution is straightforward: a and b must be 1 to make a + b equal to 2.\n\nAnother interesting case is shown below:\n\na + b + c = 2\n\na + b ≤ 1\n\nHere, the first statement is an equation and the second statement is an inequality indicating the three possible cases:\n\na = 1 and b = 0,\na = 0 and b = 1, and\na = 0 and b = 0,\n\nThe last case causes a contradiction on c by forcing c = 2, which is not possible. Therefore, either first or second case is correct. This leads to the fact that c must be 1.\n\nThe modification of a large equation into smaller form is not difficult. However, an equation set with binary variables cannot be always solved by applying linear algebra. The following is an example for applying the subtraction of two equations:\n\na + b + c + d = 3\n\nc + d = 1\n\nThe first statement has four variables whereas the second statement has only two variables. The latter one means that the sum of c and d is 1. Using this fact on the first statement, the equations above can be reduced to\n\na + b = 2\n\nc + d = 1\n\nThe algebra on a board\nthumb|right|alt=tentaizu_4x4_example|Figure 1: An example puzzle on 4x4 board\n\nA game based on the algebra with binary variables can be visualized in many different ways. One generic way is to represent the right side of an equation as a clue in a cell (clue cell), and the neighbors of a clue cell as variables. A simple case is shown in Figure 1. The neighbors can be assumed to be the up/down, left/right, and corner cells that are sharing an edge or a corner. The white cells may contain a hidden object or nothing. In other words, they are the binary variables.  They take place on the left side of the equations. Each clue cell, a cell with blue background in Figure 1, contains a positive number corresponding to the number of its neighbors that have hidden objects. The total number of the objects on the board can be given as an additional clue. The same board with variables marked is shown in Figure 2.\n\nThe reduction into a set of equations with binary variables\nThe main equation is written by using the total number of the hidden objects given. From the first figure this corresponds to the following equation\n\na + b + c + d + e + f + g + h + i + j + k + m = 3\n\nThe other equations are composed one by one for each clue cells:\n\na + b + c + e + f + h + i + j = 1\n\nf + g + j + m = 1\n\nh + i + j + k = 2\n\ni + j + m = 2\n\nAlthough there are several ways to solve the equations above, the following explicit way can be applied:\n\nIt is known from the equation set that i + j + m = 2. However, since j and m are neighbors of a cell with number 1, the following is true: j + m ≤ 1. This means that the variable i must be 1.\nSince i = 1 and the variable i is the neighbor to the clue cell with number 1, the variables a, b, c, e, f, h, and j must be zero. The same result can be obtained by replacing i = 1 into the second equation as follows: a + b + c + e + f + h + j = 0. This is equivalent to a = 0, b = 0, c = 0, e = 0, f = 0, h = 0, j = 0.\nFigure 3 is obtained after Step 1 and Step 2. The grayed cells with '–' are the variables with value 0. The cell with the symbol Δ corresponds to the variable with value 1. The variable k is the only neighbor of the left most clue cell with value 2. This clue cell has one neighbor with an object and only one remaining cell with variable k. Therefore, k must be 1.\nSimilarly, the variable m must be 1 too because it is the only remaining variable neighbor to the right most clue cell with value 2.\nSince k = 1, m = 1 and i = 1, we complete the marking of three hidden objects therefore d = 0, and g = 0. The final solution is given in Figure 4.\n\n thumb|upright|alt=tentaizu_4x4_example_with_variables|Figure 2: Binary variables are marked thumb|upright|alt=tentaizu_4x4_example_solved_partially|Figure 3: The example solved partially thumb|upright|alt=tentaizu_4x4_example_with_variables_solved|Figure 4: The example solved\n\nUse of uniqueness\n\nIn the example above (Figure 2), the variables a, b, c, and e are the neighbors of the clue cell 1 and they are not neighbors of any other cell. It is obvious that the followings are possible solutions:\n\na = 1, b = 0, c = 0, e = 0\na = 0, b = 1, c = 0, e = 0\na = 0, b = 0, c = 1, e = 0\na = 0, b = 0, c = 0, e = 1\n\nHowever, if the puzzle is prepared so that we should have one only one (unique) solution, we can set that all these variables a, b, c, and e must be 0. Otherwise there become more than one solutions.\n\nUse of partitioning\nthumb|right|alt=tentaizu_4x4_example_partitioned|Figure 5: An example for partitioning\n\nSome puzzle configurations may allow the player to use partitioningHalmos 1960 for complexity reduction. An example is given in Figure 5. Each partition corresponds to a number of the objects hidden. The sum of the hidden objects in the partitions must be equal to the total number of objects hidden on the board. One possible way to determine a partitioning is to choose the lead clue cells which have no common neighbors. The cells outside of the red transparent zones in Figure 5 must be empty. In other words, there are no hidden objects in the all-white cells. Since there must be a hidden object within the upper partition zone, the third row from top shouldn't contain a hidden object. This leads to the fact that the two variable cells on the bottom row around the clue cell must have hidden objects. The rest of the solution is straightforward.\n\nUse of try-and-check method\nthumb|left|alt=tentaizu_4x4_example_for_inconsistency|Figure 6: An example for try-and-check method\n\nAt some cases, the player can set a variable cell as 1 and check if any inconsistency occurs. The example in Figure 6 shows an inconsistency check. The cell marked with an hidden object Δ is under the test. Its marking leads to the set all the variables (grayed cells) to be 0. This follows the inconsistency. The clue cell marked red with value 1 does not have any remaining neighbor that can include a hidden object. Therefore, the cell under the test must not include a hidden object. In algebraic form we have two equations:\n\na + b + c + d = 1\n\na + b + c + d + e + f + g = 1\n\nHere a, b, c, and d correspond to the top four grayed cells in Figure 6. The cell with Δ is represented by the variable f, and the other two grayed cells are marked as e and g. If we set f = 1, then a = 0, b = 0, c = 0, d = 0, e = 0, g = 0. The first equation above will have the left hand side equal to 0 while the right hand side has 1. A contradiction.\n\nTry-and-check may need to be applied consequently in more than one step on some puzzles in order to reach a conclusion. This is equivalent to binary search algorithmDrozdek 2000 to eliminate possible paths which lead to inconsistency.\n\nComplexity\n\nBecause of binary variables, the equation set for the solution does not possess linearity property. In other words, the rank of the equation matrix may not always address the right complexity.\n\nThe complexity of this class of puzzles can be adjusted in several ways. One of the simplest method is to set a ratio of the number of the clue cells to the total number of the cells on the board. However, this may result a largely varying complexity range for a fixed ratio. Another method is to reduce clue cells based on some problem solving strategies step by step. The complex strategies may be enabled for high complexity levels such as subtracting an equation with another one, or the higher depth of try-and-check steps. When the board size increases, the range of the problem cases increases. The ratio of the number of hidden objects to the total number of cells affects the complexity of the puzzle too.\n\nSee also\nMinesweeper\n\nNotes\n\nReferences\nPaul Halmos, Naive set theory. Princeton, NJ: D. Van Nostrand Company, 1960. Reprinted by Springer-Verlag, New York, 1974.  (Springer-Verlag edition).\nAlexander Schrijver, Theory of Linear and Integer Programming. John Wiley & Sons, 1986. Reprinted in 1999. .\nAdam Drozdek, Data Structures and Algorithms in C++, Brooks/Cole, second edition, 2000. .\n\nExternal links\n Tentaizu Hex (free of charge web app)\n\nCategory:Puzzles\nCategory:Algebra\nCategory:Variables (mathematics)\nCategory:Equations"
    },
    {
      "title": "Boolean differential calculus",
      "url": "https://en.wikipedia.org/wiki/Boolean_differential_calculus",
      "text": "Boolean differential calculus (BDC) (German:  (BDK)) is a subject field of Boolean algebra discussing changes of Boolean variables and Boolean functions.\n\nThe Boolean differential calculus allows various aspects of dynamical systems theory like\n\n automata theory on finite automata\n Petri net theory\n supervisory control theory (SCT)\n\nto be discussed in a united and closed form and their specific advantages to be combined.\n\n History and applications \nOriginally inspired by the design and testing of switching circuits and the utilization of error-correcting codes in electrical engineering, the roots for the development of what later would evolve into the Boolean differential calculus were initiated by works of Irving S. Reed, David E. Muller, David A. Huffman, Sheldon B. Akers, Jr. and  (, ) between 1954 and 1959, and of Frederick F. Sellers, Jr., Mu-Yue Hsiao and Leroy W. Bearnson in 1968.\n\nSince then, significant advances were accomplished in both, the theory and in the application of the BDC in switching circuit design and logic synthesis.\n\nWorks of , Marc Davio and  in the 1970s formed the basics of BDC on which ,  and  further developed BDC into a self-contained mathematical theory later on.\n\nA complementary theory of Boolean integral calculus (German: ) has been developed as well.\n\nBDC has also found uses in discrete event dynamic systems (DEDS) in digital network communication protocols.\n\nMeanwhile BDC has seen extensions to multi-valued variables and functions as well as to lattices of Boolean functions.\n\n Overview \nBoolean differential operators play a significant role in BDC. They allow the application of differentials as known from classical analysis to be extended to logical functions.\n\nThe differentials  of a Boolean variable  models the relation:\n\n \n\nThere are no constraints in regard to the nature, the causes and consequences of a change.\n\nThe differentials  are binary. They can be used just like common binary variables.\n\n See also \n Boole's expansion theorem\n Ramadge–Wonham framework\n\n References \n\n Further reading \n  (14 pages)\n  (462 pages)\n  (9 pages) Translation of:  (9 pages)\n  (18 pages)\n  (NB. Also: Chemnitz, Technische Universität, Dissertation.) (147 pages)\n  (15 pages)\n  (392 pages)\n  (xxii+232 pages)  (NB. Per  this hardcover edition has been rereleased as softcover edition in 2010.)\n  (49 pages)\n  In  (24 of 153 pages)\n\nExternal links\n \n  with \n\nCategory:Algebra\nCategory:Automata (computation)\nCategory:Mathematical logic\nCategory:Order theory\nCategory:Set theory"
    },
    {
      "title": "Bose–Mesner algebra",
      "url": "https://en.wikipedia.org/wiki/Bose%E2%80%93Mesner_algebra",
      "text": "In mathematics, a Bose–Mesner algebra is a special set of matrices which arise from a combinatorial structure known as an association scheme, together with the usual set of rules for combining (forming the products of) those matrices, such that they form an associative algebra, or, more precisely, a unitary commutative algebra. Among these rules are:\nthe result of a product is also within the set of matrices,\nthere is an identity matrix in the set, and\ntaking products is commutative.\n\nBose–Mesner algebras have applications in physics to spin models, and in statistics to the design of experiments. They are named for R. C. Bose and Dale Marsh Mesner.Bose & Mesner (1959)\n\nDefinition\nLet X be a set of v elements. Consider a partition of the 2-element subsets of X into n non-empty subsets, R1, ..., Rn such that:\n given an , the number of  such that  depends only on i (and not on x). This number will be denoted by vi, and\n given  with , the number of  such that  and  depends only on i,j and k (and not on x and y). This number will be denoted by .\nThis structure is enhanced by adding all pairs of repeated elements of X and collecting them in a subset R0. This enhancement permits the parameters i, j, and k to take on the value of zero, and lets some of x,y or z be equal.\n\nA set with such an enhanced partition is called an association scheme. One may view an association scheme as a partition of the edges of a complete graph (with vertex set X) into n classes, often thought of as color classes. In this representation, there is a loop at each vertex and all the loops receive the same 0th color.\n\nThe association scheme can also be represented algebraically. Consider the matrices Di defined by:\n \n\nLet  be the vector space consisting of all matrices , with  complex.\n\nThe definition of an association scheme is equivalent to saying that the  are v × v (0,1)-matrices which satisfy\n\n  is symmetric,\n  (the all-ones matrix),\n \n \n\nThe (x,y)-th entry of the left side of 4. is the number of two colored paths of length two joining x and y (using \"colors\" i and j) in the graph. Note that the rows and columns of  contain  1s:\n\n \n\nFrom 1., these matrices are symmetric. From 2.,  are linearly independent, and the dimension of  is . From 4.,  is closed under multiplication, and multiplication is always associative. This associative commutative algebra  is called the Bose–Mesner algebra of the association scheme. Since the matrices in  are symmetric and commute with each other, they can be simultaneously diagonalized. This means that there is a matrix  such that to each  there is a diagonal matrix  with . This means that  is semi-simple and has a unique basis of primitive idempotents . These are complex n × n matrices satisfying\n\n \n\n \n\n \n\nThe Bose–Mesner algebra has two distinguished bases: the basis consisting of the adjacency matrices , and the basis consisting of the irreducible idempotent matrices . By definition, there exist well-defined complex numbers such that\n\n \n\nand\n\n \n\nThe p-numbers , and the q-numbers , play a prominent role in the theory. They satisfy well-defined orthogonality relations. The p-numbers are the eigenvalues of the adjacency matrix .\n\nTheorem\n\nThe eigenvalues of  and , satisfy the orthogonality conditions:\n\n \n\n \n\nAlso\n\n \n\nIn matrix notation, these are\n\n \n\n \n\nwhere \n\nProof of theorem\n\nThe eigenvalues of  are  with multiplicities . This implies that\n\n \n\nwhich proves Equation  and Equation ,\n\n \n\nwhich gives Equations ,  and .\n\nThere is an analogy between extensions of association schemes and extensions of finite fields. The cases we are most interested in are those where the extended schemes are defined on the -th Cartesian power  of a set  on which a basic association scheme  is defined. A first association scheme defined on  is called the -th Kronecker power  of . Next the extension is defined on the same set  by gathering classes of . The Kronecker power corresponds to the polynomial ring  first defined on a field , while the extension scheme corresponds to the extension field obtained as a quotient. An example of such an extended scheme is the Hamming scheme.\n\nAssociation schemes may be merged, but merging them leads to non-symmetric association schemes, whereas all usual codes are subgroups in symmetric Abelian schemes.\n\nSee also\n Association scheme\n\nNotes\n\nReferences\n \n \n \n \n \n \n \n \n \n\nCategory:Algebraic combinatorics\nCategory:Design of experiments\nCategory:Analysis of variance\nCategory:Representation theory\nCategory:Algebra"
    },
    {
      "title": "Boundedly generated group",
      "url": "https://en.wikipedia.org/wiki/Boundedly_generated_group",
      "text": "In mathematics, a  group is called boundedly generated  if it can be expressed as a finite product of cyclic subgroups. The property of bounded generation is also closely related with the congruence subgroup problem (see ).\n\n Definitions \n\nA group G is called boundedly generated if there exists a finite subset S of G and a positive integer m such that every element g of G can be represented as a product of at most m powers of the elements of S:\n \n  where  and  are integers.\n\nThe finite set S generates G, so  a boundedly generated group is finitely generated.\n\nAn equivalent definition can be given in terms of cyclic subgroups. A group G is called boundedly generated if there is a finite family C1, …, CM of not necessarily distinct cyclic subgroups such that G = C1…CM as a set.\n\n Properties \n\n Bounded generation is unaffected by passing to a subgroup of finite index: if H is a finite index subgroup of G then G is boundedly generated if and only if H is boundedly generated.\n Any quotient group of a boundedly generated group is also boundedly generated.\n A finitely generated torsion group must be finite if it is boundedly generated; equivalently, an infinite finitely generated torsion group is not boundedly generated.\n\nA pseudocharacter on a discrete group G is defined to be a real-valued function f on a G such that \n f(gh) − f(g) − f(h) is uniformly bounded and f(gn) = n·f(g).\n\n The vector space of pseudocharacters of a boundedly generated group G is finite-dimensional.\n\n Examples \n If n ≥ 3, the group SLn(Z) is boundedly generated by its elementary subgroups, formed by matrices differing from the identity matrix only in one off-diagonal entry. In 1984, Carter and Keller gave an elementary proof of this result, motivated by a question in algebraic K-theory.\n A free group on at least two generators is not boundedly generated (see below).\n The group SL2(Z) is not boundedly generated, since it contains a free subgroup with two generators of index 12.\n A Gromov-hyperbolic group is boundedly generated if and only if it is virtually cyclic (or elementary), i.e. contains a cyclic subgroup of finite index.\n\n Free groups are not boundedly generated \nSeveral authors have stated in the mathematical literature that it is obvious that finitely generated free groups are not boundedly generated. This section contains various obvious and less obvious ways of proving this. Some of the methods, which touch on bounded cohomology, are important because they are geometric rather than algebraic, so can be applied to a wider class of groups, for example Gromov-hyperbolic groups.\n\nSince for any n ≥ 2, the free group on 2 generators F2  contains the free group on n generators Fn as a subgroup of finite index (in fact n – 1), once one non-cyclic free group on finitely many generators is known to be not boundedly generated, this will be true for all of them. Similarly, since SL2(Z) contains  F2  as a subgroup of index 12, it is enough to consider SL2(Z). In other words, to show that no Fn with n ≥ 2 has bounded generation, it is sufficient to prove this for one of them or even just for SL2(Z) .\n\nBurnside couterexamples\nSince bounded generation is preserved under taking homomorphic images, if a single finitely generated group with at least two generators is known to be not boundedly generated, this will be true for the free group on the same number of generators, and hence for all free groups. To show that no (non-cyclic) free group has bounded generation, it is therefore enough to produce one example of a finitely generated group which is not boundedly generated, and any finitely generated infinite torsion group will work. The existence of such groups constitutes Golod and Shafarevich's negative solution of the generalized Burnside problem in 1964; later, other explicit examples of infinite finitely generated torsion groups were constructed by Aleshin, Olshanskii, and Grigorchuk, using automata. Consequently, free groups of rank at least two are not boundedly generated.\n\nSymmetric groups\nThe symmetric group Sn can be generated by two elements, a 2-cycle and an n-cycle, so that it is a quotient group of  F2. On the other hand, it is easy to show that the maximal order M(n) of an element in Sn satisfies\n\n log M(n) ≤ n/e\n\n(Edmund Landau proved the more precise asymptotic estimate log M(n) ~ (n log n)1/2).  In fact if the cycles in a cycle decomposition of a permutation have length N1, ..., Nk with N1 + ··· + Nk = n, then the order of the permutation divides the product N1 ···Nk, which in turn is bounded by (n/k)k, using the inequality of arithmetic and geometric means. On the other hand, (n/x)x is maximized when x=e. If F2 could be written as a product of m cyclic subgroups, then necessarily n! would have to be less than or equal to M(n)m for all n, contradicting Stirling's asymptotic formula.\n\nHyperbolic geometry\nThere is also a simple geometric proof that G = SL2(Z) is not boundedly generated. It acts by Möbius transformations on the upper half-plane H, with the Poincaré metric. Any compactly supported 1-form α on a fundamental domain of G extends uniquely to a G-invariant 1-form on H. If z is in H and γ is the geodesic from z to g(z), the function defined by\n \n\nsatisfies the first condition for a pseudocharacter since by the Stokes theorem\n \n\nwhere Δ is the geodesic triangle with vertices z, g(z) and h−1(z), and geodesics triangles have area bounded by π. The homogenized function\n\ndefines a pseudocharacter, depending only on α. As is well known from the theory of dynamical systems, any orbit (gk(z)) of a hyperbolic element g has limit set consisting of two fixed points on the extended real axis; it follows that the geodesic segment from z to g(z) cuts through only finitely many translates of the fundamental domain. It is therefore easy to choose α so that fα equals one on a given hyperbolic element and vanishes on a finite set of other hyperbolic elements with distinct fixed points. Since G therefore has an infinite-dimensional space of pseudocharacters, it cannot be boundedly generated.\n\nDynamical properties of hyperbolic elements can similarly be used to prove that any non-elementary Gromov-hyperbolic group is not boundedly generated.\n\nBrooks pseudocharacters\nRobert Brooks gave a combinatorial scheme to produce pseudocharacters of any free group Fn; this scheme was later shown to yield\nan infinite-dimensional family of pseudocharacters (see ). Epstein and Fujiwara later extended these results to all non-elementary Gromov-hyperbolic groups.\n\nGromov boundary\nThis simple folklore proof uses dynamical properties of the action of hyperbolic elements on the Gromov boundary of a Gromov-hyperbolic group. For the special case of the free group Fn, the boundary (or space of ends) can be identified with the space X of semi-infinite reduced words\n\ng1 g2 ···\n\nin the generators and their inverses. It gives a natural compactification of the tree, given by the Cayley graph with respect to the generators. A sequence of semi-infinite words converges to another such word provided that the initial segments agree after a certain stage, so that X is compact (and metrizable). The free group acts by left multiplication on the semi-infinite words. Moreover, any element g in Fn has exactly two fixed points g±∞, namely the reduced infinite words given by the limits of gn as n tends to ±∞. Furthermore, gn·w tends to g±∞ as n tends to ±∞ for any semi-infinite word w; and more generally if wn tends to w≠ g ±∞, then gn·wn tends to g+∞ as n tends to ∞.\n\nIf Fn were boundedly generated, it could be written as a product of cyclic groups Ci\ngenerated by elements hi. Let X0 be the countable subset given by the finitely many Fn-orbits\nof the fixed points hi ±∞, the fixed points of the hi and all their conjugates. Since X is uncountable, there\nis an element of g with fixed points outside X0 and a point w outside X0 different from these fixed points. Then for\nsome subsequence (gm) of (gn)\n\ngm = h1n(m,1) ··· hkn(m,k), with each n(m,i) constant or strictly monotone.\n\nOn the one hand, by successive use of the rules for computing limits of the form hn·wn, the limit of the right hand side applied to x is necessarily a fixed point of one of the conjugates of the hi's. On the other hand, this limit also must be g+∞, which is not one of these points, a contradiction.\n\n References \n\n (see pages 222-229, also available on the Cornell archive)\n\n.\n\nCategory:Algebra\nCategory:Geometric group theory"
    },
    {
      "title": "Brahmagupta–Fibonacci identity",
      "url": "https://en.wikipedia.org/wiki/Brahmagupta%E2%80%93Fibonacci_identity",
      "text": "In algebra, the Brahmagupta–Fibonacci identityhttp://www.cut-the-knot.org/m/Algebra/BrahmaguptaFibonacci.shtmlMarc Chamberland: Single Digits: In Praise of Small Numbers. Princeton University Press, 2015, , p. 60 expresses the product of two sums of two squares as a sum of two squares in two different ways. Hence the set of all sums of two squares is closed under multiplication. Specifically, the identity says\n\nFor example,\n\nThe identity is also known as the Diophantus identity,Daniel Shanks, Solved and unsolved problems in number theory, p.209, American Mathematical Society, Fourth edition 1993.  as it was first proved by Diophantus of Alexandria.  It is a special case of Euler's four-square identity, and also of Lagrange's identity. \n\nBrahmagupta proved and used a more general identity (the Brahmagupta identity), equivalent to\n\nThis shows that, for any fixed A, the set of all numbers of the form x2 + A y2 is closed under multiplication.\n\nThe identity holds in the ring of integers, the ring of rational numbers and, more generally, any commutative ring.  All four forms of the identity can be verified by expanding each side of the equation.  Also, (2) can be obtained from (1), or (1) from (2), by changing b to −b, and likewise with (3) and (4).\n\nHistory\nThe identity is actually first found in Diophantus' Arithmetica (III, 19), of the third century A.D.\nIt was rediscovered by Brahmagupta (598–668), an Indian mathematician and astronomer, who generalized it (to the Brahmagupta identity) and used it in his study of what is now called Pell's equation. His Brahmasphutasiddhanta was translated from Sanskrit into Arabic by Mohammad al-Fazari, and was subsequently translated into Latin in 1126. The identity later appeared in Fibonacci's Book of Squares in 1225.\n\nRelated identities\n\nAnalogous identities are Euler's four-square related to quaternions, and Degen's eight-square derived from the octonions which has connections to Bott periodicity. There is also Pfister's sixteen-square identity, though it is no longer bilinear.\n\n Multiplication of complex numbers \n\nIf a, b, c, and d are real numbers, the Brahmagupta–Fibonacci identity is equivalent to the multiplicativity property for absolute values of complex numbers:\n\nThis can be seen as follows: expanding the right side and squaring both sides, the multiplication property is equivalent to\n\nand by the definition of absolute value this is in turn equivalent to\n\nAn equivalent calculation in the case that the variables a, b, c, and d are rational numbers shows the identity may be interpreted as the statement that the norm in the field Q(i) is multiplicative: the norm is given by \n \nand the multiplicativity calculation is the same as the preceding one.\n\n Application to Pell's equation \nIn its original context, Brahmagupta applied his discovery of this identity to the solution of Pell's equation x2 − Ay2 = 1. Using the identity in the more general form\n\nhe was able to \"compose\" triples (x1, y1, k1) and (x2, y2, k2) that were solutions of x2 − Ay2 = k, to generate the new triple\n\nNot only did this give a way to generate infinitely many solutions to x2 − Ay2 = 1 starting with one solution, but also, by dividing such a composition by k1k2, integer or \"nearly integer\" solutions could often be obtained. The general method for solving the Pell equation given by Bhaskara II in 1150, namely the chakravala (cyclic) method, was also based on this identity.\n\n Writing integers as a sum of two squares \nWhen used in conjunction with one of Fermat's theorems, the Brahmagupta–Fibonacci identity proves that the product of a square and any number of primes of the form 4n + 1 is a sum of two squares.\n\nSee also\n\n Brahmagupta matrix\n Indian mathematics\n List of Indian mathematicians\n Sum of two squares theorem\n\nNotes\n\nReferences\n\nExternal links\nBrahmagupta's identity at PlanetMath\nBrahmagupta Identity on MathWorld\n A Collection of Algebraic Identities\nru:Брахмагупта#Тождество Брахмагупты\n\nCategory:Algebra\nCategory:Brahmagupta\nCategory:Elementary algebra\nCategory:Mathematical identities\nCategory:Squares in number theory"
    },
    {
      "title": "Brahmagupta's identity",
      "url": "https://en.wikipedia.org/wiki/Brahmagupta%27s_identity",
      "text": "In algebra, Brahmagupta's identity says that the product of two numbers of the form  is itself a number of that form. In other words, the set of such numbers is closed under multiplication. Specifically:\n\nBoth (1) and (2) can be verified by expanding each side of the equation.  Also, (2) can be obtained from (1), or (1) from (2), by changing b to −b.\n\nThis identity holds in both the ring of integers and the ring of rational numbers, and more generally in any commutative ring.\n\nHistory\nThe identity is a generalization of the so-called Fibonacci identity (where n=1) which is actually found in Diophantus' Arithmetica (III, 19).\nThat identity was rediscovered by Brahmagupta (598–668), an Indian mathematician and astronomer, who generalized it and used it in his study of what is now called Pell's equation. His Brahmasphutasiddhanta was translated from Sanskrit into Arabic by Mohammad al-Fazari, and was subsequently translated into Latin in 1126.George G. Joseph (2000). The Crest of the Peacock, p. 306. Princeton University Press. . The identity later appeared in Fibonacci's Book of Squares in 1225.\n\n Application to Pell's equation \nIn its original context, Brahmagupta applied his discovery to the solution of what was later called Pell's equation, namely x2 − Ny2 = 1. Using the identity in the form\n\nhe was able to \"compose\" triples (x1, y1, k1) and (x2, y2, k2) that were solutions of x2 − Ny2 = k, to generate the new triple\n\nNot only did this give a way to generate infinitely many solutions to x2 − Ny2 = 1 starting with one solution, but also, by dividing such a composition by k1k2, integer or \"nearly integer\" solutions could often be obtained. The general method for solving the Pell equation given by Bhaskara II in 1150, namely the chakravala (cyclic) method, was also based on this identity.\n\nSee also\n Brahmagupta matrix\n Brahmagupta–Fibonacci identity\n Brahmagupta's interpolation formula\n Indian mathematics\n List of Indian mathematicians\n\nReferences\n\nExternal links\nBrahmagupta's identity at PlanetMath\nBrahmagupta Identity on MathWorld\n A Collection of Algebraic Identities\n\nCategory:Algebra\nCategory:Elementary algebra\nCategory:Mathematical identities\nCategory:Brahmagupta"
    },
    {
      "title": "Calabi–Yau algebra",
      "url": "https://en.wikipedia.org/wiki/Calabi%E2%80%93Yau_algebra",
      "text": "In algebra, a Calabi–Yau algebra was introduced by Victor Ginzburg to transport the geometry of a Calabi–Yau manifold to noncommutative algebraic geometry.\n\n References \n\nCategory:Algebra"
    },
    {
      "title": "Canonical form",
      "url": "https://en.wikipedia.org/wiki/Canonical_form",
      "text": "thumb|Algorithmic anagram test using multisets as canonical forms: The strings \"madam curie\" and \"radium came\" are given as C arrays. Each one is converted into a canonical form by sorting. Since both sorted strings literally agree, the original strings were anagrams of each other.\nIn mathematics and computer science, a canonical, normal, or standard  form of a mathematical object is a standard way of presenting that object as a mathematical expression. The distinction between \"canonical\" and \"normal\" forms varies by subfield. In most fields, a canonical form specifies a unique representation for every object, while a normal form simply specifies its form, without the requirement of uniqueness.\n\nThe canonical form of a positive integer in decimal representation is a finite sequence of digits that does not begin with zero.\n\nMore generally, for a class of objects on which an equivalence relation is defined, a canonical form consists in the choice of a specific object in each class. For example, Jordan normal form is a canonical form for matrix similarity, and the row echelon form is a canonical form, when one considers as equivalent a matrix and its left product by an invertible matrix.\n\nIn computer science, and more specifically in computer algebra, when representing mathematical objects in a computer, there are usually many different ways to represent the same object. In this context, a canonical form is a representation such that every object has a unique representation. Thus, the equality of two objects can easily be tested by testing the equality of their canonical forms. However canonical forms frequently depend on arbitrary choices (like ordering the variables), and this introduces difficulties for testing the equality of two objects resulting on independent computations. Therefore, in computer algebra, normal form is a weaker notion: A normal form is a representation such that zero is uniquely represented. This allows testing for equality by putting the difference of two objects in normal form.\n\nCanonical form can also mean a differential form that is defined in a natural (canonical) way.\n\nIn computer science, data that has more than one possible representation can often be canonicalized into a completely unique representation called its canonical form.  Putting something into canonical form is canonicalization.The term 'canonization' is sometimes incorrectly used for this.\n\nDefinition\nSuppose we have some set S of objects, with an equivalence relation R. A canonical form is given by designating some objects of S to be \"in canonical form\", such that every object under consideration is equivalent to exactly one object in canonical form. In other words, the canonical forms in S represent the equivalence classes, once and only once. To test whether two objects are equivalent, it then suffices to test their canonical forms for equality.\nA canonical form thus provides a classification theorem and more, in that it not just classifies every class, but gives a distinguished (canonical) representative.\n\nFormally, a canonicalization with respect to an equivalence relation R on a set S is a mapping c:S→S such that for all s, s1, s2 ∈ S:\n c(s) = c(c(s))   (idempotence),\n s1 R s2 if and only if c(s1) = c(s2)   (decisiveness), and \n s R c(s)   (representativeness). \nProperty 3 is redundant, it follows by applying 2 to 1.\n\nIn practical terms, one wants to be able to recognize the canonical forms. There is also a practical, algorithmic question to consider: how to pass from a given object s in S to its canonical form s*? Canonical forms are generally used to make operating with equivalence classes more effective. For example, in modular arithmetic, the canonical form for a residue class is usually taken as the least non-negative integer in it. Operations on classes are carried out by combining these representatives and then reducing the result to its least non-negative residue.\nThe uniqueness requirement is sometimes relaxed, allowing the forms to be unique up to some finer equivalence relation, like allowing reordering of terms (if there is no natural ordering on terms).\n\nA canonical form may simply be a convention, or a deep theorem.\n\nFor example, polynomials are conventionally written with the terms in descending powers: it is more usual to write x2 + x + 30 than x + 30 + x2, although the two forms define the same polynomial. By contrast, the existence of Jordan canonical form for a matrix is a deep theorem.\n\nExamples\nNote: in this section, \"up to\" some equivalence relation E means that the canonical form is not unique in general, but that if one object has two different canonical forms, they are E-equivalent.\n\n Large number notation \n\nStandard form is used by many mathematicians and scientists to write extremely large numbers in a more concise and understandable way.\n\n Number theory \n Canonical representation of a positive integer\n Canonical form of a continued fraction\n\n Linear algebra \n Objects A is equivalent to B if: Normal form Notes Normal matrices over the complex numbers  for some unitary matrix U  Diagonal matrices (up to reordering) This is the Spectral theorem Matrices over the complex numbers  for some unitary matrices U and V Diagonal matrices with real positive entries (in descending order) Singular value decomposition Matrices over an algebraically closed field  for some invertible matrix P  Jordan normal form (up to reordering of blocks) Matrices over an algebraically closed field  for some invertible matrix P  Weyr canonical form (up to reordering of blocks) Matrices over a field  for some invertible matrix P Frobenius normal form Matrices over a principal ideal domain  for some invertible Matrices P and Q Smith normal form The equivalence is the same as allowing invertible elementary row and column transformations Matrices over the integers  for some unimodular matrix U Hermite normal form Finite-dimensional vector spaces over a field K A and B are isomorphic as vector spaces , n a non-negative integer\n\n Algebra \n Objects A is equivalent to B if: Normal form Finitely generated R-modules with R a principal ideal domain A and B are isomorphic as R-modules Primary decomposition (up to reordering) or invariant factor decomposition\n\n Geometry \nIn analytic geometry:\nThe equation of a line: Ax + By = C, with  A2 + B2 = 1 and C ≥ 0\nThe equation of a circle: \n\nBy contrast, there are alternative forms for writing equations. For example, the equation of a line may be written as a linear equation in point-slope and slope-intercept form.\n\nConvex polyhedra can be put into canonical form such that:\n All faces are flat,\n All edges are tangent to the unit sphere, and\n The centroid of the polyhedron is at the origin.\nIntegrable systems\nEvery differentiable manifold has a cotangent bundle. That bundle can always be endowed with a certain differential form, called the canonical one-form. This form gives the cotangent bundle the structure of a symplectic manifold. This allows vector fields on the manifold to be integrated by means of the Euler-Lagrange equations, or by means of Hamiltonian mechanics. Such systems of integrable differential equations are called integrable systems.\n\n Dynamical systems \nThe study of dynamical systems overlaps with that of integrable systems; there one has the idea of a normal form (dynamical systems).\n\n Three dimensional geometry \nIn the study of manifolds in three dimensions, one has the first fundamental form, the second fundamental form and the third fundamental form.\n\n Functional analysis \n Objects A is equivalent to B if: Normal form Hilbert spaces If A and B are both separable Hilbert spaces of infinite dimension, then A and B are isometrically isomorphic.  sequence spaces (up to exchanging the index set I with another index set of the same cardinality) Commutative -algebras with unit A and B are isomorphic as -algebras The algebra  of continuous functions on a compact Hausdorff space, up to homeomorphism of the base space.\n\n Classical logic \n\n Negation normal form\n Conjunctive normal form\n Disjunctive normal form\n Algebraic normal form\n Prenex normal form\n Skolem normal form\n Blake canonical form, also known as the complete sum of prime implicants, the complete sum, or the disjunctive prime form\n\n Set theory \n Cantor normal form of an ordinal number\n\n Game theory \n Normal form game\n\n Proof theory \n Normal form (natural deduction)\n\nRewriting systems\n\nThe symbolic manipulation of a formula from one form to another is called a \"rewriting\" of that formula. One can study the abstract properties of rewriting generic formulas; one need only specify a collection of rules by which formulas can be validly manipulated.  These are the \"rewriting rules\", they form an abstract rewriting system.  A common question is whether it is possible to bring some generic expression to a single, common form, the normal form. If different sequences of rewrites still result in the same form, then that form can be termed a normal form; and the rewrite is called confluent. It is not always possible to obtain a normal form.\n\n Lambda calculus\n A lambda term is in beta normal form if no beta reduction is possible; lambda calculus is a particular case of an abstract rewriting system. In the untyped lambda calculus, e.g., the term  doesn't have a normal form. In the typed lambda calculus, every well-formed term can be rewritten to its normal form.\n\nGraph theory\n\nIn graph theory, a branch of mathematics, graph canonization is the problem of finding a canonical form of a given graph G. A canonical form is a labeled graph Canon(G) that is isomorphic to G, such that every graph that is isomorphic to G has the same canonical form as G. Thus, from a solution to the graph canonization problem, one could also solve the problem of graph isomorphism: to test whether two graphs G and H are isomorphic, compute their canonical forms Canon(G) and Canon(H), and test whether these two canonical forms are identical.\n\n Computing \nIn computing, the reduction of data to any kind of canonical form is commonly called data normalization.\n\nFor instance, database normalization is the process of organizing the fields and tables of a relational database to minimize redundancy and dependency. \n\nIn the field of software security, a common vulnerability is unchecked malicious input. The mitigation for this problem is proper input validation. Before input validation may be performed, the input must be normalized, i.e., eliminating encoding (for instance HTML encoding) and reducing the input data to a single common character set.\n\nOther forms of data, typically associated with signal processing (including audio and imaging) or machine learning, can be normalized in order to provide a limited range of values.\n\nSee also\n Canonicalization\n Canonical basis\n Canonical class\n Normalization (disambiguation)\n Standardization\n\nNotes\n\nReferences\n.\n.\n\nCategory:Algebra\nCategory:Concepts in logic\nCategory:Mathematical terminology\nCategory:Formalism (deductive)\n\nnl:Normaalvorm"
    },
    {
      "title": "Classical Lie algebras",
      "url": "https://en.wikipedia.org/wiki/Classical_Lie_algebras",
      "text": "The classical Lie algebras are finite-dimensional Lie algebras that can be classified into four types:  and . These types are defined as follows:\n\n  – The special linear Lie algebra, \n\n  – The odd-dimensional orthogonal Lie algebra, \n\n  – The symplectic Lie algebra, \n\n  – The even-dimensional orthogonal Lie algebra, \n\nwhere  is the general Lie algebra of matrices  by  with coefficients in  or ,   is the identity matrix of dimension ,  denotes transposition and .\nExcept for the low-dimensional cases  and , the classical Lie algebras are simple.\n\nThe Moyal algebra is an infinite-dimensional Lie algebra that contains all classical Lie algebras as subalgebras.\n\nReferences\n\nCategory:Algebra"
    },
    {
      "title": "Closed-form expression",
      "url": "https://en.wikipedia.org/wiki/Closed-form_expression",
      "text": "In mathematics, a closed-form expression is a mathematical expression that can be evaluated in a finite number of operations. It may contain constants, variables, certain \"well-known\" operations (e.g., + − × ÷), and functions (e.g., nth root, exponent, logarithm, trigonometric functions, and inverse hyperbolic functions), but usually no limit. The set of operations and functions admitted in a closed-form expression may vary with author and context.\n\n Example: roots of polynomials \n\nThe solutions of any quadratic equation with complex coefficients can be expressed in closed form in terms of addition, subtraction, multiplication, division, and square root extraction, each of which is an elementary function. For example, the quadratic equation\n\nis tractable since its solutions can be expressed as a closed-form expression, i.e. in terms of elementary functions:\n\nSimilarly solutions of cubic and quartic (third and fourth degree) equations can be expressed using arithmetic, square roots, and cube roots, or alternatively using arithmetic and trigonometric functions. However, there are quintic equations without closed-form solutions using elementary functions, such as x5 − x + 1 = 0.\n\nAn area of study in mathematics referred to broadly as Galois theory involves proving that no closed-form expression exists in certain contexts, based on the central example of closed-form solutions to polynomials.\n\n Alternative definitions \n\nChanging the definition of \"well-known\" to include additional functions can change the set of equations with closed-form solutions.  Many cumulative distribution functions cannot be expressed in closed form, unless one considers special functions such as the error function or gamma function to be well known.  It is possible to solve the quintic equation if general hypergeometric functions are included, although the solution is far too complicated algebraically to be useful.  For many practical computer applications, it is entirely reasonable to assume that the gamma function and other special functions are well-known since numerical implementations are widely available.\n\n Analytic expression \n\nAn analytic expression (or expression in analytic form) is a mathematical expression constructed using well-known operations that lend themselves readily to calculation. Similar to closed-form expressions, the set of well-known functions allowed can vary according to context but always includes the basic arithmetic operations (addition, subtraction, multiplication, and division), exponentiation to a real exponent (which includes extraction of the th root), logarithms, and trigonometric functions.\n\nHowever, the class of expressions considered to be analytic expressions tends to be wider than that for closed-form expressions. In particular, special functions such as the Bessel functions and the gamma function are usually allowed, and often so are infinite series and continued fractions. On the other hand, limits in general, and integrals in particular, are typically excluded.\n\nIf an analytic expression involves only the algebraic operations (addition, subtraction, multiplication, division, and exponentiation to a rational exponent) and rational constants then it is more specifically referred to as an algebraic expression.\n\n Comparison of different classes of expressions \n\nClosed-form expressions are an important sub-class of analytic expressions, which contain a bounded or an unbounded number of applications of well-known functions. Unlike the broader analytic expressions, the closed-form expressions do not include infinite series or continued fractions; neither includes integrals or limits. Indeed, by the Stone–Weierstrass theorem, any continuous function on the unit interval can be expressed as a limit of polynomials, so any class of functions containing the polynomials and closed under limits will necessarily include all continuous functions.\n\nSimilarly,  an equation or system of equations is said to have a closed-form solution if, and only if, at least one solution can be expressed as a closed-form expression; and it is said to have an analytic solution if and only if at least one solution can be expressed as an analytic expression. There is a subtle distinction between a \"closed-form function\" and a \"closed-form number\" in the discussion of a \"closed-form solution\", discussed in  and below. A closed-form or analytic solution is sometimes referred to as an explicit solution.\n\n Dealing with non-closed-form expressions \n\n Transformation into closed-form expressions \n\nThe expression:\n\nis not in closed form because the summation entails an infinite number of elementary operations. However, by summing a geometric series this expression can be expressed in the closed form:\n\n Differential Galois theory \n\nThe integral of a closed-form expression may or may not itself be expressible as a closed-form expression. This study is referred to as differential Galois theory, by analogy with algebraic Galois theory.\n\nThe basic theorem of differential Galois theory is due to Joseph Liouville in the 1830s and 1840s and hence referred to as Liouville's theorem.\n\nA standard example of an elementary function whose antiderivative does not have a closed-form expression is:\n\nwhose antiderivative is (up to constants) the error function:\n\n Mathematical modelling and computer simulation \n\nEquations or systems too complex for closed-form or analytic solutions can often be analysed by mathematical modelling and computer simulation.\n\n Closed-form number \n\nThree subfields of the complex numbers C have been suggested as encoding the notion of a \"closed-form number\"; in increasing order of generality, these are the Liouville numbers, EL numbers and elementary numbers. The Liouville numbers, denoted L (not to be confused with Liouville numbers in the sense of rational approximation), form the smallest algebraically closed subfield of C closed under exponentiation and logarithm (formally, intersection of all such subfields)—that is, numbers which involve explicit exponentiation and logarithms, but allow explicit and implicit polynomials (roots of polynomials); this is defined in . L was originally referred to as elementary numbers, but this term is now used more broadly to refer to numbers defined explicitly or implicitly in terms of algebraic operations, exponentials, and logarithms. A narrower definition proposed in , denoted E, and referred to as EL numbers, is the smallest subfield of C closed under exponentiation and logarithm—this need not be algebraically closed, and correspond to explicit algebraic, exponential, and logarithmic operations. \"EL\" stands both for \"Exponential-Logarithmic\" and as an abbreviation for \"elementary\".\n\nWhether a number is a closed-form number is related to whether a number is transcendental. Formally, Liouville numbers and elementary numbers contain the algebraic numbers, and they include some but not all transcendental numbers. In contrast, EL numbers do not contain all algebraic numbers, but do include some transcendental numbers. Closed-form numbers can be studied via transcendental number theory, in which a major result is the Gelfond–Schneider theorem, and a major open question is Schanuel's conjecture.\n\n Numerical computations \n\nFor purposes of numeric computations, being in closed form is not in general necessary, as many limits and integrals can be efficiently computed.\n\n Conversion from numerical forms \n\nThere is software that attempts to find closed-form expressions for numerical values, including RIES, identify in Maple and SymPy, Plouffe's Inverter, and the Inverse Symbolic Calculator.\n\n See also \n Direct method\n Algebraic solution\n Finitary operation\n Numerical solution\n Computer simulation\n Symbolic regression\n Term (logic)\n Liouvillian function\n Elementary function\n\nReferences\n\n Further reading \n \n \n \n\n External links \n \n\nCategory:Algebra\nCategory:Special functions"
    },
    {
      "title": "Co-Hopfian group",
      "url": "https://en.wikipedia.org/wiki/Co-Hopfian_group",
      "text": "In the mathematical subject of group theory, a co-Hopfian group is a group that is not isomorphic to any of its proper subgroups. The notion is dual to that of a Hopfian group, named after Heinz Hopf. Wilhelm Magnus, Abraham Karrass, Donald Solitar, Combinatorial group theory. Presentations of groups in terms of generators and relations, Reprint of the 1976 second edition, Dover Publications, Inc., Mineola, NY, 2004. \n\nFormal definition \n\nA group G is called co-Hopfian if whenever  is an injective group homomorphism then   is bijective, that is .P. de la Harpe, Topics in geometric group theory. Chicago Lectures in Mathematics. University of Chicago Press, Chicago, IL, 2000. ; p. 58\n\nExamples and non-examples\n\nEvery finite group G is co-Hopfian.\nThe infinite cyclic group  is not co-Hopfian since  is an injective but non-surjective homomorphism.\nThe additive group of real numbers  is not co-Hopfian, since   is an infinite-dimensional vector space over  and therefore, as a group . \nThe additive group of rational numbers  and the quotient group  are co-Hopfian.\nThe multiplicative group  of nonzero rational numbers is not co-Hopfian, since the map  is an injective but non-surjective homomorphism. In the same way, the group  of positive rational numbers is not co-Hopfian.\nThe multiplicative group  of nonzero complex numbers is not co-Hopfian.\nFor every  the free abelian group  is not co-Hopfian.\nFor every  the free group  is not co-Hopfian.\nThere exists a finitely generated non-elementary (that is, not virtually cyclic)  virtually free group which is co-Hopfian. Thus a subgroup of finite index in a finitely generated co-Hopfian group need not be co-Hopfian, and being co-Hopfian is not a quasi-isometry invariant for finitely generated groups.\nBaumslag–Solitar groups , where , are not co-Hopfian.\nIf G is the fundamental group of a closed aspherical manifold with nonzero Euler characteristic (or with nonzero simplicial volume or nonzero L2-Betti number), then G is co-Hopfian.\nIf G is the fundamental group of a closed connected oriented irreducible 3-manifold M then G is co-Hopfian if and only if no finite cover of M is a torus bundle over the circle or the product of a circle and a closed surface.Shi Cheng Wang, and Ying Qing Wu, Covering invariants and co-Hopficity of 3-manifold groups.\nProceedings of the London Mathematical Society 68 (1994), no. 1, pp. 203–224 \nIf G is an irreducible lattice in a real semi-simple Lie group and G is not a virtually free group then G is co-Hopfian.Gopal Prasad\nDiscrete subgroups isomorphic to lattices in semisimple Lie groups.  American Journal of Mathematics 98 (1976), no. 1, 241–261 E.g. this fact applies to the group  for .\nIf G is a one-ended torsion-free word-hyperbolic group then G is co-Hopfian, by a result of Sela.Zlil Sela, \nStructure and rigidity in (Gromov) hyperbolic groups and discrete groups in rank 1 Lie groups. II.\nGeometric and Functional Analysis 7 (1997), no. 3, pp. 561–593 \nIf G is the fundamental group of a complete finite volume smooth Riemannian n-manifold (where n > 2) of pinched negative curvature then G is co-Hopfian. I. Belegradek, \nOn Mostow rigidity for variable negative curvature. Topology 41 (2002), no. 2, pp. 341–361 \nThe mapping class group of a closed hyperbolic surface is co-Hopfian.Nikolai Ivanov and John McCarthy, On injective homomorphisms between Teichmüller modular groups. I. Inventiones Mathematicae 135 (1999), no. 2, pp. 425–486 \nThe group Out(Fn) (where n>2) is co-Hopfian.Benson Farb and Michael Handel,\nCommensurations of Out(Fn), Publications Mathématiques de l'IHÉS 105 (2007), pp. 1–48\nDelzant and Polyagailo gave a characterization of co-Hopficity for geometrically finite Kleinian groups of isometries of  without 2-torsion.Thomas Delzant and Leonid Potyagailo, Endomorphisms of Kleinian groups. Geometric and Functional Analysis 13 (2003), no. 2, pp. 396–436 \nA right-angled Artin group  (where  is a finite nonempty graph) is not co-Hopfian; sending every standard generator of  to a power  defines and endomorphism  of   which is injective but not surjective.Montserrat Casals-Ruiz, Embeddability and quasi-isometric classification of partially commutative groups.  Algebraic and Geometric Topology 16 (2016), no. 1, 597–620\nA finitely generated torsion-free nilpotent group G may be either co-Hopfian or not co-Hopfian, depending on the properties of its associated rational Lie algebra. Igor Belegradek, On co-Hopfian nilpotent groups. Bulletin of the London Mathematical Society 35 (2003), no. 6, pp. 805–811 Yves Cornulier, Gradings on Lie algebras, systolic growth, and cohopfian properties of nilpotent groups. Bulletin de la Société Mathématique de France 144 (2016), no. 4, pp. 693–744  \nIf G is a relatively hyperbolic group and  is an injective but non-surjective endomorphism of G then either  is parabolic for some k >1 or G splits over a virtually cyclic or a parabolic subgroup.Cornelia Druţu and Mark Sapir, Groups acting on tree-graded spaces and splittings of relatively hyperbolic groups.  Advances in Mathematics 217 (2008), no. 3, pp. 1313–1367   \nGrigorchuk group G of intermediate growth is not co-Hopfian.Igor Lysënok, A set of defining relations for the Grigorchuk group. \nMatematicheskie Zametki 38 (1985), no. 4, 503–516 \nThomposon group F is not co-Hopfian.Bronlyn Wassink, Subgroups of R. Thompson's group F that are isomorphic to F. Groups, Complexity, Cryptology 3 (2011), no. 2, 239–256 \nThere exists a finitely generated group G which is not co-Hopfian but has Kazhdan's property (T).Yann Ollivier, and Daniel Wise, Kazhdan groups with infinite outer automorphism group. Transactions of the American Mathematical Society 359 (2007), no. 5, pp. 1959–1976 \nIf G is Higman's universal finitely presented group then G is not co-Hopfian, and G cannot be embedded in a finitely generated recursively presented co-Hopfian group.Charles F. Miller, and Paul Schupp, Embeddings into Hopfian groups.  Journal of Algebra 17 (1971), pp. 171–176 \n\nGeneralizations and related notions\n\nA group G is called finitely co-HopfianMartin Bridson, Daniel Groves, Jonathan Hillman, Gaven Martin, Cofinitely Hopfian groups, open mappings and kno complements. Groups, Geometry, and Dynamics 4 (2010), no. 4, pp. 693–707 if whenever  is an injective endomorphism whose image has finite index in G then . For example, for  the free group  is not co-Hopfian but it is finitely co-Hopfian.\nA finitely generated group G is called scale-invariant if there exists a nested sequence of subgroups of finite index of G, each isomorphic to G, and whose intersection is a finite group. Volodymyr Nekrashevych, and Gábor Pete, Scale-invariant groups. Groups, Geometry, and Dynamics 5 (2011), no. 1, pp. 139–167 \nA group G is called dis-cohopfian if there exists an injective endomorphism  such that . \nIn coarse geometry, a metric space X is called quasi-isometrically co-Hopf if every quasi-isometric embedding  is coarsely surjective (that is, is a quasi-isometry). Similarly, X is called coarsely co-Hopf if every coarse embedding  is coarsely surjective. Ilya Kapovich, and Anton Lukyanenko, Quasi-isometric co-Hopficity of non-uniform lattices in rank-one semi-simple Lie groups.  Conformal Geometry and Dynamics 16 (2012), pp. 269–282 \nIn metric geometry, a metric space K is called quasisymmetrically co-Hopf if every quasisymmetric embedding  is onto.  Sergei Merenkov, A Sierpiński carpet with the co-Hopfian property. Inventiones Mathematicae 180 (2010), no. 2, pp. 361–388\n\nSee also\nHopfian object\n\nReferences\n\nFurther reading\n K. Varadarajan,  Hopfian and co-Hopfian Objects, Publicacions Matemàtiques  36 (1992), no. 1, pp. 293–317\n\nCategory:Group theory Category:Algebra"
    },
    {
      "title": "Coefficient",
      "url": "https://en.wikipedia.org/wiki/Coefficient",
      "text": "In mathematics, a coefficient is a multiplicative factor in some term of a polynomial, a series, or any expression; it is usually a number, but may be any expression. In the latter case, the variables appearing in the coefficients are often called parameters, and must be clearly distinguished from the other variables. \n\nFor example, in\n\nthe first two terms respectively have the coefficients 7 and −3. The third term 1.5 is a constant coefficient. The final term does not have any explicitly written coefficient, but is considered to have coefficient 1, since multiplying by that factor would not change the term. \n\nOften coefficients are numbers as in this example, although they could be parameters of the problem or any expression in these parameters. In such a case one must clearly distinguish between symbols representing variables and symbols representing parameters. Following René Descartes, the variables are often denoted by , , ..., and the parameters by , , , ..., but it is not always the case. For example, if  is considered as a parameter in the above expression, the coefficient of  is , and the constant coefficient is .\n\nWhen one writes \n\nit is generally supposed that  is the only variable and that ,  and  are parameters; thus the constant coefficient is  in this case.\n\nSimilarly, any polynomial in one variable  can be written as\n\nfor some positive integer , where  are coefficients; to allow this kind of expression in all cases one must allow introducing terms with 0 as coefficient.\nFor the largest  with  (if any),  is called the leading coefficient of the polynomial. So for example the leading coefficient of the polynomial\n\nis 4.\n\nSome specific coefficients that occur frequently in mathematics have received a name. This is the case of the binomial coefficients, the coefficients which occur in the expanded form of , and are tabulated in Pascal's triangle.\n\nLinear algebra\nIn linear algebra, the leading coefficient (also leading entry) of a row in a matrix is the first nonzero entry in that row. So, for example, given\n\nThe leading coefficient of the first row is 1; 2 is the leading coefficient of the second row; 4 is the leading coefficient of the third row, and the last row does not have a leading coefficient.\n\nThough coefficients are frequently viewed as constants in elementary algebra, they can be variables more generally. For example, the coordinates  of a vector  in a vector space with basis , are the coefficients of the basis vectors in the expression \n\nSee also\nDegree of a polynomial\nMonic polynomial\n\nReferences\n\nSabah Al-hadad and C.H. Scott (1979) College Algebra with Applications, page 42, Winthrop Publishers, Cambridge Massachusetts  .\nGordon Fuller, Walter L Wilson, Henry C Miller, (1982) College Algebra, 5th edition, page 24, Brooks/Cole Publishing, Monterey California  .\n Steven Schwartzman (1994) The Words of Mathematics: an etymological dictionary of mathematical terms used in English, page 48, Mathematics Association of America, .\n\nCategory:Polynomials\nCategory:Mathematical terminology\nCategory:Algebra\nCategory:Numbers\nCategory:Variables (mathematics)"
    },
    {
      "title": "Coherent algebra",
      "url": "https://en.wikipedia.org/wiki/Coherent_algebra",
      "text": "A coherent algebra is an algebra of complex square matrices that is closed under ordinary matrix multiplication, Schur product, transposition, and contains both the identity matrix  and the all-ones matrix .\n\n Definitions \nA subspace  of  is said to be a coherent algebra of order  if:\n .\n  for all .\n  and  for all .\nA coherent algebra  is said to be:\n Homogeneous if every matrix in  has a constant diagonal.\n Commutative if  is commutative with respect to ordinary matrix multiplication.\n Symmetric if every matrix in  is symmetric.\nThe set  of Schur-primitive matrices in a coherent algebra  is defined as .\n\nDually, the set  of primitive matrices in a coherent algebra  is defined as .\n\n Examples \n The centralizer of a group of permutation matrices is a coherent algebra, i.e.  is a coherent algebra of order  if  for a group  of  permutation matrices.  Additionally, the centralizer of the group of permutation matrices representing the automorphism group of a graph  is homogeneous if and only if  is vertex-transitive.\n The span of the set of matrices relating pairs of elements lying in the same orbit of a diagonal action of a finite group on a finite set is a coherent algebra,   i.e.  where  is defined as for all  of a finite set  acted on by a finite group .\n The span of a regular representation of a finite group as a group of permutation matrices over  is a coherent algebra.\n\n Properties \n The intersection of a set of coherent algebras of order  is a coherent algebra.\n The tensor product of coherent algebras is a coherent algebra, i.e.  if  and  are coherent algebras.\n The symmetrization  of a commutative coherent algebra  is a coherent algebra.\n If  is a coherent algebra, then  for all , , and  if  is homogeneous.\n Dually, if  is a commutative coherent algebra (of order ), then  for all , , and  as well.\n Every symmetric coherent algebra is commutative, and every commutative coherent algebra is homogeneous.\n A coherent algebra is commutative if and only if it is the Bose–Mesner algebra of a (commutative) association scheme.\n A coherent algebra forms a principal ideal ring under Schur product; moreover, a commutative coherent algebra forms a principal ideal ring under ordinary matrix multiplication as well. \n\n See also \n Association scheme\n Bose–Mesner algebra\n\n References \n\nCategory:Algebra\nCategory:Algebraic combinatorics"
    },
    {
      "title": "Comeasuring",
      "url": "https://en.wikipedia.org/wiki/Comeasuring",
      "text": "Let A be an algebra. A comeasuring of A is a pair (B, β) where:\n B is an algebra.\n β: A → A ⊗ B is an algebra map.\n\n References \n .\n\nCategory:Algebra"
    },
    {
      "title": "Congruence relation",
      "url": "https://en.wikipedia.org/wiki/Congruence_relation",
      "text": "In abstract algebra, a congruence relation (or simply congruence) is an equivalence relation on an algebraic structure (such as a group, ring, or vector space) that is compatible with the structure in the sense that algebraic operations done with equivalent elements will yield equivalent elements.Hungerford,  Thomas W.. Algebra. Springer-Verlag, 1974, p. 27  Every congruence relation has a corresponding quotient structure, whose elements are the equivalence classes (or congruence classes) for the relation.Hungerford, 1974, p. 26\n\nBasic example\n\nThe prototypical example of a congruence relation is congruence modulo  on the set of integers.  For a given positive integer , two integers  and  are called congruent modulo , written\n \nif  is divisible by  (or equivalently if  and  have the same remainder when divided by ).\n\nfor example,  and  are congruent modulo ,\n\n \n\nsince  is a multiple of 10, or equivalently since both  and  have a remainder of  when divided by .\n\nCongruence modulo  (for a fixed ) is compatible with both addition and multiplication on the integers.  That is,\n\nif\n\n  and \n\nthen\n\n   and  \n\nThe corresponding addition and multiplication of equivalence classes is known as modular arithmetic.  From the point of view of abstract algebra, congruence modulo  is a congruence relation on the ring of integers, and arithmetic modulo  occurs on the corresponding quotient ring.\n\nDefinition\nThe definition of a congruence depends on the type of algebraic structure under consideration.  Particular definitions of congruence can be made for groups, rings, vector spaces, modules, semigroups, lattices, and so forth.  The common theme is that a congruence is an equivalence relation on an algebraic object that is compatible with the algebraic structure, in the sense that the operations are well-defined on the equivalence classes.\n\nFor example, a group is an algebraic object consisting of a set together with a single binary operation, satisfying certain axioms.  If  is a group with operation , a congruence relation on  is an equivalence relation ≡ on the elements of  satisfying\n \nfor all , , , . For a congruence on a group, the equivalence class containing the identity element is always a normal subgroup, and the other equivalence classes are the cosets of this subgroup.  Together, these equivalence classes are the elements of a quotient group.\n\nWhen an algebraic structure includes more than one operation, congruence relations are required to be compatible with each operation.  For example, a ring possesses both addition and multiplication, and a congruence relation on a ring must satisfy\n\nwhenever . For a congruence on a ring, the equivalence class containing 0 is always a two-sided ideal, and the two operations on the set of equivalence classes define the corresponding quotient ring.\n\nThe general notion of a congruence relation can be given a formal definition in the context of universal algebra, a field which studies ideas common to all algebraic structures.  In this setting, a congruence relation is an equivalence relation ≡ on an algebraic structure that satisfies\n\nfor every n-ary operation μ, and all elements  such that  for each .\n\nRelation with homomorphisms\nIf  is a homomorphism between two algebraic structures (such as homomorphism of groups, or a linear map between vector spaces), then the relation  defined by\n\n if and only if  \n\nis a congruence relation.  By the first isomorphism theorem, the image of A under  is a substructure of B isomorphic to the quotient of A by this congruence.\n\nCongruences of groups, and normal subgroups and ideals\nIn the particular case of groups, congruence relations can be described in elementary terms as follows:\nIf G is a group (with identity element e and operation *) and ~ is a binary relation on G, then ~ is a congruence whenever:\nGiven any element a of G, a ~ a (reflexivity);\nGiven any elements a and b of G, if a ~ b, then b ~ a (symmetry);\nGiven any elements a, b, and c of G, if a ~ b and b ~ c, then a ~ c (transitivity);\nGiven any elements a, a' , b, and b'  of G, if a ~ a'  and b ~ b' , then a * b ~ a'  * b' ;\nGiven any elements a and a'  of G, if a ~ a' , then a−1 ~ a' −1 (this can actually be proven from the other four, so is strictly redundant).\n\nConditions 1, 2, and 3 say that ~ is an equivalence relation.\n\nA congruence ~ is determined entirely by the set {a ∈ G : a ~ e} of those elements of G that are congruent to the identity element, and this set is a normal subgroup.\nSpecifically, a ~ b if and only if b−1 * a ~ e.\nSo instead of talking about congruences on groups, people usually speak in terms of normal subgroups of them; in fact, every congruence corresponds uniquely to some normal subgroup of G.\n\n Ideals of rings and the general case \n\nA similar trick allows one to speak of kernels in ring theory as ideals instead of congruence relations, and in module theory as submodules instead of congruence relations.\n\nA more general situation where this trick is possible is with Omega-groups (in the general sense allowing operators with multiple arity). But this cannot be done with, for example, monoids, so the study of congruence relations plays a more central role in monoid theory.\n\nUniversal algebra\n\nThe idea is generalized in universal algebra:\nA congruence relation on an algebra A is a subset of the direct product A × A that is both an equivalence relation on A and a subalgebra of A × A.\n\nThe kernel of a homomorphism is always a congruence. Indeed, every congruence arises as a kernel.\nFor a given congruence ~ on A, the set A/~ of equivalence classes can be given the structure of an algebra in a natural fashion, the quotient algebra.\nThe function that maps every element of A to its equivalence class is a homomorphism, and the kernel of this homomorphism is ~.\n\nThe lattice Con(A) of all congruence relations on an algebra A is algebraic.\n\nJohn M. Howie described how semigroup theory illustrates congruence relations in universal algebra:\nIn a group a congruence is determined if we know a single congruence class, in particular if we know the normal subgroup which is the class containing the identity. Similarly, in a ring a congruence is determined if we know the ideal which is the congruence class containing the zero. In semigroups there is no such fortunate occurrence, and we are therefore faced with the necessity of studying congruences as such. More than anything else, it is this necessity that gives semigroup theory its characteristic flavour. Semigroups are in fact the first and simplest type of algebra to which the methods of universal algebra must be applied…J. M. Howie (1975) An Introduction to Semigroup Theory, page v, Academic Press\n\nSee also\nTable of congruences\nLinear congruence theorem\nCongruence lattice problem\n\nNotes\n\nReferences\n Horn and Johnson, Matrix Analysis, Cambridge University Press, 1985. . (Section 4.5 discusses congruency of matrices.)\n\nCategory:Modular arithmetic\nCategory:Algebra\nCategory:Binary relations\nCategory:Equivalence (mathematics)"
    },
    {
      "title": "Conjugate (square roots)",
      "url": "https://en.wikipedia.org/wiki/Conjugate_%28square_roots%29",
      "text": "In mathematics, the conjugate of an expression of the form  is  provided that  does not appear in  and . One says also that the two expressions are conjugate. In particular, the conjugate of a root of a quadratic polynomial is the other root, obtained by changing the sign of the square root appearing in the quadratic formula.\n\nComplex conjugation is the special case where the square root is \n\nAs\n\nand\n\nthe sum and the product of conjugate expressions do not involve the square root anymore.\n\nThis property is used for removing a square root from a denominator, by multiplying the numerator and the denominator of a fraction by the conjugate of the denominator (see rationalisation). Typically, one has\n\nIn particular\n\n See also \n Conjugate element (field theory), the generalization to the roots of a polynomial of any degree\n\nCategory:Algebra"
    },
    {
      "title": "Conservation form",
      "url": "https://en.wikipedia.org/wiki/Conservation_form",
      "text": "Conservation form or Eulerian form refers to an arrangement of an equation or system of equations, usually representing a hyperbolic system, that emphasizes that a property represented is conserved, i.e. a type of continuity equation. The term is usually used in the context of continuum mechanics.\n\n General form \nEquations in conservation form take the form\n\nfor any conserved quantity , with a suitable function . An equation of this form can be transformed into an integral equation\n\nusing the divergence theorem. The integral equation states that the change rate of the integral of the quantity  over an arbitrary control volume  is given by the flux  through the boundary of the control volume, with  being the surface normal through the boundary.  is neither produced nor consumed inside of  and is hence conserved. A typical choice for  is , with velocity , meaning that the quantity  flows with a given velocity field.\n\nThe integral form of such equations is usually the physically more natural formulation, and the differential equation arises from differentiation. Since the integral equation can also have non-differentiable solutions, the equality of both formulations can break down in some cases, leading to weak solutions and severe numerical difficulties in simulations of such equations.\n\n Example \nAn example of a set of equations written in conservation form are the Euler equations of fluid flow:\n\nEach of these represents the conservation of mass, momentum and energy, respectively.\n\nSee also\n Conservation law\n Lagrangian and Eulerian specification of the flow field\n\n Further reading \n \n Randall J. LeVeque: Finite Volume Methods for Hyperbolic Problems. Cambridge University Press, Cambridge 2002,  (Cambridge Texts in Applied Mathematics).\n\nCategory:Algebra\nCategory:Conservation equations"
    },
    {
      "title": "Consistent and inconsistent equations",
      "url": "https://en.wikipedia.org/wiki/Consistent_and_inconsistent_equations",
      "text": "In mathematics and in particular in algebra, a linear or nonlinear system of equations is consistent if there is at least one set of values for the unknowns that satisfies every equation in the system—that is, that when substituted into each of the equations makes each equation hold true as an identity. In contrast, an equation system is inconsistent if there is no set of values for the unknowns that satisfies all of the equations.\n\nIf a system of equations is inconsistent, then it is possible to manipulate and combine the equations in such a way as to obtain contradictory information, such as 2 = 1, or x3 + y3 = 5 and x3 + y3 = 6 (which implies 5 = 6).\n\nBoth types of equation system, consistent and inconsistent, can be any of overdetermined (having more equations than unknowns), underdetermined (having fewer equations than unknowns), or exactly determined.\n\nSimple examples\n\nUnderdetermined and consistent\nThe system\n\nhas an infinite number of solutions, all of them having z = 1 (as can be seen by subtracting the first equation from the second), and all of them therefore having x+y = 2 for any values of x and y.\n\nThe nonlinear system\n\nhas an infinitude of solutions, all involving \n\nSince each of these systems has more than one solution, it is an indeterminate system.\n\nUnderdetermined and inconsistent\n\nThe system\n\nhas no solutions, as can be seen by subtracting the first equation from the second to obtain the impossible 0 = 1.\n\nThe nonlinear system\n\nhas no solutions, because if one equation is subtracted from the other we obtain the impossible 0 = 3.\n\nExactly determined and consistent\n \nThe system\n \n\n \nhas exactly one solution: x = 1, y= 2.\n \nThe nonlinear system\n \n\n \nhas the two solutions (x, y) = (1, 0) and (x, y) = (0, 1), while\n \n\n \nhas an infinite number of solutions because the third equation is the first equation plus twice the second one and hence contains no independent information; thus any value of z can be chosen and values of x and y can be found to satisfy the first two (and hence the third) equations.\n\nExactly determined and inconsistent\n\nThe system \n\nhas no solutions; the inconsistency can be seen by multiplying the first equation by 4 and subtracting the second equation to obtain the impossible 0 = 2.\n\nLikewise, \n\nis an inconsistent system because the first equation plus twice the second minus the third contains the contradiction 0 = 2.\n\nOverdetermined and consistent\n\nThe system \n\nhas a solution, x  = –1, y = 4, because the first two equations do not contradict each other and the third equation is redundant (since it contains the same information as can be obtained from the first two equations by multiplying each through by 2 and summing them).\n\nThe system\n\nhas an infinitude of solutions since all three equations give the same information as each other (as can be seen by multiplying through the first equation by either 3 or 7). Any value of y is part of a solution, with the corresponding value of x being 7–2y.\n\nThe nonlinear system\n\nhas the three solutions (x, y) = (1, –1), (–1, 1), and (1, 1).\n\nOverdetermined and inconsistent\n\nThe system\n\nis inconsistent because the last equation contradicts the information embedded in the first two, as seen by multiplying each of the first two through by 2 and summing them.\n\nThe system\n\nis inconsistent because the sum of the first two equations contradicts the third one.\n\nCriteria for consistency\n\nAs can be seen from the above examples, consistency versus inconsistency is a different issue from comparing the numbers of equations and unknowns.\n\nLinear systems\n\nA linear system is consistent if and only if its coefficient matrix has the same rank as does its augmented matrix (the coefficient matrix with an extra column added, that column being the column vector of constants).\n\nNonlinear systems\n\nCategory:Equations\nCategory:Algebra"
    },
    {
      "title": "Constant (mathematics)",
      "url": "https://en.wikipedia.org/wiki/Constant_%28mathematics%29",
      "text": "In mathematics, the adjective constant means non-varying. The noun constant may have two different meanings. It may refer to a fixed and well-defined number or other mathematical object. The term mathematical constant (and also physical constant) is sometimes used to distinguish this meaning from the other one.  A constant may also refer to a constant function or its value (it is a common usage to identify them). Such a constant is commonly represented by a variable which does not depend on the main variable(s) of the studied problem. This is the case, for example, for a constant of integration which is an arbitrary constant function (not depending on the variable of integration) added to a particular antiderivative to get all the antiderivatives of the given function.\n\nFor example, a general quadratic function is commonly written as:\n\nwhere a, b and c are constants (or parameters), while x is the variable, a placeholder for the argument of the function being studied. A more explicit way to denote this function is\n\nwhich makes the function-argument status of x clear, and thereby implicitly the constant status of a, b and c. In this example a, b and c are coefficients of the polynomial. Since c occurs in a term that does not involve x, it is called the constant term of the polynomial and can be thought of as the coefficient of x0; any polynomial term or expression of degree zero is a constant.\n\n Constant function \n\nA constant may be used to define a constant function that ignores its arguments and always gives the same value. A constant function of a single variable, such as , has a graph that is a horizontal straight line, parallel to the x-axis. Such a function always takes the same value (in this case, 5) because its argument does not appear in the expression defining the function.\n\n Context-dependence \n\nThe context-dependent nature of the concept of \"constant\" can be seen in this example from elementary calculus:\n\n\"Constant\" means not depending on some variable; not changing as that variable changes. In the first case above, it means not depending on h; in the second, it means not depending on x.\n\nNotable mathematical constants\n\nSome values occur frequently in mathematics and are conventionally denoted by a specific symbol. These standard symbols and their values are called mathematical constants. Examples include:\n 0 (zero).\n 1 (one), the natural number after zero.\n  (pi), the constant representing the ratio of a circle's circumference to its diameter, approximately equal to 3.141592653589793238462643...\n e, approximately equal to 2.718281828459045235360287...\n i, the imaginary unit such that i2 = −1.\n  (square root of 2), the length of the diagonal of a square with unit sides, approximately equal to 1.414213562373095048801688.\n φ (golden ratio), approximately equal to 1.618033988749894848204586, or algebraically, .\n\nConstants in calculus\nIn calculus, constants are treated in several different ways depending on the operation. For example, the derivative of a constant function is zero. This is because the derivative measures the rate of change of a function with respect to a variable, and since constants, by definition, do not change, their derivative is therefore zero. Conversely, when integrating a constant function, the constant is multiplied by the variable of integration. During the evaluation of a limit, the constant remains the same as it was before and after evaluation.\n\nIntegration of a function of one variable often involves a constant of integration. This arises because of the integral operator's nature as the inverse of the differential operator, meaning the aim of integration is to recover the original function before differentiation. The differential of a constant function is zero, as noted above, and the differential operator is a linear operator, so functions that only differ by a constant term have the same derivative. To acknowledge this, a constant of integration is added to an indefinite integral; this ensures that all possible solutions are included. The constant of integration is generally written as 'c' and represents a constant with a fixed but undefined value.\n\nExamples\nIf  is the constant function such that  for every  then\n\nSee also\nExpression\nPhysical constant\nConstant (disambiguation)\n\nReferences\n\nCategory:Algebra\nCategory:Elementary mathematics\n "
    },
    {
      "title": "Cramer's theorem (algebraic curves)",
      "url": "https://en.wikipedia.org/wiki/Cramer%27s_theorem_%28algebraic_curves%29",
      "text": "In mathematics, Cramer's theorem on algebraic curves gives the necessary and sufficient number of points in the real plane falling on an algebraic curve to uniquely determine the curve in non-degenerate cases. This number is \n\nwhere n is the degree of the curve. The theorem is due to Gabriel Cramer, who published it in 1750.* . Geneva: Frères Cramer & Cl. Philibert, 1750.\n\nFor example, a line (of degree 1) is determined by 2 distinct points on it: one and only one line goes through those two points. Likewise, a non-degenerate conic (polynomial equation in x and y with the sum of their powers in any term not exceeding 2, hence with degree 2) is uniquely determined by 5 points in general position (no three of which are on a straight line).\n\nThe intuition of the conic case is this: Suppose the given points fall on, specifically, an ellipse. Then five pieces of information are necessary and sufficient to identify the ellipse—the horizontal location of the ellipse's center, the vertical location of the center, the major axis (the length of the longest chord), the minor axis (the length of the shortest chord through the center, perpendicular to the major axis), and the ellipse's rotational orientation (the extent to which the major axis departs from the horizontal). Five points in general position suffice to provide these five pieces of information, while four points do not.\n\nDerivation of the formula\n\nThe number of distinct terms (including those with a zero coefficient) in an n-th degree equation in two variables is (n + 1)(n + 2) / 2. This is because the n-th degree terms are  numbering n + 1 in total; the (n − 1) degree terms are  numbering n in total; and so on through the first degree terms  and  numbering 2 in total, and the single zero degree term (the constant). The sum of these is (n + 1) + n + (n – 1) + ... + 2 + 1 = (n + 1)(n + 2) / 2 terms, each with its own coefficient. However, one of these coefficients is redundant in determining the curve, because we can always divide through the polynomial equation by any one of the coefficients, giving an equivalent equation with one coefficient fixed at 1, and thus [(n + 1)(n + 2) / 2] − 1 = n(n + 3) / 2  remaining coefficients. \n\nFor example, a fourth degree equation has the general form\n\nwith 4(4+3)/2 = 14 coefficients.\n\nDetermining an algebraic curve through a set of points consists of determining values for these coefficients in the algebraic equation such that each of the points satisfies the equation. Given n(n + 3) / 2 points (xi, yi), each of these points can be used to create a separate equation by substituting it into the general polynomial equation of degree n, giving n(n + 3) / 2 equations linear in the n(n + 3) / 2 unknown coefficients. If this system is non-degenerate in the sense of having a non-zero determinant, the unknown coefficients are uniquely determined and hence the polynomial equation and its curve are uniquely determined. More than this number of points would be redundant, and fewer would be insufficient to solve the system of equations uniquely for the coefficients.\n\nDegenerate cases\n\nAn example of a degenerate case, in which n(n + 3) / 2 points on the curve are not sufficient to determine the curve uniquely, was provided by Cramer as part of Cramer's paradox. Let the degree be n = 3, and let nine points be all combinations of x = –1, 0, 1 and y = –1, 0, 1. More than one cubic contains all of these points, namely all cubics of equation  Thus these points do not determine a unique cubic, even though there are n(n + 3) / 2 = 9 of them. More generally, there are infinitely many cubics that pass through the nine intersection points of two cubics (Bézout's theorem implies that two cubics have, in general, nine intersection points)\n\nLikewise, for the conic case of n = 2, if three of five given points all fall on the same straight line, they may not uniquely determine the curve.\n\nRestricted cases\n\nIf the curve is required to be in a particular sub-category of n-th degree polynomial equations, then fewer than n(n + 3) / 2 points may be necessary and sufficient to determine a unique curve. For example, the generic circle is given by the equation  where the center is located at (a, b) and the radius is r. Equivalently, by expanding the squared terms, the generic equation is  where  Two restrictions have been imposed here compared to the general conic case of n = 2: the coefficient of the term in xy is restricted to equal 0, and the coefficient of y2 is restricted to equal the coefficient of x2. Thus instead of five points being needed, only 5 – 2 = 3 are needed, coinciding with the 3 parameters a, b, k (equivalently a, b, r) that need to be identified.\n\nSee also\n\nFive points determine a conic\n\nReferences\n\nCategory:Algebra\nCategory:Analytic geometry"
    },
    {
      "title": "Cyclic algebra",
      "url": "https://en.wikipedia.org/wiki/Cyclic_algebra",
      "text": "In algebra, a cyclic division algebra is one of the basic examples of a division algebra over a field, and plays a key role in the theory of central simple algebras.\n\n Definition \nLet A be a finite-dimensional central simple algebra over a field F. Then A is said to be cyclic if it contains a strictly maximal subfield E such that E/F is a cyclic field extension (i.e., the Galois group is a cyclic group).\n\n See also \n Factor system#Cyclic algebras - cyclic algebras described by factor systems.\n Brauer group#Cyclic algebras - cyclic algebras are representative of Brauer classes.\n\n References \n \n \n\nCategory:Algebra"
    },
    {
      "title": "Cyclotomic polynomial",
      "url": "https://en.wikipedia.org/wiki/Cyclotomic_polynomial",
      "text": "In mathematics the nth cyclotomic polynomial, for any positive integer n, is the unique irreducible polynomial with integer coefficients that is a divisor of  and is not a divisor of  for any  Its roots are all nth primitive roots of unity \n, where k runs over the positive integers not greater than n and coprime to n. In other words, the nth cyclotomic polynomial is equal to\n\nIt may also be defined as the monic polynomial with integer coefficients that is the minimal polynomial over the field of the rational numbers of any primitive nth-root of unity ( is an example of such a root).\n\nAn important relation linking cyclotomic polynomials and primitive roots of unity is\n\nshowing that  is a root of  if and only if it is a dth primitive root of unity for some d that divides n.\n\nExamples\n\nIf n is a prime number, then \n\nIf n = 2p where p is an odd prime number, then\n\nFor n up to 30, the cyclotomic polynomials are:\n\nThe case of the 105th cyclotomic polynomial is interesting because 105 is the lowest integer that is the product of three distinct odd prime numbers (3*5*7) and this polynomial is the first one that has a coefficient other than 1, 0, or −1:\n\nProperties\n\nFundamental tools\n\nThe cyclotomic polynomials are monic polynomials with integer coefficients that are irreducible over the field of the rational numbers. Except for n equal to 1 or 2, they are palindromics of even degree.\n\nThe degree of , or in other words the number of nth primitive roots of unity, is , where  is Euler's totient function.\n\nThe fact that  is an irreducible polynomial of degree  in the ring  is a nontrivial result due to Gauss. Depending on the chosen definition, it is either the value of the degree or the irreducibility which is a nontrivial result. The case of prime n is easier to prove than the general case, thanks to Eisenstein's criterion.\n\nA fundamental relation involving cyclotomic polynomials is \n\nwhich means that each n-th root of unity is a primitive d-th root of unity for a unique d dividing n.\n\nThe Möbius inversion formula allows the expression of  as an explicit rational fraction:\n\nwhere  is the Möbius function.\n\nThe Fourier transform of functions of the greatest common divisor together with the Möbius inversion formula gives: \n\nThe cyclotomic polynomial  may be computed by (exactly) dividing  by the cyclotomic polynomials of the proper divisors of n previously computed recursively by the same method:\n\n(Recall that .)\n\nThis formula allows computation of  on a computer for any n, as soon as integer factorization and division of polynomials are available. Many computer algebra systems have a built in function to compute the cyclotomic polynomials. For example, this function is called by typing cyclotomic_polynomial(n,x) in SageMath, numtheory[cyclotomic](n,x); in Maple, and Cyclotomic[n,x] in Mathematica.\n\nEasy cases for computation\nAs noted above, if  is a prime number, then\n\nIf n is an odd integer greater than one, then \n\nIn particular, if  is twice an odd prime, then (as noted above)\n\nIf  is a prime power (where p is prime), then\n\nMore generally, if  with  relatively prime to , then \n\nThese formulas may be applied repeatedly to get a simple expression for any cyclotomic polynomial  in term of a cyclotomic polynomial of square free index: If  is the product of the prime divisors of  (its radical), then.\n\nThis allows to give formulas for the th cyclotomic polynomial when  has at most one odd prime factor: If  is an odd prime number, and  and  are positive integers, then: \n\nFor the other values of , the computation of the th cyclotomic polynomial is similarly reduced to that of   where  is the product of the distinct odd prime divisors of . To deal with this case, one has that, for  prime and not dividing ,\n\nIntegers appearing as coefficients\n\nThe problem of bounding the magnitude of the coefficients of the cyclotomic polynomials has been the object of a number of research papers.\n \nIf n has at most two distinct odd prime factors, then Migotti showed that the coefficients of  are all in the set {1, −1, 0}.\n\nThe first cyclotomic polynomial for a product of three different odd prime factors is  it has a coefficient −2 (see its expression above). The converse is not true:  only has coefficients in {1, −1, 0}.\n\nIf n is a product of more different odd prime factors, the coefficients may increase to very high values. E.g.,  has coefficients running from −22 to 23, , the smallest n with 6 different odd primes, has coefficients up to ±532.\n\nLet A(n) denote the maximum absolute value of the coefficients of Φn.  It is known that for any positive k, the number of n up to x with A(n) > nk is at least c(k)⋅x for a positive c(k) depending on k and x sufficiently large.  In the opposite direction, for any function ψ(n) tending to infinity with n we have A(n) bounded above by nψ(n) for almost all n.Meier (2008)\n\nGauss's formula\n\nLet n be odd, square-free, and greater than 3. Then:Gauss, DA, Articles 356-357Riesel, pp. 315-316, p. 436\n\nwhere  both An(z) and Bn(z) have integer coefficients, An(z) has degree φ(n)/2, and Bn(z) has degree φ(n)/2 − 2. Furthermore, An(z) is palindromic when its degree is even; if its degree is odd it is antipalindromic. Similarly, Bn(z) is palindromic unless n is composite and ≡ 3 (mod 4), in which case it is antipalindromic.\n\nThe first few cases are\n\nLucas's formula\n\nLet n be odd, square-free and greater than 3. ThenRiesel, pp. 309-315, p. 443\n\nwhere  both Un(z) and Vn(z) have integer coefficients, Un(z) has degree φ(n)/2, and Vn(z) has degree φ(n)/2 − 1. This can also be written\n\nIf n is even, square-free and greater than 2 (this forces n/2 to be odd),\n\nwhere both Cn(z) and Dn(z) have integer coefficients, Cn(z) has degree φ(n), and Dn(z) has degree φ(n) − 1. Cn(z) and Dn(z) are both palindromic.\n\nThe first few cases are:\n\n Cyclotomic polynomials over a finite field and over -adic integers \n\nOver a finite field with a prime number  of elements, for any integer  that is not a multiple of , the cyclotomic polynomial  factorizes into  irreducible polynomials of degree , where  is Euler's totient function, and  is the multiplicative order of  modulo . In particular,  is irreducible if and only if  is a primitive root modulo , that is,  does not divide , and its multiplicative order modulo  is  the degree of .\n\nThese results are also true over the -adic integers, since Hensel's lemma allows lifting a factorization over the field with  of elements to a factorization over the -adic integers.\n\nPolynomial values\n\nIf  takes any real value, then  for every  (this follows from the fact that the roots of a cyclotomic polynomial are all non-real, for ).\n\nFor studying the values that a cyclotomic polynomial may take when  is given an integer value, it suffices to consider only the case , as the cases  and   are trivial (one has  and ). \n\nFor , one has\n\n if  is not a prime power,\n if  is a prime power with .\n\nThe values that a cyclotomic polynomial  may take for other integer values of  is strongly related with the multiplicative order modulo a prime number. \n\nMore precisely, given a prime number  and an integer  coprime with , the multiplicative order of  modulo , is the smallest positive integer  such that  is a divisor of  For , the multiplicative order of  modulo  is also the shortest period of the representation of  in the numeral base  (see Unique prime; this explains the notation choice). \n\nThe definition of the multiplicative order implies that, if  is the multiplicative order of  modulo , then  is a divisor of  The converse is not true, but one has the following.\n\nIf  is a positive integer and  is an integer, then (see below for a proof)\n\nwhere \n  is a non-negative integer, always equal to 0 when  is even. (In fact, if  is neither 1 nor 2, then  is either 0 or 1. Besides, if  is not a power of 2, then  is always equal to 0)\n  is 1 or the largest odd prime factor of .\n  is odd, coprime with , and its prime factors are exactly the odd primes  such that  is the multiplicative order of  modulo .\n\nThis implies that, if  is an odd prime divisor of  then either  is a divisor of  or  is a divisor of . In the latter case  does not divides \n\nZsigmondy's theorem implies that the only cases where   and  are\n\n \n\nIt follows from above factorization that the odd prime factors of\n\nare exactly the odd primes  such that  is the multiplicative order of  modulo . This fraction may be even only when  is odd. In this case, the multiplicative order of  modulo  is always .\n\nThere are many pairs  with  such that  is prime. In fact, Bunyakovsky conjecture implies that, for every , there are infinitely many  such that  is prime. See  for the list of the smallest  such that  is prime (the smallest  such that  is prime is about , where  is Euler–Mascheroni constant, and  is Euler's totient function). See also  for the list of the smallest primes of the form  with  and , and, more generally, , for the smallest positive integers of this form.\n\n Values of  If  is a prime power, then \n \nIf  is not a prime power, let  we have  and  is the product of the  for  dividing  and different of . If  is a prime divisor of multiplicity  in , then  divide , and their values at  are  factors equal to  of  As  is the multiplicity of  in ,  cannot divide the value at  of the other factors of  Thus there is no prime that divides \n\nIf  is the multiplicative order of  modulo , then  By definition,  If  then  would divide another factor  of  and would thus divide  showing that, if there would be the case,  would not be the multiplicative order of  modulo .\n\nThe other prime divisors of  are divisors of . Let  be a prime divisor of  such that  is not be the multiplicative order of  modulo . If  is the multiplicative order of  modulo , then  divides both  and  The resultant of  and  may be written  where  and  are polynomials. Thus  divides this resultant. As  divides , and the resultant of two polynomials divides the discriminant of any common multiple of these polynomials,  divides also the discriminant  of  Thus  divides .\n\n and  are coprime. In other words, if  is a prime common divisor of  and  then  is not the multiplicative order of  modulo . By Fermat's little theorem, the multiplicative order of  is a divisor of , and thus smaller than .\n\n is square-free. In other words, if   is a prime common divisor of  and  then  does not divide  Let . It suffices to prove that  does not divides  for some polynomial , which is a multiple of  We take \n \nThe multiplicative order of  modulo  divides , which is a divisor of . Thus   is a multiple of . Now, \n \nAs  is prime and greater than 2, all the terms but the first one are multiples of  This proves that \n\nApplications\n\nUsing , one can give an elementary proof for the infinitude of primes congruent to 1 modulo n,S. Shirali. Number Theory. Orient Blackswan, 2004. p. 67.  which is a special case of Dirichlet's theorem on arithmetic progressions.\n\nSuppose  are a finite list of primes congruent to  modulo  Let  and consider . Let  be a prime factor of  (to see that  decompose it into linear factors and note that 1 is the closest root of unity to ). Since  we know that  is a new prime not in the list. We will show that \n\nLet  be the order of  modulo  Since  we have . Thus .  We will show that .\n\nAssume for contradiction that . Since \n\n \n\nwe have \n\n \n\nfor some . Then  is a double root of \n\nThus  must be a root of the derivative so \n\nBut  and therefore  This is a contradiction so . The order of  which is , must divide . Thus \n\nSee also\n Cyclotomic field\n Aurifeuillean factorization\n Root of unity\n\nNotes\n\nReferences\n\nGauss's book Disquisitiones Arithmeticae has been translated from Latin into English and German. The German edition includes all of his papers on number theory: all the proofs of quadratic reciprocity, the determination of the sign of the Gauss sum, the investigations into biquadratic reciprocity, and unpublished notes.\n\nExternal links\n\nCategory:Polynomials\nCategory:Algebra\nCategory:Number theory"
    },
    {
      "title": "Cylindrical algebraic decomposition",
      "url": "https://en.wikipedia.org/wiki/Cylindrical_algebraic_decomposition",
      "text": "In mathematics, cylindrical algebraic decomposition (CAD) is a notion, and an algorithm to compute it, which are fundamental for computer algebra and real algebraic geometry. Given a set S of polynomials in Rn, a cylindrical algebraic decomposition is a decomposition of Rn into connected semialgebraic sets called cells, on which each polynomial has constant sign, either +, − or 0. To be cylindrical, this decomposition must satisfy the following condition: If 1 ≤ k < n and π is the projection from Rn onto Rn−k consisting in removing the k last coordinates, then for every pair of cells c and d, one has either π(c) = π(d) or π(c) ∩ π(d) = ∅. This implies that the images by π of the cells define a cylindrical decomposition of Rn−k.\n\nThe notion was introduced by George E. Collins in 1975, together with an algorithm for computing it.\n\nCollins' algorithm has a computational complexity that is double exponential in n. This is an upper bound, which is reached on most entries. There are also examples for which the minimal number of cells is doubly exponential, showing that every general algorithm for cylindrical algebraic decomposition has a double exponential complexity.\n\nCAD provides an effective version of quantifier elimination over the reals, which has a much better computational complexity than that which results from the original proof of Tarski–Seidenberg theorem. It is efficient enough to be implemented on a computer. It is one of the most important algorithms of computational real algebraic geometry. Searching to improve Collins algorithm, or to provide algorithms that have a better complexity for subproblems of general interest, is an active field of research.\n\nImplementations\n Mathematica: CylindricalDecomposition\nReferences\nBasu, Saugata; Pollack, Richard; Roy, Marie-Françoise Algorithms in real algebraic geometry. Second edition. Algorithms and Computation in Mathematics, 10. Springer-Verlag, Berlin, 2006. x+662 pp. ; 3-540-33098-4\nStrzebonski, Adam. Cylindrical Algebraic Decomposition  from MathWorld.\nCylindrical Algebraic Decomposition in Planning algorithms by Steven M. LaValle. Accessed 13 July 2007\n\nCategory:Algebra\nCategory:Real algebraic geometry"
    },
    {
      "title": "De Morgan algebra",
      "url": "https://en.wikipedia.org/wiki/De_Morgan_algebra",
      "text": "__NOTOC__\nIn mathematics, a De Morgan algebra (named after Augustus De Morgan, a British mathematician and logician) is a structure A = (A, ∨, ∧, 0, 1, ¬) such that:\n\n (A, ∨, ∧, 0, 1) is a bounded distributive lattice, and\n ¬ is a De Morgan involution:  ¬(x ∧ y) = ¬x ∨ ¬y and ¬¬x = x. (i.e. an  involution that additionally satisfies De Morgan's laws)\n\nIn a De Morgan algebra, the laws\n\n ¬x ∨ x = 1 (law of the excluded middle), and\n ¬x ∧ x = 0 (law of noncontradiction)\n\ndo not always hold. In the presence of the De Morgan laws, either law implies the other, and an algebra which satisfies them becomes a Boolean algebra.\n\nRemark: It follows that ¬( x∨y)  = ¬x∧¬y,  ¬1 = 0 and ¬0 = 1 (e.g. ¬1 = ¬1∨0 = ¬1∨¬¬0 = ¬(1∧¬0) = ¬¬0 = 0). Thus ¬ is a dual automorphism.\n\nIf the lattice is defined in terms of the order instead, i.e. (A, ≤) is a bounded partial order with a least upper bound and greatest lower bound for every pair of elements, and the meet and join operations so defined satisfy the distributive law, then the complementation can also be defined as an involutive anti-automorphism, that is, a structure A = (A, ≤, ¬) such that:\n\n (A, ≤) is a bounded distributive lattice, and\n ¬¬x = x, and\n x ≤ y → ¬y ≤ ¬x.\n\nDe Morgan algebras were introduced by Grigore Moisil around 1935. although without the restriction of having a 0 and a 1. They were then variously called quasi-boolean algebras in the Polish school, e.g. by Rasiowa and also distributive i-lattices by J. A. Kalman. (i-lattice being an abbreviation for lattice with involution.) They have been further studied in the Argentinian algebraic logic school of Antonio Monteiro.\n\nDe Morgan algebras are important for the study of the mathematical aspects of fuzzy logic. The standard fuzzy algebra F = ([0,  1], max(x, y), min(x, y), 0, 1, 1 − x) is an example of a De Morgan algebra where the laws of excluded middle and noncontradiction do not hold.\n\nAnother example is Dunn's 4-valued logic, in which false < neither-true-nor-false < true and false < both-true-and-false < true, while neither-true-nor-false and both-true-and-false are not comparable.\n\n Kleene algebra \nIf a De Morgan algebra additionally satisfies x ∧ ¬x ≤ y ∨ ¬y, it is called a Kleene algebra. (This notion should not to be confused with the other Kleene algebra generalizing regular expressions.) This notion has also been called a normal i-lattice by Kalman.\n\nExamples of Kleene algebras in the sense defined above include: lattice-ordered groups, Post algebras and Łukasiewicz algebras. Boolean algebras also meet this definition of Kleene algebra. The simplest Kleene algebra that is not Boolean is Kleene's three-valued logic K3. K3 made its first appearance in Kleene's On notation for ordinal numbers (1938). The algebra was named after Kleene by Brignole and Monteiro. A (possibly abbreviated) version of this paper appeared later in Proceedings of the Japan Academy:  \n\n Related notions \nDe Morgan algebra is not the only plausible way to generalize the Boolean algebra. Another way is to keep ¬x ∧ x = 0 (i.e. the law of noncontradiction) but to drop the law of the excluded middle and the law of double negation. This approach (called semicomplementation) is well-defined even for a [meet] semilattice; if the set of semicomplements has a greatest element it is usually called pseudocomplement. If the pseudocomplement thus defined satisfies the law of the excluded middle, the resulting algebra is also Boolean. However, if only the weaker law ¬x ∨ ¬¬x = 1 is required, this results in Stone algebras. More generally, both De Morgan and Stone algebras are proper subclasses of Ockham algebras.\n\n See also \n orthocomplemented lattice\n\nReferences\n\nFurther reading\n \n \n  \n \n \n \n \n\nCategory:Algebra\nCategory:Lattice theory\nCategory:Algebraic logic\nCategory:Ockham algebras"
    },
    {
      "title": "Determinant",
      "url": "https://en.wikipedia.org/wiki/Determinant",
      "text": "In linear algebra, the determinant is a scalar value that can be computed from the elements of a square matrix and encodes certain properties of the linear transformation described by the matrix.  The determinant of a matrix  is denoted , , or . Geometrically, it can be viewed as the volume scaling factor of the linear transformation described by the matrix. This is also the signed volume of the n-dimensional parallelepiped spanned by the column or row vectors of the matrix. The determinant is positive or negative according to whether the linear mapping preserves or reverses the orientation of n-space.\n\nIn the case of a  matrix the determinant may be defined as:\n\nSimilarly, for a 3 × 3 matrix A, its determinant is:\n\nEach determinant of a  matrix in this equation is called a \"minor\" of the matrix .  This procedure can be extended to give a recursive definition for the determinant of an  matrix, the minor expansion formula.\n\nDeterminants occur throughout mathematics. For example, a matrix is often used to represent the coefficients in a system of linear equations, and the determinant can be used to solve those equations, although other methods of solution are much more computationally efficient. In linear algebra, a matrix (with entries in a field) is invertible (also called nonsingular) if and only if its determinant is non-zero, and correspondingly the matrix is singular if and only if its determinant is zero. This leads to the use of determinants in defining the characteristic polynomial of a matrix, whose roots are the eigenvalues. In analytic geometry, determinants express the signed n-dimensional volumes of n-dimensional parallelepipeds. This leads to the use of determinants in calculus, the Jacobian determinant in the change of variables rule for integrals of functions of several variables. Determinants appear frequently in algebraic identities such as the Vandermonde identity.\n\nDeterminants possess many algebraic properties, including that the determinant of a product of matrices is equal to the product of determinants. Special types of matrices have special determinants; for example, the determinant of an orthogonal matrix is always plus or minus one, and the determinant of a complex Hermitian matrix is always real.\n\nGeometric meaning\nIf an  real matrix A is written in terms of its column vectors  then \n\nThis means that  maps the unit n-cube to the n-dimensional parallelotope defined by the vectors  the region \n\nThe determinant gives the signed n-dimensional volume of this parallelotope,  and hence describes more generally the n-dimensional volume scaling factor of the linear transformation produced by A. (The sign shows whether the transformation preserves or reverses orientation.) In particular, if the determinant is zero, then this parallelotope has volume zero and is not fully n-dimensional, which indicates that the dimension of the image of A is less than n. This means that A produces a linear transformation which is neither onto nor one-to-one, and so is not invertible.\n\n Definition \nThere are various equivalent ways to define the determinant of a square matrix A, i.e. one with the same number of rows and columns.  Perhaps the simplest way to express the determinant is by considering the elements in the top row and the respective minors; starting at the left, multiply the element by the minor, then subtract the product of the next element and its minor, and alternate adding and subtracting such products until all elements in the top row have been exhausted.  For example, here is the result for a 4 × 4 matrix:\n\nAnother way to define the determinant is expressed in terms of the columns of the matrix.  If we write an  matrix A in terms of its column vectors\n \n\nwhere the  are vectors of size n, then the determinant of A is defined so that\n\nwhere b and c are scalars, v is any vector of size n and I is the identity matrix of size n.  These equations say that the determinant is a linear function of each column, that interchanging adjacent columns reverses the sign of the determinant, and that the determinant of the identity matrix is 1. These properties mean that the determinant is an alternating multilinear function of the columns that maps the identity matrix to the underlying unit scalar.  These suffice to uniquely calculate the determinant of any square matrix. Provided the underlying scalars form a field (more generally, a commutative ring with unity), the definition below shows that such a function exists, and it can be shown to be unique.Serge Lang, Linear Algebra, 2nd Edition, Addison-Wesley, 1971, pp 173, 191.\n\nEquivalently, the determinant can be expressed as a sum of products of entries of the matrix where each product has n terms and the coefficient of each product is −1 or 1 or 0 according to a given rule: it is a polynomial expression of the matrix entries. This expression grows rapidly with the size of the matrix (an  matrix contributes n! terms), so it will first be given explicitly for the case of  matrices and  matrices, followed by the rule for arbitrary size matrices, which subsumes these two cases.\n\nAssume A is a square matrix with n rows and n columns, so that it can be written as\n\nThe entries can be numbers or expressions (as happens when the determinant is used to define a characteristic polynomial); the definition of the determinant depends only on the fact that they can be added and multiplied together in a commutative manner.\n\nThe determinant of A is denoted by det(A), or it can be denoted directly in terms of the matrix entries by writing enclosing bars instead of brackets:\n\n 2 × 2 matrices \nthumb|right|The area of the parallelogram is the absolute value of the determinant of the matrix formed by the vectors representing the parallelogram's sides.\nThe Leibniz formula for the determinant of a  matrix is\n\nIf the matrix entries are real numbers, the matrix A can be used to represent two linear maps:  one that maps the standard basis vectors to the rows of A, and one that maps them to the columns of A.  In either case, the images of the basis vectors form a parallelogram that represents the image of the unit square under the mapping.  The parallelogram defined by the rows of the above matrix is the one with vertices at , , , and , as shown in the accompanying diagram.\n\nThe absolute value of  is the area of the parallelogram, and thus represents the scale factor by which areas are transformed by A. (The parallelogram formed by the columns of A is in general a different parallelogram, but since the determinant is symmetric with respect to rows and columns, the area will be the same.)\n\nThe absolute value of the determinant together with the sign becomes the oriented area of the parallelogram. The oriented area is the same as the usual area, except that it is negative when the angle from the first to the second vector defining the parallelogram turns in a clockwise direction (which is opposite to the direction one would get for the identity matrix).\n\nTo show that  is the signed area, one may consider a matrix containing two vectors  and  representing the parallelogram's sides. The signed area can be expressed as  for the angle θ between the vectors, which is simply base times height, the length of one vector times the perpendicular component of the other. Due to the sine this already is the signed area, yet it may be expressed more conveniently using the cosine of the complementary angle to a perpendicular vector, e.g. , such that , which can be determined by the pattern of the scalar product to be equal to :\n\n \n\nThus the determinant gives the scaling factor and the orientation induced by the mapping represented by A. When the determinant is equal to one, the linear mapping defined by the matrix is equi-areal and orientation-preserving.\n\nThe object known as the bivector is related to these ideas. In 2D, it can be interpreted as an oriented plane segment formed by imagining two vectors each with origin , and coordinates  and . The bivector magnitude (denoted by ) is the signed area, which is also the determinant .WildLinAlg episode 4, Norman J Wildberger, Univ. of New South Wales, 2010, lecture via youtube\n\n 3 × 3 matrices \n300px|right|thumb|The volume of this parallelepiped is the absolute value of the determinant of the matrix formed by the rows constructed from the vectors r1, r2, and r3.\n\nThe Laplace formula for the determinant of a  matrix is\n\nthis can be expanded out to give\n\nwhich is the Leibniz formula for the determinant of a  matrix.\n\nthumb|left|upright=1.25|Sarrus' rule: The determinant of the three columns on the left is the sum of the products along the solid diagonals minus the sum of the products along the dashed diagonals\n\nThe rule of Sarrus is a mnemonic for the  matrix determinant: the sum of the products of three diagonal north-west to south-east lines of matrix elements, minus the sum of the products of three diagonal south-west to north-east lines of elements, when the copies of the first two columns of the matrix are written beside it as in the illustration. This scheme for calculating the determinant of a  matrix does not carry over into higher dimensions.\n\n n × n matrices \nThe determinant of a matrix of arbitrary size can be defined by the Leibniz formula or the Laplace formula.\n\nThe Leibniz formula for the determinant of an  matrix A is\n\nHere the sum is computed over all permutations σ of the set  A permutation is a function that reorders this set of integers. The value in the ith position after the reordering σ is denoted by σi. For example, for , the original sequence 1, 2, 3 might be reordered to , with , , and .  The set of all such permutations (also known as the symmetric group on n elements) is denoted by Sn. For each permutation σ, sgn(σ) denotes the signature of σ, a value that is +1 whenever the reordering given by σ can be achieved by successively interchanging two entries an even number of times, and −1 whenever it can be achieved by an odd number of such interchanges.\n\nIn any of the  summands, the term\n\nis notation for the product of the entries at positions , where i ranges from 1 to n:\n\nFor example, the determinant of a  matrix A () is\n\n Levi-Civita symbol \nIt is sometimes useful to extend the Leibniz formula to a summation in which not only permutations, but all sequences of n indices in the  range  occur, ensuring that the contribution of a sequence will be zero unless it denotes a permutation. Thus the totally antisymmetric Levi-Civita symbol  extends the signature of a permutation, by setting  for any permutation σ of n, and  when no permutation σ exists such that  for  (or equivalently, whenever some pair of indices are equal). The determinant for an  matrix can then be expressed using an n-fold summation as\n\nor using two epsilon symbols as\n\nwhere now each ir and each jr should be summed over .\n\nHowever, through the use of tensor notation and the supression of the summation symbol (Einstein's summation convention) we can obtain a much more compact expression of the determinant of the second order system of  dimensions, ;\n\nwhere  and  represent 'e-systems' that take on the values 0, +1 and -1 given the number of permutations of  and . More specifically,  is equal to 0 when there is a repeated index in ; +1 when an even number of permutations of  is present; -1 when an odd number of permutations of  is present. Note, that the number of indices present in the e-systems is equal to  and thus can be generalized in this manner .\n\n Properties of the determinant \nThe determinant has many properties. Some basic properties of determinants are\n\n  where In is the  identity matrix.\n  where  denotes the transpose of .\n \n For square matrices A and B of equal size,\n \n  for an  matrix, A. \n For positive semidefinite matrices A, B, and C of equal size, , for  with the corollary \n If A is a triangular matrix, i.e.  whenever  or, alternatively, whenever , then its determinant equals the product of the diagonal entries:\n\nThis can be deduced from some of the properties below, but it follows most easily directly from the Leibniz formula (or from the Laplace expansion), in which the identity permutation is the only one that gives a non-zero contribution.\n\nA number of additional properties relate to the effects on the determinant of changing particular rows or columns:\n Viewing an  matrix as being composed of  columns, the determinant is an n-linear function. This means that if the  column of a matrix  is written as a sum  of two column vectors, and all other columns are left unchanged, then the determinant of  is the sum of the determinants of the matrices obtained from  by replacing the  column by  (denoted ) and then by  (denoted ) (and a similar relation holds when writing a column as a scalar multiple of a column vector).\n \n If in a matrix, any row or column has all elements equal to zero, then the determinant of that matrix is 0.\n This n-linear function is an alternating form. This means that whenever two columns of a matrix are identical, or more generally some column can be expressed as a linear combination of the other columns (i.e. the columns of the matrix form a linearly dependent set), its determinant is 0.\n\nProperties 1, 8 and 10 — which all follow from the Leibniz formula — completely characterize the determinant; in other words the determinant is the unique function from  matrices to scalars that is n-linear alternating in the columns, and takes the value 1 for the identity matrix (this characterization holds even if scalars are taken in any given commutative ring). To see this it suffices to expand the determinant by multi-linearity in the columns into a (huge) linear combination of determinants of matrices in which each column is a standard basis vector. These determinants are either 0 (by property 9) or else ±1 (by properties 1 and 12 below), so the linear combination gives the expression above in terms of the Levi-Civita symbol. While less technical in appearance, this characterization cannot entirely replace the Leibniz formula in defining the determinant, since without it the existence of an appropriate function is not clear. For matrices over non-commutative rings, properties 8 and 9 are incompatible for ,In a non-commutative setting left-linearity (compatibility with left-multiplication by scalars) should be distinguished from right-linearity. Assuming linearity in the columns is taken to be left-linearity, one would have, for non-commuting scalars a, b:\n\na contradiction. There is no useful notion of multi-linear functions over a non-commutative ring. so there is no good definition of the determinant in this setting.\n\nProperty 2 above implies that properties for columns have their counterparts in terms of rows:\n Viewing an  matrix as being composed of n rows, the determinant is an n-linear function.\n This n-linear function is an alternating form: whenever two rows of a matrix are identical, its determinant is 0.\n Interchanging any pair of columns or rows of a matrix multiplies its determinant by −1. This follows from properties 8 and 10 (it is a general property of multilinear alternating maps). More generally, any permutation of the rows or columns multiplies the determinant by the sign of the permutation. By permutation, it is meant viewing each row as a vector Ri (equivalently each column as Ci) and reordering the rows (or columns) by interchange of Rj and Rk (or Cj and Ck), where j,k are two indices chosen from 1 to n for an  square matrix.\n Adding a scalar multiple of one column to another column does not change the value of the determinant. This is a consequence of properties 8 and 10 in the following way: by property 8 the determinant changes by a multiple of the determinant of a matrix with two equal columns, which determinant is 0 by property 10. Similarly, adding a scalar multiple of one row to another row leaves the determinant unchanged.\n\nProperty 5 says that the determinant on  matrices is homogeneous of degree n. These properties can be used to facilitate the computation of determinants by simplifying the matrix to the point where the determinant can be determined immediately. Specifically, for matrices with coefficients in a field, properties 13 and 14 can be used to transform any matrix into a triangular matrix, whose determinant is given by property 7; this is essentially the method of Gaussian elimination.\n\nFor example, the determinant of\n\ncan be computed using the following matrices:\n\nHere, B is obtained from A by adding −1/2×the first row to the second, so that . C is obtained from B by adding the first to the third row, so that . Finally, D is obtained from C by exchanging the second and third row, so that . The determinant of the (upper) triangular matrix D is the product of its entries on the main diagonal: . Therefore, .\n\n Schur complement \nThe following identity holds for a Schur complement of a square matrix:\n\nThe Schur complement arises as the result of performing a block Gaussian elimination by multiplying the matrix M from the right with a block lower triangular matrix\n\nHere Ip denotes a p×p identity matrix. After multiplication with the matrix L the Schur complement appears in the upper p×p block. The product matrix is\n\nThat is, we have effected a Gaussian decomposition\n\nThe first and last matrices on the RHS have determinant unity,  so we have\n\nThis is Schur's determinant identity.\n\n Multiplicativity and matrix groups \nThe determinant of a matrix product of square matrices equals the product of their determinants:\n\nThus the determinant is a multiplicative map. This property is a consequence of the characterization given above of the determinant as the unique n-linear alternating function of the columns with value 1 on the identity matrix, since the function  that maps  can easily be seen to be n-linear and alternating in the columns of M, and takes the value det(A) at the identity. The formula can be generalized  to (square) products of rectangular matrices, giving the Cauchy–Binet formula, which also provides an independent proof of the multiplicative property.\n\nThe determinant det(A) of a matrix A is non-zero if and only if A is invertible or, yet another equivalent statement, if its rank equals the size of the matrix. If so, the determinant of the inverse matrix is given by\n\nIn particular, products and inverses of matrices with determinant one still have this property. Thus, the set of such matrices (of fixed size n) form a group known as the special linear group. More generally, the word \"special\" indicates the subgroup of another matrix group of matrices of determinant one. Examples include the special orthogonal group (which if n is 2 or 3 consists of all rotation matrices), and the special unitary group.\n\n Laplace's formula and the adjugate matrix \nLaplace's formula expresses the determinant of a matrix in terms of its minors. The minor Mi,j is defined to be the determinant of the -matrix that results from A by removing the i-th row and the j-th column. The expression  is known as a cofactor. The determinant of A is given by\n\n (for a fixed )  (for a fixed )\n\nCalculating det(A) by means of this formula is referred to as expanding the determinant along a row, the i-th row using the first form with fixed i, or expanding along a column, using the second form with fixed j. For example, the Laplace expansion of the  matrix\n\nalong the second column ( and the sum runs over i) is given by,\n\nHowever, Laplace expansion is efficient for small matrices only.\n\nThe adjugate matrix adj(A) is the transpose of the matrix consisting of the cofactors, i.e.,\n\nIn terms of the adjugate matrix, Laplace's expansion can be written as§ 0.8.2 of R. A. Horn & C. R. Johnson: Matrix Analysis 2nd ed. (2013) Cambridge University Press. .\n\n Sylvester's determinant theorem \nSylvester's determinant theorem states that for A, an  matrix, and B, an  matrix (so that A and B have dimensions allowing them to be multiplied in either order forming a square matrix):\n\n,\n\nwhere Im and In are the  and  identity matrices, respectively.\n\nFrom this general result several consequences follow.\n\n Properties of the determinant in relation to other notions \n\n Relation to eigenvalues and trace \n\nLet   be an arbitrary  matrix of complex numbers with eigenvalues , , …, . (Here it is understood that an eigenvalue with algebraic multiplicity   occurs  times in this list.) Then the determinant of    is the product of all eigenvalues,\n.\nThe product of all non-zero eigenvalues is referred to as pseudo-determinant.\n\nConversely, determinants can be used to find the eigenvalues of the matrix : they are the solutions of the characteristic equation\n\nwhere I is the identity matrix of the same dimension as  and  is a (scalar) number which solves the equation (there are no more than  solutions, where  is the dimension of ).\n\nA Hermitian matrix is positive definite if all its eigenvalues are positive. Sylvester's criterion asserts that this is equivalent to the determinants of the submatrices\n\nbeing positive, for all  between 1 and .\n\nThe trace tr(A) is by definition the sum of the diagonal entries of  and also equals the sum of the eigenvalues. Thus, for complex matrices ,\n\nor, for real matrices ,\n\nHere exp() denotes the matrix exponential of , because every eigenvalue  of  corresponds to the eigenvalue exp() of exp(). In particular, given any logarithm of , that is, any matrix  satisfying\n\nthe determinant of  is given by\n\nFor example, for , , and , respectively,\n\ncf. Cayley-Hamilton theorem. Such expressions are deducible from combinatorial arguments, Newton's identities, or the Faddeev–LeVerrier algorithm. That is, for generic ,  the signed constant term of the characteristic polynomial, determined recursively from\n\nIn the general case, this may also be obtained fromA proof can be found in the Appendix B of \n\nwhere the sum is taken over the set of all integers kl ≥ 0 satisfying the equation\n\nThe formula can be expressed in terms of the complete exponential Bell polynomial of n arguments sl = −(l – 1)! tr(Al) as\n\nThis formula can also be used to find the determinant of a matrix  with multidimensional indices  and . The product and trace of such matrices are defined in a natural way as\n\nAn important arbitrary  dimension   identity can be obtained from the  Mercator series expansion of the logarithm when the expansion converges.  If every eigenvalue of A is less than 1 in absolute value,\n\nwhere  is the identity matrix.  More generally, if \n\nis expanded as a formal power series in  then all coefficients of  for  are zero and the remaining polynomial is .\n\n Upper and lower bounds \nFor a positive definite matrix , the trace operator gives the following tight lower and upper bounds on the log determinant\n\nwith equality if and only if  . This relationship can be derived via the formula for the KL-divergence between two multivariate normal distributions.\n\nAlso,\n\nThese inequalities can be proved by bringing the matrix A to the diagonal form. As such, they represent the well-known fact that the harmonic mean is less than the geometric mean, which is less than the arithmetic mean, which is, in turn, less than the root mean square.\n\n Cramer's rule \n\nFor a matrix equation\n, given that A has a nonzero determinant,\n\nthe solution is given by Cramer's rule:\n\nwhere Ai is the matrix formed by replacing the ith column of A by the column vector b. This follows immediately by column expansion of the determinant, i.e.\n\nwhere the vectors  are the columns of A.  The rule is also implied by the identity\n\nIt has recently been shown that Cramer's rule can be implemented in O(n3) time, which is comparable to more common methods of solving systems of linear equations, such as LU, QR, or singular value decomposition.\n\n Block matrices \nSuppose A, B, C, and D are matrices of dimension , , , and , respectively. Then\n\nThis can be seen from the Leibniz formula, or from a decomposition like (for the former case)\n\nWhen A is invertible, one has\n\nas can be seen by employing the decomposition\n\nWhen D is invertible, a similar identity with  factored out can be derived analogously,These identities were taken from http://www.ee.ic.ac.uk/hp/staff/dmb/matrix/proof003.html that is,\n\nWhen the blocks are square matrices of the same order further formulas hold. For example, if C and D commute (i.e., ), then the following formula comparable to the determinant of a  matrix holds:Proofs are given in \n\nGenerally, if all pairs of  matrices of the  block matrix commute, then the determinant of the block matrix is equal to the determinant of the matrix  obtained by computing the determinant of the block matrix considering its entries as the entries of a  matrix. As the previous formula shows, for p = 2, this criterion is sufficient, but not necessary.\n\nWhen A = D and B = C, the blocks are square matrices of the same order and the following formula holds (even if A and B do not commute)\n\nWhen D is a 1×1 matrix, B is a column vector, and C is a row vector then\n\nLet  be a scalar complex number.  If a block matrix is square, its characteristic polynomial can be factored with\n \n\n Derivative \nIt can be seen, e.g. using the Leibniz formula, that the determinant of real (or analogously for complex) square matrices is a polynomial function from  to R, and so it is everywhere differentiable. Its derivative can be expressed using Jacobi's formula:§ 0.8.10 of R. A. Horn & C. R. Johnson: Matrix Analysis 2nd ed. (2013) Cambridge University Press. .\n\nwhere adj(A) denotes the adjugate of A. In particular, if A is invertible, we have\n\nExpressed in terms of the entries of A, these are\n\n \n\nYet another equivalent formulation is\n\n,\n\nusing big O notation. The special case where , the identity matrix, yields\n\nThis identity is used in describing the tangent space of certain matrix Lie groups.\n\nIf the matrix A is written as  where a, b, c are column vectors of length 3, then the gradient over one of the three vectors may be written as the cross product of the other two:\n\n \n\n Abstract algebraic aspects  \n\n Determinant of an endomorphism \nThe above identities concerning the determinant of products and inverses of matrices imply that similar matrices have the same determinant: two matrices A and B are similar, if there exists an invertible matrix X such that . Indeed, repeatedly applying the above identities yields\n\nThe determinant is therefore also called a similarity invariant. The determinant of a linear transformation\n\nfor some finite-dimensional vector space V is defined to be the determinant of the matrix describing it, with respect to an arbitrary choice of basis in V. By the similarity invariance, this determinant is independent of the choice of the basis for V and therefore only depends on the endomorphism T.\n\n Exterior algebra \n\nThe determinant of a linear transformation  of an n-dimensional vector space V can be formulated in a coordinate-free manner by considering the nth exterior power ΛnV of V. A induces a linear map\n\nAs ΛnV is one-dimensional, the map ΛnA is given by multiplying with some scalar. This scalar coincides with the determinant of A, that is to say\n\nThis definition agrees with the more concrete coordinate-dependent definition. This follows from the characterization of the determinant given above. For example, switching two columns changes the sign of the determinant; likewise, permuting the vectors in the exterior product  to , say, also changes its sign.\n\nFor this reason, the highest non-zero exterior power Λn(V) is sometimes also called the determinant of V and similarly for more involved objects such as vector bundles or chain complexes of vector spaces. Minors of a matrix can also be cast in this setting, by considering lower alternating forms ΛkV with .\n\n Transformation on alternating multilinear n-forms \nThe vector space W of all alternating multilinear n-forms on an n-dimensional vector space V has dimension one.  To each linear transformation T on V we associate a linear transformation T′ on W, where for each w in W we define .  As a linear transformation on a one-dimensional space, T′ is equivalent to a scalar multiple.  We call this scalar the determinant of T.\n\n Square matrices over commutative rings and abstract properties \nThe determinant can also be characterized as the unique function\n\nfrom the set of all  matrices with entries in a field K to this field satisfying the following three properties: first, D is an n-linear function: considering all but one column of A fixed, the determinant is linear in the remaining column, that is\n\nfor any column vectors v1, ..., vn, and w and any scalars (elements of K) a and b. Second, D is an alternating function: for any matrix A with two identical columns, . Finally, , where In is the identity matrix.\n\nThis fact also implies that every other n-linear alternating function  satisfies\n\nThis definition can also be extended where K is a commutative ring R, in which case a matrix is invertible if and only if its determinant is an invertible element in R. For example, a matrix A with entries in Z, the integers, is invertible (in the sense that there exists an inverse matrix with integer entries) if the determinant is +1 or −1. Such a matrix is called unimodular.\n\nThe determinant defines a mapping\n\nbetween the group of invertible  matrices with entries in R and the multiplicative group of units in R. Since it respects the multiplication in both groups, this map is a group homomorphism. Secondly, given a ring homomorphism , there is a map  given by replacing all entries in R by their images under f. The determinant respects these maps, i.e., given a matrix  with entries in R, the identity\n\nholds. In other words, the following diagram commutes:\n\n300px\n\nFor example, the determinant of the complex conjugate of a complex matrix (which is also the determinant of its conjugate transpose) is the complex conjugate of its determinant, and for integer matrices: the reduction modulo m of the determinant of such a matrix is equal to the determinant of the matrix reduced modulo m (the latter determinant being computed using modular arithmetic). In the language of category theory, the determinant is a natural transformation between the two functors GLn and (⋅)× (see also Natural transformation#Determinant). Adding yet another layer of abstraction, this is captured by saying that the determinant is a morphism of algebraic groups, from the general linear group to the multiplicative group,\n\n Generalizations and related notions \n\n Infinite matrices \nFor matrices with an infinite number of rows and columns, the above definitions of the determinant do not carry over directly. For example, in the Leibniz formula, an infinite sum (all of whose terms are infinite products) would have to be calculated. Functional analysis provides different extensions of the determinant for such infinite-dimensional situations, which however only work for particular kinds of operators.\n\nThe Fredholm determinant defines the determinant for operators known as trace class operators by an appropriate generalization of the formula\n\nAnother infinite-dimensional notion of determinant is the functional determinant.\n\nOperators in von Neumann algebras\nFor operators in a finite factor, one may define a positive real-valued determinant called the Fuglede−Kadison determinant using the canonical trace. In fact, corresponding to every tracial state on a von Neumann algebra there is a notion of Fuglede−Kadison determinant.\n\n Related notions for non-commutative rings \nFor square matrices with entries in a non-commutative ring, there are various difficulties in defining determinants analogously to that for commutative rings. A meaning can be given to the Leibniz formula provided that the order for the product is specified, and similarly for other ways to define the determinant, but non-commutativity then leads to the loss of many fundamental properties of the determinant, for instance the multiplicative property or the fact that the determinant is unchanged under transposition of the matrix. Over non-commutative rings, there is no reasonable notion of a multilinear form (existence of a nonzero  with a regular element of R as value on some pair of arguments implies that R is commutative). Nevertheless, various notions of non-commutative determinant have been formulated, which preserve some of the properties of determinants, notably quasideterminants and the Dieudonné determinant. It may be noted that if one considers certain specific classes of matrices with non-commutative elements, then there are examples where one can define the determinant and prove linear algebra theorems that are very similar to their commutative analogs. Examples include quantum groups and q-determinant, Capelli matrix and Capelli determinant, super-matrices and Berezinian; Manin matrices is the class of matrices which is most close to matrices with commutative elements.\n\n Further variants \nDeterminants of matrices in superrings (that is, Z2-graded rings) are known as Berezinians or superdeterminants.\n\nThe permanent of a matrix is defined as the determinant, except that the factors sgn(σ) occurring in Leibniz's rule are omitted. The immanant generalizes both by introducing a character of the symmetric group Sn in Leibniz's rule.\n\n Calculation \nDeterminants are mainly used as a theoretical tool. They are rarely calculated explicitly in numerical linear algebra, where for applications like checking invertibility and finding eigenvalues the determinant has largely been supplanted by other techniques.L. N. Trefethen and D. Bau, Numerical Linear Algebra (SIAM, 1997). e.g. in Lecture 1: \"... we mention that the determinant, though a convenient notion theoretically, rarely finds a useful role in numerical algorithms.\" Computational geometry, however, does frequently use calculations related to determinants.A survey of state-of-the-art algorithms for computing determinants and their advantages and disadvantages including results of performance tests, is included in\n\nThe survey is section 1.1 Previous work, and the results of tests are in section 4.3 Determinant computation experiments.\n\nNaive methods of implementing an algorithm to compute the determinant include using the Leibniz formula or Laplace's formula. Both these approaches are extremely inefficient for large matrices, though, since the number of required operations grows very quickly: it is of order n! (n factorial) for an  matrix M. For example, Leibniz's formula requires calculating n! products. Therefore, more involved techniques have been developed for calculating determinants.\n\n Decomposition methods \nGiven a matrix A, some methods compute its determinant by writing A as a product of matrices whose determinants can be more easily computed. Such techniques are referred to as decomposition methods. Examples include the LU decomposition, the QR decomposition or the Cholesky decomposition (for positive definite matrices). These methods are of order O(n3), which is a significant improvement over O(n!)\n\nThe LU decomposition expresses A in terms of a lower triangular matrix L, an upper triangular matrix U and a permutation matrix P:\n\nThe determinants of L and U can be quickly calculated, since they are the products of the respective diagonal entries.  The determinant of P is just the sign  of the corresponding permutation (which is +1 for an even number of permutations and is −1 for an uneven number of permutations). The determinant of A is then\n\n(See determinant identities.) Moreover, the decomposition can be chosen such that L is a unitriangular matrix and therefore has determinant 1, in which case the formula further simplifies to\n\n Further methods \nIf the determinant of A and the inverse of A have already been computed, the matrix determinant lemma allows rapid calculation of the determinant of , where u and v are column vectors.\n\nSince the definition of the determinant does not need divisions, a question arises: do fast algorithms exist that do not need divisions? This is especially interesting for matrices over rings. Indeed, algorithms with run-time proportional to n4 exist. An algorithm of Mahajan and Vinay, and Berkowitzhttp://page.inf.fu-berlin.de/~rote/Papers/pdf/Division-free+algorithms.pdf is based on closed ordered walks (short clow). It computes more products than the determinant definition requires, but some of these products cancel and the sum of these products can be computed more efficiently. The final algorithm looks very much like an iterated product of triangular matrices.\n\nIf two matrices of order n can be multiplied in time M(n), where  for some , then the determinant can be computed in time O(M(n)). This means, for example, that an O(n2.376) algorithm exists based on the Coppersmith–Winograd algorithm.\n\nCharles Dodgson (i.e. Lewis Carroll of Alice's Adventures in Wonderland fame) invented a method for computing determinants called Dodgson condensation.  Unfortunately this interesting method does not always work in its original form.\n\nAlgorithms can also be assessed according to their bit complexity, i.e., how many bits of accuracy are needed to store intermediate values occurring in the computation. For example, the Gaussian elimination (or LU decomposition) method is of order O(n3), but the bit length of intermediate values can become exponentially long. The Bareiss Algorithm, on the other hand, is an exact-division method based on Sylvester's identity is also of order n3, but the bit complexity is roughly the bit size of the original entries in the matrix times n.\n\n History \nHistorically, determinants were used long before matrices: originally, a determinant was defined as a property of a system of linear equations. \nThe determinant \"determines\" whether the system has a unique solution (which occurs precisely if the determinant is non-zero). \nIn this sense, determinants were first used in the Chinese mathematics textbook The Nine Chapters on the Mathematical Art (九章算術, Chinese scholars, around the 3rd century BCE). \nIn Europe,  determinants were considered by Cardano at the end of the 16th century and larger ones by Leibniz.Eves, H: \"An Introduction to the History of Mathematics\", pages 405, 493–494, Saunders College Publishing, 1990.A Brief History of Linear Algebra and Matrix Theory : Cajori, F. A History of Mathematics p. 80\n\nIn Japan, Seki Takakazu (関 孝和) is credited with the discovery of the resultant and the determinant (at first in 1683, the complete version no later than 1710). \nIn Europe, Cramer (1750) added to the theory, treating the subject in relation to sets of equations. \nThe recurrence law was first announced by Bézout (1764).\n\nIt was Vandermonde (1771) who first recognized determinants as independent functions.Campbell, H: \"Linear Algebra With Applications\", pages 111–112. Appleton Century Crofts, 1971 Laplace (1772)Expansion of determinants in terms of minors: Laplace, Pierre-Simon (de) \"Researches sur le calcul intégral et sur le systéme du monde,\" Histoire de l'Académie Royale des Sciences (Paris), seconde partie, pages 267–376 (1772).Muir, Sir Thomas, The Theory of Determinants in the historical Order of Development [London, England: Macmillan and Co., Ltd., 1906].  gave the general method of expanding a determinant in terms of its complementary minors: Vandermonde had already given a special case. Immediately following, Lagrange (1773) treated determinants of the second and third order and applied it to questions of elimination theory; he proved many special cases of general identities.\n\nGauss (1801) made the next advance. Like Lagrange, he made much use of determinants in the theory of numbers. He introduced the word determinant (Laplace had used resultant), though not in the present signification, but rather as applied to the discriminant of a quantic. Gauss also arrived at the notion of reciprocal (inverse) determinants, and came very near the multiplication theorem.\n\nThe next contributor of importance is Binet (1811, 1812), who formally stated the theorem relating to the product of two matrices of m columns and n rows, which for the special case of  reduces to the multiplication theorem. On the same day (November 30, 1812) that Binet presented his paper to the Academy, Cauchy also presented one on the subject. (See Cauchy–Binet formula.) In this he used the word determinant in its present sense,The first use of the word \"determinant\" in the modern sense appeared in: Cauchy, Augustin-Louis \"Memoire sur les fonctions qui ne peuvent obtenir que deux valeurs égales et des signes contraires par suite des transpositions operées entre les variables qu'elles renferment,\" which was first read at the Institute de France in Paris on November 30, 1812, and which was subsequently published in the Journal de l'Ecole Polytechnique, Cahier 17, Tome 10, pages 29–112 (1815).Origins of mathematical terms: http://jeff560.tripod.com/d.html summarized and simplified what was then known on the subject, improved the notation, and gave the multiplication theorem with a proof more satisfactory than Binet's.History of matrices and determinants: http://www-history.mcs.st-and.ac.uk/history/HistTopics/Matrices_and_determinants.html With him begins the theory in its generality.\n\nThe next important figure was Jacobi (from 1827). He early used the functional determinant which Sylvester later called the Jacobian, and in his memoirs in Crelle's Journal for 1841 he specially treats this subject, as well as the class of alternating functions which Sylvester has called alternants. About the time of Jacobi's last memoirs, Sylvester (1839) and Cayley began their work.The first use of vertical lines to denote a determinant appeared in: Cayley, Arthur \"On a theorem in the geometry of position,\" Cambridge Mathematical Journal, vol. 2, pages 267–271 (1841).History of matrix notation: http://jeff560.tripod.com/matrices.html\n\nThe study of special forms of determinants has been the natural result of the completion of the general theory. Axisymmetric determinants have been studied by Lebesgue, Hesse, and Sylvester; persymmetric determinants by Sylvester and Hankel; circulants by Catalan, Spottiswoode, Glaisher, and Scott; skew determinants and Pfaffians, in connection with the theory of orthogonal transformation, by Cayley; continuants by Sylvester; Wronskians (so called by Muir) by Christoffel and Frobenius; compound determinants by Sylvester, Reiss, and Picquet; Jacobians and Hessians by Sylvester; and symmetric gauche determinants by Trudi. Of the textbooks on the subject Spottiswoode's was the first. In America, Hanus (1886), Weld (1893), and Muir/Metzler (1933) published treatises.\n\n Applications \n\n Linear independence \nAs mentioned above, the determinant of a matrix (with real or complex entries, say) is zero if and only if the column vectors (or the row vectors) of the matrix are linearly dependent. Thus, determinants can be used to characterize linearly dependent vectors. For example, given two linearly independent vectors v1, v2 in R3, a third vector v3 lies in the plane spanned by the former two vectors exactly if the determinant of the  matrix consisting of the three vectors is zero. The same idea is also used in the theory of differential equations: given n functions f1(x), …, fn(x) (supposed to be  times differentiable), the Wronskian is defined to be\n\nIt is non-zero (for some x) in a specified interval if and only if the given functions and all their derivatives up to order n−1 are linearly independent. If it can be shown that the Wronskian is zero everywhere on an interval then, in the case of analytic functions, this implies the given functions are linearly dependent. See the Wronskian and linear independence.\n\n Orientation of a basis \n\nThe determinant can be thought of as assigning a number to every sequence of n vectors in Rn, by using the square matrix whose columns are the given vectors. For instance, an orthogonal matrix with entries in Rn represents an orthonormal basis in Euclidean space. The determinant of such a matrix determines whether the orientation of the basis is consistent with or opposite to the orientation of the standard basis. If the determinant is +1, the basis has the same orientation. If it is −1, the basis has the opposite orientation.\n\nMore generally, if the determinant of A is positive, A represents an orientation-preserving linear transformation (if A is an orthogonal  or  matrix, this is a rotation), while if it is negative, A switches the orientation of the basis.\n\n Volume and Jacobian determinant \nAs pointed out above, the absolute value of the determinant of real vectors is equal to the volume of the parallelepiped spanned by those vectors. As a consequence, if  is the linear map represented by the matrix A, and S is any measurable subset of Rn, then the volume of f(S) is given by  times the volume of S. More generally, if the linear map  is represented by the  matrix A, then the n-dimensional volume of f(S) is given by:\n\nBy calculating the volume of the tetrahedron bounded by four points, they can be used to identify skew lines. The volume of any tetrahedron, given its vertices a, b, c, and d, is , or any other combination of pairs of vertices that would form a spanning tree over the vertices.\n\nFor a general differentiable function, much of the above carries over by considering the Jacobian matrix of f. For\n\nthe Jacobian matrix is the  matrix whose entries are given by\n\nIts determinant, the Jacobian determinant, appears in the higher-dimensional version of integration by substitution: for suitable functions f and an open subset U of Rn (the domain of f), the integral over f(U) of some other function  is given by\n\nThe Jacobian also occurs in the inverse function theorem.\n\n Vandermonde determinant (alternant) \n\nThe third order Vandermonde determinant is\n\nIn general, the nth-order Vandermonde determinant is\n\nwhere the right-hand side is the continued product of all the differences that can be formed from the n(n−1)/2 pairs of numbers taken from x1, x2, …, xn, with the order of the differences taken in the reversed order of the suffixes that are involved.\n\n Circulants \n\nSecond order\n\nThird order\n\nwhere ω and ω2 are the complex cube roots of 1. In general, the nth-order circulant determinant is\n\nwhere ωj is an nth root of 1.\n\n See also \n\n Cauchy determinant\n Dieudonné determinant\n Determinant identities\n Functional determinant\n Immanant\n Matrix determinant lemma\n Permanent\n Slater determinant\n\n Notes \n\n References \n\n \n .\n \n \n \n \n G. Baley Price (1947) \"Some identities in the theory of determinants\", American Mathematical Monthly 54:75–90 \n \n \n \n\n External links \n\n \n \n \n Determinant Interactive Program and Tutorial\n Linear algebra: determinants. Compute determinants of matrices up to order 6 using Laplace expansion you choose.\n Matrices and Linear Algebra on the Earliest Uses Pages\n Determinants explained in an easy fashion in the 4th chapter as a part of a Linear Algebra course.\n Instructional Video on taking the determinant of an nxn matrix (Khan Academy)\n \n\n \nCategory:Matrix theory\nCategory:Linear algebra\nCategory:Homogeneous polynomials\nCategory:Algebra"
    },
    {
      "title": "Differential equations of addition",
      "url": "https://en.wikipedia.org/wiki/Differential_equations_of_addition",
      "text": "In cryptography, differential equations of addition (DEA) are one of the most basic equations related to differential cryptanalysis that mix additions over two different groups (e.g. addition modulo 232 and addition over GF(2)) and  where input and output differences are expressed as XORs.\n\n Examples of Differential Equations of Addition \nDifferential equations of addition (DEA) are of the following form:\n\nwhere  and  are -bit unknown variables and ,  and  are known variables. The symbols  and  denote addition modulo  and bitwise exclusive-or respectively. The above equation is denoted by .\n\nLet a set  is an integer less than  denote a system of  DEA where  is a polynomial in . It has been proved that the satisfiability of an arbitrary set of DEA is in the complexity class P when a brute force search requires an exponential time. In 2013, some properties of a special form of\n\nDEA were reported by Chengqing Li et al., where  and  is assumed known. Essentially, the special DEA can be represented as . Based on the found properties, a algorithm for deriving  was proposed and analyzed.\n\n Usage of Differential Equations of Addition \nSolution to an arbitrary set of DEA (either in batch and or in adaptive query model) was due to Souradyuti Paul and Bart Preneel. The solution techniques have been used to attack the stream cipher Helix.\n\n References \n Souradyuti Paul and Bart Preneel, Solving Systems of Differential Equations of Addition, ACISP 2005.  Full version (PDF)\n Souradyuti Paul and Bart Preneel, Near Optimal Algorithms for Solving Differential Equations of Addition With Batch Queries, Indocrypt 2005.  Full version (PDF)\n Helger Lipmaa, Johan Wallén, Philippe Dumas: On the Additive Differential Probability of Exclusive-Or. FSE 2004: 317-331.\n\nCategory:Cryptographic attacks\nCategory:Theory of cryptography\nCategory:Ciphers\nCategory:Algebra"
    },
    {
      "title": "Digital root",
      "url": "https://en.wikipedia.org/wiki/Digital_root",
      "text": "The digital root (also repeated digital sum) of a non-negative integer is the (single digit) value obtained by an iterative process of summing digits, on each iteration using the result from the previous iteration to compute a digit sum. The process continues until a single-digit number is reached.\n\nFor example, the digital root of 65,536 is 7, because  and \n\nDigital roots can be calculated with congruences in modular arithmetic rather than by adding up all the digits, a procedure that can save time in the case of very large numbers.\n\nDigital roots can be used as a sort of checksum, to check that a sum has been performed correctly. If it has, then the digital root of the sum of the given numbers will equal the digital root of the sum of the digital roots of the given numbers. This check, which involves only single-digit calculations, can catch many errors in calculation.\n\nDigital roots are used in Western numerology, but certain numbers deemed to have occult significance (such as 11 and 22) are not always completely reduced to a single digit.\n\nThe number of times the digits must be summed to reach the digital root is called a number's additive persistence; in the above example, the additive persistence of 65,536 is 2.\n\n Significance and formula of the digital root \nIt helps to see the digital root of a positive integer as the position it holds with respect to the largest multiple of 9 less than the number itself. For example, the digital root of 11 is 2, which means that 11 is the second number after 9. Likewise, the digital root of 2035 is 1, which means that 2035 − 1 is a multiple of 9. If a number produces a digital root of exactly 9, then the number is a multiple of 9.\n\nWith this in mind the digital root of a positive integer  may be defined by using floor function , as\n\n Abstract multiplication of digital roots \nThe table below shows the digital roots produced by the familiar multiplication table in the decimal system.\n   1  2  3  4  5  6  7  8  9 112345678922468135793369369369448372615955162738496639639639775318642988765432199999999999\n\nThe table shows a number of interesting patterns and symmetries and is known as the Vedic square.\n\n Formal definition \nLet  denote the sum of the digits of  and let the composition of  be as follows:\n\nEventually the sequence  becomes a one digit number. Let  (the digital root of ) represent this one digit number.\n\n Example \nLet us find the digital root of .\n\nThus,\n\nFor simplicity let us agree simply that\n\n Proof that a constant value exists \nHow do we know that the sequence  eventually becomes a one digit number? Here's a proof:\n\nLet , for all ,  is an integer greater than or equal to 0 and less than 10. Then, . This means that , unless , in which case  is a one digit number. Thus, repeatedly using the  function would cause  to decrease by at least 1, until it becomes a one digit number, at which point it will stay constant, as .\n\nCongruence formula\nThe formula is:\n\nor,\n\nTo generalize the concept of digital roots to other bases b, one can simply change the 9 in the formula to b - 1.\n\nThe digital root is the value modulo 9 because  and thus  so regardless of position, the value mod 9 is the same –  – which is why digits can be meaningfully added. Concretely, for a three-digit number,\n.\n\nTo obtain the modular value with respect to other numbers n, one can take weighted sums, where the weight on the kth digit corresponds to the value of  modulo n, or analogously for  for different bases. This is simplest for 2, 5, and 10, where higher digits vanish (since 2 and 5 divide 10), which corresponds to the familiar fact that the divisibility of a decimal number with respect to 2, 5, and 10 can be checked by the last digit (even numbers end in 0, 2, 4, 6, or 8).\n\nAlso of note is the modulus 11: since  and thus  taking the alternating sum of digits yields the value modulo 11.\n\n Some properties of digital roots \nThe digital root of a number is zero if and only if the number is itself zero.\n\nThe digital root of a number is a positive integer if and only if the number is itself a positive integer.\n\nThe digital root of  is  itself if and only if the number has exactly one digit.\n\nThe digital root of  is less than  if and only if the number is greater than or equal to 10.\n\nThe digital root of  +  is digital root of the sum of the digital root of  and the digital root of .\n\nThe digital root of  -  is congruent with the difference of the digital root of  and the digital root of  modulo 9.\n\nEspecially, we can define the digital root of minus  as follows:\n\nThe digital root of  ×  is digital root of the product of the digital root of  and the digital root of .\n\nThe digital root of a nonzero number is 9 if and only if the number is itself a multiple of 9.\n\nThe digital root of a nonzero number is a multiple of 3 if and only if the number is itself a multiple of 3.\n\nThe digital root of a factorial ≥ 6! is 9.\n\nThe digital root of a square is 1, 4, 7, or 9. Digital roots of square numbers progress in the sequence 1, 4, 9, 7, 7, 9, 4, 1, 9.\nThe digital root of a perfect cube is 1, 8 or 9, and digital roots of perfect cubes progress in that exact sequence.\nThe digital root of integers raised to integer powers greater than 3 is 1, 2, 4, 5, 7, 8 or 9.\nThe digital root of a prime number (except 3) is 1, 2, 4, 5, 7, or 8.\nThe digital root of a power of 2 is 1, 2, 4, 5, 7, or 8. Digital roots of the powers of 2 progress in the sequence 1, 2, 4, 8, 7, 5. This even applies to negative powers of 2; for example, 2 to the power of 0 is 1; 2 to the power of -1 (minus one) is .5, with a digital root of 5; 2 to the power of -2 is .25, with a digital root of 7; and so on, ad infinitum in both directions. This is because negative powers of 2 share the same digits (after removing leading zeroes) as corresponding positive powers of 5, whose digital roots progress in the sequence 1, 5, 7, 8, 4, 2.\n The digital root of a power of 5 is 1, 2, 4, 5, 7 or 8. Digital roots of the powers of 5 progress in the sequence 1, 5, 7, 8, 4, 2. This even applies to negative powers of 5; for example, 5 to the power of 0 is 1; 5 to the power of -1 (minus one) is .2, with a digital root of 2; 5 to the power of -2 is .04, with a digital root of 4; and so on, ad infinitum in both directions. This is because the negative powers of 5 share the same digits (after removing leading zeroes) as corresponding positive powers of 2, whose digital roots progress in sequence 1, 2, 4, 8, 7, 5.\n The digital roots of powered numbers progress in sequence (only certain for positive powers, although in for some exceptions it also may occur for negative powers), and this is because of one of the previously shown properties. As the digital root of a  b is congruent with the multiple of the digital root of a and the digital root of b modulo 9, the digital root of a  a will also do it. So, for example, as shown above, powers of 2 will follows the sequence 1, 2, 4, 8, 7, 5; Powers of 47 (whose digital root is 2) will also follow this sequence. The very sequence follows this rule, and is applicable to any other number. Hence, it is easy to demonstrate that an integer raised to a positive integer power will never have a digital root of 3 or 6.\n\nThe digital root of an even perfect number (except 6) is 1.\nThe digital root of a centered hexagram, or star number is 1 or 4. Digital roots of star numbers progress in the sequence 1, 4, 1.\nThe digital root of a centered hexagonal number is 1 or 7, their digital roots progressing in the sequence 1, 7, 1.\nThe digital root of a triangular number is 1, 3, 6 or 9. Digital roots of triangular numbers progress in the sequence 1, 3, 6, 1, 6, 3, 1, 9, 9, which is palindromic after the first eight terms.\nThe digital root of Fibonacci numbers is a repeating pattern of 1, 1, 2, 3, 5, 8, 4, 3, 7, 1, 8, 9, 8, 8, 7, 6, 4, 1, 5, 6, 2, 8, 1, 9.\nThe digital root of Lucas numbers is a repeating pattern of 2, 1, 3, 4, 7, 2, 9, 2, 2, 4, 6, 1, 7, 8, 6, 5, 2, 7, 9, 7, 7, 5, 3, 8.\nThe digital root of the product of twin primes, other than 3 and 5, is 8. The digital root of the product of 3 and 5 (twin primes) is 6.\n\n In other bases \nThis article is about the digital root in decimal or base ten, hence it is the number mod 9. It is nothing different as the number converted to base 9 and then only the last digit taken.\nIn other radixes the digital root is number mod (base-1) so in base 12 a digital root of a number is the number mod 11 (Ɛduod), for example, 1972duod is 1 + 9 + 7 + 2 = 19 = 17duod which is 1 + 7 = 8, while in decimal the root of the same number (3110) is 5; and in base 16 a digital root of a number is the number mod 15 (0xF), for example, 0x7DF is 7 + 13 + 15 = 35 = 0x23 which is 2 + 3 = 5, while in decimal the root of the same number (2015) is 8.\n\n In popular culture \nDigital roots form an important mechanic in the visual novel adventure game Nine Hours, Nine Persons, Nine Doors.\n\n See also \n\nBase 9\nCasting out nines\nDigit sum\nHamming weight\nMultiplicative digital root\nVedic square\n\nReferences\n ()\n ()\n ()\n ()\n ()\n\n External links \n pattern of digital root using MS Excel\n\nCategory:Algebra\nCategory:Number theory\n\nde:Quersumme#Einstellige (oder iterierte) Quersumme"
    },
    {
      "title": "Distribution (number theory)",
      "url": "https://en.wikipedia.org/wiki/Distribution_%28number_theory%29",
      "text": "In algebra and number theory, a distribution is a function on a system of finite sets into an abelian group which is analogous to an integral: it is thus the algebraic analogue of a distribution in the sense of generalised function.\n\nThe original examples of distributions occur, unnamed, as functions φ on Q/Z satisfyingKubert & Lang (1981) p.1\n\nWe shall call these ordinary distributions.Lang (1990) p.53  They also occur in p-adic integration theory in Iwasawa theory.Mazur & Swinnerton-Dyer (1972) p. 36\n\nLet ... → Xn+1 → Xn → ... be a projective system of finite sets with surjections, indexed by the natural numbers, and let X be their projective limit.  We give each Xn the discrete topology, so that X is compact.  Let φ = (φn) be a family of functions on Xn taking values in an abelian group V and compatible with the projective system:\n\nfor some weight function w.  The family φ is then a distribution on the projective system X.\n\nA function f on X is \"locally constant\", or a \"step function\" if it factors through some Xn.  We can define an integral of a step function against φ as\n\nThe definition extends to more general projective systems, such as those indexed by the positive integers ordered by divisibility.  As an important special case consider the projective system Z/nZ indexed by positive integers ordered by divisibility.   We identify this with the system (1/n)Z/Z with limit Q/Z.\n\nFor x in R we let ⟨x⟩ denote the fractional part of x normalised to 0 ≤ ⟨x⟩ < 1, and let {x} denote the fractional part normalised to 0 < {x} ≤ 1.\n\nExamples\n\nHurwitz zeta function\nThe multiplication theorem for the Hurwitz zeta function\n\ngives a distribution relation\n\nHence for given s, the map  is a distribution on Q/Z.\n\nBernoulli distribution\nRecall that the Bernoulli polynomials  Bn are defined by\n\nfor n ≥ 0, where bk are the Bernoulli numbers, with  generating function\n\nThey satisfy the distribution relation\n\nThus the map\n\ndefined by\n\nis a distribution.Lang (1990) p.36\n\nCyclotomic units\nThe cyclotomic units satisfy distribution relations.  Let a be an element of Q/Z prime to p and let ga denote exp(2πia)−1.  Then for a≠ 0 we haveLang (1990) p.157\n\nUniversal distribution\nOne considers the distributions on Z with values in some abelian group V and seek the \"universal\" or most general distribution possible.\n\nStickelberger distributions\nLet h be an ordinary distribution on Q/Z taking values in a field F.  Let G(N) denote the multiplicative group of Z/NZ, and for any function f on G(N) we extend f to a function on Z/NZ by taking f to be zero off G(N).  Define an element of the group algebra F[G(N)] by\n\nThe group algebras form a projective system with limit X.  Then the functions gN form a distribution on Q/Z with values in X, the Stickelberger distribution associated with h.\n\np-adic measures\nConsider the special case when the value group V of a distribution φ on X takes values in a local field K, finite over Qp, or more generally, in a finite-dimensional\np-adic Banach space W over K, with valuation |·|.  We call φ a measure if |φ| is bounded on compact open subsets of X.Mazur & Swinnerton-Dyer (1974) p.37  Let D be the ring of integers of K and L a lattice in W, that is, a free D-submodule of W with K⊗L = W.  Up to scaling a measure may be taken to have values in L.\n\nHecke operators and measures\nLet D be a fixed integer prime to p and consider ZD, the limit of the system Z/pnD.  Consider any eigenfunction of the Hecke operator Tp with eigenvalue λp prime to p.  We describe a procedure for deriving a measure of ZD.\n\nFix an integer N prime to p and to D.  Let F be the D-module of all functions on rational numbers with denominator coprime to N.  For any prime l not dividing N we define the Hecke operator Tl by\n\nLet f be an eigenfunction for Tp with eigenvalue λp in D.  The quadratic equation X2 − λpX + p = 0 has roots π1, π2 with π1 a unit and  π2 divisible by p.  Define a sequence a0 = 2, a1 = π1+π2 = λp and\n\nso that\n\nReferences\n\n \n \n \n\nCategory:Algebra\nCategory:Number theory"
    },
    {
      "title": "Distribution algebra",
      "url": "https://en.wikipedia.org/wiki/Distribution_algebra",
      "text": "In algebra, the distribution algebra  of a p-adic Lie group G is the K-algebra of K-valued distributions on G. (See the reference for a more precise definition.)\n\n References \n\nCategory:Algebra"
    },
    {
      "title": "Distributive homomorphism",
      "url": "https://en.wikipedia.org/wiki/Distributive_homomorphism",
      "text": "A congruence θ of a join-semilattice S is monomial, if the θ-equivalence class of any element of S has a largest element. We say that θ is distributive, if it is a join, in the congruence lattice Con S of S, of monomial join-congruences of S.\n\nThe following definition originates in Schmidt's 1968 work and was subsequently adjusted by Wehrung.\n\nDefinition (weakly distributive homomorphisms). A homomorphism \nμ : S → T between join-semilattices S and T is weakly distributive, if for all a, b in S and all c in T such that μ(c)≤ a ∨ b, there are elements x and y of S such that c≤ x ∨ y, μ(x)≤ a, and μ(y)≤ b.\n\nExamples:\n\n(1) For an algebra B and a reduct A of B (that is, an algebra with same underlying set as B but whose set of operations is a subset of the one of B), the canonical (∨, 0)-homomorphism from Conc A to Conc B is weakly distributive. Here, Conc A denotes the (∨, 0)-semilattice of all compact congruences of A.\n\n(2) For a convex sublattice K of a lattice L, the canonical (∨, 0)-homomorphism from Conc K to Conc L is weakly distributive.\n\n References \n\nE.T. Schmidt, Zur Charakterisierung der Kongruenzverbände der Verbände, Mat. Casopis Sloven. Akad. Vied. 18 (1968), 3--20.\n\nF. Wehrung, A uniform refinement property for congruence lattices, Proc. Amer. Math. Soc. 127, no. 2 (1999), 363–370.\n\nF. Wehrung, A solution to Dilworth's congruence lattice problem, preprint 2006.\n\nCategory:Algebra"
    },
    {
      "title": "Elementary algebra",
      "url": "https://en.wikipedia.org/wiki/Elementary_algebra",
      "text": "thumb|right|200px|Two-dimensional plot (red curve) of the algebraic equation \n\nElementary algebra encompasses some of the basic concepts of algebra, one of the main branches of mathematics. It is typically taught to secondary school students and builds on their understanding of arithmetic. Whereas arithmetic deals with specified numbers,H.E. Slaught and N.J. Lennes, Elementary algebra, Publ. Allyn and Bacon, 1915, page 1 (republished by Forgotten Books) algebra introduces quantities without fixed values, known as variables.Lewis Hirsch, Arthur Goodman, Understanding Elementary Algebra With Geometry: A Course for College Students, Publisher: Cengage Learning, 2005, , 9780534999728, 654 pages, page 2 This use of variables entails a use of algebraic notation and an understanding of the general rules of the operators introduced in arithmetic.  Unlike abstract algebra, elementary algebra is not concerned with algebraic structures outside the realm of real and complex numbers.\n\nThe use of variables to denote quantities allows general relationships between quantities to be formally and concisely expressed, and thus enables solving a broader scope of problems. Many quantitative relationships in science and mathematics are expressed as algebraic equations.\n\nAlgebraic notation \n\nAlgebraic notation describes the rules and conventions for writing mathematical expressions, as well as the terminology used for talking about parts of expressions. For example, the expression  has the following components:\n\n256px|thumb|center|\n\nA coefficient is a numerical value, or letter representing a numerical constant, that multiplies a variable (the operator is omitted). A term is an addend or a summand, a group of coefficients, variables, constants and exponents that may be separated from the other terms by the plus and minus operators.Richard N. Aufmann, Joanne Lockwood, Introductory Algebra: An Applied Approach, Publisher Cengage Learning, 2010, , 9781439046043, page 78 Letters represent variables and constants. By convention, letters at the beginning of the alphabet (e.g. ) are typically used to represent constants, and those toward the end of the alphabet (e.g.  and ) are used to represent variables.William L. Hosch (editor), The Britannica Guide to Algebra and Trigonometry, Britannica Educational Publishing, The Rosen Publishing Group, 2010, , 9781615302192, page 71 They are usually written in italics.James E. Gentle, Numerical Linear Algebra for Applications in Statistics, Publisher: Springer, 1998, , 9780387985428, 221 pages, [James E. Gentle page 183]\n\nAlgebraic operations work in the same way as arithmetic operations,Horatio Nelson Robinson, New elementary algebra: containing the rudiments of science for schools and academies, Ivison, Phinney, Blakeman, & Co., 1866, page 7 such as addition, subtraction, multiplication, division and exponentiation.Ron Larson, Robert Hostetler, Bruce H. Edwards, Algebra And Trigonometry: A Graphing Approach, Publisher: Cengage Learning, 2007, , 9780618851959, 1114 pages, page 6 and are applied to algebraic variables and terms.  Multiplication symbols are usually omitted, and implied when there is no space between two variables or terms, or when a coefficient is used. For example,  is written as , and  may be written .Sin Kwai Meng, Chip Wai Lung, Ng Song Beng, \"Algebraic notation\", in Mathematics Matters Secondary 1 Express Textbook, Publisher Panpac Education Pte Ltd, , 9789812738820, page 68\n\nUsually terms with the highest power (exponent), are written on the left, for example,  is written to the left of . When a coefficient is one, it is usually omitted (e.g.  is written ).David Alan Herzog, Teach Yourself Visually Algebra, Publisher John Wiley & Sons, 2008, , 9780470185599, 304 pages, page 72 Likewise when the exponent (power) is one, (e.g.  is written ).John C. Peterson, Technical Mathematics With Calculus, Publisher Cengage Learning, 2003, , 9780766861893, 1613 pages, page 31 When the exponent is zero, the result is always 1 (e.g.  is always rewritten to ).Jerome E. Kaufmann, Karen L. Schwitters, Algebra for College Students, Publisher Cengage Learning, 2010, , 9780538733540, 803 pages, page 222 However , being undefined, should not appear in an expression, and care should be taken in simplifying expressions in which variables may appear in exponents.\n\nAlternative notation\nOther types of notation are used in algebraic expressions when the required formatting is not available, or can not be implied, such as where only letters and symbols are available. For example, exponents are usually formatted using superscripts, e.g. . In plain text, and in the TeX mark-up language, the caret symbol \"^\" represents exponents, so  is written as \"x^2\".Ramesh Bangia, Dictionary of Information Technology, Publisher Laxmi Publications, Ltd., 2010, , 9789380298153, page 212George Grätzer, First Steps in LaTeX, Publisher Springer, 1999, , 9780817641320, page 17 In programming languages such as Ada,S. Tucker Taft, Robert A. Duff, Randall L. Brukardt, Erhard Ploedereder, Pascal Leroy, Ada 2005 Reference Manual, Volume 4348 of Lecture Notes in Computer Science, Publisher Springer, 2007, , 9783540693352, page 13 Fortran,C. Xavier, Fortran 77 And Numerical Methods, Publisher New Age International, 1994, , 9788122406702, page 20 Perl,Randal Schwartz, Brian Foy, Tom Phoenix, Learning Perl, Publisher O'Reilly Media, Inc., 2011, , 9781449313142, page 24 Python Matthew A. Telles, Python Power!: The Comprehensive Guide, Publisher Course Technology PTR, 2008, , 9781598631586, page 46 and Ruby,Kevin C. Baird, Ruby by Example: Concepts and Code, Publisher No Starch Press, 2007, , 9781593271480, page 72 a double asterisk is used, so  is written as \"x**2\". Many programming languages and calculators use a single asterisk to represent the multiplication symbol,William P. Berlinghoff, Fernando Q. Gouvêa, Math through the Ages: A Gentle History for Teachers and Others, Publisher MAA, 2004, , 9780883857366, page 75 and it must be explicitly used, for example,  is written \"3*x\".\n\nConcepts\n\nVariables\nthumb|right|Example of variables showing the relationship between a circle's diameter and its circumference. For any circle, its circumference , divided by its diameter , is equal to the constant pi,  (approximately 3.14).\n\nElementary algebra builds on and extends arithmeticThomas Sonnabend, Mathematics for Teachers: An Interactive Approach for Grades K-8, Publisher: Cengage Learning, 2009, , 9780495561668, 759 pages, page xvii by introducing letters called variables to represent general (non-specified) numbers.  This is useful for several reasons.\n\nVariables may represent numbers whose values are not yet known. For example, if the temperature of the current day, C, is 20 degrees higher than the temperature of the previous day, P,  then the problem can be described algebraically as .Lewis Hirsch, Arthur Goodman, Understanding Elementary Algebra With Geometry: A Course for College Students, Publisher: Cengage Learning, 2005, , 9780534999728, 654 pages, page 48\nVariables allow one to describe general problems,Lawrence S. Leff, College Algebra: Barron's Ez-101 Study Keys, Publisher: Barron's Educational Series, 2005, , 9780764129148, 230 pages, page 2 without specifying the values of the quantities that are involved. For example, it can be stated specifically that 5 minutes is equivalent to  seconds. A more general (algebraic) description may state that the number of seconds, , where m is  the number of minutes.\nVariables allow one to describe mathematical relationships between quantities that may vary.Ron Larson, Kimberly Nolting, Elementary Algebra, Publisher: Cengage Learning, 2009, , 9780547102276, 622 pages, page 210 For example, the relationship between the circumference, c, and diameter, d, of a circle is described by .\nVariables allow one to describe some mathematical properties. For example, a basic property of addition is commutativity which states that the order of numbers being added together does not matter. Commutativity is stated algebraically as .Charles P. McKeague, Elementary Algebra, Publisher: Cengage Learning, 2011, , 9780840064219, 571 pages, page 49\n\n Evaluating expressions \n\nAlgebraic expressions may be evaluated and simplified, based on the basic properties of arithmetic operations (addition, subtraction, multiplication, division and exponentiation). For example,\nAdded terms are simplified using coefficients. For example,  can be simplified as  (where 3 is a numerical coefficient).\nMultiplied terms are simplified using exponents. For example,  is represented as \nLike terms are added together,Andrew Marx, Shortcut Algebra I: A Quick and Easy Way to Increase Your Algebra I Knowledge and Test Scores, Publisher Kaplan Publishing, 2007, , 9781419552885, 288 pages, page 51 for example,  is written as , because the terms containing  are added together, and, the terms containing  are added together.\nBrackets can be \"multiplied out\", using the distributive property. For example,  can be written as  which can be written as \nExpressions can be factored. For example, , by dividing both terms by  can be written as \n\n Equations \nthumb|Animation illustrating Pythagoras' rule for a right-angle triangle, which shows the algebraic relationship between the triangle's hypotenuse, and the other two sides.\n\nAn equation states that two expressions are equal using the symbol for equality,  (the equals sign).Mark Clark, Cynthia Anfinson, Beginning Algebra: Connecting Concepts Through Applications, Publisher Cengage Learning, 2011, , 9780534419387, 793 pages, page 134 One of the best-known equations describes Pythagoras' law relating the length of the sides of a right angle triangle:Alan S. Tussy, R. David Gustafson, Elementary and Intermediate Algebra, Publisher Cengage Learning, 2012, , 9781111567682, 1163 pages, page 493\n\nThis equation states that , representing the square of the length of the side that is the hypotenuse (the side opposite the right angle), is equal to the sum (addition) of the squares of the other two sides whose lengths are represented by  and .\n\nAn equation is the claim that two expressions have the same value and are equal. Some equations are true for all values of the involved variables (such as ); such equations are called identities. Conditional equations are true for only some values of the involved variables, e.g.  is true only for  and . The values of the variables which make the equation true are the solutions of the equation and can be found through equation solving.\n\nAnother type of equation is an inequality. Inequalities are used to show that one side of the equation is greater, or less, than the other. The symbols used for this are:  where  represents 'greater than', and  where  represents 'less than'. Just like standard equality equations, numbers can be added, subtracted, multiplied or divided. The only exception is that when multiplying or dividing by a negative number, the inequality symbol must be flipped.\n\n Properties of equality \n\nBy definition, equality is an equivalence relation, meaning it has the properties (a) reflexive (i.e. ), (b) symmetric (i.e. if  then ) (c) transitive (i.e. if  and  then ).Douglas Downing, Algebra the Easy Way, Publisher Barron's Educational Series, 2003, , 9780764119729, 392 pages, page 20 It also satisfies the important property that if two symbols are used for equal things, then one symbol can be substituted for the other in any true statement about the first and the statement will remain true. This implies the following properties:\n\n if  and  then  and ;\n if  then ;\n more generally, for any function , if  then .\n\n Properties of inequality \n\nThe relations less than  and greater than  have the property of transitivity:Ron Larson, Robert Hostetler, Intermediate Algebra, Publisher Cengage Learning, 2008, , 9780618753529, 857 pages, page 96\n If      and      then   ;\n If      and      then   ;\n If      and      then   ;\n If      and      then   .\nBy reversing the inequation,  and  can be swapped,Chris Carter, Physics: Facts and Practice for A Level, Publisher Oxford University Press, 2001, , 9780199147687, 144 pages, page 50 for example:\n  is equivalent to \n\n Substitution \n\nSubstitution is replacing the terms in an expression to create a new expression. Substituting 3 for a in the expression a*5 makes a new expression 3*5 with meaning 15. Substituting the terms of a statement makes a new statement. When the original statement is true independent of the values of the terms, the statement created by substitutions is also true. Hence definitions can be made in symbolic terms and interpreted through substitution: if , where := means \"is defined to equal\", substituting 3 for  informs the reader of this statement that  means 3*3=9. Often it's not known whether the statement is true independent of the values of the terms, and substitution allows one to derive restrictions on the possible values, or show what conditions the statement holds under. For example, taking the statement x+1=0, if x is substituted with 1, this imples 1+1=2=0, which is false, which implies that if x+1=0 then x can't be 1.\n\nIf x and y are integers, rationals, or real numbers, then xy=0 implies x=0 or y=0. Suppose abc=0. Then, substituting a for x and bc for y, we learn a=0 or bc=0. Then we can substitute again, letting x=b and y=c, to show that if bc=0 then b=0 or c=0. Therefore, if abc=0, then a=0 or (b=0 or c=0), so abc=0 implies a=0 or b=0 or c=0.\n\nConsider if the original fact were stated as \"ab=0 implies a=0 or b=0.\" Then when we say \"suppose abc=0,\" we have a conflict of terms when we substitute. Yet the above logic is still valid to show that if abc=0 then a=0 or b=0 or c=0 if instead of letting a=a and b=bc we substitute a for a and b for bc (and with bc=0, substituting b for a and c for b). This shows that substituting for the terms in a statement isn't always the same as letting the terms from the statement equal the substituted terms. In this situation it's clear that if we substitute an expression a into the a term of the original equation, the a substituted does not refer to the a in the statement \"ab=0 implies a=0 or b=0.\"\n\n Solving algebraic equations \n\nthumb|A typical algebra problem.\n\nThe following sections lay out examples of some of the types of algebraic equations that may be encountered.\n\n Linear equations with one variable \n\nLinear equations are so-called, because when they are plotted, they describe a straight line. The simplest equations to solve are linear equations that have only one variable. They contain only constant numbers and a single variable without an exponent. As an example, consider:\n\nProblem in words: If you double the age of a child and add 4, the resulting answer is 12. How old is the child?\n\nEquivalent equation:  where   represent the child's age\n\nTo solve this kind of equation, the technique is add, subtract, multiply, or divide both sides of the equation by the same number in order to isolate the variable on one side of the equation.  Once the variable is isolated, the other side of the equation is the value of the variable. This problem and its solution are as follows:\n 1. Equation to solve:  2. Subtract 4 from both sides:  3. This simplifies to:  4. Divide both sides by 2:  5. This simplifies to the solution: \nIn words: the child is 4 years old.\n\nThe general form of a linear equation with one variable, can be written as: \n\nFollowing the same procedure (i.e. subtract  from both sides, and then divide by ), the general solution is given by \n\n Linear equations with two variables \nthumb|right|Two linear equations|Solving two linear equations with a unique solution at the point that they intersect.\nA linear equation with two variables has many (i.e. an infinite number of) solutions.Sinha, The Pearson Guide to Quantitative Aptitude for CAT 2/ePublisher: Pearson Education India, 2010, , 9788131723661, 599 pages, page 195 For example:\n\nProblem in words: A father is 22 years older than his son. How old are they?\nEquivalent equation:  where  is the father's age,  is the son's age.\n\nThis cannot be worked out by itself. If the son's age was made known, then there would no longer be two unknowns (variables), and the problem becomes a linear equation with just one variable, that can be solved as described above.\n\nTo solve a linear equation with two variables (unknowns), requires two related equations. For example, if it was also revealed that:\n Problem in words\n In 10 years, the father will be twice as old as his son.\nEquivalent equation\n \n\nNow there are two related linear equations, each with two unknowns, which enables the production of a linear equation with just one variable, by subtracting one from the other (called the elimination method):Cynthia Y. Young, Precalculus, Publisher John Wiley & Sons, 2010, , 9780471756842, 1175 pages, page 699\n\nIn other words, the son is aged 12, and since the father 22 years older, he must be 34. In 10 years time, the son will be 22, and the father will be twice his age, 44. This problem is illustrated on the associated plot of the equations.\n\nFor other ways to solve this kind of equations, see below, System of linear equations.\n\n Quadratic equations \n\nthumb|right|Quadratic equation plot of  showing its roots at  and , and that the quadratic can be rewritten as \nA quadratic equation is one which includes a term with an exponent of 2, for example, ,Mary Jane Sterling, Algebra II For Dummies, Publisher: John Wiley & Sons, 2006, , 9780471775812, 384 pages, page 37 and no term with higher exponent. The name derives from the Latin quadrus, meaning square.John T. Irwin, The Mystery to a Solution: Poe, Borges, and the Analytic Detective Story, Publisher JHU Press, 1996, , 9780801854668, 512 pages, page 372 In general, a quadratic equation can be expressed in the form ,Sharma/khattar, The Pearson Guide To Objective Mathematics For Engineering Entrance Examinations, 3/E, Publisher Pearson Education India, 2010, , 9788131723630, 1248 pages, page 621 where  is not zero (if it were zero, then the equation would not be quadratic but linear). Because of this a quadratic equation must contain the term , which is known as the quadratic term.  Hence , and so we may divide by  and rearrange the equation into the standard form\n\n \n\nwhere  and . Solving this, by a process known as completing the square, leads to the quadratic formula\n\nwhere the symbol \"±\" indicates that both\n\n \n\nare solutions of the quadratic equation.\n\nQuadratic equations can also be solved using factorization (the reverse process of which is expansion, but for two linear terms is sometimes denoted foiling).  As an example of factoring:\n\n \n\nwhich is the same thing as\n\n \n\nIt follows from the zero-product property that either  or  are the solutions, since precisely one of the factors must be equal to zero. All quadratic equations will have two solutions in the complex number system, but need not have any in the real number system. For example,\n\n \n\nhas no real number solution since no real number squared equals −1.\nSometimes a quadratic equation has a root of multiplicity 2, such as:\n\n \n\nFor this equation, −1 is a root of multiplicity 2. This means −1 appears twice, since the equation can be rewritten in factored form as\n\nComplex numbers\n\nAll quadratic equations have exactly two solutions in complex numbers (but they may be equal to each other), a category that includes real numbers, imaginary numbers, and sums of real and imaginary numbers. Complex numbers first arise in the teaching of quadratic equations and the quadratic formula. For example, the quadratic equation\n\nhas solutions\n\nSince  is not any real number, both of these solutions for x are complex numbers.\n\n Exponential and logarithmic equations \n\nright|thumb|upright=1.35|alt=Graph showing a logarithm curves, which crosses the x-axis where x is 1 and extend towards minus infinity along the y-axis.|The graph of the logarithm to base 2 crosses the x axis (horizontal axis) at 1 and passes through the points with coordinates , , and . For example, , because  The graph gets arbitrarily close to the y axis, but does not meet or intersect it.\n\nAn exponential equation is one which has the form  for ,Aven Choo, LMAN OL Additional Maths Revision Guide 3, Publisher Pearson Education South Asia, 2007, , 9789810600013, page 105 which has solution\n\n \n\nwhen . Elementary algebraic techniques are used to rewrite a given equation in the above way before arriving at the solution. For example, if\n\n \n\nthen, by subtracting 1 from both sides of the equation, and then dividing both sides by 3 we obtain\n\n \n\nwhence\n\n \n\nor\n\n \n\nA logarithmic equation is an equation of the form  for , which has solution\n\n \n\nFor example, if\n\n \n\nthen, by adding 2 to both sides of the equation, followed by dividing both sides by 4, we get\n\n \n\nwhence\n\n \n\nfrom which we obtain\n\n \n\n Radical equations \n\nA radical equation is one that includes a radical sign, which includes square roots,   cube roots, , and nth roots, . Recall that an nth root can be rewritten in exponential format, so that  is equivalent to . Combined with regular exponents (powers), then   (the square root of  cubed), can be rewritten as .John C. Peterson, Technical Mathematics With Calculus, Publisher Cengage Learning, 2003, , 9780766861893, 1613 pages, page 525 So a common form of a radical equation is  (equivalent to ) where  and  are integers. It has real solution(s):\n is odd is evenand  and  are evenand  is even,  is odd, and equivalentlyequivalentlyno real solution\n\nFor example, if:\n\nthen\n\n \n\n System of linear equations \n\nThere are different methods to solve a system of linear equations with two variables.\n\n Elimination method \nthumb|right|The solution set for the equations  and  is the single point (2, 3).\n\nAn example of solving a system of linear equations is by using the elimination method:\n\n \n\nMultiplying the terms in the second equation by 2:\n\n \n \n\nAdding the two equations together to get:\n\n \n\nwhich simplifies to\n\n \n\nSince the fact that  is known, it is then possible to deduce that   by either of the original two equations (by using 2 instead of  ) The full solution to this problem is then\n\n \n\nNote that this is not the only way to solve this specific system;  could have been solved before .\n\n Substitution method \n\nAnother way of solving the same system of linear equations is by substitution.\n\n \n\nAn equivalent for  can be deduced by using one of the two equations. Using the second equation:\n\n \n\nSubtracting  from each side of the equation:\n\n \n\nand multiplying by −1:\n\n \n\nUsing this  value in the first equation in the original system:\n\n \n\nAdding 2 on each side of the equation:\n\n \n\nwhich simplifies to\n\n \n\nUsing this value in one of the equations, the same solution as in the previous method is obtained.\n\n \n\nNote that this is not the only way to solve this specific system; in this case as well,  could have been solved before .\n\n Other types of systems of linear equations \n\n Inconsistent systems \nthumb|right|The equations  and  are parallel and cannot intersect, and is unsolvable.\nthumb|right|Plot of a quadratic equation (red) and a linear equation (blue) that do not intersect, and consequently for which there is no common solution.\n\nIn the above example, a solution exists. However, there are also systems of equations which do not have any solution. Such a system is called inconsistent. An obvious example is\n\n \n\nAs 0≠2, the second equation in the system has no solution. Therefore, the system has no solution.\nHowever, not all inconsistent systems are recognized at first sight. As an example, consider the system \n \n\nMultiplying by 2 both sides of the second equation, and adding it to the first one results in\n \nwhich clearly has no solution.\n\n Undetermined systems \n\nThere are also systems which have infinitely many solutions, in contrast to a system with a unique solution (meaning, a unique pair of values for  and ) For example:\n\n \n\nIsolating  in the second equation:\n\n \n\nAnd using this value in the first equation in the system:\n\n \n\nThe equality is true, but it does not provide a value for . Indeed, one can easily verify (by just filling in some values of ) that for any  there is a solution as long as . There is an infinite number of solutions for this system.\n\n Over- and underdetermined systems \n\nSystems with more variables than the number of linear equations are called underdetermined. Such a system, if it has any solutions, does not have a unique one but rather an infinitude of them. An example of such a system is\n\n \n\nWhen trying to solve it, one is led to express some variables as functions of the other ones if any solutions exist, but cannot express all solutions numerically because there are an infinite number of them if there are any.\n\nA system with a greater number of equations than variables is called overdetermined. If an overdetermined system has any solutions, necessarily some equations are linear combinations of the others.\n\n See also \n History of elementary algebra\n Binary operation\n Gaussian elimination\n Mathematics education\n Number line\n Polynomial\n Cancelling out\n Tarski's high school algebra problem\n\n References \nLeonhard Euler,   Elements of Algebra, 1770.  English translation Tarquin Press, 2007, , also online digitized editionsEuler's Elements of Algebra  2006, 1822.\nCharles Smith, A Treatise on Algebra, in Cornell University Library Historical Math Monographs.\nRedden, John. Elementary Algebra. Flat World Knowledge, 2011\n\nExternal links\n\n \nCategory:Algebra"
    },
    {
      "title": "Equivalence class",
      "url": "https://en.wikipedia.org/wiki/Equivalence_class",
      "text": "thumb|370px|Congruence is an example of an equivalence relation. The leftmost two triangles are congruent, while the third and fourth triangles are not congruent to any other triangle shown here. Thus, the first two triangles are in the same equivalence class, while the third and fourth triangles are each in their own equivalence class.\n\nIn mathematics, when the elements of some set  have a notion of equivalence (formalized as an equivalence relation) defined on them, then one may naturally split the set  into equivalence classes. These equivalence classes are constructed so that elements  and  belong to the same equivalence class if and only if  and  are equivalent.\n\nFormally, given a set  and an equivalence relation  on , the equivalence class of an element  in  is the set\n\nof elements which are equivalent to . It may be proven from the defining properties of \"equivalence relations\" that the equivalence classes form a partition of . This partition – the set of equivalence classes – is sometimes called the quotient set or the quotient space of  by  and is denoted by .\n\nWhen the set  has some structure (such as a group operation or a topology) and the equivalence relation  is defined in a manner suitably compatible with this structure, then the quotient set often inherits a similar structure from its parent set. Examples include quotient spaces in linear algebra, quotient spaces in topology, quotient groups, homogeneous spaces, quotient rings, quotient monoids, and quotient categories.\n\nExamples\n If  is the set of all cars, and  is the equivalence relation \"has the same color as\", then one particular equivalence class consists of all green cars.  could be naturally identified with the set of all car colors.\n Let  be the set of all rectangles in a plane, and  the equivalence relation \"has the same area as\". For each positive real number A there will be an equivalence class of all the rectangles that have area A.\n Consider the modulo 2 equivalence relation on the set  of integers:  if and only if their difference  is an even number. This relation gives rise to exactly two equivalence classes: one class consisting of all even numbers, and the other consisting of all odd numbers. Under this relation , , and  all represent the same element of .\n Let  be the set of ordered pairs of integers  with  not zero, and define an equivalence relation  on  according to which  if and only if . Then the equivalence class of the pair  can be identified with the rational number , and this equivalence relation and its equivalence classes can be used to give a formal definition of the set of rational numbers. The same construction can be generalized to the field of fractions of any integral domain.\n If  consists of all the lines in, say the Euclidean plane, and L ~ M means that L and M are parallel lines, then the set of lines that are parallel to each other form an equivalence class as long as a line is considered parallel to itself. In this situation, each equivalence class determines a point at infinity.\n\nNotation and formal definition\nAn equivalence relation is a binary relation  satisfying three properties:\nFor every element  in ,  (reflexivity),\nFor every two elements  and  in , if , then  (symmetry),\nFor every three elements , , and  in , if  and , then  (transitivity).\n\nThe equivalence class of an element  is denoted  and is defined as the set\n\nof elements that are related to  by . An alternative notation  can be used to denote the equivalence class of the element , specifically with respect to the equivalence relation . This is said to be the -equivalence class of .\n\nThe set of all equivalence classes in  with respect to an equivalence relation  is denoted as  and called  modulo  (or the quotient set of  by ). The surjective map  from  onto , which maps each element to its equivalence class, is called the canonical surjection or the canonical projection map.\n\nWhen an element is chosen (often implicitly) in each equivalence class, this defines an injective map called a section. If this section is denoted by , one has  for every equivalence class . The element  is called a representative of . Any element of a class may be chosen as a representative of the class, by choosing the section appropriately.\n\nSometimes, there is a section that is more \"natural\" than the other ones. In this case, the representatives are called canonical representatives. For example, in modular arithmetic, consider the equivalence relation on the integers defined by  if  is a multiple of a given integer , called the modulus. Each class contains a unique non-negative integer smaller than , and these integers are the canonical representatives. The class and its representative are more or less identified, as is witnessed by the fact that the notation  may denote either the class or its canonical representative (which is the remainder of the division of  by ).\n\nProperties\nEvery element  of  is a member of the equivalence class . Every two equivalence classes  and  are either equal or disjoint. Therefore, the set of all equivalence classes of  forms a partition of : every element of  belongs to one and only one equivalence class. Conversely every partition of  comes from an equivalence relation in this way, according to which  if and only if  and  belong to the same set of the partition.\n\nIt follows from the properties of an equivalence relation that\n  if and only if .\n\nIn other words, if  is an equivalence relation on a set , and  and  are two elements of , then these statements are equivalent:\n\n \n \n \n\nGraphical representation\nthumb|160px|Graph of an example equivalence with 7 classes\nAn undirected graph may be associated to any symmetric relation on a set , where the vertices are the elements of , and two vertices  and  are joined if and only if . Among these graphs are the graphs of equivalence relations; they are characterized as the graphs such that the connected components are cliques.\n\nInvariants\nIf  is an equivalence relation on , and  is a property of elements of  such that whenever ,  is true if  is true, then the property  is said to be an invariant of , or well-defined under the relation .\n\nA frequent particular case occurs when  is a function from  to another set ; if   whenever , then  is said to be class invariant under , or simply invariant under . This occurs, e.g. in the character theory of finite groups. Some authors use \"compatible with \" or just \"respects \" instead of \"invariant under \".\n\nAny function  itself defines an equivalence relation on  according to which  if and only if . The equivalence class of  is the set of all elements in  which get mapped to , i.e. the class  is the inverse image of . This equivalence relation is known as the kernel of .\n\nMore generally, a function may map equivalent arguments (under an equivalence relation  on ) to equivalent values (under an equivalence relation  on ). Such a function is a morphism of sets equipped with an equivalence relation.\n\nQuotient space in topology\n\nIn topology, a quotient space is a topological space formed on the set of equivalence classes of an equivalence relation on a topological space using the original space's topology to create the topology on the set of equivalence classes.\n\nIn abstract algebra, congruence relations on the underlying set of an algebra allow the algebra to induce an algebra on the equivalence classes of the relation, called a quotient algebra. In linear algebra, a quotient space is a vector space formed by taking a quotient group where the quotient homomorphism is a linear map. By extension, in abstract algebra, the term quotient space may be used for quotient modules, quotient rings, quotient groups, or any quotient algebra. However, the use of the term for the more general cases can as often be by analogy with the orbits of a group action.\n\nThe orbits of a group action on a set may be called the quotient space of the action on the set, particularly when the orbits of the group action are the right cosets of a subgroup of a group, which arise from the action of the subgroup on the group by left translations, or respectively the left cosets as orbits under right translation.\n\nA normal subgroup of a topological group, acting on the group by translation action, is a quotient space in the senses of topology, abstract algebra, and group actions simultaneously.\n\nAlthough the term can be used for any equivalence relation's set of equivalence classes, possibly with further structure, the intent of using the term is generally to compare that type of equivalence relation on a set  either to an equivalence relation that induces some structure on the set of equivalence classes from a structure of the same kind on , or to the orbits of a group action. Both the sense of a structure preserved by an equivalence relation and the study of invariants under group actions lead to the definition of invariants of equivalence relations given above.\n\nSee also\nEquivalence partitioning, a method for devising test sets in software testing based on dividing the possible program inputs into equivalence classes according to the behavior of the program on those inputs\nHomogeneous space, the quotient space of Lie groups\nTransversal (combinatorics)\n\nNotes\n\nReferences\n \n \n \n \n \n\nFurther reading\nThis material is basic and can be found in any text dealing with the fundamentals of proof technique, such as any of the following:\n\nExternal links\n\nCategory:Algebra\nCategory:Binary relations\nCategory:Set theory"
    },
    {
      "title": "Euler's totient function",
      "url": "https://en.wikipedia.org/wiki/Euler%27s_totient_function",
      "text": "thumb|The first thousand values of . The points on the top line represent  when  is a prime number, which is \n\nIn number theory, Euler's totient function counts the positive integers up to a given integer  that are relatively prime to . It is written using the Greek letter phi as  or , and may also be called Euler's phi function. In other words, it is the number of integers  in the range  for which the greatest common divisor  is equal to 1. The integers  of this form are sometimes referred to as totatives of .\n\nFor example, the totatives of  are the six numbers 1, 2, 4, 5, 7 and 8. They are all relatively prime to 9, but the other three numbers in this range, 3, 6, and 9 are not, because  and . Therefore, . As another example,  since for  the only integer in the range from 1 to  is 1 itself, and .\n\nEuler's totient function is a multiplicative function, meaning that if two numbers  and  are relatively prime, then .\nThis function gives the order of the multiplicative group of integers modulo  (the group of units of the ring ).See Euler's theorem. It also plays a key role in the definition of the RSA encryption system.\n\n History, terminology, and notation \n\nLeonhard Euler introduced the function in 1763.L. Euler \"Theoremata arithmetica nova methodo demonstrata\" (An arithmetic theorem proved by a new method), Novi commentarii academiae scientiarum imperialis Petropolitanae (New Memoirs of the Saint-Petersburg Imperial Academy of Sciences), 8 (1763), 74–104. (The work was presented at the Saint-Petersburg Academy on October 15, 1759. A work with the same title was presented at the Berlin Academy on June 8, 1758). Available on-line in: Ferdinand Rudio, , Leonhardi Euleri Commentationes Arithmeticae, volume 1, in: Leonhardi Euleri Opera Omnia, series 1, volume 2 (Leipzig, Germany, B. G. Teubner, 1915), pages 531–555. On page 531, Euler defines  as the number of integers that are smaller than  and relatively prime to  (… aequalis sit multitudini numerorum ipso N minorum, qui simul ad eum sint primi, …), which is the phi function, φ(N).Sandifer, p. 203Graham et al. p. 133 note 111 However, he did not at that time choose any specific symbol to denote it. In a 1784 publication, Euler studied the function further, choosing the Greek letter  to denote it: he wrote  for \"the multitude of numbers less than , and which have no common divisor with it\".L. Euler, Speculationes circa quasdam insignes proprietates numerorum, Acta Academiae Scientarum Imperialis Petropolitinae, vol. 4, (1784), pp. 18–30, or Opera Omnia, Series 1, volume 4, pp. 105–115. (The work was presented at the Saint-Petersburg Academy on October 9, 1775). This definition varies from the current definition for the totient function at  but is otherwise the same. The now-standard notationBoth  and  are seen in the literature. These are two forms of the lower-case Greek letter phi.  comes from Gauss's 1801 treatise Disquisitiones Arithmeticae,Gauss, Disquisitiones Arithmeticae article 38 although Gauss didn't use parentheses around the argument and wrote . Thus, it is often called Euler's phi function or simply the phi function.\n\nIn 1879, J. J. Sylvester coined the term totient for this function,J. J. Sylvester (1879) \"On certain ternary cubic-form equations\", American Journal of Mathematics, 2 : 357-393; Sylvester coins the term \"totient\" on page 361. so it is also referred to as Euler's totient function, the Euler totient, or Euler's totient. Jordan's totient is a generalization of Euler's.\n\nThe cototient of  is defined as . It counts the number of positive integers less than or equal to  that have at least one prime factor in common with .\n\n Computing Euler's totient function \n\nThere are several formulas for computing .\n\nEuler's product formula\n\nIt states\n\nwhere the product is over the distinct prime numbers dividing . (The notation is described in the article Arithmetical function.)\n\nThe proof of Euler's product formula depends on two important facts.\n\n The function is multiplicative \n\nThis means that if , then . (Outline of proof: let , ,  be the sets of nonnegative integers, which are, respectively, coprime to and less than , , and ; then there is a bijection between  and , by the Chinese remainder theorem.)\n\n Value for a prime power argument \n\nIf  is prime and , then\n\nProof: since  is a prime number the only possible values of  are , and the only way for  to not equal 1 is for  to be a multiple of . The multiples of  that are less than or equal to  are , and there are  of them. Therefore, the other  numbers are all relatively prime to .\n\nProof of Euler's product formula\n\nThe fundamental theorem of arithmetic states that if  there is a unique expression for ,\n\nwhere  are prime numbers and each . (The case  corresponds to the empty product.)\n\nRepeatedly using the multiplicative property of  and the formula for  gives\n\nThis is Euler's product formula.\n\nExample\n\nIn words, this says that the distinct prime factors of 36 are 2 and 3; half of the thirty-six integers from 1 to 36 are divisible by 2, leaving eighteen; a third of those are divisible by 3, leaving twelve numbers that are coprime to 36. And indeed there are twelve positive integers that are coprime with 36 and lower than 36: 1, 5, 7, 11, 13, 17, 19, 23, 25, 29, 31, and 35.\n\nFourier transform\n\nThe totient is the discrete Fourier transform of the gcd, evaluated at 1. Let\n\nwhere  for . Then\n\nThe real part of this formula is\n\nNote that unlike the other two formulae (the Euler product and the divisor sum) this one does not require knowing the factors of . However, it does involve the calculation of the greatest common divisor of  and every positive integer less than , which suffices to provide the factorization anyway.\n\nDivisor sum\n\nThe property established by Gauss,Gauss, DA, art 39 that\n\nwhere the sum is over all positive divisors  of , can be proven in several ways. (see Arithmetical function for notational conventions.)\n\nOne way is to note that  is also equal to the number of possible generators of the cyclic group ; specifically, if , then  is a generator for every  coprime to . Since every element of  generates a cyclic subgroup, and all subgroups of  are generated by some element of , the formula follows.Gauss, DA art. 39, arts. 52-54 In the article Root of unity Euler's formula is derived by using this argument in the special case of the multiplicative group of the th roots of unity.\n\nThis formula can also be derived in a more concrete manner.Graham et al. pp. 134-135 Let  and consider the fractions between 0 and 1 with denominator 20:\n\nPut them into lowest terms:\n\nFirst note that all the divisors of 20 are denominators. And second, note that there are 20 fractions. Which fractions have 20 as denominator? The ones whose numerators are relatively prime to 20 (, , , , , , , ). By definition this is  fractions. Similarly, there are  fractions with denominator 10 (, , , ),  fractions with denominator 5 (, , , ), and so on.\n\nIn detail, we are considering the fractions of the form  where  is an integer from 1 to  inclusive. Upon reducing these to lowest terms, each fraction will have as its denominator some divisor of . We can group the fractions together by denominator, and we must show that for a given divisor  of , the number of such fractions with denominator  is .\n\nNote that to reduce  to lowest terms, we divide the numerator and denominator by . The reduced fractions with denominator  are therefore precisely the ones originally of the form  in which . The question therefore becomes: how many  are there less than or equal to  which verify ? Any such  must clearly be a multiple of , but it must also be coprime to  (if it had any common divisor  with , then  would be a larger common divisor of  and ). Conversely, any multiple  of  which is coprime to  will satisfy . We can generate  such numbers by taking the numbers less than  coprime to  and multiplying each one by  (these products will of course each be smaller than , as required). This in fact generates all such numbers, as if  is a multiple of  coprime to  (and less than ), then  will still be coprime to , and must also be smaller than , else  would be larger than . Thus there are precisely  values of  less than or equal to  such that , which was to be demonstrated.\n\nMöbius inversion gives\n\nwhere  is the Möbius function.\n\nThis formula may also be derived from the product formula by multiplying out\n\nto get\n\nSome values of the function\n\nThe first 143 values  are shown in the table and graph below:The cell for  in the upper-left corner of the table is empty, as the function  is commonly defined only for positive integers, so it is not defined for .\n\nthumb|Graph of the first 100 values\n{| class=\"wikitable\" style=\"text-align: right\"\n|+ for \n! +\n! 0 || 1 || 2 || 3 || 4 || 5 || 6 || 7 || 8 || 9 || 10 || 11\n|- \n! 0 \n|  || 1 || 1 || 2 || 2 || 4 || 2 || 6 || 4 || 6 || 4 || 10\n|-\n! 12\n| 4 || 12 || 6 || 8 || 8 || 16 || 6 || 18 || 8 || 12 || 10 || 22\n|- \n! 24\n| 8 || 20 || 12 || 18 || 12 || 28 || 8 || 30 || 16 || 20 || 16 || 24\n|-\n! 36\n| 12 || 36 || 18 || 24 || 16 || 40 || 12 || 42 || 20 || 24 || 22 || 46\n|-\n! 48\n| 16 || 42 || 20 || 32 || 24 || 52 || 18 || 40 || 24 || 36 || 28 || 58\n|-\n! 60\n| 16 || 60 || 30 || 36 || 32 || 48 || 20 || 66 || 32 || 44 || 24 || 70\n|-\n! 72\n| 24 || 72 || 36 || 40 || 36 || 60 || 24 || 78 || 32 || 54 || 40 || 82\n|-\n! 84\n| 24 || 64 || 42 || 56 || 40 || 88 || 24 || 72 || 44 || 60 || 46 || 72\n|-\n! 96\n| 32 || 96 || 42 || 60 || 40 || 100 || 32 || 102 || 48 || 48 || 52 || 106\n|-\n! 108\n| 36 || 108 || 40 || 72 || 48 || 112 || 36 || 88 || 56 || 72 || 58 || 96\n|-\n! 120\n| 32 || 110 || 60 || 80 || 60 || 100 || 36 || 126 || 64 || 84 || 48 || 130\n|-\n! 132\n| 40 || 108 || 66 || 72 || 64 || 136 || 44 || 138 || 48 || 92 || 70 || 120\n|}\n\nThe top line in the graph, , is a true upper bound. It is attained whenever  is prime. There is no lower bound that is a straight line of positive slope; no matter how gentle the slope of a line is, there will eventually be points of the plot below the line. More precisely, the lower limit of the graph is proportional to  rather than being linear.\n\nEuler's theorem\n\nThis states that if  and  are relatively prime then\n\nThe special case where  is prime is known as Fermat's little theorem.\n\nThis follows from Lagrange's theorem and the fact that  is the order of the multiplicative group of integers modulo .\n\nThe RSA cryptosystem is based on this theorem: it implies that the inverse of the function , where  is the (public) encryption exponent, is the function , where , the (private) decryption exponent, is the multiplicative inverse of  modulo . The difficulty of computing  without knowing the factorization of  is thus the difficulty of computing : this is known as the RSA problem which can be solved by factoring . The owner of the private key knows the factorization, since an RSA private key is constructed by choosing  as the product of two (randomly chosen) large primes  and . Only  is publicly disclosed, and given the difficulty to factor large numbers we have the guarantee that no-one else knows the factorization.\n\nOther formulae\n\nNote the special cases\n\nCompare this to the formula\n\n(See least common multiple.)\n\n is even for . Moreover, if  has  distinct odd prime factors, \n For any  and  such that  there exists an  such that .\n\nwhere  is the radical of .\n Dineva (in external refs), prop. 1\n\n ( cited in)\n\n \n \n \n\n(where  is the Euler–Mascheroni constant).\n\nwhere  is a positive integer and  is the number of distinct prime factors of .Bordellès in the external links\n\nMenon's identity\n\nIn 1965 P. Kesava Menon proved\n\nwhere  is the number of divisors of .\n\nFormulae involving the golden ratio\n\nSchneiderAll formulae in the section are from Schneider (in the external links) found a pair of identities connecting the totient function, the golden ratio and the Möbius function . In this section  is the totient function, and  is the golden ratio.\n\nThey are:\n\nand\n\nSubtracting them gives\n\nApplying the exponential function to both sides of the preceding identity yields an infinite product formula for :\n\nThe proof is based on the two formulae\n\nGenerating functions\n\nThe Dirichlet series for  may be written in terms of the Riemann zeta function as:\n\nThe Lambert series generating function is\n\nwhich converges for .\n\nBoth of these are proved by elementary series manipulations and the formulae for .\n\nGrowth rate\n\nIn the words of Hardy & Wright, the order of  is “always ‘nearly ’.”\n\nFirst\n\nbut as n goes to infinity, for all \n\nThese two formulae can be proved by using little more than the formulae for  and the divisor sum function .\n\nIn fact, during the proof of the second formula, the inequality\n\ntrue for , is proved.\n \nWe also have\n\nHere  is Euler's constant, , so  and .\n\nProving this does not quite require the prime number theorem.In fact Chebyshev's theorem () and\nMertens' third theorem is all that is needed. Since  goes to infinity, this formula shows that\n\nIn fact, more is true.Theorem 15 of Bach & Shallit, thm. 8.8.7\n\nand\n\nThe second inequality was shown by Jean-Louis Nicolas. Ribenboim says \"The method of proof is interesting, in that the inequality is shown first under the assumption that the Riemann hypothesis is true, secondly under the contrary assumption.\"\n\nFor the average order, we haveSándor, Mitrinović & Crstici (2006) pp.24–25\n\ndue to Arnold Walfisz, its proof exploiting estimates on exponential sums due to I. M. Vinogradov and N. M. Korobov (this is currently the best known estimate of this type). The \"Big \" stands for a quantity that is bounded by a constant times the function of  inside the parentheses (which is small compared to ).\n\nThis result can be used to prove that the probability of two randomly chosen numbers being relatively prime is .\n\nRatio of consecutive values\n\nIn 1950 Somayajulu provedRibenboim, p.38Sándor, Mitrinović & Crstici (2006) p.16\n\nIn 1954 Schinzel and Sierpiński strengthened this, proving that the set\n\nis dense in the positive real numbers. They also proved that the set\n\nis dense in the interval (0,1).\n\nTotient numbers\nA totient number is a value of Euler's totient function: that is, an  for which there is at least one  for which . The valency or multiplicity of a totient number  is the number of solutions to this equation.Guy (2004) p.144 A nontotient is a natural number which is not a totient number. Every odd integer exceeding 1 is trivially a nontotient. There are also infinitely many even nontotients,Sándor & Crstici (2004) p.230 and indeed every positive integer has a multiple which is an even nontotient.\n\nThe number of totient numbers up to a given limit  is\n\nfor a constant .\n\nIf counted accordingly to multiplicity, the number of totient numbers up to a given limit  is\n\nwhere the error term  is of order at most  for any positive .Sándor et al (2006) p.22\n\nIt is known that the multiplicity of  exceeds  infinitely often for any .Sándor et al (2006) p.21Guy (2004) p.145\n\nFord's theorem\n\n proved that for every integer  there is a totient number  of multiplicity : that is, for which the equation  has exactly  solutions; this result had previously been conjectured by Wacław Sierpiński,Sándor & Crstici (2004) p.229 and it had been obtained as a consequence of Schinzel's hypothesis H. Indeed, each multiplicity that occurs, does so infinitely often.\n\nHowever, no number  is known with multiplicity . Carmichael's totient function conjecture is the statement that there is no such .Sándor & Crstici (2004) p.228\n\nPerfect totient numbers\n\nApplications\n\nCyclotomy\n\nIn the last section of the DisquisitionesGauss, DA. The 7th § is arts. 336–366Gauss proved if  satisfies certain conditions then the -gon can be constructed. In 1837 Pierre Wantzel proved the converse, if the -gon is constructible, then  must satisfy Gauss's conditions Gauss provesGauss, DA, art 366 that a regular -gon can be constructed with straightedge and compass if  is a power of 2. If  is a power of an odd prime number the formula for the totient says its totient can be a power of two only if  is a first power and  is a power of 2. The primes that are one more than a power of 2 are called Fermat primes, and only five are known: 3, 5, 17, 257, and 65537. Fermat and Gauss knew of these. Nobody has been able to prove whether there are any more.\n\nThus, a regular -gon has a straightedge-and-compass construction if n is a product of distinct Fermat primes and any power of 2. The first few such  areGauss, DA, art. 366. This list is the last sentence in the Disquisitiones\n2, 3, 4, 5, 6, 8, 10, 12, 15, 16, 17, 20, 24, 30, 32, 34, 40,... .\n\nThe RSA cryptosystem\n\nSetting up an RSA system involves choosing large prime numbers  and , computing  and , and finding two numbers  and  such that . The numbers  and  (the \"encryption key\") are released to the public, and  (the \"decryption key\") is kept private.\n\nA message, represented by an integer , where , is encrypted by computing .\n\nIt is decrypted by computing . Euler's Theorem can be used to show that if , then .\n\nThe security of an RSA system would be compromised if the number  could be factored or if  could be computed without factoring .\n\nUnsolved problems\n\nLehmer's conjecture\n\nIf  is prime, then . In 1932 D. H. Lehmer asked if there are any composite numbers  such that . None are known.Ribenboim, pp. 36–37.\n\nIn 1933 he proved that if any such  exists, it must be odd, square-free, and divisible by at least seven primes (i.e. ). In 1980 Cohen and Hagis proved that  and that . Further, Hagis showed that if 3 divides  then  and .Guy (2004) p.142\n\nCarmichael's conjecture\n\nThis states that there is no number  with the property that for all other numbers , , . See Ford's theorem above.\n\nAs stated in the main article, if there is a single counterexample to this conjecture, there must be infinitely many counterexamples, and the smallest one has at least ten billion digits in base 10.\n\n See also \nCarmichael function\nDuffin–Schaeffer conjecture\nGeneralizations of Fermat's little theorem\nHighly composite number\nMultiplicative group of integers modulo \nRamanujan sum\n\n Notes \n\nReferences\n\nThe Disquisitiones Arithmeticae has been translated from Latin into English and German. The German edition includes all of Gauss' papers on number theory: all the proofs of quadratic reciprocity, the determination of the sign of the Gauss sum, the investigations into biquadratic reciprocity, and unpublished notes.\n\nReferences to the Disquisitiones are of the form Gauss, DA, art. nnn.\n\n. See paragraph 24.3.2.\n\n.\n\n \n\n \n \n\n \n \n.\n\nExternal links\n \nEuler's Phi Function and the Chinese Remainder Theorem — proof that  is multiplicative\nEuler's totient function calculator in JavaScript — up to 20 digits\nDineva, Rosica, The Euler Totient, the Möbius, and the Divisor Functions\nPlytage, Loomis, Polhill Summing Up The Euler Phi Function\n\nCategory:Modular arithmetic\nCategory:Multiplicative functions\nCategory:Articles containing proofs\nCategory:Algebra\nCategory:Number theory\nCategory:Leonhard Euler"
    },
    {
      "title": "Factorization of polynomials over finite fields",
      "url": "https://en.wikipedia.org/wiki/Factorization_of_polynomials_over_finite_fields",
      "text": "In mathematics and computer algebra the factorization of a polynomial consists of decomposing it into a product of irreducible factors. This decomposition is theoretically possible and is unique for polynomials with coefficients in any field, but rather strong restrictions on the field of the coefficients are needed to allow the computation of the factorization by means of an algorithm. In practice, algorithms have been designed only for polynomials with coefficients in a finite field, in the field of rationals or in a finitely generated field extension of one of them.\n\nAll factorization algorithms, including the case of multivariate polynomials over the rational numbers, reduce the problem to this case; see polynomial factorization. It is also used for various applications of finite fields, such as coding theory (cyclic redundancy codes and BCH codes), cryptography (public key cryptography by the means of elliptic curves), and computational number theory.\n\nAs the reduction of the factorization of multivariate polynomials to that of univariate polynomials does not have any specificity in the case of coefficients in a finite field, only polynomials with one variable are considered in this article.\n\nBackground\n\nFinite field\n\nThe theory of finite fields, whose origins can be traced back to the works of Gauss and Galois, has played a part in various branches of mathematics. Due to the applicability of the concept in other topics of mathematics and sciences like computer science there has been a resurgence of interest in finite fields and this is partly due to important applications in coding theory and cryptography. Applications of finite fields introduce some of these developments in cryptography, computer algebra and coding theory.\n\nA finite field or Galois field is a field with a finite order (number of elements). The order of a finite field is always a prime or a power of prime. For each prime power q = pr, there exists exactly one finite field with q elements, up to isomorphism. This field is denoted GF(q) or Fq. If p is prime, GF(p) is the prime field of order p; it is the field of residue classes modulo p, and its p elements are denoted 0, 1, ..., p−1. Thus a = b in GF(p) means the same as a ≡ b (mod p).\n\nIrreducible polynomials\nLet F be a finite field. As for general fields, a non-constant polynomial f in F[x] is said to be irreducible over F if it is not the product of two polynomials of positive degree. A polynomial of positive degree that is not irreducible over F is called reducible over F.\n\nIrreducible polynomials allow us to construct the finite fields of non-prime order. In fact, for a prime power q, let Fq be the finite field with q elements, unique up to isomorphism. A polynomial f of degree n greater than one, which is irreducible over Fq, defines a field extension of degree n which is isomorphic to the field with qn elements: the elements of this extension are the polynomials of degree lower than n; addition, subtraction and multiplication by an element of Fq are those of the polynomials; the product of two elements is the remainder of the division by f of their product as polynomials; the inverse of an element may be computed by the extended GCD algorithm (see Arithmetic of algebraic extensions).\n\nIt follows that, to compute in a finite field of non prime order, one needs to generate an irreducible polynomial. For this, the common method is to take a polynomial at random and test it for irreducibility. For sake of efficiency of the multiplication in the field, it is usual to search for polynomials of the shape xn + ax + b.\n\nIrreducible polynomials over finite fields are also useful for Pseudorandom number generators using feedback shift registers and discrete logarithm over F2n.\n\nThe number of irreducible monic polynomials of degree n over Fq is the number of aperioidic necklaces, given by Moreau's necklace-counting function Mq(n). The closely related necklace function Nq(n) counts monic polynomials of degree n which are primary (a power of an irreducible); or alternatively irreducible polynomials of all degrees d which divide n.Christophe Reutenauer, Mots circulaires et polynomes irreductibles, Ann. Sci. math Quebec, vol 12, no 2, pp. 275-285\n\n Example \nThe polynomial P = x4 + 1 is irreducible over Q but not over any finite field.\n\n On any field extension of F2, P = (x+1)4.\nOn every other finite field, at least one of −1, 2 and −2 is a square, because the product of two non-squares is a square and so we have\nIf  then \nIf  then \nIf  then \n\nComplexity\nPolynomial factoring algorithms use basic polynomial operations such as products, divisions, gcd, powers of one polynomial modulo another, etc. A multiplication of two polynomials of degree at most n can be done in O(n2) operations in Fq using \"classical\" arithmetic, or in O(nlog(n) log(log(n)) ) operations in Fq using \"fast\" arithmetic. A Euclidean division (division with remainder) can be performed within the same time bounds. The cost of a polynomial greatest common divisor between two polynomials of degree at most n can be taken as O(n2) operations in Fq using classical methods, or as O(nlog2(n) log(log(n)) ) operations in Fq using fast methods.  For polynomials h, g of degree at most n, the exponentiation hq mod g can be done with O(log(q)) polynomial products, using exponentiation by squaring method, that is O(n2log(q)) operations in Fq using classical methods, or O(nlog(q)log(n) log(log(n))) operations in Fq using fast methods.\n\nIn the algorithms that follow, the complexities are expressed in terms of number of arithmetic operations in Fq, using classical algorithms for the arithmetic of polynomials.\n\nFactoring algorithms\nMany algorithms for factoring polynomials over finite fields include the following three stages:\n Square-free factorization\n Distinct-degree factorization\n Equal-degree factorization\nAn important exception is Berlekamp's algorithm, which combines stages 2 and 3.\n\nBerlekamp's algorithm\n\nThe Berlekamp's algorithm is historically important as being the first factorization algorithm, which works well in practice. However, it contains a loop on the elements of the ground field, which implies that it is practicable only over small finite fields. For a fixed ground field, its time complexity is polynomial, but, for general ground fields, the complexity is exponential in the size of the ground field.\n\nSquare-free factorization\nThe algorithm determines a square-free factorization for polynomials whose coefficients come from the finite field Fq of order q = pm with p a prime.  This algorithm firstly determines the derivative and then computes the gcd of the polynomial and its derivative. If it is not one then the gcd is again divided into the original polynomial, provided that the derivative is not zero (a case that exists for non-constant polynomials defined over finite fields).\n\nThis algorithm uses the fact that, if the derivative of a polynomial is zero, then it is a polynomial in xp, which is, if the coefficients belong to Fp, the pth power of the polynomial obtained by substituting x by x1/p. If the coefficients do not belong to Fp, the p-th root of a polynomial with zero derivative is obtained by the same substitution on x, completed by applying the inverse of the Frobenius automorphism to the coefficients.\n\nThis algorithm works also over a field of characteristic zero, with the only difference that it never enters in the blocks of instructions where pth roots are computed. However, in this case, Yun's algorithm is much more efficient because it computes the greatest common divisors of polynomials of lower degrees. A consequence is that, when factoring a polynomial over the integers, the algorithm which follows is not used: one compute first the square-free factorization over the integers, and to factor the resulting polynomials, one chooses a p such that they remain square-free modulo p. \n    Algorithm: SFF (Square-Free Factorization)\n    Input: A monic polynomial f in Fq[x] where q=pm\n    Output: Square-free factorization of f\n    R ← 1\n    \n    # Make w be the product (without multiplicity) of all factors of f that have \n    # multiplicity not divisible by p\n    c ← gcd(f, f′)\n    w ← f/c \n    \n    # Step 1: Identify all factors in w\n    i←1 \n    while w ≠ 1 do\n        y ← gcd(w, c)\n        fac ← w/y\n        R ← R·faci\n        w ← y; c ← c/y; i ← i+1 \n    end while\n    # c is now the product (with multiplicity) of the remaining factors of f\n    \n    # Step 2: Identify all remaining factors using recursion\n    # Note that these are the factors of f that have multiplicity divisible by p\n    if c ≠ 1 then\n        c ← c1/p\n        R ← R·SFF(c)p\n    end if \n    \n    Output(R)\n \nThe idea is to identify the product of all irreducible factors of f with the same multiplicity. This is done in two steps. The first step uses the formal derivative of f to find all the factors with multiplicity not divisible by p. The second step identifies the remaining factors. As all of the remaining factors have multiplicity divisible by p, meaning they are powers of p, one can simply take the p-th square root and apply recursion. \n\nExample of a square-free factorization\nLet\n\nto be factored over the field with three elements.\n\nThe algorithm computes first\n\nSince the derivative is non-zero we have  and we enter the while loop.  After one loop we have ,  and  with updates ,  and . The second time through the loop gives , , , with updates ,  and . The third time through the loop also does not change . For the fourth time through the loop we get , , , with updates ,  and . Since w = 1, we exit the while loop. Since c ≠ 1, it must be a perfect cube. The cube root of c, obtained by replacing x3 by x is x2 + 1, and calling the square-free procedure recursively determines that it is square-free. Therefore, cubing it and combining it with the value of R to that point gives the square-free decomposition\n\nDistinct-degree factorization\nThis algorithm splits a square-free polynomial into a product of polynomials whose irreducible factors all have the same degree. Let f ∈ Fq[x] of degree n be the polynomial to be factored.\n\n    Algorithm Distinct-degree factorization(DDF)\n    Input: A monic square-free polynomial  f ∈ Fq[x]\n    Output: The set of all pairs (g, d), such that \n              f has an irreducible factor of degree d and\n              g is the product of all monic irreducible factors of f of degree d.\n    Begin\n        \n        while  do \n            \n            if g ≠ 1, then \n              ;\n              f* := f*/g;\n            end if\n            i := i+1;\n        end while;\n        if f* ≠ 1, then ;\n        if S = ∅\n            then return {(f, 1)}\n            else return S\n     End\nThe correctness of the algorithm is based on the following:\n\nLemma. For i ≥ 1 the polynomial\n\nis the product of all monic irreducible polynomials in Fq[x] whose degree divides i.\n\nAt first glance, this is not efficient since it involves computing the GCD of polynomials of a degree which is exponential in the degree of the input polynomial. However\n\nmay be replaced by\n\nTherefore, we have to compute:\n\nthere are two methods:\n\nMethod I. Start from the value of\n\ncomputed at the preceding step and to compute its q-th power modulo the new f*, using exponentiation by squaring method. This needs\n\narithmetic operations in Fq at each step, and thus\n\narithmetic operations for the whole algorithm.\n\nMethod II. Using the fact that the q-th power is a linear map over Fq we may compute its matrix with\n\noperations. Then at each iteration of the loop, compute the product of a matrix by a vector (with O(deg(f)2) operations). This induces a total number of operations in Fq which is\n\nThus this second method is more efficient and is usually preferred. Moreover, the matrix that is computed in this method is used, by most algorithms, for equal-degree factorization (see below); thus using it for the distinct-degree factorization saves further computing time.\n\nEqual-degree factorization\n\nCantor–Zassenhaus algorithm\n\nIn this section, we consider the factorization of a monic squarefree univariate polynomial f, of degree n, over a finite field Fq, which has r ≥ 2 pairwise distinct irreducible factors  each of degree d.\n\nWe first describe an algorithm by Cantor and Zassenhaus (1981) and then a variant that has a slightly better complexity. Both are probabilistic algorithms whose running time depends on random choices (Las Vegas algorithms), and have a good average running time. In next section we describe an algorithm by Shoup (1990), which is also an equal-degree factorization algorithm, but is deterministic. All these algorithms require an odd order q for the field of coefficients. For more factorization algorithms see e.g. Knuth's book The Art of Computer Programming volume 2.\n\n     Algorithm Cantor–Zassenhaus algorithm.\n     Input: A finite field Fq of odd order q.\n            A monic square free polynomial f in Fq[x] of degree n = rd, \n                 which has r ≥ 2 irreducible factors each of degree d\n     Output: The set of monic irreducible factors of f.\n\n     Factors:={f};\n     while Size(Factors) < r do,\n        Choose h in Fq[x] with deg(h) < n at random;\n        \n        for each u in Factors with deg(u) > d do\n            if gcd(g, u) ≠ 1 and gcd(g, u) ≠ u, then\n              Factors:= Factors;\n            endif;\n     endwhile\n     return Factors.\n\nThe correctness of this algorithm relies on the fact that the ring Fq[x]/f is a direct product of the fields Fq[x]/fi where fi runs on the irreducible factors of f. As all these fields have qd elements, the component of g in any of these fields is zero with probability\n\nThis implies that the polynomial gcd(g, u) is the product of the factors of g for which the component of g is zero.\n\nIt has been shown that the average number of iterations of the while loop of the algorithm is less than , giving an average number of arithmetic operations in Fq which is .\n\nIn the typical case where dlog(q) > n, this complexity may be reduced to\n\nby choosing h in the kernel of the linear map\n\nand replacing the instruction\n\nby\n\nThe proof of validity is the same as above, replacing the direct product of the fields Fq[x]/fi by the direct product of their subfields with q elements. The complexity is decomposed in  for the algorithm itself,  for the computation of the matrix of the linear map (which may be already computed in the square-free factorization) and O(n3) for computing its kernel. It may be noted that this algorithm works also if the factors have not the same degree (in this case the number r of factors, needed for stopping the while loop, is found as the dimension of the kernel). Nevertheless, the complexity is slightly better if square-free factorization is done before using this algorithm (as n may decrease with square-free factorization, this reduces the complexity of the critical steps).\n\nVictor Shoup's algorithm\nLike the algorithms of the preceding section, Victor Shoup's algorithm is an equal-degree factorization algorithm.Victor Shoup, On the deterministic complexity of factoring polynomials over finite fields, Information Processing Letters 33:261-267, 1990 Unlike them, it is a deterministic algorithm. However, it is less efficient, in practice, that the algorithms of preceding section. For Shoup's algorithm, the input is restricted to polynomials over prime fields Fp.\n\nThe worst case time complexity of Shoup's algorithm has a factor  Although exponential, this complexity is much better that previous deterministic algorithms (Berlekamp's algorithm) which have  as a factor. However, there are very few polynomials for which the computing time is exponential, and the average time complexity of the algorithm is polynomial in  where  is the degree of the polynomial, and  is the number of elements of the ground field.\n\nLet g = g1 ... gk be the desired factorization, where the gi are distinct monic irreducible polynomials of degree d. Let n = deg(g) = kd. We consider the ring R = Fq[x]/g and denote also by x the image of x in R. The ring R is the direct product of the fields Ri = Fq[x]/gi, and we denote by pi the natural homomorphism from the R onto Ri. The Galois group of Ri over Fq is cyclic of order d, generated by the  field automorphism u → up. It follows that the roots of gi in Ri are\n\nLike in the preceding algorithm, this algorithm uses the same subalgebra B of R as the Berlekamp's algorithm, sometimes called the \"Berlekamp subagebra\" and defined as\n\nA subset S of B is said a separating set if, for every 1 ≤ i < j ≤ k there exists s ∈ S such that . In the preceding algorithm, a separating set is constructed by choosing at random the elements of S. In Shoup's algorithm, the separating set is constructed in the following way. Let s in R[Y] be such that\n\nThen  is a separating set because  for i =1, ..., k (the two monic polynomials have the same roots). As the gi are pairwise distinct, for every pair of distinct indexes (i, j), at least one of the coefficients sh will satisfy \n\nHaving a separating set, Shoup's algorithm proceeds as the last algorithm of the preceding section, simply by replacing the instruction \"choose at random h in the kernel of the linear map \" by \"choose h + i with h in S and i in {1, ..., k−1}\".\n\nTime complexity\nAs described in previous sections, for the factorization over finite fields, there are randomized algorithms of polynomial time complexity (for example Cantor-Zassenhaus algorithm). There are also deterministic algorithms with a polynomial average complexity (for example Shoup's algorithm).\n\nThe existence of a deterministic algorithm with a polynomial worst-case complexity is still an open problem.\n\nRabin's test of irreducibility\nLike distinct-degree factorization algorithm, Rabin's algorithm is based on the Lemma stated above. Distinct-degree factorization algorithm tests every d not greater than half the degree of the input polynomial. Rabin's algorithm takes advantage that the factors are not needed for considering fewer d. Otherwise, it is similar to distinct-degree factorization algorithm. It is based on the following fact.\n\nLet p1, ..., pk, be all the prime divisors of n, and denote , for 1 ≤ i ≤ k polynomial f in Fq[x] of degree n is irreducible in Fq[x] if and only if , for 1 ≤ i ≤ k, and f divides . In fact, if f has a factor of degree not dividing n, then f does not divide ; if f has a factor of degree dividing n, then this factor divides at least one of the \n\n  Algorithm Rabin Irreducibility Test\n  Input: A monic polynomial f in Fq[x] of degree n, \n         p1, ..., pk all distinct prime divisors of n.\n  Output: Either \"f is irreducible\" or \"f is reducible\".\n  Begin\n      for j = 1 to k do \n         ;\n      for i = 1 to k do \n         ;\n         g := gcd(f, h);\n         if g ≠ 1, then return 'f is reducible' and STOP;\n      end for;\n      ;\n      if g = 0, then return \"f is irreducible\", \n          else return \"f is reducible\"\n  end.\n\nThe basic idea of this algorithm is to compute  starting from the smallest  by repeated squaring or using the Frobenius automorphism, and then to take the correspondent gcd. Using the elementary polynomial arithmetic, the computation of the matrix of the Frobenius automorphism needs  operations in Fq, the computation of\n\nneeds O(n3) further operations, and the algorithm itself needs O(kn2) operations, giving a total of  operations in Fq. Using fast arithmetic (complexity  for multiplication and division, and  for GCD computation), the computation of the  by repeated squaring is , and the algorithm itself is , giving a total of  operations in Fq.\n\nSee also\n Berlekamp's algorithm\n Cantor–Zassenhaus algorithm\n Polynomial factorization\n\nReferences\n\nKEMPFERT,H (1969) On the Factorization of Polynomials Department of Mathematics, The Ohio State University,Columbus,Ohio 43210\nShoup,Victor (1996) Smoothness and Factoring Polynomials over Finite Fields Computer Science Department University of Toronto\n Von Zur Gathen, J.; Panario, D. (2001). Factoring Polynomials Over Finite Fields: A Survey. Journal of Symbolic Computation, Volume 31, Issues 1-2, January 2001, 3--17.\nGao Shuhong, Panario Daniel,Test and Construction of Irreducible Polynomials over Finite Fields Department of mathematical Sciences, Clemson University, South Carolina, 29634-1907, USA. and Department of computer science University of Toronto, Canada M5S-1A4\nShoup, Victor (1989) New Algorithms for Finding Irreducible Polynomials over Finite Fields Computer Science Department University of Wisconsin–Madison\nGeddes, Keith O.; Czapor, Stephen R.; Labahn, George (1992). Algorithms for computer algebra. Boston, MA: Kluwer Academic Publishers. pp. xxii+585. .\n\nExternal links\n Some irreducible polynomials http://www.math.umn.edu/~garrett/m/algebra/notes/07.pdf\n Field and Galois Theory :http://www.jmilne.org/math/CourseNotes/FT.pdf\n Galois Field:http://designtheory.org/library/encyc/topics/gf.pdf\n Factoring polynomials over finite fields: http://www.science.unitn.it/~degraaf/compalg/polfact.pdf\n\nNotes\n\nCategory:Polynomials\nCategory:Algebra\nCategory:Computer algebra\nCategory:Coding theory\nCategory:Cryptography\nCategory:Computational number theory"
    },
    {
      "title": "Field arithmetic",
      "url": "https://en.wikipedia.org/wiki/Field_arithmetic",
      "text": "In mathematics, field arithmetic is a subject that studies the interrelations between arithmetic properties of a  and its absolute Galois group.\nIt is an interdisciplinary subject as it uses tools from algebraic number theory, arithmetic geometry, algebraic geometry, model theory, the theory of finite groups and of profinite groups.\n\nFields with finite absolute Galois groups\nLet K be a field and let G = Gal(K) be its absolute Galois group. If K is algebraically closed, then G = 1. If K = R is the real numbers, then\n\nHere C is the field of complex numbers and Z is the ring of integer numbers. \nA theorem of Artin and Schreier asserts that (essentially) these are all the possibilities for finite absolute Galois groups.\n\nArtin–Schreier theorem. Let K be a field whose absolute Galois group G is finite. Then either K is separably closed and G is trivial or K is real closed and G = Z/2Z.\n\nFields that are defined by their absolute Galois groups\nSome profinite groups occur as the absolute Galois group of non-isomorphic fields.  A first example for this is\n\nThis group is isomorphic to the absolute Galois group of an arbitrary finite field. Also the absolute Galois group of the field of formal Laurent series C((t)) over the complex numbers is isomorphic to that group.\n\nTo get another example, we bring below two non-isomorphic fields whose absolute Galois groups are free (that is free profinite group).\n\n Let C be an algebraically closed field and x a variable.  Then Gal(C(x)) is free of rank equal to the cardinality of C. (This result is due to Adrien Douady for 0 characteristic and has its origins in Riemann's existence theorem. For a field of arbitrary characteristic it is due to David Harbater and Florian Pop, and was also proved later by Dan Haran and Moshe Jarden.)\n The absolute Galois group Gal(Q) (where Q are the rational numbers) is compact, and hence equipped with a normalized Haar measure. For a Galois automorphism s (that is an element in Gal(Q)) let Ns be the maximal Galois extension of  Q  that s fixes. Then with probability 1 the absolute Galois group Gal(Ns) is free of countable rank. (This result is due to Moshe Jarden.)\n\nIn contrast to the above examples, if the fields in question are finitely generated over Q, Florian Pop proves that an isomorphism of the absolute Galois groups yields an isomorphism of the fields:\n\nTheorem. Let K, L be finitely generated fields over Q and let a: Gal(K) → Gal(L) be an isomorphism. Then there exists a unique isomorphism of the algebraic closures, b: Kalg → Lalg, that induces  a.\n\nThis generalizes an earlier work of Jürgen Neukirch and Koji Uchida on number fields.\n\nPseudo algebraically closed fields\n\nA pseudo algebraically closed field (in short PAC) K is a field satisfying the following geometric property. Each absolutely irreducible algebraic variety V defined over K has a K-rational point.\n\nOver PAC fields there is a firm link between arithmetic properties of the field and group theoretic properties of its absolute Galois group. A nice theorem in this spirit connects Hilbertian fields with ω-free fields (K is ω-free if any embedding problem for K is properly solvable).\n\nTheorem. Let K be a PAC field. Then K is Hilbertian if and only if K is ω-free.\n\nPeter Roquette proved the right-to-left direction of this theorem and conjectured the opposite direction. Michael Fried and Helmut Völklein applied algebraic topology and complex analysis to establish Roquette's conjecture in characteristic zero. Later Pop \nproved the Theorem for arbitrary characteristic by developing \"rigid patching\".\n\nReferences\n\n \n\nCategory:Algebra\nCategory:Galois theory"
    },
    {
      "title": "Filtration (mathematics)",
      "url": "https://en.wikipedia.org/wiki/Filtration_%28mathematics%29",
      "text": "In mathematics, a filtration   is an indexed set  of subobjects of a given algebraic structure , with the index  running over some index set  that is a totally ordered set, subject to the condition that\n\nif  in , then .\n\nIf the index  is the time parameter of some stochastic process, then the filtration can be interpreted as representing all historical but not future information available about the stochastic process, with the algebraic object  gaining in complexity with time. Hence, a process that is adapted to a filtration , is also called non-anticipating, i.e. one that cannot see into the future.\n\nSometimes, as in a filtered algebra, there is instead the requirement that the  be subalgebras with respect to some operations (say, vector addition), but not with respect to other operations (say, multiplication), that satisfy , where the index set is the natural numbers; this is by analogy with a graded algebra.\n\nSometimes, filtrations are supposed to satisfy the additional requirement that the union of the  be the whole , or (in more general cases, when the notion of union does not make sense) that the canonical homomorphism from the direct limit of the  to  is an isomorphism. Whether this requirement is assumed or not usually depends on the author of the text and is often explicitly stated. This article does not impose this requirement.\n\nThere is also the notion of a descending filtration, which is required to satisfy  in lieu of  (and, occasionally,  instead of ). Again, it depends on the context how exactly the word \"filtration\" is to be understood. Descending filtrations are not to be confused with cofiltrations (which consist of quotient objects rather than subobjects).\n\nThe concept dual to a filtration is called a cofiltration.\n\nFiltrations are widely used in abstract algebra, homological algebra (where they are related in an important way to spectral sequences), and in measure theory and probability theory for nested sequences of σ-algebras. In functional analysis and numerical analysis, other terminology is usually used, such as scale of spaces or nested spaces.\n\nExamples\n\nAlgebra\n\nGroups\n\nIn algebra, filtrations are ordinarily indexed by , the set of natural numbers. A filtration of a group , is then a nested sequence  of normal subgroups of  (that is, for any  we have ). Note that this use of the word \"filtration\" corresponds to our \"descending filtration\".\n\nGiven a group  and a filtration , there is a natural way to define a topology on , said to be associated to the filtration. A basis for this topology is the set of all translates of subgroups appearing in the filtration, that is, a subset of  is defined to be open if it is a union of sets of the form , where  and  is a natural number.\n\nThe topology associated to a filtration on a group  makes  into a topological group.\n\nThe topology associated to a filtration  on a group  is Hausdorff if and only if .\n\nIf two filtrations  and  are defined on a group , then the identity map from  to , where the first copy of  is given the -topology and the second the -topology, is continuous if and only if for any  there is an  such that , that is, if and only if the identity map is continuous at 1. In particular, the two filtrations define the same topology if and only if for any subgroup appearing in one there is a smaller or equal one appearing in the other.\n\nRings and modules: descending filtrations\n\nGiven a ring  and an -module , a descending filtration of  is a decreasing sequence of submodules . This is therefore a special case of the notion for groups, with the additional condition that the subgroups be submodules. The associated topology is defined as for groups.\n\nAn important special case is known as the -adic topology (or -adic, etc.). Let  be a commutative ring, and  an ideal of .\n\nGiven an -module , the sequence  of submodules of  forms a filtration of . The -adic topology on  is then the topology associated to this filtration. If  is just the ring  itself, we have defined the -adic topology on .\n\nWhen  is given the -adic topology,  becomes a topological ring. If an -module  is then given the -adic topology, it becomes a topological -module, relative to the topology given on .\n\nRings and modules: ascending filtrations\n\nGiven a ring  and an -module , an ascending filtration of  is an increasing sequence of submodules . In particular, if  is a field, then an ascending filtration of the -vector space  is an increasing sequence of vector subspaces of . Flags are one important class of such filtrations.\n\nSets\nA maximal filtration of a set is equivalent to an ordering (a permutation) of the set. For instance, the filtration  corresponds to the ordering . From the point of view of the field with one element, an ordering on a set corresponds to a maximal flag (a filtration on a vector space), considering a set to be a vector space over the field with one element.\n\nMeasure theory\n\nIn measure theory, in particular in martingale theory and the theory of stochastic processes, a filtration is an increasing sequence of -algebras on a measurable space. That is, given a measurable space , a filtration is a sequence of -algebras  with  where each  is a non-negative real number and\n\nThe exact range of the \"times\"  will usually depend on context: the set of values for  might be discrete or continuous, bounded or unbounded. For example,\n\nSimilarly, a filtered probability space (also known as a stochastic basis) , is a probability space equipped with the filtration  of its -algebra . A filtered probability space is said to satisfy the usual conditions if it is complete (i.e.,  contains all -null sets) and right-continuous (i.e.  for all times ).\n\nIt is also useful (in the case of an unbounded index set) to define  as the -algebra generated by the infinite union of the 's, which is contained in :\n\nA σ-algebra defines the set of events that can be measured, which in a probability context is equivalent to events that can be discriminated, or \"questions that can be answered at time \". Therefore, a filtration is often used to represent the change in the set of events that can be measured, through gain or loss of information. A typical example is in mathematical finance, where a filtration represents the information available up to and including each time , and is more and more precise (the set of measurable events is staying the same or increasing) as more information from the evolution of the stock price becomes available.\n\nRelation to stopping times: stopping time sigma-algebras\n\nLet  be a filtered probability space. A random variable  is a stopping time with respect to the filtration , if  for all . \nThe stopping time -algebra is now defined as\n.\n\nIt is not difficult to show that  is indeed a -algebra.\nThe set  encodes information up to the random time  in the sense that, if the filtered probability space is interpreted as a random experiment, the maximum information that can be found out about it from arbitrarily often repeating the experiment until the random time  is . In particular, if the underlying probability space is finite (i.e.  is finite), the minimal sets of  (with respect to set inclusion) are given by the union over all  of the sets of minimal sets of  that lie in .\n\nIt can be shown that  is -measurable. However, simple examples show that, in general, . If  and  are stopping times on , and  almost surely, then \n\nSee also\nNatural filtration\n\nReferences\n\n \n\nCategory:Algebra\nCategory:Measure theory\nCategory:Stochastic processes"
    },
    {
      "title": "Free presentation",
      "url": "https://en.wikipedia.org/wiki/Free_presentation",
      "text": "In algebra, a free presentation of a module M over a commutative ring R is an exact sequence of R-modules:\n\nNote the image under g of the standard basis generates M. In particular, if J is finite, then M is a finitely generated module. If I and J are finite sets, then the presentation is called a finite presentation; a module is called finitely presented if it admits a finite presentation.\n\nSince f is a module homomorphism between free modules, it can be visualized as an (infinite) matrix with entries in R and M as its cokernel.\n\nA free presentation always exists: any module is a quotient of a free module: , but then the kernel of g is again a quotient of a free module: . The combination of f and g is a free presentation of M. Now, one can obviously keep \"resolving\" the kernels in this fashion; the result is called a free resolution. Thus, a free presentation is the early part of the free resolution.\n\nA presentation is useful for computation. For example, since tensoring is right-exact, tensoring the above presentation with a module, say, N gives:\n\n \n\nThis says that  is the cokernel of . If N is an R-algebra, then this is the presentation of the N-module ; that is, the presentation extends under base extension.\n\nFor left-exact functors, there is for example\n\nProof: Applying F to a finite presentation  results in\n\nand the same for G. Now apply the snake lemma. \n\n See also \nCoherent module\nFinitely-related module\nFitting ideal\nQuasi-coherent sheaf\n\n References \n Eisenbud, David, Commutative Algebra with a View Toward Algebraic Geometry, Graduate Texts in Mathematics, 150, Springer-Verlag, 1995, .\n\nCategory:Algebra"
    },
    {
      "title": "Free product of associative algebras",
      "url": "https://en.wikipedia.org/wiki/Free_product_of_associative_algebras",
      "text": "In algebra, the free product (coproduct) of a family of associative algebras  over a commutative ring R is the associative algebra over R that is, roughly, defined by the generators and the relations of the 's. The free product of two algebras A, B is denoted by A * B. The notion is a ring-theoretic analog of a free product of groups.\n\nIn the category of commutative R-algebras, the free product of two algebras (in that category) is their tensor product.\n\n Construction \n\nWe first define a free product of two algebras. Let A, B be two algebras over a commutative ring R. Consider their tensor algebra, the direct sum of all possible finite tensor products of A, B; explicitly,  where\n\nWe then set\n\nwhere I is the two-sided ideal generated by elements of the form\n\nWe then verify the universal property of coproduct holds for this (this is straightforward but we should give details.)\n\n References \nBeidar, Martindale and Mikhalev, Rings with generalized identities, Section 1.4. This reference was mentioned in https://math.stackexchange.com/questions/143098/coproduct-in-the-category-of-noncommutative-associative-algebras\n\n External links \nhttps://math.stackexchange.com/questions/625874/how-to-construct-the-coproduct-of-two-non-commutative-rings\n\nCategory:Algebra"
    },
    {
      "title": "Freshman's dream",
      "url": "https://en.wikipedia.org/wiki/Freshman%27s_dream",
      "text": "right|thumbnail|An illustration of the Freshman's dream in two dimensions. Each side of the square is X+Y in length. The area of the square is the sum of the area of the yellow region (=X2), the area of the green region (=Y2), and the area of the two white regions (=2×X×Y).\n\nThe freshman's dream is a name sometimes given to the erroneous equation (x + y)n = xn + yn, where n is a real number (usually a positive integer greater than 1). Beginning students commonly make this error in computing the power of a sum of real numbers, falsely assuming powers distribute over sums.Julio R. Bastida, Field Extensions and Galois Theory, Addison-Wesley Publishing Company, 1984, p.8.Fraleigh, John B., A First Course in Abstract Algebra, Addison-Wesley Publishing Company, 1993, p.453, . When n = 2, it is easy to see why this is incorrect: (x + y)2 can be correctly computed  as x2 + 2xy + y2 using distributivity (commonly known as the FOIL method). For larger positive integer values of n, the correct result is given by the binomial theorem.\n\nThe name \"freshman's dream\" also sometimes refers to the theorem that says that for a prime number p, if x and y are members of a commutative ring of characteristic p, then \n(x + y)p = xp + yp. In this more exotic type of arithmetic, the \"mistake\" actually gives the correct result, since p divides all the binomial coefficients save the first and the last, making all intermediate terms equal to zero.\n\nThe identity is actually true in the context of Tropical geometry, where multiplication is replaced with addition, and addition is replaced with minimum.\n\nExamples\n, but . \n does not generally equal . For example, , which does not equal . In this example, the error is being committed with the exponent .\n\nPrime characteristic\n\nWhen p is a prime number and x and y are members of a commutative ring of characteristic p, then . This can be seen by examining the prime factors of the binomial coefficients: the nth binomial coefficient is\n\nThe numerator is p factorial, which is divisible by p. However, when , neither n! nor  is divisible by p since all the terms are less than p and p is prime. Since a binomial coefficient is always an integer, the nth binomial coefficient is divisible by p and hence equal to 0 in the ring. We are left with the zeroth and pth coefficients, which both equal 1, yielding the desired equation.\n\nThus in characteristic p the freshman's dream is a valid identity. This result demonstrates that exponentiation by p produces an endomorphism, known as the Frobenius endomorphism of the ring.\n\nThe demand that the characteristic p be a prime number is central to the truth of the freshman's dream. A related theorem states that if a number n is prime then  in the polynomial ring . This theorem is a direct consequence of Fermat's little theorem and it is a key fact in modern primality testing.A. Granville, It Is Easy To Determine Whether A Given Integer Is Prime, Bull. of the AMS, Volume 42, Number 1 (Sep. 2004), Pages 3–38.\n\nHistory and alternate names\nThe history of the term \"freshman's dream\" is somewhat unclear. In a 1940 article on modular fields, Saunders Mac Lane quotes Stephen Kleene's remark that a knowledge of  in a field of characteristic 2 would corrupt freshman students of algebra. This may be the first connection between \"freshman\" and binomial expansion in fields of positive characteristic.Colin R. Fletcher, Review of Selected papers on algebra, edited by Susan Montgomery, Elizabeth W. Ralston and others. Pp xv, 537. 1977.  (Mathematical Association of America), The Mathematical Gazette, Vol. 62, No. 421 (Oct., 1978), The Mathematical Association. p. 221. Since then, authors of undergraduate algebra texts took note of the common error. The first actual attestation of the phrase \"freshman's dream\" seems to be in Hungerford's graduate algebra textbook (1974), where he quotes McBrien.Thomas W. Hungerford, Algebra, Springer, 1974, p. 121; also in Abstract Algebra: An Introduction, 2nd edition. Brooks Cole, July 12, 1996, p. 366. Alternative terms include \"freshman exponentiation\", used in Fraleigh (1998).John B. Fraleigh, A First Course In Abstract Algebra, 6th edition, Addison-Wesley, 1998. pp. 262 and 438. The term \"freshman's dream\" itself, in non-mathematical contexts, is recorded since the 19th century.Google books 1800–1900 search for \"freshman's dream\": Bentley's miscellany, Volume 26, p. 176, 1849\n\nSince the expansion of  is correctly given by the binomial theorem, the freshman's dream is also known as the \"child's binomial theorem\" or \"schoolboy binomial theorem\".\n\nSee also\nPrimality test\nSophomore's dream\nFrobenius endomorphism\n\nReferences\n\nCategory:Algebra\nCategory:Mathematics education"
    },
    {
      "title": "Gelfand–Kirillov dimension",
      "url": "https://en.wikipedia.org/wiki/Gelfand%E2%80%93Kirillov_dimension",
      "text": "In algebra, the Gelfand–Kirillov dimension (or GK dimension) of a right module M over a k-algebra A is:\n\nwhere the sup is taken over all finite-dimensional subspaces  and .\n\nAn algebra is said to have polynomial growth if its Gelfand–Kirillov dimension is finite.\n\n Basic facts \nThe Gelfand–Kirillov dimension of a finitely generated commutative algebra A over a field is the Krull dimension of A (or equivalently the transcendence degree of the field of fractions of A over the base field.)\nIn particular, the GK dimension of the polynomial ring  Is n.\n(Warfield) For any real number r ≥ 2, there exists a finitely generated algebra whose GK dimension is r.\n\n In the theory of D-Modules \n\nGiven a right module M over the Weyl algebra , the Gelfand–Kirillov dimension of M over the Weyl algebra coincides with the dimension of M, which is by definition the degree of the Hilbert polynomial of M. This enables to prove additivity in short exact sequences for the Gelfand–Kirillov dimension and finally to prove Bernstein's inequality, which states that the dimension of M must be at least n. This leads to the definition of holonomic D-Modules as those with the minimal dimension n, and these modules play a great role in the geometric Langlands program.\n\n References \n\n Coutinho: A primer of algebraic D-modules. Cambridge, 1995\n\nFurther reading\n\nCategory:Algebra\nCategory:Dimension"
    },
    {
      "title": "Generalized arithmetic progression",
      "url": "https://en.wikipedia.org/wiki/Generalized_arithmetic_progression",
      "text": "In mathematics, a multiple arithmetic progression, generalized arithmetic progression, k-dimensional arithmetic progression or a linear set, is a set of integers or tuples of integers constructed as an arithmetic progression is, but allowing several possible differences. So, for example, we start at 17 and may add a multiple of 3 or of 5, repeatedly.  In algebraic terms we look at integers\n\n \n\nwhere  and so on are fixed, and  and so on are confined to some ranges\n\nand so on, for a finite progression. The number  , that is the number of permissible differences, is called the dimension of the generalized progression.\n\nMore generally, let\n\nbe the set of all elements  in  of the form\n\nwith  in ,  in , and  in .  is said to be a linear set if  consists of exactly one element, and  is finite.\n\nA subset of  is said to be semilinear if it is a finite union of linear sets. The semilinear sets are exactly the sets definable in Presburger arithmetic.\n\nSee also\n Freiman's theorem\n\nReferences\n\nCategory:Algebra\nCategory:Combinatorics"
    },
    {
      "title": "Generalized Cohen–Macaulay ring",
      "url": "https://en.wikipedia.org/wiki/Generalized_Cohen%E2%80%93Macaulay_ring",
      "text": "In algebra, a generalized Cohen–Macaulay ring is a commutative Noetherian local ring  of Krull dimension d > 0 that satisfies any of the following equivalent conditions:\nFor each integer , the length of the i-th local cohomology of A is finite:\n.\n where the sup is over all parameter ideals  and  is the multiplicity of .\nThere is an -primary ideal  such that for each system of parameters  in , \nFor each prime ideal  of  that is not ,  and  is Cohen–Macaulay.\n\nThe last condition implies that the localization  is Cohen–Macaulay for each prime ideal .\n\nA standard example is the local ring at the vertex of an affine cone over a smooth projective variety. Historically, the notion grew up out of the study of a Buchsbaum ring, a Noetherian local ring A in which  is constant for -primary ideals ; see the introduction of .\n\n References \n\nHerrmann, M., S. Ikeda, and U. Orbanz: Equimultiplicity and Blowing Up. An Algebraic Study with an Appendix by B. Moonen. Springer Verlag, Berlin Heidelberg New-York, 1988.\nN. V. Trung, Towards a theory of generalized Cohen-Macaulay modules, Nagoya Math. J. 102, 1 – 49(1986)\n\nCategory:Algebra"
    },
    {
      "title": "Goursat's lemma",
      "url": "https://en.wikipedia.org/wiki/Goursat%27s_lemma",
      "text": "Goursat's lemma, named after the French mathematician Édouard Goursat, is an algebraic theorem about subgroups of the direct product of two groups.\n\nIt can be stated more generally in a Goursat variety (and consequently it also holds in any Maltsev variety), from which one recovers a more general version of Zassenhaus' butterfly lemma. In this form, Goursat's theorem also implies the snake lemma.\n\n Groups \nGoursat's lemma for groups can be stated as follows.\nLet ,  be groups, and let  be a subgroup of  such that the two projections  and  are surjective (i.e.,  is a subdirect product of  and ). Let  be the kernel of  and  the kernel of . One can identify  as a normal subgroup of , and  as a normal subgroup of . Then the image of  in  is the graph of an isomorphism .\n\nAn immediate consequence of this is that the subdirect product of two groups can be described as a fiber product and vice versa.\n\nNotice that if  is any subgroup of  (the projections  and  need not be surjective), then the projections from  onto   are surjective. Then one can apply Goursat's lemma to . \n\nTo motivate the proof, consider the slice  in , for any arbitrary . By the surjectivity of the projection map to , this has a non trivial intersection with .  Then essentially, this intersection represents exactly one particular coset of . Indeed, if we had distinct elements  with  and   , then  being a group, we get that , and hence, . But this a contradiction, as  belong to distinct cosets of , and thus , and thus the element  cannot belong to the kernel  of the projection map from  to . Thus the intersection of  with every \"horizontal\" slice isomorphic to  is exactly one particular coset of  in .\nBy an identical argument, the intersection of  with every \"vertical\" slice isomorphic to  is exactly one particular coset of  in .\n\nAll the cosets of  are present in the group , and by the above argument, there is an exact 1:1 correspondence between them. The proof below further shows that the map is an isomorphism.\n\n Proof \n\nBefore proceeding with the proof,  and  are shown to be normal in  and , respectively.  It is in this sense that  and  can be identified as normal in G and G''', respectively.\n\nSince  is a homomorphism, its kernel N is normal in H. Moreover, given , there exists , since  is surjective.  Therefore,  is normal in G, viz:\n.\nIt follows that  is normal in  since\n .\n\nThe proof that  is normal in  proceeds in a similar manner.\n\nGiven the identification of  with , we can write  and  instead of  and , .  Similarly, we can write  and , .\n\nOn to the proof. Consider the map  defined by . The image of  under this map is . Since  is surjective, this relation is the graph of a well-defined function  provided  for every , essentially an application of the vertical line test.\n\nSince  (more properly, ), we have . Thus , whence , that is, .\n\nFurthermore, for every  we have . It follows that this function is a group homomorphism.\n\nBy symmetry,  is the graph of a well-defined homomorphism . These two homomorphisms are clearly inverse to each other and thus are indeed isomorphisms.\n\n Goursat varieties \n\nAs a consequence of Goursat's theorem, one can derive a very general version on the Jordan–Hölder–Schreier theorem in Goursat varieties.\n\n References \n Édouard Goursat, \"Sur les substitutions orthogonales et les divisions régulières de l'espace\", Annales Scientifiques de l'École Normale Supérieure (1889), Volume: 6, pages 9–102\n \n Kenneth A. Ribet (Autumn 1976), \"Galois Action on Division Points of Abelian Varieties with Real Multiplications\", American Journal of Mathematics'', Vol. 98, No. 3, 751–804.\n\nCategory:Algebra\nCategory:Lemmas\nCategory:Articles containing proofs"
    },
    {
      "title": "Graph algebra (social sciences)",
      "url": "https://en.wikipedia.org/wiki/Graph_algebra_%28social_sciences%29",
      "text": "Graph algebra is systems-centric modeling tool for the social sciences..  It was first developed by Sprague, Pzeworski, and CortesCortés, Fernando, Adam Przeworski, and John Sprague. 1974. Systems Analysis for Social Scientists. New York: John Wiley & Sons. as a hybridized version of engineering plots to describe social phenomena.\n\n Notes and references \n\nCategory:Algebra\nCategory:Social science methodology"
    },
    {
      "title": "Graph dynamical system",
      "url": "https://en.wikipedia.org/wiki/Graph_dynamical_system",
      "text": "In mathematics, the concept of graph dynamical systems can be used to capture a wide range of processes taking place on graphs or networks. A major theme in the mathematical and computational analysis of GDSs is to relate their structural properties (e.g. the network connectivity) and the global dynamics that result.\n\nThe work on GDSs considers finite graphs and finite state spaces. As such, the research typically involves techniques from, e.g., graph theory, combinatorics, algebra, and dynamical systems rather than differential geometry. In principle, one could define and study GDSs over an infinite graph (e.g. cellular automata or probabilistic cellular automata over  or interacting particle systems when some randomness is included), as well as GDSs with infinite state space (e.g.  as in coupled map lattices); see, for example, Wu. In the following, everything is implicitly assumed to be finite unless stated otherwise.\n\nFormal definition\n\nA graph dynamical system is constructed from the following components:\n\n A finite graph Y with vertex set v[Y] = {1,2, ... , n}. Depending on the context the graph can be directed or undirected.\n A state xv for each vertex v of Y taken from a finite set K. The system state is the n-tuple x = (x1, x2, ... , xn), and x[v] is the tuple consisting of the states associated to the vertices in the 1-neighborhood of v in Y (in some fixed order).\n A vertex function fv for each vertex v. The vertex function maps the state of vertex v at time t to the vertex state at time t + 1 based on the states associated to the 1-neighborhood of v in Y.\n An update scheme specifying the mechanism by which the mapping of individual vertex states is carried out so as to induce a discrete dynamical system with map F: Kn → Kn.\n\nThe phase space associated to a dynamical system with map F: Kn → Kn is the finite directed graph with vertex set Kn and directed edges (x, F(x)). The structure of the phase space is governed by the properties of the graph Y, the vertex functions (fi)i, and the update scheme. The research in this area seeks to infer phase space properties based on the structure of the system constituents. The analysis has a local-to-global character.\n\n Generalized cellular automata (GCA) \n\nIf, for example, the update scheme consists of applying the vertex functions synchronously one obtains the class of generalized cellular automata (CA). In this case, the global map F: Kn → Kn is given by\n\nThis class is referred to as generalized cellular automata since the classical or standard cellular automata are typically defined and studied over regular graphs or grids, and the vertex functions are typically assumed to be identical.\n\nExample: Let Y be the circle graph on vertices {1,2,3,4} with edges {1,2}, {2,3}, {3,4} and {1,4}, denoted Circ4. Let K = {0,1} be the state space for each vertex and use the function nor3 : K3 → K defined by nor3(x,y,z) = (1 + x)(1 + y)(1 + z) with arithmetic modulo 2 for all vertex functions. Then for example the system state (0,1,0,0) is mapped to (0, 0, 0, 1) using a synchronous update. All the transitions are shown in the phase space below.\n\nframe|center | 326\n\n Sequential dynamical systems (SDS) \n\nIf the vertex functions are applied asynchronously in the sequence specified by a word w = (w1, w2, ... , wm) or permutation  = ( , ) of v[Y] one obtains the class of Sequential dynamical systems (SDS). In this case it is convenient to introduce the Y-local maps Fi constructed from the vertex functions by\n\n \n\nThe SDS map F = [FY , w] : Kn → Kn is the function composition\n\n \n\nIf the update sequence is a permutation one frequently speaks of a permutation SDS to emphasize this point.\n\nExample: Let Y be the circle graph on vertices {1,2,3,4} with edges {1,2}, {2,3}, {3,4} and {1,4}, denoted Circ4. Let K={0,1} be the state space for each vertex and use the function nor3 : K3 → K defined by nor3(x, y, z) = (1 + x)(1 + y)(1 + z) with arithmetic modulo 2 for all vertex functions. Using the update sequence (1,2,3,4) then the system state (0, 1, 0, 0) is mapped to (0, 0, 1, 0). All the system state transitions for this sequential dynamical system are shown in the phase space below.\n\nframe|center | 326\n\n Stochastic graph dynamical systems \n\nFrom, e.g., the point of view of applications it is interesting to consider the case where one or more of the components of a GDS contains stochastic elements. Motivating applications could include processes that are not fully understood (e.g. dynamics within a cell) and where certain aspects for all practical purposes seem to behave according to some probability distribution. There are also applications governed by deterministic principles whose description is so complex or unwieldy that it makes sense to consider probabilistic approximations.\n\nEvery element of a graph dynamical system can be made stochastic in several ways. For example, in a sequential dynamical system the update sequence can be made stochastic. At each iteration step one may choose the update sequence w at random from a given distribution of update sequences with corresponding probabilities. The matching probability space of update sequences induces a probability space of SDS maps. A natural object to study in this regard is the Markov chain on state space induced by this collection of SDS maps. This case is referred to as update sequence stochastic GDS and is motivated by, e.g., processes where \"events\" occur at random according to certain rates (e.g. chemical reactions), synchronization in parallel computation/discrete event simulations, and in computational paradigms described later.\n\nThis specific example with stochastic update sequence illustrates two general facts for such systems: when passing to a stochastic graph dynamical system one is generally led to (1) a study of Markov chains (with specific structure governed by the constituents of the GDS), and (2) the resulting Markov chains tend to be large having an exponential number of states. A central goal in the study of stochastic GDS is to be able to derive reduced models.\n\nOne may also consider the case where the vertex functions are stochastic, i.e., function stochastic GDS. For example, Random Boolean networks are examples of function stochastic GDS using a synchronous update scheme and where the state space is K = {0, 1}. Finite probabilistic cellular automata (PCA) is another example of function stochastic GDS. In principle the class of Interacting particle systems (IPS) covers finite and infinite PCA, but in practice the work on IPS is largely concerned with the infinite case since this allows one to introduce more interesting topologies on state space.\n\nApplications\n\nGraph dynamical systems constitute a natural framework for capturing distributed systems such as biological networks and epidemics over social networks, many of which are frequently referred to as complex systems.\n\nSee also\n\nChemical reaction network theory\nDynamic network analysis (a social science topic)\nFinite state machines\nHopfield networks\nKauffman networks\nPetri nets\n\nReferences\n\nFurther reading\n \n\nExternal links\nGraph Dynamical Systems – A Mathematical Framework for Interaction-Based Systems, Their Analysis and Simulations by Henning Mortveit\n\nCategory:Dynamical systems\nCategory:Algebra\nCategory:Graph theory\nCategory:Combinatorics"
    },
    {
      "title": "Groupoid algebra",
      "url": "https://en.wikipedia.org/wiki/Groupoid_algebra",
      "text": "In mathematics, the concept of groupoid algebra generalizes the notion of group algebra.Khalkhali (2009), [ p. 48]\n\n Definition \nGiven a groupoid  and a field  (in the sense of a category with all arrows invertible), it is possible to define the groupoid algebra  as the algebra over  formed by the vector space having the elements of (the arrows of)  as generators and having the multiplication of these elements defined by , whenever this product is defined, and  otherwise. The product is then extended by linearity.Dokuchaev, Exel & Piccione (2000), p. 7\n\n Examples \nSome examples of groupoid algebras are the following:da Silva & Weinstein (1999), [ p. 97]\n Group algebras\n Matrix algebras\n Algebras of functions\n\n Properties \n When a groupoid has a finite number of objects and a finite number of morphisms, the groupoid algebra is a direct sum of tensor products of group algebras and matrix algebras.Khalkhali & Marcolli (2008), [ p. 210]\n\n See also \n Hopf algebra\n Partial group algebra\n\n Notes \n\n References \n \n \n \n \n\nCategory:Algebra"
    },
    {
      "title": "Hecke algebra acting on modular forms",
      "url": "https://en.wikipedia.org/wiki/Hecke_algebra_acting_on_modular_forms",
      "text": "In number theory in mathematics, the Hecke algebra is the algebra generated by Hecke operators. The algebra is commutative.\n\n References \n\nJean-Pierre Serre, A course in arithmetic.\n\nCategory:Algebra\nCategory:Number theory\nCategory:Modular forms"
    },
    {
      "title": "Height function",
      "url": "https://en.wikipedia.org/wiki/Height_function",
      "text": "A height function is a function that quantifies the complexity of mathematical objects. In Diophantine geometry, height functions quantify the size of solutions to Diophantine equations and are typically functions from a set of points on algebraic varieties (or a set of algebraic varieties) to the real numbers.\n\nFor instance, the classical or naive height over the rational numbers is typically defined to be the maximum of the numerators and denominators of the coordinates (e.g. 2 for the coordinates ), but in a logarithmic scale.\n\nSignificance\nHeight functions allow mathematicians to count objects, such as rational points, that are otherwise infinite in quantity. For instance, the set of rational numbers of naive height (the maximum of the numerator and denominator when expressed in lowest terms) below any given constant is finite despite the set of rational numbers being infinite. In this sense, height functions can be used to prove asymptotic results such as Baker's theorem in transcendental number theory which was proved by .\n\nIn other cases, height functions can distinguish some objects based on their complexity. For instance, the subspace theorem proved by  demonstrates that points of small height (i.e. small complexity) in projective space lie in a finite number of hyperplanes and generalizes Siegel's theorem on integral points and solution of the S-unit equation.\n\nHeight functions were crucial to the proofs of the Mordell–Weil theorem and Faltings's theorem by  and  respectively. Several outstanding unsolved problems about the heights of rational points on algebraic varieties, such as the Manin conjecture and Vojta's conjecture, have far-reaching implications for problems in Diophantine approximation, Diophantine equations, arithmetic geometry, and mathematical logic.\n\nHeight functions in Diophantine geometry\n\nHistory\nHeights in Diophantine geometry were initially developed by André Weil and Douglas Northcott beginning in the 1920s. Innovations in 1960s were the Néron–Tate height and the realization that heights were linked to projective representations in much the same way that ample line bundles are in other parts of algebraic geometry. In the 1970s, Suren Arakelov developed Arakelov heights in Arakelov theory. In 1983, Faltings developed his theory of Faltings heights in his proof of Faltings's theorem.\n\nNaive height\nClassical or naive height is defined in terms of ordinary absolute value on homogeneous coordinates. It is typically a logarithmic scale and therefore can be viewed as being proportional to the \"algebraic complexity\" or number of bits needed to store a point. It is typically defined to be the logarithm of the maximum absolute value of the vector of coprime integers obtained by multiplying through by a lowest common denominator. This may be used to define height on a point in projective space over Q, or of a polynomial, regarded as a vector of coefficients, or of an algebraic number, from the height of its minimal polynomial.\n\nThe naive height of a rational number x = p/q (in lowest terms) is \n multiplicative height  planetmath: height function\n logarithmic height:  mathoverflow question: average-height-of-rational-points-on-a-curve\n\nTherefore, the naive multiplicative and logarithmic heights of  are  and , for example.\n\nThe naive height H of an elliptic curve E given by  is defined to be .\n\nNéron–Tate height\n\nThe Néron–Tate height, or canonical height, is a quadratic form on the Mordell–Weil group of rational points of an abelian variety defined over a global field. It is named after André Néron, who first defined it as a sum of local heights, and John Tate, who defined it globally in an unpublished work.\n\nWeil height\nThe Weil height is defined on a projective variety X over a number field K equipped with a line bundle L on X. Given a very ample line bundle L0 on X, one may define a height function using the naive height function h. Since L0 is very ample, its complete linear system gives a map ϕ from X to projective space. Then for all points p on X, define\n\nOne may write an arbitrary line bundle L on X as the difference of two very ample line bundles L1 and L2 on X, up to Serre's twisting sheaf O(1), so one may define the Weil height hL on X with respect to L via\n\n(up to O(1)).\n\nArakelov height\nThe Arakelov height on a projective space over the field of algebraic numbers is a global height function with local contributions coming from Fubini–Study metrics on the Archimedean fields and the usual metric on the non-Archimedean fields. It is the usual Weil height equipped with a different metric.\n\nFaltings height\nThe Faltings height of an abelian variety defined over a number field is a measure of its arithmetic complexity. It is defined in terms of the height of a metrized line bundle. It was introduced by  in his proof of the Mordell conjecture.\n\nHeight functions in algebra\nHeight of a polynomial\nFor a polynomial P of degree n given by\n\nthe height H(P) is defined to be the maximum of the magnitudes of its coefficients:\n\nOne could similarly define the length''' L(P) as the sum of the magnitudes of the coefficients:\n\nRelation to Mahler measure\nThe Mahler measure M(P)  of P is also a measure of the complexity of P. The three functions H(P), L(P) and M(P) are related by the  inequalities\n\nwhere  is the binomial coefficient.\n\nHeight functions in automorphic forms\nOne of the conditions in the definition of an automorphic form on the general linear group of an adelic algebraic group is moderate growth'', which is an asymptotic condition on the growth of a height function on the general linear group viewed as an affine variety.\n\nSee also\nabc conjecture\nBirch and Swinnerton-Dyer conjecture\nElliptic Lehmer conjecture\nHeath-Brown–Moroz constant\nHeight zeta function\nRaynaud's isogeny theorem\n\nReferences\n\nSources\n\n → Contains an English translation of \n\nExternal links\n\n  Polynomial height at Mathworld\n\nCategory:Number theory\nCategory:Polynomials\nCategory:Abelian varieties\nCategory:Elliptic curves\nCategory:Diophantine geometry\nCategory:Algebraic geometry\nCategory:Algebraic number theory\nCategory:Algebra"
    },
    {
      "title": "Horner's method",
      "url": "https://en.wikipedia.org/wiki/Horner%27s_method",
      "text": "In mathematics, the term Horner's rule (or Horner's method, Horner's scheme etc) refers to a polynomial evaluation method named after William George Horner expressed by\n\nThis allows evaluation of a polynomial of degree  with only  multiplications and  additions. This is optimal, since there are polynomials of degree  that cannot be evaluated with fewer arithmetic operations.\n\nThis algorithm is much older than Horner. He himself ascribed it to Joseph-Louis Lagrange but it can be traced back many hundreds of years to Chinese and Persian mathematicians.600 years earlier, by the Chinese mathematician Qin Jiushao and 700 years earlier, by the Persian mathematician Sharaf al-Dīn al-Ṭūsī\n\nHorner's root-finding method: Until computers came into general use in about 1970 the term 'Horner's method' was used to refer to a root-finding method for polynomials named after Horner who described a similar method in 1819. This method was widely used and became a standard method for hand calculation. It gave a convenient way for using the Newton–Raphson method for polynomials. It relied on the algorithm for polynomial evaluation now named after Horner. After the introduction of computers this root-finding method went out of use and as a result the term Horner's method (rule etc) has become understood to mean just the polynomial evaluation algorithm.\n\n Description of the algorithm \n\nGiven the polynomial\n\nwhere  are constant coefficients, we wish to evaluate the polynomial at a specific value of  that we'll call .\n\nTo accomplish this, we define a new sequence of constants as follows:\n\nThen  is the value of .\n\nTo see why this works, note that the polynomial can be written in the form\n\nThus, by iteratively substituting the  into the expression,\n \n\n Examples \n\nEvaluate  for \n\nWe use synthetic division as follows:\n\n  x0│   x3    x2    x1    x0\n  3 │   2    −6     2    −1\n    │         6     0     6\n    └────────────────────────\n        2     0     2     5\n\nThe entries in the third row are the sum of those in the first two. Each entry in the second row is the product of the x-value (3 in this example) with the third-row entry immediately to the left. The entries in the first row are the coefficients of the polynomial to be evaluated. Then the remainder of  on division by  is 5.\n\nBut by the polynomial remainder theorem, we know that the remainder is . Thus \n\nIn this example, if  we can see that , the entries in the third row. So, synthetic division is based on Horner's method.\n\nAs a consequence of the polynomial remainder theorem, the entries in the third row are the coefficients of the second-degree polynomial, the quotient of  on division by . \nThe remainder is 5. This makes Horner's method useful for polynomial long division.\n\nDivide  by :\n\n  2 │   1    −6    11    −6\n    │         2    −8     6\n    └────────────────────────\n        1    −4     3     0\n\nThe quotient is .\n\nLet  and . Divide  by  using Horner's method.\n\n \n \n   0.5 │ 4  -6   0   3  -5\n       │     2  -2  -1   1\n └───────────────────────\n         2  -2  -1   1  -2\n\nThe third row is the sum of the first two rows, divided by 2. Each entry in the second row is the product of 1 with the third-row entry to the left. The answer is\n\n \n\nFloating-point multiplication and division\n\nHorner's method is a fast, code-efficient method for multiplication and division of binary numbers on a microcontroller with no hardware multiplier.  One of the binary numbers to be multiplied is represented as a trivial polynomial, where (using the above notation) , and .  Then, x (or x to some power) is repeatedly factored out.  In this binary numeral system (base 2), , so powers of 2 are repeatedly factored out.\n\nExample\nFor example, to find the product of two numbers (0.15625) and m:\n\nMethod\nTo find the product of two binary numbers d and m:\n1. A register holding the intermediate result is initialized to d.\n2. Begin with the least significant (rightmost) non-zero bit in m.\n2b. Count (to the left) the number of bit positions to the next most significant non-zero bit.  If there are no more-significant bits, then take the value of the current bit position.\n2c. Using that value, perform a left-shift operation by that number of bits on the register holding the intermediate result\n3. If all the non-zero bits were counted, then the intermediate result register now holds the final result.  Otherwise, add d to the intermediate result, and continue in step 2 with the next most significant bit in m.\n\nDerivation\nIn general, for a binary number with bit values () the product is\n\nAt this stage in the algorithm, it is required that terms with zero-valued coefficients are dropped, so that only binary coefficients equal to one are counted, thus the problem of multiplication or division by zero is not an issue, despite this implication in the factored equation:\n\nThe denominators all equal one (or the term is absent), so this reduces to\n\nor equivalently (as consistent with the \"method\" described above)\n\nIn binary (base-2) math, multiplication by a power of 2 is merely a register shift operation.  Thus, multiplying by 2 is calculated in base-2 by an arithmetic shift.  The factor (2−1) is a right arithmetic shift, a (0) results in no operation (since 20 = 1 is the multiplicative identity element), and a (21) results in a left arithmetic shift.\nThe multiplication product can now be quickly calculated using only arithmetic shift operations, addition and subtraction.\n\nThe method is particularly fast on processors supporting a single-instruction shift-and-addition-accumulate.  Compared to a C floating-point library, Horner's method sacrifices some accuracy, however it is nominally 13 times faster (16 times faster when the \"canonical signed digit\" (CSD) form is used) and uses only 20% of the code space..\n\n Polynomial root finding \nUsing Horner's method in combination with Newton's method, it is possible to approximate the real roots of a polynomial. The algorithm works as follows. Given a polynomial  of degree  with zeros  make some initial guess  such that . Now iterate the following two steps:\n\n1. Using Newton's method, find the largest zero  of  using the guess .\n\n2. Using Horner's method, divide out  to obtain . Return to step 1 but use the polynomial  and the initial guess .\n\nThese two steps are repeated until all real zeros are found for the polynomial. If the approximated zeros are not precise enough, the obtained values can be used as initial guesses for Newton's method but using the full polynomial rather than the reduced polynomials..\n\n Example \n\nthumb|right|400px|Polynomial root finding using Horner's method\n\nConsider the polynomial\n\n \n\nwhich can be expanded to\n\n \n\nFrom the above we know that the largest root of this polynomial is 7 so we are able to make an initial guess of 8. Using Newton's method the first zero of 7 is found as shown in black in the figure to the right. Next  is divided by  to obtain\n\n \n\nwhich is drawn in red in the figure to the right. Newton's method is used to find the largest zero of this polynomial with an initial guess of 7. The largest zero of this polynomial which corresponds to the second largest zero of the original polynomial is found at 3 and is circled in red. The degree 5 polynomial is now divided by  to obtain\n\n \n\nwhich is shown in yellow. The zero for this polynomial is found at 2 again using Newton's method and is circled in yellow. Horner's method is now used to obtain\n\n \n\nwhich is shown in green and found to have a zero at −3. This polynomial is further reduced to\n\n \n\nwhich is shown in blue and yields a zero of −5. The final root of the original polynomial may be found by either using the final zero as an initial guess for Newton's method, or by reducing  and solving the linear equation. As can be seen, the expected roots of −8, −5, −3, 2, 3, and 7 were found.\n\n Application \n\nHorner's method can be used to convert between different positional numeral systems – in which case x is the base of the number system, and the ai coefficients are the digits of the base-x representation of a given number – and can also be used if x is a matrix, in which case the gain in computational efficiency is even greater. In fact, when x is a matrix, further acceleration is possible which exploits the structure of matrix multiplication, and only  instead of n multiplies are needed (at the expense of requiring more storage) using the 1973 method of Paterson and Stockmeyer..\n\n Efficiency \n\nEvaluation using the monomial form of a degree-n polynomial requires at most n additions and (n2 + n)/2 multiplications, if powers are calculated by repeated multiplication and each monomial is evaluated individually.  (This can be reduced to n additions and 2n − 1 multiplications by evaluating the powers of x iteratively.)  If numerical data are represented in terms of digits (or bits), then the naive algorithm also entails storing approximately 2n times the number of bits of x (the evaluated polynomial has approximate magnitude xn, and one must also store xn itself).  By contrast, Horner's method requires only n additions and n multiplications, and its storage requirements are only n times the number of bits of x. Alternatively, Horner's method can be computed with n fused multiply–adds.  Horner's method can also be extended to evaluate the first k derivatives of the polynomial with kn additions and multiplications..\n\nHorner's method is optimal, in the sense that any algorithm to evaluate an arbitrary polynomial must use at least as many operations. Alexander Ostrowski proved in 1954 that the number of additions required is minimal.. Victor Pan proved in 1966 that the number of multiplications is minimal.. However, when x is a matrix, Horner's method is not optimal.\n\nThis assumes that the polynomial is evaluated in monomial form and no preconditioning of the representation is allowed, which makes sense if the polynomial is evaluated only once. However, if preconditioning is allowed and the polynomial is to be evaluated many times, then faster algorithms are possible. They involve a transformation of the representation of the polynomial. In general, a degree-n polynomial can be evaluated using only +2 multiplications and n additions..\n\nParallel evaluation\n\nA disadvantage of Horner's rule is that all of the operations are sequentially dependent, so it is not possible to take advantage of instruction level parallelism on modern computers.  In most applications where the efficiency of polynomial evaluation matters, many low-order polynomials are evaluated simultaneously (for each pixel or polygon in computer graphics, or for each grid square in a numerical simulation), so it is not necessary to find parallelism within a single polynomial evaluation.\n\nIf, however, one is evaluating a single polynomial of very high order, it may be useful to break it up as follows:\n\nMore generally, the summation can be broken into k parts:\n\nwhere the inner summations may be evaluated using separate parallel instances of Horner's method.  This requires slightly more operations than the basic Horner's method, but allows k-way SIMD execution of most of them.\n\n Divided difference of a polynomial \n\nHorner's method can be modified to compute the divided difference  Given the polynomial (as before)\n\nproceed as follows\n\nAt completion, we have\n\nThis computation of the divided difference is subject to less\nround-off error than evaluating  and  separately, particularly when\n.  Substituting\n in this method gives , the derivative of .\n\n History \nthumb|right|200px|Qin Jiushao's algorithm for solving the quadratic polynomial equationresult: x=840.\nHorner's paper, titled \"A new method of solving numerical equations of all orders, by continuous approximation\",. was read before the Royal Society of London, at its meeting on July 1, 1819, with Davies Gilbert, Vice-President and Treasurer, in the chair; this was the final meeting of the session before the Society adjourned for its Summer recess. When a sequel was read before the Society in 1823, it was again at the final meeting of the session. On both occasions, papers by James Ivory, FRS, were also read. In 1819, it was Horner's paper that got through to publication in the \"Philosophical Transactions\". later in the year, Ivory's paper falling by the way, despite Ivory being a Fellow; in 1823, when a total of ten papers were read, fortunes as regards publication, were reversed. Gilbert, who had strong connections with the West of England and may have had social contact with Horner, resident as Horner was in Bristol and Bath, published his own survey of Horner-type methods earlier in 1823.\n\nHorner's paper in Part II of Philosophical Transactions of the Royal Society of London for 1819 was warmly and expansively welcomed by a reviewer in the issue of The Monthly Review: or, Literary Journal for April, 1820; in comparison, a technical paper by Charles Babbage is dismissed curtly in this review. However, the reviewer noted that another, similar method had also recently been published by the architect and mathematical expositor, Peter Nicholson. This theme is developed in a further review of some of Nicholson's books in the issue of The Monthly Review for December, 1820, which in turn ends with notice of the appearance of a booklet by Theophilus Holdred, from whom Nicholson acknowledges he obtained the gist of his approach in the first place, although claiming to have improved upon it. The sequence of reviews is concluded in the issue of The Monthly Review for September, 1821, with the reviewer concluding that whereas Holdred was the first person to discover a direct and general practical solution of numerical equations, he had not reduced it to its simplest form by the time of Horner's publication, and saying that had Holdred published forty years earlier when he first discovered his method, his contribution could be more easily recognized. The reviewer is exceptionally well-informed, even having cited Horner's preparatory correspondence with Peter Barlow in 1818, seeking work of Budan. The Bodlean Library, Oxford has the Editor's annotated copy of The Monthly Review from which it is clear that the most active reviewer in mathematics in 1814 and 1815 (the last years for which this information has been published) was none other than Peter Barlow, one of the foremost specialists on approximation theory of the period, suggesting that it was Barlow, who wrote this sequence of reviews. As it also happened, Henry Atkinson, of Newcastle, devised a similar approximation scheme in 1809; he had consulted his fellow Geordie, Charles Hutton, another specialist and a senior colleague of Barlow at the Royal Military Academy, Woolwich, only to be advised that, while his work was publishable, it was unlikely to have much impact. J. R. Young, writing in the mid-1830s, concluded that Holdred's first method replicated Atkinson's while his improved method was only added to Holdred's booklet some months after its first appearance in 1820, when Horner's paper was already in circulation.\n\nThe feature of Horner's writing that most distinguishes it from his English contemporaries is the way he draws on the Continental literature, notably the work of Arbogast. The advocacy, as well as the detraction, of Horner's Method has this as an unspoken subtext. Quite how he gained that familiarity has not been determined. Horner is known to have made a close reading of John Bonneycastle's book on algebra. Bonneycastle recognizes that Arbogast has the general, combinatorial expression for the reversion of series, a project going back at least to Newton. But Bonneycastle's main purpose in mentioning Arbogast is not to praise him, but to observe that Arbogast's notation is incompatible with the approach he adopts. The gap in Horner's reading was the work of Paolo Ruffini, except that, as far as awareness of Ruffini goes, citations of Ruffini's work by authors, including medical authors, in Philosophical Transactions speak volumes, as there are none — Ruffini's namehttp://hdl.handle.net/2027/njp.32101013501372?urlappend=%3Bseq=695 only appears in 1814, recording a work he donated to the Royal Society. Ruffini might have done better if his work had appeared in French, as had Malfatti's Problem in the reformulation of Joseph Diez Gergonne, or had he written in French, as had Antonio Cagnoli, a source quoted by Bonneycastle on series reversion. (Today, Cagnoli is in the Italian Wikipedia, as shown, but has yet to make it into either French or English.)\n\nFuller. showed that the method in Horner's 1819 paper differs from what afterwards became known as \"Horner's method\" and that in consequence the priority for this method should go to Holdred (1920). This view may be compared with the remarks concerning the works of Horner and Holdred in the previous paragraph. Fuller also takes aim at Augustus De Morgan. Precocious though Augustus de Morgan was, he was not the reviewer for The Monthly Review, while several others — Thomas Stephens Davies, J. R. Young, Stephen Fenwick, T. T. Wilkinson — wrote Horner firmly into their records, not least Horner, as he published extensively up until the year of his death in 1837. His paper in 1819 was one that would have been difficult to miss. In contrast, the only other mathematical sighting of Holdred is a single named contribution to The Gentleman's Mathematical Companion, an answer to a problem.\n\nIt is questionable to what extent it was De Morgan's advocacy of Horner's priority in discovery. that led to \"Horner's method\" being so called in textbooks, but it is true that those suggesting this tend to know of Horner largely through intermediaries, of whom De Morgan made himself a prime example. However, this method qua method was known long before Horner. In reverse chronological order, Horner's method was already known to:\n\n Paolo Ruffini in 1809 (see Ruffini's rule)\n Isaac Newton in 1669 (but precise reference needed)\n the Chinese mathematician Zhu Shijie in the 14th century\n the Chinese mathematician Qin Jiushao in his Mathematical Treatise in Nine Sections in the 13th century\n the Persian mathematician Sharaf al-Dīn al-Ṭūsī  in the 12th century (the first to use that method in a general case of cubic equation).\n the Chinese mathematician Jia Xian in the 11th century (Song dynasty)\n The Nine Chapters on the Mathematical Art, a Chinese work of the Han dynasty (202 BC – 220 AD) edited by Liu Hui (fl. 3rd century)..\n\nHowever, this observation on its own masks significant differences in conception and also, as noted with Ruffini's work, issues of accessibility.\n\nQin Jiushao, in his Shu Shu Jiu Zhang (Mathematical Treatise in Nine Sections; 1247), presents a portfolio of methods of Horner-type for solving polynomial equations, which was based on earlier works of the 11th century Song dynasty mathematician Jia Xian; for example, one method is specifically suited to bi-quintics, of which Qin gives an instance, in keeping with the then Chinese custom of case studies. The first person writing in English to note the connection with Horner's method was Alexander Wylie, writing in The North China Herald in 1852; perhaps conflating and misconstruing different Chinese phrases, Wylie calls the method Harmoniously Alternating Evolution (which does not agree with his Chinese, linglong kaifang, not that at that date he uses pinyin), working the case of one of Qin's quartics and giving, for comparison, the working with Horner's method. Yoshio Mikami in Development of Mathematics in China and Japan published in Leipzig in 1913, gave a detailed description of Qin's method, using the quartic illustrated to the above right in a worked example; he wrote:\n\nHowever, as Mikami is also aware, it was not altogether impossible that a related work, Si Yuan Yu Jian (Jade Mirror of the Four Unknowns; 1303) by Zhu Shijie might make the shorter journey across to Japan, but seemingly it never did, although another work of Zhu, Suan Xue Qi Meng, had a seminal influence on the development of traditional mathematics in the Edo period, starting in the mid-1600s. Ulrich Libbrecht (at the time teaching in school, but subsequently a professor of comparative philosophy) gave a detailed description in his doctoral thesis of Qin's method, he concluded: It is obvious that this procedure is a Chinese invention ... the method was not known in India. He said, Fibonacci probably learned of it from Arabs, who perhaps borrowed from the Chinese.. Here, the problems is that there is no more evidence for this speculation than there is of the method being known in India. Of course, the extraction of square and cube roots along similar lines is already discussed by Liu Hui in connection with Problems IV.16 and 22 in Jiu Zhang Suan Shu, while Wang Xiaotong in the 7th century supposes his readers can solve cubics by an approximation method described in his book Jigu Suanjing.\n\n See also \n\nClenshaw algorithm to evaluate polynomials in Chebyshev form\nDe Boor's algorithm to evaluate splines in B-spline form\nDe Casteljau's algorithm to evaluate polynomials in Bézier form\nEstrin's scheme to facilitate parallelization on modern computer architectures\nLill's method to approximate roots graphically\nRuffini's rule to divide a polynomial by a binomial of the form x − r\n\n Notes \n\n References \n\n Read before the Southwestern Section of the American Mathematical Society on November 26, 1910.\n\n Holdred's method is in the supplement following page numbered 45 (which is the 52nd page of the pdf version).\n\n Directly available online via the link, but also reprinted with appraisal in D.E. Smith: A Source Book in Mathematics, McGraw-Hill, 1929; Dover reprint, 2 vols, 1959.\n\n \n\n \n \n Reprinted from issues of The North China Herald (1852).\n\n External links \n \n Qiu Jin-Shao, Shu Shu Jiu Zhang (Cong Shu Ji Cheng ed.)\n\nCategory:Algebra\nCategory:Polynomials\nCategory:Numerical analysis\nCategory:Articles with example Python code\nCategory:Articles with example MATLAB/Octave code\nCategory:Articles with example C code"
    },
    {
      "title": "Howson property",
      "url": "https://en.wikipedia.org/wiki/Howson_property",
      "text": "In the mathematical subject of group theory, the Howson property, also known as the finitely generated intersection property (FGIP), is the property of a group saying that the intersection of any two finitely generated subgroups of this group is again finitely generated. The property is named after Albert G. Howson who in a 1954 paper established that free groups have this property.A. G. Howson, On the intersection of finitely generated free groups. \nJournal of the London Mathematical Society 29 (1954), 428–434\n\nFormal definition\n\nA group  is said to have the Howson property if for every finitely generated subgroups  of  their intersection  is again a finitely generated subgroup of .O. Bogopolski, \nIntroduction to group theory. \nTranslated, revised and expanded from the 2002 Russian original. EMS Textbooks in Mathematics. European Mathematical Society (EMS), Zürich, 2008.  ; p. 102\n\nExamples and non-examples\n\nEvery finite group has the Howson property.\nThe group  does not have the Howson property.  Specifically, if  is the generator of the  factor of , then for  and , one has . Therefore,  is not finitely generated.D. I. Moldavanskii, The intersection of finitely generated subgroups  Siberian Mathematical Journal 9 (1968), 1422–1426 \nIf  is a compact surface then the fundamental group  of  has the Howson property.L. Greenberg, Discrete groups of motions. \nCanadian Journal of Mathematics 12 (1960), 415–426\nA free-by-(infinite cyclic group) , where , never has the Howson property.R. G. Burns and A. M. Brunner, Two remarks on the group property of Howson, Algebra i Logika 18 (1979), 513–522\nIn view of the recent proof of the Virtually Haken conjecture and the Virtually fibered conjecture for 3-manifolds, previously established results imply that if M is a closed hyperbolic 3-manifold then  does not have the Howson property.T. Soma, 3-manifold groups with the finitely generated intersection property, Transactions of the American Mathematical Society, 331 (1992), no. 2, 761–769 \nAmong 3-manifold groups, there are many examples that do and do not have the Howson property. 3-manifold groups with the Howson property include fundamental groups of hyperbolic 3-manifolds of infinite volume, 3-manifold groups based on Sol and Nil geometries, as well as 3-manifold groups obtained by some connected sum and JSJ decomposition constructions.  \nFor every  the Baumslag–Solitar group  has the Howson property. \nIf G is group where every finitely generated subgroup is Noetherian then G has the Howson property. In particular, all abelian groups and all nilpotent groups have the Howson property.  \nEvery polycyclic-by-finite group has the Howson property.V. Araújo, P. Silva, M. Sykiotis, Finiteness results for subgroups of finite extensions. Journal of Algebra 423 (2015), 592–614\nIf  are groups with the Howson property then their free product  also has the Howson property.B. Baumslag, Intersections of finitely generated subgroups in free products. Journal of the London Mathematical Society 41 (1966), 673–679 More generally, the Howson property is preserved under taking amalgamated free products and HNN-extension of groups with the Howson property over finite subgroups.D. E. Cohen,\nFinitely generated subgroups of amalgamated free products and HNN groups. \nJ. Austral. Math. Soc. Ser. A 22 (1976), no. 3, 274–281\nIn general, the Howson property is rather sensitive to amalgamated products and HNN extensions over infinite subgroups. In particular, for free groups  and an infinite cyclic group , the amalgamated free product  has the Howson property if and only if  is a maximal cyclic subgroup in both  and .R. G. Burns,\nOn the finitely generated subgroups of an amalgamated product of two groups. \nTransactions of the American Mathematical Society 169 (1972), 293–306\nA right-angled Artin group  has the Howson property if and only if every connected component of  is a complete graph.H. Servatius, C. Droms, B. Servatius, The finite basis extension property and graph groups. Topology and combinatorial group theory (Hanover, NH, 1986/1987; Enfield, NH, 1988), 52–58, \nLecture Notes in Math., 1440, Springer, Berlin, 1990\nLimit groups have the Howson property.F. Dahmani, Combination of convergence groups. Geometry & Topology 7 (2003), 933–963\nIt is not known whether  has the Howson property.D. D. Long and A. W. Reid,  Small Subgroups of , Experimental Mathematics, 20(4):412–425, 2011\nFor  the group  contains a subgroup isomorphic to  and does not have the Howson property.\nMany small cancellation groups and Coxeter groups, satisfying the ``perimeter reduction\" condition on their presentation, are locally quasiconvex word-hyperbolic groups and therefore have the Howson property.J. P. McCammond, D. T. Wise, Coherence, local quasiconvexity, and the perimeter of 2-complexes. Geometric and Functional Analysis 15 (2005), no. 4, 859–927P. Schupp, Coxeter groups, 2-completion, perimeter reduction and subgroup separability, Geometriae Dedicata 96 (2003) 179–198\nOne-relator groups , where  are also locally quasiconvex word-hyperbolic groups and therefore have the Howson property.G. Ch. Hruska, D. T. Wise, \nTowers, ladders and the B. B. Newman spelling theorem.\nJournal of the Australian Mathematical Society 71 (2001), no. 1, 53–69\nThe Grigorchuk group G of intermediate growth does not have the Howson property.A. V. Rozhkov,\nCentralizers of elements in a group of tree automorphisms. \nIzv. Ross. Akad. Nauk Ser. Mat. 57 (1993), no. 6, 82–105; translation in: \nRussian Acad. Sci. Izv. Math. 43 (1993), no. 3, 471–492\nThe Howson property is not a first-order property, that is the Howson property cannot be characterized by a collection of first order group language formulas.B. Fine, A. Gaglione, A. Myasnikov, G. Rosenberger, D. Spellman, The elementary theory of groups. A guide through the proofs of the Tarski conjectures. De Gruyter Expositions in Mathematics, 60. De Gruyter, Berlin, 2014. ; Theorem 10.4.13 on p. 236\nA free pro-p group  satisfies a topological version of the Howson property: If  are topologically finitely generated closed subgroups of  then their intersection  is topologically finitely generated.L. Ribes,  and P. Zalesskii, Profinite groups. Second edition. Ergebnisse der Mathematik und ihrer Grenzgebiete. 3. Folge. A Series of Modern Surveys in Mathematics [Results in Mathematics and Related Areas. 3rd Series. A Series of Modern Surveys in Mathematics], 40. Springer-Verlag, Berlin, 2010. ; Theorem 9.1.20 on p. 366\nFor any fixed integers  a ``generic\" -generator -relator group  has the property that for any -generated subgroups  their intersection  is again finitely generated.G. N. Arzhantseva, \nGeneric properties of finitely presented groups and Howson's theorem. \nCommunications in Algebra 26 (1998), no. 11, 3783–3792\nThe wreath product  does not have the Howson property.A. S. Kirkinski,\nIntersections of finitely generated subgroups in metabelian groups.\nAlgebra i Logika 20 (1981), no. 1, 37–54; Lemma 3.\n\nSee also\nHanna Neumann conjecture\n\nReferences\n\nCategory:Group theory\nCategory:Algebra"
    },
    {
      "title": "Hundred Fowls Problem",
      "url": "https://en.wikipedia.org/wiki/Hundred_Fowls_Problem",
      "text": "The Hundred Fowls Problem is a problem first discussed in the fifth century CE Chinese mathematics text Zhang Qiujian suanjing (The Mathematical Classic of Zhang Qiujian), a book of mathematical problems written by Zhang Qiujian. It is one of the best known examples of indeterminate problems in the early history of mathematics. The problem appears as the final problem in Zhang Qiujian suanjing (Problem 38 in Chapter 3). However, the problem  and its variants have appeared in the  medieval mathematical literature of  India, Europe and the Arab world.\n\nThe name \"Hundred Fowls Problem\" is due to the Belgian historian Louis van Hee.\n\nProblem statement\n\nThe Hundred Fowls Problem as presented in Zhang Qiujian suanjing can be translated as follows:\n\n \"Now one cock is worth 5 qian, one hen 3 qian and 3 chicks 1 qian. It is required to buy 100 fowls with 100 qian. In each case, find the number of cocks, hens and chicks bought.\"\n\nMathematical formulation\nLet x be the number of cocks, y be the number of hens, and z be the number of chicks, then the problem is to find x, y and z satisfying the following equations:\n\n x + y +z = 100\n 5x + 3y + z/3 = 100\n\nObviously, only non-negative integer values are acceptable. Expressing y and z in terms of x we get\n\n y = 25 − (7/4)x\n z = 75 + (3/4)x\n\nSince x, y and z all must be integers, the expression for y suggests that x must be a multiple of 4. Hence the general solution of the system of equations can be expressed using an integer parameter t as follows:\n\n x = 4t\n y = 25 − 7t\n z = 75 + 3t\n\nSince y should be a non-negative integer, the only possible values of t are 0, 1, 2 and 3. So the complete set of solutions is given by\n\n(x,y,z) = (0,25,75), (4,18,78), (8,11,81), (12,4,84).\n\nof which the last three have been given in Zhang Qiujian suanjing.  However, no general method for solving such problems has been indicated, leading to a suspicion of whether the solutions have been obtained by trial and error.\n\nThe Hundred Fowls Problem found in Zhang Qiujian suanjing is a special case of the general problem of finding integer solutions of the following system of equations:\n\n x + y + z = d\n ax + by + cz = d\n\nAny problem of this type is sometime referred to as \"Hundred Fowls problem\".\n\nVariations\n\nSome variants of the Hundred Fowls Problem have appeared in the mathematical literature of several cultures. In the following we present a few sample problems discussed in these cultures.\n\nIndian mathematics\nMahavira's Ganita-sara-sangraha contains the following problem:\n\nPigeons are sold at the rate of 5 for 3, sarasa-birds at the rate of 7 for 5, swans at the rate of 9 for 7, and peacocks at the rate of 3 for 9 (panas).  A certain man was told to bring 100 birds for 100 panas. What does he give for each of the various kinds of birds he buys?\n\nThe Bakshali manuscript gives the problem of solving the following equations:\n\nx + y + z = 20\n3x + (3/2)y + (1/2)z = 20\n\nMedieval Europe\nThe English mathematician Alcuin of York (6th century) has stated seven problems similar to the Hundred Fowls Problem in his Propositiones ad acuendos iuvenes. Here is a typical problem:\n\nIf 100 bushels of corn be distributed among 100 people such that each man gets 3 bushels, each woman 2 bushels and each child half a bushel, then how many men, women and children were there?\n\nArabian mathematics\nAbu Kamil (850 - 930 CE) considered non-negative integer solutions of the following equations:\nx + y + z = 100\n3x + (/20)y+ (1/3)z = 100.\n\nReferences\n\nCategory:Chinese mathematics\nCategory:Algebra\nCategory:Number theory\nCategory:Diophantine equations"
    },
    {
      "title": "Idempotent matrix",
      "url": "https://en.wikipedia.org/wiki/Idempotent_matrix",
      "text": "In linear algebra, an idempotent matrix is a matrix which, when multiplied by itself, yields itself. That is, the matrix  is idempotent if and only if . For this product  to be defined,  must necessarily be a square matrix. Viewed this way, idempotent matrices are idempotent elements of matrix rings.\n\nDefinition\nA  square matrix  is called idempotent if, multiplied by itself, yields itself, i.e.\n\nExample\nExamples of  idempotent matrices are:\n\nExamples of  idempotent matrices are:\n\nReal 2 × 2 case\nIf a matrix  is idempotent, then\n \n  implying  so  or \n  implying  so  or \n \n\nThus a necessary condition for a 2 × 2 matrix to be idempotent is that either it is diagonal or its trace equals 1.\nNotice that, for idempotent diagonal matrices,  and  must be either 1 or 0.\n\nIf , the matrix  will be idempotent provided  so a satisfies the quadratic equation\n or \n\nwhich is a circle with center (1/2, 0) and radius 1/2. In terms of an angle θ,\n is idempotent.\n\nHowever,  is not a necessary condition: any matrix\n with  is idempotent.\n\nProperties\nSingularity and regularity\nThe only non-singular idempotent matrix is the identity matrix; that is, if a non-identity matrix is idempotent, its number of independent rows (and columns) is less than its number of rows (and columns).\n\nThis can be seen from writing , assuming that  has full rank (is non-singular), and pre-multiplying by  to obtain .\n\nWhen an idempotent matrix is subtracted from the identity matrix, the result is also idempotent. This holds since\n.\n\nA matrix  is idempotent if and only if for all positive integers n, . The 'if' direction trivially follows by taking . The 'only if' part can be shown using proof by induction. Clearly we have the result for , as . Suppose that . Then, , as required. Hence by the principle of induction, the result follows.\n\nEigenvalues\nAn idempotent matrix is always diagonalizable and its eigenvalues are either 0 or 1.\n\nTrace\nThe trace of an idempotent matrix — the sum of the elements on its main diagonal — equals the rank of the matrix and thus is always an integer. This provides an easy way of computing the rank, or alternatively an easy way of determining the trace of a matrix whose elements are not specifically known (which is helpful in statistics, for example, in establishing the degree of bias in using a sample variance as an estimate of a population variance).\n\nApplications\nIdempotent matrices arise frequently in regression analysis and econometrics. For example, in ordinary least squares, the regression problem is to choose a vector  of coefficient estimates so as to minimize the sum of squared residuals (mispredictions) ei: in matrix form,\n Minimize \n\nwhere  is a vector of dependent variable observations, and  is a matrix each of whose columns is a column of observations on one of the independent variables. The resulting estimator is\n\nwhere superscript T indicates a transpose, and the vector of residuals is\n\nHere both  and (the latter being known as the hat matrix) are idempotent and symmetric matrices, a fact which allows simplification when the sum of squared residuals is computed:\n\nThe idempotency of  plays a role in other calculations as well, such as in determining the variance of the estimator .\n\nAn idempotent linear operator  is a projection operator on the range space  along its null space .  is an orthogonal projection operator if and only if it is idempotent and symmetric.\n\nSee also\n Idempotence\n Nilpotent\n Projection (linear algebra)\n Hat matrix\n\nReferences\n\nCategory:Algebra\nCategory:Regression analysis\nCategory:Matrices"
    },
    {
      "title": "Identity element",
      "url": "https://en.wikipedia.org/wiki/Identity_element",
      "text": "In mathematics, an identity element or neutral element is a special type of element of a set with respect to a binary operation on that set, which leaves any element of the set unchanged when combined with it. This concept is used in algebraic structures such as groups and rings. The term identity element is often shortened to identity (as will be done in this article) when there is no possibility of confusion, but the identity implicitly depends on the binary operation it is associated with.\n\nLet  be a set  with a binary operation ∗ on it. Then an element  of  is called a left identity if  for all  in , and a right identity if  for all  in . If  is both a left identity and a right identity, then it is called a two-sided identity, or simply an identity.\n\nAn identity with respect to addition is called an additive identity (often denoted as 0) and an identity with respect to multiplication is called a multiplicative identity (often denoted as 1). These need not be ordinary addition and multiplication, but rather arbitrary operations. The distinction is used most often for sets that support both binary operations, such as rings and fields. The multiplicative identity is often called unity in the latter context (a ring with unity). This should not be confused with a unit in ring theory, which is any element having a multiplicative inverse. Unity itself is necessarily a unit.\n\nExamples\n Set  Operation  Identity Real numbers  + (addition)  0 Real numbers  · (multiplication)  1 Positive integers  Least common multiple  1 Non-negative integers  Greatest common divisor  0 (under most definitions of GCD) -by- matrices  Matrix addition Zero matrix -by- square matrices  Matrix multiplication In (identity matrix) -by- matrices  ○ (Hadamard product)  (matrix of ones) All functions from a set, , to itself  ∘ (function composition)  Identity function All distributions on a group,   ∗ (convolution)   (Dirac delta) Extended real numbers  Minimum/infimum  +∞ Extended real numbers  Maximum/supremum  −∞ Subsets of a set   ∩ (intersection)   Sets  ∪ (union)  ∅ (empty set) Strings, lists  Concatenation  Empty string, empty list A Boolean algebra  ∧ (logical and)  ⊤ (truth) A Boolean algebra  ∨ (logical or)  ⊥ (falsity) A Boolean algebra  ⊕ (exclusive or)  ⊥ (falsity) Knots  Knot sum  Unknot Compact surfaces  # (connected sum)  S2 Groups  Direct product  Trivial group Two elements,  ∗ defined by  and   Both  and  are left identities, but there is no right identity and no two-sided identity Homogeneous relations on a set X  Relative product  Identity relation\n\nProperties\nAs the last example (a semigroup) shows, it is possible for  to have several left identities. In fact, every element can be a left identity. Similarly, there can be several right identities. But if there is both a right identity and a left identity, then they are equal and there is just a single two-sided identity. To see this, note that if  is a left identity and  is a right identity then . In particular, there can never be more than one two-sided identity.  If there were two,  and , then  would have to be equal to both  and .\n\nIt is also quite possible for  to have no identity element. A common example of this is the cross product of vectors; in this case, the absence of an identity element is related to the fact that the direction of any nonzero cross product is always orthogonal to any element multiplied – so that it is not possible to obtain a non-zero vector in the same direction as the original. Another example would be the additive semigroup of positive natural numbers.\n\nSee also\n Absorbing element\n Additive inverse\n Generalized inverse\n Inverse element\n Monoid\n Pseudo-ring\n Quasigroup\n Unital (disambiguation)\n\nReferences\n M. Kilp, U. Knauer, A.V. Mikhalev, Monoids, Acts and Categories with Applications to Wreath Products and Graphs, De Gruyter Expositions in Mathematics vol. 29, Walter de Gruyter, 2000, , p. 14–15\n\nCategory:Abstract algebra\nCategory:Algebra\n*Identity element\nCategory:1 (number)"
    },
    {
      "title": "Immanant",
      "url": "https://en.wikipedia.org/wiki/Immanant",
      "text": "Not be confused with the philosophical immanent, the temporal imminence, or the prominent eminence.\n\nIn mathematics, the immanant of a matrix was defined by Dudley E. Littlewood and Archibald Read Richardson as a generalisation of the concepts of determinant and permanent.\n\nLet  be a partition of  and let  be the corresponding irreducible representation-theoretic character of the symmetric group . The immanant of an  matrix  associated with the character  is defined as the expression\n\nThe determinant is a special case of the immanant, where  is the alternating character , of Sn, defined by the parity of a permutation.\n\nThe permanent is the case where  is the trivial character, which is identically equal to 1.\n\nFor example, for  matrices, there are three irreducible representations of , as shown in the character table:\n      1 1 1  1 −1 1  2 0 −1\nAs stated above,  produces the permanent and  produces the determinant, but  produces the operation that maps as follows:\n\nLittlewood and Richardson also studied its relation to Schur functions in the representation theory of the symmetric group.\n\nReferences\n\n \n \n\nExternal links\nImmanent at PlanetMath\n\nCategory:Algebra\nCategory:Linear algebra\nCategory:Matrix theory\nCategory:Permutations"
    },
    {
      "title": "Indeterminate equation",
      "url": "https://en.wikipedia.org/wiki/Indeterminate_equation",
      "text": "An indeterminate equation, in mathematics, is an equation for which there is more than one solution; for example, 2x = y is a simple indeterminate equation, as are ax + by = c and x2 = 1.  Indeterminate equations cannot be solved uniquely. Prominent examples include the following:\n\nUnivariate polynomial equation:\n\nwhich has multiple solutions for the variable x in the complex plane unless it can be rewritten in the form .\n\nNon-degenerate conic equation:\n\nwhere at least one of the given parameters A, B, and C is non-zero, and x and y are real variables.Pell's equation:\n\nwhere P is a given integer that is not a square number, and in which the variables x and y are required to be integers.The equation of Pythagorean triples:\n\nin which the variables x, y, and z are required to be positive integers.The equation of the Fermat–Catalan conjecture':\n\nin which the variables a, b, c are required to be coprime positive integers and the variables m, n, and k'' are required to be positive integers the sum of whose reciprocals is less than 1.\n\n See also \n Indeterminate system\n Indeterminate (variable)\n Linear algebra\n\n References \n\nCategory:Algebra"
    },
    {
      "title": "Infrastructure (number theory)",
      "url": "https://en.wikipedia.org/wiki/Infrastructure_%28number_theory%29",
      "text": "In mathematics, an infrastructure is a group-like structure appearing in global fields.\n\n Historic development \n\nIn 1972, D. Shanks first discovered the infrastructure of a real quadratic number field and applied his baby-step giant-step algorithm to compute the regulator of such a field in  binary operations (for every ), where  is the discriminant of the quadratic field; previous methods required  binary operations.D. Shanks: The infrastructure of a real quadratic field and its applications. Proceedings of the Number Theory Conference (Univ. Colorado, Boulder, Colo., 1972), pp. 217-224. University of Colorado, Boulder, 1972.  Ten years later, H. W. Lenstra publishedH. W. Lenstra Jr.: On the calculation of regulators and class numbers of quadratic fields. Number theory days, 1980 (Exeter, 1980), 123–150, London Math. Soc. Lecture Note Ser., 56, Cambridge University Press, Cambridge, 1982.  a mathematical framework describing the infrastructure of a real quadratic number field in terms of \"circular groups\". It was also described by R. SchoofR. J. Schoof: Quadratic fields and factorization. Computational methods in number theory, Part II, 235–286, Math. Centre Tracts, 155, Math. Centrum, Amsterdam, 1982.  and H. C. Williams,H. C. Williams: Continued fractions and number-theoretic computations. Number theory (Winnipeg, Man., 1983). Rocky Mountain J. Math. 15 (1985), no. 2, 621–655.  and later extended by H. C. Williams, G. W. Dueck and B. K. Schmid to certain cubic number fields of unit rank oneH. C. Williams, G. W. Dueck, B. K. Schmid: A rapid method of evaluating the regulator and class number of a pure cubic field. Math. Comp. 41 (1983), no. 163, 235–286. G. W. Dueck, H. C. Williams: Computation of the class number and class group of a complex cubic field. Math. Comp. 45 (1985), no. 171, 223–231.  and by J. Buchmann and H. C. Williams to all number fields of unit rank one.J. Buchmann, H. C. Williams: On the infrastructure of the principal ideal class of an algebraic number field of unit rank one. Math. Comp. 50 (1988), no. 182, 569–579.  In his habilitation thesis, J. Buchmann presented a baby-step giant-step algorithm to compute the regulator of a number field of arbitrary unit rank.J. Buchmann: Zur Komplexität der Berechnung von Einheiten und Klassenzahlen algebraischer Zahlkörper. Habilitationsschrift, Düsseldorf, 1987. PDF The first description of infrastructures in number fields of arbitrary unit rank was given by R. Schoof using Arakelov divisors in 2008.R. Schoof: Computing Arakelov class groups. (English summary) Algorithmic number theory: lattices, number fields, curves and cryptography, 447–495, Math. Sci. Res. Inst. Publ., 44, Cambridge University Press, 2008.  PDF\n\nThe infrastructure was also described for other global fields, namely for algebraic function fields over finite fields. This was done first by A. Stein and H. G. Zimmer in the case of real hyperelliptic function fields.A. Stein, H. G. Zimmer: An algorithm for determining the regulator and the fundamental unit of hyperelliptic congruence function field. In \"Proceedings of the 1991 International Symposium on Symbolic and Algebraic Computation, ISSAC '91,\" Association for Computing Machinery, (1991), 183–184. It was extended to certain cubic function fields of unit rank one by R. Scheidler and A. Stein.R. Scheidler, A. Stein: Unit computation in purely cubic function fields of unit rank 1. (English summary) Algorithmic number theory (Portland, OR, 1998), 592–606, Lecture Notes in Comput. Sci., 1423, Springer, Berlin, 1998. R. Scheidler: Ideal arithmetic and infrastructure in purely cubic function fields. (English, French summary) J. Théor. Nombres Bordeaux 13 (2001), no. 2, 609–631.  In 1999, S. Paulus and H.-G. Rück related the infrastructure of a real quadratic function field to the divisor class group.S. Paulus, H.-G. Rück: Real and imaginary quadratic representations of hyperelliptic function fields. (English summary) Math. Comp. 68 (1999), no. 227, 1233–1241.  This connection can be generalized to arbitrary function fields and, combining with R. Schoof's results, to all global fields.\n\n One-dimensional case \n\n Abstract definition \n\nA one-dimensional (abstract) infrastructure  consists of a real number , a finite set  together with an injective map .F. Fontein: Groups from cyclic infrastructures and Pohlig-Hellman in certain infrastructures. (English summary) Adv. Math. Commun. 2 (2008), no. 3, 293–307.  The map  is often called the distance map.\n\nBy interpreting  as a circle of circumference  and by identifying  with , one can see a one-dimensional infrastructure as a circle with a finite set of points on it.\n\n Baby steps \n\nA baby step is a unary operation  on a one-dimensional infrastructure . Visualizing the infrastructure as a circle, a baby step assigns each point of  the next one. Formally, one can define this by assigning to  the real number ; then, one can define .\n\n Giant steps and reduction maps \n\nObserving that  is naturally an abelian group, one can consider the sum  for . In general, this is not an element of . But instead, one can take an element of  which lies nearby. To formalize this concept, assume that there is a map ; then, one can define  to obtain a binary operation , called the giant step operation. Note that this operation is in general not associative.\n\nThe main difficulty is how to choose the map . Assuming that one wants to have the condition , a range of possibilities remain.   One possible choice is given as follows: for , define ; then one can define . This choice, seeming somewhat arbitrary, appears in a natural way when one tries to obtain infrastructures from global fields. Other choices are possible as well, for example choosing an element  such that  is minimal (here,  is stands for , as  is of the form ); one possible construction in the case of real quadratic hyperelliptic function fields is given by S. D. Galbraith, M. Harrison and D. J. Mireles Morales.S. D. Galbraith, M. Harrison, D. J. Mireles Morales: Efficient hyperelliptic arithmetic using balanced representation for divisors. (English summary) Algorithmic number theory, 342–356, Lecture Notes in Comput. Sci., 5011, Springer, Berlin, 2008. \n\n Relation to real quadratic fields \n\nD. Shanks observed the infrastructure in real quadratic number fields when he was looking at cycles of reduced binary quadratic forms. Note that there is a close relation between reducing binary quadratic forms and continued fraction expansion; one step in the continued fraction expansion of a certain quadratic irrationality gives a unary operation on the set of reduced forms, which cycles through all reduced forms in one equivalence class. Arranging all these reduced forms in a cycle, Shanks noticed that one can quickly jump to reduced forms further away from the beginning of the circle by composing two such forms and reducing the result. He called this binary operation on the set of reduced forms a giant step, and the operation to go to the next reduced form in the cycle a baby step.\n\n Relation to  \n\nThe set  has a natural group operation and the giant step operation is defined in terms of it. Hence, it makes sense to compare the arithmetic in the infrastructure to the arithmetic in . It turns out that the group operation of  can be described using giant steps and baby steps, by representing elements of  by elements of  together with a relatively small real number; this has been first described by D. Hühnlein and S. PaulusD. Hühnlein, S. Paulus: On the implementation of cryptosystems based on real quadratic number fields (extended abstract). Selected areas in cryptography (Waterloo, ON, 2000), 288–302, Lecture Notes in Comput. Sci., 2012, Springer, 2001.  and by M. J. Jacobson, Jr., R. Scheidler and H. C. WilliamsM. J. Jacobson Jr., R. Scheidler, H. C. Williams: The efficiency and security of a real quadratic field based key exchange protocol. Public-key cryptography and computational number theory (Warsaw, 2000), 89–112, de Gruyter, Berlin, 2001  in the case of infrastructures obtained from real quadratic number fields. They used floating point numbers to represent the real numbers, and called these representations CRIAD-representations resp. -representations. More generally, one can define a similar concept for all one-dimensional infrastructures; these are sometimes called -representations.\n\nA set of -representations is a subset  of  such that the map  is a bijection and that  for every . If  is a reduction map,  is a set of -representations; conversely, if  is a set of -representations, one can obtain a reduction map by setting , where  is the projection on $X$. Hence, sets of -representations and reduction maps are in a one-to-one correspondence.\n\nUsing the bijection , one can pull over the group operation on  to , hence turning  into an abelian group  by , . In certain cases, this group operation can be explicitly described without using  and .\n\nIn case one uses the reduction map , one obtains . Given , one can consider  with  and ; this is in general no element of , but one can reduce it as follows: one computes  and ; in case the latter is not negative, one replaces  with  and continues. If the value was negative, one has that  and that , i.e. .\n\n References \n\nCategory:Algebra\nCategory:Algebraic structures"
    },
    {
      "title": "Inverse element",
      "url": "https://en.wikipedia.org/wiki/Inverse_element",
      "text": "In abstract algebra, the idea of an inverse element generalises concepts of a negation (sign reversal) in relation to addition, and a reciprocal in relation to multiplication. The intuition is of an element that can 'undo' the effect of combination with another given element. While the precise definition of an inverse element varies depending on the algebraic structure involved, these definitions coincide in a group.\n\nThe word 'inverse' is derived from  that means 'turned upside down', 'overturned'.\n\n Formal definitions \n\n In a unital magma \nLet  be a set closed under a binary operation  (i.e., a magma). If  is an identity element of  (i.e., S is a unital magma) and , then  is called a left inverse of  and  is called a right inverse of . If an element  is both a left inverse and a right inverse of , then  is called a two-sided inverse, or simply an inverse, of . An element with a two-sided inverse in  is called invertible in . An element with an inverse element only on one side is left invertible, resp. right invertible. A unital magma in which all elements are invertible is called a loop. A loop whose binary operation satisfies the associative law is a group.\n\nJust like  can have several left identities or several right identities, it is possible for an element to have several left inverses or several right inverses (but note that their definition above uses a two-sided identity ). It can even have several left inverses and several right inverses.\n\nIf the operation  is associative then if an element has both a left inverse and a right inverse, they are equal. In other words, in a monoid (an associative unital magma) every element has at most one inverse (as defined in this section). In a monoid, the set of (left and right) invertible elements is a group, called the group of units of , and denoted by  or H1.\n\nA left-invertible element is left-cancellative, and analogously for right and two-sided.\n\n In a semigroup \n\nThe definition in the previous section generalizes the notion of inverse in group relative to the notion of identity. It's also possible, albeit less obvious, to generalize the notion of an inverse by dropping the identity element but keeping associativity, i.e., in a semigroup.\n\nIn a semigroup S an element x is called (von Neumann) regular if there exists some element z in S such that xzx = x; z is sometimes called a pseudoinverse. An element y is called (simply) an inverse of x if xyx = x and y = yxy. Every regular element has at least one inverse: if x = xzx then it is easy to verify that y = zxz is an inverse of x as defined in this section. Another easy to prove fact: if y is an inverse of x then e = xy and f = yx are idempotents, that is ee = e and ff = f. Thus, every pair of (mutually) inverse elements gives rise to two idempotents, and ex = xf = x, ye = fy = y, and e acts as a left identity on x, while f acts a right identity, and the left/right roles are reversed for y. This simple observation can be generalized using Green's relations: every idempotent e in an arbitrary semigroup is a left identity for Re and right identity for Le.Howie, prop. 2.3.3, p. 51 An intuitive description of this fact is that every pair of mutually inverse elements produces a local left identity, and respectively, a local right identity.\n\nIn a monoid, the notion of inverse as defined in the previous section is strictly narrower than the definition given in this section. Only elements in the Green class  H1 have an inverse from the unital magma perspective, whereas for any idempotent e, the elements of He have an inverse as defined in this section. Under this more general definition, inverses need not be unique (or exist) in an arbitrary semigroup or monoid. If all elements are regular, then the semigroup (or monoid) is called regular, and every element has at least one inverse. If every element has exactly one inverse as defined in this section, then the semigroup is called an inverse semigroup. Finally, an inverse semigroup with only one idempotent is a group. An inverse semigroup may have an absorbing element 0 because 000 = 0, whereas a group may not.\n\nOutside semigroup theory, a unique inverse as defined in this section is sometimes called a quasi-inverse. This is generally justified because in most applications (e.g., all examples in this article) associativity holds, which makes this notion a generalization of the left/right inverse relative to an identity.\n\n U-semigroups \n\nA natural generalization of the inverse semigroup is to define an (arbitrary) unary operation ° such that (a°)° = a for all a in S; this endows S with a type 2,1 algebra. A semigroup endowed with such an operation is called a U-semigroup. Although it may seem that a° will be the inverse of a, this is not necessarily the case. In order to obtain interesting notion(s), the unary operation must somehow interact with the semigroup operation. Two classes of U-semigroups have been studied:Howie p. 102\n\n I-semigroups, in which the interaction axiom is aa°a = a\n *-semigroups, in which the interaction axiom is (ab)° = b°a°. Such an operation is called an involution, and typically denoted by a*\n\nClearly a group is both an I-semigroup and a *-semigroup. A class of semigroups important in semigroup theory are completely regular semigroups; these are I-semigroups in which one additionally has aa° = a°a; in other words every element has commuting pseudoinverse a°. There are few concrete examples of such semigroups however; most are completely simple semigroups. In contrast, a subclass of *-semigroups, the *-regular semigroups (in the sense of Drazin), yield one of best known examples of a (unique) pseudoinverse, the Moore–Penrose inverse. In this case however the involution a* is not the pseudoinverse. Rather, the pseudoinverse of x is the unique element y such that xyx = x, yxy = y,   (xy)* = xy, (yx)* = yx. Since *-regular semigroups generalize inverse semigroups, the unique element defined this way in a *-regular semigroup is called the generalized inverse or Penrose–Moore inverse.\n\n Rings and semirings \n\n Examples \nAll examples in this section involve associative operators, thus we shall use the terms left/right inverse for the unital magma-based definition, and quasi-inverse for its more general version.\n\n Real numbers \nEvery real number  has an additive inverse (i.e., an inverse with respect to addition) given by . Every nonzero real number  has a multiplicative inverse (i.e., an inverse with respect to multiplication) given by  (or ). By contrast, zero has no multiplicative inverse, but it has a unique quasi-inverse, \"\" itself.\n\n Functions and partial functions \nA function  is the left (resp. right) inverse of a function  (for function composition), if and only if  (resp. ) is the identity function on the domain (resp. codomain) of . The inverse of a function  is often written , but this notation is sometimes ambiguous. Only bijections have two-sided inverses, but any function has a quasi-inverse, i.e., the full transformation monoid is regular. The monoid of partial functions is also regular, whereas the monoid of injective partial transformations is the prototypical inverse semigroup.\n\n Galois connections \nThe lower and upper adjoints in a (monotone) Galois connection, L and G are quasi-inverses of each other, i.e. LGL = L and GLG = G and one uniquely determines the other. They are not left or right inverses of each other however.\n\n Matrices  \nA square matrix  with entries in a field  is invertible (in the set of all square matrices of the same size, under matrix multiplication) if and only if its determinant is different from zero. If the determinant of  is zero, it is impossible for it to have a one-sided inverse; therefore a left inverse or right inverse implies the existence of the other one. See invertible matrix for more.\n\nMore generally, a square matrix over a commutative ring  is invertible if and only if its determinant is invertible in .\n\nNon-square matrices of full rank have several one-sided inverses:MIT Professor Gilbert Strang Linear Algebra Lecture #33 – Left and Right Inverses; Pseudoinverse.\n For  we have a left inverse: \n For  we have a right inverse: \n\nThe left inverse can be used to determine the least norm solution of , which is also the least squares formula for regression and is given by \n\nNo rank deficient matrix has any (even one-sided) inverse.  However, the Moore–Penrose inverse exists for all matrices, and coincides with the left or right (or true) inverse when it exists.\n\nAs an example of matrix inverses, consider:\n\n \n\nSo, as m < n, we have a right inverse,  By components it is computed as\n\n \n\nThe left inverse doesn't exist, because\n\n \n\nwhich is a singular matrix, and cannot be inverted.\n\n See also \nLoop (algebra)\nDivision ring\nUnit (ring theory)\nLatin square property\n\nNotes\n\nReferences\n M. Kilp, U. Knauer, A.V. Mikhalev, Monoids, Acts and Categories with Applications to Wreath Products and Graphs, De Gruyter Expositions in Mathematics vol. 29, Walter de Gruyter, 2000, , p. 15 (def in unital magma) and p. 33 (def in semigroup)\n contains all of the semigroup material herein except *-regular semigroups.\n Drazin, M.P., Regular semigroups with involution, Proc. Symp. on Regular Semigroups (DeKalb, 1979), 29–46\n Miyuki Yamada, P-systems in regular semigroups, Semigroup Forum, 24(1), December 1982, pp. 173–187\nNordahl, T.E., and H.E. Scheiblich, Regular * Semigroups, Semigroup Forum, 16(1978), 369–377.\n\nCategory:Algebra\nCategory:Abstract algebra\n*Inverse element"
    },
    {
      "title": "Invertible module",
      "url": "https://en.wikipedia.org/wiki/Invertible_module",
      "text": "In mathematics, particularly commutative algebra, an invertible module is intuitively a module that has an inverse with respect to the tensor product. Invertible modules form the foundation for the definition of invertible sheaves in algebraic geometry.\n\nFormally, a finitely generated module M over a ring R is said to be invertible if it is locally a free module of rank 1. In other words,  for all primes P of R. Now, if M is an invertible R-module, then its dual  is its inverse with respect to the tensor product, i.e. .\n\nThe theory of invertible modules is closely related to the theory of codimension one varieties including the theory of divisors.\n\nSee also\n Picard group\n\nReferences\n Eisenbud, David, Commutative Algebra with a View Toward Algebraic Geometry, Springer, \n\nCategory:Mathematical structures\nCategory:Algebra"
    },
    {
      "title": "Irreducible polynomial",
      "url": "https://en.wikipedia.org/wiki/Irreducible_polynomial",
      "text": "In mathematics, an irreducible polynomial (or prime polynomial) is, roughly speaking, a non-constant polynomial that cannot be factored into the product of two non-constant polynomials. The property of irreducibility depends on the nature of the coefficients that are accepted for the possible factors, that is, the field or ring to which the coefficients of the polynomial and its possible factors are supposed to belong. For example, the polynomial  is a polynomial with integer coefficients, but, as every integer is also a real number, it is also a polynomial with real coefficients. It is irreducible if it is considered as a polynomial with integer coefficients, but it factors as  if it is considered as a polynomial with real coefficients. One says that the polynomial  is irreducible over the integers but not over the reals.\n\nA polynomial that is irreducible over any field containing the coefficients is absolutely irreducible. By the fundamental theorem of algebra, a univariate polynomial is absolutely irreducible if and only if its degree is one. On the other hand, with several indeterminates, there are absolutely irreducible polynomials of any degree, such as  for any positive integer .\n\nA polynomial that is not irreducible is sometimes said to be reducible.Gallian 2012, p. 311.Mac Lane and Birkhoff (1999) do not explicitly define \"reducible\", but they use it in several places. For example: \"For the present, we note only that any reducible quadratic or cubic polynomial must have a linear factor.\" (p. 268). However, this term must be used with care, as it may refer to other notions of reduction.\n\nIrreducible polynomials appear naturally in the study of polynomial factorization and algebraic field extensions.\n\nIt is helpful to compare irreducible polynomials to prime numbers: prime numbers (together with the corresponding negative numbers of equal magnitude) are the irreducible integers. They exhibit many of the general properties of the concept of \"irreducibility\" that equally apply to irreducible polynomials, such as the essentially unique factorization into prime or irreducible factors.\n\nDefinition\n\nIf F is a field, a non-constant polynomial is irreducible over F if its coefficients belong to F and it cannot be factored into the product of two non-constant polynomials with coefficients in F. \n\nA polynomial with integer coefficients, or, more generally, with coefficients in a unique factorization domain R, is sometimes said to be irreducible (or irreducible over R) if it is an irreducible element of the polynomial ring, that is, it is not invertible, not zero, and cannot be factored into the product of two non-invertible polynomials with coefficients in R. This definition generalizes the definition given for the case of coefficients in a field, because, over a field, the non-constant polynomials are exactly the polynomials that are non-invertible and non-zero.\n\nAnother definition is frequently used, saying that a polynomial is irreducible over R if it is irreducible over the field of fractions of R (the field of rational numbers, if R is the integers). This second definition is not used in this article.\n\nNature of a factor\n\nThe absence of an explicit algebraic expression for a factor does not by itself imply that a polynomial is irreducible. When a polynomial is reducible into factors, these factors may be explicit algebraic expressions or implicit expressions. For example,  can be factored explicitly over the complex numbers as  however, the Abel–Ruffini theorem states that there are polynomials of any degree greater than 4 for which complex factors exist that have no explicit algebraic expression. Such a factor can be written simply as, say,  where  is defined implicitly as a particular solution of the equation that sets the polynomial equal to 0. Further, factors of either type can also be expressed as numerical approximations obtainable by root-finding algorithms, for example as \n\n Simple examples \n\nThe following six polynomials demonstrate some elementary properties of reducible and irreducible polynomials:\n\nOver the integers, the first three polynomials are reducible (the third one is reducible because the factor 3 is not invertible in the integers); the last two are irreducible. (The fourth, of course, is not a polynomial over the integers.)\n\nOver the rational numbers, the first two and the fourth polynomials are reducible, but the other three polynomials are irreducible (as a polynomial over the rationals, 3 is a unit, and, therefore, does not count as a factor).\n\nOver the real numbers, the first five polynomials are reducible, but  is  irreducible.\n\nOver the complex numbers, all six polynomials are reducible.\n\nOver the complex numbers\n\nOver the complex field, and, more generally, over an algebraically closed field, a univariate polynomial is irreducible if and only if its degree is one. This fact is known as the fundamental theorem of algebra in the case of the complex numbers and, in general, as the condition of being algebraically closed.\n\nIt follows that every nonconstant univariate polynomial can be factored as\n\nwhere  is the degree,  is the leading coefficient and  are the zeros of the polynomial (not necessarily distinct, and not necessarily having explicit algebraic expressions).\n\nThere are irreducible multivariate polynomials of every degree over the complex numbers. For example, the polynomial \n \nwhich defines a Fermat curve, is irreducible for every positive n.\n\n Over the reals \n\nOver the field of reals, the degree of an irreducible univariate polynomial is either one or two. More precisely, the irreducible polynomials are the polynomials of degree one and the quadratic polynomials  that have a negative discriminant  \nIt follows that every non-constant univariate polynomial can be factored as a product of polynomials of degree at most two. For example,\n factors over the real numbers as  and it cannot be factored further, as both factors have a negative discriminant: \n\nUnique factorization property\n\nEvery polynomial over a field  may be factored into a product of a non-zero constant and a finite number of irreducible (over ) polynomials. This decomposition is unique up to the order of the factors and the multiplication of the factors by non-zero constants whose product is 1.\n\nOver a unique factorization domain  the same theorem is true, but is more accurately formulated by using the notion of primitive polynomial. A primitive polynomial is a polynomial over a unique factorization domain, such that 1 is a greatest common divisor of its coefficients. \n\nLet  be a unique factorization domain. A non-constant irreducible polynomial over  is primitive. A primitive polynomial over  is irreducible over  if and only if it is irreducible over the field of fractions of . Every polynomial over  may be decomposed into the product of a non-zero constant and a finite number of non-constant irreducible primitive polynomials. The non-zero constant may itself be decomposed into the product of a unit of  and a finite number of irreducible elements of .\nBoth factorizations are unique up to the order of the factors and the multiplication of the factors by a unit of .\n\nThis is this theorem which motivates that the definition of irreducible polynomial over a unique factorization domain often supposes that the polynomial is non-constant. \n\nAll algorithms which are presently implemented for factoring polynomials over the integers and over the rational numbers use this result (see Factorization of polynomials).\n\n Over the integers and finite field\nThe irreducibility of a polynomial over the integers  is related to that over the field  of  elements (for a prime ). In particular, if a univariate polynomial f over  is irreducible over  for some prime  that does not divide the leading coefficient of f (the coefficient of the highest power of the variable), then f is irreducible over . Eisenstein's criterion is a variant of this property where irreducibility over  is also involved.\n\nThe converse, however, is not true: there are polynomials of arbitrarily large degree that are irreducible over the integers and reducible over every finite field. A simple example of such a polynomial is \n\nThe relationship between irreducibility over the integers and irreducibility modulo p is deeper than the previous result: to date, all implemented algorithms for factorization and irreducibility over the integers and over the rational numbers use the factorization over finite fields as a subroutine.\n\nThe number of irreducible monic polynomials over a field  for prime  is given by the necklace counting function. For , such polynomials are commonly used to generate pseudorandom binary sequences. \n\nIn some sense, almost all polynomials with coefficients zero or one are irreducible over the integers. More precisely, if a version of the Riemann hypothesis for Dedekind zeta functions is assumed, the probability of being irreducible over the integers for a polynomial with random coefficients in  tends to one when the degree increases.\n\nAlgorithms\n\nThe unique factorization property of polynomials does not mean that the factorization of a given polynomial may always be computed. Even the irreducibility of a polynomial may not always be proved by a computation: there are fields over which no algorithm can exist for deciding the irreducibility of arbitrary polynomials. \n\nAlgorithms for factoring polynomials and deciding irreducibility are known and implemented in computer algebra systems for polynomials over the integers, the rational numbers, finite fields and finitely generated field extension of these fields. All these algorithms use the algorithms for factorization of polynomials over finite fields.\n\nField extension\n\nThe notions of irreducible polynomial and of algebraic field extension are strongly related, in the following way.\n\nLet x be an element of an extension L of a field K. This element is said to be algebraic if it is a root of a polynomial with coefficients in K. Among the polynomials of which x is a root, there is exactly one which is monic and of minimal degree, called the minimal polynomial of x. The minimal polynomial of an algebraic element x of L is irreducible, and is the unique monic irreducible polynomial of which x is a root. The minimal polynomial of x divides every polynomial which has x as a root (this is Abel's irreducibility theorem). \n\nConversely, if  is a univariate polynomial over a field K, let  be the quotient ring of the polynomial ring  by the ideal generated by . Then  is a field if and only if  is irreducible over . In this case, if  is the image of  in , the minimal polynomial of  is the quotient of  by its leading coefficient.\n\nAn example of the above is the standard definition of the complex numbers as \n\nIf a polynomial  has an irreducible factor  over , which has a degree greater than one, one may apply to  the preceding construction of an algebraic extension, to get an extension in which  has at least one more root than in . Iterating this construction, one gets eventually a field over which  factors into linear factors. This field, unique up to a field isomorphism, is called the splitting field of .\n\nOver an integral domain \nIf R is an integral domain, an element f of R that is neither zero nor a unit is called irreducible if there are no non-units g and h with f = gh. One can show that every prime element is irreducible;Consider p a prime that is reducible: p = ab. Then p | ab ⇒ p | a or p | b. Say p | a ⇒ a = pc, then we have: p = ab = pcb ⇒ p(1 − cb) = 0. Because R is a domain, we have cb = 1. So b is a unit, and p is irreducible. the converse is not true in general but holds in unique factorization domains. The polynomial ring F[x] over a field F (or any unique-factorization domain) is again a unique factorization domain. Inductively, this means that the polynomial ring in n indeterminants (over a ring R) is a unique factorization domain if the same is true for R.\n\n See also \n Gauss's lemma (polynomial)\n Rational root theorem, a method of finding whether a polynomial has a linear factor with rational coefficients\n Eisenstein's criterion\n Perron method\n Hilbert's irreducibility theorem\n Cohn's irreducibility criterion\n Irreducible component of a topological space\n Factorization of polynomials over finite fields\n \n \n Casus irreducibilis, the irreducible cubic with three real roots\n \n\n Notes \n\n References \n . This classical book covers most of the content of this article.\n \n , pp. 91.\n \n , pp. 154.\n\n External links \n \n \n Information on Primitive and Irreducible Polynomials, The (Combinatorial) Object Server.\n\nCategory:Polynomials\nCategory:Abstract algebra\nCategory:Algebra"
    },
    {
      "title": "K-theory (physics)",
      "url": "https://en.wikipedia.org/wiki/K-theory_%28physics%29",
      "text": "In string theory, K-theory classification refers to a conjectured application of K-theory (in abstract algebra and algebraic topology) to superstrings, to classify the allowed Ramond–Ramond field strengths as well as the charges of stable D-branes.\n\nIn condensed matter physics K-theory has also found important applications, specially in the topological classification of topological insulators, superconductors and stable Fermi surfaces (, ).\n\n History \n\nThis conjecture, applied to D-brane charges, was first proposed by .  It was popularized by  who demonstrated that in type IIB string theory arises naturally from Ashoke Sen's realization of arbitrary D-brane configurations as stacks of D9 and anti-D9-branes after tachyon condensation.\n\nSuch stacks of branes are inconsistent in a non-torsion Neveu–Schwarz (NS) 3-form background, which, as was highlighted by , complicates the extension of the K-theory classification to such cases.  suggested a solution to this problem: D-branes are in general classified by a twisted K-theory, that had earlier been defined by .\n\n Applications \n\nThe K-theory classification of D-branes has had numerous applications.  For example,  used it to argue that there are eight species of orientifold one-plane.  applied the K-theory classification to derive new consistency conditions for flux compactifications.  K-theory has also been used to conjecture a formula for the topologies of T-dual manifolds by .  Recently K-theory has been conjectured to classify the spinors in compactifications on generalized complex manifolds.\n\n Open problems \n\nDespite these successes, RR fluxes are not quite classified by K-theory.   argued that the K-theory classification is incompatible with S-duality in IIB string theory.\n\nIn addition, if one attempts to classify fluxes on a compact ten-dimensional spacetime, then a complication arises due to the self-duality of the RR fluxes.  The duality uses the Hodge star, which depends on the metric and so is continuously valued and in particular is generically irrational.  Thus not all of the RR fluxes, which are interpreted as the Chern characters in K-theory, can be rational.  However Chern characters are always rational, and so the K-theory classification must be replaced.  One needs to choose a half of the fluxes to quantize, or a polarization in the geometric quantization-inspired language of Diaconescu, Moore, and Witten and later of . Alternately one may use the K-theory of a 9-dimensional time slice as has been done by .\n\nK-theory classification of RR fluxes\n\nIn the classical limit of type II string theory, which is type II supergravity, the Ramond–Ramond field strengths are differential forms.  In the quantum theory the well-definedness of the partition functions of D-branes implies that the RR field strengths obey Dirac quantization conditions when spacetime is compact, or when a spatial slice is compact and one considers only the (magnetic) components of the field strength which lie along the spatial directions.  This led twentieth century physicists to classify RR field strengths using cohomology with integral coefficients.\n\nHowever some authors have argued that the cohomology of spacetime with integral coefficients is too big.  For example, in the presence of Neveu–Schwarz H-flux or non-spin cycles some RR fluxes dictate the presence of D-branes.  In the former case this is a consequence of the supergravity equation of motion which states that the product of a RR flux with the NS 3-form is a D-brane charge density.  Thus the set of topologically distinct RR field strengths that can exist in brane-free configurations is only a subset of the cohomology with integral coefficients.\n\nThis subset is still too big, because some of these classes are related by large gauge transformations.  In QED there are large gauge transformations which add integral multiples of two pi to Wilson loops.  The p-form potentials in type II supergravity theories also enjoy these large gauge transformations, but due to the presence of Chern-Simons terms in the supergravity actions these large gauge transformations transform not only the p-form potentials but also simultaneously the (p+3)-form field strengths.  Thus to obtain the space of inequivalent field strengths from the forementioned subset of integral cohomology we must quotient by these large gauge transformations.\n\nThe Atiyah–Hirzebruch spectral sequence constructs twisted K-theory, with a twist given by the NS 3-form field strength, as a quotient of a subset of the cohomology with integral coefficients.  In the classical limit, which corresponds to working with rational coefficients, this is precisely the quotient of a subset described above in supergravity.  The quantum corrections come from torsion classes and contain mod 2 torsion corrections due to the Freed-Witten anomaly.\n\nThus twisted K-theory classifies the subset of RR field strengths that can exist in the absence of D-branes quotiented by large gauge transformations. Daniel Freed has attempted to extend this classification to include also the RR potentials using differential K-theory.\n\nK-theory classification of D-branes\n\nK-theory classifies D-branes in noncompact spacetimes, intuitively in spacetimes in which we are not concerned about the flux sourced by the brane having nowhere to go.  While the K-theory of a 10d spacetime classifies D-branes as subsets of that spacetime, if the spacetime is the product of time and a fixed 9-manifold then K-theory also classifies the conserved D-brane charges on each 9-dimensional spatial slice.  While we were required to forget about RR potentials to obtain the K-theory classification of RR field strengths, we are required to forget about RR field strengths to obtain the K-theory classification of D-branes.\n\n K-theory charge versus BPS charge \n\nAs has been stressed by Petr Hořava, the K-theory classification of D-branes is independent of, and in some ways stronger than, the classification of BPS states.  K-theory appears to classify stable D-branes missed by supersymmetry based classifications.\n\nFor example, D-branes with torsion charges, that is with charges in the order N cyclic group , attract each other and so can never be BPS.  In fact, N such branes can decay, whereas no superposition of branes that satisfy a Bogomolny bound may ever decay.  However the charge of such branes is conserved modulo N, and this is captured by the K-theory classification but not by a BPS classification.  Such torsion branes have been applied, for example, to model Douglas-Shenker strings in supersymmetric U(N) gauge theories.\n\nK-theory from tachyon condensation\n\nAshoke Sen has conjectured that, in the absence of a topologically nontrivial NS 3-form flux, all IIB brane configurations can be obtained from stacks of spacefilling D9 and anti D9 branes via tachyon condensation.  The topology of the resulting branes is encoded in the topology of the gauge bundle on the stack of the spacefilling branes.  The topology of the gauge bundle of a stack of D9s and anti D9s can be decomposed into a gauge bundle on the D9's and another bundle on the anti D9's.  Tachyon condensation transforms such a pair of bundles to another pair in which the same bundle is direct summed with each component in the pair.  Thus the tachyon condensation invariant quantity, that is, the charge which is conserved by the tachyon condensation process, is not a pair of bundles but rather the equivalence class of a pair of bundles under direct sums of the same bundle on both sides of the pair.  This is precisely the usual construction of topological K-theory.  Thus the gauge bundles on stacks of D9's and anti-D9's are classified by topological K-theory.  If Sen's conjecture is right, all D-brane configurations in type IIB are then classified by K-theory.  Petr Horava has extended this conjecture to type IIA using D8-branes.\n\nTwisted K-theory from MMS instantons\n\nWhile the tachyon condensation picture of the K-theory classification classifies D-branes as subsets of a 10-dimensional spacetime with no NS 3-form flux, the Maldacena, Moore, Seiberg picture classifies stable D-branes with finite mass as subsets of a 9-dimensional spatial slice of spacetime.\n\nThe central observation is that D-branes are not classified by integral homology because Dp-branes wrapping certain cycles suffer from a Freed-Witten anomaly, which is cancelled by the insertion of D(p-2)-branes and sometimes D(p-4)-branes that end on the afflicted Dp-brane.  These inserted branes may either continue to infinity, in which case the composite object has an infinite mass, or else they may end on an anti-Dp-brane, in which case the total Dp-brane charge is zero.  In either case, one may wish to remove the anomalous Dp-branes from the spectrum, leaving only a subset of the original integral cohomology.\n\nThe inserted branes are unstable.  To see this, imagine that they extend in time away (into the past) from the anomalous brane.  This corresponds to a process in which the inserted branes decay via a Dp-brane that forms, wraps the forementioned cycle and then disappears.  MMSJuan Maldacena, Gregory Moore and Nathan Seiberg. D-Brane Instantons and K-Theory Charges. https://arxiv.org/abs/hep-th/0108100 refer to this process as an instanton, although really it need not be instantonic.\n\nThe conserved charges are thus the nonanomolous subset quotiented by the unstable insertions.  This is precisely the Atiyah-Hirzebruch spectral sequence construction of twisted K-theory as a set.\n\nReconciling twisted K-theory and S-duality\n\nDiaconescu, Moore, and Witten have pointed out that the twisted K-theory classification is not compatible with the S-duality covariance of type IIB string theory.  For example, consider the constraint on the Ramond–Ramond 3-form field strength G3 in the Atiyah-Hirzebruch spectral sequence (AHSS):\n\nwhere d3=Sq3+H is the first nontrivial differential in the AHSS, Sq3 is the third Steenrod square and the last equality follows from the fact that the nth Steenrod square acting on any n-form x is xx.\n\nThe above equation is not invariant under S-duality, which exchanges G3 and H.  Instead Diaconescu, Moore, and Witten have proposed the following S-duality covariant extension\n\nwhere P is an unknown characteristic class that depends only on the topology, and in particular not on the fluxes.  have found a constraint on P using the E8 gauge theory approach to M-theory pioneered by Diaconescu, Moore, and Witten.\n\nThus D-branes in IIB are not classified by twisted K-theory after all, but some unknown S-duality-covariant object that inevitably also classifies both fundamental strings and NS5-branes.\n\nHowever the MMS prescription for calculating twisted K-theory is easily S-covariantized, as the Freed-Witten anomalies respect S-duality.  Thus the S-covariantized form of the MMS construction may be applied to construct the S-covariantized twisted K-theory, as a set, without knowing having any geometric description for just what this strange covariant object is.  This program has been carried out in a number of papers, such as  and , and was also applied to the classification of fluxes by .   use this approach to prove Diaconescu, Moore, and Witten's conjectured constraint on the 3-fluxes, and they show that there is an additional term equal to the D3-brane charge.  shows that the Klebanov-Strassler cascade of Seiberg dualities consists of a series of S-dual MMS instantons, one for each Seiberg duality.  The group,  of universality classes of the  supersymmetric gauge theory is then shown to agree with the S-dual twisted K-theory and not with the original twisted K-theory.\n\nSome authors have proposed radically different solutions to this puzzle.  For example,  propose that instead of twisted K-theory, II string theory configurations should be classified by elliptic cohomology.\n\nResearchers\nProminent researchers in this area include Edward Witten, Peter Bouwknegt, Angel Uranga, Emanuel Diaconescu, Gregory Moore, Anton Kapustin, Jonathan Rosenberg, Ruben Minasian, Amihay Hanany, Hisham Sati, Nathan Seiberg, Juan Maldacena, Daniel Freed, and Igor Kriz.\n\nSee also\nKalb–Ramond field\n\nNotes\n\nReferences\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\n.\n\nReferences (condensed matter physics)\n.\n\n.\n.\n\nFurther reading\nAn excellent introduction to the K-theory classification of D-branes in 10 dimensions via Ashoke Sen's conjecture is the original paper \"D-branes and K-theory\" by Edward Witten; there is also an extensive review by .\n\nA very comprehensible introduction to the twisted K-theory classification of conserved D-brane charges on a 9-dimensional timeslice in the presence of Neveu–Schwarz flux is .\n\nExternal links\nK-theory on arxiv.org\n\nCategory:String theory\nCategory:Algebra\nCategory:K-theory"
    }
  ]
}