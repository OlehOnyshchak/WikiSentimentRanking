{
  "pages": [
    {
      "title": "Curse of dimensionality",
      "url": "https://en.wikipedia.org/wiki/Curse_of_dimensionality",
      "text": "'''The curse of dimensionality''' refers to various phenomena that arise when analyzing and organizing data in [[high-dimensional space]]s (often with hundreds or thousands of dimensions) that do not occur in low-dimensional settings such as the [[three-dimensional space|three-dimensional]] [[physical space]] of everyday experience.\nThe expression was coined by [[Richard E. Bellman]] when considering problems in [[dynamic programming]].<ref>{{Cite book|author1=Richard Ernest Bellman|author2=Rand Corporation|title=Dynamic programming|url=https://books.google.it/books?id=wdtoPwAACAAJ|year=1957|publisher=Princeton University Press|isbn=978-0-691-07951-6}},<br />Republished: {{Cite book|author=Richard Ernest Bellman|title=Dynamic Programming|url=https://books.google.com/books?id=fyVtp3EMxasC|year=2003|publisher=Courier Dover Publications|isbn=978-0-486-42809-3}}</ref><ref>{{Cite book|author=Richard Ernest Bellman|title=Adaptive control processes: a guided tour|url=https://books.google.com/books?id=POAmAAAAMAAJ|year=1961|publisher=Princeton University Press}}</ref>\n\nCursed phenomena occur in domains such as [[numerical analysis]], [[Sampling (statistics)|sampling]], [[combinatorics]], [[machine learning]], [[data mining]] and [[database]]s. The common theme of these problems is that when the dimensionality increases, the [[volume]] of the space increases so fast that the available data become sparse. This sparsity is problematic for any method that requires [[statistical significance]]. In order to obtain a statistically sound and reliable result, the amount of data needed to support the result often grows exponentially with the dimensionality. Also, organizing and searching data often relies on detecting areas where objects form groups with similar properties; in high dimensional data, however, all objects appear to be sparse and dissimilar in many ways, which prevents common data organization strategies from being efficient.\n\n{{toclimit|3}}\n== Domains ==\n\n=== Combinatorics ===\nIn some problems, each variable can take one of several discrete values, or the range of possible values is divided to give a finite number of possibilities. Taking the variables together, a huge number of combinations of values must be considered. This effect is also known as the [[combinatorial explosion]]. Even in the simplest case of <math>d</math> binary variables, the number of possible combinations already is <math>O(2^d)</math>, exponential in the dimensionality. Naively, each additional dimension doubles the effort needed to try all combinations.\n\n=== Sampling ===\nThere is an exponential increase in volume associated with adding extra dimensions to a [[Space (mathematics)|mathematical space]]. For example, 10<sup>2</sup>=100 evenly spaced sample points suffice to sample a [[unit interval]] (a \"1-dimensional cube\") with no more than 10<sup>−2</sup>=0.01 distance between points; an equivalent sampling of a 10-dimensional [[unit hypercube]] with a lattice that has a spacing of 10<sup>−2</sup>=0.01 between adjacent points would require 10<sup>20</sup>[=(10<sup>2</sup>)<sup>10</sup>] sample points. In general, with a spacing distance of 10<sup>−n</sup> the 10-dimensional hypercube appears to be a factor of 10<sup>n(10-1)</sup>[=(10<sup>n</sup>)<sup>10</sup>/(10<sup>n</sup>)] \"larger\" than the 1-dimensional hypercube, which is the unit interval. In the above example n=2: when using a sampling distance of 0.01 the 10-dimensional hypercube appears to be 10<sup>18</sup> \"larger\" than the unit interval. This effect is a combination of the combinatorics problems above and the distance function problems explained below.\n\n=== Optimization ===\nWhen solving dynamic [[optimization (mathematics)|optimization]] problems by numerical [[backward induction]], the objective function must be computed for each combination of values. This is a significant obstacle when the dimension of the \"state variable\" is large.\n\n=== Machine learning ===\nIn [[machine learning]] problems that involve learning a \"state-of-nature\" from a finite number of data samples in a high-dimensional [[feature space]] with each feature having a range of possible values, typically an enormous amount of training data is required to ensure that there are several samples with each combination of values. A typical rule of thumb is that there should be at least 5 training examples for each dimension in the representation.<ref name=\"Pattern recog\" >{{cite book|last1=Koutroumbas|first1=Sergios Theodoridis, Konstantinos|title=Pattern Recognition - 4th Edition|date=2008|location=Burlington|url=https://www.elsevier.com/books/pattern-recognition/theodoridis/978-1-59749-272-0|accessdate=8 January 2018|language=en}}</ref> With a fixed number of training samples, the predictive power of a classifier or regressor first increases as number of dimensions/features used is increased but then decreases,<ref>{{cite journal|last1=Trunk|first1=G. V.|title=A Problem of Dimensionality: A Simple Example|journal=IEEE Transactions on Pattern Analysis and Machine Intelligence|date=July 1979|volume=PAMI-1|issue=3|pages=306–307|doi=10.1109/TPAMI.1979.4766926}}</ref> which is known as ''Hughes phenomenon''<ref>{{cite journal |last=Hughes |first=G.F. |title=On the mean accuracy of statistical pattern recognizers |journal=IEEE Transactions on Information Theory |volume=14 |issue=1 |pages=55–63 |date=January 1968 |doi=10.1109/TIT.1968.1054102 }}</ref> or ''peaking phenomena''.<ref name=\"Pattern recog\"/>\n\n=== Distance functions ===\nWhen a measure such as a [[Euclidean distance]] is defined using many coordinates, there is little difference in the distances between different pairs of samples.\n\nOne way to illustrate the \"vastness\" of high-dimensional Euclidean space is to compare the proportion of an inscribed [[hypersphere]] with radius <math>r</math> and dimension <math>d</math>, to that of a [[hypercube]] with edges of length <math>2r</math>.\nThe volume of such a sphere is <math>\\frac{2r^d\\pi^{d/2}}{d \\; \\Gamma(d/2)}</math>, where [[Gamma function|<math>\\Gamma</math>]] is the [[gamma function]], while the volume of the cube is <math>(2r)^d</math>.\nAs the dimension <math>d</math> of the space increases, the hypersphere becomes an insignificant volume relative to that of the hypercube. This can clearly be [[:commons:File:Ball-cube-volume-ratio-semilog.png|seen]] by comparing the proportions as the dimension <math>d</math> goes to infinity:\n:<math>\\frac{V_\\mathrm{hypersphere}}{V_\\mathrm{hypercube}}=\\frac{\\pi^{d/2}}{d2^{d-1}\\Gamma(d/2)}\\rightarrow 0</math> as <math>d \\rightarrow \\infty</math>. \n\nFurthermore, the distance between the center and the corners is <math>r\\sqrt{d}</math>, which increases without bound for fixed r.\nIn this sense, nearly all of the high-dimensional space is \"far away\" from the centre. To put it another way, the high-dimensional unit hypercube can be said to consist almost entirely of the \"corners\" of the hypercube, with almost no \"middle\".\n\nThis also helps to understand the [[chi-squared distribution]]. Indeed, the (non-central) chi-squared distribution associated to a random point in the interval [-1, 1] is the same as the distribution of the length-squared of a random point in the ''d''-cube. By the law of large numbers, this distribution concentrates itself in a narrow band around ''d'' times the standard deviation squared (σ<sup>2</sup>) of the original derivation. This illuminates the chi-squared distribution and also illustrates that most of the volume of the ''d''-cube concentrates near the surface of a sphere of radius {{sqrt|''d''}}''σ''.\n\nA further development of this phenomenon is as follows. Any fixed distribution on ''[[Real number|ℝ]]'' induces a product distribution on points in ''ℝ<sup>d</sup>''. For any fixed ''n'', it turns out that the minimum and the maximum distance between a random reference point ''Q'' and a list of ''n'' random data points ''P<sub>1</sub>,...,P<sub>n</sub>'' become indiscernible compared to the minimum distance:<ref>{{Cite book | doi = 10.1007/3-540-49257-7_15 | title = When is \"Nearest Neighbor\" Meaningful? | year = 1999 | last1 = Beyer | first1 = K. | last2 = Goldstein | first2 = J. | series = LNCS | last3 = Ramakrishnan | first3 = R. | last4 = Shaft | first4 = U. | volume = 1540 | journal = Proc. 7th International Conference on Database Theory - ICDT'99| pages = 217–235| isbn = 978-3-540-65452-0}}</ref>\n:<math>\\lim_{d \\to \\infty} E\\left(\\frac{\\operatorname{dist}_{\\max} (d) - \\operatorname{dist}_{\\min} (d)}{\\operatorname{dist}_{\\min} (d)}\\right) \n\\to 0</math>.\nThis is often cited as distance functions losing their usefulness (for the nearest-neighbor criterion in feature-comparison algorithms, for example) in high dimensions. However, recent research has shown this to only hold in the artificial scenario when the one-dimensional distributions ''ℝ'' are [[Independent and identically distributed random variables|independent and identically distributed]].<ref name=\"survey\" /> When attributes are correlated, data can become easier and provide higher distance contrast and the [[signal-to-noise ratio]] was found to play an important role, thus [[feature selection]] should be used.<ref name=\"survey\" />\n\n=== Nearest neighbor search ===\nThe effect complicates [[nearest neighbor search]] in high dimensional space. It is not possible to quickly reject candidates by using the difference in one coordinate as a lower bound for a distance based on all the dimensions.<ref>{{cite journal |first1=R.B. |last1=Marimont |first2=M.B. |last2=Shapiro |title=Nearest Neighbour Searches and the Curse of Dimensionality |journal=IMA J Appl Math |volume=24 |issue=1 |pages=59–70 |year=1979 |doi=10.1093/imamat/24.1.59 |url=http://imamat.oxfordjournals.org/content/24/1/59.short}}</ref><ref>{{cite journal |first1=Edgar |last1=Chávez |first2=Gonzalo |last2=Navarro |first3=Ricardo |last3=Baeza-Yates |first4=José Luis |last4=Marroquín |title=Searching in Metric Spaces |journal=ACM Computing Surveys |volume=33 |issue=3 |pages=273–321 |year=2001 |doi=10.1145/502807.502808 |citeseerx = 10.1.1.100.7845 }}</ref>\n\nHowever, it has recently been observed that the mere number of dimensions does not necessarily result in difficulties,<ref name=\"houle-ssdbm10\">{{Cite conference | last1 = Houle | first1 = M. E. | last2 = Kriegel | first2 = H. P. | authorlink2=Hans-Peter Kriegel | last3 = Kröger | first3 = P.| last4 = Schubert | first4 = E. | last5 = Zimek | first5 = A.| title = Can Shared-Neighbor Distances Defeat the Curse of Dimensionality? | doi = 10.1007/978-3-642-13818-8_34 | conference = Scientific and Statistical Database Management | series = Lecture Notes in Computer Science | volume = 6187 | pages = 482 | year = 2010 | isbn = 978-3-642-13817-1 | pmid =  | pmc = | url = http://www.dbs.ifi.lmu.de/~zimek/publications/SSDBM2010/SNN-SSDBM2010-preprint.pdf| format = PDF}}</ref> since ''relevant'' additional dimensions can also increase the contrast. In addition, for the resulting ranking it remains useful to discern close and far neighbors. Irrelevant (\"noise\") dimensions, however, reduce the contrast in the manner described above. In [[time series analysis]], where the data are inherently high-dimensional, distance functions also work reliably as long as the [[signal-to-noise ratio]] is high enough.<ref name=\"houle-sstd11\">{{Cite conference | last1 = Bernecker | first1 = T. | last2 = Houle | first2 = M. E. | last3 = Kriegel | first3 = H. P. | authorlink3=Hans-Peter Kriegel| last4 = Kröger | first4 = P. | last5 = Renz | first5 = M. | last6 = Schubert | first6 = E. | last7 = Zimek | first7 = A. | title = Quality of Similarity Rankings in Time Series | doi = 10.1007/978-3-642-22922-0_25 | conference = Symposium on Spatial and Temporal Databases| series = Lecture Notes in Computer Science | volume = 6849 | pages = 422 | year = 2011 | isbn = 978-3-642-22921-3 }}</ref>\n\n====''k''-nearest neighbor classification====\nAnother effect of high dimensionality on distance functions concerns ''k''-nearest neighbor (''k''-NN) [[Graph (discrete mathematics)|graphs]] constructed from a [[data set]] using a distance function. As the dimension increases, the [[indegree]] distribution of the ''k''-NN [[directed graph|digraph]] becomes [[Skewness|skewed]] with a peak on the right because of the emergence of a disproportionate number of '''hubs''', that is, data-points that appear in many more ''k''-NN lists of other data-points than the average. This phenomenon can have a considerable impact on various techniques for [[Classification (machine learning)|classification]] (including the [[K-nearest neighbor algorithm|''k''-NN classifier]]), [[semi-supervised learning]], and [[Cluster analysis|clustering]],<ref>{{Cite journal\n | last1=Radovanović | first1=Miloš\n | last2=Nanopoulos | first2=Alexandros\n | last3=Ivanović | first3=Mirjana\n | year=2010\n | title=Hubs in space: Popular nearest neighbors in high-dimensional data\n | journal=Journal of Machine Learning Research\n | volume=11\n | pages=2487–2531\n | url=http://www.jmlr.org/papers/volume11/radovanovic10a/radovanovic10a.pdf\n }}</ref> and it also affects [[information retrieval]].<ref>{{Cite conference| last1 = Radovanović | first1 = M. | last2 = Nanopoulos | first2 = A. | last3 = Ivanović | first3 = M. | doi = 10.1145/1835449.1835482 | title = On the existence of obstinate results in vector space models | conference = 33rd international ACM SIGIR conference on Research and development in information retrieval - SIGIR '10 | pages = 186 | year = 2010 | isbn = 9781450301534 }}</ref>\n\n=== Anomaly detection ===\n\nIn a recent survey, Zimek et al. identified the following problems when searching for [[anomaly detection|anomalies]] in high-dimensional data:<ref name=\"survey\">{{Cite journal | last1 = Zimek | first1 = A. | last2 = Schubert | first2 = E.| last3 = Kriegel | first3 = H.-P. | authorlink3=Hans-Peter Kriegel| title = A survey on unsupervised outlier detection in high-dimensional numerical data | doi = 10.1002/sam.11161 | journal = Statistical Analysis and Data Mining | volume = 5 | issue = 5 | pages = 363–387| year = 2012 | pmid =  | pmc = }}</ref>\n\n# Concentration of scores and distances: derived values such as distances become numerically similar\n# Irrelevant attributes: in high dimensional data, a significant number of attributes may be irrelevant\n# Definition of reference sets: for local methods, reference sets are often nearest-neighbor based\n# Incomparable scores for different dimensionalities: different subspaces produce incomparable scores\n# Interpretability of scores: the scores often no longer convey a semantic meaning\n# Exponential search space: the search space can no longer be systematically scanned\n# [[Data snooping]] bias: given the large search space, for every desired significance a hypothesis can be found\n# Hubness: certain objects occur more frequently in neighbor lists than others.\n\nMany of the analyzed specialized methods tackle one or another of these problems, but there remain many open research questions.\n\n==See also==\n{{Div col|colwidth=22em}}\n*[[Bellman equation]]\n*[[Clustering high-dimensional data]]\n*[[Concentration of measure]]\n*[[Dimension reduction]]\n*[[Model Order Reduction]]\n*[[Dynamic programming]]\n*[[Fourier-related transforms]]\n*[[Linear least squares (mathematics)|Linear least squares]]\n*[[Multilinear principal component analysis|Multilinear PCA]]\n*[[Multilinear subspace learning]]\n*[[Principal component analysis]]\n*[[Singular value decomposition]]\n{{div col end}}\n\n==References==\n{{Reflist|30em}}\n\n[[Category:Numerical analysis]]\n[[Category:Dynamic programming]]\n[[Category:Machine learning]]\n[[Category:Dimension]]"
    },
    {
      "title": "Curve fitting",
      "url": "https://en.wikipedia.org/wiki/Curve_fitting",
      "text": "{{Redirect|Best fit|placing (\"fitting\") variable-sized objects in storage|Fragmentation (computer)}}\n[[File:Regression pic assymetrique.gif|thumb|upright=1.5|Fitting of a noisy curve by an asymmetrical peak model, with an iterative process ([[Gauss–Newton algorithm]] with variable damping factor α). <br /> Top: raw data and model.<br /> Bottom: evolution of the normalised sum of the squares of the errors.]]\n{{Order-of-approx}}\n\n'''Curve fitting'''<ref>Sandra Lach Arlinghaus, PHB Practical Handbook of Curve Fitting. CRC Press, 1994.</ref><ref>William M. Kolb. [https://books.google.com/books?id=ZiLYAAAAMAAJ&dq=%22Curve+Fitting+for+Programmable+Calculators%22+kolb&focus=searchwithinvolume&q=%22Curve+fitting%22 Curve Fitting for Programmable Calculators]. Syntec, Incorporated, 1984.</ref> is the process of constructing a [[curve]], or [[function (mathematics)|mathematical function]], that has the best fit to a series of [[data points]],<ref>S.S. Halli, K.V. Rao. 1992. Advanced Techniques of Population Analysis. {{ISBN|0306439972}} Page 165 (''cf''. ... functions are fulfilled if we have a good to moderate fit for the observed data.)</ref> possibly subject to constraints.<ref>[https://books.google.com/books?id=SI-VqAT4_hYC ''[[The Signal and the Noise]]]: Why So Many Predictions Fail-but Some Don't.'' By Nate Silver</ref><ref>[https://books.google.com/books?id=hhdVr9F-JfAC Data Preparation for Data Mining]: Text. By Dorian Pyle.</ref> Curve fitting can involve either [[interpolation]],<ref>Numerical Methods in Engineering with MATLAB®. By [[Jaan Kiusalaas]]. Page 24.</ref><ref>[https://books.google.com/books?id=YlkgAwAAQBAJ&printsec=frontcover#v=onepage&q=%22curve%20fitting%22&f=false Numerical Methods in Engineering with Python 3]. By Jaan Kiusalaas. Page 21.</ref> where an exact fit to the data is required, or [[smoothing]],<ref>[https://books.google.com/books?id=UjnB0FIWv_AC&printsec=frontcover#v=onepage&q=smoothing&f=false Numerical Methods of Curve Fitting]. By P. G. Guest, Philip George Guest. Page 349.</ref><ref>See also: [[Mollifier]]</ref> in which a \"smooth\" function is constructed that approximately fits the data.  A related topic is [[regression analysis]],<ref>[https://books.google.com/books?id=g1FO9pquF3kC&printsec=frontcover#v=snippet&q=%22regression%20analysis%22&f=false Fitting Models to Biological Data Using Linear and Nonlinear Regression]. By Harvey Motulsky, Arthur Christopoulos.</ref><ref>[https://books.google.com/books?id=Us4YE8lJVYMC&printsec=frontcover#v=onepage&q=%22regression%20analysis%22&f=false Regression Analysis] By Rudolf J. Freund, William J. Wilson, Ping Sa. Page 269.</ref> which focuses more on questions of [[statistical inference]] such as how much uncertainty is present in a curve that is fit to data observed with random errors. Fitted curves can be used as an aid for data visualization,<ref>Visual Informatics. Edited by Halimah Badioze Zaman, Peter Robinson, Maria Petrou, Patrick Olivier, Heiko Schröder. Page 689.</ref><ref>[https://books.google.com/books?id=rdJvXG1k3HsC&printsec=frontcover#v=onepage&q=%22Curve%20fitting%22&f=false Numerical Methods for Nonlinear Engineering Models]. By John R. Hauser. Page 227.</ref> to infer values of a function where no data are available,<ref>Methods of Experimental Physics: Spectroscopy, Volume 13, Part 1. By Claire Marton. Page 150.</ref> and to summarize the relationships among two or more variables.<ref>Encyclopedia of Research Design, Volume 1. Edited by Neil J. Salkind. Page 266.</ref> [[Extrapolation]] refers to the use of a fitted curve beyond the [[range (mathematics)|range]] of the observed data,<ref>[https://books.google.com/books?id=ba0hAQAAQBAJ&printsec=frontcover#v=snippet&q=%22Curve%20fitting%22%20OR%20extrapolation&f=false Community Analysis and Planning Techniques]. By Richard E. Klosterman. Page 1.</ref> and is subject to a [[Uncertainty|degree of uncertainty]]<ref>An Introduction to Risk and Uncertainty in the Evaluation of Environmental Investments. DIANE Publishing. [https://books.google.com/books?id=rJ23LWaZAqsC&pg=PA69 Pg 69]</ref> since it may reflect the method used to construct the curve as much as it reflects the observed data.\n\n== Types ==\n\n===Fitting functions to data points{{anchor|Functions}}===\nMost commonly, one fits a function of the form {{math|''y''{{=}}''f''(''x'')}}.\n\n====Fitting lines and polynomial functions to data points{{anchor|Polynomials}}====\n{{main|Polynomial regression}}\n{{see also|Polynomial interpolation}}\n[[File:Curve fitting.svg|alt=Polynomial curves fitting a sine function|thumb|upright=1.3|Polynomial curves fitting points generated with a sine function. <br />\nRed line is a <span style=\"color:red\">first degree polynomial</span>, green line is <span style=\"color:green\">second degree</span>, orange line is <span style=\"color:orange\">third degree</span> and blue is <span style=\"color:blue\">fourth degree</span>]]\nThe first degree [[polynomial]] equation\n\n:<math>y = ax + b\\;</math>\n\nis a line with [[slope]] ''a''.  A line will connect any two points, so a first degree polynomial equation is an exact fit through any two points with distinct x coordinates.\n\nIf the order of the equation is increased to a second degree polynomial, the following results:\n\n:<math>y = ax^2 + bx + c\\;.</math>\n\nThis will exactly fit a simple curve to three points.\n\nIf the order of the equation is increased to a third degree polynomial, the following is obtained:\n\n:<math>y = ax^3 + bx^2 + cx + d\\;.</math>\n\nThis will exactly fit four points.\n\nA more general statement would be to say it will exactly fit four '''constraints'''.  Each constraint can be a point, [[angle]], or [[curvature]] (which is the reciprocal of the radius of an [[osculating circle]]).  Angle and curvature constraints are most often added to the ends of a curve, and in such cases are called '''end conditions'''.  Identical end conditions are frequently used to ensure a smooth transition between polynomial curves contained within a single [[spline (mathematics)|spline]].  Higher-order constraints, such as \"the change in the rate of curvature\", could also be added.  This, for example, would be useful in highway [[Cloverleaf interchange|cloverleaf]] design to understand the rate of change of the forces applied to a car (see [[Jerk (physics)|jerk]]), as it follows the cloverleaf, and to set reasonable speed limits, accordingly.\n\nThe first degree polynomial equation could also be an exact fit for a single point and an angle while the third degree polynomial equation could also be an exact fit for two points, an angle constraint, and a curvature constraint.  Many other combinations of constraints are possible for these and for higher order polynomial equations.\n\nIf there are more than ''n''&nbsp;+&nbsp;1 constraints (''n'' being the degree of the polynomial), the polynomial curve can still be run through those constraints. An exact fit to all constraints is not certain (but might happen, for example, in the case of a first degree polynomial exactly fitting three [[collinear points]]). In general, however, some method is then needed to evaluate each approximation. The [[least squares]] method is one way to compare the deviations.\n\nThere are several reasons given to get an approximate fit when it is possible to simply increase the degree of the polynomial equation and get an exact match.:\n\n* Even if an exact match exists, it does not necessarily follow that it can be readily discovered. Depending on the algorithm used there may be a divergent case, where the exact fit cannot be calculated, or it might take too much computer time to find the solution. This situation might require an approximate solution.\n* The effect of averaging out questionable data points in a sample, rather than distorting the curve to fit them exactly, may be desirable.\n* [[Runge's phenomenon]]: high order polynomials can be highly oscillatory. If a curve runs through two points ''A'' and ''B'', it would be expected that the curve would run somewhat near the midpoint of ''A'' and ''B'', as well.  This may not happen with high-order polynomial curves; they may even have values that are very large in positive or negative [[magnitude (mathematics)|magnitude]].  With low-order polynomials, the curve is more likely to fall near the midpoint (it's even guaranteed to exactly run through the midpoint on a first degree polynomial).\n* Low-order polynomials tend to be smooth and high order polynomial curves tend to be \"lumpy\".  To define this more precisely, the maximum number of [[inflection point]]s possible in a polynomial curve is ''n-2'', where ''n'' is the order of the polynomial equation.  An inflection point is a location on the curve where it switches from a positive radius to negative.  We can also say this is where it transitions from \"holding water\" to \"shedding water\".  Note that it is only \"possible\" that high order polynomials will be lumpy; they could also be smooth, but there is no guarantee of this, unlike with low order polynomial curves.  A fifteenth degree polynomial could have, at most, thirteen inflection points, but could also have twelve, eleven, or any number down to zero.\n\nThe degree of the polynomial curve being higher than needed for an exact fit is undesirable for all the reasons listed previously for high order polynomials, but also leads to a case where there are an infinite number of solutions.  For example, a first degree polynomial (a line) constrained by only a single point, instead of the usual two, would give an infinite number of solutions.  This brings up the problem of how to compare and choose just one solution, which can be a problem for software and for humans, as well.  For this reason, it is usually best to choose as low a degree as possible for an exact match on all constraints, and perhaps an even lower degree, if an approximate fit is acceptable.\n\n[[File:Gohana inverted S-curve.png|thumb|upright=1.25|Relation between wheat yield and soil salinity<ref>[https://www.waterlog.info/sigmoid.htm Calculator for sigmoid regression]</ref>]]\n\n====Fitting other functions to data points====\nOther types of curves, such as [[trigonometric functions]] (such as sine and cosine), may also be used, in certain cases.\n\nIn spectroscopy, data may be fitted with [[Normal distribution|Gaussian]], [[Cauchy distribution|Lorentzian]], [[Voigt function|Voigt]] and related functions.\n\nIn [[agriculture]] the inverted logistic [[sigmoid function]] (S-curve) is used to describe the relation between crop yield and growth factors. The blue figure was made by a sigmoid regression of data measured in farm lands. It can be seen that initially, i.e. at low soil salinity, the crop yield reduces slowly at increasing soil salinity, while thereafter the decrease progresses faster.\n\n===Algebraic fit versus geometric fit for curves===\n\nFor algebraic analysis of data, \"fitting\" usually means trying to find the curve that minimizes the vertical (''y''-axis) displacement of a point from the curve (e.g., [[ordinary least squares]]). However, for graphical and image applications geometric fitting seeks to provide the best visual fit; which usually means trying to minimize the orthogonal distance to the curve (e.g., [[total least squares]]), or to otherwise include both axes of displacement of a point from the curve. Geometric fits are not popular because they usually require non-linear and/or iterative calculations, although they have the advantage of a more aesthetic and geometrically accurate result.<ref>{{citation |first=Sung-Joon |last=Ahn |title=Geometric Fitting of Parametric Curves and Surfaces |journal=Journal of Information Processing Systems |volume=4 |issue=4 |pages=153–158 |date=December 2008 |doi=10.3745/JIPS.2008.4.4.153 |url=http://jips-k.org/dlibrary/JIPS_v04_no4_paper4.pdf |deadurl=yes |archiveurl=https://web.archive.org/web/20140313084307/http://jips-k.org/dlibrary/JIPS_v04_no4_paper4.pdf |archivedate=2014-03-13 |df= }}</ref><ref>{{citation |first1=N. |last1=Chernov |first2=H. |last2=Ma |year=2011 |contribution=Least squares fitting of quadratic curves and surfaces |title=Computer Vision |editor-first=Sota R. |editor-last=Yoshida |publisher=Nova Science Publishers |isbn=9781612093994\n |pages=285–302 |url=<!-- http://people.cas.uab.edu/~mosya/papers/CM1nova.pdf No indication of copyright --> }}</ref><ref>{{citation |first1=Yang |last1=Liu |first2=Wenping |last2=Wang |year=2008 |contribution=A Revisit to Least Squares Orthogonal Distance Fitting of Parametric Curves and Surfaces |editor1-first=F. |editor1-last=Chen |editor2-first=B. |editor2-last=Juttler |title=Advances in Geometric Modeling and Processing |series=Lecture Notes in Computer Science |volume=4975 |pages=384–397 |doi=10.1007/978-3-540-79246-8_29\n |isbn=978-3-540-79245-1|citeseerx=10.1.1.306.6085 }}</ref>\n\n===Fitting plane curves to data points{{anchor|Plane curves}}===\nIf a function of the form <math>y=f(x)</math> cannot be postulated, one can still try to fit a [[plane curve]].\n\nOther types of curves, such as [[conic sections]] (circular, elliptical, parabolic, and hyperbolic arcs) or [[trigonometric functions]] (such as sine and cosine), may also be used, in certain cases.  For example, trajectories of objects under the influence of gravity follow a parabolic path, when air resistance is ignored.  Hence, matching trajectory data points to a parabolic curve would make sense.  Tides follow sinusoidal patterns, hence tidal data points should be matched to a sine wave, or the sum of two sine waves of different periods, if the effects of the Moon and Sun are both considered.\n\nFor a [[parametric curve]], it is effective to fit each of its coordinates as a separate function of [[arc length]]; assuming that data points can be ordered, the [[chord distance]] may be used.<ref>p.51 in Ahlberg & Nilson (1967) ''The theory of splines and their applications'', Academic Press, 1967  [https://books.google.com/books?id=S7d1pjJHsRgC&pg=PA51]</ref>\n\n====Fitting a circle by geometric fit{{anchor|Circles}}====\n\n[[File:Regression circulaire coope arc de cercle.svg|thumb|Circle fitting with the Coope method, the points describing a circle arc, centre (1 ; 1), radius 4.]]\n[[File:Wp ellfitting.png|thumb|different models of ellipse fitting]]\n[[File:Regression elliptique distance algebrique donnees gander.svg|thumb|Ellipse fitting minimising the algebraic distance (Fitzgibbon method).]]\n\nCoope<ref>{{cite journal|author=Coope, I.D.|title=Circle fitting by linear and nonlinear least squares|journal=Journal of Optimization Theory and Applications |volume =76|issue =2|year=1993|doi=10.1007/BF00939613|pages=381–388}}</ref> approaches the problem of trying to find the best visual fit of circle to a set of 2D data points. The method elegantly transforms the ordinarily non-linear problem into a linear problem that can be solved without using iterative numerical methods, and is hence an order of magnitude faster than previous techniques.\n\n====Fitting an ellipse by geometric fit{{anchor|Ellipses}}====\n\nThe above technique is extended to general ellipses<ref>Paul Sheer, [http://wiredspace.wits.ac.za/bitstream/handle/10539/22434/Sheer%20Paul%201997.pdf?sequence=1&isAllowed=y A software assistant for manual stereo photometrology], M.Sc. thesis, 1997</ref> by adding a non-linear step, resulting in a method that is fast, yet finds visually pleasing ellipses of arbitrary orientation and displacement.\n\n===Application to surfaces===\n{{Details|Computer representation of surfaces}}\n\nNote that while this discussion was in terms of 2D curves, much of this logic also extends to 3D surfaces, each patch of which is defined by a net of curves in two parametric directions, typically called '''u''' and '''v'''.  A surface may be composed of one or more surface patches in each direction.\n\n==Software==\n\nMany [[List of statistical packages|statistical packages]] such as [[R (programming language)|R]] and [[List of numerical analysis software|numerical software]] such as the [[GNU Scientific Library]], [[MLAB]], [[Maple (software)|Maple]], [[MATLAB]], [[Mathematica]], [[GNU Octave]], and [[SciPy]] include commands for doing curve fitting in a variety of scenarios. There are also programs specifically written to do curve fitting; they can be found in the [[List of statistical packages|lists of statistical]] and [[List of numerical analysis software|numerical analysis programs]] as well as in [[:Category:Regression and curve fitting software]].\n\n==See also==\n* [[Adjustment of observations]]\n* [[Curve-fitting compaction]]\n* [[Estimation theory]]\n* [[Function approximation]]\n* [[Goodness of fit]]\n* [[Levenberg–Marquardt algorithm]]\n* [[Line fitting]]\n* [[Nonlinear regression]]\n* [[Overfitting]]\n* [[Plane curve]]\n* [[Distribution fitting|Probability distribution fitting]]\n* [[Smoothing]]\n* [[Spline (mathematics)|Splines]] ([[Interpolating spline|interpolating]], [[Smoothing spline|smoothing]])\n* [[Time series]]\n* [[Total least squares]]\n* [[Trend estimation]]\n{{clear}}\n\n==References==\n{{Reflist|30em}}\n\n==Further reading==\n*N. Chernov (2010), ''Circular and linear regression: Fitting circles and lines by least squares'', Chapman & Hall/CRC, Monographs on Statistics and Applied Probability, Volume 117 (256 pp.). [http://people.cas.uab.edu/~mosya/cl/]\n\n{{Commons category|Curve fitting}}\n\n[[Category:Numerical analysis]]\n[[Category:Interpolation]]\n[[Category:Regression analysis]]\n[[Category:Geometric algorithms]]"
    },
    {
      "title": "De Boor's algorithm",
      "url": "https://en.wikipedia.org/wiki/De_Boor%27s_algorithm",
      "text": "In the [[mathematics|mathematical]] subfield of [[numerical analysis]] '''de Boor's algorithm'''<ref name=\"de_boor_paper\">C. de Boor [1971], \"Subroutine package for calculating with B-splines\", Techn.Rep. LA-4728-MS, Los Alamos Sci.Lab, Los Alamos NM; p. 109, 121.</ref> is a fast{{vague|date=January 2018}}{{quantify|date=January 2018}} and [[numerically stable]] [[algorithm]] for evaluating [[spline curve]]s in [[B-spline]] form. It is a generalization of [[de Casteljau's algorithm]] for [[Bézier curve]]s. The algorithm was devised by [[Carl R. de Boor]].  Simplified, potentially faster variants of the de Boor algorithm have been created but they suffer from comparatively lower stability.<ref>{{cite journal |last=Lee |first=E. T. Y. |date=December 1982 |title=A Simplified B-Spline Computation Routine |journal=Computing |volume=29 |issue=4 |pages=365–371 |publisher=Springer-Verlag|doi=10.1007/BF02246763}}</ref><ref>{{cite journal | author = Lee, E. T. Y. | journal = Computing | issue = 3 |    pages = 229–238 | publisher = Springer-Verlag | doi=10.1007/BF02240069|title = Comments on some B-spline algorithms | volume = 36 | year = 1986}}</ref>\n\n== Introduction ==\n\nA general introduction to B-splines is given in the [[B-spline|main article]]. Here we discuss de Boor's algorithm, an efficient and numerically stable scheme to evaluate a spline curve <math> \\mathbf{S}(x) </math> at position <math> x </math>. The curve is built from a sum of B-spline functions <math> B_{i,p}(x) </math> multiplied with potentially vector-valued constants <math> \\mathbf{c}_i </math>, called control points,\n\n:<math> \\mathbf{S}(x) = \\sum_i \\mathbf{c}_i B_{i,p}(x). </math>\n\nB-splines of order <math> p + 1 </math> are connected piece-wise polynomial functions of degree <math> p </math> defined over a grid of knots <math> {t_0, \\dots, t_i, \\dots, t_m} </math> (we always use zero-based indices in the following). De Boor's algorithm uses [[Big O notation|O]](p<sup>2</sup>) + [[Big O notation|O]](p) operations to evaluate the spline curve. Note: the [[B-spline|main article about B-splines]] and the classic publications<ref name=\"de_boor_paper\"></ref> use a different notation: the B-spline is indexed as <math> B_{i,n}(x) </math> with <math>n = p + 1</math>.\n\n== Local support ==\n\nB-splines have local support, meaning that the polynomials are positive only in a finite domain and zero elsewhere. The Cox-de Boor recursion formula<ref>C. de Boor, p. 90</ref> shows this:\n\n:<math>B_{i,0}(x) := \\left\\{\n\\begin{matrix}\n1 & \\mathrm{if} \\quad t_i \\leq x < t_{i+1} \\\\\n0 & \\mathrm{otherwise}\n\\end{matrix}\n\\right.\n</math>\n\n:<math>B_{i,p}(x) := \\frac{x - t_i}{t_{i+p} - t_i} B_{i,p-1}(x) + \\frac{t_{i+p+1} - x}{t_{i+p+1} - t_{i+1}} B_{i+1,p-1}(x). </math>\n\nLet the index <math> k </math> define the knot interval that contains the position, <math> x \\in [t_{k},t_{k+1}] </math>. We can see in the recursion formula that only B-splines with <math> i = k-p, \\dots, k </math> are non-zero for this knot interval. Thus, the sum is reduced to:\n\n:<math> \\mathbf{S}(x) = \\sum_{i=k-p}^{k} \\mathbf{c}_i B_{i,p}(x). </math>\n\nIt follows from <math> i \\geq 0 </math> that <math> k \\geq p </math>. Similarly, we see in the recursion that the highest queried knot location is at index <math> k + 1 + p </math>. This means that any knot interval <math> [t_k,t_{k+1}) </math> which is actually used must have at least <math> p </math> additional knots before and after. In a computer program, this is typically achieved by repeating the first and last used knot location <math> p </math> times. For example, for <math> p = 3 </math> and real knot locations <math> (0, 1, 2) </math>, one would pad the knot vector to <math> (0,0,0,0,1,2,2,2,2) </math>.\n\n== The algorithm ==\n\nWith these definitions, we can now describe de Boor's algorithm. The algorithm does not compute the B-spline functions <math> B_{i,p}(x) </math> directly. Instead it evaluates <math> \\mathbf{S}(x) </math> through an equivalent recursion formula.\n\nLet <math> \\mathbf{d}_{i,r} </math> be new control points with <math> \\mathbf{d}_{i,0} := \\mathbf{c}_{i} </math> for <math> i = k-p, \\dots, k</math>. For <math> r = 1, \\dots, p </math> the following recursion is applied:\n\n:<math> \\mathbf{d}_{i,r} = (1-\\alpha_{i,r}) \\mathbf{d}_{i-1,r-1} + \\alpha_{i,r} \\mathbf{d}_{i,r-1}; \\quad i=k-p+r,\\dots,k </math>\n\n:<math> \\alpha_{i,r} = \\frac{x-t_i}{t_{i+1+p-r}-t_i}.</math>\n\nOnce the iterations are complete, we have <math>\\mathbf{S}(x) = \\mathbf{d}_{k,p} </math>, meaning that <math> \\mathbf{d}_{k,p} </math> is the desired result.\n\nDe Boor's algorithm is more efficient than an explicit calculation of B-splines <math> B_{i,p}(x) </math> with the Cox-de Boor recursion formula, because it does not compute terms which are guaranteed to be multiplied by zero.\n\n== Optimizations ==\n\nThe algorithm above is not optimized for the implementation in a computer. It requires memory for <math> (p + 1) + p + \\dots + 1 = (p + 1)(p + 2)/2 </math> temporary control points <math> \\mathbf{d}_{i,r} </math>. Each temporary control points is written exactly once and read twice. By reversing the iteration over <math> i </math> (counting down instead of up), we can run the algorithm with memory for only <math> p + 1 </math> temporary control points, by letting <math> \\mathbf{d}_{i,r} </math> reuse the memory for <math> \\mathbf{d}_{i,r-1} </math>. Similarly, there is only one value of <math> \\alpha </math> used in each step, so we can reuse the memory as well.\n\nFurthermore, it is more convenient to use a zero-based index <math> j = 0, \\dots, p </math> for the temporary control points. The relation to the previous index is <math> i = j + k - p </math>. Thus we obtain the improved algorithm:\n\nLet <math> \\mathbf{d}_{j} := \\mathbf{c}_{j+k - p} </math> for <math> j = 0, \\dots, p</math>. Iterate for <math> r = 1, \\dots, p </math>:\n\n:<math> \\mathbf{d}_{j} := (1-\\alpha_j) \\mathbf{d}_{j-1} + \\alpha_j \\mathbf{d}_{j}; \\quad j=p, \\dots, r \\quad </math> (<math> j </math> must be counted down)\n\n:<math> \\alpha_j := \\frac{x-t_{j + k - p}}{t_{j+1+k-r}-t_{j+k-p}}. </math>\n\nAfter the iterations are complete, the result is <math>\\mathbf{S}(x) = \\mathbf{d}_{p} </math>.\n\n== Example implementation ==\n\nThe following code in the [[Python (programming language)|Python programming language]] is a naive implementation of the optimized algorithm.\n\n<source lang=\"python\">\ndef deBoor(k, x, t, c, p):\n    \"\"\"\n    Evaluates S(x).\n\n    Args\n    ----\n    k: index of knot interval that contains x\n    x: position\n    t: array of knot positions, needs to be padded as described above\n    c: array of control points\n    p: degree of B-spline\n    \"\"\"\n    d = [c[j + k - p] for j in range(0, p+1)]\n\n    for r in range(1, p+1):\n        for j in range(p, r-1, -1):\n            alpha = (x - t[j+k-p]) / (t[j+1+k-r] - t[j+k-p])\n            d[j] = (1.0 - alpha) * d[j-1] + alpha * d[j]\n\n    return d[p]\n</source>\n\n== See also ==\n*[[De Casteljau's algorithm]]\n*[[Bézier curve]]\n*[[NURBS]]\n\n== External links ==\n*[http://www.cs.mtu.edu/~shene/COURSES/cs3621/NOTES/spline/de-Boor.html  De Boor's Algorithm]\n*[http://www.idav.ucdavis.edu/education/CAGDNotes/Deboor-Cox-Calculation/Deboor-Cox-Calculation.html The DeBoor-Cox Calculation]\n\n== Computer code ==\n* [http://www.netlib.org/pppack/ PPPACK]: contains many spline algorithms in [[Fortran]]\n* [https://www.gnu.org/software/gsl/ GNU Scientific Library]: C-library, contains a sub-library for splines ported from [[Netlib|PPPACK]]\n* [https://www.scipy.org/ SciPy]: Python-library, contains a sub-library ''scipy.interpolate'' with spline functions based on [[Netlib|FITPACK]]\n* [https://github.com/msteinbeck/tinyspline TinySpline]: C-library for splines with a C++ wrapper and bindings for C#, Java, Lua, PHP, Python, and Ruby\n* [http://einspline.sourceforge.net/ Einspline]: C-library for splines in 1, 2, and 3 dimensions with Fortran wrappers\n\n== References ==\n<references/>\n\n'''Works cited'''\n*{{cite book | author = Carl de Boor | title = A Practical Guide to Splines, Revised Edition | publisher = Springer-Verlag  | year = 2003|ISBN=0-387-95366-3}}\n\n[[Category:Numerical analysis]]\n[[Category:Splines (mathematics)]]\n[[Category:Interpolation]]"
    },
    {
      "title": "De Casteljau's algorithm",
      "url": "https://en.wikipedia.org/wiki/De_Casteljau%27s_algorithm",
      "text": "In the [[mathematics|mathematical]] field of [[numerical analysis]], '''De Casteljau's algorithm''' is a [[Recursion|recursive]] method to evaluate polynomials in [[Bernstein form]] or [[Bézier curve]]s, named after its inventor [[Paul de Casteljau]]. '''De Casteljau's algorithm''' can also be used to split a single Bézier curve into two Bézier curves at an arbitrary parameter value.\n\nAlthough the algorithm is slower for most architectures when compared with the direct approach, it is more [[numerically stable]].\n\n==Definition==\nA Bézier curve ''B'' (of degree ''n'', with control points <math>\\beta_0, \\ldots, \\beta_n</math>) can be written in Bernstein form as follows\n\n:<math>B(t) = \\sum_{i=0}^{n}\\beta_{i}b_{i,n}(t) </math>,\n\nwhere ''b'' is a [[Bernstein polynomial|Bernstein basis polynomial]]\n\n:<math> b_{i,n}(t) = {n \\choose i}(1-t)^{n-i}t^i</math>.\n\nThe curve at point ''t''<sub>0</sub> can be evaluated with the [[recurrence relation]]\n\n:<math>\\beta_i^{(0)} := \\beta_i \\mbox{ , } i=0,\\ldots,n</math>\n:<math>\\beta_i^{(j)} := \\beta_i^{(j-1)} (1-t_0) + \\beta_{i+1}^{(j-1)} t_0 \\mbox{ , } i = 0,\\ldots,n-j \\mbox{ , } j= 1,\\ldots,n</math>\n\nThen, the evaluation of <math>B</math> at point <math>t_0</math> can be evaluated in <math>\\binom{n}{2}</math> operations. The result <math>B(t_0)</math> is given by :\n\n:<math>B(t_0)=\\beta_0^{(n)}.</math>\n\nMoreover, the Bézier curve <math>B</math> can be split at point <math>t_0</math> into two curves with respective control points :\n:<math>\\beta_0^{(0)},\\beta_0^{(1)},\\ldots,\\beta_0^{(n)}</math>\n:<math>\\beta_0^{(n)},\\beta_1^{(n-1)},\\ldots,\\beta_n^{(0)}</math>\n\n== Example implementation ==\n\nHere is an example implementation of De Casteljau's algorithm in [[Haskell (programming language)|Haskell]]:\n<source lang=\"haskell\">\ndeCasteljau :: Double -> [(Double, Double)] -> (Double, Double)\ndeCasteljau t [b] = b\ndeCasteljau t coefs = deCasteljau t reduced\n  where\n    reduced = zipWith (lerpP t) coefs (tail coefs)\n    lerpP t (x0, y0) (x1, y1) = (lerp t x0 x1, lerp t y0 y1)\n    lerp t a b = t * b + (1 - t) * a\n</source>\n\n==Notes==\nWhen doing the calculation by hand it is useful to write down the coefficients in a triangle scheme as\n\n:<math>\n\\begin{matrix}\n\\beta_0     & = \\beta_0^{(0)}     &                   &         &               \\\\\n            &                     & \\beta_0^{(1)}     &         &               \\\\\n\\beta_1     & = \\beta_1^{(0)}     &                   &         &               \\\\\n            &                     &                   & \\ddots  &               \\\\\n\\vdots      &                     & \\vdots            &         & \\beta_0^{(n)} \\\\\n            &                     &                   &         &               \\\\\n\\beta_{n-1} & = \\beta_{n-1}^{(0)} &                   &         &               \\\\\n            &                     & \\beta_{n-1}^{(1)} &         &               \\\\\n\\beta_n     & = \\beta_n^{(0)}     &                   &         &               \\\\\n\\end{matrix}\n</math>\n\nWhen choosing a point ''t''<sub>0</sub> to evaluate a Bernstein polynomial we can use the two diagonals of the triangle scheme to construct a division of the polynomial\n\n:<math>B(t) = \\sum_{i=0}^n \\beta_i^{(0)} b_{i,n}(t) \\mbox{ , } \\qquad t \\in [0,1]</math>\n\ninto\n\n:<math>B_1(t) = \\sum_{i=0}^n \\beta_0^{(i)} b_{i,n}\\left(\\frac{t}{t_0}\\right) \\mbox{ , } \\qquad t \\in [0,t_0]</math>\n\nand\n\n:<math>B_2(t) = \\sum_{i=0}^n \\beta_i^{(n-i)} b_{i,n}\\left(\\frac{t-t_0}{1-t_0}\\right) \\mbox{ , } \\qquad t \\in [t_0,1]</math>\n\n==Example==\nWe want to evaluate the Bernstein polynomial of degree 2 with the Bernstein coefficients\n:<math>\\beta_0^{(0)} = \\beta_0</math>\n:<math>\\beta_1^{(0)} = \\beta_1</math>\n:<math>\\beta_2^{(0)} = \\beta_2</math>\nat the point ''t''<sub>0</sub>.\n\nWe start the recursion with\n:<math>\\beta_0^{(1)} = \\beta_0^{(0)} (1-t_0) + \\beta_1^{(0)}t_0 = \\beta_0(1-t_0) + \\beta_1 t_0</math>\n:<math>\\beta_1^{(1)} = \\beta_1^{(0)} (1-t_0) + \\beta_2^{(0)}t_0 = \\beta_1(1-t_0) + \\beta_2 t_0</math>\n\nand with the second iteration the recursion stops with\n:<math> \n\\begin{align}\n\\beta_0^{(2)} & = \\beta_0^{(1)} (1-t_0) + \\beta_1^{(1)} t_0      \\\\\n\\             & = \\beta_0(1-t_0) (1-t_0) + \\beta_1 t_0 (1-t_0) + \\beta_1(1-t_0)t_0 + \\beta_2 t_0 t_0 \\\\\n\\             & = \\beta_0 (1-t_0)^2 + \\beta_1 2t_0(1-t_0) + \\beta_2 t_0^2\n\\end{align}\n</math>\n\nwhich is the expected Bernstein polynomial of degree&nbsp;''2''.\n\n==Bézier curve==\n[[File:Bézier 2 big.gif|thumb|right|A Bézier curve]]\nWhen evaluating a Bézier curve of degree ''n'' in 3-dimensional space with ''n''+1 control points '''P'''<sub>''i''</sub>\n\n:<math>\\mathbf{B}(t) = \\sum_{i=0}^{n} \\mathbf{P}_i b_{i,n}(t) \\mbox{ , } t \\in [0,1]</math>\n\nwith\n\n:<math>\\mathbf{P}_i := \n\\begin{pmatrix} \nx_i \\\\ \ny_i \\\\\nz_i \n\\end{pmatrix}\n</math>.\n\nwe split the Bézier curve into three separate equations\n\n:<math>B_1(t) = \\sum_{i=0}^{n} x_i b_{i,n}(t) \\mbox{ , } t \\in [0,1]</math>\n:<math>B_2(t) = \\sum_{i=0}^{n} y_i b_{i,n}(t) \\mbox{ , } t \\in [0,1]</math>\n:<math>B_3(t) = \\sum_{i=0}^{n} z_i b_{i,n}(t) \\mbox{ , } t \\in [0,1]</math>\n\nwhich we evaluate individually using De Casteljau's algorithm.\n\n==Geometric interpretation==\nThe geometric interpretation of De Casteljau's algorithm is straightforward. \n*Consider a Bézier curve with control points <math>\\scriptstyle P_0,...,P_n</math>. Connecting the consecutive points we create the control polygon of the curve.\n*Subdivide now each line segment of this polygon with the ratio <math>\\scriptstyle t : (1-t)</math> and connect the points you get. This way you arrive at the new polygon having one fewer segment.\n*Repeat the process until you arrive at the single point – this is the point of the curve corresponding to the parameter <math>\\scriptstyle t</math>.\nThe following picture shows this process for a cubic Bézier curve:\n\n[[Image:DeCasteljau1.svg]]\n\nNote that the intermediate points that were constructed are in fact the control points for two new Bézier curves, both exactly coincident with the old one. This algorithm not only evaluates the curve at <math>\\scriptstyle t</math>, but splits the curve into two pieces at <math>\\scriptstyle t</math>, and provides the equations of the two sub-curves in Bézier form.\n\nThe interpretation given above is valid for a nonrational Bézier curve. To evaluate a rational Bézier curve in <math>\\scriptstyle \\mathbf{R}^n</math>, we may project the point into <math>\\scriptstyle \\mathbf{R}^{n+1}</math>; for example, a curve in three dimensions may have its control points <math>\\scriptstyle \\{(x_i, y_i, z_i)\\}</math> and weights <math>\\scriptstyle \\{w_i\\}</math> projected to the weighted control points <math>\\scriptstyle \\{(w_ix_i, w_iy_i, w_iz_i, w_i)\\}</math>. The algorithm then proceeds as usual, interpolating in <math>\\scriptstyle \\mathbf{R}^4</math>. The resulting four-dimensional points may be projected back into three-space with a [[perspective divide]].\n\nIn general, operations on a rational curve (or surface) are equivalent to operations on a nonrational curve in a [[projective space]]. This representation as the \"weighted control points\" and weights is often convenient when evaluating rational curves.\n\n==See also==\n*[[Bézier curve]]s\n*[[De Boor's algorithm]]\n*[[Horner scheme]] to evaluate polynomials in [[monomial form]]\n*[[Clenshaw algorithm]] to evaluate polynomials in [[Chebyshev form]]\n\n==References==\n*Farin, Gerald & Hansford, Dianne (2000). ''The Essentials of CAGD''.  Natic, MA: A K Peters, Ltd.  {{ISBN|1-56881-123-3}}\n\n==External links==\n* [http://hcklbrrfnn.wordpress.com/2012/08/20/piecewise-linear-approximation-of-bezier-curves/ Piecewise linear approximation of Bézier curves] – description of De Casteljau's algorithm, including a criterion to determine when to stop the recusion\n* [http://jeremykun.com/2013/05/11/bezier-curves-and-picasso/ Bezier Curves and Picasso] — Description and illustration of De Casteljau's algorithm applied to cubic Bézier curves.\n\n[[Category:Splines (mathematics)]]\n[[Category:Numerical analysis]]\n[[Category:Articles with example Haskell code]]"
    },
    {
      "title": "Difference quotient",
      "url": "https://en.wikipedia.org/wiki/Difference_quotient",
      "text": "{{broader|Finite difference}}\nIn single-variable [[calculus]], the '''difference quotient''' is usually the name for the expression\n\n:<math> \\frac{f(x+h) - f(x)}{h} </math>\n\nwhich when taken to the [[Limit of a function|limit]] as ''h'' approaches 0 gives the [[derivative]] of the [[Function (mathematics)|function]] ''f''.<ref name=\"LaxTerrell2013\">{{cite book|author1=Peter D. Lax|author2=Maria Shea Terrell|title=Calculus With Applications|year=2013|publisher=Springer|isbn=978-1-4614-7946-8|page=119}}</ref><ref name=\"HockettBock2005\">{{cite book|author1=Shirley O. Hockett|author2=David Bock|title=Barron's how to Prepare for the AP Calculus|year=2005|publisher=Barron's Educational Series|isbn=978-0-7641-2382-5|page=44}}</ref><ref name=\"Ryan2010\">{{cite book|author=Mark Ryan|title=Calculus Essentials For Dummies|year=2010|publisher=John Wiley & Sons|isbn=978-0-470-64269-6|pages=41–47}}</ref><ref name=\"NealGustafson2012\">{{cite book|author1=Karla Neal|author2=R. Gustafson|author3=Jeff Hughes|title=Precalculus|year=2012|publisher=Cengage Learning|isbn=978-0-495-82662-0|page=133}}</ref> The name of the expression stems from the fact that it is the [[quotient]] of the [[Difference (mathematics)|difference]] of values of the function by the difference of the corresponding values of its argument (the latter is (''x''+''h'')-''x''=''h'' in this case).<ref name=\"Comenetz2002\">{{cite book|author=Michael Comenetz|title=Calculus: The Elements|year=2002|publisher=World Scientific|isbn=978-981-02-4904-5|pages=71–76 and 151–161}}</ref><ref name=\"Pasch2010\">{{cite book|author=Moritz Pasch|title=Essays on the Foundations of Mathematics by Moritz Pasch|year=2010|publisher=Springer|isbn=978-90-481-9416-2|page=157}}</ref> The difference quotient is a measure of the '''[[average]] rate of change''' of the function over an [[Interval (mathematics)|interval]] (in this case, an interval of length ''h'').<ref name=\"WilsonAdamson2008\">{{cite book|author1=Frank C. Wilson|author2=Scott Adamson|title=Applied Calculus|year=2008|publisher=Cengage Learning|isbn=978-0-618-61104-1|page=177}}</ref><ref name=\"RubySellers2014\"/>{{rp|237}}<ref name=\"HungerfordShaw2008\">{{cite book|author1=Thomas Hungerford|author2=Douglas Shaw|title=Contemporary Precalculus: A Graphing Approach|year=2008|publisher=Cengage Learning|isbn=978-0-495-10833-7|pages=211–212}}</ref> The limit of the difference quotient (i.e., the derivative) is thus the [[instantaneous]] rate of change.<ref name=\"HungerfordShaw2008\"/>\n\nBy a slight change in notation (and viewpoint), for an interval [''a'', ''b''], the difference quotient\n\n:<math> \\frac{f(b) - f(a)}{b-a}</math>\n\nis called<ref name=\"Comenetz2002\"/> the mean (or average) value of the derivative of ''f'' over the interval [''a'', ''b'']. This name is justified by the [[mean value theorem]], which states that for a [[differentiable function]] ''f'', its derivative ''f′'' reaches its [[Mean of a function|mean value]] at some point in the interval.<ref name=\"Comenetz2002\"/> Geometrically, this difference quotient measures the [[slope]] of the [[secant line]] passing through the points with coordinates (''a'', ''f''(''a'')) and  (''b'', ''f''(''b'')).<ref name=\"Krantz2014\">{{cite book|author=Steven G. Krantz|title=Foundations of Analysis|year=2014|publisher=CRC Press|isbn=978-1-4822-2075-9|page=127}}</ref>\n\nDifference quotients are used as approximations in [[numerical differentiation]],<ref name=\"RubySellers2014\">{{cite book|author1=Tamara Lefcourt Ruby|author2=James Sellers|author3=Lisa Korf |author4=Jeremy Van Horn |author5=Mike Munn|title=Kaplan AP Calculus AB & BC 2015|year=2014|publisher=Kaplan Publishing|isbn=978-1-61865-686-5|page=299}}</ref> but they have also been subject of criticism in this application.<ref name=\"GriewankWalther2008\">{{cite book|author1=Andreas Griewank|author2=Andrea Walther|title=Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation, Second Edition|url=https://books.google.com/books?id=qMLUIsgCwvUC&pg=PA2|year=2008|publisher=SIAM|isbn=978-0-89871-659-7|pages=2–}}</ref>\n\nThe difference quotient is sometimes also called the '''Newton quotient'''<ref name=\"Krantz2014\"/><ref name=\"Lang1968\">{{cite book|author=Serge Lang|title=Analysis 1|year=1968|publisher=Addison-Wesley Publishing Company|page=56|author-link=Serge Lang}}</ref><ref name=\"Hahn1994\">{{cite book|author=Brian D. Hahn|title=Fortran 90 for Scientists and Engineers|year=1994|publisher=Elsevier|isbn=978-0-340-60034-4|page=276}}</ref><ref name=\"ClaphamNicholson2009\">{{cite book|author1=Christopher Clapham|author2=James Nicholson|title=The Concise Oxford Dictionary of Mathematics|year=2009|publisher=Oxford University Press|isbn=978-0-19-157976-9|page=313}}</ref> (after [[Isaac Newton]]) or '''Fermat's difference quotient''' (after [[Pierre de Fermat]]).<ref>Donald C. Benson, ''A Smoother Pebble: Mathematical Explorations'', Oxford University Press, 2003, p. 176.</ref>\n\n==Overview==\nThe typical notion of the difference quotient discussed above is a particular case of a more general concept. The primary vehicle of [[calculus]] and other higher mathematics is the [[Function (mathematics)|function]].  Its \"input value\" is its ''argument'', usually a point (\"P\") expressible on a graph.  The difference between two points, themselves, is known as their [[Delta (letter)|Delta]] (Δ''P''), as is the difference in their function result, the particular notation being determined by the direction of formation:\n*Forward difference:&nbsp; Δ''F''(''P'') = ''F''(''P'' + Δ''P'') − ''F''(''P'');\n*Central difference:&nbsp; δF(P) = F(P + ½ΔP) − F(P − ½ΔP);\n*Backward difference: ∇F(P) = F(P) − F(P − ΔP).\nThe general preference is the forward orientation, as F(P) is the base, to which differences (i.e., \"ΔP\"s) are added to it.  Furthermore,\n\n*If |ΔP| is ''finite'' (meaning measurable), then ΔF(P) is known as a '''[[finite difference]]''', with specific denotations of DP and DF(P);\n*If |ΔP| is ''[[infinitesimal]]'' (an infinitely small amount—''<math>\\iota</math>''—usually expressed in standard analysis as a limit: <math>\\lim_{\\Delta P\\rightarrow 0}\\,\\!</math>), then ΔF(P) is known as an '''infinitesimal difference''', with specific denotations of dP and dF(P) (in calculus graphing, the point is almost exclusively identified as \"x\" and F(x) as \"y\").\n\nThe function difference divided by the point difference is known as \"difference quotient\":\n\n:<math>\\frac{\\Delta F(P)}{\\Delta P}=\\frac{F(P+\\Delta P)-F(P)}{\\Delta P}=\\frac{\\nabla F(P+\\Delta P)}{\\Delta P}.\\,\\!</math>\n\nIf ΔP is infinitesimal, then the difference quotient is a ''[[derivative]]'', otherwise it is a ''[[divided differences|divided difference]]'':\n\n:<math> \\text{If } |\\Delta P| = \\mathit{ \\iota}: \\quad \\frac{\\Delta F(P)}{\\Delta P}=\\frac{dF(P)}{dP}=F'(P)=G(P);\\,\\!</math>\n\n:<math> \\text{If } |\\Delta P| > \\mathit{ \\iota}: \\quad \\frac{\\Delta F(P)}{\\Delta P}=\\frac{DF(P)}{DP}=F[P,P+\\Delta P].\\,\\!</math>\n\n==Defining the point range==\nRegardless if ΔP is infinitesimal or finite, there is (at least—in the case of the derivative—theoretically) a point range, where the boundaries are P&nbsp;±&nbsp;(0.5)&nbsp;ΔP (depending on the orientation—ΔF(P), δF(P) or ∇F(P)):\n:LB = Lower Boundary; &nbsp; UB = Upper Boundary;\nDerivatives can be regarded as functions themselves, harboring their own derivatives. Thus each function is home to sequential degrees (\"higher orders\") of derivation, or ''differentiation''.  This property can be generalized to all difference quotients.<br>\nAs this sequencing requires a corresponding boundary splintering, it is practical to break up the point range into smaller, equi-sized sections, with each section being marked by an intermediary point (''P''<sub>''i''</sub>), where LB = ''P''<sub>0</sub> and UB = ''P''<sub>''ń''</sub>, the ''n''th  point, equaling the degree/order:\n<!--Improperly formatted formulae-->\n   LB =  P<sub>0</sub>  = P<sub>0</sub> + 0Δ<sub>1</sub>P     = P<sub>ń</sub> − (Ń-0)Δ<sub>1</sub>P;\n         P<sub>1</sub>  = P<sub>0</sub> + 1Δ<sub>1</sub>P     = P<sub>ń</sub> − (Ń-1)Δ<sub>1</sub>P;\n         P<sub>2</sub>  = P<sub>0</sub> + 2Δ<sub>1</sub>P     = P<sub>ń</sub> − (Ń-2)Δ<sub>1</sub>P;\n         P<sub>3</sub>  = P<sub>0</sub> + 3Δ<sub>1</sub>P     = P<sub>ń</sub> − (Ń-3)Δ<sub>1</sub>P;\n             ↓      ↓        ↓       ↓\n        P<sub>ń-3</sub> = P<sub>0</sub> + (Ń-3)Δ<sub>1</sub>P = P<sub>ń</sub> − 3Δ<sub>1</sub>P;\n        P<sub>ń-2</sub> = P<sub>0</sub> + (Ń-2)Δ<sub>1</sub>P = P<sub>ń</sub> − 2Δ<sub>1</sub>P;\n        P<sub>ń-1</sub> = P<sub>0</sub> + (Ń-1)Δ<sub>1</sub>P = P<sub>ń</sub> − 1Δ<sub>1</sub>P;\n   UB = P<sub>ń-0</sub> = P<sub>0</sub> + (Ń-0)Δ<sub>1</sub>P = P<sub>ń</sub> − 0Δ<sub>1</sub>P = P<sub>ń</sub>;\n\n   ΔP = Δ<sub>1</sub>P = P<sub>1</sub> − P<sub>0</sub> = P<sub>2</sub> − P<sub>1</sub> = P<sub>3</sub> − P<sub>2</sub> = ... = P<sub>ń</sub> − P<sub>ń-1</sub>;\n\n   ΔB = UB − LB = P<sub>ń</sub> − P<sub>0</sub> = Δ<sub>ń</sub>P = ŃΔ<sub>1</sub>P.\n\n==The primary difference quotient (''Ń'' = 1)==\n:<math>\\frac{\\Delta F(P_0)}{\\Delta P}=\\frac{F(P_{\\acute{n}})-F(P_0)}{\\Delta_{\\acute{n}}P}=\\frac{F(P_1)-F(P_0)}{\\Delta _1P}=\\frac{F(P_1)-F(P_0)}{P_1-P_0}.\\,\\!</math>\n\n===As a derivative===\n:The difference quotient as a derivative needs no explanation, other than to point out that, since P<sub>0</sub> essentially equals P<sub>1</sub> = P<sub>2</sub> = ... = P<sub>ń</sub> (as the differences are infinitesimal), the [[Leibniz notation]] and derivative expressions do not distinguish P to P<sub>0</sub> or P<sub>ń</sub>:\n\n:::<math>\\frac{dF(P)}{dP}=\\frac{F(P_1)-F(P_0)}{dP}=F'(P)=G(P).\\,\\!</math>\nThere are [[Derivative#Notation for differentiation|other derivative notations]], but these are the most recognized, standard designations.\n\n===As a divided difference===\n:A divided difference, however, does require further elucidation, as it equals the average derivative between and including LB and UB:\n\n:: <math>\n\\begin{align}\nP_{(tn)} & =LB+\\frac{TN-1}{UT-1}\\Delta B \\ =UB-\\frac{UT-TN}{UT-1}\\Delta B; \\\\[10pt]\n& {} \\qquad {\\color{white}.}(P_{(1)}=LB,\\  P_{(ut)}=UB){\\color{white}.} \\\\[10pt]\nF'(P_\\tilde{a}) & =F'(LB < P < UB)=\\sum_{TN=1}^{UT=\\infty}\\frac{F'(P_{(tn)})}{UT}.\n\\end{align}\n</math>\n\n:In this interpretation, P<sub>ã</sub> represents a function extracted, average value of P (midrange, but usually not exactly midpoint), the particular valuation depending on the function averaging it is extracted from.  More formally, P<sub>ã</sub> is found in the [[mean value theorem]] of calculus, which says:\n\n::''For any function that is continuous on [LB,UB] and differentiable on (LB,UB) there exists some P<sub>ã</sub> in the interval (LB,UB) such that the secant joining the endpoints of the interval [LB,UB] is parallel to the tangent at P<sub>ã</sub>.''\n\n:Essentially, P<sub>ã</sub> denotes some value of P between LB and UB—hence,\n\n::<math>P_\\tilde{a}:=LB < P < UB=P_0 < P < P_\\acute{n} \\,\\!</math>\n\n:which links the mean value result with the divided difference:\n\n:: <math>\n\\begin{align}\n\\frac{DF(P_0)}{DP} & = F[P_0,P_1]=\\frac{F(P_1)-F(P_0)}{P_1-P_0}=F'(P_0 < P < P_1)=\\sum_{TN=1}^{UT=\\infty}\\frac{F'(P_{(tn)})}{UT}, \\\\[8pt]\n& = \\frac{DF(LB)}{DB}=\\frac{\\Delta F(LB)}{\\Delta B}=\\frac{\\nabla F(UB)}{\\Delta B}, \\\\[8pt]\n& = F[LB,UB]=\\frac{F(UB)-F(LB)}{UB-LB}, \\\\[8pt]\n& =F'(LB < P < UB)=G(LB < P < UB).\n\\end{align}\n</math>\n\n:As there is, by its very definition, a tangible difference between LB/P<sub>0</sub> and UB/P<sub>ń</sub>, the Leibniz and derivative expressions ''do'' require [[divaricate|divarication]] of the function argument.\n\n==Higher-order difference quotients==\n\n===Second order===\n\n: <math>\n\\begin{align}\n\\frac{\\Delta^2F(P_0)}{\\Delta_1P^2} & =\\frac{\\Delta F'(P_0)}{\\Delta_1P}=\\frac{\\frac{\\Delta F(P_1)}{\\Delta_1P}-\\frac{\\Delta F(P_0)}{\\Delta_1P}}{\\Delta_1P}, \\\\[10pt]\n& =\\frac{\\frac{F(P_2)-F(P_1)}{\\Delta_1P}-\\frac{F(P_1)-F(P_0)}{\\Delta_1P}}{\\Delta_1P}, \\\\[10pt]\n& =\\frac{F(P_2)-2F(P_1)+F(P_0)}{\\Delta_1P^2};\n\\end{align}\n</math>\n\n: <math>\n\\begin{align}\n\\frac{d^2F(P)}{dP^2} & = \\frac{dF'(P)}{dP}=\\frac{F'(P_1)-F'(P_0)}{dP}, \\\\[10pt]\n& =\\ \\frac{dG(P)}{dP}=\\frac{G(P_1)-G(P_0)}{dP}, \\\\[10pt]\n& =\\frac{F(P_2)-2F(P_1)+F(P_0)}{dP^2}, \\\\[10pt]\n& =F''(P)=G'(P)=H(P)\n\\end{align}\n</math>\n\n: <math>\n\\begin{align}\n\\frac{D^2F(P_0)}{DP^2} & =\\frac{DF'(P_0)}{DP}=\\frac{F'(P_1 < P < P_2)-F'(P_0 < P < P_1)}{P_1-P_0}, \\\\[10pt]\n& {\\color{white}.} \\qquad \\ne\\frac{F'(P_1)-F'(P_0)}{P_1-P_0}, \\\\[10pt]\n& =F[P_0,P_1,P_2]=\\frac{F(P_2)-2F(P_1)+F(P_0)}{(P_1-P_0)^2}, \\\\[10pt]\n& =F''(P_0 < P < P_2)=\\sum_{TN=1}^\\infty \\frac{F''(P_{(tn)})}{UT}, \\\\[10pt]\n& =G'(P_0 < P < P_2)=H(P_0 < P < P_2).\n\\end{align}\n</math>\n\n===Third order===\n\n: <math>\n\\begin{align}\n\\frac{\\Delta^3F(P_0)}{\\Delta_1P^3} & = \\frac{\\Delta^2 F'(P_0)}{\\Delta_1P^2}=\\frac{\\Delta F''(P_0)}{\\Delta_1P}\n=\\frac{\\frac{\\Delta F'(P_1)}{\\Delta_1P}-\\frac{\\Delta F'(P_0)}{\\Delta_1P}}{\\Delta_1P}, \\\\[10pt]\n& =\\frac{\\frac{\\frac{\\Delta F(P_2)}{\\Delta_1P}-\\frac{\\Delta F'(P_1)}{\\Delta_1P}}{\\Delta_1P}-\n\\frac{\\frac{\\Delta F'(P_1)}{\\Delta_1P}-\\frac{\\Delta F'(P_0)}{\\Delta_1P}}{\\Delta_1P}}{\\Delta_1P}, \\\\[10pt]\n& =\\frac{\\frac{F(P_3)-2F(P_2)+F(P_1)}{\\Delta_1P^2}-\\frac{F(P_2)-2F(P_1)+F(P_0)}{\\Delta_1P^2}}{\\Delta_1P}, \\\\[10pt]\n& =\\frac{F(P_3)-3F(P_2)+3F(P_1)-F(P_0)}{\\Delta_1P^3};\n\\end{align}\n</math>\n\n: <math>\n\\begin{align}\n\\frac{d^3F(P)}{dP^3} & =\\frac{d^2F'(P)}{dP^2}=\\frac{dF''(P)}{dP}=\\frac{F''(P_1)-F''(P_0)}{dP}, \\\\[10pt]\n& =\\frac{d^2G(P)}{dP^2}\\ =\\frac{dG'(P)}{dP}\\ =\\frac{G'(P_1)-G'(P_0)}{dP}, \\\\[10pt]\n& {\\color{white}.}\\qquad\\qquad\\ \\ =\\frac{dH(P)}{dP}\\ =\\frac{H(P_1)-H(P_0)}{dP}, \\\\[10pt]\n& =\\frac{G(P_2)-2G(P_1)+G(P_0)}{dP^2}, \\\\[10pt]\n& =\\frac{F(P_3)-3F(P_2)+3F(P_1)-F(P_0)}{dP^3}, \\\\[10pt]\n& =F'''(P)=G''(P)=H'(P)=I(P);\n\\end{align}\n</math>\n\n: <math>\n\\begin{align}\n\\frac{D^3F(P_0)}{DP^3} & =\\frac{D^2F'(P_0)}{DP^2}=\\frac{DF''(P_0)}{DP}=\\frac{F''(P_1 < P < P_3)-F''(P_0 < P < P_2)}{P_1-P_0}, \\\\[10pt]\n& {\\color{white}.}\\qquad\\qquad\\qquad\\qquad\\qquad\\ \\ \\ne\\frac{F''(P_1)-F''(P_0)}{P_1-P_0}, \\\\[10pt]\n& =\\frac{\\frac{F'(P_2 < P < P_3)-F'(P_1 < P < P_2)}{P_1-P_0}-\\frac{F'(P_1 < P < P_2)-F'(P_0 < P < P_1)}{P_1-P_0}}{P_1-P_0}, \\\\[10pt]\n& =\\frac{F'(P_2 < P < P_3)-2F'(P_1 < P < P_2)+F'(P_0 < P < P_1)}{(P_1-P_0)^2}, \\\\[10pt]\n& =F[P_0,P_1,P_2,P_3]=\\frac{F(P_3)-3F(P_2)+3F(P_1)-F(P_0)}{(P_1-P_0)^3}, \\\\[10pt]\n& =F'''(P_0 < P < P_3)=\\sum_{TN=1}^{UT=\\infty}\\frac{F'''(P_{(tn)})}{UT}, \\\\[10pt]\n& =G''(P_0 < P < P_3)\\ =H'(P_0 < P < P_3)=I(P_0 < P < P_3).\n\\end{align}\n</math>\n\n===''Ń''th order===\n\n: <math>\n\\begin{align}\n\\Delta^\\acute{n}F(P_0) & =F^{(\\acute{n}-1)}(P_1)-F^{(\\acute{n}-1)}(P_0), \\\\[10pt]\n& =\\frac{F^{(\\acute{n}-2)}(P_2)-F^{(\\acute{n}-2)}(P_1)}{\\Delta_1P}-\\frac{F^{(\\acute{n}-2)}(P_1)-F^{(\\acute{n}-2)}(P_0)}{\\Delta_1P}, \\\\[10pt]\n& =\\frac{\\frac{F^{(\\acute{n}-3)}(P_3)-F^{(\\acute{n}-3)}(P_2)}{\\Delta_1P}-\\frac{F^{(\\acute{n}-3)}(P_2)-F^{(\\acute{n}-3)}(P_1)}{\\Delta_1P}}{\\Delta_1P} \\\\[10pt]\n& {\\color{white}.}\\qquad -\\frac{\\frac{F^{(\\acute{n}-3)}(P_2)-F^{(\\acute{n}-3)}(P_1)}{\\Delta_1P}-\\frac{F^{(\\acute{n}-3)}(P_1)-F^{(\\acute{n}-3)}(P_0)}{\\Delta_1P}}{\\Delta_1P}, \\\\[10pt]\n& = \\cdots\n\\end{align}\n</math>\n\n: <math>\n\\begin{align}\n\\frac{\\Delta^\\acute{n}F(P_0)}{\\Delta_1P^\\acute{n}} & =\\frac{\\sum_{I=0}^{\\acute{N}}{-1\\choose\\acute{N}-I}{\\acute{N}\\choose I}F(P_0+I\\Delta_1P)}{\\Delta_1P^\\acute{n}}; \\\\[10pt]\n& \\frac{\\nabla^\\acute{n}F(P_\\acute{n})}{\\Delta_1P^\\acute{n}} \\\\[10pt]\n& =\\frac{\\sum_{I=0}^{\\acute{N}}{-1\\choose I}{\\acute{N}\\choose I}F(P_\\acute{n}-I\\Delta_1P)}{\\Delta_1P^\\acute{n}};\n\\end{align}\n</math>\n\n: <math>\n\\begin{align}\n\\frac{d^\\acute{n}F(P_0)}{dP^\\acute{n}} & =\\frac{d^{\\acute{n}-1}F'(P_0)}{dP^{\\acute{n}-1}}\n=\\frac{d^{\\acute{n}-2}F''(P_0)}{dP^{\\acute{n}-2}}\n=\\frac{d^{\\acute{n}-3}F'''(P_0)}{dP^{\\acute{n}-3}}=\\cdots=\\frac{d^{\\acute{n}-r}F^{(r)}(P_0)}{dP^{\\acute{n}-r}},\n \\\\[10pt]\n& =\\frac{d^{\\acute{n}-1}G(P_0)}{dP^{\\acute{n}-1}} \\\\[10pt]\n& =\\frac{d^{\\acute{n}-2}G'(P_0)}{dP^{\\acute{n}-2}}=\\ \\frac{d^{\\acute{n}-3}G''(P_0)}{dP^{\\acute{n}-3}}=\\cdots=\\frac{d^{\\acute{n}-r}G^{(r-1)}(P_0)}{dP^{\\acute{n}-r}}, \\\\[10pt]\n& {\\color{white}.}\\qquad\\qquad\\qquad=\\frac{d^{\\acute{n}-2}H(P_0)}{dP^{\\acute{n}-2}}\n=\\ \\frac{d^{\\acute{n}-3}H'(P_0)}{dP^{\\acute{n}-3}}=\\cdots=\\frac{d^{\\acute{n}-r}H^{(r-2)}(P_0)}{dP^{\\acute{n}-r}}, \\\\\n& {\\color{white}.}\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\ =\\ \\frac{d^{\\acute{n}-3}I(P_0)}{dP^{\\acute{n}-3}}\n=\\cdots=\\frac{d^{\\acute{n}-r}I^{(r-3)}(P_0)}{dP^{\\acute{n}-r}}, \\\\[10pt]\n& =F^{(\\acute{n})}(P)=G^{(\\acute{n}-1)}(P)=H^{(\\acute{n}-2)}(P)=I^{(\\acute{n}-3)}(P)=\\cdots\n\\end{align}\n</math>\n\n: <math>\n\\begin{align}\n\\frac{D^\\acute{n}F(P_0)}{DP^\\acute{n}} & =F[P_0,P_1,P_2,P_3,\\ldots,P_{\\acute{n}-3},P_{\\acute{n}-2},P_{\\acute{n}-1},P_\\acute{n}], \\\\[10pt]\n& =F^{(\\acute{n})}(P_0 < P < P_\\acute{n})=\\sum_{TN=1}^{UT=\\infty}\\frac{F^{(\\acute{n})}(P_{(tn)})}{UT}\n \\\\[10pt]\n& =F^{(\\acute{n})}(LB < P < UB)=G^{(\\acute{n}-1)}(LB < P < UB)= \\cdots \n\\end{align}\n</math>\n\n==Applying the divided difference==\nThe quintessential application of the divided difference is in the presentation of the definite integral, which is nothing more than a finite difference:\n\n: <math>\n\\begin{align}\n\\int_{LB}^{UB} G(p) \\, dp & = \\int_{LB}^{UB} F'(p) \\, dp=F(UB)-F(LB), \\\\[10pt]\n& =F[LB,UB]\\Delta B, \\\\[10pt]\n& =F'(LB < P < UB)\\Delta B, \\\\[10pt]\n& =\\ G(LB < P < UB)\\Delta B.\n\\end{align}\n</math>\n\nGiven that the mean value, derivative expression form provides all of the same information as the classical integral notation, the mean value form may be the preferable expression, such as in writing venues that only support/accept standard [[ASCII]] text, or in cases that only require the average derivative (such as when finding the average radius in an elliptic integral).\nThis is especially true for definite integrals that technically have (e.g.) 0 and either <math>\\pi\\,\\!</math> or <math>2\\pi\\,\\!</math> as boundaries, with the same divided difference found as that with boundaries of 0 and <math>\\begin{matrix}\\frac{\\pi}{2}\\end{matrix}</math> (thus requiring less averaging effort):\n\n: <math>\n\\begin{align}\n\\int_0^{2\\pi} F'(p) \\, dp & =4\\int_0^{\\frac{\\pi}{2}} F'(p)\\, dp=F(2\\pi)-F(0)=4(F(\\begin{matrix}\\frac{\\pi}{2}\\end{matrix})-F(0)), \\\\[10pt]\n& =2\\pi F[0,2\\pi]=2\\pi F'(0 < P < 2\\pi), \\\\[10pt]\n& =2\\pi F[0,\\begin{matrix}\\frac{\\pi}{2}\\end{matrix}] =2\\pi F'(0 < P < \\begin{matrix}\\frac{\\pi}{2}\\end{matrix}).\n\\end{align}\n</math>\n\nThis also becomes particularly useful when dealing with ''iterated'' and [[multiple integral|''multiple integral''s]] (ΔA = AU − AL, ΔB = BU − BL, ΔC = CU − CL):\n\n: <math>\n\\begin{align}\n& {} \\qquad \\int_{CL}^{CU}\\int_{BL}^{BU} \\int_{AL}^{AU} F'(r,q,p)\\,dp\\,dq\\,dr \\\\[10pt]\n& =\\sum_{T\\!C=1}^{U\\!C=\\infty}\\left(\\sum_{T\\!B=1}^{U\\!B=\\infty}\n\\left(\\sum_{T\\!A=1}^{U\\!A=\\infty}F^{'}(R_{(tc)}:Q_{(tb)}:P_{(ta)})\\frac{\\Delta A}{U\\!A}\\right)\\frac{\\Delta B}{U\\!B}\\right)\\frac{\\Delta C}{U\\!C}, \\\\[10pt]\n& = F'(C\\!L < R < CU:BL < Q < BU:AL < P <\\!AU)\n\\Delta A\\,\\Delta B\\,\\Delta C.\n\\end{align}\n</math>\n\nHence,\n\n: <math>F'(R,Q:AL < P < AU)=\\sum_{T\\!A=1}^{U\\!A=\\infty}\n\\frac{F'(R,Q:P_{(ta)})}{U\\!A};\\,\\!</math>\n\nand\n:<math>F'(R:BL < Q < BU:AL < P < AU)=\\sum_{T\\!B=1}^{U\\!B=\\infty}\\left(\\sum_{T\\!A=1}^{U\\!A=\\infty}\\frac{F'(R:Q_{(tb)}:P_{(ta)})}{U\\!A}\\right)\\frac{1}{U\\!B}.\\,\\!</math>\n\n==See also==\n*[[Divided differences]]\n*[[Fermat theory]]\n*[[Newton polynomial]]\n*[[Rectangle method]]\n*[[Quotient rule]]\n*[[Symmetric difference quotient]]\n\n==References==\n{{reflist|2}}\n\n==External links==\n*[http://cis.stvincent.edu/carlsond/ma109/diffquot.html Saint Vincent College: Br. David Carlson, O.S.B.—''MA109 The Difference Quotient'']\n*[http://web.mat.bham.ac.uk/D.F.M.Hermans/msmxg6/ln/lnotes78.html University of Birmingham: Dirk Hermans—''Divided Differences'']\n*Mathworld:\n**[http://mathworld.wolfram.com/DividedDifference.html  ''Divided Difference'']\n**[http://mathworld.wolfram.com/Mean-ValueTheorem.html ''Mean-Value Theorem'']\n*[http://www.cs.wisc.edu/wpis/abstracts/tr1415r.abs.html University of Wisconsin: [[Thomas W. Reps]] and Louis B. Rall—''Computational Divided Differencing and Divided-Difference Arithmetics'']\n*[http://giraldi.org/derivata/derivata.html Interactive simulator on difference quotient to explain the derivative]\n\n{{Isaac Newton}}\n\n[[Category:Differential calculus]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Differential-algebraic system of equations",
      "url": "https://en.wikipedia.org/wiki/Differential-algebraic_system_of_equations",
      "text": "In [[mathematics]], a '''differential-algebraic system of equations''' ('''DAEs''') is a [[system of equations]] that either contains [[differential equation]]s and [[algebraic equation]]s, or is equivalent to such a system. Such systems occur as the general form of (systems of) [[differential equation]]s for vector–valued functions ''x'' in one independent variable ''t'',\n::<math>F(\\dot x(t),\\, x(t),\\,t)=0</math>\nwhere <math>x:[a,b]\\to\\R^n</math> is a vector of dependent variables <math>x(t)=(x_1(t),\\dots,x_n(t))</math> and the system has as many equations, <math>F=(F_1,\\dots,F_n):\\R^{2n+1}\\to\\R^n</math>.\nThey are distinct from [[ordinary differential equation]] (ODE) in that a DAE is not completely solvable for the derivatives of all components of the function ''x'' because these may not all appear (i.e. some equations are algebraic); technically the distinction between an implicit ODE system [that may be rendered explicit] and a DAE system is that the [[Jacobian matrix]] <math>\\frac{\\partial F(u, v, t)}{\\partial u}</math> is a [[singular matrix]] for a DAE system.<ref name=\"AscherPetzold1998\">{{cite book|author1=Uri M. Ascher|author2=Linda R. Petzold|author2-link=Linda Petzold|title=Computer Methods for Ordinary Differential Equations and Differential-Algebraic Equations|year=1998|publisher=SIAM|isbn=978-1-61197-139-2|page=12}}</ref> This distinction between ODEs and DAEs is made because DAEs have different characteristics and are generally more difficult to solve.<ref name=\"IlchmannReis2014\">{{cite book|author1=Achim Ilchmann|author2=Timo Reis|title=Surveys in Differential-Algebraic Equations II|year=2014|publisher=Springer|isbn=978-3-319-11050-9|pages=104–105}}</ref>\n\nIn practical terms, the distinction between DAEs and ODEs is often that the solution of a DAE system depends on the derivatives of the input signal and not just the signal itself as in the case of ODEs;<ref name=\"MerkerSchwarz2001\">{{cite book|editors=Renate Merker and Wolfgang Schwarz|title=System Design Automation: Fundamentals, Principles, Methods, Examples|year=2001|publisher=Springer Science & Business Media|isbn=978-0-7923-7313-1|page=221}}</ref> this issue is commonly encountered in systems with [[hysteresis]],<ref name=\"BrenanCampbell1996\">{{cite book|author1=K. E. Brenan|author2=S. L. Campbell|author3=L. R. Petzold|author3-link=Linda Petzold|title=Numerical Solution of Initial-value Problems in Differential-algebraic Equations|year=1996|publisher=SIAM|isbn=978-1-61197-122-4|pages=173–177}}</ref> such as the [[Schmitt trigger]].<ref>{{Cite book | doi = 10.1016/S1570-8659(04)13006-8| chapter = Modelling and Discretization of Circuit Problems| title = Numerical Methods in Electromagnetics| volume = 13| pages = 523| series = Handbook of Numerical Analysis| year = 2005| last1 = Günther | first1 = M. | last2 = Feldmann | first2 = U. | last3 = Ter Maten | first3 = J. | isbn = 978-0-444-51375-5}}, pp. 529-531</ref>\n\nThis difference is more clearly visible if the system may be rewritten so that instead of ''x'' we consider a pair <math>(x,y)</math> of vectors of dependent variables and the DAE has the form\n::<math>\\begin{align}\\dot x(t)&=f(x(t),y(t),t),\\\\0&=g(x(t),y(t),t).\\end{align}</math>\n:where <math>x(t)\\in\\R^n</math>, <math>y(t)\\in\\R^m</math>, <math>f:\\R^{n+m+1}\\to\\R^n</math> and <math>g:\\R^{n+m+1}\\to\\R^m.</math>\n\nA DAE system of this form is called ''semi-explicit''.<ref name=\"AscherPetzold1998\" /> Every solution of the second half ''g'' of the equation defines a unique direction for ''x'' via the first half ''f'' of the equations, while the direction for ''y'' is arbitrary. But not every point ''(x,y,t)'' is a solution of ''g''. The variables in ''x'' and the first half ''f'' of the equations get the attribute ''differential''. The components of ''y'' and the second half ''g'' of the equations are called the ''algebraic'' variables or equations of the system. [The term ''algebraic'' in the context of DAEs only means ''free of derivatives'' and is not related to (abstract) algebra.]\n\nThe solution of a DAE consists of two parts, first the search for consistent initial values and second the computation of a trajectory. To find consistent initial values it is often necessary to consider the derivatives of some of the component functions of the DAE. The highest order of a derivative that is necessary for this process is called the ''differentiation index''. The equations derived in computing the index and consistent initial values may also be of use in the computation of the trajectory. A semi-explicit DAE system can be converted to an implicit one by decreasing the differentiation index by one, and vice versa.<ref>Ascher and Petzold, p. 234</ref>\n\n== Other forms of DAEs ==\nThe distinction of DAEs to ODEs becomes apparent if some of the dependent variables occur without their derivatives. The vector of dependent variables may then be written as pair <math>(x,y)</math> and the system of differential equations of the DAE appears in the form\n::<math> F\\left(\\dot x, x, y, t\\right) = 0 </math>\nwhere\n* <math>x</math>, a vector in <math>\\R^n</math>, are dependent variables for which derivatives are present (''differential variables''),\n* <math>y</math>, a vector in <math>\\R^m</math>, are dependent variables for which no derivatives are present (''algebraic variables''),\n* <math>t</math>, a scalar (usually time) is an independent variable.\n* <math>F</math> is a vector of <math>n+m</math> functions that involve subsets of these <math>n+m</math> variables and <math>n</math> derivatives.\n\nAs a whole, the set of DAEs is a function\n::<math> F: \\R^{(2n+m+1)} \\to \\R^{(n+m)}. </math>\n\nInitial conditions must be a solution of the system of equations of the form\n::<math> F\\left(\\dot x(t_0),\\, x(t_0), y(t_0), t_0 \\right) = 0. </math>\n\n== Examples ==\nThe pendulum in Cartesian coordinates ''(x,y)'' with center in ''(0,0)'' and length ''L'' has the [[Euler–Lagrange equation]]s\n::<math>\\begin{align}\n\\dot x&=u,&\\dot y&=v,\\\\\n\\dot u&=\\lambda x,&\\dot v&=\\lambda y-g,\\\\\nx^2+y^2&=L^2,\n\\end{align}</math>\nwhere <math>\\lambda</math> is a [[Lagrange multiplier]]. The momentum variables ''u'' and ''v'' should be constrained by the law of conservation of energy and their direction should point along the circle. Neither condition is explicit in those equations. Differentiation of the last equation leads to\n::<math>\\begin{align}\n&&\\dot x\\,x+\\dot y\\,y&=0\\\\\n\\Rightarrow&& u\\,x+v\\,y&=0,\n\\end{align}</math>\nrestricting the direction of motion to the tangent of the circle. The next derivative of this equation implies\n::<math>\\begin{align}\n&&\\dot u\\,x+\\dot v\\,y+u\\,\\dot x+v\\,\\dot y&=0,\\\\\n\\Rightarrow&& \\lambda(x^2+y^2)-gy+u^2+v^2&=0,\\\\\n\\Rightarrow&& L^2\\,\\lambda-gy+u^2+v^2&=0,\n\\end{align}</math>\nand the derivative of that last identity simplifies to <math>L^2\\dot\\lambda-3gv=0</math> which implicitly implies the conservation of energy since after integration the constant <math>E=\\tfrac32gy-\\tfrac12L^2\\lambda=\\frac12(u^2+v^2)+gy</math> is the sum of kinetic and potential energy.\n\nTo obtain unique derivative values for all dependent variables the last equation was three times differentiated. This gives a differentiation index of 3, which is typical for constrained mechanical systems.\n\nIf initial values <math>(x_0,u_0)</math> and a sign for ''y'' are given, the other variables are determined via <math>y=\\pm\\sqrt{L^2-x^2}</math>, and if <math>y\\ne0</math> then <math>v=-ux/y</math> and <math>\\lambda=(gy-u^2-v^2)/L^2</math>. To proceed to the next point it is sufficient to get the derivatives of ''x'' and ''u'', that is, the system to solve is now\n\n:: <math>\\begin{align}\n\n\\dot x&=u,\\\\\n\\dot u&=\\lambda x,\\\\[0.3em]\n0&=x^2+y^2-L^2,\\\\\n0&=ux+vy,\\\\\n0&=u^2-gy+v^2+L^2\\,\\lambda.\n\\end{align}</math>\nThis is a semi-explicit DAE of index 1. Another set of similar equations may be obtained starting from <math>(y_0,v_0)</math> and a sign for ''x''.\n\nDAEs also naturally occur in the modelling of circuits with non-linear devices. [[Modified nodal analysis]] employing DAEs is used for example in the ubiquitous [[SPICE]] family of numeric circuit simulators.<ref name=\"IlchmannReis2013\">{{cite book|editors=Achim Ilchmann and Timo Reis|title=Surveys in Differential-Algebraic Equations I|year=2013|publisher=Springer Science & Business Media|isbn=978-3-642-34928-7|author=Ricardo Riaza|chapter=DAEs in Circuit Modelling: A Survey}}</ref> Similarly, [[Fraunhofer Society|Fraunhofer's]] [[Analog Insydes]] [[Mathematica]] package can be used to derive DAEs from a [[netlist]] and then simplify or even solve the equations symbolically in some cases.<ref>{{Cite book | doi = 10.1007/978-1-4020-6149-3_4| chapter = Improving Efficiency and Robustness of Analog Behavioral Models| title = Advances in Design and Specification Languages for Embedded Systems| pages = 53| year = 2007| last1 = Platte | first1 = D. | last2 = Jing | first2 = S. | last3 = Sommer | first3 = R. | last4 = Barke | first4 = E. | isbn = 978-1-4020-6147-9}}</ref><ref>{{Cite book | doi = 10.1007/978-3-642-23568-9_17| chapter = Fast and Robust Symbolic Model Order Reduction with Analog Insydes| title = Computer Algebra in Scientific Computing| volume = 6885| pages = 215| series = Lecture Notes in Computer Science| year = 2011| last1 = Hauser | first1 = M. | last2 = Salzig | first2 = C. | last3 = Dreyer | first3 = A. | isbn = 978-3-642-23567-2}}</ref> It is worth noting that the index of a DAE (of a circuit) can be made arbitrarily high by cascading/coupling via capacitors [[operational amplifiers]] with [[positive feedback]].<ref name=\"BrenanCampbell1996\" />\n\n== Semi-explicit DAE of index 1 ==\nDAE of the form\n::<math>\\begin{align}\\dot x&=f(x,y,t),\\\\0&=g(x,y,t).\\end{align}</math>\nare called semi-explicit. The index-1 property requires that ''g'' is [[implicit function theorem|solvable]] for ''y''. In other words, the differentiation index is 1 if by differentiation of the algebraic equations for ''t'' an implicit ODE system results,\n::<math>\\begin{align}\n\\dot x&=f(x,y,t)\\\\\n0&=\\partial_x g(x,y,t)\\dot x+\\partial_y g(x,y,t)\\dot y+\\partial_t g(x,y,t),\n\\end{align}</math>\nwhich is solvable for <math>(\\dot x,\\,\\dot y)</math> if <math>\\det\\left(\\partial_y g(x,y,t)\\right)\\ne 0.</math>\n\nEvery sufficiently smooth DAE is almost everywhere reducible to this semi-explicit index-1 form.\n\n== Numerical treatment of DAE and applications ==\n\nTwo major problems in the solution of DAEs are ''index reduction'' and ''consistent initial conditions''. Most numerical solvers require [[ordinary differential equations]] and [[algebraic equations]] of the form\n\n::<math>\\begin{align}\\frac{dx}{dt}&=f\\left(x,y,t\\right),\\\\0&=g\\left(x,y,t\\right).\\end{align}</math>\n\nIt is a non-trivial task to convert arbitrary DAE systems into ODEs for solution by pure ODE solvers. Techniques which can be employed include ''[[Pantelides algorithm]]'' and ''[[dummy derivative index reduction method]]''. Alternatively, a direct solution of high-index DAEs with inconsistent initial conditions is also possible. This solution approach involves a transformation of the derivative elements through ''orthogonal collocation on finite elements'' or ''direct transcription'' into algebraic expressions. This allows DAEs of any index to be solved without rearrangement in the open equation form\n\n::<math>\\begin{align}0&=f\\left(\\frac{dx}{dt},x,y,t\\right),\\\\0&=g\\left(x,y,t\\right).\\end{align}</math>\n\nOnce the model has been converted to algebraic equation form, it is solvable by large-scale nonlinear programming solvers (see [[APMonitor]]).\n\n=== Tractability ===\n{{expand section|date=December 2014}}\nSeveral measures of DAEs tractability in terms of numerical methods have developed, such as ''differentiation index'', ''perturbation index'', ''tractability index'', ''geometric index'', and the ''Kronecker index''.<ref name=\"Riaza2008\">{{cite book|author=Ricardo Riaza|title=Differential-algebraic Systems: Analytical Aspects and Circuit Applications|year=2008|publisher=World Scientific|isbn=978-981-279-181-8|pages=5–8}}</ref><ref>http://www.ise.chuo-u.ac.jp/ise-labs/takamatsu-lab/takamatsu/metr/METR08-10.pdf</ref>\n\n== Structural analysis for DAEs ==\nWe use the <math>\\Sigma</math>-method to analyze a DAE. We construct for the DAE a signature matrix <math>\\Sigma=(\\sigma_{i,j})</math>, where each row corresponds to each equation <math>f_i</math> and each column corresponds to each variable <math>x_j</math>. The entry in position <math>(i,j)</math> is <math>\\sigma_{i,j}</math>, which denotes the highest order of derivative to which <math>x_j</math> occurs in <math>f_i</math>, or <math>-\\infty</math> if <math>x_j</math> does not occur in <math>f_i</math>.\n\nFor the pendulum DAE above, the variables are <math>(x_1,x_2,x_3,x_4,x_5)=(x,y,u,v,\\lambda)</math>. The corresponding signature matrix is\n:<math>\\Sigma =\n\\begin{bmatrix}\n1 & - & 0^\\bullet & - & - \\\\\n- & 1^\\bullet & - & 0 & - \\\\\n0 & - & 1 & - & 0^\\bullet \\\\\n- & 0 & - & 1^\\bullet & 0 \\\\\n0^\\bullet & 0 & - & - & -\n\\end{bmatrix}\n</math>\n\n== See also ==\n* [[Algebraic differential equation]], a different concept despite the similar name\n* [[Delay differential equation]]\n* [[Partial differential algebraic equation]]\n* [[Modelica]] Language\n\n== References ==\n{{reflist}}\n\n== Further reading ==\n{{div col|colwidth=30em}}\n\n=== Books ===\n* {{cite book\n|last1 = Hairer\n|first1 = E.\n|last2 = Wanner\n|first2 = G.\n|title = Solving Ordinary Differential Equations II: Stiff and Differential-Algebraic Problems\n|publisher = Springer-Verlag\n|location = Berlin\n|edition = 2nd revised\n|year = 1996\n|isbn = }}\n* {{cite book\n|first1 = Uri M.\n|last1 = Ascher\n|first2 = Linda R.\n|last2 = Petzold|author2-link=Linda Petzold\n|title = Computer Methods for Ordinary Differential equations and Differential-Algebraic equations\n|publisher = SIAM\n|location = Philadelphia\n|year = 1998\n|isbn = 978-0-89871-412-8\n}}\n* {{cite book\n|url = https://books.google.com/books?id=iRZPqCwkI_IC\n|title = Differential-algebraic equations: analysis and numerical solution\n|first1=Peter\n|last1=Kunkel\n|first2=Volker Ludwig\n|last2=Mehrmann\n|publisher = European Mathematical Society\n|location=Zürich, Switzerland\n|year=2006\n|isbn=978-3-03719-017-3\n}}\n* {{cite book|author=Kazuo Murota|title=Matrices and Matroids for Systems Analysis|year=2009|publisher=Springer Science & Business Media|isbn=978-3-642-03994-2}} (Covers the structural approach to computing the DAE index.)\n* {{cite book|author=Matthias Gerdts|title=Optimal Control of ODEs and DAEs|year=2012|publisher=Walter de Gruyter|isbn=978-3-11-024999-6}}\n* {{cite book\n|first1 = René\n|last1 = Lamour\n|first2 = Roswitha\n|last2 = März\n|first3 = Caren\n|last3 = Tischendorf\n|title = Differential-Algebraic equations: a Projector based analysis\n|publisher = Springer\n|location = Heidelberg\n|year = 2013\n|isbn = 978-3-642-27554-8\n}}\n\n=== Various papers ===\n* {{cite journal\n |url         = http://se.wtb.tue.nl/~vanbeek/pub/mcmds01.pdf\n |title       = Index Reduction and Discontinuity Handling using Substitute Equations\n |author1     = G. Fábián\n |author2     = D.A. van Beek\n |author3     = J.E. Rooda\n |journal     = Mathematical and Computer Modelling of Dynamical Systems\n |volume      = 7\n |number      = 2\n |year        = 2001\n |pages       = 173–187\n |doi         = 10.1076/mcmd.7.2.173.3646\n |deadurl     = yes\n |archiveurl  = https://web.archive.org/web/20050426090003/http://se.wtb.tue.nl/~vanbeek/pub/mcmds01.pdf\n |archivedate = 2005-04-26\n |df          = \n|citeseerx       = 10.1.1.8.5859\n }}\n* {{cite journal\n|first1 = Silvana\n|last1 = Ilie\n|first2 = Robert M.\n|last2 = Corless\n|first3 = Greg\n|last3 = Reid\n|title = Numerical Solutions of Differential Algebraic Equations of Index −1 Can Be Computed in Polynomial Time\n|journal = Numerical Algorithms\n|volume = 41\n|number = 2\n|year = 2006\n|pages = 161–171\n|doi=10.1007/s11075-005-9007-1\n|citeseerx = 10.1.1.71.7366\n}}\n* {{cite journal\n|url = http://www.cas.mcmaster.ca/~nedialk/PAPERS/DAEs/taylcoeff_I/taylorcoeffsdae.pdf\n|first1 = Ned S.\n|last1 = Nedialkov\n|first2 = John D.\n|last2 = Pryce\n|title = Solving Differential-Algebraic Equations by Taylor Series (I): Computing Taylor Coefficients\n|journal = BIT\n|year = 2005\n|doi=10.1007/s10543-005-0019-y\n|volume=45\n|issue = 3\n|pages=561–591\n}}\n* {{cite journal\n|url = http://www.cas.mcmaster.ca/~nedialk/PAPERS/DAEs/taylcoeff_II/sysjac.pdf\n|first1 = Ned S.\n|last1 = Nedialkov\n|first2 = John D.\n|last2 = Pryce\n|title = Solving Differential-Algebraic Equations by Taylor Series (II): Computing the System Jacobian\n|journal = BIT\n|year = 2005\n|doi=10.1007/s10543-006-0106-8\n|volume=47\n|pages=121–135\n|citeseerx = 10.1.1.455.6965\n}}\n* {{cite journal\n|url = http://www.cas.mcmaster.ca/~nedialk/PAPERS/DAEs/daets/daets_color.pdf\n|first1 = Ned S.\n|last1 = Nedialkov\n|first2 = John D.\n|last2 = Pryce\n|title = Solving Differential-Algebraic Equations by Taylor Series (III): the DAETS Code\n|journal = Journal of Numerical Analysis, Industrial and Applied Mathematics (JNAIAM)\n|volume = 1\n|number = 1\n|year = 2007\n|issn = 1790-8140\n|pages = 1–30\n}}\n* {{cite journal\n|url = http://www.cas.mcmaster.ca/~nedialk/PAPERS/DAEs/daesa_software/daesaSoftware.pdf\n|first1 = Ned S.\n|last1 = Nedialkov\n|first2 = John D.\n|last2 = Pryce\n|first3 = Guangning\n|last3 = Tan\n|title = DAESA — a Matlab Tool for Structural Analysis of Differential-Algebraic Equations: Software\n|journal = ACM Transactions on Mathematical Software\n|volume =41\n|issue = 2\n|year = 2014\n|doi =10.1145/2700586\n|pages =1–14\n}}\n* {{cite journal\n|url = http://www.cas.mcmaster.ca/~nedialk/PAPERS/DAEs/daesa_theory/daesaTheory.pdf\n|first1 = John D.\n|last1 = Pryce\n|first2 = Ned S.\n|last2 = Nedialkov\n|first3 = Guangning\n|last3 = Tan\n|title = DAESA — a Matlab Tool for Structural Analysis of Differential-Algebraic Equations: Algorithm\n|journal = ACM Transactions on Mathematical Software\n|volume =41\n|issue = 2\n|year = 2014\n|doi = 10.1145/2689664\n|pages =1–20\n}}\n* {{cite journal\n|first1 = T.\n|last1 = Roubíček\n|first2 = M.\n|last2 = Valášek\n|title = Optimal control of causal differential algebraic systems\n|journal  = J. Math. Anal. Appl.\n|volume = 269 |issue = 2\n|year = 2002 |pages = 616–641\n |doi=10.1016/s0022-247x(02)00040-9\n}}\n{{div col end}}\n\n== External links ==\n* http://www.scholarpedia.org/article/Differential-algebraic_equations\n\n{{Differential equations topics}}\n[[Category:Differential equations]]\n[[Category:Numerical analysis]]\n[[Category:Differential calculus]]"
    },
    {
      "title": "Discrete Fourier transform",
      "url": "https://en.wikipedia.org/wiki/Discrete_Fourier_transform",
      "text": "{{distinguish|text=the [[discrete-time Fourier transform]]}}\n{{Fourier transforms}}\n\n[[File:From Continuous To Discrete Fourier Transform.gif|thumb|400px|Relationship between the (continuous) [[Fourier transform]] and the discrete Fourier transform. <u>Left column:</u> A continuous function (top) and its Fourier transform (bottom). <u>Center-left column:</u> [[Periodic summation]] of the original function (top).  Fourier transform (bottom) is zero except at discrete points.  The inverse transform is a sum of sinusoids called [[Fourier series]].  <u>Center-right column:</u> Original function is discretized (multiplied by a [[Dirac comb]]) (top).  Its Fourier transform (bottom) is a periodic summation ([[Discrete-time Fourier transform|DTFT]]) of the original transform. <u>Right column:</u> The DFT (bottom) computes discrete samples of the continuous DTFT.  The inverse DFT (top) is a periodic summation of the original samples.  The [[Fast Fourier transform|FFT]] algorithm computes one cycle of the DFT and its inverse is one cycle of the DFT inverse.]]\n\n[[File:Fourier transform, Fourier series, DTFT, DFT.svg|thumb|400px|Depiction of a Fourier transform (upper left) and its periodic summation (DTFT) in the lower left corner.  The spectral sequences at (a) upper right and (b) lower right are respectively computed from (a) one cycle of the periodic summation of s(t) and (b) one cycle of the periodic summation of the s(nT) sequence.  The respective formulas are (a) the [[Fourier series]] <u>integral</u> and (b) the '''DFT''' <u>summation</u>.  Its similarities to the original transform, S(f), and its relative computational ease are often the motivation for computing a DFT sequence.]]\n\nIn [[mathematics]], the '''discrete Fourier transform''' ('''DFT''') converts a finite sequence of equally-spaced [[Sampling (signal processing)|samples]] of a [[function (mathematics)|function]] into a same-length sequence of equally-spaced samples of the [[discrete-time Fourier transform]] (DTFT), which is a [[complex number|complex-valued]] function of frequency. The interval at which the DTFT is sampled is the reciprocal of the duration of the input sequence.  An inverse DFT is a [[Fourier series]], using the DTFT samples as coefficients of [[complex number|complex]] [[Sine wave|sinusoid]]s at the corresponding DTFT frequencies.  It has the same sample-values as the original input sequence.  The DFT is therefore said to be a [[frequency domain]] representation of the original input sequence.  If the original sequence spans all the non-zero values of a function, its DTFT is continuous (and periodic), and the DFT provides discrete samples of one cycle.  If the original sequence is one cycle of a periodic function, the DFT provides all the non-zero values of one DTFT cycle.\n\nThe DFT is the most important [[discrete transform]], used to perform [[Fourier analysis]] in many practical applications.<ref>{{cite journal|last=Strang|first=Gilbert|title=Wavelets|journal=American Scientist|date=May–June 1994|volume=82|issue=3|pages=250–255|jstor=29775194|quote=This is the most important numerical algorithm of our lifetime...}}</ref>  In [[digital signal processing]], the function is any quantity or [[signal (information theory)|signal]] that varies over time, such as the pressure of a [[sound wave]], a [[radio]] signal, or daily [[temperature]] readings, sampled over a finite time interval (often defined by a [[window function]]<ref>{{cite journal|last=Sahidullah|first=Md.|author2=Saha, Goutam|title=A Novel Windowing Technique for Efficient Computation of MFCC for Speaker Recognition|journal=IEEE Signal Processing Letters|date=Feb 2013|volume=20|issue=2|pages=149–152|doi=10.1109/LSP.2012.2235067|arxiv = 1206.2437 |bibcode = 2013ISPL...20..149S }}</ref>). In [[image processing]], the samples can be the values of [[pixel]]s along a row or column of a [[raster image]]. The DFT is also used to efficiently solve [[partial differential equations]], and to perform other operations such as [[convolution]]s or multiplying large integers.\n\nSince it deals with a finite amount of data, it can be implemented in [[computer]]s by [[numerical algorithm]]s or even dedicated [[digital circuit|hardware]]. These implementations usually employ efficient [[fast Fourier transform]] (FFT) algorithms;<ref name=colley>Cooley et al., 1969</ref> so much so that the terms \"FFT\" and \"DFT\" are often used interchangeably.  Prior to its current usage, the \"FFT\" [[initialism]] may have also been used for the ambiguous term \"[[Finite Fourier transform (disambiguation)|finite Fourier transform]]\".\n\n==Definition==\nThe ''discrete Fourier transform'' transforms a [[sequence]] of ''N'' [[complex number]]s <math>   \\left \\{ \\mathbf{ x_n } \\right \\} := x_0, x_1, \\ldots, x_{N-1}</math> into another sequence of complex numbers,  <math>\\left \\{ \\mathbf{X_k} \\right \\} := X_0, X_1, \\ldots, X_{N-1},</math> which is defined by\n\n{{Equation box 1\n|indent =\n|title=\n|equation = {{NumBlk|:|<math>\\begin{align}\nX_k &= \\sum_{n=0}^{N-1} x_n\\cdot e^{-\\frac {i 2\\pi}{N}kn}\\\\\n&= \\sum_{n=0}^{N-1} x_n\\cdot [\\cos(2 \\pi k n / N) - i\\cdot \\sin(2 \\pi k n / N)],\n\\end{align}</math>|{{EquationRef|Eq.1}}}}\n|cellpadding= 6\n|border\n|border colour = #0073CF\n|background colour=#F5FFFA}}\n\nwhere the last expression follows from the first one by [[Euler's formula]].\n\nThe transform is sometimes denoted by the symbol <math>\\mathcal{F}</math>, as in <math>\\mathbf{X} = \\mathcal{F} \\left \\{ \\mathbf{x} \\right \\} </math> or <math>\\mathcal{F} \\left ( \\mathbf{x} \\right )</math> or <math>\\mathcal{F} \\mathbf{x}</math>.<ref group=\"note\">As a [[linear transformation]] on a [[Dimension (vector space)|finite-dimensional vector space]], the DFT expression can also be written in terms of a [[DFT matrix]]; when scaled appropriately it becomes a [[unitary matrix]] and the ''X''<sub>''k''</sub> can thus be viewed as coefficients of ''x'' in an [[orthonormal basis]].</ref>\n\n==Motivation==\n{{EquationNote|Eq.1}} can also be evaluated outside the domain <math>k \\in [0,N-1]</math>, and that extended sequence is <math>N</math>-[[periodic sequence|periodic]]. Accordingly, other sequences of <math>N</math> indices are sometimes used,  such as <math>\\left[-\\tfrac{N}{2},\\tfrac{N}{2}-1\\right]</math> (if <math>N</math> is even) and <math>\\left[-\\tfrac{N-1}{2},\\tfrac{N-1}{2}\\right]</math> (if <math>N</math> is odd), which amounts to swapping the left and right halves of the result of the transform.\n<ref>{{cite web|title=Shift zero-frequency component to center of spectrum – MATLAB fftshift|url=http://www.mathworks.com/help/matlab/ref/fftshift.html|website=mathworks.com|publisher=The MathWorks, Inc.|accessdate=10 March 2014|location=Natick, MA 01760}}</ref>\n\n{{EquationNote|Eq.1}} can be interpreted or derived in various ways, for example:\n*It completely describes the [[discrete-time Fourier transform]] (DTFT) of an <math>N</math>-periodic sequence, which comprises only discrete frequency components.  ([[Discrete-time Fourier transform#Periodic data|Using the DTFT with periodic data]])\n*It can also provide uniformly spaced samples of the continuous DTFT of a finite length sequence.  ([[Discrete-time Fourier transform#Sampling the DTFT|Sampling the DTFT]])\n*It is the [[cross correlation]] of the ''input'' sequence, <math>x_n</math>, and a complex sinusoid at frequency <math>k/N</math>. &nbsp;Thus it acts like a [[matched filter]] for that frequency.\n*It is the discrete analog of the formula for the coefficients of a [[Fourier series]]:\n\n{{NumBlk|::|<math>x_n = \\frac{1}{N} \\sum_{k=0}^{N-1} X_k \\cdot e^{i 2 \\pi k n / N},  \\quad n\\in\\mathbb{Z},\\,</math>|{{EquationRef|Eq.2}}}}\n\n:which is also <math>N</math>-periodic.&nbsp; In the domain &nbsp;{{math|''n'' ∈ [0,&nbsp;''N''&nbsp;−&nbsp;1]}},&nbsp; this is the '''inverse transform''' of {{EquationNote|Eq.1}}.&nbsp; In this interpretation, each <math>X_k</math> is a complex number that encodes both amplitude and phase of a complex sinusoidal component &nbsp;<math>(e^{i 2 \\pi k n / N})</math>&nbsp; of function <math>x_n.</math>  The sinusoid's [[frequency]] is ''k'' cycles per ''N'' samples.&nbsp; Its amplitude and phase are:\n\n::<math>|X_k|/N = \\sqrt{\\operatorname{Re}(X_k)^2 + \\operatorname{Im}(X_k)^2}/N</math>\n::<math>\\arg(X_k) = \\operatorname{atan2}\\big( \\operatorname{Im}(X_k), \\operatorname{Re}(X_k) \\big)=-i\\cdot \\operatorname{ln}\\left(\\frac{X_k}{|X_k|}\\right),</math>\n\n:where [[atan2]] is the two-argument form of the [[arctan]] function. In polar form <math>X_k = |X_k| e^{i \\arg(X_k)} = |X_k| \\operatorname{cis} \\arg(X_k)</math> where [[Cis (mathematics)|cis]] is the mnemonic for&nbsp;cos&nbsp;+&nbsp;''i''&nbsp;sin.\n\nThe normalization factor multiplying the DFT and IDFT (here 1 and 1/''N'') and the signs of the exponents are merely [[sign convention|conventions]], and differ in some treatments. The only requirements of these conventions are that the DFT and IDFT have opposite-sign exponents and that the product of their normalization factors be <math>1/N</math>. &nbsp;A normalization of <math>\\scriptstyle \\sqrt{1/N}</math> for both the DFT and IDFT, for instance, makes the transforms unitary. A discrete impulse, <math>x_n=1</math> at ''n''&nbsp;=&nbsp;0 and 0 otherwise; might transform to <math>X_k=1</math> for all ''k'' (use normalization factors 1 for DFT and 1/''N'' for IDFT). A DC signal, <math>X_k = 1</math> at ''k''&nbsp;=&nbsp;0 and 0 otherwise; might inversely transform to <math>x_n=1</math> for all <math>n</math> (use <math>1/N</math> for DFT and 1 for IDFT) which is consistent with viewing [[Direct current|DC]] as the [[Mean#Arithmetic mean (AM)|mean average]] of the signal.\n\n==Example==\nLet <math>N=4</math> and\n\n: <math>\\mathbf{x} =\n\\begin{pmatrix}\nx_0 \\\\ x_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 \\\\ 2-i \\\\ -i \\\\ -1+2i\n\\end{pmatrix}\n</math>\n\nHere we demonstrate how to calculate the DFT of <math>\\mathbf{x}</math> using {{EquationNote|Eq.1}}:\n\n: <math>X_0 = e^{-i 2 \\pi 0 \\cdot 0 / 4} \\cdot 1 + e^{-i 2 \\pi 0 \\cdot 1 / 4} \\cdot (2-i) + e^{-i 2 \\pi 0 \\cdot 2 / 4} \\cdot (-i) + e^{-i 2 \\pi 0 \\cdot 3 / 4} \\cdot (-1+2i) = 2</math>\n\n: <math>X_1 = e^{-i 2 \\pi 1 \\cdot 0 / 4} \\cdot 1 + e^{-i 2 \\pi 1 \\cdot 1 / 4} \\cdot (2-i) + e^{-i 2 \\pi 1 \\cdot 2 / 4} \\cdot (-i) + e^{-i 2 \\pi 1 \\cdot 3 / 4} \\cdot (-1+2i) = -2-2i</math>\n\n: <math>X_2 = e^{-i 2 \\pi 2 \\cdot 0 / 4} \\cdot 1 + e^{-i 2 \\pi 2 \\cdot 1 / 4} \\cdot (2-i) + e^{-i 2 \\pi 2 \\cdot 2 / 4} \\cdot (-i) + e^{-i 2 \\pi 2 \\cdot 3 / 4} \\cdot (-1+2i) = -2i</math>\n\n: <math>X_3 = e^{-i 2 \\pi 3 \\cdot 0 / 4} \\cdot 1 + e^{-i 2 \\pi 3 \\cdot 1 / 4} \\cdot (2-i) + e^{-i 2 \\pi 3 \\cdot 2 / 4} \\cdot (-i) + e^{-i 2 \\pi 3 \\cdot 3 / 4} \\cdot (-1+2i) = 4+4i</math>\n\n: <math>\\mathbf{X} =\n\\begin{pmatrix}\nX_0 \\\\\nX_1 \\\\\nX_2 \\\\\nX_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n2 \\\\\n-2-2i \\\\\n-2i \\\\\n4+4i\n\\end{pmatrix}\n</math>\n\n==Inverse transform==\nThe discrete Fourier transform is an invertible, [[linear transformation]]\n:<math>\\mathcal{F}\\colon\\mathbb{C}^N \\to \\mathbb{C}^N</math>\nwith <math>\\mathbb{C}</math> denoting the set of [[complex number]]s. This is known as Inverse Discrete Fourier Transform(IDFT). In other words, for any <math>N>0</math>, an ''N''-dimensional complex vector has a DFT and an IDFT which are in turn <math>N</math>-dimensional complex vectors.\n\nThe inverse transform is given by:\n{{Equation box 1\n|indent =\n|title=\n|equation = {{NumBlk|:|<math>\nx_n = \\frac{1}{N} \\sum_{k=0}^{N-1} X_k\\cdot e^{i 2 \\pi k n / N}\n</math>|{{EquationRef|Eq.3}}}}\n|cellpadding= 6\n|border\n|border colour = #0073CF\n|background colour=#F5FFFA}}\n\n==Properties==\n\n===Linearity===\nThe DFT is a linear transform, i.e. if <math>\\mathcal{F}(\\{x_n\\})_k=X_k</math> and <math>\\mathcal{F}(\\{y_n\\})_k=Y_k</math>, then for any complex numbers <math>a,b</math>:\n:<math>\\mathcal{F}(\\{a x_n + b y_n\\})_k=a X_k + b Y_k</math>\n\n===Time and frequency reversal===\nReversing the time (i.e. replacing <math>n</math> by <math>N-n</math>)<ref group=note>Time reversal for the DFT means replacing <math>n</math> by <math>N-n</math> and not <math>n</math> by <math>-n</math> to avoid negative indices.</ref> in <math>x_n</math> corresponds to reversing the frequency (i.e. <math>k</math> by <math>N-k</math>).<ref name=ProakisManolakis>{{Citation | last =Proakis | first =John G. | last2 =Manolakis | first2 =Dimitri G. | title =Digital Signal Processing: Principles, Algorithms and Applications | place =Upper Saddle River, NJ | publisher =Prentice-Hall International | year =1996 | edition =3 | language =English | id =sAcfAQAAIAAJ | isbn =9780133942897}}</ref>{{rp|p. 421}} Mathematically, if <math>\\{x_n\\}</math> represents the vector '''x''' then\n\n:if <math>\\mathcal{F}(\\{x_n\\})_k=X_k</math>\n:then <math>\\mathcal{F}(\\{ x_{N-n} \\})_k=X_{N-k}</math>\n\n===Conjugation in time===\nIf <math>\\mathcal{F}(\\{x_n\\})_k=X_k</math> then <math>\\mathcal{F}(\\{ x_n^* \\})_k=X_{N-k}^*</math>.<ref name=ProakisManolakis/>{{rp|p. 423}}\n\n===Real and imaginary part===\nThis table shows some mathematical operations on <math>x_n</math> in the time domain and the corresponding effects on its DFT <math>X_k</math> in the frequency domain.\n\n{| class=\"wikitable\"\n|-\n! Property\n! Time domain<br/><math>x_n</math>\n! Frequency domain<br/><math>X_k</math>\n|-\n| Real part in time\n| <math>\\Re{(x_n)}</math>\n| <math>\\frac{1}{2}(X_k + X^*_{N-k})</math>\n|-\n| Imaginary part in time\n| <math>\\Im{(x_n)}</math>\n| <math>\\frac{1}{2i}(X_k - X^*_{N-k})</math>\n|-\n| Real part in frequency\n| <math>\\frac{1}{2}(x_n+x^*_{N-n})</math>\n| <math>\\Re{(X_k)}</math>\n|-\n| Imaginary part in frequency\n| <math>\\frac{1}{2i}(x_n-x^*_{N-n})</math>\n| <math>\\Im{(X_k)}</math>\n|}\n\n=== Orthogonality ===\nThe vectors <math>u_k=\\left[ e^{ \\frac{i 2\\pi}{N} kn} \\;|\\; n=0,1,\\ldots,N-1 \\right]^T</math> \nform an [[orthogonal basis]] over the set of ''N''-dimensional complex vectors:\n\n:<math>u^T_k u_{k'}^* \n = \\sum_{n=0}^{N-1} \\left(e^{ \\frac{i 2\\pi}{N} kn}\\right) \\left(e^{\\frac{i 2\\pi}{N} (-k')n}\\right)\n = \\sum_{n=0}^{N-1} e^{ \\frac{i 2\\pi}{N} (k-k') n} \n = N~\\delta_{kk'}\n</math>\n\nwhere <math>~\\delta_{kk'}</math> is the [[Kronecker delta]]. (In the last step, the summation is trivial if <math>k=k'</math>, where it is 1+1+⋅⋅⋅=''N'', and otherwise is a [[geometric series]] that can be explicitly summed to obtain zero.)  This orthogonality condition can be used to derive the formula for the IDFT from the definition of the DFT, and is equivalent to the unitarity property below.\n\n=== The Plancherel theorem and Parseval's theorem ===\nIf <math>X_k</math> and <math>Y_k</math> are the DFTs of <math>x_n</math> and <math>y_n</math> respectively then the [[Parseval's theorem]] states:\n\n:<math>\\sum_{n=0}^{N-1} x_n y^*_n = \\frac{1}{N} \\sum_{k=0}^{N-1} X_k Y^*_k</math>\n\nwhere the star denotes [[Complex conjugate|complex conjugation]].  [[Plancherel theorem]] is a special case of the Parseval's theorem and states:\n\n:<math>\\sum_{n=0}^{N-1} |x_n|^2 = \\frac{1}{N} \\sum_{k=0}^{N-1} |X_k|^2.</math>\n\nThese theorems are also equivalent to the unitary condition below.\n\n===Periodicity===\nThe periodicity can be shown directly from the definition:\n\n: <math>X_{k+N} \\ \\triangleq \\ \\sum_{n=0}^{N-1} x_n e^{-\\frac{i 2\\pi}{N} (k+N) n} =\n\\sum_{n=0}^{N-1} x_n e^{-\\frac{i 2\\pi}{N} k n}  \\underbrace{e^{-i 2 \\pi n}}_{1} = \\sum_{n=0}^{N-1} x_n e^{-\\frac{i 2\\pi}{N} k n} = X_k. </math>\n\nSimilarly, it can be shown that the IDFT formula leads to a periodic extension.\n\n===Shift theorem===\nMultiplying <math>x_n</math> by a ''linear phase'' <math>e^{\\frac{i 2\\pi}{N}n m}</math> for some integer ''m'' corresponds to a ''circular shift'' of the output <math>X_k</math>: <math>X_k</math> is replaced by <math>X_{k-m}</math>, where the subscript is interpreted [[modular arithmetic|modulo]] ''N'' (i.e., periodically).  Similarly, a circular shift of the input <math>x_n</math> corresponds to multiplying the output <math>X_k</math> by a linear phase. Mathematically, if <math>\\{x_n\\}</math> represents the vector '''x''' then\n\n:if <math>\\mathcal{F}(\\{x_n\\})_k=X_k</math>\n\n:then <math>\\mathcal{F}(\\{ x_n\\cdot e^{\\frac{i 2\\pi}{N}n m} \\})_k=X_{k-m}</math>\n\n:and <math>\\mathcal{F}(\\{x_{n-m}\\})_k=X_k\\cdot e^{-\\frac{i 2\\pi}{N}k m}</math>\n\n===Circular convolution theorem and cross-correlation theorem===\n{{anchor|Circular convolution theorem}}\n{{anchor|Cross-correlation theorem}}\n\nThe [[DTFT#Convolution|convolution theorem]] for the [[discrete-time Fourier transform]] indicates that a convolution of two infinite sequences can be obtained as the inverse transform of the product of the individual transforms. An important simplification occurs when the sequences are of finite length, <math>N</math>.  In terms of the DFT and inverse DFT, it can be written as follows''':'''\n\n:<math>\n\\mathcal{F}^{-1} \\left \\{ \\mathbf{X\\cdot Y} \\right \\}_n \\ = \\sum_{\\ell=0}^{N-1}x_\\ell \\cdot (y_N)_{n-\\ell} \\ \\ \\triangleq \\ \\ (\\mathbf{x * y_N})_n\\ ,\n</math>\n\nwhich is the convolution of the <math>\\mathbf{x}</math> sequence with a <math>\\mathbf{y}</math> sequence extended by [[periodic summation]]''':'''\n\n:<math>(\\mathbf{y_N})_n \\ \\triangleq \\ \\sum_{p=-\\infty}^\\infty y_{(n-pN)} = y_{n \\pmod N}. \\,</math>\n\nSimilarly, the [[cross-correlation]] of &nbsp;<math>\\mathbf{x}</math>&nbsp; and &nbsp;<math>\\mathbf{y_N}</math>&nbsp; is given by''':'''\n\n:<math>\n\\mathcal{F}^{-1} \\left \\{ \\mathbf{X^* \\cdot Y} \\right \\}_n\n= \\sum_{\\ell=0}^{N-1}x_\\ell^* \\cdot (y_N)_{n+\\ell} \\ \\ \\triangleq \\ \\ (\\mathbf{x \\star y_N})_n\\ .\n</math>\n\nWhen either sequence contains a string of zeros, of length <math>L</math>,&nbsp; <math>L+1</math> of the circular convolution outputs are equivalent to values of &nbsp;<math>\\mathbf{x * y}.</math>&nbsp; Methods have also been developed to use this property as part of an efficient process that constructs &nbsp;<math>\\mathbf{x * y}</math>&nbsp; with an <math>\\mathbf{x}</math> or <math>\\mathbf{y}</math> sequence potentially much longer than the practical transform size (<math>N</math>). Two such methods are called [[overlap-save method|overlap-save]] and [[overlap-add method|overlap-add]].<ref>T. G. Stockham, Jr., \"[http://dl.acm.org/citation.cfm?id=1464209 High-speed convolution and correlation],\" in 1966 ''Proc. AFIPS Spring Joint Computing Conf.'' Reprinted in Digital Signal Processing, L. R. Rabiner and C. M. Rader, editors, New York: IEEE Press, 1972.</ref>  The efficiency results from the fact that a direct evaluation of either summation (above) requires <math>\\scriptstyle O(N^2)</math> operations for an output sequence of length <math>N</math>. &nbsp;An indirect method, using transforms, can take advantage of the <math>\\scriptstyle O(N\\log N)</math> efficiency of the [[fast Fourier transform]] (FFT) to achieve much better performance.  Furthermore, convolutions can be used to efficiently compute DFTs via [[Rader's FFT algorithm]] and [[Bluestein's FFT algorithm]].\n\n=== Convolution theorem duality ===\n\nIt can also be shown that''':'''\n\n:<math>\n\\mathcal{F} \\left \\{ \\mathbf{x\\cdot y} \\right \\}_k \\ \\triangleq\n\\sum_{n=0}^{N-1} x_n \\cdot y_n \\cdot e^{-i \\frac{2\\pi}{N} k n}\n</math>\n::<math>=\\frac{1}{N} (\\mathbf{X * Y_N})_k, \\,</math> &nbsp; which is the circular convolution of <math>\\mathbf{X}</math> and <math>\\mathbf{Y}</math>.\n\n===Trigonometric interpolation polynomial===\nThe [[trigonometric interpolation polynomial]]\n:<small><math>p(t) = \\frac{1}{N} \\left[ X_0 + X_1 e^{i 2\\pi t} + \\cdots + X_{N/2-1} e^{i(N/2-1) 2\\pi t} + X_{N/2} \\cos(N\\pi t) + X_{N/2+1} e^{i(-N/2+1) 2\\pi t} + \\cdots + X_{N-1} e^{-i 2\\pi t} \\right]</math></small> for ''N'' [[Even and odd numbers|even]] ,\n:<small><math>p(t) = \\frac{1}{N} \\left[ X_0 + X_1 e^{i 2\\pi t} + \\cdots + X_{\\lfloor N/2 \\rfloor} e^{i \\lfloor N/2 \\rfloor 2\\pi t} + X_{\\lfloor N/2 \\rfloor+1} e^{-i \\lfloor N/2 \\rfloor 2\\pi t} + \\cdots + X_{N-1} e^{-i 2\\pi t} \\right]</math></small>  for ''N'' odd,\nwhere the coefficients ''X''<sub>''k''</sub> are given by the DFT of ''x''<sub>''n''</sub> above, satisfies the interpolation property <math>p(n/N) = x_n</math> for <math>n=0,\\ldots,N-1</math>.\n\nFor even ''N'', notice that the [[Nyquist frequency|Nyquist component]] <math>\\frac{X_{N/2}}{N} \\cos(N\\pi t)</math> is handled specially.\n\nThis interpolation is ''not unique'': aliasing implies that one could add ''N'' to any of the complex-sinusoid frequencies (e.g. changing <math>e^{-it}</math> to <math>e^{i(N-1)t}</math> ) without changing the interpolation property, but giving ''different'' values in between the <math>x_n</math> points.  The choice above, however, is typical because it has two useful properties.  First, it consists of sinusoids whose frequencies have the smallest possible magnitudes: the interpolation is [[bandlimited]]. Second, if the  <math>x_n</math> are real numbers, then <math>p(t)</math> is real as well.\n\nIn contrast, the most obvious trigonometric interpolation polynomial is the one in which the frequencies range from 0 to <math>N-1</math> (instead of roughly <math>-N/2</math> to <math>+N/2</math> as above), similar to the inverse DFT formula. This interpolation does ''not'' minimize the slope, and is ''not'' generally real-valued for real <math>x_n</math>; its use is a common mistake.\n\n=== The unitary DFT ===\nAnother way of looking at the DFT is to note that in the above discussion, the DFT can be expressed as the [[DFT matrix]], a [[Vandermonde matrix]], \n[[Generalizations of Pauli matrices#Construction: The clock and shift matrices|introduced by Sylvester]] in 1867,\n:<math>\\mathbf{F} =\n\\begin{bmatrix}\n \\omega_N^{0 \\cdot 0}     & \\omega_N^{0 \\cdot 1}     & \\ldots & \\omega_N^{0 \\cdot (N-1)}     \\\\\n \\omega_N^{1 \\cdot 0}     & \\omega_N^{1 \\cdot 1}     & \\ldots & \\omega_N^{1 \\cdot (N-1)}     \\\\\n \\vdots                   & \\vdots                   & \\ddots & \\vdots                       \\\\\n \\omega_N^{(N-1) \\cdot 0} & \\omega_N^{(N-1) \\cdot 1} & \\ldots & \\omega_N^{(N-1) \\cdot (N-1)} \\\\\n\\end{bmatrix}\n</math>\nwhere <math>\\omega_N = e^{-i 2 \\pi/N}</math> is a primitive [[roots of unity|''N''th root of unity]].\n\nThe inverse transform is then given by the inverse of the above matrix,\n:<math>\\mathbf{F}^{-1}=\\frac{1}{N}\\mathbf{F}^*</math>\n\nWith [[unitary operator|unitary]] normalization constants <math>1/\\sqrt{N}</math>, the DFT becomes a [[unitary transformation]], defined by a unitary matrix:\n\n:<math>\\mathbf{U}=\\mathbf{F}/\\sqrt{N}</math>\n:<math>\\mathbf{U}^{-1}=\\mathbf{U}^*</math>\n:<math>|\\det(\\mathbf{U})|=1</math>\nwhere <math>\\det()</math> is the [[determinant]] function. The determinant is the product of the eigenvalues, which are always <math>\\pm 1</math> or <math>\\pm i</math> as described below.  In a real vector space, a unitary transformation can be thought of as simply a rigid rotation of the coordinate system, and all of the properties of a rigid rotation can be found in the unitary DFT.\n\nThe orthogonality of the DFT is now expressed as an [[orthonormal]]ity condition (which arises in many areas of mathematics as described in [[root of unity]]):\n:<math>\\sum_{m=0}^{N-1}U_{km}U_{mn}^*=\\delta_{kn}</math>\n\nIf '''X'''  is defined as the unitary DFT of the vector '''x''', then\n:<math>X_k=\\sum_{n=0}^{N-1} U_{kn} x_n</math>\n\nand the [[Plancherel theorem]] is expressed as\n:<math>\\sum_{n=0}^{N-1}x_n y_n^* = \\sum_{k=0}^{N-1}X_k Y_k^*</math>\n\nIf we view the DFT as just a coordinate transformation which simply specifies the components of a vector in a new coordinate system, then the above is just the statement that the dot product of two vectors is preserved under a unitary DFT transformation. For the special case <math>\\mathbf{x} = \\mathbf{y}</math>, this implies that the length of a vector is preserved as well—this is just [[Parseval's theorem]],\n:<math>\\sum_{n=0}^{N-1}|x_n|^2 = \\sum_{k=0}^{N-1}|X_k|^2</math>\n\nA consequence of the [[Discrete Fourier transform#Circular convolution theorem and cross-correlation theorem|circular convolution theorem]] is that the DFT matrix {{mvar|F}} diagonalizes any [[circulant matrix]].\n\n=== Expressing the inverse DFT in terms of the DFT ===\nA useful property of the DFT is that the inverse DFT can be easily expressed in terms of the (forward) DFT, via several well-known \"tricks\".  (For example, in computations, it is often convenient to only implement a fast Fourier transform corresponding to one transform direction and then to get the other transform direction from the first.)\n\nFirst, we can compute the inverse DFT by reversing all but one of the inputs (Duhamel ''et al.'', 1988):\n\n:<math>\\mathcal{F}^{-1}(\\{x_n\\}) = \\mathcal{F}(\\{x_{N - n}\\}) / N</math>\n\n(As usual, the subscripts are interpreted [[modular arithmetic|modulo]] ''N''; thus, for <math>n=0</math>, we have <math>x_{N-0}=x_0</math>.)\n\nSecond, one can also conjugate the inputs and outputs:\n\n:<math>\\mathcal{F}^{-1}(\\mathbf{x}) = \\mathcal{F}(\\mathbf{x}^*)^* / N</math>\n\nThird, a variant of this conjugation trick, which is sometimes preferable because it requires no modification of the data values, involves swapping real and imaginary parts (which can be done on a computer simply by modifying [[pointer (computer programming)|pointer]]s). Define <math display=\"inline}>\\operatorname{swap}(x_n)</math> as <math>x_n</math> with its real and imaginary parts swapped—that is, if <math>x_n = a + b i</math> then <math display=\"inline>\\operatorname{swap}(x_n)</math> is <math>b + a i</math>.  Equivalently, <math display=\"inline}>\\operatorname{swap}(x_n)</math> equals <math>i x_n^*</math>.  Then\n\n:<math>\\mathcal{F}^{-1}(\\mathbf{x}) = \\operatorname{swap}(\\mathcal{F}(\\operatorname{swap}(\\mathbf{x}))) / N</math>\n\nThat is, the inverse transform is the same as the forward transform with the real and imaginary parts swapped for both input and output, up to a normalization (Duhamel ''et al.'', 1988).\n\nThe conjugation trick can also be used to define a new transform, closely related to the DFT, that is [[Involution (mathematics)|involutory]]—that is, which is its own inverse.  In particular, <math>T(\\mathbf{x}) = \\mathcal{F}(\\mathbf{x}^*) / \\sqrt{N}</math> is clearly its own inverse: <math>T(T(\\mathbf{x})) = \\mathbf{x}</math>.  A closely related involutory transformation (by a factor of (1+''i'') /{{radic|2}}) is <math>H(\\mathbf{x}) = \\mathcal{F}((1+i) \\mathbf{x}^*) / \\sqrt{2N}</math>, since the <math>(1+i)</math> factors in <math>H(H(\\mathbf{x}))</math> cancel the 2.  For real inputs <math>\\mathbf{x}</math>, the real part of <math>H(\\mathbf{x})</math> is none other than the [[discrete Hartley transform]], which is also involutory.\n\n=== Eigenvalues and eigenvectors ===\n\nThe [[eigenvalue]]s of the DFT matrix are simple and well-known, whereas the [[eigenvector]]s are complicated, not unique, and are the subject of ongoing research.\n\nConsider the unitary form <math>\\mathbf{U}</math> defined above for the DFT of length ''N'', where\n:<math>\\mathbf{U}_{m,n} = \\frac1{\\sqrt{N}}\\omega_N^{(m-1)(n-1)} = \\frac1{\\sqrt{N}}e^{-\\frac{i 2\\pi}N (m-1)(n-1)}.</math>\nThis matrix satisfies the [[matrix polynomial]] equation:\n:<math>\\mathbf{U}^4 = \\mathbf{I}.</math>\nThis can be seen from the inverse properties above: operating <math>\\mathbf{U}</math> twice gives the original data in reverse order, so operating <math>\\mathbf{U}</math> four times gives back the original data and is thus the [[identity matrix]].  This means that the eigenvalues <math>\\lambda</math> satisfy the equation:\n:<math>\\lambda^4 = 1.</math>\nTherefore, the eigenvalues of <math>\\mathbf{U}</math> are the fourth [[roots of unity]]: <math>\\lambda</math> is +1, −1, +''i'',  or −''i''.\n\nSince there are only four distinct eigenvalues for this <math>N\\times N</math> matrix, they have some [[algebraic multiplicity|multiplicity]].  The multiplicity gives the number of [[linearly independent]] eigenvectors corresponding to each eigenvalue.  (Note that there are ''N'' independent eigenvectors; a unitary matrix is never [[defective matrix|defective]].)\n\nThe problem of their multiplicity was solved by McClellan and Parks (1972), although it was later shown to have been equivalent to a problem solved by [[Carl Friedrich Gauss|Gauss]] (Dickinson and Steiglitz, 1982).  The multiplicity depends on the value of ''N'' [[modular arithmetic|modulo]] 4, and is given by the following table:\n\n{| class=\"wikitable\" style=\"margin:auto;\"\n|+ align=\"bottom\" | Multiplicities of the eigenvalues λ of the unitary DFT matrix '''U''' as a function of the transform size ''N'' (in terms of an integer ''m'').\n|-\n! size ''N''\n! λ = +1\n! λ = −1\n! λ = −''i''\n! λ = +''i''\n|-\n|    4''m''  ||  ''m'' + 1  ||  ''m''  ||  ''m''  ||  ''m'' − 1\n|-\n|    4''m'' + 1  ||  ''m'' + 1  ||  ''m''  ||  ''m''  ||  ''m''\n|-\n|    4''m'' + 2  ||  ''m'' + 1  ||  ''m'' + 1  ||  ''m''  ||  ''m''\n|-\n|    4''m'' + 3  ||  ''m'' + 1  ||  ''m'' + 1  ||  ''m'' + 1  ||  ''m''\n|}\n\nOtherwise stated, the [[characteristic polynomial]] of <math>\\mathbf{U}</math> is:\n:<math>\\det (\\lambda I - \\mathbf{U})=\n(\\lambda-1)^{\\left\\lfloor \\tfrac {N+4}{4}\\right\\rfloor}\n(\\lambda+1)^{\\left\\lfloor \\tfrac {N+2}{4}\\right\\rfloor}\n(\\lambda+i)^{\\left\\lfloor \\tfrac {N+1}{4}\\right\\rfloor}\n(\\lambda-i)^{\\left\\lfloor \\tfrac {N-1}{4}\\right\\rfloor}.</math>\n\nNo simple analytical formula for general eigenvectors is known.   Moreover, the eigenvectors are not unique because any linear combination of eigenvectors for the same eigenvalue is also an eigenvector for that eigenvalue.  Various researchers have proposed different choices of eigenvectors, selected to satisfy useful properties like [[orthogonality]] and to have \"simple\" forms (e.g., McClellan and Parks, 1972; Dickinson and Steiglitz, 1982; Grünbaum, 1982; Atakishiyev and Wolf, 1997; Candan ''et al.'', 2000; Hanna ''et al.'', 2004; Gurevich and Hadani, 2008).\n\nA straightforward approach is to discretize an eigenfunction of the continuous [[Fourier transform]],\nof which the most famous is the [[Gaussian function]].\nSince [[periodic summation]] of the function means discretizing its frequency spectrum\nand discretization means periodic summation of the spectrum,\nthe discretized and periodically summed Gaussian function yields an eigenvector of the discrete transform:\n*<math>F(m) = \\sum_{k\\in\\mathbb{Z}} \\exp\\left(-\\frac{\\pi\\cdot(m+N\\cdot k)^2}{N}\\right)</math>.\n\nThe closed form expression for the series can be expressed by [[Jacobi theta function]]s as\n\n*<math>F(m) = \\frac1{\\sqrt{N}}\\vartheta_3\\left(\\frac{\\pi m}N, \\exp\\left(-\\frac{\\pi}N \\right)\\right)</math>.\n\nTwo other simple closed-form analytical eigenvectors for special DFT period ''N'' were found (Kong, 2008):\n\nFor DFT period ''N'' = 2''L'' + 1 = 4''K'' + 1, where ''K'' is an integer, the following is an eigenvector of DFT:\n*<math>F(m)=\\prod_{s=K+1}^L\\left[\\cos\\left(\\frac{2\\pi}{N}m\\right)- \\cos\\left(\\frac{2\\pi}{N}s\\right)\\right]</math>\n\nFor DFT period ''N'' = 2''L'' = 4''K'', where ''K'' is an integer, the following is an eigenvector of DFT:\n*<math>F(m)=\\sin\\left(\\frac{2\\pi}{N}m\\right)\\prod_{s=K+1}^{L-1}\\left[\\cos\\left(\\frac{2\\pi}{N}m\\right)- \\cos\\left(\\frac{2\\pi}{N}s\\right)\\right]</math>\n\nThe choice of eigenvectors of the DFT matrix has become important in recent years in order to define a discrete analogue of the [[fractional Fourier transform]]—the DFT matrix can be taken to fractional powers by exponentiating the eigenvalues (e.g., Rubio and Santhanam, 2005).  For the [[continuous Fourier transform]], the natural orthogonal eigenfunctions are the [[Hermite function]]s, so various discrete analogues of these have been employed as the eigenvectors of the DFT, such as the [[Kravchuk polynomials]] (Atakishiyev and Wolf, 1997).  The \"best\" choice of eigenvectors to define a fractional discrete Fourier transform remains an open question, however.\n\n=== Uncertainty principles ===\n==== Probabilistic uncertainty principle ====\nIf the random variable {{math|''X''<sub>''k''</sub>}} is constrained by\n:<math>\\sum_{n=0}^{N-1}|X_n|^2=1  ~,</math>\nthen \n:<math>P_n=|X_n|^2</math> \nmay be considered to represent a discrete [[probability mass function]] of {{mvar|n}}, with an associated probability mass function constructed from the transformed variable,\n:<math>Q_m=N|x_m|^2 ~.</math>\n\nFor the case of continuous functions <math>P(x)</math> and <math>Q(k)</math>, the [[Heisenberg uncertainty principle]] states that\n:<math>D_0(X)D_0(x)\\ge\\frac{1}{16\\pi^2}</math>\nwhere  <math>D_0(X)</math> and <math>D_0(x)</math> are the variances of <math>|X|^2</math> and <math>|x|^2</math> respectively, with the equality attained in the case of a suitably normalized [[Gaussian distribution]]. Although the variances may be analogously defined for the DFT, an analogous uncertainty principle is not useful, because the uncertainty will not be shift-invariant. Still, a meaningful uncertainty principle has been introduced by Massar and Spindel.<ref>{{Cite journal | last1 = Massar | first1 = S. | last2 = Spindel | first2 = P. | doi = 10.1103/PhysRevLett.100.190401 | title = Uncertainty Relation for the Discrete Fourier Transform | journal = Physical Review Letters | volume = 100 | issue = 19 | year = 2008 | pmid =  18518426| pmc = |arxiv = 0710.0723 |bibcode = 2008PhRvL.100s0401M | page=190401}}</ref>\n\nHowever, the Hirschman [[entropic uncertainty]] will have a useful analog for the case of the DFT.<ref name=\"DeBrunner\">{{cite journal |last1=DeBrunner |first1=Victor |last2=Havlicek |first2=Joseph P. |last3=Przebinda |first3=Tomasz|last4=Özaydin |first4=Murad|year=2005 |title=Entropy-Based Uncertainty Measures for <math>L^2(\\mathbb{R}^n),\\ell^2(\\mathbb{Z})</math>, and <math>\\ell^2(\\mathbb{Z}/N\\mathbb{Z})</math> With a Hirschman Optimal Transform for <math>\\ell^2(\\mathbb{Z}/N\\mathbb{Z})</math> |journal=IEEE Transactions on Signal Processing |volume=53 |issue=8 |page=2690 |doi=  10.1109/TSP.2005.850329|url=http://redwood.berkeley.edu/w/images/9/95/2002-26.pdf |accessdate=2011-06-23 |bibcode = 2005ITSP...53.2690D }}</ref> The Hirschman uncertainty principle is expressed in terms of the [[Entropy (information theory)|Shannon entropy]] of the two probability functions.\n\nIn the discrete case, the Shannon entropies are defined as\n:<math>H(X)=-\\sum_{n=0}^{N-1} P_n\\ln P_n</math>\nand\n:<math>H(x)=-\\sum_{m=0}^{N-1} Q_m\\ln Q_m ~,</math>\nand the [[entropic uncertainty]] principle becomes<ref name=\"DeBrunner\"/>\n:<math>H(X)+H(x) \\ge \\ln(N) ~.</math>\n\nThe equality is obtained for <math>P_n</math> equal to translations and modulations of a suitably normalized [[Kronecker comb]] of period <math>A</math> where <math>A</math> is any exact integer divisor of <math>N</math>. The probability mass function <math>Q_m</math> will then be proportional to a suitably translated [[Kronecker comb]] of period <math>B=N/A</math>.<ref name=\"DeBrunner\"/>\n\n==== Deterministic uncertainty principle ====\nThere is also a well-known deterministic uncertainty principle that uses signal sparsity (or the number of non-zero coefficients).<ref name=\"Donoho\">{{cite journal |last1=Donoho |first1=D.L. |last2=Stark |first2=P.B |year=1989 |title=Uncertainty principles and signal recovery |journal=SIAM Journal on Applied Mathematics |volume=49 |issue=3 |pages=906–931 |doi=10.1137/0149053}}</ref> Let <math>\\|x\\|_0</math> and <math>\\|X\\|_0</math> be the number of non-zero elements of the time and frequency sequences <math>x_0,x_1,\\ldots,x_{N-1}</math> and <math>X_0,X_1,\\ldots,X_{N-1}</math>, respectively. Then, \n:<math>N\\leq \\|x\\|_0 \\cdot \\|X\\|_0.</math>\nAs an immediate consequence of the [[Arithmetic–geometric mean|inequality of arithmetic and geometric means]], one also has <math>2\\sqrt{N}\\leq\\|x\\|_0+\\|X\\|_0</math>. Both uncertainty principles were shown to be tight for specifically-chosen \"picket-fence\" sequences (discrete impulse trains), and find practical use for signal recovery applications.<ref name=\"Donoho\"/>\n\n=== DFT of real and purely imaginary signals ===\n*If <math>x_0, \\ldots, x_{N-1}</math> are [[real number]]s, as they often are in practical applications, then the DFT <math>X_0, \\ldots, X_{N-1}</math> is [[Even and odd functions|even symmetric]]:\n:<math>x_n \\in \\mathbb{R} \\quad \\forall n \\in \\{0,\\ldots,N-1 \\} \\implies X_k = X_{-k \\mod N}^* \\quad \\forall k \\in \\{0,\\ldots,N-1 \\}</math>, where <math>X^*\\,</math> denotes [[Complex conjugate|complex conjugation]].\n\nIt follows that for even <math>N</math> <math>X_0</math> and <math>X_{N/2}</math> are real-valued, and the remainder of the DFT is completely specified by just <math>N/2-1</math> complex numbers.\n\n*If <math>x_0, \\ldots, x_{N-1}</math> are purely imaginary numbers, then the DFT <math>X_0, \\ldots, X_{N-1}</math> is [[Even and odd functions|odd symmetric]]:\n:<math>x_n \\in i \\mathbb{R} \\quad \\forall n \\in \\{0,\\ldots,N-1 \\} \\implies X_k = -X_{-k \\mod N}^* \\quad \\forall k \\in \\{0,\\ldots,N-1 \\}</math>, where <math>X^*\\,</math> denotes [[Complex conjugate|complex conjugation]].\n\n==Generalized DFT (shifted and non-linear phase)==\nIt is possible to shift the transform sampling in time and/or frequency domain by some real shifts ''a'' and ''b'', respectively. This is sometimes known as a '''generalized DFT''' (or '''GDFT'''), also called the '''shifted DFT''' or '''offset DFT''', and has analogous properties to the ordinary DFT:\n\n:<math>X_k = \\sum_{n=0}^{N-1} x_n e^{-\\frac{i 2 \\pi}{N} (k+b) (n+a)} \\quad \\quad k = 0, \\dots, N-1.</math>\n\nMost often, shifts of <math>1/2</math> (half a sample) are used.\nWhile the ordinary DFT corresponds to a periodic signal in both time and frequency domains, <math>a=1/2</math> produces a signal that is anti-periodic in frequency domain (<math>X_{k+N} = - X_k</math>) and vice versa for <math>b=1/2</math>.\nThus, the specific case of <math>a = b = 1/2</math> is known as an ''odd-time odd-frequency'' discrete Fourier transform (or O<sup>2</sup> DFT).\nSuch shifted transforms are most often used for symmetric data, to represent different boundary symmetries, and for real-symmetric data they correspond to different forms of the discrete [[discrete cosine transform|cosine]] and [[discrete sine transform|sine]] transforms.\n\nAnother interesting choice is <math>a=b=-(N-1)/2</math>, which is called the '''centered DFT''' (or '''CDFT''').  The centered DFT has the useful property that, when ''N'' is a multiple of four, all four of its eigenvalues (see above) have equal multiplicities (Rubio and Santhanam, 2005)<ref>Santhanam, Balu; Santhanam, Thalanayar S. [http://thamakau.usc.edu/Proceedings/ICASSP%202007/pdfs/0301385.pdf \"''Discrete Gauss-Hermite functions and eigenvectors of the centered discrete Fourier transform''\"]{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}, Proceedings of the 32nd IEEE ''International Conference on Acoustics, Speech, and Signal Processing'' (ICASSP 2007, SPTM-P12.4), vol. III, pp. 1385-1388.</ref>\n\nThe term GDFT is also used for the non-linear phase extensions of DFT. Hence, GDFT method provides a generalization for constant amplitude orthogonal block transforms including linear and non-linear phase types. GDFT is a framework \nto improve time and frequency domain properties of the traditional DFT, e.g. auto/cross-correlations, by the addition of the properly designed phase shaping function (non-linear, in general) to the original linear phase functions (Akansu and Agirman-Tosun, 2010).<ref>Akansu, Ali N.; Agirman-Tosun, Handan \n[http://web.njit.edu/~akansu/PAPERS/AkansuIEEE-TSP2010.pdf \"''Generalized Discrete Fourier Transform With Nonlinear Phase''\"], IEEE ''Transactions on Signal Processing'', vol. 58, no. 9, pp. 4547-4556, Sept. 2010.</ref>\n\nThe discrete Fourier transform can be viewed as a special case of the [[z-transform]], evaluated on the unit circle in the complex plane; more general z-transforms correspond to ''complex'' shifts ''a'' and ''b'' above.\n\n==Multidimensional DFT==<!-- This section is linked from [[Fast Fourier transform]] -->\nThe ordinary DFT transforms a one-dimensional sequence or [[matrix (mathematics)|array]] <math>x_n</math> that is a function of exactly one discrete variable ''n''.  The multidimensional DFT of a multidimensional array <math>x_{n_1, n_2, \\dots, n_d}</math> that is a function of ''d'' discrete variables <math>n_\\ell = 0, 1, \\dots, N_\\ell-1</math> for <math>\\ell</math> in <math>1, 2, \\dots, d</math> is defined by:\n\n:<math>X_{k_1, k_2, \\dots, k_d} = \\sum_{n_1=0}^{N_1-1} \\left(\\omega_{N_1}^{~k_1 n_1} \\sum_{n_2=0}^{N_2-1} \\left( \\omega_{N_2}^{~k_2 n_2} \\cdots \\sum_{n_d=0}^{N_d-1} \\omega_{N_d}^{~k_d n_d}\\cdot x_{n_1, n_2, \\dots, n_d} \\right) \\right) \\, , </math>\n\nwhere <math>\\omega_{N_\\ell} = \\exp(-i 2\\pi/N_\\ell)</math> as above and the ''d'' output indices run from <math>k_\\ell = 0, 1, \\dots, N_\\ell-1</math>.  This is more compactly expressed in [[coordinate vector|vector]] notation, where we define <math>\\mathbf{n} = (n_1, n_2, \\dots, n_d)</math> and <math>\\mathbf{k} = (k_1, k_2, \\dots, k_d)</math> as ''d''-dimensional vectors of indices from 0 to <math>\\mathbf{N} - 1</math>, which we define as <math>\\mathbf{N} - 1 = (N_1 - 1, N_2 - 1, \\dots, N_d - 1)</math>:\n\n:<math>X_\\mathbf{k} = \\sum_{\\mathbf{n}=\\mathbf{0}}^{\\mathbf{N}-1} e^{-i 2\\pi \\mathbf{k} \\cdot (\\mathbf{n} / \\mathbf{N})} x_\\mathbf{n} \\, ,</math>\n\nwhere the division <math>\\mathbf{n} / \\mathbf{N}</math> is defined as <math>\\mathbf{n} / \\mathbf{N} = (n_1/N_1, \\dots, n_d/N_d)</math> to be performed element-wise, and the sum denotes the set of nested summations above.\n\nThe inverse of the multi-dimensional DFT is, analogous to the one-dimensional case, given by:\n\n:<math>x_\\mathbf{n} = \\frac{1}{\\prod_{\\ell=1}^d N_\\ell} \\sum_{\\mathbf{k}=\\mathbf{0}}^{\\mathbf{N}-1} e^{i 2\\pi \\mathbf{n} \\cdot (\\mathbf{k} / \\mathbf{N})} X_\\mathbf{k} \\, .</math>\n\nAs the one-dimensional DFT expresses the input <math>x_n</math> as a superposition of sinusoids, the multidimensional DFT expresses the input as a superposition of [[plane wave]]s, or multidimensional sinusoids. The direction of oscillation in space is <math>\\mathbf{k} / \\mathbf{N}</math>. The amplitudes  are <math>X_\\mathbf{k}</math>.  This decomposition is of great importance for everything from [[digital image processing]] (two-dimensional) to solving [[partial differential equations]]. The solution is broken up into plane waves.\n\nThe multidimensional DFT can be computed by the [[function composition|composition]] of a sequence of one-dimensional DFTs along each dimension.  In the two-dimensional case <math>x_{n_1,n_2}</math> the <math>N_1</math> independent DFTs of the rows (i.e., along <math>n_2</math>) are computed first to form a new array <math>y_{n_1,k_2}</math>. Then the <math>N_2</math> independent DFTs of ''y'' along the columns (along <math>n_1</math>) are computed to form the final result <math>X_{k_1,k_2}</math>.  Alternatively the columns can be computed first and then the rows. The order is immaterial because the nested summations above [[commutative operation|commute]].\n\nAn algorithm to compute a one-dimensional DFT is thus sufficient to efficiently compute a multidimensional DFT.  This approach is known as the ''row-column'' algorithm. There are also intrinsically [[Fast Fourier transform#Multidimensional FFTs|multidimensional FFT algorithms]].\n\n=== The real-input multidimensional DFT ===\nFor input data <math>x_{n_1, n_2, \\dots, n_d}</math> consisting of [[real numbers]], the DFT outputs have a conjugate symmetry similar to the one-dimensional case above:\n\n:<math>X_{k_1, k_2, \\dots, k_d} = X_{N_1 - k_1, N_2 - k_2, \\dots, N_d - k_d}^* ,</math>\n\nwhere the star again denotes complex conjugation and the <math>\\ell</math>-th subscript is again interpreted modulo <math>N_\\ell</math> (for <math>\\ell = 1,2,\\ldots,d</math>).\n\n== Applications ==\nThe DFT has seen wide usage across a large number of fields; we only sketch a few examples below (see also the references at the end). All applications of the DFT depend crucially on the availability of a fast algorithm to compute discrete Fourier transforms and their inverses, a [[fast Fourier transform]].\n\n=== Spectral analysis ===\nWhen the DFT is used for [[signal spectral analysis]], the <math>\\{x_n\\}\\,</math> sequence usually represents a finite set of uniformly spaced time-samples of some signal <math>x(t)\\,</math>, where <math>t</math> represents time.  The conversion from continuous time to samples (discrete-time) changes the underlying [[continuous Fourier transform|Fourier transform]] of <math>x(t)</math> into a [[discrete-time Fourier transform]] (DTFT), which generally entails a type of distortion called [[aliasing]].  Choice of an appropriate sample-rate (see ''[[Nyquist rate]]'') is the key to minimizing that distortion.  Similarly, the conversion from a very long (or infinite) sequence to a manageable size entails a type of distortion called ''[[Spectral leakage|leakage]]'', which is manifested as a loss of detail (a.k.a. resolution) in the DTFT.  Choice of an appropriate sub-sequence length is the primary key to minimizing that effect.  When the available data (and time to process it) is more than the amount needed to attain the desired frequency resolution, a standard technique is to perform multiple DFTs, for example to create a [[spectrogram]].  If the desired result is a power spectrum and noise or randomness is present in the data, averaging the magnitude components of the multiple DFTs is a useful procedure to reduce the [[variance]] of the spectrum (also called a [[periodogram]] in this context); two examples of such techniques are the [[Welch method]] and the [[Bartlett method]]; the general subject of estimating the power spectrum of a noisy signal is called [[spectral estimation]].\n\nA final source of distortion (or perhaps ''illusion'') is the DFT itself, because it is just a discrete sampling of the DTFT, which is a function of a continuous frequency domain.  That can be mitigated by increasing the resolution of the DFT.  That procedure is illustrated at [[Discrete-time Fourier transform#Sampling the DTFT|Sampling the DTFT]].\n*The procedure is sometimes referred to as ''zero-padding'', which is a particular implementation used in conjunction with the [[fast Fourier transform]] (FFT) algorithm.  The inefficiency of performing multiplications and additions with zero-valued \"samples\" is more than offset by the inherent efficiency of the FFT.\n*As already noted, leakage imposes a limit on the inherent resolution of the DTFT.  So there is a practical limit to the benefit that can be obtained from a fine-grained DFT.\n\n=== Filter bank ===\nSee [[Filter bank#FFT filter banks|FFT filter banks]] and [[Discrete-time Fourier transform#Sampling the DTFT|Sampling the DTFT]].\n\n===Data compression===\nThe field of digital signal processing relies heavily on operations in the frequency domain (i.e. on the Fourier transform). For example, several [[lossy]] image and sound compression methods employ the discrete Fourier transform: the signal is cut into short segments, each is transformed, and then the Fourier coefficients of high frequencies, which are assumed to be unnoticeable, are discarded. The decompressor computes the inverse transform based on this reduced number of Fourier coefficients. (Compression applications often use a specialized form of the DFT, the [[discrete cosine transform]] or sometimes the [[modified discrete cosine transform]].)\nSome relatively recent compression algorithms, however, use [[wavelet transform]]s, which give a more uniform compromise between time and frequency domain than obtained by chopping data into segments and transforming each segment.  In the case of [[JPEG2000]], this avoids the spurious image features that appear when images are highly compressed with the original [[JPEG]].\n\n===Partial differential equations===\nDiscrete Fourier transforms are often used to solve [[partial differential equations]], where again the DFT is used as an approximation for the [[Fourier series]] (which is recovered in the limit of infinite ''N''). The advantage of this approach is that it expands the signal in complex exponentials <math>e^{inx}</math>, which are eigenfunctions of differentiation: <math>{\\text{d} \\big( e^{inx} \\big) }/\\text{d}x = in e^{inx}</math>. Thus, in the Fourier representation, differentiation is simple—we just multiply by <math>in</math>.  (Note, however, that the choice of <math>n</math> is not unique due to aliasing; for the method to be convergent, a choice similar to that in the [[Discrete Fourier transform#Trigonometric interpolation polynomial|trigonometric interpolation]] section above should be used.) A [[linear differential equation]] with constant coefficients is transformed into an easily solvable algebraic equation. One then uses the inverse DFT to transform the result back into the ordinary spatial representation. Such an approach is called a [[spectral method]].\n\n===Polynomial multiplication===\n\nSuppose we wish to compute the polynomial product ''c''(''x'') = ''a''(''x'') · ''b''(''x'').  The ordinary product expression for the coefficients of ''c'' involves a linear (acyclic) convolution, where indices do not \"wrap around.\"  This can be rewritten as a cyclic convolution by taking the coefficient vectors for ''a''(''x'') and ''b''(''x'') with constant term first, then appending zeros so that the resultant coefficient vectors '''a''' and '''b''' have dimension ''d''&nbsp;>&nbsp;deg(''a''(''x''))&nbsp;+&nbsp;deg(''b''(''x'')).  Then,\n\n:<math>\\mathbf{c} = \\mathbf{a} * \\mathbf{b}</math>\n\nWhere '''c''' is the vector of coefficients for ''c''(''x''), and the convolution operator <math>*\\,</math> is defined so\n\n:<math>c_n = \\sum_{m=0}^{d-1}a_m b_{n-m\\ \\mathrm{mod}\\ d} \\qquad\\qquad\\qquad n=0,1\\dots,d-1</math>\n\nBut convolution becomes multiplication under the DFT:\n\n:<math>\\mathcal{F}(\\mathbf{c}) = \\mathcal{F}(\\mathbf{a})\\mathcal{F}(\\mathbf{b})</math>\n\nHere the vector product is taken elementwise.  Thus the coefficients of the product polynomial ''c''(''x'') are just the terms 0, ..., deg(''a''(''x'')) + deg(''b''(''x'')) of the coefficient vector\n\n:<math>\\mathbf{c} = \\mathcal{F}^{-1}(\\mathcal{F}(\\mathbf{a})\\mathcal{F}(\\mathbf{b})).</math>\n\nWith a [[fast Fourier transform]], the resulting algorithm takes O (''N''&nbsp;log&nbsp;''N'') arithmetic operations.  Due to its simplicity and speed, the [[Cooley–Tukey FFT algorithm]], which is limited to [[composite number|composite]] sizes, is often chosen for the transform operation.  In this case, ''d'' should be chosen as the smallest integer greater than the sum of the input polynomial degrees that is factorizable into small prime factors (e.g. 2, 3, and 5, depending upon the FFT implementation).\n\n====Multiplication of large integers====\n\nThe fastest known [[multiplication algorithms|algorithms]] for the multiplication of very large [[integer]]s use the polynomial multiplication method outlined above.  Integers can be treated as the value of a polynomial evaluated specifically at the number base, with the coefficients of the polynomial corresponding to the digits in that base (ex. <math>123 = 1 \\cdot 10^2 + 2 \\cdot 10^1 + 3 \\cdot 10^0</math>).  After polynomial multiplication, a relatively low-complexity carry-propagation step completes the multiplication.\n\n==== Convolution  ====\nWhen data is [[Convolution|convolved]] with a function with wide support, such as for downsampling by a large sampling ratio, because of the [[Convolution theorem]] and the FFT algorithm, it may be faster to transform it, multiply pointwise by the transform of the filter and then reverse transform it.  Alternatively, a good filter is obtained by simply truncating the transformed data and re-transforming the shortened data set.\n\n==Some discrete Fourier transform pairs==\n\n{| class=\"wikitable\" style=\"text-align: center;\"\n|+ '''Some DFT pairs'''\n|-\n! <math>x_n = \\frac{1}{N}\\sum_{k=0}^{N-1}X_k  e^{i 2 \\pi kn/N} </math>\n! <math>X_k = \\sum_{n=0}^{N-1}x_n  e^{-i 2 \\pi kn/N} </math>\n! Note\n|-\n| <math>x_n e^{i 2 \\pi n\\ell/N} \\,</math>\n| <math>X_{k-\\ell}\\,</math>\n| Frequency shift theorem\n|-\n| <math>x_{n-\\ell}\\,</math>\n| <math>X_k  e^{-i 2 \\pi k\\ell/N} \\,</math>\n| Time shift theorem\n|-\n| <math>x_n \\in \\mathbb{R}</math>\n| <math>X_k=X_{N-k}^*\\,</math>\n| Real DFT\n|-\n| <math>a^n\\,</math>\n| <math>\\left\\{ \\begin{matrix}\n                   N & \\mbox{if } a = e^{i 2 \\pi k/N} \\\\\n                   \\frac{1-a^N}{1-a \\, e^{-i 2 \\pi k/N} } & \\mbox{otherwise}\n                \\end{matrix} \\right. </math>\n| from the [[geometric progression]] formula\n|-\n| <math>{N-1 \\choose n}\\,</math>\n| <math>\\left(1+e^{-i 2 \\pi k/N} \\right)^{N-1}\\,</math>\n| from the [[binomial theorem]]\n|-\n| <math>\\left\\{ \\begin{matrix}\n                        \\frac{1}{W} & \\mbox{if } 2n < W \\mbox{ or } 2(N-n) < W \\\\\n                        0 & \\mbox{otherwise}\n                      \\end{matrix} \\right. </math>\n| <math>\\left\\{ \\begin{matrix}\n              1 & \\mbox{if } k = 0 \\\\\n              \\frac{\\sin\\left(\\frac{\\pi W k}{N}\\right)}\n                   {W \\sin\\left(\\frac{\\pi k}{N}\\right)} & \\mbox{otherwise}\n                      \\end{matrix} \\right. </math>\n| <math>x_n</math> is a rectangular [[window function]] of ''W'' points centered on ''n''=0, where ''W'' is an [[odd integer]], and <math>X_k</math> is a [[sinc]]-like function (specifically, <math>X_k</math> is a [[Dirichlet kernel]])\n|-\n| <math>\\sum_{j\\in\\mathbb{Z}} \\exp\\left(-\\frac{\\pi}{cN}\\cdot(n+N\\cdot j)^2\\right)</math>\n| <math>\\sqrt{cN} \\cdot \\sum_{j\\in\\mathbb{Z}} \\exp\\left(-\\frac{\\pi c}{N}\\cdot(k+N\\cdot j)^2\\right)</math>\n| [[Discretization]] and [[periodic summation]] of the scaled [[Gaussian function]]s for <math>c>0</math>. Since either <math>c</math> or <math>\\frac{1}{c}</math> is larger than one and thus warrants fast convergence of one of the two series, for large <math>c</math> you may choose to compute the frequency spectrum and convert to the time domain using the discrete Fourier transform.\n|}\n\n==Generalizations==\n\n=== Representation theory ===\n{{details|Representation theory of finite groups#Discrete Fourier transform}}\n\nThe DFT can be interpreted as the complex-valued [[representation theory]] of the finite [[cyclic group]]. In other words, a sequence of <math>n</math> complex numbers can be thought of as an element of <math>n</math>-dimensional complex space <math>\\mathbb{C}^n</math> or equivalently a function <math>f</math> from the finite cyclic group of order <math>n</math> to the complex numbers, <math>\\mathbb{Z}_n \\mapsto \\mathbb{C}</math>. So  <math>f</math> is a [[class function]] on the finite cyclic group, and thus can be expressed as a linear combination of the irreducible characters of this group, which are the roots of unity.\n\nFrom this point of view, one may generalize the DFT to representation theory generally, or more narrowly to the [[representation theory of finite groups]].\n\nMore narrowly still, one may generalize the DFT by either changing the target (taking values in a field other than the complex numbers), or the domain (a group other than a finite cyclic group), as detailed in the sequel.\n\n=== Other fields ===\n{{Main|Discrete Fourier transform (general)|Number-theoretic transform}}\nMany of the properties of the DFT only depend on the fact that <math>e^{-\\frac{i 2 \\pi}{N}}</math> is a [[primitive root of unity]], sometimes denoted <math>\\omega_N</math> or <math>W_N</math> (so that <math>\\omega_N^N = 1</math>).  Such properties include the completeness, orthogonality, Plancherel/Parseval, periodicity, shift, convolution, and unitarity properties above, as well as many FFT algorithms. For this reason, the discrete Fourier transform can be defined by using roots of unity in [[field (mathematics)|fields]] other than the complex numbers, and such generalizations are commonly called ''number-theoretic transforms'' (NTTs) in the case of [[finite field]]s. For more information, see [[number-theoretic transform]] and [[discrete Fourier transform (general)]].\n\n=== Other finite groups ===\n{{Main|Fourier transform on finite groups}}\nThe standard DFT acts on a sequence ''x''<sub>0</sub>, ''x''<sub>1</sub>, …, ''x''<sub>''N''&minus;1</sub> of complex numbers, which can be viewed as a function {0, 1, …, ''N'' &minus; 1} → '''C'''. The multidimensional DFT acts on multidimensional sequences, which can be viewed as functions\n:<math> \\{0, 1, \\ldots, N_1-1\\} \\times \\cdots \\times \\{0, 1, \\ldots, N_d-1\\} \\to \\mathbb{C}. </math>\nThis suggests the generalization to [[Fourier transform on finite groups|Fourier transforms on arbitrary finite groups]], which act on functions ''G'' → '''C''' where ''G'' is a [[finite group]]. In this framework, the standard DFT is seen as the Fourier transform on a [[cyclic group]], while the multidimensional DFT is a Fourier transform on a direct sum of cyclic groups.\n\nFurther, Fourier transform can be on cosets of a group.\n\n==Alternatives==\n{{Main|Discrete wavelet transform}}\n{{details|Discrete wavelet transform#Comparison with Fourier transform}} There are various alternatives to the DFT for various applications, prominent among which are [[wavelets]]. The analog of the DFT is the [[discrete wavelet transform]] (DWT). From the point of view of [[time–frequency analysis]], a key limitation of the Fourier transform is that it does not include ''location'' information, only ''frequency'' information, and thus has difficulty in representing transients. As wavelets have location as well as frequency, they are better able to represent location, at the expense of greater difficulty representing frequency. For details, see [[Discrete wavelet transform#Comparison with Fourier transform|comparison of the discrete wavelet transform with the discrete Fourier transform]].\n\n==See also==\n*[[Companion matrix]]\n*[[DFT matrix]]\n*[[Fast Fourier transform]]\n*[[FFTPACK]]\n*[[FFTW]]\n*[[Generalizations of Pauli matrices]]\n*[[List of Fourier-related transforms]]\n* [[Multidimensional transform]]\n* [[Zak transform]]\n*[[Quantum Fourier transform]]\n\n==Notes==\n{{reflist|group=note}}\n\n==References==\n{{Reflist|32em}}\n\n==Further reading==\n{{More footnotes|date=January 2009}}\n* {{cite book\n | last = Brigham | first = E. Oran\n | title=The fast Fourier transform and its applications\n | location = Englewood Cliffs, N.J.\n | publisher = Prentice Hall\n | year=1988\n | isbn=978-0-13-307505-2\n }}\n* {{cite book\n |author1= [[Alan V. Oppenheim|Oppenheim, Alan V.]] |author2=[[Ronald W. Schafer|Schafer, R. W.]] |author3=Buck, J. R.\n  | title = Discrete-time signal processing\n | location = Upper Saddle River, N.J.\n | publisher = Prentice Hall\n | year = 1999\n | isbn = 978-0-13-754920-7\n }}\n* {{cite book\n | last = Smith | first = Steven W.\n | chapter-url = http://www.dspguide.com/ch8/1.htm\n | title = The Scientist and Engineer's Guide to Digital Signal Processing\n | edition = Second\n | location = San Diego, Calif.\n | publisher = California Technical Publishing\n | year=1999\n | isbn=978-0-9660176-3-2\n | chapter = Chapter 8: The Discrete Fourier Transform\n}}\n* {{cite book\n | first = Thomas H. | last = Cormen | authorlink = Thomas H. Cormen |author2=[[Charles E. Leiserson]] |author3=[[Ronald L. Rivest]] |author4=[[Clifford Stein]]\n | year = 2001\n | title = Introduction to Algorithms\n | edition = Second\n | publisher = MIT Press and McGraw-Hill\n | isbn = 978-0-262-03293-3\n | chapter = Chapter 30: Polynomials and the FFT\n | pages = 822–848\n | title-link = Introduction to Algorithms }} esp. section 30.2: The DFT and FFT, pp.&nbsp;830–838.\n* {{cite journal\n |author1=P. Duhamel |author2=B. Piron |author3=J. M. Etcheto | title = On computing the inverse DFT\n | journal =IEEE Transactions on Acoustics, Speech, and Signal Processing\n | volume = 36 | issue = 2 | pages = 285–286 | year = 1988\n | doi = 10.1109/29.1519\n }}\n* {{cite journal\n |author1=J. H. McClellan |author2=T. W. Parks | title = Eigenvalues and eigenvectors of the discrete Fourier transformation\n | journal =IEEE Transactions on Audio and Electroacoustics\n | volume = 20 | issue = 1 | pages = 66–74 | year = 1972\n | doi = 10.1109/TAU.1972.1162342\n }}\n* {{cite journal\n |author1=Bradley W. Dickinson |author2=Kenneth Steiglitz | title = Eigenvectors and functions of the discrete Fourier transform\n | journal =IEEE Transactions on Acoustics, Speech, and Signal Processing\n | volume = 30 | issue = 1 | pages = 25–31 | year = 1982\n | doi = 10.1109/TASSP.1982.1163843\n | url = http://www.cs.princeton.edu/~ken/Eigenvectors82.pdf|citeseerx=10.1.1.434.5279 }} (Note that this paper has an apparent typo in its table of the eigenvalue multiplicities: the +''i''/&minus;''i'' columns are interchanged.  The correct table can be found in McClellan and Parks, 1972, and is easily confirmed numerically.)\n* {{cite journal\n | author = F. A. Grünbaum\n | title = The eigenvectors of the discrete Fourier transform\n | journal =Journal of Mathematical Analysis and Applications\n | volume = 88 | issue = 2 | pages = 355–363 | year = 1982\n | doi = 10.1016/0022-247X(82)90199-8\n }}\n* {{cite journal\n |author1=Natig M. Atakishiyev |author2=Kurt Bernardo Wolf | title = Fractional Fourier-Kravchuk transform\n | journal =Journal of the Optical Society of America A\n | volume = 14 | issue = 7 | pages = 1467–1477 | year = 1997\n | doi = 10.1364/JOSAA.14.001467\n |bibcode = 1997JOSAA..14.1467A }}\n* {{cite journal\n |author1=C. Candan |author2=M. A. Kutay |author3=H. M.Ozaktas | title = The discrete fractional Fourier transform\n | journal = IEEE Transactions on Signal Processing\n | volume = 48 | issue = 5 | pages = 1329–1337 | year = 2000\n | doi = 10.1109/78.839980\n |bibcode = 2000ITSP...48.1329C |url=http://www.ee.bilkent.edu.tr/~haldun/publications/ozaktas166.pdf|hdl=11693/11130 }}\n* {{cite journal\n | author = Magdy Tawfik Hanna, Nabila Philip Attalla Seif, and Waleed Abd El Maguid Ahmed\n | title = Hermite-Gaussian-like eigenvectors of the discrete Fourier transform matrix based on the singular-value decomposition of its orthogonal projection matrices\n | journal =IEEE Transactions on Circuits and Systems I: Regular Papers\n | volume = 51 | issue = 11 | pages = 2245–2254 | year = 2004\n | doi = 10.1109/TCSI.2004.836850\n }}\n*{{cite journal\n |author1=Shamgar Gurevich |author2=Ronny Hadani | title=On the diagonalization of the discrete Fourier transform\n | id=preprint at\n | journal=Applied and Computational Harmonic Analysis\n | volume = 27 | issue = 1 | year=2009\n | pages=87–99\n | doi=10.1016/j.acha.2008.11.003\n | arxiv=0808.3281\n}}\n*{{cite journal\n |author1=Shamgar Gurevich |author2=Ronny Hadani |author3=Nir Sochen | title=The finite harmonic oscillator and its applications to sequences, communication and radar\n | id=preprint at\n | journal= IEEE Transactions on Information Theory\n | volume = 54 | issue = 9 | pages = 4239–4253 | year=2008\n | doi=10.1109/TIT.2008.926440\n | arxiv=0808.1495\n}}\n* {{cite journal\n |author1=Juan G. Vargas-Rubio |author2=Balu Santhanam | title = On the multiangle centered discrete fractional Fourier transform\n | journal =IEEE Signal Processing Letters\n | volume = 12 | issue = 4 | pages = 273–276 | year = 2005\n | doi = 10.1109/LSP.2005.843762\n |bibcode = 2005ISPL...12..273V }}\n* {{cite journal\n | author = [[James Cooley|J. Cooley]], P. Lewis, and P. Welch\n | title = The finite Fourier transform\n | journal =IEEE Transactions on Audio and Electroacoustics\n | volume = 17 | issue = 2 | pages = 77–85 | year = 1969\n | doi = 10.1109/TAU.1969.1162036\n }}\n* {{cite journal\n | author = F.N. Kong\n | title = Analytic Expressions of Two Discrete Hermite-Gaussian Signals\n | journal =IEEE Transactions on Circuits and Systems Ii: Express Briefs\n | volume = 55 | issue = 1 | pages = 56–60 | year = 2008\n | doi = 10.1109/TCSII.2007.909865\n}}\n\n==External links==\n*[https://jackschaedler.github.io/circles-sines-signals/ Interactive explanation of the DFT]\n*[https://www.nbtwiki.net/doku.php?id=tutorial:the_discrete_fourier_transformation_dft Matlab tutorial on the Discrete Fourier Transformation]\n*[http://www.fourier-series.com/fourierseries2/DFT_tutorial.html Interactive flash tutorial on the DFT]\n*[http://ccrma.stanford.edu/~jos/mdft/mdft.html Mathematics of the Discrete Fourier Transform by Julius O. Smith III]\n*[http://www.fftw.org FFTW: Fast implementation of the DFT - coded in C and under General Public License (GPL)]\n*[http://www.kurims.kyoto-u.ac.jp/~ooura/fft.html General Purpose FFT Package:  Yet another fast DFT implementation in C &amp; FORTRAN, permissive license]\n*[http://web.mit.edu/newsoffice/2009/explained-fourier.html Explained: The Discrete Fourier Transform]\n*[https://web.archive.org/web/20171010173422/http://en.dsplib.org/content/dft.html Discrete Fourier Transform]\n*[https://web.archive.org/web/20171113135838/http://en.dsplib.org/content/dft_freq.html Indexing and shifting of Discrete Fourier Transform]\n*[https://web.archive.org/web/20171106092801/http://en.dsplib.org/content/dft_prop.html Discrete Fourier Transform Properties]\n\n{{DSP}}\n\n{{DEFAULTSORT:Discrete Fourier Transform}}\n[[Category:Fourier analysis]]\n[[Category:Digital signal processing]]\n[[Category:Numerical analysis]]\n[[Category:Discrete transforms]]\n[[Category:Unitary operators]]\n\n[[cs:Fourierova transformace#Diskrétní Fourierova transformace]]\n[[pt:Transformada de Fourier#Transformada discreta de Fourier]]\n[[fi:Fourier'n muunnos#Diskreetti Fourier'n muunnos]]"
    },
    {
      "title": "Discrete wavelet transform",
      "url": "https://en.wikipedia.org/wiki/Discrete_wavelet_transform",
      "text": "[[Image:Jpeg2000 2-level wavelet transform-lichtenstein.png|thumb|300px|An example of the 2D discrete wavelet transform that is used in [[JPEG2000]]. The original image is high-pass filtered, yielding the three large images, each describing local changes in brightness (details) in the original image. It is then low-pass filtered and downscaled, yielding an approximation image; this image is high-pass filtered to produce the three smaller detail images, and low-pass filtered to produce the final approximation image in the upper-left.]]\nIn [[numerical analysis]] and [[functional analysis]], a '''discrete wavelet transform''' ('''DWT''') is any [[wavelet transform]] for which the [[wavelet]]s are discretely sampled. As with other wavelet transforms, a key advantage it has over [[Fourier transform]]s is temporal resolution: it captures both frequency ''and'' location information (location in time).\n\n== Examples ==\n\n=== Haar wavelets ===\n{{main|Haar wavelet}}\nThe first DWT was invented by Hungarian mathematician [[Alfréd Haar]]. For an input represented by a list of <math>2^n</math> numbers, the [[Haar wavelet]] transform may be considered to pair up input values, storing the difference and passing the sum. This process is repeated recursively, pairing up the sums to prove the next scale, which leads to <math>2^n-1</math> differences and a final sum.\n\n=== Daubechies wavelets ===\n{{main|Daubechies wavelet}}\nThe most commonly used set of discrete wavelet transforms was formulated by the Belgian mathematician [[Ingrid Daubechies]] in 1988. This formulation is based on the use of [[recurrence relation]]s to generate progressively finer discrete samplings of an implicit mother wavelet function; each resolution is twice that of the previous scale. In her seminal paper, Daubechies derives a family of [[Daubechies wavelet|wavelets]], the first of which is the Haar wavelet. Interest in this field has exploded since then, and many variations of Daubechies' original wavelets were developed.<ref>A.N. Akansu, R.A. Haddad and H. Caglar, [http://web.njit.edu/~akansu/PAPERS/Akansu-BinomialQMF-Wavelet-SPIE-VCIP-Sept1990.pdf Perfect Reconstruction Binomial QMF-Wavelet Transform], Proc. SPIE Visual Communications and Image Processing, pp. 609–618, vol. 1360, Lausanne, Sept. 1990.</ref><ref>Akansu, Ali N.; Haddad, Richard A. (1992), Multiresolution signal decomposition: transforms, subbands, and wavelets, Boston, MA: Academic Press, {{ISBN|978-0-12-047141-6}}</ref>\n\n=== The dual-tree complex wavelet transform (DℂWT) ===\n{{main|Complex wavelet transform}}\nThe dual-tree complex wavelet transform (ℂWT) is a relatively recent enhancement to the discrete wavelet transform (DWT), with important additional properties: It is nearly shift invariant and directionally selective in two and higher dimensions. It achieves this with a redundancy factor of only <math>2^d</math> substantially lower than the undecimated DWT. The multidimensional (M-D) dual-tree ℂWT is nonseparable but is based on a computationally efficient, separable filter bank (FB).<ref>Selesnick, I.W.;   Baraniuk, R.G.;   Kingsbury, N.C., 2005, ''The dual-tree complex wavelet transform''</ref>\n\n=== Others ===\nOther forms of discrete wavelet transform include the [[Stationary wavelet transform|non- or undecimated wavelet transform]] (where downsampling is omitted), the [[Newland transform]] (where an [[orthonormal]] basis of wavelets is formed from appropriately constructed [[top-hat filter]]s in [[frequency space]]). [[Wavelet packet decomposition|Wavelet packet transform]]s are also related to the discrete wavelet transform.  [[Complex wavelet transform]] is another form.\n\n== Properties ==\nThe Haar DWT illustrates the desirable properties of wavelets in general. First, it can be performed in <math>O(n)</math> operations; second, it captures not only a notion of the frequency content of the input, by examining it at different scales, but also temporal content, i.e. the times at which these frequencies occur. Combined, these two properties make the [[Fast wavelet transform]] (FWT) an alternative to the conventional [[fast Fourier transform]] (FFT).\n\n=== Time issues ===\nDue to the rate-change operators in the filter bank, the discrete WT is not time-invariant but actually very sensitive to the alignment of the signal in time. To address the time-varying problem of wavelet transforms, Mallat and Zhong proposed a new algorithm for wavelet representation of a signal, which is invariant to time shifts.<ref>S. Mallat, A Wavelet Tour of Signal Processing, 2nd ed. San Diego, CA: Academic, 1999.</ref> According to this algorithm, which is called a TI-DWT, only the scale parameter is sampled along the dyadic sequence 2^j (j∈Z) and the wavelet transform is calculated for each point in time.<ref>S. G. Mallat and S. Zhong, “Characterization of signals from multiscale edges,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 14, no. 7, pp. 710– 732, Jul. 1992.</ref><ref>Ince, Kiranyaz, Gabbouj, 2009, ''A generic and robust system for automated patient-specific classification of ECG signals''</ref>\n\n== Applications ==\nThe discrete wavelet transform has a huge number of applications in science, engineering, mathematics and computer science. Most notably, it is used for [[signal coding]], to represent a discrete signal in a more redundant form, often as a preconditioning for [[data compression]]. Practical applications can also be found in signal processing of accelerations for gait analysis,<ref>[https://www.youtube.com/watch?v=DTpEVQSEBBk \"Novel method for stride length estimation with body area network accelerometers\"], ''IEEE BioWireless 2011'', pp. 79–82</ref> image processing,<ref>{{Cite web|url=http://www.rose-hulman.edu/~brought/Epubs/Imaging/waveimage.html|title=Wavelet Based Methods in Image Processing|last=Broughton|first=S. Allen|website=www.rose-hulman.edu|access-date=2017-05-02}}</ref> in digital communications and many others.<ref>A.N. Akansu and M.J.T. Smith,[https://www.amazon.com/Subband-Wavelet-Transforms-Applications-International/dp/0792396456/ref=sr_1_1?s=books&ie=UTF8&qid=1325018106&sr=1-1 Subband and Wavelet Transforms: Design and Applications], Kluwer Academic Publishers, 1995.</ref>\n<ref>A.N. Akansu and M.J. Medley, [https://www.amazon.com/Transforms-Communications-Multimedia-International-Engineering/dp/1441950869/ref=sr_1_fkmr0_3?s=books&ie=UTF8&qid=1325018358&sr=1-3-fkmr0 Wavelet, Subband and Block Transforms in Communications and Multimedia], Kluwer Academic Publishers, 1999.</ref><ref>A.N. Akansu, P. Duhamel, X. Lin and M. de Courville [http://web.njit.edu/~akansu/PAPERS/AKANSU-ORTHOGONAL-MUX-1998.pdf Orthogonal Transmultiplexers in Communication: A Review], IEEE Trans. On Signal Processing, Special Issue on Theory and Applications of Filter Banks and Wavelets. Vol. 46, No.4, pp. 979–995, April, 1998.</ref>\n\nIt is shown that discrete wavelet transform (discrete in scale and shift, and continuous in time) is successfully implemented as analog filter bank in biomedical signal processing for design of low-power pacemakers and also in ultra-wideband (UWB) wireless communications.<ref>A.N. Akansu, W.A. Serdijn, and I.W. Selesnick, [http://web.njit.edu/~akansu/PAPERS/ANA-IWS-WAS-ELSEVIER%20PHYSCOM%202010.pdf Wavelet Transforms in Signal Processing: A Review of Emerging Applications], Physical Communication, Elsevier, vol. 3, issue 1, pp. 1–18, March 2010.</ref>\n\n== Example in Image Processing ==\n[[File:Noise Wavelet.jpg|thumb|Image with Gaussian noise.|237x237px]]\n[[File:Denosied Wavelet.jpg|thumb|Image with Gaussian noise removed.|237x237px]]\nWavelets are often used to denoise two dimensional signals, such as images.  The following example provides three steps to remove unwanted white Gaussian noise from the noisy image shown. [[Matlab]] was used to import and filter the image.\n\nThe first step is to choose a wavelet type, and a level N of decomposition.  In this case [[Biorthogonal wavelet|biorthogonal]] 3.5 wavelets were chosen with a level N of 10. Biorthogonal wavelets are commonly used in image processing to detect and filter white Gaussian noise,<ref>{{Cite journal|last=Pragada|first=S.|last2=Sivaswamy|first2=J.|date=2008-12-01|title=Image Denoising Using Matched Biorthogonal Wavelets|url=http://ieeexplore.ieee.org/document/4756048/|journal=2008 Sixth Indian Conference on Computer Vision, Graphics Image Processing|pages=25–32|doi=10.1109/ICVGIP.2008.95}}</ref> due to their high contrast of neighboring pixel intensity values. Using this wavelets a [[wavelet transform]]ation is performed on the two dimensional image.\n\nFollowing the decomposition of the image file, the next step is to determine threshold values for each level from 1 to N. Birgé-Massart strategy<ref>{{Cite web|url=https://www.mathworks.com/help/wavelet/ref/wdcbm.html|title=Thresholds for wavelet 1-D using Birgé-Massart strategy - MATLAB wdcbm|website=www.mathworks.com|access-date=2017-05-03}}</ref> is a fairly common method for selecting these thresholds.  Using this process individual thresholds are made for N = 10 levels. Applying these thresholds are the majority of the actual filtering of the signal.\n\nThe final step is to reconstruct the image from the modified levels. This is accomplished using an inverse wavelet transform. The resulting image, with white Gaussian noise removed is shown below the original image. When filtering any form of data it is important to quantify the [[Signal-to-noise ratio|signal-to-noise-ratio]] of the result.<ref>{{Cite book|chapter-url=http://www.intechopen.com/books/advances-in-wavelet-theory-and-theirapplications-in-engineering-physics-and-technology/wavelet-signal-and-image-denoising|chapter=Signal and Image Denoising Using Wavelet Transform|last=Ergen|first=Burhan|date=2012-01-01|publisher=InTech|language=en|doi=10.5772/36434|title=Advances in Wavelet Theory and Their Applications in Engineering, Physics and Technology|isbn=978-953-51-0494-0}}</ref>  In this case, the SNR of the noisy image in comparison to the original was 30.4958%, and the SNR of the denoised image is 32.5525%.  The resulting improvement of the wavelet filtering is a SNR gain of 2.0567%.<ref>{{Cite web|url=https://www.mathworks.com/matlabcentral/answers/71609-how-to-get-snr-for-2-images|title=how to get SNR for 2 images - MATLAB Answers - MATLAB Central|website=www.mathworks.com|access-date=2017-05-10}}</ref>\n\nIt is important to note that choosing other wavelets, levels, and thresholding strategies can result in different types of filtering.  In this example, white Gaussian noise was chosen to be removed. Although, with different thresholding, it could just as easily have been amplified.\n\n== Comparison with Fourier transform ==\n{{see also|Discrete Fourier transform}}\n\nTo illustrate the differences and similarities between the discrete wavelet transform with the [[discrete Fourier transform]], consider the DWT and DFT of the following sequence: (1,0,0,0), a [[unit impulse]].\n\nThe DFT has orthogonal basis ([[DFT matrix]]):\n\n: <math>\n\\begin{bmatrix}\n1 &  1 &  1 &  1\\\\\n1 & -i & -1 &  i\\\\\n1 & -1 &  1 & -1\\\\\n1 &  i & -1 & -i\n\\end{bmatrix}\n</math>\n\nwhile the DWT with Haar wavelets for length 4 data has orthogonal basis in the rows of:\n\n: <math>\n\\begin{bmatrix}\n1 &  1 &  1 &  1\\\\\n1 &  1 & -1 & -1\\\\\n1 & -1 &  0 &  0\\\\\n0 &  0 &  1 & -1\n\\end{bmatrix}\n</math>\n\n(To simplify notation, whole numbers are used, so the bases are [[orthogonal]] but not [[orthonormal]].)\n\nPreliminary observations include:\n* Sinusoidal waves differ only in their frequency. The first does not complete any cycles, the second completes one full cycle, the third completes two cycles, and the fourth completes three cycles (which is equivalent to completing one cycle in the opposite direction). Differences in phase can be represented by multiplying a given basis vector by a complex constant.\n* Wavelets, by contrast, have both frequency and location. As before, the first completes zero cycles, and the second completes one cycle. However, the third and fourth both have the same frequency, twice that of the first. Rather than differing in frequency, they differ in ''location'' — the third is nonzero over the first two elements, and the fourth is nonzero over the second two elements.\n\nDecomposing the sequence with respect to these bases yields:\n\n:<math>\\begin{align}\n(1,0,0,0) &= \\frac{1}{4}(1,1,1,1) + \\frac{1}{4}(1,1,-1,-1) + \\frac{1}{2}(1,-1,0,0)  \\qquad \\text{Haar DWT}\\\\\n(1,0,0,0) &= \\frac{1}{4}(1,1,1,1) + \\frac{1}{4}(1,i,-1,-i) + \\frac{1}{4}(1,-1,1,-1) + \\frac{1}{4}(1,-i,-1,i)  \\qquad \\text{DFT}\n\\end{align}</math>\n\nThe DWT demonstrates the localization: the (1,1,1,1) term gives the average signal value, the (1,1,–1,–1) places the signal in the left side of the domain, and the \n(1,–1,0,0) places it at the left side of the left side, and truncating at any stage yields a downsampled version of the signal:\n:<math>\\begin{align}\n&\\left(\\frac{1}{4},\\frac{1}{4},\\frac{1}{4},\\frac{1}{4}\\right)\\\\\n&\\left(\\frac{1}{2},\\frac{1}{2},0,0\\right)\\qquad\\text{2-term truncation}\\\\\n&\\left(1,0,0,0\\right)\n\\end{align}</math>\n[[File:Sinc function (normalized).svg|thumb|The [[sinc function]], showing the time domain artifacts ([[undershoot (signal)|undershoot]] and [[ringing (signal)|ringing]]) of truncating a Fourier series.]]\nThe DFT, by contrast, expresses the sequence by the interference of waves of various frequencies – thus truncating the series yields a [[low-pass filter]]ed version of the series:\n:<math>\\begin{align}\n&\\left(\\frac{1}{4},\\frac{1}{4},\\frac{1}{4},\\frac{1}{4}\\right)\\\\\n&\\left(\\frac{3}{4},\\frac{1}{4},-\\frac{1}{4},\\frac{1}{4}\\right)\\qquad\\text{2-term truncation}\\\\\n&\\left(1,0,0,0\\right)\n\\end{align}</math>\nNotably, the middle approximation (2-term) differs. From the frequency domain perspective, this is a better approximation, but from the time domain perspective it has drawbacks – it exhibits [[undershoot (signal)|undershoot]] – one of the values is negative, though the original series is non-negative everywhere – and [[ringing (signal)|ringing]], where the right side is non-zero, unlike in the wavelet transform. On the other hand, the Fourier approximation correctly shows a peak, and all points are within <math>1/4</math> of their correct value, though all points have error. The wavelet approximation, by contrast, places a peak on the left half, but has no peak at the first point, and while it is exactly correct for half the values (reflecting location), it has an error of <math>1/2</math> for the other values.\n\nThis illustrates the kinds of trade-offs between these transforms, and how in some respects the DWT provides preferable behavior, particularly for the modeling of transients.\n\n== Definition ==\n\n=== One level of the transform ===\n\nThe DWT of a signal <math>x</math> is calculated by passing it through a series of filters.  First the samples are passed through a [[low pass filter]] with [[impulse response]] <math>g</math>  resulting in a [[convolution]] of the two:\n\n:<math>y[n] = (x * g)[n] = \\sum\\limits_{k =  - \\infty }^\\infty  {x[k] g[n - k]} </math>\n\nThe signal is also decomposed simultaneously using a [[high-pass filter]] <math>h</math>.  The outputs giving the detail coefficients (from the high-pass filter) and approximation coefficients (from the low-pass).  It is important that the two filters are related to each other and they are known as a [[quadrature mirror filter]].\n\n[[Image:Wavelets - DWT.png|frame|none|Block diagram of filter analysis]]\n\nHowever, since half the frequencies of the signal have now been removed, half the samples can be discarded according to Nyquist’s rule.  The filter output of the low-pass filter <math>g</math> in the diagram above is then [[Downsampling|subsampled]] by 2 and further processed by passing it again through a new low- pass filter <math>g</math>  and a high- pass filter <math>h</math> with  half the cut-off frequency of the previous one,i.e.:\n\n:<math>y_{\\mathrm{low}} [n] = \\sum\\limits_{k =  - \\infty }^\\infty  {x[k] g[2 n - k]} </math>\n:<math>y_{\\mathrm{high}} [n] = \\sum\\limits_{k =  - \\infty }^\\infty  {x[k] h[2 n - k]} </math>\n\nThis decomposition has halved the time resolution since only half of each filter output characterises the signal.  However, each output has half the frequency band of the input, so the frequency resolution has been doubled.\n\nWith the [[downsampling|subsampling operator]] <math>\\downarrow</math>\n\n:<math>(y \\downarrow k)[n] = y[k n] </math>\n\nthe above summation can be written more concisely.\n\n:<math>y_{\\mathrm{low}} = (x*g)\\downarrow 2 </math>\n:<math>y_{\\mathrm{high}} = (x*h)\\downarrow 2 </math>\n\nHowever computing a complete convolution <math>x*g</math> with subsequent downsampling would waste computation time.\n\nThe [[Lifting scheme]] is an optimization where these two computations are interleaved.\n\n=== Cascading and filter banks ===\n\nThis decomposition is repeated to further increase the frequency resolution and the approximation coefficients decomposed with high and low pass filters and then down-sampled.  This is represented as a binary tree with nodes representing a sub-space with a different time-frequency localisation.  The tree is known as a [[filter bank]].\n\n[[Image:Wavelets - Filter Bank.png|frame|none|A 3 level filter bank]]\n\nAt each level in the above diagram the signal is decomposed into low and high frequencies.  Due to the decomposition process the input signal must be a multiple of <math>2^n</math> where <math>n</math> is the number of levels.\n\nFor example a signal with 32 samples, frequency range 0 to <math>f_n</math> and 3 levels of decomposition, 4 output scales are produced:\n\n{| class=\"wikitable\"\n! Level\n! Frequencies\n! Samples\n|-\n| rowspan=\"2\" | 3\n| <math>0</math> to <math>{{f_n}}/8</math>\n| 4\n|-\n| <math>{{f_n}}/8</math> to <math>{{f_n}}/4</math>\n| 4\n|-\n| 2\n| <math>{{f_n}}/4</math> to <math>{{f_n}}/2</math>\n| 8\n|-\n| 1\n| <math>{{f_n}}/2</math> to <math>f_n</math>\n| 16\n|}\n\n[[Image:Wavelets - DWT Freq.png|frame|none|Frequency domain representation of the DWT]]\n\n== Relationship to the mother wavelet ==\n\nThe filterbank implementation of wavelets can be interpreted as computing the wavelet coefficients of a [[Wavelet#Discrete wavelet transforms .28discrete shift and scale parameters.29|discrete set of child wavelets]] for a given mother wavelet <math>\\psi(t)</math>.  In the case of the discrete wavelet transform, the mother wavelet is shifted and scaled by powers of two\n\n<math> \\psi_{j,k}(t)= \\frac{1}{\\sqrt{2^j}} \\psi \\left( \\frac{t - k 2^j}{2^j} \\right) </math>\n\nwhere <math>j</math> is the scale parameter and <math>k</math> is the shift parameter, both which are integers.\n\nRecall that the wavelet coefficient <math>\\gamma</math> of a signal <math>x(t)</math> is the projection of <math>x(t)</math> onto a wavelet, and let <math>x(t)</math> be a signal of length <math>2^N</math>.  In the case of a child wavelet in the discrete family above,\n\n<math> \\gamma_{jk} = \\int_{-\\infty}^{\\infty} x(t)  \\frac{1}{\\sqrt{2^j}} \\psi \\left( \\frac{t - k 2^j}{2^j} \\right) dt </math>\n\nNow fix <math>j</math> at a particular scale, so that <math> \\gamma_{jk} </math> is a function of <math>k</math> only.  In light of the above equation, <math>\\gamma_{jk}</math> can be viewed as a [[convolution]] of <math>x(t)</math> with a dilated, reflected, and normalized version of the mother wavelet, <math>h(t) =   \\frac{1}{\\sqrt{2^j}} \\psi \\left( \\frac{-t}{2^j} \\right) </math>, sampled at the points <math>1, 2^j, 2^{2j}, ..., 2^{N}</math>.  But this is precisely what the detail coefficients give at level <math>j</math> of the discrete wavelet transform.  Therefore, for an appropriate choice of <math>h[n]</math> and <math>g[n]</math>, the detail coefficients of the filter bank correspond exactly to a wavelet coefficient of a discrete set of child wavelets for a given mother wavelet <math>\\psi(t)</math>.\n\nAs an example, consider the discrete [[Haar wavelet]], whose mother wavelet is <math>\\psi = [1, -1]</math>.  Then the dilated, reflected, and normalized version of this wavelet is <math>h[n] = \\frac{1}{\\sqrt{2}} [-1, 1]</math>, which is, indeed, the highpass decomposition filter for the discrete Haar wavelet transform.\n\n== Time complexity ==\n\nThe filterbank implementation of the Discrete Wavelet Transform takes only [[Big O notation|O(''N'')]] in certain cases, as compared to O(''N''&nbsp;log&nbsp;''N'') for the [[fast Fourier transform]].\n\nNote that if <math>g[n]</math> and <math>h[n]</math> are both a constant length (i.e. their length is independent of N), then <math>x * h</math> and <math>x * g</math> each take [[Big O notation|O(''N'')]] time.  The wavelet filterbank does each of these two [[Big O notation|O(''N'')]] convolutions, then splits the signal into two branches of size N/2.  But it only recursively splits the upper branch convolved with <math>g[n]</math> (as contrasted with the FFT, which recursively splits both the upper branch and the lower branch).  This leads to the following [[recurrence relation]]\n\n: <math>T(N) = 2N + T\\left( \\frac N 2 \\right)</math>\n\nwhich leads to an [[Big O notation|O(''N'')]] time for the entire operation, as can be shown by a [[geometric series]] expansion of the above relation.\n\nAs an example, the discrete [[Haar wavelet]] transform is linear, since in that case <math>h[n]</math> and <math>g[n]</math> are constant length&nbsp;2.\n\n: <math>h[n] = \\left[\\frac{-\\sqrt{2}}{2}, \\frac{\\sqrt{2}}{2}\\right]  g[n] =  \\left[\\frac{\\sqrt{2}}{2}, \\frac{\\sqrt{2}}{2}\\right]</math>\n\n== Other transforms ==\n{{see also|Adam7 algorithm}}\nThe [[Adam7 algorithm]], used for [[Interlacing (bitmaps)|interlacing]] in the [[Portable Network Graphics]] (PNG) format, is a multiscale model of the data\nwhich is similar to a DWT with [[Haar wavelet]]s.\n\nUnlike the DWT, it has a specific scale – it starts from an 8×8 block, and it [[downsample]]s the image, rather than [[decimation (signal processing)|decimating]] ([[low-pass filter]]ing, then downsampling). It thus offers worse frequency behavior, showing artifacts ([[pixelation]]) at the early stages, in return for simpler implementation.\n\n== Code example ==\n\nIn its simplest form, the DWT is remarkably easy to compute.\n\nThe [[Haar wavelet]] in [[Java (programming language)|Java]]:\n<source lang=java>\npublic static int[] discreteHaarWaveletTransform(int[] input) {\n    // This function assumes that input.length=2^n, n>1\n    int[] output = new int[input.length];\n\n    for (int length = input.length / 2; ; length = length / 2) {\n        // length is the current length of the working area of the output array.\n        // length starts at half of the array size and every iteration is halved until it is 1.\n        for (int i = 0; i < length; ++i) {\n            int sum = input[i * 2] + input[i * 2 + 1];\n            int difference = input[i * 2] - input[i * 2 + 1];\n            output[i] = sum;\n            output[length + i] = difference;\n        }\n        if (length == 1) {\n            return output;\n        }\n\n        //Swap arrays to do next iteration\n        System.arraycopy(output, 0, input, 0, length);\n    }\n}\n</source>\nComplete Java code for a 1-D and 2-D DWT using [[Haar wavelet|Haar]], [[Daubechies wavelet|Daubechies]], [[Coiflet]], and [[Legendre wavelet|Legendre]] wavelets is available from the open source project: [https://github.com/cscheiblich/JWave JWave].\nFurthermore, a fast lifting implementation of the discrete biorthogonal [[Cohen-Daubechies-Feauveau wavelet|CDF]] 9/7 wavelet transform in [[C (programming language)|C]], used in the [[JPEG 2000]] image compression standard can be found [https://web.archive.org/web/20120305164605/http://www.embl.de/~gpau/misc/dwt97.c here] (archived 5 March 2012).\n\n=== Example of above code ===\n[[Image:Haar DWT of the Sound Waveform \"I Love Wavelets\".png|thumb|300px|An example of computing the discrete Haar wavelet coefficients for a sound signal of someone saying \"I Love Wavelets.\"  The original waveform is shown in blue in the upper left, and the wavelet coefficients are shown in black in the upper right.  Along the bottom is shown three zoomed-in regions of the wavelet coefficients for different ranges.]]\n\nThis figure shows an example of applying the above code to compute the Haar wavelet coefficients on a sound waveform.  This example highlights two key properties of the wavelet transform:\n\n*Natural signals often have some degree of smoothness, which makes them sparse in the wavelet domain.  There are far fewer significant components in the wavelet domain in this example than there are in the time domain, and most of the significant components are towards the coarser coefficients on the left.  Hence, natural signals are compressible in the wavelet domain.\n*The wavelet transform is a multiresolution, bandpass representation of a signal.  This can be seen directly from the filterbank definition of the discrete wavelet transform given in this article.  For a signal of length <math>2^N</math>, the coefficients in the range <math>[2^{N-j}, 2^{N-j+1}]</math> represent a version of the original signal which is in the pass-band <math> \\left[ \\frac{\\pi}{2^j}, \\frac{\\pi}{2^{j-1}} \\right]</math>.  This is why zooming in on these ranges of the wavelet coefficients looks so similar in structure to the original signal.  Ranges which are closer to the left (larger <math>j</math> in the above notation), are coarser representations of the signal, while ranges to the right represent finer details.\n\n==See also==\n* [[Wavelet]]\n* [[Wavelet series]]\n* [[Wavelet compression]]\n* Wavelet entropy\n* [[List of wavelet-related transforms]]\n\n==Notes==\n{{reflist|group=note}}\n\n==References==\n{{Reflist}}\n\n==External links==\n* Stanford's [http://statweb.stanford.edu/~wavelab/ WaveLab] in matlab\n* [http://www.fit.vutbr.cz/research/view_product.php?id=211&notitle=1 libdwt], a cross-platform DWT library written in C\n* [https://www.scribd.com/document/334619872/Concise-Introduction-to-Wavelets Concise Introduction to Wavelets] by René Puchinger\n\n{{DEFAULTSORT:Discrete Wavelet Transform}}\n[[Category:Numerical analysis]]\n[[Category:Digital signal processing]]\n[[Category:Wavelets]]\n[[Category:Articles with example Java code]]\n[[Category:Discrete transforms]]\n\n[[de:Wavelet-Transformation#Diskrete Wavelet-Transformation]]\n[[fr:Ondelette#Transformée en ondelettes discrète]]"
    },
    {
      "title": "Discretization",
      "url": "https://en.wikipedia.org/wiki/Discretization",
      "text": "[[Image:Finite element solution.svg|right|thumb|A solution to a discretized partial differential equation, obtained with the [[finite element method]].]]\nIn [[applied mathematics]], '''discretization''' is the process of transferring [[continuous function|continuous]] functions, models, variables, and equations into [[wiktionary:discrete|discrete]] counterparts.  This process is usually carried out as a first step toward making them suitable for numerical evaluation and implementation on digital computers. '''Dichotomization''' is the special case of discretization in which the number of discrete classes is 2, which can approximate a continuous variable as a [[binary variable]] (creating a [[dichotomy]] for [[conceptual model|modeling]] purposes, as in [[binary classification]]). \n\nDiscretization is also related to [[discrete mathematics]], and is an important component of [[granular computing]].  In this context, ''discretization'' may also refer to modification of variable or category ''granularity'', as when multiple discrete variables are aggregated or multiple discrete categories fused.\n\nWhenever continuous data is '''discretized''', there is always some amount of [[discretization error]]. The goal is to reduce the amount to a level considered [[wikt:negligible|negligible]] for the [[conceptual model|modeling]] purposes at hand.\n\n{{Anchor|Discretization-quantization}} The terms ''discretization '' and ''[[Quantization (signal processing)|quantization]]''<!-- Do not tag bomb this link with {{dn}}; its target is at the intentional and appropriate level of polysemy. --> often have the same [[denotation]] but not always identical [[connotations]].<!-- If you need deeper exploration of this truth, you will need to go read about [[cognitive synonymy]] and the lack of it in natural language. --> (Specifically, the two terms share a [[semantic field]].) The same is true of [[discretization error]] and [[quantization error]].\n\nMathematical methods relating to discretization include the [[Euler–Maruyama method]] and the [[zero-order hold]].\n\n== Discretization of linear state space models {{anchor|discrete function}} ==\nDiscretization is also concerned with the transformation of continuous [[differential equation]]s into discrete [[difference equations]], suitable for [[Numerical analysis|numerical computing]].\n\nThe following continuous-time [[State space (controls)|state space model]]\n\n:<math>\\dot{\\mathbf{x}}(t) = \\mathbf A \\mathbf{x}(t) + \\mathbf B \\mathbf{u}(t) + \\mathbf{w}(t)</math>\n:<math>\\mathbf{y}(t) = \\mathbf C \\mathbf{x}(t) + \\mathbf D \\mathbf{u}(t) + \\mathbf{v}(t)</math>\n\nwhere ''v'' and ''w'' are continuous zero-mean [[white noise]] sources with [[power spectral density | power spectral densities]]\n:<math>\\mathbf{w}(t) \\sim N(0,\\mathbf Q)</math>\n:<math>\\mathbf{v}(t) \\sim N(0,\\mathbf R)</math>\n\ncan be discretized, assuming [[zero-order hold]] for the input ''u'' and continuous integration for the noise ''v'', to\n\n:<math>\\mathbf{x}[k+1] = \\mathbf A_d \\mathbf{x}[k] + \\mathbf B_d \\mathbf{u}[k] + \\mathbf{w}[k]</math>\n:<math>\\mathbf{y}[k] = \\mathbf C_d \\mathbf{x}[k] + \\mathbf D_d \\mathbf{u}[k] +  \\mathbf{v}[k]</math>\n\nwith covariances\n\n:<math>\\mathbf{w}[k] \\sim N(0,\\mathbf Q_d)</math>\n:<math>\\mathbf{v}[k] \\sim N(0,\\mathbf R_d)</math>\n\nwhere\n\n:<math>\\mathbf A_d = e^{\\mathbf A T} = \\mathcal{L}^{-1}\\{(s\\mathbf I - \\mathbf A)^{-1}\\}_{t=T} </math>\n:<math>\\mathbf B_d = \\left( \\int_{\\tau=0}^{T}e^{\\mathbf A \\tau}d\\tau \\right) \\mathbf B = \\mathbf A^{-1}(\\mathbf A_d - I)\\mathbf B </math>, if <math>\\mathbf A</math> is [[Invertible matrix|nonsingular]]\n:<math>\\mathbf C_d = \\mathbf C </math>\n:<math>\\mathbf D_d = \\mathbf D </math>\n:<math>\\mathbf Q_d = \\int_{\\tau=0}^{T} e^{\\mathbf A \\tau} \\mathbf Q e^{\\mathbf A^\\top \\tau}  d\\tau </math>\n:<math>\\mathbf R_d = \\mathbf R T </math>\n\nand <math>T</math> is the sample time, although <math>\\mathbf A^\\top</math> is the transposed matrix of <math>\\mathbf A</math>.\n\nA clever trick to compute ''A''<sub>''d''</sub> and ''B''<sub>''d''</sub> in one step is by utilizing the following property:<ref>Raymond DeCarlo: ''Linear Systems: A State Variable Approach with Numerical Implementation'', Prentice Hall, NJ, 1989</ref>{{rp|p. 215}}\n:<math>e^{\\begin{bmatrix} \\mathbf{A} & \\mathbf{B} \\\\\n                 \\mathbf{0} & \\mathbf{0} \\end{bmatrix} T} = \\begin{bmatrix} \\mathbf{A_{d}} & \\mathbf{B_{d}} \\\\\n                                                            \\mathbf{0} & \\mathbf{I} \\end{bmatrix}</math>\n\nWhere <math>\\mathbf A_d</math>and <math>\\mathbf B_d</math>are the discretized state-space matrices.\n\n=== Discretization of process noise ===\nNumerical evaluation of <math>\\mathbf{Q}_d</math> is a bit trickier due to the matrix exponential integral. It can, however, be computed by first constructing a matrix, and computing the exponential of it <ref>Charles Van Loan: ''Computing integrals involving the matrix exponential'', IEEE Transactions on Automatic Control. 23 (3): 395–404, 1978</ref>\n:<math> \\mathbf{F} = \n\\begin{bmatrix} -\\mathbf{A} & \\mathbf{Q} \\\\\n                 \\mathbf{0} & \\mathbf{A}^\\top \\end{bmatrix} T</math>\n:<math> \\mathbf{G} = e^\\mathbf{F} =\n\\begin{bmatrix} \\dots & \\mathbf{A}_d^{-1}\\mathbf{Q}_d \\\\\n           \\mathbf{0} & \\mathbf{A}_d^\\top             \\end{bmatrix}.</math>\nThe discretized process noise is then evaluated by multiplying the transpose of the lower-right partition of '''G''' with the upper-right partition of '''G''':\n:<math>\\mathbf{Q}_d = (\\mathbf{A}_d^\\top)^\\top (\\mathbf{A}_d^{-1}\\mathbf{Q}_d) = \\mathbf{A}_d (\\mathbf{A}_d^{-1}\\mathbf{Q}_d). </math>\n\n=== Derivation ===\nStarting with the continuous model\n:<math>\\mathbf{\\dot{x}}(t) = \\mathbf A\\mathbf x(t) + \\mathbf B \\mathbf u(t)</math>\nwe know that the [[matrix exponential]] is\n:<math>\\frac{d}{dt}e^{\\mathbf At} = \\mathbf A e^{\\mathbf At} = e^{\\mathbf At} \\mathbf A</math>\nand by premultiplying the model we get\n:<math>e^{-\\mathbf At} \\mathbf{\\dot{x}}(t) = e^{-\\mathbf At} \\mathbf A\\mathbf x(t) + e^{-\\mathbf At} \\mathbf B\\mathbf u(t)</math>\nwhich we recognize as\n:<math>\\frac{d}{dt}(e^{-\\mathbf At}\\mathbf x(t)) = e^{-\\mathbf At} \\mathbf B\\mathbf u(t)</math>\nand by integrating..\n:<math>e^{-\\mathbf At}\\mathbf x(t) - e^0\\mathbf x(0) = \\int_0^t e^{-\\mathbf A\\tau}\\mathbf B\\mathbf u(\\tau) d\\tau</math>\n:<math>\\mathbf x(t) = e^{\\mathbf At}\\mathbf x(0) + \\int_0^t e^{\\mathbf A(t-\\tau)} \\mathbf B\\mathbf u(\\tau) d \\tau</math>\nwhich is an analytical solution to the continuous model.\n\nNow we want to discretise the above expression. We assume that u is [[mathematical constant|constant]] during each timestep.\n:<math>\\mathbf x[k] \\ \\stackrel{\\mathrm{def}}{=}\\  \\mathbf x(kT)</math>\n:<math>\\mathbf x[k] = e^{\\mathbf AkT}\\mathbf x(0) + \\int_0^{kT} e^{\\mathbf A(kT-\\tau)} \\mathbf B\\mathbf u(\\tau) d \\tau</math>\n:<math>\\mathbf x[k+1] = e^{\\mathbf A(k+1)T}\\mathbf x(0) + \\int_0^{(k+1)T} e^{\\mathbf A((k+1)T-\\tau)} \\mathbf B\\mathbf u(\\tau) d \\tau</math>\n:<math>\\mathbf x[k+1] = e^{\\mathbf AT} \\left[  e^{\\mathbf AkT}\\mathbf x(0) + \\int_0^{kT} e^{\\mathbf A(kT-\\tau)} \\mathbf B\\mathbf u(\\tau) d \\tau \\right]+ \\int_{kT}^{(k+1)T} e^{\\mathbf A(kT+T-\\tau)} \\mathbf B\\mathbf u(\\tau) d \\tau</math>\nWe recognize the bracketed expression as <math>\\mathbf x[k]</math>, and the second term can be simplified by substituting with the function <math>v(\\tau) = kT + T - \\tau</math>. Note that <math>d\\tau=-dv</math>. We also assume that <math>\\mathbf u</math> is constant during the [[integral]], which in turn yields\n:<math> \\begin{matrix} \\mathbf x[k+1]&=& e^{\\mathbf AT}\\mathbf x[k] - \\left( \\int_{v(kT)}^{v((k+1)T)} e^{\\mathbf Av} dv \\right) \\mathbf B\\mathbf u[k] \\\\\n&=& e^{\\mathbf AT}\\mathbf x[k] - \\left( \\int_T^0 e^{\\mathbf Av} dv \\right) \\mathbf B\\mathbf u[k] \\\\\n&=& e^{\\mathbf AT}\\mathbf x[k] + \\left( \\int_0^T e^{\\mathbf Av} dv \\right) \\mathbf B\\mathbf u[k] \\\\\n&=&e^{\\mathbf AT}\\mathbf x[k] + \\mathbf A^{-1}\\left(e^{\\mathbf AT}-\\mathbf I \\right) \\mathbf B\\mathbf u[k] \\end{matrix}</math>\nwhich is an exact solution to the discretization problem.\n\n=== Approximations ===\nExact discretization may sometimes be intractable due to the heavy matrix exponential and integral operations involved. It is much easier to calculate an approximate discrete model, based on that for small timesteps <math>e^{\\mathbf AT} \\approx \\mathbf I + \\mathbf A T</math>. The approximate solution then becomes:\n:<math>\\mathbf x[k+1] \\approx (\\mathbf I + \\mathbf AT) \\mathbf x[k] + T\\mathbf B \\mathbf u[k] </math>\n\nThis is also known as [[Euler method|Euler's method]]. Other possible approximations are <math>e^{\\mathbf AT} \\approx \\left( \\mathbf I - \\mathbf A T \\right)^{-1}</math> and <math>e^{\\mathbf AT} \\approx \\left( \\mathbf I +\\frac{1}{2}  \\mathbf A T \\right) \\left( \\mathbf I - \\frac{1}{2} \\mathbf A T \\right)^{-1}</math>. Each of them have different stability properties. The last one is known as the [[bilinear transform]], or Tustin transform, and preserves the (in)stability of the continuous-time system.\n\n== Discretization of continuous features ==\n\n{{Main|Discretization of continuous features}}\nIn [[statistics]] and machine learning, '''discretization''' refers to the process of converting continuous features or variables to discretized or nominal features. This can be useful when creating probability mass functions.\n\n==See also==\n*[[Discrete event simulation]]\n*[[Discrete space]]\n*[[Discrete time and continuous time]]\n*[[Finite difference method]]\n*[[Finite volume method for unsteady flow]]\n*[[Stochastic simulation]]\n*[[Time-scale calculus]]\n\n== References ==\n<references/>\n\n==Further reading==\n* {{cite book|author=Robert Grover Brown & Patrick Y. C. Hwang|title=Introduction to random signals and applied Kalman filtering|edition=3rd|isbn=978-0471128397}}\n* {{cite book|publisher=Saunders College Publishing|location=Philadelphia, PA, USA|year=1984|author=Chi-Tsong Chen|title=Linear System Theory and Design|isbn=978-0030716911}}\n* {{cite journal|author=C. Van Loan|title=Computing integrals involving the matrix exponential|url=http://ecommons.cornell.edu/bitstream/1813/7095/1/77-298.pdf|doi=10.1109/TAC.1978.1101743|via=|journal=IEEE Transactions on Automatic Control|volume=23|issue=3|pages=395–404|date=Jun 1978}}\n* {{cite book|author=R.H. Middleton & G.C. Goodwin |title=Digital control and estimation: a unified approach|year=1990|page=33f|isbn=978-0132116657}}\n\n==External links==\n{{sisterlinks}}\n\n[[Category:Numerical analysis]]\n[[Category:Applied mathematics]]\n[[Category:Functional analysis]]\n[[Category:Iterative methods]]\n[[Category:Control theory]]\n\n[[de:Diskretisierung]]\n[[hr:Diskretizacija]]\n[[it:Discretizzazione]]\n[[zh:离散化]]"
    },
    {
      "title": "Discretization error",
      "url": "https://en.wikipedia.org/wiki/Discretization_error",
      "text": "{{refimprove|date=December 2009}}\nIn [[numerical analysis]], [[computational physics]], and [[simulation]], '''discretization error'''  is the [[error]] resulting from the fact that a [[function (mathematics)|function]] of a [[continuum (set theory)|continuous]] variable is represented in the computer by a finite number of evaluations, for example, on a [[lattice model (physics)|lattice]].  Discretization error can usually be reduced by using a more finely spaced lattice, with an increased [[Computational complexity theory|computational cost]].\n\n==Examples==\nDiscretization error is the principal source of error in methods of [[finite difference]]s and the [[pseudo-spectral method]] of computational physics.\n\nWhen we define the derivative of <math>\\,\\!f(x)</math> as <math>f'(x) = \\lim_{h\\rightarrow0}{\\frac{f(x+h)-f(x)}{h}}</math> or <math>f'(x)\\approx\\frac{f(x+h)-f(x)}{h}</math>, where <math>\\,\\!h</math> is a finitely small number, the difference between the first formula and this approximation is known as discretization error.\n\n==Related phenomena==\nIn [[signal processing]], the analog of discretization is [[Sampling (signal processing)|sampling]], and results in no loss if the conditions of the [[sampling theorem]] are satisfied, otherwise the resulting error is called [[aliasing]].\n\nDiscretization error, which arises from finite resolution in the ''domain,'' should not be confused with [[quantization error]], which is finite resolution in the ''range'' (values), nor in [[round-off error]] arising from [[floating point]] arithmetic.  Discretization error would occur even if it were possible to represent the values exactly and use exact arithmetic – it is the error from representing a function by its values at a discrete set of points, not an error in these values.<ref>{{cite book|first=Nicholas | last=Higham |title=Accuracy and Stability of Numerical Algorithms (2 ed)| publisher=SIAM|year=2002 | pages=5  }}</ref>\n\n==References==\n{{Reflist}}\n\n==See also==\n* [[Discretization]]\n* [[Linear multistep method]]\n* [[Quantization error]]\n\n{{DEFAULTSORT:Discretization Error}}\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Dynamic relaxation",
      "url": "https://en.wikipedia.org/wiki/Dynamic_relaxation",
      "text": "'''Dynamic relaxation''' is a numerical method, which, among other things, can be used do \"[[form-finding]]\" for [[Tensile structure|cable and fabric structures]]. The aim is to find a geometry where all forces are in [[Mechanical equilibrium|equilibrium]]. In the past this was done by direct modelling, using hanging chains and weights (see [[Gaudi]]), or by using [[soap film]]s, which have the property of adjusting to find a \"[[minimal surface]]\".\n\nThe dynamic relaxation method is based on discretizing the continuum under consideration by lumping the mass at nodes and defining the relationship between nodes in terms of stiffness (see also the [[finite element]] method). The system oscillates about the equilibrium position under the influence of loads. An iterative process is followed by simulating a pseudo-[[dynamics (mechanics)|dynamic]] process in time, with each iteration based on an update of the geometry,<ref>W J LEWIS, ''TENSION STRUCTURES: Form and behaviour'', London, Telford, 2003</ref> similar to [[Leapfrog integration]] and related to Velocity [[Verlet integration]].\n\n==Main equations use==\nConsidering [[Newton's Laws|Newton's second law of motion]] (force is mass multiplied by acceleration) in the <math>x</math> direction at the <math>i</math><sup>th</sup> node at time <math>t</math>:\n:<math>R_{ix}(t)=M_{i}A_{ix}(t)\\frac{}{}</math>\nWhere:\n:<math>R</math> is the residual force\n:<math>M</math> is the nodal mass\n:<math>A</math> is the nodal acceleration\n\nNote that fictitious nodal masses may be chosen to speed up the process of form-finding.\n\nThe relationship between the speed <math>V</math>, the geometry <math>X</math> and the residuals can be obtained by performing a double numerical integration of the acceleration (here in [[central difference|central finite difference]] form<ref>D S WAKEFIELD, ''Engineering analysis of tension structures: theory and practice'', Bath, Tensys Limited, 1999</ref>), :\n\n:<math>V_{ix}\\left(t+ \\frac {\\Delta t} {2}\\right) = V_{ix} \\left(t- \\frac {\\Delta t} {2}\\right) + \\frac{\\Delta t}{M_i}R_{ix}(t)</math>\n\n:<math>X_i(t+ \\Delta t)=X_i(t)+\\Delta t \\times V_{ix} \\left(t+ \\frac {\\Delta t} {2}\\right) </math>\n\nWhere:\n:<math>\\Delta t</math> is the time interval between two updates.\nBy the principle of equilibrium of forces, the relationship between the residuals and the geometry can be obtained:\n\n:<math>R_{ix}(t+ \\Delta t)=P_{ix}(t+ \\Delta t)+\\sum \\frac {T_m(t+ \\Delta t)}{l_m(t+ \\Delta t)} \\times (X_j(t+ \\Delta t)-X_i(t+ \\Delta t))</math>\n\nwhere:\n\n:<math>P</math> is the applied load component\n:<math>T</math> is the tension in link <math>m</math> between nodes <math>i</math> and <math>j</math>\n:<math>l</math> is the length of the link.\nThe sum must cover the forces in all the connections between the node and other nodes.\nBy repeating the use of the relationship between the residuals and the geometry, and the relationship between the geometry and the residual, the pseudo-dynamic process is simulated.\n\n==Iteration Steps==\n1. Set the initial kinetic energy and all nodal velocity components to zero:\n:<math>E_k(t=0)=0\\frac{}{}</math>\n:<math>V_i(t=0)=0\\frac{}{}</math>\n2. Compute the geometry set and the applied load component:\n:<math>X_i(t=0)\\frac{}{}</math>\n:<math>P_i(t=0)\\frac{}{}</math>\n3. Compute the residual:\n:<math>T_m(t)\\frac{}{}</math>\n:<math>R_i(t)\\frac{}{}</math>\n4. Reset the residuals of constrained nodes to zero\n\n5. Update velocity and coordinates:\n:<math>V_i(t+ \\frac {\\Delta t}{2})\\frac{}{}</math>\n:<math>X_i(t+\\Delta t)\\frac{}{}</math>\n6. Return to step 3 until the structure is in static [[Mechanical equilibrium|equilibrium]]\n\n==Damping==\nIt is possible to make dynamic relaxation more computationally efficient (reducing the number of iterations) by using damping.<ref>W J LEWIS, ''TENSION STRUCTURES: Form and behaviour'', London, Telford, 2003</ref>\nThere are two methods of damping:\n*Viscous damping, which assumes that connection between the nodes has a viscous force component.\n*Kinetic energy damping, where the coordinates at peak kinetic energy are calculated (the equilibrium position), then updates the geometry to this position and resets the velocity to zero.\nThe advantage of viscous damping is that it represents the reality of a cable with viscous properties. Moreover, it is easy to realize because the speed is already computed.\nThe kinetic energy damping is an artificial damping which is not a real effect, but offers a drastic reduction in the number of iterations required to find a solution. However, there is a computational penalty in that the kinetic energy and peak location must be calculated, after which the geometry has to be updated to this position.\n\n==See also==\n*[[Tensile structure]]s\n*[[Optimization (mathematics)]]\n\n==Further reading==\n*A S Day, ''An introduction to dynamic relaxation.'' The Engineer 1965, 219:218–221\n*W J LEWIS, ''TENSION STRUCTURES: Form and behaviour'', London, Telford, 2003\n*D S WAKEFIELD, ''Engineering analysis of tension structures: theory and practice'', Bath, Tensys Limited, 1999\n*H.A. BUCHHOLDT, ''An introduction to cable roof structures'', 2nd ed, London, Telford, 1999\n\n==References==\n<references/>\n\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Equioscillation theorem",
      "url": "https://en.wikipedia.org/wiki/Equioscillation_theorem",
      "text": "{{Short description|Theorem}}\n{{multiple issues|\n{{context|date=March 2012}}\n{{no footnotes|date=March 2012}}\n}}\nThe '''equioscillation theorem''' concerns the [[Approximation theory|approximation]] of [[continuous function]]s using [[polynomial]]s when the merit function is the maximum difference ([[uniform norm]]). Its discovery is attributed to [[Pafnuty Chebyshev|Chebyshev]].\n\n== Statement ==\nLet <math>f</math> be a continuous function from <math>[a,b]</math> to <math>\\mathbf{R}</math>. Among all the polynomials of degree <math>\\le n</math>, the polynomial <math>g</math> minimizes the uniform norm of the difference <math> || f - g || _\\infty </math> if and only if there are <math>n+2</math> points <math>a \\le x_0 < x_1 < \\cdots < x_{n+1} \\le b</math> such that <math>f(x_i) - g(x_i) = \\sigma (-1)^i || f - g || _\\infty</math> where <math>\\sigma = \\pm 1</math>.\n\n== Algorithms ==\nSeveral [[minimax approximation algorithm]]s are available, the most common being the [[Remez algorithm]].\n\n== References ==\n* {{webarchive |url=https://web.archive.org/web/20110702221651/http://www.math.uiowa.edu/~jeichhol/qual%20prep/Notes/cheb-equiosc-thm_2007.pdf |date=July 2, 2011 |title=Notes on how to prove Chebyshev’s equioscillation theorem }}\n* [http://www.maa.org/publications/periodicals/loci/joma/the-chebyshev-equioscillation-theorem The Chebyshev Equioscillation Theorem by Robert Mayans]\n\n[[Category:Polynomials]]\n[[Category:Numerical analysis]]\n[[Category:Theorems in analysis]]\n\n\n{{mathanalysis-stub}}"
    },
    {
      "title": "Error analysis (mathematics)",
      "url": "https://en.wikipedia.org/wiki/Error_analysis_%28mathematics%29",
      "text": "In mathematics, '''error analysis''' is the study of kind and quantity of [[error]], or uncertainty, that may be present in the solution to a problem.  This issue is particularly prominent in applied areas such as [[numerical analysis]] and [[statistics]].\n\n== Error analysis in numerical modeling ==\n\nIn numerical simulation or modeling of real systems, error analysis is concerned with the changes in the output of the model as the parameters to the model [[variance|vary]] about a [[mean]].\n\nFor instance, in a system modeled as a function of two variables <math>\\scriptstyle z \\,=\\, f(x,y)</math>.  Error analysis deals with the propagation of the [[numerical error]]s in <math>\\scriptstyle x</math> and <math>\\scriptstyle y</math> (around mean values <math>\\scriptstyle\\bar{x}</math> and <math>\\scriptstyle\\bar{y}</math>) to error in <math>\\scriptstyle z</math> (around a mean <math>\\scriptstyle\\bar{z}</math>).<ref>{{cite book|title=Modeling Biological Systems: Principles and Applications|author=James W. Haefner|pages=186&ndash;189|date=1996|publisher=Springer|isbn=0412042010}}</ref>\n\nIn numerical analysis, error analysis comprises both '''forward error analysis''' and '''backward error analysis'''.\n\n===Forward error analysis===\nForward error analysis involves the analysis of a function <math>\\scriptstyle z' = f'(a_0,\\,a_1,\\,\\dots,\\,a_n)</math> which is an approximation (usually a finite polynomial) to a function <math>\\scriptstyle z \\,=\\, f(a_0,a_1,\\dots,a_n)</math> to determine the bounds on the error in the approximation; i.e., to find <math>\\scriptstyle\\epsilon</math> such that <math>\\scriptstyle 0 \\,\\le\\, |z - z'| \\,\\le\\, \\epsilon</math>. The evaluation of forward errors is desired in [[validated numerics]]<ref>Tucker, W. (2011). Validated numerics: a short introduction to rigorous computations. Princeton University Press.</ref>.\n\n===Backward error analysis===\n\nBackward error analysis involves the analysis of the approximation function <math>\\scriptstyle z' \\,=\\, f'(a_0,\\,a_1,\\,\\dots,\\,a_n)</math>, to determine the bounds on the parameters <math>\\scriptstyle a_i \\,=\\, \\bar{a_i} \\,\\pm\\, \\epsilon_i</math> such that the result <math>\\scriptstyle z' \\,=\\, z</math>.<ref>{{cite book|title=Schaum's Outline of Theory and Problems of Numerical Analysis|author=Francis J. Scheid|pages=11|date=1988|publisher=McGraw-Hill Professional|isbn=0070552215}}</ref>\n\nBackward error analysis, the theory of which was developed and popularized by [[James H. Wilkinson]], can be used to establish that an algorithm implementing a numerical function is numerically stable.<ref name=\"RalstonReilly2003\">{{cite book|author1=James H. Wilkinson |author2=Anthony Ralston(ed)|author3=Edwin D. Reilly(ed)|author4=David Hemmendinger(ed)|title=\"Error Analysis\" in Encyclopedia of Computer Science. pp. 669–674|url=https://books.google.com/books?id=OLRwQgAACAAJ|accessdate=14 May 2013|date=8 September 2003|publisher=Wiley|isbn=978-0-470-86412-8}}</ref> The basic approach is to show that although the calculated result, due to roundoff errors, will not be exactly correct, it is the exact solution to a nearby problem with slightly perturbed input data. If the perturbation required is small, on the order of the uncertainty in the input data, then the results are in some sense as accurate as the data \"deserves\". The algorithm is then defined as ''[[numerical stability#Forward, backward, and mixed stability|backward stable]]''. Stability is a measure of the sensitivity to rounding errors of a given numerical procedure;  by contrast, the [[condition number]] of a function for a given problem indicates the inherent sensitivity of the function to small perturbations in its input and is independent of the implementation used to solve the problem.<ref name=\"Einarsson2005\">{{cite book|author=Bo Einarsson|title=Accuracy and reliability in scientific computing|url=https://books.google.com/books?id=sh4orx_qB_QC&pg=PA50|accessdate=14 May 2013|year=2005|publisher=SIAM|isbn=978-0-89871-815-7|pages=50–}}</ref>\n\n==Applications==\n\n===Global positioning system===\n{{main|Global positioning system}}\nThe '''analysis of errors computed using the [[global positioning system]]''' is important for understanding how GPS works, and for knowing what magnitude errors should be expected. The Global Positioning System makes corrections for receiver clock errors and other effects but there are still residual errors which are not corrected. The Global Positioning System (GPS) was created by the United States Department of Defense (DOD) in the 1970s. It has come to be widely used for navigation both by the U.S. military and the general public.\n\n===Molecular dynamics simulation ===\n\nIn [[molecular dynamics]] (MD) simulations, there are errors due to inadequate sampling of the phase space or infrequently occurring events, these lead to the statistical error due to random fluctuation in the measurements.\n\nFor a series of ''M'' measurements of a fluctuating property ''A'', the mean value is:\n\n:<math> \\langle A \\rangle = \\frac{1}{M} \\sum_{\\mu=1}^M A_{\\mu}. </math>\n\nWhen these ''M'' measurements are independent, the variance of the mean <''A''> is:\n\n:<math> \\sigma^{2}( \\langle A \\rangle ) = \\frac{1}{M} \\sigma^{2}( A  ), </math>\n\nbut in most MD simulations, there is correlation between quantity ''A'' at different time, so the variance of the mean <''A''> will be underestimated as the effective number of independent measurements is actually less than ''M''. In such situations we rewrite the variance as :\n\n:<math> \\sigma^{2}( \\langle A \\rangle ) = \\frac{1}{M} \\sigma^{2}A \\left[ 1 + 2 \\sum_\\mu \\left( 1 - \\frac{\\mu}{M} \\right) \\phi_{\\mu} \\right],</math>\n\nwhere <math>\\phi_{\\mu}</math> is the [[autocorrelation function]] defined by\n\n:<math> \\phi_{\\mu} = \\frac{ \\langle A_{\\mu}A_{0} \\rangle - \\langle A \\rangle^{2} }{ \\langle A^{2} \\rangle - \\langle A \\rangle^{2}}.</math>\n\nWe can then use the auto correlation function to estimate the [[error bar]]. Luckily, we have a much simpler method based on [[block averaging]].<ref>D. C. Rapaport, ''The Art of Molecular Dynamics Simulation'', Cambridge University Press.</ref>\n\n===Scientific data verification===\n{{main|Scientific misconduct#Exposure of fraudulent data}}\nMeasurements generally have a small amount of error, and repeated measurements of the same item will generally result in slight differences in readings. These differences can be analyzed, and follow certain known mathematical and statistical properties. Should a set of data appear to be too faithful to the hypothesis, i.e., the amount of error that would normally be in such measurements does not appear, a conclusion can be drawn that the data may have been forged. Error analysis alone is typically not sufficient to prove that data have been falsified or fabricated, but it may provide the supporting evidence necessary to confirm suspicions of misconduct.\n\n== See also ==\n* [[Error analysis (linguistics)]]\n* [[Error bar]]\n* [[Errors and residuals in statistics]]\n* [[Propagation of uncertainty]]\n* [[Validated numerics]]\n\n== References ==\n<references />\n\n==External links==\n* [http://teacher.pas.rochester.edu/PHY_LABS/AppendixB/AppendixB.html] All about error analysis.\n\n[[Category:Numerical analysis]]\n[[Category:Error]]"
    },
    {
      "title": "Estrin's scheme",
      "url": "https://en.wikipedia.org/wiki/Estrin%27s_scheme",
      "text": "In [[numerical analysis]], '''Estrin's scheme''' (after [[Gerald Estrin]]), also known as '''Estrin's method''', is an [[algorithm]] for numerical evaluation of [[polynomial]]s.\n\n[[Horner's method]] for evaluation of polynomials is one of the most commonly used algorithms for this purpose, and unlike Estrin's scheme it is optimal in the sense that it minimizes the number of multiplications and addition required to evaluate an arbitrary polynomial. On a modern processor, instructions that do not depend on each other's results may run in parallel. Horner's method contains a series of multiplications and additions that each depend on the previous instruction and so cannot execute in parallel. Estrin's scheme is one method that attempts to overcome this serialization while still being reasonably close to optimal.\n\n== Description of the algorithm ==\nEstrin's scheme operates [[Recursion (computer science)|recursively]], converting a degree-''n'' polynomial in ''x'' (for ''n''≥2) to a degree-{{floor|''n''/2}} polynomial in ''x''<sup>2</sup> using {{ceil|''n''/2}} independent operations (plus one to compute ''x''<sup>2</sup>).\n\nGiven an arbitrary polynomial ''P''(''x'') = ''C''<sub>0</sub> + ''C''<sub>1</sub>''x'' + ''C''<sub>2</sub>''x''<sup>2</sup> + ''C''<sub>3</sub>''x''<sup>3</sup> + ⋯ + ''C<sub>n</sub>x<sup>n</sup>'', one can group adjacent terms into sub-expressions of the form (''A''&nbsp;+&nbsp;''Bx'') and rewrite it as a polynomial in ''x''<sup>2</sup>: ''P''(''x'') = (''C''<sub>0</sub> + ''C''<sub>1</sub>''x'') + (''C''<sub>2</sub> + ''C''<sub>3</sub>''x'')''x''<sup>2</sup> + (''C''<sub>4</sub> + ''C''<sub>5</sub>''x'')''x''<sup>4</sup> + ⋯ = ''Q''(''x''<sup>2</sup>).\n\nEach of these sub-expressions, and ''x''<sup>2</sup>, may be computed in parallel.  They may also be evaluated using a native [[multiply–accumulate]] instruction on some architectures, an advantage that is shared with Horner's method.\n\nThis grouping can then be repeated to get a polynomial in ''x''<sup>4</sup>: ''P''(''x'') = ''Q''(''x''<sup>2</sup>) = ((''C''<sub>0</sub> + ''C''<sub>1</sub>''x'') + (''C''<sub>2</sub> + ''C''<sub>3</sub>''x'')''x''<sup>2</sup>) + ((''C''<sub>4</sub> + ''C''<sub>5</sub>''x'')  + (''C''<sub>6</sub> + ''C''<sub>7</sub>''x'')''x''<sup>2</sup>)''x''<sup>4</sup> + ⋯ = ''R''(''x''<sup>4</sup>).\n\nRepeating this {{floor|log<sub>2</sub>''n''}}+1 times, one arrives at Estrin's scheme for parallel evaluation of a polynomial:\n\n# Compute ''D<sub>i</sub>'' = ''C''<sub>2''i''</sub> + ''C''<sub>2''i''+1</sub>''x'' for all 0&nbsp;≤&nbsp;''i''&nbsp;≤&nbsp;{{floor|''n''/2}}.  (If ''n'' is even, then ''C''<sub>''n''+1</sub>&nbsp;=&nbsp;0 and ''D''<sub>''n''/2</sub>&nbsp;=&nbsp;''C<sub>n</sub>''.)\n# If ''n''&nbsp;≤&nbsp;1, the computation is complete and ''D''<sub>0</sub> is the final answer.\n# Otherwise, compute ''y'' = ''x''<sup>2</sup> (in parallel with the computation of ''D<sub>i</sub>'').\n# Evaluate ''Q''(''y'') = ''D''<sub>0</sub> + ''D''<sub>1</sub>''y'' + ''D''<sub>2</sub>''y''<sup>2</sup>  + ⋯ + ''D''<sub>{{floor|''n''/2}}</sub>''y''<sup>{{floor|''n''/2}}</sup> using Estrin's scheme.\n\nThis performs a total of ''n'' multiply-accumulate operations (the same as Horner's method) in line 1, and an additional {{floor|log<sub>2</sub>''n''}} squarings in line 3.  In exchange for those extra squarings, all of the operations in each level of the scheme are independent and may be computed in parallel; the longest dependency path is {{floor|log<sub>2</sub>''n''}}+1 operations long.\n\n== Examples ==\nTake ''P<sub>n</sub>''(''x'') to mean the nth order polynomial of the form: ''P<sub>n</sub>''(''x'') =&nbsp;''C''<sub>0</sub> + ''C''<sub>1</sub>''x'' + ''C''<sub>2</sub>''x''<sup>2</sup> + ''C''<sub>3</sub>''x''<sup>3</sup> + ⋯ + ''C<sub>n</sub>x<sup>n</sup>''\n\nWritten with Estrin's scheme we have:\n\n: ''P''<sub>3</sub>(''x'') = (''C''<sub>0</sub> + ''C''<sub>1</sub>''x'') + (''C''<sub>2</sub> + ''C''<sub>3</sub>''x'') ''x''<sup>2</sup>\n: ''P''<sub>4</sub>(''x'') = (''C''<sub>0</sub> + ''C''<sub>1</sub>''x'') + (''C''<sub>2</sub> + ''C''<sub>3</sub>''x'') ''x''<sup>2</sup> + ''C''<sub>4</sub>''x''<sup>4</sup>\n: ''P''<sub>5</sub>(''x'') = (''C''<sub>0</sub> + ''C''<sub>1</sub>''x'') + (''C''<sub>2</sub> + ''C''<sub>3</sub>''x'') ''x''<sup>2</sup> + (''C''<sub>4</sub> + ''C''<sub>5</sub>''x'') ''x''<sup>4</sup>\n: ''P''<sub>6</sub>(''x'') = (''C''<sub>0</sub> + ''C''<sub>1</sub>''x'') + (''C''<sub>2</sub> + ''C''<sub>3</sub>''x'') ''x''<sup>2</sup> + ((''C''<sub>4</sub> + ''C''<sub>5</sub>''x'') + ''C''<sub>6</sub>''x''<sup>2</sup>)''x''<sup>4</sup>\n: ''P''<sub>7</sub>(''x'') = (''C''<sub>0</sub> + ''C''<sub>1</sub>''x'') + (''C''<sub>2</sub> + ''C''<sub>3</sub>''x'') ''x''<sup>2</sup> + ((''C''<sub>4</sub> + ''C''<sub>5</sub>''x'') + (''C''<sub>6</sub> + ''C''<sub>7</sub>''x'') ''x''<sup>2</sup>)''x''<sup>4</sup>\n: ''P''<sub>8</sub>(''x'') = (''C''<sub>0</sub> + ''C''<sub>1</sub>''x'') + (''C''<sub>2</sub> + ''C''<sub>3</sub>''x'') ''x''<sup>2</sup> + ((''C''<sub>4</sub> + ''C''<sub>5</sub>''x'') + (''C''<sub>6</sub> + ''C''<sub>7</sub>''x'') ''x''<sup>2</sup>)''x''<sup>4</sup> + ''C''<sub>8</sub>''x''<sup>8</sup>\n: ''P''<sub>9</sub>(''x'') = (''C''<sub>0</sub> + ''C''<sub>1</sub>''x'') + (''C''<sub>2</sub> + ''C''<sub>3</sub>''x'') ''x''<sup>2</sup> + ((''C''<sub>4</sub> + ''C''<sub>5</sub>''x'') + (''C''<sub>6</sub> + ''C''<sub>7</sub>''x'') ''x''<sup>2</sup>)''x''<sup>4</sup> + (''C''<sub>8</sub> + ''C''<sub>9</sub>''x'') ''x''<sup>8</sup>\n: …\n\nIn full detail, consider the evaluation of ''P''<sub>15</sub>(''x''):\n: '''Inputs:''' ''x'', ''C''<sub>0</sub>, ''C''<sub>1</sub>, ''C''<sub>2</sub>, ''C''<sub>3</sub>, ''C''<sub>4</sub>, ''C''<sub>5</sub> ''C''<sub>6</sub>, ''C''<sub>7</sub>, ''C''<sub>8</sub>, ''C''<sub>9</sub> ''C''<sub>10</sub>, ''C''<sub>11</sub>, ''C''<sub>12</sub>, ''C''<sub>13</sub> ''C''<sub>14</sub>, ''C''<sub>15</sub>\n: '''Step 1:''' ''x''<sup>2</sup>, ''C''<sub>0</sub>+''C''<sub>1</sub>''x'', ''C''<sub>2</sub>+''C''<sub>3</sub>''x'', ''C''<sub>4</sub>+''C''<sub>5</sub>''x'', ''C''<sub>6</sub>+''C''<sub>7</sub>''x'', ''C''<sub>8</sub>+''C''<sub>9</sub>''x'', ''C''<sub>10</sub>+''C''<sub>11</sub>''x'', ''C''<sub>12</sub>+''C''<sub>13</sub>''x'', ''C''<sub>14</sub>+''C''<sub>15</sub>''x''\n: '''Step 2:''' ''x''<sup>4</sup>, (''C''<sub>0</sub>+''C''<sub>1</sub>''x'') + (''C''<sub>2</sub>+''C''<sub>3</sub>''x'')''x''<sup>2</sup>, (''C''<sub>4</sub>+''C''<sub>5</sub>''x'') + (''C''<sub>6</sub>+''C''<sub>7</sub>''x'')''x''<sup>2</sup>, (''C''<sub>8</sub>+''C''<sub>9</sub>''x'') + (''C''<sub>10</sub>+''C''<sub>11</sub>''x'')''x''<sup>2</sup>, (''C''<sub>12</sub>+''C''<sub>13</sub>''x'') + (''C''<sub>14</sub>+''C''<sub>15</sub>''x'')''x''<sup>2</sup>\n: '''Step 3:''' ''x''<sup>8</sup>, ((''C''<sub>0</sub>+''C''<sub>1</sub>''x'') + (''C''<sub>2</sub>+''C''<sub>3</sub>''x'')''x''<sup>2</sup>) + ((''C''<sub>4</sub>+''C''<sub>5</sub>''x'') + (''C''<sub>6</sub>+''C''<sub>7</sub>''x'')''x''<sup>2</sup>)''x''<sup>4</sup>, ((''C''<sub>8</sub>+''C''<sub>9</sub>''x'') + (''C''<sub>10</sub>+''C''<sub>11</sub>''x'')''x''<sup>2</sup>) + ((''C''<sub>12</sub>+''C''<sub>13</sub>''x'') + (''C''<sub>14</sub>+''C''<sub>15</sub>''x'')''x''<sup>2</sup>)''x''<sup>4</sup>\n: '''Step 4:''' (((''C''<sub>0</sub>+''C''<sub>1</sub>''x'') + (''C''<sub>2</sub>+''C''<sub>3</sub>''x'')''x''<sup>2</sup>) + ((''C''<sub>4</sub>+''C''<sub>5</sub>''x'') + (''C''<sub>6</sub>+''C''<sub>7</sub>''x'')''x''<sup>2</sup>)''x''<sup>4</sup>) + (((''C''<sub>8</sub>+''C''<sub>9</sub>''x'') + (''C''<sub>10</sub>+''C''<sub>11</sub>''x'')''x''<sup>2</sup>) + ((''C''<sub>12</sub>+''C''<sub>13</sub>''x'') + (''C''<sub>14</sub>+''C''<sub>15</sub>''x'')''x''<sup>2</sup>)''x''<sup>4</sup>)''x''<sup>8</sup>\n\n== References ==\n* {{Cite journal |first=Gerald |last=Estrin |authorlink=Gerald Estrin |title=Organization of computer systems—The fixed plus variable structure computer |journal=Proc. Western Joint Comput. Conf. |date=May 1960 |location=San Francisco |pages=33–40 |doi=10.1145/1460361.1460365 |url=https://www.computer.org/csdl/proceedings/afips/1960/5056/00/50560033.pdf}}\n* {{cite book |first=Jean-Michel |last=Muller |title=Elementary Functions: Algorithms and Implementation |edition=2nd |publisher=Birkhäuser |page=58 |year=2005 |isbn=0-8176-4372-9}}\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Explicit algebraic stress model",
      "url": "https://en.wikipedia.org/wiki/Explicit_algebraic_stress_model",
      "text": "{{No footnotes|date=March 2017}}\n\nThe '''algebraic stress model''' arises in [[computational fluid dynamics]]. Two main approaches can be undertaken. In the first, the transport of the turbulent stresses is assumed proportional to the turbulent kinetic energy; while in the second, convective and diffusive effects are assumed to be negligible. Algebraic stress models can only be used where [[convective]] and [[diffusive]] fluxes are negligible, i.e. source dominated flows. In order to simplify the existing EASM and to achieve an efficient numerical implementation the underlying tensor basis plays an important role. The five-term tensor basis that is introduced here tries to combine an optimum of accuracy of the complete basis with the advantages of a pure 2d concept. Therefore a suitable five-term basis is identified. Based on that the new model is designed and validated in combination with different eddy-[[viscosity]] type background models.\n\n[[File:17357 cfd-analysis3.jpg|thumb|right|CFD ANALYSIS]]\n\n== Integrity basis==\nIn the frame work of single-point closures (Reynolds-stress transport models = RSTM) still provide the best representation of flow physics. Due to numeric requirements an explicit formulation based on a low number of [[tensors]] is desirable and was already introduced originally most explicit algebraic stress models are formulated using a 10-term basis:\n:<math>b_{ij}= \\sum_{\\lambda=1}^{10} G^{(\\lambda)} T_{ij}^{(\\lambda)}</math>\nThe reduction of the tensor basis however requires an enormous mathematical effort, to transform the algebraic stress formulation for a given linear algebraic RSTM into a given tensor basis by keeping all important properties of the underlying model. This transformation can be applied to an arbitrary tensor basis. In the present investigations an optimum set of basis tensors and the corresponding coefficients is to be found.\n\n==Projection method==\nThe projection method was introduced to enable an approximate solution of the algebraic transport equation of the Reynolds-stresses. In contrast to the approach of the tensor basis is not inserted in the algebraic equation, instead the [[algebraic equation]] is projected. Therefore, the chosen basis tensors does not need to form a complete integrity basis. However, the projection will fail if the basis tensor are [[linear]] dependent. In the case of a complete basis the projection leads to the same solution as the direct insertion, otherwise an approximate solution in the sense is obtained.\n\n==An example==\nIn order to prove, that the projection method will lead to the same solution as the direct insertion, the EASM for two-dimensional flows is derived. In two-dimensional flows only the tensors are independent.\n:<math> T_{ij}^{(1)}=s_{ij} </math>\n:<math> T_{ij}^{(2)}=s_{ik}w_{kj} - w_{ik}s{kj}</math>\n:<math> T_{ij}^{(3)}=s_{ik}s_{kj}-s_{mk}s_{km}\\frac{1}{3} \\delta_{ij} </math>\nThe projection leads then to the same coefficients. This two-dimensional EASM is used as starting point for an optimized EASM which includes three-dimensional effects. For example the shear stress variation in a rotating pipe cannot be predicted with quadratic tensors. Hence, the EASM was extended with a cubic tensor. In order to do not affect the performance in 2D flows, a tensor was chosen that vanish in 2d flows. This offers the concentration of the coefficient determination in 3d flows. A cubic [[tensor]], which vanishes in 3d flow is: \n:<math> T_{ij}^{(5)}=w_{ik}s_{kl}s_{lj}-s_{ik}s_{kl}w_{lj}</math>\nThe projection with tensors T<sup>(1)</sup>, T<sup>(2)</sup>, T<sup>(3)</sup> and T<sup>(5)</sup> yields then the coefficients of the EASM.\n\n==Limitation of '''C'''<sub>''μ''</sub>==\nA direct result of the EASM derivation is a variable formulation of '''C'''<sub>μ</sub>.As the generators of the extended EASM where chosen to preserve the existing 2D formulation the expression of '''C'''<sub>''μ''</sub> remains unchanged:\n\n:<math> C \\mu = \\frac{-A_1 g}{g^2-\\frac{2}{3} A_3^2 \\eta _1 -2 A^2 \\eta _2} </math>\nA<sub>i</sub> are the constants of the underlying pressure-strain model.\nSince η<sub>1</sub> is always positive it might be possible that '''C'''<sub>μ</sub> becomes singular. Therefore in the first EASM derivation of a regularization was introduced, which prevent a singular by cutting the range of η<sub>1</sub>. However, Wallin et al. pointed out that the regularization deteriorated the performance of the EASM. In their model the methodology was refined to account for the coefficient.\n[[File:EASM.JPG|frameless|right|Velocity Profile]]\n\n:<math> g=C_1 - 2 b_{ij} </math>\n\nThis leads to a weak [[non-linear]] conditional equation for the EASM coefficients and an additional equation for g must be solved. In 3D the equation of g is of 6th order, wherefore a closed solution is only possible in 2D flows, where the equation reduces to 3rd order. In order to circumvent the [[root]] finding of a [[polynomial equation]] quasi self-consistent approach. He showed that by using a '''C'''<sub>μ</sub>  expression of a realizable linear model instead of the EASM-'''C'''<sub>μ</sub> expression in the equation of g the same properties of g follows. For a wide range of and the quasi self-consistent approach is almost identical to the fully self-consistent solution. Thus the quality of the EASM is not affected with the advantage of no additional non-linear equation. Since in the [[projection (linear algebra)|projection]]s to determine the EASM coefficients the complexity is reduced by neglecting higher order invariants.\n\n==References==\n# Gatski, T.B. and Speziale, C.G., \"On explicit algebraic stress models for complex turbulent flows\". J. Fluid Mech.\n# Rung, T., \"Entwicklung anisotroper Wirbelzähigkeitsbeziehungen mit Hilfe von Projektionstechniken\", PHD-thesis, Technical University Berlin, 2000 \n# Taulbee, D.B., \"An improved algebraic Reynolds stress model and corresponding nonlinaer stress model\", Phys. Fluids, 28, pp 2555–2561, 1992 \n# Lübcke, H., Rung, T. and Thiele, F. \"Prediction of the Spreading Mechanism of 3D Turbulent Wall Jets with Explicit Reynolds-Stress Closures\", Eng. Turbulence Modelling and Experiments 5, Mallorca, 2002 \n# Wallin, S. and Johansson, A.V., \"A new explicit algebraic Reynolds stress turbulence model including an improved near-wall treatment\", Flow Modelling and Turbulence Measurements IV \n# Taulbee, D.B., \"An improved algebraic Reynolds stress model and corresponding nonlinear stress model\" \n# Jongen, T. and Gatski, T.B., \"General explicit algebraic stress relations and best approximations for three-dimensional flows\", Int. J. Engineering Science\n\n[[Category:Fluid mechanics]]\n[[Category:Computational fluid dynamics]]\n[[Category:Numerical analysis]]\n[[Category:Applied mathematics]]\n[[Category:Functional analysis]]"
    },
    {
      "title": "Factor combining",
      "url": "https://en.wikipedia.org/wiki/Factor_combining",
      "text": "#redirect [[CORDIC#Factor combining]] {{R to related topic}}\n\n[[Category:Numerical analysis]]\n[[Category:Digit-by-digit algorithms]]\n[[Category:Shift-and-add algorithms]]"
    },
    {
      "title": "False precision",
      "url": "https://en.wikipedia.org/wiki/False_precision",
      "text": "{{refimprove|date=October 2015}}\n{{Order-of-approx}}\n'''False precision''' (also called '''overprecision''', '''fake precision''', '''misplaced precision''' and '''spurious precision''') occurs when numerical data are presented in a manner that implies better [[Accuracy and precision|precision]] than is justified; since precision is a limit to [[Accuracy and precision|accuracy]] (in the ISO definition of accuracy), this often leads to overconfidence in the accuracy, named [[precision bias]].<ref name=ff>{{cite web|url=http://www.fallacyfiles.org/fakeprec.html|title=Overprecision|publisher=Fallacy files}}</ref>\n\n==Overview==\n[[Madsen Pirie]] defines the term \"false precision\" in a more general way: when exact numbers are used for notions that cannot be expressed in exact terms. For example, \"We know that 90% of the difficulty in writing is getting started.\" Often false precision is abused to produce an unwarranted confidence in the claim: \"our mouthwash is twice as good as our competitor's\". <ref>*{{cite book|last1=Pirie|first1=Madsen|title=How to Win Every Argument: The Use and Abuse of Logic|date=2015|publisher=Bloomsbury Publishing|pages=78-80|url=https://books.google.com/books?id=YMPuBQAAQBAJ&pg=PT78#v=onepage&q&f=false|accessdate=22 October 2015}}</ref>\n\nIn [[science]] and [[engineering]], convention dictates that unless a [[margin of error]] is explicitly stated, the number of [[significant figures]] used in the presentation of data should be limited to what is warranted by the precision of those data. For example, if an instrument can be read to tenths of a unit of measurement, results of calculations using data obtained from that instrument can only be confidently stated to the tenths place, regardless of what the raw calculation returns or whether other data used in the calculation are more accurate. Even outside these disciplines, there is a tendency to assume that all the non-zero digits of a number are meaningful; thus, providing excessive figures may lead the viewer to expect better precision than exists.\n\nHowever, in contrast, it is good practice to retain more significant figures than this in the intermediate stages of a calculation, in order to avoid accumulated [[rounding error]]s.\n\nFalse precision commonly arises when high-precision and low-precision data are combined, and in [[conversion of units]].\n\n== Examples ==\nFalse precision is the gist of numerous variations of a joke which can be summarized as follows: A tour guide at a museum says a dinosaur skeleton is 100,000,005 years old, because an expert told him that it was 100 million years old when he started working there 5 years ago.\n\nIf a car's speedometer indicates the vehicle is travelling at 60&nbsp;mph and that is converted to km/h, it would equal 96.5606&nbsp;km/h. The conversion from the whole number in one system to the precise result in another makes it seem like the measurement was very precise, when in fact it was not.\n\nMeasures that rely on [[statistical sampling]], such as [[IQ tests]], are often reported with false precision.<ref name=lie>{{cite book|last1=Huff|first1=Darrell|title=How to Lie with Statistics|publisher=W. W. Norton & Company|isbn=9780393070873|pages=144|edition=2010|url=https://books.google.com/books?id=5oSU5PepogEC&pg=PA55#v=onepage&q&f=false|accessdate=22 October 2015}} Chapter 4. Much Ado about Practically Nothing</ref>\n\n==See also==\n*[[Detection limit|Limit of detection]]\n*[[Propagation of uncertainty]]\n*[[Rounding]]\n*[[Round-off error]]\n*[[Precision bias]]\n*[[Significant figures]]\n\n==References==\n{{Reflist}}\n\n==External links==\n*{{Cite web | last=Wong | first=Lena | title=Temperature of a Healthy Human (Body Temperature) | work=The Physics Factbook | year=1997 | url=https://hypertextbook.com/facts/1997/LenaWong.shtml}}\n*[https://www.nytimes.com/2006/08/27/opinion/27pubed.html?ex=1314331200&en=589ab67d3ab298b7&ei=5090&partner=rssuserland&emc=rss Precisely False vs. Approximately Right: A Reader's Guide To Polls]\n\n{{Informal Fallacy}}\n\n{{DEFAULTSORT:False Precision}}\n[[Category:Arithmetic]]\n[[Category:Numerical analysis]]\n\n[[de:Signifikante_Stellen]]"
    },
    {
      "title": "Fast multipole method",
      "url": "https://en.wikipedia.org/wiki/Fast_multipole_method",
      "text": "__NOTOC__\nThe '''fast multipole method''' ('''FMM''') is a [[numerical analysis|numerical]] technique that was developed to speed up the calculation of long-ranged forces in the [[n-body problem|''n''-body problem]]. It does this by expanding the system [[Green's function (many-body theory)|Green's function]] using a [[multipole expansion]], which allows one to group sources that lie close together and treat them as if they are a single source.<ref>Rokhlin, Vladimir (1985). \"Rapid Solution of Integral Equations of Classic Potential Theory.\" J. Computational Physics Vol. 60, pp. 187–207.</ref>\n\nThe FMM has also been applied in accelerating the [[iterative solver]] in the [[boundary element method|method of moments]] (MOM) as applied to [[computational electromagnetics]] problems.<ref>[[Nader Engheta]], William D. Murphy, [[Vladimir Rokhlin (American scientist)|Vladimir Rokhlin]], and [[Marius Vassiliou]] (1992), “The Fast Multipole Method for Electromagnetic Scattering Computation,” IEEE Transactions on Antennas and Propagation 40, 634–641.</ref> The FMM was first introduced in this manner by [[Leslie Greengard|Greengard]] and [[Vladimir Rokhlin (American scientist)|Rokhlin]]<ref>{{cite web |url=http://www-theor.ch.cam.ac.uk/people/ross/thesis/node97.html |title=Archived copy |accessdate=2010-12-10 |deadurl=yes |archiveurl=https://web.archive.org/web/20110603231158/http://www-theor.ch.cam.ac.uk/people/ross/thesis/node97.html |archivedate=2011-06-03 |df= }} <!--link redirects to the members list--></ref> and is based on the [[multipole expansion]] of the vector [[Helmholtz equation]]. By treating the interactions between far-away basis functions using the FMM, the corresponding matrix elements do not need to be explicitly stored, resulting in a significant reduction in required memory. If the FMM is then applied in a hierarchical manner, it can improve the complexity of matrix-vector products in an iterative solver from <math>\\mathcal{O}(N^2)</math> to <math>\\mathcal{O}(N)</math> in finite arithmetic, i.e., given a tolerance <math>\\varepsilon</math>, the matrix-vector product is guaranteed to be within a tolerance <math>\\varepsilon.</math> The dependence of the complexity on the tolerance <math>\\varepsilon</math> is <math>\\mathcal{O}(\\log(1/\\varepsilon))</math>, i.e., the complexity of FMM is <math>\\mathcal{O}(\\log(1/\\varepsilon)N)</math>. This has expanded the area of applicability of the MOM to far greater problems than were previously possible.\n\nThe FMM, introduced by Rokhlin and Greengard, has been said to be one of the top ten [[algorithm]]s of the 20th century.<ref>{{cite journal |author-first=Barry Arthur |author-last=Cipra |author-link=Barry Arthur Cipra |date=May 16, 2000 |title=The Best of the 20th Century: Editors Name Top 10 Algorithms |journal=SIAM News |volume=33 |issue=4 |pages=2 |publisher=[[Society for Industrial and Applied Mathematics]] |url=https://archive.siam.org/news/news.php?id=637 |accessdate=February 27, 2019 }}</ref> The FMM algorithm reduces the complexity of matrix-vector multiplication involving a certain type of dense matrix which can arise out of many physical systems.\n\nThe FMM has also been applied for efficiently treating the Coulomb interaction in the [[Hartree–Fock method]] and [[density functional theory]] calculations in [[quantum chemistry]].\n\n==See also==\n* [[Barnes–Hut simulation]]\n* [[Multipole expansion]]\n* [[n-body simulation|''n''-body simulation]]\n\n==References==\n{{Reflist}}\n\n==External links==\n*Gibson, Walton C. ''The Method of Moments in Electromagnetics''. Chapman & Hall/CRC, 2008. {{ISBN|978-1-4200-6145-1}} \n* [http://www.altairhyperworks.com/product/FEKO FEKO] from Altair HyperWorks includes the Multilevel FMM as solution option.\n* [http://www.radarsoftware.com Serenity] A high-fidelity Radar Cross Section (RCS) code that uses the Moment Method and the FMM.\n* [http://portal.acm.org/citation.cfm?id=36901 Abstract of Greengard and Rokhlin's original paper]\n* [http://math.nyu.edu/faculty/greengar/shortcourse_fmm.pdf A short course on fast multipole methods] by Rick Beatson and Leslie Greengard.\n* [http://www.umiacs.umd.edu/~ramani/cmsc878R/fmmdemo2.jar JAVA Animation of the Fast Multipole Method] Nice animation of the Fast Multipole Method with different adaptations.\n\n===Free software===\n* [http://sourceforge.net/projects/puma-em/ Puma-EM] A high performance, parallelized, open source Method of Moments / Multilevel Fast Multipole Method electromagnetics code.\n* [http://www.harperlangston.com/kifmm3d/documentation/index.html KIFMM3d] The Kernel-Independent Fast Multipole 3d Method (kifmm3d) is a new FMM implementation which does not require the explicit multipole expansions of the underlying kernel, and it is based on kernel evaluations.\n* [http://www.yijunliu.com/Software FastBEM] Free fast multipole boundary element programs for solving 2D/3D potential, elasticity, stokes flow and acoustic problems.\n* [http://www.fastfieldsolvers.com FastFieldSolvers] maintains the distribution of the tools, called FastHenry and FastCap, developed at M.I.T. for the solution of Maxwell equations and extraction of circuit parasites (inductance and capacitance) using the FMM.\n* [https://github.com/exafmm/exafmm ExaFMM] ExaFMM is a CPU/GPU capable 3D FMM code for Laplace/Helmholtz kernels that focuses on parallel scalability.\n* [http://scalfmm-public.gforge.inria.fr/doc/ ScalFMM] ScalFMM is a C++ software library developed at [[Inria]] Bordeaux with high emphasis on genericity and parallelization (using [[OpenMP]]/[[Message Passing Interface|MPI]]).\n* [https://jacksondebuhr.github.io/dashmm/ DASHMM] DASHMM is a C++ Software library developed at Indiana University using Asynchronous Multi-Tasking HPX-5 runtime system. It provides a unified execution on shared and distributed memory computers and provides 3D Laplace, Yukawa, and Helmholtz kernels. \n* [https://zhang416.github.io/recfmm/ RECFMM] Adaptive FMM with dynamic parallelism on multicores. \n{{Portal bar|Mathematics|Physics|Astronomy}}\n\n[[Category:Numerical analysis]]\n[[Category:Numerical differential equations]]\n[[Category:Computational science]]"
    },
    {
      "title": "Finite difference",
      "url": "https://en.wikipedia.org/wiki/Finite_difference",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|Discrete analog of a derivative}}\n{{Use mdy dates|date=September 2011}}\n{{About|discretization in calculus|combinatorical exterior calculus|Discrete exterior calculus}}\nA '''finite difference''' is a mathematical expression of the form {{math|''f''&thinsp;(''x'' + ''b'') − ''f''&thinsp;(''x'' + ''a'')}}. If a finite difference is divided by {{math|''b'' − ''a''}}, one gets a [[difference quotient]]. The approximation of [[derivative]]s by finite differences plays a central role in [[finite difference method]]s for the [[numerical analysis|numerical]] solution of [[differential equation]]s, especially [[boundary value problem]]s.\n\n[[Recurrence relation#Relationship to difference equations narrowly defined|Certain recurrence relations]] can be written as '''difference equations''' by replacing iteration notation with finite differences.\n\nToday, the term \"finite difference\" is often taken as synonymous with [[#finite difference approximation|finite difference approximation]]s of derivatives, especially in the context of [[numerical methods]].<ref name=\"WilmottHowison1995\">{{cite book|author1=Paul Wilmott|author2=Sam Howison|author3=Jeff Dewynne|title=The Mathematics of Financial Derivatives: A Student Introduction|year=1995|publisher=Cambridge University Press|isbn=978-0-521-49789-3|page=137}}</ref><ref name=\"Olver2013\">{{cite book|author=Peter Olver|author-link=Peter J. Olver|title=Introduction to Partial Differential Equations|year=2013|publisher=Springer Science & Business Media|isbn=978-3-319-02099-0|page=182}}</ref><ref name=\"Chaudhry2007\">{{cite book|author=M Hanif Chaudhry|title=Open-Channel Flow|year=2007|publisher=Springer|isbn=978-0-387-68648-6|pages=369}}</ref> Finite difference approximations are finite difference quotients in the terminology employed above.\n\nFinite differences were introduced by [[Brook Taylor]] in 1715 and have also been studied as abstract self-standing mathematical objects in works by [[George Boole]] (1860), [[L. M. Milne-Thomson]] (1933), and [[:de:Károly Jordan|Károly Jordan]] (1939). Finite differences trace their origins back to one of [[Jost Bürgi]]'s algorithms ({{circa|1592}}) and work by others including [[Isaac Newton]]. The formal calculus of finite differences can be viewed as an alternative to the calculus of [[infinitesimal]]s.<ref>Jordán, op. cit., p. 1 and Milne-Thomson, p. xxi. \nMilne-Thomson,  Louis Melville (2000):  ''The Calculus of Finite Differences'' (Chelsea Pub Co, 2000)  {{ISBN|978-0821821077}}</ref>\n\n==Forward, backward, and central differences==\n\nThree forms are commonly considered: forward, backward, and central differences.<ref name=\"WilmottHowison1995\"/><ref name=\"Olver2013\"/><ref name=\"Chaudhry2007\"/>\n\nA '''forward difference''' is an expression of the form\n:<math> \\Delta_h[f](x) =  f(x + h) - f(x). </math>\n\nDepending on the application, the spacing {{mvar|h}} may be variable or constant.  When omitted, {{mvar|h}} is taken to be 1: {{math|Δ[&thinsp;''f''&thinsp;](''x'') {{=}} Δ<sub>1</sub>[&thinsp;''f''&thinsp;](''x'')}}.\n\nA '''backward difference''' uses the function values at {{mvar|x}} and {{math|''x'' − ''h''}}, instead of the values at {{math|''x'' + ''h''}} and&nbsp;{{mvar|x}}:\n:<math> \\nabla_h[f](x) =  f(x) - f(x-h). </math>\n\n{{anchor|central difference}}\nFinally, the '''central difference''' is given by\n:<math> \\delta_h[f](x) =  f\\left(x+\\tfrac12h\\right)-f\\left(x-\\tfrac12h\\right).</math>\n\n== Relation with derivatives ==\n[[File:Finite difference method.svg|thumb|right| 3 types of the finite difference method. Central gives the best approximation of the derivative]]\n{{anchor|finite difference approximation}}\nThe [[derivative]] of a function {{mvar|f}} at a point {{mvar|x}}  is defined by the [[limit of a function|limit]].\n\n:<math> f'(x) = \\lim_{h\\to0} \\frac{f(x+h) - f(x)}{h}. </math>\n\nIf {{mvar|h}} has a fixed (non-zero) value instead of approaching zero, then the right-hand side of the above equation would be written\n\n:<math> \\frac{f(x + h) - f(x)}{h} = \\frac{\\Delta_h[f](x)}{h}. </math>\n\nHence, the forward difference divided by {{mvar|h}} approximates the derivative when  {{mvar|h}} is small. The error in this approximation can be derived from [[Taylor's theorem]]. Assuming that {{mvar|f}} is differentiable, we have\n:<math> \\frac{\\Delta_h[f](x)}{h} - f'(x) = O(h)\\to 0 \\quad  \\text{as }h \\to 0. </math>\n\nThe same formula holds for the backward difference:\n:<math> \\frac{\\nabla_h[f](x)}{h} - f'(x) = O(h)\\to 0 \\quad  \\text{as }h \\to 0. </math>\n\nHowever, the central (also called centered) difference yields a more accurate approximation. If {{mvar|f}} is twice differentiable,\n:<math> \\frac{\\delta_h[f](x)}{h} - f'(x) =  O\\left(h^2\\right) . </math>\n\nThe main problem{{citation needed|date=December 2017}} with the central difference method, however, is that oscillating functions can yield zero derivative. If  {{math|''f''&thinsp;(''nh'') {{=}} 1}}  for {{mvar|n}}  odd, and {{math|''f''&thinsp;(''nh'') {{=}} 2}}  for {{mvar|n}}  even, then {{math|''f''&thinsp;′(''nh'') {{=}} 0}}  if it is calculated with the central difference scheme. This is particularly troublesome if the domain of {{mvar|f}}  is discrete.\n\nAuthors for whom finite differences mean finite difference approximations define the forward/backward/central differences as the quotients given in this section (instead of employing the definitions given in the previous section).<ref name=\"WilmottHowison1995\"/><ref name=\"Olver2013\"/><ref name=\"Chaudhry2007\"/>\n\n{{see also|Symmetric derivative}}\n\n==Higher-order differences==\n{{more citations needed|date=July 2018}}  <!-- this section is linked to further down in the article -->\n\nIn an analogous way, one can obtain finite difference approximations to higher order derivatives and differential operators. For example, by using  the above central difference formula for {{math|''f''&thinsp;′(''x'' + {{sfrac|''h''|2}})}}  and {{math|''f''&thinsp;′(''x'' − {{sfrac|''h''|2}})}}   and applying a central difference formula for the derivative of {{math|''f''&thinsp;′}} at {{mvar|x}}, we obtain the central difference approximation of the second derivative of {{mvar|f}}:\n\n;Second-order central\n:<math> f''(x) \\approx \\frac{\\delta_h^2[f](x)}{h^2} = \\frac{ \\frac{f(x+h) - f(x)}{h} - \\frac{f(x) - f(x-h)}{h} }{h} =  \\frac{f(x+h) - 2 f(x) + f(x-h)}{h^{2}} . </math>\n\nSimilarly we can apply other differencing formulas in a recursive manner.\n\n;Second order forward\n:<math> f''(x) \\approx \\frac{\\Delta_h^2[f](x)}{h^2} = \\frac{ \\frac{f(x+2h) - f(x+h)}{h} - \\frac{f(x+h) - f(x)}{h} }{h} =  \\frac{f(x+2h) - 2 f(x+h) + f(x)}{h^{2}} . </math>\n\n;Second order backward\n:<math> f''(x) \\approx \\frac{\\nabla_h^2[f](x)}{h^2} = \\frac{ \\frac{f(x) - f(x-h)}{h} - \\frac{f(x-h) - f(x-2h)}{h} }{h} = \\frac{f(x) - 2 f(x-h) + f(x - 2h)}{h^{2}} . </math>\n\nMore generally, the  '''{{mvar|n}}th order forward, backward, and central''' differences are given by, respectively,\n\n;Forward\n:<math>\\Delta^n_h[f](x) = \\sum_{i = 0}^{n} (-1)^i \\binom{n}{i} f\\bigl(x + (n - i) h\\bigr),</math>\n\nor for {{math|''h'' {{=}} 1}},\n:<math>\\Delta^n [f](x)= \\sum_{k=0}^n\\binom nk(-1)^{n-k}f(x + k)</math>\n\n;Backward\n:<math>\\nabla^n_h[f](x) = \\sum_{i = 0}^{n} (-1)^i \\binom{n}{i} f(x - ih),</math>\n\n;Central\n:<math>\\delta^n_h[f](x) = \\sum_{i = 0}^{n} (-1)^i \\binom{n}{i} f\\left(x + \\left(\\frac{n}{2} - i\\right) h\\right).</math>\n\nThese equations use [[binomial coefficient]]s after the summation sign shown as {{math|<big><big>(</big></big>{{su|p=''n''|b=''i''|a=c}}<big><big>)</big></big>}}.  Each row of [[Pascal's triangle]] provides the coefficient for each value of {{mvar|i}}.\n\nNote that the central difference will, for odd {{mvar|n}}, have {{mvar|h}}  multiplied by non-integers. This is often a problem because it amounts to changing the interval of discretization. The problem may be remedied taking the average of {{math|''δ<sup>n</sup>''[&thinsp;''f''&thinsp;](''x'' − {{sfrac|''h''|2}})}} and {{math|''δ<sup>n</sup>''[&thinsp;''f''&thinsp;](''x'' + {{sfrac|''h''|2}})}}.\n\nForward differences applied to a [[sequence]] are sometimes called the [[binomial transform]] of the sequence, and have a number of interesting combinatorial properties. Forward differences may be evaluated using the [[Nörlund&ndash;Rice integral]]. The integral representation for these types of series is interesting, because the integral can often be evaluated using [[asymptotic expansion]] or [[saddle-point]] techniques; by contrast, the forward difference series can be extremely hard to evaluate numerically, because the binomial coefficients grow rapidly for large {{mvar|n}}.\n\nThe relationship of these higher-order differences with the respective derivatives is  straightforward,\n:<math>\\frac{d^n f}{d x^n}(x) = \\frac{\\Delta_h^n[f](x)}{h^n}+O(h) = \\frac{\\nabla_h^n[f](x)}{h^n}+O(h) = \\frac{\\delta_h^n[f](x)}{h^n} + O\\left(h^2\\right).</math>\n\nHigher-order differences can also be used to construct better approximations. As mentioned above, the first-order difference approximates the first-order derivative up to a term of order {{mvar|h}}. However, the combination\n:<math> \\frac{\\Delta_h[f](x) - \\frac12 \\Delta_h^2[f](x)}{h} = - \\frac{f(x+2h)-4f(x+h)+3f(x)}{2h} </math>\napproximates {{math|''f''&thinsp;′(''x'')}} up to a term of order {{math|''h''<sup>2</sup>}}. This can be proven by expanding the above expression in [[Taylor series]], or by using the calculus of finite differences, explained below.\n\nIf necessary, the finite difference can be centered about any point by mixing forward, backward, and central differences.\n\n===Arbitrarily sized kernels===\n\nUsing linear algebra one can construct finite difference approximations which utilize an arbitrary number of points to the left and a (possibly different) number of points to the right of the evaluation point, for any order derivative. This involves solving a linear system such that the [[Taylor expansion]] of the sum of those points around the evaluation point best approximates the Taylor expansion of the desired derivative. Such formulas can be represented graphically on a hexagonal or diamond-shaped grid.<ref>{{cite journal|last1=Fraser|first1=Duncan C.|title=On the Graphic Delineation of Interpolation Formulæ|journal=Journal of the Institute of Actuaries|date=1 January 1909|volume=43|issue=2|pages=235–241|url=https://archive.org/stream/journal43instuoft#page/236/mode/2up|accessdate=17 April 2017}}</ref>\n\nThis is useful for differentiating a function on a grid, where, as one approaches the edge of the grid, one must sample fewer and fewer points on one side.\n\nThe details are outlined in these [http://commons.wikimedia.org/wiki/File:FDnotes.djvu notes].\n\nThe [http://web.media.mit.edu/~crtaylor/calculator.html Finite Difference Coefficients Calculator] constructs finite difference approximations for non-standard (and even non-integer) stencils given an arbitrary stencil and a desired derivative order.\n\n===Properties===\n* For all positive {{mvar|k}} and {{mvar|n}}\n:<math>\\Delta^n_{kh} (f, x) = \\sum\\limits_{i_1=0}^{k-1} \\sum\\limits_{i_2=0}^{k-1} \\cdots \\sum\\limits_{i_n=0}^{k-1} \\Delta^n_h \\left(f, x+i_1h+i_2h+\\cdots+i_nh\\right).</math>\n\n* [[Leibniz rule (generalized product rule)|Leibniz rule]]:\n:<math>\\Delta^n_h (fg, x) = \\sum\\limits_{k=0}^n \\binom{n}{k} \\Delta^k_h (f, x) \\Delta^{n-k}_h(g, x+kh).</math>\n\n==Finite difference methods==\n{{main article|Finite difference method}}\nAn important application of finite differences is in [[numerical analysis]], especially in [[numerical partial differential equations|numerical differential equations]], which aim at the numerical solution of [[ordinary differential equation|ordinary]] and [[partial differential equation]]s respectively. The idea is to replace the derivatives appearing in the differential equation by finite differences that approximate them. The resulting methods are called [[finite difference method]]s.\n\nCommon applications of the finite difference method are in computational science and engineering disciplines, such as [[thermal engineering]], [[fluid mechanics]], etc.\n\nAn open Python package of the finite difference method for arbitrary accuracy and order in any dimension on uniform and non-uniform grids is the [https://github.com/maroba/findiff Findiff project].\n\n==Newton's series==\nThe '''[[Newton polynomial|Newton series]]''' consists of the terms of the '''Newton forward difference equation''', named after [[Isaac Newton]];  in essence, it is  the '''Newton interpolation formula''', first published in his ''[[Philosophiæ Naturalis Principia Mathematica|Principia Mathematica]]'' in 1687,<ref>Newton, Isaac, (1687). [https://books.google.com/books?id=KaAIAAAAIAAJ&dq=sir%20isaac%20newton%20principia%20mathematica&as_brr=1&pg=PA466#v=onepage&q&f=false ''Principia'', Book III, Lemma V, Case 1]</ref> namely the discrete analog of the continuum Taylor expansion,\n\n{{Equation box 1\n|indent =:\n|equation = <math>f(x)=\\sum_{k=0}^\\infty\\frac{\\Delta^k [f](a)}{k!} \\,(x-a)_k\n= \\sum_{k=0}^\\infty \\binom{x-a}{k}\\, \\Delta^k [f](a)  ,\n</math> \n|cellpadding= 6\n|border\n|border colour = #0073CF\n|background colour=#F9FFF7}}\nwhich holds for any [[polynomial]] function {{mvar|f}} and for many (but not all)  [[analytic function]]s (It does not hold when {{mvar|f}} is [[exponential type]] <math>\\pi</math>. This is easily seen, as the sine function vanishes at integer multiples of <math>\\pi</math>; the corresponding Newton series is identically zero, as all finite differences are zero in this case. Yet clearly, the sine function is not zero.). Here, the expression\n:<math>\\binom{x}{k} = \\frac{(x)_k}{k!}</math>\n\nis the [[binomial coefficient]], and\n:<math>(x)_k=x(x-1)(x-2)\\cdots(x-k+1)</math>\n\nis the \"[[falling factorial]]\" or \"lower factorial\", while  the [[empty product]] {{math|(''x'')<sub>0</sub>}} is defined to be&nbsp;1.  In this particular case, there is an assumption of unit steps for the changes in the values of {{math|''x'', ''h'' {{=}} 1}} of the generalization below.\n\nNote the formal correspondence of this result to [[Taylor's theorem]].  Historically, this, as well as the [[Chu–Vandermonde identity]],  \n:<math>(x+y)_n=\\sum_{k=0}^n \\binom{n}{k} (x)_{n-k} \\,(y)_k ,</math>\n(following from it, and corresponding to the [[binomial theorem]]),  are included in the observations that matured to the system of [[umbral calculus]].\n\nTo illustrate how one may use Newton's formula in actual practice, consider the first few terms of doubling the [[Fibonacci sequence]] {{math|''f'' {{=}} 2,&nbsp;2,&nbsp;4,&nbsp;...}} One can find a [[polynomial]] that reproduces these values, by first computing a difference table, and then substituting the differences that correspond to {{math|''x''<sub>0</sub>}} (underlined) into the formula as follows,\n:<math>\n\\begin{matrix}\n\n\\begin{array}{|c||c|c|c|}\n\\hline\n x & f=\\Delta^0 & \\Delta^1 & \\Delta^2 \\\\\n\\hline\n1&\\underline{2}& & \\\\\n & &\\underline{0}& \\\\\n2&2& &\\underline{2} \\\\\n & &2& \\\\\n3&4& & \\\\\n\\hline\n\\end{array}\n\n&\n\n\\quad \\begin{align}\nf(x) & =\\Delta^0 \\cdot 1 +\\Delta^1 \\cdot \\dfrac{(x-x_0)_1}{1!} + \\Delta^2 \\cdot \\dfrac{(x-x_0)_2}{2!} \\quad (x_0=1)\\\\\n \\\\\n& =2 \\cdot 1 + 0 \\cdot \\dfrac{x-1}{1} + 2 \\cdot \\dfrac{(x-1)(x-2)}{2} \\\\\n \\\\\n& =2 + (x-1)(x-2) \\\\\n\\end{align}\n\\end{matrix}\n</math>\n\nFor the case of nonuniform steps in the values of {{mvar|x}}, Newton computes the [[divided differences]],\n:<math>\\Delta _{j,0}=y_j,\\qquad \\Delta _{j,k}=\\frac{\\Delta _{j+1,k-1}-\\Delta _{j,k-1}}{x_{j+k}-x_j}\\quad \\ni \\quad \\left\\{ k>0,\\; j\\le \\max \\left( j \\right)-k \\right\\},\\qquad \\Delta 0_k=\\Delta _{0,k}</math>\nthe series of products,\n:<math>{P_0}=1,\\quad \\quad P_{k+1}=P_k\\cdot \\left( \\xi -x_k \\right) ,</math>\nand the resulting polynomial is the [[scalar product]],<ref>[[Robert D. Richtmyer|Richtmeyer, D.]] and  Morton, K.W., (1967). ''Difference Methods for Initial Value Problems'', 2nd ed., Wiley, New York.</ref>\n:<math>f(\\xi ) = \\Delta 0 \\cdot P\\left( \\xi  \\right)</math> .\n\nIn analysis with [[p-adic number|{{mvar|p}}-adic numbers]], [[Mahler's theorem]] states that the assumption that {{mvar|f}} is a polynomial function can be weakened all the way to the assumption that {{mvar|f}} is merely continuous.\n\n[[Carlson's theorem]] provides necessary and sufficient conditions for a Newton series to be unique, if it exists. However, a Newton series does not, in general, exist.\n\nThe Newton series, together with the [[Stirling series]] and the [[Selberg class|Selberg series]], is a special case of the general [[difference series]], all of which are defined in terms of suitably scaled forward differences.\n\nIn a compressed and slightly more general form and equidistant nodes the formula reads\n:<math>f(x)=\\sum_{k=0}\\binom{\\frac{x-a}h}{k} \\sum_{j=0}^k (-1)^{k-j}\\binom{k}{j}f(a+j h).</math>\n\n==Calculus of finite differences==\n\nThe forward difference can be considered as an [[Operator (mathematics)|operator]], called the '''{{vanchor|difference operator}}''', which maps the function {{mvar|f}} to {{math|Δ<sub>''h''</sub>[&thinsp;''f''&thinsp;]}}.<ref>[[George Boole|Boole, George]], (1872). ''A Treatise On The Calculus of Finite Differences'', 2nd ed., Macmillan and Company. [https://archive.org/details/cu31924031240934   On line]. Also, [Dover edition 1960]</ref><ref>Jordan, Charles, (1939/1965). \"Calculus of Finite Differences\", Chelsea Publishing. On-line: [https://books.google.com/books?hl=en&lr=&id=3RfZOsDAyQsC&oi=fnd&pg=PA1&ots=AqSuAgOKs3&sig=fzPpAdvnzp7sG6PorqIe5qFjD2Q#v=onepage]</ref> This operator amounts to\n::<math>\\Delta_h = T_h-I, </math>\nwhere {{math|''T''<sub>''h''</sub>}} is the [[shift operator]] with step ''h'', defined by {{math|''T''<sub>''h''</sub>[&thinsp;''f''&thinsp;](''x'') {{=}} ''f''&thinsp;(''x'' + ''h'')}},  and {{mvar|I}} is the [[identity operator]].\n\nThe finite difference of higher orders can be defined in recursive manner as {{math|Δ{{su|b=''h''|p=''n''}} ≡ Δ<sub>''h''</sub>(Δ{{su|b=''h''|p=''n'' − 1}})}}.   Another equivalent definition is {{math|Δ{{su|b=''h''|p=''n''}} {{=}} [''T''<sub>''h''</sub> − ''I'']<sup>''n''</sup>}}.\n\nThe difference operator {{math|Δ<sub>''h''</sub>}} is a [[linear operator]], as such it satisfies {{math|Δ<sub>''h''</sub>[''αf'' + ''βg''](''x'') {{=}} ''α'' Δ<sub>''h''</sub>[&thinsp;''f''&thinsp;](''x'') + ''β'' Δ<sub>''h''</sub>[''g''](''x'')}}.\n\nIt also satisfies a special [[Leibniz rule (generalized product rule)|Leibniz rule]] indicated above,\n{{math|Δ<sub>''h''</sub>(''f''&thinsp;(''x'')''g''(''x'')) {{=}} (Δ<sub>''h''</sub>''f''&thinsp;(''x'')) ''g''(''x''+''h'') + ''f''&thinsp;(''x'') (Δ<sub>''h''</sub>''g''(''x''))}}.  Similar statements hold for the backward and central differences.\n\nFormally applying the [[Taylor series]] with respect to {{mvar|h}}, yields the formula\n:<math> \\Delta_h = hD + \\frac{1}{2!} h^2D^2 + \\frac{1}{3!} h^3D^3 + \\cdots = \\mathrm{e}^{hD} - I , </math>\nwhere {{mvar|D}} denotes the continuum derivative operator, mapping {{mvar|f}} to its derivative {{math|''f''&thinsp;′}}. The expansion is valid when both sides act on [[analytic function]]s, for sufficiently small {{mvar|h}}. Thus, {{math|''T''<sub>''h''</sub> {{=}} ''e''<sup>''hD''</sup>}}, and formally inverting the exponential yields \n:<math> hD = \\log(1+\\Delta_h) = \\Delta_h - \\tfrac{1}{2} \\Delta_h^2 + \\tfrac{1}{3} \\Delta_h^3 + \\cdots. </math>\nThis formula holds in the sense that both operators give the same result when applied to a polynomial.\n\nEven for analytic functions, the series on the right is not guaranteed to converge; it may be an [[asymptotic series]]. However, it can be used to obtain more accurate approximations for the derivative. For instance, retaining the first two terms of the series yields the second-order approximation to {{math|''f''&nbsp;′(''x'')}} mentioned at the end of the [[#Higher-order differences|section ''Higher-order differences'']].\n\nThe analogous formulas for the backward and central difference operators are\n:<math> hD = -\\log(1-\\nabla_h) \\quad\\text{and}\\quad hD = 2 \\operatorname{arsinh}\\left(\\tfrac12\\delta_h\\right). </math>\n\nThe calculus of finite differences is related to the [[umbral calculus]] of combinatorics. This remarkably systematic correspondence is due to the identity of the [[commutators]] of the umbral quantities to their continuum analogs ({{math|''h'' → 0}} limits),\n\n{{Equation box 1\n|indent =:\n|equation = \n<math>  \\left[ \\frac{\\Delta_h}{h} , x\\, T^{-1}_h \\right] = [ D , x ] = I .</math>\n|cellpadding= 6\n|border\n|border colour = #0073CF\n|background colour=#F9FFF7}}\n\nA large number of formal differential relations of standard calculus involving \nfunctions {{math|''f''&thinsp;(''x'')}} thus ''map systematically to umbral finite-difference analogs'' involving {{math|''f''&thinsp;(''xT''{{su|b=''h''|p=−1}})}}.\n\nFor instance, the umbral analog of a monomial {{mvar|x<sup>n</sup>}} is a generalization of the above falling factorial ([[Pochhammer k-symbol]]),  \n:<math>~(x)_n\\equiv  \\left(xT_h^{-1}\\right)^n=x (x-h) (x-2h) \\cdots \\bigl(x-(n-1)h\\bigr),</math> \nso that \n:<math>\\frac{\\Delta_h}{h} (x)_n=n (x)_{n-1} ,</math>\nhence the above Newton interpolation formula (by matching coefficients in the expansion of an arbitrary function {{math|''f''&thinsp;(''x'')}} in such symbols), and so on.\n\nFor example, the umbral sine is\n:<math>\\sin \\left(x\\,T_h^{-1}\\right) = x -\\frac{(x)_3}{3!} +  \\frac{(x)_5}{5!} - \\frac{(x)_7}{7!} + \\cdots</math>\n\nAs in the continuum limit, the eigenfunction of  {{math|{{sfrac|Δ<sub>''h''</sub>|''h''}}}} also happens to be an exponential,\n\n:<math>\\frac{\\Delta_h}{h}(1+\\lambda h)^\\frac{x}{h} =\\frac{\\Delta_h}{h} e^{\\ln (1+\\lambda h) \\frac{x}{h}}= \\lambda e^{\\ln (1+\\lambda h) \\frac{x}{h}} ,</math>\n\nand hence ''Fourier sums of continuum functions are readily mapped to umbral Fourier sums faithfully'', i.e., involving the same Fourier coefficients multiplying these umbral basis exponentials.<ref>{{cite journal |last =Zachos|first =C.| authorlink =Cosmas Zachos| year =2008| title =Umbral  Deformations on Discrete Space-Time  | journal =International Journal of Modern Physics A| volume =23 | issue=13| pages =2005&ndash;2014  | doi = 10.1142/S0217751X08040548  | url =https://arxiv.org/pdf/0710.2306| arxiv =0710.2306| bibcode =2008IJMPA..23.2005Z}}</ref>  This umbral exponential thus amounts to the exponential [[generating function]] of the [[Pochhammer symbol]]s.\n\nThus, for instance,  the [[Dirac delta function]] maps to its umbral correspondent, the [[Sinc function|cardinal sine function]],\n\n:<math>\\delta (x) \\mapsto \\frac{\\sin \\left[ \\frac{\\pi}{2}\\left(1+\\frac{x}{h}\\right) \\right]}{ \\pi (x+h) },</math>\n\nand so forth.<ref>{{Cite journal | last1 = Curtright | first1 = T. L. | last2 = Zachos | first2 = C. K. | doi = 10.3389/fphy.2013.00015 | title = Umbral Vade Mecum | journal = Frontiers in Physics | volume = 1 | year = 2013 | pmid =  | pmc = | arxiv = 1304.0429 | bibcode = 2013FrP.....1...15C }}</ref>  [[Difference equation]]s can often be solved with techniques very similar to those for solving [[differential equation]]s.\n\nThe inverse operator of the forward difference operator, so then the umbral integral, is the [[indefinite sum]] or antidifference operator.\n\n==Rules for calculus of finite difference operators==\nAnalogous to [[Differentiation rules|rules for finding the derivative]], we have:\n* '''Constant rule''': If {{mvar|c}} is a [[Constant (mathematics)|constant]], then\n::<math>\\Delta c = 0</math>\n* '''[[Linearity of differentiation|Linearity]]''': if {{mvar|a}} and {{mvar|b}} are [[Constant (mathematics)|constants]],\n::<math>\\Delta (a f + b g) = a \\,\\Delta f + b \\,\\Delta g</math>\n\nAll of the above rules apply equally well to any difference operator, including {{math|∇}} as to {{math|Δ}}.\n* '''[[Product rule]]''':\n::<math> \\begin{align} \\Delta (f g) &= f \\,\\Delta g + g \\,\\Delta f + \\Delta f \\,\\Delta g \\\\ \\nabla (f g) &= f \\,\\nabla g + g \\,\\nabla f - \\nabla f \\,\\nabla g \\end{align}</math>\n* '''[[Quotient rule]]''':\n::<math>\\nabla \\left( \\frac{f}{g} \\right) = \\frac{1}{g} \\det \\begin{bmatrix} \\nabla f & \\nabla g \\\\ f & g \\end{bmatrix} \\left( \\det {\\begin{bmatrix} g & \\nabla g \\\\ 1 & 1 \\end{bmatrix}}\\right)^{-1} </math>\n:or\n::<math>\\nabla\\left( \\frac{f}{g} \\right)= \\frac {g \\,\\nabla f - f \\,\\nabla g}{g \\cdot (g - \\nabla g)}</math>\n\n* '''[[Fundamental theorem of calculus|Summation rules]]''':\n::<math>\\begin{align} \\sum_{n=a}^{b} \\Delta f(n) &= f(b+1)-f(a) \\\\ \\sum_{n=a}^{b} \\nabla f(n) &= f(b)-f(a-1) \\end{align}</math>\n\nSee references.<ref>{{cite book|last=Levy|first=H.|author2=Lessman, F.|title=Finite Difference Equations|year=1992|publisher=Dover|isbn=0-486-67260-3}}</ref><ref>Ames, W. F., (1977).  ''Numerical Methods for Partial Differential Equations'', Section 1.6. Academic Press, New York. {{ISBN|0-12-056760-1}}.</ref><ref>[[Francis B. Hildebrand|Hildebrand, F. B.]], (1968). ''Finite-Difference Equations and Simulations'', Section 2.2, Prentice-Hall, Englewood Cliffs, New Jersey.</ref><ref>{{Cite journal\n | first1 = Philippe | last1 = Flajolet\n | authorlink2 = Robert Sedgewick (computer scientist) | first2 = Robert | last2 = Sedgewick\n | url = http://algo.inria.fr/flajolet/Publications/FlSe95.pdf\n | title = Mellin transforms and asymptotics: Finite differences and Rice's integrals\n | journal=Theoretical Computer Science\n | volume = 144 | issue = 1–2 | year = 1995 | pages = 101–124\n | doi = 10.1016/0304-3975(94)00281-M\n | postscript = <!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->{{inconsistent citations}}}}.</ref>\n\n==Generalizations==\n\n*A '''generalized finite difference''' is usually defined as\n:<math>\\Delta_h^\\mu[f](x) = \\sum_{k=0}^N \\mu_k f(x+kh),</math>\nwhere {{math|''μ'' {{=}} (''μ''<sub>0</sub>,… ''μ<sub>N</sub>'')}} is its coefficient vector. An '''infinite difference''' is a further generalization, where the finite sum above is replaced by an [[Series (mathematics)|infinite series]]. Another way of generalization is making coefficients {{math|''μ<sub>k</sub>''}} depend on point {{mvar|x}}: {{math|''μ<sub>k</sub>'' {{=}} ''μ<sub>k</sub>''(''x'')}}, thus considering '''weighted finite difference'''. Also one may make the step {{mvar|h}} depend on point {{mvar|x}}: {{math|''h'' {{=}} ''h''(''x'')}}. Such generalizations are useful for constructing different [[modulus of continuity]].\n\n*The generalized difference can be seen as the polynomial rings {{math|''R''[''T<sub>h</sub>'']}}. It leads to difference algebras.\n*Difference operator generalizes to [[Möbius inversion]] over a [[partially ordered set]].\n*As a convolution operator: Via the formalism of [[incidence algebra]]s, difference operators and other Möbius inversion can be represented by [[convolution]] with a function on the poset, called the [[Möbius function]] {{mvar|μ}}; for the difference operator, {{mvar|μ}} is the sequence (1,&nbsp;−1,&nbsp;0,&nbsp;0,&nbsp;0,&nbsp;...).\n\n==Finite difference in several variables==\n\nFinite differences can be considered in more than one variable. They are analogous to [[partial derivative]]s in several variables.\n\nSome partial derivative approximations are:\n\n:<math>\\begin{align}\nf_{x}(x,y)  &\\approx  \\frac{f(x+h ,y) - f(x-h,y)}{2h} \\\\\nf_{y}(x,y)  &\\approx  \\frac{f(x,y+k ) - f(x,y-k)}{2k} \\\\\nf_{xx}(x,y) &\\approx  \\frac{f(x+h ,y) - 2 f(x,y) + f(x-h,y)}{h^2} \\\\\nf_{yy}(x,y) &\\approx  \\frac{f(x,y+k) - 2 f(x,y) + f(x,y-k)}{k^2} \\\\\nf_{xy}(x,y) &\\approx  \\frac{f(x+h,y+k) - f(x+h,y-k) - f(x-h,y+k) + f(x-h,y-k)}{4hk} .\n\\end{align}</math>\n\nAlternatively, for applications in which the computation of {{mvar|f}}  is the most costly step, and both first and second derivatives must be computed, a more efficient formula for the last case is\n\n:<math> f_{xy}(x,y) \\approx \\frac{f(x+h, y+k) - f(x+h, y) - f(x, y+k) + 2 f(x,y) - f(x-h, y) - f(x, y-k) + f(x-h, y-k)}{2hk},</math>\n\nsince the only values to compute that are not already needed for the previous four equations are {{math|''f''&thinsp;(''x'' + ''h'', ''y'' + ''k'')}}   and {{math|''f''&thinsp;(''x'' − ''h'', ''y'' − ''k'')}}.\n\n==See also==\n{{columns-list|colwidth=22em|\n* [[Arc elasticity]]\n* [[Carlson's theorem]]\n* [[Central differencing scheme]]\n* [[Divided differences]]\n* [[Finite difference coefficients]]\n* [[Finite difference method]]\n* [[Finite volume method]]\n* [[Five-point stencil]]\n* [[Gilbreath's conjecture]]\n* [[Lagrange polynomial]]\n* [[Modulus of continuity]]\n* [[Multiplicative calculus#Discrete calculus|Multiplicative calculus]]\n* [[Newton polynomial]]\n* [[Nörlund–Rice integral]]\n* [[Numerical differentiation]]\n* [[Sheffer sequence]]\n* [[Summation by parts]]\n* [[Table of Newtonian series]]\n* [[Taylor series]]\n* [[Time scale calculus]]\n* [[Umbral calculus]]\n* [[Upwind differencing scheme for convection]]\n}}\n\n== References ==\n<references/>\n* Richardson, C. H. (1954): ''An Introduction to the Calculus of Finite Differences'' (Van Nostrand (1954)'' [http://babel.hathitrust.org/cgi/pt?id=mdp.39015000982945;view=1up;seq=5 online copy]\n* Mickens, R. E. (1991): ''Difference Equations: Theory and Applications'' (Chapman and Hall/CRC) {{ISBN|978-0442001360}}\n\n== External links ==\n* {{springer|title=Finite-difference calculus|id=p/f040230}}\n* [http://reference.wolfram.com/mathematica/tutorial/NDSolvePDE.html#c:4 Table of useful finite difference formula generated using [[Mathematica]] ]\n* D. Gleich (2005),  [https://web.archive.org/web/20090419132601/http://www.stanford.edu/~dgleich/publications/finite-calculus.pdf ''Finite Calculus: A Tutorial for Solving Nasty Sums'']\n* [http://mathformeremortals.wordpress.com/2013/01/12/a-numerical-second-derivative-from-three-points/ Discrete Second Derivative from Unevenly Spaced Points]\n\n{{DEFAULTSORT:Finite Difference}}\n[[Category:Finite differences| ]]\n[[Category:Numerical differential equations]]\n[[Category:Mathematical analysis]]\n[[Category:Factorial and binomial topics]]\n[[Category:Linear operators in calculus]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Finite Legendre transform",
      "url": "https://en.wikipedia.org/wiki/Finite_Legendre_transform",
      "text": "The '''finite Legendre transform (fLT)''' transforms a mathematical function defined on the finite interval into its Legendre spectrum \n<ref>{{cite book | first=A.J. | last=Jerri | title=Integral and discrete transforms with applications and error analysis| location=New York | publisher=Marcel Dekker Inc. | year=1992 | zbl= 0753.44001 | series=Pure and Applied Mathematics | volume=162 }}</ref>\n.<ref>{{cite journal | last1=Méndez-Pérez | first1=J.M.R. | last2=Miquel Morales | first2=G. | title=On the convolution of the generalized finite Legendre transform | zbl=0915.46038 | journal=Math. Nachr. | volume=188 | pages=219–236 | year=1997 | doi=10.1002/mana.19971880113 }}</ref>\nConversely, the inverse fLT (ifLT) reconstructs the original function from the components of the Legendre spectrum and the [[Legendre polynomials]], which are orthogonal on the interval [−1,1]. Specifically, assume a function ''x''(''t'') to be defined on an interval [−1,1] and discretized into ''N'' equidistant points on this interval. The fLT then yields the decomposition of ''x''(''t'') into its spectral Legendre components,\n\n:<math>L_x (k) = \\frac{2k+1}{N}\\sum_{t=-1}^{t=1}x(t)P_k(t), </math>\n\nwhere the factor (2''k''&nbsp;+&nbsp;1)/''N'' serves as normalization factor and ''L''<sub>''x''</sub>(''k'') gives the contribution of the ''k''-th Legendre polynomial to ''x''(''t'') such that (ifLT)\n\n:<math>x(t) = \\sum_k L_x(k) P_k(t). </math>\n\nThe fLT should not be confused with the Legendre transform or [[Legendre transformation]] used in thermodynamics and quantum physics.\n\n==Legendre filter==\nThe fLT of a noisy experimental outcome ''s''(''t'') and the subsequent application of the inverse fLT (ifLT) on an appropriately truncated Legendre spectrum of ''s''(''t'') gives a smoothed version of ''s''(''t''). The fLT and incomplete ifLT thus act as a filter. In contrast to the common Fourier [[low-pass filter]] which transmits low frequency harmonics and filters out high frequency harmonics, the Legendre lowpass transmits signal components proportional to low degree Legendre polynomials, while signal components proportional to higher degree Legendre polynomials are filtered out.  \n<ref>Guobin Bao and Detlev Schild, Fast and accurate fitting and filtering of noisy exponentials in legendre space, 2014. PLoS ONE, 9(3), e90500</ref>\n\n==References==\n{{Reflist|30em}}\n\n==Further reading==\n* {{cite book | last=Butzer | first=Paul L. | chapter=Legendre transform methods in the solution of basic problems in algebraic approximation | zbl=0567.41010 | title=Functions, series, operators, Proc. int. Conf., Budapest 1980, Vol. I, | series=Colloq. Math. Soc. János Bolyai | volume=35 | pages=277–301 | year=1983 }}\n\n{{DEFAULTSORT:Finite Legendre Transform}}\n[[Category:Discrete transforms]]\n[[Category:Digital signal processing]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Finite volume method",
      "url": "https://en.wikipedia.org/wiki/Finite_volume_method",
      "text": "The '''finite volume method''' ('''FVM''') is a method for representing and evaluating [[partial differential equation]]s in the form of algebraic equations [LeVeque, 2002; Toro, 1999].  \nSimilar to the [[finite difference method]] or [[finite element method]], values are calculated at discrete places on a meshed geometry. \"Finite volume\" refers to the small volume surrounding each node point on a mesh. In the finite volume method, volume integrals in a partial differential equation that contain a [[divergence]] term are converted to [[surface integral]]s, using the [[divergence theorem]]. These terms are then evaluated as fluxes at the surfaces of each finite volume. Because the flux entering a given volume is identical to that leaving the adjacent volume, these methods are [[Conservation law (physics)|conservative]]. Another advantage of the finite volume method is that it is easily formulated to allow for unstructured meshes. The method is used in many [[computational fluid dynamics]] packages.\n\n==1D example==\n\nConsider a simple 1D [[advection]] problem defined by the following [[partial differential equation]]\n\n:<math>\\quad (1) \\qquad  \\qquad \\frac{\\partial\\rho}{\\partial t}+\\frac{\\partial f}{\\partial x}=0,\\quad t\\ge0.</math>\n\nHere, <math> \\rho=\\rho \\left( x,t \\right) \\ </math> represents the state variable and <math> f=f \\left( \\rho \\left( x,t \\right) \\right) \\ </math> represents the [[flux]] or flow of <math> \\rho \\ </math>. Conventionally, positive <math> f  \\ </math> represents flow to the right while negative <math> f \\ </math> represents flow to the left. If we assume that equation (1) represents a flowing medium of constant area, we can sub-divide the spatial domain, <math> x \\ </math>, into ''finite volumes'' or ''cells'' with cell centres indexed as <math> i \\ </math>. For a particular cell, <math> i \\ </math>, we can define the ''volume average'' value of <math> {\\rho }_i \\left( t \\right) = \\rho \\left( x, t \\right) \\ </math> at time  <math> {t=t_1}\\ </math> and <math>{ x \\in \\left[ x_{i-\\frac{1}{2}} , x_{i+\\frac{1}{2}} \\right] }\\ </math>, as\n\n:<math>\\quad (2) \\qquad  \\qquad \\bar{\\rho}_i \\left( t_1 \\right) = \\frac{1}{ x_{i+\\frac{1}{2}} - x_{i-\\frac{1}{2}}} \\int_{x_{i-\\frac{1}{2}}}^{x_{i+\\frac{1}{2}}} \\rho \\left(x,t_1 \\right)\\, dx ,</math>\n\nand at time  <math> {t = t_2}\\ </math> as,\n\n:<math>\\quad (3) \\qquad  \\qquad \\bar{\\rho}_i \\left( t_2 \\right) = \\frac{1}{x_{i+\\frac{1}{2}} - x_{i-\\frac{1}{2}}} \\int_{x_{i-\\frac{1}{2}}}^{x_{i+\\frac{1}{2}}} \\rho \\left(x,t_2 \\right)\\, dx ,</math>\n\nwhere <math> x_{i-\\frac{1}{2}} \\ </math> and <math> x_{i+\\frac{1}{2}} \\ </math> represent locations of the upstream and downstream faces or edges respectively of the <math> i^{th} \\ </math> cell.\n\nIntegrating equation (1) in time, we have:\n\n:<math>\\quad (4) \\qquad  \\qquad \\rho \\left( x, t_2 \\right) = \\rho \\left( x, t_1 \\right) - \\int_{t_1}^{t_2} f_x \\left( x,t \\right)\\, dt,</math>\n\nwhere <math>f_x=\\frac{\\partial f}{\\partial x}</math>.\n\nTo obtain the volume average of <math> \\rho\\left(x,t\\right) </math> at time <math> t=t_{2} \\ </math>, we integrate <math> \\rho\\left(x,t_2 \\right) </math> over the cell volume, <math>\\left[ x_{i-\\frac{1}{2}} , x_{i+\\frac{1}{2}} \\right] </math> and divide the result by <math>\\Delta x_i = x_{i+\\frac{1}{2}}-x_{i-\\frac{1}{2}} </math>, i.e.\n\n:<math> \\quad (5) \\qquad  \\qquad \\bar{\\rho}_{i}\\left( t_{2}\\right) =\\frac{1}{\\Delta x_i}\\int_{x_{i-\\frac{1}{2}}}^{x_{i+\\frac{1}{2}}}\\left\\{ \\rho\\left( x,t_{1}\\right) - \\int_{t_{1}}^{t_2} f_{x} \\left( x,t \\right) dt \\right\\} dx.</math>\n\nWe assume that <math> f \\ </math> is well behaved and that we can reverse the order of integration. Also, recall that flow is normal to the unit area of the cell. Now, since in one dimension <math>f_x \\triangleq \\nabla \\cdot f </math>, we can apply the [[divergence theorem]], i.e. <math>\\oint_{v}\\nabla\\cdot fdv=\\oint_{S}f\\, dS </math>, and substitute for the volume integral of the [[divergence]] with the values of <math>f(x) \\ </math> evaluated at the cell surface (edges <math>x_{i-\\frac{1}{2}} \\ </math> and <math> x_{i+\\frac{1}{2}} \\ </math>) of the finite volume as follows:\n\n:<math>\\quad (6) \\qquad  \\qquad \\bar{\\rho}_i \\left( t_2 \\right) = \\bar{\\rho}_i \\left( t_1 \\right)\n\n- \\frac{1}{\\Delta x_{i}} \n  \\left( \\int_{t_1}^{t_2} f_{i + \\frac{1}{2}} dt\n- \\int_{t_1}^{t_2} f_{i - \\frac{1}{2}} dt\n\\right) .</math>\n\nwhere <math>f_{i \\pm \\frac{1}{2}} =f \\left( x_{i \\pm \\frac{1}{2}}, t \\right) </math>.\n\nWe can therefore derive a ''semi-discrete'' numerical scheme for the above problem with cell centres indexed as <math> i\\ </math>, and with cell edge fluxes indexed as <math> i\\pm\\frac{1}{2} </math>, by differentiating (6) with respect to time to obtain:\n\n:<math>\\quad (7) \\qquad  \\qquad \\frac{d \\bar{\\rho}_i}{d t} + \\frac{1}{\\Delta x_i} \\left[ \nf_{i + \\frac{1}{2}} - f_{i - \\frac{1}{2}}  \\right] =0 ,</math>\n\nwhere values for the edge fluxes, <math> f_{i \\pm \\frac{1}{2}} </math>, can be reconstructed by interpolation or extrapolation of the cell averages. Equation (7) is ''exact'' for the volume averages; i.e., no approximations have been made during its derivation.\n\nThis method can also be applied to a [[Finite volume method for two dimensional diffusion problem|2D]] situation by considering the north and south faces along with the east and west faces around a node.\n\n== General conservation law ==\n\nWe can also consider the general [[Conservation law (physics)|conservation law]] problem, represented by the following [[partial differential equation|PDE]],\n\n:<math> \\quad (8) \\qquad  \\qquad {{\\partial {\\mathbf u}} \\over {\\partial t}} + \\nabla  \\cdot {\\mathbf f}\\left( {\\mathbf u } \\right) = {\\mathbf 0} . </math>\n\nHere, <math> {\\mathbf u} \\ </math> represents a vector of states and <math>\\mathbf f \\ </math> represents the corresponding [[flux]] tensor. Again we can sub-divide the spatial domain into finite volumes or cells. For a particular cell, <math>i \\ </math>, we take the volume integral over the total volume of the cell, <math>v _{i} \\ </math>, which gives,\n\n:<math> \\quad (9) \\qquad  \\qquad \\int _{v_{i}}  {{\\partial {\\mathbf u}} \\over {\\partial t}}\\, dv \n+ \\int _{v_{i}}  \\nabla  \\cdot {\\mathbf f}\\left( {\\mathbf u } \\right)\\, dv = {\\mathbf 0} .</math>\n\nOn integrating the first term to get the ''volume average'' and applying the ''divergence theorem'' to the second, this yields\n\n:<math>\\quad (10) \\qquad  \\qquad \nv_{i} {{d {\\mathbf {\\bar u} }_{i} } \\over {dt}} + \\oint _{S_{i} } \n {\\mathbf f} \\left( {\\mathbf u } \\right) \\cdot {\\mathbf n }\\  dS  = {\\mathbf 0}, </math>\n\nwhere <math> S_{i} \\ </math> represents the total surface area of the cell and <math>{\\mathbf n}</math> is a unit vector normal to the surface and pointing outward. So, finally, we are able to present the general result equivalent to (8), i.e.\n\n:<math> \\quad (11) \\qquad  \\qquad \n{{d {\\mathbf {\\bar u} }_{i} } \\over {dt}} + {{1} \\over {v_{i}} } \\oint _{S_{i} } \n {\\mathbf f} \\left( {\\mathbf u } \\right)\\cdot {\\mathbf n }\\ dS  = {\\mathbf 0} .</math>\n\nAgain, values for the edge fluxes can be reconstructed by interpolation or extrapolation of the cell averages. The actual numerical scheme will depend upon problem geometry and mesh construction. [[MUSCL scheme|MUSCL]] reconstruction is often used in [[high resolution scheme]]s where shocks or discontinuities are present in the solution.\n\nFinite volume schemes are conservative as cell averages change through the edge fluxes. In other words, ''one cell's loss is another cell's gain''!\n\n==See also==\n*[[Finite element method]]\n*[[Flux limiter]]\n*[[Godunov's scheme]]\n*[[Godunov's theorem]]\n*[[High-resolution scheme]]\n*[[KIVA (Software)]]\n*[[MIT General Circulation Model]]\n*[[MUSCL scheme]]\n*[[Sergei K. Godunov]]\n*[[Total variation diminishing]]\n*[[Finite volume method for unsteady flow]]\n\n==References==\n*'''Eymard, R. Gallouët, T.   R. Herbin, R.''' (2000) ''The finite volume method''  Handbook of Numerical Analysis, Vol. VII, 2000, p.&nbsp;713–1020. Editors: P.G. Ciarlet and J.L. Lions.\n*'''LeVeque, Randall''' (2002), ''Finite Volume Methods for Hyperbolic Problems'', Cambridge University Press.\n*'''Toro, E. F.''' (1999), ''Riemann Solvers and Numerical Methods for Fluid Dynamics'', Springer-Verlag.\n\n==Further reading==\n*'''Patankar, Suhas V.''' (1980), ''Numerical Heat Transfer and Fluid Flow'', Hemisphere.\n*'''Hirsch, C.''' (1990), ''Numerical Computation of Internal and External Flows, Volume 2: Computational Methods for Inviscid and Viscous Flows'', Wiley.\n*'''Laney, Culbert B.''' (1998), ''Computational Gas Dynamics'', Cambridge University Press.\n*'''LeVeque, Randall''' (1990), ''Numerical Methods for Conservation Laws'', ETH Lectures in Mathematics Series, Birkhauser-Verlag.\n*'''Tannehill, John C.''', et al., (1997), ''Computational Fluid mechanics and Heat Transfer'', 2nd Ed., Taylor and Francis.\n*'''Wesseling, Pieter''' (2001), ''Principles of Computational Fluid Dynamics'', Springer-Verlag.\n\n== External links ==\n* [https://old.i2m.univ-amu.fr/~herbin/PUBLI/bookevol.pdf Finite volume methods] by R. Eymard, T Gallouët and R. Herbin, update of the article published in Handbook of Numerical Analysis, 2000\n* {{cite journal|url=http://www.imtek.uni-freiburg.de/simulation/mathematica/IMSweb/imsTOC/Lectures%20and%20Tips/Simulation%20I/FVM_introDocu.html |title=The Finite Volume Method (FVM) – An introduction|first=Oliver|last=Rübenkönig|postscript=<!-- none --> |archiveurl=https://web.archive.org/web/20091002233707/http://www.imtek.uni-freiburg.de/simulation/mathematica/IMSweb/imsTOC/Lectures%20and%20Tips/Simulation%20I/FVM_introDocu.html |archivedate=2009-10-02}}, available under the [[GNU Free Document License|GFDL]].\n* [http://www.ctcms.nist.gov/fipy/ FiPy: A Finite Volume PDE Solver Using Python] from NIST.\n* [http://depts.washington.edu/clawpack/ CLAWPACK]: a software package designed to compute numerical solutions to hyperbolic partial differential equations using a wave propagation approach\n\n{{Numerical PDE}}\n{{Differential equations topics}}\n[[Category:Numerical differential equations]]\n[[Category:Computational fluid dynamics]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Gal's accurate tables",
      "url": "https://en.wikipedia.org/wiki/Gal%27s_accurate_tables",
      "text": "'''Gal's accurate tables''' is a method devised by [[Shmuel Gal]] to provide accurate values of [[special functions]] using a [[lookup table]] and [[interpolation]]. It is a fast and efficient method for generating values of functions like the [[exponential function|exponential]] or the [[trigonometric functions]] to within last-bit \naccuracy for almost all argument values without using extended precision arithmetic.\n\nThe main idea in Gal's accurate tables is a different tabulation for the special function being computed. Commonly, the range is divided into several subranges, each with precomputed values and correction formulae.  To compute the function, look up the closest point and compute a correction as a function of the distance.\n\nGal's idea is to not precompute equally spaced values, but rather to [[perturbation theory|perturb]] the points ''x'' so that both ''x'' and ''f''(''x'') are very nearly exactly representable in the chosen numeric format.  By searching approximately 1000 values on either side of the desired value ''x'', a value can be found such that ''f''(''x'') can be represented with less than ±1/2000 bit of [[rounding error]].  If the correction is also computed to ±1/2000 bit of accuracy (which does not require extra floating-point precision as long as the correction is less than 1/2000 the magnitude of the stored value ''f''(''x''), ''and'' the computed correction is more than ±1/1000 of a bit away from exactly half a bit (the difficult rounding case), then it is known whether the exact function value should be rounded up or down.\n\nThe technique provides an efficient way to compute the function value to within ±1/1000 least-significant bit, i.e. 10 extra bits of precision.  If this approximation is more than ±1/1000 of a bit away from exactly midway between two representable values (which happens all but 2/1000 of the time, i.e. 99.8% of the time), then the correctly rounded result is clear.\n\nCombined with an extended-precision fallback algorithm, this can compute the correctly rounded result in very reasonable ''average'' time.\n\n2/1000 (0.2%) of the time, a higher-precision function evaluation is required to resolve the rounding uncertainty, but this is infrequent enough that it has little effect on the average calculation time.\n\nThe problem of generating function values which are accurate to the last bit is known as the [[table-maker's dilemma]].\n\n==See also==\n*[[Floating point]]\n*[[Rounding]]\n  \n==References==\n{{Reflist}}\n* {{cite book |author-first=Shmuel |author-last=Gal |author-link=Shmuel Gal |chapter=Computing elementary functions: A new approach for achieving high accuracy and good performance |title=Accurate Scientific Computations |date=1986 |page=1-16 |publisher=[[Springer-Verlag]] Berlin Heidelberg |location=Proceedings of Computations, Symposium, Bad Neuenahr, Federal Republic of Germany, March 12-14, 1985 |editor-last1=Miranker |editor-first1=Willard L. |editor-last2=Toupin |editor-first2=Richard A. |isbn=978-3-540-16798-3 |edition=1}}\n* {{cite journal |author-first1=Shmuel |author-last1=Gal |author-link1=Shmuel Gal |author-first2=Boris |author-last2=Bachelis |title=An accurate elementary mathematical library for the IEEE floating point standard |journal=[[ACM Transactions on Mathematical Software]] |date=1991}}\n* {{cite book |author-first=Jean-Michel |author-last=Muller |title=Elementary Functions: Algorithms and Implementation |edition=2 |publisher=[[Birkhäuser]] |location=Boston, MA, USA |date=2006 |isbn=978-0-8176-4372-0 |lccn=2005048094}}\n* {{cite book |author-first=Jean-Michel |author-last=Muller |title=Elementary Functions: Algorithms and Implementation |edition=3 |publisher=[[Birkhäuser]] |location=Boston, MA, USA |date=2016-12-12 |isbn=978-1-4899-7981-0 |id={{ISBN|1-4899-7981-6}}}}\n* {{cite journal |author-first1=Damien |author-last1=Stehlé |author-first2=Paul |author-last2=Zimmermann |author-link2=Paul Zimmermann (mathematician) |title=Gal's Accurate Tables Method Revisited |url=http://hal.inria.fr/inria-00070644/PDF/RR-5359.pdf |pages=257&ndash;264 |journal=[[17th IEEE Symposium on Computer Arithmetic]] ([[ARITH'05]]) |date=2005 |access-date=2018-01-15 |dead-url=no |archive-url=https://web.archive.org/web/20180115191657/https://hal.inria.fr/inria-00070644/PDF/RR-5359.pdf |archive-date=2018-01-15}}\n\n[[Category:Numerical analysis]]\n[[Category:Computer arithmetic]]\n[[Category:Interpolation]]"
    },
    {
      "title": "Galerkin method",
      "url": "https://en.wikipedia.org/wiki/Galerkin_method",
      "text": "{{technical|date=March 2014}}\n\nIn [[mathematics]], in the area of [[numerical analysis]], '''Galerkin methods''' are a class of methods for converting a continuous operator problem (such as a [[differential equation]]) to a discrete problem. In principle, it is the equivalent of applying the method of [[variation of parameters]] to a function space, by converting the equation to a [[weak formulation]].  Typically one then applies some constraints on the function space to characterize the space with a finite set of basis functions.\n\nThe approach is usually credited to [[Boris Galerkin]] but the method was discovered by [[Walther Ritz]],<ref>\"Le destin douloureux de Walther Ritz (1878-1909)\", (Jean-Claude Pont, editor), Cahiers de Vallesia, 24, (2012), {{ISBN|978-2-9700636-5-0}}</ref> to whom Galerkin refers. Often when referring to a Galerkin method, one also gives the name along with typical approximation methods used, such as Bubnov–Galerkin method (after [[Ivan Bubnov]]), [[Petrov–Galerkin method]] (after Georgii I. Petrov<ref name=Mikhlin>S. G. Mikhlin, \"Variational methods in Mathematical Physics\", Pergamon Press, 1964</ref><ref name=\"Birthday\">\"Georgii Ivanovich Petrov (on his 100th birthday)\", Fluid Dynamics, May 2012, Volume 47, Issue 3, pp 289-291, DOI 10.1134/S0015462812030015</ref>) or [[Ritz–Galerkin method]]<ref name=ErnGuermond>A. Ern, J.L. Guermond, ''Theory and practice of finite elements'', Springer, 2004, {{ISBN|0-387-20574-8}}</ref> (after [[Walther Ritz]]).\n\nExamples of Galerkin methods are:\n* the [[Method of mean weighted residuals|Galerkin method of weighted residuals]], the most common method of calculating the global [[stiffness matrix]] in the [[finite element method]],<ref name=BrennerScott>S. Brenner, R. L. Scott, ''The Mathematical Theory of Finite Element Methods'', 2nd edition, Springer, 2005, {{ISBN|0-387-95451-1}}</ref><ref name=Ciarlet>P. G. Ciarlet, ''The Finite Element Method for Elliptic Problems'', North-Holland, 1978, {{ISBN|0-444-85028-7}}</ref>\n* the [[boundary element method]] for solving integral equations,\n* [[Krylov subspace method]]s.<ref name=Saad>[[Yousef Saad|Y. Saad]], ''Iterative Methods for Sparse Linear Systems'', 2nd edition, SIAM, 2003, {{ISBN|0-89871-534-2}}</ref>\n\n==Introduction with an abstract problem==\n\n===A problem in weak formulation===\nLet us introduce Galerkin's method with an abstract problem posed as a [[weak formulation]] on a [[Hilbert space]] <math>V</math>, namely,\n: find <math>u\\in V</math> such that for all <math>v\\in V, a(u,v) = f(v)</math>.\n\nHere, <math>a(\\cdot,\\cdot)</math> is a [[bilinear form]] (the exact requirements on <math>a(\\cdot,\\cdot)</math> will be specified later) and <math>f</math> is a bounded linear functional on <math>V</math>.\n\n===Galerkin dimension reduction===\nChoose a subspace <math>V_n \\subset V</math> of dimension ''n'' and solve the projected problem:\n: Find <math>u_n\\in V_n</math> such that for all <math>v_n\\in V_n, a(u_n,v_n) = f(v_n)</math>.\n\nWe call this the '''Galerkin equation'''. Notice that the equation has remained unchanged and only the spaces have changed.\nReducing the problem to a finite-dimensional vector subspace allows us to numerically compute <math> u_n </math> as a finite linear combination of the basis vectors in <math> V_n </math>.\n\n===Galerkin orthogonality===\nThe key property of the Galerkin approach is that the error is orthogonal to the chosen subspaces. Since  <math>V_n \\subset V</math>, we can use <math>v_n</math> as a test vector in the original equation. Subtracting the two, we get the Galerkin orthogonality relation for the error, <math>\\epsilon_n = u-u_n</math> which is the error between the solution of the original problem, <math>u</math>, and the solution of the Galerkin equation, <math>u_n</math>\n\n:<math> a(\\epsilon_n, v_n) = a(u,v_n) - a(u_n, v_n) = f(v_n) - f(v_n) = 0.</math>\n\n===Matrix form===\nSince the aim of Galerkin's method is the production of a [[system of linear equations|linear system of equations]], we build its matrix form, which can be used to compute the solution algorithmically.\n\nLet <math>e_1, e_2,\\ldots,e_n</math> be a [[basis (linear algebra)|basis]] for <math>V_n</math>. Then, it is sufficient to use these in turn for testing the Galerkin equation, i.e.: find <math>u_n \\in V_n</math> such that\n\n:<math>a(u_n, e_i) = f(e_i) \\quad i=1,\\ldots,n.</math>\n\nWe expand <math>u_n</math> with respect to this basis, <math>u_n = \\sum_{j=1}^n u_je_j</math> and insert it into the equation above, to obtain\n\n:<math>a\\left(\\sum_{j=1}^n u_je_j, e_i\\right) = \\sum_{j=1}^n u_j a(e_j, e_i) = f(e_i) \\quad i=1,\\ldots,n.</math>\n\nThis previous equation is actually a linear system of equations <math>Au=f</math>, where\n\n:<math>A_{ij} = a(e_j, e_i), \\quad f_i = f(e_i).</math>\n\n====Symmetry of the matrix====\nDue to the definition of the matrix entries, the matrix of the Galerkin equation is [[symmetric matrix|symmetric]] if and only if the bilinear form <math>a(\\cdot,\\cdot)</math> is symmetric.\n\n==Analysis of Galerkin methods==\nHere, we will restrict ourselves to symmetric [[bilinear form]]s, that is\n\n:<math>a(u,v) = a(v,u).</math>\n\nWhile this is not really a restriction of Galerkin methods, the application of the standard theory becomes much simpler. Furthermore, a [[Petrov–Galerkin method]] may be required in the nonsymmetric case.\n\nThe analysis of these methods proceeds in two steps. First, we will show that the Galerkin equation is a [[well-posed problem]] in the sense of [[Hadamard]] and therefore admits a unique solution. In the second step, we study the quality of approximation of the Galerkin solution <math>u_n</math>.\n\nThe analysis will mostly rest on two properties of the [[bilinear form]], namely\n* Boundedness: for all <math>u,v\\in V</math> holds\n*:<math>a(u,v) \\le C \\|u\\|\\, \\|v\\|</math> for some constant <math>C>0</math>\n* Ellipticity: for all <math>u\\in V</math> holds\n*:<math>a(u,u) \\ge c \\|u\\|^2</math> for some constant <math>c>0.</math>\nBy the Lax-Milgram theorem (see [[weak formulation]]), these two conditions imply well-posedness of the original problem in weak formulation. All norms in the following sections will be norms for which the above inequalities hold (these norms are often called an energy norm).\n\n===Well-posedness of the Galerkin equation===\nSince <math>V_n \\subset V</math>, boundedness and ellipticity of the bilinear form apply to <math>V_n</math>. Therefore, the well-posedness of the Galerkin problem is actually inherited from the well-posedness of the original problem.\n\n===Quasi-best approximation (Céa's lemma)===\n{{main|Céa's lemma}}\nThe error <math>u-u_n</math> between the original and the Galerkin solution admits the estimate\n\n:<math>\\|u-u_n\\| \\le \\frac{C}{c} \\inf_{v_n\\in V_n} \\|u-v_n\\|.</math>\n\nThis means, that up to the constant <math>C/c</math>, the Galerkin solution <math>u_n</math>\nis as close to the original solution <math>u</math> as any other vector in <math>V_n</math>. In particular, it will be sufficient to study approximation by spaces <math>V_n</math>, completely forgetting about the equation being solved.\n\n====Proof====\nSince the proof is very simple and the basic principle behind all Galerkin methods, we include it here:\nby ellipticity and boundedness of the bilinear form (inequalities) and Galerkin orthogonality (equals sign in the middle), we have for arbitrary <math>v_n\\in V_n</math>:\n\n:<math>c\\|u-u_n\\|^2 \\le a(u-u_n, u-u_n) = a(u-u_n, u-v_n) \\le C \\|u-u_n\\| \\, \\|u-v_n\\|.</math>\n\nDividing by <math>c \\|u-u_n\\|</math> and taking the infimum over all possible <math>v_n</math> yields the lemma.\n\n== See also ==\n* [[Ritz method]]\n\n==References==\n<!--\n<ref name=ErnGuermond> A. Ern, J.L. Guermond, ''Theory and practice of finite elements'', Springer, 2004, ISBN 0-387-20574-8 </ref>\n<ref name=BrennerScott> S. Brenner, R. L. Scott, ''The Mathematical Theory of Finite Element Methods'', 2nd edition, Springer, 2005, ISBN 0-387-95451-1 </ref>\n<ref name=Ciarlet> P. G. Ciarlet, ''The Finite Element Method for Elliptic Problems'', North-Holland, 1978, ISBN 0-444-85028-7 </ref>\n<ref name=Saad> Y. Saad, ''Iterative Methods for Sparse Linear Systems'', 2nd edition, SIAM, 2003, ISBN 0-89871-534-2 </ref>\n-->\n<references />\n\n== External links ==\n* {{Springer |title=Galerkin method |id=p/g043040}}\n* [http://mathworld.wolfram.com/GalerkinMethod.html Galerkin Method from MathWorld]\n\n{{Numerical PDE}}\n\n{{Authority control}}\n\n{{DEFAULTSORT:Galerkin Method}}\n[[Category:Numerical analysis]]\n[[Category:Numerical differential equations]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "Generalized Gauss–Newton method",
      "url": "https://en.wikipedia.org/wiki/Generalized_Gauss%E2%80%93Newton_method",
      "text": "{{Refimprove|date=January 2017}}\nThe '''generalized Gauss–Newton method''' is a generalization of the least-squares method originally described by [[Carl Friedrich Gauss]] and of [[Newton's method]] due to [[Isaac Newton]] to the case of constrained nonlinear least-squares problems.<ref>{{citation\n | last1 = Golub | first1 = G. H.\n | last2 = Pereyra | first2 = V.\n | journal = SIAM Journal on Numerical Analysis\n | mr = 0336980\n | pages = 413–432\n | title = The differentiation of pseudo-inverses and nonlinear least squares problems whose variables separate\n | volume = 10\n | year = 1973}}.</ref>\n\n==References==\n{{reflist}}\n\n{{Isaac Newton}}\n\n{{DEFAULTSORT:Generalized Gauss-Newton method}}\n[[Category:Numerical analysis]]\n\n\n{{applied-math-stub}}"
    },
    {
      "title": "Generalized-strain mesh-free formulation",
      "url": "https://en.wikipedia.org/wiki/Generalized-strain_mesh-free_formulation",
      "text": "The '''generalized-strain mesh-free''' ('''GSMF''') '''formulation''' is a local meshfree method in the field of [[numerical analysis]], completely integration free, working as a weighted-residual weak-form collocation. This method was first presented by Oliveira and Portela (2016),<ref>Oliveira, T. and A. Portela (2016). \"Weak-Form Collocation – a Local Meshless Method in Linear Elasticity\". ''Engineering Analysis with Boundary Elements''.</ref> in order to further improve the computational efficiency of [[meshfree methods]] in numerical analysis. Local meshfree methods are derived through a weighted-residual formulation which leads to a local weak form that is the well known [[work theorem]] of the theory of structures. In an arbitrary local region, the work theorem establishes an energy relationship between a statically-admissible stress field and an independent kinematically-admissible strain field. Based on the independence of these two fields, this formulation results in a local form of the work theorem that is reduced to regular boundary terms only, integration-free and free of [[volumetric locking]].\n\nAdvantages over [[finite element methods]] are that GSMF doesn't rely on a grid, and is more precise and faster when solving bi-dimensional problems. When compared to other meshless methods, such as [[rigid-body displacement mesh-free]] (RBDMF) formulation, the [[element-free Galerkin]] (EFG)<ref>Belytschko, T., Y. Y. Lu, and L. Gu (1994). \"Element-free Galerkin methods\". ''International Journal for Numerical Methods in Engineering''. 37.2, pp. 229–256.</ref> and the [[meshless local Petrov-Galerkin]] [[finite volume method]] (MLPG FVM);<ref>Atluri, S.N., Z.D. Han, and A.M. Rajendran (2004). \"A New Implementation of the Meshless Finite Volume Method Through the MLPG Mixed Approach\". ''CMES: Computer Modeling in Engineering and Sciences''. 6, pp. 491–513.</ref> GSMF proved to be superior not only regarding the computational efficiency, but also regarding the accuracy.<ref>Oliveira, T. and A. Portela (2016). \"Comparative study of the weak-form collocation meshless formulation and other meshless methods\". ''Proceedings of the XXXVII Iberian Latin-American Congress on Computational Methods in Engineering''. ABMEC, Brazil</ref>\n\nThe [[moving least squares]] (MLS) approximation of the elastic field is used on this local meshless formulation.\n\n==Formulation==\nIn the [[local form of the work theorem]], equation:\n\n: <math>\\int_{\\Gamma_Q} \\mathbf{t}^T \\mathbf{u}^{*} d\\Gamma + \\int_{\\Omega_Q} \\mathbf{b}^{T} \\mathbf{u}^{*} d\\Omega = \\int_{\\Omega_Q} \\boldsymbol{\\sigma}^T \\boldsymbol{\\varepsilon}^{*} d\\Omega.</math>\n\nThe displacement field <math>\\mathbf{u}^{*}</math>, was assumed as a continuous function leading to a regular integrable function that is the kinematically-admissible strain field <math>\\boldsymbol{\\varepsilon}^{*}</math>. However, this continuity assumption on <math>\\mathbf{u}^{*}</math>, enforced in the local form of the work theorem, is not absolutely required but can be relaxed by convenience, provided <math>\\boldsymbol{\\varepsilon}^{*}</math> can be useful as a generalized function, in the sense of the theory of distributions, see Gelfand and Shilov.<ref>Gelfand, I.M., Shilov, G.E. (1964). Generalized Functions. Volume I, Academic Press, New York.</ref> Hence, this formulation considers that the displacement field <math>\\mathbf{u}^{*}</math>, is a piecewise continuous function, defined in terms of the Heaviside step function and therefore the corresponding strain field <math>\\boldsymbol{\\varepsilon}^{*}</math>, is a generalized function defined in terms of the [[Dirac delta function]].\n\n\nFor the sake of the simplicity, in dealing with Heaviside and Dirac delta functions in a two-dimensional coordinate space, consider a scalar function <math>d</math>, defined as:\n\n: <math>d = \\lVert\\ \\mathbf{x}-\\mathbf{x}_Q \\rVert</math>\n\nwhich represents the absolute-value function of the distance between a field point <math>\\mathbf{x}</math> and a particular reference point <math>\\mathbf{x}_Q</math>, in the local domain <math>\\Omega_Q \\cup \\Gamma_Q</math> assigned to the field node <math>Q</math>. Therefore, this definition always assumes <math>d=d(\\mathbf{x},\\mathbf{x}_Q) \\geq 0</math>, as a positive or null value, in this case whenever <math>\\mathbf{x}</math> and <math>\\mathbf{x}_Q</math> are coincident points.\n\n\nFor a scalar coordinate <math>d\\supset d(\\mathbf{x},\\mathbf{x}_Q)</math>, the [[Heaviside step function]] can be defined as\n \n: <math>H(d) = 1 \\,\\,\\,\\,\\,\\, if \\,\\,\\,\\,\\, d\\leq 0 \\,\\,\\,\\,\\,\\, (d=0 \\,\\,\\, for \\,\\,\\, \\mathbf{x} \\equiv \\mathbf{x}_Q)</math>\n: <math>H(d) = 0 \\,\\,\\,\\,\\,\\, if \\,\\,\\,\\,\\, d > 0 \\,\\,\\,\\,\\,\\, (\\mathbf{x} \\neq \\mathbf{x}_Q)</math>\n\nin which the discontinuity is assumed at <math>\\mathbf{x}_Q</math> and consequently, the [[Dirac delta function]] is defined with the following properties\n\n: <math>\\delta(d) = H'(d) = \\infty \\,\\,\\,\\,\\,\\, if \\,\\,\\,\\,\\, d=0 \\,\\,\\, that \\,\\, is \\,\\,\\, \\mathbf{x} \\equiv \\mathbf{x}_Q</math>\n: <math>\\delta(d) = H'(d) = 0 \\,\\,\\,\\,\\,\\, if \\,\\,\\,\\,\\, d\\neq 0 \\,\\,\\, (d>0 \\,\\,\\, for \\,\\,\\, \\mathbf{x} \\neq \\mathbf{x}_Q)</math>\n\nand\n\n: <math>\\int\\limits_{-\\infty}^{+\\infty} \\delta(d)\\,d d=1</math>\n\nin which <math>H'(d)</math> represents the [[distributional derivative]] of <math>H(d)</math>. Note that the derivative of <math>H(d)</math>, with respect to the coordinate <math>x_i</math>, can be defined as\n\n: <math>H(d)_{,i}=H'(d) \\,\\, d_{,i}= \\delta(d) \\,\\, d_{,i}=\\delta(d) \\,\\, n_i</math>\n\nSince the result of this equation is not affected by any particular value of the constant <math>n_i</math>, this constant will be conveniently redefined later on.\n\n\nConsider that <math>d_l</math>, <math>d_j</math> and <math>d_k</math> represent the distance function <math>d</math>, for corresponding collocation points <math>\\mathbf{x}_l</math>, <math>\\mathbf{x}_j</math> and <math>\\mathbf{x}_k</math>. The displacement field <math>\\mathbf{u}^{*}(\\mathbf{x})</math>, can be conveniently defined as\n\n: <math>\\mathbf{u}^{*}(\\mathbf{x}) = \\Bigg[\\frac{L_{i}}{n_i}\\,\\sum_{l=1}^{n_i} H(d_l)+\\frac{L_{t}}{n_t}\\,\\sum_{j=1}^{n_t} H(d_j) +\\frac{S}{n_\\Omega}\\,\\sum_{k=1}^{n_\\Omega} H(d_k)\\Bigg] \\mathbf{e}</math>\n\nin which <math>\\mathbf{e}=[1\\,\\,\\,\\, 1]^T</math> represents the metric of the orthogonal directions and <math>n_i</math>, <math>n_t</math> and <math>n_\\Omega</math> represent the number of collocation points, respectively on the local interior boundary <math>\\Gamma_{Qi}=\\Gamma_Q-\\Gamma_{Qt}-\\Gamma_{Qu}</math> with length <math>L_i</math>, on the local static boundary <math>\\Gamma_{Qt}</math> with length <math>L_t</math> and in the local domain <math>\\Omega_Q</math> with area <math>S</math>. This assumed displacement field <math>\\mathbf{u}^{*}(\\mathbf{x})</math>, a discrete rigid-body unit displacement defined at collocation points. The strain field <math>\\boldsymbol{\\varepsilon}^{*}(\\mathbf{x})</math>, is given by\n\n: <math>\\boldsymbol{\\varepsilon}^{*}(\\mathbf{x})=\\mathbf{L}\\,\\mathbf{u}^{*}(\\mathbf{x})= \\Bigg[\\frac{L_{i}}{n_i}\\,\\sum_{l=1}^{n_i} \\mathbf{L}\\,H(d_l)+\\frac{L_{t}}{n_t}\\,\\sum_{j=1}^{n_t} \\mathbf{L}\\,H(d_j) +\\frac{S}{n_\\Omega}\\,\\sum_{k=1}^{n_\\Omega} \\mathbf{L}\\,H(d_k)\\Bigg] \\mathbf{e}\n=\\Bigg[\\frac{L_{i}}{n_i}\\,\\sum_{l=1}^{n_i}\\,\\delta(d_l)\\,\\mathbf{n}^{T}\\,+\\frac{L_{t}}{n_t}\\,\\sum_{j=1}^{n_t} \\,\\delta(d_j)\\,\\mathbf{n}^{T}\\, +\\frac{S}{n_\\Omega}\\,\\sum_{k=1}^{n_\\Omega} \\,\\delta(d_k)\\,\\mathbf{n}^{T}\\Bigg] \\mathbf{e}</math>\n\nHaving defined the displacement and the strain components of the kinematically-admissible field, the local work theorem can be written as\n\n: <math>\\frac{L_{i}}{n_i}\\sum_{l=1}^{n_i}\\,\\int\\limits_{\\Gamma_Q-\\Gamma_{Qt}}\\!\\!\\!\\!\\!\\!\\mathbf{t}^{T} H(d_l)\\mathbf{e}\\,d\\Gamma +\n  \\frac{L_{t}}{n_t}\\sum_{j=1}^{n_t}\\,\\int\\limits_{\\Gamma_{Qt}}\\!\\overline{\\mathbf{t}}^{T} H(d_j)\\mathbf{e}\\,d\\Gamma +\n  \\frac{S}{n_\\Omega}\\sum_{k=1}^{n_\\Omega}\\,\\int\\limits_{\\Omega_Q}\\mathbf{b}^{T} H(d_k)\\mathbf{e}\\,d\\Omega\n  =\\frac{S}{n_\\Omega}\\sum_{k=1}^{n_\\Omega}\\,\\int\\limits_{\\Omega_Q}\\boldsymbol{\\sigma}^{T}\\delta(d_k)\\,\\mathbf{n}^{T}\\mathbf{e}\\,d\\Omega.</math>\n\nTaking into account the properties of the [[Heaviside step function]] and [[Dirac delta function]], this equation simply leads to\n\n: <math>\\frac{L_{i}}{n_i}\\sum_{l=1}^{n_i}\\,\\mathbf{t}_{\\mathbf{x}_l} = -\\,\\frac{L_{t}}{n_t}\\sum_{j=1}^{n_t}\\,\\overline{\\mathbf{t}}_{\\mathbf{x}_j} -\\,\\frac{S}{n_\\Omega}\\sum_{k=1}^{n_\\Omega}\\,\\mathbf{b}_{\\mathbf{x}_k}</math>\n\nDiscretization of this equations  can be carried out with the MLS approximation, for the local domain <math>\\Omega_Q</math>, in terms of the nodal unknowns <math>\\hat{\\mathbf{u}}</math>, thus leading to the system of linear algebraic equations that can be written as\n\n: <math>\\frac{L_{i}}{n_i}\\sum_{l=1}^{n_i}\\,\\mathbf{n}_{\\mathbf{x}_l}\\mathbf{D}\\mathbf{B}_{\\mathbf{x}_l}\\hat{\\mathbf{u}} =-\\,\\frac{L_{t}}{n_t}\\sum_{j=1}^{n_t}\\,\\overline{\\mathbf{t}}_{\\mathbf{x}_j}-\\,\\frac{S}{n_\\Omega}\\sum_{k=1}^{n_\\Omega}\\,\\mathbf{b}_{\\mathbf{x}_k}</math>\n\nor simply\n\n: <math>\\mathbf{K}_Q\\,\\hat{\\mathbf{u}}=\\mathbf{F}_Q</math>\n\n\nThis formulation states the equilibrium of tractions and body forces, pointwisely defined at collocation points, obviously, it is the pointwise version of the [[Euler-Cauchy stress principle]]. This is the equation used in the '''Generalized-Strain Mesh-Free (GSMF) formulation''' which, therefore, is free of integration. Since the [[work theorem]] is a weighted-residual weak form, it can be easily seen that this integration-free formulation is nothing else other than a weighted-residual weak-form collocation. The weighted-residual weak-form collocation readily overcomes the well-known difficulties posed by the weighted-residual strong-form collocation,<ref>Kansa, E.J.,(1990) \"Multiquadrics: A Scattered Data Approximation Scheme with Applications to Computational Fluid Dynamics\", ''Computers and Mathematics with Applications'', 19(8-9), 127--145.</ref> regarding accuracy and stability of the solution.\n\n== See also ==\n* [[Moving least squares]]\n* [[Finite element method]]\n* [[Boundary element method]]\n* [[Meshfree methods]]\n* [[Numerical analysis]]\n* Computational [[Solid Mechanics]]\n\n==References==\n{{reflist}}\n\n[[Category:Numerical analysis]]"
    },
    {
      "title": "GetFEM++",
      "url": "https://en.wikipedia.org/wiki/GetFEM%2B%2B",
      "text": "{{multiple issues|\n{{more citations needed|date=February 2014}}\n{{notability|Products|date=February 2014}}\n}}\n\n{{Infobox software\n| name                   = GetFEM++\n| title                  =\n| logo                   = <!-- [[File: ]] -->\n| logo caption           =\n| screenshot             = Crossed tubes in contact calculation with GetFEM++.png\n| screenshot size        = 250px\n| caption                = Contact simulation with GetFEM++\n| collapsible            =\n| author                 = Yves Renard, Julien Pommier\n| developer              =\n| released               = <!-- {{Start date|YYYY|MM|DD|df=yes/no}} -->\n| discontinued           =\n| latest release version = 5.3\n| latest release date    = {{Start date and age|2018|06|df=yes/no}}\n| latest preview version =\n| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| repo                   = {{URL|https://git.savannah.nongnu.org/git/getfem.git}}\n| programming language   = [[C++]]\n| operating system       = [[Unix]], [[Microsoft Windows|Windows]], [[Mac OS X]]\n| platform               =\n| size                   =\n| language               =\n| language count         = <!-- DO NOT include this parameter unless you know what it does -->\n| language footnote      =\n| status                 =\n| genre                  = Finite element library\n| license                = [[GNU Lesser General Public License]]\n| alexa                  =\n| website                = {{URL|http://getfem.org}}\n}}\n'''GetFEM++''' is a generic finite element [[C++]] library with interfaces for [[Python (programming language)|Python]], [[Matlab]] and [[Scilab]]. It aims at providing finite element methods and elementary matrix computations for solving linear and non-linear problems numerically. Its flexibility in choosing among different finite element approximations and numerical integration methods is one of its distinguishing characteristics.<ref>[http://getfem.org/userdoc/index.html GetFEM++ user documentation]</ref>\n\n== License ==\n\nGetFEM++ is released under the [[GNU Lesser General Public License]], version 3 or later, along with the GCC Runtime Library Exception, version 3.1 or later.\n\n== Components ==\n\nGmm++ is a generic matrix template library included in GetFEM++, providing tools for elementary computations with dense and sparse matrices. Among the capabilities implemented in Gmm++ there is also an interface to the popular direct solver for sparse systems of linear equations [[MUMPS (software)|MUMPS]].\n\nMesh objects in GetFEM++ contain information about the geometric transformation and connectivity of mesh elements as well as methods for accessing user defined mesh regions.\n\nFinite element methods can be defined per mesh or per element and they include a wide range of options like classical Lagrange elements  P<sub>k</sub> and Q<sub>k</sub> of arbitrary dimension and degree k, Hermite and Argyris elements, discontinuous P<sub>k</sub> and Q<sub>k</sub> elements, vectorial and hierarchical basis elements as well as [[Extended finite element method|XFEM]] elements.\n\nIntegration methods can be defined per mesh or per element as well. The possible options include exact and approximated methods.\n\nCombining so called brick objects, is the standard way of representing mathematical equations in GetFEM++. Such predefined bricks corresponding to common equation terms like elasticity, Helmholtz, Dirichlet condition and Neumann source terms are included in the software. Among other there is also a number of elaborated bricks related to contact mechanics, corresponding to different formulations of contact with or without friction.\n\nThe assembling procedures included in GetFEM++ aim at efficiently calculating the contribution of each brick to the global tangent matrix and right hand side term of the linearized system of equations.\n\n== Input/Output ==\nGetFEM++ can read meshes provided in the native formats of software like [[Gmsh]], GiD and [[Ansys]]. It can export results in the legacy POS file format of [[Gmsh]], the [[IBM OpenDX|OpenDX]] file format and the legacy [[VTK]] file format.\n\n== Awards ==\n\nIn 2007, GetFEM++ received the second prize in the category of scientific software in the [[Les Trophées du Libre]] contest.<ref>https://web.archive.org/web/20071212224326/http://www.tropheesdulibre.org/Getfem.html?lang=en</ref>\n\n== Literature ==\n[https://link.springer.com/chapter/10.1007%2F978-3-642-15291-7_9 CFD Parallel Simulation Using Getfem++ and Mumps]\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* {{official website|http://getfem.org}}\n\n{{Numerical analysis software}}\n\n[[Category:Free simulation software]]\n[[Category:Scientific simulation software]]\n[[Category:Finite element software| List of finite element software packages]]\n[[Category:Numerical analysis]]\n[[Category:Finite element software for Linux]]\n\n\n{{free-software-stub}}\n{{science-software-stub}}"
    },
    {
      "title": "Gradient discretisation method",
      "url": "https://en.wikipedia.org/wiki/Gradient_discretisation_method",
      "text": "<!-- {{more footnotes|date=March 2017}} answer and improvement completed by Cyclotourist -->\n[[Image:Plaplacien4.svg|thumb|right|400px|Exact solution <br/> <math>\\overline{u}(x) = \\frac 3 4 \\big({0.5}^{4/3}- |x - 0.5|^{4/3}\\big)</math>  <br/>\nof the ''p''-Laplace problem <math>-( |\\overline{u}'|^2 \\overline{u}')'(x) = 1</math> on the domain [0,1] with <math>\\overline{u}(0) = \\overline{u}(1) = 0</math> \n(black line) and approximate one (blue line) computed with the first degree discontinuous Galerkin method plugged into the GDM (uniform mesh with 6 elements).]]\n{{Differential equations}}\nIn numerical mathematics, the '''gradient discretisation method''' ('''GDM''') is a framework which contains classical and recent numerical schemes for diffusion problems of various kinds: linear or non-linear, steady-state or time-dependent.  The schemes may be conforming or non-conforming, and may rely on very general polygonal or polyhedral meshes (or may even be meshless).\n\nSome core properties are required to prove the convergence of a GDM. These core properties enable complete proofs of convergence of the GDM for elliptic and parabolic problems, linear or non-linear. For linear problems, stationary or transient, error estimates can be established based on three indicators specific to the GDM <ref>'''R. Eymard, C. Guichard, and R. Herbin.''' Small-stencil 3d schemes for diffusive flows in porous media. M2AN, 46:265–290, 2012.</ref> (the quantities <math>C_{D}</math>, <math>S_{D}</math> and <math>W_{D}</math>, [[#The example of a linear diffusion problem|see below]]). For non-linear problems, the proofs are based on compactness techniques and do not require any non-physical strong regularity assumption on the solution or the model data.<ref name=droniou>'''J. Droniou, R. Eymard, T. Gallouët, and R. Herbin.''' Gradient schemes: a generic framework for the discretisation of linear, nonlinear and nonlocal elliptic and parabolic equations. Math. Models Methods Appl. Sci. (M3AS), 23(13):2395–2432, 2013.</ref> [[#Some non-linear problems with complete convergence proofs of the GDM|Non-linear models]] for which such convergence proof of the GDM have been carried out comprise: the [[Stefan problem]] which is modelling a melting material, two-phase flows in porous media, the [[Richards equation]] of underground water flow, the fully non-linear Leray—Lions equations.<ref>'''J. Leray and J. Lions.''' Quelques résultats de Višik sur les problèmes elliptiques non linéaires par les méthodes de Minty-Browder. Bull. Soc. Math. France, 93:97–107, 1965.</ref>\n\nAny scheme entering the GDM framework is then known to converge on all these problems. This applies in particular to [[#Galerkin methods and conforming finite element methods|conforming Finite Elements]], [[#Mixed finite element|Mixed Finite Elements]], [[#Nonconforming finite element|nonconforming Finite Elements]], and, in the case of more recent schemes, the [[#Discontinuous Galerkin method|Discontinuous Galerkin method]], [[#Mimetic finite difference method and nodal mimetic finite difference method|Hybrid Mixed Mimetic method, the Nodal Mimetic Finite Difference method]], some Discrete Duality Finite Volume schemes, and some Multi-Point Flux Approximation schemes\n\n==The example of a linear diffusion problem==\n\nConsider [[Poisson's equation]] in a bounded open domain <math>\\Omega\\subset \\mathbb{R}^d</math>, with homogeneous [[Dirichlet boundary condition]]\n\n:<math>\\quad (1) \\qquad  \\qquad -\\Delta \\overline{u} = f,</math>\n\nwhere <math>f\\in L^2(\\Omega)</math>. The usual sense of weak solution <ref>'''H. Brezis.''' Functional analysis, Sobolev spaces and partial differential equations. Universitext. Springer, New York, 2011.</ref> to this model is:\n\n:<math>\\quad (2) \\qquad\\mbox{Find }\\overline{u}\\in H^1_0(\\Omega)\\mbox{ such that, for all } \\overline{v} \\in H^1_0(\\Omega),\\quad  \\int_{\\Omega} \\nabla \\overline{u}(x)\\cdot\\nabla \\overline{v}(x) dx = \\int_{\\Omega} f(x)\\overline{v}(x) dx. </math>\n\nIn a nutshell, the GDM for such a model consists in selecting a finite-dimensional space and two reconstruction operators (one for the functions, one for the gradients) and to substitute these discrete elements in lieu of the continuous elements in (2). More precisely, the GDM starts by defining a Gradient Discretization (GD), which is a triplet <math>D = (X_{D,0},\\Pi_D,\\nabla_D)</math>, where:\n\n* the set of discrete unknowns <math>X_{D,0}</math> is a finite dimensional real vector space,\n* the function reconstruction <math>\\Pi_D~:~X_{D,0}\\to L^2(\\Omega)</math> is a linear mapping that reconstructs, from an element of <math>X_{D,0}</math>, a function over <math>\\Omega</math>,\n* the gradient reconstruction <math>\\nabla_D~:~X_{D,0}\\to L^2(\\Omega)^d</math> is a linear mapping which reconstructs, from an element of <math>X_{D,0}</math>, a \"gradient\" (vector-valued function) over <math>\\Omega</math>. This gradient reconstruction must be chosen such that <math>\\Vert \\nabla_D \\cdot \\Vert_{L^2(\\Omega)^d}</math> is a norm on <math>X_{D,0}</math>.\n\nThe related Gradient Scheme for the approximation of (2) is given by: find <math>u\\in X_{D,0}</math> such that\n\n:<math>\\quad (3) \\qquad  \\qquad \\forall v \\in X_{D,0},\\qquad  \\int_{\\Omega} \\nabla_D u(x)\\cdot\\nabla_D v(x) dx = \\int_{\\Omega} f(x)\\Pi_D v(x) dx. </math>\n\nThe GDM is then in this case a nonconforming method for the approximation of (2), which includes the nonconforming finite element method. Note that the reciprocal is not true, in the sense that the GDM framework includes methods such that the function <math>\\nabla_D u</math> cannot be computed from the function <math>\\Pi_D u</math>.\n\nThe following error estimate, inspired by G. Strang's second lemma,<ref>'''G. Strang.''' Variational crimes in the finite element method.'' In The mathematical foundations of the finite element method with applications to partial differential equations (Proc. Sympos., Univ. Maryland, Baltimore, Md., 1972)'', pages 689–710. Academic Press, New York, 1972.</ref> holds\n\n:<math>\\quad (4) \\qquad  \\qquad  W_D(\\nabla \\overline{u}) \\le \\Vert \\nabla \\overline{u}  - \\nabla_D u_D\\Vert_{L^2(\\Omega)^d}\n\\le  W_D(\\nabla \\overline{u}) + 2 S_D(\\overline{u}), </math>\n\nand\n\n:<math>\\quad (5) \\qquad  \\qquad  \\Vert \\overline{u}  - \\Pi_D u_D\\Vert_{L^2(\\Omega)}\n\\le C_D W_D(\\nabla \\overline{u}) + (C_D+1)S_D(\\overline{u}), </math>\n\ndefining:\n\n:<math>\\quad (6) \\qquad  \\qquad C_D =  \\max_{v\\in X_{D,0}\\setminus\\{0\\}}\\frac {\\Vert \\Pi_D v\\Vert_{L^2(\\Omega)}} {\\Vert \\nabla_D v \\Vert_{L^2(\\Omega)^d}}, </math>\nwhich measures the coercivity (discrete Poincaré constant),\n\n:<math>\\quad (7) \\qquad  \\qquad \n\\forall \\varphi\\in H^1_0(\\Omega),\\,\n S_{D}(\\varphi) = \\min_{v\\in X_{D,0}}\\left(\\Vert\\Pi_D v - \\varphi\\Vert_{L^2(\\Omega)} + \\Vert\\nabla_D v -\\nabla\\varphi\\Vert_{L^2(\\Omega)^d}\\right), </math>\nwhich measures the interpolation error,\n\n:<math>\\quad (8) \\qquad  \\qquad \n\\forall \\varphi\\in H_\\operatorname{div}(\\Omega),\\,\n W_D(\\varphi) = \\max_{v\\in X_{D,0}\\setminus\\{0\\}}\\frac{\n\\left|\\int_\\Omega \\left(\\nabla_D v(x)\\cdot\\varphi(x) + \\Pi_D v(x) \\operatorname{div}\\varphi(x)\\right) \\, dx \\right|}{\\Vert \\nabla_D v \\Vert_{L^2(\\Omega)^d}}, </math>\nwhich measures the defect of conformity.\n\nNote that the following upper and lower bounds of the approximation error can be derived:\n\n:<math>\\quad (9) \\qquad  \\qquad \\begin{align} &&\\frac 1 2 [S_D(\\overline{u}) + W_D(\\nabla \\overline{u})] \\\\  &\\le & \\Vert \\overline{u}  - \\Pi_D u_D\\Vert_{L^2(\\Omega)} + \\Vert \\nabla \\overline{u}  - \\nabla_D u_D\\Vert_{L^2(\\Omega)^d} \\\\ &\\le &(C_D+2) [S_D(\\overline{u}) + W_D(\\nabla \\overline{u})].\\end{align} </math>\n\nThen the core properties which are necessary and sufficient for the convergence of the method are, for a family of GDs, the coercivity, the GD-consistency and the limit-conformity properties, as defined in the next section. More generally, these three core properties are sufficient to prove the convergence of the GDM for linear problems and for some nonlinear problems like the <math>p</math>-Laplace problem. For nonlinear problems such as nonlinear diffusion, degenerate parabolic problems..., we add in the next section two other core properties which may be required.\n\n==The core properties allowing for the convergence of a GDM==\n\nLet <math>(D_m)_{m\\in\\mathbb{N}}</math> be a family of GDs, defined as above (generally associated with a sequence of regular meshes whose size tends to 0).\n\n=== Coercivity ===\nThe sequence <math>(C_{D_m})_{m\\in\\mathbb{N}}</math> (defined by (6)) remains bounded.\n\n=== GD-consistency ===\nFor all <math>\\varphi\\in H^1_0(\\Omega)</math>, <math>\\lim_{m\\to\\infty} S_{D_m} (\\varphi) = 0</math> (defined by (7)).\n\n=== Limit-conformity ===\nFor all <math>\\varphi\\in H_\\operatorname{div}(\\Omega)</math>, <math>\\lim_{m\\to\\infty} W_{D_m}(\\varphi) = 0</math> (defined by (8)).\nThis property implies the coercivity property.\n\n=== Compactness (needed for some nonlinear problems)===\nFor all sequence <math>(u_m)_{m\\in\\mathbb{N}}</math> such that <math>u_m \\in  X_{D_m,0} </math> for all  <math>m\\in\\mathbb{N}</math> and  <math>(\\Vert u_m \\Vert_{D_m})_{m\\in\\mathbb{N}}</math> is bounded, then the sequence <math>(\\Pi_{D_m} u_m)_{m\\in\\mathbb{N}}</math> is relatively compact in <math>L^2(\\Omega)</math> (this property implies the coercivity property).\n\n=== Piecewise constant reconstruction  (needed for some nonlinear problems)===\nLet  <math>D = (X_{D,0}, \\Pi_D,\\nabla_D)</math> be a gradient discretisation as defined above.\nThe operator <math>\\Pi_D</math> is a piecewise constant reconstruction if there exists a basis <math>(e_i)_{i\\in B}</math> of <math>X_{D,0}</math> and a family of disjoint subsets  <math>(\\Omega_i)_{i\\in B}</math> of <math>\\Omega</math> such that  <math>\\Pi_D u = \\sum_{i\\in B}u_i\\chi_{\\Omega_i}</math> for all <math>u=\\sum_{i\\in B} u_i e_i\\in X_{D,0}</math>, where <math>\\chi_{\\Omega_i}</math> is the characteristic function of <math>\\Omega_i</math>.\n\n==Some non-linear problems with complete convergence proofs of the GDM==\n\nWe review some problems for which the GDM can be proved to converge when the above core properties are satisfied.\n\n=== Nonlinear stationary diffusion problems ===\n\n:<math>\\quad  \\qquad  \\qquad -\\operatorname{div}(\\Lambda(\\overline{u})\\nabla \\overline{u}) = f</math>\n\nIn this case, the GDM converges under the coercivity, GD-consistency, limit-conformity and compactness properties.\n\n=== ''p''-Laplace problem for ''p'' > 1===\n\n:<math>\\quad  \\qquad  \\qquad -\\operatorname{div}(|\\nabla \\overline{u}|^{p-2}\\nabla \\overline{u}) = f</math>\n\nIn this case, the core properties must be written, replacing <math>L^2(\\Omega)</math>  by <math>L^p(\\Omega)</math>, <math>H^1_0(\\Omega)</math> by <math>W^{1,p}_0(\\Omega)</math>  and <math>H_\\operatorname{div}(\\Omega)</math> by <math>W_\\operatorname{div}^{p'}(\\Omega)</math> with <math>\\frac 1 p +\\frac 1 {p'}=1</math>, and the GDM converges only under the coercivity, GD-consistency and limit-conformity properties.\n\n=== Linear and nonlinear heat equation ===\n\n:<math>\\quad  \\qquad  \\qquad \\partial_t \\overline{u}- \\operatorname{div}(\\Lambda (\\overline{u}) \\nabla \\overline{u}) = f</math>\n\nIn this case, the GDM converges under the coercivity, GD-consistency (adapted to space-time problems), limit-conformity and compactness (for the nonlinear case) properties.\n\n=== Degenerate parabolic problems ===\n\nAssume that <math>\\beta</math> and <math>\\zeta</math> are nondecreasing Lipschitz continuous functions:\n\n:<math>\\quad  \\qquad  \\qquad \\partial_t \\beta(\\overline{u})-\\Delta \\zeta(\\overline{u}) = f</math>\n\nNote that, for this problem, the piecewise constant reconstruction property is needed, in addition to the coercivity, GD-consistency (adapted to space-time problems), limit-conformity and compactness properties.\n\n==Review of some numerical methods which are GDM==\n\nAll the methods below satisfy the first four core properties of GDM (coercivity, GD-consistency, limit-conformity, compactness), and in some cases the fifth one (piecewise constant reconstruction).\n\n===[[Galerkin method]]s and conforming finite element methods===\n\nLet <math>V_h\\subset H^1_0(\\Omega)</math> be spanned by the finite basis   <math>(\\psi_i)_{i\\in I}</math>. The [[Galerkin method]] in <math>V_h</math> is identical to the GDM where one defines\n\n*<math>X_{D,0} = \\{ u = (u_i)_{i\\in I} \\} = \\mathbb{R}^I,</math>\n*<math>\\Pi_D u = \\sum_{i\\in I} u_i \\psi_i</math>\n*<math>\\nabla_D u = \\sum_{i\\in I} u_i \\nabla\\psi_i.</math>\n\nIn this case, <math>C_D</math> is the constant involved in the continuous Poincaré inequality, and, for all <math>\\varphi\\in H_\\operatorname{div}(\\Omega)</math>, <math>W_{D}(\\varphi) = 0</math> (defined by (8)). Then (4) and (5) are implied by [[Céa's lemma]].\n\nThe \"mass-lumped\" <math>P^1</math> finite element case enters the framework of the GDM, replacing <math>\\Pi_D u</math> by  <math>\\widetilde{\\Pi}_D u = \\sum_{i\\in I} u_i \\chi_{\\Omega_i}</math>, where <math>\\Omega_i</math> is a dual cell centred on the vertex indexed by  <math>i\\in I</math>. Using mass lumping allows to get the piecewise constant reconstruction property.\n\n=== Nonconforming finite element ===\n\nOn a mesh <math>T</math> which is a conforming set of simplices of <math>\\mathbb{R}^d</math>, the nonconforming <math>P^1</math> finite elements are defined by the basis  <math>(\\psi_i)_{i\\in I}</math> of the functions which are affine in any  <math>K\\in T</math>, and whose value at the centre of gravity of one given face of the mesh is 1 and 0 at all the others  (these finite elements are used in [Crouzeix ''et al'']<ref>'''M. Crouzeix and P.-A. Raviart.''' Conforming and nonconforming finite element methods for solving the stationary Stokes equations. I. Rev. Française Automat. Informat. Recherche Opérationnelle Sér. Rouge, 7(R-3):33–75, 1973.</ref> for the approximation of the Stokes and [[Navier-Stokes equations]]). Then the method enters the GDM framework with the same definition as in the case of the Galerkin method, except for the fact that  <math>\\nabla\\psi_i</math> must be understood as the \"broken gradient\" of <math>\\psi_i</math>, in the sense that it is the piecewise constant function equal in each simplex to the gradient of the affine function in the simplex.\n\n=== Mixed finite element ===\n\nThe [[mixed finite element method]] consists in defining two discrete spaces, one for the approximation of <math>\\nabla \\overline{u}</math> and another one for <math>\\overline{u}</math>.<ref>'''P.-A. Raviart and J. M. Thomas.''' A mixed finite element method for 2nd order elliptic problems. In Mathematical aspects of finite element methods ''(Proc. Conf., Consiglio Naz. delle Ricerche (C.N.R.), Rome, 1975)'', pages 292–315. Lecture Notes in Math., Vol. 606. Springer, Berlin, 1977.</ref> \nIt suffices to use the discrete relations between these approximations to define a GDM. Using the low degree [[Raviart–Thomas basis functions]] allows to get the piecewise constant reconstruction property.\n\n=== Discontinuous Galerkin method ===\n\nThe Discontinuous Galerkin method consists in approximating problems by a piecewise polynomial function, without requirements on the jumps from an element to the other.<ref>'''D. A. Di Pietro and A. Ern.''' Mathematical aspects of discontinuous Galerkin methods, volume 69 of Mathématiques & Applications (Berlin) [Mathematics & Applications]. Springer, Heidelberg, 2012.</ref> It is plugged in the GDM framework by including in the discrete gradient a jump term, acting as the regularization of the gradient in the distribution sense.\n\n=== Mimetic finite difference method and nodal mimetic finite difference method ===\n\nThis family of methods is introduced by [Brezzi ''et al'']<ref>'''F. Brezzi, K. Lipnikov, and M. Shashkov.''' Convergence of the mimetic finite difference method for diffusion problems on polyhedral meshes. SIAM J. Numer. Anal., 43(5):1872–1896, 2005.</ref> and completed in [Lipnikov ''et al''].<ref>'''K. Lipnikov, G. Manzini, and M. Shashkov.''' Mimetic finite difference method. J. Comput. Phys., 257-Part B:1163–1227, 2014.</ref> It allows the approximation of elliptic problems using a large class of polyhedral meshes. The proof that it enters the GDM framework is done in [Droniou ''et al''].<ref name=droniou />\n\n==See also==\n*[[Finite element method]]\n\n==References==\n{{Reflist}}\n\n== External links ==\n* [https://hal.archives-ouvertes.fr/hal-01382358v7/document The Gradient Discretisation Method] by Jérôme Droniou, Robert Eymard, Thierry Gallouët, Cindy Guichard and Raphaèle Herbin\n\n{{Numerical PDE|state=expanded}}\n\n[[Category:Numerical differential equations]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Guard digit",
      "url": "https://en.wikipedia.org/wiki/Guard_digit",
      "text": "In [[numerical analysis]], one or more '''guard digits''' can be used to reduce the amount of [[Rounding|roundoff]] error.\n\nFor example, suppose that the final result of a long, multi-step calculation can be safely rounded off to ''N'' decimal places.  That is to say, the roundoff error introduced by this final roundoff makes a negligible contribution to the overall uncertainty.\n\nHowever, it is quite likely that it is ''not'' safe to round off the intermediate steps in the calculation to the same number of digits.  Be aware that roundoff errors can accumulate.  If ''M'' decimal places are used in the intermediate calculation, we say there are ''M−N'' guard digits.\n\nGuard digits are also used in floating point operations in most computer systems.  Given <math>2^1 \\times 0.100_2 - 2^0 \\times 0.111_2</math> we have to line up the binary points.  This means we must add an extra digit to the first operand—a guard digit.  This gives us <math>2^1 \\times 0.1000_2 - 2^1 \\times 0.0111_2</math>.  Performing this operation gives us <math>2^1 \\times 0.0001_2</math> or <math>2^{-2} \\times 0.100_2</math>.  Without using a guard digit we have <math>2^1 \\times 0.100_2 - 2^1 \\times 0.011_2</math>, yielding <math>2^1 \\times 0.001_2=</math> or <math>2^{-1} \\times 0.100_2</math>.  This gives us a relative error of 1.  Therefore, we can see how important guard digits can be.\n\nAn example of the error caused by floating point roundoff is illustrated in the following [[C (programming language)|C]] code.\n\n<syntaxhighlight lang=\"c\">\nint main(){\n   double a;\n   int i;\n\n   a = 0.2; \n   a += 0.1; \n   a -= 0.3;\n\n   for(i = 0; a < 1.0; i++) \n       a += a;\n\n   printf(\"i=%d, a=%f\\n\", i, a);\n\n   return 0;\n}\n</syntaxhighlight>\nIt appears that the program should not terminate. Yet the output is :  <pre>i=54, a=1.000000</pre>\n\nAnother example is:\n\nTake 2 numbers: <br>\n\n<math>2.56*10^0</math> and <math>2.34*10^2 </math><br>\n\nwe bring the first number to the same power of <math>10</math> as the second one: <br>\n\n<math>0.0256*10^2</math>\n\nThe addition of the 2 numbers is:<br>\n\n<pre>\n0.0256*10^2 \n2.3400*10^2 +  \n____________ \n2.3656*10^2 \n</pre>\n\nAfter padding the second number (i.e., <math>2.34*10^2 </math>) with two <math>0</math>s, the bit after <math>4</math> is the guard digit, and the bit after is the round digit. The result after rounding is <math>2.37</math> as opposed to <math>2.36</math>, without the extra bits (guard and round bits), i.e., by considering only <math>0.02+2.34 = 2.36</math>. The error therefore is <math>0.01</math>.\n\n==References==\n* [[Forman S. Acton]].  ''Numerical Methods that Work'', The Mathematical Association of America (August 1997).\n* Higham, Nicholas J. ''Accuracy and Stability of Numerical Algorithms'', Washington D.C.: Society for Industrial & Applied Mathematics, 2002.\n\n[[Category:Numerical analysis]]\n[[Category:Articles with example C code]]\n\n\n{{mathanalysis-stub}}"
    },
    {
      "title": "Hermes Project",
      "url": "https://en.wikipedia.org/wiki/Hermes_Project",
      "text": "{{Notability|Products|date=December 2012}}\n{{Infobox software\n| name                   = Hermes2D\n| title                  = \n| logo                   = <!-- Image name is enough -->\n| logo caption           = \n| logo_size              = \n| logo_alt               = \n| screenshot             = Harmonic Wave Propogation.png\n| caption                = Harmonic Wave Propagation simulated by the Hermes2d Library\n| screenshot_size        = \n| screenshot_alt         = \n| collapsible            = \n| author                 = \n| developer              = \n| released               = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| discontinued           = \n| latest release version = 3.1\n| latest release date    = {{Start date and age|2015}}\n| latest preview version = \n| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| status                 = \n| programming language   = \n| operating system       = [[Linux]], [[Unix]], [[Microsoft Windows|Windows]], [[Mac OS X]]\n| platform               = \n| size                   = \n| language               = C++, Python\n| language count         = <!-- DO NOT include this parameter unless you know what it does -->\n| language footnote      = \n| genre                  = Scientific simulation software\n| license                = [[GNU Lesser General Public License]]\n| alexa                  = \n| website                = {{URL|www.hpfem.org/hermes/}}\n| standard               = \n| AsOf                   = \n}}\n'''Hermes2D''' ('''''H'''igh'''er'''-order '''m'''odular finite '''e'''lement '''s'''ystem'') is a C++/Python library of algorithms for rapid development of adaptive [[hp-FEM]] solvers.<ref>P.Solin, K. Segeth, I. Dolezel: Higher-Order Finite Element Methods, CRC Press, 2003.</ref> [[hp-FEM]] is a modern version of the [[finite element method]] (FEM) that is capable of extremely fast, exponential convergence.<ref>I. Babuska, B.Q. Guo: The h, p and h-p version of the finite element method: basis theory and applications, Advances in Engineering Software, Volume 15, Issue 3-4, 1992.</ref>\n\n== Main features of the library ==\n\nThe Hermes library can be used for a large variety of [[Partial differential equation|PDE]] problems ranging from linear elliptic equations to time-dependent [[nonlinear]] multi-physics [[Partial differential equation|PDE]] systems arising in [[Elasticity (physics)|elasticity]], [[structural mechanics]], [[fluid mechanics]], [[acoustics]], [[electromagnetics]], and other fields of [[computational Engineering|computational engineering]] and [[science]]. The Hermes libraries are available for download under the GNU Lesser General Licence Terms as a means of providing open-source software for the development of [[Computational Science|Computational Scientific]] Research. Hermes implementation of adaptive [[hp-FEM]] for improved convergence and accuracy in non-linear systems is featured in the software. The software and underlying [[numerical method]]s are developed by an international [http://spilka.math.unr.edu// hp-FEM group] at\nthe [[University of Nevada, Reno|University of Nevada]] at [[Reno]] (United States), [[University of West Bohemia]] in [[Plzeň]] and Institute of Thermomechanics in [[Prague]] ([[Czech Republic]]). Hermes is based on space- and space-time adaptive multi-mesh [[hp-FEM]] algorithms working with highly irregular [[mesh]]es. The [[mesh generation]] is designed using arbitrary-level hanging nodes.<ref>L. Dubcova, P. Solin, J. Cerveny, P. Kus: Space and Time Adaptive Two-Mesh hp-FEM for Transient Microwave Heating Problems, submitted to Electromagnetics</ref>\n\n== Documentation ==\nThe Documentation for the Hermes libraries is an extensive set of instructions, information and tutorials related to the use of Hermes and the [[Finite Element Method]]. Hermes includes instructions for the installation of collaborating Third Party Libraries (TPLs) as well as an introduction to the mathematics behind the [[hp-FEM]] method and detailed instructions on the use and modification of the code. Any user who wished to add to the capabilities of Hermes can find instructions on how to submit their work directly to the authors via [[GitHub]]. The documentation includes tutorials for the download and compilation of Hermes on multiple operating systems, as well as example problems and tests for each [[Application software|software package]].\n\n== See also ==\n* [[List of numerical analysis software]]\n* [[List of finite element software packages]]\n\n== References ==\n{{reflist}}\n\n[[Category:Scientific simulation software]]\n[[Category:Finite element software]]\n[[Category:Numerical analysis]]\n[[Category:Finite element method]]\n[[Category:Finite element software for Linux]]\n[[Category:Free software programmed in C++]]\n[[Category:Free software programmed in Python]]\n[[Category:University of West Bohemia]]"
    },
    {
      "title": "Hundred-dollar, Hundred-digit Challenge problems",
      "url": "https://en.wikipedia.org/wiki/Hundred-dollar%2C_Hundred-digit_Challenge_problems",
      "text": "The '''Hundred-dollar, Hundred-digit Challenge problems''' are 10 problems in [[numerical mathematics]] published in 2002 by {{harvs|txt|first=Nick|last= Trefethen|authorlink=Lloyd N. Trefethen|year=2002}}. A $100  prize was offered to whoever produced the most accurate solutions, measured up to 10 [[significant digits]]. The deadline for the contest was May 20, 2002. In the end, 20 teams solved all of the problems perfectly within the required precision, and an anonymous donor aided in producing the required prize monies. The challenge and its solutions were described in detail in the book {{harvs | last1=Bornemann | first1=Folkmar | last2=Laurie | first2=Dirk | last3=Wagon | first3=Stan | last4=Waldvogel | first4=Jörg | title=The SIAM 100-digit challenge: A study in high-accuracy numerical computing.| publisher=Society for Industrial and Applied Mathematics (SIAM) | location=Philadelphia, PA | isbn=978-0-89871-561-3 |mr=2076374 | year=2004|url=http://www-m3.ma.tum.de/m3old/bornemann/challengebook/index.html}}.\n\n==The problems==\nFrom {{harv|Trefethen|2002}}:\n# <math> \\lim_{\\varepsilon \\to 0}\\int_\\varepsilon^1 x^{-1} \\cos\\left(x^{-1} \\log x\\right)\\,dx</math>\n# A photon moving at speed 1 in the ''xy''-plane starts at ''t'' = 0 at (''x'', ''y'') = (0.5, 0.1) heading due east. Around every integer lattice point (''i'', ''j'') in the plane, a circular mirror of radius 1/3 has been erected. How far from the origin is the photon at ''t'' = 10?\n# The infinite matrix ''A'' with entries <math>a_{11}=1, a_{12}=1/2, a_{21}=1/3, a_{13}=1/4, a_{22}=1/5, a_{31}=1/6, \\dots </math> is a bounded operator on <math>\\ell^2</math>. What is <math>||A||</math>?\n# What is the global minimum of the function <math>\\exp\\left(\\sin\\left(50x\\right)\\right) + \\sin\\left(60e^y\\right) + \\sin\\left(70 \\sin x\\right)+\\sin\\left(\\sin\\left(80y\\right)\\right) - \\sin\\left(10\\left(x+y\\right)\\right) + 1/4\\left(x^2 + y^2\\right)</math>\n# Let <math>f(z)=1/\\Gamma(z)</math>, where <math>\\Gamma(z)</math> is the gamma function, and let <math>p(z)</math> be the cubic polynomial that best approximates <math>f(z)</math> on the unit disk in the supremum norm <math>||.||_\\infty</math>. What is <math>||f-p||_\\infty</math>?\n# A flea starts at <math>(0,0)</math> on the infinite 2D integer lattice and executes a biased [[random walk]]: At each step it hops north or south with probability <math>1/4</math>, east with probability <math>1/4+\\varepsilon</math>, and west with probability <math>1/4-\\varepsilon</math>. The probability that the flea returns to (0, 0) sometime during its wanderings is <math>1/2</math>. What is <math>\\varepsilon</math>?\n# Let A be the 20000&times;20000 matrix whose entries are zero everywhere except for the primes 2, 3, 5, 7, ..., 224737 along the main diagonal and the number 1 in all the positions <math>a_{ij}</math> with <math>|i-j|=1, 2, 4, 8, \\dots, 16384</math>. What is the (1, 1) entry of <math>A^{-1}</math>?\n# A square plate <math>[-1,1]\\times [-1,1]</math> is at temperature <math>u=0</math>. At time <math>t=0</math>, the temperature is increased to <math>u=5</math> along one of the four sides while being held at <math>u=0</math> along the other three sides, and heat then flows into the plate according to <math>u_{t} = \\Delta u</math>. When does the temperature reach <math>u=1</math> at the center of the plate?\n# The integral <math>I(\\alpha)=\\int_0^2\\left[2+\\sin\\left(10\\alpha\\right)\\right]x^\\alpha \\sin\\left(\\alpha/\\left(2-x\\right)\\right)\\,dx</math> depends on the parameter α. What is the value of α in [0,&nbsp;5] at which ''I''(α) achieves its maximum?\n# A particle at the center of a 10&times;1 rectangle undergoes Brownian motion (i.e., 2D random walk with infinitesimal step lengths) till it hits the boundary. What is the probability that it hits at one of the ends rather than at one of the sides?\n\n==Solutions==\n#0.3233674316\n#0.9952629194\n#1.274224152\n#&minus;3.306868647\n#0.2143352345\n#0.06191395447\n#0.7250783462\n#0.4240113870\n#0.7859336743\n#3.837587979 &times; 10<sup>&minus;7</sup>\n\nThese answers have been assigned the identifiers {{OEIS2C|A117231}}, {{OEIS2C|A117232}}, {{OEIS2C|A117233}}, {{OEIS2C|A117234}}, {{OEIS2C|A117235}}, {{OEIS2C|A117236}}, {{OEIS2C|A117237}}, {{OEIS2C|A117238}}, {{OEIS2C|A117239}}, and {{OEIS2C|A117240}} in the [[On-Line Encyclopedia of Integer Sequences]].\n\n==References==\n* {{cite web |author1=Bailey, D. H.  |author2=Borwein, J. M. | title=Sample Problems of Experimental Mathematics | date=2003-09-22 | url=http://www.experimentalmath.info/books/expmath-probs.pdf}}\n* {{cite web | author=Bornemann, F. | title=Short Remarks on the Solution of Trefethen's Hundred-Digit Challenge | date=2002-11-05 | url=http://www-m3.ma.tum.de/foswiki/pub/M3/Allgemeines/FolkmarBornemannPublications/short.pdf}}\n*{{Cite book | last1=Bornemann | first1=Folkmar | last2=Laurie | first2=Dirk | last3=Wagon | first3=Stan | author3-link = Stan Wagon | last4=Waldvogel | first4=Jörg | title=The SIAM 100-digit challenge: A study in high-accuracy numerical computing| publisher=Society for Industrial and Applied Mathematics (SIAM) | location=Philadelphia, PA | isbn=978-0-89871-561-3 |mr=2076374 | year=2004|url=http://www-m3.ma.tum.de/m3old/bornemann/challengebook/index.html | ref=harv | postscript=<!--None-->}}\n* {{cite journal | author=Leslie, M. (Ed.) | title=NetWatch: Decimal Decathlon | journal=Science | volume=295 | doi=10.1126/science.295.5559.1431d  | issue=5559 | year=2002 | ref=harv | pages=1431d-1431}}\n*{{Cite journal|url=http://www.siam.org/pdf/news/388.pdf |last=Trefethen|first=Nick|journal=SIAM News|volume=35|issue=1|page=65|title=A Hundred-dollar, Hundred-digit Challenge|year=2002|ref=harv|postscript=<!--None-->}}\n*{{MathWorld |title=Hundred-Dollar, Hundred-Digit Challenge Problems |urlname=Hundred-DollarHundred-DigitChallengeProblems}}\n\n[[Category:Numerical analysis]]\n[[Category:Recreational mathematics]]\n[[Category:Mathematics competitions]]"
    },
    {
      "title": "Identifiability analysis",
      "url": "https://en.wikipedia.org/wiki/Identifiability_analysis",
      "text": "{{more footnotes|date=August 2015}}\n\n'''Identifiability analysis''' is a group of methods found in [[mathematical statistics]] that are used to determine how well the parameters of a model are estimated by the quantity and quality of experimental data.<ref>Cobelli & DiStefano (1980)</ref> Therefore, these methods explore not only [[identifiability]] of a model, but also the relation of the model to particular experimental data or, more generally, the data collection process.\n\n==Introduction==\n\nAssuming a model is fit to experimental data, the [[goodness of fit]] does not reveal how reliable the parameter estimates are. The goodness of fit is also not sufficient to prove the model was chosen correctly. For example, if the experimental data is noisy or if there is an insufficient amount of data points, it could be that the estimated parameter values could vary drastically without significantly influencing the goodnes of fit. To address this issues the '''identifiability analysis''' could be applied as an important step to ensure correct choice of model, and sufficient amount of experimental data. The purpose of this analysis is either a quantified proof of correct model choice and integrality of experimental data acquired or such analysis can serve as an instrument for the detection of non-identifiable and sloppy parameters, helping planning the experiments and in building and improvement of the model at the early stages.\n\n==Structural and practical identifiability analysis==\n\nStructural identifiability analysis is a particular type of analysis in which the model structure itself is investigated for non-identifiability. Recognized non-identifiabilities may be removed analytically through substitution of the non-identifiable parameters with their combinations or by another way. The model overloading with number of independent parameters after its application to simulate finite experimental dataset may provide the good fit to experimental data by the price of making fitting results not sensible to the changes of parameters values, therefore leaving parameter values undetermined. Structural methods are also referred to as ''a priori'', because non-identifiability analysis in this case could also be performed prior to the calculation of the fitting score functions, by exploring the number [[degrees of freedom (statistics)]] for the model and the number of independent experimental conditions to be varied.\n\nPractical identifiability analysis can be performed by exploring the fit of existing model to experimental data. Once the fitting in any measure was obtained, parameter identifiability analysis can be performed either locally near a given point (usually near the parameter values provided the best model fit) or globally over the extended parameter space. The common example of the practical identifiability analysis is profile likelihood method.\n\n==See also==\n* [[Curve fitting]]\n* [[Estimation theory]]\n* [[Identifiability]]\n* [[Parameter identification problem]]\n* [[Regression analysis]]\n\n==Notes==\n{{Reflist}}\n\n==References==\n{{refbegin}}\n* {{Cite journal |doi = 10.1029/2000WR900350 |volume = 37 |issue = 4 |pages = 1015–1030 |last1 = Brun |first1 = Roland |last2 = Reichert |first2 = Peter |last3 = Künsch |first3 = Hans R. |title = Practical identifiability analysis of large environmental simulation models |journal = [[Water Resources Research]] |date = 2001 |url = http://doi.wiley.com/10.1029/2000WR900350 |bibcode=2001WRR....37.1015B}}\n* {{Cite journal\n  |last1 = Cobelli\n  |first1 = C.\n  |last2 = DiStefano\n  |first2 = J.\n  |title = Parameter and structural identifiability concepts and ambiguities: a critical review and analysis\n  |year = 1980\n  |journal = [[Am. J. Physiol. Regul. Integr. Comp. Physiol.]]\n  |issue = 239\n  |pages = 7–24\n  }}\n* {{Cite journal |doi = 10.1371/journal.pcbi.0030189 |volume = 3 |issue = 10 |pages = –189 |last1 = Gutenkunst |first1 = Ryan N. |last2 = Waterfall |first2 = Joshua J. |last3 = Casey |first3 = Fergal P. |last4 = Brown |first4 = Kevin S. |last5 = Myers |first5 = Christopher R. |last6 = Sethna |first6 = James P. |title = Universally Sloppy Parameter Sensitivities in Systems Biology Models |journal = [[PLOS Computational Biology]] |date = 2007 |url = http://dx.plos.org/10.1371/journal.pcbi.0030189}}\n*Lavielle, M.; Aarons, L. (2015), \"What do we mean by identifiability in mixed effects models?\", ''Journal of Pharmacokinetics and Pharmacodynamics'', 43: 111-122; {{doi|10.1007/s10928-015-9459-4}}.\n* {{Cite journal |doi = 10.1093/bioinformatics/17.1.3 | volume = 17 |issue = 1 |pages = 3–12 |last1 = Myasnikova |first1 = E. |last2 = Samsonova |first2 = A. |last3 = Kozlov |first3 = K. |last4 = Samsonova |first4 = M. |last5 = Reinitz |first5 = J. |title = Registration of the expression patterns of Drosophila segmentation genes by two independent methods |journal = [[Bioinformatics (journal)|Bioinformatics]] |date = 2001-01-01 |url = http://bioinformatics.oxfordjournals.org/cgi/doi/10.1093/bioinformatics/17.1.3}}\n* {{Cite journal |doi = 10.1093/bioinformatics/btp358 |volume = 25 |issue = 15 |pages = 1923–1929 |last1 = Raue |first1 = A. |last2 = Kreutz |first2 = C. |last3 = Maiwald |first3 = T. |last4 = Bachmann |first4 = J. |last5 = Schilling |first5 = M. |last6 = Klingmuller |first6 = U. |last7 = Timmer |first7 = J. |title = Structural and practical identifiability analysis of partially observed dynamical models by exploiting the profile likelihood |journal = [[Bioinformatics (journal)|Bioinformatics]] |date = 2009-08-01 |url = http://bioinformatics.oxfordjournals.org/cgi/doi/10.1093/bioinformatics/btp358 |pmid = 19505944}}\n*Stanhope, S.; Rubin, J. E.; Swigon D. (2014), \"Identifiability of linear and linear-in-parameters dynamical systems from a single trajectory\", ''SIAM Journal on Applied Dynamical Systems'', 13: 1792–1815; {{doi|10.1137/130937913}}.\n* {{Cite journal |title = Nonlinear regression analysis: Its applications |last1 = Vandeginste |first1 = B. |first2 = D. M. |last2 = Bates |first3 = D. G. |last3 = Watts |date = 1988 |publication-date = 1989 |journal = [[Journal of Chemometrics]] |volume = 3 |issue = 3 |pages = 544–545 |ISBN = 0471-816434 |doi = 10.1002/cem.1180030313|url = http://doi.wiley.com/10.1002/cem.1180030313}}\n\n[[Category:Numerical analysis]]\n[[Category:Interpolation]]\n[[Category:Regression analysis]]"
    },
    {
      "title": "Interval arithmetic",
      "url": "https://en.wikipedia.org/wiki/Interval_arithmetic",
      "text": "{{Citation style|date=June 2015|details=In some cases, only citation titles are provided}}\n[[File:Set of curves Outer approximation.png|345px|thumb|right|Tolerance function (turquoise) and interval-valued approximation (red)'']]\n'''Interval arithmetic''', '''interval mathematics''', '''interval analysis''', or '''interval computation''', is a method developed by mathematicians since the 1950s and 1960s, as [[Floating point error mitigation|an approach to putting bounds]] on [[rounding error]]s and [[measurement error]]s in [[numerical analysis|mathematical computation]] and thus developing [[numerical methods]] that yield reliable results.  Very simply put, it represents each value as a range of possibilities.  For example, instead of estimating the height of someone using standard arithmetic as 2.0 metres, using interval arithmetic we might be certain that that person is somewhere between 1.97 and 2.03 metres.\n\nThis concept is suitable for a variety of purposes.  The most common use is to keep track of and handle rounding errors directly during the calculation and of uncertainties in the knowledge of the exact values of physical and technical parameters.  The latter often arise from measurement errors and tolerances for components or due to limits on computational accuracy. Interval arithmetic also helps find reliable and guaranteed solutions to equations (such as [[differential equation]]s) and [[optimization problem]]s.\n\nMathematically, instead of working with an uncertain [[real number|real]] <math>x</math> we work with the two ends of the interval <math>[a,b]</math> that contains <math>x</math>. In interval arithmetic, any variable <math>x</math> lies between <math>a</math> and <math>b</math>, or could be one of them.  A function <math>f</math> when applied to <math>x</math> is also uncertain. In interval arithmetic <math>f</math> produces an interval <math>[c,d]</math> that is all the possible values for <math>f(x)</math> for all <math>x \\in [a,b]</math>.\n\n==Introduction==\nThe main focus of interval arithmetic is the simplest way to calculate upper and lower endpoints for the range of values of a function in one or more variables. These endpoints are not necessarily the [[supremum]] or [[infimum]], since the precise calculation of those values can be difficult or impossible.\n\nTreatment is typically limited to  real intervals, so quantities of form\n:<math>[a,b] = \\{x \\in \\mathbb{R} \\,|\\, a \\le x \\le b\\},</math>\nwhere <math> a = {-\\infty}</math> and <math> b = {\\infty}</math> are allowed; with one of them infinite we would have an unbounded interval, while with both infinite we would have the extended real number line.\n\nAs with traditional calculations with real numbers, simple arithmetic operations and functions on elementary intervals must first be defined.<ref name=\"Kulisch\">{{cite book |author-last=Kulisch |author-first=Ulrich |author-link=:de:Ulrich Kulisch |title=Wissenschaftliches Rechnen mit Ergebnisverifikation. Eine Einführung |year=1989 |publisher=[[Vieweg-Verlag]] |location=Wiesbaden |language=German |isbn=3-528-08943-1}}</ref> More complicated functions can be calculated from these basic elements.<ref name=\"Kulisch\"/>\n\n===Example===\n[[File:Interval BMI Simple Example.png|350px|thumb|left|[[Body Mass Index]] for a person 1.80&nbsp;m tall in relation to body weight ''m'' (in kilograms)]]\nTake as an example the calculation of [[body mass index]] (BMI). The BMI is the body weight in kilograms divided by the square of height in metres. A bathroom scale may have a resolution of one kilogram. We do not know intermediate values{{snd}} about 79.6&nbsp;kg or 80.3&nbsp;kg{{snd}} but information rounded to the nearest whole number. It is unlikely that when the scale reads 80&nbsp;kg, someone really weighs exactly 80.0&nbsp;kg. In normal rounding to the nearest value, the scales showing 80&nbsp;kg indicates a weight between 79.5&nbsp;kg and 80.5&nbsp;kg.  The relevant range is that of all real numbers that are greater than or equal to 79.5, while less than or equal to 80.5, or in other words the interval [79.5,80.5].\n\nFor a man who weighs 80&nbsp;kg and is 1.80&nbsp;m tall, the BMI is about 24.7. With a weight of 79.5&nbsp;kg and the same height the value is 24.5, while 80.5 kilograms gives almost 24.9. So the actual BMI is in the range [24.5,24.9]. The error in this case does not affect the conclusion (normal weight), but this is not always  the position. For example, weight fluctuates in the course of a day so that the BMI can vary between 24 (normal weight) and 25 (overweight).  Without detailed analysis it is not possible to always exclude questions as to whether an error ultimately is large enough to have significant influence.\n\nInterval arithmetic states the range of possible outcomes explicitly.  Simply put, results are no longer stated as numbers, but as intervals that represent imprecise values. The size of the intervals are similar to error bars to a metric in expressing the extent of uncertainty. Simple arithmetic operations, such as basic arithmetic and trigonometric functions, enable the calculation of outer limits of intervals.\n\n===Simple arithmetic===\n[[File:Interval BMI Example.png|260px|thumb|right|Body mass index for different weights in relation to height L (in metres)]]\nReturning to the earlier BMI example, in determining the body mass index, height and body weight both affect the result. For height, measurements are usually in round centimetres: a recorded measurement of 1.80 metres actually means a height  somewhere between 1.795&nbsp;m and 1.805&nbsp;m.  This uncertainty must be combined with the fluctuation range in weight between 79.5&nbsp;kg and 80.5&nbsp;kg.  The BMI is defined as the weight in kilograms divided by the square of height in metre.  Using either 79.5&nbsp;kg and 1.795&nbsp;m or 80.5&nbsp;kg and 1.805&nbsp;m gives approximately 24.7. But the person in question may only be 1.795&nbsp;m tall, with a weight of 80.5 kilograms{{snd}} or 1.805&nbsp;m and 79.5 kilograms: all combinations of all possible intermediate values must be considered.  Using the interval arithmetic methods described below, the BMI lies in the interval\n\n:<math>[79{.}5; 80{.}5]/([1{.}795; 1{.}805])^2 = [24{.}4; 25{.}0].</math>\n\nAn operation <math>\\langle \\mathrm{op} \\rangle,</math> such as addition or multiplication, on two intervals is defined by\n\n:<math>[x_1, x_2] {\\,\\langle \\mathrm{op} \\rangle\\,} [y_1, y_2] = \\{ x {\\,\\langle \\mathrm{op} \\rangle\\,} y \\, | \\, x \\in [x_1, x_2] \\,\\mbox{and}\\, y \\in [y_1, y_2] \\}.</math>\n\nFor the four basic arithmetic operations this can become\n\n:<math>[x_1, x_2] \\langle \\mathrm{op} \\rangle [y_1, y_2] = \\left[ \\min(x_1 {\\langle \\mathrm{op} \\rangle} y_1, x_1 \\langle \\mathrm{op} \\rangle y_2, x_2 \\langle \\mathrm{op} \\rangle y_1, x_2 \\langle \\mathrm{op} \\rangle y_2), \\max(x_1 \\langle \\mathrm{op} \\rangle y_1, x_1 \\langle \\mathrm{op} \\rangle y_2, x_2 \\langle \\mathrm{op} \\rangle y_1, x_2 \\langle \\mathrm{op} \\rangle y_2) \\right],</math>\n\nprovided that <math>x \\langle \\mathrm{op} \\rangle y</math> is well-defined for all <math>x\\in [x_1, x_2]</math> and <math>y \\in [y_1, y_2]</math>.\n\nFor practical applications this can be simplified further:\n\n* [[Addition]]: <math>[x_1, x_2] + [y_1, y_2] = [x_1+y_1, x_2+y_2]</math>\n* [[Subtraction]]: <math>[x_1, x_2] - [y_1, y_2] = [x_1-y_2, x_2-y_1]</math>\n* [[Multiplication]]: <math>[x_1, x_2] \\cdot [y_1, y_2] = [\\min(x_1 y_1,x_1 y_2,x_2 y_1,x_2 y_2), \\max(x_1 y_1,x_1 y_2,x_2 y_1,x_2 y_2)]</math>\n* [[Division (mathematics)|Division]]:\n\n::<math>\\frac{[x_1, x_2]}{[y_1, y_2]} = [x_1, x_2] \\cdot \\frac{1}{[y_1, y_2]},</math>\n\n:where\n\n::<math>\\begin{align}\n\\frac{1}{[y_1, y_2]} &= \\left [\\tfrac{1}{y_2}, \\tfrac{1}{y_1} \\right ] && 0 \\notin [y_1, y_2] \\\\\n\\frac{1}{[y_1, 0]}   &= \\left [-\\infty, \\tfrac{1}{y_1} \\right ] \\\\\n\\frac{1}{[0, y_2]}   &= \\left [\\tfrac{1}{y_2}, \\infty \\right ] \\\\\n\\frac{1}{[y_1, y_2]} &= \\left [-\\infty, \\tfrac{1}{y_1} \\right ] \\cup \\left [\\tfrac{1}{y_2}, \\infty \\right ] = [-\\infty, \\infty] && 0 \\in (y_1, y_2)\n\\end{align}</math>\n\nThe last case loses useful information about <math>(1/y_1, 1/y_2)</math>. So typically it is common to work with <math>\\left [-\\infty, \\tfrac{1}{y_1} \\right ]</math> and <math>\\left [\\tfrac{1}{y_2}, \\infty \\right ]</math> as separate intervals.\n\nBecause several such divisions may occur in an interval arithmetic calculation, it is sometimes useful to do the calculation with so-called ''multi-intervals'' of the form\n\n:<math>\\bigcup_{i=1}^l \\left [x_{i1},x_{i2} \\right ].</math>\n\nThe corresponding ''multi-interval arithmetic'' maintains a disjoint set of intervals and also provides for overlapping intervals to unite.<ref name=\"Dreyer\">{{cite book|last=Dreyer|first=Alexander|title=Interval Analysis of Analog Circuits with Component Tolerances|year=2003|publisher=[[Shaker Verlag]]|location=Aachen, Germany|isbn=3-8322-4555-3}}</ref>{{rp|15}}\n\nSince a real number <math>r\\in \\R</math> can be interpreted as the interval <math>[r,r],</math> intervals and real numbers can be freely and easily combined.\n\nWith the help of these definitions, it is already possible to calculate the range of simple functions, such as <math>f(a,b,x) = a \\cdot x + b.</math> If for example, <math>a = [1,2], b = [5,7]</math> and <math>x = [2,3],</math> it is clear\n\n:<math>f(a,b,x) = ([1,2] \\cdot [2,3]) + [5,7] = [1 \\cdot 2, 2\\cdot 3] + [5,7] = [7,13]</math>.\n\nInterpreting this as a function <math>f(a,b,x)</math> of the variable <math>x</math> with interval parameters <math>a</math> and <math>b</math>, then it is possible to find the roots of this function. It is then\n\n:<math>f([1,2],[5,7],x) = ([1,2] \\cdot x) + [5,7] = 0\\Longleftrightarrow [1,2] \\cdot x = [-7, -5]\\Longleftrightarrow x = \\frac{[-7, -5]}{[1,2]},</math>\n\nthe possible zeros are in the interval <math>[-7, -2.5].</math>\n\n[[File:Interval multiplication.png|120px|right|thumb|Multiplication of positive intervals]]\n\nAs in the above example, the multiplication of intervals often only requires two multiplications.  It is in fact\n\n:<math>[x_1, x_2] \\cdot [y_1, y_2] = [x_1 \\cdot y_1, x_2 \\cdot y_2], \\qquad \\text{ if } x_1, y_1 \\geq 0.</math>\n\nThe multiplication can be seen as a destination area of a rectangle with varying edges. The result interval covers all levels from the smallest to the largest.\n\nThe same applies when one of the two intervals is non-positive and the other non-negative.  Generally, multiplication can produce results as wide as <math>[-\\infty, \\infty].</math> for example if <math>0 \\cdot \\infty</math> is squared. This also occurs, for example, in a division, if the numerator and denominator both contain zero.\n\n===Notation===\nTo make the notation of intervals smaller in formulae, brackets can be used.\n\nSo we can use <math>[x] \\equiv [x_1, x_2]</math> to represent an interval.  For the set of all finite intervals, we can use\n\n:<math>[\\R] := \\left \\{\\, [x_1, x_2] \\,|\\, x_1 \\leq x_2 \\text{ and } x_1, x_2 \\in \\R \\cup \\{-\\infty, \\infty\\} \\right \\}</math>\n\nas an abbreviation.  For a vector of intervals <math>\\left([x]_1, \\ldots , [x]_n \\right) \\in  [\\R]^n </math> we can also use a bold font: <math>[\\mathbf{x}]</math>.\n\nNote that in such a compact notation, <math>[x]</math> should not be confused between a so-called improper or single point interval <math>[x_1, x_1]</math> and the lower and upper limit.\n\n===Elementary functions===\n[[File:Value domain of monotonic function.png|160px|right|thumb|Values of a monotonic function]]\nInterval methods can also apply to functions that do not just use simple arithmetic, and we must also use other basic functions to redefine intervals, using already known monotonicity properties.\n\nFor [[monotonic function]]s in one variable, the range of values is also easy. If <math>f: \\R \\to \\R</math> is monotonically increasing or decreasing in the interval <math>[x_1, x_2],</math> then for all <math>y_1, y_2 \\in [x_1, x_2]</math> such that <math>y_1 \\leq y_2,</math> one of the following inequalities applies:\n\n:<math>f(y_1) \\leq f(y_2) \\quad \\text{or} \\quad f(y_1) \\geq f(y_2).</math>\n\nThe range corresponding to the interval <math>[y_1, y_2] \\subseteq [x_1, x_2]</math> can be calculated by applying the function to its endpoints:\n\n:<math>f([y_1, y_2]) = \\left[\\min \\left  \\{f(y_1), f(y_2) \\right \\}, \\max \\left\\{ f(y_1), f(y_2) \\right\\}\\right].</math>\n\nFrom this the following basic features for interval functions can easily be defined:\n\n* [[Exponential function]]: <math>a^{[x_1, x_2]} = [a^{x_1},a^{x_2}],</math> for <math>a > 1,</math>\n* [[Logarithm]]: <math>\\log_a [x_1, x_2] = [\\log_a {x_1}, \\log_a {x_2}],</math> for positive intervals <math>[x_1, x_2]</math> and <math>a>1,</math>\n* Odd powers: <math>[x_1, x_2]^n = [x_1^n,x_2^n]</math>, for odd <math>n\\in \\N.</math>\n\nFor even powers, the range of values being considered is important, and needs to be dealt with before doing any multiplication. For example, <math>x^n</math> for <math>x \\in [-1,1]</math> should produce the interval <math>[0,1]</math> when <math>n = 2, 4, 6, \\ldots.</math> But if <math>[-1,1]^n</math> is taken by applying interval multiplication of form  <math>[-1,1]\\cdot \\ldots \\cdot [-1,1]</math> then the result appears to be <math>[-1,1],</math> wider than necessary.\n\nInstead consider the function <math>x^n</math> as a monotonically decreasing function for <math>x < 0</math> and a monotonically increasing function for <math>x > 0.</math> So for even <math>n\\in \\N</math>:\n\n:<math>{}[x_1, x_2]^n = \\begin{cases} \\left [x_1^n, x_2^n \\right ] & x_1 \\geq 0 \\\\ \\left [x_2^n, x_1^n \\right ] & x_2 < 0 \\\\ \\left [0, \\max \\left \\{x_1^n, x_2^n \\right \\} \\right ] & \\text{otherwise} \\end{cases}</math>\n\nMore generally, one can say that for piecewise monotonic functions it is sufficient to consider the endpoints <math>x_1, x_2</math>  of the interval <math>[x_1, x_2]</math>, together with the so-called ''critical points'' within the interval being those points where the monotonicity of the function changes direction.\n\nFor the [[sine]] and [[cosine]] functions, the critical points are at <math>\\left(\\tfrac{1}{2} + n\\right)\\pi</math> or <math>n\\pi</math> for all <math>n \\in \\Z</math> respectively. Only up to five points matter as the resulting interval is <math>[-1,1]</math> if the interval includes at least two extrema. For sine and cosine, only the endpoints need full evaluation as the critical points lead to easily pre-calculated values  – namely -1, 0, +1.\n\n===Interval extensions of general functions===\nIn general, it may not be easy to find such a simple description of the output interval for many functions.  But it may still be possible to extend functions to interval arithmetic.\nIf <math>f:\\mathbb{R}^n \\rightarrow \\mathbb{R}</math> is a function from a real vector to a real number, then &nbsp;<math>[f]:[\\mathbb{R}]^n \\rightarrow [\\mathbb{R}]</math> is called an ''interval extension'' of <math>f</math> if\n:<math>[f]([\\mathbf{x}]) \\supseteq \\{f(\\mathbf{y}) | \\mathbf{y} \\in [\\mathbf{x}]\\}</math>.\n\nThis definition of the interval extension does not give a precise result.  For example, both  <math>[f]([x_1,x_2]) =[e^{x_1}, e^{x_2}]</math> and <math>[g]([x_1,x_2]) =[{-\\infty}, {\\infty}]</math> are allowable extensions of the exponential function. Extensions as tight as possible are desirable, taking into the relative costs of calculation and imprecision; in this case <math>[f]</math> should be chosen as it give the tightest possible result.\n\nThe ''natural interval extension'' is achieved by combining the function rule <math>f(x_1, \\cdots, x_n)</math> with the equivalents of the basic arithmetic and elementary functions.\n\nThe ''Taylor interval extension'' (of degree <math>k</math> ) is a <math>k+1</math> times differentiable function <math>f</math> defined by\n\n:<math>[f]([\\mathbf{x}]) :=  f(\\mathbf{y}) + \\sum_{i=1}^k\\frac{1}{i!}\\mathrm{D}^i f(\\mathbf{y}) \\cdot ([\\mathbf{x}] - \\mathbf{y})^i + [r]([\\mathbf{x}], [\\mathbf{x}], \\mathbf{y})\n</math>,\nfor some <math>\\mathbf{y} \\in [\\mathbf{x}]</math>,\nwhere <math>\\mathrm{D}^i f(\\mathbf{y})</math> is the <math>i</math>th order differential of <math>f</math> at the point <math>\\mathbf{y}</math> and <math>[r]</math> is an interval extension of the ''Taylor remainder''\n\n:<math>r(\\mathbf{x}, \\xi, \\mathbf{y}) = \\frac{1}{(k+1)!}\\mathrm{D}^{k+1} f(\\xi) \\cdot (\\mathbf{x}-\\mathbf{y})^{k+1}. </math>\n\n[[File:Meanvalue extension.png|220px|right|thumb|Mean value form]]\nThe vector <math>\\xi</math> lies between <math>\\mathbf{x}</math>\nand <math>\\mathbf{y}</math> with <math>\\mathbf{x}, \\mathbf{y} \\in [\\mathbf{x}]</math>, <math>\\xi</math> is protected by <math>[\\mathbf{x}]</math>.\nUsually one chooses <math>\\mathbf{y}</math> to be the midpoint of the interval and uses the natural interval extension to assess the remainder.\n\nThe special case of the Taylor interval extension of degree <math>k = 0</math> is also referred to as the ''mean value form''.\nFor an interval extension of the [[Jacobian matrix and determinant|Jacobian]] <math>[J_f](\\mathbf{[x]})</math>\nwe get\n\n:<math>[f]([\\mathbf{x}]) :=\n  f(\\mathbf{y}) + [J_f](\\mathbf{[x]}) \\cdot ([\\mathbf{x}] - \\mathbf{y})\n</math>.\n\nA nonlinear function can be defined by linear features.\n\n==Complex interval arithmetic==\nAn interval can also be defined as a locus of points at a given distance from the centre, and this definition can be extended from real numbers to [[complex number]]s.<ref>[https://books.google.com/books?id=Vtqk6WgttzcC Complex interval arithmetic and its applications], Miodrag Petkovi?, Ljiljana Petkovi?, Wiley-VCH, 1998, {{ISBN|978-3-527-40134-5}}</ref> As it is the case with computing with real numbers, computing with complex numbers involves uncertain data. So, given the fact that an interval number is a real closed interval and a complex number is an ordered pair of [[real number]]s, there is no reason to limit the application of interval arithmetic to the measure of uncertainties in computations with real numbers.<ref name=\"Dawood\">Hend Dawood (2011). ''Theories of Interval Arithmetic: Mathematical Foundations and Applications''. Saarbrücken: LAP LAMBERT Academic Publishing. {{ISBN|978-3-8465-0154-2}}.</ref> Interval arithmetic can thus be extended, via complex interval numbers, to determine regions of uncertainty in computing with complex numbers.<ref name=\"Dawood\"/>\n\nThe basic algebraic operations for real interval numbers (real closed intervals) can be extended to complex numbers. It is therefore not surprising that complex interval arithmetic is similar to, but not the same as, ordinary complex arithmetic.<ref name=\"Dawood\"/> It can be shown that, as it is the case with real interval arithmetic, there is no distributivity between addition and multiplication of complex interval numbers except for certain special cases, and inverse elements do not always exist for complex interval numbers.<ref name=\"Dawood\"/> Two other useful properties of ordinary complex arithmetic fail to hold in complex interval arithmetic: the additive and multiplicative properties, of ordinary complex conjugates, do not hold for complex interval conjugates.<ref name=\"Dawood\"/>\n\nInterval arithmetic can be extended, in an analogous manner, to other multidimensional [[number systems]] such as [[quaternion]]s and [[octonion]]s, but with the expense that we have to sacrifice other useful properties of ordinary arithmetic.<ref name=\"Dawood\"/>\n\n==Interval methods==\n{{Refimprove section|date=February 2018}} \nThe methods of classical numerical analysis can not be transferred one-to-one into interval-valued algorithms, as dependencies between numerical values are usually not taken into account.\n\n===Rounded interval arithmetic===\n[[File:Illustration of outward rounding.png|200px|left|thumb|Outer bounds at different level of rounding]]\nTo work effectively in a real-life implementation, intervals must be compatible with floating point computing.  The earlier operations were based on exact arithmetic, but in general fast numerical solution methods may not be available.  The range of values of the function <math>f(x, y) = x + y</math>\nfor <math>x \\in [0.1, 0.8]</math> and <math>y \\in [0.06, 0.08]</math> are for example <math>[0.16, 0.88]</math>.  Where the same calculation is done with single digit precision, the result would normally be <math>[0.2, 0.9]</math>. But <math>[0.2, 0.9] \\not\\supseteq [0.16, 0.88]</math>,\nso this approach would contradict the basic principles of interval arithmetic, as a part of the domain of <math>f([0.1, 0.8], [0.06, 0.08])</math> would be lost.\nInstead, the outward rounded solution <math>[0.1, 0.9]</math> is used.\n\nThe standard [[IEEE 754]] for binary floating-point arithmetic also sets out procedures for the implementation of rounding.  An IEEE 754 compliant system allows programmers to round to the nearest floating point number; alternatives are rounding towards 0 (truncating), rounding toward positive infinity (i.e. up), or rounding towards negative infinity (i.e. down).\n\nThe required ''external rounding'' for interval arithmetic can thus be achieved by changing the rounding settings of the processor in the calculation of the upper limit (up) and lower limit (down). Alternatively, an appropriate small interval <math>[\\varepsilon_1, \\varepsilon_2]</math> can be added.\n\n===Dependency problem===\n[[File:Interval-dependence problem-front view.png|right|thumb|Approximate estimate of the value range]]\n\nThe so-called ''dependency problem'' is a major obstacle to the application of interval arithmetic. Although interval methods can determine the range of elementary arithmetic operations and functions very accurately, this is not always true with more complicated functions. If an interval occurs several times in a calculation using parameters, and each occurrence is taken independently then this can lead to an unwanted expansion of the resulting intervals.\n\n[[File:Interval-dependence problem.png|180px|left|thumb|Treating each occurrence of a variable independently]]\n\nAs an illustration, take the function <math>f</math> defined by <math>f(x) = x^2 + x.</math> The values of this function over the interval <math>[-1, 1]</math> are <math>\\left [-\\tfrac{1}{4}, 2 \\right ].</math> As the natural interval extension, it is calculated as:\n\n:<math>[-1, 1]^2 + [-1, 1] = [0,1] + [-1,1] = [-1,2],</math>\n\nwhich is slightly larger; we have instead calculated the infimum and supremum of the function <math>h(x, y)= x^2+y</math> over <math>x, y \\in [-1,1].</math> There is a better expression of <math>f</math> in which the variable <math>x</math> only appears once, namely by rewriting <math>f(x) = x^2 + x</math> as addition and squaring in the quadratic\n\n:<math>f(x) = \\left(x + \\frac{1}{2}\\right)^2 -\\frac{1}{4}.</math>\n\nSo the suitable interval calculation is\n\n:<math>\\left([-1,1] + \\frac{1}{2}\\right)^2 -\\frac{1}{4} =  \\left[-\\frac{1}{2}, \\frac{3}{2}\\right]^2 -\\frac{1}{4} = \\left[0, \\frac{9}{4}\\right] -\\frac{1}{4} = \\left[-\\frac{1}{4},2\\right]</math>\n\nand gives the correct values.\n\nIn general, it can be shown that the exact range of values can be achieved, if each variable appears only once and if <math>f</math> is continuous inside the box. However, not every function can be rewritten this way.\n\n[[File:Interval-wrapping effect.png|160px|right|thumb|Wrapping effect]]\nThe dependency of the problem causing over-estimation of the value range can go as far as covering a large range, preventing more meaningful conclusions.\n\nAn additional increase in the range stems from the solution of areas that do not take the form of an interval vector. The solution set of the linear system\n\n:<math>\\begin{cases} x = p \\\\ y = p \\end{cases} \\qquad  p\\in [-1,1]</math>\n\nis precisely the line between the points <math>(-1,-1)</math> and <math>(1,1).</math> Using interval methods results in the unit square, <math>[-1,1] \\times [-1,1].</math> This is known as the ''wrapping effect''.\n\n===Linear interval systems===\nA linear interval system consists of a matrix interval extension <math>[\\mathbf{A}] \\in [\\mathbb{R}]^{n\\times m}</math> and an interval vector <math>[\\mathbf{b}] \\in [\\mathbb{R}]^{n}</math>. We want the smallest cuboid <math>[\\mathbf{x}] \\in [\\mathbb{R}]^{m}</math>, for all vectors\n<math>\\mathbf{x} \\in \\mathbb{R}^{m}</math> which there is a pair <math>(\\mathbf{A}, \\mathbf{b})</math> with <math>\\mathbf{A} \\in [\\mathbf{A}]</math> and <math>\\mathbf{b} \\in [\\mathbf{b}]</math> satisfying\n:<math>\\mathbf{A} \\cdot \\mathbf{x} = \\mathbf{b}</math>.\n\nFor quadratic systems &ndash; in other words, for <math>n = m</math> &ndash; there can be such an interval vector <math>[\\mathbf{x}]</math>, which covers all possible solutions, found simply with the interval Gauss method.  This replaces the numerical operations, in that the linear algebra method known as Gaussian elimination becomes its interval version.  However, since this method uses the interval entities<math>[\\mathbf{A}]</math> and <math>[\\mathbf{b}]</math> repeatedly in the calculation, it can produce poor results for some problems. Hence using the result of the interval-valued Gauss only provides first rough estimates, since although it contains the entire solution set, it also has a large area outside it.\n\nA rough solution <math>[\\mathbf{x}]</math> can often be improved by an interval version of the [[Gauss–Seidel method]].\nThe motivation for this is that the <math>i</math>-th row of the interval extension of the linear equation\n:<math>\n\\begin{pmatrix}\n {[a_{11}]} & \\cdots & {[a_{1n}]} \\\\\n \\vdots & \\ddots & \\vdots  \\\\\n {[a_{n1}]} & \\cdots & {[a_{nn}]}\n\\end{pmatrix}\n\\cdot\n\\begin{pmatrix}\n{x_1} \\\\\n\\vdots \\\\\n{x_n}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n{[b_1]} \\\\\n\\vdots \\\\\n{[b_n]}\n\\end{pmatrix}\n</math>\ncan be determined by the variable <math>x_i</math> if the division <math>1/[a_{ii}]</math> is allowed.   It is therefore simultaneously\n:<math>x_j \\in [x_j]</math> and <math>x_j \\in \\frac{[b_i]- \\sum\\limits_{k \\not= j} [a_{ik}] \\cdot [x_k]}{[a_{ii}]}</math>.\nSo we can now replace <math>[x_j]</math> by\n:<math>[x_j] \\cap \\frac{[b_i]- \\sum\\limits_{k \\not= j} [a_{ik}] \\cdot [x_k]}{[a_{ii}]}</math>,\nand so the vector <math>[\\mathbf{x}]</math> by each element.\nSince the procedure is more efficient for a [[diagonally dominant matrix]], instead of the system <math> [\\mathbf{A}]\\cdot \\mathbf{x} = [\\mathbf{b}]\\mbox{,}</math> one can often try multiplying it by an appropriate rational matrix <math>\\mathbf{M}</math>  with the resulting matrix equation\n:<math>(\\mathbf{M}\\cdot[\\mathbf{A}])\\cdot \\mathbf{x} = \\mathbf{M}\\cdot[\\mathbf{b}]</math>\nleft to solve. If one chooses, for example, <math>\\mathbf{M} = \\mathbf{A}^{-1}</math> for the central matrix <math>\\mathbf{A} \\in [\\mathbf{A}]</math>, then <math>\\mathbf{M} \\cdot[\\mathbf{A}]</math> is outer extension of the identity matrix.\n\nThese methods only work well if the widths of the intervals occurring are sufficiently small.  For wider intervals it can be useful to use an interval-linear system on finite (albeit large) real number equivalent linear systems. If all the matrices <math>\\mathbf{A} \\in [\\mathbf{A}]</math> are invertible, it is sufficient to consider all possible combinations (upper and lower) of the endpoints occurring in the intervals.  The resulting problems can be resolved using conventional numerical methods. Interval arithmetic is still used to determine rounding errors.\n\nThis is only suitable for systems of smaller dimension, since with a fully occupied <math>n \\times n</math> matrix, <math>2^{n^2}</math> real matrices need to be inverted, with <math>2^n</math> vectors for the right hand side.  This approach was developed by Jiri Rohn and is still being developed.<ref>[http://www.cs.cas.cz/rohn/publist/000home.htm Jiri Rohn, List of publications]</ref>\n\n===Interval Newton method===\n[[File:Interval Newton step.png|250px|right|thumb|Reduction of the search area in the interval Newton step in \"thick\" functions]]\nAn interval variant of [[Newton's method]] for finding the zeros in an interval vector <math>[\\mathbf{x}]</math> can be derived from the average value extension.<ref name=\"Hansen\">{{cite book |author-last1=Walster |author-first1=G. William |author-last2=Hansen |author-first2=Eldon Robert |author-link2=Eldon R. Hansen |title=Global Optimization using Interval Analysis |edition=2nd |year=2004 |publisher=Marcel Dekker |location=New York, USA |isbn=0-8247-4059-9}}</ref> For an unknown vector <math>\\mathbf{z}\\in [\\mathbf{x}]</math> applied to <math>\\mathbf{y}\\in [\\mathbf{x}]</math>, gives\n:<math>f(\\mathbf{z}) \\in  f(\\mathbf{y}) + [J_f](\\mathbf{[x]}) \\cdot (\\mathbf{z} - \\mathbf{y})</math>.\nFor a zero <math>\\mathbf{z}</math>, that is <math>f(z)=0</math>, and thus must satisfy\n:<math> f(\\mathbf{y}) + [J_f](\\mathbf{[x]}) \\cdot (\\mathbf{z} - \\mathbf{y})=0 </math>.\nThis is equivalent to\n<math> \\mathbf{z} \\in \\mathbf{y} - [J_f](\\mathbf{[x]})^{-1}\\cdot f(\\mathbf{y})</math>.\nAn outer estimate of <math>[J_f](\\mathbf{[x]})^{-1}\\cdot f(\\mathbf{y}))</math> can be determined using linear methods.\n\nIn each step of the interval Newton method, an approximate starting value <math>[\\mathbf{x}]\\in [\\mathbb{R}]^n</math> is replaced by <math>[\\mathbf{x}]\\cap \\left(\\mathbf{y} - [J_f](\\mathbf{[x]})^{-1}\\cdot f(\\mathbf{y})\\right)</math> and so the result can be improved iteratively. In contrast to traditional methods, the interval method approaches the result by containing the zeros. This guarantees that the result produces all zeros in the initial range. Conversely, it proves that no zeros of <math>f</math> were in the initial range <math>[\\mathbf{x}]</math> if a Newton step produces the empty set.\n\nThe method converges on all zeros in the starting region. Division by zero can lead to separation of distinct zeros, though the separation may not be complete; it can be complemented by the [[#Bisection and covers|bisection method]].\n\nAs an example, consider the function <math>f(x)= x^2-2</math>, the starting range <math>[x] = [-2,2]</math>, and the point <math>y= 0</math>. We then have <math> J_f(x) = 2\\, x</math> and the first Newton step gives\n:<math>[-2,2]\\cap \\left(0 - \\frac{1}{2\\cdot[-2,2]} (0-2)\\right) = [-2,2]\\cap \\Big([{-\\infty}, {-0.5}]\\cup  [{0.5}, {\\infty}] \\Big) = [{-2}, {-0.5}] \\cup [{0.5}, {2}]</math>.\nMore Newton steps are used separately on <math>x\\in [{-2}, {-0.5}]</math> and <math>[{0.5}, {2}]</math>. These converge to arbitrarily small intervals around <math>-\\sqrt{2}</math> and <math>+\\sqrt{2}</math>.\n\nThe Interval Newton method can also be used with ''thick functions'' such as  <math>g(x)= x^2-[2,3]</math>, which would in any case have interval results. The result then produces intervals containing <math> \\left[-\\sqrt{3},-\\sqrt{2} \\right] \\cup \\left[\\sqrt{2},\\sqrt{3} \\right]</math>.\n\n===Bisection and covers===\n\n[[File:Illustration of interval mincing.png|220px|right|thumb|Rough estimate (turquoise) and improved estimates through \"mincing\" (red)]]\n\nThe various interval methods deliver conservative results as dependencies between the sizes of different intervals extensions are not taken into account. However the dependency problem becomes less significant for narrower intervals.\n\nCovering an interval vector <math>[\\mathbf{x}]</math> by smaller boxes <math>[\\mathbf{x}_1], \\ldots , [\\mathbf{x}_k],</math> so that\n\n:<math>[\\mathbf{x}] = \\bigcup_{i=1}^k [\\mathbf{x}_i],</math>\n\nis then valid for the range of values\n\n:<math>f([\\mathbf{x}]) =  \\bigcup_{i=1}^k f([\\mathbf{x}_i]).</math>\n\nSo for the interval extensions described above the following holds:\n\n:<math>[f]([\\mathbf{x}]) \\supseteq  \\bigcup_{i=1}^k [f]([\\mathbf{x}_i]).</math>\n\nSince <math>[f]([\\mathbf{x}])</math> is often a genuine [[superset]] of the right-hand side, this usually leads to an improved estimate.\n\nSuch a cover can be generated by the [[bisection method]] such as thick elements <math>[x_{i1}, x_{i2}]</math>  of the interval vector <math>[\\mathbf{x}] = ([x_{11}, x_{12}], \\ldots, [x_{n1}, x_{n2}])</math> by splitting in the centre into the two intervals <math>\\left [x_{i1}, \\tfrac{1}{2}(x_{i1}+x_{i2})\\right ]</math> and <math>\\left [ \\tfrac{1}{2}(x_{i1}+x_{i2}), x_{i2} \\right ].</math> If the result is still not suitable then further gradual subdivision is possible. Note that a cover of <math>2^r</math> intervals results from <math>r</math> divisions of vector elements, substantially increasing the computation costs.\n\nWith very wide intervals, it can be helpful to split all intervals into several subintervals with a constant (and smaller) width, a method known as ''mincing''. This then avoids the calculations for intermediate bisection steps. Both methods are only suitable for problems of low dimension.\n\n==Application==\nInterval arithmetic can be used in various areas (such as [[set inversion]], [[motion planning]], [[set estimation]] or stability analysis) to treat estimates with no exact numerical value.<ref>{{cite book |author-last1=Jaulin |author-first1=Luc |author-last2=Kieffer |author-first2=Michel |author-last3=Didrit |author-first3=Olivier |author-last4=Walter |author-first4=Eric |title=Applied Interval Analysis |year=2001 |publisher=Springer |location=Berlin |isbn=1-85233-219-0}}</ref>\n\n===Rounding error analysis===\nInterval arithmetic is used with error analysis, to control rounding errors arising from each calculation.\nThe advantage of interval arithmetic is that after each operation there is an interval that reliably includes the true result. The distance between the interval boundaries gives the current calculation of rounding errors directly:\n: Error = <math>\\mathrm{abs}(a-b)</math> for a given interval <math>[a,b]</math>.\nInterval analysis adds to rather than substituting for traditional methods for error reduction, such as [[pivot element|pivoting]].\n\n===Tolerance analysis===\nParameters for which no exact figures can be allocated often arise during the simulation of technical and physical processes.\nThe production process of technical components allows certain tolerances, so some parameters fluctuate within intervals.\nIn addition, many fundamental constants are not known precisely.<ref name=\"Dreyer\"/>\n\nIf the behavior of such a system affected by tolerances satisfies, for example,  <math>f(\\mathbf{x}, \\mathbf{p}) = 0</math>, for <math> \\mathbf{p} \\in [\\mathbf{p}]</math> and unknown <math>\\mathbf{x}</math> then the set of possible solutions\n:<math>\\{\\mathbf{x}\\,|\\, \\exists \\mathbf{p} \\in [\\mathbf{p}], f(\\mathbf{x}, \\mathbf{p})= 0\\}</math>,\ncan be found by interval methods. This provides an alternative to traditional [[propagation of error]] analysis.\nUnlike point methods, such as [[Monte Carlo simulation]], interval arithmetic methodology ensures that no part of the solution area can be overlooked.\nHowever, the result is always a worst-case analysis for the distribution of  error, as other probability-based distributions are not considered.\n\n===Fuzzy interval arithmetic===\n[[File:Fuzzy arithmetic.png|275px|right|thumb|Approximation of the [[normal distribution]] by a sequence of intervals]]\nInterval arithmetic can also be used with affiliation functions for fuzzy quantities as they are used in [[fuzzy logic]]. Apart from the strict statements <math>x\\in [x]</math> and <math>x \\not\\in [x]</math>, intermediate values are also possible, to which real numbers <math>\\mu \\in [0,1]</math> are assigned. <math>\\mu = 1</math> corresponds to definite membership while <math>\\mu = 0</math> is non-membership. A distribution function assigns uncertainty, which can be understood as a further interval.\n\nFor ''fuzzy arithmetic''<ref>[http://www.itm.uni-stuttgart.de/research/unsicherheiten/unsicherheiten_en.php Application of Fuzzy Arithmetic to Quantifying the Effects of Uncertain Model Parameters, Michael Hanss], [[University of Stuttgart]]</ref> only a finite number of discrete affiliation stages <math>\\mu_i \\in [0,1]</math> are considered. The form of such a distribution for an indistinct value can then represented by a sequence of intervals\n\n:<math>\\left[x^{(1)}\\right] \\supset \\left[x^{(2)}\\right] \\supset \\cdots \\supset \\left[x^{(k)} \\right].</math>\n\nThe interval <math>\\left [x^{(i)} \\right ]</math> corresponds exactly to the fluctuation range for the stage <math>\\mu_i.</math>\n\nThe appropriate distribution for a function <math>f(x_1, \\ldots, x_n)</math> concerning indistinct values <math>x_1, \\ldots, x_n</math> and the corresponding sequences\n\n:<math>\\left[x_1^{(1)} \\right] \\supset \\cdots \\supset \\left[x_1^{(k)} \\right], \\ldots, \\left[x_n^{(1)} \\right] \\supset \\cdots \\supset \\left[x_n^{(k)} \\right]</math>\n\ncan be approximated by the sequence\n\n:<math>\\left[y^{(1)}\\right] \\supset \\cdots \\supset \\left[y^{(k)}\\right],</math>\n\nwhere\n\n:<math>\\left[y^{(i)}\\right] = f \\left( \\left[x_{1}^{(i)}\\right], \\ldots \\left[x_{n}^{(i)}\\right]\\right)</math>\n\nand can be calculated by interval methods. The value <math>\\left[y^{(1)}\\right]</math> corresponds to the result of an interval calculation.\n===Computer-assisted proof===\n[[Warwick Tucker]] used interval arithmetic in order to solve the 14th of [[Smale's problems]] <ref>Tucker, W. (1999). The Lorenz attractor exists. Comptes Rendus de l'Académie des Sciences-Series I-Mathematics, 328(12), 1197-1202.</ref>\n\n==History==\nInterval arithmetic is not a completely new phenomenon in mathematics; it has appeared several times under different names in the course of history. For example, [[Archimedes]] calculated lower and upper bounds 223/71 < [[Pi#History|π]] < 22/7 in the 3rd century BC. Actual calculation with intervals has neither been as popular as other numerical techniques nor been completely forgotten.\n\nRules for calculating with intervals and other subsets of the real numbers were published in a 1931 work by Rosalind Cicely Young, a doctoral candidate at the [[University of Cambridge]]<ref>Young, R. C. (1931). The algebra of many-valued quantities. Mathematische Annalen, 104(1), 260-290.</ref>. Arithmetic work on range numbers to improve the reliability of digital systems were then published in a 1951 textbook on linear algebra by Paul Dwyer ([[University of Michigan]])<ref>Dwyer, P. S. (1951). Linear computations. Oxford, England: Wiley.</ref>; intervals were used to measure rounding errors associated with floating-point numbers. A comprehensive paper on interval algebra in numerical analysis was published by Teruo Sunaga (1958).<ref>{{cite book |author-first=Teruo |author-last=Sunaga |title=Theory of interval algebra and its application to numerical analysis |journal=RAAG Memoirs |issue<!-- ? -->=2 |date=1958 |pages=29–46}}</ref>\n\nThe birth of modern interval arithmetic was marked by the appearance of the book ''Interval Analysis'' by Ramon E. Moore in 1966.<ref>{{cite book |author-last=Moore |author-first=R. E. |title=Interval Analysis |year=1966 |publisher=[[Prentice-Hall]] |location=Englewood Cliff, New Jersey, USA |isbn=0-13-476853-1}}</ref><ref name=\"siam\">{{cite book |author-last1=Cloud |author-first1=Michael J. |author-last2=Moore |author-first2=Ramon E. |author-last3=Kearfott |author-first3=R. Baker |title=Introduction to Interval Analysis |year=2009 |publisher=[[Society for Industrial and Applied Mathematics]] (SIAM) |location=Philadelphia |isbn=0-89871-669-1}}</ref> He had the idea in Spring 1958, and a year later he published an article about computer interval arithmetic.<ref>{{cite web |url=http://interval.louisiana.edu/Moores_early_papers/bibliography.html |title=Publications Related to Early Interval Work of R. E. Moore |author-last=Hansen |author-first=E. R. |author-link=Eldon R. Hansen |date=2001-08-13 |publisher=University of Louisiana at Lafayette Press |access-date=2015-06-29}}</ref> Its merit was that starting with a simple principle, it provided a general method for automated error analysis, not just errors resulting from rounding.\n\nIndependently in 1956, Mieczyslaw Warmus suggested formulae for calculations with intervals,<ref>[http://www.ippt.gov.pl/~zkulpa/quaphys/warmus.html Precursory papers on interval analysis by M. Warmus] {{webarchive |url=https://web.archive.org/web/20080418041803/http://www.ippt.gov.pl/~zkulpa/quaphys/warmus.html |date=April 18, 2008 }}</ref> though Moore found the first non-trivial applications.\n\nIn the following twenty years, German groups of researchers carried out pioneering work around {{Interlanguage link multi|Götz Alefeld|de}}<ref>{{cite book |author-last1=Alefeld |author-first1=Götz |author-link=:de:Götz Alefeld |author-last2=Herzberger |author-first2=Jürgen |title=Einführung in die Intervallrechnung |series=Reihe Informatik |volume=12 |publisher=[[B.I.-Wissenschaftsverlag]] |location=Mannheim, Wien, Zürich |isbn=3-411-01466-0 |language=German}}</ref> and [[Ulrich Kulisch]]<ref name=\"Kulisch\">{{cite book |author-last=Kulisch |author-first=Ulrich |author-link=:de:Ulrich Kulisch |title=Wissenschaftliches Rechnen mit Ergebnisverifikation. Eine Einführung |year=1989 |publisher=[[Vieweg-Verlag]] |location=Wiesbaden |language=German |isbn=3-528-08943-1}}</ref><ref name=\"Kulisch_1969\">{{cite book |chapter=Grundzüge der Intervallrechnung |language=German |author-first=Ulrich |author-last=Kulisch |author-link=:de:Ulrich Kulisch |date=1969 |title=Jahrbuch Überblicke Mathematik |editor-first=Detlef |editor-last=Laugwitz |volume=2 |publisher=[[Bibliographisches Institut]] |location=Mannheim, Germany |pages=51–98}}</ref> at the [[University of Karlsruhe]] and later also at the [[University of Wuppertal|Bergische University of Wuppertal]].\nFor example, {{Interlanguage link multi|Karl Nickel|de}} explored more effective implementations, while improved containment procedures for the solution set of systems of equations were due to Arnold Neumaier among others. In the 1960s, [[Eldon R. Hansen]] dealt with interval extensions for linear equations and then provided crucial contributions to global optimisation, including what is now known as Hansen's method, perhaps the most widely used interval algorithm.<ref name=\"Hansen\">{{cite book |author-last1=Walster |author-first1=G. William |author-last2=Hansen |author-first2=Eldon Robert |author-link2=Eldon R. Hansen |title=Global Optimization using Interval Analysis |edition=2nd |year=2004 |publisher=Marcel Dekker |location=New York, USA |isbn=0-8247-4059-9}}</ref> Classical methods in this often have the problem of determining the largest (or smallest) global value, but could only find a local optimum and could not find better values; Helmut Ratschek and Jon George Rokne developed [[branch and bound]] methods, which until then had only applied to integer values, by using intervals to provide applications for continuous values.\n\nIn 1988, Rudolf Lohner developed [[Fortran]]-based software for reliable solutions for initial value problems using [[ordinary differential equations]].<ref>[http://fam-pape.de/raw/ralph/studium/dgl/dglsem.html Bounds for ordinary differential equations of Rudolf Lohner] (in German)</ref>\n\nThe journal ''Reliable Computing'' (originally ''Interval Computations'') has been published since the 1990s, dedicated to the reliability of computer-aided computations. As lead editor, R. Baker Kearfott, in addition to his work on global optimisation, has contributed significantly to the unification of notation and terminology used in interval arithmetic ([[#External links|Web]]: Kearfott).\n\nIn recent years work has concentrated in particular on the estimation of [[preimage]]s of parameterised functions and to robust control theory by the COPRIN working group of [[INRIA]] in [[Sophia Antipolis]] in France ([[#External links|Web]]: INRIA).\n\n=={{anchor|XSC|FORTRAN-SC|C-XSC|ACRITH|ACRITH-XSC}}Implementations==\nThere are many software packages that permit the development of numerical applications using interval arithmetic.<ref>[http://www.cs.utep.edu/interval-comp/main.html  Software for Interval Computations collected by [[Vladik Kreinovich]] ], [[University of Texas at El Paso]]</ref>\nThese are usually provided in the form of program libraries.  There are also [[C++]] and Fortran [[compiler]]s that handle interval data types and suitable operations as a language extension, so interval arithmetic is supported directly.\n\nSince 1967, ''Extensions for Scientific Computation'' (XSC) have been developed in the [[University of Karlsruhe]] for various [[programming language]]s, such as C++, Fortran and [[Pascal (programming language)|Pascal]].<ref>[http://www.math.uni-wuppertal.de/org/WRST/xsc/history.html History of XSC-Languages] {{webarchive|url=https://web.archive.org/web/20070929131317/http://www.math.uni-wuppertal.de/org/WRST/xsc/history.html |date=2007-09-29 }}</ref> The first platform was a [[Zuse]] [[Z23 (computer)|Z 23]], for which a new interval data type with appropriate elementary operators was made available. There followed in 1976, [[Pascal-SC]], a Pascal variant on a [[Zilog Z80]] that it made possible to create fast, complicated routines for automated result verification. Then came the [[Fortran 77]]-based ACRITH-XSC for the [[System/370]] architecture (FORTRAN-SC), which was later delivered by IBM. Starting from 1991 one could produce code for [[C (programming language)|C]] compilers with [[Pascal-XSC]]; a year later the C++ class library supported C-XSC on many different computer systems. In 1997, all XSC variants were made available under the [[GNU General Public License]]. At the beginning of 2000 C-XSC 2.0 was released under the leadership of the working group for scientific computation at the [[Bergische University of Wuppertal]] to correspond to the improved C++ standard.\n\nAnother C++-class library was created in 1993 at the [[Hamburg University of Technology]] called ''Profil/BIAS'' (Programmer's Runtime Optimized Fast Interval Library, Basic Interval Arithmetic), which made the usual interval operations more user friendly. It emphasized the efficient use of hardware, portability and independence of a particular presentation of intervals.\n\nThe [[Boost (C++ libraries)|Boost collection]] of C++ libraries contains a template class for intervals. Its authors are aiming to have interval arithmetic in the standard C++ language.<ref>[http://www-sop.inria.fr/members/Sylvain.Pion/cxx/ A Proposal to add Interval Arithmetic to the C++ Standard Library]</ref>\n\nThe [[Frink (programming language)|Frink]] programming language has an implementation of interval arithmetic that  handles [[arbitrary-precision arithmetic|arbitrary-precision number]]s. Programs written in Frink can use intervals without rewriting or recompilation.\n\nGaol<ref>[http://sourceforge.net/projects/gaol Gaol is Not Just Another Interval Arithmetic Library]</ref> is another C++ interval arithmetic library that is unique in that it offers the relational interval operators used in interval [[constraint programming]].\n\nThe Moore library <ref>[https://arxiv.org/abs/1611.09567 Moore: Interval Arithmetic in Modern C++]</ref> is an efficient implementation of interval arithmetic in C++. It provides intervals with endpoints of arbitrary precision and is based on the ``concepts´´ feature of C++.\n\nThe [[Julia (programming language)|Julia]] programming language<ref>[http://julialang.org The Julia programming language]</ref> has an implementation of interval arithmetics along with high-level features, such as [[Root-finding algorithm|root-finding]] (for both real and complex-valued functions) and interval [[constraint programming]], via the ValidatedNumerics.jl package.<ref>[http://github.com/JuliaIntervals/ValidatedNumerics.jl ValidatedNumerics.jl]</ref>\n\nIn addition computer algebra systems, such as [[Mathematica]], [[Maple (software)|Maple]] and [[MuPAD]], can handle intervals. A [[Matlab]] extension ''[http://www.ti3.tu-harburg.de/rump/intlab/ Intlab]'' builds on [[BLAS]] routines, and the Toolbox b4m makes a Profil/BIAS interface.<ref>[http://www.ti3.tu-harburg.de/~rump/intlab/ INTerval LABoratory] and [http://www.ti3.tu-harburg.de/zemke/b4m/ b4m]</ref> Moreover, the Software [[Euler (software)|Euler Math Toolbox]] includes an interval arithmetic.\n\nA library for the functional language OCaml was written in assembly language and C.<ref>{{citation |author-last1=Alliot |author-first1=Jean-Marc |author-last2=Gotteland |author-first2=Jean-Baptiste |author-last3=Vanaret |author-first3=Charlie |author-last4=Durand |author-first4=Nicolas |author-last5=Gianazza |author-first5=David |date=2012 |url=https://hal.archives-ouvertes.fr/hal-00934812 |title=Implementing an interval computation library for OCaml on x86/amd64 architectures |publisher=17th ACM SIGPLAN International Conference on Functional Programming}}</ref>\n\n=={{anchor|IEEE 1788}}IEEE Std 1788-2015 – IEEE standard for interval arithmetic==\nA standard for interval arithmetic has been approved in June 2015.<ref>[http://standards.ieee.org/findstds/standard/1788-2015.html IEEE Standard for Interval Arithmetic]</ref>  Two reference implementations are freely available.<ref>Revol, Nathalie (2015). The (near-)future IEEE 1788 standard for interval arithmetic. 8th small workshop on interval methods. [http://kam.mff.cuni.cz/conferences/swim2015/slides/revol.pdf Slides (PDF)]</ref> These have been developed by members of the standard's working group: The libieeep1788<ref>[https://github.com/nehmeier/libieeep1788 C++ implementation of the preliminary IEEE P1788 standard for interval arithmetic]</ref> library for C++, and the interval package<ref>[http://octave.sourceforge.net/interval/ GNU Octave interval package]</ref> for [[GNU Octave]].\n\nA minimal subset of the standard, IEEE Std 1788.1-2017, has been approved in December 2017 and published in February 2018. It should be easier to implement and may speed production of implementations<ref>{{cite web \n  |url= https://standards.ieee.org/findstds/standard/1788.1-2017.html\n  |title= IEEE Std 1788.1-2017 - IEEE Standard for Interval Arithmetic (Simplified)\n  |author=<!--Staff writer(s); no by-line.--> \n  |date= \n  |website= IEEE Standard\n  |publisher=IEEE Standards Association \n  |accessdate=2018-02-06\n}}</ref>.\n\n==Conferences and Workshop==\nSeveral international conferences or workshop take place every year in the world.  The main conference is probably SCAN (International Symposium on Scientific Computing, Computer Arithmetic, and Verified Numerical Computation), but there is also SWIM (Small Workshop on Interval Methods), PPAM (International Conference on Parallel Processing and Applied Mathematics), REC (International Workshop on Reliable Engineering Computing).\n\n==See also==\n* [[Affine arithmetic]]\n* [[Automatic differentiation]]\n* [[Multigrid method]]\n* [[Monte-Carlo simulation]]\n* [[Interval finite element]]\n* [[Fuzzy number]]\n* [[Significant figures]]\n* [[Karlsruhe Accurate Arithmetic]] (KAA)\n* [[Unum (number format)|Unum]]\n\n==References==\n{{Reflist}}\n\n==Further reading==\n* {{cite journal |last=Hayes |first=Brian |date=November–December 2003 |title=A Lucid Interval |journal=American Scientist |publisher=Sigma Xi |volume=91 |issue=6 |pages=484–488 |url=http://www.cs.utep.edu/interval-comp/hayes.pdf |doi=10.1511/2003.6.484}}\n* Tucker, W. (2011). [https://books.google.com/books?hl=en&lr=&id=IEN56sqHtR8C&oi=fnd&pg=PP1&dq=%22Validated+numerics:+a+short+introduction+to+rigorous+computations%22&ots=ehkHlj_HHd&sig=Zqs7dfpaGcEiU2vmXhwhv0TMViE#v=onepage&q=%22interval%20arithmetic%22&f=false Validated numerics: a short introduction to rigorous computations]. Princeton University Press.\n\n==External links==\n*[http://mathworld.wolfram.com/IntervalArithmetic.html Interval arithmetic (Wolfram Mathworld)]\n\n* [http://www-sop.inria.fr/coprin/logiciels/ALIAS/Movie/movie_undergraduate.mpg Introductory Film (mpeg)] of the [http://www-sop.inria.fr/coprin/index_english.html COPRIN] teams of [[INRIA]], [[Sophia Antipolis]]\n* [http://interval.louisiana.edu/kearfott.html Bibliography of R. Baker Kearfott], [[University of Louisiana at Lafayette]]\n* [http://www.mat.univie.ac.at/~neum/interval.html Interval Methods from Arnold Neumaier], [[University of Vienna]]\n* [http://www.ti3.tu-harburg.de/rump/intlab/ INTLAB, Institute for Reliable Computing], [[Hamburg University of Technology]]\n* [http://www.texmacs.org/joris/ball/ball.html Ball arithmetic by  Joris van der Hoeven]\n* [http://verifiedby.me/kv/index-e.html kv - a C++ Library for Verified Numerical Computation]\n* [http://arblib.org/ Arb - a C library for arbitrary-precision ball arithmetic]\n* [http://www2.math.uu.se/~warwick/main/papers/ECM04Tucker.pdf Validated Numerics for Pedestrians]\n{{Data types}}\n\n{{DEFAULTSORT:Interval Arithmetic}}\n[[Category:Arithmetic]]\n[[Category:Computer arithmetic]]\n[[Category:Numerical analysis]]\n[[Category:Data types]]\n[[Category:Articles with images not understandable by color blind users]]"
    },
    {
      "title": "Interval contractor",
      "url": "https://en.wikipedia.org/wiki/Interval_contractor",
      "text": "{{short description|mathematical construct}}\nIn [[mathematics]], an '''interval contractor''' (or '''contractor''' for short)<ref>{{cite book|last1=Jaulin|first1=Luc|last2=Kieffer|first2=Michel|last3=Didrit|first3=Olivier|last4=Walter|first4=Eric|\ntitle=Applied Interval Analysis|year=2001|publisher=Springer|location=Berlin|isbn=1-85233-219-0}}\n</ref> associated to a set ''X'' is  an operator ''C'' which associates to a box [''x''] in '''R'''<sup>''n''</sup> another box ''C''([''x'']) of '''R'''<sup>''n''</sup>  such that the two following properties are always satisfied\n\n* <math>\nC([x])\\subset [x]\n</math> (contractance property)\n\n* <math>\nC([x])\\cap X=[x]\\cap X\n</math> (completeness property)\n\nA ''contractor associated to a [[Constraint algorithm|constraint]]'' (such as an equation or an inequality) is a \ncontractor associated to the set ''X'' of all ''x'' which satisfy the constraint.  \nContractors make it possible to improve the efficiency of [[branch and bound|branch-and-bound]]\nalgorithms classically used in [[Interval arithmetic|interval analysis]].\n\n==Properties of contractors==\n\nA contractor ''C'' is [[Monotonic_function|monotonic]] if we have \n<math>\n[x] \\subset [y] \\Rightarrow C([x])\\subset C([y])\n</math>\n\nIt is ''minimal'' if for all boxes [''x''], we have \n<math>\nC([x])=[[x]\\cap X]\n</math>,\nwhere [''A''] is the ''interval hull'' of the set ''A'', i.e., the smallest\nbox enclosing ''A''.\n\nThe contractor ''C'' is ''thin'' if for all points ''x'',\n<math>\nC(\\{x\\})=\\{x\\}\\cap X\n</math>\nwhere {''x''} denotes the degenerated box enclosing ''x'' as a single point.\n\nThe contractor ''C'' is ''[[Idempotence|idempotent]]'' if for all boxes [''x''], we have\n<math>\n  C \\circ C([x]) = C([x]).\n</math>\n\nThe contractor ''C'' is ''[[Convergent matrix|convergent]]'' if for all sequences [''x''](''k'') of boxes containing ''x'', we have\n<math>\n[x](k)\\rightarrow x \\Rightarrow C([x](k))\\rightarrow \\{x\\}\\cap X.\n</math>\n\n==Illustration==\n\nFigure 1 represents the set ''X'' painted grey and some boxes. Some of them of degenerated, i.e., they correspond to singletons. Figure 2 represents these boxes \nafter [[contraction mapping|contraction]]. Note that no point of ''X'' has been removed by the contractor. The contractor\nis minimal for the cyan box but is pessimistic for the green one. All degenerated blue boxes are contracted to\nthe empty box. The magenta box and the red box cannot be contracted.\n\n[[File:Wiki contractor before.jpg|thumb|Figure 1: Boxes before contraction]]\n[[File:Wiki contractor after.jpg|thumb|Figure 2: Boxes after contraction]]\n\n==Contractor algebra==\n\nSome operations can be performed on contractors to build more complex contractors.\n<ref>\n{{cite journal|last1=Chabert|first1=G.|last2=Jaulin|first2=L.|\ntitle=Contractor programming|\njournal=Artificial Intelligence|\nyear=2009|volume=173|issue=11|\nurl=http://www.ensta-bretagne.fr/jaulin/paper_chabert_quimper.pdf|doi=10.1016/j.artint.2009.03.002|pages=1079–1100}}\n</ref>\nThe [[intersection]], [[Union (set theory)|the union]], the [[Composition algebra|composition]] and the repetition are defined as follows.\n\n: <math>\n( C_1 \\cap C_2)( [x]) =C_1 ( [x]) \\cap\nC_2 ( [x]) \n</math>\n\n: <math>\n( C_1 \\cup C_2 ) ( [x]) = [ C_1 ([x]) \\cup C_2 ( [x]) ] \n</math>\n\n: <math>\n( C_1 \\circ C_2) ( [x]) =C_1 ( C_2 ([x]) ) \n</math>\n\n: <math> C^\\infty ( [x])=C\\circ C\\circ C\\circ \\cdots \\circ C ([x])</math>\n\n== Building contractors ==\n\nThere exist different ways to build contractors associated to [[Equation|equations]] and [[Inequality (mathematics)|inequalities]],\nsay, ''f''(''x'') in [''y'']. Most of them are based on interval arithmetic.\nOne of the most efficient and most simple is the ''forward/backward contractor'' (also \ncalled as HC4-revise). \n<ref>\n{{cite book|last1=Messine|first1=F.|\ntitle=Méthode d’optimisation globale basée sur l’analyse d’intervalles pour la résolution de problèmes avec contraintes|year=1997|publisher=Thèse de doctorat, Institut National Polytechnique de Toulouse|\nurl=http://messine.perso.enseeiht.fr/PUBLIS/these.ps}}\n</ref>\n<ref>\n{{cite book|last1=Benhamou|first1=F.|\nlast2=Goualard|first2=F.|last3=Granvilliers|first3=L.|last4=Puget|first4=J.F.|\ntitle=Revising hull and box consistency|\nyear=1999|publisher=In Proceedings of the 1999 international conference on Logic programming|\nurl=http://goualard.free.fr/publications/files/BenhamouGoualardGranvilliersPuget-ICLP99.pdf}}\n</ref>\n\nThe principle is to evaluate ''f''(''x'') using [[interval arithmetic]] (this is the forward step).\nThe resulting [[Interval (mathematics)|interval]] is [[Intersection (set theory)|intersected]] with [''y'']. A backward evaluation of ''f''(''x'') is then performed \nin order to contract the intervals for the ''x''<sub>i</sub> (this is the backward step). We now illustrate the principle on a simple example.\n\nConsider the constraint\n<math>\n(x_1+x_2)\\cdot x_3 \\in [1,2].\n</math>\nWe can evaluate the function ''f''(''x'') by introducing the two intermediate\n[[Dependent and independent variables|variables]] ''a'' and ''b'', as follows\n\n: <math>\na =x_1+x_2\n</math>\n\n: <math>\nb =a\\cdot x_3\n</math>\n\nThe two previous constraints are called ''forward constraints''. We get the ''backward constraints'' \nby taking each forward constraint in the reverse order and isolating each variable on the right hand side.  We get\n\n: <math>\nx_3 =\\frac{b}{a} \n</math>\n\n: <math>\na =\\frac{b}{x_3} \n</math>\n\n: <math>\nx_1=a-x_2\n</math>\n\n: <math>\nx_2=a-x_1\n</math>\n\nThe resulting forward/backward contractor \n<math> \nC([x_1],[x_2],[x_3])\n</math>\nis obtained by\nevaluating the forward and the backward constraints using [[Interval arithmetic|interval analysis]].\n\n: <math>\n[a] =[x_1]+[x_2]\n</math>\n\n: <math>\n[b] =[a]\\cdot [x_3] \n</math>\n\n: <math>\n[b] =[b]\\cap [1,2]\n</math>\n\n: <math>\n[x_3] =[x_3]\\ \\cap \\ \\frac{[b]}{[a]} \n</math>\n\n: <math>\n[a] =[a]\\cap \\frac{[b]}{[x_3]} \n</math>\n\n: <math>\n[x_1] =[x_1]\\ \\cap \\ \\ [a]-[x_2] \n</math>\n\n: <math>\n[x_2] =[x_2]\\ \\cap \\ \\ [a]-[x_1]\n</math>\n\n==References==\n{{Reflist|2}}\n\n[[Category:Arithmetic]]\n[[Category:Computer arithmetic]]\n[[Category:Optimization algorithms and methods]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Interval propagation",
      "url": "https://en.wikipedia.org/wiki/Interval_propagation",
      "text": "{{one source|date=October 2013}}\n\nIn [[numerical mathematics]], '''interval propagation''' or '''interval constraint propagation''' is the problem of contracting interval domains associated to variables of '''R''' without removing any value that is consistent with a set of constraints (i.e., equations or inequalities). It can be used to [[Propagation of uncertainty|propagate uncertainties]] in the situation where [[set estimation|errors are represented by intervals]].<ref>\n{{cite book|last1=Jaulin|first1=L.|\nlast2=Braems|first2=I.|last3=Walter|first3=E.|\ntitle=Interval methods for nonlinear identification and robust control|\nyear=2002|publisher=In Proceedings of the 41st IEEE Conference on Decision and Control (CDC)|\nurl=http://www.ensta-bretagne.fr/jaulin/cdc02.pdf}}\n</ref>  Interval propagation considers an estimation problem as a [[constraint satisfaction]] problem.\n\n== Atomic contractors ==\nA contractor associated to an equation involving the variables ''x''<sub>1</sub>,...,''x''<sub>''n''</sub> is an operator which contracts the intervals [''x''<sub>1</sub>],..., [''x''<sub>''n''</sub>] (that are supposed to enclose the ''x''<sub>''i''</sub>'s) without removing any value for the variables that is consistent with the equation.\n\nA contractor is said to be ''atomic'' if it is not built as a composition of other contractors. The main theory that is used to build atomic contractors are based on [[Interval arithmetic|interval analysis]].\n\n'''Example'''. Consider for instance the equation\n\n: <math>\n   x_1+x_2 =x_3,\n</math>\n\nwhich involves the three variables ''x''<sub>1</sub>,''x''<sub>2</sub> and ''x''<sub>3</sub>.\n\nThe associated contractor is given by the following statements\n\n: <math>\n[x_3]:=[x_3] \\cap ([x_1]+[x_2])  \n</math>\n\n: <math>\n[x_1]:=[x_1] \\cap ( [x_3]-[x_2])  \n</math>\n\n: <math>\n[x_2]:=[x_2] \\cap ( [x_3]-[x_1])  \n</math>\n\nFor instance, if \n\n: <math>\nx_1 \\in [-\\infty ,5], \n</math>\n\n: <math>\nx_2 \\in [-\\infty ,4], \n</math>\n\n: <math>\nx_3 \\in [ 6,\\infty]\n</math>\n\nthe contractor performs the following calculus\n\n: <math>\nx_3=x_1+x_2 \\Rightarrow  x_3 \\in [6,\\infty ] \\cap ([-\\infty,5]+[-\\infty ,4]) =[6,\\infty ] \\cap [-\\infty ,9]=[6,9]. \n</math>\n\n: <math>\nx_1=x_3-x_2 \\Rightarrow  x_1 \\in [-\\infty ,5]\\cap ([6,\\infty]-[-\\infty ,4])  =[-\\infty ,5] \\cap [2,\\infty ]=[2,5]. \n</math>\n\n: <math>\nx_2=x_3-x_1 \\Rightarrow  x_2 \\in [-\\infty ,4]\\cap ([6,\\infty]-[-\\infty ,5])  = [-\\infty ,4] \\cap [1,\\infty ]=[1,4].\n</math>\n\n[[File:Before contraction.gif|thumb|Figure 1: boxes before contraction]]\n[[File:After contraction.gif|thumb|Figure 2: boxes after contraction]]\n\nFor other constraints, a specific algorithm for implementing the atomic contractor should be written. An illustration is the atomic contractor associated to the equation\n\n: <math>\nx_2=\\sin(x_1),\n</math>\n\nis provided by Figures 1 and 2.\n\n== Decomposition ==\nFor more complex constraints, a decomposition into atomic constraints (i.e., constraints for which an atomic contractor exists) should be performed. Consider for instance the constraint\n\n: <math>\nx+\\sin (xy) \\leq 0, \n</math>\n\ncould be decomposed into\n\n: <math>\na=xy \n</math>\n\n: <math>\nb=\\sin (a) \n</math>\n\n: <math>\nc=x+b.\n</math>\n\nThe interval domains that should be associated to the new intermediate variables are\n\n: <math>\na \\in [-\\infty ,\\infty ] ,\n</math>\n\n: <math>\n b \\in [-1 ,1 ] ,\n</math>\n\n: <math>\n c \\in [-\\infty ,0].\n</math>\n\n== Propagation ==\n\nThe principle of the interval propagation is to call all available atomic contractors until no more contraction could be observed. \n<ref>\n{{cite book|last=Cleary|first=J.L.|\ntitle= Logical arithmetic|\nyear=1987|publisher=Future Computing Systems}}\n</ref>\nAs a result of the [[Knaster-Tarski theorem]], the procedure always converges to intervals which enclose all feasible values for the variables.  A formalization of the interval propagation can be made thanks to the [[interval contractor|contractor algebra]].  Interval propagation converges quickly to the result and can deal with problems involving several hundred of variables.\n<ref>\n{{cite book|last=Jaulin|first=L.|\ntitle= Localization of an underwater robot using interval constraints propagation|\nyear=2006|publisher=In Proceedings of CP 2006|\nurl=http://www.ensta-bretagne.fr/jaulin/redermorcp06.pdf}}\n</ref>\n\n== Example ==\n\nConsider the electronic circuit of Figure 3.\n[[File:Electronic circuit to illustrate the interval propagation.gif|thumb|Figure 3: File:Electronic circuit to illustrate the interval propagation]] Assume that from different measurements, we know that\n\n: <math>\nE \\in [23V,26V]\n</math>\n\n: <math>\nI\\in [4A,8A]\n</math>\n\n: <math>\nU_1 \\in [10V,11V]\n</math>\n\n: <math>\nU_2 \\in [14V,17V]\n</math>\n\n: <math>\nP \\in [124W,130W]\n</math>\n\n: <math>\nR_{1} \\in [0 \\Omega,\\infty ]\n</math>\n\n: <math>\nR_{2} \\in [0 \\Omega,\\infty ].\n</math>\n\nFrom the circuit, we have the following equations\n\n: <math>\nP=EI\n</math>\n\n: <math>\nU_{1}=R_{1}I\n</math>\n\n: <math>\nU_{2}=R_{2}I\n</math>\n\n: <math>\nE=U_{1}+U_{2}.  \n</math>\n\nAfter performing the interval propagation, we get\n\n: <math>\nE \\in [24V,26V]\n</math>\n\n: <math>\nI \\in [4.769A,5.417A]\n</math>\n\n: <math>\nU_1 \\in [10V,11V]\n</math>\n\n: <math>\nU_2 \\in [14V,16V]\n</math>\n\n: <math>\nP \\in [124W,130W]\n</math>\n\n: <math>\nR_{1} \\in [1.846 \\Omega,2.307 \\Omega]\n</math>\n\n: <math>\n R_{2}\\in [2.584 \\Omega,3.355 \\Omega].\n</math>\n\n==References==\n{{Reflist|2}}\n\n[[Category:Algebra of random variables]]\n[[Category:Numerical analysis]]\n[[Category:Statistical approximations]]"
    },
    {
      "title": "Isotonic regression",
      "url": "https://en.wikipedia.org/wiki/Isotonic_regression",
      "text": "{{context|date=February 2012}}\n[[File:Isotonic regression.svg|thumb|400px|An example of isotonic regression (solid red line) compared to linear regression on the same data, both fit to minimize the [[mean squared error]]. The free-form property of isotonic regression means the line can be steeper where the data are steeper; the isotonicity constraint means the line does not decrease. ]]\n{{Regression bar}}\n\nIn [[statistics]], '''isotonic regression''' or '''[[monotonic]] regression''' is the technique of fitting a free-form line to a sequence of observations under the following constraints: the fitted free-form line has to be [[non-decreasing function|non-decreasing]] (or non-increasing) everywhere, and it has to lie as close to the observations as possible.\n\n== Applications ==\n\nIsotonic regression has applications in [[statistical inference]]. For example, one might use it to fit an isotonic curve to the means of some set of experimental results when an increase in those means according to some particular ordering is expected. A benefit of isotonic regression is that it is not constrained by any functional form, such as the linearity imposed by [[linear regression]], as long as the function is monotonic increasing.\n\nAnother application is nonmetric [[multidimensional scaling]],<ref>{{Cite journal | doi = 10.1007/BF02289694 | author = [[Joseph Kruskal|Kruskal, J. B.]] | year = 1964 | title = Nonmetric Multidimensional Scaling: A numerical method | url = | journal = Psychometrika | volume = 29 | issue = 2| pages = 115–129 }}</ref> where a low-dimensional [[embedding]] for data points is sought such that order of distances between points in the embedding matches [[order of dissimilarity]] between points.  Isotonic regression is used iteratively to fit ideal distances to preserve relative dissimilarity order.\n\nSoftware for computing isotone (monotonic) regression has been developed for the R statistical package <ref>{{cite journal|last1=Leeuw|first1=Jan de|last2=Hornik|first2=Kurt|last3=Mair|first3=Patrick|title=Isotone Optimization in R: Pool-Adjacent-Violators Algorithm (PAVA) and Active Set Methods|journal=Journal of Statistical Software|date=2009|volume=32|issue=5|pages=1–24|doi=10.18637/jss.v032.i05|issn=1548-7660}}</ref>, the [[Stata]] statistical package\nand the Python programming language.\n\n== Algorithms ==\n\nIn terms of [[numerical analysis]], isotonic regression involves finding a weighted [[least-squares]] fit <math>x\\in \\mathbb{R}^n</math> to a [[Euclidean space|vector]] <math>a\\in \\mathbb{R}^n</math> with weights vector <math>w\\in \\mathbb{R}^n</math> subject to a set of non-contradictory constraints of the kind <math>x_i \\le x_j</math>. The usual choice for the constraints is <math>x_i \\le x_{i+1}</math>, or in other words: every point must be at least as high as the previous point.\n\nSuch constraints define a [[partial order]]ing or [[total order]]ing and can be represented as a [[directed graph]] <math>G=(N,E)</math>, where <math>N</math> (nodes) is the set of variables (observed values) involved, and <math>E</math> (edges) is the set of pairs <math>(i, j)</math> for each constraint <math>x_i \\le x_j</math>.  Thus, the isotonic regression problem corresponds to the following [[quadratic programming|quadratic program]] (QP):\n\n:<math>\\min \\sum_{i=1}^n w_i (x_i - a_i)^2</math> <math>\\text{subject to } x_i \\le x_j \\text{ for all } (i,j) \\in E.</math>\n\nIn the case when <math>G=(N,E)</math> is a [[total order]]ing, a simple [[iterative algorithm]] for solving this quadratic program is called the [[pool adjacent violators algorithm]]. Conversely, Best and Chakravarti (1990) studied the problem as an active set identification problem, and proposed a primal algorithm. These two algorithms can be seen as each other's dual, and both have a [[computational complexity theory|computational complexity]] of <math>O(n).</math><ref>{{Cite journal | doi = 10.1007/BF01580873 |author1=Best, M.J. |author2=Chakravarti N. | year = 1990 | title = Active set algorithms for isotonic regression; a unifying framework | url = | journal = Mathematical Programming | volume = 47 | issue = 1–3| pages = 425–439 }}</ref>\n\n== Simply ordered case ==\nTo illustrate the above, let the <math>x_i \\le x_j</math> constraints be <math>x_1 \\leq x_2 \\leq \\ldots \\leq x_n</math>.\n\nThe isotonic estimator, <math>g^*</math>, minimizes the weighted least squares-like condition:\n\n: <math>\\min_{g\\in \\mathcal{A}} \\sum_{i=1}^n w_i (g(x_i) - f(x_i))^2</math>\n\nwhere <math>\\mathcal{A}</math> is the set of all piecewise linear, non-decreasing, continuous functions and <math>f</math> is a known function.\n\n== References ==\n{{Reflist}}\n\n== Further reading ==\n* {{cite book |last=Robertson |first=T. |last2=Wright |first2=F. T. |last3=Dykstra |first3=R. L. |year=1988 |title=Order restricted statistical inference |location=New York |publisher=Wiley |isbn=978-0-471-91787-8 }}\n* {{cite book |last=Barlow |first=R. E. |last2=Bartholomew |first2=D. J. |last3=Bremner |first3=J. M. |last4=Brunk |first4=H. D. |title=Statistical inference under order restrictions; the theory and application of isotonic regression |location=New York |publisher=Wiley |year=1972 |isbn=978-0-471-04970-8 }}\n* {{Cite journal | doi = 10.1111/j.1467-9868.2008.00677.x | author = Shively, T.S., Sager, T.W., Walker, S.G. | year = 2009 | title = A Bayesian approach to non-parametric monotone function estimation | url = | journal = Journal of the Royal Statistical Society, Series B | volume = 71 | issue = 1| pages = 159–175 | citeseerx = 10.1.1.338.3846 }}\n* {{Cite journal | doi = 10.1093/biomet/88.3.793 |author1=Wu, W. B. |author2=Woodroofe, M. |author3=Mentz, G. | year = 2001 | title = Isotonic regression: Another look at the changepoint problem | url = | journal = Biometrika | volume = 88 | issue = 3| pages = 793–804 }}\n\n{{Statistics|correlation}}\n\n{{DEFAULTSORT:Isotonic Regression}}\n[[Category:Nonparametric regression]]\n[[Category:Nonparametric Bayesian statistics]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Iterative method",
      "url": "https://en.wikipedia.org/wiki/Iterative_method",
      "text": "In [[computational mathematics]], an '''iterative method''' is a [[Algorithm|mathematical procedure]] that uses an initial guess to generate a sequence of improving approximate solutions for a class of problems, in which the ''n''-th approximation is derived from the previous ones. A specific implementation of an iterative method, including the [[Algorithm#Termination|termination]] criteria, is an [[algorithm]] of the iterative method. An iterative method is called '''convergent''' if the corresponding sequence converges for given initial approximations. A mathematically rigorous convergence analysis of an iterative method is usually performed; however, [[heuristic]]-based iterative methods are also common. \n\nIn contrast, '''direct methods''' attempt to solve the problem by a finite  sequence of operations. In the absence of [[rounding error]]s, direct methods would deliver an exact solution (like solving a linear system of equations <math>A\\mathbf{x}=\\mathbf{b}</math> by [[Gaussian elimination]]). Iterative methods are often the only choice for [[nonlinear equation]]s. However, iterative methods are often useful even for linear problems involving a large number of variables (sometimes of the order of millions), where direct methods would be prohibitively expensive (and in some cases impossible) even with the best available computing power.<ref>{{Cite journal|doi=10.1016/j.jcp.2015.09.040|title=Recycling Krylov subspaces for CFD applications and a new hybrid recycling solver|year=2015|last1=Amritkar|first1=Amit|last2=de Sturler|first2=Eric|last3=Świrydowicz|first3=Katarzyna|last4=Tafti|first4=Danesh|last5=Ahuja|first5=Kapil|journal=Journal of Computational Physics|volume=303|page=222|arxiv=1501.03358|bibcode=2015JCoPh.303..222A}}</ref>\n\n==Attractive fixed points==\nIf an equation can be put into the form ''f''(''x'') = ''x'', and a solution '''x''' is an attractive [[fixed point (mathematics)|fixed point]] of the function ''f'', then one may begin with a point ''x''<sub>1</sub> in the [[basin of attraction]] of '''x''', and let ''x''<sub>''n''+1</sub> = ''f''(''x''<sub>''n''</sub>) for ''n''&nbsp;≥&nbsp;1, and the sequence {''x''<sub>''n''</sub>}<sub>''n''&nbsp;≥&nbsp;1</sub> will converge to the solution '''x'''. Here ''x''<sub>''n''</sub> is the ''n''th approximation or iteration of ''x'' and ''x''<sub>''n''+1</sub> is the next or ''n'' + 1 iteration of ''x''.  Alternately, superscripts in parentheses are often used in numerical methods, so as not to interfere with subscripts with other meanings.  (For example, ''x''<sup>(''n''+1)</sup> = ''f''(''x''<sup>(''n'')</sup>).) If the function ''f'' is [[continuously differentiable]], a sufficient condition for convergence is that the [[spectral radius]] of the derivative is strictly bounded by one in a neighborhood of the fixed point.  If this condition holds at the fixed point, then a sufficiently small neighborhood (basin of attraction) must exist.\n\n==Linear systems==\nIn the case of a [[system of linear equations]], the two main classes of iterative methods are the '''stationary iterative methods''', and the more general [[Krylov subspace]] methods.\n\n===Stationary iterative methods===\n==== Introduction ====\nStationary iterative methods solve a linear system with an [[Operator (mathematics)|operator]] approximating the original one; and based on a measurement of the error in the result ([[Residual (numerical analysis)|the residual]]), form a \"correction equation\" for which this process is repeated. While these methods are simple to derive, implement, and analyze, convergence is only guaranteed for a limited class of matrices. \n\n====Definition====\nAn ''iterative method'' is defined by\n:<math>\n  \\mathbf{x}^{k+1} := \\Psi (  \\mathbf{x}^k ) \\,, \\quad k\\geq0\n</math>\nand for a given linear system <math> A\\mathbf x= \\mathbf b </math> with exact solution <math> \\mathbf{x}^* </math> the ''error'' by\n:<math>\n  \\mathbf{e}^k := \\mathbf{x}^k - \\mathbf{x}^* \\,, \\quad k\\geq0\\,.\n</math>\nAn iterative method is called ''linear'' if there exists a matrix <math> C \\in \\R^{n\\times n} </math> such that\n:<math>\n  \\mathbf{e}^{k+1} = C  \\mathbf{e}^k   \\quad \\forall \\, k\\geq0\n</math>\nand this matrix is called ''iteration matrix''.\nAn iterative method with a given iteration matrix <math> C </math> is called ''convergent'' if the following holds\n:<math>\n  \\lim_{k\\rightarrow \\infty} C^k=0\\,.\n</math>\n\nAn important theorem states that for a given iterative method and its iteration matrix <math> C </math> it is convergent if and only if its [[spectral radius]] <math> \\rho(C) </math>  is smaller than unity, that is,\n:<math>\n  \\rho(C) < 1 \\,.\n</math>\n\nThe basic iterative methods work by [[Matrix splitting|splitting]] the matrix <math> A </math> into\n:<math>\n  A = M - N\n</math>\nand here the matrix <math> M </math> should be easily [[Invertible matrix|invertible]].\nThe iterative methods are now defined as\n:<math>\n  M \\mathbf{x}^{k+1} = N \\mathbf{x}^k + b \\,, \\quad k\\geq0\\,.\n</math>\nFrom this follows that the iteration matrix is given by\n:<math>\n  C = I - M^{-1}A = M^{-1}N\\,.\n</math>\n\n====Examples====\n\nBasic examples of stationary iterative methods use a splitting of the matrix <math> A </math> such as\n:<math>\n  A = D-L-U\\,,\\quad D := \\text{diag}( (a_{ii})_i)\n</math>\nwhere <math> D </math> is only the diagonal part of <math> A </math>, and <math> L </math> is the strict lower [[Triangular_matrix|triangular part]] of <math> A </math>.\nRespectively, <math> U </math> is the upper triangular part of <math> A </math>.\n* [[Modified Richardson iteration|Richardson method]]: <math> M:=\\frac{1}{\\omega} I \\quad (\\omega \\neq 0) </math>\n* [[Jacobi method]]: <math> M:=D </math>\n* [[Jacobi_method#Weighted_Jacobi_method|Damped Jacobi method]]: <math> M:=\\frac{1}{\\omega}D \\quad (\\omega \\neq 0) </math>\n* [[Gauss–Seidel method]]: <math> M:=D-L </math>\n* [[Successive over-relaxation|Successive over-relaxation method]] (SOR): <math> M:=\\frac{1}{\\omega}D-L \\quad (\\omega \\neq 0) </math>\n* [[Symmetric successive over-relaxation]] (SSOR): <math> M := \\frac{1}{\\omega (2-\\omega)} (D-\\omega L) D^{-1} (D-\\omega U) \n\\quad (\\omega \\neq \\{0,2\\}) </math>\nLinear stationary iterative methods are also called [[Relaxation (iterative method)|relaxation methods]].\n\n===Krylov subspace methods===\n[[Krylov subspace]] methods work by forming a [[basis (linear algebra)|basis]] of the sequence of successive matrix powers times the initial residual (the '''Krylov sequence'''). \nThe approximations to the solution are then formed by minimizing the residual over the subspace formed. \nThe prototypical method in this class is the [[conjugate gradient method]] (CG) which assumes that the system matrix <math> A </math> is [[Symmetric_matrix|symmetric]] [[Positive-definite_matrix|positive-definite]].\nFor symmetric (and possibly indefinite) <math> A </math> one works with the [[minimal residual method]] (MINRES).\nIn the case of not even symmetric matrices methods, such as the [[generalized minimal residual method]] (GMRES) and the [[biconjugate gradient method]] (BiCG), have been derived.\n\n====Convergence of Krylov subspace methods====\nSince these methods form a basis, it is evident that the method converges in ''N'' iterations, where ''N'' is the system size. However, in the presence of rounding errors this statement does not hold; moreover, in practice ''N'' can be very large, and the iterative process reaches sufficient accuracy already far earlier. The analysis of these methods is hard, depending on a complicated function of the [[spectrum of an operator|spectrum]] of the operator.\n\n===Preconditioners===\nThe approximating operator that appears in stationary iterative methods can also be incorporated in [[Krylov subspace methods]] such as [[GMRES]] (alternatively, [[preconditioning|preconditioned]] Krylov methods can be considered as accelerations of stationary iterative methods), where they become transformations of the original operator to a presumably better conditioned one. The construction of preconditioners is a large research area.\n\n===History===\nProbably the first iterative method for solving a linear system appeared in a letter of [[Carl Friedrich Gauss|Gauss]] to a student of his.  He proposed solving a 4-by-4 system of equations by repeatedly solving the component in which the residual was the largest. \n\nThe theory of stationary iterative methods was solidly established with the work of [[D.M. Young]] starting in the 1950s. The Conjugate Gradient method was also invented in the 1950s, with independent developments by [[Cornelius Lanczos]], [[Magnus Hestenes]] and [[Eduard Stiefel]], but its nature and applicability were misunderstood at the time. Only in the 1970s was it realized that conjugacy based methods work very well for [[partial differential equation]]s, especially the elliptic type.\n\n== See also ==\n{{portal|Mathematics}}\n* [[Closed-form expression]]\n* [[Non-linear least squares]]\n* [[Numerical analysis]]\n* [[Root-finding algorithm]]\n\n==References==\n{{reflist}}\n\n==External links==\n{{Commonscat|Iterative methods}}\n*[http://www.netlib.org/linalg/html_templates/Templates.html Templates for the Solution of Linear Systems]\n*[http://www-users.cs.umn.edu/~saad/books.html Y. Saad: ''Iterative Methods for Sparse Linear Systems'', 1st edition, PWS 1996]\n\n{{Optimization algorithms}}\n\n[[Category:Iterative methods| ]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Jenkins–Traub algorithm",
      "url": "https://en.wikipedia.org/wiki/Jenkins%E2%80%93Traub_algorithm",
      "text": "The '''Jenkins–Traub algorithm for polynomial zeros''' is a fast globally convergent iterative method published in 1970 by [[Michael A. Jenkins]] and [[Joseph F. Traub]]. They gave two variants, one for general polynomials with complex coefficients, commonly known as the \"CPOLY\" algorithm, and a more complicated variant for the special case of polynomials with real coefficients, commonly known as the \"RPOLY\" algorithm. The latter is \"practically a standard in black-box polynomial root-finders\".<ref>Press, W. H., Teukolsky, S. A., Vetterling, W. T. and Flannery, B. P. (2007), Numerical Recipes: The Art of Scientific Computing, 3rd ed., Cambridge University Press, page 470.</ref>\n\nThis article describes the complex variant. Given a polynomial ''P'',\n\n:<math>P(z)=\\sum_{i=0}^na_iz^{n-i}, \\quad a_0=1,\\quad a_n\\ne 0</math>\n\nwith complex coefficients it computes approximations to the ''n'' zeros <math>\\alpha_1,\\alpha_2,\\dots,\\alpha_n</math> of ''P''(''z''), one at a time in roughly increasing order of magnitude. After each root is computed, its linear factor is removed from the polynomial. Using this ''deflation'' guarantees that each root is computed only once and that all roots are found.\n\nThe real variant follows the same pattern, but computes two roots at a time, either two real roots or a pair of conjugate complex roots. By avoiding complex arithmetic, the real variant can be faster (by a factor of 4) than the complex variant. The Jenkins–Traub algorithm has stimulated considerable research on theory and software for methods of this type.\n\n==Overview==\nThe Jenkins–Traub algorithm calculates all of the roots of a [[polynomial]] with complex coefficients.  The algorithm starts by checking the polynomial for the occurrence of very large or very small roots. If necessary, the coefficients are rescaled by a rescaling of the variable. In the algorithm proper, roots are found one by one and generally in increasing size. After each root is found, the polynomial is deflated by dividing off the corresponding linear factor. Indeed, the factorization of the polynomial into the linear factor and the remaining deflated polynomial is already a result of the root-finding procedure. The root-finding procedure has three stages that correspond to different variants of the [[inverse power iteration]]. See Jenkins and [[Joseph F Traub|Traub]].<ref>Jenkins, M. A. and Traub, J. F. (1970), [http://www.springerlink.com/content/q6w17w30035r2152/?p=ae17d723839045be82d270b45363625f&pi=1 A Three-Stage Variables-Shift Iteration for Polynomial Zeros and Its Relation to Generalized Rayleigh Iteration], Numer. Math. 14, 252–263.</ref>\nA description can also be found in Ralston and [[Philip Rabinowitz (mathematician)|\nRabinowitz]]<ref>Ralston, A. and Rabinowitz, P. (1978), A First Course in Numerical Analysis, 2nd ed., McGraw-Hill, New York.</ref> p.&nbsp;383.\nThe algorithm is similar in spirit to the two-stage algorithm studied by Traub.<ref>Traub, J. F. (1966), [https://www.jstor.org/stable/2004275 A Class of Globally Convergent Iteration Functions for the Solution of Polynomial Equations], Math. Comp., 20(93), 113–138.</ref>\n\n=== Root-finding procedure ===\n\nStarting with the current polynomial ''P''(''X'') of degree ''n'', the smallest root of ''P(x)'' is computed. To that end, a sequence of so-called ''H'' polynomials is constructed. These polynomials are all of degree ''n''&nbsp;&minus;&nbsp;1 and are supposed to converge to the factor of ''P''(''X'') containing all the remaining roots. The sequence of ''H'' polynomials occurs in two variants, an unnormalized variant that allows easy theoretical insights and a normalized variant of <math>\\bar H</math> polynomials that keeps the coefficients in a numerically sensible range.\n\nThe construction of the ''H'' polynomials <math>\\left(H^{(\\lambda)}(z)\\right)_{\\lambda=0,1,2,\\dots}</math> depends on a sequence of complex numbers <math>(s_\\lambda)_{\\lambda=0,1,2,\\dots}</math> called shifts. These shifts themselves depend, at least in the third stage, on the previous ''H'' polynomials. The ''H'' polynomials are defined as the solution to the implicit recursion\n:<math>\n  H^{(0)}(z)=P^\\prime(z)\n</math> and <math>\n  (X-s_\\lambda)\\cdot H^{(\\lambda+1)}(X)\\equiv H^{(\\lambda)}(X)\\pmod{P(X)}\\ .\n</math>\nA direct solution to this implicit equation is\n:<math>\n  H^{(\\lambda+1)}(X)\n    =\\frac1{X-s_\\lambda}\\cdot\n     \\left(\n        H^{(\\lambda)}(X)-\\frac{H^{(\\lambda)}(s_\\lambda)}{P(s_\\lambda)}P(X)   \n     \\right)\\,,\n</math>\nwhere the polynomial division is exact.\n\nAlgorithmically, one would use for instance the [[Horner scheme]] or [[Ruffini rule]] to evaluate the polynomials at <math>s_\\lambda</math> and obtain the quotients at the same time. With the resulting quotients ''p''(''X'') and ''h''(''X'') as intermediate results the next ''H'' polynomial is obtained as\n:<math>\n\\left.\\begin{align}\nP(X)&=p(X)\\cdot(X-s_\\lambda)+P(s_\\lambda)\\\\\nH^{(\\lambda)}(X)&=h(X)\\cdot(X-s_\\lambda)+H^{(\\lambda)}(s_\\lambda)\\\\\n\\end{align}\\right\\}\n\\implies H^{(\\lambda+1)}(z)=h(z)-\\frac{H^{(\\lambda)}(s_\\lambda)}{P(s_\\lambda)}p(z). \n</math>\nSince the highest degree coefficient is obtained from ''P(X)'', the leading coefficient of <math>H^{(\\lambda+1)}(X)</math> is <math>-\\tfrac{H^{(\\lambda)}(s_\\lambda)}{P(s_\\lambda)}</math>. If this is divided out the normalized ''H'' polynomial is\n:<math>\\begin{align}\n  \\bar H^{(\\lambda+1)}(X)\n    &=\\frac1{X-s_\\lambda}\\cdot\n     \\left(\n        P(X)-\\frac{P(s_\\lambda)}{H^{(\\lambda)}(s_\\lambda)}H^{(\\lambda)}(X)   \n     \\right)\\\\[1em]\n    &=\\frac1{X-s_\\lambda}\\cdot\n     \\left(\n        P(X)-\\frac{P(s_\\lambda)}{\\bar H^{(\\lambda)}(s_\\lambda)}\\bar H^{(\\lambda)}(X)   \n     \\right)\\,.\\end{align}\n</math>\n\n==== Stage one: no-shift process ====\nFor <math>\\lambda=0,1,\\dots, M-1</math> set <math>s_\\lambda=0</math>. Usually ''M=5'' is chosen for polynomials of moderate degrees up to ''n''&nbsp;=&nbsp;50. This stage is not necessary from theoretical considerations alone, but is useful in practice. It emphasizes  in the ''H'' polynomials the cofactor (of the linear factor) of the smallest root.\n\n==== Stage two: fixed-shift process ====\nThe shift for this stage is determined as some point close to the smallest root of the polynomial. It is quasi-randomly located on the circle with the inner root radius, which in turn is estimated as the positive solution of the equation\n:<math>\nR^n+|a_{n-1}|\\,R^{n-1}+\\dots+|a_{1}|\\,R=|a_0|\\,.\n</math>\nSince the left side is a convex function and increases monotonically from zero to infinity, this equation is easy to solve, for instance by [[Newton's method]].\n\nNow choose <math>s=R\\cdot \\exp(i\\,\\phi_\\text{random})</math> on the circle of this radius. The sequence of polynomials <math>H^{(\\lambda+1)}(z)</math>, <math>\\lambda=M,M+1,\\dots,L-1</math>, is generated with the fixed shift value <math>s_\\lambda=s</math>. During this iteration, the current approximation for the root \n:<math>t_\\lambda=s-\\frac{P(s)}{\\bar H^{(\\lambda)}(s)}</math>\nis traced. The second stage is finished successfully if the conditions \n:<math>\n  |t_{\\lambda+1}-t_\\lambda|<\\tfrac12\\,|t_\\lambda|\n</math> and <math>\n  |t_\\lambda-t_{\\lambda-1}|<\\tfrac12\\,|t_{\\lambda-1}|\n</math>\nare simultaneously met. If there was no success after some number of iterations, a different random point on the circle is tried. Typically one uses a number of 9  iterations for polynomials of moderate degree, with a doubling strategy for the case of multiple failures.\n\n==== Stage three: variable-shift process ====\nThe <math>H^{(\\lambda+1)}(X)</math> are now generated using the variable shifts <math>s_{\\lambda},\\quad\\lambda=L,L+1,\\dots</math> which are generated by \n:<math>s_L=t_L=s- \\frac{P(s)}{\\bar H^{(\\lambda)}(s)}</math>\nbeing the last root estimate of the second stage and\n:<math>s_{\\lambda+1}=s_\\lambda- \\frac{P(s_\\lambda)}{\\bar H^{(\\lambda+1)}(s_\\lambda)}, \\quad \\lambda=L,L+1,\\dots,</math>\n:where <math>\\bar H^{(\\lambda+1)}(z)</math> is the normalized ''H'' polynomial, that is <math>H^{(\\lambda)}(z)</math> divided by its leading coefficient.\n\nIf the step size in stage three does not fall fast enough to zero, then stage two is restarted using a different random point. If this does not succeed after a small number of restarts, the number of steps in stage two is doubled.\n\n==== Convergence ====\nIt can be shown that, provided ''L'' is chosen sufficiently large, ''s''<sub>λ</sub> always converges to a root of ''P''.\n\nThe algorithm converges for any distribution of roots, but may fail to find all roots of the polynomial. Furthermore, the convergence is slightly faster than the [[Rate of convergence|quadratic convergence]] of Newton–Raphson iteration, however, it uses at least twice as many operations per step.\n\n==What gives the algorithm its power?==\nCompare with the [[Newton–Raphson iteration]]\n\n:<math>z_{i+1}=z_i - \\frac{P(z_i)}{P^{\\prime}(z_i)}.</math>\n\nThe iteration uses the given ''P'' and <math>\\scriptstyle P^{\\prime}</math>. In contrast the third-stage of Jenkins–Traub\n\n:<math>\ns_{\\lambda+1}\n  =s_\\lambda- \\frac{P(s_\\lambda)}{\\bar H^{\\lambda+1}(s_\\lambda)}\n  =s_\\lambda-\\frac{W^\\lambda(s_\\lambda)}{(W^\\lambda)'(s_\\lambda)}\n</math>\n\nis precisely a Newton–Raphson iteration performed on certain [[rational functions]]. More precisely, Newton–Raphson is being performed on a sequence of rational functions\n\n:<math>W^\\lambda(z)=\\frac{P(z)}{H^\\lambda(z)}.</math>\n\nFor <math>\\lambda</math> sufficiently large,\n\n:<math>\\frac{P(z)}{\\bar H^{\\lambda}(z)}=W^\\lambda(z)\\,LC(H^{\\lambda})</math>\n\nis as close as desired to a first degree polynomial\n\n:<math>z-\\alpha_1, \\,</math>\n\nwhere <math>\\alpha_1</math> is one of the zeros of <math>P</math>. Even though Stage 3 is precisely a Newton–Raphson iteration, differentiation is not performed.\n\n=== Analysis of the ''H'' polynomials ===\nLet <math>\\alpha_1,\\dots,\\alpha_n</math> be the roots of ''P''(''X''). The so-called Lagrange factors of ''P(X)'' are the cofactors of these roots,\n:<math>P_m(X)=\\frac{P(X)-P(\\alpha_m)}{X-\\alpha_m}.</math>\nIf all roots are different, then the Lagrange factors form a basis of the space of polynomials of degree at most ''n''&nbsp;&minus;&nbsp;1. By analysis of the recursion procedure one finds that the ''H'' polynomials have the coordinate representation\n:<math>\nH^{(\\lambda)}(X)\n  =\\sum_{m=1}^n\n    \\left[\n      \\prod_{\\kappa=0}^{\\lambda-1}(\\alpha_m-s_\\kappa)\n    \\right]^{-1}\\,P_m(X)\\ .\n</math>\nEach Lagrange factor has leading coefficient 1, so that the leading coefficient of the H polynomials is the sum of the coefficients. The normalized H polynomials are thus\n:<math>\n\\bar H^{(\\lambda)}(X)\n  =\\frac{\\sum_{m=1}^n\n    \\left[\n      \\prod_{\\kappa=0}^{\\lambda-1}(\\alpha_m-s_\\kappa)\n    \\right]^{-1}\\,P_m(X)\n    }{\n    \\sum_{m=1}^n\n    \\left[\n      \\prod_{\\kappa=0}^{\\lambda-1}(\\alpha_m-s_\\kappa)\n    \\right]^{-1}\n  }\n=\\frac{P_1(X)+\\sum_{m=2}^n\n    \\left[\n      \\prod_{\\kappa=0}^{\\lambda-1}\\frac{\\alpha_1-s_\\kappa}{\\alpha_m-s_\\kappa}\n    \\right]\\,P_m(X)\n    }{\n    1+\\sum_{m=1}^n\n    \\left[\n      \\prod_{\\kappa=0}^{\\lambda-1}\\frac{\\alpha_1-s_\\kappa}{\\alpha_m-s_\\kappa}\n    \\right]\n  }\\ .\n</math>\n\n=== Convergence orders ===\nIf the condition <math>|\\alpha_1-s_\\kappa|<\\min{}_{m=2,3,\\dots,n}|\\alpha_m-s_\\kappa|</math> holds for almost all iterates, the normalized H polynomials will converge at least geometrically towards <math>P_1(X)</math>.\n\nUnder the condition that \n:<math>|\\alpha_1|<|\\alpha_2|=\\min{}_{m=2,3,\\dots,n}|\\alpha_m|</math>\none gets the asymptotic estimates for \n*stage 1:\n*:<math>\n  H^{(\\lambda)}(X)\n  =P_1(X)+O\\left(\\left|\\frac{\\alpha_1}{\\alpha_2}\\right|^\\lambda\\right).\n</math>\n*for stage 2, if ''s'' is close enough to <math>\\alpha_1</math>:\n*:<math>\n  H^{(\\lambda)}(X)\n  =P_1(X)\n    +O\\left(\n      \\left|\\frac{\\alpha_1}{\\alpha_2}\\right|^M\n        \\cdot\n      \\left|\\frac{\\alpha_1-s}{\\alpha_2-s}\\right|^{\\lambda-M}\\right)\n</math>\n*:and\n*:<math>\n  s-\\frac{P(s)}{\\bar H^{(\\lambda)}(s)}\n  =\\alpha_1+O\\left(\\ldots\\cdot|\\alpha_1-s|\\right).</math>\n*and for stage 3:\n*:<math>\n  H^{(\\lambda)}(X)\n  =P_1(X)\n    +O\\left(\\prod_{\\kappa=0}^{\\lambda-1}\n      \\left|\\frac{\\alpha_1-s_\\kappa}{\\alpha_2-s_\\kappa}\\right|\n     \\right)\n</math>\n*:and\n*:<math>\n  s_{\\lambda+1}=\n  s_\\lambda-\\frac{P(s)}{\\bar H^{(\\lambda+1)}(s_\\lambda)}\n    =\\alpha_1+O\\left(\\prod_{\\kappa=0}^{\\lambda-1}\n      \\left|\\frac{\\alpha_1-s_\\kappa}{\\alpha_2-s_\\kappa}\\right|\n        \\cdot\n      \\frac{|\\alpha_1-s_\\lambda|^2}{|\\alpha_2-s_\\lambda|}\n    \\right)\n</math>\n:giving rise to a higher than quadratic convergence order of <math>\\phi^2=1+\\phi\\approx 2.61</math>, where <math>\\phi=\\tfrac12(1+\\sqrt5)</math> is the [[golden ratio]].\n\n=== Interpretation as inverse power iteration ===\nAll stages of the Jenkins–Traub complex algorithm may be represented as the linear algebra problem of determining the eigenvalues of a special matrix. This matrix is the coordinate representation of a linear map in the ''n''-dimensional space of polynomials of degree ''n''&nbsp;&minus;&nbsp;1 or less. The principal idea of this map is to interpret the factorization\n:<math>P(X)=(X-\\alpha_1)\\cdot P_1(X)</math>\nwith a root <math>\\alpha_1\\in\\C</math> and <math>P_1(X)=P(X)/(X-\\alpha_1)</math> the remaining factor of degree ''n''&nbsp;&minus;&nbsp;1 as the eigenvector equation for the multiplication with the variable ''X'', followed by remainder computation with divisor ''P''(''X''), \n:<math>M_X(H)=(X\\cdot H(X)) \\bmod P(X)\\,.</math>\nThis maps polynomials of degree at most ''n''&nbsp;&minus;&nbsp;1 to polynomials of degree at most ''n''&nbsp;&minus;&nbsp;1. The eigenvalues of this map are the roots of ''P''(''X''), since the eigenvector equation reads\n:<math>0=(M_X-\\alpha\\cdot id)(H)=((X-\\alpha)\\cdot H) \\bmod P\\,,</math>\nwhich implies that <math>(X-\\alpha)\\cdot H)=C\\cdot P(X)</math>, that is, <math>(X-\\alpha)</math> is a linear factor of ''P''(''X''). In the monomial basis the linear map <math>M_X</math> is represented by a [[companion matrix]] of the polynomial ''P'', as\n:<math> M_X(H)=\\sum_{m=1}^{n-1}(H_{m-1}-P_{m}H_{n-1})X^m-P_0H_{n-1}\\,,</math>\nthe resulting coefficient matrix is\n:<math>A=\\begin{pmatrix}\n0 & 0 & \\dots & 0 & -P_0 \\\\\n1 & 0 & \\dots & 0 & -P_1 \\\\\n0 & 1 & \\dots & 0 & -P_2 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & \\dots & 1 & -P_{n-1}\n\\end{pmatrix}\\,.</math>\nTo this matrix the [[inverse power iteration]] is applied in the three variants of no shift, constant shift and generalized Rayleigh shift in the three stages of the algorithm. It is more efficient to perform the linear algebra operations in polynomial arithmetic and not by matrix operations, however, the properties of the inverse power iteration remain the same.\n\n==Real coefficients==\nThe Jenkins–Traub algorithm described earlier works for polynomials with complex coefficients. The same authors also created a three-stage algorithm for polynomials with real coefficients. See Jenkins and Traub [https://www.jstor.org/stable/2949376 A Three-Stage Algorithm for Real Polynomials Using Quadratic Iteration].<ref>Jenkins, M. A. and Traub, J. F. (1970), [https://www.jstor.org/stable/2949376 A Three-Stage Algorithm for Real Polynomials Using Quadratic Iteration], SIAM J. Numer. Anal., 7(4), 545–566.</ref> The algorithm finds either a linear or quadratic factor working completely in real arithmetic. If the complex and real algorithms are applied to the same real polynomial, the real algorithm is about four times as fast. The real algorithm always converges and the rate of convergence is greater than second order.\n\n==A connection with the shifted QR algorithm==\nThere is a surprising connection with the shifted QR algorithm for computing matrix eigenvalues. See Dekker and Traub [http://linkinghub.elsevier.com/retrieve/pii/0024379571900358 The shifted QR algorithm for Hermitian matrices].<ref>Dekker, T. J. and Traub, J. F. (1971), [http://linkinghub.elsevier.com/retrieve/pii/0024379571900358 The shifted QR algorithm for Hermitian matrices], Lin. Algebra Appl., 4(2), 137–154.</ref> Again the shifts may be viewed as Newton-Raphson iteration on a sequence of rational functions converging to a first degree polynomial.\n\n==Software and testing==\nThe software for the Jenkins–Traub algorithm was published as Jenkins and Traub [http://portal.acm.org/citation.cfm?id=361262&coll=portal&dl=ACM Algorithm 419: Zeros of a Complex Polynomial].<ref>Jenkins, M. A. and Traub, J. F. (1972), [http://portal.acm.org/citation.cfm?id=361262&coll=portal&dl=ACM Algorithm 419: Zeros of a Complex Polynomial], Comm. ACM, 15, 97–99.</ref> The software for the real algorithm was published as Jenkins [http://portal.acm.org/citation.cfm?id=355643&coll=ACM&dl=ACM Algorithm 493: Zeros of a Real Polynomial].<ref>Jenkins, M. A. (1975), [http://portal.acm.org/citation.cfm?id=355643&coll=ACM&dl=ACM Algorithm 493: Zeros of a Real Polynomial], ACM TOMS, 1, 178–189.</ref>\n\nThe methods have been extensively tested by many people. As predicted they enjoy faster than quadratic convergence for all distributions of zeros.\n\nHowever, there are polynomials which can cause loss of precision as illustrated by the following example. The polynomial has all its zeros lying on two half-circles of different radii. [[James H. Wilkinson|Wilkinson]] recommends that it is desirable for stable deflation that smaller zeros be computed first. The second-stage shifts are chosen so that the zeros on the smaller half circle are found first. After deflation the polynomial with the zeros on the half circle is known to be ill-conditioned if the degree is large; see Wilkinson,<ref>Wilkinson, J. H. (1963), Rounding Errors in Algebraic Processes, Prentice Hall, Englewood Cliffs, N.J.</ref> p.&nbsp;64. The original polynomial was of degree 60 and suffered severe deflation instability.\n\n==References==\n{{reflist}}\n\n==External links==\n*[http://www.hvks.com/Numerical/winsolve.html A free downloadable Windows application using the Jenkins–Traub Method for polynomials with real and complex coefficients]\n*[https://github.com/sweeneychris/RpolyPlusPlus RPoly++] An SSE-Optimized C++ implementation of the RPOLY algorithm.\n\n{{Root-finding algorithms}}\n\n{{DEFAULTSORT:Jenkins-Traub Algorithm}}\n[[Category:Numerical analysis]]\n[[Category:Root-finding algorithms]]"
    },
    {
      "title": "Kahan summation algorithm",
      "url": "https://en.wikipedia.org/wiki/Kahan_summation_algorithm",
      "text": "In [[numerical analysis]], the '''Kahan summation algorithm''', also known as '''compensated summation''',<ref>Strictly, there exist other variants of compensated summation as well: see {{cite book|first=Nicholas | last=Higham |title=Accuracy and Stability of Numerical Algorithms (2 ed)| publisher=SIAM|year=2002 | pages=110–123 | isbn=978-0-89871-521-7}}</ref> significantly reduces the [[numerical error]] in the total obtained by adding a [[sequence]] of finite [[decimal precision|precision]] [[floating point number]]s, compared to the obvious approach. This is done by keeping a separate ''running compensation'' (a variable to accumulate small errors).\n\nIn particular, simply summing ''n'' numbers in sequence has a worst-case error that grows proportional to ''n'', and a [[root mean square]] error that grows as <math>\\sqrt{n}</math> for random inputs (the roundoff errors form a [[random walk]]).<ref name=Higham93>{{Citation | title=The accuracy of floating point summation | first1=Nicholas J. | last1=Higham | journal=[[SIAM Journal on Scientific Computing]] |\nvolume=14 | issue=4 | pages=783–799 | doi=10.1137/0914050 | year=1993 | citeseerx=10.1.1.43.3535 | url=https://pdfs.semanticscholar.org/5c17/9d447a27c40a54b2bf8b1b2d6819e63c1a69.pdf\n}}</ref>  With compensated summation, the worst-case [[error bound]] is independent of ''n'', so a large number of values can be summed with an error that only depends on the floating-point [[precision (arithmetic)|precision]].<ref name=Higham93/>\n\nThe [[algorithm]] is attributed to [[William Kahan]].<ref name=\"kahan65\">{{Citation|last=Kahan|first=William|title=Further remarks on reducing truncation errors|date=January 1965|url=http://mgnet.org/~douglas/Classes/na-sc/notes/kahan.pdf|journal=[[Communications of the ACM]]|volume=8|issue=1|page=40|archive-url=https://web.archive.org/web/20180209003010/http://mgnet.org/~douglas/Classes/na-sc/notes/kahan.pdf|doi=10.1145/363707.363723|archive-date=9 February 2018}}\n</ref> Similar, earlier techniques are, for example, [[Bresenham's line algorithm]], keeping track of the accumulated error in integer operations (although first documented around the same time<ref>{{cite journal |first=Jack E. |last=Bresenham |url=http://www.research.ibm.com/journal/sj/041/ibmsjIVRIC.pdf |title=Algorithm for computer control of a digital plotter |journal=IBM Systems Journal |volume=4 |issue=1 |date=January 1965 |pages=25–30|doi=10.1147/sj.41.0025 }}</ref>) and the [[delta-sigma modulation]]<ref>{{cite journal |first1=H. |last1=Inose |first2=Y. |last2=Yasuda |first3=J. |last3=Murakami |title=A Telemetering System by Code Manipulation – ΔΣ Modulation |journal=IRE Transactions on Space Electronics and Telemetry |date=September 1962 |pages=204–209}}</ref> (integrating, not just summing the error).\n\n==The algorithm==\nIn [[pseudocode]], the algorithm is:\n\n '''function''' KahanSum(input)\n   '''variables''' sum,c,y,t,i          // Local to the routine.\n     sum = 0.0                    // Prepare the accumulator.\n     c = 0.0                      // A running compensation for lost low-order bits.\n     '''for''' i = 1 '''to''' input.length '''do''' // The array ''input'' has elements indexed input[1] to input[input.length].\n         y = input[i] - c         // ''c'' is zero the first time around.\n         t = sum + y              // Alas, ''sum'' is big, ''y'' small, so low-order digits of ''y'' are lost.\n         c = (t - sum) - y        // ''(t - sum)'' cancels the high-order part of ''y''; subtracting ''y'' recovers negative (low part of ''y'')\n         sum = t                  // Algebraically, ''c'' should always be zero. Beware overly-aggressive optimizing compilers!\n     '''next''' i                       // Next time around, the lost low part will be added to ''y'' in a fresh attempt.\n     '''return''' sum\n\n===Worked example===\nThis example will be given in decimal. Computers typically use binary arithmetic, but the principle being illustrated is the same. Suppose we are using six-digit decimal floating point arithmetic, <code>sum</code> has attained the value 10000.0, and the next two values of <code>input[i]</code> are 3.14159 and 2.71828. The exact result is 10005.85987, which rounds to 10005.9. With a plain summation, each incoming value would be aligned with <code>sum</code> and many low order digits lost (by truncation or rounding). The first result, after rounding, would be 10003.1. The second result would be 10005.81828 before rounding, and 10005.8 after rounding. This is not correct.\n\nHowever, with compensated summation, we get the correct rounded result of 10005.9.\n\nAssume that <code>c</code> has the initial value zero.\n   y = 3.14159 - 0                   ''y = input[i] - c''\n   t = 10000.0 + 3.14159\n     = 10003.14159                   But only six digits are retained.\n     = 10003.1                       Many digits have been lost!\n   c = (10003.1 - 10000.0) - 3.14159 This '''must''' be evaluated as written! \n     = 3.10000 - 3.14159             The assimilated part of ''y'' recovered, vs. the original full ''y''.\n     = -.0415900                     Trailing zeros shown because this is six-digit arithmetic.\n sum = 10003.1                       Thus, few digits from ''input(i'') met those of ''sum''.\n\nThe sum is so large that only the high-order digits of the input numbers are being accumulated. But on the next step, <code>c</code> gives the error.\n   y = 2.71828 - (-.0415900)         The shortfall from the previous stage gets included.\n     = 2.75987                       It is of a size similar to ''y'': most digits meet.\n   t = 10003.1 + 2.75987             But few meet the digits of ''sum''.\n     = 10005.85987                   And the result is rounded\n     = 10005.9                       To six digits.\n   c = (10005.9 - 10003.1) - 2.75987 This extracts whatever went in.\n     = 2.80000 - 2.75987             In this case, too much.\n     = .040130                       But no matter, the excess would be subtracted off next time.\n sum = 10005.9                       Exact result is 10005.85987, this is correctly rounded to 6 digits.\n\nSo the summation is performed with two accumulators: <code>sum</code> holds the sum, and <code>c</code> accumulates the parts not assimilated into <code>sum</code>, to nudge the low-order part of <code>sum</code> the next time around. Thus the summation proceeds with \"guard digits\" in <code>c</code> which is better than not having any but is not as good as performing the calculations with double the precision of the input. However, simply increasing the precision of the calculations is not practical in general; if <code>input</code> is already double precision, few systems supply [[quadruple precision]], and if they did, <code>input</code> could then be quadruple precision.\n\n==Accuracy==\nA careful analysis of the errors in compensated summation is needed to appreciate its accuracy characteristics.  While it is more accurate than naive summation, it can still give large relative errors for ill-conditioned sums.\n\nSuppose that one is summing ''n'' values ''x''<sub>''i''</sub>, for ''i''&nbsp;=&nbsp;1,...,''n''.  The exact sum is:\n:<math>S_n=\\sum_{i=1}^n x_i</math> (computed with infinite precision)\nWith compensated summation, one instead obtains <math>S_n+E_n</math>, where the error <math>E_n</math> is bounded by:<ref name=Higham93/>\n:<math>|E_n|\\le\\left[2\\varepsilon + O(n\\varepsilon^2)\\right]\\sum_{i=1}^n |x_i|</math>\nwhere ''ε'' is the [[machine precision]] of the arithmetic being employed (e.g. ''ε''&nbsp;≈&nbsp;10<sup>&minus;16</sup> for IEEE standard [[double precision]] floating point).  Usually, the quantity of interest is the [[relative error]] <math>|E_n|/|S_n|</math>, which is therefore bounded above by:\n:<math>\\frac{|E_n|}{|S_n|}\\le\\left[2\\varepsilon + O(n\\varepsilon^2)\\right] \\frac{\\sum\\limits_{i=1}^n |x_i|}{\\left|\\sum\\limits_{i=1}^n x_i\\right|}. </math>\n\nIn the expression for the relative error bound, the fraction Σ|''x<sub>i</sub>''|/|Σ''x<sub>i</sub>''| is the [[condition number]] of the summation problem.  Essentially, the condition number represents the ''intrinsic'' sensitivity of the summation problem to errors, regardless of how it is computed.<ref>{{cite book |first1=Lloyd N. |authorlink1=Lloyd N. Trefethen |last1=Trefethen |first2=David |last2=Bau |title=Numerical Linear Algebra |publisher=SIAM |location=Philadelphia |year=1997 |isbn=978-0-89871-361-9}}</ref>  The relative error bound of ''every'' ([[backwards stable]]) summation method by a fixed algorithm in fixed precision (i.e. not those that use [[arbitrary precision]] arithmetic, nor algorithms whose memory and time requirements change based on the data), is proportional to this condition number.<ref name=Higham93/>  An ''ill-conditioned'' summation problem is one in which this ratio is large, and in this case even compensated summation can have a large relative error.  For example, if the summands ''x<sub>i</sub>'' are uncorrelated random numbers with zero mean, the sum is a [[random walk]] and the condition number will grow proportional to <math>\\sqrt{n}</math>.  On the other hand, for random inputs with nonzero mean the condition number asymptotes to a finite constant as <math>n\\to\\infty</math>.  If the inputs are all [[non-negative]], then the condition number is 1.\n\nGiven a condition number, the relative error of compensated summation is effectively independent of ''n''.  In principle, there is the O(''nε''<sup>2</sup>) that grows linearly with ''n'', but in practice this term is effectively zero: since the final result is rounded to a precision ''ε'', the ''nε''<sup>2</sup> term rounds to zero unless ''n'' is roughly 1/''ε'' or larger.<ref name=Higham93/>  In double precision, this corresponds to an ''n'' of roughly 10<sup>16</sup>, much larger than most sums.  So, for a fixed condition number, the errors of compensated summation are effectively ''O''(''ε''), independent of&nbsp;''n''.\n\nIn comparison, the relative error bound for naive summation (simply adding the numbers in sequence, rounding at each step) grows as <math>O(\\varepsilon n)</math> multiplied by the condition number.<ref name=Higham93/>  This worst-case error is rarely observed in practice, however, because it only occurs if the rounding errors are all in the same direction. In practice, it is much more likely that the rounding errors have a random sign, with zero mean, so that they form a random walk; in this case, naive summation has a [[root mean square]] relative error that grows as <math>O(\\varepsilon \\sqrt{n})</math> multiplied by the condition number.<ref name=Tasche>Manfred Tasche and Hansmartin Zeuner ''Handbook of Analytic-Computational Methods in Applied Mathematics'' Boca Raton, FL: CRC Press, 2000).</ref>  This is still much worse than compensated summation, however.  Note, however, that if the sum can be performed in twice the precision, then ''ε'' is replaced by ''ε''<sup>2</sup> and naive summation has a worst-case error comparable to the O(''nε''<sup>2</sup>) term in compensated summation at the original precision.\n\nBy the same token, the Σ|''x<sub>i</sub>''| that appears in <math>E_n</math> above is a worst-case bound that occurs only if all the rounding errors have the same sign (and are of maximum possible magnitude).<ref name=Higham93/>  In practice, it is more likely that the errors have random sign, in which case terms in Σ|''x<sub>i</sub>''| are replaced by a random walk&mdash;in this case, even for random inputs with zero mean, the error <math>E_n</math> grows only as <math>O(\\varepsilon \\sqrt{n})</math> (ignoring the ''nε''<sup>2</sup> term), the same rate the sum <math>S_n</math> grows, canceling the <math>\\sqrt{n}</math> factors when the relative error is computed.  So, even for asymptotically ill-conditioned sums, the relative error for compensated summation can often be much smaller than a worst-case analysis might suggest.\n\n==Further enhancements==\nNeumaier<ref name=Neumaier74>{{cite journal |first=A. |last=Neumaier |doi=10.1002/zamm.19740540106 |title=Rundungsfehleranalyse einiger Verfahren zur Summation endlicher Summen |trans-title=Rounding Error Analysis of Some Methods for Summing Finite Sums |language=de |journal=Zeitschrift für Angewandte Mathematik und Mechanik |volume=54 |issue=1 |date=1974 |pages=39–51 |url=http://www.mat.univie.ac.at/~neum/scan/01.pdf}}</ref>  introduced a slight modification of Kahan's algorithm that also covers the case when the next term to be added is larger in absolute value than the running sum, effectively swapping the role of what is large and what is small. In [[pseudocode]], the algorithm is:\n \n  '''function''' NeumaierSum(input)\n     '''var''' sum = 0.0\n     '''var''' c = 0.0                 // A running compensation for lost low-order bits.\n     '''for''' i = 1 '''to''' input.length '''do'''\n         '''var''' t = sum + input[i]\n         '''if''' |sum| >= |input[i]| '''then'''\n             c += (sum - t) + input[i] // If ''sum'' is bigger, low-order digits of ''input[i]'' are lost.\n         '''else'''\n             c += (input[i] - t) + sum // Else low-order digits of ''sum'' are lost\n         '''endif'''\n         sum = t\n     '''next''' i\n     '''return''' sum + c              // Correction only applied once in the very end\n\nFor many sequences of numbers, both algorithms agree but a simple example due to Peters<ref name=\"python_fsum\" /> shows how they can differ. For summing <math>[1.0, +10^{100}, 1.0, -10^{100}]</math> in double precision, Kahan's algorithm yields 0.0 whereas Neumaier's algorithm yields the correct value 2.0.\n\n==Alternatives==\nAlthough Kahan's algorithm achieves <math>O(1)</math> error growth for summing ''n'' numbers, only slightly worse <math>O(\\log n)</math> growth can be achieved by [[pairwise summation]]: one [[recursively]] divides the set of numbers into two halves, sums each half, and then adds the two sums.<ref name=Higham93/>  This has the advantage of requiring the same number of arithmetic operations as the naive summation (unlike Kahan's algorithm, which requires four times the arithmetic and has a latency of four times a simple summation) and can be calculated in parallel.  The base case of the recursion could in principle be the sum of only one (or zero) numbers, but to [[amortize]] the overhead of recursion one would normally use a larger base case.  The equivalent of pairwise summation is used in many [[fast Fourier transform]] (FFT) algorithms, and is responsible for the logarithmic growth of roundoff errors in those FFTs.<ref>S. G. Johnson and M. Frigo, \"[http://cnx.org/content/m16336/latest/ Implementing FFTs in practice], in ''[http://cnx.org/content/col10550/ Fast Fourier Transforms]'', edited by [[C. Sidney Burrus]](2008).</ref> In practice, with roundoff errors of random signs, the root mean square errors of pairwise summation actually grow as <math>O(\\sqrt{\\log n})</math>.<ref name=Tasche/>\n\nAnother alternative is to use [[arbitrary precision arithmetic]], which in principle need no rounding at all with a cost of much greater computational effort.  A way of performing exactly rounded sums using arbitrary precision is to extend adaptively using multiple floating-point components. This will minimize computational cost in common cases where high precision is not needed.<ref>Jonathan R. Shewchuk, [http://www.cs.berkeley.edu/~jrs/papers/robustr.pdf Adaptive Precision Floating-Point Arithmetic and Fast Robust Geometric Predicates], ''Discrete and Computational Geometry'', vol. 18, pp. 305–363 (October 1997).</ref><ref name=\"python_fsum\">Raymond Hettinger, [http://code.activestate.com/recipes/393090/ Recipe 393090: Binary floating point summation accurate to full precision], Python implementation of algorithm from Shewchuk (1997) paper (28 March 2005).</ref>  Another method that uses only integer arithmetic, but a large accumulator was described by Kirchner and Kulisch;<ref>R. Kirchner, U. W. Kulisch, ''Accurate arithmetic for vector processors'', Journal of Parallel and Distributed Computing 5 (1988) 250-270</ref> a hardware implementation was described by Müller, Rüb and Rülling.<ref>M. Muller, C. Rub, W. Rulling [http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=145535&isnumber=3902], ''Exact accumulation of floating-point numbers'', Proceedings [[10th IEEE Symposium on Computer Arithmetic]] (Jun 1991), doi 10.1109/ARITH.1991.145535</ref>\n\n==Possible invalidation by compiler optimization==\nIn principle, a sufficiently aggressive [[Compiler optimization|optimizing compiler]] could destroy the effectiveness of Kahan summation: for example, if the compiler simplified expressions according to the [[associativity]] rules of real arithmetic, it might \"simplify\" the second step in the sequence <code>t = sum + y; c = (t - sum) - y;</code> to <code>((sum + y) - sum) - y;</code> then to <code>c = 0;</code>, eliminating the error compensation.<ref name=Goldberg91>{{Citation | title=What every computer scientist should know about floating-point arithmetic |first1=David | last1=Goldberg |\njournal=[[ACM Computing Surveys]] | volume=23 | issue=1 | pages=5–48 | date=March 1991 |\ndoi=10.1145/103162.103163 | url=http://www.validlab.com/goldberg/paper.pdf }}</ref>  In practice, many compilers do not use associativity rules (which are only approximate in floating-point arithmetic) in simplifications unless explicitly directed to do so by compiler options enabling \"unsafe\" optimizations,<ref>[[GNU Compiler Collection]] manual, version 4.4.3: [https://gcc.gnu.org/onlinedocs/gcc-4.4.3/gcc/Optimize-Options.html 3.10 Options That Control Optimization], ''-fassociative-math'' (Jan. 21, 2010).</ref><ref>''[http://h21007.www2.hp.com/portal/download/files/unprot/Fortran/docs/unix-um/dfumperf.htm Compaq Fortran User Manual for Tru64 UNIX and Linux Alpha Systems]'', section 5.9.7 Arithmetic Reordering Optimizations (retrieved March 2010).</ref><ref>Börje Lindh, [http://www.sun.com/blueprints/0302/optimize.pdf Application Performance Optimization], ''Sun BluePrints OnLine'' (March 2002).</ref><ref>Eric Fleegal, \"[http://msdn.microsoft.com/en-us/library/aa289157%28VS.71%29.aspx Microsoft Visual C++ Floating-Point Optimization]\", ''Microsoft Visual Studio Technical Articles''  (June 2004).</ref> although the [[Intel C++ Compiler]] is one example that allows associativity-based transformations by default.<ref>Martyn J. Corden, \"[http://software.intel.com/en-us/articles/consistency-of-floating-point-results-using-the-intel-compiler/ Consistency of floating-point results using the Intel compiler],\" ''Intel technical report'' (Sep. 18, 2009).</ref>  The original [[K&R C]] version of the [[C programming language]] allowed the compiler to re-order floating-point expressions according to real-arithmetic associativity rules, but the subsequent [[ANSI C]] standard prohibited re-ordering in order to make C better suited for numerical applications (and more similar to [[Fortran]], which also prohibits re-ordering),<ref>{{cite journal |first=Tom |last=MacDonald |title=C for Numerical Computing |journal=Journal of Supercomputing |volume=5 |issue=1 |pages=31–48 |year=1991 |doi=10.1007/BF00155856}}</ref> although in practice compiler options can re-enable re-ordering as mentioned above.\n\n==Support by libraries==\nIn general, built-in \"sum\" functions in computer languages typically provide no guarantees that a particular summation algorithm will be employed, much less Kahan summation.{{Citation needed|date=February 2010}}  The [[BLAS]] standard for [[linear algebra]] subroutines explicitly avoids mandating any particular computational order of operations for performance reasons,<ref>[http://www.netlib.org/blas/blast-forum/ BLAS Technical Forum], section 2.7 (August 21, 2001), [https://web.archive.org/web/20040410160918/http://www.netlib.org/blas/blast-forum/chapter2.pdf#page=17 Archived on Wayback Machine].</ref> and BLAS implementations typically do not use Kahan summation.\n \nThe standard library of the [[Python (programming language)|Python]] computer language specifies an [https://docs.python.org/library/math.html#math.fsum fsum] function for exactly rounded summation, using the [[Jonathan Shewchuk|Shewchuk]] algorithm<ref name=\"python_fsum\"/> to track multiple partial sums.\n\nIn the [[Julia (programming language)|Julia]] language, the default implementation of the <code>sum</code> function does [[pairwise summation]] for high accuracy with good performance,<ref>[https://github.com/JuliaLang/julia/pull/4039 RFC: use pairwise summation for sum, cumsum, and cumprod], github.com/JuliaLang/julia pull request #4039 (August 2013).</ref> but an external library provides an implementation of Neumaier's variant named <code>sum_kbn</code> for the cases when higher accuracy is needed.<ref>[https://github.com/JuliaMath/KahanSummation.jl KahanSummation library] in Julia.</ref>\n\nIn the [[C Sharp (programming language)|C Sharp]] language, [https://www.nuget.org/packages/HPCsharp HPCsharp nuget package] implements the Neumaier variant and [[pairwise summation]]: both as scalar, data parallel using [[SIMD]] processor instructions, and parallel multi-core.\n<ref>https://github.com/DragonSpit/HPCsharp HPCsharp nuget package of high performance algorithms</ref>\n\n==See also==\n* [[Algorithms for calculating variance]], which includes stable summation\n\n==References==\n{{reflist|30em}}\n\n==External links==\n* [http://www.ddj.com/cpp/184403224 Floating-point Summation, Dr. Dobb's Journal September, 1996]\n\n{{DEFAULTSORT:Kahan Summation Algorithm}}\n[[Category:Computer arithmetic]]\n[[Category:Numerical analysis]]\n[[Category:Articles with example pseudocode]]"
    },
    {
      "title": "Kantorovich theorem",
      "url": "https://en.wikipedia.org/wiki/Kantorovich_theorem",
      "text": "The '''Kantorovich theorem''', or Newton–Kantorovich theorem, is a mathematical statement on the semi-local [[Limit of a sequence|convergence]] of [[Newton's method]]. It was first stated by [[Leonid Kantorovich]] in 1948.<ref name=\"Deuflhard\">{{cite book |first=P. |last=Deuflhard |title=Newton Methods for Nonlinear Problems. Affine Invariance and Adaptive Algorithms |publisher=Springer |location=Berlin |year=2004 |isbn=3-540-21099-7 |series=Springer Series in Computational Mathematics |volume=Vol. 35 }}</ref><ref name=\"Zeidler\">{{cite book |last=Zeidler |first=E. |year=1985 |title=Nonlinear Functional Analysis and its Applications: Part 1: Fixed-Point Theorems |location=New York |publisher=Springer |isbn=0-387-96499-1 }}</ref> It is similar to the form of the [[Banach fixed-point theorem]], although it does not assume the existence of a [[Fixed point (mathematics)|fixed point]].<ref>{{cite book |first=John E. |last=Dennis |authorlink=John E. Dennis |first2=Robert B. |last2=Schnabel |authorlink2=Robert B. Schnabel |chapter=The Kantorovich and Contractive Mapping Theorems |title=Numerical Methods for Unconstrained Optimization and Nonlinear Equations |location=Englewood Cliffs |publisher=Prentice-Hall |year=1983 |pages=92–94 |isbn=0-13-627216-9 }}</ref>\n\nNewton's method constructs a sequence of points that under certain conditions will converge to a solution <math>x</math> of an equation <math>f(x)=0</math> or a vector solution of a system of equation <math>F(x)=0</math>. The Kantorovich theorem gives conditions on the initial point of this sequence. If those conditions are satisfied then a solution exists close to the initial point and the sequence converges to that point.<ref name=\"Deuflhard\"/><ref name=\"Zeidler\"/>\n\n== Assumptions ==\nLet <math>X\\subset\\R^n</math> be an open subset and <math>F:\\R^n\\supset X\\to\\R^n</math> a [[differentiable function]] with a [[Jacobian matrix and determinant|Jacobian]] <math>F^{\\prime}(\\mathbf x)</math> that is locally [[Lipschitz continuous]] (for instance if <math>F</math> is twice differentiable). That is, it is assumed that for any open subset <math>U\\subset X</math> there exists a constant <math>L>0</math> such that for any <math>\\mathbf x,\\mathbf y\\in U</math>\n\n:<math>\\|F'(\\mathbf x)-F'(\\mathbf y)\\|\\le L\\;\\|\\mathbf x-\\mathbf y\\|</math>\n\nholds. The norm on the left is some operator norm that is compatible with the vector norm on the right. This inequality can be rewritten to only use the vector norm. Then for any vector <math>\\mathbf v\\in\\R^n</math> the inequality\n\n:<math>\\|F'(\\mathbf x)(\\mathbf v)-F'(\\mathbf y)(\\mathbf v)\\|\\le L\\;\\|\\mathbf x-\\mathbf y\\|\\,\\|\\mathbf v\\|</math> \n\nmust hold.\n\nNow choose any initial point <math>\\mathbf x_0\\in X</math>. Assume that <math>F'(\\mathbf x_0)</math> is invertible and construct the Newton step <math>\\mathbf h_0=-F'(\\mathbf x_0)^{-1}F(\\mathbf x_0).</math>\n\nThe next assumption is that not only the next point <math>\\mathbf x_1=\\mathbf x_0+\\mathbf h_0</math> but the entire ball <math>B(\\mathbf x_1,\\|\\mathbf h_0\\|)</math> is contained inside the set <math>X</math>. Let <math>M\\le L</math> be the Lipschitz constant for the Jacobian over this ball.\n\nAs a last preparation, construct recursively, as long as it is possible, the sequences <math>(\\mathbf x_k)_k</math>, <math>(\\mathbf h_k)_k</math>, <math>(\\alpha_k)_k</math> according to\n:<math>\\begin{alignat}{2}\n\\mathbf h_k&=-F'(\\mathbf x_k)^{-1}F(\\mathbf x_k)\\\\[0.4em]\n\\alpha_k&=M\\,\\|F'(\\mathbf x_k)^{-1}\\|\\,\\|\\mathbf h_k\\|\\\\[0.4em]\n\\mathbf x_{k+1}&=\\mathbf x_k+\\mathbf h_k.\n\\end{alignat}</math>\n\n== Statement ==\nNow if <math>\\alpha_0\\le\\tfrac12</math> then \n#a solution <math>\\mathbf x^*</math> of <math>F(\\mathbf x^*)=0</math> exists inside the closed ball <math>\\bar B(\\mathbf x_1,\\|\\mathbf h_0\\|)</math> and \n#the Newton iteration starting in <math>\\mathbf x_0</math> converges to <math>\\mathbf x^*</math> with at least linear order of convergence.\n\nA statement that is more precise but slightly more difficult to prove uses the roots <math>t^\\ast\\le t^{**}</math> of the quadratic polynomial\n:<math>\np(t)\n  =\\left(\\tfrac12L\\|F'(\\mathbf x_0)^{-1}\\|^{-1}\\right)t^2\n    -t+\\|\\mathbf h_0\\|\n</math>,\n:<math>t^{\\ast/**}=\\frac{2\\|\\mathbf h_0\\|}{1\\pm\\sqrt{1-2\\alpha}}</math>\nand their ratio \n:<math>\n\\theta\n  =\\frac{t^*}{t^{**}}\n  =\\frac{1-\\sqrt{1-2\\alpha}}{1+\\sqrt{1-2\\alpha}}.\n</math>\nThen\n#a solution <math>\\mathbf x^*</math> exists inside the closed ball <math>\\bar B(\\mathbf x_1,\\theta\\|\\mathbf h_0\\|)\\subset\\bar B(\\mathbf x_0,t^*)</math>\n#it is unique inside the bigger ball <math>B(\\mathbf x_0,t^{*\\ast})</math>\n#and the convergence to the solution of <math>F</math> is dominated by the convergence of the Newton iteration of the quadratic polynomial <math>p(t)</math> towards its smallest root <math>t^\\ast</math>,<ref>{{cite journal |first=J. M. |last=Ortega |title=The Newton-Kantorovich Theorem |journal=Amer. Math. Monthly |volume=75 |year=1968 |issue=6 |pages=658–660 |jstor=2313800 |doi=10.2307/2313800}}</ref> if <math>t_0=0,\\,t_{k+1}=t_k-\\tfrac{p(t_k)}{p'(t_k)}</math>, then\n#:<math>\\|\\mathbf x_{k+p}-\\mathbf x_k\\|\\le t_{k+p}-t_k.</math>\n#The quadratic convergence is obtained from the error estimate<ref>{{cite journal |first=W. B. |last=Gragg |first2=R. A. |last2=Tapia |year=1974 |title=Optimal Error Bounds for the Newton-Kantorovich Theorem |journal=SIAM Journal on Numerical Analysis |volume=11 |issue=1 |pages=10–13 |jstor=2156425 |doi=10.1137/0711002}}</ref> \n#:<math>\n  \\|\\mathbf x_{n+1}-\\mathbf x^*\\|\n    \\le \\theta^{2^n}\\|\\mathbf x_{n+1}-\\mathbf x_n\\|\n    \\le\\frac{\\theta^{2^n}}{2^n}\\|\\mathbf h_0\\|.\n</math>\n==Corollary==\nIn 1986, Yamamoto proved that the error evaluations of the Newton method such as Doring (1969), Ostrowski (1971, 1973),<ref>{{cite journal |first=A. M. |last=Ostrowski |title=La method de Newton dans les espaces de Banach |journal=C. R. Acad. Sei. Paris |volume=27 |issue=A |year=1971 |pages=1251–1253 }}</ref><ref>{{cite book |first=A. M. |last=Ostrowski |title=Solution of Equations in Euclidean and Banach Spaces |publisher=Academic Press |location=New York |year=1973 |isbn=0-12-530260-6 }}</ref> Gragg-Tapia (1974), Potra-Ptak (1980),<ref>{{cite journal |first=F. A. |last=Potra |first2=V. |last2=Ptak |title=Sharp error bounds for Newton’s process |journal=Numer. Math. |volume=34 |year=1980 |pages=63–72 }}</ref> Miel (1981),<ref>{{cite journal |first=G. J. |last=Miel |title=An updated version of the Kantorovich theorem for Newton’s method |journal=Computing |volume=27 |year=1981 |pages=237–244 }}</ref> Potra (1984),<ref>{{cite journal |first=F. A. |last=Potra |title=On the a posteriori error estimates for Newton's method |journal=Beiträge zur Numerische Mathematik |volume=12 |year=1984 |issue= |pages=125–138 |doi= }}</ref> can be derived from the Kantorovich theorem.<ref>{{cite journal |last=Yamamoto |first=T. |year=1986 |title=A method for finding sharp error bounds for Newton's method under the Kantorovich assumptions |journal=[[Numerische Mathematik]] |volume=49 |issue=2–3 |pages=203–220 |doi=10.1007/BF01389624 }}</ref>\n\n==Generalizations==\nThere is a [[q-analog|''q''-analog]] for the Kantorovich theorem.<ref>{{cite journal |last=Rajkovic |first=P. M. |last2=Stankovic |first2=M. S. |last3=Marinkovic |first3=S. D. |year=2003 |title=On q-iterative methods for solving equations and systems |journal=Novi Sad J. Math |volume=33 |issue=2 |pages=127–137 |doi= }}</ref><ref>{{cite journal |last=Rajković |first=P. M. |last2=Marinković |first2=S. D. |last3=Stanković |first3=M. S. |year=2005 |title=On q-Newton–Kantorovich method for solving systems of equations |journal=Applied Mathematics and Computation |volume=168 |issue=2 |pages=1432–1448 |doi=10.1016/j.amc.2004.10.035 }}</ref> For other generalizations/variations, see Ortega & Rheinboldt (1970).<ref>{{cite book |last=Ortega |first=J. M. |last2=Rheinboldt |first2=W. C. |year=1970 |title=Iterative Solution of Nonlinear Equations in Several Variables |location= |publisher=SIAM |isbn= |oclc=95021 }}</ref>\n\n==Applications==\nOishi and Tanabe claimed that the Kantorovich theorem can be applied to obtain reliable solutions of [[linear programming]].<ref>{{cite journal |last=Oishi |first=S. |last2=Tanabe |first2=K. |year=2009 |title=Numerical Inclusion of Optimum Point for Linear Programming |journal=JSIAM Letters |volume=1 |issue= |pages=5–8 |doi=10.14495/jsiaml.1.5 }}</ref>\n\n== Notes ==\n<references />\n== References ==\n* John H. Hubbard and Barbara Burke Hubbard: ''Vector Calculus, Linear Algebra, and Differential Forms: A Unified Approach'', Matrix Editions, {{ISBN|978-0-9715766-3-6}} ([http://matrixeditions.com/UnifiedApproachSamples.html preview of 3. edition and sample material including Kant.-thm.])\n\n== Literature ==\n* Kantorovich, L. (1948): ''Functional analysis and applied mathematics'' (russ.). UMN3, 6 (28), 89–185.\n* Kantorovich, L. W.; Akilov, G. P. (1964): ''Functional analysis in normed spaces''.\n[[Category:Functional analysis]]\n[[Category:Numerical analysis]]\n[[Category:Theorems in analysis]]\n[[Category:Optimization in vector spaces]]\n[[Category:Optimization algorithms and methods]]"
    },
    {
      "title": "Karlsruhe Accurate Arithmetic",
      "url": "https://en.wikipedia.org/wiki/Karlsruhe_Accurate_Arithmetic",
      "text": "{{Use dmy dates|date=May 2019|cs1-dates=y}}\n{{anchor|XPA 3233|BAP-SC|ARITHMOS|Diamond}}\n'''Karlsruhe Accurate Arithmetic''' ('''KAA''') or '''Karlsruhe Accurate Arithmetic Approach''' ('''KAAA'''), augments conventional [[floating-point arithmetic]] with good error behaviour with new operations to calculate [[scalar product]]s with a single rounding error.<ref name=\"McDermid_1991\">{{cite book |title=Software Engineer's Reference Book |author-first=John A. |author-last=McDermid |edition=revised |publisher=[[Butterworth-Heinemann]] Ltd. / [[Elsevier]] |orig-year=1991 |date=1993 |page=187 |isbn=0-7506-0813-7 |id={{ISBN|1483105083}} / {{ISBN|9781483105086}} |chapter=8.5.2 |url=https://books.google.com/books?id=K_38BAAAQBAJ |access-date=2016-05-30}}</ref>\n\nThe foundations for KAA were developed at the [[University of Karlsruhe]] starting in the late 1960s.<ref name=\"IBM_4361_1985\">{{cite journal |title=Rechner liefern keine korrekten Ergebnisse - Neue Arithmetik der Karlsruher Forschungsgruppe Kulisch in der 4361 realisiert |trans-title=Computers don't deliver exact results - Karlsruhe research team Kulisch implements new computer arithmetic for the IBM 4361 |author=CW |date=1985-12-20 |journal=[[Computerwoche]] |publisher=[[IDG Business Media GmbH]] |language=German |location=Karlsruhe, Germany |url=http://www.computerwoche.de/a/rechner-liefern-keine-korrekten-ergebnisse,1172144 |access-date=2016-05-30 |dead-url=no |archive-url=https://web.archive.org/web/20160530220246/http://www.computerwoche.de/a/rechner-liefern-keine-korrekten-ergebnisse%2C1172144 |archive-date=2016-05-30}}</ref><ref name=\"Lortz_1971\">{{cite paper |title=Eine Langzahlarithmetik mit optimaler einseitiger Rundung |trans-title=A large-number arithmetic with optimal one-side rounding |language=German |author-first=Bruno |author-last=Lortz |type=Ph.D. thesis |publisher=[[Universität Karlsruhe]] |date=1971}}</ref><ref name=\"Kulisch_1976\">{{cite paper |title=Grundlagen des Numerischen Rechnens. Mathematische Begründung der Rechnerarithmetik |language=German |author-first=Ulrich |author-last=Kulisch |author-link=:de:Ulrich Kulisch |date=1976 |publisher=[[Bibliographisches Institut]] |location=Mannheim, Germany |isbn=3-411-01517-9 |id={{ISBN|978-3-411-01517-7}}}}</ref>\n\n==See also==\n*[[Ulrich Kulisch]]\n*{{Interlanguage link multi|Götz Alefeld|de}}\n*[[IBM 4361]]\n*[[PCS Cadmus]]\n*[[FORTRAN-SC]]\n*[[PASCAL-SC]]\n*[[PASCAL-XSC]]\n*[[C-XSC]]\n*[[Extensions for Scientific Computation]] (XSC)\n*[[Interval arithmetic]]\n*[[Unum (number format)|Unum]]\n*[[Catastrophic cancellation]]\n\n==References==\n{{Reflist}}\n\n==Further reading==\n<!-- <ref name=\"Kulisch_1969\">{{cite book |chapter=Grundzüge der Intervallrechnung |language=German |author-first=Ulrich W. |author-last=Kulisch |author-link=:de:Ulrich Kulisch |date=1969 |title=Jahrbuch Überblicke Mathematik |editor-first=Detlef |editor-last=Laugwitz |volume=2 |publisher=[[Bibliographisches Institut]] |location=Mannheim, Germany |pages=51-98}}</ref> -->\n<!-- * <ref name=\"Linz_1970\">{{cite journal |title=Accurate Floating-Point Summation |author-first=Peter |author-last=Linz |journal=[[Communications of the ACM]] |volume=13 |number=6 |date=June 1970 |pages=361–362 |url=https://www.researchgate.net/publication/220423009_Accurate_floating-point_summation |access-date=2016-05-30 |doi=10.1145/362384.362498}}</ref> -->\n* {{cite book |author1-first=Ulrich W. |author1-last=Kulisch |author1-link=:de:Ulrich Kulisch |author2-first=Willard L. |author2-last=Miranker |author2-link=Willard L. Miranker |date=1981 |edition=1 |title=Computer arithmetic in theory and practice |editor-first=Werner |editor-last=Rheinboldt |series=Computer Science and Applied Mathematics |publisher=[[Academic Press, Inc.]] |location=New York, USA |isbn=0-12-428650-X}}\n* {{cite journal |title=Cadmus jetzt mit Kulisch-Arithmetik - Uni Karlsruhe gibt Pascal-Compiler nach München |trans-title=Cadmus now comes with Kulisch arithmetic - University Karlsruhe delivers Pascal compiler to Munich |author=PI |date=1986-08-29 |journal=[[Computerwoche]] |publisher=[[IDG Business Media GmbH]] |language=German |location=Munich / Karlsruhe, Germany |url=http://www.computerwoche.de/a/uni-karlsruhe-gibt-pascal-compiler-nach-muenchen-cadmus-jetzt-mit-kulisch-arithmetik,1165749 |access-date=2016-05-30 |dead-url=no |archive-url=https://web.archive.org/web/20160530220339/http://www.computerwoche.de/a/uni-karlsruhe-gibt-pascal-compiler-nach-muenchen-cadmus-jetzt-mit-kulisch-arithmetik%2C1165749 |archive-date=2016-05-30}}\n* {{cite book |title=Improving Floating-Point Programming |editor-first=Peter J. L. |editor-last=Wallis |author-first1=Lothar |author-last1=Bamberger |author-first2=James H. |author-last2=Davenport |author-first3=Hans-Christoph |author-last3=Fischer |author-first4=Jan |author-last4=Kok |author-first5=Günter |author-last5=Schumacher |author-first6=Christian |author-last6=Ullrich |author-first7=Peter J. L. |author-last7=Wallis |author-first8=Dik T. |author-last8=Winter |author-first9=Jürgen |author-last9=Wolff von Gudenberg |date=1990 |edition=1st |publisher=[[John Wiley & Sons Ltd.]] |location=Bath, United Kingdom |isbn=0-471-92437-7 |id={{ISBN|978-0-471-92437-1}}}}\n* {{cite journal |title=Professoren-Porträt: Prof. Dr. Ulrich Kulisch |orig-year=October 1997 |journal=Eulenspiegel |publisher=Karlsruher Institut für Technologie, Fachschaft Mathematik Informatik |volume=WS97/98 |number=1 |date=2013-07-19 |language=German |url=https://www.fsmi.uni-karlsruhe.de/Fachschaft/Publikationen/Eulenspiegel/WS97_98/1/portrait.html |access-date=2016-05-30 |dead-url=no |archive-url=https://web.archive.org/web/20160530220204/https://www.fsmi.uni-karlsruhe.de/Fachschaft/Publikationen/Eulenspiegel/WS97_98/1/portrait.html |archive-date=2016-05-30}}\n* {{cite book |title=Numerical Methods |author-first=E. |author-last=Balagurusamy |work=[[PSG Institute of Management]] |publisher=[[Tata McGraw-Hill Publishing Company Limited]] |location=New Delhi, India |page=52 |edition=25th reprint (2008) |date=1999 |isbn=0-07-463311-2 |id={{ISBN|978-0-07-463311-3}} |url=https://books.google.com/books?id=jBkGxMkI4DMC |access-date=2016-05-30}}\n* {{cite journal |title=Numerische Mathematik: Rechnen mit garantierter Genauigkeit |trans-title=Numerical mathematics: Calculating with guaranteed accuracy |author-first=Christoph |author-last=Pöppe |journal=[[Spektrum der Wissenschaft]] |language=German |number=9 |volume=2000 |date=2000-09-01 |pages=54– |publisher=[[Spektrum der Wissenschaft Verlagsgesellschaft mbH]] |url=http://www.spektrum.de/magazin/rechnen-mit-garantierter-genauigkeit/826769 |access-date=2016-05-30 |dead-url=no |archive-url=https://web.archive.org/web/20160530222045/http://www.spektrum.de/magazin/rechnen-mit-garantierter-genauigkeit/826769 |archive-date=2016-05-30}}\n\n[[Category:Computer arithmetic]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Kempner series",
      "url": "https://en.wikipedia.org/wiki/Kempner_series",
      "text": "The '''Kempner series''' is a modification of the [[harmonic series (mathematics)|harmonic series]], formed by omitting all terms whose denominator expressed in base 10 contains the digit 9. That is, it is the sum\n:<math> {\\sideset{}{'}\\sum_{n=1}^\\infty} \\frac{1}{n}</math>\nwhere the prime indicates that ''n'' takes only values whose decimal expansion has no nines. The series was first studied by [[Aubrey J. Kempner|A. J. Kempner]] in 1914.<ref name=\"Kempner 1914\">\n{{cite journal\n | last = Kempner\n | first = A. J.\n |date=February 1914\n | title = A Curious Convergent Series\n | jstor = 2972074\n | journal = [[American Mathematical Monthly]]\n | volume = 21\n | issue = 2\n | pages = 48&ndash;50\n | publisher = Mathematical Association of America\n | location = Washington, DC\n | issn = 0002-9890\n | doi = 10.2307/2972074\n}}</ref> The series is [[counterintuitive]] because, unlike the harmonic series, it converges. Kempner showed the sum of this series is less than 80. Baillie<ref name=\"Baillie 1979\" /> showed that, rounded to 20 decimals, the actual sum is {{gaps|22.92067|66192|64150|34816}}\n{{OEIS|A082838}}.\n\nHeuristically, this series converges because most large integers contain every digit. For example, a random 100-digit integer is very likely to contain at least one '9', causing it to be excluded from the above sum.\n\nSchmelzer and Baillie<ref name=\"Schmelzer and Baillie 2008\">\n{{cite journal\n | last1 =Schmelzer\n | first1 =Thomas\n | first2=Robert \n | last2=Baillie\n |date=June–July 2008\n | title = Summing a Curious, Slowly Convergent Series\n | journal = [[American Mathematical Monthly]]\n | volume =115\n | issue = 6\n | pages = 525&ndash;540\n | publisher = Mathematical Association of America\n | location = Washington, DC\n | issn = 0002-9890\n | mr = 2416253\n | jstor = 27642532\n}}</ref> found an efficient [[algorithm]] for the more general problem of any omitted string of digits. For example, the sum of {{sfrac|1|''n''}} where ''n'' has no instances of \"42\" is about {{gaps|228.44630|41592|30813|25415}}. Another example: the sum of {{sfrac|1|''n''}} where ''n'' has no occurrence of the digit string \"314159\" is about {{gaps|2302582.33386|37826|07892|02376}}. (All values are rounded in the last decimal place).\n\n==Convergence==\n\nKempner's proof of convergence<ref name=\"Kempner 1914\" /> is repeated in many textbooks, for example Hardy and Wright<ref>{{cite book | last = Hardy | first = G. H. |author2=E. M. Wright  | title = An Introduction to the Theory of Numbers | publisher = Clarendon Press | location = Oxford | year = 1979 | edition=5th | isbn = 0-19-853171-0 }}</ref>{{Rp|120}} and Apostol.<ref>{{cite book | last = Apostol | first = Tom | title = Mathematical Analysis | publisher = Addison&ndash;Wesley | location = Boston | year = 1974 | isbn = 0-201-00288-4 }}</ref>{{Rp|212}} We group the terms of the sum by the number of digits in the denominator. The number of ''n''-digit positive integers that have no digit equal to '9' is 8&nbsp;×&nbsp;9<sup>''n''&minus;1</sup> because there are 8 choices (1 through 8) for the first digit, and 9 independent choices (0 through 8) for each of the other ''n''&minus;1 digits. Each of these numbers having no '9' is greater than or equal to 10<sup>''n''&minus;1</sup>, so the reciprocal of each of these numbers is less than or equal to 10<sup>''1''&minus;n</sup>. Therefore, the contribution of this group to the sum of reciprocals is less than 8&nbsp;×&nbsp;({{sfrac|9|10}})<sup>''n''&minus;1</sup>. Therefore the whole sum of reciprocals is at most\n\n:<math>8 \\sum_{n=1}^\\infty \\left(\\frac{9}{10}\\right)^{n-1} = 80.</math>\n\nThe same argument works for any omitted non-zero digit. The number of ''n''-digit positive integers that have no '0' is 9<sup>''n''</sup>, so the sum of {{sfrac|1|''n''}} where ''n'' has no digit '0' is at most\n\n:<math>9 \\sum_{n=1}^\\infty \\left(\\frac{9}{10}\\right)^{n-1} = 90.</math>\n\nThe series also converge if strings of ''k'' digits are omitted, for example if we omit all denominators that have a decimal substring of 42. This can be proved in almost the same way.<ref name=\"Schmelzer and Baillie 2008\" /> First we observe that we can work with numbers in base 10<sup>''k''</sup> and omit all denominators that have the given string as a \"digit\". The analogous argument to the base 10 case shows that this series converges. Now switching back to base 10, we see that this series contains all denominators that omit the given string, as well as denominators that include it if it is not on a \"''k''-digit\" boundary. For example, if we are omitting 42, the base-100 series would omit 4217 and 1742, but not 1427, so it is larger than the series that omits all 42s.\n\nFarhi<ref>\n{{cite journal\n | last = Farhi\n | first = Bakir\n |date=December 2008\n | title = A Curious Result Related to Kempner's Series\n | journal = American Mathematical Monthly\n | volume = 115\n | issue = 10\n | pages = 933&ndash;938\n | publisher = Mathematical Association of America\n | location = Washington, DC\n | issn = 0002-9890\n | mr = 2468554\n | bibcode = 2008arXiv0807.3518F\n | jstor = 27642640\n | arxiv=0807.3518\n}}</ref> considered generalized Kempner series, namely, the sums ''S''(''d'',&nbsp;''n'') of the reciprocals of the positive integers that have exactly ''n'' instances of the digit&nbsp;''d'' where 0&nbsp;≤&nbsp;''d''&nbsp;≤&nbsp;9 (so that the original Kempner series is ''S''(9,&nbsp;0)). He showed that for each ''d'' the sequence of values ''S''(''d'',&nbsp;''n'') for ''n''&nbsp;≥&nbsp;1 is decreasing and converges to 10&nbsp;ln&nbsp;10. The sequence is not in general decreasing starting with ''n''&nbsp;=&nbsp;0; for example, for the original Kempner series we have ''S''(9,&nbsp;0) ≈&nbsp;22.921&nbsp;<&nbsp;23.026 ≈&nbsp;10&nbsp;ln&nbsp;10&nbsp;<&nbsp;''S''(9,&nbsp;''n'') for&nbsp;''n''&nbsp;≥&nbsp;1.\n\n==Approximation methods==\n\nThe series converges extremely slowly. Baillie<ref name=\"Baillie 1979\" /> remarks that after summing 10<sup>24</sup> terms the remainder is still larger than 1.<ref>\n{{cite journal\n | date=December 1980\n | title = ERRATA\n | journal = American Mathematical Monthly\n | volume = 87\n | issue = 10\n | page = 866\n | publisher = Mathematical Association of America\n | location = Washington, DC\n | issn = 0002-9890\n}}</ref>\n\nThe upper bound of 80 is very crude, and Irwin showed<ref>\n{{cite journal\n | last = Irwin\n | first = Frank\n |date=May 1916\n | title = A Curious Convergent Series\n | jstor = 2974352\n | journal = American Mathematical Monthly\n | volume = 23\n | issue = 5\n | pages = 149–152\n | publisher = Mathematical Association of America\n | location = Washington, DC\n | issn = 0002-9890\n | doi = 10.2307/2974352\n}}</ref> by a slightly finer analysis of the bounds that the value of the Kempner series is near 23, since refined to the value above, 22.92067....<ref name=\"Baillie 1979\" />\n\nBaillie<ref name=\"Baillie 1979\">\n{{cite journal\n | last =Baillie\n | first =Robert\n |date=May 1979\n | title = Sums of Reciprocals of Integers Missing a Given Digit\n | jstor =2321096\n | journal = American Mathematical Monthly\n | volume = 86\n | issue = 5\n | pages = 372–374\n | publisher = Mathematical Association of America\n | location = Washington, DC\n | issn = 0002-9890\n | doi =10.2307/2321096\n}}</ref> developed a recursion that expresses the contribution from each (''k''&nbsp;+&nbsp;1)-digit block in terms of the contributions of the ''k''-digit blocks for all choices of omitted digit. This permits a very accurate estimate with a small amount of computation.\n\n==Name of this series==\n\nMost authors do not name this series. The name \"Kempner series\" is used in MathWorld<ref>{{MathWorld|title=Kempner series|urlname=KempnerSeries}}</ref> and in Havil's book ''Gamma'' on the [[Euler–Mascheroni constant]].<ref>{{cite book | last = Havil | first = Julian | title = Gamma: Exploring Euler's Constant | publisher = Princeton University Press | location = Princeton | year = 2003 | isbn = 978-0-691-09983-5 }}</ref>{{Rp|31–33}}\n\n==See also==\n* [[Small set (combinatorics)|Small set]]\n* [[List of sums of reciprocals]]\n\n==Notes==\n<references />\n\n==External links==\n* [http://eprints.maths.ox.ac.uk/1106/1/NA-06-17.pdf \"Summing Curious, Slowly Convergent, Harmonic Subseries\"]. Preprint of the paper by Thomas Schmelzer and Robert Baillie.\n\n[[Category:Mathematical series]]\n[[Category:Numerical analysis]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "Kulisch arithmetic",
      "url": "https://en.wikipedia.org/wiki/Kulisch_arithmetic",
      "text": "#REDIRECT [[Karlsruhe Accurate Arithmetic]]\n\n{{Redirect category shell|1=\n{{R from alternative name}}\n{{R from other capitalisation}}\n}}\n\n[[Category:Computer arithmetic]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Lady Windermere's Fan (mathematics)",
      "url": "https://en.wikipedia.org/wiki/Lady_Windermere%27s_Fan_%28mathematics%29",
      "text": "In [[mathematics]], '''Lady Windermere's Fan''' is a telescopic identity employed to relate global and local error of a [[Numerical analysis|numerical algorithm]]. The name is derived from [[Oscar Wilde]]'s play ''[[Lady Windermere's Fan|Lady Windermere's Fan, A Play About a Good Woman]]''.\n\n==Lady Windermere's Fan for a function of one variable==\nLet <math>E(\\ \\tau,t_0,y(t_0)\\ )</math> be the '''exact solution operator''' so that:\n::<math>y(t_0+\\tau) = E(\\tau,t_0,y(t_0))\\ y(t_0)</math>\nwith <math>t_0</math> denoting the initial time and <math>y(t)</math> the function to be approximated with a given <math>y(t_0)</math>.\n\nFurther let <math>y_n</math>, <math>n \\in \\N,\\ n\\le N</math> be the numerical approximation at time <math>t_n</math>, <math>t_0 < t_n \\le T = t_N</math>. <math>y_n</math> can be attained by means of the '''approximation operator''' <math>\\Phi(\\ h_n,t_n,y(t_n)\\ )</math> so that:\n::<math>y_n = \\Phi(\\ h_{n-1},t_{n-1},y(t_{n-1})\\ )\\ y_{n-1}\\quad</math> with <math>h_n = t_{n+1} - t_n</math>\n\nThe approximation operator represents the numerical scheme used. For a simple explicit forward [[Euler method|euler scheme]] with step witdth <math>h</math> this would be: <math>\\Phi_{\\text{Euler}}(\\ h,t_{n-1},y(t_{n-1})\\ )\\ y(t_{n-1}) = (1 + h \\frac{d}{dt})\\ y(t_{n-1})</math>\n\nThe '''local error''' <math>d_n</math> is then given by:\n::<math>d_n:= D(\\ h_{n-1},t_{n-1},y(t_{n-1}\\ )\\ y_{n-1} := \\left[ \\Phi(\\ h_{n-1},t_{n-1},y(t_{n-1})\\ ) - E(\\ h_{n-1},t_{n-1},y(t_{n-1})\\ ) \\right]\\ y_{n-1} </math>\n\nIn abbreviation we write:\n::<math>\\Phi(h_n) := \\Phi(\\ h_n,t_n,y(t_n)\\ )</math>\n::<math>E(h_n) := E(\\ h_n,t_n,y(t_n)\\ )</math>\n::<math>D(h_n) := D(\\ h_n,t_n,y(t_n)\\ )</math>\n\nThen '''Lady Windermere's Fan''' for a function of a single variable <math>t</math> writes as:\n\n<math>y_N-y(t_N) = \\prod_{j=0}^{N-1}\\Phi(h_j)\\ (y_0-y(t_0)) + \\sum_{n=1}^N\\ \\prod_{j=n}^{N-1} \\Phi(h_j)\\ d_n </math>\n\nwith a global error of <math>y_N-y(t_N)</math>\n\n===Explanation===\n<math>\\begin{align}\ny_N - y(t_N) &{}=\n    y_N - \\underbrace{\\prod_{j=0}^{N-1} \\Phi(h_j)\\ y(t_0) + \\prod_{j=0}^{N-1} \\Phi(h_j)\\ y(t_0)}_{=0} - y(t_N) \\\\\n&{}= y_N - \\prod_{j=0}^{N-1} \\Phi(h_j)\\ y(t_0) + \\underbrace{\\sum_{n=0}^{N-1}\\ \\prod_{j=n}^{N-1} \\Phi(h_j)\\ y(t_n) - \\sum_{n=1}^N\\ \\prod_{j=n}^{N-1} \\Phi(h_j)\\ y(t_n)}_{=\\prod_{n=0}^{N-1} \\Phi(h_n)\\ y(t_n)-\\sum_{n=N}^{N}\\left[\\prod_{j=n}^{N-1} \\Phi(h_j)\\right]\\ y(t_n) = \\prod_{j=0}^{N-1} \\Phi(h_j)\\ y(t_0) - y(t_N) } \\\\\n&{}= \\prod_{j=0}^{N-1}\\Phi(h_j)\\ y_0 - \\prod_{j=0}^{N-1}\\Phi(h_j)\\ y(t_0) + \\sum_{n=1}^N\\ \\prod_{j=n-1}^{N-1} \\Phi(h_j)\\ y(t_{n-1}) - \\sum_{n=1}^N\\ \\prod_{j=n}^{N-1} \\Phi(h_j)\\ y(t_n) \\\\\n&{}= \\prod_{j=0}^{N-1}\\Phi(h_j)\\ (y_0-y(t_0)) + \\sum_{n=1}^N\\ \\prod_{j=n}^{N-1} \\Phi(h_j) \\left[ \\Phi(h_{n-1}) - E(h_{n-1}) \\right] \\ y(t_{n-1}) \\\\\n&{}= \\prod_{j=0}^{N-1}\\Phi(h_j)\\ (y_0-y(t_0)) + \\sum_{n=1}^N\\ \\prod_{j=n}^{N-1} \\Phi(h_j)\\ d_n\n\\end{align}</math>\n\n==See also==\n* [[Baker–Campbell–Hausdorff formula]]\n* [[Numerical error]]\n\n{{Lady Windermere's Fan}}\n\n{{DEFAULTSORT:Lady Windermere's Fan (Mathematics)}}\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Level set (data structures)",
      "url": "https://en.wikipedia.org/wiki/Level_set_%28data_structures%29",
      "text": "In [[computer science]] a [[level set]] [[data structure]] is designed to represent discretely [[Sampling (statistics)|sampled]] dynamic level sets functions.\n\nA common use of this form of data structure is in efficient image [[Rendering (computer graphics)|rendering]]. The underlying method constructs a [[distance transform|signed distance field]] that extends from the boundary, and can be used to solve the motion of the boundary in this field.\n\n==Chronological developments==\nThe powerful [[level-set method]] is due to [[Stanley Osher|Osher]] and [[James Sethian|Sethian]] 1988.<ref name=Osher>Osher, S. & Sethian, J. A. 1988. \"Fronts propagating with curvature-dependent speed: Algorithms based on Hamilton-Jacobi\nformulations\". ''Journal of Computation Physics'' 79:12–49.</ref> However, the straightforward implementation via a dense d-dimensional [[array data structure|array]] of values, results in both time and storage complexity of <math>O(n^d)</math>, where <math>n</math> is the cross sectional resolution of the spatial extents of the domain and <math>d</math> is the number of spatial dimensions of the domain.\n\n===Narrow band===\nThe narrow band level set method, introduced in 1995 by Adalsteinsson and Sethian,<ref name=\"Adalsteinsson\">Adalsteinsson, D. & Sethian, J. A. 1995. \"A fast level set method for propagating interfaces.\" ''[[Journal of Computational Physics]]''. 118(2)269–277.</ref> restricted most computations to a thin band of active [[voxel]]s immediately surrounding the interface, thus reducing the time complexity in three dimensions to <math>O(n^2)</math> for most operations. Periodic updates of the narrowband structure, to rebuild the list of active voxels, were required which entailed an <math>O(n^3)</math> operation in which voxels over the entire volume were accessed. The storage complexity for this narrowband scheme was still <math>O(n^3).</math> Differential constructions over the narrow band domain edge require careful interpolation and domain alteration schemes to stabilise the solution.<ref>{{cite journal|title=A fast level set Method for Propagating Interfaces|author1=Adalsteinsson, D  |author2=Sethian, J|year=1994|url=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.46.1716&rep=rep1&type=ps}}</ref>\n\n===Sparse field===\nThis <math>O(n^3)</math> time complexity was eliminated in the approximate \"sparse field\" level set method introduced by Whitaker in 1998.<ref name=Whitaker>Whitaker, R. T. 1998. \"A level-set approach to 3d reconstruction from range data.\" ''[[International Journal of Computer Vision]].'' 29(3)203–231.</ref> The sparse field level set method employs a set of linked lists to track the active voxels around the interface. This allows incremental extension of the active region as needed without incurring any significant overhead. While consistently <math>O(n^2)</math> efficient in time, <math>O(n^3)</math> storage space is still required by the sparse field level set method. See<ref name=Lankton>S. Lankton. \"Sparse Field Method - Technical Report.\" April 21, 2009 <http://www.shawnlankton.com/2009/04/sfm-and-active-contours/></ref> for implementation details.\n\n===Sparse block grid===\nThe sparse block grid method, introduced by Bridson in 2003,<ref name=Bridson>Bridson, R. 2003. \"Computational aspects of dynamic surfaces (dissertation).\" [[Stanford University]], Stanford, California.</ref> divides the entire bounding volume of size <math>n^3</math> into small cubic blocks of <math>m^3</math> voxels each. A coarse grid of size <math>(n/m\n)^3</math> then stores pointers only to those blocks that intersect the narrow band of the level set. Block allocation and deallocation occur as the surface propagates to accommodate to the deformations. This method has a suboptimal storage complexity of <math>O\\left((nm)3 + m^3n^2\\right)</math>, but retains the constant time access inherent to dense grids.\n\n===Octree===\nThe [[octree]] level set method, introduced by Strain in 1999<ref name=Strain>Strain, J. 1999. \"Tree methods for moving interfaces.\" ''[[Journal of Computational Physics]]''. 151(2)616–648.</ref> and refined by Losasso, Gibou and Fedkiw,<ref name=Losasso>Losasso, F., Gibou, F., & Fedkiw, R. 2004. [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.459.1489&rep=rep1&type=pdf Simulating water and smoke with an octree data structure]. [[ACM Transactions on Graphics]]. 23(3)457–462.</ref> and more recently by Min and Gibou<ref name=MinGibou>Min, C. & Gibou, F. 2007. A second order accurate level set method on non-graded adaptive cartesian grids. [[Journal of Computational Physics]]. 225(1)300–321.</ref> uses a tree of nested cubes of which the leaf nodes contain signed distance values. Octree level sets currently require uniform refinement along the interface (i.e. the narrow band) in order to obtain sufficient precision. This representation is efficient in terms of storage, <math>O(n^2),</math> and relatively efficient in terms of access queries, <math>O(\\log\\, n).</math> An advantage of the level method on octree data structures is that one can solve the partial differential equations associated with typical free boundary problems that use the level set method. The CASL research group<ref name=CASL>http://www1.engr.ucsb.edu/~fgibou/Research.html</ref> has developed this line of work in computational materials, computational fluid dynamics, electrokinetics, image guided surgery and controls.\n\n===Run-length encoded===\nThe [[run-length encoding]] (RLE) level set method, introduced in 2004,<ref name=Houston2004>Houston, B., Nielsen, M., Batty, C., Nilsson, O. & K. Museth. 2006. \"Hierarchical RLE Level Set: A Compact and Versatile Deformable Surface Representation.\" ''[[ACM Transactions on Graphics]]''. 25(1).</ref> applies the RLE scheme to compress regions away from the narrow band to just their sign representation while storing with full precision the narrow band. The sequential traversal of the narrow band is optimal and storage efficiency is further improved over the octree level set. The addition of an acceleration lookup table allows for fast <math>O(\\log r)</math> random access, where r is the number of runs per cross section. Additional efficiency is gained by applying the RLE scheme in a dimensional recursive fashion, a technique introduced by Nielsen & Museth's similar DT-Grid.<ref name=Nielsen>Nielsen, M. B. & Museth K. 2006. \"Dynamic Tubular Grid: An efficient data structure and algorithms for high resolution level sets.\" ''[[Journal of Scientific Computing]]''. 26(1) 1–39.</ref>\n\n===Hash Table Local Level Set===\nThe Hash Table Local Level Set method, introduced in 2012 by Brun, Guittet and Gibou,<ref name=BrunGuittetGibou>Brun, E., Guittet, A. & Gibou, F. 2012. \"A local level-set method using a hash table data structure.\" ''[[Journal of Computational Physics]]''. 231(6)2528-2536.</ref> only computes the level set data in a band around the interface, as in the Narrow Band Level-Set Method, but also only stores the data in that same band. A hash table data structure is used, which provides an <math>O(1)</math> access to the data. However, the authors conclude that their method, while being easier to implement, performs worse than a quadtree implementation. They find that {{quote|as it is, [...] a quadtree data structure seems more adapted than the hash table data structure for level-set algorithms.}} Three main reasons for worse efficiency are listed:\n# to obtain accurate results, a rather large band is required close to the interface, which counterbalances the absence of grid nodes far from the interface; \n# the performances are deteriorated by extrapolation procedures on the outer edges of the local grid and \n# the width of the band restricts the time step and slows down the method.\n\n===Point-based===\n{{Expand section|date=December 2009}}\n\nCorbett in 2005 <ref name=Corbett>Corbett, R. 2005. \"Point–Based Level Sets and Progress Towards Unorganised Particle Level Sets (thesis).\" [[University of British Columbia]], [[Canada]].</ref> introduced the point-based level set method. Instead of using a uniform sampling of the level set, the continuous level set function is reconstructed from a set of unorganized point samples via [[moving least squares]].\n\n==References==\n<references />\n\n{{DEFAULTSORT:Level Set (Data Structures)}}\n[[Category:Computer graphics data structures]]\n[[Category:Image processing]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Levinson recursion",
      "url": "https://en.wikipedia.org/wiki/Levinson_recursion",
      "text": "'''Levinson recursion''' or '''Levinson–Durbin recursion''' is a procedure in [[linear algebra]] to [[recursion|recursively]] calculate the solution to an equation involving a [[Toeplitz matrix]]. The [[algorithm]] runs in [[Big O notation|Θ]](''n''<sup>2</sup>) time, which is a strong improvement over [[Gauss–Jordan elimination]], which runs in Θ(''n''<sup>3</sup>).\n\nThe Levinson–Durbin algorithm was proposed first by [[Norman Levinson]] in 1947, improved by [[James Durbin]] in 1960, and subsequently improved to 4''n''<sup>2</sup> and then 3''n''<sup>2</sup> multiplications by W. F. Trench and S. Zohar, respectively.\n\nOther methods to process data include [[Schur decomposition]] and [[Cholesky decomposition]]. In comparison to these, Levinson recursion (particularly split Levinson recursion) tends to be faster computationally, but more sensitive to computational inaccuracies like [[round-off error]]s.\n\nThe Bareiss algorithm for [[Toeplitz matrix|Toeplitz matrices]] (not to be confused with the general [[Bareiss algorithm]]) runs about as fast as Levinson recursion, but it uses ''O''(''n''<sup>2</sup>) space, whereas Levinson recursion uses only ''O''(''n'') space.  The Bareiss algorithm, though, is [[numerical stability|numerically stable]],<ref>Bojanczyk et al. (1995).</ref><ref>Brent (1999).</ref> whereas Levinson recursion is at best only weakly stable (i.e. it exhibits numerical stability for [[Condition number|well-conditioned]] linear systems).<ref>Krishna & Wang (1993).</ref>\n\nNewer algorithms, called ''asymptotically fast'' or sometimes ''superfast'' Toeplitz algorithms, can solve in Θ(''n'' log<sup>''p''</sup>''n'') for various ''p'' (e.g. ''p'' = 2,<ref>http://www.maths.anu.edu.au/~brent/pd/rpb143tr.pdf</ref><ref>{{cite web |url=http://etd.gsu.edu/theses/available/etd-04182008-174330/unrestricted/kimitei_symon_k_200804.pdf |title=Archived copy |accessdate=2009-04-28 |deadurl=yes |archiveurl=https://web.archive.org/web/20091115041852/http://etd.gsu.edu/theses/available/etd-04182008-174330/unrestricted/kimitei_symon_k_200804.pdf |archivedate=2009-11-15 |df= }}</ref> ''p'' = 3 <ref>https://web.archive.org/web/20070418074240/http://saaz.cs.gsu.edu/papers/sfast.pdf</ref>). Levinson recursion remains popular for several reasons; for one, it is relatively easy to understand in comparison; for another, it can be faster than a superfast algorithm for small ''n'' (usually ''n''&nbsp;<&nbsp;256).<ref>http://www.math.niu.edu/~ammar/papers/amgr88.pdf</ref>\n\n== Derivation ==\n\n=== Background ===\nMatrix equations follow the form:\n\n: <math> \\mathbf M \\  \\vec x = \\vec y. </math>\n\nThe Levinson–Durbin algorithm may be used for any such equation, as long as '''''M''''' is a known [[Toeplitz matrix]] with a nonzero main diagonal. Here <math> \\vec y </math> is a known [[vector space|vector]], and <math>\\vec x</math> is an unknown vector of numbers ''x''<sub>''i''</sub> yet to be determined.\n\nFor the sake of this article, ''ê''<sub>''i''</sub> is a vector made up entirely of zeroes, except for its ''i''th place, which holds the value one. Its length will be implicitly determined by the surrounding context. The term ''N'' refers to the width of the matrix above – '''''M''''' is an ''N''×''N'' matrix. Finally, in this article, superscripts refer to an ''inductive index'', whereas subscripts denote indices. For example (and definition), in this article, the matrix '''''T<sup>n</sup>''''' is an ''n×n'' matrix which copies the upper left ''n×n'' block from '''''M''''' – that is, ''T''<sup>''n''</sup><sub>''ij''</sub> = ''M''<sub>''ij''</sub>.\n\n'''''T<sup>n</sup>''''' is also a Toeplitz matrix; meaning that it can be written as:\n\n: <math> \\mathbf T^n = \\begin{bmatrix}\n    t_0    & t_{-1}  & t_{-2}  & \\dots  & t_{-n+1}   \\\\\n    t_1    & t_0     & t_{-1}  & \\dots  & t_{-n+2} \\\\\n    t_2    & t_1     & t_0     & \\dots  & t_{-n+3} \\\\\n    \\vdots & \\vdots  & \\vdots  & \\ddots & \\vdots   \\\\\n    t_{n-1}& t_{n-2} & t_{n-3} & \\dots  & t_0\n  \\end{bmatrix}. </math>\n\n=== Introductory steps ===\nThe algorithm proceeds in two steps. In the first step, two sets of vectors, called the ''forward'' and ''backward'' vectors, are established. The forward vectors are used to help get the set of backward vectors; then they can be immediately discarded. The backwards vectors are necessary for the second step, where they are used to build the solution desired.\n\nLevinson–Durbin recursion defines the ''n''<sup>th</sup> \"forward vector\", denoted <math>\\vec f^n</math>, as the vector of length ''n'' which satisfies:\n\n:<math>\\mathbf T^n \\vec f^n = \\hat e_1.</math>\n\nThe ''n''<sup>th</sup> \"backward vector\" <math>\\vec b^n</math> is defined similarly; it is the vector of length ''n'' which satisfies:\n\n:<math>\\mathbf T^n \\vec b^n = \\hat e_n.</math>\n\nAn important simplification can occur when '''''M''''' is a [[symmetric matrix]]; then the two vectors are related by ''b''<sup>''n''</sup><sub>''i''</sub> = ''f''<sup>''n''</sup><sub>''n''+1−''i''</sub>—that is, they are row-reversals of each other. This can save some extra computation in that special case.\n\n=== Obtaining the backward vectors ===\nEven if the matrix is not symmetric, then the ''n''<sup>th</sup> forward and backward vector may be found from the vectors of length ''n''&nbsp;−&nbsp;1 as follows. First, the forward vector may be extended with a zero to obtain:\n\n:<math>\\mathbf T^n \\begin{bmatrix} \\vec f^{n-1} \\\\ 0 \\\\ \\end{bmatrix} =\n  \\begin{bmatrix}\n    \\        & \\               & \\     & t_{-n+1}   \\\\\n    \\        & \\mathbf T^{n-1} & \\     & t_{-n+2} \\\\\n    \\        & \\               & \\     & \\vdots   \\\\\n    t_{n-1}  & t_{n-2}         & \\dots & t_0      \\\\\n  \\end{bmatrix}\n  \\begin{bmatrix}  \\            \\\\\n                   \\vec f^{n-1} \\\\\n                   \\            \\\\\n                   0            \\\\\n                   \\            \\\\\n  \\end{bmatrix} = \n  \\begin{bmatrix}  1            \\\\\n                   0            \\\\\n                   \\vdots       \\\\\n                   0            \\\\\n                   \\epsilon_f^n\n  \\end{bmatrix}. </math>\n\nIn going from '''''T''<sup>''n''−1</sup>''' to '''''T<sup>n</sup>''''', the extra ''column'' added to the matrix does not perturb the solution when a zero is used to extend the forward vector. However, the extra ''row'' added to the matrix ''has'' perturbed the solution; and it has created an unwanted error term ''ε''<sub>''f''</sub> which occurs in the last place. The above equation gives it the value of:\n\n: <math> \\epsilon_f^n \\ = \\  \\sum_{i=1}^{n-1} \\ M_{ni} \\  f_{i}^{n-1} \\ = \\ \\sum_{i=1}^{n-1} \\  t_{n-i} \\ f_{i}^{n-1}. </math>\n\nThis error will be returned to shortly and eliminated from the new forward vector; but first, the backwards vector must be extended in a similar (albeit reversed) fashion. For the backwards vector,\n\n:<math> \\mathbf T^n \\begin{bmatrix} 0 \\\\ \\vec b^{n-1} \\\\ \\end{bmatrix} =\n\\begin{bmatrix}\n    t_0     & \\dots & t_{-n+2}         & t_{-n+1} \\\\\n    \\vdots  & \\     & \\               & \\       \\\\\n    t_{n-2} & \\     & \\mathbf T^{n-1} & \\       \\\\\n    t_{n-1} & \\     & \\               &\n  \\end{bmatrix}\n  \\begin{bmatrix}  \\            \\\\\n                   0            \\\\\n                   \\            \\\\\n                   \\vec b^{n-1} \\\\\n                   \\            \\\\\n  \\end{bmatrix} = \n  \\begin{bmatrix}  \\epsilon_b^n  \\\\\n                   0             \\\\\n                   \\vdots        \\\\\n                   0             \\\\\n                   1\n  \\end{bmatrix}. </math>\n\nAs before, the extra column added to the matrix does not perturb this new backwards vector; but the extra row does. Here we have another unwanted error ''ε''<sub>''b''</sub> with value:\n\n:<math> \\epsilon_b^n \\ = \\ \\sum_{i=2}^n \\  M_{1i} \\ b_{i-1}^{n-1} \\ = \\ \\sum_{i=1}^{n-1} \\  t_{-i} \\ b_i^{n-1}. \\ </math>\n\nThese two error terms can be used to form higher-order forward and backward vectors described as follows. Using the linearity of matrices, the following identity holds for all <math>(\\alpha,\\beta)</math>:\n\n:<math>\\mathbf T \\left( \\alpha\n  \\begin{bmatrix}\n                   \\vec f \\\\\n                   \\            \\\\\n                   0            \\\\\n  \\end{bmatrix} + \\beta\n  \\begin{bmatrix}\n                   0            \\\\\n                   \\            \\\\\n                   \\vec b\n  \\end{bmatrix} \\right ) = \\alpha\n  \\begin{bmatrix}  1        \\\\\n                   0        \\\\\n                   \\vdots   \\\\\n                   0        \\\\\n                   \\epsilon_f \\\\\n  \\end{bmatrix} + \\beta\n  \\begin{bmatrix}  \\epsilon_b  \\\\\n                   0             \\\\\n                   \\vdots        \\\\\n                   0             \\\\\n                   1\n  \\end{bmatrix}.</math>\n\nIf ''α'' and ''β'' are chosen so that the right hand side yields ''ê''<sub>1</sub> or ''ê''<sub>''n''</sub>, then the quantity in the parentheses will fulfill the definition of the ''n''<sup>th</sup> forward or backward vector, respectively. With those alpha and beta chosen, the vector sum in the parentheses is simple and yields the desired result.\n\nTo find these coefficients, <math>\\alpha^n_{f}</math>, <math>\\beta^n_{f}</math> are such that :\n:<math>\n\\vec f_n = \\alpha^n_{f} \\begin{bmatrix} \\vec f_{n-1}\\\\\n0\n\\end{bmatrix}\n+\\beta^n_{f}\\begin{bmatrix}0\\\\\n\\vec b_{n-1}\n\\end{bmatrix}\n</math>\nand respectively  <math>\\alpha^n_{b}</math>, <math>\\beta^n_{b}</math> are such that :\n:<math>\\vec b_n = \\alpha^n_{b}\n\\begin{bmatrix}\n\\vec f_{n-1}\\\\\n0\n\\end{bmatrix}\n+\\beta^n_{b}\\begin{bmatrix}\n0\\\\\n\\vec b_{n-1}\n\\end{bmatrix}.\n</math>\nBy multiplying both previous equations by <math>{\\mathbf T}^n</math> one gets the following equation:\n: <math>\n\\begin{bmatrix} 1 & \\epsilon^n_b \\\\\n0 & 0 \\\\\n\\vdots & \\vdots \\\\\n0 & 0 \\\\\n\\epsilon^n_f & 1\n\\end{bmatrix} \\begin{bmatrix} \\alpha^n_f & \\alpha^n_b \\\\ \\beta^n_f & \\beta^n_b \\end{bmatrix}\n= \\begin{bmatrix}\n1 & 0 \\\\\n0 & 0 \\\\\n\\vdots & \\vdots \\\\\n0 & 0 \\\\\n0 & 1\n\\end{bmatrix}.</math>\n\nNow, all the zeroes in the middle of the two vectors above being disregarded and collapsed, only the following equation is left:\n\n: <math> \\begin{bmatrix} 1 & \\epsilon^n_b \\\\ \\epsilon^n_f & 1 \\end{bmatrix} \\begin{bmatrix} \\alpha^n_f & \\alpha^n_b \\\\ \\beta^n_f & \\beta^n_b \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}.</math>\n\nWith these solved for (by using the Cramer 2×2 matrix inverse formula), the new forward and backward vectors are:\n\n: <math>\\vec f^n = {1 \\over { 1 - \\epsilon_b^n \\epsilon_f^n }}          \\begin{bmatrix} \\vec f^{n-1} \\\\ 0 \\end{bmatrix}\n                 - { \\epsilon_f^n \\over { 1 - \\epsilon_b^n \\epsilon_f^n }}\\begin{bmatrix} 0 \\\\ \\vec b^{n-1} \\end{bmatrix}</math>\n\n: <math>\\vec b^n = {1 \\over { 1 - \\epsilon_b^n \\epsilon_f^n }}          \\begin{bmatrix} 0 \\\\ \\vec b^{n-1} \\end{bmatrix}\n                 - { \\epsilon_b^n \\over { 1 - \\epsilon_b^n \\epsilon_f^n }}\\begin{bmatrix} \\vec f^{n-1} \\\\ 0 \\end{bmatrix}.</math>\n\nPerforming these vector summations, then, gives the ''n''<sup>th</sup> forward and backward vectors from the prior ones. All that remains is to find the first of these vectors, and then some quick sums and multiplications give the remaining ones. The first forward and backward vectors are simply:\n\n: <math>\\vec f^1 = \\vec b^1 = \\left[ {1 \\over M_{11}} \\right] = \\left[ {1 \\over t_0} \\right].</math>\n\n=== Using the backward vectors ===\nThe above steps give the ''N'' backward vectors for '''''M'''''. From there, a more arbitrary equation is:\n\n: <math> \\vec y = \\mathbf M \\  \\vec x. </math>\n\nThe solution can be built in the same recursive way that the backwards vectors were built. Accordingly, <math>\\vec x</math> must be generalized to a sequence of intermediates <math>\\vec x^n</math>, such that <math>\\vec x^N = \\vec x</math>.\n\nThe solution is then built recursively by noticing that if\n\n: <math> \\mathbf T^{n-1}\n  \\begin{bmatrix}  x_1^{n-1}     \\\\\n                   x_2^{n-1}     \\\\\n                   \\vdots   \\\\\n                   x_{n-1}^{n-1} \\\\\n  \\end{bmatrix} =   \n  \\begin{bmatrix}  y_1     \\\\\n                   y_2     \\\\\n                   \\vdots   \\\\\n                   y_{n-1}\n  \\end{bmatrix}.</math>\n\nThen, extending with a zero again, and defining an error constant where necessary:\n\n: <math> \\mathbf T^{n}\n  \\begin{bmatrix}  x_1^{n-1}     \\\\\n                   x_2^{n-1}     \\\\\n                   \\vdots   \\\\\n                   x_{n-1}^{n-1} \\\\\n                   0\n  \\end{bmatrix} =   \n  \\begin{bmatrix}  y_1     \\\\\n                   y_2     \\\\\n                   \\vdots   \\\\\n                   y_{n-1} \\\\\n                   \\epsilon_x^{n-1}\n  \\end{bmatrix}.</math>\n\nWe can then use the ''n''<sup>th</sup> backward vector to eliminate the error term and replace it with the desired formula as follows:\n\n: <math> \\mathbf T^{n} \\left (\n  \\begin{bmatrix}  x_1^{n-1}     \\\\\n                   x_2^{n-1}     \\\\\n                   \\vdots   \\\\\n                   x_{n-1}^{n-1} \\\\\n                   0 \\\\\n  \\end{bmatrix} + (y_n - \\epsilon_x^{n-1}) \\  \\vec b^n \\right ) =   \n  \\begin{bmatrix}  y_1     \\\\\n                   y_2     \\\\\n                   \\vdots   \\\\\n                   y_{n-1} \\\\\n                   y_n\n  \\end{bmatrix}.</math>\n\nExtending this method until ''n'' = ''N'' yields the solution <math>\\vec x</math>.\n\nIn practice, these steps are often done concurrently with the rest of the procedure, but they form a coherent unit and deserve to be treated as their own step.\n\n== Block Levinson algorithm ==\nIf '''''M''''' is not strictly Toeplitz, but [[block matrix|block]] Toeplitz, the Levinson recursion can be derived in much the same way by regarding the block Toeplitz matrix as a Toeplitz matrix with matrix elements (Musicus 1988). Block Toeplitz matrices arise naturally in signal processing algorithms when dealing with multiple signal streams (e.g., in [[System analysis#Characterization of systems|MIMO]] systems) or cyclo-stationary signals.\n\n== See also ==\n*[[Split Levinson recursion]]\n*[[Linear prediction]]\n*[[Autoregressive model]]\n\n== Notes ==\n{{reflist}}\n\n== References ==\n'''Defining sources'''\n* Levinson, N. (1947). \"The Wiener RMS error criterion in filter design and prediction.\" ''J. Math. Phys.'', v. 25, pp.&nbsp;261–278.\n* Durbin, J. (1960). \"The fitting of time series models.\" ''Rev. Inst. Int. Stat.'', v. 28, pp.&nbsp;233–243.\n* Trench, W. F. (1964).  \"An algorithm for the inversion of finite Toeplitz matrices.\"  ''J. Soc. Indust. Appl. Math.'', v. 12, pp.&nbsp;515–522.\n* Musicus, B. R. (1988). \"Levinson and Fast Choleski Algorithms for Toeplitz and Almost Toeplitz Matrices.\" ''RLE TR'' No. 538, MIT. [http://dspace.mit.edu/bitstream/1721.1/4954/1/RLE-TR-538-20174000.pdf]\n* Delsarte, P. and Genin, Y. V. (1986). \"The split Levinson algorithm.\" ''IEEE Transactions on Acoustics, Speech, and Signal Processing'', v. ASSP-34(3), pp.&nbsp;470–478.\n'''Further work'''\n*{{cite journal | last1 = Bojanczyk | first1 = A.W. | last2 = Brent | first2 = R.P. | last3 = De Hoog | first3 = F.R. | last4 = Sweet | first4 = D.R. | year = 1995 | title = On the stability of the Bareiss and related Toeplitz factorization algorithms | url = | journal = [[SIAM Journal on Matrix Analysis and Applications]] | volume = 16 | issue = | pages = 40–57 | doi = 10.1137/S0895479891221563 | arxiv = 1004.5510 }}\n*[[Richard P. Brent|Brent R.P.]] (1999), \"Stability of fast algorithms for structured linear systems\", ''Fast Reliable Algorithms for Matrices with Structure'' (editors—T. Kailath, A.H. Sayed), ch.4 ([[Society for Industrial and Applied Mathematics|SIAM]]).\n* Bunch, J. R. (1985). \"Stability of methods for solving Toeplitz systems of equations.\" ''SIAM J. Sci. Stat. Comput.'', v. 6, pp.&nbsp;349–364. [https://doi.org/10.1137/0906025]\n*{{cite journal | last = Krishna | first = H. |author2=Wang, Y.  | title = The Split Levinson Algorithm is weakly stable | journal = [[SIAM Journal on Numerical Analysis]] | volume = 30 | issue = 5 | pages = 1498–1508 | date = 1993 | url = http://scitation.aip.org/getabs/servlet/GetabsServlet?prog=normal&id=SJNAAM000030000005001498000001&idtype=cvips&gifs=yes | doi = 10.1137/0730078}}\n'''Summaries'''\n* Bäckström, T. (2004). \"2.2. Levinson–Durbin Recursion.\" ''Linear Predictive Modelling of Speech – Constraints and Line Spectrum Pair Decomposition.'' Doctoral thesis. Report no. 71 / Helsinki University of Technology, Laboratory of Acoustics and Audio Signal Processing. Espoo, Finland. [http://lib.tkk.fi/Diss/2004/isbn9512269473/isbn9512269473.pdf]\n* Claerbout, Jon F. (1976). \"Chapter 7 – Waveform Applications of Least-Squares.\" ''Fundamentals of Geophysical Data Processing.''  Palo Alto: Blackwell Scientific Publications. [https://web.archive.org/web/20060921204908/http://sep.stanford.edu/oldreports/fgdp2/fgdp_07.pdf]\n*{{Citation |last1=Press|first1=WH|last2=Teukolsky|first2=SA|last3=Vetterling|first3=WT|last4=Flannery|first4=BP|year=2007|title=Numerical Recipes: The Art of Scientific Computing|edition=3rd|publisher=Cambridge University Press| publication-place=New York|isbn=978-0-521-88068-8|chapter=Section 2.8.2. Toeplitz Matrices|chapter-url=http://apps.nrbook.com/empanel/index.html?pg=96}}\n* Golub, G.H., and Loan, C.F. Van (1996). \"Section 4.7 : Toeplitz and related Systems\" ''Matrix Computations'', Johns Hopkins University Press\n\n[[Category:Matrices]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Lie group integrator",
      "url": "https://en.wikipedia.org/wiki/Lie_group_integrator",
      "text": "{{context|date=March 2017}}\nA '''Lie group integrator''' is a [[numerical integration]] [[numerical method|method]] for [[differential equation]]s built from [[coordinate-independent operation]]s such as [[Lie group action]]s on a [[manifold]].<ref>{{Cite journal|arxiv=1207.0069|title=An introduction to Lie group integrators -- basics, new developments and applications|journal=Journal of Computational Physics|volume=257|issue=2014|pages=1040–1061|last1=Celledoni|first1=Elena|last2=Marthinsen|first2=Håkon|last3=Owren|first3=Brynjulf|year=2012|doi=10.1016/j.jcp.2012.12.031|bibcode=2014JCoPh.257.1040C}}</ref><ref>{{cite web|url=http://www.math.ucsd.edu/~mleok/pdf/mleok_scicade07.pdf|title=AN OVERVIEW OF LIE GROUP VARIATIONAL INTEGRATORS AND THEIR APPLICATIONS TO OPTIMAL CONTROL}}</ref><ref>{{Cite journal|last=Iserles|first=Arieh|last2=Munthe-Kaas|first2=Hans Z.|last3=Nørsett|first3=Syvert P.|last4=Zanna|first4=Antonella|date=2000-01-01|title=Lie-group methods|url=https://www.cambridge.org/core/journals/acta-numerica/article/liegroup-methods/856125FF1EAF7762DEF6E37EEBA9CA5F|journal=Acta Numerica|volume=9|pages=215–365|issn=1474-0508}}</ref> They have been used for the [[computer animation|animation]] and control of [[vehicle]]s in [[computer graphics]] and [[control system]]s/[[artificial intelligence]] research.<ref>{{cite web|url=https://www.cs.cmu.edu/~kmcrane/Projects/LieGroupIntegrators/paper.pdf|title=Lie Group Integrators for the animation and control of vehicles}}</ref>  These tasks are particularly difficult because they feature [[nonholonomic]] [[Constraint (classical mechanics)|constraint]]s.\n\n== See also ==\n* [[Lie group]]\n* [[numerical methods for ordinary differential equations]]\n* [[Euler integration]]\n* [[Runge–Kutta methods]]\n* [[Variational integrator]]\n* [[Parallel parking problem]]\n\n==References==\n{{Reflist}}\n\n[[Category:Numerical analysis]]\n\n\n{{applied-math-stub}}"
    },
    {
      "title": "Linear algebra",
      "url": "https://en.wikipedia.org/wiki/Linear_algebra",
      "text": "{{Distinguish|Elementary algebra}}\n[[File:Linear subspaces with shading.svg|thumb|250px|right|In the three-dimensional [[Euclidean space]], these three planes represent solutions of linear equations and their intersection represents the set of common solutions: in this case, a unique point. The blue line is the common solution of a pair of linear equations. ]]\n\n'''Linear algebra''' is the branch of [[mathematics]] concerning [[linear equation]]s such as \n:<math>a_1x_1+\\cdots +a_nx_n=b,</math>\n[[linear map|linear functions]] such as\n:<math>(x_1, \\ldots, x_n) \\mapsto a_1x_1+\\ldots +a_nx_n,</math>\nand their representations through [[matrix (mathematics)|matrices]] and [[vector space]]s.<ref>{{Citation | last = Banerjee | first = Sudipto | last2 = Roy | first2 = Anindya | date = 2014 | title = Linear Algebra and Matrix Analysis for Statistics | series = Texts in Statistical Science | publisher = Chapman and Hall/CRC | edition =  1st | isbn =  978-1420095388}}</ref><ref>{{Citation|last=Strang|first=Gilbert|date=July 19, 2005|title=Linear Algebra and Its Applications|publisher=Brooks Cole|edition=4th|isbn=978-0-03-010567-8}}</ref><ref>{{cite web|last=Weisstein|first=Eric|title=Linear Algebra|url=http://mathworld.wolfram.com/LinearAlgebra.html|work=From MathWorld--A Wolfram Web Resource.|publisher=Wolfram|accessdate=16 April 2012}}</ref>\n\nLinear algebra is central to almost all areas of mathematics. For instance, linear algebra is fundamental in modern presentations of [[geometry]], including for defining basic objects such as [[line (geometry)|lines]], [[plane (geometry)|planes]] and [[rotation (mathematics)|rotations]]. Also, [[functional analysis]] may be basically viewed as the application of linear algebra to spaces of functions. Linear algebra is also used in most sciences and [[engineering]] areas, because it allows [[mathematical model|modeling]] many natural phenomena, and efficiently computing with such models. For [[nonlinear system]]s, which cannot be modeled with linear algebra, linear algebra is often used as a first-order approximation.\n\n==History==\nThe procedure for solving simultaneous linear equations now called [[Gaussian elimination]] appears in the ancient Chinese mathematical text [[Rod calculus#System of linear equations|Chapter Eight: ''Rectangular Arrays'']] of ''[[The Nine Chapters on the Mathematical Art]]''. Its use is illustrated in eighteen problems, with two to five equations. <ref>{{Cite book|last=Hart|first=Roger|title=The Chinese Roots of Linear Algebra|publisher=[[JHU Press]]|year=2010|url=https://books.google.com/books?id=zLPm3xE2qWgC&printsec=frontcover}}</ref>\n\n[[Systems of linear equations]] arose in Europe with the introduction in 1637 by [[René Descartes]] of [[coordinates]] in [[geometry]]. In fact, in this new geometry, now called [[Cartesian geometry]], lines and planes are represented by linear equations, and computing their intersections amounts to solving systems of linear equations.\n\nThe first systematic methods for solving linear systems used [[determinant]]s, first considered by [[Gottfried Wilhelm Leibniz|Leibniz]] in 1693. In 1750, [[Gabriel Cramer]] used them for giving explicit solutions of linear systems, now called [[Cramer's rule]]. Later, [[Gauss]] further described the method of elimination, which was initially listed as an advancement in [[geodesy]].<ref name=\"Vitulli, Marie\">{{cite web|last=Vitulli|first=Marie|authorlink= Marie A. Vitulli |title=A Brief History of Linear Algebra and Matrix Theory|url=http://darkwing.uoregon.edu/~vitulli/441.sp04/LinAlgHistory.html|work=Department of Mathematics|publisher=University of Oregon|archiveurl=https://web.archive.org/web/20120910034016/http://darkwing.uoregon.edu/~vitulli/441.sp04/LinAlgHistory.html|archivedate=2012-09-10| accessdate=2014-07-08}}</ref>\n\nIn 1844 [[Hermann Grassmann]] published his \"Theory of Extension\" which included foundational new topics of what is today called linear algebra. In 1848, [[James Joseph Sylvester]] introduced the term ''matrix'', which is Latin for ''womb''. \n\nLinear algebra grew with ideas noted in the [[complex plane]]. For instance, two numbers ''w'' and ''z'' in ℂ have a difference ''w'' – ''z'', and the line segments <math>\\overline{w z} \\ \\ \\text{and}\\ \\  \\overline{0(w-z)}</math> are of the same length and direction. The segments are [[equipollence (geometry)|equipollent]]. The four-dimensional system ℍ of [[quaternion]]s was started in 1843. The term ''vector'' was introduced as ''v'' = ''x'' i + ''y'' j + ''z'' k representing a point in space. The quaternion difference ''p'' – ''q'' also produces a segment equipollent to <math>\\overline{p q} .</math>  \nOther [[hypercomplex number]] systems also used the idea of a linear space with a [[basis (linear algebra)|basis]].\n\n[[Arthur Cayley]] introduced [[matrix multiplication]]  and the [[inverse matrix]] in 1856, making possible the [[general linear group]]. The mechanism of [[group representation]] became available for describing complex and hypercomplex numbers. Crucially, Cayley used a single letter to denote a matrix, thus treating a matrix as an aggregate object. He also realized the connection between matrices and determinants, and wrote \"There would be many things to say about this theory of matrices which should, it seems to me, precede the theory of determinants\".<ref name=\"Vitulli, Marie\"/>\n\n[[Benjamin Peirce]] published his ''Linear Associative Algebra'' (1872), and his son [[Charles Sanders Peirce]] extended the work later.<ref>[[Benjamin Peirce]] (1872) ''Linear Associative Algebra'', lithograph, new edition with corrections, notes, and an added 1875 paper by Peirce, plus notes by his son [[Charles Sanders Peirce]], published in the ''American Journal of Mathematics'' v. 4, 1881, Johns Hopkins University, pp.&nbsp;221–226, ''Google'' [https://books.google.com/books?id=LQgPAAAAIAAJ&pg=PA221 Eprint] and as an extract, D. Van Nostrand, 1882, ''Google'' [https://books.google.com/books?id=De0GAAAAYAAJ&printsec=frontcover Eprint].</ref>\n\nThe [[telegraph]] required an explanatory system, and the 1873 publication of [[A Treatise on Electricity and Magnetism]] instituted a [[field theory (physics)|field theory]] of forces and required [[differential geometry]] for expression. Linear algebra is flat differential geometry and serves in tangent spaces to [[manifold]]s. Electromagnetic symmetries of spacetime are expressed by the [[Lorentz transformation]]s, and much of the history of linear algebra is the [[history of Lorentz transformations]].\n\nThe first modern and more precise definition of a vector space was introduced by [[Peano]] in 1888;<ref name=\"Vitulli, Marie\"/> by 1900, a theory of linear transformations of finite-dimensional vector spaces had emerged. Linear algebra took its modern form in the first half of the twentieth century, when many ideas and methods of previous centuries were generalized as [[abstract algebra]]. The development of computers led to increased research in efficient [[algorithm]]s for Gaussian elimination and matrix decompositions, and linear algebra became an essential tool for modelling and simulations.<ref name=\"Vitulli, Marie\"/>\n\nSee also {{slink|Determinant|History}} and {{slink|Gaussian elimination|History}}.\n\n==Vector spaces==\n{{Main|Vector space}}\nUntil the 19th century, linear algebra was introduced through [[systems of linear equations]] and [[matrix (mathematics)|matrices]]. In modern mathematics, the presentation through ''vector spaces'' is generally preferred, since it is more synthetic, more general (not limited to the finite-dimensional case), and conceptually simpler, although more abstract.\n\nA vector space over a [[field (mathematics)|field]] {{math|''F''}} (often the field of the [[real number]]s) is a [[Set (mathematics)|set]] {{math|''V''}} equipped with two [[binary operation]]s satisfying the following [[axiom]]s.  \n[[element (mathematics)|Elements]] of {{math|''V''}} are called ''vectors'', and elements of ''F'' are called ''scalars''. The first operation, ''[[vector addition]]'', takes any two vectors {{math|''v''}} and {{math|''w''}} and outputs a third vector {{math|''v'' + ''w''}}. The second operation, ''[[scalar multiplication]]'', takes any scalar {{math|''a''}} and any vector {{math|''v''}} and outputs a new {{nowrap|vector {{math|''av''}}}}. The axioms that addition and scalar multiplication must satisfy are the following. (In the list below, {{math|''u'', ''v''}} and {{math|''w''}} are arbitrary elements of {{math|''V''}}, and {{math|''a''}} and {{math|''b''}} are arbitrary scalars in the field {{math|''F''}}.)<ref>{{Harvard citations|last=Roman|year=2005|nb=yes|loc=ch. 1, p. 27}}</ref>\n\n{| border=\"0\" style=\"width:100%;\"\n|-\n| '''Axiom''' ||'''Signification'''\n|-\n| [[Associativity]] of addition || {{math|1=''u'' + (''v'' + ''w'') = (''u'' + ''v'') + ''w''}}\n|- style=\"background:#F8F4FF;\"\n| [[Commutativity]] of addition || {{math|1=''u'' + ''v'' = ''v'' + ''u''}}\n|-\n| [[Identity element]] of addition || There exists an element {{math|0}} in {{math|''V''}}, called the ''[[zero vector]]'' (or simply ''zero''), such that {{math|1=''v'' + 0 = ''v''}} for all {{math|''v''}} in {{math|''V''}}.\n|- style=\"background:#F8F4FF;\"\n| [[Inverse element]]s of addition || For every {{math|''v''}} in {{math|''V''}}, there exists an element {{math|−''v''}} in {{math|''V''}}, called the ''[[additive inverse]]'' of {{math|''v''}}, such that {{math|1=''v'' + (−''v'') = 0}}\n|-\n| [[Distributivity]] of scalar multiplication with respect to vector addition  || {{math|1=''a''(''u'' + ''v'') = ''au'' + ''av''}}\n|- style=\"background:#F8F4FF;\"\n| Distributivity of scalar multiplication with respect to field addition || {{math|1=(''a'' + ''b'')''v'' = ''av'' + ''bv''}}\n|-\n| Compatibility of scalar multiplication with field multiplication || {{math|1=''a''(''bv'') = (''ab'')''v''}} <ref group=nb>This axiom is not asserting the associativity of an operation, since there are two operations in question, scalar multiplication: ''bv''; and field multiplication: ''ab''.</ref>\n|- style=\"background:#F8F4FF;\"\n| Identity element of scalar multiplication || {{math|1=1''v'' = ''v''}}, where {{math|1}} denotes the [[multiplicative identity]] of {{mvar|F}}.\n|}\n\nThe first four axioms mean that {{math|''V''}} is an [[abelian group]] under addition.\n\nElements of a vector space may have various nature; for example, they can be [[sequence]]s, [[function (mathematics)|functions]], [[polynomial ring|polynomials]] or [[matrix (mathematics)|matrices]]. Linear algebra is concerned with properties common to all vector spaces.\n\n===Linear maps===\n{{main|Linear map}}\n'''Linear maps''' are [[map (mathematics)|mappings]] between vector spaces that preserve the vector-space structure. Given two vector spaces {{math|''V''}} and {{math|''W''}} over a field {{mvar|F}}, a linear map (also called, in some contexts, linear transformation, linear mapping or linear operator) is a [[map (mathematics)|map]]\n\n: <math> T:V\\to W </math>\n\nthat is compatible with addition and scalar multiplication, that is\n\n: <math> T(u+v)=T(u)+T(v), \\quad T(av)=aT(v) </math>\n\nfor any vectors {{math|''u'',''v''}} in {{math|''V''}} and scalar {{math|''a''}} in {{mvar|F}}.\n\nThis implies that for any vectors {{math|''u'', ''v''}} in {{math|''V''}} and scalars {{math|''a'', ''b''}} in {{mvar|F}}, one has\n\n: <math>T(au+bv)=T(au)+T(bv)=aT(u)+bT(v) </math>\n\nWhen a [[bijective]] linear map exists between two vector spaces (that is, every vector from the second space is associated with exactly one in the first), the two spaces are [[isomorphic]]. Because an isomorphism preserves linear structure, two isomorphic vector spaces are \"essentially the same\" from the linear algebra point of view, in the sense that they cannot be distinguished by using vector space properties. An essential question in linear algebra is testing whether a linear map is an isomorphism or not, and, if it is not an isomorphism, finding its [[Range (mathematics)|range]] (or image) and the set of elements that are mapped to the zero vector, called the [[Kernel (linear operator)|kernel]] of the map. All these questions can be solved by using [[Gaussian elimination]] or some variant of this [[algorithm]].\n\n===Subspaces, span, and basis===\n{{main|Linear subspace|Linear span|Basis (linear algebra)}}\nThe study of subsets of vector spaces that are themselves vector spaces for the induced operations is fundamental, similarly as for many mathematical structures. These subsets are called [[linear subspace]]s. More precisely, a linear subspace of a vector space {{mvar|V}} over a field {{mvar|F}} is a [[subset]] {{mvar|W}} of {{mvar|V}} such that {{math|''u'' + ''v''}} and {{math|''au''}} are in {{mvar|W}}, for every {{mvar|u}}, {{mvar|v}} in {{mvar|W}}, and every {{mvar|a}} in {{mvar|F}}. (These conditions suffices for implying that {{mvar|W}} is a vector space.)\n\nFor example, the [[image (function)|image]] of a linear map, and the [[inverse image]] of 0 by a linear map (called [[kernel (linear algebra)|kernel]] or [[null space]]) are linear subspaces.\n\nAnother important way of forming a subspace is to consider [[linear combination]]s of a set {{mvar|S}} of vectors: the set of all sums \n: <math> a_1 v_1 + a_2 v_2 + \\cdots + a_k v_k,</math>\nwhere {{math|''v''<sub>1</sub>, ''v''<sub>2</sub>, ..., ''v<sub>k</sub>''}} are in {{mvar|V}}, and {{math|''a''<sub>1</sub>, ''a''<sub>2</sub>, ..., ''a''<sub>''k''</sub>}} are in {{mvar|F}} form a linear subspace called the [[Linear span|span]] of {{mvar|S}}. The span of {{mvar|S}} is also the intersection of all linear subspaces containing {{mvar|S}}. In other words, it is the (smallest for the inclusion relation) linear subspace containing {{mvar|S}}.\n\nA set of vectors is [[linearly independent]] if none is in the span of the others. Equivalently, a set {{mvar|S}} of vector is linearly independent if the only way to express the zero vector as a linear combination of elements of {{mvar|S}} is to take zero for every coefficient <math>a_i.</math>\n\nA set of vectors that spans a vector space is called a [[spanning set]] or [[generating set]]. If a spanning set {{mvar|S}} is ''linearly dependent'' (that is not linearly independent), then some element {{mvar|w}} of {{mvar|S}} is in the span of the other elements of {{mvar|S}}, and the span would remain the same if one remove {{mvar|w}} from {{mvar|S}}. One may continue to remove elements of {{mvar|S}} until getting a ''linearly independent spanning set''. Such a linearly independent set that spans a vector space {{mvar|V}} is called a [[Basis (linear algebra)|basis]] of {{math|''V''}}. The importance of bases lies in the fact that there are together minimal generating sets and maximal independent sets. More precisely, if {{math|S}} is a linearly independent set, and {{mvar|T}} is a spanning set such that <math>S\\subseteq T,</math> then there is a basis {{mvar|B}} such that <math>S\\subseteq B\\subseteq T.</math>\n\nAny two bases of a vector space {{math|''V''}} have the same [[cardinality]], which is called the [[Dimension (vector space)|dimension]] of {{math|''V''}}; this is the [[dimension theorem for vector spaces]]. Moreover, two vector spaces over the same field {{mvar|F}} are [[isomorphic]] if and only if they have the same dimension.<ref>Axler (2004), p. 55</ref>\n\nIf any basis of {{math|''V''}} (and therefore every basis) has a finite number of elements, {{math|''V''}} is a ''finite-dimensional vector space''. If {{math|''U''}} is a subspace of {{math|''V''}}, then {{math|dim ''U'' ≤ dim ''V''}}. In the case where {{math|''V''}} is finite-dimensional, the equality of the dimensions implies {{math|1=''U'' = ''V''}}.\n\nIf ''U''<sub>1</sub> and ''U''<sub>2</sub> are subspaces of ''V'', then\n\n:<math>\\dim(U_1 + U_2) = \\dim U_1 + \\dim U_2 - \\dim(U_1 \\cap U_2),</math>\nwhere <math>U_1+U_2</math>denotes the span of <math>U_1\\cup U_2.</math><ref>Axler (2204), p. 33</ref>\n\n==Matrices==\n{{Main|Matrix (mathematics)}}\n\nMatrices allow explicit manipulation of finite-dimensional vector spaces and [[linear maps]]. Their theory is thus an essential part of linear algebra.\n\nLet {{mvar|V}} be a finite-dimensional vector space over a field {{math|''F''}}, and {{math|(''v''<sub>1</sub>, ''v''<sub>2</sub>, ..., ''v<sub>m</sub>'')}} be a basis of {{math|''V''}} (thus {{mvar|m}} is the dimension of {{math|''V''}}). By definition of a basis, the map\n:<math>\\begin{align}\n(a_1, \\ldots, a_m)&\\mapsto a_1v_1+\\cdots a_mv_m\\\\\nF^m &\\to V\n\\end{align}</math>\nis a bijection from <math>F^m,</math> the set of the [[sequence (mathematics)|sequences]] of {{mvar|m}} elements of {{mvar|F}}, onto {{mvar|V}}. This is an [[isomorphism]] of vector spaces, if <math>F^m</math> is equipped of its standard structure of vector space, where vector addition and scalar multiplication are done component by component.\n\nThis isomorphism allows representing a vector by its [[inverse image]] under this isomorphism, that is by the [[coordinates vector]] <math>(a_1, \\ldots, a_m)</math> or by the [[column matrix]]\n:<math>\\begin{bmatrix}a_1\\\\\\vdots\\\\a_m\\end{bmatrix}.</math>\n\nIf {{mvar|W}} is another finite dimensional vector space (possibly the same), with a basis <math>(w_1,\\ldots,w_n),</math> a linear map {{mvar|f}} from {{mvar|W}} to {{mvar|V}} is well defined by its values on the basis elements, that is <math>(f(w_1),\\ldots,f(w_n)).</math> Thus, {{mvar|f}} is well represented by the list of the corresponding column matrices. That is, if \n:<math>f(w_j)=a_{1,j}v_1 + \\cdots+a_{m,j}v_m,</math> \nfor {{math|1=''j'' = 1, ..., ''n''}}, then {{mvar|f}} is represented by the matrix\n:<math>\\begin{bmatrix}a_{1,1}&\\ldots&a_{1,n}\\\\\n\\vdots&\\ldots&\\vdots\\\\\na_{m,1}&\\ldots&a_{m,n}\\end{bmatrix},</math>\nwith {{mvar|m}} rows and {{mvar|n}} columns.\n\n[[Matrix multiplication]] is defined in such a way that the product of two matrices is the matrix of the [[function composition|composition]] of the corresponding linear maps, and the product of a matrix and a column matrix is the column matrix representing the result of applying the represented linear map to the represented vector. It follows that the theory of finite-dimensional vector spaces and the theory of matrices are two different languages for expressing exactly the same concepts.\n\nTwo matrices that encode the same linear transformation in different bases are called [[similar (linear algebra)|similar]]. Equivalently, two matrices are similar if one can transform one in the other by [[Elementary matrix|elementary row and column operations]]. For a matrix representing a linear map from {{mvar|W}} to {{mvar|V}}, the row operations correspond to change of bases in {{mvar|V}} and the column operations correspond to change of bases in {{mvar|W}}. Every matrix is similar to an [[identity matrix]] possibly bordered by zero rows and zero columns. In terms of vector space, this means that, for any linear map from {{mvar|W}} to {{mvar|V}}, there are bases such that a part of the basis of {{mvar|W}} is mapped bijectively on a part of the basis of {{mvar|V}}, and that the remaining basis elements of {{mvar|W}}, if any, are mapped to zero (this is a way of expressing the [[fundamental theorem of linear algebra]]). [[Gaussian elimination]] is the basic algorithm for finding these elementary operations, and proving this theorem.\n\n==Linear systems==\n{{Main|System of linear equations}}\nSystems of linear equations form a fundamental part of linear algebra. Historically, linear algebra and matrix theory has been developed for solving such systems. In the modern presentation of linear algebra through vector spaces and matrices, many problems may be interpreted in terms of linear systems.\n\nFor example, let\n:<math>\\begin{alignat}{7}\n2x &&\\; + \\;&& y             &&\\; - \\;&& z  &&\\; = \\;&& 8  \\\\\n-3x &&\\; - \\;&& y             &&\\; + \\;&& 2z &&\\; = \\;&& -11 \\\\\n-2x &&\\; + \\;&& y &&\\; +\\;&& 2z  &&\\; = \\;&& -3 \n\\end{alignat}\\qquad \\text{(S)}</math>\nbe a linear system.\n\nTo such a system, one may associate its matrix \n:<math>M\\left[\\begin{array}{rrr}\n2 & 1 & -1\\\\\n-3 & -1 & 2  \\\\\n-2 & 1 & 2\n\\end{array}\\right]\\text{.}\n</math>\nand its right member vector\n:<math>v=\\begin{bmatrix}\n8\\\\-11\\\\-3\n\\end{bmatrix}.\n</math>\n\nLet {{mvar|T}} be the linear transformation associated to the matrix {{mvar|M}}. A solution of the system {{math|(S)}} is a vector \n:<math>X=\\begin{bmatrix}\nx\\\\y\\\\z\n\\end{bmatrix}</math> \nsuch that \n:<math>T(X)=v,</math>\nthat is an element of the [[preimage]] of {{mvar|v}} by {{mvar|T}}.\n\nLet {{math|(S')}} be the associated [[homogeneous system]], where the right-hand sides of the equations are put to zero. The solutions of {{math|(S')}} are exactly the elements of the [[kernel (linear algebra)|kernel]] of {{math|T}} or, equivalently, {{mvar|M}}.\n\nThe [[Gaussian elimination|Gaussian-elimination]] consists of performing [[elementary row operation]]s on the [[augmented matrix]]\n:<math>M\\left[\\begin{array}{rrr|r}\n2 & 1 & -1&8\\\\\n-3 & -1 & 2&-11  \\\\\n-2 & 1 & 2&-3\n\\end{array}\\right]\n</math>\nfor putting it in [[reduced row echelon form]]. These row operations do not change the set of solutions of the system of equations. In the example, the reduced echelon form is \n:<math>M\\left[\\begin{array}{rrr|r}\n1 & 0 & 0&2\\\\\n0 & 1 & 0&3  \\\\\n0 & 0 & 1&-1\n\\end{array}\\right],\n</math>\nshowing that the system {{math|(S)}} has the unique solution\n:<math>\\begin{align}x&=2\\\\y&=3\\\\z&=-1.\\end{align}</math>\n\nIt follows from this matrix interpretation of linear systems that the same methods can be applied for solving linear systems and for many operations on matrices and linear transformations, which include the computation of the [[rank of a matrix|ranks]], [[kernel (linear algebra)|kernels]], [[matrix inverse]]s.\n\n==Endomorphisms and square matrices==\n{{main|Square matrix}}\nA linear [[endomorphism]] is a linear map that maps a vector space {{mvar|V}} to itself. \nIf {{mvar|V}} has a basis of {{mvar|n}} elements, such an endomorphism is represented by a square matrix of size {{mvar|n}}.\n\nWith respect to general linear maps, linear endomorphisms and square matrices have some specific properties that make their study an important part of linear algebra, which is used in many parts of mathematics, including [[geometric transformation]]s, [[coordinate change]]s, [[quadratic form]]s, and many other part of mathematics.\n\n===Determinant===\n{{main|Determinant}}\nThe ''determinant'' of a square matrix is a [[polynomial function]] of the entries of the matrix, such that the matrix is [[invertible matrix|invertible]] if and only if the determinant is not zero. This results from the fact that the determinant of a product of matrices is the product of the determinants, and thus that a matrix is invertible if and only if its determinant is invertible.\n\n[[Cramer's rule]] is a [[closed-form expression]], in terms of determinants, of the solution of a [[system of linear equations|system of {{mvar|n}} linear equations in {{mvar|n}} unknowns]]. Cramer's rule is useful for reasoning about the solution, but, except for {{math|1=''n'' = 2}} or {{math|3}}, it is rarely used for computing a solution, since [[Gaussian elimination]] is a faster algorithm.\n\nThe ''determinant of an endomorphism'' is the determinant of the matrix representing the endomorphism in terms of some ordered basis. This definition makes sense, since this determinant is independent of the choice of the basis.\n\n===Eigenvalues and eigenvectors===\n{{main|Eigenvalues and eigenvectors}}\nIf {{mvar|f}} is a linear endomorphism of a vector space {{mvar|V}} over a field {{mvar|F}}, an ''eigenvector'' of {{mvar|f}} is a nonzero vector {{mvar|v}} of {{mvar|V}} such that {{math|1=''f''(''v'') = ''av''}} for some scalar {{mvar|a}} in {{mvar|F}}. This scalar {{mvar|a}} is an ''eigenvalue'' of {{mvar|f}}.\n\nIf the dimension of {{mvar|V}} is finite, and a basis has been chosen, {{mvar|f}} and {{mvar|v}} may be represented, respectively, by a square matrix {{mvar|M}} and a column matrix {{mvar|z}}; the equation defining eigenvectors and eigenvalues becomes\n:<math>Mz=az.</math>\nUsing the [[identity matrix]] {{mvar|I}}, whose entries are all zero, except those of the main diagonal, which are equal to one, this may be rewritten\n:<math>(M-aI)z=0.</math>\nAs {{mvar|z}} is supposed to be nonzero, this means that {{math|''M'' – ''aI''}} is a [[singular matrix]], and thus that its determinant <math>\\det(M-aI)</math> equals zero. The eigenvalues are thus the [[root of a function|roots]] of the [[polynomial]]\n:<math>\\det(xI-M).</math>\nIf {{mvar|V}} is of dimension {{mvar|n}}, this is a [[monic polynomial]] of degree {{mvar|n}}, called the [[characteristic polynomial]] of the matrix (or of the endomorphism), and there are, at most, {{mvar|n}} eigenvalues.\n\nIf a basis exists that consists only of eigenvectors, the matrix of {{mvar|f}} on this basis has a very simple structure: it is a [[diagonal matrix]] such that the entries on the [[main diagonal]] are eigenvalues, and the other entries are zero. In this case, the endomorphism and the matrix are said [[diagonalizable matrix|diagonalizable]]. More generally, an endomorphism and a matrix are also said diagonalizable, if they become diagonalizable after [[field extension|extending]] the field of scalars. In this extended sense, if the characteristic polynomial is [[square-free polynomial|square-free]], then the matrix is diagonalizable.\n\nA [[symmetric matrix]] is always diagonalizable. There are non-diagonalizable matrices, the simplest being\n:<math>\\begin{bmatrix}0&1\\\\0&0\\end{bmatrix}</math>\n(it cannot be diagonalizable since its square is the [[zero matrix]], and the square of a nonzero diagonal matrix is never zero).\n\nWhen an endomorphism is not diagonalizable, there are bases on which it has a simple form, although not as simple as the diagonal form. The [[Frobenius normal form]] does not need of extending the field of scalars and makes the characteristic polynomial immediately readable on the matrix. The [[Jordan normal form]] requires to extend the field of scalar for containing all eigenvalues, and differs from the diagonal form only by some entries that are just above the main diagonal and are equal to 1.\n\n==Duality==\n{{main|Dual space}}\nA [[linear form]] is a linear map from a vector space {{mvar|V}} over a field {{mvar|F}} to the field of scalars {{mvar|F}}, viewed as a vector space over itself. Equipped by [[pointwise]] addition and multiplication by a scalar, the linear forms form a vector space, called the '''dual space''' of {{mvar|V}}, and usually denoted <math>V^*.</math>\n\nIf <math>v_1, \\ldots, v_n</math> is a basis of {{mvar|V}} (this implies that {{mvar|V}} is finite-dimensional), then one can define, for {{math|1=''i'' = 1, ..., ''n''}}, a linear map <math>v_i^*</math> such that <math>v_i^*(e_i)=1</math> and <math>v_i^*(e_j)=0</math> if {{math|''j'' ≠ ''i''}}. These linear maps form a basis of <math>V^*,</math> called the [[dual basis]] of <math>v_1, \\ldots, v_n.</math> (If {{mvar|V}} is not finite-dimensional, the <math>v^*_i</math> may be defined similarly; they are linearly independent, but do not form a basis.)\n\nFor {{mvar|v}} in {{mvar|V}}, the map\n:<math>f\\to f(v)</math>\nis a linear form on <math>V^*.</math> This defines the [[canonical map|canonical linear map]] from {{mvar|V}} into <math>V^{**},</math> the dual of <math>V^*,</math> called the '''bidual''' of {{mvar|V}}. This canonical map is an [[isomorphism]] if {{mvar|V}} is finite-dimensional, and this allows identifying {{mvar|V}} with its bidual. (In the infinite dimensional case, the canonical map is injective, but not surjective.)\n\nThere is thus a complete symmetry between a finite-dimensional vector space and its dual. This motivates the frequent use, in this context, of the [[bra–ket notation]]\n:<math>\\langle f,x\\rangle</math>\nfor denoting {{math|''f''{{space|hair}}(''x'')}}.\n\n===Dual map===\n{{main|Transpose of a linear map}}\n\nLet \n:<math>f:V\\to W</math>\nbe a linear map. For every linear form {{mvar|h}} on {{mvar|W}}, the [[composite function]] {{math|''h'' ∘ ''f''}} is a linear form on {{mvar|V}}. This defines a linear map\n:<math>f^*:W^*\\to V^*</math>\nbetween the dual spaces, which is called the '''dual''' or the '''transpose''' of {{mvar|f}}.\n\nIf {{mvar|V}} and {{mvar|W}} are finite dimensional, and {{mvar|M}} is the matrix of {{mvar|f}} in terms of some ordered bases, then the matrix of <math>f^*</math> over the dual bases is the [[transpose]] <math>M^\\mathsf T</math> of {{mvar|M}}, obtained by exchanging rows and columns.\n\nIf elements of vector spaces and their duals are represented by column vectors, this duality may be expressed in [[bra–ket notation]] by \n:<math>\\langle h^\\mathsf T , Mv\\rangle = \\langle h^\\mathsf T M,v\\rangle.</math>\nFor highlighting this symmetry, the two members of this equality are sometimes written \n:<math>\\langle h^\\mathsf T \\mid M\\mid v\\rangle.</math>\n\n===Inner-product spaces===\n{{cleanup|section|reason=Need for a more encyclopedic style, which is homogeneous with the style of preceding sections. Also, some details do not belong to this general article but to more specialized ones. Also, inner product spaces should appear as a special instance of the more general concept of [[bilinear form]]. Finally, complex conjugation should appear in a specific section on linear algebra over the complexes.|date=August 2018}}\n{{main|Inner product space}}\nBesides these basic concepts, linear algebra also studies vector spaces with additional structure, such as an [[inner product]]. The inner product is an example of a [[bilinear form]], and it gives the vector space a geometric structure by allowing for the definition of length and angles. Formally, an ''inner product'' is a map\n\n:<math> \\langle \\cdot, \\cdot \\rangle : V \\times V \\rightarrow F </math>\n\nthat satisfies the following three [[axiom]]s for all vectors ''u'', ''v'', ''w'' in ''V'' and all scalars ''a'' in ''F'':<ref name= Jain>{{cite book|title=Functional analysis|author=P. K. Jain, Khalil Ahmad|url=https://books.google.com/?id=yZ68h97pnAkC&pg=PA203|page=203|chapter=5.1 Definitions and basic properties of inner product spaces and Hilbert spaces|isbn=81-224-0801-X|year=1995|edition=2nd|publisher=New Age International}}</ref><ref name=\"Prugovec̆ki\">{{cite book|title=Quantum mechanics in Hilbert space|author=Eduard Prugovec̆ki|url=https://books.google.com/?id=GxmQxn2PF3IC&pg=PA18|chapter=Definition 2.1|pages=18 ''ff''|isbn=0-12-566060-X|year=1981|publisher=Academic Press|edition=2nd}}</ref>\n* [[complex conjugate|Conjugate]] symmetry:\n\n::<math>\\langle u,v\\rangle =\\overline{\\langle v,u\\rangle}.</math>\nNote that in '''R''', it is symmetric.\n* [[Linear]]ity in the first argument:\n\n::<math>\\langle au,v\\rangle= a \\langle u,v\\rangle.</math>\n::<math>\\langle u+v,w\\rangle= \\langle u,w\\rangle+ \\langle v,w\\rangle.</math>\n* [[Definite bilinear form|Positive-definiteness]]:\n\n::<math>\\langle v,v\\rangle \\geq 0</math> with equality only for ''v'' = 0.\n\nWe can define the length of a vector ''v'' in ''V'' by\n:<math>\\|v\\|^2=\\langle v,v\\rangle,</math>\nand we can prove the [[Cauchy–Schwarz inequality]]:\n:<math>|\\langle u,v\\rangle| \\leq \\|u\\| \\cdot \\|v\\|.</math>\n\nIn particular, the quantity\n:<math>\\frac{|\\langle u,v\\rangle|}{\\|u\\| \\cdot \\|v\\|} \\leq 1,</math>\nand so we can call this quantity the cosine of the angle between the two vectors.\n\nTwo vectors are orthogonal if <math>\\langle u, v\\rangle =0</math>. An orthonormal basis is a basis where all basis vectors have length 1 and are orthogonal to each other. Given any finite-dimensional vector space, an orthonormal basis could be found by the [[Gram–Schmidt]] procedure. Orthonormal bases are particularly easy to deal with, since if ''v'' = ''a''<sub>1</sub> ''v''<sub>1</sub> + ... + ''a<sub>n</sub> v<sub>n</sub>'', then <math>a_i = \\langle v,v_i \\rangle</math>.\n\nThe inner product facilitates the construction of many useful concepts. For instance, given a transform ''T'', we can define its [[Hermitian conjugate]] ''T*'' as the linear transform satisfying\n:<math> \\langle T u, v \\rangle = \\langle u, T^* v\\rangle.</math>\nIf ''T'' satisfies ''TT*'' = ''T*T'', we call ''T'' [[Normal matrix|normal]]. It turns out that normal matrices are precisely the matrices that have an orthonormal system of eigenvectors that span ''V''.<!-- This is a potentially useful remark, but a proper context needs to be set for it. One can say quite simply that the [[linear]] problems of [[mathematics]]—those that exhibit [[linearity]] in their behavior—are those most likely to be solved. For example, [[differential calculus]] does a great deal with linear approximation to functions. The difference from [[nonlinearity|nonlinear]] problems is very important in practice.-->\n\n==Relationship with geometry==\nThere is a strong relationship between linear algebra and [[geometry]], which started with the introduction by [[René Descartes]], in 1637, of [[Cartesian coordinates]]. In this new (at that time) geometry, now called [[Cartesian geometry]], points are represented by [[Cartesian coordinates]], which are sequences of three real numbers (in the case of the usual [[three-dimensional space]]). The basic objects of geometry, which are [[line (geometry)|lines]] and [[plane (geometry)|planes]] are represented by linear equations. Thus, computing intersections of lines and planes amounts solving systems of linear equations. This was one of the main motivations for developing linear algebra.\n\nMost [[geometric transformation]], such as [[translation]]s, [[rotation]]s, [[reflection (mathematics)|reflection]]s, [[rigid motion]]s, [[isometries]], and [[projection (mathematics)|projection]]s transform lines into lines. It follows that they can be defined, specified and studied in terms of linear maps. This is also the case of [[homography|homographies]] and [[Möbius transformation]]s, when considered as transformations of a [[projective space]]. \n\nUntil the end of 19th century, geometric spaces were defined by [[axiom]]s relating points, lines and planes ([[synthetic geometry]]). Around this date, it appeared that one may also define geometric spaces by constructions involving vector spaces (see, for example, [[Projective space]] and [[Affine space]]) It has been shown that the two approaches are essentially equivalent.<ref>[[Emil Artin]] (1957) ''[[Geometric Algebra]]'' [[Interscience Publishers]]</ref> In classical geometry, the involved vector spaces are vector spaces over the reals, but the constructions may be extended to vector spaces over any field, allowing considering geometry over arbitrary fields, including [[finite field]]s. \n\nPresently, most textbooks, introduce geometric spaces from linear algebra, and geometry is often presented, at elementary level, as a subfield of linear algebra.\n\n== Usage and applications{{anchor|Applications}} ==\nLinear algebra is used in almost all areas of mathematics, thus making it relevant in almost all scientific domains that use mathematics. These applications may be divided into several wide categories.\n\n=== Geometry of our ambient space ===\nThe [[Mathematical model|modeling]] of our [[ambient space]] is based on [[geometry]]. Sciences concerned with this space use geometry widely. This is the case with [[mechanics]] and [[robotics]], for describing [[rigid body dynamics]]; [[geodesy]] for describing [[Earth shape]]; [[perspectivity]], [[computer vision]], and [[computer graphics]], for describing the relationship between a scene and its plane representation; and many other scientific domains.\n\nIn all these applications, [[synthetic geometry]] is often used for general descriptions and a qualitative approach, but for the study of explicit situations, one must compute with [[coordinates]]. This requires the heavy use of linear algebra.\n\n=== Functional analysis ===\n[[Functional analysis]] studies [[function space]]s. These are vector spaces with additional structure, such as [[Hilbert space]]s. Linear algebra is thus a fundamental part of functional analysis and its applications, which include, in particular, [[quantum mechanics]] ([[wave function]]s).\n\n=== Study of [[complex system]]s === \nMost physical phenomena are modeled by [[partial differential equation]]s. To solve them, one usually decomposes the space in which the solutions are searched into small, mutually interacting [[Discretization|cells]]. For [[linear system]]s this interaction involves [[linear function]]s. For [[nonlinear systems]], this interaction is often approximated by linear functions.<ref>This may have the consequence that some physically interesting solutions are omitted.</ref> In both cases, very large matrices are generally involved. [[Weather forecasting]] is a typical example, where the whole Earth [[atmosphere]] is divided in cells of, say, 100 km of width and 100 m of height.\n\n=== Scientific computation ===\nNearly all [[scientific computation]]s involve linear algebra. Consequently, linear algebra algorithms have been highly optimized. [[Basic Linear Algebra Subprograms|BLAS]] and [[LAPACK]] are the best known implementations. For improving efficiency, some of them configure the algorithms automatically, at run time, for adapting them to the specificities of the computer ([[cache (computing)|cache]] size, number of available [[multi-core processor|cores]], ...).\n\nSome [[Processor (computing)|processor]]s, typically [[graphics processing units]] (GPU), are designed with a matrix structure, for optimizing the operations of linear algebra.\n\n==Extensions and generalizations==\nThis section presents several related topics that do not appear generally in elementary textbooks on linear algebra, but are commonly considered, in advanced mathematics, as parts of linear algebra.\n\n===Module theory===\n{{main|Module (mathematics)}}\n\nThe existence of multiplicative inverses in fields is not involved in the axioms defining a vector space. One may thus replace the field of scalars by a [[ring (mathematics)|ring]] {{mvar|R}}, and this gives a structure called '''module''' over {{mvar|R}}, or {{mvar|R}}-module.\n\nThe concepts of linear independence, span, basis, and linear maps (also called [[module homomorphism]]s) are defined for modules exactly as for vector spaces, with the essential difference that, if {{mvar|R}} is not a field, there are modules that do not have any basis. The modules that have a basis are the [[free module]]s, and those that are spanned by a finite set are the [[finitely generated module]]s. Module homomorphisms between finitely generated free modules may be represented by matrices. The theory of matrices over a ring is similar to that of matrices over a field, except that [[determinant]]s exist only if the ring is [[commutative ring|commutative]], and that a square matrix over a commutative ring is [[invertible matrix|invertible]] only if its determinant has a [[multiplicative inverse]] in the ring.\n\nVector spaces are completely characterized by their dimension (up to an isomorphism). In general, there is not such a complete classification for modules, even if one restricts oneself to finitely generated modules. However, every module is a [[cokernel]] of a homomorphism of free modules.\n\nModules over the integers can be identified with [[abelian group]]s, since the multiplication by an integer may identified to a repeated addition. Most of the theory of abelian groups may be extended to modules over a [[principal ideal domain]]. In particular, over a principal ideal domain, every submodule of a free module is free, and the [[fundamental theorem of finitely generated abelian groups]] may be extended straightforwardly to finitely generated modules over a principal ring.\n\nThere are many rings for which there are algorithms for solving linear equations and systems of linear equations. However, these algorithms have generally a [[computational complexity]] that is much higher than the similar algorithms over a field. For more details, see [[Linear equation over a ring]].\n\n===Multilinear algebra and tensors=== \n{{cleanup|section|reason=The dual space is considered above, and the section must be rewritten for given a understandable summary of this subject|date=September 2018}}\nIn [[multilinear algebra]], one considers multivariable linear transformations, that is, mappings that are linear in each of a number of different variables. This line of inquiry naturally leads to the idea of the [[dual space]], the vector space ''V''<sup>∗</sup> consisting of linear maps {{nowrap|''f'': ''V'' → ''F''}} where ''F'' is the field of scalars. Multilinear maps {{nowrap|''T'': ''V<sup>n</sup>'' → ''F''}} can be described via [[tensor product]]s of elements of ''V''<sup>∗</sup>.\n\nIf, in addition to vector addition and scalar multiplication, there is a bilinear vector product {{nowrap|''V'' × ''V'' → ''V''}}, the vector space is called an [[Algebra over a field|algebra]]; for instance, associative algebras are algebras with an associate vector product (like the algebra of square matrices, or the algebra of polynomials).\n\n===Topological vector spaces===\n{{expand section|date=September 2018}}\n{{main|Topological vector space|Normed vector space|Hilbert space}}\nVector spaces that are not finite dimensional often require additional structure to be tractable. A [[normed vector space]] is a vector space along with a function called a [[Norm (mathematics)|norm]], which measures the \"size\" of elements. The norm induces a [[Metric (mathematics)|metric]], which measures the distance between elements, and induces a [[Topological space|topology]], which allows for a definition of continuous maps. The metric also allows for a definition of [[Limit (mathematics)|limits]] and [[Complete metric space|completeness]] - a metric space that is complete is known as a [[Banach space]]. A complete metric space along with the additional structure of an [[Inner product space|inner product]] (a conjugate symmetric [[sesquilinear form]]) is known as a [[Hilbert space]], which is in some sense a particularly well-behaved Banach space. [[Functional analysis]] applies the methods of linear algebra alongside those of [[mathematical analysis]] to study various function spaces; the central objects of study in functional analysis are [[Lp space|L<sup>''p''</sup> space]]s, which are Banach spaces, and especially the ''L<sup>2</sup>'' space of square integrable functions, which is the only Hilbert space among them. Functional analysis is of particular importance to quantum mechanics, the theory of partial differential equations, digital signal processing, and electrical engineering. It also provides the foundation and theoretical framework that underlies the Fourier transform and related methods.\n\n===Homological algebra===\n{{expand section|date=September 2018}}\n{{main|Homological algebra}}\n\n==See also==\n* {{Portal inline|size=tiny|Linear algebra}}\n* [[Linear equation over a ring]]\n* [[Fundamental matrix (computer vision)|Fundamental matrix]] in [[computer vision]]\n* [[Linear regression]], a statistical estimation method\n* [[List of linear algebra topics]]\n* [[Numerical linear algebra]]\n* [[Linear programming]]\n* [[Transformation matrix]]\n\n==Notes==\n{{reflist|30em}}\n\n{{reflist|group=nb}}\n\n==Further reading==\n\n===History===\n* Fearnley-Sander, Desmond, \"[https://www.jstor.org/stable/pdf/2320145.pdf?casa_token=OivQGvKhCREAAAAA:1DKkTImlBZdKc7f-9T5tb5gjNx_RBbC20OcrA_jM3_6ksW6Js7bs6qhvcCBjzBg3kD3Zq0Gr0mLe3w9HQiQIbLgv73HBTkYIj2HNlE_HyOh5fUJb7Nsy3A Hermann Grassmann and the Creation of Linear Algebra]\", American Mathematical Monthly '''86''' (1979), pp.&nbsp;809–817.\n* {{Citation|last=Grassmann|first= Hermann|authorlink=Hermann Grassmann| title=Die lineale Ausdehnungslehre ein neuer Zweig der Mathematik: dargestellt und durch Anwendungen auf die übrigen Zweige der Mathematik, wie auch auf die Statik, Mechanik, die Lehre vom Magnetismus und die Krystallonomie erläutert|publisher= O. Wigand|location= Leipzig|year= 1844}}\n\n===Introductory textbooks===\n* {{Citation|last=Anton|first=Howard|year=2005|title=Elementary Linear Algebra (Applications Version)|publisher=Wiley International|edition=9th}}\n* {{Citation | last = Banerjee | first = Sudipto | last2 = Roy | first2 = Anindya | date = 2014 | title = Linear Algebra and Matrix Analysis for Statistics | series = Texts in Statistical Science | publisher = Chapman and Hall/CRC | edition =  1st | isbn =  978-1420095388}}\n* {{Citation|last=Bretscher|first=Otto|year=2004|title=Linear Algebra with Applications|publisher=Prentice Hall|edition=3rd|isbn=978-0-13-145334-0}}\n* {{Citation|last=Farin|first=Gerald|last2=Hansford|first2=Dianne|\nyear=2004|title=Practical Linear Algebra: A Geometry Toolbox|publisher=AK Peters|isbn=978-1-56881-234-2}}\n* {{Citation|last=Hefferon|first=Jim|year=2008|title=Linear Algebra|url=http://joshua.smcvt.edu/linearalgebra/}}\n* {{Citation|last=Kolman|first=Bernard|last2=Hill|first2=David R.|year=2007|title=Elementary Linear Algebra with Applications|publisher=Prentice Hall|edition=9th|isbn=978-0-13-229654-0}}\n* {{Citation|last=Lay|first=David C.|year=2005|title=Linear Algebra and Its Applications|publisher=Addison Wesley|edition=3rd|isbn=978-0-321-28713-7}}\n* {{Citation|last=Leon|first=Steven J.|year=2006|title=Linear Algebra With Applications|publisher=Pearson Prentice Hall|edition=7th|isbn=978-0-13-185785-8}}\n* Murty, Katta G. (2014) ''[http://www.worldscientific.com/worldscibooks/10.1142/8261 Computational and Algorithmic Linear Algebra and n-Dimensional Geometry]'', World Scientific Publishing, {{isbn|978-981-4366-62-5}}. ''[http://www.worldscientific.com/doi/suppl/10.1142/8261/suppl_file/8261_chap01.pdf Chapter 1: Systems of Simultaneous Linear Equations]''\n* {{Citation|last=Poole|first=David|year=2010|title=Linear Algebra: A Modern Introduction|publisher=Cengage&nbsp;– Brooks/Cole|edition=3rd|isbn=978-0-538-73545-2}}\n* {{Citation|last=Ricardo|first=Henry|year=2010|title=A Modern Introduction To Linear Algebra|publisher=CRC Press|edition=1st|isbn=978-1-4398-0040-9}}\n* {{Citation|last=Sadun|first=Lorenzo|year=2008|title=Applied Linear Algebra: the decoupling principle|publisher=AMS|edition=2nd|isbn=978-0-8218-4441-0}}\n* {{Citation|last=Strang|first=Gilbert|authorlink=Gilbert Strang|year=2016|title=Introduction to Linear Algebra|publisher=Wellesley-Cambridge Press|edition=5th|isbn=978-09802327-7-6}}\n* The Manga Guide to Linear Algebra (2012), by [[Shin Takahashi]], Iroha Inoue and Trend-Pro Co., Ltd., {{isbn| 978-1-59327-413-9}}\n\n===Advanced textbooks===\n* {{Citation|last=Axler|first=Sheldon|authorlink=Sheldon Axler|date=February 26, 2004|title=Linear Algebra Done Right|publisher=Springer|edition=2nd|isbn=978-0-387-98258-8}}\n* {{Citation|last=Bhatia|first=Rajendra|date=November 15, 1996|title=Matrix Analysis|series=[[Graduate Texts in Mathematics]]|publisher=Springer|isbn=978-0-387-94846-1}}\n* {{Citation|last=Demmel|first=James W.|authorlink=James Demmel|date=August 1, 1997|title=Applied Numerical Linear Algebra|publisher=SIAM|isbn=978-0-89871-389-3}}\n* {{Citation|last=Dym|first=Harry|year=2007|title=Linear Algebra in Action|publisher=AMS|isbn=978-0-8218-3813-6}}\n* {{Citation|last=Gantmacher|first=Felix R.|authorlink = Felix Gantmacher|date=2005|title=Applications of the Theory of Matrices|publisher=Dover Publications|isbn=978-0-486-44554-0}}\n* {{Citation|last=Gantmacher|first=Felix R.|year=1990|title=Matrix Theory Vol. 1|publisher=American Mathematical Society|edition=2nd|isbn=978-0-8218-1376-8}}\n* {{Citation|last=Gantmacher|first=Felix R.|year=2000|title=Matrix Theory Vol. 2|publisher=American Mathematical Society|edition=2nd|isbn=978-0-8218-2664-5}}\n* {{Citation|last=Gelfand|first=Israel M.|authorlink = Israel Gelfand|year=1989|title=Lectures on Linear Algebra|publisher=Dover Publications|isbn=978-0-486-66082-0}}\n* {{Citation|last=Glazman|first=I. M.|last2=Ljubic|first2=Ju. I.|year=2006|title=Finite-Dimensional Linear Analysis|publisher=Dover Publications|isbn= 978-0-486-45332-3}}\n* {{Citation|last=Golan|first=Johnathan S.|date=January 2007|title=The Linear Algebra a Beginning Graduate Student Ought to Know|publisher=Springer|edition=2nd|isbn=978-1-4020-5494-5}}\n* {{Citation|last=Golan|first=Johnathan S.|date=August 1995|title=Foundations of Linear Algebra|publisher=Kluwer |isbn=0-7923-3614-3}}\n* {{Citation|last=Golub|first=Gene H.|last2=Van Loan|first2=Charles F.|date=October 15, 1996|title=Matrix Computations|series=Johns Hopkins Studies in Mathematical Sciences|publisher=The Johns Hopkins University Press|edition=3rd|isbn=978-0-8018-5414-9}}\n* {{Citation|last=Greub|first=Werner H.|date=October 16, 1981|title=Linear Algebra|series=Graduate Texts in Mathematics|publisher=Springer|edition=4th|isbn=978-0-8018-5414-9}}\n* {{citation\n | last1 = Hoffman | first1 = Kenneth\n | last2 = Kunze | first2 = Ray | author2-link = Ray Kunze\n | edition = 2nd\n | location = Englewood Cliffs, N.J.\n | mr = 0276251\n | publisher = Prentice-Hall, Inc.\n | title = Linear algebra\n | year = 1971}}\n* {{Citation|last=Halmos|first=Paul R.|authorlink = Paul Halmos|date=August 20, 1993|title=Finite-Dimensional Vector Spaces|series=[[Undergraduate Texts in Mathematics]]|publisher=Springer|isbn=978-0-387-90093-3}}\n* {{Citation|last=Friedberg|first=Stephen H.|last2=Insel|first2=Arnold J.|last3=Spence|first3=Lawrence E.|date=September 7, 2018|volume=|pages=|title=Linear Algebra|publisher=Pearson|edition=5th|isbn=978-0-13-486024-4}}\n* {{Citation|last=Horn|first=Roger A.|last2=Johnson|first2=Charles R.|date=February 23, 1990|title=Matrix Analysis|publisher=Cambridge University Press|isbn=978-0-521-38632-6}}\n* {{Citation|last1=Horn|first1=Roger A.|last2=Johnson|first2=Charles R.|date=June 24, 1994|title=Topics in Matrix Analysis|publisher=Cambridge University Press|isbn=978-0-521-46713-1}}\n* {{Citation|last=Lang|first=Serge|date=March 9, 2004|title=Linear Algebra|series=Undergraduate Texts in Mathematics|edition=3rd|publisher=Springer|isbn=978-0-387-96412-6}}\n* {{Citation|last1=Marcus|first1=Marvin|last2=Minc|first2=Henryk|year=2010|title=A Survey of Matrix Theory and Matrix Inequalities|publisher=Dover Publications|isbn=978-0-486-67102-4}}\n* {{Citation|last=Meyer |first=Carl D. |date=February 15, 2001 |title=Matrix Analysis and Applied Linear Algebra |publisher=Society for Industrial and Applied Mathematics (SIAM) |isbn=978-0-89871-454-8 |url=http://www.matrixanalysis.com/DownloadChapters.html |deadurl=yes |archiveurl=https://web.archive.org/web/20091031193126/http://matrixanalysis.com/DownloadChapters.html |archivedate=October 31, 2009 |df= }}\n* {{Citation|last1=Mirsky|first1=L.|authorlink=Leon Mirsky|year=1990|title=An Introduction to Linear Algebra|publisher= Dover Publications|isbn=978-0-486-66434-7}}\n* {{Citation|last=Roman|first=Steven|date=March 22, 2005|title=Advanced Linear Algebra|edition=2nd|series=Graduate Texts in Mathematics|publisher=Springer|isbn=978-0-387-24766-3}}\n* {{Citation|last1=Shafarevich|first1 = I. R.|authorlink1 = Igor Shafarevich|first2 = A. O|last2=Remizov|title = Linear Algebra and Geometry|publisher = [[Springer Science+Business Media|Springer]]|year=2012|url = https://www.springer.com/mathematics/algebra/book/978-3-642-30993-9 |isbn = 978-3-642-30993-9}}\n* {{Citation|last=Shilov|first=Georgi E.|authorlink = Georgiy Shilov|date=June 1, 1977|publisher=Dover Publications|isbn=978-0-486-63518-7|title=Linear algebra}}\n* {{Citation|last=Shores|first=Thomas S.|date=December 6, 2006|title=Applied Linear Algebra and Matrix Analysis|series=Undergraduate Texts in Mathematics|publisher=Springer|isbn=978-0-387-33194-2}}\n* {{Citation|last=Smith|first=Larry|date=May 28, 1998|title=Linear Algebra|series=Undergraduate Texts in Mathematics|publisher=Springer|isbn=978-0-387-98455-1}}\n* {{Citation|last=Trefethen|first=Lloyd N.|last2=Bau|first2=David|date=1997|title=Numerical Linear Algebra|publisher=SIAM|isbn=978-0-898-71361-9}}\n\n===Study guides and outlines===\n* {{Citation|last=Leduc|first=Steven A.|date=May 1, 1996|title=Linear Algebra (Cliffs Quick Review)|publisher=Cliffs Notes|isbn=978-0-8220-5331-6}}\n* {{Citation|last=Lipschutz|first=Seymour|last2=Lipson|first2=Marc|date=December 6, 2000|title=Schaum's Outline of Linear Algebra|publisher=McGraw-Hill|edition=3rd|isbn=978-0-07-136200-9}}\n* {{Citation|last=Lipschutz|first=Seymour|date=January 1, 1989|title=3,000 Solved Problems in Linear Algebra|publisher=McGraw–Hill|isbn=978-0-07-038023-3}}\n* {{Citation|last=McMahon|first=David|date=October 28, 2005|title=Linear Algebra Demystified|publisher=McGraw–Hill Professional|isbn=978-0-07-146579-3}}\n* {{Citation|last=Zhang|first=Fuzhen|date=April 7, 2009|title=Linear Algebra: Challenging Problems for Students|publisher=The Johns Hopkins University Press|isbn=978-0-8018-9125-0}}\n\n==External links==\n{{Wikibooks|Linear Algebra}}\n===Online Resources===\n{{Commonscat}}\n* [https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/ MIT Linear Algebra Video Lectures], a series of 34 recorded lectures by professor Gilbert Strang (Spring 2010)\n* [http://www.math.technion.ac.il/iic/ International Linear Algebra Society]\n* {{springer|title=Linear algebra|id=p/l059040}}\n* [http://mathworld.wolfram.com/topics/LinearAlgebra.html Linear Algebra] on [[MathWorld]].\n* [http://www.economics.soton.ac.uk/staff/aldrich/matrices.htm Matrix and Linear Algebra Terms] on [http://jeff560.tripod.com/mathword.html Earliest Known Uses of Some of the Words of Mathematics]\n* [http://jeff560.tripod.com/matrices.html Earliest Uses of Symbols for Matrices and Vectors] on [http://jeff560.tripod.com/mathsym.html Earliest Uses of Various Mathematical Symbols]\n* [https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab Essence of linear algebra], a video presentation of the basics of linear algebra, with emphasis on the relationship between the geometric, the matrix and the abstract points of view\n\n===Online books===\n* Beezer, Rob, ''[http://linear.ups.edu/index.html A First Course in Linear Algebra]''\n* Connell, Edwin H., ''[http://www.math.miami.edu/~ec/book/ Elements of Abstract and Linear Algebra]''\n* Hefferon, Jim, ''[http://joshua.smcvt.edu/linearalgebra/ Linear Algebra]''\n* Matthews, Keith, ''[http://www.numbertheory.org/book/ Elementary Linear Algebra]''\n* Sharipov, Ruslan, ''[https://arxiv.org/abs/math.HO/0405323 Course of linear algebra and multidimensional geometry]''\n* Treil, Sergei, ''[http://www.math.brown.edu/~treil/papers/LADW/LADW.html Linear Algebra Done Wrong]''\n\n{{Linear algebra}}\n{{Areas of mathematics}}\n{{Authority control}}\n\n{{DEFAULTSORT:Linear Algebra}}\n[[Category:Linear algebra| ]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Linear approximation",
      "url": "https://en.wikipedia.org/wiki/Linear_approximation",
      "text": "[[File:TangentGraphic2.svg|thumb|300px|Tangent line at (''a'', ''f''(''a''))]]\nIn [[mathematics]], a '''linear approximation''' is an approximation of a general [[function (mathematics)|function]] using a [[linear function]] (more precisely, an [[affine function]]). They are widely used in the method of [[finite differences]] to produce first order methods for solving or approximating solutions to equations.\n\n==Definition==\nGiven a twice continuously differentiable function <math>f</math> of one [[real number|real]] variable, [[Taylor's theorem]] for the case <math>n = 1 </math> states that\n\n:<math> f(x) = f(a) + f'(a)(x - a) + R_2\\ </math>\n\nwhere <math>R_2</math> is the remainder term. The linear approximation is obtained by dropping the remainder:\n\n:<math> f(x) \\approx f(a) + f'(a)(x - a)</math>.\n\nThis is a good approximation for <math>x</math> when it is close enough to <math>a</math>; since a curve, when closely observed, will begin to resemble a straight line. Therefore, the expression on the right-hand side is just the equation for the [[tangent line]] to the graph of <math>f</math> at <math>(a,f(a))</math>. For this reason, this process is also called the '''tangent line approximation'''.\n\nIf <math>f</math> is [[concave down]] in the interval between <math>x</math> and <math>a</math>, the approximation will be an overestimate (since the derivative is decreasing in that interval). If <math>f</math> is [[concave up]], the approximation will be an underestimate.<ref>{{cite web|title=12.1 Estimating a Function Value Using the Linear Approximation|url=http://math.mit.edu/classes/18.013A/HTML/chapter12/section01.html|accessdate=3 June 2012}}</ref>\n\nLinear approximations for [[vector (geometric)|vector]] functions of a vector variable are obtained in the same way, with the derivative at a point replaced by the [[Jacobian matrix and determinant|Jacobian]] matrix. For example, given a differentiable function <math>f(x, y)</math> with real values, one can approximate <math>f(x, y)</math> for <math>(x, y)</math> close to <math>(a, b)</math> by the formula\n:<math>f\\left(x,y\\right)\\approx f\\left(a,b\\right)+\\frac{\\partial f}{\\partial x}\\left(a,b\\right)\\left(x-a\\right)+\\frac{\\partial f}{\\partial y}\\left(a,b\\right)\\left(y-b\\right).</math>\n\nThe right-hand side is the equation of the plane tangent to the graph of <math>z=f(x, y)</math> at <math>(a, b).</math>\n\nIn the more general case of [[Banach space]]s, one has\n\n:<math> f(x) \\approx f(a) + Df(a)(x - a)</math>\n\nwhere <math>Df(a)</math> is the [[Fréchet derivative]] of <math>f</math> at <math>a</math>.\n\n==Applications==\n\n===Optics===\n{{main|Gaussian optics}}\n'''Gaussian optics''' is a technique in [[geometrical optics]] that describes the behaviour of light rays in optical systems by using the [[paraxial approximation]], in which only rays which make small angles with the [[optical axis]] of the system are considered.<ref>A.Lipson, S.G.Lipson, H.Lipson, [https://books.google.com/books?id=aow3o0dhyjYC&pg=PA51#v=onepage&q&f=false Optical Physics], IV edition, 2010, University Press, Cambridge, UK, p.51</ref> In this approximation, trigonometric functions can be expressed as linear functions of the angles. Gaussian optics applies to systems in which all the optical surfaces are either flat or are portions of a [[sphere]]. In this case, simple explicit formulae can be given for parameters of an imaging system such as focal distance, magnification and brightness, in terms of the geometrical shapes and material properties of the constituent elements.\n\n===Period of oscillation===\n{{main|Pendulum}}\nThe period of swing of a [[Pendulum (mathematics)#Simple gravity pendulum|simple gravity pendulum]] depends on its [[length]], the local [[Gravitational acceleration|strength of gravity]], and to a small extent on the maximum [[angle]] that the pendulum swings away from vertical, ''θ<sub>0</sub>'', called the [[amplitude]].<ref name=\"Milham1945\">{{cite book\n|last=Milham\n|first=Willis I.\n|title=Time and Timekeepers\n|year=1945\n|publisher=MacMillan}}, p.188-194</ref> It is independent of the [[mass]] of the bob. The true period ''T'' of a simple pendulum, the time taken for a complete cycle of an ideal simple gravity pendulum, can be written in several different forms (see [[Pendulum (mathematics)]] ), one example being the [[infinite series]]:<ref name=\"Nelson\">{{cite journal\n  | last = Nelson\n  | first = Robert\n  | authorlink =\n  |author2=M. G. Olsson\n  | title = The pendulum – Rich physics from a simple system\n  | journal = American Journal of Physics\n  | volume = 54\n  | issue = 2\n  | pages = 112–121\n  | publisher =\n  | location =\n  | date = February 1987\n  | url = http://fy.chalmers.se/~f7xiz/TIF080/pendulum.pdf\n  | doi = 10.1119/1.14703\n  | id =\n  | accessdate = 2008-10-29|bibcode = 1986AmJPh..54..112N }}</ref><ref>{{cite encyclopedia\n  | title = Clock\n  | encyclopedia = Encyclopædia Britannica, 11th Ed.\n  | volume = 6\n  | pages = 538\n  | publisher = The Encyclopædia Britannica Publishing Co.\n  | year = 1910\n  | id =\n  | url= https://books.google.com/books?id=cLsUAAAAYAAJ&pg=PA538\n  | accessdate = 2009-03-04}} includes a derivation</ref>\n\n:<math>\nT = 2\\pi \\sqrt{L\\over g} \\left( 1+ \\frac{1}{16}\\theta_0^2 + \\frac{11}{3072}\\theta_0^4 + \\cdots \\right)\n</math>\n\nwhere '''''L''''' is the length of the pendulum and '''''g''''' is the local [[Gravitational acceleration|acceleration of gravity]].\n\nHowever, if one takes the linear approximation (i.e. if the amplitude is limited to small swings,<ref group = Note>A \"small\" swing is one in which the angle θ is small enough that sin(θ) can be approximated by θ when θ is measured in radians</ref> ) the [[Frequency|period]] is:<ref>{{cite book\n  | last = Halliday\n  | first = David\n  | authorlink =\n  |author2=Robert Resnick |author3=Jearl Walker\n | title = Fundamentals of Physics, 5th Ed.\n  | publisher = John Wiley & Sons.\n  | year = 1997\n  | location = New York\n  | page = 381\n  | url =\n  | doi =\n  | id =\n  | isbn = 0-471-14854-7}}</ref>\n\n:<math>T \\approx 2\\pi \\sqrt\\frac{L}{g} \\qquad \\qquad \\qquad \\theta_0 \\ll 1  \\qquad (1)\\,</math>\n\nIn the linear approximation, the period of swing is approximately the same for different size swings: that is, ''the period is independent of amplitude''. This property, called [[isochronism]], is the reason pendulums are so useful for timekeeping.<ref>{{cite book\n  | last = Cooper\n  | first = Herbert J.\n  | title = Scientific Instruments\n  | publisher = Hutchinson's\n  | year = 2007\n  | location = New York\n  | page = 162\n  | url = https://www.google.com/books?id=t7OoPLzXwiQC&pg=PA162\n  | doi =\n  | id =\n  | isbn = 1-4067-6879-0}}</ref> Successive swings of the pendulum, even if changing in amplitude, take the same amount of time.\n\n===Electrical resistivity===\n{{main|Electrical resistivity}}\nThe electrical resistivity of most materials changes with temperature. If the temperature ''T'' does not vary too much, a linear approximation is typically used:\n:<math>\\rho(T) = \\rho_0[1+\\alpha (T - T_0)]</math>\nwhere <math>\\alpha</math> is called the ''temperature coefficient of resistivity'', <math>T_0</math> is a fixed reference temperature (usually room temperature), and <math>\\rho_0</math> is the resistivity at temperature <math>T_0</math>. The parameter <math>\\alpha</math> is an empirical parameter fitted from measurement data. Because the linear approximation is only an approximation, <math>\\alpha</math> is different for different reference temperatures. For this reason it is usual to specify the temperature that <math>\\alpha</math> was measured at with a suffix, such as <math>\\alpha_{15}</math>, and the relationship only holds in a range of temperatures around the reference.<ref>M.R. Ward (1971) ''Electrical Engineering Science'', pp. 36–40, McGraw-Hill.</ref> When the temperature varies over a large temperature range, the linear approximation is inadequate and a more detailed analysis and understanding should be used.\n\n==See also==\n* [[Binomial approximation]]\n* [[Euler's method]]\n* [[Finite differences]]\n* [[Finite difference methods]]\n* [[Newton's method]]\n* [[Power series]]\n* [[Taylor series]]\n\n==Notes==\n{{reflist|group=Note}}\n\n==References==\n{{reflist}}\n\n==Further reading==\n*{{cite book |author1=Weinstein, Alan |author2=Marsden, Jerrold E. |title=Calculus III |publisher=Springer-Verlag |location=Berlin |year=1984|isbn=0-387-90985-0 |oclc= |doi= |page= 775}}\n* {{cite book |author=Strang, Gilbert |title=Calculus |publisher=Wellesley College |location= |year=1991|isbn=0-9614088-2-0 |oclc= |doi= |page= 94}}\n*{{cite book |author1=Bock, David |author2=Hockett, Shirley O. |title=How to Prepare for the AP Calculus|publisher=Barrons Educational Series |location=Hauppauge, NY |year=2005 |isbn=0-7641-2382-3 |oclc= |doi= |page=118}}\n\n[[Category:Differential calculus]]\n[[Category:Numerical analysis]]\n[[Category:First order methods]]"
    },
    {
      "title": "Linear multistep method",
      "url": "https://en.wikipedia.org/wiki/Linear_multistep_method",
      "text": "{{redirect|Adams' method|the electoral apportionment method|Method of smallest divisors}}\n{{no footnotes|date=June 2017}}\n'''Linear multistep methods''' are used for the [[numerical ordinary differential equations|numerical solution of ordinary differential equations]]. Conceptually, a numerical method starts from an initial point and then takes a short '''step''' forward in time to find the next solution point. The process continues with subsequent steps to map out the solution. Single-step methods (such as [[Euler's method]]) refer to only one previous point and its derivative to determine the current value. Methods such as [[Runge–Kutta methods|Runge–Kutta]] take some intermediate steps (for example, a half-step) to obtain a higher order method, but then discard all previous information before taking a second step. Multistep methods attempt to gain efficiency by keeping and using the information from previous steps rather than discarding it. Consequently, multistep methods refer to several previous points and derivative values. In the case of ''linear'' multistep methods, a [[linear combination]] of the previous points and derivative values is used.\n\n== Definitions ==\nNumerical methods for ordinary differential equations approximate solutions to [[initial value problem]]s of the form\n: <math> y' = f(t,y), \\quad y(t_0) = y_0. </math>\n\nThe result is approximations for the value of <math> y(t) </math> at discrete times <math> t_i </math>:\n: <math> y_i \\approx y(t_i) \\quad\\text{where}\\quad t_i = t_0 + i h, </math>\nwhere <math> h </math> is the time step (sometimes referred to as <math> \\Delta t </math>) and <math>i</math> is an integer.\n\nMultistep methods use information from the previous <math> s </math> steps to calculate the next value. In particular, a ''linear'' multistep method uses a linear combination of <math> y_i </math> and <math> f(t_i,y_i) </math> to calculate the value of <math> y </math> for the desired current step. Thus, a linear multistep method is a method of the form\n: <math> \\begin{align}\n& y_{n+s} + a_{s-1} \\cdot y_{n+s-1} + a_{s-2} \\cdot y_{n+s-2} + \\cdots + a_0 \\cdot y_n \\\\\n& \\qquad {} = h\\cdot\\left( b_s \\cdot f(t_{n+s},y_{n+s}) + b_{s-1} \\cdot f(t_{n+s-1},y_{n+s-1}) + \\cdots + b_0 \\cdot f(t_n,y_n) \\right) \\\\\n& \\Leftrightarrow \\sum_{j=0}^s a_jy_{n+j} = h\\sum_{j=0}^sb_jf(t_{n+j},y_{n+j}),\n\\end{align} </math>\nwith <math>a_s=1</math>. The coefficients <math> a_0, \\dotsc, a_{s-1} </math> and <math> b_0, \\dotsc, b_s </math> determine the method. The designer of the method chooses the coefficients, balancing the need to get a good approximation to the true solution against the desire to get a method that is easy to apply. Often, many coefficients are zero to simplify the method.\n\nOne can distinguish between [[explicit and implicit methods]]. If <math> b_s = 0 </math>, then the method is called \"explicit\", since the formula can directly compute <math> y_{n+s} </math>. If <math> b_s \\ne 0 </math> then the method is called \"implicit\", since the value of <math> y_{n+s} </math> depends on the value of <math> f(t_{n+s}, y_{n+s}) </math>, and the equation must be solved for <math> y_{n+s} </math>. [[Iterative methods]] such as [[Newton's method]] are often used to solve the implicit formula.\n\nSometimes an explicit multistep method is used to \"predict\" the value of <math> y_{n+s} </math>. That value is then used in an implicit formula to \"correct\" the value. The result is a [[predictor–corrector method]].\n\n== Examples ==\n\nConsider for an example the problem\n: <math> y' = f(t,y)=y, \\quad y(0) = 1. </math>\nThe exact solution is <math> y(t) = \\mathrm{e}^t </math>.\n\n=== One-step Euler ===\nA simple numerical method is Euler's method:\n: <math> y_{n+1} = y_n + hf(t_n, y_n). \\, </math>\nEuler's method can be viewed as an explicit multistep method for the degenerate case of one step.\n\nThis method, applied with step size <math> h = \\tfrac{1}{2} </math> on the problem <math> y' = y </math>, gives the following results:\n: <math> \\begin{align}\n  y_1 &= y_0 + hf(t_0, y_0) = 1 + \\tfrac{1}{2}\\cdot1 = 1.5, \\\\\n  y_2 &= y_1 + hf(t_1, y_1) = 1.5 + \\tfrac{1}{2}\\cdot1.5 = 2.25, \\\\\n  y_3 &= y_2 + hf(t_2, y_2) = 2.25 + \\tfrac{1}{2}\\cdot2.25 = 3.375, \\\\\n  y_4 &= y_3 + hf(t_3, y_3) = 3.375 + \\tfrac{1}{2}\\cdot3.375 = 5.0625.\n\\end{align} </math>\n\n===Two-step Adams–Bashforth===\nEuler's method is a one-step method. A simple multistep method is the two-step Adams–Bashforth method\n: <math> y_{n+2} = y_{n+1} + \\tfrac{3}{2} hf(t_{n+1},y_{n+1}) - \\tfrac{1}{2} hf(t_n,y_n). </math>\nThis method needs two values, <math> y_{n+1} </math> and <math> y_n </math>, to compute the next value, <math> y_{n+2} </math>. However, the initial value problem provides only one value, <math> y_0 = 1 </math>. One possibility to resolve this issue is to use the <math> y_1 </math> computed by Euler's method as the second value. With this choice, the Adams–Bashforth method yields (rounded to four digits):\n: <math> \\begin{align}\n  y_2 &= y_1 + \\tfrac32 hf(t_1, y_1) - \\tfrac12 hf(t_0, y_0) = 1.5 + \\tfrac32\\cdot\\tfrac12\\cdot1.5 - \\tfrac12\\cdot\\tfrac12\\cdot1 = 2.375, \\\\\n  y_3 &= y_2 + \\tfrac32 hf(t_2, y_2) - \\tfrac12 hf(t_1, y_1) = 2.375 + \\tfrac32\\cdot\\tfrac12\\cdot2.375 - \\tfrac12\\cdot\\tfrac12\\cdot1.5 = 3.7812, \\\\\n  y_4 &= y_3 + \\tfrac32 hf(t_3, y_3) - \\tfrac12 hf(t_2, y_2) = 3.7812 + \\tfrac32\\cdot\\tfrac12\\cdot3.7812 - \\tfrac12\\cdot\\tfrac12\\cdot2.375 = 6.0234.\n\\end{align} </math>\nThe exact solution at <math> t = t_4 = 2 </math> is <math> \\mathrm{e}^2 = 7.3891\\ldots </math>, so the two-step Adams–Bashforth method is more accurate than Euler's method. This is always the case if the step size is small enough.\n\n== Families of multistep methods ==\nThree families of linear multistep methods are commonly used: Adams–Bashforth methods, Adams–Moulton methods, and the [[backward differentiation formula]]s (BDFs).\n\n=== Adams–Bashforth methods ===\nThe Adams–Bashforth methods are explicit methods. The coefficients are <math> a_{s-1}=-1 </math> and <math> a_{s-2} = \\cdots = a_0 = 0 </math>, while the <math> b_j </math> are chosen such that the methods has order ''s'' (this determines the methods uniquely).\n\nThe Adams–Bashforth methods with ''s'' = 1, 2, 3, 4, 5 are ({{harvnb|Hairer|Nørsett|Wanner|1993|loc=§III.1}}; {{harvnb|Butcher|2003|p=103}}):\n: <math> \\begin{align}\ny_{n+1} &= y_n + hf(t_n, y_n) , \\qquad\\text{(This is the Euler method)} \\\\\ny_{n+2} &= y_{n+1} + h\\left( \\frac{3}{2}f(t_{n+1}, y_{n+1}) - \\frac{1}{2}f(t_n, y_n) \\right) , \\\\\ny_{n+3} &= y_{n+2} + h\\left( \\frac{23}{12} f(t_{n+2}, y_{n+2}) - \\frac{16}{12} f(t_{n+1}, y_{n+1}) + \\frac{5}{12}f(t_n, y_n)\\right) , \\\\\ny_{n+4} &= y_{n+3} + h\\left( \\frac{55}{24} f(t_{n+3}, y_{n+3}) - \\frac{59}{24} f(t_{n+2}, y_{n+2}) + \\frac{37}{24} f(t_{n+1}, y_{n+1}) - \\frac{9}{24} f(t_n, y_n) \\right) , \\\\\ny_{n+5} &= y_{n+4} + h\\left( \\frac{1901}{720} f(t_{n+4}, y_{n+4}) - \\frac{2774}{720} f(t_{n+3}, y_{n+3}) + \\frac{2616}{720} f(t_{n+2}, y_{n+2}) - \\frac{1274}{720} f(t_{n+1}, y_{n+1}) + \\frac{251}{720} f(t_n, y_n) \\right) .\n\\end{align} </math>\n\nThe coefficients <math> b_j </math> can be determined as follows. Use [[polynomial interpolation]] to find the polynomial ''p'' of degree <math> s-1 </math> such that\n:<math> p(t_{n+i}) = f(t_{n+i}, y_{n+i}), \\qquad \\text{for } i=0,\\ldots,s-1. </math>\nThe [[Lagrange polynomial|Lagrange formula]] for polynomial interpolation yields\n:<math> p(t) = \\sum_{j=0}^{s-1} \\frac{(-1)^{s-j-1}f(t_{n+j}, y_{n+j})}{j!(s-j-1)!h^{s-1}} \\prod_{i=0 \\atop i\\ne j}^{s-1} (t-t_{n+i}). </math>\nThe polynomial ''p'' is locally a good approximation of the right-hand side of the differential equation <math> y' = f(t,y) </math> that is to be solved, so consider the equation <math> y' = p(t) </math> instead. This equation can be solved exactly; the solution is simply the integral of ''p''. This suggests taking\n:<math> y_{n+s} = y_{n+s-1} + \\int_{t_{n+s-1}}^{t_{n+s}} p(t)\\,dt. </math>\nThe Adams–Bashforth method arises when the formula for ''p'' is substituted. The coefficients <math> b_j </math> turn out to be given by\n:<math> b_{s-j-1} = \\frac{(-1)^j}{j!(s-j-1)!} \\int_0^1 \\prod_{i=0 \\atop i\\ne j}^{s-1} (u+i) \\,du, \\qquad \\text{for } j=0,\\ldots,s-1. </math>\nReplacing <math> f(t, y) </math> by its interpolant ''p'' incurs an error of order ''h''<sup>''s''</sup>, and it follows that the ''s''-step Adams–Bashforth method has indeed order ''s'' {{harv|Iserles|1996|loc=§2.1}}\n\nThe Adams–Bashforth methods were designed by [[John Couch Adams]] to solve a differential equation modelling [[capillary action]] due to [[Francis Bashforth]]. {{harvtxt|Bashforth|1883}} published his theory and Adams' numerical method {{harv|Goldstine|1977}}.\n\n=== Adams–Moulton methods ===\nThe Adams–Moulton methods are similar to the Adams–Bashforth methods in that they also have <math> a_{s-1} = -1 </math> and <math> a_{s-2} = \\cdots = a_0 = 0 </math>. Again the ''b'' coefficients are chosen to obtain the highest order possible. However, the Adams–Moulton methods are implicit methods. By removing the restriction that <math> b_s = 0 </math>, an ''s''-step Adams–Moulton method can reach order <math> s+1 </math>, while an ''s''-step Adams–Bashforth methods has only order ''s''.\n\nThe Adams–Moulton methods with ''s'' = 0, 1, 2, 3, 4 are ({{harvnb|Hairer|Nørsett|Wanner|1993|loc=§III.1}}; {{harvnb|Quarteroni|Sacco|Saleri|2000}}):\n:<math> y_n = y_{n-1} + h f(t_n,y_n), </math> This is the [[Backward Euler method|backward Euler method]]\n:<math> y_{n+1} = y_n + \\frac{1}{2} h \\left( f(t_{n+1},y_{n+1}) + f(t_n,y_n) \\right), </math> This is the [[Trapezoidal rule (differential equations)|trapezoidal rule]]\n\n: <math> \\begin{align}\ny_{n+2} &= y_{n+1} + h \\left( \\frac{5}{12} f(t_{n+2},y_{n+2}) + \\frac{2}{3} f(t_{n+1},y_{n+1}) - \\frac{1}{12} f(t_n,y_n) \\right) , \\\\\ny_{n+3} &= y_{n+2} + h \\left( \\frac{9}{24} f(t_{n+3},y_{n+3}) + \\frac{19}{24} f(t_{n+2},y_{n+2}) - \\frac{5}{24} f(t_{n+1},y_{n+1}) + \\frac{1}{24} f(t_n,y_n) \\right) , \\\\\ny_{n+4} &= y_{n+3} + h \\left( \\frac{251}{720} f(t_{n+4},y_{n+4}) + \\frac{646}{720} f(t_{n+3},y_{n+3}) - \\frac{264}{720} f(t_{n+2},y_{n+2}) + \\frac{106}{720} f(t_{n+1},y_{n+1}) - \\frac{19}{720} f(t_n,y_n) \\right) .\n\\end{align} </math>\n\nThe derivation of the Adams–Moulton methods is similar to that of the Adams–Bashforth method; however, the interpolating polynomial uses not only the points <math>t_{n-1},\\dots, t_{n-s} </math>, as above, but also <math> t_n </math>. The coefficients are given by\n:<math> b_{s-j} = \\frac{(-1)^j}{j!(s-j)!} \\int_0^1 \\prod_{i=0 \\atop i\\ne j}^{s} (u+i-1) \\,du, \\qquad \\text{for } j=0,\\ldots,s. </math>\n\nThe Adams–Moulton methods are solely due to [[John Couch Adams]], like the Adams–Bashforth methods. The name of [[Forest Ray Moulton]] became associated with these methods because he realized that they could be used in tandem with the Adams–Bashforth methods as a [[Predictor-corrector method|predictor-corrector]] pair {{harv|Moulton|1926}}; {{harvtxt|Milne|1926}} had the same idea. Adams used [[Newton's method]] to solve the implicit equation {{harv|Hairer|Nørsett|Wanner|1993|loc=§III.1}}.\n\n=== Backward differentiation formulas (BDF) ===\n:{{main|Backward differentiation formula}}\nThe BDF methods are implicit methods with <math> b_{s-1} = \\cdots = b_0 = 0 </math> and the other coefficients chosen such that the method attains order ''s'' (the maximum possible). These methods are especially used for the solution of [[stiff equation|stiff differential equation]]s.\n\n== Analysis ==\nThe central concepts in the analysis of linear multistep methods, and indeed any numerical method for differential equations, are [[Numerical ordinary differential equations#Analysis|convergence, order, and stability]].\n\n=== Consistency and order ===\nThe first question is whether the method is consistent: is the difference equation\n:<math> \\begin{align}\n& y_{n+s} + a_{s-1} y_{n+s-1} + a_{s-2} y_{n+s-2} + \\cdots + a_0 y_n \\\\\n& \\qquad {} = h \\bigl( b_s f(t_{n+s},y_{n+s}) + b_{s-1} f(t_{n+s-1},y_{n+s-1}) + \\cdots + b_0 f(t_n,y_n) \\bigr),\n\\end{align} </math>\na good approximation of the differential equation <math> y' = f(t,y) </math>? More precisely, a multistep method is ''consistent'' if the [[local truncation error]] goes to zero faster than the step size ''h'' as ''h'' goes to zero, where the ''local truncation error'' is defined to be the difference between the result <math>y_{n+s}</math> of the method, assuming that all the previous values <math>y_{n+s-1}, \\ldots, y_n</math> are exact, and the exact solution of the equation at time <math>t_{n+s}</math>. A computation using [[Taylor series]] shows that a linear multistep method is consistent if and only if\n:<math> \\sum_{k=0}^{s-1} a_k = -1 \\quad\\text{and}\\quad \\sum_{k=0}^s b_k = s + \\sum_{k=0}^{s-1} ka_k. </math>\nAll the methods mentioned above are consistent {{harv|Hairer|Nørsett|Wanner|1993|loc=§III.2}}.\n\nIf the method is consistent, then the next question is how well the difference equation defining the numerical method approximates the differential equation. A multistep method is said to have ''order'' ''p'' if the local error is of order <math>O(h^{p+1})</math> as ''h'' goes to zero. This is equivalent to the following condition on the coefficients of the methods:\n:<math> \\sum_{k=0}^{s-1} a_k = -1 \\quad\\text{and}\\quad q \\sum_{k=0}^s k^{q-1} b_k = s^q + \\sum_{k=0}^{s-1} k^q a_k \\text{ for } q=1,\\ldots,p. </math>\nThe ''s''-step Adams–Bashforth method has order ''s'', while the ''s''-step Adams–Moulton method has order <math>s+1</math> {{harv|Hairer|Nørsett|Wanner|1993|loc=§III.2}}.\n\nThese conditions are often formulated using the ''characteristic polynomials''\n:<math> \\rho(z) = z^s + \\sum_{k=0}^{s-1} a_k z^k \\quad\\text{and}\\quad \\sigma(z) = \\sum_{k=0}^s b_k z^k. </math>\nIn terms of these polynomials, the above condition for the method to have order ''p'' becomes\n:<math> \\rho(\\mathrm{e}^h) - h\\sigma(\\mathrm{e}^h) = O(h^{p+1}) \\quad \\text{as } h\\to 0. </math>\nIn particular, the method is consistent if it has order at least one, which is the case if <math>\\rho(1)=0</math> and <math>\\rho'(1)=\\sigma(1)</math>.\n\n=== Stability and convergence ===\n<!-- [[Zero-stability]] redirects here -->\n\nThe numerical solution of a one-step method depends on the initial condition <math> y_0 </math>, but the numerical solution of an ''s''-step method depend on all the ''s'' starting values, <math> y_0, y_1, \\ldots, y_{s-1} </math>. It is thus of interest whether the numerical solution is stable with respect to perturbations in the starting values. A linear multistep method is ''zero-stable'' for a certain differential equation on a given time interval, if a perturbation in the starting values of size ε causes the numerical solution over that time interval to change by no more than ''K''ε for some value of ''K'' which does not depend on the step size ''h''. This is called \"zero-stability\" because it is enough to check the condition for the differential equation <math> y' = 0 </math> {{harv|Süli|Mayers|2003|p=332}}.\n\nIf the roots of the characteristic polynomial ρ all have modulus less than or equal to 1 and the roots of modulus 1 are of multiplicity 1, we say that the ''root condition'' is satisfied. A linear multistep method is zero-stable if and only if the root condition is satisfied {{harv|Süli|Mayers|2003|p=335}}.\n\nNow suppose that a consistent linear multistep method is applied to a sufficiently smooth differential equation and that the starting values <math> y_1, \\ldots, y_{s-1}</math> all converge to the initial value <math> y_0 </math> as <math> h \\to 0 </math>. Then, the numerical solution converges to the exact solution as <math> h \\to 0 </math> if and only if the method is zero-stable. This result is known as the ''Dahlquist equivalence theorem'', named after [[Germund Dahlquist]]; this theorem is similar in spirit to the [[Lax equivalence theorem]] for [[finite difference method]]s. Furthermore, if the method has order ''p'', then the [[global truncation error|global error]] (the difference between the numerical solution and the exact solution at a fixed time) is <math> O(h^p) </math> {{harv|Süli|Mayers|2003|p=340}}.\n\nFurthermore, if the method is convergent, the method is said to be ''strongly stable'' if <math>z=1</math> is the only root of modulus 1. If it is convergent and all roots of modulus 1 are not repeated, but there is more than one such root, it is said to be ''relatively stable''. Note that 1 must be a root for the method to be convergent; thus convergent methods are always one of these two.\n\nTo assess the performance of linear multistep methods on [[stiff equation]]s, consider the linear test equation ''y''' = λ''y''. A multistep method applied to this differential equation with step size ''h'' yields a linear [[recurrence relation]] with characteristic polynomial\n:<math> \\pi(z; h\\lambda) = (1 - h\\lambda\\beta_s) z^s + \\sum_{k=0}^{s-1} (\\alpha_k - h\\lambda\\beta_k) z^k = \\rho(z) - h\\lambda\\sigma(z). </math>\nThis polynomial is called the ''stability polynomial'' of the multistep method. If all of its roots have modulus less than one then the numerical solution of the multistep method will converge to zero and the multistep method is said to be ''absolutely stable'' for that value of ''h''λ. The method is said to be ''A-stable'' if it is absolutely stable for all ''h''λ with negative real part. The ''region of absolute stability'' is the set of all ''h''λ for which the multistep method is absolutely stable {{harv|Süli|Mayers|2003|pp=347 & 348}}. For more details, see the section on [[Stiff equation#Multistep methods|stiff equations and multistep methods]].\n\n=== Example ===\nConsider the Adams–Bashforth three-step method\n<!-- save expression that computes y_n+1 rather than y_n+s\n:<math>y_{n+1} = y_n + h\\left( {23\\over 12} f(t_n, y_n) - {16 \\over 12} f(t_{n-1}, y_{n-1}) + {5\\over 12}f(t_{n-2}, y_{n-2})\\right).</math>\n-->\n:<math>y_{n+3} = y_{n+2} + h\\left( {23\\over 12} f(t_{n+2}, y_{n+2}) - {4 \\over 3} f(t_{n+1}, y_{n+1}) + {5\\over 12}f(t_{n}, y_{n})\\right).</math>\nOne characteristic polynomial is thus\n:<math>\\rho(z)=z^3-z^2=z^2(z-1)\\,</math>\nwhich has roots <math>z=0, 1</math>, and the conditions above are satisfied. As <math>z=1</math> is the only root of modulus 1, the method is strongly stable.\n\nThe other characteristic polynomial is\n\n<math>\\sigma(z) = \\frac{23}{12}z^{2} - \\frac{4}{3}z + \\frac{5}{12} </math>\n\n==First and second Dahlquist barriers==\nThese two results were proved by [[Germund Dahlquist]] and represent an important bound for the order of convergence and for the [[Stiff equation#A-stability|A-stability]] of a linear multistep method. The first Dahlquist barrier was proved in {{harvtxt|Dahlquist|1956}} and the second in {{harvtxt|Dahlquist|1963}}.\n\n===First Dahlquist barrier===\n\nA zero-stable and linear ''q''-step multistep method cannot attain an order of convergence greater than ''q'' + 1 if ''q'' is odd and greater than ''q'' + 2 if ''q'' is even. If the method is also explicit, then it cannot attain an order greater than ''q'' {{harv|Hairer|Nørsett|Wanner|1993|loc=Thm III.3.5}}.\n\n===Second Dahlquist barrier===\n\nThere are no explicit [[Stiff equation#A-stability|A-stable]] and linear multistep methods. The implicit ones have order of convergence at most 2. The [[trapezoidal rule (differential equations)|trapezoidal rule]] has the smallest error constant amongst the A-stable linear multistep methods of order 2.\n\n== See also ==\n*[[Digital energy gain]]\n\n== References ==\n* {{citation | first1 = Francis | last1 = Bashforth | year = 1883 | title = An Attempt to test the Theories of Capillary Action by comparing the theoretical and measured forms of drops of fluid. With an explanation of the method of integration employed in constructing the tables which give the theoretical forms of such drops, by J. C. Adams | location = Cambridge }}.\n* {{Citation | last1=Butcher | first1=John C. | author1-link = John C. Butcher | title=Numerical Methods for Ordinary Differential Equations | publisher=John Wiley | isbn=978-0-471-96758-3 | year=2003}}.\n* {{Citation | last1=Dahlquist | first1=Germund | author1-link=Germund Dahlquist | title=Convergence and stability in the numerical integration of ordinary differential equations | year=1956 | journal=Mathematica Scandinavica | volume=4 | pages=33--53}}.\n* {{Citation | last1=Dahlquist | first1=Germund | author1-link=Germund Dahlquist | title=A special stability problem for linear multistep methods | doi=10.1007/BF01963532 | year=1963 | journal=BIT | issn=0006-3835 | volume=3 | pages=27–43}}.\n* {{citation | first1 = Herman H. | last1 = Goldstine | author1-link = Herman Goldstine | year = 1977 | title = A History of Numerical Analysis from the 16th through the 19th Century | publisher = Springer-Verlag | location = New York | isbn = 978-0-387-90277-7 }}.\n* {{citation | first1 = Ernst | last1 = Hairer | first2 = Syvert Paul | last2 = Nørsett | first3 = Gerhard | last3 = Wanner | year = 1993 | title = Solving ordinary differential equations I: Nonstiff problems | edition = 2nd | publisher = Springer Verlag | location = Berlin | isbn = 978-3-540-56670-0 }}.\n* {{Citation | last1=Hairer | first1=Ernst | last2=Wanner | first2=Gerhard | title=Solving ordinary differential equations II: Stiff and differential-algebraic problems | publisher=[[Springer-Verlag]] | location=Berlin, New York | edition=2nd | isbn=978-3-540-60452-5 | year=1996}}.\n* {{citation | first1 = Arieh | last1 = Iserles | year = 1996 | title = A First Course in the Numerical Analysis of Differential Equations | publisher = Cambridge University Press | isbn = 978-0-521-55655-2 }}.\n* {{citation | first1 = W. E. | last1 = Milne | year = 1926 | title = Numerical integration of ordinary differential equations | journal = American Mathematical Monthly | volume = 33 | issue = 9 | pages = 455–460 | doi = 10.2307/2299609 | jstor = 2299609 | publisher = Mathematical Association of America }}.\n* {{citation | first1 = Forest R. | last1 = Moulton | author1-link = Forest Ray Moulton | year = 1926 | title = New methods in exterior ballistics | publisher = University of Chicago Press }}.\n* {{citation | first1 = Alfio | last1 = Quarteroni | first2 = Riccardo | last2 = Sacco | first3 = Fausto | last3 = Saleri | year = 2000 | title = Matematica Numerica | publisher = Springer Verlag | isbn = 978-88-470-0077-3 }}.\n* {{Citation | last1=Süli | first1=Endre | last2=Mayers | first2=David | title=An Introduction to Numerical Analysis | publisher=[[Cambridge University Press]] | isbn=0-521-00794-1 | year=2003}}.\n\n== External links ==\n* {{MathWorld | urlname= AdamsMethod| title= Adams Method }}\n*[http://www.dotnumerics.com/NumericalLibraries/DifferentialEquations/ DotNumerics: Ordinary Differential Equations for C# and VB.NET] Initial-value problem for nonstiff and stiff ordinary differential equations (explicit Runge-Kutta, implicit Runge-Kutta, Gear’s BDF and Adams-Moulton).\n\n{{Numerical integrators}}\n\n[[Category:Numerical differential equations]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "List of finite element software packages",
      "url": "https://en.wikipedia.org/wiki/List_of_finite_element_software_packages",
      "text": "This is a list of [[Computer software|software packages]] that implement the [[finite element method]] for solving [[partial differential equations]].\n\n{| class=\"wikitable sortable\" style=\" width:100%;\"\n|-\n! Software !! Features !! Developer !! Version !! Released !! License !! Price !! Platform \n|-\n| [[Agros2D]] || Multiplatform open source application for the solution of physical problems based on the Hermes library || University of West Bohemia || 3.2 || 2014-03-03 || [[GNU GPL]] || Free || [[Linux]], [[Windows]]\n|- \n| [[ANSA Pre-processor]] || An advanced CAE pre-processing software for complete model build up. || BETA ||  ||  || [[Proprietary software|Proprietary]] [[commercial software]] ||  || [[Linux]], [[Mac OS X]], [[Windows]]\n|-\n| [[Calculix|CalculiX]] ||It is an Open Source FEA project.  The solver uses a partially compatible [[ABAQUS]] file format.  The pre/post-processor generates input data for many FEA and CFD applications || Guido Dhondt, Klaus Wittig || 2.14 || 2018-04-27 || [[GNU GPL]] || Free || [[Linux]], [[Windows]]\n|-\n| [[DIANA FEA]] || General purpose finite element package utilised by civil, structural and geotechnical engineers.  || DIANA FEA BV, The Netherlands || 10.1  || 2016-11-14 || Commercial || Paid || [[Windows]], [[Linux]]\n|-\n| [[deal.II]] || Comprehensive set of tools for finite element codes, scaling from laptops to clusters with 10,000+ cores. Written in C++. || Wolfgang Bangerth, Timo Heister, Guido Kanschat, Matthias Maier et al. || 9.0  || 2018-05-12 || [[LGPL]] || Free || [[Linux]], [[Unix]], [[Mac OS X]], [[Windows]]\n|-\n| [[Dune (software)|DUNE]] || Distributed and Unified Numerics Environment, written in C++ || DUNE Developer team || 2.4.1 || 2016-02-29 || [[GPL#Version 2|GPL Version 2]] with [[GPL linking exception|Run-Time Exception]] || Free || [[Linux]], [[Unix]], [[Mac OS X]]\n|-\n| [[Elmer FEM solver|Elmer]] || Open source multiphysical simulation software developed by Finnish Ministry of Education's CSC, written primarily in Fortran (written in Fortran90, C and C++) || [[CSC – IT Center for Science|CSC]] || 8.2|| 2016-03-15 || [[GPL]] || Free || [[Linux]], [[Mac OS X]], [[Windows]]\n|-\n| [[Febio|FEBio]] || Finite Elements for Biomechanics || University of Utah (MRL), Columbia University (MBL) || 2.7 || April, 2018 || Custom || Free || [[Linux]], [[Mac OS X]], [[Windows]]\n|-\n| [[FEniCS Project]] || Software package developed by American and European researchers with the goal to enable automated solution of differential equations || FEniCS Team  || 1.6.0 || 2015-07-29 || [[LGPL]] (Core) & [[GPL]]/[[LGPL]] (Non-Core)<ref>{{cite web|url=https://fenicsproject.org/|title=FEniCS Project|access-date=2017-06-21}}</ref>|| Free || [[Linux]], [[Unix]], [[Mac OS X]], [[Windows]]\n|-\n| [[FEATool Multiphysics]] || [[MATLAB]] FEM and PDE multiphysics simulation toolbox || Precise Simulation || 1.10 || 2019-05-17 || [[Proprietary software|Proprietary]] [[EULA]] || Free for personal use<ref>{{cite web|url=https://www.featool.com |title=FEATool Multiphysics - Product Information |date= |accessdate=2018-06-12}}</ref> || [[Windows]], [[Mac OS X]], [[Linux]], [[Unix]]\n|-\n| [[Femap]] || Finite element pre- and post-processor || [[Siemens PLM Software]]  || 11.4.0 || 2016-09-16 || [[Proprietary software|Proprietary]] [[commercial software]] ||  || [[Windows]]\n|-\n|[[FreeFem++]]<ref>{{Cite web|url=https://freefem.org|title=FreeFem++|website=freefem.org|language=en|access-date=2018-11-30}}</ref>|| FreeFem++ is a free and open-source parallel FEA software for multiphysics simulations. The problems are defined in terms of their variational formulation and can be easily implemented using FreeFem++ language. Written in [[C++]]. ||[[Sorbonne University]]<ref>{{Cite web|url=http://www.sorbonne-universite.fr/en|title=Sorbonne Université {{!}} Lettres, Médecine, Sciences|website=www.sorbonne-universite.fr|language=en|access-date=2018-11-30}}</ref> and Jacques-Louis Lions Laboratory<ref>{{Cite web|url=http://sciences.sorbonne-universite.fr/en/research/modeling_and_engineering/modeling_and_engineering_laboratories/jacques_louis_lions_laboratory_ljll_umr_7598.html|title=Jacques-Louis Lions Laboratory (LJLL) - UMR 7598 - SCIENCE|last=Curie|first=UPMC - Université Pierre et Marie|website=sciences.sorbonne-universite.fr|language=en|access-date=2018-11-30}}</ref>|| 3.61 || 2018-06-20 ||[[LGPL]]|| Free || [[Linux]], [[Mac OS X|MacOS]], [[Windows]], [[Solaris (operating system)|Solaris]]\n|-\n|[[Goma_(software)|GOMA]]\n|GOMA is an open-source, parallel, and scalable multiphysics software package for modeling and simulation of real-life physical processes, with a basis in computational fluid dynamics for problems with evolving geometry.\n| [[Sandia National Laboratories]], [[University of New Mexico]]\n| [https://goma.github.io/] | 6.1 || Aug 28, 2015 || [[GNU_General_Public_License#Version_2|GPL Version 2]] || Free || [[Linux]]\n|-\n| [[GetFEM++]] || A generic finite element library written in [[C++]] with interfaces for [[Python (programming language)|Python]], [[Matlab]] and [[Scilab]]. It focuses on modeling of contact mechanics and discontinuities (e.g. cracks). || Yves Renard, Julien Pommier || 5.0 ||  2015-07 || [[LGPL]]|| Free || [[Unix]], [[Mac OS X]], [[Windows]]\n|-\n| [[Hermes Project]] || Modular C/C++ library for rapid development of space- and space-time adaptive hp-FEM solvers || hp-FEM group || 3.0 ||  2014-03-01 || [[LGPL]]|| Free || [[Linux]], [[Unix]], [[Mac OS X]], [[Windows]]\n|-\n| [[Mathematica]]<ref>[http://reference.wolfram.com/language/FEMDocumentation/guide/FiniteElementMethodGuide.html Mathematica Documentation]</ref> || General purpose computation software. || [[Wolfram Research]] || {{Latest stable software release/Mathematica}} || Regularly || [[Proprietary software|Proprietary]] ||  || [[Linux]], [[Mac OS X]], [[Windows]], [[Raspberry Pi|Raspbian]], Online service.\n|-\n|[[MFEM]]\n|MFEM is a free, lightweight, scalable C++ library for [[Finite element method|finite element methods]] that features arbitrary high-order finite element meshes and spaces, support for a wide variety of discretizations, and emphasis on usability, generality, and [[High Performance Computing|high-performance computing]] efficiency.\n|MFEM team\n|3.4 \n|2018-05-29\n|[[LGPL|LGPL-2.1]] with static linking exception\n|Free\n|[[Linux]], [[Unix]], [[Mac OS X]], [[Windows]]\n|-\n| [[MoFEM JosePH]] || Mesh Oriented hp-FE code, written in C++ || [[University of Glasgow]] || 0.6.8 || 2017-11-16 || [[LGPL]]  || Free || [[Unix]], [[Mac OS X]]\n|-\n| [[OOFEM]] || Object Oriented Finite EleMent solver, written in C++ || Bořek Patzák || 2.4 || 2016-02-15 || [[GPL#Version 2|GPL Version 2]] || Free || [[Unix]], [[Windows]]\n|-\n| [[OpenSees]] || Open System for Earthquake Engineering Simulation || || || || Non Commercial || Free || [[Unix]], [[Linux]], [[Windows]]\n|-\n| [[SESAM (FEM)]] || Software suite for structural and hydrodynamic analysis of ships and offshore structures || [[DNV GL]] || || regularly || [[Proprietary software|Proprietary]], [[Software as a service|SaaS]] || || [[Windows]], [[Web browser]] \n|-\n| [[Range Software]] || Multiphysics Finite Element Analysis Software || [[Tomáš Šoltys]] || 3.0 || 2018-04-30 || [[GPL]] || Free || [[Linux]], [[Windows]]\n|-\n| [[Z88 FEM software|Z88]]/Z88Aurora || Freeware finite element package; The present version Z88Aurora V4 offers, in addition to static strength analysis modules such as non-linear strength calculations (large displacements), simulations with non-linear materials, natural frequency, static thermal analysis and a contact module. ||  Frank Rieg  || Z88 V15, Z88Aurora V4 || 2017-07-17, 2017-04-24 || GNU GPL, Custom || Free || [[Linux]], [[Windows]], [[Mac OS X]]\n|-\n| [[Abaqus]] || Advanced Franco-USA software from [[SIMULIA]], owned by [[Dassault Systemes]] || Abaqus Inc. || 2019 || 2019-12 ||[[Proprietary software|Proprietary]] [[commercial software]] ||  || [[Linux]], [[Windows]]\n|-\n| [[CONSELF]] || CAE simulation from your browser || CONSELF SRL || 2.9 || 2015-10 || [[Software as a service|SaaS]] || [[Freemium]] || [[Web browser]]\n|-\n|[[FreeCAD]] || Parametric 3D modeler with a FEM workbench allowing it to use external solvers like CalculiX, Z88, Elmer, and OpenFoam || FreeCAD Team || 0.18 || {{date|2019-03-12}} || |[[LGPL|LGPL 2]] || Free || [[Linux]], [[Windows]], [[Mac OS X]]\n|-\n| [[HyperMesh]] || Finite element pre- and post-processor. \n\n(Hypermesh is a product within the HyperWorks suite)\n| Altair ||  ||  || [[Proprietary software|Proprietary]] [[commercial software]] ||  || [[Linux]], [[Mac OS X]], [[Windows]]\n|-\n| [[ADINA]] || Finite element software for structural, fluid, heat transfer, electromagnetic, and multiphysics problems, including fluid-structure interaction and thermo-mechanical coupling || Adina R&D ||  ||  || [[Proprietary software|Proprietary]] [[commercial software]] ||  ||\n|-\n| [[Advance Design]] || BIM software for FEM structural analysis, including international design eurocodes || [[GRAITEC]] || 2014|| 2013-09 || [[Proprietary software|Proprietary]] [[commercial software]] ||  ||\n|-\n| [[Autodesk Simulation]] || Finite Element software of Autodesk || [[Autodesk]] || ||  || [[Proprietary software|Proprietary]] [[commercial software]] || || [[Windows]]\n|-\n| [[ANSYS]] || US-based and -developed full CAE software package || Ansys Inc. || 19.2 || 2018-09-18 ||[[Proprietary software|Proprietary]] [[commercial software]] || Free student version available, up to 32,000 nodes/elements<ref>{{cite web|url=http://www.ansys.com/Products/Academic/ANSYS-Student |title=Student Products - Free Simulation Software |website=Ansys.com |date= |accessdate=2017-05-28}}</ref> || [[Windows]], [[Linux]]\n|-\n| [[COMSOL Multiphysics]]|| COMSOL Multiphysics Finite Element Analysis Software (formerly FEMLAB) || COMSOL Inc. || 5.4 || 2018-10-03 ||[[Proprietary software|Proprietary]] [[EULA]] || || [[Linux]], [[Mac OS X]], [[Windows]], [[Web browser]]\n|-\n| [[CosmosWorks]] || Part of [[SolidWorks]] || [[Dassault Systèmes|Dassault Systèmes SolidWorks Corp.]] || ||  || [[Proprietary software|Proprietary]] [[commercial software]] || || [[Windows]]\n|-\n| [[Quickfield]]||  || Tor Cooperative || 6.1 || 2015-12-22 || [[Proprietary software|Proprietary]] [[EULA]]|| || [[Windows]]\n|-\n| [[LS-DYNA]]|| Best known for explicit dynamics / crash analysis || LSTC - Livermore Software Technology Corporation || R8.0 || 2015-03 || [[Proprietary software|Proprietary]] [[commercial software]] || || [[Linux]], [[Windows]]\n|-\n| [[Nastran]]|| Originally developed for [[NASA]], now available commercially from several software companies || MSC NASTRAN, Siemens PLM NX Nastran<ref>{{cite web|url=https://www.plm.automation.siemens.com/en_us/products/simcenter/nastran/ |title=NX Nastran: Siemens PLM Software |website=Plm.automation.siemens.com |date= |accessdate=2017-05-28}}</ref> || 2014 || 2014 || [[Proprietary software|Proprietary]] [[EULA]] || || [[Linux]], [[Mac OS X]], [[Windows]]\n|-\n| [[RFEM]] || 3D finite element analysis software || Dlubal Software || 5.06 || 2016-02 || [[Proprietary software|Proprietary]] [[commercial software]] || Free student license available<ref>{{cite web|author= |url=https://www.dlubal.com/en/education/students/free-structural-analysis-software-for-students |title=Free Student License &#124; Dlubal Software |website=Dlubal.com |date= |accessdate=2017-05-28}}</ref> || [[Windows]]\n|-\n| [[SimScale]] || German 100% web-based CAE platform || SimScale GmbH || 14 || 2013-07 || [[Software as a service|SaaS]] || Free community version available<ref>{{cite web|url=https://www.simscale.com/product/pricing |title=Plans & Pricing - SimScale Simulation Platform |website=Simscale.com |date= |accessdate=2017-05-28}}</ref> || [[Web browser]]\n|-\n| [[VisualFEA]] || Finite element software for structural, geotechnical, heat transfer and seepage analysis || Intuition Software || 5.11 || 2016-01 || [[Proprietary software]] || Free educational version available<ref>{{cite web|url=https://ecommons.cornell.edu/handle/1813/43749/browse?type=title |title=Browsing VisualFEA (Finite Element Analysis) by Title |website=Ecommons.cornell.edu |date=2016-03-01 |accessdate=2017-05-28}}</ref> || [[Mac OS X]], [[Windows]]\n|-\n| [[JCMsuite]] || Finite element software for the analysis of electromagnetic waves, elasticity and heat conduction || JCMwave GmbH || 3.6.1 || 2017-01-27 || [[Proprietary software|Proprietary]] [[EULA]] ||  || [[Linux]], [[Windows]]\n|-\n|[[JMAG]] || 2D and 3D finite element analysis software for electromagnetic field, thermal, structural|| [[JSOL]] || 18.1 || 2019-06 ||[[Proprietary software|Proprietary]] [[commercial software]]||Education pack available || |[[Linux]], [[Windows]], [[Web browser]]\n|-\n|[[StressCheck]] || Finite element analysis software based on p-FEM with a focus on solid mechanics applications || [[ESRD, Inc.]] || 10.4  || 2018-04 || [[Proprietary software|Proprietary]] [[commercial software]]|| Student version available (limited to 15k DOF)|| |[[Windows]]\n\n<!--  New entries to the table should follow the format\n| Software || Features || Developer || Version || Released(YYYY-MM-DD) || License || Price || Platform \n-->\n|}\n\n==Feature comparison==\nThis is a wiki table generated by [https://github.com/kostyfisik/FEA-compare FEA-compare] project. Please, contribute changes directly to the project instead of editing the table below. The project also provides an HTML version with the first row and Feature column being fixed for ease of table exploration.\n\n{| class=\"wikitable sortable\"\n|-\n! Feature\n!Deal II\n!Elmerfem\n!FEATool Multiphysics\n!FEniCS\n!Firedrake\n!GetFEM++\n!libMesh\n!MFEM\n!Range\n!COMSOL(R)\n|-\n| license:\n| LGPL\n| GNU (L)GPL\n| Proprietary\n| GNU GPL\\LGPL\n| GNU LGPL\n| LGPL\n| GPL\n|LGPL-2.1 with static linking exception\n| GPL\n| Proprietary\n|-\n| GUI:\n| No\n| Yes, partial functionality\n| Matlab and Octave GUI\n| Postprocessing only\n| No\n| No\n| No\n|No\n| Yes\n| Yes\n|-\n| Community:\n|Google Group\n| 1000's of users, discussion forum, mailing list\n| Mailing list\n| Mailing list\n| Mailing list and IRC channel\n| Mailing list\n|mail lists\n|GitHub\n| GitHub\n|\n|-\n| Documentation:\n| 50+ tutorials, 50+ video lectures, Doxygen\n| ElmerSolver Manual, Elmer Models Manual, ElmerGUI Tutorials, etc. (>700 pages of LaTeX documentation)\n|Online FEATool documentation, ~600 pages, ~50 built-in step-by-step GUI tutorials, and 150 m-script model examples\n| Tutorial, demos (how many?), 700-page book\n| Manual, demos, API reference\n| User doc, tutorials, demos, developer's guide\n| Doxygen, 40+ example codes\n|Doxygen, Example codes\n| user manual, tutorials\n|\n|-\n| colspan=\"100501\" | '''Mesh'''\n|-\n| mesh elements:\n| intervals (1d), quads (2d), and hexes (3d) only\n| intervals (1d), triangles, quadrilaterals (2d), tetrahedra, pyramids, wedges, hexahedra (3d)\n| intervals, triangles, tetrahedra, quads, hexes\n| intervals, triangles, tetrahedra (quads, hexes - work in progress)\n| intervals, triangles, tetrahedra, quads, plus extruded meshes of hexes and wedges\n| intervals, triangles, tetrahedra, quads, hexes, prisms, some 4D elements, easily extensible.\n| Tria, Quad, Tetra, Prism, etc.\n|segments (1d), triangles, quadrilaterals (2d), tetrahedra, hexahedra (3d)\n| points(0d), segments (1d), triangles, quadrilaterals (2d), tetrahedra, hexahedra (3d)\n|\n|-\n| mesh high-order mapping:\n|any order\n| Yes, for Lagrange elements\n|\n| (Any - work in progress)\n| (Any - using appropriate branches)\n|\n|\n|any order\n|\n| Any? Second-order is the default for most cases.\n|-\n| mesh generation:\n| external+predefined shapes\n| Limited own meshing capabilities with ElmerGrid and netgen/tetgen APIs. Internal extrusion and mesh multiplication on parallel level.\n| Integrated DistMesh, Gmsh, and Triangle GUI and CLI interfaces\n| Yes, Constructive Solid Geometry (CSG) supported via mshr (CGAL and Tetgen used as backends)\n| External + predefined shapes. Internal mesh extrusion operation.\n| Experimental in any dimension + predefined shapes + Extrusion.\n| Built-in\n|Support for [[Gmsh]], CUBIT, NetGen, [[Www.truegrid.com/|Truegrid]], [[NetCDF]], etc. formats + some internal tools\n| Yes (TetGen)\n| Built-in\n|-\n| mesh adaptive-refinement:\n| h, p, and hp for CG and DG\n| h-refinement for selected equations\n|\n| Only h\n|\n| Only h\n| h, p, mached hp, singular hp\n|Conforming and non-confrorming h-refinement for any p. General mesh optimization / r-adaptivity.\n|\n| generate new mesh with variable density, no(?) p-refinement.\n|-\n| mesh input\\output:\n|\n|\n| OpenFOAM, FEniCS XML, GiD, Gmsh, GMV, Triangle\n| XDMF (and FEniCS XML)\n|\n| gmsh, GiD, Ansys\n|\n|[[Gmsh]], CUBIT, NetGen, [[Www.truegrid.com/|Truegrid]], [[NetCDF]], etc.\n| rbm, stl\n|\n|-\n| mesh check:\n|\n|\n|\n| intersections (collision testing)\n|\n| ?\n|\n|Minimal\n| limited features (double nodes, degenerated elements, intersected elements)\n|\n|-\n| CAD files support:\n| IGES, STEP (with OpenCascade wrapper)\n| Limited support via OpenCASCADE in ElmerGUI\n|\n|\n|\n| No\n|\n|NURBS meshes\n| Yes (stl)\n| STEP, IGES and many others.\n|-\n| mesh operation:\n|\n|\n| Merge, join, extrude, and revolve operations\n|\n|\n| Extrude, rotate, translation, refine\n| distort/translate/rotate/scale\n|Mesh optimization, general AMR\n| Extrude, rotate, translation, refine\n|\n|-\n| colspan=\"100501\" | '''Parallel possibilities'''\n|-\n| automatic mesh partitioning:\n| yes, shared (METIS/Parmetis) and distributed (p4est)\n| partitioning with ElmerGrid using Metis or geometric division\n|\n| Yes (ParMETIS and SCOTCH)\n| Yes\n| Yes (METIS)\n|\n|Yes\n| No\n|\n|-\n| MPI:\n| Yes (up to 147k processes), test for 4k processes and geometric multigrid for 147k, strong and weak scaling\n| Yes, demonstrated scalability up to 1000's of cores\n|\n| Yes, DOLFIN solver scales up to 24k\n| Yes, Scaling plot for Firedrake out to 24k cores.\n| Yes\n| Yes\n|Yes\n| No\n| Almost ideal for parameter sweep? For large scale simulations Comsol 4.2 bench by Pepper has 19.2 speedup on 24 core cluster (0.8 efficiency).\n|-\n| threads:\n| Threading Build Blocks\n| threadsafe, limited threading, work in progress\n|\n|\n|\n|\n| Yes\n|No\n| Yes\n|\n|-\n| OpenMP:\n| Yes (vectorization only)\n| Yes, partially\n|\n|\n| Limited\n| Yes\n|\n|Yes\n| Yes\n|\n|-\n| OpenCL:\n| No\n| No\n|\n|\n|\n| No\n|\n|Yes, based on OCCA\n| No\n|\n|-\n| CUDA:\n| work in progress\n| No\n|\n|\n|\n| No\n|\n|Yes, based on OCCA\n| No\n|\n|-\n| colspan=\"100501\" | '''Solver'''\n|-\n| Dimension:\n| 1/2/3D\n| 1D/2D/3D (dimensions may coexist)\n| 1/2/3D\n| 1/2/3D\n| 1/2/3D\n| Any, possibility to mix and couple problem of different dimension\n| 2D\\3D\n|1D/2D/3D\n| 0D/1D/2D/3D (dimensions may coexist)\n|\n|-\n| FE:\n| Lagrange elements of any order, continuous and discontinuous; Nedelec and Raviart-Thomas elements of any order; BDM and Bernstein; elements composed of other elements.\n| Lagrange elements, p-elements up to 10th order, Hcurl conforming elements (linear and quadratic) for\n| Lagrange (1st-5th order), Crouzeix-Raviart, Hermite\n| Lagrange, BDM, RT, Nedelic, Crouzeix-Raviart, all simplex elements in the Periodic Table (femtable.org), any\n| Lagrange, BDM, RT, Nedelec, all simplex elements and Q- quad elements in the Periodic Table, any\n| Continuous and discontinuous Lagrange, Hermite, Argyris, Morley, Nedelec, Raviart-Thomas, composite elements (HCT, FVS), Hierarchical elements, Xfem, easily extensible.\n| Lagrange, Hierarchic, Discontinuous Monomials\n|Arbitrary high-order: H1, H(div), and H(curl)-conforming spaces. Discontinuous L2 spaces, Numerical trace (interfacial) spaces. [[Non-uniform rational B-spline|NURBS]] spaces for [[isogeometric analysis]].\n| Lagrange elements\n| in Wave Optics Module: frequency domain and trainsient UI - 1,2, and 3 order; time explicit UI - 1,2,3, and 4 order;\n|-\n| Quadrature:\n| Gauss-Legendre, Gauss-Lobatto, midpoint, trapezoidal,  Simpson, Milne and Weddle (closed Newton-Cotes for 4 and 7 order polinomials), Gauss quadrature with logarithmic or 1/R weighting function, Telles quadrature of arbitrary order.\n|\n|\n|\n|\n|\n| Gauss-Legendre (1D and tensor product rules in 2D and 3D) tabulated up to 44th-order to high precision, best available rules for triangles and tetrahedra to very high order, best available monomial rules for quadrilaterals and hexahedra.\n|General, in particular Gauss-Legendre and Gauss-Lobatto.\n|\n|\n|-\n| Transient problems:\n| Any user implemented and/or from a set of predifined. Explicit methods: forward Euler, 3rd and 4th order Runge-Kutta. Implicit methods: backward Euler, implicit Midpoint, Crank-Nicolson, SDIRK. Embedded explicit methods: Heun-Euler, Bogacki-Shampine, Dopri, Fehlberg, Cash-Karp.\n|\n| BE, CN, and Fractional-Step-Theta schemes\n|\n|\n|\n|\n|Yes\n| Yes\n| (?) assume 2nd order leapfrog for wave optics?\n|-\n| Predifined equations:\n| Laplace?\n| Around 40 predefined solvers\n| Incompressible Navier-Stokes, Heat transfer, convection-diffusion-reaction, linear elasticity, electromagnetics, Darcy's, Brinkman equations, and support for custom PDE equations\n|\n|\n|\n| No\n|Yes, many\n| Yes (Incompressible Navier-Stokes, Heat transfer (convection-conduction-radiation), Stress analysis, Soft body dynamics, Modal analysis, Electrostatics, Magnetostatics )\n| Yes, via modules\n|-\n| Automated assembly:\n|\n|\n| Yes\n| Yes\n| Yes\n| Yes\n|\n|Yes\n| Yes\n|\n|-\n| Visualization:\n| External (export to *.vtk and many others)\n| ElmerPost, VTK widget (but Paraview is recommended)\n| Built-in with optional Plotly and GMV export\n| Buil-in simple plotting + External\n| External\n| External or with the Scilab/Matlab/Python interface. Possibility to perform complex slices.\n| No\n|GLVis, [[VisIt]]\n| GUI (built-in)\n| Built-in\n|-\n| Output format:\n| *.dx *.ucd *.gnuplot *.povray *.eps *.gmv *.tecplot *.tecplot_binary *.vtk *.vtu *.svg *.hdf5\n| Several output formats (VTU, gmsh,...)\n| GMV and Plotly\n| VTK(.pvd, .vtu) and XDMF/HDF5\n| VTK(.pvd, .vtu)\n| vtk, gmsh, OpenDX.\n|\n|Many\n|\n|\n|-\n| Boundary elements solver:\n|Yes\n| Existing but without multipole acceleration (not usable for large problems)\n|\n| No\n| No\n| No\n|\n|No\n|\n|\n|-\n| Use multiple meshes:\n|Yes, autorefined from same initial mesh for each variable of a coupled problem\n| Continuity of non-conforming interfaces ensured by mortar finite elements\n|\n| Yes, including non-matching meshes\n| Yes\n| Yes including different dimensions and taking account of any transformation.\n|\n|No\n|\n|\n|-\n| colspan=\"100501\" | '''Linear algebra'''\n|-\n| Used libs:\n| Built-in + Trilinos, PETSc, and SLEPc\n| Built-in, Hypre, Trilinos, umfpack, MUMPS, Pardiso, etc. (optional)\n| Matlab/Octave built-in (Umfpack), supports integration with the FEniCS and OpenFOAM solvers\n| PETSc, Trilinos/TPetra, Eigen.\n| PETSc\n| SuperLU, MUMPS, Built-in.\n| PETSc, Trilinos, LASPack,  SLEPc\n|''[[hypre]]'', optionally [[Portable, Extensible Toolkit for Scientific Computation|PETSc]], SUNDIALS, SuperLU, SuiteSparse, and more\n| No\n|\n|-\n| Iterative matrix solvers:\n| All Krylov (CG, Minres, GMRES, BiCGStab, QMRS)\n| Built-in Krylov solvers, Krylov and multigrid solvers from external libraries\n| Matlab/Octave built-in\n|\n|\n| All Krylov\n| LASPack serial, PETSc parallel\n|All Krylov\n| GMRES, CG\n|\n|-\n| Preconditioners:\n| Many, including algebraic multigrid (via Hypre and ML) and geometric multigrid\n| Built-in preconditioners (ILU, diagonal, vanka, block) and\n| Matlab/Octave built-in\n|\n|\n| Basic ones (ILU, ILUT)\n| LASPack serial, PETSc parallel\n|Scalable AMG via ''[[hypre]]''\n| ILU, Jacobi\n|\n|-\n| colspan=\"100501\" | '''Matrix-free'''\n|-\n| matrix-free:\n| Yes\n| Experimental implementation\n|\n|\n| Yes\n| No\n|\n|Yes\n| No\n|\n|-\n| matrix-free save memory:\n| Yes\n|\n|\n|\n|\n| No\n|\n|Yes\n| No\n|\n|-\n| matrix-free speed-up:\n|Yes\n|\n|\n|\n|\n| No\n|\n|Yes\n| No\n|\n|-\n| colspan=\"100501\" | '''Used language'''\n|-\n| Native language:\n| C++\n| Fortran (2003 standard)\n| Matlab / Octave\n| C++\n| Python (and generated C)\n| C++\n| C++\n|C++\n| C++\n|\n|-\n| Bindings to language:\n| No\n|\n|\n| Python\n|\n| Python, Scilab or Matlab\n|\n|Python\n| No\n|\n|-\n| colspan=\"100501\" | '''Other'''\n|-\n| Predefined equations:\n|\n|\n|\n|\n|\n| Model bricks: Laplace, linear and nonlinear elasticity, Helmholtz, plasticity, Mindlin and K.L. plates, boundary conditions including contact with friction.\n|\n|Many\n|\n|\n|-\n| Coupled nonlinear problems:\n|\n|\n|\n|\n|\n| Yes\n|\n|Yes\n|\n|\n|-\n| Binary:\n| Linux, Windows (work in progress), Mac\n| Windows, Linux (launchpad: Debian/Ubuntu), Mac (homebrew) (all with MPI)\n| Windows, Linux, Mac\n| Linux (Debian\\Ubuntu), Mac\n| No. Automated installers for Linux and Mac\n| Linux (Debian/Ubuntu)\n|\n|Yes\n|\n|\n|-\n| fullname:\n|\n| Elmer finite element software\n|\n|\n|\n|\n|\n|Modular Finite Element Methods Library\n|\n|\n|-\n| Testing:\n|3500+ tests\n| More than 400 consistency tests ensuring backward compatibility\n|\n|\n|\n|\n|\n|Yes\n|\n|\n|-\n| Symbolic derivation of the tangent system for nonlinear problems:\n|\n|\n|\n|\n|\n| Yes\n|\n|No\n|\n|\n|-\n| Support for fictitious domain methods:\n|\n|\n|\n|\n|\n| Yes\n|\n|No\n|\n|\n|-\n| Wilkinson Prize:\n|2007\n|\n|\n|2015 for dolfin-adjoint\n|\n|\n|\n|No\n|\n|\n|-\n| scripting:\n|\n|\n| Fully scriptable in as m-file Matlab scripts and the GUI supports exporting models in script format\n|\n|\n|\n|\n|No\n|\n|\n|-\n| multiphysics:\n|\n|\n| Arbitrary multiphysics couplings are supported\n|\n|\n|\n|\n|Yes\n|\n|\n|-\n| Optimization Solvers:\n|\n|\n|\n|\n|\n|\n| Support for TAO- and nlopt-based constrained optimization solvers incorporating gradient and Hessian information.\n|No\n|\n|\n|}\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:List Of Finite Element Software Packages}}\n[[Category:Scientific simulation software]]\n[[Category:Engineering software companies]]\n[[Category:Finite element software| List of finite element software packages]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Local convergence",
      "url": "https://en.wikipedia.org/wiki/Local_convergence",
      "text": "In [[numerical analysis]], an [[iterative method]] is called '''locally convergent''' if the successive [[approximation]]s produced by the method are guaranteed to [[Limit of a sequence|converge]] to a solution when the initial approximation is already close enough to the solution. Iterative methods for [[nonlinear equation]]s and their systems, such as [[Newton's method]] are usually only locally convergent.\n\nAn iterative method that converges for an arbitrary initial approximation is called '''globally convergent'''. Iterative methods for systems of [[linear equations]] are usually globally convergent.\n\n[[Category:Numerical analysis]]\n[[Category:Iterative methods]]\n[[Category:Optimization algorithms and methods]]\n\n\n{{mathanalysis-stub}}"
    },
    {
      "title": "Loss of significance",
      "url": "https://en.wikipedia.org/wiki/Loss_of_significance",
      "text": "{{Refimprove|date=July 2012}}\n{{Confusing|reason=there are various uses of accuracy and precision (they are not the same) and significance, with unclear relation. Please make the meaning explicit, with a specific example relating all three, and how to obtain them from digits of a number (does the digit before the decimal point count or not, etc.)|date=July 2018}}\n\n[[File:Catastrophic cancellation.svg|thumbnail|right|Example of LOS in case of computing 2 forms of the same function]]\n\n'''Loss of significance''' is an undesirable effect in calculations using finite-precision arithmetic such as [[floating-point arithmetic]]. It occurs when an operation on two numbers increases [[relative error]] substantially more than it increases [[absolute error]], for example in subtracting two nearly equal numbers (known as '''catastrophic cancellation'''). The effect is that the number of [[significant digit]]s in the result is reduced unacceptably. Ways to avoid this effect are studied in [[numerical analysis]].\n\n==Demonstration of the problem==\nThe effect can be demonstrated with decimal numbers.\nThe following example demonstrates loss of significance for a decimal floating-point data type with 10 significant digits:\n\nConsider the decimal number\n\n    0.1234567891234567890\n\nA floating-point representation of this number on a machine that keeps 10 floating-point digits would be\n\n    0.1234567891\n\nwhich is fairly close when measuring the error as a percentage of the value.  It is very different when measured in order of precision. The first is accurate to {{val|10|e=-19}}, while the second is only accurate to {{val|10|e=-10}}.\n\nNow perform the calculation\n\n    0.1234567891234567890 − 0.1234567890000000000\n\nThe answer, accurate to 20 significant digits, is\n\n    0.0000000001234567890\n\nHowever, on the 10-digit floating-point machine, the calculation yields\n\n    0.1234567891 − 0.1234567890 = 0.0000000001\n\nIn both cases the result is accurate to same order of magnitude as the inputs (−20 and −10 respectively).  In the second case, the answer seems to have one significant digit, which would amount to loss of significance.  However, in computer floating-point arithmetic, all operations can be viewed as being performed on [[antilogarithm]]s, for which the rules for [[significant figures]] indicate that the number of significant figures remains the same as the smallest number of significant figures in the [[wikt:mantissa|mantissas]].  The way to indicate this and represent the answer to 10 significant figures is\n\n    {{val|1.000000000|e=-10}}\n\n==Workarounds==\nIt is possible to do computations using an exact fractional representation of rational numbers and keep all significant digits, but this is often prohibitively slower than floating-point arithmetic.\n\nOne of the most important parts of numerical analysis is to avoid or minimize loss of significance in calculations. If the underlying problem is well-posed, there should be a stable algorithm for solving it.\n\n== Loss of significant bits ==\n\nLet ''x'' and ''y'' be positive normalized floating-point numbers.\n\nIn the subtraction ''x'' − ''y'', ''r'' significant bits are lost where\n\n:<math>q \\le r \\le p, </math>\n\n:<math>2^{-p} \\le 1 - \\frac{y}{x} \\le 2^{-q} </math>\n\nfor some positive integers ''p'' and ''q''.\n\n== Instability of the quadratic equation ==\n\nFor example, consider the [[quadratic equation]]\n\n:<math>a x^2 + b x + c = 0,</math>\n\nwith the two exact solutions:\n\n:<math> x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}.</math>\n\nThis formula may not always produce an accurate result.  For example, when <math>c</math> is very small, loss of significance can occur in either of the root calculations, depending on the sign of <math>b</math>.\n\nThe case <math>a = 1</math>, <math>b = 200</math>, <math>c = -0.000015</math> will serve to illustrate the problem:\n\n:<math>x^2 + 200 x - 0.000015 = 0.</math>\n\nWe have\n\n:<math>\\sqrt{b^2 - 4 a c} = \\sqrt{200^2 + 4 \\times 1 \\times 0.000015} = 200.00000015\\dotso</math>\n\nIn real arithmetic, the roots are\n\n:<math>(-200 - 200.00000015) / 2 = -200.000000075,</math>\n:<math>(-200 + 200.00000015) / 2 = 0.000000075.</math>\n\nIn 10-digit floating-point arithmetic:\n\n:<math>(-200 - 200.0000001) / 2 = -200.00000005,</math>\n:<math>(-200 + 200.0000001) / 2 = 0.00000005.</math>\n\nNotice that the solution of greater [[absolute value|magnitude]] is accurate to ten digits, but the first nonzero digit of the solution of lesser magnitude is wrong.\n\nBecause of the subtraction that occurs in the quadratic equation, it does not constitute a stable algorithm to calculate the two roots.\n\n=== A better algorithm ===\nA careful [[floating-point]] computer implementation combines several strategies to produce a robust result. Assuming that the discriminant {{nowrap|''b''<sup>2</sup> − 4''ac''}} is positive, and ''b'' is nonzero, the computation would be as follows:<ref>{{Citation\n |last=Press\n |first= William H.\n |last2= Flannery\n |first2= Brian P.\n |last3= Teukolsky\n |first3= Saul A.\n |last4= Vetterling\n |first4= William T.\n |title= Numerical Recipes in C\n |year= 1992\n |edition= Second\n |url=http://www.nrbook.com/a/bookcpdf.php}}, Section 5.6: \"Quadratic and Cubic Equations\".</ref> \n\n:<math>\\begin{align}\n x_1 &= \\frac{-b - \\sgn (b) \\,\\sqrt {b^2 - 4ac}}{2a}, \\\\\n x_2 &= \\frac{2c}{-b - \\sgn (b) \\,\\sqrt {b^2 - 4ac}} = \\frac{c}{ax_1}.\n\\end{align}</math>\n\nHere sgn denotes the [[sign function]], where <math>\\sgn(b)</math> is 1 if <math>b</math> is positive, and −1 if <math>b</math> is negative. This avoids cancellation problems between <math>b</math> and the square root of the discriminant by ensuring that only numbers of the same sign are added.\n\nTo illustrate the instability of the standard quadratic formula compared this formula, consider a quadratic equation with roots <math>1.786737589984535</math> and <math>1.149782767465722 \\times 10^{-8}</math>. To 16 significant digits, roughly corresponding to [[double-precision]] accuracy on a computer, the monic quadratic equation with these roots may be written as\n\n:<math>x^2 - 1.786737601482363 x + 2.054360090947453 \\times 10^{-8} = 0.</math>\n\nUsing the standard quadratic formula and maintaining 16 significant digits at each step, the standard quadratic formula yields\n:<math>\\sqrt{\\Delta} = 1.786737578486707,</math>\n:<math>x_1 = (1.786737601482363 + 1.786737578486707) / 2 = 1.786737589984535,</math>\n:<math>x_2 = (1.786737601482363 - 1.786737578486707) / 2 = 0.000000011497828.</math>\nNote how cancellation has resulted in <math>x_2</math> being computed to only 8 significant digits of accuracy.\n\nThe variant formula presented here, however, yields the following:\n:<math>x_1 = (1.786737601482363 + 1.786737578486707) / 2 = 1.786737589984535,</math>\n:<math>x_2 = 2.054360090947453 \\times 10^{-8} / 1.786737589984535 = 1.149782767465722 \\times 10^{-8}.</math>\nNote the retention of all significant digits for <math>x_2</math>.\n\nNote that while the above formulation avoids catastrophic cancellation between <math>b</math> and <math>\\sqrt{b^2 - 4ac}</math>, there remains a form of cancellation between the terms <math>b^2</math> and <math>-4ac</math> of the discriminant, which can still lead to loss of up to half of correct significant digits.<ref name=\"kahan\"/><ref name=\"Higham2002\">{{Citation |first=Nicholas |last=Higham |title=Accuracy and Stability of Numerical Algorithms |edition=2nd |publisher=SIAM |year=2002 |isbn=978-0-89871-521-7 |page=10 }}</ref>  The discriminant <math>b^2 - 4ac</math> needs to be computed in arithmetic of twice the precision of the result to avoid this (e.g. [[Quadruple-precision floating-point format|quad]] precision if the final result is to be accurate to full [[double-precision floating-point format|double]] precision).<ref>{{Citation |last=Hough |first=David |journal=IEEE Computer |title=Applications of the proposed IEEE 754 standard for floating point arithmetic |volume=14 |issue=3 |pages=70–74 |doi=10.1109/C-M.1981.220381 |date=March 1981 |postscript=.}}</ref>  This can be in the form of a [[fused multiply-add]] operation.<ref name=\"kahan\"/>\n\nTo illustrate this, consider the following quadratic equation, adapted from Kahan (2004):<ref name=\"kahan\">{{Citation |first=Willian |last=Kahan |title=On the Cost of Floating-Point Computation Without Extra-Precise Arithmetic |url=http://www.cs.berkeley.edu/~wkahan/Qdrtcs.pdf |date=November 20, 2004 |accessdate=2012-12-25}}</ref>\n:<math>94906265.625x^2 - 189812534x + 94906268.375.</math>\nThis equation has <math>\\Delta = 7.5625</math> and roots \n:<math>x_1 = 1.000000028975958,</math>\n:<math>x_2 = 1.000000000000000.</math>\nHowever, when computed using IEEE 754 double-precision arithmetic corresponding to 15 to 17 significant digits of accuracy, <math>\\Delta</math> is rounded to 0.0, and the computed roots are\n:<math>x_1 = 1.000000014487979,</math>\n:<math>x_2 = 1.000000014487979,</math> \nwhich are both false after the 8-th significant digit. This is despite the fact that superficially, the problem seems to require only 11 significant digits of accuracy for its solution.\n\n==See also==\n* [[Round-off error]]\n* [[Kahan summation algorithm]]\n* [[Karlsruhe Accurate Arithmetic]]\n* [[Exsecant]]\n* [[Exponential minus 1]]<!-- function to increase accuracy -->\n* [[Natural logarithm plus 1]]<!-- function to increase accuracy -->\n* [[wikibooks:Fractals/Mathematics/Numerical#Escaping_test|Example in wikibooks: Cancellation of significant digits in numerical computations]]\n\n==References==\n{{Reflist}}\n\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Material point method",
      "url": "https://en.wikipedia.org/wiki/Material_point_method",
      "text": "The '''material point method''' ('''MPM''') is a numerical technique used to simulate the behavior of [[solid]]s, [[liquid]]s, [[gas]]es, and any other [[Continuum mechanics|continuum]] material. Especially, it is a robust spatial discretization method for simulating multi-phase (solid-fluid-gas) interactions. In the MPM, a continuum body is described by a number of small [[Lagrangian mechanics|Lagrangian]] elements referred to as 'material points'. These material points are surrounded by a background mesh/grid that is used only to calculate gradient terms such as the deformation gradient. Unlike other mesh-based methods like the [[finite element method]], [[finite volume method]] or [[finite difference method]], the MPM is '''not''' a mesh based method and is instead categorized as a meshless/meshfree or continuum-based particle method, examples of which are [[Smoothed-particle hydrodynamics|smoothed particle hydrodynamics]] and [[peridynamics]]. Despite the presence of a background mesh, the MPM does not encounter the drawbacks of mesh-based methods (high deformation tangling, advection errors etc.) which makes it a promising and powerful tool in [[computational mechanics]].\n\nThe MPM was originally proposed, as an extension of a similar method known as [[Particle-in-cell|FLIP]] (a further extension of a method called [[Particle-in-cell|PIC]]) to computational solid dynamics, in the early 1990 by Professors [[Deborah L. Sulsky]], Zhen Chen and Howard L. Schreyer at University of New Mexico. After this initial development, the MPM has been further developed both in the national labs as well as the [[University of New Mexico]], [[Oregon State University]], [[University of Utah]] and more across the US and the world. Recently the number of institutions researching the MPM has been growing with added popularity and awareness coming from various sources such as the MPM's use in the Disney film ''[[Frozen (2013 film)|Frozen]]''.\n\n== The algorithm ==\nAn MPM simulation consists of the following stages:\n\n''(Prior to the time integration phase)''\n# '''<u>Initialization of grid and material points.</u>'''\n## A geometry is discretized into a collection of material points, each with its own material properties and initial conditions (velocity, stress, temperature, etc.)\n## The grid, being only used to provide a place for gradient calculations is normally made to cover an area large enough to fill the expected extent of computational domain needed for the simulation.  \n''(During the time integration phase - [[Dynamical system|explicit formulation]])''\n\n''<u>2.</u>'' <u>'''Material point quantities are extrapolated to grid nodes.'''</u>\n# Material point mass (<math display=\"inline\">m_{mp}</math>), momenta (<math display=\"inline\">\\vec{P_{mp}}</math>), stresses (<math>\\boldsymbol{\\bar{\\bar{\\sigma}}}_{mp}</math>), and external forces (<math>\\vec{b}</math>) are extrapolated to the nodes at the corners of the cells within which the material points reside. This is most commonly done using standard linear shape functions (<math display=\"inline\">N_{nd-mp}</math>), the same used in FEM.\n# The grid use the material point values to create the masses (<math display=\"inline\">M_{node}</math>), velocities (<math display=\"inline\">\\vec{V_{node}}</math>), internal and external force vectors (<math display=\"inline\">\\vec{F_{node}^{\\mathsf{internal}}}</math>,<math display=\"inline\">\\vec{F_{node}^{\\mathsf{external}}}</math>) for the nodes: <math display=\"block\">M_{node} = \\sum_{mp} m_{mp} ~~ N_{mp-nd}</math><math display=\"block\">\\vec{V_{node}} = {1 \\over M_{node}} ~~ \\sum_{mp} \\vec{P_{mp}} ~~ N_{mp-nd}</math><math>\\vec{F_{node}^{internal}} = \\sum_{mp} ~~\\bar{\\bar{\\sigma}}_{mp} ~~ \\nabla N_{mp-nd} </math> <math display=\"block\">\\vec{F_{node}^{\\mathsf{external}}} = \\sum_{mp} \\vec{b}~~N_{mp-nd}</math>\n<u>3. '''Equations of motion are solved on the grid.'''</u>\n# Newton's 2nd Law is solved to obtain the nodal acceleration (<math>\\vec{A_{node}}</math>)<math display=\"block\">\\vec{A_{node}} = {\\vec{F^{external}_{node}+\\vec{F^{internal}_{node}} \\over M_{node}}} </math>\n# New nodal velocities are found (<math>\\tilde{\\vec{V_{node}}}</math>).\n\n<math display=\"block\">~~~~~~~~~~~~~~~~~~~~\\tilde{\\vec{V_{node}}} = \\vec{V_{node}} + \\vec{A_{node}}\\mathrm d t</math><u>4. '''Derivative terms are extrapolated back to material points'''</u>\n#Material point acceleration (<math>\\vec{a_{mp}}</math>), deformation gradient (<math>\\mathcal{\\bar{\\bar{F_{mp}}}}</math>) (or strain rate (<math>\\bar{\\bar{\\dot{\\varepsilon}_{mp}}}</math>) depending on the [[Infinitesimal strain theory|strain theory]] used)  is extrapolated from the surrounding nodes using similar shape functions to before (<math>N_{nd-mp}</math>).<math display=\"block\">\\vec{a_{mp}} = \\sum_{nd} \\vec{A_{node}} ~~N_{nd-mp}\n</math><math>\\bar{\\bar{\\dot{\\varepsilon}_{mp}}} = \\sum_{nd} ~{1 \\over 2}~~[\\vec{V_{node}} \\nabla N_{nd-mp} + (V_{node} \\nabla N_{nd-mp})^T ]</math>\n#Variables on the material points: positions, velocities, strains, stresses etc. are then updated with these rates depending on [[Numerical integration|integration scheme]] of choice and a suitable [[Constitutive equation|constitutive model]].\n<u>5.'''Resetting of grid.'''</u>\n#Now that the material points are fully updated at the next time step, the grid is reset to allow for the next time step to begin.\n\n==History of PIC/MPM==\n\nThe PIC was originally conceived to solve problems in fluid dynamics, and developed by [[Francis H. Harlow|Harlow]] at [[Los Alamos National Laboratory]] in 1957.<ref>{{Cite journal|last=Johnson|first=N. L.|date=1996|title=The legacy and future of CFD at Los Alamos|journal=Proceedings of the 1996 Canadian CFD Conference|language=English|volume=|pages=|osti=244662}}</ref> One of the first PIC codes was the Fluid-Implicit Particle (FLIP) program, which was created by Brackbill in 1986<ref>{{Cite journal|last=Brackbill|first=J. U.|last2=Ruppel|first2=H. M.|date=1986|title=FLIP: A method for adaptively zoned, particle-in-cell calculations of fluid flows in two dimensions|url=http://www.sciencedirect.com/science/article/pii/0021999186902111|journal=Journal of Computational Physics|volume=65|issue=2|pages=314–343|doi=10.1016/0021-9991(86)90211-1|issn=0021-9991|via=}}</ref> and has been constantly in development ever since. Until the 1990s, the PIC method was used principally in fluid dynamics.\n\nMotivated by the need for better simulating penetration problems in solid dynamics, Sulsky, Chen and Schreyer started in 1993 to reformulate the PIC and develop the MPM, with funding from Sandia National Laboratories.<ref>{{Cite journal|last=Sulsky|first=D.|last2=Chen|first2=Z.|last3=Schreyer|first3=H. L.|date=1994|title=A particle method for history-dependent materials|url=http://www.sciencedirect.com/science/article/pii/0045782594901120|journal=Computer Methods in Applied Mechanics and Engineering|volume=118|issue=1|pages=179–196|doi=10.1016/0045-7825(94)90112-0|issn=0045-7825|via=}}</ref> The original MPM was then further extended by Bardenhagen ''et al.''. to include frictional contact,<ref>{{Cite journal|last=Bardenhagen|first=S. G.|last2=Brackbill|first2=J. U.|last3=Sulsky|first3=D. L.|date=1998|title=Shear deformation in granular materials|journal=|language=English|volume=|pages=|osti=329539|doi=10.2172/329539|url=https://digital.library.unt.edu/ark:/67531/metadc688688/}}</ref> which enabled the simulation of granular flow,<ref>{{Cite journal|last=Więckowski|first=Zdzisław|last2=Youn|first2=Sung-Kie|last3=Yeon|first3=Jeoung-Heum|date=1999|title=A particle-in-cell solution to the silo discharging problem|journal=International Journal for Numerical Methods in Engineering|language=en|volume=45|issue=9|pages=1203–1225|doi=10.1002/(SICI)1097-0207(19990730)45:9<1203::AID-NME626>3.0.CO;2-C|issn=1097-0207|bibcode=1999IJNME..45.1203W}}</ref> and by Nairn to include explicit cracks<ref>{{Cite journal|last=Nairn|first=J. A.|date=2003|title=Material Point Method Calculations with Explicit Cracks|journal=CMES: Computer Modeling in Engineering & Sciences|volume=4|issue=6|pages=649–664|doi=10.3970/cmes.2003.004.649}}</ref> and crack propagation (known as CRAMP).\n\nRecently, an MPM implementation based on a micro-polar Cosserat continuum<ref>{{Cite thesis|title=The modelling of granular flow using the particle-in-cell method|url=https://scholar.sun.ac.za:443/handle/10019.1/1334|publisher=Stellenbosch : University of Stellenbosch|date=2004|degree=PhD|language=en|first=Corne J.|last=Coetzee|doi=}}</ref> has been used to simulate high-shear granular flow, such as silo discharge. MPM's uses were further extended into [[Geotechnical engineering]] with the recent development of a quasi-static, implicit MPM solver which provides numerically stable analyses of large-deformation problems in [[Soil mechanics]].<ref name=\"Beuth\">Beuth, L., Coetzee, C.J., Bonnier, P. and van den Berg, P. \"Formulation and validation of a quasi-static material point method.\" In 10th International Symposium on Numerical Methods in Geomechanics, 2007.</ref>\n\nAnnual workshops on the use of MPM are held at various locations in the United States. The Fifth MPM Workshop was held at [[Oregon State University]], in [[Corvallis, OR]], on April 2 and 3, 2009.\n\n==Applications of PIC/MPM==\n\nThe uses of the PIC or MPM method can be divided into two broad categories: firstly, there are many applications involving fluid dynamics, plasma physics, magnetohydrodynamics, and multiphase applications. The second category of applications comprises problems in solid mechanics.\n\n===Fluid dynamics and multiphase simulations===\nThe PIC method has been used to simulate a wide range of fluid-solid interactions, including sea ice dynamics,<ref>{{Cite journal|last=Wang|first=R.-X|last2=Ji|first2=S.-Y.|last3=Shen|first3=Hung Tao|last4=Yue|first4=Q.-J.|date=2005|title=Modified PIC method for sea ice dynamics|url=https://www.researchgate.net/publication/297310679|journal=China Ocean Engineering|volume=19|pages=457–468|via=ResearchGate}}</ref> penetration of biological soft tissues,<ref name=\"Ionescu\">Ionescu, I., Guilkey, J., Berzins, M., Kirby, R., and Weiss, J. \"Computational simulation of penetrating trauma in biological soft tissues using MPM.\"</ref> fragmentation of gas-filled canisters,<ref>{{Cite web|url=https://www.researchgate.net/publication/51988901|title=Material point method simulations of fragmenting cylinders|last=Banerjee|first=Biswajit|date=2012|website=ResearchGate|language=en|arxiv=1201.2439|archive-url=|archive-date=|dead-url=|access-date=2019-06-18}}</ref> dispersion of atmospheric pollutants,<ref>{{Cite journal|last=Patankar|first=N. A.|last2=Joseph|first2=D. D.|date=2001|title=Lagrangian numerical simulation of particulate flows|journal=International Journal of Multiphase Flow|volume=27|issue=10|pages=1685–1706|doi=10.1016/S0301-9322(01)00025-8|issn=0301-9322}}</ref> multiscale simulations coupling molecular dynamics with MPM,<ref>{{Cite journal|last=Lu|first=H.|last2=Daphalapurkar|first2=N. P.|last3=Wang|first3=B.|last4=Roy|first4=S.|last5=Komanduri|first5=R.|date=2006|title=Multiscale simulation from atomistic to continuum – coupling molecular dynamics (MD) with the material point method (MPM)|journal=Philosophical Magazine|volume=86|issue=20|pages=2971–2994|doi=10.1080/14786430600625578|issn=1478-6435}}</ref><ref name=\"Ma\">{{Cite thesis|last=Ma|first=Jin|title=Multiscale Simulation Using the Generalized Interpolation Material Point Method, Discrete Dislocations and Molecular Dynamics|date=2006|degree=PhD|publisher=Oklahoma State University|url=https://shareok.org/handle/11244/7807|doi=}}</ref> and fluid-membrane interactions.<ref>{{Cite journal|last=York|first=Allen R.|last2=Sulsky|first2=Deborah|last3=Schreyer|first3=Howard L.|date=2000|title=Fluid–membrane interaction based on the material point method|journal=International Journal for Numerical Methods in Engineering|language=en|volume=48|issue=6|pages=901–924|doi=10.1002/(SICI)1097-0207(20000630)48:6<901::AID-NME910>3.0.CO;2-T|issn=1097-0207|bibcode=2000IJNME..48..901Y}}</ref> In addition, the PIC-based FLIP code has been applied in magnetohydrodynamics and plasma processing tools, and simulations in astrophysics and free-surface flow.<ref>{{Cite journal|last=Liu|first=Wing Kam|last2=Li|first2=Shaofan|date=2002|title=Meshfree and particle methods and their applications|journal=Applied Mechanics Reviews|volume=55|issue=1|pages=1–34|doi=10.1115/1.1431547|issn=0003-6900}}</ref>\n\nAs a result of a joint effort between UCLA's mathematics department and [[Walt Disney Animation Studios]], MPM was successfully used to simulate [[snow]] in the 2013 computer-animated film ''[[Frozen (2013 film)|Frozen]]''.<ref name=\"ucla\">{{cite news|last=Marquez |first=Letisia |title=UCLA's mathematicians bring snow to life for Disney's \"Frozen\" |url=http://today.ucla.edu/portal/ut/math-wizards-create-snow-for-disney-263913.aspx |accessdate=6 March 2014 |newspaper=UCLA Today |date=February 27, 2014 |deadurl=yes |archiveurl=https://web.archive.org/web/20140310163207/http://today.ucla.edu/portal/ut/math-wizards-create-snow-for-disney-263913.aspx |archivedate=10 March 2014 |df= }}</ref><ref>{{cite web|title=A material point method for snow simulation |url=https://disney-animation.s3.amazonaws.com/uploads/production/publication_asset/72/asset/snow.pdf |publisher=[[Walt Disney Animation Studios]] |accessdate=6 March 2014 |author1=Alexey Stomakhin |author2=Craig Schroeder |author3=Lawrence Chai |author4=Joseph Teran |author5=Andrew Selle |date=August 2013 |deadurl=yes |archiveurl=https://web.archive.org/web/20140401000000/https://disney-animation.s3.amazonaws.com/uploads/production/publication_asset/72/asset/snow.pdf |archivedate=1 April 2014 |df= }}</ref><ref name=\"cgmeetup\">{{cite news|url=http://www.cgmeetup.net/home/making-of-disneys-frozen-snow-simulation/|title=Making of Disney's Frozen: A Material Point Method For Snow Simulation|last=|first=|date=November 21, 2013|newspaper=CG Meetup|accessdate=18 January 2014}}</ref>\n\n===Solid mechanics===\nMPM has also been used extensively in solid mechanics, to simulate impact, penetration, collision and rebound, as well as crack propagation.<ref name=\"Karuppiah\">{{Cite thesis|last=Karuppiah|first=Venkatesh|title=Implementation of irregular mesh in MPM for simulation of mixed mode crack opening in tension|date=2004|degree=Master's|publisher=Oklahoma State University|url=https://shareok.org/handle/11244/9959|doi=}}</ref><ref>{{Cite journal|last=Daphalapurkar|first=Nitin P.|last2=Lu|first2=Hongbing|last3=Coker|first3=Demir|last4=Komanduri|first4=Ranga|date=2007-01-01|title=Simulation of dynamic crack growth using the generalized interpolation material point (GIMP) method|journal=International Journal of Fracture|language=en|volume=143|issue=1|pages=79–102|doi=10.1007/s10704-007-9051-z|issn=1573-2673}}</ref> MPM has also become a widely used method within the field of soil mechanics: it has been used to simulate granular flow, silo discharge, pile driving, bucket filling, and material failure; and to model soil stress distribution, compaction, and hardening. It is now being used in wood mechanics problems such as simulations of transverse compression on the cellular level including cell wall contact.<ref>{{Cite journal|last=Nairn|first=John A.|date=2007|title=Numerical Simulations of Transverse Compression and Densification in Wood|url=https://wfs.swst.org/index.php/wfs/article/view/2|journal=Wood and Fiber Science|language=en-US|volume=38|issue=4|pages=576–591|issn=0735-6161|via=}}</ref> The work also received the George Marra Award for paper of the year from the Society of Wood Science and Technology.<ref>{{Cite web|url=https://web.archive.org/web/20070923193216/http://www.swst.org/marrarecip.html|title=Society of Wood Science and Technology: George Marra Award Recipients|last=|first=|date=2007|website=web.archive.org|archive-url=|archive-date=|dead-url=|access-date=2019-06-18}}</ref>\n\n==Classification of PIC/MPM codes==\n\n===MPM in the context of numerical methods===\n\nOne subset of numerical methods are [[Meshfree methods]], which are defined as methods for which \"a predefined mesh is not necessary, at least in field variable interpolation\". Ideally, a meshfree method does not make use of a mesh \"throughout the process of solving the problem governed by partial differential equations, on a given arbitrary domain, subject to all kinds of boundary conditions,\" although existing methods are not ideal and fail in at least one of these respects. Meshless methods, which are also sometimes called particle methods, share a \"common feature that the history of state variables is traced at points (particles) which are not connected with any element mesh, the distortion of which is a source of numerical difficulties.\" As can be seen by these varying interpretations, some scientists consider MPM to be a meshless method, while others do not. All agree, however, that MPM is a particle method.\n\nThe [[Arbitrary Lagrangian Eulerian]] (ALE) methods form another subset of numerical methods which includes MPM. Purely [[Lagrangian and Eulerian coordinates|''Lagrangian'']] methods employ a framework in which a space is discretised into initial subvolumes, whose flowpaths are then charted over time. Purely [[Lagrangian and Eulerian coordinates|''Eulerian'']] methods, on the other hand, employ a framework in which the motion of material is described relative to a mesh that remains fixed in space throughout the calculation. As the name indicates, ALE methods combine Lagrangian and Eulerian frames of reference.\n\n===Subclassification of MPM/PIC===\nPIC methods may be based on either the strong form collocation or a weak form discretisation of the underlying [[partial differential equation]] (PDE). Those based on the strong form are properly referred to as finite-volume PIC methods. Those based on the weak form discretisation of PDEs may be called either PIC or MPM.\n\nMPM solvers can model problems in one, two, or three spatial dimensions, and can also model [[axisymmetric]] problems. MPM can be implemented to solve either quasi-static or dynamic [[equations of motion]], depending on the type of problem that is to be modeled.\n\nThe time-integration used for MPM may be either [[explicit and implicit methods|''explicit'']] or [[explicit and implicit methods|''implicit'']]. The advantage to implicit integration is guaranteed stability, even for large timesteps. On the other hand, explicit integration runs much faster and is easier to implement.\n\n==Advantages==\n\n===Compared to FEM===\nUnlike [[finite element method|''FEM'']], MPM does not require periodical remeshing steps and remapping of state variables, and is therefore better suited to the modeling of large material deformations. In MPM, particles and not the mesh points store all the information on the state of the calculation. Therefore, no numerical error results from the mesh returning to its original position after each calculation cycle, and no remeshing algorithm is required.\n\nThe particle basis of MPM allows it to treat crack propagation and other discontinuities better than FEM, which is known to impose the mesh orientation on crack propagation in a material. Also, particle methods are better at handling history-dependent constitutive models.\n\n===Compared to pure particle methods===\nBecause in MPM nodes remain fixed on a regular grid, the calculation of gradients is trivial.\n\nIn simulations with two or more phases it is rather easy to detect contact between entities, as particles can interact via the grid with other particles in the same body, with other solid bodies, and with fluids.\n\n==Disadvantages of MPM==\n\nMPM is more expensive in terms of storage than other methods, as MPM makes use of mesh as well as particle data. MPM is more computationally expensive than FEM, as the grid must be reset at the end of each MPM calculation step and reinitialised at the beginning of the following step. Spurious oscillation may occur as particles cross the boundaries of the mesh in MPM, although this effect can be minimized by using generalized interpolation methods (GIMP). In MPM as in FEM, the size and orientation of the mesh can impact the results of a calculation: for example, in MPM, strain localisation is known to be particularly sensitive to mesh refinement.\n\n==Commercial packages==\nA commercial package based on a meshless method is [http://www.mpmsim.com/ MPMsim].\n\n==Notes==\n{{Reflist|30em}}\n\n== External links ==\n* [http://www.csafe.utah.edu/ Center for Simulation of Accidental Fires and Explosions – MPM code available]\n* [https://code.google.com/p/nairn-mpm-fea/ NairnMPM – open source]\n* [http://comdyn.hy.tsinghua.edu.cn/english/mpm3d MPM3D - open source (MPM3D-F90) and free trial version (MPM3D)]\n* [http://taichi.graphics/ Taichi - Physically Based Computer Graphics Library – open source MPM code available]\n\n{{Numerical PDE}}\n\n[[Category:Numerical analysis]]\n[[Category:Numerical differential equations]]\n[[Category:Computational fluid dynamics]]\n[[Category:Computational mathematics]]\n[[Category:Simulation]]"
    },
    {
      "title": "Matrix analysis",
      "url": "https://en.wikipedia.org/wiki/Matrix_analysis",
      "text": "In [[mathematics]], particularly in [[linear algebra]] and applications, '''matrix analysis''' is the study of [[matrix (mathematics)|matrices]] and their algebraic properties.<ref>{{cite book|title=Matrix Analysis|author=R. A. Horn, C. R. Johnson|year=2012|publisher=Cambridge University Press|isbn=052-183-940-8|edition=2nd|url=https://books.google.com/books?id=5I5AYeeh0JUC&printsec=frontcover&dq=matrix+analysis&hl=en&sa=X&ei=bC91Ut2rCPKO7Qbh8IBI&redir_esc=y#v=onepage&q=matrix%20analysis&f=false}}\n</ref> Some particular topics out of many include; operations defined on matrices (such as [[matrix addition]], [[matrix multiplication]] and operations derived from these), functions of matrices (such as [[matrix exponentiation]] and [[matrix logarithm]], and even [[sine]]s and cosines etc. of matrices), and the [[eigenvalue]]s of matrices ([[eigendecomposition of a matrix]], [[eigenvalue perturbation]] theory).<ref>{{cite book|title=Functions of Matrices: Theory and Computation|author=N. J. Higham|year=2000 |publisher=SIAM|isbn=089-871-777-9|url=https://books.google.com/books?id=S6gpNn1JmbgC&printsec=frontcover&dq=matrix+functions&hl=en&sa=X&ei=_x1-UqDLE4qV7Qa5s4DAAg&redir_esc=y#v=onepage&q=matrix%20functions&f=false}}\n</ref>\n\n==Matrix spaces==\n\nThe set of all ''m''×''n'' matrices over a number [[field (mathematics)|field]] ''F'' denoted in this article ''M''<sub>''mn''</sub>(''F'') form a [[vector space]]. Examples of ''F'' include the set of [[integer]]s ℤ, the [[real number]]s ℝ, and set of [[complex number]]s ℂ. The spaces ''M''<sub>''mn''</sub>(''F'') and ''M''<sub>''pq''</sub>(''F'') are different spaces if ''m'' and ''p'' are unequal, and if ''n'' and ''q'' are unequal; for instance ''M''<sub>32</sub>(''F'') ≠ ''M''<sub>23</sub>(''F''). Two ''m''×''n'' matrices '''A''' and '''B''' in ''M''<sub>''mn''</sub>(''F'') can be added together to form another matrix in the space ''M''<sub>''mn''</sub>(''F''):\n\n:<math>\\mathbf{A},\\mathbf{B} \\in M_{mn}(F)\\,,\\quad \\mathbf{A} + \\mathbf{B} \\in M_{mn}(F) </math>\n\nand multiplied by a ''α'' in ''F'', to obtain another matrix in ''M''<sub>''mn''</sub>(''F''):\n\n:<math>\\alpha \\in F \\,,\\quad \\alpha \\mathbf{A} \\in M_{mn}(F) </math>\n\nCombining these two properties, a [[linear combination]] of matrices '''A''' and '''B''' are in ''M''<sub>''mn''</sub>(''F'') is another matrix in ''M''<sub>''mn''</sub>(''F''):\n\n:<math>\\alpha \\mathbf{A} + \\beta\\mathbf{B} \\in M_{mn}(F) </math>\n\nwhere ''α'' and ''β'' are numbers in ''F''.\n\nAny matrix can be expressed as a linear combination of basis matrices, which play the role of the [[basis vector]]s for the matrix space. For example, for the set of 2×2 matrices over the field of real numbers, ''M''<sub>22</sub>(ℝ), one legitimate basis set of matrices is:\n\n:<math>\\begin{pmatrix}1&0\\\\0&0\\end{pmatrix}\\,,\\quad\n\\begin{pmatrix}0&1\\\\0&0\\end{pmatrix}\\,,\\quad\n\\begin{pmatrix}0&0\\\\1&0\\end{pmatrix}\\,,\\quad\n\\begin{pmatrix}0&0\\\\0&1\\end{pmatrix}\\,,</math>\n\nbecause any 2×2 matrix can be expressed as:\n\n:<math>\\begin{pmatrix}a&b\\\\c&d\\end{pmatrix}=a \\begin{pmatrix}1&0\\\\0&0\\end{pmatrix}\n+b\\begin{pmatrix}0&1\\\\0&0\\end{pmatrix}\n+c\\begin{pmatrix}0&0\\\\1&0\\end{pmatrix}\n+d\\begin{pmatrix}0&0\\\\0&1\\end{pmatrix}\\,,</math>\n\nwhere ''a'', ''b'', ''c'',''d'' are all real numbers. This idea applies to other fields and matrices of higher dimensions.\n\n==Determinants==\n\n{{main|Determinant}}\n\nThe '''determinant''' of a square matrix is an important property. The determinant indicates if a matrix is [[invertible]] (i.e. the [[inverse matrix|inverse of a matrix]] exists when the determinant is nonzero). Determinants are used for finding eigenvalues of matrices (see below), and for solving a [[system of linear equations]] (see [[Cramer's rule]]).\n\n==Eigenvalues and eigenvectors of matrices==\n\n{{main|Eigenvalues and eigenvectors}}\n\n===Definitions===\n\nAn ''n''×''n'' matrix '''A''' has '''eigenvectors''' '''x''' and '''eigenvalues''' ''λ'' defined by the relation:\n\n:<math>\\mathbf{A}\\mathbf{x} = \\lambda \\mathbf{x}</math>\n\nIn words, the [[matrix multiplication]] of '''A''' followed by an eigenvector '''x''' (here an ''n''-dimensional [[column matrix]]), is the same as multiplying the eigenvector by the eigenvalue. For an ''n''×''n'' matrix, there are ''n'' eigenvalues. The eigenvalues are the roots of the [[characteristic polynomial]]:\n\n:<math>p_\\mathbf{A}(\\lambda) = \\det(\\mathbf{A} - \\lambda \\mathbf{I}) = 0</math>\n\nwhere '''I''' is the ''n''×''n'' [[identity matrix]].\n\n[[Properties of polynomial roots|Roots of polynomial]]s, in this context the eigenvalues, can all be different, or some may be equal (in which case eigenvalue has [[Multiplicity (mathematics)#Multiplicity of a root of a polynomial|multiplicity]], the number of times an eigenvalue occurs). After solving for the eigenvalues, the eigenvectors corresponding to the eigenvalues can be found by the defining equation.\n\n===Perturbations of eigenvalues===\n\n{{main|Eigenvalue perturbation}}\n\n==Matrix similarity==\n\n{{main|Matrix similarity|Change of basis}}\n\nTwo ''n''×''n'' matrices '''A''' and '''B''' are similar if they are related by a '''similarity transformation''':\n\n:<math>\\mathbf{B} = \\mathbf{P}\\mathbf{A}\\mathbf{P}^{-1}</math>\n\nThe matrix '''P''' is called a '''similarity matrix''', and is necessarily [[matrix inverse|invertible]].\n\n===Unitary similarity===\n\n{{main|Unitary matrix}}\n\n==Canonical forms==\n\n{{other uses|Canonical form}}\n\n===Row echelon form===\n\n{{main|Row echelon form}}\n\n===Jordan normal form===\n\n{{main|Jordan normal form}}\n\n===Weyr canonical form===\n\n{{main|Weyr canonical form}}\n\n===Frobenius normal form===\n\n{{main|Frobenius normal form}}\n\n==Triangular factorization==\n\n===LU decomposition===\n\n{{main|LU decomposition}}\n\n'''LU decomposition''' splits a matrix into a matrix product of an upper [[triangular matrix]] and a lower triangle matrix.\n\n==Matrix norms==\n\n{{Main|Matrix norm}}\n\nSince matrices form vector spaces, one can form axioms (analogous to those of vectors) to define a \"size\" of a particular matrix. The norm of a matrix is a positive real number.\n\n===Definition and axioms===\n\nFor all matrices '''A''' and '''B''' in ''M''<sub>''mn''</sub>(''F''), and all numbers ''α'' in ''F'', a matrix norm, delimited by double vertical bars || ... ||, fulfills:<ref group=\"note\">Some authors, e.g. Horn and Johnson, use triple vertical bars instead of double: |||'''A'''|||.</ref>\n\n*[[Nonnegative]]:\n::<math>\\| \\mathbf{A} \\| \\ge 0</math>\n:with equality only for '''A''' = '''0''', the [[zero matrix]].\n*[[Scalar multiplication]]:\n::<math>\\|\\alpha \\mathbf{A}\\|=|\\alpha| \\|\\mathbf{A}\\|</math>\n*The [[triangular inequality]]:\n::<math>\\|\\mathbf{A}+\\mathbf{B}\\| \\leq \\|\\mathbf{A}\\|+\\|\\mathbf{B}\\|</math>\n\n===Frobenius norm===\n\nThe '''Frobenius norm''' is analogous to the [[dot product]] of Euclidean vectors; multiply matrix elements entry-wise, add up the results, then take the positive square root:\n\n:<math>\\|\\mathbf{A}\\| = \\sqrt{\\mathbf{A}:\\mathbf{A}} = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n (A_{ij})^2}</math>\n\nIt is defined for matrices of any dimension (i.e. no restriction to square matrices).\n\n==Positive definite and semidefinite matrices==\n\n{{main|Positive definite matrix}}\n\n==Functions==\n\n{{main|Function (mathematics)}}\n\nMatrix elements are not restricted to constant numbers, they can be [[mathematical variable]]s.\n\n===Functions of matrices===\n\nA functions of a matrix takes in a matrix, and return something else (a number, vector, matrix, etc...).\n\n===Matrix-valued functions===\n\nA matrix valued function takes in something (a number, vector, matrix, etc...) and returns a matrix.\n\n==See also==\n\n<!---rather than simply deleting, please include these in the article somewhere wherever relevant after the real content is written--->\n\n===Other branches of analysis===\n\n*[[Mathematical analysis]]\n*[[Tensor analysis]]\n*[[Matrix calculus]]\n*[[Numerical analysis]]\n\n===Other concepts of linear algebra===\n\n*[[Tensor product]]\n*[[Spectrum of an operator]]\n*[[Matrix geometrical series]]\n\n===Types of matrix===\n\n*[[Orthogonal matrix]], [[unitary matrix]]\n*[[Symmetric matrix]], [[antisymmetric matrix]]\n*[[Stochastic matrix]]\n\n===Matrix functions===\n\n*[[Matrix polynomial]]\n*[[Matrix exponential]]\n\n==Footnotes==\n\n{{Reflist|group=\"note\"|1}}\n\n==References==\n\n===Notes===\n\n{{reflist}}\n\n===Further reading===\n\n*{{cite book|title=Matrix Analysis and Applied Linear Algebra Book and Solutions Manual|author=C. Meyer|year=2000 |publisher=SIAM|isbn=089-871-454-0|volume=2|series=Matrix Analysis and Applied Linear Algebra|url=https://books.google.com/books?id=Zg4M0iFlbGcC&printsec=frontcover&dq=Matrix+Analysis&hl=en&sa=X&ei=SCd1UryWD_LG7Aag_4HwBg&ved=0CGoQ6AEwCQ#v=onepage&q=Matrix%20Analysis&f=false}}\n*{{cite book|title=Applied Linear Algebra and Matrix Analysis|author=T. S. Shores|year=2007|publisher=Springer|isbn=038-733-195-6|series=[[Undergraduate Texts in Mathematics]]|url=https://books.google.com/books?id=8qwTb9P-iW8C&printsec=frontcover&dq=Matrix+Analysis&hl=en&sa=X&ei=SCd1UryWD_LG7Aag_4HwBg&ved=0CGQQ6AEwCA#v=onepage&q=Matrix%20Analysis&f=false}}\n*{{cite book|title=Matrix Analysis|author=Rajendra Bhatia|year=1997|volume=169|series=Matrix Analysis Series|publisher=Springer|isbn=038-794-846-5|url=https://books.google.com/books?id=F4hRy1F1M6QC&printsec=frontcover&dq=matrix+analysis&hl=en&sa=X&ei=_SR1UpbnNarA7AaPjIHIDA&redir_esc=y#v=onepage&q=matrix%20analysis&f=false}}\n*{{cite book|title=Computational Matrix Analysis|author=Alan J. Laub|year=2012|publisher=SIAM|isbn=161-197-221-3|url=https://books.google.com/books?id=RJBZBuHpVjEC&printsec=frontcover&dq=Matrix+Analysis&hl=en&sa=X&ei=Iyl1UtCuEIbm7Abc4YHoCg&ved=0CDAQ6AEwADgK#v=onepage&q=Matrix%20Analysis&f=false}}\n\n[[Category:Linear algebra]]\n[[Category:Matrices]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Mesh generation",
      "url": "https://en.wikipedia.org/wiki/Mesh_generation",
      "text": "{{TOC right}}\n'''Mesh generation''' is the practice of creating a [[polygon mesh|mesh]], a subdivision of a continuous geometric space into discrete geometric and topological cells.\nOften these cells form a [[simplicial complex]].\nUsually the cells partition the geometric input domain.\nMesh cells are used as discrete local approximations of the larger domain. Meshes are created by computer algorithms, often with human guidance through a [[GUI]] , depending on the complexity of the domain and the type of mesh desired.\nThe goal is to create a mesh that accurately captures the input domain geometry, with high-quality (well-shaped) cells, and without so many cells as to make subsequent calculations intractable.\nThe mesh should also be fine (have small elements) in areas that are important for the subsequent calculations.\n\nMeshes are used for rendering to a [[computer screen]] and for [[physical simulation]] such as [[finite element analysis]] or [[computational fluid dynamics]]. Meshes are composed of simple cells like triangles because, e.g., we know how to perform operations such as finite element calculations (engineering) or ray tracing (computer graphics) on triangles, but we do not know how to perform these operations directly on complicated spaces and shapes such as a roadway bridge. We can simulate the strength of the bridge, or draw it on a computer screen, by performing calculations on each triangle and calculating the interactions between triangles.\n\nA major distinction is between structured and unstructured meshing. In structured meshing the mesh is a regular lattice, such as an array, with implied connectivity between elements. In unstructured meshing, elements may be connected to each other in irregular patterns, and more complicated domains can be captured. This page is primarily about unstructured meshes.\nWhile a mesh may be a [[Triangulation_(geometry)|triangulation]], the process of meshing is distinguished from [[point set triangulation]] in that meshing includes the freedom to add vertices not present in the input. \"Facetting\" (triangulating) [[Computer-aided design|CAD]] models for drafting has the same freedom to add vertices, but the goal is to represent the shape accurately using as few triangles as possible and the shape of individual triangles is not important. Computer graphics renderings of textures and realistic lighting conditions use meshes instead.\n\nMany mesh generation software is coupled to a [[Computer-aided_design|CAD system]] defining its input, and simulation software for taking its output. The input can vary greatly but common forms are [[Solid modeling]], [[Geometric modeling]], [[NURBS]], [[B-rep]], [[STL (file format)|STL]] or a [[point cloud]]. \n\n=== Terminology ===\n\nThe terms \"'''mesh generation,'''\"  \"'''grid generation,'''\"  \"'''meshing,'''\" \" and \"'''gridding,'''\" are often used interchangeably, although strictly speaking the latter two are broader and encompass mesh improvement: changing the mesh with the goal of increasing the speed or accuracy of the numerical calculations that will be performed over it. In [[computer graphics]] rendering, and [[mathematics]], a mesh is sometimes referred to as a '''[[Tessellation_(computer_graphics)|tessellation]].'''\n\nMesh faces (cells, entities) have different names depending on their dimension and the context in which the mesh will be used. In finite elements, the highest-dimensional mesh entities are called \"elements,\" \"edges\" are 1D and \"nodes\" are 0D. If the elements are 3D, then the 2D entities are \"faces.\" In computational geometry, the 0D points are called vertices. Tetrahedra are often abbreviated as \"tets\"; triangles are \"tris\", quadrilaterals are \"quads\" and hexahedra (topological cubes) are \"hexes.\"\n\n=== Techniques ===\n\nMany meshing techniques are built on the principles of the [[Delaunay triangulation]], together with rules for adding vertices, such as [[Ruppert's algorithm]].\nA distinguishing feature is that an initial coarse mesh of the entire space is formed, then vertices and triangles are added.\nIn contrast, [[advancing front algorithm]]s start from the domain boundary, and add elements incrementally filling up the interior. \nHybrid techniques do both. A special class of advancing front techniques creates thin [[boundary layer]]s of elements for fluid flow.\nIn structured mesh generation the entire mesh is a [[lattice graph]], such as a regular grid of squares. [[Principles_of_grid_generation|Structured mesh generation]] for [[regular grid]]s and is an entire field itself, with mathematical techniques applied to ensure high-polynomial-order grid lines follow the solution space smoothly and accurately.\nIn block-structured meshing, the domain is divided into large subregions, each of which is a structured mesh.\nSome direct methods start with a block-structured mesh and then move the mesh to conform to the input; see [https://www.cs.ubc.ca/~sheffa/hexing/hexing.htm Automatic Hex-Mesh Generation] based on [[polycube]]. Another direct method is to cut the structured cells by the domain boundary; see [https://cubit.sandia.gov/public/15.1/help_manual/WebHelp/mesh_generation/meshing_schemes/parallel/sculpt.htm sculpt] based on [[Marching cubes]].\n\nSome types of meshes are much more difficult to create than others. Simplicial meshes tend to be easier than cubical meshes. An important category is generating a hex mesh conforming to a fixed quad surface mesh; a research subarea is studying the existence and generation of meshes of specific small configurations, such as the [[tetragonal trapezohedron]]. Because of the difficulty of this problem, the existence of combinatorial hex meshes has been studied apart from the problem of generating good geometric realizations. While known algorithms generate simplicial meshes with guaranteed minimum quality, such guarantees are rare for cubical meshes, and many popular implementations generate inverted (inside-out) hexes from some inputs.\n\nMeshes are often created in serial on workstations, even when subsequent calculations over the mesh will be done in [[Parallel_computing|parallel]] on super-computers. This is both because of the limitation that most mesh generators are interactive, and because mesh generation runtime is typically insignificant compared to solver time. However, if the mesh is too large to fit in the memory of a single serial machine, or the mesh must be changed (adapted) during the simulation, meshing is done in parallel.\n\n=== Types of Meshes ===\n\nSee also [[Types of mesh]].\n\n==== Cell Topology ====\nUsually the cells are [[polygon]]al or [[polyhedron|polyhedral]] and form a [[polygon mesh|mesh]] that partitions the domain.\nImportant classes of two-dimensional elements include triangles (simplices) and quadrilaterals (topological squares).\nIn three-dimensions the most-common cells are tetrahedra (simplices) and hexahedra (topological cubes).\n[[Simplex|Simplicial]] meshes may be of any dimension and include triangles (2D) and tetrahedra (3D) as important instances.\n''Cubical meshes'' is the pan-dimensional category that includes quads (2D) and hexes (3D). In 3D, 4-sided pyramids and 3-sided prisms appear in conformal meshes of mixed cell type.\n\n==== Cell Dimension ====\nThe mesh is embedded in a geometric space that is typically [[two-dimensional space|two]] or [[Three-dimensional space|three dimension]]al, although sometimes the dimension is increased by one by adding the time-dimension. Higher dimensional meshes are used in niche contexts. One-dimensional meshes are useful as well. A significant category is surface meshes, which are 2D meshes embedded in 3D to represent a curved surface. \n\n==== Duality ====\n[[Dual graph]]s have several roles in meshing. One can make a polyhedral [[Voronoi diagram]] mesh by dualizing a [[Delaunay triangulation]] simplicial mesh. One can create a cubical mesh by generating an arrangement of surfaces and dualizing the intersection graph; see [[spatial twist continuum]]. Sometimes both the primal mesh and its dual mesh are used in the same simulation; see [[Hodge star operator]]. This arises from physics involving [[divergence]] and [[curl (mathematics)]] operators, such as [[flux]] & [[vorticity]] or [[Electromagnetism| electricity & magnetism]], where one variable naturally lives on the primal faces and its counterpart on the dual faces. \n\n==== Mesh Type by Use ====\nThree-dimensional meshes created for [[finite element analysis]] need to consist of [[tetrahedron|tetrahedra]], [[pyramid (geometry)|pyramid]]s, [[prism (geometry)|prism]]s or [[hexahedron|hexahedra]]. Those used for the [[finite volume method]] can consist of arbitrary [[polyhedron|polyhedra]]. Those used for [[finite difference method]]s consist of piecewise structured arrays of [[hexahedra]] known as multi-block structured meshes. \n4-sided pyramids are useful to conformally connect hexes to tets. 3-sided prisms are used for boundary layers conforming to a tet mesh of the far-interior of the object.\n\nSurface meshes are useful in computer graphics where the surfaces of objects reflect light (also [[subsurface scattering]]) and a full 3D mesh is not needed. Surface meshes are also used to model thin objects such as sheet metal in auto manufacturing and building exteriors in architecture. High (e.g., 17) dimensional cubical meshes are common in astrophysics and [[string theory]].\n\n==== Mathematical Definition and Variants ====\nWhat is the precise definition of a ''mesh?'' There is not a universally-accepted mathematical description that applies in all contexts. \nHowever, some mathematical objects are clearly meshes: a [[simplicial complex]] is a mesh composed of simplices.\nMost polyhedral (e.g. cubical) meshes are ''conformal,'' meaning they have the cell structure of a [[CW complex]], a generalization of a [[simplicial complex]]. A mesh need not be simplicial because an arbitrary subset of nodes of a cell is not necessarily a cell: e.g., three nodes of a quad does not define a cell.\nHowever, two cells intersect at cells: e.g. a quad does not have a node in its interior. The intersection of two cells may be several cells: e.g., two quads may share two edges. An intersection being more than one cell is sometimes forbidden and rarely desired; the goal of some mesh improvement techniques (e.g. pillowing) is to remove these configurations. In some contexts, a distinction is made between a topological mesh and a geometric mesh whose embedding satisfies certain quality criteria.\n\nImportant mesh variants that are not CW complexes include non-conformal meshes where cells do not meet strictly face-to-face, but the cells nonetheless partition the domain. An example of this is an [[octree]], where an element face may be partitioned by the faces of adjacent elements. Such meshes are useful for flux-based simulations. In overset grids, there are multiple conformal meshes that overlap geometrically and do not partition the domain; see e.g., [[Overflow_(software)|Overflow, the OVERset grid FLOW solver]]. So-called meshless or [[meshfree methods]] often make use of some mesh-like discretization of the domain, and have basis functions with overlapping support. Sometimes a local mesh is created near each simulation degree-of-freedom point, and these meshes may overlap and be non-conformal to one another.\n\n=== Element Shape and Quality ===\n\n''Continuous.'' For numeric computations over a mesh, there is a mapping from a reference element, an abstract element of perfect shape, into each element of the realized mesh. For linear elements, this map is linear. Ideally, the mapping is bijective. If a mesh element is distorted to the extent that two abstract points map to the same mesh point, the element is said to be inverted. In this case the Jacobian of the map has zeros.\n\n''Discrete.'' Nodes are \"irregular\" if the number of attached edges differs from that of an ideal lattice: 6 for triangles, 4 for quads, and 6 for hexes. Keeping the number of irregular nodes to a minimum is valued in computer graphics meshes of smooth objects. Usually we want the intersection of two cells to be a single cell.\n\n==== High-Order Elements ====\n\nMany meshes use linear elements, where the mapping from the abstract to realized element is linear, and mesh edges are straight segments.\nHigher order polynomial mappings are common, especially quadratic. \nA primary goal for higher-order elements is to more accurately represent the domain boundary, although they have accuracy benefits in the interior of the mesh as well.\nOne of the motivations for cubical meshes is that linear cubical elements have some of the same numerical advantages as quadratic simplicial elements.\nIn the [[isogeometric analysis]] simulation technique, the mesh cells containing the domain boundary use the CAD representation directly instead of a linear or polynomial approximation.\n\n=== Mesh Improvement ===\n\nImproving a mesh involves changing its discrete connectivity, the continuous geometric position of its cells, or both. For discrete changes, for simplicial elements one swaps edges and inserts/removes nodes. The same kinds of operations are done for cubical (quad/hex) meshes, although there are fewer possible operations and local changes have global consequences. E.g., for a hexahedral mesh, merging two nodes creates cells that are not hexes, but if diagonally-opposite nodes on a quadrilateral are merged and this is propagated into collapsing an entire face-connected column of hexes, then all remaining cells will still be hexes. \nIn [[adaptive mesh refinement]], elements are split (h-refinement) in areas where the function being calculated has a high gradient.\nMeshes are also coarsened, removing elements for efficiency. The [[multigrid method]] does something similar to refinement and coarsening to speed up the numerical solve, but without actually changing the mesh.\n\nFor continuous changes, nodes are moved, or the higher-dimensional faces are moved by changing the polynomial order of elements. Moving nodes to improve quality is called \"smoothing\" or \"r-refinement\" and increasing the order of elements is called \"p-refinement.\" Nodes are also moved in simulations where the shape of objects change over time. This degrades the shape of the elements. If the object deforms enough, the entire object is remeshed and the current solution mapped from the old mesh to the new mesh.\n\n=== Practitioners ===\n\nThe field is highly interdisciplinary, with contributions found in [[mathematics]], [[computer science]], and [[engineering]]. Meshing R&D is distinguished by an equal focus on discrete and continuous math and computation, as with [[computational geometry]], but in contrast to [[graph theory]] (discrete) and [[numerical analysis]] (continuous). Mesh generation is deceptively difficult: it is easy for humans to see how to create a mesh of a given object, but difficult to program a computer to make good decisions for arbitrary input a priori. There is an infinite variety of geometry found in nature and man-made objects. Many mesh generation researchers were first users of meshes. Mesh generation continues to receive widespread attention, support and funding because the human-time to create a mesh dwarfs the time to set up and solve the calculation once the mesh is finished. This has always been the situation since numerical simulation and computer graphics were invented, because as computer hardware and simple equation-solving software have improved, people have been drawn to larger and more complex geometric models in a drive for greater fidelity, scientific insight, and artistic expression.\n\n== Community Activities ==\n[http://www.robertschneiders.de/meshgeneration/literature.html Literature on Mesh Generation] list website\n\n=== Journals ===\nMeshing research is published in a broad range of journals. This is in keeping with the interdisciplinary nature of the research required to make progress, and also the wide variety of applications that make use of meshes. About 150 meshing publications appear each year across 20 journals, with at most 20 publications appearing in any one journal. There is no journal whose primary topic is meshing. The journals that publish at least 10 meshing papers per year are in '''bold.'''\n\n* Advances in Engineering Software\n* American Institute of Aeronautics and Astronautics Journal (AIAAJ)\n* Algorithmica\n* Applied Computational Electromagnetics Society Journal\n* Applied Numerical Mathematics\n* Astronomy and Computing\n* [[Computational_Geometry_(journal)|Computational Geometry: Theory and Applications]]\n* [https://www.journals.elsevier.com/computer-aided-design Computer-Aided Design (CAD)]\n* [https://www.journals.elsevier.com/computer-aided-geometric-design '''Computer Aided Geometric Design (CAGD)''']\n* [https://www.eg.org/wp/eurographics-publications/cgf/ '''Computer Graphics Forum (Eurographics)''']\n* [https://www.journals.elsevier.com/computer-methods-in-applied-mechanics-and-engineering '''Computer Methods in Applied Mechanics and Engineering''']\n* Discrete and Computational Geometry\n* [https://link.springer.com/journal/366 '''Engineering with Computers''']\n* Finite Elements in Analysis and Design\n* [https://onlinelibrary.wiley.com/journal/10970207 '''International Journal for Numerical Methods in Engineering (IJNME)''']\n* International Journal for Numerical Methods in Fluids\n* [https://onlinelibrary.wiley.com/journal/20407947 '''International Journal for Numerical Methods in Biomedical Engineering''']\n* International Journal of Computational Geometry & Applications\n* [[Journal_of_Computational_Physics|'''Journal of Computational Physics (JCP)''']]\n* Journal on Numerical Analysis\n* Journal on Scientific Computing (SISC)\n* [[ACM_Transactions_on_Graphics|Transactions on Graphics (ACM TOG)]]\n* [[ACM_Transactions_on_Mathematical_Software|Transactions on Mathematical Software (ACM TOMS)]]\n* [[IEEE_Transactions_on_Visualization_and_Computer_Graphics|Transactions on Visualization and Computer Graphics (IEEE TVCG)]]\n\n=== Conferences ===\nConferences whose primary topic is meshing are in '''bold.'''\n\n[http://www.robertschneiders.de/meshgeneration/conferences.html Conferences, Workshops, Summerschools] list website\n\n* Aerospace Sciences Meeting AIAA\n* Canadian Conference on Computational Geometry CCCG\n* CompIMAGE: International Symposium Computational Modeling of Objects Represented in Images\n* Computational Fluid Dynamics Conference AIAA\n* Computational Fluid Dynamics Conference ECCOMAS \n* Computational Science & Engineering CS&E\n* Conference on Numerical Grid Generation ISGG\n* [https://www.eg.org/wp/eg-events/ Eurographics Annual Conference (Eurographics)] (proceedings in [[Computer Graphics Forum]])\n* Geometric & Physical Modeling SIAM\n* International Conference on Isogeometric Analysis IGA\n* '''[https://www.imr.sandia.gov International Meshing Roundtable IMR]'''\n* [[Symposium_on_Computational_Geometry|International Symposium on Computational Geometry SoCG]]\n* Numerical Geometry, Grid Generation and Scientific Computing\n* [[SIGGRAPH]] (proceedings in [[ACM Transactions on Graphics]])\n* [[Symposium_on_Geometry_Processing|Symposium on Geometry Processing SGP]] ([[Eurographics]]) (proceedings in [[Computer Graphics Forum]])\n* World Congress on Engineering\n\n=== Workshops ===\nWorkshops whose primary topic is meshing are in '''bold.'''\n\n* Conference on Geometry: Theory and Applications CGTA \n* European Workshop on Computational Geometry EuroCG \n* Fall Workshop on Computational Geometry\n* Finite Elements in Fluids FEF\n* '''MeshTrends Symposium''' (in WCCM or USNCCM alternate years)\n* Polytopal Element Methods in Mathematics and Engineering\n* [http://tetrahedron.montefiore.ulg.ac.be/ '''Tetrahedron workshop''']\n\n== Community Resources ==\n\n=== Mesh generators ===\nMany commercial product descriptions emphasize simulation rather than the meshing technology that enables simulation.\n*Lists of mesh generators (external):\n**[http://graphics.tudelft.nl/~matthijss/oss_meshing_software.html Free/open source mesh generators]\n**[http://www.robertschneiders.de/meshgeneration/software.html Public domain and commercial mesh generators]\n* [[ANSYS]]\n* [[CD-adapco]] and Seimens\n* [http://cometsolutions.com/ Comet Solutions]\n* [[CGAL]] Computational Geometry Algorithms Library\n** [http://www.cgal.org/Part/Meshing Mesh generation]\n***[http://www.cgal.org/Pkg/Mesh2 2D Conforming Triangulations and Meshes]\n***[http://www.cgal.org/Pkg/Mesh_3 3D Mesh Generation]\n* [https://cubit.sandia.gov CUBIT]\n* [[Gmsh]]\n* [https://Hextreme.eu Hextreme meshes]\n* [[MSC Software]]\n* [https://github.com/SNLComputation/omega_h Omega_h] Tri/Tet Adaptivity\n* [https://www.openfoam.com/documentation/user-guide/mesh.php Open FOAM] Mesh generation and conversion\n* [[TetGen]]\n* [https://github.com/Yixin-Hu/TetWild TetWild]\n* [https://people.sc.fsu.edu/~jburkardt/c_src/triangle/triangle.html TRIANGLE Mesh generation and Delaunay triangulation]\n\n=== Resources ===\n* [https://blog.pointwise.com/tag/meshtrends/ Another Fine Mesh, MeshTrends Blog, Pointwise]\n* [http://www.robertschneiders.de/meshgeneration/meshgeneration.html Mesh Generation & Grid Generation on the Web]\n* [https://www.linkedin.com/groups/3673020/ Mesh Generation group on LinkedIn]\n\n=== Research Groups & People ===\n*[https://scholar.google.com/citations?view_op=search_authors&hl=en&mauthors=label:mesh_generation Mesh Generation people on Google Scholar]\n\n* [https://www.ics.uci.edu/~eppstein/gina/meshgen.html David Eppstein's Geometry in Action, Mesh Generation]\n* [[Jonathan Shewchuk]]'s [http://people.eecs.berkeley.edu/~jrs/meshf99/ Meshing and Triangulation in Graphics, Engineering, and Modeling]\n* [[Scott A. Mitchell]]\n* [http://www.robertschneiders.de/meshgeneration/ Robert Schneiders]\n\n=== Models and Meshes ===\nUseful models (inputs) and meshes (outputs) for comparing meshing algorithms and meshes.\n\n* [https://www.hexalab.net/ HexaLab] has models and meshes that have been published in research papers, reconstructed or from the original paper.\n* [http://shape.cs.princeton.edu/benchmark/ Princeton Shape Benchmark]\n* [http://www.shrec.net/ Shape Retrieval Contest SHREC] has different models each year, e.g.,\n** [https://www.itl.nist.gov/iad/vug/sharp/contest/2011/NonRigid/ Shape Retrieval Contest of Non-rigid 3D Watertight Meshes 2011]\n* [https://ten-thousand-models.appspot.com/ Thingi10k] meshed models from the [https://www.thingiverse.com/ Thingiverse]\n\n=== [[Computer-aided_design|CAD Models]] ===\nModeling engines linked with mesh generation software to represent the domain geometry.\n\n* [https://www.spatial.com/products/3d-acis-modeling ACIS by Spatial]\n* [https://www.opencascade.com/ Open Cascade]\n\n=== Mesh File Formats ===\nCommon (output) file formats for describing meshes.\n\n* [https://www.unidata.ucar.edu/software/netcdf/ NetCDF]\n* [https://gsjaardema.github.io/seacas/html/index.html Genesis/Exodus]\n* [http://www.xdmf.org/index.php/XDMF_Model_and_Format XDMF]\n* [https://www.vtk.org/VTK/img/file-formats.pdf VTK/VTU]\n* [https://people.sc.fsu.edu/~jburkardt/data/medit/medit.html MEDIT]\n* [https://docs.salome-platform.org/latest/dev/MEDCoupling/developer/med-file.html MED/Salome]\n* [http://gmsh.info/doc/texinfo/gmsh.html#File-formats Gmsh]\n* [http://www.afs.enea.it/fluent/Public/Fluent-Doc/PDF/chp03.pdf ANSYS mesh]\n\n[https://github.com/nschloe/meshio meshio] can convert between all of the above formats.\n\n=== Mesh Visualizers ===\nTools to visualize meshes.\n\n* [https://www.paraview.org/ Paraview]\n\n== See also ==\n*[[Delaunay triangulation]]\n*[[Fortune's algorithm]]\n*[[Grid classification]]\n*[[Mesh parameterization]]\n*[[Meshfree methods]]\n*[[Parallel mesh generation]]\n*[http://femtable.org/ Periodic Table of the Finite Elements]\n*[[Principles of grid generation]]\n*[[Polygon mesh]]\n*[[Regular grid]]\n*[[Ruppert's algorithm]]\n*[[Stretched grid method]]\n*[[Tessellation (computer graphics)|Tessellation]]\n*[[Types of mesh]]\n*[[Unstructured grid]]\n\n==References==\n*{{citation\n | last = Edelsbrunner | first = Herbert | authorlink = Herbert Edelsbrunner\n | isbn = 978-0-521-79309-4\n | publisher = Cambridge University Press\n | title = Geometry and Topology for Mesh Generation\n | year = 2001\n | postscript = <!--none-->}}.\n*{{citation\n | last1 = Frey | first1 = Pascal Jean\n | last2 = George | first2 = Paul-Louis\n | isbn = 978-1-903398-00-5\n | publisher = Hermes Science\n | title = Mesh Generation: Application to Finite Elements\n | year = 2000\n | postscript = <!--none-->}}.\n* {{Citation\n | author = P. Smith and S. S. Sritharan\n | year = 1988\n | title = Theory of Harmonic Grid Generation\n | journal = Complex Variables\n | volume = 10\n |pages= 359–369.\n |url= http://www.nps.edu/Academics/Schools/GSEAS/SRI/R3.pdf\n | postscript = <!--none-->\n | doi=10.1080/17476938808814314\n}}\n* {{Citation\n | doi = 10.1080/00036819208840072\n | author = S. S. Sritharan\n | year = 1992\n | title = Theory of Harmonic Grid Generation-II\n | journal = Applicable Analysis\n | volume = 44\n | issue = 1\n |pages= 127–149.\n | postscript = <!--none-->\n }}\n*{{citation\n | last1 = Thompson | first1 = J. F. |authorlink1 = Joe F. Thompson\n | last2 = Warsi | first2 = Z. U. A.\n | last3 = Mastin | first3 = C. W.\n | publisher = [[North-Holland]], [[Elsevier]]\n | title = Numerical Grid Generation: Foundations and Applications\n | year = 1985\n | postscript = <!--none-->}}.\n* [[CGAL]] The Computational Geometry Algorithms Library\n*{{Citation\n | doi = 10.1016/0045-7825(95)00986-8\n | last1 = Oden | first1 = J.Tinsley\n | last2 = Cho  | first2 = J.R.\n | year = 1996\n | title = Adaptive hpq-Finite Element Methods of Hierarchical Models for Plate- and Shell-like Structures \n | journal = Computer Methods in Applied Mechanics and Engineering\n | volume = 136\n | number = 3\n |pages= 317 - 345.\n | postscript = <!--none-->\n }}\n*{{Citation\n | author = Steven J. Owen\n | title = A Survey of Unstructured Mesh Generation Technology\n | conference = International Meshing Roundtable,\n | year = 1998\n | pages = 239--267\n | url = https://www.semanticscholar.org/paper/A-Survey-of-Unstructured-Mesh-Generation-Technology-Owen/54939736c8bc62e783bf26d9332ff80483de55bb\n }}\n*{{Citation\n | title = Bubble Mesh: Automated Triangular Meshing of Non-Manifold Geometry by Sphere Packing \n | last1 = Shimada | first1 = Kenji\n | last2 = Gossard | first2 = David C.\n | conference = ACM Symposium on Solid Modeling and Applications, SMA\n | year = 1995\n | isbn = 0-89791-672-7\n | pages = 409-419\n | url = http://doi.acm.org/10.1145/218013.218095\n | doi = 10.1145/218013.218095\n | publisher = ACM\n}} \n\n[[Category:Mesh generation]]\n[[Category:Mesh generation people]]\n[[Category:Mesh generators]]\n[[Category:Geometric algorithms]]\n[[Category:Computer-aided design]]\n[[Category:Triangulation (geometry)]]\n[[Category:Numerical analysis]]\n[[Category:Numerical differential equations]]\n[[Category:Computational fluid dynamics]]\n[[Category:3D computer graphics]]\n\nPlease help by expanding the Community Resources sections."
    },
    {
      "title": "Meshfree methods",
      "url": "https://en.wikipedia.org/wiki/Meshfree_methods",
      "text": "{{Use American English|date = February 2019}}\n{{Short description|Methods in numerical analysis not requiring knowledge of neighboring piints}}\n[[Image:Euclidean Voronoi diagram.svg|thumb|20 points and their Voronoi cells]]\nIn the field of [[numerical analysis]], '''meshfree methods''' are those that do not require connection between nodes of the simulation domain, i.e. a [[Types of mesh|mesh]], but are rather based on interaction of each node with all its neighbors. As a consequence, original extensive properties such as mass or kinetic energy are no longer assigned to mesh elements but rather to the single nodes. Meshfree methods enable the simulation of some otherwise difficult types of problems, at the cost of extra computing time and programming effort. The absence of a mesh allows [[Lagrangian and Eulerian specification of the flow field|Lagrangian]] simulations, in which the nodes can move according to the [[velocity field]].\n\n==Motivation==\nNumerical methods such as the [[finite difference method]], [[finite-volume method]], and [[finite element method]] were originally defined on meshes of data points. In such a mesh, each point has a fixed number of predefined neighbors, and this connectivity between neighbors can be used to define mathematical operators like the [[numerical differentiation|derivative]]. These operators are then used to construct the equations to simulate—such as the [[Euler equations (fluid dynamics)|Euler equations]] or the [[Navier–Stokes equations]].\n\nBut in simulations where the material being simulated can move around (as in [[computational fluid dynamics]]) or where large [[Deformation (mechanics)|deformations]] of the material can occur (as in simulations of [[Plasticity (physics)|plastic materials]]), the connectivity of the mesh can be difficult to maintain without introducing error into the simulation. If the mesh becomes tangled or degenerate during simulation, the operators defined on it may no longer give correct values. The mesh may be recreated during simulation (a process called remeshing), but this can also introduce error, since all the existing data points must be mapped onto a new and different set of data points. Meshfree methods are intended to remedy these problems. Meshfree methods are also useful for:\n* Simulations where [[Mesh generation|creating a useful mesh from the geometry of a complex 3D object]] may be especially difficult or require human assistance\n* Simulations where nodes may be created or destroyed, such as in cracking simulations\n* Simulations where the problem geometry may move out of alignment with a fixed mesh, such as in bending simulations\n* Simulations containing nonlinear material behavior, discontinuities or singularities\n\n==Example==\nIn a traditional [[finite difference method|finite difference]] simulation, the domain of a one-dimensional simulation would be some function <math>u(x, t)</math>, represented as a mesh of data values <math>u_i^n</math> at points <math>x_i</math>, where\n:<math>i=0,1,2...</math>\n:<math>n=0,1,2...</math>\n:<math>x_{i+1}-x_i=h\\ \\forall i</math>\n:<math>t_{n+1}-t_n=k\\ \\forall n</math>\nWe can define the derivatives that occur in the equation being simulated using some finite difference formulae on this domain, for example\n:<math>{\\partial u\\over \\partial x}={u_{i+1}^n-u_{i-1}^n\\over 2h}</math>\nand\n:<math>{\\partial u\\over \\partial t}={u_i^{n+1}-u_i^n\\over k}</math>\nThen we can use these definitions of <math>u(x,t)</math> and its spatial and temporal derivatives to write the equation being simulated in finite difference form, then simulate the equation with one of many [[finite difference method]]s.\n\nIn this simple example, the steps (here the spatial step <math>h</math> and timestep <math>k</math>) are constant along all the mesh, and the left and right mesh neighbors of the data value at <math>x_i</math> are the values at <math>x_{i-1}</math> and <math>x_{i+1}</math>, respectively. Generally in finite differences one can allow very simply for steps variable along the mesh, but all the original nodes should be preserved and they can move independently only by deforming the original elements. If even only two of all the nodes change their order, or even only one node is added to or removed from the simulation, that creates a defect in the original mesh and the simple finite difference approximation can no longer hold.\n\n[[Smoothed-particle hydrodynamics]] (SPH), one of the oldest meshfree methods, solves this problem by treating data points as physical particles with mass and density that can move around over time, and carry some value <math>u_i</math> with them. SPH then defines the value of <math>u(x,t)</math> between the particles by\n\n:<math>u(x,t_n) = \\sum_i m_i \\frac{u_i^n}{\\rho_i} W(|x-x_i|)</math>\n\nwhere <math>m_i</math> is the mass of particle <math>i</math>, <math>\\rho_i</math> is the density of particle <math>i</math>, and <math>W</math> is a kernel function that operates on nearby data points and is chosen for smoothness and other useful qualities. By linearity, we can write the spatial derivative as\n\n:<math>{\\partial u\\over \\partial x} = \\sum_i m_i \\frac{u_i^n}{\\rho_i} {\\partial W(|x-x_i|) \\over \\partial x}</math>\n\nThen we can use these definitions of <math>u(x,t)</math> and its spatial derivatives to write the equation being simulated as an [[Differential equation|ordinary differential equation]], and simulate the equation with one of many [[Numerical methods for ordinary differential equations|numerical methods]]. In physical terms, this means calculating the forces between the particles, then integrating these forces over time to determine their motion.\n\nThe advantage of SPH in this situation is that the formulae for <math>u(x,t)</math> and its derivatives do not depend on any adjacency information about the particles; they can use the particles in any order, so it doesn't matter if the particles move around or even exchange places.\n\nOne disadvantage of SPH is that it requires extra programming to determine the nearest neighbors of a particle. Since the kernel function <math>W</math> only returns nonzero results for nearby particles within twice the \"smoothing length\" (because we typically choose kernel functions with [[Support (mathematics)#Compact support|compact support]]), it would be a waste of effort to calculate the summations above over every particle in a large simulation. So typically SPH simulators require some extra code to speed up this nearest neighbor calculation.\n\n==History==\n\nOne of the earliest meshfree methods is [[smoothed particle hydrodynamics]], presented in 1977.<ref>Gingold RA, Monaghan JJ (1977). Smoothed particle hydrodynamics – theory and application to non-spherical stars. Mon Not R Astron Soc 181:375–389</ref> Libersky ''et al.''<ref>Libersky, L.D., Petscheck, A.G., Carney, T.C., Hipp, J.R., Allahdadi, F.A. (1993). High Strain Lagrangian Hydrodynamics. ''Journal of Computational Physics''.</ref> were the first to apply SPH in solid mechanics. The main drawbacks of SPH are inaccurate results near boundaries and tension instability that was first investigated by Swegle.<ref>Swegle, J.W., Hicks, D.L., Attaway, S.W. (1995). Smoothed Particle Hydrodynamics Stability Analysis. ''Journal of Computational Physics''. 116(1), 123-134</ref> \n\nIn the 1990s a new class of meshfree methods emerged based on the [[Galerkin method]]. This first method called the diffuse element method<ref>Nayroles, B., Touzot, G., Villon, P. (1992). Generalizing the finite element method: Diffuse approximation and diffuse elements. ''Computational Mechanics''. 10, 307-318.</ref> (DEM), pioneered by Nayroles et al., utilized the [[Moving least squares|MLS]] approximation in the Galerkin solution of partial differential equations, with approximate derivatives of the MLS function. Thereafter [[Ted Belytschko|Belytschko]] pioneered the Element Free Galerkin (EFG) method<ref>Belytschko, T., Lu, Y.Y., Gu, L. (1994). Element-free Galerkin Methods. ''International Journal for Numerical Methods in Engineering''. 37, 229-256.</ref>, which employed MLS with Lagrange multipliers to enforce boundary conditions, higher order numerical quadrature in the weak form, and full derivatives of the MLS approximation which gave better accuracy. Around the same time, the [[reproducing kernel particle method]]<ref>Liu, W.K., Jun, S., Zhang, Y.F. (1995), Reproducing kernel particle methods, ''International Journal of Numerical Methods in Fluids''. 20, 1081-1106. </ref> (RKPM) emerged, the approximation motivated in part to correct the kernel estimate in SPH: to give accuracy near boundaries, in non-uniform discretizations, and higher-order accuracy in general. Notably, in a parallel development, the [[Material point method|Material point methods]] were developed around the same time<ref>D. Sulsky, Z., Chen, H. Schreyer (1994). a Particle Method for History-Dependent Materials. ''Computer Methods in Applied Mechanics and Engineering'' (118) 1, 179-196.</ref> which offer similar capabilities. Material point methods are widely used in the movie industry to simulate large deformation solid mechanics, such as snow in the movie [[Frozen (2013 film)|Frozen]]<ref>https://www.math.ucla.edu/~jteran/papers/SSCTS13.pdf</ref>. RKPM and other meshfree methods were extensively developed by Chen, Liu, and Li in the late 1990s for a variety of applications and various classes of problems <ref>Liu, W., Chen, Y.,  Jun., S, Chen., J.S., Belytschko, T., Pan, C., Uras, R., Chang, C. (1996) Overview and applications of the reproducing Kernel Particle methods. ''Archives of Computational Methods in Engineering'' (3) 1 3-80.</ref>. During the 1990s and thereafter several other varieties were developed including those listed below.\n\n==List of methods and acronyms==\n\nThe following numerical methods are generally considered to fall within the general class of \"meshfree\" methods.  Acronyms are provided in parentheses.\n\n* [[Smoothed particle hydrodynamics]] (SPH) (1977)\n* [[Diffuse element method]] (DEM) (1992)\n* [[Dissipative particle dynamics]] (DPD) (1992)\n* [[Element-free Galerkin]] method (EFG / EFGM) (1994)\n* [[Reproducing kernel particle method]] (RKPM) (1995)\n* [[Finite point method]] (FPM) (1996)\n* [[Finite pointset method]] (FPM) (1998)\n* [[hp-clouds]]\n* [[Natural element method]] (NEM)\n* [[Material point method]] (MPM)\n* [[Meshless local Petrov Galerkin]] (MLPG) (1998)<ref>Atluri, S.N., Zhu, T. (1998). A new Meshless Local Petrov-Galerkin (MLPG) approach in computational mechanics. ''Computational Mechanics''. 22(2), 117-127.</ref>\n* [[Generalized-strain mesh-free formulation|Generalized-strain mesh-free (GSMF) formulation]] (2016)<ref>Oliveira, T. and A. Portela (2016). Weak-Form Collocation – a Local Meshless Method in Linear Elasticity. ''Engineering Analysis with Boundary Elements''.</ref>\n* [[Moving particle semi-implicit]] (MPS)\n* [[Generalized finite difference method]] (GFDM)\n* [[Particle-in-cell]] (PIC)\n* [[Moving particle finite element method]] (MPFEM)\n* [[Finite cloud method]] (FCM)\n* [[Boundary node method]] (BNM)\n* [[Meshfree moving Kriging interpolation method]] (MK)\n* [[Boundary cloud method]] (BCM)\n* [[Method of fundamental solutions]] (MFS)\n* [[Method of particular solution]] (MPS)\n* [[Method of finite spheres]] (MFS)\n* [[Discrete vortex method]] (DVM)\n* [[Finite mass method]] (FMM) (2000)<ref>C. Gauger, P. Leinen, H. Yserentant [http://leinen.rz.uni-mannheim.de/Projekte/Finite%20Massen%20Methode/gly.pdf The finite mass method] {{webarchive|url=https://web.archive.org/web/20150403011046/http://leinen.rz.uni-mannheim.de/Projekte/Finite%20Massen%20Methode/gly.pdf |date=2015-04-03 }}. SIAM J. Numer. Anal. 37 (2000), 176</ref>\n* [[Smoothed point interpolation method]] (S-PIM) (2005).<ref name=\"Liu\" />\n* Meshfree local [[radial point interpolation method]] (RPIM).<ref name=\"Liu\" />\n* Local radial basis function collocation Method (LRBFCM)<ref>Sarler B, Vertnik R. Meshfree</ref>\n* [[Viscous vortex domains method]] (VVD)\n* [[Cracking Particles Method]] (CPM) (2004)\n* [[Discrete least squares meshless method]] (DLSM) (2006)\n* [[Immersed Particle Method]] (IPM) (2006)\n* [[Optimal Transportation Meshfree method]] (OTM) (2010)<ref>{{Cite journal | doi=10.1002/nme.2869|title = Optimal transportation meshfree approximation schemes for fluid and plastic flows| journal=International Journal for Numerical Methods in Engineering| volume=83| issue=12| pages=1541–1579|year = 2010|last1 = Li|first1 = B.| last2=Habbal| first2=F.| last3=Ortiz| first3=M.}}</ref>\n* [[Repeated replacement method]] (RRM) (2012)<ref>Walker WA (2012) [http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0039999 The Repeated Replacement Method: A Pure Lagrangian Meshfree Method for Computational Fluid Dynamics]. PLoS ONE 7(7): e39999. doi:10.1371/journal.pone.0039999</ref>\n* [[Radial basis integral equation method]]<ref>Ooi EH, Popov V (2012) An efficient implementation of the radial basis integral equation method. Engineering Analysis with Boundary Elements, 36: 716-726</ref>\n* Least-square collocation meshless method (2001)<ref>{{Cite journal | doi=10.1002/nme.200|title = Least‐squares collocation meshless method| journal=International Journal for Numerical Methods in Engineering| volume=51| issue=9| pages=1089–1100|year = 2001|last1 = Zhang|first1 = Xiong| last2=Liu| first2=Xiao‐Hu| last3=Song| first3=Kang‐Zu| last4=Lu| first4=Ming‐Wan}}</ref>\n* [[Peridynamics]] (PD)\n*[https://onlinelibrary.wiley.com/doi/abs/10.1002/nme.2718 Exponential Basis Functions method] (EBFs) (2010)<ref>{{Cite journal|last=Boroomand|first=B.|last2=Soghrati|first2=S.|last3=Movahedian|first3=B.|date=2010|title=Exponential basis functions in solution of static and time harmonic elastic problems in a meshless style|url=https://onlinelibrary.wiley.com/doi/abs/10.1002/nme.2718|journal=International Journal for Numerical Methods in Engineering|language=en|volume=81|issue=8|pages=971–1018|doi=10.1002/nme.2718|issn=1097-0207}}</ref>\n\nRelated methods:\n\n* [[Moving least squares]] (MLS) – provide general approximation method for arbitrary set of nodes\n* [[Partition of unity]] methods (PoUM) – provide general approximation formulation used in some meshfree methods\n* Continuous blending method (enrichment and coupling of finite elements and meshless methods) – see {{harvtxt|Huerta|Fernández-Méndez|2000}}\n* [[Extended finite element method|eXtended FEM]], [[Generalized finite element method|Generalized FEM]] (XFEM, GFEM) – variants of FEM (finite element method) combining some meshless aspects\n* [[Smoothed finite element method]] (S-FEM) (2007)\n* [[Gradient smoothing method]] (GSM) (2008)\n* Local maximum-entropy (LME) – see {{harvtxt|Arroyo|Ortiz|2006}}\n* Space-Time Meshfree Collocation Method (STMCM) – see {{harvtxt|Netuzhylov|2008}}, {{harvtxt|Netuzhylov|Zilian|2009}}\n\n==Recent development==\n\nThe primary areas of advancement in meshfree methods are to address issues with essential boundary enforcement, numerical quadrature, and contact and large deformations<ref>Chen, J.S., Hillman, M., Chi, S.W. (2017). Meshfree Methods: Progress Made after 20 Years. ''Journal of Engineering Mechanics'', 143(4).</ref>. The common [[Weak formulation|weak form]] requires strong enforcement of the essential boundary conditions, yet meshfree methods in general lack the [[Kronecker delta]] property. This make essential boundary condition enforcement non-trivial, at least more difficult than the [[Finite element method]], where they can be imposed directly. Techniques have been developed to overcome this difficulty and impose conditions strongly. Several methods have been developed to impose the essential boundary conditions [[Weak formulation|weakly]], including [[Lagrange multipliers]], Nitche's method, and the penalty method. \n\nAs for [[Numerical integration|quadrature]], nodal integration is generally preferred which offers simplicity, efficiency, and keeps the meshfree method free of any mesh (as opposed to using [[Gaussian quadrature|Gauss quadrature]], which necessitates a mesh to generate quadrature points and weights). Nodal integration however, suffers from numerical instability due to underestimation of strain energy associated with short-wavelength modes<ref>Belytschko, T., Guo, Y., Liu, W.K., Xiao, S.P. (2000), A unified stability analysis of meshless particle methods, ''International Journal of Numerical Methods in Engineering''. 48, 1359-1400.</ref>, and also yields inaccurate and non-convergent results due to under-integration of the weak form<ref>Chen, J.S., Wu, C.T., Yoon, S. (2001), A stabilized conforming nodal integration for Galerkin mesh-free methods, ''International Journal of Numerical Methods in Engineering''. 435-466</ref>. One major advance in numerical integration has been the development of a stabilized conforming nodal integration (SCNI) which provides a nodal integration method which does not suffer from either of these problems<ref>Chen, J.S., Wu, C.T., Yoon, S. (2001), A stabilized conforming nodal integration for Galerkin mesh-free methods, ''International Journal of Numerical Methods in Engineering''. 435-466</ref>. The method is based on strain-smoothing which satisfies the first order [[Patch test (finite elements)|patch test]]. However, it was later realized that low-energy modes were still present in SCNI, and additional stabilization methods have been developed. This method has been applied to a variety of problems including thin and thick plates, poromechanics, convection-dominated problems, among others<ref>Chen, J.S., Hillman, M., Chi, S.W. (2017). Meshfree Methods: Progress Made after 20 Years. ''Journal of Engineering Mechanics'', 143(4).</ref>. More recently, a framework has been developed to pass arbitrary-order patch tests, based on a [[Petrov–Galerkin method]]<ref>J.-S. Chen, M. Hillman, M. Rüter (2013), An arbitrary order variationally consistent integration for Galerkin meshfree methods, ''International Journal of Numerical Methods in Engineering''. 95 387–418</ref>.\n\nOne recent advance in meshfree methods aims at the development of computational tools for automation in modeling and simulations.  This is enabled by the so-called weakened weak (W2) formulation based on the [[G space]] theory.<ref name=\"Liu_b\">G.R. Liu. A G space theory and a weakened weak (W2) form for a unified formulation of compatible and incompatible methods: Part I theory and Part II applications to solid mechanics problems. International Journal for Numerical Methods in Engineering, 81: 1093–1126, 2010</ref> The W2 formulation offers possibilities to formulate various (uniformly) \"soft\" models that work well with triangular meshes. Because a triangular mesh can be generated automatically, it becomes much easier in re-meshing and hence enables automation in modeling and simulation.  In addition, W2 models can be made soft enough (in uniform fashion) to produce upper bound solutions (for force-driving problems). Together with stiff models (such as the fully compatible FEM models), one can conveniently bound the solution from both sides.  This allows easy error estimation for generally complicated problems, as long as a triangular mesh can be generated. Typical W2 models are the Smoothed Point Interpolation Methods (or S-PIM).<ref name=\"Liu\">Liu, G.R. 2nd edn: 2009 ''Mesh Free Methods'',  CRC Press.  978-1-4200-8209-9</ref> The S-PIM can be node-based (known as NS-PIM or LC-PIM),<ref>Liu GR, Zhang GY, Dai KY, Wang YY, Zhong ZH, Li GY and Han X, A linearly conforming point interpolation method (LC-PIM) for 2D solid mechanics problems, [[International Journal of Computational Methods]], 2(4): 645–665, 2005.</ref> edge-based (ES-PIM),<ref>G.R. Liu, G.R. Zhang. Edge-based Smoothed Point Interpolation Methods. International Journal of Computational Methods, 5(4): 621–646, 2008</ref> and cell-based (CS-PIM).<ref>G.R. Liu, G.R. Zhang. A normed G space and weakened weak (W2) formulation of a cell-based Smoothed Point Interpolation Method. International Journal of Computational Methods, 6(1): 147–179, 2009</ref> The NS-PIM was developed using the so-called SCNI technique.<ref>Chen, J. S., Wu, C. T., Yoon, S. and You, Y. (2001). A stabilized conforming nodal integration for Galerkin mesh-free methods. Int. J. Numer. Meth. Eng. 50: 435–466.</ref> It was then discovered that NS-PIM is capable of producing upper bound solution and volumetric locking free.<ref>G. R. Liu and G. Y. Zhang. Upper bound solution to elasticity problems: A unique property of the linearly conforming point interpolation method (LC-PIM). International Journal for Numerical Methods in Engineering, 74: 1128–1161, 2008.</ref> The ES-PIM is found superior in accuracy, and CS-PIM behaves in between the NS-PIM and ES-PIM. Moreover, W2 formulations allow the use of polynomial and radial basis functions in the creation of shape functions (it accommodates the discontinuous displacement functions, as long as it is in G1 space), which opens further rooms for future developments. The W2 formulation has also led to the development of combination of meshfree techniques with the well-developed FEM techniques, and one can now use triangular mesh with excellent accuracy and desired softness. A typical such a formulation is the so-called smoothed finite element method (or S-FEM).<ref name=\"Liu_a\">Liu, G.R., 2010 ''Smoothed Finite Element Methods'', CRC Press, {{ISBN|978-1-4398-2027-8}}.</ref> The S-FEM is the linear version of S-PIM, but with most of the properties of the S-PIM and much simpler.\n\nIt is a general perception that meshfree methods are much more expensive than the FEM counterparts. The recent study has found however, some meshfree methods such as the S-PIM and S-FEM can be much faster than the FEM counterparts.<ref name=\"Liu\" /><ref name=\"Liu_a\" />\n\nThe S-PIM and S-FEM works well for solid mechanics problems.  For CFD problems, the formulation can be simpler, via strong formulation. A Gradient Smoothing Methods (GSM) has also been developed recently for CFD problems, implementing the gradient smoothing idea in strong form.<ref>G. R. Liu, George X. Xu. A gradient smoothing method (GSM) for fluid dynamics problems. International Journal for Numerical Methods in Fluids, 58: 1101–1133, 2008.</ref><ref>J. Zhang, G. R. Liu, K.Y. Lam, H. Li, G. Xu. A gradient smoothing method (GSM) based on strong form governing equation for adaptive analysis of solid mechanics problems. Finite Elements in Analysis and Design, 44: 889–909, 2008.</ref> The GSM is similar to [FVM], but uses gradient smoothing operations exclusively in nested fashions, and is a general numerical method for PDEs.\n\nNodal integration has been proposed as a technique to use finite elements to emulate a meshfree behaviour.<ref>{{cite journal|last=Simoes|first=D. A.|author2=Jadhav, T. A.|title=Nodally Integrated Finite Element Formulation for Mindlin-Reissner Plates|journal=International Journal of Scientific & Engineering Research|date=January 2014|volume=5|issue=1|pages=1977–1982|doi=10.14299/ijser.2014.01.001|issn=2229-5518|bibcode=2014IJSER...5.1977S|citeseerx=10.1.1.429.1065}}</ref>  However, the obstacle that must be overcome in using nodally integrated elements is that the quantities at nodal points are not continuous, and the nodes are shared among multiple elements.\n\n==See also==\n\n* [[Continuum mechanics]]\n* [[Smoothed finite element method]]<ref name=\"Liu_a\" />\n* [[G space]]<ref>Liu GR, ON G SPACE THEORY, INTERNATIONAL JOURNAL OF COMPUTATIONAL METHODS, Vol. 6   Issue: 2,257-289, 2009</ref>\n* [[Weakened weak form]]<ref name=\"Liu_b\" />\n* [[Boundary element method]]\n* [[Immersed boundary method]]\n* [[Stencil code]]\n* [[Particle method]]\n\n==References==\n{{reflist}}\n\n==Further reading==\n{{refbegin}}\n* Liu MB, Liu GR, Zong Z, AN OVERVIEW ON SMOOTHED PARTICLE HYDRODYNAMICS, INTERNATIONAL JOURNAL OF COMPUTATIONAL METHODS   Vol. 5   Issue: 1, 135–188, 2008.\n* Liu, G.R., Liu, M.B. (2003).  ''Smoothed Particle Hydrodynamics, a meshfree and Particle Method'',  World Scientific, {{ISBN|981-238-456-1}}.\n* {{Citation |first1=S.N. | last1=Atluri| title = The Meshless Method (MLPG) for Domain & BIE Discretization | year=2004 | journal= Tech Science Press}}. {{ISBN|0-9657001-8-6}}\n*{{Citation | last1=Arroyo | first1=M. | last2=Ortiz | first2=M. | title=Local maximum-entropy approximation schemes: a seamless bridge between finite elements and meshfree methods | doi=10.1002/nme.1534 | year=2006 | journal=International Journal for Numerical Methods in Engineering | volume=65 | issue=13 | pages=2167–2202|bibcode = 2006IJNME..65.2167A | citeseerx=10.1.1.68.2696 }}.\n* Belytschko, T., Chen, J.S. (2007).  ''Meshfree and Particle Methods'',  John Wiley and Sons Ltd. {{ISBN|0-470-84800-6}}\n* {{Citation |first1=T. | last1=Belytschko | first2=A. |last2=Huerta | first3=S | last3=Fernández-Méndez | first4=T. | last4= Rabczuk | title = Meshless methods | year=2004 | journal= Encyclopedia of Computational Mechanics Vol. 1 Chapter 10, John Wiley & Sons}}. {{ISBN|0-470-84699-2}}\n* Liu, G.R. 1st edn, 2002. ''Mesh Free Methods'',  CRC Press. {{ISBN|0-8493-1238-8}}.\n* Li, S., Liu, W.K. (2004). ''Meshfree Particle Methods'',  Berlin: Springer Verlag. {{ISBN|3-540-22256-1}}\n* {{Citation | first1 =A. | last1=Huerta | first2=S. |last2=Fernández-Méndez | title=Enrichment and coupling of the finite element and meshless methods | journal=International Journal for Numerical Methods in Engineering | year =2000 |volume=11| pages = 1615–1636| doi=10.1002/1097-0207(20000820)48:11<1615::AID-NME883>3.0.CO;2-S | issue =11|bibcode = 2000IJNME..48.1615H | hdl=2117/8264 }}.\n* {{Citation |first1=H. | last1=Netuzhylov  | title = A Space-Time Meshfree Collocation Method for Coupled Problems on Irregularly-Shaped Domains | year=2008 | journal= Dissertation, TU Braunschweig, CSE – Computational Sciences in Engineering}} {{ISBN|978-3-00-026744-4}}, also as [https://web.archive.org/web/20100326144956/http://netuzhylov.net/publications/publications.html electronic ed.].\n* {{Citation |first1=H. | last1=Netuzhylov | first2=A. | last2=Zilian  | title = Space-time meshfree collocation method: methodology and application to initial-boundary value problems | doi=10.1002/nme.2638 | year = 2009 | journal= International Journal for Numerical Methods in Engineering | volume=80 | issue=3 | pages=355–380|bibcode = 2009IJNME..80..355N }}\n*  Alhuri. Y, A. Naji, D. Ouazar and A. Taik. (2010). '' RBF Based Meshless Method for Large Scale Shallow Water Simulations: Experimental Validation'',  Math. Model. Nat. Phenom, Vol. 5, No. 7, 2010, pp.&nbsp;4–10.\n{{refend}}\n*  P. L. Machado, R. M. S. de Oliveira, W. C. B. Souza, R. C. F. Araújo, M. E. L. Tostes, and C. Gonçalves. (2011). '' An Automatic Methodology for Obtaining Optimum Shape Factors for the Radial Point Interpolation Method'',  Journal of Microwaves, Optoelectronics and Electromagnetic Applications, Vol. 10, No. 2, 2011, pp.&nbsp;389–401.[http://www.jmoe.org/index.php/jmoe/article/view/280/271]\n{{refend}}\n* W. C. B. Sousa,  R. M. S. de Oliveira, (2015). '' Coulomb`s Law Discretization Method: a New Methodology of Spatial Discretization for the Radial Point Interpolation Method.'',  IEEE Antennas & Propagation Magazine, Vol. 57, No. 2, 2015, pp.&nbsp;277–293.[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7087357]\n{{refend}}\n\n==External links==\n* [http://meshfreemethods.blogspot.com/ The USACM blog on Meshfree Methods]\n\n{{Numerical PDE}}\n\n{{DEFAULTSORT:Meshfree Methods}}\n[[Category:Numerical analysis]]\n[[Category:Numerical differential equations]]\n[[Category:Computational fluid dynamics]]"
    },
    {
      "title": "Method of fundamental solutions",
      "url": "https://en.wikipedia.org/wiki/Method_of_fundamental_solutions",
      "text": "In [[scientific computation]] and [[simulation]], the '''method of fundamental solutions''' ('''MFS''') is getting a growing attention.  The method is essentially based on the [[fundamental solution]] of a [[partial differential equation]] of interest as its basis function. The MFS was developed to overcome the major drawbacks in the [[boundary element method]] (BEM) which also uses the fundamental solution to satisfy the governing equation. Consequently, both the MFS and the BEM are of a boundary discretization numerical technique and reduce the computational complexity by one dimensionality and have particular edge over the domain-type numerical techniques such as the [[finite element]] and finite volume methods on the solution of infinite domain, thin-walled structures, and [[inverse problems]]. \n\nIn contrast to the BEM, the MFS avoids the numerical integration of singular fundamental solution and is an inherent [[meshfree method]]. The method, however, is compromised by requiring a controversial fictitious boundary outside the physical domain to circumvent the singularity of fundamental solution, which has seriously restricted its applicability to real-world problems. But nevertheless the MFS has been found very competitive to some application areas such as infinite domain problems.    \n\nThe MFS is also known by quite a few different names in the literature. Among these are the charge simulation method, the superposition method, the desingularized method, the indirect boundary element method, and the virtual boundary element method, just to name a few.\n\n== MFS formulation ==\n\nConsider a partial differential equation governing certain type of problems\n: <math>Lu=f\\left( x,y \\right),\\ \\ \\left( x,y \\right)\\in \\Omega, </math>\n:<math>u=g\\left( x,y \\right),\\ \\ \\left( x,y \\right)\\in \\partial \\Omega_D,</math>\n:<math>\\frac{\\partial u}{\\partial n}=h\\left( x,y \\right),\\ \\ \\left( x,y \\right)\\in \\partial \\Omega_N,</math>\nwhere <math>L</math> is the differential partial operator, <math>\\Omega </math> represents the computational domain, <math>\\partial \\Omega_D</math> and <math>\\partial \\Omega_N</math> denote the Dirichlet and Neumann boundary, respectively, <math>\\partial \\Omega_D \\cup \\partial \\Omega_N =\\partial \\Omega </math> \nand <math>\\partial \\Omega_D \\cap \\partial \\Omega_N =\\varnothing </math>.\n\nThe MFS employs the fundamental solution of the operator   as its basis function to represent the approximation of unknown function u as follows\n\n:<math>{{u}^{*}}\\left( x,y \\right)=\\sum\\limits_{i=1}^N \\alpha_i\\phi \\left( r_i \\right)</math>\n\nwhere <math>r_i =\\left\\| \\left( x,y \\right)-\\left( s x_i,s y_i \\right) \\right\\|</math> denotes the Euclidean distance between collocation points <math>\\left( x,y \\right)</math> and source points <math>\\left( s x_i,s y_i \\right)</math>, <math>\\phi \\left( \\cdot  \\right)</math> is the fundamental solution which satisfies\n\n:<math>L\\phi =\\delta \\, </math> \n\nwhere <math>\\delta </math> denotes Dirac delta function, and <math>{{\\alpha }_{i}}</math> are the unknown coefficients. \n\nWith the source points located outside the physical domain, the MFS avoid the fundamental solution singularity. Substituting the approximation into boundary condition yields the following matrix equation\n\n:<math>\\left[ \\begin{matrix}\n   \\phi \\left( \\left. r_j \\right|_{x_i,y_i} \\right)  \\\\\n   \\frac{\\partial \\phi \\left( \\left. r_j \\right|_{x_k,y_k} \\right)}{\\partial n}  \\\\\n\\end{matrix} \\right]\\ \\cdot \\ \\alpha =\\left( \\begin{matrix}\n   g\\left( x_i,y_i \\right)  \\\\\n   h\\left( x_k,y_k \\right)  \\\\\n\\end{matrix} \\right),</math>\n\nwhere <math>\\left( x_i,y_i \\right)</math> and <math>\\left( x_k,y_k \\right)</math> denote the collocation points, respectively, on Dirichlet and Neumann boundaries. The unknown coefficients <math>\\alpha_i</math> can uniquely be determined by the above algebraic equation. And then we can evaluate numerical solution at any location in physical domain.\n\n== History and recent developments ==\n\nThe ideas behind the MFS have been around for a few decades and were developed primarily by V. D. Kupradze and M. A. Alexidze in the late 1950s and early 1960s.<ref>K. VD, A. MA, The method of functional equations for the approximate solution of certain boundary value problems, ''USSR Comput Math Math Phys''. 4 (1964) 82–126.</ref> However, the method was proposed as a computational technique much later by R. Mathon and R. L. Johnston in the late 1970s,<ref>R. Mathon, R.L. Johnston, The approximate solution of elliptic boundary-value problems by fundamental solutions, ''SIAM Journal on Numerical Analysis''. (1977) 638–650.</ref> followed by a number of papers by Mathon, Johnston and Graeme Fairweather with applications. Slowly but surely the MFS becomes a useful tool for the solution of a large variety of physical and engineering problems.<ref>Z. Fu, W. Chen, W. Yang, [http://em.hhu.edu.cn/chenwen/papers/softmatter/CompMech.pdf Winkler plate bending problems by a truly boundary-only boundary particle method], ''Computational Mechanics''. 44 (2009) 757–763.</ref><ref>W. Chen, J. Lin, F. Wang, [http://em.hhu.edu.cn/chenwen/papers/softmatter/EABE-RMM.pdf Regularized meshless method for nonhomogeneous problems] {{webarchive|url=https://web.archive.org/web/20150606191121/http://em.hhu.edu.cn/chenwen/papers/softmatter/EABE-RMM.pdf |date=2015-06-06 }}, ''Engineering Analysis with Boundary Elements''. 35 (2011) 253–257.</ref><ref>W. Chen, F.Z. Wang, [http://em.hhu.edu.cn/chenwen/papers/rbf/EABE-SBM.pdf A method of fundamental solutions without fictitious boundary] {{webarchive|url=https://web.archive.org/web/20150606193008/http://em.hhu.edu.cn/chenwen/papers/rbf/EABE-SBM.pdf |date=2015-06-06 }}, ''Engineering Analysis with Boundary Elements''. 34 (2010) 530–532.</ref><ref>JIANG Xin-rong, CHEN Wen, Method of fundamental solution and boundary knot method for helmholtz equations: a comparative study, ''Chinese Journal of Computational Mechanics'', 28:3(2011) 338–344 (in Chinese)</ref> \n\nA major obstacle was overcome when, in the 1990s, M. A. Golberg and C. S. Chen extended the MFS to deal with inhomogeneous equations and time-dependent problems.<ref>M.A. Golberg, C.S. Chen, The theory of radial basis functions applied to the BEM for inhomogeneous partial differential equations, ''Boundary Elements Communications''. 5 (1994) 57–61.</ref><ref>M. a. Golberg, C.S. Chen, H. Bowman, H. Power, Some comments on the use of Radial Basis Functions in the Dual Reciprocity Method, ''Computational Mechanics''. 21 (1998) 141–148.</ref> Recent developments indicate that the MFS can be used to solve partial differential equations with variable coefficients.<ref>C.M. Fan, C.S. Chen, J. Monroe, The method of fundamental solutions for solving convection-diffusion equations with variable coefficients, ''Advances in Applied Mathematics and Mechanics''. 1 (2009) 215–230</ref> The MFS has proved particularly effective for certain classes of problems such as inverse,<ref>Y.C. Hon, T. Wei, The method of fundamental solution for solving multidimensional inverse heat conduction problems, ''CMES Comput. Model. Eng. Sci''. 7 (2005) 119–132</ref> unbounded domain, and free-boundary problems.<ref>A.K. G. Fairweather, The method of fundamental solutions for elliptic boundary value problems, ''Advances in Computational Mathematics''. 9 (1998) 69–95.</ref>\n\nSome new techniques have recently been developed to cure the fictitious boundary problem in the MFS, such as the [[boundary knot method]], [[singular boundary method]], and [[regularized meshless method]].\n\n== See also ==\n*[[Radial basis function]]\n*[[Boundary element method]]\n*[[Boundary knot method]]\n*[[Boundary particle method]]\n*[[Singular boundary method]]\n*[[Regularized meshless method]]\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [http://www.ccms.ac.cn/english/ International Center for Numerical Simulation Software in Engineering & Sciences]\n\n{{Numerical PDE}}\n\n{{DEFAULTSORT:Meshfree Methods}}\n[[Category:Numerical analysis]]\n[[Category:Numerical differential equations]]"
    },
    {
      "title": "Minimax approximation algorithm",
      "url": "https://en.wikipedia.org/wiki/Minimax_approximation_algorithm",
      "text": "A '''minimax approximation algorithm''' (or '''L<sup>∞</sup> approximation''' or '''uniform approximation''') is a method to find an approximation of a [[mathematical function]] that  minimizes maximum error.<ref name=\"Muller_2010\">{{cite book |author-last1=Muller |author-first1=Jean-Michel |author-last2=Brisebarre |author-first2=Nicolas |author-last3=de Dinechin |author-first3=Florent |author-last4=Jeannerod |author-first4=Claude-Pierre |author-last5=Lefèvre |author-first5=Vincent |author-last6=Melquiond |author-first6=Guillaume |author-last7=Revol |author-first7=Nathalie |author-last8=Stehlé |author-first8=Damien |author-last9=Torres |author-first9=Serge |title=Handbook of Floating-Point Arithmetic |year=2010 |publisher=[[Birkhäuser]] |edition=1 |isbn=978-0-8176-4704-9<!-- print --> |doi=10.1007/978-0-8176-4705-6 |lccn=2009939668<!-- |id=ISBN 978-0-8176-4705-6 (online), ISBN 0-8176-4704-X (print) --> |page=376}}</ref><ref name=\"phillips\">{{Cite book | doi = 10.1007/0-387-21682-0_2 | first = George M. | last = Phillips| chapter = Best Approximation | title = Interpolation and Approximation by Polynomials | series = CMS Books in Mathematics | pages = 49–11 | year = 2003 | publisher = Springer | isbn = 0-387-00215-4 | pmid =  | pmc = }}</ref>\n\nFor example, given a function <math>f</math> defined on the interval <math>[a,b]</math> and a degree bound <math>n</math>, a minimax polynomial approximation algorithm will find a polynomial <math>p</math> of degree at most <math>n</math> to minimize\n::<math>\\max_{a \\leq x \\leq b}|f(x)-p(x)|.</math><ref name=\"powell\">{{cite book | chapter = 7: The theory of minimax approximation | first = M. J. D. | last= Powell | authorlink=Michael J. D. Powell | year = 1981 | publisher= Cambridge University Press | title = Approximation Theory and Methods | isbn = 0521295149}}</ref>\n\n==Polynomial approximations==\n\nThe [[Weierstrass approximation theorem]] states that every continuous function defined on a closed interval [a,b] can be uniformly approximated as closely as desired by a polynomial function.<ref name=\"phillips\" />\nFor practical work it is often desirable to minimize the maximum absolute or relative error of a polynomial fit for any given number of terms in an effort to reduce computational expense of repeated evaluation.\n\nPolynomial expansions such as the [[Taylor series]] expansion are often convenient for theoretical work but less useful for practical applications. Truncated [[Chebyshev series]], however, closely approximate the minimax polynomial.\n\nOne popular minimax approximation algorithm is the [[Remez algorithm]].\n\n==External links==\n*[http://mathworld.wolfram.com/MinimaxApproximation.html Minimax approximation algorithm at MathWorld]\n\n==References==\n\n{{Reflist}}\n\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Minimum polynomial extrapolation",
      "url": "https://en.wikipedia.org/wiki/Minimum_polynomial_extrapolation",
      "text": "In [[mathematics]], '''minimum polynomial extrapolation''' is a [[sequence transformation]] used for [[convergence acceleration]] of vector sequences, due to Sabay and Jackson.<ref>{{Citation | title = A polynomial extrapolation method for finding limits and antilimits of vector sequences | last1 = Cabay | first1 = S. | last2 = Jackson | first2 = L.W. | date = 1976 | journal = SIAM Journal on Numerical Analysis | doi = 10.1137/0713060}}</ref>\n\nWhile [[Aitken's method]] is the most famous, it often fails for vector sequences. An effective method for vector sequences is the minimum polynomial extrapolation. It is usually phrased in terms of the [[fixed point iteration]]:\n\n: <math>x_{k+1}=f(x_k).</math>\n\nGiven iterates <math>x_1, x_2, ..., x_k</math> in <math>\\mathbb R^n</math>, one constructs the <math>n \\times (k-1)</math> matrix <math>U=(x_2-x_1, x_3-x_2, ..., x_k-x_{k-1})</math> whose columns are the <math>k-1</math> differences. Then, one computes the vector <math>c=-U^+ (x_{k+1}-x_k)</math> where <math>U^+</math> denotes the Moore–Penrose [[Moore–Penrose pseudoinverse|pseudoinverse]] of <math>U</math>. The number 1 is then appended to the end of <math>c</math>, and the extrapolated limit is\n\n:<math>s={ X c \\over \\sum_{i=1}^k c_i },</math>\n\nwhere <math>X=(x_2, x_3, ..., x_{k+1})</math> is the matrix whose columns are the <math>k</math> iterates starting at 2.\n\nThe following 4 line MATLAB code segment implements the MPE algorithm:\n\n<source lang=\"matlab\">\nU=x(:,2:end-1)-x(:,1:end-2);\nc=-pinv(U)*(x(:,end)-x(:,end-1));\nc(end+1,1)=1;\ns=(x(:,2:end)*c)/sum(c);\n</source>\n\n==References==\n<references/>\n\n[[Category:Numerical analysis]]\n[[Category:Articles with example MATLAB/Octave code]]\n\n\n{{mathanalysis-stub}}"
    },
    {
      "title": "Model order reduction",
      "url": "https://en.wikipedia.org/wiki/Model_order_reduction",
      "text": "'''Model order reduction (MOR)''' is a technique for reducing the [[Computational complexity theory|computational complexity]] of [[mathematical model]]s in [[numerical simulation]]s. As such it is closely related to the concept of [[metamodeling]] with applications in all areas of [[mathematical model]]ling.\n\n== Overview ==\nMany modern [[mathematical model]]s of real-life processes pose challenges when used in [[numerical simulation]]s, due to complexity and large size (dimension). '''Model order reduction''' aims to lower the computational complexity of such problems, for example, in simulations of large-scale [[dynamical system]]s and [[control system]]s. By a reduction of the model's associated [[state space]] dimension or [[degrees of freedom]], an approximation to the original model is computed which is commonly referred to as a reduced order model.\n\nReduced order models are useful in settings where it is often unfeasible to perform [[numerical simulation]]s using the complete full order model.  This can be due to limitations in [[computational resource]]s or the requirements of the simulations setting, for instance [[Real-time computing|real-time simulation]] settings or many-query settings in which a large number of simulations needs to be performed.<ref name=\":0\">{{cite book |last1=Lassila |first1=Toni |last2=Manzoni |first2=Andrea |last3=Quarteroni |first3=Alfio |last4=Rozza |first4=Gianluigi |title=Model Order Reduction in Fluid Dynamics: Challenges and Perspectives |journal=Reduced Order Methods for Modeling and Computational Reduction |date=2014 |pages=235–273 |doi=10.1007/978-3-319-02090-7_9 |language=en|isbn=978-3-319-02089-1 |url=http://eprints.whiterose.ac.uk/137452/1/LMQR_ROMReview.pdf }}</ref><ref>{{Cite journal|last=Rozza|first=G.|last2=Huynh|first2=D. B. P.|last3=Patera|first3=A. T.|date=2008-05-21|title=Reduced Basis Approximation and a Posteriori Error Estimation for Affinely Parametrized Elliptic Coercive Partial Differential Equations|journal=Archives of Computational Methods in Engineering|language=en|volume=15|issue=3|pages=229–275|doi=10.1007/s11831-008-9019-9|issn=1134-3060|url=http://infoscience.epfl.ch/record/124831}}</ref> Examples of Real-time simulation settings include [[controller (control theory)|control systems]] in electronics and [[Data visualization|visualization]] of model results while examples for a many-query setting can include [[optimization]] problems and design exploration. In order to be applicable to real-world problems, often the requirements of a reduced order model are:<ref name=Schilders>{{cite book |last1=Schilders |first1=Wilhelmus |last2=van der Vorst |first2=Henk |last3=Rommes |first3=Joost |title=Model Order Reduction: Theory, Research Aspects and Applications |date=2008 |publisher=Springer-Verlag |isbn=978-3-540-78841-6 }}</ref><ref>{{cite journal|last1=Antoulas|first1=A.C.|date=July 2004|title=Approximation of Large-Scale Dynamical Systems: An Overview|url=|journal=IFAC Proceedings Volumes|volume=37|issue=11|pages=19–28|doi=10.1016/S1474-6670(17)31584-7|via=|citeseerx=10.1.1.29.3565}}</ref>\n* A small [[approximation error]] compared to the full order model.\n* Conservation of the properties and characteristics of the full order model (E.g. stability and [[Passivity (engineering)|passivity]] in electronics).\n* Computationally efficient and robust reduced order modelling techniques.\n\n== Methods ==\n\nModel order reduction techniques used most commonly nowadays can be broadly classified into 4 classes:<ref name=\":0\" /><ref>{{Citation|last=Silva|first=João M. S.|title=Outstanding Issues in Model Order Reduction|date=2007|last2=Villena|first2=Jorge Fernández|last3=Flores|first3=Paulo|last4=Silveira|first4=L. Miguel|work=Scientific Computing in Electrical Engineering|pages=139–152|publisher=Springer Berlin Heidelberg|language=en|doi=10.1007/978-3-540-71980-9_13|isbn=9783540719793}}</ref>\n\n* Proper orthogonal decomposition methods.<ref name=\":1\">{{Cite journal|last=Kerschen|first=Gaetan|last2=Golinval|first2=Jean-claude|last3=VAKAKIS|first3=ALEXANDER F.|last4=BERGMAN|first4=LAWRENCE A.|date=2005|title=The Method of Proper Orthogonal Decomposition for Dynamical Characterization and Order Reduction of Mechanical Systems: An Overview|journal=Nonlinear Dynamics|language=en|volume=41|issue=1–3|pages=147–169|doi=10.1007/s11071-005-2803-2|issn=0924-090X|citeseerx=10.1.1.530.8349}}</ref><!-- Do not link \"Principal Component Analysis\" here! -->\n* Reduced basis methods.<ref>{{cite journal |last1=Boyaval |first1=S. |last2=Le Bris |first2=C. |last3=Lelièvre |first3=T. |last4=Maday |first4=Y. |last5=Nguyen |first5=N. C. |last6=Patera |first6=A. T. |title=Reduced Basis Techniques for Stochastic Problems |journal=Archives of Computational Methods in Engineering |date=16 October 2010 |volume=17 |issue=4 |pages=435–454 |doi=10.1007/s11831-010-9056-z|hdl=1721.1/63915 }}</ref>\n* Balancing methods\n* simplified physics<ref name=\":2\">{{Cite journal|last=Benner|first=Peter|last2=Gugercin|first2=Serkan|last3=Willcox|first3=Karen|date=2015|title=A Survey of Projection-Based Model Reduction Methods for Parametric Dynamical Systems|journal=SIAM Review|language=en-US|volume=57|issue=4|pages=483–531|doi=10.1137/130932715|issn=0036-1445|hdl=1721.1/100939}}</ref> or operational based reduction methods.<ref name=Schilders />\n\nThe simplified physics approach can be described to be analogous to the traditional [[Mathematical model]]ling approach, in which a less complex description of a system is constructed based on assumptions and simplifications using physical insight or otherwise derived information. However, this approach is not often the topic of discussion in the context of model order reduction as it is a general method in science, engineering and mathematics and is not the subject of the current article.\n\nThe remaining listed methods fall into the category of projection-based reduction. Projection-based reduction relies on the  projection of either the model equations or the solution onto a basis of reduced dimensionality compared to the original solution space. Methods that also fall into this class but are perhaps less commonly found are:\n\n* [[Proper generalized decomposition]]<ref name=PGD>{{cite journal |last1=Chinesta |first1=Francisco |last2=Ladeveze |first2=Pierre |last3=Cueto |first3=Elías |title=A Short Review on Model Order Reduction Based on Proper Generalized Decomposition |journal=Archives of Computational Methods in Engineering |date=11 October 2011 |volume=18 |issue=4 |pages=395–404 |doi=10.1007/s11831-011-9064-7}}</ref>\n* Matrix interpolation\n* Transfer function interpolation\n* Piecewise tangential interpolation\n* Loewner framework\n* (Empirical) [[cross Gramian]]\n* [[Krylov subspace|Krylov subspace methods]]<ref name=Bai2002>{{cite journal|authors=Bai, Zhaojun|title=Krylov subspace techniques for reduced-order modeling of large-scale dynamical systems|journal=Applied Numerical Mathematics|date=2002|volume=43|issue=1–2|pages=9–44|doi=10.1016/S0168-9274(02)00116-2|citeseerx=10.1.1.131.8251}}</ref>\n\n== Applications ==\n\nModel order reduction finds application within all fields involving mathematical modelling and many reviews exist for the topics of [[electronics]], [[Fluid mechanics|fluid-]] and [[structural mechanics]].<ref name=\":2\" /><ref name=PGD /><ref>{{Cite book|title=Turbulence, Coherent Structures, Dynamical Systems and Symmetry|last=Holmes|first=Philip|last2=Lumley|first2=John L.|last3=Berkooz|first3=Gal|date=1996|publisher=Cambridge University Press|isbn=9780511622700|location=Cambridge|language=en|doi=10.1017/cbo9780511622700}}</ref><ref name=\":1\" />\n\n===Fluid mechanics===\n\nCurrent Problems in fluid mechanics involve large [[dynamical system]]s representing many effects on many different scales. [[Computational fluid dynamics]] studies often involve models solving the [[Navier–Stokes equations]] with a number of [[degrees of freedom]] in the order of magnitude upwards of <math>10^6</math>. The first usage of model order reduction techniques dates back to the work of Lumley in 1967<ref>{{cite book |last1=Lumley |first1=J.L. |title=The Structure of Inhomogeneous Turbulence,\" In: A. M. Yaglom and V. I. Tatarski, Eds., Atmospheric Turbulence and Wave Propagation |date=1967 |publisher=Nauka |location=Moscow}}</ref> where it was used to gain insight into the mechanisms and intensity of [[turbulence]] and [[Coherent turbulent structure|large coherent structure]]s present in fluid flow problems. Model order reduction also finds modern applications in [[Aeronautics]] to model the flow over the body of aircraft.<ref>{{Cite journal|last=Walton|first=S.|last2=Hassan|first2=O.|last3=Morgan|first3=K.|date=2013|title=Reduced order modelling for unsteady fluid flow using proper orthogonal decomposition and radial basis functions|journal=Applied Mathematical Modelling|volume=37|issue=20–21|pages=8930–8945|doi=10.1016/j.apm.2013.04.025|issn=0307-904X}}</ref> An example can be found in Lieu et al<ref>{{Cite journal|last=Lieu|first=T.|last2=Farhat|first2=C.|last3=Lesoinne|first3=M.|date=2006|title=Reduced-order fluid/structure modeling of a complete aircraft configuration|journal=Computer Methods in Applied Mechanics and Engineering|volume=195|issue=41–43|pages=5730–5742|doi=10.1016/j.cma.2005.08.026|issn=0045-7825}}</ref> in which the full order model of an [[General Dynamics F-16 Fighting Falcon|F16]] fighter-aircraft with over 2.1 million degrees of freedom, was reduced to a model of just 90 degrees of freedom. Additionally reduced order modeling has been applied to study [[rheology]] in [[Hemodynamics]] and the [[Fluid–structure interaction]] between the blood flowing through the vascular system and the vascular walls.<ref>{{Cite journal|last=Xiao|first=D.|last2=Yang|first2=P.|last3=Fang|first3=F.|last4=Xiang|first4=J.|last5=Pain|first5=C.C.|last6=Navon|first6=I.M.|date=2016|title=Non-intrusive reduced order modelling of fluid–structure interactions|journal=Computer Methods in Applied Mechanics and Engineering|volume=303|pages=35–54|doi=10.1016/j.cma.2015.12.029|issn=0045-7825}}</ref><ref>{{Cite journal|last=Colciago|first=C.M.|last2=Deparis|first2=S.|last3=Quarteroni|first3=A.|date=2014|title=Comparisons between reduced order models and full 3D models for fluid–structure interaction problems in haemodynamics|journal=Journal of Computational and Applied Mathematics|volume=265|pages=120–138|doi=10.1016/j.cam.2013.09.049|issn=0377-0427}}</ref>\n\n== See also ==\n\n* [[Dimension reduction]]\n* [[Metamodeling]]\n* [[Principal component analysis]]\n* [[Singular value decomposition]]\n* [[Nonlinear dimensionality reduction]]\n* [[System identification]]\n\n== References ==\n\n{{reflist}}\n<!-- this is a specific method and does not fit a general MOR page * Michal Rewienski, and Jacob White. \"A trajectory piecewise-linear approach to model order reduction and fast simulation of nonlinear circuits and micromachined devices.\" Computer-Aided Design of Integrated Circuits and Systems, IEEE Transactions on 22.2 (2003): 155-170. -->\n\n== Further reading ==\n\n* {{Cite book\n | last =  Antoulas\n | first = Athanasios C.\n | title = Approximation of Large-Scale Dynamical Systems\n | doi = 10.1137/1.9780898718713\n | year = 2005\n | publisher = SIAM\n| isbn = 978-0-89871-529-3\n }}\n\n* {{Citation\n | last1 = Benner\n | first1 = Peter\n | last2 = Fassbender\n | first2 = Heike | author2-link = Heike Fassbender\n | chapter = Model Order Reduction: Techniques and Tools\n | title = Encyclopedia of Systems and Control \n | doi = 10.1007/978-1-4471-5102-9_142-1\n | year = 2014\n | publisher = Springer\n | isbn = 978-1-4471-5102-9\n | chapter-url = https://www.tu-braunschweig.de/Medien-DB/numerik/benner-fassbender-mor.pdf\n }}\n\n*{{citation\n | last1 = Antoulas | first1 = A. C.\n | last2 = Sorensen | first2 = D. C.\n | last3 = Gugercin | first3 = S.\n | contribution = A survey of model reduction methods for large-scale systems\n | contribution-url = https://www.math.vt.edu/people/gugercin/papers/survey.pdf\n | doi = 10.1090/conm/280/04630\n | mr = 1850408\n | pages = 193–219\n | publisher = American Mathematical Society | location = Providence, RI\n | series = Contemporary Mathematics\n | title = Structured matrices in mathematics, computer science, and engineering, I (Boulder, CO, 1999)\n | volume = 280\n | year = 2001| isbn = 9780821819210\n | citeseerx = 10.1.1.210.9685\n }}\n\n* {{Citation\n | last1 = Benner\n | first1 = Peter\n | last2 = Gugercin\n | first2 = Serkan\n | last3 = Willcox\n | first3 = Karen\n | title = A survey of model reduction methods for parametric systems\n | year = 2013\n | url = http://www2.mpi-magdeburg.mpg.de/preprints/2013/MPIMD13-14.pdf\n }}\n\n* {{Citation\n | last1 = Baur\n | first1 = Ulrike\n | last2 = Benner\n | first2 = Peter\n | last3 = Feng\n | first3 = Lihong\n | title = Model order reduction for linear and nonlinear systems: a system-theoretic perspective\n | journal = Archives of Computational Methods in Engineering\n | volume = 21\n | pages = 331–358\n | number = 4\n | year = 2014\n | doi = 10.1007/s11831-014-9111-2\n | url = http://www2.mpi-magdeburg.mpg.de/preprints/2014/MPIMD14-07.pdf\n}}\n\n== External links ==\n* [http://modelreduction.org Model Order Reduction Wiki]\n* [http://morepas.org Model Reduction for Parametrized Systems]\n* [http://eu-mor.net European Model Reduction Network]\n\n<!-- Just press the \"Save page\" button below without changing anything! Doing so will submit your article submission for review. Once you have saved this page you will find a new yellow 'Review waiting' box at the bottom of your submission page. If you have submitted your page previously, either the old pink 'Submission declined' template or the old grey 'Draft' template will still appear at the top of your submission page, but you should ignore it. Again, please don't change anything in this text box. Just press the \"Save page\" button below. -->\n\n[[Category:Numerical analysis]]\n[[Category:Mathematical modeling]]"
    },
    {
      "title": "Monte Carlo method",
      "url": "https://en.wikipedia.org/wiki/Monte_Carlo_method",
      "text": "{{distinguish|Monte Carlo algorithm}}\n{{Short description|Probabilistic problem-solving algorithm}}\n'''Monte Carlo methods''', or '''Monte Carlo experiments''', are a broad class of [[computation]]al [[algorithm]]s that rely on repeated [[random sampling]] to obtain numerical results. The underlying concept is to use [[randomness]] to solve problems that might be [[deterministic system|deterministic]] in principle. They are often used in [[physics|physical]] and [[mathematics|mathematical]] problems and are most useful when it is difficult or impossible to use other approaches. Monte Carlo methods are mainly used in three problem classes:<ref>{{cite journal|last1 = Kroese|first1 = D. P.|last2=Brereton|first2=T.|last3 = Taimre|first3 = T.|last4 = Botev|first4 = Z. I. |year =2014 |title=Why the Monte Carlo method is so important today |journal = WIREs Comput Stat|volume=6|issue = 6|pages = 386–392 |doi=10.1002/wics.1314}}</ref> [[optimization]], [[numerical integration]], and generating draws from a [[probability distribution]].\n\nIn physics-related problems, Monte Carlo methods are useful for simulating systems with many [[coupling (physics)|coupled]] [[degrees of freedom]], such as fluids, disordered materials, strongly coupled solids, and cellular structures (see [[cellular Potts model]], [[interacting particle systems]], [[McKean–Vlasov process]]es, [[kinetic theory of gases|kinetic models of gases]]). Other examples include modeling phenomena with significant [[uncertainty]] in inputs such as the calculation of [[risk]] in business and, in maths, evaluation of multidimensional [[Integral|definite integral]]s  with complicated [[boundary conditions]].  In application to systems engineering problems (space, [[oil exploration]], aircraft design, etc.), Monte Carlo&ndash;based predictions of failure, [[cost overrun]]s and schedule overruns are routinely better than human intuition or alternative \"soft\" methods.<ref>{{cite journal|last1 = Hubbard|first1 = Douglas|last2=Samuelson|first2 = Douglas A. |date = October 2009 |title=Modeling Without Measurements |url=http://viewer.zmags.com/publication/357348e6#/357348e6/28|journal = OR/MS Today|pages = 28–33}}</ref>\n\nIn principle, Monte Carlo methods can be used to solve any problem having a probabilistic interpretation. By the [[law of large numbers]], integrals described by the [[expected value]] of some random variable can be approximated by taking the [[Sample mean and sample covariance|empirical mean]] (a.k.a. the sample mean) of independent samples of the variable. When the [[probability distribution]] of the variable is parametrized, mathematicians often use a [[Markov chain Monte Carlo]] (MCMC) sampler.<ref>{{Cite journal|title = Equation of State Calculations by Fast Computing Machines|journal = The Journal of Chemical Physics|date = 1953-06-01|issn = 0021-9606|pages = 1087–1092|volume = 21|issue = 6|doi = 10.1063/1.1699114|first = Nicholas|last = Metropolis|first2 = Arianna W.|last2 = Rosenbluth|first3 = Marshall N.|last3 = Rosenbluth|first4 = Augusta H.|last4 = Teller|first5 = Edward|last5 = Teller|bibcode=1953JChPh..21.1087M}}</ref><ref>{{Cite journal|title = Monte Carlo sampling methods using Markov chains and their applications|url = http://biomet.oxfordjournals.org/content/57/1/97|journal = Biometrika|date = 1970-04-01|issn = 0006-3444|pages = 97–109|volume = 57|issue = 1|doi = 10.1093/biomet/57.1.97|first = W. K.|last = Hastings|bibcode = 1970Bimka..57...97H}}</ref><ref>{{Cite journal|title = The Multiple-Try Method and Local Optimization in Metropolis Sampling|journal = Journal of the American Statistical Association|date = 2000-03-01|issn = 0162-1459|pages = 121–134|volume = 95|issue = 449|doi = 10.1080/01621459.2000.10473908|first = Jun S.|last = Liu|first2 = Faming|last2 = Liang|first3 = Wing Hung|last3 = Wong}}</ref><ref>{{Cite journal|title = On the flexibility of the design of multiple try Metropolis schemes|journal = Computational Statistics|date = 2013-07-11|issn = 0943-4062|pages = 2797–2823|volume = 28|issue = 6|doi = 10.1007/s00180-013-0429-2|first = Luca|last = Martino|first2 = Jesse|last2 = Read|arxiv = 1201.0646}}</ref> The central idea is to design a judicious [[Markov chain]] model with a prescribed [[stationary probability distribution]]. That is, in the limit, the samples being generated by the MCMC method will be samples from the desired (target) distribution.<ref>{{cite journal | last1 = Spall | first1 = J. C. | year = 2003 | title = Estimation via Markov Chain Monte Carlo | doi = 10.1109/MCS.2003.1188770 | journal = IEEE Control Systems Magazine | volume = 23 | issue = 2| pages = 34–45 }}</ref><ref>Hill, S. D. and Spall, J. C. (2019), “Stationarity and Convergence of the Metropolis-Hastings Algorithm: Insights into Theoretical Aspects,” ''IEEE Control Systems Magazine'', vol. 39(1), pp. 56–67. https://doi.org/10.1109/MCS.2018.2876959</ref> By the [[ergodic theorem]], the stationary distribution is approximated by the [[empirical measure]]s of the random states of the MCMC sampler.\n\nIn other problems, the objective is generating draws from a sequence of probability distributions satisfying a nonlinear evolution equation. These flows of probability distributions can always be interpreted as the distributions of the random states of a [[Markov process]] whose transition probabilities depend on the distributions of the current random states (see [[McKean–Vlasov process]]es, [[particle filter|nonlinear filtering equation]]).<ref name=\"kol10\">{{cite book|last = Kolokoltsov|first = Vassili|title = Nonlinear Markov processes|year = 2010|publisher = Cambridge Univ. Press|pages = 375}}</ref><ref name=\"dp13\">{{cite book|last = Del Moral|first = Pierre|title = Mean field simulation for Monte Carlo integration|year = 2013|publisher = Chapman & Hall/CRC Press|quote = Monographs on Statistics & Applied Probability|url = http://www.crcpress.com/product/isbn/9781466504059|pages = 626}}</ref> In other instances we are given a flow of probability distributions with an increasing level of sampling complexity (path spaces models with an increasing time horizon, Boltzmann–Gibbs measures associated with decreasing temperature parameters, and many others). These models can also be seen as the evolution of the law of the random states of a nonlinear Markov chain.<ref name=\"dp13\" /><ref>{{Cite journal|title = Sequential Monte Carlo samplers | last = Del Moral | first = P | last2 = Doucet | first2 = A | last3 = Jasra | first3 = A | year = 2006 |doi=10.1111/j.1467-9868.2006.00553.x|volume=68| issue = 3 |journal=Journal of the Royal Statistical Society, Series B|pages=411–436|arxiv = cond-mat/0212648}}</ref>  A natural way to simulate these sophisticated nonlinear Markov processes is to sample a large number of copies of the process, replacing in the evolution equation the unknown distributions of the random states by the sampled [[empirical measure]]s. In contrast with traditional Monte Carlo and MCMC methodologies these [[Mean field particle methods|mean field particle]] techniques rely on sequential interacting samples. The terminology ''mean field'' reflects the fact that each of the ''samples'' (a.k.a. particles, individuals, walkers, agents, creatures, or phenotypes) interacts with the empirical measures of the process. When the size of the system tends to infinity, these random empirical measures converge to the deterministic distribution of the random states of the nonlinear Markov chain, so that the statistical interaction between particles vanishes.\n\n== Overview ==\n[[File:Pi 30K.gif|thumb|right| Monte Carlo method applied to approximating the value of {{pi}}. After placing 30,000 random points, the estimate for {{pi}} is 0.34% more than the actual value.]]\n\nMonte Carlo methods vary, but tend to follow a particular pattern:\n# Define a domain of possible inputs\n# Generate inputs randomly from a [[probability distribution]] over the domain\n# Perform a [[Deterministic algorithm|deterministic]] computation on the inputs\n# Aggregate the results\n\nFor example, consider a [[circular sector#Quadrant|quadrant (circular sector)]] inscribed in a [[unit square]]. Given that the ratio of their areas is {{sfrac|{{pi}}|4}}, the value of [[pi|{{pi}}]] can be approximated using a Monte Carlo method:{{sfn|Kalos|Whitlock|2008}}\n# Draw a square, then [[inscribed figure|inscribe]] a quadrant within it\n# [[uniform distribution (continuous)|Uniformly]] scatter a given number of points over the square\n# Count the number of points inside the quadrant, i.e. having a distance from the origin of less than 1\n# The ratio of the inside-count and the total-sample-count is an estimate of the ratio of the two areas, {{sfrac|{{pi}}|4}}. Multiply the result by 4 to estimate {{pi}}.\nIn this procedure the domain of inputs is the square that circumscribes the quadrant.  We generate random inputs by scattering grains over the square then perform a computation on each input (test whether it falls within the quadrant). Aggregating the results yields our final result, the approximation of {{pi}}.\n\nThere are two important points:\n# If the points are not uniformly distributed, then the approximation will be poor.\n# There are a large number of points. The approximation is generally poor if only a few points are randomly placed in the whole square. On average, the approximation improves as more points are placed.\n\nUses of Monte Carlo methods require large amounts of random numbers, and it was their use that spurred the development of [[pseudorandom number generator]]s, which were far quicker to use than the tables of random numbers that had been previously used for statistical sampling.\n\n== History ==\nBefore the Monte Carlo method was developed, simulations tested a previously understood deterministic problem, and statistical sampling was used to estimate uncertainties in the simulations. Monte Carlo simulations invert this approach, solving deterministic problems using a [[probabilistic]] [[meta-algorithm|analog]] (see [[Simulated annealing]]).\n<!-- work on small samples by [[William Sealy Gosset]] was mentioned in an earlier draft, but it is not clear how it was related. -->\n\nAn early variant of the Monte Carlo method can be seen in the [[Buffon's needle]] experiment, in which {{pi}} can be estimated by dropping needles on a floor made of parallel and equidistant strips. In the 1930s, [[Enrico Fermi]] first experimented with the Monte Carlo method while studying neutron diffusion, but did not publish anything on it.{{sfn|Metropolis|1987}}\n\nThe modern version of the Markov Chain Monte Carlo method was invented in the late 1940s by [[Stanislaw Ulam]], while he was working on nuclear weapons projects at the [[Los Alamos National Laboratory]]. Immediately after Ulam's breakthrough, [[John von Neumann]] understood its importance and programmed the [[ENIAC]] computer to carry out Monte Carlo calculations. In 1946, physicists at [[Los Alamos Scientific Laboratory]] were investigating [[radiation shielding]] and the distance that [[neutrons]] would likely travel through various materials. Despite having most of the necessary data, such as the average distance a neutron would travel in a substance before it collided with an atomic nucleus, and how much energy the neutron was likely to give off following a collision, the Los Alamos physicists were unable to solve the problem using conventional, deterministic mathematical methods. Ulam had the idea of using random experiments. He recounts his inspiration as follows:\n:: The first thoughts and attempts I made to practice [the Monte Carlo Method] were suggested by a question which occurred to me in 1946 as I was convalescing from an illness and playing solitaires. The question was what are the chances that a [[Canfield (solitaire)|Canfield solitaire]] laid out with 52 cards will come out successfully? After spending a lot of time trying to estimate them by pure combinatorial calculations, I wondered whether a more practical method than \"abstract thinking\" might not be to lay it out say one hundred times and simply observe and count the number of successful plays. This was already possible to envisage with the beginning of the new era of fast computers, and I immediately thought of problems of neutron diffusion and other questions of mathematical physics, and more generally how to change processes described by certain differential equations into an equivalent form interpretable as a succession of random operations. Later [in 1946], I described the idea to [[John von Neumann]], and we began to plan actual calculations.{{sfn|Eckhardt|1987}}\n\nBeing secret, the work of von Neumann and Ulam required a code name.{{sfn|Mazhdrakov|Benov|Valkanov|2018|p=250}} A colleague of von Neumann and Ulam, [[Nicholas Metropolis]], suggested using the name ''Monte Carlo'', which refers to the [[Monte Carlo Casino]] in [[Monaco]] where Ulam's uncle would borrow money from relatives to gamble.{{sfn|Metropolis|1987}} Using [[A Million Random Digits with 100,000 Normal Deviates|lists of \"truly random\" random numbers]] was extremely slow, but von Neumann developed a way to calculate [[pseudorandom number]]s, using the [[middle-square method]]. Though this method has been criticized as crude, von Neumann was aware of this: he justified it as being faster than any other method at his disposal, and also noted that when it went awry it did so obviously, unlike methods that could be subtly incorrect.<ref>{{cite book |last = Peragine |first = Michael |title = The Universal Mind: The Evolution of Machine Intelligence and Human Psychology |year = 2013 |publisher = Xiphias Press |url = https://books.google.com/?id=Dvb0DAAAQBAJ&pg=PT201&lpg=PT201&dq=he+justified+it+as+being+faster+than+any+other+method+at+his+disposal,+and+also+noted+that+when+it+went+awry+it+did+so+obviously,+unlike+methods+that+could+be+subtly+incorrect.#v=onepage&q=he%20justified%20it%20as%20being%20faster%20than%20any%20other%20method%20at%20his%20disposal%2C%20and%20also%20noted%20that%20when%20it%20went%20awry%20it%20did%20so%20obviously%2C%20unlike%20methods%20that%20could%20be%20subtly%20incorrect.&f=false |access-date = 2018-12-17 }}</ref>\n\nMonte Carlo methods were central to the [[simulation]]s required for the [[Manhattan Project]], though severely limited by the computational tools at the time. In the 1950s they were used at [[Los Alamos National Laboratory|Los Alamos]] for early work relating to the development of the [[hydrogen bomb]], and became popularized in the fields of [[physics]], [[physical chemistry]], and [[operations research]]. The [[Rand Corporation]] and the [[U.S. Air Force]] were two of the major organizations responsible for funding and disseminating information on Monte Carlo methods during this time, and they began to find a wide application in many different fields.\n\nThe theory of more sophisticated mean field type particle Monte Carlo methods had certainly started by the mid-1960s, with the work of [[Henry McKean|Henry P. McKean Jr.]] on Markov interpretations of a class of nonlinear parabolic partial differential equations arising in fluid mechanics.<ref name=\"mck67\">{{cite journal |last = McKean |first = Henry, P. |title = Propagation of chaos for a class of non-linear parabolic equations |journal = Lecture Series in Differential Equations, Catholic Univ. |year = 1967 |volume = 7 |pages = 41–57 }}</ref><ref>{{cite journal |last1 = McKean |first1 = Henry, P. |title = A class of Markov processes associated with nonlinear parabolic equations |journal = Proc. Natl. Acad. Sci. USA |year = 1966 |volume = 56 |issue = 6 |pages = 1907–1911 |doi = 10.1073/pnas.56.6.1907 |pmid = 16591437 |pmc = 220210 |bibcode = 1966PNAS...56.1907M }}</ref> We also quote an earlier pioneering article by [[Ted Harris (mathematician)|Theodore E. Harris]] and Herman Kahn, published in 1951, using mean field [[genetic algorithm|genetic]]-type Monte Carlo methods for estimating particle transmission energies.<ref>{{cite journal |last1 = Herman |first1 = Kahn |last2 = Theodore |first2 = Harris E. |title = Estimation of particle transmission by random sampling |journal = Natl. Bur. Stand. Appl. Math. Ser. |year = 1951 |volume = 12 |pages = 27–30 |url = https://dornsifecms.usc.edu/assets/sites/520/docs/kahnharris.pdf }}</ref> Mean field genetic type Monte Carlo methodologies are also used as heuristic natural search algorithms (a.k.a. [[Metaheuristic]]) in evolutionary computing. The origins of these mean field computational techniques can be traced to 1950 and 1954 with the work of [[Alan Turing]] on genetic type mutation-selection learning machines<ref>{{cite journal |last = Turing |first = Alan M. |title = Computing machinery and intelligence|journal = Mind|volume = LIX |issue = 238 |pages = 433–460 |doi = 10.1093/mind/LIX.236.433 |url = http://mind.oxfordjournals.org/content/LIX/236/433 |year = 1950 }}</ref> and the articles by [[Nils Aall Barricelli]] at the [[Institute for Advanced Study]] in [[Princeton, New Jersey]].<ref>{{cite journal |last = Barricelli |first = Nils Aall |year = 1954 |author-link = Nils Aall Barricelli |title = Esempi numerici di processi di evoluzione |journal = Methodos |pages = 45–68 }}</ref><ref>{{cite journal |last = Barricelli |first = Nils Aall |year = 1957 |author-link = Nils Aall Barricelli |title = Symbiogenetic evolution processes realized by artificial methods |journal = Methodos |pages = 143–182 }}</ref>\n\n[[Quantum Monte Carlo]], and more specifically [[Diffusion Monte Carlo|Diffusion Monte Carlo methods]] can also be interpreted as a mean field particle Monte Carlo approximation of [[Richard Feynman|Feynman]]-[[Mark Kac|Kac]] path integrals.<ref name=\"dp04\">{{cite book |last = Del Moral |first = Pierre|title = Feynman–Kac formulae. Genealogical and interacting particle approximations |year = 2004 |publisher = Springer |quote = Series: Probability and Applications |url = https://www.springer.com/mathematics/probability/book/978-0-387-20268-6 |page = 575 }}</ref><ref name=\"dmm002\">{{cite book\n | last1 = Del Moral | first1 = P.\n | last2 = Miclo | first2 = L.\n | contribution = Branching and interacting particle systems approximations of Feynman–Kac formulae with applications to non-linear filtering\n | contribution-url = http://archive.numdam.org/item/SPS_2000__34__1_0\n | doi = 10.1007/BFb0103798\n | mr = 1768060\n | pages = 1–145\n | publisher = Springer |location = Berlin\n | series = Lecture Notes in Mathematics\n | title = Séminaire de Probabilités, XXXIV\n | volume = 1729\n | year = 2000\n |isbn = 978-3-540-67314-9\n }}</ref><ref name=\"dmm00m\">{{cite journal|last1 = Del Moral|first1 = Pierre|last2 = Miclo|first2 = Laurent|title = A Moran particle system approximation of Feynman–Kac formulae.|journal = Stochastic Processes and Their Applications |year = 2000|volume = 86|issue = 2|pages = 193–216|doi = 10.1016/S0304-4149(99)00094-0}}</ref><ref name=\"dm-esaim03\">{{cite journal|last1 = Del Moral|first1 = Pierre|title = Particle approximations of Lyapunov exponents connected to Schrödinger operators and Feynman–Kac semigroups|journal = ESAIM Probability & Statistics|date = 2003|volume = 7|pages = 171–208|url = http://journals.cambridge.org/download.php?file=%2FPSS%2FPSS7%2FS1292810003000016a.pdf&code=a0dbaa7ffca871126dc05fe2f918880a|doi = 10.1051/ps:2003001}}</ref><ref name=\"caffarel1\">{{cite journal|last1 = Assaraf|first1 = Roland|last2 = Caffarel|first2 = Michel|last3 = Khelif|first3 = Anatole|title = Diffusion Monte Carlo Methods with a fixed number of walkers|journal = Phys. Rev. E|url = http://qmcchem.ups-tlse.fr/files/caffarel/31.pdf|date = 2000|volume = 61|issue = 4|pages = 4566–4575|doi = 10.1103/physreve.61.4566|bibcode = 2000PhRvE..61.4566A|deadurl = yes|archiveurl = https://web.archive.org/web/20141107015724/http://qmcchem.ups-tlse.fr/files/caffarel/31.pdf|archivedate = 2014-11-07 }}</ref><ref name=\"caffarel2\">{{cite journal|last1 = Caffarel|first1 = Michel|last2 = Ceperley|first2 = David |last3 = Kalos|first3 = Malvin|title = Comment on Feynman–Kac Path-Integral Calculation of the Ground-State Energies of Atoms|journal = Phys. Rev. Lett.|date = 1993|volume = 71|issue = 13|doi = 10.1103/physrevlett.71.2159|bibcode = 1993PhRvL..71.2159C|pages=2159|pmid=10054598}}</ref><ref name=\"h84\">{{cite journal |last = Hetherington|first = Jack, H.|title = Observations on the statistical iteration of matrices|journal = Phys. Rev. A |date = 1984|volume = 30|issue = 2713|doi = 10.1103/PhysRevA.30.2713|pages = 2713–2719|bibcode = 1984PhRvA..30.2713H}}</ref> The origins of Quantum Monte Carlo methods are often attributed to Enrico Fermi and [[Robert D. Richtmyer|Robert Richtmyer]] who developed in 1948 a mean field particle interpretation of neutron-chain reactions,<ref>{{cite journal|last1 = Fermi|first1 = Enrique|last2 = Richtmyer|first2 = Robert, D.|title = Note on census-taking in Monte Carlo calculations|journal = LAM|date = 1948|volume = 805|issue = A|url = http://scienze-como.uninsubria.it/bressanini/montecarlo-history/fermi-1948.pdf|quote = Declassified report Los Alamos Archive}}</ref> but the first heuristic-like and genetic type particle algorithm (a.k.a. Resampled or Reconfiguration Monte Carlo methods) for estimating ground state energies of quantum systems (in reduced matrix models) is due to Jack H. Hetherington in 1984<ref name=\"h84\" /> In molecular chemistry, the use of genetic heuristic-like particle methodologies (a.k.a. pruning and enrichment strategies) can be traced back to 1955 with the seminal work of [[Marshall Rosenbluth|Marshall. N. Rosenbluth]] and [[Arianna W. Rosenbluth|Arianna. W. Rosenbluth]].<ref name=\":0\">{{cite journal |last1 = Rosenbluth|first1 = Marshall, N.|last2 = Rosenbluth|first2 = Arianna, W.|title = Monte-Carlo calculations of the average extension of macromolecular chains|journal = J. Chem. Phys.|date = 1955|volume = 23|issue = 2|pages = 356–359|bibcode = 1955JChPh..23..356R|doi = 10.1063/1.1741967 }}</ref>\n\nThe use of [[Sequential Monte Carlo method|Sequential Monte Carlo]] in advanced [[signal processing]] and [[Bayesian inference]] is more recent. It was in 1993, that Gordon et al., published in their seminal work<ref>{{Cite journal|title = Novel approach to nonlinear/non-Gaussian Bayesian state estimation|url = https://ieeexplore.ieee.org/document/210672|journal = Radar and Signal Processing, IEE Proceedings F|date = April 1993|issn = 0956-375X|pages = 107–113|volume = 140|issue = 2|first = N.J.|last = Gordon|first2 = D.J.|last2 = Salmond|first3 = A.F.M.|last3 = Smith|doi=10.1049/ip-f-2.1993.0015}}</ref> the first application of a Monte Carlo [[Resampling (statistics)|resampling]] algorithm in Bayesian statistical inference. The authors named their algorithm 'the bootstrap filter', and demonstrated that compared to other filtering methods, their bootstrap algorithm does not require any assumption about that state-space or the noise of the system. We also quote another pioneering article in this field of Genshiro Kitagawa on a related \"Monte Carlo filter\",<ref>{{cite journal\n |last = Kitagawa|first = G.|year = 1996|title = Monte carlo filter and smoother for non-Gaussian nonlinear state space models|volume = 5|issue = 1|journal = Journal of Computational and Graphical Statistics|pages = 1–25|doi = 10.2307/1390750|jstor = 1390750}}\n</ref> and the ones by Pierre Del Moral<ref name=\"dm9622\">{{cite journal|last1 = Del Moral|first1 = Pierre|title = Non Linear Filtering: Interacting Particle Solution.|journal = Markov Processes and Related Fields|date = 1996|volume = 2|issue = 4|pages = 555–580|url = http://web.maths.unsw.edu.au/~peterdel-moral/mprfs.pdf}}</ref> and Himilcon Carvalho, Pierre Del Moral, André Monin and Gérard Salut<ref>{{cite journal|last1 = Carvalho|first1 = Himilcon|last2 = Del Moral|first2 = Pierre|last3 = Monin|first3 = André|last4 = Salut|first4 = Gérard|title = Optimal Non-linear Filtering in GPS/INS Integration.|journal = IEEE Transactions on Aerospace and Electronic Systems|date = July 1997|volume = 33|issue = 3|url = http://homepages.laas.fr/monin/Version_anglaise/Publications_files/GPS.pdf}}</ref> on particle filters published in the mid-1990s. Particle filters were also developed in signal processing in the early 1989-1992 by P. Del Moral, J.C. Noyer, G. Rigal, and G. Salut in the LAAS-CNRS in a series of restricted and classified research reports with STCAN (Service Technique des Constructions et Armes Navales), the IT company DIGILOG, and the [https://www.laas.fr/public/en LAAS-CNRS] (the Laboratory for Analysis and Architecture of Systems) on RADAR/SONAR and GPS signal processing problems.<ref>P. Del Moral, G. Rigal, and G. Salut. Estimation and nonlinear optimal control : An unified framework for particle solutions <br>\nLAAS-CNRS, Toulouse, Research Report no. 91137, DRET-DIGILOG- LAAS/CNRS contract, April (1991).</ref><ref>P. Del Moral, G. Rigal, and G. Salut. Nonlinear and non Gaussian particle filters applied to inertial platform repositioning.<br>\nLAAS-CNRS, Toulouse, Research Report no. 92207, STCAN/DIGILOG-LAAS/CNRS Convention STCAN no. A.91.77.013, (94p.) September (1991).</ref><ref>P. Del Moral, G. Rigal, and G. Salut. Estimation and nonlinear optimal control : Particle resolution in filtering and estimation. Experimental results.<br>\nConvention DRET no. 89.34.553.00.470.75.01, Research report no.2 (54p.), January (1992).</ref><ref>P. Del Moral, G. Rigal, and G. Salut. Estimation and nonlinear optimal control : Particle resolution in filtering and estimation. Theoretical results <br>\nConvention DRET no. 89.34.553.00.470.75.01, Research report no.3 (123p.), October (1992).</ref><ref>P. Del Moral, J.-Ch. Noyer, G. Rigal, and G. Salut. Particle filters in radar signal processing : detection, estimation and air targets recognition.<br>\nLAAS-CNRS, Toulouse, Research report no. 92495, December (1992).</ref><ref>P. Del Moral, G. Rigal, and G. Salut. Estimation and nonlinear optimal control : Particle resolution in filtering and estimation. <br>\nStudies on: Filtering, optimal control, and maximum likelihood estimation. Convention DRET no. 89.34.553.00.470.75.01. Research report no.4 (210p.), January (1993).</ref> These Sequential Monte Carlo methodologies can be interpreted as an acceptance-rejection sampler equipped with an interacting recycling mechanism.\n\nFrom 1950 to 1996, all the publications on Sequential Monte Carlo methodologies including the pruning and resample Monte Carlo methods introduced in computational physics and molecular chemistry, present natural and heuristic-like algorithms applied to different situations without a single proof of their consistency, nor a discussion on the bias of the estimates and on genealogical and ancestral tree based algorithms. The mathematical foundations and the first rigorous analysis of these particle algorithms are due to Pierre Del Moral<ref name=\"dm9622\"/><ref name=\":22\">{{cite journal|last1 = Del Moral|first1 = Pierre|title = Measure Valued Processes and Interacting Particle Systems. Application to Non Linear Filtering Problems|journal = Annals of Applied Probability|date = 1998|edition = Publications du Laboratoire de Statistique et Probabilités, 96-15 (1996)|volume = 8|issue = 2|pages = 438–495|url = http://projecteuclid.org/download/pdf_1/euclid.aoap/1028903535|doi = 10.1214/aoap/1028903535|citeseerx = 10.1.1.55.5257}}</ref> in 1996. Branching type particle methodologies with varying population sizes were also developed in the end of the 1990s by Dan Crisan, Jessica Gaines and Terry Lyons,<ref name=\":42\">{{cite journal|last1 = Crisan|first1 = Dan|last2 = Gaines|first2 = Jessica|last3 = Lyons|first3 = Terry|title = Convergence of a branching particle method to the solution of the Zakai|journal = SIAM Journal on Applied Mathematics|date = 1998|volume = 58|issue = 5|pages = 1568–1590|doi = 10.1137/s0036139996307371}}</ref><ref>{{cite journal|last1 = Crisan|first1 = Dan|last2 = Lyons|first2 = Terry|title = Nonlinear filtering and measure-valued processes|journal = Probability Theory and Related Fields|date = 1997|volume = 109|issue = 2|pages = 217–244|doi = 10.1007/s004400050131}}</ref><ref>{{cite journal|last1 = Crisan|first1 = Dan|last2 = Lyons|first2 = Terry|title = A particle approximation of the solution of the Kushner–Stratonovitch equation|journal = Probability Theory and Related Fields|date = 1999|volume = 115|issue = 4|pages = 549–578|doi = 10.1007/s004400050249}}</ref> and by Dan Crisan, Pierre Del Moral and Terry Lyons.<ref name=\":52\">{{cite journal|last1 = Crisan|first1 = Dan|last2 = Del Moral|first2 = Pierre|last3 = Lyons|first3 = Terry|title = Discrete filtering using branching and interacting particle systems|journal = Markov Processes and Related Fields|date = 1999|volume = 5|issue = 3|pages = 293–318|url = http://web.maths.unsw.edu.au/~peterdel-moral/crisan98discrete.pdf}}</ref> Further developments in this field were developed in 2000 by P. Del Moral, A. Guionnet and L. Miclo.<ref name=\"dmm002\"/><ref name=\"dg99\">{{cite journal|last1 = Del Moral|first1 = Pierre|last2 = Guionnet|first2 = Alice|title = On the stability of Measure Valued Processes with Applications to filtering|journal = C. R. Acad. Sci. Paris|date = 1999|volume = 39|issue = 1|pages = 429–434}}</ref><ref name=\"dg01\">{{cite journal|last1 = Del Moral|first1 = Pierre|last2 = Guionnet|first2 = Alice|title = On the stability of interacting processes with applications to filtering and genetic algorithms|journal = Annales de l'Institut Henri Poincaré|date = 2001|volume = 37|issue = 2|pages = 155–194|url = http://web.maths.unsw.edu.au/~peterdel-moral/ihp.ps|doi = 10.1016/s0246-0203(00)01064-5|bibcode=2001AnIHP..37..155D}}</ref>\n\n==Definitions==\n\nThere is no consensus on how ''Monte Carlo'' should be defined. For example, Ripley<ref name=Ripley>{{harvnb|Ripley|1987}}</ref> defines most probabilistic modeling as ''[[stochastic simulation]]'', with ''Monte Carlo'' being reserved for [[Monte Carlo integration]] and Monte Carlo statistical tests. [[Shlomo Sawilowsky|Sawilowsky]]<ref name=Sawilowsky>{{harvnb|Sawilowsky|2003}}</ref> distinguishes between a [[simulation]], a Monte Carlo method, and a Monte Carlo simulation: a simulation is a fictitious representation of reality, a Monte Carlo method is a technique that can be used to solve a mathematical or statistical problem, and a Monte Carlo simulation uses repeated sampling to obtain the statistical properties of some phenomenon (or behavior). Examples:\n*Simulation: Drawing '''one''' pseudo-random uniform variable from the interval [0,1] can be used to simulate the tossing of a coin: If the value is less than or equal to 0.50 designate the outcome as heads, but if the value is greater than 0.50 designate the outcome as tails. This is a simulation, but not a Monte Carlo simulation.\n*Monte Carlo method: Pouring out a box of coins on a table, and then computing the ratio of coins that land heads versus tails is a Monte Carlo method of determining the behavior of repeated coin tosses, but it is not a simulation.\n*Monte Carlo simulation: Drawing '''a large number''' of pseudo-random uniform variables from the interval [0,1] at one time, or once at a large number of different times, and assigning values less than or equal to 0.50 as heads and greater than 0.50 as tails, is a ''Monte Carlo simulation'' of the behavior of repeatedly tossing a coin.\n\nKalos and Whitlock<ref name=\"Kalos\">{{harvnb|Kalos|Whitlock|2008}}</ref> point out that such distinctions are not always easy to maintain. For example, the emission of radiation from atoms is a natural stochastic process. It can be simulated directly, or its average behavior can be described by stochastic equations that can themselves be solved using Monte Carlo methods. \"Indeed, the same computer code can be viewed simultaneously as a 'natural simulation' or as a solution of the equations by natural sampling.\"\n\n===Monte Carlo and random numbers===\n\nThe main idea behind this method is that the results are computed based on repeated random sampling and statistical analysis. The Monte Carlo simulation is, in fact, random experimentations, in the case that, the results of these experiments are not well known.\nMonte Carlo simulations are typically characterized by a large number of unknown parameters, many of which are difficult to obtain experimentally.<ref name=\"usaus\">{{cite journal|last1 = Shojaeefard|first1 = MH| last2 = Khalkhali|first2 = A| last3 =Yarmohammadisatri|first3 = Sadegh|title = An efficient sensitivity analysis method for modified geometry of Macpherson suspension based on Pearson Correlation Coefficient|journal = Vehicle System Dynamics|volume = 55|issue = 6|pages = 827–852|doi = 10.1080/00423114.2017.1283046|year = 2017}}</ref> Monte Carlo simulation methods do not always require [[Random number generation#\"True\" random numbers vs. pseudo-random numbers|truly random number]]s to be useful (although, for some applications such as [[primality testing]], unpredictability is vital).<ref>{{harvnb|Davenport|1992}}</ref> Many of the most useful techniques use deterministic, [[pseudorandom number generator|pseudorandom]] sequences, making it easy to test and re-run simulations. The only quality usually necessary to make good [[simulation]]s is for the pseudo-random sequence to appear \"random enough\" in a certain sense.\n\nWhat this means depends on the application, but typically they should pass a series of statistical tests. Testing that the numbers are [[Uniform distribution (continuous)|uniformly distributed]] or follow another desired distribution when a large enough number of elements of the sequence are considered is one of the simplest and most common ones. Weak correlations between successive samples are also often desirable/necessary.\n\nSawilowsky lists the characteristics of a high-quality Monte Carlo simulation:<ref name=Sawilowsky/>\n*the (pseudo-random) number generator has certain characteristics (''e.g.'', a long \"period\" before the sequence repeats)\n*the (pseudo-random) number generator produces values that pass tests for randomness\n*there are enough samples to ensure accurate results\n*the proper sampling technique is used\n*the algorithm used is valid for what is being modeled\n*it simulates the phenomenon in question.\n\n[[Pseudo-random number sampling]] algorithms are used to transform uniformly distributed pseudo-random numbers into numbers that are distributed according to a given [[probability distribution]].\n\n[[Low-discrepancy sequences]] are often used instead of random sampling from a space as they ensure even coverage and normally have a faster order of convergence than Monte Carlo simulations using random or pseudorandom sequences. Methods based on their use are called [[quasi-Monte Carlo method]]s.\n\nIn an effort to assess the impact of random number quality on Monte Carlo simulation outcomes, astrophysical researchers tested cryptographically-secure pseudorandom numbers generated via Intel's [[RdRand]] instruction set, as compared to those derived from algorithms, like the [[Mersenne Twister]], in Monte Carlo simulations of radio flares from [[brown dwarfs]].  RdRand is the closest pseudorandom number generator to a true random number generator.  No statistically significant difference was found between models generated with typical pseudorandom number generators and RdRand for trials consisting of the generation of 10<sup>7</sup> random numbers.<ref>{{cite journal|last1=Route|first1=Matthew|title=Radio-flaring Ultracool Dwarf Population Synthesis|journal=The Astrophysical Journal|date=August 10, 2017|volume=845|issue=1|page=66|doi=10.3847/1538-4357/aa7ede|arxiv=1707.02212|bibcode=2017ApJ...845...66R}}</ref>\n\n=== Monte Carlo simulation versus \"what if\" scenarios ===\nThere are ways of using probabilities that are definitely not Monte Carlo simulations &mdash; for example, deterministic modeling using single-point estimates. Each uncertain variable within a model is assigned a \"best guess\" estimate.  Scenarios (such as best, worst, or most likely case) for each input variable are chosen and the results recorded.<ref>{{harvnb|Vose|2000|page=13}}</ref>\n\nBy contrast, Monte Carlo simulations sample from a [[probability distribution]] for each variable to produce hundreds or thousands of possible outcomes. The results are analyzed to get probabilities of different outcomes occurring.<ref>{{harvnb|Vose|2000|page=16}}</ref> For example, a comparison of a spreadsheet cost construction model run using traditional \"what if\" scenarios, and then running the comparison again with Monte Carlo simulation and [[triangular distribution|triangular probability distribution]]s shows that the Monte Carlo analysis has a narrower range than the \"what if\" analysis.{{Examples|date=May 2012}}  This is because the \"what if\" analysis gives equal weight to all scenarios (see [[Corporate finance#Quantifying uncertainty|quantifying uncertainty in corporate finance]]), while the Monte Carlo method hardly samples in the very low probability regions. The samples in such regions are called \"rare events\".\n\n==Applications==\nMonte Carlo methods are especially useful for simulating phenomena with significant [[uncertainty]] in inputs and systems with a large number of [[coupling (physics)|coupled]] degrees of freedom. Areas of application include:\n\n===Physical sciences===\n{{Computational physics}}\n{{See also|Monte Carlo method in statistical physics}}\nMonte Carlo methods are very important in [[computational physics]], [[physical chemistry]], and related applied fields, and have diverse applications from complicated [[quantum chromodynamics]] calculations to designing [[heat shield]]s and [[aerodynamics|aerodynamic]] forms as well as in modeling radiation transport for radiation dosimetry calculations.<ref>{{cite journal | doi = 10.1088/0031-9155/59/4/R151 | pmid=24486639 | volume=59 | issue=4 | title=GPU-based high-performance computing for radiation therapy | journal=Physics in Medicine and Biology | pages=R151–R182|bibcode = 2014PMB....59R.151J | year=2014 | last1=Jia | first1=Xun | last2=Ziegenhein | first2=Peter | last3=Jiang | first3=Steve B | pmc=4003902 }}</ref><ref>{{cite journal | doi = 10.1088/0031-9155/59/6/R183 | volume=59 | issue=6 | title=Advances in kilovoltage x-ray beam dosimetry | journal=Physics in Medicine and Biology | pages=R183–R231|bibcode = 2014PMB....59R.183H | pmid=24584183 | date=Mar 2014| author1=Hill | first1=R | last2=Healy | first2=B | last3=Holloway | first3=L | last4=Kuncic | first4=Z | last5=Thwaites | first5=D | last6=Baldock | first6=C }}</ref><ref>{{cite journal | doi = 10.1088/0031-9155/51/13/R17 | pmid=16790908 | volume=51 | issue=13 | title=Fifty years of Monte Carlo simulations for medical physics | journal=Physics in Medicine and Biology | pages=R287–R301|bibcode = 2006PMB....51R.287R | year=2006 | last1=Rogers | first1=D W O }}</ref>  In [[statistical physics]] [[Monte Carlo molecular modeling]] is an alternative to computational [[molecular dynamics]], and Monte Carlo methods are used to compute [[statistical field theory|statistical field theories]] of simple particle and polymer systems.<ref name=\":0\" /><ref>{{harvnb|Baeurle|2009}}</ref>  [[Quantum Monte Carlo]] methods solve the [[many-body problem]] for quantum systems.<ref name=\"kol10\" /><ref name=\"dp13\" /><ref name=\"dp04\" /> In [[Radiation material science|radiation materials science]], the [[binary collision approximation]] for simulating [[ion implantation]] is usually based on a Monte Carlo approach to select the next colliding atom.<ref>{{Cite journal|last=Möller|first=W.|last2=Eckstein|first2=W.|date=1984-03-01|title=Tridyn — A TRIM simulation code including dynamic composition changes|url=http://www.sciencedirect.com/science/article/pii/0168583X84903215|journal=Nuclear Instruments and Methods in Physics Research Section B: Beam Interactions with Materials and Atoms|volume=2|issue=1|pages=814–818|doi=10.1016/0168-583X(84)90321-5|bibcode=1984NIMPB...2..814M}}</ref> In experimental [[particle physics]], Monte Carlo methods are used for designing [[particle detector|detectors]], understanding their behavior and comparing experimental data to theory. In [[astrophysics]], they are used in such diverse manners as to model both [[galaxy]] evolution<ref>{{harvnb|MacGillivray|Dodd|1982}}</ref> and microwave radiation transmission through a rough planetary surface.<ref>{{harvnb|Golden|1979}}</ref> Monte Carlo methods are also used in the [[Ensemble forecasting|ensemble models]] that form the basis of modern [[Numerical weather prediction|weather forecasting]].\n\n===Engineering===\nMonte Carlo methods are widely used in engineering for [[sensitivity analysis]] and quantitative [[probabilistic]] analysis in [[Process design (chemical engineering)|process design]]. The need arises from the interactive, co-linear and non-linear behavior of typical process simulations. For example,\n* In [[microelectronics|microelectronics engineering]], Monte Carlo methods are applied to analyze correlated and uncorrelated variations in [[Analog signal|analog]] and [[Digital data|digital]] [[integrated circuits]].\n* In [[geostatistics]] and [[geometallurgy]], Monte Carlo methods underpin the design of [[mineral processing]] [[process flow diagram|flowsheets]] and contribute to quantitative [[Quantitative risk analysis|risk analysis]].<ref name=\"mbv01\">{{Cite book | last =Mazhdrakov | first =Metodi | last2 =Benov | first2 =Dobriyan |last3=Valkanov|first3=Nikolai | year =2018 | title =The Monte Carlo Method. Engineering Applications | publisher =ACMO Academic Press | volume = | pages = 250| isbn =978-619-90684-3-4 | doi =   | ref =harv|url=https://books.google.com/?id=t0BqDwAAQBAJ&dq=the+monte+carlo+method+engineering+applications+mazhdrakov}}</ref>\n* In [[wind energy]] yield analysis, the predicted energy output of a wind farm during its lifetime is calculated giving different levels of uncertainty ([[Percentile|P90]], P50, etc.)\n* impacts of pollution are simulated<ref name=\"IntPanis1\">{{harvnb|Int Panis|De Nocker|De Vlieger|Torfs|2001}}</ref> and diesel compared with petrol.<ref name=\"IntPanis2\">{{harvnb|Int Panis|Rabl|De Nocker|Torfs|2002}}</ref>\n* In [[fluid dynamics]], in particular [[gas dynamics|rarefied gas dynamics]], where the Boltzmann equation is solved for finite Knudsen number fluid flows using the [[direct simulation Monte Carlo]] <ref>G. A. Bird, Molecular Gas Dynamics, Clarendon, Oxford (1976)</ref> method in combination with highly efficient computational algorithms.<ref>{{cite journal | last1 = Dietrich | first1 = S. | last2 = Boyd | first2 = I. | year = 1996 | title = A Scalar optimized parallel implementation of the DSMC technique | url = | journal = Journal of Computational Physics | volume = 126 | issue = 2| pages = 328–42 | doi=10.1006/jcph.1996.0141|bibcode = 1996JCoPh.126..328D }}</ref>\n* In [[autonomous robotics]], [[Monte Carlo localization]] can determine the position of a robot. It is often applied to stochastic filters such as the [[Kalman filter]] or [[particle filter]] that forms the heart of the [[Simultaneous localization and mapping|SLAM]] (simultaneous localization and mapping) algorithm.\n* In [[telecommunications]], when planning a wireless network, design must be proved to work for a wide variety of scenarios that depend mainly on the number of users, their locations and the services they want to use. Monte Carlo methods are typically used to generate these users and their states. The network performance is then evaluated and, if results are not satisfactory, the network design goes through an optimization process.\n* In [[reliability engineering]], Monte Carlo simulation is used to compute system-level response given the component-level response. For example, for a transportation network subject to an earthquake event, Monte Carlo simulation can be used to assess the ''k''-terminal reliability of the network given the failure probability of its components, e.g. bridges, roadways, etc.<ref>{{cite journal|last=Nabian|first=Mohammad Amin|last2=Meidani|first2=Hadi|date=2017-08-28|title=Deep Learning for Accelerated Reliability Analysis of Infrastructure Networks|journal=Computer-Aided Civil and Infrastructure Engineering|volume=33|issue=6|pages=443–458|arxiv=1708.08551|doi=10.1111/mice.12359}}</ref><ref>{{Cite journal|last=Nabian|first=Mohammad Amin|last2=Meidani|first2=Hadi|date=2018|title=Accelerating Stochastic Assessment of Post-Earthquake Transportation Network Connectivity via Machine-Learning-Based Surrogates|url=https://trid.trb.org/view/1496617|journal=Transportation Research Board 97th Annual Meeting|volume=|pages=|via=}}</ref><ref>{{Cite journal|last=Nabian|first=Mohammad Amin|last2=Meidani|first2=Hadi|date=2017|title=Uncertainty Quantification and PCA-Based Model Reduction for Parallel Monte Carlo Analysis of Infrastructure System Reliability|url=https://trid.trb.org/view/1439614|journal=Transportation Research Board 96th Annual Meeting|volume=|pages=|via=}}</ref>\n* In [[signal processing]] and [[Bayesian inference]], [[particle filter]]s and [[Sequential Monte Carlo method|sequential Monte Carlo techniques]] are a class of [[mean field particle methods]] for sampling and computing the posterior distribution of a signal process given some noisy and partial observations using interacting [[empirical measure]]<nowiki/>s.\n\n===Climate change and radiative forcing===\n\nThe [[IPCC|Intergovernmental Panel on Climate Change]] relies on Monte Carlo methods in [[probability density function]] analysis of [[radiative forcing]].\n\n{{Quote|text=Probability density function (PDF) of ERF due to total GHG, aerosol forcing and total anthropogenic forcing. The GHG consists of WMGHG, ozone and stratospheric water vapour. The PDFs are generated based on uncertainties provided in Table 8.6. The combination of the individual RF agents to derive total forcing over the Industrial Era are done by Monte Carlo simulations and based on the method in Boucher and Haywood (2001). PDF of the ERF from surface albedo changes and combined contrails and contrail-induced cirrus are included in the total anthropogenic forcing, but not shown as a separate PDF. We currently do not have ERF estimates for some forcing mechanisms: ozone, land use, solar, etc.<ref>{{cite book|title=Climate Change 2013 The Physical Science Basis|date=2013|publisher=Cambridge University Press|isbn=978-1-107-66182-0|page=697|url=http://www.climatechange2013.org/images/report/WG1AR5_ALL_FINAL.pdf|accessdate=2 March 2016}}</ref>}}\n\n===Computational biology===\n\nMonte Carlo methods are used in various fields of [[computational biology]], for example for [[Bayesian inference in phylogeny]], or for studying biological systems such as genomes, proteins,<ref>{{harvnb|Ojeda|et al.|2009}},</ref> or membranes.<ref>{{harvnb|Milik|Skolnick|1993}}</ref>\nThe systems can be studied in the coarse-grained or ''ab initio'' frameworks depending on the desired accuracy. \nComputer simulations allow us to monitor the local environment of a particular [[biomolecule|molecule]] to see if some [[chemical reaction]] is happening for instance. In cases where it is not feasible to conduct a physical experiment, [[thought experiment]]s can be conducted (for instance: breaking bonds, introducing impurities at specific sites, changing the local/global structure, or introducing external fields).\n\n===Computer graphics===\n[[Path tracing]], occasionally referred to as Monte Carlo ray tracing, renders a 3D scene by randomly tracing samples of possible light paths. Repeated sampling of any given pixel will eventually cause the average of the samples to converge on the correct solution of the [[rendering equation]], making it one of the most physically accurate 3D graphics rendering methods in existence.\n\n===Applied statistics===\nThe standards for Monte Carlo experiments in statistics were set by Sawilowsky.<ref>{{cite journal | last1 = Cassey | last2 = Smith | year = 2014 | title = Simulating confidence for the Ellison-Glaeser Index | url = | journal = Journal of Urban Economics | volume = 81 | issue = | page = 93 | doi =  10.1016/j.jue.2014.02.005}}</ref><ref>Grissom & Kim (2005), p. 131</ref> In applied statistics, Monte Carlo methods may be used for at least four purposes:\n#To compare competing statistics for small samples under realistic data conditions. Although [[type I error]] and power properties of statistics can be calculated for data drawn from classical theoretical distributions (''e.g.'', [[normal curve]], [[Cauchy distribution]]) for [[asymptotic]] conditions (''i. e'', infinite sample size and infinitesimally small treatment effect), real data often do not have such distributions.<ref>{{harvnb|Sawilowsky|Fahoome|2003}}</ref>\n#To provide implementations of [[Statistical hypothesis testing|hypothesis tests]] that are more efficient than exact tests such as [[permutation tests]] (which are often impossible to compute) while being more accurate than critical values for [[asymptotic distribution]]s.\n#To provide a random sample from the posterior distribution in [[Bayesian inference]]. This sample then approximates and summarizes all the essential  features of the posterior.\n#To provide efficient random estimates of the Hessian matrix of the negative log-likelihood function that may be averaged to form an estimate of the [[Fisher information]] matrix.<ref>Spall, J. C. (2005), “Monte Carlo Computation of the Fisher Information Matrix in Nonstandard Settings,” ''Journal of Computational and Graphical Statistics'', vol. 14(4), pp. 889–909. http://dx.doi.org/10.1198/106186005X78800</ref><ref>Das, S., Spall, J. C., and Ghanem, R. (2010), “Efficient Monte Carlo Computation of Fisher Information Matrix Using Prior Information,” ''Computational Statistics and Data Analysis'', vol. 54(2), pp. 272–289. http://dx.doi.org/10.1016/j.csda.2009.09.018</ref> \n\nMonte Carlo methods are also a compromise between approximate randomization and permutation tests. An approximate [[randomization test]] is based on a specified subset of all permutations (which entails potentially enormous housekeeping of which permutations have been considered). The Monte Carlo approach is based on a specified number of randomly drawn permutations (exchanging a minor loss in precision if a permutation is drawn twice—or more frequently—for the efficiency of not having to track which permutations have already been selected).\n\n{{anchor|Monte Carlo tree search}}\n\n===Artificial intelligence for games===\n{{Main|Monte Carlo tree search}}\nMonte Carlo methods have been developed into a technique called [[Monte-Carlo tree search]] that is useful for searching for the best move in a game.  Possible moves are organized in a [[search tree]] and a large number of random simulations are used to estimate the long-term potential of each move. A black box simulator represents the opponent's moves.<ref>{{cite web|url=http://sander.landofsand.com/publications/Monte-Carlo_Tree_Search_-_A_New_Framework_for_Game_AI.pdf|title=Monte-Carlo Tree Search: A New Framework for Game AI|author1=Guillaume Chaslot|author2=Sander Bakkes|author3=Istvan Szita|author4=Pieter Spronck|website=Sander.landofsand.com|accessdate=28 October 2017}}</ref>\n\nThe Monte Carlo tree search (MCTS) method has four steps:<ref>{{cite web|url=http://mcts.ai/about/index.html|title=Monte Carlo Tree Search - About}}</ref>\n#Starting at root node of the tree, select optimal child nodes until a leaf node is reached.\n#Expand the leaf node and choose one of its children.\n#Play a simulated game starting with that node.\n#Use the results of that simulated game to update the node and its ancestors.\n\nThe net effect, over the course of many simulated games, is that the value of a node representing a move will go up or down, hopefully corresponding to whether or not that node represents a good move.\n\nMonte Carlo Tree Search has been used successfully to play games such as [[Go (game)|Go]],<ref>{{cite book|title=Parallel Monte-Carlo Tree Search| doi=10.1007/978-3-540-87608-3_6|volume=5131|pages=60–71|series=Lecture Notes in Computer Science|year=2008|last1=Chaslot|first1=Guillaume M. J. -B|last2=Winands|first2=Mark H. M|last3=Van Den Herik|first3=H. Jaap|isbn=978-3-540-87607-6|citeseerx = 10.1.1.159.4373}}</ref> [[Tantrix]],<ref>{{cite report|url=https://www.tantrix.com/Tantrix/TRobot/MCTS%20Final%20Report.pdf|title=Monte-Carlo Tree Search in the game of Tantrix: Cosc490 Final Report|last=Bruns|first=Pete}}</ref> [[Battleship (game)|Battleship]],<ref>{{cite web|url=http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Publications_files/pomcp.pdf|title=Monte-Carlo Planning in Large POMDPs|author1=David Silver|author2=Joel Veness|website=0.cs.ucl.ac.uk|accessdate=28 October 2017}}</ref> [[Havannah]],<ref>{{cite book|chapter=Improving Monte–Carlo Tree Search in Havannah| doi=10.1007/978-3-642-17928-0_10|volume=6515|pages=105–115|bibcode=2011LNCS.6515..105L|series=Lecture Notes in Computer Science|year=2011|last1=Lorentz|first1=Richard J|title=Computers and Games|isbn=978-3-642-17927-3}}</ref> and [[Arimaa]].<ref>{{cite web|url=http://www.arimaa.com/arimaa/papers/ThomasJakl/bc-thesis.pdf|author=Tomas Jakl|title=Arimaa challenge – comparison study of MCTS versus alpha-beta methods|website=Arimaa.com|accessdate=28 October 2017}}</ref>\n\n{{See also|Computer Go}}\n\n===Design and visuals===\nMonte Carlo methods are also efficient in solving coupled integral differential equations of radiation fields and energy transport, and thus these methods have been used in [[global illumination]] computations that produce photo-realistic images of virtual 3D models, with applications in [[video game]]s, [[architecture]], [[design]], computer generated [[film]]s, and cinematic special effects.<ref>{{harvnb|Szirmay–Kalos|2008}}</ref>\n\n===Search and rescue===\nThe [[US Coast Guard]] utilizes Monte Carlo methods within its computer modeling software [[SAROPS]] in order to calculate the probable locations of vessels during [[search and rescue]] operations. Each simulation can generate as many as ten thousand data points that are randomly distributed based upon provided variables.<ref>{{cite web|url=http://insights.dice.com/2014/01/03/how-the-coast-guard-uses-analytics-to-search-for-those-lost-at-sea|title=How the Coast Guard Uses Analytics to Search for Those Lost at Sea|work=Dice Insights}}</ref> Search patterns are then generated based upon extrapolations of these data in order to optimize the probability of containment (POC) and the probability of detection (POD), which together will equal an overall probability of success (POS). Ultimately this serves as a practical application of [[probability distribution]] in order to provide the swiftest and most expedient method of rescue, saving both lives and resources.<ref>{{cite web|url=http://www.ifremer.fr/web-com/sar2011/Presentations/SARWS2011_STONE_L.pdf|title=Search Modeling and Optimization in USCG's Search and Rescue Optimal Planning System (SAROPS)|author1=Lawrence D. Stone|author2=Thomas M. Kratzke|author3=John R. Frost|website=Ifremer.fr|accessdate=28 October 2017}}</ref>\n\n===Finance and business===\n{{See also|Monte Carlo methods in finance| Quasi-Monte Carlo methods in finance| Monte Carlo methods for option pricing| Stochastic modelling (insurance) | Stochastic asset model}}\nMonte Carlo simulation is commonly used to evaluate the risk and uncertainty that would affect the outcome of different decision options. Monte Carlo simulation allows the business risk analyst to incorporate the total effects of uncertainty in variables like sales volume, commodity and labour prices, interest and exchange rates, as well as the effect of distinct risk events like the cancellation of a contract or the change of a tax law.\n\n[[Monte Carlo methods in finance]] are often used to [[Corporate finance#Quantifying uncertainty|evaluate investments in projects]] at a business unit or corporate level, or to evaluate [[derivative (finance)|financial derivatives]]. They can be used to model [[project management|project schedules]], where simulations aggregate estimates for worst-case, best-case, and most likely durations for each task to determine outcomes for the overall project.[https://risk.octigo.pl/] Monte Carlo methods are also used in option pricing, default risk analysis.<ref>{{Cite book|title = An Introduction to Particle Methods with Financial Applications|publisher = Springer Berlin Heidelberg|journal = Numerical Methods in Finance|date = 2012|isbn = 978-3-642-25745-2|pages = 3–49|series = Springer Proceedings in Mathematics|volume = 12|first = René|last = Carmona|first2 = Pierre|last2 = Del Moral|first3 = Peng|last3 = Hu|first4 = Nadia|last4 = Oudjane|editor-first = René A.|editor-last = Carmona|editor2-first = Pierre Del|editor2-last = Moral|editor3-first = Peng|editor3-last = Hu|editor4-first = Nadia|display-editors = 3 |editor4-last = Oudjane|doi=10.1007/978-3-642-25746-9_1|citeseerx = 10.1.1.359.7957}}</ref><ref>{{Cite book|title = Numerical Methods in Finance - Springer|journal = Link.springer.com|volume = 12|doi=10.1007/978-3-642-25746-9|series = Springer Proceedings in Mathematics|year = 2012|isbn = 978-3-642-25745-2}}</ref><ref name=\"kr11\">{{cite book|last1 = Kroese|first1 = D. P.|last2 = Taimre|first2 = T.|last3 = Botev|first3 = Z. I. |title = Handbook of Monte Carlo Methods|year = 2011|publisher = John Wiley & Sons}}</ref> Additionally, they can be used to estimate the financial impact of medical interventions.<ref>Arenas, Daniel J.; Lett, Lanair A.; Klusaritz, Heather; Teitelman, Anne M.; van Wouwe, Jacobus P. (28 December 2017). \"A Monte Carlo simulation approach for estimating the health and economic impact of interventions provided at a student-run clinic\". PLOS ONE. 12 (12): e0189718. |  https://doi.org/10.1371/journal.pone.0189718</ref>\n\n===Law===\nA Monte Carlo approach was used for evaluating the potential value of a proposed program to help female petitioners in Wisconsin be successful in their applications for [[Harassment Restraining Order|harassment]] and [[Domestic Abuse Restraining Order|domestic abuse restraining orders]].  It was proposed to help women succeed in their petitions by providing them with greater advocacy thereby potentially reducing the risk of [[rape]] and [[physical assault]].  However, there were many variables in play that could not be estimated perfectly, including the effectiveness of restraining orders, the success rate of petitioners both with and without advocacy, and many others.  The study ran trials that varied these variables to come up with an overall estimate of the success level of the proposed program as a whole.<ref>{{cite web|url=http://legalaidresearch.org/wp-content/uploads/Research-Increasing-Access-to-REstraining-Order-for-Low-Income-Victims-of-DV-A-Cost-Benefit-Analysis-of-the-Proposed-Domestic-Abuse-Grant-Program.pdf| title=Increasing Access to Restraining Orders for Low Income Victims of Domestic Violence: A Cost-Benefit Analysis of the Proposed Domestic Abuse Grant Program |publisher=[[State Bar of Wisconsin]] |date=December 2006 |accessdate=2016-12-12|last1=Elwart|first1=Liz|last2=Emerson|first2=Nina|last3=Enders|first3=Christina|last4=Fumia|first4=Dani|last5=Murphy|first5=Kevin}}</ref>\n\n==Use in mathematics==\nIn general, the Monte Carlo methods are used in mathematics to solve various problems by generating suitable random numbers (see also [[Random number generation]]) and observing that fraction of the numbers that obeys some property or properties. The method is useful for obtaining numerical solutions to problems too complicated to solve analytically.  The most common application of the Monte Carlo method is Monte Carlo integration.\n\n=== Integration ===\n{{Main|Monte Carlo integration}}\n\n[[File:Monte-carlo2.gif|thumb|Monte-Carlo integration works by comparing random points with the value of the function]]\n[[File:Monte-Carlo method (errors).png|thumb|Errors reduce by a factor of <math>\\scriptstyle 1/\\sqrt{N}</math>]]\n\nDeterministic [[numerical integration]] algorithms work well in a small number of dimensions, but encounter two problems when the functions have many variables. First, the number of function evaluations needed increases rapidly with the number of dimensions. For example, if 10 evaluations provide adequate accuracy in one dimension, then [[googol|10<sup>100</sup>]] points are needed for 100 dimensions—far too many to be computed. This is called the [[curse of dimensionality]]. Second, the boundary of a multidimensional region may be very complicated, so it may not be feasible to reduce the problem to an [[iterated integral]].<ref name=Press>{{harvnb|Press|Teukolsky|Vetterling|Flannery|1996}}</ref> 100 [[dimension]]s is by no means unusual, since in many physical problems, a \"dimension\" is equivalent to a [[degrees of freedom (physics and chemistry)|degree of freedom]].\n\nMonte Carlo methods provide a way out of this exponential increase in computation time. As long as the function in question is reasonably [[well-behaved]], it can be estimated by randomly selecting points in 100-dimensional space, and taking some kind of average of the function values at these points. By the [[central limit theorem]], this method displays <math>\\scriptstyle 1/\\sqrt{N}</math> convergence—i.e., quadrupling the number of sampled points halves the error, regardless of the number of dimensions.<ref name=Press/>\n\nA refinement of this method, known as [[importance sampling]] in statistics, involves sampling the points randomly, but more frequently where the integrand is large. To do this precisely one would have to already know the integral, but one can approximate the integral by an integral of a similar function or use adaptive routines such as [[stratified sampling]], [[Monte Carlo integration#Recursive stratified sampling|recursive stratified sampling]], adaptive umbrella sampling<ref>{{cite journal|last=MEZEI|first=M|title=Adaptive umbrella sampling: Self-consistent determination of the non-Boltzmann bias|journal=Journal of Computational Physics|date=31 December 1986|volume=68|issue=1|pages=237–248|doi=10.1016/0021-9991(87)90054-4|bibcode = 1987JCoPh..68..237M|ref=harv }}</ref><ref>{{cite journal|last=Bartels|first=Christian|last2=Karplus|first2=Martin|title=Probability Distributions for Complex Systems: Adaptive Umbrella Sampling of the Potential Energy|journal=The Journal of Physical Chemistry B|date=31 December 1997|volume=102|issue=5|pages=865–880|doi=10.1021/jp972280j|ref=harv}}</ref> or the [[VEGAS algorithm]].\n\nA similar approach, the [[quasi-Monte Carlo method]], uses [[low-discrepancy sequence]]s. These sequences \"fill\" the area better and sample the most important points more frequently, so quasi-Monte Carlo methods can often converge on the integral more quickly.\n\nAnother class of methods for sampling points in a volume is to simulate random walks over it ([[Markov chain Monte Carlo]]). Such methods include the [[Metropolis–Hastings algorithm]], [[Gibbs sampling]], [[Wang and Landau algorithm]], and interacting type MCMC methodologies such as the [[Particle filter|sequential Monte Carlo]] samplers.<ref>{{Cite journal|title = Sequential Monte Carlo samplers - Del Moral - Doucet - Jasra- 2006 - Journal of the Royal Statistical Society: Series B (Statistical Methodology) - Wiley Online Library|journal = Journal of the Royal Statistical Society, Series B|doi=10.1111/j.1467-9868.2006.00553.x|volume=68|issue = 3|pages=411–436|year = 2006|last1 = Del Moral|first1 = Pierre|last2 = Doucet|first2 = Arnaud|last3 = Jasra|first3 = Ajay|arxiv = cond-mat/0212648}}</ref>\n\n=== Simulation and optimization ===\n{{Main|Stochastic optimization}}\nAnother powerful and very popular application for random numbers in numerical simulation is in [[Optimization (mathematics)|numerical optimization]]. The problem is to minimize (or maximize) functions of some vector that often has a large number of dimensions. Many problems can be phrased in this way: for example, a [[computer chess]] program could be seen as trying to find the set of, say, 10 moves that produces the best evaluation function at the end. In the [[traveling salesman problem]] the goal is to minimize distance traveled. There are also applications to engineering design, such as [[multidisciplinary design optimization]]. It has been applied with [[quasi-one-dimensional models]] to solve particle dynamics problems by efficiently exploring large configuration space. Reference <ref>Spall, J. C. (2003), ''Introduction to Stochastic Search and Optimization: Estimation, Simulation, and Control'', Wiley, Hoboken, NJ. http://www.jhuapl.edu/ISSO</ref> is a comprehensive review of many issues related to simulation and optimization.\n\nThe [[traveling salesman problem]] is what is called a conventional optimization problem. That is, all the facts (distances between each destination point) needed to determine the optimal path to follow are known with certainty and the goal is to run through the possible travel choices to come up with the one with the lowest total distance. However, let's assume that instead of wanting to minimize the total distance traveled to visit each desired destination, we wanted to minimize the total time needed to reach each destination. This goes beyond conventional optimization since travel time is inherently uncertain (traffic jams, time of day, etc.). As a result, to determine our optimal path we would want to use simulation - optimization to first understand the range of potential times it could take to go from one point to another (represented by a probability distribution in this case rather than a specific distance) and then optimize our travel decisions to identify the best path to follow taking that uncertainty into account.\n\n===Inverse problems===\nProbabilistic formulation of [[inverse problem]]s leads to the definition of a [[probability distribution]] in the model space. This probability distribution combines [[prior probability|prior]] information with new information obtained by measuring some observable parameters (data).\nAs, in the general case, the theory linking data with model parameters is nonlinear, the posterior probability in the model space may not be easy to describe (it may be multimodal, some moments may not be defined, etc.).\n\nWhen analyzing an inverse problem, obtaining a maximum likelihood model is usually not sufficient, as we normally also wish to have information on the resolution power of the data. In the general case we may have a large number of model parameters, and an inspection of the marginal probability densities of interest may be impractical, or even useless. But it is possible to pseudorandomly generate a large collection of models according to the posterior probability distribution and to analyze and display the models in such a way that information on the relative likelihoods of model properties is conveyed to the spectator. This can be accomplished by means of an efficient Monte Carlo method, even in cases where no explicit formula for the ''a priori'' distribution is available.\n\nThe best-known importance sampling method, the Metropolis algorithm, can be generalized, and this gives a method that allows analysis of (possibly highly nonlinear) inverse problems with complex ''a priori'' information and data with an arbitrary noise distribution.<ref>{{harvnb|Mosegaard|Tarantola|1995}}</ref><ref>{{harvnb|Tarantola|2005}}</ref>\n\n== See also ==\n{{Portal|Statistics}}\n\n{{Div col|colwidth=30em}}\n* [[Auxiliary field Monte Carlo]]\n* [[Biology Monte Carlo method]]\n* [[Comparison of risk analysis Microsoft Excel add-ins]]\n* [[Direct simulation Monte Carlo]]\n* [[Dynamic Monte Carlo method]]\n* [[Genetic algorithms]]\n* [[Kinetic Monte Carlo]]\n* [[List of software for Monte Carlo molecular modeling]]\n* [[Mean field particle methods]]\n* [[Monte Carlo method for photon transport]]\n* [[Monte Carlo methods for electron transport]]\n* [[Morris method]]\n* [[Multilevel Monte Carlo method]]\n* [[Particle filter]]\n* [[Quasi-Monte Carlo method]]\n* [[Sobol sequence]]\n* [[Temporal difference learning]]\n{{div col end}}\n\n== References ==\n=== Citations ===\n{{Reflist}}\n\n=== Sources ===\n{{refbegin}}\n* {{cite journal |first = Herbert L. |last = Anderson |author-link = Herbert L. Anderson |url = http://library.lanl.gov/cgi-bin/getfile?00326886.pdf |title = Metropolis, Monte Carlo and the MANIAC |journal = [[Los Alamos Science]] |volume = 14 |pages = 96–108 |year= 1986 |ref = harv }}\n* {{cite journal |last = Benov |first = Dobriyan M. |title=The Manhattan Project, the first electronic computer and the Monte Carlo method |journal = Monte Carlo Methods and Applications |volume = 22 |issue = 1 |pages = 73–79 |year=2016 |ref = harvn |doi=10.1515/mcma-2016-0102 }}\n* {{cite journal |first=Stephan A. |last=Baeurle |title = Multiscale modeling of polymer materials using field-theoretic methodologies: A survey about recent developments |journal = Journal of Mathematical Chemistry|volume=46 |issue=2 |pages=363–426 |year=2009 |doi = 10.1007/s10910-008-9467-3 |ref = harv }}\n* {{cite book |title = Markov Chain Monte Carlo Simulations and Their Statistical Analysis (With Web-Based Fortran Code) |last=Berg |first=Bernd A. |year=2004 |publisher=World Scientific |location=Hackensack, NJ |isbn=978-981-238-935-0 |url = |ref = harv }}\n* {{cite book |title=The Monte Carlo Method in Condensed Matter Physics |last=Binder |first=Kurt |author-link=Kurt Binder |year=1995 |publisher=Springer |location=New York |isbn=978-0-387-54369-7 |url = |ref = harv }}\n* {{cite book |title = Monte Carlo and quasi-Monte Carlo methods |last=Caflisch |first=R. E. |authorlink=Russel E. Caflisch |year=1998 |series = Acta Numerica |volume=7 |publisher=Cambridge University Press |location= |isbn= |pages=1–49 |url = |ref = harv }}\n* {{cite book |last = Davenport |first=J. H. |authorlink= |title=Primality testing revisited |journal = Proceeding ISSAC '92 Papers from the International Symposium on Symbolic and Algebraic Computation |pages=123–129 |doi = 10.1145/143242.143290  |isbn = 978-0-89791-489-5 |year=1992 |citeseerx=10.1.1.43.9296 |ref = harv }}\n* {{cite book |title=Sequential Monte Carlo methods in practice |last1=Doucet |first1=Arnaud |last2=Freitas |first2=Nando de |last3=Gordon |first3=Neil |year=2001 |publisher=Springer |location=New York |isbn=978-0-387-95146-1 |url =  |ref = harv }}\n* {{cite journal |last = Eckhardt |first= Roger |year = 1987 |title = Stan Ulam, John von Neumann, and the Monte Carlo method |journal= Los Alamos Science |issue=15 |pages= 131–137 |ref = harv |url = http://library.lanl.gov/cgi-bin/getfile?15-13.pdf }}\n* {{cite book |title=Monte Carlo: Concepts, Algorithms, and Applications |last=Fishman |first=G. S.  |year=1995 |publisher=Springer |location=New York |isbn = 978-0-387-94527-9 |url = |ref=harv }}\n* {{cite journal |title = A Monte Carlo tool to simulate breast cancer screening programmes | author = C. Forastero and L. Zamora and D. Guirado and A. Lallena |journal = Phys. Med. Biol. |volume = 55 |issue = 17 |pages = 5213–5229 |year = 2010 |doi = 10.1088/0031-9155/55/17/021 |pmid = 20714045 |bibcode = 2010PMB....55.5213F |ref = harv }}\n* {{cite journal |first = Leslie M. |last = Golden |title = The Effect of Surface Roughness on the Transmission of Microwave Radiation Through a Planetary Surface |journal = [[Icarus (journal)|Icarus]] |volume = 38 |year = 1979 |pages = 451–455 |bibcode = 1979Icar...38..451G |doi = 10.1016/0019-1035(79)90199-4 |issue = 3 |ref = harv }}\n* {{cite book |title = An Introduction to Computer Simulation Methods, Part 2, Applications to Physical Systems |last = Gould |first1 = Harvey |last2=Tobochnik |first2=Jan |year=1988 |publisher=Addison-Wesley |location = Reading |isbn=978-0-201-16504-3 |url = |ref = harv }}\n* {{cite book |first=Charles |last=Grinstead |first2= J. Laurie |last2=Snell |title=Introduction to Probability |pages = 10–11 |publisher=[[American Mathematical Society]] |year= 1997 |ref = harv }}\n* {{cite book |title = Monte Carlo Methods |last1 = Hammersley |first1=J. M. |last2=Handscomb |first2=D. C. |year=1975 |publisher=Methuen |location=London |isbn=978-0-416-52340-9 |ref = harv }}\n* {{cite book |last=Hartmann |first=A.K. |year=2009 |title=Practical Guide to Computer Simulations |publisher=World Scientific |isbn=978-981-283-415-7 |url = http://www.worldscibooks.com/physics/6988.html |archive-url = https://web.archive.org/web/20090211113048/http://worldscibooks.com/physics/6988.html |dead-url=yes |archive-date=2009-02-11 }}\n* {{cite book |first=Douglas |last=Hubbard |title = How to Measure Anything: Finding the Value of Intangibles in Business |page= 46 |publisher=[[John Wiley & Sons]] |year= 2007 |ref = harv }}\n* {{cite book |first=Douglas |last=Hubbard  |title=The Failure of Risk Management: Why It's Broken and How to Fix It |publisher= [[John Wiley & Sons]] |year=2009 |ref = harv }}\n* {{cite book |title=Judgement under Uncertainty: Heuristics and Biases |last1 = Kahneman |first1 = D. |last2=Tversky |first2 = A. |year=1982 |publisher=Cambridge University Press |location= |isbn=  |url=  |ref = harv }}\n* {{cite book |first1=Malvin H. |last1=Kalos |first2=Paula A. |last2=Whitlock |title=Monte Carlo Methods |publisher=[[Wiley-VCH]] |year=2008 |isbn=978-3-527-40760-6 |ref = harv }}\n* {{cite book |title=Handbook of Monte Carlo Methods |last1 = Kroese |first1=D. P. |last2=Taimre |first2 = T. |last3=Botev |first3=Z.I. |year=2011 |publisher=[[John Wiley & Sons]] |location=New York |isbn=978-0-470-17793-8 |page=772 |url = http://www.montecarlohandbook.org |ref = harv }}\n* {{cite journal |first1 = H. T.|last1 = MacGillivray |first2 = R. J. |last2=Dodd |title=Monte-Carlo simulations of galaxy systems |journal=[[Astrophysics and Space Science]] |volume = 86 |issue= 2 |pages= 419–435 |year= 1982  |url = http://www.springerlink.com/content/rp3g1q05j176r108/fulltext.pdf |doi = 10.1007/BF00683346 |ref = harv }}\n* {{cite book |title=Stochastic Simulation in Physics |last=MacKeown |first = P. Kevin |authorlink= |year=1997 |publisher=Springer |location=New York |isbn=978-981-3083-26-4 |page= |pages= |url = |ref = harv }}\n* {{cite journal |last = Metropolis |first = N. |author-link = Nicholas Metropolis |url = http://library.lanl.gov/cgi-bin/getfile?15-12.pdf |title=The beginning of the Monte Carlo method |journal = Los Alamos Science |issue=1987 Special Issue dedicated to Stanislaw Ulam |pages=125–130 |year=1987 |ref = harv }}\n* {{cite journal |last1 = Metropolis |first1 = N. |author1-link = Nicholas Metropolis |last2=Rosenbluth |first2=Arianna W.|last3=Rosenbluth |first3=Marshall N. |last4=Teller |first4=Augusta H. |last5=Teller |first5=Edward |year=1953 |title=Equation of State Calculations by Fast Computing Machines |journal=Journal of Chemical Physics |volume=21 |issue=6 |page=1087 |doi = 10.1063/1.1699114 |url = \n|bibcode = 1953JChPh..21.1087M |title-link = Equation of State Calculations by Fast Computing Machines |ref = harv }}\n* {{cite journal |last1 = Metropolis |first1 = N. |author1-link = Nicholas Metropolis |last2 = Ulam |first2 = S. |authorlink2=Stanislaw Ulam |year=1949 |title = The Monte Carlo Method |journal = Journal of the American Statistical Association |volume=44 |issue=247 |pages=335–341 |doi = 10.2307/2280232 |pmid=18139350 |jstor=2280232 |ref = harv }}\n* {{cite journal |doi = 10.1002/prot.340150104 |title = Insertion of peptide chains into lipid membranes: an off-lattice Monte Carlo dynamics model |first1 = M. |last1 = Milik |first2 = J. |last2 = Skolnick |journal = Proteins |volume = 15 |issue = 1 |pages = 10–25 |date=Jan 1993 |pmid = 8451235 |ref = harv }}\n* {{cite journal |last1 = Mosegaard |first1 = Klaus |last2 = Tarantola |first2 = Albert |year = 1995 |title = Monte Carlo sampling of solutions to inverse problems |journal = J. Geophys. Res. |volume = 100 |number =B7 |pages = 12431–12447 |doi = 10.1029/94JB03097 |url = http://www.math.pitt.edu/~cbsg/Materials/MonteCarlo_latex.pdf |bibcode = 1995JGR...10012431M |ref = harv }}\n* {{cite journal |title = Monte Carlo Simulations of Proteins in Cages: Influence of Confinement on the Stability of Intermediate States |author=P. Ojeda |author2=M. Garcia |author3=A. Londono |author4=N.Y. Chen |journal = Biophys. J. |volume = 96 |issue = 3 |pages = 1076–1082 |date=Feb 2009 |doi = 10.1529/biophysj.107.125369 |pmid = 18849410 |pmc = 2716574 |bibcode = 2009BpJ....96.1076O |ref = harv }}\n* {{cite journal |doi = 10.1504/IJVD.2001.001963 |last1 = Int Panis |first1 = L. |last2 = de Nocker |first2 = L. |last3 = De Vlieger |first3 = I. |last4= Torfs |first4= R. |year=2001 |title = Trends and uncertainty in air pollution impacts and external costs of Belgian passenger car traffic International |journal=Journal of Vehicle Design |volume = 27 |issue = 1–4 |pages = 183–194 |ref = harv }}\n* {{cite journal |last1 = Int Panis |first1 = L. |last2 = Rabl |first2 = A. |last3 = de Nocker |first3 = L. |last4 = Torfs |first4 = R. |year = 2002 |title = Diesel or Petrol ? An environmental comparison hampered by uncertainty |journal = Mitteilungen Institut für Verbrennungskraftmaschinen und Thermodynamik |editor-first = P. |editor-last = Sturm |publisher = Technische Universität Graz Austria |volume = Heft 81 Vol 1 |pages = 48–54 |ref = harv }}\n* {{cite book |last1 = Press |first1 = William H. |last2 = Teukolsky |first2 = Saul A. |last3 = Vetterling |first3 = William T. |last4 = Flannery |first4 = Brian P. |title = Numerical Recipes in Fortran 77: The Art of Scientific Computing |edition = 2nd |series = Fortran Numerical Recipes |volume = 1 |year = 1996 |orig-year = 1986 |publisher = [[Cambridge University Press]] |isbn=978-0-521-43064-7 |ref = harv }}\n* {{cite book |last = Ripley |first = B. D. |title = Stochastic Simulation |publisher = [[Wiley & Sons]] |year=1987 }}\n* {{cite book |title = Monte Carlo Statistical Methods |last1=Robert |first1 = C. |last2 = Casella |first2 = G. |year=2004 |edition=2nd |publisher=Springer |location=New York |isbn=978-0-387-21239-5 |url = |ref = harv }}\n* {{cite book |title = Simulation and the Monte Carlo Method |last1 = Rubinstein |first1 = R. Y. |last2 = Kroese |first2 = D. P. |year=2007 |edition=2nd |publisher = John Wiley & Sons |location=New York |isbn = 978-0-470-17793-8 |url =  |ref = harv }}\n* {{cite journal |last = Savvides |first = Savvakis C. |title = Risk Analysis in Investment Appraisal |journal= Project Appraisal Journal |year= 1994 |volume = 9 |issue= 1 |doi = 10.2139/ssrn.265905 |ref = harv }}\n* {{cite book |last1 = Sawilowsky |first1 = Shlomo S. |last2 = Fahoome |first2 = Gail C. |year = 2003 |title = Statistics via Monte Carlo Simulation with Fortran |location = Rochester Hills, MI |publisher = JMASM |isbn=978-0-9740236-0-1  |ref = harv }}\n* {{cite journal |last = Sawilowsky |first = Shlomo S. |title = You think you've got trivials? |journal=[[Journal of Modern Applied Statistical Methods]] |volume=2 |issue=1 |pages=218–225 |year=2003 |url = https://digitalcommons.wayne.edu/cgi/viewcontent.cgi?article=1744&context=jmasm |doi = 10.22237/jmasm/1051748460  |ref = harv }}\n* {{cite conference |conference = Neural Information Processing Systems 2010 |last1 = Silver |first1 = David |last2 = Veness |first2 = Joel |year = 2010 |title = Monte-Carlo Planning in Large POMDPs |url = http://books.nips.cc/papers/files/nips23/NIPS2010_0740.pdf |editor1-last = Lafferty |editor1-first = J. |editor2-last = Williams |editor2-first = C. K. I. |editor3-last = Shawe-Taylor |editor3-first = J. |editor4-last = Zemel |editor4-first = R. S. |editor5-last = Culotta |editor5-first = A. |book-title = Advances in Neural Information Processing Systems 23 |publisher = Neural Information Processing Systems Foundation |ref = harv }}\n* {{cite book |first = László |last = Szirmay-Kalos |title = Monte Carlo Methods in Global Illumination - Photo-realistic Rendering with Randomization |publisher = VDM Verlag Dr. Mueller e.K. |year = 2008 |isbn = 978-3-8364-7919-6 |ref = harv }}\n* {{cite book |title = Inverse Problem Theory |last = Tarantola |first = Albert |author-link = Albert Tarantola |year = 2005 |publisher = Society for Industrial and Applied Mathematics |location = Philadelphia |isbn = 978-0-89871-572-9 |url = http://www.ipgp.jussieu.fr/~tarantola/Files/Professional/SIAM/index.html |ref = harv }}\n* {{cite book |first = David |last = Vose |title = Risk Analysis, A Quantitative Guide |edition = 3rd |publisher =[[John Wiley & Sons]] |year = 2008 |ref = harv }}\n* {{cite book |last1 = Mazhdrakov |first1 = Metodi |last2 = Benov |first2 = Dobriyan |last3 = Valkanov |first3 = Nikolai |year = 2018 |title = The Monte Carlo Method. Engineering Applications |publisher = ACMO Academic Press |page = 250 |isbn = 978-619-90684-3-4 |url = https://books.google.com/?id=t0BqDwAAQBAJ&dq=the+monte+carlo+method+engineering+applications+mazhdrakov |ref = harv }}\n{{refend}}\n\n== External links ==\n* {{Commons category-inline|Monte Carlo method}}\n<!--======================== {{No more links}} ============================\n    | PLEASE BE CAUTIOUS IN ADDING MORE LINKS TO THIS ARTICLE. Wikipedia  |\n    | is not a collection of links nor should it be used for advertising. |\n    |                                                                     |\n    |           Excessive or inappropriate links WILL BE DELETED.         |\n    | See [[Wikipedia:External links]] & [[Wikipedia:Spam]] for details.  |\n    |                                                                     |\n    | If there are already plentiful links, please propose additions or   |\n    | replacements on this article's discussion page, or submit your link |\n    | to the relevant category at the Open Directory Project (dmoz.org)   |\n    | and link back to that category using the {{dmoz}} template.         |\n    ======================= {{No more links}} =============================-->\n\n{{-}}\n{{Statistics}}\n\n{{Authority control}}\n\n[[Category:Monte Carlo methods| ]]\n[[Category:Numerical analysis]]\n[[Category:Statistical mechanics]]\n[[Category:Computational physics]]\n[[Category:Sampling techniques]]\n[[Category:Statistical approximations]]\n[[Category:Stochastic simulation]]\n[[Category:Randomized algorithms]]\n[[Category:Risk analysis methodologies]]"
    },
    {
      "title": "Movable cellular automaton",
      "url": "https://en.wikipedia.org/wiki/Movable_cellular_automaton",
      "text": "{{short description|A method in computational solid mechanics based on the discrete concept}}\n{{inline|date=May 2014}}\n{{Infobox\n|title       = Movable cellular automaton method\n|titlestyle  = \n|datastyle   = \n|headerstyle  = background:#ccf;\n|labelstyle   = background:#ddf;\n|image        = [[File:MCA friction net.gif|300px|alt=modeling of contact interaction]]\n|imagestyle   = \n|caption      = Movable cellular automaton actively changed self neighbors by means switching neighbors state from linked to unlinked and vice versa (modeling of contact interaction)\n|header1 = Method type\n|label1  = \n|data1   = \n|label2  = Continuous/Discrete\n|data2   = Discrete\n|label3  = Analytical/Computational\n|data3   = Computational\n|header4 = Characteristics\n|label4  = \n|data4   = \n|header5 = \n|label5  = Influenced by\n|data5   = [[cellular automaton]], [[discrete element method|discrete element]]\n|label6  = Method in\n|data6   = [[Computational mechanics|computational solid mechanics]]\n}}\nThe '''Movable cellular automaton (MCA)''' method is a method in [[Computational mechanics|computational solid mechanics]] based on the discrete concept. It provides advantages both of classical [[cellular automaton]] and [[discrete element method|discrete element]] methods. Important advantage of the МСА method is a possibility of direct [[Computer simulation|simulation]] of materials fracture including damage generation, crack propagation, fragmentation and mass mixing. It is difficult to simulate these processes by means of [[continuum mechanics]] methods (For example: [[finite element method]], [[finite difference method]], etc.), so some new concepts like [[peridynamics]] are required. [[Discrete element method]] is very effective to simulate granular materials, but mutual forces among movable cellular automata provides simulating solids behavior. If size of automaton will be close to zero then MCA behavior becomes like classical [[continuum mechanics]] methods.\n\n==Keystone of the movable cellular automaton method==\n\n[[File:MCA elements.png|thumb|left|<center>Object (at left) is described as set of interacted automata (at center). At right is shown velocity field of automata.</center>]]\nIn framework of the '''MCA''' approach an object under modeling is considered as a set of interacting elements/automata. The dynamics of the set of automata are defined by their mutual forces and rules for their relationships. This system exists and operates in time and space. Its evolution in time and space is governed by the equations of motion. The mutual forces and rules for inter-elements relationships are defined by the function of the automaton response. This function has to be specified for each automaton. Due to mobility of automata the following new parameters of cellular automata have to be included into consideration: ''R<sup>i</sup>'' &ndash; radius-vector of automaton; ''V<sup>i</sup>'' &ndash; velocity of automaton; '''ω<sup>i</sup>''' &ndash; rotation velocity of automaton; ''θ<sup>i</sup>'' &ndash; rotation vector of automaton; ''m<sup>i</sup>'' &ndash; mass of automaton; ''J<sup>i</sup>'' &ndash; moment of inertia of automaton.\n\n==New concept: neighbours==\n\n[[File:MCA neighbors.gif|thumb|<center>Each automaton has some neighbors</center>]]\nThe new concept of the MCA method is based on the introducing of the '''state of the pair of automata''' (relation of interacting pairs of automata) in addition to the conventional one &ndash; the state of a separate automaton. Note that the introduction of this definition allows to go from the static net concept to the '''concept of neighbours'''. As a result of this, the automata have the ability to change their neighbors by switching the states (relationships) of the pairs.\n\n==Definition of the parameter of pair state==\n\nThe introducing of new type of states leads to new parameter to use it as criteria for '''switching relationships'''.  It is defined as an automaton overlapping parameters&nbsp;''h<sup>ij</sup>''. So the relationship of the cellular automata is characterised by the value of their '''overlapping'''.\n\n[[File:MCA sh1.gif]] [[File:MCA sh2.gif]]\n\nThe initial structure is formed by setting up certain relationships among each pair of neighboring elements.\n\n==Criterion of switching of the state of pair relationships==\n\n[[File:MCA switch.gif|thumb|<center>At left pair of automata ij is linked. At right pair of automata ij is unlinked.</center>]]\nIn contrast to the classical cellular automaton method in the MCA method not only a single automaton but also a '''relationship of pair of automata can be switched'''. According with the bistable automata concept there are two types of the pair states (relationships):\n    \n{| class=\"wikitable\" border=\"1\"\n|-\n| '''linked'''\n| – both automata belong to a solid\n|-\n| '''unlinked'''\n| – each automaton of the pair belongs to different bodies or parts of damaged body.\n|}\n\nSo the '''changing of the state of pair relationships''' is controlled by relative movements of the automata and the media formed by such pairs can be considered as bistable media.\n\n==Equations of MCA motion==\n\nThe evolution of MCA media is described by the following '''equations of motion for translation''':\n\n:<math>{d^2 h^{ij} \\over dt^2} = \\left( {1 \\over m^i} + {1 \\over m^j} \\right) p^{ij} + \\sum_{k\\neq j} C(ij,ik) \\psi(\\alpha_{ij,ik}) {1 \\over m^i} p^{ik} + \\sum_{l \\neq i} C(ij,jl) \\psi(\\alpha_{ij,jl}) {1 \\over m^j} p^{jl}</math>\n\n[[File:MCA neighbour in pair.png|thumb|<center>Forces between automata ij coming from their neighbors.</center>]]\nHere m<sup>i</sup> is the mass of automaton i, p<sup>ij</sup> is central force acting between automata i and j, C(ij,ik) is certain coefficient associated with transferring the h parameter from pair '''ij''' to pair '''ik''', ψ(α<sub>ij,ik</sub>) is angle between directions '''ij''' and '''ik'''.\n\nDue to finite size of movable automata the rotation effects have to be taken into account. The '''equations of motion for rotation''' can be written as follows:\n\n:<math>{d^2 \\theta^{ij} \\over dt^2} = \\left( {q^{ij} \\over J^i} + {q^{ji} \\over J^j} \\right) \\tau^{ij} + \\sum_{k\\neq j} S(ij,ik) {q^{ik} \\over J^i} \\tau^{ik} + \\sum_{l\\neq j} S(ij,jl) {q^{jl} \\over J^j} \\tau^{jl}</math>\n\nHere Θ<sup>ij</sup> is the angle of relative rotation (it is a switching parameter like h<sup>ij</sup> for translation), q<sup>ij</sup> is the distance from center of automaton '''i''' to contact point of automaton '''j''' (moment arm), τ<sup>ij</sup> is the pair tangential interaction, S(ij,ik) is certain coefficient associated with transferring the  Θ parameter from one pair to other (it is similar to C(ij,ik) from the equation for translation).\n\nIt should be noted that these equations are completely similar to the equations of motion for the many&ndash;particle approach.\n\n==Definition of deformation in pair of automata==\n\n[[File:Deformation_in_movable_cellular_automatation.svg|thumb|left|<center>Rotation of body as whole not caused to deformation in pair of automata</center>]]\n'''Translation of the pair automata'''\nThe dimensionless deformation parameter for translation of the '''i j''' automata pair can be presented as:\n\n:<math> \\varepsilon^{ij} = {h^{ij} \\over r_{0}^{ij}} = { \\left( q^{ij} + q^{ji} \\right) - \\left( d^{i} + d^{j} \\right) \\big / 2 \\over \\left( d^{i} + d^{j} \\right) \\big / 2 }</math>\n\nIn this case:\n\n:<math>\\left( \\Delta{\\varepsilon^{i(j)}} + \\Delta{\\varepsilon^{j(i)}} \\right)  \n{ \\left( d^{i} + d^{j} \\right) \\over 2} = V_{n}^{ij} \\Delta{t}</math>\n\nwhere '''Δt''' time step, '''V<sub>n</sub><sup>ij</sup>''' &ndash; relative velocity.\n\nRotation of the pair automata can be calculated by analogy with the last translation relationships.\n\n==Modeling of irreversible deformation in the MCA method==\n\n[[File:MCA Irreversible Deformation.gif|thumb|<center>Deformation is determine by value of distance from the center of automaton</center>]]\n[[File:MCA response function of automata.gif|thumb|<center>There are two types of the response function of automata</center>]]\n\nThe '''ε<sup>ij</sup>''' parameter is used as a measure of deformation of automaton '''i''' under its interaction with automaton '''j'''. Where '''q<sup>ij</sup>''' &ndash; is a distance from the center of automaton '''i''' to its contact point with automaton '''j''';  '''R<sup>i</sup> = d<sup>i</sup>/2''' ('''d<sup>i</sup>''' &ndash; is the size of automaton '''i''').\n\n'''As an example''' the titanium specimen under cyclic loading (tension &ndash; compression) is considered. The loading diagram is shown in the next figure:\n\n{| class=\"wikitable\" border=\"1\"\n|-\n! Scheme of loading\n! Loading diagram\n|-\n| [[File:MCA cyclic schem.gif]]\n| [[File:MCA cyclic diag.gif]]\n|-\n| \n| ('''Red marks''' are the experimental data)\n|}\n\n==Advantages of MCA method==\n\nDue to mobility of each automaton the MCA method allows to take into account directly such actions as:\n\n* mass mixing\n* penetration effects\n* chemical reactions\n* intensive deformation\n* phase transformations\n* accumulation of damages\n* fragmentation and fracture\n* cracks generation and development\n\nUsing boundary conditions of different types (fixed, elastic, viscous-elastic, etc.) it is possible to imitate different properties of surrounding medium, containing the simulated system. It is possible to model different modes of mechanical loading (tension, compression, shear strain, etc.) by setting up additional conditions at the boundaries.\n\n==See also==\n*{{annotated link|Continuum mechanics}}\n*{{annotated link|Solid mechanics}}\n*{{annotated link|Fracture mechanics}}\n*{{annotated link|Peridynamics}}\n*{{annotated link|Computer simulation}}\n*{{annotated link|Discrete element method}}\n*{{annotated link|Cellular automaton}}\n*{{annotated link|Finite element method}}\n*{{annotated link|Finite difference method}}\n\n==References==\n*{{cite journal|last =Psakhie| first=S.G.| authorlink=|author2=Horie, Y. |author3=Korostelev, S.Yu. |author4=Smolin, A.Yu. |author5=Dmitriev, A.I. |author6=Shilko, E.V. |author7= Alekseev, S.V. | title=Method of movable cellular automata as a tool for simulation within the framework of mesomechanics| journal=Russian Physics Journal| volume=38| issue=11| pages=1157–1168| date = November 1995| url=| doi=10.1007/BF00559396| id=| accessdate=}}\n*{{cite journal|last =Psakhie| first=S.G.| authorlink=|author2=Korostelev, S.Y. |author3=Smolin, A.Y. |author4=Dmitriev, A.I. |author5=Shilko, E.V. |author6=Moiseenko D.D. |author7=Tatarincev E.M. |author8= Alekseev, S.V. | title=Movable cellular automata method as a tool for physical mesomechanics of materials| journal=Physical Mesomechanics| volume=1| issue=1| pages=95–108| year = 1998| url=| doi = | id =| accessdate=}} ({{cite journal|last =Псахье| first=С.Г.| authorlink=|author2=Коростелев, С.Ю. |author3=Смолин, А.Ю. |author4=Дмитриев, А.И. |author5=Шилько, Е.В. |author6=Моисеенко, Д.Д. |author7=Татаринцев, Е.М. |author8= Алексеев, С.В. | title=Метод подвижных клеточных автоматов как инструмент физической мезомеханики материалов| journal=Физическая мезомеханика| volume=1| issue=1| pages=95–108| year = 1998| url = http://elibrary.ru/item.asp?id=12913617| doi = | id =| accessdate = 2010-03-03}})\n*{{cite journal|last =Psakhie| first=S.G.| authorlink=|author2=Ostermeyer, G.P. |author3=Dmitriev, A.I. |author4=Shilko, E.V. |author5=Smolin, A.Y. |author6= Korostelev, S.Y. | title=Movable cellular automata method as new direction in numerical discrete mechanics. I. Theoretical description| journal=Physical Mesomechanics| volume=3| issue=2| pages=5–13| year=2000| url=| doi = | id = | accessdate = }} ({{cite journal|last =Псахье| first=С.Г.| authorlink=|author2=Остермайер, Г.П. |author3=Дмитриев, А.И. |author4=Шилько, Е.В. |author5=Смолин, А.Ю. |author6= Коростелев, С.Ю. | title=Метод подвижных клеточных автоматов как новое направление дискретной вычислительной механики. I. Теоретическое описание| journal=Физическая мезомеханика| volume=3| issue=2| pages=5–13| year=2000 | url=http://elibrary.ru/item.asp?id=12913646| doi = | id = | accessdate = 2010-03-03}})\n*{{cite journal| last=Psakhie| first=S.G.| authorlink=| author2=Horie, Y.| author3=Ostermeyer, G.P.| author4=Korostelev, S.Yu.| author5=Smolin, A.Yu.| author6=Shilko, E.V.| author7=Dmitriev, A.I.| author8=Blatnik, S.| author9=Spegel, M.| author10=Zavsek, S.| title=Movable cellular automata method for simulating materials with mesostructure| journal=Theoretical and Applied Fracture Mechanics| volume=37| issue=1–3| pages=311–334| date=December 2001| url=http://mechanik.tu-berlin.de/popov/software/mca/TAFMEC.pdf| doi=10.1016/S0167-8442(01)00079-9| id=| accessdate=| deadurl=yes| archiveurl=https://web.archive.org/web/20110719095651/http://mechanik.tu-berlin.de/popov/software/mca/TAFMEC.pdf| archivedate=2011-07-19| df=}}\n*{{cite journal|last =Psakhie| first=S.G.| authorlink=|author2=Smolin, A.Y. |author3=Stefanov, Y.P. |author4=Makarov, P.V. |author5= Chertov, M.A. | title=Modeling the behavior of complex media by jointly using discrete and continuum approaches| journal=Technical Physics Letters| volume=30| issue=9| pages=712–714| year = 2004| url = https://zenodo.org/record/1163164| doi=10.1134/1.1804572| id = | accessdate = }}\n*{{cite book| last=Shimizu| first=Y.| authorlink =  |author2=Hart, R. |author3=Cundall, P. | title=Numerical modeling in Micromechanics via Particle Methods| publisher =  | year=2004| location =  | page =  | url=https://books.google.com/?id=GVAUH98Yr6oC&lpg=PA242&dq=%22Movable%20cellular%20automaton%22%20-inpublisher%3Aicon&pg=PR4#v=onepage&q=%22Movable%20cellular%20automaton%22%20-inpublisher:icon&f=false| doi =  | id = | isbn=978-90-5809-679-1 | accessdate = 2010-03-03}}\n*{{cite book |editor-last=Gnecco |editor-first=E. |editor2=Meyer E. | title=Fundamentals of friction and wear on the Nanoscale| publisher =  | year=2007| location =  | page =  | url=https://books.google.com/?id=v2Pe5thhNiwC&lpg=PP1&pg=PP1#v=onepage&q=&f=false| doi =  | id =    | isbn=978-3-540-36806-9 | accessdate = 2010-03-03}}\n*{{cite journal|last =Yunliang| first=Tan| authorlink =|author2=Guirong, Teng |author3=Haitao, Li |title=MCA Model for Simulating the Failure of Microinhomogeneous Materials| journal=Journal of Nanomaterials | volume=2008 | issue=| pages=1–7 | url = | doi = 10.1155/2008/946038 | id = 946038| accessdate = | year=2008}}\n*{{cite book| last=Fomin| first=V.M. |author2=Andreev, A.N. |display-authors=etal | title=Mechanics - from discrete to continuous| publisher=Russian academy of science, Siberian branch, Institute of theoretical and applied mechanics (named S.A. Khristianovich)| year=2008| location =  | page=344| url=| doi =  | id =    | isbn=978-5-7692-0974-1}} ({{cite book| last=Фомин| first=В.М.| authorlink =  |author2=Андреев А.Н. и др.| title=Механика - от дискретного к сплошному| publisher=Рос. акад наук, Сиб. отд-ние, Ин-т теоретической и прикладной механики им. С.А. Христиановича| year=2008 | page=344| url=http://www.sibran.ru/psb/books/show_text.phtml?rus+1880+psb| isbn=978-5-7692-0974-1| accessdate = 3 March 2010 |language=ru}})\n*{{cite journal| last=Smolin| first=A.Y.| authorlink=|author2=Roman, N.V. |author3=Dobrynin, S.A. |author4= Psakhie, S.G. | title=On rotation in the movable cellular automaton method| journal=Physical Mesomechanics| volume=12| issue=3–4| pages=124–129| date=May–August 2009 | doi=10.1016/j.physme.2009.07.004| id = | accessdate = }}\n*{{cite book| last=Popov| first=Valentin L.| authorlink =  | title= Kontaktmechanik und Reibung (Ein Lehr- und Anwendungsbuch von der Nanotribologie bis zur numerischen Simulation)| publisher=Springer Berlin Heidelberg| year=2009| location =  | page =  | url=| doi=10.1007/978-3-540-88837-6| id =    | isbn=978-3-540-88836-9| accessdate=}}\n*{{cite book| last=Dobrynin| first=S.A.| authorlink =  | title=Development of movable cellular atomaton method for modeling generation and propagation of elastic waves under contact interaction of solids| publisher=PhD thesis in physics and mathematics| year=2010| location=Tomsk| page=130| url =  | doi=| id =    | isbn = }} ({{cite book| last=Добрынин| first=С.А.| authorlink =  | title=Развитие метода подвижных клеточных автоматов для моделирования генерации и распространения упругих волн при контактном взаимодействии твердых тел| publisher=Диссертация … кандидата физико-математических наук| year=2010| location=Томск| page=130| url=http://serg-dobrinin.narod.ru/disert/index.html |accessdate=3 March 2010 |language=ru}})\n* {{cite book| last=Dobrynin| first=Sergey| authorlink =  | title=Computer simulation by movable cellular automaton method| publisher=LAP LAMBERT Academic Publishing| year=2011| location=Saarbrücken Germany| page=132| url= | doi =  | id =    | isbn = 978-3-8443-5954-1}} ({{cite book| last=Добрынин| first=Сергей| authorlink =  | title=Компьютерное моделирование методом подвижных клеточных автоматов| publisher=LAP LAMBERT Academic Publishing| year=2011| location=Saarbrücken Germany| page=132| url=https://www.lap-publishing.com/catalog/details/store/gb/book/978-3-8443-5954-1/%D0%9A%D0%BE%D0%BC%D0%BF%D1%8C%D1%8E%D1%82%D0%B5%D1%80%D0%BD%D0%BE%D0%B5-%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5-%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D0%BE%D0%BC-%D0%BF%D0%BE%D0%B4%D0%B2%D0%B8%D0%B6%D0%BD%D1%8B%D1%85-%D0%BA%D0%BB%D0%B5%D1%82%D0%BE%D1%87%D0%BD%D1%8B%D1%85-%D0%B0%D0%B2%D1%82%D0%BE%D0%BC%D0%B0%D1%82%D0%BE%D0%B2| isbn = 978-3-8443-5954-1| accessdate = 2011-11-19 |language=ru}})\n\n==Software==\n* [https://web.archive.org/web/20110507053114/http://mechanik.tu-berlin.de/popov/software/mca/mca.htm MCA software package]\n* Software for simulation of materials in discrete-continuous approach «FEM+MCA»: Number of state registration in Applied Research Foundation of  Algorithms and Software (AFAS): 50208802297 / Smolin A.Y., Zelepugin S.A., Dobrynin S.A.; applicant and development center is Tomsk State University. – register date 28.11.2008; [http://serg-dobrinin.narod.ru/mydiploms/reg_program.jpg certificate AFAS N 11826 date 01.12.2008.]\n\n[[Category:Solid mechanics]]\n[[Category:Numerical analysis]]\n[[Category:Cellular automata]]\n[[Category:Condensed matter physics]]\n[[Category:Mathematical modeling]]"
    },
    {
      "title": "Multi-time-step integration",
      "url": "https://en.wikipedia.org/wiki/Multi-time-step_integration",
      "text": "In [[numerical analysis]], '''multi-time-step integration''', also referred to as multiple-step or asynchronous time integration, is a numerical time-integration method that uses different time-steps or time-integrators for different parts of the problem. There are different approaches to multi-time-step integration. They are based on domain decomposition and can be classified into strong (monolithic) or weak (staggered) schemes.<ref>{{Cite book|url=https://global.oup.com/academic/product/domain-decomposition-methods-for-partial-differential-equations-9780198501787?cc=us&lang=en&|title=Domain Decomposition Methods for Partial Differential Equations|isbn=9780198501787|publisher=Oxford University Press|date=1999-07-29|series=Numerical Mathematics and Scientific Computation}}</ref><ref>{{Cite book|title=Domain Decomposition Methods — Algorithms and Theory – Springer|volume = 34|last=Toselli|first=Andrea|last2=Widlund|first2=Olof B.|doi=10.1007/b137868|series = Springer Series in Computational Mathematics|year = 2005|isbn = 978-3-540-20696-5}}</ref><ref>{{Cite journal|last=Felippa|first=Carlos A.|last2=Park|first2=K. C.|last3=Farhat|first3=Charbel|date=2001-03-02|title=Partitioned analysis of coupled mechanical systems|journal=Computer Methods in Applied Mechanics and Engineering|series=Advances in Computational Methods for Fluid-Structure Interaction|volume=190|issue=24–25|pages=3247–3270|doi=10.1016/S0045-7825(00)00391-1|bibcode=2001CMAME.190.3247F}}</ref> Using different time-steps or time-integrators in the context of a weak algorithm is rather straightforward, because the numerical solvers operate independently. However, this is not the case in a strong algorithm. In the past few years a number of research articles have addressed the development of strong multi-time-step algorithms.<ref>{{Cite journal|last=Gravouil|first=Anthony|last2=Combescure|first2=Alain|date=2001-01-10|title=Multi-time-step explicit–implicit method for non-linear structural dynamics|url=http://onlinelibrary.wiley.com/doi/10.1002/1097-0207(20010110)50:13.0.CO;2-A/abstract|journal=International Journal for Numerical Methods in Engineering|language=en|volume=50|issue=1|pages=199–225|doi=10.1002/1097-0207(20010110)50:13.0.CO;2-A|issn=1097-0207|doi-broken-date=2019-03-15}}</ref><ref>{{Cite journal|last=Prakash|first=A.|last2=Hjelmstad|first2=K. D.|date=2004-12-07|title=A FETI-based multi-time-step coupling method for Newmark schemes in structural dynamics|journal=International Journal for Numerical Methods in Engineering|language=en|volume=61|issue=13|pages=2183–2204|doi=10.1002/nme.1136|issn=1097-0207|bibcode=2004IJNME..61.2183P}}</ref><ref>{{Cite journal|last=Karimi|first=S.|last2=Nakshatrala|first2=K. B.|date=2014-09-15|title=On multi-time-step monolithic coupling algorithms for elastodynamics|journal=Journal of Computational Physics|volume=273|pages=671–705|doi=10.1016/j.jcp.2014.05.034|arxiv=1305.6355|bibcode=2014JCoPh.273..671K}}</ref><ref>{{Cite journal|last=Karimi|first=S.|last2=Nakshatrala|first2=K. B.|date=2015-01-01|title=A monolithic multi-time-step computational framework for first-order transient systems with disparate scales|journal=Computer Methods in Applied Mechanics and Engineering|volume=283|pages=419–453|doi=10.1016/j.cma.2014.10.003|arxiv=1405.3230|bibcode=2015CMAME.283..419K}}</ref> In either case, strong or weak, the numerical accuracy and stability needs to be carefully studied. Other approaches to multi-time-step integration in the context of operator splitting [http://www.mathematik.uni-dortmund.de/~kuzmin/cfdintro/lecture11.pdf methods] have also been developed; i.e., multi-rate GARK [http://www.imacm.uni-wuppertal.de/fileadmin/imacm/preprints/2015/imacm_15_06.pdf method] and multi-step methods for molecular dynamics simulations.<ref>{{Cite journal|last=Jia|first=Zhidong|last2=Leimkuhler|first2=Ben|date=2006-01-01|title=Geometric integrators for multiple time-scale simulation|url=http://stacks.iop.org/0305-4470/39/i=19/a=S04|journal=Journal of Physics A: Mathematical and General|language=en|volume=39|issue=19|pages=5379|doi=10.1088/0305-4470/39/19/S04|issn=0305-4470|bibcode=2006JPhA...39.5379J}}</ref>\n\n==References==\n{{Reflist}}\n\n[[Category:Numerical analysis]]\n[[Category:Applied mathematics]]"
    },
    {
      "title": "Multigrid method",
      "url": "https://en.wikipedia.org/wiki/Multigrid_method",
      "text": "'''Multigrid (MG) methods''' in [[numerical analysis]] are [[algorithm]]s for solving [[differential equations]] using a [[hierarchy]] of [[discretization]]s. They are an example of a class of techniques called [[Multiresolution analysis|multiresolution methods]], very useful in problems exhibiting [[Multiscale modeling|multiple scales]] of behavior. For example, many basic [[relaxation method]]s exhibit different rates of convergence for short- and long-wavelength components, suggesting these different scales be treated differently, as in a [[Fourier analysis]] approach to multigrid.<ref>{{cite book |title=Practical Fourier analysis for multigrid methods |author1=Roman Wienands |author2=Wolfgang Joppich |page=17 |url=https://books.google.com/books?id=IOSux5GxacsC&pg=PA17 |isbn=978-1-58488-492-7 |publisher=CRC Press |year=2005}}</ref> MG methods can be used as solvers as well as [[preconditioner]]s.\n\nThe main idea of multigrid is to accelerate the convergence of a basic iterative method (known as relaxation, which generally reduces short-wavelength error) by a ''global'' correction of the fine grid solution approximation from time to time, accomplished by solving a [[coarse problem]]. The coarse problem, while cheaper to solve, is similar to the fine grid problem in that it also has short- and long-wavelength errors. It can also be solved by a combination of relaxation and appeal to still coarser grids. This recursive process is repeated until a grid is reached where the cost of direct solution there is negligible compared to the cost of one relaxation sweep on the fine grid. This multigrid cycle typically reduces all error components by a fixed amount bounded well below one, independent of the fine grid mesh size. The typical application for multigrid is in the numerical solution of [[elliptic partial differential equation]]s in two or more dimensions.<ref>{{cite book |title=Multigrid |author1=U. Trottenberg |author2=C. W. Oosterlee |author3=A. Schüller |publisher=Academic Press |year=2001 |isbn=978-0-12-701070-0 |url=https://books.google.com/books?id=-og1wD-Nx_wC&printsec=frontcover&dq=isbn:012701070X#v=onepage&q=elliptic&f=false}}</ref>\n\nMultigrid methods can be applied in combination with any of the common discretization techniques. For example, the [[finite element method]] may be recast as a multigrid method.<ref>{{cite book |title=Multigrid finite element methods for electromagnetic field modeling |author1=Yu Zhu |author2=Andreas C. Cangellaris |url=https://books.google.com/books?id=amq9j71_nqAC&pg=PA132 |page=132 ''ff'' |isbn=978-0-471-74110-7 |year=2006 |publisher=Wiley}}</ref> In these cases, multigrid methods are among the fastest solution techniques known today. In contrast to other methods, multigrid methods are general in that they can treat arbitrary regions and [[boundary condition]]s. They do not depend on the [[Separable partial differential equation|separability of the equations]] or other special properties of the equation.  They have also been widely used for more-complicated non-symmetric and nonlinear systems of equations, like the [[Linear elasticity|Lamé equations]] of [[Elasticity (physics)|elasticity]] or the [[Navier-Stokes equations]].<ref>{{cite thesis |last=Shah |first=Tasneem Mohammad |title=Analysis of the multigrid method |bibcode=1989STIN...9123418S |publisher=Oxford University |year=1989 }}</ref>\n\n== Algorithm ==\n[[File:Multigrid Visualization.png|456x456px|\nVisualization of iterative Multigrid algorithm for fast O(n) convergence.\n|thumb]]There are many variations of multigrid algorithms, but the common features are that a hierarchy of discretizations (grids) is considered. The important steps are:<ref>{{cite book |title=Scientific Computing: An Introductory Survey |chapter-url=https://books.google.com/books?id=DPkYAQAAIAAJ&dq=isbn:007112229X |author=M. T. Heath |page=478 ''ff'' |chapter=Section 11.5.7 Multigrid Methods |year=2002 |publisher=McGraw-Hill Higher Education |isbn=978-0-07-112229-0 }}</ref><ref>{{cite book |title=An Introduction to Multigrid Methods |author=P. Wesseling |year=1992 |publisher=Wiley |isbn=978-0-471-93083-9 |url=https://books.google.com/?id=ywOzQgAACAAJ&dq=isbn:0471930830}}</ref>\n\n* '''Smoothing''' – reducing high frequency errors, for example using a few iterations of the [[Gauss–Seidel method]].\n* '''Restriction''' – downsampling the [[residual (numerical analysis)|residual]] error to a coarser grid.\n* '''[[Interpolation]]''' or '''prolongation''' – interpolating a correction computed on a coarser grid into a finer grid.\n\n== Computational cost ==\n\nThis approach has the advantage over other methods that it often scales linearly with the number of discrete nodes used. In other words, it can solve these problems to a given accuracy in a number of operations that is proportional to the number of unknowns.\n\nAssume that one has a differential equation which can be solved approximately (with a given accuracy) on a grid <math>i</math> with a given grid point \ndensity <math>N_i</math>. Assume furthermore that a solution on any grid <math>N_i</math> may be obtained with a given \neffort <math>W_i = \\rho K N_i</math> from a solution on a coarser grid <math>i+1</math>. Here, <math>\\rho = N_{i+1} / N_i < 1</math> is the ratio of grid points on \"neighboring\" grids and is assumed to be constant throughout the grid hierarchy, and <math>K</math> is some constant modeling the effort of computing the result for one grid point.\n\nThe following recurrence relation is then obtained for the effort of obtaining the solution on grid <math>k</math>:\n:<math>W_k = W_{k+1} + \\rho K N_k</math>\nAnd in particular, we find for the finest grid <math>N_1</math> that\n:<math>W_1 = W_2 + \\rho K N_1</math>\nCombining these two expressions (and using <math>N_{k} = \\rho^{k-1} N_1</math>) gives\n:<math>W_1 = K N_1 \\sum_{p=0}^n \\rho^p </math>\nUsing the [[geometric series]], we then find (for finite <math>n</math>)\n:<math>W_1 < K N_1 \\frac{1}{1 - \\rho}</math>\nthat is, a solution may be obtained in <math>O(N)</math> time.\n\n==Multigrid preconditioning==\n\nA multigrid method with an intentionally reduced tolerance can be used as an efficient [[preconditioning|preconditioner]] for an external iterative solver, e.g.,.<ref>Andrew V Knyazev, Klaus Neymeyr. Efficient solution of symmetric eigenvalue problems using multigrid preconditioners in the locally optimal block conjugate gradient method. Electronic Transactions on Numerical Analysis, 15, 38-55, 2003. http://emis.ams.org/journals/ETNA/vol.15.2003/pp38-55.dir/pp38-55.pdf</ref> The solution may still be obtained in <math>O(N)</math> time as well as in the case where the multigrid method is used as a solver. Multigrid preconditioning is used in practice even for linear systems, typically with one cycle per iteration, e.g., in [[Hypre]]. Its main advantage versus a purely multigrid solver is particularly clear for nonlinear problems, e.g., [[eigenvalue]] problems.\n\nIf the matrix of the original equation or an eigenvalue problem is symmetric positive definite (SPD), the preconditioner is commonly constructed to be SPD as well, so that the standard [[conjugate gradient]] (CG) [[iterative methods]] can still be used. Such imposed SPD constraints may complicate the construction of the preconditioner, e.g., requiring coordinated pre- and post-smoothing. However, [[preconditioning|preconditioned]] [[steepest descent]] and [[Conjugate_gradient_method#The_flexible_preconditioned_conjugate_gradient_method|flexible CG methods]] for SPD linear systems and [[LOBPCG]] for symmetric eigenvalue problems are all shown<ref>Henricus Bouwmeester, Andrew Dougherty, Andrew V Knyazev. Nonsymmetric Preconditioning for Conjugate Gradient and Steepest Descent Methods. Procedia Computer Science, Volume 51, Pages 276-285, Elsevier, 2015. https://doi.org/10.1016/j.procs.2015.05.241</ref> to be robust if the preconditioner is not SPD.\n\n==Generalized multigrid methods==\n\nMultigrid methods can be generalized in many different ways.  They can be applied naturally in a time-stepping solution of [[parabolic partial differential equation]]s, or they can be applied directly to time-dependent [[partial differential equation]]s.<ref>{{cite book |chapter-url=https://books.google.com/books?id=GKDQUXzLTkIC&pg=PA165 |editors=Are Magnus Bruaset, Aslak Tveito |title=Numerical solution of partial differential equations on parallel computers |page=165 |chapter=Parallel geometric multigrid |author1=F. Hülsemann |author2=M. Kowarschik |author3=M. Mohr |author4=U. Rüde |publisher=Birkhäuser |year=2006 |isbn=978-3-540-29076-6}}</ref>  Research on multilevel techniques for [[hyperbolic partial differential equation]]s is underway.<ref>For example, {{cite book |title=Computational fluid dynamics: principles and applications |page=305 |url=https://books.google.com/?id=asWGy362QFIC&pg=PA305&lpg=PA305&dq=%22The+goal+of+the+current+research+is+the+significant+improvement+of+the+efficiency+of+multigrid+for+hyperbolic+flow+problems%22#v=onepage&q=%22The%20goal%20of%20the%20current%20research%20is%20the%20significant%20improvement%20of%20the%20efficiency%20of%20multigrid%20for%20hyperbolic%20flow%20problems%22&f=false |author= J. Blaz̆ek |year=2001 |isbn=978-0-08-043009-6 |publisher=Elsevier}} and {{cite book |chapter-url=https://books.google.com/books?id=TapltAX3ry8C&pg=PA369 |author=Achi Brandt and Rima Gandlin |chapter=Multigrid for Atmospheric Data Assimilation: Analysis |page=369 |editors=Thomas Y. Hou, [[Eitan Tadmor]] |title=Hyperbolic problems: theory, numerics, applications: proceedings of the Ninth International Conference on Hyperbolic Problems of 2002 |year=2003 |isbn=978-3-540-44333-9 |publisher=Springer}}</ref> Multigrid methods can also be applied to [[integral equation]]s, or for problems in [[statistical physics]].<ref>{{cite book |title=Multiscale and multiresolution methods: theory and applications |author=Achi Brandt |chapter-url=https://books.google.com/books?id=mtsy6Ci2TRoC&pg=PA53 |editors=Timothy J. Barth, Tony Chan, Robert Haimes |page=53 |chapter=Multiscale scientific computation: review |isbn=978-3-540-42420-8 |year=2002 |publisher=Springer}}</ref>\n\nAnother set of multiresolution methods is based upon [[wavelets]]. These wavelet methods can be combined with multigrid methods.<ref>{{cite book |chapter-url=https://books.google.com/books?id=mtsy6Ci2TRoC&pg=PA140 |author1=Björn Engquist |author2=Olof Runborg |editors=Timothy J. Barth, Tony Chan, Robert Haimes |chapter=Wavelet-based numerical homogenization with applications |title=Multiscale and Multiresolution Methods |isbn=978-3-540-42420-8 |volume=Vol. 20 of Lecture Notes in Computational Science and Engineering |publisher=Springer |year=2002 |page=140 ''ff''}}</ref><ref>{{cite book |url=https://books.google.com/?id=-og1wD-Nx_wC&dq=wavelet+multigrid&printsec=frontcover#v=snippet&q=wavelet%20&f=false |author1=U. Trottenberg |author2=C. W. Oosterlee |author3=A. Schüller |title=''op. cit.'' |isbn=978-0-12-701070-0|year=2001 }}</ref> For example, one use of wavelets is to reformulate the finite element approach in terms of a multilevel method.<ref>{{cite book |title=Numerical Analysis of Wavelet Methods |author=Albert Cohen |url=https://books.google.com/books?id=Dz9RnDItrAYC&pg=PA44 |page=44 |publisher=Elsevier |year=2003 |isbn=978-0-444-51124-9}}</ref>\n\n'''Adaptive multigrid''' exhibits [[adaptive mesh refinement]], that is, it adjusts the grid as the computation proceeds, in a manner dependent upon the computation itself.<ref>{{cite book |author1=U. Trottenberg |author2=C. W. Oosterlee |author3=A. Schüller |title=''op. cit.'' |chapter=Chapter 9: Adaptive Multigrid |chapter-url=https://books.google.com/books?id=-og1wD-Nx_wC&pg=PA356 |page=356 |isbn=978-0-12-701070-0|year=2001 }}</ref> The idea is to increase resolution of the grid only in regions of the solution where it is needed.\n\n==Algebraic MultiGrid (AMG)==\nPractically important extensions of multigrid methods include techniques where no partial differential equation nor geometrical problem background is used to construct the multilevel hierarchy.<ref>{{cite book |title=Matrix-based multigrid: theory and applications |author=Yair Shapira |chapter-url=https://books.google.com/books?id=lCDGhpDDk5IC&pg=PA66 |chapter=Algebraic multigrid |page=66 |isbn=978-1-4020-7485-1 |publisher=Springer |year=2003}}</ref> Such '''algebraic multigrid methods''' (AMG) construct their hierarchy of operators directly from the system matrix. In classical AMG, the levels of the hierarchy are simply subsets of unknowns without any geometric interpretation. (More generally, coarse grid unknowns can be particular linear combinations of fine grid unknowns.) Thus, AMG methods become black-box solvers for certain classes of [[sparse matrices]]. AMG is regarded as advantageous mainly where geometric multigrid is too difficult to apply,<ref>{{cite book |author1=U. Trottenberg |author2=C. W. Oosterlee |author3=A. Schüller |title=''op. cit.'' |url=https://books.google.com/books?id=-og1wD-Nx_wC&pg=PA417 |page=417 |isbn=978-0-12-701070-0|year=2001 }}</ref> but is often used simply because it avoids the coding necessary for a true multigrid implementation. While classical AMG was developed first, a related algebraic method is known as smoothed aggregation (SA).\n\n== Multigrid in time methods ==\nMultigrid methods have also been adopted for the solution of [[initial value problem]]s.<ref>{{cite journal\n| last       = Hackbusch\n| first      =  Wolfgang\n| date       = 1985\n| title      = Parabolic multi-grid methods\n| url        = http://dl.acm.org/citation.cfm?id=4673.4714\n| journal    = Computing Methods in Applied Sciences and Engineering, VI\n| volume     = \n| issue      = \n| pages      = 189–197 \n| bibcode    = \n| doi        = \n| access-date= August 2015\n}}</ref>\nOf particular interest here are parallel-in-time multigrid methods:<ref>{{cite journal\n| last       = Horton\n| first      =  Graham\n| date       = 1992\n| title      = The time-parallel multigrid method\n| journal    = Communications in Applied Numerical Methods\n| volume     = 8\n| issue      = 9\n| pages      =  585–595\n| bibcode    = \n| doi        = 10.1002/cnm.1630080906\n}}</ref>\nin contrast to classical [[Runge–Kutta methods|Runge-Kutta]] or [[Linear multistep method|linear multistep]] methods, they can offer [[Parallel computing|concurrency]] in temporal direction.\nThe well known [[Parareal]] parallel-in-time integration method can also be reformulated as a two-level multigrid in time.\n\n==Notes==\n{{reflist|30em}}\n\n== References ==\n* G. P. Astrachancev (1971), [https://www.sciencedirect.com/science/article/pii/0041555371901704 An iterative method of solving elliptic net problems]. USSR Comp. Math. Math. Phys. 11, 171–182.\n* N. S. [[Bakhvalov]] (1966), [https://www.sciencedirect.com/science/article/pii/0041555366901182 On the convergence of a relaxation method with natural constraints on the elliptic operator]. USSR Comp. Math. Math. Phys. 6, 101–13.\n* [[Achi Brandt]] (April 1977), \"[https://www.jstor.org/stable/2006422 Multi-Level Adaptive Solutions to Boundary-Value Problems]\", ''Mathematics of Computation'', '''31''': 333–90.\n* William L. Briggs, Van Emden Henson, and Steve F. McCormick (2000), ''[https://web.archive.org/web/20061006153457/http://www.llnl.gov/casc/people/henson/mgtut/welcome.html A Multigrid Tutorial]'' (2nd ed.), Philadelphia: [[Society for Industrial and Applied Mathematics]], {{ISBN|0-89871-462-1}}.\n* R. P. Fedorenko (1961), [http://www.mathnet.ru/eng/zvmmf8014 A relaxation method for solving elliptic difference equations]. USSR Comput. Math. Math. Phys. 1, p.&nbsp;1092.\n* R. P. Fedorenko (1964), The speed of convergence of one iterative process. USSR Comput. Math. Math. Phys. 4, p.&nbsp;227.\n* {{cite book | last1=Press | first1=W. H. | last2=Teukolsky | first2=S. A. | last3=Vetterling | first3=W. T. | last4=Flannery | first4=B. P. | year=2007 | title=Numerical Recipes: The Art of Scientific Computing | edition=3rd | publisher=Cambridge University Press | location=New York | isbn=978-0-521-88068-8 | chapter=Section 20.6. Multigrid Methods for Boundary Value Problems | chapter-url=http://apps.nrbook.com/empanel/index.html#pg=1066}}\n\n== External links ==\n*[http://www.mgnet.org/ Repository for multigrid, multilevel, multiscale, aggregation, defect correction, and domain decomposition methods]\n*[http://www.mgnet.org/mgnet/tutorials/xwb/xwb.html Multigrid tutorial]\n*[https://computation.llnl.gov/project/linear_solvers/talks/AMG_TUT_PPT/sld001.htm Algebraic multigrid tutorial]\n*[https://web.archive.org/web/20100527194456/https://computation.llnl.gov/casc/linear_solvers/present.html Links to AMG presentations]\n\n{{Numerical PDE}}\n\n{{DEFAULTSORT:Multigrid Method}}\n[[Category:Numerical analysis]]\n[[Category:Partial differential equations]]\n[[Category:Wavelets]]"
    },
    {
      "title": "Multilevel fast multipole method",
      "url": "https://en.wikipedia.org/wiki/Multilevel_fast_multipole_method",
      "text": "\nThe '''multilevel fast multipole method (MLFMM)''' is used along with [[Boundary element method|method of moments (MoM)]] a numerical computational method of solving  linear partial differential equations which have been formulated as integral equations  of large objects almost faster without loss in accuracy.<ref>{{cite web | url=http://www.kfs.oeaw.ac.at/index.php?option=com_content&view=article&id=361:multilevel-fast-multipole-method-mlfmm&catid=104&Itemid=696&lang=en | title=Multilevel Fast Multipole Method (MLFMM) | publisher=Austrian Academy of Sciences – Acoustics Research Institute | accessdate=20 April 2014}}</ref> This method is an alternative formulation of the technology behind the MoM and is applicable to much larger structures like [[radar cross-section]] (RCS) analysis, antenna integration on large structures, [[Reflector (antenna)|reflector antenna]] design, finite size [[Antenna array (electromagnetic)|antenna arrays]], etc., making full-wave current-based solutions of such structures a possibility.<ref>{{cite web | url=https://www.feko.info/product-detail/numerical_methods/mlfmm/mlfmm | title=Multilevel Fast Multipole Method (MLFMM) | publisher=Feko | accessdate=20 April 2014}}</ref><ref>{{cite web | url=http://www.efieldsolutions.com/mlfmm.php | title=Multilevel Fast Multipole Method (MLFMM) | publisher=E field | accessdate=20 April 2014}}</ref>\n\n== Method ==\nThe MLFMM is based on the Method of Moments (MoM), but reduces the memory complexity from N<sup>2</sup> to NlogN, and the solving complexity from N<sup>3</sup> to N<sub>iter</sub>NlogN, where N represents the number of unknowns and N<sub>iter</sub> the number of iterations in the solver. This method subdivides the Boundary Element mesh into different clusters and if two clusters are in each other's far field, all calculations that would have to be made for every pair of nodes can be reduced to the midpoints of the clusters with almost no loss of accuracy. For clusters not in the far field, the traditional BEM has to be applied. That is MLFMM introduces different levels of clustering (clusters made out of smaller clusters) to additionally enhance computation speed.<ref>{{cite web | url=http://www.jpier.org/PIER/pier.php?paper=08013003 | title=SCHWARZ-KRYLOV SUBSPACE METHOD FOR MLFMM ANALYSIS OF ELECTROMAGNETIC WAVE SCATTERING PROBLEMS | publisher=PIER | accessdate=20 April 2014 |author1=P.-L. Rui |author2=R.-S. Chen |author3=Z.-W. Liu |author4=Y.-N. Gan  |last-author-amp=yes }}</ref><ref>{{cite web | url=http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=6105819&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6105819 | title=Theory and application of an MLFMM/FEM hybrid framework in FEKO | date=7–9 Nov 2011 | accessdate=20 April 2014 |author1=Bingle, M Burger, E. |author2=Jakobus, U. |author3=van Tonder, J.J. }}</ref><ref>{{cite web | url=http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5031571 | title=Parallel computation methods for enhanced MOM and MLFMM performance | date=11 May 2009 | accessdate=20 April 2014 |author1=D'Ambrosio, K. |author2=Pirich, R. |author3=Kaufman, A. |author4=Mesecher, D. }}</ref><ref>{{cite web | url=http://www.emss.de/downloads/pdffiles/jak08c_p.pdf | title=Advanced EMC Modeling by Means of a Parallel MLFMM and Coupling with Network Theory | publisher=EMSS | accessdate=20 April 2014 |author1=Ulrich Jakobus |author2=Johann van Tonder |author3=Marlize Schoeman  |last-author-amp=yes }}</ref><ref>{{cite web | url=https://www.cst.com/Content/Events/UGM2009/6-1-3-Electrically-Large-Applications-and-Integral-Equation-solver.pdf | title=(Electrically) Large Applications & Integral Equation solver | publisher=CST | accessdate=20 April 2014}}</ref><ref>{{cite web | url=https://www.esi-group.com/software-services/virtual-environment/electromagnetics/cem-solutions/multilevel-fast-multipole | title=Multilevel Fast Multipole Method (MLFMM) | publisher=ESI | accessdate=20 April 2014}}</ref>\n\n== References ==\n{{Reflist}}\n\n[[Category:Numerical differential equations]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Multilevel Monte Carlo method",
      "url": "https://en.wikipedia.org/wiki/Multilevel_Monte_Carlo_method",
      "text": "'''Multilevel Monte Carlo (MLMC) methods''' in [[numerical analysis]] are [[algorithm]]s for computing [[Expected value|expectation]]s that arise in [[stochastic simulation]]s. Just as [[Monte Carlo method]]s, they rely on repeated [[Simple random sample|random sampling]], but these samples are taken on different levels of accuracy. MLMC methods can greatly reduce the computational cost of standard Monte Carlo methods by taking most samples with a low accuracy and corresponding low cost, and only very few samples are taken at high accuracy and corresponding high cost.\n\n== Goal ==\nThe goal of a multilevel Monte Carlo method is to approximate the [[expected value]] <math>\\operatorname{E}[G]</math> of the [[random variable]] <math>G</math> that is the output of a [[stochastic simulation]]. Suppose this random variable cannot be simulated exactly, but there is a sequence of approximations <math>G_0, G_1, \\ldots, G_L</math> with increasing accuracy, but also increasing cost, that converges to <math>G</math> as <math>L\\rightarrow\\infty</math>. The basis of the multilevel method is the [[telescoping sum]] identity,<ref>{{cite journal |last=Giles |first=M. B. |date=2015 |title=Multilevel Monte Carlo Methods |journal=Acta Numerica |volume=24 |pages=259–328}}</ref>\n\n{{align|center|<math> \\operatorname{E}[G_{L}] = \\operatorname{E}[G_{0}] + \\sum_{\\ell=1}^L \\operatorname{E}[G_\\ell - G_{\\ell-1}],</math>}}\n\nthat is trivially satisfied because of the linearity of the expectation operator. Each of the expectations <math>\\operatorname{E}[G_\\ell - G_{\\ell-1}]</math> is then approximated by a Monte Carlo method, resulting in the multilevel Monte Carlo method. Note that taking a sample of the difference <math>G_\\ell - G_{\\ell-1}</math> at ''level'' <math>\\ell</math> requires a simulation of both <math>G_{\\ell}</math> and <math>G_{\\ell-1}</math>.\n\nThe MLMC method works if the [[variance]]s <math>\\operatorname{V}[G_\\ell - G_{\\ell-1}]\\rightarrow0</math> as <math>\\ell\\rightarrow\\infty</math>, which will be the case if both <math>G_{\\ell}</math> and <math>G_{\\ell-1}</math> approximate the same random variable <math>G</math>. By the [[Central Limit Theorem]], this implies that one needs fewer and fewer samples to accurately approximate the expectation of the difference <math>G_\\ell - G_{\\ell-1}</math> as <math>\\ell\\rightarrow\\infty</math>. Hence, most samples will be taken on level <math>0</math>, where samples are cheap, and only very few samples will be required at the finest level <math>L</math>. In this sense, MLMC can be considered as a recursive [[control variate]] strategy.\n\n==Applications==\n[[File:Multilevel monte carlo sample paths for an SDE.png|thumb|Approximation of a sample path of an SDE on different levels.|right]]\nThe first application of MLMC is attributed to Giles,<ref>{{cite journal |last=Giles |first=M. B. |date=2008 |title=Multilevel Monte Carlo Path Simulation |journal=Operations Research |volume=56 |issue=3 |pages=607–617|url=https://ora.ox.ac.uk/objects/uuid:d9d28973-94aa-4179-962a-28bcfa8d8f00/datastreams/ATTACHMENT01}}</ref> in the context of [[stochastic differential equations]] (SDEs) for [[Monte Carlo option model|option pricing]], however, earlier traces are found in the work of Heinrich in the context of parametric integration.<ref>{{cite journal |last=Heinrich |first=S. |date=2001 |title=Multilevel Monte Carlo Methods |journal=Lecture Notes in Computer Science (Multigrid Methods) |publisher=Springer |volume=2179 |pages=58–67}}</ref> Here, the random variable <math>G=f(X(T))</math> is known as the payoff function, and the sequence of approximations <math>G_\\ell</math>, <math>\\ell=0,\\ldots,L</math> use an approximation to the sample path <math>X(t)</math> with time step <math>h_\\ell=2^{-\\ell}T</math>.\n\nThe application of MLMC to problems in [[uncertainty quantification]] (UQ) is an active area of research.<ref>{{cite journal |last1=Cliffe |first1=A. |last2=Giles |first2=M. B. |last3=Scheichl |first3=R. |last4=Teckentrup |first4=A. |date=2011 |title=Multilevel Monte Carlo Methods and Applications to Elliptic PDEs with Random Coefficients |journal=Computing and Visualization in Science |volume=14 |issue=1 |pages=3–15|url=http://people.maths.ox.ac.uk/~gilesm/files/cgst.pdf}}</ref><ref>{{cite journal |last1=Pisaroni |first1=M. |last2=Nobile |first2=F. B. |last3=Leyland |first3=P. |date=2017 |title=A Continuation Multi Level Monte Carlo Method for Uncertainty Quantification in Compressible Inviscid Aerodynamics |journal=Computer Methods in Applied Mechanics and Engineering |volume=326 |issue=C |pages=20–50|url=https://pdfs.semanticscholar.org/6dbc/8dde601b1757c42a4c54fa9cfd69317c82c8.pdf}}</ref> An important prototypical example of these problems are [[partial differential equations]] (PDEs) with [[Stochastic partial differential equation|random coefficients]]. In this context, the random variable <math>G</math> is known as the quantity of interest, and the sequence of approximations corresponds to a [[discretization]] of the PDE with different mesh sizes.\n\n== An algorithm for MLMC simulation ==\nA simple level-adaptive algorithm for MLMC simulation is given below in pseudo-code.\n <math>L\\gets0</math>\n '''repeat'''\n    Take warm-up samples at level <math>L</math>\n    Compute the sample variance on all levels <math>\\ell=0,\\ldots,L</math>\n    Define the optimal number of samples <math>N_\\ell</math> on all levels <math>\\ell=0,\\ldots,L</math>\n    Take additional samples on each level <math>\\ell</math> according to <math>N_\\ell</math>\n    '''if''' <math>L\\geq2</math> '''then'''\n       Test for convergence\n    '''end'''\n    '''if''' not converged '''then'''\n       <math>L\\gets L+1</math>\n    '''end'''\n '''until''' converged\n\n==Extensions of MLMC==\nRecent extensions of the multilevel Monte Carlo method include multi-index Monte Carlo,<ref>{{cite journal |last1=Haji-Ali |first1=A. L. |last2=Nobile |first2=F. |last3=Tempone |first3=R. |date=2016 |title=Multi-Index Monte Carlo: When Sparsity Meets Sampling |journal=Numerische Mathematik |volume=132 |issue=4 |pages=767--806|url=https://arxiv.org/pdf/1405.3757}}</ref> where more than one direction of refinement is considered, and the combination of MLMC with the [[Quasi-Monte Carlo method]].<ref>{{cite journal |last1=Giles |first1=M. B. |last2=Waterhouse |first2=B. |date=2009 |title=Multilevel Quasi-Monte Carlo Path Simulation |journal=Advanced Financial Modelling, Radon Series on Computational and Applied Mathematics |publisher=De Gruyter |pages=165--181|url=http://people.maths.ox.ac.uk/gilesm/files/jcf07.pdf}}</ref><ref>{{cite journal |last1=Robbe |first1=P. |last2=Nuyens |first2=D. |last3=Vandewalle |first3=S. |date=2017 |title=A Multi-Index Quasi-Monte Carlo Algorithm for Lognormal Diffusion Problems |journal=SIAM Journal on Scientific Computing |volume=39 |issue=5 |pages=A1811-C392|url=https://arxiv.org/pdf/1608.03157}}</ref>\n\n== See also ==\n* [[Monte Carlo method]]\n* [[Monte Carlo methods in finance]]\n* [[Quasi-Monte Carlo methods in finance]]\n* [[Uncertainty quantification]]\n* [[Stochastic partial differential equation|Partial differential equations with random coefficients]]\n\n== References ==\n{{reflist}}\n\n[[Category:Monte Carlo methods]]\n[[Category:Numerical analysis]]\n[[Category:Sampling techniques]]\n[[Category:Stochastic simulation]]\n[[Category:Randomized algorithms]]"
    },
    {
      "title": "Multiphysics",
      "url": "https://en.wikipedia.org/wiki/Multiphysics",
      "text": "{{Underlinked|date=August 2018}}\n\n'''Multiphysics''' is defined as the coupled processes or systems involving more than one simultaneously occurring physical fields and the studies of and knowledge about these processes and systems.<ref name=\":0\">{{Cite book|url=https://www.springer.com/us/book/9783319930275|title=Multiphysics in Porous Materials {{!}} Zhen (Leo) Liu {{!}} Springer|language=en}}</ref> As an interdisciplinary study area, multiphysics spans over many science and engineering disciplines. Multiphysics is a practice built on mathematics, physics, application, and [[numerical analysis]]. The mathematics involved usually contains [[partial differential equations]] and [[tensor analysis]]. The physics refers to common types of physical processes, e.g., [[heat transfer]] (thermo-), pore water movement (hydro-), concentration field (concentro or diffuso/convecto/advecto), stress and strain (mechano-), dynamics (dyno-), chemical reactions (chemo- or chemico-), electrostatics (electro-), and magnetostatics (magneto-).<ref>{{Cite web|url=https://www.multiphysics.us|title=Multiphysics Learning & Networking - Home Page|website=www.multiphysics.us|access-date=2018-08-19}}</ref>\n\n== Definition ==\nThere are multiple definitions for multiphysics. In a broad sense, multiphysics refers to simulations that involve multiple physical models or multiple simultaneous physical phenomena. The inclusion of “multiple physical models” makes this definition a very broad and general concept, but this definition is a little bit self-contradictory as the implication of physical models may include that of physical phenomena.<ref name=\":0\" /> COMSOL defines multiphysics in a relatively narrow sense: multiphysics includes 1. coupled physical phenomena in computer simulation and 2. the study of multiple interacting physical properties. In another definition, a multiphysics system consists of more than one component governed by its own principle(s) for evolution or equilibrium, typically conservation or constitutive laws.<ref name=\":1\">{{Citation|last=Krzhizhanovskaya|first=Valeria V.|title=Simulation of Multiphysics Multiscale Systems: Introduction to the ICCS'2007 Workshop|date=2007|work=Computational Science – ICCS 2007|pages=755–761|publisher=Springer Berlin Heidelberg|language=en|doi=10.1007/978-3-540-72584-8_100|isbn=9783540725831|last2=Sun|first2=Shuyu}}</ref><ref name=\":2\">{{cite arxiv|last=Groen|first=Derek|last2=Zasada|first2=Stefan J.|last3=Coveney|first3=Peter V.|date=2012-08-31|title=Survey of Multiscale and Multiphysics Applications and Communities|eprint=1208.6444|class=cs.OH}}</ref> This definition is very close to the previous one except for that it does not emphasize physical properties. In a more strict way, multiphysics can be defined as those processes including closely coupled interactions among separate continuum physics phenomena.<ref>{{Cite web|url=https://nafems.org/downloads/FENet.../St...2005/fenet_malta_may2005_mpa.pdf|title=NAFEMS downloads engineering analysis and simulation - FEA, Finite Element Analysis, CFD, Computational Fluid Dynamics, and Simulation|last=www.duodesign.co.uk|website=nafems.org|access-date=2018-08-19}}</ref> In this definition, two-way exchange of information between physical fields, which could involve implicit convergence within a time step is the essential feature. Based on the above definitions, multiphysics is defined as the coupled processes or systems involving more than one simultaneously occurring physical fields and also the studies of and knowledge about these processes and systems.<ref name=\":0\" />\n\n== History and future ==\nMultiphysics is neither a research concept far from daily life nor a recently developed theory or technique. In fact, we live in a multiphysics world. Natural and artificial systems are running with various types of physical phenomena at different spatial and temporal scales: from atoms to galaxies and from pico-seconds to centuries. A few representative examples in fundamental and applied sciences are loads and deformations on solids, complex flows, fluid-structure interactions, plasma and chemical processes, thermo-mechanical and electromagnetic systems.<ref name=\":0\" /><ref name=\":1\" />\n\nMultiphysics has rapidly developed into a research and application area across many science and engineering disciplines. There is a clear trend that more and more challenging problems we are faced with involve physical processes that cannot be covered by a single traditional discipline. This trend requires us to extend our analysis capacity to solve more complicated and more multidisciplinary problems. Modern academic communities are confronted with problems of rapidly increasing complexity, which straddle across the traditional disciplinary boundaries between physics, chemistry, material science and biology. Multiphysics has also become a frontier in industrial practice. Simulation programs have been evolving into a tool in design, product development, and quality control. During these creation processes, engineers are now required to think in areas outside of their training, even with the assistance of the simulation tools. It is more and more necessary for the modern engineers to know and grasp the concept of what is known deep inside the engineering world as “multiphysics.” <ref>{{Cite news|url=https://eandt.theiet.org/content/articles/2015/03/multiphysics-brings-the-real-world-into-simulations/|title=Multiphysics brings the real world into simulations|date=2015-03-16|access-date=2018-08-19|language=en-US}}</ref> The auto industry gives out a good example. Traditionally, different groups of people focus on the structure, fluids, electromagnets and the other individual aspect separately. By contrast, the intersection of aspects, which may represent two physics topics and once was a gray area, can be the essential link in the life cycle of the product. As commented by,<ref>{{Cite journal|last=Thilmany|first=Jean|date=2010-02-01|title=Multiphysics: All at Once|journal=Mechanical Engineering Magazine Select Articles|volume=132|issue=2|pages=39–41|doi=10.1115/1.2010-Feb-5|issn=0025-6501}}</ref> “Design engineers are running more and more multiphysics simulations every day because they need to add reality into their models.”\n\n== Types of multiphysics ==\nThe part “physics” in “multiphysics” denotes “physical field”. There, multiphysics means the coexistence of multiple physical fields in a process or a system. In physics, a field is a physical quantity that has a value for each point in space and time. For example, on a weather map, a vector at each point of the map can used to represent the surface wind velocity with both speed and direction for the movement of air at that point.<ref name=\":0\" />\n\n== How to do multiphysics ==\nThe implementation of multiphysics usually follows the following procedure: identifying a multiphysical process/system, developing a mathematical description of this process/system, discretizing this mathematical model into an algebraic system, solving this algebraic equation system, and postprocessing the data. The abstraction of a multiphysical problem from a complex phenomenon and the description of such a problem are usually not emphasized but very critical to the success of the multiphysics analysis. This requires to identify the system to be analyzed, including geometry, materials and dominant mechanisms. The identified system will be interpreted using mathematics languages (function, tensor, differential equation) as computational domain, boundary conditions, auxiliary equations and governing equations. Discretization, solution and postprocessing are carried out using computers. Therefore, the above procedure is not much different from those in general numerical simulation based on the discretization of partial differential equations.<ref name=\":0\" />\n\n=== Mathematical model ===\nA mathematical model is essentially a set of equations. The equations can be divided into three categories according to the nature and intended role. The first category is governing equations. A governing equation describes the major physical mechanisms and process without further revealing the change and nonlinearity of the material properties. For example, in a heat transfer problem, the governing equation could describe a process in which the thermal energy (represented using temperature or enthalpy) at an infinitesimal point or a representative element volume is changed due to energy transferred from surrounding points via conduction, advection, radiation, and internal heat sources or any combinations of these four heat transfer mechanisms as the following equation:<ref name=\":0\" />\n\n<math>{\\underbrace{\\frac{\\partial u}{\\partial t} }_{{\\rm Accumulation}}\\underbrace{+\\nabla \\cdot \\left(uv\\right)}_{{\\rm Advection}}\\underbrace{-\\nabla \\cdot \\left(D\\nabla u\\right)}_{{\\rm Diffusion\\; }\\left({\\rm Conduction, Dispersion}\\right)}=\\underbrace{Q}_{{\\rm Source}}} \n</math>.\n\nCouplings between fields can be achieved in each category.\n\n==Discretization method==\nMultiphysics is usually numerical implemented with discrectization methods such Finite Element Method, Finite Difference Method, and Finite Volume Method. Many software packages mainly rely on the [[finite element method]] or similar commonplace numerical methods for simulating coupled physics: thermal stress, electro- and acousto- magnetomechanical interaction.<ref>S. Bagwell, P.D. Ledger, A.J. Gil, M. Mallett, M. Kruip, A linearised hp–finite element framework for acousto-magneto-mechanical coupling in axisymmetric MRI scanners, DOI: 10.1002/nme.5559</ref>\n\n==References==\n{{reflist}}\n* [[Susan L. Graham]], Marc Snir, and Cynthia A. Patterson (Editors), ''Getting Up to Speed: The Future of Supercomputing,'' [http://books.nap.edu/html/up_to_speed/appD.html Appendix D]. The National Academies Press, Washington DC, 2004. {{ISBN|0-309-09502-6}}.\n* Paul Lethbridge, ''Multiphysics Analysis'', p26, The Industrial Physicist, Dec 2004/Jan 2005, [http://www.aip.org/tip/INPHFA/vol-10/iss-6/p26.html], Archived at: [https://web.archive.org/web/20041204052110/http://www.aip.org:80/tip/INPHFA/vol-10/iss-6/p26.html]\n\n[[Category:Numerical analysis]]\n[[Category:Computational physics]]"
    },
    {
      "title": "ND4J (software)",
      "url": "https://en.wikipedia.org/wiki/ND4J_%28software%29",
      "text": "{{Infobox software\n| name                   = ND4J: N-Dimensional Arrays and Linear Algebra for Java\n| logo                   = \n| screenshot             = \n| caption                =\n| collapsible            =\n| author                 = Adam Gibson, Alex D. Black, V. Kokorin\n| developer              = Various<ref>[https://github.com/deeplearning4j/nd4j ND4J developers]</ref>\n| released               = {{Start date and age|2014|09|15|df=yes}}\n| latest release version = 0.9.1\n| latest release date    = {{Start date and age|2017|08|03|df=yes}}\n| latest preview version = \n| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD}} -->\n| programming language   = [[Java (programming language)]], [[C++]]\n| operating system       = [[Linux]], [[macOS]], [[Microsoft Windows|Windows]], [[Android (operating system)|Android]]\n| platform               = [[Cross-platform]]\n| size                   =\n| language               = English\n| genre                  = [[Natural language processing]], [[deep learning]], [[machine vision]]\n| license                = [[Apache License|Apache]] 2.0\n| website                = {{URL|nd4j.org}}\n}}\n\n'''ND4J''' is a scientific computing and linear algebra [[Library (computing)|library]], written in the programming language [[Java (programming language)|Java]], operating on the [[Java virtual machine]] (JVM), and compatible with other languages such as [[Scala (programming language)|Scala]], and [[Clojure]].<ref>{{cite web|title=Official website|url=https://deeplearning4j.org/docs/latest/nd4j-overview}}</ref><ref>{{cite web|title=The Deeplearning4j Framework|url=http://gtts.ehu.es/WDW/biblio/JavaMagazine/javamagazine20150506-dl.pdf|access-date=2016-02-29|archive-url=https://web.archive.org/web/20160305172929/http://gtts.ehu.es/WDW/biblio/JavaMagazine/javamagazine20150506-dl.pdf|archive-date=2016-03-05|dead-url=yes|df=}}</ref> ND4J was contributed to the [[Eclipse Foundation]] in October 2017.<ref>{{cite web|title=Eclipse Deeplearning4j Project Page|url=https://projects.eclipse.org/proposals/deeplearning4j}}</ref>\n\nND4J is for performing [[linear algebra]] and [[Matrix (mathematics)|matrix]] manipulation in a production environment, integrating with [[Apache Hadoop]] and [[Apache Spark|Spark]] to work with distributed [[central processing unit]]s (CPUs) or [[graphics processing unit]]s (GPUs). It supports n-dimensional arrays for JVM-based languages.\n\nND4J is [[free and open-source software]], released under [[Apache License]] 2.0, and developed mostly by the group in [[San Francisco]] that built [[Deeplearning4j]].<ref>{{cite web|title=Github Repository|url=https://github.com/deeplearning4j/deeplearning4j/tree/master/nd4j}}</ref> It was created under an [[Apache Software Foundation]] license.\n\n==Distributed Computing==\n\nND4J's operations include [[Distributed computing|distributed]] [[Parallel computing|parallel]] versions. Operation can occur in a cluster and process massive amounts of data. Matrix manipulation occurs in parallel on CPUs or GPUs over [[cloud computing]], and can work in Apache Spark or Hadoop clusters.\n\n==Other JVM Scientific Computing Libraries==\n\nA usability gap has separated Java, Scala,<ref>{{cite web|title=Github Repository|url=https://github.com/deeplearning4j/deeplearning4j/tree/master/nd4s}}</ref> Kotlin and Clojure programmers from powerful tools in data analysis such as NumPy or Matlab. Libraries like Breeze don't support n-dimensional arrays, or tensors, which are necessary for deep learning and other tasks. Libraries like Colt and Parallel Colt use or have dependencies with GPL in the license, making them unsuitable for commercial use. ND4J was built to address those functional and licenses issues.\n\n==See also==\n{{Portal|Free and open-source software|Java (programming language)}}\n* [[NumPy]]\n* [[SciPy]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* {{Official website|https://deeplearning4j.org/docs/latest/nd4j-overview}}\n\n[[Category:Applied machine learning]]\n[[Category:Array programming languages]]\n[[Category:Numerical programming languages]]\n[[Category:Data mining and machine learning software]]\n[[Category:Free statistical software]]\n[[Category:Java platform]]\n[[Category:Java programming language family]]\n[[Category:JVM programming languages]]\n[[Category:Scala (programming language)]]\n[[Category:Linear algebra]]\n[[Category:Numerical analysis]]\n[[Category:Computational statistics]]\n[[Category:Artificial neural networks]]\n[[Category:Free software programmed in Java (programming language)]]\n[[Category:Free data analysis software]]\n[[Category:Free science software]]\n[[Category:Numerical analysis software for Linux]]\n[[Category:Numerical analysis software for MacOS]]\n[[Category:Numerical analysis software for Windows]]\n[[Category:Free mathematics software]]\n[[Category:Java (programming language) libraries]]\n[[Category:Numerical software]]\n[[Category:Cluster computing]]\n[[Category:Hadoop]]\n[[Category:Software using the Apache license]]\n[[Category:Deep learning]]\n[[Category:Neural network software]]\n[[Category:Open-source artificial intelligence]]\n\n\n{{compu-stub}}"
    },
    {
      "title": "ND4S",
      "url": "https://en.wikipedia.org/wiki/ND4S",
      "text": "{{More citations needed|date=August 2015}}\n{{Infobox software\n| name                   = ND4S: N-Dimensional Arrays for Scala\n| logo                   = \n| screenshot             = \n| caption                =\n| collapsible            =\n| author                 = Adam Gibson, Taisuke Oe\n| developer              = [https://github.com/deeplearning4j/nd4s various]\n| released               =\n| latest release version = 0.7.2\n| latest release date    = {{Start date and age|2016|12|27|df=yes}}\n| latest preview version = \n| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD}} -->\n| frequently updated     = <!-- DO NOT include this parameter unless you know what it does -->\n| programming language   = [[Scala (programming language)|Java]]\n| operating system       = [[Linux]], [[macOS]], [[Microsoft Windows|Windows]], [[Android (operating system)|Android]]\n| platform               = [[Cross-platform]]\n| size                   =\n| language               = \n| status                 = active\n| genre                  = [[Natural language processing]], [[Deep learning]], [[Machine vision]]\n| license                = [[Apache License|Apache 2.0]]\n| website                = {{URL|https://github.com/deeplearning4j/nd4s}}\n}}\n\n'''ND4S''' is a free, [[Open-source software|open-source]] extension of the [[Scala (programming language)|Scala programming language]] operating on the [[JVM|Java Virtual Machine]] – though it is compatible with both [[Java (programming language)|Java]] and [[Clojure (programming language)|Clojure]].<ref>{{cite web|title=Official Website|url=http://nd4j.org/}}</ref>\n\nND4S is a scientific [[Library (computing)|computing library]] for [[linear algebra]] and [[Matrix (mathematics)|matrix]] manipulation in a production environment, integrating with [[Apache Hadoop|Hadoop]] and [[Apache Spark|Spark]] to work with distributed [[Graphics processing unit|GPUs]]. It supports n-dimensional arrays for [[List of JVM languages|JVM-based languages]].\n\nND4S has primarily been developed by the group in San Francisco that built [[Deeplearning4j]], led by Adam Gibson.<ref>{{cite web|title=Github Repository|url=https://github.com/deeplearning4j/nd4s}}</ref> It was created under an [[Apache Software Foundation]] license.\n\n==Distributed==\nND4S's operations include [[Distributed computing|distributed]] [[Parallel computing|parallel]] versions. They can take place in a [[Computer cluster|cluster]] and process massive amounts of data. Matrix manipulation occurs in parallel on GPUs or CPUs in the cloud, and can work within Spark or Hadoop clusters.\n\n==Benchmarks==\nBenchmarks show that ND4S runs roughly twice as fast as [[NumPy]] on large matrices.<ref>{{cite web|title=ND4J Benchmarks|url=http://nd4j.org/benchmarking}}</ref>{{failed verification|date=October 2017}}\n\n==See also==\n{{Portal|Free and open-source software|Java (programming language)}}\n* [[NumPy]]\n* [[SciPy]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* {{official website|https://github.com/deeplearning4j/nd4s}}\n\n[[Category:Array programming languages]]\n[[Category:Numerical programming languages]]\n[[Category:Data mining and machine learning software]]\n[[Category:Free statistical software]]\n[[Category:Java platform]]\n[[Category:Java programming language family]]\n[[Category:JVM programming languages]]\n[[Category:Scala (programming language)]]\n[[Category:Linear algebra]]\n[[Category:Numerical analysis]]\n[[Category:Computational statistics]]\n[[Category:Artificial neural networks]]\n[[Category:Free software programmed in Java (programming language)]]\n[[Category:Free science software]]\n[[Category:Numerical analysis software for Linux]]\n[[Category:Numerical analysis software for MacOS]]\n[[Category:Numerical analysis software for Windows]]\n[[Category:Free mathematics software]]\n[[Category:Java (programming language) libraries]]\n[[Category:Numerical software]]\n[[Category:Cluster computing]]\n[[Category:Hadoop]]\n[[Category:Software using the Apache license]]\n[[Category:Free software programmed in Scala]]\n\n\n{{compu-stub}}"
    },
    {
      "title": "Newton fractal",
      "url": "https://en.wikipedia.org/wiki/Newton_fractal",
      "text": "[[File:Julia set for the rational function.png|240px|right|thumb|Julia set for the rational function associated to Newton's method for ƒ:z→z<sup>3</sup>−1.]]\n\nThe '''Newton fractal''' is a [[boundary set]] in the [[complex plane]] which is characterized by [[Newton's method]] applied to a fixed [[polynomial]] <math>p(Z)\\in\\mathbb{C}[Z]</math> or [[transcendental function]]. It is the [[Julia set]] of the [[meromorphic function]] <math>z\\mapsto z-\\tfrac{p(z)}{p'(z)}</math> which is given by Newton's method. When there are no attractive cycles (of order greater than 1), it divides the complex plane into regions <math>G_k</math>, each of which is associated with a [[root of a function|root]] <math>\\zeta_k</math> of the polynomial, <math>k=1,\\ldots,\\deg(p)</math>.  In this way the Newton fractal is similar to the [[Mandelbrot set]], and like other fractals it exhibits an intricate appearance arising from a simple description.  It is relevant to [[numerical analysis]] because it shows that (outside the region of [[quadratic convergence]]) the Newton method can be very sensitive to its choice of start point.\n\nMany points of the complex plane are associated with one of the <math>\\deg(p)</math> roots of the polynomial in the following way:  the point is used as starting value <math>z_0</math> for Newton's iteration <math>z_{n+1}:=z_n-\\frac{p(z_n)}{p'(z_n)}</math>, yielding a sequence of points <math>z_1,z_2,\\ldots,</math> If the sequence converges to the root <math>\\zeta_k</math>, then <math>z_0</math> was an element of the region <math>G_k</math>. However, for every polynomial of degree at least 2 there are points for which the Newton iteration does not converge to any root: examples are the boundaries of the basins of attraction of the various roots. There are even polynomials for which open sets of starting points fail to converge to any root: a simple example is <math>z^3-2z+2</math>, where some points are attracted by the cycle 0,&nbsp;1,&nbsp;0,&nbsp;1&nbsp;... rather than by a root.\n\nAn open set for which the iterations converge towards a given root or cycle (that is not a fixed point), is a [[Fatou set]] for the iteration. The complementary set to the union of all these, is the Julia set. The Fatou sets have common boundary, namely the Julia set. Therefore, each point of the Julia set is a point of accumulation for each of the Fatou sets. It is this property that causes the fractal structure of the Julia set (when the degree of the polynomial is larger than 2).\n\nTo plot interesting pictures, one may first choose a specified number <math>d</math> of complex points <math>(\\zeta_1,\\ldots,\\zeta_d)</math> and compute the coefficients <math>(p_1,\\ldots,p_d)</math> of the polynomial\n:<math>p(z)=z^d+p_1z^{d-1}+\\cdots+p_{d-1}z+p_d:=(z-\\zeta_1)\\cdot\\cdots\\cdot(z-\\zeta_d)</math>.\nThen for a rectangular lattice <math>z_{mn} = z_{00} + m \\, \\Delta x + in \\, \\Delta y</math>, <math>m = 0, \\ldots, M - 1</math>, <math>n = 0, \\ldots, N - 1</math> of points in <math>\\mathbb{C}</math>, one finds the index <math>k(m,n)</math> of the corresponding root <math>\\zeta_{k(m,n)}</math> and uses this to fill an <math>M</math>×<math>N</math> raster grid by assigning to each point <math>(m,n)</math> a color <math>f_{k(m,n)}</math>.  Additionally or alternatively the colors may be dependent on the distance <math>D(m,n)</math>, which is defined to be the first value <math>D</math> such that <math>|z_D - \\zeta_{k(m,n)}| < \\epsilon</math> for some previously fixed small <math>\\epsilon > 0</math>.\n\n== Generalization of Newton fractals ==\n\nA generalization of Newton's iteration is\n\n: <math>z_{n+1}=z_n- a \\frac{p(z_n)}{p'(z_n)} </math>\n\nwhere <math>a</math> is any complex number.<ref>{{cite web | url = http://www.chiark.greenend.org.uk/~sgtatham/newton/ | title = Fractals derived from Newton–Raphson | author = Simon Tatham }}</ref> The special choice <math>a=1</math> corresponds to the Newton fractal.  \nThe fixed points of this map are stable when <math>a</math> lies inside the disk of radius 1 centered at&nbsp;1. When <math>a</math> is outside this disk, the fixed points are locally unstable, however the map still exhibits a fractal structure in the sense of [[Julia set]]. If <math>p</math> is a polynomial of degree <math>d</math>, then the sequence <math>z_n</math> is [[Bounded set|bounded]] provided that <math>a</math> is inside a disk of radius <math>d</math> centered at <math>d</math>.\n\nMore generally, Newton's fractal is a special case of a [[Julia set]].\n\n<gallery>\nImage:FRACT008.png|Newton fractal for three degree-3 roots (<math>p(z)=z^3-1</math>), coloured by number of iterations required\nImage:Newtroot 1 0 0 m1.png|Newton fractal for three degree-3 roots (<math>p(z)=z^3-1</math>), coloured by root reached\nImage:Newton_z3-2z+2.png|Newton fractal for <math>p(z)=z^3-2z+2</math>. Points in the red basins do not reach a root.\nImage:Colored Newton Fractal 2.png|Newton fractal for a 7th order polynomial, colored by root reached and shaded by rate of convergence.\nImage:timelapse34.jpg|Newton fractal for <math>x^8+15x^4-16</math>\nImage:Newtroot 1 0 m3i m5m2i 3 1.png|Newton fractal for <math>p(z)=z^5-3iz^3-(5+2i)z^2+3z+1</math>, coloured by root reached, shaded by number of iterations required.\nImage:timelapse4.jpg|Newton fractal for <math>p(z)=\\sin(z)</math>, coloured by root reached, shaded by number of iterations required\nImage:Sin(x)_detail.png|Another Newton fractal for <math>\\sin(x)</math>\nImage:Mnfrac1.png| Generalized Newton fractal for <math>p(z)=z^3-1</math>, <math>a=-0.5.</math> The colour was chosen based on the argument after 40 iterations.\nImage:Mnfrac2.png| Generalized Newton fractal for <math>p(z)=z^2-1</math>, <math> a=1+i.</math>\nImage:Mnfrac3.png| Generalized Newton fractal for <math>p(z)=z^3-1</math>, <math>a=2.</math>\nImage:Mnfrac4.png| Generalized Newton fractal for <math>p(z)=z^{4+3i}-1</math>, <math>a=2.1.</math>\nImage:Newton z6 z3.jmb.jpg|<center><math>p(z) = z^6 + z^3 - 1</math></center>\nImage:Newton SINUS.jmb.jpg|<center><math>p(z) = \\sin(z)- 1</math></center>\nImage:Newton COSH.jmb.jpg|<center><math>p(z) = \\cosh(z)- 1</math></center>\n</gallery>\n\n=== Nova fractal ===\n\nThe Nova fractal invented in the mid 1990s by Paul Derbyshire,<ref>{{ cite web | url = http://formulas.ultrafractal.com/reference/Standard/Standard_NovaMandel.html | title = class Standard_NovaMandel (Ultra Fractal formula reference) | author = Damien M. Jones }}</ref><ref>{{ cite web | url = http://www.icd.com/tsd/fractals/ | title = dmj's nova fractals 1995-6 | author = Damien M. Jones }}</ref> is a generalization of the Newton fractal with the addition of a value <math>c</math> at each step:<ref>{{ cite web | url = http://hpdz.net/TechInfo/Convergent.htm#Nova | title = Relaxed Newton's Method and the Nova Fractal | author = Michael Condron }}</ref>\n\n: <math>z_{n+1}=z_n- a \\frac{p(z_n)}{p'(z_n)} + c = G(a, c, z)</math>\n\nThe \"Julia\" variant of the Nova fractal keeps <math>c</math> constant over the image and initializes <math>z_0</math> to the pixel coordinates.  The \"Mandelbrot\" variant of the Nova fractal initializes <math>c</math> to the pixel coordinates and sets <math>z_0</math> to a critical point, where <math>\\frac{\\partial}{\\partial z} G(a, c, z) = 0</math>.<ref>{{ cite web | url = https://www.ultrafractal.com/help/index.html?/help/formulas/standard/nova.html | title = Ultra Fractal Manual: Nova (Julia, Mandelbrot) | author = Frederik Slijkerman }}</ref>  Commonly-used polynomials like <math>p(z) = z^3 - 1</math> or <math>p(z) = (z-1)^3</math> lead to a critical point at <math>z = 1</math>.\n\n== Implementation ==\nIn order to implement the Newton Fractal, it is necessary to have a starting function as well as its derivative function:\n\n: <math>f(z) = z^3 - 1</math>\n\n: <math>f'(z) = 3z^2</math>\n\nThe roots of the function are\n\n: <math>z = 1, -0.5 \\pm \\sqrt{3}/2i</math>\n\nThe above-defined functions can be translated in pseudocode as follows:\n<syntaxhighlight lang=\"c\">\n//z^3-1 \nfloat2 Function (float2 z)\n{\n\treturn cpow(z, 3) - float2(1, 0); //cpow is an exponential function for complex numbers\n}\n\n//3*z^2\nfloat2 Derivative (float2 z)\n{\n\treturn 3 * cmul(z, z); //cmul is a function that handles multiplication of complex numbers\n}\n</syntaxhighlight>\nIt is now just a matter of implementing the Newton method using the given functions.\n<syntaxhighlight lang=\"c\">\nFor each pixel (x, y) on the target, do:\n{\n\tzx = scaled x coordinate of pixel (scaled to lie in the Mandelbrot X scale (-2.5, 1))\n    zy = scaled y coordinate of pixel (scaled to lie in the Mandelbrot Y scale (-1, 1))\n\n    float2 z = float2(zx, zy); //Z is originally set to the pixel coordinates\n\n\tfloat2 roots[3] = //Roots (solutions) of the polynomial\n\t{\n\t\tfloat2(1, 0), \n\t\tfloat2(-.5, sqrt(3)/2), \n\t\tfloat2(-.5, -sqrt(3)/2)\n\t};\n\t\n\tcolor colors[3] =  //Assign a color for each root\n\t{\n\t    red,\n\t    green,\n\t    blue\n    }\n\n\tfor (int iteration = 0;\n\t     iteration < maxIteration;\n\t     iteration++;)\n\t{\n\t\tz -= cdiv(Function(z), Derivative(z)); //cdiv is a function for dividing complex numbers\n\n        float tolerance = 0.000001;\n        \n\t\tfor (int i = 0; i < roots.Length; i++)\n\t\t{\n\t\t    float difference = z - roots[i];\n\t\t    \n\t\t\t//If the current iteration is close enough to a root, color the pixel.\n\t\t\tif (abs(difference.x) < tolerance && abs(difference.y) < tolerance)\n\t\t\t{\n\t\t\t\treturn colors[i]; //Return the color corresponding to the root\n\t\t\t}\n\t\t}\n\t\t\n    }\n    \n    return black; //If no solution is found\n}\n</syntaxhighlight>\n\n== See also ==\n* [[Julia set]]\n* [[Mandelbrot set]]\n\n==References==\n{{commons category|Newton fractals}}\n{{reflist}}\n* [http://www.math.sunysb.edu/~scott/  J. H. Hubbard, D. Schleicher, S. Sutherland]: ''How to Find All Roots of Complex Polynomials by Newton's Method'', Inventiones Mathematicae vol. 146 (2001) – with a discussion of the global structure of Newton fractals\n*[http://citeseer.ist.psu.edu/313081.html ''On the Number of Iterations for Newton's Method''] by Dierk Schleicher July 21, 2000\n*[http://www.math.sunysb.edu/cgi-bin/thesis.pl?thesis06-1 ''Newton's Method as a Dynamical System''] by Johannes Rueckert\n\n{{Isaac Newton}}\n\n[[Category:Numerical analysis]]\n[[Category:Fractals]]"
    },
    {
      "title": "Newton–Krylov method",
      "url": "https://en.wikipedia.org/wiki/Newton%E2%80%93Krylov_method",
      "text": "\n{{orphan|date=December 2015}}\n'''Newton–Krylov methods''' are [[numerical method]]s for solving non-linear problems using Krylov subspace linear solvers.<ref>{{cite journal|doi=10.1016/j.jcp.2003.08.010|title=Jacobian-free Newton–Krylov methods: a survey of approaches and applications|year=2004|last1=Knoll|first1=D.A.|last2=Keyes|first2=D.E.|journal=Journal of Computational Physics|volume=193|issue=2|page=357|citeseerx=10.1.1.636.3743}}</ref><ref>{{cite book|last=Kelley|first=C.T.|title=Solving nonlinear equations with Newton's method|edition=1|year=2003|publisher=[[Society for Industrial and Applied Mathematics|SIAM]]}}</ref>\n\n== References ==\n{{reflist}}\n\n{{DEFAULTSORT:Newton-Krylov method}}\n[[Category:Numerical analysis]]\n{{applied-math-stub}}"
    },
    {
      "title": "Nonstandard finite difference scheme",
      "url": "https://en.wikipedia.org/wiki/Nonstandard_finite_difference_scheme",
      "text": "'''Nonstandard finite difference schemes''' is a general set of methods in [[numerical analysis]] that gives numerical solutions to [[differential equations]] by constructing a discrete model. The general rules for such schemes are not precisely known.<ref name=Mickens2000>{{cite book\n | author = Mickens, R.E.\n | year = 2000\n | title = Applications of Nonstandard Finite Difference Schemes\n | publisher = World Scientific\n | isbn = \n}}</ref><ref name=Cole2002> JB Cole, High Accuracy Yee Algorithm Based on Nonstandard Finite Differences: New Developments and Verifications, IEEE Trans. on Antennas and Propagation, vol. 50, no. 9, pp. 1185-1191 (2002)</ref>\n\n==Overview==\nA finite difference (FD) model of a differential equation (DE) can be formed by simply replacing the derivatives with FD approximations. But this is a naive \"translation.\" If\nwe literally translate from English to Japanese by making a one-to-one correspondence between words, the original meaning is often lost. Similarly the naive FD model of a DE can be very different from the original DE, because the FD model is a difference equation with solutions that may be quite different from solutions of the DE. For a more technical definition see Mickens 2000.<ref name=Mickens2000/>\n\nA nonstandard (NS) finite difference model, is a free and more accurate \"translation\" of a differential equation. For example, a parameter (call it ''v'') in the DE may take another value ''u'' in the NS-FD model.\n\n==Example==\nAs an example let us model the wave equation,\n: <math> (\\partial_t^2-v^2 \\partial_x^2 ) \\Psi(x,t) = 0 .</math>\nThe naive finite difference model, which we now call the standard (S) FD model is found by approximating the derivatives with FD approximations. The central second order FD approximation of the first derivative is\n: <math>f'(x) \\approx \\frac{f(x+ \\Delta x/2)-f(x- \\Delta x/2)}{\\Delta x}. </math>\nApplying the above FD approximation to <math> f '(x)</math>, we can derive the FD approximation for <math>f '' (x)</math>,\n: <math> f''(x) \\approx \\frac{\\text{d}_x^2 f(x)}{\\Delta x^2}, </math>\nwhere we have introduced the shortcut <math> \\text{d}_x f(x) = f(x+\\Delta x/2) - f(x-\\Delta x/2)</math> for simplicity such that <math> \\text{d}_x{}^2 f(x) = f(x+\\Delta x) + f(x-\\Delta x) -2 f(x)</math> which can be check by applying <math> \\text{d}_x </math> on <math>f(x)</math> twice. \nApproximating both derivatives in the wave equation, leads to the S-FD model,\n: <math> \\left[\\text{d}_t^2 - (v \\Delta t/\\Delta x)^2 \\text{d}_x^2 \\right] \\Psi(x,t) = 0. </math>\nIf you insert the solution <math> \\phi(x,t) = e^{i (kx -\\omega t)} </math> of the wave equation  (with <math> \\omega/k = v </math>)into the S-FD model you find that\n: <math> \\left[\\text{d}_t^2 - (v \\Delta t/\\Delta x)^2\\text{d}_x^2 \\right] \\phi(x,t) = \\epsilon. \n </math>\nIn general <math>\\epsilon \\neq 0</math> because the solution of the FD approximation to the wave equation is not the same as the wave equation itself.\n\nTo construct a NS-FD model which has the same solution as the wave equation, put a free parameter, call it ''u'', in place of <math>v \\Delta t/ \\Delta x</math> and try to find a value of ''u'' which makes <math>\\epsilon = 0 </math>.\nIt turns out that this value of ''u'' is\n: <math>u = \\frac{\\sin(\\omega \\Delta t/2)}{\\sin(k \\Delta x/2) }. </math>\nThus an exact nonstandard finite difference model of the wave equation is\n: <math> \\left[ \\text{d}_t^2 - (u\\Delta t/\\Delta x)^2 \\text{d}_x^2 \\right] \\Psi(x,t) =0. </math>\nFurther details and extensions of to two and three dimensions as well as to Maxwell's equations can be found in Cole 2002.\n<ref name=Cole2002/>\n\n==References==\n{{Reflist}}\n\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Numeric precision in Microsoft Excel",
      "url": "https://en.wikipedia.org/wiki/Numeric_precision_in_Microsoft_Excel",
      "text": "As with other spreadsheets, [[Microsoft Excel]] works only to limited accuracy because it retains only a certain number of figures to describe numbers (it has limited [[Arithmetic precision|precision]]). With some exceptions regarding erroneous values, infinities, and denormalized numbers, Excel calculates in [[double-precision floating-point format]] from the [[IEEE 754-2008|IEEE 754 specification]]<ref name=microsoft_spec>\n\n{{cite web |url=http://support.microsoft.com/kb/78113/en-us |title=Floating-point arithmetic may give inaccurate results in Excel |publisher=Microsoft support |work=Revision 8.2 ; article ID: 78113 |date=June 30, 2010 |accessdate=2010-07-02}}\n\n</ref> (besides numbers, Excel uses a few other data types<ref name=Dalton>\n\n{{cite book |title=Financial Applications Using Excel Add-in Development in C/C++ \n|author=Steve Dalton |chapter=Table 2.3: Worksheet data types and limits |pages=13–14 |isbn=0-470-02797-5 |edition=2nd |publisher=Wiley |year=2007 |url=https://books.google.com/books?id=ABUSU9PWUuIC&pg=PA13}}\n\n</ref>). Although Excel can display 30 decimal places, its precision for a specified number is confined to 15 [[significant figures]], and calculations may have an accuracy that is even less due to three issues: [[round-off error|round off]],<ref name=roundoff>\n\nRound-off is the loss of accuracy when numbers that differ by small amounts are subtracted. Because each number has only fifteen significant digits, their difference is inaccurate when there aren't enough significant digits to express the difference.\n\n</ref> [[truncation]], and [[Binary numeral system|binary storage]].\n\n==Accuracy and binary storage==\n[[File:Excel fifteen figure.PNG|thumb|390px|Excel maintains 15 figures in its numbers, but they are not always accurate: the bottom line should be the same as the top line.]]\n[[File:Excel errors.PNG|thumb|390px|Of course, 1 + x − 1 = x. The discrepancy indicates the error. All errors but the last are beyond the 15-th decimal.]]\nIn the top figure the fraction 1/9000 in Excel is displayed. Although this number has a decimal representation that is an infinite string of ones, Excel displays only the leading 15 figures. In the second line, the number one is added to the fraction, and again Excel displays only 15 figures. In the third line, one is subtracted from the sum using Excel. Because the sum has only eleven 1's after the decimal, the true difference when ‘1’ is subtracted is three 0's followed by a string of eleven 1's.   However, the difference reported by Excel is three 0's followed by a 15-digit string of ''thirteen'' 1's and two extra erroneous digits. Thus, the numbers Excel calculates with are ''not'' the numbers that it displays. Moreover, the error in Excel's answer is not simply round-off error.\n\nThe inaccuracy in Excel calculations is more complicated than errors due to a precision of 15 significant figures. Excel's storage of numbers in binary format also affects its accuracy.<ref name=deLevie>\n\n{{cite book |title=Advanced Excel for scientific data analysis |publisher=Oxford University Press |author=Robert de Levie |year=2004 |isbn=0-19-515275-1 |page=44 |chapter=Algorithmic accuracy |url=https://www.amazon.com/Advanced-Excel-Scientific-Data-Analysis/dp/0195152751/ref=sr_1_1?ie=UTF8&s=books&qid=1270770876&sr=1-1#reader_0195152751}}\n\n</ref> To illustrate, the lower figure tabulates the simple addition {{nowrap|1 + ''x'' − 1}} for several values of ''x''. All the values of ''x'' begin at the 15-th decimal, so Excel must take them into account. Before calculating the sum 1 + ''x'', Excel first approximates ''x'' as a binary number. If this binary version of ''x'' is a simple power of 2, the 15-digit decimal approximation to ''x'' is stored in the sum, and the top two examples of the figure indicate recovery of ''x'' without error. In the third example, ''x'' is a more complicated binary number, ''x'' = 1.110111⋯111 × 2<sup>−49</sup> (15 bits altogether). Here ''x'' is approximated by the 4-bit binary 1.111 × 2<sup>−49</sup> (some insight into this approximation can be found using [[geometric progression]]: ''x'' = 1.11 × 2<sup>−49</sup> + 2<sup>−52</sup> × (1 − 2<sup>−11</sup>) ≈ 1.11 × 2<sup>−49</sup> + 2<sup>−52</sup> = 1.111 × 2<sup>−49</sup> ) and the decimal equivalent of this crude 4-bit approximation is used. In the fourth example,  ''x'' is a ''decimal'' number not equivalent to a simple binary (although it agrees with the binary of the third example to the precision displayed). The decimal input is approximated by a binary and then ''that'' decimal is used. These two middle examples in the figure show that some error is introduced.\n\nThe last two examples illustrate what happens if ''x'' is a rather small number. In the second from last example, ''x'' = 1.110111⋯111 × 2<sup>−50</sup>; 15 bits altogether. the binary is replaced very crudely by a single power of 2 (in this example, 2<sup>−49</sup>) and its decimal equivalent is used. In the bottom example, a decimal identical with the binary above to the precision shown, is nonetheless approximated differently from the binary, and is eliminated by truncation to 15 significant figures, making no contribution to {{nowrap|1 + ''x'' − 1}}, leading to ''x'' = 0.<ref name=decimal_input>\n\nTo input a number as binary, the number is submitted as a string of powers of 2: 2^(−50)*(2^0 + 2^−1 + ⋯). To input a number as decimal, the decimal number is typed in directly.\n\n</ref>\n\nFor ''x''′s that are not simple powers of 2, a noticeable error in {{nowrap|1 + ''x'' − 1}} can occur even when ''x'' is quite large. For example, if ''x'' = 1/1000, then {{nowrap|1 + ''x'' − 1}} = 9.99999999999'''''89''''' × 10<sup>−4</sup>, an error in the 13-th significant figure. In this case, if Excel simply added and subtracted the decimal numbers, avoiding the conversion to binary and back again to decimal, no round-off error would occur and accuracy actually would be better. Excel has the option to \"Set precision as displayed\".<ref name= discuss>This option is found on the \"Excel options/Advanced\" tab. See [http://support.microsoft.com/kb/214118 How to correct rounding errors: Method 2]\n\n</ref> With this option, depending upon circumstance, accuracy may turn out to be better or worse, but you will know exactly what Excel is doing. (It should be noted, however, that only the selected precision is retained, and one cannot recover extra digits by reversing this option.) Some similar examples can be found at this link.<ref name =arithmetic>\n\n[http://news.office-watch.com/t/n.aspx?a=612&z=9 Excel addition strangeness]\n\n</ref>\n\nIn short, a variety of accuracy behavior is introduced by the combination of representing a number with a limited number of binary digits, along with [[Truncation error|truncating]] numbers beyond the fifteenth significant figure.<ref name=deLevie3>\n\n{{cite book |title=cited work |author=Robert de Levie |year=2004 |isbn=0-19-515275-1 |pages=45–46 |url=https://www.amazon.com/Advanced-Excel-Scientific-Data-Analysis/dp/0195152751/ref=sr_1_1?ie=UTF8&s=books&qid=1270770876&sr=1-1#reader_0195152751}}\n\n</ref><!-- These figures are simply screen shots of the listed arithmetic using Excel 2007 --> Excel's treatment of numbers beyond 15 significant figures sometimes contributes better accuracy to the final few significant figures of a computation than working directly with only 15 significant figures, and sometimes not.\n\nFor the reasoning behind the conversion to binary representation and back to decimal, and for more detail about accuracy in Excel and VBA consult these links.<ref name=accuracy_links>\nAccuracy in Excel:\n*[http://support.microsoft.com/kb/78113/en-us Floating point arithmetic may give inaccurate results]: A detailed explanation with examples of the binary/15 sig fig storage consequences.\n*[http://blogs.msdn.com/excel/archive/2008/04/10/understanding-floating-point-precision-aka-why-does-excel-give-me-seemingly-wrong-answers.aspx Why does Excel seem to give wrong answers?] {{webarchive|url=https://web.archive.org/web/20100330212040/http://blogs.msdn.com/excel/archive/2008/04/10/understanding-floating-point-precision-aka-why-does-excel-give-me-seemingly-wrong-answers.aspx |date=2010-03-30 }}: Another detailed discussion with examples and some fixes.\n*[http://docs.sun.com/source/806-3568/ncg_goldberg.html What every computer scientist should know about floating point] Focuses upon examples of floating point representations of numbers.\n*[http://support.microsoft.com/default.aspx?scid=http://support.microsoft.com:80/support/kb/articles/Q279/7/55.ASP&NoWebContent=1 Visual basic and arithmetic precision]: Oriented toward VBA, which does things a bit differently.\n*{{cite book |title=A guide to Microsoft Excel 2007 for scientists and engineers |author=Bernard V. Liengme |chapter=Mathematical limitations of Excel |page=31 ''ff'' |url=https://books.google.com/books?id=0qDm7uuDmv0C&pg=PA31 |isbn=0-12-374623-X |year=2008 |publisher=Academic Press }}\n\n</ref>\n\n==Examples where precision is no indicator of accuracy==\n{{Expand section|date=April 2010}}\n\n===Statistical functions===\n[[File:Excel Std Dev Error.PNG|thumb|450px|Error in Excel 2007 calculation of standard deviation. All four columns have the same deviation of 0.5]]\n\nAccuracy in Excel-provided functions can be an issue. [[Micah Altman]] ''et al.'' provide this example:<ref name=Altman>\n\n{{cite book |title=Numerical issues in statistical computing for the social scientist |author1=Micah Altman |author2=Jeff Gill |author3=Michael McDonald |year=2004 |publisher=Wiley-IEEE |isbn=0-471-23633-0 |url=https://books.google.com/books?id=j_KevqVO3zAC&pg=PA12 |chapter=§2.1.1 Revealing example: Computing the coefficient standard deviation |page=12}}\n\n</ref> The population standard deviation given by:\n\n:<math> \\sqrt{ \\frac{ \\Sigma (x - \\bar{x})^2 }{n}  } = \\sqrt{  \\frac{ \\Sigma \\left[ x - \\left( \\Sigma x \\right) /n \\right] ^2}{n}  } \\ , </math>\n\nis mathematically equivalent to:\n\n:<math>\\sqrt{ \\frac{ n\\Sigma x^2 - \\left( \\Sigma x \\right) ^2 }{n^2}  } \\ . </math>\n\nHowever, the first form keeps better numerical accuracy for large values of ''x'', because squares of differences between  ''x'' and ''x''<sub>av</sub> leads to less round-off than the differences between the much larger numbers Σx<sup>2</sup>  and (Σx)<sup>2</sup>. The built-in Excel function STDEVP(), however, uses the less accurate formulation because it is faster computationally.<ref name=Levie>\n\n{{cite book |title=Advanced Excel for scientific data analysis |author=Robert de Levie |publisher=Oxford University Press |year=2004 |isbn=0-19-515275-1 |url=https://books.google.com/books?id=IAnO-2qVazsC&printsec=frontcove|pages=45–46}}\n\n</ref>\n\nBoth the \"compatibility\" function STDEVP and the \"consistency\" function STDEV.P in Excel 2010 return the 0.5 population standard deviation for the given set of values. However, numerical inaccuracy still can be shown using this example by extending the existing figure to include 10<sup>15</sup>, whereupon the erroneous standard deviation found by Excel 2010 will be zero.\n\n===Subtraction of Subtraction Results===\nDoing simple subtractions may lead to errors as two cells may display the same numeric value while storing two separate values.\nAn example of this occurs in a sheet where the following cells are set to the following numeric values:\n:<math>A1:= 28.552</math>\n:<math>A2:= 27.399</math>\n:<math>A3:= 26.246</math>\nand the following cells contain the following formulas\n:<math>B1: = A1 - A2</math>\n:<math>B2: = A2 - A3</math>\nBoth cells <math>B1</math> and <math>B2</math> display <math>1.1530</math>. \nHowever, if cell <math>C1</math> contains the formula <math>B1 - B2</math>\nthen <math>C1</math> does not display <math>0</math> as would be expected,\nbut displays <math>-3.55271E-15</math> instead.\n\n===Round-off error===\nUser computations must be carefully organized to ensure round-off error does not become an issue. An example occurs in solving a [[quadratic equation]]:\n:<math>ax^2 + b x +c = 0 \\ .</math>\nThe solutions (the roots) of this equation are exactly determined by the [[quadratic formula]]:\n\n:<math>x= \\frac{-b \\pm \\sqrt{b^2-4ac} }{2a}. </math>\n\nWhen one of these roots is very large compared to the other, that is, when the square root is close to the value ''b'', the evaluation of the root corresponding to subtraction of the two terms becomes very inaccurate due to round-off.\n\nIt is possible to determine the round-off error by using the [[Taylor series]] formula for the square root:\n<ref name=\"Zwillinger_2014\">{{cite book |author-first1=Izrail Solomonovich |author-last1=Gradshteyn |author-link1=Izrail Solomonovich Gradshteyn |author-first2=Iosif Moiseevich |author-last2=Ryzhik |author-link2=Iosif Moiseevich Ryzhik |author-first3=Yuri Veniaminovich |author-last3=Geronimus |author-link3=Yuri Veniaminovich Geronimus |author-first4=Michail Yulyevich |author-last4=Tseytlin |author-link4=Michail Yulyevich Tseytlin |author-first5=Alan |author-last5=Jeffrey |editor-first1=Daniel |editor-last1=Zwillinger |editor-first2=Victor Hugo |editor-last2=Moll |translator=Scripta Technica, Inc. |title=Table of Integrals, Series, and Products |publisher=[[Academic Press, Inc.]] |date=2015 |orig-year=October 2014 |edition=8 |language=English |isbn=0-12-384933-0 <!--|ISBN=978-0-12-384933-5 --> |lccn=2014010276 <!-- |url=https://books.google.com/books?id=NjnLAwAAQBAJ |access-date=2016-02-21-->|title-link=Gradshteyn and Ryzhik |chapter=1.112. Power series |page=25}}</ref>\n:<math>\\sqrt{b^2-4ac} = b \\ \\sqrt{1-\\frac{4ac}{b^2}} \\approx b \\left( 1 -\\frac{2ac}{b^2} + \\frac{2 a^2 c^2 }{b^4} + \\cdots \\right ). </math>\n\nConsequently, \n:<math> b - \\sqrt{b^2-4ac} \\approx b \\left (  \\frac{2ac}{b^2} - \\frac{2 a^2 c^2 }{b^4} + \\cdots \\right ), </math>\nindicating that, as ''b'' becomes larger, the first surviving term, say ε:\n\n:<math> \\varepsilon = \\frac{2ac}{b}, </math>\n\nbecomes smaller and smaller. The numbers for ''b'' and the square root become nearly the same, and the difference becomes small:\n\n:<math>b - \\sqrt{b^2-4ac} \\approx b - b + \\varepsilon. </math>\n\nUnder these circumstances, all the significant figures go into expressing ''b''. For example, if the precision is 15 figures, and these two numbers, ''b'' and the square root, are the same to 15 figures, the difference will be zero instead of the difference ε.\n\nA better accuracy can be obtained from a different approach, outlined below.<ref name=Step_response>\n\nThis approximate method is used often in the design of feedback amplifiers, where the two roots represent the response times of the system. See the article on [[step response]].\n\n</ref> If we denote the two roots by ''r''&thinsp;<sub>1</sub> and ''r''&thinsp;<sub>2</sub>, the quadratic equation can be written:\n\n:<math>\\left(x - r_1\\right) \\left( x - r_2 \\right) = x^2 - \\left( r_1  + r_2 \\right) x + r_1 \\  r_2 = 0. </math>\n\nWhen the root ''r''&thinsp;<sub>1</sub> >> ''r''&thinsp;<sub>2</sub>, the sum (''r''&thinsp;<sub>1</sub> + ''r''&thinsp;<sub>2</sub>&thinsp;) ≈ ''r''&thinsp;<sub>1</sub> and comparison of the two forms shows approximately:\n:<math> r_1  \\approx -\\frac{b}{a}, </math>\nwhile\n:<math> r_1 \\  r_2 = \\frac{c}{a}. </math>\nThus, we find the approximate form:\n:<math>r_2 = \\frac {c}{a \\ r_1} \\approx -\\frac {c}{b}. </math>\nThese results are not subject to round-off error, but they are not accurate unless ''b''<sup>2</sup> is large compared to&nbsp;''ac''.\n\n[[File:Excel quadratic error.PNG|thumb|350px| Excel graph of the difference between two evaluations of the smallest root of a quadratic: direct evaluation using the quadratic formula (accurate at smaller ''b'') and an approximation for widely spaced roots (accurate for larger ''b''). The difference reaches a minimum at the large dots, and round-off causes squiggles in the curves beyond this minimum.]]\n\nThe bottom line is that in doing this calculation using Excel, as the roots become farther apart in value, the method of calculation will have to switch from direct evaluation of the quadratic formula to some other method so as to limit round-off error. The point to switch methods varies according to the size of coefficients ''a'' and&nbsp;''b''.\n\nIn the figure, Excel is used to find the smallest root of the quadratic equation ''x''<sup>2</sup>&nbsp;+&nbsp;''bx''&nbsp;+&nbsp;''c''&nbsp;=&nbsp;0  for ''c''&nbsp;=&nbsp;4 and&nbsp;''c''&nbsp;=&nbsp;4&nbsp;×&nbsp;10<sup>5</sup>. The difference between direct evaluation using the quadratic formula and the approximation described above for widely spaced roots is plotted ''vs.'' ''b''. Initially the difference between the methods declines because the widely spaced root method becomes more accurate at larger ''b''-values. However, beyond some ''b''-value the difference increases because the quadratic formula (good for smaller ''b''-values) becomes worse due to round-off, while the widely spaced root method (good for large ''b''-values) continues to improve. The point to switch methods is indicated by large dots, and is larger for larger ''c''-values. At large ''b''-values, the upward sloping curve is Excel's round-off error in the quadratic formula, whose erratic behavior causes the curves to squiggle.\n\nA different field where accuracy is an issue is the area of [[Numerical integration|numerical computing of integrals]] and the [[Numerical ordinary differential equations|solution of differential equations]]. Examples are [[Simpson's rule]], the [[Runge–Kutta method]], and the Numerov algorithm for the [[Schrödinger equation]].<ref name=Blom>\n\n[https://www.researchgate.net/profile/Anders_Blom5/publication/242226580_Computer_algorithms_for_solving_the_Schrodinger_and_Poisson_equations/links/55a1d42a08aec9ca1e63e3a5/Computer-algorithms-for-solving-the-Schrodinger-and-Poisson-equations.pdf Anders Blom] ''Computer algorithms for solving the Schrödinger and Poisson equations'', Department of Physics, Lund University, 2002.\n\n</ref> Using Visual Basic for Applications, any of these methods can be implemented in Excel. Numerical methods use a grid where functions are evaluated. The functions may be interpolated between grid points or extrapolated to locate adjacent grid points. These formulas involve comparisons of adjacent values. If the grid is spaced very finely, round-off error will occur, and the less the precision used, the worse the round-off error. If spaced widely, accuracy will suffer. If the numerical procedure is thought of as a [[Negative feedback amplifier|feedback system]], this calculation noise may be viewed as a signal that is applied to the system, which will lead to instability unless the system is carefully designed.<ref name=Hamming>\n\n{{cite book |author=[[Richard Hamming|R. W. Hamming]]  |title=Numerical Methods for Scientists and Engineers |year= 1986 |isbn=0-486-65241-6 |url=https://books.google.com/books?id=Y3YSCmWBVwoC&printsec=frontcover |publisher=Courier Dover Publications |edition=2nd}} This book discusses round-off, truncation and stability extensively. For example, see Chapter 21: [https://books.google.com/books?id=Y3YSCmWBVwoC&pg=PA357 Indefinite integrals – feedback], page 357.\n\n</ref>\n\n===Accuracy within VBA===\nAlthough Excel nominally works with [[byte|8-byte]] numbers by default, [[Visual Basic for Applications|VBA]] has a variety of data types. The ''Double'' data type is 8 bytes, the ''Integer'' data type is 2 bytes, and the general purpose 16 byte ''Variant'' data type can be converted to a 12 byte ''Decimal'' data type using the VBA conversion function ''CDec''.<ref name=John_Walkenbach>\n\n{{cite book |title=Excel 2010 Power Programming with VBA |chapter=Defining data types |pages=198 ''ff'' and Table 8-1|isbn=0-470-47535-8 |author=John Walkenbach |year=2010 |publisher=Wiley |url=https://books.google.com/books?id=dtSdrjjVXrwC&pg=PA198}}\n\n</ref> Choice of variable types in a VBA calculation involves consideration of storage requirements, accuracy and speed.\n\n==References==\n<references/>\n\n[[Category:Microsoft software]]\n[[Category:Numerical analysis]]\n[[Category:Spreadsheet software]]"
    },
    {
      "title": "Numerical continuation",
      "url": "https://en.wikipedia.org/wiki/Numerical_continuation",
      "text": "{{expert needed|mathematics|date=July 2009}}\n'''Numerical continuation''' is a method of computing approximate solutions of a system of parameterized nonlinear equations,\n:<math>F(\\mathbf u,\\lambda) = 0.</math><ref>[http://inis.jinr.ru/sl/M_Mathematics/MN_Numerical%20methods/MNd_Numerical%20calculus/Allogower%20Introduction.pdf Introduction to Numerical Continuation Methods by Eugene L. Allgower and Kurt Georg Colorado State University 1990]</ref>\n\nThe ''parameter'' <math>\\lambda</math> is usually a real [[scalar (mathematics)|scalar]], and the ''solution'' <math>\\mathbf u</math> an [[n-tuple|''n''-vector]]. For a fixed ''parameter value'' <math>\\lambda</math>, <math>F(\\ast,\\lambda)</math> maps [[Euclidean n-space]] into itself.\n\nOften the original mapping <math>F</math> is from a [[Banach space]] into itself, and the [[Euclidean n-space]] is a finite-dimensional approximation to the Banach space.\n\nA [[steady state]], or [[fixed point (mathematics)|fixed point]], of a [[parameterized family]] of [[flow (mathematics)|flows]] or [[map (mathematics)|maps]] are of this form, and by [[discretization|discretizing]] trajectories of a flow or iterating a map, [[periodic orbit]]s  and [[heteroclinic orbit]]s can also be posed as a solution of <math>F=0</math>.\n\n== Other forms ==\nIn some nonlinear systems, parameters are explicit. In others they are implicit, and the system of nonlinear equations is written\n\n:<math>F(\\mathbf u) = 0</math>\nwhere <math>\\mathbf u</math> is an ''n''-vector, and its image <math>F(\\mathbf u)</math> is an ''n-1'' vector.\n\nThis formulation, without an explicit parameter space is not usually suitable for the formulations in the following sections, because they refer to parameterized autonomous nonlinear [[dynamical systems]] of the form:\n\n:<math>\\mathbf u' = F(\\mathbf u,\\lambda).</math>\n\nHowever, in an algebraic system there is no distinction between unknowns <math>\\mathbf u</math> and the parameters.\n\n== Periodic motions ==\nA [[periodic motion]] is a closed curve in phase space. That is, for some ''period'' <math>T</math>,\n\n:<math>\\mathbf u' = F(\\mathbf u,\\lambda),\\, \\mathbf u(0) = \\mathbf u(T).</math>\n\nThe textbook example of a periodic motion is the undamped [[pendulum]].\n\nIf the [[phase space]] is periodic in one or more coordinates, say <math>\\mathbf u(t) = \\mathbf u(t + \\Omega)</math>, with <math>\\Omega</math> a vector {{Clarify|reason=if t is a scalar, Omega should be a scalar, or otherwise a definition for the addition of scalars and vectors would be welcome|date=July 2017}} , then there is a second kind of periodic motions defined by\n\n:<math>\\mathbf u' = \\mathbf F(\\mathbf u,\\lambda),\\, \\mathbf u(0) = \\mathbf u(T + N.\\Omega)</math>\n\nfor every integer <math>N</math>.\n: [[Image:PeriodicMotion.gif]][[Image:PeriodicOrbitTime.gif]]\n\nThe first step in writing an implicit system for a periodic motion is to move the period <math>T</math> from the boundary conditions to the [[Ordinary differential equation|ODE]]:\n\n:<math>\\mathbf u' = T\\mathbf F(\\mathbf u,\\lambda),\\, \\mathbf u(0)=\\mathbf u(1 + N.\\Omega).</math>\n\nThe second step is to add an additional equation, a ''phase constraint'', that can be thought of as determining the period. This is necessary because any solution of the above boundary value problem can be shifted in time by an arbitrary amount (time does not appear in the defining equations—the dynamical system is called autonomous).\n\nThere are several choices for the phase constraint. If <math>\\mathbf u_0(t)</math> is a known periodic orbit at a parameter value <math>\\lambda_0</math> near <math>\\lambda</math>, then, Poincaré used\n\n:<math><\\mathbf u(0) - \\mathbf u_0(0),\\mathbf F(\\mathbf u_0(0),\\lambda_0)> = 0.</math>\n\nwhich states that <math>\\mathbf u</math> lies in a plane which is orthogonal to the tangent vector of the closed curve. This plane is called a ''[[Poincaré section]]''.\n:[[Image:PoincareSection.gif]]\nFor a general problem a better phase constraint is an integral constraint introduced by Eusebius Doedel, which chooses the phase so that the distance between the known and unknown orbits is minimized:\n\n:<math>\\int_0^1 <\\mathbf u(t) - \\mathbf u_0(t),\\mathbf F(\\mathbf u_0(t),\\lambda_0)>\\, dt = 0.</math>\n\n== Homoclinic and heteroclinic motions ==\n\n: [[Image:HomoclinicOrbit.gif]][[Image:HomoclinicOrbitTime.gif]]\n\n== Definitions ==\n=== Solution component ===\n\n[[Image:Numerical Continuation components.png|thumb|Two solution components, one red and the other blue. Note that these two components may be connected outside the region of interest.]]\nA solution component <math>\\Gamma(\\mathbf u_0,\\lambda_0)</math> of the nonlinear system <math>F</math> is a set of points <math>(\\mathbf u,\\lambda)</math> which satisfy <math>F(\\mathbf u,\\lambda)=0</math> and are ''connected'' to the initial solution <math>(\\mathbf u_0,\\lambda_0)</math> by a path of solutions <math>(\\mathbf u(s),\\lambda(s))</math> for which <math>(\\mathbf u(0),\\lambda(0))=(\\mathbf u_0,\\lambda_0),\\, (\\mathbf u(1),\\lambda(1)) = (\\mathbf u,\\lambda)</math>\nand <math>F(\\mathbf u(s),\\lambda(s))=0</math>.\n\n=== Numerical continuation ===\n\nA numerical continuation is an algorithm which takes as input a system of parametrized nonlinear equations and an initial solution <math>(\\mathbf u_0,\\lambda_0)</math>, <math>F(\\mathbf u_0,\\lambda_0)=0</math>, and produces a set of points on the solution component <math>\\Gamma(\\mathbf u_0,\\lambda_0)</math>.\n\n=== Regular point ===\n\nA regular point of <math>F</math> is a point <math>(\\mathbf u,\\lambda)</math> at which the [[Jacobian matrix and determinant|Jacobian]] of <math>F</math> is full rank <math>(n)</math>.\n\nNear a regular point the solution component is an isolated curve passing through the regular point (the [[implicit function theorem]]). In the figure above the point <math>(\\mathbf u_0,\\lambda_0)</math> is a regular point.\n\n=== Singular point ===\n\nA singular point of <math>F</math> is a point <math>(\\mathbf u,\\lambda)</math> at which the [[Jacobian matrix and determinant|Jacobian]] of F is not full rank.\n\nNear a singular point the solution component may not be an isolated curve passing through the regular point. The local structure is determined by higher derivatives of <math>F</math>. In the figure above the point where the two blue curves cross is a singular point.\n\nIn general solution components <math>\\Gamma</math> are [[branched curves]]. The branch points are singular points. Finding the solution curves leaving a\nsingular point is called branch switching, and uses techniques from [[bifurcation theory]] ([[singularity theory]], [[catastrophe theory]]).\n\nFor finite-dimensional systems (as defined above) the Lyapunov-Schmidt decomposition may be used to produce two systems to which the Implicit Function Theorem applies. The Lyapunov-Schmidt decomposition uses the restriction of the system to the complement of the null space of the Jacobian and the range of the Jacobian.\n\nIf the columns of the matrix <math>\\Phi</math> are an orthonormal basis for the null space of\n:<math>J=\\left[\n\\begin{array}{cc}\nF_x & F_{\\lambda}\\\\\n\\end{array}\n\\right]</math>\nand the columns of the matrix <math>\\Psi</math> are an orthonormal basis for the left null space of <math>J</math>, then\nthe system <math>F(x,\\lambda)=0</math>\ncan be rewritten as\n:<math>\n\\left[\n\\begin{array}{l}\n(I-\\Psi\\Psi^T)F(x+\\Phi\\xi + \\eta)\\\\\n\\Psi^T F(x+\\Phi\\xi + \\eta)\\\\\n\\end{array}\n\\right]=0,\n</math>\nwhere <math>\\eta</math> is in the complement of the null space of <math>J</math> <math>(\\Phi^T\\,\\eta=0)</math>.\n\nIn the first equation, which is parametrized by the null space of the Jacobian (<math>\\xi</math>), the Jacobian with respect to <math>\\eta</math> is non-singular. So the implicit function theorem states that there is a mapping <math>\\eta(\\xi)</math> such that <math>\\eta(0)=0</math> and <math>(I-\\Psi\\Psi^T)F(x+\\Phi\\xi+\\eta(\\xi))=0)</math>. The second equation (with <math>\\eta(\\xi)</math> substituted) is called the bifurcation equation (though it may be a system of equations).\n\nThe bifurcation equation has a Taylor expansion which lacks the constant and linear terms. By scaling the equations and the null space of the Jacobian of the original system a system can be found with non-singular Jacobian. The constant term in the Taylor series of the scaled bifurcation equation is called the algebraic bifurcation equation, and the implicit function theorem applied the bifurcation equations states that for each isolated solution of the algebraic bifurcation equation there is a branch of solutions of the original problem which  passes through the singular point.\n\nAnother type of singular point is a [[turning point bifurcation]], or [[saddle-node bifurcation]], where the direction of the parameter <math>\\lambda</math>\nreverses as the curve is followed. The red curve in the figure above illustrates a turning point.\n\n== Particular algorithms ==\n=== Natural parameter continuation ===\nMost methods of solution of nonlinear systems of equations are iterative methods. For a particular parameter value <math>\\lambda_0</math> a mapping is repeatedly applied to an initial guess <math>\\mathbf u_0</math>. If the method converges, and is consistent, then in the\nlimit the iteration approaches a solution of <math>F(\\mathbf u,\\lambda_0)=0</math>.\n\n''Natural parameter continuation'' is a very simple adaptation of the iterative solver to a parametrized problem. The solution at\none value of <math>\\lambda</math> is used as the initial guess for the solution at <math>\\lambda+\\Delta \\lambda</math>. With <math>\\Delta \\lambda</math> sufficiently small the iteration applied to the initial\nguess should converge.\n\n: [[Image:NaturalParameter.gif]]\n\nOne advantage of natural parameter continuation is that it uses the solution method for the problem as a black box. All that is required is that an initial solution be given (some solvers used to always start at a fixed initial guess). There has been a lot of work in the area of large scale continuation on applying more sophisticated algorithms to black box solvers (see e.g. [http://www.cs.sandia.gov/loca/ LOCA]).\n\nHowever, natural parameter continuation fails at turning points, where the branch of solutions turns round. So for problems with turning points, a more sophisticated method such as pseudo-arclength continuation must be used (see below).\n\n=== Simplicial or piecewise linear continuation ===\n{{main|Piecewise linear continuation}}\n\nSimplicial Continuation, or Piecewise Linear Continuation (Allgower and Georg) is based on three basic results.\n\nThe first is\n:{| class=\"wikitable\"\n|-\n|If F(x) maps IR^n into IR^(n-1), there is a unique linear interpolant on an (n-1)-dimensional [[simplex]] which agrees with the function values at the vertices of the simplex.\n|}\n\nThe second result is:\n:{| class=\"wikitable\"\n|-\n|An (n-1)-dimensional simplex can be tested to determine if the unique linear interpolant takes on the value 0 inside the simplex.\n|}\n\nPlease see the article on  [[piecewise linear continuation]] for details.\n\nWith these two operations this continuation algorithm is easy to state (although of course an efficient implementation requires a more sophisticated approach. See [B1]). An initial simplex is assumed to be given, from a reference simplicial decomposition of IR^n. The initial simplex must have at least one face which contains a zero of the unique linear interpolant on that face. The other faces of the simplex are then tested, and typically there will be one additional face with an interior zero. The initial simplex is then replaced by the simplex which lies across either face containing zero, and the process is repeated.\n\n: [[Image:Simplicial.gif]]\n\nReferences: Allgower and Georg [B1] provides a crisp, clear description of the algotihm.\n\n=== Pseudo-arclength continuation ===\nThis method is based on the observation that the \"ideal\" parameterization of a curve is arclength. Pseudo-arclength is an approximation of the arclength in the tangent space of the curve. The resulting modified natural continuation method makes a step in pseudo-arclength (rather than <math>\\lambda</math>). The iterative solver is required to find a point at the given pseudo-arclength, which requires appending an additional\nconstraint (the pseudo-arclength constraint) to the n by n+1 Jacobian. It produces a square Jacobian, and if the stepsize is sufficiently small the modified Jacobian is full rank.\n\nPseudo-arclength continuation was independently developed by Edward Riks and Gerald Wempner for finite element applications in the late 1960s, and published in journals in the early 1970s by H.B. Keller. A detailed  account of these early developments is\nprovided in the textbook by M. A. Crisfield: Nonlinear Finite Element Analysis of Solids and Structures, Vol 1: Basic Concepts, Wiley, 1991.  Crisfield was one of the most active developers of this class of methods, which are by now standard procedures of commercial nonlinear finite element programs.\n: [[Image:PseudoArclength.gif]]\nThe algorithm is a predictor-corrector method. The prediction step finds the point (in IR^(n+1) ) which is a step <math>\\Delta s</math> along the tangent vector at the current pointer. The corrector is usually Newton's method, or some variant, to solve the nonlinear system\n:<math>\n\\begin{array}{l}\nF(u,\\lambda)=0\\\\\n\\dot u^*_0(u-u_0)+\\dot \\lambda_0 (\\lambda-\\lambda_0) = \\Delta s\\\\\n\\end{array}\n</math>\nwhere <math>(\\dot u_0,\\dot\\lambda_0)</math> is the tangent vector at <math>(u_0,\\lambda_0)</math>.\nThe Jacobian of this system is the bordered matrix\n:<math>\\left[\n\\begin{array}{cc}\nF_u & F_{\\lambda}\\\\\n\\dot u^* & \\dot \\lambda\\\\\n\\end{array}\n\\right]</math>\nAt regular points, where the unmodified Jacobian is full rank, the tangent vector spans the null space of the top row of this new Jacobian. Appending the tangent vector as the last row can be seen as determining the coefficient of the null vector in the general solution of the Newton system (particular solution plus an arbitrary multiple of the null vector).\n\n=== Gauss–Newton continuation ===\nThis method is a variant of pseudo-arclength continuation. Instead of using the tangent at the initial point in the arclength constraint, the tangent at the current solution is used. This is equivalent to using the pseudo-inverse of the Jacobian in Newton's method, and allows longer steps to be made. [B17]\n\n== Continuation in more than one parameter ==\nThe parameter <math>\\lambda</math> in the algorithms described above is a real scalar. Most physical and design problems generally have many more than one parameter. Higher-dimensional continuation refers to the case when <math>\\lambda</math> is a k-vector.\n\nThe same terminology applies. A '''regular solution''' is a solution at which the Jacobian is full rank <math>(n)</math>. A singular solution is a solution at which the Jacobian is less than full rank.\n\nA regular solution lies on a k-dimensional surface, which can be parameterized by a point in the tangent space (the null space of the Jacobian). This is again a straightforward application of the Implicit Function Theorem.\n\n== Applications of numerical continuation techniques ==\nNumerical continuation techniques have found a great degree of acceptance in the study of chaotic dynamical systems and various other systems which belong to the realm of [[catastrophe theory]]. The reason for such usage stems from the fact that various non-linear dynamical systems behave in a deterministic and predictable manner within a range of parameters which are included in the equations of the system. However, for a certain parameter value the system starts behaving chaotically and hence it became necessary to follow the parameter in order to be able to decipher the occurrences of when the system starts being non-predictable, and what exactly (theoretically) makes the system become unstable.\n\nAnalysis of parameter continuation can lead to more insights about stable/critical point bifurcations. Study of saddle-node, transcritical, pitch-fork, period doubling, Hopf, secondary Hopf (Neimark) bifurcations of stable solutions allows for a theoretical discussion of the circumstances and occurrences which arise at the critical points. Parameter continuation also gives a more dependable system to analyze a dynamical system as it is more stable than more interactive, time-stepped numerical solutions. Especially in cases where the dynamical system is prone to blow-up at certain parameter values (or combination of values for multiple parameters).<ref>{{cite book |last1=Engelnkemper |first1=S. |last2=Gurevich |first2=S. V. |last3=Uecker |first3=H. |last4=Wetzel |first4=D. |last5=Thiele |first5=U. |title=Computational modelling of bifurcations and instabilities in fluid dynamics |date=7 July 2018 |publisher=Springer |isbn=9783319914930 |pages=459–501 |url=https://link.springer.com/chapter/10.1007/978-3-319-91494-7_13}}</ref>\n\nIt is extremely insightful as to the presence of stable solutions (attracting or repelling) in the study of [[Nonlinear]] [[Partial Differential Equations]] where times stepping in the form of the Crank Nicolson algorithm is extremely time consuming as well as unstable in cases of nonlinear growth of the dependent variables in the system. The study of turbulence is another field where the Numerical Continuation techniques have been used to study the advent of [[turbulence]] in a system starting at low Reynolds numbers. Also, research using these techniques has provided the possibility of finding stable manifolds and bifurcations to invariant-tori in the case of the [[restricted three-body problem]] in Newtonian gravity and have also given interesting and deep insights into the behaviour of systems such as the [[Lorenz equations]].\n\n== Software ==\n(Under Construction) See also The SIAM Activity Group on Dynamical Systems' list [http://www.dynamicalsystems.org/sw/sw/ http://www.dynamicalsystems.org/sw/sw/]\n\n* AUTO: Computation of the solutions of Two Point Boundary Value Problems (TPBVPs) with integral constraints. [https://sourceforge.net/projects/auto-07p/ https://sourceforge.net/projects/auto-07p/ Available on SourceForge.]\n* HOMCONT: Computation of homoclinic and heteroclinic orbits. Included in AUTO\n* MATCONT:  Matlab toolbox for numerical continuation and bifurcation [http://www.matcont.ugent.be/][https://sourceforge.net/projects/matcont/ Available on SourceForge.]\n* DDEBIFTOOL: Computation of solutions of Delay Differential Equations. A MATLAB package. [http://www.cs.kuleuven.be/cwis/research/twr/research/software/delay/ddebiftool.shtml Available from K. U. Leuven]\n* PyCont: A Python toolbox for numerical continuation and bifurcation. Native Python algorithms for fixed point continuation, sophisticated interface to AUTO for other types of problem. Included as part of [http://www2.gsu.edu/~matrhc/PyDSTool.htm PyDSTool]\n* CANDYS/QA: [http://www.agnld.uni-potsdam.de/~wolfgang/ca-ov.html Available from the Universität Potsdam] [A16]\n* MANPAK: [http://netlib.org/contin/manpak/ Available from Netlib]  [A15]\n* PDDE-CONT: [http://seis.bris.ac.uk/~rs1909/pdde/ http://seis.bris.ac.uk/~rs1909/pdde/]\n* multifario: [http://multifario.sourceforge.net/ http://multifario.sourceforge.net/]\n* LOCA: [https://trilinos.org/packages/nox-and-loca/ https://trilinos.org/packages/nox-and-loca/]\n* DSTool\n* GAIO\n* OSCILL8: Oscill8 is a dynamical systems tool that allows a user to explore high-dimensional parameter space of nonlinear ODEs using bifurcation analytic techniques. [http://sourceforge.net/projects/oscill8 Available from SourceForge].\n\n== Examples ==\nThis problem, of finding the points which ''F'' maps into the origin appears in [[computer graphics]] as the problems of drawing [[contour maps]] (n=2), or [[isosurface]](n=3). The contour with value ''h'' is the set of all solution components of ''F-h=0''\n\n== References ==\n<references/>\n\n=== Books ===\n[B1] \"''Introduction to Numerical Continuation Methods''\", Eugene L. Allgower and Kurt Georg, SIAM Classics in Applied Mathematics 45. 2003.\n\n[B2] \"''Numerical Methods for Bifurcations of Dynamical Equilibria''\", Willy J. F. Govaerts, SIAM 2000.\n\n[B3] \"''Lyapunov-Schmidt Methods in Nonlinear Analysis and Applications''\", Nikolay Sidorov, Boris Loginov, Aleksandr Sinitsyn, and Michail Falaleev, Kluwer Academic Publishers, 2002.\n\n[B4] \"''Methods of Bifurcation Theory''\", Shui-Nee Chow and Jack K. Hale, Springer-Verlag 1982.\n\n[B5] \"''Elements of Applied Bifurcation Theory''\", Yuri A. Kunetsov, Springer-Verlag Applied Mathematical Sciences 112, 1995.\n\n[B6] \"Nonlinear Oscillations, Dynamical Systems, and Bifurcations of Vector Fields\", [[John Guckenheimer]] and [[Philip Holmes]], Springer-Verlag Applied Mathematical Sciences 42, 1983.\n\n[B7] \"''Elementary Stability and Bifurcation Theory''\", Gerard Iooss and Daniel D. Joseph, Springer-Verlag [[Undergraduate Texts in Mathematics]], 1980.\n\n[B8] \"''Singularity Theory and an Introduction to Catastrophe Theory''\", Yung-Chen Lu, Springer-Verlag, 1976.\n\n[B9] \"''Global Bifurcations and Chaos, Analytic Methods''\", S. Wiggins, Springer-Verlag Applied Mathematical Sciences 73, 1988.\n\n[B10] \"''Singularities and Groups in Bifurcation Theory, volume I''\", [[Marty Golubitsky|Martin Golubitsky]] and David G. Schaeffer, Springer-Verlag Applied Mathematical Sciences 51, 1985.\n\n[B11] \"''Singularities and Groups in Bifurcation Theory, volume II''\", [[Marty Golubitsky|Martin Golubitsky]], Ian Stewart and David G. Schaeffer, Springer-Verlag Applied Mathematical Sciences 69, 1988.\n\n[B12] \"''Solving Polynomial Systems Using Continuation for Engineering and Scientific Problems''\", Alexander Morgan, Prentice-Hall, Englewood Cliffs, N.J. 1987.\n\n[B13] \"''Pathways to Solutions, Fixed Points and Equilibria''\", C. B. Garcia and W. I. Zangwill, Prentice-Hall, 1981.\n\n[B14] \"''The Implicit Function Theorem: History, Theory and Applications''\", Steven G. Krantz and [[Harold R. Parks]], Birkhauser, 2002.\n\n[B15] \"''Nonlinear Functional Analysis''\", J. T. Schwartz, Gordon and Breach Science Publishers, Notes on Mathematics and its Applications, 1969.\n\n[B16] \"''Topics in Nonlinear Functional Analysis''\", Louis Nirenberg (notes by Ralph A. Artino), AMS Courant Lecture Notes in Mathematics 6, 1974.\n\n[B17] \"''Newton Methods for Nonlinear Problems -- Affine Invariance and Adaptive Algorithms''\", P. Deuflhard,\nSeries Computational Mathematics 35, Springer, 2006.\n\n=== Journal articles ===\n[A1] \"''An Algorithm for Piecewise Linear Approximation of Implicitly Defined Two-Dimensional Surfaces''\", Eugene L. Allgower and Stefan Gnutzmann, SIAM Journal on Numerical Analysis, Volume 24, Number 2, 452—469, 1987.\n\n[A2] \"''Simplicial and Continuation Methods for Approximations, Fixed Points and Solutions to Systems of Equations''\", E. L. Allgower and K. Georg, SIAM Review, Volume 22, 28—85, 1980.\n\n[A3] \"''An Algorithm for Piecewise-Linear Approximation of an Implicitly Defined Manifold''\", Eugene L. Allgower and Phillip H. Schmidt, SIAM Journal on Numerical Analysis, Volume 22, Number 2, 322—346, April 1985.\n\n[A4] \"''Contour Tracing by Piecewise Linear Approximations''\", [[David P. Dobkin]], Silvio V. F. Levy, [[William Thurston|William P. Thurston]] and Allan R. Wilks, ACM Transactions on Graphics, 9(4) 389-423, 1990.\n\n[A5] \"''Numerical Solution of Bifurcation and Nonlinear Eigenvalue Problems''\", H. B. Keller, in \"Applications of Bifurcation Theory\", P. Rabinowitz ed., Academic Press, 1977.\n\n[A6] \"''A Locally Parameterized Continuation Process''\", W.C. Rheinboldt and J.V. Burkardt, ACM Transactions on Mathematical Software, Volume 9, 236—246, 1983.\n\n[A7] \"''Nonlinear Numerics''\" E. Doedel, [[International Journal of Bifurcation and Chaos]], 7(9):2127-2143, 1997.\n\n[A8] \"''Nonlinear Computation''\", R. Seydel, [[International Journal of Bifurcation and Chaos]], 7(9):2105-2126, 1997.\n\n[A9] \"''On a Moving Frame Algorithm and the Triangulation of Equilibrium Manifolds''\", W.C. Rheinboldt, In T. Kuper, R. Seydel, and H. Troger eds. \"ISNM79: Bifurcation: Analysis, Algorithms, Applications\", pages 256-267. Birkhauser, 1987.\n\n[A10] \"''On the Computation of Multi-Dimensional Solution Manifolds of Parameterized Equations''\", W.C. Rheinboldt, Numerishe Mathematik, 53, 1988, pages 165-181.\n\n[A11] \"''On the Simplicial Approximation of Implicitly Defined Two-Dimensional Manifolds''\", M. L. Brodzik and W.C. Rheinboldt, Computers and Mathematics with Applications, 28(9): 9-21, 1994.\n\n[A12] \"''The Computation of Simplicial Approximations of Implicitly Defined p-Manifolds''\", M. L. Brodzik, Computers and Mathematics with Applications, 36(6):93-113, 1998.\n\n[A13] \"''New Algorithm for Two-Dimensional Numerical Continuation''\", R. Melville and D. S. Mackey, Computers and Mathematics with Applications, 30(1):31-46, 1995.\n\n[A14] \"''Multiple Parameter Continuation: Computing Implicitly Defined k-manifolds''\", M. E. Henderson, IJBC 12[3]:451-76, 2003.\n\n[A15] \"''MANPACK: a set of algorithms for computations on implicitly defined manifolds''\", W. C. Rheinboldt, Comput. Math. Applic. 27 pages 15–9, 1996.\n\n[A16] \"''CANDYS/QA - A Software System For Qualitative Analysis Of Nonlinear Dynamical Systems''\", Feudel, U. and W. Jansen, Int. J. Bifurcation and Chaos, vol. 2 no. 4, pp.&nbsp;773–794, World Scientific, 1992.\n\n[[Category:Numerical analysis]]\n[[Category:Dynamical systems]]"
    },
    {
      "title": "Numerical differentiation",
      "url": "https://en.wikipedia.org/wiki/Numerical_differentiation",
      "text": "In [[numerical analysis]], '''numerical differentiation''' describes [[algorithm]]s for estimating the [[derivative]] of a [[mathematical function]] or function [[subroutine]] using values of the function and perhaps other knowledge about the function.\n\n[[Image:Derivative.svg|230px|right]]\n\n==Finite difference formulas==\nThe simplest method is to use finite difference approximations.\n\nA simple two-point estimation is to compute the slope of a nearby [[secant line]] through the points (''x'', ''f''(''x'')) and (''x'' + ''h'', ''f''(''x'' + ''h'')).<ref>Richard L. Burden, J. Douglas Faires (2000), ''Numerical Analysis'', (7th Ed),  Brooks/Cole. {{isbn|0-534-38216-9}}.</ref> Choosing a small number ''h'',  ''h'' represents a small change in ''x'', and it can be either positive or negative.  The slope of this line is\n: <math>\\frac{f(x + h) - f(x)}{h}.</math>\nThis expression is [[Isaac Newton|Newton]]'s [[difference quotient]] (also known as a first-order [[divided difference]]).\n\nThe slope of this secant line differs from the slope of the tangent line by an amount that is approximately proportional to ''h''. As ''h'' approaches zero, the slope of the secant line approaches the slope of the tangent line. Therefore, the true '''derivative of''' '''''f''''' '''at''' '''''x''''' is the limit of the value of the difference quotient as the secant lines get closer and closer to being a tangent line:\n: <math>f'(x) = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}.</math>\n\nSince immediately [[substitution (logic)|substituting]] 0 for ''h'' results in [[division by zero]], calculating the derivative directly can be unintuitive.\n\nEquivalently, the slope could be estimated by employing positions (''x'' − ''h'') and ''x''.\n\nAnother two-point formula is to compute the slope of a nearby secant line through the points (''x'' - ''h'', ''f''(''x'' − ''h'')) and (''x'' + ''h'', ''f''(''x'' + ''h'')). The slope of this line is\n: <math>\\frac{f(x + h) - f(x - h)}{2h}.</math>\n\nThis formula is known as the [[symmetric difference quotient]]. In this case the first-order errors cancel, so the slope of these secant lines differ from the slope of the tangent line by an amount that is approximately proportional to <math>h^2</math>. Hence for small values of ''h'' this is a more accurate approximation to the tangent line than the one-sided estimation. Note however that although the slope is being computed at ''x'', the value of the function at ''x'' is not involved.\n\nThe estimation error is given by\n\n: <math>R = \\frac{-f^{(3)}(c)}{6} h^2</math>,\n\nwhere <math>c</math> is some point between <math>x - h</math> and <math>x + h</math>.\nThis error does not include the [[rounding error]] due to numbers being represented and calculations being performed in limited precision.\n\nThe symmetric difference quotient is employed as the method of approximating the derivative in a number of calculators, including [[TI-82]], [[TI-83]], [[TI-84]], [[TI-85]], all of which use this method with ''h'' = 0.001.<ref name=\"Merseth2003\">{{cite book |author=Katherine Klippert Merseth |title=Windows on Teaching Math: Cases of Middle and Secondary Classrooms |year=2003 |publisher=Teachers College Press |isbn=978-0-8077-4279-2 |page=34}}</ref><ref name=\"RubySellers2014\">{{cite book |author1=Tamara Lefcourt Ruby |author2=James Sellers |author3=Lisa Korf |author4=Jeremy Van Horn |author5=Mike Munn |title=Kaplan AP Calculus AB & BC 2015 |year=2014 |publisher=Kaplan Publishing |isbn=978-1-61865-686-5 |page=299}}</ref>\n\n===Practical considerations using floating-point arithmetic===\n[[Image:AbsoluteErrorNumericalDifferentiationExample.png|thumb|300px|Example showing the difficulty of choosing <math>h</math> due to both rounding error and formula error]]\n\nAn important consideration in practice when the function is calculated using [[floating-point arithmetic]] is how small a value of ''h'' to choose. If chosen too small, the subtraction will yield a large [[rounding error]]. In fact, all the finite-difference formulae are [[ill-conditioned]]<ref name=Fornberg1>Numerical Differentiation of Analytic Functions, B Fornberg – ACM Transactions on Mathematical Software (TOMS), 1981.</ref> and due to cancellation will produce a value of zero if ''h'' is small enough.<ref name=SquireTrapp1>Using Complex Variables to Estimate Derivatives of Real Functions, W. Squire, G. Trapp – SIAM REVIEW, 1998.</ref> If too large, the calculation of the slope of the secant line will be more accurately calculated, but the estimate of the slope of the tangent by using the secant could be worse.\n\nFor the numerical derivative formula evaluated at ''x'' and ''x'' + ''h'', a choice for ''h'' that is small without producing a large rounding error is <math>\\sqrt{\\varepsilon} x</math> (though not when ''x'' = 0), where the [[machine epsilon]] ''ε'' is typically of the order of 2.2{{e|−16}}.\n<ref>Following ''[[Numerical Recipes]] in C'', [http://www.nrbook.com/a/bookcpdf/c5-7.pdf Chapter 5.7].</ref> A formula for ''h'' that balances the rounding error against the secant error for optimum accuracy is<ref>[http://www.uio.no/studier/emner/matnat/math/MAT-INF1100/h10/kompendiet/kap11.pdf p. 263].</ref>\n\n: <math>h = 2\\sqrt{\\varepsilon\\left|\\frac{f(x)}{f''(x)}\\right|}</math>\n\n(though not when <math>f''(x) = 0</math>), and to employ it will require knowledge of the function.\n\nThis epsilon is for double-precision (64-bit) variables: such calculations in single precision are rarely useful. The resulting value is unlikely to be a \"round\" number in binary, so it is important to realise that although ''x'' is a [[Floating point#Representable numbers, conversion and rounding|machine-representable]] number, ''x'' + ''h'' almost certainly will not be. This means that ''x'' + ''h'' will be changed (by rounding or truncation) to a nearby machine-representable number, with the consequence that (''x'' + ''h'') − ''x'' will ''not'' equal ''h''; the two function evaluations will not be exactly ''h'' apart. In this regard, since most decimal fractions are recurring sequences in binary (just as 1/3 is in decimal) a seemingly round step such as ''h'' = 0.1 will not be a round number in binary; it is 0.000110011001100...<sub>2</sub> A possible approach is as follows:\n  h := sqrt(eps) * x;\n  xph := x + h;\n  dx := xph - x;\n  slope := (F(xph) - F(x)) / dx;\nHowever, with computers, [[compiler optimization]] facilities may fail to attend to the details of actual computer arithmetic and instead apply the axioms of mathematics to deduce that ''dx'' and ''h'' are the same. With [[C (programming language)|C]] and similar languages, a directive that ''xph'' is a [[volatile variable]] will prevent this.\n\n===Higher-order methods===\n{{further|Finite difference coefficient}}\nHigher-order methods for approximating the derivative, as well as methods for higher derivatives, exist.\n\nGiven below is the five-point method for the first derivative ([[five-point stencil]] in one dimension):<ref>Abramowitz & Stegun, Table 25.2.</ref>\n: <math>f'(x) = \\frac{-f(x + 2h) + 8 f(x + h) - 8 f(x - h) + f(x - 2h)}{12h} + \\frac{h^4}{30} f^{(5)}(c),</math>\nwhere <math>c \\in [x - 2h, x + 2h]</math>.\n\nFor other stencil configurations and derivative orders, the [http://web.media.mit.edu/~crtaylor/calculator.html Finite Difference Coefficients Calculator] is a tool that can be used to generate derivative approximation methods for any stencil with any derivative order (provided a solution exists).\n\n=== Higher derivatives ===\nUsing Newton's difference quotient,\n\n: <math>f'(x) = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}</math>\n\nthe following can be shown<ref>{{cite book |last1=Shilov |first1=George |title=Elementary Real and Complex Analysis}}</ref> (for ''n>0''):\n\n:<math>f^{(n)}(x) = \\lim_{h\\to 0} \\frac{1}{h^n} \\sum_{k=0}^n (-1)^{k+n} \\binom{n}{k} f(x + kh).</math>\n<!-- [the following clearly has a wrong sign for n = 1]\nHowever, a factor seems to be missing:\n\n: <math>f^{(n)}(x)= (-1)^n \\lim_{h\\to 0} \\frac{1}{h^n} \\sum_{k=0}^n (-1)^{k+1} \\binom{n}{k} f(x + kh).</math>\n-->\n<br />\n\n== Practical Implementation ==\nA fast open source Python implementation of numerical derivatives using finite differences is the [https://github.com/maroba/findiff findiff] project. It features calculation of any derivative and any accuracy order in an arbitrary number of dimensions.\n\n==Differential quadrature==\n[[Differential quadrature]] is the approximation of derivatives by using weighted sums of function values.<ref>Differential Quadrature and Its Application in Engineering: Engineering Applications, Chang Shu, Springer, 2000, {{isbn|978-1-85233-209-9}}.</ref><ref>Advanced Differential Quadrature Methods, Yingyan Zhang, CRC Press, 2009, {{isbn|978-1-4200-8248-7}}.</ref> The name is in analogy with ''quadrature'', meaning [[numerical integration]], where weighted sums are used in methods such as [[Simpson's method]] or the [[Trapezoidal rule]]. There are various methods for determining the weight coefficients. Differential quadrature is used to solve [[partial differential equations]].\n\n==Complex-variable methods==\n\nThe classical finite-difference approximations for numerical differentiation are ill-conditioned. However, if <math>f</math> is a [[holomorphic function]], real-valued on the real line, which can be evaluated at points in the complex plane near <math>x</math>, then there are [[Numerical stability|stable]] methods. For example,<ref name=SquireTrapp1/> the first derivative can be calculated by the complex-step derivative formula:<ref>{{cite journal | last1 = Martins | first1 = J. R. R. A. | first2 = P. | last2 = Sturdza | first3 = J. J. | last3 = Alonso | year = 2003 | citeseerx=10.1.1.141.8002 | title = The Complex-Step Derivative Approximation | journal = ACM Transactions on Mathematical Software | volume = 29 | issue = 3 | pages = 245–262 | doi=10.1145/838250.838251}}</ref>\n\n: <math>f'(x) \\approx \\Im(f(x + ih)) / h.</math>\n\nThe above formula is only valid for calculating a first-order derivative. A generalization of the above for calculating derivatives of any order employ [[multicomplex numbers]], resulting in multicomplex derivatives.<ref>http://russell.ae.utexas.edu/FinalPublications/ConferencePapers/2010Feb_SanDiego_AAS-10-218_mulicomplex.pdf</ref>\n\nIn general, derivatives of any order can be calculated using [[Cauchy's integral formula]]:\n\n: <math>f^{(n)}(a) = \\frac{n!}{2\\pi i} \\oint_\\gamma \\frac{f(z)}{(z - a)^{n+1}} \\,\\mathrm{d}z,</math>\n\nwhere the integration is done [[Numerical integration|numerically]].\n\nUsing complex variables for numerical differentiation was started by Lyness and Moler in 1967.<ref name=LynessMoler1>{{cite journal | first1 = J. N. | last1 = Lyness | first2 = C. B. | last2 = Moler | title = Numerical differentiation of analytic functions | journal = SIAM J. Numer. Anal. | volume = 4 | year = 1967 | pages = 202–210 | doi=10.1137/0704019}}</ref> A method based on numerical inversion of a complex [[Laplace transform]] was developed by Abate and Dubner.<ref>{{cite journal | title = A New Method for Generating Power Series Expansions of Functions | first1 = J | last1 = Abate | first2 = H | last2 = Dubner | journal = SIAM J. Numer. Anal. | volume =5 | issue = 1 | pages = 102–112 |date=March 1968 | doi=10.1137/0705008}}</ref> An algorithm that can be used without requiring knowledge about the method or the character of the function was developed by Fornberg.<ref name=Fornberg1/>\n\n==See also==\n*[[Automatic differentiation]]\n*[[Finite difference]]\n*[[Five-point stencil]]\n*[[Numerical integration]]\n*[[Numerical ordinary differential equations]]\n*[[Numerical smoothing and differentiation]]\n*[[List of numerical analysis software]]\n\n==References==\n{{reflist}}\n\n== External links ==\n{{wikibooks|Numerical Methods}}\n* http://mathworld.wolfram.com/NumericalDifferentiation.html\n*[http://numericalmethods.eng.usf.edu/topics/continuous_02dif.html Numerical Differentiation Resources: Textbook notes, PPT, Worksheets, Audiovisual YouTube Lectures] at [http://numericalmethods.eng.usf.edu/ Numerical Methods for STEM Undergraduate]\n*ftp://math.nist.gov/pub/repository/diff/src/DIFF Fortran code for the numerical differentiation of a function using Neville's process to extrapolate from a sequence of simple polynomial approximations.\n* [http://www.nag.co.uk/numeric/fl/nagdoc_fl24/html/D04/d04conts.html NAG Library numerical differentiation routines]\n* http://graphulator.com [http://graphulator.com Online numerical graphing calculator with calculus function.]\n* [http://www.boost.org/doc/libs/release/libs/math/doc/html/math_toolkit/diff.html Boost. Math numerical differentiation, including finite differencing and the complex step derivative]\n*[https://blogs.mathworks.com/cleve/2013/10/14/complex-step-differentiation/ Complex Step Differentiation]\n*[https://sinews.siam.org/Details-Page/differentiation-without-a-difference Differentiation With(out) a Difference] by [[Nicholas Higham]], [[SIAM]] News.\n\n{{DEFAULTSORT:Numerical Differentiation}}\n[[Category:Numerical analysis]]\n[[Category:Differential calculus]]"
    },
    {
      "title": "Numerical error",
      "url": "https://en.wikipedia.org/wiki/Numerical_error",
      "text": "In [[software engineering]] and [[mathematics]], '''numerical error''' is the error in [[Numerical computation|the numerical computations]]. \n[[File:Time series of the Tent map for the parameter m=2.0 which shows numerical error.svg|thumb|right|Time series of the Tent map for the parameter m=2.0 which shows numerical error: \"the plot of time series (plot of x variable with respect to number of iterations) stops fluctuating and no values are observed after n=50\". Parameter m= 2.0, initial point is random.]]\n==Types==\nIt can be the combined effect of two kinds of error in a calculation.\n* the first is caused by the finite [[Precision (computer science)|precision]] of computations involving [[floating-point]] or integer values\n* the second usually called truncation error is the difference between the exact mathematical solution and the approximate solution obtained when simplifications are made to the mathematical equations to make them more amenable to calculation.  The term truncation comes from the fact that either these simplifications usually involve the truncation of an [[infinite series]] expansion so as to make the computation possible and practical, or because the least significant bits of an arithmetic operation are thrown away.\n\n==Measure==\nFloating-point numerical error is often measured in ULP ([[unit in the last place]]).\n\n==See also==\n* [[Loss of significance]]\n* [[Numerical analysis]]\n* [[Round-off error]]\n* [[Kahan summation algorithm]]\n* [[Impact of error]]\n* [[Numerical sign problem]]\n\n==References==\n\n* ''Accuracy and Stability of Numerical Algorithms'', Nicholas J. Higham, {{isbn|0-89871-355-2}}\n* \"Computational Error And Complexity In Science And Engineering\", V. Lakshmikantham, S.K. Sen,  {{isbn|0444518606}}\n\n[[Category:Computer arithmetic]]\n[[Category:Numerical analysis]]\n\n\n{{software-eng-stub}}\n{{applied-math-stub}}"
    },
    {
      "title": "Numerical method",
      "url": "https://en.wikipedia.org/wiki/Numerical_method",
      "text": "{{more footnotes|date=September 2016}}\nIn [[numerical analysis]], a '''numerical method''' is a mathematical tool designed to solve numerical problems. The implementation of a numerical method with an appropriate convergence check in a programming language is called a numerical algorithm.\n\n==Mathematical definition==\nLet <math>F(x,y)=0</math> be a [[well-posed problem (numerical analysis)|well-posed problem]], i.e. <math>F:X \\times Y \\rightarrow \\mathbb{R}</math> is a [[Real number|real]] or [[Complex Numbers|complex]] functional relationship, defined on the cross-product of an input data set <math>X</math> and an output data set <math>Y</math>, such that exists a [[Lipschitz continuity|locally lipschitz]] function <math>g:X \\rightarrow Y</math> called [[Resolvent (direct problem)|resolvent]], which has the property that for every root <math>(x,y)</math> of <math>F</math>, <math>y=g(x)</math>. We define '''numerical method''' for the approximation of <math>F(x,y)=0</math>, the [[sequence]] of problems\n\n: <math>\\left \\{ M_n \\right \\}_{n \\in \\mathbb{N}} = \\left \\{ F_n(x_n,y_n)=0 \\right \\}_{n \\in \\mathbb{N}},</math>\n\nwith <math>F_n:X_n \\times Y_n \\rightarrow \\mathbb{R}</math>, <math>x_n \\in X_n</math> and <math>y_n \\in Y_n</math> for every <math>n \\in \\mathbb{N}</math>. The problems of which the method consists need not be well-posed. If they are, the method is said to be ''stable'' or ''well-posed''.<ref name=\"quartsaccsal\">{{cite book\n| last = Quarteroni, Sacco, Saleri\n| title = Numerical Mathematics\n| publisher=Springer\n| location = Milano\n| year = 2000\n| page = 33\n| url = http://www.techmat.vgtu.lt/~inga/Files/Quarteroni-SkaitMetod.pdf}}\n</ref>\n\n==Consistency==\nNecessary conditions for a numerical method to effectively approximate <math>F(x,y)=0</math> are that <math>x_n \\rightarrow x</math> and that <math>F_n</math> behaves like <math>F</math> when <math>n \\rightarrow \\infty</math>. So, a numerical method is called ''consistent'' if and only if the sequence of functions <math>\\left \\{ F_n \\right \\}_{n \\in \\mathbb{N}}</math> pointwise converges to <math>F</math> on the set <math>S</math> of its solutions:\n\n: <math>\n\\lim F_n(x,y) = F(x,y) = 0, \\quad \\quad \\forall (x,y) \\in S.\n</math>\n\nWhen <math>F_n=F, \\forall n \\in \\mathbb{N}</math> on <math>S</math> the method is said to be ''strictly consistent''.<ref name=\"quartsaccsal\" />\n\n==Convergence==\nDenote by <math>\\ell_n</math> a sequence of ''admissible perturbations'' of <math>x \\in X</math> for some numerical method <math>M</math> (i.e. <math>x+\\ell_n \\in X_n \\forall n \\in \\mathbb{N}</math>) and with <math>y_n(x+\\ell_n) \\in Y_n</math> the value such that <math>F_n(x+\\ell_n,y_n(x+\\ell_n)) = 0</math>. A condition which the method has to satisfy to be a meaningful tool for solving the problem <math>F(x,y)=0</math> is ''convergence'':\n\n: <math>\n\\begin{align}\n&\\forall \\varepsilon > 0, \\exist n_0(\\varepsilon) > 0, \\exist \\delta_{\\varepsilon, n_0} \\text{ such that} \\\\\n&\\forall n > n_0, \\forall \\ell_n : \\| \\ell_n \\| < \\delta_{\\varepsilon,n_0} \\Rightarrow \\| y_n(x+\\ell_n) - y \\| \\leq \\varepsilon.\n\\end{align}\n</math>\n\nOne can easily prove that the point-wise convergence of <math> \\{y_n\\} _{n \\in \\mathbb{N}}</math> to <math>y</math> implies the convergence of the associated method.<ref name=\"quartsaccsal\" />\n\n==References==\n{{Reflist}}\n\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Numerical methods in fluid mechanics",
      "url": "https://en.wikipedia.org/wiki/Numerical_methods_in_fluid_mechanics",
      "text": "[[Fluid motion]] is governed by the [[Navier–Stokes equations]], a set of coupled and nonlinear\npartial differential equations derived from the basic laws of conservation of [[mass]], [[momentum]]\nand [[energy]]. The unknowns are usually the [[flow velocity]], the [[pressure]] and [[density]]  and [[temperature]]. The [[analytical solution]] of this equation is impossible hence scientists resort to laboratory experiments in such situations. The answers delivered are, however, usually qualitatively different since dynamical and geometric similitude are difficult to enforce simultaneously between the lab experiment and the [[prototype]]. Furthermore, the design and construction of these experiments can be difficult (and costly), particularly for stratified rotating flows. [[Computational fluid dynamics]] (CFD) is an additional tool in the arsenal of scientists. In its early days CFD was often controversial, as it involved additional approximation to the governing equations and raised additional (legitimate) issues. Nowadays CFD is an established discipline alongside theoretical and experimental methods. This position is in large part due to the exponential growth of computer power which has allowed us to tackle ever larger and more complex problems.\n\n==Discretization==\nThe central process in CFD is the process of [[discretization]], i.e. the process of taking differential equations with an infinite number of [[degrees of freedom]], and reducing it to a system of finite degrees of freedom. Hence, instead of determining the solution everywhere and for all times, we will be satisfied with its calculation at a finite number of locations and at specified time intervals. The [[partial differential equations]] are then reduced to a system of algebraic equations that can be solved on a computer. Errors creep in during the discretization process. The nature and characteristics of the errors must be controlled in order to ensure that:\n* we are solving the correct equations (consistency property)\n* that the error can be decreased as we increase the number of degrees of freedom (stability and convergence).\nOnce these two criteria are established, the power of computing machines can be leveraged to solve the problem in a numerically reliable fashion. Various discretization schemes have been developed to cope with a variety of issues. The most notable for our purposes are: [[finite difference methods]], finite volume methods, [[finite element methods]], and [[spectral methods]].\n\n==Finite difference method==\nFinite difference replace the infinitesimal limiting process of derivative calculation:\n:<math> \\lim_{\\Delta x \\to 0}f'(x) = \\frac {f(x+\\Delta x)-f(x)}{\\Delta x} </math>\n\nwith a finite limiting process,i.e.\n\n:<math> f'(x) =\\frac {f(x+\\Delta x) - f(x)}{\\Delta x} + O(\\Delta x) </math>\n\nThe term <math>O(\\Delta x)</math> gives an indication of the magnitude of the error as a function of the mesh spacing. In this instance, the error is halfed if the grid spacing, _x is halved, and we say that this is a first order method. Most FDM used in practice are at least second order accurate except in very special circumstances. Finite Difference method is still the most popular numerical method for solution of PDEs because of their simplicity, efficiency and low computational cost. Their major drawback is in their geometric inflexibility which complicates their applications to general complex domains. These can be alleviated by the use of either mapping techniques and/or masking to fit the computational mesh to the computational domain.\n\n==Finite element method==\nThe finite element method was designed to deal with problem with complicated computational regions. The PDE is first recast into a variational form which essentially forces the mean error to be small everywhere. The discretization step proceeds by dividing the computational domain into elements of triangular or rectangular shape. The solution within each element is interpolated with a polynomial of usually low order. Again, the unknowns are the solution at the collocation points. The CFD community adopted the FEM in the 1980s when reliable methods for dealing with advection dominated problems were devised.\n\n==Spectral method==\nBoth finite element and finite difference methods are low order methods, usually of 2nd&nbsp;&minus;&nbsp;4th order, and have local approximation property. By local we mean that a particular collocation point is affected by a limited number of points around it. In contrast, spectral method have global approximation property. The interpolation functions, either polynomials or trigonomic functions are global in nature. Their main benefits is in the rate of convergence which depends on the smoothness of the solution (i.e. how many continuous derivatives does it admit). For infinitely smooth solution, the error decreases exponentially, i.e. faster than algebraic. Spectral methods are mostly used in the computations of homogeneous turbulence, and require relatively simple geometries. Atmospheric model have also adopted spectral methods because of their convergence properties and the regular spherical shape of their computational domain.\n\n==Finite volume method==\nFinite volume methods are primarily used in [[aerodynamics]] applications where strong shocks and discontinuities in the solution occur. Finite volume method solves an integral form of the governing equations so that local continuity property do not have to hold.\n\n==Computational cost==\nThe [[CPU]] time to solve the system of equations differs substantially from method to method. Finite differences are usually the cheapest on a per grid point basis followed by the finite element method and spectral method. However, a per grid point basis comparison is a little like comparing apple and oranges. Spectral methods deliver more accuracy on a per grid point basis than either [[Finite element method|FEM]] or [[Finite difference method|FDM]]. The comparison is more meaningful if the question is recast as ”what is the computational cost to achieve a given error tolerance?”. The problem becomes one of defining the error measure which is a complicated task in general situations.\n\n==Forward Euler approximation==\n:<math> \\frac {u^{n+1} -u^n}{\\Delta t } \\approx \\kappa u^n </math>\n\nEquation is an explicit approximation to the original differential equation since no information about the unknown function at the future time (''n''&nbsp;+&nbsp;1)<sub>''t''</sub> has been used on the right hand side of the equation. In order to derive the error committed in the approximation we rely again on Taylor series.\n\n==Backward difference==\nThis is an example of an implicit method since the unknown ''u''(''n''&nbsp;+&nbsp;1) has been used in evaluating the slope of the solution on the right hand side; this is not a problem to solve for ''u''(''n''&nbsp;+&nbsp;1) in this scalar and linear case. For more complicated situations like a nonlinear right hand side or a system of equations, a nonlinear system of equations may have to be inverted.\n\n==References==\n\n# Zalesak, S. T., 2005. The design of flux-corrected transport algorithms for structured grids. In: Kuzmin, D., Löhner, R., Turek, S. (Eds.), Flux-Corrected Transport. Springer\n# Zalesak, S. T., 1979. Fully multidimensional flux-corrected transport algorithms for fluids. Journal of Computational Physics.\n# Leonard, B. P., MacVean, M. K., Lock, A. P., 1995. The flux integral method for [[Convection–diffusion equation|multi-dimensional convection]] and diffusion. Applied Mathematical Modelling.\n# Shchepetkin, A. F., McWilliams, J. C., 1998. Quasi-monotone advection schemes based on explicit locally adaptive [[dissipation]]. Montlhy Weather Review\n# Jiang, C.-S., Shu, C.-W., 1996. Efficient implementation of weighed eno schemes. Journal of Computational Physics\n# Finlayson, B. A., 1972. The Method of Weighed Residuals and Variational Principles. Academic Press.\n# Durran, D. R., 1999. Numerical Methods for [[Wave function|Wave Equations]] in Geophysical Fluid Dynamics. Springer, New York.\n# Dukowicz, J. K., 1995. Mesh effects for rossby waves. Journal of Computational Physics\n# Canuto, C., Hussaini, M. Y., Quarteroni, A., Zang, T. A., 1988. Spectral Methods in Fluid Dynamics. Springer Series in Computational Physics. Springer-Verlag, New York.\n# Butcher, J. C., 1987. The Numerical Analysis of [[Ordinary Differential Equations]]. John Wiley and Sons Inc., NY.\n# Boris, J. P., Book, D. L., 1973. Flux corrected transport, i: Shasta, a fluid transport algorithm that works. Journal of Computational Physics\n\n[[Category:Computational fluid dynamics]]\n[[Category:Numerical analysis]]\n[[Category:Functional analysis]]"
    },
    {
      "title": "Numerical model of the Solar System",
      "url": "https://en.wikipedia.org/wiki/Numerical_model_of_the_Solar_System",
      "text": "{{No footnotes|article|date=April 2009}}A '''numerical model of the [[Solar System]]''' is a set of mathematical equations, which, when solved, give the approximate positions of the planets as a function of time. Attempts to create such a model established the more general field of [[celestial mechanics]]. The results of this simulation can be compared with past measurements to check for accuracy and then be used to predict future positions. Its main use therefore is in preparation of almanacs.\n\n==Older efforts==\nThe simulations can be done in either [[Cartesian coordinate system|Cartesian]] or in [[Spherical coordinate system|spherical]] coordinates. The former are easier, but extremely calculation intensive, and only practical on an electronic computer. As such only the latter was used in former times. Strictly speaking not much less calculation intensive, but it was possible to start with some simple approximations and then to add [[Perturbation (astronomy)|perturbations]], as much as needed to reach the wanted accuracy.\n\nIn essence this mathematical simulation of the Solar System is a form of the ''[[N-body problem]]''. The symbol '''''N''''' represents the number of bodies, which can grow quite large if one includes the Sun, 8 planets, dozens of moons, and countless planetoids, comets and so forth. However the influence of the Sun on any other body is so large, and the influence of all the other bodies on each other so small, that the problem can be reduced to the analytically solvable 2-body problem. The result for each planet is an orbit, a simple description of its position as function of time. Once this is solved the influences moons and planets have on each other are added as small corrections. These are small compared to a full planetary orbit. Some corrections might be still several degrees large, while measurements can be made to an accuracy of better than 1″.\n\nAlthough this method is no longer used for simulations, it is still useful to find an approximate ephemeris as one can take the relatively simple main solution, perhaps add a few of the largest perturbations, and arrive without too much effort at the wanted planetary position. The disadvantage is that perturbation theory is very advanced mathematics.\n\n==Modern method==\nThe modern method consists of numerical integration in 3-dimensional space. One starts with a high accuracy value for the position (''x'', ''y'', ''z'') and the velocity (''v<sub>x</sub>'', ''v<sub>y</sub>'', ''v<sub>z</sub>'') for each of the bodies involved. When also the mass of each body is known, the acceleration (''a<sub>x</sub>'', ''a<sub>y</sub>'', ''a<sub>z</sub>'') can be calculated from [[Newton's Law of Gravitation]]. Each body attracts each other body, the total acceleration being the sum of all these attractions. Next one chooses a small time-step Δ''t'' and applies [[Newton's Second Law of Motion]]. The acceleration multiplied with Δ''t'' gives a correction to the velocity. The velocity multiplied with Δ''t'' gives a correction to the position. This procedure is repeated for all other bodies.\n\nThe result is a new value for position and velocity for all bodies. Then, using these new values one starts over the whole calculation for the next time-step Δ''t''. Repeating this procedure often enough, and one ends up with a description of the positions of all bodies over time.\n\nThe advantage of this method is that for a computer it is a very easy job to do, and it yields highly accurate results for all bodies at the same time, doing away with the complex and difficult procedures for determining perturbations. The disadvantage is that one must start with highly accurate figures in the first place, or the results will drift away from the reality in time; that one gets ''x'', ''y'', ''z'' positions which are often first to be transformed into more practical ecliptical or equatorial coordinates before they can be used; and that it is an all or nothing approach. If one wants to know the position of one planet on one particular time, then all other planets and all intermediate time-steps are to be calculated too.\n\n==Integration==\nIn the previous section it was assumed that acceleration remains constant over a small timestep Δt so that the calculation reduces to simply the addition of V × Δt to R and so forth. In reality this is not the case, except when one takes Δt so small that the number of steps to be taken would be prohibitively high. Because while at any time the position is changed by the acceleration, the value of the acceleration is determined by the instantaneous position. Evidently a full integration is needed.\n\nSeveral methods are available. First notice the needed equations:\n\n<math>\\vec{a}_j = \\sum_{i \\neq j}^n G \\frac{M_i}{|\\vec{r}_i - \\vec{r}_j|^3} (\\vec{r}_i - \\vec{r}_j)</math>\n\nThis equation describes the acceleration all bodies '''i''' running from 1 to N exercise on a particular body '''j'''. It is a vector equation, so it is to be split in 3 equations for each of the X, Y, Z components, yielding:\n\n<math>(a_j)_x = \\sum_{i \\neq j}^n G \\frac{M_i}{( (x_i - x_j)^2 + (y_i - y_j)^2 + (z_i - z_j)^2 )^{3/2}} (x_i - x_j)</math>\n\nwith the additional relationships\n\n<math>a_{x} = \\frac{dv_{x}}{dt}</math>, <math>v_{x} = \\frac{dx}{dt}</math>\n\nlikewise for Y and Z.\n\nThe former equation (gravitation) may look foreboding, but its calculation is no problem. The latter equations (motion laws) seems simpler, but yet it cannot be calculated. Computers cannot integrate, they cannot work with infinitesimal values, so instead of dt we use Δt and bringing the resulting variable to the left:\n\n<math>\\Delta v_x = a_{x} \\Delta t </math>, and: <math>\\Delta x = v_{x} \\Delta t </math>\n\nRemember that '''a''' is still a function of time. The simplest way to solve these is just the [[Euler]] algorithm, which in essence is the linear addition described above. Limiting ourselves to 1 dimension only in some general computer language:\n a.old = gravitationfunction(x.old)\n x.new = x.old + v.old * dt\n v.new = v.old + a.old * dt\n\nAs in essence the acceleration used for the whole duration of the timestep, is the one as it was in the beginning of the timestep, this simple method has no high accuracy. Much better results are achieved by taking a mean acceleration, the average between the beginning value and the expected (unperturbed) end value:\n\n a.old = gravitationfunction(x.old)\n x.expect = x.old + v.old * dt\n a.expect = gravitationfunction(x.expect)\n v.new = v.old + (a.old + a.expect) * 0.5 * dt\n x.new = x.old + (v.new + v.old) * 0.5 * dt\n\nOf course still better results can be expected by taking intermediate values. This is what happens when using the [[Runge-Kutta]] method, especially the one of grade 4 or 5 are most useful. The most common method used is the [[leapfrog method]] due to its good long term energy conservation. \n\nA completely different method is the use of [[Taylor series]]. In that case we write: <math>r = r_0 + r'_0 t + r''_0 \\frac{t^2}{2!} + ... </math>\n\nbut rather than developing up to some higher derivative in r only, one can develop in r and v (that is r') by writing <math>r = f r_0 + g r'_0</math>and then write out the factors ''f'' and ''g'' in a series.\n\n==Approximations==\nTo calculate the accelerations the gravitational attraction of each body on each other body is to be taken into account. As a consequence the amount of calculation in the simulation goes up with the square of the number of bodies: Doubling the number of bodies increases the work with a factor four. To increase the accuracy of the simulation not only more decimals are to be taken but also smaller timesteps, again quickly increasing the amount of work. Evidently tricks are to be applied to reduce the amount of work. Some of these tricks are given here.\n\nBy far the most important trick is the use of a proper integration method, as already outlined above.\n\nThe choice of units is important. Rather than to work in [[SI units]], which would make some values extremely small and some extremely large, all units are to be scaled such that they are in the neighbourhood of 1. For example for distances in the Solar System the [[astronomical unit]] is most straightforward. If this is not done one is almost certain to see a simulation aborted in the middle of a calculation on a [[floating point]] [[arithmetic overflow|overflow]] or [[arithmetic underflow|underflow]], and if not that bad, still accuracy is likely to get lost due to [[truncation]] errors.\n\nIf N is large (not so much in Solar System simulations, but more in galaxy simulations) it is customary to create dynamic groups of bodies. All bodies in a particular direction and on large distance from the reference body, which is being calculated at that moment, are taken together and their gravitational attraction is averaged over the whole group.\n\nThe total amount of [[energy]] and [[angular momentum]] of a closed system are conserved quantities. By calculating these amounts after every time step the simulation can be programmed to increase the stepsize Δt if they do not change significantly, and to reduce it if they start to do so. Combining the bodies in groups as in the previous and apply larger and thus less timesteps on the faraway bodies than on the closer ones, is also possible.\n\nTo allow for an excessively rapid change of the acceleration when a particular body is close to the reference body, it is customary to introduce a small softness parameter ''e'' so that\n<math>a = \\frac{G M}{r^2 + e}</math>\n\n==Complications==\nIf the highest possible accuracy is needed, the calculations become much more complex. In the case of comets, nongravitational forces, such as radiation pressure and gas drag, must be taken into account. In the case of Mercury, and other planets for long term calculations, relativistic effects cannot be ignored. Then also the total energy is no longer a constant (because the four vector energy with linear momentum is). The finite speed of light also makes it important to allow for light-time effects, both classical and relativistic. Planets can no longer be considered as particles, but their shape and density must also be considered. For example, the flattening of the Earth causes precession, which causes the axial tilt to change, which affects the long-term movements of all planets.\nLong term models, going beyond a few tens of millions of years, are not possible due to the lack of [[stability of the Solar System]].\n\n==See also==\n*[[Ephemeris]]\n*[[Astronomical algorithm]]\n*[[VSOP (planets)]]\n\n==References==\n*{{Cite book|first=Dan L. |last=Boulet |title=Methods of orbit determination for the microcomputer |publisher=Willmann-Bell, Inc |location=[[Richmond, Virginia]] |year=1991 |pages= |isbn=978-0-943396-34-7 |oclc=23287041}}{{Page needed|date=September 2010}}\n\n{{DEFAULTSORT:Numerical Model Of Solar System}}\n[[Category:Numerical analysis]]\n[[Category:Computational physics]]\n[[Category:Dynamical systems]]\n[[Category:Dynamics of the Solar System]]"
    },
    {
      "title": "Exponential integrator",
      "url": "https://en.wikipedia.org/wiki/Exponential_integrator",
      "text": "{{distinguish|text=[[exponential integral]] of [[exponential function]]s}}\n\n'''Exponential integrators''' are a class of [[numerical method]]s for the solution of [[ordinary differential equations]], specifically [[initial value problem]]s.  This large class of methods from [[numerical analysis]] is based on the exact integration of the [[Linear system|linear]] part of the initial value problem. Because the linear part is [[Integral|integrated]] exactly, this can help to mitigate the [[Stiff equation|stiffness]] of a differential equation. Exponential integrators can be constructed to be [[Explicit and implicit methods|explicit or implicit]] for [[Numerical methods for ordinary differential equations|numerical ordinary differential equations]] or serve as the [[integrator|time integrator]] for [[numerical partial differential equations]].\n\n== Background ==\nDating back to at least the 1960s, these methods were recognized by Certaine<ref>Certaine (1960)</ref> and Pope.<ref>Pope (1963)</ref> As of late exponential integrators have\nbecome an active area of research, see Hochbruck and Ostermann (2010).<ref name=\"Exponential integrators\"/>  Originally developed for solving [[Stiff equation|stiff differential equations]], the methods have been used to solve [[partial differential equations]] including [[Hyperbolic partial differential equations|hyperbolic]] as well as [[Parabolic partial differential equation|parabolic]]\nproblems<ref>Hochbruck and Ostermann, (2006)</ref> such as the [[heat equation]].\n\n==Introduction==\n\nWe consider [[initial value problem]]s of the form,\n:<math>u'(t) = L u(t) + N(u(t) ), \\qquad u(t_0)=u_0, \\qquad \\qquad (1)</math>\nwhere <math>L</math> is composed of [[Linear system|linear terms]], \nand <math> N</math> is composed of the [[Nonlinear system|non-linear]] terms.\nThese problems can come from a more typical initial value problem\n:<math>u'(t) = f(u(t)), \\qquad u(t_0)=u_0, </math>\nafter linearizing locally about a fixed or local state <math>u^*</math>:\n:<math> L = \\frac{\\partial f}{\\partial u}(u^*); \\qquad N = f(u) - L u.</math>\nHere, <math>\\frac{\\partial f}{\\partial u}</math> refers to the [[partial derivative]] of <math>f</math> with respect to <math>u</math> (the Jacobian of f).\n\nExact integration of this problem from time 0 to a later time <math>t</math> can be performed using [[matrix exponential]]s to define an integral equation for the exact solution:<ref name=\"Exponential integrators\">Hochbruck and Ostermann, (2010)</ref>\n:<math> u(t) = e^{L t } u_0 + \\int_{0}^{t} e^{ L (t-\\tau) } N\\left( u\\left( \\tau \\right) \\right)\\, d\\tau. \\qquad (2) </math>\nThis is similar to the exact integral used in the [[Picard–Lindelöf theorem]].  In the case of <math>N\\equiv 0</math>, this formulation is the exact solution to the [[linear differential equation]].\n\nNumerical methods require a [[discretization]] of equation (2). They can be based on \n[[Runge-kutta method|Runge-Kutta]] discretizations,<ref name=\"Cox and Mathews 2002\">Cox and Mathews (2002)</ref><ref>Tokman (2006, 2011)</ref>\n[[linear multistep method]]s or a variety of other options.\n\n==Exponential Rosenbrock methods==\nExponential Rosenbrock methods were shown to be very efficient in solving large systems of stiff ordinary differential equations, usually resulted from spatial discretization of time dependent (parabolic) PDEs. These integrators are constructed based on a continuous linearization of (1) along the numerical solution <math>u_n</math> \n:<math>\nu'(t)=L_{n} u(t)+ N_n(u(t)), \\qquad u(t_0)=u_0, \\qquad (3)\n</math>\nwhere \n<math> \nL_{n}=\\frac{\\partial f}{\\partial u}(u_n),  N_n (u)=f(u)-L_{n} u.\n</math>\nThis procedure enjoys the advantage, in each step, that\n<math> \n\\frac{\\partial N_n}{\\partial u}(u_n)= 0.\n</math>\nThis considerably simplifies the derivation of the order conditions and improves the stability when integrating the nonlinearity <math>N(u(t))</math>.\nAgain, applying the variation-of-constants formula (2) gives the exact solution at time <math>t_{n+1}</math> as\n:<math>\nu(t_{n+1})= e^{h_n L_n}u(t_n) + \\int_{0}^{h_n} e^{(h_n-\\tau) L_n}  N_n ( u(t_n+\\tau ))  d\\tau.  \\qquad (4)\n</math>\nThe idea now is to approximate the integral in (4) by some quadrature rule with nodes <math>c_i</math> and weights <math>b_i(h_n L_n)</math> (<math>1\\leq i\\leq s</math>). \nThis yields the following class of <math> s-stage </math> explicit exponential Rosenbrock methods, see Hochbruck and Ostermann (2006), Hochbruck, Ostermann and Schweitzer (2009):\n:<math>\nU_{ni}= e^{c_i h_n L_n}u_n +h_n \\sum_{j=1}^{i-1}a_{ij}(h_n L_n) N_n( U_{nj}),\n</math>\n:<math>\nu_{n+1} = e^{h_n L_n}u_n +  h_n \\sum_{i=1}^{s}b_{i}(h_n L_n)N_n(U_{ni})\n</math>\nwith \n<math>u_n \\approx u(t_n), U_{ni}\\approx u(t_n +c_i h_n), h_n = t_{n+1}-t_n  </math>. The coefficients <math> a_{ij}(z), b_i (z)</math> are usually chosen as linear combinations of the entire functions <math>\\varphi_{k}(c_i z), \\varphi_{k}(z) </math>, respectively, where\n:<math>\n\\varphi_0(z) = e^z,\\quad \\varphi _{k}(z)=\\int_{0}^{1} e^{(1-\\theta )z} \\frac{\\theta ^{k-1}}{(k-1)!} d\\theta , \\quad k\\geq 1.\n</math>\nThese functions satisfy the recursion relation\n:<math>\n\\varphi _{k+1}(z)=\\frac{\\varphi _{k}(z)-\\varphi_k(0)}{z}, \\ k\\geq 0.\n</math>\nBy introducing the difference <math>D_{ni}=N_n (U_{ni})-N_n (u_n)</math>, they can be reformulated in a more efficient way for implementation (see also <ref name=\"Exponential integrators\"/>) as\n:<math>\nU_{ni}= u_n + c_i h_n \\varphi _{1} ( c_i h_n L_n)f(u_n) +h_n \\sum_{j=2}^{i-1}a_{ij}(h_n L_n) D_{nj}, \n</math>\n:<math>\nu_{n+1}= u_n + h_n \\varphi _{1} ( h_n L_n)f(u_n) + h_n \\sum_{i=2}^{s}b_{i}(h_n L_n) D_{ni}.\n</math>\n\nIn order to implement this scheme with adaptive step size, one can consider, for the purpose of local error estimation, the following embedded methods\n:<math>\n\\bar{u}_{n+1}= u_n + h_n \\varphi _{1} ( h_n L_n)f(u_n) + h_n \\sum_{i=2}^{s} \\bar{b}_{i}(h_n L_n) D_{ni},\n</math>\nwhich use the same stages <math>U_{ni}</math> but with weights <math>\\bar{b}_{i}</math>.\n\nFor convenience, the coefficients of the explicit exponential Rosenbrock methods together with their embedded methods can be represented by using the so-called reduced Butcher tableau as follows:\n\n:{| style=\"text-align: center\" cellpadding=3px cellspacing=0px\n| style=\"border-right:1px solid;\" | <math> c_2 </math> ||\n|-\n| style=\"border-right:1px solid;\" | <math> c_3 </math> || <math> a_{32} </math>\n|-\n| style=\"border-right:1px solid;\" | <math> \\vdots </math> || <math> \\vdots </math> || || <math> \\ddots </math>\n|-\n| style=\"border-right:1px solid; border-bottom:1px solid;\" | <math> c_s </math>\n| style=\"border-bottom:1px solid;\" | <math> a_{s2} </math>\n| style=\"border-bottom:1px solid;\" | <math> a_{s3} </math>\n| style=\"border-bottom:1px solid;\" | <math> \\cdots </math>\n| style=\"border-bottom:1px solid;\" | <math> a_{s,s-1} </math> || style=\"border-bottom:1px solid;\" |\n|-\n| style=\"border-right:1px solid;\" | || <math> b_2 </math> || <math> b_3 </math> || <math> \\cdots </math> || <math> b_{s-1} </math> || <math> b_s </math>\n|-\n| style=\"border-right:1px solid;\" | || <math> \\bar{b}_2 </math> || <math> \\bar{b}_3 </math> || <math> \\cdots </math> || <math> \\bar{b}_{s-1} </math> || <math> \\bar{b}_s </math>\n|}\n\n===Stiff order conditions=== \nMoreover, it is shown in Luan and Osterman (2014a) <ref>Luan and Osterman (2014a)</ref> that the reformulation approach offers a new and simple way to analyze the local errors and thus to derive the stiff order conditions for exponential Rosenbrock methods up to order 5. With the help of this new technique together with an extension of the B-series concept, a theory for deriving the stiff order conditions for exponential Rosenbrock integrators of arbitrary order has been finally given in Luan and Osterman (2013).<ref>Luan and Osterman (2013)</ref> As an example, in that work the stiff order conditions for exponential Rosenbrock methods up to order 6 have been derived, which are stated in the following table:\n:<math>\n\\begin{array}{ |c|c|c| }\n\\hline\nNo. & \\text{Stiff order condition} & \\text{Order} \\\\\n\\hline\n1&\\sum_{i=2}^{s} b_i (Z)c^2_i=2\\varphi_3 (Z) &3 \\\\\n \\hline\n2&\\sum_{i=2}^{s} b_i (Z)c^3_i=6\\varphi_4 (Z) &4 \\\\\n\\hline\n3&\\sum_{i=2}^{s} b_i (Z)c^4_i=24\\varphi_5 (Z) &5 \\\\\n4&\\sum_{i=2}^{s} b_i (Z)c_i  K \\psi _{3,i}(Z)=0 &5 \\\\\n\\hline\n5&\\sum_{i=2}^{s}b_{i}(Z)c_i^5 = 120\\varphi_6(Z) & 6\\\\\n6&\\sum_{i=2}^{s}b_{i}(Z) c_i^2 M \\psi_{3,i}(Z)=0 & 6 \\\\\n7&\\sum_{i=2}^{s}b_{i}(Z) c_i K \\psi_{4,i}(Z)=0 & 6\\\\\n\\hline\n\\end{array}\n</math>\n\nHere  <math> Z, K, M.\\ </math> denote arbitrary square matrices.\n\n=== Convergence analysis=== \nThe stability and convergence results for exponential Rosenbrock methods are proved in the framework of strongly continuous semigroups in some Banach space.\n\n=== Examples=== \nAll the schemes presented below fulfill the stiff order conditions and thus are also suitable for solving stiff problems.\n\n==== Second-order method ====\nThe simplest exponential Rosenbrock method is the exponential Rosenbrock–Euler scheme, which has order 2, see, for example Hochbruck et al (2009):\n:<math>\nu_{n+1} =u_n + h_n \\ \\varphi_1(h_n L_n) f(u_n).\n</math>\n\n==== Third-order methods==== \nA class of third-order exponential Rosenbrock methods was derived in Hochbruck et al. (2009), named as  exprb32, is given as:\n\nexprb32:\n:{| style=\"text-align: center\" cellpadding=3px cellspacing=0px\n| style=\"border-right:1px solid; border-bottom:1px solid;\" | 1 || style=\"border-bottom:1px solid;\" | || style=\"border-bottom:1px solid;\" |\n|-\n| style=\"border-right:1px solid;\" | || <math>2\\varphi_3</math>\n|-\n| style=\"border-right:1px solid;\" | || 0\n|}\nwhich reads as \n:<math>\nU_{n2} =u_n + h_n \\ \\varphi_1( h_n L_n) f(u_n),\n</math>\n:<math>\nu_{n+1} =u_n + h_n \\ \\varphi_1(h_n L_n) f(u_n)+ h_n \\ 2\\varphi_3(h_n L_n) D_{n2},\n</math>\nwhere <math>D_{n2}=N_n (U_{n2})-N_n (u_n).</math>\n\nFor a variable step size implementation of this scheme, one can embed it with the exponential Rosenbrock–Euler:\n:<math>\n\\hat{u}_{n+1} =u_n + h_n \\ \\varphi_1(h_n L_n) f(u_n).\n</math>\n\n== Fourth-order ETDRK4 method of Cox and Matthews ==\n\nCox and Mathews<ref>Cox and Mathews 2002</ref> describe a fourth-order method exponential time differencing (ETD) method that they used [[Maple (software)|Maple]] to derive.\n\nWe use their notation, and assume that the unknown function is <math>u</math>, and that we have a known solution <math>u_n</math> at time <math>t_n</math>.\nFurthermore, we'll make explicit use of a possibly time dependent right hand side: <math>\\mathcal{N} = \\mathcal{N}( u, t )</math>.\n\nThree stage values are first constructed:\n:<math>\na_n = e^{ L h / 2 } u_n + L^{-1} \\left( e^{Lh/2} - I \\right) \\mathcal{N}( u_n, t_n )\n</math>\n:<math>\nb_n = e^{ L h / 2 } u_n + L^{-1} \\left( e^{Lh/2} - I \\right) \\mathcal{N}( a_n, t_n + h/2 )\n</math>\n:<math>\nc_n = e^{ L h / 2 } a_n + L^{-1} \\left( e^{Lh/2} - I \\right) \\left( 2 \\mathcal{N}( b_n, t_n + h/2 ) - \\mathcal{N}(u_n,t_n) \\right)\n</math>\nThe final update is given by,\n:<math>\nu_{n+1} = e^{L h} u_n + h^{-2} L^{-3} \\left\\{ \n\\left[ -4 - Lh + e^{Lh} \\left( 4 - 3 L h + (L h)^2 \\right)  \\right] \\mathcal{N}( u_n, t_n ) +\n2 \\left[ 2 + L h + e^{Lh} \\left( -2 + L h \\right) \\right] \\left( \\mathcal{N}( a_n, t_n+h/2 ) + \\mathcal{N}( b_n, t_n + h / 2 ) \\right) +\n\\left[ -4 - 3L h - (Lh)^2 + e^{Lh} \\left(4 - Lh \\right) \\right] \\mathcal{N}( c_n, t_n + h )\n\\right\\}.\n</math>\n\nIf implemented naively, the above algorithm suffers from numerical instabilities due to [[floating point]] round-off errors.<ref name=\"Kassam and Trefethen 2005\">Kassam and Trefethen (2005)</ref>  To see why, consider the first function,\n:<math> \\varphi_1( z ) = \\frac{ 1 - e^z }{ z },</math>\nwhich is present in the first-order Euler method, as well as all three stages of ETDRK4.  For small values of <math>z</math>, this function suffers from numerical cancellation errors.  However, these numerical issues can be avoided by evaluating the <math> \\varphi_1 </math> function via a contour integral approach <ref name=\"Kassam and Trefethen 2005\"/> or by a [[Padé approximant]].<ref>Berland et al. (2007)</ref>\n\n== Applications ==\n\nExponential integrators are used for the simulation of stiff scenarios in [[Computational science|scientific]] and [[Visual computing|visual]] computing, for example in [[molecular dynamics]],<ref>Michels and Desbrun (2015)</ref> for [[Very-large-scale integration|VLSI]] circuit simulation,<ref>Zhuang et al. (2014)</ref> and in [[computer graphics]].<ref>Michels et al. (2014)</ref> They are also applied in the context of [[Hybrid Monte Carlo|hybrid monte carlo]] methods.<ref>Chao et al. (2015)</ref> In these applications, exponential integrators show the advantage of large time stepping capability and high accuracy. To accelerate the evaluation of matrix functions in such complex scenarios, exponential integrators are often combined with Krylov subspace projection methods.\n\n== See also ==\n*[[General linear methods]]\n\n== Notes ==\n<references/>\n\n==References==\n* {{cite journal|title=B-series and Order Conditions for Exponential Integrators|first1=Havard|last1=Berland|first2=Brynjulf|last2=Owren|first3=Bard|last3=Skaflestad|journal=SIAM Journal on Numerical Analysis|volume=43|issue=4|pages=1715–1727|year=2005|doi=10.1137/040612683|citeseerx=10.1.1.216.5645}}\n* {{cite journal|last1=Berland|first1=Havard|last2=Skaflestad|first2=Bard|last3=Wright|first3=Will M.|title=EXPINT-A MATLAB Package for Exponential Integrators|journal=ACM Transactions on Mathematical Software|year=2007|volume=33|issue=1|pages=4–es|doi=10.1145/1206040.1206044|url=http://cds.cern.ch/record/848126}}\n* {{cite journal|last1=Chao|first1=Wei-Lun|last2=Solomon|first2=Justin|last3=Michels|first3=Dominik L.|last4=Sha|first4=Fei|title=Exponential Integration for Hamiltonian Monte Carlo|journal=Proceedings of the 32nd International Conference on Machine Learning (ICML-15)|pages=1142–1151|year=2015}}\n* {{cite book| last=Certaine | first=John|title=The solution of ordinary differential equations with large time constants|year=1960|pages=128–132|publisher=Wiley|booktitle=Mathematical methods for digital computers}}\n* {{cite journal|last1=Cox|first1=S. M.|last2=Matthews|first2=P.C.|\ntitle=Exponential time differencing for stiff systems|\ndoi=10.1006/jcph.2002.6995|journal=Journal of Computational Physics|volume=176|issue=2| date=March 2002 |pages=430–455|bibcode=2002JCoPh.176..430C}}\n* {{cite journal|last1=Hochbruck|first1=Marlis|author1-link=Marlis Hochbruck|last2=Ostermann|first2=Alexander|title=Exponential integrators|journal=Acta Numerica| date=May 2010 |volume=19|pages=209–286|doi= 10.1017/S0962492910000048 |url=http://journals.cambridge.org/action/displayAbstract?fromPage=online&aid=7701740|bibcode=2010AcNum..19..209H|citeseerx=10.1.1.187.6794}}\n* {{cite journal|last1=Hochbruck|first1=Marlis|author1-link=Marlis Hochbruck|last2=Ostermann|first2=Alexander|\ntitle=Explicit exponential Runge-Kutta methods for semilinear parabolic problems|\njournal=SIAM Journal on Numerical Analysis|volume=43|issue=3|pages=1069–1090|year=2005|doi=10.1137/040611434|url=https://publikationen.bibliothek.kit.edu/1000042061/3153602}}\n* {{cite journal|last1=Hochbruck|first1=Marlis|author1-link=Marlis Hochbruck|last2=Ostermann|first2=Alexander|\ntitle=Exponential Runge–Kutta methods for parabolic problems|\ndoi=10.1016/j.apnum.2004.08.005|\njournal=Applied Numerical Mathematics|\nvolume=53|issue=2–4| date=May 2005 |pages=323–339|url=https://publikationen.bibliothek.kit.edu/1000042292/3168261}}\n* {{cite journal|last1=Luan|first1=Vu Thai|last2=Ostermann|first2=Alexander|\ntitle=Exponential Rosenbrock methods of order five-construction, analysis and numerical comparisons|\njournal=Journal of Computational and Applied Mathematics|\nvolume=255|year=2014a|pages=417–431|doi=10.1016/j.cam.2013.04.041}}\n* {{cite journal|last1=Luan|first1=Vu Thai|last2=Ostermann|first2=Alexander|\ntitle=Explicit exponential Runge-Kutta methods of high order for parabolic problems|\njournal=Journal of Computational and Applied Mathematics|\nvolume=256|year=2014c|pages=168–179|doi=10.1016/j.cam.2013.07.027|arxiv=1307.0661}}\n* {{cite journal|last1=Luan|first1=Vu Thai|last2=Ostermann|first2=Alexander|\ntitle=Exponential B-series: The stiff case|\njournal=SIAM Journal on Numerical Analysis|\nvolume=51|issue=6|year=2013|pages=3431–3445|doi=10.1137/130920204}}\n* {{cite book|last1=Luan|first1=Vu Thai|last2=Ostermann|first2=Alexander|\ntitle= Stiff order conditions for exponential Runge-Kutta methods of order five|\njournal=Modeling, Simulation and Optimization of Complex Processes - HPSC 2012 (H.G. Bock et Al. Eds.)|\nvolume=|year=2014b|pages=133–143|doi=10.1007/978-3-319-09063-4_11|isbn=978-3-319-09062-7}}\n* {{cite journal|last1=Luan|first1=Vu Thai|last2=Ostermann|first2=Alexander|\ntitle=Parallel exponential Rosenbrock methods|\njournal=Computers and Mathematics with Applications|\nvolume=71|issue=5|pages=1137–1150|year=2016|doi=10.1016/j.camwa.2016.01.020}}\n* {{cite journal|last1=Michels|first1=Dominik L.|last2=Desbrun|first2=Mathieu|title=A Semi-analytical Approach to Molecular Dynamics|journal=Journal of Computational Physics|volume=303|pages=336–354|year=2015|doi=10.1016/j.jcp.2015.10.009|bibcode=2015JCoPh.303..336M}}\n* {{cite journal|last1=Michels|first1=Dominik L.|last2=Sobottka|first2=Gerrit A.|last3=Weber|first3=Andreas G.|title=Exponential Integrators for Stiff Elastodynamic Problems|journal=ACM Transactions on Graphics|volume=33|pages=7:1–7:20|year=2014|doi=10.1145/2508462|url=http://dl.acm.org/citation.cfm?doid=2577382.2508462}}\n* {{cite journal|last=Pope|first=David A|title=An exponential method of numerical integration of ordinary differential equations|journal=Communications of the ACM|volume=6|issue=8|pages=491–493|year=1963|doi=10.1145/366707.367592}}\n* {{cite journal|\ntitle=A new class of exponential propagation iterative methods of Runge–Kutta type (EPIRK)|\nlast=Tokman|first=Mayya|\ndoi=10.1016/j.jcp.2011.08.023|\njournal=Journal of Computational Physics|\nvolume=230|issue=24| date=October 2011 |pages=8762–8778|bibcode=2011JCoPh.230.8762T}}\n* {{cite journal|title=Efficient integration of large stiff systems of ODEs with exponential propagation iterative (EPI) methods|\nlast=Tokman|first=Mayya| date=April 2006 |doi=10.1016/j.jcp.2005.08.032|\njournal=Journal of Computational Physics|\nvolume=213|issue=2|pages=748–776|bibcode=2006JCoPh.213..748T}}\n*{{cite journal|last=Trefethen|first=Lloyd N.|author2=Aly-Khan Kassam |title=Fourth-Order Time-Stepping for Stiff PDEs|journal=SIAM Journal on Scientific Computing|year=2005|volume=26|issue=4|pages=1214–1233|doi=10.1137/S1064827502410633|citeseerx=10.1.1.15.6467}}\n* {{cite conference|last=Zhuang|first=Hao|last2=Weng |first2=Shih-Hung |last3=Lin |first3=Jeng-Hau |last4=Cheng|first4=Chung-Kuan |title=MATEX: A Distributed Framework for Transient Simulation of Power Distribution Networks. |journal=ACM/IEEE Proceedings of the 51st Annual Design Automation Conference (DAC) |year=2014 |doi=10.1145/2593069.2593160 |url=http://cseweb.ucsd.edu/~hazhuang/papers/dac14_matex.pdf|arxiv=1511.04519}}\n\n== External links ==\n* [http://hgpu.org/?p=6901|Exponential integrators on GPGPUs]\n* [http://www.mathworks.com/matlabcentral/fileexchange/40949-meshfree-exponential-integrator|MATLAB code for a meshfree exponential integrator]\n\n{{Numerical integrators}}\n\n{{DEFAULTSORT:Numerical Ordinary Differential Equations}}\n[[Category:Numerical differential equations| ]]\n[[Category:Ordinary differential equations]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Numerical smoothing and differentiation",
      "url": "https://en.wikipedia.org/wiki/Numerical_smoothing_and_differentiation",
      "text": "#REDIRECT [[Savitzky–Golay filter]]\n\n[[Category:Numerical analysis]]\n[[Category:Interpolation]]\n[[Category:Mathematical chemistry]]\n[[Category:Cheminformatics]]\n[[Category:Least squares]]\n{{R from merge}}"
    },
    {
      "title": "Numerical stability",
      "url": "https://en.wikipedia.org/wiki/Numerical_stability",
      "text": "{{More footnotes|date=February 2012}}\nIn the [[mathematics|mathematical]] subfield of [[numerical analysis]], '''numerical stability''' is a generally desirable property of [[numerical algorithm]]s. The precise definition of stability depends on the context. One is [[numerical linear algebra]] and the other is algorithms for solving ordinary and partial differential equations by discrete approximation.\n\nIn numerical linear algebra the principal concern is instabilities caused by proximity to singularities of various kinds, such as very small or nearly colliding [[eigenvalues]]. On the other hand, in numerical algorithms for differential equations the concern is the growth of round-off errors and/or initially small fluctuations in initial data which might cause a large deviation of final answer from the exact solution {{Citation needed|date=October 2017}}.\n\nSome numerical algorithms may damp out the small fluctuations (errors) in the input data; others might magnify such errors.  Calculations that can be proven not to magnify approximation errors are called ''numerically stable''.  One of the common tasks of numerical analysis is to try to select algorithms which are ''robust''&nbsp;– that is to say, do not produce a wildly different result for very small change in the input data.\n\nAn [[opposite (semantics)|opposite]] phenomenon is '''instability'''. Typically,  an algorithm involves an approximative method, and in some cases one could prove that the algorithm would approach the right solution in some limit (when using actual real numbers, not floating point numbers). Even in this case, there is no guarantee that it would converge to the correct solution, because the floating-point round-off or truncation errors can be magnified, instead of damped, causing the deviation from the exact solution to grow exponentially.<ref>{{cite book | title=Numerical Algorithms with C | author1=Giesela Engeln-Müllges|author2= Frank Uhlig |others= M. Schon (Translator), F. Uhlig (Translator) | edition=1 |date=2 July 1996 | publisher=Springer | pages=10 | isbn=978-3-540-60530-0 | url=https://books.google.com/books?id=HurESoDQljcC&pg=PA10#v=onepage&q&f=false}}</ref>\n\n== Stability in numerical linear algebra ==\nThere are different ways to formalize the concept of stability. The following definitions of forward, backward, and mixed stability are often used in [[numerical linear algebra]].\n\n[[File:Forward and backward error.svg|frame|Diagram showing the '''forward error''' Δ''y'' and the '''backward error''' Δ''x'', and their relation to the exact solution map&nbsp;{{mvar|f}} and the numerical solution&nbsp;{{mvar|f}}*.]]\n\nConsider the problem to be solved by the numerical algorithm as a [[function (mathematics)|function]]&nbsp;{{mvar|f}} mapping the data&nbsp;{{mvar|x}} to the solution&nbsp;{{mvar|y}}. The result of the algorithm, say {{mvar|y}}*, will usually deviate from the \"true\" solution&nbsp;{{mvar|y}}. The main causes of error are [[round-off error]] and [[truncation error]]. The ''forward error'' of the algorithm is the difference between the result and the solution; in this case, {{math|1=Δ''y'' = ''y''* − ''y''}}. The ''backward error'' is the smallest Δ{{mvar|x}} such that {{math|1=''f'' (''x'' + Δ''x'') = ''y''*}}; in other words, the backward error tells us what problem the algorithm actually solved. The forward and backward error are related by the [[condition number]]: the forward error is at most as big in magnitude as the condition number multiplied by the magnitude of the backward error.\n\nIn many cases, it is more natural to consider the [[relative error]]\n:<math> \\frac{|\\Delta x|}{|x|} </math>\ninstead of the absolute error Δ{{mvar|x}}.\n\nThe algorithm is said to be ''backward stable'' if the backward error is small for all inputs&nbsp;{{mvar|x}}. Of course, \"small\" is a relative term and its definition will depend on the context. Often, we want the error to be of the same order as, or perhaps only a few [[orders of magnitude]] bigger than, the [[unit round-off]].\n\n[[File:Mixed stability diagram.svg|thumb|Mixed stability combines the concepts of forward error and backward error.]]\n\nThe usual definition of numerical stability uses a more general concept, called ''mixed stability'', which combines the forward error and the backward error. An algorithm is stable in this sense if it solves a nearby problem approximately, i.e., if there exists a Δ{{mvar|x}} such that both Δ{{mvar|x}} is small and {{math|''f'' (''x'' + Δ''x'') − ''y''*}} is small. Hence, a backward stable algorithm is always stable.\n\nAn algorithm is ''forward stable'' if its forward error divided by the condition number of the problem is small. This means that an algorithm is forward stable if it has a forward error of magnitude similar to some backward stable algorithm.\n\n== Stability in numerical differential equations ==\nThe above definitions are particularly relevant in situations where truncation errors are not important. In other contexts, for instance when solving [[differential equation]]s, a different definition of numerical stability is used.\n\nIn [[Numerical methods for ordinary differential equations|numerical ordinary differential equations]], various concepts of numerical stability exist, for instance [[Stiff equation#A-stability|A-stability]]. They are related to some concept of stability in the [[dynamical system]]s sense, often [[Lyapunov stability]]. It is important to use a stable method when solving a [[stiff equation]].\n\nYet another definition is used in [[numerical partial differential equations]]. An algorithm for solving a linear evolutionary [[partial differential equation]] is stable if the [[total variation]] of the numerical solution at a fixed time remains bounded as the step size goes to zero. The [[Lax equivalence theorem]] states that an algorithm [[Numerical_methods_for_ordinary_differential_equations#Convergence|converges]] if it is [[Numerical_methods_for_ordinary_differential_equations#Consistency_and_order|consistent]] and [[Numerical_methods_for_ordinary_differential_equations#Stability_and_stiffness|stable]] (in this sense). Stability is sometimes achieved by including [[numerical diffusion]]. Numerical diffusion is a mathematical term which ensures that roundoff and other errors in the calculation get spread out and do not add up to cause the calculation to \"blow up\". [[Von Neumann stability analysis]] is a commonly used procedure for the stability analysis of [[finite difference method|finite difference schemes]] as applied to linear partial differential equations.  These results do not hold for nonlinear PDEs, where a general, consistent definition of stability is complicated by many properties absent in linear equations.\n\n== See also ==\n* [[Algorithms for calculating variance]]\n* [[Stability theory]]\n* [[Chaos theory]]\n\n== References ==\n{{reflist}}\n* {{cite book|author=[[Nicholas J. Higham]]|title=Accuracy and Stability of Numerical Algorithms|publisher= Society of Industrial and Applied Mathematics|location= Philadelphia|date= 1996|isbn= 0-89871-355-2}}\n* {{cite book|author1=Richard L. Burden |author2= J. Douglas Faires|title=Numerical Analysis|edition= 8th|publisher= Thomson Brooks/Cole|location= U.S.|date= 2005|isbn= 0-534-39200-8}}\n* {{cite arxiv|eprint=1605.04339 |last1=Mesnard |first1=Olivier |title=Reproducible and replicable CFD: It's harder than you think |last2= Barba |first2=Lorena A. |class=physics.comp-ph |year=2016 }}\n\n[[Category:Numerical analysis]]"
    },
    {
      "title": "List of operator splitting topics",
      "url": "https://en.wikipedia.org/wiki/List_of_operator_splitting_topics",
      "text": "This is a '''list of operator splitting topics'''.\n\n==General==\n*[[Alternating direction implicit method]] — finite difference method for parabolic, hyperbolic, and elliptic partial differential equations\n*[[GRADELA]] — simple gradient elasticity model\n*[[Matrix splitting]] — general method of splitting a matrix operator into a sum or difference of matrices\n*[[Paul Tseng]] — resolved question on covergence of matrix splitting algorithms\n*[[PISO algorithm]] — pressure-velocity calculation for Navier-Stokes equations\n*[[Projection method (fluid dynamics)]] — computational fluid dynamics method\n*[[Reactive transport modeling in porous media]] — modeling of chemical reactions and fluid flow through the Earth's crust\n*[[Richard S. Varga]] — developed matrix splitting\n*[[Strang splitting]] — specific numerical method for solving differential equations using operator splitting\n\n[[Category:Numerical analysis|Operator splitting]]\n[[Category:Mathematics-related lists|Operator splitting]]\n[[Category:Wikipedia outlines|Operator splitting]]"
    },
    {
      "title": "Order of accuracy",
      "url": "https://en.wikipedia.org/wiki/Order_of_accuracy",
      "text": "In [[numerical analysis]], '''order of accuracy''' quantifies the [[rate of convergence]] of a numerical approximation of a [[differential equation]] to the exact solution.\nConsider <math>u</math>, the exact solution to a differential equation in an appropriate normed space <math>(V,||\\ ||)</math>. Consider a numerical approximation <math>u_h</math>, where <math>h</math> is a parameter characterizing the approximation, such as the step size in a finite difference scheme or the diameter of the cells in a [[Finite_element_method|finite element method]].\nThe numerical solution <math>u_h</math> is said to be '''<math>n</math>th-order accurate''' if the error, <math>E(h):= ||u-u_h||</math> is proportional to the step-size <math>h</math> to the <math>n</math>th power;<ref name=\"LeVeque\">{{cite book|last=LeVeque|first=Randall J|title=Finite Difference Methods for Differential Equations|url=http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.111.1693|year=2006|publisher=University of Washington|pages=3–5}}</ref>\n\n:<math> E(h) = ||u-u_h|| \\leq Ch^n </math>\n\nWhere the constant <math>C</math> is independent of h and usually depends on the solution <math>u</math>.<ref>{{cite book|last=Ciarliet|first=Philippe J|title=The Finite Element Method for Elliptic Problems|url=https://epubs.siam.org/doi/book/10.1137/1.9780898719208|year=1978|publisher=Elsevier|pages=105-106}}</ref>. Using the [[big O notation]] an <math>n</math>th-order accurate numerical method is notated as\n\n:<math> ||u-u_h|| = O(h^n) </math>\n\nThis definition is strictly depended on the norm used in the space; the choice of such norm is fundamental to estimate the rate of convergence and, in general, all numerical errors correctly.\n\nThe size of the error of a first-order accurate approximation is directly proportional to <math>h</math>.\n[[Partial differential equations]] which vary over both time and space are said to be accurate to order <math>n</math> in time and to order <math>m</math> in space.<ref name=\"Strikwerda\">{{cite book|last=Strikwerda|first=John C|title=Finite Difference Schemes and Partial Differential Equations|edition=2|year=2004|isbn=978-0-898716-39-9|pages=62–66}}</ref>\n\n== References ==\n{{reflist}}\n\n[[Category:Numerical analysis]]\n\n\n{{applied-math-stub}}"
    },
    {
      "title": "Order of approximation",
      "url": "https://en.wikipedia.org/wiki/Order_of_approximation",
      "text": "{{Order-of-approx}}\n{{multiple issues|{{unclear|date=March 2016}}\n{{unreferenced|date=March 2008}}}}\n\nIn [[science]], [[engineering]], and other quantitative disciplines, '''orders of approximation''' refer to formal or informal expressions for how [[Accuracy_and_precision|accurate]] an [[approximation]] is. In formal expressions, the [[English_numerals#Ordinal_numbers|ordinal number]] used before the word '''order''' refers to the highest term in the [[series expansion]] used in the approximation. The choice of series expansion depends on the [[scientific method]] used to investigate a [[Phenomenon#Scientific|phenomenon]].  The expression '''order of approximation''' is  expected to indicate progressively more refined approximations of a [[Function_(mathematics)|function]] in a specified [[Interval_(mathematics)|interval]]. If a quantity is constant within the whole interval, approximating it with a second-order Taylor series will not increase the accuracy. Thus the numbers ''zeroth'', ''first'', ''second'' etc. used formally in the above meaning do not directly give information about [[percent error]] or [[significant figures]].\n\nThis formal usage of ''order of approximation'' corresponds to the [[Power series#Order_of_a_power_series|order]] of the [[power series]] representing the error, which is the first [[Derivative#Higher_derivatives|first nonzero higher derivative]] of the error. The expressions: a '''zeroth-order''' approximation, a '''first-order''' approximation, a '''second-order''' approximation, and so forth are used as [[fixed phrase]]s. \n\nThe omission of the word ''order'' leads to [[Phrase|phrases]] that have less formal meaning. Phrases like '''first approximation''' or '''to a first approximation''' may refer to ''a roughly approximate value of a quantity''. <ref> ''first approximation'' in Webster's Third New International Dictionary, Könemann, {{ISBN|3-8290-5292-8}} </ref> <ref>[http://www.webster-dictionary.org/definition/to%20a%20first%20approximation  ''to a first approximation'' in Online Dictionary and Translations Webster-dictionary.org]</ref>  The phrase '''to a zeroth approximation''' indicates ''a wild guess''.  <ref>[http://www.webster-dictionary.org/definition/to%20a%20zeroth%20approximation ''to a zeroth approximation'' in Online Dictionary and Translations Webster-dictionary.org]</ref>  The expression ''order of approximation'' is sometimes informally used to mean the number of [[significant figure]]s, in increasing order of accuracy, or to the [[order of magnitude]]. However, this may be confusing as these formal expressions do not directly refer to the order of derivatives. \n \nFormally, an ''n''th-order approximation is one where the [[order of magnitude]] of the error is at most <math>x^{n+1}</math>, or {{cns|text=in terms of [[big O notation]], the error is <math>O(x^{n+1}).</math>|date=March 2016}}\nIn the case of a [[smooth function]],  the ''n''th-order approximation is a [[polynomial]] of [[degree of a polynomial|degree]] ''n'', which is obtained by truncating the [[Taylor series]] to this degree.\n\n==Usage in science and engineering==\n\n===Zeroth-order ===\n''Zeroth-order approximation'' (also 0th order) is the term [[scientist]]s use for a first rough answer. Many [[Approximation#Science|simplifying assumptions]] are made, and when a number is needed, an order-of-magnitude answer (or zero [[significant figure]]s) is often given.  For example, you might say \"the town has '''a few thousand''' residents\", when it has 3,914 people in actuality. This is also sometimes referred to as an [[order of magnitude|order-of-magnitude]] approximation. The zero of \"zeroth-order\" represents the fact that even the only number given, \"a few,\" is itself loosely defined. \n\nA zeroth-order approximation of a [[function (mathematics)|function]] (that is, [[mathematics|mathematically]] determining a [[formula]] to fit multiple [[data point]]s) will be [[Constant (mathematics)|constant]], or a flat [[line (mathematics)|line]] with no [[slope]]: a polynomial of degree 0.  For example,\n\n:<math>x=[0,1,2]\\,</math>\n:<math>y=[3,3,5]\\,</math>\n:<math>y\\sim f(x)=3.67\\,</math>\n\nis an approximate fit to the data, obtained by simply averaging the x-values and the y-values then deriving a multiplicative function for that average.  Other methods for selecting a constant approximation can be used, such as taking the average of x-values and y-values and then working out the average difference between them:\n:<math>y \\sim\\ x+2.67 </math>\n\n===First-order===\n''First-order approximation'' (also 1st order) is the term scientists use for a slightly better answer. Some simplifying assumptions are made, and when a number is needed, an answer with only one significant figure is often given (\"the town has 4×10<sup>3</sup> or '''four thousand''' residents\").  In the case of a first-order approximation, at least one number given is exact. In the zeroth order example above, the quantity \"a few\" was given but in the first order example, the number \"4\" is given.\n\nA first-order approximation of a function (that is, mathematically determining a formula to fit multiple data points) will be a [[linear approximation]], straight line with a slope: a polynomial of degree 1.  For example,\n\n:<math>x=[0,1,2]\\,</math>\n:<math>y=[3,3,5]\\,</math>\n:<math>y\\sim f(x)=x+2.67\\,</math>\n\nis an approximate fit to the data.\nIn this example there is a zeroth order approximation that is the same as the first order but the method of getting there is different.  i.e. a wild stab in the dark at a relationship happened to be as good as an 'educated guess'.\n\n===Second-order===\n''Second-order approximation'' (also 2nd order) is the term scientists use for a decent-quality answer.  Few simplifying assumptions are made, and when a number is needed, an answer with two or more significant figures (\"the town has 3.9×10<sup>3</sup> or '''thirty nine hundred''' residents\") is generally given. In [[mathematical finance]], second-order approximations are known as [[convexity correction]]s. As in the examples above, the term \"2nd order\" refers to the number of exact numerals given for the imprecise quantity. In this case, \"3\" and \"9\" are given as the two successive levels of precision, instead of simply the \"4\" from the first order, or \"a few\" from the zeroth-order found in the examples above.\n\nA second-order approximation of a function (that is, mathematically determining a formula to fit multiple data points) will be a [[quadratic polynomial]], geometrically, a [[parabola]]: a polynomial of degree 2.  For example,\n\n:<math>x=[0,1,2]\\,</math>\n:<math>y=[3,3,5]\\,</math>\n:<math>y\\sim f(x)=x^2-x+3\\,</math>\n\nis an approximate fit to the data.  In this case, with only three data points, a parabola is an exact fit.\n\n===Higher-order===\nWhile higher-order approximations exist and are crucial to a better understanding and description of reality, they are not typically referred to by number.  \n\nContinuing the above, a third-order approximation would be required to perfectly fit four data points, and so on. See [[polynomial interpolation]].\n\nThese terms are also used colloquially by scientists and engineers to describe phenomena that can be neglected as not significant (e.g. \"Of course the rotation of the Earth affects our experiment, but it's such a high-order effect that we wouldn't be able to measure it\" or \"At these velocities, relativity is a fourth-order effect that we only worry about at the annual calibration.\") In this usage, the ordinality of the approximation is not exact, but is used to emphasize its insignificance; the higher the number used, the less important the effect. The terminology, in this context, represents a high level of precision required to account for an effect which is inferred to be very small when compared to the overall subject matter. The higher the order, the more precision is required to measure the effect, and therefore the smallness of the effect in comparison to the overall measurement.\n\n== See also ==\n* [[Linearization]]\n* [[Perturbation theory]]\n* [[Taylor series]]\n* [[Chapman–Enskog_theory#Mathematical_Formulation | Chapman-Enskog method ]]\n* [[Big O notation]]\n==References==\n{{reflist}}\n\n[[Category:Perturbation theory]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Overlap–add method",
      "url": "https://en.wikipedia.org/wiki/Overlap%E2%80%93add_method",
      "text": "{{hatnote|This article is about a method of performing convolution, not to be confused with WOLA (Weight, OverLap, Add), which is a method of performing channelization.  See [[Discrete-time_Fourier_transform#Sampling_the_DTFT|Sampling the DTFT]]}}\n\n{{hatnote|This article uses common abstract notations, such as &nbsp;<math>\\scriptstyle y(t) = x(t) * h(t),</math>&nbsp; or &nbsp;<math>\\scriptstyle y(t) = \\mathcal{H}\\{x(t)\\},</math>&nbsp; in which it is understood that the functions should be thought of in their totality, rather than at specific instants <math>t.</math>  (see [[Convolution#Notation]])}}\n\nIn [[signal processing]], the '''overlap–add method (OA, OLA)''' is an efficient way to evaluate the discrete [[convolution]] of a very long signal <math>x[n]</math> with a [[finite impulse response]] (FIR) filter <math>h[n]</math>''':'''\n\n:<math>\n\\begin{align}\ny[n] = x[n] * h[n] \\ \\stackrel{\\mathrm{def}}{=} \\ \\sum_{m=-\\infty}^{\\infty} h[m] \\cdot x[n-m]\n= \\sum_{m=1}^{M} h[m] \\cdot x[n-m],\n\\end{align}</math>\n\nwhere ''h''[''m''] = 0 for ''m'' outside the region [1, ''M''].\n\nThe concept is to divide the problem into multiple convolutions of ''h''[''n''] with short segments of <math>x[n]</math>''':'''\n\n:<math>x_k[n]  \\ \\stackrel{\\mathrm{def}}{=} \n\\begin{cases}\nx[n+kL] & n=1,2,\\ldots,L\\\\\n0 & \\textrm{otherwise},\n\\end{cases}\n</math>\n\nwhere ''L'' is an arbitrary segment length.  Then''':'''\n\n:<math>x[n] = \\sum_{k} x_k[n-kL],\\,</math>\n\nand ''y''[''n''] can be written as a sum of short convolutions''':'''\n\n:<math>\n\\begin{align}\ny[n] = \\left(\\sum_{k} x_k[n-kL]\\right) * h[n] &= \\sum_{k} \\left(x_k[n-kL]* h[n]\\right)\\\\\n&= \\sum_{k} y_k[n-kL],\n\\end{align}\n</math>\n\nwhere &nbsp;<math>y_k[n] \\ \\stackrel{\\mathrm{def}}{=} \\ x_k[n]*h[n]\\,</math>&nbsp; is zero outside the region [1,&nbsp;''L''&nbsp;+&nbsp;''M''&nbsp;&minus;&nbsp;1]. &nbsp;And for any parameter &nbsp;<math>N\\ge L+M-1,\\,</math>&nbsp; it is equivalent to the <math>N\\,</math>-point [[circular convolution]] of <math>x_k[n]\\,</math> with <math>h[n]\\,</math>&nbsp; in the region&nbsp;[1,&nbsp;''N''].\n\nThe advantage is that the [[circular convolution]] can be computed very efficiently as follows, according to the [[Discrete Fourier transform#Circular convolution theorem and cross-correlation theorem|circular convolution theorem]]''':'''\n\n{{NumBlk|:|<math>y_k[n] = \\textrm{IFFT}\\left(\\textrm{FFT}\\left(x_k[n]\\right)\\cdot\\textrm{FFT}\\left(h[n]\\right)\\right)</math>|{{EquationRef|Eq.1}}}}\n\nwhere FFT and IFFT refer to the [[fast Fourier transform]] and inverse\nfast Fourier transform, respectively, evaluated over <math>N</math> discrete\npoints.\n\n== The algorithm ==\n\n[[Image:Depiction of overlap-add algorithm.png|frame|none|Figure 1: the overlap–add method]]\n\nFig. 1 sketches the idea of the overlap–add method. The\nsignal <math>x[n]</math> is first partitioned into non-overlapping sequences,\nthen the [[discrete Fourier transform]]s of the sequences <math>y_k[n]</math>\nare evaluated by multiplying the FFT of <math>x_k[n]</math> with the FFT of\n<math>h[n]</math>. After recovering of <math>y_k[n]</math> by inverse FFT, the resulting\noutput signal is reconstructed by overlapping and adding the <math>y_k[n]</math>\nas shown in the figure. The overlap arises from the fact that a linear\nconvolution is always longer than the original sequences. In the early days of development of the fast Fourier transform, <math>L</math> was often  chosen to be a power of 2 for efficiency, but further development has revealed efficient transforms for larger prime factorizations of L, reducing computational sensitivity to this parameter. A [[pseudocode]] of the algorithm is the\nfollowing:\n\n    '''Algorithm 1''' (''OA for linear convolution'')\n    Evaluate the best value of N and L (L>0, N = M+L-1 nearest to power of 2).\n    Nx = length(x);\n    H = FFT(h,N)       <span style=\"color:green;\">(''zero-padded FFT'')</span>\n    i = 1\n    y = zeros(1, M+Nx-1)\n    '''while''' i <= Nx  <span style=\"color:green;\">(''Nx: the last index of x[n]'')</span>\n        il = min(i+L-1,Nx)\n        yt = IFFT( FFT(x(i:il),N) * H, N)\n        k  = min(i+N-1,M+Nx-1)\n        y(i:k) = y(i:k) + yt(1:k-i+1)    <span style=\"color:green;\">(''add the overlapped output blocks'')</span>\n        i = i+L\n    '''end'''\nThis algorithm is based on the linearity of the DFT.\n\n== Circular convolution with the overlap–add method ==\n\nWhen sequence ''x''[''n''] is periodic, and ''N''<sub>''x''</sub> is the period, then ''y''[''n''] is also periodic, with the same period. &nbsp;To compute one period of y[n], Algorithm 1 can first be used to convolve ''h''[''n''] with just one period of ''x''[''n'']. &nbsp;In the region ''M'' ≤ ''n'' ≤ ''N''<sub>''x''</sub>, &nbsp;the resultant ''y''[''n''] sequence is correct. &nbsp;And if the next ''M''&nbsp;&minus;&nbsp;1 values are added to the first ''M''&nbsp;&minus;&nbsp;1 values, then the region 1 ≤ ''n'' ≤ ''N''<sub>''x''</sub> will represent the desired convolution.  The modified pseudocode is''':'''\n\n    '''Algorithm 2''' (''OA for circular convolution'')\n    Evaluate Algorithm 1\n    y(1:M-1) = y(1:M-1) + y(Nx+1:Nx+M-1)\n    y = y(1:Nx)\n    '''end'''\n\n== Cost of the overlap-add method ==\n\nThe cost of the convolution can be associated to the number of complex\nmultiplications involved in the operation. The major computational\neffort is due to the FFT operation, which for a radix-2 algorithm\napplied to a signal of length <math>N</math> roughly calls for <math>C=\\frac{N}{2}\\log_2 N</math>\ncomplex multiplications. It turns out that the number of complex multiplications\nof the overlap-add method are:\n\n:<math>C_{OA}=\\left\\lceil \\frac{N_x}{N-M+1}\\right\\rceil\nN\\left(\\log_2 N+1\\right)\\,</math>\n\n<math>C_{OA}</math> accounts for the FFT+filter multiplication+IFFT operation.\n\nThe additional cost of the <math>M_L</math> sections involved in the circular\nversion of the overlap–add method is usually very small and can be\nneglected for the sake of simplicity. The best value of <math>N</math>\ncan be found by numerical search of the minimum of <math>C_{OA}\\left(N\\right)=C_{OA}\\left(2^m \\right)</math>\nby spanning the integer <math>m</math> in the range <math>\\log_2\\left(M\\right)\\le m\\le\\log_2 \\left(N_x\\right)</math>.\nBeing <math>N</math> a power of two, the FFTs of the overlap–add method\nare computed efficiently. Once evaluated the value of <math>N</math> it\nturns out that the optimal partitioning of <math>x[n]</math> has <math>L=N-M+1</math>.\nFor comparison, the cost of the standard circular convolution of <math>x[n]</math>\nand <math>h[n]</math> is:\n\n:<math>C_S=N_x\\left(\\log_2 N_x+1\\right)\\,</math>\n\nHence the cost of the overlap–add method scales almost as <math>O\\left(N_x\\log_2 N\\right)</math>\nwhile the cost of the standard circular convolution method is almost\n<math>O\\left(N_x\\log_2 N_x \\right)</math>. However such functions accounts\nonly for the cost of the complex multiplications, regardless of the\nother operations involved in the algorithm. A direct measure of the\ncomputational time required by the algorithms is of much interest.\nFig. 2 shows the ratio of the measured time to evaluate\na standard circular convolution using &nbsp;{{EquationNote|Eq.1}} with\nthe time elapsed by the same convolution using the overlap–add method\nin the form of Alg 2, vs. the sequence and the filter length. Both algorithms have been implemented under [[Matlab]]. The\nbold line represent the boundary of the region where the overlap–add\nmethod is faster (ratio>1) than the standard circular convolution.\nNote that the overlap–add method in the tested cases can be three\ntimes faster than the standard method.\n\n[[Image:gain oa method.png|frame|none|Figure 2: Ratio between the time required by &nbsp;{{EquationNote|Eq.1}} and the time required by the overlap–add Alg. 2 to evaluate\na complex circular convolution, vs the sequence length <math>N_x</math> and\nthe filter length <math>M</math>.]]\n\n== See also ==\n\n*[[Overlap–save method]]\n\n== References ==\n\n*{{Cite book\n |author1=Rabiner, Lawrence R. |author2=Gold, Bernard | title=Theory and application of digital signal processing\n | year=1975\n | publisher=Prentice-Hall\n | location=Englewood Cliffs, N.J. \n | isbn=0-13-914101-4\n | pages=63–67\n}}\n*{{Cite book\n |author1=Oppenheim, Alan V. |author2=Schafer, Ronald W. | title=Digital signal processing \n | year=1975 \n | publisher=Prentice-Hall \n | location=Englewood Cliffs, N.J.  \n | isbn=0-13-214635-5 \n | pages=\n}}\n*{{Cite book\n | author=Hayes, M. Horace \n | title = Digital Signal Processing\n | series = Schaum's Outline Series\n | year=1999 \n | publisher=McGraw Hill \n | location=New York  \n | isbn=0-07-027389-8 \n | pages=\n}}\n\n== External links ==\n\n{{DEFAULTSORT:Overlap-Add Method}}\n[[Category:Signal processing]]\n[[Category:Transforms]]\n[[Category:Fourier analysis]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Overlap–save method",
      "url": "https://en.wikipedia.org/wiki/Overlap%E2%80%93save_method",
      "text": "'''Overlap–save''' is the traditional name for an efficient way to evaluate the [[Convolution#Discrete convolution|discrete convolution]] between a very long signal <math>x[n]</math> and a [[finite impulse response]] (FIR) filter <math>h[n]</math>''':'''\n\n{{NumBlk|:|<math>y[n] = x[n] * h[n] \\ \\triangleq \\ \\sum_{m=-\\infty}^{\\infty} h[m] \\cdot x[n-m] = \\sum_{m=1}^{M} h[m] \\cdot x[n-m],\\,</math>|{{EquationRef|Eq.1}}}}\n\nwhere h[m]=0 for m outside the region [1, ''M''].\n\n[[Image:Overlap-save algorithm.png|thumb|500px|A sequence of 4 plots depicts one cycle of the overlap–save convolution algorithm. The 1st plot is a long sequence of data to be processed with a lowpass FIR filter. The 2nd plot is one segment of the data to be processed in piecewise fashion. The 3rd plot is the filtered segment, with the usable portion colored red. The 4th plot shows the filtered segment appended to the output stream. The FIR filter is a boxcar lowpass with M=16 samples, the length of the segments is L=100 samples and the overlap is 15 samples.]]\nThe concept is to compute short segments of ''y''[''n''] of an arbitrary length ''L'', and concatenate the segments together.  Consider a segment that begins at ''n'' = ''kL''&nbsp;+&nbsp;''M'', for any integer ''k'', and define''':'''\n\n:<math>x_k[n]  \\ \\triangleq\n\\begin{cases}\nx[n+kL] & 1 \\le n \\le L+M-1\\\\\n0 & \\textrm{otherwise}.\n\\end{cases}\n</math>\n\n:<math>y_k[n] \\ \\triangleq \\ x_k[n]*h[n]  = \\sum_{m=1}^{M} h[m] \\cdot x_k[n-m].</math>\n\nThen, for ''kL''&nbsp;+&nbsp;''M'' &nbsp;≤&nbsp; ''n'' &nbsp;≤&nbsp; ''kL''&nbsp;+&nbsp;''L''&nbsp;+&nbsp;''M''&nbsp;−&nbsp;1, and equivalently ''M'' &nbsp;≤&nbsp; ''n''&nbsp;−&nbsp;''kL'' &nbsp;≤&nbsp; ''L''&nbsp;+&nbsp;''M''&nbsp;−&nbsp;1, we can write''':'''\n\n:<math>y[n] = \\sum_{m=1}^{M} h[m] \\cdot x_k[n-kL-m] \\ \\ \\triangleq \\ \\ y_k[n-kL].</math>\n\nThe task is thereby reduced to computing ''y''<sub>''k''</sub>[''n''], for ''M'' &nbsp;≤&nbsp; ''n'' &nbsp;≤&nbsp; ''L''&nbsp;+''&nbsp;M''&nbsp;−&nbsp;1.  The process described above is illustrated in the accompanying figure.\n\nNow note that if we periodically extend ''x''<sub>''k''</sub>[''n''] with period ''N'' &nbsp;≥&nbsp; ''L''&nbsp;+&nbsp;''M''&nbsp;−&nbsp;1, according to''':'''\n\n:<math>x_{k,N}[n] \\ \\triangleq \\ \\sum_{\\ell=-\\infty}^{\\infty} x_k[n - \\ell N],</math>\n\nthe convolutions &nbsp;<math>(x_{k,N})*h\\,</math>&nbsp; and &nbsp;<math>x_k*h\\,</math>&nbsp; are equivalent in the region ''M'' &nbsp;≤&nbsp; ''n'' &nbsp;≤&nbsp; ''L''&nbsp;+&nbsp;''M''&nbsp;−&nbsp;1.  So it is sufficient to compute the '''N'''-point [[circular convolution|circular (or cyclic) convolution]] of <math>x_k[n]\\,</math> with <math>h[n]\\,</math>&nbsp; in the region [1,&nbsp;''N'']. &nbsp;The subregion [''M'',&nbsp;''L''&nbsp;+&nbsp;''M''&nbsp;−&nbsp;1] is appended to the output stream, and the other values are <u>discarded</u>.\n\nThe advantage is that the circular convolution can be computed very efficiently as follows, according to the [[Discrete Fourier transform#Circular convolution theorem and cross-correlation theorem|circular convolution theorem]]''':'''\n\n:<math>y_k[n] = \\scriptstyle \\text{DFT}^{-1} \\displaystyle  (\\ \\scriptstyle \\text{DFT} \\displaystyle (x_k[n])\\cdot \\scriptstyle \\text{DFT} \\displaystyle (h[n])\\ ),</math>\n\nwhere''':'''\n*DFT and DFT<sup>−1</sup> refer to the Discrete Fourier transform and inverse Discrete Fourier transform, respectively, evaluated over ''N'' discrete points, and\n*''N'' is customarily chosen to be an integer power-of-2, which optimizes the efficiency of the [[Fast Fourier transform|FFT]] algorithm.\n*Optimal N is in the range [4M, 8M].\n*Unlike the third graph in the figure above, depicting separate leading and trailing edge-effects, this method causes them to be overlapped and added.  So they are discarded together.  In other words, with circular convolution, the first output value is a weighted average of the <u>last</u> M-1 samples of the input segment (and the first sample of the segment).  The next M-2 outputs are weighted averages of both the beginning and the end of the segment.  The M<sup>th</sup> output value is the first one that combines only samples from the beginning of the segment.\n\n==Pseudocode==\n<code>\n  <span style=\"color:green;\">(''Overlap–save algorithm for linear convolution'')</span>\n  h = FIR_impulse_response\n  M = length(h)\n  overlap = M-1\n  N = 4*overlap    <span style=\"color:green;\">(or a nearby power-of-2)</span>\n  step_size = N-overlap\n  H = DFT(h, N)\n  position = 0\n  '''while''' position+N <= length(x)\n      yt = IDFT( DFT( x(1+position : N+position), N ) * H, N )\n      y(1+position : step_size+position) = yt(M : N)    #discard M-1 y-values\n      position = position + step_size\n  '''end'''\n</code>\n\n==Efficiency==\nWhen the DFT and its inverse is implemented by the FFT algorithm, the pseudocode above requires about '''N log<sub>2</sub>(N) + N''' complex multiplications for the FFT, product of arrays, and IFFT.<ref group=note>Cooley–Tukey FFT algorithm for N=2<sup>k</sup> needs (N/2) log<sub>2</sub>(N) – see [[Fast Fourier transform#Definition and speed|FFT – Definition and speed]]</ref>  Each iteration produces '''N-M+1''' output samples, so the number of complex multiplications per output sample is about''':'''\n\n{{NumBlk|:|<math>\\frac{N \\log_2(N) + N}{N-M+1}.\\,</math>|{{EquationRef|Eq.2}}}}\n\nFor example, when '''M'''=201 and '''N'''=1024, {{EquationNote|Eq.2}} equals 13.67, whereas direct evaluation of {{EquationNote|Eq.1}} would require up to 201 complex multiplications per output sample, the worst case being when both '''x''' and '''h''' are complex-valued.  Also note that for any given '''M''', {{EquationNote|Eq.2}} has a minimum with respect to '''N'''.  It diverges for both small and large block sizes.\n\n==Overlap–discard==\n''Overlap–discard''<ref>Harris, F.J. (1987). \"Time domain signal processing with the DFT\". ''Handbook of Digital Signal Processing'', D.F.Elliot, ed., San Diego: Academic Press. pp 633–699. {{ISBN|0122370759}}.</ref> and ''Overlap–scrap''<ref>Frerking, Marvin (1994). ''Digital Signal Processing in Communication Systems''. New York: Van Nostrand Reinhold. {{ISBN|0442016166}}.</ref> are less commonly used labels for the same method described here.  However, these labels are actually better (than ''overlap–save'') to distinguish from [[Overlap–add method|overlap–add]], because <u>both</u> methods \"save\", but only one discards.  \"Save\" merely refers to the fact that ''M''&nbsp;−&nbsp;1 input (or output) samples from segment ''k'' are needed to process segment ''k'' + 1.\n\n===Extending overlap–save===\nThe overlap–save algorithm may be extended to include other common operations of a system:<ref group=note>[[#refCarlin|Carlin et al. 1999]], p 31, col 20.</ref><ref>{{Citation |last=Borgerding |first=Mark |title=Turning Overlap–Save into a Multiband Mixing, Downsampling Filter Bank |journal=IEEE Signal Processing Magazine |issue= March 2006 |pages=158–161 |year=2006 |url=http://www.3db-labs.com/01598092_MultibandFilterbank.pdf}}</ref>\n\n* additional channels can be processed more cheaply than the first by reusing the forward FFT\n* sampling rates can be changed by using different sized forward and inverse FFTs\n* frequency translation (mixing) can be accomplished by rearranging frequency bins\n\n== See also ==\n*[[Overlap–add method]]\n\n==Notes==\n{{reflist|group=note}}\n\n==References==\n{{reflist}}\n{{refbegin}}\n#<li value=\"4\">{{cite patent\n | ref=refCarlin\n | inventor-last =Carlin\n | inventor-first =Joe \n | inventor2-last =Collins\n | inventor2-first =Terry \n | inventor3-last =Hays\n | inventor3-first =Peter \n | inventor4-last =Hemmerdinger\n | inventor4-first =Barry \n | inventor5-last =Kellogg\n | inventor5-first =Robert \n | inventor6-last =Kettig\n | inventor6-first =Robert \n | inventor7-last =Lemmon\n | inventor7-first =Bradley \n | inventor8-last =Murdock\n | inventor8-first =Thomas \n | inventor9-last =Tamaru\n | inventor9-first =Robert \n | inventor10-last =Ware\n | inventor10-first =Stuart \n | publication-date = 1999\n | issue-date = 2005\n | title = Wideband communication intercept and direction finding device using hyperchannelization \n | country-code = US\n | description = patent \n | patent-number = 6898235\n}}</li>\n\n==Further reading==\n*Rabiner, Lawrence R.; Gold, Bernard (1975). ''Theory and application of digital signal processing''. Englewood Cliffs, N.J.: Prentice-Hall. pp 65–67. {{ISBN|0139141014}}.\n{{refend}}\n\n{{DEFAULTSORT:Overlap-save method}}\n[[Category:Signal processing]]\n[[Category:Transforms]]\n[[Category:Fourier analysis]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Padé approximant",
      "url": "https://en.wikipedia.org/wiki/Pad%C3%A9_approximant",
      "text": "[[File:Henri Padé.jpeg|thumb|right|[[Henri Padé]].]] \nIn [[mathematics]] a '''Padé approximant''' is the 'best' approximation of a function by a [[rational function]] of given order – under this technique, the approximant's [[power series]] agrees with the power series of the function it is approximating. <!-- [http://www.dattalo.com/technical/theory/sinewave.html]-->The technique was developed around 1890 by [[Henri Padé]], but goes back to [[Georg Frobenius]] who introduced the idea and investigated the features of rational approximations of power series.\n\nThe Padé approximant often gives better approximation of the function than truncating its [[Taylor series]], and it may still work where the Taylor series does not [[Convergent series|converge]].  For these reasons Padé approximants are used extensively in computer [[calculation]]s. They have also been used as [[auxiliary function]]s, in [[Diophantine approximation]] and [[transcendental number theory]], though for sharp results ''ad hoc'' methods, in some sense inspired by the Padé theory, typically replace them.\n\n==Definition==\nGiven a function ''f'' and two [[integer]]s ''m'' ≥ 0 and ''n'' ≥ 1, the ''Padé approximant'' of order [''m''/''n''] is the rational function\n\n<!--:<math>R(x)= \\frac{\\sum_{j=0}^m p_j x^j}{1+\\sum_{k=1}^n q_k x^k}=\\frac{p_0+p_1x+p_2x^2+\\cdots+p_mx^m}{1+q_1 x+q_2x^2+\\cdots+q_nx^n}</math>-->\n:<math>R(x)= \\frac{\\sum_{j=0}^m a_j x^j}{1+\\sum_{k=1}^{n}b_k x^k}=\\frac{a_0+a_1x+a_2x^2+\\cdots+a_mx^m}{1+b_1 x+b_2x^2+\\cdots+b_nx^n}</math>\n\nwhich agrees with ''f''(''x'') to the highest possible order, which amounts to\n:<math>\\begin{array}{rcl}\n        f(0)&=&R(0)\\\\\n       f'(0)&=&R'(0)\\\\\n      f''(0)&=&R''(0)\\\\\n            &\\vdots& \\\\\nf^{(m+n)}(0)&=&R^{(m+n)}(0)\\end{array}.</math>\n\nEquivalently, if ''R''(''x'') is expanded in a Maclaurin series ([[Taylor series]] at 0), its first ''m'' + ''n'' terms would cancel the first ''m'' + ''n'' terms of ''f''(''x''), and as such:\n\n:<math>f(x)-R(x) = c_{m+n+1}x^{m+n+1}+c_{m+n+2}x^{m+n+2}+\\cdots</math>\n\nThe Padé approximant is unique for given ''m'' and ''n'', that is, the coefficients <!--<math>p_0, p_1, \\dots, p_m, q_1, \\dots, q_n</math>--> <math>a_0, a_1, \\dots, a_m, b_1, \\dots, b_n</math> can be uniquely determined. It is for reasons of uniqueness that the zero-th order term at the denominator of ''R''(''x'') was chosen to be 1, otherwise the numerator and denominator of ''R''(''x'') would have been unique only [[up to]] multiplication by a constant.\n\nThe Padé approximant defined above is also denoted as\n\n:<math>[m/n]_f(x). \\,</math>\n\n==Computation==\nFor given ''x'', Padé approximants can be computed by [[Peter Wynn (mathematician)|Wynn]]'s epsilon algorithm<ref>Theorem 1 in {{citation|title=On the Convergence and Stability of the Epsilon Algorithm| first=Peter |last=Wynn| journal=SIAM Journal on Numerical Analysis|volume=3|date=Mar 1966| pages=91–122| jstor=2949688| issue=1|bibcode=1966SJNA....3...91W|doi=10.1137/0703007}}</ref> and also other [[sequence transformation]]s<ref>{{citation|title=Extrapolation algorithms and Padé approximations|first=C.|last=Brezenski|journal=Applied Numerical Mathematics|volume=20|year=1996|pages=299–318|doi=10.1016/0168-9274(95)00110-7|issue=3|citeseerx=10.1.1.20.9528}}</ref> from the partial sums\n\n:<math>T_N(x)=c_0 + c_1 x + c_2 x^2 + \\cdots + c_N x^N </math>\n\nof the [[Taylor series]] of ''f'', i.e., we have\n\n:<math>c_k = \\frac{f^{(k)}(0)}{k!}.</math>\n\n''f'' can also be a [[formal power series]], and, hence, Padé approximants can also be applied to the summation of [[divergent series]].\n\nOne way to compute a Padé approximant is via the [[extended Euclidean algorithm]] for the [[polynomial greatest common divisor]].<ref>Problem 5.2b and Algorithm 5.2 (p. 46) in {{citation|title=Polynomial and Matrix computations - Volume 1. Fundamental Algorithms|first1=Dario | last1=Bini| first2=Victor | last2=Pan|publisher=Birkhäuser| series=Progress in Theoretical Computer Science| year=1994| isbn=978-0-8176-3786-6}}</ref> The relation\n\n:<math>R(x)=P(x)/Q(x)=T_{m+n}(x) \\text{ mod }x^{m+n+1}</math>\n\nis equivalent to the existence of some factor ''K''(''x'') such that\n\n:<math>P(x)=Q(x)T_{m+n}(x)+K(x)x^{m+n+1},</math>\n\nwhich can be interpreted as the [[Bézout's identity|Bézout identity]] of one step in the computation of the extended gcd of the polynomials <math>T_{m+n}(x)</math> and <math>x^{m+n+1}</math>.\n\nTo recapitulate: to compute the gcd of two polynomials ''p'' and ''q'', one computes via long division the remainder sequence\n\n:<math>r_0=p,\\;r_1=q,\\quad r_{k-1}=q_kr_k+r_{k+1},</math>\n\n''k'' = 1, 2, 3, ... with <math>\\deg r_{k+1}<\\deg r_k\\,</math>, until <math>r_{k+1}=0</math>. For the Bézout identities of the extended gcd one computes simultaneously the two polynomial sequences\n\n:<math>u_0=1,\\;v_0=0,\\quad u_1=0,\\;v_1=1,\\quad u_{k+1}=u_{k-1}-q_ku_k,\\;v_{k+1}=v_{k-1}-q_kv_k</math>\n\nto obtain in each step the Bézout identity\n\n:<math>r_k(x)=u_k(x)p(x)+v_k(x)q(x).</math>\n\nFor the [''m''/''n''] approximant, one thus carries out the extended euclidean algorithm for\n\n:<math>r_0=x^{m+n+1},\\;r_1=T_{m+n}(x)</math>\n\nand stops it at the last instant that <math>v_k</math> has degree ''n'' or smaller.\n\nThen the polynomials <math>P=r_k,\\;Q=v_k</math> give the [''m''/''n''] Padé approximant. If one were to compute all steps of the extended gcd computation, one would obtain an anti-diagonal of the [[Pade table]].\n\n==Riemann–Padé zeta function==\nTo study the resummation of a [[divergent series]], say\n\n:<math> \\sum_{z=1}^\\infty f(z), </math>\n \nit can be useful to introduce the Padé or simply rational zeta function as\n\n:<math> \\zeta_R(s) = \\sum_{z=1}^\\infty \\frac{R(z)}{z^s},  </math>\n\nwhere\n\n:<math> R(x) = [m/n]_f(x)\\,</math>\n\nis the Padé approximation of order (''m'', ''n'') of the function ''f''(''x''). The [[zeta regularization]] value at ''s'' = 0 is taken to be the sum of the divergent series.\n\nThe functional equation for this Padé zeta function is\n\n:<math> \\sum_{j=0}^n a_j \\zeta_R(s-j)= \\sum_{j=0}^m b_j \\zeta_0(s-j), </math>\n\nwhere ''a<sub>j</sub>'' and ''b<sub>j</sub>'' are the coefficients in the Padé approximation. The subscript '0' means that the Padé is of order [0/0] and hence, we have the Riemann zeta function.\n\n==DLog Padé method==\nPadé approximants can be used to extract critical points and exponents of functions. In thermodynamics, if a function ''f''(''x'') behaves in a non-analytic way near a point ''x'' = ''r'' like <math>f(x)\\sim |x-r|^p</math>, one calls ''x'' = ''r'' a critical point and ''p'' the associated critical exponent of ''f''. If sufficient terms of the series expansion of ''f'' are known, one can approximately extract the critical points and the critical exponents from respectively the poles and residues of the Padé approximants <math>[n/n+1]_g(x)</math> where <math>g=\\frac{f'}{f}</math>.\n\n==Generalizations==\nA Padé approximant approximates a function in one variable.  An approximant in two variables is called a Chisholm approximant (after [[J. S. R. Chisholm]])<ref>{{Cite journal|last=Chisholm|first=J. S. R.|date=1973|title=Rational approximants defined from double power series|url=http://www.ams.org/mcom/1973-27-124/S0025-5718-1973-0382928-6/|journal=Mathematics of Computation|volume=27|issue=124|pages=841–848|doi=10.1090/S0025-5718-1973-0382928-6|issn=0025-5718}}</ref>, in multiple variables a Canterbury approximant (after Graves-Morris at the University of Kent)<ref>{{Cite journal|last1=Graves-Morris|first1=P.R.|last2=Roberts|first2=D.E.|date=1975|title=Calculation of Canterbury approximants|url=https://doi.org/10.1016/0010-4655(75)90068-5|journal=Computer Physics Communications|volume=10|issue=4|pages=234–244|doi=10.1016/0010-4655(75)90068-5}}</ref>.\n\n==Examples==\n{{Unreferenced section|date=September 2018}}\n;sin(''x''):\n:<math>\\sin(x) \\approx \\frac{ (12671/4363920)x^5-(2363/18183)x^3+x }{ 1+(445/12122)x^2+(601/872784)x^4+(121/16662240)x^6}</math>\n\n;exp(''x''):\n: <math>\\exp(x) \\approx \\frac{1+(1/2)x+(1/9)x^2+(1/72)x^3+(1/1008)x^4+(1/30240)x^5}{1-(1/2)x+(1/9)x^2-(1/72)x^3+(1/1008)x^4-(1/30240)x^5   }</math>\n\n;Jacobi SN(''z'', 3):\n:<math>\\mathrm{sn}(z|3) \\approx \\frac{ -(9851629/283609260)z^5-(572744/4726821)z^3+z }{ 1+(859490/1575607)z^2-(5922035/56721852)z^4+(62531591/2977897230)z^6  }</math>\n\n;Bessel ''J''(5, ''x''):\n:<math>J_5(x) \\approx \\frac{-(107/28416000)x^7+(1/3840)x^5  }{ 1+(151/5550)x^2+(1453/3729600)x^4+(1339/358041600)x^6+(2767/120301977600)x^8  }</math>\n\n;erf(''x''):\n: <math>\\operatorname{erf}(x) \\approx \\frac{ (2/15)\\cdot (49140x+3570x^3+739x^5) }{ \\sqrt{\\pi} \\cdot (165x^4+1330x^2+3276) }</math>\n\n;Fresnel ''C''(''x''):\n:<math>C(x) \\approx \\frac{ (1/135)\\cdot (990791x^9\\pi^4-147189744x^5\\pi^2+8714684160x)}{(1749\\pi^4x^8+523536\\pi^2x^4+64553216) }</math>\n\n==See also==\n*[[Padé table]]\n\n==References==\n<references />\n\n==Literature==\n* Baker, G. A., Jr.; and Graves-Morris, P. '' Padé Approximants''.  Cambridge U.P., 1996\n* Baker, G. A., Jr. [http://www.scholarpedia.org/article/Pad%C3%A9_approximant Padé approximant], [http://www.scholarpedia.org/ Scholarpedia], 7(6):9756.\n* Brezinski, C.; and Redivo Zaglia, M. ''Extrapolation Methods. Theory and Practice''.  North-Holland, 1991\n* {{Citation | last1=Press | first1=WH | last2=Teukolsky | first2=SA | last3=Vetterling | first3=WT | last4=Flannery | first4=BP | year=2007 | title=Numerical Recipes: The Art of Scientific Computing | edition=3rd | publisher=Cambridge University Press |  location=New York | isbn=978-0-521-88068-8 | chapter=Section 5.12 Padé Approximants | chapter-url=http://apps.nrbook.com/empanel/index.html?pg=245}}\n* Frobenius, G.; ''Ueber Relationen zwischen den Näherungsbrüchen von Potenzreihen'', [Journal für die reine und angewandte Mathematik (Crelle's Journal)]. Volume 1881, Issue 90, Pages 1–17\n* Gragg, W.B.; ''The Pade Table and Its Relation to Certain Algorithms of Numerical Analysis'' [SIAM Review], Vol. 14, No. 1, 1972, pp.&nbsp;1–62.\n* Padé, H.; ''Sur la répresentation approchée d'une fonction par des fractions rationelles'', Thesis, [Ann. \\'Ecole Nor. (3), 9, 1892, pp.&nbsp;1–93 supplement. \n<!--[http://www.nrbook.com/a/bookcpdf/c5-12.pdf available online]-->\n* {{Citation |first1=P. |last1=Wynn |authorlink=Peter Wynn (mathematician) |journal=Numerische Mathematik |title=Upon systems of recursions which obtain among the quotients of the Padé table |volume=8 |issue=3 |pages=264–269 |doi=10.1007/BF02162562 |year=1966}}\n\n==External links==\n* {{mathworld|urlname=PadeApproximant|title= Padé Approximant}}\n* [http://demonstrations.wolfram.com/PadeApproximants/ Padé Approximants], Oleksandr Pavlyk, [[The Wolfram Demonstrations Project]]\n* [https://web.archive.org/web/20071213000637/http://rkb.home.cern.ch/rkb/AN16pp/node203.html#SECTION0002030000000000000000 Data Analysis BriefBook: Pade Approximation], Rudolf K. Bock  [[European Laboratory for Particle Physics]], [[CERN]]\n* [https://web.archive.org/web/20100220110347/http://www.dattalo.com/technical/theory/sinewave.html Sinewave], Scott Dattalo, last accessed 2010-11-11.\n* [http://www.mathworks.com/help/toolbox/control/ref/pade.html MATLAB function] for Pade approximation of models with time delays.\n\n{{DEFAULTSORT:Pade approximant}}\n[[Category:Continued fractions]]\n[[Category:Numerical analysis]]\n[[Category:Rational functions]]"
    },
    {
      "title": "Padé table",
      "url": "https://en.wikipedia.org/wiki/Pad%C3%A9_table",
      "text": "[[File:Henri Padé.jpeg|thumb|right|[[Henri Padé]].]]\n\nIn [[complex analysis]], a '''Padé table''' is an array, possibly of infinite extent, of the rational [[Padé approximant]]s\n\n:''R''<sub>''m'', ''n''</sub>\n\nto a given complex [[formal power series]]. Certain sequences of approximants lying within a Padé table can often be shown to correspond with successive [[Convergent (continued fraction)|convergents]] of a [[generalized continued fraction|continued fraction]] representation of a [[Holomorphic function|holomorphic]] or [[meromorphic]] function.\n\n==History==\nAlthough earlier mathematicians had obtained sporadic results involving sequences of rational approximations to [[transcendental function]]s, [[Ferdinand Georg Frobenius|Frobenius]] (in 1881) was apparently the first to organize the approximants in the form of a table. [[Henri Padé]] further expanded this notion in his doctoral thesis ''Sur la representation approchee d'une fonction par des fractions rationelles'', in 1892. Over the ensuing 16 years Padé published 28 additional papers exploring the properties of his table, and relating the table to analytic continued fractions.<ref>{{MacTutor Biography|id=Pade}}</ref>\n\nModern interest in Padé tables was revived by [[Hubert Stanley Wall|H. S. Wall]] and [[Oskar Perron]], who were primarily interested in the connections between the tables and certain classes of continued fractions. [[Daniel Shanks]] and [[Peter Wynn (mathematician)|Peter Wynn]] published influential papers about 1955, and [[William Bryant Gragg|W. B. Gragg]] obtained far-reaching convergence results during the '70s. More recently, the widespread use of electronic computers has stimulated a great deal of additional interest in the subject.<ref name=\"JT\">Jones and Thron, 1980.</ref>\n\n==Notation==\nA function ''f''(''z'') is represented by a formal power series:\n\n:<math>\nf(z) = c_0 + c_1 z + c_2 z^2 + \\cdots = \\sum_{l=0}^\\infty c_l z^l,\n</math>\n\nwhere ''c''<sub>0</sub> &ne; 0, by convention. The (''m'', ''n'')th entry<ref>The (''m'', ''n'')th entry is considered to lie in row ''m'' and column ''n'', and numbering of the rows and columns begins at (0,&nbsp;0).</ref> ''R<sub>m, n</sub>'' in the Padé table for ''f''(''z'') is then given by\n\n:<math>\nR_{m,n}(z) = \\frac{P_m(z)}{Q_n(z)} = \n\\frac{a_0 + a_1 z + a_2 z^2 + \\cdots + a_m z^m}{b_0 + b_1 z + b_2 z^2 + \\cdots + b_n z^n}\n</math>\n\nwhere ''P<sub>m</sub>''(''z'') and ''Q<sub>n</sub>''(''z'') are polynomials of degrees not more than ''m'' and ''n'', respectively. The coefficients {''a<sub>i</sub>''} and {''b<sub>i</sub>''} can always be found by considering the expression\n\n:<math>\nf(z) \\approx \\sum_{l=0}^{m+n} c_l z^l =: f_{apx}(z)\n</math>\n\n:<math>\nQ_n(z) f_{apx}(z) = P_m(z)\n</math>\n\n:<math>\nQ_n(z) \\left(c_0 + c_1 z + c_2 z^2 + \\cdots + c_{m+n} z^{m+n} \\right) = P_m(z)\n</math>\n\nand equating coefficients of like powers of ''z'' up through ''m''&nbsp;+&nbsp;''n''. For the coefficients of powers ''m''&nbsp;+&nbsp;1 to ''m''&nbsp;+&nbsp;''n'', the right hand side is 0 and the resulting [[system of linear equations]] contains a homogeneous system of ''n'' equations in the ''n''&nbsp;+&nbsp;1 unknowns ''b<sub>i</sub>'', and so admits of infinitely many solutions each of which determines a possible ''Q<sub>n</sub>''. ''P<sub>m</sub>'' is then easily found by equating the first ''m'' coefficients of the equation above. However, it can be shown that, due to cancellation, the generated rational functions ''R<sub>m,&nbsp;n</sub>'' are all the same, so that the (''m'',&nbsp;''n'')th entry in the Padé table is unique.<ref name=\"JT\"/> Alternatively, we may require that ''b''<sub>0</sub>&nbsp;=&nbsp;1, thus putting the table in a standard form.\n\nAlthough the entries in the Padé table can always be generated by solving this system of equations, that approach is computationally expensive. Usage of the Padé table has been extended to meromorphic functions by newer, timesaving methods such as the epsilon algorithm.<ref>{{cite journal|doi = 10.2307/2002183|jstor = 2002183|publisher = American Mathematical Society|pages = 91–96|date = Apr 1956|title = On a Device for Computing the ''e<sub>m</sub>''(''S<sub>n</sub>'') Transformation|first = Peter|journal = Mathematical Tables and Other Aids to Computation|volume = 10|issue = 54|last = Wynn}}</ref>\n\n==The block theorem and normal approximants==\nBecause of the way the (''m'', ''n'')th approximant is constructed, the difference\n\n:''Q<sub>n</sub>''(''z'')''f''(''z'')&nbsp;&minus;&nbsp;''P<sub>m</sub>''(''z'')\n\nis a power series whose first term is of degree no less than\n\n:''m''&nbsp;+&nbsp;''n''&nbsp;+&nbsp;1.\n\nIf the first term of that difference is of degree\n\n:''m''&nbsp;+&nbsp;''n''&nbsp;+&nbsp;''r''&nbsp;+&nbsp;1, ''r''&nbsp;>&nbsp;0,\n\nthen the rational function ''R<sub>m,&nbsp;n</sub>'' occupies\n\n:(''r'' + 1)<sup>2</sup>\n\ncells in the Padé table, from position (''m'',&nbsp;''n'') through position (''m''+''r'',&nbsp;''n''+''r''), inclusive. In other words, if the same rational function appears more than once in the table, that rational function occupies a square block of cells within the table. This result is known as the '''block theorem'''.\n\nIf a particular rational function occurs exactly once in the Padé table, it is called a '''normal''' approximant to ''f''(''z''). If every entry in the complete Padé table is normal, the table itself is said to be normal. Normal Padé approximants can be characterized using [[determinant]]s of the coefficients ''c<sub>n</sub>'' in the Taylor series expansion of ''f''(''z''), as follows. Define the (''m'',&nbsp;''n'')th determinant by\n\n:<math>D_{m,n} = \\left|\\begin{matrix}\nc_m & c_{m-1} & \\ldots & c_{m-n+2} & c_{m-n+1}\\\\\nc_{m+1} & c_m & \\ldots & c_{m-n+3} & c_{m-n+2}\\\\\n\\vdots & \\vdots & & \\vdots & \\vdots\\\\\nc_{m+n-2} & c_{m+n-3} & \\ldots & c_m & c_{m-1}\\\\\nc_{m+n-1} & c_{m+n-2} & \\ldots & c_{m+1} & c_m\\\\\n\\end{matrix}\\right|\n</math>\n\nwith ''D''<sub>''m'',0</sub> = 1, ''D''<sub>''m'',1</sub> = ''c<sub>m</sub>'', and ''c<sub>k</sub>''&nbsp;=&nbsp;0 for ''k''&nbsp;<&nbsp;0. Then\n* the (''m'', ''n'')th approximant to ''f''(''z'') is normal if and only if none of the four determinants ''D''<sub>''m'',''n''&minus;1</sub>, ''D<sub>m,n</sub>'', ''D''<sub>''m''+1,''n''</sub>, and ''D''<sub>''m''+1,''n''+1</sub> vanish; and\n* the Padé table is normal if and only if none of the determinants ''D<sub>m,n</sub>'' are equal to zero (note in particular that this means none of the coefficients ''c<sub>k</sub>'' in the series representation of ''f''(''z'') can be zero).<ref>{{cite journal|pages = 1–62|doi = 10.1137/1014001|last = Gragg|date = Jan 1972|first = W.B.|title = The Padé Table and its Relation to Certain Algorithms of Numerical Analysis|journal = SIAM Review|issue = 1|volume = 14|issn = 0036-1445|jstor=2028911}}</ref>\n\n==Connection with continued fractions==\nOne of the most important forms in which an analytic continued fraction can appear is as a regular [[C-fraction]], which is a continued fraction of the form\n\n:<math>\nf(z) = b_0 + \\cfrac{a_1z}{1 - \\cfrac{a_2z}{1 - \\cfrac{a_3z}{1 - \\cfrac{a_4z}{1 - \\ddots}}}}.\n</math>\n\nwhere the ''a<sub>i</sub>'' &ne; 0 are complex constants, and ''z'' is a complex variable.\n\nThere is an intimate connection between regular C-fractions and Padé tables with normal approximants along the main diagonal: the \"stairstep\" sequence of Padé approximants ''R''<sub>0,0</sub>, ''R''<sub>1,0</sub>, ''R''<sub>1,1</sub>, ''R''<sub>2,1</sub>, ''R''<sub>2,2</sub>, &hellip; is normal if and only if that sequence coincides with the successive [[Convergent (continued fraction)|convergents]] of a regular C-fraction. In other words, if the Padé table is normal along the main diagonal, it can be used to construct a regular C-fraction, and if a regular C-fraction representation for the function ''f''(''z'') exists, then the main diagonal of the Padé table representing ''f''(''z'') is normal.<ref name=\"JT\"/>\n\n==An example – the exponential function<!--linked from 'Exponential function'-->==\nHere is an example of a Padé table, for the [[exponential function]].\n\n{| class = \"wikitable\" style=\"text-align: center;\"\n|+A portion of the Padé table for the exponential function ''e<sup>z</sup>''\n|-\n! {{diagonal split header|''m''|''n''}} !! 0 !! 1 !! 2 !! 3\n|-\n! 0\n| <math>\\frac{1}{1}</math>\n||<math>\\frac{1}{1 - z}</math>\n||<math>\\frac{1}{1 - z + {\\scriptstyle\\frac{1}{2}}z^2}</math>\n||<math>\\frac{1}{1 - z + {\\scriptstyle\\frac{1}{2}}z^2 - {\\scriptstyle\\frac{1}{6}}z^3}</math>\n|-\n! 1\n| <math>\\frac{1 + z}{1}</math> \n||<math>\\frac{1 + {\\scriptstyle\\frac{1}{2}}z}{1 - {\\scriptstyle\\frac{1}{2}}z}</math>\n||<math>\\frac{1 + {\\scriptstyle\\frac{1}{3}}z}\n{1 - {\\scriptstyle\\frac{2}{3}}z + {\\scriptstyle\\frac{1}{6}}z^2}</math>\n||<math>\\frac{1 + {\\scriptstyle\\frac{1}{4}}z}\n{1 - {\\scriptstyle\\frac{3}{4}}z + {\\scriptstyle\\frac{1}{4}}z^2 - {\\scriptstyle\\frac{1}{24}}z^3}</math>\n|-\n! 2\n| <math>\\frac{1 + z + {\\scriptstyle\\frac{1}{2}}z^2}{1}</math>\n||<math>\\frac{1 + {\\scriptstyle\\frac{2}{3}}z + {\\scriptstyle\\frac{1}{6}}z^2}\n{1 - {\\scriptstyle\\frac{1}{3}}z}</math>\n||<math>\\frac{1 + {\\scriptstyle\\frac{1}{2}}z + {\\scriptstyle\\frac{1}{12}}z^2}\n{1 - {\\scriptstyle\\frac{1}{2}}z + {\\scriptstyle\\frac{1}{12}}z^2}</math>\n||<math>\\frac{1 + {\\scriptstyle\\frac{2}{5}}z + {\\scriptstyle\\frac{1}{20}}z^2}\n{1 - {\\scriptstyle\\frac{3}{5}}z + {\\scriptstyle\\frac{3}{20}}z^2 - {\\scriptstyle\\frac{1}{60}}z^3}</math>\n|-\n! 3\n| <math>\\frac{1 + z + {\\scriptstyle\\frac{1}{2}}z^2 + {\\scriptstyle\\frac{1}{6}}z^3}{1}</math>\n||<math>\\frac{1 + {\\scriptstyle\\frac{3}{4}}z + {\\scriptstyle\\frac{1}{4}}z^2 + {\\scriptstyle\\frac{1}{24}}z^3}\n{1 - {\\scriptstyle\\frac{1}{4}}z}</math>\n||<math>\\frac{1 + {\\scriptstyle\\frac{3}{5}}z + {\\scriptstyle\\frac{3}{20}}z^2 + {\\scriptstyle\\frac{1}{60}}z^3}\n{1 - {\\scriptstyle\\frac{2}{5}}z + {\\scriptstyle\\frac{1}{20}}z^2}</math>\n||<math>\\frac{1 + {\\scriptstyle\\frac{1}{2}}z + {\\scriptstyle\\frac{1}{10}}z^2 + {\\scriptstyle\\frac{1}{120}}z^3}\n{1 - {\\scriptstyle\\frac{1}{2}}z + {\\scriptstyle\\frac{1}{10}}z^2 - {\\scriptstyle\\frac{1}{120}}z^3}</math>\n|-\n! 4\n| <math>\\frac{1 + z + {\\scriptstyle\\frac{1}{2}}z^2 + {\\scriptstyle\\frac{1}{6}}z^3+ {\\scriptstyle\\frac{1}{24}}z^4}{1}</math>\n||<math>\\frac{1 + {\\scriptstyle\\frac{4}{5}}z + {\\scriptstyle\\frac{3}{10}}z^2 + {\\scriptstyle\\frac{1}{15}}z^3+ {\\scriptstyle\\frac{1}{120}}z^4}\n{1 - {\\scriptstyle\\frac{1}{5}}z}</math>\n||<math>\\frac{1 + {\\scriptstyle\\frac{2}{3}}z + {\\scriptstyle\\frac{1}{5}}z^2 + {\\scriptstyle\\frac{1}{30}}z^3+ {\\scriptstyle\\frac{1}{360}}z^4}\n{1 - {\\scriptstyle\\frac{1}{3}}z + {\\scriptstyle\\frac{1}{30}}z^2}</math>\n||<math>\\frac{1 + {\\scriptstyle\\frac{4}{7}}z + {\\scriptstyle\\frac{1}{7}}z^2 + {\\scriptstyle\\frac{2}{105}}z^3+ {\\scriptstyle\\frac{1}{840}}z^4}\n{1 - {\\scriptstyle\\frac{3}{7}}z + {\\scriptstyle\\frac{1}{14}}z^2 - {\\scriptstyle\\frac{1}{210}}z^3}</math>\n|}\n\nSeveral features are immediately apparent.\n* The first column of the table consists of the successive truncations of the [[Taylor series]] for ''e<sup>z</sup>''.\n* Similarly, the first row contains the reciprocals of successive truncations of the series expansion of ''e''<sup>&minus;z</sup>''.\n* The approximants ''R<sub>m,n</sub>'' and ''R<sub>n,m</sub>'' are quite symmetrical &ndash; the numerators and denominators are interchanged, and the patterns of plus and minus signs are different, but the same coefficients appear in both of these approximants. In fact, using the <math>{}_1F_1</math> notation of [[generalized hypergeometric series]],\n::<math>R_{m,n}=\\frac{{}_1F_1(-m;-m-n;z)}{{}_1F_1(-n;-m-n;-z)}</math>\n* Computations involving the ''R<sub>n,n</sub>'' (on the main diagonal) can be done quite efficiently. For example, ''R<sub>3,3</sub>'' reproduces the power series for the exponential function perfectly up through <sup>1</sup>/<sub>720</sub> ''z''<sup>6</sup>, but because of the symmetry of the two cubic polynomials, a very fast evaluation algorithm can be devised.\n\nThe procedure used to derive [[Gauss's continued fraction]] can be applied to a certain [[confluent hypergeometric series]] to derive the following C-fraction expansion for the exponential function, valid throughout the entire complex plane:\n\n:<math>\ne^z = 1 + \\cfrac{z}{1 - \\cfrac{\\frac{1}{2}z}{1 + \\cfrac{\\frac{1}{6}z}{1 - \\cfrac{\\frac{1}{6}z}\n{1 + \\cfrac{\\frac{1}{10}z}{1 - \\cfrac{\\frac{1}{10}z}{1 + - \\ddots}}}}}}.\n</math>\n\nBy applying the [[fundamental recurrence formulas]] one may easily verify that the successive convergents of this C-fraction are the stairstep sequence of Padé approximants ''R''<sub>0,0</sub>, ''R''<sub>1,0</sub>, ''R''<sub>1,1</sub>, &hellip; In this particular case a closely related continued fraction can be obtained from the identity\n\n:<math>\ne^z = \\frac{1}{e^{-z}};\n</math>\n\nthat continued fraction looks like this:\n\n:<math>\ne^z = \\cfrac{1}{1 - \\cfrac{z}{1 + \\cfrac{\\frac{1}{2}z}{1 - \\cfrac{\\frac{1}{6}z}{1 + \\cfrac{\\frac{1}{6}z}\n{1 - \\cfrac{\\frac{1}{10}z}{1 + \\cfrac{\\frac{1}{10}z}{1 - + \\ddots}}}}}}}.\n</math>\n\nThis fraction's successive convergents also appear in the Padé table, and form the sequence ''R''<sub>0,0</sub>, ''R''<sub>0,1</sub>, ''R''<sub>1,1</sub>, ''R''<sub>1,2</sub>, ''R''<sub>2,2</sub>, &hellip;\n\n==Generalizations==\nA [[formal Newton series]] ''L'' is of the form\n\n:<math>\nL(z) = c_0 + \\sum_{n=1}^\\infty c_n \\prod_{k=1}^n (z - \\beta_k)\n</math>\n\nwhere the sequence {&beta;<sub>''k''</sub>} of points in the complex plane is known as the set of ''interpolation points''. A sequence of rational approximants ''R<sub>m,n</sub>'' can be formed for such a series ''L'' in a manner entirely analogous to the procedure described above, and the approximants can be arranged in a ''Newton-Padé table''. It has been shown<ref>{{cite book|last = Thiele|first = T.N.|title = Interpolationsrechnung|publisher = Teubner|location = Leipzig|year = 1909|isbn = 1-4297-0249-4}}</ref> that some \"staircase\" sequences in the Newton-Padé table correspond with the successive convergents of a Thiele-type continued fraction, which is of the form\n\n:<math>\na_0 + \\cfrac{a_1(z - \\beta_1)}{1 - \\cfrac{a_2(z - \\beta_2)}{1 - \\cfrac{a_3(z - \\beta_3)}{1 - \\ddots}}}.\n</math>\n\nMathematicians have also constructed ''two-point Padé tables'' by considering two series, one in powers of ''z'', the other in powers of 1/''z'', which alternately represent the function ''f''(''z'') in a neighborhood of zero and in a neighborhood of infinity.<ref name=\"JT\"/>\n\n==See also==\n*[[Shanks transformation]]\n\n==Notes==\n{{reflist}}\n\n==References==\n*{{cite book|last = Jones|first = William B.|author2=Thron, W. J.|title = Continued Fractions: Theory and Applications|publisher = Addison-Wesley Publishing Company|location = Reading, Massachusetts|year = 1980|pages = 185–197|isbn = 0-201-13510-8}}\n*{{cite book|last = Wall|first = H. S.|title = Analytic Theory of Continued Fractions|publisher = Chelsea Publishing Company|year = 1973|pages = 377–415|isbn = 0-8284-0207-8}}<br><small>(This is a reprint of the volume originally published by D. Van Nostrand Company, Inc., in 1948.)</small>\n\n{{DEFAULTSORT:Pade table}}\n[[Category:Continued fractions]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Pairwise summation",
      "url": "https://en.wikipedia.org/wiki/Pairwise_summation",
      "text": "In [[numerical analysis]], '''pairwise summation''', also called '''cascade summation''', is a technique to sum a sequence of finite-[[arithmetic precision|precision]] [[floating-point]] numbers that substantially reduces the accumulated [[round-off error]] compared to naively accumulating the sum in sequence.<ref name=Higham93>{{Citation | title=The accuracy of floating point summation |\nfirst1=Nicholas J. | last1=Higham | journal=[[SIAM Journal on Scientific Computing]] |\nvolume=14 | issue=4 | pages=783–799 | doi=10.1137/0914050 | year=1993\n| citeseerx=10.1.1.43.3535 }}</ref>  Although there are other techniques such as [[Kahan summation]] that typically have even smaller round-off errors, pairwise summation is nearly as good (differing only by a logarithmic factor) while having much lower computational cost&mdash;it can be implemented so as to have nearly the same cost (and exactly the same number of arithmetic operations) as naive summation.\n\nIn particular, pairwise summation of a sequence of ''n'' numbers ''x<sub>n</sub>'' works by [[recursion (computer science)|recursively]] breaking the sequence into two halves, summing each half, and adding the two sums: a [[divide and conquer algorithm]].  Its worst-case roundoff errors grow [[Big O notation|asymptotically]] as at most ''O''(ε&nbsp;log&nbsp;''n''), where ε is the [[machine precision]] (assuming a fixed [[condition number]], as discussed below).<ref name=Higham93/>  In comparison, the naive technique of accumulating the sum in sequence (adding each ''x<sub>i</sub>'' one at a time for ''i''&nbsp;=&nbsp;1,&nbsp;...,&nbsp;''n'') has roundoff errors that grow at worst as ''O''(ε''n'').<ref name=Higham93/>  [[Kahan summation]] has a [[error bound|worst-case error]] of roughly ''O''(ε), independent of ''n'', but requires several times more arithmetic operations.<ref name=Higham93/>   If the roundoff errors are random, and in particular have random signs, then they form a [[random walk]] and the error growth is reduced to an average of <math>O(\\varepsilon \\sqrt{\\log n})</math> for pairwise summation.<ref name=Tasche>Manfred Tasche and Hansmartin Zeuner ''Handbook of Analytic-Computational Methods in Applied Mathematics'' Boca Raton, FL: CRC Press, 2000).</ref>\n\nA very similar recursive structure of summation is found in many [[fast Fourier transform]] (FFT) algorithms, and is responsible for the same slow roundoff accumulation of those FFTs.<ref name=\"Tasche\"/><ref name=JohnsonFrigo08>S. G. Johnson and M. Frigo, \"[http://cnx.org/content/m16336/latest/ Implementing FFTs in practice], in ''[http://cnx.org/content/col10550/ Fast Fourier Transforms]'', edited by [[C. Sidney Burrus]] (2008).</ref>\n\nPairwise summation is the default summation algorithm in [[NumPy]]<ref>[https://github.com/numpy/numpy/pull/3685 ENH: implement pairwise summation], github.com/numpy/numpy pull request #3685 (September 2013).</ref> and the [[Julia (programming language)|Julia technical-computing language]],<ref>[https://github.com/JuliaLang/julia/pull/4039 RFC: use pairwise summation for sum, cumsum, and cumprod], github.com/JuliaLang/julia pull request #4039 (August 2013).</ref> where in both cases it was found to have comparable speed to naive summation (thanks to the use of a large base case).\n\n==The algorithm==\n\nIn [[pseudocode]], the pairwise summation algorithm for an [[Array data type|array]] ''x'' of length ''n'' > 0 can be written:\n\n ''s'' = '''pairwise'''(''x''[1&hellip;''n''])\n       if ''n'' &le; ''N''                    ''base case: naive summation for a sufficiently small array''\n           ''s'' = ''x''[1]\n           for ''i'' = 2 to ''n''\n               ''s'' = ''s'' + ''x''[''i'']\n       else                        ''divide and conquer: recursively sum two halves of the array''\n           ''m'' = [[Floor and ceiling functions|floor]](''n'' / 2)\n           ''s'' = '''pairwise'''(''x''[1&hellip;''m'']) + '''pairwise'''(''x''[''m''+1&hellip;''n''])\n       endif\n\nFor some sufficiently small ''N'', this algorithm switches to a naive loop-based summation as a [[Recursion#base case|base case]], whose error bound is O(Nε).<ref>{{cite book|first=Nicholas | last=Higham |title=Accuracy and Stability of Numerical Algorithms (2 ed)| publisher=SIAM|year=2002 | pages=81–82}}</ref>  The entire sum has a worst-case error that grows asymptotically as ''O''(ε&nbsp;log&nbsp;''n'') for large ''n'', for a given condition number (see below).\n\nIn an algorithm of this sort (as for [[Divide and conquer algorithm#Choosing the base cases|divide and conquer algorithm]]s in general<ref>Radu Rugina and Martin Rinard, \"[http://people.csail.mit.edu/rinard/paper/lcpc00.pdf Recursion unrolling for divide and conquer programs],\" in ''Languages and Compilers for Parallel Computing'', chapter 3, pp. 34–48.  ''Lecture Notes in Computer Science'' vol. 2017 (Berlin: Springer, 2001).</ref>), it is desirable to use a larger base case in order to [[Amortized analysis|amortize]] the overhead of the recursion. If ''N''&nbsp;=&nbsp;1, then there is roughly one recursive subroutine call for every input, but more generally there is one recursive call for (roughly) every ''N''/2 inputs if the recursion stops at exactly&nbsp;''n''&nbsp;=&nbsp;''N''.  By making ''N'' sufficiently large, the overhead of recursion can be made negligible (precisely this technique of a large base case for recursive summation is employed by high-performance FFT implementations<ref name=JohnsonFrigo08/>).\n\nRegardless of ''N'', exactly ''n''&minus;1 additions are performed in total, the same as for naive summation, so if the recursion overhead is made negligible then pairwise summation has essentially the same computational cost as for naive summation.\n\nA variation on this idea is to break the sum into ''b'' blocks at each recursive stage, summing each block recursively, and then summing the results, which was dubbed a \"superblock\" algorithm by its proposers.<ref name=Castaldo08>Anthony M. Castaldo, R. Clint Whaley, and Anthony T. Chronopoulos, \"Reducing floating-point error in dot product using the superblock family of algorithms,\" ''SIAM J. Sci. Comput.'', vol. 32, pp. 1156–1174 (2008).</ref>  The above pairwise algorithm corresponds to ''b''&nbsp;=&nbsp;2 for every stage except for the last stage which is&nbsp;''b''&nbsp;=&nbsp;''N''.\n\n==Accuracy==\n\nSuppose that one is summing ''n'' values ''x''<sub>''i''</sub>, for ''i''&nbsp;=&nbsp;1,&nbsp;...,&nbsp;''n''.  The exact sum is:\n:<math>S_n = \\sum_{i=1}^n x_i</math>\n\n(computed with infinite precision).\n\nWith pairwise summation for a base case ''N''&nbsp;=&nbsp;1, one instead obtains <math>S_n + E_n</math>, where the error <math>E_n</math> is bounded above by:<ref name=Higham93/>\n\n:<math>|E_n| \\leq \\frac{\\varepsilon \\log_2 n}{1 - \\varepsilon \\log_2 n} \\sum_{i=1}^n |x_i| </math>\n\nwhere ε is the [[machine precision]] of the arithmetic being employed (e.g. ε&nbsp;&asymp;&nbsp;10<sup>&minus;16</sup> for standard [[double precision]] floating point).  Usually, the quantity of interest is the [[relative error]] <math>|E_n|/|S_n|</math>, which is therefore bounded above by:\n:<math>\\frac{|E_n|}{|S_n|} \\leq \\frac{\\varepsilon \\log_2 n}{1 - \\varepsilon \\log_2 n} \\left(\\frac{\\sum_{i=1}^n |x_i|}{\\left| \\sum_{i=1}^n x_i \\right|}\\right). </math>\n\nIn the expression for the relative error bound, the fraction (&Sigma;|''x<sub>i</sub>''|/|&Sigma;''x<sub>i</sub>''|) is the [[condition number]] of the summation problem.  Essentially, the condition number represents the ''intrinsic'' sensitivity of the summation problem to errors, regardless of how it is computed.<ref>L. N. Trefethen and D. Bau, ''Numerical Linear Algebra'' (SIAM: Philadelphia, 1997).</ref>  The relative error bound of ''every'' ([[backwards stable]]) summation method by a fixed algorithm in fixed precision (i.e. not those that use [[arbitrary-precision arithmetic]], nor algorithms whose memory and time requirements change based on the data), is proportional to this condition number.<ref name=Higham93/>  An ''ill-conditioned'' summation problem is one in which this ratio is large, and in this case even pairwise summation can have a large relative error.  For example, if the summands ''x<sub>i</sub>'' are uncorrelated random numbers with zero mean, the sum is a [[random walk]] and the condition number will grow proportional to <math>\\sqrt{n}</math>.  On the other hand, for random inputs with nonzero mean the condition number asymptotes to a finite constant as <math>n\\to\\infty</math>.  If the inputs are all [[non-negative]], then the condition number is 1.\n\nNote that the <math>1 - \\varepsilon \\log_2 n</math> denominator is effectively 1 in practice, since <math>\\varepsilon \\log_2 n</math> is much smaller than 1 until ''n'' becomes of order 2<sup>1/ε</sup>, which is roughly 10<sup>10<sup>15</sup></sup> in double precision.\n\nIn comparison, the relative error bound for naive summation (simply adding the numbers in sequence, rounding at each step) grows as <math>O(\\varepsilon n)</math> multiplied by the condition number.<ref name=Higham93/>  In practice, it is much more likely that the rounding errors have a random sign, with zero mean, so that they form a random walk; in this case, naive summation has a [[root mean square]] relative error that grows as <math>O(\\varepsilon \\sqrt{n})</math> and pairwise summation has an error that grows as <math>O(\\varepsilon \\sqrt{\\log n})</math> on average.<ref name=\"Tasche\"/>\n\n==Support by libraries==\nIn the [[C Sharp (programming language)|C Sharp]] language, [https://www.nuget.org/packages/HPCsharp HPCsharp nuget package] implements pairwise summation, as well as [[Kahan summation]] (Neumaier variation): serial, data parallel using [[SIMD]] processor instructions, and parallel multi-core.\n<ref>https://github.com/DragonSpit/HPCsharp HPCsharp nuget package of high performance algorithms</ref>\n\n==References==\n{{reflist}}\n\n[[Category:Computer arithmetic]]\n[[Category:Numerical analysis]]\n[[Category:Articles with example pseudocode]]"
    },
    {
      "title": "Parareal",
      "url": "https://en.wikipedia.org/wiki/Parareal",
      "text": "'''Parareal''' is a [[parallel algorithm]] from [[numerical analysis]] and used for the solution of [[initial value problem]]s.<ref>{{cite journal\n| last       = Lions\n| first      =  Jacques-Louis\n| last2      = Maday\n| first2     = Yvon\n| last3      = Turinici\n| first3     = Gabriel\n| date       = 2015\n| title      = A \"parareal\" in time discretization of PDE's\n| journal    = Comptes Rendus de l'Académie des Sciences, Série I\n| volume     = 332\n| issue      = 7\n| pages      = 661–668\n| bibcode    =2001CRASM.332..661L \n| doi        = 10.1016/S0764-4442(00)01793-6\n| url      =  https://hal.archives-ouvertes.fr/hal-00798372/file/CRAS_01_lions_maday_turinici.pdf\n}}</ref>\nIt was introduced in 2001 by Lions, Maday and Turinici. Since then, it has become one of the most widely studied parallel-in-time integration methods.{{Citation needed|date=November 2018}}\n\n== Parallel-in-time integration methods ==\n\nIn contrast to e.g. [[Runge–Kutta methods|Runge-Kutta]] or [[Linear multistep method|multi-step]] methods, some of the computations in Parareal can be performed [[Parallel computing|in parallel]] and Parareal is therefore one example of a ''parallel-in-time'' integration method. While historically most efforts to parallelize the [[Numerical partial differential equations|numerical solution]] of [[partial differential equation]]s focussed on the spatial discretization, in view of the challenges from [[exascale computing]], parallel methods for [[temporal discretization]] have been identified as a possible way to increase concurrency in [[List of numerical analysis software|numerical software]].<ref>{{Cite report\n | author     = Jack Dongarra\n |author2=Jeffrey Hittinger |author3=John Bell |author4=Luis Chacon |author5=Robert Falgout |author6=Michael Heroux |author7=Paul Hovland |author8=Esmond Ng |author9=Clayton Webster |author10=Stefan Wild\n | date       = March 2014\n | title      = Applied Mathematics Research for Exascale Computing\n | url        = http://science.energy.gov/~/media/ascr/pdf/research/am/docs/EMWGreport.pdf\n | publisher  = US Department of Energy\n | accessdate = August 2015\n}}</ref>\nBecause Parareal computes the numerical solution for multiple time steps in parallel, it is categorized as a ''parallel across the steps'' method.<ref>{{cite journal\n| last       = Burrage\n| first      =  Kevin\n| date       = 1997\n| title      = Parallel methods for ODEs\n| url        =\n| journal    = Advances in Computational Mathematics\n| volume     = 7\n| issue      = 1–2\n| pages      = 1–31\n| bibcode    = \n| doi        = 10.1023/A:1018997130884\n}}</ref>\nThis is in contrast to approaches using ''parallelism across the method'' like parallel Runge-Kutta or extrapolation methods, where independent stages can be computed in parallel or ''parallel across the system'' methods like waveform relaxation.<ref>{{Cite journal|last=Iserles|first=A.|last2=NøRSETT|first2=S. P.|date=1990-10-01|title=On the Theory of Parallel Runge—Kutta Methods|url=http://imajna.oxfordjournals.org/content/10/4/463|journal=IMA Journal of Numerical Analysis|language=en|volume=10|issue=4|pages=463–488|doi=10.1093/imanum/10.4.463|issn=0272-4979}}</ref><ref>{{Cite journal|last=Ketcheson|first=David|last2=Waheed|first2=Umair bin|date=2014-06-13|title=A comparison of high-order explicit Runge–Kutta, extrapolation, and deferred correction methods in serial and parallel|journal=Communications in Applied Mathematics and Computational Science|volume=9|issue=2|pages=175–200|doi=10.2140/camcos.2014.9.175|issn=2157-5452|arxiv=1305.6165}}</ref>\n\n== History ==\n\nParareal can be derived as both a [[multigrid method]] in time method or as [[Direct multiple shooting method|multiple shooting]] along the time axis.<ref name=\"gander2007\">{{cite journal\n| last       = Gander\n| first      =  Martin J.\n| last2      = Vandewalle\n| first2     = Stefan\n| date       = 2007\n| title      = Analysis of the Parareal Time‐Parallel Time‐Integration Method\n| journal    = SIAM Journal on Scientific Computing\n| volume     = 29\n| issue      = 2\n| pages      = 556–578\n| bibcode    = \n| doi        = 10.1137/05064607X\n| citeseerx      =  10.1.1.154.6042\n}}</ref>\nBoth ideas, multigrid in time as well as adopting multiple shooting for time integration, go back to the 1980s and 1990s.<ref>{{cite book\n| last       = Hackbusch\n| first      =  Wolfgang\n| date       = 1985\n| title      = Parabolic multi-grid methods\n| url        = http://dl.acm.org/citation.cfm?id=4673.4714\n| journal    = Computing Methods in Applied Sciences and Engineering, VI\n| volume     = \n| issue      = \n| pages      = 189–197 \n| bibcode    = \n| doi        = \n| access-date= August 2015\n| isbn      =  9780444875976\n}}</ref><ref>\n{{cite journal\n| last       = Kiehl\n| first      =  Martin\n| date       = 1994\n| title      = Parallel multiple shooting for the solution of initial value problems\n| journal    = Parallel Computing\n| volume     = 20\n| issue      = 3\n| pages      =  275–295\n| bibcode    = \n| doi        = 10.1016/S0167-8191(06)80013-X\n}}</ref>\nParareal is a widely studied method and has been used and modified for a range of different applications.<ref>{{cite conference\n | last = Gander\n | first = Martin J.\n | authorlink =\n | title = 50 years of Time Parallel Time Integration\n | publisher = Springer International Publishing\n | series = Contributions in Mathematical and Computational Sciences\n | volume = 9\n | edition = 1\n | date = 2015\n | location =\n | pages =\n | language =\n | url =\n | doi = 10.1007/978-3-319-23321-5\n | id =\n | isbn = 978-3-319-23321-5\n | mr =\n | zbl =\n | jfm = }}</ref>\nIdeas to parallelize the solution of initial value problems go back even further: the first paper proposing a parallel-in-time integration method appeared in 1964.<ref>\n{{cite journal\n| last       = Nievergelt\n| first      = Jürg\n| date       = 1964\n| title      =  Parallel methods for integrating ordinary differential equations\n| journal    = Communications of the ACM\n| volume     = 7\n| issue      =  12\n| pages      =  731–733\n| bibcode    = \n| doi        = 10.1145/355588.365137\n}}</ref>\n\n== Algorithm ==\n[[File:Parareal Animation.ogg|thumb|708x708px|Visualization of the Parareal algorithm. The coarse propagator here is labelled <math>\\bar{\\varphi}</math> whereas the fine propagator is labelled <math>\\varphi</math>.]]\nParareal solves an initial value problem of the form\n\n<math> \\dot{y}(t) = f(y(t), t), \\quad y(t_0) = y_0 \\quad \\text{with} \\quad t_0 \\leq t \\leq T.</math>\n\nHere, the right hand side <math>f</math> can correspond to the spatial discretization of a partial differential equation in a [[method of lines]] approach.\n\nParareal now requires a [[Domain decomposition methods|decomposition]] of the time interval <math>[t_0, T]</math> into <math>P</math> so-called time slices <math>[t_j, t_{j+1}]</math> such that\n\n<math> [t_0, T] = [t_0, t_1] \\cup [t_1, t_2] \\cup \\ldots \\cup [t_{P-1}, t_{P} ].</math>\n\nEach time slice is assigned to one processing unit when parallelizing the algorithm, so that <math>P</math> is equal to the number of processing units used for Parareal: in an [[Message Passing Interface|MPI]] based code for example, this would be the number of processes, while in an [[OpenMP]] based code, <math>P</math> would be equal to the number of [[Thread (computing)|threads]].\n\nParareal is based on the iterative application of two methods for [[Numerical methods for ordinary differential equations|integration of ordinary differential equations]].\nOne, commonly labelled <math>\\mathcal{F}</math>, should be of high accuracy and computational cost while the other, typically labelled <math>\\mathcal{G}</math>, must be computationally cheap but can be much less accurate. \nTypically, some form of Runge-Kutta method is chosen for both coarse and fine integrator, where <math>\\mathcal{G}</math> might be of lower order and use a larger time step than <math>\\mathcal{F}</math>.\nIf the initial value problem stems from the discretization of a PDE, <math>\\mathcal{G}</math> can also use a coarser spatial discretization, but this can negatively impact convergence unless high order interpolation is used.<ref>{{Cite journal|last=Ruprecht|first=Daniel|date=2014-12-01|title=Convergence of Parareal with spatial coarsening|journal=PAMM|language=en|volume=14|issue=1|pages=1031–1034|doi=10.1002/pamm.201410490|issn=1617-7061|url=http://eprints.whiterose.ac.uk/90536/1/paper.pdf}}</ref>\nThe result of numerical integration with one of these methods over a time slice <math>[t_{j}, t_{j+1}]</math> for some starting value <math>y_j</math> given at <math>t_j</math> is then written as\n\n<math> y = \\mathcal{F}(y_j, t_j, t_{j+1}) </math>   or   <math>y = \\mathcal{G}(y_j, t_j, t_{j+1}) </math>.\n\nSerial time integration with the fine method would then correspond to a step-by-step computation of\n\n<math> y_{j+1} = \\mathcal{F}(y_j, t_j, t_{j+1}), \\quad j=0, \\ldots, P-1.</math>\n\nParareal instead uses the following iteration\n\n<math> y_{j+1}^{k+1} = \\mathcal{G}(y^{k+1}_j, t_j, t_{j+1}) + \\mathcal{F}(y^k_j, t_j, t_{j+1}) - \\mathcal{G}(y^k_j, t_j, t_{j+1}), \\quad j=0, \\ldots, P-1, \\quad k=0, \\ldots, K-1,</math>\n\nwhere <math>k</math> is the iteration counter. \nAs the iteration converges and <math>y^{k+1}_j - y^k_j \\to 0</math>, the terms from the coarse method cancel out and Parareal reproduces the solution that is obtained by the serial execution of the fine method only.\nIt can be shown that Parareal converges after a maximum of <math>P</math> iterations.<ref name=\"gander2007\" />\nFor Parareal to provide speedup, however, it has to converge in a number of iterations significantly smaller than the number of time slices, that is <math>K \\ll P</math>.\n\nIn the Parareal iteration, the computationally expensive evaluation of <math>\\mathcal{F}(y^k_j, t_j, t_{j+1})</math> can be performed in parallel on <math>P</math> processing units.\nBy contrast, the dependency of <math>y^{k+1}_{j+1}</math> on <math>\\mathcal{G}(y^{k+1}_j, t_j, t_{j+1})</math> means that the coarse correction has to be computed in serial order.\n\n===Speedup===\nUnder some assumptions, a simple theoretical model for the [[Amdahl's law|speedup]] of Parareal can be derived.<ref>{{cite journal\n| last       = Minion\n| first      = Michael L.\n| date       = 2010\n| title      =  A Hybrid Parareal Spectral Deferred Corrections Method\n| journal    = Communications in Applied Mathematics and Computational Science\n| volume     = 5\n| issue      =  2\n| pages      = 265–301\n| bibcode    = \n| doi        = 10.2140/camcos.2010.5.265\n}}</ref>\nAlthough in applications these assumptions can be too restrictive, the model still is useful to illustrate the trade offs that are involved in obtaining speedup with Parareal.\n\nFirst, assume that every time slice <math>[t_j, t_{j+1}]</math> consists of exactly <math>N_f</math> steps of the fine integrator and of <math>N_c</math> steps of the coarse integrator.\nThis includes in particular the assumption that all time slices are of identical length and that both coarse and fine integrator use a constant step size over the full simulation.\nSecond, denote by <math>\\tau_f</math> and <math>\\tau_c</math> the computing time required for a single step of the fine and coarse methods, respectively, and assume that both are constant.\nThis is typically not exactly true when an [[Temporal discretization#Implicit Time Integration|implicit]] method is used, because then runtimes vary depending on the number of iterations required by the [[Iterative method|iterative solver]].\n\nUnder these two assumptions, the runtime for the fine method integrating over <math>P</math> time slices can be modelled as\n\n<math> c_{\\text{fine}} = P N_{f} \\tau_f. </math>\n\nThe runtime of Parareal using <math>P</math> processing units and performing <math>K</math> iterations is\n\n<math> c_{\\text{parareal}} = (K+1) P N_{c} \\tau_c + K N_{f} \\tau_f. </math>\n\nSpeedup of Parareal then is\n\n<math> S_{p} = \\frac{c_{\\text{fine}}}{c_{\\text{parareal}}} = \\frac{1}{ (K+1) \\frac{N_c}{N_f} \\frac{\\tau_c}{\\tau_f} + \\frac{K}{P}} \\leq \\min\\left\\{ \\frac{N_f \\tau_f}{N_c \\tau_c}, \\frac{P}{K}  \\right\\}.</math>\n\nThese two bounds illustrate the trade off that has to be made in choosing the coarse method: on the one hand, it has to be cheap and/or use a much larger time step to make the first bound as large as possible, on the other hand the number of iterations <math>K</math> has to be kept low to keep the second bound large.\nIn particular, [[Speedup#Additional Details|Parareal's parallel efficiency]] is bounded by\n\n<math> E_{p} = \\frac{S_p}{P} \\leq \\frac{1}{K},</math>\n\nthat is by the inverse of the number of required iterations.\n\n=== Instability for imaginary eigenvalues ===\nThe vanilla version of Parareal has issues for problems with imaginary [[Eigenvalues and eigenvectors|eigenvalues]].<ref name=\"gander2007\" /> It typically only converges toward the very last iterations, that is as <math>k</math> approaches <math>P</math>, and the speedup <math> S_p</math> is always going to be smaller than one.  So either the number of iterations is small and Parareal is unstable or, if <math>k</math> is large enough to make Parareal stable, no speedup is possible. This also means that Parareal is typically unstable for [[Hyperbolic partial differential equation|hyperbolic]] equations.<ref>{{Cite book|title=Stability of the Parareal Algorithm|last=Staff|first=Gunnar Andreas|last2=Rønquist|first2=Einar M.|date=2005-01-01|publisher=Springer Berlin Heidelberg|isbn=9783540225232|editor-last=Barth|editor-first=Timothy J.|series=Lecture Notes in Computational Science and Engineering|pages=449–456|language=en|doi=10.1007/3-540-26825-1_46|editor-last2=Griebel|editor-first2=Michael|editor-last3=Keyes|editor-first3=David E.|editor-last4=Nieminen|editor-first4=Risto M.|editor-last5=Roose|editor-first5=Dirk|editor-last6=Schlick|editor-first6=Tamar|editor-last7=Kornhuber|editor-first7=Ralf|editor-last8=Hoppe|editor-first8=Ronald|editor-last9=Périaux|editor-first9=Jacques}}</ref> Even though the formal analysis by Gander and Vandewalle covers only linear problems with constant coefficients, the problem also arises when Parareal is applied to the nonlinear [[Navier–Stokes equations]] when the [[viscosity]] coefficient becomes too small and the [[Reynolds number]] too large.<ref>{{Cite book|title=Convergence of Parareal for the Navier-Stokes Equations Depending on the Reynolds Number|last=Steiner|first=Johannes|last2=Ruprecht|first2=Daniel|last3=Speck|first3=Robert|last4=Krause|first4=Rolf|date=2015-01-01|publisher=Springer International Publishing|isbn=9783319107042|editor-last=Abdulle|editor-first=Assyr|series=Lecture Notes in Computational Science and Engineering|pages=195–202|language=en|doi=10.1007/978-3-319-10705-9_19|editor-last2=Deparis|editor-first2=Simone|editor-last3=Kressner|editor-first3=Daniel|editor-last4=Nobile|editor-first4=Fabio|editor-last5=Picasso|editor-first5=Marco|citeseerx = 10.1.1.764.6242}}</ref> Different approaches exist to stabilise Parareal,<ref>{{Cite journal|last=Dai|first=X.|last2=Maday|first2=Y.|date=2013-01-01|title=Stable Parareal in Time Method for First- and Second-Order Hyperbolic Systems|journal=SIAM Journal on Scientific Computing|volume=35|issue=1|pages=A52–A78|doi=10.1137/110861002|issn=1064-8275}}</ref><ref name=\":0\">{{Cite journal|last=Farhat|first=Charbel|last2=Cortial|first2=Julien|last3=Dastillung|first3=Climène|last4=Bavestrello|first4=Henri|date=2006-07-30|title=Time-parallel implicit integrators for the near-real-time prediction of linear structural dynamic responses|journal=International Journal for Numerical Methods in Engineering|language=en|volume=67|issue=5|pages=697–724|doi=10.1002/nme.1653|issn=1097-0207|bibcode=2006IJNME..67..697F}}</ref><ref name=\":1\">{{Cite book|title=On the Use of Reduced Basis Methods to Accelerate and Stabilize the Parareal Method|last=Chen|first=Feng|last2=Hesthaven|first2=Jan S.|last3=Zhu|first3=Xueyu|date=2014-01-01|publisher=Springer International Publishing|isbn=9783319020891|editor-last=Quarteroni|editor-first=Alfio|series=MS&A - Modeling, Simulation and Applications|pages=187–214|language=en|doi=10.1007/978-3-319-02090-7_7|editor-last2=Rozza|editor-first2=Gianluigi|url = http://infoscience.epfl.ch/record/190666}}</ref> one being Krylov-subspace enhanced Parareal.\n\n== Variants ==\nThere are multiple algorithms that are directly based or at least inspired by the original Parareal algorithm.\n\n=== Krylov-subspace enhanced Parareal ===\nEarly on it was recognised that for linear problems information generated by the fine method <math>\\mathcal{F}_{\\delta t}</math> can be used to improve the accuracy of the coarse method <math>\\mathcal{G}_{\\Delta t}</math>.<ref name=\":0\" /> Originally, the idea was formulated for the parallel implicit time-integrator PITA,<ref>{{Cite journal|last=Farhat|first=Charbel|last2=Chandesris|first2=Marion|date=2003-11-07|title=Time-decomposed parallel time-integrators: theory and feasibility studies for fluid, structure, and fluid–structure applications|journal=International Journal for Numerical Methods in Engineering|language=en|volume=58|issue=9|pages=1397–1434|doi=10.1002/nme.860|issn=1097-0207|bibcode=2003IJNME..58.1397F}}</ref> a method closely related to Parareal but with small differences in how the correction is done. In every iteration <math>k</math> the result <math>\\mathcal{F}_{\\delta t}(y^k_j)</math> is computed for values <math>u^k_j \\in \\mathbb{R}^d</math> for <math>j=0, \\ldots, P-1</math>. Based on this information, the [[Vector space|subspace]]\n\n<math>\nS_k := \\left\\{ y^{k'}_j : 0 \\leq k' \\leq k, j=0, \\ldots, P-1 \\right\\} </math>\n\nis defined and updated after every Parareal iteration.<ref>{{Cite journal|last=Gander|first=M.|last2=Petcu|first2=M.|title=Analysis of a Krylov subspace enhanced parareal algorithm for linear problems|journal=ESAIM: Proceedings|volume=25|pages=114–129|doi=10.1051/proc:082508|year=2008}}</ref> Denote as <math> P_k</math> the [[orthogonal projection]] from <math> \\mathbb{R}^d</math> to <math> S_k</math>. Then, replace the coarse method with the improved integrator <math> \\mathcal{K}_{\\Delta t}(y) = \\mathcal{F}_{\\delta t}(P_k y) + \\mathcal{G}_{\\Delta t}((I-P_k)y)</math>.\n\nAs the number of iterations increases, the space <math> S_k</math> will grow and the modified propagator <math> \\mathcal{K}_{\\Delta t}</math> will become more accurate. This will lead to faster convergence. This version of Parareal can also stably integrate linear hyperbolic partial differential equations.<ref>{{Cite journal|last=Ruprecht|first=D.|last2=Krause|first2=R.|date=2012-04-30|title=Explicit parallel-in-time integration of a linear acoustic-advection system|journal=Computers & Fluids|volume=59|pages=72–83|doi=10.1016/j.compfluid.2012.02.015|arxiv=1510.02237}}</ref> An extension to nonlinear problems based on the reduced basis method exists as well.<ref name=\":1\" />\n\n=== Hybrid Parareal spectral deferred corrections ===\nA method with improved parallel efficiency based on a combination of Parareal with spectral deferred corrections (SDC) <ref>{{Cite journal|last=Dutt|first=Alok|last2=Greengard|first2=Leslie|last3=Rokhlin|first3=Vladimir|date=2000-06-01|title=Spectral Deferred Correction Methods for Ordinary Differential Equations|journal=BIT Numerical Mathematics|language=en|volume=40|issue=2|pages=241–266|doi=10.1023/A:1022338906936|issn=0006-3835}}</ref> has been proposed by M. Minion.<ref>{{Cite journal|last=Minion|first=Michael|date=2011-01-05|title=A hybrid parareal spectral deferred corrections method|journal=Communications in Applied Mathematics and Computational Science|volume=5|issue=2|pages=265–301|doi=10.2140/camcos.2010.5.265|issn=2157-5452}}</ref> It limits the choice for coarse and fine integrator to SDC, sacrificing flexibility for improved parallel efficiency. Instead of the limit of <math>1/K</math>, the bound on parallel efficiency in the hybrid method becomes\n\n<math>E_p \\leq \\frac{K_s}{K_p}</math>\n\nwith <math>K_s</math> being the number of iterations of the serial SDC base method and <math>K_p</math> the typically greater number of iterations of the parallel hybrid method. The Parareal-SDC hybrid has been further improved by addition of a ''full approximation scheme''  as used in nonlinear [[Multigrid method|multigrid]]. This led to the development of the ''parallel full approximation scheme in space and time'' (PFASST).<ref>{{Cite journal|last=Emmett|first=Matthew|last2=Minion|first2=Michael|date=2012-03-28|title=Toward an efficient parallel in time method for partial differential equations|journal=Communications in Applied Mathematics and Computational Science|volume=7|issue=1|pages=105–132|doi=10.2140/camcos.2012.7.105|issn=2157-5452}}</ref> Performance of PFASST has been studied for PEPC, a [[Barnes–Hut simulation|Barnes-Hut]] tree code based particle solver developed at [[Jülich Supercomputing Centre|Juelich Supercomputing Centre]]. Simulations using all 262,144 cores on the IBM [[Blue Gene|BlueGene]]/P system JUGENE showed that PFASST could produce additional speedup beyond saturation of the spatial tree parallelisation.<ref>{{Cite book|last=Speck|first=R.|last2=Ruprecht|first2=D.|last3=Krause|first3=R.|last4=Emmett|first4=M.|last5=Minion|first5=M.|last6=Winkel|first6=M.|last7=Gibbon|first7=P.|date=2012-11-01|title=A massively space-time parallel N-body solver|journal=High Performance Computing, Networking, Storage and Analysis (SC), 2012 International Conference for|pages=1–11|doi=10.1109/SC.2012.6|isbn=978-1-4673-0805-2}}</ref>\n\n=== Multigrid reduction in time (MGRIT) ===\nThe multigrid reduction in time method (MGRIT) generalises the interpretation of Parareal as a multigrid-in-time algorithms to multiple levels using different smoothers.<ref>{{Cite journal|last=Falgout|first=R.|last2=Friedhoff|first2=S.|last3=Kolev|first3=T.|last4=MacLachlan|first4=S.|last5=Schroder|first5=J.|date=2014-01-01|title=Parallel Time Integration with Multigrid|journal=SIAM Journal on Scientific Computing|volume=36|issue=6|pages=C635–C661|doi=10.1137/130944230|issn=1064-8275|citeseerx=10.1.1.701.2603}}</ref> It is a more general approach but for a specific choice of parameters it is equivalent to Parareal. The [http://computation.llnl.gov/projects/parallel-time-integration-multigrid XBraid] library implementing MGRIT is being developed by [[Lawrence Livermore National Laboratory]].\n\n=== ParaExp ===\nParaExp uses [[exponential integrator]]s within Parareal.<ref>{{Cite journal|last=Gander|first=M.|last2=Güttel|first2=S.|date=2013-01-01|title=PARAEXP: A Parallel Integrator for Linear Initial-Value Problems|journal=SIAM Journal on Scientific Computing|volume=35|issue=2|pages=C123–C142|doi=10.1137/110856137|issn=1064-8275|citeseerx=10.1.1.800.5938}}</ref> While limited to linear problems, it can produce almost optimal parallel speedup.\n\n==References==\n{{Reflist}}\n\n== External links ==\n*[https://www.parallel-in-time.org/ parallel-in-time.org]\n\n[[Category:Numerical analysis]]\n[[Category:Numerical differential equations]]\n[[Category:Parallel computing]]\n[[Category:Computational science]]"
    },
    {
      "title": "Partial differential algebraic equation",
      "url": "https://en.wikipedia.org/wiki/Partial_differential_algebraic_equation",
      "text": "In [[mathematics]] a '''partial differential algebraic equation (PDAE)''' set is an incomplete system of [[partial differential equation]]s that is closed with a set of [[algebraic equation]]s.\n\n== Definition ==\nA general PDAE is defined as:\n\n: <math>0 = \\mathbf F \\left( \\mathbf x, \\mathbf y, \\frac{\\partial y_i}{\\partial x_j}, \\frac{\\partial^2 y_i}{\\partial x_j \\partial x_k}, \\ldots, \\mathbf z \\right),</math>\n\nwhere:\n\n* '''F''' is a set of arbitrary functions;\n* '''x''' is a set of independent variables;\n* '''y''' is a set of dependent variables for which partial derivatives are defined; and\n* '''z''' is a set of dependent variables for which no partial derivatives are defined.\n\nThe relationship between a PDAE and a [[partial differential equation]] (PDE) is analogous to the relationship between an [[ordinary differential equation]] (ODE) and a [[differential algebraic equation]] (DAE).\n\nPDAEs of this general form are challenging to solve.  Simplified forms are studied in more detail in the literature.<ref>Wagner, Y.  2000.  \"A further index concept for linear PDAEs of hyperbolic type,\" Mathematics and Computers in Simulation, v. 53, pp. 287–291.</ref><ref>W. S. Martinson, P. I. Barton. (2002) \"Index and characteristic analysis of linear PDAE systems,\" SIAM Journal on Scientific Computing, v. 24, n. 3, pp. 905–923.</ref><ref>Lucht, W.; Strehmel, K..  1998.  \"Discretization based indices for semilinear partial differential algebraic equations,\" Applied Numerical Mathematics, v. 28, pp. 371–386.</ref>  Even as recently as 2000, the term \"PDAE\" has been handled as unfamiliar by those in related fields.<ref>Simeon, B.; Arnold, M..  2000.  \"Coupling DAEs and PDEs for simulating the interaction of pantograph and catenary,\" Mathematical and Computer Modelling of Dynamical Systems, v. 6, pp. 129–144.</ref>\n\n== Solution methods ==\n[[Semi-discretization]] is a common method for solving PDAEs whose independent variables are those of [[time]] and [[space]], and has been used for decades.<ref>Jacob, J.; Le Lann, J; Pinguad, H.; Capdeville, B.. 1996. \"A generalized approach for dynamic modelling \nand simulation of biofilters: application to waste-water denitrification,\" Chemical Engineering Journal, v. \n65, pp. 133–143.</ref><ref>de Dieuvleveult, C.; Erhel, J.; Kern, M..  2009.  \"A global strategy for solving reactive transport equations,\" Journal of Computational Physics, v. 228, pp. 6395–6410.</ref>  This method involves removing the spatial variables using a [[discretization]] method, such as the [[finite volume method]], and incorporating the resulting linear equations as part of the algebraic relations.  This reduces the system to a [[differential algebraic equation|DAE]], for which conventional solution methods can be employed.\n\n== References ==\n{{Reflist}}\n\n[[Category:Partial differential equations]]\n[[Category:Differential equations]]\n[[Category:Multivariable calculus]]\n[[Category:Numerical analysis]]\n\n\n{{applied-math-stub}}"
    },
    {
      "title": "Particle method",
      "url": "https://en.wikipedia.org/wiki/Particle_method",
      "text": "{{Use American English|date = March 2019}}\nIn the field of [[numerical analysis]], '''particle methods''' discretize fluid into particles. Particle methods enable the simulation of some otherwise difficult types of problems, at the cost of extra computing time and programming effort. \n\nSome of particle methods are [[meshfree methods]] and vice versa.\n\n==History==\n\nOne of the earliest particle methods is [[smoothed particle hydrodynamics]], presented in 1977.<ref>Gingold RA, Monaghan JJ (1977). Smoothed particle hydrodynamics – theory and application to non-spherical stars. Mon Not R Astron Soc 181:375–389</ref> Libersky ''et al.''<ref>Libersky, L.D., Petscheck, A.G., Carney, T.C., Hipp, J.R., Allahdadi, F.A. (1993). High Strain Lagrangian Hydrodynamics. ''Journal of Computational Physics''.</ref> were the first to apply SPH in solid mechanics. The main drawbacks of SPH are inaccurate results near boundaries and tension instability that was first investigated by Swegle.<ref>Swegle, J.W., Hicks, D.L., Attaway, S.W. (1995). Smoothed Particle Hydrodynamics Stability Analysis. ''Journal of Computational Physics''. 116(1), 123-134</ref> \n\nIn the 1990s a new class of particle methods emerged. The [[reproducing kernel particle method]]<ref>Liu, W.K., Jun, S., Zhang, Y.F. (1995), Reproducing kernel particle methods, ''International Journal of Numerical Methods in Fluids''. 20, 1081-1106. </ref> (RKPM) emerged, the approximation motivated in part to correct the kernel estimate in SPH: to give accuracy near boundaries, in non-uniform discretizations, and higher-order accuracy in general. Notably, in a parallel development, the [[Material point method|Material point methods]] were developed around the same time<ref>D. Sulsky, Z., Chen, H. Schreyer (1994). a Particle Method for History-Dependent Materials. ''Computer Methods in Applied Mechanics and Engineering'' (118) 1, 179-196.</ref> which offer similar capabilities. During the 1990s and thereafter several other varieties were developed including those listed below.\n\n==List of methods and acronyms==\n\nThe following numerical methods are generally considered to fall within the general class of \"particle\" methods.  Acronyms are provided in parentheses.\n\n* [[Smoothed particle hydrodynamics]] (SPH) (1977)\n* [[Dissipative particle dynamics]] (DPD) (1992)\n* [[Reproducing kernel particle method]] (RKPM) (1995)\n* [[Moving particle semi-implicit]] (MPS)\n* [[Particle-in-cell]] (PIC)\n* [[Moving particle finite element method]] (MPFEM)\n* [[Cracking particles method]] (CPM) (2004)\n* [[Immersed particle method]] (IPM) (2006)\n\n==See also==\n\n* [[Continuum mechanics]]\n* [[Boundary element method]]\n* [[Immersed boundary method]]\n* [[Stencil code]]\n* [[Meshfree methods]]\n\n==References==\n{{reflist}}\n\n==Further reading==\n{{refbegin}}\n* Liu MB, Liu GR, Zong Z, AN OVERVIEW ON SMOOTHED PARTICLE HYDRODYNAMICS, INTERNATIONAL JOURNAL OF COMPUTATIONAL METHODS   Vol. 5   Issue: 1, 135–188, 2008.\n* Liu, G.R., Liu, M.B. (2003).  ''Smoothed Particle Hydrodynamics, a meshfree and Particle Method'',  World Scientific, {{ISBN|981-238-456-1}}.\n{{refend}}\n\n==External links==\n* [https://www.particleworks.com/technical_column01_en.html Partcle Methods]\n\n{{Numerical PDE}}\n\n{{DEFAULTSORT:Particle Method}}\n[[Category:Numerical analysis]]\n[[Category:Numerical differential equations]]\n[[Category:Computational fluid dynamics]]"
    },
    {
      "title": "Piecewise linear continuation",
      "url": "https://en.wikipedia.org/wiki/Piecewise_linear_continuation",
      "text": "'''Simplicial continuation''', or '''piecewise linear continuation''' (Allgower and Georg),<ref name=\"one\">Eugene L. Allgower, K. Georg, \"Introduction to Numerical Continuation Methods\", ''SIAM Classics in Applied Mathematics'' 45, 2003.</ref><ref name=\"three\">E. L. Allgower, K. Georg, \"Simplicial and Continuation Methods for Approximating Fixed Points and Solutions to Systems of Equations\", ''SIAM Review'', Volume 22, 28-85, 1980.</ref> is a one-parameter [[numerical continuation|continuation method]] which is well suited to small to medium embedding spaces. The algorithm has been generalized to compute higher-dimensional manifolds by (Allgower and Gnutzman)<ref name=\"two\">Eugene L. Allgower, Stefan Gnutzmann, \"An Algorithm for Piecewise Linear Approximation of Implicitly Defined Two-Dimensional Surfaces\", ''SIAM Journal on Numerical Analysis'', Volume 24, Number 2, 452-469, 1987.</ref> and (Allgower and Schmidt).<ref name=\"four\">Eugene L. Allgower, Phillip H. Schmidt, \"An Algorithm for Piecewise-Linear Approximation of an Implicitly Defined Manifold\", ''SIAM Journal on Numerical Analysis'', Volume 22, Number 2, 322-346, April 1985.</ref>\n\nThe algorithm for drawing contours is a simplicial continuation algorithm, and since it is easy to visualize, it serves as a good introduction to the algorithm.\n\n== Contour plotting ==\n\nThe contour plotting problem is to find the zeros (contours) of <math> f(x,y)=0\\,</math> (<math> f(\\cdot)\\,</math> a smooth scalar valued function) in the square <math>0\\leq x \\leq 1, 0\\leq y \\leq 1\\,</math>,\n<center>\n[[Image:Contours.gif|An example of contours]] [[Image:ContoursA.gif|Contours, three-dimensional view]]\n</center>\nThe square is divided into small triangles, usually by introducing points at the corners of a regular square mesh <math>ih_x\\leq x\\leq (i+1)h_x\\,</math>, <math>jh_y\\leq y \\leq (j+1)h_y\\,</math>, making a table of the values of <math>f(x_i,y_j)\\,</math> at each corner <math>(i,j)\\,</math>, and then dividing each square into two triangles. The value of <math>f(x_i,y_j)\\,</math> at the corners of the triangle defines a unique Piecewise Linear interpolant <math>lf(x,y)\\,</math> to <math>f(\\cdot)\\,</math> over each triangle. One way of writing this interpolant on the triangle with corners\n<math>(x_0,y_0),~(x_1,y_1),~(x_2,y_2)\\,</math> is as the set of equations\n\n: <math> (x,y) = (x_0,y_0)+(x_1-x_0,y_1-y_0)s+(x_2-x_0,y_2-y_0)t\\,</math>\n: <math> 0\\leq s\\,</math>\n: <math> 0\\leq t\\,</math>\n: <math> s+t \\leq 1\\,</math>\n: <math> lf(x,y) = f(x_0,y_0)+(f(x_1,y_1)-f(x_0,y_0))s+(f(x_2,y_2)-f(x_0,y_0))t\\,</math>\n\nThe first four equations can be solved for <math>(s,t)\\,</math> (this maps the original triangle to a right unit triangle), then the remaining equation gives the interpolated value of <math>f(\\cdot)\\,</math>. Over the whole mesh of triangles, this piecewise linear interpolant is continuous.\n<center>\n[[Image:LinearInterpolant.gif|An example of a triangulation and marked vertices]] [[Image:LinearInterpolantA.gif|Linear interpolant, three-dimensional view]]\n</center>\nThe contour of the interpolant on an individual triangle is a line segment (it is an interval on the intersection of two planes). The equation for the line can be found, however the points where the line crosses the edges of the triangle are the endpoints of the line segment.\n<center>\n[[Image:Contour.gif|The unique linear interpolant on a simplex and its zero set]][[Image:TriangleContour.gif|The contour of the linear interpolant over a triangle]]\n</center>\nThe contour of the piecewise linear interpolant is a set of curves made up of these line segments. Any point on the edge connecting <math>(x_0,y_0)\\,</math> and <math>(x_1,y_1)\\,</math> can be written as\n\n: <math>(x,y) = (x_0,y_0) + t (x_1-x_0,y_1-y_0),\\,</math>\n\nwith <math>t\\,</math> in <math>(0,1)\\,</math>, and the linear interpolant over the edge is\n\n: <math> f \\sim f_0 + t (f_1-f_0)\\,</math>\n\nSo setting <math> f = 0\\,</math>\n\n: <math>t = -f_0/(f_1-f_0)\\,</math> and <math> (x,y) = (x_0,y_0)-f_0*(x_1-x_0,y_1-y_0)/(f_1-f_0)\\,</math>\n\nSince this only depends on values on the edge, every triangle which shares this edge will produce the same point, so the contour will be continuous. Each triangle can be tested independently, and if all are checked the entire set of contour curves can be found.\n\n== Piecewise linear continuation ==\n\nPiecewise linear continuation is similar to contour plotting (Dobkin, Levy, Thurston and Wilks),<ref name=\"five\">[[David P. Dobkin]], Silvio V. F. Levy, [[William Thurston|William P. Thurston]] and Allan R. Wilks, \"Contour Tracing by Piecewise Linear Approximations\", ''ACM Transactions on Graphics'', 9(4) 389-423, 1990.</ref> but in higher dimensions. The algorithm is based on the following results:\n\n=== Lemma 1 ===\n\n<center>\n{| class=\"wikitable\"\n|-\n| If F(x) maps <math>\\mathbb{R}^n</math> into <math>\\mathbb{R}^{n-1}</math>, there is a unique linear interpolant on an '(n-1)'-dimensional [[simplex]] which agrees with the function values at the vertices of the simplex.\n|}\n</center>\n\nAn '(n-1)'-dimensional simplex has n vertices, and the function F assigns an 'n'-vector to each. The simplex is [[convex set|convex]], and any point within the simplex is a [[convex combination]] of the vertices. That is:\n</center>\nIf x is in the interior of an (n-1)-dimensional simplex with n vertices <math> v_i </math>, then there are positive scalars <math>0<\\alpha_i</math> such that\n</center>\n<center>\n: <math> \\mathbf{x} = \\sum_i \\alpha_i \\mathbf{v}_i </math>\n: <math> \\sum_i \\alpha_i = 1.\\,</math>\n</center>\n\nIf the vertices of the simplex are [[Linear independence|linearly independent]] the non-negative scalars <math>\\alpha</math> are unique for each point x, and are called the [[Barycentric coordinates (mathematics)|barycentric coordinates]] of x. They determine the value of the unique [[interpolation|interpolant]] by the formula:\n<center>\n: <math>LF = \\sum_i \\alpha_i F(\\mathbf{v}_i)</math>\n</center>\n\n=== Lemma 2 ===\n\n<center>\n{| class=\"wikitable\"\n|-\n| An (n-1)-dimensional simplex can be tested to determine if it contains the origin.\n|}\n</center>\n\nThere are basically two tests. The one which was first used labels the vertices of the simplex with a vector of signs (+/-) of the coordinates of the vertex. For example the vertex (.5,-.2,1.) would be labelled (+,-,+). A simplex is called ''completely labelled'' if there is a vertex whose label begins with a string of \"+\" signs of length 0,1,2,3,4,...n. A completely labelled simplex contains a neighborhood of the origin. This may be surprising, but what underlies this result is that for each coordinate of a completely labelled simplex there is a vector with \"+\" and another with a \"-\". Put another way, the smallest cube with edges parallel to the coordinate axes and which covers the simplex has pairs of faces on opposite sides of 0. (i.e. a \"+\" and a \"-\" for each coordinate).\n\nThe second approach is called ''vector labelling''. It is based on the barycentric coordindates of the vertices of the simplex. The first step is to find the barycentric coordinates of the origin, and then the test that the simplex contains the origin is simply that all the barycentric coordinates are positive and the sum is less than 1.\n\n=== Lemma 3 ===\n\n<center>\n{| class=\"wikitable\"\n|-\n| There is a triangulation (the Coxeter-Freudenthal-Kuhn triangulation [1]) which is invariant under the pivot operation\n<center><math>P_{v_i} (v_0,v_1,...,v_n) := (v_0,v_1,...,v_{i-1},\\tilde v_i,v_{i+1},...,v_n)</math></center>\nwhere\n<center>\n<math>\n\\tilde v_i = \\left\\{ \\begin{array}{lcl}\nv_1+v_n-v_0 & &i=0 \\\\\nv_{i+1}+v_{i-1}-v_i&\\qquad\\qquad&0\\\\\nv_{n-1}+v_0-v_n & &i=n \\\\ \\end{array}\\right.\n</math>\n</center>\n|}\n</center>\n\n<center>\n[[Image:Simplical3dOne.gif|The first step of three-dimensional simplicial continuation]] [[Image:Simplical3dTwo.gif|The second step of three-dimensional simplicial continuation]]\n</center>\n\n<center>\n[[Image:Simplicial.gif]]\n</center>\n\n== References ==\n{{reflist}}\n\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Predictor–corrector method",
      "url": "https://en.wikipedia.org/wiki/Predictor%E2%80%93corrector_method",
      "text": "In [[numerical analysis]], '''predictor–corrector methods''' belong to a class of [[algorithm]]s designed to integrate ordinary differential equations{{snd}}to find an unknown function that satisfies a given differential equation.  All such algorithms proceed in two steps: \n\n# The initial, \"prediction\" step, starts from a function fitted to the function-values and derivative-values at a preceding set of points to extrapolate (\"anticipate\") this function's value at a subsequent, new point. \n# The next, \"corrector\" step refines the initial approximation by using the ''predicted'' value of the function and ''another method'' to interpolate that unknown function's value at the '''same''' subsequent point.\n\n== Predictor–corrector methods for solving ODEs ==\n\nWhen considering the [[numerical methods for ordinary differential equations|numerical solution of ordinary differential equations (ODEs)]], a predictor–corrector method typically uses an [[explicit and implicit methods|explicit method]] for the predictor step and an implicit method for the corrector step.\n\n=== Example: Euler method with the trapezoidal rule ===\n\nA simple predictor–corrector method (known as [[Heun's method]]) can be constructed from the [[Euler method]] (an explicit method) and the [[trapezoidal rule (differential equations)|trapezoidal rule]] (an implicit method).\n\nConsider the differential equation\n\n: <math> y' = f(t,y), \\quad y(t_0) = y_0, </math>\n\nand denote the step size by <math>h</math>.\n\nFirst, the predictor step: starting from the current value <math>y_i</math>, calculate an initial guess value <math>\\tilde{y}_{i+1}</math> via the Euler method,\n\n: <math>\\tilde{y}_{i+1} = y_i + h f(t_i,y_i). </math>\n\nNext, the corrector step: improve the initial guess using trapezoidal rule,\n\n: <math> y_{i+1} = y_i + \\tfrac12 h \\bigl( f(t_i, y_i) + f(t_{i+1},\\tilde{y}_{i+1}) \\bigr). </math>\n\nThat value is used as the next step.\n\n=== PEC mode and PECE mode ===\n\nThere are different variants of a predictor–corrector method, depending on how often the corrector method is applied. The Predict–Evaluate–Correct–Evaluate (PECE) mode refers to the variant in the above example:\n\n: <math> \\begin{align}\n\\tilde{y}_{i+1} &= y_i + h f(t_i,y_i), \\\\\ny_{i+1} &= y_i + \\tfrac12 h \\bigl( f(t_i, y_i) + f(t_{i+1},\\tilde{y}_{i+1}) \\bigr). \n\\end{align} </math>\n\nIt is also possible to evaluate the function ''f'' only once per step by using the method in Predict–Evaluate–Correct (PEC) mode:\n\n: <math> \\begin{align}\n\\tilde{y}_{i+1} &= y_i + h f(t_i,\\tilde{y}_i), \\\\\ny_{i+1} &= y_i + \\tfrac12 h \\bigl( f(t_i, \\tilde{y}_i) + f(t_{i+1},\\tilde{y}_{i+1}) \\bigr). \n\\end{align} </math>\n\nAdditionally, the corrector step can be repeated in the hope that this achieves an even better approximation to the true solution. If the corrector method is run twice, this yields the PECECE mode:\n\n: <math> \\begin{align}\n\\tilde{y}_{i+1} &= y_i + h f(t_i,y_i), \\\\\n\\hat{y}_{i+1} &= y_i + \\tfrac12 h \\bigl( f(t_i, y_i) + f(t_{i+1},\\tilde{y}_{i+1}) \\bigr), \\\\\ny_{i+1} &= y_i + \\tfrac12 h \\bigl( f(t_i, y_i) + f(t_{i+1},\\hat{y}_{i+1}) \\bigr). \n\\end{align} </math>\n\nThe PECEC mode has one fewer function evaluation. More generally, if the corrector is run ''k'' times, the method is in P(EC)<sup>''k''</sup>\nor P(EC)<sup>''k''</sup>E mode. If the corrector method is iterated until it converges, this could be called PE(CE)<sup>∞</sup>.<ref>{{harvnb|Butcher|2003|p=104}}</ref>\n\n== See also ==\n\n* [[Backward differentiation formula]]\n* [[Beeman's algorithm]]\n* [[Heun's method]]\n* [[Mehrotra predictor–corrector method]]\n* [[Numerical continuation]]\n\n==Notes==\n{{reflist}}\n\n==References==\n* {{Citation | last1=Butcher | first1=John C. | author1-link=John C. Butcher | title=Numerical Methods for Ordinary Differential Equations | publisher=[[John Wiley & Sons]] | location=New York | isbn=978-0-471-96758-3 | year=2003}}.\n*{{Cite book | last1=Press | first1=WH | last2=Teukolsky | first2=SA | last3=Vetterling | first3=WT | last4=Flannery | first4=BP | year=2007 | title=Numerical Recipes: The Art of Scientific Computing | edition=3rd | publisher=Cambridge University Press |  publication-place=New York | isbn=978-0-521-88068-8 | chapter=Section 17.6. Multistep, Multivalue, and Predictor-Corrector Methods | chapter-url=http://apps.nrbook.com/empanel/index.html#pg=942}}\n\n== External links ==\n\n* {{MathWorld |title=Predictor-Corrector Methods |urlname=Predictor-CorrectorMethods}}\n* [http://www.fisica.uniud.it/~ercolessi/md/md/node22.html Predictor–corrector methods] for differential equations\n\n{{DEFAULTSORT:Predictor-corrector method}}\n[[Category:Algorithms]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Principles of grid generation",
      "url": "https://en.wikipedia.org/wiki/Principles_of_grid_generation",
      "text": "\nA '''grid''' is a small-sized geometrical shape that covers the physical domain, whose objective is to identify the [[discrete space|discrete]] volumes or elements where [[Conservation law (physics)|conservation law]]s can be applied. Grid generation is the first process involved in computing [[numerical analysis|numerical]] solutions to the equations that describe a physical process. The result of the [[solution]] depends upon the quality of the grid. A well-constructed grid can improve the quality of solution whereas, deviations from the numerical solution can be observed with a poorly constructed grid.\n[[Scientific technique|Technique]]s for creating the cell forms the basis of grid generation. Various methods to generate grids are discussed below.\n\n==Algebraic methods ==\n[[File:Algebraic methods 1.png|thumb|Nozzle Geometry]]\n[[File:Algebraic methods 2.png|thumb|Computational Mesh in Physical Space]]\nThe grid generation by algebraic methods is done by using known functions in one, two or three [[dimensions]] taking arbitrary shaped regions. The computational domain might not be rectangular, but for the sake of simplicity, the domain is taken to be rectangular. The simplest procedure that may be used to produce boundary fitted computational mesh is the normalization transformation.<ref>{{cite book|last=Anderson|first=Dale|title=Computational Fluid Mechanics and Heat Transfer, Third Edition Series in Computational and Physical Processes in Mechanics and Thermal Sciences|year=2012|publisher=CRC Press|isbn=978-1591690375|pages=679–712|url=https://books.google.com/?id=Cv4IERczJ4oC&dq=computational+fluid+dynamics+anderson}}</ref><br>\nFor a nozzle, with the describing function <math>y = x^2</math> the grid can easily be generated using uniform division in y-direction with equally spaced increments in x-direction, which are described by\n\n: <math>\\xi=x \\, </math>\n: <math>\\eta = \\frac{y}{y_\\max} \\, </math>\n\nwhere <math>y_\\max</math> denotes the y-coordinate of the nozzle wall. For given values of (<math>\\xi</math>, <math>\\eta</math>) the values of (<math>x</math>, <math>y</math>) can be easily  recovered.\n\n==Differential equation methods ==\nLike algebraic methods, differential equation methods are also used to generate grids. The advantage of using the [[partial differential equations]] (PDEs) is that the solution of grid generating equations can be exploited to generate the mesh. Grid construction can be done using all three classes of partial differential equations.\n\n===Elliptic schemes===\nElliptic [[Partial differential equation|PDEs]] generally have very smooth solutions leading to smooth contours. Using its smoothness as an advantage [[Laplace's equation|Laplace Equations]] can preferably be used because the [[Jacobian matrix and determinant|Jacobian]] found out to be positive as a result of maximum principle for [[harmonic function]]s. After extensive work done by Crowley (1962) and Winslow (1966)<ref>{{cite journal|last=Winslow|first=A|title=Numerical Solution of Quasi-linear Poisson Equation|journal=J. Comput. Phys.|year=1966|volume=1|issue=2|pages=149–172|doi=10.1016/0021-9991(66)90001-5}}</ref> on PDEs by transforming physical domain into computational plane while mapping using [[Poisson's equation|Poisson’s equation]], Thompson et al. (1974)<ref>{{cite journal|last=Thompson|first=J.F.|author2=Thames, F.C. |author3=Mastin, C.W. |title=Automatic Numerical Generation of Body-fitted Curvilinear Coordinate System for Field  Containing any Number of Arbitrary Two-Dimensional Bodies|journal=J. Comput. Phys.|year=1974|volume=15|issue=3|pages=299–319|doi=10.1016/0021-9991(74)90114-4}}</ref>  have worked extensively on elliptic [[Partial differential equation|PDEs]] to generate grids. In Poisson grid generators, the mapping is accomplished by marking the desired grid points (x, y) on the boundary of the physical domain, with the interior point distribution determined through the solution of equations written below\n\n: <math> \\xi_{xx} + \\xi_{yy} = P(\\xi, \\eta)</math>\n: <math> \\eta_{xx} + \\eta_{yy} = Q(\\xi, \\eta)</math>\n\nwhere (''ξ'', ''η'') are the co-ordinates in the computational domain, while P and Q are responsible for point spacing within D. Transforming above equations in computational space yields a set of two elliptical PDEs of the form,\n\n: <math>\\alpha x_{\\xi\\xi} -2\\beta x_{\\xi\\eta} + \\gamma x_{\\eta\\eta} = -I^2 (Px_\\xi + Qx_\\eta) </math>\n\n: <math>\\alpha y_{\\xi\\xi} -2\\beta y_{\\xi\\eta} + \\gamma y_{\\eta\\eta} = -I^2 (Py_\\xi + Qy_\\eta)</math>\n\nwhere\n\n: <math>\n\\begin{align}\n\\alpha & = x^2_\\eta + y^2_\\eta \\\\\n\\beta & = x_\\eta x_\\xi + y_\\xi y_\\eta \\\\\n\\gamma & = x^2_\\xi + y^2_\\xi \\\\\nI & = \\frac{\\delta(x, y)}{\\delta(\\xi, \\eta)} = y_\\eta x_\\xi - y_\\xi x_\\eta\n\\end{align}\n</math>\n\nThese systems of equations are solved in the computational plane on uniformly spaced grid which provides us with the (''x'',&nbsp;''y'') co-ordinates of each point in physical space. The advantage of using Elliptical PDEs is the solution linked to them is smooth and the resulting grid is smooth. But, specification of P and Q becomes a difficult task thus adding it to its disadvantages. Moreover, the grid has to be computed after each time step which adds up to computational time.<ref>{{cite journal|last=Young|first=David|title=Iterative methods for solving partial difference equations of elliptic type|journal=Transactions of the American Mathematical Society|year=1954|volume=76|issue=1|pages=92–111|issn=1088-6850|url=http://www.ams.org/journals/tran/1954-076-01/S0002-9947-1954-0059635-7/|doi=10.2307/1990745|jstor=1990745}}</ref>\n\n===Hyperbolic schemes===\nThis grid generation scheme is generally applicable to problems with open domains consistent with the type of [[Partial differential equation|PDE]] describing the physical problem. The advantage associated with Hyperbolic PDEs is that the governing equations need to be solved only once for generating grid. The initial point distribution along with the approximate boundary conditions forms the required input and the solution is the then marched outward. Steger and Sorenson (1980)<ref>{{cite journal|last=Steger|first=J.L|author2=Sorenson, R.L|title=Use of hyperbolic partial differential equation to generate body fitted coordinates, Numerical Grid Generation Techniques|journal=NASA Conference Publication 2166|year=1980|pages=463–478}}</ref> proposed a volume orthogonality method that uses Hyperbolic PDEs for mesh generation.\nFor a 2-D problem, Considering computational space to be given by <math>\\Delta\\xi = \\Delta\\eta = 1 </math>, the inverse of the [[Jacobian matrix and determinant|Jacobian]] is given by,\n\n: <math>x_\\xi y_\\eta - x_\\eta y_\\xi = I</math>\n\nwhere <math>I</math> represents the area in physical space for a given area in computational space. The second equation links the orthogonality of grid lines at the boundary in physical space which can be written as\n\n: <math>d\\xi = 0 = \\xi_x \\, dx + \\xi_y \\, dy.</math>\n\nFor <math>\\xi</math> and <math>\\eta</math> surfaces to be perpendicular the equation becomes\n\n: <math>x_\\xi x_\\eta + y_\\xi y_\\eta = 0.</math>\n\nThe problem associated with such system of equations is the specification of <math>I</math>. Poor selection of <math>I</math> may lead to shock and discontinuous propagation of this information throughout the mesh. While mesh being orthogonal is generated very rapidly which comes out as an advantage with this method.\n\n===Parabolic schemes===\nThe solving technique is similar to that of hyperbolic [[PDEs]] by advancing the solution away from the initial data surface satisfying the boundary conditions at the end. Nakamura (1982) and Edwards (1985) developed the basic ideas for parabolic grid generation. The idea uses either of [[Laplace]] or the [[Poisson's equation]] and especially treating the parts which controls elliptic behavior. The initial values are given as the coordinates of the point along the surface <math>\\eta = 0</math> and the advancing the solutions to the outer surface of the object satisfying the boundary conditions along <math>\\xi</math> edges.\n\nThe control of the grid spacing has not been suggested till now. Nakamura and Edwards, grid control was accomplished using non uniform spacing. The parabolic grid generation shows an advantage over the hyperbolic grid generation that, no shocks or discontinuities occur and the grid is relatively smooth. The specifications of initial values and selection of step size to control the grid points is however time consuming, but these techniques can be effective when familiarity and experience is gained.\n\n==Variational method==\nThis method includes a technique that minimizes [[Grid (spatial index)|grid]] smoothness, [[orthogonality]] and volume variation. This method forms mathematical platform to solve grid generation problems. In this method an alternative grid is generated by a new [[mesh]] after each iteration and computing the grid speed using [[backward difference method]]. This technique is a powerful one with a disadvantage that effort is required to solve the equations related to grid. Further work needed to be done to minimize the [[integrals]] that will reduce the CPU time.\n\n==Unstructured grid generation==\nSee also [[Mesh generation]]. \nThe main importance of this scheme is that it provides a method that will generate the grid automatically. Using this method, grids are segmented into blocks according to the surface of the element and a structure is provided to ensure appropriate connectivity. To interpret the data [[Fluid dynamics|flow]] solver is used. When an unstructured scheme is employed, the main interest is to fulfill the demand of the user and a grid generator is used to accomplish this task.  The information storage in structured scheme is [[cell (geometry)|cell]] to cell instead of grid to grid and hence the more memory space is needed. Due to random cell location, the solver [[efficiency]] in unstructured is less as compared to the structured scheme.<ref>{{cite journal|last=Venkatakrishnan|first=V|author2=Mavriplis, D. J|title=Implicit solvers for unstructured meshes|journal=Journal of Computational Physics|date=May 1991|volume=105|issue=1|page=23|doi=10.1006/jcph.1993.1055 }}</ref> \n\nSome points are needed to be kept in mind at the time of grid [[construction]]. The grid point with high resolution creates difficulty for both structured and unstructured. For example, in case of [[boundary layer]], structured scheme produces elongated grid in the direction of flow. On the other hand unstructured grids require a higher cell [[density]] in the boundary layer because the cell needs to be as [[equilateral]] as possible to avoid errors.<ref>{{cite journal|last=Weatherill|first=N.P|title=Delaunay triangulation in computational fluid dynamics|journal=Computers & Mathematics with Applications|date=September 1992|volume=24|issue=5–6|pages=129–150|doi=10.1016/0898-1221(92)90045-j}}</ref>\n\n===Connectivity information===\n\nWe must identify what information is required to identify the cell and all the neighbors of the cell in the [[computational grid|computational]] mesh. We can choose to locate the [[arbitrary]] points anywhere we want for the unstructured grid. A point insertion scheme is used to insert the points independently and the cell connectivity is determined. This suggests that the point be identified as they are inserted.    \n[[Logic]] for establishing new connectivity is determined once the points are inserted. Data that form grid point that identifies grid cell are needed. As each cell is formed it is numbered and the points are sorted. In addition the neighbor cell information is needed.\n\n==Adaptive grid==\n\nA problem in solving [[partial differential equation]]s using previous methods is that the grid is constructed and the points are distributed in the physical domain before details of the solution is known. So the grid may or may not be the best for the given problem.<ref>{{cite journal|last=Anderson|first=D.A|author2=Sharpe H.N.|title=Orthogonal Adaptive Grid Generation with Fixed Internal Boundaries for Oil Reservoir Simulation|journal=SPE Advanced Technology Series|date=July 1993|volume=1|issue=2|series=2|pages=53–62|doi=10.2118/21235-PA|url=http://www.onepetro.org/mslib/servlet/onepetropreview?id=00021235}}</ref> \n\nAdaptive methods are used to improve the [[accuracy]] of the solutions. The adaptive method is referred to as ‘h’ method if mesh refinement is used, ‘r’ method if the number of grid point is fixed and not redistributed and ‘p’ if the order of solution scheme is increased in finite-element theory. The multi dimensional problems using the equidistribution scheme can be accomplished in several ways. The simplest to understand are the Poisson Grid Generators with control function based on the equidistribution of the weight function with the [[diffusion]] set as a multiple of desired cell volume. The equidistribution scheme can also be applied to the unstructured problem. The problem is the connectivity hampers if mesh point movement is very large.\n    \n[[Steady flow]] and the time-accurate flow calculation can be solved through this adaptive method.  The grid is refined and after a predetermined number of iteration in order to adapt it in a steady flow problem. The grid will stop adjusting to the changes once the solution converges. In time accurate case coupling of the Partial Differential Equations of the physical problem and those describing the grid movement is required.\n\n== See also ==\n*[[Mesh generation]]\n*[[Types of mesh]]\n\n==References==\n{{Reflist}}\n\n[[Category:Mesh generation]]\n[[Category:Numerical analysis]]\n[[Category:Numerical differential equations]]\n[[Category:Triangulation (geometry)]]"
    },
    {
      "title": "Probability box",
      "url": "https://en.wikipedia.org/wiki/Probability_box",
      "text": "{{Other uses of|p-box|P-box (disambiguation){{!}}P-box}}\n [[Image:continuous p-box gamma(2,interval(2,4)),steps=500.png|thumb|300px|alt=A continuous p-box depicted as a graph with abscissa labeled X and ordinate labeled Probability|A p-box (probability box).]]\n\nA '''probability box''' (or '''p-box''') is a characterization of an [[uncertain number]] consisting of both [[uncertainty quantification#Aleatoric and epistemic uncertainty|aleatoric and epistemic uncertainties]] that is often used in [[risk analysis]] or quantitative [[uncertainty]] modeling where numerical calculations must be performed.  [[Probability bounds analysis]] is used to make arithmetic and logical calculations with p-boxes.\n\nAn example p-box is shown in the figure at right for an uncertain number ''x'' consisting of a left (upper) bound and a right (lower) bound on the probability distribution for ''x''.  The bounds are coincident for values of ''x'' below 0 and above 24.  The bounds may have almost any shape, including step functions, so long as they are monotonically increasing and do not cross each other.  A p-box is used to express simultaneously incertitude (epistemic uncertainty), which is represented by the breadth between the left and right edges of the p-box, and variability (aleatory uncertainty), which is represented by the overall slant of the p-box.\n\n==Interpretation==\n{{multiple image\n   | direction = vertical\n   | width     = 160\n   | footer    = '''Dual interpretation of p-boxes'''\n   | image1    = Continuous p-box showing interval probability interval(0.4, 0.36) that x is 2.5 or less.png\n   | alt1      = p-box with dotted lines showing probability interval associated with an ''x''-value\n   | caption1  = Probability that ''x'' is 2.5 or less is between 4% and 36%\n   | image2    = Continuous p-box showning interval 95th percentile interval(9,16).png\n   | alt2      = P-box with dotted lines showing interval 95th percentile\n   | caption2  = 95th percentile is between 9 and 16\n  }}\nThere are dual interpretations of a p-box.  It can be understood as [[Upper and lower bounds|bounds]] on the cumulative probability associated with any ''x''-value.  For instance, in the p-box depicted at right, the probability that the value will be 2.5 or less is between 4% and 36%.  A p-box can also be understood as bounds on the ''x''-value at any particular probability level.  In the example, the 95th percentile is sure to be between 9 and 16.\n\nIf the left and right bounds of a p-box are sure to enclose the unknown distribution, the bounds are said to be ''rigorous'', or absolute.  The bounds may also be the tightest possible such bounds on the distribution function given the available information about it, in which case the bounds are therefore said to be ''best-possible''.  It may commonly be the case, however, that not every distribution that lies within these bounds is a possible distribution for the uncertain number, even when the bounds are rigorous and best-possible.\n\n==Mathematical definition==\nP-boxes are specified by left and right bounds on the [[cumulative distribution function|cumulative probability distribution function]] (or, equivalently, the [[survival function]]) of a quantity and, optionally, additional information about the quantity’s [[mean]], [[variance]] and distributional shape (family, unimodality, symmetry, etc.).  A p-box represents a class of probability distributions consistent with these constraints.\n\nLet &#x1D53B; denote the space of distribution functions on the [[real number]]s ℝ, i.e., &#x1D53B; = {''D'' | ''D'' : ℝ → [0,1], ''D''(''x'') ≤ ''D''(''y'') whenever ''x'' < ''y'', for all ''x'', ''y'' [[Naive set theory#Sets.2C membership and equality|∈]] ℝ}, and let &#x1D540; denote the set of real [[Interval (mathematics)|intervals]], i.e., &#x1D540; = {''i'' | ''i'' = [''i''<sub>1</sub>, ''i''<sub>2</sub>], ''i''<sub>1</sub> ≤ ''i''<sub>2</sub>, ''i''<sub>1</sub>, ''i''<sub>2</sub> ∈ ℝ}.  Then a p-box is a quintuple {''{{overbar|F}}'', <u>''F''</u>, ''m'', ''v'', '''F'''}, where ''{{overbar|F}}'', <u>''F''</u> ∈ &#x1D53B;, while ''m'', ''v'' ∈ &#x1D540;, and '''F''' ⊆ &#x1D53B;.  This quintuple denotes the set of distribution functions ''F'' ∈ &#x1D53B; matching the following constraints: <!--are the formulas only good for non-negative random variables?-->\n\n:<u>''F''</u> (''x'') ≤ ''F''(''x'') ≤ ''{{overbar|F}}''(''x''),\n:<big>∫ </big>{{su|p=∞|b=−∞}} ''x'' [[Riemann–Stieltjes integral|d''F''(''x'')]] [[Naive set theory#Sets.2C membership and equality|∈]]  ''m'',\n:<big>∫</big>{{su|p=∞|b=−∞}} ''x''<sup>2</sup>&nbsp;d''F''(''x'') – <big>(∫ </big>{{su|p=∞|b=−∞}} x d''F''(''x'')<big>)</big><sup>2</sup> [[Naive set theory#Sets.2C membership and equality|∈]] ''v'', and\n:''F'' [[Naive set theory#Sets.2C membership and equality|∈]] '''F'''.\n\nThus, the constraints are that the distribution function ''F'' falls within prescribed bounds, the mean of the distribution (given by the [[Riemann–Stieltjes integral]]) is in the interval ''m'', the variance of the distribution is in the interval ''v'', and the distribution is within some admissible class of distributions '''F'''.\nThe Riemann–Stieltjes integrals do not depend on the differentiability of ''F''.\n\nP-boxes serve the same role for [[random variables]] that [[upper and lower probabilities]] serve for [[event (probability theory)|events]].  In [[robust Bayes analysis]]<ref>Berger, J.O. (1984). The robust Bayesian viewpoint. Pages 63–144 in ''Robustness of Bayesian analyses'', edited by J.B. Kadane, Elsevier Science.</ref> a p-box is also known as a [[distribution band]].<ref>Basu, S. (1994). [https://www.jstor.org/pss/25050991 Variations of posterior expectations for symmetric unimodal priors in a distribution band]. ''[[Sankhya (journal)|Sankhyā: The Indian Journal of Statistics]], Series A'' '''56''': 320–334.</ref><ref>Basu, S., and A. DasGupta (1995). [http://www.math.niu.edu/~basu/Research/dband.ps Robust Bayesian analysis with distribution bands]. ''Statistics and Decisions'' '''13''': 333–349.</ref>  A p-box can be constructed as a closed neighborhood of a distribution ''F'' ∈ &#x1D53B; under the [[Kolmogorov–Smirnov test|Kolmogorov]], [[Lévy metric|Lévy]] or [[Wasserstein metric]].  A p-box is a crude but computationally convenient kind of [[credal set]].  Whereas a credal set is defined solely in terms of the constraint '''F''' as a convex set of distributions (which automatically determine ''{{overbar|F}}'', <u>''F''</u>, ''m'', and ''v'', but are often very difficult to compute with), a p-box usually has a loosely constraining specification of '''F''', or even no constraint so that '''F '''= &#x1D53B;.  Calculations with p-boxes, unlike credal sets, are often quite efficient, and algorithms for all standard mathematical functions are known.\n\nA p-box is minimally specified by its left and right bounds, in which case the other constraints are understood to be vacuous as {''{{overbar|F}}'', <u>''F''</u>, [– <small><math>\\infty</math></small>, <math>\\infty</math>], [0, <math>\\infty</math>], &#x1D53B;}.  Even when these ancillary constraints are vacuous, there may still be nontrivial bounds on the mean and variance that can be inferred from the left and right edges of the p-box.\n\n==Where p-boxes come from==\nP-boxes may arise from a variety of kinds of incomplete information about a quantity, and there are several ways to obtain p-boxes from data and analytical judgment.\n\n===Distributional p-boxes===\nWhen a probability distribution is known to have a particular shape (e.g., normal, uniform, beta, Weibull, etc.) but its parameters can only be specified imprecisely as intervals, the result is called a distributional p-box, or sometimes a parametric p-box.  Such a p-box is usually easy to obtain by enveloping extreme distributions given the possible parameters.  For instance, if a quantity is known to be normal with mean somewhere in the interval [7,8] and standard deviation within the interval [1,2], the left and right edges of the p-box can be found by enveloping the distribution functions of four probability distributions, namely, normal(7,1), normal(8,1), normal(7,2), and normal(8,2), where normal(μ,σ) represents a normal distribution with mean μ and standard deviation σ.  All probability distributions that are normal and have means and standard deviations inside these respective intervals will have distribution functions that fall entirely within this p-box.  The left and right bounds enclose many non-normal distributions, but these would be excluded from the p-box by specifying normality as the distribution family.\n\n===Distribution-free p-boxes===\nEven if the parameters such as mean and variance of a distribution are known precisely, the distribution cannot be specified precisely if the distribution family is unknown.  In such situations, envelopes of all distributions matching given moments can be constructed from inequalities such as those due to [[Markov's inequality|Markov]], [[Chebyshev's inequality|Chebyshev]], [[Chebyshev's inequality#Variant: One-sided Chebyshev inequality|Cantelli]], or Rowe<ref>Rowe, N.C. (1988). [http://faculty.nps.edu/ncrowe/unary2.htm Absolute bounds on the mean and standard deviation of transformed data for constant-sign-derivative transformations]. ''SIAM Journal of Scientific and Statistical Computing'' '''9''': 1098–1113.</ref><ref>Smith, J.E. (1995). Generalized Chebychev inequalities: theory and applications in decision analysis. ''Operations Research'' '''43''': 807–825.</ref> that enclose all distribution functions having specified parameters.  These define distribution-free p-boxes because they make no assumption whatever about the family or shape of the uncertain distribution.  When qualitative information is available, such as that the distribution is [[Unimodal#Unimodal probability distribution|unimodal]], the p-boxes can often be tightened substantially.<ref>\nZhang, J. and D. Berleant (2005). Arithmetic on random variables: squeezing the envelopes with new joint distribution constraints. Pages 416–422 in ''Proceedings of the Fourth International Symposium On Imprecise Probabilities and Their Applications (ISIPTA ’05)'', Carnegie Mellon University, Pittsburgh, July 20–23, 2005.</ref>\n\n===P-boxes from imprecise measurements===\nWhen all members of a population can be measured, or when random sample data are abundant, analysts often use an [[empirical distribution function|empirical distribution]] to summarize the values.  When those data have non-negligible [[measurement uncertainty]] represented by interval ranges about each sample value, an empirical distribution may be generalized to a p-box.<ref name=Ferson-etal-2007>Ferson, S., V. Kreinovich, J. Hajagos, W. Oberkampf, and L. Ginzburg (2007). [http://www.ramas.com/intstats.pdf ''Experimental Uncertainty Estimation and Statistics for Data Having Interval Uncertainty'']. Sandia National Laboratories, SAND 2007-0939, Albuquerque, NM.</ref>  Such a p-box can be specified by cumulating the lower endpoints of all the interval measurements into a cumulative distribution forming the left edge of the p-box, and cumulating the upper endpoints to form the right edge.  The broader the measurement uncertainty, the wider the resulting p-box.\n\nInterval measurements can also be used to generalize distributional estimates based on the [[method of moments (statistics)|method of matching moments]] or [[maximum likelihood]], that make shape assumptions such as normality or lognormality, etc.<ref name=Ferson-etal-2007 /><ref>Xiang, G., V. Kreinovich and S. Ferson, (2007). Fitting a normal distribution to interval and fuzzy data. Pages 560–565 in ''Proceedings of the 26th International Conference of the North American Fuzzy Information Processing Society NAFIPS'2007'', M. Reformat and M.R. Berthold (eds.).</ref>  Although the measurement uncertainty can be treated rigorously, the resulting distributional p-box generally will not be rigorous when it is a sample estimate based on only a subsample of the possible values. But, because these calculations take account of the dependence between the parameters of the distribution, they will often yield tighter p-boxes than could be obtained by treating the interval estimates of the parameters as unrelated as is done for distributional p-boxes.\n\n===Confidence bands===\nThere may be uncertainty about the shape of a probability distribution because the sample size of the empirical data characterizing it is small.  Several methods in traditional statistics have been proposed to account for this sampling uncertainty about the distribution shape, including [[Kolmogorov–Smirnov test#Setting confidence limits for the shape of a distribution function|Kolmogorov–Smirnov]]<ref>Kolmogorov, A. (1941). Confidence Limits for an Unknown Distribution Function. ''Annals of Mathematical Statistics'' '''12''': 461–463.</ref> and similar<ref>Owen, A.B. (1995). Nonparametric likelihood confidence bands for a distribution function. ''Journal of the American Statistical Association'' '''90''': 516–521.</ref> [[confidence band]]s, which are [[distribution-free]] in the sense that they make no assumption about the shape of the underlying distribution.  There are related confidence-band methods that do make assumptions about the shape or family of the underlying distribution, which can often result in tighter confidence bands.<ref>Cheng, R.C.H., and T.C. Iles (1983). Confidence bands for cumulative distribution functions of continuous random variables. ''Technometrics'' '''25''': 77–86.</ref><ref>Cheng, R.C.H., B.E. Evans and J.E. Williams (1988). Confidence band estimations for distributions used in probabilistic design. ''International Journal of Mechanical Sciences'' '''30''': 835–845.</ref><ref name=murphy95>Murphy, S.A. (1995). [http://www.stat.lsa.umich.edu/~samurphy/papers/conf_int.pdf Likelihood ratio-based confidence intervals in survival analysis]. ''Journal of the American Statistical Association'' '''90''': 1399–1405.</ref>  Constructing confidence bands requires one to select the probability defining the confidence level, which usually must be less than 100% for the result to be non-vacuous.  Confidence bands at the (1&nbsp;−&nbsp;α)% confidence level are defined such that, (1&nbsp;−&nbsp;α)% of the time they are constructed, they will completely enclose the distribution from which the data were randomly sampled.  A confidence band about a distribution function is sometimes used as a p-box even though it represents statistical rather than rigorous or sure bounds.  This use implicitly assumes that the true distribution, whatever it is, is inside the p-box.\n\nAn analogous Bayesian structure is called a Bayesian p-box,<ref>Montgomery, V. (2009). [http://maths.dur.ac.uk/stats/people/fc/thesis-VM.pdf ''New Statistical Methods in Risk Assessment by Probability Bounds'']. Ph.D. dissertation, Durham University, UK.</ref> which encloses all distributions having parameters within a subset of parameter space corresponding to some specified probability level from a Bayesian analysis of the data.  This subset is the [[credible interval|credible region]] for the parameters given the data, which could be defined as the highest posterior probability density region, or the lowest posterior loss region, or in some other suitable way.  To construct a Bayesian p-box one must select a prior distribution, in addition to specifying the credibility level (analogous to a confidence level).\n\n===C-boxes===\nC-boxes (or confidence structures<ref name=balch>M.S. Balch (2012). Mathematical foundations for a theory of confidence structures. ''International Journal of Approximate Reasoning'' 53: 1003–1019.</ref>) are estimators of fixed, real-valued quantities that depend on random sample data and encode Neyman<ref>J. Neyman (1937). Outline of a theory of statistical estimation based on the classical theory of probability. ''Philosophical Transactions of the Royal Society'' A237: 333–380.</ref> [[confidence interval]]s at every level of confidence.<ref>Confidence boxes [https://sites.google.com/site/confidenceboxes/ website].</ref><ref name=isipta>Ferson, S., M. Balch, K. Sentz, and J. Siegrist. 2013. Computing with confidence. ''Proceedings of the 8th International Symposium on Imprecise Probability: Theories and Applications'', edited by F. Cozman, T. Denoeux, S. Destercke and T. Seidenfeld. SIPTA, Compiègne, France.</ref><ref name=balch /> They characterize the inferential uncertainty about the estimate in the form of a collection of focal intervals (or sets), each with associated confidence (probability) mass. This collection can be depicted as a p-box and can project the confidence interpretation through [[probability bounds analysis]].  \n\nUnlike traditional confidence intervals which cannot usually be propagated through mathematical calculations, c-boxes can be used in calculations in ways that preserve the ability to obtain arbitrary confidence intervals for the results.<ref name=icvram>Ferson, S., J O’Rawe and M. Balch. 2014. Computing with confidence: imprecise posteriors and predictive distributions. ''Proceedings of the International Conference on Vulnerability and Risk Analysis and Management and International Symposium on Uncertainty Modeling and Analysis''.</ref><ref name=isipta /> For instance, they can be used to compute probability boxes for both prediction and tolerance distributions. \n  \nC-boxes can be computed in a variety of ways directly from random sample data.  There are confidence boxes for both parametric problems where the family of the underlying distribution from which the data were randomly generated is known (including normal, lognormal, exponential, Bernoulli, binomial, Poisson), and nonparametric problems in which the shape of the underlying distribution is unknown.<ref name=icvram /> Confidence boxes account for the uncertainty about a parameter that comes from the inference from observations, including the effect of small sample size, but also potentially the effects of imprecision in the data and demographic uncertainty which arises from trying to characterize a continuous parameter from discrete data observations.\n\nC-boxes are closely related to several other concepts. They are comparable to [[bootstrapping|bootstrap distributions]]<ref>B. Efron (1998). R.A. Fisher in the 21st century. ''Statistical Science'' 13: 95–122.</ref>, and are imprecise generalizations of traditional [[confidence distribution]]s such as [[Student%27s_t-distribution|Student's ''t''-distribution]]. Like it, c-boxes encode frequentist confidence intervals for parameters of interest at every confidence level. They are analogous to Bayesian [[Posterior_probability|posterior distributions]] in that they characterize the inferential uncertainty about statistical parameters estimated from sparse or imprecise sample data, but they can have a purely frequentist interpretation that makes them useful in engineering because they offer a guarantee of statistical performance through repeated use. In the case of the Bernoulli or binomial rate parameter, the c-box is mathematically equivalent to Walley's imprecise beta model<ref>P. Walley (1996). Inferences from multinomial data: learning about a bag of marbles. ''Journal of the Royal Statistical Society, Series B'' 58: 3–57.</ref><ref>J.-M. Bernard (2005). An introduction to the imprecise Dirichlet model for multinomial data. ''International Journal of Approximate Reasoning'' 39: 123–150.</ref> with the parameter ''s''=1, which is a special case of the [[imprecise Dirichlet process]], a central idea in [[robust Bayes analysis]].\n\nUnlike [[Probability_box#Confidence_bands|confidence bands]] which are confidence limits about an entire distribution function at some particular confidence level, c-boxes encode confidence intervals about a fixed quantity at all possible confidence levels at the same time.\n\n===Envelopes of possible distributions===\nWhen there are multiple possible probability distributions that might describe a variable, and an analyst cannot discount any of them based on available information, a p-box can be constructed as the envelope of the various cumulative distributions.<ref name=Fersonetal03>Ferson, S., V. Kreinovich, L. Ginzburg, D.S. Myers, and K. Sentz (2003). [http://www.ramas.com/unabridged.zip ''Constructing Probability Boxes and Dempster–Shafer Structures''] {{webarchive|url=https://web.archive.org/web/20110722073459/http://www.ramas.com/unabridged.zip |date=2011-07-22 }}. Sandia National Laboratories, SAND2002-4015, Albuquerque, NM.</ref><ref>Fu, G., D. Butler, S.-T. Khu, and S. Sun (2011). Imprecise probabilistic evaluation of sewer flooding in urban drainage systems using random set theory. ''Water Resources Research'' '''47''': W02534.</ref>  It is also possible to account for the uncertainty about which distribution is the correct one with a sensitivity study, but such studies become more complex as the number of possible distributions grows, and combinatorially more complex as the number of variables about which there could be multiple distributions increases.  An enveloping approach is more conservative about this uncertainty than various alternative approaches to handle the uncertainty which average together distributions in [[stochastic mixture]] models or [[Bayesian model average]]s.  The unknown true distribution is likely to be within the class of distributions encompassed by the p-box.  In contrast, assuming the true distribution is one of the distributions being averaged, the average distribution is sure to be unlike the unknown true distribution.\n\n===P-boxes from calculation results===\nP-boxes can arise from computations involving probability distributions, or involving both a probability distribution and an interval, or involving other p-boxes.   For example, the sum of a quantity represented by a probability distribution and a quantity represented by an interval will generally be characterized by a p-box.<ref>Ferson, S., and L.R. Ginzburg (1996). Different methods are needed to propagate ignorance and variability. ''Reliability Engineering and Systems Safety'' '''54''': 133–144.</ref>  The sum of two random variables characterized by well-specified probability distributions is another precise probability distribution typically only when the [[copula (statistics)|copula]] (dependence function) between the two summands is completely specified.  When their dependence is unknown or only partially specified, the sum will be more appropriately represented by a p-box because different dependence relations lead to many different distributions for the sum.  [[Kolmogorov]] originally asked what bounds could be placed about the distribution of a sum when nothing is known about the dependence between the distributions of the addends.<ref name=Frank-etal-1987>Frank, M.J., R.B. Nelsen and B. Schweizer (1987). Best-possible bounds for the distribution of a sum—a problem of Kolmogorov. ''Probability Theory and Related Fields'' '''74''': 199–211.</ref>  The question was only answered in the early 1980s.  Since that time, formulas and algorithms for sums have been generalized and extended to differences, products, quotients and other binary and unary functions under various dependence assumptions.<ref name=Frank-etal-1987 /><ref>Yager, R.R. (1986). Arithmetic and other operations on Dempster–Shafer structures. ''International Journal of Man-machine Studies'' '''25''': 357–366.</ref><ref>Williamson, R.C., and T. Downs (1990). Probabilistic arithmetic I: Numerical methods for calculating convolutions and dependency bounds. ''International Journal of Approximate Reasoning'' '''4''': 89–158.</ref><ref>Berleant, D. (1993). Automatically verified reasoning with both intervals and probability density functions. ''Interval Computations'' '''1993 (2) ''': 48–70.</ref><ref>Berleant, D., G. Anderson, and C. Goodman-Strauss (2008). Arithmetic on bounded families of distributions: a DEnv algorithm tutorial. Pages 183–210 in ''Knowledge Processing with Interval and Soft Computing'', edited by C. Hu, R.B. Kearfott, A. de Korvin and V. Kreinovich, Springer ({{isbn|978-1-84800-325-5}}).</ref><ref>Berleant, D., and C. Goodman-Strauss (1998). Bounding the results of arithmetic operations on random variables of unknown dependency using intervals. ''Reliable Computing'' '''4''': 147–165.</ref><ref>Ferson, S., R. Nelsen, J. Hajagos, D. Berleant, J. Zhang, W.T. Tucker, L. Ginzburg and W.L. Oberkampf (2004). [http://www.ramas.com/depend.pdf ''Dependence in Probabilistic Modeling, Dempster–Shafer Theory, and Probability Bounds Analysis'']. Sandia National Laboratories, SAND2004-3072, Albuquerque, NM.</ref>\n\nThese methods, collectively called [[probability bounds analysis]], provide algorithms to evaluate mathematical expressions when there is uncertainty about the input values, their dependencies, or even the form of mathematical expression itself.  The calculations yield results that are guaranteed to enclose all possible distributions of the output variable if the input p-boxes were also sure to enclose their respective distributions.  In some cases, a calculated p-box will also be best-possible in the sense that ''only'' possible distributions are within the p-box, but this is not always guaranteed.\nFor instance, the set of probability distributions that could result from adding random values without the independence assumption from two (precise) distributions is generally a proper [[subset]] of all the distributions admitted by the computed p-box.  That is, there are distributions within the output p-box that could not arise under any dependence between the two input distributions.  The output p-box will, however, always contain all distributions that are possible, so long as the input p-boxes were sure to enclose their respective underlying distributions.  This property often suffices for use in [[risk analysis]].\n\n==Special cases==\nPrecise [[probability distribution#Probability distributions of real-valued random variables|probability distributions]] and [[interval (mathematics)|interval]]s are special cases of p-boxes, as are [[real number|real values]] and [[integer]]s.  Because a probability distribution expresses variability and lacks incertitude, the left and right bounds of its p-box are coincident for all ''x''-values at the value of the cumulative distribution function (which is a non-decreasing function from zero to one).  Mathematically, a probability distribution ''F'' is the degenerate p-box {''F'', ''F'', E(''F''), V(''F''), ''F''}, where E and V denote the expectation and variance operators.  An interval expresses only incertitude.  Its p-box looks like a rectangular box whose upper and lower bounds jump from zero to one at the endpoints of the interval.  Mathematically, an interval [''a'', ''b''] corresponds to the degenerate p-box {H(''a''), H(''b''), [''a'', ''b''], [0, (''b''–''a'')<sup>2</sup>/4], &#x1D53B;}, where H denotes the [[Heaviside step function]].  A precise scalar number ''c'' lacks both kinds of uncertainty.  Its p-box is just a step function from 0 to 1 at the value ''c''; mathematically this is {H(''c''), H(''c''), ''c'', 0, H(''c'')}.\n\n==Applications==\n{{:Applications of p-boxes and probability bounds analysis}}\n\n==Criticisms==\n''No internal structure''.  Because a p-box retains little information about any internal structure within the bounds, it does not elucidate which distributions within the p-box are most likely, nor whether the edges represent very unlikely or distinctly likely scenarios.  This could complicate decisions in some cases if an edge of a p-box encloses a decision threshold.\n\n''Loses information''.  To achieve computational efficiency, p-boxes lose information compared to more complex [[Dempster–Shafer theory#Formal definition|Dempster–Shafer structures]] or [[credal set]]s.<ref name=Fersonetal03 />  In particular, p-boxes lose information about the mode (most probable value) of a quantity.  This information could be useful to keep, especially in situations where the quantity is an unknown but fixed value.\n\n''Traditional probability sufficient''.  Some critics of p-boxes argue that precisely specified probability distributions are sufficient to characterize uncertainty of all kinds.  For instance, [[Dennis Lindley|Lindley]] has asserted, \"Whatever way uncertainty is approached, probability is the ''only'' sound way to think about it.\"<ref>{{cite book |page=71 |last=Lindley |first=D. V. |year=2006 |title=Understanding Uncertainty |publisher=John Wiley & Sons |location=Hoboken, New Jersey |isbn=978-0-470-04383-7 }}</ref><ref>https://en.wikiquote.org/wiki/Dennis_Lindley</ref><!--ref><<O’Hagan>> < / ref -->  These critics argue that it is meaningless to talk about ‘uncertainty about probability’ and that traditional [[probability theory|probability]] is a complete theory that is sufficient to characterize all forms of uncertainty.  Under this criticism, users of p-boxes have simply not made the requisite effort to identify the appropriate precisely specified distribution functions.\n\n''Possibility theory can do better''.  Some critics contend that it makes sense in some cases to work with a [[possibility theory|possibility]] distribution rather than working separately with the left and right edges of p-boxes.  They argue that the set of probability distributions [[Possibility theory#Possibility theory as an imprecise probability theory|induced]] by a possibility distribution is a subset of those enclosed by an analogous p-box's edges.<ref>Baudrit, C., D. Dubois, H. Fargier (2003). Représentation de la connaissance probabiliste incomplète. Pages 65–72 in ''Actes Rencontres Francophones sur la Logique Floue et ses Applications (LFA'03), Tours, France''. Cépaduès-Éditions.</ref><ref>Baudrit, C. (2005.) [http://www.univ-orleans.fr/mapmo/membres/baudrit/manuscrit_these.pdf ''Représentation et propagation de connaissances imprécises et incertaines : Application à l'évaluation des risques liés aux sites et aux sols pollués'']. Ph.D. dissertation, Université Paul Sabatier, Toulouse III.</ref>  Others make a counterargument that one cannot do better with a possibility distribution than with a p-box.<ref>Troffaes, M.C.M., and S. Destercke (2011).  [https://dx.doi.org/10.1016/j.ijar.2011.02.001 Probability boxes on totally preordered spaces for multivariate modelling]. ''International Journal of Approximate Reasoning'' (in press).</ref>\n\n==See also==\n* [[uncertain number]]\n* [[interval (mathematics)|interval]]\n* [[cumulative distribution function|cumulative probability distribution]]\n* [[upper and lower probabilities]]\n* [[credal set]]\n* [[risk analysis]]\n* [[uncertainty propagation]]\n* [[probability bounds analysis]]\n* [[Dempster–Shafer theory]] and the section on [[Dempster–Shafer theory#Formal definition|Dempster–Shafer structure]]\n* [[imprecise probability]]\n* [[Confidence band#Pointwise and simultaneous confidence bands|simultaneous confidence bands]] on distribution and [[survival function]]s using likelihood ratios<ref name=murphy95 />\n* [[Confidence band#Pointwise and simultaneous confidence bands|pointwise]] binomial confidence intervals for ''F''(''X'') for a given ''X''<ref>Meeker, W.Q., and L.A. Escobar (1998). ''Statistical Methods for Reliability Data'', John Wiley and Sons, New York.</ref>\n* [[list of uncertainty propagation software|uncertainty propagation software]]\n\n==References==\n{{Reflist|30em}}\n\n==Additional references==\n* Baudrit, C., and D. Dubois (2006). [http://www.univ-orleans.fr/mapmo/membres/baudrit/csda06.pdf Practical representations of incomplete probabilistic knowledge]. ''Computational Statistics & Data Analysis'' '''51''': 86–108.\n* Baudrit, C., D. Dubois, D. Guyonnet (2006). [http://www.univ-orleans.fr/mapmo/membres/baudrit/ieee06.pdf Joint propagation and exploitation of probabilistic and possibilistic information in risk assessment]. ''IEEE Transactions on Fuzzy Systems'' '''14''': 593–608.\n* Bernardini, A., and F. Tonon (2009). [https://web.archive.org/web/20110911141406/http://www.caee.utexas.edu/prof/tonon/Publications/Papers/Paper%20P%202008-02%20Extreme%20probability%20distributions%20of.pdf Extreme probability distributions of random/fuzzy sets and p-boxes]. ''International Journal of Reliability and Safety'' '''3''': 57-78. [http://savannah.gatech.edu/workshop/rec08/documents/REC08_Paper_Bernardini.pdf (alternative link)]\n* Destercke, S., D. Dubois and E. Chojnacki (2008). [https://archive.today/20130202224101/http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6V07-4T2M5W9-1&_user=10&_coverDate=11/30/2008&_rdoc=1&_fmt=high&_orig=gateway&_origin=gateway&_sort=d&_docanchor=&view=c&_rerunOrigin=scholar.google&_acct=C000050221&_version=1&_urlVersion=0&_userid=10&md5=5e765b0819db1b4f43f21cd7e90fe509&searchtype=a Unifying practical uncertainty representations – I: Generalized p-boxes].   ''International Journal of Approximate Reasoning'' '''49''': 649–663 .\n* Dubois, D. (2010). (Commentary) Representation, propagation, and decision issues in risk analysis under incomplete probabilistic information. ''Risk Analysis'' '''30''': 361–368. {{doi|10.1111/j.1539-6924.2010.01359.x}}.\n* Dubois, D., and D. Guyonnet (2011). Risk-informed decision-making in the presence of epistemic uncertainty. ''International Journal of General Systems'' '''40''': 145–167.\n* Guyonnet, D., F. Blanchard, C. Harpet, Y. Ménard, B. Côme and C. Baudrit (2005). [https://web.archive.org/web/20120311172706/http://www.brgm.fr/publication/pubDetailRapportSP.jsp?id=RSP-BRGM%2FRP-54099-FR Projet IREA—Traitement des incertitudes en évaluation des risques d'exposition]. Rapport BRGM/RP-54099-FR, Bureau de Recherches Géologiques et Minières, France.\n\n{{DEFAULTSORT:Probability Box}}\n[[Category:Probability bounds analysis]]\n[[Category:Risk analysis methodologies]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Propagation of uncertainty",
      "url": "https://en.wikipedia.org/wiki/Propagation_of_uncertainty",
      "text": "{{For|the propagation of uncertainty through time|Chaos theory#Sensitivity to initial conditions}}\n\nIn [[statistics]], '''propagation of uncertainty''' (or '''propagation of error''') is the effect of [[Variable (mathematics)|variables]]' [[uncertainty|uncertainties]] (or [[Errors and residuals in statistics|errors]], more specifically [[random error]]s) on the uncertainty of a [[function (mathematics)|function]] based on them. When the variables are the values of experimental measurements they have [[Observational error|uncertainties due to measurement limitations]] (e.g., instrument [[Accuracy and precision|precision]]) which propagate due to the combination of variables in the function.\n\nThe uncertainty ''u'' can be expressed in a number of ways.\nIt may be defined by the [[absolute error]] {{math|Δ''x''}}. Uncertainties can also be defined by the [[relative error]] {{math|(Δ''x'')/''x''}}, which is usually written as a percentage.\nMost commonly, the uncertainty on a quantity is quantified in terms of the [[standard deviation]], {{mvar|σ}}, the positive square root of [[variance]], {{math|''σ''<sup>2</sup>}}. The value of a quantity and its error are then expressed as an interval {{math|''x'' ± ''u''}}. If the statistical [[probability distribution]] of the variable is known or can be assumed, it is possible to derive [[confidence limits]] to describe the region within which the true value of the variable may be found. For example, the 68% confidence limits for a one-dimensional variable belonging to a [[normal distribution]] are approximately ± one standard deviation {{math|''σ''}} from the central value {{math|''x''}}, which means that the region {{math|''x'' ± ''σ''}} will cover the true value in roughly 68% of cases.\n\nIf the uncertainties are [[correlated]] then [[covariance]] must be taken into account. Correlation can arise from two different sources. First, the ''measurement errors'' may be correlated. Second, when the underlying values are correlated across a population, the ''uncertainties in the group averages'' will be correlated.<ref>{{cite web|last1=Kirchner|first1=James|title=Data Analysis Toolkit #5: Uncertainty Analysis and Error Propagation|url=http://seismo.berkeley.edu/~kirchner/eps_120/Toolkits/Toolkit_05.pdf|website=Berkeley Seismology Laboratory|publisher=University of California|accessdate=22 April 2016}}</ref>\n\n==Linear combinations==\nLet <math>\\{f_k(x_1,x_2,\\dots,x_n)\\}</math> be a set of ''m'' functions which are linear combinations of <math>n</math> variables <math>x_1,x_2,\\dots,x_n</math> with combination coefficients <math>A_{k1},A_{k2},\\dots,A_{kn}, (k=1,\\dots, m)</math>:\n\n:<math>f_k=\\sum_{i=1}^n A_{ki} x_i \\text{ or } \\mathrm{f}=\\mathrm{Ax} \\ .</math>\n\nAlso let the [[variance-covariance matrix]] of ''x''&nbsp;=&nbsp;(''x''<sub>1</sub>,&nbsp;...,&nbsp;''x''<sub>''n''</sub>) be denoted by <math>\\Sigma^x\\,</math>.\n\n:<math>\\Sigma^x =\n\\begin{pmatrix}\n   \\sigma^2_1 & \\sigma_{12} & \\sigma_{13} & \\cdots \\\\\n   \\sigma_{12} & \\sigma^2_2 & \\sigma_{23} & \\cdots\\\\\n   \\sigma_{13} & \\sigma_{23} & \\sigma^2_3 & \\cdots \\\\\n   \\vdots & \\vdots & \\vdots & \\ddots\n\\end{pmatrix} = \n\\begin{pmatrix}\n   {\\Sigma}^x_{11} & {\\Sigma}^x_{12} & {\\Sigma}^x_{13} & \\cdots \\\\\n   {\\Sigma}^x_{12} & {\\Sigma}^x_{22} & {\\Sigma}^x_{23} & \\cdots\\\\\n   {\\Sigma}^x_{13} & {\\Sigma}^x_{23} & {\\Sigma}^x_{33} & \\cdots \\\\\n   \\vdots & \\vdots & \\vdots & \\ddots\n\\end{pmatrix}\n</math>\n\nThen, the variance-covariance matrix <math>\\Sigma^f\\,</math> of ''f'' is given by\n\n:<math>{\\Sigma}^f_{ij}= \\sum_k^n \\sum_\\ell^n A_{ik} {\\Sigma}^x_{k\\ell} A_{j\\ell},</math>\n\nor, in matrix notation:\n\n:<math> \\Sigma^f= \\mathrm{A} \\Sigma^x \\mathrm{A}^\\top.</math>\n\nThis is the most general expression for the propagation of error from one set of variables onto another. When the errors on ''x'' are uncorrelated the general expression simplifies to\n\n:<math>{\\Sigma}^f_{ij}= \\sum_k^n  A_{ik} {\\Sigma}^x_k A_{jk}.</math>\nwhere <math>{\\Sigma}^x_k = \\sigma^2_{x_k}</math> is the variance of ''k''-th element of the ''x'' vector.\nNote that even though the errors on ''x'' may be uncorrelated, the errors on ''f'' are in general correlated; in other words, even if <math>\\mathrm{\\Sigma^x}</math> is a diagonal matrix, <math>\\mathrm{\\Sigma^f}</math> is in general a full matrix.\n\nThe general expressions for a scalar-valued function, ''f'', are a little simpler:\n\n: <math>f=\\sum_i^n a_i x_i: f=\\mathrm{a} x \\,</math>\n: <math>\\sigma^2_f= \\sum_i^n \\sum_j^n a_i {\\Sigma}^x_{ij} a_j= \\mathrm{a} \\Sigma^x \\mathrm{a}^\\top</math>\n\n(where ''a'' is a row vector).\n\nEach covariance term, <math>\\sigma_{ij}</math> can be expressed in terms of the [[Pearson product-moment correlation coefficient|correlation coefficient]] <math>\\rho_{ij}\\,</math> by <math>\\sigma_{ij}=\\rho_{ij}\\sigma_i\\sigma_j\\,</math>, so that an alternative expression for the variance of ''f'' is\n\n:<math>\\sigma^2_f= \\sum_i^n a_i^2\\sigma^2_i+\\sum_i^n \\sum_{j (j \\ne i)}^n a_i a_j \\rho_{ij} \\sigma_i \\sigma_j. </math>\n\nIn the case that the variables in ''x'' are uncorrelated this simplifies further to\n\n:<math>\\sigma^2_f= \\sum_i^n a_i^2 \\sigma^2_i.</math>\n\nIn the simplest case of identical coefficients and variances, we find\n\n:<math>\\sigma_f= \\sqrt{n} a \\sigma.</math>\n\n== Non-linear combinations ==\n{{See also|Taylor expansions for the moments of functions of random variables}}\nWhen ''f'' is a set of non-linear combination of the variables ''x'', an [[interval propagation]] could be performed in order to compute intervals which contain all consistent values for the variables. In a probabilistic approach, the function ''f'' must usually be linearized by approximation to a first-order [[Taylor series]] expansion, though in some cases, exact formulas can be derived that do not depend on the expansion as is the case for the exact variance of products.<ref name=\"Goodman1960\">{{Cite journal\n | last = Goodman\n | first= Leo\n | authorlink = Leo Goodman\n | title = On the Exact Variance of Products\n | journal = Journal of the American Statistical Association\n | year = 1960\n | volume = 55\n | issue = 292\n | pages = 708–713\n | doi = 10.2307/2281592\n | jstor=2281592\n}}</ref> The Taylor expansion would be:\n\n:<math>f_k \\approx f^0_k+  \\sum_i^n \\frac{\\partial f_k}{\\partial {x_i}} x_i </math>\n\nwhere <math>\\partial f_k/\\partial x_i</math> denotes the [[partial derivative]] of ''f<sub>k</sub>'' with respect to the ''i''-th variable, evaluated at the mean value of all components of vector ''x''. Or in [[matrix notation]],\n:<math>\\mathrm{f} \\approx \\mathrm{f}^0 + \\mathrm{J} \\mathrm{x}\\,</math>\nwhere J is the [[Jacobian matrix]]. Since f<sup>0</sup> is a constant it does not contribute to the error on f. Therefore, the propagation of error follows the linear case, above, but replacing the linear coefficients, ''A<sub>ki</sub>'' and ''A<sub>kj</sub>'' by the partial derivatives, <math>\\frac{\\partial f_k}{\\partial x_i}</math> and <math>\\frac{\\partial f_k}{\\partial x_j}</math>. In matrix notation,<ref>Ochoa1,Benjamin; Belongie, Serge [http://vision.ucsd.edu/sites/default/files/ochoa06.pdf \"Covariance Propagation for Guided Matching\"]</ref>\n\n: <math>\\mathrm{\\Sigma}^\\mathrm{f} = \\mathrm{J} \\mathrm{\\Sigma}^\\mathrm{x} \\mathrm{J}^\\top.</math>\n\nThat is, the Jacobian of the function is used to transform the rows and columns of the variance-covariance matrix of the argument.\nNote this is equivalent to the matrix expression for the linear case with <math>\\mathrm{J = A}</math>.\n\n=== Simplification ===\nNeglecting correlations or assuming independent variables yields a common formula among engineers and experimental scientists to calculate error propagation, the variance formula:<ref>{{cite journal|last=Ku|first=H. H.|date=October 1966|title=Notes on the use of propagation of error formulas|url=http://nistdigitalarchives.contentdm.oclc.org/cdm/compoundobject/collection/p16009coll6/id/99848/rec/1|journal=Journal of Research of the National Bureau of Standards|volume=70C|issue=4|page=262|doi=10.6028/jres.070c.025|issn=0022-4316|accessdate=3 October 2012|via=}}</ref>\n\n:<math>s_f = \\sqrt{ \\left(\\frac{\\partial f}{\\partial x}\\right)^2 s_x^2 + \\left(\\frac{\\partial f}{\\partial y} \\right)^2 s_y^2 + \\left(\\frac{\\partial f}{\\partial z} \\right)^2 s_z^2 + \\cdots}</math>\n\nwhere <math>s_f</math> represents the standard deviation of the function <math>f</math>, <math>s_x</math> represents the standard deviation of <math>x</math>, <math>s_y</math> represents the standard deviation of <math>y</math>, and so forth.\n\nIt is important to note that this formula is based on the linear characteristics of the gradient of <math>f</math> and therefore it is a good estimation for the standard deviation of <math>f</math> as long as <math>s_x, s_y, s_z,\\ldots</math> are small enough. Specifically, the linear approximation of  <math>f</math> has to be close to <math>f</math> inside a neighborhood of radius <math>s_x, s_y, s_z,\\ldots</math>.<ref>{{Cite book |last=Clifford |first=A. A. |title=Multivariate error analysis: a handbook of error propagation and calculation in many-parameter systems |publisher=John Wiley & Sons |year=1973 |isbn=978-0470160558 }}{{page needed|date=October 2012}}</ref>\n\n=== Example ===\nAny non-linear differentiable function, <math>f(a,b)</math>, of two variables, <math>a</math> and <math>b</math>, can be expanded as\n:<math>f\\approx f^0+\\frac{\\partial f}{\\partial a}a+\\frac{\\partial f}{\\partial b}b</math>\nhence:\n:<math>\\sigma^2_f\\approx\\left| \\frac{\\partial f}{\\partial a}\\right| ^2\\sigma^2_a+\\left| \\frac{\\partial f}{\\partial b}\\right|^2\\sigma^2_b+2\\frac{\\partial f}{\\partial a}\\frac{\\partial f} {\\partial b}\\sigma_{ab}</math>\nwhere <math>\\sigma_{f}</math> is the standard deviation of the function <math>f</math>, <math>\\sigma_{a}</math> is the standard deviation of <math>a</math>, <math>\\sigma_{b}</math> is the standard deviation of <math>b</math> and <math>\\sigma_{ab}</math> is the covariance between <math>a</math> and <math>b</math>. \n\nIn the particular case that <math>f=ab\\!</math>, <math>\\frac{\\partial f}{\\partial a}=b, \\frac{\\partial f}{\\partial b}=a</math>. Then\n:<math>\\sigma^2_f \\approx b^2\\sigma^2_a+a^2 \\sigma_b^2+2ab\\,\\sigma_{ab}</math>\nor\n:<math>\\left(\\frac{\\sigma_f}{f}\\right)^2 \\approx \\left(\\frac{\\sigma_a}{a} \\right)^2 + \\left(\\frac{\\sigma_b}{b}\\right)^2 + 2\\left(\\frac{\\sigma_a}{a}\\right)\\left(\\frac{\\sigma_b}{b}\\right)\\rho_{ab}</math>\nwhere <math>\\rho_{ab}</math> is the correlation between <math>a</math> and <math>b</math>.\n\nWhen the variables <math>a</math> and <math>b</math> are uncorrelated, <math>\\rho_{ab}=0</math>. Then\n:<math>\\left(\\frac{\\sigma_f}{f}\\right)^2 \\approx \\left(\\frac{\\sigma_a}{a} \\right)^2 + \\left(\\frac{\\sigma_b}{b}\\right)^2.</math>\n\n===Caveats and warnings===\nError estimates for non-linear functions are [[Bias of an estimator|biased]] on account of using a truncated series expansion. The extent of this bias depends on the nature of the function. For example, the bias on the error calculated for log(1+''x'') increases as ''x'' increases, since the expansion to ''x'' is a good approximation only when ''x'' is near zero.\n\nFor highly non-linear functions, there exist five categories of probabilistic approaches for uncertainty propagation;<ref>{{cite journal |first=S. H. |last=Lee |first2=W. |last2=Chen |title=A comparative study of uncertainty propagation methods for black-box-type problems |journal=Structural and Multidisciplinary Optimization |volume=37 |issue=3 |year=2009 |pages=239–253 |doi=10.1007/s00158-008-0234-7 }}</ref> see [[Uncertainty Quantification#Methodologies for forward uncertainty propagation]] for details.\n\n====Reciprocal====\nIn the special case of the inverse or reciprocal <math>1/B</math>, where <math>B=N(0,1)</math>, the distribution is a [[Inverse distribution#Reciprocal normal distribution|reciprocal normal distribution]], and there is no definable variance. For such [[inverse distribution]]s and for [[ratio distribution]]s, there can be defined probabilities for intervals, which can be computed either by [[Monte Carlo simulation]] or, in some cases, by using the Geary–Hinkley transformation.<ref name=\"HayyaJ1975On\">{{Cite journal\n | last1 = Hayya\n | first1 = Jack\n | authorlink1 = Jack Hayya\n | last2 = Armstrong\n | first2 = Donald\n | last3 = Gressis\n | first3 = Nicolas\n | title = A Note on the Ratio of Two Normally Distributed Variables\n | journal = [[Management Science (journal)|Management Science]]\n |date=July 1975\n | volume = 21\n | issue = 11\n | pages = 1338–1341\n | doi = 10.1287/mnsc.21.11.1338\n| jstor=2629897\n}}</ref>\n\n====Shifted reciprocal====\nThe statistics, mean and variance, of the shifted reciprocal function <math> \\frac{1}{p-B} </math> for <math>B=N(\\mu,\\sigma)</math>, however, exist in a [[principal value]] sense if the difference between the shift or pole <math>p</math> and the mean <math>\\mu</math> is real.  The mean of this transformed random variable is then indeed the scaled [[Dawson's function]] <math>\\frac{\\sqrt{2}}{\\sigma} F \\left(\\frac{p-\\mu}{\\sqrt{2}\\sigma}\\right)</math>.<ref name=lecomte2013exact>{{Cite journal\n| last1= Lecomte\n| first1 = Christophe\n| title = Exact statistics of systems with uncertainties: an analytical theory of rank-one stochastic dynamic systems\n| journal = Journal of Sound and Vibrations\n| volume = 332\n| issue =  11\n|date=May 2013\n| pages = 2750–2776\n| doi = 10.1016/j.jsv.2012.12.009\n}}</ref>  In contrast, if the shift <math>p-\\mu</math> is purely complex, the mean exists and is a scaled [[Faddeeva function]], whose exact expression depends on the sign of the imaginary part, <math>\\operatorname{Im}(p-\\mu)</math>.\nIn both cases, the variance is a simple function of the mean.<ref>{{Cite journal\n| last1= Lecomte\n| first1 = Christophe\n| title = Exact statistics of systems with uncertainties: an analytical theory of rank-one stochastic dynamic systems\n| journal = Journal of Sound and Vibrations\n| volume = 332\n| issue =  11\n|date=May 2013\n| at = Section (4.1.1)\n| doi = 10.1016/j.jsv.2012.12.009\n}}</ref> Therefore, the variance has to be considered in a principal value sense if <math>p-\\mu</math> is real, while it exists if the imaginary part of <math>p-\\mu</math> is non-zero. Note that these means and variances are exact, as they do not recur to linearisation of the ratio.  The exact covariance of two ratios with a pair of different poles <math>p_1</math> and <math>p_2</math> is similarly available.<ref>{{Cite journal\n| last1= Lecomte\n| first1 = Christophe\n| title = Exact statistics of systems with uncertainties: an analytical theory of rank-one stochastic dynamic systems\n| journal = Journal of Sound and Vibrations\n| volume = 332\n| issue =  11\n|date=May 2013\n| at = Eq.(39)-(40)\n| doi = 10.1016/j.jsv.2012.12.009\n}}</ref>\nThe case of the inverse of a '''complex''' normal variable <math>B</math>, shifted or not, exhibits different characteristics.<ref name=lecomte2013exact />\n\n==Example formulae==\nThis table shows the variances of simple functions of the real variables <math>A,B\\!</math>, with standard deviations <math>\\sigma_A, \\sigma_B,\\,</math> [[Covariance and correlation|covariance]] <math>\\sigma_{AB}</math> and exactly known real-valued constants <math>a,b\\,</math> (i.e., <math>\\sigma_a=\\sigma_b=0</math>).\n\n:{| class=\"wikitable\"  background: white\"\n! style=\"background:#ffdead;\" | Function !! style=\"background:#ffdead;\" | Variance !! style=\"background:#ffdead;\" | Standard Deviation\n|-\n| <math>f = aA\\,</math>\n|| <math>\\sigma_f^2 = a^2\\sigma_A^2</math>\n|| <math>\\sigma_f = |a|\\sigma_A</math>\n|-\n| <math>f = aA + bB\\,</math>\n|| <math>\\sigma_f^2 = a^2\\sigma_A^2 + b^2\\sigma_B^2 + 2ab\\,\\sigma_{AB}</math>\n|| <math>\\sigma_f = \\sqrt{a^2\\sigma_A^2 + b^2\\sigma_B^2 + 2ab\\,\\sigma_{AB}}</math>\n|-\n| <math>f = aA - bB\\,</math>\n|| <math>\\sigma_f^2 = a^2\\sigma_A^2 + b^2\\sigma_B^2 - 2ab\\,\\sigma_{AB}</math>\n|| <math>\\sigma_f = \\sqrt{a^2\\sigma_A^2 + b^2\\sigma_B^2 - 2ab\\,\\sigma_{AB}}</math>\n|-\n| <math>f = AB\\,</math>\n|| <math>\\sigma_f^2 \\approx f^2 \\left[\\left(\\frac{\\sigma_A}{A}\\right)^2 + \\left(\\frac{\\sigma_B}{B}\\right)^2 + 2\\frac{\\sigma_{AB}}{AB} \\right]</math><ref>{{cite web |last= |first= |url=http://ipl.physics.harvard.edu/wp-uploads/2013/03/PS3_Error_Propagation_sp13.pdf |title=A Summary of Error Propagation |page=2 |accessdate=2016-04-04}}</ref><ref>{{cite web |last= |first= |url=http://web.mit.edu/fluids-modules/www/exper_techniques/2.Propagation_of_Uncertaint.pdf |title=Propagation of Uncertainty through Mathematical Operations|page=5 |accessdate=2016-04-04}}</ref>\n|| <math>\\sigma_f \\approx \\left| f \\right| \\sqrt{ \\left(\\frac{\\sigma_A}{A}\\right)^2 + \\left(\\frac{\\sigma_B}{B}\\right)^2 + 2\\frac{\\sigma_{AB}}{AB} }</math>\n|-\n| <math>f = \\frac{A}{B}\\,</math>\n|| <math>\\sigma_f^2 \\approx f^2 \\left[\\left(\\frac{\\sigma_A}{A}\\right)^2 + \\left(\\frac{\\sigma_B}{B}\\right)^2 - 2\\frac{\\sigma_{AB}}{AB} \\right]</math><ref>{{cite web |last= |first= |url=http://www.sagepub.com/upm-data/6427_Chapter_4__Lee_%28Analyzing%29_I_PDF_6.pdf |title=Strategies for Variance Estimation |page=37 |accessdate=2013-01-18}}</ref>\n|| <math>\\sigma_f \\approx \\left| f \\right| \\sqrt{ \\left(\\frac{\\sigma_A}{A}\\right)^2 + \\left(\\frac{\\sigma_B}{B}\\right)^2 - 2\\frac{\\sigma_{AB}}{AB} }</math>\n|-\n| <math>f = a A^{b}\\,</math>\n|| <math>\\sigma_f^2 \\approx \\left( {a}{b}{A}^{b-1}{\\sigma_A} \\right)^2 = \\left( \\frac{{f}{b}{\\sigma_A}}{A} \\right)^2 </math>\n|| <math>\\sigma_f \\approx \\left| {a}{b}{A}^{b-1}{\\sigma_A} \\right| = \\left| \\frac{{f}{b}{\\sigma_A}}{A} \\right| </math>\n|-\n| <math>f = a \\ln(bA)\\,</math>\n|| <math>\\sigma_f^2 \\approx \\left(a \\frac{\\sigma_A}{A} \\right)^2</math><ref name=harris2003>{{citation |first1=Daniel C. |last1=Harris | title=Quantitative chemical analysis |edition=6th |publisher=Macmillan |year=2003 |isbn=978-0-7167-4464-1 |page=56 |url=https://books.google.com/books?id=csTsQr-v0d0C&pg=PA56 }}</ref>\n|| <math>\\sigma_f \\approx \\left| a \\frac{\\sigma_A}{A}\\right|</math>\n|-\n| <math>f = a \\log_{10}(bA)\\,</math>\n|| <math>\\sigma_f^2 \\approx \\left(a \\frac{\\sigma_A}{A \\ln(10)} \\right)^2</math><ref name=harris2003/>\n|| <math>\\sigma_f \\approx \\left| a \\frac{\\sigma_A}{A \\ln(10)} \\right|</math>\n|-\n| <math>f = a e^{bA}\\,</math>\n|| <math>\\sigma_f^2 \\approx f^2 \\left( b\\sigma_A \\right)^2</math><ref>{{cite web|url=http://www.foothill.edu/psme/daley/tutorials_files/10.%20Error%20Propagation.pdf|date=October 9, 2009|title=Error Propagation tutorial|work=Foothill College|accessdate=2012-03-01}}</ref>\n|| <math>\\sigma_f \\approx \\left| f \\left( b\\sigma_A \\right) \\right| </math>\n|-\n| <math>f = a^{bA}\\,</math>\n|| <math>\\sigma_f^2 \\approx f^2 (b\\ln(a)\\sigma_A)^2</math>\n|| <math>\\sigma_f \\approx \\left| f(b\\ln(a)\\sigma_A) \\right|</math>\n|-\n| <math>f = a \\sin(bA)\\,</math>\n|| <math>\\sigma_f^2 \\approx \\left[ a b \\cos(b A) \\sigma_A \\right]^2</math>\n|| <math>\\sigma_f \\approx \\left| a b \\cos(b A) \\sigma_A \\right|</math>\n|-\n| <math>f = a \\cos \\left( b A \\right)\\,</math>\n|| <math>\\sigma_f^2 \\approx \\left[ a b \\sin(b A) \\sigma_A \\right]^2</math>\n|| <math>\\sigma_f \\approx \\left| a b \\sin(b A) \\sigma_A \\right|</math>\n|-\n|<math>f = a \\tan \\left( b A \\right)\\,</math>\n|<math>\\sigma_f^2 \\approx \\left[ a b \\sec^2(b A) \\sigma_A \\right]^2</math>\n|<math>\\sigma_f \\approx \\left| a b \\sec^2(b A) \\sigma_A \\right|</math>\n|-\n| <math>f = A^B\\,</math>\n|| <math>\\sigma_f^2 \\approx f^2 \\left[ \\left( \\frac{B}{A}\\sigma_A \\right)^2 +\\left( \\ln(A)\\sigma_B \\right)^2 + 2 \\frac{B \\ln(A)}{A} \\sigma_{AB} \\right]</math>\n|| <math>\\sigma_f \\approx \\left| f \\right| \\sqrt{ \\left( \\frac{B}{A}\\sigma_A \\right)^2 +\\left( \\ln(A)\\sigma_B \\right)^2 + 2 \\frac{B \\ln(A)}{A} \\sigma_{AB} } </math>\n|-\n| <math>f = \\sqrt{aA^2 \\pm bB^2}\\,</math>\n|| <math>\\sigma_f^2 \\approx \\left(\\frac{A}{f}\\right)^2 a^2\\sigma_A^2 + \\left(\\frac{B}{f}\\right)^2 b^2\\sigma_B^2 \\pm 2ab\\frac{AB}{f^2}\\,\\sigma_{AB}</math>\n|| <math>\\sigma_f \\approx \\sqrt{\\left(\\frac{A}{f}\\right)^2 a^2\\sigma_A^2 + \\left(\\frac{B}{f}\\right)^2 b^2\\sigma_B^2 \\pm 2ab\\frac{AB}{f^2}\\,\\sigma_{AB}}</math>\n|}\n\nFor uncorrelated variables (<math>\\rho_{AB}=0</math>) the covariance terms are also zero, as <math>\\sigma_{AB}=\\rho_{AB}\\sigma_A\\sigma_B\\,</math>.\n\nIn this case, expressions for more complicated functions can be derived by combining simpler functions. For example, repeated multiplication, assuming no correlation gives\n:<math>f = ABC; \\qquad \\left(\\frac{\\sigma_f}{f}\\right)^2 \\approx \\left(\\frac{\\sigma_A}{A}\\right)^2 + \\left(\\frac{\\sigma_B}{B}\\right)^2+ \\left(\\frac{\\sigma_C}{C}\\right)^2.</math>\n\nFor the case <math>f = AB </math> we also have Goodman's expression<ref name=\"Goodman1960\"/> for the exact variance: for the uncorrelated case it is\n\n: <math>V(XY)= E(X)^2 V(Y) + E(Y)^2 V(X) + E((X-E(X))^2 (Y-E(Y))^2)</math>\n\nand therefore we have:\n\n: <math>\\sigma_f^2 = A^2\\sigma_B^2 + B^2\\sigma_A^2 +  \\sigma_A^2\\sigma_B^2 </math>\n\n==Example calculations==\n\n===Inverse tangent function===\nWe can calculate the uncertainty propagation for the inverse tangent function as an example of using partial derivatives to propagate error.\n\nDefine\n\n:<math>f(x) = \\arctan(x),</math>\n\nwhere <math>\\Delta_x</math> is the absolute uncertainty on our measurement of {{mvar|x}}. The derivative of {{math|''f''(''x'')}} with respect to {{mvar|x}} is\n\n:<math>\\frac{d f}{d x} = \\frac{1}{1+x^2}.</math>\n\nTherefore, our propagated uncertainty is\n\n:<math>\\Delta_{f} \\approx \\frac{\\Delta_x}{1+x^2},</math>\n\nwhere <math>\\Delta_f</math> is the absolute propagated uncertainty.\n\n===Resistance measurement===\nA practical application is an [[experiment]] in which one measures [[current (electricity)|current]], {{mvar|I}}, and [[voltage]], {{mvar|V}}, on a [[resistor]] in order to determine the [[electrical resistance|resistance]], {{mvar|R}}, using [[Ohm's law]], {{math|''R'' {{=}} ''V'' / ''I''}}.\n\nGiven the measured variables with uncertainties, {{math|''I'' ± σ<sub>''I''</sub>}} and {{math|''V'' ± σ<sub>''V''</sub>}}, and neglecting their possible correlation, the uncertainty in the computed quantity, {{math|σ<sub>''R''</sub>}}, is:\n\n: <math>\\sigma_R \\approx \\sqrt{ \\sigma_V^2 \\left(\\frac{1}{I}\\right)^2 + \\sigma_I^2 \\left(\\frac{-V}{I^2}\\right)^2 } = R\\sqrt{ \\left(\\frac{\\sigma_V}{V}\\right)^2 + \\left(\\frac{\\sigma_I}{I}\\right)^2 }.</math>\n\n==See also==\n* [[Accuracy and precision]]\n* [[Automatic differentiation]]\n* [[Delta method]]\n* [[Errors and residuals in statistics]]\n* [[Experimental uncertainty analysis]]\n* [[Interval finite element]]\n* [[Measurement uncertainty]]\n* [[Probability bounds analysis]]\n* [[Significance arithmetic]]\n* [[Uncertainty quantification]]\n\n== References ==\n{{reflist|30em}}\n\n==Further reading==\n*{{Citation |last=Bevington |first=Philip R. |last2=Robinson |first2=D. Keith |year=2002 |title=Data Reduction and Error Analysis for the Physical Sciences |edition=3rd |publisher=McGraw-Hill |isbn=978-0-07-119926-1 |ref=none }}\n*{{citation | first1=Paolo | last1=Fornasini | title=The uncertainty in physical measurements: an introduction to data analysis in the physics laboratory | publisher=Springer | year=2008 | isbn=978-0-387-78649-0 | page=161 | url=https://books.google.com/books?id=PBJgvPgf2NkC&pg=PA161 |ref=none}}\n*{{Citation |last=Meyer |first=Stuart L. |year=1975 |title=Data Analysis for Scientists and Engineers |publisher=Wiley |isbn=978-0-471-59995-1 |ref=none}}\n*{{Citation |last=Peralta |first=M. |date=2012 |title=Propagation Of Errors: How To Mathematically Predict Measurement Errors |publisher=CreateSpace |ref=none}}\n*{{Citation |last=Rouaud |first=M. |date=2013 |url=http://www.incertitudes.fr/book.pdf |title=Probability, Statistics and Estimation: Propagation of Uncertainties in Experimental Measurement |edition=short |ref=none}}\n*{{Citation |last=Taylor |first=J. R. |date=1997 |title=An Introduction to Error Analysis: The Study of Uncertainties in Physical Measurements |edition=2nd |publisher=University Science Books |isbn= |ref=none}}\n\n==External links==\n*[http://ScienceMathMastery.com ScienceMathMastery.com] - [http://www.sciencemathmastery.com/propagation-of-error-with-single-and-multiple-independent-variables/ Propagation of error with single and multiple independent variables.]\n*[http://www.av8n.com/physics/uncertainty.htm A detailed discussion of measurements and the propagation of uncertainty] explaining the benefits of using error propagation formulas and Monte Carlo simulations instead of simple [[significance arithmetic]]\n*[http://www.bipm.org/en/publications/guides/gum.html GUM], Guide to the Expression of Uncertainty in Measurement\n*[http://infoscience.epfl.ch/record/97374/files/TR-98-01R3.pdf EPFL An Introduction to Error Propagation], Derivation, Meaning and Examples of Cy = Fx Cx Fx'\n*[http://packages.python.org/uncertainties/ uncertainties package], a program/library for transparently performing calculations with uncertainties (and error correlations).\n*[https://pypi.python.org/pypi/soerp soerp package], a python program/library for transparently performing *second-order* calculations with uncertainties (and error correlations).\n*{{cite techreport| author=Joint Committee for Guides in Metrology| title=JCGM 102: Evaluation of Measurement Data - Supplement 2 to the \"Guide to the Expression of Uncertainty in Measurement\" - Extension to Any Number of Output Quantities| year=2011| institution=JCGM| url=http://www.bipm.org/utils/common/documents/jcgm/JCGM_102_2011_E.pdf| accessdate=13 February 2013}}\n\n{{Authority control}}\n\n[[Category:Algebra of random variables]]\n[[Category:Numerical analysis]]\n[[Category:Statistical approximations]]\n[[Category:Statistical deviation and dispersion]]"
    },
    {
      "title": "Proper generalized decomposition",
      "url": "https://en.wikipedia.org/wiki/Proper_generalized_decomposition",
      "text": "The '''proper generalized decomposition''' ('''PGD''') is a [[numerical method]] for solving [[boundary value problem]]s. It assumes that the solution of a multidimensional (or multiparametric) problem can be expressed in a separated representation of the form\n\n::<math> \\mathbf{u} \\approx \\mathbf{u}^N(x_1, x_2, \\ldots, x_d) = \\sum_{i=1}^N \\mathbf{X_1}_i(x_1) \\cdot \\mathbf{X_2}_i(x_2) \\cdots \\mathbf{X_d}_i(x_d), </math>\n\nwhere the number of terms ''N'', and the functions ''X'' are a priori unknown.\n\nSince solving decoupled problems is computationally much less expensive than solving multidimensional problems, PGD is usually considered a [[dimensionality reduction algorithm]].\n\n== References ==\n* Amine Ammar, B Mokdad, Francisco Chinesta, and Roland Keunings. A new family of solvers for some classes of multidimensional partial differential equations encountered in kinetic theory modeling of complex fluids. ''Journal of Non-Newtonian Fluid Mechanics'', 139(3):153–176, 2006.\n* Amine Ammar, B Mokdad, Francisco Chinesta, and Roland Keunings. A new family of solvers for some classes of multidimensional partial differential equations encountered in kinetic theory modelling of complex fluids: Part II: Transient simulation using space-time separated representations. ''Journal of Non-Newtonian Fluid Mechanics'', 144(2):98–121, 2007.\n* F. Chinesta, R. Keunings, and A. Leygue. ''The Proper Generalized Decomposition for Advanced Numerical Simulations: A Primer''. SpringerBriefs in Applied Sciences and Technology. Springer International Publishing, 2013.\n\n[[Category:Numerical analysis]]\n[[Category:Mathematical modeling]]\n[[Category:Dimension reduction]]"
    },
    {
      "title": "Pseudo-multiplication",
      "url": "https://en.wikipedia.org/wiki/Pseudo-multiplication",
      "text": "#REDIRECT [[CORDIC#Pseudo-multiplication]]\n\n{{Redirect category shell|1=\n{{R to related topic}}\n{{R with possibilities}}\n}}\n<!-- #REDIRECT [[Pseudo-multiplication]]\n\n{{Redirect category shell|1=\n{{R from modification}}\n}} -->\n\n[[Category:Numerical analysis]]\n[[Category:Digit-by-digit algorithms]]\n[[Category:Shift-and-add algorithms]]"
    }
  ]
}