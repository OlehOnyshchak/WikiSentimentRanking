{
  "pages": [
    {
      "title": "Wigner–d'Espagnat inequality",
      "url": "https://en.wikipedia.org/wiki/Wigner%E2%80%93d%27Espagnat_inequality",
      "text": "The '''Wigner–d'Espagnat inequality''' is a basic result of [[set theory]].\nIt is named for [[Eugene Wigner]] and [[Bernard d'Espagnat]] who (as pointed out by [[John Stewart Bell|Bell]]) both employed it in their popularizations of [[quantum mechanics]].\n\nGiven a set S with three subsets, J, K, and L, the following holds:\n\n* each member of S which is a member of J, but not of L \n:: is either a member of J, but neither of K, nor of L, \n:: or else is a member of J and of K, but not of L;\n* each member of J which is neither a member of K, nor of L, is therefore a member of J, but not of K; and\n* each member of J, which is a member of K, but not of L, is therefore a member of K, but not of L.\n\nThe number of members of J which are not members of L is consequently less than, or at most equal to, the sum of the number of members of J which are not members of K, and the number of members of K which are not members of L;\n\n''n<sub>(incl J) (excl L)</sub> ≤ n<sub>(incl J) (excl K)</sub> + n<sub>(incl K) (excl L)</sub>''.\n\nIf the ratios ''N'' of these numbers to the number ''n<sub>(incl S)</sub>'' of all members of set S can be evaluated, e.g.\n\n''N<sub>(incl J) (excl L)</sub> = n<sub>(incl J) (excl L)</sub> / n<sub>(incl S)</sub>'',\n\nthen the '''Wigner–d'Espagnat inequality''' is obtained as:\n\n''N''<sub>(incl ''J'') (excl ''L'')</sub> ≤ ''N''<sub>(incl ''J'') (excl ''K'')</sub> + ''N''<sub>(incl ''K'') (excl ''L'')</sub>.\n\nConsidering this particular form in which the Wigner–d'Espagnat inequality is thereby expressed, and noting that the various non-negative ratios ''N'' satisfy\n\n# ''N<sub>(incl J) (incl K)</sub> + N<sub>(incl J) (excl K)</sub> + N<sub>(excl J) (incl K)</sub> + N<sub>(excl J) (excl K)</sub> = 1'',\n# ''N<sub>(incl J) (incl L)</sub> + N<sub>(incl J) (excl L)</sub> + N<sub>(excl J) (incl L)</sub> + N<sub>(excl J) (excl L)</sub> = 1'', and\n# ''N<sub>(incl K) (incl L)</sub> + N<sub>(incl K) (excl L)</sub> + N<sub>(excl K) (incl L)</sub> + N<sub>(excl K) (excl L)</sub> = 1'',\n\nit is probably worth mentioning that certain non-negative ratios are readily encountered, which are appropriately labelled by similarly related indices, and which '''do''' satisfy equations corresponding to 1., 2. and 3., but which nevertheless '''don't''' satisfy the Wigner–d'Espagnat inequality. For instance:\n\nif three observers, A, B, and C, had each detected signals in one of two distinct own channels (e.g. as ''(hit A)'' vs. ''(miss A)'', ''(hit B)'' vs. ''(miss B)'', and ''(hit C)'' vs. ''(miss C)'', respectively), over several (at least pairwise defined) trials, then non-negative ratios ''N'' may be evaluated, appropriately labelled, and found to satisfy\n\n# ''N<sub>(hit A) (hit B)</sub> + N<sub>(hit A) (miss B)</sub> + N<sub>(miss A) (hit B)</sub> + N<sub>(miss A) (miss B)</sub> = 1'',\n# ''N<sub>(hit A) (hit C)</sub> + N<sub>(hit A) (miss C)</sub> + N<sub>(miss A) (hit C)</sub> + N<sub>(miss A) (miss C)</sub> = 1'', and\n# ''N<sub>(hit B) (hit C)</sub> + N<sub>(hit B) (miss C)</sub> + N<sub>(miss B) (hit C)</sub> + N<sub>(miss B) (miss C)</sub> = 1''.\n\nHowever, if the pairwise ''orientation angles'' between these three observers are determined (following the inverse of a quantum-mechanical interpretation of [[Malus's law]]) from the measured ratios as\n\n: ''orientation angle( A, B ) = 1/2 arccos( N<sub>(hit A) (hit B)</sub> – N<sub>(hit A) (miss B)</sub> – N<sub>(miss A) (hit B)</sub> + N<sub>(miss A) (miss B)</sub> )'', \n: ''orientation angle( A, C ) = 1/2 arccos( N<sub>(hit A) (hit C)</sub> – N<sub>(hit A) (miss C)</sub> – N<sub>(miss A) (hit C)</sub> + N<sub>(miss A) (miss C)</sub> )'',\n: ''orientation angle( B, C ) = 1/2 arccos( N<sub>(hit B) (hit C)</sub> – N<sub>(hit B) (miss C)</sub> – N<sub>(miss B) (hit C)</sub> + N<sub>(miss B) (miss C)</sub> )'',\n\n'''and''' if A's, B's, and C's channels are considered having been properly ''set up'' only if the constraints <BR>\n''orientation angle( A, B ) = orientation angle( B, C ) = orientation angle( A, C )/2 < π/4'' <BR> \nhad been found satisfied (as one may well require, to any accuracy; where the accuracy depends on the number of trials from which the orientation angle values were obtained), then necessarily (given sufficient accuracy)\n\n''(cos( orientation angle( A, C ) ))² =''<BR>\n: ''(N<sub>(hit A) (hit C)</sub> + N<sub>(miss A) (miss C)</sub>) = (2 (N<sub>(hit A) (hit B)</sub> + N<sub>(miss A) (miss B)</sub>) – 1)<sup>2</sup> > 0''.\n\nSince\n\n''1 ≥ (N<sub>(hit A) (hit B)</sub> + N<sub>(miss A) (miss B)</sub>)'',\n\ntherefore\n\n1 ≥ 2 (N<sub>(hit A) (hit B)</sub> + N<sub>(miss A) (miss B)</sub>) – 1, <BR>\n(2 (N<sub>(hit A) (hit B)</sub> + N<sub>(miss A) (miss B)</sub>) – 1) ≥ (2 (N<sub>(hit A) (hit B)</sub> + N<sub>(miss A) (miss B)</sub>) – 1)<sup>2</sup>, <BR>\n''(2 (N<sub>(hit A) (hit B)</sub> + N<sub>(miss A) (miss B)</sub>) – 1) ≥(N<sub>(hit A) (hit C)</sub> + N<sub>(miss A) (miss C)</sub>)'', <BR>\n''(1 – 2 (N<sub>(hit A) (miss B)</sub> + N<sub>(miss A) (hit B)</sub>)) ≥ (1 – (N<sub>(hit A) (miss C)</sub> + N<sub>(miss A) (hit C)</sub>))'', <BR>\n''(N<sub>(hit A) (miss C)</sub> + N<sub>(miss A) (hit C)</sub>) ≥ 2 (N<sub>(hit A) (miss B)</sub> + N<sub>(miss A) (hit B)</sub>)'',\n\n''(N<sub>(hit A) (miss C)</sub> + N<sub>(miss A) (hit C)</sub>) ≥'' \n::: ''(N<sub>(hit A) (miss B)</sub> + N<sub>(miss A) (hit B)</sub>) + (N<sub>(hit B) (miss C)</sub> + N<sub>(miss B) (hit C)</sub>)'',\n\nwhich is in (formal) contradiction to the Wigner–d'Espagnat inequalities\n\n''N<sub>(hit A) (miss C)</sub> ≤ N<sub>(hit A) (miss B)</sub> + N<sub>(hit B) (miss C)</sub>'', or <BR>\n''N<sub>(miss A) (hit C)</sub> ≤ N<sub>(miss A) (hit B)</sub> + N<sub>(miss B) (hit C)</sub>'', or both.\n\nAccordingly, the ratios ''N'' obtained by A, B, and C, with the particular constraints on their ''setup'' in terms of values of orientation angles, '''cannot''' have been derived all at once, in one and the same set of trials together; otherwise they'd necessarily satisfy the Wigner–d'Espagnat inequalities.\nInstead, they had to be derived in three distinct sets of trials, separately and pairwise by A and B, by A and C, and by B and C, respectively.\n\nThe failure of certain measurements (such as the non-negative ratios in the example) to be obtained at once, together from one and the same set of trials, and thus their failure to satisfy Wigner–d'Espagnat inequalities, has been characterized as constituting disproof of [[Albert Einstein|Einstein]]'s notion of ''local realism''.\n\nSimilar interdependencies between ''two'' particular measurements and the corresponding operators are the [[uncertainty principle|uncertainty relations]] as first expressed by [[Werner Heisenberg|Heisenberg]] for the interdependence between measurements of distance and of momentum, and as generalized by [[Edward Condon]], [[Howard Percy Robertson]], and [[Erwin Schrödinger]].\n\n==References==\n* John S. Bell, ''Bertlmann's socks and the nature of reality'', Journal de Physique '''42''', no. 3, p.&nbsp;41 (1981); and references therein.\n\n{{DEFAULTSORT:Wigner-d'Espagnat inequality}}\n[[Category:Inequalities]]"
    },
    {
      "title": "Wirtinger inequality (2-forms)",
      "url": "https://en.wikipedia.org/wiki/Wirtinger_inequality_%282-forms%29",
      "text": ": ''For other inequalities named after Wirtinger, see [[Wirtinger's inequality (disambiguation)|Wirtinger's inequality]].''\n\nIn mathematics, the '''Wirtinger inequality for 2-forms''', named after [[Wilhelm Wirtinger]], states that on a [[Kähler manifold]] <math>M</math>, the [[exterior power|exterior <math>k</math><sup>th</sup> power]] of the [[symplectic form]] (Kähler form) ω, when evaluated on a simple (decomposable) <math>(2k)</math>-vector ζ of unit volume, is bounded above by <math>k!</math>.  That is,\n\n:<math> \\omega^k(\\zeta) \\leq k !\\,.</math>\n\nIn other words, <math> \\textstyle{\\frac{\\omega^k}{k!}} </math> is a [[calibrated geometry|calibration]] on <math> M </math>.  An important corollary is that every complex submanifold of a Kähler manifold is volume minimizing in its homology class.\n\n==See also==\n*[[2-form]]\n*[[Gromov's inequality for complex projective space]]\n*[[Systolic geometry]]\n\n==References==\n*[[Victor Bangert]]; [[Mikhail Katz]]; [[Steve Shnider]]; [[Shmuel Weinberger]]: E_7, Wirtinger inequalities, Cayley 4-form, and homotopy.  Duke Math. J.  146 ('09), no. 1, 35-70. See [https://arxiv.org/abs/math/0608006 arXiv:math.DG/0608006]\n\n[[Category:Inequalities]]\n[[Category:Differential geometry]]\n[[Category:Systolic geometry]]"
    },
    {
      "title": "Wirtinger's inequality for functions",
      "url": "https://en.wikipedia.org/wiki/Wirtinger%27s_inequality_for_functions",
      "text": ": ''For other inequalities named after Wirtinger, see [[Wirtinger's inequality (disambiguation)|Wirtinger's inequality]].''\n\nIn [[mathematics]], historically '''Wirtinger's inequality''' for real functions was an [[inequality (mathematics)|inequality]] used in [[Fourier analysis]]. It was named after [[Wilhelm Wirtinger]]. It was used in 1904  to prove the [[isoperimetric inequality]].  A variety of closely related results are today known as Wirtinger's inequality.\n\n==Theorem==\n===First version===\nLet <math>f : \\mathbb{R} \\to \\mathbb{R}</math> be a [[periodic function]] of period 2π, which is continuous and has a continuous derivative throughout '''R''', and such that\n\n:<math>\\int_0^{2\\pi}f(x) \\, dx = 0.</math>\n\nThen\n\n:<math>\\int_0^{2\\pi}f'^2(x) \\, dx \\ge \\int_0^{2\\pi}f^2(x) \\, dx</math>\n\nwith equality [[if and only if]] ''f''(''x'') = ''a'' sin(''x'') + ''b'' cos(''x'') for some ''a'' and ''b'' (or equivalently ''f''(''x'') = ''c'' sin (''x'' + ''d'') for some ''c'' and ''d'').\n\nThis version of the Wirtinger inequality is the one-dimensional [[Poincaré inequality]], with optimal constant.\n\n===Second version===\nThe following related inequality is also called Wirtinger's inequality {{harv|Dym|McKean|1985}}:\n\n:<math>\\pi^{2}\\int_0^a |f|^2 \\le a^2 \\int_0^a|f'|^2</math>\n\nwhenever ''f'' is a C<sup>1</sup> function such that ''f''(0)&nbsp;=&nbsp;''f''(''a'')&nbsp;=&nbsp;0.  In this form, Wirtinger's inequality is seen as the one-dimensional version of [[Friedrichs' inequality]].\n\n===Proof===\nThe proof of the two versions are similar.  Here is a proof of the first version of the inequality.  Since [[Dirichlet's conditions]] are met, we can write\n\n:<math>f(x)=\\frac{1}{2}a_0+\\sum_{n\\ge 1}\\left(a_n\\frac{\\sin nx}{\\sqrt{\\pi}}+b_n\\frac{\\cos nx}{\\sqrt{\\pi}}\\right),</math>\n\nand moreover ''a''<sub>0</sub> = 0 since the integral of ''f'' vanishes. By [[Parseval's identity]],\n\n:<math>\\int_0^{2\\pi}f^2(x)dx=\\sum_{n=1}^\\infty(a_n^2+b_n^2)</math>\n\nand\n\n:<math>\\int_0^{2\\pi}f'^2(x) \\, dx = \\sum_{n=1}^\\infty n^2(a_n^2+b_n^2)</math>\n\nand since the summands are all ≥ 0, we get the desired inequality, with equality if and only if ''a<sub>n</sub>'' = ''b<sub>n</sub>'' = 0 for all ''n'' ≥ 2.\n\n==References==\n*{{citation|first1=H|last1=Dym|authorlink1=Harry Dym|first2=H|last2=McKean|title=Fourier series and integrals|publisher=Academic press|year=1985|isbn=978-0-12-226451-1}}\n* [[Paul J. Nahin]] (2006) ''Dr. Euler's Fabulous Formula'', page 183, [[Princeton University Press]] {{ISBN|0-691-11822-1}}\n\n*[[Vadim Komkov|Komkov, Vadim]] (1983) Euler's buckling formula and Wirtinger's inequality. Internat. J. Math. Ed. Sci. Tech. 14, no. 6, 661—668. \n\n{{PlanetMath attribution|id=5393|title=Wirtinger's inequality}}\n\n[[Category:Fourier analysis]]\n[[Category:Inequalities]]\n[[Category:Theorems in analysis]]"
    },
    {
      "title": "Young's convolution inequality",
      "url": "https://en.wikipedia.org/wiki/Young%27s_convolution_inequality",
      "text": "In [[mathematics]], '''Young's convolution inequality''' is a [[inequality (mathematics)|mathematical inequality]] about the [[convolution]] of two functions,<ref>{{citation | first1 = W. H. | last1 = Young | author-link = William Henry Young | year = 1912 | title = On the multiplication of successions of Fourier constants | journal = [[Proceedings of the Royal Society A]] | volume = 87 | issue = 596 | pages = 331–339 | doi = 10.1098/rspa.1912.0086 | jfm = 44.0298.02 | jstor = 93120}}</ref> named after [[William Henry Young]].\n\n== Statement ==\n\n===Euclidean Space===\n\nIn [[real analysis]], the following result is called Young's convolution inequality:<ref>{{citation\n | last       = Bogachev\n | first      = Vladimir I.\n | title      = Measure Theory\n | volume     = I\n | publisher  = Springer-Verlag\n | location   = Berlin, Heidelberg, New York\n | year       = 2007\n | isbn       = 978-3-540-34513-8\n | mr       = 2267655\n | zbl       = 1120.28001 \n}}, Theorem&nbsp;3.9.4</ref>\n\nSuppose ''f'' is in [[Lp space|''L''<sup>''p''</sup>('''R'''<sup>''d''</sup>)]] and ''g'' is in ''L''<sup>''q''</sup>('''R'''<sup>''d''</sup>) and\n\n:<math>\\frac{1}{p} + \\frac{1}{q} = \\frac{1}{r} +1</math>\n\nwith 1 ≤ ''p'', ''q'', ''r'' ≤ ∞ . Then\n\n: <math>\\|f*g\\| _r\\le\\|f\\|_p\\|g\\|_q.</math>\n\nHere the star denotes [[convolution]], ''L''<sup>''p''</sup> is [[Lp space|Lebesgue space]], and\n\n:<math>\\|f\\|_p = \\Bigl(\\int_{\\mathbf{R}^d} |f(x)|^p\\,dx \\Bigr)^{1/p}</math>\n\ndenotes the usual ''L''<sup>''p''</sup> norm.\n\nEquivalently, if <math>p, q, r \\ge 1</math> and <math>\\textstyle \\frac{1}{p} + \\frac{1}{q} + \\frac{1}{r} = 2</math> then \n:<math> \n\\int_{\\mathbf{R}^d} \\int_{\\mathbf{R}^d} f (x) g (x - y) h (y) \\,\\mathrm{d}x \\,\\mathrm{d}y\n\\le \\left(\\int_{\\mathbf{R}^d} \\vert f\\vert^p \\right)^\\frac{1}{p}\\left(\\int_{\\mathbf{R}^d} \\vert g\\vert^q \\right)^\\frac{1}{q}\\left(\\int_{\\mathbf{R}^d} \\vert h\\vert^r \\right)^\\frac{1}{r}\n</math>\n\n===Generalizations===\n\nYoung's convolution inequality has a natural generalization in which we replace <math>\\mathbb{R}^d</math> by a [[unimodular group]] <math>G</math>. If we let <math>\\mu</math> be a bi-invariant [[Haar measure]] on <math>G</math> and we let <math>f,g:G\\to\\mathbb R</math> or <math>\\mathbb C</math> be integrable functions, then we define <math>f*g</math> by\n:<math>\nf*g(x)=\\int_G f(y)g(y^{-1}x)\\,\\mathrm{d}\\mu(y).\n</math>\nThen in this case, Young's inequality states that for <math>f\\in L^p(G,\\mu)</math> and <math>g\\in L^q(G,\\mu)</math> and <math>p,q,r\\in[1,\\infty]</math> such that \n:<math> \\frac{1}{p} + \\frac{1}{q} = \\frac{1}{r} +1 </math>\nwe have a bound \n:<math>\n\\lVert f*g\\rVert_r\\le\\lVert f\\rVert_p\\lVert g\\rVert_q.\n</math>\nEquivalently, if <math>p, q, r \\ge 1</math> and <math>\\textstyle \\frac{1}{p} + \\frac{1}{q} + \\frac{1}{r} = 2</math> then \n:<math> \n\\int_G \\int_G f (x) g (y^{-1}x) h (y) \\,\\mathrm{d}\\mu(x) \\,\\mathrm{d}\\mu(y)\n\\le \\left(\\int_G \\vert f\\vert^p \\right)^\\frac{1}{p}\\left(\\int_G \\vert g\\vert^q \\right)^\\frac{1}{q}\\left(\\int_G \\vert h\\vert^r \\right)^\\frac{1}{r}.\n</math>\nSince <math>\\mathbb{R}^d</math> is in fact a locally compact abelian group (and therefore unimodular) with the Lebesgue measure the desired Haar measure, this is in fact a generalization.\n\n==Applications==\nAn example application is that Young's inequality can be used to show that the [[Weierstrass transform|heat semigroup]] is a contracting semigroup using the ''L''<sup>2</sup> norm (i.e. the [[Weierstrass transform]] does not enlarge the ''L''<sup>2</sup> norm).\n\n==Proof==\n\n===Proof by Hölder's inequality===\n\nYoung's inequality has an elementary proof with the non-optimal constant 1.<ref>{{Cite book|title=Analysis|last=Lieb|first=Elliott H.|last2=Loss|first2=Michael|date=2001|publisher=American Mathematical Society|others=|isbn=978-0-8218-2783-3|edition=2nd|series=Graduate Studies in Mathematics|location=Providence, R.I.|pages=100|oclc=45799429|author-link=Elliott H. Lieb}}</ref>\n\nWe assume that the functions <math>f,g,h:G\\to\\mathbb{R}</math> are nonnegative and integrable, where <math>G</math> is a unimodular group endowed with a bi-invariant Haar measure <math>\\mu</math>. We use the fact that <math>\\mu(S)=\\mu(S^{-1})</math> for any measurable <math>S\\subset G</math>.\nSince <math>\\textstyle p(2 - \\frac{1}{q} - \\frac{1}{r}) = q(2 - \\frac{1}{p} - \\frac{1}{r}) = r(2 - \\frac{1}{p} - \\frac{1}{q}) = 1 </math>\n:<math>\n\\int_G \\int_G f (x) g (y^{-1}x) h (y) \\,\\mathrm{d}\\mu(x) \\,\\mathrm{d}\\mu(y)\n= \\int_G \\int_G \\left(f (x)^{p} g (y^{-1}x)^q \\right)^{1 - \\frac{1}{r}}\n\\left(f (x)^p h (y)^r \\right)^{1 - \\frac{1}{q}} \\left(g (y^{-1}x)^{q} h (y)^r \\right)^{1 - \\frac{1}{p}}\\,\\mathrm{d}\\mu(x) \\,\\mathrm{d}\\mu(y)\n</math>\nBy the [[Hölder inequality]] for three functions we deduce that \n:<math>\n\\int_G \\int_G f (x) g (y^{-1}x) h (y) \\,\\mathrm{d}\\mu(x) \\,\\mathrm{d}\\mu(y)\n\\le \n\\left(\\int_G \\int_G f (x)^{p} g (y^{-1}x)^q \\,\\mathrm{d}\\mu(x) \\,\\mathrm{d}\\mu(y) \\right)^{1 - \\frac{1}{r}}\n\\left(\n\\int_G \\int_G f (x)^p h (y)^r \\,\\mathrm{d}\\mu(x) \\,\\mathrm{d}\\mu(y) \\right)^{1 - \\frac{1}{q}}\n\\left(\\int_G \\int_G g (y^{-1}x)^{q} h (y)^r \\,\\mathrm{d}\\mu(x) \\,\\mathrm{d}\\mu(y)\\right)^{1 - \\frac{1}{p}}.\n</math>\nThe conclusion follows then by left-invariance of the Haar measure, the fact that integrals are preserved by inversion of the domain, and by [[Fubini's theorem]].\n\n==Sharp constant==\nIn case ''p'',&nbsp;''q''&nbsp;>&nbsp;1 Young's inequality can be strengthened to a sharp form, via\n\n: <math>\\|f*g\\| _r\\le c_{p,q} \\|f\\|_p\\|g\\|_q.</math>\n\nwhere the constant ''c''<sub>''p'',''q''</sub>&nbsp;<&nbsp;1.<ref>{{Cite journal|last=Beckner|first=William|date=1975|title=Inequalities in Fourier Analysis|jstor=1970980|journal=Annals of Mathematics|volume=102|issue=1|pages=159–182|doi=10.2307/1970980}}</ref><ref>{{Cite journal|last=Brascamp|first=Herm Jan|last2=Lieb|first2=Elliott H|date=1976-05-01|title=Best constants in Young's inequality, its converse, and its generalization to more than three functions|url=http://www.sciencedirect.com/science/article/pii/0001870876901845|journal=Advances in Mathematics|volume=20|issue=2|pages=151–173|doi=10.1016/0001-8708(76)90184-5}}</ref><ref>{{Citation\n  | last = Fournier\n  | first = John J. F.\n  | title = Sharpness in Young's inequality for convolution\n  | journal = Pacific J. Math.\n  | volume = 72\n  | issue = 2\n  | pages = 383&ndash;397\n  | year = 1977\n  | url = http://projecteuclid.org/DPubS?verb=Display&version=1.0&service=UI&handle=euclid.pjm/1102811121&page=record\n  | doi = 10.2140/pjm.1977.72.383\n  | zbl = 0357.43002\n  | mr = 0461034}}\n</ref> When this optimal constant is achieved, the function <math>f</math> and <math>g</math> are [[Gaussian function|multidimensional Gaussian functions]].\n\n==Notes==\n{{reflist}}\n\n==External links==\n*[https://proofwiki.org/wiki/Young's_Inequality_for_Convolutions ''Young's Inequality for Convolutions] at ProofWiki\n\n{{DEFAULTSORT:Young's Convolution Inequality}}\n[[Category:Inequalities]]"
    },
    {
      "title": "Young's inequality for products",
      "url": "https://en.wikipedia.org/wiki/Young%27s_inequality_for_products",
      "text": "In [[mathematics]], '''Young's inequality for products''' is a [[inequality (mathematics)|mathematical inequality]] about the product of two numbers.<ref>{{citation | first1 = W. H. | last1 = Young | author-link = William Henry Young | year = 1912 | title = On classes of summable functions and their Fourier series | journal = [[Proceedings of the Royal Society A]] | volume=87 | pages=225–229 | jfm = 43.1114.12 | jstor = 93236 | doi = 10.1098/rspa.1912.0076 | issue = 594 }}</ref> The inequality is named after [[William Henry Young]] and should not be confused with [[Young's convolution inequality]].\n\nYoung's inequality for products can be used to prove [[Hölder's inequality]]. It is also widely used to estimate the norm of nonlinear terms in [[partial differential equation|PDE theory]], since it allows one to estimate a product of two terms by a sum of the same terms raised to a power and scaled.\n\n==Standard version for conjugate Hölder exponents==\nIn its standard form, the inequality states that if ''a'' and ''b'' are [[nonnegative]] [[real number]]s and ''p'' and ''q'' are real numbers greater than 1 such that 1/''p''&nbsp;+&nbsp;1/''q''&nbsp;=&nbsp;1, then\n:<math>ab \\le \\frac{a^p}{p} + \\frac{b^q}{q}~.</math>\n\nThe equality holds if and only if {{math|''a''<sup>''p''</sup> {{=}} ''b''<sup>''q''</sup>}}. This form of Young's inequality can be proved by [[Jensen's inequality]] and can be used to prove [[Hölder's inequality]].\n\n<div style=\"clear:both;width:95%;\" class=\"NavFrame\">\n<div class=\"NavHead\" style=\"background-color:#FFFAF0; text-align:left; font-size:larger;\">Proof</div>\n<div class=\"NavContent\" style=\"text-align:left;display:none;\">\nThe claim is certainly true if ''a''&nbsp;=&nbsp;0 or ''b''&nbsp;=&nbsp;0.  Therefore, assume ''a''&nbsp;>&nbsp;0 and ''b''&nbsp;>&nbsp;0 in the following. Put ''t''&nbsp;=&nbsp;1/''p'', and (1&nbsp;&minus;&nbsp;''t'')&nbsp;=&nbsp;1/''q''. Then since the [[logarithm]] function is [[concave function|concave]],\n:<math> \\ln(t a^p + (1-t) b^q) \\ge t \\ln(a^p) + (1-t) \\ln(b^q) =\n                          \\ln(a) +\\ln(b) = \\ln(ab)\n</math>\nwith the equality holding if and only if ''a''<sup>''p''</sup>&nbsp;=&nbsp;''b''<sup>''q''</sup>. Young's inequality follows by exponentiating.\n</div>\n</div>\n\nEquivalently, it could be written as\n:<math>a^\\alpha b^\\beta \\le \\alpha a + \\beta b, 0\\le \\alpha,\\beta\\le 1,\\alpha+\\beta=1 ~.</math>\nIt is just the concavity of the logarithm function. The equality holds if and only if <math>a=b</math>.\n\nGenerally, \n:<math>\\prod_i{a_i}^{\\alpha_i} \\le \\sum_i \\alpha_i a_i, 0\\le \\alpha_i\\le 1, \\sum_i \\alpha_i=1~.</math>\nThe equality holds if and only if <math>a_i</math> is a constant.\n\n==Elementary case==\nAn elementary case of Young's inequality is the inequality with [[exponent]] 2,\n\n:<math>ab \\le \\frac{a^2}{2} + \\frac{b^2}{2},</math>\n\nwhich also gives rise to the so-called Young's inequality with ''ε'' (valid for every ''ε''&nbsp;>&nbsp;0), sometimes called the Peter–Paul inequality.\n<ref>{{citation\n | last1       = Tisdell\n | first1      = Chris\n | title      = The Peter Paul Inequality\n | publisher  = YouTube video on Dr Chris Tisdell's YouTube channel\n | year       = 2013\n | url        = https://www.youtube.com/watch?v=C_bjbjTzHP4\n}},</ref>  This name refers to the fact that tighter control of the second term is achieved at the cost of losing some control of the first term – one must \"rob Peter to pay Paul\"\n\n:<math>ab \\le \\frac{a^2}{2\\varepsilon} + \\frac{\\varepsilon b^2}{2}.</math>\n\n<div style=\"clear:both;width:95%;\" class=\"NavFrame\">\n<div class=\"NavHead\" style=\"background-color:#FFFAF0; text-align:left; font-size:larger;\">Proof</div>\n<div class=\"NavContent\" style=\"text-align:left;display:none;\">\nYoung's inequality with exponent 2 is the special case ''p'' = ''q'' = 2. However, it has a more elementary proof.\n\nStart by observing that the square of every real number is zero or positive.  Therefore, for every pair of real numbers ''a'' and ''b'' we can write:\n\n:<math>0\\le (a-b)^2</math> \nWork out the square of the right hand side:\n:<math>0\\le a^2-2ab+b^2</math> \nAdd '2ab' to both sides:\n:<math>2ab\\le a^2+b^2</math>\nDivide both sides by 2 and we have Young's inequality with exponent&nbsp;2:\n:<math>ab\\le \\frac{a^2}{2}+ \\frac{b^2}{2}</math>\n\nYoung's inequality with ''ε'' follows by substituting <math>a'</math> and <math>b'</math> as below into Young's inequality with exponent&nbsp;2: \n:<math>a'=a/\\sqrt{\\varepsilon},\\text{ }b'=\\sqrt{\\varepsilon}b.</math>\n</div>\n</div>\n\n==Matricial generalization==\nT. Ando proved a generalization of Young's inequality for complex matrices ordered \nby [[Loewner order]]ing.<ref>{{cite book|author1=T. Ando|editor1-last=Huijsmans|editor1-first=C. B.|editor2-last=Kaashoek|editor2-first=M. A.|editor3-last=Luxemburg|editor3-first=W. A. J.|editor4-last=de Pagter|editor4-first=B.| display-editors = 3|title=Operator Theory in Function Spaces and Banach Lattices|date=1995|publisher=Springer|isbn=978-3-0348-9076-2|pages=33–38|chapter=Matrix Young Inequalities}}</ref> It states that for any pair ''A'', ''B'' of complex matrices of order ''n'' there exists a unitary matrix ''U'' such that\n\n:<math> U^{*} |AB^{*}| U \\preceq \\frac{1}{p}|A|^{p} + \\frac{1}{q} |B|^{q}, </math>\n\nwhere * denotes the [[conjugate transpose]] of the matrix and <math>|A| = \\sqrt{A^{*}A}</math>.\n\n==Standard version for increasing functions==\n[[File:Young.png|thumb|The area of the rectangle a,b can't be larger than sum of the areas under the functions <math>f</math> (red) and <math>f^{-1}</math> (yellow)]]\nFor the standard version<ref>{{citation\n | last1       = Hardy\n | first1      = G. H.\n | last2       = Littlewood\n | first2      = J. E.\n | last3       = Pólya\n | first3      = G.\n | author1-link = Godfrey Harold Hardy\n | author2-link = John Edensor Littlewood\n | author3-link = George Pólya\n | title      = Inequalities\n | edition    = 2nd\n | series     = Cambridge Mathematical Library\n | publisher  = Cambridge University Press\n | location   = Cambridge\n | year       = 1952\n | origyear   = 1934\n | pages      = \n | isbn       = 0-521-05206-8 \n | url        = http://www.cambridge.org/catalogue/catalogue.asp?isbn=9780521358804\n | mr       = 0046395\n | zbl       = 0047.05302\n}}, Chapter&nbsp;4.8</ref><ref>{{citation\n | last1       = Henstock\n | first1      = Ralph\n | author1-link = Ralph Henstock\n | title      = Lectures on the Theory of Integration\n | series     = Series in Real Analysis Volume I\n | publisher  = World Scientific\n | location   = Singapore, New Jersey\n | year       = 1988\n | isbn       = 9971-5-0450-2\n | mr       = 0963249\n | zbl       = 0668.28001 \n}}, Theorem&nbsp;2.9</ref> of the inequality,\nlet ''f'' denote a real-valued, continuous and strictly increasing function on [0,&nbsp;''c''] with ''c''&nbsp;>&nbsp;0 and ''f''(0)&nbsp;=&nbsp;0. Let ''f''<sup>&minus;1</sup> denote the [[inverse function]] of&nbsp;''f''. Then, for all ''a''&nbsp;∈&nbsp;[0,&nbsp;''c''] and ''b''&nbsp;∈&nbsp;[0,&nbsp;''f''(''c'')],\n\n: <math>ab \\le \\int_0^a f(x)\\,dx + \\int_0^b f^{-1}(x)\\,dx</math>\n\nwith equality if and only if ''b''&nbsp;=&nbsp;''f''(''a'').\n\nWith <math>f(x)=x^{p-1}</math> and <math>f^{-1}(y)=y^{q-1}</math>, this reduces to standard version for conjugate Hölder exponents.\n\n\nFor details and generalizations we refer to the paper of Mitroi & Niculescu <ref>Mitroi, F. C., & Niculescu, C. P. (2011). An extension of Young's inequality. In Abstract and Applied Analysis (Vol. 2011). Hindawi.</ref>.\n\n==Generalization using Fenchel–Legendre transforms ==\nIf ''f'' is a [[convex function]] and its [[Legendre transformation]] ([[convex conjugate]]) is denoted by ''g'', then\n:<math> ab \\le f(a) + g(b). </math>\nThis follows immediately from the definition of the Legendre transform.\n\nMore generally, if ''f'' is a [[convex function]] defined on a real vector space <math>X</math> and its [[convex conjugate]] is denoted by <math>f^\\star</math> (and is defined on the [[dual space]] <math>X^\\star</math>), then\n:<math> \\langle u, v \\rangle \\le f^\\star(u) + f(v). </math>\nwhere <math>\\langle \\cdot , \\cdot \\rangle : X^\\star \\times X \\to \\mathbb{R}</math> is the [[dual pair]]ing.\n\n===Examples===\n*The Legendre transform of ''f''(''a'') = ''a''<sup>''p''</sup>/''p'' is ''g''(''b'') = ''b''<sup>''q''</sup>/''q'' with ''q'' such that 1/''p''&nbsp;+&nbsp;1/''q''&nbsp;=&nbsp;1, and thus Young's inequality for conjugate Hölder exponents mentioned above is a special case.\n*The  Legendre transform of ''f''(''a'') = e<sup>''a''</sup> – 1 is  ''g''(''b'') = 1&nbsp;&minus;&nbsp;''b''&nbsp;+&nbsp;''b'' ln ''b'', hence ''ab''&nbsp;≤&nbsp;e<sup>''a''</sup>&nbsp;&minus;&nbsp;''b''&nbsp;+&nbsp;''b''&nbsp;ln&nbsp;''b'' for all non-negative ''a'' and ''b''. This estimate is useful in [[large deviations theory]] under exponential moment conditions, because ''b'' ln ''b'' appears in the definition of [[relative entropy]], which is the [[rate function]] in [[Sanov's theorem]].\n\n== See also ==\n* [[Convex conjugate]]\n* [[Legendre transformation]]\n\n==Notes==\n{{reflist}}\n\n==External links==\n*[http://planetmath.org/encyclopedia/YoungsInequality.html ''Young's Inequality] at [[PlanetMath]]\n*{{MathWorld|title=Young's Inequality|urlname=YoungsInequality}}\n\n{{DEFAULTSORT:Young's Inequality}}\n[[Category:Inequalities]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "Young's inequality for integral operators",
      "url": "https://en.wikipedia.org/wiki/Young%27s_inequality_for_integral_operators",
      "text": "{{technical|date=July 2017}}\n\nIn [[mathematical analysis]], the '''Young's inequality for integral operators''', is a bound on the <math>L^p\\to L^q</math> [[operator norm]] of an [[integral operator]] in terms of <math>L^r</math> norms of the kernel itself.\n\n==Statement==\n\nIf <math>X</math> and <math>Y</math> are measurable spaces, if <math>K : X \\times Y \\to \\mathbf{R}</math> and <math>\\frac{1}{q} = \\frac{1}{p} + \\frac{1}{r} -1</math>, if \n:<math>\n\\int_{X} |K (x, y)|^r \\,\\mathrm{d} x \\le C^r\n</math>\nand \n:<math>\n\\int_{Y} |K (x, y)|^r \\,\\mathrm{d} y \\le C^r\n</math>\nthen <ref>Theorem 0.3.1 in: [[Christopher D. Sogge|C. D. Sogge]], ''Fourier integral operators in classical analysis'', Cambridge University Press, 1993. {{ISBN|0-521-43464-5}}</ref>\n:<math>\n  \\int_{X} \\left|\\int_{Y} K (x, y) f(y) \\,\\mathrm{d} y\\right|^q \\, \\mathrm{d} x\n\\le C^q \\left( \\int_{Y} |f(y)|^p \\,\\mathrm{d} y\\right)^\\frac{q}{p}.\n</math>\n\n==Particular cases==\n\n=== Convolution kernel ===\n\nIf <math>X = Y = \\mathbb{R}^d</math> and <math>K (x, y) = h (x - y) </math>, then the inequality becomes [[Young's convolution inequality]].\n\n==See also==\n[[Young's inequality for products]]\n\n==Notes==\n{{reflist}}\n\n[[Category:Inequalities]]\n\n\n{{mathanalysis-stub}}"
    },
    {
      "title": "Z-channel (information theory)",
      "url": "https://en.wikipedia.org/wiki/Z-channel_%28information_theory%29",
      "text": "A '''Z-channel''' is a [[communications channel]] used in [[coding theory]] and [[information theory]] to model the behaviour of some data storage systems.\n\n== Definition ==\nA ''Z-channel'' (or a ''binary asymmetric channel'') is a channel with binary input and binary output where the crossover 1 → 0 occurs with nonnegative probability ''p'', whereas the crossover 0 → 1 never occurs. In other words, if ''X'' and ''Y'' are the [[random variable]]s describing the probability distributions of the input and the output of the channel, respectively, then the crossovers of the channel are characterized by the [[conditional probability|conditional probabilities]]\n: Prob{''Y'' = 0 | ''X'' = 0} = 1\n: Prob{''Y'' = 0 | ''X'' = 1} = ''p''\n: Prob{''Y'' = 1 | ''X'' = 0} = 0\n: Prob{''Y'' = 1 | ''X'' = 1} = 1&minus;''p''\n\n== Capacity ==\nThe [[channel capacity|capacity]] <math>\\mathsf{cap}(\\mathbb{Z})</math> of the Z-channel <math>\\mathbb{Z}</math> with the crossover 1 → 0 probability ''p'', when the input random variable ''X'' is distributed according to the [[Bernoulli distribution]] with probability ''<math>\\alpha</math>'' for the occurrence of 0, is calculated as follows.\n:<math>\\mathsf{cap}(\\mathbb{Z}) = \n\\max_\\alpha\\{\\mathsf{H}(Y) - \\mathsf{H}(Y \\mid X)\\} = \\max_\\alpha\\Bigl\\{\\mathsf{H}(Y) - \\sum_{x \\in \\{0,1\\}}\\mathsf{H}(Y \\mid X = x) \\mathsf{Prob}\\{X = x\\}\\Bigr\\} </math>\n::::<math>=\\max_\\alpha\\{\\mathsf{H}((1-\\alpha)(1-p)) - \\mathsf{H}(Y \\mid X = 1) \\mathsf{Prob}\\{X = 1\\} \\}</math>\n::::<math>=\\max_\\alpha\\{\\mathsf{H}((1-\\alpha)(1-p)) - (1-\\alpha)\\mathsf{H}(p) \\},</math>\nwhere <math>\\mathsf{H}(\\cdot)</math> is the [[binary entropy function]].\n\nThe maximum is attained for\n:<math>\\alpha = 1 - \\frac{1}{(1-p)(1+2^{\\mathsf{H}(p)/(1-p)})},</math>\nyielding the following value of <math>\\mathsf{cap}(\\mathbb{Z})</math> as a function of ''p''\n:<math>\\mathsf{cap}(\\mathbb{Z}) = \\mathsf{H}\\left(\\frac{1}{1+2^{\\mathsf{s}(p)}}\\right) - \\frac{\\mathsf{s}(p)}{1+2^{\\mathsf{s}(p)}} = \\log_2(1{+}2^{-\\mathsf{s}(p)}) = \\log_2\\left(1+(1-p) p^{p/(1-p)}\\right) \\; \\textrm{ where } \\; \\mathsf{s}(p) = \\frac{\\mathsf{H}(p)}{1-p}.</math>\n\nFor small ''p'', the capacity is approximated by\n\n:<math> \\mathsf{cap}(\\mathbb{Z}) \\approx 1- 0.5 \\mathsf{H}(p) </math>\nas compared to the capacity <math>1{-}\\mathsf{H}(p)</math> of the [[binary symmetric channel]] with crossover probability ''p''.\n\n== Bounds on the size of an asymmetric-error-correcting code ==\nDefine the following distance function <math>\\mathsf{d}_A(\\mathbf{x}, \\mathbf{y})</math> on the words <math>\\mathbf{x}, \\mathbf{y} \\in \\{0,1\\}^n</math> of length ''n'' transmitted via a Z-channel\n:<math>\\mathsf{d}_A(\\mathbf{x}, \\mathbf{y}) \\stackrel{\\vartriangle}{=} \\max\\left\\{ \\big|\\{i \\mid x_i = 0, y_i = 1\\}\\big| , \\big|\\{i \\mid x_i = 1, y_i = 0\\}\\big| \\right\\}.</math>\nDefine the sphere <math>V_t(\\mathbf{x})</math> of radius ''t'' around a word <math>\\mathbf{x} \\in \\{0,1\\}^n</math> of length ''n'' as the set of all the words at distance ''t'' or less from <math>\\mathbf{x}</math>, in other words,\n:<math>V_t(\\mathbf{x}) = \\{\\mathbf{y} \\in \\{0, 1\\}^n \\mid \\mathsf{d}_A(\\mathbf{x}, \\mathbf{y}) \\leq t\\}.</math>\nA [[code]] <math>\\mathcal{C}</math> of length ''n'' is said to be ''t''-asymmetric-error-correcting if for any two codewords <math>\\mathbf{c}\\ne \\mathbf{c}' \\in \\{0,1\\}^n</math>, one has <math>V_t(\\mathbf{c}) \\cap V_t(\\mathbf{c}') = \\emptyset</math>. Denote by <math>M(n,t)</math> the maximum number of codewords in a ''t''-asymmetric-error-correcting code of length ''n''.\n\n'''The Varshamov bound'''.\nFor ''n''≥1 and ''t''≥1,\n:<math>M(n,t) \\leq \\frac{2^{n+1}}{\\sum_{j = 0}^t{\\left( \\binom{\\lfloor n/2\\rfloor}{j}+\\binom{\\lceil n/2\\rceil}{j}\\right)}}.</math>\n\n'''The constant-weight{{what|date=November 2014}} code bound'''.\nFor ''n > 2t ≥ 2'', let the sequence ''B<sub>0</sub>, B<sub>1</sub>, ..., B<sub>n-2t-1</sub>'' be defined as\n:<math>B_0 = 2, \\quad B_i = \\min_{0 \\leq j < i}\\{ B_j + A(n{+}t{+}i{-}j{-}1, 2t{+}2, t{+}i)\\}</math> for <math>i > 0</math>.\nThen <math>M(n,t) \\leq B_{n-2t-1}.</math>\n\n== References ==\n* {{Smallcaps|T. Kløve,}} Error correcting codes for the asymmetric channel, ''Technical Report 18–09–07–81,'' Department of Informatics, University of Bergen, Norway, 1981.\n* {{Smallcaps|S. Verdú,}} Channel Capacity, in ''The electrical engineering handbook,'' 2nd ed., IEEE Press and CRC Press, 1997, ch. 73.5, pp. 1671-1678. \n* {{Smallcaps|L.G. Tallini, S. Al-Bassam, B. Bose,}} On the capacity and codes for the Z-channel, ''Proceedings of the IEEE International Symposium on Information Theory,'' Lausanne, Switzerland, 2002, p.&nbsp;422.\n\n[[Category:Coding theory]]\n[[Category:Information theory]]\n[[Category:Inequalities]]"
    },
    {
      "title": "Barrow's inequality",
      "url": "https://en.wikipedia.org/wiki/Barrow%27s_inequality",
      "text": "[[Image:Barrow inequality.svg|thumb|right|300px]]\n\nIn [[geometry]], '''Barrow's inequality''' is an [[Inequality (mathematics)|inequality]] relating the [[Euclidean distance|distances]] between an arbitrary point within a [[triangle]], the vertices of the triangle, and certain points on the sides of the triangle.\n\n==Statement==\nLet ''P'' be an arbitrary point inside the [[triangle]] ''ABC''. From ''P'' and ''ABC'', define ''U'', ''V'', and ''W'' as the points where the [[angle bisector]]s of ''BPC'', ''CPA'', and ''APB'' intersect the sides ''BC'', ''CA'', ''AB'', respectively. Then Barrow's inequality states that\n\n: <math>PA+PB+PC\\geq 2(PU+PV+PW),\\,</math>\n\nwith equality holding only in the case of an [[equilateral triangle]].\n\n==History==\nBarrow's inequality strengthens the [[Erdős–Mordell inequality]], which has identical form except with ''PU'', ''PV'', and ''PW'' replaced by the three distances of ''P'' from the triangle's sides. It is named after [[David Francis Barrow]]. Barrow's proof of this inequality was published in 1937, as his solution to a problem posed in the [[American Mathematical Monthly]] of proving the Erdős–Mordell inequality.<ref>{{citation\n | last1 = Erdős | first1 = Paul | author1-link = Paul Erdős\n | last2 = Mordell | first2 = L. J. | author2-link = Louis J. Mordell\n | last3 = Barrow | first3 = David F. | author3-link = David Francis Barrow\n | issue = 4\n | journal = [[American Mathematical Monthly]]\n | jstor = 2300713\n | pages = 252–254\n | title = Solution to problem 3740\n | volume = 44\n | year = 1937 | doi=10.2307/2300713}}.</ref> \n\nA simpler proof was later given by Mordell.<ref>{{citation\n | last = Mordell | first = L. J. | author-link = Louis J. Mordell\n | issue = 357\n | journal = Mathematical Gazette\n | jstor = 3614019\n | pages = 213–215\n | title = On geometric problems of Erdös and Oppenheim\n | volume = 46\n | year = 1962}}.</ref>\n\n== See also ==\n* [[Euler's theorem in geometry]]\n* [[List of triangle inequalities]]\n\n==References==\n{{reflist}}\n\n==External links==\n*[https://web.archive.org/web/20101226171804/http://www.eleves.ens.fr/home/kortchem/olympiades/Cours/Inegalites/tin2006.pdf Hojoo Lee: Topics in Inequalities - Theorems and Techniques]\n\n[[Category:Triangle geometry]]\n[[Category:Geometric inequalities]]"
    },
    {
      "title": "Berger–Kazdan comparison theorem",
      "url": "https://en.wikipedia.org/wiki/Berger%E2%80%93Kazdan_comparison_theorem",
      "text": "In [[mathematics]], the '''Berger&ndash;Kazdan comparison theorem''' is a result in [[Riemannian geometry]] that gives a lower bound on the volume of a [[Riemannian manifold]] and also gives a [[necessary and sufficient condition]] for the manifold to be [[Isometry (Riemannian geometry)|isometric]] to the ''m''-[[dimension]]al [[sphere]] with its usual \"round\" metric. The theorem is named after the [[mathematician]]s [[Marcel Berger]] and [[Jerry Kazdan]].\n\n==Statement of the theorem==\nLet (''M'',&nbsp;''g'') be a [[compact space|compact]] ''m''-dimensional Riemannian manifold with [[injectivity radius]] inj(''M''). Let ''vol'' denote the volume form on ''M'' and let ''c''<sub>''m''</sub>(''r'') denote the volume of the standard ''m''-dimensional sphere of radius ''r''. Then\n\n:<math>\\mathrm{vol} (M) \\geq \\frac{c_{m} (\\mathrm{inj}(M))}{\\pi^m},</math>\n\nwith equality [[if and only if]] (''M'',&nbsp;''g'') is isometric to the ''m''-sphere '''S'''<sup>''m''</sup> with its usual round metric.\n\n==References==\n*{{cite book|last = Berger|first = Marcel|authorlink=Marcel Berger|author2=Kazdan, Jerry L. |authorlink2=Jerry Kazdan |chapter = A Sturm&ndash;Liouville inequality with applications to an isoperimetric inequality for volume in terms of injectivity radius, and to Wiedersehen manifolds|title = Proceedings of Second International Conference on General Inequalities, 1978|publisher = Birkhauser|year = 1980|pages = 367&ndash;377}}\n*{{cite journal|last = Kodani|first = Shigeru|title = An Estimate on the Volume of Metric Balls|journal = Kodai Mathematical Journal|volume = 11|issue = 2|year = 1988|pages = 300&ndash;305|url = http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.kmj/1138038881|doi = 10.2996/kmj/1138038881}}\n\n==External links==\n* {{MathWorld|urlname=Berger-KazdanComparisonTheorem|title=Berger-Kazdan comparison theorem}}\n\n{{DEFAULTSORT:Berger-Kazdan comparison theorem}}\n[[Category:Geometric inequalities]]\n[[Category:Theorems in Riemannian geometry]]\n\n\n{{differential-geometry-stub}}"
    },
    {
      "title": "Berger's inequality for Einstein manifolds",
      "url": "https://en.wikipedia.org/wiki/Berger%27s_inequality_for_Einstein_manifolds",
      "text": "In [[mathematics]] &mdash; specifically, in [[differential topology]] &mdash; '''Berger's inequality for Einstein manifolds''' is the statement that any 4-dimensional [[Einstein manifold]] (''M'',&nbsp;''g'') has non-negative [[Euler characteristic]] ''&chi;''(''M'')&nbsp;≥&nbsp;0. The inequality is named after the [[France|French]] [[mathematician]] [[Marcel Berger]].\n\n==See also==\n*[[Hitchin–Thorpe inequality]]\n\n== References ==\n*{{cite book | first = Arthur L. | last = Besse | title = Einstein Manifolds | series = Classics in Mathematics | publisher = Springer | location = Berlin | year = 1987 | isbn = 3-540-74120-8}}\n\n[[Category:Riemannian manifolds|Einstein manifolds]]\n[[Category:4-manifolds|Einstein manifolds]]\n[[Category:Geometric inequalities]]\n[[Category:Differential topology]]\n\n{{differential-geometry-stub}}"
    },
    {
      "title": "Bishop–Gromov inequality",
      "url": "https://en.wikipedia.org/wiki/Bishop%E2%80%93Gromov_inequality",
      "text": "In [[mathematics]], the '''Bishop–Gromov inequality''' is a [[comparison theorem#Riemannian geometry|comparison theorem in Riemannian geometry]], named after [[Richard L. Bishop]] and [[Mikhail Gromov (mathematician)|Mikhail Gromov]]. It is closely related to [[Myers' theorem]], and is the key point in the proof of [[Gromov's compactness theorem (geometry)|Gromov's compactness theorem]].<ref>{{cite book|last=Petersen|first=Peter|title=Riemannian Geometry|edition=3|publisher=Springer|year=2016|section=Section 11.1.3|ISBN=978-3-319-26652-7}}</ref>\n\n==Statement==\nLet <math>M</math> be a complete ''n''-dimensional Riemannian manifold whose [[Ricci curvature]] satisfies the lower bound\n\n: <math>\\mathrm{Ric} \\geq (n-1) K \\,</math>\n\nfor a constant <math>K\\in \\mathbb{R}</math>. Let <math>M_K^n</math> be the complete ''n''-dimensional [[simply connected]]  space  of constant [[sectional curvature]] <math>K</math> (and hence of constant Ricci curvature <math>(n-1)K</math>); thus  <math>M_K^n</math> is the ''n''-[[sphere]] of  radius <math>1/\\sqrt{K}</math> if ''K''&nbsp;>&nbsp;0, or   ''n''-dimensional [[Euclidean space]] if <math>K=0</math>, or  an appropriately  rescaled version of ''n''-dimensional [[hyperbolic space]] if <math>K<0</math>. Denote by ''B''(''p'',&nbsp;''r'') the ball of radius ''r'' around a point ''p'', defined with respect to the [[Riemannian manifold#Riemannian manifolds as metric spaces|Riemannian distance function]].\n\nThen, for any  <math>p\\in M</math> and <math>p_K\\in M_K^n</math>,   the function\n\n: <math> \\phi(r) =  \\frac{\\mathrm{Vol} \\, B(p,r)}{\\mathrm{Vol}\\, B(p_K,r)} </math>\n\nis non-increasing on (0,&nbsp;∞).\n\nAs ''r'' goes to zero, the ratio approaches one, so together with the monotonicity this implies that\n: <math>\\mathrm{Vol} \\,B(p,r) \\leq \\mathrm{Vol} \\, B(p_K,r).</math>\nThis is the version first proved by  Bishop.<ref>Bishop, R. A relation between volume, mean curvature,\nand diameter. Amer. Math. Soc. Not. 10 (1963), p. 364.</ref><ref>Bishop R.L., Crittenden R.J. Geometry of manifolds, Corollary 4, p. 256</ref><!-- NOTE: Bishop had no assumption on the injectivity radius-->\n\n==See also==\n* [[Comparison theorem]]\n* [[Gromov's inequality (disambiguation)]]\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Bishop-Gromov Inequality}}\n[[Category:Riemannian geometry]]\n[[Category:Geometric inequalities]]"
    },
    {
      "title": "Bonnesen's inequality",
      "url": "https://en.wikipedia.org/wiki/Bonnesen%27s_inequality",
      "text": "'''Bonnesen's inequality''' is an [[inequality (mathematics)|inequality]] relating the length, the area, the radius of the [[incircle]] and the radius of the [[circumcircle]] of a [[Jordan curve]].  It is a strengthening of the classical [[isoperimetry|isoperimetric inequality]].\n\nMore precisely, consider a planar simple closed curve of length <math>L</math> bounding a domain of area <math>A</math>. Let <math>r</math> and <math>R</math> denote the radii of the incircle and the circumcircle.  Bonnesen proved the inequality\n\n:<math> \\pi^2 (R-r)^2 \\leq L^2-4\\pi A. \\, </math>\n\nThe term <math> \\pi^2 (R-r)^2</math> in the left hand side is known as the ''isoperimetric defect''.\n\n[[Loewner's torus inequality]] with isosystolic defect is a [[Systolic geometry|systolic]] analogue of Bonnesen's inequality.\n\n==References==\n* Bonnesen, T.: \"Sur une amélioration de l'inégalité isopérimetrique du cercle et la démonstration d'une inégalité de Minkowski,\" ''C. R. Acad. Sci. Paris'' '''172''' (1921), 1087–1089.\n* Yu. D. Burago and V. A. Zalgaller, ''Geometric inequalities''. Translated from the Russian by A. B. Sosinskiĭ. Springer-Verlag, Berlin, 1988. {{ISBN|3-540-13615-0}}.\n\n[[Category:Elementary geometry]]\n[[Category:Geometric inequalities]]\n\n\n{{geometry-stub}}"
    },
    {
      "title": "Borell–Brascamp–Lieb inequality",
      "url": "https://en.wikipedia.org/wiki/Borell%E2%80%93Brascamp%E2%80%93Lieb_inequality",
      "text": "In [[mathematics]], the '''Borell–Brascamp–Lieb inequality''' is an [[integral]] [[inequality (mathematics)|inequality]] due to many different mathematicians but named after [[Christer Borell]], [[Herm Jan Brascamp]] and [[Elliott Lieb]].\n\nThe result was proved for ''p''&nbsp;&gt;&nbsp;0 by Henstock and Macbeath in 1953. The case ''p''&nbsp;=&nbsp;0 is known as the [[Prékopa–Leindler inequality]] and was re-discovered by Brascamp and Lieb in 1976, when they proved the general version below; working independently, Borell had done the same in 1975. The nomenclature of \"Borell–Brascamp–Lieb inequality\" is due to Cordero-Erausquin, [[Robert McCann (mathematician)|McCann]] and Schmuckenschläger, who in 2001 generalized the result to [[Riemannian manifold]]s such as the [[sphere]] and [[hyperbolic space]].\n\n==Statement of the inequality in '''R'''<sup>''n''</sup>==\n\nLet 0&nbsp;&lt;&nbsp;''λ''&nbsp;&lt;&nbsp;1, let &minus;1&nbsp;/&nbsp;''n''&nbsp;≤&nbsp;''p''&nbsp;≤&nbsp;+∞, and let ''f'', ''g'', ''h''&nbsp;:&nbsp;'''R'''<sup>''n''</sup>&nbsp;→&nbsp;[0,&nbsp;+∞) be integrable functions such that, for all ''x'' and ''y'' in '''R'''<sup>''n''</sup>,\n\n:<math>h \\left( (1 - \\lambda) x + \\lambda y \\right) \\geq M_{p} \\left( f(x), g(y), \\lambda \\right),</math>\n\nwhere\n\n:<math>\n\\begin{align}\nM_{p} (a, b, \\lambda) =\n\\begin{cases}\n &\\left( (1 - \\lambda) a^{p} + \\lambda b^{p} \\right)^{1/p} \\; \\quad \\text{if} \\quad ab\\neq 0\\\\\n&0  \\quad \\text{if} \\quad ab=0\n\\end{cases}\n\\end{align}\n</math>\nand <math>M_{0}(a,b,\\lambda) = a^{1-\\lambda}b^{\\lambda}</math>.\n\nThen\n\n:<math>\\int_{\\mathbb{R}^{n}} h(x) \\, \\mathrm{d} x \\geq M_{p / (n p + 1)} \\left( \\int_{\\mathbb{R}^{n}} f(x) \\, \\mathrm{d} x, \\int_{\\mathbb{R}^{n}} g(x) \\, \\mathrm{d} x, \\lambda \\right).</math>\n\n(When ''p''&nbsp;=&nbsp;&minus;1&nbsp;/&nbsp;''n'', the convention is to take ''p''&nbsp;/&nbsp;(''n''&nbsp;''p''&nbsp;+&nbsp;1) to be &minus;∞; when ''p''&nbsp;=&nbsp;+∞, it is taken to be 1&nbsp;/&nbsp;''n''.)\n\n==References==\n\n* {{cite journal\n|     last = Borell\n|    first = Christer\n|    title = Convex set functions in ''d''-space\n|  journal = Period. Math. Hungar.\n|   volume = 6\n|     year = 1975\n|   number = 2\n|    pages = 111&ndash;136\n|    doi = 10.1007/BF02018814\n}}\n* {{cite journal\n|author1=Brascamp, Herm Jan  |author2=Lieb, Elliott H.\n |lastauthoramp=yes |    title = On extensions of the Brunn–Minkowski and Prékopa–Leindler theorems, including inequalities for log concave functions, and with an application to the diffusion equation\n|  journal = J. Functional Analysis\n|   volume = 22\n|     year = 1976\n|   number = 4\n|    pages = 366&ndash;389\n|    doi = 10.1016/0022-1236(76)90004-5\n}}\n* {{cite journal\n|   author = Cordero-Erausquin, Dario\n|   author2 = McCann, Robert J.\n|   author2-link = Robert McCann (mathematician)\n|   author3 = Schmuckenschläger, Michael\n|   last-author-amp = yes\n|    title = A Riemannian interpolation inequality à la Borell, Brascamp and Lieb\n|  journal = Invent. Math.\n|   volume = 146\n|     year = 2001\n|   number = 2\n|    pages = 219&ndash;257\n|    doi = 10.1007/s002220100160\n}}\n* {{cite journal \n| last=Gardner \n| first=Richard J. \n| title=The Brunn–Minkowski inequality \n| journal=Bull. Amer. Math. Soc. (N.S.) \n| volume=39 \n| issue=3 \n| year=2002 \n| pages=355&ndash;405 (electronic)\n| url = http://www.ams.org/bull/2002-39-03/S0273-0979-02-00941-2/S0273-0979-02-00941-2.pdf \n| doi=10.1090/S0273-0979-02-00941-2\n}}\n* {{cite journal\n|   author = Henstock, R. |author2=Macbeath, A. M. |author2-link=Alexander M. Macbeath\n|    title = On the measure of sum-sets. I. The theorems of Brunn, Minkowski, and Lusternik\n|  journal = Proc. London Math. Soc. |series=Series 3\n|   volume = 3\n|     year = 1953\n|    pages = 182&ndash;194\n|    doi = 10.1112/plms/s3-3.1.182\n}}\n\n{{DEFAULTSORT:Borell-Brascamp-Lieb inequality}}\n[[Category:Geometric inequalities]]\n[[Category:Integral geometry]]"
    },
    {
      "title": "Brascamp–Lieb inequality",
      "url": "https://en.wikipedia.org/wiki/Brascamp%E2%80%93Lieb_inequality",
      "text": "In [[mathematics]], the '''Brascamp–Lieb inequality''' is either of two inequalities. The first is a result in [[geometry]] concerning [[integrable function]]s on ''n''-[[dimension]]al [[Euclidean space]] <math>\\mathbb{R}^{n}</math>. It generalizes the [[Loomis–Whitney inequality]] and [[Hölder's inequality]]. The second is a result of probability theory which gives a concentration inequality for log-concave probability distributions. Both are named after [[Herm Jan Brascamp]] and [[Elliott H. Lieb]].\n\n==The geometric inequality==\n\nFix [[natural number]]s ''m'' and ''n''. For 1&nbsp;≤&nbsp;''i''&nbsp;≤&nbsp;''m'', let ''n''<sub>''i''</sub>&nbsp;∈&nbsp;'''N''' and let ''c''<sub>''i''</sub>&nbsp;&gt;&nbsp;0 so that\n\n:<math>\\sum_{i = 1}^m c_i n_i = n.</math>\n\nChoose non-negative, integrable functions\n\n:<math>f_i \\in L^1 \\left( \\mathbb{R}^{n_i} ; [0, + \\infty] \\right)</math>\n\nand [[surjective]] [[linear map]]s\n\n:<math>B_i : \\mathbb{R}^n \\to \\mathbb{R}^{n_i}.</math>\n\nThen the following inequality holds:\n\n:<math>\\int_{\\mathbb{R}^n} \\prod_{i = 1}^m f_i \\left( B_i x \\right)^{c_i} \\, \\mathrm{d} x \\leq D^{- 1/2} \\prod_{i = 1}^m \\left( \\int_{\\mathbb{R}^{n_i}} f_i (y) \\, \\mathrm{d} y \\right)^{c_i},</math>\n\nwhere ''D'' is given by\n\n:<math>D = \\inf \\left\\{ \\left. \\frac{\\det \\left( \\sum_{i = 1}^m c_i B_i^{*} A_i B_i \\right)}{\\prod_{i = 1}^m ( \\det A_i )^{c_i}} \\right| A_i \\text{ is a positive-definite } n_i \\times n_i \\text{ matrix} \\right\\}.</math>\n\nAnother way to state this is that the constant ''D'' is what one would obtain by restricting attention to the case in which each <math>f_{i}</math> is a centered Gaussian function, namely <math>f_{i}(y) = \\exp \\{-(y,\\, A_{i}\\, y)\\}</math>.<ref>This inequality is in {{cite journal |first=E. H. |last=Lieb |title=Gaussian Kernels have only Gaussian Maximizers |journal=Inventiones Mathematicae |volume=102 |issue= |pages=179–208 |year=1990 |doi= 10.1007/bf01233426|bibcode=1990InMat.102..179L }}</ref>\n\n===Relationships to other inequalities===\n\n====The geometric Brascamp–Lieb inequality====\n\nThe geometric Brascamp–Lieb inequality is a special case of the above,<ref>This was derived first in {{cite journal |first=H. J. |last=Brascamp |first2=E. H. |last2=Lieb |title=Best Constants in Young's Inequality, Its Converse and Its Generalization to More Than Three Functions |journal=Adv. Math. |volume=20 |issue= 2|pages=151–172 |year=1976 |doi=10.1016/0001-8708(76)90184-5}}</ref> and was used by [[Keith Martin Ball | Keith Ball]], in 1989, to provide upper bounds for volumes of central sections of cubes.<ref>{{cite book |last=Ball |first=Keith M. |chapter=Volumes of Sections of Cubes and Related Problems |title=Geometric Aspects of Functional Analysis (1987–88) |editor-first=J. |editor-last=Lindenstrauss |editor-link=Joram Lindenstrauss |editor2-first=V. D. |editor2-last=Milman |series=Lecture Notes in Math. |volume=1376 |pages=251–260 |publisher=Springer |location=Berlin |year=1989 }}</ref>\n\nFor ''i''&nbsp;=&nbsp;1, ..., ''m'', let ''c''<sub>''i''</sub>&nbsp;&gt;&nbsp;0 and let ''u''<sub>''i''</sub>&nbsp;∈&nbsp;'''S'''<sup>''n''−1</sup> be a unit vector; suppose that ''c''<sub>''i''</sub> and ''u''<sub>''i''</sub> satisfy\n\n:<math>x = \\sum_{i = 1}^m c_i (x \\cdot u_i) u_i</math>\n\nfor all ''x'' in '''R'''<sup>''n''</sup>. Let ''f''<sub>''i''</sub>&nbsp;∈&nbsp;''L''<sup>1</sup>('''R''';&nbsp;[0,&nbsp;+∞]) for each ''i''&nbsp;=&nbsp;1, ..., ''m''. Then\n\n:<math>\\int_{\\mathbb{R}^n} \\prod_{i = 1}^m f_i (x \\cdot u_i)^{c_i} \\, \\mathrm{d} x \\leq \\prod_{i = 1}^m \\left( \\int_{\\mathbb{R}} f_i (y) \\, \\mathrm{d} y \\right)^{c_i}.</math>\n\nThe geometric Brascamp–Lieb inequality follows from the Brascamp–Lieb inequality as stated above by taking ''n''<sub>''i''</sub>&nbsp;=&nbsp;1 and ''B''<sub>''i''</sub>(''x'')&nbsp;=&nbsp;''x''&nbsp;·&nbsp;''u''<sub>''i''</sub>. Then, for ''z''<sub>''i''</sub>&nbsp;∈&nbsp;'''R''',\n\n:<math>B_i^{*} (z_i) = z_i u_i.</math>\n\nIt follows that ''D''&nbsp;=&nbsp;1 in this case.\n\n====Hölder's inequality====\n\nAs another special case, take ''n''<sub>''i''</sub>&nbsp;=&nbsp;''n'', ''B''<sub>''i''</sub>&nbsp;=&nbsp;id, the [[identity function|identity map]] on <math>\\mathbb{R}^{n}</math>, replacing ''f''<sub>''i''</sub> by ''f''{{su|b=''i''|p=1/''c''<sub>''i''</sub>}}, and let ''c''<sub>''i''</sub>&nbsp;=&nbsp;1&nbsp;/&nbsp;''p''<sub>''i''</sub> for 1&nbsp;≤&nbsp;''i''&nbsp;≤&nbsp;''m''. Then\n\n:<math>\\sum_{i = 1}^m \\frac{1}{p_i} = 1</math>\n\nand the [[Logarithmically concave function|log-concavity]] of the [[determinant]] of a [[positive definite matrix]] implies that ''D''&nbsp;=&nbsp;1. This yields Hölder's inequality in <math>\\mathbb{R}^{n}</math>:\n\n:<math>\\int_{\\mathbb{R}^n} \\prod_{i = 1}^m f_{i} (x) \\, \\mathrm{d} x \\leq \\prod_{i = 1}^{m} \\| f_i \\|_{p_i}.</math>\n\n==The concentration inequality==\n\nConsider a probability density function <math>p(x)=\\exp(-\\phi(x))</math>. <math> p(x) </math> is said to be a [[log-concave measure]] if the <math> \\phi(x) </math> function is convex. Such probability density functions have tails which decay exponentially fast, so most of the probability mass resides in a small region around the mode of <math> p(x) </math>. The Brascamp–Lieb inequality gives another characterization of the compactness of <math> p(x) </math> by bounding the mean of any statistic <math> S(x)</math>.\n\nFormally, let <math> S(x) </math> be any derivable function. The Brascamp–Lieb inequality reads:\n\n:<math> \\operatorname{var}_p (S(x)) \\leq E_p (\\nabla^T S(x) [H \\phi(x)]^{-1} \\nabla S(x)) </math>\n\nwhere H is the [[Hessian matrix|Hessian]] and <math>\\nabla</math> is the [[Nabla symbol]].<ref>This theorem was originally derived in {{cite journal |first=H. J. |last=Brascamp |first2=E. H. |last2=Lieb |title=On Extensions of the Brunn–Minkowski and Prékopa–Leindler theorems, including inequalities for log concave functions, and with an application to the diffusion equation |journal=[[Journal of Functional Analysis]] |volume=22 |issue=4 |pages=366–389 |year=1976 |doi=10.1016/0022-1236(76)90004-5}} Extensions of the inequality can be found in {{cite journal |first=Gilles |last=Hargé |title=Reinforcement of an Inequality due to Brascamp and Lieb |journal=Journal of Functional Analysis |volume=254 |issue= 2|pages=267–300 |year=2008 |postscript=none |doi=10.1016/j.jfa.2007.07.019}} and {{cite journal |first=Eric A. |last=Carlen |first2=Dario |last2=Cordero-Erausquin |first3=Elliott H. |last3=Lieb |title=Asymmetric Covariance Estimates of Brascamp-Lieb Type and Related Inequalities for Log-concave Measures |journal=Annales de l'Institut Henri Poincaré B |volume=49 |issue= 1|pages=1–12 |year=2013 |jstor= |doi=10.1214/11-aihp462|arxiv=1106.0709 |bibcode=2013AIHPB..49....1C }}</ref>\n\n===Relationship with other inequalities===\n\nThe Brascamp–Lieb inequality is an extension of the [[Poincaré inequality]] which only concerns Gaussian probability distributions.\n\nThe Brascamp–Lieb inequality is also related to the [[Cramér–Rao bound]]. While Brascamp–Lieb is an upper-bound, the Cramér–Rao bound lower-bounds the variance of <math>\\operatorname{var}_p (S(x))</math>. The expressions are almost identical:\n\n:<math> \\operatorname{var}_p (S(x)) \\geq E_p (\\nabla^T S(x) ) [ E_p( H \\phi(x) )]^{-1} E_p( \\nabla S(x) ) </math>\n\nFurther reference for both points can be found in \"Log-concavity and strong log-concavity: A review\", by A. Saumard and J. Wellner.\n\n==References==\n{{Reflist}}\n\n* {{cite journal\n| last=Gardner\n| first=Richard J.\n| title=The Brunn–Minkowski inequality\n| journal=Bull. Amer. Math. Soc. (N.S.)\n| volume=39\n| issue=3\n| year=2002\n| pages=355–405\n| url = http://www.ams.org/bull/2002-39-03/S0273-0979-02-00941-2/S0273-0979-02-00941-2.pdf\n| doi=10.1090/S0273-0979-02-00941-2\n}}\n\n{{DEFAULTSORT:Brascamp-Lieb Inequality}}\n[[Category:Geometric inequalities]]"
    },
    {
      "title": "Brunn–Minkowski theorem",
      "url": "https://en.wikipedia.org/wiki/Brunn%E2%80%93Minkowski_theorem",
      "text": "In [[mathematics]], the '''Brunn–Minkowski theorem''' (or '''Brunn–Minkowski inequality''') is an inequality relating the volumes (or more generally [[Lebesgue measure]]s) of [[compact space|compact]] [[subset]]s of [[Euclidean space]]. The original version of the Brunn–Minkowski theorem ([[Hermann Brunn]] 1887; [[Hermann Minkowski]] 1896) applied to convex sets; the generalization to compact nonconvex sets stated here is due to [[Lazar Lyusternik]] (1935).\n\n==Statement of the theorem==\nLet ''n'' ≥ 1 and let ''μ'' denote the [[Lebesgue measure]] on '''R'''<sup>''n''</sup>. Let ''A'' and ''B'' be two nonempty compact subsets of '''R'''<sup>''n''</sup>. Then the following [[inequality (mathematics)|inequality]] holds:\n\n:<math>[ \\mu (A + B) ]^{1/n} \\geq [\\mu (A)]^{1/n} + [\\mu (B)]^{1/n},</math>\n\nwhere ''A'' + ''B'' denotes the [[Minkowski sum]]:\n\n:<math>A + B := \\{\\, a + b \\in \\mathbb{R}^{n} \\mid a \\in A,\\ b \\in B \\,\\}.</math>\n\n==Remarks==\nThe proof of the Brunn–Minkowski theorem establishes that the function\n\n:<math>A \\mapsto [\\mu (A)]^{1/n}</math>\n\nis [[concave function|concave]] in the sense that, for every pair of nonempty compact subsets ''A'' and ''B'' of '''R'''<sup>''n''</sup> and every 0 ≤ ''t'' ≤ 1,\n\n:<math>\\left[ \\mu (t A + (1 - t) B ) \\right]^{1/n} \\geq t [ \\mu (A) ]^{1/n} + (1 - t) [ \\mu (B) ]^{1/n}.</math>\n\nFor [[convex set|convex]] sets ''A'' and ''B'' of positive measure, the inequality in the theorem is strict\nfor 0 < ''t'' < 1 unless ''A'' and ''B'' are positive [[Homothetic transformation|homothetic]], i.e. are equal up to [[translation (geometry)|translation]] and [[Scaling (geometry)|dilation]] by a positive factor.\n\n==See also==\n* [[Isoperimetric inequality]]\n* [[Milman's reverse Brunn–Minkowski inequality]]\n* [[Minkowski–Steiner formula]]\n* [[Prékopa–Leindler inequality]]\n* [[Vitale's random Brunn–Minkowski inequality]]\n* [[Mixed volume]]\n\n==References==\n* {{cite journal | author=Brunn, H. | author-link=Hermann Brunn | title=Über Ovale und Eiflächen | year = 1887 | version=Inaugural Dissertation, München}}\n*{{cite book\n | last=Fenchel\n | first=Werner\n | author-link = Werner Fenchel\n |author2=Bonnesen, Tommy\n | title=Theorie der konvexen Körper\n | series=Ergebnisse der Mathematik und ihrer Grenzgebiete\n | volume=3\n | publisher=1. Verlag von Julius Springer\n | location=Berlin\n | year=1934\n}}\n*{{cite book\n | last=Fenchel\n | first=Werner\n | author-link=Werner Fenchel\n |author2=Bonnesen, Tommy\n | title=Theory of convex bodies\n | publisher=L. Boron, C. Christenson and B. Smith. BCS Associates\n | location=Moscow, Idaho\n | year=1987\n}}\n* {{cite book | last=Dacorogna | first=Bernard | title=Introduction to the Calculus of Variations | publisher=Imperial College Press | location=London | year=2004 | isbn=1-86094-508-2}}\n* [[Heinrich Guggenheimer]] (1977) ''Applicable Geometry'', page 146, Krieger, Huntington {{ISBN|0-88275-368-1}} .\n* {{cite journal | last=Lyusternik | first=Lazar A. | authorlink=Lazar Lyusternik | title=Die Brunn–Minkowskische Ungleichnung für beliebige messbare Mengen | journal = Comptes Rendus de l'Académie des Sciences de l'URSS |series=Nouvelle Série | volume = III | year = 1935 | pages = 55&ndash;58}}\n* {{cite book | last=Minkowski | first=Hermann | authorlink=Hermann Minkowski | title = Geometrie der Zahlen | location = Leipzig | publisher = Teubner | year = 1896}}\n* {{cite news|last=Ruzsa|first=Imre&nbsp;Z.|authorlink=Imre Z. Ruzsa|title=The Brunn–Minkowski inequality and nonconvex sets|journal=Geometriae Dedicata|volume=67|doi=10.1023/A:1004958110076|year=1997|number=3|pages=337–348|mr=1475877}}\n* [[Rolf Schneider]], ''Convex bodies: the Brunn–Minkowski theory,'' Cambridge University Press, Cambridge, 1993.\n\n{{DEFAULTSORT:Brunn-Minkowski theorem}}\n[[Category:Theorems in measure theory]]\n[[Category:Theorems in convex geometry]]\n[[Category:Calculus of variations]]\n[[Category:Geometric inequalities]]\n[[Category:Sumsets]]\n[[Category:Hermann Minkowski]]"
    },
    {
      "title": "Busemann's theorem",
      "url": "https://en.wikipedia.org/wiki/Busemann%27s_theorem",
      "text": "In [[mathematics]], '''Busemann's theorem''' is a [[theorem]] in [[Euclidean geometry]] and [[geometric tomography]]. It was first proved by [[Herbert Busemann]] in 1949 and was motivated by his theory of area in [[Finsler space]]s.\n\n==Statement of the theorem==\n\nLet ''K'' be a [[convex body]] in ''n''-[[dimension]]al [[Euclidean space]] '''R'''<sup>''n''</sup> containing the [[Origin (mathematics)|origin]] in its [[interior (topology)|interior]]. Let ''S'' be an (''n''&nbsp;&minus;&nbsp;2)-dimensional [[linear subspace]] of '''R'''<sup>''n''</sup>. For each [[unit vector]] ''θ'' in ''S''<sup>⊥</sup>, the [[orthogonal complement]] of ''S'', let ''S''<sub>''θ''</sub> denote the (''n''&nbsp;&minus;&nbsp;1)-dimensional [[hyperplane]] containing ''θ'' and ''S''. Define ''r''(''θ'') to be the (''n''&nbsp;&minus;&nbsp;1)-dimensional volume of ''K''&nbsp;∩&nbsp;''S''<sub>''θ''</sub>.  Let ''C'' be the curve {''θr''(''θ'')} in ''S''<sup>⊥</sup>.  Then ''C'' forms the boundary of a convex body in ''S''<sup>⊥</sup>.\n\n==See also==\n* [[Brunn–Minkowski inequality]]\n* [[Prékopa–Leindler inequality]]\n\n==References==\n\n* {{cite journal\n|     last = Busemann\n|    first = Herbert\n|    title = A theorem on convex bodies of the Brunn-Minkowski type\n|  journal = Proc. Natl. Acad. Sci. U.S.A.\n|   volume = 35\n|    issue = 1\n|     year = 1949\n|    pages = 27&ndash;31\n| pmc=1062952\n|     doi = 10.1073/pnas.35.1.27\n| pmid=16588849\n}}\n* {{cite journal\n| last=Gardner \n| first=Richard J. \n| title=The Brunn-Minkowski inequality \n| journal=Bull. Amer. Math. Soc. (N.S.) \n| volume=39 \n| issue=3 \n| year=2002 \n| pages=355&ndash;405 (electronic) \n| doi=10.1090/S0273-0979-02-00941-2 \n}}\n\n[[Category:Euclidean geometry]]\n[[Category:Geometric inequalities]]\n[[Category:Theorems in convex geometry]]"
    },
    {
      "title": "Erdős–Mordell inequality",
      "url": "https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93Mordell_inequality",
      "text": "In [[Euclidean geometry]], the '''Erdős–Mordell inequality''' states that for any triangle ''ABC'' and point ''P'' inside ''ABC'', the sum of the distances from ''P'' to the sides is less than or equal to half of the sum of the distances from ''P'' to the vertices. It is named after [[Paul Erdős]] and [[Louis Mordell]]. {{harvtxt|Erdős|1935}} posed the problem of proving the inequality; a proof was provided two years later by {{harvs|last1=Mordell|first2=D. F.|last2=Barrow|year=1937|txt}}. This solution was however not very elementary. Subsequent simpler proofs were then found by {{harvtxt|Kazarinoff|1957}}, {{harvtxt|Bankoff|1958}}, and {{harvtxt|Alsina|Nelsen|2007}}.  \n\n[[Barrow's inequality]] is a strengthened version of the Erdős–Mordell inequality in which the distances from ''P'' to the sides are replaced by the distances from ''P'' to the points where the [[angle bisector]]s of  ∠''APB'', ∠''BPC'', and ∠''CPA'' cross the sides. Although the replaced distances are longer, their sum is still less than or equal to half the sum of the distances to the vertices.\n\n==Statement==\n[[File:Pedal Triangle.svg|right|thumb|]]\nFrom a point P inside a given triangle ABC the perpendiculars PL,PM,PN are drawn to its sides. \n::<math>PA+PB+PC\\geq 2(PL+PM+PN)</math> \n\n== Proof ==\n\nLet the sides of ABC be ''a'' opposite A, ''b'' opposite B, and ''c'' opposite C; also let PA = ''p'', PB = ''q'', PC = ''r'', dist(P;BC) = ''x'', dist(P;CA) = ''y'', dist(P;AB) = ''z''. First, we prove that \n\n:<math>cr\\geq ax+by.</math>\n\nThis is equivalent to \n\n:<math>\\frac{c(r+z)}2\\geq \\frac{ax+by+cz}2.</math>\n\nThe right side is the area of triangle ABC, but on the left side, ''r'' + ''z'' is at least the height of the triangle; consequently, the left side cannot be smaller than the right side. Now reflect P on the angle bisector at C. We find that ''cr'' ≥ ''ay'' + ''bx'' for P's reflection. Similarly, ''bq'' ≥ ''az'' + ''cx'' and ''ap'' ≥ ''bz'' + ''cy''. We solve these inequalities for ''r'', ''q'', and ''p'':\n\n:<math>r\\geq (a/c)y+(b/c)x,</math>\n\n:<math>q\\geq (a/b)z+(c/b)x,</math>\n\n:<math>p\\geq (b/a)z+(c/a)y.</math>\n\nAdding the three up, we get\n\n:<math>\n    p + q + r\n\\geq\n    \\left( \\frac{b}{c} + \\frac{c}{b} \\right) x +\n    \\left( \\frac{a}{c} + \\frac{c}{a} \\right) y +\n    \\left( \\frac{a}{b} + \\frac{b}{a} \\right) z.\n</math>\n\nSince the sum of a positive number and its reciprocal is at least 2 by [[Inequality of arithmetic and geometric means|AM–GM inequality]], we are finished. Equality holds only for the equilateral triangle, where P is its centroid.\n\n==Another strengthened version==\nLet ABC be a triangle inscribed into a circle (O) and P be a point inside of ABC. Let D, E, F be the orthogonal projections of P onto BC, CA, AB. M, N, Q be the orthogonal projections of P onto tangents to (O) at A, B, C respectively, then:\n:<math> PM+PN+PQ \\ge 2(PD+PE+PF)</math>\nEquality hold if and only if triangle ABC is equilateral ({{harvnb|Dao|Nguyen|Pham|2016}}; {{harvnb|Marinescu|Monea|2017}})\n\n==A generalization==\nLet <math>A_1A_2...A_n</math> be a convex polygon, and <math>P</math> be an interior point of <math>A_1A_2...A_n</math>. Let <math>R_i</math> be the distance from <math>P</math> to the vertex <math>A_i</math> , <math>r_i</math> the distance from <math>P</math> to the side <math>A_iA_{i+1}</math>, <math>w_i</math> the segment of the bisector of the angle <math>A_iPA_{i+1}</math> from <math>P</math> to its intersection with the side <math>A_iA_{i+1}</math> then {{harv|Lenhard|1961}}:\n:<math> \\sum_{i=1}^{n}R_i \\ge \\sec{\\frac{\\pi}{n}}\\left(\\sum_{i=1}^{n} w_i\\right)=\\sec{\\frac{\\pi}{n}}\\left(\\sum_{i=1}^{n} r_i\\right) </math>\n\n==See also==\n*[[List of triangle inequalities]]\n\n== References ==\n*{{citation\n | last1 = Alsina | first1 = Claudi\n | last2 = Nelsen | first2 = Roger B.\n | journal = Forum Geometricorum\n | pages = 99–102\n | title = A visual proof of the Erdős-Mordell inequality\n | url = http://forumgeom.fau.edu/FG2007volume7/FG200711index.html\n | volume = 7\n | year = 2007}}.\n*{{citation\n | last = Bankoff | first = Leon | author-link = Leon Bankoff\n | journal = [[American Mathematical Monthly]]\n | page = 521\n | title = An elementary proof of the Erdős-Mordell theorem\n | issue = 7\n | jstor = 2308580\n | volume = 65\n | year = 1958\n | doi=10.2307/2308580}}.\n*{{citation\n | last1 = Dao | first1 = Thanh Oai\n | last2 = Nguyen | first2 = Tien Dung\n | last3 = Pham | first3 = Ngoc Mai\n | journal = Forum Geometricorum\n | mr = 3556993\n | pages = 317–321\n | title = A strengthened version of the Erdős-Mordell inequality\n | volume = 16\n | year = 2016\n | url = http://forumgeom.fau.edu/FG2016volume16/FG201638.pdf}}.\n*{{citation\n | last = Erdős | first = Paul | author-link = Paul Erdős\n | journal = [[American Mathematical Monthly]]\n | page = 396\n | title = Problem 3740\n | volume = 42\n | year = 1935\n | doi=10.2307/2301373}}.\n*{{citation\n | last = Kazarinoff | first = D. K.\n | doi = 10.1307/mmj/1028988998\n | issue = 2\n | journal = [[Michigan Mathematical Journal]]\n | pages = 97–98\n | title = A simple proof of the Erdős-Mordell inequality for triangles\n | volume = 4\n | year = 1957}}.\n*{{citation\n | last = Lenhard | first = Hans-Christof\n | doi = 10.1007/BF01650566\n | journal = Archiv für Mathematische Logik und Grundlagenforschung\n | mr = 0133060\n | pages = 311–314\n | title = Verallgemeinerung und Verschärfung der Erdös-Mordellschen Ungleichung für Polygone\n | volume = 12\n | year = 1961}}.\n*{{citation\n | last1 = Marinescu | first1 = Dan Ștefan\n | last2 = Monea | first2 = Mihai\n | title = About a strengthened version of the Erdős-Mordell inequality\n | journal = Forum Geometricorum\n | volume = 17\n | year = 2017\n | pages = 197–202\n | url = http://forumgeom.fau.edu/FG2017volume17/FG201723.pdf}}.\n*{{citation\n | last1 = Mordell | first1 = L. J. | author1-link = Louis Mordell\n | last2 = Barrow | first2 = D. F.\n | journal = [[American Mathematical Monthly]]\n | pages = 252–254\n | title = Solution to 3740\n | volume = 44\n | year = 1937 | doi=10.2307/2300713}}. \n\n== External links ==\n*{{mathworld|urlname=Erdos-MordellTheorem|title=Erdős-Mordell Theorem}}\n*[[Alexander Bogomolny]], \"[http://www.cut-the-knot.org/triangle/ErdosMordell.shtml Erdös-Mordell Inequality]\", from [[Cut-the-Knot]].\n\n{{DEFAULTSORT:Erdos-Mordell inequality}}\n[[Category:Triangle geometry]]\n[[Category:Geometric inequalities]]"
    },
    {
      "title": "Euler's theorem in geometry",
      "url": "https://en.wikipedia.org/wiki/Euler%27s_theorem_in_geometry",
      "text": "[[File:Euler theorem2.svg|thumb|right|upright=1.0|Euler's theorem:<br/><math>d=|IO| =\\sqrt{R (R-2r)}</math>]]\nIn [[geometry]], '''Euler's theorem''' states that the distance ''d'' between the [[circumcentre]] and [[incentre]] of a [[triangle]] is given by<ref name=Johnson>{{citation|last=Johnson|first=Roger A.|title=Advanced Euclidean Geometry|publisher=Dover Publ.|year=2007|origyear=1929|page=186}}.</ref><ref name=\"wlim\">{{citation\n | last1 = Alsina | first1 = Claudi\n | last2 = Nelsen | first2 = Roger\n | isbn = 9780883853429\n | page = 56\n | publisher = Mathematical Association of America\n | series = Dolciani Mathematical Expositions\n | title = When Less is More: Visualizing Basic Inequalities\n | url = https://books.google.com/books?id=U1ovBsSRNscC&pg=PA56\n | volume = 36\n | year = 2009}}.</ref><ref name=\"lle\">{{citation\n | last = Debnath | first = Lokenath\n | isbn = 9781848165250\n | page = 124\n | publisher = World Scientific\n | title = The Legacy of Leonhard Euler: A Tricentennial Tribute\n | url = https://books.google.com/books?id=K2liU-SHl6EC&pg=PA124\n | year = 2010}}.</ref><ref>{{citation\n | last = Dunham | first = William\n | isbn = 9780883855584\n | page = 300\n | publisher = Mathematical Association of America\n | series = Spectrum Series\n | title = The Genius of Euler: Reflections on his Life and Work\n | url = https://books.google.com/books?id=M4-zUnrSxNoC&pg=PA300\n | volume = 2\n | year = 2007}}.</ref>\n\n:<math> d^2=R (R-2r) </math>\n\nor equivalently\n:<math>\\frac{1}{R-d} + \\frac{1}{R+d} = \\frac{1}{r},</math>\n\nwhere ''R'' and ''r'' denote the circumradius and inradius respectively (the radii of the [[circumscribed circle]] and [[inscribed circle]] respectively). The theorem is named for  [[Leonhard Euler]], who published it in 1767.<ref>{{citation\n | last = Euler | first = Leonhard | author-link = Leonhard Euler\n | journal = Novi Commentarii academiae scientiarum Petropolitanae\n | language = Latin\n | pages = 103–123\n | title = Solutio facilis problematum quorumdam geometricorum difficillimorum\n | url = http://www.math.dartmouth.edu/~euler/docs/originals/E325.pdf\n | volume = 11\n | year = 1767}}.</ref> However, the same result was published earlier by William Chapple in 1746.<ref>{{citation\n | last = Chapple | first = William\n | journal = Miscellanea Curiosa Mathematica\n | pages = 117–124\n | title = An essay on the properties of triangles inscribed in and circumscribed about two given circles\n | url = https://books.google.com/books?id=a95JAAAAMAAJ&pg=PA118-IA1\n | volume = 4\n | year = 1746}}. The formula for the distance is near the bottom of p.123.</ref>\n\nFrom the theorem follows the '''Euler inequality''':<ref name=\"wlim\"/><ref name=\"lle\"/>\n:<math>R \\ge 2r,</math>\n\nwhich holds with equality only in the [[equilateral triangle|equilateral]] case.<ref name=SV/>{{rp|p. 198}}\n\n==Proof==\n[[Image:GeometryEulerTheorem.png|300px|thumb|Proof of Euler's theorem in geometry]]\nLetting ''O'' be the circumcentre of triangle ''ABC'', and ''I'' be its incentre, the extension of ''AI'' intersects the circumcircle at ''L''. Then ''L'' is the midpoint of arc ''BC''. Join ''LO'' and extend it so that it intersects the circumcircle at ''M''. From ''I'' construct a perpendicular to AB, and let D be its foot, so ''ID'' = ''r''. It is not difficult to prove that triangle ''ADI'' is similar to triangle ''MBL'', so ''ID'' / ''BL'' = ''AI'' / ''ML'', i.e. ''ID'' × ''ML'' = ''AI'' × ''BL''. Therefore 2''Rr'' = ''AI'' × ''BL''. Join ''BI''. Because\n\n: ∠ ''BIL'' = ∠  ''A'' / 2 + ∠  ''ABC'' / 2,\n\n:  ∠ ''IBL'' =  ∠ ''ABC'' / 2 +  ∠ ''CBL'' =  ∠ ''ABC'' / 2 +  ∠ ''A'' / 2,\n\nwe have  ∠ ''BIL'' =  ∠ ''IBL'', so ''BL'' = ''IL'', and ''AI'' × ''IL'' = 2''Rr''. Extend ''OI'' so that it intersects the circumcircle at ''P'' and ''Q''; then ''PI'' × ''QI'' = ''AI'' × ''IL'' = 2''Rr'', so (''R''&nbsp;+&nbsp;''d'')(''R''&nbsp;&minus;&nbsp;''d'') = 2''Rr'', i.e. ''d''<sup>2</sup> = ''R''(''R''&nbsp;&minus;&nbsp;2''r'').\n\n==Stronger version of the inequality==\n\nA stronger version<ref name=SV>{{citation|first1=Dragutin|last1=Svrtan|first2=Darko|last2=Veljan|title=Non-Euclidean versions of some classical triangle inequalities|journal=Forum Geometricorum|volume=12|year=2012|pages=197–209|url=http://forumgeom.fau.edu/FG2012volume12/FG201217index.html}}.</ref>{{rp|p. 198}} is\n\n:<math>\\frac{R}{r} \\geq \\frac{abc+a^3+b^3+c^3}{2abc} \\geq \\frac{a}{b}+\\frac{b}{c}+\\frac{c}{a}-1 \\geq \\frac{2}{3} \\left(\\frac{a}{b}+\\frac{b}{c}+\\frac{c}{a} \\right) \\geq 2,</math>\n\nwhere ''a, b, c'' are the sidelengths of the triangle.\n\n==Euler's theorem for the exscribed circle==\n\nIf <math>r_a</math> and <math>d_a</math> denote respectively the radius of the exscribed circle opposite to the vertex <math>A</math> and the distance between its centre and the centre of \nthe circumscribed circle, then <math>d_a^2=R(R+2r_a)</math>.\n\n==Euler's inequality in absolute geometry==\n\nEuler's inequality, in the form stating that, for all triangles inscribed in a given circle, the maximum of the radius of the inscribed circle is reached for the equilateral triangle and only for it, is valid in [[absolute geometry]]. <ref name=PS>{{citation|first1=Victor|last1=Pambuccian|first2=Celia|last2=Schacht|title=Euler's inequality in absolute geoemtry|journal=Journal of Geometry|volume=109 (Art. 8)|year=2018|pages=1–11|doi=10.1007/s00022-018-0414-6}}.</ref>\n\n==See also==\n*[[Bicentric quadrilateral#Fuss' theorem|Fuss' theorem]] for the relation among the same three variables in bicentric quadrilaterals\n*[[Poncelet's closure theorem]], showing that there is an infinity of triangles with the same ''R'', ''r'', and ''d''\n*[[List of triangle inequalities]]\n\n==References==\n{{reflist}}\n\n==External links==\n{{Commons category|Euler&#39;s theorem in geometry}}\n*{{mathworld|id=EulerTriangleFormula.html|title=Euler Triangle Formula}}\n\n[[Category:Triangle geometry]]\n[[Category:Articles containing proofs]]\n[[Category:Geometric inequalities]]"
    },
    {
      "title": "Gromov's inequality for complex projective space",
      "url": "https://en.wikipedia.org/wiki/Gromov%27s_inequality_for_complex_projective_space",
      "text": "In [[Riemannian geometry]], [[Mikhail Gromov (mathematician)|Gromov]]'s optimal stable 2-[[systolic geometry|systolic]] inequality is the inequality\n\n: <math>\\mathrm{stsys}_2{}^n \\leq n!\n\\;\\mathrm{vol}_{2n}(\\mathbb{CP}^n)</math>,\n\nvalid for an arbitrary Riemannian metric on the [[complex projective space]], where the optimal bound is attained\nby the symmetric [[Fubini–Study metric]], providing a natural geometrisation of [[quantum mechanics]].  Here <math>\\operatorname{stsys_2}</math> is the stable 2-systole, which in this case can be defined as the infimum of the areas of rational 2-cycles representing the class of the complex projective line <math>\\mathbb{CP}^1 \\subset \\mathbb{CP}^n</math> in 2-dimensional homology.\n\nThe inequality first appeared in {{harvtxt|Gromov|1981}} as Theorem 4.36.\n\nThe proof of Gromov's inequality relies on the [[Wirtinger inequality (2-forms)|Wirtinger inequality for exterior 2-forms]].\n\n==Projective planes over division algebras <math> \\mathbb{R,C,H}</math>==\n\nIn the special case n=2, Gromov's inequality becomes <math>\\mathrm{stsys}_2{}^2 \\leq 2 \\mathrm{vol}_4(\\mathbb{CP}^2)</math>.  This inequality can be thought of as an analog of [[Pu's inequality|Pu's inequality for the real projective plane]] <math>\\mathbb{RP}^2</math>.  In both cases, the boundary case of equality is attained by the symmetric metric of the projective plane.  Meanwhile, in the quaternionic case, the symmetric metric on <math>\\mathbb{HP}^2</math> is not its systolically optimal metric.  In other words, the manifold <math>\\mathbb{HP}^2</math> admits Riemannian metrics with higher systolic ratio <math>\\mathrm{stsys}_4{}^2/\\mathrm{vol}_8</math> than for its symmetric metric {{harv|Bangert|Katz|Shnider|Weinberger|2009}}.\n\n==See also==\n*[[Loewner's torus inequality]]\n*[[Pu's inequality]]\n*[[Gromov's inequality (disambiguation)]]\n*[[Gromov's systolic inequality for essential manifolds]]\n*[[Systolic geometry]]\n\n==References==\n*{{cite journal | first1=Victor | last1=Bangert | first2=Mikhail G. | last2=Katz | first3=Steve | last3=Shnider | first4=Shmuel | last4=Weinberger | title='''E<sub>7</sub>''', Wirtinger inequalities, Cayley 4-form, and homotopy| journal=[[Duke Mathematical Journal]] | volume=146 | year=2009 | issue=1 | pages=35–70 | arxiv=math.DG/0608006 | mr=2475399 | doi=10.1215/00127094-2008-061 | ref=harv}}\n*{{cite book | last=Gromov | first=Mikhail | title=Structures métriques pour les variétés riemanniennes | trans-title=Metric structures for Riemann manifolds | language=fr | editor1=J. Lafontaine | editor2=P. Pansu. | series=Textes Mathématiques | volume=1 | publisher=CEDIC | location=Paris | year=1981 | mr=0682063 | isbn=2-7124-0714-8 | ref=harv}}\n*{{cite book | last1=Katz | first1=Mikhail G. | title=Systolic geometry and topology|pages=19 | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Mathematical Surveys and Monographs | isbn=978-0-8218-4177-8 | year=2007 | volume=137 | others=With an appendix by Jake P. Solomon. | mr=2292367 | doi=10.1090/surv/137}}\n\n{{Systolic geometry navbox}}\n\n[[Category:Geometric inequalities]]\n[[Category:Differential geometry]]\n[[Category:Riemannian geometry]]\n[[Category:Systolic geometry]]"
    },
    {
      "title": "Gromov's systolic inequality for essential manifolds",
      "url": "https://en.wikipedia.org/wiki/Gromov%27s_systolic_inequality_for_essential_manifolds",
      "text": "In the [[mathematics|mathematical]] field of [[Riemannian geometry]], [[Mikhail Gromov (mathematician)|M. Gromov]]'s '''systolic inequality''' bounds the length of the shortest [[contractible space|non-contractible]] loop on a [[Riemannian manifold]] in terms of the volume of the manifold. Gromov's systolic inequality was proved in 1983;<ref>see {{harvtxt|Gromov|1983}}</ref> it can be viewed as a generalisation, albeit non-optimal, of [[Loewner's torus inequality]] and [[Pu's inequality|Pu's inequality for the real projective plane]].\n\nTechnically, let ''M'' be an [[essential manifold|essential]] Riemannian manifold of dimension ''n''; denote by sys''&pi;''<sub>1</sub>(''M'') the homotopy 1-systole of ''M'', that is, the least length of a non-contractible loop on ''M''. Then Gromov's inequality takes the form\n\n:<math> \\left(\\operatorname{sys\\pi}_1(M)\\right)^n \\leq C_n \\operatorname{vol}(M),</math>\n\nwhere ''C''<sub>''n''</sub> is a universal constant only depending on the dimension of ''M''.\n\n==Essential manifolds==\n{{main|essential manifold}}\n\nA closed manifold is called ''essential'' if its [[fundamental class]] defines a nonzero element in the [[homology (mathematics)|homology]] of its [[fundamental group]], or more precisely in the homology of the corresponding [[Eilenberg–MacLane space]].  Here the fundamental class is taken in homology with integer coefficients if the manifold is orientable, and in coefficients modulo 2, otherwise.\n\nExamples of essential manifolds include [[aspherical manifold]]s, [[real projective space]]s, and [[lens space]]s.\n\n==Proofs of Gromov's inequality==\n\nGromov's original 1983 proof is about 35 pages long.  It relies on a number of techniques and inequalities of global Riemannian geometry.  The starting point of the proof is the imbedding of X into the Banach space of Borel functions on X, equipped with the sup norm.  The imbedding is defined by mapping a point ''p'' of ''X'', to the real function on ''X'' given by the distance from the point ''p''.  The proof utilizes the [[coarea formula|coarea inequality]], the [[isoperimetry|isoperimetric inequality]], the cone inequality, and the deformation theorem of [[Herbert Federer]].\n\n==Filling invariants and recent work==\n\nOne of the key ideas of the proof is the introduction of filling invariants, namely the [[filling radius]] and the filling volume of ''X''. Namely, Gromov proved a sharp inequality relating the systole and the filling radius,\n\n:<math>\\mathrm{sys\\pi}_1 \\leq 6\\; \\mathrm{FillRad}(X),</math>\n\nvalid for all essential manifolds ''X''; as well as an inequality\n\n:<math>\\mathrm{FillRad}(X) \\leq C_n \\mathrm{vol}_n{}^{\\tfrac{1}{n}}(X),</math>\n\nvalid for all closed manifolds ''X''.\n\nIt was shown by {{harvtxt|Brunnbauer|2008}} that the filling invariants, unlike the systolic invariants, are independent of the topology of the manifold in a suitable sense.\n\n{{harvtxt|Guth|2011}} and {{harvtxt|Ambrosio|Katz|2011}} developed approaches to the proof of Gromov's systolic inequality for essential manifolds.\n\n==Inequalities for surfaces and polyhedra==\n\nStronger results are available for surfaces, where the asymptotics when the genus tends to infinity are by now well understood, see [[systoles of surfaces]].  A uniform inequality for arbitrary 2-complexes with non-free fundamental groups is available, whose proof relies on the [[Grushko theorem|Grushko decomposition theorem]].\n\n==Notes==\n{{Reflist}}\n\n==See also==\n*[[Filling area conjecture]]\n*[[Gromov's inequality (disambiguation)]]\n*[[Gromov's inequality for complex projective space]]\n*[[Loewner's torus inequality]]\n*[[Pu's inequality]]\n*[[Systolic geometry]]\n\n==References==\n*{{citation\n | mr = 2803853\n | last1 = Ambrosio | first1 = Luigi\n | author1-link = Luigi Ambrosio\n | last2 = Katz | first2 = Mikhail\n | author2-link = Mikhail Katz\n | arxiv = 1004.1374\n | doi = 10.4171/CMH/234\n | issue = 3\n | journal = [[Commentarii Mathematici Helvetici]]\n | pages = 557–592\n | title = Flat currents modulo p  in metric spaces and filling radius inequalities\n | volume = 86\n | year = 2011}}.\n*{{Citation|last=Brunnbauer|first=M.|title=Filling inequalities do not depend on topology|volume=624|year=2008|pages=217&ndash;231|journal=J. Reine Angew. Math.}}\n*{{Citation|last=Gromov|first=M.|title=Filling Riemannian manifolds|journal=J. Diff. Geom.|volume=18|year=1983|pages=1&ndash;147|mr=0697984|id={{Euclid|euclid.jdg/1214509283}}|zbl=0515.53037}}\n*{{Citation|mr=2753599|last=Guth|first=Larry\n |author1-link= Larry Guth\n |title=Volumes of balls in large Riemannian manifolds|journal=[[Annals of Mathematics]]|volume=173|year=2011|issue=1|pages=51&ndash;76|doi=10.4007/annals.2011.173.1.2|arxiv=math/0610212}}\n*{{Citation | last1=Katz | first1=Mikhail G. | title=Systolic geometry and topology|pages=19 | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Mathematical Surveys and Monographs | isbn=978-0-8218-4177-8 | year=2007 | volume=137}}\n\n{{Systolic geometry navbox}}\n\n[[Category:Geometric inequalities]]\n[[Category:Riemannian geometry]]\n[[Category:Systolic geometry]]"
    },
    {
      "title": "Hadwiger–Finsler inequality",
      "url": "https://en.wikipedia.org/wiki/Hadwiger%E2%80%93Finsler_inequality",
      "text": "In [[mathematics]], the '''Hadwiger–Finsler inequality''' is a result on the [[geometry]] of [[triangle]]s in the [[Euclidean plane]]. It states that if a triangle in the plane has side lengths ''a'', ''b'' and ''c'' and area ''T'', then\n\n:<math>a^{2} + b^{2} + c^{2} \\geq (a - b)^{2} + (b - c)^{2} + (c - a)^{2} + 4 \\sqrt{3} T \\quad \\mbox{(HF)}.</math>\n\n==Related inequalities==\n* [[Weitzenböck's inequality]] is a straightforward [[corollary]] of the Hadwiger–Finsler inequality: if a triangle in the plane has side lengths ''a'', ''b'' and ''c'' and area ''T'', then\n\n:<math>a^{2} + b^{2} + c^{2} \\geq 4 \\sqrt{3} T \\quad \\mbox{(W)}.</math>\n\nWeitzenböck's inequality can also be proved using [[Heron's formula]], by which route it can be seen that equality holds in (W) [[if and only if]] the triangle is an [[equilateral triangle]], i.e. ''a''&nbsp;=&nbsp;''b''&nbsp;=&nbsp;''c''.\n\n* A version for [[quadrilateral]]: Let ''ABCD'' be a convex quadrilateral with the lengths ''a'', ''b'', ''c'', ''d'' and the area ''T'' then:<ref>Leonard Mihai Giugiuc, Dao Thanh Oai and Kadir Altintas, ''An inequality related to the lengths and area of a convex quadrilateral'', International Journal of Geometry, Vol. 7 (2018), No. 1, pp. 81 - 86, [https://ijgeometry.com/wp-content/uploads/2018/04/81-86.pdf]</ref>\n\n:<math> a^2+b^2+c^2+d^2 \\ge 4T + \\frac{\\sqrt{3}-1}{\\sqrt{3}}\\sum{(a-b)^2} </math> with equality only for a [[square]]. \nWhere <math> \\sum{(a-b)^2}=(a-b)^2+(a-c)^2+(a-d)^2+(b-c)^2+(b-d)^2+(c-d)^2 </math>\n\n==History==\nThe Hadwiger–Finsler inequality is named after {{harvs|first1=Paul|last1=Finsler|author1-link=Paul Finsler|first2=Hugo|last2=Hadwiger|author2-link=Hugo Hadwiger|year=1937|txt}}, who also published in the same paper the [[Finsler–Hadwiger theorem]] on a square derived from two other squares that share a vertex.\n\n==See also==\n*[[List of triangle inequalities]]\n*[[Isoperimetric inequality]]\n\n==References==\n{{Reflist}}\n*{{cite journal|last1 = Finsler|first1 = Paul | author1-link = Paul Finsler | last2=Hadwiger | first2 = Hugo | author2-link = Hugo Hadwiger | title = Einige Relationen im Dreieck|journal = [[Commentarii Mathematici Helvetici]]|volume = 10|issue = 1|year = 1937|pages = 316–326|doi = 10.1007/BF01214300 | ref=harv}}\n*Claudi Alsina, Roger B. Nelsen: ''When Less is More: Visualizing Basic Inequalities''. MAA, 2009, {{ISBN|9780883853429}}, pp. [https://books.google.de/books?id=U1ovBsSRNscC&pg=PA84 84-86]\n\n==External links==\n* {{PlanetMath|urlname=proofofhadwigerfinslerinequality|title=Proof of Hadwiger-Finsler inequality}}\n* {{PlanetMath|urlname=WeizenbocksInequality|title=Weizenbock's inequality}}\n\n{{DEFAULTSORT:Hadwiger-Finsler inequality}}\n[[Category:Euclidean geometry]]\n[[Category:Geometric inequalities]]\n[[Category:Triangle geometry]]"
    },
    {
      "title": "Hitchin–Thorpe inequality",
      "url": "https://en.wikipedia.org/wiki/Hitchin%E2%80%93Thorpe_inequality",
      "text": "In [[differential geometry]] the '''Hitchin&ndash;Thorpe inequality''' is a relation which restricts the topology of [[4-manifold]]s that carry an [[Einstein manifold|Einstein metric]].\n\n== Statement of the Hitchin&ndash;Thorpe inequality ==\nLet ''M'' be a compact, oriented, smooth four-dimensional manifold.  If there exists a [[Riemannian metric]] on ''M'' which is an [[Einstein metric]], then following inequality holds\n\n: <math>\\chi(M) \\geq \\frac{3}{2}|\\tau(M)|,</math>\n\nwhere <math>\\chi(M)</math> is the [[Euler characteristic]] of <math>M</math> and <math>\\tau(M)</math> is the [[signature (topology)|signature]] of <math>M</math>.  This inequality was first stated by John Thorpe<ref>J. Thorpe, ''Some remarks on the Gauss-Bonnet formula'', J. Math. Mech. 18 (1969) pp. 779--786.</ref> in a footnote to a 1969 paper focusing\non manifolds of higher dimension. [[Nigel Hitchin]] then rediscovered the inequality, and gave a complete characterization <ref>N. Hitchin, ''On compact four-dimensional Einstein manifolds'', J. Diff. Geom. 9 (1974) pp. 435--442.</ref> of the equality  case  in 1974; he found that if <math>(M,g)</math> is an Einstein manifold with <math>\\chi(M) = \\frac{3}{2}|\\tau(M)|,</math> then <math>(M,g)</math> must be a flat torus, a [[Calabi&ndash;Yau manifold]], or a quotient thereof.\n\n== Idea of the proof ==\nThe main ingredients in the proof of the Hitchin&ndash;Thorpe inequality are the [[Ricci decomposition|decomposition]] of the [[Riemann curvature tensor]] and the [[Generalized Gauss-Bonnet theorem]].\n\n== Failure of the converse ==\nA natural question to ask is whether the Hitchin&ndash;Thorpe inequality provides a [[sufficient condition]] for the existence of Einstein metrics.  In 1995, [[Claude LeBrun]] and \nAndrea Sambusetti  independently showed that the answer is no:  there exist infinitely many non-homeomorphic compact, smooth, oriented 4-manifolds ''M'' that carry no Einstein metrics but nevertheless satisfy\n\n: <math>\\chi(M) > \\frac{3}{2}|\\tau(M)|.</math>\n\nLeBrun's examples <ref>[[Claude LeBrun|C. LeBrun]], ''Four-manifolds without Einstein Metrics'', Math. Res. Letters 3 (1996) pp. 133--147.</ref> are actually simply connected, and the relevant obstruction depends on the smooth structure of the manifold. By contrast,  Sambusetti's obstruction <ref>A. Sambusetti, ''An obstruction to the existence of Einstein metrics on 4-manifolds'', C. R. Acad. Sci. Paris 322 (1996) pp. 1213--1218.</ref> only applies to 4-manifolds with infinite fundamental group, but the volume-entropy estimate he uses to prove non-existence  only depends on the homotopy type of the manifold.\n\n==Footnotes==\n<references/>\n\n== References ==\n*{{cite book | first = Arthur L. | last = Besse | title = Einstein Manifolds | series = Classics in Mathematics | publisher = Springer | location = Berlin | year = 1987 | isbn = 3-540-74120-8}}\n\n{{DEFAULTSORT:Hitchin-Thorpe inequality}}\n[[Category:Riemannian manifolds|Einstein manifolds]]\n[[Category:Geometric inequalities]]\n[[Category:4-manifolds|Einstein manifold]]"
    },
    {
      "title": "Isoperimetric inequality",
      "url": "https://en.wikipedia.org/wiki/Isoperimetric_inequality",
      "text": "{{Use dmy dates|date=July 2013}}\nIn mathematics, the '''isoperimetric inequality''' is a [[geometry|geometric]] [[inequality (mathematics)|inequality]] involving the surface area of a set and its volume. In <math>n</math>-dimensional space <math>\\R^n</math> the inequality lower bounds the [[surface area]] <math>\\mathrm{surf}(S)</math> of a set <math>S\\subset\\R^n</math> by its [[volume]] <math>\\mathrm{vol}(S)</math>,\n\n:<math>\\mathrm{surf}(S)\\geq n \\mathrm{vol}(S)^{\\frac{n-1}{n}}\\mathrm{vol}(B_1)^{\\frac{1}{n}}</math>,\n\nwhere <math>B_1\\subset\\R^n</math> is a [[Unit sphere|unit ball]]. The equality holds when <math>S</math> is a ball in <math>\\R^n</math>.\n\nOn a plane, i.e. when <math>n=2</math>, the isoperimetric inequality relates square of the [[circumference]] of a [[closed curve]] and the [[area]] of a plane region it encloses. ''[[wikt:isoperimetric#English|Isoperimetric]]'' literally means \"having the same [[perimeter]]\". Specifically in <math>\\R ^2</math>, the isoperimetric inequality states, for the length ''L'' of a closed curve and the area ''A'' of the planar region that it encloses, that\n\n:<math>4\\pi A \\le L^2,</math>\n\nand that equality holds if and only if the curve is a circle.\n\nThe '''isoperimetric problem''' is to determine a [[plane figure]] of the largest possible area whose [[boundary (topology)|boundary]] has a specified length.<ref>{{cite journal|author=Blåsjö, Viktor|title=The Evolution of the Isoperimetric Problem|journal=Amer. Math. Monthly|volume=112|year=2005|pages=526–566|doi=10.2307/30037526|url=http://www.maa.org/programs/maa-awards/writing-awards/the-evolution-of-the-isoperimetric-problem}}</ref> The closely related ''Dido's problem'' asks for a region of the maximal area bounded by a straight line and a curvilinear [[arc (geometry)|arc]] whose endpoints belong to that line. It is named after [[Dido (Queen of Carthage)|Dido]], the legendary founder and first queen of [[Carthage]]. The solution to the isoperimetric problem is given by a [[circle]] and was known already in [[Ancient Greece]]. However, the first mathematically rigorous proof of this fact was obtained only in the 19th century. Since then, many other proofs have been found.\n\nThe isoperimetric problem has been extended in multiple ways, for example, to curves on [[differential geometry of surfaces|surfaces]] and to regions in higher-dimensional spaces. Perhaps the most familiar physical manifestation of the 3-dimensional isoperimetric inequality is the shape of a drop of water. Namely, a drop will typically assume a symmetric round shape. Since the amount of water in a drop is fixed, [[surface tension]] forces the drop into a shape which minimizes the surface area of the drop, namely a round sphere.\n\n== The isoperimetric problem in the plane ==\n[[File:Isoperimetric inequality illustr1.svg|right|thumb|If a region is not convex, a \"dent\" in its boundary can be \"flipped\" to increase the area of the region while keeping the perimeter unchanged.]]\n[[File:Isoperimetric inequality illustr2.svg|right|thumb|An elongated shape can be made more round while keeping its perimeter fixed and increasing its area.]]\n\nThe classical ''isoperimetric problem'' dates back to antiquity. The problem can be stated as follows: Among all closed [[curve]]s in the plane of fixed perimeter, which curve (if any) maximizes the area of its enclosed region? This question can be shown to be equivalent to the following problem: Among all closed curves in the plane enclosing a fixed area, which curve (if any) minimizes the perimeter?\n\nThis problem is conceptually related to the [[principle of least action]] in [[physics]], in that it can be restated: what is the principle of action which encloses the greatest area, with the greatest economy of effort? The 15th-century philosopher and scientist, Cardinal [[Nicholas of Cusa]], considered [[rotation]]al action, the process by which a [[circle]] is generated, to be the most direct reflection, in the realm of sensory impressions, of the process by which the universe is created. German astronomer and astrologer [[Johannes Kepler]] invoked the isoperimetric principle in discussing the morphology of the solar system, in ''[[Mysterium Cosmographicum]]'' (''The Sacred Mystery of the Cosmos'', 1596).\n\nAlthough the circle appears to be an obvious solution to the problem, proving this fact is rather difficult. The first progress toward the solution was made by Swiss geometer [[Jakob Steiner]] in 1838, using a geometric method later named [[Symmetrization methods#Steiner Symmetrization|''Steiner symmetrisation'']].<ref>J. Steiner, ''Einfacher Beweis der isoperimetrischen Hauptsätze'', J. reine angew Math. '''18''', (1838), pp. 281&ndash;296; and Gesammelte Werke Vol. 2, pp. 77&ndash;91, Reimer, Berlin, (1882).</ref> Steiner showed that if a solution existed, then it must be the circle. Steiner's proof was completed later by several other mathematicians.\n\nSteiner begins with some geometric constructions which are easily understood; for example, it can be shown that any closed curve enclosing a region that is not fully [[Convex set|convex]] can be modified to enclose more area, by \"flipping\" the concave areas so that they become convex. It can further be shown that any closed curve which is not fully symmetrical can be \"tilted\" so that it encloses more area. The one shape that is perfectly convex and symmetrical is the circle, although this, in itself, does not represent a rigorous proof of the isoperimetric theorem (see external links).\n\n== On a plane ==\nThe solution to the isoperimetric problem is usually expressed in the form of an [[inequality (mathematics)|inequality]] that relates the length ''L'' of a closed curve and the area ''A'' of the planar region that it encloses. The '''isoperimetric inequality''' states that\n\n:<math>4\\pi A \\le L^2,</math>\n\nand that the equality holds if and only if the curve is a circle. The [[area of a disk]] of radius ''R'' is ''πR''<sup>2</sup> and the circumference of the circle is 2''πR'', so both sides of the inequality are equal to 4''π''<sup>2</sup>''R''<sup>2</sup> in this case.\n\nDozens of proofs of the isoperimetric inequality have been found. In 1902, [[Adolf Hurwitz|Hurwitz]] published a short proof using the [[Fourier series]] that applies to arbitrary [[rectifiable curve]]s (not assumed to be smooth). An elegant direct proof based on comparison of a smooth simple closed curve with an appropriate circle was given by E. Schmidt in 1938. It uses only the [[arc length]] formula, expression for the area of a plane region from [[Green's theorem]], and the [[Cauchy–Schwarz inequality]].\n\nFor a given closed curve, the '''isoperimetric quotient''' is defined as the ratio of its area and that of the circle having the same perimeter. This is equal to\n\n:<math>Q=\\frac{4\\pi A}{L^2}</math>\n\nand the isoperimetric inequality says that ''Q'' ≤ 1. Equivalently, the [[isoperimetric ratio]] {{math|''L''<sup>2</sup>/''A''}} is at least 4{{pi}} for every curve.\n\nThe isoperimetric quotient of a regular ''n''-gon is\n\n:<math>Q_n=\\frac{\\pi}{n \\tan \\tfrac{\\pi}{n}}.</math>\n\nLet <math>C</math> be a smooth regular convex closed curve. Then the '''improved isoperimetric inequality''' states the following\n\n:<math>L^2\\geqslant 4\\pi A+8\\pi\\left|\\widetilde{A}_{0.5}\\right|,</math>\n\nwhere <math>L, A, \\widetilde{A}_{0.5}</math> denote the length of <math>C</math>, the area of the region bounded by <math>C</math> and the oriented area of the [[Wigner caustic]] of <math>C</math>, respectively, and the equality holds if and only if <math>C</math> is a [[curve of constant width]].<ref>{{cite journal| url=http://www.sciencedirect.com/science/article/pii/S0022247X16301585 | title = The improved isoperimetric inequality and the Wigner caustic of planar ovals | first = Michał | last = Zwierzyński | journal = J. Math. Anal. Appl. | volume = 442 | date = 2016| pages = 726–739|doi=10.1016/j.jmaa.2016.05.016|issue=2 | arxiv=1512.06684}}</ref>\n\n== On a sphere ==\n\nLet ''C'' be a simple closed curve on a [[sphere]] of radius 1. Denote by ''L'' the length of ''C'' and by ''A'' the area enclosed by ''C''. The '''spherical isoperimetric inequality''' states that\n\n:<math>L^2 \\ge A (4\\pi - A),</math>\n\nand that the equality holds if and only if the curve is a circle. There are, in fact, two ways to measure the spherical area enclosed by a simple closed curve, but the inequality is symmetric with the respect to taking the complement.\n\nThis inequality was discovered by [[Paul Lévy (mathematician)|Paul Lévy]] (1919) who also extended it to higher dimensions and general surfaces<ref>{{Cite book|url=http://cds.cern.ch/record/1412861|title=Metric Structures for Riemannian and Non-Riemannian Spaces|last=Gromov|first=Mikhail|last2=Pansu|first2=Pierre|date=2006|publisher=Springer|year=2006|isbn=9780817645830|series=Modern Birkhäuser Classics|location=Dordrecht|pages=519|chapter=Appendix C. Paul Levy's Isoperimetric Inequality}}</ref>.\n\nIn the more general case of arbitrary radius ''R'', it is known <ref>[[Robert Osserman|Osserman, Robert]]. \"The Isoperimetric Inequality.\" Bulletin of the American Mathematical Society. 84.6 (1978) http://www.ams.org/journals/bull/1978-84-06/S0002-9904-1978-14553-4/S0002-9904-1978-14553-4.pdf</ref> that\n\n:<math>L^2\\ge 4\\pi A - \\frac{A^2}{R^2}.</math>\n\n== In <math>\\R^n</math> ==\nThe isoperimetric inequality states that a [[sphere]] has the smallest surface area per given volume. Given a set <math>S\\subset\\R ^n</math> with [[surface area]] <math>\\mathrm{surf}(S)</math> and [[volume]] <math>\\mathrm{vol}(S)</math>, the isoperimetric inequality states\n\n:<math>\\mathrm{surf}(S)\\geq n \\mathrm{vol}(S)^{\\frac{n-1}{n}}\\mathrm{vol}(B_1)^{\\frac{1}{n}}</math>,\n\nwhere <math>B_1\\subset\\R ^n</math> is a [[Unit sphere|unit ball]]. The equality holds when <math>S</math> is a ball in <math>\\R ^n</math>.\n\nThe proof on the inequality follows directly from [[Brunn–Minkowski theorem|Brunn–Minkowski inequality]] between a set <math>S</math> and a ball with radius <math>\\epsilon</math>, i.e. <math>B_\\epsilon=\\epsilon B_1</math>. By taking Brunn–Minkowski inequality to the power <math>n</math>, subtracting <math>\\mathrm{vol}(S)</math> from both sides, dividing them by <math>\\epsilon</math>, and taking the limit as <math>\\epsilon\\to 0.</math> ({{harvtxt|Osserman|1978}}; {{harvtxt|Federer|1969|loc=§3.2.43}}).\n\nIn full generality {{harv|Federer|1969|loc=§3.2.43}}, the isoperimetric inequality states that for any set <math>S\\subset\\R^n</math> whose [[closure of a set|closure]] has finite [[Lebesgue measure]]\n\n:<math>n\\omega_n^{\\frac{1}{n}} L^n(\\bar{S})^{\\frac{n-1}{n}} \\le M^{n-1}_*(\\partial S)</math>\n\nwhere <math>M_*^{n-1}</math> is the (''n''-1)-dimensional [[Minkowski content]], ''L<sup>n</sup>'' is the ''n''-dimensional Lebesgue measure, and ''ω<sub>n</sub>'' is the volume of the [[unit ball]] in <math>\\R^n</math>. If the boundary of ''S'' is [[rectifiable curve|rectifiable]], then the Minkowski content is the (''n''-1)-dimensional [[Hausdorff measure]].\n\nThe ''n''-dimensional isoperimetric inequality is equivalent (for sufficiently smooth domains) to the [[Sobolev inequality]] on <math>\\R^n</math> with optimal constant:\n\n:<math>\\left( \\int_{\\R^n} |u|^{\\frac{n}{n-1}}\\right)^{\\frac{n-1}{n}} \\le n^{-1}\\omega_{n}^{-\\frac{1}{n}}\\int_{\\R^n}|\\nabla u|</math>\n\nfor all <math>u\\in W^{1,1}(\\R^n)</math>.\n\n==In a metric measure space==\n\nMost of the work on isoperimetric problem has been done in the context of smooth regions in [[Euclidean space]]s, or more generally, in [[Riemannian manifold]]s. However, the isoperimetric problem can be formulated in much greater generality, using the notion of ''Minkowski content''. Let <math>(X, \\mu, d)</math> be a ''metric measure space'': ''X'' is a [[metric space]] with [[metric (mathematics)|metric]] ''d'', and ''μ'' is a [[Borel measure]] on ''X''. The ''boundary measure'', or [[Minkowski content]], of a [[measurable]] subset ''A'' of ''X'' is defined as the [[lim inf]]\n\n: <math>\\mu^+(A) = \\liminf_{\\varepsilon \\to 0+} \\frac{\\mu(A_\\varepsilon) - \\mu(A)}{\\varepsilon},</math>\n\nwhere\n\n: <math>A_\\varepsilon = \\{ x \\in X | d(x, A) \\leq \\varepsilon \\}</math>\n\nis the ε-''extension'' of ''A''.\n\nThe isoperimetric problem in ''X'' asks how small can <math>\\mu^+(A)</math> be for a given ''μ''(''A''). If ''X'' is the [[plane (mathematics)|Euclidean plane]] with the usual distance and the [[Lebesgue measure]] then this question generalizes the classical isoperimetric problem to planar regions whose boundary is not necessarily smooth, although the answer turns out to be the same.\n\nThe function\n\n:<math>I(a) = \\inf \\{ \\mu^+(A) | \\mu(A) = a\\}</math>\n\nis called the ''isoperimetric profile'' of the metric measure space <math>(X, \\mu, d)</math>. Isoperimetric profiles have been studied for [[Cayley graph]]s of [[discrete group]]s and for special classes of Riemannian manifolds (where usually only regions ''A'' with regular boundary are considered).\n\n== For graphs ==\n\n{{main|Expander graph}}\n\nIn [[graph theory]], isoperimetric inequalities are at the heart of the study of [[expander graphs]], which are [[sparse graph]]s that have strong connectivity properties. Expander constructions have spawned research in pure and applied mathematics, with several applications to [[Computational complexity theory|complexity theory]], design of robust [[computer network]]s, and the theory of [[error-correcting code]]s.<ref>{{harvtxt|Hoory|Linial|Widgerson|2006}}</ref>\n\nIsoperimetric inequalities for graphs relate the size of vertex subsets to the size of their boundary, which is usually measured by the number of edges leaving the subset (edge expansion) or by the number of neighbouring vertices (vertex expansion). For a graph <math>G</math> and a number <math>k</math>, the following are two standard isoperimetric parameters for graphs.<ref>Definitions 4.2 and 4.3 of {{harvtxt|Hoory|Linial|Widgerson|2006}}</ref>\n\n*The edge isoperimetric parameter: \n::<math>\\Phi_E(G,k)=\\min_{S\\subseteq V} \\left\\{|E(S,\\overline{S})| : |S|=k \\right\\}</math>\n*The vertex isoperimetric parameter: \n::<math>\\Phi_V(G,k)=\\min_{S\\subseteq V} \\left\\{|\\Gamma(S)\\setminus S| : |S|=k \\right\\}</math>\n\nHere <math>E(S,\\overline{S})</math> denotes the set of edges leaving <math>S</math> and <math>\\Gamma(S)</math> denotes the set of vertices that have a neighbour in <math>S</math>. The isoperimetric problem consists of understanding how the parameters <math>\\Phi_E</math> and <math>\\Phi_V</math> behave for natural families of graphs.\n\n=== Example: Isoperimetric inequalities for hypercubes ===\nThe <math>d</math>-dimensional [[hypercube]] <math>Q_d</math> is the graph whose vertices are all Boolean vectors of length <math>d</math>, that is, the set <math>\\{0,1\\}^d</math>. Two such vectors are connected by an edge in <math>Q_d</math> if they are equal up to a single bit flip, that is, their [[Hamming distance]] is exactly one.\nThe following are the isoperimetric inequalities for the Boolean hypercube.<ref>See {{harvtxt|Bollobás|1986}} and Section 4 in {{harvtxt|Hoory|Linial|Widgerson|2006}}</ref>\n\n==== Edge isoperimetric inequality ====\nThe edge isoperimetric inequality of the hypercube is <math>\\Phi_E(Q_d,k) \\geq k(d-\\log_2 k)</math>. This bound is tight, as is witnessed by each set <math>S</math> that is the set of vertices of any subcube of <math>Q_d</math>.\n\n==== Vertex isoperimetric inequality ====\nHarper's theorem<ref>Cf. {{harvtxt|Calabro|2004}} or {{harvtxt|Bollobás|1986}}</ref> says that ''Hamming balls'' have the smallest vertex boundary among all sets of a given size. Hamming balls are sets that contain all points of [[Hamming weight]] at most <math>r</math> and no points of Hamming weight larger than <math>r+1</math> for some integer <math>r</math>. This theorem implies that any set <math>S\\subseteq V</math> with \n\n:<math>|S|\\geq\\sum_{i=0}^{r} {d\\choose i}</math> \n\nsatisfies \n\n:<math>|S\\cup\\Gamma(S)|\\geq \\sum_{i=0}^{r+1}{d\\choose i}.</math><ref>cf. {{harvtxt|Leader|1991}}</ref>\n\nAs a special case, consider set sizes <math>k=|S|</math> of the form \n\n:<math>k={d \\choose 0} + {d \\choose 1} + \\dots + {d \\choose r}</math> \n\nfor some integer <math>r</math>. Then the above implies that the exact vertex isoperimetric parameter is \n\n:<math>\\Phi_V(Q_d,k) = {d\\choose r+1}.</math><ref>Also stated in {{harvtxt|Hoory|Linial|Widgerson|2006}}</ref>\n\n==Isoperimetric inequality for triangles==\n\nThe isoperimetric inequality for triangles in terms of perimeter ''p'' and area ''T'' states that<ref name=Chakerian>Chakerian, G. D. \"A Distorted View of Geometry.\" Ch. 7 in ''Mathematical Plums'' (R. Honsberger, editor). Washington, DC: Mathematical Association of America, 1979: 147.</ref><ref>[https://math.stackexchange.com/questions/2325779/the-isoperimetric-inequality-for-triangles/]</ref>\n\n:<math>p^2 \\ge 12\\sqrt{3} \\cdot T,</math>\n\nwith equality for the [[equilateral triangle]]. This is implied, via the [[inequality of arithmetic and geometric means|AM-GM inequality]], by a stronger inequality which has also been called the isoperimetric inequality for triangles:<ref>Dragutin Svrtan and Darko Veljan, \"Non-Euclidean Versions of Some Classical Triangle Inequalities\", ''Forum Geometricorum'' 12, 2012, 197–209. http://forumgeom.fau.edu/FG2012volume12/FG201217.pdf</ref>\n\n:<math>T \\le \\frac{\\sqrt{3}}{4}(abc)^{\\frac{2}{3}}.</math>\n\n==See also==\n* [[Blaschke–Lebesgue theorem]]\n* [[Chaplygin problem]]\n* [[Curve-shortening flow]]\n* [[Expander graph]]\n* [[Gaussian isoperimetric inequality]]\n* [[Isoperimetric dimension]]\n* [[Isoperimetric point]]\n* [[List of triangle inequalities]]\n* [[Planar separator theorem]]\n* [[Mixed volume]]\n\n==Notes==\n{{reflist}}\n\n== References ==\n{{Refbegin|colwidth=25em}}\n*[[Wilhelm Blaschke|Blaschke]] and Leichtweiß, ''Elementare Differentialgeometrie'' (in German), 5th edition, completely revised by K. Leichtweiß. Die Grundlehren der mathematischen Wissenschaften, Band 1. [[Springer-Verlag]], New York Heidelberg Berlin, 1973 {{isbn|0-387-05889-3}}\n*{{Cite book | first1=Béla | last1=Bollobás | author1-link = Béla Bollobás | title=Combinatorics: set systems, hypergraphs, families of vectors, and combinatorial probability| year=1986 | isbn=978-0-521-33703-8 | publisher=Cambridge University Press | ref=harv | postscript=<!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->{{inconsistent citations}}}}\n*{{Springer |author=Burago |title=Isoperimetric inequality |id=I/i052860}}\n*{{Cite web | last = Calabro | first = Chris | title = Harper's Theorem | year = 2004 | url=http://cseweb.ucsd.edu/~ccalabro/essays/harper.pdf| ref=harv | accessdate = 8 February 2011}}\n*{{cite book |last= Capogna |first= Luca |author2=Donatella Danielli |author3=Scott Pauls |author4=Jeremy Tyson |title= An Introduction to the Heisenberg Group and the Sub-Riemannian Isoperimetric Problem |publisher= [[Birkhäuser Verlag]] |year= 2007 |isbn= 3-7643-8132-9}}\n*{{cite book | last=[[Werner Fenchel|Fenchel]] | first=[[Werner Fenchel|Werner]] |author2=Bonnesen, Tommy | title=Theorie der konvexen Körper | series=Ergebnisse der Mathematik und ihrer Grenzgebiete | volume=3 | publisher=1. Verlag von Julius Springer | location=Berlin | year=1934}}\n*{{cite book | last= [[Werner Fenchel|Fenchel]] | first=[[Werner Fenchel|Werner]] |author2=Bonnesen, Tommy | title=Theory of convex bodies | publisher=L. Boron, C. Christenson and B. Smith. BCS Associates | location=Moscow, Idaho | year=1987}}\n*{{Cite book|first=Herbert|last=Federer|authorlink=Herbert Federer|title=Geometric measure theory|publisher=Springer-Verlag|year=1969|isbn=3-540-60656-4|ref=harv|postscript=<!--None-->}}.\n*[[Mikhail Gromov (mathematician)|Gromov, M.]]: \"Paul Levy's isoperimetric inequality\". Appendix C in ''Metric structures for Riemannian and non-Riemannian spaces''. Based on the 1981 French original. With appendices by M. Katz, P. Pansu and S. Semmes. Translated from the French by Sean Michael Bates. Progress in Mathematics, 152. Birkhäuser Boston, Inc., Boston, Massachusetts, 1999.\n*Hadwiger, H. (1957), ''Vorlesungen über Inhalt, Oberfläche und Isoperimetrie'' (in German), [[Springer-Verlag]], Berlin Göttingen Heidelberg.\n*{{Cite journal|first1=Shlomo| last1=Hoory | first2=Nathan | last2=Linial | author2-link = Nati Linial | first3=Avi | last3=Widgerson | author3-link = Avi Wigderson | title=Expander graphs and their applications | journal= Bulletin (New Series) of the American Mathematical Society | volume=43 | issue=4 | pages=439–561 | url=http://www.cs.huji.ac.il/~nati/PAPERS/expander_survey.pdf | year=2006 | doi = 10.1090/S0273-0979-06-01126-8 | ref=harv | postscript=<!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->{{inconsistent citations}}}}\n*{{Cite conference | last1=Leader | first1=Imre | title=Discrete isoperimetric inequalities | booktitle=Proceedings of Symposia in Applied Mathematics\n | volume=44 | pages=57–80 | year=1991 | ref=harv}}\n*{{Cite journal|first=Robert|last=Osserman|url=http://www.ams.org/bull/1978-84-06/S0002-9904-1978-14553-4/|title=The isoperimetric inequality| journal=Bull. Amer. Math. Soc.|volume=84|year=1978|pages=1182–1238|doi=10.1090/S0002-9904-1978-14553-4|issue=6|ref=harv|postscript=<!--None-->}}.\n*{{Cite journal | url=http://www.sciencedirect.com/science/article/pii/S0022247X16301585 | title = The improved isoperimetric inequality and the Wigner caustic of planar ovals | first = Michał | last = Zwierzyński | journal = J. Math. Anal. Appl. | volume = 442 | date = 2016| pages = 726–739| doi=10.1016/j.jmaa.2016.05.016|issue=2|postscript=<!--None--> |arxiv=1512.06684}}\n{{Refend}}\n\n== External links ==\n{{commons category|Isoperimetric inequality}}\n*[https://web.archive.org/web/20070715043457/http://mathdl.maa.org/convergence/1/?pa=content&sa=viewDocument&nodeId=1186&bodyId=1314 History of the Isoperimetric Problem] at [https://web.archive.org/web/20070713083148/http://mathdl.maa.org/convergence/1/ Convergence]\n* [http://www.math.utah.edu/~treiberg/isoperim/isop.pdf Treiberg: Several proofs of the isoperimetric inequality]\n* [http://www.cut-the-knot.org/do_you_know/isoperimetric.shtml Isoperimetric Theorem] at [[cut-the-knot]]\n\n[[Category:Multivariable calculus]]\n[[Category:Calculus of variations]]\n[[Category:Geometric inequalities]]\n[[Category:Analytic geometry]]"
    },
    {
      "title": "Jung's theorem",
      "url": "https://en.wikipedia.org/wiki/Jung%27s_theorem",
      "text": "In [[geometry]], '''Jung's theorem''' is an [[inequality (mathematics)|inequality]] between the [[diameter]] of a set of points in any [[Euclidean space]] and the radius of the [[circumradius|minimum enclosing ball]] of that set. It is named after [[Heinrich Jung]], who first studied this inequality in 1901.\n\n== Statement ==\nConsider a [[compact set]]\n\n:<math>K\\subset \\mathbb{R}^n</math>\n\nand let\n\n:<math>d = \\max_{p,q\\,\\in\\, K} \\| p - q \\|_2</math>\n\nbe the [[diameter]] of ''K'', that is, the largest [[distance|Euclidean distance]] between any two of its points. Jung's theorem states that there exists a [[closed ball]] with [[radius]]\n\n:<math>r \\leq d \\sqrt{\\frac{n}{2(n+1)}}</math>\n\nthat contains ''K''.  The boundary case of equality is attained by the regular ''n''-[[simplex]].\n\n== Jung's theorem in the plane ==\nMost common is the case of Jung's theorem in the [[Euclidean plane|plane]], that is ''n''&nbsp;=&nbsp;2. In this case the theorem states that there exists a circle enclosing all points whose radius satisfies\n\n:<math>r \\leq \\frac{d}{\\sqrt{3}}.</math>\n\nNo tighter bound on ''r'' can be shown: when ''K'' is an equilateral triangle (or its three vertices), then\n\n:<math>r = \\frac{d}{\\sqrt{3}}.</math>\n\n== General metric spaces ==\nFor any bounded set ''S'' in any [[metric space]], ''d''/2 ≤ ''r'' ≤ ''d''. The first inequality is implied by the [[triangle inequality]] for the center of the ball and the two diametral points, and the second inequality follows since a ball of radius ''d'' centered at any point of ''S'' will contain all of ''S''. In a ''uniform metric space'', that is, a space in which all distances are equal, ''r'' = ''d''. At the other end of the spectrum, in an [[injective metric space]] such as the [[taxicab geometry|Manhattan distance]] in the plane, ''r'' = ''d''/2: any two closed balls of radius ''d''/2 centered at points of ''S'' have a nonempty intersection, therefore all such balls have a common intersection, and a radius ''d''/2 ball centered at a point of this intersection contains all of ''S''. Versions of Jung's theorem for various [[non-Euclidean geometry|non-Euclidean geometries]] are also known (see e.g. Dekster 1995, 1997).\n\n== References ==\n*{{cite journal\n | author = Katz, M.\n | title = Jung's theorem in complex projective geometry\n | journal =  Quart. J. Math. Oxford\n | volume = 36\n | issue = 4\n | year = 1985\n | pages = 451–466\n | doi = 10.1093/qmath/36.4.451}}\n*{{cite journal\n | author = Dekster, B. V.\n | title = The Jung theorem for the spherical and hyperbolic spaces\n | journal = Acta Math. Hungar.\n | volume = 67\n | issue = 4\n | year = 1995\n | pages = 315–331\n | doi = 10.1007/BF01874495}}\n*{{cite journal\n | author = Dekster, B. V.\n | title = The Jung theorem in metric spaces of curvature bounded above\n | journal = Proceedings of the American Mathematical Society\n | volume = 125\n | issue = 8\n | year = 1997\n | pages = 2425–2433\n | doi = 10.1090/S0002-9939-97-03842-2}}\n*{{cite journal\n | author = Jung, Heinrich\n | title = Über die kleinste Kugel, die eine räumliche Figur einschließt\n | journal = J. Reine Angew. Math.\n | volume = 123\n | year = 1901\n | pages = 241–257\n | url = https://eudml.org/doc/149122\n | language = German}}\n*{{cite journal\n | author = Jung, Heinrich\n | title = Über den kleinsten Kreis, der eine ebene Figur einschließt\n | journal = J. Reine Angew. Math.\n | volume = 137\n | year = 1910\n | pages = 310–313\n | url = https://eudml.org/doc/149324\n | language = German}}\n*{{cite book\n |author1=Rademacher, Hans |author2=Toeplitz, Otto | title = The Enjoyment of Mathematics\n | year = 1990\n | publisher = Dover\n | isbn = 978-0-486-26242-0\n | nopp = true\n | page = chapter 16}}\n\n== External links ==\n*{{mathworld | title = Jung's Theorem | urlname = JungsTheorem}}\n\n[[Category:Geometric inequalities]]\n[[Category:Euclidean geometry]]\n[[Category:Theorems in geometry]]\n[[Category:Metric geometry]]"
    },
    {
      "title": "Loewner's torus inequality",
      "url": "https://en.wikipedia.org/wiki/Loewner%27s_torus_inequality",
      "text": "[[Image:Loewner63.jpg|right|thumb|200px|Charles Loewner in 1963]]\nIn [[differential geometry]], '''Loewner's torus inequality''' is an [[inequality (mathematics)|inequality]] due to [[Charles Loewner]].  It relates the [[Systolic geometry|systole]] and the [[area]] of an arbitrary [[Riemannian metric]] on the [[2-torus]].\n\n==Statement==\n[[Image:TorusSystoleLoop.png|right|thumb|200px|Shortest loop on a torus]]\nIn 1949 [[Charles Loewner]] proved that every metric on the 2-[[torus]] <math>\\mathbb T^2</math> satisfies the optimal inequality\n\n:<math> \\operatorname{sys}^2 \\leq \\frac{2}{\\sqrt{3}} \\;\\operatorname{area}(\\mathbb T^2),</math>\n\nwhere \"sys\" is its [[Systolic geometry|systole]], i.e. least length of a noncontractible loop.  The constant appearing on the right hand side is the [[Hermite constant]] <math>\\gamma_2</math> in dimension 2, so that Loewner's torus inequality can be rewritten as\n\n:<math> \\operatorname{sys}^2 \\leq \\gamma_2\\;\\operatorname{area}(\\mathbb T^2).</math>\n\nThe inequality was first mentioned in the literature in {{harvtxt|Pu|1952}}.\n\n==Case of equality==\n\nThe boundary case of equality is attained if and only if the metric is flat and homothetic to the so-called ''equilateral torus'', i.e. torus whose group of deck transformations is precisely the [[hexagonal lattice]] spanned by the cube roots of unity in <math>\\mathbb C</math>.\n\n==Alternative formulation==\n\nGiven a doubly periodic metric on <math>\\mathbb R^2</math> (e.g. an imbedding in <math>\\mathbb R^3</math> which is invariant by a <math>\\mathbb Z^2</math> isometric action), there is a nonzero element <math>g\\in \\mathbb Z^2</math> and a point <math>p\\in \\mathbb R^2</math> such that <math>\\operatorname{dist}(p, g.p)^2 \\leq \\frac{2}{\\sqrt{3}} \\operatorname{area} (F)</math>, where <math>F</math> is a fundamental domain for the action, while <math>\\operatorname{dist}</math> is the Riemannian distance, namely least length of a path joining <math>p</math> and <math> g . p </math>.\n\n==Proof of Loewner's torus inequality==\n\nLoewner's torus inequality can be proved most easily by using the [[computational formula for the variance]], \n\n:<math>E(X^2)-(E(X))^2=\\mathrm{var}(X).\\,</math>\n\nNamely, the formula is applied to the [[probability measure]] defined by the measure of the unit area flat torus in the conformal class of the given torus.  For the random variable ''X'', one takes the conformal factor of the given metric with respect to the flat one.  Then the expected value E(''X''<sup>&nbsp;2</sup>) of ''X''<sup>&nbsp;2</sup> expresses the total area of the given metric.  Meanwhile, the expected value E(''X'') of ''X'' can be related to the systole by using [[Fubini's theorem]].  The variance of ''X'' can then be thought of as the isosystolic defect, analogous to the isoperimetric defect of [[Bonnesen's inequality]].  This approach therefore produces the following version of Loewner's torus inequality with isosystolic defect:\n\n:<math>\\mathrm{area}-\\frac{\\sqrt{3}}{2}(\\mathrm{sys})^2\\geq \\mathrm{var}(f),</math>\n\nwhere ''ƒ'' is the conformal factor of the metric with respect to a unit area flat metric in its conformal class.\n\n==Higher genus==\n\nWhether or not the inequality \n\n:<math> (\\mathrm{sys})^2 \\leq \\gamma_2\\,\\mathrm{area}</math>\n\nis satisfied by all surfaces of nonpositive [[Euler characteristic]] is unknown.  For [[orientable surface]]s of genus 2 and genus 20 and above, the answer is affirmative, see work by Katz and Sabourau below.\n\n==See also==\n*[[Pu's inequality|Pu's inequality for the real projective plane]]\n*[[Gromov's systolic inequality for essential manifolds]]\n*[[Gromov's inequality for complex projective space]]\n*[[Eisenstein integer]] (an example of a hexagonal lattice)\n*[[Systoles of surfaces]]\n\n==References==\n*{{cite journal |first=Charles |last=Horowitz |first2=Karin Usadi |last2=Katz |authorlink3=Mikhail Katz |first3=Mikhail G. |last3=Katz |year=2009 |title=Loewner's torus inequality with isosystolic defect |journal=Journal of Geometric Analysis |volume=19 |issue=4 |pages=796–808 |arxiv=0803.0690 |doi=10.1007/s12220-009-9090-y | mr=2538936}}\n* {{cite book | last=Katz | first=Mikhail G. | title=Systolic geometry and topology | others=With an appendix by J. Solomon. | series=Mathematical Surveys and Monographs | volume=137 | publisher=[[American Mathematical Society]] | year=2007 | isbn=978-0-8218-4177-8 | location=Providence, RI | mr=2292367 | doi=10.1090/surv/137}}\n*{{cite journal |last=Katz |first=Mikhail G. |last2=Sabourau |first2=Stéphane |title=Entropy of systolically extremal surfaces and asymptotic bounds |journal=Ergodic Theory Dynam. Systems |volume=25 |year=2005 |issue= 4 |pages=1209–1220 |arxiv=math.DG/0410312 |doi=10.1017/S0143385704001014 | mr=2158402}}\n*{{cite journal |last=Katz |first=Mikhail G. |last2=Sabourau |first2=Stéphane |title=Hyperelliptic surfaces are Loewner |journal=[[Proc. Amer. Math. Soc.]] |volume=134 |year=2006 |issue=4 |pages=1189–1195 |arxiv=math.DG/0407009 |doi=10.1090/S0002-9939-05-08057-3 | mr=2196056}}\n*{{cite journal | last=Pu | first=Pao Ming | title=Some inequalities in certain nonorientable Riemannian manifolds | journal=[[Pacific J. Math.]] | volume=2 | year=1952 | pages=55&ndash;71 | mr=0048886 | url=http://projecteuclid.org/euclid.pjm/1103051942 | issue=1 | ref=harv}}\n\n{{Systolic geometry navbox}}\n\n[[Category:Riemannian geometry]]\n[[Category:Differential geometry]]\n[[Category:Geometric inequalities]]\n[[Category:Differential geometry of surfaces]]\n[[Category:Systolic geometry]]"
    },
    {
      "title": "Loomis–Whitney inequality",
      "url": "https://en.wikipedia.org/wiki/Loomis%E2%80%93Whitney_inequality",
      "text": "In [[mathematics]], the '''Loomis–Whitney inequality''' is a result in [[geometry]], which in its simplest form, allows one to estimate the \"size\" of a <math>d</math>-[[dimension]]al set by the sizes of its <math>(d-1)</math>-dimensional projections. The inequality has applications in [[incidence geometry]], the study of so-called \"lattice animals\", and other areas.\n\nThe result is named after the [[United States|American]] [[mathematicians]] [[Lynn Harold Loomis]] and [[Hassler Whitney]], and was published in 1949.\n\n==Statement of the inequality==\nFix a dimension <math>d\\ge 2</math> and consider the projections\n\n:<math>\\pi_{j} : \\mathbb{R}^{d} \\to \\mathbb{R}^{d - 1},</math>\n:<math>\\pi_{j} : x = (x_{1}, \\dots, x_{d}) \\mapsto \\hat{x}_{j} = (x_{1}, \\dots, x_{j - 1}, x_{j + 1}, \\dots, x_{d}).</math>\n\nFor each 1 ≤ ''j'' ≤ ''d'', let\n\n:<math>g_{j} : \\mathbb{R}^{d - 1} \\to [0, + \\infty),</math>\n:<math>g_{j} \\in L^{d - 1} (\\mathbb{R}^{d -1}).</math>\n\nThen the '''Loomis–Whitney inequality''' holds:\n\n:<math>\\int_{\\mathbb{R}^{d}} \\prod_{j = 1}^{d} g_{j} ( \\pi_{j} (x) ) \\, \\mathrm{d} x \\leq \\prod_{j = 1}^{d} \\| g_{j} \\|_{L^{d - 1} (\\mathbb{R}^{d - 1})}.</math>\n\nEquivalently, taking\n\n:<math>f_{j} (x) = g_{j} (x)^{d - 1},</math>\n\n:<math>\\int_{\\mathbb{R}^{d}} \\prod_{j = 1}^{d} f_{j} ( \\pi_{j} (x) )^{1 / (d - 1)} \\, \\mathrm{d} x \\leq \\prod_{j = 1}^{d} \\left( \\int_{\\mathbb{R}^{d - 1}} f_{j} (\\hat{x}_{j}) \\, \\mathrm{d} \\hat{x}_{j} \\right)^{1 / (d - 1)}.</math>\n\n==A special case==\nThe Loomis–Whitney inequality can be used to relate the [[Lebesgue measure]] of a subset of [[Euclidean space]] <math>\\mathbb{R}^{d}</math> to its \"average widths\" in the coordinate directions. Let ''E'' be some [[measurable set|measurable subset]] of <math>\\mathbb{R}^{d}</math> and let\n\n:<math>f_{j} = \\mathbf{1}_{\\pi_{j} (E)}</math>\n\nbe the [[indicator function]] of the projection of ''E'' onto the ''j''th coordinate hyperplane. It follows that for any point ''x'' in ''E'',\n\n:<math>\\prod_{j = 1}^{d} f_{j} (\\pi_{j} (x))^{1 / (d - 1)} = 1.</math>\n\nHence, by the Loomis–Whitney inequality,\n\n:<math>| E | \\leq \\prod_{j = 1}^{d} | \\pi_{j} (E) |^{1 / (d - 1)},</math>\n\nand hence\n\n:<math>| E | \\geq \\prod_{j = 1}^{d} \\frac{| E |}{| \\pi_{j} (E) |}.</math>\n\nThe quantity\n\n:<math>\\frac{| E |}{| \\pi_{j} (E) |}</math>\n\ncan be thought of as the average width of <math>E</math> in the <math>j</math>th coordinate direction. This interpretation of the Loomis–Whitney inequality also holds if we consider a finite subset of Euclidean space and replace Lebesgue measure by [[counting measure]].\n\n==Generalizations==\nThe Loomis–Whitney inequality is a special case of the [[Brascamp–Lieb inequality]], in which the projections ''π<sub>j</sub>'' above are replaced by more general [[linear map]]s, not necessarily all mapping onto spaces of the same dimension.\n\n==References==\n* {{cite journal\n| last1 = Loomis\n| first1 = Lynn H.\n|authorlink1=Lynn Harold Loomis\n|last2=Whitney\n|first2=Hassler\n|authorlink2=Hassler Whitney\n | title = An inequality related to the isoperimetric inequality\n| journal = [[Bulletin of the American Mathematical Society]]\n| volume = 55\n| year = 1949\n| pages = 961–962\n| doi = 10.1090/S0002-9904-1949-09320-5\n}} {{MathSciNet|id=0031538}}\n\n{{DEFAULTSORT:Loomis-Whitney inequality}}\n[[Category:Incidence geometry]]\n[[Category:Geometric inequalities]]"
    },
    {
      "title": "Mahler volume",
      "url": "https://en.wikipedia.org/wiki/Mahler_volume",
      "text": "In [[convex geometry]], the '''Mahler volume''' of a [[central symmetry|centrally symmetric]] [[convex body]] is a [[dimensionless quantity]] that is associated with the body and is invariant under [[linear transformation]]s. It is named after German-English mathematician [[Kurt Mahler]]. It is known that the shapes with the largest possible Mahler volume are the balls and solid ellipsoids; this is now known as the '''Blaschke–Santaló inequality'''. The still-unsolved '''Mahler conjecture''' states that the minimum possible Mahler volume is attained by a [[hypercube]].\n\n==Definition==\nA convex body in [[Euclidean space]] is defined as a [[compact space|compact]] convex set with non-empty interior.  If ''B'' is a centrally symmetric convex body in ''n''-dimensional [[Euclidean space]], the [[Polar set|polar body]] ''B''<sup>o</sup> is another centrally symmetric body in the same space, defined as the set\n\n:<math>\\left\\{ x\\mid x\\cdot y\\le 1 \\text{ for all } y\\in B \\right\\}.</math>\n\nThe Mahler volume of ''B'' is the product of the volumes of ''B'' and ''B''<sup>o</sup>.<ref name=\"tao\">{{harvtxt|Tao|2007}}.</ref>\n\nIf ''T'' is an invertible linear transformation, then <math>(TB)^\\circ = (T^{-1})^\\ast B^\\circ</math>; thus applying ''T'' to ''B'' changes its volume by <math>\\det T</math> and changes the volume of ''B''<sup>o</sup> by <math>\\det (T^{-1})^\\ast</math>.  Thus the overall Mahler volume of ''B'' is preserved by linear transformations.\n\n==Examples==\nThe polar body of an ''n''-dimensional [[n-sphere|unit sphere]] is itself another unit sphere. Thus, its Mahler volume is just the square of its volume, \n:<math>\\frac{\\Gamma(3/2)^{2n}4^n}{\\Gamma(\\frac{n}{2}+1)^2}.</math>\nHere Γ represents the [[Gamma function]].\nBy affine invariance, any [[ellipsoid]] has the same Mahler volume.<ref name=\"tao\"/>\n\nThe polar body of a [[polyhedron]] or [[polytope]] is its [[dual polyhedron]] or dual polytope. In particular, the polar body of a [[cube]] or [[hypercube]] is an [[octahedron]] or [[cross polytope]]. Its Mahler volume can be calculated as<ref name=\"tao\"/>\n:<math>\\frac{4^n}{\\Gamma(n+1)}.</math>\n\nThe Mahler volume of the sphere is larger than the Mahler volume of the hypercube by a factor of approximately <math>\\left(\\tfrac{\\pi}{2}\\right)^n</math>.<ref name=\"tao\"/>\n\n==Extreme shapes==\nThe Blaschke–Santaló inequality states that the shapes with maximum Mahler volume are the spheres and ellipsoids. The three-dimensional case of this result was proven by [[Wilhelm Blaschke]]; the full result was proven much later by {{harvs|first=Luis|last=Santaló|authorlink=Luis Santaló|txt|year=1949}} using a technique known as [[Steiner symmetrization]] by which any centrally symmetric convex body can be replaced with a more sphere-like body without decreasing its Mahler volume.<ref name=\"tao\"/>\n\nThe shapes with the minimum known Mahler volume are [[hypercube]]s, [[cross polytope]]s, and more generally the [[Hanner polytope]]s which include these two types of shapes, as well as their affine transformations. The Mahler conjecture states that the Mahler volume of these shapes is the smallest of any ''n''-dimensional symmetric convex body; it remains unsolved. As [[Terence Tao|Terry Tao]] writes:<ref name=\"tao\"/>\n{{cquote|The main reason why this conjecture is so difficult is that unlike the upper bound, in which there is essentially only one extremiser up to affine transformations (namely the ball), there are many distinct extremisers for the lower bound - not only the cube and the octahedron, but also products of cubes and octahedra, polar bodies of products of cubes and octahedra, products of polar bodies of… well, you get the idea. It is really difficult to conceive of any sort of flow or optimisation procedure which would converge to exactly these bodies and no others; a radically different type of argument might be needed.}}\n\n{{harvtxt|Bourgain|Milman|1987}} prove that the Mahler volume is bounded below by ''c<sup>n</sup>'' times the volume of a sphere for some absolute constant ''c''&nbsp;&gt;&nbsp;0, matching the scaling behavior of the hypercube volume but with a smaller constant. A result of this type is known as a '''reverse Santaló inequality'''.\n\n==Notes==\n{{reflist}}\n\n==References==\n*{{citation\n | last1 = Bourgain | first1 = J.\n | last2 = Milman | first2 = V. D.\n | doi = 10.1007/BF01388911\n | mr = 880954\n | issue = 2\n | journal = Inventiones Mathematicae\n | pages = 319–340\n | title = New volume ratio properties for convex symmetric bodies in '''R'''<sup>''n''</sup>\n | volume = 88\n | year = 1987}}.\n*{{citation\n | last = Santaló | first = L. A.\n | format = In Spanish\n | mr = 0039293\n | journal = Portugaliae Math.\n | pages = 155–161\n | title = An affine invariant for convex bodies of ''n''-dimensional space\n | volume = 8\n | year = 1949}}.\n*{{citation|url=http://terrytao.wordpress.com/2007/03/08/open-problem-the-mahler-conjecture-on-convex-bodies/|first=Terence|last=Tao|authorlink=Terence Tao|title=Open question: the Mahler conjecture on convex bodies|date=March 8, 2007}}. Revised and reprinted in {{citation|first=Terence|last=Tao|authorlink=Terence Tao|contribution=3.8 Mahler's conjecture for convex bodies|pages=216–219|title=Structure and Randomness: Pages from Year One of a Mathematical Blog|publisher=[[American Mathematical Society]]|year=2009|isbn=978-0-8218-4695-7}}.\n\n[[Category:Volume]]\n[[Category:Geometric inequalities]]\n[[Category:Convex geometry]]"
    },
    {
      "title": "Milman's reverse Brunn–Minkowski inequality",
      "url": "https://en.wikipedia.org/wiki/Milman%27s_reverse_Brunn%E2%80%93Minkowski_inequality",
      "text": "In [[mathematics]], particularly, in asymptotic [[convex geometry]], '''Milman's reverse Brunn–Minkowski inequality''' is a result due to [[Vitali Milman]]<ref>{{harvtxt|Milman|1986}}</ref> that provides a reverse inequality to the famous [[Brunn&ndash;Minkowski inequality]] for [[convex body|convex bodies]] in ''n''-[[dimension]]al [[Euclidean space]] '''R'''<sup>''n''</sup>. Namely, it bounds the volume of the [[Minkowski sum]] of two bodies from above in terms of the volumes of the bodies.\n\n==Introduction==\n\nLet ''K'' and ''L'' be convex bodies in '''R'''<sup>''n''</sup>. The Brunn&ndash;Minkowski inequality states that\n\n:<math> \\mathrm{vol}(K+L)^{1/n} \\geq \\mathrm{vol}(K)^{1/n} + \\mathrm{vol}(L)^{1/n}~,</math>\n\nwhere vol denotes ''n''-dimensional [[Lebesgue measure]] and the + on the left-hand side denotes Minkowski addition.\n\nIn general, no reverse bound is possible, since one can find convex bodies ''K'' and ''L'' of unit volume so that the volume of their Minkowski sum is arbitrarily large. Milman's theorem states that one can replace one of the bodies by its image under a properly chosen volume-preserving [[linear map]] so that the left-hand side of the Brunn&ndash;Minkowski inequality is bounded by a constant multiple of the right-hand side.\n\nThe result is one of the main structural theorems in the local theory of [[Banach spaces]].<ref>{{harvtxt|Pisier|1989}}</ref>\n\n==Statement of the inequality==\n\nThere is a constant ''C'', independent of ''n'', such that for any two centrally symmetric convex bodies ''K'' and ''L'' in '''R'''<sup>''n''</sup>, there are volume-preserving linear maps ''φ'' and ''ψ'' from '''R'''<sup>''n''</sup> to itself such that for any real numbers ''s'',&nbsp;''t''&nbsp;>&nbsp;0\n\n:<math>\\mathrm{vol} ( s \\, \\varphi K + t \\, \\psi L )^{1/n} \\leq C \\left( s\\, \\mathrm{vol} ( \\varphi K )^{1/n} + t\\, \\mathrm{vol} ( \\psi L )^{1/n} \\right)~.</math>\n\nOne of the maps may be chosen to be the identity.<ref>{{harvtxt|Pisier|1989}}</ref>\n\n==Notes==\n{{Reflist}}\n\n==References==\n\n* {{cite journal|\nmr=0827101|\nlast=Milman|\nfirst=Vitali D.|\ntitle=Inégalité de Brunn-Minkowski inverse et applications à la théorie locale des espaces normés. [An inverse form of the Brunn-Minkowski inequality, with applications to the local theory of normed spaces] |\njournal=Comptes Rendus de l'Académie des Sciences, Série I|\nvolume= 302|\nyear=1986|\nissue=1|\npages=25&ndash;28|ref=harv}}\n\n* {{cite book|\nmr=1036275|\nlast=Pisier|\nfirst= Gilles|\ntitle=The volume of convex bodies and Banach space geometry|\nseries=Cambridge Tracts in Mathematics|volume=94|\npublisher=Cambridge University Press|\nlocation=Cambridge|\nyear=1989|\nisbn=0-521-36465-5|ref=harv}}\n\n{{DEFAULTSORT:Milman's reverse Brunn-Minkowski inequality}}\n[[Category:Euclidean geometry]]\n[[Category:Geometric inequalities]]\n[[Category:Asymptotic geometric analysis]]"
    },
    {
      "title": "Minkowski's first inequality for convex bodies",
      "url": "https://en.wikipedia.org/wiki/Minkowski%27s_first_inequality_for_convex_bodies",
      "text": "In [[mathematics]], '''Minkowski's first inequality for convex bodies''' is a [[geometrical]] result due to the [[Germany|German]] [[mathematician]] [[Hermann Minkowski]]. The inequality is closely related to the [[Brunn–Minkowski inequality]] and the [[isoperimetric inequality]].\n\n==Statement of the inequality==\n\nLet ''K'' and ''L'' be two ''n''-[[dimension]]al [[convex body|convex bodies]] in ''n''-dimensional [[Euclidean space]] '''R'''<sup>''n''</sup>. Define a quantity ''V''<sub>1</sub>(''K'',&nbsp;''L'') by\n\n:<math>n V_{1} (K, L) = \\lim_{\\varepsilon \\downarrow 0} \\frac{V (K + \\varepsilon L) - V(K)}{\\varepsilon},</math>\n\nwhere ''V'' denotes the ''n''-dimensional [[Lebesgue measure]] and + denotes the [[Minkowski sum]]. Then\n\n:<math>V_{1} (K, L) \\geq V(K)^{(n - 1) / n} V(L)^{1 / n},</math>\n\nwith equality [[if and only if]] ''K'' and ''L'' are [[Homothetic transformation|homothetic]], i.e. are equal up to [[translation (geometry)|translation]] and [[dilation (metric space)|dilation]].\n\n==Remarks==\n\n* ''V''<sub>1</sub> is just one example of a class of quantities known as ''[[mixed volume]]s''.\n* If ''L'' is the ''n''-dimensional [[unit ball]] ''B'', then ''n''&nbsp;''V''<sub>1</sub>(''K'',&nbsp;''B'') is the (''n''&nbsp;&minus;&nbsp;1)-dimensional surface measure of ''K'', denoted ''S''(''K'').\n\n==Connection to other inequalities==\n===The Brunn–Minkowski inequality===\n\nOne can show that the Brunn–Minkowski inequality for convex bodies in '''R'''<sup>''n''</sup> implies Minkowski's first inequality for convex bodies in '''R'''<sup>''n''</sup>, and that equality in the Brunn–Minkowski inequality implies equality in Minkowski's first inequality.\n\n===The isoperimetric inequality===\n\nBy taking ''L''&nbsp;=&nbsp;''B'', the ''n''-dimensional unit ball, in Minkowski's first inequality for convex bodies, one obtains the isoperimetric inequality for convex bodies in '''R'''<sup>''n''</sup>: if ''K'' is a convex body in '''R'''<sup>''n''</sup>, then\n\n:<math>\\left( \\frac{V(K)}{V(B)} \\right)^{1 / n} \\leq \\left( \\frac{S(K)}{S(B)} \\right)^{1 / (n - 1)},</math>\n\nwith equality if and only if ''K'' is a ball of some radius.\n\n==References==\n\n* {{cite journal | last=Gardner | first=Richard J. | title=The Brunn–Minkowski inequality | journal=Bull. Amer. Math. Soc. (N.S.) | volume=39 | issue=3 | year=2002 | pages=355&ndash;405 (electronic) | doi=10.1090/S0273-0979-02-00941-2 }}\n\n[[Category:Calculus of variations]]\n[[Category:Geometric inequalities]]\n[[Category:Normed spaces]]"
    },
    {
      "title": "Myers's theorem",
      "url": "https://en.wikipedia.org/wiki/Myers%27s_theorem",
      "text": "The '''Myers theorem,''' also known as the '''Bonnet–Myers theorem''', is a classical theorem in [[Riemannian geometry]].  The strong form was proven by [[Sumner Byron Myers]].  The theorem states that if [[Ricci curvature]] of an  \n''n''-dimensional  [[complete space|complete]] Riemannian manifold ''M'' is bounded below by (''n''&nbsp;&minus;&nbsp;1)''k''&nbsp;>&nbsp;0, then its diameter is at most π/{{radic|''k''}}. In particular, this shows that any such ''M'' is necessarily compact.  A weaker result, due to [[Ossian Bonnet]], has the same conclusion but under the stronger assumption that the [[sectional curvature]]s is bounded below by ''k''.\n\nMoreover, if the diameter is equal to π/{{radic|''k''}}, then the manifold is [[Isometry|isometric]] to a sphere of a constant [[sectional curvature]] ''k''.  This ''rigidity result'' is due to {{harvtxt|Cheng|1975}}, and is often known as '''Cheng's theorem'''.\n\nThis result also holds for the [[universal cover]] of such a Riemannian manifold, in particular both ''M'' and its cover are compact, so the cover is finite-sheeted and ''M'' has finite [[fundamental group]].\n\n== See also ==\n*[[Gromov's compactness theorem (geometry)]]\n\n==References==\n*{{Citation | doi=10.1007/BF01214381 | last1=Cheng | first1=Shiu Yuen | title=Eigenvalue comparison theorems and its geometric applications |mr=0378001 | year=1975 | journal=[[Mathematische Zeitschrift]] | issn=0025-5874 | volume=143 | issue=3 | pages=289&ndash;297}}\n* {{citation|first=M. P.|last=do Carmo|authorlink=Manfredo do Carmo |title=Riemannian Geometry|publisher=Birkhäuser|publication-place=Boston, Mass.|year=1992 |isbn=0-8176-3490-8 }}\n* {{citation|doi=10.1215/S0012-7094-41-00832-3|first=S. B.|last=Myers|title=Riemannian manifolds with positive mean curvature|journal=Duke Mathematical Journal|volume=8|issue=2|year=1941|pages=401&ndash;404}}\n\n[[Category:Geometric inequalities]]\n[[Category:Theorems in Riemannian geometry]]"
    },
    {
      "title": "Ono's inequality",
      "url": "https://en.wikipedia.org/wiki/Ono%27s_inequality",
      "text": "In [[mathematics]], '''Ono's inequality''' is a [[theorem]] about [[triangle]]s in the [[Euclidean plane]]. In its original form, as [[conjecture]]d by T. Ono in 1914, the inequality is actually false; however, the statement is true for [[acute triangle]]s and [[right triangle]]s, as shown by F. Balitrand in 1916.\n\n==Statement of the inequality==\n\nConsider an acute or right triangle in the Euclidean plane with side lengths ''a'', ''b'' and ''c'' and area ''A''. Then\n\n:<math>27 (b^2 + c^2 - a^2)^2 (c^2 + a^2 - b^2)^2 (a^2 + b^2 - c^2)^2 \\leq (4 A)^6.</math>\n\nThis inequality fails for general triangles (to which Ono's original conjecture applied), as shown by the [[counterexample]]  <math>a=2, \\, \\, b=3, \\, \\, c=4, \\, \\, A=3\\sqrt{15}/4.</math>\n\nThe inequality holds with equality in the case of an [[equilateral triangle]], in which up to [[similarity (geometry)|similarity]] we have sides <math>1,1,1</math> and area <math>\\sqrt{3}/4.</math>\n\n==See also==\n*[[List of triangle inequalities]]\n\n==References==\n\n* {{cite journal\n| last = Balitrand\n| first = F.\n| title = Problem 4417\n| journal = Intermed. Math.\n| volume = 23\n| pages = 86–87\n| year = 1916\n| JFM = 46.0859.06\n}}\n* {{cite journal\n| last = Ono\n| first = T.\n| title = Problem 4417\n| journal = Intermed. Math.\n| volume = 21\n| pages = 146\n| year = 1914\n}}\n* {{cite journal\n| last = Quijano\n| first = G.\n| title = Problem 4417\n| journal = Intermed. Math.\n| volume = 22\n| pages = 66\n| year = 1915\n}}\n\n==External links==\n\n* {{MathWorld|urlname=OnoInequality|title=Ono inequality}}\n\n{{Disproved conjectures}}\n\n[[Category:Disproved conjectures]]\n[[Category:Triangle geometry]]\n[[Category:Geometric inequalities]]"
    },
    {
      "title": "Pedoe's inequality",
      "url": "https://en.wikipedia.org/wiki/Pedoe%27s_inequality",
      "text": "In [[geometry]], '''Pedoe's inequality''' (also '''Neuberg–Pedoe inequality'''), named after [[Daniel Pedoe]] (1910–1998) and [[Joseph Jean Baptiste Neuberg]] (1840–1926), states that if ''a'', ''b'', and ''c'' are the lengths of the sides of a [[triangle]] with area ''&fnof;'', and ''A'', ''B'', and ''C'' are the lengths of the sides of a triangle with area  ''F'', then\n\n:<math>A^2(b^2+c^2-a^2)+B^2(a^2+c^2-b^2)+C^2(a^2+b^2-c^2)\\geq 16Ff,\\,</math>\n\nwith equality [[if and only if]] the two triangles are [[similarity (geometry)|similar]] with pairs of [[corresponding sides]] (''A, a''), (''B, b''), and (''C, c'').\n\nThe expression on the left is not only symmetric under any of the six permutations of the set {&nbsp;(''A'',&nbsp;''a''),&nbsp;(''B'',&nbsp;''b''),&nbsp;(''C'',&nbsp;''c'')&nbsp;} of pairs, but also&mdash;perhaps not so obviously&mdash;remains the same if ''a'' is interchanged with ''A'' and ''b'' with ''B'' and ''c'' with&nbsp;''C''.  In other words, it is a symmetric function of the pair of triangles.\n\nPedoe's inequality is a generalization of [[Weitzenböck's inequality]], which is the case in which one of the triangles is [[equilateral triangle|equilateral]].\n\nPedoe discovered the inequality in 1941 and published it subsequently in several articles. Later he learned that the inequality was already known in the 19th century to Neuberg, who however did not prove that the equality implies the similarity of the two triangles.\n\n==See also==\n*[[List of triangle inequalities]]\n\n==References==\n*[[Daniel Pedoe]]: ''An Inequality Connecting Any Two Triangles''. The Mathematical Gazette, Vol. 25, No. 267 (Dec., 1941), pp. 310-311 ([https://www.jstor.org/stable/3606570 JSTOR])\n*Daniel Pedoe: ''A Two-Triangle Inequality''. The [[American Mathematical Monthly]], volume 70, number 9, page 1012, November, 1963.\n*Daniel Pedoe: ''An Inequality for Two Triangles''. [[Proceedings of the Cambridge Philosophical Society]], volume 38, part 4, page 397, 1943.\n* Claudi Alsina, Roger B. Nelsen: ''When Less is More: Visualizing Basic Inequalities''. MAA, 2009, {{isbn|978-0-88385-342-9}}, p. [https://books.google.de/books?id=U1ovBsSRNscC&pg=PA108 108]\n* D.S. Mitrinović, Josip Pečarić: ''About the Neuberg-Pedoe and the Oppenheim inequalities''. Journal of Mathematical Analysis and Applications 129(1):196–210 · January 1988 ([https://www.researchgate.net/publication/256246330_About_the_Neuberg-Pedoe_and_the_Oppenheim_inequalities online copy])\n\n\n[[Category:Geometric inequalities]]\n[[Category:Triangle geometry]]"
    },
    {
      "title": "Prékopa–Leindler inequality",
      "url": "https://en.wikipedia.org/wiki/Pr%C3%A9kopa%E2%80%93Leindler_inequality",
      "text": "In [[mathematics]], the '''Prékopa–Leindler inequality''' is an [[integral]] [[inequality (mathematics)|inequality]] closely related to the [[reverse Young's inequality]], the [[Brunn–Minkowski inequality]] and a number of other important and classical inequalities in [[mathematical analysis|analysis]]. The result is named after the [[Hungary|Hungarian]] [[mathematician]]s [[András Prékopa]] and [[László Leindler]].\n\n==Statement of the inequality==\nLet 0&nbsp;<&nbsp;''λ''&nbsp;<&nbsp;1 and let ''f'', ''g'', ''h''&nbsp;:&nbsp;'''R'''<sup>''n''</sup>&nbsp;→&nbsp;[0,&nbsp;+∞) be non-[[negative number|negative]] [[real number|real-valued]] [[measurable function]]s defined on ''n''-dimensional [[Euclidean space]] '''R'''<sup>''n''</sup>. Suppose that these functions satisfy\n\n{{NumBlk|:|<math>h \\left( (1-\\lambda)x + \\lambda y \\right) \\geq f(x)^{1 - \\lambda} g(y)^\\lambda </math>|{{EquationRef|1}}}}\n\nfor all ''x'' and ''y'' in '''R'''<sup>''n''</sup>. Then\n\n:<math>\\| h\\|_{1} := \\int_{\\mathbb{R}^n} h(x) \\, \\mathrm{d} x \\geq \\left( \\int_{\\mathbb{R}^n} f(x) \\, \\mathrm{d} x \\right)^{1 -\\lambda} \\left( \\int_{\\mathbb{R}^n} g(x) \\, \\mathrm{d} x \\right)^\\lambda =: \\| f\\|_1^{1 -\\lambda} \\| g\\|_1^\\lambda. </math>\n\n==Essential form of the inequality==\nRecall that the [[essential supremum]] of a measurable function ''f''&nbsp;:&nbsp;'''R'''<sup>''n''</sup>&nbsp;→&nbsp;'''R''' is defined by\n\n:<math>\\mathop{\\mathrm{ess\\,sup}}_{x \\in \\mathbb{R}^{n}} f(x) = \\inf \\left\\{ t \\in [- \\infty, + \\infty] \\mid f(x) \\leq t \\text{ for almost all } x \\in \\mathbb{R}^{n} \\right\\}.</math>\n\nThis notation allows the following ''essential form'' of the Prékopa–Leindler inequality: let 0&nbsp;&lt;&nbsp;''λ''&nbsp;<&nbsp;1 and let ''f'', ''g''&nbsp;∈&nbsp;''L''<sup>1</sup>('''R'''<sup>''n''</sup>;&nbsp;[0,&nbsp;+∞)) be non-negative [[absolutely integrable function|absolutely integrable]] functions. Let\n\n:<math>s(x) = \\mathop{\\mathrm{ess\\,sup}}_{y \\in \\mathbb{R}^n} f \\left( \\frac{x - y}{1 - \\lambda} \\right)^{1 - \\lambda} g \\left( \\frac{y}{\\lambda} \\right)^\\lambda.</math>\n\nThen ''s'' is measurable and\n\n:<math>\\| s \\|_1 \\geq \\| f \\|_1^{1 - \\lambda} \\| g \\|_1^\\lambda.</math>\n\nThe essential supremum form was given in.<ref>{{cite journal | authors = [[Herm Jan Brascamp]] and [[Elliott H. Lieb]] | title = On extensions of the Brunn–Minkowski and Prekopa–Leindler theorems, including inequalities for log concave functions and with an application to the diffusion equation | journal = Journal of Functional Analysis | volume = 22 |issue=4 | pages = 366–389 | year = 1976 |doi=10.1016/0022-1236(76)90004-5 }}</ref>  Its use can change the left side of the inequality. For example, a function ''g'' that takes the value 1 at exactly one point will not usually yield a zero left side in the \"non-essential sup\" form but it will always yield a zero left side in the \"essential sup\" form.\n\n==Relationship to the Brunn–Minkowski inequality==\nIt can be shown that the usual Prékopa–Leindler inequality implies the [[Brunn–Minkowski inequality]] in the following form: if 0&nbsp;&lt;&nbsp;''λ''&nbsp;&lt;&nbsp;1 and ''A'' and ''B'' are [[bounded set|bounded]], [[measurable set|measurable subsets]] of '''R'''<sup>''n''</sup> such that the [[Minkowski sum]] (1&nbsp;&minus;&nbsp;''λ'')''A''&nbsp;+&nbsp;λ''B'' is also measurable, then\n\n:<math>\\mu \\left( (1 - \\lambda) A + \\lambda B \\right) \\geq \\mu (A)^{1 - \\lambda} \\mu (B)^{\\lambda},</math>\n\nwhere ''μ'' denotes ''n''-dimensional [[Lebesgue measure]]. Hence, the Prékopa–Leindler inequality can also be used<ref>Gardner, Richard J. (2002). \"The Brunn–Minkowski inequality\". Bull. Amer. Math. Soc. (N.S.) 39 (3): pp. 355–405 (electronic). doi:10.1090/S0273-0979-02-00941-2. {{issn|0273-0979}}.</ref> to prove the Brunn–Minkowski inequality in its more familiar form: if 0&nbsp;&lt;&nbsp;''λ''&nbsp;<&nbsp;1 and ''A'' and ''B'' are non-[[empty set|empty]], [[bounded set|bounded]], [[measurable set|measurable subsets]] of '''R'''<sup>''n''</sup> such that (1&nbsp;&minus;&nbsp;''λ'')''A''&nbsp;+&nbsp;λ''B'' is also measurable, then\n\n:<math>\\mu \\left( (1 - \\lambda) A + \\lambda B \\right)^{1 / n} \\geq (1 - \\lambda) \\mu (A)^{1 / n} + \\lambda \\mu (B)^{1 / n}.</math>\n\n==Applications in probability and statistics==\nThe Prékopa–Leindler inequality is useful in the theory of [[Logarithmically concave function|log-concave distributions]], as it can be used to show that log-concavity is preserved by [[Marginal distribution|marginalization]] and [[Independence (probability)|independent]] summation of log-concave distributed random variables. Suppose that ''H''(''x'',''y'') is a log-concave distribution for (''x'',''y'') ∈ '''R'''<sup>''m''</sup> × '''R'''<sup>''n''</sup>, so that by definition we have\n\n{{NumBlk|:|<math>H \\left( (1 - \\lambda)(x_1,y_1) + \\lambda (x_2,y_2) \\right) \\geq H(x_1,y_1)^{1 - \\lambda}  H(x_2,y_2)^{\\lambda},</math>|{{EquationRef|2}}}}\n\nand let ''M''(''y'') denote the marginal distribution obtained by integrating over ''x'':\n\n:<math>M(y) = \\int_{\\mathbb{R}^m} H(x,y) \\, dx.</math>\n\nLet ''y''<sub>1</sub>, ''y''<sub>2</sub> ∈ '''R'''<sup>''n''</sup> and 0&nbsp;<&nbsp;''λ''&nbsp;<&nbsp;1 be given. Then equation ({{EquationNote|2}}) satisfies condition ({{EquationNote|1}}) with ''h''(''x'') = ''H''(''x'',(1&nbsp;&minus;&nbsp;''λ'')y<sub>1</sub> + ''λy''<sub>2</sub>),  ''f''(''x'') = ''H''(''x'',''y''<sub>1</sub>) and ''g''(''x'') = ''H''(''x'',''y''<sub>2</sub>), so the Prékopa–Leindler inequality applies. It can be written in terms of ''M'' as\n\n:<math>M((1-\\lambda) y_1 + \\lambda y_2) \\geq M(y_1)^{1-\\lambda} M(y_2)^\\lambda,</math>\n\nwhich is the definition of log-concavity for ''M''.\n\nTo see how this implies the preservation of log-convexity by independent sums, suppose that ''X'' and ''Y'' are independent random variables with log-concave distribution. Since the product of two log-concave functions is log-concave, the joint distribution of (''X'',''Y'') is also log-concave. Log-concavity is preserved by affine changes of coordinates, so the distribution of (''X''&nbsp;+&nbsp;''Y'',&nbsp;''X''&nbsp;−&nbsp;''Y'') is log-concave as well. Since the distribution of ''X+Y'' is a marginal over the joint distribution of (''X''&nbsp;+&nbsp;''Y'',&nbsp;''X''&nbsp;−&nbsp;''Y''), we conclude that ''X''&nbsp;+&nbsp;''Y'' has a log-concave distribution.\n\n==Notes==\n<references/>\n\n==References==\n* {{cite journal \n| last=Gardner \n| first=Richard J. \n| title=The Brunn–Minkowski inequality \n| journal=Bull. Amer. Math. Soc. (N.S.) \n| volume=39 \n| issue=3 \n| year=2002 \n| pages =355–405 (electronic) \n| issn = 0273-0979 \n| url = http://www.ams.org/bull/2002-39-03/S0273-0979-02-00941-2/S0273-0979-02-00941-2.pdf \n| doi=10.1090/S0273-0979-02-00941-2\n}}\n\n* {{ cite journal\n| last=Prékopa\n| first=András\n| title=Logarithmic concave measures with application to stochastic programming\n| journal=[[Acta Scientiarum Mathematicarum|Acta Sci. Math.]]\n| volume=32\n| year=1971\n| pages=301–316\n| url=http://rutcor.rutgers.edu/~prekopa/SCIENT1.pdf\n}}\n\n* {{ cite journal\n| last=Prékopa\n| first=András\n| title=On logarithmic concave measures and functions\n| journal=[[Acta Scientiarum Mathematicarum|Acta Sci. Math.]]\n| volume=34\n| year=1973\n| pages=335–343\n| url=http://rutcor.rutgers.edu/~prekopa/SCIENT2.pdf\n}}\n\n{{DEFAULTSORT:Prekopa-Leindler Inequality}}\n[[Category:Geometric inequalities]]\n[[Category:Integral geometry]]\n[[Category:Real analysis]]\n[[Category:Theorems in analysis]]"
    },
    {
      "title": "Ptolemy's inequality",
      "url": "https://en.wikipedia.org/wiki/Ptolemy%27s_inequality",
      "text": "[[Image:Ptolemy Inequality.svg|thumb|Four points and their six distances. The points are not co-circular, so Ptolemy's inequality is strict for these points.]]\nIn [[Euclidean geometry]], '''Ptolemy's inequality''' relates the six distances determined by four points in the [[Euclidean plane|plane]] or in a higher-dimensional space. It states that, for any four points {{mvar|A}}, {{mvar|B}}, {{mvar|C}}, and {{mvar|D}}, the following [[Inequality (mathematics)|inequality]] holds:\n:<math>\\overline{AB}\\cdot \\overline{CD}+\\overline{BC}\\cdot \\overline{DA} \\ge \\overline{AC}\\cdot \\overline{BD}.</math>\nIt is named after the [[Roman Greece|Greek]] [[astronomy|astronomer]] and [[mathematics|mathematician]] [[Ptolemy]].\n\nThe four points can be ordered in any of three distinct ways (counting reversals as not distinct) to form three different [[quadrilateral]]s, for each of which the sum of the products of opposite sides is at least as large as the product of the diagonals. Thus the three product  terms in the inequality can be additively permuted to put any one of them on the right side of the inequality, so the three products of opposite sides or of diagonals of any one of the quadrilaterals must obey the [[triangle inequality]].<ref name=\"s40\">{{citation\n | last = Schoenberg | first = I. J. | authorlink = Isaac Jacob Schoenberg\n | doi = 10.2307/1968849\n | journal = Annals of Mathematics\n | mr = 0002903\n | pages = 715–726\n | series = Second Series\n | title = On metric arcs of vanishing Menger curvature\n | volume = 41\n | year = 1940}}.</ref>\n\nAs a special case, [[Ptolemy's theorem]] states that the inequality becomes an equality exactly when the four points lie in cyclic order on a [[circle]].\nThe inequality does not generalize from [[Euclidean space]]s to arbitrary [[metric spaces]]. The spaces where it remains valid are called the ''Ptolemaic spaces''; they include the [[inner product space]]s, [[Hadamard space]]s, and [[shortest path]] distances on [[Ptolemaic graph]]s.\n\n==Assumptions and derivation==\nPtolemy's inequality is often stated for a special case, in which the four points are the [[vertex (geometry)|vertices]] of a [[convex polygon|convex]] [[quadrilateral]], given in cyclic order.<ref>{{citation|title=The Cauchy-Schwarz Master Class: An Introduction to the Art of Mathematical Inequalities|series=MAA problem books|first=J. Michael|last=Steele|authorlink=J. Michael Steele|publisher=Cambridge University Press|year=2004|isbn=9780521546775|contribution-url=https://books.google.com/books?id=7GDyRMrlgDsC&pg=PA69|page=69|contribution=Exercise 4.6 (Ptolemy's Inequality)}}.</ref><ref>{{citation|title=When Less is More: Visualizing Basic Inequalities|volume=36|series=Dolciani Mathematical Expositions|first1=Claudi|last1=Alsina|first2=Roger B.|last2=Nelsen|publisher=Mathematical Association of America|year=2009|isbn=9780883853429|pages=82–83|contribution-url=https://books.google.com/books?id=U1ovBsSRNscC&pg=PA82|contribution=6.1 Ptolemy's inequality}}.</ref> However, the theorem applies more generally to any four points; it is not required that the quadrilateral they form be convex, simple, or even planar.\n\nFor points in the plane, Ptolemy's inequality can be derived from the [[triangle inequality]] by an [[Inversive geometry|inversion]] centered at one of the four points.<ref>{{harvtxt|Apostol|1967}} attributes the inversion-based proof to textbooks by R. A. Johnson (1929) and [[Howard Eves]] (1963).</ref><ref name=\"bmc\">{{citation|title=A Decade of the Berkeley Math Circle: The American Experience|volume=1|series=MSRI Mathematical Circles Library|editor1-first=Zvezdelina|editor1-last=Stankova|editor1-link=Zvezdelina Stankova|editor2-first=Tom|editor2-last=Rike|publisher=American Mathematical Society|year=2008|isbn=9780821846834|page=18|contribution-url=https://books.google.com/books?id=vix-AwAAQBAJ&pg=PA18|contribution=Problem 7 (Ptolemy's Inequality)}}.</ref> Alternatively, it can be derived by interpreting the four points as [[complex number]]s, using the complex number identity\n:<math>(A-B)(C-D)+(B-C)(A-D)=(A-C)(B-D)</math>\nto construct a triangle whose side lengths are the products of sides of the given quadrilateral, and  applying the triangle inequality to this triangle.<ref name=\"apostol\">{{citation\n | last = Apostol | first = Tom M. | authorlink = Tom M. Apostol\n | journal = Mathematics Magazine\n | mr = 0225213\n | pages = 233–235\n | title = Ptolemy's inequality and the chordal metric\n | volume = 40\n | year = 1967}}.</ref> One can also view the points as belonging to the complex [[projective line]], express the inequality in the form that the [[absolute value]]s of two [[cross-ratio]]s of the points sum to at least one, and deduce this from the fact that the cross-ratios themselves add to exactly one.<ref>{{citation|title=Geometry: Ancient and Modern|first=John R.|last=Silvester|publisher=Oxford University Press|year=2001|isbn=9780198508250|contribution-url=https://books.google.com/books?id=VtH_QG6scSUC&pg=PA229|page=229|contribution=Proposition 9.10 (Ptolemy's theorem)}}.</ref>\n\nA proof of the inequality for points in three-dimensional space can be reduced to the planar case, by observing that for any non-planar quadrilateral, it is possible to rotate one of the points around the diagonal until the quadrilateral becomes planar, increasing the other diagonal's length and keeping the other five distances constant.<ref name=\"apostol\"/> In spaces of higher dimension than three, any four points lie in a three-dimensional subspace, and the same three-dimensional proof can be used.\n\n==Four concyclic points==\n{{main|Ptolemy's theorem}}\nFor four [[concyclic points|points in order around a circle]], Ptolemy's inequality becomes an equality, known as [[Ptolemy's theorem]]:\n:<math>\\overline{AB}\\cdot \\overline{CD}+\\overline{BC}\\cdot \\overline{DA} = \\overline{AC}\\cdot \\overline{BD}.</math>\nIn the inversion-based proof of Ptolemy's inequality, transforming four co-circular points by an inversion centered at one of them causes the other three to become collinear, so the triangle equality for these three points (from which Ptolemy's inequality may be derived) also becomes an equality.<ref name=\"bmc\"/> For any other four points, Ptolemy's inequality is strict.\n\n==In general metric spaces==\n[[File:Circle graph C4.svg|thumb|upright=0.75|A [[cycle graph]] in which the distances disobey Ptolemy's inequality]]\nPtolemy's inequality holds more generally in any [[inner product space]],<ref name=\"s40\"/><ref name=\"nls\"/> and whenever it is true for a real [[normed vector space]], that space must be an inner product space.<ref name=\"nls\">{{citation|title=Introduction to the Analysis of Normed Linear Spaces|volume=13|series=Australian Mathematical Society lecture series|first=J. R.|last=Giles|publisher=Cambridge University Press|year=2000|isbn=9780521653756|page=47|contribution=Exercise 12|contribution-url=https://books.google.com/books?id=VVeV6EKimjQC&pg=PA47}}.</ref><ref>{{citation\n | last = Schoenberg | first = I. J. | authorlink = Isaac Jacob Schoenberg\n | doi = 10.2307/2031742\n | journal = Proceedings of the American Mathematical Society\n | mr = 0052035\n | pages = 961–964\n | title = A remark on M. M. Day's characterization of inner-product spaces and a conjecture of L. M. Blumenthal\n | volume = 3\n | year = 1952}}.</ref>\n \nFor other types of [[metric space]], the inequality may or may not be valid. A space in which it holds is called ''Ptolemaic''.\nFor instance, consider the four-vertex [[cycle graph]], shown in the figure, with all edge lengths equal to 1. The sum of the products of opposite sides is 2. However, diagonally opposite [[vertex (graph theory)|vertices]] are at distance 2 from each other, so the product of the diagonals is 4, bigger than the sum of products of sides. Therefore, the [[shortest path]] distances in this graph are not Ptolemaic.\nThe graphs in which the distances obey Ptolemy's inequality are called the [[Ptolemaic graph]]s and have a restricted structure compared to arbitrary graphs; in particular, they disallow [[induced cycle]]s of length greater than three, such as the one shown.<ref>{{citation\n | last = Howorka | first = Edward\n | doi = 10.1002/jgt.3190050314\n | issue = 3\n | journal = Journal of Graph Theory\n | mr = 625074\n | pages = 323–331\n | title = A characterization of Ptolemaic graphs\n | volume = 5\n | year = 1981}}.</ref>\n\nThe Ptolemaic spaces include all [[CAT(k) space|CAT(0) spaces]] and in particular all [[Hadamard space]]s. If a complete [[Riemannian manifold]] is Ptolemaic, it is necessarily a Hadamard space.<ref>{{citation\n | last1 = Buckley | first1 = S. M.\n | last2 = Falk | first2 = K.\n | last3 = Wraith | first3 = D. J.\n | doi = 10.1017/S0017089509004984\n | issue = 2\n | journal = Glasgow Mathematical Journal\n | mr = 2500753\n | pages = 301–314\n | title = Ptolemaic spaces and CAT(0)\n | volume = 51\n | year = 2009}}.</ref>\n\n==References==\n{{reflist|30em}}\n\n[[Category:Geometric inequalities]]"
    },
    {
      "title": "Pu's inequality",
      "url": "https://en.wikipedia.org/wiki/Pu%27s_inequality",
      "text": "[[Image:Steiner%27s_Roman_Surface.gif|thumb|An animation of the [[Roman surface]] representing '''RP'''<sup>2</sup> in '''R'''<sup>3</sup>]]\n\nIn [[differential geometry]], '''Pu's inequality''', proved by [[Pao Ming Pu]], relates the [[area]] of an arbitrary [[Riemannian manifold|Riemannian surface]] homeomorphic to the [[real projective plane]] with the [[length|lengths]] of the closed curves contained in it.\n\n==Statement==\nA student of [[Charles Loewner]], Pu proved in his 1950 thesis {{harv|Pu|1952}} that every Riemannian surface <math> M </math> homeomorphic to the [[real projective plane]] satisfies the inequality\n\n: <math> \\operatorname{Area}(M) \\geq \\frac{2}{\\pi} \\operatorname{Systole}(M)^2 ,</math>\n\nwhere <math> \\operatorname{Systole}(M) </math> is the [[Systolic geometry|systole]] of <math> M </math>.\nThe equality is attained precisely when the metric has constant [[Gaussian curvature]].\n\nIn other words, if all [[noncontractible loops]] in <math> M </math> have length at least <math> L </math>, then <math> \\operatorname{Area}(M) \\geq \\frac{2}{\\pi} L^2, </math> and the equality holds if and only if <math> M </math> is obtained from an Euclidean sphere of radius <math> r=L/\\pi </math> by identifying each point with its antipodal.\n\nPu's paper also stated for the first time [[Loewner's torus inequality|Loewner's inequality]], a similar result for Riemannian metrics on the [[torus]].\n\n== Proof ==\nPu's original proof relies on the [[uniformization theorem]] and employs an averaging argument, as follows.\n\nBy uniformization, the Riemannian surface <math> (M,g) </math> is [[conformal map|conformally diffeomorphic]] to a round projective plane. This means that we may assume that the surface <math> M </math> is obtained from the Euclidean unit sphere <math> S^2 </math> by identifying antipodal points, and the Riemannian length element at each point <math> x </math> is\n:<math> \\mathrm{dLength} = f(x) \\mathrm{dLength}_{\\text{Euclidean}}, </math>\nwhere <math> \\mathrm{dLength}_{\\text{Euclidean}} </math> is the Euclidean length element and the function <math> f: S^2\\to(0,+\\infty) </math>, called the '''conformal factor''', satisfies <math> f(-x)=f(x) </math>.\n\nMore precisely, the universal cover of <math> M </math> is <math> S^2 </math>, a loop <math>\\gamma\\subseteq M </math> is noncontractible if and only if its lift <math> \\widetilde\\gamma\\subseteq S^2</math> goes from one point to its opposite, and the length of each curve <math>\\gamma</math> is\n\n:<math> \\operatorname{Length}(\\gamma)=\\int_{\\widetilde\\gamma} f \\, \\mathrm{dLength}_{\\text{Euclidean}}.</math>\n\nSubject to the restriction that each of these lengths is at least <math> L </math>, we want to find an <math> f </math> that minimizes the \n:<math> \\operatorname{Area}(M,g)=\\int_{S^2_+} f(x)^2\\,\\mathrm{dArea}_{\\text{Euclidean}}(x),</math>\nwhere <math> S^2_+ </math> is the upper half of the sphere.\n\nA key observation is that if we average several different <math> f_i </math> that satisfy the length restriction and have the same area <math> A </math>, then we obtain a better conformal factor <math> f_{\\text{new}} = \\frac{1}{n} \\sum_{0\\leq i<n} f_i</math>, that also satisfies the length restriction and has\n:<math> \\operatorname{Area}(M,g_{\\text{new}}) = \\int_{S^2_+}\\left(\\frac 1n\\sum_i f_i(x)\\right)^2\\mathrm{dArea}_{\\text{Euclidean}}(x) </math>\n:<math> \\qquad\\qquad \\leq \\frac{1}{n}\\sum_i\\left(\\int_{S^2_+} f_i(x)^2\\mathrm{dArea}_{\\text{Euclidean}}(x)\\right) = A,</math>\nand the inequality is strict unless the functions <math> f_i </math> are equal.\n\nA way to improve any non-constant <math> f </math> is to obtain the different functions <math> f_i </math> from <math> f </math> using [[rotations]] of the sphere <math> R_i\\in SO^3 </math>, defining <math> f_i(x)=f(R_i(x))</math>. If we [[Haar measure|average over all possible rotations]], then we get an <math> f_{\\text{new}} </math> that is constant over all the sphere. We can further reduce this constant to minimum value <math> r=\\frac L\\pi </math> allowed by the length restriction. Then we obtain the obtain the unique metric that attains the minimum area <math> 2\\pi r^2 = \\frac 2\\pi L^2 </math>.\n\n==Reformulation==\n\nAlternatively, every metric on the sphere <math>S^2</math> invariant under the antipodal map admits a pair of opposite points <math>p,q\\in S^2</math> at Riemannian distance <math>d=d(p,q)</math> satisfying <math>d^2 \\leq \\frac{\\pi}{4} \\operatorname{area} (S^2).</math>\n\nA more detailed explanation of this viewpoint may be found at the page [[Introduction to systolic geometry]].\n\n==Filling area conjecture==\n\nAn alternative formulation of Pu's inequality is the following.  Of all possible fillings of the [[Riemannian circle]] of length <math>2\\pi</math> by a <math>2</math>-dimensional disk with the strongly isometric property, the round [[Sphere|hemisphere]] has the least area.\n\nTo explain this formulation, we start with the observation that the equatorial circle of the unit <math>2</math>-sphere <math>S^2 \\subset \\mathbb R^3</math> is a [[Riemannian circle]] <math>S^1</math> of length <math>2\\pi</math>.  More precisely, the Riemannian distance function\nof <math>S^1</math> is induced from the ambient Riemannian distance on the sphere.  Note that this property is not satisfied by the standard imbedding of the unit circle in the Euclidean plane.  Indeed, the Euclidean distance between a pair of opposite points of the circle is\nonly <math>2</math>, whereas in the Riemannian circle it is <math>\\pi</math>.\n\nWe consider all fillings of <math>S^1</math> by a <math>2</math>-dimensional disk, such that the metric induced by the inclusion of the circle as the boundary of the disk is the Riemannian\nmetric of a circle of length <math>2\\pi</math>.  The inclusion of the circle as the boundary is then called a strongly isometric imbedding of the circle.\n\nGromov [[Filling area conjecture|conjectured]] that the round hemisphere gives the \"best\" way of filling the circle even when the filling surface is allowed to have positive genus {{harv|Gromov|1983}}.\n\n==Isoperimetric inequality==\n\nPu's inequality bears a curious resemblance to the classical [[isoperimetry|isoperimetric inequality]] \n\n:<math> L^2 \\geq 4\\pi A </math>\n\nfor [[Jordan curve theorem|Jordan curves]] in the plane, where <math>L</math> is the length of the curve while <math>A</math> is the area of the region it bounds.  Namely, in both cases a 2-dimensional quantity (area) is bounded by (the square of) a 1-dimensional quantity (length).  However, the inequality goes in the opposite direction.  Thus, Pu's inequality can be thought of as an \n\"opposite\" isoperimetric inequality.\n\n==See also==\n*[[Filling area conjecture]]\n*[[Gromov's systolic inequality for essential manifolds]]\n*[[Gromov's inequality for complex projective space]]\n*[[Loewner's torus inequality]]\n*[[Systolic geometry]]\n*[[Systoles of surfaces]]\n\n==References==\n* {{cite journal | first=Mikhael | last=Gromov | title=Filling Riemannian manifolds | journal=[[J. Differential Geom.]] | volume=18 | year=1983 | pages=1-147 | issue=1 | url=http://projecteuclid.org/euclid.jdg/1214509283 | mr=697984 | ref=harv}}\n* {{cite encyclopedia | last=Gromov | first=Mikhael | chapter=Systoles and intersystolic inequalities | title=Actes de la Table Ronde de Géométrie Différentielle (Luminy, 1992) | trans-title=Proceedings of the Roundtable on Differential Geometry | pages=291-362 | series=Séminaires et Congrès | volume=1 | publisher=Soc. Math. France | location=Paris | year=1996 | editor1-first=Arthur L. | editor1-last=Besse | isbn=2-85629-047-7 | mr=1427752}}\n* {{cite book | last=Gromov | first=Misha | title=Metric structures for Riemannian and non-Riemannian spaces | origyear=1981 | others=With appendices by M. Katz, P. Pansu and S. Semmes. Translated from the French by Sean Michael Bates. | series=Progress in Mathematics | volume=152 | publisher=Birkhäuser Boston, Inc. | location=Boston, MA | year=1999 | mr=1699320 | isbn=0-8176-3898-9 }}\n* {{cite book | last=Katz | first=Mikhail G. | title=Systolic geometry and topology | others=With an appendix by J. Solomon. | series=Mathematical Surveys and Monographs | volume=137 | publisher=[[American Mathematical Society]] | year=2007 | isbn=978-0-8218-4177-8 | location=Providence, RI | mr=2292367 | doi=10.1090/surv/137}}\n* {{cite journal | last=Pu | first=Pao Ming | title=Some inequalities in certain nonorientable Riemannian manifolds | journal=[[Pacific J. Math.]] | volume=2 | year=1952 | pages=55&ndash;71 | mr=0048886 | url=http://projecteuclid.org/euclid.pjm/1103051942 | issue=1 | ref=harv}}\n\n{{Systolic geometry navbox}}\n\n[[Category:Riemannian geometry]]\n[[Category:Geometric inequalities]]\n[[Category:Differential geometry of surfaces]]\n[[Category:Systolic geometry]]"
    },
    {
      "title": "Riemannian Penrose inequality",
      "url": "https://en.wikipedia.org/wiki/Riemannian_Penrose_inequality",
      "text": "In mathematical [[general relativity]], the '''Penrose inequality''', first conjectured by Sir [[Roger Penrose]], estimates the mass of a [[spacetime]] in terms of the total area of its [[black holes]] and is a generalization of the [[positive mass theorem]].  The '''Riemannian Penrose inequality''' is an important special case.  Specifically, if (''M'',&nbsp;''g'') is an [[asymptotically flat]] Riemannian [[3-manifold]] with nonnegative [[scalar curvature]] and [[ADM mass]] ''m'', and ''A'' is the area of the outermost [[minimal surface]] (possibly with multiple [[Connected component (topology)|connected components]]), then the Riemannian Penrose inequality asserts\n\n: <math>m \\geq \\sqrt{\\frac{A}{16\\pi}}.</math>\n\nThis is purely a geometrical fact, and it corresponds to the case of a complete three-dimensional, [[space-like]], [[totally geodesic]] [[submanifold]]\nof a (3&nbsp;+&nbsp;1)-dimensional spacetime.  Such a submanifold is often called a time-symmetric initial data set for a spacetime.  The condition of (''M'',&nbsp;''g'') having nonnegative scalar curvature is equivalent to the spacetime obeying the [[dominant energy condition]].\n\nThis inequality was first proved by [[Gerhard Huisken]] and [[Tom Ilmanen]] in 1997 in the case where ''A'' is the area of the largest component of the outermost minimal surface.  Their proof relied on the machinery of weakly defined [[inverse mean curvature flow]], which they developed.  In 1999, [[Hubert Bray]] gave the first complete proof of the above inequality using a conformal [[geometric flow|flow]] of metrics.  Both of the papers were published in 2001.\n\n== Physical motivation ==\nThe original physical argument that led Penrose to conjecture such an inequality invoked the [[Hawking area theorem]] and the [[cosmic censorship hypothesis]].\n\n== Case of equality ==\nBoth the Bray and Huisken–Ilmanen proofs of the Riemannian Penrose inequality state that under the hypotheses, if\n\n: <math>m = \\sqrt{\\frac{A}{16\\pi}},</math>\nthen the manifold in question is isometric to a slice of the [[Schwarzschild spacetime]] outside of the outermost minimal surface.\n\n== Penrose conjecture ==\nMore generally, Penrose conjectured that an inequality as above should hold for spacelike submanifolds of spacetimes that are not necessarily time-symmetric. In this case, nonnegative scalar curvature is replaced with the [[dominant energy condition]], and one possibility is to replace the minimal surface condition  with an [[apparent horizon]] condition. Proving such an inequality remains an open problem in general relativity, called the '''Penrose conjecture.'''\n\n== In popular culture ==\n*In episode 6 of season 8 of the television sitcom ''[[The Big Bang Theory]]'', Dr. Sheldon Cooper claims to be in the process of solving the Penrose Conjecture while at the same time composing his Nobel Prize acceptance speech.\n\n== References ==\n* {{cite journal |last=Bray |first=H. |title=Proof of the Riemannian Penrose inequality using the positive mass theorem |journal=Journal of Differential Geometry |volume=59 |issue=2 |year=2001 |pages=177–267 |doi= 10.4310/jdg/1090349428|mr=1908823 |bibcode = 2001JDGeo..59..177B }}\n* {{cite arxiv |last=Bray |first=H. |last2=Chruściel |first2=P. |year=2003 |title=The Penrose Inequality |eprint=gr-qc/0312047 }}\n* {{Cite journal | last1=Huisken | first1=G. | last2=Ilmanen | first2=T. | title=The Riemannian Penrose inequality | doi=10.1155/S1073792897000664 |mr=1486695 | year=1997 | journal=International Mathematics Research Notices | issn=1073-7928 |volume=1997 | issue=20 | pages=1045–1058}}\n* {{cite journal |last=Huisken |first=G. |last2=Ilmanen |first2=T. |title=The inverse mean curvature flow and the Riemannian Penrose inequality |journal=Journal of Differential Geometry |volume=59 |issue=3 |year=2001 |pages=353–437 |mr=1916951 |doi=10.4310/jdg/1090349447}}\n\n[[Category:Riemannian geometry]]\n[[Category:Geometric inequalities]]\n[[Category:General relativity]]\n[[Category:Theorems in geometry]]"
    },
    {
      "title": "Symmetrization methods",
      "url": "https://en.wikipedia.org/wiki/Symmetrization_methods",
      "text": "In [[mathematics]] the '''symmetrization methods''' are algorithms of transforming a [[set (mathematics)|set]] [[Euclidean space|<math>A\\subset \\mathbb{R}^n</math>]] to a ball <math>B\\subset \\mathbb{R}^n</math> with equal volume <math>\\operatorname{vol}(B)=\\operatorname{vol}(A)</math> and centered at the origin. ''B'' is called the symmetrized version of ''A'', usually denoted <math>A^{*}</math>. These algorithms show up in solving the classical [[isoperimetric inequality]] problem, which asks: Given all two-dimensional shapes of a given area, which of them has the minimal [[perimeter]] (for details see [[Isoperimetric inequality]]). The conjectured answer was the disk and [[Jakob Steiner|Steiner]] in 1838 showed this to be true using the Steiner symmetrization method (described below). From this many other isoperimetric problems sprung and other symmetrization algorithms. For example, Rayleigh's conjecture is that the first [[eigenvalue]] of the [[Dirichlet problem]] is minimized for the ball (see [[Rayleigh–Faber–Krahn inequality]] for details). Another problem is that the Newtonian [[capacity of a set]] A is minimized by <math>A^{*}</math> and this was proved by Polya and G. Szego (1951) using circular symmetrization (described below).\n\n==Symmetrization==\nIf <math> \\Omega\\subset \\mathbb{R}^n</math> is measurable, then it is denoted by <math>\\Omega^{*}</math> the symmetrized version of <math>\\Omega</math> i.e. a ball <math>\\Omega^{*}:=B_r(0)\\subset\\mathbb{R}^n</math> such that <math>\\operatorname{vol}(\\Omega^{*})=\\operatorname{vol}(\\Omega)</math>. We denote by <math>f^{*}</math> the [[symmetric decreasing rearrangement]] of nonnegative measurable function f and define it as <math>f^{*}(x):=\\int_0^\\infty 1_{\\{f(x)>t\\}^{*}}(t) \\, dt</math>, where <math>\\{f(x)>t\\}^{*}</math> is the symmetrized version of preimage set <math>\\{f(x)>t\\}</math>. The methods described below have been proved to transform <math>\\Omega</math> to <math>\\Omega^{*}</math> i.e. given a sequence of symmetrization transformations <math>\\{T_k\\}</math> there is <math>\\lim\\limits_{k\\to \\infty}d_{Ha}(\\Omega^{*}, T_k(K) )=0</math>, where <math>d_{Ha}</math> is the [[Hausdorff distance]] (for discussion and proofs see {{harvtxt|Burchard |2009}})\n\n==Steiner symmetrization==\n[[File:Steiner Symmetrization.jpg|thumbnail|Steiner Symmetrization of set <math>\\Omega</math>]]\nSteiner symmetrization was introduced by Steiner (1838) to solve the isoperimetric theorem stated above. Let <math>H\\subset\\mathbb{R}^n</math> be a [[hyperplane]] through the origin. Rotate space so that <math>H</math> is the <math>x_n=0</math> (<math>x_n</math> is the ''n''th coordinate in <math>\\mathbb{R}^n</math>) hyperplane. For each <math>x\\in H</math> let the perpendicular line through <math>x\\in H </math> be <math>L_x = \\{x+ye_n:y\\in \\mathbb{R}\\}</math>. Then by replacing each <math>\\Omega\\cap L_x</math> by a line centered at H and with length <math>|\\Omega\\cap L_x|</math> we obtain the Steiner symmetrized version.\n\n:<math> \\operatorname{St}(\\Omega):=\\{x+ye_n:x+ze_n\\in \\Omega \\text{ for some } z \\text{ and } |y|\\leq\\frac{1}{2} |\\Omega\\cap L_x|\\}.</math>\n\nIt is denoted by <math>\\operatorname{St}(f)</math> the Steiner symmetrization wrt to <math>x_n=0</math> hyperplane of nonnegative measurable function <math>f:\\mathbb{R}^d\\to \\mathbb{R}</math> and for fixed <math>x_1,\\ldots,x_{n-1}</math> define it as\n\n:<math> St: f(x_1,\\ldots,x_{n-1},\\cdot)\\mapsto (f(x_1,\\ldots,x_{n-1},\\cdot))^{*}.</math>\n\n=== <u>Properties</u> ===\n\n* It preserves convexity: if <math> \\Omega </math> is convex, then <math> St(\\Omega) </math> is also convex.\n*It is linear: <math>St(x+\\lambda \\Omega)=St(x)+\\lambda St(\\Omega)</math>.\n*Super-additive: <math> St(K)+St(U)\\subset St(K+U)</math>.\n\n==Circular symmetrization==\n[[File:Circular symmetrization.png|thumbnail|Circular symmetrization of set <math>\\Omega</math>]]\nA popular method for symmetrization in the plane is Polya's circular symmetrization. After, its generalization will be described to higher dimensions. Let <math>\\Omega\\subset \\mathbb{C}</math> be a domain; then its circular symmetrization <math>\\operatorname{Circ}(\\Omega)</math> with regards to the positive real axis is defined as follows: Let\n\n<math>\\Omega_t:=\\{\\theta \\in [0,2\\pi]:te^{i\\theta}\\in \\Omega\\} </math>\n\ni.e. contain the arcs of radius t contained in <math>\\Omega</math>. So it is defined \n* If <math>\\Omega_t</math> is the full circle, then <math>\\operatorname{Circ}(\\Omega)\\cap \\{|z|=t\\}:=\\{|z|=t\\} </math>. \n* If the length is <math>m(\\Omega_t)=\\alpha</math>, then <math>\\operatorname{Circ}(\\Omega)\\cap \\{|z|=t\\}:=\\{te^{i\\theta}: |\\theta|<\\frac{\\alpha}{2}\\}</math>.\n* <math>0,\\infty\\in \\operatorname{Circ}(\\Omega)</math> iff <math>0,\\infty \\in \\Omega</math>.\n\nIn higher dimensions <math>\\Omega\\subset \\mathbb{R}^n</math>, its spherical symmetrization <math>Sp^n(\\Omega)</math> wrt to positive axis of <math>x_1</math> is defined as follows: Let\n<math>\\Omega_r:=\\{x\\in \\mathbb{S}^{n-1}: rx\\in \\Omega\\}</math> \ni.e. contain the caps of radius r contained in <math>\\Omega</math>. Also, for the first coordinate let <math>\\operatorname{angle}(x_1):=\\theta</math> if <math>x_1=rcos\\theta</math>. So as above\n* If <math>\\Omega_r</math> is the full cap, then <math>Sp^n(\\Omega)\\cap \\{|z|=r\\}:=\\{|z|=r\\}</math>.\n* If the surface area is <math>m_s(\\Omega_t)=\\alpha</math>, then <math>Sp^n(\\Omega)\\cap \\{|z|=r\\}:=\\{x:|x|=r</math> and <math>0\\leq \\operatorname{angle}(x_1)\\leq \\theta_\\alpha\\}=:C(\\theta_\\alpha)</math> where <math>\\theta_\\alpha</math> is picked so that its surface area is <math>m_s (C(\\theta_\\alpha)=\\alpha</math>. In words, <math>C(\\theta_\\alpha)</math> is a cap symmetric around the positive axis <math>x_1</math> with the same area as the intersection <math>\\Omega\\cap \\{|z|=r\\}</math>.\n* <math>0,\\infty\\in Sp^n(\\Omega)</math> iff <math>0,\\infty \\in \\Omega</math>.\n\n==Polarization==\n[[File:Polarization symmetrization.png|thumbnail|Polarization of set <math>\\Omega</math>]]\nLet <math>\\Omega\\subset\\mathbb{R}^n</math> be a domain and <math>H^{n-1}\\subset\\mathbb{R}^n</math> be a hyperplane through the origin. Denote the reflection across that plane to the positive halfspace <math>\\mathbb{H}^{+}</math> as <math>\\sigma_H</math> or just <math>\\sigma</math> when it is clear from the context. Also, the reflected <math>\\Omega</math> across hyperplane H is defined as <math>\\sigma \\Omega</math>. Then, the polarized <math>\\Omega</math> is denoted as <math>\\Omega^\\sigma</math> and defined as follows\n\n*  If <math>x\\in  \\Omega\\cap \\mathbb{H}^{+}</math>, then <math>x\\in \\Omega^{\\sigma}</math>.\n* If <math>x\\in \\Omega\\cap \\sigma(\\Omega) \\cap \\mathbb{H}^{-}</math>, then <math>x\\in \\Omega^{\\sigma}</math>.\n* If <math>x\\in (\\Omega\\setminus \\sigma(\\Omega)) \\cap \\mathbb{H}^{-}</math>, then <math>\\sigma x\\in \\Omega^{\\sigma}</math>.\n\nIn words, <math>(\\Omega\\setminus \\sigma(\\Omega)) \\cap \\mathbb{H}^{-}</math> is simply reflected to the halfspace <math>\\mathbb{H}^{+}</math>. It turns out that this transformation can approximate the above ones (in the [[Hausdorff distance]]) (see {{harvtxt|Brock|Solynin |2000}}).\n\n==References==\n{{reflist}}\n\n*{{Cite web\n  | last = Morgan \n  | first = Frank\n  | title = Symmetrization\n  | year = 2009\n  | url=http://math.williams.edu/symmmetrization/\n  | ref=Mor09\n  | accessdate = November 2015 }}\n*{{Cite web\n  | last = Burchard \n  | first = Almut\n  | title = A Short Course on Rearrangement Inequalities\n  | year = 2009\n  | url=http://www.math.toronto.edu/almut/rearrange.pdf\n  | ref=Bur09\n  | accessdate = November 2015 }}\n\n*{{cite arXiv\n  | last = Kojar\n  | first = Tomas\n  | title = Brownian Motion and Symmetrization\n  | year = 2015\n  | arxiv=1505.01868| ref=Koj15\n  }}\n\n*{{Citation\n|last=Brock\n|first= Friedemann\n|last2=Solynin\n|first2=  Alexander\n|title=An approach to symmetrization via polarization.\n|journal= Transactions of the American Mathematical Society\n|volume= 352\n|year= 2000\n|pages=  1759–1796\n|doi=10.1090/S0002-9947-99-02558-1\n|mr=1695019}}\n\n[[Category:Geometric inequalities]]\n[[Category:Geometric algorithms]]"
    },
    {
      "title": "Pólya–Szegő inequality",
      "url": "https://en.wikipedia.org/wiki/P%C3%B3lya%E2%80%93Szeg%C5%91_inequality",
      "text": "In [[mathematical analysis]], the '''Pólya–Szegő inequality''' (or '''Szegő inequality''') states that the Sobolev energy of a function in a [[Sobolev space]] does not increase under [[symmetric decreasing rearrangement]].<ref name=\":0\" /> The inequality is named after the [[mathematician]]s [[George Pólya]] and [[Gábor Szegő]].\n\n== Mathematical setting and statement ==\n\nGiven a [[Measurable function|Lebesgue measurable]] [[Function (mathematics)|function]] <math>u:\\R^n\\to \\R^+,</math>the symmetric decreasing rearrangement <math>u^*:\\R^n\\to \\R^+,</math> is the unique function such that for every <math>t \\in \\R,</math> the sublevel set <math>u^*{}^{-1}((t, +\\infty))</math> is an [[Ball (mathematics)|open ball]] centred at the origin <math>0 \\in \\R^n</math> that has the same [[Lebesgue measure]] as <math>u^{-1}((t, +\\infty)).</math> \n\nEquivalently, <math>u^*</math> is the unique [[Radial function|radial]] and radially [[Monotonic function|nonincreasing function]], whose [[Level set#Sublevel and superlevel sets|strict sublevel sets]] are open and have the same measure as those of the function <math>u</math>.\n\nThe Pólya–Szegő inequality states that if moreover <math>u \\in  W^{1,p}(\\R^n),</math> then <math>u^* \\in W^{1,p}(\\R^n)</math> and\n\n:<math> \\int_{\\R^n} |\\nabla u^*|^p  \\leq \\int_{\\R^n} |\\nabla u|^p.</math>\n\n== Applications of the inequality ==\nThe Pólya–Szegő inequality is used to prove the [[Rayleigh–Faber–Krahn inequality]], which states that among all the domains of a given fixed volume, the ball has the smallest first [[Eigenvalues and eigenvectors|eigenvalue]] for the [[Laplace operator|Laplacian]] with [[Dirichlet boundary condition]]s. The proof goes by restating the problem as a minimization of the [[Rayleigh quotient]].<ref name=\":0\" />\n\nThe [[isoperimetric inequality]] can be deduced from the Pólya–Szegő inequality with <math>p = 1</math>.\n\nThe optimal constant in the [[Sobolev inequality]] can be obtained by combining the Pólya–Szegő inequality with some integral inequalities.<ref name=\":1\" /><ref>{{Cite journal|last=Aubin|first=Thierry|date=1976-01-01|title=Problèmes isopérimétriques et espaces de Sobolev|url=http://projecteuclid.org/euclid.jdg/1214433725|journal=Journal of Differential Geometry|language=FR|volume=11|issue=4|pages=573–598|issn=0022-040X|doi=10.4310/jdg/1214433725}}</ref>\n\n== Equality cases ==\nSince the Sobolev energy is invariant under translations, any translation of a radial function achieves equality in the Pólya–Szegő inequality. There are however other functions that can achieve equality, obtained for example by taking a radial nonincreasing function that achieves its maximum on a ball of positive radius and adding to this function another function which is radial with respect to a different point and whose support is contained in the maximum set of the first function. In order to avoid this obstruction, an additional condition is thus needed.\n\nIt has been proved that if the function <math>u</math> achieves equality in the Pólya–Szegő inequality and if the set <math>\\{ x \\in \\mathbb{R}^n : u (x) > 0 \\text{ and } \\nabla u (x) = 0\\}</math> is a [[null set]] for Lebesgue's measure, then the function <math>u</math> is radial and radially nonincreasing with respect to some point <math>a \\in \\mathbb{R}^n</math>.<ref>{{Cite journal|last=Brothers|first=John E.|last2=Ziemer|first2=William P.|year=1988|title=Minimal rearrangements of Sobolev functions.|url=https://eudml.org/doc/153002|journal=Journal für die Reine und Angewandte Mathematik|language=en|volume=384|pages=153–179|issn=0075-4102|via=}}</ref>\n\n== Generalizations ==\nThe Pólya–Szegő inequality is still valid for symmetrizations on the [[sphere]] or the [[hyperbolic space]].<ref>{{cite encyclopedia\n  | last = Baernstein II\n  | first = Albert\n  | editor-last1 = Alvino \n  | editor-first1 = Angelo\n  | editor-last2 = Fabes\n  | editor-first2 = Eugenes\n  | editor-last3 = Talenti\n  | editor-first3 = Giorgio\n  | encyclopedia = Partial Differential Equations of Elliptic Type\n  | title = A unified approach to symmetrization\n  | language = English\n  | date = 1994\n  | series = Symposia Mathematica\n  | publisher = Cambridge University Press\n  | isbn = 9780521460484\n  | pages = 47–92}}</ref>\n\nThe inequality also holds for partial symmetrizations defined by foliating the space into planes (Steiner symmetrization)<ref>{{Cite book|title=Rearrangements and Convexity of Level Sets in PDE - Springer|volume=1150|last=Kawohl|first=Bernhard|publisher=Springer|year=1985|isbn=978-3-540-15693-2|series=Lecture Notes in Mathematics|location=Berlin Heidelberg|pages=|language=en|doi=10.1007/bfb0075060|issn=0075-8434|quote=|via=}}</ref><ref>{{Cite journal|last=Brock|first=Friedemann|last2=Solynin|first2=Alexander|date=2000|title=An approach to symmetrization via polarization|journal=Transactions of the American Mathematical Society|volume=352|issue=4|pages=1759–1796|doi=10.1090/S0002-9947-99-02558-1|issn=0002-9947}}</ref> and into spheres (cap symmetrization).<ref>{{Cite book|url=https://books.google.com/?id=a4RxcAAACAAJ|title=Symmetrization of Condensers in N-space|last=Sarvas|first=Jukka|date=1972|publisher=Suomalainen Tiedeakatemia|isbn=9789514100635|location=|pages=|language=en|quote=|via=}}</ref><ref>{{Cite journal|last=Smets|first=Didier|last2=Willem|first2=Michel|year=2003|title=Partial symmetry and asymptotic behavior for some elliptic variational problems|journal=Calculus of Variations and Partial Differential Equations|language=en|volume=18|issue=1|pages=57–75|doi=10.1007/s00526-002-0180-y|issn=0944-2669}}</ref>\n\nThere are also Pólya−Szegő inequalities for rearrangements with respect to non-Euclidean norms and using the [[dual norm]] of the gradient.<ref>{{Cite journal|last=Angelo|first=Alvino|last2=Vincenzo|first2=Ferone|last3=Guido|first3=Trombetti|last4=Pierre-Louis|first4=Lions|author-link4=Pierre-Louis Lions|date=1997|title=Convex symmetrization and applications|url=http://www.numdam.org/item?id=AIHPC_1997__14_2_275_0|journal=Annales de l'I.H.P. Analyse non Linéaire|language=fr|volume=14|issue=2|pages=|via=}}</ref><ref>{{Cite journal|last=Van Schaftingen|first=Jean|date=2006|title=Anisotropic symmetrization|journal=Annales de l'Institut Henri Poincare (C) non Linear Analysis|volume=23|issue=4|pages=539–565|doi=10.1016/j.anihpc.2005.06.001}}</ref><ref>{{Cite journal|last=Cianchi|first=Andrea|date=2007|title=Symmetrization in Anisotropic Elliptic Problems|journal=Communications in Partial Differential Equations|volume=32|issue=5|pages=693–717|doi=10.1080/03605300600634973|issn=0360-5302}}</ref>\n\n== Proofs of the inequality ==\n\n=== Original proof by a cylindrical isoperimetric inequality ===\n\nThe original proof by Pólya and Szegő for <math>p = 2</math> was based on an isoperimetric inequality comparing sets with cylinders and an asymptotics expansion of the area of the area of the graph of a function.<ref name=\":0\">{{Cite book|title=Isoperimetric Inequalities in Mathematical Physics| last=Pólya|first=George|last2=Szegő|first2=Gábor|publisher=Princeton University Press|year=1951|isbn=9780691079882|series=Annals of Mathematics Studies |location=Princeton, N.J.|pages=|issn=0066-2313|quote=|author-link=George Pólya|author-link2=Gábor Szegő|via=}}</ref> The inequality is proved for a smooth function <math>u</math> that vanishes outside a compact subset of the Euclidean space <math>\\R^n.</math> For every <math>\\varepsilon > 0</math>, they define the sets \n\n:<math>\\begin{align}\nC_\\varepsilon   &= \\{ (x, t) \\in \\R^n \\times \\R \\, : \\, 0 < t < \\varepsilon u (x)  \\} \\\\\nC_\\varepsilon^* &= \\{ (x, t) \\in \\R^n \\times \\R \\, : \\, 0 < t < \\varepsilon u^* (x)\\}\n\\end{align}</math>\n\nThese sets are the sets of points who lie between the domain of the functions <math>\\varepsilon u</math> and <math>\\varepsilon u^*</math> and their respective graphs. They use then the geometrical fact that since the horizontal slices of both sets have the same measure and those of the second are balls, to deduce that the area of the boundary of the cylindrical set <math>C_\\varepsilon^*</math> cannot exceed the one of <math>C_\\varepsilon</math>. These areas can be computed by the [[area formula]] yielding the inequality\n\n:<math>\\int_{u^*{}^{-1} ((0,+\\infty))} 1 + \\sqrt{1 + \\varepsilon^2 |\\nabla u^*|^2}\\le \\int_{u^{-1} ((0,+\\infty))} 1 + \\sqrt{1 + \\varepsilon^2 |\\nabla u|^2}.</math>\n\nSince the sets <math>u^{-1} ((0, +\\infty))</math> and <math>u{}^*{}^{-1} ((0, +\\infty))</math> have the same measure, this is equivalent to\n\n:<math>\\frac{1}{\\varepsilon} \\int_{u^*{}^{-1} ((0, +\\infty))} \\sqrt{1 + \\varepsilon^2 | \\nabla u^*|^2} - 1 \\le \\frac{1}{\\varepsilon} \\int_{u^{-1} ((0, +\\infty))} \\sqrt{1 + \\varepsilon^2 |\\nabla u|^2} - 1.</math>\n\nThe conclusion then follows from the fact that \n\n:<math>\\lim_{\\varepsilon \\to 0} \\frac{1}{\\varepsilon} \\int_{u^{-1} ((0, +\\infty))} \\sqrt{1 + \\varepsilon^2 | \\nabla u|^2} - 1 = \\frac{1}{2} \\int_{\\R^n} | \\nabla u|^2.</math>\n\n=== Coarea formula and isoperimetric inequality ===\nThe Pólya–Szegő inequality can be proved by combining the [[coarea formula]], [[Hölder's inequality|Hölder’s inequality]] and the classical [[isoperimetric inequality]].<ref name=\":1\">{{Cite journal|last=Talenti|first=Giorgio|year=1976|title=Best constant in Sobolev inequality|journal=Annali di Matematica Pura ed Applicata|language=en|volume=110|issue=1|pages=353–372|doi=10.1007/BF02418013|issn=0373-3114|citeseerx=10.1.1.615.4193}}</ref>\n\nIf the function <math>u</math> is smooth enough, the coarea formula can be used to write\n\n:<math>\\int_{\\R^n} | \\nabla u |^p = \\int_0^{+\\infty} \\int_{u^{-1} ({t})} | \\nabla u | ^{p - 1} \\, d \\mathcal{H}^{n - 1} \\, dt,</math>\n\nwhere <math>\\mathcal{H}^{n-1}</math> denotes the <math>(n-1)</math>–dimensional [[Hausdorff measure]] on the Euclidean space <math>\\R ^n</math>. For almost every each <math>t \\in (0, +\\infty)</math>, we have by Hölder's inequality,\n\n:<math>\\mathcal{H}^{n-1} \\left(u^{-1} (\\{t\\})\\right) \\le \\left(\\int_{u^{-1} (\\{t\\})} |\\nabla u | ^{p - 1}\\right )^\\frac{1}{p} \\left(\\int_{u^{-1} (\\{t\\})} \\frac{1}{|\\nabla u | } \\right )^{1 - \\frac{1}{p}}.</math>\n\nTherefore, we have\n\n:<math>\\int_{u^{-1} (\\{t\\})} |\\nabla u | ^{p-1} \\ge \\frac{\\mathcal{H}^{n - 1} \\left(u^{-1} (\\{t\\})\\right )^p}{\\left(\\int_{u^{-1} (\\{t\\})} \\frac{1}{| \\nabla u| }\\right)^{p - 1}}.</math>\n\nSince the set <math> u^*{}^{-1} ((t, +\\infty))</math> is a ball that has the same measure as the set <math> u^{-1} ((t, +\\infty))</math>, by the classical isoperimetric inequality, we have \n\n:<math>\\mathcal{H}^{n-1} \\left( u^*{}^{-1}(\\{t\\})\\right) \\le \\mathcal{H}^{n - 1} \\left( u^{-1}(\\{t\\})\\right).</math>\n\nMoreover, recalling that the sublevel sets of the functions <math> u</math> and <math> u^*</math> have the same measure,\n\n:<math>\\int_{u^*{}^{-1} (\\{t\\})} \\frac{1}{|\\nabla u^*| } = \\int_{u^{-1} (\\{t\\})} \\frac{1}{| \\nabla u| }, </math>\n\nand therefore,\n\n:<math>\\int_{\\R^n} |\\nabla u |^p \\ge \\int_0^{+\\infty} \\frac{\\mathcal{H}^{n-1} \\left (u^*{}^{-1} (\\{t\\})\\right)^p}{\\left(\\int_{u^*{}^{-1} (\\{t\\})} \\frac{1}{|\\nabla u^*| }\\right)^{p-1}}\\,dt. </math>\n\nSince the function <math>u^* </math> is radial, one has\n\n:<math>\\frac{\\mathcal{H}^{n-1} \\left(u^*{}^{-1} (\\{t\\})\\right)^p} {\\left(\\int_{u^*{}^{-1} (\\{t\\})} \\frac{1}{| \\nabla u^*| }\\right)^{p-1}}\n= \\int_{u^*{}^{-1} (\\{t\\})} |\\nabla u^*| ^{p - 1}, </math>\n\nand the conclusion follows by applying the coarea formula again.\n\n=== Rearrangement inequalities for convolution ===\nWhen <math>p=2</math>, the Pólya–Szegő inequality can be proved by representing the Sobolev energy by the [[heat kernel]].<ref>{{Cite book|title=Analysis|last=Lieb|first=Elliott H.|last2=Loss|first2=Michael|date=2001-01-01|publisher=American mathematical Society|isbn=9780821827833|edition=2|location=|pages=|oclc=468606724|quote=|author-link=Elliott H. Lieb|author-link2=Michael Loss}}</ref> One begins by observing that \n\n:<math>\\int_{\\R^n} |\\nabla u|^2 = \\lim_{t \\to 0} \\frac{1}{t} \\left (\\int_{\\R^n} |u|^2 - \\int_{\\R^n}\\int_{\\R^n} K_t (x-y) u(x) u(y) \\,dx \\,dy\\right ),</math>\n\nwhere for <math>t \\in (0, +\\infty)</math>, the function <math>K_t : \\R^n \\to \\R</math> is the heat kernel, defined for every <math>z \\in \\R^n</math> by \n\n:<math>K_t (z) = \\frac{1}{(4 \\pi t)^\\frac{n}{2}} e^{-\\frac{|z|^2}{4t}}.</math>\n\nSince for every <math>t \\in (0, + \\infty)</math> the function <math>K_t</math> is radial and radially decreasing, we have by the [[Riesz rearrangement inequality]]\n\n:<math> \\int_{\\R^n}\\int_{\\R^n} K_t (x - y)\\, u (x)\\, u (y) \\, dx \\,dy \\le \\int_{\\R^n}\\int_{\\R^n} K_t (x-y)\\, u^*(x)\\, u^*(y) \\, dx \\,dy</math>\n\nHence, we deduce that\n\n:<math>\\begin{align}\n\\int_{\\R^n} |\\nabla u|^2 &= \\lim_{t \\to 0} \\frac{1}{t} \\left (\\int_{\\R^n} |u|^2 - \\int_{\\R^n}\\int_{\\R^n} K_t (x-y) u(x) u(y) \\, dx \\,dy\\right ) \\\\[6pt]\n&\\ge \\lim_{t \\to 0} \\frac{1}{t} \\left (\\int_{\\R^n} |u|^2 - \\int_{\\R^n}\\int_{\\R^n} K_t (x-y) u^*(x) u^*(y) \\, dx \\,dy\\right) \\\\[6pt]\n&= \\int_{\\R^n} | \\nabla u^*|^2.\n\\end{align}\n</math>\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Szego inequality}}\n[[Category:Sobolev spaces]]\n[[Category:Geometric inequalities]]\n[[Category:Rearrangement inequalities]]"
    },
    {
      "title": "Toponogov's theorem",
      "url": "https://en.wikipedia.org/wiki/Toponogov%27s_theorem",
      "text": "In the [[mathematics|mathematical]] field of [[Riemannian geometry]], '''Toponogov's theorem''' (named after [[Victor Andreevich Toponogov]]) is a triangle comparison theorem. It is one of a family of theorems that quantify the assertion that a pair of geodesics emanating from a point ''p'' spread apart more slowly in a region of high curvature than they would in a region of low curvature.\n\nLet ''M'' be an ''m''-dimensional [[Riemannian manifold]] with [[sectional curvature]] ''K'' satisfying\n\n<math> K\\ge \\delta\\,.</math>\n\nLet ''pqr'' be a [[geodesic triangle]], i.e. a triangle whose sides are geodesics, in ''M'', such that the geodesic ''pq'' is minimal and if &delta; > ''0'', the length of the side ''pr'' is less than <math>\\pi / \\sqrt \\delta</math>.\nLet ''p''&prime;''q''&prime;''r''&prime; be a geodesic triangle in the model space ''M''<sub>&delta;</sub>, i.e. the [[simply connected]] space of [[constant curvature]] &delta;, such that the length of sides ''p&prime;q&prime;'' and ''p&prime;r&prime;''is equal to that of ''pq'' and ''pr'' respectively and the angle at ''p&prime;'' is equal to that at ''p''. \nThen\n\n:<math>d(q,r) \\le d(q',r').\\,</math>\n\nWhen the sectional curvature is bounded from above, a corollary to the [[Rauch comparison theorem]] yields an analogous statement, but with the reverse inequality {{citation needed|date=April 2014}}.\n\n==References==\n* {{citation | last=Chavel|first=Isaac | title=Riemannian Geometry; A Modern Introduction | edition=second | publisher=Cambridge University Press | year=2006 }}\n* {{citation | last=Berger|first=Marcel | authorlink=Marcel Berger|title= A Panoramic View of Riemannian Geometry | publisher=Springer-Verlag | year=2004 | isbn = 3-540-65317-1}}\n* {{Citation | last1=Cheeger | first1=Jeff | last2=Ebin | first2=David G. | title=Comparison theorems in Riemannian geometry | publisher=AMS Chelsea Publishing, Providence, RI | isbn=978-0-8218-4417-5 |mr=2394158 | year=2008}}\n\n==External links==\n* [http://tzamfirescu.tricube.de/TZamfirescu-183.pdf Pambuccian V., Zamfirescu T. \"Paolo Pizzetti: The forgotten originator of triangle comparison geometry\". ''Hist Math'' 38:8 (2011)]\n\n[[Category:Theorems in Riemannian geometry]]\n[[Category:Geometric inequalities]]\n\n\n{{differential-geometry-stub}}"
    },
    {
      "title": "List of triangle inequalities",
      "url": "https://en.wikipedia.org/wiki/List_of_triangle_inequalities",
      "text": ":''For the basic inequality'' ''a'' < ''b'' + ''c'', ''see [[Triangle inequality]].''\n:''For inequalities of acute or obtuse triangles, see [[Acute and obtuse triangles]].''\n\nIn [[geometry]], '''triangle inequalities''' are [[inequality (mathematics)|inequalities]] involving the [[parameter]]s of [[triangle]]s, that hold for every triangle, or for every triangle meeting certain conditions. The inequalities give an ordering of two different values: they are of the form \"less than\", \"less than or equal to\", \"greater than\", or \"greater than or equal to\". The parameters in a triangle inequality can be the side lengths, the [[semiperimeter]], the [[angle]] measures, the values of [[trigonometric function]]s of those angles, the [[area (geometry)|area]] of the triangle, the [[median (geometry)|medians]] of the sides, the [[altitude (geometry)|altitudes]], the lengths of the internal [[bisection#Angle bisector|angle bisectors]] from each angle to the opposite side, the [[bisection#Perpendicular bisectors of the sides of a polygon|perpendicular bisectors]] of the sides, the distance from an arbitrary point to another point, the [[inradius]], the [[excircle|exradii]], the [[circumradius]], and/or other quantities.\n\nUnless otherwise specified, this article deals with triangles in the [[Euclidean plane]].\n\n==Main parameters and notation==\n\nThe parameters most commonly appearing in triangle inequalities are:\n\n*the side lengths ''a'', ''b'', and ''c'';\n*the [[semiperimeter]] ''s'' =&nbsp;(''a''&nbsp;+&nbsp;''b''&nbsp;+&nbsp;''c'')&nbsp;/&nbsp;2 (half the [[perimeter]] ''p'');\n*the [[angle]] measures ''A'', ''B'', and ''C'' of the angles of the [[vertex (geometry)#Of a polytope|vertices]] opposite the respective sides ''a'', ''b'', and ''c'' (with the vertices denoted with the same symbols as their angle measures);\n*the values of [[trigonometric function]]s of the angles;\n*the [[area (geometry)|area]] ''T'' of the triangle;\n*the [[median (geometry)|medians]] ''m''<sub>''a''</sub>, ''m''<sub>''b''</sub>, and ''m''<sub>''c''</sub> of the sides (each being the length of the line segment from the [[midpoint]] of the side to the opposite vertex);\n*the [[altitude (geometry)|altitudes]] ''h''<sub>''a''</sub>, ''h''<sub>''b''</sub>, and ''h''<sub>''c''</sub> (each being the length of a segment [[perpendicular]] to one side and reaching from that side (or possibly the extension of that side) to the opposite vertex);\n*the lengths of the [[bisection#Angle bisector|internal angle bisectors]] ''t''<sub>''a''</sub>, ''t''<sub>''b''</sub>, and ''t''<sub>''c''</sub> (each being a segment from a vertex to the opposite side and bisecting the vertex's angle);\n*the [[bisection#Bisectors of the sides of a polygon|perpendicular bisectors]] ''p''<sub>''a''</sub>, ''p''<sub>''b''</sub>, and ''p''<sub>''c''</sub> of the sides (each being the length of a segment perpendicular to one side at its midpoint and reaching to one of the other sides);\n*the lengths of line segments with an endpoint at an arbitrary point ''P'' in the plane (for example, the length of the segment from ''P'' to vertex ''A'' is denoted ''PA'' or ''AP'');\n*the [[inradius]] ''r'' (radius of the [[circle]] [[Inscribed figure|inscribed]] in the triangle, [[tangent]] to all three sides), the [[excircle|exradii]] ''r''<sub>''a''</sub>, ''r''<sub>''b''</sub>, and ''r''<sub>''c''</sub> (each being the radius of an excircle tangent to side ''a'', ''b'', or ''c'' respectively and tangent to the extensions of the other two sides), and the [[circumradius]] ''R'' (radius of the circle circumscribed around the triangle and passing through all three vertices).\n\n==Side lengths==\n\nThe basic [[triangle inequality]] is\n\n:<math>a < b+c, \\quad b < c + a, \\quad c < a + b</math>\n\nor equivalently\n\n::<math>\\text{max}(a, b, c)<s.</math>\n\nIn addition,\n\n:<math>\\frac{3}{2} \\le \\frac{a}{b+c} + \\frac{b}{a+c} + \\frac{c}{a+b} < 2,</math>\n\nwhere the value of the right side is the lowest possible bound,<ref name=PL>[[Alfred Posamentier|Posamentier, Alfred S.]] and Lehmann, Ingmar. ''The Secrets of Triangles'', Prometheus Books, 2012.</ref>{{rp|p. 259}} approached [[limit (mathematics)|asymptotically]] as certain classes of triangles approach the [[Degeneracy (mathematics)|degenerate]] case of zero area. The left inequality, which holds for all positive ''a, b, c'', is [[Nesbitt's inequality]].\n\nWe have\n\n:<math>3\\left( \\frac{a}{b}+\\frac{b}{c}+\\frac{c}{a}\\right) \\geq 2\\left( \\frac{b}{a}+\\frac{c}{b}+\\frac{a}{c} \\right) +3.</math><ref name=Crux/>{{rp|p.250,#82}}\n\n:<math>abc \\geq (a+b-c)(a-b+c)(-a+b+c). \\quad </math><ref name=PL/>{{rp|p. 260}}\n\n:<math>\\frac{1}{3} \\leq \\frac{a^2+b^2+c^2}{(a+b+c)^2} \\leq \\frac{1}{2}. \\quad </math><ref name=PL/>{{rp|p. 261}}\n\n:<math>\\sqrt{a+b-c} + \\sqrt{a-b+c} + \\sqrt{-a+b+c} \\leq \\sqrt{a}+\\sqrt{b} + \\sqrt{c}.</math><ref name=PL/>{{rp|p. 261}}\n\n:<math>a^2b(a-b) + b^2c(b-c) + c^2a(c-a) \\geq 0.</math><ref name=PL/>{{rp|p. 261}}\n\nIf angle ''C'' is obtuse (greater than 90°) then\n\n:<math>a^2+b^2 < c^2;</math>\n\nif ''C'' is acute (less than 90°) then\n\n:<math>a^2+b^2 > c^2.</math>\n\nThe in-between case of equality when ''C'' is a [[right angle]] is the [[Pythagorean theorem]].\n\nIn general,<ref name=Crux>''Inequalities proposed in “[[Crux Mathematicorum]]” and elsewhere\", [http://www.imomath.com/othercomp/Journ/ineq.pdf].</ref>{{rp|p.1,#74}}\n\n:<math>a^2+b^2 > \\frac{c^2}{2},</math>\n\nwith equality approached in the limit only as the apex angle of an isosceles triangle approaches 180°.\n\nIf the [[centroid]] of the triangle is inside the triangle's [[incircle]], then<ref>Nyugen, Minh Ha, and Dergiades, Nikolaos.  \"Garfunkel's Inequality\", ''Forum Geometricorum'' 4, 2004, 153–156. http://forumgeom.fau.edu/FG2004volume4/FG200419index.html</ref>{{rp|p. 153}}\n\n:<math>a^2 < 4bc, \\quad b^2 < 4ac, \\quad c^2 < 4ab.</math>\n\nWhile all of the above inequalities are true because ''a'', ''b'', and ''c'' must follow the basic triangle inequality that the longest side is less than half the perimeter, the following relations hold for all positive ''a'', ''b'', and ''c'':<ref name=PL/>{{rp|p.267}}\n\n:<math>\\frac{3abc}{ab+bc+ca} \\leq \\sqrt[3]{abc} \\leq \\frac{a+b+c}{3},</math>\n\neach holding with equality only when ''a'' = ''b'' = ''c''. This says that in the non-equilateral case the [[harmonic mean]] of the sides is less than their [[geometric mean]] which in turn is less than their [[arithmetic mean]].\n\n==Angles==\n\n:<math>\\cos A + \\cos B + \\cos C \\leq \\frac{3}{2}.</math>  <ref name=PL/>{{rp|p. 286}}\n\n:<math>(1-\\cos A)(1-\\cos B)(1-\\cos C) \\geq \\cos A \\cdot \\cos B \\cdot \\cos C.</math><ref name=Crux/>{{rp|p.21,#836}}\n\n:<math>\\cos ^4\\frac{A}{2} + \\cos ^4\\frac{B}{2} + \\cos ^4\\frac{C}{2} \\leq \\frac{s^3}{2abc}</math>\n\nfor semi-perimeter ''s'', with equality only in the equilateral case.<ref name=Crux/>{{rp|p.13,#608}}\n\n:<math>a+b+c \\ge 2\\sqrt{bc} \\cos A + 2 \\sqrt{ca} \\cos B + 2\\sqrt{ab} \\cos C.</math> <ref>Lu, Zhiqin. \"An optimal inequality\", ''[[Mathematical Gazette]]'' 91, November 2007, 521–523.</ref>{{rp|Thm.1}}\n\n:<math>\\sin A + \\sin B + \\sin C \\leq \\frac{3\\sqrt{3}}{2}.</math> <ref name=PL/>{{rp|p.286}}\n\n:<math>\\sin ^2 A + \\sin ^2 B + \\sin ^2 C \\leq \\frac{9}{4}.</math>  <ref name=PL/>{{rp|p. 286}}\n\n:<math>\\sin A \\cdot \\sin B \\cdot \\sin C  \\leq \\left(\\frac{\\sin A+\\sin B+\\sin C}{3}\\right)^3 \\leq\\left(\\sin\\frac{A+B+C}{3}\\right)^3 =\\sin^3\\left(\\frac{\\pi}{3}\\right)= \\frac{3\\sqrt{3}}{8}.</math>  <ref name=SV>Svrtan, Dragutin and Veljan, Darko. \"Non-Euclidean versions of some classical triangle inequalities\", ''Forum Geometricorum'' 12, 2012, 197–209. http://forumgeom.fau.edu/FG2012volume12/FG201217index.html</ref>{{rp|p. 203}}\n\n:<math>\\sin A+\\sin B \\cdot \\sin C \\leq \\varphi</math><ref name=Crux/>{{rp|p.149,#3297}}\n\nwhere <math>\\varphi = \\frac{1+\\sqrt{5}}{2},</math> the [[golden ratio]].\n\n:<math>\\sin \\frac{A}{2} \\cdot \\sin \\frac{B}{2} \\cdot \\sin \\frac{C}{2}  \\leq \\frac{1}{8}.</math> <ref name=PL/>{{rp|p. 286}}\n\n:<math>\\tan ^2 \\frac{A}{2} + \\tan ^2 \\frac{B}{2} + \\tan ^2 \\frac{C}{2} \\geq 1.</math>  <ref name=PL/>{{rp|p. 286}}\n\n:<math>\\cot A + \\cot B + \\cot C \\geq \\sqrt{3}.</math>  <ref name= Scott>Scott, J. A., \"A cotangent inequality for two triangles\", ''Mathematical Gazette 89, November 2005, 473–474.</ref>\n\n:<math>\\sin A \\cdot \\cos B +\\sin B \\cdot \\cos C+\\sin C \\cdot \\cos A \\leq \\frac{3\\sqrt{3}}{4}.</math><ref name=Crux/>{{rp|p.187,#309.2}}\n\nFor circumradius ''R'' and inradius ''r'' we have\n\n:<math>\\max\\left(\\sin \\frac{A}{2}, \\sin \\frac{B}{2}, \\sin \\frac{C}{2} \\right) \\le \\frac{1}{2} \\left(1+\\sqrt{1-\\frac{2r}{R}} \\right),</math>\n\nwith equality if and only if the triangle is isosceles with apex angle greater than or equal to 60°;<ref name= Birsan/>{{rp|Cor. 3}} and\n\n:<math>\\min\\left(\\sin \\frac{A}{2}, \\sin \\frac{B}{2}, \\sin \\frac{C}{2} \\right) \\ge \\frac{1}{2} \\left(1-\\sqrt{1-\\frac{2r}{R}} \\right),</math>\n\nwith equality if and only if the triangle is isosceles with apex angle less than or equal to 60°.<ref name=Birsan/>{{rp|Cor. 3}}\n\nWe also have\n\n:<math>\\frac{r}{R}-\\sqrt{1-\\frac{2r}{R}} \\le \\cos A \\le \\frac{r}{R}+\\sqrt{1-\\frac{2r}{R}}</math>\n\nand likewise for angles ''B, C'', with equality in the first part if the triangle is isosceles and the apex angle is at least 60° and equality in the second part if and only if the triangle is isosceles with apex angle no greater than 60°.<ref name=Birsan/>{{rp|Prop. 5}}\n\nFurther, any two angle measures ''A'' and ''B'' opposite sides ''a'' and ''b'' respectively are related according to<ref name=PL/>{{rp|p. 264}}\n\n:<math>A>B \\quad \\text{if and only if} \\quad a > b,</math>\n\nwhich is related to the [[isosceles triangle theorem]] and its converse, which state that ''A'' = ''B'' if and only if ''a'' = ''b''.\n\nBy [[Euclid]]'s [[exterior angle theorem]], any [[exterior angle]] of a triangle is greater than either of the [[interior angle]]s at the opposite vertices:<ref name=PL/>{{rp|p. 261}}\n\n:<math>180\\text{°} - A > \\max(B,C).</math>\n\nIf a point ''D'' is in the interior of triangle ''ABC'', then\n\n:<math>\\angle BDC > \\angle A.</math><ref name=PL/>{{rp|p. 263}}\n\nFor an acute triangle we have<ref name=Crux/>{{rp|p.26,#954}}\n\n:<math>\\cos^2A+\\cos^2B+\\cos^2C < 1,</math>\n\nwith the reverse inequality holding for an obtuse triangle.\n\nFurthermore, for non-obtuse triangles we have<ref>Shattuck, Mark. “A Geometric Inequality for Cyclic Quadrilaterals”, ''Forum Geometricorum'' 18, 2018, 141-154. http://forumgeom.fau.edu/FG2018volume18/FG201822.pdf</ref>{{rp|Corollary 3}}\n\n:<math>\\frac{2R+r}{R}\\le \\sqrt{2}\\left(\\cos\\left(\\frac{A-C}{2}\\right)+\\cos\\left(\\frac{B}{2}\\right)\\right)</math>\n\nwith equality if and only if it is a right triangle with hypotenuse AC.\n\n==Area==\n\n[[Weitzenböck's inequality]] is, in terms of area ''T'',<ref name=PL/>{{rp|p. 290}}\n\n: <math>a^2 + b^2 + c^2 \\geq 4\\sqrt{3}\\cdot T, </math>\n\nwith equality only in the equilateral case. This is a [[corollary]] of the [[Hadwiger–Finsler inequality]], which is\n\n:<math>a^{2} + b^{2} + c^{2} \\geq (a - b)^{2} + (b - c)^{2} + (c - a)^{2} + 4 \\sqrt{3} \\cdot T .</math>\n\nAlso,\n\n:<math>ab+bc+ca \\geq 4\\sqrt{3} \\cdot T</math><ref name=Torrejon/>{{rp|p. 138}}\n\nand<ref name=Crux/>{{rp|p.192,#340.3}}<ref name=SV/>{{rp|p. 204}}\n\n:<math>T \\leq \\frac{abc}{2}\\sqrt{\\frac{a+b+c}{a^3+b^3+c^3+abc}} \\leq \\frac{1}{4}\\sqrt[6] \\frac{3(a+b+c)^3(abc)^4}{a^3+b^3+c^3}  \\leq \\frac{\\sqrt{3}}{4}(abc)^{2/3}.</math>\n\nFrom the rightmost upper bound on ''T'', using the [[arithmetic-geometric mean inequality]], is obtained the [[isoperimetric inequality for triangles]]:\n\n:<math>T \\leq \\frac{\\sqrt{3}}{36}(a+b+c)^2 = \\frac{\\sqrt{3}}{9}s^2</math> <ref name=SV/>{{rp|p. 203}}\n\nfor semiperimeter ''s''. This is sometimes stated in terms of perimeter ''p'' as\n\n:<math>p^2 \\ge 12\\sqrt{3} \\cdot T,</math>\n\nwith equality for the [[equilateral triangle]].<ref name=Chakerian>Chakerian, G. D. \"A Distorted View of Geometry.\" Ch. 7 in ''Mathematical Plums'' (R. Honsberger, editor). Washington, DC: Mathematical Association of America, 1979: 147.</ref> This is strengthened by\n\n:<math>T \\le \\frac{\\sqrt{3}}{4}(abc)^{2/3}.</math>\n\n[[Bonnesen's inequality]] also strengthens the isoperimetric inequality:\n\n:<math> \\pi^2 (R-r)^2 \\leq (a+b+c)^2-4\\pi T. </math>\n\nWe also have\n\n:<math>\\frac{9abc}{a+b+c} \\ge 4\\sqrt{3} \\cdot T</math> <ref name=PL/>{{rp|p. 290}}<ref name=Torrejon>Torrejon, Ricardo M. \"On an Erdos inscribed triangle inequality\", ''Forum Geometricorum'' 5, 2005, 137–141. http://forumgeom.fau.edu/FG2005volume5/FG200519index.html</ref>{{rp|p. 138}}\n\nwith equality only in the equilateral case;\n\n:<math>38T^2 \\leq 2s^4-a^4-b^4-c^4</math><ref name=Crux/>{{rp|p.111,#2807}}\n\nfor semiperimeter ''s''; and\n\n:<math>\\frac{1}{a}+\\frac{1}{b}+\\frac{1}{c} < \\frac{s}{T}.</math><ref name=Crux/>{{rp|p.88,#2188}}\n\n[[Ono's inequality]] for acute triangles (those with all angles less than 90°) is\n\n:<math>27 (b^2 + c^2 - a^2)^2 (c^2 + a^2 - b^2)^2 (a^2 + b^2 - c^2)^2 \\leq (4 T)^6.</math>\n\nThe area of the triangle can be compared to the area of the [[incircle]]:\n\n:<math>\\frac{\\text{Area of incircle}}{\\text{Area of triangle}} \\leq \\frac{\\pi}{3\\sqrt{3}}</math>\n\nwith equality only for the equilateral triangle.<ref name=MP>Minda, D., and Phelps, S., \"Triangles, ellipses, and cubic polynomials\", ''[[American Mathematical Monthly]]'' 115, October 2008, 679–689: Theorem 4.1.</ref>\n\nIf an inner triangle is inscribed in a reference triangle so that the inner triangle's vertices partition the perimeter of the reference triangle into equal length segments, the ratio of their areas is bounded by<ref name=Torrejon/>{{rp|p. 138}}\n\n:<math>\\frac{\\text{Area of inscribed triangle}}{\\text{Area of reference triangle}} \\leq \\frac{1}{4}.</math>\n\nLet the interior angle bisectors of ''A'', ''B'', and ''C'' meet the opposite sides at ''D'', ''E'', and ''F''. Then<ref name=Crux/>{{rp|p.18,#762}}\n\n:<math>\\frac{3abc}{4(a^3+b^3+c^3)} \\leq \\frac{\\text{Area of triangle} \\,DEF}{\\text{Area of triangle} \\, ABC} \\leq \\frac{1}{4}.</math>\n\nA line through a triangle’s median splits the area such that the ratio of the smaller sub-area to the original triangle’s area is at least 4/9.<ref>Henry Bottomley, “Medians and Area Bisectors of a Triangle” http://www.se16.info/js/halfarea.htm</ref>\n\n==Medians and centroid==\n\nLet ''ABC'' be a triangle, let ''G'' be its centroid, and let ''D'', ''E'', and ''F'' be the midpoints of ''BC'', ''CA'', and ''AB'', respectively. For any point ''P'' in the plane of ''ABC'' then \n\n:<math>PA+PB+PC \\le 2(PD+PE+PF)+3PG.</math><ref> Dao Thanh Oai, Problem 12015, The American Mathematical Monthly, Vol.125, January 2018</ref>\n\nThe three [[median]]s <math>m_a, \\,m_b, \\, m_c</math> of a triangle each connect a vertex with the midpoint of the opposite side, and the sum of their lengths satisfies<ref name=PL/>{{rp|p. 271}}\n\n:<math>\\frac{3}{4}(a+b+c) < m_a+m_b+m_c < a+b+c.</math>\n\nMoreover,<ref name=Crux/>{{rp|p.12,#589}}\n\n:<math>\\left( \\frac{m_a}{a} \\right)^2 + \\left( \\frac{m_b}{b} \\right)^2  + \\left( \\frac{m_c}{c} \\right)^2 \\geq \\frac{9}{4},</math>\n\nwith equality only in the equilateral case, and for inradius ''r'',<ref name=Crux/>{{rp|p.22,#846}}\n\n:<math>\\frac{m_am_bm_c}{m_a^2+m_b^2+m_c^2} \\geq r.</math>\n\nIf we further denote the lengths of the medians extended to their intersections with the circumcircle as ''M''<sub>''a''</sub> , \n''M''<sub>''b''</sub> , and ''M''<sub>''c''</sub> , then<ref name=Crux/>{{rp|p.16,#689}}\n\n:<math>\\frac{M_a}{m_a} + \\frac{M_b}{m_b} + \\frac{M_c}{m_c} \\geq 4.</math>\n\nThe [[centroid]] ''G'' is the intersection of the medians. Let ''AG'', ''BG'', and ''CG'' meet the circumcircle at ''U'', ''V'', and ''W'' respectively. Then both<ref name=Crux/>{{rp|p.17#723}}\n\n:<math>GU+GV+GW \\geq AG+BG+CG</math>\n\nand\n\n:<math>GU \\cdot GV \\cdot GW \\geq AG \\cdot BG \\cdot CG;</math>\n\nin addition,<ref name=Crux/>{{rp|p.156,#S56}}\n\n:<math>\\sin GBC+\\sin GCA+\\sin GAB \\leq \\frac{3}{2}.</math>\n\nFor an acute triangle we have<ref name=Crux/>{{rp|p.26,#954}}\n\n:<math>m_a^2+m_b^2+m_c^2 > 6R^2</math>\n\nin terms of the circumradius ''R'', while the opposite inequality holds for an obtuse triangle.\n\nDenoting as ''IA, IB, IC'' the distances of the [[incenter]] from the vertices, the following holds:<ref name=Crux/>{{rp|p.192,#339.3}}\n\n:<math>\\frac{IA^2}{m_a^2}+\\frac{IB^2}{m_b^2}+\\frac{IC^2}{m_c^2} \\leq \\frac{3}{4}.</math>\n\nThe three medians of any triangle can form the sides of another triangle:<ref>Benyi, A ́rpad, and C ́́urgus, Branko. \"Ceva's triangle inequalities\", ''Mathematical Inequalities & Applications''  17 (2), 2014, 591-609.</ref>{{rp|p. 592}}\n\n:<math>m_a < m_b+m_c, \\quad m_b<m_c+m_a, \\quad m_c< m_a+m_b.</math>\n\nFurthermore,<ref>Michel Bataille, “Constructing a Triangle from Two Vertices and the Symmedian Point”, ''Forum Geometricorum'' 18 (2018), 129--133.</ref>{{rp|Coro. 6}}\n\n:<math>\\max\\{bm_c +cm_b, \\quad cm_a +am_c,\\quad am_b +bm_a\\} \\le \\frac{a^2+b^2+c^2}{\\sqrt{3}}.</math>\n\n==Altitudes==\n\nThe altitudes ''h''<sub>''a''</sub> , etc. each connect a vertex to the opposite side and are perpendicular to that side. They satisfy both<ref name=PL/>{{rp|p. 274}}\n\n:<math>h_a+h_b+h_c \\leq \\frac {\\sqrt{3}}{2}(a+b+c)</math>\n\nand\n\n:<math>h_a^2+h_b^2+h_c^2 \\le \\frac{3}{4}(a^2+b^2+c^2).</math>\n\nIn addition, if <math>a\\geq b \\geq c,</math> then<ref name=Crux/>{{rp|222,#67}}\n\n:<math>a+h_a \\geq b+h_b \\geq c+h_c.</math>\n\nWe also have<ref name=Crux/>{{rp|p.140,#3150}}\n\n:<math>\\frac{h_a^2}{(b^2+c^2)}\\cdot \\frac{h_b^2}{(c^2+a^2)} \\cdot \\frac{h_c^2}{(a^2+b^2)} \\leq \\left(\\frac{3}{8} \\right)^3.</math>\n\nFor internal angle bisectors ''t''<sub>''a''</sub>, ''t''<sub>''b''</sub>, ''t''<sub>''c''</sub> from vertices ''A, B, C'' and circumcenter ''R'' and incenter ''r'', we have<ref name=Crux/>{{rp|p.125,#3005}}\n\n:<math>\\frac{h_a}{t_a}+\\frac{h_b}{t_b}+\\frac{h_c}{t_c} \\geq \\frac{R+4r}{R}.</math>\n\nThe reciprocals of the altitudes of any triangle can themselves form a triangle:<ref>Mitchell, Douglas W., \"A Heron-type formula for the reciprocal area of a triangle\", ''Mathematical Gazette'' 89 (November 2005), 494.</ref>\n\n:<math>\\frac{1}{h_a}<\\frac{1}{h_b}+\\frac{1}{h_c}, \\quad \\frac{1}{h_b}<\\frac{1}{h_c}+\\frac{1}{h_a},  \\quad \\frac{1}{h_c}<\\frac{1}{h_a}+\\frac{1}{h_b}.</math>\n\n==Internal angle bisectors and incenter==\n\nThe internal angle bisectors are segments in the interior of the triangle reaching from one vertex to the opposite side and bisecting the vertex angle into two equal angles. The angle bisectors ''t''<sub>''a''</sub> etc. satisfy\n\n:<math>t_a+t_b+t_c \\leq \\frac{3}{2}(a+b+c)</math>\n\nin terms of the sides, and\n\n:<math>h_a \\leq t_a \\leq m_a</math>\n\nin terms of the altitudes and medians, and likewise for ''t''<sub>''b''</sub> and ''t''<sub>''c''</sub> .<ref name=PL/>{{rp|pp. 271–3}} Further,<ref name=Crux/>{{rp|p.224,#132}}\n\n:<math>\\sqrt{m_a}+\\sqrt{m_b}+\\sqrt{m_c} \\geq \\sqrt{t_a}+\\sqrt{t_b}+\\sqrt{t_c}</math>\n\nin terms of the medians, and<ref name=Crux/>{{rp|p.125,#3005}}\n\n:<math>\\frac{h_a}{t_a}+\\frac{h_b}{t_b}+\\frac{h_c}{t_c}\\geq 1+\\frac{4r}{R}</math>\n\nin terms of the altitudes, inradius ''r'' and circumradius ''R''.\n\nLet ''T''<sub>''a''</sub> ,  ''T''<sub>''b''</sub> , and ''T''<sub>''c''</sub> be the lengths of the angle bisectors extended to the circumcircle. Then<ref name=Crux/>{{rp|p.11,#535}}\n\n:<math>T_aT_bT_c \\geq \\frac{8\\sqrt{3}}{9}abc,</math>\n\nwith equality only in the equilateral case, and<ref name=Crux/>{{rp|p.14,#628}}\n\n:<math>T_a+T_b+T_c \\leq 5R +2r</math>\n\nfor circumradius ''R'' and inradius ''r'', again with equality only in the equilateral case. In addition,.<ref name=Crux/>{{rp|p.20,#795}}\n\n:<math>T_a+T_b+T_c \\geq \\frac{4}{3}(t_a+t_b+t_c).</math>\n\nFor [[incenter]] ''I'' (the intersection of the internal angle bisectors),<ref name=Crux/>{{rp|p.127,#3033}}\n\n:<math>6r \\leq AI+BI+CI \\leq \\sqrt{12(R^2-Rr+r^2)}.</math>\n\nFor midpoints ''L, M, N'' of the sides,<ref name=Crux/>{{rp|p.152,#J53}}\n\n:<math>IL^2+IM^2+IN^2 \\geq r(R+r).</math>\n\nFor incenter ''I'', [[centroid]] ''G'', [[circumcenter]] ''O'', [[nine-point center]] ''N'', and [[orthocenter]] ''H'', we have for non-equilateral triangles the distance inequalities<ref name=Franzsen>[http://forumgeom.fau.edu/FG2011volume11/FG201126.pdf Franzsen, William N.. \"The distance from the incenter to the Euler line\", ''Forum Geometricorum'' 11 (2011): 231–236.]</ref>{{rp|p.232}}\n\n:<math>IG<HG,</math>\n\n:<math>IH<HG,</math>\n\n:<math>IG<IO,</math>\n\nand\n\n:<math>IN < \\frac{1}{2}IO;</math>\n\nand we have the angle inequality<ref name=Franzsen/>{{rp|p.233}}\n\n:<math>\\angle IOH < \\frac{\\pi}{6}.</math>\n\nIn addition,<ref name=Franzsen/>{{rp|p.233,Lemma 3}}\n\n:<math>IG < \\frac{1}{3}v,</math>\n\nwhere ''v'' is the longest median.\n\nThree triangles with vertex at the incenter, ''OIH'', ''GIH'', and ''OGI'', are obtuse:<ref name=Franzsen/>{{rp|p.232}}\n\n:<math>\\angle OIH </math> > <math> \\angle GIH</math>  > 90° , <math> \\angle OGI </math> > 90°.\n\nSince these triangles have the indicated obtuse angles, we have\n\n:<math>OI^2+IH^2 < OH^2, \\quad GI^2+IH^2 < GH^2, \\quad OG^2+GI^2 < OI^2,</math>\n\nand in fact the second of these is equivalent to a result stronger than the first, shown by [[Euler]]:<ref>L. Euler, \"Solutio facilis problematum quorundam geometricorum difficillimorum\", ''Novi Comm. Acad. Scie. Petropolitanae'' 11 (1765); reprinted in ''Opera Omnia, serie prima'', vol. 26 (A. Speiser,\ned.), n. 325, 139–157.</ref><ref>{{cite journal | last1 = Stern | first1 = Joseph | year = 2007 | title = Euler's triangle determination problem | url = http://forumgeom.fau.edu/FG2007volume7/FG200701index.html | journal = Forum Geometricorum | volume = 7 | issue = | pages = 1–9 }}</ref>\n\n:<math>  OI^2 < OH^2 - 2 \\cdot IH^2 <  2\\cdot OI^2.</math>\n\nThe larger of two angles of a triangle has the shorter internal angle bisector:<ref name=ac>Altshiller-Court, Nathan. ''College Geometry''. Dover Publications, 2007.</ref>{{rp|p.72,#114}}\n\n:<math>\\text{If} \\quad A>B \\quad \\text{then} \\quad t_a<t_b.</math>\n\n==Perpendicular bisectors of sides==\n\nThese inequalities deal with the lengths ''p''<sub>''a''</sub> etc. of the triangle-interior portions of the perpendicular bisectors of sides of the triangle. Denoting the sides so that <math>a \\geq b \\geq c,</math> we have<ref name=Mitchell>Mitchell, Douglas W. \"Perpendicular bisectors of triangle sides\", ''Forum Geometricorum'' 13, 2013, 53–59: Theorem 4. http://forumgeom.fau.edu/FG2013volume13/FG201307index.html</ref>\n\n:<math>p_a \\geq p_b</math>\n\nand\n\n:<math>p_c \\geq p_b.</math>\n\n==Segments from an arbitrary point==\n===Interior point===\n\nConsider any point ''P'' in the interior of the triangle, with the triangle's vertices denoted ''A'', ''B'', and ''C'' and with the lengths of line segments denoted ''PA'' etc. We have<ref name=PL/>{{rp|pp. 275–7}}\n\n:<math>2(PA+PB+PC) > AB+BC+CA > PA+PB+PC,</math>\n\nand more strongly than the second of these inequalities is<ref name=PL/>{{rp|p. 278}}\n\n:<math>PA+PB+PC \\leq AC+BC, \\quad PA+PB+PC \\leq AB+BC, \\quad PA+PB+PC \\leq AB+AC.</math>\n\nWe also have [[Ptolemy's inequality]]<ref name=Crux/>{{rp|p.19,#770}}\n\n:<math>PA \\cdot BC + PB \\cdot CA > PC \\cdot AB</math>\n\nfor interior point P and likewise for cyclic permutations of the vertices.\n\nIf we draw perpendiculars from interior point ''P'' to the sides of the triangle, intersecting the sides at ''D'', ''E'', and ''F'', we have<ref name=PL/>{{rp|p. 278}}\n\n:<math>PA \\cdot PB \\cdot PC \\geq (PD+PE)(PE+PF)(PF+PD).</math>\n\nFurther, the [[Erdős–Mordell inequality]] states that<ref>{{citation\n | last1 = Alsina | first1 = Claudi\n | last2 = Nelsen | first2 = Roger B.\n | journal = Forum Geometricorum\n | pages = 99–102\n | title = A visual proof of the Erdős–Mordell inequality\n | url = http://forumgeom.fau.edu/FG2007volume7/FG200711index.html\n | volume = 7\n | year = 2007}}. http://forumgeom.fau.edu/FG2007volume7/FG200711index.html</ref>\n<ref>{{citation\n | last = Bankoff | first = Leon | author-link = Leon Bankoff\n | journal = [[American Mathematical Monthly]]\n | page = 521\n | title = An elementary proof of the Erdős–Mordell theorem\n | issue = 7\n | jstor = 2308580\n | volume = 65\n | year = 1958\n | doi=10.2307/2308580}}.</ref>\n\n:<math>\\frac{PA+PB+PC}{PD+PE+PF} \\geq 2</math>\n\nwith equality in the equilateral case. More strongly, [[Barrow's inequality]] states that if the interior bisectors of the angles at interior point ''P'' (namely, of ∠''APB'', ∠''BPC'', and ∠''CPA'') intersect the triangle's sides at ''U'', ''V'', and ''W'', then<ref>{{citation\n | last = Mordell | first = L. J. | author-link = Louis J. Mordell\n | issue = 357\n | journal = Mathematical Gazette\n | jstor = 3614019\n | pages = 213–215\n | title = On geometric problems of Erdös and Oppenheim\n | volume = 46\n | year = 1962}}.</ref>\n\n:<math>\\frac{PA+PB+PC}{PU+PV+PW} \\geq 2.</math>\n\nAlso stronger than the Erdős–Mordell inequality is the following:<ref>Dao Thanh Oai, Nguyen Tien Dung, and Pham Ngoc Mai, \"A strengthened version of the Erdős-Mordell inequality\", ''Forum Geometricorum'' 16 (2016), pp. 317--321, Theorem 2  http://forumgeom.fau.edu/FG2016volume16/FG201638.pdf</ref> Let ''D, E, F'' be the orthogonal projections of ''P'' onto ''BC, CA, AB'' respectively, and ''H, K, L'' be the orthogonal projections of ''P'' onto the tangents to the triangle's circumcircle at ''A, B, C'' respectively. Then\n\n:<math>PH + PK + PL \\ge 2(PD + P E + P F ).</math>\n\nWith orthogonal projections ''H, K, L'' from ''P'' onto the tangents to the triangle's circumcircle at ''A, B, C'' respectively, we have<ref>Dan S ̧tefan Marinescu and Mihai Monea, \"About a Strengthened Version of the Erdo ̋s-Mordell Inequality\", ''Forum Geometricorum'' Volume 17 (2017), pp. 197–202, Corollary 7. http://forumgeom.fau.edu/FG2017volume17/FG201723.pdf</ref>\n \n:<math>\\frac{PH}{a^2}+\\frac{PK}{b^2}+\\frac{PL}{c^2}\\ge \\frac{1}{R}</math>\nwhere ''R'' is the circumradius.\n\nAgain with distances ''PD, PE, PF'' of the interior point ''P'' from the sides we have these three inequalities:<ref name=Crux/>{{rp|p.29,#1045}}\n\n:<math>\\frac{PA^2}{PE\\cdot PF}+\\frac{PB^2}{PF\\cdot PD}+\\frac{PC^2}{PD\\cdot PE} \\geq 12;</math>\n\n:<math>\\frac{PA}{\\sqrt{PE\\cdot PF}}+\\frac{PB}{\\sqrt{PF\\cdot PD}}+\\frac{PC}{\\sqrt{PD\\cdot PE}}\\geq 6;</math>\n\n:<math>\\frac{PA}{PE+PF}+\\frac{PB}{PF+PD}+\\frac{PC}{PD+PE}\\geq 3.</math>\n\nFor interior point ''P'' with distances ''PA, PB, PC'' from the vertices and with triangle area ''T'',<ref name=Crux/>{{rp|p.37,#1159}}\n\n:<math>(b+c)PA+(c+a)PB+(a+b)PC \\geq 8T</math>\n\nand<ref name=Crux/>{{rp|p.26,#965}}\n\n:<math>\\frac{PA}{a}+\\frac{PB}{b}+\\frac{PC}{c} \\geq \\sqrt{3}.</math>\n\nFor an interior point ''P'', centroid ''G'', midpoints ''L, M, N'' of the sides, and semiperimeter ''s'',<ref name=Crux/>{{rp|p.140,#3164}}<ref name=Crux/>{{rp|p.130,#3052}}\n\n:<math>2(PL+PM+PN) \\leq 3PG+PA+PB+PC \\leq s + 2(PL+PM+PN) .</math>\n\nMoreover, for positive numbers ''k''<sub>1</sub>, ''k''<sub>2</sub>, ''k''<sub>3</sub>, and ''t'' with ''t'' less than or equal to 1:<ref name=Janous>Janous, Walther. \"Further inequalities of Erdos–Mordell type\", ''Forum Geometricorum'' 4, 2004, 203–206. http://forumgeom.fau.edu/FG2004volume4/FG200423index.html</ref>{{rp|Thm.1}}\n\n:<math>k_1\\cdot (PA)^t + k_2\\cdot (PB)^t + k_3\\cdot (PC)^t \\geq 2^t \\sqrt{k_1k_2k_3} \\left(\\frac{(PD)^t}{\\sqrt{k_1}} + \\frac{(PE)^t}{\\sqrt{k_2}} + \\frac{(PF)^t}{\\sqrt{k_3}} \\right),</math>\n\nwhile for ''t'' > 1 we have<ref name=Janous/>{{rp|Thm.2}}\n\n:<math>k_1\\cdot (PA)^t + k_2\\cdot (PB)^t + k_3\\cdot (PC)^t \\geq 2 \\sqrt{k_1k_2k_3} \\left(\\frac{(PD)^t}{\\sqrt{k_1}} + \\frac{(PE)^t}{\\sqrt{k_2}} + \\frac{(PF)^t}{\\sqrt{k_3}} \\right).</math>\n\n===Interior or exterior point===\n\nThere are various inequalities for an arbitrary interior or exterior point in the plane in terms of the radius ''r'' of the triangle's inscribed circle. For example,<ref name=Sandor>Sandor, Jozsef. \"On the geometry of equilateral triangles\", ''Forum Geometricorum'' 5, 2005, 107–117. http://forumgeom.fau.edu/FG2005volume5/FG200514index.html</ref>{{rp|p. 109}}\n\n:<math>PA+PB+PC \\geq 6r.</math>\n\nOthers include:<ref>Mansour, Toufik, and Shattuck, Mark. \"On a certain cubic geometric inequality\", ''Forum Geometricorum'' 11, 2011, 175–181. http://forumgeom.fau.edu/FG2011volume11/FG201118index.html</ref>{{rp|pp. 180–1}}\n\n:<math>PA^3+PB^3+PC^3 + k \\cdot (PA \\cdot PB \\cdot PC) \\geq8(k+3)r^3</math>\n\nfor ''k'' = 0, 1, ..., 6;\n\n:<math>PA^2+PB^2+PC^2 + (PA \\cdot PB \\cdot PC)^{2/3} \\geq 16r^2;</math>\n\n:<math>PA^2+PB^2+PC^2 + 2(PA \\cdot PB \\cdot PC)^{2/3} \\geq 20r^2;</math>\n\nand\n\n:<math>PA^4+PB^4+PC^4 + k(PA \\cdot PB \\cdot PC)^{4/3} \\geq 16(k+3)r^4</math>\n\nfor ''k'' = 0, 1, ..., 9.\n\nFurthermore, for circumradius ''R'',\n\n:<math>(PA \\cdot PB)^{3/2} + (PB \\cdot PC)^{3/2} + (PC \\cdot PA)^{3/2} \\geq 12Rr^2;</math><ref name=MS2>Mansour, Toufik  and Shattuck, Mark.  \"Improving upon a geometric inequality of third order\", ''Forum Geometricorum'' 12, 2012, 227–235. http://forumgeom.fau.edu/FG2012volume12/FG201221index.html</ref>{{rp|p. 227}}\n\n:<math>(PA \\cdot PB)^{2} + (PB \\cdot PC)^{2} + (PC \\cdot PA)^{2} \\geq 8(R+r)Rr^2;</math><ref name=MS2/>{{rp|p. 233}}\n\n:<math>(PA \\cdot PB)^{2} + (PB \\cdot PC)^{2} + (PC \\cdot PA)^{2} \\geq 48r^4;</math><ref name=MS2/>{{rp|p. 233}}\n\n:<math>(PA \\cdot PB)^{2} + (PB \\cdot PC)^{2} + (PC \\cdot PA)^{2} \\geq 6(7R-6r)r^3.</math><ref name=MS2/>{{rp|p. 233}}\n\n==Inradius, exradii, and circumradius==\n\n===Inradius and circumradius===\n\nThe [[Euler inequality]] for the [[circumradius]] ''R'' and the [[inradius]] ''r'' states that\n\n:<math>\\frac{R}{r} \\geq 2,</math>\n\nwith equality only in the [[equilateral triangle|equilateral]] case.<ref>Dragutin Svrtan and Darko Veljan, \"Non-Euclidean versions of some classical triangle inequalities\", ''Forum Geometricorum'' 12 (2012),  197–209.  http://forumgeom.fau.edu/FG2012volume12/FG201217index.html</ref>{{rp|p. 198}}\n\nA stronger version<ref name=SV/>{{rp|p. 198}} is\n\n:<math>\\frac{R}{r} \\geq \\frac{abc+a^3+b^3+c^3}{2abc} \\geq \\frac{a}{b}+\\frac{b}{c}+\\frac{c}{a}-1 \\geq \\frac{2}{3} \\left(\\frac{a}{b}+\\frac{b}{c}+\\frac{c}{a} \\right) \\geq 2.</math>\n\nBy comparison,<ref name=Crux/>{{rp|p.183,#276.2}}\n\n:<math>\\frac{r}{R} \\geq \\frac{4abc-a^3-b^3-c^3}{2abc},</math>\n\nwhere the right side could be positive or negative.\n\nTwo other refinements of Euler's inequality are<ref name=Crux/>{{rp|p.134,#3087}}\n\n:<math> \\frac{R}{r} \\geq \\frac{(b+c)}{3a}+\\frac{(c+a)}{3b}+\\frac{(a+b)}{3c} \\geq 2</math>\n\nand\n\n:<math>\\left( \\frac{R}{r} \\right)^3 \\geq \\left( \\frac{a}{b}+\\frac{b}{a}\\right)\\left(\\frac{b}{c}+\\frac{c}{b}\\right) \\left( \\frac{c}{a}+\\frac{a}{c}\\right) \\geq 8.</math>\n\nAnother symmetric inequality is<ref name=Crux/>{{rp|p.125,#3004}}\n\n:<math> \\frac{\\left(\\sqrt{a}-\\sqrt{b}\\right)^2+\\left(\\sqrt{b}-\\sqrt{c}\\right)^2+\\left(\\sqrt{c}-\\sqrt{a}\\right)^2}{\\left(\\sqrt{a}+\\sqrt{b}+\\sqrt{c}\\right)^2}\\leq \\frac{4}{9}\\left(\\frac{R}{r}-2\\right).</math>\n\nMoreover,\n\n:<math>\\frac{R}{r} \\geq \\frac{2(a^2+b^2+c^2)}{ab+bc+ca};</math><ref name=PL/>{{rp|288}}\n\n:<math>a^3+b^3+c^3 \\leq 8s(R^2-r^2)</math>\n\nin terms of the semiperimeter ''s'';<ref name=Crux/>{{rp|p.20,#816}}\n\n:<math>r(r+4R) \\geq \\sqrt{3} \\cdot T</math>\n\nin terms of the area ''T'';<ref name=SV/>{{rp|p. 201}}\n\n:<math>s\\sqrt{3} \\leq r+4R</math> <ref name=SV/>{{rp|p. 201}}\n\nand\n\n:<math>s^2 \\geq 16Rr - 5r^2</math> <ref name=Crux/>{{rp|p.17#708}}\n\nin terms of the semiperimeter ''s''; and\n\n:<math>2R^2+10Rr-r^2-2(R-2r)\\sqrt{R^2-2Rr} \\leq s^2</math>\n::<math>\\leq 2R^2+10Rr-r^2+2(R-2r)\\sqrt{R^2-2Rr}</math>\n\nalso in terms of the semiperimeter.<ref name=SV/>{{rp|p. 206}}<ref name=Birsan>{{cite journal | last1 = Birsan | first1 = Temistocle | year = 2015 | title = Bounds for elements of a triangle expressed by R, r, and s | url = http://forumgeom.fau.edu/FG2015volume15/FG201508.pdf | format = PDF | journal = Forum Geometricorum | volume = 15 | issue = | pages = 99–103 }}</ref>{{rp|p. 99}} Here the expression <math>\\sqrt{R^2-2Rr}=d</math> where ''d'' is the distance between the incenter and the circumcenter. In the latter double inequality, the first part holds with equality if and only if the triangle is isosceles with an [[apex (geometry)|apex]] angle of at least 60°, and the last part holds with equality if and only if the triangle is isosceles with an apex angle of at most 60°. Thus both are equalities if and only if the triangle is equilateral.<ref name=Birsan/>{{rp|Thm. 1}}\n\nWe also have for any side ''a''<ref name=MK>Yurii, N. Maltsev and Anna S. Kuzmina, \"An improvement of Birsan's inequalities for the sides of a triangle\", ''Forum Geometricorum'' 16, 2016, pp. 81−84.</ref>\n\n:<math>(R-d)^2-r^2 \\le 4R^2 r^2\\left(\\frac{(R+d)^2-r^2}{(R+d)^4} \\right) \\le \\frac{a^2}{4} \\le Q \\le (R+d)^2-r^2,</math>\n\nwhere <math>Q=R^2</math> if the [[circumcenter]] is on or outside of the [[incircle]] and <math>Q=4R^2 r^2 \\left(\\frac{(R-d)^2-r^2}{(R-d)^4}\\right)</math> if the circumcenter is inside the incircle. The circumcenter is inside the incircle if and only if<ref name=MK/>\n\n:<math>\\frac{R}{r} <\\sqrt{2}+1.</math>\n\nFurther,\n\n:<math>\\frac{9r}{2T} \\leq \\frac{1}{a}+\\frac{1}{b}+\\frac{1}{c} \\leq \\frac{9R}{4T}.</math><ref name=PL/>{{rp|p. 291}}\n\n'''Blundon's inequality''' states that<ref name=SV/>{{rp|p. 206;}}<ref>{{cite journal | last1 = Blundon | first1 = W. J. | year = 1965 | title = Inequalities associated with the triangle | url = | journal = Canad. Math. Bull. | volume = 8 | issue = | pages = 615–626 | doi=10.4153/cmb-1965-044-9}}</ref><ref>Dorin Andrica, Cătălin Barbu. \"A Geometric Proof of Blundon’s Inequalities\", ''Mathematical Inequalities & Applications'', Volume 15, Number 2 (2012), 361–370. http://mia.ele-math.com/15-30/A-geometric-proof-of-Blundon-s-inequalities</ref>\n\n:<math>s \\leq (3\\sqrt{3}-4)r+2R.</math>\n\nWe also have, for all acute triangles,<ref>Miha ́ly Bencze and Marius Dra ̆gan, “The Blundon Theorem in an Acute Triangle and Some Consequences”,''Forum Geometricorum'' 18, 2018, pp. 185–194.  http://forumgeom.fau.edu/FG2018volume18/FG201825.pdf</ref>\n\n:<math>s > 2R+r.</math>\n\nFor incircle center ''I'', let ''AI'', ''BI'', and ''CI'' extend beyond ''I'' to intersect the circumcircle at ''D'', ''E'', and ''F'' respectively. Then<ref name=Crux/>{{rp|p.14,#644}}\n\n:<math>\\frac{AI}{ID} + \\frac{BI}{IE} + \\frac{CI}{IF} \\geq 3.</math>\n\nIn terms of the vertex angles we have <ref name=Crux/>{{rp|p.193,#342.6}}\n\n:<math>\\cos A \\cdot \\cos B \\cdot \\cos C \\leq \\left( \\frac{r}{R\\sqrt{2}} \\right)^2.</math>\n\nDenote as <math>R_A , R_B , R_C</math> the radii of the tangent circles at the vertices to the triangle's circumcircle and to the opposite sides. Then<ref name=AM>Dorin Andrica and Dan S ̧tefan Marinescu. \"New Interpolation Inequalities to Euler’s R ≥ 2r\".  ''Forum Geometricorum'', Volume 17 (2017), pp. 149–156.   http://forumgeom.fau.edu/FG2017volume17/FG201719.pdf</ref>{{rp|Thm. 4}}\n\n:<math>\\frac{4}{R}\\le \\frac{1}{R_A}+\\frac{1}{R_B}+\\frac{1}{R_C}\\le \\frac{2}{r}</math>\n\nwith equality only in the equilateral case, and<ref name=AM/>{{rp|Thm. 6}}\n\n:<math>\\frac{9}{2}r\\le R_A+R_B+R_C \\le \\frac{9}{4}R</math>\n\nwith equality only in the equilateral case.\n\n===Circumradius and other lengths===\n\nFor the circumradius ''R'' we have<ref name=Crux/>{{rp|p.101,#2625}}\n\n:<math>18R^3\\geq (a^2+b^2+c^2)R+abc\\sqrt{3}</math>\n\nand<ref name=Crux/> {{rp|p.35,#1130}}\n\n:<math>a^{2/3}+b^{2/3}+c^{2/3} \\leq 3^{7/4}R^{3/2}.</math>\n\nWe also have<ref name=PL/>{{rp|pp. 287–90}}\n\n:<math>a+b+c \\leq 3\\sqrt{3} \\cdot R,</math>\n\n:<math>9R^2 \\geq a^2+b^2+c^2,</math>\n\n:<math>h_a+h_b+h_c \\leq 3\\sqrt{3} \\cdot R</math>\n\nin terms of the altitudes,\n\n:<math>m_a^2+m_b^2+m_c^2 \\leq \\frac{27}{4}R^2</math>\n\nin terms of the medians, and<ref name=Crux/>{{rp|p.26,#957}}\n\n:<math>\\frac{ab}{a+b}+\\frac{bc}{b+c}+\\frac{ca}{c+a} \\geq \\frac{2T}{R}</math>\n\nin terms of the area.\n\nMoreover, for circumcenter ''O'', let lines ''AO'', ''BO'', and ''CO'' intersect the opposite sides ''BC'', ''CA'', and ''AB'' at ''U'', ''V'', and ''W'' respectively. Then<ref name=Crux/>{{rp|p.17,#718}}\n\n:<math>OU+OV + OW \\geq \\frac{3}{2}R.</math>\n\nFor an acute triangle the distance between the circumcenter ''O'' and the orthocenter ''H'' satisfies<ref name=Crux/>{{rp|p.26,#954}}\n\n:<math>OH < R,</math>\n\nwith the opposite inequality holding for an obtuse triangle.\n\nThe circumradius is at least twice the distance between the first and second [[Brocard points]] ''B''<sub>1</sub> and ''B''<sub>2</sub>:<ref>Scott, J. A. \"Some examples of the use of areal coordinates in triangle geometry\", ''[[Mathematical Gazette]]'' 83, November 1999, 472–477.</ref>\n\n:<math>R \\ge 2B_1B_2.</math>\n\n===Inradius, exradii, and other lengths===\n\nFor the inradius ''r'' we have<ref name=PL/>{{rp|pp. 289–90}}\n\n:<math>\\frac{1}{a}+\\frac{1}{b}+\\frac{1}{c} \\leq \\frac{\\sqrt{3}}{2r},</math>\n\n:<math>9r \\leq h_a+h_b+h_c</math>\n\nin terms of the altitudes, and\n\n:<math>\\sqrt{r_a^2+r_b^2+r_c^2} \\geq 6r</math>\n\nin terms of the radii of the excircles. We additionally have\n\n:<math>\\sqrt{s}(\\sqrt{a}+\\sqrt{b}+\\sqrt{c}) \\leq \\sqrt{2}(r_a+r_b+r_c)</math><ref name=Crux/>{{rp|p.66,#1678}}\n\nand\n\n:<math>\\frac{abc}{r} \\geq \\frac{a^3}{r_a}+\\frac{b^3}{r_b}+\\frac{c^3}{r_c}.</math><ref name=Crux/>{{rp|p.183,#281.2}}\n\nThe exradii and medians are related by<ref name=Crux/>{{rp|p.66,#1680}}\n\n:<math>\\frac{r_ar_b}{m_am_b}+\\frac{r_br_c}{m_bm_c}+\\frac{r_cr_a}{m_cm_a} \\geq 3.</math>\n\nIn addition, for an acute triangle the distance between the incircle center ''I'' and orthocenter ''H'' satisfies<ref name=Crux/>{{rp|p.26,#954}}\n\n:<math>IH < r\\sqrt{2},</math>\n\nwith the reverse inequality for an obtuse triangle.\n\nAlso, an acute triangle satisfies<ref name=Crux/>{{rp|p.26,#954}}\n\n:<math>r^2+r_a^2+r_b^2+r_c^2 < 8R^2,</math>\n\nin terms of the circumradius ''R'', again with the reverse inequality holding for an obtuse triangle.\n\nIf the internal angle bisectors of angles ''A'', ''B'', ''C'' meet the opposite sides at ''U'', ''V'', ''W'' then<ref name=Crux/>{{rp|p.215,32nd IMO,#1}}\n\n:<math>\\frac{1}{4} < \\frac{AI\\cdot BI \\cdot CI}{AU \\cdot BV \\cdot CW} \\leq \\frac{8}{27}.</math>\n\nIf the internal angle bisectors through incenter ''I'' extend to meet the circumcircle at ''X'', ''Y'' and ''Z'' then <ref name=Crux/>{{rp|p.181,#264.4}}\n\n:<math>\\frac{1}{IX}+\\frac{1}{IY}+\\frac{1}{IZ} \\geq \\frac{3}{R}</math>\n\nfor circumradius ''R'', and<ref name=Crux/>{{rp|p.181,#264.4}}<ref name=Crux/>{{rp|p.45,#1282}}\n\n:<math>0\\leq (IX-IA)+(IY-IB)+(IZ-IC) \\leq 2(R-2r). </math>\n\nIf the incircle is tangent to the sides at ''D'', ''E'', ''F'', then<ref name=Crux/>{{rp|p.115,#2875}}\n\n:<math>EF^2+FD^2+DE^2 \\leq \\frac{s^2}{3}</math>\n\nfor semiperimeter ''s''.\n\n==Inscribed figures==\n===Inscribed hexagon===\n\nIf a [[tangential polygon|tangential hexagon]] is formed by drawing three segments tangent to a triangle's incircle and parallel to a side, so that the hexagon is inscribed in the triangle with its other three sides coinciding with parts of the triangle's sides, then<ref name=Crux/>{{rp|p.42,#1245}}\n\n:<math>\\text{Perimeter of hexagon} \\leq \\frac{2}{3}(\\text{Perimeter of triangle}).</math>\n\n===Inscribed triangle===\n\nIf three points D, E, F on the respective sides AB, BC, and CA of a reference triangle ABC are the vertices of an inscribed triangle, which thereby partitions the reference triangle into four triangles, then the area of the inscribed triangle is greater than the area of at least one of the other interior triangles, unless the vertices of the inscribed triangle are at the midpoints of the sides of the reference triangle (in which case the inscribed triangle is the [[medial triangle]] and all four interior triangles have equal areas):<ref name=Torrejon/>{{rp|p.137}}\n\n:<math>\\text{Area(DEF)} \\ge \\text{min(Area(BED), Area(CFE), Area(ADF))}.</math>\n\n===Inscribed squares===\n\nAn acute triangle has three [[inscribed figure|inscribed squares]], each with one side coinciding with part of a side of the triangle and with the square's other two vertices on the remaining two sides of the triangle. (A right triangle has only two distinct inscribed squares.) If one of these squares has side length ''x''<sub>''a''</sub> and another has side length ''x''<sub>''b''</sub> with ''x''<sub>''a''</sub> < ''x''<sub>''b''</sub>, then<ref name=Ox>Oxman, Victor,  and Stupel, Moshe. \"Why are the side lengths of the squares inscribed in a triangle so close to each other?\" ''Forum Geometricorum'' 13, 2013, 113–115. http://forumgeom.fau.edu/FG2013volume13/FG201311index.html</ref>{{rp|p. 115}}\n\n:<math>1 \\geq \\frac{x_a}{x_b} \\geq \\frac{2\\sqrt{2}}{3} \\approx 0.94.</math>\n\nMoreover, for any square inscribed in any triangle we have<ref name=Crux/>{{rp|p.18,#729}}<ref name=Ox/>\n\n:<math>\\frac{\\text{Area of triangle}}{\\text{Area of inscribed square}} \\geq 2.</math>\n\n==Euler line==\n\nA triangle's [[Euler line]] goes through its [[orthocenter]], its [[circumcenter]], and its [[centroid]], but does not go through its [[incenter]] unless the triangle is [[isosceles triangle|isosceles]].<ref name=Franzsen/>{{rp|p.231}} For all non-isosceles triangles, the distance ''d'' from the incenter to the Euler line satisfies the following inequalities in terms of the triangle's longest [[median (geometry)|median]] ''v'', its longest side ''u'', and its semiperimeter ''s'':<ref name=Franzsen/>{{rp|p. 234,Propos.5}}\n\n:<math>\\frac{d}{s} < \\frac{d}{u} < \\frac{d}{v} < \\frac{1}{3}.</math>\n\nFor all of these ratios, the upper bound of 1/3 is the tightest possible.<ref name=Franzsen/>{{rp|p.235,Thm.6}}\n\n==Right triangle==\n\nIn [[right triangle]]s the legs ''a'' and ''b'' and the [[hypotenuse]] ''c'' obey the following, with equality only in the isosceles case:<ref name=PL/>{{rp|p. 280}}\n\n:<math>a+b \\leq c\\sqrt{2}.</math>\n\nIn terms of the inradius, the hypotenuse obeys<ref name=PL/>{{rp|p. 281}}\n\n:<math>2r \\leq c(\\sqrt{2}-1),</math>\n\nand in terms of the altitude from the hypotenuse the legs obey<ref name=PL/>{{rp|p. 282}}\n\n:<math>h_c \\leq \\frac{\\sqrt{2}}{4}(a+b).</math>\n\n==Isosceles triangle==\n\nIf the two equal sides of an [[isosceles triangle]] have length ''a'' and the other side has length ''c'', then the internal [[angle bisector]] ''t'' from one of the two equal-angled vertices satisfies<ref name=Crux/>{{rp|p.169,#<math>\\eta</math>44}}\n\n:<math>\\frac{2ac}{a+c} > t > \\frac{ac\\sqrt{2}}{a+c}.</math>\n\n==Equilateral triangle==\n\nFor any point ''P'' in the plane of an [[equilateral triangle]] ''ABC'', the distances of ''P'' from the vertices, ''PA'', ''PB'', and ''PC'', are such that, unless ''P'' is on the triangle's [[circumcircle]], they obey the basic triangle inequality and thus can themselves form the sides of a triangle:<ref name=PL/>{{rp|p. 279}}\n\n:<math>PA+PB > PC, \\quad PB+PC > PA, \\quad PC+PA > PB.</math>\n\nHowever, when ''P'' is on the circumcircle the sum of the distances from ''P'' to the nearest two vertices exactly equals the distance to the farthest vertex.\n\nA triangle is equilateral if and only if, for ''every'' point ''P'' in the plane, with distances ''PD'', ''PE'', and ''PF'' to the triangle's sides and distances ''PA'', ''PB'', and ''PC'' to its vertices,<ref name=Crux/>{{rp|p.178,#235.4}}\n\n::<math>4(PD^2+PE^2+PF^2) \\geq PA^2+PB^2+PC^2.</math>\n\n==Two triangles==\n\n[[Pedoe's inequality]] for two triangles, one with sides ''a'', ''b'', and ''c'' and area ''T'', and the other with sides ''d'', ''e'', and ''f'' and area  ''S'', states that\n\n:<math>d^2(b^2+c^2-a^2)+e^2(a^2+c^2-b^2)+f^2(a^2+b^2-c^2)\\geq 16TS,</math>\n\nwith equality [[if and only if]] the two triangles are [[similarity (geometry)|similar]].\n\nThe [[hinge theorem]] or open-mouth theorem states that if two sides of one triangle are congruent to two sides of another triangle, and the included angle of the first is larger than the included angle of the second, then the third side of the first triangle is longer than the third side of the second triangle. That is, in triangles ''ABC'' and ''DEF'' with sides ''a'', ''b'', ''c'', and ''d'', ''e'', ''f'' respectively (with ''a'' opposite ''A'' etc.), if ''a'' = ''d'' and ''b'' = ''e'' and angle ''C'' > angle ''F'', then\n\n:<math> c>f.</math>\n\nThe converse also holds: if ''c'' > ''f'', then ''C'' > ''F''.\n\nThe angles in any two triangles ''ABC'' and ''DEF'' are related in terms of the [[cotangent]] function according to<ref name=Scott/>\n\n:<math>\\cot A (\\cot E + \\cot F) + \\cot B(\\cot F+\\cot D) + \\cot C(\\cot D + \\cot E) \\geq 2.</math>\n\n==Non-Euclidean triangles==\n\nIn a [[Solution of triangles#Solving spherical triangles|triangle on the surface of a sphere]], as well as in [[elliptic geometry]],\n\n:<math>\\angle A+\\angle B+\\angle C >180\\text{°}.</math>\n\nThis inequality is reversed for [[hyperbolic triangle]]s.\n\n==See also==\n\n*[[List of inequalities]]\n*[[List of triangle topics]]\n*[[Quadrilateral#Inequalities]]\n*[[Quadrilateral#Maximum and minimum properties]]\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Triangle inequalities}}\n[[Category:Triangle geometry|Inequalities]]\n[[Category:Mathematics-related lists]]\n[[Category:Geometric inequalities]]"
    },
    {
      "title": "Triangle inequality",
      "url": "https://en.wikipedia.org/wiki/Triangle_inequality",
      "text": "{{about|the basic inequality <math>z\\le x+y</math><!-- DO NOT USE {{math}}. IT DOES NOT WORK INSIDE HATNOTES. -->|other inequalities associated with triangles|List of triangle inequalities}}\n\n[[File:TriangleInequality.svg|thumb|Three examples of the triangle inequality for triangles with sides of lengths {{math|''x''}}, {{math|''y''}}, {{math|''z''}}. The top example shows a case where {{math|''z''}} is much less than the sum {{math|''x'' + ''y''}} of the other two sides,  and the bottom example shows a case where the side {{math|''z''}} is only slightly less than {{math|''x'' + ''y''}}.]]\n\nIn [[mathematics]], the '''triangle inequality''' states that for any [[triangle]], the sum of the lengths of any two sides must be greater than or equal to the length of the remaining side.<ref>Wolfram MathWorld – http://mathworld.wolfram.com/TriangleInequality.html</ref><ref name=Khamsi>\n{{cite book |title=An introduction to metric spaces and fixed point theory |author1=Mohamed A. Khamsi |author2=William A. Kirk |url=https://books.google.com/?id=4qXbEpAK5eUC&pg=PA8 |chapter=§1.4 The triangle inequality in {{math|ℝ<sup>n</sup>}} |isbn=0-471-41825-0 |year=2001 |publisher=Wiley-IEEE}}</ref> This statement permits the inclusion of [[Degeneracy (mathematics)#Triangle|degenerate triangles]], but some authors, especially those writing about elementary geometry, will exclude this possibility, thus leaving out the possibility of equality.<ref>for instance, {{citation|first=Harold R.|last=Jacobs|title=Geometry|year=1974|publisher=W. H. Freeman & Co.|isbn=0-7167-0456-0|page=246}}</ref> If {{math|''x''}}, {{math|''y''}}, and {{math|''z''}} are the lengths of the sides of the triangle, with no side being greater than {{math|''z''}}, then the triangle inequality states that\n:<math>z \\leq x + y ,</math>\nwith equality only in the degenerate case of a triangle with zero area.\nIn [[Euclidean geometry]] and some other geometries, the triangle inequality is a theorem about distances, and it is written using vectors and vector lengths ([[Norm (mathematics)|norms]]):\n:<math>\\|\\mathbf x + \\mathbf y\\| \\leq \\|\\mathbf x\\| + \\|\\mathbf y\\| ,</math>\nwhere the length {{math|''z''}} of the third side has been replaced by the vector sum {{math|'''x''' + '''y'''}}. When {{math|'''x'''}} and {{math|'''y'''}} are [[real number]]s, they can be viewed as vectors in {{math|ℝ<sup>1</sup>}}, and the triangle inequality expresses a relationship between [[absolute value]]s.\n\nIn Euclidean geometry, for [[right triangle]]s the triangle inequality is a consequence of the [[Pythagorean theorem]], and for general triangles, a consequence of the [[law of cosines]], although it may be proven without these theorems. The inequality can be viewed intuitively in either {{math|ℝ<sup>2</sup>}} or {{math|ℝ<sup>3</sup>}}. The figure at the right shows three examples beginning with clear inequality (top) and approaching equality (bottom). In the Euclidean case, equality occurs only if the triangle has a {{math|180°}} angle and two {{math|0°}} angles, making the three [[Vertex (geometry)|vertices]] [[Straight line|collinear]], as shown in the bottom example. Thus, in Euclidean geometry, the shortest distance between two points is a straight line.\n\nIn [[spherical geometry]], the shortest distance between two points is an arc of a [[great circle]], but the triangle inequality holds provided the restriction is made that the distance between two points on a sphere is the length of a minor spherical line segment (that is, one with central angle in {{math|[0, {{pi}}]}}) with those endpoints.<ref name= Ramos>\n\n{{cite book |title=Robotics: Science and Systems IV |author1=Oliver Brock |author2=Jeff Trinkle |author3=Fabio Ramos |url=https://books.google.com/?id=fvCaQfBQ7qEC&pg=PA195 |page=195 |isbn=0-262-51309-9 |publisher=MIT Press |year=2009}}\n\n</ref><ref name=Ramsay>\n\n{{cite book |title=Introduction to hyperbolic geometry |author1=Arlan Ramsay |author2=Robert D. Richtmyer |url=https://books.google.com/?id=0QA_1lKC0dwC&pg=PA17 |page=17 |isbn=0-387-94339-0 |year=1995 |publisher=Springer}}\n\n</ref>\n\nThe triangle inequality is a ''defining property'' of [[norm (mathematics)|norms]] and measures of [[Metric (mathematics)#Definition|distance]]. This property must be established as a theorem for any function proposed for such purposes for each particular space: for example, spaces such as the [[real number]]s, [[Euclidean space]]s, the [[Lp space|L<sup>p</sup> space]]s ({{math|''p'' ≥ 1}}), and [[inner product space]]s.\n\n==Euclidean geometry==\n\n[[File:Euclid triangle inequality.svg|thumb|Euclid's construction for proof of the triangle inequality for plane geometry.]]\n\nEuclid proved the triangle inequality for distances in [[Euclidean geometry|plane geometry]] using the construction in the figure.<ref name=Jacobs>\n\n{{cite book |page=201 |author=Harold R. Jacobs |title=Geometry: seeing, doing, understanding |url=https://books.google.com/?id=XhQRgZRDDq0C&pg=PA201 |isbn=0-7167-4361-2 |edition=3rd |publisher=Macmillan |year=2003}}\n\n</ref> Beginning with triangle {{math|ABC}}, an isosceles triangle is constructed with one side taken as {{math|BC}} and the other equal leg {{math|BD}} along the extension of side {{math|AB}}. It then is argued that angle {{math|''β'' > ''α''}}, so side {{math|{{overline|AD}} > {{overline|AC}}}}. But {{math|{{overline|AD}} {{=}} {{overline|AB}} + {{overline|BD}} {{=}} {{overline|AB}} + {{overline|BC}}}} so the sum of sides {{math|{{overline|AB}} + {{overline|BC}} > {{overline|AC}}}}. This proof appears in [[Euclid's Elements]], Book 1, Proposition 20.<ref name=Joyce>\n\n{{cite web\n| url         = http://aleph0.clarku.edu/~djoyce/java/elements/bookI/propI20.html\n| title       = Euclid's elements, Book 1, Proposition 20\n| author      = David E. Joyce\n| date        = \n| month       = \n| year        = 1997\n| work        = Euclid's elements\n| publisher   = Dept. Math and Computer Science, Clark University\n| location    = \n| page        = \n| pages       = \n| at          = \n| language    = \n|trans-title=| doi         = \n| archiveurl  = \n| archivedate = \n| accessdate  = 2010-06-25\n| quote       = \n| ref         = \n| separator   = \n| postscript  = \n}}\n\n</ref>\n\n===Mathematical expression of the constraint on the sides of a triangle===\n\nFor a proper triangle, the triangle inequality, as stated in words, literally translates into three inequalities (given that a proper triangle has side lengths {{math|''a''}}, {{math|''b''}}, {{math|''c''}} that are all positive and excludes the degenerate case of zero area):\n:<math>a + b > c ,\\quad b + c > a ,\\quad c + a > b .</math>\nA more succinct form of this inequality system can be shown to be\n:<math>|a - b| < c < a + b .</math>\nAnother way to state it is\n:<math>\\max(a,\\text{ }b,\\text{ }c) < a + b + c - \\max(a,\\text{ }b,\\text{ } c)</math>\nimplying\n:<math>2 \\max(a,\\text{ }b,\\text{ } c) < a + b + c</math>\nand thus that the longest side length is less than the [[semiperimeter]].\n\nA mathematically equivalent formulation is that the area of a triangle with sides ''a'', ''b'', ''c'' must be a real number greater than zero. [[Heron's formula]] for the area is\n\n:<math>\n\\begin{align}\n4\\cdot \\text{area} & =\\sqrt{(a+b+c)(-a+b+c)(a-b+c)(a+b-c)} \\\\\n& = \\sqrt{-a^4-b^4-c^4+2a^2b^2+2a^2c^2+2b^2c^2}.\n\\end{align}\n</math>\n\nIn terms of either area expression, the triangle inequality imposed on all sides is equivalent to the condition that the expression under the square root sign be real and greater than zero (so the area expression is real and greater than zero).\n\nThe triangle inequality provides two more interesting constraints for triangles whose sides are ''a, b, c'', where ''a &ge; b &ge; c'' and ''<math>\\phi</math>'' is the [[golden ratio]], as\n:<math>1<\\frac{a+c}{b}<3</math>\n\n:<math>1\\le\\min(\\frac{a}{b},\\text{ }\n \\frac{b}{c})<\\phi.</math><ref>''[[American Mathematical Monthly]]'', pp. 49-50, 1954.</ref>\n\n===Right triangle===\n\n[[File:Isosceles triangle made of right triangles.svg|thumb|Isosceles triangle with equal sides {{math|{{overline|AB}} {{=}} {{overline|AC}}}} divided into two right triangles by an altitude drawn from one of the two base angles.]]\n\nIn the case of right triangles, the triangle inequality specializes to the statement that the hypotenuse is greater than either of the two sides and less than their sum.<ref name=Palmer>\n{{cite book |title=Practical mathematics for home study: being the essentials of arithmetic, geometry, algebra and trigonometry |author=Claude Irwin Palmer |url=https://books.google.com/?id=EAmgAAAAMAAJ&pg=PA422 |page=422 |publisher=McGraw-Hill |year=1919}}\n</ref>\n\nThe second part of this theorem is already established above for any side of any triangle. The first part is established using the lower figure. In the figure, consider the right triangle {{math|ADC}}. An isosceles triangle {{math|ABC}} is constructed with equal sides {{math|{{overline|AB}} {{=}} {{overline|AC}}}}. From the [[triangle postulate]], the angles in the right triangle {{math|ADC}} satisfy:\n:<math> \\alpha + \\gamma = \\pi /2 \\ . </math>\nLikewise, in the isosceles triangle {{math|ABC}}, the angles satisfy:\n:<math>2\\beta + \\gamma = \\pi \\ . </math>\nTherefore,\n:<math> \\alpha = \\pi/2 - \\gamma ,\\ \\mathrm{while} \\ \\beta= \\pi/2 - \\gamma /2  \\ ,</math>\nand so, in particular,\n:<math>\\alpha < \\beta \\ . </math>\nThat means side {{math|AD}} opposite angle {{math|''α''}} is shorter than side {{math|AB}} opposite the larger angle {{math|''β''}}. But {{math|{{overline|AB}} {{=}} {{overline|AC}}}}. Hence:\n:<math>\\overline{\\mathrm{AC}} > \\overline{\\mathrm{AD}} \\ . </math>\nA similar construction shows {{math|{{overline|AC}} > {{overline|DC}}}}, establishing the theorem.\n\nAn alternative proof (also based upon the triangle postulate) proceeds by considering three positions for point {{math|B}}:<ref name=Zawaira>\n\n{{cite book |title=A primer for mathematics competitions |url=https://books.google.com/?id=A21T73sqZ3AC&pg=PA30 |chapter=Lemma 1: In a right-angled triangle the hypotenuse is greater than either of the other two sides |author1=Alexander Zawaira |author2=Gavin Hitchcock |isbn=0-19-953988-X |year=2009 |publisher=Oxford University Press}}\n\n</ref> (i) as depicted (which is to be proven), or (ii) {{math|B}} coincident with {{math|D}} (which would mean the isosceles triangle had two right angles as base angles plus the vertex angle {{math|''γ''}}, which would violate the [[triangle postulate]]), or lastly, (iii) {{math|B}} interior to the right triangle between points {{math|A}} and {{math|D}} (in which case angle {{math|ABC}} is an exterior angle of a right triangle {{math|BDC}} and therefore larger than {{math|''π''/2}}, meaning the other base angle of the isosceles triangle also is greater than {{math|''π''/2}} and their sum exceeds {{math|''π''}} in violation of the triangle postulate).\n\nThis theorem establishing inequalities is sharpened by [[Pythagoras' theorem]] to the equality that the square of the length of the hypotenuse equals the sum of the squares of the other two sides.\n\n===Examples of use===\nConsider a triangle whose sides are in an [[arithmetic progression]] and let the sides be {{math|''a''}}, {{math|''a'' + ''d''}}, {{math|''a'' + 2''d''}}. Then the triangle inequality requires that\n\n:<math>\n0<a<2a+3d </math>\n:<math>\n0<a+d<2a+2d </math>\n:<math>\n0<a+2d<2a+d. </math>\n\nTo satisfy all these inequalities requires\n\n:<math> a>0 \\text{ and } -\\frac{a}{3}<d<a. </math><ref>{{cite journal|title=input: ''solve 0<a<2a+3d, 0<a+d<2a+2d, 0<a+2d<2a+d,'' |last=Wolfram{{!}}Alpha|journal=Wolfram Research|url=http://www.wolframalpha.com/input/?i=solve%200%3Ca%3C2a%2B3d%2C%200%3Ca%2Bd%3C2a%2B2d%2C%200%3Ca%2B2d%3C2a%2Bd&t=ff3tb01|accessdate=2010-09-07}}</ref>\n\nWhen {{math|''d''}} is chosen such that {{math|''d'' {{=}} ''a''/3}}, it generates a right triangle that is always similar to the [[Pythagorean triple]] with sides {{math|3}}, {{math|4}}, {{math|5}}.\n\nNow consider a triangle whose sides are in a [[geometric progression]] and let the sides be {{math|''a''}}, {{math|''ar''}}, {{math|''ar''<sup>2</sup>}}. Then the triangle inequality requires that\n\n:<math> 0<a<ar+ar^2 </math>\n:<math> 0<ar<a+ar^2 </math>\n:<math> 0<ar^2<a+ar. </math>\n\nThe first inequality requires {{math|''a'' > 0}}; consequently it can be divided through and eliminated. With {{math|''a'' > 0}}, the middle inequality only requires {{math|''r'' > 0}}. This now leaves the first and third inequalities needing to satisfy\n\n:<math>\n\\begin{align}\nr^2+r-1 & {} >0 \\\\\nr^2-r-1 & {} <0.\n\\end{align}\n</math>\n\nThe first of these quadratic inequalities requires {{math|''r''}} to range in the region beyond the value of the positive root of the quadratic equation {{math|''r''<sup>2</sup> + ''r'' − 1 {{=}} 0}}, i.e. {{math|''r'' > ''φ'' − 1}}  where {{math|''φ''}} is the [[golden ratio]]. The second quadratic inequality requires {{math|''r''}} to range between 0 and the positive root of the quadratic equation {{math|''r''<sup>2</sup> − ''r'' − 1 {{=}} 0}}, i.e. {{math|0 < ''r'' < ''φ''}}. The combined requirements result in {{math|''r''}} being confined to the range\n:<math>\\varphi - 1 < r <\\varphi\\, \\text{ and } a >0.</math><ref>{{cite journal|title=input: ''solve 0<a<ar+ar<sup>2</sup>, 0<ar<a+ar<sup>2</sup>, 0<ar<sup>2</sup><a+ar'' |last=Wolfram{{!}}Alpha|journal=Wolfram Research|url=http://wolframalpha.com/input?i=solve+0%3Ca%3Car%2Bar^2%2C+0%3Car%3Ca%2Bar^2%2C+0%3Car^2%3Ca%2Bar|accessdate=2010-09-07}}</ref>\n\nWhen {{math|''r''}} the common ratio is chosen such that {{math|''r'' {{=}} {{sqrt|''φ''}}}} it generates a right triangle that is always similar to the [[Kepler triangle]].\n\n===Generalization to any polygon===\nThe triangle inequality can be extended by [[mathematical induction]] to arbitrary polygonal paths, showing that the total length of such a path is no less than the length of the straight line between its endpoints. Consequently, the length of any polygon side is always less than the sum of the other polygon side lengths.\n\n====Example of the generalized polygon inequality for a quadrilateral====\nConsider a quadrilateral whose sides are in a [[geometric progression]] and let the sides be {{math|''a''}}, {{math|''ar''}}, {{math|''ar''<sup>2</sup>}}, {{math|''ar''<sup>3</sup>}}. Then the generalized polygon inequality requires that\n\n:<math> 0<a<ar+ar^2+ar^3 </math>\n:<math> 0<ar<a+ar^2+ar^3 </math>\n:<math> 0<ar^2<a+ar+ar^3 </math>\n:<math> 0<ar^3<a+ar+ar^2. </math>\n\nThese inequalities for {{math|''a'' > 0}} reduce to the following\n\n:<math> r^3+r^2+r-1>0 </math>\n:<math> r^3-r^2-r-1<0. </math><ref>{{cite journal|title=input: ''solve 0<a<ar+ar<sup>2</sup>+ar<sup>3</sup>,  0<ar<sup>3</sup><a+ar+ar<sup>2</sup>'' |last=Wolfram{{!}}Alpha|journal=Wolfram Research|url=http://www.wolframalpha.com/input/?i=solve%20{0%3Ca%3Ca*r%2Ba*r^2%2Ba*r^3%2C%200%3Ca*r^3%3Ca%2Ba*r%2Ba*r^2}&t=ff3tb01|accessdate=2012-07-29}}</ref>\nThe left-hand side polynomials of these two inequalities have roots that are the [[Generalizations of Fibonacci numbers#Tribonacci numbers|tribonacci constant]] and its reciprocal. Consequently, {{math|''r''}} is limited to the range {{math|1/''t'' < ''r'' < ''t''}} where {{math|''t''}} is the tribonacci constant.\n\n====Relationship with shortest paths====\n[[File:Arclength.svg|300px|thumb|The arc length of a curve is defined as the least upper bound of the lengths of polygonal approximations.]]\nThis generalization can be used to prove that the shortest curve between two points in Euclidean geometry is a straight line.\n\nNo polygonal path between two points is shorter than the line between them. This implies that no curve can have an [[arc length]] less than the distance between its endpoints. By definition, the arc length of a curve is the [[least upper bound]] of the lengths of all polygonal approximations of the curve. The result for polygonal paths shows that the straight line between the endpoints is the shortest of all the polygonal approximations. Because the arc length of the curve is greater than or equal to the length of every polygonal approximation, the curve itself cannot be shorter than the straight line path.<ref>{{cite book|title=Numbers and Geometry|author=John Stillwell|authorlink=John Stillwell|year=1997|publisher=Springer|isbn=978-0-387-98289-2|url=https://books.google.com/?id=4elkHwVS0eUC&pg=PA95}} p. 95.</ref>\n\n===Converse===\n\nThe converse of the triangle inequality theorem is also true: if three real numbers are such that each is less than the sum of the others, then there exists a triangle with these numbers as its side lengths and with positive area; and if one number equals the sum of the other two, there exists a degenerate triangle (that is, with zero area) with these numbers as its side lengths.\n\nIn either case, if the side lengths are ''a, b, c'' we can attempt to place a triangle in the [[Euclidean plane]] as shown in the diagram. We need to prove that there exists a real number ''h'' consistent with the values ''a, b,'' and ''c'', in which case this triangle exists.\n\n[[Image:Triangle with notations 3.svg|thumb|270px|Triangle with altitude {{math|''h''}} cutting base {{math|''c''}} into {{math|''d'' + (''c'' − ''d'')}}.]]\n\nBy the [[Pythagorean theorem]] we have {{math|''b''{{sup|2}} {{=}} ''h''{{sup|2}} + ''d''{{sup|2}}}} and {{math|''a''{{sup|2}} {{=}} ''h''{{sup|2}} + (''c'' − ''d''){{sup|2}}}} according to the figure at the right. Subtracting these yields {{math|''a''{{sup|2}} − ''b''{{sup|2}} {{=}} ''c''{{sup|2}} − 2''cd''}}. This equation allows us to express {{math|''d''}} in terms of the sides of the triangle:\n:<math>d=\\frac{-a^2+b^2+c^2}{2c}.</math>\nFor the height of the triangle we have that {{math|''h''{{sup|2}} {{=}} ''b''{{sup|2}} − ''d''{{sup|2}}}}. By replacing {{math|''d''}} with the formula given above, we have\n\n:<math>h^2 = b^2-\\left(\\frac{-a^2+b^2+c^2}{2c}\\right)^2.</math>\n\nFor a real number ''h'' to satisfy this, <math>h^2</math> must be non-negative:\n:<math>b^2-\\left (\\frac{-a^2+b^2+c^2}{2c}\\right) ^2 \\ge 0,</math>\n:<math>\\left( b- \\frac{-a^2+b^2+c^2}{2c}\\right) \\left( b+ \\frac{-a^2+b^2+c^2}{2c}\\right) \\ge 0,</math>\n:<math>\\left(a^2-(b-c)^2)((b+c)^2-a^2 \\right) \\ge 0,</math>\n:<math>(a+b-c)(a-b+c)(b+c+a)(b+c-a) \\ge 0,</math>\n:<math>(a+b-c)(a+c-b)(b+c-a) \\ge 0,</math>\nwhich holds if the triangle inequality is satisfied for all sides. Therefore there does exist a real number ''h'' consistent with the sides ''a, b, c'', and the triangle exists. If each triangle inequality holds [[strict inequality|strictly]], ''h'' > 0 and the triangle is non-degenerate (has positive area); but if one of the inequalities holds with equality, so ''h'' = 0, the triangle is degenerate.\n\n===Generalization to higher dimensions===\nIn Euclidean space, the hypervolume of an {{math|(''n'' − 1)}}-[[Facet (mathematics)|facet]] of an {{math|''n''}}-[[simplex]] is less than or equal to the sum of the hypervolumes of the other {{math|''n''}} facets.  In particular, the area of a triangular face of a [[tetrahedron]] is less than or equal to the sum of the areas of the other three sides.\n\n==Normed vector space==\n[[File:Vector-triangle-inequality.svg|thumb|300px|Triangle inequality for norms of vectors.]]\nIn a [[normed vector space]] {{math|''V''}}, one of the defining properties of the [[norm (mathematics)|norm]] is the triangle inequality:\n\n:<math>\\displaystyle \\|x + y\\| \\leq \\|x\\| + \\|y\\| \\quad \\forall \\, x, y \\in V</math>\n\nthat is, the norm of the [[Vector sum#Addition and subtraction|sum of two vectors]] is at most as large as the sum of the norms of the two vectors.  This is also referred to as [[subadditivity]]. For any proposed function to behave as a norm, it must satisfy this requirement.<ref name=Kress>\n\n{{cite book |title=Numerical analysis |author=Rainer Kress |chapter=§3.1: Normed spaces |url=https://books.google.com/?id=e7ZmHRIxum0C&pg=PA26 |page=26 |isbn=0-387-98408-9 |year=1988 |publisher=Springer}}</ref>\n\nIf the normed space is [[euclidean space|euclidean]], or, more generally, [[strictly convex space|strictly convex]], then <math>\\|x+y\\|=\\|x\\|+\\|y\\|</math> if and\nonly if the triangle formed by {{math|''x''}}, {{math|''y''}}, and {{math|''x'' + ''y''}}, is degenerate, that is,\n{{math|''x''}} and {{math|''y''}} are on the same ray, i.e., {{math|''x'' {{=}} 0}} or {{math|''y'' {{=}} 0}}, or\n{{math|''x'' {{=}} ''α y''}} for some {{math|''α'' > 0}}. This property characterizes strictly convex normed spaces such as\nthe {{math|''ℓ<sub>p</sub>''}} spaces with {{math|1 < ''p'' < ∞}}. However, there are normed spaces in which this is\nnot true. For instance, consider the plane with the {{math|''ℓ''<sub>1</sub>}} norm (the [[Manhattan distance]]) and\ndenote {{math|''x'' {{=}} (1, 0)}} and {{math|''y'' {{=}} (0, 1)}}. Then the triangle formed by\n{{math|''x''}}, {{math|''y''}}, and {{math|''x'' + ''y''}}, is non-degenerate but\n\n:<math>\\|x+y\\|=\\|(1,1)\\|=|1|+|1|=2=\\|x\\|+\\|y\\|.</math>\n\n===Example norms===\n*''Absolute value as norm for the [[real line]].'' To be a norm, the triangle inequality requires that the [[absolute value]] satisfy for any real numbers {{math|''x''}} and {{math|''y''}}:\n::<math>|x + y| \\leq |x|+|y|,</math>\n\n:which it does.\n\nProof:<ref name=Stewart>\n\n{{cite book |page=A10 |author=James Stewart |title=Essential Calculus |isbn=978-0-495-10860-3 |publisher=Thomson Brooks/Cole |year=2008}}\n\n</ref>\n\n:<math>-\\left\\vert x \\right\\vert \\leq x \\leq \\left\\vert x \\right\\vert</math>\n:<math>-\\left\\vert y \\right\\vert \\leq y \\leq \\left\\vert y \\right\\vert</math>\nAfter adding, \n:<math>-( \\left\\vert x \\right\\vert + \\left\\vert y \\right\\vert ) \\leq x+y \\leq \\left\\vert x \\right\\vert + \\left\\vert y \\right\\vert</math>\nUse the fact that <math>\\left\\vert b \\right\\vert \\leq a \\Leftrightarrow -a \\leq b \\leq a</math>\n(with ''b'' replaced by ''x''+''y'' and ''a'' by <math>\\left\\vert x \\right\\vert + \\left\\vert y \\right\\vert</math>), we have\n\n:<math>|x + y| \\leq |x|+|y|</math>\n\nThe triangle inequality is useful in [[mathematical analysis]] for determining the best upper estimate on the size of the sum of two numbers, in terms of the sizes of the individual numbers.\n\nThere is also a lower estimate, which can be found using the ''reverse triangle inequality'' which states that for any real numbers {{math|''x''}} and {{math|''y''}}:\n\n:<math>|x-y| \\geq \\bigg||x|-|y|\\bigg|.</math>\n\n*''Inner product as norm in an [[inner product space]].'' If the norm arises from an inner product (as is the case for Euclidean spaces), then the triangle inequality follows from the [[Cauchy–Schwarz inequality]] as follows: Given vectors <math>x</math> and <math>y</math>, and denoting the inner product as <math>\\langle x , y\\rangle </math>:<ref name= Stillwell>\n\n{{cite book |title=The four pillars of geometry |author=John Stillwell |page=80 |url=https://books.google.com/?id=fpAjJ6VJ3y8C&pg=PA80 |isbn=0-387-25530-3 |year=2005 |publisher=Springer}}\n\n</ref>\n:{|\n|<math>\\|x + y\\|^2</math> || <math>= \\langle x + y, x + y \\rangle</math>\n|-\n| || <math>= \\|x\\|^2 + \\langle x, y \\rangle + \\langle y, x \\rangle + \\|y\\|^2</math>\n|-\n| || <math>\\le \\|x\\|^2 + 2|\\langle x, y \\rangle| + \\|y\\|^2</math>\n|-\n| || <math>\\le \\|x\\|^2 + 2\\|x\\|\\|y\\| + \\|y\\|^2</math> (by the Cauchy–Schwarz inequality)\n|-\n| || <math>=  \\left(\\|x\\| + \\|y\\|\\right)^2</math>\n|}\n:where the last form is a consequence of:\n\n::<math>\\|x\\|^2 + 2\\|x\\|\\|y\\| + \\|y\\|^2 = \\left(\\|x\\| + \\|y\\|\\right)^2 \\ .</math>\n\nThe Cauchy–Schwarz inequality turns into an equality if and only if {{math|''x''}} and {{math|''y''}}\nare linearly dependent. The inequality\n<math>\\langle x, y \\rangle + \\langle y, x \\rangle \\le 2|\\langle x, y \\rangle| </math>\nturns into an equality for linearly dependent <math>x</math> and  <math>y</math>\nif and only if one of the vectors {{math|''x''}} or {{math|''y''}} is a ''nonnegative'' scalar of the other.\n\n:Taking the square root of the final result gives the triangle inequality.\n*[[p-norm|{{math|''p''}}-norm]]: a commonly used norm is the ''p''-norm:\n\n::<math>\\|x\\|_p = \\left( \\sum_{i=1}^n |x_i|^p \\right) ^{1/p} \\ , </math>\n\n:where the {{math|''x<sub>i</sub>''}} are the components of vector {{math|''x''}}. For {{math|''p'' {{=}} 2}} the {{math|''p''}}-norm becomes the ''Euclidean norm'':\n::<math>\\|x\\|_2 = \\left( \\sum_{i=1}^n |x_i|^2 \\right) ^{1/2} = \\left( \\sum_{i=1}^n x_{i}^2 \\right) ^{1/2} \\ , </math>\n:which is [[Pythagoras' theorem]] in {{math|''n''}}-dimensions, a very special case corresponding to an inner product norm. Except for the case {{math|''p'' {{=}} 2}}, the {{math|''p''}}-norm is ''not'' an inner product norm, because it does not satisfy the [[parallelogram law]]. The triangle inequality for general values of {{math|''p''}} is called [[Minkowski's inequality]].<ref name=Saxe>\n\n{{cite book |title=Beginning functional analysis |author= Karen Saxe|authorlink= Karen Saxe |url=https://books.google.com/?id=0LeWJ74j8GQC&pg=PA61 |page=61 |isbn=0-387-95224-1 |publisher=Springer |year=2002}}\n\n</ref> It takes the form:\n::<math>\\|x+y\\|_p \\le \\|x\\|_p + \\|y\\|_p \\ .</math>\n\n==Metric space==\nIn a [[metric space]] {{math|''M''}} with metric {{math|''d''}}, the triangle inequality is a requirement upon [[Metric (mathematics)#Definition|distance]]:\n:<math>d(x,\\ z) \\le d(x,\\ y) + d(y,\\ z) \\ , </math>\n\nfor all {{math|''x''}}, {{math|''y''}}, {{math|''z''}} in {{math|''M''}}. That is, the distance from {{math|''x''}} to {{math|''z''}} is at most as large as the sum of the distance from {{math|''x''}} to {{math|''y''}} and the distance from {{math|''y''}} to {{math|''z''}}.\n\nThe triangle inequality is responsible for most of the interesting structure on a metric space, namely, convergence.  This is because the remaining requirements for a metric are rather simplistic in comparison.  For example, the fact that any [[limit of a sequence|convergent sequence]] in a metric space is a [[Cauchy sequence]] is a direct consequence of the triangle inequality, because if we choose any {{math|''x<sub>n</sub>''}} and {{math|''x<sub>m</sub>''}} such that {{math|''d''(''x<sub>n</sub>'', ''x'') < ''ε''/2}} and {{math|''d''(''x<sub>m</sub>'', ''x'') < ''ε''/2}}, where {{math|''ε'' > 0}} is given and arbitrary (as in the definition of a limit in a metric space), then by the triangle inequality, {{math|''d''(''x<sub>n</sub>'', ''x<sub>m</sub>'') ≤ ''d''(''x<sub>n</sub>'', ''x'') + ''d''(''x<sub>m</sub>'', ''x'') < ''ε''/2 + ''ε''/2 {{=}} ''ε''}}, so that the sequence {{math|{{mset|''x<sub>n</sub>''}}}} is a Cauchy sequence, by definition.\n\nThis version of the triangle inequality reduces to the one stated above in case of normed vector spaces where a metric is induced via {{math|''d''(''x'', ''y'') ≔ ‖''x'' − ''y''‖}}, with {{math|''x'' − ''y''}} being the vector pointing from point {{math|''y''}} to {{math|''x''}}.\n\n==Reverse triangle inequality==\nThe '''reverse triangle inequality''' is an elementary consequence of the triangle inequality that gives lower bounds instead of upper bounds. For plane geometry, the statement is:<ref name=inequality>\n\n{{cite book |title=The popular educator; fourth volume |url=https://books.google.com/?id=lTACAAAAQAAJ&pg=PA196 |page=196 |chapter=Exercise I. to proposition XIX |year=1854 |publisher=John Cassell |location=Ludgate Hill, London |author=Anonymous}}\n\n</ref>\n\n:''Any side of a triangle is greater than the difference between the other two sides''.\n\nIn the case of a normed vector space, the statement is:\n: <math>\\bigg|\\|x\\|-\\|y\\|\\bigg| \\leq \\|x-y\\|,</math>\nor for metric spaces, {{math|{{!}}''d''(''y'', ''x'') − ''d''(''x'', ''z''){{!}} ≤ ''d''(''y'', ''z'')}}.\nThis implies that the norm <math>\\|\\cdot\\|</math> as well as the distance function <math>d(x,\\cdot)</math> are [[Lipschitz continuity|Lipschitz continuous]] with Lipschitz constant {{math|1}}, and therefore are in particular [[uniform continuity|uniformly continuous]].\n\nThe proof for the reverse triangle uses the regular triangle inequality, and <math> \\|y-x\\| = \\|{-}1(x-y)\\| = |{-}1|\\cdot\\|x-y\\| = \\|x-y\\| </math>:\n: <math> \\|x\\| = \\|(x-y) + y\\| \\leq \\|x-y\\| + \\|y\\| \\Rightarrow \\|x\\| - \\|y\\| \\leq \\|x-y\\|, </math>\n: <math> \\|y\\| = \\|(y-x) + x\\| \\leq \\|y-x\\| + \\|x\\| \\Rightarrow \\|x\\| - \\|y\\| \\geq -\\|x-y\\|, </math>\n\nCombining these two statements gives:\n: <math> -\\|x-y\\| \\leq \\|x\\|-\\|y\\| \\leq \\|x-y\\| \\Rightarrow \\bigg|\\|x\\|-\\|y\\|\\bigg| \\leq \\|x-y\\|.</math>\n\n==Reversal in Minkowski space==\n\nThe [[Minkowski space]] metric <math> \\eta_{\\mu \\nu} </math> is not positive-definite, which means that <math> \\|x\\|^2 = \\eta_{\\mu \\nu} x^\\mu x^\\nu</math> can have either sign or vanish, even if the vector ''x'' is non-zero. Moreover, if ''x'' and ''y'' are both timelike vectors lying in the future light cone, the triangle inequality is reversed:\n\n: <math> \\|x+y\\| \\geq \\|x\\| + \\|y\\|. </math>\n\nA physical example of this inequality is the [[twin paradox]] in [[special relativity]]. The same reversed form of the inequality holds if both vectors lie in the past light cone, and if one or both are null vectors. The result holds in ''n'' + 1 dimensions for any ''n'' ≥ 1.  If the plane defined by ''x'' and ''y'' is spacelike (and therefore a Euclidean subspace) then the usual triangle inequality holds.\n\n==See also==\n* [[Subadditivity]]\n* [[Minkowski inequality]]\n* [[Ptolemy's inequality]]\n\n==Notes==\n{{reflist|30em}}\n\n==References==\n* {{Cite book|authorlink = Daniel Pedoe|last=Pedoe|first=Daniel|title = Geometry: A comprehensive course|publisher=Dover|year=1988|isbn = 0-486-65812-0|postscript = <!--None-->}}.\n* {{Cite book | last1=Rudin | first1=Walter | author1-link=Walter Rudin | title=Principles of Mathematical Analysis | publisher=[[McGraw-Hill]]| location=New York | isbn=0-07-054235-X | year=1976 | postscript=<!--None-->}}.\n\n==External links==\n{{ProofWiki|id=Triangle_Inequality|title=Triangle inequality}}\n\n{{DEFAULTSORT:Triangle Inequality}}\n[[Category:Geometric inequalities]]\n[[Category:Linear algebra]]\n[[Category:Metric geometry]]\n[[Category:Articles containing proofs]]\n[[Category:Theorems in geometry]]"
    },
    {
      "title": "Weitzenböck's inequality",
      "url": "https://en.wikipedia.org/wiki/Weitzenb%C3%B6ck%27s_inequality",
      "text": "{{distinguish|Weitzenböck identity}}\n[[File:LabeledTriangle.svg|thumb|right|220px|According to Weitzenböck's inequality, the [[area]] of this [[triangle]] is at most {{math|(''a''<sup>2</sup> + ''b''<sup>2</sup> + ''c''<sup>2</sup>) ⁄ 4√{{overline|3}}.}}]]\n[[File:Hadwiger finsler inequality.svg|thumb|upright=1.5|<math>\\begin{align}&\\text{all inner angles} < 120^\\circ : \\\\ &\\text{grey area} = 3 \\Delta \\leq \\Delta_a+\\Delta_b+\\Delta_c \\end{align}</math>]]\n[[File:Hadwiger finsler inequality2.svg|thumb|upright=1.5|<math>\\begin{align}&\\text{one inner angle} \\geq 120^\\circ : \\\\ &\\text{grey area} = 3 \\Delta \\leq \\Delta_c < \\Delta_a+\\Delta_b+\\Delta_c \\end{align}</math>]]\nIn [[mathematics]], '''Weitzenböck's inequality''', named after [[Roland Weitzenböck]], states that for a triangle of side lengths <math>a</math>, <math>b</math>, <math>c</math>, and area <math>\\Delta</math>, the following inequality holds:\n\n: <math>a^2 + b^2 + c^2 \\geq 4\\sqrt{3}\\, \\Delta </math>.\n\nEquality occurs if and only if the triangle is equilateral. [[Pedoe's inequality]]  is a generalization of Weitzenböck's inequality. The [[Hadwiger-Finsler inequality]] is a strengthened version of Weitzenböck's inequality.\n\n==Geometric interpretation and proof==\nRewriting the inequality above allows for a more concrete geometric interpretation, which in turn provides an immediate proof.<ref>Claudi Alsina, Roger B. Nelsen: ''Geometric Proofs of the Weitzenböck and Hadwiger-Finsler Inequalities''. Mathematics Magazine, Vol. 81, No. 3 (Jun., 2008), pp.&nbsp;216–219 ([https://www.jstor.org/stable/27643111 JSTOR]) </ref>\n\n: <math>\\frac{\\sqrt{3}}{4}a^2 + \\frac{\\sqrt{3}}{4}b^2 + \\frac{\\sqrt{3}}{4}c^2 \\geq 3\\, \\Delta </math>.\n\nNow the summands on the left side are the areas of equilateral triangles erected over the sides of the original triangle and hence the equations states that the sum of areas of the equilateral triangles is always greater or equal than the threefold area of the original triangle.\n\n: <math>\\Delta_a + \\Delta_b + \\Delta_c \\geq 3\\, \\Delta </math>.\n\nThis can now can be shown by replicating area of the triangle three times within the equilateral triangles. To achieve that the [[Fermat point]] is used to partition the triangle into three obtuse subtriangles with a <math>120^\\circ</math> angle and each of those subtriangles is replicated three times within the equilateral triangle next to it. This only works if every angle of the triangle is smaller than <math>120^\\circ</math>, since otherwise the Fermat point is not located in the interior of the triangle and becomes a vertex instead. However if one angle is greater or equal to <math>120^\\circ</math> it is possible to replicate the whole triangle three times within the largest equilateral triangle, so the sum of areas of all equilateral triangles stays greater than the threefold area of the triangle anyhow.\n\n== Further proofs ==\nThe proof of this inequality was set as a question in the [[International Mathematical Olympiad]] of 1961. Even so, the result is not too difficult to derive using [[Heron's formula]] for the area of a triangle:\n\n:<math>\n\\begin{align}\n\\Delta & {} =\\frac{1}{4}\\sqrt{(a+b+c)(a+b-c)(b+c-a)(c+a-b)} \\\\\n& {} =\\frac{1}{4}\\sqrt{2(a^2 b^2+a^2c^2+b^2c^2)-(a^4+b^4+c^4)}.\n\\end{align}\n</math>\n\n=== First method ===\n\nIt can be shown that the area of the inner [[Napoleon's triangle]], which must be nonnegative, is<ref>Coxeter, H.S.M., and  Greitzer, Samuel L. ''Geometry Revisited'', page 64.</ref>\n\n:<math>\\frac{\\sqrt{3}}{24}(a^2+b^2+c^2-4\\sqrt{3}\\Delta),</math>\n\nso the expression in parentheses must be greater than or equal to 0.\n\n=== Second method ===\n\nThis method assumes no knowledge of inequalities except that all squares are nonnegative.\n\n: <math>\n\\begin{align}\n{} & (a^2 - b^2)^2 + (b^2 - c^2)^2 + (c^2 - a^2)^2 \\geq 0 \\\\\n{} \\iff & 2(a^4+b^4+c^4) - 2(a^2 b^2+a^2c^2+b^2c^2) \\geq 0 \\\\\n{} \\iff & \\frac{4(a^4+b^4+c^4)}{3} \\geq \\frac{4(a^2 b^2+a^2c^2+b^2c^2)}{3} \\\\\n{} \\iff & \\frac{(a^4+b^4+c^4) + 2(a^2 b^2+a^2c^2+b^2c^2)}{3} \\geq 2(a^2 b^2+a^2c^2+b^2c^2)-(a^4+b^4+c^4) \\\\\n{} \\iff & \\frac{(a^2 + b^2 + c^2)^2}{3} \\geq (4\\Delta)^2,\n\\end{align}\n</math>\n\nand the result follows immediately by taking the positive square root of both sides. From the first inequality we can also see that equality occurs only when <math>a = b = c</math> and the triangle is equilateral.\n\n===Third method===\n\nThis proof assumes knowledge of the [[AM–GM inequality]].\n\n: <math>\n\\begin{align}\n& & (a-b)^2+(b-c)^2+(c-a)^2 & \\geq & & 0 \\\\\n\\Rightarrow & & 2a^2+2b^2+2c^2 & \\geq & & 2ab+2bc+2ac \\\\\n\\iff & & 3(a^2+b^2+c^2) & \\geq & & (a + b + c)^2 \\\\\n\\iff & & a^2+b^2+c^2 & \\geq & & \\sqrt{3(a+b+c)\\left(\\frac{a+b+c}{3}\\right)^3} \\\\\n\\Rightarrow & & a^2+b^2+c^2 & \\geq & & \\sqrt{3 (a+b+c)(-a+b+c)(a-b+c)(a+b-c)} \\\\\n\\iff & & a^2+b^2+c^2 & \\geq & & 4 \\sqrt3 \\Delta.\n\\end{align}\n</math>\n\nAs we have used the arithmetic-geometric mean inequality, equality only occurs when <math>a = b = c</math> and the triangle is equilateral.\n\n===Fourth method===\n\nWrite <math>x=\\cot A, c=\\cot A+\\cot B>0</math> so the sum <math>S=\\cot A+\\cot B+\\cot C=c+\\frac{1-x(c-x)}{c}</math> and <math>cS=c^2-xc+x^2+1=\\left(x-\\frac{c}{2}\\right)^2+\\left(\\frac{c\\sqrt{3}}{2}-1\\right)^2+c\\sqrt{3}\\ge c\\sqrt{3}</math> i.e. <math>S\\ge\\sqrt{3}</math>. But <math>\\cot A=\\frac{b^2+c^2-a^2}{4\\Delta}</math>, so <math>S=\\frac{a^2+b^2+c^2}{4\\Delta}</math>.\n\n==See also==\n*[[List of triangle inequalities]]\n*[[Isoperimetric inequality]]\n*[[Hadwiger–Finsler inequality]]\n\n==Notes==\n{{Reflist}}\n\n==References & further reading==\n*Claudi Alsina, Roger B. Nelsen: ''When Less is More: Visualizing Basic Inequalities''. MAA, 2009, {{isbn|9780883853429}}, pp. [https://books.google.de/books?id=U1ovBsSRNscC&pg=PA84 84-86]\n*Claudi Alsina, Roger B. Nelsen: ''Geometric Proofs of the Weitzenböck and Hadwiger-Finsler Inequalities''. Mathematics Magazine, Vol. 81, No. 3 (Jun., 2008), pp.&nbsp;216–219 ([https://www.jstor.org/stable/27643111 JSTOR])\n*D. M. Batinetu-Giurgiu, Nicusor Minculete, Nevulai Stanciu: [http://ijgeometry.com/product/d-m-batinetu-giurgiu-nicusor-minculete-and-neculai-stanciu-some-geometric-inequalities-of-ionescu-weitzebbock-type/ ''Some geometric inequalities of Ionescu-Weitzebböck type'']. International Journal of Geometry, Vol. 2 (2013), No. 1, April\n*D. M. Batinetu-Giurgiu, Nevulai Stanciu: ''The inequality Ionescu - Weitzenböck''. MateInfo.ro, April 2013, ([http://www.mateinfo.ro/reviste-de-matematica/revista-electronica-de-matematica-mateinfo-ro-aparitie-lunara-2065-6432/reviste-electronice-mateinfo-ro-din-anul-2013/206-revista-electronica-mateinfo-ro-issn-2065-6432-aprilie-2013/file online copy])\n*[[Daniel Pedoe]]: ''On Some Geometrical Inequalities''. The Mathematical Gazette, Vol. 26, No. 272 (Dec., 1942), pp. 202-208 ([https://www.jstor.org/stable/3607041 JSTOR])\n*[[Roland Weitzenböck]]: ''Über eine Ungleichung in der Dreiecksgeometrie''. Mathematische Zeitschrift, Volume 5, 1919, pp. 137-146 ([http://gdz.sub.uni-goettingen.de/dms/load/img/?PPN=PPN266833020_0005&DMDID=DMDLOG_0017 online copy] at [[Göttinger Digitalisierungszentrum]])\n*Dragutin Svrtan, Darko Veljan: ''Non-Euclidean Versions of Some Classical Triangle Inequalities''. Forum Geometricorum, Volume 12, 2012, pp. 197–209 ([http://forumgeom.fau.edu/FG2012volume12/FG201217.pdf online copy])\n*Mihaly Bencze, Nicusor Minculete, Ovidiu T. Pop: ''New inequalities for the triangle''. Octogon Mathematical Magazine, Vol. 17, No.1, April 2009, pp. 70-89 ([http://www.uni-miskolc.hu/~matsefi/Octogon/volumes/volume1/article1_4.pdf online copy])\n\n== External links ==\n*{{MathWorld | urlname=WeitzenboecksInequality | title=Weitzenböck's Inequality}}\n*\"[http://demonstrations.wolfram.com/WeitzenboecksInequality/ Weitzenböck's Inequality],\" an interactive demonstration by Jay Warendorff - [[Wolfram Demonstrations Project]].\n\n{{DEFAULTSORT:Weitzenbock's inequality}}\n[[Category:Elementary geometry]]\n[[Category:Geometric inequalities]]\n[[Category:Triangle geometry]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "Azuma's inequality",
      "url": "https://en.wikipedia.org/wiki/Azuma%27s_inequality",
      "text": "In [[probability theory]], the '''Azuma–Hoeffding inequality''' (named after [[Kazuoki Azuma]] and [[Wassily Hoeffding]]) gives a [[concentration inequality|concentration result]] for the values of [[martingale (probability theory)|martingale]]s that have bounded differences.\n\nSuppose { ''X''<sub>''k''</sub> : ''k'' = 0, 1, 2, 3, ... } is a [[Martingale (probability theory)|martingale]] (or [[Martingale_(probability_theory)#Submartingales.2C_supermartingales.2C_and_relationship_to_harmonic_functions|super-martingale]]) and\n\n:<math>|X_k - X_{k-1}| < c_k, \\, </math>\n\n[[almost surely]]. Then for all positive integers ''N'' and all positive [[real number|reals]] ''t'',\n\n:<math>P(X_N - X_0 \\geq t) \\leq \\exp\\left ({-t^2 \\over 2 \\sum_{k=1}^N c_k^2} \\right). </math>\n\nAnd symmetrically (when ''X''<sub>''k''</sub> is a sub-martingale):\n\n:<math>P(X_N - X_0 \\leq -t) \\leq \\exp\\left ({-t^2 \\over 2 \\sum_{k=1}^N c_k^2} \\right). </math>\n\nIf ''X'' is a martingale, using both inequalities above and applying the [[union bound]] allows one to obtain a two-sided bound:\n\n:<math>P(|X_N - X_0| \\geq t) \\leq 2\\exp\\left ({-t^2 \\over 2 \\sum_{k=1}^N c_k^2} \\right). </math>\n\nAzuma's inequality applied to the [[Doob martingale]] gives [[Doob_martingale#McDiarmid's_inequality|McDiarmid's inequality]] which is common in the analysis of [[randomized algorithm]]s.\n\n==Simple example of Azuma's inequality for coin flips==\nLet ''F''<sub>''i''</sub> be a sequence of independent and identically distributed random coin flips (i.e., let ''F''<sub>''i''</sub> be equally likely to be −1 or 1 independent of the other values of ''F''<sub>''i''</sub>).  Defining <math>X_i = \\sum_{j=1}^i F_j</math> yields a [[Martingale (probability theory)|martingale]] with |''X''<sub>''k''</sub>&nbsp;&minus;&nbsp;''X''<sub>''k''&minus;1</sub>|&nbsp;≤&nbsp;1, allowing us to apply Azuma's inequality.  Specifically, we get\n\n:<math> \\Pr[X_n > t] \\leq \\exp\\left(\\frac{-t^2}{2 n}\\right).</math>\n\nFor example, if we set ''t'' proportional to ''n'', then this tells us that although the ''maximum'' possible value of ''X''<sub>''n''</sub> scales linearly with ''n'', the ''probability'' that the sum scales linearly with ''n'' [[exponential decay|decreases exponentially fast]] with&nbsp;''n''.\n\nIf we set <math>t=\\sqrt{2 n \\ln n}</math> we get:\n\n:<math> \\Pr[X_n > \\sqrt{2 n \\ln n}] \\leq 1/n,</math>\n\nwhich means that the probability of deviating more than <math>\\sqrt{2 n \\ln n}</math> approaches 0 as ''n'' goes to infinity.\n\n==Remark==\n\nA [[Bernstein inequalities (probability theory)|similar inequality]] was proved under weaker assumptions by [[Sergei Bernstein]] in 1937.\n\nHoeffding proved this result for independent variables rather than martingale differences, and also observed that slight modifications of his argument establish the result for martingale differences (see page 18 of his 1963 paper).\n\n==See also==\n* [[Concentration inequality]] - a summary of tail-bounds on random variables.\n\n{{No footnotes|date=July 2010}}\n\n==References==\n\n* {{cite book|first1=N. |last1=Alon |first2= J. |last2=Spencer|title=The Probabilistic Method|publisher= Wiley|location=New York|year= 1992}}\n* {{cite journal|doi=10.2748/tmj/1178243286|first=K. |last=Azuma|title=Weighted Sums of Certain Dependent Random Variables|journal=[[Tôhoku Mathematical Journal]]|volume=19|pages= 357–367 |year=1967|issue=3|url=http://projecteuclid.org/DPubS/Repository/1.0/Disseminate?view=body&id=pdf_1&handle=euclid.tmj/1178243286|format=PDF|mr=0221571}}\n* {{cite journal| last=Bernstein | first=Sergei N. | authorlink=Sergei Natanovich Bernstein | year=1937 |trans-title=On certain modifications of Chebyshev's inequality | journal=Doklady Akademii Nauk SSSR | volume=17 | issue=6 | pages=275–277 |script-title=ru:На определенных модификациях неравенства Чебишева|language=Russian }} (vol. 4, item 22 in the collected works)\n* {{cite book| first=C. |last=McDiarmid|chapter= On the method of bounded differences|title=Surveys in Combinatorics|series= London Math. Soc. Lectures Notes 141|publisher= Cambridge Univ. Press|location= Cambridge |year=1989|pages=148–188|mr=1036755}}\n* {{Cite journal|doi=10.2307/2282952|first1=W. |last1=Hoeffding|title=Probability inequalities for sums of bounded random variables|journal=Journal of the American Statistical Association|volume=58|issue=301|pages= 13–30|year= 1963|mr=  0144363  |jstor=2282952 }}\n* {{Cite book|first1=A. P. |last1=Godbole |first2= P. |last2=Hitczenko|title=Beyond the method of bounded differences|journal=DIMACS Series in Discrete Mathematics and Theoretical Computer Science|volume= 41|pages=43–58|year= 1998 |mr=1630408  |doi=10.1090/dimacs/041/03 |isbn=9780821808276 }}\n\n[[Category:Probabilistic inequalities]]\n[[Category:Martingale theory]]"
    },
    {
      "title": "Bennett's inequality",
      "url": "https://en.wikipedia.org/wiki/Bennett%27s_inequality",
      "text": "In [[probability theory]], '''Bennett's [[inequality (mathematics)|inequality]]''' provides an [[upper bound]] on the [[probability]] that the sum of [[independent random variables]] deviates from its [[expected value]] by more than any specified amount. Bennett's inequality was proved by George Bennett of the [[University of New South Wales]] in 1962.<ref name=bennett>{{Cite journal | last1 = Bennett | first1 = G. | title = Probability Inequalities for the Sum of Independent Random Variables | journal = [[Journal of the American Statistical Association]] | volume = 57 | issue = 297 | pages = 33–45 | doi = 10.2307/2282438 | jstor = 2282438| year = 1962 | pmid =  | pmc = }}</ref>\n\n== Statement ==\nLet \n{{math|''X''<sub>1</sub>, … ''X''<sub>''n''</sub>}}\nbe [[independent random variables]] with finite variance and assume (for simplicity but [[without loss of generality]]) they all have zero expected value. Further assume {{math|''X''<sub>''i''</sub> ≤ ''a''}} [[almost surely]] for all {{math|''i''}}, and define <math> S_n = \\sum_{i = 1}^n X_i</math> and <math> \\sigma^2 = \\sum_{i=1}^n \\operatorname{E}(X_i^2).</math> \nThen for any {{math|''t'' ≥ 0}},\n\n:<math>\\Pr\\left( S_n > t \\right) \\leq\n\\exp\\left( - \\frac{\\sigma^2}{a^2} h\\left(\\frac{at}{\\sigma^2} \\right)\\right),</math>\n\nwhere {{math|''h''(''u'') {{=}} (1 + ''u'')log(1 + ''u'') – ''u''}}.<ref name=devroye>{{cite book|title=Combinatorial methods in density estimation| first1=Luc |last1=Devroye| authorlink1=Luc Devroye| first2=Gábor |last2=Lugosi| publisher=[[Springer (publisher)|Springer]]| year=2001| isbn=978-0-387-95117-1| page=11| url=https://books.google.com/books?id=jvT-sUt1HZYC&pg=PA11}}</ref><ref name=BLM2013>{{cite book|title=Concentration inequalities, a nonasymptotic theory of independence | first1=Stephane | last1=Boucheron | first2=Gabor|last2=Lugosi|first3=Pascal|last3=Massart|publisher=Oxford University Press|year=2013|ISBN=978-0-19-953525-5}}</ref>\n\n== Generalizations and comparisons to other bounds ==\nFor generalizations see Freedman (1975)<ref>{{cite journal |title=On tail probabilities for martingales.| first1=D. A. |last1=Freedman| authorlink1=David A. Freedman |  publisher=[[The Annals of Probability]]|volume= 3| year=1975| pages=100–118|jstor=2959268 }}</ref> and Fan, Grama and Liu (2012)<ref>{{cite journal |title=Hoeffding's inequality for supermartingales| first1=X. |last1=Fan|  first2=I. |last2=Grama |first3=Q. |last3=Liu| journal=[[Stochastic Processes and their Applications]]|volume= 122| year=2012| pages=3545–3559 | doi=10.1016/j.spa.2012.06.009|arxiv=1109.4359}}</ref> for a [[Martingale (probability theory)|martingale]] version of Bennett's inequality and its improvement, respectively.\n\n[[Hoeffding's inequality]] only assumes the summands are bounded almost surely, while Bennett's inequality offers some improvement when the variances of the summands are small compared to their almost sure bounds. However Hoeffding's inequality entails sub-Gaussian tails, whereas in general Bennett's inequality has Poissonian tails.{{cn|date=September 2016}}\nIn both inequalities, unlike some other inequalities or limit theorems, there is no requirement that the component variables have identical or similar distributions.{{cn|date=September 2016}}\n\n==See also==\n*[[Concentration inequality]] - a summary of tail-bounds on random variables.\n\n==References==\n<references />\n\n[[Category:Probabilistic inequalities]]\n\n\n{{probability-stub}}"
    },
    {
      "title": "Bernstein inequalities (probability theory)",
      "url": "https://en.wikipedia.org/wiki/Bernstein_inequalities_%28probability_theory%29",
      "text": "In [[probability theory]], '''Bernstein inequalities''' give bounds on the probability that the sum of random variables deviates from its mean. In the simplest case, let ''X''<sub>1</sub>,&nbsp;...,&nbsp;''X''<sub>''n''</sub> be independent [[Bernoulli trial|Bernoulli random variables]] taking values +1 and &minus;1 with probability&nbsp;1/2 (this distribution is also known as the [[Rademacher distribution]]), then for every positive <math>\\varepsilon</math>,\n\n:<math>\\mathbb{P}\\left (\\left|\\frac{1}{n}\\sum_{i=1}^n X_i\\right| > \\varepsilon \\right ) \\leq 2\\exp \\left (-\\frac{n\\varepsilon^2}{2(1+\\frac{\\varepsilon}{3})} \\right).</math>\n\n'''Bernstein inequalities''' were proved and published by [[Sergei Bernstein]] in the 1920s and 1930s.<ref>S.N.Bernstein, \"On a modification of Chebyshev’s inequality and of the error formula of Laplace\" vol. 4, #5 (original publication: Ann. Sci. Inst. Sav. Ukraine, Sect. Math. 1, 1924)</ref><ref>{{cite journal |last=Bernstein |first=S. N. |year=1937 |title=Об определенных модификациях неравенства Чебышева |trans-title=On certain modifications of Chebyshev's inequality |journal=[[Doklady Akademii Nauk SSSR]] |volume=17 |issue=6 |pages=275&ndash;277}}</ref><ref>S.N.Bernstein, \"Theory of Probability\" (Russian), Moscow, 1927</ref><ref>J.V.Uspensky, \"Introduction to Mathematical Probability\", McGraw-Hill Book Company, 1937</ref> Later, these inequalities were rediscovered several times in various forms. Thus, special cases of the Bernstein inequalities are also known as the [[Chernoff bound]], [[Hoeffding's inequality]] and [[Azuma's inequality]]. \n\n==Some of the inequalities==\n1. Let <math>X_1, \\ldots, X_n</math> be independent zero-mean random variables. Suppose that <math>|X_i|\\leq M</math> almost surely, for all <math>i.</math> Then, for all positive <math>t</math>,\n\n:<math>\\mathbb{P} \\left (\\sum_{i=1}^n X_i > t \\right ) \\leq \\exp \\left ( -\\frac{\\tfrac{1}{2} t^2}{\\sum \\mathbb{E} \\left[X_j^2 \\right ]+\\tfrac{1}{3} Mt} \\right ).</math>\n\n2. Let <math>X_1, \\ldots, X_n</math> be independent random variables. Suppose that for some positive real <math>L</math> and every integer <math>k>1</math>,\n\n:<math> \\mathbb{E} \\left[ \\left |X_i^k \\right |\\right ] \\leq \\frac{1}{2} \\mathbb{E} \\left[X_i^2\\right] L^{k-2} k!</math>\n\nThen\n\n:<math>\\mathbb{P} \\left (\\sum_{i=1}^n X_i \\geq 2t \\sqrt{\\sum \\mathbb{E} \\left [X_i^2 \\right ]} \\right ) < \\exp(-t^2), \\qquad \\text{for}\\quad 0 < t \\leq \\frac{1}{2L}\\sqrt{\\sum \\mathbb{E} \\left[X_j^2\\right ]}. </math>\n\n3. Let <math>X_1, \\ldots, X_n</math> be independent random variables. Suppose that\n\n:<math> \\mathbb{E} \\left[ \\left |X_i^k \\right |\\right ] \\leq \\frac{k!}{4!} \\left(\\frac{L}{5}\\right)^{k-4}</math>\n\nfor all integer <math>k>3.</math> Denote\n\n:<math> A_k = \\sum \\mathbb{E} \\left [ X_i^k\\right ].</math>\n\nThen,\n\n:<math>\\mathbb{P} \\left( \\left| \\sum_{j=1}^n X_j - \\frac{A_3 t^2}{3A_2} \\right|\\geq \\sqrt{2A_2} \\, t \\left[ 1 + \\frac{A_4 t^2}{6 A_2^2} \\right] \\right ) < 2 \\exp (- t^2), \\qquad \\text{for} \\quad 0 < t \\leq \\frac{5 \\sqrt{2A_2}}{4L}. </math>\n\n4. Bernstein also proved generalizations of the inequalities above to weakly dependent random variables. For example, inequality (2) can be extended as follows. <math>X_1, \\ldots, X_n</math> be possibly non-independent random variables. Suppose that for all integer <math>i>0</math>,\n\n:<math>\\begin{align}\n\\mathbb{E} \\left. \\left [ X_i   \\right | X_1, \\ldots, X_{i-1} \\right ] &= 0, \\\\\n\\mathbb{E} \\left. \\left [ X_i^2 \\right | X_1, \\ldots, X_{i-1} \\right ] &\\leq R_i \\mathbb{E} \\left [ X_i^2 \\right ], \\\\\n\\mathbb{E} \\left. \\left [ X_i^k \\right | X_1, \\ldots, X_{i-1} \\right ] &\\leq \\tfrac{1}{2} \\mathbb{E} \\left. \\left[ X_i^2 \\right | X_1, \\ldots, X_{i-1} \\right ] L^{k-2} k!\n\\end{align}</math>\n\nThen\n\n:<math>\\mathbb{P} \\left( \\sum_{i=1}^n X_i \\geq 2t \\sqrt{\\sum_{i=1}^n R_i \\mathbb{E}\\left [ X_i^2 \\right ]} \\right) < \\exp(-t^2), \\qquad \\text{for}\\quad  0 < t \\leq \\frac{1}{2L} \\sqrt{\\sum_{i=1}^n R_i \\mathbb{E} \\left [X_i^2 \\right ]}. </math>\n\nMore general results for martingales can be found in  Fan et al. (2015).<ref name=fan>{{cite journal |title=Exponential inequalities for martingales with applications| first1=X. |last1=Fan|  first2=I. |last2=Grama | first3=Q. |last3=Liu |publisher=Electron. J. Probab. 20| year=2015| pages=1–22| url=http://projecteuclid.org/euclid.ejp/1465067107 | doi=10.1214/EJP.v20-3496|arxiv=1311.6273}}</ref>\n\n==Proofs==\n\nThe proofs are based on an application of [[Markov's inequality]] to the random variable\n\n:<math> \\exp \\left ( \\lambda \\sum_{j=1}^n X_j \\right ),</math>\n\nfor a suitable choice of the parameter <math>\\lambda > 0</math>.\n\n==See also==\n*[[Concentration inequality]] - a summary of tail-bounds on random variables.\n\n==References==\n\n(according to: S.N.Bernstein, Collected Works, Nauka, 1964)\n\n{{reflist}}\n\nA modern translation of some of these results can also be found in {{SpringerEOM| title=Bernstein inequality | id=Bernstein_inequality | oldid=15217 | first=A.V. | last=Prokhorov | first2=N.P. | last2=Korneichuk | first3=V.P. | last3=Motornyi }}\n\n{{DEFAULTSORT:Bernstein Inequalities (Probability Theory)}}\n[[Category:Probabilistic inequalities]]"
    },
    {
      "title": "Boole's inequality",
      "url": "https://en.wikipedia.org/wiki/Boole%27s_inequality",
      "text": "{{more footnotes|date=February 2012}}\n{{Probability fundamentals}}\n\nIn [[probability theory]], '''Boole's inequality''', also known as the '''union bound''', says that for any [[finite set|finite]] or [[countable]] [[Set (mathematics)|set]] of [[Event (probability theory)|event]]s, the probability that at least one of the events happens is no greater than the sum of the probabilities of the individual events. Boole's inequality is named after [[George Boole]].\n\nFormally, for a countable set of events ''A''<sub>1</sub>, ''A''<sub>2</sub>, ''A''<sub>3</sub>, ..., we have\n\n:<math>\\mathbb{P}\\left(\\bigcup_{i} A_i\\right) \\le \\sum_i {\\mathbb P}(A_i).</math>\n\nIn [[measure theory|measure-theoretic]] terms, Boole's inequality follows from the fact that a measure (and certainly any [[probability measure]]) is ''σ''-[[Subadditivity|sub-additive]].\n\n==Proof==\n\n=== Proof using induction ===\nBoole's inequality may be proved for finite collections of events using the method of induction.\n\nFor the <math>n=1</math> case, it follows that\n\n:<math>\\mathbb P(A_1) \\le \\mathbb P(A_1).</math>\n\nFor the case <math>n</math>, we have\n\n:<math>{\\mathbb P}\\left (\\bigcup_{i=1}^{n} A_i\\right ) \\le \\sum_{i=1}^{n} {\\mathbb P}(A_i).</math>\n\nSince <math>\\mathbb P(A \\cup B) = \\mathbb P(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B),</math> and because the union operation is [[associative]], we have\n\n:<math>\\mathbb{P}\\left(\\bigcup_{i=1}^{n+1}A_i\\right) = \\mathbb{P}\\left(\\bigcup_{i=1}^n A_i\\right) + \\mathbb{P}(A_{n+1}) -\\mathbb{P} \\left(\\bigcup_{i=1}^n A_i \\cap A_{n+1}\\right).</math>\n\nSince \n\n:<math>{\\mathbb P}\\biggl(\\bigcup_{i=1}^n A_i \\cap A_{n+1}\\biggr) \\ge 0,</math> \n\nby the [[Probability Axioms#First axiom|first axiom of probability]], we have\n\n:<math>\\mathbb{P}\\left(\\bigcup_{i=1}^{n+1} A_i\\right ) \\le \\mathbb{P}\\left (\\bigcup_{i=1}^n A_i\\right) + \\mathbb{P}(A_{n+1}),</math>\n\nand therefore\n\n:<math>\\mathbb{P}\\left(\\bigcup_{i=1}^{n+1} A_i\\right ) \\le \\sum_{i=1}^{n} \\mathbb{P}(A_i) + \\mathbb{P}(A_{n+1}) = \\sum_{i=1}^{n+1} \\mathbb{P}(A_i).</math>\n\n=== Proof without using induction ===\nFor any events in <math>A_1, A_2, A_3, \\dots </math>in our probability space we have\n\n:<math>\\mathbb{P}\\left(\\bigcup_{i} A_i\\right) \\leq \\sum_i \\mathbb P(A_i)</math>\n\nOne of the ''axioms'' of a probability space is that if <math>B_1, B_2, B_3, \\dots</math> are ''disjoint'' subsets of the probability space then\n\n:<math>\\mathbb{P}\\left(\\bigcup_{i} B_i\\right) = \\sum_i \\mathbb P(B_i)</math>\n\nthis is called ''countable additivity.''\n\nIf <math>B \\subset A</math> then <math>\\mathbb P (B) \\leq \\mathbb P(A)</math>\n\nIndeed, from the axioms of a probability distribution,\n\n:<math>\\mathbb P (A) = \\mathbb P(B) + \\mathbb P(A-B)</math>\n\nNote that both terms on the right are nonnegative.\n\nNow we have to modify the sets <math>A_i</math>, so they become disjoint.\n\n:<math>B_i = A_i - \\bigcup^{i-1}_{j=1} A_j</math>\n\nSo if <math>B_i \\subset A_i</math>, then we know\n\n:<math>\\bigcup^{\\infty}_{i=1} B_i = \\bigcup^{\\infty}_{i=1} A_i</math>\n\nTherefore, we can deduce the following equation\n\n:<math>\\mathbb P \\left(\\bigcup_iA_i\\right) = \\mathbb P \\left(\\bigcup_iB_i\\right) = \\sum_i \\mathbb P (B_i) \\leq \\sum_i \\mathbb P(A_i)</math>\n\n==Bonferroni inequalities==\nBoole's inequality may be generalized to find [[upper bound|upper]] and [[upper bound|lower bound]]s on the probability of [[finite unions]] of events.<ref>{{cite book |first=George |last=Casella |first2=Roger L. |last2=Berger |title=Statistical Inference |location= |publisher=Duxbury |year=2002 |isbn=0-534-24312-6 |pages=11–13 |url=https://books.google.com/books?id=0x_vAAAAMAAJ&pg=PA11 }}</ref> These bounds are known as '''Bonferroni inequalities''', after [[Carlo Emilio Bonferroni]], see {{harvtxt|Bonferroni|1936}}.\n\nDefine\n\n:<math>S_1 := \\sum_{i=1}^n {\\mathbb P}(A_i),</math>\n\nand\n\n:<math>S_2 := \\sum_{1\\le i<j\\le n} {\\mathbb P} \\left (A_i \\cap A_j \\right ),</math>\n\nas well as\n\n:<math>S_k := \\sum_{1\\le i_1<\\cdots<i_k\\le n} {\\mathbb P} \\left (A_{i_1}\\cap \\cdots \\cap A_{i_k} \\right )</math>\n\nfor all integers ''k'' in {3, ..., ''n''}.\n\nThen, for [[even and odd numbers|odd]] ''k'' in {1, ..., ''n''},\n\n:<math>{\\mathbb P} \\left( \\bigcup_{i=1}^n A_i \\right) \\le \\sum_{j=1}^k (-1)^{j-1} S_j,</math>\n\nand for [[even and odd numbers|even]] ''k'' in {2, ..., ''n''},\n\n:<math>{\\mathbb P}\\left ( \\bigcup_{i=1}^n A_i \\right) \\ge \\sum_{j=1}^k (-1)^{j-1} S_j.</math>\n\nBoole's inequality is recovered by setting ''k'' = 1. When ''k'' = ''n'', then equality holds and the resulting identity is the [[inclusion–exclusion principle]].\n\n==See also==\n* [[Diluted inclusion–exclusion principle]]\n* [[Schuette–Nesbitt formula]]\n* [[Fréchet inequalities|Boole–Fréchet inequalities]]\n\n==References==\n{{Reflist}}\n* {{Citation | last = Bonferroni | first = Carlo E.\n  | author-link = Carlo Emilio Bonferroni\n  | title = Teoria statistica delle classi e calcolo delle probabilità\n  | journal = Pubbl. d. R. Ist. Super. di Sci. Econom. e Commerciali di Firenze\n  | volume = 8\n  | pages = 1–62\n  | year = 1936\n  | language = Italian\n  | zbl = 0016.41103}}\n* {{Citation\n  | last = Dohmen\n  | first = Klaus\n  | title = Improved Bonferroni Inequalities via Abstract Tubes. Inequalities and Identities of Inclusion–Exclusion Type\n  | place = Berlin\n  | publisher = [[Springer-Verlag]]\n  | series = Lecture Notes in Mathematics\n  | year = 2003\n  | volume = 1826\n  | pages = viii+113\n  | isbn = 3-540-20025-8\n  | mr = 2019293\n  | zbl = 1026.05009}}\n* {{Citation\n  | last = Galambos\n  | first = János\n  | author-link = Janos Galambos\n  | last2 = Simonelli\n  | first2 = Italo\n  | title = Bonferroni-Type Inequalities with Applications\n  | place = New York\n  | publisher = [[Springer-Verlag]]\n  | series = Probability and Its Applications\n  | year = 1996\n  | pages = x+269\n  | isbn = 0-387-94776-0\n  | mr = 1402242\n  | zbl = 0869.60014}}\n* {{Citation\n  | last = Galambos\n  | first = János\n  | author-link = Janos Galambos\n  | title = Bonferroni inequalities\n  | journal = Annals of Probability\n  | volume = 5\n  | issue = 4\n  | pages = 577–581\n  | year = 1977\n  | url = http://projecteuclid.org/euclid.aop/1176995765\n  | doi = 10.1214/aop/1176995765\n  | jstor = 2243081\n  | mr = 0448478\n  | zbl = 0369.60018}}\n* {{SpringerEOM| title = Bonferroni inequalities | id= Bonferroni_inequalities | oldid = 18515 | last = Galambos | first = János | author-link= Janos Galambos}}\n\n{{PlanetMath attribution|id=6049|title=Bonferroni inequalities}}\n\n[[Category:Probabilistic inequalities]]\n[[Category:Statistical inequalities]]"
    },
    {
      "title": "Borell–TIS inequality",
      "url": "https://en.wikipedia.org/wiki/Borell%E2%80%93TIS_inequality",
      "text": "In [[mathematics]] and [[probability]], the '''Borell–TIS inequality''' is a result bounding the probability of a deviation of the [[uniform norm]] of a centred [[Gaussian measure|Gaussian]] [[stochastic process]] above its [[expected value]].  The result is named for [[Christer Borell]] and its independent discoverers [[Boris Tsirelson]], [[Ildar Ibragimov (mathematician)|Ildar Ibragimov]], and [[Vladimir Sudakov]].  The inequality has been described as &ldquo;the single most important tool in the study of Gaussian processes.&rdquo;<ref name=\"RFG\">{{cite book\n| chapter = Gaussian Inequalities\n| title = Random Fields and Geometry\n| year = 2007\n| publisher = Springer New York\n| location = New York, NY\n| pages = 49&ndash;64\n| isbn = 978-0-387-48116-6\n| doi = 10.1007/978-0-387-48116-6_2}}\n</ref>\n\n==Statement==\n\nLet <math>T</math> be a [[topological space]], and let <math>\\{ f_t \\}_{t \\in T}</math> be a centred (i.e. mean zero) [[Gaussian process]] on <math>T</math>, with\n\n::<math>\\| f \\|_T := \\sup_{t \\in T} | f_t |</math>\n\n[[almost surely]] finite, and let\n\n::<math>\\sigma_T^2 := \\sup_{t \\in T} \\operatorname{E}| f_t |^2.</math>\n\nThen<ref name=\"RFG\"/> <math>\\operatorname{E}(\\| f \\|_T)</math> and <math>\\sigma_T</math> are both finite, and, for each <math>u > 0</math>,\n\n::<math>\\operatorname{P} \\big( \\| f \\|_T > \\operatorname{E}(\\| f \\|_T) + u \\big) \\leq \\exp\\left(  \\frac{- u^2}{2\\sigma_T^2} \\right). </math>\n\n== See also ==\n* [[Gaussian isoperimetric inequality]]\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Borell-TIS inequality}}\n[[Category:Probabilistic inequalities]]"
    },
    {
      "title": "Cantelli's inequality",
      "url": "https://en.wikipedia.org/wiki/Cantelli%27s_inequality",
      "text": "In [[probability theory]], '''Cantelli's inequality''' is a generalization of [[Chebyshev's inequality]] in the case of a single \"tail\".<ref>''Research and practice in multiple criteria decision making: proceedings of the XIVth International Conference on Multiple Criteria Decision Making (MCDM), Charlottesville, Virginia, USA, June 8–12, 1998'', edited by Y.Y. Haimes and R.E. Steuer, [[Springer Verlag|Springer]], 2000, {{ISBN|3540672664}}.</ref><ref>[http://www.cse.buffalo.edu/~hungngo/classes/2011/Spring-694/lectures/l4.pdf \"Tail and Concentration Inequalities\" by Hung Q. Ngo]</ref><ref>[http://www.econ.upf.edu/~lugosi/anu.pdf \"Concentration-of-measure inequalities\" by Gábor Lugosi]</ref> The inequality states that\n\n: <math>\n\\Pr(X-\\mathbb{E}[X]\\ge\\lambda)\\quad\\begin{cases}\n\\le \\frac{\\sigma^2}{\\sigma^2 + \\lambda^2} & \\text{if } \\lambda > 0, \\\\[8pt]\n\\ge 1 - \\frac{\\sigma^2}{\\sigma^2 + \\lambda^2} & \\text{if }\\lambda < 0.\n\\end{cases}\n</math>\n\nwhere\n\n:<math>X</math> is a real-valued [[random variable]],\n:<math>\\Pr</math> is the [[probability measure]],\n:<math>\\mathbb{E}[X]</math> is the [[expected value]] of <math>X</math>,\n:<math>\\sigma^2</math> is the [[variance]] of <math>X</math>.\n\nCombining the cases of <math>\\lambda >0</math> and <math>\\lambda < 0</math> gives, for <math>\\delta >0,</math>\n\n: <math>\n\\Pr(| X-\\mathbb{E}[X]|\\ge\\delta)\\le \\frac{2\\sigma^2}{\\sigma^2+\\delta^2}.\n</math>\n\nWhile the inequality is often attributed to [[Francesco Paolo Cantelli]] who published it in 1928<ref>Cantelli, F. P. (1928), \"Sui confini della probabilita,\" Atti del Congresso Inter-\n nazional del Matematici, Bologna, 6, 47-5</ref>, it originates in Chebyshev's work of 1874<ref>[https://www.jstor.org/stable/3087296 Ghosh, B.K., 2002. Probability inequalities related to Markov's theorem. ''The American Statistician'', 56(3), pp.186-190]</ref>. The Chebyshev inequality implies that in any [[sample (statistics)|data sample]] or [[probability distribution]], \"nearly all\" values are close to the [[expected value|mean]] in terms of the [[absolute value]] of the difference between the points of the data sample and the weighted average of the data sample. The Cantelli inequality (sometimes called the \"Chebyshev–Cantelli inequality\" or the \"one-sided Chebyshev inequality\") gives a way of estimating how the points of the data sample are bigger than or smaller than their weighted average without the two tails of the absolute value estimate. The Chebyshev inequality has [[Chebyshev's inequality#Higher moments|\"higher moments versions\"]] and [[Chebyshev's inequality#Vector version|\"vector versions\"]], and so does the Cantelli inequality.\n\n==Proof==\n* Case <math>\\lambda > 0</math>:\n\nLet <math>X</math> be a real-valued random variable with finite variance <math>\\sigma^2</math> and expectation <math>\\mu</math>, and define <math>Y = X - \\mathbb{E}[X]</math> (so that <math>\\mathbb{E}[Y] = 0</math> and <math>\\operatorname{Var}(Y) = \\sigma^2</math>).\n\nThen, for any <math>u\\geq 0</math>, we have\n: <math>\n\\Pr( X-\\mathbb{E}[X]\\geq\\lambda)\n= \\Pr( Y  \\geq \\lambda) \n= \\Pr( Y + u  \\geq \\lambda + u)\n\\leq  \\Pr( (Y + u)^2  \\geq (\\lambda + u)^2 )\n\\leq \\frac{\\mathbb{E}[(Y + u)^2] }{(\\lambda + u)^2}\n= \\frac{\\sigma^2 + u^2 }{(\\lambda + u)^2}.\n</math>\nthe last inequality being a consequence of [[Markov's inequality]]. As the above holds for any choice of <math>u\\in\\mathbb{R}</math>, we can choose to apply it with the value that minimizes the function <math>u \\geq 0 \\mapsto \\frac{\\sigma^2 + u^2 }{(\\lambda + u)^2}</math>. By differentiating, this can be seen to be <math>u_\\ast = \\frac{\\sigma^2}{\\lambda}</math>, leading to\n: <math>\n\\Pr( X-\\mathbb{E}[X] \\geq\\lambda) \\leq \\frac{\\sigma^2 + u_\\ast^2 }{(\\lambda + u_\\ast)^2} = \\frac{\\sigma^2}{\\lambda^2 + \\sigma^2}\n</math> if <math>\\lambda > 0</math>\n\n* Case <math>\\lambda < 0</math>: we proceed as before, writing <math>\\alpha = -\\lambda > 0</math> and for any <math>u \\geq 0</math>\n: <math>\n\\Pr( X-\\mathbb{E}[X]< \\lambda)\n= \\Pr( -Y  > \\alpha) \\leq  \\frac{\\sigma^2}{\\alpha^2 + \\sigma^2} = \\frac{\\sigma^2}{\\lambda^2 + \\sigma^2}\n</math>\nusing the previous derivation on <math>-Y</math>. By taking the complement of the left-hand side, we obtain\n: <math>\n\\Pr( X-\\mathbb{E}[X] \\geq \\lambda) \\geq \\frac{\\lambda^2}{\\lambda^2 + \\sigma^2}\n</math> if <math>\\lambda < 0</math>\n\n==References==\n{{reflist}}\n\n\n{{probability-stub}}\n\n[[Category:Probabilistic inequalities]]"
    },
    {
      "title": "Chebyshev's inequality",
      "url": "https://en.wikipedia.org/wiki/Chebyshev%27s_inequality",
      "text": "{{For|the similarly named inequality involving series|Chebyshev's sum inequality}}\n\nIn [[probability theory]], '''Chebyshev's inequality''' (also called the '''Bienaymé–Chebyshev inequality''') guarantees that, for a wide class of [[probability distributions]], no more than a certain fraction of values can be more than a certain distance from the [[expected value|mean]]. Specifically, no more than 1/''k''<sup>2</sup> of the distribution's values can be more than ''k'' [[standard deviations]] away from the mean (or equivalently, at least 1&nbsp;−&nbsp;1/''k''<sup>2</sup> of the distribution's values are within ''k'' standard deviations of the mean). The rule is often called Chebyshev's theorem, about the range of standard deviations around the mean, in statistics. The inequality has great utility because it can be applied to any probability distribution in which the mean and variance are defined. For example, it can be used to prove the [[weak law of large numbers]].\n\nIn practical usage, in contrast to the [[68–95–99.7 rule]], which applies to [[normal distribution]]s, Chebyshev's inequality is weaker, stating that a minimum of just 75% of values must lie within two standard deviations of the mean and 89% within three standard deviations.<ref name=Kvanli>{{cite book |last1=Kvanli |first1=Alan H. |last2=Pavur |first2=Robert J. |last3=Keeling |first3= Kellie B. |title=Concise Managerial Statistics |url=https://books.google.com/books?id=h6CQ1J0gwNgC&pg=PT95 |year=2006 |publisher=[[cEngage Learning]] |isbn=9780324223880 |pages=81–82}}</ref><ref name=Chernick>{{cite book |last=Chernick |first=Michael R. |title=The Essentials of Biostatistics for Physicians, Nurses, and Clinicians |url=https://books.google.com/books?id=JP4azqd8ONEC&pg=PA50 |year=2011 |publisher=[[John Wiley & Sons]] |isbn=9780470641859 |pages=49–50}}</ref>\n\nThe term ''Chebyshev's inequality'' also refer to [[Markov's inequality]], especially in the context of analysis. They are closely related, and some authors refer to [[Markov's inequality]] as \"Chebyshev's First Inequality,\" and the similar one referred to on this page as \"Chebyshev's Second Inequality.\"\n\n==History==\nThe theorem is named after Russian mathematician [[Pafnuty Chebyshev]], although it was first formulated by his friend and colleague [[Irénée-Jules Bienaymé]].<ref>{{cite book\n |title=The Art of Computer Programming: Fundamental Algorithms, Volume 1\n |edition=3rd\n |last1=Knuth |first1=Donald |authorlink1=Donald Knuth\n |year=1997\n |publisher=Addison–Wesley\n |location=Reading, Massachusetts\n |isbn=978-0-201-89683-1\n |url=http://www-cs-faculty.stanford.edu/~uno/taocp.html\n |accessdate=1 October 2012\n |ref=KnuthTAOCP1\n}}\n</ref>{{rp|98}} The theorem was first stated without proof by Bienaymé in 1853<ref name=\"Bienaymé1853\">{{cite journal | last1 = Bienaymé | first1 = I.-J. | year = 1853 | title = Considérations àl'appui de la découverte de Laplace | url = | journal = Comptes Rendus de l'Académie des Sciences | volume = 37 | issue = | pages = 309–324 }}</ref> and later proved by Chebyshev in 1867.<ref name=Chebyshev1867>{{cite journal|last=Tchebichef|first=P.|title=Des valeurs moyennes|journal=Journal de Mathématiques Pures et Appliquées|year=1867|volume=12|series=2|pages=177–184}}</ref> His student [[Andrey Markov]] provided another proof in his 1884 Ph.D. thesis.<ref name=Markov1884>Markov A. (1884) On certain applications of algebraic continued fractions, Ph.D. thesis, St. Petersburg</ref>\n\n==Statement==\nChebyshev's inequality is usually stated for [[random variable]]s, but can be generalized to a statement about [[measure theory|measure spaces]].\n\n===Probabilistic statement===\nLet ''X'' (integrable) be a [[random variable]] with finite [[expected value]] ''μ'' and finite non-zero [[variance]] ''σ''<sup>2</sup>. Then for any [[real number]] {{nowrap|''k'' > 0}},\n: <math>\n    \\Pr(|X-\\mu|\\geq k\\sigma) \\leq \\frac{1}{k^2}.\n  </math>\nOnly the case <math>k > 1</math> is useful. When <math>k \\leq 1</math> the right-hand side <math> \\frac{1}{k^2} \\geq 1 </math> and the inequality is trivial as all probabilities are&nbsp;≤&nbsp;1.\n\nAs an example, using <math>k = \\sqrt{2}</math> shows that the probability that values lie outside the interval <math>(\\mu - \\sqrt{2}\\sigma, \\mu + \\sqrt{2}\\sigma)</math> does not exceed <math>\\frac{1}{2}</math>.\n\nBecause it can be applied to completely arbitrary distributions provided they have a known finite mean and variance, the inequality generally gives a poor bound compared to what might be deduced if more aspects are known about the distribution involved.\n\n{|class=\"wikitable\" style=\"background-color:#FFFFFF; text-align:center\"\n|-\n! k\n! Min. % within ''k'' standard{{ns}}<br>deviations of mean\n! Max. % beyond ''k'' standard<br>deviations from mean\n|-\n|  1\n|| 0%\n|| 100%\n|-\n|  {{sqrt|2}}\n|| 50%\n|| 50%\n|-\n|  1.5\n|| 55.56%\n|| 44.44%\n|-\n|  2\n|| 75%\n|| 25%\n|-\n|  2{{sqrt|2}}\n|| 87.5%\n|| 12.5%\n|-\n|  3\n|| 88.8889%\n|| 11.1111%\n|-\n|  4\n|| 93.75%\n|| 6.25%\n|-\n|  5\n|| 96%\n|| 4%\n|-\n|  6\n|| 97.2222%\n|| 2.7778%\n|-\n|  7\n|| 97.9592%\n|| 2.0408%\n|-\n|  8\n|| 98.4375%\n|| 1.5625%\n|-\n|  9\n|| 98.7654%\n|| 1.2346%\n|-\n|  10\n|| 99%\n|| 1%\n|}\n\n===Measure-theoretic statement===\nLet (''X'',&nbsp;Σ,&nbsp;μ) be a [[measure space]], and let ''f'' be an [[extended real number line|extended real]]-valued [[measurable function]] defined on ''X''. Then for any real number ''t'' > 0 and 0 < ''p'' < ∞,<ref>{{cite book|last1=Grafakos|first1=Lukas|title=Classical and Modern Fourier Analysis|date=2004|publisher=Pearson Education Inc.|page=5}}</ref>\n\n:<math>\\mu(\\{x\\in X\\,:\\,\\,|f(x)|\\geq t\\}) \\leq {1\\over t^p} \\int_{|f| \\geq t} |f|^p \\, d\\mu.</math>\n\nMore generally, if ''g'' is an extended real-valued measurable function, nonnegative and nondecreasing on the range of ''f'', then{{citation needed|date=May 2012}}\n\n:<math>\\mu(\\{x\\in X\\,:\\,\\,f(x)\\geq t\\}) \\leq {1\\over g(t)} \\int_X g\\circ f\\, d\\mu.</math>\n\nThe previous statement then follows by defining <math>g(x)</math> as <math>|x|^p</math> if <math>x\\ge t</math> and <math>0</math> otherwise.\n\n==Example==\nSuppose we randomly select a journal article from a source with an average of 1000 words per article, with a standard deviation of 200 words. We can then infer that the probability that it has between 600 and 1400 words (i.e. within ''k''&nbsp;=&nbsp;2 standard deviations of the mean) must be at least 75%, because there is no more than {{nowrap|{{frac|1|''k''{{su|p=2}}}} {{=}} {{frac2|1|4}}}} chance to be outside that range, by Chebyshev's inequality. But if we additionally know that the distribution is normal, we can say there is a 75% chance the word count is between 770 and 1230 (which is an even tighter bound).\n\n==Sharpness of bounds==\nAs shown in the example above, the theorem typically provides rather loose bounds. However, these bounds cannot in general (remaining true for arbitrary distributions) be improved upon. The bounds are sharp for the following example: for any ''k''&nbsp;≥&nbsp;1,\n: <math>\n    X = \\begin{cases}\n        -1, & \\text{with probability }\\frac{1}{2k^2} \\\\\n         0, & \\text{with probability }1 - \\frac{1}{k^2} \\\\\n         1, & \\text{with probability }\\frac{1}{2k^2}\n        \\end{cases}\n  </math>\n\nFor this distribution, the mean ''μ'' = 0 and the standard deviation ''σ'' = {{frac2|1|''k'' }}, so\n: <math>\n    \\Pr(|X-\\mu| \\ge k\\sigma) = \\Pr(|X| \\ge 1) = \\frac{1}{k^2}.\n  </math>\nChebyshev's inequality is an equality for precisely those distributions that are a linear transformation of this example.\n\n==Proof (of the two-sided version)==\n\n===Probabilistic proof===\n[[Markov's inequality]] states that for any real-valued random variable ''Y'' and any positive number ''a'', we have Pr(|''Y''|&nbsp;>&nbsp;''a'') ≤ E(|''Y''|)/''a''. One way to prove Chebyshev's inequality is to apply Markov's inequality to the random variable {{math|''Y'' {{=}} (''X'' − ''μ'')<sup>2</sup>}} with ''a'' = (''kσ'')<sup>2</sup>.\n\nIt can also be proved directly. For any event ''A'', let ''I''<sub>''A''</sub> be the indicator random variable of ''A'', i.e. ''I''<sub>''A''</sub> equals 1 if ''A'' occurs and 0 otherwise. Then\n\n:<math>\\begin{align}\n\\Pr(|X-\\mu| \\geq k\\sigma) &= \\operatorname{E} \\left (I_{|X-\\mu| \\geq k\\sigma} \\right ) \\\\[6pt]\n&= \\operatorname{E} \\left (I_{\\left (\\frac{X-\\mu}{k\\sigma} \\right )^2 \\geq 1} \\right ) \\\\[6pt]\n&\\leq \\operatorname{E}\\left(\\left({X-\\mu \\over k\\sigma} \\right)^2 \\right) \\\\[6pt]\n&= {1 \\over k^2} {\\operatorname{E}((X-\\mu)^2) \\over \\sigma^2} \\\\[6pt]\n&= {1 \\over k^2}.\n\\end{align}</math>\n\nThe Inequality of direct proof shows why the bounds are quite loose in typical cases:\n\n# If <math>0\\leq(\\frac{X-\\mu}{k\\sigma})^2 < 1</math>, instead of taking the indicating value 0 as given by the left side of the inequality, a positive value of <math>(\\frac{X-\\mu}{k\\sigma})^2</math> is counted.\n# If <math>(\\frac{X-\\mu}{k\\sigma})^2 \\geq 1</math>, instead of taking the indicating value 1 as given by the left side of the inequality, a value <math>(\\frac{X-\\mu}{k\\sigma})^2</math> greater or equal to 1 is counted. In some cases it exceeds 1 by a very wide margin.\n\n===Measure-theoretic proof===\nFix <math>t</math> and let <math>A_t</math> be defined as <math>A_t = \\{x\\in X\\mid f(x)\\ge t\\}</math>, and let <math>1_{A_t}</math> be the [[indicator function]] of the set&nbsp;<math>A_t</math>. Then, it is easy to check that, for any <math>x</math>,\n\n:<math>0\\leq g(t) 1_{A_t}\\leq g(f(x))\\,1_{A_t},</math>\n\nsince ''g'' is nondecreasing on the range of ''f'', and therefore,\n\n:<math>\\begin{align}g(t)\\mu(A_t)&=\\int_X g(t)1_{A_t}\\,d\\mu\\\\ &\\leq\\int_{A_t} g\\circ f\\,d\\mu\\\\ &\\leq\\int_X g\\circ f\\,d\\mu.\\end{align}</math>\n\nThe desired inequality follows from dividing the above inequality by&nbsp;''g''(''t'').\n\n==Extensions==\nSeveral extensions of Chebyshev's inequality have been developed.\n\n===Asymmetric two-sided case===\nAn asymmetric two-sided version of this inequality is also known.<ref name=Steliga2010>{{cite journal |last1=Steliga |first1=Katarzyna |last2=Szynal |first2=Dominik |title=On Markov-Type Inequalities |journal=International Journal of Pure and Applied Mathematics |year=2010 |volume=58 |issue=2 |pages=137–152 |url=http://ijpam.eu/contents/2010-58-2/2/2.pdf |accessdate=10 October 2012 |issn=1311-8080}}</ref>\n\nWhen the distribution is known to be symmetric for any <math> k_1 + k_2 = 0 </math>\n: <math> \\Pr( k_1 < X < k_2) \\ge 1 - \\frac{ 4 \\sigma^2 }{ ( k_2 - k_1 )^2 }</math>\nwhere ''σ''<sup>2</sup> is the [[variance]].\n\nSimilarly when the distribution is asymmetric or is unknown and <math> k_1 + k_2 = 2 \\mu </math>\n\n: <math> \\Pr( k_1 < X < k_2 ) \\ge \\frac{ 4 [ ( \\mu - k_1 )( k_2 - \\mu ) - \\sigma^2 ] }{ ( k_2 - k_1 )^2 },</math>\n\nwhere {{math|''σ''<sup>2</sup>}} is the variance and {{mvar|μ}} is the [[mean]].\n\n===Bivariate case===\nA version for the bivariate case is known.<ref name=Ferentinos1982>{{cite journal | last1 = Ferentinos | first1 = K | year = 1982 | title = On Tchebycheﬀ type inequalities | url = | journal = Trabajos Estadıst Investigacion Oper | volume = 33 | issue = | pages = 125–132 | doi = 10.1007/BF02888707 }}</ref>\n\nLet {{math|''X''<sub>1</sub>, ''X''<sub>2</sub>}} be two random variables with means {{math|''μ''<sub>1</sub>, ''μ''<sub>2</sub>}} and finite variances {{math|''σ''<sub>1</sub>, ''σ''<sub>2</sub>}} respectively. Then\n\n:<math> \\Pr( k_{ 11 } \\le X_1 \\le k_{ 12 }, k_{ 21 } \\le X_2 \\le k_{ 22 }) \\ge 1 - \\sum T_i</math>\n\nwhere for ''i'' = 1, 2,\n\n:<math> T_i = \\frac{ 4 \\sigma_i^2 + [ 2 \\mu_i - ( k_{ i1 } + k_{ i2 } ) ]^2 } { ( k_{ i2 } - k_{ i1 } )^2 }.</math>\n\n===Two correlated variables===\nBerge derived an inequality for two correlated variables {{math|''X''<sub>1</sub>, ''X''<sub>2</sub>}}.<ref name=Berge1938>{{cite journal | last1 = Berge | first1 = P. O. | year = 1938 | title = A note on a form of Tchebycheff's theorem for two variables | url = | journal = Biometrika | volume = 29 | issue = 3/4| pages = 405–406 | doi=10.2307/2332015| jstor = 2332015 }}</ref> Let {{mvar|ρ}} be the correlation coefficient between ''X''<sub>1</sub> and ''X''<sub>2</sub> and let ''σ''<sub>''i''</sub><sup>2</sup> be the variance of {{mvar|X<sub>i</sub>}}. Then\n\n: <math> \\Pr\\left( \\bigcap_{ i = 1}^2 \\left[ \\frac{ | X_i - \\mu_i | } { \\sigma_i }  < k \\right] \\right) \\ge 1 - \\frac{ 1 + \\sqrt{ 1 - \\rho^2 } } { k^2 }.</math>\n\nLal later obtained an alternative bound<ref name=Lal1955>Lal D. N. (1955) A note on a form of Tchebycheﬀ's inequality for two or more variables. [[Sankhya (journal)|Sankhya]] 15(3):317–320</ref>\n\n: <math> \\Pr\\left( \\bigcap_{i=1}^2 \\left[ \\frac{|X_i - \\mu_i|}{\\sigma_i} \\le k_i \\right] \\right) \\ge 1 - \\frac{k_1^2 + k_2^2 + \\sqrt{ ( k_1^2 + k_2^2 )^2 - 4 k_1^2 k_2^2 \\rho}}{2( k_1 k_2)^2} </math>\n\nIsii derived a further generalisation.<ref name=Isii1959>Isii K. (1959) On a method for generalizations of Tchebycheff's inequality. Ann Inst Stat Math 10: 65–88</ref> Let\n\n: <math> Z = \\Pr\\left(\\left (-k_1 < X_1 < k_2 \\right) \\cap \\left(-k_1 < X_2 < k_2 \\right) \\right), \\qquad 0 < k_1 \\leq k_2.</math>\n\nand define:\n\n: <math> \\lambda = \\frac{ k_1( 1 + \\rho ) + \\sqrt{ ( 1 - \\rho^2 )( k_1^2 + \\rho ) } }{ 2k_1 - 1 + \\rho } </math>\n\nThere are now three cases.\n\n*'''Case A:''' If <math> 2k_1^2 > 1 - \\rho </math> and <math> k_2 - k_1 \\ge 2 \\lambda </math> then\n::<math> Z \\le \\frac{ 2 \\lambda^2 } { 2 \\lambda^2 + 1 + \\rho }. </math>\n\n*'''Case B:''' If the conditions in case A are not met but {{math|''k''<sub>1</sub>''k''<sub>2</sub> ≥ 1}} and\n::<math> 2 ( k_1 k_2 - 1 )^2 \\ge 2( 1 - \\rho^2 ) + ( 1 - \\rho )( k_2 - k_1 )^2 </math>\n:then\n::<math> Z \\le \\frac{ ( k_2 - k_1 )^2 + 4 + \\sqrt{ 16 ( 1 - \\rho^2 ) + 8 ( 1 - \\rho )( k_2 - k_1 ) } }{ ( k_1 +k_2 )^2 }.</math>\n\n*'''Case C:''' If none of the conditions in cases A or B are satisfied then there is no universal bound other than 1.\n\n===Multivariate case===\nThe general case is known as the Birnbaum–Raymond–Zuckerman inequality after the authors who proved it for two dimensions.<ref name=Birnbaum1947>{{cite journal |last1=Birnbaum |first1=Z. W. |last2=Raymond |first2=J. |last3=Zuckerman |first3=H. S. |title=A Generalization of Tshebyshev's Inequality to Two Dimensions |journal=The Annals of Mathematical Statistics |issn=0003-4851 |year=1947 |volume=18 |issue=1 |pages=70–79 |doi=10.1214/aoms/1177730493 |mr=19849 |zbl=0032.03402 |url=http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aoms/1177730493 |accessdate=7 October 2012}}</ref>\n\n:<math>\\Pr\\left(\\sum_{i=1}^n \\frac{(X_i - \\mu_i)^2}{\\sigma_i^2 t_i^2} \\ge k^2 \\right) \\le \\frac{1}{k^2} \\sum_{i=1}^n \\frac{1}{t_i^2} </math>\n\nwhere {{mvar|X<sub>i</sub>}} is the {{mvar|i}}-th random variable, {{mvar|μ<sub>i</sub>}} is the {{mvar|i}}-th mean and ''σ''<sub>i</sub><sup>2</sup> is the {{mvar|i}}-th variance.\n\nIf the variables are independent this inequality can be sharpened.<ref name=Kotz2000>{{cite book |last1=Kotz |first1=Samuel  |authorlink1=Samuel Kotz |last2=Balakrishnan |first2=N. |last3= Johnson |first3=Norman L. |authorlink3=Norman Lloyd Johnson |title=Continuous Multivariate Distributions, Volume 1, Models and Applications |year=2000 |publisher=Houghton Mifflin |location=Boston [u.a.] |isbn=978-0-471-18387-7 |url=http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471183873.html |edition=2nd |accessdate=7 October 2012}}</ref>\n\n:<math>\\Pr\\left (\\bigcap_{i = 1}^n \\frac{|X_i - \\mu_i|}{\\sigma_i} \\le k_i \\right ) \\ge \\prod_{i=1}^n \\left (1 - \\frac{1}{k_i^2} \\right)</math>\n\nOlkin and Pratt derived an inequality for {{mvar|n}} correlated variables.<ref name=Olkin1958>{{cite journal|last1=Olkin|first1=Ingram |authorlink1=Ingram Olkin | last2=Pratt |first2=John W. |authorlink2=John W. Pratt |title=A Multivariate Tchebycheff Inequality| journal=The Annals of Mathematical Statistics|year=1958|volume=29|issue=1|pages=226–234|doi=10.1214/aoms/1177706720|url=http://projecteuclid.org/euclid.aoms/1177706720|zbl=0085.35204 |mr=93865 |accessdate=2 October 2012}}</ref>\n\n: <math> \\Pr\\left(\\bigcap_{i = 1 }^n \\frac{|X_i - \\mu_i|}{\\sigma_i} < k_i \\right) \\ge 1 - \\frac{1}{n^2} \\left(\\sqrt{u} + \\sqrt{n-1} \\sqrt{n \\sum_i \\frac 1 { k_i^2} - u} \\right)^2 </math>\n\nwhere the sum is taken over the ''n'' variables and\n\n: <math> u = \\sum_{i=1}^n \\frac{1}{ k_i^2} + 2\\sum_{i=1}^n \\sum_{j<i} \\frac{\\rho_{ij}}{k_i k_j} </math>\n\nwhere {{mvar|ρ<sub>ij</sub>}} is the correlation between {{mvar|X<sub>i</sub>}} and {{mvar|X<sub>j</sub>}}.\n\nOlkin and Pratt's inequality was subsequently generalised by Godwin.<ref name=Godwin1964>Godwin H. J. (1964) Inequalities on distribution functions. New York, Hafner Pub. Co.</ref>\n\n===Vector version===\nFerentinos<ref name=\"Ferentinos1982\"/> has shown that for a [[Multivariate random variable|vector]] {{math|''X'' {{=}} (''x''<sub>1</sub>, ''x''<sub>2</sub>, ...)}} with mean {{math|''μ'' {{=}} (''μ''<sub>1</sub>, ''μ''<sub>2</sub>, ...)}}, standard deviation ''σ'' = (''σ''<sub>1</sub>, ''σ''<sub>2</sub>, ...) and the Euclidean norm {{math|{{!!}} ⋅ {{!!}}}} that\n\n: <math> \\Pr(\\| X - \\mu \\| \\ge k \\| \\sigma \\|) \\le \\frac{ 1 } { k^2 }. </math>\n\nA second related inequality has also been derived by Chen.<ref name=Chen2007>{{cite arXiv |author=Xinjia Chen |eprint=0707.0805v2 |title=A New Generalization of Chebyshev Inequality for Random Vectors |year=2007 |version=|class=math.ST }}</ref> Let {{mvar|n}} be the [[dimension]] of the stochastic vector {{mvar|X}} and let {{math|E(''X'')}} be the mean of {{mvar|X}}. Let {{mvar|S}} be the [[covariance matrix]] and {{math|''k'' > 0}}. Then\n\n: <math> \\Pr \\left( ( X - \\operatorname{E}(X) )^T S^{-1} (X - \\operatorname{E}(X)) < k  \\right) \\ge 1 - \\frac{n}{k} </math>\n\nwhere ''Y''<sup>T</sup> is the [[transpose]] of {{mvar|Y}}.  A simple proof was obtained in Navarro<ref name=Navarro2013>{{cite journal |author=Jorge Navarro |volume=45 |issue=12 |pages=3458–3463 |title=A very simple proof of the multivariate Chebyshev's inequality |journal=Communications in Statistics – Theory and Methods |year=2016 |doi=10.1080/03610926.2013.873135}}</ref> as follows:\n\n: <math> Z=( X - \\operatorname{E}(X) )^T S^{-1} (X - \\operatorname{E}(X)) \n=( X - \\operatorname{E}(X) )^T S^{-1/2}S^{-1/2} (X - \\operatorname{E}(X))=Y^TY\\geq 0 </math>\n\nwhere\n\n:<math> Y=(Y_1,...,Y_n)^T=S^{-1/2}(X-\\operatorname{E}(X)) </math>\n\nand <math>S^{-1/2}</math> is a symmetric invertible matrix such that: <math>S^{-1/2}S^{-1/2}=S^{-1}</math>. Hence <math> \\operatorname E(Y)=(0,\\ldots,0)^T </math> and <math> \\operatorname{Cov}(Y)=I_n </math> where <math> I_n </math> represents the identity matrix of dimension&nbsp;''n''. Then <math> \\operatorname E(Y_i^2)=\\operatorname{Var}(Y_i)=1</math> and\n\n: <math>\\operatorname E(Z) = \\operatorname E(Y^TY)=\\sum_{i=1}^n \\operatorname E(Y_i^2)= n</math>\n\nFinally, by applying [[Markov's inequality]] to Z we get\n\n: <math>  \\Pr\\left( Z\\geq k\\right )=\n\\Pr \\left( ( X - \\operatorname{E}(X) )^T S^{-1} (X - \\operatorname{E}(X)) \\geq k  \\right) \n\\le \\frac{\\operatorname E(Z)}{k}=\\frac{n}{k} </math>\n\nand so the desired inequality holds.\n\nThe inequality can be written in terms of the [[Mahalanobis distance]] as\n\n: <math> \\Pr \\left( d^2_S(X,\\operatorname{E}(X)) < k  \\right) \\ge 1 - \\frac{n}{k} </math>\n\nwhere the Mahalanobis distance based on S is defined by\n\n: <math> d_S(x,y) =\\sqrt{ (x -y)^T S^{-1} (x -y) } </math>\n\nNavarro<ref name=Navarro2014>{{cite journal |author=Jorge Navarro |volume=91 |pages=1–5 |title=Can the bounds in the multivariate Chebyshev inequality be attained?   |journal=Statistics and Probability Letters  |year=2014 |doi=10.1016/j.spl.2014.03.028}}</ref> proved that these bounds are sharp, that is, they are the best possible bounds for that regions when we just know the mean and the covariance matrix of X.\n\nStellato et al.<ref name=\":0\">{{Cite journal|last=Stellato|first=Bartolomeo|last2=Parys|first2=Bart P. G. Van|last3=Goulart|first3=Paul J.|date=2016-05-31|title=Multivariate Chebyshev Inequality with Estimated Mean and Variance|journal=The American Statistician|volume=0|issue=ja|pages=123–127|doi=10.1080/00031305.2016.1186559|issn=0003-1305|arxiv=1509.08398}}</ref> showed that this multivariate version of the Chebyshev inequality can be easily derived analytically as a special case of Vandenberghe et al.<ref>{{Cite journal|last=Vandenberghe|first=L.|last2=Boyd|first2=S.|last3=Comanor|first3=K.|date=2007-01-01|title=Generalized Chebyshev Bounds via Semidefinite Programming|journal=SIAM Review|volume=49|issue=1|pages=52–64|doi=10.1137/S0036144504440543|issn=0036-1445|bibcode=2007SIAMR..49...52V|citeseerx=10.1.1.126.9105}}</ref> where the bound is computed by solving a [[Semidefinite programming|semidefinite program (SDP).]]\n\n===Infinite dimensions===\nThere is a straightforward extension of the vector version of Chebyshev's inequality to infinite dimensional settings. Let {{mvar|X}} be a random variable which takes values in a [[Fréchet space]] <math>\\mathcal X</math> (equipped with seminorms {{math|{{!!}} ⋅ {{!!}}<sub>''α''</sub>}}). This includes most common settings of vector-valued random variables, e.g., when <math>\\mathcal X</math> is a [[Banach space]] (equipped with a single norm), a [[Hilbert space]], or the finite-dimensional setting as described above.\n\nSuppose that {{mvar|X}} is of \"[[strong order two]]\", meaning that\n\n: <math> \\operatorname{E}\\left(\\| X\\|_\\alpha^2 \\right) < \\infty </math>\n\nfor every seminorm {{math|{{!!}} ⋅ {{!!}}<sub>''α''</sub>}}. This is a generalization of the requirement that {{mvar|X}} have finite variance, and is necessary for this strong form of Chebyshev's inequality in infinite dimensions. The terminology \"strong order two\" is due to [[Vakhania]].<ref>Vakhania, Nikolai Nikolaevich. Probability distributions on linear spaces. New York: North Holland, 1981.</ref>\n\nLet <math>\\mu \\in \\mathcal X</math> be the [[Pettis integral]] of {{mvar|X}} (i.e., the vector generalization of the mean), and let\n\n:<math>\\sigma_a := \\sqrt{\\operatorname{E}\\|X - \\mu\\|_\\alpha^2}</math>\n\nbe the standard deviation with respect to the seminorm {{math|{{!!}} ⋅ {{!!}}<sub>''α''</sub>}}. In this setting we can state the following:\n\n:'''General version of Chebyshev's inequality.''' <math>\\forall k > 0: \\quad \\Pr\\left( \\|X - \\mu\\|_\\alpha \\ge k \\sigma_\\alpha \\right) \\le \\frac{1}{ k^2 }.</math>\n\n'''Proof.''' The proof is straightforward, and essentially the same as the finitary version. If {{math|''σ<sub>α</sub>'' {{=}} 0}}, then {{mvar|X}} is constant (and equal to {{mvar|μ}}) almost surely, so the inequality is trivial.\n\nIf\n\n:<math>\\|X - \\mu\\|_\\alpha \\ge k \\sigma_\\alpha^2</math>\n\nthen {{math|{{!!}}''X'' − ''μ''{{!!}}<sub>''α''</sub> > 0}}, so we may safely divide by {{math|{{!!}}''X'' − ''μ''{{!!}}<sub>''α''</sub>}}. The crucial trick in Chebyshev's inequality is to recognize that <math>1 = \\tfrac{\\|X - \\mu\\|_\\alpha^2}{\\|X - \\mu\\|_\\alpha^2}</math>.\n\nThe following calculations complete the proof:\n\n:<math>\\begin{align}\n\\Pr\\left( \\|X - \\mu\\|_\\alpha \\ge k \\sigma_\\alpha \\right) &= \\int_\\Omega \\mathbf{1}_{\\|X - \\mu\\|_\\alpha \\ge k \\sigma_\\alpha} \\, \\mathrm d\\Pr \\\\\n& = \\int_\\Omega \\left ( \\frac{\\|X - \\mu\\|_\\alpha^2}{\\|X - \\mu\\|_\\alpha^2} \\right ) \\cdot \\mathbf{1}_{\\|X - \\mu\\|_\\alpha \\ge k \\sigma_\\alpha} \\, \\mathrm d\\Pr \\\\[6pt]\n&\\le \\int_\\Omega \\left (\\frac{\\|X - \\mu\\|_\\alpha^2}{(k\\sigma_\\alpha)^2} \\right ) \\cdot \\mathbf{1}_{\\|X - \\mu\\|_\\alpha \\ge k \\sigma_\\alpha} \\, \\mathrm d\\Pr \\\\[6pt]\n&\\le \\frac{1}{k^2 \\sigma_\\alpha^2} \\int_\\Omega \\|X - \\mu\\|_\\alpha^2 \\, \\mathrm d\\Pr && \\mathbf{1}_{\\|X - \\mu\\|_\\alpha \\ge k \\sigma_\\alpha} \\le 1\\\\[6pt]\n&= \\frac{1}{k^2 \\sigma_\\alpha^2} \\left (\\operatorname{E}\\|X - \\mu\\|_\\alpha^2 \\right )\\\\[6pt]\n&= \\frac{1}{k^2 \\sigma_\\alpha^2} \\left (\\sigma_\\alpha^2 \\right )\\\\[6pt]\n&= \\frac{1}{k^2}\n\\end{align}</math>\n\n===Higher moments===\nAn extension to higher moments is also possible:\n\n:<math> \\Pr\\left(| X - \\operatorname{E}(X) | \\ge k \\operatorname{E}(|X - \\operatorname{E}(X) |^n )^{ \\frac{1}{n} }\\right) \\le \\frac{1 } {k^n}, \\qquad k >0, n \\geq 2.</math>\n\n===Exponential version===\nA related inequality sometimes known as the exponential Chebyshev's inequality<ref name=RassoulAgha2010>[http://www.math.utah.edu/~firas/Papers/rassoul-seppalainen-ldp.pdf  Section 2.1] {{webarchive |url=https://web.archive.org/web/20150430075226/http://www.math.utah.edu/~firas/Papers/rassoul-seppalainen-ldp.pdf |date=April 30, 2015 }}</ref> is the inequality\n\n:<math> \\Pr(X \\ge \\varepsilon) \\le e^{ -t \\varepsilon }\\operatorname{E}\\left (e^{ t X } \\right), \\qquad t > 0.</math>\n\nLet {{math|''K''(''t'')}} be the [[cumulant generating function]],\n\n: <math> K( t ) = \\log \\left(\\operatorname{E}\\left( e^{ t x } \\right) \\right). </math>\n\nTaking the [[Legendre–Fenchel transformation]]{{clarify|reason=articles should be reasonably self contained, more explanation needed|date=May 2012}} of {{math|''K''(''t'')}} and using the exponential Chebyshev's inequality we have\n\n: <math>-\\log( \\Pr (X \\ge \\varepsilon )) \\ge \\sup_t( t \\varepsilon - K( t ) ). </math>\n\nThis inequality may be used to obtain exponential inequalities for unbounded variables.<ref name=Baranoski2001>{{cite journal |last1=Baranoski |first1=Gladimir V. G. |last2=Rokne |first2=Jon G. |last3=Xu |first3=Guangwu |title=Applying the exponential Chebyshev inequality to the nondeterministic computation of form factors |journal=Journal of Quantitative Spectroscopy and Radiative Transfer |date=15 May 2001 |volume=69 |issue=4 |pages=199–200 |doi=10.1016/S0022-4073(00)00095-9 |bibcode=2001JQSRT..69..447B}} (the references for this article are corrected by {{cite journal|last1=Baranoski |first1=Gladimir V. G. |last2=Rokne |first2=Jon G. |author3=Guangwu Xu |title=Corrigendum to: 'Applying the exponential Chebyshev inequality to the nondeterministic computation of form factors' |journal=Journal of Quantitative Spectroscopy and Radiative Transfer |date=15 January 2002 |volume=72 |issue=2 |pages=199–200 |doi=10.1016/S0022-4073(01)00171-6 |bibcode=2002JQSRT..72..199B}})</ref>\n\n===Inequalities for bounded variables===\nIf P(''x'') has finite support based on the interval {{math|[''a'', ''b'']}}, let {{math|''M'' {{=}} max({{!}}''a''{{!}}, {{!}}''b''{{!}})}} where |''x''| is the [[absolute value]] of {{mvar|x}}. If the mean of P(''x'') is zero then for all {{math|''k'' > 0}}<ref name=Dufour2003>Dufour (2003) [http://www2.cirano.qc.ca/~dufourj/Web_Site/ResE/Dufour_1999_C_TS_Moments.pdf Properties of moments of random variables]</ref>\n\n: <math>\\frac{\\operatorname{E}(|X|^r ) - k^r }{M^r} \\le \\Pr( | X |  \\ge k ) \\le \\frac{\\operatorname{E}(| X |^r ) }{ k^r }.</math>\n\nThe second of these inequalities with {{math|''r'' {{=}} 2}} is the Chebyshev bound. The first provides a lower bound for the value of P(''x'').\n\nSharp bounds for a bounded variate have been proposed by Niemitalo, but without a proof though<ref name=Niemitalo2012>Niemitalo O. (2012) [http://yehar.com/blog/?p=1225 One-sided Chebyshev-type inequalities for bounded probability distributions.]</ref>\n\nLet {{math|0 ≤ ''X'' ≤ ''M''}} where {{math|''M'' > 0}}. Then\n\n*'''Case 1:'''\n:: <math>\\Pr(X<k) = 0 \\qquad \\text{if} \\qquad \\operatorname{E}(X) > k \\quad \\text{and} \\quad \\operatorname{E}(X^2) < k\\operatorname{E}(X) + M\\operatorname{E}(X) - kM </math>\n\n*'''Case 2:'''\n::<math>\\Pr(X<k) \\ge  1 - \\frac{k\\operatorname{E}(X) + M\\operatorname{E}(X) - \\operatorname{E}(X^2)}{kM} \\qquad \\text{if} \\qquad \\begin{cases} \\operatorname{E}(X)> k \\quad \\text{and} \\quad \\operatorname E( X^2 )\\ge k\\operatorname{E}(X) + M\\operatorname{E}(X) - kM \\\\ \\qquad \\qquad \\qquad \\text{or} \\\\ \\operatorname{E}(X) \\le k \\quad \\text{and} \\quad \\operatorname{E}(X^2)\\ge k\\operatorname{E}(X) \\end{cases}</math>\n\n*'''Case 3:'''\n::<math>\\Pr(X<k) \\ge \\frac{\\operatorname{E}(X)^2 - 2 k\\operatorname{E}(X) + k^2 }{\\operatorname{E}(X^2)- 2 k\\operatorname{E}(X) + k^2 } \\qquad \\text{if} \\qquad \\operatorname{E}(X) \\le k \\quad \\text{and} \\quad \\operatorname{E}(X^2) < k\\operatorname{E}(X)</math>\n\n==Finite samples==\n\n=== Univariate case ===\nSaw ''et al'' extended Chebyshev's inequality to cases where the population mean and variance are not known and may not exist, but the sample mean and sample standard deviation from ''N'' samples are to be employed to bound the expected value of a new drawing from the same distribution.<ref name=\":1\">{{cite journal\n  |title = Chebyshev Inequality with Estimated Mean and Variance\n  |last1 = Saw\n  |first1 = John G.\n  |last2 = Yang\n  |first2 = Mark C. K.\n  |last3 = Mo\n  |first3 = Tse Chin\n  |journal = [[The American Statistician]]\n  |issn = 0003-1305\n  |volume = 38\n  |issue = 2\n  |year = 1984\n  |pages = 130–2\n  |doi = 10.2307/2683249\n  |jstor = 2683249\n  \n  }}</ref>\n\n: <math> P( | X - m | \\ge ks) \\le \\frac{ g_{N + 1}\\left( \\frac{N k^2}{N - 1 + k^2} \\right) }{N + 1} \\left( \\frac N {N + 1} \\right)^{1/2} </math>\n\nwhere ''X'' is a random variable which we have sampled ''N'' times, ''m'' is the sample mean, ''k'' is a constant and ''s'' is the sample standard deviation. ''g''(''x'') is defined as follows:\n\nLet ''x'' ≥ 1, ''Q'' = ''N'' + 1, and ''R'' be the greatest integer less than ''Q''/''x''. Let\n\n: <math> a^ 2 = \\frac{ Q( Q - R ) } { 1 + R( Q - R ) }. </math>\n\nNow\n\n: <math>\ng_Q(x) = \\begin{cases}\nR & \\text{if }R \\text{ is even,} \\\\\nR & \\text{if }R \\text{ is odd and }x < a^2, \\\\\nR - 1 & \\text{if } R \\text{ is odd and } x \\ge a^2.\n\\end{cases}\n</math>\n\nThis inequality holds even when the population moments do not exist, and when the sample is only [[Exchangeable random variables|weakly exchangeably]] distributed; this criterion is met for randomised sampling. A table of values for the Saw–Yang–Mo inequality for finite sample sizes (''N'' < 100) has been determined by Konijn.<ref name=Konijn1987>{{cite journal |last=Konijn |first=Hendrik S. |title=Distribution-Free and Other Prediction Intervals |journal=[[The American Statistician]] |date=February 1987 |volume=41 |issue=1 |pages=11–15 |jstor=2684311 |doi=10.2307/2684311  }}</ref> The table allows the calculation of various confidence intervals for the mean, based on multiples, C, of the standard error of the mean as calculated from the sample. For example, Konijn shows that for ''N''&nbsp;=&nbsp;59, the 95 percent confidence interval for the mean ''m'' is {{nowrap|(''m'' − ''Cs'', ''m'' + ''Cs'')}} where {{nowrap|1=''C'' = 4.447 × 1.006 = 4.47}} (this is 2.28 times larger than the value found on the assumption of normality showing the loss on precision resulting from ignorance of the precise nature of the distribution).\n\nKabán gives a somewhat less complex version of this inequality.<ref name=\"Kabán2011\">{{cite journal\n  |last = Kabán\n  |first = Ata\n  |title = Non-parametric detection of meaningless distances in high dimensional data\n  |journal = [[Statistics and Computing]]\n  |volume = 22\n  |issue = 2\n  |pages = 375–85\n  |year = 2012\n  |doi = 10.1007/s11222-011-9229-0\n}}</ref>\n\n: <math>P( | X - m | \\ge ks ) \\le \\frac 1 {N + 1} \\left\\lfloor \\frac {N+1} N \\left(\\frac{N - 1}{k^2} + 1 \\right) \\right\\rfloor</math>\n\nIf the standard deviation is a multiple of the mean then a further inequality can be derived,<ref name=\"Kabán2011\" />\n\n: <math>P( | X - m | \\ge ks ) \\le \\frac{N - 1} N \\frac 1 {k^2} \\frac{s^2}{m^2} + \\frac 1 N.</math>\n\nA table of values for the Saw–Yang–Mo inequality for finite sample sizes (''N'' < 100) has been determined by Konijn.<ref name=\"Konijn1987\"/>\n\nFor fixed ''N'' and large ''m'' the Saw–Yang–Mo inequality is approximately<ref name=Beasley2004>{{cite journal |last1=Beasley |first1=T. Mark |last2=Page |first2=Grier P. |last3=Brand |first3=Jaap P. L. |last4=Gadbury |first4=Gary L. |last5=Mountz |first5=John D. |last6=Allison |first6=David B. |authorlink6=David B. Allison |title=Chebyshev's inequality for nonparametric testing with small ''N'' and α in microarray research |journal=Journal of the Royal Statistical Society |issn=1467-9876 |date=January 2004 |volume=53 |series=C (Applied Statistics) |issue=1 |pages=95–108 |doi=10.1111/j.1467-9876.2004.00428.x }}</ref>\n\n: <math> P( | X - m | \\ge ks ) \\le \\frac 1 {N + 1}. </math>\n\nBeasley ''et al'' have suggested a modification of this inequality<ref name=Beasley2004 />\n\n: <math> P( | X - m | \\ge ks ) \\le \\frac 1 {k^2( N + 1 )}. </math>\n\nIn empirical testing this modification is conservative but appears to have low statistical power. Its theoretical basis currently remains unexplored.\n\n====Dependence on sample size====\nThe bounds these inequalities give on a finite sample are less tight than those the Chebyshev inequality gives for a distribution. To illustrate this let the sample size ''N'' = 100 and let ''k'' = 3. Chebyshev's inequality states that at most approximately 11.11% of the distribution will lie at least three standard deviations away from the mean. Kabán's version of the inequality for a finite sample states that at most approximately 12.05% of the sample lies outside these limits. The dependence of the confidence intervals on sample size is further illustrated below.\n\nFor ''N'' = 10, the 95% confidence interval is approximately ±13.5789 standard deviations.\n\nFor ''N'' = 100 the 95% confidence interval is approximately ±4.9595 standard deviations; the 99% confidence interval is approximately ±140.0 standard deviations.\n\nFor ''N'' = 500 the 95% confidence interval is approximately ±4.5574 standard deviations; the 99% confidence interval is approximately ±11.1620 standard deviations.\n\nFor ''N'' = 1000 the 95% and 99% confidence intervals are approximately ±4.5141 and approximately ±10.5330 standard deviations respectively.\n\nThe Chebyshev inequality for the distribution gives 95% and 99% confidence intervals of approximately ±4.472 standard deviations and ±10 standard deviations respectively.\n\n====Samuelson's inequality====\nAlthough Chebyshev's inequality is the best possible bound for an arbitrary distribution, this is not necessarily true for finite samples. [[Samuelson's inequality]] states that all values of a sample will lie within  {{radic|''N''&nbsp;−&nbsp;1}} standard deviations of the mean. Chebyshev's bound improves as the sample size increases.\n\nWhen ''N'' = 10, Samuelson's inequality states that all members of the sample lie within 3 standard deviations of the mean: in contrast Chebyshev's states that 99.5% of the sample lies within 13.5789 standard deviations of the mean.\n\nWhen ''N'' = 100, Samuelson's inequality states that all members of the sample lie within approximately 9.9499 standard deviations of the mean: Chebyshev's states that 99% of the sample lies within 10 standard deviations of the mean.\n\nWhen ''N'' = 500, Samuelson's inequality states that all members of the sample lie within approximately 22.3383 standard deviations of the mean: Chebyshev's states that 99% of the sample lies within 10 standard deviations of the mean.\n\n=== Multivariate case ===\nStellato et al.<ref name=\":0\" /> simplified the notation and extended the empirical Chebyshev inequality from Saw et al.<ref name=\":1\" /> to the multivariate case. Let <math display=\"inline\">\\xi \\in \\mathbb{R}^{n_\\xi}</math> be a random variable and let <math display=\"inline\">N \\in \\mathbb{Z}_{\\geq n_\\xi}</math>. We draw <math display=\"inline\">N+1</math> iid samples of <math display=\"inline\">\\xi</math> denoted as <math display=\"inline\">\\xi^{(1)},\\dots,\\xi^{(N)},\\xi^{(N+1)} \\in \\mathbb{R}^{n_\\xi}</math>. Based on the first <math display=\"inline\">N</math> samples, we define the empirical mean as <math display=\"inline\">\\mu_N = \\frac 1 N \\sum_{i=1}^N \\xi^{(i)}</math> and the unbiased empirical covariance as <math display=\"inline\">\\Sigma_N = \\frac 1 N \\sum_{i=1}^N (\\xi^{(i)} - \\mu_{N})(\\xi^{(i)} - \\mu_N)^\\top</math>. If <math>\\Sigma_N</math> is nonsingular, then for all <math>\\lambda \\in \\mathbb{R}_{\\geq 0} </math> then\n\n: <math>\n\\begin{align}\n& P^{N+1} \\left((\\xi^{(N+1)} - \\mu_N)^\\top \\Sigma_N^{-1}(\\xi^{(N+1)} - \\mu_N) \\geq \\lambda^2\\right) \\\\[8pt]\n\\leq {} & \\min\\left\\{1, \\frac 1 {N+1} \\left\\lfloor \\frac{n_\\xi(N+1)(N^2 - 1 + N\\lambda^2)}{N^2\\lambda^2}\\right\\rfloor\\right\\}.\n\\end{align}\n</math>\n\n==== Remarks ====\nIn the univariate case, i.e. <math display=\"inline\">n_\\xi = 1</math>, this inequality corresponds to the one from Saw et al.<ref name=\":1\" />  Moreover, the right-hand side can be simplified by upper bounding the floor function by its argument\n\n: <math>P^{N+1}\\left((\\xi^{(N+1)} - \\mu_N)^\\top \\Sigma_N^{-1}(\\xi^{(N+1)} - \\mu_N) \\geq \\lambda^2\\right) \\leq \\min\\left\\{1,  \\frac{n_\\xi(N^2 - 1 + N\\lambda^2)}{N^2\\lambda^2}\\right\\}.</math>\n\nAs <math display=\"inline\">N \\to \\infty</math>, the right-hand side tends to <math display=\"inline\">\\min \\left\\{1, \\frac{n_\\xi}{\\lambda^2}\\right\\}</math> which corresponds to the [[Chebyshev's inequality#Vector version|multivariate Chebyshev inequality]] over ellipsoids shaped according to <math display=\"inline\">\\Sigma</math> and centered in <math display=\"inline\">\\mu</math>.\n\n==Sharpened bounds==\nChebyshev's inequality is important because of its applicability to any distribution. As a result of its generality it may not (and usually does not) provide as sharp a bound as alternative methods that can be used if the distribution of the random variable is known. To improve the sharpness of the bounds provided by Chebyshev's inequality a number of methods have been developed; for a review see eg.<ref>[http://nvlpubs.nist.gov/nistpubs/jres/65B/jresv65Bn3p211_A1b.pdf Savage, I. Richard. \"Probability inequalities of the Tchebycheff type.\" Journal of Research of the National Bureau of Standards-B. Mathematics and Mathematical Physics B 65 (1961): 211-222]</ref>\n\n===Standardised variables===\nSharpened bounds can be derived by first standardising the random variable.<ref name=Ion2001>{{cite book|last=Ion|first=Roxana Alice|title=Nonparametric Statistical Process Control|year=2001|publisher=Universiteit van Amsterdam|isbn=978-9057760761|chapter-url=http://dare.uva.nl/document/60326|accessdate=1 October 2012|chapter=Chapter 4: Sharp Chebyshev-type inequalities}}</ref>\n\nLet ''X'' be a random variable with finite variance Var(''X''). Let ''Z'' be the standardised form defined as\n\n:<math> Z = \\frac {X - \\operatorname{E}(X) } {  \\operatorname{Var}(X)^{ 1/2 } }.</math>\n\n[[Cantelli's inequality|Cantelli's lemma]] is then\n\n:<math> P(Z \\ge k) \\le \\frac{ 1 } { 1 + k^2 }.</math>\n\nThis inequality is sharp and is attained by ''k'' and −1/''k'' with probability 1/(1&nbsp;+&nbsp;''k''<sup>2</sup>) and ''k''<sup>2</sup>/(1&nbsp;+&nbsp;''k''<sup>2</sup>) respectively.\n\nIf ''k'' > 1 and the distribution of ''X'' is symmetric then we have\n\n:<math> P(Z \\ge k) \\le \\frac { 1 } { 2 k^2 } .</math>\n\nEquality holds if and only if ''Z'' = −''k'', 0 or ''k'' with probabilities {{nowrap|1= 1 / 2 ''k''<sup>2</sup>}}, {{nowrap|1 − 1 / ''k''<sup>2</sup>}} and {{nowrap|1 / 2 ''k''<sup>2</sup>}} respectively.<ref name=Ion2001/>\nAn extension to a two-sided inequality is also possible.\n\nLet ''u'', ''v'' > 0. Then we have<ref name=Ion2001/>\n:<math> P(Z \\le -u  \\text{ or }  Z \\ge v) \\le \\frac{ 4 + (u - v)^2 } { (u + v)^2  } .</math>\n\n===Semivariances===\nAn alternative method of obtaining sharper bounds is through the use of [[semivariance]]s (partial variances). The upper (''σ''<sub>+</sub><sup>2</sup>) and lower (''σ''<sub>−</sub><sup>2</sup>) semivariances are defined as\n\n: <math> \\sigma_+^2 = \\frac { \\sum_{x>m} (x - m)^2 } { n - 1 } ,</math>\n\n: <math> \\sigma_-^2 = \\frac { \\sum_{x<m} (m - x)^2 } { n - 1 }, </math>\n\nwhere ''m'' is the arithmetic mean of the sample and ''n'' is the number of elements in the sample.\n\nThe variance of the sample is the sum of the two semivariances:\n\n: <math> \\sigma^2 = \\sigma_+^2 + \\sigma_-^2. </math>\n\nIn terms of the lower semivariance Chebyshev's inequality can be written<ref name=Berck1982>{{cite journal|authorlink1=Peter Berck |last1=Berck |first1=Peter |last2=Hihn |first2=Jairus M. |title=Using the Semivariance to Estimate Safety-First Rules |journal=American Journal of Agricultural Economics |date=May 1982 |volume=64 |issue=2 |pages=298–300 |doi=10.2307/1241139 |url=http://ajae.oxfordjournals.org/content/64/2/298.full.pdf+html |accessdate=8 October 2012 |issn=0002-9092|jstor=1241139 }}</ref>\n\n: <math> \\Pr(x \\le m - a \\sigma_-) \\le \\frac { 1 } { a^2 }.</math>\n\nPutting\n\n: <math> a = \\frac{ k \\sigma } { \\sigma_- }. </math>\n\nChebyshev's inequality can now be written\n\n: <math> \\Pr(x \\le m - k \\sigma) \\le \\frac { 1 } { k^2 } \\frac { \\sigma_-^2 } { \\sigma^2 }.</math>\n\nA similar result can also be derived for the upper semivariance.\n\nIf we put\n\n: <math> \\sigma_u^2 = \\max(\\sigma_-^2, \\sigma_+^2) , </math>\n\nChebyshev's inequality can be written\n\n: <math> \\Pr(| x \\le m - k \\sigma |) \\le \\frac 1 {k^2} \\frac { \\sigma_u^2 } { \\sigma^2 } .</math>\n\nBecause ''σ''<sub>u</sub><sup>2</sup> ≤ ''σ''<sup>2</sup>, use of the semivariance sharpens the original inequality.\n\nIf the distribution is known to be symmetric, then\n\n: <math> \\sigma_+^2 = \\sigma_-^2  = \\frac{ 1 } { 2 } \\sigma^2 </math>\n\nand\n\n: <math> \\Pr(x \\le m - k \\sigma) \\le \\frac 1 {2k^2} .</math>\n\nThis result agrees with that derived using standardised variables.\n\n;Note: The inequality with the lower semivariance has been found to be of use in estimating downside risk in finance and agriculture.<ref name=\"Berck1982\"/><ref name=Nantell1979>{{cite journal |last1=Nantell |first1=Timothy J. |last2=Price |first2=Barbara |title=An Analytical Comparison of Variance and Semivariance Capital Market Theories |journal=[[The Journal of Financial and Quantitative Analysis]] |date=June 1979 |volume=14 |issue=2 |pages=221–42 |doi=10.2307/2330500 |jstor=2330500  }}</ref><ref name=Neave2008>{{cite journal\n  |title = Distinguishing upside potential from downside risk\n  |last1 = Neave\n  |first1 = Edwin H.\n  |last2 = Ross\n  |first2 = Michael N.\n  |last3 = Yang\n  |first3 = Jun\n  |journal = [[Management Research News]]\n  |issn = 0140-9174\n  |year = 2009\n  |volume = 32\n  |issue = 1\n  |pages = 26–36\n  |doi = 10.1108/01409170910922005\n  }}</ref>\n\n===Selberg's inequality===\nSelberg derived an inequality for ''P''(''x'') when ''a'' ≤ ''x'' ≤ ''b''.<ref name=Selberg1940>{{cite journal |last=Selberg |first=Henrik L. |title=Zwei Ungleichungen zur Ergänzung des Tchebycheffschen Lemmas |trans-title=Two Inequalities Supplementing the Tchebycheff Lemma |journal=Skandinavisk Aktuarietidskrift (Scandinavian Actuarial Journal) |year=1940 |volume=1940 |issue=3–4 |pages=121–125 |doi=10.1080/03461238.1940.10404804 |language=German |issn=0346-1238 |oclc=610399869}}</ref> To simplify the notation let\n\n: <math> Y = \\alpha X + \\beta </math>\n\nwhere\n\n: <math> \\alpha  = \\frac{ 2 k }{ b - a }</math>\n\nand\n\n: <math> \\beta  = \\frac{ - ( b + a ) k }{ b - a }. </math>\n\nThe result of this linear transformation is to make ''P''(''a'' ≤ ''X'' ≤ ''b'') equal to ''P''(|''Y''| ≤ ''k'').\n\nThe mean (''μ''<sub>''X''</sub>) and variance (''σ''<sub>''X''</sub>) of ''X'' are related to the mean (''μ''<sub>''Y''</sub>) and variance (''σ''<sub>''Y''</sub>) of ''Y'':\n\n: <math> \\mu_Y = \\alpha \\mu_X + \\beta </math>\n\n: <math> \\sigma_Y^2 = \\alpha^2 \\sigma_X^2. </math>\n\nWith this notation Selberg's inequality states that\n\n: <math> \\Pr( | Y | < k ) \\ge \\frac{ ( k - \\mu_Y )^ 2 }{ ( k - \\mu_Y )^2 + \\sigma_Y^2 } \\quad\\text{ if }\\quad \\sigma_Y^2 \\le \\mu_Y ( k - \\mu_Y ) </math>\n\n: <math>  \\Pr( | Y | < k ) \\ge 1 - \\frac{ \\sigma_Y^2 + \\mu_Y^2 }{ k^2 } \\quad\\text{ if }\\quad \\mu_Y ( k - \\mu_Y ) \\le \\sigma_Y^2 \\le k^2 - \\mu_Y^2 </math>\n\n: <math>  P( | Y | < k ) \\ge 0 \\quad\\text{ if }\\quad k^2 - \\mu_Y^2 \\le \\sigma_Y^2.</math>\n\nThese are known to be the best possible bounds.<ref name=Conlon00>{{cite journal |last1=Conlon |first1=J. |last2=Dulá |first2=J. H. |title=A geometric derivation and interpretation of Tchebyscheff's Inequality |url=http://www.people.vcu.edu/~jdula/WORKINGPAPERS/tcheby.pdf |accessdate=2 October 2012}}</ref>\n\n===Cantelli's inequality===\n[[Cantelli's inequality]]<ref name=Cantelli1910>Cantelli F. (1910) Intorno ad un teorema fondamentale della teoria del rischio. Bolletino dell Associazione degli Attuari Italiani</ref> due to [[Francesco Paolo Cantelli]] states that for a real random variable (''X'') with mean (''μ'') and variance (''σ''<sup>2</sup>)\n\n: <math> P(X - \\mu \\ge a) \\le \\frac{\\sigma^2}{ \\sigma^2 + a^2 } </math>\n\nwhere ''a'' ≥ 0.\n\nThis inequality can be used to prove a one tailed variant of Chebyshev's inequality with ''k'' > 0<ref name=Grimmett00>Grimmett and Stirzaker, problem 7.11.9. Several proofs of this result can be found in [http://www.mcdowella.demon.co.uk/Chebyshev.html Chebyshev's Inequalities] by A. G. McDowell.</ref>\n\n:<math> \\Pr(X - \\mu \\geq k \\sigma) \\leq \\frac{ 1 }{ 1 + k^2 }. </math>\n\nThe bound on the one tailed variant is known to be sharp. To see this consider the random variable ''X'' that takes the values\n\n: <math> X = 1 </math> with probability <math> \\frac{ \\sigma^2 } { 1 + \\sigma^2 }</math>\n: <math> X = - \\sigma^2 </math> with probability <math> \\frac{ 1 } { 1 + \\sigma^2 }.</math>\n\nThen E(''X'') = 0 and E(''X''<sup>2</sup>) = ''σ''<sup>2</sup> and P(''X'' < 1) = 1 / (1 + ''σ''<sup>2</sup>).\n\n; An application – distance between the mean and the median <!-- This section is linked from [[median]]. -->\n\nThe one-sided variant can be used to prove the proposition that for [[probability distribution]]s having an [[expected value]] and a [[median]], the mean and the median can never differ from each other by more than one [[standard deviation]].  To express this in symbols let ''μ'', ''ν'', and ''σ'' be respectively the mean, the median, and the standard deviation. Then\n\n:<math> \\left | \\mu - \\nu \\right | \\leq \\sigma. </math>\n\nThere is no need to assume that the variance is finite because this inequality is trivially true if the variance is infinite.\n\nThe proof is as follows. Setting ''k''&nbsp;=&nbsp;1 in the statement for the one-sided inequality gives:\n\n:<math>\\Pr(X - \\mu \\geq \\sigma) \\leq \\frac{ 1 }{ 2 } \\implies \\Pr(X \\geq \\mu + \\sigma) \\leq \\frac{ 1 }{ 2 }. </math>\n\nChanging the sign of ''X'' and of ''μ'', we get\n\n:<math>\\Pr(X \\leq \\mu - \\sigma) \\leq \\frac{ 1 }{ 2 }. </math>\n\nAs the median is by definition any real number&nbsp;''m'' that satisfies the inequalities\n\n:<math>\\operatorname{P}(X\\leq m) \\geq \\frac{1}{2}\\text{ and }\\operatorname{P}(X\\geq m) \\geq \\frac{1}{2}</math>\n\nthis implies that the median lies within one standard deviation of the mean. A proof using Jensen's inequality also [[Median#An inequality relating means and medians|exists]].\n\n===Bhattacharyya's inequality===\nBhattacharyya<ref name=Bhattacharyya1987>{{cite journal |last=Bhattacharyya |first=B. B. |title=One-sided chebyshev inequality when the first four moments are known |journal=Communications in Statistics – Theory and Methods |year=1987 |volume=16 |issue=9 |pages=2789–91 |doi=10.1080/03610928708829540 |issn=0361-0926}}</ref> extended Cantelli's inequality using the third and fourth moments of the distribution.\n\nLet ''μ'' = 0 and ''σ''<sup>2</sup> be the variance. Let ''γ'' = E(''X''<sup>3</sup>)/''σ''<sup>3</sup> and κ = E(''X''<sup>4</sup>)/''σ''<sup>4</sup>.\n\nIf ''k''<sup>2</sup> − ''k''γ − 1 > 0 then\n\n:<math> P(X > k\\sigma) \\le \\frac{ \\kappa - \\gamma^2 - 1 }{ (\\kappa - \\gamma^2 - 1) (1 + k^2) + (k^2 - k\\gamma - 1) }.</math>\n\nThe necessity of ''k''<sup>2</sup> − ''k''γ − 1 > 0 requires that ''k'' be reasonably large.\n\n===Mitzenmacher and Upfal's inequality===\n[[Michael Mitzenmacher|Mitzenmacher]] and [[Eli Upfal|Upfal]]<ref name=Mitzenmacher2005>{{cite book |last1=Mitzenmacher |first1=Michael |authorlink1=Michael Mitzenmacher |last2=Upfal |first2=Eli |authorlink2=Eli Upfal |title=Probability and Computing: Randomized Algorithms and Probabilistic Analysis |date=January 2005 |publisher=Cambridge Univ. Press |location=Cambridge [u.a.] |isbn=9780521835404 |url=http://www.cambridge.org/us/knowledge/isbn/item1171566/?site_locale=en_US |edition=Repr. |accessdate=6 October 2012}}</ref> note that\n\n: <math>  ( X - \\operatorname{E}[ X ] )^{ 2k } > 0 </math>\n\nfor any integer ''k'' > 0 and that\n\n: <math>  \\operatorname{E}[ ( X -  \\operatorname{E}( X ) )^{ 2k } ] </math>\n\nis the 2''k''<sup>th</sup> central moment. They then show that for ''t'' > 0\n\n: <math> \\Pr\\left( | X - \\operatorname{E}[ X ] | > t \\operatorname{E}[ ( X - \\operatorname{E}[ X ] )^{2k} ]^{1/2k} \\right) \\le \\frac 1 { t^{2k} } . </math>\n\nFor ''k'' = 1 we obtain Chebyshev's inequality. For ''t'' ≥ 1, ''k'' > 2 and assuming that the ''k''<sup>th</sup> moment exists, this bound is tighter than Chebyshev's inequality.\n\n==Related inequalities==\nSeveral other related inequalities are also known.\n\n===Zelen's inequality===\nZelen has shown that<ref name=Zelen1954>Zelen M. (1954) Bounds on a distribution function that are functions of moments to order four. J Res Nat Bur Stand 53:377–381</ref>\n\n:<math>\\Pr( X - \\mu  \\ge k \\sigma ) \\le \\left[1 + k^2 + \\frac{\\left( k^2 - k \\theta_3 - 1 \\right)^2}{\\theta_4 - \\theta_3^2 - 1} \\right]^{-1} </math>\n\nwith\n\n:<math>k \\ge \\frac{\\theta_3 + \\sqrt{\\theta_3^2 + 4}}{ 2 }, \\qquad \\theta_m = \\frac{ M_m }{ \\sigma } </math>\n\nwhere {{mvar|M<sub>m</sub>}} is the {{mvar|m}}-th moment{{clarify|date=June 2019|reason=In the formula, the θ's must be dimensionless. Which moments are being referred to here?}} and {{mvar|σ}} is the standard deviation.\n\n===He, Zhang and Zhang's inequality===\nFor any collection of {{mvar|n}} non-negative independent random variables {{mvar|X<sub>i</sub>}} with expectation 1 <ref name=He2010>{{cite journal | last1 = He | first1 = S. | last2 = Zhang | first2 = J. | last3 = Zhang | first3 = S. | year = 2010 | title = Bounding probability of small deviation: A fourth moment approach | url = | journal = Mathematics of Operations Research | volume = 35 | issue = 1| pages = 208–232 | doi = 10.1287/moor.1090.0438 }}</ref>\n\n: <math> \\Pr\\left ( \\frac{\\sum_{i=1}^n X_i }{n} - 1 \\ge \\frac{1}{n} \\right) \\le \\frac{ 7 }{ 8 }. </math>\n\n===Hoeffding's lemma===\n{{main|Hoeffding's lemma}}\nLet {{mvar|X}} be a random variable with {{math|''a'' ≤ ''X'' ≤ ''b''}} and {{math|E[''X''] {{=}} 0}}, then for any {{math|''s'' > 0}}, we have\n\n: <math> E \\left[ e^{ sX } \\right ] \\le e^{ \\frac{1}{8}s^2 ( b - a )^2 }.</math>\n\n===Van Zuijlen's bound===\nLet {{mvar|X<sub>i</sub>}} be a set of independent [[Rademacher distribution|Rademacher random variables]]: {{math|Pr(''X<sub>i</sub>'' {{=}} 1) {{=}} Pr(''X<sub>i</sub>'' {{=}} −1) {{=}} 0.5}}. Then<ref name=vanZuijlen2011>Martien C. A. van Zuijlen (2011) [https://arxiv.org/abs/1112.4988 On a conjecture concerning the sum of independent Rademacher random variables]</ref>\n\n: <math> \\Pr \\left( \\left | \\frac{\\sum_{i=1}^n X_i} \\sqrt n \\right| \\le 1  \\right ) \\ge 0.5. </math>\n\nThe bound is sharp and better than that which can be derived from the normal distribution (approximately {{math|Pr > 0.31}}).\n\n==Unimodal distributions==\nA distribution function ''F'' is unimodal at ''ν'' if its cumulative distribution function is [[convex function|convex]] on (−∞, ''ν'') and [[concave function|concave]] on (''ν'',∞)<ref name=Feller1966>{{cite book |last=Feller |first=William |authorlink=William Feller |title=An Introduction to Probability Theory and Its Applications, Volume 2 |year=1966 |publisher=Wiley |url=https://books.google.com/?id=LhrvAAAAMAAJ&dq=%22An+introduction+to+probability+theory+and+its+applications%22+%22volume+2%22+feller |edition=2 |accessdate=6 October 2012 |page=155}}</ref> An empirical distribution can be tested for unimodality with the [[dip test]].<ref name=Hartigan1985>Hartigan J. A., Hartigan P. M. (1985) [http://projecteuclid.org/euclid.aos/1176346577 \"The dip test of unimodality\"]. ''Annals of Statistics'' 13(1):70–84  {{doi|10.1214/aos/1176346577}} {{MR|773153}}</ref>\n\nIn 1823 [[Gauss]] showed that for a [[unimodal distribution]] with a mode of zero<ref name=Gauss1823>Gauss C. F. Theoria Combinationis Observationum Erroribus Minimis Obnoxiae. Pars Prior. Pars Posterior. Supplementum. Theory of the Combination of Observations Least Subject to Errors. Part One. Part Two. Supplement. 1995. Translated by G. W. Stewart. Classics in Applied Mathematics Series, Society for Industrial and Applied Mathematics, Philadelphia</ref>\n\n: <math> P( | X | \\ge k ) \\le  \\frac{ 4 \\operatorname{ E }( X^2 ) } { 9k^2 } \\quad\\text{if} \\quad k^2 \\ge \\frac{ 4 } { 3 } \\operatorname{E} (X^2) ,</math>\n\n: <math> P( | X | \\ge k ) \\le  1 - \\frac{ k } { \\sqrt{3} \\operatorname{ E }( X^2 ) } \\quad \\text{if} \\quad k^2 \\le \\frac{ 4 } { 3 } \\operatorname{ E }( X^2 ). </math>\n\nIf the mode is not zero and the mean (''μ'') and standard deviation (''σ'') are both finite,  then denoting the median as ''ν'' and the root mean square deviation from the mode by ''ω'', we have{{citation needed|date=July 2012}}\n\n: <math> \\sigma \\le \\omega \\le 2 \\sigma </math>\n\nand\n\n: <math> | \\nu - \\mu | \\le \\sqrt{ \\frac{ 3 }{ 4 } } \\omega. </math>\n\nWinkler in 1866 extended Gauss' inequality to ''r''<sup>th</sup> moments <ref name=Winkler1886>Winkler A. (1886) Math-Natur theorie Kl. Akad. Wiss Wien Zweite Abt 53, 6–41</ref> where ''r'' > 0 and the distribution is unimodal with a mode of zero:\n\n: <math> P( | X | \\ge k ) \\le  \\left( \\frac{ r } { r + 1 } \\right)^r \\frac{ \\operatorname{ E }( | X | )^r } { k^r } \\quad \\text{if} \\quad k^r \\ge \\frac{ r^r } { ( r + 1 )^{ r + 1 } } \\operatorname{ E }( | X |^r ), </math>\n\n: <math> P( | X | \\ge k) \\le \\left( 1 - \\left[ \\frac{ k^r }{ ( r + 1 ) \\operatorname{ E }( | X | )^r } \\right]^{ 1 / r } \\right) \\quad \\text{if} \\quad k^r \\le \\frac{r^r} { (r + 1)^{r + 1} } \\operatorname{E}( | X |^r ). </math>\n\nGauss' bound has been subsequently sharpened and extended to apply to departures from the mean rather than the mode due to the [[Vysochanskiï–Petunin inequality]]. The latter has been extended by Dharmadhikari and Joag-Dev<ref name=Dharmadhikari1985>{{cite journal | last1 = Dharmadhikari | first1 = S. W. | last2 = Joag-Dev | first2 = K. | year = 1985 | title = The Gauss–Tchebyshev inequality for unimodal distributions | url = http://www.mathnet.ru/links/1043b08ee307d37ab64be90bd9332b88/tvp1919.pdf| journal = Teoriya Veroyatnostei i ee Primeneniya | volume = 30 | issue = 4| pages = 817–820 }}</ref>\n\n: <math> P( | X | > k ) \\le \\max\\left( \\left[ \\frac r {( r + 1 ) k } \\right]^r E| X^r |, \\frac s {( s - 1 ) k^r } E| X^r | - \\frac 1 { s - 1 } \\right) </math>\n\nwhere ''s'' is a constant satisfying both ''s'' > ''r'' + 1 and ''s''(''s''&nbsp;−&nbsp;''r''&nbsp;−&nbsp;1) =&nbsp;''r''<sup>''r''</sup> and&nbsp;''r''&nbsp;>&nbsp;0.\n\nIt can be shown that these inequalities are the best possible and that further sharpening of the bounds requires that additional restrictions be placed on the distributions.\n\n===Unimodal symmetrical distributions===\nThe bounds on this inequality can also be sharpened if the distribution is both [[unimodal]] and [[Symmetric probability distribution|symmetrical]].<ref name=Clarkson2009>{{cite journal\n  |title = ROC and the bounds on tail probabilities via theorems of Dubins and F. Riesz\n  |last1 = Clarkson\n  |first1 = Eric\n  |last2 = Denny\n  |first2 = J. L.\n  |last3 = Shepp\n  |first3 = Larry\n  |journal = [[The Annals of Applied Probability]]\n  |volume = 19\n  |issue = 1\n  |pages = 467–76\n  |year = 2009\n  |pmid =  20191100\n  |pmc = 2828638\n  |doi = 10.1214/08-AAP536\n|arxiv = 0903.0518\n  }}</ref> An empirical distribution can be tested for symmetry with a number of tests including McWilliam's R*.<ref>{{cite journal\n  |title = A Distribution-Free Test for Symmetry Based on a Runs Statistic\n  |last = McWilliams\n  |first = Thomas P.\n  |journal = [[Journal of the American Statistical Association]]\n  |issn = 0162-1459\n  |volume = 85\n  |issue = 412\n  |year = 1990\n  |pages = 1130–3\n  |doi = 10.2307/2289611\n  |jstor = 2289611\n  \n  }}</ref> It is known that the variance of a unimodal symmetrical distribution with finite support [''a'',&nbsp;''b''] is less than or equal to ( ''b''&nbsp;−&nbsp;''a'' )<sup>2</sup> / 12.<ref name=Seaman1987>{{cite journal |last1=Seaman |first1=John W., Jr. |last3=Odell |first3=Patrick L. |last2=Young |first2=Dean M. |title=Improving small sample variance estimators for bounded random variables |journal=Industrial Mathematics |issn=0019-8528 |year=1987 |volume=37 |zbl=0637.62024 | pages=65–75}}</ref>\n\nLet the distribution be supported on the finite [[interval (mathematics)|interval]] [ −''N'',&nbsp;''N'' ] and the variance be finite. Let the [[mode (statistics)|mode]] of the distribution be zero and rescale the variance to&nbsp;1. Let ''k''&nbsp;>&nbsp;0 and assume ''k''&nbsp;<&nbsp;2''N''/3. Then<ref name=\"Clarkson2009\"/>\n\n: <math> P( X \\ge k ) \\le \\frac{ 1 }{ 2 } - \\frac{ k }{ 2 \\sqrt{ 3 } } \\quad \\text{if} \\quad 0 \\le k \\le \\frac{ 2 }{ \\sqrt 3  },</math>\n\n: <math> P( X \\ge k ) \\le \\frac{ 2 }{ 9k^2 } \\quad \\text{if} \\quad \\frac{ 2 }{ \\sqrt{ 3 } } \\le k \\le \\frac{ 2N }{ 3 }. </math>\n\nIf 0 < ''k'' ≤ 2 / {{radic|3}} the bounds are reached with the density<ref name=\"Clarkson2009\"/>\n\n: <math> f( x ) = \\frac{ 1 }{ 2 \\sqrt{ 3 } } \\quad \\text{if} \\quad | x | < \\sqrt{ 3 } </math>\n\n: <math> f( x ) = 0 \\quad \\text{if} \\quad  | x | \\ge \\sqrt{ 3 }. </math>\n\nIf 2 / {{radic|3}} < ''k'' ≤ 2''N'' / 3 the bounds are attained by the distribution\n\n:<math> ( 1 - \\beta_k ) \\delta_0 ( x ) + \\beta_k f_k( x ), </math>\n\nwhere ''β''<sub>k</sub> = 4 / 3''k''<sup>2</sup>, ''δ''<sub>0</sub> is the [[Dirac delta function]] and where\n\n: <math> f_k( x ) = \\frac{ 1 }{ 3k } \\quad \\text{if} \\quad | x | < \\frac{ 3k }{ 2 }, </math>\n\n: <math> f_k( x ) = 0 \\quad \\text{if} \\quad | x | \\ge \\frac{ 3k }{ 2 }. </math>\n\nThe existence of these densities shows that the bounds are optimal. Since ''N'' is arbitrary these bounds apply to any value of ''N''.\n\nThe Camp–Meidell's inequality is a related inequality.<ref name=Bickel1992>{{cite journal |last1=Bickel |first1=Peter J. |authorlink1=Peter J. Bickel |last2=Krieger |first2=Abba M. |title=Extensions of Chebyshev's Inequality with Applications |journal=Probability and Mathematical Statistics |year=1992 |volume=13 |issue=2 |pages=293–310 |url=http://www.math.uni.wroc.pl/~pms/files/13.2/Article/13.2.11.pdf |accessdate=6 October 2012 |issn=0208-4147}}</ref> For an absolutely continuous unimodal and symmetrical distribution\n\n: <math> P( | X - \\mu | \\ge k \\sigma ) \\le 1 - \\frac{ k }{ \\sqrt{ 3 } } \\quad \\text{if} \\quad k \\le \\frac{ 2 }{ \\sqrt { 3 } },</math>\n\n: <math> P( | X - \\mu | \\ge k \\sigma ) \\le \\frac{ 4 }{ 9k^2 } \\quad \\text{if} \\quad k > \\frac{ 2 }{ \\sqrt { 3 } }.</math>\n\nDasGupta has shown that if the distribution is known to be normal<ref name=DasGupta2000>{{cite journal | last1 = DasGupta | first1 = A | year = 2000 | title = Best constants in Chebychev inequalities with various applications | url = | journal = Metrika | volume = 5 | issue = 1| pages = 185–200 | doi = 10.1007/s184-000-8316-9 }}</ref>\n\n: <math> P( | X - \\mu | \\ge k \\sigma ) \\le \\frac{ 1 }{ 3 k^2 } .</math>\n\n===Notes===\n====Effects of symmetry and unimodality====\nSymmetry of the distribution decreases the inequality's bounds by a factor of 2 while unimodality sharpens the bounds by a factor of 4/9.{{citation needed|date=March 2019}}\n\nBecause the mean and the mode in a unimodal distribution differ by at most {{radic|3}} standard deviations<ref name=unimodal>{{cite web|url=http://www.se16.info/~se16/hgb/cheb2.htm#3unimodalinequalities |title=More thoughts on a one tailed version of Chebyshev's inequality – by Henry Bottomley |publisher=se16.info |accessdate=2012-06-12}}</ref> at most 5% of a symmetrical unimodal distribution lies outside (2{{radic|10}}&nbsp;+&nbsp;3{{radic|3}})/3 standard deviations of the mean (approximately 3.840 standard deviations). This is sharper than the bounds provided by the Chebyshev inequality (approximately 4.472 standard deviations).\n\nThese bounds on the mean are less sharp than those that can be derived from symmetry of the distribution alone which shows that at most 5% of the distribution lies outside approximately 3.162 standard deviations of the mean. The Vysochanskiï–Petunin inequality further sharpens this bound by showing that for such a distribution that at most 5% of the distribution lies outside 4{{radic|5}}/3 (approximately 2.981) standard deviations of the mean.\n\n====Symmetrical unimodal distributions====\nFor any symmetrical unimodal distribution{{citation needed|date=March 2019}}\n\n*  at most approximately 5.784% of the distribution lies outside 1.96 standard deviations of the mode\n*  at most 5% of the distribution lies outside 2{{radic|10}}/3 (approximately 2.11) standard deviations of the mode\n\n====Normal distributions====\n\nDasGupta's inequality states that for a normal distribution at least 95% lies within approximately 2.582 standard deviations of the mean. This is less sharp than the true figure (approximately 1.96 standard deviations of the mean).\n\n==Bounds for specific distributions==\n*DasGupta has determined a set of best possible bounds for a [[normal distribution]] for this inequality.<ref name=DasGupta2000 />\n*Steliga and Szynal have extended these bounds to the [[Pareto distribution]].<ref name=Steliga2010 />\n*Grechuk et.al. developed a general method for deriving the best possible bounds in Chebyshev's inequality for any family of distributions, and any [[deviation risk measure]] in place of standard deviation. In particular, they derived Chebyshev inequality for distributions with [[Logarithmically concave function|log-concave]] densities.<ref name=\"cheb\">Grechuk, B., Molyboha, A., Zabarankin, M. (2010).\n[https://www.researchgate.net/publication/231939730_Chebyshev_inequalities_with_law-invariant_deviation_measures Chebyshev Inequalities with Law Invariant Deviation Measures], Probability in the Engineering and Informational Sciences, 24(1), 145-170.</ref>\n\n==Zero means==\nWhen the mean (''μ'') is zero Chebyshev's inequality takes a simple form. Let ''σ''<sup>2</sup> be the variance. Then\n\n: <math> P(| X | \\ge 1) \\le \\sigma^2 .</math>\n\nWith the same conditions Cantelli's inequality takes the form\n\n: <math> P(X \\ge 1) \\le \\frac{ \\sigma^2 }{ 1 + \\sigma^2 } .</math>\n\n===Unit variance===\nIf in addition E( ''X''<sup>2</sup> ) = 1 and E( ''X''<sup>4</sup> ) = ''ψ'' then for any 0 ≤ ''ε'' ≤ 1<ref name=Godwin1964a>Godwin H. J. (1964) Inequalities on distribution functions. (Chapter 3) New York, Hafner Pub. Co.</ref>\n\n: <math> \\Pr( | X | > \\varepsilon ) \\ge \\frac{ ( 1 - \\epsilon^2 )^2 }{ \\psi - 1 + ( 1 - \\varepsilon^2 )^2 } \\ge \\frac{( 1 - \\varepsilon^2 )^2 }{ \\psi }.</math>\n\nThe first inequality is sharp. This is known as the [[Paley–Zygmund inequality]].\n\nIt is also known that for a random variable obeying the above conditions that<ref name=Lesley2003>Lesley F. D., Rotar V. I. (2003) Some remarks on lower bounds of Chebyshev's type for half-lines. J Inequalities Pure Appl Math 4(5) Art 96</ref>\n\n: <math> P( X \\ge \\varepsilon ) \\ge \\frac{ C_0 }{ \\psi } - \\frac{ C_1 }{ \\sqrt{ \\psi } } \\varepsilon + \\frac{ C_2 }{ \\psi \\sqrt{ \\psi } } \\varepsilon </math>\n\nwhere\n\n: <math> C_0 = 2 \\sqrt{ 3 } - 3 \\quad ( \\approxeq 0.464 ), </math>\n\n: <math> C_1 = 1.397 ,</math>\n\n: <math> C_2 = 0.0231 .</math>\n\nIt is also known that<ref name=\"Lesley2003\"/>\n\n: <math> \\Pr( X > 0 ) \\ge \\frac{ C_0 }{ \\psi }. </math>\n\nThe value of C<sub>0</sub> is optimal and the bounds are sharp if\n\n: <math> \\psi \\ge \\frac{ 3 }{ \\sqrt{ 3 } + 1 } \\quad ( \\approxeq 1.098 ) .</math>\n\nIf\n\n: <math> \\psi \\le \\frac{ 3 }{ \\sqrt{ 3 } + 1 } </math>\n\nthen the sharp bound is\n\n: <math> P( X > 0 ) \\ge \\frac{ 2 }{ 3 + \\psi + \\sqrt{ ( 1 + \\psi )^2 - 4 } }. </math>\n\n==Integral Chebyshev inequality==\n\nThere is a second (less well known) inequality also named after Chebyshev<ref name=Fink1984>{{cite book |last1=Fink |first1=A. M. |last2=Jodeit |first2=Max, Jr. |title=On Chebyshev's Other Inequality |journal=Institute of Mathematical Statistics Lecture Notes – Monograph Series |isbn=978-0-940600-04-1 |mr=789242 |editor1-first=Y. L. |editor1-last=Tong |editor2-last=Gupta |editor2-first=Shanti S. |year=1984 |volume=5 |series=Institute of Mathematical Statistics Lecture Notes - Monograph Series |pages=115–120 |doi=10.1214/lnms/1215465637 |url=http://projecteuclid.org/euclid.lnms/1215465617 |accessdate=7 October 2012}}</ref>\n\nIf ''f'', ''g'' : [''a'', ''b''] → '''R''' are two [[monotonic]] [[function (mathematics)|function]]s of the same monotonicity, then\n\n: <math> \\frac{ 1 }{ b - a } \\int_a^b \\! f(x) g(x) \\,dx \\ge  \\left[ \\frac{ 1 }{ b - a } \\int_a^b \\! f(x) \\,dx \\right] \\left[ \\frac{ 1 }{ b - a } \\int_a^b \\! g(x) \\,dx \\right] .</math>\n\nIf ''f'' and ''g'' are of opposite monotonicity, then the above inequality works in the reverse way.\n\nThis inequality is related to [[Jensen's inequality]],<ref name=Niculescu2001>{{cite journal |last=Niculescu |first=Constantin P. |title=An extension of Chebyshev's inequality and its connection with Jensen's inequality |journal=Journal of Inequalities and Applications |year=2001 |volume=6 |issue=4 |pages=451–462 |doi=10.1155/S1025583401000273 |url=http://emis.matem.unam.mx/journals/HOA/JIA/Volume6_4/462.html |accessdate=6 October 2012 |issn=1025-5834|citeseerx=10.1.1.612.7056 }}</ref> [[Kantorovich's inequality]],<ref name=Niculescu2001a>{{cite journal |last1=Niculescu |first1=Constantin P. |last2=Pečarić |first2=Josip |authorlink2=Josip Pečarić |title=The Equivalence of Chebyshev's Inequality to the Hermite–Hadamard Inequality |journal=Mathematical Reports |year=2010 |volume=12 |issue=62 |pages=145–156 |url=http://www.csm.ro/reviste/Mathematical_Reports/Pdfs/2010/2/Niculescu.pdf |accessdate=6 October 2012 |issn=1582-3067}}</ref> the [[Hermite–Hadamard inequality]]<ref name=\"Niculescu2001a\"/> and [[Walter's conjecture]].<ref name=Malamud2001>{{cite journal |last=Malamud |first=S. M. |title=Some complements to the Jensen and Chebyshev inequalities and a problem of W. Walter |journal=Proceedings of the American Mathematical Society |date=15 February 2001 |volume=129 |issue=9 |pages=2671–2678 |doi=10.1090/S0002-9939-01-05849-X |mr=1838791 |url=http://www.ams.org/journals/proc/2001-129-09/S0002-9939-01-05849-X/ |accessdate=7 October 2012 |issn=0002-9939}}</ref>\n\n===Other inequalities===\n\nThere are also a number of other inequalities associated with Chebyshev:\n\n*[[Chebyshev's sum inequality]]\n*[[Chebyshev–Markov–Stieltjes inequalities]]\n\n==Haldane's transformation==\nOne use of Chebyshev's inequality in applications is to create confidence intervals for variates with an unknown distribution. [[J. B. S. Haldane|Haldane]] noted,<ref name=Haldane1952>{{cite journal | last1 = Haldane | first1 = J. B.|authorlink=J. B. S. Haldane | year = 1952 | title = Simple tests for bimodality and bitangentiality | url = | journal = [[Annals of Eugenics]] | volume = 16 | issue = 4| pages = 359–364 | doi = 10.1111/j.1469-1809.1951.tb02488.x }}</ref> using an equation derived by [[Maurice Kendall|Kendall]],<ref name=Kendall1943>Kendall M. G. (1943) The Advanced Theory of Statistics, 1. London</ref> that if a variate (''x'') has a zero mean, unit variance and both finite [[skewness]] (''γ'') and [[kurtosis]] (''κ'') then the variate can be converted to a normally distributed [[standard score]] (''z''):\n\n: <math> z = x - \\frac{\\gamma}{6} (x^2 - 1) + \\frac{ x }{ 72 } [ 2 \\gamma^2 (4 x^2 - 7) - 3 \\kappa (x^2 - 3) ] + \\cdots </math>\n\nThis transformation may be useful as an alternative to Chebyshev's inequality or as an adjunct to it for deriving confidence intervals for variates with unknown distributions.\n\nWhile this transformation may be useful for moderately skewed and/or kurtotic distributions, it performs poorly when the distribution is markedly skewed and/or kurtotic.\n\n==Notes==\n\nThe [[Environmental Protection Agency]] has suggested best practices for the use of Chebyshev's inequality for estimating confidence intervals.<ref>{{cite report\n | title      = Calculating Upper Confidence Limits for Exposure Point Concentrations at hazardous Waste Sites\n | publisher  = Office of Emergency and Remedial Response of the U.S. Environmental Protection Agency\n |date=December 2002\n | url        = http://nepis.epa.gov/Exe/ZyNET.exe/P100CYCE.TXT?ZyActionD=ZyDocument&Client=EPA&Index=2000+Thru+2005&Docs=&Query=&Time=&EndTime=&SearchMethod=1&TocRestrict=n&Toc=&TocEntry=&QField=&QFieldYear=&QFieldMonth=&QFieldDay=&IntQFieldOp=0&ExtQFieldOp=0&XmlQuery=&File=D%3A%5Czyfiles%5CIndex%20Data%5C00thru05%5CTxt%5C00000029%5CP100CYCE.txt&User=ANONYMOUS&Password=anonymous&SortMethod=h%7C-&MaximumDocuments=1&FuzzyDegree=0&ImageQuality=r75g8/r75g8/x150y150g16/i425&Display=p%7Cf&DefSeekPage=x&SearchBack=ZyActionL&Back=ZyActionS&BackDesc=Results%20page&MaximumPages=1&ZyEntry=1&SeekPage=x&ZyPURL#\n | accessdate = 5 August 2016}}</ref> This caution appears to be justified as its use in this context may be seriously misleading.<ref>{{cite web\n  |title = Statistical Tests: The Chebyshev UCL Proposal\n  |website = Quantitative Decisions\n  |date = 25 March 2001\n  |accessdate = 26 November 2015\n  |url = http://www.quantdec.com/envstats/notes/class_12/ucl.htm\n}}</ref>\n\n==See also==\n*[[Multidimensional Chebyshev's inequality]]\n*[[Concentration inequality]] – a summary of tail-bounds on random variables.\n*[[Cornish–Fisher expansion]]\n*[[Eaton's inequality]]\n*[[Kolmogorov's inequality]]\n*[[Law of large numbers/Proof|Proof of the weak law of large numbers]] using Chebyshev's inequality\n*[[Le Cam's theorem]]\n*[[Paley–Zygmund inequality]]\n*[[Vysochanskiï–Petunin inequality]] — a stronger result applicable to [[unimodal probability distributions]]\n\n==References==\n{{reflist|30em}}\n\n==Further reading==\n* A. Papoulis (1991), ''Probability, Random Variables, and Stochastic Processes'', 3rd ed. McGraw–Hill. {{isbn|0-07-100870-5}}. pp.&nbsp;113–114.\n* [[Geoffrey Grimmett|G. Grimmett]] and D. Stirzaker (2001), ''Probability and Random Processes'', 3rd ed. Oxford. {{isbn|0-19-857222-0}}. Section 7.3.\n\n==External links==\n{{commons category}}\n* {{springer|title=Chebyshev inequality in probability theory|id=p/c021890}}\n* [http://mws.cs.ru.nl/mwiki/random_2.html#T7 Formal proof] in the [[Mizar system]].\n\n[[Category:Articles containing proofs]]\n[[Category:Probabilistic inequalities]]\n[[Category:Statistical inequalities]]"
    },
    {
      "title": "Cheeger bound",
      "url": "https://en.wikipedia.org/wiki/Cheeger_bound",
      "text": "In [[mathematics]], the '''Cheeger bound''' is a bound of the second largest eigenvalue of the [[Stochastic matrix|transition matrix]] of a finite-state, discrete-time, reversible stationary [[Markov chain]]. It can be seen as a special case of [[Expander graphs#Cheeger inequalities|Cheeger inequalities]] in [[expander graph]]s.\n\nLet <math>X</math> be a finite set and let <math>K(x,y)</math> be the transition probability for a reversible Markov chain on <math>X</math>. Assume this chain has [[stationary distribution]] <math>\\pi</math>.\n\nDefine\n\n:<math>Q(x,y) = \\pi(x) K(x,y) </math>\n\nand for <math>A,B \\subset X </math> define\n\n: <math>Q(A \\times B) = \\sum_{x \\in A, y \\in B} Q(x,y). </math>\n\nDefine the constant <math>\\Phi</math> as\n\n: <math> \\Phi = \\min_{S \\subset X, \\pi(S) \\leq \\frac{1}{2}} \\frac{Q (S \\times S^c)}{\\pi(S)}. </math>\n\nThe operator <math>K,</math> acting on the [[space of functions]] from <math>|X|</math> to <math>|X|</math>, defined by\n\n: <math> (K \\phi)(x) = \\sum_y K(x,y) \\phi(y) </math>\n\nhas [[eigenvalue]]s <math> \\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_n </math>.  It is known that  <math>\\lambda_1 = 1</math>.   The Cheeger bound is a bound on the second largest eigenvalue <math>\\lambda_2</math>.\n\n''' Theorem (Cheeger bound):'''\n\n:<math> 1 - 2 \\Phi \\leq \\lambda_2 \\leq 1 - \\frac{\\Phi^2}{2}. </math>\n\n== See also ==\n* [[Stochastic matrix]]\n* [[Cheeger constant]]\n\n== References ==\n* J. Cheeger, ''A lower bound for the smallest eigenvalue of the Laplacian,'' Problems in Analysis, Papers dedicated to Salomon Bochner, 1969, Princeton University Press, Princeton, 195-199.\n* P. Diaconis, D. Stroock, ''Geometric bounds for eigenvalues of Markov chains,'' Annals of  Applied Probability, vol. 1, 36-61, 1991, containing the version of the bound presented here.\n\n[[Category:Probabilistic inequalities]]\n[[Category:Stochastic processes]]\n[[Category:Statistical inequalities]]\n\n\n{{statistics-stub}}"
    },
    {
      "title": "Chernoff bound",
      "url": "https://en.wikipedia.org/wiki/Chernoff_bound",
      "text": "{{Short description|exponentially decreasing bounds on tail distributions of sums of independent random variables}}\n\nIn [[probability theory]], the '''Chernoff bound''', named after [[Herman Chernoff]] but due to Herman Rubin,<ref>{{cite book | url=http://www.crcpress.com/product/isbn/9781482204964 | title=Past, Present, and Future of Statistics | chapter=A career in statistics | page=35 | publisher=CRC Press | last1=Chernoff | first1=Herman | editor-first1=Xihong | editor-last1=Lin | editor-first2=Christian | editor-last2=Genest | editor-first3=David L. | editor-last3=Banks | editor-first4=Geert | editor-last4=Molenberghs | editor-first5=David W. | editor-last5=Scott | editor-first6=Jane-Ling | editor-last6=Wang  | editor6-link = Jane-Ling Wang| year=2014 | isbn=9781482204964 | chapterurl=http://nisla05.niss.org/copss/past-present-future-copss.pdf}}</ref> gives exponentially decreasing bounds on [[Cumulative distribution function#Complementary_cumulative_distribution_function_.28tail_distribution.29|tail distributions]] of sums of independent random variables. It is a sharper bound than the known first- or second-moment-based tail bounds such as [[Markov's inequality]] or [[Chebyshev's inequality]], which only yield power-law bounds on tail decay. However, the Chernoff bound requires that the variates be independent – a condition that neither Markov's inequality nor Chebyshev's inequality require, although Chebyshev's inequality does require the variates to be pairwise independent.\n\nIt is related to the (historically prior) [[Bernstein inequalities (probability theory)|Bernstein inequalities]] and to [[Hoeffding's inequality]].\n\n== The generic bound ==\nThe generic Chernoff bound for a random variable {{mvar|X}} is attained by applying [[Markov's inequality]] to {{mvar|e<sup>tX</sup>}}.<ref>This method was first applied by [[Sergei Bernstein]] to prove the related [[Bernstein inequalities (probability theory)|Bernstein inequalities]].</ref> For every <math>t>0</math>:\n\n:<math>\\Pr(X \\geq a) = \\Pr(e^{t\\cdot X} \\geq e^{t\\cdot a})  \\leq \\frac{\\mathrm{E}\\left [e^{t\\cdot X}\\right]}{e^{t\\cdot a}}.</math>\n\nWhen {{mvar|X}} is the sum of {{mvar|n}} random variables {{math|''X''<sub>1</sub>, ..., ''X<sub>n</sub>''}}, we get for any ''t'' > 0,\n\n: <math>\\Pr(X \\geq a)  \\leq e^{-ta}\\mathrm{E} \\left [\\prod_i e^{t\\cdot X_i} \\right].</math>\n\nIn particular, optimizing over ''t'' and using the assumption that {{mvar|X<sub>i</sub>}} are independent, we obtain,\n\n{{NumBlk|:| <math> \\Pr(X \\geq a)  \\leq \\min_{t>0} e^{-ta} \\prod_i \\mathrm{E} \\left [e^{tX_i} \\right ].</math>|{{EquationRef|1}}}}\n\nSimilarly,\n\n: <math>\\Pr (X \\leq a) = \\Pr\\left (e^{-tX} \\ge e^{-ta}\\right) </math>\n\nand so,\n\n: <math> \\Pr (X \\leq a) \\leq \\min_{t>0} e^{ta} \\prod_i \\mathrm{E} \\left[e^{-t X_i} \\right ]</math>\n\nSpecific Chernoff bounds are attained by calculating <math>\\mathrm{E} \\left[e^{-t\\cdot X_i} \\right ]</math> for specific instances of the basic variables <math>X_i</math>.\n\n==Example==\nLet {{math|''X''<sub>1</sub>, ..., ''X<sub>n</sub>''}} be independent [[Bernoulli random variable]]s, whose sum is {{math|''X''}}, each having probability ''p'' > 1/2 of being equal to 1. For a Bernoulli variable:\n\n:<math>\\mathrm{E} \\left[e^{t\\cdot X_i} \\right] = 1 + p (e^t -1) \\leq e^{p (e^t - 1)}</math>\n\nSo:\n\n:<math>\\mathrm{E} \\left[e^{t\\cdot X} \\right] \\leq e^{n\\cdot p (e^t - 1)}</math>\n\nFor any <math>\\delta>0</math>, taking <math>t = \\ln(1+\\delta)>0</math> and <math>a=(1+\\delta)np</math> gives:\n\n:<math>\\mathrm{E} \\left[e^{t\\cdot X} \\right] \\leq e^{\\delta n p}</math> and <math>e^{-t a} = \\frac{1}{(1+\\delta)^{(1+\\delta)np}}</math>\n\nand the generic Chernoff bound gives:<ref name=\"MitzenmacherUpfal\"/>{{rp|64}}\n\n:<math>\\Pr[X \\geq (1+\\delta)np] \\leq \\frac{e^{\\delta n p}}{(1+\\delta)^{(1+\\delta)np}} = \\left[\\frac{e^{\\delta}}{(1+\\delta)^{1+\\delta}}\\right]^{np} </math>\n\nThe probability of simultaneous occurrence of more than ''n''/2 of the events {{math|{''X<sub>k</sub>'' {{=}} 1} }} has an exact value:\n\n:<math>\\Pr[X > {n \\over 2}] = \\sum_{i = \\lfloor \\tfrac{n}{2} \\rfloor + 1}^n \\binom{n}{i}p^i (1 - p)^{n - i} .</math>\n\nA lower bound on this probability can be calculated based on Chernoff's inequality:\n\n:<math>\\Pr[X > {n \\over 2}] \\ge 1 - e^{-\\frac{1}{2p}n \\left(p - \\frac{1}{2} \\right)^2} .</math>\n\nIndeed, noticing that {{math|''μ'' {{=}} ''np''}}, we get by the multiplicative form of Chernoff bound (see below or Corollary 13.3 in Sinclair's class notes),<ref>{{Cite web|url = http://www.cs.berkeley.edu/~sinclair/cs271/n13.pdf|title = Class notes for the course \"Randomness and Computation\"|date = Fall 2011|accessdate=30 October 2014|website = |publisher = |last =Sinclair |first =Alistair }}</ref>\n\n:<math>\\begin{align}\n\\Pr\\left (X \\le\\left\\lfloor \\tfrac{n}{2}\\right\\rfloor \\right ) &=\\Pr\\left (X\\le\\left(1-\\left(1-\\tfrac{1}{2p}\\right)\\right)\\mu\\right ) \\\\\n&\\leq e^{-\\frac{\\mu}{2}\\left(1-\\frac{1}{2p}\\right)^2} \\\\\n&=e^{-\\frac{n}{2p}\\left(p-\\frac{1}{2}\\right)^2}\n\\end{align}</math>\n\nThis result admits various generalizations as outlined below. One can encounter many flavors of Chernoff bounds: the original ''additive form'' (which gives a bound on the [[Approximation error|absolute error]]) or the more practical ''multiplicative form'' (which bounds the [[Approximation error|error relative]] to the mean).\n\n== Additive form (absolute error) ==\nThe following Theorem is due to [[Wassily Hoeffding]]<ref>{{cite journal\n |last1=Hoeffding |first1=W.\n |year=1963\n |title=Probability Inequalities for Sums of Bounded Random Variables\n |journal=[[Journal of the American Statistical Association]]\n |volume=58 |issue=301 |pages=13–30\n |doi=10.2307/2282952\n |jstor=2282952\n}}</ref> and hence is called the Chernoff–Hoeffding theorem.\n\n:'''Chernoff–Hoeffding Theorem.''' Suppose {{math|''X''<sub>1</sub>, ..., ''X<sub>n</sub>''}} are [[i.i.d.]] random variables, taking values in {{math|{0, 1}.}} Let {{math|''p'' {{=}} E[''X<sub>i</sub>'']}} and {{math|''ε'' > 0}}. Then\n::<math>\\begin{align}\n\\Pr \\left (\\frac{1}{n} \\sum X_i \\geq p + \\varepsilon \\right ) \\leq \\left (\\left (\\frac{p}{p + \\varepsilon}\\right )^{p+\\varepsilon} {\\left (\\frac{1 - p}{1-p- \\varepsilon}\\right )}^{1 - p- \\varepsilon}\\right )^n &= e^{-D(p+\\varepsilon\\|p) n} \\\\\n\\Pr \\left (\\frac{1}{n} \\sum X_i \\leq p - \\varepsilon \\right ) \\leq \\left (\\left (\\frac{p}{p - \\varepsilon}\\right )^{p-\\varepsilon} {\\left (\\frac{1 - p}{1-p+ \\varepsilon}\\right )}^{1 - p+ \\varepsilon}\\right )^n &= e^{-D(p-\\varepsilon\\|p) n}\n\\end{align}</math>\n:where\n::<math> D(x\\|y) = x \\ln \\frac{x}{y} + (1-x) \\ln \\left (\\frac{1-x}{1-y} \\right )</math>\n:is the [[Kullback–Leibler divergence]] between [[Bernoulli distribution|Bernoulli distributed]] random variables with parameters ''x'' and ''y'' respectively. If {{math|''p'' ≥ {{sfrac|1|2}},}} then\n::<math> \\Pr\\left ( \\sum X_i>np+x \\right ) \\leq \\exp \\left (-\\frac{x^2}{2np(1-p)} \\right ).</math>\n\nA simpler bound follows by relaxing the theorem using {{math|''D''(''p'' + ''ε'' {{!!}} ''p'') ≥ 2''ε''<sup>2</sup>}}, which follows from the [[Convex function|convexity]] of {{math|''D''(''p'' + ''ε'' {{!!}} ''p'')}} and the fact that\n\n:<math>\\frac{d^2}{d\\varepsilon^2} D(p+\\varepsilon\\|p) = \\frac{1}{(p+\\varepsilon)(1-p-\\varepsilon) }\\geq 4=\\frac{d^2}{d\\varepsilon^2}(2\\varepsilon^2).</math>\n\nThis result is a special case of [[Hoeffding's inequality]]. Sometimes, the bounds\n\n:<math>\n\\begin{align}\nD( (1+x) p \\| p) \\geq \\tfrac{1}{4} x^2 p, & \\qquad -\\tfrac{1}{2} \\leq x \\leq \\tfrac{1}{2},\\\\\nD(x \\| y) \\geq \\tfrac{3(x-y)^2}{2(2y+x)}, \\\\\nD(x \\| y) \\geq \\tfrac{(x-y)^2}{2y}, & \\qquad x \\leq y,\\\\\nD(x \\| y) \\geq \\tfrac{(x-y)^2}{2x}, & \\qquad x \\geq y\\\\\n\\end{align}\n</math>\n\nwhich are stronger for {{math|''p'' < {{sfrac|1|8}},}} are also used.\n\n==Multiplicative form (relative error)==\n:'''Multiplicative Chernoff Bound.''' Suppose {{math|''X''<sub>1</sub>, ..., ''X<sub>n</sub>''}} are [[Statistical independence|independent]] random variables taking values in {{math|{0, 1}.}} Let {{mvar|X}} denote their sum and let {{math|''μ'' {{=}} E[''X'']}} denote the sum's expected value. Then for any {{math|''δ'' > 0}},\n::<math>\\Pr ( X > (1+\\delta)\\mu) < \\left(\\frac{e^\\delta}{(1+\\delta)^{(1+\\delta)}}\\right)^\\mu.</math>\nA similar proof strategy can be used to show that\n\n:<math>\\Pr(X < (1-\\delta)\\mu) < \\left(\\frac{e^{-\\delta}}{(1-\\delta)^{(1-\\delta)}}\\right)^\\mu.</math>\n\nThe above formula is often unwieldy in practice,<ref name=\"MitzenmacherUpfal\">{{cite book | url=https://books.google.com/books?id=0bAYl6d7hvkC | title=Probability and Computing: Randomized Algorithms and Probabilistic Analysis | publisher=Cambridge University Press |author1=Mitzenmacher, Michael  |author2=Upfal, Eli | year=2005 | isbn=978-0-521-83540-4}}</ref> so the following looser but more convenient bounds are often used:\n\n:<math>\\Pr( X \\le (1-\\delta)\\mu) \\le e^{-\\frac{\\delta^2\\mu}{2}}, \\qquad 0 \\le \\delta \\le 1.</math>\n:<math>\\Pr( X \\ge (1+\\delta)\\mu)\\le e^{-\\frac{\\delta^2\\mu}{2+\\delta}}, \\qquad 0 \\le \\delta,</math>\n\nwhich follow from the inequality <math>\\frac{2\\delta}{2+\\delta} \\le \\log(1+\\delta)</math> from [[List_of_logarithmic_identities#Inequalities|the list of logarithmic inequalities]].\nOr looser still:\n\n:<math>\\Pr( X \\ge (1+\\delta)\\mu) \\le e^{-\\frac{\\delta^2\\mu}{3}}, \\qquad 0 \\le \\delta \\le 1,</math>\n:<math>\\Pr( X \\ge (1+\\delta)\\mu) \\le e^{-\\frac{\\delta\\mu}{3}}, \\qquad 1 \\le \\delta,</math>\n\n==Applications==\nChernoff bounds have very useful applications in [[set balancing]] and [[Packet (information technology)|packet]] [[routing]] in [[sparse graph|sparse]] networks.\n\nThe set balancing problem arises while designing statistical experiments. Typically while designing a statistical experiment, given the features of each participant in the experiment, we need to know how to divide the participants into 2 disjoint groups such that each feature is roughly as balanced as possible between the two groups. Refer to this [https://books.google.com/books?id=0bAYl6d7hvkC&printsec=frontcover&source=gbs_summary_r&cad=0#PPA71,M1 book section] for more info on the problem.\n\nChernoff bounds are also used to obtain tight bounds for permutation routing problems which reduce [[network congestion]] while routing packets in sparse networks. Refer to this [https://books.google.com/books?id=0bAYl6d7hvkC&printsec=frontcover&source=gbs_summary_r&cad=0#PPA72,M1 book section]  for a thorough treatment of the problem.\n\nChernoff bounds are used in [[computational learning theory]] to prove that a learning algorithm is [[Probably approximately correct learning|probably approximately correct]], i.e. with high probability the algorithm has small error on a sufficiently large training data set.<ref>M. Kearns, U. Vazirani. ''An Introduction to Computational Learning Theory.'' Chapter 9 (Appendix), pages 190-192. MIT Press, 1994.</ref>\n\nChernoff bounds can be effectively used to evaluate the \"robustness level\" of an application/algorithm by exploring its perturbation space with randomization.<ref name=\"Alippi2014\">C.Alippi: \"Randomized Algorithms\" chapter in ''Intelligence for Embedded Systems.'' Springer, 2014, 283pp, {{isbn|978-3-319-05278-6}}.</ref>\nThe use of the Chernoff bound permits to abandon the strong -and mostly unrealistic- small perturbation hypothesis (the perturbation magnitude is small). The robustness level can be, in turn, used either to validate or reject a specific algorithmic choice, a hardware implementation or the appropriateness of a solution whose structural parameters are affected by uncertainties.\n\n== Matrix bound ==\n{{main|Matrix Chernoff bound}}\n\n[[Rudolf Ahlswede]] and [[Andreas Winter]] introduced a Chernoff bound for matrix-valued random variables.<ref>{{cite journal\n |last1=Ahlswede |first1=R.\n |last2=Winter |first2=A.\n |year=2003\n |title=Strong Converse for Identification via Quantum Channels\n |volume=48 |issue=3 |pages=569–579\n |journal=[[IEEE Transactions on Information Theory]]\n |arxiv=quant-ph/0012127\n |doi=10.1109/18.985947\n |ref=harv\n}}</ref> The following version of the inequality can be found in the work of Tropp.<ref>{{cite journal\n |last1=Tropp |first1=J.\n |year=2010\n |title=User-friendly tail bounds for sums of random matrices\n |arxiv=1004.4389\n |ref=harv\n |doi=10.1007/s10208-011-9099-z\n |volume=12\n |issue=4\n |journal=Foundations of Computational Mathematics\n |pages=389–434\n}}</ref>\n\nLet {{math|''M''<sub>1</sub>, ..., ''M<sub>t</sub>''}} be independent matrix valued random variables such that <math> M_i\\in \\mathbb{C}^{d_1 \\times d_2} </math> and <math> \\mathbb{E}[M_i]=0</math>.\nLet us denote by <math> \\lVert M \\rVert </math> the operator norm of the matrix <math> M </math>. If <math> \\lVert M_i \\rVert \\leq \\gamma </math> holds almost surely for all <math> i\\in\\{1,\\ldots, t\\} </math>, then for every {{math|''ε'' > 0}}\n\n:<math>\\Pr\\left( \\left\\| \\frac{1}{t} \\sum_{i=1}^t M_i \\right\\| > \\varepsilon \\right) \\leq (d_1+d_2) \\exp \\left( -\\frac{3\\varepsilon^2 t}{8\\gamma^2} \\right).</math>\n\nNotice that in order to conclude that the deviation from 0 is bounded by {{math|''ε''}} with high probability, we need to choose a number of samples <math>t </math> proportional to the logarithm of <math> d_1+d_2 </math>. In general, unfortunately, a dependence on  <math> \\log(\\min(d_1,d_2)) </math> is inevitable: take for example a diagonal random sign matrix of dimension <math>d\\times d </math>. The operator norm of the sum of ''t'' independent samples is precisely the maximum deviation among ''d'' independent random walks of length ''t''. In order to achieve a fixed bound on the maximum deviation with constant probability, it is easy to see that ''t'' should grow logarithmically with ''d'' in this scenario.<ref>{{cite arXiv |last1=Magen |first1=A.|author1-link=Avner Magen |last2=Zouzias |first2=A. |year=2011 |title=Low Rank Matrix-Valued Chernoff Bounds and Approximate Matrix Multiplication |class=cs.DM |eprint=1005.2724 }}</ref>\n\nThe following theorem can be obtained by assuming ''M'' has low rank, in order to avoid the dependency on the dimensions.\n\n===Theorem without the dependency on the dimensions===\nLet {{math|0 < ''ε'' < 1}} and ''M'' be a random symmetric real matrix with <math>\\| \\mathrm{E}[M] \\| \\leq 1 </math> and <math>\\| M\\| \\leq \\gamma </math> almost surely. Assume that each element on the support of ''M'' has at most rank ''r''. Set \n:<math> t = \\Omega \\left( \\frac{\\gamma\\log (\\gamma/\\varepsilon^2)}{\\varepsilon^2} \\right).</math>\nIf <math> r \\leq t </math> holds almost surely, then\n\n:<math>\\Pr\\left(\\left\\| \\frac{1}{t} \\sum_{i=1}^t M_i - \\mathrm{E}[M] \\right\\| > \\varepsilon \\right) \\leq \\frac{1}{\\mathbf{poly}(t)}</math>\n\nwhere {{math|''M''<sub>1</sub>, ..., ''M<sub>t</sub>''}} are i.i.d. copies of ''M''.\n\n===Theorem with matrices that are not completely random===\nGarg, Lee, Song and Srivastava <ref>{{cite conference|title=A Matrix Expander Chernoff Bound| conference = STOC '18 Proceedings of the fifty annual ACM symposium on Theory of Computing|last1=Garg|first1=Ankit|last2=Lee|first2=Yin Tat|last3=Song|first3=Zhao|last4=Srivastava|first4=Nikhil|year=2018| arxiv = 1704.03864}}</ref> proved a Chernoff-type bound for sums of matrix-valued random variables sampled via a random walk on an expander, confirming a conjecture due to Wigderson and Xiao. \n\nKyng and Song <ref>{{cite conference|title=A Matrix Chernoff Bound for Strongly Rayleigh Distributions and Spectral Sparsifiers from a few Random Spanning Trees| conference = FOCS '18 IEEE Symposium on Foundations of Computer Science|last1=Kyng|first1=Rasmus|last2=Song|first2=Zhao|year=2018| arxiv = 1810.08345}}</ref> proved a Chernoff-type bound for sums of Laplacian matrix of random spanning trees.\n\n==Sampling variant==\n\nThe following variant of Chernoff's bound can be used to bound the probability that a majority in a population will become a minority in a sample, or vice versa.<ref>{{Cite book | doi = 10.1007/3-540-44676-1_35| chapter = Competitive Auctions for Multiple Digital Goods| title = Algorithms — ESA 2001| volume = 2161| pages = 416| series = Lecture Notes in Computer Science| year = 2001| last1 = Goldberg | first1 = A. V. | last2 = Hartline | first2 = J. D. | isbn = 978-3-540-42493-2| citeseerx = 10.1.1.8.5115}}; lemma 6.1</ref>\n\nSuppose there is a general population ''A'' and a sub-population ''B''⊆''A''. Mark the relative size of the sub-population (|''B''|/|''A''|) by ''r''.\n\nSuppose we pick an integer ''k'' and a random sample ''S''⊂''A'' of size ''k''. Mark the relative size of the sub-population in the sample (|''B''∩''S''|/|''S''|) by ''r<sub>S</sub>''.\n\nThen, for every fraction ''d''∈[0,1]:\n\n:<math>\\mathrm{Pr}\\left(r_S < (1-d)\\cdot r\\right) < \\exp\\left(-r\\cdot d^2 \\cdot k/2\\right)</math>\n\nIn particular, if ''B'' is a majority in ''A'' (i.e. ''r'' > 0.5) we can bound the probability that ''B'' will remain majority in ''S'' (''r<sub>S</sub>''>0.5) by taking: ''d'' = 1 - 1 / (2 ''r''):<ref>See graphs of: [https://www.desmos.com/calculator/eqvyjug0re the bound as a function of ''r'' when ''k'' changes] and [https://www.desmos.com/calculator/nxurzg7bqj the bound as a function of ''k'' when ''r'' changes].</ref>\n\n:<math>\\mathrm{Pr}\\left(r_S > 0.5\\right) > 1 - \\exp\\left(-r\\cdot \\left(1 - \\frac{1}{2 r}\\right)^2 \\cdot k/2\\right)</math>\n\nThis bound is of course not tight at all. For example, when ''r''=0.5 we get a trivial bound ''Prob'' > 0.\n\n==Proofs==\n\n===Chernoff–Hoeffding theorem (additive form)===\nLet {{math|''q'' {{=}} ''p'' + ''ε''}}. Taking {{math|''a'' {{=}} ''nq''}} in ({{EquationNote|1}}), we obtain:\n\n:<math>\\Pr\\left ( \\frac{1}{n} \\sum X_i \\ge q\\right )\\le \\inf_{t>0} \\frac{E \\left[\\prod e^{t X_i}\\right]}{e^{tnq}} = \\inf_{t>0} \\left ( \\frac{ E\\left[e^{tX_i} \\right] }{e^{tq}}\\right )^n.</math>\n\nNow, knowing that {{math|Pr(''X<sub>i</sub>'' {{=}} 1) {{=}} ''p'', Pr(''X<sub>i</sub>'' {{=}} 0) {{=}} 1 − ''p''}}, we have\n\n:<math>\\left (\\frac{\\mathrm{E}\\left[e^{tX_i} \\right] }{e^{tq}}\\right )^n = \\left (\\frac{p e^t + (1-p)}{e^{tq} }\\right )^n = \\left ( pe^{(1-q)t} + (1-p)e^{-qt} \\right )^n.</math>\n\nTherefore, we can easily compute the infimum, using calculus:\n\n:<math>\\frac{d}{dt} \\left (pe^{(1-q)t} + (1-p)e^{-qt} \\right) = (1-q)pe^{(1-q)t}-q(1-p)e^{-qt}</math>\n\nSetting the equation to zero and solving, we have\n\n:<math>\\begin{align}\n(1-q)pe^{(1-q)t} &= q(1-p)e^{-qt} \\\\\n(1-q)pe^{t} &= q(1-p)\n\\end{align}</math>\n\nso that\n\n:<math>e^t = \\frac{(1-p)q}{(1-q)p}.</math>\n\nThus,\n\n:<math>t = \\log\\left(\\frac{(1-p)q}{(1-q)p}\\right).</math>\n\nAs {{math|''q'' {{=}} ''p'' + ''ε'' > ''p''}}, we see that {{math|''t'' > 0}}, so our bound is satisfied on {{mvar|t}}. Having solved for {{mvar|t}}, we can plug back into the equations above to find that\n\n:<math>\\begin{align}\n\\log \\left (pe^{(1-q)t} + (1-p)e^{-qt} \\right ) &= \\log \\left ( e^{-qt}(1-p+pe^t) \\right ) \\\\\n&= \\log\\left (e^{-q \\log\\left(\\frac{(1-p)q}{(1-q)p}\\right)}\\right) + \\log\\left(1-p+pe^{\\log\\left(\\frac{1-p}{1-q}\\right)}e^{\\log\\frac{q}{p}}\\right ) \\\\\n&= -q\\log\\frac{1-p}{1-q} -q \\log\\frac{q}{p} + \\log\\left(1-p+ p\\left(\\frac{1-p}{1-q}\\right)\\frac{q}{p}\\right) \\\\\n&= -q\\log\\frac{1-p}{1-q} -q \\log\\frac{q}{p} + \\log\\left(\\frac{(1-p)(1-q)}{1-q}+\\frac{(1-p)q}{1-q}\\right) \\\\\n&= -q \\log\\frac{q}{p} + \\left ( -q\\log\\frac{1-p}{1-q} + \\log\\frac{1-p}{1-q} \\right ) \\\\\n&= -q\\log\\frac{q}{p} + (1-q)\\log\\frac{1-p}{1-q} \\\\\n&= -D(q \\| p).\n\\end{align}</math>\n\nWe now have our desired result, that\n\n:<math>\\Pr \\left (\\tfrac{1}{n}\\sum X_i \\ge p + \\varepsilon\\right ) \\le e^{-D(p+\\varepsilon\\|p) n}.</math>\n\nTo complete the proof for the symmetric case, we simply define the random variable {{math|''Y<sub>i</sub>'' {{=}} 1 − ''X<sub>i</sub>''}}, apply the same proof, and plug it into our bound.\n\n===Multiplicative form===\nSet {{math|Pr(''X<sub>i</sub>'' {{=}} 1) {{=}} ''p<sub>i</sub>''}}.\nAccording to ({{EquationNote|1}}),\n\n:<math>\\begin{align}\n\\Pr (X > (1 + \\delta)\\mu) &\\le \\inf_{t > 0} \\frac{\\mathrm{E}\\left[\\prod_{i=1}^n\\exp(tX_i)\\right]}{\\exp(t(1+\\delta)\\mu)}\\\\\n& = \\inf_{t > 0} \\frac{\\prod_{i=1}^n\\mathrm{E}\\left [e^{tX_i} \\right]}{\\exp(t(1+\\delta)\\mu)} \\\\\n& = \\inf_{t > 0} \\frac{\\prod_{i=1}^n\\left[p_ie^t + (1-p_i)\\right]}{\\exp(t(1+\\delta)\\mu)}\n\\end{align}</math>\n\nThe third line above follows because <math>e^{tX_i}</math> takes the value {{mvar|e<sup>t</sup>}} with probability {{mvar|p<sub>i</sub>}} and the value 1 with probability {{math|1 − ''p<sub>i</sub>''}}. This is identical to the calculation above in the proof of the [[#Theorem for additive form (absolute error)|Theorem for additive form (absolute error)]].\n\nRewriting <math>p_ie^t + (1-p_i)</math> as <math>p_i(e^t-1) + 1</math> and recalling that <math>1+x \\le e^x</math> (with strict inequality if {{math|''x'' > 0}}), we set <math>x = p_i(e^t-1)</math>. The same result can be obtained by directly replacing {{mvar|a}} in the equation for the Chernoff bound with {{math|(1 + ''δ'')''μ''}}.<ref>Refer to the proof above</ref>\n\nThus,\n\n:<math>\\Pr(X > (1+\\delta)\\mu) < \\frac{\\prod_{i=1}^n\\exp(p_i(e^t-1))}{\\exp(t(1+\\delta)\\mu)} = \\frac{\\exp\\left((e^t-1)\\sum_{i=1}^n p_i\\right)}{\\exp(t(1+\\delta)\\mu)}  = \\frac{\\exp((e^t-1)\\mu)}{\\exp(t(1+\\delta)\\mu)}.</math>\n\nIf we simply set {{math|''t'' {{=}} log(1 + ''δ'')}} so that {{math|''t'' > 0}} for {{math|''δ'' > 0}}, we can substitute and find\n\n:<math>\\frac{\\exp((e^t-1)\\mu)}{\\exp(t(1+\\delta)\\mu)} = \\frac{\\exp((1+\\delta - 1)\\mu)}{(1+\\delta)^{(1+\\delta)\\mu}} = \\left[\\frac{e^\\delta}{(1+\\delta)^{(1+\\delta)}}\\right]^\\mu</math>\n\nThis proves the result desired.\n\n==See also==\n* [[Concentration inequality]] - a summary of tail-bounds on random variables.\n* [[Entropic value at risk]]\n\n== References ==\n{{Reflist}}\n\n==Further reading==\n* {{cite journal\n |last1=Chernoff |first1=H.\n |year=1952\n |title=A Measure of Asymptotic Efficiency for Tests of a Hypothesis Based on the sum of Observations\n |journal=[[Annals of Mathematical Statistics]]\n |volume=23 |issue=4 |pages=493&ndash;507\n |doi=10.1214/aoms/1177729330\n |jstor=2236576\n |mr=57518\n |zbl=0048.11804\n}}\n* {{cite journal\n |last1=Chernoff |first1=H.\n |year=1981\n |title=A Note on an Inequality Involving the Normal Distribution\n |journal=[[Annals of Probability]]\n |volume=9 |issue=3 |pages=533–535\n |doi=10.1214/aop/1176994428\n |jstor=2243541\n |mr=614640\n |zbl=0457.60014\n}}\n* {{cite journal\n |last1=Hagerup |first1=T.\n |last2=Rüb |first2=C.\n |year=1990\n |title=A guided tour of Chernoff bounds\n |journal=[[Information Processing Letters]]\n |volume=33 |issue=6 |pages=305\n |doi=10.1016/0020-0190(90)90214-I\n}}\n* {{cite arXiv\n |last=Nielsen |first=F.\n |year=2011\n |title=Chernoff information of exponential families\n |class=cs.IT\n |eprint=1102.2684\n}}\n\n{{DEFAULTSORT:Chernoff Bound}}\n[[Category:Probabilistic inequalities]]"
    },
    {
      "title": "Chung–Erdős inequality",
      "url": "https://en.wikipedia.org/wiki/Chung%E2%80%93Erd%C5%91s_inequality",
      "text": "In [[probability theory]], the '''Chung–Erdős inequality''' provides a lower bound on the probability that one out of many (possibly dependent) events occurs. The lower bound is expressed in terms of the probabilities for pairs of events.\n\nFormally, let <math>A_1,\\ldots,A_n</math> be events. Assume that <math>\\Pr[A_i]>0</math> for some <math>i</math>. Then\n\n: <math>\\Pr[A_1\\vee\\cdots\\vee A_n]\n\\geq\n\\frac{\n\\left(\\sum_{i=1}^n \\Pr[A_i]\\right)^2\n}{\n\\sum_{i=1}^n\\sum_{j=1}^n \\Pr[A_i\\wedge A_j]\n}.\n</math>\n\nThe inequality was first derived by [[Kai Lai Chung]] and [[Paul Erdős]] (in,<ref>{{Cite journal|last=Chung|first=K. L.|last2=Erdös|first2=P.|date=1952-01-01|title=On the application of the Borel–Cantelli lemma|journal=Transactions of the American Mathematical Society|volume=72|issue=1|pages=179–186|doi=10.1090/S0002-9947-1952-0045327-5|issn=0002-9947}}</ref> equation (4)). It was stated in the form given above by Petrov (in,<ref>{{Cite book|title=Limit theorems of probability theory : sequences of independent random variables|last=Petrov|first=Valentin Vladimirovich|date=1995-01-01|publisher=Clarendon Press|oclc=301554906}}</ref> equation (6.10)).\n\n== References ==\n{{reflist}}\n\n{{DEFAULTSORT:Chung-Erdős inequality}}\n[[Category:Probability theorems]]\n[[Category:Probabilistic inequalities]]\n[[Category:Paul Erdős]]\n\n\n{{probability-stub}}"
    },
    {
      "title": "Concentration inequality",
      "url": "https://en.wikipedia.org/wiki/Concentration_inequality",
      "text": "In [[probability theory]], '''concentration inequalities''' provide bounds on how a [[random variable]] deviates from some value (typically, its [[expected value]]). The [[law of large numbers]] of classical probability theory states that sums of independent random variables are, under very mild conditions, close to their expectation with a large probability. Such sums are the most basic examples of random variables concentrated around their [[mean]]. Recent results show that such behavior is shared by other functions of independent random variables.\n\nConcentration inequalities can be sorted according to how much information about the random variable is needed in order to use them.\n\n==Markov's inequality==\n{{Main|Markov's inequality}}\nLet <math>X</math> be a random variable that is non-negative ([[almost surely]]). Then, for every constant <math>a > 0</math>, \n\n: <math>\\Pr(X \\geq a) \\leq \\frac{\\operatorname{E}(X)}{a}.</math>\n\nNote the following extension to Markov's inequality: if <math>\\Phi</math> is a strictly increasing and non-negative function, then \n\n:<math>\\Pr(X \\geq a) = \\Pr(\\Phi (X) \\geq \\Phi (a)) \\leq \\frac{\\operatorname{E}(\\Phi(X))}{\\Phi (a)}.</math>\n\n==Chebyshev's inequality==\n{{Main|Chebyshev's inequality}}\nChebyshev's inequality requires the following information on a random variable <math>X</math>:\n* The expected value <math>\\operatorname{E}[X]</math> is finite.\n* The [[variance]] <math>\\operatorname{Var}[X] = \\operatorname{E}[(X - \\operatorname{E}[X] )^2]</math> is finite.\n\nThen, for every constant <math>a > 0</math>, \n\n:<math>\\Pr(|X-\\operatorname{E}[X]| \\geq a) \\leq \\frac{\\operatorname{Var}[X]}{a^2},</math>\n\nor equivalently, \n\n:<math>\\Pr(|X-\\operatorname{E}[X]| \\geq a\\cdot \\operatorname{Std}[X]) \\leq \\frac{1}{a^2},</math>\n\nwhere <math>\\operatorname{Std}[X]</math> is the [[standard deviation]] of <math>X</math>.\n\nChebyshev's inequality can be seen as a special case of the generalized Markov's inequality applied to the random variable <math>|X-\\operatorname{E}[X]|</math> with <math>\\Phi(x) = x^2</math>.\n\n==Chernoff bounds==\n{{Main|Chernoff bound}}\nThe generic Chernoff bound<ref name=\"MitzenmacherUpfal\">{{cite book | url=https://books.google.com/books?id=0bAYl6d7hvkC | title=Probability and Computing: Randomized Algorithms and Probabilistic Analysis | publisher=Cambridge University Press |author1=Mitzenmacher, Michael  |author2=Upfal, Eli | year=2005 | isbn=0-521-83540-2}}</ref>{{rp|63–65}} requires only the [[moment generating function]] of <math>X</math>, defined as: <math>M_X(t) := \\operatorname{E}\\!\\left[e^{tX}\\right]</math>, provided it exists. Based on Markov's inequality, for every <math>t>0</math>:\n\n:<math>\\Pr(X \\geq a) \\leq \\frac{\\operatorname{E}[e^{t\\cdot X}]}{e^{t\\cdot a}},</math>\n\nand for every <math>t<0</math>:\n\n:<math>\\Pr(X \\leq a) \\leq \\frac{\\operatorname{E}[e^{t\\cdot X}]}{e^{t\\cdot a}}.</math>\n\nThere are various Chernoff bounds for different distributions and different values of the parameter <math>t</math>. See <ref name=\"OneHundredNPS\">{{cite article | url=http://www.npslagle.info/articles/onehundredprobabilityinequalities.pdf | title=One Hundred Statistics and Probability Inequalities | publisher= |author1=Slagle, N.P.  | year=2012}}</ref>{{rp|5–7}} for a compilation of more concentration inequalities.\n\n== Bounds on sums of independent variables ==\n{{Main|Hoeffding's inequality|Azuma's inequality|McDiarmid's inequality|Bennett's inequality|Bernstein inequalities (probability theory)}}\nLet <math>X_1, X_2,\\dots,X_n</math> be independent random variables such that, for all ''i'':\n:<math>a_i\\leq X_i\\leq b_i</math> [[almost surely]].\n:<math>c_i := b_i-a_i</math>\n:<math>\\forall i: c_i \\leq C</math>\nLet <math>S_n</math> be their sum, <math>E_n</math> its [[expected value]] and <math>V_n</math> its variance:\n:<math>S_n := \\sum_{i=1}^n X_i</math>\n:<math>E_n := \\operatorname{E}[S_n] = \\sum_{i=1}^n \\operatorname{E}[X_i]</math>\n:<math>V_n := \\operatorname{Var}[S_n] = \\sum_{i=1}^n \\operatorname{Var}[X_i]</math>\n\nIt is often interesting to bound the difference between the sum and its expected value. Several inequalities can be used.\n\n1. [[Hoeffding's inequality]] says that:\n::<math>\\Pr\\left[|S_n-E_n|>t\\right] < 2 \\exp \\left(-\\frac{2t^2}{\\sum_{i=1}^n c_i^2} \\right)< 2 \\exp \\left(-\\frac{2t^2}{n C^2} \\right)</math>\n\n2. The random variable <math>S_n-E_n</math> is a special case of a [[martingale (probability theory)|martingale]], and <math>S_0-E_0=0</math>. Hence, [[Azuma's inequality]] can also be used and it yields a similar bound:\n::<math>\\Pr\\left[|S_n-E_n|>t\\right] < 2 \\exp \\left(-\\frac{t^2}{2\\sum_{i=1}^n c_i^2}\\right)< 2 \\exp \\left(-\\frac{t^2}{2 n C^2} \\right) </math>\nThis is a generalization of Hoeffding's since it can handle other types of martingales, as well as [[supermartingale]]s and [[submartingale]]s.\n\n3. The sum function, <math>S_n=f(X_1,\\dots,X_n)</math>, is a special case of a function of ''n'' variables. This function changes in a bounded way: if variable ''i'' is changed, the value of ''f'' changes by at most <math>b_i-a_i<C</math>. Hence, [[McDiarmid's inequality]] can also be used and it yields a similar bound:\n::<math>\\Pr\\left[|S_n-E_n|>t\\right] < 2 \\exp \\left(-\\frac{2t^2}{\\sum_{i=1}^n c_i^2} \\right)< 2 \\exp \\left(-\\frac{2t^2}{n C^2} \\right)</math>\nThis is a different generalization of Hoeffding's since it can handle other functions besides the sum function, as long as they change in a bounded way.\n\n4. [[Bennett's inequality]] offers some improvement over Hoeffding's when the variances of the summands are small compared to their almost-sure bounds ''C''. It says that:\n::<math>\\Pr\\left[|S_n-E_n| > t \\right] \\leq\n2\\exp\\left[ - \\frac{V_n}{C^2} h\\left(\\frac{C t}{V_n} \\right)\\right],</math> where <math>h(u) = (1+u)\\log(1+u)-u</math>\n\n5. The first of [[Bernstein inequalities (probability theory)|Bernstein's inequalities]] says that:\n::<math>\\Pr\\left[|S_n-E_n|>t\\right] < 2 \\exp \\left(-\\frac{t^2/2}{V_n + C\\cdot t/3} \\right)</math>\nThis is a generalization of Hoeffding's since it can handle not only independent variables but also weakly-dependent variables.\n\n6. Chernoff bounds have a particularly simple form in the case of sum of independent variables, since <math>\\operatorname{E}[e^{t\\cdot S_n}] = \\prod_{i=1}^n {\\operatorname{E}[e^{t\\cdot X_i}]}</math>.\n\nFor example,<ref name=ChungChernoff>{{cite web|last1=Chung|first1=Fan| author1-link= Fan Chung | first2= Linyuan  | last2=Lu |title=Old and new concentration inequalities|url=http://www.math.ucsd.edu/~fan/complex/ch2.pdf|work= Complex Graphs and Networks | publisher= [[American Mathematical Society]] | year= 2010 | |accessdate=August 14, 2018}}</ref> suppose the variables <math>X_i</math> satisfy <math>X_i \\geq E(X_i)-a_i-M</math>, for <math>1 \\leq i \\leq n</math>. Then we have lower tail inequality:\n::<math>\\Pr[S_n - E_n < -\\lambda]\\leq \\exp\\left(-\\frac{\\lambda^2}{2(V_n+\\sum_{i=1}^n a_i^2+M\\lambda/3)}\\right)</math>\n\nIf <math>X_i</math> satisfies <math>X_i \\leq E(X_i)+a_i+M</math>, we have upper tail inequality:\n::<math>\\Pr[S_n - E_n > \\lambda]\\leq \\exp\\left(-\\frac{\\lambda^2}{2(V_n + \\sum_{i=1}^n a_i^2+M\\lambda/3)}\\right)</math>\n\nIf <math>X_i</math> are i.i.d., <math>|X_i| \\leq 1</math> and <math>\\sigma^2</math> is the variance of <math>X_i</math>, a typical version of Chernoff inequality is:\n::<math>\\Pr[|S_n| \\geq k\\sigma]\\leq 2e^{-k^2/4n} \\text{ for } 0 \\leq k\\leq 2\\sigma.</math>\n\n7. Similar bounds can be found in: [[Rademacher distribution#Bounds on sums]]\n\n==Efron–Stein inequality==\nThe Efron–Stein inequality (or influence inequality, or MG bound on variance) bounds the variance of a general function.\n\nSuppose that <math>X_1 \\dots X_n</math>, <math>X_1' \\dots X_n'</math> are independent with <math>X_i'</math> and <math>X_i</math> having the same distribution for all <math>i</math>.\n\nLet <math>X = (X_1,\\dots , X_n), X^{(i)} = (X_1, \\dots , X_{i-1}, X_i',X_{i+1}, \\dots , X_n).</math> Then\n\n: <math>\n\\mathrm{Var}(f(X)) \\leq \\frac{1}{2} \\sum_{i=1}^{n} E[(f(X)-f(X^{(i)}))^2].\n</math>\n\n== Dvoretzky–Kiefer–Wolfowitz inequality ==\n{{Main|Dvoretzky–Kiefer–Wolfowitz inequality}}\n\nThe Dvoretzky–Kiefer–Wolfowitz inequality bounds the difference between the real and the empirical [[cumulative distribution function]].\n\nGiven a natural number <math>n</math>, let <math>X_1, X_2,\\dots,X_n</math> be real-valued [[independent and identically distributed]] [[random variable]]s with [[cumulative distribution function]] ''F''(·).  Let <math>F_n</math> denote the associated [[empirical distribution function]] defined by\n:<math>F_n(x) = \\frac1n \\sum_{i=1}^n \\mathbf{1}_{\\{X_i\\leq x\\}},\\qquad x\\in\\mathbb{R}.</math>\nSo <math>F(x)</math> is the probability that a ''single'' random variable <math>X</math> is smaller than <math>x</math>, and <math>F_n(x)</math> is the ''average number'' of random variables that are smaller than <math>x</math>.\n\nThen\n::<math>\\Pr\\left(\\sup_{x\\in\\mathbb R} \\bigl(F_n(x) - F(x)\\bigr) > \\varepsilon \\right) \\le e^{-2n\\varepsilon^2} \\text{ for every } \\varepsilon \\geq \\sqrt{\\tfrac 1 {2n} \\ln2}.</math>\n\n==Anti-concentration inequalities==\n'''Anti-concentration inequalities''', on the other hand, provide an ''upper bound'' on how much a random variable can concentrate around a quantity.\n\nFor example, Rao and Yehudayoff<ref name=\"RaoYehudayoff\">{{cite article | url=https://eccc.weizmann.ac.il/report/2018/194/ | title=Anti-concentration in most directions | publisher=Electronic Colloquium on Computational Complexity |author1=Rao, Anup  |author2=Yehudayoff, Amir | year=2018}}</ref> show that there exists some <math>C > 0</math> such that, for most directions of the hypercube <math>x \\in \\{\\pm 1\\}^n</math>, the following is true:\n::<math>\n\\Pr\\left(\\langle x, Y\\rangle = k\\right) \\le \\frac{C}{\\sqrt{n}},\n</math>\nwhere <math>Y</math> is drawn uniformly from a subset <math>B \\subseteq \\{\\pm 1\\}^n</math> of large enough size.\n\nSuch inequalities are of importance in several fields, including [[communication complexity]] (''e.g.'', in proofs of the [[Gap-Hamming problem|gap&nbsp;Hamming problem]]<ref name=\"Sherstov\">{{cite article | url=https://theoryofcomputing.org/articles/v008a008/ | title=The Communication Complexity of Gap Hamming Distance | publisher=[[Theory of Computing]] |author1=Sherstov, Alexander A. | year=2012}}</ref>) and [[graph theory]].<ref name=\"Kwan\">{{cite article | url=https://londmathsoc.onlinelibrary.wiley.com/doi/abs/10.1112/jlms.12192 | title=Anticoncentration for subgraph statistics | publisher=Journal of the London Mathematical Society |author1=Matthew Kwan | author2=Benny Sudakov | author3= Tuan Tran | year=2018}}</ref>\n\nAn interesting anti-concentration inequality for weighted sums of independent [[Rademacher distribution|Rademacher]] random variables can be obtained using the [[Paley–Zygmund inequality|Paley–Zygmund]] and the [[Khintchine inequality|Khintchine]] inequalities.<ref name=\"Veraar\">{{cite article | url=https://arxiv.org/pdf/0909.2586v1.pdf | title=On Khintchine inequalities with a weight | publisher=[[arXiv]] |author1=Veraar, Mark | year=2009}}</ref>\n\n==References==\n{{reflist}}\n\n==External links==\nKarthik Sridharan, \"[https://www.cs.cornell.edu/~sridharan/concentration.pdf A Gentle Introduction to Concentration Inequalities]\" &nbsp;—[[Cornell University]]\n\n[[Category:Probabilistic inequalities]]"
    },
    {
      "title": "Doob martingale",
      "url": "https://en.wikipedia.org/wiki/Doob_martingale",
      "text": "{{context|date=March 2011}}\nA '''Doob martingale''' (also known as a '''Levy martingale''') is a mathematical construction of a [[stochastic process]] which approximates a given [[random variable]] and has the [[Martingale (probability theory)|martingale property]] with respect to the given [[filtration (probability theory)|filtration]]. It may be thought of as the evolving sequence of best approximations to the random variable based on information accumulated up to a certain time.\n\nWhen analyzing sums, [[random walk]]s, or other additive functions of [[Statistical independence|independent random variables]], one can often apply the [[central limit theorem]], [[law of large numbers]], [[Chernoff's inequality]], [[Chebyshev's inequality]] or similar tools. When analyzing similar objects where the differences are not independent, the main tools are [[Martingale (probability theory)|martingale]]s and [[Azuma's inequality]].{{Clarify|reason=what has this para to do with Doob martingale in particular|date=May 2012}}\n\n==Definition==\nA Doob martingale (named after [[Joseph L. Doob]])<ref>{{cite journal |last=Doob |first=J. L. |year=1940 |title=Regularity properties of certain families of chance variables |journal=Transactions of the American Mathematical Society |volume=47 |issue=3 |pages=455–486 |url=http://www.ams.org/journals/tran/1940-047-03/S0002-9947-1940-0002052-6/S0002-9947-1940-0002052-6.pdf |doi=10.2307/1989964|jstor=1989964 }}\n</ref> \nis a generic construction that is always a martingale. Specifically, consider any set of random variables \n\n:<math>\\vec{X}=X_1, X_2, ..., X_n</math> \n\ntaking values in a set <math>A</math> for which we are interested in the function <math>f:A^n \\to \\mathbb{R}</math> and define:\n\n:<math>B_i=E[f(\\vec{X})|X_{1},X_{2},...X_{i}]</math>\n\nwhere the above expectation is itself a random quantity since the expectation is only taken over \n<math>X_{i+1},X_{i+2},...,X_{n},</math> \nand \n<math>X_{1},X_{2},...X_{i}</math> \nare treated as random variables.  It is possible to show that <math>B_i</math> is always a martingale regardless of the properties of <math>X_i</math>.{{Citation needed|date=May 2012}}  \n\nThe sequence <math>{B_i}</math> is the Doob martingale for ''f''.<ref>Anupam Gupta (2011) http://www.cs.cmu.edu/~avrim/Randalgs11/lectures/lect0321.pdf Lecture notes</ref>\n\n==Application==\nThus if one can bound the differences \n\n:<math>|B_{i+1}-B_i|</math>, \n\none can apply [[Azuma's inequality]] and show that with high probability <math>f(\\vec{X})</math> is concentrated around its expected value \n\n:<math>E[f(\\vec{X})]=B_0.</math>\n\n== McDiarmid's inequality ==\n\nOne common way of bounding the differences and applying [[Azuma's inequality]] to a Doob martingale is called McDiarmid's inequality.<ref name=mcdiarmid>{{cite journal |last=McDiarmid |first=Colin |year=1989 |title=On the Method of Bounded Differences |journal=Surveys in Combinatorics |volume=141 |issue= |pages=148–188 |url=http://www.stats.ox.ac.uk/people/academic_staff/colin_mcdiarmid/?a=4113 }}</ref> \n\nSuppose <math>X_1, X_2, \\dots, X_n</math> are independent and assume that\n<math>f</math> satisfies\n\n: <math>\\sup_{x_1,x_2,\\dots,x_n, \\hat x_i} |f(x_1,x_2,\\dots,x_n) - f(x_1,x_2,\\dots,x_{i-1},\\hat x_i, x_{i+1}, \\dots, x_n)| \n\\le c_i \\qquad \\text{for} \\quad 1 \\le i \\le n \\; .\n</math>\n\n(In other words, replacing the <math>i</math>-th coordinate <math>x_i</math> by some other value changes the value of <math>f</math>\nby at most <math>c_i</math>.) \n\nIt follows that\n\n:<math>|B_{i+1}-B_i| \\le c_i</math>\n\nand therefore [[Azuma's inequality]] yields the following '''McDiarmid inequalities''' for any <math>\\varepsilon > 0</math>:\n\n: <math>\n\\Pr \\left\\{ f(X_1, X_2, \\dots, X_n) - E[f(X_1, X_2, \\dots, X_n)] \\ge \\varepsilon \\right\\} \n\\le \n\\exp \\left( - \\frac{2 \\varepsilon^2}{\\sum_{i=1}^n c_i^2} \\right) \n</math>\n\nand\n\n: <math>\n\\Pr \\left\\{ E[f(X_1, X_2, \\dots, X_n)] - f(X_1, X_2, \\dots, X_n) \\ge \\varepsilon \\right\\} \n\\le \n\\exp \\left( - \\frac{2 \\varepsilon^2}{\\sum_{i=1}^n c_i^2} \\right)\n</math>\n\nand \n\n: <math>\n\\Pr \\left\\{ |E[f(X_1, X_2, \\dots, X_n)] - f(X_1, X_2, \\dots, X_n)| \\ge \\varepsilon \\right\\} \n\\le 2 \\exp \\left( - \\frac{2 \\varepsilon^2}{\\sum_{i=1}^n c_i^2} \\right). \\;\n</math>\n\n==See also==\n* [[Concentration inequality]] - a summary of McDiarmid's and several similar inequalities.\n\n==Notes==\n{{reflist}}\n\n==References==\n*{{cite journal |last=McDiarmid |first=Colin |year=1989 |title=On the Method of Bounded Differences |journal=Surveys in Combinatorics |volume=141 |issue= |pages=148–188 |url=http://www.stats.ox.ac.uk/people/academic_staff/colin_mcdiarmid/?a=4113 }}\n\n[[Category:Probabilistic inequalities]]\n[[Category:Statistical inequalities]]\n[[Category:Martingale theory]]"
    },
    {
      "title": "Doob's martingale inequality",
      "url": "https://en.wikipedia.org/wiki/Doob%27s_martingale_inequality",
      "text": "In [[mathematics]], '''Doob's martingale inequality''' is a result in the study of [[stochastic processes]]. It gives a bound on the probability that a stochastic process exceeds any given value over a given interval of time. As the name suggests, the result is usually given in the case that the process is a non-negative [[Martingale (probability theory)|martingale]], but the result is also valid for non-negative submartingales.\n\nThe inequality is due to the American mathematician [[Joseph L. Doob]].\n\n==Statement of the inequality==\nLet ''X'' be a [[Martingale (probability theory)#Submartingales, supermartingales, and relationship to harmonic functions|submartingale]] taking non-negative real values, either in discrete or continuous time. That is, for all times ''s'' and ''t'' with ''s''&nbsp;<&nbsp;''t'',\n\n:<math> X_{s}\\leq\\mathbf{E} \\left[ X_{t} \\big| \\mathcal{F}_{s} \\right].</math>\n\n(For a continuous-time submartingale, assume further that the process is [[càdlàg]].) Then, for any constant ''C''&nbsp;>&nbsp;0,\n\n:<math>\\mathbf{P} \\left[ \\sup_{0 \\leq t \\leq T} X_{t} \\geq C \\right] \\leq \\frac{\\mathbf{E} \\left[ X_{T} \\right]}{C}.</math>\n\nIn the above, as is conventional, '''P''' denotes the [[probability measure]] on the sample space Ω of the stochastic process\n\n:<math>X : [0, T] \\times \\Omega \\to [0, + \\infty)</math>\n\nand '''E''' denotes the [[expected value]] with respect to the probability measure '''P''', i.e. the integral\n\n:<math>\\mathbf{E}[X_T] = \\int_{\\Omega} X_{T} (\\omega) \\, \\mathrm{d} \\mathbf{P} (\\omega)</math>\n\nin the sense of [[Lebesgue integration]]. <math>\\mathcal{F}_{s}</math> denotes the [[sigma algebra|σ-algebra]] generated by all the [[random variable]]s ''X<sub>i</sub>'' with ''i''&nbsp;&le;&nbsp;''s''; the collection of such σ-algebras forms a [[filtration (abstract algebra)|filtration]] of the probability space.\n\n==Further inequalities==\nThere are further (sub)martingale inequalities also due to Doob. With the same assumptions on ''X'' as above, let\n\n:<math>S_{t} = \\sup_{0 \\leq s \\leq t} X_{s},</math>\n\nand for ''p''&nbsp;&ge;&nbsp;1 let\n\n:<math>\\| X_{t} \\|_{p} = \\| X_{t} \\|_{L^{p} (\\Omega, \\mathcal{F}, \\mathbf{P})} = \\left( \\mathbf{E} \\left[ | X_{t} |^{p} \\right] \\right)^{\\frac{1}{p}}.</math>\n\nIn this notation, Doob's inequality as stated above reads\n\n:<math>\\mathbf{P} \\left[ S_{T} \\geq C \\right] \\leq \\frac{\\| X_{T} \\|_{1}}{C}.</math>\n\nThe following inequalities also hold, : for ''p''&nbsp;=&nbsp;1,\n\n:<math>\\| S_{T} \\|_{p} \\leq \\frac{e}{e - 1} \\left( 1 + \\| X_{T} \\log^+ X_{T} \\|_{p} \\right)</math>\n\nand, for ''p''&nbsp;>&nbsp;1,\n\n:<math>\\| X_{T} \\|_{p} \\leq \\| S_{T} \\|_{p} \\leq \\frac{p}{p-1} \\| X_{T} \\|_{p}.</math>\n\n==Related inequalities==\nDoob's inequality for discrete-time martingales implies [[Kolmogorov's inequality]]: if ''X''<sub>1</sub>, ''X''<sub>2</sub>, ... is a sequence of real-valued [[independent random variables]], each with mean zero, it is clear that\n\n:<math>\\begin{align}\n\\mathbf{E} \\left[ X_{1} + \\dots + X_{n} + X_{n + 1} \\big| X_{1}, \\dots, X_{n} \\right] &= X_{1} + \\dots + X_{n} + \\mathbf{E} \\left[ X_{n + 1} \\big| X_{1}, \\dots, X_{n} \\right] \\\\\n&= X_{1} + \\cdots + X_{n},\n\\end{align}</math>\n\nso ''M<sub>n</sub>''&nbsp;=&nbsp;''X''<sub>1</sub>&nbsp;+&nbsp;...&nbsp;+&nbsp;''X<sub>n</sub>'' is a martingale. Note that [[Jensen's inequality]] implies that |''M<sub>n</sub>''| is a nonnegative submartingale if ''M<sub>n</sub>'' is a martingale. Hence, taking ''p''&nbsp;=&nbsp;2 in Doob's martingale inequality,\n\n:<math>\\mathbf{P} \\left[ \\max_{1 \\leq i \\leq n} \\left| M_{i} \\right| \\geq \\lambda \\right] \\leq \\frac{\\mathbf{E} \\left[ M_{n}^{2} \\right]}{\\lambda^{2}},</math>\n\nwhich is precisely the statement of Kolmogorov's inequality.\n\n==Application: Brownian motion==\nLet ''B'' denote canonical one-dimensional [[Brownian motion]]. Then\n\n:<math>\\mathbf{P} \\left[ \\sup_{0 \\leq t \\leq T} B_{t} \\geq C \\right] \\leq \\exp \\left( - \\frac{C^2}{2T} \\right).</math>\n\nThe proof is just as follows: since the exponential function is monotonically increasing, for any non-negative λ,\n\n:<math>\\left\\{ \\sup_{0 \\leq t \\leq T} B_{t} \\geq C \\right\\} = \\left\\{ \\sup_{0 \\leq t \\leq T} \\exp ( \\lambda B_{t} ) \\geq \\exp ( \\lambda C ) \\right\\}.</math>\n\nBy Doob's inequality, and since the exponential of Brownian motion is a positive submartingale,\n\n:<math>\\begin{align}\n\\mathbf{P} \\left[ \\sup_{0 \\leq t \\leq T} B_{t} \\geq C \\right] & = \\mathbf{P} \\left[ \\sup_{0 \\leq t \\leq T} \\exp ( \\lambda B_{t} ) \\geq \\exp ( \\lambda C ) \\right] \\\\\n& \\leq \\frac{\\mathbf{E} \\left[ \\exp (\\lambda B_{T}) \\right ]}{\\exp (\\lambda C)} \\\\\n& = \\exp \\left( \\tfrac{1}{2}\\lambda^{2}T - \\lambda C \\right) && \\mathbf{E} \\left[ \\exp (\\lambda B_{t}) \\right] = \\exp \\left( \\tfrac{1}{2}\\lambda^{2} t \\right)\n\\end{align}</math>\n\nSince the left-hand side does not depend on λ, choose λ to minimize the right-hand side: λ&nbsp;=&nbsp;''C''/''T'' gives the desired inequality.\n\n==References==\n* {{cite book |author1=Revuz, Daniel  |author2=Yor, Marc | title=Continuous martingales and Brownian motion | edition=Third | publisher=Springer| location=Berlin | year=1999 | isbn=3-540-64325-7}} (Theorem II.1.7)\n* {{springer|id=M/m062570|title=Martingale|author=[[Albert Shiryaev|Shiryaev, Albert N.]]}}\n\n{{Stochastic processes}}\n\n[[Category:Probabilistic inequalities]]\n[[Category:Statistical inequalities]]\n[[Category:Martingale theory]]"
    },
    {
      "title": "Eaton's inequality",
      "url": "https://en.wikipedia.org/wiki/Eaton%27s_inequality",
      "text": "{{primary sources|date=April 2013}}\n\nIn [[probability theory]], '''Eaton's inequality''' is a bound on the largest values of a linear combination of bounded [[random variables]].  This inequality was described in 1974 by Morris L. Eaton.<ref name=Eaton1974>Eaton, Morris L. (1974) \"A probability inequality for linear combinations of bounded random variables.\" ''Annals of Statistics'' 2(3) 609–614</ref>\n\n==Statement of the inequality==\n\nLet {''X''<sub>i</sub>} be a set of real independent random variables, each with an [[expected value]] of zero and bounded above by 1 ( |''X''<sub>''i''</sub> | ≤ 1, for 1 ≤ ''i'' ≤ ''n''). The variates do not have to be identically or symmetrically distributed. Let {''a''<sub>''i''</sub>} be a set of ''n'' fixed real numbers with\n\n: <math> \\sum_{ i = 1 }^n a_i^2 = 1 .</math>\n\nEaton showed that\n\n: <math> P\\left( \\left| \\sum_{ i = 1 }^n a_i X_i \\right| \\ge k \\right) \\le 2 \\inf_{ 0 \\le c \\le k } \\int_c^\\infty \\left( \\frac{ z - c }{ k - c } \\right)^3 \\phi( z ) \\, dz = 2 B_E( k ) ,</math>\n\nwhere ''φ''(''x'') is the [[probability density function]] of the [[standard normal distribution]].\n\nA related bound is Edelman's{{cn|date=April 2013}}\n\n: <math> P\\left( \\left| \\sum_{ i = 1 }^n a_i X_i \\right| \\ge k \\right) \\le 2 \\left( 1 - \\Phi\\left[ k - \\frac{ 1.5 }{ k } \\right] \\right) = 2 B_{ Ed }( k ) , </math>\n\nwhere Φ(''x'') is [[cumulative distribution function]] of the standard normal distribution.\n\nPinelis has shown that Eaton's bound can be sharpened:<ref name=Pinelis1994>Pinelis, I. (1994) \"Extremal probabilistic problems and Hotelling's ''T''<sup>2</sup> test under a symmetry condition.\" ''Annals of Statistics'' 22(1), 357–368</ref>\n\n: <math> B_{ EP } = \\min\\{ 1, k^{ -2 }, 2 B_E \\} </math>\n\nA set of critical values for Eaton's bound have been determined.<ref name=Dufour1993>Dufour, J-M; Hallin, M (1993) \"Improved Eaton bounds for linear combinations of bounded random variables, with statistical applications\", ''Journal of the American Statistical Association'', 88(243) 1026–1033</ref>\n\n==Related inequalities==\n\nLet {''a''<sub>i</sub>} be a set of independent [[Rademacher distribution|Rademacher random variables]] – ''P''( ''a''<sub>''i''</sub> = 1 ) = ''P''( ''a''<sub>''i''</sub> = −1 ) = 1/2. Let ''Z'' be a normally distributed variate with a [[mean]] 0 and [[variance]] of 1. Let {''b''<sub>''i''</sub>} be a set of ''n'' fixed real numbers such that\n\n: <math> \\sum_{ i = 1 }^n b_i^2 = 1 .</math>\n\nThis last condition is required by the [[Riesz–Fischer theorem]] which states that \n\n:<math> a_i b_i + \\cdots + a_n b_n </math>\n\nwill converge if and only if \n\n: <math> \\sum_{ i = 1 }^n b_i^2 </math>\n\nis finite. \n\nThen \n\n: <math> E f( a_i b_i + \\cdots + a_n b_n ) \\le E f( Z ) </math>\n\nfor ''f''(x) = | x |<sup>p</sup>. The case for ''p'' ≥ 3 was proved by Whittle<ref name=Whittle1960>Whittle P (1960) Bounds for the moments of linear and quadratic forms in independent variables. Teor Verojatnost i Primenen 5: 331–335 MR0133849</ref> and ''p'' ≥ 2 was proved by Haagerup.<ref name=Haagerup1982>Haagerup U (1982) The best constants in the Khinchine inequality. Studia Math 70: 231–283 MR0654838</ref>\n\n\nIf ''f''(x) = ''e''<sup>λx</sup> with ''λ'' ≥ 0 then \n\n:<math> E f( a_i b_i + \\cdots + a_n b_n ) \\le \\inf \\left[ \\frac{ E ( e^{ \\lambda Z  } ) }{ e^{ \\lambda x } } \\right] = e^{ -x^2 / 2 } </math>\n\nwhere ''inf'' is the [[infimum]].<ref name=Hoeffding1963>Hoeffding W (1963) Probability inequalities for sums of bounded random variables. J Amer Statist Assoc 58: 13–30 MR144363</ref>\n\n\nLet \n\n:<math> S_n =  a_i b_i + \\cdots + a_n b_n </math>\n\n\nThen<ref name=Pinelis1994a>Pinelis I (1994) Optimum bounds for the distributions of martingales in Banach spaces. Ann Probab 22(4):1679–1706</ref>\n\n:<math> P( S_n \\ge x ) \\le \\frac{ 2e^3 }{ 9 } P( Z \\ge x ) </math>\n\nThe constant in the last inequality is approximately 4.4634.\n\n\nAn alternative bound is also known:<ref name=delaPena2009>de la Pena, VH, Lai TL, Shao Q (2009) Self normalized processes. Springer-Verlag, New York</ref>\n\n:<math> P( S_n \\ge x ) \\le e^{ -x^2 / 2 } </math>\n\nThis last bound is related to the [[Hoeffding's inequality]].\n\n\nIn the uniform case where all the ''b''<sub>''i''</sub> = ''n''<sup>−1/2</sup> the maximum value of ''S''<sub>''n''</sub> is ''n''<sup>1/2</sup>. In this case van Zuijlen has shown that<ref name=vanZuijlen2011>van Zuijlen  Martien CA (2011) On a conjecture concerning the sum of independent Rademacher random variables. https://arxiv.org/abs/1112.4988</ref>\n\n: <math> P( | \\mu - \\sigma | ) \\le 0.5 \\, </math>{{clarification needed|date=April 2013}}\n\nwhere ''μ'' is the [[mean]] and ''σ'' is the [[standard deviation]] of the sum.\n\n==References==\n{{reflist}}\n\n[[Category:Probabilistic inequalities]]\n[[Category:Statistical inequalities]]"
    },
    {
      "title": "Entropy power inequality",
      "url": "https://en.wikipedia.org/wiki/Entropy_power_inequality",
      "text": "In [[information theory]], the '''entropy power inequality''' is a result that relates to so-called \"entropy power\" of [[random variable]]s. It shows that the entropy power of suitably [[well-behaved]] random variables is a [[superadditive]] [[function (mathematics)|function]]. The entropy power inequality was proved in 1948 by [[Claude Shannon]] in his seminal paper \"[[A Mathematical Theory of Communication]]\". Shannon also provided a sufficient condition for equality to hold; Stam (1959) showed that the condition is in fact necessary.\n\n==Statement of the inequality==\n\nFor a random variable ''X''&nbsp;:&nbsp;Ω&nbsp;→&nbsp;'''R'''<sup>''n''</sup> with [[probability density function]] ''f''&nbsp;:&nbsp;'''R'''<sup>''n''</sup>&nbsp;→&nbsp;'''R''', the [[differential entropy]] of ''X'', denoted ''h''(''X''), is defined to be\n\n:<math>h(X) = - \\int_{\\mathbb{R}^{n}} f(x) \\log f(x) \\, d x</math>\n\nand the entropy power of ''X'', denoted ''N''(''X''), is defined to be\n\n:<math> N(X) = \\frac{1}{2\\pi e} e^{ \\frac{2}{n} h(X) }.</math>\n\nIn particular, ''N''(''X'') = |''K''| <sup>1/''n''</sup> when ''X''&nbsp;is normal distributed with covariance matrix ''K''.\n\nLet ''X'' and ''Y'' be [[independent random variables]] with probability density functions in the [[Lp space|''L''<sup>''p''</sup> space]] ''L''<sup>''p''</sup>('''R'''<sup>''n''</sup>) for some ''p''&nbsp;&gt;&nbsp;1. Then\n\n:<math>N(X + Y) \\geq N(X) + N(Y). \\,</math>\n\nMoreover, equality holds [[if and only if]] ''X'' and ''Y'' are [[multivariate normal]] random variables with proportional [[covariance matrix|covariance matrices]].\n\n==See also==\n*[[Information entropy]]\n*[[Information theory]]\n*[[Limiting density of discrete points]]\n*[[Self-information]]\n*[[Kullback–Leibler divergence]]\n*[[Entropy estimation]]\n\n==References==\n\n* {{cite journal\n| last = Dembo\n| first = Amir |author2=Cover, Thomas M. |author3=Thomas, Joy A.\n| title = Information-theoretic inequalities\n| journal = IEEE Trans. Inf. Theory\n| volume = 37\n| year = 1991\n| issue = 6\n| pages = 1501&ndash;1518\n| doi = 10.1109/18.104312\n| mr = 1134291\n}}\n* {{cite journal\n| last=Gardner \n| first=Richard J. \n| title=The Brunn–Minkowski inequality \n| journal=Bull. Amer. Math. Soc. (N.S.) \n| volume=39 \n| issue=3 \n| year=2002 \n| pages=355&ndash;405 (electronic) \n| doi=10.1090/S0273-0979-02-00941-2 \n}}\n* {{cite journal\n| last = Shannon\n| first = Claude E.\n| authorlink = Claude Shannon\n| title = A mathematical theory of communication\n| journal = [[Bell System Technical Journal|Bell System Tech. J.]]\n| volume = 27\n| issue = 3\n| year = 1948\n| pages = 379&ndash;423, 623&ndash;656\n| doi = 10.1002/j.1538-7305.1948.tb01338.x\n}}\n* {{cite journal\n| last = Stam\n| first = A. J.\n| title = Some inequalities satisfied by the quantities of information of Fisher and Shannon\n| journal = Information and Control\n| volume = 2\n| year = 1959\n| pages = 101&ndash;112\n| doi = 10.1016/S0019-9958(59)90348-1\n| issue = 2\n}}\n\n[[Category:Information theory]]\n[[Category:Probabilistic inequalities]]\n[[Category:Statistical inequalities]]"
    },
    {
      "title": "Etemadi's inequality",
      "url": "https://en.wikipedia.org/wiki/Etemadi%27s_inequality",
      "text": "In [[probability theory]], '''Etemadi's inequality''' is a so-called \"maximal inequality\", an [[inequality (mathematics)|inequality]] that gives a bound on the [[probability]] that the [[partial sum]]s of a [[Finite set|finite]] collection of [[independent random variables]] exceed some specified bound. The result is due to [[Nasrollah Etemadi]].\n\n==Statement of the inequality==\n\nLet ''X''<sub>1</sub>, ..., ''X''<sub>''n''</sub> be independent real-valued random variables defined on some common [[probability space]], and let ''α'' ≥ 0. Let ''S''<sub>''k''</sub> denote the partial sum\n\n:<math>S_{k} = X_{1} + \\cdots + X_{k}.\\,</math>\n\nThen\n\n:<math>\\mathbb{P} \\Bigl( \\max_{1 \\leq k \\leq n} | S_{k} | \\geq 3 \\alpha \\Bigr) \\leq 3 \\max_{1 \\leq k \\leq n} \\mathbb{P} \\bigl( | S_{k} | \\geq \\alpha \\bigr).</math>\n\n==Remark==\n\nSuppose that the random variables ''X''<sub>''k''</sub> have common [[expected value]] zero. Apply [[Chebyshev's inequality]] to the right-hand side of Etemadi's inequality and replace ''α'' by ''α'' / 3. The result is [[Kolmogorov's inequality]] with an extra factor of 27 on the right-hand side:\n\n:<math>\\mathbb{P} \\Bigl( \\max_{1 \\leq k \\leq n} | S_{k} | \\geq \\alpha \\Bigr) \\leq \\frac{27}{\\alpha^{2}} \\mathrm{Var} (S_{n}).</math>\n\n==References==\n\n* {{cite book | last=Billingsley | first=Patrick | title=Probability and Measure | publisher=John Wiley & Sons, Inc. | location=New York | year=1995 | isbn=0-471-00710-2}} (Theorem 22.5)\n* {{cite journal | last=Etemadi | first=Nasrollah | title=On some classical results in probability theory | journal=[[Sankhya (journal)|Sankhyā]] Ser. A | volume=47 | year=1985 | pages=215&ndash;221 |mr=0844022 | jstor = 25050536 | issue=2 }}\n\n[[Category:Probabilistic inequalities]]\n[[Category:Statistical inequalities]]"
    },
    {
      "title": "Fréchet inequalities",
      "url": "https://en.wikipedia.org/wiki/Fr%C3%A9chet_inequalities",
      "text": "In [[probabilistic logic]], the '''Fréchet inequalities''', also known as the '''Boole–Fréchet inequalities''', are rules implicit in the work of [[George Boole]]<ref name=boole54>Boole, G. (1854). ''An Investigation of the Laws of Thought, On Which Are Founded the Mathematical Theories of Logic and Probability.'' Walton and Maberly, London. See Boole's \"major\" and \"minor\" limits of a conjunction on page 299.</ref><ref name=hailperin86>Hailperin, T. (1986). ''Boole's Logic and Probability''. North-Holland, Amsterdam.</ref> and explicitly derived by [[Maurice René Fréchet|Maurice Fréchet]]<ref name=frechet35>Fréchet, M. (1935). Généralisations du théorème des probabilités totales. ''Fundamenta Mathematicae'' '''25''': 379–387.</ref><ref name=frechet51>Fréchet, M. (1951). Sur les tableaux de corrélation dont les marges sont données. ''Annales de l'Université de Lyon. Section A: Sciences mathématiques et astronomie'' '''9''': 53–77.</ref> that govern the combination of probabilities about [[logical proposition]]s or [[Event (probability theory)|events]] logically linked together in [[logical conjunction|conjunctions]] ([[AND gate|AND]] operations) or [[logical disjunction|disjunctions]] ([[OR gate|OR]] operations) as in [[Boolean algebra|Boolean expressions]] or [[fault tree|fault]] or [[event trees]] common in [[risk assessments]], [[engineering design]] and [[artificial intelligence]].  These inequalities can be considered rules about how to bound calculations involving probabilities without assuming [[independence]] or, indeed, without making any [[stochastic dependence|dependence]] assumptions whatsoever.  The Fréchet inequalities are closely related to the [[Boole's inequality#Bonferroni inequalities|Boole–Bonferroni–Fréchet inequalities]], and to [[Copula (probability theory)#Fr.C3.A9chet.E2.80.93Hoeffding copula bounds|Fréchet bounds]].\n\nIf ''A''<sub>''i''</sub> are [[logical proposition]]s or [[Event (probability theory)|events]], the Fréchet inequalities are\n\n:Probability of a [[logical conjunction]] (&)\n::max(0, P(''A''<sub>1</sub>) + P(''A''<sub>2</sub>) + ... + P(''A''<sub>''n''</sub>) &minus; (''n'' &minus; 1)) ≤ P(''A''<sub>1</sub> & ''A''<sub>2</sub> & ... & ''A''<sub>''n''</sub>) ≤ min(P(''A''<sub>1</sub>), P(''A''<sub>2</sub>), ..., P(''A''<sub>''n''</sub>)),\n\n:Probability of a [[logical disjunction]] (∨)\n::max(P(''A''<sub>1</sub>), P(''A''<sub>2</sub>), ..., P(''A''<sub>''n''</sub>)) ≤ P(''A''<sub>1</sub> ∨ ''A''<sub>2</sub> ∨ ... ∨ ''A''<sub>''n''</sub>) ≤ min(1, P(''A''<sub>1</sub>) + P(''A''<sub>2</sub>) + ... + P(''A''<sub>''n''</sub>)),\n\nwhere P( ) denotes the probability of an event or proposition.  In the case where there are only two events, say ''A'' and ''B'', the inequalities reduce to\n\n:Probability of a logical conjunction (&)\n::max(0, P(''A'') + P(''B'') &minus; 1) ≤ P(''A'' & ''B'') ≤ min(P(''A''), P(''B'')),\n\n:Probability of a logical disjunction (∨)\n::max(P(''A''), P(''B'')) ≤ P(''A'' ∨ ''B'') ≤ min(1, P(''A'') + P(''B'')).\n\nThe inequalities bound the probabilities of the two kinds of joint events given the probabilities of the individual events.  For example, if A is \"has lung cancer\", and B is \"has mesothelioma\", then A&nbsp;&&nbsp;B is \"has both lung cancer and mesothelioma\", and A&nbsp;∨&nbsp;B is \"has lung cancer or mesothelioma or both diseases\", and the inequalities relate the risks of these events.\n\nNote that logical conjunctions are denoted in various ways in different fields, including AND, &, ∧ and graphical [[AND gate#Symbols|AND-gates]].  Logical disjunctions are likewise denoted in various ways, including OR, |, ∨, and graphical [[OR gate#Symbols|OR-gates]].  If events are taken to be [[Set (mathematics)|sets]] rather than [[logical proposition]]s, the [[set theory|set-theoretic]] versions of the Fréchet inequalities are\n\n:Probability of an [[intersection (set theory)|intersection]] of events\n::max(0,P(''A'') + P(''B'') &minus; 1) ≤ P(''A'' ∩ ''B'') ≤ min(P(''A''), P(''B'')),\n\n:Probability of a [[union (set theory)|union]] of events\n::max(P(''A''), P(''B'')) ≤ P(''A'' ∪<!--the cup ∪ is perversely smaller than the cap ∩, so we might use the capital letter U but it looks much worse in print outs--> ''B'') ≤ min(1, P(''A'') + P(''B'')).\n\n==Numerical examples==\nIf the probability of an event A is P(A) = ''a'' = 0.7, and the probability of the event B is P(B) = ''b'' = 0.8, then the probability of the [[logical conjunction|conjunction]]. i.e., the joint event A & B, is surely in the interval\n: &nbsp;&nbsp;P(A & B) &isin; [max(0, ''a'' + ''b'' &minus; 1), min(''a'', ''b'')]\n:::: = [max(0, 0.7 + 0.8&minus;1), min(0.7, 0.8)]\n:::: = [0.5, 0.7].\nLikewise, the probability of the [[logical disjunction|disjunction]] A ∨ B is surely in the interval\n: &nbsp;&nbsp;P(A ∨ B) &isin; [max(''a'', ''b''), min(1, ''a'' + ''b'')]\n:::: = [max(0.7, 0.8), min(1, 0.7 + 0.8)]\n:::: = [0.8, 1].\n\nThese intervals are contrasted with the results obtained from the rules of [[Probability#Independent probability|probability assuming independence]], where the probability of the conjunction is P(A & B) = ''a'' &times; ''b'' = 0.7 &times; 0.8 = 0.56, and the probability of the disjunction is P(A ∨ B) = ''a'' + ''b'' &minus; ''a'' &times; ''b'' = 0.94.\n\nWhen the marginal probabilities are very small (or large), the \nFréchet intervals are strongly asymmetric about the analogous results under independence.  For example, suppose P(A) = 0.000002 = 2&times;10<sup>&minus;6</sup> and P(B) = 0.000003 = 3&times;10<sup>&minus;6</sup>.  Then the Fréchet inequalities say P(A & B) is in the interval [0, 2&times;10<sup>&minus;6</sup>], and P(A ∨ B) is in the interval [3&times;10<sup>&minus;6</sup>, 5&times;10<sup>&minus;6</sup>]. If A and B are independent, however, the probability of A & B is 6&times;10<sup>&minus;12</sup> which is, comparatively, very close to the lower limit (zero) of the Fréchet interval.  Similarly, the probability of A ∨ B is 4.999994&times;10<sup>&minus;6</sup>, which is very close to the upper limit of the Fréchet interval.  This is what justifies the rare-event approximation<ref>Collet, J. (1996). Some remarks on rare-event approximation. ''IEEE Transactions on Reliability'' '''45''': 106–108.</ref> often used in [[reliability theory]].<!--  \n==Applications==\nKrzysztofowicz, Roman (2002). Probabilistic flood forecast: bounds and approximations. ''Journal of Hydrology'' '''268''': 41–55.\n-->\n\n==Proofs==\nThe proofs are elementary.  Recall that P(''A'' ∨ ''B'') = P(''A'') + P(''B'') &minus; P(''A'' & ''B''), which implies P(''A'') + P(''B'') &minus; P(''A'' ∨ ''B'') = P(''A'' & ''B'').  Because all probabilities are no bigger than 1, we know P(''A'' ∨ ''B'') ≤ 1, which implies that P(''A'') + P(''B'') &minus; 1 ≤ P(''A'' & ''B''). Because all probabilities are also positive we can similarly say 0 ≤ P(''A'' & ''B''), so max(0, P(''A'') + P(''B'') &minus; 1) ≤ P(''A'' & ''B''). This gives the lower bound on the conjunction.\n\nTo get the upper bound, recall that P(''A'' & ''B'') = P(''A''|''B'') P(''B'') = P(''B''|''A'') P(''A''). Because P(''A''|''B'') ≤ 1 and P(''B''|''A'') ≤ 1, we know P(''A'' & ''B'') ≤ P(''A'') and P(''A'' & ''B'') ≤ P(''B'').  Therefore, P(''A'' & ''B'') ≤ min(P(''A''), P(''B'')), which is the upper bound.\n\nThe best-possible nature of these bounds follows from observing that they are realized by some dependency between the events A and B.  Comparable bounds on the disjunction are similarly derived.\n\n==Extensions==\nWhen the input probabilities are themselves interval ranges, the Fréchet formulas still work as a [[Probability bounds analysis#Logical expressions|probability bounds analysis]].\nHailperin<ref name=hailperin86 /> considered the problem of evaluating probabilistic Boolean expressions involving many events in complex conjunctions and disjunctions.\nSome<ref name=wise&henrion86>Wise, B.P., and M. Henrion (1986). A framework for comparing uncertain inference systems to probability. ''Uncertainty in Artificial Intelligence'', edited by L.N. Kanal and J.F. Lemmer, Elsevier Science Publishers, B.V. North-Holland, Amsterdam.</ref><ref name=williamson89>Williamson, R.C. (1989). [http://www.ressources-actuarielles.net/EXT/ISFA/1226.nsf/769998e0a65ea348c1257052003eb94f/d3a13f9fb23c0e17c1257827004bd249/$FILE/thesis300dpi.pdf ''Probabilistic Arithmetic'']. Dissertation, University of Queensland.</ref> have suggested using the inequalities in various applications of artificial intelligence and have extended the rules to account for various assumptions about the dependence among the events.  The inequalities can also be generalized to other logical operations, including even [[modus ponens]].<ref name=wise&henrion86 /><ref>Wagner, C.G. (2004). [http://www.math.utk.edu/~wagner/papers/2004.pdf ''Modus tollens'' probabilized]. British Journal for the Philosophy of Science'' '''55''': 747–753.</ref>  When the input probabilities are characterized by [[probability distribution]]s, analogous operations that generalize logical and arithmetic convolutions without assumptions about the dependence between the inputs can be defined based on the related notion of [[Fréchet bounds]].<ref name=williamson89 /><ref>Weisstein, Eric W. [http://mathworld.wolfram.com/FrechetBounds.html Fréchet bounds]. MathWorld--A Wolfram Web Resource.</ref><ref>Rüschendorf, L. (1991). [http://www.stochastik.uni-freiburg.de/homepages/rueschendorf/papers/frechetbounds.pdf Fréchet-bounds and their applications]. Pages 151–187 in ''Advances in Probability Distributions with Given Marginals, Mathematics and Its Applications'' '''67''', edited by G. Dall'Aglio, S. Kotz and G. Salinetti, Kluwer, Dordrecht.</ref>\n\n==Quantum Fréchet bounds==\nIt is interesting that similar bounds hold also in [[Quantum Mechanics]] in the case of [[Separable state|separable quantum systems]] and that [[Entangled state|entangled]] states violate these bounds.<ref name=\"benavoli2016\">{{cite journal| last1=Benavoli| first1=A.| last2=Facchini| first2=A.| last3=Zaffalon|first3=M.|title=Quantum mechanics: The Bayesian theory generalized to the space of Hermitian matrices|journal=Physical Review A |date=10 October 2016|volume=94|issue=4|pages=1–27| doi=10.1103/PhysRevA.94.042106| arxiv=1605.08177| url=http://journals.aps.org/pra/abstract/10.1103/PhysRevA.94.042106| bibcode=2016PhRvA..94d2106B}}</ref> Consider a composite quantum system. In particular, we focus on a composite quantum system ''AB'' made by two finite subsystems denoted as ''A'' and ''B''. Assume that we know the [[density matrix]] of the subsystem ''A'', i.e., <math>\\rho^A</math> that is a trace-one positive definite matrix in <math>\\Complex_{h}^{n\\times n}</math> (the space of [[Hermitian matrices]] of dimension <math>n \\times n</math>), and the density matrix of subsystem ''B'' denoted as <math>\\rho^B.</math> We can think of <math>\\rho^A</math> and <math>\\rho^B</math> as the ''marginals'' of the subsystems ''A'' and ''B''. From the knowledge of these marginals, we want to infer something about the ''joint'' <math>\\rho^{AB}</math> in <math>\\Complex_{h}^{nm\\times nm}.</math> We restrict our attention to ''joint'' <math>\\rho^{AB}</math> that are [[Separable state|separable]]. A density matrix on a composite system is separable if there exist <math>p_k\\geq 0, \\{\\rho_1^k \\}</math> and <math>\\{ \\rho_2^k \\}</math> which are mixed states of the respective subsystems such that\n\n:<math>\\rho^{AB}=\\sum_k p_k \\rho_1^k \\otimes \\rho_2^k</math>\n\nwhere\n\n:<math>\\sum_k p_k = 1.</math>\n\nOtherwise <math>\\rho^{AB}</math> is called an entangled state.\n\nFor [[Separable state#Separability for mixed states|separable density matrices]] <math> \\rho^{AB}</math> in <math>\\Complex_{h}^{nm\\times nm}</math> the following Fréchet like bounds hold:\n\n:<math> \\begin{cases} \\rho^{AB} \\leq \\rho^{A} \\otimes I_m \\\\ \\rho^{AB} \\leq I_n \\otimes \\rho^{B} \\\\[6pt] \\rho^{AB} \\geq \\rho^A \\otimes I_m + I_n \\otimes \\rho^B-I_{nm} \\\\ \\rho^{AB} \\gneq 0 \\end{cases}</math>\n\nThe inequalities are [[Positive-definite matrix|matrix inequalities]], <math>\\otimes</math> denotes the [[tensor product]] and <math>I_x</math> the [[identity matrix]] of dimension <math>x</math>. It is evident that structurally the above inequalities are analogues of the classical Fréchet bounds for the logical conjunction. It is also worth to notice that when the matrices <math>\\rho^A,\\rho^B</math> and <math>\\rho^{AB}</math> are restricted to be diagonal, we obtain the classical Fréchet bounds.\n\nThe upper bound is known in Quantum Mechanics as [[reduction criterion]] for density matrices; it was first proven by <ref>{{cite journal|journal=Phys. Rev. A|volume='''59'''|pages= 4206|year=1999|author=M. Horodecki and P. Horodecki| title=Reduction criterion of separability and limits for a class of distillation protocols |doi=10.1103/PhysRevA.59.4206| arxiv=quant-ph/9708015|bibcode = 1999PhRvA..59.4206H }}</ref> and independently formulated by.<ref>{{cite journal|journal=Phys. Rev. A|volume='''60'''|pages= 898|year=1999|author=N. Cerf|title=Reduction criterion for separability|doi=10.1103/PhysRevA.60.898|arxiv=quant-ph/9710001|bibcode = 1999PhRvA..60..898C |display-authors=etal}}</ref> The lower bound has been obtained in <ref name=benavoli2016/>{{rp|Theorem A.16}} that provides a Bayesian interpretation of these bounds.\n\n===Numerical examples===\nWe have observed when the matrices <math>\\rho^A,\\rho^B</math> and <math>\\rho^{AB}</math> are all diagonal, we obtain the classical Fréchet bounds. To show that, consider again the previous numerical example:\n\n:<math>\\begin{align}\n\\rho^A & = \\text{diag}(p_{a},p_{\\bar{a}})=\\text{diag}(0.7,0.3) \\\\\n\\rho^B & = \\text{diag}(p_{b},p_{\\bar{b}})=\\text{diag}(0.8,0.2)\n\\end{align}</math>\n\nthen we have:\n\n:<math>\\begin{align}\n\\rho^{AB}&= \\text{diag}(p_{ab},p_{a\\bar{b}},p_{\\bar{a}b},p_{\\bar{a}\\bar{b}}) \\leqslant \\rho^{A} \\otimes I_2 =\\text{diag}(0.7,0.7,0.3,0.3) \\\\\n\\rho^{AB}&= \\text{diag}(p_{ab},p_{a\\bar{b}},p_{\\bar{a}b},p_{\\bar{a}\\bar{b}}) \\leqslant I_2 \\otimes \\rho^{B} =\\text{diag}(0.8,0.2,0.8,0.2) \\\\\n\\rho^{AB}&= \\text{diag}(p_{ab},p_{a\\bar{b}},p_{\\bar{a}b},p_{\\bar{a}\\bar{b}}) \\geqslant \\rho^{A} \\otimes I_2 + I_2 \\otimes \\rho^{B} - I_4 = \\text{diag}(0.5, -0.1, 0.1, -0.5) \\\\\n\\rho^{AB}&=\\text{diag}(p_{ab},p_{a\\bar{b}},p_{\\bar{a}b},p_{\\bar{a}\\bar{b}}) \\geqslant 0\n\\end{align}</math>\n\nwhich means:\n\n:<math>\\begin{align}\n 0.5  &\\leqslant  p_{ab}             \\leqslant 0.7 \\\\\n 0    &\\leqslant  p_{a\\bar{b}}       \\leqslant 0.2 \\\\\n 0.1  &\\leqslant  p_{\\bar{a}b}       \\leqslant 0.3 \\\\\n 0    &\\leqslant  p_{\\bar{a}\\bar{b}} \\leqslant 0.2 \n\\end{align}</math>\n\nIt is worth to point out that [[Entangled state|entangled]] states violate the above Fréchet bounds. Consider for instance the entangled density matrix (which is not separable):\n\n:<math>\\rho^{AB}=\\frac{1}{2} \\begin{bmatrix}\n1 & 0 &0 & 1\\\\ \n0 & 0 &0 & 0\\\\ \n0 & 0 &0 & 0\\\\ \n1 & 0 &0 & 1\\\\ \n \\end{bmatrix},</math>\n\nwhich has marginal\n\n:<math>\\rho^A=\\rho^B=\\text{diag}\\left(\\tfrac{1}{2}, \\tfrac{1}{2} \\right).</math>\n\nEntangled states are not separable and it can easily be verified that\n\n:<math>\\begin{cases} \\rho^A \\otimes I_m-\\rho^{AB} \\ngeqslant0 \\\\ I_n \\otimes \\rho^B -\\rho^{AB} \\ngeqslant0 \\end{cases} </math>\n\nsince the resulting matrices have one negative eigenvalue.\n\nAnother example of violation of probabilistic bounds is provided by the famous [[Bell inequality|Bell's inequality]]: entangled states exhibit a form of ''stochastic'' dependence stronger than the strongest classical dependence: and in fact they violate Fréchet like bounds.\n\n==See also==\n*[[Probabilistic logic]]\n*[[Logical conjunction]]\n*[[Logical disjunction]]\n*[[Copula (probability theory)#Fr.C3.A9chet.E2.80.93Hoeffding copula bounds|Fréchet bounds]]\n*[[Boole's inequality|Boole’s inequality]]\n*[[Boole's inequality#Bonferroni inequalities|Bonferroni inequalities]]\n*[[Bernstein–Fréchet inequalities]]\n*[[Probability bounds analysis]]\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Frechet inequalities}}\n[[Category:Articles containing proofs]]\n[[Category:Probabilistic inequalities]]\n[[Category:Statistical inequalities]]\n[[Category:Probability bounds analysis]]"
    },
    {
      "title": "Gauss's inequality",
      "url": "https://en.wikipedia.org/wiki/Gauss%27s_inequality",
      "text": "{{distinguish|text=the [[Gaussian correlation inequality]] or the [[Gaussian isoperimetric inequality]]}}\nIn [[probability theory]], '''Gauss's inequality''' (or the '''Gauss inequality''') gives an upper bound on the probability that a [[unimodal]] [[random variable]] lies more than any given distance from its [[mode (statistics)|mode]].\n\nLet ''X'' be a unimodal random variable with mode ''m'', and let ''&tau;''<sup>&nbsp;2</sup> be the [[expected value]] of (''X''&nbsp;&minus;&nbsp;''m'')<sup>2</sup>. (''&tau;''<sup>&nbsp;2</sup> can also be expressed as (''&mu;''&nbsp;&minus;&nbsp;''m'')<sup>2</sup>&nbsp;+&nbsp;''&sigma;''<sup>&nbsp;2</sup>, where ''&mu;'' and ''&sigma;'' are the mean and [[standard deviation]] of ''X''.)  Then for any positive value of ''k'',\n\n:<math>\n\\Pr(\\mid X - m \\mid > k) \\leq \\begin{cases}\n\\left( \\frac{2\\tau}{3k} \\right)^2 & \\text{if } k \\geq \\frac{2\\tau}{\\sqrt{3}} \\\\[6pt]\n1 - \\frac{k}{\\tau\\sqrt{3}}        & \\text{if } 0 \\leq k \\leq \\frac{2\\tau}{\\sqrt{3}}.\n\\end{cases}</math>\n\nThe theorem was first proved by [[Carl Friedrich Gauss]] in 1823.\n\n==See also==\n*[[Vysochanskiï–Petunin inequality]], a similar result for the distance from the mean rather than the mode \n*[[Chebyshev's inequality]], concerns distance from the mean without requiring unimodality\n\n==References==\n*{{cite journal|last=Gauss|first=C. F.|authorlink=Carl Friedrich Gauss|date=1823|title=Theoria Combinationis Observationum Erroribus Minimis Obnoxiae, Pars Prior|journal=Commentationes Societatis Regiae Scientiarum Gottingensis Recentiores|volume=5}}\n*{{cite book|title=A Dictionary of Statistics| publisher=Oxford University Press| last= Upton | first = Graham |author2=Cook, Ian| year=2008|chapter=Gauss inequality| url=http://www.answers.com/topic/gauss-inequality}}\n*{{Cite journal\n | doi = 10.2307/2684690\n | title = Chebyshev inequalities for unimodal distributions\n | year = 1997\n | journal = [[American Statistician]]\n | volume = 51\n | issue = 1\n | pages = 34–40\n | last1 = Sellke\t | first1 =  T.M.\n | last2 =  Sellke\t | first2 =  S.H.\n | publisher = American Statistical Association\n | jstor = 2684690\n}}\n*{{Cite journal\n | doi = 10.2307/2684253\n | title = The Three Sigma Rule\n | year = 1994\n | author = Pukelsheim, F.\n | journal = American Statistician\n | volume = 48\n | issue = 2\n | pages = 88–91\n | publisher = American Statistical Association\n | jstor = 2684253\n}}\n\n[[Category:Probabilistic inequalities]]"
    },
    {
      "title": "Gaussian isoperimetric inequality",
      "url": "https://en.wikipedia.org/wiki/Gaussian_isoperimetric_inequality",
      "text": "In mathematics, the '''Gaussian isoperimetric inequality''', proved by [[Boris Tsirelson]] and [[Vladimir Sudakov]]<ref>{{Cite journal|last=Sudakov|first=V. N.|last2=Tsirel'son|first2=B. S.|date=1978-01-01|orig-year=Translated from Zapiski Nauchnykh Seminarov Leningradskogo Otdeleniya Matematicheskogo Instituta im. V. A. Steklova AN SSSR, Vol. 41, pp. 14–24, 1974|title=Extremal properties of half-spaces for spherically invariant measures|url=https://doi.org/10.1007/BF01086099|journal=Journal of Soviet Mathematics|language=en|volume=9|issue=1|pages=9–18|doi=10.1007/BF01086099|issn=1573-8795|via=}}</ref>, and later independently by [[Christer Borell]]<ref>{{Cite journal|last=Borell|first=Christer|date=1975|title=The Brunn-Minkowski Inequality in Gauss Space.|url=https://eudml.org/doc/142349|journal=Inventiones mathematicae|volume=30|pages=207–216|issn=0020-9910}}</ref>, states that among all sets of given [[Gaussian measure]] in the ''n''-dimensional [[Euclidean space]], [[Half-space (geometry)|half-space]]s have the minimal Gaussian [[Minkowski content|boundary measure]].\n\n== Mathematical formulation ==\nLet <math>\\scriptstyle A</math> be a [[measurable]] subset of <math>\\scriptstyle\\mathbf{R}^n </math> endowed with the standard Gaussian measure <math>\\gamma^n</math> with the density <math> {\\exp(-\\|x\\|^2/2)}/(2\\pi)^{n/2}</math>. Denote by \n: <math>A_\\varepsilon = \\left\\{ x \\in \\mathbf{R}^n \\, | \\, \n\\text{dist}(x, A) \\leq \\varepsilon \\right\\}</math>\n\nthe &epsilon;-extension of ''A''. Then the ''Gaussian isoperimetric inequality'' states that\n\n: <math>\\liminf_{\\varepsilon \\to +0} \n \\varepsilon^{-1} \\left\\{ \\gamma^n (A_\\varepsilon) - \\gamma^n(A) \\right\\}\n \\geq \\varphi(\\Phi^{-1}(\\gamma^n(A))),</math>\n\nwhere\n\n: <math>\\varphi(t) = \\frac{\\exp(-t^2/2)}{\\sqrt{2\\pi}}\\quad{\\rm and}\\quad\\Phi(t) = \\int_{-\\infty}^t \\varphi(s)\\, ds. </math>\n\n== Proofs and generalizations ==\nThe original proofs by Sudakov, Tsirelson and Borell were based on [[Paul Lévy (mathematician)|Paul Lévy]]'s [[spherical isoperimetric inequality]]. \n\n[[Sergey Bobkov]] proved a functional generalization of the Gaussian isoperimetric inequality, from a certain \"two point analytic inequality\"<ref>{{Cite journal|last=Bobkov|first=S. G.|date=1997|title=An isoperimetric inequality on the discrete cube, and an elementary proof of the isoperimetric inequality in Gauss space|url=https://projecteuclid.org/euclid.aop/1024404285|journal=The Annals of Probability|language=en|volume=25|issue=1|pages=206–214|doi=10.1214/aop/1024404285|issn=0091-1798|via=}}</ref>. Bakry and Ledoux gave another proof of Bobkov's functional inequality based on the [[semigroup]] techniques which works in a much more abstract setting<ref>{{Cite journal|last=Bakry|first=D.|last2=Ledoux|first2=M.|date=1996-02-01|title=Lévy–Gromov’s isoperimetric inequality for an infinite dimensional diffusion generator|url=https://doi.org/10.1007/s002220050026|journal=Inventiones mathematicae|language=en|volume=123|issue=2|pages=259–281|doi=10.1007/s002220050026|issn=1432-1297}}</ref>. Later Barthe and Maurey gave yet another proof using the [[Brownian motion]]<ref>{{Cite journal|last=Barthe|first=F.|last2=Maurey|first2=B.|date=2000-07-01|title=Some remarks on isoperimetry of Gaussian type|url=http://www.sciencedirect.com/science/article/pii/S024602030000131X|journal=Annales de l'Institut Henri Poincare (B) Probability and Statistics|volume=36|issue=4|pages=419–434|doi=10.1016/S0246-0203(00)00131-X|issn=0246-0203}}</ref>. \n\nThe Gaussian isoperimetric inequality also follows from [[Ehrhard's inequality]]<ref>{{Cite journal|last=Latała|first=Rafał|date=1996|title=A note on the Ehrhard inequality|url=https://www.infona.pl//resource/bwmeta1.element.bwnjournal-article-smv118i2p169bwm|journal=Studia Mathematica|language=English|volume=2|issue=118|pages=169–174|issn=0039-3223}}</ref><ref>{{Cite journal|last=Borell|first=Christer|date=2003-11-15|title=The Ehrhard inequality|url=http://www.sciencedirect.com/science/article/pii/S1631073X03004461|journal=Comptes Rendus Mathematique|volume=337|issue=10|pages=663–666|doi=10.1016/j.crma.2003.09.031|issn=1631-073X}}</ref>.\n\n== See also ==\n* [[Concentration of measure]]\n* [[Borell–TIS inequality]]\n\n[[Category:Probabilistic inequalities]]\n\n== References ==\n{{reflist}}"
    },
    {
      "title": "Grönwall's inequality",
      "url": "https://en.wikipedia.org/wiki/Gr%C3%B6nwall%27s_inequality",
      "text": "In [[mathematics]], '''Grönwall's inequality''' (also called '''Grönwall's lemma''' or the '''Grönwall–Bellman inequality''') allows one to bound a function that is known to satisfy a certain [[differential inequality|differential]] or [[integral inequality]] by the solution of the corresponding differential or [[integral equation]]. There are two forms of the lemma, a differential form and an integral form. For the latter there are several variants.\n\nGrönwall's inequality is an important tool to obtain various estimates in the theory of [[ordinary differential equation|ordinary]] and [[stochastic differential equation]]s. In particular, it provides a [[comparison theorem]] that can be used to prove [[uniqueness quantification|uniqueness]] of a solution to the [[initial value problem]]; see the [[Picard–Lindelöf theorem]].\n\nIt is named for [[Thomas Hakon Grönwall]] (1877–1932). Grönwall is the Swedish spelling of his name, but he spelled his name as Gronwall in his scientific publications after emigrating to the United States.\n\nThe differential form was proven by Grönwall in 1919.<ref name=\"gronwall\">{{Citation | last = Gronwall | first = Thomas H. | author-link = Thomas Hakon Grönwall | title = Note on the derivatives with respect to a parameter of the solutions of a system of differential equations | journal = [[Annals of Mathematics|Ann. of Math.]] | volume = 20 | issue = 2 | pages = 292–296 | year = 1919 | jstor = 1967124 | mr = 1502565 | jfm = 47.0399.02}}</ref>\nThe integral form was proven by [[Richard Bellman]] in 1943.<ref>{{Citation | last = Bellman | first = Richard | author-link = Richard Bellman | title = The stability of solutions of linear differential equations | journal = [[Duke Mathematical Journal|Duke Math. J.]] | volume = 10 | issue =  4 | pages = 643–647 | year = 1943 | url = http://projecteuclid.org/euclid.dmj/1077472225 | mr = 0009408 | zbl = 0061.18502 | doi=10.1215/s0012-7094-43-01059-2}}</ref>\n\nA nonlinear generalization of the Grönwall–Bellman inequality is known as [[Bihari–LaSalle inequality]]. Other variants and generalizations can be found in Pachpatte, B.G. (1998).<ref>{{cite book|last1=Pachpatte|first1=B.G.|title=Inequalities for differential and integral equations|date=1998|publisher=Academic Press|location=San Diego|isbn=9780080534640}}</ref>\n\n== Differential form ==\nLet {{math|''I''}} denote an [[Interval (mathematics)|interval]] of the [[real line]] of the form {{closed-open|''a'',&thinsp;∞}} or {{closed-closed|''a'', ''b''}} or {{closed-open|''a'', ''b''}} with {{math|''a'' < ''b''}}. Let {{math|''β''}} and {{math|''u''}} be real-valued [[continuous functions]] defined on {{math|''I''}}.  If&nbsp;{{math|''u''}} is [[derivative|differentiable]] in the [[Interior (topology)|interior]] {{math|''I''<sup>o</sup>}} of {{math|''I''}} (the interval {{math|''I''}} without the end points {{math|''a''}} and possibly {{math|''b''}}) and satisfies the differential inequality\n\n:<math>u'(t) \\le \\beta(t)\\,u(t),\\qquad t\\in I^\\circ,</math>\n\nthen {{math|''u''}} is bounded by the solution of the corresponding differential ''equation'' {{math|''v''&thinsp;&prime;(''t'') {{=}} ''β''(''t'')&thinsp;''v''(''t'')}}:\n\n:<math>u(t) \\le u(a) \\exp\\biggl(\\int_a^t \\beta(s)\\, \\mathrm{d} s\\biggr)</math>\n\nfor all {{math|''t'' ∈ ''I''}}.\n\n'''Remark:''' There are no assumptions on the signs of the functions {{math|''β''}} and&nbsp;{{math|''u''}}.\n\n=== Proof ===\n\nDefine the function\n\n:<math>v(t) =  \\exp\\biggl(\\int_a^t \\beta(s)\\, \\mathrm{d} s\\biggr),\\qquad t\\in I.</math>\n\nNote that {{math|''v''}} satisfies\n\n:<math>v'(t) = \\beta(t)\\,v(t),\\qquad t\\in I^\\circ,</math>\n\nwith {{math|''v''(''a'') {{=}} 1}} and {{math|''v''(''t'') > 0}} for all {{math|''t'' ∈ ''I''}}. By the [[quotient rule]]\n\n:<math>\\frac{d}{dt}\\frac{u(t)}{v(t)} = \\frac{u'(t)\\,v(t)-v'(t)\\,u(t)}{v^2(t)} = \\frac{u'(t)\\,v(t) - \\beta(t)\\,v(t)\\,u(t)}{v^2(t)} \\le 0,\\qquad t\\in I^\\circ,</math>\n\nThus the derivative of the function <math>u(t)/v(t)</math> is non-positive and the function is bounded above by its value at the initial point <math>a</math> of the interval <math>I</math>:\n\n:<math>\\frac{u(t)}{v(t)}\\le \\frac{u(a)}{v(a)}=u(a),\\qquad t\\in I,</math>\n\nwhich is Grönwall's inequality.\n\n== Integral form for continuous functions ==\nLet {{math|''I''}} denote an [[Interval (mathematics)|interval]] of the [[real line]] of the form {{closed-open|''a'', ∞}} or {{closed-closed|''a'', ''b''}} or {{closed-open|''a'', ''b''}} with {{math|''a'' < ''b''}}. Let {{math|''α''}}, {{math|''β''}} and {{math|''u''}} be real-valued functions defined on&nbsp;{{math|''I''}}. Assume that {{math|''β''}} and {{math|''u''}} are continuous and that the negative part of {{math|''α''}} is integrable on every closed and bounded subinterval of&nbsp;{{math|''I''}}.\n\n*(a) If&nbsp;{{math|''β''}} is non-negative and if {{math|''u''}} satisfies the integral inequality\n::<math>u(t) \\le \\alpha(t) + \\int_a^t \\beta(s) u(s)\\,\\mathrm{d}s,\\qquad \\forall t\\in I,</math>\n:then\n::<math> u(t) \\le \\alpha(t) + \\int_a^t\\alpha(s)\\beta(s)\\exp\\biggl(\\int_s^t\\beta(r)\\,\\mathrm{d}r\\biggr)\\mathrm{d}s,\\qquad t\\in I.</math>\n*(b) If, in addition, the function {{math|''α''}} is non-decreasing, then\n\n::<math>u(t) \\le \\alpha(t)\\exp\\biggl(\\int_a^t\\beta(s)\\,\\mathrm{d}s\\biggr),\\qquad t\\in I.</math>\n\n'''Remarks:'''\n* There are no assumptions on the signs of the functions {{math|''α''}} and&nbsp;{{math|''u''}}.\n* Compared to the differential form, differentiability of {{math|''u''}} is not needed for the integral form.\n* For a version of Grönwall's inequality which doesn't need continuity of {{math|''β''}} and {{math|''u''}}, see the version in the next section.\n\n=== Proof ===\n(a) Define\n\n:<math>v(s) = \\exp\\biggl({-}\\int_a^s\\beta(r)\\,\\mathrm{d}r\\biggr)\\int_a^s\\beta(r)u(r)\\,\\mathrm{d}r,\\qquad s\\in I.</math>\n\nUsing the [[product rule]], the [[chain rule]], the derivative of the [[exponential function]] and the [[fundamental theorem of calculus]], we obtain for the derivative\n\n:<math>v'(s) = \\biggl(\\underbrace{u(s)-\\int_a^s\\beta(r)u(r)\\,\\mathrm{d}r}_{\\le\\,\\alpha(s)}\\biggr)\\beta(s)\\exp\\biggl({-}\\int_a^s\\beta(r)\\mathrm{d}r\\biggr),\n\\qquad s\\in I,</math>\n\nwhere we used the assumed integral inequality for the upper estimate. Since {{math|''β''}} and the exponential are non-negative, this gives an upper estimate for the derivative of&nbsp;{{math|''v''}}. Since {{math|''v''(''a'') {{=}} 0}}, integration of this inequality from {{math|''a''}} to {{math|''t''}} gives\n\n:<math>v(t) \\le\\int_a^t\\alpha(s)\\beta(s)\\exp\\biggl({-}\\int_a^s\\beta(r)\\,\\mathrm{d}r\\biggr)\\mathrm{d}s.</math>\n\nUsing the definition of {{math|''v''(''t'')}} for the first step, and then this inequality and the [[functional equation]] of the exponential function, we obtain\n\n:<math>\\begin{align}\\int_a^t\\beta(s)u(s)\\,\\mathrm{d}s\n&=\\exp\\biggl(\\int_a^t\\beta(r)\\,\\mathrm{d}r\\biggr)v(t)\\\\\n&\\le\\int_a^t\\alpha(s)\\beta(s)\\exp\\biggl(\\underbrace{\\int_a^t\\beta(r)\\,\\mathrm{d}r-\\int_a^s\\beta(r)\\,\\mathrm{d}r}_{=\\,\\int_s^t\\beta(r)\\,\\mathrm{d}r}\\biggr)\\mathrm{d}s.\n\\end{align}</math>\n\nSubstituting this result into the assumed integral inequality gives Grönwall's inequality.\n\n(b) If the function {{math|''α''}} is non-decreasing, then part (a), the fact {{math|''α''(''s'') ≤ ''α''(''t'')}}, and the fundamental theorem of calculus imply that\n\n:<math>\\begin{align}u(t)&\\le\\alpha(t)+\\biggl({-}\\alpha(t)\\exp\\biggl(\\int_s^t\\beta(r)\\,\\mathrm{d}r\\biggr)\\biggr)\\biggr|^{s=t}_{s=a}\\\\\n&=\\alpha(t)\\exp\\biggl(\\int_a^t\\beta(r)\\,\\mathrm{d}r\\biggr),\\qquad t\\in I.\\end{align}</math>\n\n== Integral form with locally finite measures ==\nLet {{math|''I''}} denote an [[Interval (mathematics)|interval]] of the [[real line]] of the form {{closed-open|''a'', ∞}} or {{closed-closed|''a'', ''b''}} or {{closed-open|''a'', ''b''}} with {{math|''a'' < ''b''}}. Let {{math|''α''}} and {{math|''u''}} be [[measurable function]]s defined on&nbsp;{{math|''I''}} and let {{math|''μ''}} be a continuous non-negative measure on the [[Borel σ-algebra]] of {{math|''I''}} satisfying {{math|''μ''(<nowiki>[</nowiki>''a'', ''t''<nowiki>]</nowiki>) < ∞}} for all {{math|''t'' ∈ ''I''}} (this is certainly satisfied when {{math|''μ''}} is a [[locally finite measure]]). Assume that {{math|''u''}} is integrable with respect to {{math|''μ''}} in the sense that\n\n:<math>\\int_{[a,t)}|u(s)|\\,\\mu(\\mathrm{d}s)<\\infty,\\qquad t\\in I,</math>\n\nand that {{math|''u''}} satisfies the integral inequality\n\n:<math>u(t) \\le \\alpha(t) + \\int_{[a,t)} u(s)\\,\\mu(\\mathrm{d}s),\\qquad t\\in I.</math>\n\nIf, in addition,\n* the function {{math|''α''}} is non-negative or\n* the function {{math|''t'' {{mapsto}} ''μ''(<nowiki>[</nowiki>''a'', ''t''<nowiki>]</nowiki>)}} is continuous for {{math|''t'' ∈ ''I''}} and the function {{math|''α''}} is integrable with respect to {{math|''μ''}} in the sense that\n\n:: <math>\\int_{[a,t)}|\\alpha(s)|\\,\\mu(\\mathrm{d}s)<\\infty,\\qquad t\\in I,</math>\n\nthen {{math|''u''}} satisfies Grönwall's inequality\n\n:<math>u(t) \\le \\alpha(t) + \\int_{[a,t)}\\alpha(s)\\exp\\bigl(\\mu(I_{s,t})\\bigr)\\,\\mu(\\mathrm{d}s)</math>\n\nfor all {{math|''t'' ∈ ''I''}}, where {{math|''I<sub>s,t</sub>''}} denotes to open interval {{open-open|''s'', ''t''}}.\n\n===Remarks===\n* There are no continuity assumptions on the functions  {{math|''α''}} and {{math|''u''}}.\n* The integral in Grönwall's inequality is allowed to give the value infinity.\n* If {{math|''α''}} is the zero function and {{math|''u''}} is non-negative, then Grönwall's inequality implies that {{math|''u''}} is the zero function.\n* The integrability of {{math|''u''}} with respect to {{math|''μ''}} is essential for the result. For a [[counterexample]], let {{math|''μ''}} denote [[Lebesgue measure]] on the [[unit interval]] {{closed-closed|0,&thinsp;1}}, define {{math|''u''(0) {{=}} 0}} and {{math|''u''(''t'') {{=}} 1/''t''}} for {{math|''t'' ∈ }}{{open-closed|0, 1}}, and let {{math|''α''}} be the zero function.\n* The version given in the textbook by S.&nbsp;Ethier and T.&nbsp;Kurtz.<ref>{{Citation | last = Ethier | first = Steward N. | last2 = Kurtz | first2 = Thomas G. | title = Markov Processes, Characterization and Convergence | place = New York | publisher = [[John Wiley & Sons]] | year = 1986 | page = 498 | isbn = 0-471-08186-8 | mr = 0838085 | zbl = 0592.60049}}</ref> makes the stronger assumptions that {{math|''α''}} is a non-negative constant and {{math|''u''}} is bounded on bounded intervals, but doesn't assume that the measure {{math|''μ''}} is locally finite. Compared to the one given below, their proof does not discuss the behaviour of the remainder {{math|''R<sub>n</sub>''(''t'')}}.\n\n===Special cases===\n* If the measure {{math|''μ''}} has a density {{math|''β''}} with respect to  Lebesgue measure, then Grönwall's inequality can be rewritten as\n\n:: <math>u(t) \\le \\alpha(t) + \\int_a^t \\alpha(s)\\beta(s)\\exp\\biggl(\\int_s^t\\beta(r)\\,\\mathrm{d}r\\biggr)\\,\\mathrm{d}s,\\qquad t\\in I.</math>\n\n* If the function {{math|''α''}} is non-negative and the density {{math|''β''}} of {{math|''μ''}} is bounded by a constant {{math|''c''}}, then\n\n:: <math>u(t) \\le \\alpha(t) + c\\int_a^t \\alpha(s)\\exp\\bigl(c(t-s)\\bigr)\\,\\mathrm{d}s,\\qquad t\\in I.</math>\n\n* If, in addition, the non-negative function {{math|''α''}} is non-decreasing, then\n\n:: <math>u(t) \\le \\alpha(t) + c\\alpha(t)\\int_a^t \\exp\\bigl(c(t-s)\\bigr)\\,\\mathrm{d}s\n=\\alpha(t)\\exp(c(t-a)),\\qquad t\\in I.</math>\n\n=== Outline of proof ===\nThe proof is divided into three steps. In idea is to substitute the assumed integral inequality into itself {{math|''n''}} times. This is done in Claim&nbsp;1 using mathematical induction. In Claim&nbsp;2 we rewrite the measure of a simplex in a convenient form, using the permutation invariance of product measures. In the third step we pass to the limit {{math|''n''}} to infinity to derive the desired variant of Grönwall's inequality.\n\n=== Detailed proof ===\n\n====Claim 1: Iterating the inequality====\nFor every natural number {{math|''n''}} including zero,\n\n:<math>u(t) \\le \\alpha(t) + \\int_{[a,t)} \\alpha(s) \\sum_{k=0}^{n-1} \\mu^{\\otimes k}(A_k(s,t))\\,\\mu(\\mathrm{d}s) + R_n(t)</math>\n\nwith remainder\n\n:<math>R_n(t) :=\\int_{[a,t)}u(s)\\mu^{\\otimes n}(A_n(s,t))\\,\\mu(\\mathrm{d}s),\\qquad t\\in I,</math>\n\nwhere\n\n:<math>A_n(s,t)=\\{(s_1,\\ldots,s_n)\\in I_{s,t}^n\\mid s_1<s_2<\\cdots<s_n\\},\\qquad n\\ge1,</math>\n\nis an {{math|''n''}}-dimensional [[simplex]] and\n\n:<math>\\mu^{\\otimes 0}(A_0(s,t)):=1.</math>\n\n====Proof of Claim 1====\nWe use [[mathematical induction]]. For {{math|''n'' {{=}} 0}} this is just the assumed integral inequality, because the [[empty sum]] is defined as zero.\n\nInduction step from {{math|''n''}} to {{math|''n'' + 1}}:\nInserting the assumed integral inequality for the function {{math|''u''}} into the remainder gives\n\n:<math>R_n(t)\\le\\int_{[a,t)} \\alpha(s) \\mu^{\\otimes n}(A_n(s,t))\\,\\mu(\\mathrm{d}s) +\\tilde R_n(t)</math>\n\nwith\n\n:<math>\\tilde R_n(t):=\\int_{[a,t)} \\biggl(\\int_{[a,q)} u(s)\\,\\mu(\\mathrm{d}s)\\biggr)\\mu^{\\otimes n}(A_n(q,t))\\,\\mu(\\mathrm{d}q),\\qquad t\\in I.</math>\n\nUsing the [[Fubini's theorem|Fubini–Tonelli theorem]] to interchange the two integrals, we obtain\n\n:<math>\\tilde R_n(t)\n=\\int_{[a,t)} u(s)\\underbrace{\\int_{(s,t)} \\mu^{\\otimes n}(A_n(q,t))\\,\\mu(\\mathrm{d}q)}_{=\\,\\mu^{\\otimes n+1}(A_{n+1}(s,t))}\\,\\mu(\\mathrm{d}s)\n=R_{n+1}(t),\\qquad t\\in I.</math>\n\nHence [[#Claim 1: Iterating the inequality|Claim 1]] is proved for {{math|''n'' + 1}}.\n\n====Claim 2: Measure of the simplex====\nFor every natural number {{math|''n''}} including zero and all {{math|''s'' < ''t''}} in {{math|''I''}}\n\n:<math>\\mu^{\\otimes n}(A_n(s,t))\\le\\frac{\\bigl(\\mu(I_{s,t})\\bigr)^n}{n!}</math>\n\nwith equality in case {{math|''t'' {{mapsto}} ''μ''(<nowiki>[</nowiki>''a'', ''t''<nowiki>]</nowiki>)}} is continuous for {{math|''t'' ∈ ''I''}}.\n\n====Proof of Claim 2====\nFor {{math|''n'' {{=}} 0}}, the claim is true by our definitions. Therefore, consider {{math|''n'' ≥ 1}} in the following.\n\nLet {{math|''S<sub>n</sub>''}} denote the set of all [[permutation]]s of the indices in {{math|{1, 2, . . . , ''n''}}}. For every permutation {{math|''σ'' ∈ ''S<sub>n</sub>''}} define\n\n:<math>A_{n,\\sigma}(s,t)=\\{(s_1,\\ldots,s_n)\\in I_{s,t}^n\\mid s_{\\sigma(1)}<s_{\\sigma(2)}<\\cdots<s_{\\sigma(n)}\\}.</math>\n\nThese sets are disjoint for different permutations and\n\n:<math>\\bigcup_{\\sigma\\in S_n}A_{n,\\sigma}(s,t)\\subset I_{s,t}^n.</math>\n\nTherefore,\n\n:<math>\\sum_{\\sigma\\in S_n} \\mu^{\\otimes n}(A_{n,\\sigma}(s,t))\n\\le\\mu^{\\otimes n}\\bigl(I_{s,t}^n\\bigr)=\\bigl(\\mu(I_{s,t})\\bigr)^n.</math>\n\nSince they all have the same measure with respect to the {{math|''n''}}-fold product of {{math|''μ''}}, and since there are {{math|''n''!}} permutations in&nbsp;{{math|''S<sub>n</sub>''}}, the claimed inequality follows.\n\nAssume now that {{math|''t'' {{mapsto}} ''μ''(<nowiki>[</nowiki>''a'', ''t''<nowiki>]</nowiki>)}} is continuous for {{math|''t'' ∈ ''I''}}. Then, for different indices {{math|''i'', ''j'' ∈ {1, 2, . . . , ''n''}}}, the set\n\n:<math>\\{(s_1,\\ldots,s_n)\\in I_{s,t}^n\\mid s_i=s_j\\}</math>\n\nis contained in a [[hyperplane]], hence by an application of [[Fubini's theorem]] its measure with respect to the {{math|''n''}}-fold product of {{math|''μ''}} is zero. Since\n\n:<math>I_{s,t}^n\\subset\\bigcup_{\\sigma\\in S_n}A_{n,\\sigma}(s,t) \\cup \\bigcup_{1\\le i<j\\le n}\\{(s_1,\\ldots,s_n)\\in I_{s,t}^n\\mid s_i=s_j\\},</math>\n\nthe claimed equality follows.\n\n====Proof of Grönwall's inequality====\nFor every natural number {{math|''n''}}, [[#Claim 2: Measure of the simplex|Claim&nbsp;2]] implies for the remainder of [[#Claim 1: Iterating the inequality|Claim&nbsp;1]] that\n\n:<math>|R_n(t)| \\le \\frac{\\bigl(\\mu(I_{a,t})\\bigr)^n}{n!} \\int_{[a,t)} |u(s)|\\,\\mu(\\mathrm{d}s),\\qquad t\\in I.</math>\n\nBy assumption we have {{math|''μ''(''I''<sub>''a'',''t''</sub>) < ∞}}. Hence, the integrability assumption on {{math|''u''}} implies that\n\n:<math>\\lim_{n\\to\\infty}R_n(t)=0,\\qquad t\\in I.</math>\n\n[[#Claim 2: Measure of the simplex|Claim&nbsp;2]] and the [[Characterizations of the exponential function|series representation]] of the exponential function imply the estimate\n\n:<math>\\sum_{k=0}^{n-1} \\mu^{\\otimes k}(A_k(s,t))\n\\le\\sum_{k=0}^{n-1} \\frac{\\bigl(\\mu(I_{s,t})\\bigr)^k}{k!}\n\\le\\exp\\bigl(\\mu(I_{s,t})\\bigr)</math>\n\nfor all {{math|''s'' < ''t''}} in&nbsp;{{math|''I''}}. If the function&nbsp;{{math|''α''}} is non-negative, then it suffices to insert these results into [[#Claim 1: Iterating the inequality|Claim&nbsp;1]] to derive the above variant of Grönwall's inequality for the function&nbsp;{{math|''u''}}.\n\nIn case {{math|''t'' {{mapsto}} ''μ''(<nowiki>[</nowiki>''a'', ''t''<nowiki>]</nowiki>)}} is continuous for {{math|''t'' ∈ ''I''}}, [[#Claim 2: Measure of the simplex|Claim&nbsp;2]] gives\n\n:<math>\\sum_{k=0}^{n-1} \\mu^{\\otimes k}(A_k(s,t))\n=\\sum_{k=0}^{n-1} \\frac{\\bigl(\\mu(I_{s,t})\\bigr)^k}{k!}\n\\to\\exp\\bigl(\\mu(I_{s,t})\\bigr)\\qquad\\text{as }n\\to\\infty</math>\n\nand the integrability of the function {{math|''α''}} permits to use the [[dominated convergence theorem]] to derive Grönwall's inequality.\n\n==References==\n<references />\n\n==See also==\n*[[Logarithmic norm#Original definition|Logarithmic norm]], for a version of Gronwall's lemma that gives upper and lower bounds to the norm of the state transition matrix.\n\n{{PlanetMath attribution|id=3901|title=Gronwall's lemma}}\n\n{{DEFAULTSORT:Gronwall's inequality}}\n[[Category:Lemmas]]\n[[Category:Ordinary differential equations]]\n[[Category:Stochastic differential equations]]\n[[Category:Articles containing proofs]]\n[[Category:Probabilistic inequalities]]"
    },
    {
      "title": "Hoeffding's inequality",
      "url": "https://en.wikipedia.org/wiki/Hoeffding%27s_inequality",
      "text": "In [[probability theory]], '''Hoeffding's inequality''' provides an [[upper bound]] on the [[probability]] that the sum of bounded [[independent random variables|independent]] [[random variables]] deviates from its [[expected value]] by more than a certain amount. Hoeffding's inequality was proven by [[Wassily Hoeffding]] in 1963.<ref>{{harvtxt|Hoeffding|1963}}</ref>\n\nHoeffding's inequality is a generalization of the [[Chernoff bound]], which applies only to Bernoulli random variables<ref>{{harvtxt|Nowak|2009}}; for a more intuitive proof, see [http://statisticallearningtheory.weebly.com/uploads/2/4/6/2/24624554/exponential_tail_bounds_notes.pdf this note]</ref>, and a special case of the [[Azuma–Hoeffding inequality]] and the [[McDiarmid's inequality]]. It is similar to, but incomparable with, the [[Bernstein inequalities in probability theory|Bernstein inequality]], proved by [[Sergei Bernstein]] in 1923.\n\n== Special case of Bernoulli random variables ==\nHoeffding's inequality can be applied to the important special case of [[identically distributed]] [[Bernoulli trial|Bernoulli random variables]], and this is how the inequality is often used in [[combinatorics]] and [[computer science]]. We consider a coin that shows heads with probability {{mvar|p}} and tails with probability {{math|1 − ''p''}}. We toss the coin {{mvar|n}} times. The [[expected value|expected]] number of times the coin comes up heads is {{math|''pn''}}. Furthermore, the probability that the coin comes up heads at most {{mvar|k}} times can be exactly quantified by the following expression:\n\n:<math>\\operatorname{P}( H(n) \\leq k)= \\sum_{i=0}^{k} \\binom{n}{i} p^i (1-p)^{n-i},</math>\n\nwhere {{math|''H''(''n'')}} is the number of heads in {{mvar|n}} coin tosses.\n\nWhen {{math|1=''k'' = (''p'' − ''ε'')''n''}} for some {{math|''ε'' > 0}}, Hoeffding's inequality bounds this probability by a term that is exponentially small in {{math|''ε''<sup>2</sup>''n''}}:\n\n:<math>\\operatorname{P}( H(n) \\leq (p-\\varepsilon) n)\\leq\\exp\\left(-2\\varepsilon^2 n\\right).</math>\n\nSimilarly, when {{math|1=''k'' = (''p'' + ''ε'')''n''}} for some {{math|''ε'' > 0}}, Hoeffding's inequality bounds the probability that we see at least {{math|''εn''}} more tosses that show heads than we would expect:\n\n:<math>\\operatorname{P}(H(n) \\geq (p+\\varepsilon)n)\\leq\\exp\\left(-2\\varepsilon^2 n\\right).</math>\n\nHence Hoeffding's inequality implies that the number of heads that we see is concentrated around its mean, with exponentially small tail.\n\n:<math>\\operatorname{P}\\left((p-\\varepsilon)n \\leq H(n) \\leq (p+\\varepsilon)n\\right)\\geq 1-2\\exp\\left(-2\\varepsilon^2 n\\right).</math>\n\nFor example, taking <math>\\varepsilon=\\sqrt{\\ln{n}/n}</math> gives:\n\n:<math>\\operatorname{P}\\left(|H(n)-pn| \\leq \\sqrt{n\\ln n}\\right) \\geq 1-2\\exp\\left(-2\\ln n\\right) = 1-2/n^2.</math>\n\n== General case of bounded random variables ==\nLet {{math|''X''<sub>1</sub>, ..., ''X<sub>n</sub>''}} be [[independent random variables]] bounded by the interval {{math|[0, 1]}}:  {{math|0 ≤  ''X''<sub>''i'' </sub> ≤ 1}}. We define the empirical mean of these variables by\n\n:<math>\\overline X = \\frac{1}{n}(X_1 + \\cdots + X_n).</math>\n\nOne of the inequalities in Theorem 1 of {{harvtxt|Hoeffding|1963}} states  \n\n:<math>\\begin{align}\n\\operatorname{P}(\\overline X - \\mathrm{E}[\\overline X] \\geq t) \\leq e^{-2nt^2}\n\\end{align}</math>\nwhere <math>t \\geq 0 </math>.\n\nTheorem 2 of {{harvtxt|Hoeffding|1963}} is a generalization of the above inequality when it is known that {{math|''X''<sub>''i''</sub>}} are strictly bounded by the intervals {{math|[''a''<sub>''i''</sub>, ''b''<sub>''i''</sub>]}}:\n\n:<math>\\begin{align}\n\\operatorname{P} \\left(\\overline X - \\mathrm{E}\\left [\\overline X \\right] \\geq t \\right) &\\leq \\exp \\left(-\\frac{2n^2t^2}{\\sum_{i=1}^n (b_i - a_i)^2} \\right) \\\\\n\\operatorname{P} \\left(\\left |\\overline X - \\mathrm{E}\\left [\\overline X \\right] \\right | \\geq t \\right) &\\leq 2\\exp \\left(-\\frac{2n^2t^2}{\\sum_{i=1}^n(b_i - a_i)^2} \\right)\n\\end{align}</math>\n\nwhich are valid for positive values of {{mvar|t}}. Here {{math|E[{{overline|''X''}}]}} is the [[expected value]] of {{math|{{overline|''X''}}}}. The inequalities can be also stated in terms of the sum\n\n:<math>S_{n} = X_1 + \\cdots + X_n</math>\n\nof the random variables:\n\n:<math>\\operatorname{P}(S_{n} - \\mathrm{E}[S_{n}] \\geq t) \\leq \\exp \\left( - \\frac{2t^2}{\\sum_{i=1}^n (b_i - a_i)^2} \\right),</math>\n:<math>\\operatorname{P}(|S_{n} - \\mathrm{E}[S_{n}]| \\geq t) \\leq 2\\exp \\left( - \\frac{2t^2}{\\sum_{i=1}^n (b_i - a_i)^2} \\right).</math>\n\nNote that the inequalities also hold when the {{mvar|X<sub>i</sub>}} have been obtained using sampling without replacement; in this case the random variables are not independent anymore. A proof of this statement can be found in Hoeffding's paper. For slightly better bounds in the case of sampling without replacement, see for instance the paper by {{harvtxt|Serfling|1974}}.\n\n== General case of sub-Gaussian random variables ==\n\nA random variable {{math|''X''}} is called sub-Gaussian, if \n\n:<math>\\mathrm{P}(|X|\\geq t)\\leq 2e^{-ct^2},</math>\n\nfor some c>0. For a random variable {{math|''X''}}, the following norm is finite if and only if it is sub-Gaussian:\n\n:<math>\\Vert X \\Vert_{\\psi_2} := \\inf\\{c\\geq 0: \\mathrm{E}(e^{X^2/c^2})\\leq 2\\}.</math>\n\nThen let {{math|''X''<sub>1</sub>, ..., ''X<sub>n</sub>''}} be mean zero independent sub-Gaussian random variables, the general version of the Hoeffding's inequality states that:\n\n:<math>\n\\mathrm{P}\\left( \\left| \\sum_{i=1}^n X_i \\right| \\geq t \\right) \\leq 2\\exp\\left( -\\frac{ct^2}{\\sum_{i=1}^n \\Vert X_i \\Vert^2_{\\psi_2}} \\right),\n</math>\n\nwhere c > 0 is an absolute constant. See Theorem 2.6.2 of {{harvtxt|Vershynin|2018}} for details. \n\n== Proof ==\nIn this section, we give a proof of Hoeffding's inequality.<ref>{{harvtxt|Nowak|2009}}; for a more intuitive proof, see [http://statisticallearningtheory.weebly.com/uploads/2/4/6/2/24624554/exponential_tail_bounds_notes.pdf this note]</ref> The proof uses [[Hoeffding's lemma|'''Hoeffding's Lemma''']]:\n\n::Suppose {{mvar|X}} is a real random variable with mean zero such that <math>\\textstyle\\operatorname{P}\\left(X\\in\\left[a,b\\right]\\right)=1</math>. Then\n::::<math>\\mathrm{E} \\left [e^{sX} \\right ]\\leq \\exp\\left(\\tfrac{1}{8} s^2 (b-a)^2\\right).</math>\n\nUsing this lemma, we can prove Hoeffding's inequality. Suppose {{math|''X''<sub>1</sub>, ..., ''X<sub>n</sub>''}} are {{mvar|n}} independent random variables such that \n\n:<math>\\operatorname{P}\\left (X_{i}\\in [a_{i},b_{i}] \\right)=1, \\qquad 1\\leq i\\leq n.</math>\n\nLet \n\n:<math>S_n = X_1 + \\cdots + X_n.</math>\n\nThen for {{math|''s'', ''t'' > 0}}, [[Markov's inequality]] and the independence of {{mvar|X<sub>i</sub>}} implies:\n\n:<math>\\begin{align}\n\\operatorname{P}\\left(S_{n}-\\mathrm{E}\\left [S_n \\right ]\\geq t \\right) &= \\operatorname{P} \\left (e^{s(S_n-\\mathrm{E}\\left [S_n \\right ])} \\geq e^{st} \\right)\\\\\n&\\leq e^{-st} \\mathrm{E} \\left [e^{s(S_{n}-\\mathrm{E}\\left [S_n \\right ])} \\right ]\\\\\n&= e^{-st} \\prod_{i=1}^{n}\\mathrm{E} \\left [e^{s(X_i-\\mathrm{E}\\left [X_{i}\\right])} \\right ]\\\\\n&\\leq e^{-st} \\prod_{i=1}^{n} e^{\\frac{s^2 (b_i-a_i)^2}{8} } \\\\\n&= \\exp\\left(-st+\\tfrac{1}{8} s^2 \\sum_{i=1}^{n}(b_{i}-a_{i})^{2}\\right)\n\\end{align}</math>\n\nTo get the best possible upper bound, we find the minimum of the right hand side of the last inequality as a function of {{mvar|s}}. Define \n\n:<math>\\begin{cases} g\\colon \\mathbf{R_{+}} \\to \\mathbf{R} \\\\ g(s)=-st+\\frac{s^{2}}{8}\\sum_{i=1}^{n}(b_{i}-a_{i})^2 \\end{cases}</math>\n\nNote that {{mvar|g}} is a [[quadratic function]] and achieves its minimum at\n\n:<math>s=\\frac{4t}{\\sum_{i=1}^{n}(b_{i}-a_{i})^{2}}.</math>\n\nThus we get\n\n:<math>\\operatorname{P} \\left(S_{n}-\\mathrm{E}\\left [S_{n} \\right ]\\geq t \\right)\\leq \\exp\\left(-\\frac{2t^{2}}{\\sum_{i=1}^{n}(b_{i}-a_{i})^{2}}\\right).</math>\n\n==Usage==\n===Confidence Intervals===\nHoeffding's inequality is useful to analyse the number of required samples needed to obtain a [[confidence interval]] by solving the inequality in Theorem 1: \n\n:<math>\\operatorname{P}(\\overline X - \\mathrm{E}[\\overline X] \\geq t) \\leq e^{-2nt^2}</math>\n\nThe inequality states that the probability that the estimated and true values differ by more than {{mvar|t}} is bounded by {{mvar|e<sup>−2nt<sup>2</sup></sup>}}.\nSymmetrically, the inequality is also valid for another side of the difference:\n\n:<math>\\operatorname{P}(-\\overline X + \\mathrm{E}[\\overline X] \\geq t) \\leq e^{-2nt^2}</math>\n\nBy adding them both up, we can obtain two-sided variant of this inequality:\n\n:<math>\\operatorname{P}(|\\overline X - \\mathrm{E}[\\overline X]| \\geq t) \\leq 2e^{-2nt^2}</math>\n\nThis probability can be interpreted as the level of significance <math>\\alpha</math> (probability of making an error) for a confidence interval around <math>\\mathrm{E}[\\overline X]</math> of size {{mvar|2t}}:\n\n:<math>\\alpha=\\operatorname{P}(\\overline X \\notin [\\mathrm{E}[\\overline X]-t, \\mathrm{E}[\\overline X]+t]) \\leq 2e^{-2nt^2}</math>\n\nSolving the above for {{mvar|n}} gives us the following:\n\n:<math>n\\leq \\frac{\\log(2/\\alpha)}{2t^2}</math>\n\nTherefore, we require at least <math>\\textstyle \\frac{\\log(2/\\alpha)}{2t^2}</math> samples to acquire <math>\\textstyle (1-\\alpha)</math>-confidence interval <math>\\textstyle \\mathrm{E}[\\overline X]\\pm t</math>.\n\nHence, the cost of acquiring the confidence interval is sublinear in terms of confidence level and quadratic in terms of precision.\n\nNote that this inequality is the most conservative of the three in Theorem 1, and there are more efficient methods of estimating a [[confidence interval]].\n\n==See also==\n*[[Concentration inequality]] – a summary of tail-bounds on random variables.\n*[[Hoeffding's lemma]]\n\n==Notes==\n{{reflist}}\n\n==References==\n{{refbegin}}\n* {{cite journal\n | first1=Robert J. | last1=Serfling\n | title=Probability Inequalities for the Sum in Sampling without Replacement\n | journal=The Annals of Statistics\n | pages=39–48\n | year=1974\n | ref=harv\n | volume=2\n | number=1\n | doi=10.1214/aos/1176342611\n | mr=0420967}}\n* {{cite journal\n | first1=Wassily | last1=Hoeffding\n | title=Probability inequalities for sums of bounded random variables\n | journal=Journal of the American Statistical Association\n | pages=13–30\n | year=1963\n | ref=harv\n | volume=58\n | number=301\n | jstor=2282952\n | doi=10.1080/01621459.1963.10500830 \n | mr=0144363}}\n* {{cite web \n | url = http://nowak.ece.wisc.edu/SLT09/lecture7.pdf \n | title = Lecture 7: Chernoff's Bound and Hoeffding's Inequality \n | author = Robert Nowak \n | publisher = University of Wisconsin-Madison \n | date = 2009\n | work = ECE 901 (Summer '09) : Statistical Learning Theory Lecture Notes \n | accessdate = May 16, 2014}}\n* {{cite book  \n | first1=Roman | last1=Vershynin\n | title = High-Dimensional Probability \n | publisher = Cambridge University Press \n | year = 2018\n | isbn = 9781108415194}}\n  {{refend}}\n\n[[Category:Probabilistic inequalities]]"
    },
    {
      "title": "Hoeffding's lemma",
      "url": "https://en.wikipedia.org/wiki/Hoeffding%27s_lemma",
      "text": "In [[probability theory]], '''Hoeffding's lemma''' is an [[inequality (mathematics)|inequality]] that bounds the [[moment-generating function]] of any [[bounded function|bounded]] [[random variable]].<ref>{{cite book|author=Pascal Massart|title=Concentration Inequalities and Model Selection: Ecole d'Eté de Probabilités de Saint-Flour XXXIII - 2003|url=https://books.google.com/books?id=ZI67BQAAQBAJ&pg=PA21|date=26 April 2007|publisher=Springer|isbn=978-3-540-48503-2|page=21}}</ref>  It is named after the [[Finnish people|Finnish]]&ndash;[[United States|American]] [[mathematical statistics|mathematical statistician]] [[Wassily Hoeffding]].\n\nThe proof of Hoeffding's lemma uses [[Taylor's theorem]] and [[Jensen's inequality]].  Hoeffding's lemma is itself used in the proof of [[McDiarmid's inequality]].\n\n==Statement of the lemma==\n\nLet ''X'' be any real-valued random variable with [[expected value]] <math>\\mathbb{E}[X] = 0</math> and such that <math>a \\leq X \\leq b</math> [[almost surely]].  Then, for all <math> \\lambda \\in \\mathbb{R}</math>,\n\n:<math>\\mathbb{E} \\left[ e^{\\lambda X} \\right] \\leq \\exp \\left( \\frac{\\lambda^2 (b - a)^2}{8} \\right).</math>\n\n\nNote that because of the assumption that the random variable <math>X</math> has zero expectation, the <math>a</math> and <math>b</math> in the lemma must satisfy <math>a \\leq 0 \\leq b</math>.\n\n==A brief proof of the lemma==\n\n\nSince <math> e^{\\lambda x}</math> is a convex function of <math>x</math>, we have\n\n:<math>e^{\\lambda x}\\leq \\frac{b-x}{b-a}e^{\\lambda a}+\\frac{x-a}{b-a}e^{\\lambda b}\\qquad \\forall a\\leq x\\leq b</math>\n\nSo, <math> \\mathbb{E}\\left[e^{\\lambda X}\\right] \\leq \\frac{b-\\mathbb{E}[X]}{b-a}e^{\\lambda a}+\\frac{\\mathbb{E}[X]-a}{b-a}e^{\\lambda b}.</math>\n\nLet <math> h=\\lambda(b-a)</math>,  <math> p=\\frac{-a}{b-a}</math> and  <math> L(h)=-hp+\\ln(1-p+pe^h)</math>\n\nThen, <math>\\frac{b-\\mathbb{E}[X]}{b-a}e^{\\lambda a}+\\frac{\\mathbb{E}[X]-a}{b-a}e^{\\lambda b}=e^{L(h)}</math> since <math> \\mathbb{E}[X]=0</math>\n\nTaking derivative of <math> L(h)</math>, \n:<math> L(0)=L^{'}(0)=0\\text{ and } L^{''}(h)\\leq \\frac{1}{4}</math> for all h.\n\nBy Taylor's expansion,\n\n<math> L(h)\\leq \\frac{1}{8}h^2=\\frac{1}{8}\\lambda^2(b-a)^2</math>\n\nHence, <math> \\mathbb{E}\\left[e^{\\lambda X}\\right] \\leq e^{\\frac{1}{8}\\lambda^2(b-a)^2}</math>\n\n(The proof below is the same proof with more explanation.)\n\n==More detailed proof==\nFirst note that if one of <math>a</math> or <math>b</math> is zero, then <math>\\textstyle\\mathbb{P}\\left(X=0\\right)=1</math> and the inequality follows. If both are nonzero, then <math>a</math> must be negative and <math>b</math> must be positive.\n\nNext, recall that <math>e^{sx}</math> is a [[convex function]] on the real line:\n\n:<math>\\forall x \\in [a, b]: \\qquad e^{sx}\\leq \\frac{b-x}{b-a}e^{sa}+\\frac{x-a}{b-a}e^{sb}.</math>\n\nApplying <math>\\mathbb{E}</math> to both sides of the above inequality gives us:\n\n:<math>\\begin{align}\n\\mathbb{E} \\left [e^{sX} \\right ] &\\leq \\frac{b-\\mathbb{E}[X]}{b-a} e^{sa} + \\frac{\\mathbb{E}[X]-a}{b-a}e^{sb} \\\\\n&= \\frac{b}{b-a} e^{sa} + \\frac{-a}{b-a}e^{sb} && \\mathbb{E}(X) = 0\\\\\n&= (1-\\theta)e^{sa} + \\theta e^{sb} && \\theta=-\\frac{a}{b-a}>0 \\\\\n&= e^{sa} \\left( 1 - \\theta + \\theta e^{s(b-a)}\\right) \\\\\n&= \\left (1-\\theta+\\theta e^{s(b-a)} \\right ) e^{-s\\theta(b-a)} \\\\\n\\end{align}</math>\n\nLet <math>u=s(b-a)</math> and define:\n\n:<math>\\begin{cases} \\varphi:\\mathbb{R}\\to\\mathbb{R} \\\\ \\varphi(u)=-\\theta u+\\log \\left(1-\\theta+\\theta e^u \\right)\\end{cases}</math>\n\n<math>\\varphi</math> is well defined on <math>\\mathbb{R}</math>, to see this we calculate:\n\n:<math>\\begin{align}\n1-\\theta+\\theta e^u &= \\theta \\left (\\frac{1}{\\theta} - 1 + e^u \\right) \\\\\n& = \\theta \\left ( -\\frac{b}{a} + e^u \\right ) \\\\\n& > 0 && \\theta > 0, \\quad \\frac{b}{a} <0\n\\end{align}</math>\n\nThe definition of <math>\\varphi</math> implies \n\n:<math>\\mathbb{E} \\left [e^{sX} \\right ] \\leq e^{\\varphi(u)}.</math> \n\nBy [[Taylor's theorem]], for every real <math>u</math> there exists a <math>v</math> between <math>0</math> and <math>u</math> such that\n\n:<math>\\varphi(u)=\\varphi(0)+u\\varphi'(0)+\\tfrac{1}{2} u^2\\varphi''(v).</math>\n\nNote that: \n\n:<math>\\begin{align}\n\\varphi(0)  &= 0 \\\\\n\\varphi'(0) &= -\\theta+ \\left.\\frac{\\theta e^u}{1-\\theta +\\theta e^u}\\right|_{u=0} \\\\\n&=0 \\\\[6pt]\n\\varphi''(v) &= \\frac{\\theta e^v \\left (1-\\theta+\\theta e^v \\right)-\\theta^{2}e^{2v}}{\\left (1-\\theta+\\theta e^v \\right)^2 }\\\\[6pt]\n&=\\frac{\\theta e^v}{1-\\theta+\\theta e^v}\\left(1-\\frac{\\theta e^v}{1-\\theta+\\theta e^v}\\right)\\\\[6pt]\n&= t(1-t) && t=\\frac{\\theta e^v}{1-\\theta+\\theta e^v} \\\\\n&\\leq \\tfrac{1}{4} && t > 0\n\\end{align}</math>\n\nTherefore, \n\n:<math>\\varphi (u)\\leq 0 + u \\cdot 0 + \\tfrac{1}{2}u^2 \\cdot \\tfrac{1}{4} = \\tfrac{1}{8} u^2 = \\tfrac{1}{8}s^2(b-a)^2.</math>\n\nThis implies\n\n:<math>\\mathbb{E} \\left [e^{sX} \\right ] \\leq \\exp\\left(\\tfrac{1}{8}s^2(b-a)^2\\right).</math>\n\n==See also==\n*[[Hoeffding's inequality]]\n*[[Bennett's inequality]]\n\n==Notes==\n{{reflist}}\n\n[[Category:Probabilistic inequalities]]\n\n\n{{probability-stub}}"
    },
    {
      "title": "Hsu–Robbins–Erdős theorem",
      "url": "https://en.wikipedia.org/wiki/Hsu%E2%80%93Robbins%E2%80%93Erd%C5%91s_theorem",
      "text": "In the [[mathematics|mathematical]] [[probability theory|theory of probability]], the '''Hsu–Robbins–Erdős theorem''' states that if <math>X_1, \\ldots ,X_n</math> is a sequence of i.i.d. [[random variable]]s with zero mean and finite variance and\n\n: <math>S_n = X_1 +  \\cdots  + X_n, \\, </math>\n\nthen\n\n: <math>\\sum\\limits_{n \\geqslant 1} P( | S_n | > \\varepsilon n) < \\infty</math>\n\nfor every <math>\\varepsilon  > 0</math>.\n\nThe result was proved by [[Pao-Lu Hsu]] and [[Herbert Robbins]] in 1947.\n\nThis is an interesting strengthening of the classical strong [[law of large numbers]] in the direction of the [[Borel–Cantelli lemma]]. The idea of such a result is probably due to Robbins, but the method of proof is vintage Hsu.<ref>Chung, K. L. (1979). Hsu's work in probability. The Annals of Statistics, 479–483.</ref> Hsu and Robbins further conjectured in <ref>Hsu, P. L., & Robbins, H. (1947). Complete convergence and the law of large numbers. Proceedings of the National Academy of Sciences of the United States of America, 33(2), 25.</ref> that the condition of finiteness of the variance of <math>X</math> is also a necessary condition for <math>\\sum\\limits_{n \\geqslant 1} P(| S_n | > \\varepsilon n) < \\infty</math>  to hold. Two years later, the famed mathematician [[Paul Erdős]] proved the conjecture.<ref>Erdos, P. (1949). On a theorem of Hsu and Robbins. The Annals of Mathematical Statistics, 286–291.</ref>\n\nSince then, many authors extended this result in several directions.<ref>[http://samos.univ-paris1.fr/IMG/pdf_Tudor_expo-ConfMAGDA.pdf Hsu-Robbins theorem for the correlated sequences]</ref>\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Hsu-Robbins-Erdos Theorem}}\n[[Category:Theorems in measure theory]]\n[[Category:Probabilistic inequalities]]"
    },
    {
      "title": "Khintchine inequality",
      "url": "https://en.wikipedia.org/wiki/Khintchine_inequality",
      "text": "In [[mathematics]], the '''Khintchine inequality''', named after [[Aleksandr Khinchin]] and spelled in multiple ways in the Latin alphabet, is a theorem from [[probability]], and is also frequently used in [[mathematical analysis|analysis]]. Heuristically, it says that if we pick <math> N </math> [[complex numbers]] <math> x_1,\\dots,x_N \\in\\mathbb{C}</math>, and add them together each multiplied by a random sign <math>\\pm 1 </math>, then the [[expected value]] of the sum's [[absolute value|modulus]], or the modulus it will be closest to on average, will be not too far off from <math> \\sqrt{|x_1|^{2}+\\cdots + |x_N|^{2}}</math>.\n\n==Statement of theorem==\n\nLet <math> \\{\\varepsilon_n\\}_{n=1}^N </math> be [[i.i.d.]] [[random variables]] \nwith <math>P(\\varepsilon_n=\\pm1)=\\frac12</math> for <math>n=1,\\ldots, N</math>, \ni.e., a sequence with [[Rademacher distribution]]. Let <math> 0<p<\\infty</math> and let <math> x_1,\\ldots,x_N\\in \\mathbb{C}</math>. Then\n\n:<math> A_p \\left( \\sum_{n=1}^N |x_n|^2 \\right)^{1/2} \\leq \\left(\\operatorname{E} \\left|\\sum_{n=1}^N \\varepsilon_n x_n\\right|^p \\right)^{1/p}  \\leq B_p \\left(\\sum_{n=1}^N |x_n|^2\\right)^{1/2} </math>\n\nfor some constants <math> A_p,B_p>0 </math> depending only on <math>p</math> (see [[Expected value]] for notation). The sharp values of the constants <math>A_p,B_p</math> were found by Haagerup (Ref. 2; see Ref. 3 for a simpler proof). It is a simple matter to see that <math>A_p = 1</math> when <math>p \\ge 2</math>, and <math>B_p = 1</math> when <math>0 < p \\le 2</math>.\n\nHaagerup found that\n:<math>\n\\begin{align}\nA_p &= \\begin{cases}\n2^{1/2-1/p} & 0<p\\le p_0, \\\\\n2^{1/2}(\\Gamma((p+1)/2)/\\sqrt{\\pi})^{1/p} & p_0 < p < 2\\\\\n1 & 2 \\le p < \\infty\n\\end{cases}\n\\\\\n&\\text{and}\n\\\\\nB_p &= \\begin{cases}\n1 & 0 < p \\le 2 \\\\\n2^{1/2}(\\Gamma((p+1)/2)/\\sqrt\\pi)^{1/p} & 2 < p < \\infty\n\\end{cases},\n\\end{align}\n</math>\nwhere <math>p_0\\approx 1.847</math> and <math>\\Gamma</math> is the [[Gamma function]].\nOne may note in particular that <math>B_p</math> matches exactly [[Normal distribution#Moments|the moments of a normal distribution]].\n\nhttps://en.wikipedia.org/wiki/Normal_distribution#Moments\n\n==Uses in analysis==\n\nThe uses of this inequality are not limited to applications in [[probability theory]]. One example of its use in [[mathematical analysis|analysis]] is the following: if we let <math>T</math> be a [[linear operator]] between two [[Lp space|L<sup>''p''</sup> spaces]] <math> L^p(X,\\mu)</math> and <math> L^p(Y,\\nu) </math>, <math>1\\leq p < \\infty</math>, with bounded [[operator norm|norm]] <math> \\|T\\|<\\infty </math>, then one can use Khintchine's inequality to show that\n\n:<math> \\left\\|\\left(\\sum_{n=1}^N |Tf_n|^2 \\right)^{1/2} \\right\\|_{L^p(Y,\\nu)}\\leq C_p \\left\\|\\left(\\sum_{n=1}^N |f_n|^2\\right)^{1/2} \\right\\|_{L^p(X,\\mu)} </math>\n\nfor some constant <math>C_p>0</math> depending only on <math>p</math> and <math>\\|T\\|</math>.{{Citation needed|date=August 2018}}\n\n==Generalizations==\n\nFor the case of [[Rademacher]] random variables, Pawel Hitczenko showed <ref>[[Hitczenko|Pawel Hitczenko]], \"On the Rademacher Series\". Probability in Banach Spaces, 9 pp 31-36. {{ISBN|978-1-4612-0253-0}}</ref> that the sharpest version is:\n\n:<math>\nA \\left(\\sqrt{p}\\left(\\sum_{n=b+1}^N x_n^2\\right)^{1/2} + \\sum_{n=1}^b x_n\\right)\n\\leq \\left(\\operatorname{E} \\left|\\sum_{n=1}^N \\varepsilon_n x_n\\right|^p \\right)^{1/p}\n\\leq B \\left(\\sqrt{p}\\left(\\sum_{n=b+1}^N x_n^2\\right)^{1/2} + \\sum_{n=1}^b x_n\\right)\n</math>\n\nwhere <math>A</math> and <math>B</math> are universal constants independent of <math>p</math>.\n\nHere we assume that the <math>x_i</math> are non-negative and non-increasing.\n\n== See also ==\n* [[Marcinkiewicz–Zygmund inequality]]\n* [[Burkholder-Davis-Gundy inequality]]\n\n==References==\n{{Reflist}}\n#[[Thomas Wolff|Thomas H. Wolff]], \"Lectures on Harmonic Analysis\". American Mathematical Society, University Lecture Series vol. 29, 2003. {{ISBN|0-8218-3449-5}}\n#Uffe Haagerup, \"The best constants in the Khintchine inequality\", Studia Math. 70 (1981), no. 3, 231&ndash;283 (1982).\n#[[Fedor Nazarov]] and Anatoliy Podkorytov, \"Ball, Haagerup, and distribution functions\", Complex analysis, operators, and related topics, 247&ndash;267, Oper. Theory Adv. Appl., 113, Birkhäuser, Basel, 2000.\n\n[[Category:Theorems in analysis]]\n[[Category:Probabilistic inequalities]]"
    },
    {
      "title": "Kolmogorov's inequality",
      "url": "https://en.wikipedia.org/wiki/Kolmogorov%27s_inequality",
      "text": "In [[probability theory]], '''Kolmogorov's inequality''' is a so-called \"maximal [[inequality (mathematics)|inequality]]\" that gives a bound on the probability that the [[partial sum]]s of a [[Finite set|finite]] collection of [[independent random variables]] exceed some specified bound. The inequality is named after the [[Russia]]n [[mathematician]] [[Andrey Kolmogorov]].{{Citation needed|date=May 2007}}\n\n==Statement of the inequality==\nLet ''X''<sub>1</sub>, ..., ''X''<sub>''n''</sub>&nbsp;:&nbsp;Ω&nbsp;→&nbsp;'''R''' be [[Statistical independence|independent]] [[random variable]]s defined on a common [[probability space]] (Ω,&nbsp;''F'',&nbsp;Pr), with [[expected value]] E[''X''<sub>''k''</sub>]&nbsp;=&nbsp;0 and [[variance]] Var[''X''<sub>''k''</sub>]&nbsp;&lt;&nbsp;+∞ for ''k''&nbsp;=&nbsp;1, ..., ''n''. Then, for each λ&nbsp;&gt;&nbsp;0,\n\n:<math>\\Pr \\left(\\max_{1\\leq k\\leq n} | S_k |\\geq\\lambda\\right)\\leq \\frac{1}{\\lambda^2} \\operatorname{Var} [S_n] \\equiv \\frac{1}{\\lambda^2}\\sum_{k=1}^n \\operatorname{Var}[X_k]=\\frac{1}{\\lambda^2}\\sum_{k=1}^{n}\\text{E}[X_k^2], </math>\n\nwhere ''S''<sub>''k''</sub>&nbsp;=&nbsp;''X''<sub>1</sub>&nbsp;+&nbsp;...&nbsp;+&nbsp;''X''<sub>''k''</sub>.\n\nThe convenience of this result is that we can bound the worst case deviation of a [[random walk]] at any point of time using its value at the end of time interval.\n\n==Proof==\n{{Multiple issues|section=yes|\n{{Unreferenced section|date=November 2017}}\n{{Disputed-section|date=November 2017}}\n}}\nThe following argument is due to [[Kareem Amin]] and employs discrete [[Martingale (probability theory)|martingale]]s. \nAs argued in the discussion of [[Doob's martingale inequality]], the sequence <math>S_1, S_2, \\dots, S_n</math> is a martingale.\n[[Without loss of generality]], we can assume that <math>S_0 = 0</math> and <math>S_i \\geq 0</math> for all <math>i</math>.\nDefine <math>(Z_i)_{i=0}^n</math> as follows. Let <math>Z_0 = 0</math>, and\n:<math>Z_{i+1} = \\left\\{ \\begin{array}{ll}\nS_{i+1} & \\text{ if } \\displaystyle \\max_{1 \\leq j \\leq i} S_j < \\lambda \\\\ Z_i & \\text{ otherwise}\n\\end{array}\n\\right.\n</math>\nfor all <math>i</math>.\nThen <math>(Z_i)_{i=0}^n</math> is also a martingale. Since <math>S_i-S_{i-1}</math> is independent and mean zero,\n:<math>\\begin{align}\n\\sum_{i=1}^n \\text{E}[ (S_i - S_{i-1})^2] &= \\sum_{i=1}^n \\text{E}[ S_i^2 - 2 S_i S_{i-1} + S_{i-1}^2 ] \\\\\n&= \\sum_{i=1}^n \\text{E}\\left[ S_i^2 - 2 (S_{i-1} + S_{i} - S_{i-1}) S_{i-1}   + S_{i-1}^2 \\right] \\\\\n&= \\sum_{i=1}^n \\text{E}\\left[ S_i^2 - S_{i-1}^2 \\right] - 2\\text{E}\\left[ S_{i-1} (S_{i}-S_{i-1})\\right]\\\\\n&= \\text{E}[S_n^2] - \\text{E}[S_0^2] = \\text{E}[S_n^2].\n\\end{align}\n</math>\nThe same is true for <math>(Z_i)_{i=0}^n</math>. Thus\n:<math>\\begin{align}\n\\text{Pr}\\left( \\max_{1 \\leq i \\leq n} S_i \\geq \\lambda\\right) &=\n\\text{Pr}[Z_n \\geq \\lambda] \\\\\n&\\leq \\frac{1}{\\lambda^2} \\text{E}[Z_n^2]\n=\\frac{1}{\\lambda^2} \\sum_{i=1}^n \\text{E}[(Z_i - Z_{i-1})^2] \\\\\n&\\leq \\frac{1}{\\lambda^2} \\sum_{i=1}^n \\text{E}[(S_i - S_{i-1})^2]\n=\\frac{1}{\\lambda^2} \\text{E}[S_n^2] = \\frac{1}{\\lambda^2} \\text{Var}[S_n]\n\\end{align}\n</math>\nby [[Chebyshev's inequality]].\n\n\nThis inequality was generalized by Hájek and Rényi in 1955.\n\n==See also==\n* [[Chebyshev's inequality]]\n* [[Etemadi's inequality]]\n* [[Landau–Kolmogorov inequality]]\n* [[Markov's inequality]]\n* [[Bernstein inequalities (probability theory)]]\n\n==References==\n* {{cite book | last=Billingsley | first=Patrick | title=Probability and Measure | publisher=John Wiley & Sons, Inc. | location=New York | year=1995 | isbn=0-471-00710-2}} (Theorem 22.4)\n* {{cite book | last=Feller | first=William | authorlink=William Feller | title=An Introduction to Probability Theory and its Applications, Vol 1 | edition=Third | origyear=1950 | year=1968 | publisher=John Wiley &amp; Sons, Inc. | location=New York | isbn=0-471-25708-7 | nopp=true | page=xviii+509 }}\n\n{{PlanetMath attribution|id=3687|title=Kolmogorov's inequality}}\n\n[[Category:Stochastic processes]]\n[[Category:Probabilistic inequalities]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "Kunita–Watanabe inequality",
      "url": "https://en.wikipedia.org/wiki/Kunita%E2%80%93Watanabe_inequality",
      "text": "In [[stochastic calculus]], the '''Kunita–Watanabe inequality''' is a generalization of the [[Cauchy–Schwarz inequality]] to integrals of [[stochastic processes]].\n\n== Statement of the theorem ==\nLet ''M'', ''N'' be continuous [[local martingale]]s and ''H'', ''K'' [[measurable]] processes. Then\n\n: <math> \\int_0^t \\left| H_s \\right| \\left| K_s \\right| \\left| \\mathrm{d} \\langle M,N \\rangle_s \\right| \\leq  \\sqrt{\\int_0^t  H_s^2  \\,\\mathrm{d} \\langle M \\rangle_s} \\sqrt{\\int_0^t K_s^2 \\,\\mathrm{d} \\langle N \\rangle_s} </math>\n\nwhere the brackets indicates the [[Quadratic variation|quadratic variation and quadratic covariation]] operators. The integrals are understood in the [[Lebesgue–Stieltjes integration|Lebesgue–Stieltjes]] sense.\n\n==References==\n{{reflist}}\n*{{Cite book | title=Diffusions, Markov Processes and Martingales |volume=II, Itô; Calculus | page = 50 | first1=L. C. G. | last1=Rogers | first2= D. | last2=Williams | authorlink2=David Williams (mathematician) | authorlink1=Chris Rogers (mathematician) | year=1987 | publisher=Cambridge University Press |isbn=0-521-77593-0 |doi=10.1017/CBO9780511805141 |url=https://books.google.com/books?id=bDQy-zoHWfcC&pg=PA50 |postscript=<!--None-->}}\n\n{{DEFAULTSORT:Kunita-Watanabe theorem}}\n[[Category:Probability theorems]]\n[[Category:Probabilistic inequalities]]"
    },
    {
      "title": "Le Cam's theorem",
      "url": "https://en.wikipedia.org/wiki/Le_Cam%27s_theorem",
      "text": "In [[probability theory]], '''Le Cam's theorem''', named after [[Lucien le Cam]] (1924 &ndash; 2000), states the following.<ref name=LeCam:1960 /><ref name=LeCam:1963 /><ref name=Steele:1994 />\n\nSuppose:\n\n* <math>X_1, X_2, X_3, \\ldots </math> are [[statistical independence|independent]] [[random variable]]s, each with a [[Bernoulli distribution]] (i.e. equal to either 0 or 1), not necessarily identically distributed.\n* <math>P(X_i = 1) = p_i, \\text{ for } i = 1, 2, 3, \\ldots.</math>\n* <math>\\lambda_n = p_1 + \\cdots + p_n.</math>\n* <math>S_n = X_1 + \\cdots + X_n.</math>  (i.e. <math>S_n</math> follows a [[Poisson binomial distribution]])\n\nThen\n\n:<math>\\sum_{k=0}^\\infty \\left| \\Pr(S_n=k) - {\\lambda_n^k e^{-\\lambda_n} \\over k!} \\right| < 2 \\sum_{i=1}^n p_i^2. </math>\n\nIn other words, the sum has approximately a [[Poisson distribution]] and the above inequality bounds the approximation error in terms of the [[Total variation distance of probability measures|total variation distance]].\n\nBy setting ''p''<sub>''i''</sub> = λ<sub>''n''</sub>/''n'', we see that this generalizes the usual [[Poisson limit theorem]].\n\nWhen <math>\\lambda_n</math> is large a better bound is possible: <math>\\sum_{k=0}^\\infty \\left| \\Pr(S_n=k) - {\\lambda_n^k e^{-\\lambda_n} \\over k!} \\right| < 2 \\left(1 \\wedge \\frac 1 \\lambda_n\\right) \\sum_{i=1}^n p_i^2. </math> <ref name=denHollander:2012 />\n\nIt is also possible to weaken the independence requirement.<ref name=denHollander:2012 />\n\n==References==\n<references>\n<ref name=denHollander:2012>{{cite book | last1 = den Hollander | first1 = Frank | title = Probability Theory: the Coupling Method}}</ref>\n<ref name=LeCam:1960>{{cite journal\n |last=Le Cam |first=L. |authorlink=Lucien le Cam\n |title=An Approximation Theorem for the Poisson Binomial Distribution\n |journal=Pacific Journal of Mathematics\n |volume=10 |issue=4 |pages=1181&ndash;1197 |year=1960\n |url=http://projecteuclid.org/euclid.pjm/1103038058 |accessdate=2009-05-13\n |mr=0142174 | zbl = 0118.33601 |doi=10.2140/pjm.1960.10.1181\n}}</ref>\n<ref name=LeCam:1963>{{cite conference\n |last=Le Cam |first=L. |authorlink=Lucien le Cam\n |title=On the Distribution of Sums of Independent Random Variables\n |booktitle=Bernoulli, Bayes, Laplace: Proceedings of an International Research Seminar\n |editor1=[[Jerzy Neyman]] |editor2=Lucien le Cam\n |publisher=Springer-Verlag |location=New York\n |pages=179&ndash;202 |year=1963\n |mr=0199871\n}}</ref>\n<ref name=Steele:1994>{{Cite journal | last1 = Steele | first1 = J. M.| title = Le Cam's Inequality and Poisson Approximations | jstor = 2325124 | journal = The American Mathematical Monthly | volume = 101 | issue = 1 | pages = 48–54 | year = 1994 | doi = 10.2307/2325124}}</ref>\n</references>\n\n==External links==\n* {{MathWorld|urlname=LeCamsInequality|title=Le Cam's Inequality}}\n\n[[Category:Probability theorems]]\n[[Category:Probabilistic inequalities]]\n[[Category:Statistical inequalities]]\n[[Category:Statistical theorems]]"
    },
    {
      "title": "Lorden's inequality",
      "url": "https://en.wikipedia.org/wiki/Lorden%27s_inequality",
      "text": "In [[probability theory]], '''Lorden's inequality''' is a bound for the [[moment (mathematics)|moment]]s of overshoot for a stopped sum of [[random variable]]s, first published by Gary Lorden in 1970.<ref name=\"lorden\">{{Cite journal | last1 = Lorden | first1 = G. | title = On Excess over the Boundary | doi = 10.1214/aoms/1177697092 | journal = The Annals of Mathematical Statistics | volume = 41 | issue = 2 | pages = 520 | year = 1970 | pmid =  | pmc = | jstor = 2239350}}</ref> Overshoots play a central role in [[renewal theory]].<ref name=\"spouge\" />\n\n==Statement of inequality==\n\nLet ''X''<sub>1</sub>, ''X''<sub>2</sub>, ... be [[independent and identially distributed positive random variables]] and define the sum ''S''<sub>''n''</sub>&nbsp;=&nbsp;''X''<sub>1</sub>&nbsp;+&nbsp;''X''<sub>2</sub>&nbsp;+&nbsp;...&nbsp;+&nbsp;''X''<sub>''n''</sub>. Consider the first time ''S''<sub>''n''</sub> exceeds a given value ''b'' and at that time compute ''R''<sub>''b''</sub>&nbsp;=&nbsp;''S''<sub>''n''</sub>&nbsp;−&nbsp;''b''. ''R''<sub>''b''</sub> is called the overshoot or excess at ''b''. Lorden's inequality states that the expectation of this overshoot is bounded as<ref name=\"spouge\">{{Cite journal | last1 = Spouge | first1 = John L. | title = Inequalities on the overshoot beyond a boundary for independent summands with differing distributions | doi = 10.1016/j.spl.2007.02.013 | journal = Statistics & Probability Letters | volume = 77 | issue = 14 | pages = 1486–1489 | year = 2007 | pmid =  | pmc = 2683021}}</ref>\n::<math>\\operatorname E (R_b) \\leq \\frac{\\operatorname E (X^2)}{\\operatorname E(X)}.</math>\n\n===Proof===\n\nThree proofs are known due to Lorden,<ref name=\"lorden\" /> Carlsson and Nerman<ref>{{cite journal | last1 = Carlsson | first1 = Hasse | last2 = Nerman | first2 = Olle | year = 1986 | title = An Alternative Proof of Lorden's Renewal Inequality | journal = Advances in Applied Probability  | volume = 18 | issue = 4 | pages = 1015–1016 | publisher = Applied Probability Trust |jstor=1427260}}</ref> and Chang.<ref>{{Cite journal | last1 = Chang | first1 = J. T. | title = Inequalities for the Overshoot | doi = 10.1214/aoap/1177004913 | journal = The Annals of Applied Probability | volume = 4 | issue = 4 | pages = 1223 | year = 1994 | pmid =  | pmc = }}</ref>\n\n==See also==\n\n* [[Wald's equation]]\n\n==References==\n\n{{Reflist}}\n\n[[Category:Stochastic processes]]\n[[Category:Probabilistic inequalities]]\n\n\n{{probability-stub}}"
    },
    {
      "title": "Marcinkiewicz–Zygmund inequality",
      "url": "https://en.wikipedia.org/wiki/Marcinkiewicz%E2%80%93Zygmund_inequality",
      "text": "In [[mathematics]], the '''Marcinkiewicz&ndash;Zygmund inequality''', named after [[Józef Marcinkiewicz]] and [[Antoni Zygmund]], gives relations between [[Moment (mathematics)|moments]] of a collection of [[independent random variables]]. It is a generalization of the rule for the sum of [[variance]]s of independent random variables to moments of arbitrary order. It is a special case of the [[Burkholder-Davis-Gundy inequality]] in the case of discrete-time martingales.\n\n==Statement of the inequality==\n\n'''Theorem''' <ref name=\"Marcinkiewicz-1937-FI\">J. Marcinkiewicz and A. Zygmund. Sur les foncions independantes. ''Fund. Math.'', 28:60&ndash;90, 1937. Reprinted in Józef Marcinkiewicz, ''Collected papers'', edited by Antoni Zygmund, Panstwowe Wydawnictwo Naukowe, Warsaw, 1964, pp. 233&ndash;259.\n\n</ref><ref name=\"Chow-1988-PT\">Yuan Shih Chow and Henry Teicher. ''Probability theory. Independence, interchangeability, martingales''. Springer-Verlag, New York, second edition, 1988.\n\n</ref> If <math>\\textstyle X_{i}</math>, <math>\\textstyle i=1,\\ldots,n</math>, are independent random variables such that <math>\\textstyle E\\left( X_{i}\\right)  =0</math> and <math>\\textstyle E\\left(  \\left\\vert X_{i}\\right\\vert ^{p}\\right) <+\\infty</math>, <math>\\textstyle 1\\leq p<+\\infty</math>, then\n\n:<math> A_{p}E\\left(  \\left(  \\sum_{i=1}^{n}\\left\\vert X_{i}\\right\\vert ^{2}\\right) _{{}}^{p/2}\\right)  \\leq E\\left(  \\left\\vert \\sum_{i=1}^{n}X_{i}\\right\\vert ^{p}\\right)  \\leq B_{p}E\\left(  \\left(  \\sum_{i=1}^{n}\\left\\vert X_{i}\\right\\vert ^{2}\\right)  _{{}}^{p/2}\\right) </math>\n\nwhere <math>\\textstyle A_{p}</math> and <math>\\textstyle B_{p}</math> are positive constants, which depend only on <math>\\textstyle p</math> and not on the underlying distribution of the random variables involved.\n\n==The second-order case==\n\nIn the case <math>\\textstyle p=2</math>, the inequality holds with <math>\\textstyle A_{2}=B_{2}=1</math>, and it reduces to the rule for the sum of variances of independent random variables with zero mean, known from elementary statistics: If <math>\\textstyle E\\left( X_{i}\\right)  =0</math> and <math>\\textstyle E\\left(  \\left\\vert X_{i}\\right\\vert ^{2}\\right) <+\\infty</math>, then\n\n:<math> \\mathrm{Var}\\left(\\sum_{i=1}^{n}X_{i}\\right)=E\\left(  \\left\\vert \\sum_{i=1}^{n}X_{i}\\right\\vert ^{2}\\right)  =\\sum_{i=1}^{n}\\sum_{j=1}^{n}E\\left( X_{i}\\overline{X}_{j}\\right)  =\\sum_{i=1}^{n}E\\left(  \\left\\vert X_{i}\\right\\vert ^{2}\\right)  =\\sum_{i=1}^{n}\\mathrm{Var}\\left(X_{i}\\right). </math>\n\n==See also==\nSeveral similar moment inequalities are known as [[Khintchine inequality]] and [[Rosenthal inequalities]], and there are also extensions to more general symmetric [[statistic]]s of independent random variables.<ref name=\"Ibragimov-1999-AKM\">R. Ibragimov and Sh. Sharakhmetov. Analogues of Khintchine, Marcinkiewicz&ndash;Zygmund and Rosenthal inequalities for symmetric statistics. ''Scandinavian Journal of Statistics'', 26(4):621&ndash;633, 1999.</ref>\n\n==Notes==\n<references/>\n\n{{DEFAULTSORT:Marcinkiewicz-Zygmund inequality}}\n[[Category:Statistical inequalities]]\n[[Category:Probabilistic inequalities]]\n[[Category:Probability theorems]]\n[[Category:Theorems in functional analysis]]"
    },
    {
      "title": "Markov's inequality",
      "url": "https://en.wikipedia.org/wiki/Markov%27s_inequality",
      "text": "[[File:Markov Inequality.svg|thumb|300px|right|Markov's inequality gives an upper bound for the measure of the set (indicated in red) where <math>f(x)</math> exceeds a given level <math>\\varepsilon</math>. The bound combines the level <math>\\varepsilon</math> with the average value of <math>f</math>.]]\nIn [[probability theory]], '''Markov's inequality''' gives an [[upper bound]] for the [[probability]] that a [[non-negative]] [[function (mathematics)|function]] of a [[random variable]] is greater than or equal to some positive [[Constant (mathematics)|constant]].  It is named after the Russian mathematician [[Andrey Markov]], although it appeared earlier in the work of [[Pafnuty Chebyshev]] (Markov's teacher), and many sources, especially in [[Mathematical analysis|analysis]], refer to it as [[Chebyshev's inequality]] (sometimes, calling it the first Chebyshev inequality, while referring to [[Chebyshev's inequality]] as the second Chebyshev's inequality) or [[Irénée-Jules Bienaymé|Bienaymé]]'s inequality.\n\nMarkov's inequality (and other similar inequalities) relate probabilities to [[expected value|expectation]]s, and provide (frequently loose but still useful) bounds for the [[cumulative distribution function]] of a random variable.\n\n==Statement==\nIf {{mvar|X}} is a nonnegative random variable and {{math|''a''&nbsp;>&nbsp;0}}, then the probability\nthat {{mvar|X}} is at least {{mvar|a}} is at most the expectation of {{mvar|X}} divided by {{mvar|a}}:<ref name=\"ProbabilityCourse\">{{cite web\n |title=Markov and Chebyshev Inequalities\n |url=https://www.probabilitycourse.com/chapter6/6_2_2_markov_chebyshev_inequalities.php\n |accessdate=4 February 2016\n}}</ref>\n\n:<math>\\operatorname{P}(X \\geq a) \\leq \\frac{\\operatorname{E}(X)}{a}.</math>\n\nLet <math> a = \\tilde{a} \\cdot \\operatorname{E}(X) </math> (where <math> \\tilde{a} > 0 </math>); then we can rewrite the previous inequality as\n\n:<math>\\operatorname{P}(X \\geq \\tilde{a} \\cdot \\operatorname{E}(X)) \\leq \\frac{1}{\\tilde{a}}.</math>\n\nIn the language of [[measure theory]], Markov's inequality states that if {{math|(''X'',&nbsp;Σ,&nbsp;''μ'')}} is a [[Measure (mathematics)|measure space]], {{mvar|f}} a [[measurable function|measurable]] [[extended real number line|extended real]]-valued function, and {{math|''&epsilon;'' > 0}}, then\n\n:<math> \\mu(\\{x\\in X:|f(x)|\\geq \\varepsilon \\}) \\leq \\frac 1 \\varepsilon \\int_X |f|\\,d\\mu.</math>\n\nThis measure-theoretic definition is sometimes referred to as [[Chebyshev's inequality]].<ref>{{citation| author1-first= E. M. | author1-last= Stein| author2-first=R. | author2-last=Shakarchi | author1-link= Elias M. Stein | title=Real Analysis |  series=[[Princeton Lectures in Analysis]] | volume=3 | edition= 1st | year= 2005| page= 91}}.</ref>\n\n===Extended version for monotonically increasing functions===\nIf {{mvar|&phi;}} is a [[monotonically increasing]] nonnegative function for the nonnegative reals, {{mvar|X}} is a random variable, {{math|''a'' &ge; 0}}, and {{math|''&phi;''(''a'') > 0}}, then\n\n:<math>\\operatorname P (|X| \\ge a) \\le \\frac{\\operatorname E(\\varphi(|X|))}{\\varphi(a)}.</math>\n\nAn immediate corollary, using higher moments of {{mvar|X}} supported on values larger than 0, is\n\n:<math>\\operatorname P (X \\ge a) \\le \\frac{\\operatorname E(X^n)}{a^n}.</math>\n\n==Proofs==\nWe separate the case in which the measure space is a probability space from the more general case because the probability case is more accessible for the general reader.\n\n===Intuitive===\n<math>\\operatorname{E}(X) = \\operatorname{P}(X < a)\\cdot\\overline{a} + \\operatorname{P}(X \\geq a)\\cdot a</math>, where <math>a</math> and <math>\\overline{a}</math> cover all possible values of <math>X</math> and \"''P(X...)•...''\" is meant to include integral expressions, too.\n\n<math>\\operatorname{E}[X]=0\\cdot\\overline{a} + \\dfrac{E[X]}{a}\\cdot a</math>, shows lower and upper bounds on <math>\\operatorname{P}(X < a)</math> and <math>\\operatorname{P}(X \\geq a)</math>, respectively, for which the equation is still true.\n\n===Proof in the language of probability theory===\n'''Method 1:'''\nFrom the definition of expectation: \n:<math>\\operatorname{E}(X)=\\int_{-\\infty}^{\\infty} xf(x) \\, dx</math>\n\nHowever, X is a non-negative random variable thus,  \n:<math>\\operatorname{E}(X)=\\int_{-\\infty}^\\infty xf(x) \\, dx = \\int_0^\\infty xf(x) \\, dx </math>\n\nFrom this we can derive, \n:<math>\\operatorname{E}(X)=\\int_0^a xf(x) \\, dx + \\int_a^\\infty xf(x) \\, dx \\ge  \\int_a^\\infty xf(x) \\, dx \\ge\\int_a^\\infty af(x) \\, dx = a\\int_a^\\infty f(x) \\, dx= a \\operatorname{Pr}(X \\ge a)</math>\n\nFrom here it is easy to see that \n:<math>\\Pr(X \\ge a) \\le \\operatorname{E}(X)/a</math>\n\n'''Method 2:'''\nFor any event <math>E</math>, let <math>I_E</math> be the indicator random variable of <math> E </math>, that is, <math>I_E=1</math> if <math>E</math> occurs and <math>I_E=0</math> otherwise.\n\nUsing this notation, we have <math>I_{(X\\geq a)}=1</math> if the event <math>X\\geq a</math> occurs, and <math>I_{(X\\geq a)}=0</math> if <math>X<a</math>.  Then, given <math>a>0</math>,\n\n:<math>aI_{(X \\geq a)} \\leq X</math>\n\nwhich is clear if we consider the two possible values of <math>X\\geq a</math>.  If <math>X<a</math>, then <math>I_{(X\\geq a)}=0</math>, and so <math>a I_{(X\\geq a)}=0\\leq X</math>.  Otherwise, we have <math>X\\geq a</math>, for which <math>I_{X\\geq a}=1</math> and so <math>aI_{X\\geq a}=a\\leq X</math>.\n\nSince <math>\\operatorname{E}</math> is a monotonically increasing function, taking expectation of both sides of an inequality cannot reverse it. Therefore,\n\n:<math>\\operatorname{E}(aI_{(X \\geq a)}) \\leq \\operatorname{E}(X).</math>\n\nNow, using linearity of expectations, the left side of this inequality is the same as\n\n:<math>a\\operatorname{E}(I_{(X \\geq a)}) = a(1\\cdot\\operatorname{P}(X \\geq a) + 0\\cdot\\operatorname{P}(X < a)) = a\\operatorname{P}(X \\geq a).</math>\n\nThus we have\n\n:<math>a\\operatorname{P}(X \\geq a) \\leq \\operatorname{E}(X)</math>\n\nand since ''a''&nbsp;>&nbsp;0, we can divide both sides by&nbsp;''a''.\n\n===In the language of measure theory===\nWe may assume that the function <math>f</math> is non-negative, since only its absolute value enters in the equation. Now, consider the real-valued function ''s'' on ''X'' given by\n\n:<math>\ns(x) =\n\\begin{cases}\n  \\varepsilon, & \\text{if } f(x) \\geq \\varepsilon  \\\\\n  0, & \\text{if } f(x) < \\varepsilon\n\\end{cases}\n</math>\n\nThen <math>0\\leq s(x)\\leq f(x)</math>. By the definition of the [[Lebesgue integral]]\n\n:<math>\n\\int_X f(x) \\, d\\mu \\geq \\int_X s(x) \\, d \\mu = \\varepsilon \\mu( \\{ x\\in X : \\, f(x) \\geq \\varepsilon \\} )\n</math>\n\nand since <math>\\varepsilon >0 </math>, both sides can be divided by <math>\\varepsilon</math>, obtaining\n\n:<math>\\mu(\\{x\\in X : \\, f(x) \\geq \\varepsilon \\}) \\leq {1\\over \\varepsilon }\\int_X f \\,d\\mu.</math>\n\n==Corollaries==\n\n=== Chebyshev's inequality ===\n[[Chebyshev's inequality]] uses the [[variance]] to bound the probability that a random variable deviates far from the mean. Specifically, \n\n:<math>\\operatorname{P}(|X-\\operatorname{E}(X)| \\geq a) \\leq \\frac{\\operatorname{Var}(X)}{a^2},</math>\n\nfor any {{math|''a'' > 0}}. Here {{math|Var(''X'')}} is the [[variance]] of X, defined as:\n\n:<math> \\operatorname{Var}(X) = \\operatorname{E}[(X - \\operatorname{E}(X) )^2]. </math>\n\nChebyshev's inequality follows from Markov's inequality by considering the random variable\n\n: <math> (X - \\operatorname{E}(X))^2 </math>\n\nand the constant <math>a^2,</math> for which Markov's inequality reads\n\n: <math> \\operatorname{P}( (X - \\operatorname{E}(X))^2 \\ge a^2) \\le \\frac{\\operatorname{Var}(X)}{a^2}.</math>\n\nThis argument can be summarized (where \"MI\" indicates use of Markov's inequality):\n\n:<math>\\operatorname{P}(|X-\\operatorname{E}(X)| \\geq a) = \n\\operatorname{P}\\left((X-\\operatorname{E}(X))^2 \\geq a^2\\right) \\,\\overset{\\underset{\\mathrm{MI}}{}}{\\leq}\\, \n\\frac {\\operatorname{E} \\left( (X-\\operatorname{E}(X))^2 \\right)}{a^2} =\n\\frac{\\operatorname{Var}(X)}{a^2}.</math>\n\n===Other corollaries===\n#The \"monotonic\" result can be demonstrated by:\n#:<math>\\operatorname P (|X| \\ge a) = \\operatorname P \\big(\\varphi(|X|) \\ge \\varphi(a)\\big) \\,\\overset{\\underset{\\mathrm{MI}}{}}{\\leq}\\, \n\\frac{\\operatorname E(\\varphi(|X|))}{\\varphi(a)}</math>\n#:\n#The result that, for a nonnegative random variable {{mvar|X}},  the [[quantile function]] of {{mvar|X}} satisfies:\n#:<math>Q_X(1-p) \\leq \\frac {\\operatorname E(X)}{p},</math>\n#:the proof using\n#:<math>p \\leq \\operatorname P(X \\geq Q_X(1-p)) \\,\\overset{\\underset{\\mathrm{MI}}{}}{\\leq}\\, \\frac {\\operatorname E(X)}{Q_X(1-p)}.</math>\n#:\n#Let <math> M \\succeq 0 </math> be a self-adjoint matrix-valued random variable and {{math|''a'' > 0}}. Then\n#:<math>\n\\operatorname{P}(M \\npreceq a \\cdot I) \\leq \\frac{\\operatorname{tr}\\left( E(M) \\right)}{n a}.\n</math>\n#:can be shown in a similar manner.\n\n==Examples==\nAssuming no income is negative, Markov's inequality shows that no more than 1/5 of the population can have more than 5 times the average income.\n\n==See also==\n* [[Paley–Zygmund inequality]] – a corresponding lower bound \n* [[Concentration inequality]] – a summary of tail-bounds on random variables.\n\n== References ==\n{{reflist}}\n\n==External links==\n*[http://mws.cs.ru.nl/mwiki/random_1.html#T36 The formal proof of Markov's inequality] in the [[Mizar system]].\n\n{{Refimprove|date=September 2010}}\n\n{{DEFAULTSORT:Markov's Inequality}}\n[[Category:Probabilistic inequalities]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "Multidimensional Chebyshev's inequality",
      "url": "https://en.wikipedia.org/wiki/Multidimensional_Chebyshev%27s_inequality",
      "text": "{{Unreferenced|date=August 2008}}\n\nIn [[probability theory]], the '''multidimensional Chebyshev's inequality''' is a generalization of [[Chebyshev's inequality]], which puts a bound on the probability of the event that a [[random variable]] differs from its [[expected value]] by more than a specified amount.\n\nLet ''X'' be an ''N''-dimensional [[random vector]] with [[expected value]] <math>\\mu=\\operatorname{E}[X] </math> and [[covariance matrix]]\n\n: <math>V=\\operatorname{E} [(X - \\mu) (X - \\mu)^T]. \\, </math>\n\nIf <math>V</math> is a [[positive-definite matrix]], for any [[real number]] <math>t>0</math>:\n:<math>\n\\Pr \\left( \\sqrt{( X-\\mu)^T  V^{-1} (X-\\mu) } > t\\right) \\le \\frac N {t^2}\n</math>\n\n==Proof==\nSince <math>V</math> is positive-definite, so is <math>V^{-1}</math>. Define the random variable\n\n:<math>\ny = (X-\\mu)^T V^{-1} (X-\\mu).\n</math>\n\nSince <math>y</math> is positive, [[Markov's inequality]] holds:\n\n:<math>\n\\Pr\\left( \\sqrt{(X-\\mu)^T V^{-1} (X-\\mu) } > t\\right) = \\Pr( \\sqrt{y} > t) = \\Pr(y > t^2)\n\\le \\frac{\\operatorname{E}[y]}{t^2}.\n</math>\n\nFinally,\n\n:<math>\\begin{align}\n\\operatorname{E}[y] &= \\operatorname{E}[(X-\\mu)^T V^{-1} (X-\\mu)]\\\\[6pt]\n&=\\operatorname{E}[ \\operatorname{trace} (  V^{-1} (X-\\mu) (X-\\mu)^T )]\\\\[6pt]\n&= \\operatorname{trace} (  V^{-1} V ) = N\n\\end{align}.</math>\n\n\n[[Category:Probabilistic inequalities]]\n[[Category:Statistical inequalities]]"
    },
    {
      "title": "Paley–Zygmund inequality",
      "url": "https://en.wikipedia.org/wiki/Paley%E2%80%93Zygmund_inequality",
      "text": "In [[mathematics]], the '''Paley–Zygmund inequality''' bounds the\nprobability that a positive random variable is small, in terms of\nits first two [[moment (mathematics)|moments]]. The inequality was\nproved by [[Raymond Paley]] and [[Antoni Zygmund]].\n\n'''Theorem''': If ''Z''&nbsp;≥&nbsp;0 is a [[random variable]] with\nfinite variance, and if <math>0 \\le \\theta \\le 1</math>, then\n\n:<math>\n\\operatorname{P}( Z > \\theta\\operatorname{E}[Z] )\n\\ge (1-\\theta)^2 \\frac{\\operatorname{E}[Z]^2}{\\operatorname{E}[Z^2]}.\n</math>\n\n'''Proof''': First, \n:<math>\n\\operatorname{E}[Z] = \\operatorname{E}[ Z \\, \\mathbf{1}_{\\{ Z \\le \\theta \\operatorname{E}[Z] \\}}]  + \\operatorname{E}[ Z \\, \\mathbf{1}_{\\{ Z > \\theta \\operatorname{E}[Z] \\}} ].\n</math>\nThe first addend is at most <math>\\theta \\operatorname{E}[Z]</math>, while the second is at most <math> \\operatorname{E}[Z^2]^{1/2} \\operatorname{P}( Z > \\theta\\operatorname{E}[Z])^{1/2} </math> by the [[Cauchy–Schwarz inequality]]. The desired inequality then follows. ∎\n\n== Related inequalities ==\n\nThe Paley–Zygmund inequality can be written as\n\n:<math>\n\\operatorname{P}( Z > \\theta \\operatorname{E}[Z] )\n\\ge \\frac{(1-\\theta)^2 \\, \\operatorname{E}[Z]^2}{\\operatorname{Var} Z + \\operatorname{E}[Z]^2}.\n</math>\n\nThis can be improved. By the [[Cauchy–Schwarz inequality]],\n\n:<math>\n\\operatorname{E}[Z - \\theta \\operatorname{E}[Z]]\n\\le \\operatorname{E}[ (Z - \\theta \\operatorname{E}[Z]) \\mathbf{1}_{\\{ Z > \\theta \\operatorname{E}[Z] \\}} ]\n\\le \\operatorname{E}[ (Z - \\theta \\operatorname{E}[Z])^2 ]^{1/2} \\operatorname{P}( Z > \\theta \\operatorname{E}[Z] )^{1/2}\n</math>\n\nwhich, after rearranging, implies that\n\n:<math>\n\\operatorname{P}(Z > \\theta \\operatorname{E}[Z])\n\\ge \\frac{(1-\\theta)^2 \\operatorname{E}[Z]^2}{\\operatorname{E}[( Z - \\theta \\operatorname{E}[Z] )^2]}\n= \\frac{(1-\\theta)^2 \\operatorname{E}[Z]^2}{\\operatorname{Var} Z + (1-\\theta)^2 \\operatorname{E}[Z]^2}.\n</math>\n\nThis inequality is sharp; equality is achieved if Z almost surely equals a positive constant.\n\n== References==\n\n*R. E. A. C. Paley and A. Zygmund, \"On some series of functions, (3),\" Proc. Camb. Phil. Soc. 28 (1932), 190–205, (cf. Lemma 19 page 192).\n*R. E. A. C. Paley and A. Zygmund, ''A note on analytic functions in the unit circle'', Proc. Camb. Phil. Soc. 28 (1932), 266–272\n\n{{DEFAULTSORT:Paley-Zygmund inequality}}\n[[Category:Probabilistic inequalities]]"
    },
    {
      "title": "Pinsker's inequality",
      "url": "https://en.wikipedia.org/wiki/Pinsker%27s_inequality",
      "text": "In [[information theory]], '''Pinsker's inequality''', named after its inventor [[Mark Semenovich Pinsker]], is an [[inequality (mathematics)|inequality]] that bounds the [[total variation distance]] (or statistical distance) in terms of the [[Kullback–Leibler divergence]].\nThe inequality is tight up to constant factors.<ref>{{cite book|title=Information Theory: Coding Theorems for Discrete Memoryless Systems|first1=Imre|last1=Csiszár|first2=János|last2=Körner|publisher=Cambridge University Press|year=2011|isbn=9781139499989|page=44|url=https://books.google.com/books?id=2gsLkQlb8JAC&pg=PA44}}</ref>\n\n==Formal statement==\nPinsker's inequality states that, if <math>P</math> and <math>Q</math> are two [[probability distribution]]s on a [[measurable space]] <math>(X, \\Sigma)</math>, then\n\n:<math>\\delta(P,Q) \\le \\sqrt{\\frac{1}{2} D_{\\mathrm{KL}}(P\\|Q)},</math>\n\nwhere\n\n:<math>\\delta(P,Q)=\\sup \\bigl\\{ |P(A) - Q(A)| \\big| A \\in \\Sigma \\text{ is a measurable event} \\bigr\\}</math>\n\nis the [[Total variation distance of probability measures|total variation distance]] (or statistical distance) between <math>P</math> and <math>Q</math> and\n\n:<math>D_{\\mathrm{KL}}(P\\|Q) = \\operatorname{E}_P \\left( \\log \\frac{\\mathrm{d} P}{\\mathrm{d} Q} \\right) = \\int_X \\left( \\log \\frac{\\mathrm{d} P}{\\mathrm{d} Q} \\right) \\, \\mathrm{d} P</math>\n\nis the [[Kullback–Leibler divergence]] in [[Nat (unit)|nats]].  When the sample space <math>X</math> is a finite set, the Kullback–Leibler divergence is given by\n\n: <math>D_{\\mathrm{KL}}(P\\|Q) = \\sum_{i \\in X} \\left( \\log \\frac{P(i)}{Q(i)}\\right) P(i)\\!</math>\n\nNote that in terms of the [[Total_variation#Total_variation_of_a_measure|total variation norm]] <math>\\| P - Q \\|</math> of the [[signed measure]] <math>P - Q</math>, Pinsker's inequality differs from the one given above by a factor of two:\n:<math>\\| P - Q \\| \\le \\sqrt{2 D_{\\mathrm{KL}}(P\\|Q)}.</math>\n\nA proof of Pinsker's inequality uses the [[partition inequality]] for [[f-divergence|''f''-divergences]].\n\n==History==\nPinsker first proved the inequality with a worse constant. The inequality in the above form was proved independently by [[Solomon Kullback|Kullback]], [[Imre Csiszár|Csiszár]], and [[Johannes Kemperman|Kemperman]].<ref>{{cite book|last=Tsybakov|first=Alexandre|title=Introduction to Nonparametric Estimation|year=2009|publisher=Springer|isbn=9780387790527|page=132}}</ref>\n\n==Inverse problem==\nA precise inverse of the inequality cannot hold:  for every <math>\\varepsilon > 0</math>, there are distributions <math>P_\\varepsilon, Q</math> with <math>\\delta(P_\\varepsilon,Q)\\le\\varepsilon</math> but <math>D_{\\mathrm{KL}}(P_\\varepsilon\\|Q) = \\infty</math>. An easy example is given by the two-point space <math>\\{0,1\\}</math> with <math>Q(0) = 0, Q(1) = 1</math> and <math>P_\\varepsilon(0) = \\varepsilon, P_\\varepsilon(1) = 1-\\varepsilon</math>. <ref>The divergence becomes infinite whenever one of the two distributions assigns probability zero to an event while the other assigns it a nonzero probability (no matter how small); see e.g. {{cite book|title=Data Complexity in Pattern Recognition|first1=Mitra|last1=Basu|first2=Tin Kam|last2=Ho|publisher=Springer|year=2006|isbn=9781846281723|page=161|url=https://books.google.com/books?id=GflBKbzym9oC&pg=PA161}}.</ref>\n\nHowever, an inverse inequality holds on finite spaces <math>X</math> with a constant depending on <math>Q</math>.<ref>see Lemma 4.1 in {{cite arXiv |last1=Götze |first1=Friedrich |last2=Sambale |first2=Holger |last3=Sinulis |first3=Arthur |title=Higher order concentration for functions of weakly dependent random variables |eprint=1801.06348}}</ref> More specifically, it can be shown that with the definition <math>\\alpha_Q := \\min_{x \\in X: Q(x) > 0} Q(x)</math> we have for any measure <math>P</math> which is absolutely continuous to <math>Q</math>\n: <math>\\frac{1}{2} D_{\\mathrm{KL}}(P\\|Q) \\le \\frac{1}{\\alpha_Q} \\delta(P,Q)^2. </math>\n\nAs a consequence, if <math>Q</math> has full [[Support_(measure_theory)|support]] (i.e. <math>Q(x) > 0</math> for all <math>x \\in X</math>), then\n\n: <math> \\delta(P,Q)^2 \\le \\frac{1}{2} D(P\\|Q) \\le \\frac{1}{\\alpha_Q} \\delta(P,Q)^2. </math>\n\n==References==\n{{Reflist}}\n\n==Further reading==\n* Thomas M. Cover and Joy A. Thomas: ''Elements of Information Theory'', 2nd edition, Willey-Interscience, 2006\n* Nicolo Cesa-Bianchi and Gábor Lugosi: ''Prediction, Learning, and Games'', Cambridge University Press, 2006\n\n[[Category:Information theory]]\n[[Category:Probabilistic inequalities]]"
    },
    {
      "title": "Ross's conjecture",
      "url": "https://en.wikipedia.org/wiki/Ross%27s_conjecture",
      "text": "In [[queueing theory]], a discipline within the mathematical theory of probability, '''Ross's conjecture''' gives a lower bound for the average waiting-time experienced by a customer when arrivals to the queue do not follow the simplest model for random arrivals.  It was proposed by Sheldon M. Ross in 1978 and proved in 1981 by Tomasz Rolski.<ref name=\"rolski\" /> Equality can be obtained in the bound; and the bound does not hold for finite buffer queues.<ref>{{citation\n | last = Heyman | first = D. P.\n | issue = 1\n | journal = Journal of Applied Probability\n | jstor = 3213936\n | mr = 644439\n | pages = 245–249\n | title = On Ross's conjectures about queues with non-stationary Poisson arrivals\n | volume = 19\n | year = 1982\n | doi = 10.2307/3213936}}.</ref>\n\n==Bound==\n\nRoss's conjecture is a bound for the mean delay in a queue where arrivals are governed by a [[doubly stochastic Poisson process]] <ref>{{citation\n | last = Huang | first = J.\n | issue = 1\n | journal = Ph.D Dissertation\n | title = A Study on Queuing Theory and Teletraffic Models (Part 1 of 3)\n | year = 1991\n | doi = 10.13140/RG.2.1.1259.6329 }}.</ref>\nor by a non-stationary [[Poisson process]].<ref name=\"rolski\">{{citation\n | last = Rolski | first = Tomasz\n | doi = 10.2307/1426787\n | issue = 3\n | journal = Advances in Applied Probability\n | jstor = 1426787\n | mr = 615953\n | pages = 603–618\n | title = Queues with non-stationary input stream: Ross's conjecture\n | volume = 13\n | year = 1981}}.</ref><ref name=\"ross\">{{citation\n | last = Ross | first = Sheldon M.\n | issue = 3\n | journal = Journal of Applied Probability\n | jstor = 3213122\n | mr = 0483101\n | pages = 602–609\n | title = Average delay in queues with non-stationary Poisson arrivals\n | volume = 15\n | year = 1978\n | doi = 10.2307/3213122}}.</ref> The conjecture states that the average amount of time that a customer spends waiting in a queue is greater than or equal to\n::<math>\\frac{\\lambda \\operatorname E (S^2)}{2 \\{1-\\lambda \\operatorname E (S) \\}}</math>\nwhere ''S'' is the service time and &lambda; is the average arrival rate (in the limit as the length of the time period increases).<ref name=\"rolski\" />\n\n==References==\n\n{{Reflist}}\n\n[[Category:Probabilistic inequalities]]\n[[Category:Queueing theory]]\n\n\n{{probability-stub}}"
    },
    {
      "title": "Sanov's theorem",
      "url": "https://en.wikipedia.org/wiki/Sanov%27s_theorem",
      "text": "{{multiple issues|\n{{context|date=February 2012}}\n{{no footnotes|date=February 2012}}\n}}\n\nIn [[information theory]], '''Sanov's theorem''' gives a bound on the probability of observing an [[Typical set|atypical]] sequence of samples from a given [[probability distribution]]. In the language of [[large deviations theory]], Sanov's theorem identifies the [[rate function]] for large deviations of the [[Empirical measure|empirical measure]] of a sequence of [[Independent and identically distributed random variables|i.i.d.]] random variables. \n\nLet ''A'' be a set of probability distributions over an alphabet ''X'', and let ''q'' be an arbitrary distribution over ''X'' (where ''q'' may or may not be in ''A'').  Suppose we draw ''n'' i.i.d. samples from ''q'', represented by the vector <math>x^n = x_1, x_2, \\ldots, x_n</math>.  Further, let us ask that the empirical measure, <math>\\hat{p}_{x^n}</math>, of the samples falls within the set ''A''—formally, we write <math>\\{x^n : \\hat{p}_{x^n} \\in A\\}</math>. Then,\n\n:<math>q^n(x^n) \\le (n+1)^{|X|} 2^{-nD_{\\mathrm{KL}}(p^*||q)}</math>,\n\nwhere\n* <math>q^n(x^n)</math> is shorthand for <math>q(x_1)q(x_2) \\cdots q(x_n)</math>, and\n* <math>p^*</math> is the [[information projection]] of ''q'' onto ''A''.\n\nIn words, the probability of drawing an atypical distribution is a function of the [[Kullback–Leibler divergence|KL divergence]] from the true distribution to the atypical one; in the case that we consider a set of possible atypical distributions, there is a dominant atypical distribution, given by the information projection.\n\nFurthermore, if ''A'' is the [[Closure (topology)|closure]] of its [[Interior (topology)|interior]],\n\n:<math>\\lim_{n\\to\\infty}\\frac{1}{n}\\log q^n(x^n) = -D_{\\mathrm{KL}}(p^*||q).</math>\n\n==References==\n\n*{{Cite book\n  | last1 = Cover  | first1 = Thomas M.\n  | last2 = Thomas | first2 = Joy A.\n  | title = Elements of Information Theory\n  | publisher = Wiley Interscience\n  | edition = 2\n  | date = 2006\n  | location = Hoboken, New Jersey\n  | pages = 362}}\n\n*Sanov, I. N. (1957) \"On the probability of large deviations of random variables\". ''Mat. Sbornik'' 42(84), No. 1, 11–44.\n*Санов, И. Н. (1957) \"О вероятности больших отклонений случайных величин\". ''МАТЕМАТИЧЕСКИЙ СБОРНИК' 42(84), No. 1, 11–44.\n\n[[Category:Information theory]]\n[[Category:Probabilistic inequalities]]\n\n\n{{probability-stub}}"
    },
    {
      "title": "Second moment method",
      "url": "https://en.wikipedia.org/wiki/Second_moment_method",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|Method in probability theory}}\n{{Cleanup|date=January 2009}}\n\nIn mathematics, the '''second moment method''' is a technique used in [[probability theory]] and [[analysis]] to show that a [[random variable]] has positive probability of being positive. More generally, the \"moment method\" consists of bounding the probability that a random variable fluctuates far from its mean, by using its moments.<ref>{{cite web |url=http://terrytao.wordpress.com/2008/06/18/the-strong-law-of-large-numbers/ |title=The strong law of large numbers|author=[[Terence Tao]]|accessdate=2009-02-10 |work=What’s new?|date=2008-06-18 }}</ref>\n\nThe method is often quantitative, in that one can often deduce a lower bound on the probability that the random variable is larger than some constant times its expectation. The method involves comparing the second [[moment (mathematics)|moment]] of random variables to the square of the first moment.\n\n==First moment method==\nThe first moment method is a simple application of [[Markov's inequality]] for integer-valued variables.  For a '''non-negative''', '''integer-valued''' random variable ''X'', we may want to prove that ''X'' = 0  with high probability. To obtain an upper bound for P(''X'' > 0), and thus a lower bound for P(''X'' = 0), we first note that since ''X'' takes only integer values, P(''X'' > 0) = P(''X'' ≥ 1).  Since ''X'' is non-negative we can now apply [[Markov's inequality]] to obtain P(''X'' ≥ 1) ≤ E[''X'']. Combining these we have P(''X'' > 0) ≤ E[''X'']; the first moment method is simply the use of this inequality.\n\n==Second moment method==\nIn the other direction, E[''X''] being \"large\" does not directly imply that P(''X'' = 0) is small. However, we can often use the second moment to derive such a conclusion, using [[Cauchy–Schwarz inequality]].\n\n'''Theorem''': If ''X''&nbsp;≥&nbsp;0 is a [[random variable]] with\nfinite variance, then\n\n:<math>\n\\operatorname{P}( X > 0 )\n\\ge \\frac{(\\operatorname{E}[X])^2}{\\operatorname{E}[X^2]}.\n</math>\n\n'''Proof''': Using the [[Cauchy–Schwarz inequality]], we have\n:<math>\n\\operatorname{E}[X] = \\operatorname{E}[ X \\, \\mathbf{1}_{\\{ X > 0 \\}} ] \\le \\operatorname{E}[X^2]^{1/2} \\operatorname{P}( X > 0)^{1/2}.\n</math>\nSolving for <math>\\operatorname{P}( X > 0)</math>, the desired inequality then follows. ∎\n\nThe method can also be used on distributional limits of random variables. Furthermore, the estimate of the previous theorem can be refined by means of the so-called  [[Paley–Zygmund inequality]]. Suppose that ''X<sub>n</sub>'' is a sequence of non-negative real-valued random variables which [[converge in law]] to a random variable ''X''. If there are finite positive constants ''c''<sub>1</sub>, ''c''<sub>2</sub> such that\n\n:<math>E \\left [X_n^2 \\right ] \\le c_1 E[X_n]^2</math>\n:<math>E \\left [X_n \\right ]\\ge c_2</math>\n\nhold for every ''n'', then it follows from the [[Paley–Zygmund inequality]] that for every ''n'' and θ in (0, 1)\n\n:<math> P (X_n \\geq c_2 \\theta) \\geq \\frac{(1-\\theta)^2}{c_1}.</math>\n\nConsequently, the same inequality is satisfied by ''X''.\n\n==Example application of method==\n\n===Setup of problem===\nThe [[Bernoulli bond percolation]] [[Glossary of graph theory#Subgraphs|subgraph]] of a graph ''G'' at parameter ''p'' is a random subgraph obtained from ''G'' by deleting every edge of ''G'' with probability 1−''p'', independently. The [[binary tree|infinite complete binary tree]] ''T'' is an infinite [[tree (graph theory)|tree]] where one vertex (called the root) has two neighbors and every other vertex has three neighbors. The second moment method can be used to show that at every parameter ''p'' ∈ (1/2, 1] with positive probability the connected component of the root in the percolation subgraph of ''T'' is infinite.\n\n===Application of method===\nLet ''K'' be the percolation component of the root, and let ''T<sub>n</sub>'' be the set of vertices of ''T'' that are at distance ''n'' from the root. Let ''X<sub>n</sub>'' be the number of vertices in ''T<sub>n</sub>'' ∩ ''K''. To prove that ''K'' is infinite with positive probability, it is enough to show that <math>\\limsup_{n\\to\\infty}1_{X_n>0}>0</math> with positive probability. By the [[reverse Fatou lemma]], it suffices to show that <math>\\inf_{n\\to\\infty}P(X_n>0)>0</math>. The [[Cauchy–Schwarz inequality]] gives\n:<math>E[X_n]^2\\le E[X_n^2] \\, E\\left [(1_{X_n>0})^2\\right ]=E[X_n^2]\\,P(X_n>0).</math>\nTherefore, it is sufficient to show that\n:<math> \\inf_n \\frac{E \\left[ X_n \\right ]^2}{E \\left[ X_n^2 \\right ]}>0\\,,</math>\nthat is, that the second moment is bounded from above by a constant times the first moment squared (and both are nonzero). In many applications of the second moment method, one is not able to calculate the moments precisely, but can nevertheless establish this inequality.\n\nIn this particular application, these moments can be calculated. For every specific ''v'' in ''T<sub>n</sub>'',\n\n:<math>P(v\\in K)=p^n. </math>\n\nSince <math>|T_n|=2^n</math>, it follows that\n\n:<math>E[X_n]=2^n\\,p^n</math>\n\nwhich is the first moment. Now comes the second moment calculation.\n\n:<math>E\\!\\left[X_n^2 \\right ] = E\\!\\left[\\sum_{v\\in T_n} \\sum_{u\\in T_n}1_{v\\in K}\\,1_{u\\in K}\\right] = \\sum_{v\\in T_n} \\sum_{u\\in T_n} P(v,u\\in K).</math>\n\nFor each pair ''v'', ''u'' in ''T<sub>n</sub>'' let ''w(v, u)'' denote the vertex in ''T'' that is farthest away from the root and lies on the simple path in ''T'' to each of the two vertices ''v'' and ''u'', and let ''k(v, u)'' denote the distance from ''w'' to the root. In order for ''v'', ''u'' to both be in ''K'', it is necessary and sufficient for the three simple paths from ''w(v, u)'' to ''v'', ''u'' and the root to be in ''K''. Since the number of edges contained in the union of these three paths is 2''n'' − ''k(v, u)'', we obtain\n\n:<math>P(v,u\\in K)= p^{2n-k(v,u)}.</math>\n\nThe number of pairs ''(v, u)'' such that ''k(v, u)'' = ''s'' is equal to <math>2^s\\,2^{n-s}\\,2^{n-s-1}=2^{2n-s-1}</math>, for ''s'' = 0, 1, ..., ''n''. Hence,\n\n:<math>E\\!\\left[X_n^2 \\right ] = \\sum_{s=0}^n 2^{2n-s-1} p^{2n-s} = \\frac 12\\,(2p)^n \\sum_{s=0}^n (2p)^s = \\frac12\\,(2p)^n \\, \\frac{(2p)^{n+1}-1}{2p-1} \\le \\frac p{2p-1} \\,E[X_n]^2,</math>\n\nwhich completes the proof.\n\n===Discussion===\n*The choice of the random variables ''X''<sub>''n''</sub> was rather natural in this setup. In some more difficult applications of the method, some ingenuity might be required in order to choose the random variables ''X''<sub>''n''</sub> for which the argument can be carried through.\n*The [[Paley–Zygmund inequality]] is sometimes used instead of the [[Cauchy–Schwarz inequality]] and may occasionally give more refined results.\n*Under the (incorrect) assumption that the events ''v'', ''u'' in ''K'' are always independent, one has <math>P(v,u\\in K)=P(v\\in K)\\,P(u\\in K)</math>, and the second moment is equal to the first moment squared. The second moment method typically works in situations in which the corresponding events or random variables are “nearly independent\".\n*In this application, the random variables ''X''<sub>''n''</sub> are given as sums\n:: <math>X_n=\\sum_{v\\in T_n}1_{v\\in K}.</math>\n: In other applications, the corresponding useful random variables are [[integral]]s\n:: <math>X_n=\\int f_n(t)\\,d\\mu(t),</math>\n: where the functions ''f''<sub>''n''</sub> are random. In such a situation, one considers the product measure ''&mu;''&nbsp;&times;&nbsp;''&mu;'' and calculates\n:: <math> \\begin{align}\nE \\left[X_n^2 \\right ] & = E\\left[\\int\\int f_n(x)\\,f_n(y)\\,d\\mu(x)\\,d\\mu(y)\\right ] \\\\\n& = E\\left[ \\int\\int E\\left[f_n(x)\\,f_n(y)\\right]\\,d\\mu(x)\\,d\\mu(y)\\right ],\n\\end{align}</math>\n:where the last step is typically justified using [[Fubini's theorem]].\n\n==References==\n*{{Citation | last1=Burdzy | first1=Krzysztof | last2=Adelman | first2=Omer | last3=Pemantle | first3=Robin | title=Sets avoided by Brownian motion | hdl=1773/2194  | year=1998 | journal=Annals of Probability | volume=26 | issue=2 | pages=429–464 | doi=10.1214/aop/1022855639| arxiv=math/9701225 }}\n*{{Citation | last1=Lyons | first1=Russell | title=Random walk, capacity, and percolation on trees | year=1992 | journal=Annals of Probability | volume=20 | issue=4 | pages=2043–2088 | doi=10.1214/aop/1176989540}}\n*{{Citation | last1=Lyons | first1=Russell | last2=Peres | first2=Yuval | title=Probability on trees and networks | url=http://mypage.iu.edu/~rdlyons/prbtree/prbtree.html}}\n<references/>\n\n[[Category:Probabilistic inequalities]]\n[[Category:Articles containing proofs]]\n[[Category:Moment (mathematics)]]"
    },
    {
      "title": "Talagrand's concentration inequality",
      "url": "https://en.wikipedia.org/wiki/Talagrand%27s_concentration_inequality",
      "text": "{{context|date=September 2011}}\n\nIn [[probability theory]], '''Talagrand's concentration inequality''' is an [[isoperimetric]]-type [[inequality (mathematics)|inequality]] for [[product space|product]] [[probability space]]s.<ref>{{Cite book | last1=Alon | first1=Noga | last2=Spencer | first2=Joel H. | title=The Probabilistic Method | edition=2nd | year=2000 | publisher=John Wiley & Sons, Inc. | isbn=0-471-37046-0}}</ref><ref name=ledoux>{{cite book | last       = Ledoux | first      = Michel   | title      = The Concentration of Measure Phenomenon | publisher  = American Mathematical Society | year       = 2001 | isbn         = 0-8218-2864-9}}</ref> It was first proved by the French mathematician [[Michel Talagrand]].<ref>{{Cite book | last1=Talagrand | first1=Michel | title=Concentration of measure and isoperimetric inequalities in product spaces | journal=Publications Mathématiques de l'IHÉS| year=1995 | url=http://www.springerlink.com/content/q2533680323tm2j8/ | publisher=Springer-Verlag | issn=0073-8301|doi=10.1007/BF02699376}}</ref> The inequality is one of the manifestations of the [[concentration of measure]] phenomenon.<ref name=ledoux/>\n\n==Statement==\nThe inequality states that if <math>\\Omega = \\Omega_1 \\times \\Omega_2 \\times \\cdots \\times \\Omega_n</math> is a [[product space]] endowed with a [[product measure|product probability measure]] and <math>A</math>\nis a subset in this space, then for any <math>t \\ge 0</math>\n\n: <math>\\Pr[A] \\cdot \\Pr\\left[{A^c_t}\\right] \\le e^{-t^2/4} \\, ,</math>\n\nwhere <math>{A^c_t}</math> is the complement of ''<math>A_{t}</math>'' where this is defined by\n:<math>A_t = \\{ x \\in \\Omega ~:~ \\rho(A,x) \\le t \\}</math>\nand where <math>\\rho</math> is Talagrand's convex distance defined as\n\n: <math>\\rho(A,x) = \\max_{\\alpha, \\|\\alpha\\|_2 \\le 1} \\ \\min_{y \\in A} \\ \\sum_{i~:~x_i \\neq y_i} \\alpha_i</math>\n\nwhere <math>\\alpha \\in \\mathbf{R}^n</math>, <math>x,y \\in \\Omega</math> are <math>n</math>-dimensional vectors with entries\n<math>\\alpha_i, x_i, y_i</math> respectively and <math>\\|\\cdot\\|_2</math> is the <math>\\ell^2</math>-norm.  That is,\n\n:<math>\\|\\alpha\\|_2=\\left(\\sum_i\\alpha_i^2\\right)^{1/2}</math>\n\n==References==\n\n{{Reflist}}\n\n{{DEFAULTSORT:Talagrand's Concentration Inequality}}\n[[Category:Probabilistic inequalities]]\n[[Category:Measure theory]]\n\n\n{{probability-stub}}"
    },
    {
      "title": "Vitale's random Brunn–Minkowski inequality",
      "url": "https://en.wikipedia.org/wiki/Vitale%27s_random_Brunn%E2%80%93Minkowski_inequality",
      "text": "In [[mathematics]], '''Vitale's random Brunn–Minkowski inequality''' is a [[theorem]] due to [[Richard Vitale]] that generalizes the classical [[Brunn–Minkowski inequality]] for [[compact space|compact subsets]] of ''n''-[[dimension]]al [[Euclidean space]] '''R'''<sup>''n''</sup> to [[random compact set]]s.\n\n==Statement of the inequality==\n\nLet ''X'' be a random compact set in '''R'''<sup>''n''</sup>; that is, a [[Borel sigma algebra|Borel]]&ndash;[[measurable function]] from some [[probability space]] (&Omega;,&nbsp;&Sigma;,&nbsp;Pr) to the space of [[non-empty]], [[compact space|compact]] [[subset]]s of '''R'''<sup>''n''</sup> equipped with the [[Hausdorff metric]]. A [[random vector]] ''V''&nbsp;:&nbsp;&Omega;&nbsp;&rarr;&nbsp;'''R'''<sup>''n''</sup> is called a selection of ''X'' if Pr(''V''&nbsp;&isin;&nbsp;''X'')&nbsp;=&nbsp;1. If ''K'' is a non-empty, compact subset of '''R'''<sup>''n''</sup>, let\n\n:<math>\\| K \\| = \\max \\left\\{ \\left. \\| v \\|_{\\mathbb{R}^{n}} \\right| v \\in K \\right\\}</math>\n\nand define the set-valued [[expected value|expectation]] E[''X''] of ''X'' to be\n\n:<math>\\mathrm{E} [X] = \\{ \\mathrm{E} [V] | V \\mbox{ is a selection of } X \\mbox{ and } \\mathrm{E} \\| V \\| < + \\infty \\}.</math>\n\nNote that E[''X''] is a subset of '''R'''<sup>''n''</sup>. In this notation, Vitale's random Brunn–Minkowski inequality is that, for any random compact set ''X'' with <math>E[\\|X\\|]<+\\infty</math>,\n\n:<math>\\left( \\mathrm{vol}_n \\left( \\mathrm{E} [X] \\right) \\right)^{1/n} \\geq \\mathrm{E} \\left[ \\mathrm{vol}_n (X)^{1/n} \\right],</math>\n\nwhere \"<math>vol_n</math>\" denotes ''n''-dimensional [[Lebesgue measure]].\n\n==Relationship to the Brunn–Minkowski inequality==\n\nIf ''X'' takes the values (non-empty, compact sets) ''K'' and ''L'' with probabilities 1&nbsp;&minus;&nbsp;''&lambda;'' and ''&lambda;'' respectively, then Vitale's random Brunn–Minkowski inequality is simply the original Brunn–Minkowski inequality for compact sets.\n\n==References==\n\n* {{cite journal \n| last=Gardner \n| first=Richard J. \n| title=The Brunn-Minkowski inequality \n| journal=Bull. Amer. Math. Soc. (N.S.) \n| volume=39 \n| issue=3 \n| year=2002 \n| pages=355–405 (electronic) \n| url = http://www.ams.org/bull/2002-39-03/S0273-0979-02-00941-2/S0273-0979-02-00941-2.pdf \n| doi=10.1090/S0273-0979-02-00941-2\n}}\n* {{cite journal\n|     last = Vitale\n|    first = Richard A.\n|    title = The Brunn-Minkowski inequality for random sets\n|  journal = J. Multivariate Anal.\n|   volume = 33\n|    issue = 2\n|     year = 1990\n|    pages = 286–293\n|    doi = 10.1016/0047-259X(90)90052-J\n}}\n\n{{DEFAULTSORT:Vitale's random Brunn-Minkowski inequality}}\n[[Category:Probabilistic inequalities]]"
    },
    {
      "title": "Vysochanskij–Petunin inequality",
      "url": "https://en.wikipedia.org/wiki/Vysochanskij%E2%80%93Petunin_inequality",
      "text": "In [[probability theory]], the ''' Vysochanskij–[[Yuri Petunin|Petunin]] inequality ''' gives a lower bound for the [[probability]] that a [[random variable]] with finite [[variance]] lies within a certain number of [[standard deviation]]s of the variable's [[expected value|mean]], or equivalently an upper bound for the probability that it lies further away. The sole restrictions on the [[probability distribution|distribution]] are that it be [[unimodal function|unimodal]] and have finite [[variance]]. (This implies that it is a [[continuous probability distribution]] except at the [[mode (statistics)|mode]], which may have a non-zero probability.)\nThe theorem applies even to heavily skewed distributions and puts bounds on how much of the data is, or is not, \"in the middle.\"{{citation needed|date=March 2019}}\n\n== Theorem ==\n\nLet ''X'' be a random variable with unimodal distribution, mean &mu; and finite, non-zero variance&nbsp;&sigma;<sup>2</sup>. Then, for any &lambda;&nbsp;>&nbsp;&radic;(8/3)&nbsp;=&nbsp;1.63299...,\n\n:<math>P(\\left|X-\\mu\\right|\\geq \\lambda\\sigma)\\leq\\frac{4}{9\\lambda^2}.</math>\n\n(For a relatively elementary proof see e.g. <ref>[https://www.jstor.org/stable/pdf/2684253.pdf Pukelsheim, F., 1994. The Three Sigma Rule. ''The American Statistician'', 48(2), pp.88-91]</ref>). Furthermore, the equality is attained for a random variable having a probability 1&nbsp;&minus;&nbsp;4/(3&nbsp;λ<sup>2</sup>) of being exactly equal to the mean, and which, when it is not equal to the mean, is distributed uniformly in an interval centred on the mean. When &lambda; is less than &radic;(8/3), there exist [[Symmetric distribution|non-symmetric distributions]] for which the 4/(9&nbsp;λ<sup>2</sup>) bound is exceeded.\n\n== Properties ==\n\nThe theorem refines [[Chebyshev's inequality]] by including the factor of 4/9, made possible by the condition that the distribution be unimodal.\n\nIt is common, in the construction of [[control chart]]s and other statistical heuristics, to set&nbsp;&lambda;&nbsp;=&nbsp;3, corresponding to an upper probability bound of 4/81=&nbsp;0.04938..., and to construct ''3-sigma'' limits to bound ''nearly all'' (i.e. 99.73%) of the values of a process output. Without unimodality Chebyshev's inequality would give a looser bound of&nbsp;1/9&nbsp;=&nbsp;0.11111....\n\n==See also==\n*[[Gauss's inequality]], a similar result for the distance from the mode rather than the mean\n*[[Rule of three (statistics)]], a similar result for the Bernoulli distribution\n\n==References==\n{{Reflist}}\n* {{cite journal |author=D. F. Vysochanskij,  Y. I. Petunin |year=1980 |title=Justification of the 3&sigma; rule for unimodal distributions |journal=Theory of Probability and Mathematical Statistics |volume=21 |pages=25–36}}\n* [http://m.njit.edu/CAMS/Technical_Reports/CAMS02_03/report4.pdf Report (on cancer diagnosis) by Petunin and others stating theorem in English]\n\n{{DEFAULTSORT:Vysochanskij-Petunin inequality}}\n[[Category:Probabilistic inequalities]]\n[[Category:Statistical inequalities]]"
    },
    {
      "title": "Bhatia–Davis inequality",
      "url": "https://en.wikipedia.org/wiki/Bhatia%E2%80%93Davis_inequality",
      "text": "{{primary sources|date=February 2016}}\nIn mathematics, the '''Bhatia–Davis inequality''', named after [[Rajendra Bhatia]] and [[Chandler Davis]], is an [[upper bound]] on the [[variance]] ''σ''<sup>2</sup> of any bounded [[probability distribution]] on the real line.\n\nSuppose a distribution has minimum&nbsp;''m'', maximum&nbsp;''M'', and [[expected value]]&nbsp;''μ''.  Then the inequality says:\n\n: <math> \\sigma^2 \\le (M - \\mu)(\\mu - m). \\, </math>\n\nEquality holds precisely if all of the probability is concentrated at the endpoints&nbsp;''m'' and&nbsp;''M''.\n\nThe Bhatia–Davis inequality is stronger than [[Popoviciu's inequality on variances]].\n<!--\nA  lower bound for the variance based on the Bhatia&ndash;Davis inequality has been found by Agarwal et al.<ref name=Agarwal2005>Agarwal R P, Barnett N S, Cerone P and Dragomir S S (2005) \"A survey on some inequalities for expectation and variance.\" ''Computers and mathematics with applications'' 49 (2005) 429–480</ref>\n\n: <math> ( M - \\mu )( \\mu - m ) - \\frac{ ( M - m )^3 } 6 \\le \\sigma^2 </math> -->\n\n==See also==\n*[[Cramér–Rao bound]]\n*[[Chapman–Robbins bound]]\n\n== References ==\n{{Reflist}}\n*{{cite journal\n| doi        = 10.2307/2589180\n| last        = Bhatia\n| first       = Rajendra\n|author2=Davis, Chandler |authorlink2=Chandler Davis \n|date=April 2000\n| title       = A Better Bound on the Variance\n| journal     = [[American Mathematical Monthly]]\n| volume      = 107\n| issue       = 4\n| pages       = 353–357\n| publisher   = [[Mathematical Association of America]]\n| issn        = 0002-9890\n| jstor        = 2589180\n}}\n\n{{DEFAULTSORT:Bhatia-Davis Inequality}}\n[[Category:Statistical inequalities]]\n[[Category:Theory of probability distributions]]\n\n\n{{probability-stub}}"
    },
    {
      "title": "Binomial sum variance inequality",
      "url": "https://en.wikipedia.org/wiki/Binomial_sum_variance_inequality",
      "text": "The '''binomial sum variance inequality''' states that the variance of the sum of [[Binomial distribution|binomially distributed]] [[random variable]]s will always be less than or equal to the variance of a binomial variable with the same [[Binomial distribution#Mean and variance|''n'' and ''p'']] parameters. In [[probability theory]] and [[statistics]], the [[Binomial distribution#Sums of binomials|sum]] of independent binomial random variables is itself a binomial random variable if all the component variables share the same [[Binomial distribution#Mean and variance|success probability]]. If success probabilities differ, the probability distribution of the sum is not binomial.<ref>{{cite journal |last=Butler |first=K'. |last2=Stephens |first2=M. |year=1993 |title=The distribution of a sum of binomial random variables |work=Technical Report No. 467 |publisher=Department of Statistics, Stanford University |url=http://www.dtic.mil/dtic/tr/fulltext/u2/a266969.pdf }}</ref> The lack of uniformity in success probabilities across independent trials leads to a smaller variance.<ref>Nedelman, J and Wallenius, T., 1986. Bernoulli trials, Poisson trials, surprising variances, and Jensen’s Inequality. The American Statistician, 40(4):286–289.</ref><ref>Feller, W. 1968. An introduction to probability theory and its applications (Vol. 1, 3rd ed.). New York: John Wiley.</ref><ref>Johnson, N. L. and Kotz, S. 1969. Discrete distributions. New York: John Wiley</ref><ref>Kendall, M. and Stuart, A. 1977. The advanced theory of statistics. New York: Macmillan.</ref><ref name=\"DreznerFarnum1993\">{{cite journal |last1=Drezner |first1=Zvi |last2=Farnum |first2=Nicholas |title=A generalized binomial distribution |journal=Communications in Statistics - Theory and Methods |volume=22 |issue=11 |year=1993 |pages=3051–3063 |issn=0361-0926 |doi=10.1080/03610929308831202}}</ref> and is a special case of a more general theorem involving the [[expected value]] of convex functions.<ref>Hoeffding, W. 1956. On the distribution of the number of successes in independent trials. Annals of Mathematical Statistics (27):713–721.</ref> In some statistical applications, the standard binomial variance estimator can be used even if the component probabilities differ, though with a variance estimate that has an upward [[Bias (statistics)|bias]].\n\n==Inequality statement==\nConsider the sum, ''Z'', of two independent binomial random variables, ''X'' ~ B(''m''<sub>0</sub>, ''p''<sub>0</sub>) and ''Y'' ~ B(''m''<sub>1</sub>, ''p''<sub>1</sub>), where ''Z'' = ''X'' + ''Y''. Then, the variance of ''Z'' is less than or equal to its variance under the assumption that ''p''<sub>0</sub> = ''p''<sub>1</sub>, that is, if ''Z'' had a binomial distribution.<ref>{{cite journal |last=Millstein |first=J. |last2=Volfson |first2=D. |year=2013 |title=Computationally efficient permutation-based confidence interval estimation for tail-area FDR |journal=Frontiers in Genetics |volume=4 |issue=179 |pages=1–11 |pmc=3775454 |doi=10.3389/fgene.2013.00179 }}</ref> Symbolically, <math>Var(Z) \\leqslant E[Z] (1 - \\tfrac{E[Z]}{m_0+m_1})</math>.\n\n{{hidden begin|style=width:60%|ta1=center|border=1px #aaa solid|title=[Proof]}}\nWe wish to prove that \n:<math>Var(Z) \\leqslant E[Z] (1 - \\frac{E[Z]}{m_0+m_1})</math>\nWe will prove this inequality by finding an expression for Var(''Z'') and substituting it on the left-hand side, then showing that the inequality always holds.\n\nIf ''Z'' has a binomial distribution with parameters ''n'' and ''p'', then the [[expected value]] of ''Z''  is given by [[Binomial distribution#Mean and variance| E[''Z''] = ''np'']] and the variance of ''Z'' is given by [[Binomial distribution#Mean and variance| Var[''Z''] = ''np''(1 – ''p'')]]. Letting ''n'' = ''m''<sub>0</sub> + ''m''<sub>1</sub> and substituting E[''Z''] for ''np'' gives \n:<math>Var(Z) = E[Z] (1 - \\frac{E[Z]}{m_0+m_1})</math>\nThe random variables ''X'' and ''Y'' are independent, so the [[Variance#Sum of uncorrelated variables|variance of the sum is equal to the sum of the variances]], that is\n:<math>Var(Z) = E[X] (1-\\frac{E[X]}{m_0}) + E[Y] (1-\\frac{E[Y]}{m_1})</math>\n\nIn order to prove the theorem, it is therefore sufficient to prove that\n:<math>E[X](1 - \\frac{E[X]}{m_0}) + E[Y](1 - \\frac{E[Y]}{m_1}) \\leqslant E[Z](1 - \\frac{E[Z]}{m1+m0})</math>\n\n\nSubstituting E[''X''] + E[''Y''] for E[''Z''] gives\n:<math>E[X](1 - \\frac{E[X]}{m_0}) + E[Y](1 - \\frac{E[Y]}{m_1}) \\leqslant (E[X]+E[Y])(1 - \\frac{E[X]+E[Y]}{m_0+m_1})</math>\nMultiplying out the brackets and subtracting E[X] + E[Y] from both sides yields\n:<math>- \\frac{E[X]^2}{m_0} - \\frac{E[Y]^2}{m_1} \\leqslant - \\frac{(E[X]+E[Y])^2}{m_0+m_1}</math>\nMultiplying out the brackets yields\n:<math>E[X] - \\frac{E[X]^2}{m_0} + E[Y] - \\frac{E[Y]^2}{m_1} \\leqslant E[X] + E[Y] - \\frac{(E[X]+E[Y])^2}{m_0+m_1}</math>\nSubtracting E[X] and E[Y] from both sides and reversing the inequality gives\n:<math>\\frac{E[X]^2}{m_0} + \\frac{E[Y]^2}{m_1} \\geqslant \\frac{(E[X]+E[Y])^2}{m_0+m_1}</math>\nExpanding the right-hand side gives\n:<math>\\frac{E[X]^2}{m_0} + \\frac{E[Y]^2}{m_1} \\geqslant \\frac{E[X]^2+2E[X]E[Y]+E[Y]^2}{m_0+m_1}</math>\nMultiplying by <math>m_0 m_1 (m_0+m_1)</math> yields\n:<math>(m_0m_1+{m_1}^2){E[X]^2}+ ({m_0}^2+m_0m_1){E[Y]^2} \\geqslant m_0m_1({E[X]}^2+2E[X]E[Y]+{E[Y]]^2})</math>\nDeducting the right-hand side gives the relation \n:<math>{m_1}^2{E[X]^2} -2m_0m_1E[X]E[Y] + {m_0}^2{E[Y]^2} \\geqslant 0</math>\nor equivalently\n:<math>(m_1E[X] - m_0E[Y])^2 \\geqslant 0</math>\nThe square of a real number is always greater than or equal to zero, so this is true for all independent binomial distributions that X and Y could take. This is sufficient to prove the theorem.\n{{hidden end}}\n\n\nAlthough this proof was developed for the sum of two variables, it is easily generalized to greater than two. Additionally, if the individual success probabilities are known, then the variance is known to take the form<ref name=DreznerFarnum1993 />\n\n: <math> \\operatorname{Var}(Z) = n \\bar{p} (1 - \\bar{p}) - ns^2,</math>\n\nwhere <math> s^2 = \\frac{1}{n}\\sum_{i=1}^n (p_i-\\bar{p})^2</math>. This expression also implies that the variance is always less than that of the binomial distribution with <math>p=\\bar{p}</math>, because the standard expression for the variance is decreased by ''ns''<sup>2</sup>, a positive number.\n\n==Applications==\nThe inequality can be useful in the context of [[Multiple comparisons problem|multiple testing]], where many [[statistical hypothesis testing|statistical hypothesis tests]] are conducted within a particular study. Each test can be treated as a [[Bernoulli distribution|Bernoulli variable]] with a success probability ''p''. Consider the total number of positive tests as a random variable denoted by ''S''. This quantity is important in the estimation of [[false discovery rate | false discovery rates (FDR)]], which quantify uncertainty in the test results. If the [[statistical hypothesis testing#the testing process|null hypothesis]] is true for some tests and the [[statistical hypothesis testing#the testing process|alternative hypothesis]] is true for other tests, then success probabilities are likely to differ between these two groups. However, the variance inequality theorem states that if the tests are independent, the variance of ''S'' will be no greater than it would be under a binomial distribution.\n\n==References==\n{{reflist}}\n\n[[Category:Probability theorems]]\n[[Category:Statistical theorems]]\n[[Category:Statistical inequalities]]"
    },
    {
      "title": "Chapman–Robbins bound",
      "url": "https://en.wikipedia.org/wiki/Chapman%E2%80%93Robbins_bound",
      "text": "In [[statistics]], the '''Chapman–Robbins bound''' or '''Hammersley–Chapman–Robbins bound''' is a lower bound on the [[variance]] of [[estimator]]s of a deterministic parameter. It is a generalization of the [[Cramér–Rao bound]]; compared to the Cramér–Rao bound, it is both tighter and applicable to a wider range of problems. However, it is usually more difficult to compute.\n\nThe bound was independently discovered by [[John Hammersley]] in 1950,<ref>{{Citation\n  | last = Hammersley   | first = J. M. |authorlink=John Hammersley\n  | title = On estimating restricted parameters\n  | journal = [[Journal of the Royal Statistical Society]], Series B\n  | volume = 12 | issue = 2 | pages = 192–240 | year = 1950\n  | mr = 40631\n  | jstor = 2983981\n}}</ref> and by Douglas Chapman and [[Herbert Robbins]] in 1951.<ref>{{Citation\n  | last = Chapman | first =  D. G.\n  | last2 = Robbins | first2 = H. | author2-link = Herbert Robbins\n  | title = Minimum variance estimation without regularity assumptions\n  | journal = [[Annals of Mathematical Statistics]]\n  | volume = 22 | issue =4 | pages =581–586 | year =1951\n  | doi = 10.1214/aoms/1177729548\n  | mr = 44084\n  | jstor = 2236927\n}}</ref>\n\n== Statement ==\nLet {{nowrap|''θ'' ∈ '''R'''<sup>''n''</sup>}} be an unknown, deterministic parameter, and let {{nowrap|''X'' ∈ '''R'''<sup>''k''</sup>}} be a random variable, interpreted as a measurement of ''θ''. Suppose the [[probability density function]] of ''X'' is given by ''p''(''x''; ''θ''). It is assumed that ''p''(''x''; ''θ'') is well-defined and that {{nowrap|''p''(''x''; ''θ'') > 0}} for all values of ''x'' and ''θ''.\n\nSuppose ''δ''(''X'') is an [[bias (statistics)|unbiased]] estimate of an arbitrary scalar function {{nowrap|''g'':&thinsp;'''R'''<sup>''n''</sup> → '''R'''}} of ''θ'', i.e., \n\n:<math>\\operatorname E_\\theta\\{\\delta(X)\\} = g(\\theta)\\text{ for all }\\theta.\\,</math>\n\nThe Chapman–Robbins bound then states that\n\n:<math>\\operatorname{Var}_\\theta(\\delta(X)) \\ge \\sup_\\Delta \\frac{\\left[ g(\\theta+\\Delta) - g(\\theta) \\right]^2}{\\operatorname E_\\theta \\left[ \\tfrac{p(X;\\theta+\\Delta)}{p(X;\\theta)} - 1 \\right]^2}.</math>\n\nNote that the denominator in the lower bound above is exactly the [[F-divergence#Instances_of_f-divergences|<math> \\chi^2</math>-divergence]] of <math> p(\\cdot; \\theta+\\Delta)</math> with respect to <math> p(\\cdot; \\theta)</math>.\n\n== Relation to Cramér–Rao bound ==\nThe expression inside the supremum in the Chapman–Robbins bound converges to the [[Cramér–Rao bound]] when {{nowrap|Δ → 0}}, assuming the regularity conditions of the Cramér–Rao bound hold. This implies that, when both bounds exist, the Chapman–Robbins version is always at least as tight as the Cramér–Rao bound; in many cases, it is substantially tighter. \n\nThe Chapman–Robbins bound also holds under much weaker regularity conditions. For example, no assumption is made regarding differentiability of the probability density function ''p''(''x''; ''θ''). When ''p''(''x''; ''θ'') is non-differentiable, the [[Fisher information]] is not defined, and hence the Cramér–Rao bound does not exist.\n\n== See also ==\n* [[Cramér–Rao bound]]\n* [[Estimation theory]]\n\n== References ==\n{{reflist}}\n\n== Further reading ==\n* {{Citation\n  | last = Lehmann\n  | first = E. L.\n  | last2= Casella |first2=G.\n  | title = Theory of Point Estimation\n  | year = 1998\n  | publisher = Springer\n  | isbn = 0-387-98502-6\n  | edition = 2nd\n  | pages = 113–114 }}\n\n{{DEFAULTSORT:Chapman-Robbins bound}}\n[[Category:Statistical inequalities]]\n[[Category:Estimation theory]]"
    },
    {
      "title": "Cramér–Rao bound",
      "url": "https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound",
      "text": "{{Short description|Lower bound on variance of an estimator}}In [[estimation theory]] and [[statistics]], the '''Cramér–Rao bound (CRB)''', '''Cramér–Rao lower bound (CRLB)''', '''Cramér–Rao inequality''', '''Fréchet–Darmois–Cramér–Rao  inequality''', or '''information inequality''' expresses a lower bound on the [[variance]] of unbiased [[estimator]]s of a deterministic (fixed, though unknown) parameter. This term is named in honor of [[Harald Cramér]],<ref name=\"Cramér\">{{cite book  | last = Cramér | first = Harald | title = Mathematical Methods of Statistics | place = Princeton, NJ | publisher = Princeton Univ. Press | year = 1946 | isbn = 0-691-08004-6  | oclc = 185436716 }}</ref> [[C. R. Rao|Calyampudi Radhakrishna Rao]],<ref name=\"Rao\">{{cite journal  | last = Rao | first = Calyampudi Radakrishna | title = Information and the accuracy attainable in the estimation of statistical parameters | journal = Bulletin of the [[Calcutta Mathematical Society]] |mr=0015748  | volume = 37 | pages = 81–89  | year = 1945 }}</ref><ref name=\"Rao papers\">{{cite book  | last = Rao | first = Calyampudi Radakrishna | title = Selected Papers of C. R. Rao | editor = S. Das Gupta | place = New York | publisher = Wiley | year = 1994 | isbn = 978-0-470-22091-7  | oclc = 174244259 }}</ref> [[Maurice René Fréchet|Maurice Fréchet]]<ref name=\"Fréchet 1943\">{{cite journal|last1=Fréchet|first1=Maurice|title=Sur l'extension de certaines évaluations statistiques au cas de petits échantillons|journal=Rev. Inst. Int. Statist.|date=1943|volume=11|pages=182–205}}</ref> and [[Georges Darmois]]<ref name=\"Darmois 1945\">{{cite journal|last1=Darmois|first1=Georges|title=Sur les limites de la dispersion de certaines estimations|journal=Rev. Int. Inst. Statist.|date=1945|volume=13|pages=9–15}}</ref> all of whom independently derived this limit to statistical precision in the 1940s.<ref name=\"Gart 1958\">{{cite journal|last1=Gart|first1=John J.|title=An extension of the Cramér–Rao inequality|journal=Ann. Math. Stat.|date=1958|volume=29|pages=367–380}}</ref><ref name=\"Malécot 1947\">{{cite journal|last1=Malécot|first1=Gustave|title=Statistical methods and the subjective basis of scientific knowledge|journal=Genet. Sel. Evol.|date=1999|volume=31|pages=269–298|trans-title=translated from Année X 1947 by Daniel Gianola}}</ref>\n\nIn its simplest form, the bound states that the variance of any [[bias of an estimator|unbiased]] estimator is at least as high as the inverse of the [[Fisher information]]. An unbiased estimator which achieves this lower bound is said to be (fully) [[Efficiency (statistics)|efficient]]. Such a solution achieves the lowest possible [[mean squared error]] among all unbiased methods, and is therefore the [[minimum variance unbiased]] (MVU) estimator. However, in some cases, no unbiased technique exists which achieves the bound. This may occur either if for any unbiased estimator, there exists another with a strictly smaller variance, or if an MVU estimator exists, but its variance is strictly greater than the inverse of the Fisher information.\n\nThe Cramér–Rao bound can also be used to bound the variance of [[estimator bias|''biased'' estimators]] of given bias. In some cases, a biased approach can result in both a variance and a [[mean squared error]] that are ''below'' the unbiased Cramér–Rao lower bound; see [[estimator bias]].\n\n== Statement ==\n\nThe Cramér–Rao bound is stated in this section for several increasingly general cases, beginning with the case in which the parameter is a [[Scalar (mathematics)|scalar]] and its estimator is [[estimator bias|unbiased]]. All versions of the bound require certain regularity conditions, which hold for most well-behaved distributions. These conditions are listed [[#Regularity conditions|later in this section]].\n\n=== Scalar unbiased case ===\nSuppose <math>\\theta</math> is an unknown deterministic parameter which is to be estimated from measurements <math>x</math>, distributed according to some [[probability density function]] <math>f(x;\\theta)</math>. The [[variance]] of any ''unbiased'' estimator <math>\\hat{\\theta}</math> of <math>\\theta</math> is then bounded by the [[multiplicative inverse|reciprocal]] of the [[Fisher information]] <math>I(\\theta)</math>:\n\n:<math>\\operatorname{var}(\\hat{\\theta})\n\\geq\n\\frac{1}{I(\\theta)}\n</math>\nwhere the Fisher information <math>I(\\theta)</math> is defined by\n:<math>\nI(\\theta) = \\operatorname{E}\n \\left[\n  \\left(\n   \\frac{\\partial \\ell(x;\\theta)}{\\partial\\theta}\n  \\right)^2\n \\right] = -\\operatorname{E}\\left[ \\frac{\\partial^2 \\ell(x;\\theta)}{\\partial\\theta^2} \\right]\n</math>\nand <math>\\ell(x;\\theta)=\\log  (f(x;\\theta))</math> is the [[natural logarithm]] of the [[likelihood function]] and <math>\\operatorname{E}</math> denotes the [[expected value]] (over <math>x</math>).\n\nThe [[efficiency (statistics)|efficiency]] of an unbiased estimator <math>\\hat{\\theta}</math> measures how close this estimator's variance comes to this lower bound; estimator efficiency is defined as\n\n:<math>e(\\hat{\\theta}) = \\frac{I(\\theta)^{-1}}{\\operatorname{var}(\\hat{\\theta})}</math>\n\nor the minimum possible variance for an unbiased estimator divided by its actual variance.\nThe Cramér–Rao lower bound thus gives\n:<math>e(\\hat{\\theta}) \\le 1.</math>\n\n=== General scalar case ===\nA more general form of the bound can be obtained by considering a biased estimator <math>T(X)</math> of a function of the parameter, <math>\\psi(\\theta)</math>. Here, biasedness is understood as stating that <math> E\\{T(X)\\} - \\theta = \\psi(\\theta) - \\theta </math>  is not generally equal to 0. In this case, the bound is given by\n:<math>\n\\operatorname{var}(T)\n\\geq\n\\frac{[\\psi'(\\theta)]^2}{I(\\theta)}\n</math>\nwhere <math>\\psi'(\\theta)</math> is the derivative of <math>\\psi(\\theta)</math> (by <math>\\theta</math>), and <math>I(\\theta)</math> is the Fisher information defined above.\n\n=== Bound on the variance of biased estimators ===\nApart from being a bound on estimators of functions of the parameter, this approach can be used to derive a bound on the variance of biased estimators with a given bias, as follows. Consider an estimator <math>\\hat{\\theta}</math> with bias <math>b(\\theta) = E\\{\\hat{\\theta}\\} - \\theta</math>, and let <math>\\psi(\\theta) = b(\\theta) + \\theta</math>. By the result above, any unbiased estimator whose expectation is <math>\\psi(\\theta)</math> has variance greater than or equal to <math>(\\psi'(\\theta))^2/I(\\theta)</math>. Thus, any estimator <math>\\hat{\\theta}</math> whose bias is given by a function <math>b(\\theta)</math> satisfies\n:<math>\n\\operatorname{var} \\left(\\hat{\\theta}\\right)\n\\geq\n\\frac{[1+b'(\\theta)]^2}{I(\\theta)}.\n</math>\nThe unbiased version of the bound is a special case of this result, with <math>b(\\theta)=0</math>.\n\nIt's trivial to have a small variance − an \"estimator\" that is constant has a variance of zero. But from the above equation we find that the [[mean squared error]] of a biased estimator is bounded by\n\n:<math>\\operatorname{E}\\left((\\hat{\\theta}-\\theta)^2\\right)\\geq\\frac{[1+b'(\\theta)]^2}{I(\\theta)}+b(\\theta)^2,</math>\n\nusing the standard decomposition of the MSE. Note, however, that if <math>1+b'(\\theta)<1</math> this bound might be less than the unbiased Cramér–Rao bound <math>1/I(\\theta)</math>. For instance, in the [[#Normal_variance_with_known_mean|example of estimating variance below]], <math>1+b'(\\theta)= \\frac{n}{n+2} <1</math>.\n\n=== Multivariate case ===\nExtending the Cramér–Rao bound to multiple parameters, define a parameter column [[vector space|vector]]\n:<math>\\boldsymbol{\\theta} = \\left[ \\theta_1, \\theta_2, \\dots, \\theta_d \\right]^T \\in \\mathbb{R}^d</math>\nwith probability density function <math>f(x; \\boldsymbol{\\theta})</math> which satisfies the two [[#Regularity conditions|regularity conditions]] below.\n\nThe [[Fisher information matrix]] is a <math>d \\times d</math> matrix with element <math>I_{m, k}</math> defined as\n: <math>\nI_{m, k}\n= \\operatorname{E} \\left[\n \\frac{\\partial }{\\partial \\theta_m} \\log f\\left(x; \\boldsymbol{\\theta}\\right)\n \\frac{\\partial }{\\partial \\theta_k} \\log f\\left(x; \\boldsymbol{\\theta}\\right)\n\\right] = -\\operatorname{E} \\left[\n \\frac{\\partial ^2}{\\partial \\theta_m \\, \\partial \\theta_k} \\log f\\left(x; \\boldsymbol{\\theta}\\right)\n\\right].\n</math>\n\nLet <math>\\boldsymbol{T}(X)</math> be an estimator of any vector function of parameters, <math>\\boldsymbol{T}(X) = (T_1(X), \\ldots, T_d(X))^T</math>, and denote its expectation vector <math>\\operatorname{E}[\\boldsymbol{T}(X)]</math> by <math>\\boldsymbol{\\psi}(\\boldsymbol{\\theta})</math>. The Cramér–Rao bound then states that the [[covariance matrix]] of <math>\\boldsymbol{T}(X)</math> satisfies\n:<math>\n\\operatorname{cov}_{\\boldsymbol{\\theta}}\\left(\\boldsymbol{T}(X)\\right)\n\\geq\n\\frac\n {\\partial \\boldsymbol{\\psi} \\left(\\boldsymbol{\\theta}\\right)}\n {\\partial \\boldsymbol{\\theta}}\n\\left(\n[I\\left(\\boldsymbol{\\theta}\\right)]^{-1}\n \\frac\n  {\\partial \\boldsymbol{\\psi}\\left(\\boldsymbol{\\theta}\\right)}\n  {\\partial \\boldsymbol{\\theta}}\n\\right)^T\n</math>\nwhere\n* The matrix inequality <math>A \\ge B</math> is understood to mean that the matrix <math>A-B</math> is [[positive semidefinite matrix|positive semidefinite]], and\n* <math>\\partial \\boldsymbol{\\psi}(\\boldsymbol{\\theta})/\\partial \\boldsymbol{\\theta}</math> is the [[Jacobian matrix]] whose <math>ij</math> element is given by <math>\\partial \\psi_i(\\boldsymbol{\\theta})/\\partial \\theta_j</math>.\n\n<!-- please leave this extra space as it improves legibility. -->\n\nIf <math>\\boldsymbol{T}(X)</math> is an [[estimator bias|unbiased]] estimator of <math>\\boldsymbol{\\theta}</math> (i.e., <math>\\boldsymbol{\\psi}\\left(\\boldsymbol{\\theta}\\right) = \\boldsymbol{\\theta}</math>), then the Cramér–Rao bound reduces to\n: <math>\n\\operatorname{cov}_{\\boldsymbol{\\theta}}\\left(\\boldsymbol{T}(X)\\right)\n\\geq\nI\\left(\\boldsymbol{\\theta}\\right)^{-1}.\n</math>\n\nIf it is inconvenient to compute the inverse of the [[Fisher information matrix]],\nthen one can simply take the reciprocal of the corresponding diagonal element\nto find a (possibly loose) lower bound.<ref>For the Bayesian case, see eqn. (11) of {{cite journal |last=Bobrovsky |last2=Mayer-Wolf |last3=Zakai |title=Some classes of global Cramer–Rao bounds |journal=Ann. Stat. |volume=15 |issue=4 |pages=1421–38 |year=1987 }}</ref>\n\n: <math>\n\\operatorname{var}_{\\boldsymbol{\\theta}}(T_m(X))\n=\n\\left[\\operatorname{cov}_{\\boldsymbol{\\theta}}\\left(\\boldsymbol{T}(X)\\right)\\right]_{mm}\n\\geq\n\\left[I\\left(\\boldsymbol{\\theta}\\right)^{-1}\\right]_{mm}\n\\geq\n\\left(\\left[I\\left(\\boldsymbol{\\theta}\\right)\\right]_{mm}\\right)^{-1}.\n</math>\n\n=== Regularity conditions ===\nThe bound relies on two weak regularity conditions on the [[probability density function]], <math>f(x; \\theta)</math>, and the estimator <math>T(X)</math>:\n* The Fisher information is always defined; equivalently, for all <math>x</math> such that <math>f(x; \\theta) > 0</math>,\n::<math> \\frac{\\partial}{\\partial\\theta} \\log f(x;\\theta)</math>\n:exists, and is finite.\n* The operations of integration with respect to <math>x</math> and differentiation with respect to <math>\\theta</math> can be interchanged in the expectation of <math>T</math>; that is,\n::<math>\n \\frac{\\partial}{\\partial\\theta}\n \\left[\n  \\int T(x) f(x;\\theta) \\,dx\n \\right]\n =\n \\int T(x)\n  \\left[\n   \\frac{\\partial}{\\partial\\theta} f(x;\\theta)\n  \\right]\n \\,dx\n</math>\n:whenever the right-hand side is finite.\n:This condition can often be confirmed by using the fact that integration and differentiation can be swapped when either of the following cases hold:\n:# The function <math>f(x;\\theta)</math> has bounded support in <math>x</math>, and the bounds do not depend on <math>\\theta</math>;\n:# The function <math>f(x;\\theta)</math> has infinite support, is [[continuously differentiable]], and the integral converges uniformly for all <math>\\theta</math>.\n\n=== Simplified form of the Fisher information ===\nSuppose, in addition, that the operations of integration and differentiation can be swapped for the second derivative of <math>f(x;\\theta)</math> as well, i.e.,\n:<math> \\frac{\\partial^2}{\\partial\\theta^2}\n \\left[\n  \\int T(x) f(x;\\theta) \\,dx\n \\right]\n =\n \\int T(x)\n  \\left[\n   \\frac{\\partial^2}{\\partial\\theta^2} f(x;\\theta)\n  \\right]\n \\,dx.\n</math>\nIn this case, it can be shown that the Fisher information equals\n:<math>\nI(\\theta)\n=\n -\\operatorname{E}\n \\left[\n  \\frac{\\partial^2}{\\partial\\theta^2} \\log f(X;\\theta)\n \\right].\n</math>\nThe Cramèr–Rao bound can then be written as\n:<math>\n\\operatorname{var} \\left(\\widehat{\\theta}\\right)\n\\geq\n\\frac{1}{I(\\theta)}\n= \\frac{1}\n{ -\\operatorname{E}\n \\left[\n  \\frac{\\partial^2}{\\partial\\theta^2} \\log f(X;\\theta)\n \\right]}.\n</math>\nIn some cases, this formula gives a more convenient technique for evaluating the bound.\n\n== Single-parameter proof ==\nThe following is a proof of the general scalar case of the Cramér–Rao bound described [[#General scalar case|above]].  Assume that <math>T=t(X)</math> is an unbiased estimator for the value <math>\\psi(\\theta)</math> (based on the observations <math>X</math>), and so <math>\\operatorname{E}(T) = \\psi (\\theta)</math>.  The goal is to prove that, for all <math>\\theta</math>,\n:<math>\\operatorname{var}(t(X)) \\geq \\frac{[\\psi^\\prime(\\theta)]^2}{I(\\theta)}.</math>\n\nLet <math>X</math> be a [[random variable]] with probability density function <math>f(x; \\theta)</math>.\nHere <math>T = t(X)</math> is a [[statistic]], which is used as an [[estimator]] for <math>\\psi (\\theta)</math>.  Define <math>V</math> as the [[score (statistics)|score]]:\n\n:<math>V = \\frac{\\partial}{\\partial\\theta} \\ln f(X;\\theta) = \\frac{1}{f(X;\\theta)}\\frac{\\partial}{\\partial\\theta}f(X;\\theta)</math>\n\nwhere the [[chain rule]] is used in the final equality above. Then the [[expected value|expectation]] of <math>V</math>, written <math>\\operatorname{E}(V)</math>, is zero.  This is because:\n\n: <math>\n\\operatorname{E}(V)  = \\int f(x;\\theta)\\left[\\frac{1}{f(x;\\theta)}\\frac{\\partial }{\\partial \\theta} f(x;\\theta)\\right] \\, dx = \\frac{\\partial}{\\partial\\theta}\\int f(x;\\theta) \\, dx = 0\n</math>\n\nwhere the integral and partial derivative have been interchanged (justified by the second regularity condition).\n\n\nIf we consider the [[covariance]] <math>\\operatorname{cov}(V, T)</math> of <math>V</math> and <math>T</math>, we have <math>\\operatorname{cov}(V, T) = \\operatorname{E}(V T)</math>, because <math>\\operatorname{E}(V) = 0</math>.  Expanding this expression we have\n\n:<math>\n\\begin{align}\n\\operatorname{cov}(V,T)\n& = \\operatorname{E}\n\\left(\n T \\cdot\\left[\\frac{1}{f(X;\\theta)}\\frac{\\partial}{\\partial\\theta}f(X;\\theta)  \\right]\n\\right) \\\\[6pt]\n& = \\int t(x) \\left[\\frac{1}{f(x;\\theta)} \\frac{\\partial}{\\partial\\theta} f(x;\\theta) \\right] f(x;\\theta)\\, dx \\\\[6pt]\n& = \\frac{\\partial}{\\partial\\theta}\n\\left[ \\int t(x) f(x;\\theta)\\,dx \\right]\n= \\psi^\\prime(\\theta)\n\\end{align}\n</math>\n\nagain because the integration and differentiation operations commute (second condition).\n\nThe [[Cauchy–Schwarz inequality]] shows that\n\n:<math>\n\\sqrt{ \\operatorname{var} (T) \\operatorname{var} (V)} \\geq \\left| \\operatorname{cov}(V,T) \\right| = \\left | \\psi^\\prime (\\theta)\n\\right |</math>\n\ntherefore\n\n:<math>\n\\operatorname{var}  (T) \\geq \\frac{[\\psi^\\prime(\\theta)]^2}{\\operatorname{var} (V)}\n= \\frac{[\\psi^\\prime(\\theta)]^2}{I(\\theta)}\n</math>\nwhich proves the proposition.\n\n== Examples ==\n\n=== Multivariate normal distribution ===\nFor the case of a [[multivariate normal distribution|''d''-variate normal distribution]]\n: <math>\n\\boldsymbol{x}\n\\sim\nN_d\n\\left(\n \\boldsymbol{\\mu}( \\boldsymbol{\\theta})\n ,\n {\\boldsymbol C} ( \\boldsymbol{\\theta})\n\\right)\n</math>\nthe [[Fisher information matrix]] has elements<ref>{{cite book\n  | last = Kay\n  | first = S. M.\n  | title = Fundamentals of Statistical Signal Processing: Estimation Theory\n  | year = 1993\n  | publisher = Prentice Hall\n  | page = 47\n  | isbn = 0-13-042268-1 }}\n</ref>\n:<math>\nI_{m, k}\n= \\frac{\\partial \\boldsymbol{\\mu}^T}{\\partial \\theta_m}\n{\\boldsymbol C}^{-1}\n\\frac{\\partial \\boldsymbol{\\mu}}{\\partial \\theta_k}\n+ \\frac{1}{2}\n\\operatorname{tr}\n\\left(\n {\\boldsymbol C}^{-1}\n \\frac{\\partial {\\boldsymbol C}}{\\partial \\theta_m}\n {\\boldsymbol C}^{-1}\n \\frac{\\partial {\\boldsymbol C}}{\\partial \\theta_k}\n\\right)\n</math>\nwhere \"tr\" is the [[trace (matrix)|trace]].\n\nFor example, let <math>w[n]</math> be a sample of <math>N</math> independent observations with unknown mean <math>\\theta</math> and known variance <math>\\sigma^2</math> .\n:<math>w[n] \\sim \\mathbb{N}_N \\left(\\theta {\\boldsymbol 1}, \\sigma^2 {\\boldsymbol I} \\right).</math>\nThen the Fisher information is a scalar given by\n\n:<math>\nI(\\theta)\n=\n\\left(\\frac{\\partial\\boldsymbol{\\mu}(\\theta)}{\\partial\\theta}\\right)^T{\\boldsymbol C}^{-1} \\left(\\frac{\\partial\\boldsymbol{\\mu}(\\theta)}{\\partial\\theta}\\right)\n= \\sum^N_{i=1}\\frac{1}{\\sigma^2} = \\frac{N}{\\sigma^2},\n</math>\n\nand so the Cramér–Rao bound is\n\n:<math>\n\\operatorname{var}\\left(\\hat \\theta\\right)\n\\geq\n\\frac{\\sigma^2}{N}.\n</math>\n\n=== Normal variance with known mean ===\nSuppose ''X'' is a [[normal distribution|normally distributed]] random variable with known mean <math>\\mu</math> and unknown variance <math>\\sigma^2</math>.  Consider the following statistic:\n\n:<math>\nT=\\frac{\\sum_{i=1}^n (X_i-\\mu)^2}{n-1}.\n</math>\n\nThen ''T'' is unbiased for <math>\\sigma^2</math>, as <math>E(T)=\\sigma^2</math>.  What is the variance of ''T''?\n\n:<math>\n\\operatorname{var}(T) = \\frac{\\operatorname{var}(X-\\mu)^2}{n-1}=\\frac{1}{n-1}\n\\left[\n\\operatorname{E}\\left\\{(X-\\mu)^4\\right\\}-\\left(\\operatorname{E}\\{(X-\\mu)^2\\}\\right)^2\n\\right]\n</math>\n\n(the second equality follows directly from the definition of variance).  The first term is the fourth [[moment about the mean]] and has value <math>3(\\sigma^2)^2</math>; the second is the square of the variance, or <math>(\\sigma^2)^2</math>.\nThus\n\n:<math>\\operatorname{var}(T)=\\frac{2(\\sigma^2)^2}{n}.</math>\n\nNow, what is the [[Fisher information]] in the sample? Recall that the [[score (statistics)|score]] ''V'' is defined as\n\n:<math>\nV=\\frac{\\partial}{\\partial\\sigma^2}\\log L(\\sigma^2,X)\n</math>\n\nwhere <math>L</math> is the [[likelihood function]].  Thus in this case,\n\n:<math>\nV=\\frac{\\partial}{\\partial\\sigma^2}\\log\\left[\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-(X-\\mu)^2 /{2\\sigma^2}}\\right] =\\frac{(X-\\mu)^2}{2(\\sigma^2)^2}-\\frac{1}{2\\sigma^2}\n</math>\n\nwhere the second equality is from elementary calculus.  Thus, the information in a single observation is just minus the expectation of the derivative of ''V'', or\n\n:<math>\nI\n=-\\operatorname{E}\\left(\\frac{\\partial V}{\\partial\\sigma^2}\\right)\n=-\\operatorname{E}\\left(-\\frac{(X-\\mu)^2}{(\\sigma^2)^3}+\\frac{1}{2(\\sigma^2)^2}\\right)\n=\\frac{\\sigma^2}{(\\sigma^2)^3}-\\frac{1}{2(\\sigma^2)^2}\n=\\frac{1}{2(\\sigma^2)^2}.</math>\n\nThus the information in a sample of <math>n</math> independent observations is just <math>n</math> times this, or <math>\\frac{n}{2(\\sigma^2)^2}.</math>\n\nThe Cramer–Rao bound states that\n\n:<math>\n\\operatorname{var}(T)\\geq\\frac{1}{I}.</math>\n\nIn this case, the inequality is saturated (equality is achieved), showing that the [[estimator]] is [[efficiency (statistics)|efficient]].\n\nHowever, we can achieve a lower [[mean squared error]] using a biased estimator. The estimator\n\n:<math>\nT=\\frac{\\sum_{i=1}^n (X_i-\\mu)^2}{n+2}.\n</math>\n\nobviously has a smaller variance, which is in fact\n\n:<math>\\operatorname{var}(T)=\\frac{2n(\\sigma^2)^2}{(n+2)^2}.</math>\n\nIts bias is\n\n: <math>\\left(1-\\frac{n}{n+2}\\right)\\sigma^2=\\frac{2\\sigma^2}{n+2}</math>\n\nso its mean squared error is\n\n:<math>\\operatorname{MSE}(T)=\\left(\\frac{2n}{(n+2)^2}+\\frac{4}{(n+2)^2}\\right)(\\sigma^2)^2\n=\\frac{2(\\sigma^2)^2}{n+2}</math>\n\nwhich is clearly less than the Cramér–Rao bound found above.\n\nWhen the mean is not known, the minimum mean squared error estimate of the variance of a sample from Gaussian distribution is achieved by dividing by ''n''&nbsp;+&nbsp;1, rather than ''n''&nbsp;−&nbsp;1 or ''n''&nbsp;+&nbsp;2.\n\n== See also ==\n* [[Chapman–Robbins bound]]\n* [[Kullback's inequality]]\n* [[Brascamp–Lieb inequality]]\n\n== References and notes ==\n{{reflist}}\n\n== Further reading ==\n* {{cite book |last=Amemiya |first=Takeshi |authorlink=Takeshi Amemiya |title=Advanced Econometrics |location=Cambridge |publisher=Harvard University Press |year=1985 |pages=14–17 |isbn=0-674-00560-0 |url=https://books.google.com/books?id=0bzGQE14CwEC&pg=PA14 }}\n* {{cite book |last=Bos |first=Adriaan van den |title=Parameter Estimation for Scientists and Engineers |location=Hoboken |publisher=John Wiley & Sons |year=2007 |isbn=0-470-14781-4 |pages=45–98 }}\n* {{Cite book\n  | last = Kay\n  | first = Steven M.\n  | title = Fundamentals of Statistical Signal Processing, Volume I: Estimation Theory\n  | publisher = Prentice Hall\n  | year = 1993\n  | isbn = 0-13-345711-7 }}. Chapter 3.\n* {{Cite book\n  | last = Shao\n  | first = Jun\n  | title = Mathematical Statistics\n  | place = New York\n  | publisher = Springer\n  | year = 1998\n  | isbn = 0-387-98674-X }}. Section 3.1.3.\n\n== External links ==\n*[https://archive.is/20121215055105/http://www4.utsouthwestern.edu/wardlab/fandplimittool.asp FandPLimitTool] a GUI-based software to calculate the Fisher information and Cramer-Rao Lower Bound with application to single-molecule microscopy.\n\n{{DEFAULTSORT:Cramer-Rao bound}}\n[[Category:Articles containing proofs]]\n[[Category:Statistical inequalities]]\n[[Category:Estimation theory]]"
    },
    {
      "title": "Dvoretzky–Kiefer–Wolfowitz inequality",
      "url": "https://en.wikipedia.org/wiki/Dvoretzky%E2%80%93Kiefer%E2%80%93Wolfowitz_inequality",
      "text": "[[File:DKW bounds.svg|thumb|300px|The above chart shows an example application of the DKW inequality in constructing confidence bounds (in purple) around an empirical distribution function (in light blue). In this random draw, the true CDF (orange) is entirely contained within the DKW bounds.]]\n\nIn the theory of [[probability]] and [[statistics]], the '''Dvoretzky–Kiefer–Wolfowitz inequality''' bounds how close an [[empirical distribution function|empirically determined distribution function]] will be to the  [[cumulative distribution function|distribution function]] from which the empirical samples are drawn. It is named after [[Aryeh Dvoretzky]], [[Jack Kiefer (mathematician)|Jack Kiefer]], and [[Jacob Wolfowitz]], who in 1956 proved\nthe inequality with an unspecified multiplicative constant&nbsp;''C'' in front of the exponent on the right-hand side.<ref name=\"Dvoretzky\">{{citation\n | last1 = Dvoretzky\n | first1 = A.\n | authorlink1 = Aryeh Dvoretzky\n | last2 = Kiefer\n | first2 = J.\n | authorlink2 = Jack Kiefer (mathematician)\n | last3 = Wolfowitz\n | first3 = J.\n | authorlink3 = Jacob Wolfowitz\n | title = Asymptotic minimax character of the sample distribution function and of the classical multinomial estimator\n | journal = [[Annals of Mathematical Statistics]]\n | volume = 27\n | issue = 3\n | year = 1956\n | pages = 642–669\n | url = http://projecteuclid.org/euclid.aoms/1177728174\n | mr = 0083864\n | doi = 10.1214/aoms/1177728174}}</ref> In&nbsp;1990, Pascal Massart proved the inequality with the sharp constant ''C''&nbsp;=&nbsp;2,<ref name=\"Massart\">{{citation\n | last=Massart\n | first = P.\n | title = The tight constant in the Dvoretzky–Kiefer–Wolfowitz inequality\n | journal = [[Annals of Probability]]\n | volume = 18\n | issue = 3\n | year = 1990\n | pages = 1269–1283\n | url = http://projecteuclid.org/euclid.aop/1176990746\n | mr = 1062069\n | doi=10.1214/aop/1176990746}}</ref> confirming a conjecture due to [[Allan Birnbaum|Birnbaum]] and McCarty.<ref>{{cite journal\n| mr=0093874\n| zbl = 0087.34002\n| last = Birnbaum\n| first = Z. W.\n| last2 = McCarty\n| first2 = R. C.\n| title = A distribution-free upper confidence bound for Pr{Y<X}, based on independent samples of X and Y\n| journal = Annals of Mathematical Statistics\n| volume = 29\n| year = 1958\n| pages = 558–562\n| doi = 10.1214/aoms/1177706631\n| url =  http://projecteuclid.org/euclid.aoms/1177706631}}</ref>\n\n==The DKW inequality==\nGiven a natural number ''n'', let ''X''<sub>1</sub>, ''X''<sub>2</sub>, …, ''X<sub>n</sub>'' be real-valued [[independent and identically distributed]] [[random variable]]s with [[cumulative distribution function]] ''F''(·).  Let ''F<sub>n</sub>'' denote the associated [[empirical distribution function]] defined by\n: <math>\n    F_n(x) = \\frac1n \\sum_{i=1}^n \\mathbf{1}_{\\{X_i\\leq x\\}},\\qquad x\\in\\mathbb{R}.\n  </math>\nSo <math>F(x)</math> is the ''probability'' that a ''single'' random variable <math>X</math> is smaller than <math>x</math>, and <math>F_n(x)</math> is the ''fraction'' of random variables that are smaller than <math>x</math>.\n\nThe Dvoretzky–Kiefer–Wolfowitz inequality bounds the probability that the [[random function]] ''F<sub>n</sub>'' differs from  ''F'' by more than a given constant ''ε''&nbsp;>&nbsp;0 anywhere on the real line. More precisely, there is the one-sided estimate\n: <math>\n    \\Pr\\Bigl(\\sup_{x\\in\\mathbb R} \\bigl(F_n(x) - F(x)\\bigr) > \\varepsilon \\Bigr) \\le e^{-2n\\varepsilon^2}\\qquad \\text{for every }\\varepsilon\\geq\\sqrt{\\tfrac{1}{2n}\\ln2},\n  </math>\n\nwhich also implies a two-sided estimate<ref name=\"Kosorok\">{{citation\n | last1 = Kosorok\n | first1 = M.R.\n | title = Introduction to Empirical Processes and Semiparametric Inference\n | year = 2008\n | chapter = Chapter 11: Additional Empirical Process Results\n | page = 210\n | isbn = 9780387749778\n | publisher=Springer }}</ref>\n: <math>\n    \\Pr\\Bigl(\\sup_{x\\in\\mathbb R} |F_n(x) - F(x)| > \\varepsilon \\Bigr) \\le 2e^{-2n\\varepsilon^2}\\qquad \\text{for every }\\varepsilon>0.\n  </math>\n\nThis strengthens the [[Glivenko–Cantelli theorem]] by quantifying the [[rate of convergence]] as ''n'' tends to infinity. It also estimates the tail probability of the [[Kolmogorov–Smirnov test|Kolmogorov–Smirnov statistic]].   The inequalities above follow from the case where ''F'' corresponds to be the [[uniform distribution (continuous)|uniform distribution]] on [0,1] in view of the fact<ref name=\"Shorack\">\n{{citation\n | last1 = Shorack\n | first1 = G.R.\n | last2 = Wellner\n | first2 = J.A.\n | title = Empirical Processes with Applications to Statistics\n | year = 1986 |isbn=0-471-86725-X\n | publisher=Wiley }}\n</ref>\nthat ''F<sub>n</sub>'' has the same distributions as ''G<sub>n</sub>''(''F'') where ''G<sub>n</sub>'' is the empirical distribution of\n''U''<sub>1</sub>, ''U''<sub>2</sub>, …, ''U<sub>n</sub>'' where these are independent and Uniform(0,1), and noting that\n: <math>\n    \\sup_{x\\in\\mathbb R} |F_n(x) - F(x)| \\; \\stackrel{d}{=} \\; \\sup_{x \\in \\mathbb R} | G_n (F(x)) - F(x) | \\le \\sup_{0 \\le t \\le 1} | G_n (t) -t | ,\n  </math>\nwith equality if and only if ''F'' is continuous.\n\n==Building CDF bands==\n{{See also|CDF-based nonparametric confidence interval}}\n\nThe Dvoretzky–Kiefer–Wolfowitz inequality is one method for generating CDF-based confidence bounds and producing a [[Confidence_and_prediction_bands|confidence band]]. The purpose of this confidence interval is to contain the entire CDF at the specified confidence level, while alternative approaches attempt to only achieve the confidence level on each individual point which can allow for a tighter bound. The DKW bounds runs parallel to, and is equally above and below, the empirical CDF. The equally spaced confidence interval around the empirical CDF allows for different rates of violations across the support of the distribution. In particular, it is more common for a CDF to be outside of the CDF bound estimated using the DKW inequality near the median of the distribution than near the endpoints of the distribution. \n\nThe interval that contains the true CDF, <math>F(x)</math>, with probability <math>1-\\alpha</math> is often specified as\n\n: <math>\n    F_n(x) - \\varepsilon \\le F(x) \\le F_n(x) + \\varepsilon \\; \\text{ where } \\varepsilon = \\sqrt{\\frac{\\ln{\\frac{2}{\\alpha}}}{2n}}.\n  </math>\n\n==See also==\n* [[Concentration inequality]] – a summary of bounds on sets of random variables.\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Dvoretzky-Kiefer-Wolfowitz inequality}}\n[[Category:Asymptotic theory (statistics)]]\n[[Category:Statistical inequalities]]\n[[Category:Empirical process]]"
    },
    {
      "title": "Fisher's inequality",
      "url": "https://en.wikipedia.org/wiki/Fisher%27s_inequality",
      "text": "'''Fisher's inequality''', is a [[Necessity and sufficiency|necessary condition]] for the existence of a balanced incomplete [[block design]], that is, a system of subsets that satisfy certain prescribed conditions in [[combinatorics|combinatorial]] [[mathematics]]. Outlined by [[Ronald Fisher]], a [[population genetics|population geneticist]] and [[statistics|statistician]], who was concerned with the [[design of experiments]]; studying the differences among several different [[variety (botany)|varieties]] of [[plant]]s, under each of a number of different growing conditions, called ''blocks''.\n\nLet:\n\n* {{math|''v''}} be the number of varieties of plants;\n* {{math|''b''}} be the number of blocks.\n\nTo be a balanced incomplete block design it is required that:\n\n* {{math|''k''}} different varieties are in each block, {{math|1 ≤ ''k'' < ''v''}}; no variety occurs twice in any one block;\n* any two varieties occur together in exactly {{math|λ}} blocks;\n* each variety occurs in exactly {{math|''r''}} blocks.\n\nFisher's inequality states simply that\n\n:: {{math|''b'' ≥ ''v''}}.\n\n== Proof ==\nLet the incidence matrix {{math|'''M'''}} be a {{math|''v'' × ''b''}} matrix defined so that {{math|'''M'''<sub>i,j</sub>}} is 1 if element {{math|''i''}} is in block {{math|''j''}} and 0 otherwise. Then {{math|1='''B'''&nbsp;=&nbsp;'''MM'''<sup>T</sup>}} is a {{math|''v'' × ''v''}} matrix such that {{math|1='''B'''<sub>i,i</sub> = ''r''}} and {{math|1='''B'''<sub>i,j</sub> = λ}} for {{math|''i'' ≠ ''j''}}. Since {{math|''r'' ≠ λ}}, {{math|det('''B''') ≠ 0}}, so {{math|1=rank('''B''') = ''v''}}; on the other hand, {{math|1=rank('''B''') ≤ rank('''M''') ≤ ''b''}}, so {{math|''v'' ≤ ''b''}}.\n\n==Generalization==\nFisher's inequality is valid for more general classes of designs. A ''pairwise balanced design'' (or PBD) is a set {{math|''X''}} together with a family of non-empty subsets of {{math|''X''}} (which need not have the same size and may contain repeats) such that every pair of distinct elements of {{math|''X''}} is contained in exactly {{math|λ}} (a positive integer) subsets. The set {{math|''X''}} is allowed to be one of the subsets, and if all the subsets are copies of {{math|''X''}}, the PBD is called \"trivial\". The size of {{math|''X''}} is {{math|''v''}} and the number of subsets in the family (counted with multiplicity) is {{math|''b''}}.\n\nTheorem: For any non-trivial PBD, {{math|''v'' ≤ ''b''}}.<ref>{{harvnb|Stinson|2003|loc = pg.193}}</ref>\n\nThis result also generalizes the [[De Bruijn–Erdős theorem (incidence geometry)|Erdős–De Bruijn theorem]]:\n\nFor a PBD with {{math|1=λ = 1}} having no blocks of size 1 or size {{mvar|v}}, {{math|''v'' ≤ ''b''}}, with equality if and only if the PBD is a [[projective plane]] or a near-pencil (meaning that exactly {{math|''n''&nbsp;−&nbsp;1}} of the points are [[collinear]]).<ref>{{harvnb|Stinson|2003|loc= pg.183}}</ref>\n\nIn another direction, [[Dijen K. Ray-Chaudhuri|Ray-Chaudhuri]] and [[R. M. Wilson|Wilson]] proved in 1975 that in a {{math|2''s''-(''v'', ''k'', λ)}} design, the number of blocks is at least <math>\\binom{v}{s}</math>.<ref>{{citation|first1=Dijen K.|last1=Ray-Chaudhuri|first2=Richard M.|last2=Wilson|title=On t-designs|year=1975|journal=Osaka Journal of Mathematics|url=https://projecteuclid.org/euclid.ojm/1200758175|volume=12|pages=737–744|MR=0592624|zbl=0342.05018}}</ref>\n\n==Notes==\n{{reflist}}\n\n==References==\n* [[R. C. Bose]], \"A Note on Fisher's Inequality for Balanced Incomplete Block Designs\", ''[[Annals of Mathematical Statistics]]'', 1949, pages 619&ndash;620.\n* R. A. Fisher, \"An examination of the different possible solutions of a problem in incomplete blocks\", ''[[Annals of Human Genetics|Annals of Eugenics]]'', volume 10, 1940, pages 52&ndash;75.\n* {{citation|last=Stinson|first=Douglas R.|title=Combinatorial Designs: Constructions and Analysis|year=2003|publisher=Springer|location=New York|isbn=0-387-95487-2}}\n*{{cite book\n|author1=Street, Anne Penfold|author1-link= Anne Penfold Street  |author2=Street, Deborah J.\n|title=Combinatorics of Experimental Design\n|publisher=Oxford U. P. [Clarendon]\n|year=1987\n|pages=400+xiv\n|isbn=0-19-853256-3\n}}\n\n{{Experimental design|state=expanded}}\n\n[[Category:Design theory]]\n[[Category:Design of experiments]]\n[[Category:Set families]]\n[[Category:Statistical inequalities]]"
    },
    {
      "title": "Kullback's inequality",
      "url": "https://en.wikipedia.org/wiki/Kullback%27s_inequality",
      "text": "In [[information theory]] and [[statistics]], '''Kullback's inequality''' is a lower bound on the [[Kullback–Leibler divergence]] expressed in terms of the [[large deviations theory|large deviations]] [[rate function]].<ref>{{cite book |first=Aimé |last=Fuchs |first2=Giorgio |last2=Letta |title=L'inégalité de Kullback. Application à la théorie de l'estimation |series=Séminaire de probabilités |location=Strasbourg |volume=4 |pages=108–131 |year=1970 |url=http://www.numdam.org/item?id=SPS_1970__4__108_0 |isbn= }}</ref>  If ''P'' and ''Q'' are [[probability distribution]]s on the real line, such that ''P'' is '''absolutely continuous''' with respect to ''Q'', i.e. ''P''<<''Q'', and whose first moments exist, then\n:<math>D_{KL}(P\\|Q) \\ge \\Psi_Q^*(\\mu'_1(P)),</math>\nwhere <math>\\Psi_Q^*</math> is the rate function, i.e. the [[convex conjugate]] of the [[cumulant]]-generating function, of <math>Q</math>, and <math>\\mu'_1(P)</math> is the first [[Moment (mathematics)|moment]] of <math>P.</math>\n\nThe [[Cramér–Rao bound]] is a corollary of this result.\n\n==Proof==\nLet ''P'' and ''Q'' be [[probability distribution]]s (measures) on the real line, whose first moments exist, and such that [[Absolutely_continuous#Absolute_continuity_of_measures|''P''<<''Q'']]. Consider the '''[[natural exponential family]]''' of ''Q'' given by\n:<math>Q_\\theta(A) = \\frac{\\int_A e^{\\theta x}Q(dx)}{\\int_{-\\infty}^\\infty e^{\\theta x}Q(dx)}\n   = \\frac{1}{M_Q(\\theta)} \\int_A e^{\\theta x}Q(dx)</math>\nfor every measurable set ''A'', where <math>M_Q</math> is the '''[[moment-generating function]]''' of ''Q''.  (Note that ''Q''<sub>0</sub>=''Q''.)  Then\n:<math>D_{KL}(P\\|Q) = D_{KL}(P\\|Q_\\theta)\n   + \\int_{\\mathrm{supp}P}\\left(\\log\\frac{\\mathrm dQ_\\theta}{\\mathrm dQ}\\right)\\mathrm dP.</math>\nBy [[Gibbs' inequality]] we have <math>D_{KL}(P\\|Q_\\theta) \\ge 0</math> so that\n:<math>D_{KL}(P\\|Q) \\ge\n   \\int_{\\mathrm{supp}P}\\left(\\log\\frac{\\mathrm dQ_\\theta}{\\mathrm dQ}\\right)\\mathrm dP\n = \\int_{\\mathrm{supp}P}\\left(\\log\\frac{e^{\\theta x}}{M_Q(\\theta)}\\right) P(dx)</math>\nSimplifying the right side, we have, for every real θ where <math>M_Q(\\theta) < \\infty:</math>\n:<math>D_{KL}(P\\|Q) \\ge \\mu'_1(P) \\theta - \\Psi_Q(\\theta),</math>\nwhere <math>\\mu'_1(P)</math> is the first moment, or mean, of ''P'', and <math>\\Psi_Q = \\log M_Q</math> is called the '''[[cumulant|cumulant-generating function]]'''.  Taking the supremum completes the process of [[convex conjugate|convex conjugation]] and yields the [[rate function]]:\n:<math>D_{KL}(P\\|Q) \\ge \\sup_\\theta \\left\\{ \\mu'_1(P) \\theta - \\Psi_Q(\\theta) \\right\\}\n   = \\Psi_Q^*(\\mu'_1(P)).</math>\n\n==Corollary: the Cramér–Rao bound==\n{{main|Cramér–Rao bound}}\n===Start with Kullback's inequality===\nLet ''X''<sub>θ</sub> be a family of probability distributions on the real line indexed by the real parameter θ, and satisfying certain [[Cramér–Rao_bound#Regularity_conditions|regularity conditions]].  Then\n:<math> \\lim_{h\\rightarrow 0} \\frac {D_{KL}(X_{\\theta+h}\\|X_\\theta)} {h^2}\n    \\ge \\lim_{h\\rightarrow 0} \\frac {\\Psi^*_\\theta (\\mu_{\\theta+h})}{h^2},\n</math>\n\nwhere <math>\\Psi^*_\\theta</math> is the [[convex conjugate]] of the [[Cumulant|cumulant-generating function]] of <math>X_\\theta</math> and <math>\\mu_{\\theta+h}</math> is the first moment of <math>X_{\\theta+h}.</math>\n\n===Left side===\nThe left side of this inequality can be simplified as follows:\n\n:<math>\\begin{align}\n\\lim_{h\\to  0} \\frac {D_{KL}(X_{\\theta+h}\\|X_\\theta)} {h^2} &=\\lim_{h\\to  0} \\frac 1 {h^2} \\int_{-\\infty}^\\infty \\log \\left( \\frac{\\mathrm dX_{\\theta+h}}{\\mathrm dX_\\theta} \\right) \\mathrm dX_{\\theta+h} \\\\\n&=\\lim_{h\\to  0} \\frac 1 {h^2} \\int_{-\\infty}^\\infty \\log\\left( 1- \\left (1-\\frac{\\mathrm dX_{\\theta+h}}{\\mathrm dX_\\theta} \\right ) \\right) \\mathrm dX_{\\theta+h} \\\\\n&= \\lim_{h\\to  0} \\frac 1 {h^2} \\int_{-\\infty}^\\infty \\left[ \\left( 1 - \\frac{\\mathrm dX_\\theta}{\\mathrm dX_{\\theta+h}} \\right) +\\frac 1 2 \\left( 1 - \\frac{\\mathrm dX_\\theta}{\\mathrm dX_{\\theta+h}} \\right) ^ 2\n + o \\left( \\left( 1 - \\frac{\\mathrm dX_\\theta}{\\mathrm dX_{\\theta+h}} \\right) ^ 2 \\right) \\right]\\mathrm dX_{\\theta+h} && \\text{Taylor series for } \\log(1-t) \\\\\n&= \\lim_{h\\to  0} \\frac 1 {h^2} \\int_{-\\infty}^\\infty \\left[ \\frac 1 2 \\left( 1 - \\frac{\\mathrm dX_\\theta}{\\mathrm dX_{\\theta+h}} \\right)^2 \\right]\\mathrm dX_{\\theta+h} \\\\\n&= \\lim_{h\\to  0} \\frac 1 {h^2} \\int_{-\\infty}^\\infty \\left[ \\frac 1 2 \\left( \\frac{\\mathrm dX_{\\theta+h} - \\mathrm dX_\\theta}{\\mathrm dX_{\\theta+h}} \\right)^2  \\right]\\mathrm dX_{\\theta+h} \\\\\n&= \\frac 1 2 \\mathcal I_X(\\theta)\n\\end{align}</math>\nwhich is half the [[Fisher information]] of the parameter θ.\n\n===Right side===\nThe right side of the inequality can be developed as follows:\n:<math>\n  \\lim_{h\\rightarrow 0} \\frac {\\Psi^*_\\theta (\\mu_{\\theta+h})}{h^2}\n= \\lim_{h\\rightarrow 0} \\frac 1 {h^2} {\\sup_t \\{\\mu_{\\theta+h}t - \\Psi_\\theta(t)\\} }.\n</math>\nThis supremum is attained at a value of ''t''=τ where the first derivative of the cumulant-generating function is <math>\\Psi'_\\theta(\\tau) = \\mu_{\\theta+h},</math> but we have <math>\\Psi'_\\theta(0) = \\mu_\\theta,</math> so that\n:<math>\\Psi''_\\theta(0) = \\frac{d\\mu_\\theta}{d\\theta} \\lim_{h \\rightarrow 0} \\frac h \\tau.</math>\nMoreover,\n:<math>\\lim_{h\\rightarrow 0} \\frac {\\Psi^*_\\theta (\\mu_{\\theta+h})}{h^2}\n   = \\frac 1 {2\\Psi''_\\theta(0)}\\left(\\frac {d\\mu_\\theta}{d\\theta}\\right)^2\n   = \\frac 1 {2\\mathrm{Var}(X_\\theta)}\\left(\\frac {d\\mu_\\theta}{d\\theta}\\right)^2.</math>\n===Putting both sides back together===\nWe have:\n:<math>\\frac 1 2 \\mathcal I_X(\\theta)\n   \\ge \\frac 1 {2\\mathrm{Var}(X_\\theta)}\\left(\\frac {d\\mu_\\theta}{d\\theta}\\right)^2,</math>\nwhich can be rearranged as:\n:<math>\\mathrm{Var}(X_\\theta) \\ge \\frac{(d\\mu_\\theta / d\\theta)^2} {\\mathcal I_X(\\theta)}.</math>\n\n==See also==\n* [[Kullback–Leibler divergence]]\n* [[Cramér–Rao bound]]\n* [[Fisher information]]\n* [[Large deviations theory]]\n* [[Convex conjugate]]\n* [[Rate function]]\n* [[Moment-generating function]]\n\n==Notes and references==\n<references/>\n\n{{DEFAULTSORT:Kullback's Inequality}}\n[[Category:Information theory]]\n[[Category:Statistical inequalities]]\n[[Category:Estimation theory]]"
    },
    {
      "title": "Popoviciu's inequality on variances",
      "url": "https://en.wikipedia.org/wiki/Popoviciu%27s_inequality_on_variances",
      "text": "In [[probability theory]], '''Popoviciu's inequality''', named after [[Tiberiu Popoviciu]], is an [[upper bound]] on the [[variance]] ''σ²'' of any bounded [[probability distribution]].  Let ''M'' and ''m'' be upper and lower bounds on the values of any [[random variable]] with a particular probability distribution.  Then Popoviciu's inequality states:<ref name=Popoviciu>{{cite journal|author=Popoviciu, T.|year=1935|title= Sur les équations algébriques ayant toutes leurs racines réelles|journal= Mathematica (Cluj) |volume=9|pages= 129–145}}</ref>\n\n: <math> \\sigma^2 \\le \\frac14 ( M - m )^2. </math>\n\nThis equality holds precisely when half of the probability is concentrated at each of the two bounds.\n\nSharma ''et al''. have sharpened Popoviciu's inequality:<ref name=\"Sharma et al.\">{{cite journal|author=Sharma, R., Gupta, M., Kapoor, G.|year=2010|title= Some better bound on variance with applications|journal= Journal of Mathematical Inequalities |volume=4|pages= 355–363 |doi=10.7153/jmi-04-32 }}</ref>\n\n: <math> {\\sigma^2 + \\left( \\frac \\text {Third central moment} {2\\sigma^2} \\right)^2} \\le \\frac14 (M - m)^2. </math>\n\nIf the sample size is finite then the [[von Szokefalvi Nagy inequality]]<ref name=Nagy1918>Nagy JVS (1918) Uber algebraische Gleichungen mit lauter reellen Wurzeln, Jahresbericht der deutschen mathematiker-Vereingung, 27:37–43</ref> gives a lower bound to the variance\n\n: <math> \\sigma^2 \\ge \\frac{ ( M - m )^2} { 2n }</math>\n\nwhere ''n'' is the sample size.\n\nPopoviciu's inequality is weaker than the [[Bhatia&ndash;Davis inequality]] which states\n\n: <math> \\sigma^2 \\le ( M - \\mu )( \\mu - m ) </math>\n\nwhere ''μ'' is the expectation of the random variable. \n\nA  lower bound for the variance based on the Bhatia&ndash;Davis inequality has been found by Agarwal ''et al''<ref name=Agarwal2005>Agarwal RP, Barnett NS, Cerone P and Dragomir SS (2005) A survey on some inequalities for expectation and variance. Computers and mathematics with applications 49 (2005) 429-480</ref>\n\n: <math> ( M - \\mu )( \\mu - m ) - \\frac{ ( M - m )^3 }{ 6 }\\le \\sigma^2 </math>\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Popoviciu's Inequality On Variances}}\n[[Category:Theory of probability distributions]]\n[[Category:Statistical inequalities]]\n[[Category:Statistical deviation and dispersion]]\n\n\n{{probability-stub}}"
    },
    {
      "title": "Samuelson's inequality",
      "url": "https://en.wikipedia.org/wiki/Samuelson%27s_inequality",
      "text": "In [[statistics]], '''Samuelson's inequality''', named after the economist [[Paul Samuelson]],<ref>{{cite journal |first=Paul |last=Samuelson |title=How Deviant Can You Be? |journal=[[Journal of the American Statistical Association]] |volume=63 |issue=324 |year=1968 |pages=1522&ndash;1525 |jstor=2285901 |doi=10.2307/2285901 }}</ref> also called the '''Laguerre&ndash;Samuelson inequality''',<ref name=Jensen>{{cite thesis |type=MSc |last=Jensen |first=Shane Tyler |date=1999 |title=The Laguerre&ndash;Samuelson Inequality with Extensions and Applications in Statistics and Matrix Theory |publisher=Department of Mathematics and Statistics, [[McGill University]] |url=http://www.collectionscanada.gc.ca/obj/s4/f2/dsk1/tape10/PQDD_0027/MQ50799.pdf }}</ref><ref>{{cite book |first=Shane T. |last=Jensen |first2=George P. H. |last2=Styan |year=1999 |chapter=Some Comments and a Bibliography on the Laguerre-Samuelson Inequality with Extensions and Applications in Statistics and Matrix Theory |title=Analytic and Geometric Inequalities and Applications |location= |publisher= |pages=151–181 |doi=10.1007/978-94-011-4577-0_10 }}</ref> after the mathematician [[Edmond Laguerre]], states that every one of any collection ''x''<sub>1</sub>,&nbsp;...,&nbsp;''x''<sub>''n''</sub>, is within {{radic|''n''&nbsp;&minus;&nbsp;1}} uncorrected sample [[standard deviation]]s of their sample mean.\n\n==Statement of the inequality==\nIf we let\n\n: <math> \\overline{x} = \\frac{x_1+\\cdots+x_n}{n} </math>\n\nbe the sample [[mean]] and\n\n: <math> s = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_i - \\overline{x})^2 } </math>\n\nbe the standard deviation of the sample, then\n\n: <math> \\overline{x} - s\\sqrt{n-1} \\le x_j \\le \\overline{x} + s\\sqrt{n-1}\\qquad \\text{for } j = 1,\\dots,n. </math><ref>{{cite book |title=Advances in Inequalities from Probability Theory and Statistics |first=Neil S. |last=Barnett |first2=Sever Silvestru |last2=Dragomir |location= |publisher=Nova Publishers |year=2008 |isbn=978-1-60021-943-6 |page=164 }}</ref>\n\nEquality holds on the left (or right) for <math>x_j</math> [[if and only if]] all the ''n''&nbsp;&minus;&nbsp;1 <math>x_i</math>s other than <math>x_j</math> are equal to each other and greater (smaller) than <math>x_j.</math><ref name=Jensen/>\n\n==Comparison to Chebyshev's inequality==\n{{Main|Chebychev's inequality#Samuelson's inequality}}\n\n[[Chebyshev's inequality]] locates a certain fraction of the data within certain bounds, while Samuelson's inequality locates ''all'' the data points within certain bounds.\n\nThe bounds given by Chebyshev's inequality are unaffected by the number of data points, while for Samuelson's inequality the bounds loosen as the sample size increases. Thus for large enough data sets, Chebychev's inequality is more useful.\n\n==Applications==\n{{expand section|date=July 2017}}\n\nSamuelson's inequality may be considered a reason why [[studentized residuals|studentization of residuals]] should be done [[Studentized residual#Internal and external studentization|externally]].\n\n==Relationship to polynomials==\n\nSamuelson was not the first to describe this relationship: the first  was probably [[Laguerre]] in 1880 while investigating the [[Equation solving|root]]s (zeros) of [[polynomial]]s.<ref name=Jensen/><ref name=Laguerre1880>Laguerre E. (1880) Mémoire pour obtenir par approximation les racines d'une équation algébrique qui a toutes les racines réelles. Nouv Ann Math 2<sup>e</sup> série, 19, 161–172, 193–202</ref>\n  \nConsider a polynomial with all roots real:\n \n: <math> a_0x^n + a_1x^{n-1} + \\cdots + a_{n-1}x + a_n = 0 </math>\n \t\nWithout loss of generality let <math>a_0 = 1</math> and let\n\n: <math> t_1 = \\sum x_i </math> and <math> t_2 = \\sum x_i^2 </math>\n \t\nThen\n\n: <math> a_1 = - \\sum x_i = -t_1 </math>\n\nand\n\n: <math> a_2 = \\sum x_ix_j = \\frac{t_1^2 - t_2}{2} \\qquad \\text{ where } i < j </math>\n\nIn terms of the coefficients\n\n: <math> t_2 = a_1^2 - 2a_2 </math>\n\nLaguerre showed that the roots of this polynomial were bounded by\n\n: <math> -a_1 / n \\pm b \\sqrt{n - 1} </math>\n\nwhere\n\n: <math> b = \\frac{\\sqrt{nt_2 - t_1^2}}{n} = \\frac{\\sqrt{na_1^2 - a_1^2 - 2na_2}}{n} </math>\n\nInspection shows that <math>-\\tfrac{a_1}{n}</math> is the [[mean]] of the roots and that ''b'' is the standard deviation of the roots.\n\nLaguerre failed to notice this relationship with the means and standard deviations of the roots, being more interested in the bounds themselves. This relationship permits a rapid estimate of the bounds of the roots and may be of use in their location.\n\nWhen the coefficients <math> a_1 </math> and <math> a_2 </math> are both zero no information can be obtained about the location of the roots, because not all roots are real (as can be seen from [[Descartes' rule of signs]]) unless the constant term is also zero.\n\n== References ==\n{{reflist}}\n\n[[Category:Statistical inequalities]]"
    },
    {
      "title": "Equivalence relation",
      "url": "https://en.wikipedia.org/wiki/Equivalence_relation",
      "text": "{{about|the mathematical concept|the patent doctrine|Doctrine of equivalents}}\n{{redirect|Equivalency||Equivalence (disambiguation){{!}}Equivalence}}\n[[File:Set partitions 5; matrices.svg|right|thumb|The [[Bell number|52]] equivalence relations on a 5-element set depicted as 5×5 [[logical matrix|logical matrices]] (colored fields, including those in light gray, stand for ones; white fields for zeros.) The row and column indices of nonwhite cells are the related elements, while the different colors, other than light gray, indicate the  equivalence classes (each light gray cell is its own equivalence class).]]\n\nIn [[mathematics]], an '''equivalence relation''' is a [[binary relation]] that is [[reflexive relation|reflexive]], [[symmetric relation|symmetric]] and [[transitive relation|transitive]]. The relation \"is equal to\" is the canonical example of an equivalence relation, where for any objects {{mvar|a}}, {{mvar|b}}, and {{mvar|c}}:\n* {{math|''a'' {{=}} ''a''}} (reflexive property),\n* if {{math|''a'' {{=}} ''b''}} then {{math|''b'' {{=}} ''a''}} (symmetric property), and\n* if {{math|''a'' {{=}} ''b''}} and {{math|''b'' {{=}} ''c''}} then {{math|''a'' {{=}} ''c''}} (transitive property).\n\nAs a consequence of the reflexive, symmetric, and transitive properties, any equivalence relation provides a [[partition of a set|partition]] of the underlying set into disjoint [[equivalence classes]]. Two elements of the given set are equivalent to each other if and only if they belong to the same equivalence class.\n\n== Notation ==\nVarious notations are used in the literature to denote that two elements {{math|''a''}} and {{math|''b''}} of a set are equivalent with respect to an equivalence relation {{math|''R''}}; the most common are \"{{math|''a'' ~ ''b''}}\" and \"{{math|''a'' ≡ ''b''}}\", which are used when {{math|''R''}} is implicit, and variations of \"{{math|''a'' ~<sub>''R''</sub> ''b''}}\", \"{{math|''a'' ≡<sub>''R''</sub> ''b''}}\", or \"{{math|''aRb''}}\" to specify {{math|''R''}} explicitly. Non-equivalence may be written \"{{math|''a'' ≁ ''b''}}\" or \"<math>a \\not\\equiv b</math>\".\n\n== Definition ==\nA given [[binary relation]] ~ on a set ''X'' is said to be an equivalence relation [[if and only if]] it is reflexive, symmetric and transitive. That is, for all ''a'', ''b'' and ''c'' in ''X'':\n*''a'' ~ ''a''. ([[reflexive relation|Reflexivity]])\n*''a'' ~ ''b'' if and only if ''b'' ~ ''a''. ([[symmetric relation|Symmetry]])\n*if ''a'' ~ ''b'' and ''b'' ~ ''c'' then ''a'' ~ ''c''. ([[transitive relation|Transitivity]])\n\n''X'' together with the relation ~ is called a [[setoid]]. The [[equivalence class]] of <math>a</math> under ~, denoted <math>[a]</math>, is defined as <math>[a] = \\{b\\in X \\mid a\\sim b\\}</math>.\n\n== Examples ==\n\n=== Simple example ===\nLet the set <math>\\{a, b, c\\}</math> have the equivalence relation <math>\\{(a, a), (b, b), (c, c), (b, c), (c, b)\\}</math>. The following sets are [[equivalence classes]] of this relation:\n\n: <math>[a] = \\{a\\}, ~~~~ [b] = [c] = \\{b, c\\}</math>.\n\nThe set of all equivalence classes for this relation is <math>\\{\\{a\\}, \\{b, c\\}\\}</math>. This set is a [[partition of a set|partition]] of the set <math>\\{a, b, c\\}</math>.\n\n=== Equivalence relations ===\n\nThe following are all equivalence relations:\n* \"Is equal to\" on the set of numbers.  For example, <math>\\tfrac{1}{2}</math> is equal to <math>\\tfrac{4}{8}</math>.\n* \"Has the same birthday as\" on the set of all people.\n* \"Is [[Similarity (geometry)|similar]] to\" on the set of all [[triangle (geometry)|triangle]]s.\n* \"Is [[Congruence (geometry)|congruent]] to\" on the set of all [[triangle (geometry)|triangle]]s.\n* \"Is congruent to, [[modular arithmetic|modulo]] ''n''\" on the [[integers]].\n* \"Has the same [[image (mathematics)|image]] under a [[function (mathematics)|function]]\" on the elements of the [[domain (mathematics)|domain of the function]].\n* \"Has the same absolute value\" on the set of real numbers\n* \"Has the same cosine\" on the set of all angles.\n\n=== Relations that are not equivalences ===\n* The relation \"≥\" between real numbers is reflexive and transitive, but not symmetric. For example, 7 ≥ 5 does not imply that 5 ≥ 7.  It is, however, a [[total order]].\n* The relation \"has a [[common factor]] greater than 1 with\" between [[natural numbers]] greater than 1, is reflexive and symmetric, but not transitive. (Example: The natural numbers 2 and 6 have a common factor greater than 1, and 6 and 3 have a common factor greater than 1, but 2 and 3 do not have a common factor greater than 1).\n* The [[empty relation]] ''R'' on a [[non-empty]] set ''X'' (i.e. ''aRb'' is never true) is [[vacuously true|vacuously]] symmetric and transitive, but not reflexive. (If ''X'' is also empty then ''R'' ''is'' reflexive.)\n* The relation \"is approximately equal to\" between real numbers, even if more precisely defined, is not an equivalence relation, because although reflexive and symmetric, it is not transitive, since multiple small changes can accumulate to become a big change. However, if the approximation is defined asymptotically, for example by saying that two functions ''f'' and ''g'' are approximately equal near some point if the limit of ''f − g'' is 0 at that point, then this defines an equivalence relation.\n\n==Connections to other relations==\n\n*A [[partial order]] is a relation that is reflexive, ''[[antisymmetric relation|antisymmetric]]'', and transitive.\n*[[Equality (mathematics)|Equality]] is both an equivalence relation and a partial order. Equality is also the only relation on a set that is reflexive, symmetric and antisymmetric. In [[algebraic expression]]s, equal variables may be [[substitution (algebra)|substituted]] for one another, a facility that is not available for equivalence related variables. The [[equivalence class]]es of an equivalence relation can substitute for one another, but not individuals within a class.\n*A [[strict partial order]] is irreflexive, transitive, and [[asymmetric relation|asymmetric]].\n*A [[partial equivalence relation]] is transitive and symmetric. Transitive and symmetric imply reflexive [[if and only if]] for all ''a ''∈ ''X'', there exists a ''b ''∈ ''X'' such that ''a ''~ ''b''.\n*A reflexive and symmetric relation is a [[dependency relation]], if finite, and a [[tolerance relation]] if infinite.\n*A [[preorder]] is reflexive and transitive.\n*A [[congruence relation]] is an equivalence relation whose domain ''X'' is also the underlying set for an [[algebraic structure]], and which respects the additional structure. In general, congruence relations play the role of [[Kernel (algebra)|kernels]] of homomorphisms, and the quotient of a structure by a congruence relation can be formed. In many important cases congruence relations have an alternative representation as substructures of the structure on which they are defined. E.g. the congruence relations on groups correspond to the [[normal subgroup]]s.\n*Any equivalence relation is the negation of an [[apartness relation]], though the converse statement only holds in [[classical mathematics]] (as opposed to [[constructive mathematics]]), since it is equivalent to the [[law of excluded middle]].\n*A [[serial relation]] ~ satisfies &forall; ''a'' &exist; ''b'' &nbsp; ''a'' ~ ''b''. Evidently it is sufficient for a serial relation ~ to be symmetric and transitive for it also to be reflexive. Indeed, since the relation is serial, every element is in the relation. Then, using symmetry, reflexivity can be concluded from transitivity by identifying the first and third variables in the transitive axiom. Therefore, an equivalence relation may be alternatively defined as a symmetric, transitive, serial relation.\n\n== Well-definedness under an equivalence relation ==\n\nIf ~ is an equivalence relation on ''X'', and ''P''(''x'') is a property of elements of ''X'', such that whenever ''x'' ~ ''y'', ''P''(''x'') is true if ''P''(''y'') is true, then the property ''P'' is said to be [[well-defined]] or a ''class invariant'' under the relation ~.\n\nA frequent particular case occurs when ''f'' is a function from ''X'' to another set ''Y''; if ''x''<sub>1</sub> ~ ''x''<sub>2</sub> implies ''f''(''x''<sub>1</sub>) = ''f''(''x''<sub>2</sub>) then ''f'' is said to be a ''morphism'' for ~, a ''class invariant under'' ~, or simply ''invariant under'' ~. This occurs, e.g. in the character theory of finite groups. The latter case with the function ''f'' can be expressed by a commutative triangle. See also [[invariant (mathematics)|invariant]].  Some authors use \"compatible with ~\" or just \"respects ~\" instead of \"invariant under ~\".\n\nMore generally, a function may map equivalent arguments (under an equivalence relation ~<sub>A</sub>) to equivalent values (under an equivalence relation ~<sub>B</sub>).  Such a function is known as a morphism from ~<sub>A</sub> to ~<sub>B</sub>.\n\n== Equivalence class, quotient set, partition ==\nLet <math>a,b\\in X</math>. Some definitions:\n\n=== Equivalence class ===\n{{main|Equivalence class}}\nA subset ''Y'' of ''X'' such that ''a'' ~ ''b'' holds for all ''a'' and ''b'' in ''Y'', and never for ''a'' in ''Y'' and ''b'' outside ''Y'', is called an '''equivalence class''' of ''X'' by ~. Let <math>[a] := \\{x \\in X \\mid a \\sim x\\}</math> denote the equivalence class to which ''a'' belongs. All elements of ''X'' equivalent to each other are also elements of the same equivalence class.\n\n=== Quotient set ===\n{{main|Quotient set}}\nThe set of all possible equivalence classes of ''X'' by ~, denoted <math> X/\\mathord{\\sim} := \\{[x] \\mid x \\in X\\}</math>, is the '''quotient set''' of ''X'' by ~. If ''X'' is a [[topological space]], there is a natural way of transforming ''X''/~ into a topological space; see [[Quotient space (topology)|quotient space]] for the details.\n\n=== Projection ===\n{{main|Projection (relational algebra)}}\nThe '''projection''' of ~ is the function <math>\\pi: X \\to X/\\mathord{\\sim} </math> defined by <math>\\pi(x) = [x]</math> which maps elements of ''X'' into their respective equivalence classes by ~.\n\n:'''Theorem''' on [[Projection (set theory)|projection]]s:<ref>[[Garrett Birkhoff]] and [[Saunders Mac Lane]], 1999 (1967). ''Algebra'', 3rd ed. p.&nbsp;35, Th. 19. Chelsea.</ref>  Let the function ''f'': ''X'' &rarr; ''B'' be such that ''a'' ~ ''b'' &rarr; ''f''(''a'') = ''f''(''b''). Then there is a unique function ''g'' : ''X/~'' &rarr; ''B'', such that ''f'' = ''g''&pi;. If ''f'' is a [[surjection]] and ''a'' ~ ''b'' &harr; ''f''(''a'') = ''f''(''b''), then ''g'' is a [[bijection]].\n\n=== Equivalence kernel ===\nThe '''equivalence kernel''' of a function ''f'' is the equivalence relation ~ defined by <math>x\\sim y \\iff f(x) = f(y)</math>. The equivalence kernel of an [[Injective function|injection]] is the [[identity relation]].\n\n=== Partition ===\n{{main|Partition of a set}}\nA '''partition''' of ''X'' is a set ''P'' of nonempty subsets of ''X'', such that every element of ''X'' is an element of a single element of ''P''. Each element of ''P'' is a ''cell'' of the partition. Moreover, the elements of ''P'' are [[pairwise disjoint]] and their [[Union (set theory)|union]] is ''X''.\n\n==== Counting possible partitions ====\nLet ''X'' be a finite set with ''n'' elements. Since every equivalence relation over ''X'' corresponds to a partition of ''X'', and vice versa, the number of possible equivalence relations on ''X'' equals the number of distinct partitions of ''X'', which is the ''n''th [[Bell numbers|Bell number]] ''B<sub>n</sub>'':\n: <math>B_n = \\frac{1}{e}\\sum_{k=0}^\\infty \\frac{k^n}{k!},</math>\nwhere the [[Dobinski's formula|above]] is one of the ways to write the ''n''th [[Bell number]].\n\n==Fundamental theorem of equivalence relations==\n\nA key result links equivalence relations and partitions:<ref>Wallace, D. A. R., 1998. ''Groups, Rings and Fields''. p.&nbsp;31, Th. 8. Springer-Verlag.</ref><ref>Dummit, D. S., and Foote, R. M., 2004. ''Abstract Algebra'', 3rd ed. p.&nbsp;3, Prop. 2. John Wiley & Sons.</ref><ref>[[Karel Hrbacek]] & [[Thomas Jech]] (1999) ''Introduction to Set Theory'', 3rd edition, pages 29–32, [[Marcel Dekker]]</ref>\n*An equivalence relation ~ on a set ''X'' partitions ''X''.\n*Conversely, corresponding to any partition of ''X'', there exists an equivalence relation ~ on ''X''.\nIn both cases, the cells of the partition of ''X'' are the equivalence classes of ''X'' by ~. Since each element of ''X'' belongs to a unique cell of any partition of ''X'', and since each cell of the partition is identical to an [[equivalence class]] of ''X'' by ~, each element of ''X'' belongs to a unique equivalence class of ''X'' by ~. Thus there is a natural [[bijection]] between the set of all possible equivalence relations on ''X'' and the set of all partitions of ''X''.\n\n== Comparing equivalence relations ==\n{{see also|Partition of a set#Refinement of partitions}}\nIf ~ and ≈ are two equivalence relations on the same set ''S'', and ''a''~''b'' implies ''a''≈''b'' for all ''a'',''b'' ∈ ''S'', then ≈ is said to be a '''coarser''' relation than ~, and ~ is a '''finer''' relation than ≈.  Equivalently,\n* ~ is finer than ≈ if every equivalence class of ~ is a subset of an equivalence class of ≈, and thus every equivalence class of ≈ is a union of equivalence classes of ~.\n* ~ is finer than ≈ if the partition created by ~ is a refinement of the partition created by ≈.\n\nThe equality equivalence relation is the finest equivalence relation on any set, while the trivial relation that makes all pairs of elements related is the coarsest.\n\nThe relation \"~ is finer than ≈\" on the collection of all equivalence relations on a fixed set is itself a partial order relation, which makes the collection a [[geometric lattice]].<ref>{{citation|title=Lattice Theory|volume=25|series=Colloquium Publications|publisher=American Mathematical Society|first=Garrett|last=Birkhoff|authorlink=Garrett Birkhoff|edition=3rd|year=1995|isbn=9780821810255}}. Sect. IV.9, Theorem 12, page 95</ref>\n\n== Generating equivalence relations ==\n\nGiven any binary relation <math>A \\subset X \\times X</math> on <math> X </math>,  the '''equivalence relation generated by <math>A</math>''' is the intersection of the equivalence relations on <math>X</math> that contain <math> A</math>. (Since <math>X \\times X</math> is an equivalence relation, the intersection is nontrivial.)\n\n* Given any set ''X'', there is an equivalence relation over the set [''X'' → ''X''] of all possible functions ''X''→''X''. Two such functions are deemed equivalent when their respective sets of [[fixpoint]]s have the same [[cardinality]], corresponding to cycles of length one in a [[permutation]]. Functions equivalent in this manner form an equivalence class on [''X'' → ''X''], and these equivalence classes partition [''X'' → ''X''].\n* An equivalence relation ~ on ''X'' is the [[equivalence relation#Equivalence kernel|equivalence kernel]] of its [[surjective]] [[equivalence relation#Projection|projection]] π : ''X'' → ''X''/~.<ref>[[Garrett Birkhoff]] and [[Saunders Mac Lane]], 1999 (1967). ''Algebra'', 3rd ed. p.&nbsp;33, Th. 18. Chelsea.</ref> Conversely, any [[surjection]] between sets determines a partition on its [[Relation (mathematics)|domain]], the set of [[preimage]]s of [[Singleton (mathematics)|singleton]]s in the [[codomain]]. Thus an equivalence relation over ''X'', a partition of ''X'', and a projection whose domain is ''X'', are three equivalent ways of specifying the same thing.\n* The intersection of any collection of equivalence relations over ''X'' (binary relations viewed as a [[subset]] of ''X'' × ''X'') is also an equivalence relation. This yields a convenient way of generating an equivalence relation: given any binary relation ''R'' on ''X'', the equivalence relation ''generated by R'' is the smallest equivalence relation containing ''R''. Concretely, ''R'' generates the equivalence relation ''a'' ~ ''b'' [[if and only if]] there exist elements ''x''<sub>1</sub>, ''x''<sub>2</sub>, ..., ''x''<sub>''n''</sub> in ''X'' such that ''a'' = ''x''<sub>1</sub>, ''b'' = ''x''<sub>''n''</sub>, and (''x''<sub>''i''</sub>, ''x''<sub>''i''+1</sub>) ∈ ''R'' or (''x''<sub>''i''+1</sub>, ''x''<sub>''i''</sub>) ∈ ''R'', ''i'' = 1, ..., ''n''−1.{{paragraph}} Note that the equivalence relation generated in this manner can be trivial. For instance, the equivalence relation ~ generated by any [[total order]] on ''X'' has exactly one equivalence class, ''X'' itself, because ''x'' ~ ''y'' for all ''x'' and ''y''. As another example, any subset of the [[identity relation]] on ''X'' has equivalence classes that are the singletons of ''X''.\n* Equivalence relations can construct new spaces by \"gluing things together.\" Let ''X'' be the unit [[Cartesian square]] [0, 1] × [0, 1], and let ~ be the equivalence relation on ''X'' defined by (''a'', 0) ~ (''a'', 1) for all ''a'' ∈ [0, 1] and (0, ''b'') ~ (1, ''b'') for all ''b'' ∈ [0, 1]. Then the [[quotient space (topology)|quotient space]] ''X''/~ can be naturally identified ([[homeomorphism]]) with a [[torus]]: take a square piece of paper, bend and glue together the upper and lower edge to form a cylinder, then bend the resulting cylinder so as to glue together its two open ends, resulting in a torus.\n\n==Algebraic structure==\nMuch of mathematics is grounded in the study of equivalences, and [[order relation]]s. [[Lattice (order)|Lattice theory]] captures the mathematical structure of order relations. Even though equivalence relations are as ubiquitous in mathematics as order relations, the algebraic structure of equivalences is not as well known as that of orders. The former structure draws primarily on [[group theory]] and, to a lesser extent, on the theory of lattices, [[category theory|categories]], and [[groupoid]]s.\n\n===Group theory===\nJust as [[order relation]]s are grounded in [[Partially ordered set|ordered sets]], sets closed under pairwise [[supremum]] and [[infimum]], equivalence relations are grounded in [[partition of a set|partitioned sets]], which are sets closed under [[bijection]]s that preserve partition structure. Since all such bijections map an equivalence class onto itself, such bijections are also known as [[permutation]]s. Hence [[permutation group]]s (also known as [[Group action (mathematics)|transformation groups]]) and the related notion of [[orbit (group theory)|orbit]] shed light on the mathematical structure of equivalence relations.\n\nLet '~' denote an equivalence relation over some nonempty set ''A'', called the [[universe (mathematics)|universe]] or underlying set. Let ''G'' denote the set of bijective functions over ''A'' that preserve the partition structure of ''A'': ∀''x'' ∈ ''A'' ∀''g'' ∈ ''G'' (''g''(''x'') ∈ [''x'']). Then the following three connected theorems hold:<ref>Rosen (2008), pp.&nbsp;243–45. Less clear is §10.3 of [[Bas van Fraassen]], 1989. ''Laws and Symmetry''. Oxford Univ. Press.</ref>\n* ~ partitions ''A'' into equivalence classes. (This is the ''Fundamental Theorem of Equivalence Relations,'' mentioned above);\n* Given a partition of ''A'', ''G'' is a transformation group under composition, whose orbits are the [[partitions of a set|cells]] of the partition;{{#tag:ref|\n''Proof''.<ref>Bas van Fraassen, 1989. ''Laws and Symmetry''. Oxford Univ. Press: 246.</ref> Let [[function composition]] interpret group multiplication, and function inverse interpret group inverse. Then ''G'' is a group under composition, meaning that  ∀''x'' ∈ ''A'' ∀''g'' ∈ ''G'' ([''g''(''x'')] = [''x'']), because ''G'' satisfies the following four conditions:\n* ''G is closed under composition''. The composition of any two elements of ''G'' exists, because the [[domain (mathematics)|domain]] and [[codomain]] of any element of ''G'' is ''A''. Moreover, the composition of bijections is [[bijective]];<ref>Wallace, D. A. R., 1998. ''Groups, Rings and Fields''. Springer-Verlag: 22, Th. 6.</ref>\n* ''Existence of [[identity function]]''. The [[identity function]], ''I''(''x'') = ''x'', is an obvious element of ''G'';\n* ''Existence of [[inverse function]]''. Every [[bijective function]] ''g'' has an inverse ''g''<sup>&minus;1</sup>, such that ''gg''<sup>−1</sup> = ''I'';\n* ''Composition [[associativity|associates]]''. ''f''(''gh'') = (''fg'')''h''. This holds for all functions over all domains.<ref>Wallace, D. A. R., 1998. ''Groups, Rings and Fields''.  Springer-Verlag: 24, Th. 7.</ref>\nLet ''f'' and ''g'' be any two elements of ''G''. By virtue of the definition of ''G'', [''g''(''f''(''x''))] = [''f''(''x'')] and [''f''(''x'')] = [''x''], so that [''g''(''f''(''x''))] = [''x'']. Hence ''G'' is also a transformation group (and an [[automorphism group]]) because function composition preserves the partitioning of ''A''.\n\n<math>\\square</math>}}\n\n* Given a transformation group ''G'' over ''A'', there exists an equivalence relation ~ over ''A'', whose equivalence classes are the orbits of ''G''.<ref>Wallace, D. A. R., 1998. ''Groups, Rings and Fields''. Springer-Verlag: 202, Th. 6.</ref><ref>Dummit, D. S., and Foote, R. M., 2004. ''Abstract Algebra'', 3rd ed. John Wiley & Sons: 114, Prop. 2.</ref>\n\nIn sum, given an equivalence relation ~ over ''A'', there exists a [[transformation group]] ''G'' over ''A'' whose orbits are the equivalence classes of ''A'' under ~.\n\nThis transformation group characterisation of equivalence relations differs fundamentally from the way [[lattice (order)|lattices]] characterize order relations. The arguments of the lattice theory operations [[meet (mathematics)|meet]] and [[Join (mathematics)|join]] are elements of some universe ''A''. Meanwhile, the arguments of the transformation group operations [[function composition|composition]] and [[inverse function|inverse]] are elements of a set of [[bijections]], ''A'' → ''A''.\n\nMoving to groups in general, let ''H'' be a [[subgroup]] of some [[group (mathematics)|group]] ''G''. Let ~ be an equivalence relation on ''G'', such that ''a'' ~ ''b'' ↔ (''ab''<sup>−1</sup> ∈ ''H''). The equivalence classes of ~&mdash;also called the orbits of the [[Group action (mathematics)|action]] of ''H'' on ''G''&mdash;are the right '''[[coset]]s''' of ''H'' in ''G''. Interchanging ''a'' and ''b'' yields the left cosets.\n\nRelated thinking can be found in Rosen (2008: chpt. 10).\n\n===Categories and groupoids===\nLet ''G'' be a set and let \"~\" denote an equivalence relation over ''G''. Then we can form a [[groupoid]] representing this equivalence relation as follows. The objects are the elements of ''G'', and for any two elements ''x'' and ''y'' of ''G'', there exists a unique morphism from ''x'' to ''y'' [[if and only if]] ''x''~''y''.\n\nThe advantages of regarding an equivalence relation as a special case of a groupoid include:\n*Whereas the notion of \"free equivalence relation\" does not exist, that of a [[free object|free groupoid]] on a [[directed graph]] does. Thus it is meaningful to speak of a \"presentation of an equivalence relation,\" i.e., a presentation of the corresponding groupoid;\n* Bundles of groups, [[Group action (mathematics)|group action]]s, sets, and equivalence relations can be regarded as special cases of the notion of groupoid, a point of view that suggests a number of analogies;\n*In many contexts \"quotienting,\" and hence the appropriate equivalence relations often called [[Congruence relation|congruences]], are important. This leads to the notion of an internal groupoid in a category.<ref>Borceux, F. and Janelidze, G., 2001. ''Galois theories'', Cambridge University Press, {{ISBN|0-521-80309-8}}</ref>\n\n===Lattices===\nThe possible equivalence relations on any set ''X'', when ordered by [[set inclusion]], form a [[complete lattice]], called '''Con''' ''X'' by convention. The canonical [[map (mathematics)|map]] '''ker''': ''X''^''X'' → '''Con''' ''X'', relates the [[monoid]] ''X''^''X'' of all [[function (mathematics)|function]]s on ''X'' and '''Con''' ''X''. '''ker''' is [[surjective]] but not [[injective]]. Less formally, the equivalence relation '''ker''' on ''X'', takes each function ''f'': ''X''→''X'' to its [[kernel (algebra)|kernel]] '''ker''' ''f''. Likewise, '''ker(ker)''' is an equivalence relation on ''X''^''X''.\n\n==Equivalence relations and mathematical logic==\nEquivalence relations are a ready source of examples or counterexamples. For example, an equivalence relation with exactly two infinite equivalence classes is an easy example of a theory which is ω-[[Morley's categoricity theorem|categorical]], but not categorical for any larger [[cardinal number]].\n\nAn implication of [[model theory]] is that the properties defining a relation can be proved independent of each other (and hence necessary parts of the definition) if and only if, for each property, examples can be found of relations not satisfying the given property while satisfying all the other properties. Hence the three defining properties of equivalence relations can be proved mutually independent by the following three examples:\n* ''Reflexive and transitive'':  The relation ≤ on '''N'''. Or any [[preorder]];\n* ''Symmetric and transitive'': The relation ''R'' on '''N''', defined as ''aRb'' ↔ ''ab'' ≠ 0. Or any [[partial equivalence relation]];\n* ''Reflexive and symmetric'': The relation ''R'' on '''Z''', defined as ''aRb'' ↔ \"''a'' &minus; ''b'' is divisible by at least one of 2 or 3.\" Or any [[dependency relation]].\n\nProperties definable in [[first-order logic]] that an equivalence relation may or may not possess include:\n*The number of [[equivalence classes]] is finite or infinite;\n*The number of equivalence classes equals the (finite) natural number ''n'';\n*All equivalence classes have infinite [[cardinality]];\n*The number of elements in each equivalence class is the natural number ''n''.\n\n== Euclidean relations ==\n[[Euclid]]'s ''[[Euclid's Elements|The Elements]]'' includes the following \"Common Notion 1\":\n\n:Things which equal the same thing also equal one another.\n\nNowadays, the property described by Common Notion 1 is called [[Euclidean relation|Euclidean]] (replacing \"equal\" by \"are in relation with\"). By \"relation\" is meant a [[binary relation]], in which ''aRb'' is generally distinct from ''bRa''. A Euclidean relation thus comes in two forms:\n\n:(''aRc'' ∧ ''bRc'') → ''aRb'' (Left-Euclidean relation)\n:(''cRa'' ∧ ''cRb'') → ''aRb'' (Right-Euclidean relation)\n\nThe following theorem connects [[Euclidean relation]]s and equivalence relations:\n\n; Theorem\n: If a relation is (left or right) Euclidean and [[reflexive relation|reflexive]], it is also symmetric and transitive.\n; Proof for a left-Euclidean relation:\n: (''aRc'' ∧ ''bRc'') → ''aRb'' [''a/c''] = (''aRa'' ∧ ''bRa'') → ''aRb'' [''reflexive''; erase '''T'''∧] = ''bRa'' → ''aRb''. Hence ''R'' is [[symmetric relation|symmetric]].\n: (''aRc'' ∧ ''bRc'') → ''aRb'' [''symmetry''] = (''aRc'' ∧ ''cRb'') → ''aRb''. Hence ''R'' is [[transitive relation|transitive]]. <math>_{\\Box}</math>\n\nwith an analogous proof for a right-Euclidean relation. Hence an equivalence relation is a relation that is ''Euclidean'' and ''reflexive''. ''The Elements'' mentions neither symmetry nor reflexivity, and Euclid probably would have deemed the reflexivity of equality too obvious to warrant explicit mention.\n\n==See also==\n* [[Apartness relation]]\n* [[Conjugacy class]]\n* [[Equipollence (geometry)]]\n* [[Topological conjugacy]]\n* [[Up to]]\n\n==Notes==\n{{reflist|30em}}\n\n==References==\n*Brown, Ronald, 2006. ''[http://arquivo.pt/wayback/20160514115224/http://www.bangor.ac.uk/r.brown/topgpds.html Topology and Groupoids.]'' Booksurge LLC. {{ISBN|1-4196-2722-8}}.\n*Castellani, E., 2003, \"Symmetry and equivalence\" in Brading, Katherine, and E. Castellani, eds., ''Symmetries in Physics: Philosophical Reflections''. Cambridge Univ. Press: 422-433.\n*[[Robert Dilworth]] and Crawley, Peter, 1973. ''Algebraic Theory of Lattices''. Prentice Hall. Chpt. 12 discusses how equivalence relations arise in [[lattice (order)|lattice]] theory.\n*Higgins, P.J., 1971. ''[http://www.emis.de/journals/TAC/reprints/articles/7/tr7abs.html Categories and groupoids.]'' Van Nostrand. Downloadable since 2005 as a TAC Reprint.\n*[[John Lucas (philosopher)|John Randolph Lucas]], 1973. ''A Treatise on Time and Space''. London: Methuen. Section 31.\n*Rosen, Joseph (2008) ''Symmetry Rules: How Science and Nature are Founded on Symmetry''. Springer-Verlag. Mostly chpts. 9,10.\n*[[Raymond Wilder]] (1965) ''Introduction to the Foundations of Mathematics'' 2nd edition, Chapter 2-8: Axioms defining equivalence, pp 48&ndash;50, [[John Wiley & Sons]].\n\n==External links==\n* {{springer|title=Equivalence relation|id=p/e036030}}\n* [[Alexander Bogomolny|Bogomolny, A.]], \"[http://www.cut-the-knot.org/blue/equi.shtml Equivalence Relationship]\" [[cut-the-knot]]. Accessed 1 September 2009\n* [https://web.archive.org/web/20130509233055/http://planetmath.org/equivalencerelation Equivalence relation] at PlanetMath\n* {{OEIS el|1=A231428|2=Binary matrices representing equivalence relations}}\n\n{{DEFAULTSORT:Equivalence Relation}}\n\n[[Category:Reflexive relations]]\n[[Category:Symmetric relations]]\n[[Category:Transitive relations]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Reflexive relation",
      "url": "https://en.wikipedia.org/wiki/Reflexive_relation",
      "text": "In [[mathematics]], a [[binary relation]] ''R'' over a set ''X'' is '''reflexive''' if every element of ''X'' is related to itself.<ref>Levy 1979:74</ref><ref>Relational Mathematics, 2010</ref> Formally, this may be written {{math|[[universal quantification|∀]]''x'' ∈ ''X'' : ''x R x''}}.\n\nAn example of a reflexive relation is the relation \"[[equality (mathematics)|is equal to]]\" on the set of [[real number]]s, since every real number is equal to itself.  A reflexive relation is said to have the '''reflexive property''' or is said to possess '''reflexivity'''.  Along with [[symmetric relation|symmetry]] and [[transitive relation|transitivity]], reflexivity is one of three properties defining [[equivalence relation]]s.\n\n==Related terms==\nA binary relation is called '''{{visible anchor|irreflexive|irreflexivity}}''', or '''anti-reflexive''', if it doesn't relate any element to itself. An example is the \"greater than\" relation (''x'' > ''y'') on the [[real number]]s. Not every relation which is not reflexive is irreflexive; it is possible to define relations where some elements are related to themselves but others are not (i.e., neither all nor none are). For example, the binary relation \"the product of ''x'' and ''y'' is even\" is reflexive on the set of [[even number]]s, irreflexive on the set of odd numbers, and neither reflexive nor irreflexive on the set of [[natural number]]s.\n\nA relation ~ on a set ''X'' is called '''quasi-reflexive''' if every element that is related to some element is also related to itself, formally: {{math|∀ ''x'', ''y'' ∈ ''X'' : ''x'' ~ ''y'' ⇒ (''x'' ~ ''x'' ∧ ''y'' ~ ''y'')}}. An example is the relation \"has the same limit as\" on the set of sequences of real numbers: not every sequence has a limit, and thus the relation is not reflexive, but if a sequence has the same limit as some sequence, then it has the same limit as itself. It does make sense to distunguish '''left''' and '''right quasi-reflexivity''', defined by {{math|∀ ''x'', ''y'' ∈ ''X'' : ''x'' ~ ''y'' ⇒ ''x'' ~ ''x'' }}<ref>The [https://www.britannica.com/topic/formal-logic/The-predicate-calculus#ref534730 Encyclopedia Britannica] calls this property quasi-reflexivity.</ref> and {{math|∀ ''x'', ''y'' ∈ ''X'' : ''x'' ~ ''y'' ⇒ ''y'' ~ ''y''}}, repsectively. For example, a left [[Euclidean relation]] is always left, but not necessarily right, quasi-reflexive.\n\nA relation ~ on a set ''X'' is called '''coreflexive''' if for all ''x'' and ''y'' in ''X'' it holds that if ''x'' ~ ''y'' then ''x'' = ''y''.<ref>Fonseca de Oliveira, J. N., & Pereira Cunha Rodrigues, C. D. J. (2004). Transposing Relations: From Maybe Functions to Hash Tables. In Mathematics of Program Construction (p. 337).</ref> An example of a coreflexive relation is the relation on integers in which each odd number is related to itself and there are no other relations. The equality relation is the only example of a both reflexive and coreflexive relation, and any coreflexive relation is a subset of the identity relation. The union of a coreflexive and a transitive relation is always transitive. \n\nA reflexive relation on a nonempty set ''X'' can neither be irreflexive, nor [[asymmetric relation|asymmetric]], nor [[antitransitive]].\n\nThe '''[[reflexive closure]]''' ≃ of a binary relation ~ on a set ''X'' is the smallest reflexive relation on ''X'' that is a [[superset]] of ~. Equivalently, it is the union of ~ and the [[Equality (mathematics)|identity relation]] on ''X'', formally: (≃) = (~) ∪ (=). For example, the reflexive closure of (<) is (≤).\n\nThe '''reflexive reduction''', or '''irreflexive kernel''', of a binary relation ~ on a set ''X'' is the smallest relation ≆ such that ≆ shares the same reflexive closure as ~. It can be seen in a way as the opposite of the reflexive closure. It is equivalent to the complement of the identity relation on ''X'' with regard to ~, formally: (≆) = (~) \\ (=). That is, it is equivalent to ~ except for where ''x''~''x'' is true. For example, the reflexive reduction of (≤) is (<).\n\n==Examples==\n{{multiple image\n| image1=GreaterThanOrEqualTo.png\n| width1=250\n| image2=GreaterThan.png\n| width2=200\n}}\n\n'''Examples&nbsp;of reflexive relations include:'''\n* \"is equal to\" ([[equality (mathematics)|equality]])\n* \"is a [[subset]] of\" (set inclusion)\n* \"divides\" ([[divisor|divisibility]])\n* \"is greater than or equal to\"\n* \"is less than or equal to\"\n'''Examples of irreflexive relations include:'''\n* \"is not equal to\"\n* \"is [[coprime]] to\" (for the integers>1, since 1 is coprime to itself)\n* \"is a proper subset of\"\n* \"is greater than\"\n* \"is less than\"\n\n{{clear}}\n\n==Number of reflexive relations==\nThe number of reflexive relations on an ''n''-element set is 2<sup>''n''<sup>2</sup>−''n''</sup>.<ref>On-Line Encyclopedia of Integer Sequences [[OEIS:A053763|A053763]]</ref>\n\n{{Number of relations}}\n\n==Philosophical logic==\nAuthors in [[philosophical logic]] often use different terminology.\nReflexive relations in the mathematical sense are called '''totally reflexive''' in philosophical logic, and quasi-reflexive relations are called '''reflexive'''.<ref>{{cite book|author1=Alan Hausman |author2=Howard Kahane |author3=Paul Tidman | title=Logic and Philosophy — A Modern Introduction| year=2013| publisher=Wadsworth| isbn=1-133-05000-X}} Here: p.327-328</ref><ref>{{cite book|author1=D.S. Clarke |author2=Richard Behling | title=Deductive Logic — An Introduction to Evaluation Techniques and Logical Theory| year=1998| publisher=University Press of America| isbn=0-7618-0922-8}} Here: p.187</ref>\n\n==See also==\n*[[Coreflexive relation]] &mdash; a relation that satisfies ∀''x'',''y'': ''xRy'' ⇒ ''x''=''y''\n*[[Antisymmetric relation]] &mdash; a relation that satisfies ∀''x'',''y'': ''xRy'' ∧ ''yRx'' ⇒ ''x''=''y''\n\n==Notes==\n{{reflist}}\n\n==References==\n* Levy, A. (1979) ''Basic Set Theory'', Perspectives in Mathematical Logic, Springer-Verlag. Reprinted 2002, Dover. {{isbn|0-486-42079-5}}\n* Lidl, R. and Pilz, G. (1998). ''Applied abstract algebra'', [[Undergraduate Texts in Mathematics]], Springer-Verlag. {{isbn|0-387-98290-6}}\n* Quine, W. V. (1951). ''Mathematical Logic'', Revised Edition. Reprinted 2003, Harvard University Press. {{isbn|0-674-55451-5}}\n* Gunther Schmidt, 2010. ''Relational Mathematics''. Cambridge University Press, {{isbn|978-0-521-76268-7}}.\n\n==External links==\n* {{springer|title=Reflexivity|id=p/r080590}}\n\n[[Category:Reflexive relations]]"
    },
    {
      "title": "Tolerance relation",
      "url": "https://en.wikipedia.org/wiki/Tolerance_relation",
      "text": "In mathematics, a '''tolerance relation''' is a [[binary relation|relation]] that is [[reflexive relation|reflexive]] and [[symmetric relation|symmetric]], but not necessarily [[transitive relation|transitive]]; a set ''X'' that possesses a tolerance relation can be described as a '''tolerance space'''.<ref>{{Cite journal|last=Sossinsky|first=Alexey|date=1986-02-01|title=Tolerance space theory and some applications|url=https://www.researchgate.net/publication/225214345_Tolerance_space_theory_and_some_applications|journal=[[Acta Applicandae Mathematicae]]|volume=5|issue=2|pages=137-167|doi=10.1007/BF00046585|via=}}</ref> Tolerance relations provide a convenient general tool for studying [[indiscernibility]]/indistinguishability phenomena. The importance of those for [[Mathematics]] had been first recognized by [[Henri_Poincaré|Poincaré]].<ref>{{cite book|last1=Poincare|first1=H.|title=Science and Hypothesis|date=1905|publisher=The Walter Scott Publishing Co., Ltd.|edition=with a preface by J.Larmor|location=New York: 3 East 14th Street|pages=22-23}}</ref>\n\n== See also ==\n*[[Quasitransitive relation]] &mdash; a generalization to formalize indifference in social choice theory\n*[[Rough set]]\n\n==References==\n{{Reflist}}\n\n==Further reading==\n*Gerasin, S. N., Shlyakhov, V. V., and Yakovlev, S. V. 2008. Set coverings and tolerance relations. Cybernetics and Sys. Anal. 44, 3 (May 2008), 333&ndash;340. {{DOI|10.1007/s10559-008-9007-y}}\n*Hryniewiecki, K. 1991, [http://mizar.org/fm/1991-2/pdf2-1/toler_1.pdf Relations of Tolerance], FORMALIZED MATHEMATICS, Vol. 2, No. 1, January–February  1991.\n\n[[Category:Reflexive relations]]\n[[Category:Symmetric relations]]\n[[Category:Approximations]]\n\n{{math-stub}}"
    },
    {
      "title": "Nominal scale",
      "url": "https://en.wikipedia.org/wiki/Nominal_scale",
      "text": "#REDIRECT [[Level of measurement#Nominal level]]\n\n{{R to section}}\n\n[[Category:Symmetric relations]]<!-- Ideally [[Category:Equivalence relations]] -->"
    },
    {
      "title": "Partial equivalence relation",
      "url": "https://en.wikipedia.org/wiki/Partial_equivalence_relation",
      "text": "In [[mathematics]], a '''partial equivalence relation''' (often abbreviated as '''PER''', in older literature also called '''restricted equivalence relation''') <math>R</math> on a set <math>X</math> is a relation that is ''[[symmetric relation|symmetric]]'' and ''[[transitive relation|transitive]]''.  In other words, it holds for all <math>a, b, c \\in X</math> that:\n\n# if <math>a R b</math>, then <math>b R a</math> (symmetry)\n# if <math>a R b</math> and <math>b R c</math>, then <math>a R c</math> (transitivity)\n\nIf <math>R</math> is also [[reflexive relation|reflexive]], then <math>R</math> is an [[equivalence relation]].\n\n== Properties and applications ==\nIn a [[set-theoretic]] context, there is a simple structure to the general PER <math>R</math> on <math>X</math>: it is an equivalence relation on the subset <math>Y = \\{ x \\in X | x\\,R\\,x\\} \\subseteq X</math>.  (<math>Y</math> is the subset of <math>X</math> such that in the [[Complement (set theory)|complement]] of <math>Y</math> (<math>X\\setminus Y</math>) no element is related by <math>R</math> to any other.)  By construction, <math>R</math> is reflexive on <math>Y</math> and therefore an equivalence relation on <math>Y</math>.  Notice that <math>R</math> is actually only true on elements of <math>Y</math>: if <math>x R y</math>, then <math>y R x</math> by symmetry, so <math>x R x</math> and <math>y R y</math> by transitivity. Conversely, given a subset ''Y'' of ''X'', any equivalence relation on ''Y'' is automatically a PER on ''X''. Hence, in set theory one typically studies the equivalence relation associated with a PER, rather than the PER itself.\n\nBut in [[type theory]], [[constructive mathematics]] and their applications to [[computer science]], constructing analogues of subsets is often problematic<ref>http://ieeexplore.ieee.org/document/5135/</ref>—in these contexts PERs are therefore more commonly used, particularly to define [[setoid]]s, sometimes called partial setoids. Forming a partial setoid from a type and a PER is analogous to forming subsets and quotients in classical set-theoretic mathematics. \n\nEvery partial equivalence relation is a [[difunctional|difunctional relation]], but the converse does not hold.\n\nThe algebraic notion of [[Congruence relation|congruence]] can also be generalized to partial equivalences, yielding the notion of [[subcongruence]], i.e. a [[homomorphic relation]] that is symmetric and transitive, but not necessarily reflexive.<ref>{{cite book|editors=Aldo Ursini, Paulo Agliano|title=Logic and Algebra|year=1996|publisher=CRC Press|isbn=978-0-8247-9606-8|pages=161–180|author=J. Lambek|chapter=The Butterfly and the Serpent}}</ref>\n\n==Examples==\n\nA simple example of a PER that is ''not'' an equivalence relation is the [[empty relation]] <math>R=\\emptyset</math> (unless <math>X=\\emptyset</math>, in which case the empty relation ''is'' an equivalence relation (and is the only relation on <math>X</math>)).\n\n===Kernels of partial functions===\nFor another example of a PER, consider a set <math>A</math> and a [[partial function]] <math>f</math> that is defined on some elements of <math>A</math> but not all.  Then the relation <math>\\approx</math> defined by \n: <math>x \\approx y</math> if and only if <math>f</math> is defined at <math>x</math>, <math>f</math> is defined at <math>y</math>, and <math>f(x) = f(y)</math>\nis a partial equivalence relation but not an equivalence relation.  It possesses the symmetry and transitivity properties, but it is not reflexive since if <math>f(x)</math> is not defined then <math>x \\not\\approx x</math> &mdash; in fact, for such an <math>x</math> there is no <math>y \\in A</math> such that <math>x \\approx y</math>.  (It follows immediately that the subset of <math>A</math> for which <math>\\approx</math> is an equivalence relation is precisely the subset on which <math>f</math> is defined.)\n\n===Functions respecting equivalence relations===\nLet ''X'' and ''Y'' be sets equipped with equivalence relations (or PERs) <math>\\approx_X, \\approx_Y</math>. For <math>f,g : X \\to Y</math>, define <math>f \\approx g</math> to mean:\n\n: <math>\\forall x_0 \\; x_1, \\quad x_0 \\approx_X x_1 \\Rightarrow f(x_0) \\approx_Y g(x_1)</math>\n\nthen <math>f \\approx f</math> means that ''f'' induces a well-defined function of the quotients <math>X / \\approx_X \\; \\to \\; Y /\n\\approx_Y</math>. Thus, the PER <math>\\approx</math> captures both the idea of ''definedness'' on the quotients and of two functions inducing the same function on the quotient.\n\n===Equality of [[IEEE floating point]] values===\nIEEE 754:2008 floating point standard defines an \"EQ\" relation for floating point values. This predicate is symmetrical and transitive, but is not reflexive because of the presence of [NaN] values that are not EQ to themselves.\n\n==References==\n{{reflist}}\n* Mitchell, John C. ''[http://portal.acm.org/citation.cfm?id=237842 Foundations of programming languages.]'' MIT Press, 1996.\n* D.S. Scott. \"Data types as lattices\". ''SIAM Journ. Comput.'', 3:523-587, 1976.\n\n==See also==\n*[[Equivalence relation]]\n*[[Binary relation]]\n\n{{DEFAULTSORT:Partial Equivalence Relation}}\n[[Category:Symmetric relations]]\n[[Category:Transitive relations]]\n[[Category:Equivalence (mathematics)]]"
    },
    {
      "title": "Symmetric power",
      "url": "https://en.wikipedia.org/wiki/Symmetric_power",
      "text": "In mathematics, the '''''n''-th symmetric power''' of an object ''X'' is the quotient of the ''n''-fold product <math>X^n:=X \\times \\cdots \\times X</math> by the permutation action of the [[symmetric group]] <math>\\mathfrak{S}_n</math>.\n\nMore precisely, the notion exists at least in the following three areas:\n*In [[linear algebra]], the ''n''-th symmetric power of a vector space ''V'' is the vector subspace of the [[symmetric algebra]] of ''V'' consisting of degree-''n'' elements (here the product is a [[tensor product]]).\n*In [[algebraic topology]], the ''n''-th symmetric power of a [[topological space]] ''X'' is the [[quotient space (topology)|quotient space]] <math>X^n/\\mathfrak{S}_n</math>, as in the beginning of this article.\n*In [[algebraic geometry]], a symmetric power is defined in a way similar to that in algebraic topology. For example, if <math>X = \\operatorname{Spec}(A)</math> is an [[affine variety]], then the [[GIT quotient]] <math>\\operatorname{Spec}((A \\otimes_k \\dots \\otimes_k A)^{\\mathfrak{S}_n})</math> is the ''n''-th symmetric power of ''X''.\n\n== References ==\n* {{Citation | last1=Eisenbud | first1=David | authorlink1=David Eisenbud| last2=Harris | first2=Joe | authorlink2=Joe Harris (mathematician)| title=3264 and All That: A Second Course in Algebraic Geometry }}\n\n== External links ==\n*{{cite web|last=Hopkins|first=Michael J.|authorlink=Michael J. Hopkins|url=http://www.math.harvard.edu/~lurie/ThursdayFall2017/Lecture13-Symmetric-power.pdf|title=Symmetric powers of the sphere}}\n\n\n{{math-stub}}\n\n\n\n[[Category:Symmetric relations]]"
    },
    {
      "title": "Symmetric relation",
      "url": "https://en.wikipedia.org/wiki/Symmetric_relation",
      "text": "{{unref|date=February 2019}}\nA '''symmetric relation''' is a type of [[binary relation]]. An example is the relation \"is equal to\", because if ''a'' = ''b'' is true then ''b'' = ''a'' is also true. Formally, a binary relation ''R'' over a [[Set (mathematics)|set]] ''X'' is symmetric [[if and only if]]:\n:<math>\\forall a, b \\in X(a R b \\Leftrightarrow b R a) .</math>\n\nIf ''R''<sup>T</sup> represents the [[converse relation|converse]] of ''R'', then ''R'' is symmetric if and only if ''R'' = ''R''<sup>T</sup>.\n\nSymmetry, along with [[Reflexive relation|reflexivity]] and [[Transitive relation|transitivity]], are the three defining properties of an [[equivalence relation]].\n\n==Examples==\n===In mathematics===\n* \"is equal to\" ([[equality (mathematics)|equality]]) (whereas \"is less than\" is not symmetric)\n* \"is [[comparability|comparable]] to\", for elements of a [[partially ordered set]]\n* \"... and ... are odd\":\n::::::[[Image:Bothodd.png]]\n\n===Outside mathematics===\n* \"is married to\" (in most legal systems)\n* \"is a fully biological sibling of\"\n* \"is a [[homophone]] of\"\n* \"is co-worker of\"\n* \"is teammate of\"\n\n==Relationship to asymmetric and antisymmetric relations==\n\nBy definition, a nonempty relation cannot be both symmetric and [[asymmetric relation|asymmetric]] (where if ''a'' is related to ''b'', then ''b'' cannot be related to ''a'' (in the same way)). However, a relation can be neither symmetric nor asymmetric, which is the case for \"is less than or equal to\" and \"preys on\").\n\nSymmetric and [[antisymmetric relation|antisymmetric]] (where the only way ''a'' can be related to ''b'' and ''b'' be related to ''a'' is if ''a'' = ''b'') are actually independent of each other, as these examples show.\n\n{| class=\"wikitable\"\n|+Mathematical examples\n|-\n| || '''Symmetric''' || '''Not symmetric'''\n|-\n| '''Antisymmetric''' || [[equality (mathematics)|equality]] || \"is less than or equal to\"\n|-\n| '''Not antisymmetric''' || [[congruence relation|congruence]] in [[modular arithmetic]] || \"is divisible by\", over the set of integers\n|}\n\n{| class=\"wikitable\"\n|+Non-mathematical examples\n|-\n| || '''Symmetric''' || '''Not symmetric'''\n|-\n| '''Antisymmetric''' || \"is the same person as, and is married\"  || \"is the plural of\"\n|-\n| '''Not antisymmetric''' || \"is a full biological sibling of\" || \"preys on\"\n|}\n\n==Additional aspects==\n\nA symmetric relation that is also [[transitive relation|transitive]] and [[reflexive relation|reflexive]] is an [[equivalence relation]].\n\nOne way to conceptualize a symmetric relation in graph theory is that a symmetric relation is an edge, with the edge's two vertices being the two entities so related. Thus, symmetric relations and undirected graphs are combinatorially equivalent objects.\n\n==See also==\n\n* [[Asymmetric relation]]\n* [[Antisymmetric relation]]\n* [[Commutative property]]\n* [[Symmetry in mathematics]]\n* [[Symmetry]]\n\n[[Category:Symmetric relations]]"
    },
    {
      "title": "Transitive relation",
      "url": "https://en.wikipedia.org/wiki/Transitive_relation",
      "text": "{{refimprove|date=October 2013}}\nIn [[mathematics]], a [[binary relation]] {{math|''R''}} over a [[Set (mathematics)|set]] {{math|''X''}} is '''transitive''' if whenever an element {{math|''a''}} is related to an element {{math|''b''}} and {{math|''b''}} is related to an element {{math|''c''}} then {{math|''a''}} is also related to {{math|''c''}}. Transitivity (or ''transitiveness'') is a key property of both [[partial order]] relations and [[equivalence relation]]s.\n\n== Formal definition ==\nIn terms of [[set theory]], the binary relation {{mvar|R}} defined on the set {{mvar|X}} is a ''transitive relation'' if,<ref>{{harvnb|Smith|Eggen|St. Andre|2006|loc=p. 145}}</ref>\n:for all {{math|''a'', ''b'', ''c'' ∈ ''X''}}, if {{math|''a R b''}} and {{math|''b R c''}}, then {{math|''a R c''}}.\nOr, in symbolic form,\n:<math>\\forall a,b,c \\in X: (aRb \\wedge bRc) \\Rightarrow aRc.</math>\nWhere, for example, {{math|''a R b''}} is the [[infix notation]] for {{math|(''a'', ''b'') ∈ ''R''}}.\n\n==Examples==\n<!--\n   '''a  {{resize|200%|<———}}  b '''                '''b {{resize|200%|<———}}  c'''               '''a  {{resize|200%|———>}}  c'''\n                                  \n                                          {{resize|350%|↗}}                 {{resize|350%|↘ ↗}}\n          '''c'''                             '''a'''                            ''b''\n     (TRANSITIVE)                (NOT TRANSITIVE)               (TRANSITIVE)\n-->\n\"Is greater than\", \"is at least as great as\", and \"is equal to\" ([[equality (mathematics)|equality]]) are transitive relations on various sets, for instance, the set of real numbers or the set of natural numbers:\n\n: whenever ''x'' &gt; ''y'' and ''y'' &gt; ''z'', then also ''x'' &gt; ''z''\n: whenever ''x'' &ge; ''y'' and ''y'' &ge; ''z'', then also ''x'' &ge; ''z''\n: whenever ''x'' = ''y'' and ''y'' = ''z'', then also ''x'' = ''z''.\n\nOn the other hand, \"is the mother of\" is not a transitive relation, because if Alice is the mother of Brenda, and Brenda is the mother of Claire, then Alice is not the mother of Claire. What is more, it is [[antitransitive]]: Alice can ''never'' be the mother of Claire.\n\nMore examples of transitive relations:\n* \"is a [[subset]] of\" ([[set inclusion]])\n* \"divides\" ([[divisor|divisibility]])\n* \"implies\" ([[material conditional|implication]])\n\nThe [[empty relation]] on any non-empty set {{math|''X''}} is transitive,<ref>{{harvnb|Smith|Eggen|St. Andre|2006|loc=p. 146}}</ref><ref>https://courses.engr.illinois.edu/cs173/sp2011/Lectures/relations.pdf</ref> because the [[Material conditional|conditional]] defining a transitive relation is logically true if the [[Antecedent (logic)|antecedent]] is false, resulting in the statement being true ([[vacuous truth]]). \n\nA relation {{math|''R''}} containing only one [[ordered pair]] is transitive for the same reason.\n\n== Properties ==\n\n=== Closure properties ===\n* The [[inverse relation|inverse]] (converse) of a transitive relation is always transitive. For instance, knowing that \"is a [[subset]] of\" is transitive and \"is a [[superset]] of\" is its inverse, one can conclude that the latter is transitive as well.\n\n* The intersection of two transitive relations is always transitive. For instance, knowing that \"was born before\" and \"has the same first name as\" are transitive, one can conclude that \"was born before and also has the same first name as\" is also transitive.\n\n* The union of two transitive relations need not be transitive. For instance, \"was born before or has the same first name as\" is not a transitive relation, since e.g. [[Herbert Hoover]] is related to [[Franklin D. Roosevelt]], which is in turn related to [[Franklin Pierce]], while Hoover is not related to Franklin Pierce.\n\n* The complement of a transitive relation need not be transitive. For instance, while \"equal to\" is transitive, \"not equal to\" is only transitive on sets with at most one element.\n\n=== Other properties ===\nA transitive relation is [[asymmetric relation|asymmetric]] if and only if it is [[irreflexive relation|irreflexive]].<ref>{{cite book|last1=Flaška|first1=V.|last2=Ježek|first2=J.|last3=Kepka|first3=T.|last4=Kortelainen|first4=J.|title=Transitive Closures of Binary Relations I|year=2007|publisher=School of Mathematics - Physics Charles University|location=Prague|page=1|url=http://www.karlin.mff.cuni.cz/~jezek/120/transitive1.pdf|deadurl=yes|archiveurl=https://web.archive.org/web/20131102214049/http://www.karlin.mff.cuni.cz/~jezek/120/transitive1.pdf|archivedate=2013-11-02|df=}} Lemma 1.1 (iv). Note that this source refers to asymmetric relations as \"strictly antisymmetric\".</ref>\n\nA transitive relation need not be [[Reflexive relation|reflexive]]. When it is, it is called a [[preorder]]. For example, on set ''X'' ={1,2,3}, the relations;\n\n* ''R'' = {(1,1),(2,2),(3,3),(1,3),(3,2)} is reflexive, but not transitive, as element (1,2) is absent,\n \n* ''R'' = {(1,1),(2,2),(3,3),(1,3)} is reflexive as well as transitive, so, it is a preorder,\n \n* ''R'' = {(1,1),(2,2),(3,3)} is reflexive as well as transitive, another preorder.\n\n==Transitive extensions and transitive closure==\n{{main|Transitive closure}}\n\nLet {{mvar|R}} be a binary relation on set {{mvar|X}}. The ''transitive extension'' of {{mvar|R}}, denoted {{math|''R''<sub>1</sub>}}, is the smallest binary relation on {{mvar|X}} such that {{math|''R''<sub>1</sub>}} contains {{mvar|R}}, and if {{math|(''a'', ''b'') ∈ ''R''}} and {{math|(''b'', ''c'') ∈ ''R''}} then {{math|(''a'', ''c'') ∈ ''R''<sub>1</sub>}}.<ref>{{harvnb|Liu|1985|loc=p. 111}}</ref> For example, suppose {{mvar|X}} is a set of towns, some of which are connected by roads. Let {{mvar|R}} be the relation on towns where {{math|(''A'', ''B'') ∈ ''R''}} if there is a road directly linking town {{mvar|A}} and town {{mvar|B}}. This relation need not be transitive. The transitive extension of this relation can be defined by {{math|(''A'', ''C'') ∈ ''R''<sub>1</sub>}} if you can travel between towns {{mvar|A}} and {{mvar|C}} by using at most two roads.\n\nIf a relation is transitive then its transitive extension is itself, that is, if {{mvar|R}} is a transitive relation then {{math|1=''R''<sub>1</sub> = ''R''}}.\n\nThe transitive extension of {{math|''R''<sub>1</sub>}} would be denoted by {{math|''R''<sub>2</sub>}}, and continuing in this way, in general, the transitive extension of {{math|''R''<sub>''i''</sub>}} would be {{math|''R''<sub>''i'' + 1</sub>}}. The ''transitive closure'' of {{mvar|R}}, denoted by {{math|''R''*}} or {{math|''R''<sup>∞</sup>}} is the set union of {{mvar|R}}, {{math|''R''<sub>1</sub>}}, {{math|''R''<sub>2</sub>}}, ... .<ref name=Liu112>{{harvnb|Liu|1985|loc=p. 112}}</ref>\n\nThe transitive closure of a relation is a transitive relation.<ref name=Liu112 />\n\nThe relation \"is the mother of\" on a set of people is not a transitive relation. However, in biology the need often arises to consider motherhood over an arbitrary number of generations: the relation \"is a [[matrilinear]] ancestor of\" ''is'' a transitive relation and it is the transitive closure of the relation \"is the mother of\".\n\nFor the example of towns and roads above, {{math|(''A'', ''C'') ∈ ''R''*}} provided you can travel between towns {{mvar|A}} and {{mvar|C}} using any number of roads.\n\n== Relation properties that require transitivity ==\n* [[Preorder]] – a [[reflexive relation|reflexive]] transitive relation\n* [[Partially ordered set|Partial order]] – an [[antisymmetric relation|antisymmetric]] preorder\n* [[Total preorder]] – a [[total relation|total]] preorder\n* [[Equivalence relation]] – a [[symmetric relation|symmetric]] preorder\n* [[Strict weak ordering]] – a strict partial order in which incomparability is an equivalence relation\n* [[Total ordering]] – a [[total relation|total]], [[antisymmetric relation|antisymmetric]] transitive relation\n\n==Counting transitive relations==\n\nNo general formula that counts the number of transitive relations on a finite set {{OEIS|id=A006905}} is known.<ref>Steven R. Finch, [http://www.people.fas.harvard.edu/~sfinch/csolve/posets.pdf \"Transitive relations, topologies and partial orders\"], 2003.</ref> However, there is a formula for finding the number of relations that are simultaneously reflexive, symmetric, and transitive – in other words, [[equivalence relation]]s – {{OEIS|id=A000110}}, those that are symmetric and transitive, those that are symmetric, transitive, and antisymmetric, and those that are total, transitive, and antisymmetric. Pfeiffer<ref>Götz Pfeiffer, \"[http://www.cs.uwaterloo.ca/journals/JIS/VOL7/Pfeiffer/pfeiffer6.html Counting Transitive Relations]\", ''Journal of Integer Sequences'', Vol. 7 (2004), Article 04.3.2.</ref> has made some progress in this direction, expressing relations with combinations of these properties in terms of each other, but still calculating any one is difficult. See also.<ref>Gunnar Brinkmann and Brendan D. McKay,\"[http://cs.anu.edu.au/~bdm/papers/topologies.pdf Counting unlabelled topologies and transitive relations]\"</ref>\n\n{{number of relations}}\n\n== Intransitivity ==\n[[File:Three-part cycle diagram.png|alt=Cycle diagram|thumb|Sometimes, when people are asked their preferences through a series of binary questions, {{clarify span|they|Should probably be singular: different persons having different preferences is not logically impossible.|date=April 2019}} will give logically impossible responses:  1 is better than 2, and 2 is better than 3, but 3 is better than 1.]]\n{{Main|Intransitivity}}\nRelations that are not transitive are called ''intransitive''.  An example is \"''x'' is the successor number of ''y''\". Unexpected examples arise in situations such as political questions or group preferences.<ref>{{Cite news|url=https://www.motherjones.com/kevin-drum/2018/11/preferences-are-not-transitive/|title=Preferences are not transitive|last=Drum|first=Kevin|date=November 2018|work=Mother Jones|access-date=2018-11-29|language=en-US}}</ref>\n\n==See also==\n\n* [[Transitive reduction]]\n* [[Quasitransitive relation]]\n* [[Nontransitive dice]]\n* [[Rational choice theory#Formal statement|Rational choice theory]]\n\n== Notes ==\n{{reflist}}\n\n== References ==\n* {{citation|first=Ralph P.|last=Grimaldi|authorlink=Ralph Grimaldi|title=Discrete and Combinatorial Mathematics|year=1994|publisher=Addison-Wesley|edition=3rd|isbn=0-201-19912-2}}\n* {{citation|first= C.L.|last=Liu|title=Elements of Discrete Mathematics|year=1985|publisher=McGraw-Hill|isbn=0-07-038133-X}}\n*[[Gunther Schmidt]], 2010. ''Relational Mathematics''. Cambridge University Press, {{isbn|978-0-521-76268-7}}.\n* {{citation|first1=Douglas|last1=Smith|first2=Maurice|last2=Eggen|first3=Richard|last3=St.Andre|title=A Transition to Advanced Mathematics|edition=6th|year=2006|publisher=Brooks/Cole|isbn=978-0-534-39900-9}}\n\n==External links==\n* {{springer|title=Transitivity|id=p/t093810}}\n* [http://www.cut-the-knot.org/triangle/remarkable.shtml Transitivity in Action] at [[cut-the-knot]]\n\n{{DEFAULTSORT:Transitive Relation}}\n[[Category:Transitive relations]]\n[[Category:Elementary algebra]]"
    },
    {
      "title": "Approximation",
      "url": "https://en.wikipedia.org/wiki/Approximation",
      "text": "{{for|the sound change|Lenition}}\n{{Refimprove|date=April 2013}}\n{{Certainty}}\n\nAn '''approximation''' is anything that is similar but not exactly [[equality (mathematics)|equal]] to something else.\n\n==Etymology and usage==\nThe word ''approximation'' is derived from [[Latin]] ''approximatus'', from ''proximus'' meaning ''very near'' and the prefix ''ap-'' (''ad-'' before ''p'') meaning ''to''.<ref>The Concise Oxford Dictionary, ''Eighth edition 1990, {{ISBN|0-19-861243-5}}''</ref> Words like ''approximate'', ''approximately'' and ''approximation'' are used especially in technical or scientific contexts. In everyday English, words such as ''roughly'' or ''around'' are used with a similar meaning.<ref>Longman Dictionary of Contemporary English, ''Pearson Education Ltd 2009, {{ISBN|978 1 4082 1532 6}}''</ref> It is often found abbreviated as ''approx.''\n\nThe term can be applied to various properties (e.g., value, quantity, image, description) that are nearly, but not exactly correct; similar, but not exactly the same (e.g., the approximate time was 10 o'clock).\n \nAlthough approximation is most often applied to [[number]]s, it is also frequently applied to such things as [[Function (mathematics)|mathematical functions]], [[shape]]s, and [[physical law]]s.\n\nIn science, approximation can refer to using a simpler process or model when the correct model is difficult to use. An approximate model is used to make calculations easier. Approximations might also be used if incomplete [[information]] prevents use of exact representations.\n\nThe type of approximation used depends on the available [[information]], [[Order of approximation|the degree of accuracy required]], the sensitivity of the problem to this data, and the savings (usually in time and effort) that can be achieved by approximation.\n\n== Mathematics ==\n[[Approximation theory]] is a branch of mathematics, a quantitative part of [[functional analysis]]. [[Diophantine approximation]] deals with approximations of [[real number]]s by [[rational number]]s. Approximation usually occurs when an exact form  or an exact numerical number is unknown or difficult to obtain. However some known form may exist and may be able to represent the real form so that no significant deviation can be found. It also is used when a number is [[Irrational number|not rational]], such as the number [[Pi|π]], which often is shortened to 3.14159, or {{radic|2}} to 1.414.\n\n[[Numerical approximation]]s sometimes result from using a small number of [[Significant figures|significant digits]]. Calculations are likely to involve [[Round-off error|rounding errors]] leading to approximation. [[Logarithm|Log tables]], slide rules and calculators produce approximate answers to all but the simplest calculations. The results of computer calculations are normally an approximation expressed in a limited number of significant digits, although they can be programmed to produce more precise results.<ref>[http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html Numerical Computation Guide]</ref> Approximation can occur when a decimal number cannot be expressed in a finite number of binary digits.\n\nRelated to approximation of functions is the [[Asymptotic analysis|asymptotic]] value of a function, i.e. the value as one or more of a function's parameters becomes arbitrarily large. For example, the sum  (''k''/2)+(''k''/4)+(''k''/8)+...(''k''/2^''n'') is asymptotically equal to ''k''. Unfortunately no consistent notation is used throughout mathematics and some texts will use ≈ to mean approximately equal and ~ to mean asymptotically equal whereas other texts use the symbols the other way around.\n\nAs another example, in order to accelerate the convergence rate of evolutionary algorithms, [[fitness approximation]]—that leads to build model of the fitness function to choose smart search steps—is a good solution.\n\n== Science ==\nApproximation arises naturally in [[scientific experiment]]s. The predictions of a scientific theory can differ from actual measurements. This can be because there are factors in the real situation that are not included in the theory. For example, simple calculations may not include the effect of air resistance. Under these circumstances, the theory is an approximation to reality. Differences may also arise because of limitations in the measuring technique. In this case, the measurement is an approximation to the actual value.\n\nThe [[history of science]] shows that earlier theories and laws can be ''approximations'' to some deeper set of laws. Under the [[correspondence principle]], a new scientific theory should reproduce the results of older, well-established, theories in those domains where the old theories work.<ref>[http://www.britannica.com/EBchecked/topic/138678/correspondence-principle Encyclopædia Britannica]</ref> The old theory becomes an approximation to the new theory.\n\nSome problems in physics are too complex to solve by direct analysis, or progress could be limited by available analytical tools. Thus, even when the exact representation is known, an approximation may yield a sufficiently accurate solution while reducing the complexity of the problem significantly. [[Physicists]] often approximate the [[shape of the Earth]] as a [[sphere]] even though more accurate representations are possible, because many physical characteristics (e.g., [[gravity]]) are much easier to calculate for a sphere than for other shapes.\n\nApproximation is also used to analyze the motion of several planets orbiting a star. This is extremely difficult due to the complex interactions of the planets' gravitational effects on each other.<ref>[http://plus.maths.org/content/mathematical-mysteries-three-body-problem The three body problem]</ref> An approximate solution is effected by performing [[iteration]]s. In the first iteration, the planets' gravitational interactions are ignored, and the star is assumed to be fixed. If a more precise solution is desired, another iteration is then performed, using the positions and motions of the planets as identified in the first iteration, but adding a first-order gravity interaction from each planet on the others. This process may be repeated until a satisfactorily precise solution is obtained.\n\nThe use of [[Perturbation theory|perturbations]] to correct for the errors can yield more accurate solutions. Simulations of the motions of the planets and the star also yields more accurate solutions.\n\nThe most common versions of [[philosophy of science]] accept that empirical [[measurement]]s are always ''approximations''—they do not perfectly represent what is being measured.\n\nThe error-tolerance property of several applications (e.g., graphics applications) allows use of approximation (e.g., lowering the precision of numerical computations) to improve performance and energy efficiency.<ref name=\"surveyACT\">{{cite journal|last1=Mittal|first1=Sparsh|title=A Survey of Techniques for Approximate Computing|journal=ACM Comput. Surv.|date=May 2016|volume=48|issue=4|pages=62:1–62:33|doi=10.1145/2893356|publisher=ACM|language=en|url=https://zenodo.org/record/1236172}}</ref> This approach of using deliberate, controlled approximation for achieving various optimizations is referred to as [[approximate computing]].\n\n==Unicode==\n{{See also|Unicode mathematical operators}}\n\nSymbols used to denote items that are approximately equal are wavy or dotted equals signs.<ref>{{cite web| title =Mathematical Operators – Unicode| url =https://www.unicode.org/charts/PDF/U2200.pdf| accessdate =2013-04-20}}</ref>\n* <span style=\"font-size: 150%;line-height:50%;\">≈</span> ([[Unicode|U]]+2248, ''almost equal to'')\n* <span style=\"font-size: 150%;line-height:50%;\">≉</span> ([[Unicode|U]]+2249, ''not almost equal to'')\n* <span style=\"font-size: 150%;line-height:50%\">[[≃]]</span> (U+2243), a combination of \"≈\" and \"=\", also used to indicate [[≃|asymptotically equal to]]{{clarify|Target article mentions nothing of this symbol|date=January 2016}}\n** <span style=\"font-size: 150%;line-height:50%\">≒</span> (U+2252), which is used like \"<big>≃</big>\" in [[Japanese language|Japan]], [[Taiwanese Mandarin|Taiwan]], and [[Korean language|Korea]]\n** <span style=\"font-size: 150%;line-height:50%\">≓</span> (U+2253), a reversed variation of \"<big>≒</big>\"\n* <span style=\"font-size: 150%;line-height:50%\">[[≅ (disambiguation)|≅]]</span> (U+2245), another combination of \"≈\" and \"=\", which is used to indicate [[isomorphism]] or [[congruence relation|congruence]]\n* <span style=\"font-size: 150%;line-height:50%\">≊</span> (U+224A), yet another combination of \"≈\" and \"=\", used to indicate equivalence or approximate equivalence\n* <span style=\"font-size: 150%;line-height:50%\">∼</span> (U+223C), which is also sometimes used to indicate [[proportionality (mathematics)|proportionality]]\n* <span style=\"font-size: 150%;line-height:50%\">∽</span> (U+223D), which is also sometimes used to indicate [[proportionality (mathematics)|proportionality]]\n* <span style=\"font-size: 150%;line-height:50%\">≐</span> (U+2250, ''approaches the limit''), which can be used to represent the approach of a variable, {{math|y}}, to a [[limit (mathematics)|limit]]; like the common syntax, <math>\\scriptstyle \\lim_{x \\to \\inf} y(x)</math> <small>≐ 0</small>{{citation needed|reason=This is what the unicode spec. implies, but I have no direct confirmation|date=January 2016}}\n\n==LaTeX symbols==\nSymbols used in [[LaTeX]] markup.\n* <math> \\approx </math> (<code>\\approx</code>), usually to indicate approximation between numbers, like <math> \\pi \\approx 3.14</math>.\n* <math> \\not\\approx </math> (<code>\\not\\approx</code>), usually to indicate that numbers are not approximately equal (1 <math> \\not\\approx </math> 2).\n* <math> \\simeq </math> (<code>\\simeq</code>), usually to indicate asymptotic equivalence between functions, like <math> f(n) \\simeq 3n^2 </math>. So writing <math> \\pi \\simeq 3.14 </math> would be wrong, despite wide use.\n* <math> \\sim </math> (<code>\\sim</code>), usually to indicate proportionality between functions, the same  <math> f(n)  </math> of the line above will be <math> f(n) \\sim n^2 </math>.\n* <math> \\cong </math> (<code>\\cong</code>), usually to indicate congruence between figures, like <math> \\Delta ABC \\cong \\Delta A'B'C' </math>.\n\n== See also ==\n{{columns-list|colwidth=22em|\n* [[Approximately equals sign]]\n* [[Approximation error]]\n* [[Congruence relation]]\n* [[Estimation]]\n* [[Fermi estimate]]\n* [[Fitness approximation]]\n* [[Least squares]]\n* [[Linear approximation]]\n* [[Binomial approximation]]\n* [[Newton's method]]\n* [[Numerical analysis]]\n* [[Orders of approximation]]\n* [[Runge–Kutta methods]]\n* [[Successive approximation ADC]]\n* [[Taylor series]]\n* [[Small-angle approximation]]\n* [[Approximate computing]]\n* [[Tolerance relation]]\n* [[Rough set]]\n}}\n\n== References ==\n{{Reflist}}\n\n==External links==\n{{Wiktionary|approximation}}\n*{{Commonscatinline}}\n\n{{Authority control}}\n\n[[Category:Approximations| ]]\n[[Category:Numerical analysis]]\n[[Category:Equivalence (mathematics)]]\n[[Category:Comparison (mathematical)]]\n<!-- this category expressly includes similarities-->"
    },
    {
      "title": "Comparator",
      "url": "https://en.wikipedia.org/wiki/Comparator",
      "text": "{{other uses}}\n[[File:Opamp105.gif|thumb|Figure 1. Illustration of how a comparator works]]\nIn [[electronics]], a '''comparator''' is a device that compares two [[voltage]]s or [[Electric current|currents]] and outputs a digital signal indicating which is larger.  It has two analog input terminals <math>V_+\\,</math> and <math>V_-\\,</math> and one binary digital output <math>V_{\\rm o}\\,</math>.  The output is ideally \n:<math>V_{\\rm o} = \n\\begin{cases} \n1,  & \\mbox{if }V_+ > V_-  \\\\\n0,  & \\mbox{if }V_+ < V_- \n\\end{cases}</math>\nA comparator consists of a specialized high-[[gain (electronics)|gain]] [[differential amplifier]].  They are commonly used in devices that measure and digitize analog signals, such as [[analog-to-digital converter]]s{{clarify|a comparator could be considered a simple ADC, right?|date=May 2017}} (ADCs), as well as [[relaxation oscillator]]s.\n\n== Differential voltage ==\n\nThe differential voltages must stay within the limits specified by the manufacturer. Early integrated comparators, like the LM111 family, and certain high-speed comparators like the LM119 family, require differential voltage ranges substantially lower than the power supply voltages (±15&nbsp;V vs. 36&nbsp;V).<ref>''[http://www.ti.com/lit/ds/symlink/lm111.pdf LM111/LM211/LM311 datasheet]. Texas Instruments. August 2003. Retrieved 2014-07-02.</ref> ''Rail-to-rail'' comparators allow any differential voltages within the power supply range. When powered from a bipolar (dual rail) supply, \n \n:<math>V_{S-} \\le V_+,V_- \\le V_{S+}</math>\n\nor, when powered from a unipolar [[Transistor-transistor logic|TTL]]/[[CMOS]] power supply:\n\n:<math>0 \\le V_+,V_- \\le V_{\\rm cc}</math>\n\nSpecific rail-to-rail comparators with [[Bipolar junction transistor#PNP|p-n-p]] input transistors, like the LM139 family, allow the input potential to drop 0.3&nbsp;volts ''below'' the negative supply rail, but do not allow it to rise above the positive rail.<ref>''[http://www.ti.com/lit/ds/symlink/lm139.pdf LM139/LM239/LM339/LM2901/LM3302 datasheet]. Texas Instruments. August 2012. Retrieved 2014-07-02.</ref> Specific ultra-fast comparators, like the LMH7322, allow input signal to swing below the negative rail ''and'' above the positive rail, although by a narrow margin of only 0.2&nbsp;V.<ref>''[http://www.ti.com/lit/ds/symlink/lmh7322.pdf LMH7322 datasheet]. Texas Instruments. March 2013. Retrieved 2014-07-02.</ref> Differential input voltage (the voltage between two inputs) of a modern rail-to-rail comparator is usually limited only by the full swing of power supply.\n\n==Op-amp voltage comparator==\n\n[[image:Op-Amp Comparator.svg|frame|right|A simple op-amp comparator]]\nAn [[operational amplifier]] (op-amp) has a well balanced difference input and a very high [[gain (electronics)|gain]]. This parallels the characteristics of comparators and can be substituted in applications with low-performance requirements.<ref>Malmstadt, Enke and Crouch, Electronics and Instrumentation for Scientists, The Benjamin/Cummings Publishing Company, Inc., 1981, {{ISBN|0-8053-6917-1}}, Chapter 5.</ref>\n\nA comparator circuit compares two voltages and outputs either a 1 (the voltage at the plus side; VDD in the illustration) or a 0 (the voltage at the negative side) to indicate which is larger. Comparators are often used, for example, to check whether an input has reached some predetermined value. In most cases a comparator is implemented using a dedicated comparator IC, but op-amps may be used as an alternative. Comparator diagrams and op-amp diagrams use the same symbols.\n\nFigure 1 above shows a comparator circuit. Note first that the circuit does not use feedback. The circuit amplifies the voltage difference between Vin and VREF, and it outputs the result at Vout. If Vin is greater than VREF, then voltage at Vout will rise to its positive saturation level; that is, to the voltage at the positive side. If Vin is lower than VREF, then Vout will fall to its negative saturation level, equal to the voltage at the negative side.\n\nIn practice, this circuit can be improved by incorporating a hysteresis voltage range to reduce its sensitivity to noise. The circuit shown in Figure 1, for example, will provide stable operation even when the Vin signal is somewhat noisy.\n\nIn practice, using an [[operational amplifier]] as a comparator presents several disadvantages as compared to using a dedicated comparator:<ref>Ron Mancini, \"[http://www.edn.com/design/analog/4353925/Designing-with-comparators Designing with comparators],\" EDN, March 29, 2001.</ref>\n\n# Op-amps are designed to operate in the linear mode with negative feedback. Hence, an op-amp typically has a lengthy recovery time from saturation. Almost all op-amps have an internal compensation capacitor which imposes [[slew rate]] limitations for high frequency signals. Consequently, an op-amp makes a sloppy comparator with [[propagation delay]]s that can be as long as tens of microseconds.\n# Since op-amps do not have any internal hysteresis, an external hysteresis network is always necessary for slow moving input signals.\n# The quiescent current specification of an op-amp is valid only when the feedback is active. Some op-amps show an increased quiescent current when the inputs are not equal.\n# A comparator is designed to produce well limited output voltages that easily interface with digital logic. Compatibility with digital logic must be verified while using an op-amp as a comparator.\n# Some multiple-section op-amps may exhibit extreme channel-channel interaction when used as comparators.\n# Many op-amps have back to back diodes between their inputs. Op-amp inputs usually follow each other so this is fine. But comparator inputs are not usually the same. The diodes can cause unexpected current through inputs.\n\n== Working ==\n\n\nA dedicated voltage comparator will generally be faster than a general-purpose operational amplifier pressed into service as a comparator. A dedicated voltage comparator may also contain additional features such as an accurate, internal voltage reference, an adjustable [[hysteresis]] and a clock gated input.\n\nA dedicated voltage comparator chip such as LM339 is designed to interface with a digital logic interface (to a [[Transistor-transistor logic|TTL]] or a [[CMOS]]). The output is a binary state often used to interface real world signals to digital circuitry (see [[analog to digital converter]]). If there is a fixed voltage source from, for example, a DC adjustable device in the signal path, a comparator is just the equivalent of a cascade of amplifiers. When the voltages are nearly equal, the output voltage will not fall into one of the logic levels, thus analog signals will enter the digital domain with unpredictable results. To make this range as small as possible, the amplifier cascade is high gain. The circuit consists of mainly [[Bipolar transistor]]s. For very high frequencies, the input [[Electrical impedance|impedance]] of the stages is low. This reduces the saturation of the slow, large [[P-n junction|P-N junction]] bipolar transistors that would otherwise lead to long recovery times. Fast small [[Schottky diode]]s, like those found in binary logic designs, improve the performance significantly though the performance still lags that of circuits with amplifiers using analog signals. Slew rate has no meaning for these devices. For applications in [[flash ADC]]s the distributed signal across eight ports matches the voltage and current gain after each amplifier, and resistors then behave as level-shifters.\n\nThe LM339 accomplishes this with an [[open collector]] output. When the inverting input is at a higher voltage than the non inverting input, the output of the comparator connects to the negative power supply. When the non inverting input is higher than the inverting input, the output is 'floating' (has a very high impedance to ground).\nThe gain of op amp as comparator is given by this equation\nV(out)=V(in)\n\n==Key specifications==\n\nWhile it is easy to understand the basic task of a comparator, that is, comparing two voltages or currents, several parameters must be considered while selecting a suitable comparator:\n\n===Speed and power===\n\nWhile in general comparators are \"fast,\" their circuits are not immune to the classic speed-power tradeoff. High speed comparators use transistors with larger aspect ratios and hence also consume more power.<ref>Rogenmoser, R.; Kaeslin, H, \"The impact of transistor sizing on power efficiency in submicron CMOS circuits,\" Solid-State Circuits, IEEE Journal of Volume 32, Issue 7, Jul 1997 Page(s):1142–1145.</ref> Depending on the application, select either a comparator with high speed or one that saves power. For example, nano-powered comparators in space-saving chip-scale packages (UCSP), DFN or SC70 packages such as [http://www.maxim-ic.com/quick_view2.cfm/qv_pk/4268 MAX9027], [http://arquivo.pt/wayback/20160515060645/http://www.linear.com/pc/productDetail.jsp?navId=H0,C1,C1154,C1002,C1463,P1593 LTC1540], [https://web.archive.org/web/20090503133424/http://www.national.com/pf/LP/LPV7215.html LPV7215], [http://www.maxim-ic.com/quick_view2.cfm/qv_pk/5823 MAX9060] and [http://www.microchip.com/wwwproducts/Devices.aspx?dDocName=en010414 MCP6541] are ideal for ultra-low-power, portable applications. Likewise if a comparator is needed to implement a relaxation oscillator circuit to create a high speed clock signal then comparators having few nano seconds of propagation delay may be suitable. [http://www.analog.com/en/amplifiers-and-comparators/comparators/adcmp572/products/product.html ADCMP572] (CML output), [http://www.ti.com/product/lmh7220 LMH7220] (LVDS Output), [http://www.maxim-ic.com/quick_view2.cfm/qv_pk/1481 MAX999] (CMOS output / TTL output), [https://archive.is/20130127214618/http://www.linear.com/pc/productDetail.jsp?navId=H0,C1,C1154,C1004,C1012,P1817 LT1719] (CMOS output / TTL output), [http://www.maxim-ic.com/quick_view2.cfm/qv_pk/2490/t/al MAX9010] (TTL output), and [http://www.maxim-ic.com/quick_view2.cfm/qv_pk/3400/t/al MAX9601] (PECL output) are examples of some good high speed comparators.\n\n=== Hysteresis ===\n\nA comparator normally changes its output state when the voltage between its inputs crosses through approximately zero volts. Small voltage fluctuations due to noise, always present on the inputs, can cause undesirable rapid changes between the two output states when the input voltage difference is near zero volts. To prevent this output oscillation, a small [[hysteresis]] of a few millivolts is integrated into many modern comparators.<ref>Ron Mancini, \"[http://www.edn.com/article/CA84881.html Adding Hysteresis to comparators] {{webarchive|url=https://web.archive.org/web/20050221054157/http://edn.com/article/CA84881.html |date=2005-02-21 }},\" EDN, May 3, 2001.</ref> \nFor example, the [http://www.linear.com/pc/productDetail.jsp?navId=H0,C1,C1154,C1004,C1012,P38930 LTC6702], [http://www.maxim-ic.com/quick_view2.cfm/qv_pk/2411/t/al MAX9021] and [http://www.maxim-ic.com/quick_view2.cfm/qv_pk/2363/t/al MAX9031] have internal hysteresis desensitizing them from input noise. In place of one switching point, hysteresis introduces two: one for rising voltages, and one for falling voltages. The difference between the higher-level trip value (VTRIP+) and the lower-level trip value (VTRIP-) equals the hysteresis voltage (VHYST).\n\nIf the comparator does not have internal hysteresis or if the input noise is greater than the internal hysteresis then an external hysteresis network can be built using positive feedback from the output to the non-inverting input of the comparator. The resulting [[Schmitt trigger]] circuit gives additional noise immunity and a cleaner output signal. Some comparators such as [https://web.archive.org/web/20090503133351/http://www.national.com/pf/LM/LMP7300.html LMP7300], [http://www.linear.com/pc/productDetail.jsp?navId=H0,C1,C1154,C1004,C1013,P1593 LTC1540], [http://www.maxim-ic.com/quick_view2.cfm/qv_pk/1219 MAX931], [http://www.maxim-ic.com/quick_view2.cfm/qv_pk/1279/t/al MAX971] and [http://www.analog.com/en/power-management/battery-management/adcmp341/products/product.html ADCMP341] also provide the hysteresis control through a separate hysteresis pin. These comparators make it possible to add a programmable hysteresis without feedback or complicated equations. Using a dedicated hysteresis pin is also convenient if the source impedance is high since the inputs are isolated from the hysteresis network.<ref>AN3616, Maxim Integrated Products, [http://www.maxim-ic.com/appnotes.cfm/an_pk/886/ Adding Extra Hysteresis to Comparators].</ref> When hysteresis is added then a comparator cannot resolve signals within the hysteresis band.\n\n===Output type===\n[[File:Dynamic Comparator.png|thumb|right|350px|A Low Power CMOS Clocked Comparator]]\n\nBecause comparators have only two output states, their outputs are near zero or near the supply voltage. Bipolar rail-to-rail comparators have a common-emitter output that produces a small voltage drop between the output and each rail. That drop is equal to the collector-to-emitter voltage of a saturated transistor. When output currents are light, output voltages of CMOS rail-to-rail comparators, which rely on a saturated MOSFET, range closer to the rails than their bipolar counterparts.<ref name=\"test\">AN886, Maxim Integrated Products, [http://www.maxim-ic.com/appnotes.cfm/an_pk/886/ Selecting the Right Comparator].</ref>\n\nOn the basis of outputs, comparators can also be classified as [[open drain]] or [[push–pull output|push–pull]]. Comparators with an open-drain output stage use an external pull up resistor to a positive supply that defines the logic high level. Open drain comparators are more suitable for mixed-voltage system design. Since the output is high impedance for logic level high, open drain comparators can also be used to connect multiple comparators on to a single bus. Push pull output does not need a pull up resistor and can also source current unlike an open drain output.\n\n===Internal reference===\n\nThe most frequent application for comparators is the comparison between a voltage and a stable reference. Most comparator manufacturers also offer comparators in which a reference voltage is integrated on to the chip. Combining the reference and comparator in one chip not only saves space, but also draws less supply current than a comparator with an external reference.<ref name=\"test\"/> ICs with wide range of references are available such as [http://www.maxim-ic.com/quick_view2.cfm/qv_pk/5823/t/al MAX9062] (200 mV reference), [http://arquivo.pt/wayback/20160515060838/http://www.linear.com/pc/productDetail.jsp?navId=H0,C1,C1154,C1004,C1013,P2340 LT6700] (400 mV reference), [http://www.analog.com/en/power-management/battery-management/adcmp350/products/product.html ADCMP350] (600&nbsp;mV reference), [http://www.maxim-ic.com/quick_view2.cfm/qv_pk/4268/t/al MAX9025] (1.236&nbsp;V reference), [http://www.maxim-ic.com/quick_view2.cfm/qv_pk/2122/t/al MAX9040] (2.048&nbsp;V reference), [http://focus.ti.com/docs/prod/folders/print/tlv3012.html TLV3012] (1.24&nbsp;V reference) and [http://www.datasheetcatalog.org/datasheet/stmicroelectronics/9208.pdf TSM109] (2.5&nbsp;V reference).\n\n===Continuous versus clocked===\n\nA continuous comparator will output either a \"1\" or a \"0\" any time a high or low signal is applied to its input and will change quickly when the inputs are updated.  However, many applications only require comparator outputs at certain instances, such as in A/D converters and memory. By only strobing a comparator at certain intervals, higher accuracy and lower power can be achieved with a clocked (or dynamic) comparator structure, also called a latched comparator. Often latched comparators employ strong positive feedback for a \"regeneration phase\" when a clock is high, and have a \"reset phase\" when the clock is low.<ref>\n{{cite book\n | title = Offset Reduction Techniques in High-Speed Analog-to-Digital Converters: Analysis, Design and Tradeoffs\n | author = Pedro M. Figueiredo, João C. Vital\n | publisher = Springer\n | year = 2009\n | isbn = 978-1-4020-9715-7\n | pages = 54–62\n | url = https://books.google.com/books?id=El9Ki0spMEwC&pg=PA55&dq=%22output+voltages+of+the+latched+comparator%22+regeneration+reset+phase#v=onepage&q=%22output%20voltages%20of%20the%20latched%20comparator%22%20regeneration%20reset%20phase&f=false\n }}</ref>  \nThis is in contrast to a continuous comparator, which can only employ weak positive feedback since there is no reset period.\n\n== Applications ==\n{{main|Comparator applications}}\n\n=== Null detectors ===\n\nA null detector is one that functions to identify when a given value is zero. Comparators can be a type of amplifier distinctively for null comparison measurements. It is the equivalent to a very high gain amplifier with well-balanced inputs and controlled output limits. The circuit compares the two input voltages, determining the larger. The inputs are an unknown voltage and a reference voltage, usually referred to as v<sub>u</sub> and v<sub>r</sub>. A reference voltage is generally on the non-inverting input (+), while v<sub>u</sub> is usually on the inverting input (&minus;). (A circuit diagram would display the inputs according to their sign with respect to the output when a particular input is greater than the other.)  The output is either positive or negative, for example ±12&nbsp;V. In this case, the idea is to detect when there is no difference between in the input voltages. This gives the identity of the unknown voltage since the reference voltage is known.\n\nWhen using a comparator as a null detector, there are limits as to the accuracy of the zero value measurable. Zero output is given when the magnitude of the difference in the voltages multiplied by the gain of the amplifier is less than the voltage limits. For example, if the gain of the amplifier is 10<sup>6</sup>, and the voltage limits are ±6&nbsp;V, then no output will be given if the difference in the voltages is less than 6&nbsp;μV. One could refer to this as a sort of uncertainty in the measurement.<ref>{{Citation |title=Electronics and Instrumentation for Scientists |last1=Malmstadt |first1=Howard V. |last2=Enke |first2=Christie G. |last3=Crouch |first3=Stanley R. |publisher=The Benjamin/Cummings Publishing Co |date=January 1981  |isbn=978-0-8053-6917-5 |pages=108–110}}</ref>\n\n=== Zero-crossing detectors ===\n\nFor this type of detector, a comparator detects each time an [[Alternating Current|ac]] pulse changes polarity. The output of the comparator changes state each time the pulse changes its polarity, that is the output is HI (high) for a positive pulse and LO (low) for a negative pulse squares the input signal.<ref>''Electronics and Instrumentation for Scientists.''  Malmstadt, Enke, and Crouch, The Benjamin/Cummings Publishing Co., In., 1981, p.230.</ref>\n\n===Relaxation oscillator===\n\nA comparator can be used to build a [[relaxation oscillator]]. It uses both positive and negative feedback. The positive feedback is a [[Schmitt trigger]] configuration. Alone, the trigger is a [[bistable multivibrator]]. However, the slow [[negative feedback]] added to the trigger by the RC circuit causes the circuit to oscillate automatically. That is, the addition of the RC circuit turns the hysteretic bistable [[multivibrator]] into an [[astable multivibrator]].<ref>Paul Horowitz and Winfield Hill: ''The Art of Electronics'',\nCambridge University Press, Second edition, Cambridge 1989, pp.284–285.</ref>\n\n===Level shifter===\n[[File:DOV-1X - National Semiconductor LM393N on printed circuit board-9800.jpg|thumb|National Semiconductor LM393]]\nThis circuit requires only a single comparator with an open-drain output as in the [http://www.ti.com/product/lm393 LM393], [http://focus.ti.com/docs/prod/folders/print/tlv3011.html TLV3011] or [http://www.maxim-ic.com/quick_view2.cfm/qv_pk/4268/t/al MAX9028]. The circuit provides great flexibility in choosing the voltages to be translated by using a suitable pull up voltage. It also allows the translation of bipolar ±5&nbsp;V logic to unipolar 3&nbsp;V logic by using a comparator like the [http://www.maxim-ic.com/quick_view2.cfm/qv_pk/1279 MAX972].<ref name=\"test\"/>\n\n===Analog-to-digital converters===\n\nWhen a comparator performs the function of telling if an input voltage is above or below a given threshold, it is essentially performing a 1-bit [[Quantization (signal processing)|quantization]]. This function is used in nearly all analog to digital converters (such as [[Flash ADC|flash]], pipeline, [[Successive approximation ADC|successive approximation]], [[delta-sigma modulation]], folding, interpolating, [[Integrating ADC|dual-slope]] and others) in combination with other devices to achieve a multi-bit quantization.<ref>Phillip Allen and Douglas Holberg: ''CMOS Analog Circuit Design'',\nOxford University Press, Second edition, Oxford 2002.</ref>\n\n===Window detectors===\n\nComparators can also be used as window detectors. In a [[window detector]], a comparator is used to compare two voltages and determine whether a given input voltage is under voltage or over voltage.\n\n===Absolute value detectors===\n\nComparators can be used to create absolute value detectors. In an absolute value detector, two comparators and a digital logic gate are used to compare the absolute values of two voltages.<ref>{{cite web|url=http://ieeexplore.ieee.org/document/7604807/ |title=Iranmanesh, S., Rodriguez-Villegas, E. (2016). CMOS implementation of a low power absolute value comparator circuit. IEEE NEWCAS, June 2016.}}</ref>\n\n==See also==\n* [[Constant fraction discriminator]]\n* [[Digital comparator]]\n* [[Flash ADC]]\n* [[List of LM-series integrated circuits]]\n* [[Sorting network]]\n* [[Zero crossing threshold detector]]\n\n{{FS1037C}}\n\n== References ==\n\n{{reflist}}\n\n== External links ==\n{{Wiktionary}}\n* [https://web.archive.org/web/20070125032516/http://home.cogeco.ca/%7Erpaisley4/Comparators.html IC Comparator reference page at home.cogeco.ca]\n* [http://www.labbookpages.co.uk/electronics/resNetworks/comparator.html A Java based resistor value search tool for analysing an inverting comparator circuit with hysteresis]\n\n{{Authority control}}\n[[Category:Electronic circuits]]\n[[Category:Comparison (mathematical)]]"
    }
  ]
}