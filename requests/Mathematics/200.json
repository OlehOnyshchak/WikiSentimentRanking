{
  "pages": [
    {
      "title": "Kernel (algebra)",
      "url": "https://en.wikipedia.org/wiki/Kernel_%28algebra%29",
      "text": "In the various branches of mathematics that fall under the heading of abstract algebra, the kernel of a homomorphism measures the degree to which the homomorphism fails to be injective.See  and . An important special case is the kernel of a linear map. The kernel of a matrix, also called the null space, is the kernel of the linear map defined by the matrix.\n\nThe definition of kernel takes various forms in various contexts. But in all of them, the kernel of a homomorphism is trivial (in a sense relevant to that context) if and only if the homomorphism is injective. The fundamental theorem on homomorphisms (or first isomorphism theorem) is a theorem, again taking various forms, that involves the quotient object (also called quotient algebra in universal algebra, and cokernel in category theory) defined by the kernel.\n\nIn this article, we first survey kernels for some important types of algebraic structures; then we give general definitions from universal algebra for generic algebraic structures.\n\nSurvey of examples\n\n Linear maps \n\nLet V and W be vector spaces over a field (or more generally, modules over a ring) and let T be a linear map from V to W. If 0W is the zero vector of W, then the kernel of T is the preimage of the zero subspace {0W}; that is, the subset of V consisting of all those elements of V that are mapped by T to the element 0W. The kernel is usually denoted as , or some variation thereof:\n\nSince a linear map preserves zero vectors, the zero vector 0V of V must belong to the kernel. The transformation T is injective if and only if its kernel is reduced to the zero subspace.\n\nThe kernel ker T is always a linear subspace of V. Thus, it makes sense to speak of the quotient space V/(ker T). The first isomorphism theorem for vector spaces states that this quotient space is naturally isomorphic to the image of T (which is a subspace of W). As a consequence, the dimension of V equals the dimension of the kernel plus the dimension of the image.\n\nIf V and W are finite-dimensional and bases have been chosen, then T can be described by a matrix M, and the kernel can be computed by solving the homogeneous system of linear equations . In this case, the kernel of T may be identified to the kernel of the matrix M, also called \"null space\" of M. The dimension of the null space, called the nullity of M, is given by the number of columns of M minus the rank of M, as a consequence of the rank–nullity theorem.\n\nSolving homogeneous differential equations often amounts to computing the kernel of certain differential operators.\nFor instance, in order to find all twice-differentiable functions f from the real line to itself such that\n\nlet V be the space of all twice differentiable functions, let W be the space of all functions, and define a linear operator T from V to W by\n\nfor f in V and x an arbitrary real number.\nThen all solutions to the differential equation are in ker T.\n\nOne can define kernels for homomorphisms between modules over a ring in an analogous manner. This includes kernels for homomorphisms between abelian groups as a special case. This example captures the essence of kernels in general abelian categories; see Kernel (category theory).\n\nGroup homomorphisms\nLet G and H be groups and let f be a group homomorphism from G to H.\nIf eH is the identity element of H, then the kernel of f is the preimage of the singleton set {eH}; that is, the subset of G consisting of all those elements of G that are mapped by f to the element eH.\nThe kernel is usually denoted  (or a variation).\nIn symbols:\n \n\nSince a group homomorphism preserves identity elements, the identity element eG of G must belong to the kernel.\nThe homomorphism f is injective if and only if its kernel is only the singleton set {eG}. This is true because if the homomorphism f is not injective, then there exists  with  such that . This means that , which is equivalent to stating that  since group homomorphisms carry inverses into inverses and since . In other words, . Conversely, if there exists an element , then , thus f is not injective.\n\nIt turns out that ker f is not only a subgroup of G but in fact a normal subgroup.\nThus, it makes sense to speak of the quotient group G/(ker f).\nThe first isomorphism theorem for groups states that this quotient group is naturally isomorphic to the image of f (which is a subgroup of H).\n\nIn the special case of abelian groups, this works in exactly the same way as in the previous section.\n\nRing homomorphisms\nLet R and S be rings (assumed unital) and let f be a ring homomorphism from R to S.\nIf 0S is the zero element of S, then the kernel of f is its kernel as linear map over the integers, or, equivalently, as additive groups. It is the preimage of the zero ideal {0S}, which is, the subset of R consisting of all those elements of R that are mapped by f to the element 0S.\nThe kernel is usually denoted  (or a variation).\nIn symbols:\n \n\nSince a ring homomorphism preserves zero elements, the zero element 0R of R must belong to the kernel.\nThe homomorphism f is injective if and only if its kernel is only the singleton set {0R}.\nThis is always the case if R is a field, and S is not the zero ring.\n\nSince ker f contains the multiplicative identity only when S is the zero ring, it turns out that the kernel is generally not a subring of R. The kernel is a subrng, and, more precisely, a two-sided ideal of R.\nThus, it makes sense to speak of the quotient ring R/(ker f).\nThe first isomorphism theorem for rings states that this quotient ring is naturally isomorphic to the image of f (which is a subring of S). (note that rings need not be unital for the kernel definition).\n\nTo some extent, this can be thought of as a special case of the situation for modules, since these are all bimodules over a ring R:\n R itself;\n any two-sided ideal of R (such as ker f);\n any quotient ring of R (such as R/(ker f)); and\n the codomain of any ring homomorphism whose domain is R (such as S, the codomain of f).\nHowever, the isomorphism theorem gives a stronger result, because ring isomorphisms preserve multiplication while module isomorphisms (even between rings) in general do not.\n\nThis example captures the essence of kernels in general Mal'cev algebras.\n\nMonoid homomorphisms\nLet M and N be monoids and let f be a monoid homomorphism from M to N.\nThen the kernel of f is the subset of the direct product M × M consisting of all those ordered pairs of elements of M whose components are both mapped by f to the same element in N.\nThe kernel is usually denoted  (or a variation).\nIn symbols:\n \n\nSince f is a function, the elements of the form (m,m) must belong to the kernel.\nThe homomorphism f is injective if and only if its kernel is only the diagonal set {(m,m) : m in M}.\n\nIt turns out that ker f is an equivalence relation on M, and in fact a congruence relation.\nThus, it makes sense to speak of the quotient monoid M/(ker f).\nThe first isomorphism theorem for monoids states that this quotient monoid is naturally isomorphic to the image of f (which is a submonoid of N),(for the congruence relation).\n\nThis is very different in flavour from the above examples.\nIn particular, the preimage of the identity element of N is not enough to determine the kernel of f.\n\nUniversal algebra\nAll the above cases may be unified and generalized in universal algebra.\n\nGeneral case\nLet A and B be algebraic structures of a given type and let f be a homomorphism of that type from A to B.\nThen the kernel of f is the subset of the direct product A × A consisting of all those ordered pairs of elements of A whose components are both mapped by f to the same element in B.\nThe kernel is usually denoted  (or a variation).\nIn symbols:\n \n\nSince f is a function, the elements of the form (a,a) must belong to the kernel.\n\nThe homomorphism f is injective if and only if its kernel is exactly the diagonal set {(a,a) : a∈A}.\n\nIt is easy to see that ker f is an equivalence relation on A, and in fact a congruence relation.\nThus, it makes sense to speak of the quotient algebra A/(ker f).\nThe first isomorphism theorem in general universal algebra states that this quotient algebra is naturally isomorphic to the image of f (which is a subalgebra of B).\n\nNote that the definition of kernel here (as in the monoid example) doesn't depend on the algebraic structure; it is a purely set-theoretic concept.\nFor more on this general concept, outside of abstract algebra, see kernel of a function.\n\nMal'cev algebras\n\nIn the case of Mal'cev algebras, this construction can be simplified. Every Mal'cev algebra has a special neutral element (the zero vector in the case of vector spaces, the identity element in the case of commutative groups, and the zero element in the case of rings or modules). The characteristic feature of a Mal'cev algebra is that we can recover the entire equivalence relation ker f from the equivalence class of the neutral element.\n\nTo be specific, let A and B be Mal'cev algebraic structures of a given type and let f be a homomorphism of that type from A to B. If eB is the neutral element of B, then the kernel of f is the preimage of the singleton set {eB}; that is, the subset of A consisting of all those elements of A that are mapped by f to the element eB.\nThe kernel is usually denoted  (or a variation). In symbols:\n \n\nSince a Mal'cev algebra homomorphism preserves neutral elements, the identity element eA of A must belong to the kernel. The homomorphism f is injective if and only if its kernel is only the singleton set {eA}.\n\nThe notion of ideal generalises to any Mal'cev algebra (as linear subspace in the case of vector spaces, normal subgroup in the case of groups, two-sided ideals in the case of rings, and submodule in the case of modules). \nIt turns out that ker f is not a subalgebra of A, but it is an ideal.\nThen it makes sense to speak of the quotient algebra G/(ker f).\nThe first isomorphism theorem for Mal'cev algebras states that this quotient algebra is naturally isomorphic to the image of f (which is a subalgebra of B).\n\nThe connection between this and the congruence relation for more general types of algebras is as follows.\nFirst, the kernel-as-an-ideal is the equivalence class of the neutral element eA under the kernel-as-a-congruence. For the converse direction, we need the notion of quotient in the Mal'cev algebra (which is division on either side for groups and subtraction for vector spaces, modules, and rings).\nUsing this, elements a and b of A are equivalent under the kernel-as-a-congruence if and only if their quotient a/b is an element of the kernel-as-an-ideal.\n\nAlgebras with nonalgebraic structure\nSometimes algebras are equipped with a nonalgebraic structure in addition to their algebraic operations.\nFor example, one may consider topological groups or topological vector spaces, with are equipped with a topology.\nIn this case, we would expect the homomorphism f to preserve this additional structure; in the topological examples, we would want f to be a continuous map.\nThe process may run into a snag with the quotient algebras, which may not be well-behaved.\nIn the topological examples, we can avoid problems by requiring that topological algebraic structures be Hausdorff (as is usually done); then the kernel (however it is constructed) will be a closed set and the quotient space will work fine (and also be Hausdorff).\n\nKernels in category theory\nThe notion of kernel in category theory is a generalisation of the kernels of abelian algebras; see Kernel (category theory).\nThe categorical generalisation of the kernel as a congruence relation is the kernel pair.\n(There is also the notion of difference kernel, or binary equaliser.)\n\nSee also\n\n Kernel (linear algebra)\n Zero set\n\nNotes\n\nReferences\n\nCategory:Algebra\nCategory:Isomorphism theorems\nCategory:Linear algebra"
    },
    {
      "title": "Kernel (set theory)",
      "url": "https://en.wikipedia.org/wiki/Kernel_%28set_theory%29",
      "text": "In set theory, the kernel of a function f may be taken to be either\n\nthe equivalence relation on the function's domain that roughly expresses the idea of \"equivalent as far as the function f can tell\",. or\nthe corresponding partition of the domain.\n\nDefinition\nFor the formal definition, let X and Y be sets and let f be a function from X to Y.\nElements x1 and x2 of X are equivalent if f(x1) and f(x2) are equal, i.e. are the same element of Y.\nThe kernel of f is the equivalence relation thus defined.\n\nQuotients\nLike any equivalence relation, the kernel can be modded out to form a quotient set, and the quotient set is the partition:\n\nThis quotient set  /= is called the coimage of the function , and denoted  (or a variation).\nThe coimage is naturally isomorphic (in the set-theoretic sense of a bijection) to the image, ; specifically, the equivalence class of  in  (which is an element of ) corresponds to  in  (which is an element of ).\n\nAs a subset of the square\nLike any binary relation, the kernel of a function may be thought of as a subset of the Cartesian product X × X.\nIn this guise, the kernel may be denoted  (or a variation) and may be defined symbolically as\n\n .\n\nThe study of the properties of this subset can shed light on .\n\nIn algebraic structures\nIf X and Y are algebraic structures of some fixed type (such as groups, rings, or vector spaces), and if the function f from X to Y is a homomorphism, then ker f is a congruence relation (that is an equivalence relation that is compatible with the algebraic structure), and the coimage of f is a quotient of X.\nThe bijection between the coinage and the image of f is an isomorphism in the algebraic sense; this is the most general form of the first isomorphism theorem. See also Kernel (algebra).\n\nIn topological spaces\nIf X and Y are topological spaces and f is a continuous function between them, then the topological properties of ker f can shed light on the spaces X and Y.\nFor example, if Y is a Hausdorff space, then ker f must be a closed set.\nConversely, if X is a Hausdorff space and ker f is a closed set, then the coimage of f, if given the quotient space topology, must also be a Hausdorff space.\n\nReferences\n\nSources\n \n\nCategory:Algebra\nCategory:Topology\nCategory:Abstract algebra"
    },
    {
      "title": "Omar Khayyam",
      "url": "https://en.wikipedia.org/wiki/Omar_Khayyam",
      "text": "Omar Khayyam (;  ; 18 May 1048 – 4 December 1131) was a Persian mathematician, astronomer, and poet. He was born in Nishapur, in northeastern Iran, and spent most of his life near the court of the Karakhanid and Seljuq rulers in the period which witnessed the First Crusade.\n\nAs a mathematician, he is most notable for his work on the classification and solution of cubic equations, where he provided geometric solutions by the intersection of conics. Khayyam also contributed to the understanding of the parallel axiom.Struik, D. (1958). “Omar Khayyam, mathematician”. The Mathematics Teacher, 51(4), 280–285. As an astronomer, he designed the Jalali calendar, a solar calendar with a very precise  33-year intercalation cycle.With an error of one day accumulating over 5,000 years, it was more precise than the Gregorian calendar of 1582, which has an error of one day in  3,330 years in the Gregorian calendar (Aminrazavi 2007:200).The Cambridge History of Iran, Volume 4. Cambridge University Press (1975): Richard Nelson Frye\n\nThere is a tradition of attributing poetry to Omar Khayyam, written in the form of quatrains (rubāʿiyāt ). This poetry became widely known to the English-reading world in a translation by Edward FitzGerald (Rubaiyat of Omar Khayyam, 1859), which enjoyed great success in the Orientalism of the fin de siècle.\n\nLife\nOmar Khayyam was born May 18, 1048 in Nishapur, the son of Ebrahim Khayyami. Nishapur was a leading metropolis in Khorasan during medieval times that reached its zenith of prosperity in the eleventh century under the Seljuq dynasty.“The Tomb of Omar Khayyâm”, George Sarton, Isis, Vol. 29, No. 1 (Jul., 1938), 15.Edward FitzGerald, Rubaiyat of Omar Khayyam, Ed. Christopher Decker, (University of Virginia Press, 1997), xv; \"The Saljuq Turks had invaded the province of Khorasan in the 1030s, and the city of Nishapur surrendered to them voluntarily in 1038. Thus Omar Khayyam grew to maturity during the first of the several alien dynasties that would rule Iran until the twentieth century.\".Peter Avery and John Heath-Stubbs, The Ruba'iyat of Omar Khayyam, (Penguin Group, 1981), 14; \"These dates, 1048–1031, tell us that Khayyam lived when the Saljuq Turkish Sultans were extending and consolidating their power over Persia and when the effects of this power were particularly felt in Nishapur, Khayyam's birthplace. as well as a major center of the Zoroastrian religion. It is likely that Khayyam's father was a Zoroastrian who had converted to Islam.Mehdi Aminrazavi, The Wine of Wisdom: The Life, Poetry and Philosophy of Omar Khayyam, Oneworld Publications (2007) Although Khayyam means tent maker in Persian, there is no reliable source for the often repeated assertion that this was his father's profession, or that he was from a family of tent-makers. His full name, as it appears in the Arabic sources, was Abu’l Fath Omar ibn Ibrāhīm al-Khayyām.in e.g.  Al-Qifti  (Aminrazavi 2007:55) or Abu'l-Hasan Bayhaqi. (E. D. R., & H. A. R. G. (1929:436).\nIn medieval Persian texts he is usually simply called Omar Khayyām.Frye (1975:658); e.g. in Rashid-al-Din Hamadani (Browne 1899:409f) or in Munis al-ahrār (Ross 1927:436).\nThe historian Bayhaqi, who was personally acquainted with Omar, provides the full details of his horoscope: \"he was Gemini, the sun and Mercury being in the ascendant[...]\".E. D. R., & H. A. R. G. (1929). The Earliest Account of 'Umar Khayyām. Bulletin of the School of Oriental Studies, University of London, 5(3), 467–473. This was used by modern scholars to establish his date of birth as 18 May 1048.\n\nHis boyhood was spent in Nishapur. His gifts were recognized by his early tutors who sent him to study under Imam Muwaffaq Nīshābūrī, the greatest teacher of the Khorasan region who tutored the children of the highest nobility.\nAfter studying science, philosophy, mathematics and astronomy at Nishapur, he travelled in 1068 to Bukhara, where he frequented the renowned library of the Ark. In 1070 he moved to Samarkand, where he was employed by Abu Tahir, governor and chief judge of the city. During this time 1070, he wrote his most famous algebraic work, the Treatise on Demonstration of Problems of Algebra and Balancing (Risāla fī’l-barāhīn ˓ala masā’il al-jabr wa’l-muqābala ) which was dedicated to his mentor,  judge Abu Tahir Boris A. Rosenfeld «Umar al-Khayyam» in Helaine Selin, Encyclopaedia of the History of Science, Technology, and Medicine in Non-Western Cultures, Springer-Verlag, 2008, , p. 2175-2176.\n\nIn 1073, at the age of twenty-six, he entered the service of Sultan Malik-Shah I as an adviser. In 1076 Khayyam was invited to Isfahan by the vizier and political figure Nizam al-Mulk to take advantage of the libraries and centers in learning there. It was at this time that he began to study the work of Greek mathematicians Euclid and Apollonius much more closely. At the request of vizier Nizam-ul-Mulk, he proceeded to set up an observatory in Isfahan, leading a group of scientists in carrying out precise astronomical observations aimed at the revision of the Persian calendar. In 1079 he and his team concluded their measurements of the length of the year, made with astounding accuracy, reporting it to 14 significant figures as: 365.24219858156 days. In fact, according to the highest precision modern measurements, this is correct in the first eight figures, and the variation from year to year occurs in the eighth figure, making the calendar he devised the most accurately based system ever in use.\n \nAfter the death of Malik-Shah and his vizier (murdered, it is thought, by the Ismaili order of Assassins), Omar fell from favour at court, and as a result, he soon set out on his pilgrimage to Mecca. A possible ulterior motive for his pilgrimage reported by Al-Qifti, was a public demonstration of his faith with a view to allaying suspicions of skepticism  and confuting the allegations of unorthodoxy  levelled at him by a hostile clergy. He was then invited by the new Sultan Sanjar to Marv, possibly to work as a court astrologer. He was later allowed to return to Nishapur owing to his declining health. Upon his return, he seems to have lived the life of a recluse.Great Muslim Mathematicians. Penerbit UTM (July 2000): Mohaini Mohamed \n\nOmar Khayyam died at the age of 83 in his hometown of Nishapur on December 4, 1131. He was buried in a tomb whose location he had foreseen in his poetry, in an orchard where blossoms would fall twice a year, which is now the Khayyam Garden.\n\nMathematics\nKhayyam was famous during his life as a mathematician. His surviving mathematical works include: A commentary on the difficulties concerning the postulates of Euclid's Elements (, completed in December 1077), On the division of a quadrant of a circle (, undated but completed prior to the treatise on algebra), and On proofs for problems concerning Algebra (, most likely completed in 1079). He furthermore wrote a treatise on extracting binomial theorem and the nth root of natural numbers, which has been lost.\n\nTheory of parallels\n\nA part of Khayyam's commentary on Euclid's Elements deals with the parallel axiom. The treatise of Khayyam can be considered the first treatment of the axiom not based on petitio principii, but on a more intuitive postulate. Khayyam refutes the previous attempts by other mathematicians to prove the proposition, mainly on grounds that each of them had postulated something that was by no means easier to admit than the Fifth Postulate itself. Drawing upon Aristotle's views, he rejects the usage of movement in geometry and therefore dismisses the different attempt by Al-Haytham.. Excerpt: In some sense, his treatment was better than ibn al-Haytham's because he explicitly formulated a new postulate to replace Euclid's rather than have the latter hidden in a new definition. Unsatisfied with the failure of mathematicians to prove Euclid's statement from his other postulates, Omar tried to connect the axiom with the Fourth Postulate, which states that all right angles are equal to one another.\n\nKhayyam was the first to consider the three cases of acute, obtuse, and right angle for the summit angles of a Khayyam-Saccheri quadrilateral, three cases which are exhaustive and pairwise mutually exclusive. After proving a number of theorems about them, he proved that the Postulate V is a consequence of the right angle hypothesis, and refuted the obtuse and acute cases as self-contradictory. Khayyam's elaborate attempt to prove the parallel postulate was significant for the further development of geometry, as it clearly shows the possibility of non-Euclidean geometries. The hypothesis of the acute, obtuse, and that of the right angle are now known to lead respectively to the non-Euclidean hyperbolic geometry of Gauss-Bolyai-Lobachevsky, to that of Riemannian geometry, and to Euclidean geometry.Rolwing, R. & Levine, M. (1969). ”The Parallel Postulate”. The Mathematics Teacher, 62(8), 665–669.\n\nthumb|\"Cubic equation and intersection of conic sections\" the first page of two-chaptered manuscript kept in Tehran University.\n\nTusi's commentaries on Khayyam's treatment of parallels made its way to Europe. John Wallis, professor of geometry at Oxford, translated Tusi's commentary into Latin. Jesuit geometrician Girolamo Saccheri, whose work (euclides ab omni naevo vindicatus, 1733) is generally considered as the first step in the eventual development of non-Euclidean geometry, was familiar with the work of Wallis. The American historian of mathematics, David Eugene Smith mentions that Saccheri \"used the same lemma as the one of Tusi, even lettering the figure in precisely the same way and using the lemma for the same purpose\". He further says that \"Tusi distinctly states that it is due to Omar Khayyam, and from the text, it seems clear that the latter was his inspirer.\"Smith, David (1935). \"Euclid, Omar Khayyam and Saccheri,\" Scripta Mathematica.\n\nThe real number concept\nThis treatise on Euclid contains another contribution dealing with the theory of proportions and with the compounding of ratios. Khayyam discusses the relationship between the concept of ratio and the concept of number and explicitly raises various theoretical difficulties. In particular, he contributes to the theoretical study of the concept of irrational number. Displeased with Euclid's definition of equal ratios, he redefined the concept of a number by the use of a continuous fraction as the means of expressing a ratio. Rosenfeld and Youschkevitch (1973) argue that \"by placing irrational quantities and numbers on the same operational scale, [Khayyam] began a true revolution in the doctrine of number.\" Likewise, it was noted by D. J. Struik that Omar was \"on the road to that extension of the number concept which leads to the notion of the real number.\"\n\nGeometric algebra\nthumb|right|Omar Khayyam's construction of a solution to the cubic x3 + 2x = 2x2 + 2. The intersection point produced by the circle and the hyperbola determine the desired segment.\n\nRashed and Vahabzadeh (2000) have argued that because of his thoroughgoing geometrical approach to algebraic equations, Khayyam can be considered the precursor of Descartes in the invention of analytic geometry.Cooper, G. (2003). Journal of the American Oriental Society, 123(1), 248–249. In The Treatise on the Division of a Quadrant of a Circle Khayyam applied algebra to geometry. In this work, he devoted himself mainly to investigating whether it is possible to divide a circular quadrant into two parts such that the line segments projected from the dividing point to the perpendicular diameters of the circle form a specific ratio. His solution, in turn, employed several curve constructions that led to equations containing cubic and quadratic terms.\n\nThe solution of cubic equations\nKhayyam seems to have been the first to conceive a general theory of cubic equations and the first to geometrically solve every type of cubic equation, so far as positive roots are concerned.Howard Eves (1958). “Omar Khayyam's Solution of Cubic Equations”, The Mathematics Teacher (1958), pp. 302–303. The treatise on algebra contains his work on cubic equations.\"Omar Al Hay of Chorassan, about 1079 AD did most to elevate to a method the solution of the algebraic equations by intersecting conics.\" It is divided into three parts: (i) equations which can be solved with compass and straight edge, (ii) equations which can be solved by means of conic sections, and (iii) equations which involve the inverse of the unknown.Bijan Vahabzadeh,\n \"Khayyam, Omar xv. As Mathematician\", Encyclopædia Iranica.\n\nKhayyam produced an exhaustive list of all possible equations involving lines, squares, and cubes.Netz, R. (1999). “Archimedes Transformed: The Case of a Result Stating a Maximum for a Cubic Equation”. Archive for History of Exact Sciences, 54(1), 1–47. He considered three binomial equations, nine trinomial equations, and seven tetranomial equations. For the first and second degree polynomials, he provided numerical solutions by geometric construction. He concluded that there are fourteen different types of cubics that cannot be reduced to an equation of a lesser degree. For these he could not accomplish the construction of his unknown segment with compass and straight edge. He proceeded to present geometric solutions to all types of cubic equations using the properties of conic sections.Deborah A. Kent, & David J. Muraki (2016). “A Geometric Solution of a Cubic by Omar Khayyam … in Which Colored Diagrams Are Used Instead of Letters for the Greater Ease of Learners”.  The American Mathematical Monthly, 123(2), 149–160. The prerequisite lemmas for Khayyam’s geometrical proof include Euclid VI, Prop 13, and Apollonius II, Prop 12. The positive root of a cubic equation was determined as the abscissa of a point of intersection of two conics, for instance, the intersection of two parabolas, or the intersection of a parabola and a circle, etc.Kennedy, E. (1958). “Omar Khayyam”.  The Mathematics Teacher, Vol. 59, No. 2 (1966), pp. 140–142. However, he acknowledged that the arithmetic problem of these cubics was still unsolved, adding that \"possibly someone else will come to know it after us\". This task remained open until the sixteenth century, where algebraic solution of the cubic equation was found in its generality by Cardano, Del Ferro, and Tartaglia in Renaissance Italy.\n\nIn effect, Khayyam's work is an effort to unify algebra and geometry.The Mathematics Teacher, 25(4), 238–241. (1932). This particular geometric solution of cubic equations has been further investigated by M. Hachtroudi and extended to solving fourth-degree equations.A. R. Amir-Moez, Khayyam's Solution of Cubic Equations, Mathematics Magazine, Vol. 35, No. 5  (November 1962), pp. 269–271. This paper contains an extension by the late Mohsen Hashtroodi of Khayyam's method to degree four equations. Although similar methods had appeared sporadically since Menaechmus, and further developed by the 10th-century mathematician Abu al-Jud, Khayyam's work can be considered the first systematic study and the first exact method of solving cubic equations.Mathematical Masterpieces: Further Chronicles by the Explorers, p. 92 The mathematician Woepcke (1851) who offered translations of Khayyam's algebra into French praised him for his \"power of generalization and his rigorously systematic procedure.\"E. H. Whinfield, The Quatrains of Omar Khayyam, Psychology Press (2000)\n\nBinomial theorem and extraction of roots\n\nIn his algebraic treatise, Khayyam alludes to a book he had written on the extraction of the th root of the numbers using a law he had discovered which did not depend on geometric figures. This book was most likely titled The difficulties of arithmetic (), and is not extant. Based on the context, some historians of mathematics such as D. J. Struik, believe that Omar must have known the formula for the expansion of the binomial , where  is a positive integer. The case of power 2 is explicitly stated in Euclid's elements and the case of at most power 3 had been established by Indian mathematicians. Khayyam was the mathematician who noticed the importance of a general binomial theorem. The argument supporting the claim that Khayyam had a general binomial theorem is based on his ability to extract roots.J. L. Coolidge, The Story of the Binomial Theorem, Amer. Math. Monthly, Vol. 56, No. 3 (Mar., 1949), pp. 147–157 The arrangement of numbers known as Pascal's triangle enables one to write down the coefficients in a binomial expansion. This triangular array sometimes is known as Omar Khayyam's triangle.\n\nAstronomy\n\nthumb|Representation of the intercalation scheme of the Jalali calendar\nIn 1074, Omar Khayyam was commissioned by Sultan Malik-Shah to build an observatory at Isfahan and reform the Persian calendar. There was a panel of eight scholars working under the direction of Khayyam to make large-scale astronomical observations and revise the astronomical tables. Recalibrating the calendar fixed the first day of the year at the exact moment of the passing of the Sun's center across vernal equinox. This marks the beginning of spring or Nowrūz, a day in which the Sun enters the first degree of Aries before noon. The resulted calendar was named in Malik-Shah's honor as the Jalālī calendar, and was inaugurated on  March 15, 1079.. The observatory itself was disused after the death of Malik-Shah in 1092.\n\nThe Jalālī calendar was a true solar calendar where the duration of each month is equal to the time of the passage of the Sun across the corresponding sign of the Zodiac. The calendar reform introduced a unique 33-year intercalation cycle. As indicated by the works of Khazini, Khayyam's group implemented an intercalation system based on quadrennial and quinquennial leap years. Therefore, the calendar consisted of 25 ordinary years that included 365 days, and 8 leap years that included 366 days. The calendar remained in use across Greater Iran from the 11th to the 20th centuries. In 1911 the Jalali calendar became the official national calendar of Qajar Iran. In 1925 this calendar was simplified and the names of the months were modernized, resulting in the modern Iranian calendar. The Jalali calendar is more accurate than the Gregorian calendar of 1582, with an error of one day accumulating over 5,000 years, compared to one day every 3,330 years in the Gregorian calendar. \nMoritz Cantor considered it the most perfect calendar ever devised.\n\nOne of his pupils Nizami Aruzi of Samarcand relates that Khayyam apparently did not have a belief in astrology and divination: \"I did not observe that he (scil. Omar Khayyam) had any great belief in astrological predictions, nor have I seen or heard of any of the great [scientists] who had such belief.\" While working for Sultan Sanjar as an astrologer he was asked to predict the weather – a job that he apparently did not do well. George Saliba (2002) explains that the term , used in various sources in which references to Omar's life and work could be found, has sometimes been incorrectly translated to mean astrology. He adds: \"from at least the middle of the tenth century, according to Farabi's enumeration of the sciences, that this science, , was already split into two parts, one dealing with astrology and the other with theoretical mathematical astronomy.\"Saliba, G. (2002). Iranian Studies, 35(1/3), 220–225.\n\nA popular claim to the effect that Khayyam believed in heliocentrism is based on Edward FitzGerald's popular but anachronistic rendering  of Khayyam's poetry, in which the first lines are mistranslated with a heliocentric image of the Sun flinging \"the Stone that puts the Stars to Flight\".Donald and Marilynn Olson (1988). “Zodiac Light, False Dawn, and Omar Khayyam”, The Observatory, vol. 108, pp. 181–182.\n\nOther works\n\nHe has a short treatise devoted to Archimedes' principle (in full title, On the Deception of Knowing the Two Quantities of Gold and Silver in a Compound Made of the Two). For a compound of gold adulterated with silver, he describes a method to measure more exactly the weight per capacity of each element. It involves weighing the compound both in air and in water, since weights are easier to measure exactly than volumes. By repeating the same with both gold and silver one finds exactly how much heavier than water gold, silver and the compound were. This treatise was extensively examined by Eilhard Wiedemann who believed that Khayyam's solution was more accurate and sophisticated than that of Khazini and Al-Nayrizi who also dealt with the subject elsewhere.\n\nAnother short treatise is concerned with music theory in which he discusses the connection between music and arithmetic. Khayyam's contribution was in providing a systematic classification of musical scales, and discussing the mathematical relationship among notes, minor, major and tetrachords.\n\nPoetry\nthumb|right|Rendition of a ruba'i from the Bodleian ms, rendered in Shekasteh calligraphy.\nThe earliest allusion to Omar Khayyam's poetry is from the historian Imad ad-Din al-Isfahani, a younger contemporary of Khayyam, who explicitly identifies him as both a poet and a scientist (, 1174).Ali Dashti (translated by L. P. Elwell-Sutton), In Search of Omar Khayyam, Routledge Library Editions: Iran (2012) One of the earliest specimens of Omar Khayyam's Rubiyat is from Fakhr al-Din Razi. In his work  (ca. 1160), he quotes one of his poems (corresponding to quatrain LXII of FitzGerald's first edition). Daya in his writings (, ca. 1230) quotes two quatrains, one of which is the same as the one already reported by Razi. An additional quatrain is quoted by the historian Juvayni (, ca. 1226–1283). In 1340 Jajarmi includes thirteen quatrains of Khayyam in his work containing an anthology of the works of famous Persian poets (), two of which have hitherto been known from the older sources.Edward Denison Ross, Omar Khayyam, Bulletin of the School Of Oriental Studies London Institution (1927) A comparatively late manuscript is the Bodleian MS. Ouseley 140, written in Shiraz in 1460, which contains 158 quatrains on 47 folia. The manuscript belonged to William Ouseley (1767–1842) and was purchased by the Bodleian Library in 1844.thumb|upright|Ottoman Era inscription of a poem written by Omar Khayyam at Morića Han in Sarajevo, Bosnia and HerzegovinaThere are occasional quotes of verses attributed to Omar in texts attributed to authors of the 13th and 14th centuries, but these are also of doubtful authenticity, so that skeptic scholars point out that the entire tradition may be pseudepigraphic. \nHans Heinrich Schaeder in 1934 commented that the name of Omar Khayyam \"is to be struck out from the history of Persian literature\" due to the lack of any material that could confidently be attributed to him. \nDe Blois (2004) presents a bibliography of the manuscript tradition, concluding pessimistically that the situation has not changed significantly since Schaeder's time.Francois De Blois,  Persian Literature – A Bio-Bibliographical Survey: Poetry of the Pre-Mongol Period (2004), p. 307.\nFive of the quatrains later attributed to Omar are found as early as 30 years after his death, quoted in  Sindbad-Nameh. While this establishes that these specific verses were in circulation in Omar's time or shortly later, it doesn't imply that the verses must be his. De Blois concludes that at the least the process of attributing poetry to Omar Khayyam appears to have begun already in the 13th century.Francois De Blois\n, Persian Literature – A Bio-Bibliographical Survey: Poetry of the Pre-Mongol Period (2004), p. 305. Edward Granville Browne (1906) notes the difficulty of disentangling authentic from spurious quatrains: \"while it is certain that Khayyam wrote many quatrains, it is hardly possible, save in a few exceptional cases, to assert positively that he wrote any of those ascribed to him\".\n\nIn addition to the Persian quatrains, there are twenty-five Arabic poems attributed to Khayyam which are attested by historians such as al-Isfahani, Shahrazuri (, ca. 1201–1211), Qifti (, 1255), and Hamdallah Mustawfi (, 1339).\n\nRichard N. Frye (1975) emphasizes that there are a number of other Persian scholars who occasionally wrote quatrains, including Avicenna, Ghazzali, and Tusi. He concludes that it is also possible that poetry with Khayyam was the amusement of his leisure hours: \"these brief poems seem often to have been the work of scholars and scientists who composed them, perhaps, in moments of relaxation to edify or amuse the inner circle of their disciples\".\n\nThe poetry attributed to Omar Khayyam has contributed greatly to his popular fame in the modern period as a direct result of the extreme popularity of the translation of such verses into English by Edward FitzGerald (1859). FitzGerald's  Rubaiyat of Omar Khayyam contains loose translations of quatrains from The Bodleian manuscript. It enjoyed such success in the fin de siècle period that a bibliography compiled in 1929 listed more than 300 separate editions,Ambrose George Potter, A Bibliography of the Rubaiyat of Omar Khayyam (1929).  and many more have been published since.Francois De Blois\n, Persian Literature – A Bio-Bibliographical Survey: Poetry of the Pre-Mongol Period (2004), p. 312.\n\nPhilosophy\nthumb|upright|Statue of Omar Khayyam in Bucharest\nKhayyam considered himself intellectually to be a student of Avicenna.Nasr, S. H., & Aminrazavi, M. (2007). Anthology of philosophy in Persia: from Zoroaster to Omar Khayyam. According to Al-Bayhaqi, he was reading the metaphysics in Avicenna's the Book of Healing before he died. There are six philosophical papers believed to have been written by Khayyam. One of them, On existence (), was written originally in Persian and deals with the subject of existence and its relationship to universals. Another paper, titled The necessity of contradiction in the world, determinism and subsistence (), is written in Arabic and deals with free will and determinism. The titles of his other works are On being and necessity (), The Treatise on Transcendence in Existence (), On the knowledge of the universal principles of existence (), and Abridgement concerning natural phenomena ().\n\nReligious views\nA literal reading of Khayyam's quatrains leads to the interpretation of his philosophic attitude toward life as a combination of pessimism, nihilism, Epicureanism, fatalism, and agnosticism.Boscaglia, F. (2015). Pessoa, Borges and Khayyam. Variaciones Borges This view is taken by Iranologists such as Arthur Christensen, H. Schaeder, Richard N. Frye, E. D. Ross,Ross, E. (1898). Al-Musaffariyé: Containing a Recent Contribution to the Study of 'Omar Khayyām. Journal of the Royal Asiatic Society of Great Britain and Ireland, 349–366. E.H. Whinfield and George Sarton. Conversely, the Khayyamic quatrains have also been described as mystical Sufi poetry. However, this is the view of a minority of scholars. In addition to his Persian quatrains, J. C. E. Bowen (1973) mentions that Khayyam's Arabic poems also \"express a pessimistic viewpoint which is entirely consonant with the outlook of the deeply thoughtful rationalist philosopher that Khayyam is known historically to have been.\"J. C. E. Bowen. (1973). The Rubāՙiyyāt of Omar Khayyam: A Critical Assessment of Robert Graves' and Omar Ali Shah's Translation. Iran, 11, 63–73. Edward FitzGerald emphasized the religious skepticism he found in Khayyam. In his preface to the Rubáiyát he claimed that he \"was hated and dreaded by the Sufis\",FitzGerald, E. (2010). Rubaiyat of Omar Khayyam (p. 12). Champaign, Ill.: Project Gutenberg  and denied any pretense at divine allegory: \"his Wine is the veritable Juice of the Grape: his Tavern, where it was to be had: his Saki, the Flesh and\nBlood that poured it out for him.\"Schenker, D. (1981). Fugitive Articulation: An Introduction to \"The Rubáiyát of Omar Khayyam\". Victorian Poetry, 19(1), 49–64. Sadegh Hedayat is one of the most notable proponents of Khayyam's philosophy as agnostic skepticism, and according to Jan Rypka (1934), he even considered Khayyam an atheist.Hedayat's \"Blind Owl\" as a Western Novel. Princeton Legacy Library: Michael Beard Hedayat (1923) states that \"while Khayyam believes in the transmutation and transformation of the human body, he does not believe in a separate soul; if we are lucky, our bodily particles would be used in the making of a jug of wine.\"Katouzian, H. (1991). Sadeq Hedayat: The life and literature of an Iranian writer (p. 138). London: I.B. Tauris \nIn a later study (1934–35) he further contends that Khayyam's usage of Sufic terminology such as \"wine\" is literal and that he turned to the pleasures of the moment as an antidote to his existential sorrow: \"Khayyam took refuge in wine to ward off bitterness and to blunt the cutting edge of his thoughts.\" In this tradition, Omar Khayyam's poetry has been cited in the context of New Atheism, e.g. in The Portable Atheist by  Christopher Hitchens.Hitchens, C. (2007). The portable atheist: Essential readings for the nonbeliever (p. 7). Philadelphia, PA: Da Capo.\n\nAl-Qifti (ca. 1172–1248) appears to confirm this view of Omar's philosophy. In his work The History of Learned Men\nhe reports that Omar's poems were only outwardly in the Sufi style, but were written with an anti-religious agenda. He also mentions that he was at one point indicted for impiety, but went on a pilgrimage to prove he was pious. The report has it that upon returning to his native city he concealed his deepest convictions and practised a strictly religious life, going morning and evening to the place of worship.\n\nIn the context of a piece entitled On the Knowledge Of the Principals of Existence, Khayyam endorses the Sufi path. Csillik (1960) suggests the possibility that Omar Khayyam could see in Sufism an ally against orthodox religiosity.Csillik, B. (1960). ”The Real 'Omar Khayyām’”. Acta Orientalia Academiae Scientiarum Hungaricae, 10(1), 59–77. Retrieved from https://www.jstor.org/stable/23682646 Other commentators do not accept that Omar's poetry has an anti-religious agenda and interpret his references to wine and drunkenness in the conventional metaphorical sense common in Sufism. The French translator J. B. Nicolas held that Omar's constant exhortations to drink wine should not be taken literally, but should be regarded rather in the light of Sufi thought where rapturous intoxication by \"wine\" is to be understood as a metaphor for the enlightened state or divine rapture of baqaa.Albano, G. (2008). The Benefits of Reading the \"Rubáiyát of Omar Khayyám\" as Pastoral. Victorian Poetry, 46(1), 55–67. The view of Omar Khayyam as a Sufi was defended by Bjerregaard (1915),C. H. A. Bjerregaard, Sufism: Omar Khayyam and E. Fitzgerald, The Sufi Publishing Society (1915), p. 3 Idries Shah (1999),Idries Shah, The Sufis, Octagon Press (1999), pp. 165–166 and Dougan (1991) who attributes the reputation of hedonism to the failings of FitzGerald's translation, arguing that Omar's poetry is to be understood as \"deeply esoteric\".\"Every line of the Rubaiyat has more meaning than almost anything you could read in Sufi literature\" Abdullah Dougan Who is the Potter? Gnostic Press 1991  On the other hand, Iranian experts such as Mohammad Ali Foroughi and Mojtaba Minovi unanimously rejected the hypothesis that Omar Khayyam was a Sufi. Foroughi stated that Khayyam's ideas may have been consistent with that of Sufis at times but there is no evidence that he was formally a Sufi. Aminrazavi (2007) states that \"Sufi interpretation of Khayyam is possible only by reading into his Rubāʿīyyāt extensively and by stretching the content to fit the classical Sufi doctrine.\" Furthermore, Frye (1975) emphasizes that Khayyam was intensely disliked by a number of celebrated Sufi mystics who belonged to the same century. This includes Shams Tabrizi (spiritual guide of Rumi), Najm al-Din Daya who described Omar Khayyam as \"an unhappy philosopher, atheist, and materialist\", and Attar who regarded him not as a fellow-mystic but a free-thinking scientist who awaited punishments hereafter.\n\nSeyyed Hossein Nasr argues that it is \"reductive\" to use a literal interpretation of his verses (many of which are of uncertain authenticity to begin with) to establish Omar Khayyam's philosophy. Instead, he adduces Khayyam's interpretive translation of Avicenna's treatise Discourse on Unity (), where he\nexpresses orthodox views on Divine Unity in agreement with the author.S. H. Nasr, 2006, Islamic Philosophy from Its Origin to the Present, Chapter 9., pp. 165–183 The prose works believed to be Omar's are written in the Peripatetic style and are explicitly theistic, dealing with subjects such as the existence of God and theodicy. As noted by Bowen these works indicate his involvement in the problems of metaphysics rather than in the subtleties of Sufism. As evidence of Khayyam's faith and/or conformity to Islamic customs, Aminrazavi mentions that in his treatises he offers salutations and prayers, praising God and Muhammad. In most biographical extracts, he is referred to with religious honorifics such as , The Patron of Faith (), and The Evidence of Truth (). He also notes that biographers who praise his religiosity generally avoid making reference to his poetry, while the ones who mention them often do not praise his religious character. For instance Al-Bayhaqi's account which antedates by some years other biographical notices, speaks of Omar as a very pious man who professed orthodox views down to his last hour.Meyerhof, M. (1948). 'Alī al-Bayhaqī's Tatimmat Siwān al-Hikma: A Biographical Work on Learned Men of the Islam. Osiris, 8, 122–217.\n\nOn the basis of all the existing textual and biographical evidence, the question remains somewhat open, and as a result Khayyam has received sharply conflicting appreciations and criticisms.\n\nReception\nthumb|upright|\"A Ruby kindles in the vine\", illustration for FitzGerald's Rubaiyat of Omar Khayyam by Adelaide Hanscom Leeson (ca. 1905).\nthumb|upright|\"At the Tomb of Omar Khayyam\" by Jay Hambidge (1911).\nThe various biographical extracts referring to Omar Khayyam describe him as unequalled in scientific knowledge and achievement during his time.e.g. by the author of Firdaws al-tawārikh (Ross 1898:356), author of Tārikh alfī (Ross 1898:358), and al-Isfahani  (Aminrazavi 2007:49). Many called him by the epithet King of the Wise (). Shahrazuri (d. 1300) esteems him highly as a mathematician, and claims that he may be regarded as \"the successor of Avicenna in the various branches of philosophic learning.\" Al-Qifti (d. 1248) even though disagreeing with his views concedes he was \"unrivalled in his knowledge of natural philosophy and astronomy.\" Despite being hailed as a poet by a number of biographers, according to Richard Nelson Frye \"it is still possible to argue that Khayyam's status as a poet of the first rank is a comparatively late development.\"\n\nThomas Hyde was the first European to call attention to Omar and to translate one of his quatrains into Latin (Historia religionis veterum Persarum eorumque magorum, 1700).Beveridge, H. (1905). XVIII. Omar Khayyam. Journal of the Royal Asiatic Society, 37(3), 521–526. Western interest in Persia grew with the Orientalism movement in the 19th century. Joseph von Hammer-Purgstall (1774–1856) translated some of Khayyam's poems into German in 1818, and Gore Ouseley (1770–1844) into English in 1846, but Khayyam remained relatively unknown in the West until after the publication of Edward FitzGerald's Rubaiyat of Omar Khayyam in 1859. FitzGerald's work at first was unsuccessful but was popularised by Whitley Stokes from 1861 onward, and the work came to be greatly admired by the Pre-Raphaelites. In 1872 FitzGerald had a third edition printed which increased interest in the work in America. By the 1880s, the book was extremely well known throughout the English-speaking world, to the extent of the formation of numerous \"Omar Khayyam Clubs\" and a \"fin de siècle cult of the Rubaiyat\"J. D. Yohannan, Persian Poetry in England and America, 1977. p. 202. Khayyam's poems have been translated into many languages; many of the more recent ones are more literal than that of FitzGerald.The Great Umar Khayyam: A Global Reception of the Rubaiyat (AUP – Leiden University Press) by A. A. Seyed-Gohrab, 2012.\n\nFitzGerald's translation was a factor in rekindling interest in Khayyam as a poet even in his native Iran.Simidchieva, M. (2011). FitzGerald's Rubáiyát and Agnosticism. In A. Poole, C. Van Ruymbeke, & W. Martin (Eds.), FitzGerald's Rubáiyát of Omar Khayyám: Popularity and Neglect (pp. 55–72). Anthem Press. Sadegh Hedayat in his Songs of Khayyam (Taranehha-ye Khayyam, 1934) reintroduced Omar's poetic legacy to modern Iran. Under the Pahlavi dynasty, a new monument of white marble, designed by the architect Houshang Seyhoun, was erected over his tomb. A statue by Abolhassan Sadighi was erected in Laleh Park, Tehran in the 1960s, and a bust by the same sculptor was placed near Khayyam's mausoleum in Nishapur. In 2009, the state of Iran donated a pavilion to the United Nations Office in Vienna, inaugurated at Vienna International Center. In 2016, three statues of Khayyam were unveiled: one at the  University of Oklahoma, one in Nishapur and one in Florence, Italy. Over 150 composers have used the Rubaiyat as their source of inspiration. The earliest such composer was Liza Lehmann.\n\nFitzGerald rendered Omar's name as \"Tentmaker\", and the anglicized name of \"Omar the Tentmaker\" resonated in English-speaking popular culture for a while. Thus, Nathan Haskell Dole published a novel called Omar, the Tentmaker: A Romance of Old Persia in 1898. Omar the Tentmaker of Naishapur is a historical novel by John Smith Clarke, published in 1910.  \"Omar the Tentmaker\" is also the title of a 1914 play  by Richard Walton Tully in an oriental setting, adapted as a silent film in 1922. US General Omar Bradley was given the nickname \"Omar the Tent-Maker\" in World War II.Jeffrey D. Lavoie, The Private Life of General Omar N. Bradley (2015), p. 13.\n\nThe lunar crater Omar Khayyam was named in his honour in 1970, as was the minor planet 3095 Omarkhayyam discovered by Soviet astronomer Lyudmila Zhuravlyova in 1980.\n\nGoogle released two Google Doodles commemorating him. The first was on his 964th birthday on May 18, 2012. The second was on his 971st birthday on May 18, 2019. \n\nthumb|upright|The statue of Khayyam in United Nations Office in Vienna as a part of Persian Scholars Pavilion donated by Iran.\n\nSee also\n\n Astronomy in medieval Islam\n Mathematics in medieval Islam\n Nozhat al-Majales\n Omar Khayyam (film)\n The Keeper: The Legend of Omar Khayyam\n\nCitations\n\nReferences\n Browne, E. (1899). Yet More Light on 'Umar-i-Khayyām. Journal of the Royal Asiatic Society of Great Britain and Ireland, 409–420.\n\n <cite id=refkhayyam-EI>\n <cite id=refNasr>\n \n <cite id=refmathmaster>\n <cite id=refchi5> \n \n \n Ross, E. (1927). 'Omar Khayyam. Bulletin of the School of Oriental Studies, University of London, 4(3), 433–439.\n Jan Rypka (1968). History of Iranian Literature. Reidel Publishing Company. . \n\nExternal links\n\n Online text of the Rubaiyat of Omar Khayyam, translated by Edward Fitzgerald (1859) plus concordance and index. Browse over 500 words taken from the quatrains ( poems in four lines with the rhyme scheme of AABA ), which are alphabetized and shown next to the lines in which they appear. Click the link next to every word to go to the exact poem in the text of the Rubaiyat of Omar Khayyam. prepared by WhoSaidSo.org\n\n  (PDF version)\nUmar Khayyam, at the Stanford Encyclopedia of Philosophy\nKhayyam's works in original Persian at Ganjoor Persian Library\nKhayyam in Tarikhema.ir\n\n \n\nThe illustrated Rubáiyát of Omar Khayyám at Internet Archive.\nOmar Khayyam's Rubaiyat as translated by Edward Fitzgerald – 1st edition\nThe Rubaiyat by Omar Khayyam – The Internet Classics Archive\nIllustrations to the Rubaiyat by Adelaide Hanscom\nBarney Rickenbacker, Exploring Khayyaam website. Different versions of well-known quatrains compared, with notes.\n\n \nCategory:1048 births\nCategory:1131 deaths\nCategory:Algebra\nCategory:Philosophers from Nishapur\nCategory:Mathematicians from Nishapur\nCategory:11th-century Persian poets\nCategory:12th-century Persian poets\nCategory:Medieval Persian astronomers\nCategory:Medieval Persian mathematicians\nCategory:Persian philosophers\nCategory:Persian spiritual writers\nCategory:11th-century mathematicians\nCategory:12th-century mathematicians\nCategory:Medieval Persian writers\nCategory:Astronomers of medieval Islam\nCategory:Mathematicians of medieval Islam\nCategory:12th-century astronomers\nCategory:11th-century Iranian people\nCategory:12th-century Iranian people\nCategory:Seljuq scholars\nCategory:Seljuq-period poets\nCategory:Persian physicists\nCategory:Persian poets"
    },
    {
      "title": "Laws of Form",
      "url": "https://en.wikipedia.org/wiki/Laws_of_Form",
      "text": "Laws of Form (hereinafter LoF) is a book by G. Spencer-Brown, published in 1969, that straddles the boundary between mathematics and philosophy. LoF describes three distinct logical systems:\n The \"primary arithmetic\" (described in Chapter 4 of LoF), whose models include Boolean arithmetic;\n The \"primary algebra\" (Chapter 6 of LoF), whose models include the two-element Boolean algebra (hereinafter abbreviated 2), Boolean logic, and the classical propositional calculus;\n \"Equations of the second degree\" (Chapter 11), whose interpretations include finite automata and Alonzo Church's Restricted Recursive Arithmetic (RRA).\n\n\"Boundary algebra\" is Meguire's (2011)Meguire, P. (2011) Boundary Algebra: A Simpler Approach to Basic Logic and Boolean Algebra. Saarbrücken: VDM Publishing Ltd. 168pp term for the union of the primary algebra and the primary arithmetic. Laws of Form sometimes loosely refers to the \"primary algebra\" as well as to LoF.\n\nThe book\n\nLoF emerged from work in electronic engineering its author did around 1960, and from subsequent lectures on mathematical logic he gave under the auspices of the University of London's extension program. LoF has appeared in several editions, the most recent being a 1997 German translation, and has never gone out of print.\n\nThe mathematics fills only about 55pp and is rather elementary. But LoFs mystical and declamatory prose, and its love of paradox, make it a challenging read for all. Spencer-Brown was influenced by Wittgenstein and R. D. Laing. LoF also echoes a number of themes from the writings of Charles Sanders Peirce, Bertrand Russell, and Alfred North Whitehead.\n\nThe entire book is written in an operational way, giving instructions to the reader instead of telling them what \"is\". In accordance with G. Spencer-Brown's interest in paradoxes, the only sentence that makes a statement that something is, is the statement, which says no such statements are used in this book.Felix Lau: Die Form der Paradoxie, 2005 Carl-Auer Verlag,  Except for this one sentence the book can be seen as an example of E-Prime.\n\nReception\nOstensibly a work of formal mathematics and philosophy, LoF became something of a cult classic, praised in the Whole Earth Catalog. Those who agree point to LoF as embodying an enigmatic \"mathematics of consciousness,\" its algebraic symbolism capturing an (perhaps even \"the\") implicit root of cognition:  the ability to \"distinguish\". LoF argues that primary algebra reveals striking connections among logic, Boolean algebra, and arithmetic, and the philosophy of language and mind.\n\nBanaschewski (1977) argues that the primary algebra is nothing but new notation for Boolean algebra. Indeed, the two-element Boolean algebra 2 can be seen as the intended interpretation of the primary algebra. Yet the notation of the primary algebra:\n Fully exploits the duality characterizing not just Boolean algebras but all lattices;\nHighlights how syntactically distinct statements in logic and 2 can have identical semantics;\n Dramatically simplifies Boolean algebra calculations, and proofs in sentential and syllogistic logic.\nMoreover, the syntax of the primary algebra can be extended to formal systems other than 2 and sentential logic, resulting in boundary mathematics (see Related Work below).\n\nLoF has influenced, among others, Heinz von Foerster, Louis Kauffman, Niklas Luhmann, Humberto Maturana, Francisco Varela and William Bricken. Some of these authors have modified the primary algebra in a variety of interesting ways.\n\nLoF claimed that certain well-known mathematical conjectures of very long standing, such as the Four Color Theorem, Fermat's Last Theorem, and the Goldbach conjecture, are provable using extensions of the primary algebra. Spencer-Brown eventually circulated a purported proof of the Four Color Theorem, but it met with skepticism.For a sympathetic evaluation, see Kauffman (2001).\n\nThe form (Chapter 1)\nThe symbol:\n\nImage:Laws of Form - cross.gif\n\nalso called the \"mark\" or \"cross\", is the essential feature of the Laws of Form. In Spencer-Brown's inimitable and enigmatic fashion, the Mark symbolizes the root of cognition, i.e., the dualistic Mark indicates the capability of differentiating a \"this\" from \"everything else but this.\"\n\nIn LoF, a Cross denotes the drawing of a \"distinction\", and can be thought of as signifying the following, all at once:\n The act of drawing a boundary around something, thus separating it from everything else;\n That which becomes distinct from everything by drawing the boundary;\n Crossing from one side of the boundary to the other.\n\nAll three ways imply an action on the part of the cognitive entity (e.g., person) making the distinction.  As LoF puts it:\n\"The first command:\n Draw a distinction\ncan well be expressed in such ways as:\n Let there be a distinction,\n Find a distinction,\n See a distinction,\n Describe a distinction,\n Define a distinction,\nOr:\n Let a distinction be drawn.\" (LoF, Notes to chapter 2)\n\nThe counterpoint to the Marked state is the Unmarked state, which is simply nothing, the void, or the un-expressable infinite represented by a blank space. It is simply the absence of a Cross. No distinction has been made and nothing has been crossed. The Marked state and the void are the two primitive values of the Laws of Form.\n\nThe Cross can be seen as denoting the distinction between two states, one \"considered as a symbol\" and another not so considered. From this fact arises a curious resonance with some theories of consciousness and language. Paradoxically, the Form is at once Observer and Observed, and is also the creative act of making an observation. LoF (excluding back matter) closes with the words:\n\n\"...the first distinction, the Mark and the observer are not only interchangeable, but, in the form, identical.\"\n\nC. S. Peirce came to a related insight in the 1890s; see Related Work.\n\nThe primary arithmetic (Chapter 4)\nThe syntax of the primary arithmetic goes as follows. There are just two atomic expressions:\n The empty Cross Image:Laws of Form - cross.gif  ;\nAll or part of the blank page (the \"void\").\nThere are two inductive rules:\n A Cross Image:Laws of Form - cross.gif  may be written over any expression;\n Any two expressions may be concatenated.\nThe semantics of the primary arithmetic are perhaps nothing more than the sole explicit definition in LoF: \"Distinction is perfect continence\".\n\nLet the \"unmarked state\" be a synonym for the void. Let an empty Cross denote the \"marked state\". To cross is to move from one value, the unmarked or marked state, to the other. We can now state the \"arithmetical\" axioms A1 and A2, which ground the primary arithmetic (and hence all of the Laws of Form):\n\n\"A1. The law of Calling\". Calling twice from a state is indistinguishable from calling once. To make a distinction twice has the same effect as making it once. For example, saying \"Let there be light\" and then saying \"Let there be light\" again, is the same as saying it once.  Formally:\n\nImage:Laws of Form - cross.gif Image:Laws of Form - cross.gif Image:Laws of Form - cross.gif\n\n\"A2. The law of Crossing.\" After crossing from the unmarked to the marked state, crossing again (\"recrossing\") starting from the marked state returns one to the unmarked state. Hence recrossing annuls crossing. Formally:\n\nImage:Laws of Form - double cross.gif \n\nIn both A1 and A2, the expression to the right of '=' has fewer symbols than the expression to the left of '='. This suggests that every primary arithmetic expression can, by repeated application of A1 and A2, be simplified to one of two states: the marked or the unmarked state. This is indeed the case, and the result is the expression's \"simplification\". The two fundamental metatheorems of the primary arithmetic state that:\n Every finite expression has a unique simplification. (T3 in LoF);\n Starting from an initial marked or unmarked state, \"complicating\" an expression by a finite number of repeated application of A1 and A2 cannot yield an expression whose simplification differs from the initial state. (T4 in LoF).\nThus the relation of logical equivalence partitions all primary arithmetic expressions into two equivalence classes: those that simplify to the Cross, and those that simplify to the void.\n\nA1 and A2 have loose analogs in the properties of series and parallel electrical circuits, and in other ways of diagramming processes, including flowcharting. A1 corresponds to a parallel connection and A2 to a series connection, with the understanding that making a distinction corresponds to changing how two points in a circuit are connected, and not simply to adding wiring.\n\nThe primary arithmetic is analogous to the following formal languages from mathematics and computer science:\n A Dyck language of order 1 with a null alphabet;\n The simplest context-free language in the Chomsky hierarchy;\n A rewrite system that is strongly normalizing and confluent.\n\nThe phrase \"calculus of indications\" in LoF is a synonym for \"primary arithmetic\".\n\nThe notion of canon\nA concept peculiar to LoF is that of \"canon\". While LoF does not define canon, the following two excerpts from the Notes to chpt. 2 are apt:\n\n\"The more important structures of command are sometimes called canons. They are the ways in which the guiding injunctions appear to group themselves in constellations, and are thus by no means independent of each other. A canon bears the distinction of being outside (i.e., describing) the system under construction, but a command to construct (e.g., 'draw a distinction'), even though it may be of central importance, is not a canon. A canon is an order, or set of orders, to permit or allow, but not to construct or create.\"\n\n\"...the primary form of mathematical communication is not description but injunction... Music is a similar art form, the composer does not even attempt to describe the set of sounds he has in mind, much less the set of feelings occasioned through them, but writes down a set of commands which, if they are obeyed by the performer, can result in a reproduction, to the listener, of the composer's original experience.\"\n\nThese excerpts relate to the distinction in metalogic between the object language, the formal language of the logical system under discussion, and the metalanguage, a language (often a natural language) distinct from the object language, employed to exposit and discuss the object language. The first quote seems to assert that the canons are part of the metalanguage. The second quote seems to assert that statements in the object language are essentially commands addressed to the reader by the author. Neither assertion holds in standard metalogic.\n\nThe primary algebra (Chapter 6)\n\nSyntax\nGiven any valid primary arithmetic expression, insert into one or more locations any number of Latin letters bearing optional numerical subscripts; the result is a primary algebra formula. Letters so employed in mathematics and logic are called variables. A primary algebra variable indicates a location where one can write the primitive value Image:Laws of Form - cross.gif or its complement Image:Laws of Form - double cross.gif. Multiple instances of the same variable denote multiple locations of the same primitive value.\n\nRules governing logical equivalence\nThe sign '=' may link two logically equivalent expressions; the result is an equation. By \"logically equivalent\" is meant that the two expressions have the same simplification. Logical equivalence is an equivalence relation over the set of primary algebra formulas, governed by the rules R1 and R2. Let \"C\" and \"D\" be formulae each containing at least one instance of the subformula A:R1, Substitution of equals. Replace one or more instances of A in C by B, resulting in E. If A=B, then C=E.R2, Uniform replacement. Replace all instances of A in C and D with B. C becomes E and D becomes F. If C=D, then E=F. Note that A=B is not required.R2 is employed very frequently in primary algebra demonstrations (see below), almost always silently. These rules are routinely invoked in logic and most of mathematics, nearly always unconsciously.\n\nThe primary algebra consists of equations, i.e., pairs of formulae linked by an infix '='. R1 and R2 enable transforming one equation into another. Hence the primary algebra is an equational  formal system, like the many algebraic structures, including Boolean algebra, that are varieties. Equational logic was common before Principia Mathematica (e.g., Peirce,1,2,3 Johnson 1892), and has present-day advocates (Gries and Schneider 1993).\n\nConventional mathematical logic consists of tautological formulae, signalled by a prefixed turnstile. To denote that the primary algebra formula A is a tautology, simply write \"A =Image:Laws of Form - cross.gif \". If one replaces '=' in R1 and R2 with the biconditional, the resulting rules hold in conventional logic. However, conventional logic relies mainly on the rule modus ponens; thus conventional logic is ponential. The equational-ponential dichotomy distills much of what distinguishes mathematical logic from the rest of mathematics.\n\nInitials\nAn initial is a primary algebra equation verifiable by a decision procedure and as such is not an axiom. LoF lays down the initials:\n\n \nJ1: A A = .\nThe absence of anything to the right of the \"=\" above, is deliberate.\n\n \nJ2: A B C = A C B C.J2 is the familiar distributive law of sentential logic and Boolean algebra.\n\nAnother set of initials, friendlier to calculations, is:\n\n \nJ0: A = A.\n\nJ1a: A A =.\n\n \nC2: A A B =  A B.\n\nIt is thanks to C2 that the primary algebra is a lattice. By virtue of J1a, it is a complemented lattice whose upper bound is Image:Laws_of_Form_-_cross.gif. By J0, Image:Laws_of_Form_-_double_cross.gif is the corresponding lower bound and identity element. J0 is also an algebraic version of A2 and makes clear the sense in which Image:Laws_of_Form_-_double_cross.gif aliases with the blank page.\n\nT13 in LoF generalizes C2 as follows. Any primary algebra (or sentential logic) formula B can be viewed as an ordered tree with branches. Then:T13: A subformula A can be copied at will into any depth of B greater than that of A, as long as A and its copy are in the same branch of B. Also, given multiple instances of A in the same branch of B, all instances but the shallowest are redundant.\n\nWhile a proof of T13 would require induction, the intuition underlying it should be clear.C2 or its equivalent is named:\n\"Generation\" in LoF;\n\"Exclusion\" in Johnson (1892);\n\"Pervasion\" in the work of William Bricken.\nPerhaps the first instance of an axiom or rule with the power of C2 was the \"Rule of (De)Iteration,\" combining T13 and AA=A, of C. S. Peirce's existential graphs.\n\nLoF asserts that concatenation can be read as commuting and associating by default and hence need not be explicitly assumed or demonstrated. (Peirce made a similar assertion about his existential graphs.) Let a period be a temporary notation to establish grouping. That concatenation commutes and associates may then be demonstrated from the:\n Initial AC.D=CD.A and the consequence AA=A (Byrne 1946). This result holds for all lattices, because AA=A is an easy consequence of the absorption law, which holds for all lattices;\n Initials AC.D=AD.C and J0. Since J0 holds only for lattices with a lower bound, this method holds only for bounded lattices (which include the primary algebra and 2). Commutativity is trivial; just set A=Image:Laws_of_Form_-_double_cross.gif. Associativity: AC.D = CA.D = CD.A = A.CD.\nHaving demonstrated associativity, the period can be discarded.\n\nThe initials in Meguire (2011) are AC.D=CD.A, called B1; B2, J0 above; B3, J1a above; and B4, C2. By design, these initials are very similar to the axioms for an abelian group, G1-G3 below.\n\nProof theory\nThe primary algebra contains three kinds of proved assertions:\n Consequence is a primary algebra equation verified by a demonstration. A demonstration consists of a sequence of steps, each step justified by an initial or a previously demonstrated consequence.\n Theorem is a statement in the metalanguage verified by a proof, i.e., an argument, formulated in the metalanguage, that is accepted by trained mathematicians and logicians.\n Initial, defined above. Demonstrations and proofs invoke an initial as if it were an axiom.\n\nThe distinction between consequence and theorem holds for all formal systems, including mathematics and logic, but is usually not made explicit. A demonstration or decision procedure can be carried out and verified by computer. The proof of a theorem cannot be.\n\nLet A and B be primary algebra formulas. A demonstration of A=B may proceed in either of two ways:\n Modify A in steps until B is obtained, or vice versa;\n Simplify both 50px and 50px to Image:Laws of Form - cross.gif. This is known as a \"calculation\".\nOnce A=B has been demonstrated, A=B can be invoked to justify steps in subsequent demonstrations. primary algebra demonstrations and calculations often require no more than J1a, J2, C2, and the consequences 80px (C3 in LoF), 80px (C1), and AA=A (C5).\n\nThe consequence 170px, C7 in LoF, enables an algorithm, sketched in LoFs proof of T14, that transforms an arbitrary primary algebra formula to an equivalent formula whose depth does not exceed two. The result is a normal form, the primary algebra analog of the conjunctive normal form. LoF (T14-15) proves the primary algebra analog of the well-known Boolean algebra theorem that every formula has a normal form.\n\nLet A be a subformula of some formula B. When paired with C3, J1a can be viewed as the closure condition for calculations: B is a tautology if and only if A and (A) both appear in depth 0 of B. A related condition appears in some versions of natural deduction. A demonstration by calculation is often little more than:\n Invoking T13 repeatedly to eliminate redundant subformulae;\n Erasing any subformulae having the form 50px.\nThe last step of a calculation always invokes J1a.\n\nLoF includes elegant new proofs of the following standard metatheory:\n Completeness: all primary algebra consequences are demonstrable from the initials (T17).\n Independence: J1 cannot be demonstrated from J2 and vice versa (T18).\nThat sentential logic is complete is taught in every first university course in mathematical logic. But university courses in Boolean algebra seldom mention the completeness of 2.\n\nInterpretations\nIf the Marked and Unmarked states are read as the Boolean values 1 and 0 (or True and False), the primary algebra interprets 2 (or sentential logic). LoF shows how the primary algebra can interpret the syllogism. Each of these interpretations is discussed in a subsection below. Extending the primary algebra so that it could interpret standard first-order logic has yet to be done, but Peirce's beta existential graphs suggest that this extension is feasible.\n\nTwo-element Boolean algebra 2\nThe primary algebra is an elegant minimalist notation for the two-element Boolean algebra 2. Let:\n One of Boolean join (+) or meet (×) interpret concatenation;\n The complement of A interpret Image:Laws of Form - not a.gif\n 0 (1) interpret the empty Mark if join (meet) interprets concatenation (because a binary operation applied to zero operands may be regarded as being equal to the identity element of that operation; or to put it in another way, an operand that is missing could be regarded as acting by default like the identity element). \nIf join (meet) interprets AC, then meet (join) interprets ((A)(C)). Hence the primary algebra and 2 are isomorphic but for one detail: primary algebra complementation can be nullary, in which case it denotes a primitive value. Modulo this detail, 2 is a model of the primary algebra. The primary arithmetic suggests the following arithmetic axiomatization of 2: 1+1=1+0=0+1=1=~0, and 0+0=0=~1.\n\nThe set Image:Laws of Form - cross.gif  Image:Laws of Form - double cross.gif is the Boolean domain or carrier. In the language of universal algebra, the primary algebra is the algebraic structure  of type . The expressive adequacy of the Sheffer stroke points to the primary algebra also being a  algebra of type . In both cases, the identities are J1a, J0, C2, and ACD=CDA. Since the primary algebra and 2 are isomorphic, 2 can be seen as a  algebra of type . This description of 2 is simpler than the conventional one, namely an  algebra of type .\n\nThe two possible interpretations are dual to each other in the Boolean sense. (In Boolean algebra, exchanging AND ↔ OR and 1 ↔ 0 throughout an equation yields an equally valid equation.) The identities remain invariant regardless of which interpretation is chosen, so the transformations or modes of calculation remain the same; only the interpretation of each form would be different. Example: J1a is 80px. Interpreting juxtaposition as OR and 30px as 1, this translates to  which is true. Interpreting juxtaposition as AND and 30px as 0, this translates to  which is true as well (and the dual of ).\n\nSentential logic\nLet the blank page denote True or False, and let a Cross be read as Not. Then the primary arithmetic has the following sentential reading:\n\n =   FalseImage:Laws of Form - cross.gif  =  True  =  not FalseImage:Laws of Form - double cross.gif  =  Not True  =  FalseThe primary algebra interprets sentential logic as follows. A letter represents any given sentential expression. Thus:\n\nImage:Laws of Form - not a.gif interprets Not AImage:Laws of Form - a or b.gif interprets A Or BImage:Laws of Form - if a then b.gif interprets Not A Or B  or   If A Then B.\n\nImage:Laws of Form - a and b.gif interprets Not (Not A Or Not B)or Not (If A Then Not B)or A And B.\n\n          <big><big><big>a <big><big><big>b <big><big><big>a <big><big><big>b, <big><big><big>a <big><big><big>b <big><big><big>a b both interpret A if and only if B or A is equivalent to B.\n\nThus any expression in sentential logic has a primary algebra translation. Equivalently, the primary algebra interprets sentential logic. Given an assignment of every variable to the Marked or Unmarked states, this primary algebra translation reduces to a primary arithmetic expression, which can be simplified. Repeating this exercise for all possible assignments of the two primitive values to each variable, reveals whether the original expression is tautological or satisfiable. This is an example of a decision procedure, one more or less in the spirit of conventional truth tables. Given some primary algebra formula containing N variables, this decision procedure requires simplifying 2N primary arithmetic formulae. For a less tedious decision procedure more in the spirit of Quine's \"truth value analysis,\" see Meguire (2003).\n\nSchwartz (1981) proved that the primary algebra is equivalent -- syntactically, semantically, and proof theoretically—with the classical propositional calculus. Likewise, it can be shown that the primary algebra is syntactically equivalent with expressions built up in the usual way from the classical truth values true and false, the logical connectives NOT, OR, and AND, and parentheses.\n\nInterpreting the Unmarked State as False is wholly arbitrary; that state can equally well be read as True. All that is required is that the interpretation of concatenation change from OR to AND. IF A THEN B now translates as 50px instead of 50px. More generally, the primary algebra is \"self-dual,\" meaning that any primary algebra formula has two sentential or Boolean readings, each the dual of the other. Another consequence of self-duality is the irrelevance of De Morgan's laws; those laws are built into the syntax of the primary algebra from the outset.\n\nThe true nature of the distinction between the primary algebra on the one hand, and 2 and sentential logic on the other, now emerges. In the latter formalisms, complementation/negation operating on \"nothing\" is not well-formed. But an empty Cross is a well-formed primary algebra expression, denoting the Marked state, a primitive value. Hence a nonempty Cross is an operator, while an empty Cross is an operand because it denotes a primitive value. Thus the primary algebra reveals that the heretofore distinct mathematical concepts of operator and operand are in fact merely different facets of a single fundamental action, the making of a distinction.\n\nSyllogisms\nAppendix 2 of LoF shows how to translate traditional syllogisms and sorites into the primary algebra. A valid syllogism is simply one whose primary algebra translation simplifies to an empty Cross. Let A* denote a literal, i.e., either A or (A), indifferently. Then every syllogism that does not require that one or more terms be assumed nonempty is one of 24 possible permutations of a generalization of Barbara whose primary algebra equivalent is (A*B)((B)C*)A*C*. These 24 possible permutations include the 19 syllogistic forms deemed valid in Aristotelian and medieval logic. This primary algebra translation of syllogistic logic also suggests that the primary algebra can interpret monadic and term logic, and that the primary algebra has affinities to the Boolean term schemata of Quine (1982: Part II).\n\nAn example of calculation\nThe following calculation of Leibniz's nontrivial Praeclarum Theorema exemplifies the demonstrative power of the primary algebra. Let C1 be ((A))=A, C2 be A(A B)=A(B), C3 be ()A=(), J1a be (A)A=(), and let OI mean that variables and subformulae have been reordered in a way that commutativity and associativity permit.\n\n [(P→R)∧(Q→S)]→[(P∧Q)→(R∧S)]. Praeclarum Theorema. P R Q S P Q R S . primary algebra translation P R Q S P Q R S . C1. P R Q S P Q R S . C1. P P R Q S Q R S . OI. P R Q S Q R S . C2. P R Q Q S R S . OI. P R Q S R S . C2. P Q S R R S . OI. P Q S R S . C2. P Q S R S . C1. P Q S S R . OI. P Q B R . J1a. B P Q R. OI. \n B C3. \n\nRelation to magmas\nThe primary algebra embodies a point noted by Huntington in 1933: Boolean algebra requires, in addition to one unary operation, one, and not two, binary operations. Hence the seldom-noted fact that Boolean algebras are magmas. (Magmas were called groupoids until the latter term was appropriated by category theory.) To see this, note that the primary algebra is a commutative:\nSemigroup because primary algebra juxtaposition commutes and associates;\nMonoid with identity element Image:Laws of Form - double cross.gif, by virtue of J0.\n\nGroups also require a unary operation, called inverse, the group counterpart of Boolean complementation. Let 20px denote the inverse of a. Let Image:Laws of Form - cross.gif denote the group identity element. Then groups and the primary algebra have the same signatures, namely they are both 〈--,(-),()〉 algebras of type 〈2,1,0〉. Hence the primary algebra is a boundary algebra. The axioms for an abelian group, in boundary notation, are:\n G1. abc = acb (assuming association from the left);\n G2. 80px\n G3. 80px.\nFrom G1 and G2, the commutativity and associativity of concatenation may be derived, as above. Note that G3 and J1a are identical. G2 and J0 would be identical if   25px = 20px   replaced A2. This is the defining arithmetical identity of group theory, in boundary notation.\n\nThe primary algebra differs from an abelian group in two ways:\nFrom A2, it follows that Image:Laws of Form - double cross.gif ≠ Image:Laws of Form - cross.gif. If the primary algebra were a group, Image:Laws of Form - double cross.gif = Image:Laws of Form - cross.gif would hold, and one of   20px a = 30px   or   a 30px = a   would have to be a primary algebra consequence. Note that 20px and 25px are mutual primary algebra complements, as group theory requires, so that ((())) = () is true of both group theory and the primary algebra;C2 most clearly demarcates the primary algebra from other magmas, because C2 enables demonstrating the absorption law that defines lattices, and the distributive law central to Boolean algebra.\nBoth A2 and C2 follow from B 's being an ordered set.\n\nEquations of the second degree (Chapter 11)\nChapter 11 of LoF introduces equations of the second degree, composed of recursive formulae that can be seen as having \"infinite\" depth. Some recursive formulae simplify to the marked or unmarked state. Others \"oscillate\" indefinitely between the two states depending on whether a given depth is even or odd. Specifically, certain recursive formulae can be interpreted as oscillating between true and false over successive intervals of time, in which case a formula is deemed to have an \"imaginary\" truth value. Thus the flow of time may be introduced into the primary algebra.\n\nTurney (1986) shows how these recursive formulae can be interpreted via Alonzo Church's Restricted Recursive Arithmetic (RRA). Church introduced RRA in 1955 as an axiomatic formalization of finite automata. Turney (1986) presents a general method for translating equations of the second degree into Church's RRA, illustrating his method using the formulae E1, E2, and E4 in chapter 11 of LoF. This translation into RRA sheds light on the names Spencer-Brown gave to E1 and E4, namely \"memory\" and \"counter\". RRA thus formalizes and clarifies LoF 's notion of an imaginary truth value.\n\nRelated work\nGottfried Leibniz, in memoranda not published before the late 19th and early 20th centuries, invented Boolean logic. His notation was isomorphic to that of LoF: concatenation read as conjunction, and \"non-(X)\" read as the complement of X. Leibniz's pioneering role in algebraic logic was foreshadowed by Lewis (1918) and Rescher (1954). But a full appreciation of Leibniz's accomplishments had to await the work of Wolfgang Lenzen, published in the 1980s and reviewed in Lenzen (2004).\n\nCharles Sanders Peirce (1839–1914) anticipated the primary algebra in three veins of work:\nTwo papers he wrote in 1886 proposed a logical algebra employing but one symbol, the streamer, nearly identical to the Cross of LoF.  The semantics of the streamer are identical to those of the Cross, except that Peirce never wrote a streamer with nothing under it. An excerpt from one of these papers was published in 1976,\"Qualitative Logic\", MS 736 (c. 1886) in Eisele, Carolyn, ed. 1976. The New Elements of Mathematics by Charles S. Peirce. Vol. 4, Mathematical Philosophy. (The Hague) Mouton: 101-15.1 but they were not published in full until 1993.\"Qualitative Logic\", MS 582 (1886) in Kloesel, Christian et al., eds., 1993. Writings of Charles S. Peirce: A Chronological Edition, Vol. 5, 1884-1886. Indiana University Press: 323-71. \"The Logic of Relatives: Qualitative and Quantitative\", MS 584 (1886) in Kloesel, Christian et al., eds., 1993. Writings of Charles S. Peirce: A Chronological Edition, Vol. 5, 1884-1886. Indiana University Press: 372-78.\nIn a 1902 encyclopedia article,Reprinted in Peirce, C.S. (1933) Collected Papers of Charles Sanders Peirce, Vol. 4, Charles Hartshorne and Paul Weiss, eds. Harvard University Press. Paragraphs 378-383 Peirce notated Boolean algebra and sentential logic in the manner of this entry, except that he employed two styles of brackets, toggling between '(', ')' and '[', ']' with each increment in formula depth.\nThe syntax of his alpha existential graphs is merely concatenation, read as conjunction, and enclosure by ovals, read as negation.The existential graphs are described at length in Peirce, C.S. (1933) Collected Papers, Vol. 4, Charles Hartshorne and Paul Weiss, eds. Harvard University Press. Paragraphs 347-529. If primary algebra concatenation is read as conjunction, then these graphs are isomorphic to the primary algebra (Kauffman 2001).\nIronically, LoF cites vol. 4 of Peirce's Collected Papers, the source for the formalisms in (2) and (3) above.\n(1)-(3) were virtually unknown at the time when (1960s) and in the place where (UK) LoF was written. Peirce's semiotics, about which LoF is silent, may yet shed light on the philosophical aspects of LoF.\n\nKauffman  (2001) discusses another notation similar to that of LoF, that of a 1917 article by Jean Nicod, who was a disciple of Bertrand Russell's.\n\nThe above formalisms are, like the primary algebra, all instances of boundary mathematics, i.e., mathematics whose syntax is limited to letters and brackets (enclosing devices). A minimalist syntax of this nature is a \"boundary notation.\" Boundary notation is free of infix, prefix, or postfix operator symbols. The very well known curly braces ('{', '}') of set theory can be seen as a boundary notation.\n\nThe work of Leibniz, Peirce, and Nicod is innocent of metatheory, as they wrote before Emil Post's landmark 1920 paper (which LoF cites), proving that sentential logic is complete, and before Hilbert and Łukasiewicz showed how to prove axiom independence using models.\n\nCraig (1979) argued that the world, and how humans perceive and interact with that world, has a rich Boolean structure. Craig was an orthodox logician and an authority on algebraic logic.\n\nSecond-generation cognitive science emerged in the 1970s, after LoF was written. On cognitive science and its relevance to Boolean algebra, logic, and set theory, see Lakoff (1987) (see index entries under \"Image schema examples: container\") and Lakoff and Núñez (2001). Neither book cites LoF.\n\nThe biologists and cognitive scientists Humberto Maturana and his student Francisco Varela both discuss LoF in their writings, which identify \"distinction\" as the fundamental cognitive act. The Berkeley psychologist and cognitive scientist Eleanor Rosch has written extensively on the closely related notion of categorization.\n\nOther formal systems with possible affinities to the primary algebra include:\nMereology which typically has a lattice structure very similar to that of Boolean algebra. For a few authors, mereology is simply a model of Boolean algebra and hence of the primary algebra as well.\nMereotopology, which is inherently richer than Boolean algebra;\nThe system of  Whitehead (1934), whose fundamental primitive is \"indication.\"\n\nThe primary arithmetic and algebra are a minimalist formalism for sentential logic and Boolean algebra. Other minimalist formalisms having the power of set theory include:\n The lambda calculus;\n Combinatory logic with two (S and K) or even one (X''') primitive combinators;\n Mathematical logic done with merely three primitive notions: one connective, NAND (whose primary algebra translation is (AB) or, dually, (A)(B)), universal quantification, and one binary atomic formula, denoting set membership. This is the system of Quine (1951).\n The beta existential graphs, with a single binary predicate denoting set membership. This has yet to be explored. The alpha graphs mentioned above are a special case of the beta graphs.\n\nSee also\n Boolean algebra (Simple English Wikipedia)\nBoolean algebra (introduction)\nBoolean algebra (logic)\nBoolean algebra (structure)\nBoolean algebras canonically defined\nBoolean logic\nEntitative graph\nExistential graph\nList of Boolean algebra topics\nPropositional calculus\nTwo-element Boolean algebra\n\nNotes\n\nReferences\nEditions of Laws of Form:\n1969. London: Allen & Unwin, hardcover.\n1972. Crown Publishers, hardcover: \n1973. Bantam Books, paperback. \n1979. E.P. Dutton, paperback. \n1994. Portland OR: Cognizer Company, paperback. \n1997 German translation, titled Gesetze der Form. Lübeck: Bohmeier Verlag. \n2008 Bohmeier Verlag, Leipzig, 5th international edition. \nBostock, David, 1997. Intermediate Logic. Oxford Univ. Press.\nByrne, Lee, 1946, \"Two Formulations of Boolean Algebra,\" Bulletin of the American Mathematical Society: 268-71.\n\n David Gries, and Schneider, F B, 1993. A Logical Approach to Discrete Math. Springer-Verlag.\nWilliam Ernest Johnson, 1892, \"The Logical Calculus,\" Mind 1 (n.s.): 3-30.\n Louis H. Kauffman, 2001, \"The Mathematics of C.S. Peirce\", Cybernetics and Human Knowing 8: 79-110.\n ------, 2006, \"Reformulating the Map Color Theorem.\"\n ------, 2006a. \"Laws of Form - An Exploration in Mathematics and Foundations.\" Book draft (hence big).\n Lenzen, Wolfgang, 2004, \"Leibniz's Logic\" in Gabbay, D., and Woods, J., eds., The Rise of Modern Logic: From Leibniz to Frege (Handbook of the History of Logic – Vol. 3). Amsterdam: Elsevier, 1-83.\nLakoff, George, 1987. Women, Fire, and Dangerous Things. University of Chicago Press.\n-------- and Rafael E. Núñez, 2001. Where Mathematics Comes From: How the Embodied Mind Brings Mathematics into Being. Basic Books.\n \n--------, 2011. Boundary Algebra: A Simpler Approach to Basic Logic and Boolean Algebra. VDM Publishing Ltd. . The source for much of this entry, including the notation which encloses in parentheses what LoF places under a cross. Steers clear of the more speculative aspects of LoF.\nWillard Quine, 1951. Mathematical Logic, 2nd ed. Harvard University Press.\n--------, 1982. Methods of Logic, 4th ed. Harvard University Press.\n \n \n \nA. N. Whitehead, 1934, \"Indication, classes, number, validation,\" Mind 43 (n.s.): 281-97, 543. The corrigenda on p. 543 are numerous and important, and later reprints of this article do not incorporate them.\nDirk Baecker (ed.) (1993), Kalkül der Form. Suhrkamp; Dirk Baecker (ed.), Probleme der Form. Suhrkamp.\nDirk Baecker (ed.) (1999), Problems of Form, Stanford University Press.\nDirk Baecker (ed.) (2013), A Mathematics of Form, A Sociology of Observers, Cybernetics & Human Knowing, vol. 20, no. 3-4.\n\nExternal links\n Laws of Form, archive of website by Richard Shoup.\n Spencer-Brown's talks at Esalen, 1973. Self-referential forms are introduced in the section entitled \"Degree of Equations and the Theory of Types.\"\n Louis H. Kauffman, \"Box Algebra, Boundary Mathematics, Logic, and Laws of Form.\"\n Kissel, Matthias, \" A nonsystematic but easy to understand introduction to Laws of Form.\"\n The Laws of Form Forum, where the primary algebra and related formalisms have been discussed since 2002.\n A meeting  with G.S.B by Moshe Klein\n\n The Markable Mark, an introduction by easy stages to the ideas of Laws of Form''\n\nCategory:Algebra\nCategory:Boolean algebra\nCategory:Logic books\nCategory:Logical calculi\nCategory:Mathematical logic"
    },
    {
      "title": "Lie operad",
      "url": "https://en.wikipedia.org/wiki/Lie_operad",
      "text": "In mathematics, the Lie operad is an operad whose algebras are Lie algebras. The notion (at least one version) was introduced by  in their formulation of Koszul duality.\n\n Definition à la Ginzburg–Kapranov \nLet  denote the free Lie algebra (over some field) with the generators  and  the subspace spanned by all the bracket monomials containing each  exactly once. The symmetric group  acts on  by permutations and, under that action,  is invariant. Hence,  is an operad.\n\nThe Koszul-dual of  is the commutative-ring operad, an operad whose algebras are commutative rings.\n\n Notes \n\nReferences\n\n External links \nTodd Trimble, Notes on operads and the Lie operad\nhttps://ncatlab.org/nlab/show/Lie+operad\n\nCategory:Algebra"
    },
    {
      "title": "Linearly disjoint",
      "url": "https://en.wikipedia.org/wiki/Linearly_disjoint",
      "text": "In mathematics, algebras A, B over a field k inside some field extension  of k  are said to be linearly disjoint over k if the following equivalent conditions are met:\n(i) The map  induced by  is injective.\n(ii) Any k-basis of A remains linearly independent over B.\n(iii) If  are k-bases for A, B, then the products  are linearly independent over k.\n\nNote that, since every subalgebra of  is a domain, (i) implies  is a domain (in particular reduced). Conversely if A and B are fields and either A or B is an algebraic extension of k and  is a domain then it is a field and A and B are linearly disjoint. However there are examples where  is a domain but A and B are not linearly disjoint: for example, A=B=k(t), the field of rational functions over k.\n\nOne also has: A, B are linearly disjoint over k if and only if subfields of  generated by , resp. are linearly disjoint over k. (cf. tensor product of fields)\n\nSuppose A, B are linearly disjoint over k. If ,  are subalgebras, then  and  are linearly disjoint over k. Conversely, if any finitely generated subalgebras of algebras A, B are linearly disjoint, then A, B are linearly disjoint (since the condition involves only finite sets of elements.)\n\n See also \nTensor product of fields\n\n References \n P.M. Cohn (2003). Basic algebra\n\nCategory:Algebra"
    },
    {
      "title": "Map algebra",
      "url": "https://en.wikipedia.org/wiki/Map_algebra",
      "text": "Map algebra is a set-based algebra for manipulating geographic data, proposed by Dr. Dana Tomlin in the early 1980s.  It is a set of primitive operations in a geographic information system (GIS) which allows two or more raster layers (\"maps\") of similar dimensions to produce a new raster layer (map) using algebraic operations such as addition, subtraction etc.\n\nDepending on the spatial neighborhood, GIS transformations are categorized into four classes: local, focal, global, and zonal. Local operations works on individual raster cells, or pixels. Focal operations work on cells and their neighbors, whereas global operations work on the entire layer. Finally, zonal operations work on areas of cells that share the same value. The input and output for each operator being map, the operators can be combined into a procedure or script, to perform complex tasks.\n\nWhen map algebra is performed in cells from local operations, different types of operations can be used:\n Arithmetic operations uses basic mathematical functions like addition, subtraction, multiplication and division.\n Statistical operations uses statistical operations such as minimum, maximum, average and median.\n Relational operations compares cells using functions such as greater than, smaller than or equal to.\n Trigonometric operations uses sine, cosine, tangent, arcsine between two or more raster layers.\n Exponential and logarithmic operations use exponent and logarithm functions.\n\nSeveral major GIS systems use map algebra concepts, including ERDAS Imagine and ArcGIS. ArcGIS 10 implements Map Algebra in Python; functions are imported Python methods and Python's overloading capability is used for operators. For example, rasters can be multiplied using the \"*\" arithmetic operator.\n\nHere are some examples, in MapBasic:\n# demo for Brown's Pond data set\n# Give layers\n#  altitude\n#  development – 0: vacant, 1: major, 2: minor, 3: houses, 4: buildings, 5 cement\n#  water – 0: dry, 2: wet, 3: pond\n\n# calculate the slope at each location based on altitude\nslope = IncrementalGradient of altitude\n\n# identify the areas that are too steep\ntoosteep = LocalRating of slope\n  where 1 replaces 4 5 6\n  where VOID replaces ...\n\n# create layer unifying water and development\noccupied = LocalRating of development\n  where water replaces VOID\n\nnotbad = LocalRating of occupied and toosteep\n  where 1 replaces VOID and VOID\n  where VOID replaces ... and ...\n\nroads = LocalRating of development\n  where 1 replaces 1 2\n  where VOID replaces ...\n\nnearread = FocalNeighbor of roads at 0 ... 10\n\naspect = IncrementalAspect of altitude\n\nsouthface = LocalRating of aspect\n  where 1 replaces 135 ... 225\n  where VOID replaces ...\n\nsites = LocalMinimum of nearroad and southface and notbad\n\nsitenums = FocalInsularity of sites at 0 ... 1\n\nsitesize = ZonalSum of 1 within sitenums\n\nbestsites = LocalRating of sitesize\n  where sitesize replaces 100 ... 300\n  where VOID replaces ...\n\nExternal links \n osGeo-RFC-39 about Layer Algebra\n\nReferences\n\nB. E. Davis GIS: A Visual Approach (2001 Cengage Learning) pp. 249ff.\n\nCategory:Geographic information systems\nCategory:Applied mathematics\nCategory:Algebra"
    },
    {
      "title": "Median algebra",
      "url": "https://en.wikipedia.org/wiki/Median_algebra",
      "text": "In mathematics, a median algebra is a set with a ternary operation  satisfying a set of axioms which generalise the notion of median or majority function, as a Boolean function.\n\nThe axioms are \n   \n     \n       \n   \n\nThe second and third axioms imply commutativity: it is possible (but not easy) to show that in the presence of the other three, axiom (3) is redundant.  The fourth axiom implies associativity.\nThere are other possible axiom systems: for example the two\n  \n  \nalso suffice.\n\nIn a Boolean algebra, or more generally a distributive lattice, the median function  satisfies these axioms, so that every Boolean algebra and every distributive lattice forms a median algebra.\n\nBirkhoff and Kiss showed that a median algebra with elements  and  satisfying  is a distributive lattice.\n\nRelation to median graphs\nA median graph is an undirected graph in which for every three vertices , , and  there is a unique vertex  that belongs to shortest paths between any two of , , and . If this is the case, then the operation  defines a median algebra having the vertices of the graph as its elements.\n\nConversely, in any median algebra, one may define an interval  to be the set of elements  such that . One may define a graph from a median algebra by creating a vertex for each algebra element and an edge for each pair  such that the interval  contains no other elements. If the algebra has the property that every interval is finite, then this graph is a median graph, and it accurately represents the algebra in that the median operation defined by shortest paths on the graph coincides with the algebra's original median operation.\n\nReferences\n \n \n \n\nExternal links\n Median Algebra Proof\n\nCategory:Algebra\nCategory:Ternary operations"
    },
    {
      "title": "Minimal algebra",
      "url": "https://en.wikipedia.org/wiki/Minimal_algebra",
      "text": "Minimal algebra is an important concept in tame congruence theory, a theory that has been developed by Ralph McKenzie and David Hobby .\nDefinition\nA minimal algebra is a finite algebra with more than one element, in which every non-constant unary polynomial is a permutation on its domain.\nClassification\nA polynomial of an algebra is a composition of its basic operations, -ary operations and the projections. Two algebras are called polynomially equivalent if they have the same universe and precisely the same polynomial operations. A minimal algebra  falls into one of the following types (P. P. Pálfy)   \n\n  is of type , or unary type, iff , where  denotes the universe of ,  denotes the set of all polynomials of an algebra  and  is a subgroup of the symmetric group over .\n\n  is of type , or affine type, iff  is polynomially equivalent to a vector space.\n\n  is of type , or Boolean type, iff  is polynomially equivalent to a two-element Boolean algebra.\n\n  is of type , or lattice type, iff  is polynomially equivalent to a two-element lattice.\n\n  is of type , or semilattice type, iff  is polynomially equivalent to a two-element semilattice.\n\nReferences\n\nCategory:Algebra"
    },
    {
      "title": "Mode-k flattening",
      "url": "https://en.wikipedia.org/wiki/Mode-k_flattening",
      "text": "In multi-linear algebra, mode-k flattening (also matricisation, matricizing, or unfolding) is an operation on tensor (a multi-dimensional array)   denoted by  turning it into a matrix (a two-dimensional array).\n\nMatricization may be regarded as a generalization of the mathematical concept of vectorization.\n\nMatricization may be applied in connection with determination of the factors in the PARAFAC model.\n\n Application \nThis operation is used widely in HoSVD.\n\nAlso, there is a mode-k flattening representation of SVD.\n\n References \n\nCategory:Algebra\nCategory:Multilinear algebra"
    },
    {
      "title": "Moderne Algebra",
      "url": "https://en.wikipedia.org/wiki/Moderne_Algebra",
      "text": "Moderne Algebra is a two-volume German textbook on graduate abstract algebra by , originally based on lectures given by Emil Artin in 1926 and by  from 1924 to 1928. The English translation of 1949–1950 had the title Modern algebra, though a later, extensively revised edition in 1970 had the title Algebra.\n\nThe book was one of the first textbooks to use an abstract axiomatic approach to groups, rings, and fields, and was by far the most successful, becoming the standard reference for graduate algebra for several decades. It \"had a tremendous impact, and is widely considered to be the major text on algebra in the twentieth century.\"Bernard Behan & Karl Sigmund (2008) \"A Short Tale of Two Cities: Otto Schrier and the Hamburg–Vienna Connection\", p 33, Mathematical Intelligencer 30(3):27–35\n\nIn 1975 van der Waerden described the sources he drew upon to write the book.\n\nIn 1997 Saunders Mac Lane recollected the book's influence: \n Upon its publication it was soon clear that this was the way that algebra should be presented.\n Its simple but austere style set the pattern for mathematical texts in other subjects, from Banach algebras to topological group theory.\n [Van der Waerden's] two volumes on modern algebra ... dramatically changed the way algebra is now taught by providing a decisive example of a clear and perspicacious presentation. It is, in my view, the most influential text of algebra of the twentieth century.\n\nPublication history\n\nModerne Algebra has a rather confusing publication history, because it went through many different editions, several of which were extensively rewritten with chapters and major topics added, deleted, or rearranged. In addition the new editions of first and second volumes were issued almost independently and at different times, and the numbering of the English editions does not correspond to the numbering of the German editions. In 1955 the title was changed from \"Moderne Algebra\" to \"Algebra\" following a suggestion of Brandt, with the result that the two volumes of the third German edition do not even have the same title.\n\nFor volume 1, the first German edition was published in 1930, the second in 1937 (with the axiom of choice removed), the third in 1951 (with the axiom of choice reinstated, and with more on valuations). The fourth edition appeared in 1955 (with the title changed to Algebra), the fifth in 1960, the sixth in 1964, the seventh in 1966, the eighth in 1971, the ninth in 1993. For volume 2, the first edition was published in 1931, the second in 1940, the third in 1955 (with the title changed to Algebra), the fourth in 1959 (extensively rewritten, with elimination theory replaced by algebraic functions of 1 variable), the fifth in 1967, and the sixth in 1993. The German editions were all published by Springer.\n\nThe first English edition was published in 1949–1950 and was a translation of the second German edition. There was a second edition in 1953, and a third edition under the new title Algebra in 1970 translated from the 7th German edition of volume 1 and the 5th German edition of volume 2. The three English editions were originally published by Ungar, though the 3rd English edition was later reprinted by Springer.\n\nThere were also Russian editions published in 1976 and 1979, and Japanese editions published in 1959 and 1967–1971.\n\nReferences\n\nCategory:History of mathematics\nCategory:Mathematics textbooks\nCategory:1930 books\nCategory:Algebra\nCategory:Springer Science+Business Media books"
    },
    {
      "title": "Module homomorphism",
      "url": "https://en.wikipedia.org/wiki/Module_homomorphism",
      "text": "In algebra, a module homomorphism is a function between modules that preserves the module structures. Explicitly, if M and N are left modules over a ring R, then a function  is called a module homomorphism or an R-linear map if for any x, y in M and r in R,\n\nIf M, N are right R-modules, then the second condition is replaced with\n\nThe pre-image of the zero element under f is called the kernel of f. The set of all module homomorphisms from M to N is denoted by HomR(M, N). It is an abelian group (under pointwise addition) but is not necessarily a module unless R is commutative.\n\nThe composition of module homomorphisms is again a module homomorphism. Thus, all the (say left) modules together with all the module homomorphisms between them form the category of modules.\n\n Terminology \nA module homomorphism is called an isomorphism if it admits an inverse homomorphism; in particular, it is a bijection. One can show a bijective module homomorphism is an isomorphism; i.e., the inverse is a module homomorphism. In particular, a module homomorphism is an isomorphism if and only if it is an isomorphism between the underlying abelian groups.\n\nThe isomorphism theorems hold for module homomorphisms.\n\nA module homomorphism from a module M to itself is called an endomorphism and an isomorphism from M to itself an automorphism. One writes  for the set of all endomorphisms between a module M. It is not only an abelian group but is also a ring with multiplication given by function composition, called the endomorphism ring of M. The group of units of this ring is the automorphism group of M. \n\nSchur's lemma says that a homomorphism between simple modules (a module having only two submodules) must be either zero or an isomorphism. In particular, the endomorphism ring of a simple module is a division ring.\n\nIn the language of the category theory, an injective homomorphism is also called a monomorphism and a surjective homomorphism an epimorphism.\n\n Examples \nThe zero map M → N that maps every element to zero.\nA linear transformation between vector spaces.\n.\nFor a commutative ring R and ideals I, J, there is the canonical identification\n\ngiven by . In particular,  is the annihilator of I.\nGiven a ring R and an element r, let  denote the left multiplication by r. Then for any s, t in R,\n.\nThat is,  is right R-linear.\nFor any ring R,\n as rings when R is viewed as a right module over itself. Explicitly, this isomorphism is given by the left regular representation .\n through  for any left module M. (The module structure on Hom here comes from the right R-action on R; see #Module structures on Hom below.)\n is called the dual module of M; it is a left (resp. right) module if M is a right (resp. left) module over R with the module structure coming from the R-action on R. It is denoted by .\nGiven a ring homomorphism R → S of commutative rings and an S-module M, an R-linear map θ: S → M is called a derivation if for any f, g in S, .\nIf S, T are unital associative algebras over a ring R, then an algebra homomorphism from S to T is a ring homomorphism that is also an R-module homomorphism.\n\n Module structures on Hom \nIn short, Hom inherits a ring action that was not used up to form Hom. More precise, let M, N be left R-modules. Suppose M has a right action of a ring S that commutes with the R-action; i.e., M is an (R, S)-module. Then\n\nhas the structure of a left S-module defined by: for s in S and x in M,\n\nIt is well-defined (i.e.,  is R-linear) since\n\nSimilarly,  is a ring action since\n.\n\nNote: the above verification would \"fail\" if one used the left R-action in place of the right S-action. In this sense, Hom is often said to \"use up\" the R-action.\n\nSimilarly, if M is a left R-module and N is an (R, S)-module, then  is a right S-module by .\n\n A matrix representation \nThe relationship between matrices and linear transformations in linear algebra generalizes in a natural way to module homomorphisms. Precisely, given a right R-module U, there is the canonical isomorphism of the abelian groups\n\nobtained by viewing  consisting of column vectors and then writing f as an m × n matrix. In particular, viewing R as a right R-module and using , one has\n,\nwhich turns out to be a ring isomorphism (as a composition corresponds to a matrix multiplication).\n\nNote the above isomorphism is canonical; no choice is involved. On the other hand, if one is given a module homomorphism between finite-rank free modules, then a choice of an ordered basis corresponds to a choice of an isomorphism . The above procedure then gives the matrix representation with respect to such choices of the bases. For more general modules, matrix representations may either lack uniqueness or not exist.\n\n Defining \nIn practice, one often defines a module homomorphism by specifying its values on a generating set. More precisely, let M and N be left R-modules. Suppose a subset S generates M; i.e., there is a surjection  with a free module F with a basis indexed by S and kernel K (i.e., one has a free presentation). Then to give a module homomorphism  is to give a module homomorphism  that kills K (i.e., maps K to zero).\n\n Operations \nIf  and  are module homomorphisms, then their direct sum is\n\nand their tensor product is\n\nLet  be a module homomorphism between left modules. The graph Γf of f is the submodule of M ⊕ N given by\n,\nwhich is the image of the module homomorphism \n\nThe transpose of f is\n\nIf f is an isomorphism, then the transpose of the inverse of f is called the contragredient of f.\n\n Exact sequences \n\nA short sequence of modules \n\nconsists of modules , and homomorphisms . It is exact if the image of any arrow is the kernel of the next one; that is,  is injective, the kernel of  is the image of  and  is surjective. A longer exact sequence is defined in a similar way. A sequence of modules is exact if and only if it is exact as a sequence of abelian groups. \n\nAny module homomorphism  defines an exact sequence\n\nwhere  is the kernel of , and  is the cokernel, that is the quotient of  by the image of .\n\nIn the case of modules over a commutative ring, a sequence is exact if and only if it is exact at all the maximal ideals; that is all sequences \n\nare exact, where the subscript  means the localization at a maximal ideal .\n\nIf  are module homomorphisms, then they are said to form a fiber square (or pullback square), denoted by M ×B N, if it fits into\n\nwhere .\n\nExample: Let  be commutative rings, and let I be the annihilator of the quotient B-module A/B (which is an ideal of A). Then canonical maps  form a fiber square with \n\n Endomorphisms of finitely generated modules \nLet  be an endomorphism between finitely generated R-modules for a commutative ring R. Then\n is killed by its characteristic polynomial relative to the generators of M; see Nakayama's lemma#Proof.\nIf  is surjective, then it is injective.\n\nSee also: Herbrand quotient (which can be defined for any endomorphism with some finiteness conditions.)\n\n Variants \n Additive relations \n\nAn additive relation  from a module M to a module N is a submodule of  In other words, it is a \"many-valued\" homomorphism defined on some submodule of M. The inverse  of f is the submodule . Any additive relation f determines a homomorphism from a submodule of M to a quotient of N\n\nwhere  consists of all elements x in M such that (x, y) belongs to f for some y in N.\n\nA transgression that arises from a spectral sequence is an example of an additive relation.\n\n See also \nMapping cone (homological algebra)\nSmith normal form\nChain complex\nPairing\n\n Notes \n\n References \nBourbaki, Algebra\nS. MacLane, Homology\nH. Matsumura, Commutative ring theory. Translated from the Japanese by M. Reid. Second edition. Cambridge Studies in Advanced Mathematics, 8.\n\nCategory:Algebra"
    },
    {
      "title": "Monomial",
      "url": "https://en.wikipedia.org/wiki/Monomial",
      "text": "In mathematics, a monomial is, roughly speaking, a polynomial which has only one term. Two definitions of a monomial may be encountered:\n(1): A monomial, also called power product, is a product of powers of variables with nonnegative integer exponents, or, in other words, a product of variables, possibly with repetitions. The constant 1 is a monomial, being equal to the empty product and 0 for any variable . If only a single variable  is considered, this means that a monomial is either 1 or a power  of , with  a positive integer. If several variables are considered, say,  then each can be given an exponent, so that any monomial is of the form  with  non-negative integers (taking note that any exponent 0 makes the corresponding factor equal to 1).\n(2): A monomial is a monomial in the first sense multiplied by a nonzero constant, called the coefficient of the monomial. A monomial in the first sense is a special case of a monomial in the second sense, where the coefficient is 1. For example, in this interpretation  and  are monomials (in the second example, the variables are  and the coefficient is a complex number).\n\nIn the context of Laurent polynomials and Laurent series, the exponents of a monomial may be negative, and in the context of Puiseux series, the exponents may be rational numbers.\n\nSince the word \"monomial\", as well as the word \"polynomial\", comes from the late Latin word \"binomium\" (binomial), by changing the prefix \"bi\" (two in Latin), a monomial should theoretically be called a \"mononomial\". \"Monomial\" is a syncope by haplology of \"mononomial\".American Heritage Dictionary of the English Language, 1969.\n\n Comparison of the two definitions \nWith either definition, the set of monomials is a subset of all polynomials that is closed under multiplication.\n\nBoth uses of this notion can be found, and in many cases the distinction is simply ignored, see for instance examples for the first and second meaning. In informal discussions the distinction is seldom important, and tendency is towards the broader second meaning. When studying the structure of polynomials however, one often definitely needs a notion with the first meaning. This is for instance the case when considering a monomial basis of a polynomial ring, or a monomial ordering of that basis. An argument in favor of the first meaning is also that no obvious other notion is available to designate these values (the term power product is in use, in particular when monomial is used with the first meaning, but it does not make the absence of constants clear either), while the notion term of a polynomial unambiguously coincides with the second meaning of monomial. \n\nThe remainder of this article assumes the first meaning of \"monomial\".\n\nMonomial basis\n\nThe most obvious fact about monomials (first meaning) is that any polynomial is a linear combination of them, so they form a basis of the vector space of all polynomials, called the monomial basis - a fact of constant implicit use in mathematics.\n\nNumber\nThe number of monomials of degree d in n variables is the number of multicombinations of d elements chosen among the n variables (a variable can be chosen more than once, but order does not matter), which is given by the multiset coefficient . This expression can also be given in the form of a binomial coefficient, as a polynomial expression in d, or using a rising factorial power of :\n\nThe latter forms are particularly useful when one fixes the number of variables and lets the degree vary. From these expressions one sees that for fixed n, the number of monomials of degree d is a polynomial expression in d of degree  with leading coefficient .\n\nFor example, the number of monomials in three variables () of degree d is ; these numbers form the sequence 1, 3, 6, 10, 15, ... of triangular numbers.\n\nThe Hilbert series is a compact way to express the number of monomials of a given degree: the number of monomials of degree  in  variables is the coefficient of degree  of the formal power series expansion of\n\nThe number of monomials of degree at most  in  variables is  This follows from the one-to-one correspondence between the monomials of degree  in  variables and the monomials of degree at most  in  variables, which consists in substituting by 1 the extra variable.\n\nNotation\n\nNotation for monomials is constantly required in fields like partial differential equations. If the variables being used form an indexed family like , , , ..., then multi-index notation is helpful: if we write \n\nwe can define \n\nand save a great deal of space.\n\nDegree\n\nThe degree of a monomial is defined as the sum of all the exponents of the variables, including the implicit exponents of 1 for the variables which appear without exponent; e.g., in the example of the previous section, the degree is . The degree of  is 1+1+2=4. The degree of a nonzero constant is 0. For example, the degree of -7 is 0. \n\nThe degree of a monomial is sometimes called order, mainly in the context of series. It is also called total degree when it is needed to distinguish it from the degree in one of the variables.\n\nMonomial degree is fundamental to the theory of univariate and multivariate polynomials. Explicitly, it is used to define the degree of a polynomial and the notion of homogeneous polynomial, as well as for graded monomial orderings used in formulating and computing Gröbner bases. Implicitly, it is used in grouping the terms of a Taylor series in several variables.\n\nGeometry\n\nIn algebraic geometry the varieties defined by monomial equations  for some set of α have special properties of homogeneity. This can be phrased in the language of algebraic groups, in terms of the existence of a group action of an algebraic torus (equivalently by a multiplicative group of diagonal matrices). This area is studied under the name of torus embeddings.\n\nSee also\n Monomial representation\n Monomial matrix\n Homogeneous polynomial\n Homogeneous function\n Multilinear form\n Log-log plot\n Power law\n\n Notes \n\nCategory:Homogeneous polynomials\nCategory:Algebra"
    },
    {
      "title": "Monomial basis",
      "url": "https://en.wikipedia.org/wiki/Monomial_basis",
      "text": "In mathematics the monomial basis of a polynomial ring is its basis (as vector space or free module over the field or ring of coefficients) that consists in the set of all monomials. The monomials form a basis because every polynomial may be uniquely written as a finite linear combination of monomials (this is an immediate consequence of the definition of a polynomial).\n\nOne indeterminate\n\nThe polynomial ring  of the univariate polynomial over a field  is a -vector space, which has \n\nas an (infinite) basis. More generally, if  is a ring,  is a free module, which has the same basis.\n\nThe polynomials of degree at most  form also a vector space (or a free module in the case of a ring of coefficients), which has \n\nas a basis\n\nThe canonical form of a polynomial is its expression on this basis:\n\nor, using the shorter sigma notation:\n\nThe monomial basis is naturally totally ordered, either by increasing degrees\n\nor by decreasing degrees\n\nSeveral indeterminates\n\nIn the case of several indeterminates  a monomial is a product\n\nwhere the  are non-negative integers. Note that, as  an exponent equal to zero means that the corresponding indeterminate does not appear in the monomial; in particular\n\nis a monomial.\n\nSimilar to the case of univariate polynomials, the polynomials in  form a vector space (if the coefficients belong to a field) or a free module (if the coefficients belong to a ring), which has the set of all monomials as a basis, called the monomial basis\n\nThe homogeneous polynomials of degree  form a subspace which has the monomials of degree  as a basis. The dimension of this subspace is the number of monomials of degree , which is \n\nwhere  denotes a binomial coefficient.\n\nThe polynomials of degree at most  form also a subspace, which has the monomials of degree at most  as a basis. The number of these monomials is the dimension of this subspace, equal to\n\nDespite the univariate case, there is no natural total order of the monomial basis. For problem which require to choose a total order, such Gröbner basis computation, one generally chooses an admissible monomial order that is a total order on the set of monomials such that\n\nand\n\nfor every monomials \n\nA polynomial can always be converted into monomial form by calculating its Taylor expansion around 0. For example, a polynomial in :\n\nSee also\nHorner's method\nPolynomial sequence\nNewton polynomial\nLagrange polynomial\nLegendre polynomial\nBernstein form\nChebyshev form\n\nCategory:Algebra\nCategory:Polynomials"
    },
    {
      "title": "Monomial ideal",
      "url": "https://en.wikipedia.org/wiki/Monomial_ideal",
      "text": "In abstract algebra, a monomial ideal is an ideal generated by monomials in a multivariate polynomial ring over a field.\n\nA toric ideal is an ideal generated by differences of monomials (provided the ideal is a prime ideal). An affine or projective algebraic variety defined by a toric ideal or a homogeneous toric ideal is an affine or projective toric variety, possibly non-normal.\n\nDefinitions and Properties\n\nLet  be a field and  be the polynomial ring over  with n variables . \n\nA monomial in  is a product  for an n-tuple  of nonnegative integers.\n\nThe following three conditions are equivalent for an ideal :\n  is generated by monomials,\n If , then , provided that  is nonzero.\n  is torus fixed, i.e, given , then  is fixed under the action  for all .\n\nWe say that  is a monomial ideal if it satisfies any of these equivalent conditions.\n\nGiven a monomial ideal ,  is in  if and only if every monomial ideal term  of  is a multiple of one the .\n\nProof:\nSuppose  and that  is in . Then , for some . \n\nFor all , we can express each  as the sum of monomials, so that  can be written as a sum of multiples of the . Hence,  will be a sum of multiples of monomial terms for at least one of the .\n\nConversely, let  and let each monomial term in  be a multiple of one of the  in . Then each monomial term in  can be factored from each monomial in . Hence  is of the form  for some , as a result . \n\nThe following illustrates an example of monomial and polynomial ideals.\n\nLet  then the polynomial  is in  since each term is a multiple of an element in  i.e., they can be rewritten as  and  both in  However, if , then this polynomial  is not in  since its terms are not multiples of elements in \n\n Monomial Ideals and Young Diagrams \n\nA monomial ideal can be interpreted as a Young diagram. Suppose , then  can be interpreted in terms of the minimal monomials generators as , where  and . The minimal monomial generators of  can be seen as the inner corners of the Young diagram. The minimal generators would determine where we would draw the staircase diagram.\nThe monomials not in  lie inside the staircase, and these monomials form a vector space basis for the quotient ring .\n\nConsider the following example. \nLet  be a monomial ideal. Then the set of grid points  corresponds to the minimal monomial generators  in . Then as the figure shows, the pink Young diagram consists of the monomials that are not in . The points in the inner corners of the Young diagram, allow us to identify the minimal monomials  in  as seen in the green boxes. Hence, .\n\n thumb|A Young diagram and its connection with its monomial ideal.\nIn general, to any set of grid points, we can associate a Young diagram, so that the monomial ideal is constructed by determining the inner corners that make up the staircase diagram; likewise, given a monomial ideal, we can make up the Young diagram by looking at the  and representing them as the inner corners of the Young diagram. The coordinates of the inner corners would represent the powers of the minimal monomials in . Thus, monomial ideals can be described by Young diagrams of partitions.\n\nMoreover, the -action on the set of   such that  as a vector space over  has fixed points corresponding to monomial ideals only, which correspond to partitions of size n, which are identified by Young diagrams with n boxes.\n\nMonomial Ordering and Gröbner Basis\nA monomial ordering is a well ordering  on the set of monomials such that if  are monomials, then .\n\nBy the monomial order, we can state the following definitions for a polynomial in . \n\nDefinition\n\n Consider an ideal , and a fixed monomial ordering. The leading term of a nonzero polynomial , denoted by  is the monomial term of maximal order in  and the leading term of  is .\n The ideal of leading terms, denoted by , is the ideal generated by the leading terms of every element in the ideal, that is, .\n A Gröbner basis for an ideal  is a finite set of generators  for  whose leading terms generate the ideal of all the leading terms in , i.e.,  and .\n\nNote that  in general depends of the ordering used; for example, if we choose the lexicographical order on  subject to x > y, then , but if we take y > x then .\n\nIn addition, monomials are present on Gröbner basis and to define the division algorithm for polynomials with several variables. \n\nNotice that for a monomial ideal , the finite set of generators  is a Gröbner basis for . To see this, note that any polynomial  can be expressed as  for . Then the leading term of  is a multiple for some . As a result,  is generated by the  likewise.\n\n See also \n\nStanley–Reisner ring\nHodge algebra\n\n Footnotes \n\nReferences\n \n \n\nFurther reading\n\nSturmfels, B. (1996) Gröbner Bases and Convex Polytopes. American Mathematical Society, Providence\nD. Taylor, Ideals generated by monomials in an R-sequence, Ph. D. thesis, University of Chicago, 1966.\nBernard Teissier, Monomial Ideals, Binomial Ideals, Polynomial Ideals, 2004\n\nCategory:Homogeneous polynomials\nCategory:Algebra"
    },
    {
      "title": "Multiplicative digital root",
      "url": "https://en.wikipedia.org/wiki/Multiplicative_digital_root",
      "text": "The multiplicative digital root of a positive integer n is found by multiplying the digits of n together, then repeating this operation until only a single digit remains. This single-digit number is called the multiplicative digital root of n.\n\nMultiplicative digital roots depend upon the base in which n is written. If the term is used without qualification, it is assumed that n is written in base 10.\n\nMultiplicative digital roots are the multiplicative equivalent of digital roots.\n\n Example \n9876 would be reduced as 9876 -> 9×8×7×6 = 3024 -> 3×0×2×4 = 0. So the multiplicative digital root of 9876 is 0 and its multiplicative persistence (the number of steps required to reach a single digit) is 2.\n\n References \n\nCategory:Algebra\nCategory:Number theory"
    },
    {
      "title": "Nested radical",
      "url": "https://en.wikipedia.org/wiki/Nested_radical",
      "text": "In algebra, a nested radical is a radical expression (one containing a square root sign, cube root sign, etc.) that contains (nests) another radical expression. Examples include\n\nwhich arises in discussing the regular pentagon, and more complicated ones such as\n\n Denesting\n\nSome nested radicals can be rewritten in a form that is not nested. For example,\n\nRewriting a nested radical in this way is called denesting. This is not always possible, and, even when possible, it is often difficult.\n\nTwo nested square roots\nIn the case of two nested square roots, the following theorem completely solves the problem of denesting.\n\nIf , , and  are rational numbers and  is not a square, there are two rational numbers  \nand  such that\n\nif and only if  is the square of a rational number . If the nested radical is real,  and  are the two numbers   In particular, if , , and  are integers, then  and  are integers.\n\nThis result includes denestings of the form\n\nas  may always be written  and at least one of the terms must be positive (because the left-hand side of the equation is positive).\n\nA more general denesting formula could have the form\n\nHowever, Galois theory implies that either the left-and side belongs to   or it must be obtained by changing the sign of either   or both. In the first case, this means that one can take  and  In the second case,  and another coefficient must be zero. If , one may rename  as  for getting . Proceeding similarly if  it results that one can suppose  This shows that the apparently more general denesting can always be reduced to the above one.\n\nProof: By squaring, the equation \n\nis equivalent with \n\nand, in the case of a minus in the right-hand side, \n,\n(square roots are nonnegative by definition of the notation). As the inequality may always be satisfied by possibly exchanging  and , solving the first equation in  and  is equivalent with solving  \n\nThis equality implies that  belongs to the quadratic field  In this field every element may be uniquely written  with  and  being rational numbers. This implies that  is not rational (otherwise, the left-hand side of the equation would be irrational and the right-hand side would be rational). As  and  must be rational, the square of  must be rational. This implies that  in the expression of  as  Thus\n\nfor some rational number \nThe uniqueness of the decomposition over  and  implies thus that the equation that is considered is equivalent with\n\nIt follows by Vieta's formulas that  and  must be the roots of the quadratic equation\n\nthat is \n\nThus  and  are rational if and only  is a rational number.\n\nFor explicitly choosing the various signs, one must consider only positive real square roots, and thus assuming . The equation  shows that . Thus, if the nested radical is real, and denesting is possible, then . Then, the two possible cases for the sign of  may be considered simultaneously, by assuming  and writing the solution\n\n Some identities of Ramanujan \n\nSrinivasa Ramanujan demonstrated a number of curious identities involving nested radicals.  Among them are the following:\n\n \n\n \n\n \n\n  \n\nOther odd-looking radicals inspired by Ramanujan include:\n\n \n\n \n\n Landau's algorithm \n\nIn 1989 Susan Landau introduced the first algorithm for deciding which nested radicals can be denested.  Earlier algorithms worked in some cases but not others.\n\nIn trigonometry\n\nIn trigonometry, the sines and cosines of many angles can be expressed in terms of nested radicals. For example,\n\n \n\nand\n\n \nThe last equality results directly from of the results of .\n\nIn the solution of the cubic equation\n\nNested radicals appear in the algebraic solution of the cubic equation. Any cubic equation can be written in simplified form without a quadratic term, as\n\nwhose general solution for one of the roots is\n\nIn the case in which the cubic has only one real root, the real root is given by this expression with the radicands of the cube roots being real and with the cube roots being the real cube roots. In the case of three real roots, the square root expression is an imaginary number; here any real root is expressed by defining the first cube root to be any specific complex cube root of the complex radicand, and by defining the second cube root to be the complex conjugate of the first one. The nested radicals in this solution cannot in general be simplified unless the cubic equation has at least one rational solution. Indeed, if the cubic has three irrational but real solutions, we have the casus irreducibilis, in which all three real solutions are written in terms of cube roots of complex numbers. On the other hand, consider the equation\n\nwhich has the rational solutions 1, 2, and −3. The general solution formula given above gives the solutions\n\nFor any given choice of cube root and its conjugate, this contains nested radicals involving complex numbers, yet it is reducible (even though not obviously so) to one of the solutions 1, 2, or –3.\n\n Infinitely nested radicals \n\n Square roots \n\nUnder certain conditions infinitely nested square roots such as\n\nrepresent rational numbers. This rational number can be found by realizing that x also appears under the radical sign, which gives the equation\n\nIf we solve this equation, we find that x = 2 (the second solution x = −1 doesn't apply, under the convention that the positive square root is meant). This approach can also be used to show that generally, if n > 0, then\n\nand is the positive root of the equation x2 − x − n = 0. For n = 1, this root is the golden ratio φ, approximately equal to 1.618. The same procedure also works to obtain, if n > 1,\n\nwhich is the positive root of the equation x2 + x − n = 0.\n\nRamanujan's infinite radicals\n\nRamanujan posed the following problem to the Journal of Indian Mathematical Society:\n\n \n\nThis can be solved by noting a more general formulation:\n\n \n\nSetting this to F(x) and squaring both sides gives us\n\n \n\nwhich can be simplified to\n\n \n\nIt can then be shown that\n\n \n\nSo, setting a = 0, n = 1, and x = 2, we have\n\n \nRamanujan stated the following infinite radical denesting in his lost notebook:\n\nThe repeating pattern of the signs is \n\nViète's expression for \n\nViète's formula for , the ratio of a circle's circumference to its diameter, is\n\n Cube roots \n\nIn certain cases, infinitely nested cube roots such as\n\ncan represent rational numbers as well.  Again, by realizing that the whole expression appears inside itself, we are left with the equation\n\nIf we solve this equation, we find that x = 2. More generally, we find that\n\nis the positive real root of the equation x3 − x − n = 0 for all n > 0. For n = 1, this root is the plastic number ρ, approximately equal to 1.3247.\n\nThe same procedure also works to get\n\nas the real root of the equation x3 + x − n = 0 for all n > 1.\n\nSee also\nSum of radicals\nSpiral of Theodorus\n\n References \n\nFurther reading \n \n\n Decreasing the Nesting Depth of Expressions Involving Square Roots\n Simplifying Square Roots of Square Roots\n \n \n\nCategory:Algebra"
    },
    {
      "title": "Operand",
      "url": "https://en.wikipedia.org/wiki/Operand",
      "text": "In mathematics an operand is the object of a mathematical operation, i.e., it is the object or quantity that is operated on.American Heritage Dictionary\n\nExample \nThe following arithmetic expression shows an example of operators and operands:\n\nIn the above example, '+' is the symbol for the operation called addition. \n\nThe operand '3' is one of the inputs (quantities) followed by the addition operator, and the operand '6' is the other input necessary for the operation.\n\nThe result of the operation is 9. (The number '9' is also called the sum of the augend 3 and the addend 6.)\n\nAn operand, then, is also referred to as \"one of the inputs (quantities) for an operation\".\n\nNotation\n\nExpressions as operands\nOperands may be complex, and may consist of expressions also made up of operators with operands.\n\nIn the above expression '(3 + 5)' is the first operand for the multiplication operator and '2' the second. The operand '(3 + 5)' is an expression in itself, which contains an addition operator, with the operands '3' and '5'.\n\nOrder of operations\n\nRules of precedence affect which values form operands for which operators:\n\nIn the above expression, the multiplication operator has the higher precedence than the addition operator, so the multiplication operator has operands of '5' and '2'. The addition operator has operands of '3' and '5 × 2'.\n\nPositioning of operands\nDepending on the mathematical notation being used the position of an operator in relation to its operand(s) may vary. In everyday usage infix notation is the most common, however other notations also exist, such as the prefix and postfix notations. These alternate notations are most common within computer science.\n\nBelow is a comparison of three different notations — all represent an addition of the numbers '1' and '2'\n\n (infix notation)\n\n (prefix notation)\n\n (postfix notation)\n\nInfix notation and the order of operation\n\nIn a mathematical expression, the order of operation is carried out from left to right. Start with the leftmost value and seek the first operation to be carried out in accordance with the order specified above (i.e., start with parentheses and end with the addition/subtraction group). For example, in the expression\n\n,\n\nthe first operation to be acted upon is any and all expressions found inside a parenthesis. So beginning at the left and moving to the right, find the first (and in this case, the only) parenthesis, that is, (2 + 22). Within the parenthesis itself is found the expression 22. The reader is required to find the value of 22 before going any further. The value of 22 is 4. Having found this value, the remaining expression looks like this:\n\nThe next step is to calculate the value of expression inside the parenthesis itself, that is, (2 + 4) = 6. Our expression now looks like this:\n\nHaving calculated the parenthetical part of the expression, we start over again beginning with the left most value and move right. The next order of operation (according to the rules) is exponents. Start at the left most value, that is, 4, and scan your eyes to the right and search for the first exponent you come across. The first (and only) expression we come across that is expressed with an exponent is 22.  We find the value of 22, which is 4. What we have left is the expression\n\n.\n\nThe next order of operation is multiplication. 4 × 4 is 16. Now our expression looks like this:\n\nThe next order of operation according to the rules is division. However, there is no division operator sign (÷) in the expression, 16 − 6. So we move on to the next order of operation, i.e., addition and subtraction, which have the same precedence and are done left to right.\n\n. \n\nSo the correct value for our original expression, 4 × 22 − (2 + 22), is 10. \n\nIt is important to carry out the order of operation in accordance with rules set by convention. If the reader evaluates an expression but does not follow the correct order of operation, the reader will come forth with a different value. The different value will be the incorrect value because the order of operation was not followed. The reader will arrive at the correct value for the expression if and only if each operation is carried out in the proper order.\n\nArity\nThe number of operands of an operator is called its arity.: \"Each connective has associated with it a natural number, called its rank, or arity.\" Based on arity, operators are classified as nullary (no operands), unary (1 operand), binary (2 operands), ternary (3 operands), etc.\n\nComputer science\nIn computer programming languages, the definitions of operator and operand are almost the same as in mathematics.\n\nIn computing, an operand is the part of a computer instruction which specifies what data is to be manipulated or operated on, while at the same time representing the data itself.\nA computer instruction describes an operation such as add or multiply X, while the operand (or operands, as there can be more than one) specify on which X to operate as well as the value of X.\n\nAdditionally, in assembly language, an operand is a value (an argument) on which the instruction, named by mnemonic, operates.  The operand may be a processor register, a memory address, a literal constant, or a label.  A simple example (in the x86 architecture) is\nMOV DS, AX\nwhere the value in register operand AX is to be moved (MOV) into register DS. Depending on the instruction, there may be zero, one, two, or more operands.\n\nSee also\n\nInstruction set\nOpcode\n\nReferences\n\nCategory:Algebra\nCategory:Mathematical notation\nCategory:Operators (programming)\nCategory:Machine code"
    },
    {
      "title": "Operation (mathematics)",
      "url": "https://en.wikipedia.org/wiki/Operation_%28mathematics%29",
      "text": "thumb|right|Elementary arithmetic operations:\nIn mathematics, an operation is a calculation from zero or more input values (called operands) to an output value. The number of operands is the arity of the operation. The most commonly studied operations are binary operations, (that is, operations of arity 2) such as addition and multiplication, and unary operations (operations of arity 1), such as additive inverse and multiplicative inverse. An operation of arity zero, or nullary operation, is a constant. The mixed product is an example of an operation of arity 3, also called ternary operation. Generally, the arity is supposed to be finite. However, infinitary operations are sometimes considered, in which context the \"usual\" operations of finite arity are called finitary operations.\n\nTypes of operation\nthumb|A binary operation takes two arguments  and  and returns the result \nThere are two common types of operations: unary and binary. Unary operations involve only one value, such as negation and trigonometric functions. Binary operations, on the other hand, take two values, and include addition, subtraction, multiplication, division, and exponentiation.\n\nOperations can involve mathematical objects other than numbers. The logical values true and false can be combined using logic operations, such as and, or, and not. Vectors can be added and subtracted. Rotations can be combined using the function composition operation, performing the first rotation and then the second. Operations on sets include the binary operations union and intersection and the unary operation of complementation. Operations on functions include composition and convolution.\n\nOperations may not be defined for every possible value. For example, in the real numbers one cannot divide by zero or take square roots of negative numbers. The values for which an operation is defined form a set called its domain. The set which contains the values produced is called the codomain, but the set of actual values attained by the operation is its range. For example, in the real numbers, the squaring operation only produces non-negative numbers; the codomain is the set of real numbers, but the range is the non-negative numbers.\n\nOperations can involve dissimilar objects. A vector can be multiplied by a scalar to form another vector. And the inner product operation on two vectors produces a scalar. An operation may or may not have certain properties, for example it may be associative, commutative, anticommutative, idempotent, and so on.\n\nThe values combined are called operands, arguments, or inputs, and the value produced is called the value, result, or output. Operations can have fewer or more than two inputs.\n\nAn operation is like an operator, but the point of view is different. For instance, one often speaks of \"the operation of addition\" or \"addition operation\" when focusing on the operands and result, but one says \"addition operator\" (rarely \"operator of addition\") when focusing on the process, or from the more abstract viewpoint, the function .\n\nDefinition\nAn operation ω is a function of the form , where . The sets Xk are called the domains of the operation, the set Y is called the codomain of the operation, and the fixed non-negative integer k (the number of arguments) is called the type or arity of the operation. Thus a unary operation has arity one, and a binary operation has arity two. An operation of arity zero, called a nullary operation, is simply an element of the codomain Y. An operation of arity k is called a k-ary operation. Thus a k-ary operation is a (k+1)-ary relation that is functional on its first k domains.\n\nThe above describes what is usually called a finitary operation, referring to the finite number of arguments (the value k). There are obvious extensions where the arity is taken to be an infinite ordinal or cardinal, or even an arbitrary set indexing the arguments.\n\nOften, use of the term operation implies that the domain of the function is a power of the codomain (i.e. the Cartesian product of one or more copies of the codomain), although this is by no means universal, as in the example of multiplying a vector by a scalar.\n\nSee also\nHyperoperation\nOrder of operations\n\nNotes\n\nCategory:Elementary mathematics\nCategory:Algebra\nCategory:Arithmetic"
    },
    {
      "title": "Operator (mathematics)",
      "url": "https://en.wikipedia.org/wiki/Operator_%28mathematics%29",
      "text": "In mathematics, an operator is generally a mapping that acts on elements of a space to produce elements of another space (possibly the same space, sometimes required to be the same space). The most common operators are linear maps, which act on vector spaces. However, when using \"linear operator\" instead of \"linear map\", mathematicians often mean actions on vector spaces of functions, which also preserve other properties, such as continuity. For example, differentiation and indefinite integration are linear operators; operators that are built from them are called differential operators, integral operators or integro-differential operators.\n\nOperator is also used for denoting the symbol of a mathematical operation. This is related with the meaning of \"operator\" in computer programming, see operator (computer programming).\n\n Linear operators \n\nThe most common kind of operator encountered are linear operators. Let U and V be vector spaces over a field K. A mapping A: U → V is linear if\n\nfor all x, y in U and for all α, β in K.  \nThis means that a linear operator preserves vector space operations, in the sense that it does not matter whether you apply the linear operator before or after the operations of addition and scalar multiplication. In more technical words, linear operators are morphisms between vector spaces.\n\nIn finite-dimensional case linear operators can be represented by matrices in the following way. Let  be a field, and  and  be finite-dimensional vector spaces over . Let us select a basis  in  and  in . Then let  be an arbitrary vector in  (assuming Einstein convention), and  be a linear operator. Then\n .\nThen  is the matrix of the operator  in fixed bases.  does not depend on the choice of , and  if . Thus in fixed bases n-by-m matrices are in bijective correspondence to linear operators from  to .\n\nThe important concepts directly related to operators between finite-dimensional vector spaces are the ones of rank, determinant, inverse operator, and eigenspace.\n\nLinear operators also play a great role in the infinite-dimensional case. The concepts of rank and determinant cannot be extended to infinite-dimensional matrices. This is why very different techniques are employed when studying linear operators (and operators in general) in the infinite-dimensional case. The study of linear operators in the infinite-dimensional case is known as functional analysis (so called because various classes of functions form interesting examples of infinite-dimensional vector spaces).\n\nThe space of sequences of real numbers, or more generally sequences of vectors in any vector space, themselves form an infinite-dimensional vector space. The most important cases are sequences of real or complex numbers, and these spaces, together with linear subspaces, are known as sequence spaces. Operators on these spaces are known as sequence transformations.\n\nBounded linear operators over Banach space form a Banach algebra in respect to the standard operator norm. The theory of Banach algebras develops a very general concept of spectra that elegantly generalizes the theory of eigenspaces.\n\n Bounded operators\n\nLet U and V be two vector spaces over the same ordered field (for example, ), and they are equipped with norms. Then a linear operator from U to V is called bounded if there exists C > 0 such that\n\nfor all x in U.\n\nBounded operators form a vector space. On this vector space we can introduce a norm that is compatible with the norms of U and V:\n.\n\nIn case of operators from U to itself it can be shown that\n.\n\nAny unital normed algebra with this property is called a Banach algebra. It is possible to generalize spectral theory to such algebras. C*-algebras, which are Banach algebras with some additional structure, play an important role in quantum mechanics.\n\n Examples \n Geometry \n\nIn geometry, additional structures on vector spaces are sometimes studied. Operators that map such vector spaces to themselves bijectively are very useful in these studies, they naturally form groups by composition.\n\nFor example, bijective operators preserving the structure of a vector space are precisely the invertible linear operators. They form the general linear group under composition. They do not form a vector space under the addition of operators, e.g. both id and -id are invertible (bijective), but their sum, 0, is not.\n\nOperators preserving the Euclidean metric on such a space form the isometry group, and those that fix the origin form a subgroup known as the orthogonal group. Operators in the orthogonal group that also preserve the orientation of vector tuples form the special orthogonal group, or the group of rotations.\n\n Probability theory \n\nOperators are also involved in probability theory, such as expectation, variance,  and covariance. Indeed, every covariance is basically a dot product; every variance is a dot product of a vector with itself, and thus is a quadratic norm; every standard deviation is a norm (square root of the quadratic norm); the corresponding cosine to this dot product is the Pearson correlation coefficient; expected value is basically an integral operator (used to measure weighted shapes in the space).\n\n Calculus \n\nFrom the point of view of functional analysis, calculus is the study of two linear operators: the differential operator , and the Volterra operator .\n\n Fourier series and Fourier transform \n\nThe Fourier transform is useful in applied mathematics, particularly physics and signal processing. It is another integral operator; it is useful mainly because it converts a function on one (temporal) domain to a function on another (frequency) domain, in a way effectively invertible. No information is lost, as there is an inverse transform operator. In the simple case of periodic functions, this result is based on the theorem that any continuous periodic function can be represented as the sum of a series of sine waves and cosine waves:\n\nThe tuple (a0, a1, b1, a2, b2, ...) is in fact an element of an infinite-dimensional vector space ℓ2, and thus Fourier series is a linear operator.\n\nWhen dealing with general function R → C, the transform takes on an integral form:\n\n Laplace transform \n\nThe Laplace transform is another integral operator and is involved in simplifying the process of solving differential equations.\n\nGiven f = f(s), it is defined by:\n\n Fundamental operators on scalar and vector fields \n\nThree operators are key to vector calculus:\n Grad (gradient), (with operator symbol ) assigns a vector at every point in a scalar field that points in the direction of greatest rate of change of that field and whose norm measures the absolute value of that greatest rate of change.\n Div (divergence), (with operator symbol ) is a vector operator that measures a vector field's divergence from or convergence towards a given point.\n Curl, (with operator symbol ) is a vector operator that measures a vector field's curling (winding around, rotating around) trend about a given point.\n\nAs an extension of vector calculus operators to physics, engineering and tensor spaces, Grad, Div and Curl operators also are often associated with Tensor calculus as well as vector calculus.\n\n See also \n Function\n Operator algebra\n List of operators\n Operator (physics)\n\n References \n\nCategory:Algebra\nCategory:Functional analysis\nCategory:Mathematical notation"
    },
    {
      "title": "Order of operations",
      "url": "https://en.wikipedia.org/wiki/Order_of_operations",
      "text": "In mathematics and computer programming, the order of operations (or operator precedence) is a collection of rules that reflect conventions about which procedures to perform first in order to evaluate a given mathematical expression.\n\nFor example, in mathematics and most computer languages, multiplication is granted a higher precedence than addition, and it has been this way since the introduction of modern algebraic notation. Thus, the expression  is interpreted to have the value , not . With the introduction of exponents in the 16th and 17th centuries, they were given precedence over both addition and multiplication and could be placed only as a superscript to the right of their base. Thus  and .\n\nThese conventions exist to eliminate ambiguity while allowing notation to be as brief as possible. Where it is desired to override the precedence conventions, or even simply to emphasize them, parentheses ( ) (sometimes replaced by brackets [ ] or braces { } for readability) can indicate an alternate order or reinforce the default order to avoid confusion. For example,  forces addition to precede multiplication, and  forces addition to precede exponentiation.\n\nDefinition\nThe order of operations used throughout mathematics, science, technology and many computer programming languages is expressed here:\n exponentiation and root extraction\n multiplication and division\n addition and subtraction\n\nThis means that if, in a mathematical expression, a subexpression appears between two operators, the operator that is higher in the above list should be applied first.\n\nThe commutative and associative laws of addition and multiplication allow adding terms in any order, and multiplying factors in any order—but mixed operations must obey the standard order of operations.\n\nIn some contexts, it is helpful to replace a division by multiplication by the reciprocal (multiplicative inverse) and a subtraction by addition of the opposite (additive inverse). For example, in computer algebra, this allows manipulating fewer binary operations and makes it easier to use commutativity and associativity when simplifying large  expressions – for more details, see .  Thus ; in other words, the quotient of 3 and 4 equals the product of 3 and . Also ; in other words the difference of 3 and 4 equals the sum of 3 and −4. Thus,  can be thought of as the sum of , and the three summands may be added in any order, in all cases giving 5 as the result. \n\nThe root symbol √ is traditionally prolongated by a bar (called vinculum) over the radicand (this allows avoiding parentheses for eliminating the radicand). Other functions use parentheses around the input to avoid ambiguity. The parentheses are sometimes omitted if the input is a monomial. Thus, , but , because  is not a monomial. Some calculators and programming languages require parentheses around function inputs, some do not.\n\nSymbols of grouping can be used to override the usual order of operations. Grouped symbols can be treated as a single expression. Symbols of grouping can be removed using the associative and distributive laws, also they can be removed if the expression inside the symbol of grouping is sufficiently simplified so no ambiguity results from their removal.\n\nExamples\n \n\nA horizontal fractional line also acts as a symbol of grouping:\n \n\nFor ease in reading, other grouping symbols, such as curly braces { } or square brackets [ ], are often used along with parentheses ( ). For example:\n \n\nUnary minus sign\nThere are differing conventions concerning the unary operator − (usually read \"minus\"). In written or printed mathematics, the expression −32 is interpreted to mean ,\n\nSome applications and programming languages, notably Microsoft Excel (and other spreadsheet applications) and the programming language bc, unary operators have a higher priority than binary operators, that is, the unary minus has higher precedence than exponentiation, so in those languages −32 will be interpreted as . This does not apply to the binary minus operator −; for example while the formulas =-2^2 and =0+-2^2 return 4 in Microsoft Excel, the formula =0-2^2 returns −4. In cases where there is the possibility that the notation might be misinterpreted, a binary minus operation can be enforced by explicitly specifying a leading 0 (as in 0-2^2 instead of just -2^2), or parentheses can be used to clarify the intended meaning.\n\nMixed division and multiplication\nSimilarly, there can be ambiguity in the use of the slash symbol / in expressions such as 1/2x. If one rewrites this expression as  and then interprets the division symbol as indicating multiplication by the reciprocal, this becomes:\n .\n\nWith this interpretation  is equal to .\"division and multiplication have the same priority\", http://www.mathcentre.ac.uk/resources/uploaded/mc-ty-rules-2009-1.pdf However, in some of the academic literature, multiplication denoted by juxtaposition (also known as implied multiplication) is interpreted as having higher precedence than division, so that  equals , not .\n\nFor example, the manuscript submission instructions for the Physical Review journals state that multiplication is of higher precedence than division with a slash, and this is also the convention observed in prominent physics textbooks such as the Course of Theoretical Physics by Landau and Lifshitz and the Feynman Lectures on Physics.For example, the third edition of Mechanics by Landau and Lifshitz contains expressions such as hPz/2 (p. 22), and the first volume of the Feynman Lectures contains expressions such as 1/2 (p. 6–7). In both books these expressions are written with the convention that the solidus is evaluated last.\n\nMnemonics\nMnemonics are often used to help students remember the rules, involving the first letters of words representing various operations.  Different mnemonics are in use in different countries.http://www.mathcentre.ac.uk/resources/uploaded/mc-ty-rules-2009-1.pdf\n In the United States, the acronym PEMDAS is common. It stands for Parentheses, Exponents, Multiplication/Division, Addition/Subtraction. PEMDAS is often expanded to the mnemonic \"Please Excuse My Dear Aunt Sally\". \n Canada and New Zealand use BEDMAS, standing for Brackets, Exponents, Division/Multiplication, Addition/Subtraction. \n Most common in the UK, India, Bangladesh and Australiahttp://syllabus.bos.nsw.edu.au/assets/global/files/maths_s3_sampleu1.doc and some other English-speaking countries are BODMAS meaning Brackets, Order, Division/Multiplication, Addition/Subtraction. Nigeria and some other West African countries also use BODMAS. Similarly in the UK, BIDMAS is used, standing for Brackets, Indices, Division/Multiplication, Addition/Subtraction.\n\nThese mnemonics may be misleading when written this way. For example, misinterpreting any of the above rules to mean \"addition first, subtraction afterward\" would incorrectly evaluate the expression\n .\n\nThe correct value is 9 (and not 5, as if the addition would be carried out first and the result used with the subtraction afterwards).\n\nSpecial cases\nSerial exponentiation\nIf exponentiation is indicated by stacked symbols using superscript notation, the usual rule is to work from the top down:\n \nwhich typically is not equal to (ab)c.\n\nHowever, when using operator notation with a caret (^) or arrow (↑), there is no common standard.Exponentiation Associativity and Standard Math Notation Codeplea. 23 Aug 2016. Retrieved 20 Sep 2016. For example, Microsoft Excel and computation programming language MATLAB evaluate a^b^c as (ab)c, but Google Search and Wolfram Alpha as a(bc). Thus 4^3^2 is evaluated to 4,096 in the first case and to 262,144 in the second case.\n\nExponentiation and negation\nAnother difference between computer systems is -a^b, which  in Microsoft Excel is evaluated as (-a)^b, in Google Search as -(a^b). For compatibility, the Excel behavior is observed on LibreOffice.\n\nSerial division\nA similar ambiguity exists in the case of serial division, for example, the expression  can either be interpreted as \n \nor as\n \n\nThe left-to-right operation convention would resolve the ambiguity in favor of the last expression. Further, the mathematical habit of combining factors and representing division as multiplication by a reciprocal both greatly reduce the frequency of ambiguous division. However, when two long expressions are combined by division, the correct order of operations can be lost in the notation.\n\nCalculators\n\nDifferent calculators follow different orders of operations. Many simple calculators without a stack implement chain input working left to right without any priority given to different operators, for example typing\n 1 + 2 × 3 yields 9,\nwhile more sophisticated calculators will use a more standard priority, for example typing\n 1 + 2 × 3 yields 7.\n\nThe Microsoft Calculator program uses the former in its standard view and the latter in its scientific and programmer views.\n\nChain input expects two operands and an operator. When the next operator is pressed, the expression is immediately evaluated and the answer becomes the left hand of the next operator. Advanced calculators allow entry of the whole expression, grouped as necessary, and evaluates only when the user uses the equals sign.\n\nCalculators may associate exponents to the left or to the right depending on the model or the evaluation mode. For example, the expression a^b^c is interpreted as a(bc) on the TI-92 and the TI-30XS MultiView in \"Mathprint mode\", whereas it is interpreted as (ab)c on the TI-30XII and the TI-30XS MultiView in \"Classic mode\".\n\nAn expression like 1/2x is interpreted as 1/(2x) by TI-82, but as (1/2)x by TI-83 and every other TI calculator released since 1996, as well as by all Hewlett-Packard calculators with algebraic notation. While the first interpretation may be expected by some users, only the latter is in agreement with the standard rule that multiplication and division are of equal precedence, so 1/2x is read one divided by two and the answer multiplied by x.\n\nWhen the user is unsure how a calculator will interpret an expression, it is a good idea to use parentheses so there is no ambiguity.\n\nCalculators that utilize reverse Polish notation (RPN), also known as postfix notation, use a stack to enter formulas without the need for parentheses.\n\nProgramming languages\nSome programming languages use precedence levels that conform to the order commonly used in mathematics, though others, such as APL, Smalltalk, Occam and Mary, have no operator precedence rules (in APL, evaluation is strictly right to left; in Smalltalk etc. it is strictly left to right).\n\nIn addition, because many operators are not associative, the order within any single level is usually defined by grouping left to right so that 16/4/4 is interpreted as  rather than ; such operators are perhaps misleadingly referred to as \"left associative\". Exceptions exist; for example, languages with operators corresponding to the cons operation on lists usually make them group right to left (\"right associative\"), e.g. in Haskell, 1:2:3:4:[] == 1:(2:(3:(4:[]))) == [1,2,3,4].\n\nThe logical bitwise operators in C (and all programming languages that borrow precedence rules from C, for example, C++, Perl and PHP) have a precedence level that the creator of the C language considered unsatisfactory.Dennis M. Ritchie: The Development of the C Language. In History of Programming Languages, 2nd ed., ACM Press 1996. However, many programmers have become accustomed to this order. The relative precedence levels of operators found in many C-style languages are as follows:\n\n1  ()   []   ->   .   ::  Function call, scope, array/member access2  !   ~   -   +   *   &   sizeof   type cast   ++   --    (most) unary operators, sizeof and type casts (right to left)3  *   /   % MOD  Multiplication, division, modulo4  +   -  Addition and subtraction5  <<   >>  Bitwise shift left and right6  <   <=   >   >=  Comparisons: less-than and greater-than7  ==   !=  Comparisons: equal and not equal8  &  Bitwise AND9  ^  Bitwise exclusive OR (XOR)10  |  Bitwise inclusive (normal) OR11  &&  Logical AND12  ||  Logical OR13  ? :  Conditional expression (ternary)14  =   +=   -=   *=   /=   %=   &=   |=   ^=   <<=   >>=  Assignment operators (right to left)15 ,  Comma operator\n\nExamples:\n(Note: in the examples below, '≡' is used to mean \"is equivalent to\", and not to be interpreted as an actual assignment operator used as part of the example expression.)\n !A + !B ≡ (!A) + (!B)\n ++A + !B ≡ (++A) + (!B)\n A + B * C ≡ A + (B * C)\n A || B && C ≡ A || (B && C)\n A && B == C ≡ A && (B == C)\n A & B == C ≡ A & (B == C)\n\nSource-to-source compilers that compile to multiple languages need to explicitly deal with the issue of different order of operations across languages. Haxe for example standardizes the order and enforces it by inserting brackets where it is appropriate.6÷2(1+2)=? Andy Li's Blog. 2 May 2011. Retrieved 31 December 2012.\n\nThe accuracy of software developer knowledge about binary operator precedence has been found to closely follow their frequency of occurrence in source code.\"Developer beliefs about binary operator precedence\" Derek M. Jones, CVu 18(4):14–21\n\nSee also\n Associativity\n Common operator notation (for a more formal description)\n Commutativity\n Distributivity\n Hyperoperation\n Operator (programming)\n Operator associativity\n Operator overloading\n Operator precedence in C and C++\n Polish notation\n Reverse Polish notation\n\nNotes\n\nReferences\n\nExternal links\n \n\nCategory:Abstract algebra\nCategory:Algebra\nCategory:Mnemonics\nCategory:Operators (programming)"
    },
    {
      "title": "Oriented Point Relation Algebra",
      "url": "https://en.wikipedia.org/wiki/Oriented_Point_Relation_Algebra",
      "text": "The Oriented Point Relation Algebra (OPRA) serves for qualitative spatial representation and reasoning. OPRA is an orientation calculus with adjustable granularity.  OPRA is based on objects which are represented as oriented points.  Oriented points are specified as pair of a point and a direction on the 2D-plane.\n\nReferences\n Till Mossakowski, Reinhard Moratz (2012). Qualitative Reasoning about Relative Direction of Oriented Points.  Artificial Intelligence, Vol. 180–181, pp. 34–45.\n\nCategory:Algebra\nCategory:Logical calculi"
    },
    {
      "title": "Partial fraction decomposition",
      "url": "https://en.wikipedia.org/wiki/Partial_fraction_decomposition",
      "text": "In algebra, the partial fraction decomposition or partial fraction expansion of a rational function (that is, a fraction such that the numerator and the denominator are both polynomials) is an operation that consists of expressing the fraction as a sum of a polynomial (possibly zero) and one or several fractions with a simpler denominator.\n\nThe importance of the partial fraction decomposition lies in the fact that it provides algorithms for various computations with rational functions, including the explicit computation of antiderivatives,Horowitz, Ellis. \"Algorithms for partial fraction decomposition and rational function integration.\" Proceedings of the second ACM symposium on Symbolic and algebraic manipulation. ACM, 1971.  Taylor series expansions, inverse Z-transforms, inverse Laplace transforms. The concept was discovered independently in 1702 by both Johann Bernoulli and Gottfried Leibniz.\n\nIn symbols, one can use partial fraction expansion to change a rational fraction in the form\n\n \n\nwhere  and  are polynomials, into an expression of the form\n\n \n\nwhere:\n\n the denominator, , of each fraction is a power of an irreducible (not factorable into polynomials of positive degree) polynomial and\n the numerator is a polynomial of smaller degree than this irreducible polynomial.\n\nAs factorization of polynomials may be difficult, a coarser decomposition is often preferred, which consists of replacing factorization by square-free factorization. This amounts to replace \"irreducible\" by \"square-free\" in the preceding description of the outcome.\n\n Basic principles \n\nIf a rational function  in one indeterminate  has a denominator that factors as\n\nover a field  (we can take this to be real numbers, or complex numbers) and if in addition  and  have no common factor, then by Bézout's identity for polynomials, there exist polynomials  and  such that , , and\n\nThus\n\nand hence  may be written as\n\nwhere all numerators are polynomials.\n\nUsing this idea inductively the rational function  can be written as a sum with denominators being powers of irreducible polynomials. To take this further, if required, write:\n\nas a sum with denominators powers of  and numerators of degree lower than the degree of , plus a possible extra polynomial. This can be done by the Euclidean algorithm for polynomials.  The result is the following theorem:\n\nIf  is field of complex numbers, the fundamental theorem of algebra implies that all   have degree one, and all numerators  are constants. When  is the field of real numbers, some of the  may be quadratic, so, in the partial fraction decomposition, quotients of linear polynomials by powers of quadratic polynomials may also occur.\n\nIn the preceding theorem, one may replace \"distinct irreducible polynomials\" by \"pairwise coprime polynomials that are coprime with their derivative\". For example, the  may be the factors of the square-free factorization of . When  is the field of rational numbers, as it is typically the case in computer algebra, this allows to replace factorization by greatest common divisor computation for computing a partial fraction decomposition.\n\nApplication to symbolic integration\n\nFor the purpose of symbolic integration, the preceding result may be refined into\n\nThis reduces the computation of the antiderivative of a rational function to the integration of the last sum, which is called the logarithmic part, because its antiderivative is a linear combination of logarithms. In fact, we have\n\nThere are various methods to compute above decomposition. The one that is the simplest to describe is probably the so-called Hermite's method. As the degree of cij is bounded by the degree of pi, and the degree of b is the difference of the degrees of f and g (if this difference is non negative; otherwise, b=0), one may write these unknowns polynomials as polynomials with unknown coefficients. Reducing the two members of above formula to the same denominator and writing that the coefficients of each power of x are the same in the two numerators, one gets a system of linear equations which can be solved to obtain the desired values for the unknowns coefficients.\n\n Procedure \n\nGiven two polynomials  and , where the αi are distinct constants and deg P < n, partial fractions are generally obtained by supposing that\n\n \n\nand solving for the ci constants, by substitution, by equating the coefficients of terms involving the powers of x, or otherwise.  (This is a variant of the method of undetermined coefficients.)\n\nA more direct computation, which is strongly related with Lagrange interpolation consists of writing\n\n \n\nwhere  is the derivative of the polynomial .\n\nThis approach does not account for several other cases, but can be modified accordingly:\n\n If  then it is necessary to perform the Euclidean division of P by Q, using polynomial long division, giving P(x) = E(x) Q(x) + R(x) with deg R < n. Dividing by Q(x) this gives\n \nand then seek partial fractions for the remainder fraction (which by definition satisfies deg R < deg Q).\n If Q(x) contains factors which are irreducible over the given field, then the numerator N(x) of each partial fraction with such a factor F(x) in the denominator must be sought as a polynomial with deg N < deg F, rather than as a constant. For example, take the following decomposition over R:\n \n Suppose Q(x) = (x − α)rS(x) and S(α) ≠ 0. Then Q(x) has a zero α of multiplicity r, and in the partial fraction decomposition, r of the partial fractions will involve the powers of (x − α). For illustration, take S(x) = 1 to get the following decomposition:\n \n\n Illustration \n\nIn an example application of this procedure,  can be decomposed in the form\n\n \n\nClearing denominators shows that . Expanding and equating the coefficients of powers of x gives\n\n 5 = A + B and  3x = −2Bx\n\nSolving for A and B yields A = 13/2 and B = −3/2.  Hence,\n\n \n\n Residue method \n\nOver the complex numbers, suppose f(x) is a rational proper fraction, and can be decomposed into\n\n \n\nLet\n\n \n\nthen according to the uniqueness of Laurent series, aij is the coefficient of the term (x − xi)−1 in the Laurent expansion of gij(x) about the point xi, i.e., its residue\n\n \n\nThis is given directly by the formula\n\n \n\nor in the special case when xi is a simple root,\n\n \n\nwhen\n\n \n\n Over the reals \n\nPartial fractions are used in real-variable integral calculus to find real-valued antiderivatives of rational functions. Partial fraction decomposition of real rational functions is also used to find their Inverse Laplace transforms. For applications of partial fraction decomposition over the reals, see\n\n Application to symbolic integration, above\n Partial fractions in Laplace transforms\n\n General result \n\nLet f(x) be any rational function over the real numbers.  In other words, suppose there exist real polynomials functions p(x) and q(x)≠ 0, such that\n\n \n\nBy dividing both the numerator and the denominator by the leading coefficient of q(x), we may assume without loss of generality that q(x) is monic. By the fundamental theorem of algebra, we can write\n\n \n\nwhere a1,..., am, b1,..., bn, c1,..., cn are real numbers with bi2 − 4ci < 0, and j1,..., jm, k1,..., kn are positive integers.  The terms (x − ai) are the linear factors of q(x) which correspond to real roots of q(x), and the terms (xi2 + bix + ci) are the irreducible quadratic factors of q(x) which correspond to pairs of complex conjugate roots of q(x).\n\nThen the partial fraction decomposition of f(x) is the following:\n\n \n\nHere, P(x) is a (possibly zero) polynomial, and the Air, Bir, and Cir are real constants.  There are a number of ways the constants can be found.\n\nThe most straightforward method is to multiply through by the common denominator q(x).  We then obtain an equation of polynomials whose left-hand side is simply p(x) and whose right-hand side has coefficients which are linear expressions of the constants Air, Bir, and Cir.  Since two polynomials are equal if and only if their corresponding coefficients are equal, we can equate the coefficients of like terms.  In this way, a system of linear equations is obtained which always has a unique solution.  This solution can be found using any of the standard methods of linear algebra. It can also be found with limits (see Example 5).\n\n Examples \n\n Example 1 \n\n \n\nHere, the denominator splits into two distinct linear factors:\n\n \n\nso we have the partial fraction decomposition\n\n \n\nMultiplying through by the denominator on the left-hand side gives us the polynomial identity\n\n \n\nSubstituting x  = −3 into this equation gives A = −1/4, and substituting x  = 1 gives B = 1/4, so that\n\n \n\n Example 2 \n\n \n\nAfter long-division, we have\n\n \n\nThe factor x2 − 4x + 8 is irreducible over the reals, as its discriminant  is negative. Thus the partial fraction decomposition over the reals has the shape\n\n \n\nMultiplying through by x3 − 4x2 + 8x, we have the polynomial identity\n\n \n\nTaking x = 0, we see that 16 = 8A, so A = 2.  Comparing the x2 coefficients, we see that 4 = A + B = 2 + B, so B = 2.  Comparing linear coefficients, we see that −8 = −4A + C = −8 + C, so C = 0.  Altogether,\n\n \n\nThe fraction can be completely decomposed using complex numbers. According to the fundamental theorem of algebra every complex polynomial of degree n has n (complex) roots (some of which can be repeated). The second fraction can be decomposed to:\n\nMultiplying through by the denominator gives:\n\nEquating the coefficients of  and the constant (with respect to ) coefficients of both sides of this equation, one gets a system of two linear equations in  and , whose solution is\n\nThus we have a complete decomposition:\n\nOne may also compute directly  and  with the residue method (see also example 4 below).\n\n Example 3 \n\nThis example illustrates almost all the \"tricks\" we might need to use, short of consulting a computer algebra system.\n\n \n\nAfter long-division and factoring the denominator, we have\n\n \n\nThe partial fraction decomposition takes the form\n\nMultiplying through by the denominator on the left-hand side we have the polynomial identity\n\nNow we use different values of x to compute the coefficients:\n\nSolving this we have:\n\nUsing these values we can write:\n\n \n\nWe compare the coefficients of x6 and x5 on both side and we have:\n\nTherefore:\n\nwhich gives us B = 0. Thus the partial fraction decomposition is given by:\n\n \n\nAlternatively, instead of expanding, one can obtain other linear dependences on the coefficients computing some derivatives at  in the above polynomial identity. (To this end, recall that the derivative at x = a of (x − a)mp(x) vanishes if m > 1 and is just p(a) for m = 1.) For instance the first derivative at x = 1 gives\n\n \n\nthat is 8 = 4B + 8  so B = 0.\n\nExample 4 (residue method)\n\nThus, f(z) can be decomposed into rational functions whose denominators are z+1, z−1, z+i, z−i. Since each term is of power one, −1, 1, −i and i are simple poles.\n\nHence, the residues associated with each pole, given by\n\nare \n\nrespectively, and\n\nExample 5 (limit method)\n\nLimits can be used to find a partial fraction decomposition. Consider the following example:\n\nFirst, factor the denominator which determines the decomposition:\n\nMultiplying everything by , and taking the limit when , we get\n\nOn the other hand,\n\nand thus:\n\nMultiplying by  and taking the limit when , we have\n\nand\n\nThis implies  and so .\n\nFor , we get   and thus  .\n\nPutting everything together, we get the decomposition\n\nExample 6 (integral)\nSuppose we have the indefinite integral:\n\nBefore performing decomposition, it is obvious we must perform polynomial long division and factor the denominator. Doing this would result in:\n\nUpon this, we may now perform partial fraction decomposition.\n\nso:\n.\nUpon substituting our values, in this case, where x=1 to solve for B and x=-2 to solve for A, we will result in:\n\nPlugging all of this back into our integral allows us to find the answer:\n\n The role of the Taylor polynomial \n\nThe partial fraction decomposition of a rational function can be related to Taylor's theorem as follows. Let\n\nbe real or complex polynomials \nassume that\n\nsatisfies\n\nAlso define\n\nThen we have\n\nif, and only if, each polynomial  is the Taylor polynomial of  of order  at the point :\n\nTaylor's theorem (in the real or complex case) then provides a proof of the existence and uniqueness of the partial fraction decomposition, and a characterization of the coefficients.\n\nSketch of the proof\n\nThe above partial fraction decomposition implies, for each 1 ≤ i ≤ r, a polynomial expansion\n\nso  is the Taylor polynomial of , because of the unicity of the polynomial expansion of order , and by assumption .\n\nConversely, if the  are the Taylor polynomials, the above expansions at each  hold, therefore we also have\n\nwhich implies that the polynomial  is divisible by \n\nFor  is also divisible by , so\n\nis divisible by . Since\n\nwe then have\n\nand we find the partial fraction decomposition dividing by .\n\n Fractions of integers \n\nThe idea of partial fractions can be generalized to other integral domains, say the ring of integers where prime numbers take the role of irreducible denominators. For example:\n\n \n\n Notes \n\n References \n\n \n\n External links \n \n \n  Make partial fraction decompositions with Scilab.\n\nCategory:Algebra\nCategory:Elementary algebra\nCategory:Partial fractions"
    },
    {
      "title": "Permanent (mathematics)",
      "url": "https://en.wikipedia.org/wiki/Permanent_%28mathematics%29",
      "text": "In linear algebra, the permanent of a square matrix is a function of the matrix similar to the determinant. The permanent, as well as the determinant, is a polynomial in the entries of the matrix. Both are special cases of a more general function of a matrix called the immanant.\n\n Definition \nThe permanent of an n-by-n matrix A = (ai,j) is defined as\n\n \n\nThe sum here extends over all elements σ of the symmetric group Sn; i.e. over all permutations of the numbers 1, 2, ..., n.\n\nFor example,\n\nand\n\nThe definition of the permanent of A differs from that of the determinant of A in that the signatures of the permutations are not taken into account.\n\nThe permanent of a matrix A is denoted per A, perm A, or Per A, sometimes with parentheses around the argument. In his monograph,  uses Per(A) for the permanent of rectangular matrices, and uses per(A) when A is a square matrix.  uses the notation .\n\nThe word, permanent, originated with Cauchy in 1812 as “fonctions symétriques permanentes” for a related type of function, and was used by  in the modern, more specific, sense.\n\n Properties and applications \nIf one views the permanent as a map that takes n vectors as arguments, then it is a multilinear map and it is symmetric (meaning that any order of the vectors results in the same permanent). Furthermore, given a square matrix  of order n, we have:\n perm(A) is invariant under arbitrary permutations of the rows and/or columns of A. This property may be written symbolically as perm(A) = perm(PAQ) for any appropriately sized permutation matrices P and Q,\n multiplying any single row or column of A by a scalar s changes perm(A) to s⋅perm(A),\n perm(A) is invariant under transposition, that is, perm(A) = perm(AT).\n\nIf  and  are square matrices of order n then,\n\nwhere s and t are subsets of the same size of {1,2,...,n} and  are their respective complements in that set.\n\nOn the other hand, the basic multiplicative property of determinants is not valid for permanents. A simple example shows that this is so.\n\n \n\nA formula similar to Laplace's for the development of a determinant along a row, column or diagonal is also valid for the permanent; all signs have to be ignored for the permanent. For example, expanding along the first column,\n\n  \n\nwhile expanding along the last row gives,\n\n \n\nUnlike the determinant, the permanent has no easy geometrical interpretation; it is mainly used in combinatorics, in treating boson Green's functions in quantum field theory, and in determining state probabilities of boson sampling systems. However, it has two graph-theoretic interpretations: as the sum of weights of cycle covers of a directed graph, and as the sum of weights of perfect matchings in a bipartite graph.\n\nSymmetric tensors\n\nThe permanent arises naturally in the study of the symmetric tensor power of Hilbert spaces.  In particular, for a Hilbert space , let  denote the th symmetric tensor power of , which is the space of symmetric tensors.  Note in particular that  is spanned by the Symmetric products of elements in .  For , we define the symmetric product of these elements by\n\nIf we consider  (as a subspace of , the kth tensor power of ) and define the inner product on  accordingly, we find that for \n\nApplying the Cauchy–Schwarz inequality, we find that , and that\n\nCycle covers\nAny square matrix  can be viewed as the adjacency matrix of a weighted directed graph, with  representing the weight of the arc from vertex i to vertex j. \nA cycle cover of a weighted directed graph is a collection of vertex-disjoint directed cycles in the digraph that covers all vertices in the graph. Thus, each vertex i in the digraph has a unique \"successor\"  in the cycle cover, and  is a permutation on  where n is the number of vertices in the digraph. Conversely, any permutation  on  corresponds to a cycle cover in which there is an arc from vertex i to vertex  for each i.\n \nIf the weight of a cycle-cover is defined to be the product of the weights of the arcs in each cycle, then\n\n \n\nThe permanent of an  matrix A is defined as\n\nwhere  is a permutation over . Thus the permanent of A is equal to the sum of the weights of all cycle-covers of the digraph.\n\nPerfect matchings\nA square matrix  can also be viewed as the adjacency matrix of a bipartite graph which has vertices  on one side and  on the other side, with  representing the weight of the edge from vertex  to vertex . If the weight of a perfect matching  that matches  to  is defined to be the product of the weights of the edges in the matching, then\n\nThus the permanent of A is equal to the sum of the weights of all perfect matchings of the graph.\n\n Permanents of (0, 1) matrices \n Enumeration \nThe answers to many counting questions can be computed as permanents of matrices that only have 0 and 1 as entries. \n\nLet Ω(n,k) be the class of all (0, 1)-matrices of order n with each row and column sum equal to k. Every matrix A in this class has perm(A) > 0. The incidence matrices of projective planes are in the class Ω(n2 + n + 1, n + 1) for n an integer > 1. The permanents corresponding to the smallest projective planes have been calculated. For n = 2, 3, and 4 the values are 24, 3852 and 18,534,400 respectively. Let Z be the incidence matrix of the projective plane with n = 2, the Fano plane. Remarkably, perm(Z) = 24 = |det (Z)|, the absolute value of the determinant of Z. This is a consequence of Z being a circulant matrix and the theorem:\n\nIf A is a circulant matrix in the class Ω(n,k) then if k > 3, perm(A) > |det (A)| and if k = 3, perm(A) = |det (A)|. Furthermore, when k = 3, by permuting rows and columns, A can be put into the form of a direct sum of e copies of the matrix Z and consequently, n = 7e and perm(A) = 24e.\n\nPermanents can also be used to calculate the number of permutations with restricted (prohibited) positions. For the standard n-set {1, 2, ..., n}, let  be the (0, 1)-matrix where aij = 1 if i → j is allowed in a permutation and aij = 0 otherwise. Then perm(A) is equal to the number of permutations of the n-set that satisfy all the restrictions. Two well known special cases of this are the solution of the derangement problem and the ménage problem: the number of permutations of an n-set with no fixed points (derangements) is given by\n\nwhere J is the n×n all 1's matrix and I is the identity matrix, and the ménage numbers are given by\n\n \n\nwhere I is the (0, 1)-matrix with nonzero entries in positions (i, i + 1) and (n, 1).\n\n Bounds \nThe Bregman–Minc inequality, conjectured by H. Minc in 1963 and proved by L. M. Brégman in 1973, gives an upper bound for the permanent of an n × n (0, 1)-matrix.  If A has ri ones in row i for each 1 ≤ i ≤ n, the inequality states that\n\n Van der Waerden's conjecture \nIn 1926 Van der Waerden conjectured that the minimum permanent among all  doubly stochastic matrices is n!/nn, achieved by the matrix for which all entries are equal to 1/n.. Proofs of this conjecture were published in 1980 by B. Gyires. and in 1981 by G. P. Egorychev. . . and D. I. Falikman;. Egorychev's proof is an application of the Alexandrov–Fenchel inequality.Brualdi (2006) p.487 For this work, Egorychev and Falikman won the Fulkerson Prize in 1982.Fulkerson Prize, Mathematical Optimization Society, retrieved 2012-08-19.\n\n Computation \n\nThe naïve approach, using the definition, of computing permanents is computationally infeasible even for relatively small matrices. One of the fastest known algorithms is due to H. J. Ryser (). Ryser’s method is based on an inclusion–exclusion formula that can be given p. 99 as follows: Let  be obtained from A by deleting k columns, let  be the product of the row-sums of , and  let  be the sum of the values of  over all possible . Then  \n\nIt may be rewritten in terms of the matrix entries as follows:\n \n\nThe permanent is believed to be more difficult to compute than the determinant. While the determinant can be computed in polynomial time by Gaussian elimination, Gaussian elimination cannot be used to compute the permanent. Moreover, computing the permanent of a (0,1)-matrix is #P-complete. Thus, if the permanent can be computed in polynomial time by any method, then FP = #P, which is an even stronger statement than P = NP. When the entries of A are nonnegative, however, the permanent can be computed approximately in probabilistic polynomial time, up to an error of , where  is the value of the permanent and  is arbitrary. The permanent of a certain set of positive semidefinite matrices can also be approximated in probabilistic polynomial time: the best achievable error of this approximation is  ( is again the value of the permanent).\n\nMacMahon's Master Theorem\n\nAnother way to view permanents is via multivariate generating functions. Let  be a square matrix of order n. Consider the multivariate generating function:\n\nThe coefficient of   in  is perm(A).\n\nAs a generalization, for any sequence of n non-negative integers,  define:\n  as the coefficient of  in \n MacMahon's Master Theorem''' relating permanents and determinants is:\n\nwhere I is the order n identity matrix and X is the diagonal matrix with diagonal \n\nPermanents of rectangular matrices\nThe permanent function can be generalized to apply to non-square matrices. Indeed, several authors make this the definition of a permanent and consider the restriction to square matrices a special case.In particular,  and  do this. Specifically, for an m × n matrix  with m ≤ n, define\n\nwhere P(n,m) is the set of all m-permutations of the n-set {1,2,...,n}.\n\nRyser's computational result for permanents also generalizes. If A is an m × n matrix with m ≤ n, let  be obtained from A by deleting k columns, let  be the product of the row-sums of , and  let  be the sum of the values of  over all possible . Then  \n\nSystems of distinct representatives\nThe generalization of the definition of a permanent to non-square matrices allows the concept to be used in a more natural way in some applications. For instance:\n\nLet S1, S2, ..., Sm be subsets (not necessarily distinct) of an n-set with m ≤ n. The incidence matrix of this collection of subsets is an m × n (0,1)-matrix A. The number of systems of distinct representatives (SDR's) of this collection is perm(A'').\n\nSee also\nComputing the permanent\nBapat–Beg theorem, an application of permanents in order statistics\nSlater determinant, an application of permanents in quantum mechanics\n\nNotes\n\nReferences\n\nFurther reading\n  Contains a proof of the Van der Waerden conjecture.\n \n\nExternal links\nPermanent at PlanetMath\n\nCategory:Algebra\nCategory:Linear algebra\nCategory:Matrix theory\nCategory:Permutations"
    },
    {
      "title": "Ping-pong lemma",
      "url": "https://en.wikipedia.org/wiki/Ping-pong_lemma",
      "text": "In mathematics, the ping-pong lemma, or table-tennis lemma, is any of several mathematical statements that ensure that several elements in a group acting on a set freely generates a free subgroup of that group.\n\nHistory\n\nThe ping-pong argument goes back to late 19th century and is commonly attributed to Felix Klein who used it to study subgroups of Kleinian groups, that is, of discrete groups of isometries of the hyperbolic 3-space or, equivalently Möbius transformations of the Riemann sphere. The ping-pong lemma was a key tool used by Jacques Tits in his 1972 paperJ. Tits. Free subgroups in linear groups. Journal of Algebra, vol. 20 (1972), pp. 250–270 containing the proof of a famous result now known as the Tits alternative. The result states that a finitely generated linear group is either virtually solvable or contains a free subgroup of rank two. The ping-pong lemma and its variations are widely used in geometric topology and geometric group theory.\n\nModern versions of the ping-pong lemma can be found in many books such as Lyndon&Schupp,Roger C. Lyndon and Paul E. Schupp. Combinatorial Group Theory. Springer-Verlag, New York, 2001. \"Classics in Mathematics\" series, reprint of the 1977 edition. ; Ch II, Section 12, pp. 167–169 de la Harpe,Pierre de la Harpe. Topics in geometric group theory. Chicago Lectures in Mathematics. University of Chicago Press, Chicago. ; Ch. II.B \"The table-Tennis Lemma (Klein's criterion) and examples of free products\"; pp. 25–41. Bridson&HaefligerMartin R. Bridson, and André Haefliger. Metric spaces of non-positive curvature. Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences], 319. Springer-Verlag, Berlin, 1999. ; Ch.III.Γ, pp. 467–468 and others.\n\nFormal statements\n\nPing-pong lemma for several subgroups\n\nThis version of the ping-pong lemma ensures that several subgroups of a group acting on a set generate a free product. The following statement appears inAndrij Olijnyk and Vitaly Suchchansky. Representations of free products by infinite unitriangular matrices over finite fields. International Journal of Algebra and Computation. Vol. 14 (2004), no. 5–6, pp. 741–749; Lemma 2.1, and the proof is from.\n\nLet G be a group acting on a set X and let H1, H2,...., Hk be nontrivial subgroups of G where k≥2, such that at least one of these subgroups has order greater than 2.\nSuppose there exist pairwise disjoint nonempty subsets X1, X2,....,Xk of X such that the following holds:\n\nFor any i≠s and for any h∈Hi, h≠1  we have h(Xs)⊆Xi.\n\nThen\n\nProof\nBy the definition of free product, it suffices to check that a given reduced word is nontrivial. Let w be such a word, and let\n\nWhere ws,βj∈ Hs for all such βj, and since w is fully reduced αi≠ αi+1  for any i. We then let w act on an element of one of the sets Xi. As we assume for at least one subgroup Hi has order at least 3, without loss of generality we may assume that H1 has order at least 3. We first make the assumption that α1 and αm are both 1. From here we consider w acting on X2. We get the following chain of containments and note that since the Xi are disjoint that w acts nontrivially and is thus not the identity element.\n\n \n\nTo finish the proof we must consider the three cases: \nIf , then let  (Such  exists since by assumption H1 has order at least 3.)\nIf , then let \nAnd if , then let \nIn each case, hwh−1 is a reduced word with α1'  and αm '' both 1, and thus is nontrivial. Finally, hwh−1 is not 1, and so neither is w. This proves the claim.\n\nThe Ping-pong lemma for cyclic subgroups\n\nLet G be a group acting on a set X. Let a1,...,ak be elements of G of infinite order, where k ≥ 2. Suppose there exist disjoint nonempty subsets\n\nX1+,...,Xk+ and X1–,...,Xk–\n\nof X with the following properties:\n\nai(X − Xi–) ⊆ Xi+ for i = 1, ..., k;\nai−1(X − Xi+) ⊆ Xi– for i = 1, ..., k.\n\nThen the subgroup H = <a1, ..., ak> ≤ G generated by a1, ..., ak is free with free basis {a1, ..., ak}.\n\nProof\nThis statement follows as a corollary of the version for general subgroups if we let Xi= Xi+∪Xi− and let Hi = ⟨ai⟩.\n\nExamples\n\nSpecial linear group example\nOne can use the ping-pong lemma to prove that the subgroup H = <A,B>≤SL(2,Z), generated by  the matrices\n\n and \n\nis free of rank two.\n\nProof\nIndeed, let H1 = <A> and H2 = <B> be cyclic subgroups of SL(2,Z) generated by A and B accordingly.   It is not hard to check that A and B are elements of infinite order in SL(2,Z) and that\n\nand\n\nConsider the standard action of SL(2,Z) on R2 by linear transformations. Put\n\nand\n\nIt is not hard to check, using the above explicitly descriptions of H1 and H2 that for every nontrivial g ∈ H1 we have g(X2) ⊆ X1 and that for every nontrivial g ∈ H2 we have g(X1) ⊆ X2. Using the alternative form of the ping-pong lemma, for two subgroups, given above, we conclude that H = H1∗H2. Since the groups H1 and H2 are infinite cyclic, it follows that H is a free group of rank two.\n\nWord-hyperbolic group example\n\nLet G be a word-hyperbolic group which is torsion-free, that is, with no nontrivial elements of finite order. Let g, h ∈ G be two non-commuting elements, that is such that gh ≠ hg. Then there exists M≥1 such that for any integers n ≥ M, m ≥ M the subgroup H = <gn, hm> ≤ G is free of rank two.\n\nSketch of the proofM. Gromov. Hyperbolic groups. Essays in group theory, pp. 75–263, Mathematical Sciences Research Institute Publications, 8, Springer, New York, 1987;  ; Ch. 8.2, pp. 211–219.\nThe group G acts on its hyperbolic boundary ∂G by homeomorphisms. It is known that if a ∈ G is a nontrivial element then a has exactly two distinct fixed points, a∞ and a−∞ in ∂G and that a∞ is an attracting fixed point while a−∞ is a repelling fixed point.\n\nSince g and h do not commute, the basic facts about word-hyperbolic groups imply that g∞, g−∞, h∞ and h−∞ are four distinct points in   ∂G. Take disjoint neighborhoods U+, U–, V+ and V– of g∞, g−∞, h∞ and h−∞ in ∂G respectively.\nThen the attracting/repelling properties of the fixed points of g and h imply that there exists M ≥ 1 such that for any integers n ≥ M, m ≥ M we have:\ngn(∂G – U–) ⊆ U+\ng−n(∂G – U+) ⊆ U–\nhm(∂G – V–) ⊆ V+\nh−m(∂G – V+) ⊆ V–\n\nThe ping-pong lemma now implies that H = <gn, hm> ≤ G is free of rank two.\n\nApplications of the ping-pong lemma\n\nThe ping-pong lemma is used in Kleinian groups to study their so-called Schottky subgroups.  In the Kleinian groups context the ping-pong lemma can be used to show that a particular group of isometries of the hyperbolic 3-space is not just free but also properly discontinuous and geometrically finite.\nSimilar Schottky-type arguments are widely used in geometric group theory, particularly for subgroups of word-hyperbolic groups and for automorphism groups of trees.Alexander Lubotzky. Lattices in rank one Lie groups over local fields.  Geometric and Functional Analysis, vol. 1 (1991),  no. 4, pp. 406–431\nPing-pong lemma is also used for studying Schottky-type subgroups of mapping class groups of Riemann surfaces, where the set on which the mapping class group acts is the Thurston boundary of the Teichmüller space.Richard P. Kent, and Christopher J. Leininger. Subgroups of mapping class groups from the geometrical viewpoint. In the tradition of Ahlfors-Bers. IV, pp. 119–141,\nContemporary Mathematics series, 432, American Mathematical Society, Providence, RI, 2007; ; 0-8218-4227-7 A similar argument is also utilized in the study of subgroups of the outer automorphism group of a free group.M. Bestvina, M. Feighn, and M. Handel. Laminations, trees, and irreducible automorphisms of free groups.  Geometric and Functional Analysis, vol. 7  (1997),  no. 2, pp. 215–244.\nOne of the most famous applications of the ping-pong lemma is in the proof of Jacques Tits of the so-called Tits alternative for linear groups. (see also Pierre de la Harpe. Free groups in linear groups. L'Enseignement Mathématique (2), vol. 29 (1983), no. 1-2, pp. 129–144 for an overview of Tits' proof and an explanation of the ideas involved, including the use of the ping-pong lemma).\nThere are generalizations of the ping-pong lemma that produce not just free products but also amalgamated free products and HNN extensions. These generalizations are used, in particular, in the proof of Maskit's Combination Theorem for Kleinian groups.Bernard Maskit.\nKleinian groups. Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences], 287. Springer-Verlag, Berlin, 1988. ;  Ch. VII.C and Ch. VII.E pp.149–156 and pp. 160–167\nThere are also versions of the ping-pong lemma which guarantee that several elements in a group generate a free semigroup. Such versions are available both in the general context of a group action on a set,Pierre de la Harpe. Topics in geometric group theory. Chicago Lectures in Mathematics. University of Chicago Press, Chicago. ; Ch. II.B \"The table-Tennis Lemma (Klein's criterion) and examples of free products\"; pp. 187–188.  and for specific types of actions, e.g. in the context of linear groups,Alex Eskin, Shahar Mozes and Hee Oh. On uniform exponential growth for linear groups. Inventiones Mathematicae. vol. 60 (2005), no. 1, pp.1432–1297; Lemma 2.2 groups acting on treesRoger C. Alperin and Guennadi A. Noskov. Uniform growth, actions on trees and GL2. Computational and Statistical Group Theory:AMS Special Session Geometric Group Theory, April 21–22, 2001, Las Vegas, Nevada, AMS Special Session Computational Group Theory, April 28–29, 2001, Hoboken, New Jersey. (Robert H. Gilman, Vladimir Shpilrain, Alexei G. Myasnikov, editors). American Mathematical Society, 2002. ; page 2, Lemma 3.1 and others.Yves de Cornulier and Romain Tessera. Quasi-isometrically embedded free sub-semigroups. Geometry & Topology, vol. 12 (2008), pp. 461–473; Lemma 2.1\n\nReferences\n\nSee also\nFree group\nFree product\nKleinian group\nTits alternative\nWord-hyperbolic group\nSchottky group\n\nCategory:Algebra\nCategory:Group theory\nCategory:Discrete groups\nCategory:Lie groups\nCategory:Combinatorics on words"
    },
    {
      "title": "Polynomial",
      "url": "https://en.wikipedia.org/wiki/Polynomial",
      "text": "The graph of a polynomial function of degree 3.|thumb\n\nIn mathematics, a polynomial is an expression consisting of variables (also called indeterminates) and coefficients, that involves only the operations of addition, subtraction, multiplication, and non-negative integer exponents of variables. An example of a polynomial of a single indeterminate, , is . An example in three variables is .\n\nPolynomials appear in many areas of mathematics and science. For example, they are used to form polynomial equations, which encode a wide range of problems, from elementary word problems to complicated scientific problems; they are used to define polynomial functions, which appear in settings ranging from basic chemistry and physics to economics and social science; they are used in calculus and numerical analysis to approximate other functions. In advanced mathematics, polynomials are used to construct polynomial rings and algebraic varieties, central concepts in algebra and algebraic geometry.\n\nEtymology\nThe word polynomial joins two diverse roots: the Greek poly, meaning \"many,\" and the Latin nomen, or name. It was derived from the term binomial by replacing the Latin root bi- with the Greek poly-. The word polynomial was first used in the 17th century.See \"polynomial\" and \"binomial\", Compact Oxford English Dictionary\n\nNotation and terminology\nThe x occurring in a polynomial is commonly called either a variable or an indeterminate. When the polynomial is considered as an expression, x is a fixed symbol which does not have any value (its value is \"indeterminate\"). However, when one considers the function defined by the polynomial, then x represents the argument of the function, and is therefore called a \"variable\". Many authors use these two words interchangeably.\n\nIt is common to use uppercase letters for indeterminates and corresponding lowercase letters for the variables (or arguments) of the associated function .\n\nA polynomial P in the indeterminate x is commonly denoted either as P or as P(x). Formally, the name of the polynomial is P, not P(x), but the use of the functional notation P(x) date from the time where the distinction between a polynomial and the associated function was unclear. Moreover, the functional notation is often useful for specifying, in a single phrase, a polynomial and its indeterminate. For example, \"let P(x) be a polynomial\" is a shorthand for \"let P be a polynomial in the indeterminate x\". On the other hand, when it is not necessary to emphasize the name of the indeterminate, many formulas are much simpler and easier to read if the name(s) of the indeterminate(s) do not appear at each occurrence of the polynomial.\n\nThe ambiguity of having two notations for a single mathematical object may be formally resolved by considering the general meaning of the functional notation for polynomials.\nIf a denotes a number, a variable, another polynomial, or, more generally any expression, then P(a) denotes, by convention, the result of substituting a for x in P. Thus, the polynomial P defines the function\n\nwhich is the polynomial function associated to P.\nFrequently, when using this notation, one supposes that a is a number. However one may use it over any domain where addition and multiplication are defined (that is, any ring). In particular, if a is a polynomial then P(a) is also a polynomial.\n \nMore specifically, when a is the indeterminate x, then the image of x by this function is the polynomial P itself (substituting x to x does not change anything). In other words,\n\nwhich justifies formally the existence of two notations for the same polynomial.\n\nDefinition\nA polynomial is an expression that can be built from constants and symbols called indeterminates or variables by means of addition, multiplication and exponentiation to a non-negative integer power. Two such expressions that may be transformed, one to the other, by applying the usual properties of commutativity, associativity and distributivity of addition and multiplication are considered as defining the same polynomial.\n\nA polynomial in a single indeterminate x can always be written (or rewritten) in the form\n\nwhere  are constants and  is the indeterminate. The word \"indeterminate\" means that  represents no particular value, although any value may be substituted for it. The mapping that associates the result of this substitution to the substituted value is a function, called a polynomial function.\n\nThis can be expressed more concisely by using summation notation:\n\nThat is, a polynomial can either be zero or can be written as the sum of a finite number of non-zero terms. Each term consists of the product of a number—called the coefficient of the termThe coefficient of a term may be any number from a specified set. If that set is the set of real numbers, we speak of \"polynomials over the reals\". Other common kinds of polynomials are polynomials with integer coefficients, polynomials with complex coefficients, and polynomials with coefficients that are integers modulo of some prime number .—and a finite number of indeterminates, raised to nonnegative integer powers.\n\nClassification\n\nThe exponent on an indeterminate in a term is called the degree of that indeterminate in that term; the degree of the term is the sum of the degrees of the indeterminates in that term, and the degree of a polynomial is the largest degree of any one term with nonzero coefficient. Because , the degree of an indeterminate without a written exponent is one.\n\nA term with no indeterminates and a polynomial with no indeterminates are called, respectively, a constant term and a constant polynomial.This terminology dates from the time when the distinction was not clear between a polynomial and the function that it defines: a constant term and a constant polynomial define constant functions. The degree of a constant term and of a nonzero constant polynomial is 0. The degree of the zero polynomial, 0, (which has no terms at all) is generally treated as not defined (but see below).\n\nFor example:\n\nis a term. The coefficient is , the indeterminates are  and , the degree of  is two, while the degree of  is one. The degree of the entire term is the sum of the degrees of each indeterminate in it, so in this example the degree is .\n\nForming a sum of several terms produces a polynomial. For example, the following is a polynomial:\n\nIt consists of three terms: the first is degree two, the second is degree one, and the third is degree zero.\n\nPolynomials of small degree have been given specific names. A polynomial of degree zero is a constant polynomial or simply a constant. Polynomials of degree one, two or three are respectively linear polynomials, quadratic polynomials and cubic polynomials. For higher degrees the specific names are not commonly used, although quartic polynomial (for degree four) and quintic polynomial (for degree five) are sometimes used. The names for the degrees may be applied to the polynomial or to its terms. For example, in  the term  is a linear term in a quadratic polynomial.\n\nThe polynomial 0, which may be considered to have no terms at all, is called the zero polynomial. Unlike other constant polynomials, its degree is not zero. Rather the degree of the zero polynomial is either left explicitly undefined, or defined as negative (either −1 or −∞). These conventions are useful when defining Euclidean division of polynomials. The zero polynomial is also unique in that it is the only polynomial having an infinite number of roots. The graph of the zero polynomial, , is the -axis.\n\nIn the case of polynomials in more than one indeterminate, a polynomial is called homogeneous of  if all its non-zero terms have . The zero polynomial is homogeneous, and, as homogeneous polynomial, its degree is undefined.In fact, as homogeneous function, it is homogeneous of every degree For example,  is homogeneous of degree 5. For more details, see homogeneous polynomial.\n\nThe commutative law of addition can be used to rearrange terms into any preferred order. In polynomials with one indeterminate, the terms are usually ordered according to degree, either in \"descending powers of \", with the term of largest degree first, or in \"ascending powers of \". The polynomial in the example above is written in descending powers of . The first term has coefficient , indeterminate , and exponent . In the second term, the coefficient . The third term is a constant. Because the degree of a non-zero polynomial is the largest degree of any one term, this polynomial has degree two.\n\nTwo terms with the same indeterminates raised to the same powers are called \"similar terms\" or \"like terms\", and they can be combined, using the distributive law, into a single term whose coefficient is the sum of the coefficients of the terms that were combined. It may happen that this makes the coefficient 0. Polynomials can be classified by the number of terms with nonzero coefficients, so that a one-term polynomial is called a monomial,Some authors use \"monomial\" to mean \"monic monomial\". See  a two-term polynomial is called a binomial, and a three-term polynomial is called a trinomial. The term \"quadrinomial\" is occasionally used for a four-term polynomial.\n\nA real polynomial is a polynomial with real coefficients. The argument of the polynomial is not necessarily so restricted, for instance the s-plane variable in Laplace transforms. A real polynomial function is a function from the reals to the reals that is defined by a real polynomial. Similarly, an integer polynomial is a polynomial with integer coefficients, and a complex polynomial is a polynomial with complex coefficients.\n\nA polynomial in one indeterminate is called a univariate polynomial, a polynomial in more than one indeterminate is called a multivariate polynomial. A polynomial with two indeterminates is called a bivariate polynomial. These notions refer more to the kind of polynomials one is generally working with than to individual polynomials; for instance when working with univariate polynomials one does not exclude constant polynomials (which may result, for instance, from the subtraction of non-constant polynomials), although strictly speaking constant polynomials do not contain any indeterminates at all. It is possible to further classify multivariate polynomials as bivariate, trivariate, and so on, according to the maximum number of indeterminates allowed. Again, so that the set of objects under consideration be closed under subtraction, a study of trivariate polynomials usually allows bivariate polynomials, and so on. It is common, also, to say simply \"polynomials in , and \", listing the indeterminates allowed.\n\nThe evaluation of a polynomial consists of substituting a numerical value to each indeterminate and carrying out the indicated multiplications and additions. For polynomials in one indeterminate, the evaluation is usually more efficient (lower number of arithmetic operations to perform) using Horner's method:\n\nArithmetic\nPolynomials can be added using the associative law of addition (grouping all their terms together into a single sum), possibly followed by reordering, and combining of like terms. For example, if\n\nthen\n\nwhich can be simplified to\n\nTo work out the product of two polynomials into a sum of terms, the distributive law is repeatedly applied, which results in each term of one polynomial being multiplied by every term of the other. For example, if\n\nthen\n\nwhich can be simplified to\n\nPolynomial evaluation can be used to compute the remainder of polynomial division by a polynomial of degree one, because the remainder of the division of  by  is ; see the polynomial remainder theorem. This is more efficient than the usual algorithm of division when the quotient is not needed.\nA sum of polynomials is a polynomial.\nA product of polynomials is a polynomial.\nA composition of two polynomials is a polynomial, which is obtained by substituting a variable of the first polynomial by the second polynomial.\nThe derivative of the polynomial  is the polynomial . If the set of the coefficients does not contain the integers (for example if the coefficients are integers modulo some prime number ), then  should be interpreted as the sum of  with itself,  times. For example, over the integers modulo , the derivative of the polynomial  is the polynomial .\nA primitive integral or antiderivative of the polynomial  is the polynomial , where  is an arbitrary constant. For instance, the antiderivatives of  have the form .\n\nAs for the integers, two kinds of divisions are considered for the polynomials. The Euclidean division of polynomials that generalizes the Euclidean division of the integers. It results in two polynomials, a quotient and a remainder that are characterized by the following property of the polynomials: given two polynomials  and  such that , there exists a unique pair of polynomials, , the quotient, and , the remainder, such that  and  (here the polynomial zero is supposed to have a negative degree). By hand as well as with a computer, this division can be computed by the polynomial long division algorithm.Peter H. Selby, Steve Slavin, Practical Algebra: A Self-Teaching Guide, 2nd Edition, Wiley,  \n\nAll polynomials with coefficients in a unique factorization domain (for example, the integers or a field) also have a factored form in which the polynomial is written as a product of irreducible polynomials and a constant. This factored form is unique up to the order of the factors and their multiplication by an invertible constant. In the case of the field of complex numbers, the irreducible factors are linear. Over the real numbers, they have the degree either one or two. Over the integers and the rational numbers the irreducible factors may have any degree. For example, the factored form of\n\nis\n\nover the integers and the reals and\n\nover the complex numbers.\n\nThe computation of the factored form, called factorization is, in general, too difficult to be done by hand-written computation. However, efficient polynomial factorization algorithms are available in most computer algebra systems.\n\nA formal quotient of polynomials, that is, an algebraic fraction wherein the numerator and denominator are polynomials, is called a \"rational expression\" or \"rational fraction\" and is not, in general, a polynomial. Division of a polynomial by a number, however, yields another polynomial. For example,  is considered a valid term in a polynomial (and a polynomial by itself) because it is equivalent to  and  is just a constant. When this expression is used as a term, its coefficient is therefore . For similar reasons, if complex coefficients are allowed, one may have a single term like ; even though it looks like it should be expanded to two terms, the complex number  is one complex number, and is the coefficient of that term. The expression  is not a polynomial because it includes division by a non-constant polynomial. The expression  is not a polynomial, because it contains an indeterminate used as exponent.\n\nBecause subtraction can be replaced by addition of the opposite quantity, and because positive integer exponents can be replaced by repeated multiplication, all polynomials can be constructed from constants and indeterminates using only addition and multiplication.\n\nPolynomial functions\n\nA polynomial function is a function that can be defined by evaluating a polynomial. More precisely, a function  of one argument from a given domain is a polynomial function if there exists a polynomial\n\nthat evaluates to  for all  in the domain of  (here,  is a non-negative integer and  are constant coefficients).\n\nGenerally, unless otherwise specified, polynomial functions have complex coefficients, arguments, and values. In particular, a polynomial, restricted to have real coefficients, defines a function from the complex numbers to the complex numbers. If the domain of this function is also restricted to the reals, the resulting function maps reals to reals.\n\nFor example, the function , defined by\n\nis a polynomial function of one variable. Polynomial functions of several variables are similarly defined, using polynomials in more than one indeterminate, as in\n\nAccording to the definition of polynomial functions, there may be expressions that obviously are not polynomials but nevertheless define polynomial functions. An example is the expression  which takes the same values as the polynomial  on the interval , and thus both expressions define the same polynomial function on this interval.\n\nEvery polynomial function is continuous, smooth, and entire.\n\nGraphs\n\nA polynomial function in one real variable can be represented by a graph.\nThe graph of the zero polynomial\n\nis the -axis.\n\nThe graph of a degree 0 polynomial\n, where ,\nis a horizontal line with \n\nThe graph of a degree 1 polynomial (or linear function)\n , where ,\nis an oblique line with  and slope .\n\nThe graph of a degree 2 polynomial\n, where \nis a parabola.\n\nThe graph of a degree 3 polynomial\n, where \nis a cubic curve.\n\nThe graph of any polynomial with degree 2 or greater\n , where \nis a continuous non-linear curve.\n\nA non-constant polynomial function tends to infinity when the variable increases indefinitely (in absolute value). If the degree is higher than one, the graph does not have any asymptote. It has two parabolic branches with vertical direction (one branch for positive x and one for negative x).\n\nPolynomial graphs are analyzed in calculus using intercepts, slopes, concavity, and end behavior.\n\nEquations\n\nA polynomial equation, also called algebraic equation, is an equation of the form\n\nFor example,\n\nis a polynomial equation.\n\nWhen considering equations, the indeterminates (variables) of polynomials are also called unknowns, and the solutions are the possible values of the unknowns for which the equality is true (in general more than one solution may exist). A polynomial equation stands in contrast to a polynomial identity like , where both expressions represent the same polynomial in different forms, and as a consequence any evaluation of both members gives a valid equality.\n\nIn elementary algebra, methods such as the quadratic formula are taught for solving all first degree and second degree polynomial equations in one variable. There are also formulas for the cubic and quartic equations. For higher degrees, the Abel–Ruffini theorem asserts that there can not exist a general formula in radicals. However, root-finding algorithms may be used to find numerical approximations of the roots of a polynomial expression of any degree.\n\nThe number of real solutions of a polynomial equation with real coefficients may not exceed the degree, and equals the degree when the complex solutions are counted with their multiplicity. This fact is called the fundamental theorem of algebra.\n\nSolving equations\n\nEvery polynomial  in  defines a function  called the polynomial function associated to ; the equation  is the polynomial equation associated to . The solutions of this equation are called the roots of the polynomial, or the zeros of the associated function (they correspond to the points where the graph of the function meets the -axis).\n\nA number  is a root of a polynomial  if and only if the linear polynomial  divides , that is if there is another polynomial  such that . It may happen that  divides  more than once: if  divides  then  is called a multiple root of , and otherwise  is called a simple root of . If  is a nonzero polynomial, there is a highest power  such that  divides , which is called the multiplicity of the root  in . When  is the zero polynomial, the corresponding polynomial equation is trivial, and this case is usually excluded when considering roots, as, with the above definitions, every number is a root of the zero polynomial, with an undefined multiplicity. With this exception made, the number of roots of , even counted with their respective multiplicities, cannot exceed the degree of . \nThe relation between the coefficients of a polynomial and its roots is described by Vieta's formulas.\n\nSome polynomials, such as , do not have any roots among the real numbers. If, however, the set of accepted solutions is expanded to the complex numbers, every non-constant polynomial has at least one root; this is the fundamental theorem of algebra. By successively dividing out factors , one sees that any polynomial with complex coefficients can be written as a constant (its leading coefficient) times a product of such polynomial factors of degree 1; as a consequence, the number of (complex) roots counted with their multiplicities is exactly equal to the degree of the polynomial.\n\nThere may be several meanings of \"solving an equation\". One may want to express the solutions as explicit numbers; for example, the unique solution of  is . Unfortunately, this is, in general, impossible for equations of degree greater than one, and, since the ancient times, mathematicians have searched to express the solutions as algebraic expression; for example the golden ratio  is the unique positive solution of  In the ancient times, they succeeded only for degrees one and two. For quadratic equations, the quadratic formula provides such expressions of the solutions. Since the 16th century, similar formulas (using cube roots in addition to square roots), but much more complicated are known for equations of degree three and four (see cubic equation and quartic equation). But formulas for degree 5 and higher eluded researchers for several centuries. In 1824, Niels Henrik Abel proved the striking result that there are equations of degree 5 whose solutions cannot be expressed by a (finite) formula, involving only arithmetic operations and radicals (see Abel–Ruffini theorem). In 1830, Évariste Galois proved that most equations of degree higher than four cannot be solved by radicals, and showed that for each equation, one may decide whether it is solvable by radicals, and, if it is, solve it. This result marked the start of Galois theory and group theory, two important branches of modern algebra. Galois himself noted that the computations implied by his method were impracticable. Nevertheless, formulas for solvable equations of degrees 5 and 6 have been published (see quintic function and sextic equation).\n\nWhen there is no algebraic expression for the roots, and when such an algebraic expression exists but is too complicated to be useful, the unique way of solving is to compute numerical approximations of the solutions. There are many methods for that; some are restricted to polynomials and others may apply to any continuous function. The most efficient algorithms allow solving easily (on a computer) polynomial equations of degree higher than 1,000 (see Root-finding algorithm).\n\nFor polynomials in more than one indeterminate, the combinations of values for the variables for which the polynomial function takes the value zero are generally called zeros instead of \"roots\". The study of the sets of zeros of polynomials is the object of algebraic geometry. For a set of polynomial equations in several unknowns, there are algorithms to decide whether they have a finite number of complex solutions, and, if this number is finite, for computing the solutions. See System of polynomial equations.\n\nThe special case where all the polynomials are of degree one is called a system of linear equations, for which another range of different solution methods exist, including the classical Gaussian elimination.\n\nA polynomial equation for which one is interested only in the solutions which are integers is called a Diophantine equation. Solving Diophantine equations is generally a very hard task. It has been proved that there cannot be any general algorithm for solving them, and even for deciding whether the set of solutions is empty (see Hilbert's tenth problem). Some of the most famous problems that have been solved during the fifty last years are related to Diophantine equations, such as Fermat's Last Theorem.\n\nGeneralizations\n\nThere are several generalizations of the concept of polynomials.\n\nTrigonometric polynomials\n\nA trigonometric polynomial is a finite linear combination of functions sin(nx) and cos(nx) with n taking on the values of one or more natural numbers. The coefficients may be taken as real numbers, for real-valued functions.\n\nIf sin(nx) and cos(nx) are expanded in terms of sin(x) and cos(x), a trigonometric polynomial becomes a polynomial in the two variables sin(x) and cos(x) (using List of trigonometric identities#Multiple-angle formulae). Conversely, every polynomial in sin(x) and cos(x) may be converted, with Product-to-sum identities, into a linear combination of functions sin(nx) and cos(nx). This equivalence explains why linear combinations are called polynomials.\n\nFor complex coefficients, there is no difference between such a function and a finite Fourier series.\n\nTrigonometric polynomials are widely used, for example in trigonometric interpolation applied to the interpolation of periodic functions. They are used also in the discrete Fourier transform.\n\nMatrix polynomials\n\nA matrix polynomial is a polynomial with square matrices as variables. Given an ordinary, scalar-valued polynomial\n\nthis polynomial evaluated at a matrix A is\n\nwhere I is the identity matrix.\n\nA matrix polynomial equation is an equality between two matrix polynomials, which holds for the specific matrices in question. A matrix polynomial identity is a matrix polynomial equation which holds for all matrices A in a specified matrix ring Mn(R).\n\nLaurent polynomials\n\nLaurent polynomials are like polynomials, but allow negative powers of the variable(s) to occur.\n\nRational functions\n\nA rational fraction is the quotient (algebraic fraction) of two polynomials. Any algebraic expression that can be rewritten as a rational fraction is a rational function.\n\nWhile polynomial functions are defined for all values of the variables, a rational function is defined only for the values of the variables for which the denominator is not zero.\n\nThe rational fractions include the Laurent polynomials, but do not limit denominators to powers of an indeterminate.\n\nPower series\n\nFormal power series are like polynomials, but allow infinitely many non-zero terms to occur, so that they do not have finite degree. Unlike polynomials they cannot in general be explicitly and fully written down (just like irrational numbers cannot), but the rules for manipulating their terms are the same as for polynomials. Non-formal power series also generalize polynomials, but the multiplication of two power series may not converge.\n\nOther examples\nA bivariate polynomial where the second variable is substituted by an exponential function applied to the first variable, for example , may be called an exponential polynomial.\n\nApplications\n\nCalculus\n\nThe simple structure of polynomial functions makes them quite useful in analyzing general functions using polynomial approximations. An important example in calculus is Taylor's theorem, which roughly states that every differentiable function locally looks like a polynomial function, and the Stone–Weierstrass theorem, which states that every continuous function defined on a compact interval of the real axis can be approximated on the whole interval as closely as desired by a polynomial function.\n\nCalculating derivatives and integrals of polynomial functions is particularly simple. For the polynomial function\n\nthe derivative with respect to x is\n\nand the indefinite integral is\n\nAbstract algebra\n\nIn abstract algebra, one distinguishes between polynomials and polynomial functions. A polynomial  in one indeterminate  over a ring  is defined as a formal expression of the form\n\nwhere  is a natural number, the coefficients  are elements of , and  is a formal symbol, whose powers  are just placeholders for the corresponding coefficients , so that the given formal expression is just a way to encode the sequence , where there is an  such that  for all . Two polynomials sharing the same value of n are considered equal if and only if the sequences of their coefficients are equal; furthermore any polynomial is equal to any polynomial with greater value of  obtained from it by adding terms in front whose coefficient is zero. These polynomials can be added by simply adding corresponding coefficients (the rule for extending by terms with zero coefficients can be used to make sure such coefficients exist). Thus each polynomial is actually equal to the sum of the terms used in its formal expression, if such a term  is interpreted as a polynomial that has zero coefficients at all powers of  other than . Then to define multiplication, it suffices by the distributive law to describe the product of any two such terms, which is given by the rule\n\n for all elements a, b of the ring R and all natural numbers k and l.\n\nThus the set of all polynomials with coefficients in the ring  forms itself a ring, the ring of polynomials over , which is denoted by . The map from  to  sending  to  is an injective homomorphism of rings, by which  is viewed as a subring of . If  is commutative, then  is an algebra over .\n\nOne can think of the ring  as arising from  by adding one new element x to R, and extending in a minimal way to a ring in which  satisfies no other relations than the obligatory ones, plus commutation with all elements of  (that is ). To do this, one must add all powers of  and their linear combinations as well.\n\nFormation of the polynomial ring, together with forming factor rings by factoring out ideals, are important tools for constructing new rings out of known ones. For instance, the ring (in fact field) of complex numbers, which can be constructed from the polynomial ring  over the real numbers by factoring out the ideal of multiples of the polynomial . Another example is the construction of finite fields, which proceeds similarly, starting out with the field of integers modulo some prime number as the coefficient ring  (see modular arithmetic).\n\nIf  is commutative, then one can associate to every polynomial  in , a polynomial function  with domain and range equal to  (more generally one can take domain and range to be the same unital associative algebra over ). One obtains the value  by substitution of the value  for the symbol  in . One reason to distinguish between polynomials and polynomial functions is that over some rings different polynomials may give rise to the same polynomial function (see Fermat's little theorem for an example where  is the integers modulo ). This is not the case when  is the real or complex numbers, whence the two concepts are not always distinguished in analysis. An even more important reason to distinguish between polynomials and polynomial functions is that many operations on polynomials (like Euclidean division) require looking at what a polynomial is composed of as an expression rather than evaluating it at some constant value for .\n\nDivisibility\n\nIn commutative algebra, one major focus of study is divisibility among polynomials. If  is an integral domain and  and  are polynomials in , it is said that  divides  or  is a divisor of  if there exists a polynomial  in  such that . One can show that every zero gives rise to a linear divisor, or more formally, if  is a polynomial in  and  is an element of  such that , then the polynomial () divides . The converse is also true. The quotient can be computed using the polynomial long division.\n\nIf  is a field and  and  are polynomials in  with , then there exist unique polynomials  and  in  with\n\nand such that the degree of  is smaller than the degree of  (using the convention that the polynomial 0 has a negative degree). The polynomials  and  are uniquely determined by  and . This is called Euclidean division, division with remainder or polynomial long division and shows that the ring  is a Euclidean domain.\n\nAnalogously, prime polynomials (more correctly, irreducible polynomials) can be defined as non-zero polynomials which cannot be factorized into the product of two non-constant polynomials. In the case of coefficients in a ring, \"non-constant\" must be replaced by \"non-constant or non-unit\" (both definitions agree in the case of coefficients in a field). Any polynomial may be decomposed into the product of an invertible constant by a product of irreducible polynomials. If the coefficients belong to a field or a unique factorization domain this decomposition is unique up to the order of the factors and the multiplication of any non-unit factor by a unit (and division of the unit factor by the same unit). When the coefficients belong to integers, rational numbers or a finite field, there are algorithms to test irreducibility and to compute the factorization into irreducible polynomials (see Factorization of polynomials). These algorithms are not practicable for hand-written computation, but are available in any computer algebra system. Eisenstein's criterion can also be used in some cases to determine irreducibility.\n\nPositional notation\n\nIn modern positional numbers systems, such as the decimal system, the digits and their positions in the representation of an integer, for example, 45, are a shorthand notation for a polynomial in the radix or base, in this case, . As another example, in radix 5, a string of digits such as 132 denotes the (decimal) number  = 42. This representation is unique.  Let b be a positive integer greater than 1.  Then every positive integer a can be expressed uniquely in the form\n\nwhere m is a nonnegative integer and the r'''s are integers such that\n\n and  for .\n\nOther applications\n\nPolynomials serve to approximate other functions, such as the use of splines.\n\nPolynomials are frequently used to encode information about some other object. The characteristic polynomial of a matrix or linear operator contains information about the operator's eigenvalues. The minimal polynomial of an algebraic element records the simplest algebraic relation satisfied by that element. The chromatic polynomial of a graph counts the number of proper colourings of that graph.\n\nThe term \"polynomial\", as an adjective, can also be used for quantities or functions that can be written in polynomial form. For example, in computational complexity theory the phrase polynomial time means that the time it takes to complete an algorithm is bounded by a polynomial function of some variable, such as the size of the input.\n\nIn computer graphics they are used to interpolate between values to evaluate animation of dynamic graphical objects.\n\nHistory\n\nDetermining the roots of polynomials, or \"solving algebraic equations\", is among the oldest problems in mathematics. However, the elegant and practical notation we use today only developed beginning in the 15th century. Before that, equations were written out in words. For example, an algebra problem from the Chinese Arithmetic in Nine Sections, circa 200 BCE, begins \"Three sheafs of good crop, two sheafs of mediocre crop, and one sheaf of bad crop are sold for 29 dou.\" We would write .\n\nHistory of the notation\n\nThe earliest known use of the equal sign is in Robert Recorde's The Whetstone of Witte, 1557. The signs + for addition, − for subtraction, and the use of a letter for an unknown appear in Michael Stifel's Arithemetica integra, 1544. René Descartes, in La géometrie, 1637, introduced the concept of the graph of a polynomial equation. He popularized the use of letters from the beginning of the alphabet to denote constants and letters from the end of the alphabet to denote variables, as can be seen above, in the general formula for a polynomial in one variable, where the 's denote constants and  denotes a variable. Descartes introduced the use of superscripts to denote exponents as well.Howard Eves, An Introduction to the History of Mathematics, Sixth Edition, Saunders, \n\nSee also\nLill's method\nList of polynomial topics\nPolynomials on vector spaces\nPower series\nTable of mathematical expressions\nPolynomial transformations\nPolynomial mapping\nPolynomial functor\n\nNotes\n\nReferences\n\n. This classical book covers most of the content of this article.\n\nMayr, K. Über die Auflösung algebraischer Gleichungssysteme durch hypergeometrische Funktionen. Monatshefte für Mathematik und Physik vol. 45, (1937) pp. 280–313.\n \n\nUmemura, H. Solution of algebraic equations in terms of theta constants. In D. Mumford, Tata Lectures on Theta II'', Progress in Mathematics 43, Birkhäuser, Boston, 1984.\nvon Lindemann, F. Über die Auflösung der algebraischen Gleichungen durch transcendente Functionen. Nachrichten von der Königl. Gesellschaft der Wissenschaften, vol. 7, 1884. Polynomial solutions in terms of theta functions.\nvon Lindemann, F. Über die Auflösung der algebraischen Gleichungen durch transcendente Functionen II. Nachrichten von der Königl. Gesellschaft der Wissenschaften und der Georg-Augusts-Universität zu Göttingen, 1892 edition.\n\nExternal links\n\n \nCategory:Algebra"
    },
    {
      "title": "Polynomial arithmetic",
      "url": "https://en.wikipedia.org/wiki/Polynomial_arithmetic",
      "text": "Polynomial arithmetic is a branch of algebra dealing with some properties of polynomials which share strong analogies with properties of number theory relative to integers.\nIt includes basic mathematical operations such as addition, subtraction, and multiplication, as well as more elaborate operations like Euclidean division, and properties related to roots of polynomials. The latter are essentially connected to the fact that the set K[X] of univariate polynomials with coefficients in a field K is a commutative ring, such as the ring of integers .\n\nElementary operations on polynomials\nAddition and subtraction of two polynomials are performed by adding or subtracting corresponding coefficients. If\n\n \n\nthen addition is defined as\n\n   where m > n\n\nMultiplication is performed much the same way as addition and subtraction, but instead by multiplying the corresponding coefficients. If  then multiplication is defined as  where . Note that we treat  as zero for  and that the degree of the product is equal to the sum of the degrees of the two polynomials.\n\nAdvanced polynomial arithmetics and comparison with number theory\nMany fascinating properties of polynomials can be found when, thanks to the basic operations that can be performed on two polynomials and the underlying commutative ring structure of the set they live in, one tries to apply reasonings similar to those known from number theory.\n\nTo see this, one first needs to introduce two concepts: the notion of root of a polynomial and that of divisibility for pairs of polynomials. \n\nIf one considers a polynomial  of a single variable X in a field K (typically  or ), and with coefficients in that field, a root  of  is an element of K such that\n\n \n\nThe second concept, divisibility of polynomials, allows to see a first analogy with number theory: a polynomial  is said to divide another polynomial  when the latter can be written as\n\n \n\nwith C being ALSO a polynomial. This definition is similar to divisibility for integers, and the fact that  divides  is also denoted .\n\nThe relation between both concepts above arises when noticing the following property:  is a root of  if and only if . Whereas one logical inclusion (\"if\") is obvious, the other (\"only if\") relies on a more elaborate concept, the Euclidean division of polynomials, here again strongly reminding of the Euclidean division of integers.\n\nFrom this it follows that one can define prime polynomials, as polynomials that cannot be divided by any other polynomials but 1 and themselves (up to an overall constant factor) - here again the analogously with prime integers is manifest, and allows that some of the main definitions and theorems related to prime numbers and number theory have their counterpart in polynomial algebra. The most important result is the fundamental theorem of algebra, allowing for factorization of any polynomial as a product of prime ones. Worth mentioning is also the Bézout's identity in the context of polynomials. It states that two given polynomials P and Q have as greatest common divisor (GCD) a third polynomial D (D is then unique as GCD of P and Q up to a finite constant factor), if and only if there exists polynomials U and V such that\n .\n\nSee also\n Polynomial long division\n Polynomial greatest common divisor\n\nReferences\n Stallings, William; : \"Cryptography And Network Security: Principles and Practice\", pages 121-126. Prentice Hall, 1999.\n\nExternal links\nJ.A. Beachy and W.D. Blair; : \"Polynomials\", from \"Abstract algebra\", 2nd edition, 1996.\n\nCategory:Polynomials\nCategory:Algebra"
    },
    {
      "title": "Polynomial Diophantine equation",
      "url": "https://en.wikipedia.org/wiki/Polynomial_Diophantine_equation",
      "text": "In mathematics, a polynomial Diophantine equation is an indeterminate polynomial equation for which one seeks solutions restricted to be polynomials in the indeterminate. A Diophantine equation, in general, is one where the solutions are restricted to some algebraic system, typically integers.  (In another usage )  Diophantine refers to the Hellenistic mathematician of the 3rd century, Diophantus of Alexandria, who made initial studies of integer Diophantine equations.\n\nAn important type of polynomial Diophantine equations takes the form:\n\nwhere a, b, and c are known polynomials, and we wish to solve for s and t.\n\nA simple example (and a solution) is:\n\nA necessary and sufficient condition for a polynomial Diophantine equation to have a solution is for c to be a multiple of the GCD of a and b.  In the example above, the GCD of a and b was 1, so solutions would exist for any value of c.\n\nSolutions to polynomial Diophantine equations are not unique.  Any multiple of  (say ) can be used to transform  and  into another solution  :\n\nSome polynomial Diophantine equations can be solved using the extended Euclidean algorithm, which works as well with polynomials as it does with integers.\n\nReferences\n\nCategory:Algebra"
    },
    {
      "title": "Polynomial mapping",
      "url": "https://en.wikipedia.org/wiki/Polynomial_mapping",
      "text": "In algebra, a polynomial mapping  between vector spaces over an infinite field k is a polynomial in linear functionals with coefficients in W; i.e., it can be written as\n\nwhere  are linear functionals. For example, if , then it can also be expressed as  where  are (scalar-valued) polynomial functions on V.\n\nWhen V, W are finite-dimensional vector spaces and are viewed as algebraic varieties, then a polynomial mapping is precisely a morphism of algebraic varieties.\n\nOne fundamental outstanding question regarding polynomial mapping is the Jacobian conjecture, which concerns the sufficiency of a polynomial mapping to be invertible.\n\n See also \nPolynomial functor\n\n References \nClaudio Procesi (2007) Lie Groups: an approach through invariants and representation, Springer, .\n\nCategory:Algebra"
    },
    {
      "title": "Polynomial transformation",
      "url": "https://en.wikipedia.org/wiki/Polynomial_transformation",
      "text": "In mathematics, a polynomial transformation consists of computing the polynomial whose roots are a given function of the roots of polynomial. Polynomial transformations such as Tschirnhaus transformations are often used to simplify the solution of algebraic equations.\n\nSimple examples\n\nTranslating the roots\nLet \n\nbe a polynomial, and \n \nbe its complex roots (not necessarily distinct).\n\nFor any constant , the polynomial whose roots are \n \nis \n\nIf the coefficients of  are integers and the constant  is a rational number, the coefficients of  may be not integer, but the polynomial  has integers coefficients and has the same roots as .\n\nA special case is when  The resulting polynomial  does not have any term in .\n\nReciprocals of the roots\nLet \n\nbe a polynomial. The polynomial whose roots are the reciprocals of the roots of  as roots is its reciprocal polynomial\n\nScaling the roots\n\nLet \n\nbe a polynomial, and  be a non-zero constant. A polynomial whose roots are the product by  of the roots of  is \n\nThe factor  appears here because, if  and the coefficients of  are integers or belong to some integral domain, the same is true for the coefficients of .\n\nIn the special case where , all coefficients of  are multiple of , and  is a monic polynomial, whose coefficients belong to any integral domain containing  and the coefficients of . This polynomial transformation is often used to reduce questions on algebraic numbers to questions on algebraic integers.\n\nCombining this with a translation of the roots by , allows to reduce any question on the roots of a polynomial, such as root-finding, to a similar question on a simpler polynomial, which is monic and does not have a term of degree . For examples of this, see Cubic function § Reduction to a depressed cubic or Quartic function § Converting to a depressed quartic.\n\nTransformation by a rational function\nAll preceding examples are polynomial transformations by a rational function, also called Tschirnhaus transformations. Let \n\nbe a rational function, where  and  are coprime polynomials. The polynomial transformation of a polynomial  by  is the polynomial  (defined up to the product by a non-zero constant) whose roots are the images by  of the roots of .\n\nSuch a polynomial transformation may be computed as a resultant. In fact, the roots of the desired polynomial  are exactly the complex numbers  such that there is a complex number  such one has simultaneously (if the coefficients of  and  are not real or complex numbers, \"complex number\" has to be replaced by \"element of an algebraically closed field containing the coefficients of the input polynomials\")\n\nThis is exactly the defining property of the resultant\n\nThis is generally difficult to compute this through a hand-written computation. However, as most computer algebra systems have a built-in function to compute resultants, it is straightforward to compute it with a computer.\n\nProperties\nIf the polynomial  is irreducible, then either the resulting polynomial  is irreducible, or it is a power of an irreducible polynomial. Let  be a root of  and consider , the field extension generated by . The former case means that  is a primitive element of , which has  as minimal polynomial. In the latter case,  belongs to a subfield of  and its minimal polynomial is the irreducible polynomial that has  as power.\n\nTransformation for equation-solving\n\nPolynomial transformations have been applied to the simplification of polynomial equations for solution, where possible, by radicals.  Descartes introduced the transformation of a polynomial of degree d which eliminates the term of degree d−1 by a translation of the roots.  Such a polynomial is termed depressed. this already suffices to solve the quadratic by square roots.   In the case of the cubic, Tschirnhaus transformations replace the variable by a quadratic function, thereby making it possible to eliminate two terms, and so can be used to eliminate the linear term in a depressed cubic to achieve the solution of the cubic by a combination of square and cube roots.  The Bring–Jerrard transformation, which is quartic in the variable, brings a quintic into \"principal\" or Bring-Jerrard normal form with terms of degree 5,1 and zero.\n\nReferences\n \n\nCategory:Algebra"
    },
    {
      "title": "Power set",
      "url": "https://en.wikipedia.org/wiki/Power_set",
      "text": "thumb|250px|The elements of the power set of the set {x, y, z}  ordered with respect to inclusion.\n\nIn mathematics, the power set (or powerset) of any set  is the set of all subsets of , including the empty set and  itself, variously denoted as (), 𝒫(), ℘() (using the \"Weierstrass p\"), , , or, identifying the powerset of  with the set of all functions from  to a given set of two elements, . In axiomatic set theory (as developed, for example, in the ZFC axioms), the existence of the power set of any set is postulated by the axiom of power set.\n\nAny subset of () is called a family of sets over .\n\nExample\nIf  is the set , then the subsets of  are\n\n  (also denoted  or , the  empty set or the null set)\n \n \n \n \n \n \n \nand hence the power set of  is .\n\nProperties\nIf  is a finite set with  elements, then the number of subsets of  is . This fact, which is the motivation for the notation , may be demonstrated simply as follows,\n First, order the elements of  in any manner. We write any subset of  in the format } where , can take the value of  or . If , the -th element of  is in the subset; otherwise, the -th element is not in the subset. Clearly the number of distinct subsets that can be constructed this way is  as .\n\nCantor's diagonal argument shows that the power set of a set (whether infinite or not) always has strictly higher cardinality than the set itself (informally the power set must be larger than the original set). In particular, Cantor's theorem shows that the power set of a countably infinite set is uncountably infinite. The power set of the set of natural numbers can be put in a one-to-one correspondence with the set of real numbers (see Cardinality of the continuum).\n\nThe power set of a set , together with the operations of union, intersection and complement can be viewed as the prototypical example of a Boolean algebra. In fact, one can show that any finite Boolean algebra is isomorphic to the Boolean algebra of the power set of a finite set. For infinite Boolean algebras this is no longer true, but every infinite Boolean algebra can be represented as a subalgebra of a power set Boolean algebra (see Stone's representation theorem).\n\nThe power set of a set  forms an abelian group when considered with the operation of symmetric difference (with the empty set as the identity element and each set being its own inverse) and a commutative monoid when considered with the operation of intersection. It can hence be shown (by proving the distributive laws) that the power set considered together with both of these operations forms a Boolean ring.\n\nRepresenting subsets as functions \nIn set theory,  is the set of all functions from  to .  As \"2\" can be defined as  (see natural number),  (i.e., ) is the set of all functions from  to {0,1}.  By identifying a function in  with the corresponding preimage of , we see that there is a bijection between  and (), where each function is the characteristic function of the subset in () with which it is identified.  Hence  and () could be considered identical set-theoretically.  (Thus there are two distinct notational motivations for denoting the power set by :  the fact that this function-representation of subsets makes it a special case of the  notation and the property, mentioned above, that .)\n\nThis notion can be applied to the example above in which  to see the isomorphism with the binary numbers\nfrom 0 to  with  being the number of elements in the set.\nIn , a \"1\" in the position corresponding to the location in the enumerated set  indicates the presence of the element. So .\n\nFor the whole power set of  we get:\n Subset Sequence of digits Binary interpretation Decimal equivalent                                                        \n\nSuch bijective mapping of S to integers is arbitrary, so this representation of subsets of S is not unique, but the sort order of the enumerated set does not change its cardinality.\n\nHowever, such finite binary representation is only possible if S can be enumerated (this is possible even if S has an infinite cardinality, such as the set of integers or rationals, but not for example if S is the set of real numbers, in which we cannot enumerate all irrational numbers to assign them a defined finite location in an ordered set containing all irrational numbers).\n\nRelation to binomial theorem\nThe power set is closely related to the binomial theorem.  The number of subsets with  elements in the power set of a set with  elements is given by the number of combinations, , also called binomial coefficients.\n\nFor example, the power set of a set with three elements, has:\n\nC(3, 0) = 1 subset with 0 elements (the empty subset),\nC(3, 1) = 3 subsets with 1 element (the singleton subsets),\nC(3, 2) = 3 subsets with 2 elements (the complements of the singleton subsets),\nC(3, 3) = 1 subset with 3 elements (the original set itself).\n\nUsing this relationship we can compute  using the formula:\n\nTherefore one can deduce the following identity, assuming :\n\nAlgorithms\nIf  is a finite set, there is a recursive algorithm to calculate ().\n\nDefine the operation }.\n\nIn English, return the set with the element  added to each set  in .\n\nIf , then  is returned.\nOtherwise:\nLet  be any single element of .\nLet , where  denotes the relative complement of  in .\nAnd the result:  is returned.\n\nIn other words, the power set of the empty set is the set containing the empty set and the power set of any other set is all the subsets of the set containing some specific element and all the subsets of the set not containing that specific element.\n\nSubsets of limited cardinality\nThe set of subsets of  of cardinality less than or equal to  is sometimes denoted by  or , and the set of subsets with cardinality strictly less than  is sometimes denoted  or . Similarly, the set of non-empty subsets of  might be denoted by  or .\n\nPower object\nA set can be regarded as an algebra having no nontrivial operations or defining equations.  From this perspective the idea of the power set of  as the set of subsets of  generalizes naturally to the subalgebras of an algebraic structure or algebra.\n\nNow the power set of a set, when ordered by inclusion, is always a complete atomic Boolean algebra, and every complete atomic Boolean algebra arises as the lattice of all subsets of some set.  The generalization to arbitrary algebras is that the set of subalgebras of an algebra, again ordered by inclusion, is always an algebraic lattice, and every algebraic lattice arises as the lattice of subalgebras of some algebra.  So in that regard subalgebras behave analogously to subsets.\n\nHowever, there are two important properties of subsets that do not carry over to subalgebras in general.  First, although the subsets of a set form a set (as well as a lattice), in some classes it may not be possible to organize the subalgebras of an algebra as itself an algebra in that class, although they can always be organized as a lattice.  Secondly, whereas the subsets of a set are in bijection with the functions from that set to the set {0,1} = 2, there is no guarantee that a class of algebras contains an algebra that can play the role of 2 in this way.\n\nCertain classes of algebras enjoy both of these properties.  The first property is more common, the case of having both is relatively rare.  One class that does have both is that of multigraphs.  Given two multigraphs  and , a homomorphism  consists of two functions, one mapping vertices to vertices and the other mapping edges to edges.  The set  of homomorphisms from  to  can then be organized as the graph whose vertices and edges are respectively the vertex and edge functions appearing in that set.  Furthermore, the subgraphs of a multigraph  are in bijection with the graph homomorphisms from  to the multigraph  definable as the complete directed graph on two vertices (hence four edges, namely two self-loops and two more edges forming a cycle) augmented with a fifth edge, namely a second self-loop at one of the vertices.  We can therefore organize the subgraphs of  as the multigraph , called the power object of .\n\nWhat is special about a multigraph as an algebra is that its operations are unary.  A multigraph has two sorts of elements forming a set  of vertices and  of edges, and has two unary operations  giving the source (start) and target (end) vertices of each edge.  An algebra all of whose operations are unary is called a presheaf.  Every class of presheaves contains a presheaf  that plays the role for subalgebras that 2 plays for subsets.  Such a class is a special case of the more general notion of elementary topos as a category that is closed (and moreover cartesian closed) and has an object , called a subobject classifier.  Although the term \"power object\" is sometimes used synonymously with exponential object , in topos theory  is required to be .\n\nFunctors and quantifiers\nIn category theory and the theory of elementary topoi, the universal quantifier can be understood as the right adjoint of a functor between power sets, the inverse image functor of a function between sets; likewise, the existential quantifier is the left adjoint.Saunders Mac Lane, Ieke Moerdijk, (1992) Sheaves in Geometry and Logic Springer-Verlag.  See page 58\n\nSee also\n Set theory\n Axiomatic set theory\n Family of sets\n Field of sets\n\nNotes\n\nReferences\n \n \n \n\nExternal links\n\n \n \n Power set Algorithm in C++\n\nCategory:Abstract algebra\nCategory:Algebra\nCategory:Basic concepts in set theory"
    },
    {
      "title": "Prime avoidance lemma",
      "url": "https://en.wikipedia.org/wiki/Prime_avoidance_lemma",
      "text": "In algebra, the prime avoidance lemma says that if an ideal I in a commutative ring R is contained in a union of finitely many prime ideals Pi's, then it is contained in Pi for some i.\n\nThere are many variations of the lemma (cf. Hochster); for example, if the ring R contains an infinite field or a finite field of sufficiently large cardinality, then the statement follows from a fact in linear algebra that a vector space over an infinite field or a finite field of large cardinality is not a finite union of its proper vector subspaces.Proof of the fact: suppose the vector space is a finite union of proper subspaces. Consider a finite product of linear functionals, each of which vanishes on a proper subspace that appears in the union; then it is a nonzero polynomial vanishing identically, a contradiction.\n\n Statement and proof \nThe following statement and argument are perhaps the most standard.\n\nStatement: Let E be a subset of R that is an additive subgroup of R and is multiplicatively closed. Let  be ideals such that  are prime ideals for . If E is not contained in any of 's, then E is not contained in the union .\n\nProof by induction on n: The idea is to find an element that is in E and not in any of 's. The basic case n = 1 is trivial. Next suppose n ≥ 2. For each i choose\n\nwhere the set on the right is nonempty by inductive hypothesis. We can assume  for all i; otherwise, some  avoids  all the 's and we are done. Put\n.\nThen z is in E but not in any of 's. Indeed, if z is in  for some , then  is in , a contradiction. Suppose z is in . Then   is in . If n is 2, we are done. If n > 2, then, since  is a prime ideal, some  is in , a contradiction.\n\n Notes \n\n References \nMel Hochster, Dimension theory and systems of parameters, a supplementary note\n\nCategory:Algebra"
    },
    {
      "title": "Primitive part and content",
      "url": "https://en.wikipedia.org/wiki/Primitive_part_and_content",
      "text": "In algebra, the content of a polynomial with integer coefficients (or, more generally,  with coefficients in a unique factorization domain) is the greatest common divisor of its coefficients. The primitive part of such a polynomial is the quotient of the polynomial by its content. Thus a polynomial is the product of its primitive part and its content, and this factorization is unique up to the multiplication of the content by a unit of the ring of the coefficients (and the multiplication of the primitive part by the inverse of the unit).\n\nA polynomial is primitive if its content equals 1. Thus the primitive part of a polynomial is a primitive polynomial.\n\nGauss's lemma for polynomials states that the product of primitive polynomials (with coefficients in the same unique factorization domain) also is primitive. This implies that the content and the primitive part of the product of two polynomials are, respectively, the product of the contents and the product of the primitive parts.\n\nAs the computation of greatest common divisors is generally much easier than polynomial factorization, the first step of a polynomial factorization algorithm is generally the computation of its primitive part–content factorization (see ). Then the factorization problem is reduced to factorize separately the content and the primitive part.\n\nContent and primitive part may be generalized to polynomials over the rational numbers, and, more generally, to polynomials over the field of fractions of a unique factorization domain. This makes essentially equivalent the problems of computing greatest common divisors and factorization of polynomials over the integers and of polynomials over the rational numbers.\n\nOver the integers\nFor a polynomial with integer coefficients, the content may be either the greatest common divisor of the coefficients or its additive inverse. The choice is arbitrary, and may depend on a further convention, which is commonly that the leading coefficient of the primitive part be positive.\n\nFor example, the content of  may be either 2 or –2, since 2 is the greatest common divisor of –12, 30, and -20. If one chooses 2 as the content, the primitive part of this polynomial is \n\nand thus the primitive–part–content factorization is\n\nFor aesthetic reasons, one often prefers choosing a negative content, here –2, giving the primitive–part–content factorization\n\nProperties\nIn the remainder of this article, we consider polynomials over a unique factorization domain , which can typically be the ring of integers, or a polynomial ring over a field. In , greatest common divisors are well defined, and are unique up to the multiplication by a unit of .\n\nThe content  of a polynomial  with coefficients in  is the greatest common divisor of its coefficients, and, as such, is defined up to the multiplication by a unit. The primitive part  of  is the quotient  of  by its content; it is a polynomial with coefficients in , which is unique up to the multiplication by a unit. If the content is changed by multiplication by a unit , then the primitive part must be changed by dividing it by the same unit, in order to keep the equality\n\nwhich is called the primitive-part-content factorization of .\n\nThe main properties of the content and the primitive part are results of Gauss's lemma, which asserts that the product of two primitive polynomials is primitive, where a polynomial is primitive if 1 is the greatest common divisor of its coefficients. This implies:\nThe content of a product of polynomials is the product of their contents:\n\nThe primitive part of a product of polynomials is the product of their primitive parts:\n\nThe content of a greatest common divisor of polynomials is the greatest common divisor (in ) of their contents:\n\nThe primitive part of a greatest common divisor of polynomials is the greatest common divisor (in ) of their primitive parts:\n\nThe complete factorization of a polynomial over  is the product of the factorization (in ) of the content and of the factorization (in the polynomial ring) of the primitive part.\n\nThe last property implies that the computation of the primitive-part-content factorization of a polynomial reduces the computation of its complete factorization to the separate factorization of the content and the primitive part. This is generally interesting, because the computation of the prime-part-content factorization involves only greatest common divisor computation in , which is usually much easier than factorization.\n\nOver the rationals\nThe primitive-part-content factorization may be extended to polynomials with rational coefficients as follows.\n\nGiven a polynomial  with rational coefficients, by rewriting its coefficients with the same common denominator , one may rewrite  as\n\nwhere  is a polynomial with integer coefficients.\nThe content of  is the quotient by  of the content of , that is \n\nand the primitive part of  is the primitive part of :\n\nIt is easy to show that this definition does not depend on the choice of the common denominator, and that the primitive-part-content factorization remains valid:\n\nThis shows that every polynomial over the rationals is associated with a unique primitive polynomial over the integers, and that the Euclidean algorithm allows the computation of this primitive polynomial.\n\nA consequence is that factoring polynomials over the rationals is equivalent to factoring primitive polynomials over the integers. As polynomials with coefficients in a field are more common than polynomials with integer coefficients, it may seem that this equivalence may be used for factoring polynomials with integer coefficients. In fact, the truth is exactly the opposite: every known efficient algorithm for factoring polynomials with rational coefficient uses this equivalence for reducing the problem modulo some prime number  (see Factorization of polynomials).\n\nThis equivalence is also used for computing greatest common divisors of polynomials, although the Euclidean algorithm is defined for polynomials with rational coefficients. In fact, in this case, the Euclidean algorithm requires one to compute the reduced form of many fractions, and this makes the Euclidean algorithm less efficient than algorithms which work only with polynomials over the integers (see Polynomial greatest common divisor).\n\nOver a field of fractions\nThe results of the preceding section remain valid if the ring of integers and the field of rationals are respectively replaced by any unique factorization domain  and its field of fractions .\n\nThis is typically used for factoring multivariate polynomials, and for proving that a polynomial ring over a unique factorization domain is also a unique factorization domain.\n\nUnique factorization property of polynomial rings\n\nA polynomial ring over a field is a unique factorization domain. The same is true for a polynomial ring over a unique factorization domain. To prove this, it suffices to consider the univariate case, as the general case may be deduced by a recurrence on the number of indeterminates.\n\nThe unique factorization property is a direct consequence of Euclid's lemma: If an irreducible element divides a product, then it divides one of the factors.  For univariate polynomials over a field, this results from Bézout's identity, which itself results from Euclidean algorithm.\n\nSo, let  be a unique factorization domain, which is not a field, and  the univariate polynomial ring over . An irreducible element  in  is either an irreducible element in  or an irreducible primitive polynomial.\n\nIf  is in  and divides a product  of two polynomials, then it divides the content  Thus, by Euclid's lemma in , it divides one of the contents, and therefore one of the polynomials.\n\nIf  is not , it is a primitive polynomial (because it is irreducible). Then Euclid's lemma in  results immediately from Euclid's lemma in , where  is the field of fractions of .\n\nFactorization of multivariate polynomials\n\nFor factoring a multivariate polynomial over a field or over the integers, one may consider it as a univariate polynomial with coefficients in a polynomial ring with one less indeterminate. Then the factorization is reduced to factorizing separately the primitive part and the content. As the content has one less indeterminate, it may be factorized by applying the method recursively. For factorizing the primitive part, the standard method consists of substituting integers to the indeterminates of the coefficients in a way that does not change the degree in the remaining variable, factorizing the resulting univariate polynomial, and lifting the result to a factorization of the primitive part.\n\nSee also\nRational root theorem\n\nReferences\n \n Page 181 of \n \n\nCategory:Algebra\nCategory:Polynomials"
    },
    {
      "title": "Primordial element (algebra)",
      "url": "https://en.wikipedia.org/wiki/Primordial_element_%28algebra%29",
      "text": "In algebra, a primordial element is a particular kind of a vector in a vector space. Let V be a vector space over a field k and fix a basis for V of vectors  for . By the definition of a basis, every vector v in V can be expressed uniquely as\n\nDefine , the set of indices for which the expression of v has a nonzero coefficient. Given a subspace W of V, a nonzero vector w in W is said to be \"primordial\" if it has the following two properties:Milne, J., Class field theory course notes, updated March 23, 2013, Ch IV, §2.\n is minimal among the sets ,  and\n for some i\n\n References \n\nCategory:Algebra"
    },
    {
      "title": "Prosolvable group",
      "url": "https://en.wikipedia.org/wiki/Prosolvable_group",
      "text": "In mathematics, more precisely in algebra, a prosolvable group (less common: prosoluble group) is a group that is isomorphic to the inverse limit of an inverse system of solvable groups. Equivalently, a group is called prosolvable, if, viewed as a topological group, every open neighborhood of the identity contains a normal subgroup whose corresponding quotient group is a solvable group.\n\n Examples \n Let p be a prime, and denote the field of p-adic numbers, as usually, by . Then the Galois group , where  denotes the algebraic closure of , is prosolvable. This follows from the fact that, for any finite Galois extension  of , the Galois group  can be written as semidirect product , with  cyclic of order  for some ,  cyclic of order dividing , and  of -power order. Therefore,  is solvable.\n\n See also \n Galois theory\n\nReferences\n\nCategory:Mathematical structures\nCategory:Algebra\nCategory:Number theory\nCategory:Topology\nCategory:Properties of groups\nCategory:Topological groups"
    },
    {
      "title": "Quadratic-linear algebra",
      "url": "https://en.wikipedia.org/wiki/Quadratic-linear_algebra",
      "text": "In mathematics, a quadratic-linear algebra is an algebra over a field with a presentation such that all relations are sums of monomials of degrees 1 or 2 in the generators. They were introduced by . An example is the universal enveloping algebra of a Lie algebra, with generators a basis of the Lie algebra and relations of the form XY – YX – [X, Y] = 0.\n\nReferences\n\nCategory:Algebra"
    },
    {
      "title": "Quasi-unmixed ring",
      "url": "https://en.wikipedia.org/wiki/Quasi-unmixed_ring",
      "text": "In algebra, specifically in the theory of commutative rings, a quasi-unmixed ring (also called a formally equidimensional ring in EGA) is a Noetherian ring  such that for each prime ideal p, the completion of the localization Ap is equidimensional, i.e. for each minimal prime ideal q in the completion ,  = the Krull dimension of Ap.\n\nEquivalent conditions\nA Noetherian integral domain is quasi-unmixed if and only if it satisfies Nagata's altitude formula. (See also: #formally catenary ring below.)\n\nPrecisely, a quasi-unmixed ring is a ring in which the unmixed theorem, which characterizes a Cohen–Macaulay ring, holds for integral closure of an ideal; specifically, for a Noetherian ring , the following are equivalent:\n is quasi-unmixed.\nFor each ideal I generated by a number of elements equal to its height, the integral closure  is unmixed in height (each prime divisor has the same height as the others).\nFor each ideal I generated by a number of elements equal to its height and for each integer n > 0,  is unmixed.\n\n Formally catenary ring \nA Noetherian local ring  is said to be formally catenary if for every prime ideal ,  is quasi-unmixed. As it turns out, this notion is redundant: Ratliff has shown that a Noetherian local ring is formally catenary if and only if it is universally catenary.L. J. Ratliff, Jr., Characterizations of catenary rings, Amer. J. Math. 93 (1971)\n\nReferences\n\nAppendix of Stephen McAdam, Asymptotic Prime Divisors. Lecture notes in Mathematics.\nL.J Ratliff Jr., Locally quasi-unmixed Noetherian rings and ideals of the principal class Pacific J. Math., 52 (1974), pp. 185–205\n\nFurther reading\nHerrmann, M., S. Ikeda, and U. Orbanz: Equimultiplicity and Blowing Up. An Algebraic Study with an Appendix by B. Moonen. Springer Verlag, Berlin Heidelberg New-York, 1988.\n\nCategory:Algebra\nCategory:Commutative algebra"
    },
    {
      "title": "Ratio",
      "url": "https://en.wikipedia.org/wiki/Ratio",
      "text": "thumb|The ratio of width to height of standard-definition television\nIn mathematics, a ratio is a relationship between two numbers indicating how many times the first number contains the second.Penny Cyclopedia, p. 307 For example, if a bowl of fruit contains eight oranges and six lemons, then the ratio of oranges to lemons is eight to six (that is, 8:6, which is equivalent to the ratio 4:3). Similarly, the ratio of lemons to oranges is 6:8 (or 3:4) and the ratio of oranges to the total amount of fruit is 8:14 (or 4:7).\n\nThe numbers in a ratio may be quantities of any kind, such as counts of persons or objects, or such as measurements of lengths, weights, time, etc. In most contexts both numbers are restricted to be positive.\n\nA ratio may be specified either by giving both constituting numbers, written as \"a to b\" or \"a:b\", or by giving just the value of their quotient ,New International Encyclopedia since the product of the quotient and the second number yields the first, as required by the above definition.\n\nConsequently, a ratio may be considered as an ordered pair of numbers, as a fraction with the first number in the numerator and the second as denominator, or as the value denoted by this fraction. Ratios of counts, given by (non-zero) natural numbers, are rational numbers, and may sometimes be natural numbers. When two quantities are measured with the same unit, as is often the case, their ratio is a dimensionless number. A quotient of two quantities that are measured with different units is called a rate.\"The quotient of two numbers (or quantities); the relative sizes of two numbers (or quantities)\", \"The Mathematics Dictionary\" \n\n Notation and terminology \nThe ratio of numbers A and B can be expressed as:New International Encyclopedia\nthe ratio of A to B\nA is to B (often followed by \"as C is to D\")\nA∶B\na fraction with A as numerator and B as denominator, that represents the quotient: A divided by B: . This can be expressed as a simple or a decimal fraction, or as a percentage, etc.Decimal fractions are frequently used in technological areas where ratio comparisons are important, such as aspect ratios (imaging), compression ratios (engines or data storage), etc.\n\nThere are several variants in the same vein as percentage, with the meaning of per hundred, equivalent to  for example, the per mill from Latin mille, meaning thousand, equivalent to ‰  or, especially for particle densities, parts per billion, abbreviated as ppb. There is also a specific unicode sign, the per sign, looking like ⅌, for the use of arbitrary denominators, e.g., ⅌ 12 as per dozen.\n\nThe numbers A and B are sometimes called terms of the ratio with A being the antecedent and B being the consequent.from the Encyclopædia Britannica\n\nA statement expressing the equality of two ratios A∶B and C∶D is called a proportion, written as A∶B = C∶D or A∶B::C∶D. This latter form, when spoken or written in the English language, is often expressed as\n(A is to B) as (C is to D).\n\nA, B, C and D are called the terms of the proportion. A and D are called its extremes, and B and C are called its means. The equality of three or more ratios, like A∶B = C∶D = E∶F, is called a continued proportion.New International Encyclopedia\n\nRatios are sometimes used with three or even more terms, e.g., the proportion for the edge lengths of a \"two by four\" that is ten inches long is therefore\n\n(Note: Unplaned measurements)\n\na good concrete mix (in volume units) is sometimes quoted as\nBelle Group concrete mixing hints\n\nFor a (rather dry) mixture of 4/1 parts in volume of cement to water, it could be said that the ratio of cement to water is 4∶1, that there is 4 times as much cement as water, or that there is a quarter (1/4) as much water as cement.\n\nThe meaning of such a proportion of ratios with more than two terms is that the ratio of any two terms on the LHS makes up a standard proportion with the corresponding two terms on the RHS. The corresponding terms are called the homologues in the proportion.\n\nHistory and etymology\nIt is impossible to trace the origin of the concept of ratio because the ideas from which it developed would have been familiar to preliterate cultures. For example, the idea of one village being twice as large as another is so basic that it would have been understood in prehistoric society.Smith, p. 477 However, it is possible to trace the origin of the word \"ratio\" to the Ancient Greek  (logos). Early translators rendered this into Latin as  (\"reason\"; as in the word \"rational\"). (A rational number may be expressed as the quotient of two integers.) A more modern interpretation of Euclid's meaning is more akin to computation or reckoning.Penny Cyclopedia, p. 307 Medieval writers used the word  (\"proportion\") to indicate ratio and  (\"proportionality\") for the equality of ratios.Smith, p. 478\n\nEuclid collected the results appearing in the Elements from earlier sources. The Pythagoreans developed a theory of ratio and proportion as applied to numbers.Heath, p. 112 The Pythagoreans' conception of number included only what would today be called rational numbers, casting doubt on the validity of the theory in geometry where, as the Pythagoreans also discovered, incommensurable ratios (corresponding to irrational numbers) exist. The discovery of a theory of ratios that does not assume commensurability is probably due to Eudoxus of Cnidus. The exposition of the theory of proportions that appears in Book VII of The Elements reflects the earlier theory of ratios of commensurables.Heath, p. 113\n\nThe existence of multiple theories seems unnecessarily complex to modern sensibility since ratios are, to a large extent, identified with quotients. This is a comparatively recent development however, as can be seen from the fact that modern geometry textbooks still use distinct terminology and notation for ratios and quotients. The reasons for this are twofold. First, there was the previously mentioned reluctance to accept irrational numbers as true numbers. Second, the lack of a widely used symbolism to replace the already established terminology of ratios delayed the full acceptance of fractions as alternative until the 16th century.Smith, p. 480\n\nEuclid's definitions\nBook V of Euclid's Elements has 18 definitions, all of which relate to ratios.Heath, reference for section In addition, Euclid uses ideas that were in such common usage that he did not include definitions for them. The first two definitions say that a part of a quantity is another quantity that \"measures\" it and conversely, a multiple of a quantity is another quantity that it measures. In modern terminology, this means that a multiple of a quantity is that quantity multiplied by an integer greater than one—and a part of a quantity (meaning aliquot part) is a part that, when multiplied by an integer greater than one, gives the quantity.\n\nEuclid does not define the term \"measure\" as used here, However, one may infer that if a quantity is taken as a unit of measurement, and a second quantity is given as an integral number of these units, then the first quantity measures the second. Note that these definitions are repeated, nearly word for word, as definitions 3 and 5 in book VII.\n\nDefinition 3 describes what a ratio is in a general way. It is not rigorous in a mathematical sense and some have ascribed it to Euclid's editors rather than Euclid himself.\"Geometry, Euclidean\" Encyclopædia Britannica Eleventh Edition p682. Euclid defines a ratio as between two quantities of the same type, so by this definition the ratios of two lengths or of two areas are defined, but not the ratio of a length and an area. Definition 4 makes this more rigorous. It states that a ratio of two quantities exists when there is a multiple of each that exceeds the other. In modern notation, a ratio exists between quantities p and q if there exist integers m and n so that mp>q and nq>p. This condition is known as the Archimedes property.\n\nDefinition 5 is the most complex and difficult. It defines what it means for two ratios to be equal. Today, this can be done by simply stating that ratios are equal when the quotients of the terms are equal, but Euclid did not accept the existence of the quotients of incommensurate, so such a definition would have been meaningless to him. Thus, a more subtle definition is needed where quantities involved are not measured directly to one another. Though it may not be possible to assign a rational value to a ratio, it is possible to compare a ratio with a rational number. Specifically, given two quantities, p and q, and a rational number m/n we can say that the ratio of p to q is less than, equal to, or greater than m/n when np is less than, equal to, or greater than mq respectively. Euclid's definition of equality can be stated as that two ratios are equal when they behave identically with respect to being less than, equal to, or greater than any rational number. In modern notation this says that given quantities p, q, r and s, then p:q::r:s if for any positive integers m and n, np<mq, np=mq, np>mq according as nr<ms, nr=ms, nr>ms respectively. There is a remarkable similarity between this definition and the theory of Dedekind cuts used in the modern definition of irrational numbers.Heath p. 125\n\nDefinition 6 says that quantities that have the same ratio are proportional or in proportion. Euclid uses the Greek ἀναλόγον (analogon), this has the same root as λόγος and is related to the English word \"analog\".\n\nDefinition 7 defines what it means for one ratio to be less than or greater than another and is based on the ideas present in definition 5. In modern notation it says that given quantities p, q, r and s, then p:q>r:s if there are positive integers m and n so that np>mq and nr≤ms.\n\nAs with definition 3, definition 8 is regarded by some as being a later insertion by Euclid's editors. It defines three terms p, q and r to be in proportion when p:q::q:r. This is extended to 4 terms p, q, r and s as p:q::q:r::r:s, and so on. Sequences that have the property that the ratios of consecutive terms are equal are called geometric progressions. Definitions 9 and 10 apply this, saying that if p, q and r are in proportion then p:r is the duplicate ratio of p:q and if p, q, r and s are in proportion then p:s is the triplicate ratio of p:q. If p, q and r are in proportion then q is called a mean proportional to (or the geometric mean of) p and r. Similarly, if p, q, r and s are in proportion then q and r are called two mean proportionals to p and s.\n\nNumber of terms and use of fractions\nIn general, a comparison of the quantities of a two-entity ratio can be expressed as a fraction derived from the ratio. For example, in a ratio of 2:3, the amount, size, volume, or quantity of the first entity is  that of the second entity.\n\nIf there are 2 oranges and 3 apples, the ratio of oranges to apples is 2:3, and the ratio of oranges to the total number of pieces of fruit is 2:5.  These ratios can also be expressed in fraction form: there are 2/3 as many oranges as apples, and 2/5 of the pieces of fruit are oranges.  If orange juice concentrate is to be diluted with water in the ratio 1:4, then one part of concentrate is mixed with four parts of water, giving five parts total; the amount of orange juice concentrate is 1/4 the amount of water, while the amount of orange juice concentrate is 1/5 of the total liquid.  In both ratios and fractions, it is important to be clear what is being compared to what, and beginners often make mistakes for this reason.\n\nFractions can also be inferred from ratios with more than two entities; however, a ratio with more than two entities cannot be completely converted into a single fraction, because a fraction can only compare two quantities. A separate fraction can be used to compare the quantities of any two of the entities covered by the ratio: for example, from a ratio of 2:3:7 we can infer that the quantity of the second entity is  that of the third entity.\n\nProportions and percentage ratios\nIf we multiply all quantities involved in a ratio by the same number, the ratio remains valid. For example, a ratio of 3:2 is the same as 12:8. It is usual either to reduce terms to the lowest common denominator, or to express them in parts per hundred (percent).\n\nIf a mixture contains substances A, B, C and D in the ratio 5:9:4:2 then there are 5 parts of A for every 9 parts of B, 4 parts of C and 2 parts of D.  As 5+9+4+2=20, the total mixture contains 5/20 of A (5 parts out of 20), 9/20 of B, 4/20 of C, and 2/20 of D.  If we divide all numbers by the total and multiply by 100, we have converted to percentages: 25% A, 45% B, 20% C, and 10% D (equivalent to writing the ratio as 25:45:20:10).\n\nIf the two or more ratio quantities encompass all of the quantities in a particular situation, it is said that \"the whole\" contains the sum of the parts: for example, a fruit basket containing two apples and three oranges and no other fruit is made up of two parts apples and three parts oranges. In this case, , or 40% of the whole is apples and , or 60% of the whole is oranges. This comparison of a specific quantity to \"the whole\" is called a proportion.\n\nIf the ratio consists of only two values, it can be represented as a fraction, in particular as a decimal fraction. For example, older televisions have a 4:3 aspect ratio, which means that the width is 4/3 of the height (this can also be expressed as 1.33:1 or just 1.33 rounded to two decimal places). More recent widescreen TVs have a 16:9 aspect ratio, or 1.78 rounded to two decimal places. One of the popular widescreen movie formats is 2.35:1 or simply 2.35. Representing ratios as decimal fractions simplifies their comparison. When comparing 1.33, 1.78 and 2.35, it is obvious which format offers wider image. Such a comparison works only when values being compared are consistent, like always expressing width in relation to height.\n\nReduction\nRatios can be reduced (as fractions are) by dividing each quantity by the common factors of all the quantities. As for fractions, the simplest form is considered that in which the numbers in the ratio are the smallest possible integers.\n\nThus, the ratio 40:60 is equivalent in meaning to the ratio 2:3, the latter being obtained from the former by dividing both quantities by 20. Mathematically, we write 40:60 = 2:3, or equivalently 40:60::2:3. The verbal equivalent is \"40 is to 60 as 2 is to 3.\"\n\nA ratio that has integers for both quantities and that cannot be reduced any further (using integers) is said to be in simplest form or lowest terms.\n\nSometimes it is useful to write a ratio in the form 1:x or x:1, where x is not necessarily an integer, to enable comparisons of different ratios. For example, the ratio 4:5 can be written as 1:1.25 (dividing both sides by 4) Alternatively, it can be written as 0.8:1 (dividing both sides by 5).\n\nWhere the context makes the meaning clear, a ratio in this form is sometimes written without the 1 and the colon, though, mathematically, this makes it a factor or multiplier.\n\nIrrational ratios\nRatios may also be established between incommensurable quantities (quantities whose ratio, as value of a fraction, amounts to an irrational number). The earliest discovered example, found by the Pythagoreans, is the ratio of the length of the diagonal  to the length of a side  of a square, which is the square root of 2, formally  Another example is the ratio of a circle's circumference to its diameter, which is called , and is not just an algebraically irrational number, but a transcendental irrational.\n\nAlso well-known is the golden ratio of two (mostly) lengths  and , which is defined by the proportion\n  or, equivalently \nTaking the ratios as fractions and  as having the value , yields the equation\n or \nwhich has the positive, irrational solution \nThus at least one of a and b has to be irrational for them to be in the golden ratio. An example of an occurrence of the golden ratio in math is as the limiting value of the ratio of two consecutive Fibonacci numbers: even though all these ratios are ratios of two integers and hence are rational, the limit of the sequence of these rational ratios is the irrational golden ratio.\n\nSimilarly, the silver ratio of  and  is defined by the proportion\n corresponding to  \nThis equation has the positive, irrational solution   so again at least one of the two quantities a and b in the silver ratio must be irrational.\n\nOdds\n\nOdds (as in gambling) are expressed as a ratio.  For example, odds of \"7 to 3 against\" (7:3) mean that there are seven chances that the event will not happen to every three chances that it will happen.  The probability of success is 30%.  In every ten trials, there are expected to be three wins and seven losses.\n\nUnits\nRatios may be unitless, as in the case they relate quantities in units of the same dimension, even if their units of measurement are initially different.\nFor example, the ratio 1 minute : 40 seconds can be reduced by changing the first value to 60 seconds. Once the units are the same, they can be omitted, and the ratio can be reduced to 3:2.\n\nOn the other hand, there are non-dimensionless ratios, also known as rates.\"'Velocity' can be defined as the ratio... 'Population density' is the ratio... 'Gasoline consumption' is measure as the ratio...\", \"Ratio and Proportion: Research and Teaching in Mathematics Teachers\"  \"Ratio as a Rate. The first type [of ratio] defined by Freudenthal, above, is known as rate, and illustrates a comparison between two variables with difference units. (...) A ratio of this sort produces a unique, new concept with its own entity, and this new concept is usually not considered a ratio, per se, but a rate or density.\", \"Ratio and Proportion: Research and Teaching in Mathematics Teachers\" \nIn chemistry, mass concentration ratios are usually expressed as weight/volume fractions.\nFor example, a concentration of 3% w/v usually means 3 g of substance in every 100 mL of solution.  This cannot be converted to a dimensionless ratio, as in weight/weight or volume/volume fractions.\n\nTriangular coordinates\nThe locations of points relative to a triangle with vertices A, B, and C and sides AB, BC, and CA  are often expressed in extended ratio form as triangular coordinates.\n\nIn barycentric coordinates, a point with coordinates  is the point upon which a weightless sheet of metal in the shape and size of the triangle would exactly balance if weights were put on the vertices, with the ratio of the weights at A and B being  the ratio of the weights at B and C being  and therefore the ratio of weights at A and C being \n\nIn trilinear coordinates, a point with coordinates x:y:z has perpendicular distances to side BC (across from vertex A) and side  CA (across from vertex B) in the ratio x:y, distances to side CA and side AB (across from C) in the ratio y:z, and therefore distances to sides BC and AB in the ratio x:z.\n\nSince all information is expressed in terms of ratios (the individual numbers denoted by  x, y, and z have no meaning by themselves), a triangle analysis using barycentric or trilinear coordinates applies regardless of the size of the triangle.\n\nSee also\nDilution ratio\nDisplacement–length ratio\nDimensionless quantity\nFinancial ratio\nFold change\nInterval (music)\nOdds ratio\nParts-per notation\nPrice–performance ratio\nProportionality (mathematics)\nRatio distribution\nRatio estimator\nRate (mathematics)\nRate ratio\nRelative risk\nRule of three (mathematics)\nScale (map)\nSex ratio\nSlope\n\nReferences\n\nFurther reading\n\"Ratio\" The Penny Cyclopædia vol. 19, The Society for the Diffusion of Useful Knowledge (1841) Charles Knight and Co., London pp. 307ff\n\"Proportion\" New International Encyclopedia, Vol. 19 2nd ed. (1916) Dodd Mead & Co. pp270-271\n\"Ratio and Proportion\" Fundamentals of practical mathematics, George Wentworth, David Eugene Smith, Herbert Druery Harper (1922) Ginn and Co. pp. 55ff\n\nD.E. Smith, History of Mathematics, vol 2 Dover (1958) pp. 477ff\n\nExternal links\n\nCategory:Elementary mathematics\nCategory:Algebra\n "
    },
    {
      "title": "Recurrence relation",
      "url": "https://en.wikipedia.org/wiki/Recurrence_relation",
      "text": "In mathematics, a recurrence relation is an equation that recursively defines a sequence or multidimensional array of values, once one or more initial terms are given: each further term of the sequence or array is defined as a function of the preceding terms.\n\nThe term difference equation sometimes (and for the purposes of this article) refers to a specific type of recurrence relation.  However, \"difference equation\" is frequently used to refer to any recurrence relation.\n\nDefinition\nA recurrence relation is an equation that expresses each element of a sequence as a function of the preceding ones. More precisely, in the case where only the immediately preceding element is involved, a recurrence relation has the form\n\nwhere \n\nis a function, where  is a set to which the elements of a sequence must belong. For any , this defines a unique sequence with  as its first element, called the initial value.Jacobson, Nathan , Basic Algebra 2 (2nd ed.), § 0.4. pg 16.\n\nIt is easy to modify the definition for getting sequences starting from the term of index 1 or higher.\n\nThis defines recurrence relation of first order. A recurrence relation of order  has the form \n\nwhere  is a function that involves  consecutive elements of the sequence.\nIn this case,  initial values are needed for defining a sequence.\n\nExamples\nFactorial\nThe factorial is defined by the recurrence relation\n\nand the initial condition\n\nLogistic map\nAn example of a recurrence relation is the logistic map:\n\nwith a given constant r; given the initial term x0 each subsequent term is determined by this relation.\n\nSolving a recurrence relation means obtaining a closed-form solution: a non-recursive function of n.\n\nFibonacci numbers\nThe recurrence of order two satisfied by the Fibonacci numbers is the archetype of a homogeneous linear recurrence relation with constant coefficients (see below).  The Fibonacci sequence is defined using the recurrence\n\nwith initial conditions (seed values)\n\nExplicitly, the recurrence yields the equations\n\netc.\n\nWe obtain the sequence of Fibonacci numbers, which begins\n0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, ...\n\nThe recurrence can be solved by methods described below yielding Binet's formula, which involves powers of the two roots of the characteristic polynomial t2 = t + 1; the generating function of the sequence is the rational function\n  \n\nBinomial coefficients\nA simple example of a multidimensional recurrence relation is given by the binomial coefficients , which count the number of ways of selecting k elements out of a set of n elements.\nThey can be computed by the recurrence relation\n\nwith the base cases . Using this formula to compute the values of all binomial coefficients generates an infinite array called Pascal's triangle. The same values can also be computed directly by a different formula that is not a recurrence, but that requires multiplication and not just addition to compute:\n\n Relationship to difference equations narrowly defined \nGiven an ordered sequence  of real numbers: the first difference  is defined as\n\nThe second difference  is defined as\n\nwhich can be simplified to\n\nMore generally: the  k-th difference of the sequence an written as  is defined recursively as\n\n(The sequence and its differences are related by a binomial transform.) The more restrictive definition of difference equation is an equation composed of an and its kth differences. (A widely used broader definition treats \"difference equation\" as synonymous with \"recurrence relation\".  See for example rational difference equation and matrix difference equation.)\n\nActually, it is easily seen that,\n\nThus, a difference equation can be defined as an equation that involves \nan, an-1, an-2 etc. (or equivalently\nan, an+1, an+2 etc.)\n\nSince difference equations are a very common form of recurrence, some authors use the two terms interchangeably. For example, the difference equation\n\nis equivalent to the recurrence relation\n\nThus one can solve many recurrence relations by rephrasing them as difference equations, and then solving the difference equation, analogously to how one solves ordinary differential equations.  However, the Ackermann numbers are an example of a recurrence relation that do not map to a difference equation, much less points on the solution to a differential equation.\n\nSee time scale calculus for a unification of the theory of difference equations with that of differential equations.\n\nSummation equations relate to difference equations as integral equations relate to differential equations.\n\nFrom sequences to grids\nSingle-variable or one-dimensional recurrence relations are about sequences (i.e. functions defined on one-dimensional grids). Multi-variable or n-dimensional recurrence relations are about n-dimensional grids. Functions defined on n-grids can also be studied with partial difference equations.Partial difference equations, Sui Sun Cheng, CRC Press, 2003, \n\n Solving \n\n Solving homogeneous linear recurrence relations with constant coefficients \n\nRoots of the characteristic polynomial\nAn order-d homogeneous linear recurrence with constant coefficients  is an equation of the form\n\nwhere the d coefficients ci (for all i) are constants, and .\n\nA constant-recursive sequence is a sequence satisfying a recurrence of this form.  There are d degrees of freedom for solutions to this recurrence, i.e., the initial values  can be taken to be any values but then the recurrence determines the sequence uniquely.\n\nThe same coefficients yield the characteristic polynomial (also \"auxiliary polynomial\")\n\nwhose d roots play a crucial role in finding and understanding the sequences satisfying the recurrence. If the roots r1, r2, ... are all distinct, then each solution to the recurrence takes the form\n\nwhere the coefficients ki are determined in order to fit the initial conditions of the recurrence. When the same roots occur multiple times, the terms in this formula corresponding to the second and later occurrences of the same root are multiplied by increasing powers of n. For instance, if the characteristic polynomial can be factored as (x−r)3, with the same root r occurring three times, then the solution would take the form\n.\n\nAs well as the Fibonacci numbers, other constant-recursive sequences include the Lucas numbers and Lucas sequences, the Jacobsthal numbers, the Pell numbers and more generally the solutions to Pell's equation.\n\nFor order 1, the recurrence\n\nhas the solution an = rn with a0 = 1 and the most general solution is an = krn with a0 = k. The  characteristic polynomial equated to zero (the characteristic equation) is simply t − r = 0.\n\nSolutions to such recurrence relations of higher order are found by systematic means, often using the fact that an = rn is a solution for the recurrence exactly when t = r is a root of the characteristic polynomial. This can be approached directly or using generating functions (formal power series) or matrices.\n\nConsider, for example, a recurrence relation of the form\n\nWhen does it have a solution of the same general form as an = rn? Substituting this guess (ansatz) in the recurrence relation, we find that\n\nmust be true for all n > 1.\n\nDividing through by rn−2, we get that all these equations reduce to the same thing:\n\nwhich is the characteristic equation of the recurrence relation.  Solve for r to obtain the two roots λ1, λ2: these roots are known as the characteristic roots or eigenvalues of the characteristic equation.  Different solutions are obtained depending on the nature of the roots:  If these roots are distinct, we have the general solution\n\nwhile if they are identical (when A2 + 4B = 0), we have\n\nThis is the most general solution; the two constants C and D can be chosen based on two given initial conditions a0 and a1 to produce a specific solution.\n\nIn the case of complex eigenvalues (which also gives rise to complex values for the solution parameters C and D), the use of complex numbers can be eliminated by rewriting the solution in trigonometric form.  In this case we can write the eigenvalues as  Then it can be shown that\n\ncan be rewritten asChiang, Alpha C., Fundamental Methods of Mathematical Economics, third edition, McGraw-Hill, 1984.\n\nwhere\n\nHere E and F (or equivalently, G and δ)  are real constants which depend on the initial conditions. Using \n\none may simplify the solution given above as\n\nwhere a1 and a2 are the initial conditions and\n\nIn this way there is no need to solve for λ1 and λ2.\n\nIn all cases—real distinct eigenvalues, real duplicated eigenvalues, and complex conjugate eigenvalues—the equation is stable (that is, the variable a converges to a fixed value [specifically, zero]) if and only if both eigenvalues are smaller than one in absolute value.  In this second-order case, this condition on the eigenvalues can be shownPapanicolaou, Vassilis, \"On the asymptotic stability of a class of linear difference equations,\" Mathematics Magazine 69(1), February 1996, 34–43. to be equivalent to |A| < 1 − B < 2, which is equivalent to |B| < 1 and |A| < 1 − B.\n\nThe equation in the above example was homogeneous, in that there was no constant term.  If one starts with the non-homogeneous recurrence\n\nwith constant term K, this can be converted into homogeneous form as follows:  The steady state is found by setting bn = bn−1 = bn−2 = b* to obtain\n\nThen the non-homogeneous recurrence can be rewritten in homogeneous form as\n\nwhich can be solved as above.\n\nThe stability condition stated above in terms of eigenvalues for the second-order case remains valid for the general nth-order case:  the equation is stable if and only if all eigenvalues of the characteristic equation are less than one in absolute value.\n\nGiven a homogeneous linear recurrence relation with constant coefficients of order d, let p(t) be the characteristic polynomial (also \"auxiliary polynomial\")\n\nsuch that each ci corresponds to each ci in the original recurrence relation (see the general form above). Suppose λ is a root of p(t) having multiplicity r. This is to say that (t−λ)r divides p(t). The following two properties hold:\n\n Each of the r sequences  satisfies the recurrence relation.\n Any sequence satisfying the recurrence relation can be written uniquely as a linear combination of solutions constructed in part 1 as λ varies over all distinct roots of p(t).\n\nAs a result of this theorem a homogeneous linear recurrence relation with constant coefficients can be solved in the following manner:\n\n Find the characteristic polynomial p(t).\n Find the roots of p(t) counting multiplicity.\n Write an as a linear combination of all the roots (counting multiplicity as shown in the theorem above) with unknown coefficients bi.\n\nThis is the general solution to the original recurrence relation. (q is the multiplicity of λ*)\n\n4. Equate each  from part 3 (plugging in n = 0, ..., d into the general solution of the recurrence relation) with the known values  from the original recurrence relation. However, the values an from the original recurrence relation used do not usually have to be contiguous: excluding exceptional cases, just d of them are needed (i.e., for an original homogeneous linear recurrence relation of order 3 one could use the values a0, a1, a4).  This process will produce a linear system of d equations with d unknowns. Solving these equations for the unknown coefficients  of the general solution and plugging these values back into the general solution will produce the particular solution to the original recurrence relation that fits the original recurrence relation's initial conditions (as well as all subsequent values  of the original recurrence relation).\n\nThe method for solving linear differential equations is similar to the method above—the \"intelligent guess\" (ansatz) for linear differential equations with constant coefficients is eλx where λ is a complex number that is determined by substituting the guess into the differential equation.\n\nThis is not a coincidence.  Considering the Taylor series of the solution to a linear differential equation:\n\nit can be seen that the coefficients of the series are given by the nth derivative of f(x) evaluated at the point a. The differential equation provides a linear difference equation relating these coefficients.\n\nThis equivalence can be used to quickly solve for the recurrence relationship for the coefficients in the power series solution of a linear differential equation.\n\nThe rule of thumb (for equations in which the polynomial multiplying the first term is non-zero at zero) is that:\n\nand more generally\n\nExample: The recurrence relationship for the Taylor series coefficients of the equation:\n\nis given by\n\nor\n\nThis example shows how problems generally solved using the power series solution method taught in normal differential equation classes can be solved in a much easier way.\n\nExample: The differential equation\n\nhas solution\n\nThe conversion of the differential equation to a difference equation of the Taylor coefficients is\n\nIt is easy to see that the nth derivative of eax evaluated at 0 is an\n\nSolving via linear algebra\nA linearly recursive sequence y of order n\n\nis identical to\n\nExpanded with n-1 identities of kind  this n-th order equation is translated into a matrix difference equation system of n first-order linear equations,\n\nObserve that the vector  can be computed by n applications of the companion matrix, C, to the initial state vector, . Thereby, n-th entry of the sought sequence y, is the top component of .\n\nEigendecomposition,  into eigenvalues, , and eigenvectors, , is used to compute  Thanks to the crucial fact that system C time-shifts every eigenvector, e, by simply scaling its components λ times,\n\nthat is, time-shifted version of eigenvector,e, has components λ times larger, the eigenvector components are powers of λ,  and, thus, recurrent homogeneous linear equation solution is a combination of exponential functions, . The components  can be determined out of initial conditions:\n\nSolving for coefficients,\n\nThis also works with arbitrary boundary conditions , not necessary the initial ones,\n\nThis description is really no different from general method above, however it is more succinct. It also works nicely for situations like\n\nwhere there are several linked recurrences..\n\nSolving with z-transforms\nCertain difference equations - in particular, linear constant coefficient difference equations - can be solved using z-transforms. The z-transforms are a class of integral transforms that lead to more convenient algebraic manipulations and more straightforward solutions. There are cases in which obtaining a direct solution would be all but impossible, yet solving the problem via a thoughtfully chosen integral transform is straightforward.\n\nSolving non-homogeneous linear recurrence relations with constant coefficients\nIf the recurrence is non-homogeneous, a particular solution can be found by the method of undetermined coefficients and the solution is the sum of the solution of the homogeneous and the particular solutions.  Another method to solve a non-homogeneous recurrence is the method of symbolic differentiation.  For example, consider the following recurrence:\n\nThis is a non-homogeneous recurrence.  If we substitute n ↦ n+1, we obtain the recurrence\n\nSubtracting the original recurrence from this equation yields\n\nor equivalently\n\nThis is a homogeneous recurrence, which can be solved by the methods explained above.  In general, if a linear recurrence has the form\n\nwhere  are constant coefficients and p(n) is the inhomogeneity, then if p(n) is a polynomial with degree r, then this non-homogeneous recurrence can be reduced to a homogeneous recurrence by applying the method of symbolic differencing r times.\n\nIf\n\nis the generating function of the inhomogeneity, the generating function\n\nof the non-homogeneous recurrence \n\nwith constant coefficients  is derived from\n\nIf P(x) is a rational generating function, A(x) is also one. The case discussed above, where pn = K is a constant, emerges as one example of this formula, with P(x) = K/(1−x). Another example, the recurrence  with linear inhomogeneity, arises in the definition of the schizophrenic numbers. The solution of homogeneous recurrences is incorporated as p = P = 0.\n\nSolving first-order non-homogeneous recurrence relations with variable coefficients\nMoreover, for the general first-order non-homogeneous linear recurrence relation with variable coefficients:\n\nthere is also a nice method to solve it:\n\nLet \n\nThen \n\nIf we apply the formula to  and take the limit h→0, we get the formula for first order linear differential equations with variable coefficients; the sum becomes an integral, and the product becomes the exponential function of an integral.\n\nSolving general homogeneous linear recurrence relations\nMany homogeneous linear recurrence relations may be solved by means of the generalized hypergeometric series. Special cases of these lead to recurrence relations for the orthogonal polynomials, and many special functions. For example, the solution to\n\nis given by\n\nthe Bessel function, while\n\nis solved by\n\nthe confluent hypergeometric series. Sequences which are the solutions of linear difference equations with polynomial coefficients are called P-recursive. For these specific recurrence equations algorithms are known which find polynomial, rational or hypergeometric solutions.\n\nSolving first-order rational difference equations\n\nA first order rational difference equation has the form . Such an equation can be solved by writing  as a nonlinear transformation of another variable  which itself evolves linearly.  Then standard methods can be used to solve the linear difference equation in .\n\nStability\n\nStability of linear higher-order recurrences\nThe linear recurrence of order d,\n\nhas the characteristic equation\n\nThe recurrence is stable, meaning that the iterates converge asymptotically to a fixed value, if and only if the eigenvalues (i.e., the roots of the characteristic equation), whether real or complex, are all less than unity in absolute value.\n\nStability of linear first-order matrix recurrences\n\nIn the first-order matrix difference equation\n\nwith state vector x and transition matrix A, x converges asymptotically to the steady state vector x* if and only if all eigenvalues of the transition matrix A (whether real or complex) have an absolute value which is less than 1.\n\nStability of nonlinear first-order recurrences\nConsider the nonlinear first-order recurrence\n\nThis recurrence is locally stable, meaning that it converges to a fixed point x* from points sufficiently close to x*, if the slope of f  in the neighborhood of x* is smaller than unity in absolute value:  that is,\n\n \n\nA nonlinear recurrence could have multiple fixed points, in which case some fixed points may be locally stable and others locally unstable; for continuous f  two adjacent fixed points cannot both be locally stable.\n\nA nonlinear recurrence relation could also have a cycle of period k for k > 1.  Such a cycle is stable, meaning that it attracts a set of initial conditions of positive measure, if the composite function\n\nwith f appearing k times is locally stable according to the same criterion:\n\n \n\nwhere x* is any point on the cycle.\n\nIn a chaotic recurrence relation, the variable x stays in a bounded region but never converges to a fixed point or an attracting cycle; any fixed points or cycles of the equation are unstable.  See also logistic map, dyadic transformation, and tent map.\n\n Relationship to differential equations \nWhen solving an ordinary differential equation numerically, one typically encounters a recurrence relation. For example, when solving the initial value problem\n\nwith Euler's method and a step size h, one calculates the values\n\nby the recurrence\n\nSystems of linear first order differential equations can be discretized exactly analytically using the methods shown in the discretization article.\n\n Applications \n\n Biology \nSome of the best-known difference equations have their origins in the attempt to model population dynamics. For example, the Fibonacci numbers were once used as a model for the growth of a rabbit population.\n\nThe logistic map is used either directly to model population growth, or as a starting point for more detailed models of population dynamics. In this context, coupled difference equations are often used to model the interaction of two or more populations. For example, the Nicholson-Bailey model for a host-parasite interaction is given by\n\nwith Nt representing the hosts, and Pt the parasites, at time t.\n\nIntegrodifference equations are a form of recurrence relation important to spatial ecology. These and other difference equations are particularly suited to modeling univoltine populations.\n\nComputer science\nRecurrence relations are also of fundamental importance in analysis of algorithms.Cormen, T. et al, Introduction to Algorithms, MIT Press, 2009R. Sedgewick, F. Flajolet, An Introduction to the Analysis of Algorithms, Addison-Wesley, 2013 If an algorithm is designed so that it will break a problem into smaller subproblems (divide and conquer), its running time is described by a recurrence relation.\n\nA simple example is the time an algorithm takes to find an element in an ordered vector with  elements,  in the worst case.\n\nA naive algorithm will search from left to right, one element at a time. The worst possible scenario is when the required element is the last, so the number of comparisons is .\n\nA better algorithm is called binary search. However, it requires a sorted vector. It will first check if the element is at the middle of the vector. If not, then it will check if the middle element is greater or lesser than the sought element. At this point, half of the vector can be discarded, and the algorithm can be run again on the other half. The number of comparisons will be given by\n\nthe time complexity of which will be .\n\n Digital signal processing \nIn digital signal processing, recurrence relations can model feedback in a system, where outputs at one time become inputs for future time. They thus arise in infinite impulse response (IIR) digital filters.\n\nFor example, the equation for a \"feedforward\" IIR comb filter of delay T is:\n\nWhere  is the input at time t,  is the output at time t, and α controls how much of the delayed signal is fed back into the output.  From this we can see that\n\netc.\n\nEconomics\nRecurrence relations, especially linear recurrence relations, are used extensively in both theoretical and empirical economics.  In particular, in macroeconomics one might develop a model of various broad sectors of the economy (the financial sector, the goods sector, the labor market, etc.) in which some agents' actions depend on lagged variables.  The model would then be solved for current values of key variables (interest rate, real GDP, etc.) in terms of exogenous variables and lagged endogenous variables.  See also time series analysis.\n\nSee also\n\n Holonomic sequences\n Iterated function\n Orthogonal polynomials\n Recursion\n Recursion (computer science)\n Lagged Fibonacci generator\n Master theorem (analysis of algorithms)\n Circle points segments proof\n Continued fraction\n Time scale calculus\n Integrodifference equation\n Combinatorial principles\n Infinite impulse response\n Multiplicative calculus\n Integration by reduction formulae\n Mathematical induction\n\n Notes \n\n References \n \n \n \n \n Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to Algorithms, Second Edition. MIT Press and McGraw-Hill, 1990. . Chapter 4: Recurrences, pp. 62–90.\n\n chapter 7.\n  Chapter 9.1: Difference Equations.\n \n  at EqWorld - The World of Mathematical Equations.\n  at EqWorld - The World of Mathematical Equations.\n\n External links \n \n \n  OEIS index to a few thousand examples of linear recurrences, sorted by order (number of terms) and signature (vector of values of the constant coefficients)\n\nCategory:Algebra\n \nCategory:Combinatorics"
    },
    {
      "title": "Reduct",
      "url": "https://en.wikipedia.org/wiki/Reduct",
      "text": "In universal algebra and in model theory, a reduct of an algebraic structure is obtained by omitting some of the operations and relations of that structure.  The converse of \"reduct\" is \"expansion.\"\n\nDefinition\nLet A be an algebraic structure (in the sense of universal algebra) or equivalently a structure in the sense of model theory, organized as a set X together with an indexed family of operations and relations φi on that set, with index set I.  Then the reduct of A defined by a subset J of I is the structure consisting of the set X and J-indexed family of operations and relations whose j-th operation or relation for j∈J is the j-th operation or relation of A.  That is, this reduct is the structure A with the omission of those operations and relations φi for which i is not in J.\n\nA structure A is an expansion of B just when B is a reduct of A.  That is, reduct and expansion are mutual converses.\n\nExamples\nThe monoid (Z, +, 0) of integers under addition is a reduct of the group (Z, +, −, 0) of integers under addition and negation, obtained by omitting negation. By contrast, the monoid (N,+,0) of natural numbers under addition is not the reduct of any group.\n\nConversely the group (Z, +, −, 0) is the expansion of the monoid (Z, +, 0), expanding it with the operation of negation.\n\nReferences\n \n \n\nCategory:Algebra\nCategory:Mathematical relations\nCategory:Model theory\nCategory:Universal algebra"
    },
    {
      "title": "Regular chain",
      "url": "https://en.wikipedia.org/wiki/Regular_chain",
      "text": "In computer algebra, a regular chain is a particular kind of triangular set in a multivariate polynomial ring over a field. It enhances the notion of characteristic set.\n\n Introduction \n\nGiven a linear system, one can convert it to a triangular system via Gaussian elimination. For the non-linear case, given a polynomial system F over a field, one can convert (decompose or triangularize) it to a finite set of triangular sets, in the sense that the algebraic variety V(F) is described by these triangular sets.\n\nA triangular set may merely describe the empty set. To fix this degenerated case, the notion of regular chain was introduced, independently by Kalkbrener (1993), Yang and Zhang (1994). Regular chains also appear in Chou and Gao (1992). Regular chains are special triangular sets which are used in different algorithms for computing unmixed-dimensional decompositions of algebraic varieties. Without using factorization, these decompositions have better properties that the ones produced by Wu's algorithm. Kalkbrener's original definition was based on the following observation: every irreducible variety is uniquely determined by one of its generic points and varieties can be represented by describing the generic points of their irreducible components. These generic points are given by regular chains.\n\n Examples \n\nDenote Q the rational number field. In Q[x1, x2, x3] with variable ordering x1 < x2 < x3,\n \nis a triangular set and also a regular chain. Two generic points given by T are (a, a, a) and (a, -a, a) where a is transcendental over Q.\nThus there are two irreducible components, given by { x2 - x1, x3 - x1 } and { x2 + x1, x3 - x1 }, respectively.\nNote that: (1) the content of the second polynomial is x2, which does not contribute to the generic points represented and thus can be removed; (2) the dimension of each component is 1, the number of free variables in the regular chain.\n\n Formal definitions \n\nThe variables in the polynomial ring \n \nare always sorted as x1 < ... < xn. \nA non-constant polynomial f in   can be seen as a univariate polynomial in its greatest variable.\nThe greatest variable in f is called its main variable, denoted by mvar(f). Let u be \nthe main variable of f and write it as \n, \nwhere e is the degree of f w.r.t. u  and  is \nthe leading coefficient of f w.r.t. u.  Then the initial of f\nis  and e is its main degree.\n\nTriangular set\n\nA non-empty subset T of  is a triangular set, \nif the polynomials in T are non-constant and have distinct main variables. \nHence, a triangular set is finite, and has cardinality at most n.\n\nRegular chain\n\nLet T = {t1, ..., ts} be a triangular set such that \nmvar(t1) < ... < mvar(ts), \n be the initial of ti and h be the product of hi's. \nThen T is a regular chain if \n , \nwhere each resultant is computed with respect to the main variable of ti, respectively.  \nThis definition is from Yang and Zhang, which is of much algorithmic flavor.\n\nQuasi-component and saturated ideal of a regular chain\n\nThe quasi-component W(T) described by the regular chain T is \n, that is,\nthe set difference of the varieties V(T) and V(h). \nThe attached algebraic object of a regular chain is its saturated ideal \n. \nA classic result is that the Zariski closure of W(T) equals the variety defined by sat(T), that is,\n,\nand its dimension is n - |T|, the difference of the number of variables and the number of polynomials in T.\n\nTriangular decompositions\n\nIn general, there are two ways to decompose a polynomial system F. The first one is to decompose lazily, that is, only to represent its generic points in the (Kalkbrener) sense,\n .\nThe second is to describe all zeroes in the Lazard sense,\n .\nThere are various algorithms available for triangular decompositions in either sense.\n\n Properties \n\nLet T be a regular chain in the polynomial ring R.\n\n The saturated ideal sat(T) is an unmixed ideal with dimension n − |T|.\n A regular chain holds a strong elimination property in the sense that:\n .\n\n A polynomial p is in sat(T) if and only if p is pseudo-reduced to zero by T, that is,\n .\nHence the membership test for sat(T) is algorithmic.\n\n A polynomial p is a zero-divisor modulo sat(T) if and only if  and .\nHence the regularity test for sat(T) is algorithmic.\n\n Given a prime ideal P, there exists a regular chain C such that P = sat(C).\n If the first element of a regular chain C is an irreducible polynomial and the others are linear in their main variable, then sat(C) is a prime ideal.\n Conversely, if P is a prime ideal, then, after almost all linear changes of variables, there exists a regular chain C of the preceding shape such that P = sat(C).\n A triangular set is a regular chain if and only if it is a Ritt characteristic set of its saturated ideal.\n\n See also \nWu's method of characteristic set\nGröbner basis\nRegular semi-algebraic system\nTriangular decomposition\n\n Further references \n\n P. Aubry, D. Lazard, M. Moreno Maza. On the theories of triangular sets. Journal of Symbolic Computation, 28(1–2):105–124, 1999.\n F. Boulier and F. Lemaire and M. Moreno Maza. Well known theorems on triangular systems and the D5 principle. Transgressive Computing 2006, Granada, Spain.\n E. Hubert. Notes on triangular sets and triangulation-decomposition algorithms I: Polynomial systems.  LNCS, volume 2630, Springer-Verlag Heidelberg.\n F. Lemaire and M. Moreno Maza and Y. Xie. The RegularChains library. Maple Conference 2005. \n M. Kalkbrener: Algorithmic Properties of Polynomial Rings. J. Symb. Comput. 26(5): 525–581 (1998).\n M. Kalkbrener: A Generalized Euclidean Algorithm for Computing Triangular Representations of Algebraic Varieties. J. Symb. Comput. 15(2): 143–167 (1993).\n D. Wang. Computing Triangular Systems and Regular Systems. Journal of Symbolic Computation 30(2) (2000) 221–236.\n Yang, L., Zhang, J. (1994). Searching dependency between algebraic equations: an algorithm applied to automated reasoning. Artificial Intel ligence in Mathematics, pp. 14715,  Oxford University Press.\n\nCategory:Equations\nCategory:Algebra\nCategory:Polynomials\nCategory:Algebraic geometry\nCategory:Computer algebra"
    },
    {
      "title": "Regular semi-algebraic system",
      "url": "https://en.wikipedia.org/wiki/Regular_semi-algebraic_system",
      "text": "In computer algebra, a regular semi-algebraic system is a particular kind of triangular  system  of multivariate polynomials over a real closed field.\n\n Introduction \nRegular chains and triangular decompositions are fundamental and well-developed tools for describing the complex solutions of polynomial systems. The notion of a regular semi-algebraic system is an adaptation of the concept of a regular chain focusing on solutions of the real analogue: semi-algebraic systems.\n\nAny semi-algebraic system  can be decomposed into finitely many regular semi-algebraic systems  such that a point (with real coordinates) is a solution of  if and only if it is a solution of one of the systems .Changbo Chen, James H. Davenport, John P. May, Marc Moreno-Maza, Bican Xia, Rong Xiao. Triangular decomposition of semi-algebraic systems.  Proceedings of 2010 International Symposium on Symbolic and Algebraic Computation (ISSAC 2010), ACM Press, pp. 187–194, 2010.\n\n Formal definition \n\nLet  be a regular chain of  for some ordering of the variables  and a real closed field . Let  and  designate respectively the variables of  that are free and algebraic with respect to . Let  be finite such that each polynomial in  is regular with respect to the saturated ideal of . Define . Let  be a quantifier-free formula of  involving only the variables of . We say that  is a regular semi-algebraic system if the following three conditions hold.\n\n  defines a non-empty open semi-algebraic set  of ,\n the regular system  specializes well at every point  of ,\n at each point  of , the specialized system  has at least one real zero.\n\nThe zero set of , denoted by , is defined as the set of points  such that  is true and , for all and all . Observe that  has dimension  in the affine space .\n\n See also \nReal algebraic geometry\n\n References \n\nCategory:Equations\nCategory:Algebra\nCategory:Polynomials\nCategory:Algebraic geometry\nCategory:Computer algebra"
    },
    {
      "title": "Resolvent cubic",
      "url": "https://en.wikipedia.org/wiki/Resolvent_cubic",
      "text": "In algebra, a resolvent cubic is one of several distinct, although related, cubic polynomials defined from a monic polynomial of degree four:\n\nIn each case:\n The coefficients of the resolvent cubic can be obtained from the coefficients of  using only sums, subtractions and multiplications.\n Knowing the roots of the resolvent cubic of  is useful for finding the roots of  itself. Hence the name “resolvent cubic”.\n The polynomial  has a multiple root if and only if its resolvent cubic has a multiple root.\n\nDefinitions\nSuppose that the coefficients of  belong to a field  whose characteristic is different from . In other words, we are working in a field in which . Whenever roots of  are mentioned, they belong to some extension  of  such that  factors into linear factors in . If  is the field  of rational numbers, then  can be the field  of complex numbers or the field  of algebraic numbers.\n\nIn some cases, the concept of resolvent cubic is defined only when  is a quartic in depressed form—that is, when .\n\nNote that the fourth and fifth definitions below also make sense and that the relationship between these resolvent cubics and  are still valid if the characteristic of  is equal to .\n\nFirst definition\nSuppose that  is a depressed quartic—that is, that . A possible definition of the resolvent cubic of  is:\n\nThe origin of this definition lies in applying Ferrari's method to find the roots of . To be more precise:\n\nAdd a new unknown, , to . Now you have:\n\nIf this expression is a square, it can only be the square of\n\nBut the equality\n\nis equivalent to\n\nand this is the same thing as the assertion that  = 0.\n\nIf  is a root of , then it is a consequence of the computations made above that the roots of  are the roots of the polynomial\n\ntogether with the roots of the polynomial\n\nOf course, this makes no sense if , but since the constant term of  is ,  is a root of  if and only if , and in this case the roots of  can be found using the quadratic formula.\n\nSecond definition\nAnother possible definition (still supposing that  is a depressed quartic) is\n\nThe origin of this definition is similar to the previous one. This time, we start by doing:\n\nand a computation similar to the previous one shows that this last expression is a square if and only if\n\nA simple computation shows that\n\nThird definition\nAnother possible definition (again, supposing that  is a depressed quartic) is\n\nThe origin of this definition lies in another method of solving quartic equations, namely Descartes' method. If you try to find the roots of  by expressing it as a product of two monic quadratic polynomials  and , then\n\nIf there is a solution of this system with  (note that if , then this is automatically true for any solution), the previous system is equivalent to\n\nIt is a consequence of the first two equations that then\n\nand\n\nAfter replacing, in the third equation,  and  by these values one gets that\n\nand this is equivalent to the assertion that  is a root of . So, again, knowing the roots of  helps to determine the roots of .\n\nNote that\n\nFourth definition\nthumb|right|upright=1.5|Graph of the polynomial function  (in green) together with the graph of its resolvent cubic  (in red). The roots of both polynomials are visible too.\n\nStill another possible definition is\n\nIn fact, if the roots of  are , and , then\n\na fact the follows from Vieta's formulas. In other words, R4(y) is the monic polynomial whose roots are \n, \n, and \n.\n\nIt is easy to see that\n\nand\n\nTherefore,  has a multiple root if and only if  has a multiple root. More precisely,  and  have the same discriminant.\n\nOne should note that if  is a depressed polynomial, then\n\nFifth definition\nYet another definition is\n\nIf, as above, the roots of  are , and , then\n\nagain as a consequence of Vieta's formulas. In other words,  is the monic polynomial whose roots are \n,\n, and \n.\n\nIt is easy to see that\n\nand\n\nTherefore, as it happens with ,  has a multiple root if and only if  has a multiple root. More precisely,  and  have the same discriminant. This is also a consequence of the fact that  = .\n\nNote that if  is a depressed polynomial, then\n\nApplications\nSolving quartic equations\nIt was explained above how , , and  can be used to find the roots of  if this polynomial is depressed. In the general case, one simply has to find the roots of the depressed polynomial . For each root  of this polynomial,  is a root of .\n\nFactoring quartic polynomials\nIf a quartic polynomial  is reducible in , then it is the product of two quadratic polynomials or the product of a linear polynomial by a cubic polynomial. This second possibility occurs if and only if  has a root in . In order to determine whether or not  can be expressed as the product of two quadratic polynomials, let us assume, for simplicity, that  is a depressed polynomial. Then it was seen above that if the resolvent cubic  has a non-null root of the form , for some , then such a decomposition exists.\n\nThis can be used to prove that, in , every quartic polynomial without real roots can be expressed as the product of two quadratic polynomials. Let  be such a polynomial. We can assume without loss of generality that  is monic. We can also assume without loss of generality that it is a reduced polynomial, because  can be expressed as the product of two quadratic polynomials if and only if  can and this polynomial is a reduced one. Then  = . There are two cases:\n If  then  = . Since  if  is large enough, then, by the intermediate value theorem,  has a root  with . So, we can take  = .\n If  = , then  = . The roots of this polynomial are  and the roots of the quadratic polynomial . If , then the product of the two roots of this polynomial is smaller than  and therefore it has a root greater than  (which happens to be ) and we can take  as the square root of that root. Otherwise,  and then,\n\nMore generally, if  is a real closed field, then every quartic polynomial without roots in  can be expressed as the product of two quadratic polynomials in . Indeed, this statement can be expressed in first-order logic and any such statement that holds for  also holds for any real closed field.\n\nA similar approach can be used to get an algorithm to determine whether or not a quartic polynomial  is reducible and, if it is, how to express it as a product of polynomials of smaller degree. Again, we will suppose that  is monic and depressed. Then  is reducible if and only if at least one of the following conditions holds:\n The polynomial  has a rational root (this can be determined using the rational root theorem).\n The resolvent cubic  has a root of the form , for some non-null rational number  (again, this can be determined using the rational root theorem).\n The number  is the square of a rational number and  = .\nIndeed:\n If  has a rational root , then  is the product of  by a cubic polynomial in , which can be determined by polynomial long division or by Ruffini's rule.\n If there is a rational number  such that  is a root of , it was shown above how to express  as the product of two quadratic polynomials in .\n Finally, if the third condition holds and if  is such that =, then  = .\n\nGalois groups of irreducible quartic polynomials\nThe resolvent cubic of an irreducible quartic polynomial  can be used to determine its Galois group ; that is, the Galois group of the splitting field of . Let  be the degree over  of the splitting field of the resolvent cubic (it can be either  or ; they have the same splitting field). Then the group  is a subgroup of the symmetric group . More precisely:\n If  (that is, if the resolvent cubic factors into linear factors in ), then  is the group }.\n If  (that is, if the resolvent cubic has one and, up to multiplicity, only one root in ), then, in order to determine , one can determine whether or not  is still irreducible after adjoining to the field  the roots of the resolvent cubic. If not, then  is a cyclic group of order 4; more precisely, it is one of the three cyclic subgroups of  generated by any of its six -cycles. If it is still irreducible, then  is one of the three subgroups of  of order , each of which is isomorphic to the dihedral group of order .\n If , then  is the alternating group .\n If , then  is the whole group .\n\nSee also\nResolvent (Galois theory)\n\nReferences\n \n\nCategory:Algebra\nCategory:Equations\nCategory:Polynomials"
    },
    {
      "title": "Schur algebra",
      "url": "https://en.wikipedia.org/wiki/Schur_algebra",
      "text": "In mathematics, Schur algebras, named after Issai Schur, are certain finite-dimensional algebras closely associated with Schur–Weyl duality between general linear and symmetric groups. They are used to relate the representation theories of those two groups. Their use was promoted by the influential monograph of J. A. Green first published in 1980.J. A. Green, Polynomial Representations of GLn, Springer Lecture Notes 830, Springer-Verlag 1980. , ,  The name \"Schur algebra\" is due to Green. In the modular case (over infinite fields of positive characteristic) Schur algebras were used by Gordon James and Karin Erdmann to show that the (still open) problems of computing decomposition numbers for general linear groups and symmetric groups are actually equivalent.Karin Erdmann, Decomposition numbers for symmetric groups and composition factors of Weyl modules. Journal of Algebra 180 (1996), 316–320.   Schur algebras were used by Friedlander and Suslin to prove finite generation of cohomology of finite group schemes.Eric Friedlander and Andrei Suslin, Cohomology of finite group schemes over a field. Inventiones Mathematicae 127 (1997), 209--270.  \n\n Construction \nThe Schur algebra  can be defined for any commutative ring  and integers . Consider the algebra  of polynomials (with coefficients in ) in  commuting variables , 1 ≤ i, j ≤ . Denote by  the homogeneous polynomials of degree . Elements of  are k-linear combinations of monomials formed by multiplying together  of the generators  (allowing repetition). Thus\n\n \n\nNow,  has a natural coalgebra structure with comultiplication  and counit  the algebra homomorphisms given on generators by\n\n       (Kronecker's delta).\n\nSince comultiplication is an algebra homomorphism,  is a bialgebra. One easily\nchecks that  is a subcoalgebra of the bialgebra , for every r ≥ 0.\n\nDefinition. The Schur algebra (in degree ) is the algebra . That is,  is the linear dual of .\n\nIt is a general fact that the linear dual of a coalgebra  is an algebra in a natural way, where the multiplication in the algebra is induced by dualizing the comultiplication in the coalgebra. To see this, let \n \nand, given linear functionals ,  on , define their product to be the linear functional given by \n \nThe identity element for this multiplication of functionals is the counit in .\n\n Main properties \n\n One of the most basic properties expresses  as a centralizer algebra. Let  be the space of rank  column vectors over , and form the tensor power\n\n \nThen the symmetric group  on  letters acts naturally on the tensor space by place permutation, and one has an isomorphism \n \nIn other words,  may be viewed as the algebra of endomorphisms of tensor space commuting with the action of the symmetric group.\n\n  is free over  of rank given by the binomial coefficient .\n Various bases of  are known, many of which are indexed by pairs of semistandard Young tableaux of shape , as  varies over the set of partitions of  into no more than  parts.\n In case k is an infinite field,  may also be identified with the enveloping algebra (in the sense of H. Weyl) for the action of the general linear group  acting on tensor space (via the diagonal action on tensors, induced from the natural action of  on  given by matrix multiplication).\n Schur algebras are \"defined over the integers\". This means that they satisfy the following change of scalars property:\n\n \nfor any commutative ring .\n\n Schur algebras provide natural examples of quasihereditary algebrasEdward Cline, Brian Parshall, and Leonard Scott, Finite-dimensional algebras and highest weight categories. Journal für die Reine und Angewandte Mathematik [Crelle's Journal]  391 (1988), 85–99.  (as defined by Cline, Parshall, and Scott), and thus have nice homological properties. In particular, Schur algebras have finite global dimension.\n\n Generalizations \n Generalized Schur algebras (associated to any reductive algebraic group) were introduced by Donkin in the 1980s.Stephen Donkin, On Schur algebras and related algebras, I. Journal of Algebra 104 (1986), 310–328.    These are also quasihereditary.\n Around the same time, Dipper and JamesRichard Dipper and Gordon James,  The q-Schur algebra. Proceedings of the London Math. Society (3) 59 (1989), 23–50.   introduced the quantized Schur algebras (or q-Schur algebras for short), which are a type of q-deformation of the classical Schur algebras described above, in which the symmetric group is replaced by the corresponding Hecke algebra and the general linear group by an appropriate quantum group.\n There are also generalized q-Schur algebras, which are obtained by generalizing the work of Dipper and James in the same way that Donkin generalized the classical Schur algebras.Stephen Doty, Presenting generalized q-Schur algebras. Representation Theory 7 (2003), 196--213 (electronic). \n There are further generalizations, such as the affine q-Schur algebrasR. M. Green, The affine q-Schur algebra. Journal of Algebra 215 (1999), 379--411.  related to affine Kac–Moody Lie algebras and other generalizations, such as the cyclotomic q-Schur algebrasRichard Dipper, Gordon James, and Andrew Mathas, Cyclotomic q-Schur algebras. Math. Zeitschrift 229 (1998), 385--416.   related to Ariki-Koike algebras (which are q-deformations of certain complex reflection groups).\n\nThe study of these various classes of generalizations forms an active area of contemporary research.\n\n References \n\n Further reading \n Stuart Martin, Schur Algebras and Representation Theory, Cambridge University Press 1993. , \n Andrew Mathas, Iwahori-Hecke algebras and Schur algebras of the symmetric group, University Lecture Series, vol.15, American Mathematical Society, 1999. , \n Hermann Weyl, The Classical Groups. Their Invariants and Representations. Princeton University Press, Princeton, N.J., 1939. , \n\nCategory:Algebra\nCategory:Representation theory"
    },
    {
      "title": "Series expansion",
      "url": "https://en.wikipedia.org/wiki/Series_expansion",
      "text": "In mathematics, a series expansion is a method for calculating a function that cannot be expressed by just elementary operators (addition, subtraction, multiplication and division).\n\nThe resulting so-called series often can be limited to a finite number of terms, thus yielding an approximation of the function. The  fewer terms of the sequence are used, the simpler this approximation will be. Often, the resulting inaccuracy (i.e., the partial sum of the omitted terms) can be described by an equation involving Big O notation (see also asymptotic expansion). The series expansion on an open interval will also be an approximation for non-analytic functions.\n\nThere are several kinds of series expansions, such as:\n\n Taylor series: A power series based on a function’s derivatives at a single point.\n Maclaurin series: A special case of a Taylor series, centred at zero.\n Laurent series: An extension of the Taylor series, allowing negative exponent values.\n Dirichlet series: Used in number theory.\n Fourier series: Describes periodical functions as a series of sine and cosine functions. In acoustics, e.g., the fundamental tone and the overtones together form an example of a Fourier series.\n Newtonian series\n Legendre polynomials: Used in physics to describe an arbitrary electrical field as a superposition of a dipole field, a quadrupole field, an octupole field, etc.\n Zernike polynomials: Used in optics to calculate aberrations of optical systems. Each term in the series describes a particular type of aberration.\n Stirling series: Used as an approximation for factorials.\n\nFor more details, refer to the articles mentioned.\n\nCategory:Algebra\nCategory:Polynomials\nCategory:Mathematical analysis\nCategory:Mathematical series\nCategory:Series expansions"
    },
    {
      "title": "Series multisection",
      "url": "https://en.wikipedia.org/wiki/Series_multisection",
      "text": "In mathematics, a multisection of a power series is a new power series composed of equally spaced terms extracted unaltered from the original series. Formally, if one is given a power series\n\n \n\nthen its multisection is a power series of the form\n\n \n\nwhere p, q are integers, with 0 ≤ p < q.\n\n Multisection of analytic functions \nA multisection of the series of an analytic function\n\n \n\nhas a closed-form expression in terms of the function :\n\n \n\nwhere  is a primitive q-th root of unity.  This solution was first discovered by Thomas Simpson.  This expression is especially useful in that it can convert an infinite sum into a finite sum.  It is used, for example, in a key step of a standard proof of Gauss's digamma theorem, which gives a closed-form solution to the digamma function evaluated at rational values p/q.\n\n Examples \n\nBisection\nIn general, the bisections of a series are the even and odd parts of the series.\n\nGeometric series\nConsider the geometric series\n\n \n\nBy setting  in the above series, its multisections are easily seen to be\n\n \n\nRemembering that the sum of the multisections must equal the original series, we recover the familiar identity\n\n \n\nExponential function\nThe exponential function\n\n \n\nby means of the above formula for analytic functions separates into\n\n \n\nThe bisections are trivially the hyperbolic functions:\n\n \n \n\nHigher order multisections are found by noting that all such series must be real-valued along the real line.  By taking the real part and using standard trigonometric identities, the formulas may be written in explicitly real form as\n\n \n\nThese can be seen as solutions to the linear differential equation  with boundary conditions , using Kronecker delta notation.  In particular, the trisections are\n\n \n \n \n\nand the quadrusections are\n\n \n \n \n \n\nBinomial theorem\nMultisection of a binomial expansion\n\n \n\nat x = 1 gives the following identity for the sum of binomial coefficients with step q:\n\n \n\n References  \n\nSomos, Michael A Multisection of q-Series, 2006.\n\nCategory:Algebra\nCategory:Combinatorics\nCategory:Mathematical analysis\nCategory:Complex analysis\nCategory:Mathematical series"
    },
    {
      "title": "Shuffle algebra",
      "url": "https://en.wikipedia.org/wiki/Shuffle_algebra",
      "text": "In mathematics, a shuffle algebra is a Hopf algebra with a basis corresponding to words on some set, whose product is given by the shuffle product X ⧢ Y of two words X, Y: the sum of all ways of interlacing them.  The interlacing is given by the riffle shuffle permutation.\n\nThe shuffle algebra on a finite set is the graded dual of the universal enveloping algebra of the free Lie algebra on the set.\n\nOver the rational numbers, the shuffle algebra is isomorphic to the polynomial algebra in the Lyndon words.\n\nShuffle product\nThe shuffle product of words of lengths m and n is a sum over the  ways of interleaving the two words, as shown in the following examples:\nab ⧢ xy = abxy + axby + xaby + axyb + xayb + xyab\naaa ⧢ aa = 10aaaaa\n\nIt may be defined inductively byLothaire (1997) pp.101,126\nu ⧢ ε = ε ⧢ u = u\nua ⧢ vb = (u ⧢ vb)a + (ua ⧢ v)b\nwhere ε is the empty word, a and b are single elements, and u and v are arbitrary words.\n\nThe shuffle product was introduced by . The name \"shuffle product\" refers to the fact that the product can be thought of as a sum over all ways of riffle shuffling two words together: this is the riffle shuffle permutation.  The product is commutative and associative.Lothaire (1997) p.126\n\nThe shuffle product of two words in some alphabet is often denoted by the shuffle product symbol ⧢ (Unicode character U+29E2 , derived from the Cyrillic letter  sha).\n\nInfiltration product\nThe closely related infiltration product was introduced by .  It is defined inductively on words over an alphabet A by\n\nfa ↑ ga = (f ↑ ga)a + (fa ↑ g)a + (f ↑ g)a\nfa ↑ gb = (f ↑ gb)a + (fa ↑ g)b\n\nFor example:\nab ↑ ab = ab + 2aab + 2abb + 4 aabb + 2abab\nab ↑ ba = aba + bab + abab + 2abba + 2baab + baba\n\nThe infiltration product is also commutative and associative.Lothaire (1997) p.128\n\nSee also\n Hopf algebra of permutations\n Zinbiel algebra\n\nReferences\n\nExternal links\n Shuffle product symbol\n\nCategory:Combinatorics\nCategory:Algebra"
    },
    {
      "title": "Smooth algebra",
      "url": "https://en.wikipedia.org/wiki/Smooth_algebra",
      "text": "In algebra, a commutative k-algebra A is said to be 0-smooth if it satisfies the following lifting property: given a k-algebra C, an ideal N of C whose square is zero and a k-algebra map  , there exists a k-algebra map  such that u is v followed by the canonical map. If there exists at most one such lifting v, then A is said to be 0-unramified (or 0-neat). A is said to be 0-étale if it is 0-smooth and 0-unramified.\n\nA finitely generated k-algebra A is 0-smooth over k if and only if Spec A is a smooth scheme over k.\n\nA separable algebraic field extension L of k is 0-étale over k. The formal power series ring  is 0-smooth only when  and  (i.e., k has a finite p-basis.)\n\n I-smooth \nLet B be an A-algebra and suppose B is given the I-adic topology, I an ideal of B. We say B is I-smooth over A if it satisfies the lifting property: given an A-algebra C, an ideal N of C whose square is zero and an A-algebra map   that is continuous when  is given the discrete topology, there exists an A-algebra map  such that u is v followed by the canonical map. As before, if there exists at most one such lift v, then B is said to be I-unramified over A (or I-neat). B is said to be I-étale if it is I-smooth and I-unramified. If I is the zero ideal and A is a field, these notions coincide with 0-smooth etc. as defined above.\n\nA standard example is this: let A be a ring,  and  Then B is I-smooth over A.\n\nLet A be a noetherian local k-algebra with maximal ideal . Then A is -smooth over k if and only if  is a regular ring for any finite extension field  of k.\n\n See also \nétale morphism\nformally smooth morphism\nPopescu’s theorem\n\n References \n\n H. Matsumura Commutative ring theory. Translated from the Japanese by M. Reid. Second edition. Cambridge Studies in Advanced Mathematics, 8.\n\nCategory:Algebra"
    },
    {
      "title": "Spherical design",
      "url": "https://en.wikipedia.org/wiki/Spherical_design",
      "text": "A spherical design, part of combinatorial design theory in mathematics, is a finite set of N points on the d-dimensional unit d-sphere Sd such that the average value of any polynomial f of degree t or less on the set equals the average value of f on the whole sphere (that is, the integral of f over Sd divided by the area or measure of Sd).  Such a set is often called a spherical t-design to indicate the value of t, which is a fundamental parameter.\n\nSpherical designs can be of value in approximation theory, in statistics for experimental design (being usable to construct rotatable designs), in combinatorics, and in geometry.  The main problem is to find examples, given d and t, that are not too large; however, such examples may be hard to come by.\nSpherical t-designs have also recently been appropriated in quantum mechanics in the form of quantum t-designs with various applications to quantum information theory, quantum computing and POVMs.\n\nThe concept of a spherical design is due to Delsarte, Goethals, and Seidel (1977). \n\n Spherical designs in the circle \n\nThe existence and structure of spherical designs with d = 1 (that is, in a circle) were studied in depth by Hong (1982).\n\n Spherical designs in any dimension \n\nShortly thereafter, Seymour and Zaslavsky (1984) proved that such designs exist of all sufficiently large sizes; that is, given positive integers n and t, there is a number N(d,t) such that for every N ≥ N(d,t) there exists a spherical t-design of N points in dimension d.  However, their proof gave no idea of how big N(d,t) is.  Good estimates for that were found later on.  Besides these \"large\" sizes, there are many sporadic small spherical designs; many of them are related to finite group actions on the sphere and are of great interest in themselves.\n\nRecently, Bondarenko, Radchenko, and Viazovska obtained the asymptotic upper bound\n for all positive integers d and t.  This is optimal except that the value of Cd is unknown.\n\n 2-dimensional spherical design \n\nSpherical designs with d = 2 (that is, on the surface of a sphere) ...\n\nOne application of spherical designs is for whole-sphere data collection.\nSpherical t-designs meet the \"accurately approximate integrals by sums\" criteria for \"good\" pixelizations of the sphere.\"An Icosahedron-based Method for Pixelizing the Celestial Sphere\"\nby Max Tegmark 1996\n\nSee also\n Thomson problem\n\nExternal links\n Spherical t-designs for different values of  N and t can be found precomputed at Neil Sloane's website.\n\nNotes\n\nReferences\n.\n. Reprinted in .\n.\n.\n\nCategory:Algebra\nCategory:Design of experiments"
    },
    {
      "title": "Spherically complete field",
      "url": "https://en.wikipedia.org/wiki/Spherically_complete_field",
      "text": "In mathematics, a field K with an absolute value is called spherically complete if the intersection of every decreasing sequence of balls (in the sense of the metric induced by the absolute value) is nonempty:\n\nThe definition can be adapted also to a field K with a valuation v taking values in an arbitrary ordered abelian group: (K,v) is spherically complete if every collection of balls that is totally ordered by inclusion has a nonempty intersection.\n\nSpherically complete fields are important in nonarchimedean functional analysis, since many results analogous to theorems of classical functional analysis require the base field to be spherically complete.\n\nExamples\nAny locally compact field is spherically complete. This includes, in particular, the fields Qp of p-adic numbers, and any of their finite extensions.\nOn the other hand, Cp, the completion of the algebraic closure of Qp, is not spherically complete.Robert, p. 143\nAny field of Hahn series is spherically complete.\n\nReferences\n\nCategory:Algebra\nCategory:Functional analysis"
    },
    {
      "title": "Square (algebra)",
      "url": "https://en.wikipedia.org/wiki/Square_%28algebra%29",
      "text": "thumb|right|168px|, or  (5 squared), can be shown graphically using a square. Each block represents one unit, , and the entire square represents , or the area of the square.\nIn mathematics, a square is the result of multiplying a number by itself. The verb \"to square\" is used to denote this operation. Squaring is the same as raising to the power 2, and is denoted by a superscript 2; for instance, the square of 3 may be written as 32, which is the number 9.\nIn some cases when superscripts are not available, as for instance in programming languages or plain text files, the notations x^2 or x**2 may be used in place of x2.\n\nThe adjective which corresponds to squaring is quadratic.\n\nThe square of an integer may also be called a square number or a perfect square. In algebra, the operation of squaring is often generalized to polynomials, other expressions, or values in systems of mathematical values other than the numbers. For instance, the square of the linear polynomial  is the quadratic polynomial .\n\nOne of the important properties of squaring, for numbers as well as in many other mathematical systems, is that (for all numbers ), the square of  is the same as the square of its additive inverse . That is,  the square function satisfies the identity . This can also be expressed by saying that the square function is an even function.\n\n In real numbers \nthumb|240px|The graph of the square function  is a parabola.\nThe squaring operation defines a real function called the  or the . Its domain is the whole real line, and its image is the set of nonnegative real numbers.\n\nThe square function preserves the order of positive numbers: larger numbers have larger squares. In other words, the square is a monotonic function on the interval . On the negative numbers, numbers with greater absolute value have greater squares, so the square is a monotonically decreasing function on . Hence, zero is the (global) minimum of the square function.\nThe square  of a number  is less than  (that is ) if and only if , that is, if  belongs to the open interval . This implies that the square of an integer is never less than the original number .\n\nEvery positive real number is the square of exactly two numbers, one of which is strictly positive and the other of which is strictly negative. Zero is the square of only one number, itself. For this reason, it is possible to define the square root function, which associates with a non-negative real number the non-negative number whose square is the original number.\n\nNo square root can be taken of a negative number within the system of real numbers, because squares of all real numbers are non-negative. The lack of real square roots for the negative numbers can be used to expand the real number system to the complex numbers, by postulating the imaginary unit , which is one of the square roots of −1.\n\nThe property \"every non-negative real number is a square\" has been generalized to the notion of a real closed field, which is an ordered field such that every non-negative element is a square and every polynomial of odd degree has a root. The real closed fields cannot be distinguished from the field of real numbers by their algebraic properties: every property of the real numbers, which may be expressed in first-order logic (that is expressed by a formula in which the variables that are quantified by ∀ or ∃ represent elements, not sets), is true for every real closed field, and conversely every property of the first-order logic, which is true for a specific real closed field is also true for the real numbers.\n\n In geometry \nThere are several major uses of the square function in geometry.\n\nThe name of the square function shows its importance in the definition of the area:  it comes from the fact that the area of a square with sides of length   is equal to . The area depends quadratically on the size: the area of a shape  times larger is  times greater. This holds for areas in three dimensions as well as in the plane: for instance, the surface area of a sphere is proportional to the square of its radius, a fact that is manifested physically by the inverse-square law describing how the strength of physical forces such as gravity varies according to distance.\n\nthumb|right|Fresnel's zone plates have rings with equally spaced squared distances to the center\nThe square function is related to distance through the Pythagorean theorem and its generalization, the parallelogram law. Euclidean distance is not a smooth function: the three-dimensional graph of distance from a fixed point forms a cone, with a non-smooth point at the tip of the cone. However, the square of the distance (denoted  or ), which has a paraboloid as its graph, is a smooth and analytic function. The dot product of a Euclidean vector with itself is equal to the square of its length: . This is further generalised to quadratic forms in linear spaces. The inertia tensor in mechanics is an example of a quadratic form. It demonstrates a quadratic relation of the moment of inertia to the size (length).\n\nThere are infinitely many Pythagorean triples, sets of three positive integers such that the sum of the squares of the first two equals the square of the third. Each of these triples gives the integer sides of a right triangle.\n\nIn abstract algebra and number theory\nThe square function is defined in any field or ring. An element in the image of this function is called a square, and the inverse images of a square are called square roots.\n\nThe notion of squaring is particularly important in the finite fields  Z/pZ formed by the numbers modulo an odd prime number . A non-zero element of this field is called a quadratic residue if it is a square in Z/pZ, and otherwise, it is called a quadratic non-residue. Zero, while a square, is not considered to be a quadratic residue. Every finite field of this type has exactly  quadratic residues and exactly  quadratic non-residues. The quadratic residues form a group under multiplication. The properties of quadratic residues are widely used in number theory.\n\nMore generally, in rings, the square function may have different properties that are sometimes used to classify rings.\n\nZero may be the square of some non-zero elements. A commutative ring such that the square of a non zero element is never zero is called a reduced ring. More generally, in a commutative ring, a radical ideal is an ideal  such that  implies . Both notions are important in algebraic geometry, because of Hilbert's Nullstellensatz.\n\nAn element of a ring that is equal to its own square is called an idempotent. In any ring, 0 and 1 are idempotents. There are no other idempotents in fields and more generally in integral domains. However, \nthe ring of the integers modulo  has  idempotents, where  is the number of distinct prime factors of .\nA commutative ring in which every element is equal to its square (every element is idempotent) is called a Boolean ring; an example from computer science is the ring whose elements are binary numbers, with bitwise AND as the multiplication operation and bitwise XOR as the addition operation.\n\nIn a supercommutative algebra (away from 2), the square of any odd element equals to zero.\n\nIf A is a commutative semigroup, then one has\n\nIn the language of quadratic forms, this equality says that the square function is a \"form permitting composition\". In fact, the square function is the foundation upon which other quadratic forms are constructed which also permit composition. The procedure was introduced by L. E. Dickson to produce the octonions out of quaternions by doubling. The doubling method was formalized by A. A. Albert who started with the real number field ℝ and the square function, doubling it to obtain the complex number field with quadratic form x2 + y2, and then doubling again to obtain quaternions. The doubling procedure is called the Cayley–Dickson process and the structures produced are composition algebras.\n\nThe square function can be used with ℂ as the start for another use of the Cayley–Dickson process leading to bicomplex, biquaternion, and bioctonion composition algebras.\n\nIn complex numbers and related algebras over the reals\n\nThe complex square function  is a twofold cover of the complex plane, such that each non-zero complex number has exactly two square roots. This map is related to parabolic coordinates.\n\n Other uses \nSquares are ubiquitous in algebra, more generally, in almost every branch of mathematics, and also in physics where many units are defined using squares and inverse squares: see below.\n\nLeast squares is the standard method used with overdetermined systems.\n\nSquaring is used in statistics and probability theory in determining the standard deviation of a set of values, or a random variable. The deviation of each value  from the mean  of the set is defined as the difference . These deviations are squared, then a mean is taken of the new set of numbers (each of which is positive). This mean is the variance, and its square root is the standard deviation. In finance, the volatility of a financial instrument is the standard deviation of its values.\n\nSee also\n\n Exponentiation by squaring\n Polynomial SOS, the representation of a non-negative polynomial as the sum of squares of polynomials\n Hilbert's seventeenth problem, for the representation of positive polynomials as a sum of squares of rational functions\n Square-free polynomial\n Cube (algebra)\n Metric tensor\n Quadratic equation\n Polynomial ring\n Sums of squares (disambiguation page with various relevant links)\n\n Related identities \n Algebraic (need a commutative ring)\n Difference of two squares\n Brahmagupta–Fibonacci identity, related to complex numbers in the sense discussed above\n Euler's four-square identity, related to quaternions in the same way\n Degen's eight-square identity, related to octonions in the same way\n Lagrange's identity\n Other\n Pythagorean trigonometric identity\n Parseval's identity\n\n Related physical quantities \n acceleration, length per square time\n cross section (physics), an area-dimensioned quantity\n coupling constant (has square charge in the denominator, and may be expressed with square distance in the numerator)\n kinetic energy (quadratic dependence on velocity)\n specific energy, a (square velocity)-dimensioned quantity\n\n Footnotes \n\nFurther reading\n Marshall, Murray Positive polynomials and sums of squares. Mathematical Surveys and Monographs, 146. American Mathematical Society, Providence, RI, 2008. xii+187 pp. , \n \n\nCategory:Algebra\nCategory:Elementary arithmetic\n2\nCategory:Squares in number theory\nCategory:Unary operations"
    },
    {
      "title": "Stereotype algebra",
      "url": "https://en.wikipedia.org/wiki/Stereotype_algebra",
      "text": "In functional analysis and related areas of mathematics, stereotype algebras are topological algebras defined as stereotype spaces with a (unital and associative) multiplication satisfying the condition of separate uniform continuity on compact sets.\n\n Definition \nThere are two equivalent ways to define stereotype algebra. \n\n An abstract definition: a (projectiveThere is another notion, an injective stereotype algebra, which is defined as a monoid in the monoidal category (Ste,,) of stereotype spaces with the dual tensor product, . The class of these algebras is much narrower.) stereotype algebra is a monoid in the monoidal category (Ste,,) of stereotype spaces with the tensor product .\n\n An analytic definition: let  be a stereotype space (over the field  of complex numbers...or the field  of real numbers) and suppose it is endowed with a bilinear operation  that turns  into a unital and associative algebra (over the field ). Then  is called a (projective) stereotype algebra if the multiplication  is a continuous bilinear map in the stereotype sense, i.e. if for each compact set  and for each neighbourhood of zero  there is a neighbourhood of zero  such that The requirement on  in the analytic definition of stereotype algebra can be equivalently expressed as follows: for each compact set  and for each net  tending to zero in ,\n \nthe nets  and  tend to zero in  uniformly on :\n\nA morphism of stereotype algebras  and  is a continuous unitalA homomorphism  is said to be unital if it preserves unit: . homomorphism .\n\nStereotype algebras form a subclass in the class of separately continuous topological algebras.\n\n Examples \n\n1. All Fréchet algebras are stereotype. In particular, all Banach algebras are stereotype.  \n2. For each stereotype space  the stereotype space of operators , , is a stereotype algebra with respect to the operation of composition.\n3. The main examples of functional algebras in Analysis and Geometry are stereotype algebras: \n the algebra  of continuous functions on a paracompact locally compact topological space  with the topology of uniform convergence on compact sets , \n the algebra  of smooth functions on a smooth manifold  with the topology of uniform convergence with all derivatives on compact sets , \n the algebra  of holomorphic functions on a Stein manifold  with the topology of uniform convergence on compact sets , \n the algebra  of polynomials (regular functions) on an affine algebraic variety  with the strongest locally convex topology. \n4. Stereotype group algebras. In the case when the space  in the previous examples has a structure of a topological group, the dual stereotype spaces to the functional algebras on  are stereotype algebras with respect to the operation of convolution: \n the algebra  of Radon measures with compact support on a locally compact group ,If  is an infinite locally compact group then the algebra  of measures on  is not a Fréchet algebra. In the case when  is compact,  is a Smith space. If  is -compact, then  is a Brauner space. \n the algebra  of distributions with compact support on a real Lie group , \n the algebra  of holomorphic functionals on a Stein groupA Stein group is a complex Lie group  which is a Stein manifold. , \n the algebra  of currents on an affine algebraic group .\nIn contrast to the other models of group algebras in functional analysis, the stereotype group algebras possess the universal propertyI.e. the homomorphisms of stereotype algebras  are in one-to-one correspondence with the continuous representations  of the group , and the same is true for the algebras ,  and  with the smooth, holomorphic and polynomial representations of the corresponding groups. and the structure of Hopf algebra.\n\n Stereotype modules \n\nA stereotype space  is called a left stereotype module over a stereotype algebra , if a bilinear operation  is defined that turns  into a left module over , and this operation is a continuous bilinear map in the stereotype sense: for each compact set  and for each neighbourhood of zero  there is a neighbourhood of zero  such that \n\nand at the same time for each compact set  and for each neighbourhood of zero  there is a neighbourhood of zero  such that \n\nThe notion of a right stereotype module over a stereotype algebra  is defined by analogy.\n\nThe theory of stereotype algebras allows to simplify the formulations in the theory of topological algebras and to bring this field closer to abstract algebra. The following two results (which do not hold for general jointly or separately continuous algebras) are typical illustrations.Other illustrations are the already mentioned universality property for the stereotype group algebras and the structure of Hopf algebra on them.\n\nTheorem. A stereotype space  with a structure of left (right) module over a stereotype algebra  is a stereotype module over  if and only if the operation of multiplication  (respectively, ) defines a continuous map (a representation) .Here  is the stereotype space of operators .\n\nTheorem. For each stereotype algebra  the categories Ste and Ste of left and right stereotype modules over  are enriched categories over the monoidal category (Ste,,) of stereotype spaces.\n\n Applications \n\nThe notion of stereotype algebra is used in the generalizations of the Pontryagin  duality theory to the classes of non-commutative groups based on the notion of envelope: the holomorphic, the smooth and the continuous envelopes of stereotype algebras lead respectively to the constructions of the holomorphic, the smooth and the continuous dualities in big geometric disciplines – complex geometry, differential geometry, and topology – for certain classes of (not necessarily commutative) topological groups considered in these disciplines (affine algebraic groups, and some classes of Lie groups and Moore groups).\n\nSee also\n\n Stereotype space\n Topological algebra\n Fréchet algebra\n Banach algebra\n C*-algebra\n Operator algebra\n\nNotes\n\n References \n\nCategory:Category theory\nCategory:Functional analysis\nCategory:Algebra\nCategory:Duality theories\nCategory:Monoidal categories"
    },
    {
      "title": "Stereotype space",
      "url": "https://en.wikipedia.org/wiki/Stereotype_space",
      "text": "In functional analysis and related areas of mathematics, stereotype spaces are topological vector spaces defined by a special variant of reflexivity condition. They form a class of spaces with a series of remarkable properties, in particular, this class is very wide (for instance, it contains all Fréchet spaces and thus, all Banach spaces), it consists of spaces satisfying a natural condition of completeness, and it forms a *-autonomous category with the standard analytical tools for constructing new spaces, like taking dual spaces, spaces of operators, tensor products, and in addition, immediate subspaces, immediate quotient spaces, products and coproducts, limits and colimits, etc. \nthumb|Mutual embeddings of the main classes of locally convex spaces.\n\nDefinition\n\nA stereotype space is a topological vector space  over the field  of complex numbers...or over the field  of real numbers, with the similar definition. such that the natural map into the second dual space\n\nis an isomorphism of topological vector spaces (i.e. a linear and a homeomorphic map). Here the dual space  is defined as the space of all linear continuous functionals  endowed with the topology of uniform convergence on totally bounded sets in , and the second dual space  is the space dual to  in the same sense.\n\nThe following criterion holds: a topological vector space  is stereotype if and only if it is locally convex and satisfies the following two conditions:\n\n pseudocompleteness: each totally bounded Cauchy net in  converges,\n\n pseudosaturateness: each closed convex balanced capaciousA set  is said to be capacious if for each totally bounded set  there is a finite set  such that . set  in  is a neighborhood of zero in .\n\nThe property of being pseudocomplete is a weakening of the usual notion of completeness, while the property of being pseudosaturated is a weakening of the notion of barreledness of a topological vector space.\n\nExamples\nThe class Ste of stereotype spaces is extremely wide, so that it will not be a serious exaggeration to say that all topological vector spaces really used in analysis are stereotype. Each pseudocomplete barreled space  (in particular, each Banach space and each Fréchet space) is stereotype. Its dual space  (which is not barreled, unless  is a Montel space) is stereotype as wellOf course, this is a general fact: if  is stereotype then  is also stereotype.. There exist stereotype spaces which are not Mackey spaces.\n\nSome simple connections between the properties of a stereotype space  and those of its dual space  are expressed in the following list of regularities:\n\n  is a normed space    is a Banach space   is a Smith space;\n\n  is metrizable    is a Fréchet space    is a Brauner space;\n\n  is barreled    has the Heine-Borel property;\n\n  is quasi-barreled  in  if a set  is absorbed by each barrel, then  is totally bounded;\n\n  is a Mackey space  in  every -weakly compact set is compact;\n\n  is a Montel space   is barreled and has the Heine-Borel property    is a Montel space;\n\n  is a space with a weak topology  in  every compact set  is finite-dimensional;\n\n  is separable  in  there is a sequence of closed subspaces  of finite co-dimension with trivial intersection: .\n\n  has the (classical) approximation property   has the (classical) approximation property;\n\n  is complete   is co-completeA locally convex space   is called co-complete if each linear functional  which is continuous on every totally bounded set , is automatically continuous on the whole space .    is saturated;A locally convex space  is said to be saturated if for an absolutely convex set  being a neighbourhood of zero in  is equivalent to the following: for each totally bounded set   there is a closed neighbourhood of zero  in  such that .\n\n  is a Pták spaceA locally convex space  is called a Pták space, or a fully complete space, if in its dual space   a subspace  is -weakly closed when it has -weakly closed intersection with the polar  of each neighbourhood of zero .  in  a subspace  is closed if it has the closed intersection  with each compact set ;\n\n  is hypercompleteA locally convex space  is said to be hypercomplete if in its dual space  every absolutely convex space  is -weakly closed if it  has -weakly closed intersection with the polar  of each neighbourhood of zero  .  in  an absolutely convex set   is closed if it has the closed intersection  with each compact set .\n\nCounterexamples: \n\n1. If a metrizable locally convex space  is not complete, then it is not stereotype. \n\n2. If  is an infinite dimensional Banach  space, and  is its dual space (of linear continuous functionals ) considered with the -weak topology, then  is not stereotype.\n\nHistory\n\nThe first results on this type of reflexivity of topological vector spaces were obtained by M. F. Smith in 1952. Further investigations were conducted by B. S. Brudovskii,\n W. C. Waterhouse, K. Brauner, S. S. Akbarov, and E. T. Shavgulidze. The term \"stereotype space\" was introduced by S. S. Akbarov in 1995. The main properties of the category of stereotype spaces were described by S. S. Akbarov in his series of works of 1995-2017.\n\nPseudocompletion and pseudosaturation\n\nEach locally convex space  can be transformed into a stereotype space with the help of two standard operations, pseudocompletion and pseudosaturation, defined by the following two propositions.\nthumb\nTheorem. For each locally convex space  there exists a pseudocomplete locally convex space  and a linear continuous mapping  such that for every pseudocomplete locally convex space  and for every linear continuous mapping  there is a unique linear continuous mapping  such that\n.\nThe space  is called a pseudocompletion of the space . It is unique up to an isomorphism of locally convex spaces. \nthumb\nFor each linear continuous mapping of locally convex spaces  there is a unique linear continuous mapping  such that \n,\nand the correspondence  can be defined as a (covariant) functor.\n\nThe pseudocompletion  can be defined as an envelope of the locally convex space  in the class  of all pseudocomplete locally convex spaces with respect to the same class :\n\nOne can imagine the pseudocompletion of  as the \"nearest to  from the outside\" pseudocomplete locally convex space, so that the operation  adds to  some supplementary elements, but does not change the topology of  (like the usual operation of completion).\nthumb\nTheorem. For each locally convex space  there is a pseudosaturated locally convex space  and a linear continuous mapping  such that for each pseudosaturated locally convex space  and for each linear continuous mapping  there is a unique linear continuous mapping  such that\nThe space  is called a pseudosaturation of the space . It is unique up to an isomorphism of locally convex spaces.\nthumb\nFor each linear continuous mapping of locally convex spaces  there is a unique linear continuous mapping  such that\n, \nand the correspondence  can be defined as a (covariant) functor.\n\nThe pseudosaturation  can be defined as a refinement of the locally convex space  in the class  of all pseudosaturated locally convex spaces with respect to the same class :\n\nOne can imagine the pseudosaturation of  as the \"nearest to  from the inside\" pseudosaturated locally convex space, so that the operation  strengthens the topology of , but does not change the elements of .\n\nIf  is a pseudocomplete locally convex space, then its pseudosaturation  is stereotype. Dually, if  is a pseudosaturated locally convex space, then its pseudocompletion   is stereotype. For arbitrary locally convex space  the spaces   and  are stereotype.It is not clear (2017) whether  and  coincide.\n\nImmediate subspaces and immediate quotient spaces\nThe idea of subspace (and of quotient space) in stereotype theory leads to more complicated results than in the theory of locally convex spaces.\n\nImmediate subspaces and envelopes\nThe notion of immediate subspace gives a \"concrete description\" of the abstract notion of immediate monomorphismA monomorphism  is said to be immediate if in each representation , where  is a monomorphism and  is an epimorphism, the morphism  is automatically an isomorphism., or, what is equivalent in this situation, strong monomorphismthumb A monomorphism  is said to be strong, if for any epimorphism  and for any morphisms   and  such that  there exists a morphism , such that  and . in the category Ste. Surprisingly, this description does not coincide with the construction of closed subspace in the category LocConv of locally convex spaces.\n\n Suppose  is a subset in a stereotype space  endowed with a structure of a stereotype space in such a way that the set-theoretic inclusion  is a morphism of stereotype spaces (i.e. a continuous linear map). Then  is called a subspace of the stereotype space , with the notation\n\n.\n\n Suppose we have a chain of stereotype subspaces \n\n,\n\nand the first mapping  is a bimorphism of stereotype spaces. Then the space  is called a mediator of the subspace  in the space .\n\n A subspace  in a stereotype space  is called an immediate subspace in ,  with the notation\n,\nif it has no non-trivial mediators, i.e. for any mediator  of  in  the inclusion  is an isomorphism.\n\nExamples:\n\n1. An immediate subspace  in a stereotype space  is said to be closed, if  (as a set) is closed in  (as a topological space). If  is a closed subspace in a stereotype space  (as in a locally convex space), then its pseudosaturation  is a closed immediate subspace in . All closed immediate subspaces have this form.\n \n2. There are stereotype spaces  with closed immediate subspaces  whose topology is not inherited from In other words, in this case the topology of  inherited from  is not pseudosaturated. (this is one of the qualitative differences with the category LocConv of locally convex spaces).\n\n3. In contrast to the category LocConv of locally convex spaces in the category Ste the immediate subspaces are not always closed.\n\nTheorem. For any set  in a stereotype space  there is a minimal immediate subspace  in , containing :\n\n(i) \n\n(ii) ,\n\nand this subspace  is an immediate subspace in each immediate subspace, containing :\n\n(iii) ,\n\n The subspace  is called an envelope of the set  in the stereotype space . \n\nTheorem. Each set  in a stereotype space  is a total setI.e. the linear span of  is dense in   (as in a locally convex space). in its envelope .\n\nIf  denotes the space of all functions  with finite support, endowed with the strongest locally convex topology, and the mapping   acts by the formula , then the envelope  coincides with the abstract categorical envelope of the space  in the class  of all epimorphisms in the category Ste with respect to the morphism :\n \n\nImmediate quotient spaces and refinements\nDually, the notion of immediate quotient space gives a \"concrete description\" of the abstract notion of immediate epimorphismAn epimorphism  is said to be immediate if in each representation , where  is a monomorphism and  is an epimorphism, the morphism  is automatically an isomorphism., or, what is equivalent here, strong epimorphismthumb An epimorphism  is said to be strong, if for any monomorphism  and for any morphisms  and  such that  there exists a morphism , such that  and .  in the category Ste. Like in the situation with monomorphisms, this description does not coincide with the construction of quotient space in the category LocConv of locally convex spaces.\n\n Let  be a closed subspace (in the usual sense) in a stereotype space . Consider a topology  on the quotient space , which is majorized by the usual quotient topology of . Let  be a completion of  with respect to the topology . Suppose  is a subset in the locally convex space  which contains  and at the same time is a stereotype space. Then  is called a quotient space of the stereotype space ,  with the notation\n\n.\n\n Suppose we have two quotient spaces  and . It is said that the  subordinates  (notation: ) if there is a morphism  such that  (where  and  are the natural mappings).\n\n Suppose that the quotient space  subordinates the quotient space  (i.e. ) and the corresponding morphism  is a bimorphism. Then the quotient space  is called a mediator of the quotient space  of the space .\n\n A quotient space  of a stereotype space  is called an immediate quotient space of , with the notation\n,\nif it has no non-trivial mediators, i.e. for any mediator  of  the morphism  is an isomorphism.\n\nExamples:\n\n1. An immediate quotient space  of a stereotype space  is said to be open, if the corresponding map  is openA linear map  is said to be open, if for each neighborhood of zero  there is a neighborhood of zero  such that .. If  is a closed subspace in a stereotype space , then the pseudocompletion  of the (locally convex) quotient space  is an open immediate quotient space of . All open immediate quotient spaces have this form. \n\n2. There are stereotype spaces  with immediate quotient spaces  which cannot be represented in the form .  \n\n3. In contrast to the category LocConv of locally convex spaces in the category Ste immediate quotient spaces are not always open. \n\nTheorem. For any set  of linear continuous functionals on a stereotype space  there is a minimal immediate quotient space  of  to which all functionals  can be extended:\n\n(i) \n\n(ii) ,\n\nand this quotient space  is (up to an isomorphism) an immediate quotient space of each immediate quotient space, to which the functionals  are extended:\n\n(iii) .\n\n The quotient space  is called a refinement of the set  on the stereotype space . \n\nTheorem.This proposition is dual to the corresponding theorem on envelopes in stereotype spaces. Each set  of linear continuous functionals on a stereotype space  is a total setI.e. if  , then there exists  such that . on its refinement .\n\nIf  denotes the space of all functions , endowed with the topology of pointwise convergence, and the mapping  acts by the formula , then the refinement  coincides with the abstract categorical refinement of the space  in the class  of all monomorphisms in the category Ste by means of the morphism :\n \n\nCategory Ste of stereotype spaces\nThe class Ste of stereotype spaces forms a category with linear continuous maps as morphisms and possesses the following properties:\n\n Ste is pre-abelian;\n\n Ste is bicomplete;\n\n Ste is autodual with respect to the functor ;\n\n Ste is a category with nodal decomposition.\n\n Ste is a *-autonomous category.\n\nKernel and cokernel in the category Ste\nSte is a pre-abelian category: each morphism  in the category Ste has a kernel\n\nand a cokernel  \n\nAs a corollary,  has an image  and a coimage  as well. The following natural identities hold:\n \nwhere  denotes the pseudosaturation of the annihilator of the subspace  in the dual space :\n .\n\nSte as a *-autonomous category\nFor any two stereotype spaces  and  the stereotype space of operators  from  into , is defined as the pseudosaturation of the space  of all linear continuous maps  endowed with the topology of uniform convergeance on totally bounded sets. The space  is stereotype. It defines two natural tensor products\n\nTheorem. In the category  Ste the following natural identities hold::\n\nIn particular, Ste is a symmetric monoidal category with respect to the bifunctor , a closed symmetric monoidal category with respect to the bifunctor  and the internal hom-functor , and a *-autonomous category:\n\nExamples:\n\n1. If  and  are Fréchet spaces, then their stereotype tensor product  coincides with the usual projective tensor product  of locally convex spaces  and .\n\n2. If  and  are Fréchet spaces and at least one of them possesses the (classical) approximation property, then their stereotype tensor product  coincides with the usual injective tensor product  of locally convex spaces  and .\n\nLimits and colimits in the category Ste\nSte is a bicomplete category: each small diagram Ste has a limit, , which coincides with the pseudocompletion of the corresponding limit in the category LocConv of locally convex spaces \n, \nand a colimit, , which coincides with the pseudosaturation of the corresponding colimit in LocConv\n. \nHowever, the direct sum and the direct product in Ste coincide with the corresponding constructions in LocConv:\n\nThe following natural identities hold:\n\nGrothendieck transformation\n\nIf  and  are stereotype spaces then for each elements  and  the formula \n\ndefines an elementary tensor , and the formula\n\ndefines an elementary tensor \n\nTheorem. For each stereotype spaces   and  there is a unique linear continuous map  which turns elementary tensors  into elementary tensors :\n\nThe family of maps  defines a natural transformation of the bifunctor  into the bifunctor .\n\nThe map  is called the Grothendieck transformation.\n\nStereotype approximation property\nA stereotype space  is said to have the stereotype approximation property, if each linear continuous map  can be approximated in the stereotype space of operators  by the linear continuous maps of finite rank. This condition is weaker than the existence of the Schauder basis, but formally stronger than the classical approximation property (however, it is not clear (2017) whether the stereotype approximation property coincides with the classical one, or not). \n\nTheorem. For a stereotype space  the following conditions are equivalent:  \n(i)  has the stereotype approximation property;\n(ii) the Grothendieck transformation  is a monomorphism (in the category Ste);\n(iii) the Grothendieck transformation  is an epimorphism (in the category Ste);\n(iv) for any stereotype space  the Grothendieck transformation  is a monomorphism (in the category Ste);\n(v) for any stereotype space  the Grothendieck transformation  is an epimorphism (in the category Ste).\n\nTheorem. If two stereotype spaces  and  have the stereotype approximation property, then the spaces ,  and  have the stereotype approximation property as well.\n\nIn particular, if  has the stereotype approximation property, then the same is true for  and for .\n\nUniversality of tensor product\nFor any stereotype spaces , ,  a bilinear map  is said to be continuous (as a bilinear map of stereotype spaces) if \n\n 1) for each neighborhood of zero  and for each compact set  there exists a neighborhood of zero   such that , and\n\n 2) for each neighborhood of zero  and for each compact set  there exists a neighborhood of zero  such that .\n\nExamples:\n\n1. For any stereotype space  the pairing  is a continuous bilinear map.\n\n2. For any two stereotype spaces  and  the map  is a continuous bilinear map.\n\n3. For any two stereotype spaces  and  the map  is a continuous bilinear map.\n\nthumb\nTheorem. For any stereotype spaces , ,  and for any continuous bilinear map  there exists a unique continuous linear map  such that , where .\n\nCorollary. For any stereotype space  the pairing  has a unique extension to a linear continuous functional . This functional in its turn can be represented as a trace of the operators  occurring as images of the tensors  under the Grothendieck transformation  if and only if the space  has the stereotype approximation property.\n\nApplications\n\nBeing a symmetric monoidal category, Ste generates the notions of a stereotype algebra (as a monoid in Ste) and a stereotype module (as a module in Ste over such a monoid), and it turns out that for each stereotype algebra  the categories Ste and Ste of left and right stereotype modules over  have the structure of enriched categories over Ste. This distinguishes the category Ste from the other known categories of locally convex spaces, since up to the recent time only the category Ban of Banach spaces and the category Fin of finite-dimensional spaces had been known to possess this property. On the other hand, the category Ste is so wide, and the tools for creating new spaces in Ste are so diverse, that this suggests the idea that all the results of functional analysis can be reformulated inside the stereotype theory without essential losses. On this way one can even try to completely replace the category of locally convex spaces in analysis (and in related areas) by the category Ste of stereotype spaces with the view of possible simplifications – this program was announced by S. Akbarov in 2005 and the following results can be considered as evidences of its reasonableness:\n\n In the theory of stereotype spaces the approximation property is inherited by the spaces of operators and by tensor products. This allows to reduce the list of counterexamples in comparison with the Banach theory, where as is known the space of operators does not inherit the approximation property.\n\n The arising theory of stereotype algebras allows to simplify constructions in the duality theories for non-commutative groups. In particular, the group algebras (and their envelopes in the necessary cases) in these theories become Hopf algebras in the standard algebraic sense. \n\n This in its turn leads to a family of generalizations of the Pontryagin duality based on the notion of envelope: the  holomorphic, the smooth and the continuous envelopes of stereotype algebras give rise respectively to the holomorphic, the smooth and the continuous dualities in big geometric disciplines – complex geometry, differential geometry, and topology – for certain classes of (not necessarily commutative) topological groups considered in these disciplines (affine algebraic groups, and some classes of Lie groups and Moore groups).\n\nSee also\nStereotype algebra\nStereotype group algebra\nReflexive space\nDuality (mathematics)\nPontryagin duality\nEnvelope (category theory)\nRefinement (category theory)\nBrauner space\nSmith space\n\nNotes\n\n External links \n \n\nReferences\n\nCategory:Algebra\nCategory:Duality theories\nCategory:Functional analysis\nCategory:Monoidal categories"
    },
    {
      "title": "Straight-line program",
      "url": "https://en.wikipedia.org/wiki/Straight-line_program",
      "text": "In mathematics, more specifically in computational algebra, a straight-line program (SLP) for a finite group G = 〈S〉 is a  finite sequence L of elements of G such that every element of L either belongs to S, is the inverse of a preceding element, or the product of two preceding elements. An SLP L is said to compute a group element g ∈ G if g ∈ L, where g is encoded by a word in S and its inverses.\n\nIntuitively, an SLP computing some g ∈ G is  an efficient way of storing g as a group word over S; observe that if g is constructed in i steps, the word length of g may be exponential in i, but the length of the corresponding SLP is linear in i. This has important applications in computational group theory, by using SLPs to efficiently encode group elements as words over a given generating set.\n\nStraight-line programs were introduced by Babai and Szemerédi in 1984Babai, László, and Endre Szemerédi. \"On the complexity of matrix group problems I.\" Foundations of Computer Science, 1984. 25th Annual Symposium on Foundations of Computer Science. IEEE, 1984 as a tool for studying the computational complexity of certain matrix group properties. Babai and Szemerédi prove that every element of a finite group G has an SLP of length O(log2|G|) in every generating set.\n\nAn efficient solution to the constructive membership problem is crucial to many group-theoretic algorithms. It can be stated in terms of SLPs as follows. Given a finite group G = 〈S〉 and g ∈ G, find a straight-line program computing g over S. The constructive membership problem is often studied in the setting of black box groups. The elements are encoded by bit strings of a fixed length.  Three oracles are provided for the group-theoretic functions of multiplication, inversion, and checking for equality with the identity. A black box algorithm is one which uses only these oracles. Hence, straight-line programs for black box groups are black box algorithms.\n\nExplicit straight-line programs are given for a wealth of finite simple groups in the online ATLAS of Finite Groups.\n\nDefinition\n\nInformal Definition\nLet G be a finite group and let S be a subset of G.  A  sequence L = (g1,…,gm) of elements of G is a straight-line program over S if each gi can be obtained by one of the following three rules: \n gi ∈ S\n gi = gj  gk for some j,k < i \n gi = g for some j < i.\nThe straight-line cost c(g|S) of an element g ∈ G is the length of a shortest straight-line program over S computing g. The cost is infinite if g is not in the subgroup generated by S.\n\nA straight-line program is similar to a derivation in predicate logic. The elements of S correspond to axioms and the group operations correspond to the rules of inference.\n\nFormal definition\n\nLet G be a finite group and let S be a subset of G. A straight-line program of length m over S computing some g ∈ G is a sequence of expressions (w1,…,wm) such that for each i, wi is a symbol for some element of S, or wi = (wj,-1) for some j < i, or wi = (wj,wk) for some j,k < i, such that wm takes upon the value g when evaluated in G in the obvious manner.\n\nThe original definition appearing in Ákos Seress. (2003). Permutation Group Algorithms. [Online]. Cambridge Tracts in Mathematics. (No. 152). Cambridge: Cambridge University Press. requires that G =〈S〉. The definition presented above is a common generalisation of this.\n\nFrom a computational perspective, the formal definition of a straight-line program has some advantages. Firstly, a sequence of abstract expressions requires less memory than terms over the generating set. Secondly, it allows straight-line programs to be constructed in one representation of G and evaluated in another. This is an important feature of some algorithms.\n\nExamples\n\nThe dihedral group  D12 is  the group of symmetries of a hexagon. It can be generated by a 60 degree rotation ρ and one reflection λ. The leftmost column of the following is a straight-line program for λρ3:\n\n λ\n ρ\n ρ2\n ρ3\n λρ3\n\n λ is a generator.\n ρ is a generator.\n Second rule: (2).(2)\n Second rule: (3).(2)\n Second rule: (1).(4)\n\nIn S6, the group of permutations on six letters, we can take α=(1 2 3 4 5 6) and β=(1 2) as generators. The leftmost column here is an example of a straight-line program to compute (1 2 3)(4 5 6):\n\n α\n β\n α2 \n α2β\n α2βα\n α2βαβ\n α2βαβα2βαβ \n\n (1 2 3 4 5 6)\n (1 2)\n (1 3 5)(2 4 6)\n (1 3 5 2 4 6)\n (1 4)(2 5 3 6)\n (1 4 2 5 3 6)\n (1 2 3)(4 5 6)\n\n α is a generator\n β is a generator\n Second rule: (1).(1)\n Second rule: (3).(2)\n Second rule: (4).(1)\n Second rule: (5).(2)\n Second rule: (6).(6)\n\nApplications\n\nShort descriptions of finite groups. Straight-line programs can be used to study compression of finite groups via  first-order logic. They provide a tool to construct \"short\" sentences describing G (i.e. much shorter than |G|). In more detail, SLPs are used to prove that every finite simple group has a first-order description of length O(log|G|), and every finite group G has a first-order description of length O(log3|G|).Nies, A., & Tent, K. (2016). Describing finite groups by short first-order sentences. Israel J. Mathematics, to appear. https://arxiv.org/abs/1409.8390\n\nStraight-line programs computing generating sets for maximal subgroups of finite simple groups. The online ATLAS of Finite Group Representationshttp://brauer.maths.qmul.ac.uk/Atlas/v3/ provides abstract straight-line programs for computing generating sets of maximal subgroups for many finite simple groups.\n\nExample: The group Sz(32), belonging to the infinite family of Suzuki groups, has rank 2 via generators a and b, where a has order 2, b has order 4, ab has order 5, ab2 has order 25 and abab2ab3 has order 25. The following is a straight-line program that computes a generating set for a maximal subgroup E32E32C31. This straight-line program can be found in the online ATLAS of Finite Group Representations.\n\n a\n b\n ab\n abb\n ababb\n ababbb\n (abb)18\n (abb)−18\n (abb)−18b\n (abb)−18b(abb)18\n (ababb)14\n (ababb)−14\n (ababb)−14ababbb\n (ababb)−14ababbb(ababb)14\n\n a is a generator.\n b is a generator.\n Second rule: (1).(2)\n Second rule: (3).(2)\n Second rule: (3).(4)\n Second rule: (5).(2)\n Second rule iterated: (4) multiplied 18 times\n Third rule: (7) inverse\n Second rule: (8).(2)\n Second rule: (9).(7)\n Second rule iterated: (5) multiplied 14 times\n Third rule: (11) inverse\n Second rule: (12).(6)\n Second rule: (13).(11)\n\nReachability theorem\nThe reachability theorem states that, given a finite group G generated by S, each g ∈ G has a maximum cost of . This can be understood as a bound on how hard it is to generate a group element from the generators.\n\nHere the function lg(x) is an integer-valued version of  the logarithm function: for k≥1 let lg(k) = max{r : 2r ≤ k}.\n\nThe idea of the proof is to construct a set Z = {z1,…,zs} that will work as a new generating set (s will be defined during the process). It is usually larger than S, but any element of G can be expressed as a word of length at most  over Z. The set Z is constructed by inductively defining an increasing sequence of sets K(i).\n\nLet K(i) = {z1α1·z2α2·…·ziαi : αj ∈ {0,1}}, where zi is the group element added to Z at the i-th step. Let c(i) denote the length of a shortest straight-line program that contains Z(i) = {z1,…,zi}. Let K(0) = {1G} and c(0)=0. We define the set Z recursively:\n If K(i)−1K(i) = G, declare s to take upon the value i and stop.\n Else, choose some zi+1 ∈ G\\K(i)−1K(i) (which is non-empty) that minimises the \"cost increase\" c(i+1) − c(i).\n\nBy this process, Z is defined in a way so that any g ∈ G can be written as an element of K(i)−1K(i), effectively making it easier to generate from Z.\n\nWe now need to verify the following claim to ensure that the process terminates within lg(|G|) many steps:\n\nThe next claim is used to show that the cost of every group element is within the required bound.\n\nIt takes at most 2i steps to generate g1 ∈ K(i)−1K(i). There is no point in generating the element of maximum length, since it is the identity. Hence  steps suffice. To generate g1·g2 ∈ G\\K(i)−1K(i), 2i steps  are sufficient.\n\nWe now finish the theorem. Since K(s)−1K(s) = G, any g ∈ G can be written in the form k·k2 with k,k2 ∈ K(s). By Corollary 2, we need at most  steps to generate Z(s) = Z, and no more than  steps to generate g from Z(s).\n\nTherefore .\n\nReferences\n\nCategory:Algebra"
    },
    {
      "title": "Subfield of an algebra",
      "url": "https://en.wikipedia.org/wiki/Subfield_of_an_algebra",
      "text": "In algebra, a subfield of an algebra A over a field F is an F-subalgebra that is also a field. A maximal subfield is a subfield that is not contained in a strictly larger subfield of A.\n\nIf A is a finite-dimensional central simple algebra, then a subfield E of A is called a strictly maximal subfield if .\n\n References \n Richard S. Pierce. Associative algebras. Graduate texts in mathematics, Vol. 88, Springer-Verlag, 1982, \n\nCategory:Algebra"
    },
    {
      "title": "Substitution (algebra)",
      "url": "https://en.wikipedia.org/wiki/Substitution_%28algebra%29",
      "text": "In algebra, the operation of substitution can be applied in various contexts involving formal objects containing symbols (often called variables or indeterminates); the operation consists of systematically replacing occurrences of some symbol by a given value.\n\nSubstitution is a basic operation of computer algebra. It is generally called \"subs\" or \"subst\" in computer algebra systems.\n\nA common case of substitution involves polynomials, where substitution of a numerical value for the indeterminate of a (univariate) polynomial amounts to evaluating the polynomial at that value. Indeed this operation occurs so frequently that the notation for polynomials is often adapted to it; instead of designating a polynomial by a name like P, as one would do for other mathematical objects, one could define\n\nso that substitution for X can be designated by replacement inside \"P(X)\", say\n\nor\n.\nSubstitution can however also be applied to other kinds of formal objects built from symbols, for instance elements of free groups. In order for substitution to be defined, one needs an algebraic structure with an appropriate universal property, that asserts the existence of unique homomorphisms that send indeterminates to specific values; the substitution then amounts to finding the image under such a homomorphism.\n\nSubstitution is related to, but not identical to, function composition; it is also closely related to β-reduction in lambda calculus. In contrast to these notions, however, the accent in algebra is on the preservation of algebraic structure by the substitution operation, the fact that substitution gives a homomorphism for the structure at hand (in the case of polynomials, the ring structure).\n\nSee also\nSubstitution (logic) — about a formal treatment of substitution\nTrigonometric substitution\n\nReferences\n\nCategory:Algebra"
    },
    {
      "title": "Suspension of a ring",
      "url": "https://en.wikipedia.org/wiki/Suspension_of_a_ring",
      "text": "In algebra, more specifically in algebraic K-theory, the suspension  of a ring R is given byWeibel, III, Ex. 1.15  where  is the ring of all infinite matrices with coefficients in R having only finitely many nonzero elements in each row or column and  is its ideal of matrices having only finitely many nonzero elements. It is an analog of suspension in topology.\n\nOne then has: .\n\n References \n\n C. Weibel \"The K-book: An introduction to algebraic K-theory\"\n\nCategory:Algebra"
    },
    {
      "title": "Symbolic method",
      "url": "https://en.wikipedia.org/wiki/Symbolic_method",
      "text": "In mathematics, the symbolic method in invariant theory is an algorithm developed by , ,  , and  in the 19th century for computing  invariants of algebraic forms. It is based on treating the form as if it were a power of a degree one form, which corresponds to embedding a symmetric power of a vector space into the symmetric elements of a tensor product of copies of it.\n\nSymbolic notation\n\nThe symbolic method uses a compact, but rather confusing and mysterious notation for invariants, depending on the introduction of new symbols a, b, c, ...  (from which the symbolic method gets its name) with apparently contradictory properties.\n\nExample: the discriminant of a binary quadratic form\nThese symbols can be explained by the following example from . Suppose that \n\nis a binary quadratic form with an invariant given by the discriminant\n\nThe symbolic representation of the discriminant is \n\nwhere a and b are the symbols. The meaning of  the expression (ab)2 is as follows. First of all, (ab) is a shorthand form for the determinant of a matrix whose rows are a1, a2 and b1, b2, so \n\nSquaring this we get\n\nNext we pretend that\n\nso that\n\nand we ignore the fact that this does not seem to make sense if f is not a power of a linear form. \nSubstituting these values gives\n\nHigher degrees\nMore generally if \n\nis a binary form of higher degree, then one introduces new variables a1, a2,  b1, b2, c1, c2, with the properties\n\nWhat this means is that the following two vector spaces are naturally isomorphic:\nThe  vector space of homogeneous polynomials in A0,...An of degree m\nThe vector space of  polynomials in 2m variables a1, a2,  b1, b2, c1, c2, ... that have degree n in each of the m pairs of variables (a1, a2),  (b1, b2), (c1, c2), ... and are symmetric under permutations of the m symbols a, b, ....,\nThe isomorphism is given by mapping aa, bb, .... to Aj. This mapping does not preserve products of polynomials.\n\nMore variables\nThe extension to a form f in more than two variables x1, x2,x3,... is similar: one introduces symbols a1, a2,a3 and so on  with the properties\n\nSymmetric products\n\nThe rather mysterious formalism of the symbolic method corresponds to embedding a symmetric product Sn(V) of a vector space V into a tensor product of n copies of V, as the elements preserved by the action of the symmetric group. In fact this is done twice, because the invariants of degree n of a quantic of degree m are the invariant elements of SnSm(V), which gets embedded into a tensor product of mn copies of V, as the elements invariant under a wreath product of the two symmetric groups. The brackets of the symbolic method are really invariant linear forms on this tensor product, which give invariants of SnSm(V) by restriction.\n\nSee also\n\nUmbral calculus\n\nReferences\n\n, pages 32–37, \"Invariants of n-ary forms: the symbolic method. Reprinted as  \n\nCategory:Algebra\nCategory:Invariant theory"
    },
    {
      "title": "Symbolic power of an ideal",
      "url": "https://en.wikipedia.org/wiki/Symbolic_power_of_an_ideal",
      "text": "In algebra and algebraic geometry, given a commutative Noetherian ring  and an ideal  in it, the n-th symbolic power of  is the ideal\n\nwhere  is the localization of  to  and the intersection runs through all of the associated primes of \n\nThough this definition does not require  to be prime, this assumption is often worked with because in the case of a prime ideal, the symbolic power can be equivalently defined as the -primary component of  . Very roughly, it consists of functions with zeros of order n along the variety defined by .  We have:  and if  is a maximal ideal, then .\n\nSymbolic powers induce the following chain of ideals:\n\n Uses \nThe study and use of symbolic powers has a long history in commutative algebra. Krull’s famous proof of his principal ideal theorem uses them in an essential way. They first arose after primary decompositions were proved for Noetherian rings. Zariski used symbolic powers in his study of the analytic normality of algebraic varieties. Chevalley's famous lemma comparing topologies states that in a complete local domain the symbolic powers topology of any prime is finer than the m-adic topology. A crucial step in the vanishing theorem on local cohomology of Hartshorne and Lichtenbaum uses that for a prime  defining a curve in a complete local domain, the powers of  are cofinal with the symbolic powers of . This important property of being cofinal was further developed by Schenzel in the 1970s.\n\n In algebraic geometry \nThough generators for ordinary powers of  are well understood when  is given in terms of its generators as , it is still very difficult in many cases to determine the generators of symbolic powers of . But in the geometric setting, there is a clear geometric interpretation in the case when  is a radical ideal over an algebraically closed field of characteristic zero.\n\nIf  is an irreducible variety whose ideal of vanishing is , then the differential power of   consists of all the functions in  that vanish\nto order ≥ n on , i.e.\n\nOr equivalently, if  is the maximal ideal for a point , .\n\nTheorem (Nagata, Zariski)David Eisenbud. Commutative Algebra: with a view toward algebraic geometry, volume 150. Springer Science & Business Media, 2013. Let  be a prime ideal in a polynomial ring  over an algebraically closed field. Then\n\nThis result can be extended to any radical ideal. This formulation is very useful because, in characteristic zero, we can compute the differential powers in terms of generators as:\n\nFor another formulation, we can consider the case when the base ring is a polynomial ring over a field. In this case, we can interpret the n-th symbolic power as the sheaf of all function germs over \nIn fact, if  is a smooth variety over a perfect field, then\n\n Containments \nIt is natural to consider whether or not symbolic powers agree with ordinary powers, i.e. does  hold? In general this is not the case. One example of this is the prime ideal . Here we have that .  However,  does hold and the generalization of this containment is well understood. Indeed, the containment follows from the definition. Further, it is known that  if and only if . The proof follows from Nakayama's lemma.Thomas Bauer, S Di Rocco, Brian Harbourne, Micha l Kapustka, Andreas Knutsen, Wioletta Syzdek, and Tomasz Szemberg. A primer on seshadri constants. Contemporary Mathematics, 496:33, 2009.\n\nThere has been extensive study into the other containment, when symbolic powers are contained in ordinary powers of ideals, referred to as the Containment Problem. Once again this has an easily stated answer summarized in the following theorem. It was developed by Ein, Lazarfeld, and Smith in characteristic zero Lawrence Ein, Robert Lazarsfeld, and Karen E Smith. Uniform bounds and symbolic powers on smooth varieties. Inventiones mathematicae, 144(2):241–252, 2001 and was expanded to positive characteristic by Hochster and Huneke.Melvin Hochster and Craig Huneke. Comparison of symbolic and ordinary powers of ideals. Inventiones mathematicae, 147(2):349–369, 2002. Their papers both build upon the results of Irena Swanson in Linear Equivalence of Ideal Topologies (2000).Irena Swanson. Linear equivalence of ideal topologies. Mathematische Zeitschrift, 234(4):755–775, 2000\n\nTheorem  (Ein, Lazarfeld, Smith; Hochster, Huneke) Let  be a homogeneous ideal. Then the containment\n  holds for all .\nIt was later verified that the bound of  in the theorem cannot be tightened for general ideals. However, following a question posed by Bocci, Harbourne, and Huneke, it was discovered that a better bound exists in some cases.\n\nTheorem The containment  for all  holds \n for arbitrary ideals in characteristic 2;Tomasz Szemberg and Justyna Szpond. On the containment problem. Rendiconti del Circolo Matematico di Palermo Series 2, pages 1–13, 2016.\n for monomial ideals in arbitrary characteristic \n for ideals of d-stars \n for ideals of general points in Marcin Dumnicki. Containments of symbolic powers of ideals of generic points in P 3 . Proceedings of the American Mathematical Society, 143(2):513–530, 2015.\n\n References \nthumb|From left: Brian Harbourne, Sandra Di Rocco, , and Thomas Bauer at the MFO mini-workshop Linear Series on Algebraic Varieties, 2010\n\nExternal links\n Melvin Hochster, Math 711: Lecture of September 7, 2007\n\nCategory:Algebra"
    },
    {
      "title": "Symmetric difference",
      "url": "https://en.wikipedia.org/wiki/Symmetric_difference",
      "text": "thumb|\nVenn diagram of . The symmetric difference is the union without the intersection:\n  40px\n\nIn mathematics, the symmetric difference, also known as the disjunctive union, of two sets is the set of elements which are in either of the sets and not in their intersection.  The symmetric difference of the sets A and B is commonly denoted by\n\nor\n\nor\n\nFor example, the symmetric difference of the sets  and  is .\n\nThe power set of any set becomes an abelian group under the operation of symmetric difference, with the empty set as the neutral element of the group and every element in this group being its own inverse. The power set of any set becomes a Boolean ring with symmetric difference as the addition of the ring and intersection as the multiplication of the ring.\n\n Properties \nthumb|\nVenn diagram of \n\n  40px\n\nThe symmetric difference is equivalent to the union of both relative complements, that is:\n\nThe symmetric difference can also be expressed using the XOR operation ⊕ on the predicates describing the two sets in set-builder notation:\n\nThe same fact can be stated as the indicator function (which we denote here by ) of the symmetric difference being the XOR (or addition mod 2) of the indicator functions of its two arguments:  or using the Iverson bracket notation .\n\nThe symmetric difference can also be expressed as the union of the two sets, minus their intersection:\n\nIn particular, ; the equality in this non-strict inclusion occurs if and only if  and  are disjoint sets. Furthermore, if we denote  and , then  and  are always disjoint, so  and  partition . Consequently, assuming intersection and symmetric difference as primitive operations, the union of two sets can be well defined in terms of symmetric difference by the right-hand side of the equality\n\n.\n\nThe symmetric difference is commutative and associative (and consequently the leftmost set of parentheses in the previous expression were thus redundant):\n\nThe empty set is neutral, and every set is its own inverse:\n\nTaken together, we see that the power set of any set X becomes an abelian group if we use the symmetric difference as operation. (More generally, any field of sets forms a group with the symmetric difference as operation.) A group in which every element is its own inverse (or, equivalently, in which every element has order 2) is sometimes called a Boolean group; the symmetric difference provides a prototypical example of such groups. Sometimes the Boolean group is actually defined as the symmetric difference operation on a set. In the case where X has only two elements, the group thus obtained is the Klein four-group.\n\nEquivalently, a Boolean group is an elementary abelian 2-group. Consequently, the group induced by the symmetric difference is in fact a vector space over the field with 2 elements Z2. If X is finite, then the singletons form a basis of this vector space, and its dimension is therefore equal to the number of elements of X. This construction is used in graph theory, to define the cycle space of a graph.\n\nFrom the property of the inverses in a Boolean group, it follows that the symmetric difference of two repeated symmetric differences is equivalent to the repeated symmetric difference of the join of the two multisets, where for each double set both can be removed. In particular:\n\nThis implies triangle inequality: the symmetric difference of A and C is contained in the union of the symmetric difference of A and B and that of B and C. (But note that for the diameter of the symmetric difference the triangle inequality does not hold.)\n\nIntersection distributes over symmetric difference:\n\nand this shows that the power set of X becomes a ring with symmetric difference as addition and intersection as multiplication. This is the prototypical example of a Boolean ring.\n\nFurther properties of the symmetric difference:\n\n , where ,  is 's complement, 's complement, respectively, relative to any (fixed) set that contains both.\n , where  is an arbitrary non-empty index set.\n If  is any function and  are any sets in 's codomain, then .\n\nThe symmetric difference can be defined in any Boolean algebra, by writing\n\nThis operation has the same properties as the symmetric difference of sets.\n\nn-ary symmetric difference\nThe repeated symmetric difference is in a sense equivalent to an operation on a multiset of sets giving the set of elements which are in an odd number of sets.\n\nAs above, the symmetric difference of a collection of sets contains just elements which are in an odd number of the sets in the collection:\n.\n\nEvidently, this is well-defined only when each element of the union  is contributed by a finite number of elements of .\n\nSuppose  is a multiset and . Then there is a formula for , the number of elements in , given solely in terms of intersections of elements of :\n.\n\nSymmetric difference on measure spaces\nAs long as there is a notion of \"how big\" a set is, the symmetric difference between two sets can be considered a measure of how \"far apart\" they are.\n\nFirst consider a finite set S and the counting measure on subsets given by their size. Now consider two subsets of S and set their distance apart as the size of their symmetric difference. This distance is in fact a metric  so that the power set on S is a metric space. If S has n elements, then the distance from the empty set to S is n, and this is the maximum distance for any pair of subsets.Claude Flament (1963) Applications of Graph Theory to Group Structure, page 16, Prentice-Hall \n\nUsing the ideas of measure theory, the separation of measurable sets can be defined to be the measure of their symmetric difference. If μ is a σ-finite measure defined on a σ-algebra Σ, the function\n\nis a pseudometric on Σ.  dμ becomes a metric if Σ is considered modulo the equivalence relation X ~ Y if and only if . It is sometimes called Fréchet-Nikodym metric.  The resulting metric space is separable if and only if L2(μ) is separable.\n\nIf , we have: . Indeed,\n\nIf  is a measure space and  are measurable sets, then their symmetric difference is also measurable: .  One may define an equivalence relation on measurable sets by letting F and G be related if .  This relation is denoted .\n\nGiven , one writes  if to each  there's some  such that . The relation \"\" is a partial order on the family of subsets of .\n\nWe write  if  and .  The relation \"\" is an equivalence relationship between the subsets of .\n\nThe symmetric closure of  is the collection of all -measurable sets that are  to some . The symmetric closure of  contains . If  is a sub--algebra of , so is the symmetric closure of .\n\n iff   almost everywhere.\n\n Hausdorff distance vs. symmetric difference \nthumb|right\nThe Hausdorff distance and the (area of the) symmetric difference are both pseudo-metrics on the set of measurable geometric shapes. However, they behave quite differently. The figure at the right shows two sequences of shapes, \"Red\" and \"Red ∪ Green\". When the Hausdorff distance between them becomes smaller, the area of the symmetric difference between them becomes larger, and vice versa. By continuing these sequences in both directions, it is possible to get two sequences such that the Hausdorff distance between them converges to 0 and the symmetric distance between them diverges, or vice versa.\n\nSee also\n\n Algebra of sets\n Boolean function\n Difference (set theory)\n Exclusive or\n Fuzzy set\n\n Logical graph\n Separable sigma algebras\n Set theory\n Symmetry\n\nReferences\n\n \n \n \n Symmetric difference of sets. In Encyclopaedia of Mathematics\n\nCategory:Algebra\nCategory:Basic concepts in set theory\nCategory:Binary operations\nCategory:Subtraction"
    },
    {
      "title": "System of polynomial equations",
      "url": "https://en.wikipedia.org/wiki/System_of_polynomial_equations",
      "text": "A system of polynomial equations (sometimes simply a polynomial system) is a set of simultaneous equations  where the  are polynomials in several variables, say , over some field . \n\nA solution of a polynomial system is a set of values for the s which belong to some algebraically closed field extension  of , and make all equations true. When  is the field of rational numbers,  is generally assumed to be the field of complex numbers, because each solution belongs to a field extension of , which is isomorphic to a subfield of the complex numbers.\n\nThis article is about the methods for solving, that is, finding all solutions or describing them. As these methods are designed for being implemented in a computer, emphasis is given on fields  in which computation (including equality testing) is easy and efficient, that is the field of rational numbers and finite fields.\n\nSearching for solutions that belong to a specific set is a problem which is generally much more difficult, and is outside the scope of this article, except for the case of the solutions in a given finite field. For the case of solutions of which all components are integers or rational numbers, see Diophantine equation.\n\nExamples and extensions\nTrigonometric equations\nA trigonometric equation is an equation  where  is a trigonometric polynomial. Such an equation may be converted into a polynomial system by expanding the sines and cosines in it (using sum and difference formulas), replacing  and  by two new variables  and  and adding the new equation .\n\nFor example, because of the identity\n\nsolving the equation\n\nis equivalent to solving the polynomial system\n\nFor each solution  of this system, there is a unique solution  of the equation such that .\n\nIn the case of this simple example, it may be unclear whether the system is, or not, easier to solve than the equation. On more complicated examples, one lacks systematics methods for solving directly the equation, while software are available for automatically solving the corresponding system.\n\n Solutions in a finite field \nWhen solving a system over a finite field  with  elements, one is primarily interested in the solutions in . As the elements of  are exactly the solutions of the equation , it suffices, for restricting the solutions to , to add the equation  for each variable .\n\n Coefficients in a number field or in a finite field with non-prime order \n\nThe elements of a number field are usually represented as polynomials in a generator of the field which satisfies some univariate polynomial equation. To work with a polynomial system whose coefficients belong to a number field, it suffices to consider this generator as a new variable and to add the equation of the generator to the equations of the system. Thus solving a polynomial system over a number field is reduced to solving another system over the rational numbers.\n\nFor example, if a system contains , a system over the rational numbers is obtained by adding the equation  and replacing  by  in the other equations.\n\nIn the case of a finite field, the same transformation allows always to suppose that the field  has a prime order.\n\nBasic properties and definitions\nA system is overdetermined if the number of equations is higher than the number of variables. A system is inconsistent if it has no solutions. By Hilbert's Nullstellensatz this means that 1 is a linear combination (with polynomials as coefficients) of the first members of the equations. Most but not all overdetermined systems, when constructed with random coefficients, are inconsistent. For example, the system  x3 − 1 = 0, x2 − 1 = 0 is overdetermined (having two equations but only one unknown),  but it is not inconsistent since it has the solution x =1.\n\nA system is underdetermined if the number of equations is lower than the number of the variables. An underdetermined system is either inconsistent or has infinitely many solutions in an algebraically closed extension K of k.\n\nA system is zero-dimensional if it has a finite number of solutions in an algebraically closed extension K of k. This terminology comes from the fact that the algebraic variety of the solutions has dimension zero. A system with infinitely many solutions is said to be positive-dimensional.\n\nA zero-dimensional system with as many equations as variables is said to be well-behaved.Songxin Liang, J. Gerhard, D.J. Jeffrey, G. Moroz, A Package for Solving Parametric Polynomial Systems. Communications in Computer Algebra (2009)\nBézout's theorem asserts that a well-behaved system whose equations have degrees d1, ..., dn has at most d1...dn solutions. This bound is sharp. If all the degrees are equal to d, this bound becomes dn and is exponential in the number of variables.\n\nThis exponential behavior makes solving polynomial systems difficult and explains why there are few solvers that are able to automatically solve systems with Bézout's bound higher than, say, 25 (three equations of degree 3 or five equations of degree 2 are beyond this bound).\n\n What is solving? \nThe first thing to do in solving a polynomial system is to decide if it is inconsistent, zero-dimensional or positive dimensional. This may be done by the computation of a Gröbner basis of the left-hand sides of the equations. The system is inconsistent if this Gröbner basis is reduced to 1. The system is zero-dimensional if, for every variable there is a leading monomial of some element of the Gröbner basis which is a pure power of this variable. For this test, the best monomial order is usually the graded reverse lexicographic one (grevlex).\n\nIf the system is positive-dimensional, it has infinitely many solutions. It is thus not possible to enumerate them. It follows that, in this case, solving may only mean \"finding a description of the solutions from which the relevant properties of the solutions are easy to extract\". There is no commonly accepted such description. In fact there are many different \"relevant properties\", which involve almost every subfield of algebraic geometry.\n\nA natural example of such a question positive-dimensional systems is the following: decide if a polynomial system over the rational numbers has a finite number of real solutions and compute them. A generalization of this question is find at least one solution in each connected component of the set of real solutions of a polynomial system.  The classical algorithm for solving these question is cylindrical algebraic decomposition, which has a doubly exponential computational complexity and therefore cannot be used in practice, except for very small examples.\n\nFor zero-dimensional systems, solving consists of computing all the solutions. There are two different ways of outputting the solutions. The most common, possible for real or complex solutions, consists of outputting numeric approximations of the solutions. Such a solution is called numeric. A solution is certified if it is provided with a bound on the error of the approximations which separates the different solutions.\n\nThe other way to represent the solutions is said to be algebraic. It uses the fact that, for a zero-dimensional system, the solutions belong to the algebraic closure of the field k of the coefficients of the system. There are several ways to represent the solution in an algebraic closure, which are discussed below. All of them allow one to compute a numerical approximation of the solutions by solving one or several univariate equations. For this computation, the representation involving the solving of only one univariate polynomial for each solution is preferable: computing the roots of a polynomial which has approximate coefficients is a highly unstable problem.\n\n Algebraic representation of the solutions\n Regular chains \n\nThe usual way of representing the solutions is through zero-dimensional regular chains. Such a chain consists of a sequence of polynomials , , ...,  such that, for every  such that \n  is a polynomial in  only, which has a degree  in ;\n the coefficient of  in  is a polynomial in  which does not have any common zero with , ..., .\n\nTo such a regular chain is associated a triangular system of equations\n\nThe solutions of this system are obtained by solving the first univariate equation, substituting the solutions in the other equations, then solving the second equation which is now univariate, and so on. The definition of regular chains implies that the univariate equation obtained from  has degree  and thus that the system has  solutions, provided that there is no multiple root in this resolution process (fundamental theorem of algebra).\n\nEvery zero-dimensional system of polynomial equations is equivalent (i.e. has the same solutions) to a finite number of regular chains. Several regular chains may be needed, as it is the case for the following system which has three solutions.\n\nThere are several algorithms for computing a triangular decomposition of an arbitrary polynomial system (not necessarily zero-dimensional) into regular chains (or regular semi-algebraic systems).\n\nThere is also an algorithm which is specific to the zero-dimensional case and is competitive, in this case, with the direct algorithms. It consists in computing first the Gröbner basis for the graded reverse lexicographic order (grevlex), then deducing the lexicographical Gröbner basis by FGLM algorithm and finally applying the Lextriangular algorithm.\n\nThis representation of the solutions are fully convenient for coefficients in a finite field. However, for rational coefficients, two aspects have to be taken care of:\n The output may involve huge integers which may make the computation and the use of the result problematic.\n To deduce the numeric values of the solutions from the output, one has to solve univariate polynomials with approximate coefficients, which is a highly unstable problem.\n\nThe first issue has been solved by Dahan and Schost:Xavier Dahan and Eric Schost. Sharp Estimates for Triangular Sets. Moreover, recent algorithms for decomposing polynomial systems into triangular decompositions produce regular chains with coefficients matching the results of Dahan and Schost.  In proc. ISSAC'04, pages 103--110, ACM Press, 2004 Among the sets of regular chains that represent a given set of solutions, there is a set for which the coefficients are explicitly bounded in terms of the size of the input system, with a nearly optimal bound. This set, called equiprojectable decomposition, depends only on the choice of the coordinates. This allows the use of modular methods for computing efficiently the equiprojectable decomposition.Changbo Chen and Marc Moreno-Maza. Algorithms for Computing Triangular Decomposition of Polynomial Systems.In proc. ISSAC'2011, pages 83-90, ACM Press, 2011 and Journal of Symbolic Computation (to appear)\n\nThe second issue is generally solved by outputting regular chains of a special form, sometimes called shape lemma, for which all  but the first one are equal to . For getting such regular chains, one may have to add a further variable, called separating variable, which is given the index . The rational univariate representation, described below,  allows computing such a special regular chain, satisfying Dahan–Schost bound, by starting from either a regular chain or a Gröbner basis.\n\nRational univariate representation\nThe rational univariate representation or RUR is a representation of the solutions of a zero-dimensional polynomial system over the rational numbers which has been introduced by  F. Rouillier.\n\nA RUR of a zero-dimensional system consists in a linear combination  of the variables, called separating variable, and a system of equations\n\nwhere  is a univariate polynomial in  of degree  and  are univariate polynomials in  of degree less than .\n\nGiven a zero-dimensional polynomial system over the rational numbers, the RUR has the following properties.\n\n All but a finite number linear combinations of the variables are separating variables.\n When the separating variable is chosen, the RUR exists and is unique. In particular  and the  are defined independently of any algorithm to compute them.\n The solutions of the system are in one-to-one correspondence with the roots of  and the multiplicity of each root of  equals the multiplicity of the corresponding solution.\n The solutions of the system are obtained by substituting the roots of  in the other equations.\n If  does not have any multiple root then  is the derivative of .\n\nFor example, for the system in the previous section, every linear combination of the variable, except the multiples of ,  and , is a separating variable. If one chooses  as a separating variable, then the RUR is\n\nThe RUR is uniquely defined for a given separating variable, independently of any algorithm, and it preserves the multiplicities of the roots. This is a notable difference with triangular decompositions (even the equiprojectable decomposition), which, in general, do not preserve multiplicities. The RUR shares with equiprojectable decomposition the property of producing an output with coefficients of relatively small size.\n\nFor zero-dimensional systems, the RUR allows retrieval of the numeric values of the solutions by solving a single univariate polynomial and substituting them in rational functions. This allows production of certified approximations of the solutions to any given precision.\n\nMoreover, the univariate polynomial  of the RUR may be factorized, and this gives a RUR for every irreducible factor. This provides the prime decomposition of the given ideal (that is the primary decomposition of the radical of the ideal). In practice, this provides an output with much smaller coefficients, especially in the case of systems with high multiplicities.\n\nContrarily to triangular decompositions and equiprojectable decompositions, the RUR is not defined in positive dimension.\n\nAlgorithms for numerically solving\nGeneral solving algorithms\nThe general numerical algorithms which are designed for any system of nonlinear equations work also for polynomial systems. However the specific methods will generally be preferred, as the general methods generally do not allow one to find all solutions. In particular, when a general method does not find any solution, this is usually not an indication that there is no solution.\n\nNevertheless, two methods deserve to be mentioned here.\n\nNewton's method may be used if the number of equations is equal to the number of variables. It does not allow one to find all the solutions nor to prove that there is no solution. But it is very fast when starting from a point which is close to a solution. Therefore, it is a basic tool for the homotopy continuation method described below.\nOptimization is rarely used for solving polynomial systems, but it succeeded, around 1970, in showing that a system of 81 quadratic equations in 56 variables is not inconsistent. With the other known methods this remains beyond the possibilities of modern technology. This method consists simply in minimizing the sum of the squares of the equations. If zero is found as a local minimum, then it is attained at a solution. This method works for overdetermined systems, but outputs an empty information if all local minimums which are found are positive.\n\n Homotopy continuation method \n\nThis is a semi-numeric method which supposes that the number of equations is equal to the number of variables. This method is relatively old but it has been dramatically improved in the last decades.\n\nThis method divides into three steps. First an upper bound on the number of solutions is computed. This bound has to be as sharp as possible. Therefore, it is computed by, at least, four different methods and the best value, say , is kept.\n\nIn the second step, a system  of polynomial equations is generated which has exactly  solutions that are easy to compute. This new system has the same number  of variables and the same number  of equations and the same general structure as the system to solve, .\n\nThen a homotopy between the two systems is considered. It consists, for example, of the straight line between the two systems, but other paths may be considered, in particular to avoid some singularities, in the system\n\n.\n\nThe homotopy continuation consists in deforming the parameter  from 0 to 1 and following the  solutions during this deformation. This gives the desired solutions for . Following means that, if , the solutions for  are deduced from the solutions for  by Newton's method. The difficulty here is to well choose the value of  Too large, Newton's convergence may be slow and may even jump from a solution path to another one. Too small, and the number of steps slows down the method.\n\nNumerically solving from the rational univariate representation\n\nTo deduce the numeric values of the solutions from a RUR seems easy: it suffices to compute the roots of the univariate polynomial and to substitute them in the other equations. This is not so easy because the evaluation of a polynomial at the roots of another polynomial is highly unstable.\n\nThe roots of the univariate polynomial have thus to be computed at a high precision which may not be defined once for all. There are two algorithms which fulfill this requirement.\n\n Aberth method, implemented in MPSolve computes all the complex roots to any precision.\n Uspensky's algorithm of Collins and Akritas,George E. Collins and Alkiviadis G. Akritas, Polynomial Real Root Isolation Using Descarte's Rule of Signs. Proceedings of the 1976 ACM Symposium on Symbolic and Algebraic Computation improved by Rouillier and Zimmermann  and based on Descartes' rule of signs. This algorithms computes the real roots, isolated in intervals of arbitrary small width. It is implemented in Maple (functions fsolve and RootFinding[Isolate]).\n\nSoftware packages\nThere are at least four software packages which can solve zero-dimensional systems automatically (by automatically, one means that no human intervention is needed between input and output, and thus that no knowledge of the method by the user is needed). There are also several other software packages which may be useful for solving zero-dimensional systems. Some of them are listed after the automatic solvers.\n\nThe Maple function RootFinding[Isolate] takes as input any polynomial system over the rational numbers (if some coefficients are floating point numbers, they are converted to rational numbers) and outputs the real solutions represented either (optionally) as intervals of rational numbers or as floating point approximations of arbitrary precision. If the system is not zero dimensional, this is signaled as an error.\n\nInternally, this solver, designed by F. Rouillier computes first a Gröbner basis and then a Rational Univariate Representation from which the required approximation of the solutions are deduced. It works routinely for systems having up to a few hundred complex solutions.\n\nThe rational univariate representation may be computed with Maple function Groebner[RationalUnivariateRepresentation].\n\nTo extract all the complex solutions from a rational univariate representation, one may use MPSolve, which computes the complex roots of univariate polynomials to any precision. It is recommended to run MPSolve several times, doubling the precision each time, until solutions remain stable, as the substitution of the roots in the equations of the input variables can be highly unstable.\n\nThe second solver is PHCpack,Release 2.3.86 of PHCpack written under the direction of J. Verschelde. PHCpack implements the homotopy continuation method. This solver computes the isolated complex solutions of polynomial systems having as many equations as variables.\n\nThe third solver is Bertini,Bates, D. J., Hauenstein, J. D., Sommese, A. J., & Wampler, C. W. (2013). Numerically solving polynomial systems with Bertini (Vol. 25). SIAM.Bertini: Software for Numerical Algebraic Geometry written by D. J. Bates, J. D. Hauenstein, A. J. Sommese, and C. W. Wampler. Bertini uses numerical homotopy continuation with adaptive precision.  In addition to computing zero-dimensional solution sets, both PHCpack and Bertini are capable of working with positive dimensional solution sets.\n\nThe fourth solver is the Maple library RegularChains, written by Marc Moreno-Maza and collaborators. It contains various functions for solving polynomial systems by means of regular chains.\n\nSee also\nTriangular decomposition\nWu's method of characteristic set\n\n References \n\nCategory:Equations\nCategory:Algebra\nCategory:Computer algebra\nCategory:Polynomials\nCategory:Algebraic geometry"
    },
    {
      "title": "Temperley–Lieb algebra",
      "url": "https://en.wikipedia.org/wiki/Temperley%E2%80%93Lieb_algebra",
      "text": "In statistical mechanics, the Temperley–Lieb algebra is an algebra from which are built certain transfer matrices, invented by Neville Temperley and Elliott Lieb. It is also related to  integrable models, knot theory and the braid group, quantum groups and subfactors of von Neumann algebras.\n\nDefinition\n\nLet  be a commutative ring and fix . The Temperley–Lieb algebra  is the -algebra generated by the elements , subject to the Jones relations: \n for all \n for all \n for all \n for all  such that \n\n may be represented diagrammatically as the vector space over noncrossing pairings on a rectangle with n points on two opposite sides.  The five basis elements of  are the following:\n\n340px|Basis of the Temperley–Lieb algebra .\n\nMultiplication on basis elements can be performed by placing two rectangles side by side, and replacing any closed loops by a factor of , for example:\n\n50px  ×  50px  =  50px50px  =    50px.\n\nThe identity element is the diagram in which each point is connected to the one directly across the rectangle from it, and the generator  is the diagram in which the -th point is connected to the -th point, the -th point is connected to the -th point, and all other points are connected to the point directly across the rectangle.  The generators of  are:\n\n340px|Generators of the Temperley–Lieb algebra \n\nFrom left to right, the unit 1 and the generators U1, U2, U3, U4.\n\nThe Jones relations can be seen graphically:\n\n50px 50px  =    50px\n\n50px 50px 50px  =  50px\n\n50px 50px  =  50px 50px\n\nThe Temperley–Lieb Hamiltonian\n\nConsider an interaction-round-a-face model e.g. a square lattice model and let  be the number of sites on the lattice. Following Temperley and Lieb we define the Temperley–Lieb Hamiltonian (the TL Hamiltonian) as\n\nwhere , for some spectral parameter .\n\nApplications\n\nWe will firstly consider the case . The TL Hamiltonian is , namely \n\n  =  2  50px  -  50px  -  50px.\n\nWe have two possible states,\n\n40px and 40px.\n\nIn acting by  on these states, we find\n\n 40px  =  2  50px40px  -  50px40px  -  50px40px  =  40px  -  40px,\n\nand\n\n 40px  =  2  50px40px  -  50px40px  -  50px40px  =  -  40px  +  40px.\n\nWriting  as a matrix in the basis of possible states we have,\n\nThe eigenvector of  with the lowest eigenvalue is known as the ground state. In this case, the lowest eigenvalue  for  is . The corresponding eigenvector is . As we vary the number of sites  we find the following table\n\n     2 (1)3(1, 1) 4(2, 1)5 6 789\n\nwhere we have used the notation  -times e.g., .\n\nCombinatorial Properties\nAn interesting observation is that the largest components of the ground state of  have a combinatorial enumeration as we vary the number of sites, as was first observed by Murray Batchelor, Jan de Gier and Bernard Nienhuis. Using the resources of the on-line encyclopedia of integer sequences, Batchelor et al. found, for an even numbers of sites \n\nand for an odd numbers of sites \n\nSurprisingly, these sequences corresponded to well known combinatorial objects. For  even, this  corresponds to cyclically symmetric transpose complement plane partitions and for  odd, , these correspond to  alternating sign matrices symmetric about the vertical axis.\n\nReferences\n\nFurther reading\n\nCategory:Von Neumann algebras\nCategory:Algebra\nCategory:Knot theory\nCategory:Braids\nCategory:Diagram algebras"
    },
    {
      "title": "Tertiary ideal",
      "url": "https://en.wikipedia.org/wiki/Tertiary_ideal",
      "text": "In mathematics, a tertiary ideal is an (two-sided) ideal in a (perhaps noncommutative) ring that cannot be expressed as a nontrivial intersection of a right fractional ideal with another ideal. Tertiary ideals generalize primary ideals to the case of noncommutative rings. Although primary decompositions do not exist in general for ideals in noncommutative rings, tertiary decompositions do, at least if the ring is Noetherian.\n\nEvery primary ideal is tertiary. Tertiary ideals and primary ideals coincide for commutative rings. To any (two-sided) ideal, a tertiary ideal can be associated called the tertiary radical, defined as\n\nThen t(I) always contains I.\n\nIf R is a (not necessarily commutative) Noetherian ring and I a right ideal in R, then I has a unique irredundant decomposition into tertiary ideals\n\n.\n\n See also \n Primary ideal\n Lasker–Noether theorem\n\n References \n \n Tertiary ideal, Encyclopedia of Mathematics, Springer Online Reference Works.\n \n \n\nCategory:Algebra"
    },
    {
      "title": "Topological module",
      "url": "https://en.wikipedia.org/wiki/Topological_module",
      "text": "In mathematics, a topological module is a module over a topological ring such that scalar multiplication and addition are continuous.\n\n Examples \n\nA topological vector space is a topological module over a topological field.\n\nAn abelian topological group can be considered as a topological module over Z, where Z is the ring of integers with the discrete topology.\n\nA topological ring is a topological module over each of its subrings.\n\nA more complicated example is the I-adic topology on a ring and its modules. Let I be an ideal of a ring R. The sets of the form , for all x in R and all positive integers n, form a base for a topology on R that makes R into a topological ring. Then for any left R-module M, the sets of the form , for all x in M and all positive integers n, form a base for a topology on M that makes M into a topological module over the topological ring R.\n\n See also \nLinear topology\n\nCategory:Algebra\nCategory:Topology"
    },
    {
      "title": "Triangular decomposition",
      "url": "https://en.wikipedia.org/wiki/Triangular_decomposition",
      "text": "In computer algebra, a triangular decomposition of a polynomial system  is a set of simpler polynomial systems  such that a point is a solution of  if and only if it is a solution of one of the systems .\n\nWhen the purpose is to describe the solution set of  in the algebraic closure of its coefficient field, those simpler systems are regular chains. If the coefficients of the polynomial systems  are real numbers, then the real solutions of   can be obtained by a triangular decomposition into regular semi-algebraic systems. In both cases, each of these simpler systems has a triangular shape and remarkable properties, which justifies the terminology.\n\n History \nThe Characteristic Set Method is the first factorization-free algorithm, which was proposed for decomposing an algebraic variety into equidimensional components.  Moreover, the Author, Wen-Tsun Wu, realized an implementation of this method and reported experimental data in his 1987 pioneer article titled \"A zero structure theorem for polynomial equations solving\".Wu, W. T. (1987). A zero structure theorem for polynomial equations solving. MM Research Preprints, 1, 2–12 To put this work into context, let us recall what was the common idea of an algebraic set decomposition at the time this article was written.\n\nLet  be an algebraically closed field and  be a subfield of . A subset  is an (affine) algebraic variety over  if there exists a polynomial set  such that the zero set  of  equals .\n\nRecall that  is said irreducible if for all  algebraic varieties  the relation  implies either  or . A first algebraic variety decomposition result is the famous Lasker–Noether theorem which implies the following.\n\nTheorem (Lasker - Noether). For each algebraic variety  there exist finitely many irreducible algebraic varieties  such that we have\n\nMoreover, if   holds for  then the set is unique and forms the irreducible decomposition of .\n\nThe varieties  in the above Theorem are called the irreducible components of  and can be regarded as a natural output for a decomposition algorithm, or, in other words, for an algorithm solving a system of equations in .\n\nIn order to lead to a computer program, this algorithm specification  should prescribe how irreducible components are represented.  Such an encoding is introduced  by Joseph RittRitt, J. (1966). Differential Algebra. New York, Dover Publications  through the following result.\n\nTheorem (Ritt). If  is a non-empty and irreducible variety then one can compute a reduced triangular set  contained in the ideal  generated by  in  and such that all polynomials  in  reduces to zero by pseudo-division w.r.t .\n\nWe call the set  in Ritt's Theorem a Ritt characteristic set of the ideal . Please refer to regular chain for the notion of a triangular set.\n\nJoseph Ritt described  a method for solving polynomial systems  based on polynomial factorization over field extensions and computation of characteristic sets of prime ideals.\n\nDeriving a practical implementation of this method, however,  was and remains a difficult problem. In the 80's, when the Characteristic set Method was introduced,  polynomial factorization was an active research area and certain fundamental questions on this subject were solved  recentlyA. M. Steel Conquering inseparability: Primary decomposition and multivariate factorization over algebraic function fields of positive characteristic\n\nNowadays, decomposing an algebraic variety into irreducible components is not essential to process most application problems, since weaker notions of decompositions, less costly to compute, are sufficient.\n\nThe Characteristic Set Method relies on the following variant of Ritt's Theorem.\n\nTheorem (Wen-Tsun Wu). For any finite polynomial set , one can compute a reduced triangular set  such that  all polynomial  in  reduces to zero by pseudo-division w.r.t .\n\nDifferent concepts and algorithms extended the work of Wen-Tsun Wu. In the early 90's, the notion of a regular chain, introduced independently by Michael Kalkbrener in 1991 in his PhD Thesis and, by Lu Yang and Jingzhong ZhangYang, L., Zhang, J. (1994). Searching dependency between algebraic equations: an algorithm applied to automated reasoning. Artificial Intelligence in Mathematics, pp. 14715,  Oxford University Press. led to important algorithmic discoveries.\n\nIn Kalkbrener's vision,M. Kalkbrener: A Generalized Euclidean Algorithm for Computing Triangular Representations of Algebraic Varieties. J. Symb. Comput. 15(2): 143–167 (1993) regular chains are used to represent the generic zeros of the irreducible components of an algebraic variety.  In the original work of Yang and Zhang, they are used to decide whether a hypersurface intersects a quasi-variety (given by a regular chain).  Regular chains have, in fact, several interesting properties and are the key notion in many algorithms for decomposing systems of algebraic or differential equations.\n\nRegular chains have been investigated in many papers.S.C. Chou and X.S. Gao. On the dimension of an arbitrary ascending chain.  Chinese Bull. of Sci., 38:799--804, 1991.Michael Kalkbrener. Algorithmic properties of polynomial rings. J. Symb. Comput.}, 26(5):525--581, 1998.P. Aubry, D. Lazard, M. Moreno Maza. On the theories of triangular sets. Journal of Symbolic Computation, 28(1–2):105–124, 1999.\n\nThe abundant literature on the subject can be explained by the many equivalent definitions of a regular chain. Actually, the original formulation of Kalkbrener is quite  different from that of Yang and Zhang.  A bridge between these two notions, the point of view of Kalkbrener and that of Yang and Zhang, appears in Dongming Wang's paper.D. Wang. Computing Triangular Systems and Regular Systems. Journal of Symbolic Computation 30(2) (2000) 221–236\n\nThere are various algorithms available for obtaining triangular decomposition of  both in the sense of Kalkbrener and in the sense of Lazard and Wen-Tsun Wu. The Lextriangular Algorithm by Daniel LazardD. Lazard, Solving zero-dimensional algebraic systems. Journal of Symbolic Computation 13, 1992 and the Triade Algorithm by Marc Moreno MazaM. Moreno Maza: On triangular decomposition of algebraic varieties. MEGA 2000 (2000). together with the Characteristic Set Method are available in various computer algebra systems, including Axiom and Maple.\n\n Formal definitions \nLet  be a field and  be ordered variables. We denote by  the corresponding polynomial ring. For , regarded as a system of polynomial equations, there are two notions of a triangular decomposition over the algebraic closure of . The first one is to decompose lazily, by representing only the generic points of the algebraic set  in the so-called sense of Kalkbrener.\n\n \n\nThe second is to describe explicitly all the points of  in the so-called sense of in Lazard and Wen-Tsun Wu.\n\n \n\nIn both cases  are finitely many regular chains of  and  denotes the radical of the saturated ideal of  while  denotes the quasi-component of . Please refer to regular chain for definitions of these notions.\n\nAssume from now on that  is a real closed field. Consider  a semi-algebraic system with polynomials in . There existChangbo Chen, James H. Davenport, John P. May, Marc Moreno-Maza, Bican Xia, Rong Xiao. Triangular decomposition of semi-algebraic systems.  Proceedings of 2010 International Symposium on Symbolic and Algebraic Computation (ISSAC 2010), ACM Press, pp. 187--194, 2010. finitely many regular semi-algebraic systems  such that we have\n\nwhere  denotes the points of  which solve . The regular semi-algebraic systems  form a triangular decomposition of the semi-algebraic system .\n\n Examples \nDenote  the rational number field. In  with variable ordering , consider the following polynomial system: \n\nAccording to the Maple code: with(RegularChains):\nR := PolynomialRing([x, y, z]):\nsys := {x^2+y+z-1, x+y^2+z-1, x+y+z^2-1}:\nl := Triangularize(sys, R):\nmap(Equations, l, R);One possible triangular decompositions of the solution set of  with using RegularChains library is:\n\n See also \nWu's method of characteristic set\nRegular chain\nRegular semi-algebraic system\n\n References \n\nCategory:Equations\nCategory:Algebra\nCategory:Polynomials\nCategory:Algebraic geometry\nCategory:Computer algebra\nCategory:Computer algebra systems"
    },
    {
      "title": "Unitary method",
      "url": "https://en.wikipedia.org/wiki/Unitary_method",
      "text": "The unitary method is a technique for solving a problem by first finding the value of a single unit, and then finding the necessary value by multiplying the single unit value. In essence, this method is used to find the value of a unit from the value of a multiple, and hence the value of a multiple.\n\nExamples\n\nFor example, to solve the problem: \"A man walks 7 miles in 2 hours. How far does he walk in 7 hours?\", one would first calculate how far the man walks in 1 hour. One can safely assume that he would walk half the distance in half the time. Therefore, dividing by 2, the man walks 3.5 miles in 1 hour. Multiplying by 7 for 7 hours, the man walks 7x3.5=24.5 miles, or consider the distance travelled by the man be X, then divide it given distance that is 7 (x/7). It is equal to the time taken to travel X distance that is 7 hours divided by the time taken to travel 7 miles, that is 2 hours (7/2), therefore x/7=7/2, hence X=24.5 miles.\n\nThe same method can be applied to the problem: \"A man walks at 4 miles per hour. How long would it take him to cover 5 miles?\". Dividing by 4 shows that the man covers 1 mile in a quarter (0.25) of an hour. Multiplying by 5 shows that the man therefore takes 1 hour and a quarter (1.25 hours) to cover 5 miles. Similarly, by the second method, we can find the value of time taken to cover 5 miles.\nThe fir1st method is preferable and easier.\n\nReferences\n\nExternal links\n http://www.math-only-math.com/unitary-method.html\n http://thesaurus.maths.org/mmkb/entry.html?action=entryById&id=4175\n http://tutorteddy.com/the_unitary_method.php\n\nCategory:Elementary algebra\nCategory:Algebra"
    },
    {
      "title": "Variable (mathematics)",
      "url": "https://en.wikipedia.org/wiki/Variable_%28mathematics%29",
      "text": "In elementary mathematics, a variable is a symbol, commonly a single letter, that represents a number, called the value of the variable, which is either arbitrary, not fully specified, or unknown. Making algebraic computations with variables as if they were explicit numbers allows one to solve a range of problems in a single computation. A typical example is the quadratic formula, which allows one to solve every quadratic equation by simply substituting the numeric values of the coefficients of the given equation for the variables that represent them.\n\nThe concept of a variable is also fundamental in calculus.\nTypically, a function  involves two variables,  and , representing respectively the value and the argument of the function. The term \"variable\" comes from the fact that, when the argument (also called the \"variable of the function\") varies, then the value varies accordingly.\n\nIn more advanced mathematics, a variable is a symbol that denotes a mathematical object, which could be a number, a vector, a matrix, or even a function.  In this case, the original property of \"variability\" of a variable is not kept (except, sometimes, for informal explanations).\n\nSimilarly, in computer science, a variable is a name (commonly an alphabetic character or a word) representing some value stored in computer memory. In mathematical logic, a variable is either a symbol representing an unspecified term of the theory, or a basic object of the theory, which is manipulated without referring to its possible intuitive interpretation.\n\nEtymology\n\"Variable\" comes from a Latin word, variābilis, with \"vari(us)\"' meaning \"various\" and \"-ābilis\"' meaning \"-able\", meaning \"capable of changing\".\n\nGenesis and evolution of the concept\n\nIn the 7th century Brahmagupta used different colours to represent the unknowns in algebraic equations in the Brāhmasphuṭasiddhānta. One section of this book is called \"Equations of Several Colours\".\n\nAt the end of the 16th century François Viète introduced the idea of representing known and unknown numbers by letters, nowadays called variables, and of computing with them as if they were numbers, in order to obtain the result by a simple replacement. Viète's convention was to use consonants for known values and vowels for unknowns.\n\nIn 1637, René Descartes \"invented the convention of representing unknowns in equations by x, y, and z, and knowns by a, b, and c\".Tom Sorell, Descartes: A Very Short Introduction, (2000). New York: Oxford University Press. p. 19. Contrarily to Viète's convention, Descartes' is still commonly in use.\n\nStarting in the 1660s, Isaac Newton and Gottfried Wilhelm Leibniz independently developed the infinitesimal calculus, which essentially consists of studying how an infinitesimal variation of a variable quantity induces a corresponding variation of another quantity which is a function of the first variable (quantity). Almost a century later Leonhard Euler fixed the terminology of infinitesimal calculus and introduced the notation  for a function , its variable  and its value . Until the end of the 19th century, the word variable referred almost exclusively to the arguments and the values of functions.\n\nIn the second half of the 19th century, it appeared that the foundation of infinitesimal calculus was not formalized enough to deal with apparent paradoxes such as a continuous function which is nowhere differentiable. To solve this problem, Karl Weierstrass introduced a new formalism consisting of replacing the intuitive notion of limit by a formal definition. The older notion of limit was \"when the variable  varies and tends toward , then  tends toward \", without any accurate definition of \"tends\". Weierstrass replaced this sentence by the formula\n\nin which none of the five variables is considered as varying.\n\nThis static formulation led to the modern notion of variable which is simply a symbol representing a mathematical object which either is unknown or may be replaced by any element of a given set; for example, the set of real numbers.\n\nSpecific kinds of variables\nIt is common for variables to play different roles in the same mathematical formula and names or qualifiers have been introduced to distinguish them.\nFor example, the general cubic equation\n\nis interpreted as having five variables: four, , which are taken to be given numbers and the fifth variable,  is understood to be an unknown number. To distinguish them, the variable  is called an unknown, and the other variables are called parameters or coefficients, or sometimes constants, although this last terminology is incorrect for an equation and should be reserved for the function defined by the left-hand side of this equation.\n\nIn the context of functions, the term variable refers commonly to the arguments of the functions. This is typically the case in sentences like \"function of a real variable\", \" is the variable of the function \", \" is a function of the variable \" (meaning that the argument of the function is referred to by the variable ).\n\nIn the same context, variables that are independent of  define constant functions and are therefore called constant. For example, a constant of integration is an arbitrary constant function that is added to a particular antiderivative to obtain the other antiderivatives. Because the strong relationship between polynomials and polynomial function, the term \"constant\" is often used to denote the coefficients of a polynomial, which are constant functions of the indeterminates.\n\nThis use of \"constant\" as an abbreviation of \"constant function\" must be distinguished from the normal meaning of the word in mathematics. A constant, or mathematical constant is a well and unambiguously defined number or other mathematical object, as, for example, the numbers 0, 1,  and the identity element of a group.\n\nOther specific names for variables are:\n An unknown is a variable in an equation which has to be solved for.\n An indeterminate is a symbol, commonly called variable, that appears in a polynomial or a formal power series. Formally speaking, an indeterminate is not a variable, but a constant in the polynomial ring or the ring of formal power series. However, because of the strong relationship between polynomials or power series and the functions that they define, many authors consider indeterminates as a special kind of variables.\n A parameter is a quantity (usually a number) which is a part of the input of a problem, and remains constant during the whole solution of this problem. For example, in mechanics the mass and the size of a solid body are parameters for the study of its movement. In computer science, parameter has a different meaning and denotes an argument of a function.\n Free variables and bound variables\n A random variable is a kind of variable that is used in probability theory and its applications.\n\nIt should be emphasized that all these denominations of variables are of semantic nature and that the way of computing with them (syntax) is the same for all.\n\nDependent and independent variables\n\nIn calculus and its application to physics and other sciences, it is rather common to consider a variable, say , whose possible values depend on the value of another variable, say . In mathematical terms, the dependent variable  represents the value of a function of . To simplify formulas, it is often useful to use the same symbol for the dependent variable  and the function mapping  onto . For example, the state of a physical system depends on measurable quantities such as the pressure, the temperature, the spatial position, ..., and all these quantities vary when the system evolves, that is, they are function of the time. In the formulas describing the system, these quantities are represented by variables which are dependent on the time, and thus considered implicitly as functions of the time.\n\nTherefore, in a formula, a dependent variable is a variable that is implicitly a function of another (or several other) variables. An independent variable is a variable that is not dependent.Edwards Art. 5\n\nThe property of a variable to be dependent or independent depends often of the point of view and is not intrinsic. For example, in the notation , the three variables may be all independent and the notation represents a function of three variables. On the other hand, if  and  depend on  (are dependent variables) then the notation represents a function of the single independent variable .Edwards Art. 6\n\nExamples\nIf one defines a function f from the real numbers to the real numbers by\n\nthen x is a variable standing for the argument of the function being defined, which can be any real number. In the identity\n\nthe variable i is a summation variable which designates in turn each of the integers 1, 2, ...,  n (it is also called index because its variation is over a discrete set of values) while n is a parameter (it does not vary within the formula).\n\nIn the theory of polynomials, a polynomial of degree 2 is generally denoted as ax2 + bx + c, where a, b and c are called coefficients (they are assumed to be fixed, i.e., parameters of the problem considered) while x is called a variable. When studying this polynomial for its polynomial function this x stands for the function argument. When studying the polynomial as an object in itself, x is taken to be an indeterminate, and would often be written with a capital letter instead to indicate this status.\n\nNotation\nIn mathematics, the variables are generally denoted by a single letter. However, this letter is frequently followed by a subscript, as in , and this subscript may be a number, another variable (), a word or the abbreviation of a word ( and ), and even a mathematical expression. Under the influence of computer science, one may encounter in pure mathematics some variable names consisting in several letters and digits.\n\nFollowing the 17th century French philosopher and mathematician, René Descartes, letters at the beginning of the alphabet, e.g.  a, b, c are commonly used for known values and parameters, and letters at the end of the alphabet, e.g. x, y, z, and t are commonly used for unknowns and variables of functions.Edwards Art. 4 In printed mathematics, the norm is to set variables and constants in an italic typeface.William L. Hosch (editor), The Britannica Guide to Algebra and Trigonometry, Britannica Educational Publishing, The Rosen Publishing Group, 2010, , p. 71\n\nFor example, a general quadratic function is conventionally written as:\n\nwhere a, b and c are parameters (also called constants, because they are constant functions), while x is the variable of the function. A more explicit way to denote this function is\n\nwhich makes the function-argument status of x clear, and thereby implicitly the constant status of a, b and c. Since c occurs in a term that is a constant function of x, it is called the constant term.\n\nSpecific branches and applications of mathematics usually have specific naming conventions for variables.  Variables with similar roles or meanings are often assigned consecutive letters.  For example, the three axes in 3D coordinate space are conventionally called x, y, and z.  In physics, the names of variables are largely determined by the physical quantity they describe, but various naming conventions exist.\nA convention often followed in probability and statistics is to use X, Y, Z for the names of random variables, keeping x, y, z for variables representing corresponding actual values.\n\nThere are many other notational usages. Usually, variables that play a similar role are represented by consecutive letters or by the same letter with different subscript. Below are some of the most common usages.\n a, b, c, and d (sometimes extended to e and f) often represent parameters or coefficients.\n a0, a1, a2, ... play a similar role, when otherwise too many different letters would be needed.\n ai or ui is often used to denote the i-th term of a sequence or the i-th coefficient of a series.\n f and g (sometimes h) commonly denote functions.\n i, j, and k (sometimes l or h) are often used to denote varying integers or indices in an indexed family. They may also be used to denote unit vectors.\n l and w are often used to represent the length and width of a figure.\n l is also used to denote a line.  In number theory, l often denotes a prime number not equal to p.\n n usually denotes a fixed integer, such as a count of objects or the degree of an equation.\n When two integers are needed, for example for the dimensions of a matrix, one uses commonly m and n.\n p often denotes a prime numbers or a probability.\n q often denotes a prime power or a quotient\n r often denotes a radius, a remainder or a correlation coefficient.\n t often denotes time.\n x, y and z usually denote the three Cartesian coordinates of a point in Euclidean geometry. By extension, they are used to name the corresponding axes.\n z typically denotes a complex number, or, in statistics, a normal random variable.\n α, β, γ, θ and φ commonly denote angle measures.\n ε usually represents an arbitrarily small positive number.\n ε and δ commonly denote two small positives.\n λ is used for eigenvalues.\n σ often denotes a sum, or, in statistics, the standard deviation.\n\nSee also\n Coefficient\n Constant of integration\n Constant term of a polynomial\n Free variables and bound variables (Bound variables are also known as dummy variables)\n Indeterminate (variable)\n Lambda calculus\n Mathematical expression\n Observable variable\n Physical constant\n Variable (computer science)\n\nBibliography\n \n Karl Menger, \"On Variables in Mathematics and in Natural Science\", The British Journal for the Philosophy of Science 5:18:134–142 (August 1954) \n Jaroslav Peregrin, \"Variables in Natural Language: Where do they come from?\", in M. Boettner, W. Thümmel, eds., Variable-Free Semantics, 2000, pp. 46–65.\n W.V. Quine, \"Variables Explained Away\", Proceedings of the American Philosophical Society 104:343–347 (1960).\n\nReferences\n\n \nCategory:Algebra\nCategory:Calculus\nCategory:Elementary mathematics\nCategory:Syntax (logic)"
    },
    {
      "title": "Vector algebra",
      "url": "https://en.wikipedia.org/wiki/Vector_algebra",
      "text": "In mathematics, vector algebra may mean:\n Linear algebra, specifically the basic algebraic operations of vector addition and scalar multiplication; see vector space.\n The algebraic operations in vector calculus, namely the specific additional structure of vectors in 3-dimensional Euclidean space  of dot product and especially cross product. In this sense, vector algebra is contrasted with geometric algebra, which provides an alternative generalization to higher dimensions.\n An algebra over a field, a vector space equipped with a bilinear product\nOriginal vector algebras of the nineteenth century like quaternions, tessarines, or coquaternions, each of which has its own product. The vector algebras biquaternions and hyperbolic quaternions enabled the revolution in physics called special relativity by providing mathematical models.\n\nCategory:Algebra"
    },
    {
      "title": "Wild problem",
      "url": "https://en.wikipedia.org/wiki/Wild_problem",
      "text": "A mathematical problem is wild if it contains the problem of classifying pairs of square matrices up to simultaneous similarity. Examples of wild problems are classifying indecomposable representations of any quiver that is neither a Dynkin quiver (i.e. the underlying undirected graph of the quiver is a (finite) Dynkin diagram) nor a Euclidean quiver (i.e., the underlying undirected graph of the quiver is an Affine Dynkin diagram).\n\nNecessary and sufficient conditions have been proposed to check the simultaneously block triangularization and diagonalization of a finite set of matrices under the assumption that each matrix is diagonalizable over the field of the complex numbers.\n\n References  \n\nCategory:Linear algebra\nCategory:Algebra\nCategory:Representation theory"
    },
    {
      "title": "Abū Kāmil Shujāʿ ibn Aslam",
      "url": "https://en.wikipedia.org/wiki/Ab%C5%AB_K%C4%81mil_Shuj%C4%81%CA%BF_ibn_Aslam",
      "text": "Abū Kāmil, Shujāʿ ibn Aslam ibn Muḥammad Ibn Shujāʿ (Latinized as Auoquamel, , also known as al-ḥāsib al-miṣrī—lit. \"the Egyptian reckoner\") (c. 850 – c. 930) was an Egyptian mathematician during the Islamic Golden Age. He is considered the first mathematician to systematically use and accept irrational numbers as solutions and coefficients to equations. His mathematical techniques were later adopted by Fibonacci, thus allowing Abu Kamil an important part in introducing algebra to Europe.\n\nAbu Kamil made important contributions to algebra and geometry. He was the first Islamic mathematician to work easily with algebraic equations with powers higher than  (up to ), and solved sets of non-linear simultaneous equations with three unknown variables. He illustrated the rules of signs for expanding the multiplication . He also enumerated all the possible solutions to some of his problems. He wrote all problems rhetorically, and some of his books lacked any mathematical notation beside those of integers. For example, he uses the Arabic expression \"māl māl shayʾ\" (\"square-square-thing\") for  (as ).\n\nThe muslim encyclopedist Ibn Khaldūn classified Abū Kāmil as the second greatest algebraist chronologically after al-Khwarizmi.\n\n Life \n\nAlmost nothing is known about the life and career of Abu Kamil except that he was a successor of al-Khwarizmi, whom he never personally met.\n\nWorks\n\n Book of Algebra (Kitāb fī al-jabr wa al-muqābala) \n\nThe Algebra is perhaps Abu Kamil's most influential work, which he intended to supersede and expand upon that of Al-Khwarizmi. Whereas the Algebra of al-Khwarizmi was geared towards the general public, Abu Kamil was addressing other mathematicians, or readers familiar with Euclid's Elements.  In this book Abu Kamil solves systems of equations whose solutions are whole numbers and fractions, and accepted irrational numbers (in the form of a square root or fourth root) as solutions and coefficients to quadratic equations.\n\nThe first chapter teaches algebra by solving problems of application to geometry, often involving an unknown variable and square roots. The second chapter deals with the six types of problems found in Al-Khwarizmi's book, but some of which, especially those of , were now worked out directly instead of first solving for  and accompanied with geometrical illustrations and proofs. The third chapter contains examples of quadratic irrationalities as solutions and coefficients. The fourth chapter shows how these irrationalities are used to solve problems involving polygons. The rest of the book contains solutions for sets of indeterminate equations, problems of application in realistic situations, and problems involving unrealistic situations intended for recreational mathematics.\n\nA number of Islamic mathematicians wrote commentaries on this work, including al-Iṣṭakhrī al-Ḥāsib and ʿAli ibn Aḥmad al-ʿImrānī (d. 955-6), but both commentaries are now lost.\n\nIn Europe, similar material to this book is found in the writings of Fibonacci, and some sections were incorporated and improved upon in the Latin work of John of Seville, Liber mahameleth. A partial translation to Latin was done in the 14th century by William of Luna, and in the 15th century the whole work also appeared in a Hebrew translation by Mordekhai Finzi.\n\n Book of Rare Things in the Art of Calculation (Kitāb al-ṭarā’if fi’l-ḥisāb) \n\nAbu Kamil describes a number of systematic procedures for finding integral solutions for indeterminate equations. It is also the earliest known Arabic work where solutions are sought to the type of indeterminate equations found in Diophantus's Arithmetica. However, Abu Kamil explains certain methods not found in any extant copy of the Arithmetica. He also describes one problem for which he found 2,678 solutions.\n\n On the Pentagon and Decagon (Kitāb al-mukhammas wa’al-mu‘ashshar)  \n\nIn this treatise algebraic methods are used to solve geometrical problems. Abu Kamil uses the equation  to calculate a numerical approximation for the side of a regular pentagon in a circle of diameter 10. He also uses the golden ratio in some of his calculations. Fibonacci knew about this treatise and made extensive use of it in his Practica geometriae.\n\n Book of Birds (Kitāb al-ṭair) \n\nA small treatise teaching how to solve indeterminate linear systems with positive integral solutions. The title is derived from a type of problems known in the east which involve the purchase of different species of birds. Abu Kamil wrote in the introduction:\n\nI found myself before a problem that I solved and for which I discovered a great many solutions; looking deeper for its solutions, I obtained two thousand six hundred and seventy-six correct ones. My astonishment about that was great, but I found out that, when I recounted this discovery, those who did not know me were arrogant, shocked, and suspicious of me. I thus decided to write a book on this kind of calculations, with the purpose of facilitating its treatment and making it more accessible.\n\nAccording to Jacques Sesiano, Abu Kamil remained seemingly unparalleled throughout the Middle Ages in trying to find all the possible solutions to some of his problems.\n\n On Measurement and Geometry (Kitāb al-misāḥa wa al-handasa) \n\nA manual of geometry for non-mathematicians, like land surveyors and other government officials, which presents a set of rules for calculating the volume and surface area of solids (mainly rectangular parallelepipeds, right circular prisms, square pyramids, and circular cones). The first few chapters contain rules for determining the area, diagonal, perimeter, and other parameters for different types of triangles, rectangles and squares.\n\n Lost works \n\nSome of Abu Kamil's lost works include:\n\n A treatise on the use of double false position, known as the Book of the Two Errors (Kitāb al-khaṭaʾayn). Available online at:  http://facstaff.uindy.edu/~oaks/Biblio/COMHISMA8paper.doc and \n Book on Augmentation and Diminution (Kitāb al-jamʿ wa al-tafrīq), which gained more attention after historian Franz Woepcke linked it with an anonymous Latin work, Liber augmenti et diminutionis.\n Book of Estate Sharing using Algebra (Kitāb al-waṣāyā bi al-jabr wa al-muqābala), which contains algebraic solutions for problems of Islamic inheritance and discusses the opinions of known jurists.\n\nIbn al-Nadim in his Fihrist listed the following additional titles: Book of Fortune (Kitāb al-falāḥ), Book of the Key to Fortune (Kitāb miftāḥ al-falāḥ), Book of the Adequate (Kitāb al-kifāya), and Book of the Kernel (Kitāb al-ʿasīr).\n\n Legacy \n\nThe works of Abu Kamil influenced other mathematicians, like al-Karaji and Fibonacci, and as such had a lasting impact on the development of algebra. Many of his examples and algebraic techniques were later copied by Fibonacci in his Practica geometriae and other works. Unmistakable borrowings, but without Abu Kamil being explicitly mentioned and perhaps mediated by lost treatises, are also found in Fibonacci's Liber Abaci.\n\nOn al-Khwarizmi\n\nAbu Kamil was one of the earliest mathematicians to recognize al-Khwarizmi's contributions to algebra, defending him against Ibn Barza who attributed the authority and precedent in algebra to his grandfather, 'Abd al-Hamīd ibn Turk. Abu Kamil wrote in the introduction of his Algebra:\n\nI have studied with great attention the writings of the mathematicians, examined their assertions, and scrutinized what they explain in their works; I thus observed that the book by Muḥammad ibn Mūsā al-Khwārizmī known as Algebra is superior in the accuracy of its principle and the exactness of its argumentation. It thus behooves us, the community of mathematicians, to recognize his priority and to admit his knowledge and his superiority, as in writing his book on algebra he was an initiator and the discoverer of its principles, ...\n\nNotes\n\nReferences\n \n \n \n\nFurther reading\n \n \n \n Djebbar, Ahmed. Une histoire de la science arabe: Entretiens avec Jean Rosmorduc. Seuil (2001)\n\nCategory:Mathematicians of medieval Islam\nCategory:Medieval Arab mathematicians\nCategory:9th-century mathematicians\nCategory:10th-century mathematicians\nCategory:Algebraists\nCategory:850 births\nCategory:930 deaths\nCategory:Egyptian Muslims\nCategory:Egyptian scientists\nCategory:Medieval Egyptian mathematicians\nCategory:Mathematicians who worked on Islamic inheritance"
    },
    {
      "title": "Abraham Adrian Albert",
      "url": "https://en.wikipedia.org/wiki/Abraham_Adrian_Albert",
      "text": "Abraham Adrian Albert (November 9, 1905 – June 6, 1972) was an American mathematician.http://www.jinfo.org/Mathematics_Comp.html In 1939, he received the American Mathematical Society's Cole Prize in Algebra for his work on Riemann matrices.Jewish recipients of the Frank Nelson Cole Prizes in algebra and number theory (43% of recipients)  He is best known for his work on the Albert–Brauer–Hasse–Noether theorem  on finite-dimensional division algebras over number fields and as the developer of Albert algebras, which are also known as exceptional Jordan algebras.\n\nProfessional overview\nA first generation American, he was born in Chicago and most associated with that city. He received his Bachelor of Science in 1926, Masters in 1927, and PhD in 1928, at the age of 22.  All degrees were obtained from the University of Chicago.  He married around the same time as his graduation. He spent his postdoctoral year at Princeton University and then from 1929 to 1931 he was an instructor at Columbia University. During this period he worked on Abelian varieties and their endomorphism algebras. He returned to Princeton for the opening year of the Institute for Advanced Study in 1933-34 and spent another year in Princeton in 1961-62 as the first Director of the Communications Research Division of IDA (the Institute for Defense Analyses).\n\nFrom 1931 to 1972, he served on the mathematics faculty at the University of Chicago, where he became chair of the Mathematics Department in 1958 and Dean of the Physical Sciences Division in 1961.\n\nAs a research mathematician, he is primarily known for his work as one of the principal developers of the theory of linear associative algebras and as a pioneer in the development of linear non-associative algebras, although all of this grew out of his work on endomorphism algebras of Abelian varieties.\n\nAs an applied mathematician, he also did work for the military during World War II and thereafter.  One of his most notable achievements was his groundbreaking work on cryptography. He prepared a manuscript, \"Some Mathematical Aspects of Cryptography,\" for his invited address at a meeting of the American Mathematical Society in November 1941. The theory that developed from this work can be seen in digital communications technologies.\n\nAfter WWII, he became a forceful advocate favoring government support for research in mathematics on a par with other physical sciences. He served on policy-making bodies at the Office of Naval Research, the United States National Research Council, and the National Science Foundation that funneled research grants into mathematics, giving many young mathematicians career opportunities previously unavailable.  Due to his success in helping to give mathematical research a sound financial footing, he earned a reputation as a \"statesman for mathematics.\" Albert was elected a Fellow of the American Academy of Arts and Sciences in 1968.\n\nPublications\n\nBooks\n A. A. Albert, Algebras and their radicals, and division algebras, 1928.\n .\n A. A. Albert, Structure of algebras, 1939. Colloquium publications 24, American Mathematical Society, 2003, .\n \n \n \n \n with Rebeun Sandler: \n\nArticles in PNAS\n\nReferences\n\n Further reading \n Nancy E. Albert, A3 and His Algebra: How a Boy from Chicago's West Side Became a Force in American Mathematics, iUniverse, Lincoln, NE, 2005. .\n\n External links \n \n \n Abraham Adrian Albert 1905–1972, A Biographical Memoir by Irving Kaplansky\nNational Academy of Sciences Biographical Memoir\nsearch on author Abraham Adrian Albert from Google Scholar\n\nCategory:1905 births\nCategory:1972 deaths\nCategory:20th-century American mathematicians\nCategory:American Jews\nCategory:Algebraists\nCategory:Fellows of the American Academy of Arts and Sciences\nCategory:Institute for Advanced Study visiting scholars\nCategory:Members of the United States National Academy of Sciences\nCategory:Presidents of the American Mathematical Society\nCategory:Princeton University faculty\nCategory:University of Chicago alumni\nCategory:University of Chicago faculty\nCategory:Columbia University faculty\nCategory:People from Chicago\nCategory:Mathematicians from Illinois"
    },
    {
      "title": "Shimshon Amitsur",
      "url": "https://en.wikipedia.org/wiki/Shimshon_Amitsur",
      "text": "Shimshon Avraham Amitsur (born Kaplan; ; August 26, 1921 – September 5, 1994) was an Israeli mathematician. He is best known for his work in ring theory, in particular PI rings,  an area of abstract algebra.\n\nBiography\nAmitsur was born in Jerusalem and studied at the Hebrew University under the supervision of Jacob Levitzki.  His studies were repeatedly interrupted, first by World War II and then by the Israel's War of Independence.  He received his M.Sc. degree in 1946, and his Ph.D. in 1950.  Later, for his joint work with Levitzki, he received the first Israel Prize in Exact Sciences.  He worked at the Hebrew University until his retirement in 1989. Amitsur was a visiting scholar at the Institute for Advanced Study from 1952 to 1954.Institute for Advanced Study: A Community of Scholars  He was an Invited Speaker at the ICM in 1970 in Nice.Amitsur, S. A. \"Some results on rings with polynomial identities.\"  Actes, Congrès. intern. math. Tome 1 (1970): 269–272. He was a member of the Israel Academy of Sciences, where he was the Head for Experimental Science Section.  He was one of the founding editors of the Israel Journal of Mathematics, and the mathematical editor of the Hebrew Encyclopedia.  Amitsur received a number of awards, including the honorary doctorate from Ben-Gurion University in 1990.  His students included Avinoam Mann, Amitai Regev, Eliyahu Rips and Aner Shalev.\n\nAwards\nAmitsur and Jacob Levitzki were each awarded the Israel Prize in exact sciences, in 1953, its inaugural year.\n\nSee also\n\nAmitsur–Levitzki theorem\nList of Israel Prize recipients\n\nPublications\n\nReferences\n\n \"Shimshon Avraham Amitsur (1921 — 1994)\", by A. Mann, Israel Journal of Mathematics, Vol. 96 (December 1996), ix - xxvii.\n\n External links \n \n \n\nCategory:Hebrew University of Jerusalem alumni\nCategory:Hebrew University of Jerusalem faculty\nCategory:Jews in Mandatory Palestine\nCategory:Israeli Jews\nCategory:Israel Prize in exact science recipients\nCategory:Israel Prize in exact science recipients who were mathematicians\nCategory:Members of the Israel Academy of Sciences and Humanities\nCategory:Institute for Advanced Study visiting scholars\nCategory:Israeli mathematicians\nCategory:20th-century Israeli mathematicians\nCategory:Algebraists\nCategory:People from Jerusalem\nCategory:1921 births\nCategory:1994 deaths"
    },
    {
      "title": "Alexander Anderson (mathematician)",
      "url": "https://en.wikipedia.org/wiki/Alexander_Anderson_%28mathematician%29",
      "text": "thumb|Supplementum Apollonii redivivi, 1612\n\nAlexander Anderson ( in Aberdeen –  in Paris) was a Scottish mathematician.\n\nLife\nHe was born in Aberdeen, possibly in 1582, according to a print which suggests he was aged 35 in 1617.Wikisource:Anderson, Alexander (1582-1619?) (DNB00) It is unknown where he was educated, but it is likely that he initially studied writing and philosophy (the \"belles lettres\") in his home city of Aberdeen.\n\nHe then went to the continent, and was a professor of mathematics in Paris by the start of the seventeenth century. There he published or edited, between the years 1612 and 1619, various geometric and algebraic tracts. He described himself as having \"more wisdom than riches\" in the dedication of Vindiciae Archimedis (1616).\n\nHe was first cousin of David Anderson of Finshaugh, a celebrated mathematician, and David Anderson's daughter was the mother of mathematician James Gregory.\n\nWork\nHe was selected by the executors of François Viète to revise and edit Viète's manuscript works. Viète died in 1603, and it is unclear if Anderson knew him, but his eminence was sufficient to attract the attention of the dead man's executors. Anderson corrected and expanded upon Viète's manuscripts, which extended known geometry to the new algebra, which used general symbols to represent quantities.\n\nPublications\nThe known works of Anderson amount to six thin quarto volumes, and as the last of them was published in 1619, it is probable that the author died soon after that year, but the precise date is unknown. He wrote other works that have since been lost. From his last work it appears he wrote another piece, \"A Treatise on the Mensuration of Solids,\" and copies of two other works, Ex. Math. and Stereometria Triangulorum Sphæricorum, were in the possession of Sir Alexander Hume until the after the middle of the seventeenth century.\n\n1612: Supplementum Apollonii Redivivi\n1615: Ad Angularum Sectionem Analytica Theoremata F. Vieta\n1615: Pro Zetetico Apolloniani\n1615: Francisci Vietae Fontenaeensis\n1616: Vindiciae Archimedis\n1619: Alexandri Andersoni Exercitationum Mathematicarum Decas Prima\n\nSee also\n Marin Getaldić\n Denis Henrion\n Frans van Schooten\n\nReferences\n\nAttribution:\n\nFurther reading\n \n\nCategory:1580s births\nCategory:1620 deaths\nCategory:People from Aberdeen\nCategory:Algebraists\nCategory:Geometers\nCategory:Scottish mathematicians\nCategory:17th-century mathematicians\nCategory:17th-century Scottish people\nCategory:Scottish scholars and academics\nCategory:University of Paris faculty\nCategory:17th-century Scottish scientists\nCategory:17th-century Scottish mathematicians"
    },
    {
      "title": "Anton Sushkevich",
      "url": "https://en.wikipedia.org/wiki/Anton_Sushkevich",
      "text": "Anton Kazimirovich Sushkevich (23 January 1889, Borisoglebsk, Russia — 30 August 1961, Kharkov, Ukraine) was a Russian mathematician and textbook author who expanded group theory to include semigroups and other magmas.\n\nSushkevich attended secondary school in Voronezh and studied in Berlin from 1906 to 1911. There he attended lectures of F. G. Frobenius, Issai Schur, and Hermann Schwarz. Sushkevich studied piano with L. V. Rostropovich, father of Mstislav Rostropovich. In 1906 he was  a cello student at Stern Conservatory (now part of Berlin University of the Arts). In 1911 he moved to Saint Petersburg, graduating from the Imperial University in 1913.\n\nMoving to Kharkov, Suskevich taught in secondary education while he pursued a graduate degree at Kharkov State University. His dissertation was The theory of operations as the general theory of groups. Obtaining the degree, he became an assistant professor at the university in 1918, and adjunct professor in 1920.\nVoronezh State University employed Sushkevich in 1921 a professor of mathematics. He published the first edition of his Higher Algebra (1923). He published a generalization of Cayley's theorem for certain finite semigroups in 1926.A. Sushkevich (1926) \"Über die Darstellung der eindeutig nicht umkehrbaren Gruppen mittels der verallgemeinerten Substitutionen\", Matematicheskii Sbornik 33: 371–4 The next year he was in Moscow for the Russian Mathematical Congress, and the following year in Bologna for the International Congress of Mathematicians.\n\nIn Kharkov, the Ukrainian Scientific Research Institute of Mathematics and Mechanics was established in 1929 with Sushkevich as a member. With a rising interest in abstract algebra, he wrote a second book on algebra: Foundations of Higher Algebra which was published both in Russian and Ukrainian. In 1933 he directed the Algebra & Number Theory section of Kharkov State University's department of mathematics. At that time Stalin caused a famine in Ukraine, the Holodomor, killing millions especially in rural areas. Suskevich survived to edit new editions of his textbook that included \"new algebra\": fields, integral domains, ring (mathematics)s, ideals, and quaternions. His original work, The Theory of Generalized Groups (1937) opened up the area of semigroups. According to biographer Hollings, \"He sought to describe his semigroups of interest in terms of certain of their subgroups: from Sushkevich's point of view, groups were objects of known structure.\"(page 46)\n\nSelected works\n 1928: .\n 1929: \"On a generalization of the associative law\", Transactions of the American Mathematical Society 31(1):204–14  \n 1951: \"Materials for the History of Algebra in Russia in the 19th and beginning of the 20th centuries\", \n 1954: Theory of Numbers, second edition 1956 \n\nReferences\n\n Christopher Hollings (2014) Mathematics Across the Iron Curtain: A History of the Algebraic Theory of Semigroups, chapter 3: Anton Kazimirovich Sushkevich, pages 45 to 76, American Mathematical Society,  \n \n\nCategory:20th-century Russian mathematicians\nCategory:Textbook writers\nCategory:Algebraists"
    },
    {
      "title": "Vladimir Arnold",
      "url": "https://en.wikipedia.org/wiki/Vladimir_Arnold",
      "text": "Vladimir Igorevich Arnold (alternative spelling Arnol'd, , 12 June 1937 – 3 June 2010)Mort d'un grand mathématicien russe, AFP (Le Figaro) was a Soviet and Russian  mathematician. While he is best known for the Kolmogorov–Arnold–Moser theorem regarding the stability of integrable systems, he made important contributions in several areas including dynamical systems theory, algebra, catastrophe theory, topology, algebraic geometry, symplectic geometry, differential equations, classical mechanics, hydrodynamics and singularity theory, including posing the ADE classification problem, since his first main result—the solution of Hilbert's thirteenth problem in 1957 at the age of 19. He co-founded two new branches of mathematics—KAM theory, and topological Galois theory (this, with his student Askold Khovanskii).\n\nArnold was also known as a popularizer of mathematics. Through his lectures, seminars, and as the author of several textbooks (such as the famous Mathematical Methods of Classical Mechanics, and Lectures on Partial Differential Equations) and popular mathematics books, he influenced many mathematicians and physicists. Many of his books were translated into English. His views on education were particularly anti-Bourbaki.\n\nBiography\nVladimir Igorevich Arnold was born on 12 June 1937 in Odessa, Soviet Union. His father was Igor Vladimirovich Arnold (1900–1948), a mathematician. His mother was Nina Alexandrovna Arnold (1909–1986, née Isakovich), an art historian. When Arnold was thirteen, an uncle who was an engineer told him about calculus and how it could be used to understand some physical phenomena, this contributed to spark his interest for mathematics, and he started to study by himself the mathematical books his father had left to him, which included some works of Leonhard Euler and Charles Hermite.Табачников, С. Л. . \"Интервью с В.И.Арнольдом\", Квант, 1990, Nº 7, pp. 2–7. (in Russian)\n\nWhile a student of Andrey Kolmogorov at Moscow State University and still a teenager, Arnold showed in 1957 that any continuous function of several variables can be constructed with a finite number of two-variable functions, thereby solving Hilbert's thirteenth problem. This is the Kolmogorov–Arnold representation theorem.\n\nAfter graduating from Moscow State University in 1959, he worked there until 1986 (a professor since 1965), and then at Steklov Mathematical Institute.\n\nHe became an academician of the Academy of Sciences of the Soviet Union (Russian Academy of Science since 1991) in 1990.Great Russian Encyclopedia (2005), Moscow: Bol'shaya Rossiyskaya Enciklopediya Publisher, vol. 2. Arnold can be said to have initiated the theory of symplectic topology as a distinct discipline. The Arnold conjecture on the number of fixed points of Hamiltonian symplectomorphisms and Lagrangian intersections were also a major motivation in the development of Floer homology.\n\nIn 1999 he suffered a serious bike accident in Paris, resulting in traumatic brain injury, and though he regained consciousness after a few weeks, he had amnesia and for some time could not even recognize his own wife at the hospital,Arnold: Yesterday and Long Ago (2010) but he went on to make a good recovery.Polterovich and Scherbak (2011)\n\nArnold worked at the Steklov Mathematical Institute in Moscow and at Paris Dauphine University up until his death.  he was reported to have the highest citation index among Russian scientists,List of Russian Scientists with High Citation Index and h-index of 40.\n\nTo his students and colleagues Arnold was known also for his sense of humour. For example, once at his seminar in Moscow, at the beginning of the school year, when he usually was formulating new problems, he said:\n\n Death \nArnold died of acute pancreatitis on 3 June 2010 in Paris, nine days before his 73rd birthday. His students include Alexander Givental, Victor Goryunov, Sabir Gusein-Zade, Emil Horozov, Boris Khesin, Askold Khovanskii, Nikolay Nekhoroshev, Boris Shapiro, Alexander Varchenko, Victor Vassiliev and Vladimir Zakalyukin.\n\nHe was buried on June 15 in Moscow, at the Novodevichy Monastery.\n\nIn a telegram to Arnold's family, Russian President Dmitry Medvedev stated:\n\nPopular mathematical writings\nArnold is well known for his lucid writing style, combining mathematical rigour with physical intuition, and an easy conversational style of teaching and education. His writings present a fresh, often geometric approach to traditional mathematical topics like ordinary differential equations, and his many textbooks have proved influential in the development of new areas of mathematics. The standard criticism about Arnold's pedagogy is that his books \"are beautiful treatments of their subjects that are appreciated by experts, but too many details are omitted for students to learn the mathematics required to prove the statements that he so effortlessly justifies.\" His defense is that his books are meant to teach the subject to \"those who truly wish to understand it\" (Chicone, 2007).Carmen Chicone (2007), Book review of \"Ordinary Differential Equations\", by Vladimir I. Arnold. Springer-Verlag, Berlin, 2006. SIAM Review 49(2):335–336. (Chicone mentions the criticism but does not agree with it.)\n\nArnold was an outspoken critic of the trend towards high levels of abstraction in mathematics during the middle of the last century. He had very strong opinions on how this approach—which was most popularly implemented by the Bourbaki school in France—initially had a negative impact on French mathematical education, and then later on that of other countries as well.See  and other essays in .An Interview with Vladimir Arnol'd, by S. H. Lui, AMS Notices, 1991. Arnold was very interested in the history of mathematics.Oleg Karpenkov. \"Vladimir Igorevich Arnold\" In an interview, he said he had learned much of what he knew about mathematics through the study of Felix Klein's book Development of Mathematics in the 19th Century —a book he often recommended to his students.B. Khesin and S. Tabachnikov, Tribute to Vladimir Arnold, Notices of the AMS, 59:3 (2012) 378–399. He liked to study the classics, most notably the works of Huygens, Newton and Poincaré,. and many times he reported to have found in their works ideas that had not been explored yet.See for example: Arnold, V. I.; Vasilev, V. A. (1989), \"Newton's Principia read 300 years later\" and  Arnold, V. I. (2006); \"Forgotten and neglected theories of Poincaré\".\n\nWork\n\nArnold worked on dynamical systems theory, catastrophe theory, topology, algebraic geometry, symplectic geometry, differential equations, classical mechanics, hydrodynamics and singularity theory.\n\n Hilbert's thirteenth problem \nThe problem is the following question: can every continuous function of three variables be expressed as a composition of finitely many continuous functions of two variables?  The affirmative answer to this general question was given in 1957 by Vladimir Arnold, then only nineteen years old and a student of Andrey Kolmogorov. Kolmogorov had shown in the previous year that any function of several variables can be constructed with a finite number of three-variable functions. Arnold then expanded on this work to show that only two-variable functions were in fact required, thus answering the Hilbert's question when posed for the class of continuous functions.\n\n Dynamical systems \n\nMoser and Arnold expanded the ideas of Kolmogorov (who was inspired by questions of Poincaré) and gave rise to what is now known as Kolmogorov–Arnold–Moser theorem (or \"KAM theory\"), which concerns the persistence of some quasi-periodic motions (nearly integrable Hamiltonian systems) when they are perturbed. KAM theory shows that, despite the perturbations, such systems can be stable over an infinite period of time, and specifies what the conditions for this are.\n\nSingularity theory\nIn 1965, Arnold attended René Thom's seminar on catastrophe theory. He later said of it: \"I am deeply indebted to Thom, whose singularity seminar at the Institut des Hautes Etudes Scientifiques, which I frequented throughout the year 1965, profoundly changed my mathematical universe.\" After this event, singularity theory became one of the major interests of Arnold and his students.http://www.ias.ac.in/resonance/Volumes/19/09/0787-0796.pdf Among his most famous results in this area is his classification of simple singularities, contained in his paper \"Normal forms of functions near degenerate critical points, the Weyl groups of Ak,Dk,Ek and Lagrangian singularities\".Note: It also appears in another article by him, but in English: Local Normal Forms of Functions, http://www.maths.ed.ac.uk/~aar/papers/arnold15.pdf\n\nFluid dynamics\nIn 1966, Arnold published \"\", in which he presented a common geometric interpretation for both the Euler's equations for rotating rigid bodies and the Euler's equations of fluid dynamics, this effectively linked topics previously thought to be unrelated, and enabled mathematical solutions to many questions related to fluid flows and their turbulence.https://www.theguardian.com/science/2010/aug/19/v-i-arnold-obituaryIAMP News Bulletin, July 2010, pp. 25–26\n\nReal algebraic geometry\nIn the year 1971, Arnold published \"On the arrangement of ovals of real plane algebraic curves, involutions of four-dimensional smooth manifolds, and the arithmetic of integral quadratic forms\",Note: The paper also appears with other names, as in http://perso.univ-rennes1.fr/marie-francoise.roy/cirm07/arnold.pdf which gave new life to real algebraic geometry. In it, he made major advances in the direction of a solution to Gudkov's conjecture, by finding a connection between it and four-dimensional topology. The conjecture was to be later fully solved by V. A. Rokhlin building on Arnold's work.\n\n Symplectic geometry\nThe Arnold conjecture, linking the number of fixed points of Hamiltonian symplectomorphisms and the topology of the subjacent manifolds, was the motivating source of many of the pioneer studies in symplectic topology.\"Arnold and Symplectic Geometry\", by Helmut Hofer\"Vladimir Igorevich Arnold and the invention of symplectic topology\", by Michèle Audin\n\nTopology\nAccording to Victor Vassiliev, Arnold \"worked comparatively little on topology for topology's sake.\" And he was rather motivated by problems on other areas of mathematics where topology could be of use. His contributions include the invention of a topological form of the Abel–Ruffini theorem and the initial development of some of the consequent ideas, a work which resulted in the creation of the field of topological Galois theory in the 1960s.\"Topology in Arnold's work\", by Victor Vassilievhttp://www.ams.org/journals/bull/2008-45-02/S0273-0979-07-01165-2/S0273-0979-07-01165-2.pdf Bulletin (New Series) of The American Mathematical Society Volume 45, Number 2, April 2008, pp. 329–334\n\n Theory of plane curves \nArnold revolutionized plane curves theory.A Panoramic View of Riemannian Geometry, by Marcel Berger\n\n Other \nArnold conjectured the existence of the gömböc.\n\nHonours and awards\n300px|right|thumb|Arnold and Russia's president\n Lenin Prize (1965, with Andrey Kolmogorov),O. Karpenkov, \"Vladimir Igorevich Arnold\", Internat. Math. Nachrichten, no. 214, pp. 49–57, 2010. (link to arXiv preprint) \"for work on celestial mechanics.\"\n Crafoord Prize (1982, with Louis Nirenberg), \"for contributions to the theory of non-linear differential equations.\"\n Foreign Honorary Member of the American Academy of Arts and Sciences (1987)\n Elected a Foreign Member of the Royal Society (ForMemRS) of London in 1988.\n Lobachevsky Prize of the Russian Academy of Sciences (1992)D. B. Anosov, A. A. Bolibrukh, Lyudvig D. Faddeev, A. A. Gonchar, M. L. Gromov, S. M. Gusein-Zade, Yu. S. Il'yashenko, B. A. Khesin, A. G. Khovanskii, M. L. Kontsevich, V. V. Kozlov, Yu. I. Manin, A. I. Neishtadt, S. P. Novikov, Yu. S. Osipov, M. B. Sevryuk, Yakov G. Sinai, A. N. Tyurin, A. N. Varchenko, V. A. Vasil'ev, V. M. Vershik and V. M. Zakalyukin (1997) . \"Vladimir Igorevich Arnol'd (on his sixtieth birthday)\". Russian Mathematical Surveys, Volume 52, Number 5. (translated from the Russian by R. F. Wheeler)\n Harvey Prize (1994), \"for basic contribution to the stability theory of dynamical systems, his pioneering work on singularity theory and seminal contributions to analysis and geometry.\"\n Dannie Heineman Prize for Mathematical Physics (2001), \"for his fundamental contributions to our understanding of dynamics and of singularities of maps with profound consequences for mechanics, astrophysics, statistical mechanics, hydrodynamics and optics.\"American Physical Society – 2001 Dannie Heineman Prize for Mathematical Physics Recipient\n Wolf Prize in Mathematics (2001), \"for his deep and influential work in a multitude of areas of mathematics, including dynamical systems, differential equations, and singularity theory.\"The Wolf Foundation – Vladimir I. Arnold Winner of Wolf Prize in Mathematics\n State Prize of the Russian Federation (2007),Названы лауреаты Государственной премии РФ Kommersant 20 May 2008. \"for outstanding success in mathematics.\"\n Shaw Prize in mathematical sciences (2008, with Ludwig Faddeev), \"for their contributions to mathematical physics.\"\n\nThe minor planet 10031 Vladarnolda was named after him in 1981 by Lyudmila Georgievna Karachkina.\n\nThe Arnold Mathematical Journal, published for the first time in 2015, is named after him..\n\nHe was a plenary speaker at both the 1974 and 1983 International Congress of Mathematicians in Vancouver and Warsaw, respectively.http://www.mathunion.org/db/ICM/Speakers/SortedByLastname.php\n\nFields Medal omission\nEven though Arnold was nominated for the 1974 Fields Medal, which was then viewed as the highest honour a mathematician could receive, interference from the Soviet government led to it being withdrawn. Arnold's public opposition to the persecution of dissidents had led him into direct conflict with influential Soviet officials, and he suffered persecution himself, including not being allowed to leave the Soviet Union during most of the 1970s and 1980s.\n\nSelected bibliography\n 1966: \"Sur la géométrie différentielle des groupes de Lie de dimension infine et ses applications a l'hydrodynamique des fluides parfaits\" Annales de l'Institut Fourier 16: 319–361 \n 1980: Mathematical Methods of Classical Mechanics, Springer-Verlag, .Review by Ian N. Sneddon (Bulletin of the American Mathematical Society, Vol. 2): http://www.ams.org/journals/bull/1980-02-02/S0273-0979-1980-14755-2/S0273-0979-1980-14755-2.pdfReview by R. Broucke (Celestial Mechanics, Vol. 28): .\n 1985: (with S. M. Gusein-Zade & A. N. Varchenko) Singularities of Differentiable Maps, Volume I: The Classification of Critical Points, Caustics and Wave Fronts. Birkhäuser.\n 1988: (with S. M. Gusein-Zade & A. N. Varchenko) Singularities of Differentiable Maps, Volume II: Monodromy and Asymptotics of Integrals. Monographs in Mathematics. Birkhäuser.\n 1988: Geometrical Methods In The Theory Of Ordinary Differential Equations, Springer-Verlag .\n 1978; Ordinary Differential Equations, The MIT Press .\n 1989: (with A. Avez) Ergodic Problems of Classical Mechanics, Addison-Wesley .\n 1990: Huygens and Barrow, Newton and Hooke: Pioneers in mathematical analysis and catastrophe theory from evolvents to quasicrystals, Eric J.F. Primrose translator, Birkhäuser Verlag (1990) .\n 1995:Topological Invariants of Plane Curves and Caustics, American Mathematical Society (1994) \n 1999: (with Valentin Afraimovich) Bifurcation Theory And Catastrophe Theory Springer \n 1998: \"On the teaching of mathematics\" (Russian) Uspekhi Mat. Nauk 53 (1998), no. 1(319), 229–234; translation in Russian Math. Surveys 53(1): 229–236.\n 2004: Teoriya Katastrof (Catastrophe Theory, in Russian), 4th ed. Moscow, Editorial-URSS (2004), .\n 2001: \"Tsepniye Drobi\" (Continued Fractions, in Russian), Moscow (2001).\n 2007; Yesterday and Long Ago, Springer (2007), .\n 2004: \n 2014: \n Real Algebraic Geometry.\n Lectures on Partial Differential Equations.\n 2015: Experimental Mathematics. American Mathematical Society (translated from Russian, 2015).\n 2015: [http://www.ams.org/bookstore-getitem/item=mcl-17Lectures and Problems: A Gift to Young Mathematicians], American Math Society, (translated from Russian, 2015)\n\nCollected works\n 2009: A. B. Givental; B. A. Khesin; J. E. Marsden; A. N. Varchenko; V. A. Vassilev; O. Ya. Viro; V. M. Zakalyukin (editors). Collected Works, Volume I: Representations of Functions, Celestial Mechanics, and KAM Theory (1957–1965). Springer\n 2013: A. B. Givental; B. A. Khesin; A. N. Varchenko; V. A. Vassilev; O. Ya. Viro; (editors). Collected Works, Volume II: Hydrodynamics, Bifurcation Theory, and Algebraic Geometry (1965–1972). Springer.\n 2016: Givental, A.B., Khesin, B., Sevryuk, M.B., Vassiliev, V.A., Viro, O.Y. (Eds.). Collected Works, Volume III: Singularity Theory 1972–1979. Springer.\n 2018: Givental, A.B., Khesin, B., Sevryuk, M.B., Vassiliev, V.A., Viro, O.Y. (Eds.). Collected Works, Volume IV: Singularities in Symplectic and Contact Geometry 1980-1985. Springer.\n\nSee also\n\nList of things named after Vladimir Arnold\nGömböc\nIndependent University of Moscow\nGeometric mechanics\n\nReferences\n\nFurther reading\n Khesin, Boris; Tabachnikov, Serge (Coordinating Editors). \"Tribute to Vladimir Arnold\", Notices of the American Mathematical Society, March 2012, Volume 59, Number 3, pp. 378–399.\n Khesin, Boris; Tabachnikov, Serge (Coordinating Editors). \"Memories of Vladimir Arnold\", Notices of the American Mathematical Society'', April 2012, Volume 59, Number 4, pp. 482–502.\n \n \n \n\nExternal links\n\n V. I. Arnold's web page\n Personal web page\n V. I. Arnold lecturing on Continued Fractions\n A short curriculum vitae\n On Teaching Mathematics, text of a talk espousing Arnold's opinions on mathematical instruction\n Problems from 5 to 15, a text by Arnold for school students, available at the IMAGINARY platform\n \n S. Kutateladze, Arnold Is Gone\n В.Б.Демидовичем (2009), МЕХМАТЯНЕ ВСПОМИНАЮТ 2: В.И.Арнольд, pp. 25–58\n Author profile in the database zbMATH\n\nCategory:1937 births\nCategory:2010 deaths\nCategory:People from Odessa\nCategory:Russian Jews\nCategory:Soviet Jews\nCategory:20th-century Russian mathematicians\nCategory:21st-century mathematicians\nCategory:Fellows of the American Academy of Arts and Sciences\nCategory:Foreign Members of the Royal Society\nCategory:Lenin Prize winners\nCategory:Mathematical analysts\nCategory:Full Members of the USSR Academy of Sciences\nCategory:Full Members of the Russian Academy of Sciences\nCategory:Members of the French Academy of Sciences\nCategory:Foreign associates of the National Academy of Sciences\nCategory:Moscow State University alumni\nCategory:Soviet mathematicians\nCategory:State Prize of the Russian Federation laureates\nCategory:Topologists\nCategory:Fluid dynamicists\nCategory:University of Paris faculty\nCategory:Wolf Prize in Mathematics laureates\nCategory:Mathematical physicists\nCategory:Textbook writers\nCategory:Geometers\nCategory:Algebraic geometers\nCategory:Differential geometers\nCategory:Dynamical systems theorists\nCategory:Newton scholars\nCategory:Deaths from peritonitis\nCategory:Moscow State University faculty\nCategory:Members of the American Philosophical Society\nCategory:Members of the German Academy of Sciences at Berlin\nCategory:Algebraists\nCategory:Jewish Ukrainian mathematicians\nCategory:Odessa Jews\nCategory:Ukrainian Jews"
    },
    {
      "title": "Emil Artin",
      "url": "https://en.wikipedia.org/wiki/Emil_Artin",
      "text": "Emil Artin (; March 3, 1898 – December 20, 1962) was an Austrian mathematician of Armenian descent. Artin was one of the leading mathematicians of the twentieth century. He is best known for his work on algebraic number theory, contributing largely to class field theory and a new construction of L-functions. He also contributed to the pure theories of rings, groups and fields.\n\nEarly life and education\n\nParents\nEmil Artin was born in Vienna to parents Emma Maria, née Laura (stage name Clarus), a soubrette on the operetta stages of Austria and Germany, and Emil Hadochadus Maria Artin, Austrian-born of mixed Austrian and Armenian descent.Armenia honors mathematician Dmitry Mirimanoff Ben Yandell, The honors class: Hilbert’s problems and their solvers, A K Peters, Ltd., 2002, , 9781568812168Notices of the AMS. Vol. 49, # 4, April 2002, pp. 469–470 Several documents, including Emil's birth certificate, list the father's occupation as “opera singer” though others list it as “art dealer.” It seems at least plausible that he and Emma had met as colleagues in the theater. They were married in St. Stephen's Parish on July 24, 1895.\n\nEarly education\nArtin entered school in September 1904, presumably in Vienna. By then, his father was already suffering symptoms of advanced syphilis, among them increasing mental instability, and was eventually institutionalized at the recently established (and imperially sponsored) insane asylum at Mauer Öhling, 125 kilometers west of Vienna.  It is notable that neither wife nor child contracted this highly infectious disease. Artin's father died there July 20, 1906.  Young Artin was eight.\n\nOn July 15, 1907, Artin's mother remarried to a man named Rudolf Hübner: a prosperous manufacturing entrepreneur in the German-speaking city then called Reichenberg, Bohemia (currently known as Liberec, in the Czech Republic). Documentary evidence suggests that Emma had already been a resident in Reichenberg the previous year, and in deference to her new husband, she had abandoned her vocal career. Hübner deemed a life in the theater unseemly unfit for the wife of a man of his position.\n\nIn September, 1907, Artin entered the Volksschule in Strobnitz, a small town in southern Czechoslovakia near the Austrian border. For that year, he lived away from home, boarding on a local farm.  The following year, he returned to the home of his mother and stepfather, and entered the Realschule in Reichenberg, where he pursued his secondary education until June, 1916.\n\nIn Reichenberg, Artin formed a lifelong friendship with a young neighbor, Arthur Baer, who became an astronomer, teaching for many years at Cambridge University.  Astronomy was an interest the two boys shared already at this time. They each had telescopes. They also rigged a telegraph between their houses, over which once Baer excitedly reported to his friend an astronomical discovery he thought he had made—perhaps a supernova, he thought—and told Artin where in the sky to look. Artin tapped back the terse reply “A-N-D-R-O-M-E-D-A N-E-B-E-L.” (Andromeda nebula)\n\nArtin's academic performance in the first years at the Realschule was spotty. Up to the end of the 1911–1912 school year, for instance, his grade in mathematics was merely “genügend,” (satisfactory). Of his mathematical inclinations at this early period he later wrote, “Meine eigene Vorliebe zur Mathematik zeigte sich erst im sechzehnten Lebensjahr, während vorher von irgendeiner Anlage dazu überhaupt nicht die Rede sein konnte.” (“My own predilection for mathematics manifested itself only in my sixteenth year; before that, one could certainly not speak of any particular aptitude for it.”) His grade in French for 1912 was actually “nicht genügend” (unsatisfactory). He did rather better work in physics and chemistry. But from 1910 to 1912, his grade for “Comportment” was “nicht genügend.”\n\nArtin spent the school year 1912–1913 away from home, in France, a period he spoke of later as one of the happiest of his life. He lived that year with the family of Edmond Fritz, in the vicinity of Paris, and attended a school there. When he returned from France to Reichenberg, his academic work markedly improved, and he began consistently receiving grades of “gut” or “sehr gut” (good or very good) in virtually all subjects—including French and “Comportment.” By the time he completed studies at the Realschule in June, 1916, he was awarded the Reifezeugnis (diploma—not to be confused with the Abitur) that affirmed him “reif mit Auszeichnung” (qualified with distinction) for graduation to a technical university.\n\nUniversity education\nNow that it was time to move on to university studies, Artin was no doubt content but to leave Reichenberg, for relations with his stepfather were clouded. According to him, Hübner reproached him “day and night” with being a financial burden, and even when Artin became a university lecturer and then a professor, Hübner deprecated his academic career as self-indulgent and belittled its paltry emolument.\n\nIn October, 1916, Artin matriculated at the University of Vienna, having focused by now on mathematics. He studied there with Philipp Furtwängler, and also took courses in astrophysics and Latin.\n\nStudies at Vienna were interrupted when Artin was drafted in June, 1918 into the Austrian army (his Army photo ID is dated July 1, 1918). Assigned to the K.u. K. 44th Infantry Regiment, he was stationed northwest of Venice at Primolano, on the Italian front in the foothills of the Dolomites.  To his great relief, Artin managed to avoid combat by volunteering for service as a translator—his ignorance of Italian notwithstanding. He did know French, of course, and some Latin, was generally a quick study, and was motivated by a highly rational fear in a theater of that war that had often proven a meat-grinder. In his scramble to learn at least some Italian, Artin had recourse to an encyclopedia, which he once consulted for help in dealing with the cockroaches that infested the Austrian barracks. At some length, the article described a variety of technical methods, concluding finally with—Artin laughingly recalled in later years—“la caccia diretta\" (\"the direct hunt\"). Indeed, “la caccia diretta” was the straightforward method he and his fellow infantrymen adopted.\n\nArtin survived both war and vermin on the Italian front, and returned late in 1918 to the University of Vienna, where he remained through Easter of the following year.\n\nBy June 1919, he had moved to Leipzig and matriculated at the University there as a \"Class 2 Auditor\" (\"Hörer zweiter Ordnung\").  Late the same year, Artin undertook the formality of standing for a qualifying examination by an academic board of the Oberrealschule in Leipzig, which he passed with the grade of “gut” (good), receiving for the second time the Reifezeugnis (diploma attesting the equivalence of satisfactory completion of 6 years at a Realschule). How this Leipzig Reifezeugnis differed technically from the one he had been granted at Reichenberg is unclear from the document, but it apparently qualified him for regular matriculation as a student at the University, which normally required the Abitur.\n\nFrom 1919 to June 1921, Artin pursued mostly mathematical studies at Leipzig.  His principal teacher and dissertation advisor was Gustav Herglotz. Additionally, Artin took courses in chemistry and various fields of physics, including mechanics, atomic theory, quantum theory, Maxwellian theory, radioactivity, and astrophysics. In June, 1921 he was awarded the Doctor of Philosophy degree, based on his “excellent” dissertation, “Quadratische Körper im Gebiete der höheren Kongruenzen“ (\"On the Arithmetic of Quadratic Function Fields over Finite Fields\"), and the oral examination which—his diploma affirms—he had passed three days earlier “with extraordinary success.”\n\nIn the fall of 1921, Artin moved to the University of Göttingen, considered the \"Mecca\" of mathematics at the time, where he pursued one year of post-doctoral studies in mathematics and mathematical physics with Richard Courant and David Hilbert. While at Göttingen, he worked closely with Emmy Noether and Helmut Hasse.\n\nAside from consistently good school grades in singing, the first documentary evidence of Artin's deep and lifelong engagement with music comes from the year in Göttingen, where he was regularly invited to join in the chamber music sessions hosted by Richard Courant. He played all the keyboard instruments, and was an especially accomplished flautist, although it is not known exactly by what instruction he had achieved proficiency on these instruments. He became especially devoted to the music of Johann Sebastian Bach.\n\nCareer\n\nProfessorship at Hamburg\nCourant arranged for Artin to receive a stipend for the summer of 1922 in Göttingen, which occasioned his declining a position offered him at the University of Kiel. The following October, however, he accepted an equivalent position at Hamburg, where in 1923, he completed the Habilitation thesis (required of aspirants to a professorship in Germany), and on July 24 advanced to the rank of Privatdozent.\n\nOn April 1, 1925, Artin was promoted to Associate Professor (außerordentlicher Professor). In this year also, Artin applied for and was granted German citizenship. He was promoted to full Professor (ordentlicher Professor) on October 15, 1926.\n\nEarly in the summer of 1925, Artin attended the Congress of the Wandervogel youth movement at Wilhelmshausen near Kassel with the intention of gathering a congenial group to undertake a trek through Iceland later that summer. Iceland (before the transforming presence of American and British forces stationed there during World War II) was still a primitive country in 1925, with a thinly scattered population and little transportation infrastructure.  Artin succeeded in finding six young men to join him in this adventure. In the second half of August, 1925, the group set out by steamer from Hamburg, first to Norway, where they boarded a second steamer that took them to Iceland, stopping at several of the small east fjord ports before arriving at their destination, Húsavík in the north of the island.  Here the Wandervogel group disembarked, their initial goal, trekking down the Laxá River to Lake Mývatn.  They made a circuit of the large, irregular lake, staying in farm houses, barns, and occasionally a tent as they went. When they slept in barns, it was often on piles of wet straw or hay. On those lucky occasions when they slept in beds, it could be nearly as damp on account of the rain trickling through the sod roofs. The tent leaked as well.\n\nArtin kept a meticulous journal of this trip, making daily entries in a neat, minuscule hand. He and several of the young men had brought cameras, so that the trek is documented also by nearly 200 small photographs.  Artin's journal attests to his overarching interest in the geology of this mid-Atlantic island, situated over the boundary of two tectonic plates whose shifting relation makes it geologically hyperactive.\n\nIn keeping with the Wandervogel ethos, Artin and his companions carried music with them wherever they visited. The young men had packed guitars and violins, and Artin played the harmoniums common in the isolated farmsteads where they found lodging. The group regularly entertained their Icelandic hosts, not in full exchange for board and lodging, to be sure, but for goodwill certainly, and sometimes for a little extra on their plates, or a modestly discounted tariff.\n\nFrom Lake Mývatn, Artin and his companions headed west towards Akureyri, passing the large waterfall Goðafoss on the way. From Akureyri, they trekked west down the Öxnadalur (Ox Valley) intending to rent pack horses and cross the high and barren interior by foot to Reykjavík. By the time they reached the lower end of Skagafjörður, however, they were persuaded by a local farmer from whom they had hoped to rent the horses that a cross-country trek was by then impracticable; with the approach of winter, highland routes were already snow-bound and impassable.  Instead of turning south, then, they turned north to Siglufjörður, where they boarded another steamer that took them around the western peninsula and down the coast to Reykjavík.  From Reykjavík, they returned via Norway to Hamburg. By Artin's calculation the distance they had covered on foot through Iceland totaled 450 kilometers.\n\nEarly in 1926, the University of Münster offered Artin a professorial position; however, Hamburg matched the offer financially, and (as noted above) promoted him to full professor, making him (along with his young colleague Helmut Hasse) one of the two youngest professors of mathematics in Germany.\n\nIt was in this period that he acquired his lifelong nickname, “Ma,” short for mathematics, which he came to prefer to his given name, and which virtually everyone who knew him well used. Although the nickname might seem to imply a narrow intellectual focus, quite the reverse was true of Artin. Even his teaching at the University of Hamburg went beyond the strict boundaries of mathematics to include mechanics and relativity theory. He kept up on a serious level with advances in astronomy, chemistry and biology (he owned and used a fine microscope), and the circle of his friends in Hamburg attests to the catholicity of his interests. It included the painter Heinrich Stegemann, and the author and organ-builder Hans Henny Jahn. Stegemann was a particularly close friend, and made portraits of Artin, his wife  Natascha, and their two Hamburg-born children. Music continued to play a central role in his life; he acquired a Neupert double manual harpsichord, and a clavichord made by the Hamburg builder Walter Ebeloe, as well as a silver flute made in Hamburg by  G. Urban. Chamber music gatherings became a regular event at the Artin apartment as they had been at the Courants in Göttingen.\n\nOn August 15, 1929, Artin married Natalia Naumovna Jasny (Natascha), a young Russian émigré who had been a student in several of his classes. One of their shared interests was photography, and when Artin bought a Leica for their joint use (a Leica A, the first commercial model of this legendary camera), Natascha began chronicling the life of the family, as well as the city of Hamburg. For the next decade, she made a series of artful and expressive portraits of Artin that remain by far the best images of him taken at any age. Artin, in turn, took many fine and evocative portraits of Natascha.  Lacking access to a professional darkroom, their films and prints had to be developed in a makeshift darkroom set up each time (and then dismantled again) in the small bathroom of whatever apartment they were occupying.  The makeshift darkroom notwithstanding, the high artistic level of the resulting photographic prints is attested to by the exhibit of Natascha's photographs mounted in 2001 by the Museum für Kunst und Gewerbe Hamburg, and its accompanying catalogue, “Hamburg—Wie Ich Es Sah.”\n\nIn 1930, Artin was offered a professorship at ETH (Eidgenössische Technische Hochschule) in Zürich, to replace Hermann Weyl, who had moved to Göttingen. He chose to remain at Hamburg, however.  Two years later, in 1932, for contributions leading to the advancement of mathematics, Artin was honored—jointly with Emmy Noether—with the Ackermann–Teubner Memorial Award, which carried a grant of 500 marks.\n\nNazi period\nIn January 1933, Natascha gave birth to their first child, Karin. A year and a half later, in the summer of 1934, son Michael was born. The political climate at Hamburg was not so poisonous as that at Göttingen, where by 1935 the mathematics department had been purged of Jewish and dissident professors. Still, Artin's situation became increasingly precarious, not only because Natascha was half Jewish, but also because Artin made no secret of his distaste for the Hitler regime.  At one point, Wilhelm Blaschke, by then a Nazi Party member, but nonetheless solicitous of the Artins’ well-being, warned Artin discreetly to close his classroom door so his frankly anti-Nazi comments could not be heard by passersby in the hallway.\n\nNatascha recalled going down to the newsstand on the corner one day and being warned in hushed tones by the man from whom she and Artin bought their paper that a man had daily been watching their apartment from across the street. Once tipped off, she and Artin became very aware of the watcher (Natascha liked to refer to him as their “spy”), and even rather enjoyed the idea of his being forced to follow them on the long walks they loved taking in the afternoons to a café far out in the countryside.\n\nToying with their watcher on a fine autumn afternoon was one thing, but the atmosphere was in fact growing inexorably serious. Natascha's Jewish father and her sister, seeing the handwriting on the wall, had already left for the U.S. in the summer of 1933. As half-Jewish, Natascha's status was, if not ultimately quite hopeless, certainly not good. Hasse, like Blaschke a nationalistic supporter of the regime, had applied for Party membership, but was nonetheless no anti-Semite. Besides he was a long-time friend and colleague of Artin's. He suggested that the two Artin children—only one quarter Jewish, or in Nazi terminology, “Mischlinge zweiten Grades”—might, if a few strategic strings could be pulled, be officially “aryanized.” Hasse offered to exert his influence with the Ministry of Education (Kultur- und Schulbehörde, Hochschulwesen), and Artin—not daring to leave any stone unturned, especially with respect to the safety of his children—went along with this effort. He asked his father-in-law, by then resident in Washington D.C., to draft and have notarized an affidavit attesting to the Christian lineage of his late wife, Natascha's mother. Artin submitted this affidavit to the Ministry of Education, but to no avail.\n\nBy this time, to be precise, on July 15, 1937, because of Natascha's status as “Mischling ersten Grades,” Artin had lost his post at the University—technically, compelled into early retirement—on the grounds of paragraph 6 of the Act to Restore the Professional Civil Service (Gesetz zur Wiederherstellung des Berufsbeamtentums) of April 7, 1933. Ironically, he had applied only some months earlier, on February 8, 1937, for a leave of absence from the University in order to accept a position offered him at Stanford.  On March 15, 1937, the response had come back denying his application for leave on the grounds that his services to the University were indispensable (“Da die Tätigkeit des Professors Dr. Artin an der Universität Hamburg nicht entbehrt werden kann. . .”).\n\nBy July, when he was summarily “retired,” (“in Ruhestand versetzt”) the position at Stanford University had been filled. However, through the efforts of Richard Courant (by then at New York University), and Solomon Lefschetz at Princeton University, a position was found for him at the University of Notre Dame in South Bend, Indiana.\n\nEmigration to the U.S.\nThe family must have worked feverishly to prepare for emigration to the United States, for this entailed among other things packing their entire household for shipment.  Since German law forbade emigrants taking more than a token sum of money out of the country, the Artins sank all the funds at their disposal into shipping their entire household, from beds, tables, chairs and double-manual harpsichord down to the last kitchen knife, cucumber slicer, and potato masher to their new home. This is why each of their residences in the United States bore such a striking resemblance to the rooms photographed so beautifully by Natascha in their Hamburg apartment (see Natascha A. Brunswick, “Hamburg:  Wie Ich Es Sah,” Dokumente der Photographie 6, Museum für Kunst und Gewerbe Hamburg, 2001, pp. 48–53) .\n\nOn the morning they were to board the Hamburg-Amerika line ship in Bremerhaven, October 21, 1937, daughter Karin woke with a high temperature. Terrified that should this opportunity be missed, the window of escape from Nazi Germany might close forever, Artin and Natascha chose to risk somehow getting Karin past emigration and customs officials without their noticing her condition. They managed to conceal Karin's feverish state, and without incident boarded the ship, as many left behind were tragically never able to do. When they landed a week later at Hoboken, New Jersey, Richard Courant and Natascha's father, the Russian agronomist Naum Jasny (then working for the U.S. Department of Agriculture) were on the dock to welcome the family to the United States.\n\nBloomington years\nIt was early November, 1937 by the time they arrived in South Bend, where Artin joined the faculty at Notre Dame, and taught for the rest of that academic year.  He was offered a permanent position the following year 170 miles to the south at Indiana University, in Bloomington. Shortly after the family resettled there, a second son, Thomas, was born on November 12, 1938.\n\nAfter moving to Bloomington, Artin quickly acquired a piano, and soon after that a Hammond Organ, a recently invented electronic instrument that simulated the sound of a pipe organ. He wanted this instrument in order primarily to play the works of J. S. Bach, and because the pedal set that came with the production model had a range of only two octaves (not quite wide enough for all the Bach pieces), he set about extending its range. Music was a constant presence in the Artin household.  Karin played the cello, and then the piano as well, and Michael played the violin. As in Hamburg, the Artin living room was regularly the venue for amateur chamber music performances.\n\nThe circle of the Artins’ University friends reflected Artin's wide cultural and intellectual interests. Notable among them were Alfred Kinsey and his wife of the Psychology Department, as well as prominent members of the Fine Arts, Art History, Anthropology, German Literature, and Music Departments. For several summer semesters, Artin accepted teaching positions at other universities, viz., Stanford in 1939 and 1940, The University of Michigan at Ann Arbor in 1941 and 1951, and The University of Colorado, in Boulder, in 1953. On each of these occasions, the family accompanied him.\n\nArtin insisted that only German be spoken in the house. Even Tom, born in the U.S., spoke German as his  first language, acquiring English only from his siblings and his playmates in the neighborhood; for the first four or five years of his life, he spoke English with a pronounced German accent. Consistent with his program of maintaining the family's German cultural heritage, Artin gave high priority to regularly reading German literature aloud to the children.  The text was frequently from Goethe's autobiographical \"Dichtung und Wahrheit,\" or his poems, \"Erlkönig,\" for instance.  Occasionally, he would read from an English text.  Favorites were Mark Twain's \"Tom Sawyer,\" Charles Dickens’s “A Christmas Carol,” and Oscar Wilde’s “The Canterville Ghost.”  For the Artin children, these readings replaced radio entertainment, which was strictly banned from the house.  There was a radio, but (with the notable exception of Sunday morning broadcasts by E. Power Biggs from the organ at the Busch-Reisinger Museum in Cambridge, to which Artin and Natascha listened still lounging in bed) it was switched on only to hear news of the war.  Similarly, the Artin household would never in years to come harbor a television set.  Once the war had ended, the radio was retired to the rear of a dark closet.\n\nAs German citizens, Artin and Natascha were technically classified as enemy aliens for the duration of the war. On April 12, 1945, with the end of the war in Europe only weeks away, they applied for naturalization as American citizens. American citizenship was granted them on February 7, 1946.\n\nOn the orders of a Hamburg doctor whom he had consulted about a chronic cough, Artin had given up smoking years before. He had vowed not to smoke so long as Adolf Hitler remained in power.  On May 8, 1945, at the news of Germany's surrender and the fall of the Third Reich, Natascha made the mistake of reminding him of this vow, and in lieu of a champagne toast, he indulged in what was intended to be the smoking of a single, celebratory cigarette. Unfortunately, the single cigarette led to a second, and another after that.  Artin returned to heavy smoking for the rest of his life.\n\nPrinceton years\nIf Göttingen had been the “Mecca” of mathematics in the 1920s and early ‘30s, Princeton, following the decimation of German mathematics under the Nazis, had become the center of the mathematical world in the 1940s. In April, 1946, Artin was appointed Professor at Princeton, at a yearly salary of $8,000. The family moved there in the fall of 1946.\n\nNotable among his graduate students at Princeton are Serge Lang, John Tate, Harold N. Shapiro, and Timothy O’Meara. Emil chose also to teach the honors section of Freshman calculus each year.  He was renowned for the elegance of his teaching.  Frei and Roquette write that Artin’s “main medium of communication was teaching and conversation:  in groups, seminars and in smaller circles.  We have many statements of people near to him describing his unpretentious way of communicating with everybody, demanding quick grasp of the essentials but never tired of explaining the necessary. He was open to all kinds of suggestions, and distributed joyfully what he knew.  He liked to teach, also to young students, and his excellent lectures, always well prepared but without written notes, were hailed for their clarity and beauty.”  (Emil Artin and Helmut Hasse:  Their Correspondence 1923–1934, Introduction.)\n\nWhenever he was asked whether mathematics was a science, Artin would reply unhesitatingly, “No. An art.” His elegant elaboration of this idea is often cited, and worth repeating here: “We all believe that mathematics is an art. The author of a book, the lecturer in a classroom tries to convey the structural beauty of mathematics to his readers, to his listeners.  In this attempt, he must always fail. Mathematics is logical to be sure, each conclusion is drawn from previously derived statements.  Yet the whole of it, the real piece of art, is not linear; worse than that, its perception should be instantaneous.  We have all experienced on some rare occasion the feeling of elation in realizing that we have enabled our listeners to see at a glance the whole architecture and all its ramifications.”\n\nIt has even been said—only half in jest—that his lectures could be too perfect, lulling a hearer into believing he had understood and assimilated an idea or a proof which, on waking the following day might seem as remote and chimerical as ever.\n\nDuring the Princeton years, Artin built a  reflecting telescope to plans he found in the magazine Sky and Telescope, which he subscribed to. He spent weeks in the basement attempting to grind the mirror to specifications, without success, and his continued failure to get it right led to increasing frustration.  Then, in California to give a talk, he made a side trip to the Mt. Wilson Observatory, where he discussed his project with the astronomers. Whether it was their technical advice, or Natascha's intuitive suggestion that it might be too cold in the basement, and that he should try the procedure upstairs in the warmth of his study (which he did), he completed the grinding of the mirror in a matter of days.  With this telescope, he surveyed the night skies over Princeton.\n\nIn September 1955, Artin accepted an invitation to visit Japan. From his letters, it is clear he was treated like royalty by the Japanese mathematical community, and was charmed by the country. He was interested in learning about the diverse threads of Buddhism, and visiting its holy sites.  In a letter home he describes his visit to the temples at Nara. “Then we were driven to a place nearby, Horiuji [Horyu-ji] where a very beautiful Buddhist temple is. We were received by the abbot, and a priest translated into English. We obtained the first sensible explanation about modern Buddhism. The difficulty of obtaining such an explanation is enormous.  To begin with most Japanese do not know and do not understand our questions. All this is made more complicated by the fact that there are numerous sects and each one has another theory. Since you get your information only piece wise, you cannot put it together.  This results in an absurd picture. I am talking of the present day, not of its original form.”\n\nHis letter goes on to outline at length the general eschatological framework of Buddhist belief.  Then he adds, “By the way, a problem given by the Zens for meditation is the following: If you clap your hands, does the sound come from the left hand or from the right?”\n\nReturn to Hamburg and personal life\nThe following year, Artin took a leave of absence to return to Germany for the first time since emigration, nearly twenty years earlier. He spent the fall semester at Göttingen, and the next at Hamburg.  For the Christmas holidays, he travelled to his birthplace, Vienna, to visit his mother, Vienna being a city he had not seen in decades.  In a letter home he described the experience of his return in a single, oddly laconic sentence:  “It is kind of amusing to walk through Vienna again.” In 1957, an honorary doctorate was conferred on Artin by the University of Freiburg.  That fall, he returned to Princeton for what would be his final academic year at that institution. He was elected a Fellow of the American Academy of Arts and Sciences in 1957.\n\nArtin's marriage to Natascha had by this time seriously frayed. Though nominally still husband and wife, resident in the same house, they were for all intents and purposes living separate lives. Artin was offered a professorship at Hamburg, and at the conclusion of Princeton's spring semester, 1958, he moved permanently to Germany.  His decision to leave Princeton University and the United States was complicated, based on multiple factors, prominent among them Princeton's (then operative) mandatory retirement age of 65. Artin had no wish to retire from teaching and direct involvement with students.  Hamburg's offer was open-ended.\n\nArtin and Natascha were divorced in 1959. In Hamburg, Artin had taken an apartment, but soon gave it over to his mother whom he had brought from Vienna to live near him in Hamburg. He in turn moved into the apartment of the mathematician Hel Braun in the same neighborhood; though they never married, their relationship was equivalent to marriage. On January 4, 1961, he was granted German citizenship. In June, 1962, on the occasion of the 300th anniversary of the death of Blaise Pascal, the University of Clermont-Ferrand conferred an honorary doctorate on him. On December 20 of the same year, Artin died at home in Hamburg, aged 64, of a heart attack.\n\nThe University of Hamburg honored his memory on April 26, 2005 by naming one of its newly renovated lecture halls The Emil Artin Lecture Hall.Zum Gedenken an Emil Artin (1898–1962)\n\nInfluence and work\nArtin was one of the leading algebraists of the century, with an influence larger than might be guessed from the one volume of his Collected Papers edited by  Serge Lang and John Tate. He worked in algebraic number theory, contributing largely to class field theory and a new construction of L-functions. He also contributed to the pure theories of rings, groups and fields. The influential treatment of abstract algebra by van der Waerden is said to derive in part from Artin's ideas, as well as those of Emmy Noether. Artin solved Hilbert's seventeenth problem in 1927. He also developed the theory of braids as a branch of algebraic topology.\n\nIn 1955 Artin was teaching foundations of geometry at New York University. He used his notes to publish Geometric Algebra in 1957, where he extended the material to include symplectic geometry.\n\nArtin was also an important expositor of Galois theory, and of the group cohomology approach to class ring theory (with John Tate), to mention two theories where his formulations became standard.\n\nConjectures\n\nHe left two conjectures, both known as Artin's conjecture. The first concerns Artin L-functions for a linear representation of a Galois group; and the second the frequency with which a given integer a is a primitive root modulo primes p, when a is fixed and p varies. These are unproven; in 1967, Hooley published a conditional proof for the second conjecture, assuming certain cases of the Generalized Riemann hypothesis.\n\nSupervision of research\nArtin advised over thirty doctoral students, including Bernard Dwork, Serge Lang, K. G. Ramanathan, John Tate, Harold N. Shapiro, Hans Zassenhaus and Max Zorn.  A more complete list of his students can be found at the Mathematics Genealogy Project website (see \"External Links,\" below).\n\nFamily\nIn 1932 he married Natascha Jasny, born in Russia to mixed parentage (her mother was Christian, her father, Jewish).Natascha Artin–Brunswick, née Jasny | Memorial2U.com Artin was not himself Jewish, but, on account of his wife's racial status in Nazi Germany, was dismissed from his university position in 1937. They had three children, one of whom is Michael Artin, an American algebraist currently at the Massachusetts Institute of Technology.\n\nSelected bibliography\n\n Reprinted in \n\n   Reprinted in \n\n Reprinted in \n \n \n\n  Artin, Emil. (1898–1962) Beiträge zu Leben, Werk und Persönlichkeit, eds., Karin Reich and Alexander Kreuzer (Dr. Erwin Rauner Verlag, Augsburg, 2007).\n \n Reprints Artin's books on the gamma function, Galois theory, the theory of algebraic numbers, and several of his papers.\n\nSee also\nList of things named after Emil Artin\n\nReferences\n\nFurther reading\n \n \n\nExternal links\n\n \n \n \"Fine Hall in its golden age: Remembrances of Princeton in the early fifties\", by Gian-Carlo Rota. Contains a section on Artin at Princeton.\n Author profile in the database zbMATH\n\nCategory:1898 births\nCategory:1962 deaths\nCategory:20th-century American mathematicians\nCategory:Algebraists\nCategory:American people of Armenian descent\nCategory:Austrian Armenians\nCategory:Austrian emigrants to the United States\nCategory:Austrian expatriates in Germany\nCategory:Fellows of the American Academy of Arts and Sciences\nCategory:Indiana University faculty\nCategory:Number theorists\nCategory:People from Hamburg\nCategory:People from Liberec\nCategory:Princeton University faculty\nCategory:Scientists from Vienna\nCategory:University of Michigan faculty\nCategory:University of Notre Dame faculty\nCategory:Ethnic Armenian mathematicians"
    },
    {
      "title": "Michael Artin",
      "url": "https://en.wikipedia.org/wiki/Michael_Artin",
      "text": "Michael Artin (; born 28 June 1934) is an American mathematician and a professor emeritus in the Massachusetts Institute of Technology mathematics department, known for his  contributions to algebraic geometry.Faculty profile, MIT mathematics department, retrieved 2011-01-03\n\nLife and career\nArtin was born in Hamburg, Germany, and brought up in Indiana. His parents were Natalia Naumovna Jasny (Natascha) and Emil Artin, preeminent algebraist of the 20th century. Artin's parents left Germany in 1937, because Michael Artin's maternal grandfather was Jewish.\n\nArtin did his undergraduate studies at Princeton University, receiving an A.B. in 1955; he then moved to Harvard University, where he received a Ph.D. in 1960 under the supervision of Oscar Zariski, defending a thesis about Enriques surfaces.\n\nIn the early 1960s, Artin spent time at the IHÉS in France, contributing to the SGA4 volumes of the Séminaire de géométrie algébrique, on topos theory and étale cohomology. His work on the problem of characterising the representable functors in the category of schemes has led to the Artin approximation theorem, in local algebra. This work also gave rise to the ideas of an algebraic space and algebraic stack, and has proved very influential in moduli theory.  Additionally, he has made contributions to the deformation theory of algebraic varieties. He began to turn his interest from algebraic geometry to noncommutative algebra (noncommutative ring theory), especially geometric aspects, after a talk by Shimshon Amitsur and an encounter in Chicago with Claudio Procesi and Lance W. Small, \"which prompted [his] first foray into ring theory\".From the MacTutor biography: \"His main research area changed from algebraic geometry to noncommutative ring theory\".\n\nIn 2002, Artin won the American Mathematical Society's annual Steele Prize for Lifetime Achievement.  In 2005, he was awarded the Harvard Centennial Medal. In 2013, he won the Wolf Prize in Mathematics, and in 2015 was awarded the National Medal of Science. He is also a member of the National Academy of Sciences and a Fellow of the American Academy of Arts and Sciences (1969), the American Association for the Advancement of Science, the Society for Industrial and Applied Mathematics, and the American Mathematical Society.List of Fellows of the American Mathematical Society, retrieved 2012-11-03.\n\nBooks\nAs author\nwith Barry Mazur: \n\nin collaboration with Alexandru Lascu & Jean-François Boutot: \nwith notes by C.S. Sephardi & Allen Tannenbaum: \n \n\nAs editor\nwith David Mumford: \nwith John Tate: \nwith Hanspeter Kraft & Reinhold Remmert: \n\nSee also\n Artin–Mazur zeta function\n Artin stacks\n Artin–Verdier duality\n\nReferences\n\n External links \n\n \n Michael Artin at MIT Mathematics\n \n\nCategory:1934 births\nCategory:Living people\nCategory:20th-century American mathematicians\nCategory:21st-century mathematicians\nCategory:People from Hamburg\nCategory:Algebraic geometers\nCategory:Algebraists\nCategory:American people of Armenian descent\nCategory:American people of German-Jewish descent\nCategory:German Armenians\nCategory:German emigrants to the United States\nCategory:German mathematicians\nCategory:German people of Austrian descent\nCategory:Harvard University alumni\nCategory:Massachusetts Institute of Technology faculty\nCategory:Fellows of the American Academy of Arts and Sciences\nCategory:Fellows of the American Association for the Advancement of Science\nCategory:Fellows of the American Mathematical Society\nCategory:Fellows of the Society for Industrial and Applied Mathematics\nCategory:Members of the United States National Academy of Sciences\nCategory:Presidents of the American Mathematical Society\nCategory:Wolf Prize in Mathematics laureates"
    },
    {
      "title": "Reinhold Baer",
      "url": "https://en.wikipedia.org/wiki/Reinhold_Baer",
      "text": "Reinhold Baer (22 July 1902 – 22 October 1979) was a German mathematician, known for his work in algebra. He introduced injective modules in 1940.  He is the eponym of Baer rings and Baer groups.\n\nBiography\nBaer studied mechanical engineering for a year at the University of Hanover. He then went to study philosophy at Freiburg in 1921. While he was at Göttingen in 1922 he was influenced by Emmy Noether and Hellmuth Kneser. In 1924 he won a scholarship for specially gifted students. Baer wrote up his doctoral dissertation and it was published in Crelle's Journal in 1927.\n\nBaer accepted a post at Halle in 1928. There, he published Ernst Steinitz's \"Algebraische Theorie der Körper\" with Helmut Hasse, first published in Crelle's Journal in 1910.\n\nWhile Baer was with his wife in Austria, Adolf Hitler and the Nazis came into power. Both of Baer's parents were Jewish, and he was for this reason informed that his services at Halle were no longer required. Louis Mordell invited him to go to Manchester and Baer accepted.\n\nBaer stayed at Princeton University and was a visiting scholar at the nearby Institute for Advanced Study from 1935 to 1937.Institute for Advanced Study: A Community of Scholars   For a short while he lived in North Carolina. From 1938 to 1956 he worked at the University of Illinois at Urbana-Champaign. He returned to Germany in 1956.\n\nAccording to biographer K. W. Gruenberg,\nThe rapid development of lattice theory in the mid-thirties suggested that projective geometry should be viewed as a special kind of lattice, the lattice of all subspaces of a vector space... [Linear Algebra and Projective Geometry (1952)] is an account of the representation of vector spaces over division rings, of projectivities by semi-linear transformations and of dualities by semi-bilinear forms.K.W. Gruenberg (2003) Illinois Journal of Mathematics 27:12,3\n\nHe died of heart failure on October 22nd in 1979.\n\nIn 2016 the Reinhold Baer Prize for the best Ph.D. thesis in Group Theory was set up in his honour.http://www.advgrouptheory.com/BaerPrize.html\n\nBibliography\n 1934: \"Erweiterung von Gruppen und ihren Isomorphismen\", Mathematische Zeitschrift 38(1): 375–416 (German)  \n 1940: \"Nilpotent groups and their generalizations\", Transactions of the American Mathematical Society 47: 393–434 \n 1944: \"The higher commutator subgroups of a group\", Bulletin of the American Mathematical Society 50: 143–160  \n 1945: \"Representations of groups as quotient groups. II. Minimal central chains of a group\", Transactions of the American Mathematical Society 58: 348–389 \n 1945: \"Representations of groups as quotient groups. III. Invariants of classes of related representations\", Transactions of the American Mathematical Society 58: 390–419 \n\nSee also\nBaer–Suzuki theorem\nBaer–Specker group\n\nReferences\n\n O. H. Kegel (1979) \"Reinhold Baer (1902 — 1979)\", Mathematical Intelligencer 2:181,2.\n\nExternal links\n\n \n K.W. Gruenberg & Derek Robinson (2003) The Mathematical Legacy of Reinhold Baer, Illinois Journal of Mathematics'' 47(1-2) from Project Euclid.\n Author profile in the database zbMATH \n Baer Family's Schedule of 1940 US Census.\n Reproduction of a talk given by Baer on his last lecture in 1967, before his retirement from the University of Frankfurt  - here is a translation.\n\nCategory:1902 births\nCategory:1979 deaths\nCategory:People from Berlin\nCategory:German Jews\nCategory:20th-century German mathematicians\nCategory:Algebraists\nCategory:Exiles from Nazi Germany\nCategory:University of Freiburg alumni\nCategory:University of Göttingen alumni\nCategory:Martin Luther University of Halle-Wittenberg faculty\nCategory:Princeton University faculty\nCategory:Institute for Advanced Study visiting scholars\nCategory:University of Illinois at Urbana–Champaign faculty\nCategory:Goethe University Frankfurt faculty"
    },
    {
      "title": "Rosemary A. Bailey",
      "url": "https://en.wikipedia.org/wiki/Rosemary_A._Bailey",
      "text": " \n\nRosemary A. Bailey  (born 1947) is a British statistician who works in the design of experiments and the analysis of variance and in related areas of combinatorial design, especially in association schemes. She has written books on the design of experiments, on association schemes, and on linear models in statistics.\n\nEducation and career\nBailey earned her Ph.D. in 1974 at the University of Oxford. Her dissertation concerned permutation groups; it was Finite Permutation Groups and was supervised by Graham Higman.\n\nShe worked at the University of Edinburgh with David Finney and at The Open University. \n\nShe is Professor Emerita of Statistics in the School of Mathematical Sciences at Queen Mary, University of London, England. She is currently Professor of Mathematics and Statistics in the School of Mathematics and Statistics at the University of St Andrews, Scotland.\n\nRecognition\nBailey is a Fellow of the Institute of Mathematical Statistics and in 2015 was elected a Fellow of the Royal Society of Edinburgh.\n\nSelected publications\n \n \n \n \n\nReferences\n\nExternal resources\n Homepage of Professor Bailey at Queen Mary University of London\n Homepage of Professor Bailey at the School of Mathematics and Statistics, University of St Andrews\n R.A. Bailey at theoremoftheday.org\n\nCategory:20th-century English mathematicians\nCategory:21st-century English mathematicians\nCategory:Academics of Queen Mary University of London\nCategory:Algebraists\nCategory:Alumni of St Hugh's College, Oxford\nCategory:Combinatorialists\nCategory:English statisticians\nCategory:Living people\nCategory:Rothamsted statisticians\nCategory:British women mathematicians\nCategory:Women statisticians\nCategory:1947 births\nCategory:Academics of the University of St Andrews\nCategory:Fellows of the Institute of Mathematical Statistics\nCategory:20th-century women mathematicians\nCategory:21st-century women mathematicians"
    },
    {
      "title": "Hyman Bass",
      "url": "https://en.wikipedia.org/wiki/Hyman_Bass",
      "text": "Hyman Bass (; born October 5, 1932)Hyman Bass. MacTutor History of Mathematics archive. Accessed January 31, 2010 is an American mathematician, known for work in algebra and in mathematics education. From 1959 to 1998 he was Professor in the Mathematics Department at Columbia University. He is currently the Samuel Eilenberg Distinguished University Professor of Mathematics and Professor of Mathematics Education at the University of Michigan.\n\nLife\nBorn to a Jewish family in Houston, Texas, he earned his B.A. in 1955 from Princeton University and his Ph.D. in 1959 from the University of Chicago.  His thesis, titled Global dimensions of rings, was written under the supervision of Irving Kaplansky.\n\nHe has held visiting appointments at the Institute for Advanced Study in Princeton, New Jersey,Institute for Advanced Study: A Community of Scholars  Institut des Hautes Études Scientifiques and École Normale Supérieure,  (Paris), Tata Institute of Fundamental Research (Bombay), University of Cambridge, University of California, Berkeley, University of Rome, IMPA (Rio), National Autonomous University of Mexico, Mittag-Leffler Institute (Stockholm), and the University of Utah. He was president of the American Mathematical Society.\n\nBass formerly chaired the Mathematical Sciences Education Board (1992–2000) at the National Academy of Sciences, and the Committee on Education of the American Mathematical Society. He was the President of ICMI from 1999 to 2006.ICMI Executive Committees 1908–2009.  International Commission on Mathematical Instruction. Accessed January 31, 2010  Since 1996 he has been collaborating with Deborah Ball and her research group at the University of Michigan on the mathematical knowledge and resources entailed in the teaching of mathematics at the elementary level. He has worked to build bridges between diverse professional communities and stakeholders involved in mathematics education.\n\nWork\nHis research interests have been in algebraic K-theory, commutative algebra and algebraic geometry, algebraic groups, geometric methods in group theory, and ζ functions on finite simple graphs.\n\nAwards and recognitions\nBass was elected as a member of the National Academy of Sciences in 1982. In 1983, he was elected a Fellow of the American Academy of Arts and Sciences. In 2002 he was elected a fellow of The World Academy of Sciences.Hyman Bass, CV, twas.org He is a 2006 National Medal of Science laureate.President to Award 2005–2006 National Medals of Science and National Medals of Technology Honoring Nation's Leading Researchers, Inventors and Innovator. National Science Foundation. Accessed January 31, 2010 In 2009 he was elected a member of the National Academy of Education.Hyman Bass Elected to the National Academy of Education, U. of Michigan Department of Education In 2012 he became a fellow of the American Mathematical Society.List of Fellows of the American Mathematical Society, retrieved 2012-11-10.\n\nSee also\n\nBass number\nBass–Serre theory\nBass–Quillen conjecture\n\nReferences\n\nExternal links\n\nDirectory page at University of Michigan\n Author profile in the database zbMATH\n\nCategory:1932 births\nCategory:American mathematicians\nCategory:American Jews\nCategory:Algebraists\nCategory:Columbia University faculty\nCategory:Fellows of the American Academy of Arts and Sciences\nCategory:Fellows of the American Mathematical Society\nCategory:Guggenheim Fellows\nCategory:Living people\nCategory:Mathematics educators\nCategory:Members of the United States National Academy of Sciences\nCategory:National Medal of Science laureates\nCategory:Institute for Advanced Study visiting scholars\nCategory:Nicolas Bourbaki\nCategory:Presidents of the American Mathematical Society\nCategory:People from Houston\nCategory:Princeton University alumni\nCategory:University of Chicago alumni\nCategory:University of Michigan faculty\nCategory:Mathematicians from Texas"
    },
    {
      "title": "George Bergman",
      "url": "https://en.wikipedia.org/wiki/George_Bergman",
      "text": "George Mark Bergman, born on 22 July 1943 in Brooklyn, New York,CV Berkeley is an American mathematician. He attended Stuyvesant High School in New York CityThe Campaign for Stuyvesant and received his Ph.D. from Harvard University in 1968, under the direction of John Tate. The year before he had been appointed Assistant Professor of mathematics at the University of California, Berkeley, where he has taught ever since, being promoted to Associate Professor in 1974 and to Professor in 1978.\n\nHis primary research area is algebra, in particular associative rings, universal algebra, category theory and the construction of counterexamples. Mathematical logic is an additional research area. Bergman officially retired in 2009, but is still teaching.Faculty website His interests beyond mathematics include subjects as diverse as third-party politics and the works of James Joyce.\n\nHe was designated a member of the Inaugural Class of Fellows of the American Mathematical Society in 2013.\n\nSelected bibliography\n An Invitation to General Algebra and Universal Constructions (updated 2014)\n Homomorphic images of pro-nilpotent algebras. Illinois J. Math., 55, 719-748. (2011)\n Generating infinite symmetric groups. Bull. London Math. Soc. 38 429-440. (2006)\n (with Adam O. Hausknecht) Co-groups and co-rings in categories of associative rings. Mathematical Surveys and Monographs, Vol. 45. American Mathematical Society Providence, RI x+388. (1996)\n Embedding rings in completed graded rings. IV. Commutative algebras. J. Algebra 84 No.1, 62-106. (1983)\n The diamond lemma for ring theory. Adv. in Math. 29 No.2, 178-218. (1978)\n Rational relations and rational identities in division rings. II. J. Algebra 43 No.1, 267-297. (1976)\n Coproducts and some universal ring constructions. Trans. Amer. Math. Soc. 200 33-88. (1974)\n\nReferences\n\nExternal links\n \n Wall Street Journal article, Oct 27, 2009\n UC Berkeley website\n\nCategory:University of California, Berkeley faculty\nCategory:20th-century American mathematicians\nCategory:21st-century American mathematicians\nCategory:Stuyvesant High School alumni\nCategory:Living people\nCategory:1943 births\nCategory:Harvard University alumni\nCategory:People from Brooklyn\nCategory:Mathematicians from New York (state)\nCategory:Algebraists\nCategory:Fellows of the American Mathematical Society"
    },
    {
      "title": "Étienne Bézout",
      "url": "https://en.wikipedia.org/wiki/%C3%89tienne_B%C3%A9zout",
      "text": "Étienne Bézout (; 31 March 1730 – 27 September 1783) was a French mathematician who was born in Nemours, Seine-et-Marne, France,  and died in Avon (near Fontainebleau), France.\n\nWork\nIn 1758 Bézout was elected an adjoint in mechanics of the French Academy of Sciences.\nBesides numerous minor works, wrote a Théorie générale des équations algébriques, published at Paris in 1779, which in particular contained much new and valuable matter on the theory of elimination and symmetrical functions of the roots of an equation: he used determinants in a paper in the Histoire de l'académie royale, 1764, but did not treat the general theory.\n\nSee also\n Bézout's theorem\n Bézout's identity\n Bézout matrix\n Bézout domain\n\nReferences\nThe original version of this article was taken from the  public domain Rouse History of Mathematics\n\nExternal links\n \n \n\nCategory:1730 births\nCategory:1783 deaths\nCategory:People from Nemours\nCategory:18th-century French mathematicians\nCategory:Algebraists\nCategory:Number theorists\nCategory:Members of the French Academy of Sciences\nCategory:Burials in Île-de-France"
    },
    {
      "title": "Bhāskara II",
      "url": "https://en.wikipedia.org/wiki/Bh%C4%81skara_II",
      "text": "Bhāskara (also known as Bhāskarāchārya (\"Bhāskara, the teacher\"), and as Bhaskara II to avoid confusion with Bhāskara I) (1114–1185), was an Indian mathematician and astronomer. He was born in Bijapur in Karnataka.Mathematical Achievements of Pre-modern Indian Mathematicians  by T.K Puttaswamy p.331\n\nBhāskara and his works represent a significant contribution to mathematical and astronomical knowledge in the 12th century. He has been called the greatest mathematician of medieval India. His main work Siddhānta Shiromani, (Sanskrit for \"Crown of Treatises\") is divided into four parts called Lilāvatī, Bījagaṇita, Grahagaṇita and Golādhyāya, which are also sometimes considered four independent works. These four sections deal with arithmetic, algebra, mathematics of the planets, and spheres respectively. He also wrote another treatise named Karaṇa Kautūhala.\n\nBhāskara's work on calculus predates Newton and Leibniz by over half a millennium. He is particularly known in the discovery of the principles of differential calculus and its application to astronomical problems and computations. While Newton and Leibniz have been credited with differential and integral calculus, there is strong evidence to suggest that Bhāskara was a pioneer in some of the principles of differential calculus. He was perhaps the first to conceive the differential coefficient and differential calculus.\n\nOn 20 November 1981 the Indian Space Research Organisation launched the Bhaskara II satellite honouring the mathematician and astronomer.Bhaskara NASA 16 September 2017\n\nDate, place and family\nBhāskara gives his date of birth, and date of composition of his major work, in a verse in the Āryā metre:\n\nThis reveals that he was born in 1036 of the Shaka era (1114 CE), and that he composed the Siddhānta Śiromaṇī when he was 36 years old. He also wrote another work called the Karaṇa-kutūhala when he was 69 (in 1183). His works show the influence of Brahmagupta, Sridhara, Mahāvīra, Padmanābha and other predecessors.\n\nHe was born near Vijjadavida (believed to be Bijjaragi of Vijayapur in modern Karnataka). Bhāskara is said to have been the head of an astronomical observatory at Ujjain, the leading mathematical center of medieval India. He lived in the Sahyadri region (Patnadevi, in Jalgaon district, Maharashtra).\n\nHistory records his great-great-great-grandfather holding a hereditary post as a court scholar, as did his son and other descendants. His father Mahesvara (Maheśvaropādhyāya)  was a mathematician, astronomer and astrologer, who taught him mathematics, which he later passed on to his son Loksamudra. Loksamudra's son helped to set up a school in 1207 for the study of Bhāskara's writings. He died in 1185 CE.\n\nThe Siddhanta-Shiromani\n\nLilavati\nThe first section Līlāvatī (also known as pāṭīgaṇita or aṅkagaṇita), named after his daughter, consists of 277 verses. It covers calculations, progressions, measurement, permutations, and other topics.\n\nBijaganita\nThe second section Bījagaṇita has 213 verses. It discusses zero, infinity, positive and negative numbers, and indeterminate equations including (the now called) Pell's equation, solving it using a kuṭṭaka method. In particular, he also solved the  case that was to elude Fermat and his European contemporaries centuries later.\n\nGrahaganita\nIn the third section Grahagaṇita, while treating the motion of planets, he considered their instantaneous speeds. He arrived at the approximation:\n  for  close to , or in modern notation:\n .\nIn his words:\n\nThis result had also been observed earlier by Muñjalācārya (or Mañjulācārya) in 932, in his astronomical work Laghu-mānasam, in the context of a table of sines.\n\nBhāskara also stated that at its highest point a planet's instantaneous speed is zero.\n\nMathematics\nSome of Bhaskara's contributions to mathematics include the following:\n\n A proof of the Pythagorean theorem by calculating the same area in two different ways and then canceling out terms to get a2 + b2 = c2.Verses 128, 129 in Bijaganita  \n In Lilavati, solutions of quadratic, cubic and quartic indeterminate equations are explained.Mathematical Achievements of Pre-modern Indian Mathematicians von T.K Puttaswamy\n Solutions of indeterminate quadratic equations (of the type ax2 + b = y2).\n Integer solutions of linear and quadratic indeterminate equations (Kuṭṭaka). The rules he gives are (in effect) the same as those given by the Renaissance European mathematicians of the 17th century\n A cyclic Chakravala method for solving indeterminate equations of the form ax2 + bx + c = y.  The solution to this equation was traditionally attributed to William Brouncker in 1657, though his method was more difficult than the chakravala method.\n The first general method for finding the solutions of the problem x2 − ny2 = 1 (so-called \"Pell's equation\") was given by Bhaskara II.\n Solutions of Diophantine equations of the second order, such as 61x2 + 1 = y2. This very equation was posed as a problem in 1657 by the French mathematician Pierre de Fermat, but its solution was unknown in Europe until the time of Euler in the 18th century.\n Solved quadratic equations with more than one unknown, and found negative and irrational solutions.\n Preliminary concept of mathematical analysis.\n Preliminary concept of infinitesimal calculus, along with notable contributions towards integral calculus.Students& Britannica India. 1. A to C by Indu Ramchandani\n Conceived differential calculus, after discovering an approximation of the derivative and differential coefficient.\n Stated Rolle's theorem, a special case of one of the most important theorems in analysis, the mean value theorem. Traces of the general mean value theorem are also found in his works.\n Calculated the derivatives of trigonometric functions and formulae. (See Calculus section below.)\n In Siddhanta Shiromani, Bhaskara developed spherical trigonometry along with a number of other trigonometric results. (See Trigonometry section below.)\n\nArithmetic \nBhaskara's arithmetic text Leelavati covers the topics of definitions, arithmetical terms, interest computation, arithmetical and geometrical progressions, plane geometry, solid geometry, the shadow of the gnomon, methods to solve indeterminate equations, and combinations.\n\nLilavati is divided into 13 chapters and covers many branches of mathematics, arithmetic, algebra, geometry, and a little trigonometry and measurement. More specifically the contents include:\n\n Definitions.\n Properties of zero (including division, and rules of operations with zero).\n Further extensive numerical work, including use of negative numbers and surds.\n Estimation of π.\n Arithmetical terms, methods of multiplication, and squaring.\n Inverse rule of three, and rules of 3, 5, 7, 9, and 11.\n Problems involving interest and interest computation.\n Indeterminate equations (Kuṭṭaka), integer solutions (first and second order). His contributions to this topic are particularly important, since the rules he gives are (in effect) the same as those given by the renaissance European mathematicians of the 17th century, yet his work was of the 12th century. Bhaskara's method of solving was an improvement of the methods found in the work of Aryabhata and subsequent mathematicians.\n\nHis work is outstanding for its systemisation, improved methods and the new topics that he has introduced. Furthermore, the Lilavati contained excellent recreative problems and it is thought that Bhaskara's intention may have been that a student of 'Lilavati' should concern himself with the mechanical application of the method.\n\nAlgebra\nHis Bijaganita (\"Algebra\") was a work in twelve chapters. It was the first text to recognize that a positive number has two square roots (a positive and negative square root).50 Timeless Scientists von K.Krishna Murty His work Bijaganita is effectively a treatise on algebra and contains the following topics:\n\n Positive and negative numbers.\n The 'unknown' (includes determining unknown quantities).\n Determining unknown quantities.\n Surds (includes evaluating surds).\n Kuṭṭaka (for solving indeterminate equations and Diophantine equations).\n Simple equations (indeterminate of second, third and fourth degree).\n Simple equations with more than one unknown.\n Indeterminate quadratic equations (of the type ax2 + b = y2).\n Solutions of indeterminate equations of the second, third and fourth degree.\n Quadratic equations.\n Quadratic equations with more than one unknown.\n Operations with products of several unknowns.\n\nBhaskara derived a cyclic, chakravala method for solving indeterminate quadratic equations of the form ax2 + bx + c = y. Bhaskara's method for finding the solutions of the problem Nx2 + 1 = y2 (the so-called \"Pell's equation\") is of considerable importance.\n\nTrigonometry\n\nThe Siddhānta Shiromani (written in 1150) demonstrates Bhaskara's knowledge of trigonometry, including the sine table and relationships between different trigonometric functions. He also developed spherical trigonometry, along with other interesting trigonometrical results. In particular Bhaskara seemed more interested in trigonometry for its own sake than his predecessors who saw it only as a tool for calculation. Among the many interesting results given by Bhaskara, results found in his works include computation of sines of angles of 18 and 36 degrees, and the now well known formulae for  and .\n\nCalculus\nHis work, the Siddhānta Shiromani, is an astronomical treatise and contains many theories not found in earlier works. Preliminary concepts of infinitesimal calculus and mathematical analysis, along with a number of results in trigonometry, differential calculus and integral calculus that are found in the work are of particular interest.\n\nEvidence suggests Bhaskara was acquainted with some ideas of differential calculus.  Bhaskara also goes deeper into the 'differential calculus' and suggests the differential coefficient vanishes at an extremum value of the function, indicating knowledge of the concept of 'infinitesimals'.\n\n There is evidence of an early form of Rolle's theorem in his work\n If  then  for some  with \n He gave the result that if  then , thereby finding the derivative of sine, although he never developed the notion of derivatives.\n Bhaskara uses this result to work out the position angle of the ecliptic, a quantity required for accurately predicting the time of an eclipse.\n In computing the instantaneous motion of a planet, the time interval between successive positions of the planets was no greater than a truti, or a  of a second, and his measure of velocity was expressed in this infinitesimal unit of time.\n He was aware that when a variable attains the maximum value, its differential vanishes.\n He also showed that when a planet is at its farthest from the earth, or at its closest, the equation of the centre (measure of how far a planet is from the position in which it is predicted to be, by assuming it is to move uniformly) vanishes. He therefore concluded that for some intermediate position the differential of the equation of the centre is equal to zero. In this result, there are traces of the general mean value theorem, one of the most important theorems in analysis, which today is usually derived from Rolle's theorem. The mean value theorem was later found by Parameshvara in the 15th century in the Lilavati Bhasya, a commentary on Bhaskara's Lilavati.\n\nMadhava (1340–1425) and the Kerala School mathematicians (including Parameshvara) from the 14th century to the 16th century expanded on Bhaskara's work and further advanced the development of calculus in India.\n\nAstronomy\nUsing an astronomical model developed by Brahmagupta in the 7th century, Bhāskara accurately defined many astronomical quantities, including, for example, the length of the sidereal year, the time that is required for the Earth to orbit the Sun, as approximately 365.2588 days which is the same as in Suryasiddhanta.  The modern accepted measurement is 365.25636 days, a difference of just 3.5 minutes.IERS EOP PC Useful constants.\nAn SI day or mean solar day equals 86400 SI seconds.\nFrom the mean longitude referred to the mean ecliptic and the equinox J2000 given in Simon, J. L.,  et al., \"Numerical Expressions for Precession Formulae and Mean Elements for the Moon and the Planets\" Astronomy and Astrophysics 282 (1994), 663–683.\n\nHis mathematical astronomy text Siddhanta Shiromani is written in two parts: the first part on mathematical astronomy and the second part on the sphere.\n\nThe twelve chapters of the first part cover topics such as:\n\n Mean longitudes of the planets.\n True longitudes of the planets.\n The three problems of diurnal rotation.(Diurnal motion is an astronomical term referring to the apparent daily motion of stars around the Earth, or more precisely around the two celestial poles. It is caused by the Earth's rotation on its axis, so every star apparently moves on a circle, that is called the diurnal circle.)\n Syzygies.\n Lunar eclipses.\n Solar eclipses.\n Latitudes of the planets.\n Sunrise equation\n The Moon's crescent.\n Conjunctions of the planets with each other.\n Conjunctions of the planets with the fixed stars.\n The paths of the Sun and Moon.\n\nThe second part contains thirteen chapters on the sphere. It covers topics such as:\n\nPraise of study of the sphere.\nNature of the sphere.\nCosmography and geography.\nPlanetary mean motion.\nEccentric epicyclic model of the planets.\nThe armillary sphere.\nSpherical trigonometry.\nEllipse calculations.\nFirst visibilities of the planets.\nCalculating the lunar crescent.\nAstronomical instruments.\nThe seasons.\nProblems of astronomical calculations.\n\nEngineering\nThe earliest reference to a perpetual motion machine date back to 1150, when Bhāskara II described a wheel that he claimed would run forever.\n\nBhāskara II used a measuring device known as Yaṣṭi-yantra. This device could vary from a simple stick to V-shaped staffs designed specifically for determining angles with the help of a calibrated scale.\n\nLegends\nIn his book Lilavati, he reasons: \"In this quantity also which has zero as its divisor there is no change even when many quantities have entered into it or come out [of it], just as at the time of destruction and creation when throngs of creatures enter into and come out of [him, there is no change in] the infinite and unchanging [Vishnu]\".\n\n\"Behold!\"\nIt has been stated, by several authors, that Bhaskara II proved the Pythagorean theorem by drawing a diagram and providing the single word \"Behold!\". Sometimes Bhaskara's name is omitted and this is referred to as the Hindu proof, well known by schoolchildren.\n\nHowever, as mathematics historian Kim Plofker points out, after presenting a worked out example, Bhaskara II states the Pythagorean theorem:\nHence, for the sake of brevity, the square root of the sum of the squares of the arm and upright is the hypotenuse: thus it is demonstrated.\nThis is followed by:\nAnd otherwise, when one has set down those parts of the figure there [merely] seeing [it is sufficient].\nPlofker suggests that this additional statement may be the ultimate source of the widespread \"Behold!\" legend.\n\nSee also\n List of Indian mathematicians\n\nNotes\n\nReferences\n \n \n \n \n \n \n \n \n \n \n\n \n \n \n \n \n \n \n\nFurther reading\n\n W. W. Rouse Ball. A Short Account of the History of Mathematics, 4th Edition. Dover Publications, 1960.\n George Gheverghese Joseph. The Crest of the Peacock: Non-European Roots of Mathematics, 2nd Edition. Penguin Books, 2000.\n  University of St Andrews, 2000.\n Ian Pearce. Bhaskaracharya II at the MacTutor archive. St Andrews University, 2002.\n \n\nExternal links\n\n MacTutor biography\n 4to40 Biography\n Calculus in Kerala\n\nCategory:12th-century Indian mathematicians\nCategory:12th-century Indian astronomers\nCategory:People from Bijapur, Karnataka\nCategory:1114 births\nCategory:1185 deaths\nCategory:Algebraists\nCategory:Scientists from Karnataka\nCategory:Scholars from Karnataka"
    },
    {
      "title": "Norman L. Biggs",
      "url": "https://en.wikipedia.org/wiki/Norman_L._Biggs",
      "text": "Norman Linstead Biggs (born 2 January 1941) is a leading British mathematician focusing on discrete mathematics and in particular algebraic combinatorics..\n\nEducation\nBiggs was educated at Harrow County Grammar School and then studied mathematics at Selwyn College, Cambridge. In 1962, Biggs gained first-class honours in his third year of the University's undergraduate degree in mathematics.\n\n1946–1952: Uxendon Manor Primary School, Kenton, Middlesex\n1952–1959: Harrow County Grammar School\n1959–1963: Selwyn College, Cambridge (Entrance Exhibition 1959, Scholarship 1961)\n1960: First Class, Mathematical Tripos Pt. I\n1962: Wrangler, Mathematical Tripos Pt. II; B.A. (Cantab.)\n1963: Distinction, Mathematical Tripos Pt. III\n1988: D.Sc. (London); M.A. (Cantab.)\n\nCareer\nHe was a lecturer at University of Southampton, lecturer then reader at Royal Holloway, University of London, and Professor of Mathematics at the London School of Economics. He has been on the editorial board of a number of journals, including the Journal of Algebraic Combinatorics. He has been a member of the Council of the London Mathematical Society.\n\nHe has written 12 books and over 100 papers on mathematical topics, many of them in algebraic combinatorics and its applications. He became Emeritus Professor in 2006 and continue to teach History of Mathematics in Finance and Economics for undergraduates. He is also Vice-President of the British Society for the History of Mathematics.\n\nFamily\nBiggs married Christine Mary Farmer in 1975 and has one daughter Clare Juliet born in 1980.\n\nInterests and Hobbies\nBiggs' interests include computational learning theory, the history of mathematics and historical metrology. Since 2006, he has been an Emeritus Professor at the London School of Economics.\n\nBiggs hobbies consist of writing about the history of weights and scales. He currently holds the position of Chair of the International Society of Antique Scale Collectors (Europe), and a member of the British Numismatic Society.\n\nWork\n\nMathematics\nIn 2002, Biggs wrote the second edition of Discrete Mathematics breaking down a wide range of topics into a clear and organised style. Biggs organised the book into four major sections; The Language of Mathematics, Techniques, Algorithms and Graphs, and Algebraic Methods. This book was an accumulation of Discrete Mathematics, first edition, textbook published in 1985 which dealt with calculations involving a finite number of steps rather than limiting processes. The second edition added nine new introductory chapters; Fundamental language of mathematicians, statements and proofs, the logical framework, sets and functions, and number system. This book stresses the significance of simple logical reasoning, shown by the exercises and examples given in the book. Each chapter contains modelled solutions, examples, exercises including hints and answers.\n\nAlgebraic Graph Theory\nIn 1974, Biggs published Algebraic Graph Theory which articulates properties of graphs in algebraic terms, then works out theorems regarding them. In the first section, he tackles the applications of linear algebra and matrix theory; algebraic constructions such as adjacency matrix and the incidence matrix and their applications are discussed in depth. Next, there is and wide-ranging description of the theory of chromatic polynomials. The last section discusses symmetry and regularity properties. Biggs makes important connections with other branches of algebraic combinatorics and group theory.\n\nComputational Learning Theory\nIn 1997, N. Biggs and M. Anthony wrote a book titled Computational Learning Theory: an Introduction. Both Biggs and Anthony focused on the necessary background material from logic, probability, and complex theory. This book is an introduction to computational learning.\n\nHistory of Mathematics\nBiggs contributed to thirteen journals and books developing topics such as the four-colour conjecture, the roots/history of combinatorics, calculus, Topology on the 19th century, and mathematicians. In addition, Biggs examined the ideas of William Ludlam, Thomas Harriot, John Arbuthnot, and Leonhard Euler.\n\nChip-Firing Game\n\nThe chip-firing game has been around for less than 20 years. It has become an important part of the study of structural combinatorics. The set of configurations that are stable and recurrent for this game can be given the structure of an abelian group. In addition, the order of the group is equal to the tree number of the graph.\n\nPublications\n\nSummary of Biggs' published Books on Mathematics\nFinite Groups of Automorphisms, Cambridge University Press (1971)\nAlgebraic Graph Theory, Cambridge University Press (1974)\nGraph Theory 1736-1936 (with E.K. Lloyd and R.J. Wilson), Oxford University Press (1976) (Japanese edition 1986)\nInteraction Models, Cambridge University Press (1977)\nPermutation Groups and Combinatorial Structures (with A.T. White), Cambridge University Press, (1979), (Chinese edition 1988)\nDiscrete Mathematics, Oxford University Press (1989) (Spanish edition 1994)\nIntroduction to Computing with Pascal, Oxford University Press (1989)\nComputational Learning Theory: an Introduction (with M. Anthony) (1997)\nAlgebraic Graph Theory (Second Edition), Cambridge University Press (1993)\nMathematics for Economics and Finance (with M. Anthony), Cambridge University Press (1996) (Chinese edition 1998; Japanese edition 2000)\nDiscrete Mathematics, (Second Edition), Oxford University Press (2002) \nCodes: An Introduction to Information Communication and Cryptography, Springer Verlag (2008)\n\nSummary of Biggs' latest published Papers on Mathematics\n2000\n'A matrix method for chromatic polynomials – II', CDAM Research Report Series, LSE-CDAM 2000–04, April 2000.\n(with P.Reinfeld), 'The chromatic roots of generalised dodecahedra', CDAM Research Report Series, LSE-CDAM 2000–07, June 2000.\n\n2001\n'Equimodular curves for reducible matrices', CDAM Research Report Series, LSE-CDAM 2001-01, January 2001. \n'A matrix method for chromatic polynomials', Journal of Combinatorial Theory, Series B, 82 (2001) 19–29.\n\n2002\n'Chromatic polynomials for twisted bracelets', Bull. London Math. Soc. 34 (2002) 129–139.\n'Chromatic polynomials and representations of the symmetric group', Linear Algebra and its Applications 356 (2002) 3–26.\n'Equimodular curves', Discrete Mathematics 259 (2002) 37–57.\n\n2004\n'Algebraic methods for chromatic polynomials' (with M H Klin and P Reinfeld), Europ. J. Combinatorics 25 (2004) 147–160.\n'Specht modules and chromatic polynomials', Journal of Combinatorial Theory, Series B 92 (2004) 359 – 377.\n\n2005\n'Chromatic polynomials of some families of graphs I: Theorems and Conjectures', CDAM Research Report Series, LSE-CDAM 2005–09, May 2005.\n\n2007\n'The critical group from a cryptographic perspective', Bull. London Math. Soc., 39 (2007) 829–836.\n\n2008\n'Chromatic Roots of the Quartic Mobius Ladders', CDAM Research Report LSE-CDAM 2008-05, May 2008. \n'A Matrix Method for Flow Polynomials', CDAM Research Report LSE-CDAM 2008-08, June 2008.\n\n2009\n'Tutte Polynomials of Bracelets', CDAM Research Report LSE-CDAM-2009-01, January 2009. \n'Strongly Regular Graphs with No Triangles', Research Report, September 2009. arXiv:0911.2160v1 \n'Families of Parameters for SRNT Graphs', Research Report, October 2009. arXiv:0911.2455v1\n\n2010\n'Tutte Polynomials of Bracelets', J. Algebraic Combinatorics 32 (2010) 389–398.\n'The Second Subconstituent of some Strongly Regular Graphs', Research Report', February 2010. arXiv:1003.0175v1\n\n2011\n'Some Properties of Strongly Regular Graphs', Research Report'', May 2011. arXiv:1106.0889v1\n\nFor other published work on the history of mathematics, please see.\n\nSee also\nComputational learning theory\nFour color theorem\n\nReferences\n\nExternal links\nNorman Biggs personal web page at LSE\n\nCambridge University Press: Norman L Biggs \n\nCategory:1941 births\nCategory:Living people\nCategory:People educated at Harrow High School\nCategory:Alumni of Selwyn College, Cambridge\nCategory:English mathematicians\nCategory:20th-century British mathematicians\nCategory:21st-century mathematicians\nCategory:Algebraists\nCategory:Historians of mathematics\nCategory:Theoretical computer scientists\nCategory:Academics of the University of Southampton\nCategory:Academics of Royal Holloway, University of London\nCategory:Academics of the London School of Economics"
    },
    {
      "title": "Garrett Birkhoff",
      "url": "https://en.wikipedia.org/wiki/Garrett_Birkhoff",
      "text": "Garrett Birkhoff (January 19, 1911 – November 22, 1996) was an American mathematician. He is best known for his work in lattice theory.\n\nThe mathematician George Birkhoff (1884–1944) was his father.\n\nLife\nThe son of the mathematician George David Birkhoff, Garrett was born in Princeton, New Jersey.Staff. A COMMUNITY OF SCHOLARS: The Institute for Advanced Study Faculty and Members 1930-1980, p. 90. Institute for Advanced Study, 1980. Accessed November 20, 2015. \"Birkhoff, Garrett 40s M Born 1911 Princeton, NJ.\" He began the Harvard University BA course in 1928 after less than seven years of prior formal education. Upon completing his Harvard BA in 1932, he went to Cambridge University in England to study mathematical physics but switched to studying abstract algebra under Philip Hall. While visiting the University of Munich, he met Carathéodory who pointed him towards two important texts, Van der Waerden on abstract algebra and Speiser on group theory.\n\nBirkhoff held no Ph.D., a qualification British higher education did not emphasize at that time, and did not even bother obtaining an M.A. Nevertheless, after being a member of Harvard's Society of Fellows, 1933–36, he spent the rest of his career teaching at Harvard. From these facts can be inferred the number and quality of Birkhoff's papers published by his 25th year.\n\nDuring the 1930s, Birkhoff, along with his Harvard colleagues Marshall Stone and Saunders Mac Lane, substantially advanced American teaching and research in abstract algebra. In 1941 he and Mac Lane published A Survey of Modern Algebra, the second undergraduate textbook in English on the subject (Cyrus Colton MacDuffee's An Introduction to Abstract Algebra was published in 1940). Mac Lane and Birkhoff's Algebra (1967) is a more advanced text on abstract algebra. A number of papers he wrote in the 1930s, culminating in his monograph, Lattice Theory (1940; the third edition remains in print), turned lattice theory into a major branch of abstract algebra. His 1935 paper, \"On the Structure of Abstract Algebras\" founded a new branch of mathematics, universal algebra. Birkhoff's approach to this development of universal algebra and lattice theory acknowledged prior ideas of Charles Sanders Peirce, Ernst Schröder, and Alfred North Whitehead; in fact, Whitehead had written an 1898 monograph entitled Universal Algebra. Further, in 1935, Birkoff showed that any equivalence between expressions that holds for all possible forms of operator must have a finite proof using certain underlying rules about equality. However, as soon as one introduces actual axioms that constrain the operators this is no longer true—and in general it can be undecidable whether or not a particular equivalence holds.\n\nDuring and after World War II, Birkhoff's interests gravitated towards what he called \"engineering\" mathematics. During the war, he worked on radar aiming and ballistics, including the bazooka. In the development of weapons, mathematical questions arose, some of which had not yet been addressed by the literature on fluid dynamics. Birkhoff's research was presented in his texts on fluid dynamics, Hydrodynamics (1950) and Jets, Wakes and Cavities (1957).\n\nBirkhoff, a friend of John von Neumann, took a close interest in the rise of the electronic computer. Birkhoff supervised the Ph.D. thesis of David M. Young on the numerical solution of the partial differential equation of Poisson, in which Young proposed the successive over-relaxation (SOR) method. Birkhoff then worked with Richard S. Varga, a former student, who was employed at Bettis Atomic Power Laboratory of the Westinghouse Electronic Corporation in Pittsburgh and was helping to design nuclear reactors. Extending the results of Young, the Birkhoff-Varga collaboration led to many publications on positive operators and iterative methods for p-cyclic matrices.\n\nBirkhoff's research and consulting work (notably for General Motors) developed computational methods besides numerical linear algebra, notably the representation of smooth curves via cubic splines.\n\nBirkhoff published more than 200 papers and supervised more than 50 Ph.D.s. He was a member of the National Academy of Sciences and the American Academy of Arts and Sciences. He was a Guggenheim Fellow for the academic year 1948–1949 and the president of the Society for Industrial and Applied Mathematics for 1966–1968. He won a Lester R. Ford Award in 1974.\n\nSelected books\n\nSee also\nBirkhoff polytope\nBirkhoff's representation theorem\nBirkhoff's HSP theorem\nBirkhoff's theorem (disambiguation)\nPierce–Birkhoff conjecture\nPoincaré–Birkhoff–Witt theorem\n\nReferences\n\nExternal links\n\n \n\nCategory:1911 births\nCategory:Members of the United States National Academy of Sciences\nCategory:1996 deaths\nCategory:20th-century American mathematicians\nCategory:Guggenheim Fellows\nCategory:Harvard University faculty\nCategory:Institute for Advanced Study visiting scholars\nCategory:Algebraists\nCategory:Lattice theorists\nCategory:Fluid dynamicists\nCategory:Harvard University alumni\nCategory:People from Princeton, New Jersey\nCategory:Presidents of the Society for Industrial and Applied Mathematics\nCategory:American expatriates in the United Kingdom\nCategory:Mathematicians from New Jersey"
    },
    {
      "title": "Rafael Bombelli",
      "url": "https://en.wikipedia.org/wiki/Rafael_Bombelli",
      "text": "thumb|right|L'Algebra by Rafael Bombelli: frontispiece of the Bologna edition of 1579\n\nRafael Bombelli (baptised on 20 January 1526; died 1572) was an Italian mathematician. Born in Bologna, he is the author of a treatise on algebra and is a central figure in the understanding of imaginary numbers.\n\nHe was the one who finally managed to address the problem with imaginary numbers. In his 1572 book, L'Algebra, Bombelli solved equations using the method of del Ferro/Tartaglia. He introduced the rhetoric that preceded the representative symbols +i and -i and described how they both worked.\n\nLife\nRafael Bombelli was baptised on 20 January 1526http://www.gavagai.de/philosoph/HHP78.htm in Bologna, Papal States.  He was born to Antonio Mazzoli, a wool merchant, and Diamante Scudieri, a tailor's daughter. The Mazzoli family was once quite powerful in Bologna.  When Pope Julius II came to power, in 1506, he exiled the ruling family, the Bentivoglios. The Bentivoglio family attempted to retake Bologna in 1508, but failed.  Rafael's grandfather participated in the coup attempt, and was captured and executed.  Later, Antonio was able to return to Bologna, having changed his surname to Bombelli to escape the reputation of the Mazzoli family.  Rafael was the oldest of six children.  Rafael received no college education, but was instead taught by an engineer-architect by the name of Pier Francesco Clementi.\n\nRafael Bombelli felt that none of the works on algebra by the leading mathematicians of his day provided a careful and thorough exposition of the subject.  Instead of another convoluted treatise that only mathematicians could comprehend, Rafael decided to write a book on algebra that could be understood by anyone.  His text would be self-contained and easily read by those without higher education.\n\nRafael Bombelli died in 1572 in Rome, Italy.\n\nBombelli's Algebra\nthumb|Algebra, 1572\n\nIn the book that was published in 1572, entitled Algebra, Bombelli gave a comprehensive account of the algebra known at the time.  He was the first European to write down the way of performing computations with negative numbers.  The following is an excerpt from the text:\n\n\t\"Plus times plus makes plus \n\tMinus times minus makes plus \n\tPlus times minus makes minus \n\tMinus times plus makes minus \n\tPlus 8 times plus 8 makes plus 64 \n\tMinus 5 times minus 6 makes plus 30 \n\tMinus 4 times plus 5 makes minus 20 \n\tPlus 5 times minus 4 makes minus 20\"\n\nAs was intended, Bombelli used simple language as can be seen above so that anybody could understand it.  But at the same time, he was thorough.\n\nPerhaps more importantly than his work with algebra, however, the book also includes Bombelli's monumental contributions to complex number theory.  Before he writes about complex numbers, he points out that they occur in solutions of equations of the form  given that  which is another way of stating that the discriminant of the cubic is negative.  The solution of this kind of equation requires taking the cube root of the sum of one number and the square root of some negative number.\n\nBefore Bombelli delves into using imaginary numbers practically, he goes into a detailed explanation of the properties of complex numbers.  Right away, he makes it clear that the rules of arithmetic for imaginary numbers are not the same as for real numbers.  This was a big accomplishment, as even numerous subsequent mathematicians were extremely confused on the topic.\n\nBombelli avoided confusion by giving a special name to square roots of negative numbers, instead of just trying to deal with them as regular radicals like other mathematicians did.  This made it clear that these numbers were neither positive nor negative.  This kind of system avoids the confusion that Euler encountered.  Bombelli called the imaginary number i “plus of minus” and used “minus of minus” for -i.\n\nBombelli had the foresight to see that imaginary numbers were crucial and necessary to solving quartic and cubic equations.  At the time, people cared about complex numbers only as tools to solve practical equations.  As such, Bombelli was able to get solutions using Scipione del Ferro's rule, even in the irreducible case, where other mathematicians such as Cardano had given up.\n\nIn his book, Bombelli explains complex arithmetic as follows:\n\n\t\"Plus by plus of minus, makes plus of minus. \n\tMinus by plus of minus, makes minus of minus. \n\tPlus by minus of minus, makes minus of minus. \n\tMinus by minus of minus, makes plus of minus. \n\tPlus of minus by plus of minus, makes minus. \n\tPlus of minus by minus of minus, makes plus. \n\tMinus of minus by plus of minus, makes plus. \n\tMinus of minus by minus of minus makes minus.\"\n\nAfter dealing with the multiplication of real and imaginary numbers, Bombelli goes on to talk about the rules of addition and subtraction.  He is careful to point out that real parts add to real parts, and imaginary parts add to imaginary parts.\n\nReputation\n\nBombelli is generally regarded as the inventor of complex numbers, as no one before him had made rules for dealing with such numbers, and no one believed that working with imaginary numbers would have useful results.  Upon reading Bombelli's Algebra, Leibniz praised Bombelli as an \". . . outstanding master of the analytical art.\"  Crossley  writes in his book, \"Thus we have an engineer, Bombelli, making practical use of complex numbers perhaps because they gave him useful results, while Cardan found the square roots of negative numbers useless.  Bombelli is the first to give a treatment of any complex numbers. . . It is remarkable how thorough he is in his presentation of the laws of calculation of complex numbers. . .\"[3]\n\nIn honor of his accomplishments, a moon crater was named  Bombelli.\n\nBombelli's method of calculating square roots\nBombelli used a method related to continued fractions to calculate square roots.  His method for finding  begins with  with , from which it can be shown that .  Repeated substitution of the expression on the right hand side for  into itself yields a continued fraction\n\n \n\nfor the root but Bombelli is more concerned with better approximations for .  The value chosen for  is either of the whole numbers whose squares  lies between. The method gives the following convergents for  while the actual value is 3.605551275... :\n\n \n\nThe last convergent equals 3.605550883... .  Bombelli's method should be compared with formulas and results used by Heros and Archimedes. The result  used by Archimedes in his determination of the value of  can be found by using 1 and 0 for the initial values of .\n\nReferences\n Morris Kline, Mathematical Thought from Ancient to Modern Times, 1972, Oxford University Press, New York, \n David Eugene Smith, A Source Book in Mathematics, 1959, Dover Publications, New York, \n\nReferences\nFootnotes\n\nCitations\n\nExternal links\nL'Algebra, Libri I, II, III, IV e V, original Italian texts.\n\nBackground\n\nCategory:1526 births\nCategory:1572 deaths\nCategory:Italian mathematicians\nCategory:16th-century Italian mathematicians\nCategory:Algebraists\nCategory:Italian engineers"
    },
    {
      "title": "Carl Wilhelm Borchardt",
      "url": "https://en.wikipedia.org/wiki/Carl_Wilhelm_Borchardt",
      "text": "Carl Wilhelm Borchardt (22 February 1817 – 27 June 1880) was a German mathematician.\n\nBorchardt was born to a Jewish family in Berlin. His father, Moritz, was a respected merchant, and his mother was Emma Heilborn. Borchardt studied under a number of tutors, including Julius Plücker and Jakob Steiner. He studied at the University of Berlin under Lejeune Dirichlet in 1836 and at the University of Königsberg in 1839. In 1848 he began teaching at the University of Berlin.\n\nHe did research in the area of arithmetic-geometric mean, continuing work by Gauss and Lagrange. He generalised the results of Kummer diagonalising symmetric matrices, using determinants and Sturm functions. He was also an editor of Crelle's Journal from 1856–80, during which time it was known as Borchardt's Journal.\n\nHe died in Rüdersdorf, Germany. His grave is preserved in the Protestant Friedhof III der Jerusalems- und Neuen Kirchengemeinde (Cemetery No. III of the congregations of Jerusalem's Church and New Church) in Berlin-Kreuzberg, south of Hallesches Tor.\n\n See also \n Cayley's formula\n\n References\n\nCategory:1817 births\nCategory:1880 deaths\n\nCategory:Algebraists\nCategory:19th-century German mathematicians\nCategory:German Jews\nCategory:Members of the Prussian Academy of Sciences\nCategory:Scientists from Berlin\nCategory:People from the Province of Brandenburg\nCategory:Humboldt University of Berlin alumni\nCategory:Humboldt University of Berlin faculty\nCategory:University of Königsberg alumni"
    },
    {
      "title": "Erland Samuel Bring",
      "url": "https://en.wikipedia.org/wiki/Erland_Samuel_Bring",
      "text": "Erland Samuel Bring (19 August 1736 – 20 May 1798) was a Swedish mathematician.\n\nBring studied at Lund University between 1750 and 1757. In 1762 he obtained a position of a reader in history and was promoted to professor in 1779. At Lund he wrote eight volumes of mathematical work in the fields of algebra, geometry, analysis and astronomy, including Meletemata quaedam mathematica circa transformationem aequationum algebraicarum (1786). This work describes Bring's contribution to the algebraic solution of equations.\n\nBring had developed an important transformation to simplify a quintic equation to the form  (see Bring radical). In 1832–35 the same transformation was independently derived by George Jerrard. However, whereas Jerrard knew from the past work by Paolo Ruffini and Niels Henrik Abel that a general quintic equation can not be solved, this fact was not known to Bring, putting him in a disadvantage.J J O'Connor and E F Robertson Erland Samuel Bring\n\nBring's curve is named after him.\n\nReferences\n\nCategory:1736 births\nCategory:1798 deaths\nCategory:18th-century mathematicians\nCategory:Swedish mathematicians\nCategory:Algebraists"
    },
    {
      "title": "Peter Cameron (mathematician)",
      "url": "https://en.wikipedia.org/wiki/Peter_Cameron_%28mathematician%29",
      "text": "Peter Jephson Cameron FRSE (born 23 January 1947) is an Australian mathematician who works in\ngroup theory, combinatorics, coding theory, and model theory. He is currently half-time Professor of Mathematics at the University of St Andrews, and Emeritus Professor at Queen Mary University of London.\n\nCameron received a B.Sc. from the University of Queensland and a D.Phil. in 1971 from University of Oxford, with Peter M. Neumann as his supervisor.Peter M. Neumann at the Mathematics Genealogy Project Subsequently, he was a Junior Research Fellow and later a Tutorial Fellow at Merton College, Oxford, and also lecturer at Bedford College, London. \n\nWork\nCameron specialises in algebra and combinatorics; he has written books about combinatorics, algebra, permutation groups, and logic, and has produced over 250 academic papers.Recent publications of Peter J. Cameron He posed the Cameron–Erdős conjecture with Paul Erdős.\n\n Honours and awards \nHe was awarded the London Mathematical Society's Whitehead Prize in 1979 and is joint winner of the 2003 Euler Medal.In 2018 he was elected a Fellow of the Royal Society of Edinburgh.thumb|Peter Cameron giving the 2007 Dame Kathleen Ollerenshaw lecture at the School of Mathematics, University of Manchester\n\nBooks\n with J H van Lint: Graph Theory, Coding Theory and Block Designs (1975)\n Parallelisms of Complete Designs (1976)\n Oligomorphic Permutation Groups (1990)\n with J H van Lint: Designs, Graphs, Codes and their Links (1991)\n Combinatorics: Topics, Techniques, Algorithms (1994)\n Sets, Logic and Categories (1999)\n Permutation Groups (1999)\n Introduction to Algebra (first edition) (1998)\n Introduction to Algebra (second edition) (2008)\n\nNotes\n\nReferences\nShort biography\n\nExternal links\nHome page at Queen Mary University of London\nHome page at University of St Andrews\nPeter Cameron's 60th birthday conference\nTheorems by Peter Cameron at Theorem of the Day\nPeter Cameron's blog\n\nCategory:Academics of Queen Mary University of London\nCategory:1947 births\nCategory:Living people\nCategory:Australian Rhodes Scholars\nCategory:Algebraists\nCategory:Coding theorists\nCategory:Combinatorialists\nCategory:Alumni of Balliol College, Oxford\nCategory:University of Queensland alumni\nCategory:Whitehead Prize winners\nCategory:Model theorists\nCategory:20th-century Australian mathematicians\nCategory:21st-century Australian mathematicians"
    },
    {
      "title": "Bill Casselman (mathematician)",
      "url": "https://en.wikipedia.org/wiki/Bill_Casselman_%28mathematician%29",
      "text": "William Allen \"Bill\" Casselman (born November 27, 1941) is an American Canadian mathematician who works in group theory. He is closely connected to the Langlands program and has been involved in posting all of the work of Robert Langlands on the internet.Institute for Advanced Study: The Work of Robert Langlands\n\nWork\nCasselman did his undergraduate work at Harvard College where his advisor was Raoul Bott and received his Ph.D from Princeton University in 1966 where his advisor was Goro Shimura.  He was a visiting scholar at the Institute for Advanced Study in 1974, 1983, and 2001.Institute for Advanced Study: A Community of Scholars: Casselman, William He emigrated to Canada in 1971 and is now Professor Emeritus in \nmathematics  at the University of British Columbia in Vancouver, British Columbia.UBC Faculty & Administrative Directory: Dr William Casselman\n\nCasselman specializes in representation theory, automorphic forms, geometric combinatorics, and the structure of algebraic groups. He has a special interest in Mathematical graphicsMathematical Illustrations: A Manual of Geometry and PostScript reviewed by Denis Roegel in Notices of the AMS and has been the graphics editor of the Notices of the American Mathematical Society since January, 2001.Notices of the American Mathematical Society: Editors and Staff\n\nIn 2012, he became one of the inaugural fellows of the American Mathematical Society..\n\nSelected publications\n On some results of Atkin and Lehner, Mathematische Annalen 201 (1973), 301-314\n Mathematical Illustrations: A Manual of Geometry and PostScript, by Bill Casselman, Cambridge University Press, 2005, \n\nReferences\n\nExternal links\n Publications of Bill Casselman\n Bill Casselman's Home Page  \n William Allen Casselman at the Mathematics Genealogy Project\n\nCategory:1941 births\nCategory:Group theorists\nCategory:Algebraists\nCategory:20th-century Canadian mathematicians\nCategory:21st-century Canadian mathematicians\nCategory:University of British Columbia faculty\nCategory:Institute for Advanced Study visiting scholars\nCategory:Harvard University alumni\nCategory:Princeton University alumni\nCategory:Living people\nCategory:Fellows of the American Mathematical Society\nCategory:People from Glen Ridge, New Jersey"
    },
    {
      "title": "Ivan Cherednik",
      "url": "https://en.wikipedia.org/wiki/Ivan_Cherednik",
      "text": "Ivan Cherednik (Иван Владимирович Чередник) is a Russian mathematician.   He introduced double affine Hecke algebras, and used them to prove Macdonald's constant term conjecture in . He has also dealt with algebraic geometry, number theory and Soliton equations. His research interests include representation theory, mathematical physics, and algebraic combinatorics. He is currently a professor of mathematics at the University of North Carolina at Chapel Hill, specializing in combinatorics.\n\nPublications\n\nReferences\n\nUniversity of North Carolina page about Ivan Cherednik\nCherednik on Math-Net.Ru\n\nCategory:Living people\nCategory:Russian mathematicians\nCategory:American mathematicians\nCategory:University of North Carolina at Chapel Hill faculty\nCategory:Algebraists\nCategory:1953 births\nCategory:Guggenheim Fellows"
    },
    {
      "title": "Mitrofan Cioban",
      "url": "https://en.wikipedia.org/wiki/Mitrofan_Cioban",
      "text": "Mitrofan Cioban (born 5 January 1942, Copceac, Ștefan Vodă) is a Moldovan mathematician, a member of the Academy of Sciences of Moldova (2000).\n\nReferences\n\nCategory:Algebraists\nCategory:Topologists\nCategory:Moldovan mathematicians\nCategory:Titular members of the Academy of Sciences of Moldova\nCategory:1942 births\nCategory:Living people\n\nCategory:Recipients of the Order of Honour (Moldova)"
    },
    {
      "title": "William Kingdon Clifford",
      "url": "https://en.wikipedia.org/wiki/William_Kingdon_Clifford",
      "text": "William Kingdon Clifford  (4 May 1845 – 3 March 1879) was an English mathematician and philosopher. Building on the work of Hermann Grassmann, he introduced what is now termed geometric algebra, a special case of the Clifford algebra named in his honour. The operations of geometric algebra have the effect of mirroring, rotating, translating, and mapping the geometric objects that are being modelled to new positions. Clifford algebras in general and geometric algebra in particular have been of ever increasing importance to mathematical physics, geometry, and computing. Clifford was the first to suggest that gravitation might be a manifestation of an underlying geometry. In his philosophical writings he coined the expression \"mind-stuff\".\n\nBiography\nBorn at Exeter, William Clifford showed great promise at school. He went on to King's College London (at age 15) and Trinity College, Cambridge, where he was elected fellow in 1868, after being second wrangler in 1867 and second Smith's prizeman.  Being second was a fate he shared with others who became famous mathematicians, including William Thomson (Lord Kelvin) and James Clerk Maxwell. In 1870, he was part of an expedition to Italy to observe the solar eclipse of December 22, 1870. During that voyage he survived a shipwreck along the Sicilian coast.\n\nIn 1871, he was appointed professor of mathematics and mechanics at University College London, and in 1874 became a fellow of the Royal Society.  He was also a member of the London Mathematical Society and the Metaphysical Society.\n\nOn 7 April 1875 Clifford married Lucy Lane. In 1876, Clifford suffered a breakdown, probably brought on by overwork. He taught and administered by day, and wrote by night. A half-year holiday in Algeria and Spain allowed him to resume his duties for 18 months, after which he collapsed again. He went to the island of Madeira to recover, but died there of tuberculosis after a few months, leaving a widow with two children.\n\nClifford enjoyed entertaining children and wrote a collection of fairy stories, The Little People.\n\nClifford and his wife are buried in London's Highgate Cemetery just north of the grave of Karl Marx, and near the graves of George Eliot and Herbert Spencer.\n\n Mathematician \n\"Clifford was above all and before all a geometer.\" (H. J. S. Smith). \nThe discovery of non-Euclidean geometry opened new possibilities in geometry in Clifford's era. The field of intrinsic differential geometry was born, with the concept of curvature broadly applied to space itself as well as to curved lines and surfaces. Clifford was very much impressed by Bernhard Riemann’s 1854 essay \"On the hypotheses which lie at the bases of geometry\".Bernhard Riemann (1854, 1867) On the hypotheses which lie at the bases of geometry, Habilitationsschrift and posthumous publication, translated by Clifford, link from School of Mathematics, Trinity College Dublin In 1870 he reported to the Cambridge Philosophical Society on the curved space concepts of Riemann, and included speculation on the bending of space by gravity. Clifford's translationW. K. Clifford (1873) \"On the hypotheses which lie at the bases of geometry\", Nature 8:14 to 17, 36, 37; also Paper #9 in Mathematical Papers (1882), page 55, synopsis pp 70,1 of Riemann's paper was published in Nature in 1873. His report at Cambridge, On the Space-Theory of Matter, was published in 1876, anticipating Albert Einstein’s general relativity by 40 years. Clifford elaborated elliptic space geometry as a non-Euclidean metric space. Equidistant curves in elliptic space are now said to be Clifford parallels.\nright|thumb|Clifford by John Collier\n\nClifford's contemporaries considered him acute and original, witty and warm. He often worked late into the night, which may have hastened his death. He published papers on a range of topics including algebraic forms and projective geometry and the textbook Elements of Dynamic. His application of graph theory to invariant theory was followed up by William Spottiswoode and Alfred Kempe.\n\nAlgebras\nIn 1878 Clifford published a seminal work, building on Grassmann's extensive algebra. He had succeeded in unifying the quaternions, developed by William Rowan Hamilton, with Grassmann's outer product (also known as the exterior product). He understood the geometric nature of Grassmann's creation, and that the quaternions fit cleanly into the algebra Grassmann had developed. The versors in quaternions facilitate representation of rotation. Clifford laid the foundation for a geometric product, composed of the sum of the inner product and Grassmann's outer product. The geometric product was eventually formalized by the Hungarian mathematician Marcel Riesz. The inner product equips geometric algebra with a metric, fully incorporating distance and angle relationships for lines, planes, and volumes, while the outer product gives those planes and volumes vector-like properties, including a directional bias.\n\nCombining the two brought the operation of division into play. This greatly expanded our qualitative understanding of how objects interact in space. Crucially, it also provided the means for quantitatively calculating the spatial consequences of those interactions. The resulting geometric algebra, as he called it, eventually realized the long sought goalGottfried Leibniz, letter to Christian Huygens (8 September 1679) \"I believe that, so far as geometry is concerned, we need still another analysis which is distinctly geometrical or linear and which will express situation directly as algebra expresses magnitude directly.\", in Gottfried Leibniz (2nd edition 1976) Philosophical Papers and Letters, Springer of creating an algebra that mirrors the movements and projections of objects in 3-dimensional space.\n\nMoreover, Clifford's algebraic schema extends to higher dimensions. The algebraic operations have the same symbolic form as they do in 2 or 3-dimensions. The importance of general Clifford algebras has grown over time, while their isomorphism classes - as real algebras - have been identified in other mathematical systems beyond simply the quaternions.\n\nThe realms of real analysis and complex analysis have been expanded through the algebra H of quaternions, thanks to its notion of a three-dimensional sphere embedded in a four-dimensional space. Quaternion versors, which inhabit this 3-sphere, provide a representation of the rotation group SO(3). Clifford noted that Hamilton’s biquaternions were a tensor product  of known algebras, and proposed instead two other tensor products of H: Clifford argued that the \"scalars\" taken from the complex numbers C might instead be taken from split-complex numbers D or from the dual numbers N. In terms of tensor products,  produces split-biquaternions, while  forms dual quaternions. The algebra of dual quaternions is used to express screw displacement, a common mapping in kinematics.\n\nPhilosopher\n\ndesk|right|thumb|William Kingdon Clifford\n\nAs a philosopher, Clifford's name is chiefly associated with two phrases of his coining, \"mind-stuff\" and the \"tribal self\". The former symbolizes his metaphysical conception, suggested to him by his reading of Spinoza. Sir Frederick Pollock wrote about Clifford as follows:\n\nClifford himself defined \"mind-stuff\" as follows (1878, \"On the Nature of Things-in-Themselves\", Mind, Vol. 3, No. 9, pp. 57–67):\n\nThat element of which, as we have seen, even the simplest feeling is a complex, I shall call Mind-stuff. A moving molecule of inorganic matter does not possess mind or consciousness ; but it possesses a small piece of mind-stuff. When molecules are so combined together as to form the film on the under side of a jelly-fish, the elements of mind-stuff which go along with them are so combined as to form the faint beginnings of Sentience. When the molecules are so combined as to form the brain and nervous system of a vertebrate, the corresponding elements of mind-stuff are so combined as to form some kind of consciousness; that is to say, changes in the complex which take place at the same time get so linked together that the repetition of one implies the repetition of the other. When matter takes the complex form of a living human brain, the corresponding mind-stuff takes the form of a human consciousness, having intelligence and volition.\n\nThe other phrase, \"tribal self\", gives the key to Clifford's ethical view, which explains conscience and the moral law by the development in each individual of a \"self\", which prescribes the conduct conducive to the welfare of the \"tribe.\" Much of Clifford's contemporary prominence was due to his attitude toward religion. Animated by an intense love of his conception of truth and devotion to public duty, he waged war on such ecclesiastical systems as seemed to him to favour obscurantism, and to put the claims of sect above those of human society. The alarm was greater, as theology was still unreconciled with Darwinism; and Clifford was regarded as a dangerous champion of the antispiritual tendencies then imputed to modern science.  There has also been debate on the extent to which Clifford’s doctrine of \"concomitance\" or \"psychophysical parallelism\" influenced John Hughlings Jackson's model of the nervous system and through him the work of Janet, Freud, Ribot, and Ey.\n\nEthics\n\nIn his essay, “The Ethics of Belief” published in 1877, Clifford argued that it was immoral to believe things for which one lacks evidence. He describes a ship-owner who planned to send to sea an old and not well built ship full of passengers. The ship-owner had doubts suggested to him that the ship might not be seaworthy. “These doubts preyed upon his mind, and made him unhappy.” He considered having the ship refitted even though it would be expensive. At last, “He succeeded in overcoming these melancholy reflections.” He watched the ship depart, “with a light heart… and he got his insurance money when she went down in mid-ocean and told no tales.” Clifford, William, Kingdon, essay “The Ethics of Belief.” published in 1877.\n\nClifford argued that the ship-owner was guilty of the deaths of the passengers even though he sincerely believed the ship was sound. “[H]e had no right to believe on such evidence as was before him.” (The italics are in the original.) Clifford famously concludes, “it is wrong always, everywhere, and for anyone, to believe anything upon insufficient evidence.\" Clifford, William, Kingdon, essay “The Ethics of Belief.” published in 1877. \n\nAs such, he was arguing in direct opposition to religious thinkers for whom \"blind faith\" (i.e. belief in things in spite of the lack of evidence for them) was a virtue. This paper was famously attacked by pragmatist philosopher William James in his \"Will to Believe\" lecture. Often these two works are read and published together as touchstones for the debate over evidentialism, faith, and overbelief.\n\nPremonition of relativity\nThough Clifford never constructed a full theory of spacetime and relativity, there are some remarkable observations he made in print that foreshadowed these modern concepts:\nIn his book Elements of Dynamic (1878), he introduced \"quasi-harmonic motion in a hyperbola\". He wrote an expression for a parametrized unit hyperbola, which other authors later used as a model for relativistic velocity. Elsewhere he states,\nThe geometry of rotors and motors ... forms the basis of the whole modern theory of the relative rest (Static) and the relative motion (Kinematic and Kinetic) of invariable systems.Common Sense of the Exact Sciences (1885), page 214 (page 193 of the Dover reprint), immediately followed by a section on \"The bending of space\". However, according to the preface (p.vii) this section was written by Karl Pearson\nThis passage makes reference to biquaternions, though Clifford made these into split-biquaternions as his independent development.\nThe book continues with a chapter \"On the bending of space\", the substance of general relativity. Clifford also discussed his views in On the Space-Theory of Matter in 1876.\n\nIn 1910 William Barrett Frankland quoted the Space-Theory of Matter in his book on parallelism.William Barrett Frankland (1910) Theories of Parallelism, pp 48,9, Cambridge University Press He wrote:\nThe boldness of this speculation is surely unexcelled in the history of thought. Up to the present, however, it presents the appearance of an Icarian flight.\n\nYears later, after general relativity had been advanced by Albert Einstein, various authors noted that Clifford had anticipated Einstein:\n\nIn 1923 Hermann Weyl mentioned CliffordRaum Zeit Materie, page 101, Springer-Verlag, Berlin as one of those who, like Bernhard Riemann, anticipated the geometric ideas of relativity.\n\nIn 1940 Eric Temple Bell published his The Development of Mathematics. There on pages 359 and 360 he discusses the prescience of Clifford on relativity:\nBolder even than Riemann, Clifford confessed his belief (1870) that matter is only a manifestation of curvature in a space-time manifold. This embryonic divination has been acclaimed as an anticipation of Einstein’s (1915–16) relativistic theory of the gravitational field. The actual theory, however, bears but slight resemblance to Clifford’s rather detailed creed. As a rule, those mathematical prophets who never descend to particulars make the top scores. Almost anyone can hit the side of a barn at forty yards with a charge of buckshot.\n\nAlso in 1960, at Stanford University for the International Congress for Logic, Methodology, and Philosophy of Science, John Archibald Wheeler introduced his geometrodynamics formulation of general relativity by crediting Clifford as the initiator.J. Wheeler (1960) \"Curved empty space as the building material of the physical world: an assessment\", in Ernest Nagel (1962) Logic, Methodology, and Philosophy of Science, Stanford University Press\n\nIn his The Natural Philosophy of Time (1961, 1980) Gerald James Whitrow recalls Clifford's prescience by quoting him to describe the Friedmann–Lemaître–Robertson–Walker metric in cosmology (1st ed pp 246,7; 2nd ed p 291).\n\nIn 1970 Cornelius Lanczos summarizes Clifford's premonitions this way:\n[He] with great ingenuity foresaw in a qualitative fashion that physical matter might be conceived as a curved ripple on a generally flat plane. Many of his ingenious hunches were later realized in Einstein's gravitational theory. Such speculations were automatically premature and could not lead to anything constructive without an intermediate link which demanded the extension of 3-dimensional geometry to the inclusion of time. The theory of curved spaces had to be preceded by the realization that space and time form a single four-dimensional entity.Cornelius Lanczos (1970) Space through the Ages: The evolution of geometrical ideas from Pythagoras to Hilbert and Einstein, page 222, Academic Press\n\nIn 1973 Banesh Hoffmann wrote:\nRiemann, and more specifically Clifford, conjectured that forces and matter might be local irregularities in the curvature of space, and in this they were strikingly prophetic, though for their pains they were dismissed at the time as visionaries.Banesh Hoffmann (1973) \"Relativity\" in Dictionary of the History of Ideas 4:80, Charles Scribner's Sons\n\nIn 1990 Ruth Farwell and Christopher Knee examined the record on acknowledgement of Clifford's foresight. They conclude \"it was Clifford, not Riemann, who anticipated some of the conceptual ideas of General Relativity\". To explain the lack of recognition of Clifford's prescience, they point out that he was an expert in metric geometry, and \"metric geometry was too challenging to orthodox epistemology to be pursued.\" Farwell & Knee (1990)Studies in History and Philosophy of Science 21:91–121\nIn 1992 Farwell and Knee continued their study with \"The Geometric Challenge of Riemann and Clifford\"Farwell & Knee (1992) in 1830–1930: A Century of Geometry, pages 98 to 106, Lecture Notes in Physics #402, Springer-Verlag \nThey \"hold that once tensors had been used in the theory of general relativity, the framework existed in which a geometrical perspective in physics could be developed and allowed the challenging geometrical conceptions of Riemann and Clifford to be rediscovered.\"\n\n Selected writings \n 1872: On the aims and instruments of scientific thought, 524-41.\n 1876: On the Space-Theory of Matter.\n 1877: \"The Ethics of Belief\", Contemporary Review.\n 1878:  Elements of Dynamic, books I, II, III (1878) London: MacMillan & Co; on-line presentation by Cornell University Historical Mathematical Monographs.\n 1878: \"Applications of Grassmann's Extensive Algebra\", American Journal of Mathematics 1(4): 353 \n 1879: Seeing and Thinking, popular science lectures. \n 1879: Lectures and Essays, with an introduction by Sir Frederick Pollock.\n 1881: Mathematical fragments, being facsimiles of his unfinished papers relating to the Theory of Graphs, Macmillan Publishers via University of Bordeaux \n 1882: (edited by Robert Tucker, with an introduction by Henry J. S. Smith.) Mathematical Papers via Internet Archive \n 1885: The Common Sense of the Exact Sciences. Completed by Karl Pearson. \n 1887: Elements of Dynamic, vol. 2, in Ewald, William B., ed., 1996. From Kant to Hilbert: A Source Book in the Foundations of Mathematics, 2 vols. Oxford University Press.\n\nLegacy\nThe academic journal Advances in Applied Clifford Algebras publishes on Clifford’s legacy in kinematics and abstract algebra.\n\nQuotations\nthumb|Marker for W. K. Clifford and his wife in Highgate Cemetery (c. 1986)\n\"I ... hold that in the physical world nothing else takes place but this variation [of the curvature of space].\" — Mathematical Papers (1882).\n\"There is no scientific discoverer, no poet, no painter, no musician, who will not tell you that he found ready made his discovery or poem or picture — that it came to him from outside, and that he did not consciously create it from within.\" (From an 1868 lecture to the Royal Institution titled \"Some of the conditions of mental development\")\n\"It is wrong always, everywhere, and for anyone, to believe anything upon insufficient evidence.\" — The Ethics of Belief (1879 [1877])\n\"I was not, and was conceived. I loved and did a little work. I am not and grieve not.\" — Epitaph.\n\"If a man, holding a belief which he was taught in childhood or persuaded of afterwards, keeps down and pushes away any doubts which arise about it in his mind, purposely avoids the reading of books and the company of men that call in question or discuss it, and regards as impious those questions which cannot easily be asked without disturbing it — the life of that man is one long sin against mankind.\" — Contemporary Review (1877)\n\n See also \n Clifford–Klein form\n Will to Believe Doctrine\n\nNotes\n\n References \n\n Further reading \n\n  (The on-line version lacks the article's photographs.)\n \n \n  (See especially pages 78 – 91)\nMadigan, Timothy J. (2010). W.K. Clifford and \"The Ethics of Belief Cambridge Scholars Press, Cambridge, UK 978-1847-18503-7.\n  (See especially Chapter 11)\n \n \n\n External links \n\n \n\n William and Lucy Clifford (with pictures)\n\n \n \n \n Clifford, William Kingdon, William James, and A.J. Burger (Ed.), The Ethics of Belief.\n \n Joe Rooney William Kingdon Clifford, Department of Design and Innovation, the Open University, London.\n\nCategory:1845 births\nCategory:1879 deaths\nCategory:19th-century deaths from tuberculosis\nCategory:19th-century British philosophers\nCategory:19th-century English mathematicians\nCategory:English atheists\nCategory:Algebraists\nCategory:Relativity theorists\nCategory:Alumni of Trinity College, Cambridge\nCategory:Fellows of Trinity College, Cambridge\nCategory:Alumni of King's College London\nCategory:Academics of University College London\nCategory:Fellows of the Royal Society\nCategory:Burials at Highgate Cemetery\nCategory:Second Wranglers\nCategory:Panpsychism\nCategory:People from Exeter"
    },
    {
      "title": "Paul Cohn",
      "url": "https://en.wikipedia.org/wiki/Paul_Cohn",
      "text": "Not to be confused with Paul Cohen.\n\nPaul Moritz Cohn FRS (8 January 1924 – 20 April 2006) was Astor Professor of Mathematics at University College London, 1986-9, and author of many textbooks on algebra. His work was mostly in the area of algebra, especially non-commutative rings.Independent\n\n Ancestry and early life \nHe was the only child of Jewish parents, James (or Jakob) Cohn, owner of an import business, and Julia (née Cohen), a schoolteacher.Autobiography\n\nBoth of his parents were born in Hamburg, as were three of his grandparents. His ancestors came from various parts of Germany. His father fought in the German army in World War I; he was wounded several times and awarded the Iron Cross. A street in Hamburg is named in memory of his mother.De Morgan\n\nWhen he was born, his parents were living with his mother's mother in Isestraße. After her death in October 1925, the family moved to a rented flat in a new building in Lattenkamp, in the Winterhude quarter. He attended a kindergarten then, in April 1930, moved to Alsterdorfer Straße School. After a while, he had a new teacher, a National Socialist, who picked on him and punished him without cause. Thus in 1931, he moved to the Meerweinstraße School where his mother taught.\n\nFollowing the rise of the Nazis in 1933, his father's business was confiscated and his mother dismissed. He moved to the Talmud-Tora-Schule, a Jewish school. In mid-1937, the family moved to Klosterallee. This was nearer the school, the synagogue and other pupils, being in the Jewish area. His German teacher was Dr. Ernst Loewenberg, the son of the poet Jakob Loewenberg.\n\nOn the night of 9/10 November 1938 (Kristallnacht), his father was arrested and sent to Sachsenhausen concentration camp. He was released after four months but told to emigrate. Cohn went to Britain in May 1939 on the Kindertransport to work on a chicken farm, and never saw his parents again. He corresponded regularly with them until late 1941. At the end of the War, he learned that they were deported to Riga on 6 December 1941 and never returned. At the end of 1941, the farm closed. He trained as a precision engineer, acquired a work permit and worked in a factory for 4½ years. He passed the Cambridge Scholarship Examination, and won an exhibition to Trinity College, Cambridge.\n\n Career \nHe received a B.A in Mathematics from Cambridge University in 1948 and a Ph.D. (supervised by Philip Hall) in 1951. He then spent a year as a Chargé de Recherches at the University of Nancy. On his return, he became a lecturer in mathematics at Manchester University. He was a visiting professor at Yale University in 1961–62, and for part of 1962 was at the University of California at Berkeley. On his return, he became Reader at Queen Mary College. He was a visiting professor at the University of Chicago in 1964 and at the State University of New York at Stony Brook in 1967. By then, he was regarded as one of the world's leading algebraists.\n\nAlso in 1967, he became head of the Department of Mathematics at Bedford College. He held several visiting professorships, in America, Paris, Delhi, Canada, Haifa and Bielefeld. He was awarded the Lester R Ford Award from the Mathematical Association of America in 1972 and the Senior Berwick Prize of the London Mathematical Society in 1974.\n\nIn the early 1980s, funding cuts caused the closure of the small colleges of the University of London. Cohn moved to University College in 1984,Times together with the two other experts at Bedford on ring theory, Bill Stephenson and Warren Dicks. He became Astor Professor of Mathematics there in 1986. He continued to be a visiting professor, for example to the University of Alberta in 1986 and to Bar Ilan University in 1987. He retired in 1989, but remained active as Professor Emeritus and Honorary Research Fellow until his death.\n\nHe was President of the London Mathematical Society, 1982-4, having been its secretary, 1965–67 and a Council member in 1968–71, 1972–75 and 1979–82. He was editor of the Society's Monographs in 1968–77 and 1980–93. He was elected a Fellow of the Royal Society in 1980 and was on its council, 1985–87. He was a member of the Mathematical Committee of the Science Research Council, 1977–1980. He chaired the National Committee for Mathematics, 1988-9.\n\n Mathematical work \nIn all, Cohn wrote nearly 200 mathematical papers. He worked in many areas of algebra, mainly in non-commutative ring theory. His first papers, covering many topics, were published in 1952. He generalised a theorem due to Wilhelm Magnus, and worked on the structure of tensor spaces. In 1953 he published a joint paper with Kurt Mahler on pseudo-valuations and in 1954 he published a work on Lie algebras.\n\nPapers over the next few years covered areas such as group theory, field theory, Lie rings, semigroups, Abelian groups and ring theory. He published his first book, Lie groups, in 1957. After that, he moved into the areas of Jordan algebras, Lie division rings, skew fields, free ideal rings and non-commutative unique factorisation domains. He published his second book, Linear equations, in 1958 and his third, Solid geometry, in 1961. Universal algebra appeared in 1965 (second edition 1981). After that, he concentrated on non-commutative ring theory and the theory of algebras.\n\nHis monograph Free rings and their relations appeared in 1971. It covered the work of Cohn and others on free associative algebras and related classes of rings, especially free ideal rings. He included all of his own published results on the embedding of rings into skew fields. The second, enlarged edition appeared in 1985.\n\nCohn also wrote undergraduate textbooks. Algebra volume I appeared in 1974 and volume II in 1977. The second edition, in three volumes, was published by Wiley between 1982 and 1991. These volumes were in line with the British (rather than American) curricula at the time and include both linear algebra and abstract algebra. Cohn wrote a subsequent revised iteration the first volume as Classical Algebra (Wiley, 2000) as a more \"user friendly\" version for undergraduates (according to its preface); this book also includes a few selected topics from volumes II and III of Algebra. The final incarnation of Cohn's algebra textbooks appeared in 2003 as two Springer volumes Basic Algebra and Further Algebra and Applications; the material in Basic Algebra is (according to its preface) rather more concise and while corresponding roughly with volume I of Algebra assumes knowledge of linear algebra; the material on basic theories (groups, rings, fields) is pursued in more depth in Basic Algebra compared to volume I of Algebra. Further Algebra and Applications roughly corresponds to volumes II and III of Algebra, but reflects the shift of some material from these volumes to Basic Algebra.\n\n Private life \nHis recreation was etymology and language in all its forms. He married Deirdre Sharon in 1958, and they had two daughters.\n\n Publications \n Lie Groups (1957)\n \n \n \n Universal Algebra (1965, 1981)\n Free Rings and Their Relations (1971, 1985)\n Algebra I (1974, 1982)\n Algebra II (1977, 1989)\n Skew Field Constructions (1977)\n Algebra III (1990)\n Algebraic Numbers and Algebraic Functions (1991)\n Elements of Linear Algebra (1994)\n Skew Fields, Theory of General Division Rings (in Encyclopedia of Mathematics and its Applications, vol 57, 1995)\n \n Introduction to Ring Theory (2000)\n Classic Algebra (2000)\n Basic Algebra (2002)\n Further Algebra and Applications (2003)\n Oxford Dictionary of National Biography (contributor, 2004)\n Free Ideal Rings and Localization in General Rings (2006)\n\n References \n\nBibliography \n\nCategory:1924 births\nCategory:2006 deaths\nCategory:Algebraists\nCategory:British Jews\nCategory:20th-century British mathematicians\nCategory:21st-century British mathematicians\nCategory:British academics\nCategory:German Jews\nCategory:German mathematicians\nCategory:Jewish scientists\nCategory:Kindertransport refugees\nCategory:Alumni of Trinity College, Cambridge\nCategory:Academics of University College London\nCategory:People from Hamburg\nCategory:Academics of Queen Mary University of London\nCategory:Fellows of the Royal Society\nCategory:British people of German-Jewish descent"
    },
    {
      "title": "John Horton Conway",
      "url": "https://en.wikipedia.org/wiki/John_Horton_Conway",
      "text": "John Horton Conway The Royal Society: John Conway Biography (born 26 December 1937) is an English mathematician active in the theory of finite groups, knot theory, number theory, combinatorial game theory and coding theory. He has also contributed to many branches of recreational mathematics, notably the invention of the cellular automaton called the Game of Life. Conway spent the first half of his long career at the University of Cambridge, in England, and the second half at Princeton University in New Jersey, where he now holds the title Professor Emeritus.MacTutor History of Mathematics archive: John Horton Conway\n\nEducation and early life\nConway was born in Liverpool, the son of Cyril Horton Conway and Agnes Boyce. He became interested in mathematics at a very early age; his mother has recalled that he could recite the powers of two when he was four years old. By the age of eleven his ambition was to become a mathematician.\n\nAfter leaving sixth form, Conway entered Gonville and Caius College, Cambridge to study mathematics. Conway, who was a \"terribly introverted adolescent\" in school, interpreted his admission to Cambridge as an opportunity to transform himself into a new person: an \"extrovert\".\n\nHe was awarded his Bachelor of Arts degree in 1959 and began to undertake research in number theory supervised by Harold Davenport. Having solved the open problem posed by Davenport on writing numbers as the sums of fifth powers, Conway began to become interested in infinite ordinals. It appears that his interest in games began during his years studying the Cambridge Mathematical Tripos, where he became an avid backgammon player, spending hours playing the game in the common room. He was awarded his doctorate in 1964 and was appointed as College Fellow and Lecturer in Mathematics at the University of Cambridge.\n\nAfter leaving Cambridge in 1986, he took up the appointment to the John von Neumann Chair of Mathematics at Princeton University.\n\nConway's Game of Life\n\nthumb|right|A single Gosper's Glider Gun creating \"gliders\" in Conway's Game of Life\nConway is especially known for the invention of the Game of Life, one of the early examples of a cellular automaton. His initial experiments in that field were done with pen and paper, long before personal computers existed.\n\nSince the game was introduced by Martin Gardner in Scientific American in 1970, it has spawned hundreds of computer programs, web sites, and articles.DMOZ: Conway's Game of Life: Sites It is a staple of recreational mathematics. There is an extensive wiki devoted to curating and cataloging the various aspects of the game.LifeWiki From the earliest days it has been a favorite in computer labs, both for its theoretical interest and as a practical exercise in programming and data display. At times Conway has said he hates the Game of Life–largely because it has come to overshadow some of the other deeper and more important things he has done.Does John Conway hate his Game of Life? (video) Nevertheless, the game did help launch a new branch of mathematics, the field of cellular automata.MacTutor History: The game made Conway instantly famous, but it also opened up a whole new field of mathematical research, the field of cellular automata.\n\nThe Game of Life is now known to be Turing complete.Rendell (2015)Case (2014)\n\nConway and Martin Gardner\nConway's career is intertwined with mathematics popularizer and Scientific American columnist Martin Gardner. When Gardner featured Conway's Game of Life in his Mathematical Games column in October 1970, it became the most widely read of all his columns and made Conway an instant celebrity.Martin Gardner, puzzle master extraordinaire by Colm Mulcahy, BBC News Magazine, 21 October 2014: \"The Game of Life appeared in Scientific American in 1970, and was by far the most successful of Gardner's columns, in terms of reader response.\"The Top 10 Martin Gardner Scientific American Articles Gardner and Conway had first corresponded in the late 1950s, and over the years Gardner had frequently written about recreational aspects of Conway's work.The Math Factor Podcast Website John H. Conway reminisces on his long friendship and collaboration with Martin Gardner. For instance, he discussed Conway's game of Sprouts (Jul 1967), Hackenbush (Jan 1972), and his angel and devil problem (Feb 1974). In the September 1976 column he reviewed Conway's book On Numbers and Games and introduced the public to Conway's surreal numbers.Martin Gardner, Penrose Tiles to Trapdoor Ciphers, W. H. Freeman & Co., 1989, , Chapter 4. A non-technical overview; reprint of the 1976 Scientific American article. Conferences called Gathering 4 Gardner are held every two years to celebrate the legacy of Martin Gardner, and Conway himself has often been a featured speaker at these events, discussing various aspects of recreational mathematics.Presentation Videos  from 2014 Gathering 4 GardnerBellos, Alex (2008). The science of fun The Guardian, 30 May 2008\n\nMajor areas of research\n\nCombinatorial game theory\nConway is widely known for his contributions to combinatorial game theory (CGT), a theory of partisan games. This he developed with Elwyn Berlekamp and Richard Guy, and with them also co-authored the book Winning Ways for your Mathematical Plays. He also wrote the book On Numbers and Games (ONAG) which lays out the mathematical foundations of CGT.\n\nHe is also one of the inventors of sprouts, as well as philosopher's football. He developed detailed analyses of many other games and puzzles, such as the Soma cube, peg solitaire, and Conway's soldiers. He came up with the angel problem, which was solved in 2006.\n\nHe invented a new system of numbers, the surreal numbers, which are closely related to certain games and have been the subject of a mathematical novel by Donald Knuth.Infinity Plus One, and Other Surreal Numbers by Polly Shulman, Discover Magazine, 1 December 1995 He also invented a nomenclature for exceedingly large numbers, the Conway chained arrow notation. Much of this is discussed in the 0th part of ONAG.\n\nGeometry\nIn the mid-1960s with Michael Guy, son of Richard Guy, Conway established that there are sixty-four convex uniform polychora excluding two infinite sets of prismatic forms. They discovered the grand antiprism in the process, the only non-Wythoffian uniform polychoron. Conway has also suggested a system of notation dedicated to describing polyhedra called Conway polyhedron notation.\n\nIn the theory of tessellations, he devised the Conway criterion which describes rules for deciding if a prototile will tile the plane.\n\nHe investigated lattices in higher dimensions, and was the first to determine the symmetry group of the Leech lattice.\n\nGeometric topology\nIn knot theory, Conway formulated a new variation of the Alexander polynomial and produced a new invariant now called the Conway polynomial.Conway Polynomial Wolfram MathWorld After lying dormant for more than a decade, this concept became central to work in the 1980s on the novel knot polynomials.Livingston, Charles, Knot Theory (MAA Textbooks), 1993,  Conway further developed tangle theory and invented a system of notation for tabulating knots, nowadays known as Conway notation, while correcting a number of errors in the 19th century knot tables and extending them to include all but four of the non-alternating primes with 11 crossings. See Topology Proceedings 7 (1982) 118.\n\nGroup theory\nHe was the primary author of the ATLAS of Finite Groups giving properties of many finite simple groups. Working with his colleagues Robert Curtis and Simon P. Norton he constructed the first concrete representations of some of the sporadic groups. More specifically, he discovered three sporadic groups based on the symmetry of the Leech lattice, which have been designated the Conway groups.Harris (2015) This work made him a key player in the successful classification of the finite simple groups.\n\nBased on a 1978 observation by mathematician John McKay, Conway and Norton formulated the complex of conjectures known as monstrous moonshine. This subject, named by Conway, relates the monster group with elliptic modular functions, thus bridging two previously distinct areas of mathematics–finite groups and complex function theory. Monstrous moonshine theory has now been revealed to also have deep connections to string theory.Monstrous Moonshine conjecture David Darling: Encyclopedia of Science\n\nConway introduced the Mathieu groupoid, an extension of the Mathieu group M12 to 13 points.\n\nNumber theory\nAs a graduate student, he proved one case of a conjecture by Edward Waring, that in which every integer could be written as the sum of 37 numbers, each raised to the fifth power, though Chen Jingrun solved the problem independently before Conway's work could be published.Breakfast with John Horton Conway\n\nAlgebra\nConway has written textbooks and done original work in algebra, focusing particularly on quaternions and octonions.Conway and Smith (2003): \"Conway and Smith's book is a wonderful introduction to the normed division algebras: the real numbers, the complex numbers, the quaternions, and the octonions.\"  Together with Neil Sloane, he invented the icosians.\n\nAnalysis\nHe invented a base 13 function as a counterexample to the converse of the intermediate value theorem: the function takes on every real value in each interval on the real line, so it has a Darboux property but is not continuous.\n\nAlgorithmics\nFor calculating the day of the week, he invented the Doomsday algorithm. The algorithm is simple enough for anyone with basic arithmetic ability to do the calculations mentally. Conway can usually give the correct answer in under two seconds. To improve his speed, he practices his calendrical calculations on his computer, which is programmed to quiz him with random dates every time he logs on. One of his early books was on finite state machines.\n\nTheoretical physics\nIn 2004, Conway and Simon B. Kochen, another Princeton mathematician, proved the free will theorem, a startling version of the 'no hidden variables' principle of quantum mechanics. It states that given certain conditions, if an experimenter can freely decide what quantities to measure in a particular experiment, then elementary particles must be free to choose their spins to make the measurements consistent with physical law. In Conway's provocative wording: \"if experimenters have free will, then so do elementary particles.\"Conway's Proof Of The Free Will Theorem by Jasvir Nagra\n\nAwards and honours\nConway received the Berwick Prize (1971),London Mathematical Society Prizewinners was elected a Fellow of the Royal Society (1981), was the first recipient of the Pólya Prize (LMS) (1987), won the Nemmers Prize in Mathematics (1998) and received the Leroy P. Steele Prize for Mathematical Exposition (2000) of the American Mathematical Society.\nHis nomination, in 1981, reads: \n\nIn 2017 Conway was given honorary membership of the British Mathematical Association.\n\nPublications\n 2008 The symmetries of things (with Heidi Burgiel and Chaim Goodman-Strauss). A. K. Peters, Wellesley, MA, 2008, .\n 1997 The sensual (quadratic) form (with Francis Yein Chei Fung). Mathematical Association of America, Washington, DC, 1997, Series: Carus mathematical monographs, no. 26, .\n 1996 The book of numbers (with Richard K. Guy). Copernicus, New York, 1996, .\n 1995 Minimal-Energy Clusters of Hard Spheres (with Neil Sloane, R. H. Hardin, and Tom Duff). Discrete & Computational Geometry, vol. 14, no. 3, pp. 237–259.\n 1988 Sphere packings, lattices, and groups (with Neil Sloane). Springer-Verlag, New York, Series: Grundlehren der mathematischen Wissenschaften, 290, .\n 1985 Atlas of finite groups (with Robert Turner Curtis, Simon Phillips Norton, Richard A. Parker, and Robert Arnott Wilson). Clarendon Press, New York, Oxford University Press, 1985, .\n 1982 Winning Ways for your Mathematical Plays (with Richard K. Guy and Elwyn Berlekamp). Academic Press, .\n 1979 Monstrous Moonshine (with Simon P. Norton).http://blms.oxfordjournals.org/content/11/3/308 Bulletin of the London Mathematical Society, vol. 11, issue 2, pp. 308–339.\n 1979 On the Distribution of Values of Angles Determined by Coplanar Points (with Paul Erdős, Michael Guy, and H. T. Croft). Journal of the London Mathematical Society, vol. II, series 19, pp. 137–143.\n 1976 On numbers and games. Academic Press, New York, 1976, Series: L.M.S. monographs, 6, .\n 1971 Regular algebra and finite machines. Chapman and Hall, London, 1971, Series: Chapman and Hall mathematics series, .\n\nReferences\n\nSources\n Alpert, Mark (1999). Not Just Fun and Games Scientific American, April 1999\n Conway, John and Smith, Derek A. (2003). On quaternions and Octonions : their Geometry, Arithmetic, and Symmetry Bull. Amer. Math. Soc. 2005, vol=42, issue=2, pp. 229–243, \n Boden, Margaret (2006). Mind As Machine, Oxford University Press, 2006, p. 1271\n Case, James (2014). Martin Gardner’s Mathematical Grapevine Book Reviews of Undiluted Hocus-Pocus: The Autobiography of Martin Gardner and Martin Gardner in the Twenty-First Century, SIAM News, Volume 47, Number 3, April 2014\n Conway, John and Sigur, Steve (2005). The Triangle Book AK Peters, Ltd, 15 June 2005, \n du Sautoy, Marcus (2008). Symmetry, HarperCollins, p. 308\n Guy, Richard K (1983). Conway's Prime Producing Machine Mathematics Magazine, Vol. 56, No. 1 (Jan. 1983), pp. 26–33\n Harris, Michael (2015). Review of Genius At Play: The Curious Mind of John Horton Conway Nature, 23 July 2015\n \n \n \n Princeton University (2009). Bibliography of John H. Conway Mathematics Department\n Rendell, Paul (2015). Turing Machine Universality of the Game of Life Springer, July 2015, \n Seife, Charles (1994). Impressions of Conway The Sciences\n\nExternal links\n\n Photos of John Horton Conway\n \n \n \n \n  Conway leading a tour of brickwork patterns in Princeton, lecturing on the ordinals and on sums of powers and the Bernoulli numbers\n\nCategory:1937 births\nCategory:Living people\nCategory:20th-century British mathematicians\nCategory:21st-century mathematicians\nCategory:Algebraists\nCategory:Group theorists\nCategory:Combinatorial game theorists\nCategory:Cellular automatists\nCategory:Mathematics popularizers\nCategory:Recreational mathematicians\nCategory:English mathematicians\nCategory:Alumni of Gonville and Caius College, Cambridge\nCategory:Fellows of the Royal Society\nCategory:Princeton University faculty\nCategory:Scientists from Liverpool\nCategory:British expatriate academics in the United States\nCategory:English atheists"
    },
    {
      "title": "Richard Dedekind",
      "url": "https://en.wikipedia.org/wiki/Richard_Dedekind",
      "text": "Julius Wilhelm Richard Dedekind (6 October 1831 – 12 February 1916) was a German mathematician who made important contributions to abstract algebra (particularly ring theory),\naxiomatic foundation for the natural numbers, algebraic number theory and the definition of the real numbers.\n\nLife\nDedekind's father was Julius Levin Ulrich Dedekind, an administrator of Collegium Carolinum in Braunschweig. Dedekind had three older siblings. As an adult, he never used the names Julius Wilhelm. He was born, lived most of his life, and died in Braunschweig (often called \"Brunswick\" in English).\n\nHe first attended the Collegium Carolinum in 1848 before transferring to the University of Göttingen in  1850. There, Dedekind was taught number theory by professor Moritz Stern. Gauss was still teaching, although mostly at an elementary level, and Dedekind became his last student. Dedekind received his doctorate in 1852, for a thesis titled Über die Theorie der Eulerschen Integrale (\"On the Theory of Eulerian integrals\"). This thesis did not display the talent evident by Dedekind's subsequent publications.\n\nAt that time, the University of Berlin, not Göttingen, was the main facility for mathematical research in Germany. Thus Dedekind went to Berlin for two years of study, where he and Bernhard Riemann were contemporaries; they were both awarded the habilitation in 1854. Dedekind returned to Göttingen to teach as a Privatdozent, giving courses on probability and geometry. He studied for a while with Peter Gustav Lejeune Dirichlet, and they became good friends. Because of lingering weaknesses in his mathematical knowledge, he studied elliptic and abelian functions. Yet he was also the first at Göttingen to lecture concerning Galois theory. About this time, he became one of the first people to understand the importance of the notion of groups for algebra and arithmetic.\n\nIn 1858, he began teaching at the Polytechnic school in  Zürich (now ETH Zürich). When the Collegium Carolinum was upgraded to a Technische Hochschule (Institute of Technology) in 1862, Dedekind returned to his native Braunschweig, where he spent the rest of his life, teaching at the Institute. He retired in 1894, but did occasional teaching and continued to publish. He never married, instead living with his sister Julia.\n\nDedekind was elected to the Academies of Berlin (1880) and Rome, and to the French Academy of Sciences (1900). He received honorary doctorates from the universities of Oslo, Zurich, and Braunschweig.\n\nWork\nthumb|upright|Dedekind, before 1886\nWhile teaching calculus for the first time at the Polytechnic school, Dedekind developed the notion now known as a Dedekind cut (German: Schnitt), now a standard definition of the real numbers. The idea of a cut is that an irrational number divides the rational numbers into two classes (sets), with all the numbers of one class (greater) being strictly greater than all the numbers of the other (lesser) class. For example, the square root of 2 defines all the nonnegative numbers whose squares are less than 2 and the negative numbers into the lesser class, and the positive numbers whose squares are greater than 2 into the greater class. Every location on the number line continuum contains either a rational or an irrational number. Thus there are no empty locations, gaps, or discontinuities. Dedekind published his thoughts on irrational numbers and Dedekind cuts in his pamphlet \"Stetigkeit und irrationale Zahlen\" (\"Continuity and irrational numbers\");Ewald, William B., ed. (1996) \"Continuity and irrational numbers\", p. 766 in From Kant to Hilbert: A Source Book in the Foundations of Mathematics, 2 vols. Oxford University Press. full text in modern terminology, Vollständigkeit, completeness.\n\nDedekind's theorem states that if there existed a one-to-one correspondence between two sets, then the two sets were \"similar\". He invoked similarity to give the first precise definition of an infinite set: a set is infinite when it is \"similar to a proper part of itself,\" in modern terminology, is equinumerous to one of its proper subsets. Thus the set N of natural numbers can be shown to be similar to the subset of N whose members are the squares of every member of N, (N →  N2):\n\n N    1  2  3  4  5  6  7  8  9  10 ...\n              ↓           \n N2   1  4  9  16 25 36 49 64 81 100 ...\n\nDedekind edited the collected works of Lejeune Dirichlet, Gauss, and Riemann. Dedekind's study of Lejeune Dirichlet's work led him to his later study of algebraic number fields and ideals. In 1863, he published Lejeune Dirichlet's lectures on number theory as Vorlesungen über Zahlentheorie (\"Lectures on Number Theory\") about which it has been written that:\n\nThe 1879 and 1894 editions of the Vorlesungen included supplements introducing the notion of an ideal, fundamental to ring theory. (The word \"Ring\", introduced later by Hilbert, does not appear in Dedekind's work.) Dedekind defined an ideal as a subset of a set of numbers, composed of algebraic integers that satisfy polynomial equations with integer coefficients. The concept underwent further development in the hands of Hilbert and, especially, of Emmy Noether. Ideals generalize Ernst Eduard Kummer's ideal numbers, devised as part of Kummer's 1843 attempt to prove Fermat's Last Theorem. (Thus Dedekind can be said to have been Kummer's most important disciple.) In an 1882 article, Dedekind and Heinrich Martin Weber applied ideals to Riemann surfaces, giving an algebraic proof of the Riemann–Roch theorem.\n\nIn 1888, he published a short monograph titled Was sind und was sollen die Zahlen? (\"What are numbers and what are they good for?\" Ewald 1996: 790), Online available at: MPIWG GDZ UBS which included his definition of an infinite set. He also proposed an axiomatic foundation for the natural numbers, whose primitive notions were the number one and the successor function. The next year, Giuseppe Peano, citing Dedekind, formulated an equivalent but simpler set of axioms, now the standard ones.\n\nDedekind made other contributions to algebra. For instance, around 1900, he wrote the first papers on modular lattices. In 1872, while on holiday in Interlaken, Dedekind met Georg Cantor. Thus began an enduring relationship of mutual respect, and Dedekind became one of the very first mathematicians to admire Cantor's work concerning infinite sets, proving a valued ally in Cantor's disputes with Leopold Kronecker, who was philosophically opposed to Cantor's transfinite numbers..\n\nBibliography\nPrimary literature in English:\n1890. \"Letter to Keferstein\" in Jean van Heijenoort, 1967. A Source Book in Mathematical Logic, 1879–1931. Harvard Univ. Press: 98–103.\n 1963 (1901). Essays on the Theory of Numbers. Beman, W. W., ed. and trans. Dover.  Contains English translations of Stetigkeit und irrationale Zahlen and Was sind und was sollen die Zahlen?\n 1996. Theory of Algebraic Integers. Stillwell, John, ed. and trans. Cambridge Uni. Press. A translation of Über die Theorie der ganzen algebraischen Zahlen.\n Ewald, William B., ed., 1996. From Kant to Hilbert: A Source Book in the Foundations of Mathematics, 2 vols. Oxford Uni. Press.\n1854. \"On the introduction of new functions in mathematics,\" 754–61.\n1872. \"Continuity and irrational numbers,\" 765–78. (translation of Stetigkeit...)\n1888. What are numbers and what should they be?, 787–832. (translation of Was sind und...)\n1872–82, 1899. Correspondence with Cantor, 843–77, 930–40.\n\nPrimary literature in German:\nGesammelte mathematische Werke (Complete mathematical works, Vol. 1–3). Retrieved 5 August 2009.\n\nSee also\nList of things named after Richard Dedekind\nDedekind cut\nDedekind domain\nDedekind eta function\nDedekind-infinite set\nDedekind number\nDedekind psi function\nDedekind sum\nDedekind zeta function\nIdeal (ring theory)\n\nNotes\n\nReferences\n \n\nFurther reading\nEdwards, H. M., 1983, \"Dedekind's invention of ideals,\" Bull. London Math. Soc. 15: 8–17.\n\nGillies, Douglas A., 1982. Frege, Dedekind, and Peano on the foundations of arithmetic. Assen, Netherlands: Van Gorcum.\nIvor Grattan-Guinness, 2000. The Search for Mathematical Roots 1870–1940. Princeton Uni. Press.\n\nThere is an online bibliography of the secondary literature on Dedekind. Also consult Stillwell's \"Introduction\" to Dedekind (1996).\n\nExternal links\n\n \n \n \n Dedekind, Richard, Essays on the Theory of Numbers. Open Court Publishing Company, Chicago, 1901. at the Internet Archive\n Dedekind's Contributions to the Foundations of Mathematics http://plato.stanford.edu/entries/dedekind-foundations/.\n\nCategory:1831 births\nCategory:1916 deaths\nCategory:19th-century German mathematicians\nCategory:19th-century philosophers\nCategory:20th-century German mathematicians\nCategory:German philosophers\nCategory:ETH Zurich faculty\nCategory:Braunschweig University of Technology faculty\nCategory:University of Göttingen alumni\nCategory:University of Göttingen faculty\nCategory:Humboldt University of Berlin alumni\nCategory:Number theorists\nCategory:Algebraists\nCategory:People from Braunschweig\nCategory:People from the Duchy of Brunswick\nCategory:Members of the French Academy of Sciences\nCategory:Philosophers of mathematics\nCategory:19th-century German writers\nCategory:19th-century German male writers"
    },
    {
      "title": "Vinay V. Deodhar",
      "url": "https://en.wikipedia.org/wiki/Vinay_V._Deodhar",
      "text": "Vinay Vithal Deodhar (3 December 1948 – 18 January 2015) was a Professor Emeritus with the Department of Mathematics at the Indiana University who worked in the area of algebraic groups and representation theory.\n\nEarly life\nDeodhar was born in Bombay, India in 1948.\n\nCareer\nHe earned his Ph.D. from the University of Mumbai in 1974 for his work On Central Extensions of Rational Points of Algebraic Groups done under the supervision of M. S. Raghunathan.\n\nHe was a visiting scholar at the Institute for Advanced Study during 1975-77 and 1992-93.\n\nReferences\n\nExternal links\n \n\nCategory:20th-century Indian mathematicians\nCategory:Algebraists\nCategory:University of Mumbai alumni\nCategory:Scientists from Mumbai\nCategory:Tata Institute of Fundamental Research alumni\nCategory:Institute for Advanced Study visiting scholars\nCategory:1948 births\nCategory:2015 deaths"
    },
    {
      "title": "Leonard Eugene Dickson",
      "url": "https://en.wikipedia.org/wiki/Leonard_Eugene_Dickson",
      "text": "Leonard Eugene Dickson (January 22, 1874 – January 17, 1954) was an American mathematician. He was one of the first American researchers in abstract algebra, in particular the theory of finite fields and classical groups, and is also remembered for a three-volume history of number theory, History of the Theory of Numbers.\n\nLife\nDickson considered himself a Texan by virtue of having grown up in Cleburne, where his father was a banker, merchant, and real estate investor. He attended the University of Texas at Austin, where George Bruce Halsted encouraged his study of mathematics. Dickson earned a B.S. in 1893 and an M.S. in 1894, under Halsted's supervision. Dickson first specialised in Halsted's own specialty, geometry.A. A. Albert (1955) Leonard Eugene Dickson 1874–1954 from National Academy of Sciences\n\nBoth the University of Chicago and Harvard University welcomed Dickson as a Ph.D. student, and Dickson initially accepted Harvard's offer, but chose to attend Chicago instead. In 1896, when he was only 22 years of age, he was awarded Chicago's first doctorate in mathematics, for a dissertation titled The Analytic Representation of Substitutions on a Power of a Prime Number of Letters with a Discussion of the Linear Group, supervised by E. H. Moore.\n\nDickson then went to Leipzig and Paris to study under Sophus Lie and Camille Jordan, respectively. On returning to the USA, he became an instructor at the University of California. In 1899 and at the extraordinarily young age of 25, Dickson was appointed associate professor at the University of Texas. Chicago countered by offering him a position in 1900, and he spent the balance of his career there. At Chicago, he supervised 53 Ph.D. theses; his most accomplished student was probably A. A. Albert. He was a visiting professor at the University of California in 1914, 1918, and 1922. In 1939, he returned to Texas to retire.\n\nDickson married Susan McLeod Davis in 1902; they had two children, Campbell and Eleanor.\n\nDickson was elected to the National Academy of Sciences in 1913, and was also a member of the American Philosophical Society, the American Academy of Arts and Sciences, the London Mathematical Society, the French Academy of Sciences and the Union of Czech Mathematicians and Physicists. Dickson was the first recipient of a prize created in 1924 by The American Association for the Advancement of Science, for his work on the arithmetics of algebras. Harvard (1936) and Princeton (1941) awarded him honorary doctorates.\n\nDickson presided over the  American Mathematical Society in 1917–1918. His December 1918 presidential address, titled \"Mathematics in War Perspective,\" criticized American mathematics for falling short of those of Britain, France, and Germany:\n\n\"Let it not again become possible that thousands of young men shall be so seriously handicapped in their Army and Navy work by lack of adequate preparation in mathematics.\"\n\nIn 1928, he was also the first recipient of the Cole Prize for algebra, awarded annually by the AMS, for his book Algebren und ihre Zahlentheorie.\n\nIt appears that Dickson was a hard man:\n\n\"A hard-bitten character, Dickson tended to speak his mind bluntly; he was always sparing in his praise for the work of others. ... he indulged his serious passions for bridge and billiards and reportedly did not like to lose at either game.\"Karen Parshall (1999) \"Leonard Eugene Dickson\" in American National Biography, volume 6, Oxford University Press, pp 578–79\n\n\"He delivered terse and unpolished lectures and spoke sternly to his students. ... Given Dickson's intolerance for student weaknesses in mathematics, however, his comments could be harsh, even though not intended to be personal. He did not aim to make students feel good about themselves.\"\n\n\"Dickson had a sudden death trial for his prospective doctoral students: he assigned a preliminary problem which was shorter than a dissertation problem, and if the student could solve it in three months, Dickson would agree to oversee the graduate student's work. If not the student had to look elsewhere for an advisor.\"\n\nWork\nDickson had a major impact on American mathematics, especially abstract algebra. His mathematical output consists of 18 books and more than 250 papers. The Collected Mathematical Papers of Leonard Eugene Dickson fill six large volumes.\n\nThe algebraist\nIn 1901, Dickson published his first book Linear groups with an exposition of the Galois field theory, a revision and expansion of his Ph.D. thesis. Teubner in Leipzig published the book, as there was no well-established American scientific publisher at the time. Dickson had already published 43 research papers in the preceding five years; all but seven on finite linear groups. Parshall (1991) described the book as follows:\n\n\"Dickson presented a unified, complete, and general theory of the classical linear groups—not merely over the prime field GF(p) as Jordan had done—but over the general finite field GF(pn), and he did this against the backdrop of a well-developed theory of these underlying fields. ... his book represented the first systematic treatment of finite fields in the mathematical literature.\"\n\nAn appendix in this book lists the non-abelian simple groups then known having order less than 1 billion. He listed 53 of the 56 having order less than 1 million. The remaining three were found in 1960, 1965, and 1967.\n\nDickson worked on finite fields and extended the theory of linear associative algebras initiated by Joseph Wedderburn and Cartan.\n\nHe started the study of modular invariants of a group.\n\nIn 1905, Wedderburn, then at Chicago on a Carnegie Fellowship, published a paper that included three claimed proofs of a theorem stating that all finite division algebras were commutative, now known as Wedderburn's theorem. The proofs all made clever use of the interplay between the additive group of a finite division algebra A, and the multiplicative group A* = A − {0}. Karen Parshall noted that the first of these three proofs had a gap not noticed at the time. Dickson also found a proof of this result but, believing Wedderburn's first proof to be correct, Dickson acknowledged Wedderburn's priority. But Dickson also noted that Wedderburn constructed his second and third proofs only after having seen Dickson's proof. She concluded that Dickson should be credited with the first correct proof.\n\nDickson's search for a counterexample to Wedderburn's theorem led him to investigate nonassociative algebras, and in a series of papers he found all possible three and four-dimensional (nonassociative) division algebras over a field.\n\nIn 1919 Dickson constructed Cayley numbers by a doubling process starting with quaternions ℍ. His method was extended to a doubling of ℝ to produce ℂ, and of ℂ to produce ℍ by A. A. Albert in 1922, and the procedure is known now as the Cayley–Dickson construction of composition algebras.\n\nThe number theorist\nDickson proved many interesting results in number theory, using results of Vinogradov to deduce the ideal Waring theorem in his investigations of additive number theory. He proved the Waring's problem for  under the further condition of\n\nindependently of Subbayya Sivasankaranarayana Pillai who proved it for  ahead of him.\n\nThe three-volume History of the Theory of Numbers (1919–23) is still much consulted today, covering divisibility and primality, Diophantine analysis, and quadratic and higher forms. The work contains little interpretation and makes no attempt to contextualize the results being described, yet it contains essentially every significant number theoretic idea from the dawn of mathematics up to the 1920s except for quadratic reciprocity and higher reciprocity laws. A planned fourth volume on these topics was never written. A. A. Albert remarked that this three volume work \"would be a life's work by itself for a more ordinary man.\"\n\nBibliography\nKaren Parshall (1991) \"A study in group theory: Leonard Eugene Dickson's Linear groups\", Mathematical Intelligencer 13: 7–11 ()\n ()\n\n 1926. Modern algebraic theories\n1923, 1928. Algebraic Numbers. Report with others for U. S. National Research Council.\n1929. Introduction to the Theory of Numbers\n1930. Studies in the Theory of Numbers\n1935. (with G. A. Bliss) \"Biographical Memoir of Eliakim Hastings Moore 1862–1932.\"\n1935. Researches on Waring's problem\n1938. (with H. F. Blichfeldt and G. A. Miller) Theory and Applications of Finite Groups\n1938. Algebras And Their Arithmetics (1st edn. in 1923)\n1939. Modern Elementary Theory of Numbers\n1939. New First Course in the Theory of Equations\nPlane Trigonometry With Practical Applications\n \n\nNotes\n\nExternal links\n \n \n\n \n \n\nCategory:1874 births\nCategory:1954 deaths\nCategory:19th-century American mathematicians\nCategory:20th-century American mathematicians\nCategory:Algebraists\nCategory:Number theorists\nCategory:University of Texas at Austin alumni\nCategory:University of Chicago alumni\nCategory:University of Texas at Austin faculty\nCategory:University of Chicago faculty\nCategory:Historians of mathematics\nCategory:Presidents of the American Mathematical Society\nCategory:Members of the United States National Academy of Sciences\nCategory:People from Independence, Iowa\nCategory:People from Cleburne, Texas\nCategory:Mathematicians from Iowa"
    },
    {
      "title": "Marie-Louise Dubreil-Jacotin",
      "url": "https://en.wikipedia.org/wiki/Marie-Louise_Dubreil-Jacotin",
      "text": "Marie-Louise Dubreil-Jacotin (7 July 1905 – 19 October 1972) was a French mathematician, the second woman to obtain a doctorate in pure mathematics in France, the first woman to become a full professor of mathematics in France, and an expert on fluid mechanics and abstract algebra.\n\nEarly life and education\nMarie-Louise Jacotin was the daughter of a lawyer for a French bank, and the grand-daughter (through her mother) of a glassblower from a family of Greek origin. Her mathematics teacher at the lycée was a sister of mathematician Élie Cartan, and after passing the baccalaureate she was allowed (through the intervention of a friend's father, the head of the institution) to continue studying mathematics at the Collège de Chaptal. On her second attempt, she placed second in the entrance examination for the École Normale Supérieure in 1926 (tied with Claude Chevalley), but by a ministerial decree was moved down to 21st position. After the intervention of Fernand Hauser, the editor of the Journal of the ENS, she was admitted to the school. Her teachers there included Henri Lebesgue and Jacques Hadamard, and she finished her studies in 1929..Marie-Louise Dubreil-Jacotin, Extract from the Annuaire des Anciens Élèves de l'École Normale Supérieure (1974) by Jean Leray, translation by Jean O'Connor, November 2002.\n\nWith the encouragement of ENS director Ernest Vessiot she traveled to Oslo to work with Vilhelm Bjerknes, under whose influence she became interested in the mathematics of waves and the work of Tullio Levi-Civita in this subject. She returned to Paris in 1930, married another mathematician, Paul Dubreil, and joined him on another tour of the mathematics centers of Germany and Italy, including a visit with Levi-Civita. The Dubreils returned to France again in 1931.\n\nCareer and research\nthumb|80px|Charpentier 1932\nWhile her husband taught at Lille, Dubreil-Jacotin continued her research, finishing a doctorate in 1934 concerning the existence of infinitely many different waves in ideal liquids, under the supervision of Henri Villat. Before her, the only women to obtain doctorates in mathematics in France were Marie Charpentier in 1931 (also in pure mathematics) and Edmée Chandon in 1930 (in astronomy and geodesy).\n\nFollowing her husband, she moved to Nancy, but was unable to obtain a faculty position there herself because that was viewed as nepotism; instead, she became a research assistant at the University of Rennes. She was promoted to a teaching position in 1938, and became an assistant professor at the University of Lyon in 1939, while also continuing to teach at Rennes. In 1943 she became a full professor at the University of Poitiers, the first woman to become a full professor of mathematics in France, and in 1955 she was given a chair there in differential and integral calculus. In 1956 she moved to the University of Paris and after the university split she held a professorship at Pierre and Marie Curie University..\n\nIn the 1950s, motivated by the study of averaging operators for turbulence, Dubreil-Jacotin's interests turned towards abstract algebra, and she later performed research in semigroups and graded algebraic structures. She was the author of two textbooks, one on lattice theory and the other on abstract algebra. As well as her technical publications, Jacotin was the author of a work in the history of mathematics, Portraits of women mathematicians.\n\nLegacy\nRue Marie-Louise-Dubreil-Jacotin, a street in the 13th arrondissement of Paris within Paris Diderot University, is named after her, and the University of Poitiers also has a street with the same name.Map of the south campus, Univ. of Poitiers, retrieved 19 May 2015. In semigroup theory, the Dubreil-Jacotin semigroups are also named after her,. as is the Dubreil-Jacotin–Long equation, \"the standard model for internal gravity waves\" in fluid mechanics..\n\nReferences\n\nCategory:1905 births\nCategory:1972 deaths\nCategory:20th-century French mathematicians\nCategory:Algebraists\nCategory:French people of Greek descent\nCategory:École Normale Supérieure alumni\nCategory:University of Lyon faculty\nCategory:University of Poitiers faculty\nCategory:University of Paris faculty\nCategory:Textbook writers\nCategory:French women mathematicians\nCategory:20th-century women mathematicians"
    },
    {
      "title": "Eugene Dynkin",
      "url": "https://en.wikipedia.org/wiki/Eugene_Dynkin",
      "text": "Eugene Borisovich Dynkin (; 11 May 1924 – 14 November 2014) was a Soviet and American mathematician.  He has made contributions to the fields of probability and algebra, especially semisimple Lie groups, Lie algebras, and Markov processes. The Dynkin diagram, the Dynkin system, and Dynkin's lemma are named after him.\n\nBiography \nDynkin was born into a Jewish family, living in Leningrad until 1935, when his family was exiled to Kazakhstan. Two years later, when Dynkin was 13, his father disappeared in the Gulag.\n\nMoscow University\nAt the age of 16, in 1940, Dynkin was admitted to Moscow University. He avoided military service in World War II because of his poor eyesight, and received his M.S. in 1945 and his Ph.D. in 1948. He became an assistant professor at Moscow, but was not awarded a \"chair\" until 1954 because of his political undesirability. His academic progress was made difficult due to his father's fate, as well as Dynkin's Jewish origin; the special efforts of Andrey Kolmogorov, his Ph.D. supervisor, made it possible for Dynkin to progress through graduate school into a teaching position.\n\nUSSR Academy of Sciences\nIn 1968, Dynkin was forced to transfer from the Moscow University to the Central Economic Mathematical Institute of the USSR Academy of Sciences.  He worked there on the theory of economic growth and economic equilibrium.\n\nCornell\nHe remained at the Institute until 1976, when he emigrated to the United States.  In 1977, he became a professor at Cornell University.In , Dynkin states \"I came to the United States from the Soviet Union in 1977\".\n\nDeath\nDynkin died at a hospital in Ithaca, New York, aged 90.Cornell Chronicle obit  Dynkin was an atheist.http://dynkincollection.library.cornell.edu/sites/default/files/Dynkin%20and%20Kuznetsov%20July%2025,%201999-Final%20English%20transcript_0.pdf\n\nMathematical work\nthumb|Eugene Dynkin, 1976\nDynkin is considered to be a rare example of a mathematician who made fundamental contributions to two very distinct areas of mathematics: algebra and probability theory. The algebraic period of Dynkin's mathematical work was between 1944 and 1954, though even during this time a probabilistic theme was noticeable. Indeed, Dynkin's first publication was in 1945, jointly with N. A. Dmitriev, solved a problem on the eigenvalues of stochastic matrices. This problem was raised at Kolmogorov's seminar on Markov chains, while both Dynkin and Dmitriev were undergraduates.\n\nLie Theory\nWhile Dynkin was a student at Moscow University, he attended Israel Gelfand's seminar on Lie groups. In 1944, Gelfand asked him to prepare a survey on the structure and classification of semisimple Lie groups, based on the papers by Hermann Weyl and Bartel Leendert van der Waerden. Dynkin found the papers difficult to read, and in an attempt to better understand the results, he invented the notion of a \"simple root\" in a root system. He represented the pairwise angles between these simple roots in the form of a Dynkin diagram. In this way he obtained a cleaner exposition of the classification of complex semisimple Lie algebras. Of Dynkin's 1947 paper \"Structure of semisimple Lie algebras\", Bertram Kostant wrote: \n\nDynkin's 1952 influential paper \"Semisimple subalgebras of semisimple Lie algebras\", contained large tables and lists, and studied the subalgebras of the exceptional Lie algebras.\n\nProbability theory\nDynkin is considered one of the founders of the modern theory of Markov processes. The results obtained by Dynkin and other participants of his seminar at Moscow University were summarized in two books. The first of these, \"Theory of Markov Processes\", was published in 1959, and laid the foundations of the theory.\n\nDynkin's one-hour talk at the 1962 International Congress of Mathematicians in Stockholm, was delivered by Kolmogorov, since prior to his emigration, Dynkin was never permitted to travel to the West.  This talk was titled \"Markov processes and problems in analysis\".\n\nPrizes and awards\nPrize of the Moscow Mathematical Society, 1951\nInstitute of Mathematical Statistics, Fellow, 1962\nAmerican Academy of Arts and Sciences, Fellow, 1978\nNational Academy of Sciences of the USA, Member, 1985\nAmerican Mathematical Society, Leroy P. Steele Prize for Total Mathematical Work, 1993\nMoscow Mathematical Society, Honorary Member, 1995\nDoctor Honoris Causa of the Pierre and Marie Curie University (Paris 6), 1997\nDoctor of Science (honoris causa) of the University of Warwick, 2003.\nDoctor Honoris Causa of the Independent Moscow University (Russia), 2003\nFellow of the American Mathematical Society, 2012List of Fellows of the American Mathematical Society, retrieved 2012-11-10.\n\nPublications\n\nSee also\nAlgebra\nAffine Dynkin diagram\nCoxeter–Dynkin diagram\nDynkin index\nDynkin–Specht–Wever Lemma\n\nProbability\nDynkin's formula\nDynkin system\n\nNotes\n\nExternal links\n\nDepartment listing at Cornell University\nPersonal web page\nGenealogy Tree of Dynkin's School\nCollection of interviews assembled by Dynkin\n\nCategory:1924 births\nCategory:2014 deaths\nCategory:American atheists\nCategory:Fellows of the American Mathematical Society\nCategory:Members of the United States National Academy of Sciences\nCategory:20th-century American mathematicians\nCategory:21st-century American mathematicians\nCategory:Soviet mathematicians\nCategory:Cornell University faculty\nCategory:Russian atheists\nCategory:Russian Jews\nCategory:Russian mathematicians\nCategory:Algebraists\nCategory:Jewish atheists\nCategory:Probability theorists\nCategory:People from Saint Petersburg"
    }
  ]
}