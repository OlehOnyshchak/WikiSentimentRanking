{
  "pages": [
    {
      "title": "Birkhoff's representation theorem",
      "url": "https://en.wikipedia.org/wiki/Birkhoff%27s_representation_theorem",
      "text": ":''This is about [[lattice theory]]. For other similarly named results, see [[Birkhoff's theorem (disambiguation)]].''\nIn [[mathematics]], '''Birkhoff's representation theorem''' for distributive lattices states that the elements of any [[finite set|finite]] [[distributive lattice]] can be represented as [[finite set]]s, in such a way that the lattice operations correspond to [[Union (set theory)|unions]] and [[Intersection (set theory)|intersections]] of sets. The theorem can be interpreted as providing a [[Bijection|one-to-one correspondence]] between distributive lattices and [[partial order]]s, between [[Knowledge space|quasi-ordinal knowledge spaces]] and [[preorder]]s, or between [[finite topological space]]s and preorders. It is named after [[Garrett Birkhoff]], who published a proof of it in 1937.<ref name=\"birkhoff\">{{harvtxt|Birkhoff|1937}}.</ref>\n\nThe name “Birkhoff's representation theorem” has also been applied to two other results of Birkhoff, one from 1935 on the [[Boolean algebras canonically defined#Representation theorems|representation of Boolean algebras]] as families of sets closed under union, intersection, and complement (so-called ''fields of sets'', closely related to the ''rings of sets'' used by Birkhoff to represent distributive lattices), and [[Birkhoff's HSP theorem]] representing algebras as products of irreducible algebras. Birkhoff's representation theorem has also been called the '''fundamental theorem for finite distributive lattices'''.<ref name=\"stanley\">{{harv|Stanley|1997}}.</ref>\n\n== Understanding the theorem ==\nMany lattices can be defined in such a way that the elements of the lattice are represented by sets, the join operation of the lattice is represented by set union, and the meet operation of the lattice is represented by set intersection. For instance, the [[Boolean lattice]] defined from the family of all subsets of a finite set has this property. More generally any [[finite topological space]] has a lattice of sets as its family of open sets. Because set unions and intersections obey the [[distributive law]], any lattice defined in this way is a distributive lattice. Birkhoff's theorem states that in fact ''all'' finite distributive lattices can be obtained this way, and later generalizations of Birkhoff's theorem state a similar thing for infinite distributive lattices.\n\n==Examples==\n[[File:Birkhoff120.svg|thumb|upright=1.8|The distributive lattice of divisors of 120, and its representation as sets of prime powers.]]\nConsider the [[divisor]]s of some composite number, such as (in the figure) 120,  partially ordered by divisibility. Any two divisors of 120, such as 12 and 20, have a unique [[Greatest common divisor|greatest common factor]] 12&nbsp;∧&nbsp;20&nbsp;=&nbsp;4, the largest number that divides both of them, and a unique [[least common multiple]] 12&nbsp;∨&nbsp;20&nbsp;=&nbsp;60; both of these numbers are also divisors of 120. These two operations ∨ and ∧ satisfy the [[distributive law]], in either of two equivalent forms: (''x''&nbsp;∧&nbsp;''y'')&nbsp;∨&nbsp;''z''&nbsp;=&nbsp;(''x''&nbsp;∨&nbsp;''z'')&nbsp;∧&nbsp;(''y''&nbsp;∨&nbsp;''z'') and (''x''&nbsp;∨&nbsp;''y'')&nbsp;∧&nbsp;''z''&nbsp;=&nbsp;(''x''&nbsp;∧&nbsp;''z'')&nbsp;∨&nbsp;(''y''&nbsp;∧&nbsp;''z''), for all ''x'', ''y'', and ''z''. Therefore, the divisors form a finite [[distributive lattice]].\n\nOne may associate each divisor with the set of [[prime power]]s that divide it: thus, 12 is associated with the set {2,3,4}, while 20 is associated with the set {2,4,5}. Then 12&nbsp;∧&nbsp;20&nbsp;=&nbsp;4 is associated with the set {2,3,4}&nbsp;∩&nbsp;{2,4,5}&nbsp;=&nbsp;{2,4}, while 12&nbsp;∨&nbsp;20&nbsp;=&nbsp;60 is associated with the set {2,3,4}&nbsp;∪&nbsp;{2,4,5}&nbsp;=&nbsp;{2,3,4,5}, so the join and meet operations of the lattice correspond to union and intersection of sets. \n\nThe prime powers 2, 3, 4, 5, and 8 appearing as elements in these sets may themselves be partially ordered by divisibility; in this smaller partial order, 2 ≤ 4 ≤ 8 and there are no order relations between other pairs. The 16 sets that are associated with divisors of 120 are the [[lower set]]s of this smaller partial order, subsets of elements such that if ''x'' ≤ ''y'' and ''y'' belongs to the subset, then ''x'' must also belong to the subset. From any lower set ''L'', one can recover the associated divisor by computing the least common multiple of the prime powers in ''L''. Thus, the partial order on the five prime powers 2, 3, 4, 5, and 8 carries enough information to recover the entire original 16-element divisibility lattice.\n\nBirkhoff's theorem states that this relation between the operations ∧ and ∨ of the lattice of divisors and the operations ∩ and ∪ of the associated sets of prime powers is not coincidental, and not dependent on the specific properties of prime numbers and divisibility: the elements of any finite distributive lattice may be associated with lower sets of a partial order in the same way.\n\nAs another example, the application of Birkhoff's theorem to the family of [[subset]]s of an ''n''-element set, partially ordered by inclusion, produces the [[free distributive lattice]] with ''n'' generators. The number of elements in this lattice is given by the [[Dedekind number]]s.\n\n==The partial order of join-irreducibles==\nIn a lattice, an element ''x'' is ''join-irreducible'' if ''x'' is not the join of a finite set of other elements. Equivalently, ''x'' is join-irreducible if it is neither the bottom element of the lattice (the join of zero elements) nor the join of any two smaller elements. For instance, in the lattice of divisors of 120, there is no pair of elements whose join is 4, so 4 is join-irreducible. An element ''x'' is ''join-prime'' if, whenever ''x''&nbsp;≤&nbsp;''y''&nbsp;∨&nbsp;''z'', either ''x''&nbsp;≤&nbsp;''y'' or ''x''&nbsp;≤&nbsp;''z''. In the same lattice, 4 is join-prime: whenever lcm(''y'',''z'') is divisible by 4, at least one of ''y'' and ''z'' must itself be divisible by 4.\n\nIn any lattice, a join-prime element must be join-irreducible. Equivalently, an element that is not join-irreducible is not join-prime. For, if an element ''x'' is not join-irreducible, there exist smaller ''y'' and ''z'' such that ''x''&nbsp;=&nbsp;''y''&nbsp;∨&nbsp;''z''. But then ''x''&nbsp;≤&nbsp;''y''&nbsp;∨&nbsp;''z'', and ''x'' is not less than or equal to either ''y'' or ''z'', showing that it is not join-prime.\n\nThere exist lattices in which the join-prime elements form a proper subset of the join-irreducible elements, but in a distributive lattice the two types of elements coincide. For, suppose that ''x'' is join-irreducible, and that ''x''&nbsp;≤&nbsp;''y''&nbsp;∨&nbsp;''z''. This inequality is equivalent to the statement that ''x''&nbsp;=&nbsp;''x''&nbsp;∧&nbsp;(''y''&nbsp;∨&nbsp;''z''), and by the distributive law ''x''&nbsp;=&nbsp;(''x''&nbsp;∧&nbsp;''y'')&nbsp;∨&nbsp;(''x''&nbsp;∧&nbsp;''z''). But since ''x'' is join-irreducible, at least one of the two terms in this join must be ''x'' itself, showing that either ''x''&nbsp;=&nbsp;''x''&nbsp;∧&nbsp;''y'' (equivalently ''x''&nbsp;≤&nbsp;''y'') or ''x''&nbsp;=&nbsp;''x''&nbsp;∧&nbsp;''z'' (equivalently ''x''&nbsp;≤&nbsp;''z'').\n\nThe lattice ordering on the subset of join-irreducible elements forms a [[partial order]]; Birkhoff's theorem states that the lattice itself can be recovered from the lower sets of this partial order.\n\n==Birkhoff's theorem==\n[[File:Birkhoff representation theorem.gif|thumb|upright=1.2|Distributive example lattice, with join-irreducible elements a,...,g (shadowed nodes). The lower set a node corresponds to by Birkhoff's isomorphism is shown in blue.]]\nIn any partial order, the [[lower set]]s form a lattice in which the lattice's partial ordering is given by set inclusion, the join operation corresponds to set union, and the meet operation corresponds to set intersection, because unions and intersections preserve the property of being a lower set. Because set unions and intersections obey the distributive law, this is a distributive lattice. Birkhoff's theorem states that any finite distributive lattice can be constructed in this way.\n\n:'''Theorem'''. Any finite distributive lattice ''L'' is isomorphic to the lattice of lower sets of the partial order of the join-irreducible elements of ''L''.\n\nThat is, there is a one-to-one order-preserving correspondence between elements of ''L'' and lower sets of the partial order. The lower set corresponding to an element ''x'' of ''L'' is simply the set of join-irreducible elements of ''L'' that are less than or equal to ''x'', and the element of ''L'' corresponding to a lower set ''S'' of join-irreducible elements is the join of ''S''.\n\nFor any lower set ''S'' of join-irreducible elements, let ''x'' be the join of ''S'', and let ''T'' be the  lower set of the join-irreducible elements less than or equal to ''x''. Then ''S''&nbsp;=&nbsp;''T''. For, every element of ''S'' clearly belongs to ''T'', and any join-irreducible element less than or equal to ''x'' must (by join-primality) be less than or equal to one of the members of ''S'', and therefore must (by the assumption that ''S'' is a lower set) belong to ''S'' itself. Conversely, for any element ''x'' of ''L'', let ''S'' be the join-irreducible elements less than or equal to ''x'', and let ''y'' be the join of ''S''. Then ''x''&nbsp;=&nbsp;''y''. For, as a join of elements less than or equal to ''x'', ''y'' can be no greater than ''x'' itself, but if ''x'' is join-irreducible then ''x'' belongs to ''S'' while if ''x'' is the join of two or more join-irreducible items then they must again belong to ''S'', so ''y''&nbsp;≥&nbsp;''x''. Therefore, the correspondence is one-to-one and the theorem is proved.\n\n==Rings of sets and preorders==\n{{harvtxt|Birkhoff|1937}} defined a ''ring of sets'' to be a [[family of sets]] that is [[Closure (mathematics)|closed]] under the operations of set unions and set intersections; later, motivated by applications in [[mathematical psychology]], {{harvtxt|Doignon|Falmagne|1999}} called the same structure a ''quasi-ordinal [[knowledge space]]''. If the sets in a ring of sets are ordered by inclusion, they form a distributive lattice. The elements of the sets may be given a [[preorder]] in which ''x''&nbsp;≤&nbsp;''y'' whenever some set in the ring contains ''x'' but not ''y''. The ring of sets itself is then the family of lower sets of this preorder, and any preorder gives rise to a ring of sets in this way.\n\n==Functoriality==\nBirkhoff's theorem, as stated above, is a correspondence between individual partial orders and distributive lattices. However, it can also be extended to a correspondence between order-preserving functions of partial orders and [[lattice homomorphism|bounded homomorphisms]] of the corresponding distributive lattices. The direction of these maps is reversed in this correspondence.\n\nLet '''2''' denote the partial order on the two-element set {0, 1}, with the order relation 0 &lt; 1, and (following Stanley) let ''J(P)'' denote the distributive lattice of lower sets of a finite partial order ''P''. Then the elements of ''J(P)'' correspond one-for-one to the order-preserving functions from ''P'' to '''2'''.<ref name=\"stanley\"/> For, if ƒ is such a function, ƒ<sup>−1</sup>(0) forms a lower set, and conversely if ''L'' is a lower set one may define an order-preserving function ƒ<sub>''L''</sub> that maps ''L'' to 0 and that maps the remaining elements of ''P'' to 1. If ''g'' is any order-preserving function from ''Q'' to ''P'', one may define a function ''g''* from ''J(P)'' to ''J(Q)'' that uses the [[composition of functions]] to map any element ''L'' of ''J(P)'' to  ƒ<sub>''L''</sub>&nbsp;∘&nbsp;''g''. This composite function maps ''Q'' to '''2''' and therefore corresponds to an element ''g''*(''L'')&nbsp;=&nbsp;(ƒ<sub>''L''</sub>&nbsp;∘&nbsp;''g'')<sup>−1</sup>(0) of ''J(Q)''. Further, for any ''x'' and ''y'' in ''J(P)'', ''g''*(''x''&nbsp;∧&nbsp;''y'')&nbsp;=&nbsp;''g''*(''x'')&nbsp;∧&nbsp;''g''*(''y'') (an element of ''Q'' is mapped by ''g'' to the lower set ''x''&nbsp;∩&nbsp;''y'' if and only if belongs both to the set of elements mapped to ''x'' and the set of elements mapped to ''y'') and symmetrically ''g''*(''x''&nbsp;∨&nbsp;''y'')&nbsp;=&nbsp;''g''*(''x'')&nbsp;∨&nbsp;''g''*(''y''). Additionally, the bottom element of ''J(P)'' (the function that maps all elements of ''P'' to 0) is mapped by ''g''* to the bottom element of ''J(Q)'', and the top element of ''J(P)'' is mapped by ''g''* to the top element of ''J(Q)''. That is, ''g''* is a homomorphism of bounded lattices.\n\nHowever, the elements of ''P'' themselves correspond one-for-one with bounded lattice homomorphisms from ''J(P)'' to '''2'''. For, if ''x'' is any element of ''P'', one may define a bounded lattice homomorphism ''j<sub>x</sub>'' that maps all lower sets containing ''x'' to 1 and all other lower sets to 0. And, for any lattice homomorphism from ''J(P)'' to '''2''', the elements of ''J(P)'' that are mapped to 1 must have a unique minimal element ''x'' (the meet of all elements mapped to 1), which must be join-irreducible (it cannot be the join of any set of elements mapped to 0), so every lattice homomorphism has the form ''j<sub>x</sub>'' for some ''x''. Again, from any bounded lattice homomorphism ''h'' from ''J(P)'' to ''J(Q)'' one may use  composition of functions to define an order-preserving map ''h''* from ''Q'' to ''P''. It may be verified that ''g''**&nbsp;=&nbsp;''g'' for any order-preserving map ''g'' from ''Q'' to ''P'' and that and ''h''**&nbsp;=&nbsp;''h'' for any bounded lattice homomorphism ''h'' from ''J(P)'' to ''J(Q)''.\n\nIn [[category theory|category theoretic]] terminology, ''J'' is a [[Hom-functor|contravariant hom-functor]] ''J''&nbsp;=&nbsp;Hom(—,'''2''') that defines a [[Equivalence of categories|duality of categories]] between, on the one hand, the category of finite partial orders and order-preserving maps, and on the other hand the category of finite distributive lattices and bounded lattice homomorphisms.\n\n==Generalizations==\nIn an infinite distributive lattice, it may not be the case that the lower sets of the join-irreducible elements are in one-to-one correspondence with lattice elements. Indeed, there may be no join-irreducibles at all. This happens, for instance, in the lattice of all natural numbers, ordered with the reverse of the usual divisibility ordering (so ''x''&nbsp;≤&nbsp;''y'' when ''y'' divides ''x''): any number ''x'' can be expressed as the join of numbers ''xp'' and ''xq'' where ''p'' and ''q'' are distinct [[prime number]]s. However, elements in infinite distributive lattices may still be represented as sets via [[Stone's representation theorem]] for distributive lattices, a form of [[Stone duality]] in which each lattice element corresponds to a [[compact space|compact]] [[open set]] in a certain [[topological space]]. This generalized representation theorem can be expressed as a [[category theory|category-theoretic]] [[Equivalence of categories|duality]] between distributive lattices and [[spectral space]]s (sometimes called coherent spaces, but not the same as the [[coherent space|coherent spaces in linear logic]]), topological spaces in which the compact open sets are closed under intersection and form a [[Base (topology)|base]] for the topology.<ref>{{harvtxt|Johnstone|1982}}.</ref> [[Hilary Priestley]] showed that Stone's representation theorem could be interpreted as an extension of the idea of representing lattice elements by lower sets of a partial order, using Nachbin's idea of ordered topological spaces. Stone spaces with an additional partial order linked with the topology via [[Priestley space|Priestley separation axiom]] can also be used to represent bounded distributive lattices. Such spaces are known as [[Priestley space]]s. Further, certain [[bitopological space]]s, namely [[pairwise Stone space]]s, generalize Stone's original approach by utilizing ''two'' topologies on a set to represent an abstract distributive lattice. Thus, Birkhoff's representation theorem extends to the case of infinite (bounded) distributive lattices in at least three different ways, summed up in [[duality theory for distributive lattices]].\n\nBirkhoff's representation theorem may also be generalized to finite structures other than distributive lattices. In a distributive lattice, the self-dual median operation<ref>{{harvtxt|Birkhoff|Kiss|1947}}.</ref>\n:<math>m(x,y,z)=(x\\vee y)\\wedge(x\\vee z)\\wedge(y\\vee z)=(x\\wedge y)\\vee(x\\wedge z)\\vee(y\\wedge z)</math>\ngives rise to a [[median algebra]], and the covering relation of the lattice forms a [[median graph]]. Finite median algebras and median graphs have a dual structure\nas the set of solutions of a [[2-satisfiability]] instance; {{harvtxt|Barthélemy|Constantin|1993}} formulate this structure equivalently as the family of initial [[Independent set (graph theory)|stable sets]] in a [[mixed graph]].<ref>A minor difference between the 2-SAT and initial stable set formulations is that the latter presupposes the choice of a fixed base point from the median graph that corresponds to the empty initial stable set.</ref> For a distributive lattice, the corresponding mixed graph has no undirected edges, and the initial stable sets are just the lower sets of the [[transitive closure]] of the graph. Equivalently, for a distributive lattice, the [[implication graph]] of the 2-satisfiability instance can be partitioned into two [[Connected component (graph theory)|connected components]], one on the positive variables of the instance and the other on the negative variables; the transitive closure of the positive component is the underlying partial order of the distributive lattice.\n\nAnother result analogous to Birkhoff's representation theorem, but applying to a broader class of lattices, is the theorem of {{harvtxt|Edelman|1980}} that any finite join-distributive lattice may be represented as an [[antimatroid]], a family of sets closed under unions but in which closure under intersections has been replaced by the property that each nonempty set has a removable element.\n\n==Notes==\n{{reflist|2}}\n\n==References==\n*{{citation\n | last1 = Barthélemy | first1 = J.-P.\n | last2 = Constantin | first2 = J.\n | doi = 10.1016/0012-365X(93)90140-O\n | issue = 1–3\n | journal = [[Discrete Mathematics (journal)|Discrete Mathematics]]\n | pages = 49–63\n | title = Median graphs, parallelism and posets\n | volume = 111\n | year = 1993}}.\n*{{citation\n | last = Birkhoff | first = Garrett | authorlink = Garrett Birkhoff\n | doi = 10.1215/S0012-7094-37-00334-X\n | issue = 3\n | journal = Duke Mathematical Journal\n | pages = 443–454\n | title = Rings of sets\n | volume = 3\n | year = 1937}}.\n*{{citation\n | last1 = Birkhoff | first1 = Garrett | author1-link = Garrett Birkhoff\n | last2 = Kiss | first2 = S. A.\n | mr = 0021540\n | issue = 1\n | journal = Bulletin of the American Mathematical Society\n | pages = 749–752\n | title = A ternary operation in distributive lattices\n | url = http://projecteuclid.org/euclid.bams/1183510977 | doi = 10.1090/S0002-9904-1947-08864-9\n | volume = 53\n | year = 1947}}.\n*{{citation\n | last1 = Doignon | first1 = J.-P.\n | last2 = Falmagne | first2 = J.-Cl. | author2-link = Jean-Claude Falmagne\n | isbn = 3-540-64501-2\n | publisher = Springer-Verlag\n | title = Knowledge Spaces\n | year = 1999}}.\n*{{citation\n | last = Edelman | first = Paul H.\n | doi = 10.1007/BF02482912\n | issue = 1\n | journal = Algebra Universalis\n | pages = 290–299\n | title = Meet-distributive lattices and the anti-exchange closure\n | volume = 10\n | year = 1980}}.\n*{{citation\n | last = Johnstone | first = Peter | author-link = Peter Johnstone (mathematician)\n | isbn = 978-0-521-33779-3\n | publisher = Cambridge University Press\n | title = Stone Spaces\n | contribution = II.3 Coherent locales\n | pages = 62–69\n | year = 1982}}.\n*{{citation\n | doi = 10.1112/blms/2.2.186\n | last = Priestley | first = H. A.\n | journal = Bulletin of the London Mathematical Society\n | pages = 186–190\n | title = Representation of distributive lattices by means of ordered Stone spaces\n | issue = 2\n | volume = 2\n | year = 1970}}.\n*{{citation\n | doi = 10.1112/plms/s3-24.3.507\n | last = Priestley | first = H. A.\n | journal = Proceedings of the London Mathematical Society\n | pages = 507–530\n | title = Ordered topological spaces and the representation of distributive lattices\n | issue = 3\n | volume = 24\n | year = 1972}}.\n*{{citation\n | last = Stanley | first = R. P. | author-link = Richard P. Stanley\n | title = Enumerative Combinatorics, Volume I\n | series = Cambridge Studies in Advanced Mathematics 49\n | publisher = Cambridge University Press\n | year = 1997\n | pages = 104–112}}.\n\n[[Category:Lattice theory]]\n[[Category:Theorems in algebra]]"
    },
    {
      "title": "Boolean prime ideal theorem",
      "url": "https://en.wikipedia.org/wiki/Boolean_prime_ideal_theorem",
      "text": "In [[mathematics]], the '''Boolean prime ideal theorem''' states that [[ideal (order theory)|ideals]] in a [[Boolean algebra (structure)|Boolean algebra]] can be extended to [[Ideal (order theory)#Prime ideals | prime ideal]]s.  A variation of this statement for [[filter (mathematics)|filters]] on sets is known as the [[#The ultrafilter lemma|ultrafilter lemma]].  Other theorems are obtained by considering different mathematical structures with appropriate notions of ideals, for example, [[ring (mathematics)|rings]] and prime ideals (of ring theory), or [[distributive lattice]]s and ''maximal'' ideals (of [[order theory]]).  This article focuses on prime ideal theorems from order theory.\n\nAlthough the various prime ideal theorems may appear simple and intuitive, they cannot be deduced in general from the axioms of [[Zermelo–Fraenkel set theory]] without the axiom of choice  (abbreviated ZF). Instead, some of the statements turn out to be equivalent to the [[axiom of choice]] (AC), while others—the Boolean prime ideal theorem, for instance—represent a property that is strictly weaker than AC. It is due to this intermediate status between ZF and ZF&nbsp;+&nbsp;AC (ZFC) that the Boolean prime ideal theorem is often taken as an axiom of set theory. The abbreviations BPI or PIT (for Boolean algebras) are sometimes used to refer to this additional axiom.\n\n==Prime ideal theorems==\nAn [[ideal (order theory)|order ideal]] is a (non-empty) [[directed set|directed]] [[lower set]]. If the considered [[partially ordered set]] (poset) has binary [[supremum|suprema]] (a.k.a. [[join and meet|joins]]), as do the posets within this article, then this is equivalently characterized as a non-empty lower set ''I'' that is closed for binary suprema (i.e. ''x'', ''y'' in ''I'' imply ''x''<math>\\vee</math>''y'' in ''I''). An ideal ''I'' is prime if its set-theoretic complement in the poset is a [[Filter (mathematics)|filter]]. Ideals are proper if they are not equal to the whole poset.\n\nHistorically, the first statement relating to later prime ideal theorems was in fact referring to filters—subsets that are ideals with respect to the [[duality (order theory)|dual]] order. The ultrafilter lemma states that every filter on a set is contained within some maximal (proper) filter—an ''ultrafilter''. Recall that filters on sets are proper filters of the Boolean algebra of its [[powerset]]. In this special case, maximal filters (i.e. filters that are not strict subsets of any proper filter) and prime filters (i.e. filters that with each union of subsets ''X''  and ''Y'' contain also ''X'' or ''Y'') coincide. The dual of this statement thus assures that every ideal of a powerset is contained in a prime ideal.\n\nThe above statement led to various generalized prime ideal theorems, each of which exists in a weak and in a strong form. ''Weak prime ideal theorems'' state that every ''non-trivial'' algebra of a certain class has at least one prime ideal. In contrast, ''strong prime ideal theorems'' require that every ideal that is disjoint from a given filter can be extended to a prime ideal that is still disjoint from that filter. In the case of algebras that are not posets, one uses different substructures instead of filters. Many forms of these theorems are actually known to be equivalent, so that the assertion that \"PIT\" holds is usually taken as the assertion that the corresponding statement for Boolean algebras (BPI) is valid.\n\nAnother variation of similar theorems is obtained by replacing each occurrence of ''prime ideal'' by ''maximal ideal''. The corresponding maximal ideal theorems (MIT) are often—though not always—stronger than their PIT equivalents.\n\n== Boolean prime ideal theorem ==\nThe Boolean prime ideal theorem is the strong prime ideal theorem for Boolean algebras. Thus the formal statement is:\n\n: Let ''B'' be a Boolean algebra, let ''I'' be an ideal and let ''F'' be a filter of ''B'', such that ''I'' and ''F'' are [[disjoint set|disjoint]]. Then ''I'' is contained in some prime ideal of ''B'' that is disjoint from ''F''.\n\nThe weak prime ideal theorem for Boolean algebras simply states:\n\n: Every Boolean algebra contains a prime ideal.\n\nWe refer to these statements as the weak and strong ''BPI''.  The two are equivalent, as the strong BPI clearly implies the weak BPI, and the reverse implication can be achieved by using the weak BPI to find prime ideals in the appropriate quotient algebra.\n\nThe BPI can be expressed in various ways. For this purpose, recall the following theorem:\n\nFor any ideal ''I'' of a Boolean algebra ''B'', the following are equivalent:\n* ''I'' is a prime ideal.\n* ''I'' is a maximal ideal, i.e. for any proper ideal ''J'', if ''I'' is contained in ''J'' then ''I'' = ''J''.\n* For every element ''a'' of ''B'', ''I'' contains exactly one of {''a'', ¬''a''}.\nThis theorem is a well-known fact for Boolean algebras. Its dual establishes the equivalence of prime filters and ultrafilters. Note that the last property is in fact self-dual—only the prior assumption that ''I'' is an ideal gives the full characterization. All of the implications within this theorem can be proven in ZF.\n\nThus the following (strong) maximal ideal theorem (MIT) for Boolean algebras is equivalent to BPI:\n\n:Let ''B'' be a Boolean algebra, let ''I'' be an ideal and let ''F'' be a filter of ''B'', such that ''I'' and ''F'' are disjoint. Then ''I'' is contained in some maximal ideal of ''B'' that is disjoint from ''F''.\n\nNote that one requires \"global\" maximality, not just maximality with respect to being disjoint from ''F''. Yet, this variation yields another equivalent characterization of BPI:\n\n:Let ''B'' be a Boolean algebra, let ''I'' be an ideal and let ''F'' be a filter of ''B'', such that ''I'' and ''F'' are disjoint. Then ''I'' is contained in some ideal of ''B'' that is maximal among all ideals disjoint from ''F''.\n\nThe fact that this statement is equivalent to BPI is easily established by noting the following theorem: For any [[distributive lattice]] ''L'', if an ideal ''I'' is maximal among all ideals of ''L'' that are disjoint to a given filter ''F'', then ''I'' is a prime ideal. The proof for this statement (which can again be carried out in ZF set theory) is included in the article on ideals. Since any Boolean algebra is a distributive lattice, this shows the desired implication.\n\nAll of the above statements are now easily seen to be equivalent. Going even further, one can exploit the fact the dual orders of Boolean algebras are exactly the Boolean algebras themselves. Hence, when taking the equivalent duals of all former statements, one ends up with a number of theorems that equally apply to Boolean algebras, but where every occurrence of ''ideal'' is replaced by ''filter''. It is worth noting that for the special case where the Boolean algebra under consideration is a [[powerset]] with the [[subset]] ordering, the \"maximal filter theorem\" is called the ultrafilter lemma.\n\nSumming up, for Boolean algebras, the weak and strong MIT, the weak and strong PIT, and these statements with filters in place of ideals are all equivalent. It is known that all of these statements are consequences of the [[Axiom of Choice]], ''AC'', (the easy proof makes use of [[Zorn's lemma]]), but cannot be proven in [[Zermelo–Fraenkel set theory|ZF]] (Zermelo-Fraenkel set theory without ''AC''), if ZF is [[consistent]]. Yet, the BPI is strictly weaker than the axiom of choice, though the proof of this statement, due to  J. D. Halpern and [[Azriel Lévy]] is rather non-trivial.\n\n== Further prime ideal theorems ==\nThe prototypical properties that were discussed for Boolean algebras in the above section can easily be modified to include more general [[lattice (order)|lattices]], such as [[distributive lattice]]s or [[Heyting algebra]]s. However, in these cases maximal ideals are different from prime ideals, and the relation between PITs and MITs is not obvious.\n\nIndeed, it turns out that the MITs for distributive lattices and even for Heyting algebras are equivalent to the axiom of choice. On the other hand, it is known that the strong PIT for distributive lattices is equivalent to BPI (i.e. to the MIT and PIT for Boolean algebras). Hence this statement is strictly weaker than the axiom of choice. Furthermore, observe that Heyting algebras are not self dual, and thus using filters in place of ideals yields different theorems in this setting. Maybe surprisingly, the MIT for the duals of Heyting algebras is not stronger than BPI, which is in sharp contrast to the abovementioned MIT for Heyting algebras.\n\nFinally, prime ideal theorems do also exist for other (not order-theoretical) abstract algebras. For example, the MIT for rings implies the axiom of choice. This situation requires to replace the order-theoretic term \"filter\" by other concepts—for rings a \"multiplicatively closed subset\" is appropriate.\n\n== The ultrafilter lemma ==\nA filter on a set ''X'' is a nonempty collection of nonempty subsets of ''X'' that is closed under finite intersection and under superset.  An ultrafilter is a maximal filter.   The ultrafilter lemma states that every filter  on a set ''X'' is a subset of some [[ultrafilter]] on ''X''.<ref>{{citation\n | last = Halpern | first = James D.\n | issue = 3\n | journal = Proceedings of the American Mathematical Society\n | pages = 670–673\n | title = Bases in Vector Spaces and the Axiom of Choice\n | jstor = 2035388\n | volume = 17\n | year = 1966\n | publisher = American Mathematical Society\n | doi = 10.1090/S0002-9939-1966-0194340-1}}.</ref>  This lemma is most often used in the study of [[topology]].  An ultrafilter that does not contain finite sets is called \"non-principal\".The ultrafilter lemma, and in particular the existence of non-principal ultrafilters (consider the filter of all sets with finite complements), follows easily from [[Zorn's lemma]].\n\nThe ultrafilter lemma is equivalent to the Boolean prime ideal theorem, with the equivalence provable in ZF set theory without the axiom of choice.  The idea behind the proof is that the subsets of any set form a Boolean algebra partially ordered by inclusion, and any Boolean algebra is representable as an algebra of sets by [[Stone's representation theorem]].\n\n== Applications ==\nIntuitively, the Boolean prime ideal theorem states that there are \"enough\" prime ideals in a Boolean algebra in the sense that we can extend ''every'' ideal to a maximal one. This is of practical importance for proving [[Stone's representation theorem for Boolean algebras]], a special case of [[Stone duality]], in which one equips the set of all prime ideals with a certain topology and can indeed regain the original Boolean algebra ([[up to]] [[isomorphism]]) from this data. Furthermore, it turns out that in applications one can freely choose either to work with prime ideals or with prime filters, because every ideal uniquely determines a filter: the set of all Boolean complements of its elements. Both approaches are found in the literature.\n\nMany other theorems of general topology that are often said to rely on the axiom of choice are in fact equivalent to BPI. For example, the theorem that a product of compact [[Hausdorff spaces]] is compact is equivalent to it. If we leave out \"Hausdorff\" we get a [[Tychonoff's theorem|theorem]] equivalent to the full axiom of choice.\n\nIn [[graph theory]], the [[De Bruijn–Erdős theorem (graph theory)|de Bruijn–Erdős theorem]] is another equivalent to BPI. It states that, if a given infinite graph requires at least some finite number {{mvar|k}} in any [[graph coloring]], then it has a finite subgraph that also requires {{mvar|k|colors}}.<ref>{{citation\n | last = Läuchli | first = H.\n | doi = 10.1007/BF02771458\n | journal = Israel Journal of Mathematics\n | mr = 0288051\n | pages = 422–429\n | title = Coloring infinite graphs and the Boolean prime ideal theorem\n | volume = 9\n | year = 1971}}.</ref>\n\nA not too well known application of the Boolean prime ideal theorem is the existence of a [[non-measurable set]]<ref>{{citation\n | last = Sierpiński | first = Wacław | author-link = Wacław Sierpiński\n | journal = [[Fundamenta Mathematicae]]\n | pages = 96–99\n | title = Fonctions additives non complètement additives et fonctions non mesurables\n | volume = 30\n | year = 1938}}</ref> (the example usually given is the [[Vitali set]], which requires the axiom of choice).  From this and the fact that the BPI is strictly weaker than the axiom of choice, it follows that the existence of non-measurable sets is strictly weaker than the axiom of choice.\n\nIn linear algebra, the Boolean prime ideal theorem can be used to prove that any two [[Basis (linear algebra)|bases]] of a given [[vector space]] have the same [[cardinality]].\n\n==See also==\n* [[List of Boolean algebra topics]]\n\n==Notes==\n{{reflist}}\n\n==References==\n\n*{{citation\n | last1 = Davey | first1 = B. A.\n | last2 = Priestley | first2 = H. A. | author2-link = Hilary Priestley\n | edition = 2nd\n | isbn = 978-0-521-78451-1\n | publisher = Cambridge University Press\n | title = Introduction to Lattices and Order\n | year = 2002}}.\n: ''An easy to read introduction, showing the equivalence of PIT for Boolean algebras and distributive lattices.''\n\n*{{citation\n | last = Johnstone | first = Peter | author-link = Peter Johnstone (mathematician)\n | isbn = 978-0-521-33779-3\n | publisher = Cambridge University Press\n | series = Cambridge studies in advanced mathematics\n | title = Stone Spaces\n | volume = 3\n | year = 1982}}.\n: ''The theory in this book often requires choice principles. The notes on various chapters discuss the general relation of the theorems to PIT and MIT for various structures (though mostly lattices) and give pointers to further literature.''\n\n*{{citation\n | last = Banaschewski | first = B.\n | doi = 10.1112/jlms/s2-27.2.193\n | issue = 2\n | journal = [[Journal of the London Mathematical Society]] |series=Second Series\n\n| pages = 193–202\n\n | title = The power of the ultrafilter theorem\n | volume = 27\n | year = 1983}}.\n: ''Discusses the status of the ultrafilter lemma.''\n\n*{{citation\n | last = Erné | first = M.\n | journal = Applied Categorical Structures\n | pages = 115–144\n | title = Prime ideal theory for general algebras\n | volume = 8\n | year = 2000\n | doi = 10.1023/A:1008611926427}}.\n: ''Gives many equivalent statements for the BPI, including prime ideal theorems for other algebraic structures. PITs are considered as special instances of separation lemmas.''\n\n[[Category:Theorems in algebra]]\n[[Category:Boolean algebra]]\n[[Category:Order theory]]\n[[Category:Axiom of choice]]"
    },
    {
      "title": "Cartan–Brauer–Hua theorem",
      "url": "https://en.wikipedia.org/wiki/Cartan%E2%80%93Brauer%E2%80%93Hua_theorem",
      "text": "In [[abstract algebra]], the '''Cartan&ndash;Brauer&ndash;Hua theorem''' (named after [[Richard Brauer]], [[Élie Cartan]], and [[Hua Luogeng]]) is a theorem pertaining to [[division ring]]s. It says that given two division rings {{nowrap|''K'' ⊆ ''D''}} such that ''xKx''<sup>−1</sup> is contained in ''K'' for every ''x'' not equal to 0 in ''D'', either ''K'' is contained in the [[Center (algebra)|center]] of ''D'', or {{nowrap|1=''K'' = ''D''}}.  In other words, if the [[unit group]] of ''K'' is a [[normal subgroup]] of the unit group of ''D'', then either {{nowrap|1=''K'' = ''D''}} or ''K'' is central {{harv|Lam|2001|p=211}}.\n\n==References==\n{{reflist}}\n*{{cite book |last=Herstein |first=I. N. |authorlink=Israel Nathan Herstein |title=Topics in algebra |publisher=Wiley |location=New York |year=1975 |page=368 |isbn=0-471-01090-1}}\n* {{Cite book | last1=Lam | first1=Tsit-Yuen | title=A First Course in Noncommutative Rings | publisher=[[Springer-Verlag]] | location=Berlin, New York | edition=2nd | isbn=978-0-387-95325-0 | mr=1838439 | year=2001 | ref=harv}}\n\n{{DEFAULTSORT:Cartan-Brauer-Hua theorem}}\n[[Category:Ring theory]]\n[[Category:Theorems in algebra]]\n\n\n{{Abstract-algebra-stub}}"
    },
    {
      "title": "Chevalley–Warning theorem",
      "url": "https://en.wikipedia.org/wiki/Chevalley%E2%80%93Warning_theorem",
      "text": "In number theory, the  '''Chevalley–Warning theorem''' implies that certain [[polynomial|polynomial equations]] in sufficiently many variables over a [[finite field]] have solutions. It was proved by {{harvs|txt|first=Ewald |last=Warning|year=1935}} and a slightly weaker form of the theorem, known as '''Chevalley's theorem''', was proved by {{harvs|txt|authorlink=Claude Chevalley|last=Chevalley|year=  1935}}. Chevalley's theorem implied Artin's and Dickson's conjecture that finite fields are [[quasi-algebraically closed field]]s {{harv|Artin|1982|loc=page x}}.\n\n== Statement of the theorems ==\n\nLet <math>\\mathbb{F}</math> be a finite field and <math>\\{f_j\\}_{j=1}^r\\subseteq\\mathbb{F}[X_1,\\ldots,X_n]</math> be a set of polynomials such that the number of variables satisfies \n\n:<math>n>\\sum_{j=1}^r d_j</math>\n\nwhere <math>d_j</math> is the [[total degree]] of <math>f_j</math>. The theorems are statements about the solutions of the following system of polynomial equations\n\n:<math>f_j(x_1,\\dots,x_n)=0\\quad\\text{for}\\, j=1,\\ldots, r.</math>\n\n* ''Chevalley–Warning theorem'' states that the number of common solutions <math>(a_1,\\dots,a_n) \\in \\mathbb{F}^n</math> is divisible by the [[characteristic (algebra)|characteristic]] <math>p</math> of <math>\\mathbb{F}</math>. Or in other words, the cardinality of the vanishing set of <math>\\{f_j\\}_{j=1}^r</math> is <math>0</math> modulo <math>p</math>.\n* ''Chevalley's theorem'' states that if the system has the trivial solution <math>(0,\\dots,0) \\in \\mathbb{F}^n</math>, i.e. if the polynomials have no constant terms, then the system also has a non-trivial solution <math>(a_1,\\dots,a_n) \\in \\mathbb{F}^n \\backslash \\{(0,\\dots,0)\\}</math>. \n\nChevalley's theorem is an immediate consequence of the Chevalley–Warning theorem since <math>p</math> is at least 2. \n\nBoth theorems are best possible in the sense that, given any <math>n</math>, the list <math>f_j = x_j, j=1,\\dots,n</math> has total degree <math>n</math> and only the trivial solution. Alternatively, using just one polynomial, we can take ''f''<sub>1</sub> to be the degree ''n'' polynomial given by the [[Field norm|norm]] of ''x''<sub>1</sub>''a''<sub>1</sub> + ... + ''x''<sub>''n''</sub>''a''<sub>''n''</sub> where the elements ''a'' form a basis of the finite field of order ''p''<sup>''n''</sup>.\n\nWarning proved another theorem, known as Warning's second theorem, which states that if the system of polynomial equations has the trivial solution, then it has at least <math>q^{n-d}</math> solutions where <math>q</math> is the size of the finite field and <math>d := d_1 + \\dots + d_r</math>. Chevalley's theorem also follows directly from this.\n\n==Proof of Warning's theorem==\n''Remark:'' If <math>i<q-1</math> then \n:<math>\\sum_{x\\in\\mathbb{F}}x^i=0</math>\nso the sum over <math>\\mathbb{F}^n</math> of any polynomial in <math>x_1,\\ldots,x_n</math> of degree less than <math>n(q-1)</math> also vanishes.\n\nThe total number of common solutions modulo <math>p</math> of <math>f_1, \\ldots, f_r = 0</math> is equal to\n:<math>\\sum_{x\\in\\mathbb{F}^n}(1-f_1^{q-1}(x))\\cdot\\ldots\\cdot(1-f_r^{q-1}(x)) </math>\nbecause each term is 1 for a solution and 0 otherwise.\nIf the sum of the degrees of the polynomials <math>f_i</math> is less than ''n'' then this vanishes by the remark above.\n\n== Artin's conjecture ==\n\nIt is a consequence of Chevalley's theorem that finite fields are [[quasi-algebraically closed]]. This had been conjectured by [[Emil Artin]] in 1935. The motivation behind Artin's conjecture was his observation that quasi-algebraically closed fields have trivial [[Brauer group]], together with the fact that finite fields have trivial Brauer group by [[Wedderburn's little theorem|Wedderburn's theorem]].\n\n== The Ax–Katz theorem ==\n\nThe '''Ax–Katz theorem''', named after [[James Ax]] and [[Nicholas Katz]], determines more accurately a power <math>q^b</math> of the cardinality <math>q</math> of <math>\\mathbb{F}</math> dividing the number of solutions; here, if <math>d</math> is the largest of the <math>d_j</math>, then the exponent <math>b</math> can be taken as the [[ceiling function]] of \n\n: <math>\\frac{n - \\sum_j d_j}{d}.</math>\n\nThe Ax–Katz result has an interpretation in [[étale cohomology]] as a divisibility result for the (reciprocals of) the zeroes and poles of the [[local zeta-function]]. Namely, the same power of <math>q</math> divides each of these [[algebraic integer]]s.\n\n== See also ==\n* [[Combinatorial Nullstellensatz]]\n\n==References==\n*{{Citation | last1=Artin | first1=Emil | author1-link=Emil Artin | editor1-last=Lang | editor1-first=Serge. | editor2-last=Tate | editor2-first=John | editor2-link=John Tate | title=Collected papers | publisher=[[Springer-Verlag]] | location=Berlin, New York | isbn=978-0-387-90686-7 | mr=671416 | year=1982}}\n*{{citation| last=Ax | first=James | authorlink=James Ax | year=1964 | title=Zeros of polynomials over finite fields | journal=American Journal of Mathematics | volume=86 | pages=255–261 | mr=0160775 | doi=10.2307/2373163}}\n*{{citation| last=Chevalley | first=Claude | authorlink=Claude Chevalley | year=1935 | title=Démonstration d'une hypothèse de M. Artin | language=French| journal=Abhandlungen aus dem Mathematischen Seminar der Universität Hamburg | volume=11 | pages=73–75 | zbl=0011.14504 |jfm=61.1043.01| doi=10.1007/BF02940714}}\n*{{citation| last=Katz | first=Nicholas M. | authorlink=Nicholas Katz | year=1971 | title=On a theorem of Ax | journal=Amer. J. Math. | volume=93 | issue=2 | pages=485–499 | doi=10.2307/2373389}}\n*{{citation| last=Warning | first=Ewald | year=1935 | title=Bemerkung zur vorstehenden Arbeit von Herrn Chevalley|language=German | journal=Abhandlungen aus dem Mathematischen Seminar der Universität Hamburg | volume=11 | pages=76–83 | zbl=0011.14601 |jfm=61.1043.02| doi=10.1007/BF02940715}}\n*{{citation| last=Serre | first=Jean-Pierre | authorlink = Jean-Pierre Serre| year=1973 | title=A course in arithmetic| pages=5–6 | isbn=0-387-90040-3}}\n\n==External links==\n*{{cite web |url=http://mathoverflow.net/questions/178318/proofs-of-the-chevalley-warning-theorem |title=Proof's of the Chevalley-Warning theorem}}\n\n{{DEFAULTSORT:Chevalley-Warning theorem}}\n[[Category:Finite fields]]\n[[Category:Diophantine geometry]]\n[[Category:Theorems in algebra]]"
    },
    {
      "title": "Classification of finite simple groups",
      "url": "https://en.wikipedia.org/wiki/Classification_of_finite_simple_groups",
      "text": "{{Group theory sidebar|Finite}}\n\nIn [[mathematics]], the '''classification of the finite [[Simple group|simple groups]]''' is a theorem stating that every [[List of finite simple groups|finite simple group]] belongs to one of four broad classes described below. These [[group (mathematics)|groups]] can be seen as the basic building blocks of all [[finite group]]s, in a way reminiscent of the way the [[prime number]]s are the basic building blocks of the [[natural number]]s.  The [[Jordan–Hölder theorem]] is a more precise way of stating this fact about finite groups. However, a significant difference from  [[integer factorization]] is that such \"building blocks\" do not necessarily determine  a unique group, since there might be many non-[[isomorphic]] groups with the same [[composition series]] or, put in another way, the [[group extension#Extension problem|extension problem]] does not have a unique solution.\n\n[[Group theory]] is central to many areas of pure and applied mathematics and the classification theorem is one of the great achievements of modern mathematics. The proof consists of tens of thousands of pages in several hundred journal articles written by about 100 authors, published mostly between 1955 and 2004. [[Daniel Gorenstein|Gorenstein]] (d.1992), [[Richard Lyons (mathematician)|Lyons]], and [[Ronald Solomon|Solomon]] are gradually publishing a simplified and revised version of the proof.\n\n==Statement of the classification theorem==\n{{Main|List of finite simple groups}}\n{{math_theorem|Every finite [[simple group]] is isomorphic to one of the following groups:\n* a member of one of three infinite classes of such, namely:\n** the [[cyclic groups]] of prime order,\n** the [[alternating groups]] of degree at least 5,\n** the [[groups of Lie type]]\n* one of 26 groups called the \"[[sporadic groups]]\"\n* the [[Tits group]] (which is sometimes considered a 27th sporadic group).}}\n\nThe classification theorem has applications in many branches of mathematics, as questions about the structure of [[finite group]]s (and their action on other mathematical objects) can sometimes be reduced to questions about finite simple groups. Thanks to the classification theorem, such questions can sometimes be answered by checking each family of simple groups and each sporadic group.\n\n[[Daniel Gorenstein]] announced in 1983 that the finite simple groups had all been classified, but this was premature as he had been misinformed about the proof of the classification of [[quasithin group]]s. The completed proof of the classification was announced by {{harvtxt|Aschbacher|2004}} after Aschbacher and Smith published a 1221-page proof for the missing quasithin case.\n\n==Overview of the proof of the classification theorem==\n{{harvs|txt|last=Gorenstein|year1=1982|year2=1983}} wrote two volumes outlining the low rank and odd characteristic part of the proof, and {{harvs|txt| | last1=Aschbacher | first1=Michael | author1-link=Michael Aschbacher | last2=Lyons | first2=Richard | last3=Smith | first3=Stephen D. | last4=Solomon | first4=Ronald | title=The Classification of Finite Simple Groups: Groups of Characteristic 2 Type | url=http://www.ams.org/bookstore?fn=20&ikey=SURV-172 | series=Mathematical Surveys and Monographs | isbn=978-0-8218-5336-8 | year=2011 | volume=172}}\nwrote a 3rd volume covering the remaining characteristic 2 case. The proof can be broken up into several major pieces as follows:\n\n===Groups of small 2-rank===\nThe simple groups of low [[rank of a group#Generalizations and related notions|2-rank]] are mostly groups of Lie type of small rank over fields of odd characteristic, together with five alternating and seven characteristic 2 type and nine sporadic groups.\n\nThe simple groups of small 2-rank include:\n*Groups of 2-rank 0, in other words groups of odd order, which are all [[Solvable group|solvable]] by the [[Feit–Thompson theorem]].\n*Groups of 2-rank 1. The Sylow 2-subgroups are either cyclic, which is easy to handle using the transfer map, or generalized [[quaternion]], which are handled with the [[Brauer–Suzuki theorem]]: in particular there are no simple groups of 2-rank 1.\n*Groups of 2-rank 2. Alperin showed that the Sylow subgroup must be dihedral, quasidihedral, wreathed, or a Sylow 2-subgroup of ''U''<sub>3</sub>(4). The first case was done by the [[Gorenstein–Walter theorem]] which showed that the only simple groups are isomorphic to ''L''<sub>2</sub>(''q'') for ''q'' odd or ''A''<sub>7</sub>, the second and third cases were done by the [[Alperin–Brauer–Gorenstein theorem]] which implies that the only simple groups are isomorphic to ''L''<sub>3</sub>(''q'') or ''U''<sub>3</sub>(''q'') for ''q'' odd or ''M''<sub>11</sub>, and the last case was done by Lyons who showed that ''U''<sub>3</sub>(4) is the only simple possibility.\n*Groups of sectional 2-rank at most 4, classified by the [[Gorenstein–Harada theorem]].\nThe classification of groups of small 2-rank, especially ranks at most 2, makes heavy use of ordinary and modular character theory, which is almost never directly used elsewhere in the classification.\n\nAll groups not of small 2 rank can be split into two major classes: groups of component type and groups of characteristic 2 type. This is because if a group has sectional 2-rank at least 5 then MacWilliams showed that its Sylow 2-subgroups are connected, and the [[balance theorem]] implies that any simple group with connected Sylow 2-subgroups is either of component type or characteristic 2 type. (For groups of low 2-rank the proof of this breaks down, because theorems such as the [[signalizer functor]] theorem only work for groups with elementary abelian subgroups of rank at least 3.)\n\n===Groups of component type===\nA group is said to be of component type if for some centralizer ''C'' of an involution, ''C''/''O''(''C'') has a component (where ''O''(''C'') is the core of ''C'', the maximal normal subgroup of odd order).\nThese are more or less the groups of Lie type of odd characteristic of large rank, and alternating groups, together with some sporadic groups.\nA major step in this case is to eliminate the obstruction of the core of an involution. This is accomplished by the [[B-theorem]], which states that every component of ''C''/''O''(''C'') is the image of a component of ''C''.\n\nThe idea is that these groups have a centralizer of an involution with a component that is a smaller quasisimple group, which can be assumed to be already known by induction. So to classify these groups one takes every central extension of every known finite simple group, and finds all simple groups with a centralizer of involution with this as a component. This gives a rather large number of different cases to check: there are not only 26 sporadic groups and 16 families of groups of Lie type and the alternating groups, but also many of the groups of small rank or over small fields behave differently from the general case and have to be treated separately, and the groups of Lie type of even and odd characteristic are also quite different.\n\n===Groups of characteristic 2 type===\n\nA group is of characteristic 2 type if the [[generalized Fitting subgroup]] ''F''*(''Y'') of every 2-local subgroup ''Y'' is a 2-group.\nAs the name suggests these are roughly the groups of Lie type over fields of characteristic 2, plus a handful of others that are alternating or sporadic or of odd characteristic. Their classification is divided into the small and large rank cases, where the rank is the largest rank of an odd abelian subgroup normalizing a nontrivial 2-subgroup, which is often (but not always) the same as the rank of a Cartan subalgebra when the group is a group of Lie type in characteristic 2.\n\nThe rank 1 groups are the thin groups, classified by Aschbacher, and the rank 2 ones are the notorious [[quasithin group]]s, classified by Aschbacher and Smith. These correspond roughly to groups of Lie type of ranks 1 or 2 over fields of characteristic 2.\n\nGroups of rank at least 3 are further subdivided into 3 classes by the [[trichotomy theorem]], proved by Aschbacher for rank 3 and by Gorenstein and Lyons for rank at least 4.\nThe three classes are groups of GF(2) type (classified mainly by Timmesfeld), groups of \"standard type\" for some odd prime (classified by the Gilman–Griess theorem and work by several others), and groups of uniqueness type, where a result of Aschbacher implies that there are no simple groups.\nThe general higher rank case consists mostly of the groups of Lie type over fields of characteristic 2 of rank at least 3 or 4.\n\n===Existence and uniqueness of the simple groups===\n\nThe main part of the classification produces a characterization of each simple group. It is then necessary to check that there exists a simple group for each characterization and that it is unique. This gives a large number of separate problems; for example, the original proofs of existence and uniqueness of the [[monster group]] totaled about 200 pages, and the identification of the [[Ree groups]] by Thompson and Bombieri was one of the hardest parts of the classification. Many of the existence proofs and some of the uniqueness proofs for the sporadic groups originally used computer calculations, most of which have since been replaced by shorter hand proofs.\n\n==History of the proof==\n\n===Gorenstein's program===\nIn 1972 {{harvtxt|Gorenstein|1979|loc=Appendix}} announced a program for completing the classification of finite simple groups, consisting of the following 16 steps:\n# Groups of low 2-rank. This was essentially done by Gorenstein and Harada, who classified the groups with sectional 2-rank at most 4. Most of the cases of 2-rank at most 2 had been done by the time Gorenstein announced his program.\n# The semisimplicity of 2-layers. The problem is to prove that the 2-layer of the centralizer of an involution in a simple group is semisimple.\n# Standard form in odd characteristic. If a group has an involution with a 2-component that is a group of Lie type of odd characteristic, the goal is to show that it has a centralizer of involution in \"standard form\" meaning that a centralizer of involution has a component that is of Lie type in odd characteristic and also has a centralizer of 2-rank 1.\n# Classification of groups of odd type. The problem is to show that if a group has a centralizer of involution in \"standard form\" then it is a group of Lie type of odd characteristic. This was solved by Aschbacher's [[classical involution theorem]].\n# Quasi-standard form\n# Central involutions\n# Classification of alternating groups.\n# Some sporadic groups\n# Thin groups. The simple [[thin finite groups]], those with 2-local ''p''-rank at most 1 for odd primes ''p'', were classified by Aschbacher in 1978\n# Groups with a strongly p-embedded subgroup for ''p'' odd\n# The signalizer functor method for odd primes. The main problem is to prove a [[signalizer functor]] theorem for nonsolvable signalizer functors. This was solved by McBride in 1982.\n# Groups of characteristic ''p'' type. This is the problem of groups with a strongly ''p''-embedded 2-local subgroup with ''p'' odd, which was handled by Aschbacher.\n# Quasithin groups. A [[quasithin group]] is one whose 2-local subgroups have ''p''-rank at most 2 for all odd primes ''p'', and the problem is to classify the simple ones of characteristic 2 type. This was completed by Aschbacher and Smith in 2004.\n# Groups of low 2-local 3-rank. This was essentially solved by Aschbacher's [[trichotomy theorem]] for groups with ''e''(''G'')=3. The main change is that 2-local 3-rank is replaced by 2-local ''p''-rank for odd primes.\n# Centralizers of 3-elements in standard form. This was essentially done by the [[Trichotomy theorem]].\n# Classification of simple groups of characteristic 2 type. This was handled by the [[Gilman–Griess theorem]], with 3-elements replaced by ''p''-elements for odd primes.\n\n===Timeline of the proof===\n\nMany of the items in the list below are taken from {{harvtxt|Solomon|2001}}. The date given is usually the publication date of the complete proof of a result, which is sometimes several years later than the proof or first announcement of the result, so some of the items appear in the \"wrong\" order.\n{|\n|-\n!Publication date\n!\n|-\n|1832\n|Galois introduces normal subgroups and finds the simple groups A<sub>''n''</sub> (''n'' ≥ 5) and PSL<sub>2</sub>('''F'''<sub>''p''</sub>) (''p'' ≥ 5)\n|-\n|1854\n|Cayley defines abstract groups\n|-\n|1861\n|Mathieu describes the first two [[Mathieu group]]s M<sub>11</sub>, M<sub>12</sub>, the first sporadic simple groups, and announces the existence of M<sub>24</sub>.\n|-\n|1870\n|Jordan lists some simple groups: the alternating and projective special linear ones, and emphasizes the importance of the simple groups.\n|-\n|1872\n|Sylow proves the [[Sylow theorem]]s\n|-\n|1873\n|Mathieu introduces three more [[Mathieu group]]s M<sub>22</sub>, M<sub>23</sub>, M<sub>24</sub>.\n|-\n|1892\n|Otto Hölder proves that the order of any nonabelian finite simple group must be a product of at least four (not necessarily distinct) primes, and asks for a classification of finite simple groups.\n|-\n|1893\n|Cole classifies simple groups of order up to 660\n|-\n|1896\n|Frobenius and Burnside begin the study of character theory of finite groups.\n|-\n|1899\n|Burnside classifies the simple groups such that the centralizer of every involution is a non-trivial elementary abelian 2-group.\n|-\n|1901\n|Frobenius proves that a [[Frobenius group]] has a Frobenius kernel, so in particular is not simple.\n|-\n|1901\n|Dickson defines classical groups over arbitrary finite fields, and exceptional groups of type ''G''<sub>2</sub> over fields of odd characteristic.\n|-\n|1901\n|Dickson introduces the exceptional finite simple groups of type ''E''<sub>6</sub>.\n|-\n|1904\n|Burnside uses character theory to prove [[Burnside's theorem]] that the order of any non-abelian finite simple group must be divisible by at least 3 distinct primes.\n|-\n|1905\n|Dickson introduces simple groups of type G<sub>2</sub> over fields of even characteristic\n|-\n| 1911\n|Burnside conjectures that every non-abelian finite simple group has even order\n|-\n|1928\n|Philip Hall proves the existence of [[Hall subgroup]]s of solvable groups\n|-\n|1933\n|Hall begins his study of ''p''-groups\n|-\n|1935\n|Brauer begins the study of [[modular character]]s.\n|-\n|1936\n|Zassenhaus classifies finite sharply 3-transitive permutation groups\n|-\n|1938\n|Fitting introduces the [[Fitting subgroup]] and proves Fitting's theorem that for solvable groups the Fitting subgroup contains its centralizer.\n|-\n|1942\n|Brauer describes the modular characters of a group divisible by a prime to the first power.\n|-\n|1954\n|Brauer classifies simple groups with GL<sub>2</sub>('''F'''<sub>''q''</sub>) as the centralizer of an involution.\n|-\n|1955\n|The [[Brauer–Fowler theorem]] implies that the number of finite simple groups with given centralizer of involution is finite, suggesting an attack on the classification using centralizers of involutions.\n|-\n|1955\n|Chevalley introduces the [[Chevalley groups]], in particular introducing exceptional simple groups of types ''F''<sub>4</sub>, ''E''<sub>7</sub>, and ''E''<sub>8</sub>.\n|-\n|1956\n|[[Hall–Higman theorem]]\n|-\n|1957\n| Suzuki shows that all finite simple [[CA group]]s of odd order are cyclic.\n|-\n|1958\n|The [[Brauer–Suzuki–Wall theorem]] characterizes the projective special linear groups of rank 1, and classifies the simple [[CA group]]s.\n|-\n|1959\n|Steinberg introduces the [[Group of Lie type#Steinberg groups|Steinberg group]]s, giving some new finite simple groups, of types <sup>3</sup>''D''<sub>4</sub> and <sup>2</sup>''E''<sub>6</sub> (the latter were independently found at about the same time by [[Jacques Tits]]).\n|-\n|1959\n|The [[Brauer–Suzuki theorem]] about groups with generalized quaternion Sylow 2-subgroups shows in particular that none of them are simple.\n|-\n|1960\n|Thompson proves that a group with a fixed-point-free automorphism of prime order is nilpotent.\n|-\n|1960\n| Feit, Marshall Hall, and Thompson show that all finite simple [[CN group]]s of odd order are cyclic.\n|-\n|1960\n|Suzuki introduces the [[Suzuki groups]], with types <sup>2</sup>''B''<sub>2</sub>.\n|-\n|1961\n|Ree introduces the [[Ree group]]s, with types <sup>2</sup>''F''<sub>4</sub> and  <sup>2</sup>''G''<sub>2</sub>.\n|-\n| 1963\n| Feit and Thompson prove the [[odd order theorem]].\n|-\n|1964\n|Tits introduces BN pairs for groups of Lie type and finds the [[Tits group]]\n|-\n|1965\n|The [[Gorenstein–Walter theorem]] classifies groups with a dihedral Sylow 2-subgroup.\n|-\n|1966\n|Glauberman proves the [[Z* theorem]]\n|-\n|1966\n|Janko introduces the [[Janko group J1]], the first new sporadic group for about a century.\n|-\n|1968\n|Glauberman proves the [[ZJ theorem]]\n|-\n|1968\n|Higman and Sims introduce the [[Higman–Sims group]]\n|-\n|1968\n|Conway introduces the [[Conway groups]]\n|-\n|1969\n|[[Walter's theorem]] classifies groups with abelian Sylow 2-subgroups\n|-\n|1969\n|Introduction of the [[Suzuki sporadic group]], the [[Janko group J2]], the [[Janko group J3]], the [[McLaughlin group (mathematics)|McLaughlin group]], and the [[Held group]].\n|-\n|1969\n|Gorenstein introduces [[signalizer functor]]s based on Thompson's ideas.\n|-\n|1970\n|MacWilliams shows that the 2-groups with no normal abelian subgroup of rank 3 have sectional 2-rank at most 4. (The simple groups with Sylow subgroups satisfying the latter condition were later classified by Gorenstein and Harada.)\n|-\n|1970\n|Bender introduced the [[generalized Fitting subgroup]]\n|-\n|1970\n|The [[Alperin–Brauer–Gorenstein theorem]] classifies groups with quasi-dihedral or wreathed Sylow 2-subgroups, completing the classification of the simple groups of 2-rank at most 2\n|-\n|1971\n|Fischer introduces the three [[Fischer groups]]\n|-\n|1971\n|Thompson classifies [[quadratic pair]]s\n|-\n|1971\n|Bender classifies group with a [[strongly embedded subgroup]]\n|-\n|1972\n|Gorenstein proposes a 16-step program for classifying finite simple groups; the final classification follows his outline quite closely.\n|-\n|1972\n|Lyons introduces the [[Lyons group]]\n|-\n|1973\n|Rudvalis introduces the [[Rudvalis group]]\n|-\n|1973\n|Fischer discovers the [[baby monster group]] (unpublished), which Fischer and Griess use to discover the [[monster group]], which in turn leads Thompson to the [[Thompson sporadic group]] and Norton to the [[Harada–Norton group]] (also found in a different way by Harada).\n|-\n|1974\n|Thompson classifies [[N-group (finite group theory)|N-groups]], groups all of whose local subgroups are solvable.\n|-\n|1974\n|The [[Gorenstein–Harada theorem]] classifies the simple groups of sectional 2-rank at most 4, dividing the remaining finite simple groups into those of component type and those of characteristic 2 type.\n|-\n|1974\n|Tits shows that groups with [[BN pair]]s of rank at least 3 are groups of Lie type\n|-\n|1974\n|Aschbacher classifies the groups with a proper [[2-generated core]]\n|-\n|1975\n|Gorenstein and Walter prove the [[L-balance theorem]]\n|-\n|1976\n|Glauberman proves the solvable [[signalizer functor]] theorem\n|-\n|1976\n|Aschbacher proves the [[component theorem]], showing roughly that groups of odd type satisfying some conditions have a component in standard form. The groups with a component of standard form were classified in a large collection of papers by many authors.\n|-\n|1976\n|O'Nan introduces the [[O'Nan group]]\n|-\n|1976\n|Janko introduces the [[Janko group J4]], the last sporadic group to be discovered\n|-\n|1977\n|Aschbacher characterizes the groups of Lie type of odd characteristic in his [[classical involution theorem]]. After this theorem, which in some sense deals with \"most\" of the simple groups, it was generally felt that the end of the classification was in sight.\n|-\n|1978\n|Timmesfeld proves the O<sub>2</sub> extraspecial theorem, breaking the classification of [[groups of GF(2)-type]] into several smaller problems.\n|-\n|1978\n|Aschbacher classifies the [[thin finite group]]s, which are mostly rank 1 groups of Lie type over fields of even characteristic.\n|-\n|1981\n|Bombieri uses elimination theory to complete Thompson's work on the characterization of [[Ree group]]s, one of the hardest steps of the classification.\n|-\n|1982\n|McBride proves the [[signalizer functor theorem]] for all finite groups.\n|-\n|1982\n|Griess constructs the [[monster group]] by hand\n|-\n|1983\n| The [[Gilman–Griess theorem]] classifies groups of characteristic 2 type and rank at least 4 with standard components, one of the three cases of the trichotomy theorem.\n|-\n|1983\n|Aschbacher proves that no finite group satisfies the hypothesis of the [[uniqueness case]], one of the three cases given by the trichotomy theorem for groups of characteristic 2 type.\n|-\n|1983\n|Gorenstein and Lyons prove the [[trichotomy theorem]] for groups of characteristic 2 type and rank at least 4, while Aschbacher does the case of rank 3. This divides these groups into 3 subcases: the uniqueness case, groups of GF(2) type, and groups with a standard component.\n|-\n| 1983\n| Gorenstein announces the proof of the classification is complete, somewhat prematurely as the proof of the quasithin case was incomplete.\n|-\n|1994\n| Gorenstein, Lyons, and Solomon begin publication of the revised classification\n|-\n|2004\n|Aschbacher and Smith publish their work on [[quasithin group]]s (which are mostly groups of Lie type of rank at most 2 over fields of even characteristic), filling the last gap in the classification known at that time.\n|-\n|2008\n|Harada and Solomon fill a minor gap in the classification by describing groups with a standard component that is a cover of the [[Mathieu group M22]], a case that was accidentally omitted from the proof of the classification due to an error in the calculation of the Schur multiplier of M22.\n|-\n|2012\n|[[Georges Gonthier]] and collaborators announce a computer-checked version of the [[Feit–Thompson theorem]] using the [[Coq]] [[proof assistant]].<ref>{{cite web|url=http://www.msr-inria.fr/news/feit-thomson-proved-in-coq/ |title=Feit–Thompson theorem has been totally checked in Coq |publisher=Msr-inria.inria.fr |date=2012-09-20 |accessdate=2012-09-25}}</ref>\n|}\n\n==Second-generation classification==\nThe proof of the theorem, as it stood around 1985 or so, can be called ''first generation''. Because of the extreme length of the first generation proof, much effort has been devoted to finding a simpler proof, called a '''second-generation classification proof'''. This effort, called \"revisionism\", was originally led by [[Daniel Gorenstein]].\n\n{{As of|2019}}, eight volumes of the second generation proof have been published (Gorenstein, Lyons & Solomon 1994, 1996, 1998, 1999, 2002, 2005, 2018a, 2018b). In 2012 Solomon estimated that the project would need another 5 volumes, but said that progress on them was slow. It is estimated that the new proof will eventually fill approximately 5,000 pages. (This length stems in part from the second generation proof being written in a more relaxed style.) Aschbacher and Smith wrote their two volumes devoted to the quasithin case in such a way that those volumes can be part of the second generation proof.\n\nGorenstein and his collaborators have given several reasons why a simpler proof is possible.\n* The most important is that the correct, final statement of the theorem is now known. Simpler techniques can be applied that are known to be adequate for the types of groups we know to be finite simple. In contrast, those who worked on the first generation proof did not know how many sporadic groups there were, and in fact some of the sporadic groups (e.g., the [[Janko group]]s) were discovered while proving other cases of the classification theorem. As a result, many of the pieces of the theorem were proved using techniques that were overly general.\n*Because the conclusion was unknown, the first generation proof consists of many stand-alone theorems, dealing with important special cases. Much of the work of proving these theorems was devoted to the analysis of numerous special cases. Given a larger, orchestrated proof, dealing with many of these special cases can be postponed until the most powerful assumptions can be applied. The price paid under this revised strategy is that these first generation theorems no longer have comparatively short proofs, but instead rely on the complete classification.\n*Many first generation theorems overlap, and so divide the possible cases in inefficient ways. As a result, families and subfamilies of finite simple groups were identified multiple times. The revised proof eliminates these redundancies by relying on a different subdivision of cases.\n*Finite group theorists have more experience at this sort of exercise, and have new techniques at their disposal.\n\n{{harvtxt|Aschbacher|2004}} has called the work on the classification problem by Ulrich Meierfrankenfeld, Bernd Stellmacher, Gernot Stroth, and a few others, a '''third generation program'''. One goal of this is to treat all groups in characteristic 2 uniformly using the amalgam method.\n\n===Why is the proof so long?===\n\nGorenstein has discussed some of the reasons why there might not be a short proof of the classification similar to the classification of [[compact Lie group]]s.\n\n*The most obvious reason is that the list of simple groups is quite complicated: with 26 sporadic groups there are likely to be many special cases that have to be considered in any proof. So far no one has yet found a clean uniform description of the finite simple groups similar to the parameterization of the compact Lie groups by [[Dynkin diagram]]s.\n*Atiyah and others have suggested that the classification ought to be simplified by constructing some geometric object that the groups act on and then classifying these geometric structures. The problem is that no one has been able to suggest an easy way to find such a geometric structure associated to a simple group. In some sense the classification does work by finding geometric structures such as [[BN-pair]]s, but this only comes at the end of a very long and difficult analysis of the structure of a finite simple group.\n*Another suggestion for simplifying the proof is to make greater use of [[representation theory]]. The problem here is that representation theory seems to require very tight control over the subgroups of a group in order to work well. For groups of small rank one has such control and representation theory works very well, but for groups of larger rank no-one has succeeded in using it to simplify the classification. In the early days of the classification there was considerable effort made to use representation theory, but this never achieved much success in the higher rank case.\n\n==Consequences of the classification==\n\nThis section lists some results that have been proved using the classification of finite simple groups.\n\n*The [[Schreier conjecture]]\n*The [[Signalizer functor theorem]]\n*The [[B conjecture]]\n*The [[Schur–Zassenhaus theorem]] for all groups (though this only uses the [[Feit–Thompson theorem]]).\n*A transitive permutation group on a finite set with more than 1 element has a fixed-point-free element of prime power order.\n*The classification of [[multiple transitivity|2-transitive permutation groups]].\n*The classification of [[rank 3 permutation group]]s.\n*The [[Sims conjecture]]<ref>{{cite journal|last=Cameron |first1=P. J. |last2=Praeger |first2=C. E. |last3=Saxl |first3=J. |last4=Seitz |first4=G. M. |authorlink1=Peter Cameron (mathematician) |authorlink2=Cheryl Praeger |authorlink4=Gary Seitz |title=On the Sims conjecture and distance transitive graphs |journal=[[Bull. London Math. Soc.]] |volume=15 |year=1983 |pages=499–506 |doi=10.1112/blms/15.5.499}}</ref>\n*[[Frobenius's theorem (group theory)|Frobenius's conjecture]] on the number of solutions of {{nowrap|1=''x''<sup>''n''</sup> = 1}}.\n\n==See also==\n\n*[[O'Nan–Scott theorem]]\n\n==References==\n{{reflist}}\n* {{cite news | first = Michael | last = Aschbacher | authorlink = Michael Aschbacher | year = 2004 | url = http://www.ams.org/notices/200407/fea-aschbacher.pdf | title = The Status of the Classification of the Finite Simple Groups | journal = [[Notices of the American Mathematical Society]] | volume = 51 | issue = 7 | pages = 736–740 | ref=harv}}\n*{{Citation | last1=Aschbacher | first1=Michael | author1-link=Michael Aschbacher | last2=Lyons | first2=Richard | last3=Smith | first3=Stephen D. | last4=Solomon | first4=Ronald | title=The Classification of Finite Simple Groups: Groups of Characteristic 2 Type | url=http://www.ams.org/bookstore?fn=20&ikey=SURV-172 | series=Mathematical Surveys and Monographs | isbn=978-0-8218-5336-8 | year=2011 | volume=172}}\n* {{Citation |last1=Conway |first1=John Horton |authorlink1=John Horton Conway |last2=Curtis |first2=Robert Turner |last3=Norton |first3=Simon Phillips |authorlink3=Simon P. Norton |last4=Parker |first4=Richard A |authorlink4=Richard A. Parker |last5=Wilson |first5=Robert Arnott |authorlink5=Robert Arnott Wilson |title=Atlas of Finite Groups: Maximal Subgroups and Ordinary Characters for Simple Groups |year=1985 |month= |origyear= |publisher=Oxford University Press |isbn=978-0-19-853199-9 |title-link=ATLAS of Finite Groups }}\n*{{Citation | last1=Gorenstein | first1=D. | author1-link=Daniel Gorenstein | title=The classification of finite simple groups. I. Simple groups and local analysis | doi=10.1090/S0273-0979-1979-14551-8 | year=1979 | journal=American Mathematical Society. Bulletin. New Series | issn=0002-9904 | volume=1 | issue=1 | pages=43–199 | mr=513750}}\n*{{Citation | last1=Gorenstein | first1=D. | author1-link=Daniel Gorenstein | title=Finite simple groups | publisher=Plenum Publishing Corp. | location=New York | series=University Series in Mathematics | isbn=978-0-306-40779-6 | year=1982 | mr=698782}}\n*{{Citation | last1=Gorenstein | first1=D. | author1-link=Daniel Gorenstein | title=The classification of finite simple groups. Vol. 1. Groups of noncharacteristic 2 type | publisher=Plenum Press | series=The University Series in Mathematics | isbn=978-0-306-41305-6 | year=1983 | mr=746470}}\n* [[Daniel Gorenstein]] (1985), \"The Enormous Theorem\", ''Scientific American'', December 1, 1985, vol. 253, no. 6, pp.&nbsp;104–115.\n*{{Citation | last1=Gorenstein | first1=D. | author1-link=Daniel Gorenstein | title=Classifying the finite simple groups | doi=10.1090/S0273-0979-1986-15392-9 | year=1986 | journal=American Mathematical Society. Bulletin. New Series | issn=0002-9904 | volume=14 | issue=1 | pages=1–98 | mr=818060}}\n*{{Citation | last1=Gorenstein | first1=D. | author1-link=Daniel Gorenstein | last2=Lyons | first2=Richard | last3=Solomon | first3=Ronald | title=The classification of the finite simple groups | url=http://www.ams.org/online_bks/surv401 | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Mathematical Surveys and Monographs | isbn=978-0-8218-0334-9 | year=1994 | volume=40 | mr=1303592}}\n*{{Citation | last1=Gorenstein | first1=D. | author1-link=Daniel Gorenstein | last2=Lyons | first2=Richard | last3=Solomon | first3=Ronald | title=The classification of the finite simple groups. Number 2. Part I. Chapter G | url=http://www.ams.org/online_bks/surv402 | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Mathematical Surveys and Monographs | isbn=978-0-8218-0390-5 | year=1996 | volume=40 | mr=1358135}}\n*{{Citation | last1=Gorenstein | first1=D. | author1-link=Daniel Gorenstein | last2=Lyons | first2=Richard | last3=Solomon | first3=Ronald | title=The classification of the finite simple groups. Number 3. Part I. Chapter A | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Mathematical Surveys and Monographs | isbn=978-0-8218-0391-2 | year=1998 | volume=40 | mr=1490581}}\n*{{Citation | last1=Gorenstein | first1=D. | author1-link=Daniel Gorenstein | last2=Lyons | first2=Richard | last3=Solomon | first3=Ronald | title=The classification of the finite simple groups. Number 4. Part II, Chapters 1-4: Uniqueness Theorems | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Mathematical Surveys and Monographs | isbn=978-0-8218-1379-9 | year=1999 | volume=40 | mr=1675976}}\n*{{Citation | last1=Gorenstein | first1=D. | author1-link=Daniel Gorenstein | last2=Lyons | first2=Richard | last3=Solomon | first3=Ronald | title=The classification of the finite simple groups. Number 5. Part III. Chapters 1–6 | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Mathematical Surveys and Monographs | isbn=978-0-8218-2776-5 | year=2002 | volume=40 | mr=1923000}}\n*{{Citation | last1=Gorenstein | first1=D. | author1-link=Daniel Gorenstein | last2=Lyons | first2=Richard | last3=Solomon | first3=Ronald | title=The classification of the finite simple groups. Number 6. Part IV: The Special Odd Case | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Mathematical Surveys and Monographs | isbn=978-0-8218-2777-2 | year=2005 | volume=40 | mr=2104668}}\n*{{Citation | last1=Gorenstein | first1=D. | author1-link=Daniel Gorenstein | last2=Lyons | first2=Richard | last3=Solomon | first3=Ronald | title=The classification of the finite simple groups. Number 7. Part III, Chapters 7–11: The Generic Case, Stages 3b and 4a | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Mathematical Surveys and Monographs | isbn=978-0-8218-4069-6 | year=2018 | volume=40 | mr=3752626}}\n*{{Citation | last1=Gorenstein | first1=D. | author1-link=Daniel Gorenstein | last2=Lyons | first2=Richard | last3=Solomon | first3=Ronald | title=The Classification of the Finite Simple Groups, Number 8: Part III, Chapters 12–17: The Generic Case, Completed | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Mathematical Surveys and Monographs | isbn=978-1-4704-4189-0 | year=2018 | volume=40}}\n* [[Mark Ronan]], ''Symmetry and the Monster'', {{ISBN|978-0-19-280723-6}}, Oxford University Press, 2006. (Concise introduction for lay reader)\n*[[Marcus du Sautoy]], ''Finding Moonshine'', Fourth Estate, 2008, {{ISBN|978-0-00-721461-7}} (another introduction for the lay reader)\n* [[Ronald Solomon|Ron Solomon]] (1995) \"[http://www.ams.org/notices/199502/solomon.pdf On Finite Simple Groups and their Classification,]\" ''Notices of the American Mathematical Society''. (Not too technical and good on history)\n* {{Citation | last1=Solomon | first1=Ronald | title=A brief history of the classification of the finite simple groups | url=http://www.ams.org/bull/2001-38-03/S0273-0979-01-00909-0/S0273-0979-01-00909-0.pdf | doi=10.1090/S0273-0979-01-00909-0 | year=2001 | journal=American Mathematical Society. Bulletin. New Series | issn=0002-9904 | volume=38 | issue=3 | pages=315–352 | mr=1824893}} – article won [http://www.ams.org/notices/200604/comm-conant.pdf Levi L. Conant prize] for exposition\n*{{Citation | last1=Thompson | first1=John G. | author1-link=John G. Thompson | editor1-last=Gruenberg | editor1-first=K. W. | editor2-last=Roseblade | editor2-first=J. E. | title=Group theory. Essays for Philip Hall | publisher=[[Academic Press]] | location=Boston, MA | isbn=978-0-12-304880-6 | year=1984 | chapter=Finite nonsolvable groups | pages=1–12 | mr=780566}}\n*{{Citation | last1=Wilson | first1=Robert A. | authorlink = Robert Arnott Wilson | title=The finite simple groups | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=[[Graduate Texts in Mathematics]] 251 | isbn=978-1-84800-987-5 | doi=10.1007/978-1-84800-988-2 | year=2009 | zbl=1203.20012 | volume=251 }}\n\n==External links==\n* ''[http://brauer.maths.qmul.ac.uk/Atlas/v3/ ATLAS of Finite Group Representations.]'' Searchable database of [[Group representation|representations]] and other data for many finite simple groups.\n* Elwes, Richard, \"[http://plus.maths.org/issue41/features/elwes/index.html An enormous theorem: the classification of finite simple groups,]\" ''Plus Magazine'', Issue 41, December 2006. For laypeople.\n* Madore, David (2003) ''[http://www.eleves.ens.fr:8080/home/madore/math/simplegroups.html Orders of nonabelian simple groups.]'' Includes a list of all nonabelian simple groups up to order 10<sup>10</sup>.\n* http://mathoverflow.net/questions/180355/in-what-sense-is-the-classification-of-all-finite-groups-impossible\n\n{{DEFAULTSORT:Classification Of Finite Simple Groups}}\n[[Category:Group theory]]\n[[Category:Sporadic groups|*]]\n[[Category:Finite groups]]\n[[Category:Theorems in algebra]]\n[[Category:2004 in science]]\n[[Category:History of mathematics]]\n[[Category:Classification systems]]"
    },
    {
      "title": "Cohn's irreducibility criterion",
      "url": "https://en.wikipedia.org/wiki/Cohn%27s_irreducibility_criterion",
      "text": "'''Arthur Cohn's irreducibility criterion''' is a sufficient condition for a [[polynomial]] to be [[irreducible polynomial|irreducible]] in [[polynomial ring|<math>\\mathbb{Z}[x]</math>]]—that is, for it to be unfactorable into the product of lower-degree polynomials with integer coefficients.\n\nThe criterion is often stated as follows:\n:If a [[prime number]] <math>p</math> is expressed in [[base (exponentiation)|base]] 10 as <math>p=a_m10^m+a_{m-1}10^{m-1}+\\cdots+a_110+a_0</math> (where <math>0\\leq a_i\\leq 9</math>) then the polynomial\n::<math>f(x)=a_mx^m+a_{m-1}x^{m-1}+\\cdots+a_1x+a_0</math>\n:is irreducible in <math>\\mathbb{Z}[x]</math>.\n\nThe theorem can be generalized to other bases as follows:\n:Assume that <math>b \\ge 2</math> is a natural number and <math>p(x)=a_kx^k+a_{k-1}x^{k-1}+\\cdots+a_1x+a_0</math> is a polynomial such that <math>0\\leq a_i\\leq b-1</math>. If <math>p(b)</math> is a prime number then <math>p(x)</math> is irreducible in <math>\\mathbb{Z}[x]</math>.\n\nThe base-10 version of the theorem is attributed to Cohn by [[George Pólya|Pólya]] and [[Gábor Szegő|Szegő]] in one of their books<ref name=Polya>{{cite book |last1=Pólya |first1=George |last2=Szegő  |first2=Gábor |title=Aufgaben und Lehrsätze aus der Analysis, Bd 2 |year=1925 |publisher=Springer, Berlin |oclc=73165700 }} English translation in: {{cite book |last1=Pólya |first1=George |last2=Szegő  |first2=Gábor |title=Problems and theorems in analysis, volume 2 |publisher=Springer |year=2004 |volume=2 |isbn=978-3-540-63686-1 |page=137}}</ref> while the generalization to any base''b'' is due to Brillhart, [[Michael Filaseta|Filaseta]], and [[Andrew Odlyzko|Odlyzko]].<ref name=Brillhart>{{cite journal |last1=Brillhart |first1=John |authorlink1=John Brillhart |last2=Filaseta |first2=Michael |last3=Odlyzko |first3=Andrew |authorlink3=Andrew Odlyzko |title=On an irreducibility theorem of A. Cohn | journal = Canadian Journal of Mathematics | year = 1981 | volume = 33 | issue = 5 | pages = 1055–1059 | doi = 10.4153/CJM-1981-080-0 }}</ref>\n\nIn 2002, [[Ram Murty]] gave a simplified proof as well as some history of the theorem in a paper that is available online.<ref>{{cite journal | last = Murty | first = Ram | title = Prime Numbers and Irreducible Polynomials | journal = [[American Mathematical Monthly]] | year = 2002 | volume = 109 | issue = 5 | pages = 452–458 | url = http://www.mast.queensu.ca/~murty/murty.pdf | doi = 10.2307/2695645 | jstor = 2695645 | citeseerx = 10.1.1.225.8606 }} (dvi file)</ref>\n\nThe converse of this criterion is that, if ''p'' is an irreducible polynomial with integer coefficients that have greatest common divisor 1, then there exists a base such that the coefficients of ''p'' form the representation of a prime number in that base; this is the [[Bunyakovsky conjecture]] and its truth or falsity remains an open question.\n\n==Historical notes==\n*Polya and Szegő gave their own generalization but it has many side conditions (on the locations of the roots, for instance){{Citation needed|date=September 2007}} so it lacks the elegance of Brillhart's, Filaseta's, and Odlyzko's generalization.\n*It is clear from context that the \"A. Cohn\" mentioned by Polya and Szegő is Arthur Cohn (1894–1940), a student of [[Issai Schur]] who was awarded his doctorate from [[Frederick William University]] in 1921.<ref>[http://genealogy.math.ndsu.nodak.edu/html/id.phtml?id=17963 Arthur Cohn's entry at the Mathematics Genealogy Project]</ref><ref>{{cite book|last1=Siegmund-Schultze|first1=Reinhard|title=Mathematicians Fleeing from Nazi Germany: Individual Fates and Global Impact|date=2009|publisher=Princeton University Press|location=Princeton, N.J.|isbn=9781400831401|page=346}}<!--|accessdate=9 March 2015--></ref>\n\n==See also==\n\n*[[Eisenstein's criterion]]\n\n==References==\n<references/>\n\n==External links==\n*{{planetmath reference|id=6194|title=A. Cohn's irreducibility criterion}}\n\n[[Category:Polynomials]]\n[[Category:Theorems in algebra]]"
    },
    {
      "title": "Complex conjugate root theorem",
      "url": "https://en.wikipedia.org/wiki/Complex_conjugate_root_theorem",
      "text": "In [[mathematics]], the '''complex conjugate root theorem''' states that if ''P'' is a [[polynomial]] in one variable with [[Real numbers|real]] [[coefficients]], and ''a''&nbsp;+&nbsp;''bi'' is a [[root of a function|root]] of ''P'' with ''a'' and ''b'' real numbers, then its [[complex conjugate]] ''a''&nbsp;&minus;&nbsp;''bi'' is also a root of ''P''.<ref>{{cite book|title=Maynooth Mathematical Olympiad Manual|author=Anthony G. O'Farell and Gary McGuire|pages=104|chapter=Complex numbers, 8.4.2 Complex roots of real polynomials|year=2002|publisher=Logic Press |isbn=0954426908}} Preview available at [https://books.google.com/books?q=Maynooth+Mathematical+Olympiad+Manual&ots=xQ0hpAQkpc&sa=X&oi=print&ct=title Google books]</ref>\n\nIt follows from this (and the [[fundamental theorem of algebra]]), that if the degree of a real polynomial is odd, it must have at least one real root.<ref name=Jeffrey>{{cite book|title=Complex Analysis and Applications|author=Alan Jeffrey|chapter=Analytic Functions|pages=22&ndash;23|year=2005|publisher=CRC Press|isbn=158488553X}}</ref>  That fact can also be proven by using the [[intermediate value theorem]].\n\n== Examples and consequences ==\n* The polynomial ''x''<sup>2</sup>&nbsp;+&nbsp;1&nbsp;=&nbsp;0 has roots ±''i''.\n* Any real square [[matrix(mathematics)|matrix]] of odd degree has at least one real [[eigenvalue]]. For example, if the matrix is [[orthogonal matrix|orthogonal]], then 1 or &minus;1 is an eigenvalue.\n* The polynomial\n::<math> x^3 - 7x^2 + 41x - 87</math>\n:has roots\n::<math> 3,\\, 2 + 5i,\\, 2 - 5i,</math>\n:and thus can be factored as\n::<math> (x - 3)(x - 2 - 5i)(x - 2 + 5i).</math>\n:In computing the product of the last two factors, the imaginary parts cancel, and we get\n::<math> (x - 3)(x^2 - 4x + 29).</math>\n:The non-real factors come in pairs which when multiplied give quadratic polynomials with real coefficients. Since every polynomial with complex coefficients can be factored into 1st-degree factors (that is one way of stating the [[fundamental theorem of algebra]]), it follows that every polynomial with real coefficients can be factored into factors of degree no higher than 2: just 1st-degree and quadratic factors.\n* If the roots are {{math|''a+bi''}} and {{math|''a-bi''}}, they form a quadratic\n::<math>x^2 - 2ax + (a^2 + b^2)</math>.\nIf the third root is {{math|''c''}}, this becomes\n::<math>(x^2 - 2ax + (a^2 + b^2))(x-c)</math>\n::<math>=x^3 + x^2(-2a-c) + x(2ac+a^2+b^2) - c(a^2 + b^2)</math>.\n\n=== Corollary on odd-degree polynomials ===\nIt follows from the present theorem and the [[fundamental theorem of algebra]] that if the degree of a real polynomial is odd, it must have at least one real root.<ref name=\"Jeffrey\"/>\n\nThis can be proved as follows.\n*Since non-real complex roots come in conjugate pairs, there are an even number of them;\n*But a polynomial of odd degree has an odd number of roots;\n*Therefore some of them must be real.\n \nThis requires some care in the presence of [[multiple root]]s; but a complex root and its conjugate do have the same [[Multiplicity (mathematics)|multiplicity]] (and this [[lemma (mathematics)|lemma]] is not hard to prove). It can also be worked around by considering only [[irreducible polynomial]]s; any real polynomial of odd degree must have an irreducible factor of odd degree, which (having no multiple roots) must have a real root by the reasoning above.\n\nThis corollary can also be proved directly by using the [[intermediate value theorem]].\n\n== Proof ==\nOne proof of the theorem is as follows:<ref name=Jeffrey />\n\nConsider the polynomial\n\n: <math>P(z) = a_0 + a_1z + a_2z^2 + \\cdots + a_nz^n</math>\n\nwhere all ''a''<sub>''r''</sub> are real.  Suppose some complex number ''&zeta;'' is a root of ''P'', that is ''P''(''&zeta;'')&nbsp;=&nbsp;0. It needs to be shown that \n: <math>P(\\overline{\\zeta}) = 0</math>\n\nas well.\n\nIf ''P''(''&zeta;'')&nbsp;=&nbsp;0, then\n\n: <math>a_0 + a_1\\zeta + a_2\\zeta^2 + \\cdots + a_n\\zeta^n = 0</math>\n\nwhich can be put as\n\n: <math>\\sum_{r=0}^n a_r\\zeta^r = 0.</math>\n\nNow\n: <math>P(\\overline{\\zeta}) = \\sum_{r=0}^n a_r\\left(\\overline{\\zeta}\\right)^r</math>\n\nand given the [[Complex conjugation#Properties|properties of complex conjugation]],\n\n: <math>\\sum_{r=0}^n a_r\\left(\\overline{\\zeta}\\right)^r = \\sum_{r=0}^n a_r \\overline{\\zeta^r} = \\sum_{r=0}^n \\overline{a_r\\zeta^r} = \\overline {\\sum_{r=0}^n a_r\\zeta^r}.</math>\n\nSince,\n: <math>\\overline {\\sum_{r=0}^n a_r\\zeta^r} = \\overline{0}</math>\n\nit follows that\n\n: <math>\\sum_{r=0}^n a_r\\left(\\overline{\\zeta}\\right)^r = \\overline{0} = 0.</math>\n\nThat is,\n\n: <math>P(\\overline{\\zeta}) = a_0 + a_1\\overline{\\zeta} + a_2\\left(\\overline{\\zeta}\\right)^2 + \\cdots + a_n\\left(\\overline{\\zeta}\\right)^n = 0.</math>\n\nNote that this works only because the ''a''<sub>''r''</sub> are real, that is, <math>\\overline {a_r} = a_r </math>. If any of the coefficients were nonreal, the roots would not necessarily come in conjugate pairs.\n\n== Notes ==\n{{Reflist}}\n\n[[Category:Theorems in complex analysis]]\n[[Category:Polynomials]]\n[[Category:Theorems in algebra]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "Cramer's rule",
      "url": "https://en.wikipedia.org/wiki/Cramer%27s_rule",
      "text": "{{Use American English|date = January 2019}}\n{{Short description|Formula for solving systems of linear equations}}\nIn [[linear algebra]], '''Cramer's rule''' is an explicit formula for the solution of a [[system of linear equations]] with as many equations as unknowns, valid whenever the system has a unique solution. It expresses the solution in terms of the [[determinant]]s of the (square) coefficient [[Matrix (mathematics)|matrix]] and of matrices obtained from it by replacing one column by the column vector of right-hand-sides of the equations. It is named after [[Gabriel Cramer]] (1704&ndash;1752), who published the rule for an arbitrary number of unknowns in 1750,<ref>{{cite web \n  | title = Introduction à l'Analyse des lignes Courbes algébriques\n  | author = Cramer, Gabriel\n  | year = 1750\n  | location = Geneva\n  | language = French\n  | url = https://www.europeana.eu/resolve/record/03486/E71FE3799CEC1F8E2B76962513829D2E36B63015\n  | accessdate = 2012-05-18\n  | publisher = Europeana\n  | pages = 656–659\n}}</ref><ref>\n{{Cite journal\n | last = Kosinski\n | first = A. A.\n | title = Cramer's Rule is due to Cramer\n | journal = Mathematics Magazine\n | volume = 74\n | pages = 310–312\n | year = 2001\n | doi = 10.2307/2691101\n}}</ref> although [[Colin Maclaurin]] also published special cases of the rule in 1748<ref>{{Cite book\n  | last = MacLaurin\n  | first = Colin\n  | title = A Treatise of Algebra, in Three Parts.\n  | url = https://archive.org/details/atreatisealgebr03maclgoog\n  | year = 1748\n}}</ref> (and possibly knew of it as early as 1729).<ref>\n{{Cite book\n  | last = Boyer\n  | first = Carl B.\n  | authorlink = Carl Benjamin Boyer\n  | title = A History of Mathematics\n  | edition = 2nd\n  | publisher = Wiley\n  | year = 1968\n  | pages = 431\n}}\n</ref><ref>\n{{cite book\n | last = Katz\n | first = Victor\n | title = A History of Mathematics\n | publisher = Pearson Education\n | edition = Brief\n | year = 2004\n | pages = 378–379\n}}\n</ref><ref>\n{{Cite journal\n | last = Hedman\n | first = Bruce A.\n | title = An Earlier Date for \"Cramer's Rule\"\n | journal = Historia Mathematica\n | volume = 26\n | issue =4\n | pages = 365–368\n | year = 1999\n | url = http://professorhedman.com/Cramers.Rule.pdf\n | doi = 10.1006/hmat.1999.2247\n \n }}\n</ref>\n\nCramer's rule implemented in a naïve way is computationally inefficient for systems of more than two or three equations.<ref name=\"Poole2014\">{{cite book|author=David Poole|title=Linear Algebra: A Modern Introduction|year=2014|publisher=Cengage Learning|isbn=978-1-285-98283-0|page=276}}</ref> In the case of {{mvar|n}} equations in {{mvar|n}} unknowns, it requires computation of {{math|''n'' + 1}} determinants, while [[Gaussian elimination]] produces the result with the same [[computational complexity]] as the computation of a single determinant.<ref name=\"HoffmanFrankel2001\">{{cite book|author1=Joe D. Hoffman|author2=Steven Frankel|title=Numerical Methods for Engineers and Scientists, Second Edition,|year=2001|publisher=CRC Press|isbn=978-0-8247-0443-8|page=30}}</ref><ref name=\"Shores2007\">{{cite book|author=Thomas S. Shores|title=Applied Linear Algebra and Matrix Analysis|year=2007|publisher=Springer Science & Business Media|isbn=978-0-387-48947-6|page=132}}</ref>{{check|date=March 2018}} Cramer's rule can also be [[numerically unstable]] even for 2×2 systems.<ref name=\"Higham2002\">{{cite book|author=Nicholas J. Higham|title=Accuracy and Stability of Numerical Algorithms: Second Edition|year=2002|publisher=SIAM|isbn=978-0-89871-521-7|page=13}}</ref>  However, it has recently been shown that Cramer's rule can be implemented in O(''n''<sup>3</sup>) time,<ref>{{cite journal |author1=Ken Habgood |author2=Itamar Arel |title=A condensation-based application of Cramerʼs rule for solving large-scale linear systems |journal=Journal of Discrete Algorithms |volume=10 |year=2012 |pages=98–109 |doi=10.1016/j.jda.2011.06.007|url=http://web.eecs.utk.edu/~itamar/Papers/JDA2011.pdf}}</ref> which is comparable to more common methods of solving systems of linear equations, such as [[Gaussian elimination]] (consistently requiring 2.5 times as many arithmetic operations for all matrix sizes), while exhibiting comparable numeric stability in most cases.\n\n==General case==\nConsider a system of {{mvar|n}} linear equations for {{mvar|n}} unknowns, represented in matrix multiplication form as follows:\n\n:<math> Ax = b</math>\n\nwhere the {{math|''n'' × ''n''}} matrix {{mvar|A}} has a nonzero determinant, and the vector <math> x = (x_1, \\ldots, x_n)^\\mathrm{T} </math> is the column vector of the variables. Then the theorem states that in this case the system has a unique solution, whose individual values for the unknowns are given by:\n\n:<math> x_i = \\frac{\\det(A_i)}{\\det(A)} \\qquad i = 1, \\ldots, n</math>\n\nwhere <math> A_i </math> is the matrix formed by replacing the {{mvar|i}}-th column of {{mvar|A}} by the column vector {{mvar|b}}.\n\nA more general version of Cramer's rule<ref>{{cite journal |author1=Zhiming Gong |author2=M. Aldeen |author3=L. Elsner |title=A note on a generalized Cramer’s rule |journal=Linear Algebra and its Applications |volume=340 |year=2002 |pages=253–254 |doi=10.1016/S0024-3795(01)00469-4}}</ref> considers the matrix equation\n\n:<math> AX = B</math>\n\nwhere the {{math|''n'' × ''n''}} matrix {{mvar|A}} has a nonzero determinant, and {{mvar|X}}, {{mvar|B}} are {{math|''n'' × ''m''}} matrices. Given sequences <math> 1 \\leq i_1 < i_2 < \\ldots < i_k \\leq n </math> and <math> 1 \\leq j_1 < j_2 < \\ldots < j_k \\leq m </math>, let <math> X_{I,J} </math> be the {{math|''k'' × ''k''}} submatrix of {{mvar|X}} with rows in <math> I := (i_1, \\ldots, i_k ) </math> and columns in <math> J := (j_1, \\ldots, j_k ) </math>. Let <math> A_{B}(I,J) </math> be the {{math|''n'' × ''n''}} matrix formed by replacing the <math>i_s</math> column of {{mvar|A}} by the <math>j_s</math> column of {{Mvar|B}}, for all <math> s = 1,\\ldots, k </math>. Then\n\n:<math> \\det X_{I,J} = \\frac{\\det(A_{B}(I,J))}{\\det(A)}. </math>\n\nIn the case <math> k = 1 </math>, this reduces to the normal Cramer's rule.\n\nThe rule holds for systems of equations with coefficients and unknowns in any [[field (mathematics)|field]], not just in the [[real number]]s.\n\n==Proof==\nThe proof for Cramer's rule uses just two properties of determinants: linearity with respect to any given column (taking for that column a [[linear combination]] of column vectors produces as determinant the corresponding linear combination of their determinants), and the fact that the determinant is zero whenever two columns are equal (which is implied by the basic property that the sign of the determinant flips if you switch two columns).\n\nFix the index ''j'' of a column. Linearity means that if we consider only column ''j'' as variable (fixing the others arbitrarily), the resulting function {{math|'''R'''<sup>''n''</sup> → '''R'''}} (assuming matrix entries are in {{math|'''R'''}}) can be given by a matrix, with one row and ''n'' columns, that acts on column ''j''. In fact this is precisely what [[Laplace expansion]] does, writing {{math|det(''A'') {{=}} ''C''<sub>1</sub>''a''<sub>1,''j''</sub> + ... + ''C<sub>n</sub>a<sub>n,j</sub>''}} for certain coefficients ''C''<sub>1</sub>, ..., ''C<sub>n</sub>'' that depend on the columns of {{mvar|A}} other than column ''j'' (the precise expression for these [[cofactor (linear algebra)|cofactor]]s is not important here). The value {{math|det(''A'')}} is then the result of applying the one-line matrix {{math|''L''<sub>(''j'')</sub> {{=}} (''C''<sub>1</sub> ''C''<sub>2</sub> ... ''C<sub>n</sub>'')}} to column ''j'' of {{mvar|A}}. If {{math|''L''<sub>(''j'')</sub>}} is applied to any ''other'' column ''k'' of {{mvar|A}}, then the result is the determinant of the matrix obtained from {{mvar|A}} by replacing column ''j'' by a copy of column ''k'', so the resulting determinant is 0 (the case of two equal columns).\n\nNow consider a system of {{mvar|n}} linear equations in {{mvar|n}} unknowns <math>x_1, \\ldots,x_n</math>, whose coefficient matrix is {{mvar|A}}, with det(''A'') assumed to be nonzero:\n\n:<math>\\begin{matrix}a_{11}x_1+a_{12}x_2+\\cdots+a_{1n}x_n&=&b_1\\\\a_{21}x_1+a_{22}x_2+\\cdots+a_{2n}x_n&=&b_2\\\\\\vdots&\\vdots&\\vdots\\\\a_{n1}x_1+a_{n2}x_2+\\cdots+a_{nn}x_n&=&b_n.\\end{matrix}</math>\n\nIf one combines these equations by taking ''C''<sub>1</sub> times the first equation, plus ''C''<sub>2</sub> times the second, and so forth until ''C''<sub>''n''</sub> times the last, then the coefficient of {{mvar|x<sub>j</sub>}} will become {{math|''C''<sub>1</sub>''a''<sub>1, ''j''</sub> + ... + ''C<sub>n</sub>a<sub>n,j</sub>'' {{=}} det(''A'')}}, while the coefficients of all other unknowns become 0; the left hand side becomes simply det(''A'')''x<sub>j</sub>''. The right hand side is {{math|''C''<sub>1</sub>''b''<sub>1</sub> + ... + ''C<sub>n</sub>b<sub>n</sub>''}}, which is {{math|''L''<sub>(''j'')</sub>}} applied to the column vector '''b''' of the right hand side {{mvar|b<sub>i</sub>}}. In fact what has been done here is multiply the matrix equation {{math|''A'''''x''' {{=}} '''b'''}} on the left by {{math|''L''<sub>(''j'')</sub>}}. Dividing by the nonzero number det(''A'') one finds the following equation, necessary to satisfy the system:\n\n:<math>x_j=\\frac{L_{(j)}\\cdot\\mathbf{b}}{\\det(A)}.</math>\n\nBut by construction the numerator is the determinant of the matrix obtained from {{mvar|A}} by replacing column ''j'' by '''b''', so we get the expression of Cramer's rule as a necessary condition for a solution. The same procedure can be repeated for other values of ''j'' to find values for the other unknowns.\n\nThe only point that remains to prove is that these values for the unknowns, the only possible ones, do indeed together form a solution. But if the matrix {{mvar|A}} is invertible with inverse {{math|''A''<sup>−1</sup>}}, then {{math|'''x''' {{=}} ''A''<sup>−1</sup>'''b'''}} will be a solution, thus showing its existence. To see that {{mvar|A}} is invertible when det(''A'') is nonzero, consider the {{math|''n'' × ''n''}} matrix ''M'' obtained by stacking the one-line matrices {{math|''L''<sub>(''j'')</sub>}} on top of each other for ''j'' = 1, ..., ''n'' (this gives the [[adjugate matrix]] for {{mvar|A}}). It was shown that {{math|''L''<sub>(''j'')</sub>''A'' {{=}} (0 ... 0 det(''A'') 0 ... 0)}} where {{math|det(''A'')}} appears at the position ''j''; from this it follows that {{math|''MA'' {{=}} det(''A'')''I<sub>n</sub>''}}. Therefore,\n\n:<math>\\frac1{\\det(A)}M=A^{-1},</math>\n\ncompleting the proof.\n\nFor other proofs, see [[#Other proofs|below]].\n\n==Finding inverse matrix==\n{{main article|Invertible matrix#Methods of matrix inversion}}\nLet {{mvar|A}} be an {{math|''n'' × ''n''}} matrix. Then\n\n:<math>A\\,\\operatorname{adj}A = (\\operatorname{adj}A)\\,A=\\operatorname{det}(A) I</math>\n\nwhere adj(''A'') denotes the [[adjugate matrix]] of {{mvar|A}}, {{math|det(''A'')}} is the determinant, and ''I'' is the [[identity matrix]].  If det(''A'') is invertible in ''R'', then the inverse matrix of {{mvar|A}} is\n\n:<math>A^{-1} = \\frac{1}{\\operatorname{det}(A)} \\operatorname{adj}(A).</math>\n\nIf ''R'' is a [[field (mathematics)|field]] (such as the field of real numbers), then this gives a formula for the inverse of {{mvar|A}}, provided {{math|det(''A'') ≠ 0}}. In fact, this formula will work whenever ''R'' is a [[commutative ring]], provided that det(''A'') is a [[Unit (ring theory)|unit]]. If det(''A'') is not a unit, then {{mvar|A}} is not invertible.\n\n==Applications==\n\n===Explicit formulas for small systems===\nConsider the linear system\n\n:<math>\\left\\{\\begin{matrix}a_1x+b_1y&={\\color{red}c_1}\\\\ a_2x + b_2y&= {\\color{red}c_2}\\end{matrix}\\right.</math>\n\nwhich in matrix format is\n\n:<math>\\begin{bmatrix} a_1 & b_1 \\\\ a_2 & b_2 \\end{bmatrix}\\begin{bmatrix} x \\\\ y \\end{bmatrix}=\\begin{bmatrix} {\\color{red}c_1} \\\\ {\\color{red}c_2} \\end{bmatrix}.</math>\n\nAssume {{math|''a''<sub>1</sub>''b''<sub>2</sub> − ''b''<sub>1</sub>''a''<sub>2</sub>}} nonzero. Then, with help of [[determinant]]s, {{mvar|x}} and {{mvar|y}} can be found with Cramer's rule as\n\n:<math>\\begin{align}\nx &= \\frac{\\begin{vmatrix} {\\color{red}{c_1}} & b_1 \\\\ {\\color{red}{c_2}} & b_2 \\end{vmatrix}}{\\begin{vmatrix} a_1 & b_1 \\\\ a_2 & b_2 \\end{vmatrix}} = { {\\color{red}c_1}b_2 - b_1{\\color{red}c_2} \\over a_1b_2 - b_1a_2}, \\quad y = \\frac{\\begin{vmatrix} a_1 & {\\color{red}{c_1}} \\\\ a_2 & {\\color{red}{c_2}} \\end{vmatrix}}{\\begin{vmatrix} a_1 & b_1 \\\\ a_2 & b_2 \\end{vmatrix}}  = { a_1{\\color{red}c_2} - {\\color{red}c_1}a_2 \\over a_1b_2 - b_1a_2}\n\\end{align}.</math>\n\nThe rules for {{math|3 × 3}} matrices are similar.  Given\n\n:<math>\\left\\{\\begin{matrix}a_1x + b_1y + c_1z&= {\\color{red}d_1}\\\\a_2x + b_2y + c_2z&= {\\color{red}d_2}\\\\a_3x + b_3y + c_3z&= {\\color{red}d_3}\\end{matrix}\\right.</math>\n\nwhich in matrix format is\n\n<math>\\begin{bmatrix} a_1 & b_1 & c_1 \\\\ a_2 & b_2 & c_2 \\\\ a_3 & b_3 & c_3 \\end{bmatrix}\\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}=\\begin{bmatrix} {\\color{red}d_1} \\\\ {\\color{red}d_2} \\\\ {\\color{red}d_3} \\end{bmatrix}.</math>\n\nThen the values of {{mvar|x, y}} and {{mvar|z}} can be found as follows:\n\n:<math>x = \\frac{\\begin{vmatrix} {\\color{red}d_1} & b_1 & c_1 \\\\ {\\color{red}d_2} & b_2 & c_2 \\\\ {\\color{red}d_3} & b_3 & c_3 \\end{vmatrix} } { \\begin{vmatrix} a_1 & b_1 & c_1 \\\\ a_2 & b_2 & c_2 \\\\ a_3 & b_3 & c_3 \\end{vmatrix}}, \\quad y = \\frac {\\begin{vmatrix} a_1 & {\\color{red}d_1} & c_1 \\\\ a_2 & {\\color{red}d_2} & c_2 \\\\ a_3 & {\\color{red}d_3} & c_3 \\end{vmatrix}} {\\begin{vmatrix} a_1 & b_1 & c_1 \\\\ a_2 & b_2 & c_2 \\\\ a_3 & b_3 & c_3 \\end{vmatrix}}, \\text{ and }z = \\frac { \\begin{vmatrix} a_1 & b_1 & {\\color{red}d_1} \\\\ a_2 & b_2 & {\\color{red}d_2} \\\\ a_3 & b_3 & {\\color{red}d_3} \\end{vmatrix}} {\\begin{vmatrix} a_1 & b_1 & c_1 \\\\ a_2 & b_2 & c_2 \\\\ a_3 & b_3 & c_3 \\end{vmatrix} }.</math>\n\n===Differential geometry===\n\n====Ricci calculus====\nCramer's rule is used in the [[Ricci calculus]] in various calculations involving the [[Christoffel symbols]] of the first and second kind.<ref>{{Cite book|title=The Absolute Differential Calculus (Calculus of Tensors)|last=Levi-Civita|first=Tullio|publisher=Dover|year=1926|isbn=9780486634012|location=|pages=111–112|quote=|via=}}</ref>\n\nIn particular, Cramer's rule can be used to prove that the divergence operator on a Riemannian manifold is invariant with respect to change of coordinates. We give a direct proof, suppressing the role of the Christoffel symbols.\nLet <math>(M,g)</math> be a [[Riemannian manifold]] equipped with [[Manifold#Charts|local coordinates]] <math> (x^1, x^2, \\dots, x^n)</math>. Let <math>A=A^i \\frac{\\partial}{\\partial x^i}</math> be a [[vector field]].  We use the [[Einstein notation|summation convention]] throughout.\n\n:'''Theorem'''.\n:''The ''divergence'' of <math>A</math>,  \n:<math> \\operatorname{div} A = \\frac{1}{\\sqrt{\\det g}} \\frac{\\partial}{\\partial x^i} \\left( A^i \\sqrt{\\det g} \\right),</math>\n:is invariant under change of coordinates.''\n\n{{Collapse top|title=''Proof''}}\nLet <math>(x^1,x^2,\\ldots,x^n)\\mapsto (\\bar x^1,\\ldots,\\bar x^n)</math> be a [[coordinate transformation]] with [[invertible matrix|non-singular]] [[Jacobian matrix and determinant|Jacobian]].  Then the classical [[Vector field#Coordinate transformation law|transformation laws]] imply that <math>A=\\bar A^{k}\\frac{\\partial}{\\partial\\bar x^{k}}</math> where <math>\\bar A^{k}=\\frac{\\partial \\bar x^{k}}{\\partial x^{j}}A^{j}</math>.  Similarly, if <math>g=g_{mk}\\,dx^{m}\\otimes dx^{k}=\\bar{g}_{ij}\\,d\\bar x^{i}\\otimes d\\bar x^{j}</math>, then <math>\\bar{g}_{ij}=\\,\\frac{\\partial x^{m}}{\\partial\\bar x^{i}}\\frac{\\partial x^{k}}{\\partial \\bar x^{j}}g_{mk}</math>.  \nWriting this transformation law in terms of matrices yields <math>\\bar g=\\left(\\frac{\\partial x}{\\partial\\bar{x}}\\right)^{\\text{T}}g\\left(\\frac{\\partial x}{\\partial\\bar{x}}\\right)</math>, which implies <math>\\det\\bar g=\\left(\\det\\left(\\frac{\\partial x}{\\partial\\bar{x}}\\right)\\right)^{2}\\det g</math>.\n\nNow one computes\n:<math>\\begin{align}\n\\operatorname{div} A &=\\frac{1}{\\sqrt{\\det g}}\\frac{\\partial}{\\partial x^{i}}\\left( A^{i}\\sqrt{\\det g}\\right)\\\\\n\t&=\\det\\left(\\frac{\\partial x}{\\partial\\bar{x}}\\right)\\frac{1}{\\sqrt{\\det\\bar g}}\\frac{\\partial \\bar x^k}{\\partial x^{i}}\\frac{\\partial}{\\partial\\bar x^{k}}\\left(\\frac{\\partial x^{i}}{\\partial \\bar x^{\\ell}}\\bar{A}^{\\ell}\\det\\!\\left(\\frac{\\partial x}{\\partial\\bar{x}}\\right)^{\\!\\!-1}\\!\\sqrt{\\det\\bar g}\\right).\n\\end{align}</math>\nIn order to show that this equals \n<math>\\frac{1}{\\sqrt{\\det\\bar g}}\\frac{\\partial}{\\partial\\bar x^{k}}\\left(\\bar A^{k}\\sqrt{\\det\\bar{g}}\\right)</math>,\nit is necessary and sufficient to show that \n:<math>\\frac{\\partial\\bar x^{k}}{\\partial x^{i}}\\frac{\\partial}{\\partial\\bar x^{k}}\\left(\\frac{\\partial x^{i}}{\\partial \\bar x^{\\ell}}\\det\\!\\left(\\frac{\\partial x}{\\partial\\bar{x}}\\right)^{\\!\\!\\!-1}\\right)=0\\qquad\\text{for all } \\ell, </math>\nwhich is equivalent to\n:<math>\\frac{\\partial}{\\partial \\bar x^{\\ell}}\\det\\left(\\frac{\\partial x}{\\partial\\bar{x}}\\right)\n=\\det\\left(\\frac{\\partial x}{\\partial\\bar{x}}\\right)\\frac{\\partial\\bar x^{k}}{\\partial x^{i}}\\frac{\\partial^{2}x^{i}}{\\partial\\bar x^{k}\\partial\\bar x^{\\ell}}.\n</math>\nCarrying out the differentiation on the left-hand side, we get:\n:<math>\\begin{align}\n\t\\frac{\\partial}{\\partial\\bar x^{\\ell}}\\det\\left(\\frac{\\partial x}{\\partial\\bar{x}}\\right)\n\t&=(-1)^{i+j}\\frac{\\partial^{2}x^{i}}{\\partial\\bar x^{\\ell}\\partial\\bar x^{j}}\\det M(i|j)\\\\\n\t&=\\frac{\\partial^{2}x^{i}}{\\partial\\bar x^{\\ell}\\partial\\bar x^{j}}\\det\\left(\\frac{\\partial x}{\\partial\\bar{x}}\\right)\\frac{(-1)^{i+j}}{\\det\\left(\\frac{\\partial x}{\\partial\\bar{x}}\\right)}\\det M(i|j)=(\\ast),\n\t\\end{align}</math>\nwhere <math>M(i|j)</math> denotes the matrix obtained from <math>\\left(\\frac{\\partial x}{\\partial\\bar{x}}\\right)</math> by deleting the <math>i</math>th row and <math>j</math>th column.\nBut Cramer's Rule says that \n:<math>\\frac{(-1)^{i+j}}{\\det\\left(\\frac{\\partial x}{\\partial\\bar{x}}\\right)}\\det M(i|j) </math>\nis the <math>(j,i)</math>th entry of the matrix <math>\\left(\\frac{\\partial \\bar{x}}{\\partial x}\\right)</math>.\nThus\n:<math>(\\ast)=\\det\\left(\\frac{\\partial x}{\\partial\\bar{x}}\\right)\\frac{\\partial^{2}x^{i}}{\\partial\\bar x^{\\ell}\\partial\\bar x^{j}}\\frac{\\partial\\bar x^{j}}{\\partial x^{i}},</math>\ncompleting the proof.\n\n{{Collapse bottom}}\n\n====Computing derivatives implicitly====\nConsider the two equations <math>F(x, y, u, v) = 0</math> and <math>G(x, y, u, v) = 0</math>.  When ''u'' and ''v'' are independent variables, we can define <math>x = X(u, v)</math> and <math>y = Y(u, v).</math>\n\nFinding an equation for <math>\\dfrac{\\partial x}{\\partial u}</math> is a trivial application of Cramer's rule.\n\n{{Collapse top|title=''Calculation of <math>\\dfrac{\\partial x}{\\partial u}</math>''}}\nFirst, calculate the first derivatives of ''F'', ''G'', ''x'', and ''y'':\n\n:<math>\\begin{align}\ndF &= \\frac{\\partial F}{\\partial x} dx + \\frac{\\partial F}{\\partial y} dy +\\frac{\\partial F}{\\partial u} du +\\frac{\\partial F}{\\partial v} dv = 0 \\\\[6pt]\ndG &= \\frac{\\partial G}{\\partial x} dx + \\frac{\\partial G}{\\partial y} dy +\\frac{\\partial G}{\\partial u} du +\\frac{\\partial G}{\\partial v} dv = 0 \\\\[6pt]\ndx &= \\frac{\\partial X}{\\partial u} du + \\frac{\\partial X}{\\partial v} dv \\\\[6pt]\ndy &= \\frac{\\partial Y}{\\partial u} du + \\frac{\\partial Y}{\\partial v} dv.\n\\end{align}</math>\n\nSubstituting ''dx'', ''dy'' into ''dF'' and ''dG'', we have:\n\n:<math>\\begin{align}\ndF &= \\left(\\frac{\\partial F}{\\partial x} \\frac{\\partial x}{\\partial u} +\\frac{\\partial F}{\\partial y} \\frac{\\partial y}{\\partial u} + \\frac{\\partial F}{\\partial u} \\right) du + \\left(\\frac{\\partial F}{\\partial x} \\frac{\\partial x}{\\partial v} +\\frac{\\partial F}{\\partial y} \\frac{\\partial y}{\\partial v} +\\frac{\\partial F}{\\partial v} \\right) dv = 0 \\\\ [6pt]\ndG &= \\left(\\frac{\\partial G}{\\partial x} \\frac{\\partial x}{\\partial u} +\\frac{\\partial G}{\\partial y} \\frac{\\partial y}{\\partial u} +\\frac{\\partial G}{\\partial u} \\right) du + \\left(\\frac{\\partial G}{\\partial x} \\frac{\\partial x}{\\partial v} +\\frac{\\partial G}{\\partial y} \\frac{\\partial y}{\\partial v} +\\frac{\\partial G}{\\partial v} \\right) dv = 0.\n\\end{align}</math>\n\nSince ''u'', ''v'' are both independent, the coefficients of ''du'', ''dv'' must be zero.  So we can write out equations for the coefficients:\n\n:<math>\\begin{align}\n\\frac{\\partial F}{\\partial x} \\frac{\\partial x}{\\partial u} +\\frac{\\partial F}{\\partial y} \\frac{\\partial y}{\\partial u} & = -\\frac{\\partial F}{\\partial u} \\\\[6pt]\n\\frac{\\partial G}{\\partial x} \\frac{\\partial x}{\\partial u} +\\frac{\\partial G}{\\partial y} \\frac{\\partial y}{\\partial u} & = -\\frac{\\partial G}{\\partial u} \\\\[6pt]\n\\frac{\\partial F}{\\partial x} \\frac{\\partial x}{\\partial v} +\\frac{\\partial F}{\\partial y} \\frac{\\partial y}{\\partial v} & = -\\frac{\\partial F}{\\partial v} \\\\[6pt]\n\\frac{\\partial G}{\\partial x} \\frac{\\partial x}{\\partial v} +\\frac{\\partial G}{\\partial y} \\frac{\\partial y}{\\partial v} & = -\\frac{\\partial G}{\\partial v}.\n\\end{align}</math>\n\nNow, by Cramer's rule, we see that:\n\n:<math>\\frac{\\partial x}{\\partial u} = \\frac{\\begin{vmatrix} -\\frac{\\partial F}{\\partial u} & \\frac{\\partial F}{\\partial y} \\\\ -\\frac{\\partial G}{\\partial u} & \\frac{\\partial G}{\\partial y}\\end{vmatrix}}{\\begin{vmatrix}\\frac{\\partial F}{\\partial x} & \\frac{\\partial F}{\\partial y} \\\\ \\frac{\\partial G}{\\partial x} & \\frac{\\partial G}{\\partial y}\\end{vmatrix}}.</math>\n\nThis is now a formula in terms of two [[Jacobian matrix and determinant|Jacobian]]s:\n\n:<math>\\frac{\\partial x}{\\partial u} = -\\frac{\\left(\\frac{\\partial (F, G)}{\\partial (u, y)}\\right)}{\\left(\\frac{\\partial (F, G)}{\\partial(x, y)}\\right)}.</math>\n\nSimilar formulas can be derived for <math>\\frac{\\partial x}{\\partial v}, \\frac{\\partial y}{\\partial u}, \\frac{\\partial y}{\\partial v}.</math>\n{{Collapse bottom}}\n\n===Integer programming===\nCramer's rule can be used to prove that an [[integer programming]] problem whose constraint matrix is [[totally unimodular]] and whose right-hand side is integer, has integer basic solutions.  This makes the integer program substantially easier to solve.\n\n===Ordinary differential equations===\nCramer's rule is used to derive the general solution to an inhomogeneous linear differential equation by the method of [[variation of parameters]].\n\n==Geometric interpretation==\n[[File:Cramer.jpg|thumb|400px|Geometric interpretation of Cramer's rule. The areas of the second and third shaded parallelograms are the same and the second is <math>x_1</math> times the first. From this equality Cramer's rule follows.]]\nCramer's rule has a geometric interpretation that can be considered also a proof or simply giving insight about its geometric nature. These geometric arguments work in general and not only in the case of two equations with two unknowns presented here.\n\nGiven the system of equations\n\n:<math>\\begin{matrix}a_{11}x_1+a_{12}x_2&=b_1\\\\a_{21}x_1+a_{22}x_2&=b_2\\end{matrix}</math>\n\nit can be considered as an equation between vectors\n\n:<math>x_1\\binom{a_{11}}{a_{21}}+x_2\\binom{a_{12}}{a_{22}}=\\binom{b_1}{b_2}. </math>\n\nThe area of the parallelogram determined by <math>\\binom{a_{11}}{a_{21}}</math> and <math>\\binom{a_{12}}{a_{22}}</math> is given by the determinant of the system of equations:\n\n:<math>\\begin{vmatrix}a_{11}&a_{12}\\\\a_{21}&a_{22}\\end{vmatrix}.</math>\n\nIn general, when there are more variables and equations, the determinant of {{mvar|n}} vectors of length {{mvar|n}} will give the ''volume'' of the ''[[parallelepiped]]'' determined by those vectors in the {{mvar|n}}-th dimensional [[Euclidean space]].\n\nTherefore, the area of the parallelogram determined by <math>x_1\\binom{a_{11}}{a_{21}}</math> and <math>\\binom{a_{12}}{a_{22}}</math> has to be <math>x_1</math> times the area of the first one since one of the sides has been multiplied by this factor. Now, this last parallelogram, by [[Cavalieri's principle]], has the same area as the parallelogram determined by <math>\\binom{b_1}{b_2}=x_1\\binom{a_{11}}{a_{21}}+x_2\\binom{a_{12}}{a_{22}}</math> and <math>\\binom{a_{12}}{a_{22}}.</math>\n\nEquating the areas of this last and the second parallelogram gives the equation\n\n:<math>\\begin{vmatrix}b_1&a_{12}\\\\b_2&a_{22}\\end{vmatrix} = \\begin{vmatrix}a_{11}x_1&a_{12}\\\\a_{21}x_1&a_{22}\\end{vmatrix} =x_1 \\begin{vmatrix}a_{11}&a_{12}\\\\a_{21}&a_{22}\\end{vmatrix} </math>\n\nfrom which Cramer's rule follows.\n\n==Other proofs==\n\n===A proof by abstract linear algebra ===\n\nThis is a restatement of the proof above in abstract language.\n\nConsider the map <math>\\vec{x}=(x_1,\\ldots, x_n) \\mapsto  \\frac{1}{\\det A} (\\det (A_1),\\ldots, \\det(A_n)),</math> where <math>A_i</math> is the matrix <math>A</math> with <math>\\vec{x}</math> substituted in the <math>i</math>th column, as in Cramer's rule. Because of linearity of determinant in every column, this map is linear. Observe that it sends the \n<math>i</math>th column of <math>A</math> to the <math>i</math>th basis vector <math>\\vec{e}_i=(0,\\ldots, 1, \\ldots, 0) </math> (with 1 in the <math>i</math>th  place), because determinant of a matrix with a repeated column is 0. So we have a linear map which agrees with the inverse of <math>A</math> on the column space; hence it agrees with   <math>A^{-1}</math>  on the span of the column space. Since <math>A</math> is invertible, the column vectors span all of <math>\\mathbb{R}^n</math>, so our map really is the inverse of <math>A</math>. Cramer's rule follows.\n\n===A short proof===\nA short proof of Cramer's rule <ref>{{cite journal | last = Robinson | first = Stephen M. | title = A Short Proof of Cramer's Rule | journal = Mathematics Magazine| volume = 43 | pages = 94–95 | year = 1970}}</ref> can be given by noticing that <math>x_1</math> is the determinant of the matrix\n\n:<math>X_1=\\begin{bmatrix}\nx_1 & 0 & 0 & \\dots & 0\\\\\nx_2 & 1 & 0 & \\dots & 0\\\\\nx_3 & 0 & 1 & \\dots & 0\\\\\n\\vdots & \\vdots & \\vdots & \\ddots &\\vdots \\\\\nx_n & 0 & 0 & \\dots & 1\n\\end{bmatrix}</math>\n\nOn the other hand, assuming that our original matrix {{mvar|A}} is invertible, this matrix <math>X_1</math> has columns <math>A^{-1}b, A^{-1}v_2, \\ldots, A^{-1}v_n </math>, where <math>v_n</math> is the ''n''-th column of the matrix {{mvar|A}}. Recall that the matrix <math>A_1</math> has columns <math>b, v_2, \\ldots, v_n </math>. Hence we have\n\n:<math> x_1= \\det (X_1) = \\det (A^{-1}) \\det (A_1)= \\frac{\\det (A_1)}{\\det (A)}.</math>\n\nThe proof for other <math>x_j</math> is similar.\n\n=== Proof using [[Clifford algebra]] ===\nConsider the system of three scalar equations in three unknown scalars <math>x_1, x_2, x_3</math>\n\n:<math>\\begin{align}\n  a_{11} x_{1} +a_{12} x_{2} +a_{13} x_{3} & =  c_{1}\\\\\n  a_{21} x_{1} +a_{22} x_{2} +a_{23} x_{3} & =  c_{2}\\\\\n  a_{31} x_{1} +a_{32} x_{2} +a_{33} x_{3} & =  c_{3}\n\\end{align}</math>\n\nand assign an orthonormal vector basis <math>\\mathbf{e}_{1} ,\\mathbf{e}_{2} ,\\mathbf{e}_{3}</math> for <math>\\mathcal{G}_{3}</math> as\n\n:<math>\\begin{align}\n  a_{11} \\mathbf{e}_{1} x_{1} +a_{12} \\mathbf{e}_{1} x_{2} +a_{13} \\mathbf{e}_{1} x_{3} & = c_{1} \\mathbf{e}_{1}\\\\\n  a_{21} \\mathbf{e}_{2} x_{1} +a_{22} \\mathbf{e}_{2} x_{2} +a_{23} \\mathbf{e}_{2} x_{3} & = c_{2} \\mathbf{e}_{2}\\\\\n  a_{31} \\mathbf{e}_{3} x_{1} +a_{32} \\mathbf{e}_{3} x_{2} +a_{33} \\mathbf{e}_{3} x_{3} & = c_{3} \\mathbf{e}_{3} \n\\end{align}</math>\n\nLet the vectors\n\n:<math>\\begin{align}\n  \\mathbf{a}_{1} & = a_{11} \\mathbf{e}_{1} +a_{21} \\mathbf{e}_{2} +a_{31} \\mathbf{e}_{3}\\\\\n  \\mathbf{a}_{2} & = a_{12} \\mathbf{e}_{1} +a_{22} \\mathbf{e}_{2} +a_{32} \\mathbf{e}_{3}\\\\\n  \\mathbf{a}_{3} & = a_{13} \\mathbf{e}_{1} +a_{23} \\mathbf{e}_{2} +a_{33} \\mathbf{e}_{3}\n\\end{align}</math>\n\nAdding the system of equations, it is seen that\n\n:<math>\\begin{align}\n  \\mathbf{c} & = c_{1} \\mathbf{e}_{1} +c_{2} \\mathbf{e}_{2} +c_{3} \\mathbf{e}_{3}\\\\\n  & = x_{1} \\mathbf{a}_{1} +x_{2} \\mathbf{a}_{2} +x_{3} \\mathbf{a}_{3}\n\\end{align}</math>\n\nUsing the [[exterior product]], each unknown scalar <math>x_{k}</math> can be solved as\n\n:<math>\\begin{align}\n\\mathbf{c} \\wedge \\mathbf{a}_{2} \\wedge \\mathbf{a}_{3} &= x_{1} \\mathbf{a}_{1} \\wedge \\mathbf{a}_{2} \\wedge \\mathbf{a}_{3}\\\\\n\\mathbf{c} \\wedge \\mathbf{a}_{1} \\wedge \\mathbf{a}_{3} &= x_{2} \\mathbf{a}_{2} \\wedge \\mathbf{a}_{1} \\wedge \\mathbf{a}_{3}\\\\\n\\mathbf{c} \\wedge \\mathbf{a}_{1} \\wedge \\mathbf{a}_{2} &= x_{3} \\mathbf{a}_{3} \\wedge \\mathbf{a}_{1} \\wedge \\mathbf{a}_{2}\\\\\nx_{1} &= \\frac{\\mathbf{c} \\wedge \\mathbf{a}_{2} \\wedge \\mathbf{a}_{3}}{\\mathbf{a}_{1} \\wedge \\mathbf{a}_{2} \\wedge \\mathbf{a}_{3}}\\\\\nx_{2} &= \\frac{\\mathbf{c} \\wedge \\mathbf{a}_{1} \\wedge \\mathbf{a}_{3}}{\\mathbf{a}_{2} \\wedge \\mathbf{a}_{1} \\wedge \\mathbf{a}_{3}} = \\frac{\\mathbf{a}_{1} \\wedge \\mathbf{c} \\wedge \\mathbf{a}_{3}}{\\mathbf{a}_{1} \\wedge \\mathbf{a}_{2} \\wedge \\mathbf{a}_{3}}\\\\\nx_{3} &= \\frac{\\mathbf{c} \\wedge \\mathbf{a}_{1} \\wedge \\mathbf{a}_{2}}{\\mathbf{a}_{3} \\wedge \\mathbf{a}_{1} \\wedge \\mathbf{a}_{2}} = \\frac{\\mathbf{a}_{1} \\wedge \\mathbf{a}_{2} \\wedge \\mathbf{c}}{\\mathbf{a}_{1} \\wedge \\mathbf{a}_{2} \\wedge \\mathbf{a}_{3}} \n\\end{align}</math>\n\nFor {{mvar|n}} equations in {{mvar|n}} unknowns, the solution for the {{mvar|k}}-th unknown <math>x_{k}</math> generalizes to\n\n:<math>\\begin{align}\nx_k &= \\frac{\\mathbf{a}_{1} \\wedge \\cdots \\wedge (\\mathbf{c})_k \\wedge \\cdots \\wedge \\mathbf{a}_{n}}{\\mathbf{a}_{1} \\wedge \\cdots \\wedge \\mathbf{a}_{k} \\wedge \\cdots \\wedge \\mathbf{a}_{n}}\\\\\n    &= (\\mathbf{a}_{1} \\wedge \\cdots \\wedge (\\mathbf{c})_k \\wedge \\cdots \\wedge \\mathbf{a}_{n}) (\\mathbf{a}_{1} \\wedge \\cdots \\wedge \\mathbf{a}_{k} \\wedge \\cdots \\wedge \\mathbf{a}_{n} )^{-1}\\\\\n    &= \\frac{(\\mathbf{a}_{1} \\wedge \\cdots \\wedge (\\mathbf{c})_k \\wedge \\cdots \\wedge \\mathbf{a}_{n}) (\\mathbf{a}_{1} \\wedge \\cdots \\wedge \\mathbf{a}_{k} \\wedge \\cdots \\wedge \\mathbf{a}_{n})}{(\\mathbf{a}_{1} \\wedge \\cdots \\wedge \\mathbf{a}_{k} \\wedge \\cdots \\wedge \\mathbf{a}_{n}) (\\mathbf{a}_{1} \\wedge \\cdots \\wedge \\mathbf{a}_{k} \\wedge \\cdots \\wedge \\mathbf{a}_{n})}\\\\\n    &= \\frac{( \\mathbf{a}_{1} \\wedge \\cdots \\wedge (\\mathbf{c})_k \\wedge \\cdots \\wedge \\mathbf{a}_{n} ) \\cdot ( \\mathbf{a}_{1} \\wedge \\cdots \\wedge \\mathbf{a}_{k} \\wedge \\cdots \\wedge \\mathbf{a}_{n} )}{(-1)^{\\frac{n(n-1)}{2}}  ( \\mathbf{a}_{n} \\wedge \\cdots \\wedge \\mathbf{a}_{k} \\wedge \\cdots \\wedge \\mathbf{a}_{1} ) \\cdot (\\mathbf{a}_{1} \\wedge \\cdots \\wedge \\mathbf{a}_{k} \\wedge \\cdots \\wedge \\mathbf{a}_{n} )}\\\\\n    &= \\frac{( \\mathbf{a}_{n} \\wedge \\cdots \\wedge (\\mathbf{c})_k \\wedge \\cdots \\wedge \\mathbf{a}_{1} ) \\cdot (\\mathbf{a}_{1} \\wedge \\cdots \\wedge \\mathbf{a}_{k} \\wedge \\cdots \\wedge \\mathbf{a}_{n} )}{( \\mathbf{a}_{n} \\wedge \\cdots \\wedge \\mathbf{a}_{k} \\wedge \\cdots \\wedge \\mathbf{a}_{1} ) \\cdot ( \\mathbf{a}_{1} \\wedge \\cdots \\wedge \\mathbf{a}_{k} \\wedge \\cdots \\wedge \\mathbf{a}_{n} )} \n\\end{align}</math>\n\nIf {{math|'''a'''<sub>''k''</sub>}} are linearly independent, then the <math>x_{k}</math> can be expressed in determinant form identical to Cramer’s Rule as\n\n:<math>\\begin{align}\nx_k &= \\frac{( \\mathbf{a}_{n} \\wedge \\cdots \\wedge ( \\mathbf{c} )_{k} \\wedge \\cdots \\wedge \\mathbf{a}_{1} ) \\cdot ( \\mathbf{a}_1 \\wedge \\cdots \\wedge \\mathbf{a}_{k} \\wedge \\cdots \\wedge \\mathbf{a}_{n} )}{(\\mathbf{a}_{n} \\wedge \\cdots \\wedge \\mathbf{a}_{k} \\wedge \\cdots \\wedge \\mathbf{a}_1 ) \\cdot ( \\mathbf{a}_1 \\wedge \\cdots \\wedge \\mathbf{a}_{k} \\wedge \\cdots \\wedge \\mathbf{a}_{n})}\\\\ [8pt]\n&= \\begin{vmatrix}\n    \\mathbf{a}_{1} \\cdot \\mathbf{a}_1 & \\cdots & \\mathbf{a}_{1} \\cdot (\n    \\mathbf{c} )_{k} & \\cdots & \\mathbf{a}_1 \\cdot \\mathbf{a}_{n}\\\\\n    \\vdots & \\ddots & \\vdots & \\ddots & \\vdots\\\\\n    \\mathbf{a}_{k} \\cdot \\mathbf{a}_{1} & \\cdots & \\mathbf{a}_{k} \\cdot (\n    \\mathbf{c} )_{k} & \\cdots & \\mathbf{a}_{k} \\cdot \\mathbf{a}_{n}\\\\\n    \\vdots & \\ddots & \\vdots & \\ddots & \\vdots\\\\\n    \\mathbf{a}_{n} \\cdot \\mathbf{a}_{1} & \\cdots & \\mathbf{a}_{n} \\cdot (\n    \\mathbf{c} )_{k} & \\cdots & \\mathbf{a}_{n} \\cdot \\mathbf{a}_{n}\n  \\end{vmatrix} \\begin{vmatrix}\n    \\mathbf{a}_{1} \\cdot \\mathbf{a}_{1} & \\cdots & \\mathbf{a}_{1} \\cdot\n    \\mathbf{a}_{k} & \\cdots & \\mathbf{a}_{1} \\cdot \\mathbf{a}_{n}\\\\\n    \\vdots & \\ddots & \\vdots & \\ddots & \\vdots\\\\\n    \\mathbf{a}_{k} \\cdot \\mathbf{a}_{1} & \\cdots & \\mathbf{a}_{k} \\cdot\n    \\mathbf{a}_{k} & \\cdots & \\mathbf{a}_{k} \\cdot \\mathbf{a}_{n}\\\\\n    \\vdots & \\ddots & \\vdots & \\ddots & \\vdots\\\\\n    \\mathbf{a}_{n} \\cdot \\mathbf{a}_{1} & \\cdots & \\mathbf{a}_{n} \\cdot\n    \\mathbf{a}_{k} & \\cdots & \\mathbf{a}_{n} \\cdot \\mathbf{a}_{n}\n  \\end{vmatrix}^{-1}  \\\\ [8pt]\n&= \\begin{vmatrix} \\mathbf{a}_{1}\\\\ \\vdots\\\\ \\mathbf{a}_{k}\\\\ \\vdots\\\\ \\mathbf{a}_{n} \\end{vmatrix} \\begin{vmatrix} \\mathbf{a}_{1} & \\cdots & ( \\mathbf{c} )_{k} & \\cdots & \\mathbf{a}_{n} \\end{vmatrix} \\begin{vmatrix} \\mathbf{a}_{1}\\\\ \\vdots\\\\ \\mathbf{a}_{k}\\\\ \\vdots\\\\\n\\mathbf{a}_{n} \\end{vmatrix}^{-1} \\begin{vmatrix} \\mathbf{a}_{1} & \\cdots & \\mathbf{a}_{k} & \\cdots & \\mathbf{a}_{n} \\end{vmatrix}^{-1}\\\\ [8pt]\n&= \\begin{vmatrix} \\mathbf{a}_1 & \\cdots & (\\mathbf{c})_{k} & \\cdots & \\mathbf{a}_{n} \\end{vmatrix} \\begin{vmatrix} \\mathbf{a}_{1} & \\cdots & \\mathbf{a}_{k} & \\cdots & \\mathbf{a}_{n} \\end{vmatrix}^{-1} \\\\ [8pt]\n&= \\begin{vmatrix}\n    a_{11} & \\ldots & c_{1} & \\cdots & a_{1n}\\\\\n    \\vdots & \\ddots & \\vdots & \\ddots & \\vdots\\\\\n    a_{k1} & \\cdots & c_{k} & \\cdots & a_{k n}\\\\\n    \\vdots & \\ddots & \\vdots & \\ddots & \\vdots\\\\\n    a_{n1} & \\cdots & c_{n} & \\cdots & a_{n n}\n  \\end{vmatrix} \\begin{vmatrix}\n    a_{11} & \\ldots & a_{1k} & \\cdots & a_{1n}\\\\\n    \\vdots\n  \\end{vmatrix}^{-1}\n\\end{align}</math>\n\nwhere {{math|('''c''')<sub>''k''</sub>}} denotes the substitution of vector {{math|'''a'''<sub>''k''</sub>}} with vector {{math|'''c'''}} in the {{mvar|k}}-th numerator position.\n\n==Incompatible and indeterminate cases==\nA system of equations is said to be incompatible or [[inconsistent equations|inconsistent]] when there are no solutions and it is called [[indeterminate system|indeterminate]] when there is more than one solution. For linear equations, an indeterminate system will have infinitely many solutions (if it is over an infinite field), since the solutions can be expressed in terms of one or more parameters that can take arbitrary values.\n\nCramer's rule applies to the case where the coefficient determinant is nonzero. In the 2×2 case, if the coefficient determinant is zero, then the system is incompatible if the numerator determinants are nonzero, or indeterminate if the numerator determinants are zero.\n\nFor 3×3 or higher systems, the only thing one can say when the coefficient determinant equals zero is that if any of the numerator determinants are nonzero, then the system must be incompatible. However, having all determinants zero does not imply that the system is indeterminate. A simple example where all determinants vanish (equal zero) but the system is still incompatible is the 3×3 system x+y+z=1, x+y+z=2, x+y+z=3.\n\n==References==\n{{Reflist|30em}}\n\n==External links==\n{{wikibooks|Linear Algebra/Cramer's Rule}}\n* [http://planetmath.org/encyclopedia/ProofOfCramersRule.html Proof of Cramer's Rule]\n* [http://sole.ooz.ie/ WebApp descriptively solving systems of linear equations with Cramer's Rule]\n* [http://www.elektro-energetika.cz/calculations/linrov.php?language=english Online Calculator of System of linear equations]\n\n{{linear algebra}}\n\n{{DEFAULTSORT:Cramer's Rule}}\n[[Category:Linear algebra]]\n[[Category:Theorems in algebra]]\n[[Category:Determinants]]\n[[Category:1750 in science]]"
    },
    {
      "title": "Crystallographic restriction theorem",
      "url": "https://en.wikipedia.org/wiki/Crystallographic_restriction_theorem",
      "text": "{{Use dmy dates|date=January 2012}}{{Short description|Theorem about admissible crystal symmetries}}{{Missing information|a succinct statement of the theorem|date=December 2018}}\nThe '''crystallographic restriction theorem''' in its basic form was based on the observation that the [[rotational symmetry|rotational symmetries]] of a [[crystal]] are usually limited to 2-fold, 3-fold, 4-fold, and 6-fold. However, [[quasicrystal]]s can occur with other diffraction pattern symmetries, such as 5-fold; these were not discovered until 1982 by [[Dan Shechtman]].<ref>Shechtman et al (1982)</ref>\n\nCrystals are modeled as discrete [[lattice (group)|lattice]]s, generated by a list of [[linear independence|independent]] finite [[translation]]s {{Harv|Coxeter|1989}}. Because discreteness requires that the spacings between lattice points have a lower bound, the [[group (mathematics)|group]] of rotational symmetries of the lattice at any point must be a [[finite group]] (alternatively, the point is the only system allowing for infinite rotational symmetry). The strength of the theorem is that ''not all'' finite groups are compatible with a discrete lattice; in any dimension, we will have only a finite number of compatible groups.\n\n==Dimensions 2 and 3==\n\nThe special cases of 2D ([[wallpaper group]]s) and 3D ([[space group]]s) are most heavily used in applications, and they can be treated together.\n\n=== Lattice proof ===\n\nA rotation symmetry in dimension 2 or 3 must move a lattice point to a [[Group action (mathematics)#Orbits and stabilizers|succession]] of other lattice points in the same plane, generating a [[regular polygon]] of coplanar lattice points. We now confine our attention to the plane in which the symmetry acts {{Harv|Scherrer|1946}}, illustrated with lattice [[Vector (geometric)|vector]]s in the figure.\n\n[[Image:Crystallographic restriction polygons.png|right|280px|thumb|Lattices restrict polygons<br />\n&nbsp;''Compatible'': 6-fold (3-fold), 4-fold (2-fold)<br />\n&nbsp;''Incompatible'': 8-fold, 5-fold]]\n\nNow consider an 8-fold rotation, and the displacement vectors between adjacent points of the polygon. If a displacement exists between any two lattice points, then that same displacement is repeated everywhere in the lattice. So collect all the edge displacements to begin at a single lattice point. The [[edge vector]]s become radial vectors, and their 8-fold symmetry implies a regular octagon of lattice points around the collection point. But this is ''impossible'', because the new octagon is about 80% as large as the original. The significance of the shrinking is that it is unlimited. The same construction can be repeated with the new octagon, and again and again until the distance between lattice points is as small as we like; thus no ''discrete'' lattice can have 8-fold symmetry. The same argument applies to any ''k''-fold rotation, for ''k'' greater than 6.\n\nA shrinking argument also eliminates 5-fold symmetry. Consider a regular pentagon of lattice points. If it exists, then we can take every ''other'' edge displacement and (head-to-tail) assemble a 5-point star, with the last edge returning to the starting point. The vertices of such a star are again vertices of a regular pentagon with 5-fold symmetry, but about 60% smaller than the original.\n\nThus the theorem is proved.\n\nThe existence of quasicrystals and [[Penrose tiling]]s shows that the assumption of a linear translation is necessary.  Penrose tilings may have 5-fold [[rotational symmetry]] and a discrete lattice, and any local neighborhood of the tiling is repeated infinitely many times, but there is no linear translation for the tiling as a whole.  And without the discrete lattice assumption, the above construction not only fails to reach a contradiction, but produces a (non-discrete) counterexample.  Thus 5-fold rotational symmetry cannot be eliminated by an argument missing either of those assumptions.  A Penrose tiling of the whole (infinite) plane can only have exact 5-fold rotational symmetry (of the whole tiling) about a single point, however, whereas the 4-fold and 6-fold lattices have infinitely many centres of rotational symmetry.\n\n=== Trigonometry proof ===\n\nConsider two lattice points A and B separated by a translation vector '''''r'''''. Consider an angle α such that a rotation of angle α about any lattice point is a symmetry of the lattice. Rotating about point B by α maps point A to a new point A'.  Similarly, rotating about point A by α maps B to a point B'.  Since both rotations mentioned are symmetry operations, A' and B' must both be lattice points.  Due to periodicity of the crystal, the new vector '''''r'''''' which connects them must be equal to an integer multiple of '''''r''''':\n\n: <math> \\mathbf{r}' = m\\mathbf{r} </math>\n\nwith <math>m</math> integer. The four translation vectors, three of length <math>r=|\\mathbf{r}|</math> and one, connecting A' and B', of length <math>r'=|\\mathbf{r}'|</math>, form a trapezium.  Therefore, the length of '''''r'''''' is also given by:\n\n: <math> r' = 2r\\cos\\alpha - r. </math>\n\nCombining the two equations gives:\n\n: <math> \\cos\\alpha = \\frac{m+1}{2} = \\frac{M}{2}  </math>\n\nwhere <math>M=m+1</math> is also an integer. Bearing in mind that <math>|\\cos\\alpha|\\le 1</math> we have allowed integers <math>M\\in\\{-2,-1,0,1,2\\}</math>. Solving for possible values of <math>\\alpha</math> reveals that the only values in the 0° to 180° range are 0°, 60°, 90°, 120°, and 180°.  In radians, the only allowed rotations consistent with lattice periodicity are given by 2π/''n'', where ''n'' = 1, 2, 3, 4, 6.  This corresponds to 1-, 2-, 3-, 4-, and 6-fold symmetry, respectively, and therefore excludes the possibility of 5-fold or greater than 6-fold symmetry.\n\n=== Short trigonometry proof ===\n[[Image:Crystallographic restriction.png|thumb|right]]\n[[Image:Crystallographic restriction 2.png|thumb|right]]\n\nConsider a line of atoms ''A-O-B'', separated by distance ''a''. Rotate the entire row by θ = +2π/''n'' and θ = −2π/''n'', with point ''O'' kept fixed. After the rotation by +2π/''n'', A is moved to the lattice point ''C'' and after the rotation by -2π/''n'', B is moved to the lattice point ''D''. Due to the assumed periodicity of the lattice, the two lattice points ''C'' and ''D'' will be also in a line directly below the initial row; moreover ''C'' and ''D'' will be separated by ''r'' = ''ma'', with ''m'' an integer. But by the geometry, the separation between these points is: \n\n: <math>2a\\cos{\\theta} = 2a\\cos{\\frac{2\\pi}{n}}</math>. \n\nEquating the two relations gives: \n\n: <math>2\\cos{\\frac{2\\pi}{n}}=m</math>\n\nThis is satisfied by only ''n'' = 1, 2, 3, 4, 6.\n\n=== Matrix proof ===\n\nFor an alternative proof, consider [[matrix (mathematics)|matrix]] properties. The sum of the diagonal elements of a matrix is called the [[trace (matrix)|trace]] of the matrix. In 2D and 3D every rotation is a planar rotation, and the trace is a function of the angle alone. For a 2D rotation, the trace is 2 cos θ; for a 3D rotation, 1 + 2 cos θ.\n\n'''Examples'''\n*Consider a 60° (6-fold) [[rotation matrix]] with respect to an [[orthonormal basis]] in 2D.\n::<math>\\begin{bmatrix} {1/2} & -{\\sqrt{3}/2} \\\\ {\\sqrt{3}/2} & {1/2} \\end{bmatrix}</math>\n:The trace is precisely 1, an [[integer]].\n*Consider a 45° (8-fold) rotation matrix.\n::<math>\\begin{bmatrix} {1/\\sqrt{2}} & -{1/\\sqrt{2}} \\\\ {1/\\sqrt{2}} & {1/\\sqrt{2}} \\end{bmatrix}</math>\n:The trace is 2/{{radic|2}}, not an integer.\n\n\nSelecting a basis formed from vectors that spans the lattice, neither orthogonality nor unit length is guaranteed, only linear independence. However the trace of the rotation matrix is the same with respect to any basis. The trace is a [[similarity invariance|similarity invariant]] under linear transformations. In the lattice basis, the rotation operation must map every lattice point into an integer number of lattice vectors, so the entries of the rotation matrix in the lattice basis — and hence the trace — are necessarily integers. Similar as in other proofs, this implies that the only allowed rotational symmetries correspond to 1,2,3,4 or 6-fold invariance.  For example, wallpapers and crystals cannot be rotated by 45° and remain invariant, the only possible angles are: 360°, 180°, 120°, 90° or 60°.\n\n'''Example'''\n*Consider a 60° (360°/6) rotation matrix with respect to the [[:wikt:oblique|oblique]] lattice basis for a [[tessellation|tiling]] by equilateral triangles.\n::<math>\\begin{bmatrix} 0 & -1 \\\\ 1 & 1 \\end{bmatrix}</math>\n:The trace is still 1. The [[determinant]] (always +1 for a rotation) is also preserved.\n\nThe general crystallographic restriction on rotations does ''not'' guarantee that a rotation will be compatible with a specific lattice. For example, a 60° rotation will not work with a square lattice; nor will a 90° rotation work with a rectangular lattice.\n\n==Higher dimensions==\nWhen the dimension of the lattice rises to four or more, rotations need no longer be planar; the 2D proof is inadequate. However, restrictions still apply, though more symmetries are permissible. For example, the [[hypercubic lattice]] has an eightfold rotational symmetry, corresponding to an eightfold rotational symmetry of the [[hypercube]]. This is of interest, not just for mathematics, but for the physics of quasicrystals under the [[Aperiodic_tiling#Cut-and-project_method|cut-and-project theory]]. In this view, a 3D quasicrystal with 8-fold rotation symmetry might be described as the projection of a slab cut from a 4D lattice.\n\nThe following 4D rotation matrix is the aforementioned eightfold symmetry of the [[hypercube]] (and the [[cross-polytope]]):\n:<math>A = \\begin{bmatrix} 0 & 0 & 0 & -1 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & -1 & 0 & 0 \\\\ 0 & 0 & -1 & 0 \\end{bmatrix}.</math>\nTransforming this matrix to the new coordinates given by \n:<math>B = \\begin{bmatrix} -1/2 & 0 & -1/2 & \\sqrt 2/2 \\\\ 1/2 & \\sqrt 2/2 & -1/2 & 0 \\\\ -1/2 & 0 & -1/2 & -\\sqrt 2/2 \\\\ -1/2 & \\sqrt 2/2 & 1/2 & 0 \\end{bmatrix}</math> will produce:\n:<math>B A B^{-1} = \\begin{bmatrix} \\sqrt 2/2 & \\sqrt 2/2 & 0 & 0 \\\\ -\\sqrt 2/2 & \\sqrt 2/2 & 0 & 0 \\\\ 0 & 0 & -\\sqrt 2/2 & \\sqrt 2/2 \\\\ 0 & 0 & -\\sqrt 2/2 & -\\sqrt 2/2 \\end{bmatrix}.</math>\nThis third matrix then corresponds to a rotation both by 45° (in the first two dimensions) and by 135° (in the last two). Projecting a slab of hypercubes along the first two dimensions of the new coordinates produces an [[Ammann–Beenker tiling]] (another such tiling is produced by projecting along the last two dimensions), which therefore also has 8-fold rotational symmetry on average.\n\nThe [[A4 lattice]] and [[F4 lattice]] have order 10 and order 12 rotational symmetries, respectively.\n\nTo state the restriction for all dimensions, it is convenient to shift attention away from rotations alone and concentrate on the integer matrices {{Harv|Bamberg|Cairns|Kilminster|2003}}. We say that a matrix A has '''[[order (group theory)|order]]''' ''k'' when its ''k''-th power (but no lower), A<sup>''k''</sup>, equals the identity. Thus a 6-fold rotation matrix in the equilateral triangle basis is an integer matrix with order 6. Let Ord<sub>''N''</sub> denote the set of integers that can be the order of an ''N''×''N'' integer matrix. For example, Ord<sub>2</sub> = {1, 2, 3, 4, 6}. We wish to state an explicit formula for Ord<sub>''N''</sub>.\n\nDefine a function ψ based on [[Euler's totient function]] φ; it will map positive integers to non-negative integers. For an odd [[prime number|prime]], ''p'', and a positive integer, ''k'', set ψ(''p''<sup>''k''</sup>) equal to the totient function value, \nφ(''p''<sup>''k''</sup>), which in this case is ''p''<sup>''k''</sup>−''p''<sup>''k−1''</sup>. Do the same for ψ(2<sup>''k''</sup>) when ''k'' > 1. Set ψ(2) and ψ(1) to 0. Using the [[fundamental theorem of arithmetic]], we can write any other positive integer uniquely as a product of prime powers, ''m'' = ∏<sub>α</sub> ''p''<sub>α</sub><span><sup>''k''<sub> α</sub></sup></span>; set ψ(''m'') = ∑<sub>α</sub> ψ(''p''<sub>α</sub><sup><span>''k''<sub> α</sub></sup></span>). This differs from the totient itself, because it is a sum instead of a product.\n\nThe crystallographic restriction in general form states that Ord<sub>''N''</sub> consists of those positive integers ''m'' such that ψ(''m'') ≤ ''N''.\n\n:{| class=\"wikitable\"\n|+ '''Smallest dimension for a given order'''{{OEIS2C|id=A080737}}\n|- align=\"center\"\n| ''m'' || 1 || 2 || 3 || 4 || 5 || 6 || 7 || 8 || 9 || 10 || 11 || 12 || 13 || 14 || 15 || 16 || 17 || 18 || 19 || 20 || 21 || 22 || 23 || 24 || 25 || 26 || 27 || 28 || 29 || 30 || 31\n|- align=\"center\"\n| ψ(''m'') || 0 || 0 || 2 || 2 || 4 || 2 || 6 || 4 || 6 ||  4 || 10 ||  4 || 12 ||  6 || 6 || 8 || 16 || 6 || 18 || 6 || 8 || 10 || 22 || 6 || 20 || 12 || 18 || 8 || 28 || 6 || 30\n|}\n\nFor ''m''>2, the values of ψ(''m'') are equal to twice the [[Algebraic_number#Properties|algebraic degree]] of cos(2&pi;/''m''); therefore, ψ(''m'') is strictly less than ''m'' and reaches this maximum value if and only if ''m'' is a [[prime number|prime]].\n\nNote that these additional symmetries do not allow a planar slice to have, say, 8-fold rotation symmetry. In the plane, the 2D restrictions still apply. Thus the cuts used to model quasicrystals necessarily have thickness.\n\nInteger matrices are not limited to rotations; for example, a reflection is also a symmetry of order 2. But by insisting on determinant +1, we can restrict the matrices to [[proper rotation]]s.\n\n==Formulation in terms of isometries==\n\nThe crystallographic restriction theorem can be formulated in terms of [[isometry|isometries]] of [[Euclidean space]]. A set of isometries can form a [[group (mathematics)|group]]. By a ''discrete isometry group'' we will mean an isometry group that maps every point to a discrete subset of '''R'''<sup>''N''</sup>, i.e. a set of [[isolated point]]s. With this terminology, the crystallographic restriction theorem in two and three dimensions can be formulated as follows.\n\n:For every discrete [[isometry group]] in two- and three-dimensional space which includes translations spanning the whole space, all isometries of finite [[order (group theory)|order]] are of order 1, 2, 3, 4 or 6.\n\nNote that isometries of order ''n'' include, but are not restricted to, ''n''-fold rotations. The theorem also excludes ''S<sub>8</sub>'', ''S<sub>12</sub>'', ''D<sub>4d</sub>'', and ''D<sub>6d</sub>'' (see [[point groups in three dimensions]]), even though they have 4- and 6-fold rotational symmetry only.\n\nNote also that rotational symmetry of any order about an axis is compatible with translational symmetry along that axis.\n\nThe result in the table above implies that for every discrete isometry group in four- and five-dimensional space which includes translations spanning the whole space, all isometries of finite order are of order 1, 2, 3, 4, 5, 6, 8, 10, or 12.\n\nAll isometries of finite order in six- and seven-dimensional space are of order 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 15, 18, 20, 24 or 30 .\n\n==See also==\n*[[Crystallographic point group]]\n*[[Crystallography]]\n\n==Notes==\n<references/>\n\n== References ==\n* {{Citation\n  | last1 =Bamberg | first1 =John\n  | last2 =Cairns | first2 =Grant\n  | last3 =Kilminster | first3 =Devin\n  | title =The crystallographic restriction, permutations, and Goldbach's conjecture\n  | journal =[[American Mathematical Monthly]]\n  |date=March 2003\n  | volume =110  | issue=3 | pages =202–209\n  | url =http://cage.ugent.be/~bamberg/Research_files/The%20crystallographic%20restriction,%20permutations%20and%20Goldbach%27s%20conjecture.pdf\n  | doi =10.2307/3647934\n  | jstor =3647934\n| citeseerx =10.1.1.124.8582\n  }}\n* {{Citation\n  | last =Elliott  | first =Stephen\n  | author-link =Stephen Elliott (mathematician)\n  | title =The Physics and Chemistry of Solids\n  | publisher =Wiley\n  | year =1998\n  | isbn =978-0-471-98194-7\n}}\n* {{Citation\n  | last =Coxeter  | first =H. S. M.\n  | author-link =H. S. M. Coxeter\n  | title =Introduction to Geometry\n  | publisher =Wiley\n  | year =1989\n  | edition =2nd\n  | isbn =978-0-471-50458-0\n}}\n* {{Citation\n  | last =Scherrer | first =W.\n  | title =Die Einlagerung eines regulären Vielecks in ein Gitter\n  | journal =[[Elemente der Mathematik]]\n  | year =1946\n  | volume =1  | issue =6 | pages =97–98\n  | url =http://www-gdz.sub.uni-goettingen.de/cgi-bin/digbib.cgi?PPN378850199_0001\n}}\n* {{Citation\n  | last =Shechtman | first =D.\n  | last2 =Blech | first2 =I.\n  | last3 =Gratias | first3 =D.\n  | last4 =Cahn | first4 =JW\n  | year =1984\n  | title =Metallic phase with long-range orientational order and no translational symmetry\n  | journal =[[Physical Review Letters]]\n  | volume =53  | issue =20  | pages =1951–1953\n  | doi = 10.1103/PhysRevLett.53.1951 | bibcode=1984PhRvL..53.1951S\n}}\n\n==External links==\n*[http://www-history.mcs.st-and.ac.uk/~john/geometry/Lectures/A2.html The crystallographic restriction]\n\n[[Category:Crystallography]]\n[[Category:Group theory]]\n[[Category:Theorems in algebra]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "Descartes' rule of signs",
      "url": "https://en.wikipedia.org/wiki/Descartes%27_rule_of_signs",
      "text": "{{short description|Theorem linking the number of positive roots of a polynomial to the signs of its coefficients}}\nIn [[mathematics]], '''Descartes' rule of signs''', first described by [[René Descartes]] in his work ''[[La Géométrie]]'', is a technique for getting information on the number of positive real [[root of a function|roots]] of a [[polynomial]]. It asserts that the number of positive roots is at most the number of sign changes in the sequence of polynomial's coefficients (omitting the zero coefficients), and that the difference between these two numbers is always even. This implies in particular that, if this difference is zero or one, then there is exactly zero or one positive root, respectively.\n\nBy a [[homographic transformation]] of the variable, one may use Descartes' rule of signs for getting a similar information on the number of roots in any interval. This is the basic idea of [[Budan's theorem]] and [[Budan–Fourier theorem]]. By repeating the division of an interval into two intervals, one gets eventually a list of disjoints intervals containing together all real roots of the polynomial, and containing each exactly one real root. Descartes rule of signs and homographic transformations of the variable are, nowadays, the basis of the fastest algorithms for computer computation of real roots of polynomials (see [[Real-root isolation]]).\n\nDescartes himself used the transformation {{math|''x'' → –''x''}} for using his rule for getting information of the number of negative roots.\n\n==Descartes' rule of signs==\n===Positive roots===\nThe rule states that if the terms of a single-variable [[polynomial]] with [[Real number|real]] [[coefficient]]s are ordered by descending variable exponent, then the number of positive [[Root of a function|roots]] of the polynomial is either equal to the number of sign differences between consecutive nonzero coefficients, or is less than it by an even number. [[Multiple root]]s of the same value are counted separately.\n\n===Negative roots===\nAs a [[corollary]] of the rule, the number of negative roots is the number of sign changes after multiplying the coefficients of odd-power terms by −1, or fewer than it by an even number. This procedure is equivalent to substituting the negation of the variable for the variable itself.\nFor example, to find the number of negative roots of <math>f(x)=ax^3+bx^2+cx+d</math>, we equivalently ask how many positive roots there are for <math>-x</math> in \n:<math>f(-x)=a(-x)^3+b(-x)^2+c(-x)+d = -ax^3+bx^2-cx+d \\equiv g(x). </math> \nUsing Descartes' rule of signs on <math>g(x)</math> gives the number of positive roots <math>x_i</math> of ''g'', and since <math>g(x) = f(-x)</math> it gives the number of positive roots <math>(-x_i)</math> of ''f'', which is the same as the number of negative roots <math>x_i</math> of ''f''.\n\n===Example: real roots===\nThe polynomial\n\n:<math>f(x) = + x^3 + x^2 - x - 1 </math>\n\nhas one sign change between the second and third terms (the sequence of pairs of successive signs is +&#x202f;→&#x202f;+, +&#x202f;→&#x202f;&minus;, &minus;&#x202f;→&#x202f;&minus;&#x202f;). Therefore it has exactly one positive root. Note that the sign of the leading coefficient needs to be considered.\nTo find the number of negative roots, change the signs of the coefficients of the terms with odd exponents, i.e., apply Descartes' rule of signs to the polynomial <math>f(-x)</math>, to obtain a second polynomial\n\n:<math>f(-x)= - x^3 + x^2 + x - 1 </math>\n\nThis polynomial has two sign changes (the sequence of pairs of successive signs is &minus;&#x202f;→&#x202f;+, +&#x202f;→&#x202f;+, +&#x202f;→&#x202f;&minus;&#x202f;), meaning that this second polynomial has two or zero positive roots; thus the original polynomial has two or zero negative roots.\n\nIn fact, the factorization of the first polynomial is\n\t\n:<math>f(x)=(x + 1)^{2}(x - 1), </math>\n\t \t\t\nso the roots are &minus;1 (twice) and +1 (once).\n\nThe factorization of the second polynomial is\n\n:<math>f(-x)=-(x - 1)^{2}(x + 1), </math>\n\nSo here, the roots are +1 (twice) and &minus;1 (once), the negation of the roots of the original polynomial.\n\n==Nonreal roots==\n\nAny ''n''th degree polynomial has exactly ''n''  roots in the [[complex plane]], if counted according to multiplicity. So if ''f''(''x'') is a polynomial which does not have a root at 0 (which can be determined by inspection) then the <u>minimum</u> number of nonreal roots is equal to\n\n:<math>n-(p+q),</math>\n\nwhere ''p'' denotes the maximum number of positive roots, ''q'' denotes the maximum number of negative roots (both of which can be found using Descartes' rule of signs), and ''n'' denotes the degree of the equation.\n\n===Example: zero coefficients, nonreal roots===\n\nThe polynomial\n\n:<math>f(x) = x^3-1\\, ,</math>\n\nhas one sign change, so the maximum number of positive real roots is 1. From\n\n:<math>f(-x) = -x^3-1\\, ,</math>\n\nwe can tell that the polynomial has no negative real roots. So the minimum number of nonreal roots is\n\n:<math>3 - (1+0) = 2 \\, .</math>\n\nSince nonreal roots of a polynomial with real coefficients must occur in conjugate pairs, we can see that {{nowrap|''x''<sup>3</sup> − 1}} has exactly 2 nonreal roots and 1 real (and positive) root.\n\n==Special case==\nThe subtraction of only multiples of 2 from the maximal number of positive roots occurs because the polynomial may have nonreal roots, which always come in pairs since the rule applies to polynomials whose coefficients are real. Thus if the polynomial is known to have all real roots, this rule allows one to find the exact number of positive and negative roots. Since it is easy to determine the multiplicity of zero as a root, the sign of all roots can be determined in this case.\n\n==Generalizations==\n\nIf the real polynomial ''P'' has ''k'' real positive roots counted with multiplicity, then for every ''a'' > 0 there are at least ''k'' changes of sign in the sequence of coefficients of the Taylor series of the function ''e''<sup>''ax''</sup>''P''(''x''). For ''a'' sufficiently large, there are exactly ''k'' such changes of sign.<ref>D. R. Curtiss, ''Recent extensions of Descartes' rule of signs'', Annals of Mathematics., Vol. 19, No. 4, 1918, pp. 251–278.</ref><ref>Vladimir P. Kostov, ''A mapping defined by the Schur–Szegő composition'', Comptes Rendus Acad. Bulg. Sci. tome 63, No. 7, 2010, pp. 943–952.</ref>\n\nIn the 1970s [[Askold Georgevich Khovanskiǐ]] developed the theory of ''[[fewnomial]]s'' that generalises Descartes' rule.<ref>{{cite book |last=Khovanskiǐ |first=A.G. |title=Fewnomials |others=Translated from the Russian by Smilka Zdravkovska |zbl=0728.12002 |series=Translations of Mathematical Monographs |page=88 |location=Providence, RI |publisher=[[American Mathematical Society]] |year=1991 |isbn=0-8218-4547-0}}</ref> The rule of signs can be thought of as stating that the number of real roots of a polynomial is dependent on the polynomial's complexity, and that this complexity is proportional to the number of monomials it has, not its degree. Khovanskiǐ showed that this holds true not just for polynomials but for algebraic combinations of many transcendental functions, the so-called [[Pfaffian function]]s.\n\n==See also==\n*[[Sturm's theorem]]\n*[[Rational root theorem]]\n*[[Properties of polynomial roots]]\n*[[Gauss–Lucas theorem]]\n\n==Notes==\n{{reflist}}\n\n==External links==\n{{PlanetMath attribution|id=5997|title=Descartes' rule of signs}}\n*[http://www.cut-the-knot.org/fta/ROS2.shtml Descartes' Rule of Signs] – Proof of the rule\n*[http://www.purplemath.com/modules/drofsign.htm Descartes' Rule of Signs] – Basic explanation\n\n[[Category:Polynomials]]\n[[Category:Theorems in algebra]]"
    },
    {
      "title": "Factor theorem",
      "url": "https://en.wikipedia.org/wiki/Factor_theorem",
      "text": "In [[algebra]], the '''factor theorem''' is a theorem linking factors and [[Zero of a function|zeros]] of a [[polynomial]]. It is a [[special case]] of the [[polynomial remainder theorem]].<ref>{{citation|first=Michael|last=Sullivan|title=Algebra and Trigonometry|page=381|publisher=Prentice Hall|year=1996|isbn=0-13-370149-2}}.</ref>\n\nThe factor theorem states that a polynomial <math>f(x)</math> has a factor <math>(x - k)</math> [[if and only if]] <math>f(k)=0</math> (i.e. <math>k</math> is a root).<ref>{{citation|first1=V K|last1=Sehgal|first2=Sonal|last2=Gupta|title=Longman ICSE Mathematics Class 10|page=119|publisher=Dorling Kindersley (India)|isbn=978-81-317-2816-1}}.</ref>\n\n==Factorization of polynomials==\n{{Main|Factorization of polynomials}}\nTwo problems where the factor theorem is commonly applied are those of factoring a polynomial and finding the roots of a polynomial equation; it is a direct consequence of the theorem that these problems are essentially equivalent.\n\nThe factor theorem is also used to remove known zeros from a polynomial while leaving all unknown zeros intact, thus producing a lower degree polynomial whose zeros may be easier to find. Abstractly, the method is as follows:<ref>{{citation|first=R. K.|last=Bansal|title=Comprehensive Mathematics IX|page=142|publisher=Laxmi Publications|isbn=81-7008-629-9}}.</ref>\n# \"Guess\" a zero <math>a</math> of the polynomial <math>f</math>. (In general, this can be ''very hard'', but maths textbook problems that involve solving a polynomial equation are often designed so that some roots are easy to discover.)\n# Use the factor theorem to conclude that <math>(x-a)</math> is a factor of <math>f(x)</math>.\n# Compute the polynomial <math> g(x) = f(x) \\big/ (x-a) </math>, for example using [[polynomial long division]] or [[synthetic division]].\n# Conclude that any root <math>x \\neq a</math> of <math>f(x)=0</math> is a root of <math>g(x)=0</math>. Since the [[polynomial degree]] of <math>g</math> is one less than that of <math>f</math>, it is \"simpler\" to find the remaining zeros by studying <math>g</math>.\n\n===Example===\nFind the factors of\n: <math>x^3 + 7x^2 + 8x + 2.</math>\n\nTo do this one would use trial and error (or the [[rational root theorem]]) to find the first x value that causes the expression to equal zero.  To find out if <math>(x - 1)</math> is a factor, substitute <math>x = 1</math> into the polynomial above:\n: <math>x^3 + 7x^2 + 8x + 2 = (1)^3 + 7(1)^2 + 8(1) + 2</math>\n: <math>= 1 + 7 + 8 + 2</math>\n: <math>= 18.</math>\n\nAs this is equal to 18 and not 0 this means <math>(x - 1)</math> is not a factor of <math>x^3 + 7x^2 + 8x + 2</math>. So, we next try <math>(x + 1)</math> (substituting <math>x = -1</math> into the polynomial):\n: <math>(-1)^3 + 7(-1)^2 + 8(-1) + 2.</math>\n\nThis is equal to <math>0</math>. Therefore <math>x-(-1)</math>, which is to say <math>x+1</math>, is a factor, and <math>-1</math> is a [[Root of a function|root]] of <math>x^3 + 7x^2 + 8x + 2.</math>\n\nThe next two roots can be found by algebraically dividing <math>x^3 + 7x^2 + 8x + 2</math> by <math>(x+1)</math> to get a quadratic:\n\n: <math>{x^3 + 7x^2 + 8x + 2 \\over x + 1} = x^2 + 6x + 2,</math>\n\nand therefore <math>(x+1)</math> and <math>x^2 + 6x + 2</math> are factors of <math>x^3 + 7x^2 + 8x + 2.</math> Of these the quadratic factor can be further factored using the [[quadratic formula]], which gives as roots of the quadratic <math>-3\\pm \\sqrt{7}.</math> Thus the three [[polynomial factorization|irreducible factors]] of the original polynomial are <math>x+1, </math> <math>x-(-3+\\sqrt{7}),</math> and <math>x-(-3-\\sqrt{7}).</math>\n==References==\n{{reflist}}\n \n\n[[Category:Theorems in algebra]]"
    },
    {
      "title": "Frobenius determinant theorem",
      "url": "https://en.wikipedia.org/wiki/Frobenius_determinant_theorem",
      "text": "In mathematics, the '''Frobenius determinant theorem''' was a conjecture made in 1896 by the mathematician [[Richard Dedekind]], who wrote a letter to [[Ferdinand Georg Frobenius|F. G. Frobenius]] about it (reproduced in {{harv|Dedekind|1968}}, with an English translation in {{harv|Curtis|2003|loc=p.&nbsp;51}}).\n\nIf one takes the multiplication table of a finite [[Group (mathematics)|group]] ''G'' and replaces each entry ''g'' with the variable ''x''<sub>''g''</sub>, and subsequently takes the [[determinant]], then the determinant factors as a product of ''n'' irreducible polynomials, where ''n'' is the number of conjugacy classes.  Moreover, each polynomial is raised to a power equal to its degree.  Frobenius proved this surprising conjecture, and it became known as the Frobenius determinant theorem.\n\n==Formal statement==\nLet a [[finite group]] <math>G</math> have elements <math>g_1, g_2,\\dots,g_n</math>, and let <math>x_{g_i}</math> be associated with each element of <math>G</math>. Define the matrix <math>X_G</math> with entries <math>a_{ij}=x_{g_i g_j}</math>. Then\n\n: <math> \\det X_G = \\prod_{j=1}^r P_j(x_{g_1},x_{g_2},\\dots,x_{g_n})^{\\deg P_j}</math>\n\nwhere ''r'' is the number of conjugacy classes of&nbsp;''G''.<ref>{{harvnb|Etingof|loc=Theorem 5.4.}}</ref>\n\n==References==\n{{reflist}}\n*{{Citation | last1=Curtis | first1=Charles W. | authorlink = Charles W. Curtis | title=Pioneers of Representation Theory: Frobenius, Burnside, Schur, and Brauer | url=https://books.google.com/books?isbn=0821826778 | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=History of Mathematics | isbn=978-0-8218-2677-5 |doi=10.1090/S0273-0979-00-00867-3  \t   | mr=1715145 | year=2003}}  [http://www.ams.org/journals/bull/2000-37-03/S0273-0979-00-00867-3/ Review]\n*{{Citation | last1=Dedekind | first1=Richard | author1-link=Richard Dedekind | editor1-last=Fricke | editor1-first=Robert | editor2-last=Noether | editor2-first=Emmy | editor2-link=Emmy Noether | editor3-last=Ore | editor3-first=öystein | title=Gesammelte mathematische Werke. Bände I--III | origyear=1931 | publisher=Chelsea Publishing Co. | location=New York | mr=0237282 |jfm=56.0024.05 | year=1968}}\n*Etingof, Pavel. ''[http://www-math.mit.edu/~etingof/cltrunc.pdf Lectures on Representation Theory]''. \n*{{Citation | last1=Frobenius | first1=Ferdinand Georg | author1-link=Ferdinand Georg Frobenius | title=Gesammelte Abhandlungen. Bände I, II, III | publisher=[[Springer-Verlag]] | location=Berlin, New York | editor-first= J.-P.|editor-last= Serre | isbn=978-3-540-04120-7 | mr=0235974 | year=1968}}\n\n[[Category:Theorems in algebra]]\n[[Category:Determinants]]\n[[Category:Group theory]]\n[[Category:Matrix theory]]"
    },
    {
      "title": "Frobenius reciprocity theorem",
      "url": "https://en.wikipedia.org/wiki/Frobenius_reciprocity_theorem",
      "text": "#REDIRECT [[Frobenius reciprocity]]\n[[Category:Theorems in algebra]]"
    },
    {
      "title": "Fundamental lemma (Langlands program)",
      "url": "https://en.wikipedia.org/wiki/Fundamental_lemma_%28Langlands_program%29",
      "text": "In the mathematical theory of [[automorphic form]]s, the  '''fundamental lemma''' relates orbital integrals on a [[reductive group]] over a [[local field]] to stable orbital integrals on its [[endoscopic group]]s. It was conjectured by {{harvs|txt|authorlink=Robert Langlands|last=Langlands|year=1983}} in the course of developing the [[Langlands program]].  The fundamental lemma was proved by [[Gérard Laumon]] and [[Ngô Bảo Châu]] in the case of [[unitary group]]s and then by Ngô for general reductive groups, building on a series of important reductions made by [[Jean-Loup Waldspurger]] to the case of [[Lie algebras]]. ''[[Time (magazine)|Time]]'' magazine placed Ngô's proof on the list of the \"Top 10 scientific discoveries of 2009\".<ref>[http://www.time.com/time/specials/packages/article/0,28804,1945379_1944416_1944435,00.html Top 10 Scientific Discoveries of 2009], ''Time''</ref> In 2010, Ngô was awarded the [[Fields medal]] for this proof.\n\n== Motivation and history ==\n\n[[Robert Langlands]] outlined a strategy for proving local and global [[Langlands conjectures]] using the [[Arthur–Selberg trace formula]], but in order for this approach to work, the geometric sides of the trace formula for different groups must be related in a particular way. This relationship takes the form of identities between [[orbital integral]]s on [[reductive group]]s ''G'' and ''H'' over a nonarchimedean [[local field]] ''F'', where the group ''H'', called an [[endoscopic group]] of ''G'', is constructed from ''G'' and some additional data.\n\nThe first case considered was ''G'' = ''SL''<sub>2</sub> {{harv|Labesse|Langlands|1979}}. {{harvs|txt|last1=Langlands|author2-link=Diana Shelstad|last2=Shelstad|year=1987}} then developed the general framework for the theory of endoscopic transfer and formulated specific conjectures. However, during the next two decades only partial progress was made towards proving the fundamental lemma.<ref>Kottwitz and Rogawski for ''U''<sub>3</sub>, Wadspurger for ''SL''<sub>''n''</sub>,  Hales and Weissauer for ''Sp''<sub>4</sub>.</ref><ref>[http://www.newton.ac.uk/programmes/ALT/seminars/051316301.pdf Fundamental Lemma and Hitchin Fibration], Gérard Laumon, May 13, 2009</ref> Harris called it a \"bottleneck limiting progress on a host of arithmetic questions\".<ref>[http://www.institut.math.jussieu.fr/projets/fa/bpFiles/Introduction.pdf INTRODUCTION TO “THE STABLE TRACE FORMULA, SHIMURA VARIETIES, AND ARITHMETIC APPLICATIONS”], p. 1., Michael Harris</ref> Langlands himself, writing on the origins of endoscopy, commented:\n\n{{cquote|... it is not the fundamental lemma as such that is critical for the analytic theory of automorphic forms and for the arithmetic of [[Shimura varieties]]; it is the stabilized (or stable) trace formula, the reduction of the trace formula itself to the stable trace formula for a group and its endoscopic groups, and the stabilization of the [[Grothendieck–Lefschetz formula]]. None of these are possible without the fundamental lemma and its absence rendered progress almost impossible for more than twenty years.<ref>[http://publications.ias.edu/rpl/series.php?series=55 publications.ias.edu<!-- Bot generated title -->]</ref>}}\n\n==Statement==\n\nThe fundamental lemma states that an orbital integral ''O'' for a group ''G'' is equal to a stable orbital integral ''SO'' for an endoscopic group ''H'', up to a transfer factor Δ {{harv|Nadler|2012}}:\n:<math>SO_{\\gamma_H}(1_{K_H}) = \\Delta(\\gamma_H,\\gamma_G)O^\\kappa_{\\gamma_G}(1_{K_G})</math>\nwhere\n*''F'' is a local field\n*''G'' is an unramified group defined over ''F'', in other words a quasi-split reductive group defined over ''F'' that splits over an unramified extension of ''F''\n*''H'' is an unramified endoscopic group of ''G'' associated to κ\n*''K''<sub>''G''</sub> and ''K''<sub>''H''</sub> are hyperspecial maximal compact subgroups of ''G'' and ''H'', which means roughly that they are the subgroups of points with coefficients in the ring of integers of ''F''.\n*1<sub>''K''<sub>''G''</sub></sub> and 1<sub>''K''<sub>''H''</sub></sub> are the characteristic functions of ''K''<sub>''G''</sub> and ''K''<sub>''H''</sub>.\n*Δ(γ<sub>''H''</sub>,γ<sub>''G''</sub>) is a transfer factor, a certain elementary expression depending on γ<sub>''H''</sub> and γ<sub>''G''</sub>\n*γ<sub>''H''</sub> and  γ<sub>''G''</sub> are elements of ''G'' and ''H'' representing stable conjugacy classes, such that the stable conjugacy class of ''G'' is the transfer of the stable conjugacy class of ''H''.\n*κ is a character of the group of conjugacy classes in the stable conjugacy class of γ<sub>''G''</sub>\n*''SO'' and ''O'' are stable orbital integrals and orbital integrals depending on their parameters.\n\n== Approaches ==\n\n{{harvtxt|Shelstad|1982}} proved the fundamental lemma for Archimedean fields.\n\n{{harvtxt|Waldspurger|1991}} verified the fundamental lemma for general linear groups.\n\n{{harvtxt|Kottwitz|1992}} and {{harvtxt|Blasius|Rogawski|1992}} verified some cases of the fundamental lemma for 3-dimensional unitary groups.\n\n{{harvtxt|Hales|1997}} and {{harvtxt|Weissauer|2009}} verified the fundamental lemma for the symplectic and general symplectic groups Sp<sub>4</sub>, GSp<sub>4</sub>.\n\nA paper of [[George Lusztig]] and [[David Kazhdan]] pointed out that orbital integrals could be interpreted as counting points on certain algebraic varieties over finite fields. Further, the integrals in question can be computed in a way that depends only on the residue field of ''F''; and the issue can be reduced to the Lie algebra version of the orbital integrals. Then the problem was restated in terms of the [[Springer fiber]] of algebraic groups.<ref>[http://www.claymath.org/research_award/Laumon-Ngo/laumon.pdf The Fundamental Lemma for Unitary Groups] {{webarchive|url=https://web.archive.org/web/20100612234411/http://claymath.org/research_award/Laumon-Ngo/laumon.pdf |date=2010-06-12 }}, at p. 12., Gérard Laumon</ref> The circle of ideas was connected to a [[purity conjecture]]; Laumon gave a conditional proof based on such a conjecture, for unitary groups. {{harvs|txt|last1=Laumon|last2=Ngô|year=2008}} then proved the fundamental lemma for unitary groups, using  [[Hitchin fibration]] introduced by {{harvs|txt|last=Ngô|year=2006}}, which is an abstract geometric analogue of the [[Hitchin system]] of complex algebraic geometry.\n{{harvtxt|Waldspurger|2006}} showed for Lie algebras that the function field case implies the fundamental lemma over all local fields, and {{harvtxt|Waldspurger|2008}} showed that the fundamental lemma for Lie algebras implies the fundamental lemma for groups.\n\n==Notes==\n{{reflist}}\n\n== References ==\n\n*{{Citation | last1=Blasius | first1=Don | last2=Rogawski | first2=Jonathan D. | editor1-last=Langlands | editor1-first=Robert P. | editor2-last=Ramakrishnan | editor2-first=Dinakar | title=The zeta functions of Picard modular surfaces | publisher=Univ. Montréal | location=Montreal, QC | isbn=978-2-921120-08-1  | mr=1155234  | year=1992 | chapter=Fundamental lemmas for U(3) and related groups | pages=363–394}}\n*{{citation|last=Casselman|first=W.|title=Langlands' Fundamental Lemma for SL(2)|url=http://www.math.ubc.ca/~cass/research/pdf/SL2.pdf|year=2009}}\n*{{citation|first=Jean-François|last=Dat|url=http://www.math.univ-paris13.fr/~dat/publis/lf.pdf|title=Lemme fondamental et endoscopie, une approche géométrique, d'après Gérard Laumon et Ngô Bao Châu|publisher=[[Bourbaki seminar|Séminaire Bourbaki]], no 940|date=November 2004}}\n*{{Citation | last1=Hales | first1=Thomas C. | title=The fundamental lemma for Sp(4) | doi=10.1090/S0002-9939-97-03546-6 | mr=1346977  | year=1997 | journal=[[Proceedings of the American Mathematical Society]] | issn=0002-9939 | volume=125 | issue=1 | pages=301–308}}\n*{{citation|url=http://www.institut.math.jussieu.fr/projets/fa/bp0.html|editor-last=Harris|editor-first=M.|title=Stabilisation de la formule des traces, variétés de Shimura, et applications arithmétiques}}\n*{{Citation | last1=Kazhdan | first1=David | last2=Lusztig | first2=George | title=Fixed point varieties on affine flag manifolds | doi=10.1007/BF02787119 | mr=947819  | year=1988 | journal=Israel Journal of Mathematics | issn=0021-2172 | volume=62 | issue=2 | pages=129–168}}\n*{{Citation | last1=Kottwitz | first1=Robert E. | authorlink1=Robert Kottwitz | editor1-last=Langlands | editor1-first=Robert P. | editor2-last=Ramakrishnan | editor2-first=Dinakar | title=The zeta functions of Picard modular surfaces | publisher=Univ. Montréal | location=Montreal, QC | isbn=978-2-921120-08-1  | mr=1155233  | year=1992 | chapter=Calculation of some orbital integrals | pages=349–362}}\n*{{Citation | last1=Labesse | first1=Jean-Pierre | last2=Langlands | first2=R. P. | title=L-indistinguishability for SL(2) | doi=10.4153/CJM-1979-070-3 | mr=540902  | year=1979 | journal=[[Canadian Journal of Mathematics]] | issn=0008-414X | volume=31 | issue=4 | pages=726–785}}\n*{{Citation | last1=Langlands | first1=Robert P. | title=Les débuts d'une formule des traces stable | url=http://www.sunsite.ubc.ca/DigitalMathArchive/Langlands/endoscopy.html#debuts | publisher=Université de Paris VII U.E.R. de Mathématiques | location=Paris | series=Publications Mathématiques de l'Université Paris VII [Mathematical Publications of the University of Paris VII] | mr=697567  | year=1983 | volume=13}}\n*{{Citation | last1=Langlands | first1=Robert P. | last2=Shelstad | first2=Diana | title=On the definition of transfer factors | doi=10.1007/BF01458070 | mr=909227  | year=1987 | journal=[[Mathematische Annalen]] | issn=0025-5831 | volume=278 | issue=1 | pages=219–271}}\n*{{Citation | last1=Laumon | first1=Gérard | title=International Congress of Mathematicians. Vol. II | url=http://mathunion.org/ICM/ICM2006.2/ | publisher=Eur. Math. Soc., Zürich | mr=2275603  | year=2006 | chapter=Aspects géométriques du Lemme Fondamental de Langlands-Shelstad | pages=401–419}}\n*{{Citation | last1=Laumon | first1=Gérard | last2=Ngô | first2=Bao Châu | title=Le lemme fondamental pour les groupes unitaires | doi=10.4007/annals.2008.168.477 | mr=2434884  | year=2008 | journal=[[Annals of Mathematics]] |series=Second Series | issn=0003-486X | volume=168 | issue=2 | pages=477–573| arxiv=math/0404454 }}\n*{{Citation | last1=Nadler | first1=David | title=The geometric nature of the fundamental lemma | doi=10.1090/S0273-0979-2011-01342-8  | year=2012 | journal=[[Bulletin of the American Mathematical Society]] | issn=0002-9904 | volume=49 | pages= 1–50 | arxiv=1009.1862 }}\n*{{Citation | last1=Ngô | first1=Bao Châu | title=Fibration de Hitchin et endoscopie | doi=10.1007/s00222-005-0483-7 | mr=2218781  | year=2006 | journal=[[Inventiones Mathematicae]] | issn=0020-9910 | volume=164 | issue=2 | pages=399–453| bibcode=2006InMat.164..399N | arxiv=math/0406599 }}\n*{{Citation | last1=Ngô | first1=Bao Châu | title=Le lemme fondamental pour les algèbres de Lie | doi=10.1007/s10240-010-0026-7 | mr=2653248  | year=2010 | journal=Institut des Hautes Études Scientifiques. Publications Mathématiques | issn=0073-8301 | volume=111 | pages=1–169}}\n*{{Citation | last1=Shelstad | first1=Diana | title=L-indistinguishability for real groups | doi=10.1007/BF01456950 | mr=661206  | year=1982 | journal=[[Mathematische Annalen]] | issn=0025-5831 | volume=259 | issue=3 | pages=385–430}}\n*{{Citation | last1=Waldspurger | first1=Jean-Loup | title=Sur les intégrales orbitales tordues pour les groupes linéaires: un lemme fondamental | doi=10.4153/CJM-1991-049-5 | mr=1127034  | year=1991 | journal=[[Canadian Journal of Mathematics]] | issn=0008-414X | volume=43 | issue=4 | pages=852–896}}\n*{{Citation | last1=Waldspurger | first1=Jean-Loup | title=Endoscopie et changement de caractéristique | doi=10.1017/S1474748006000041 | mr=2241929  | year=2006 | journal=Journal of the Institute of Mathematics of Jussieu. JIMJ. Journal de l'Institut de Mathématiques de Jussieu | issn=1474-7480 | volume=5 | issue=3 | pages=423–525}}\n*{{Citation | last1=Waldspurger | first1=Jean-Loup | title=L'endoscopie tordue n'est pas si tordue | url=http://www.math.jussieu.fr/~waldspur/endoscopietordue.pdf | publisher=[[American Mathematical Society]] | location=Providence, R.I. | isbn=978-0-8218-4469-4 | mr=2418405  | year=2008 | journal=Memoirs of the American Mathematical Society | issn=0065-9266 | volume=194 | issue=908 | pages=261 | trans-title=Twisted endoscopy is not so twisted | language=fr}}\n*{{Citation | last1=Weissauer | first1=Rainer | title=Endoscopy for GSp(4) and the cohomology of Siegel modular threefolds | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Lecture Notes in Mathematics | isbn=978-3-540-89305-9 | doi=10.1007/978-3-540-89306-6 | mr=2498783  | year=2009 | volume=1968}}\n\n== External links ==\n* [http://www.fields.utoronto.ca/audio/02-03/shimura/laumon/ Gerard Laumon lecture on the fundamental lemma for unitary groups]\n* {{cite journal |last=Basken |first=Paul |url=http://chronicle.com/article/Understanding-the-Langlands/124368/ |title=Understanding the Langlands Fundamental Lemma |journal=[[The Chronicle of Higher Education]] |date=September 12, 2010}}\n\n[[Category:Algebraic groups]]\n[[Category:Automorphic forms]]\n[[Category:Theorems in algebra]]\n[[Category:Theorems in number theory]]\n[[Category:Langlands program]]"
    },
    {
      "title": "Fundamental theorem of algebra",
      "url": "https://en.wikipedia.org/wiki/Fundamental_theorem_of_algebra",
      "text": "{{short description|Theorem that guarantees the existence of a complex root of a complex polynomial}}\n{{Distinguish|Fundamental theorem of arithmetic}}\nThe '''[[fundamental theorem]] of [[algebra]]''' states that every non-[[constant polynomial|constant]] single-variable [[polynomial]] with [[Complex number|complex]] [[coefficient]]s has at least one complex [[root of a function|root]]. This includes polynomials with real coefficients, since every real number can be considered a complex number with its [[imaginary part]] equal to zero.\n\nEquivalently (by definition), the theorem states that the [[field (mathematics)|field]] of [[complex number]]s is [[algebraically closed field|algebraically closed]].\n\nThe theorem is also stated as follows: every non-zero, single-variable, [[Degree of a polynomial|degree]] ''n'' polynomial with complex coefficients has, counted with [[Multiplicity (mathematics)#Multiplicity of a root of a polynomial|multiplicity]], exactly ''n'' complex roots. The equivalence of the two statements can be proven through the use of successive [[polynomial division]].\n\nIn spite of its name, there is no purely algebraic proof of the theorem, since any proof must use some form of [[Completeness of the real numbers|completeness]], which is not an algebraic concept.<ref>Even the proof that the equation <math>x^2-2=0</math> has a solution involves the [[construction of the real numbers|definition of the real numbers]] through some form of completeness.</ref>{{Citation needed|date=December 2018}} Additionally, it is not fundamental for [[modern algebra]]; its name was given at a time when algebra was synonymous with [[theory of equations]].\n\n==History==\nPeter Roth, in his book ''Arithmetica Philosophica'' (published in 1608, at Nürnberg, by Johann Lantzenberger),<ref>[http://www.e-rara.ch/doi/10.3931/e-rara-4843 Rare books]</ref> wrote that a polynomial equation of degree ''n'' (with real coefficients) ''may'' have ''n'' solutions. [[Albert Girard]], in his book ''L'invention nouvelle en l'Algèbre'' (published in 1629), asserted that a polynomial equation of degree ''n'' has ''n'' solutions, but he did not state that they had to be real numbers. Furthermore, he added that his assertion holds \"unless the equation is incomplete\", by which he meant that no coefficient is equal to 0. However, when he explains in detail what he means, it is clear that he actually believes that his assertion is always true; for instance, he shows that the equation <math>x^4 = 4x-3,</math> although incomplete, has four solutions (counting multiplicities): 1 (twice), <math>-1+i\\sqrt{2},</math> and <math>-1-i\\sqrt{2}.</math>\n\nAs will be mentioned again below, it follows from the fundamental theorem of algebra that every non-constant polynomial with real coefficients can be written as a product of polynomials with real coefficients whose degrees are either 1 or 2. However, in 1702 [[Gottfried Leibniz|Leibniz]] said that no polynomial of the type ''x''<sup>4</sup>&nbsp;+&nbsp;''a''<sup>4</sup> (with ''a'' real and distinct from 0) can be written in such a way. Later, [[Nicolaus I Bernoulli|Nikolaus Bernoulli]] made the same assertion concerning the polynomial ''x''<sup>4</sup>&nbsp;−&nbsp;4''x''<sup>3</sup>&nbsp;+&nbsp;2''x''<sup>2</sup>&nbsp;+&nbsp;4''x''&nbsp;+&nbsp;4, but he got a letter from [[Leonhard Euler|Euler]] in 1742<ref>See section ''Le rôle d'Euler'' in C. Gilain's article ''Sur l'histoire du théorème fondamental de l'algèbre: théorie des équations et calcul intégral''.</ref> in which he was told that his polynomial happened to be equal to\n\n:<math>\\left (x^2-(2+\\alpha)x+1+\\sqrt{7}+\\alpha \\right ) \\left (x^2-(2-\\alpha)x+1+\\sqrt{7}-\\alpha \\right ), \\qquad \\alpha = \\sqrt{4+2\\sqrt{7}}.</math>\n\nAlso, Euler mentioned that\n\n:<math>x^4+a^4= \\left (x^2+a\\sqrt{2}\\cdot x+a^2 \\right ) \\left (x^2-a\\sqrt{2}\\cdot x+a^2 \\right ).</math>\n\nA first attempt at proving the theorem was made by [[Jean le Rond d'Alembert|d'Alembert]] in 1746, but his proof was incomplete. Among other problems, it assumed implicitly a theorem (now known as [[Puiseux's theorem]]) which would not be proved until more than a century later, and furthermore the proof assumed the fundamental theorem of algebra. Other attempts were made by [[Leonhard Euler|Euler]] (1749), [[François Daviet de Foncenex|de Foncenex]] (1759), [[Joseph Louis Lagrange|Lagrange]] (1772), and [[Pierre-Simon Laplace|Laplace]] (1795). These last four attempts assumed implicitly Girard's assertion; to be more precise, the existence of solutions was assumed and all that remained to be proved was that their form was ''a''&nbsp;+&nbsp;''bi'' for some real numbers ''a'' and ''b''. In modern terms, Euler, de Foncenex, Lagrange, and Laplace were assuming the existence of a [[splitting field]] of the polynomial ''p''(''z'').\n\nAt the end of the 18th century, two new proofs were published which did not assume the existence of roots, but neither of which was complete. One of them, due to James Wood and mainly algebraic, was published in 1798 and it was totally ignored. Wood's proof had an algebraic gap.<ref>Concerning Wood's proof, see the article ''A forgotten paper on the fundamental theorem of algebra'', by Frank Smithies.</ref> The other one was published by [[Carl Friedrich Gauss|Gauss]] in 1799 and it was mainly geometric, but it had a topological gap, filled by [[Alexander Ostrowski]] in 1920, as discussed in Smale 1981 [http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.bams/1183547848] (Smale writes, \"...I wish to point out what an immense gap Gauss' proof contained. It is a subtle point even today that a real algebraic plane curve cannot enter a disk without leaving. In fact even though Gauss redid this proof 50 years later, the gap remained. It was not until 1920 that Gauss' proof was completed. In the reference Gauss, A. Ostrowski has a paper which does this and gives an excellent discussion of the problem as well...\"). A rigorous proof was first published by [[Jean-Robert Argand|Argand]] in 1806 (and revisited in 1813);<ref> {{MacTutor Biography|id=Argand|title=Jean-Robert Argand}}</ref> it was here that, for the first time, the fundamental theorem of algebra was stated for polynomials with complex coefficients, rather than just real coefficients. Gauss produced two other proofs in 1816 and another version of his original proof in 1849.\n\nThe first textbook containing a proof of the theorem was [[Cauchy]]'s ''Cours d'analyse de l'École Royale Polytechnique'' (1821). It contained Argand's proof, although [[Jean Robert Argand|Argand]] is not credited for it.\n\nNone of the proofs mentioned so far is [[Constructivism (mathematics)|constructive]]. It was [[Weierstrass]] who raised for the first time, in the middle of the 19th century, the problem of finding a [[constructive proof]] of the fundamental theorem of algebra. He presented his solution, that amounts in modern terms to a combination of the [[Durand–Kerner method]] with the [[homotopy continuation]] principle, in 1891. Another proof of this kind was obtained by [[Hellmuth Kneser]] in 1940 and simplified by his son [[Martin Kneser]] in 1981.\n\nWithout using [[countable choice]], it is not possible to constructively prove the fundamental theorem of algebra for complex numbers based on the [[construction of the real numbers|Dedekind real numbers]] (which are not constructively equivalent to the Cauchy real numbers without countable choice<ref>For the minimum necessary to prove their equivalence, see Bridges, Schuster, and Richman; 1998; <cite>A weak countable choice principle</cite>; available from [http://math.fau.edu/richman/HTML/DOCS.HTM].</ref>). However, [[Fred Richman]] proved a reformulated version of the theorem that does work.<ref>See Fred Richman; 1998; <cite>The fundamental theorem of algebra: a constructive development without choice</cite>; available from [http://math.fau.edu/richman/HTML/DOCS.HTM].</ref>\n\n==Proofs==\nAll proofs below involve some [[mathematical analysis|analysis]], or at least the [[Topology|topological]] concept of [[continuous function|continuity]] of real or complex functions. Some also use [[Derivative|differentiable]] or even [[Analytic function|analytic]] functions. This fact has led to the remark that the Fundamental Theorem of Algebra is neither fundamental, nor a theorem of algebra.{{Citation needed|reason=Who made the remark?|date=February 2016}}\n\nSome proofs of the theorem only prove that any non-constant polynomial with '''real''' coefficients has some complex root. This is enough to establish the theorem in the general case because, given a non-constant polynomial ''p''(''z'') with complex coefficients, the polynomial\n\n:<math>q(z)=p(z)\\overline{p(\\overline z)}</math>\n\nhas only real coefficients and, if ''z'' is a zero of ''q''(''z''), then either ''z'' or its conjugate is a root of ''p''(''z'').\n\nA large number of non-algebraic proofs of the theorem use the fact (sometimes called \"growth lemma\") that an ''n''-th degree polynomial function ''p''(''z'') whose dominant coefficient is 1 behaves like ''z<sup>n</sup>'' when |''z''| is large enough. A more precise statement is: there is some positive real number ''R'' such that:\n\n:<math>\\tfrac{1}{2}|z^n|<|p(z)|<\\tfrac{3}{2}|z^n|</math>\n\nwhen |''z''|&nbsp;>&nbsp;''R''.\n\n===Complex-analytic proofs===\nFind a closed [[disk (mathematics)|disk]] ''D'' of radius ''r'' centered at the origin such that |''p''(''z'')|&nbsp;>&nbsp;|''p''(0)| whenever |''z''|&nbsp;≥&nbsp;''r''. The minimum of |''p''(''z'')| on ''D'', which must exist since ''D'' is [[compact set|compact]], is therefore achieved at some point ''z''<sub>0</sub> in the interior of ''D'', but not at any point of its boundary. The [[Maximum modulus principle]] (applied to 1/''p''(''z'')) implies then that ''p''(''z''<sub>0</sub>)&nbsp;=&nbsp;0. In other words, ''z''<sub>0</sub> is a zero of ''p''(''z'').\n\n'''A variation of this proof''' does not require the use of the maximum modulus principle (in fact, the same argument with minor changes also gives a proof of the maximum modulus principle for holomorphic functions). If we assume by contradiction that ''a'' := ''p''(''z''<sub>0</sub>) ≠ 0, then, expanding ''p''(''z'') in powers of ''z'' − ''z''<sub>0</sub> we can write\n\n:<math>p(z) = a + c_k (z-z_0)^k + c_{k+1} (z-z_0)^{k+1} + \\cdots + c_n (z-z_0)^n.</math>\n\nHere, the ''c<sub>j</sub>'' are simply the coefficients of the polynomial ''z'' → ''p''(''z'' + ''z''<sub>0</sub>), and we let ''k'' be the index of the first coefficient following the constant term that is non-zero. But now we see that for ''z'' sufficiently close to ''z''<sub>0</sub> this has behavior asymptotically similar to the simpler polynomial <math>q(z) = a+c_k (z-z_0)^k</math>, \n\nin the sense that (as is easy to check) the function\n\n:<math>\\left|\\frac{p(z)-q(z)}{(z-z_0)^{k+1}}\\right|</math> \n\nis bounded by some positive constant ''M'' in some neighborhood of ''z''<sub>0</sub>. Therefore if we define <math>\\theta_0 = (\\arg(a)+\\pi-\\arg(c_k)) /k</math> and let <math>z = z_0 + r e^{i \\theta_0}</math>, then for any sufficiently small positive number ''r'' (so that the bound ''M'' mentioned above holds), using the triangle inequality we see that\n\n:<math>\\begin{align}\n|p(z)| &\\le |q(z)| + r^{k+1} \\left|\\frac{p(z)-q(z)}{r^{k+1}}\\right|\\\\[4pt]\n&\\le \\left|a +(-1)c_k r^k e^{i(\\arg(a)-\\arg(c_k))}\\right| + M r^{k+1} \\\\[4pt]\n&= |a|-|c_k|r^k + M r^{k+1}\n\\end{align}</math>\n\nWhen ''r'' is sufficiently close to 0 this upper bound for |''p''(''z'')| is strictly smaller than |''a''|, in contradiction to the definition of ''z''<sub>0</sub>. (Geometrically, we have found an explicit direction θ<sub>0</sub> such that if one approaches ''z''<sub>0</sub> from that direction one can obtain values ''p''(''z'') smaller in absolute value than |''p''(''z''<sub>0</sub>)|.)\n\n'''Another''' analytic proof can be obtained along this line of thought observing that, since |''p''(''z'')|&nbsp;>&nbsp;|''p''(0)| outside ''D'', the minimum of |''p''(''z'')| on the whole complex plane is achieved at ''z''<sub>0</sub>. If |''p''(''z''<sub>0</sub>)|&nbsp;>&nbsp;0, then 1/''p'' is a bounded [[holomorphic function]] in the entire complex plane since, for each complex number ''z'', |1/''p''(''z'')|&nbsp;≤&nbsp;|1/''p''(''z''<sub>0</sub>)|. Applying [[Liouville's theorem (complex analysis)|Liouville's theorem]], which states that a bounded entire function must be constant, this would imply that 1/''p'' is constant and therefore that ''p'' is constant. This gives a contradiction, and hence ''p''(''z''<sub>0</sub>)&nbsp;=&nbsp;0.\n\n'''Yet another''' analytic proof uses the [[argument principle]]. Let ''R'' be a positive real number large enough so that every root of ''p''(''z'') has absolute value smaller than ''R''; such a number must exist because every non-constant polynomial function of degree ''n'' has at most ''n'' zeros. For each ''r''&nbsp;>&nbsp;''R'', consider the number\n\n:<math>\\frac{1}{2\\pi i}\\int_{c(r)}\\frac{p'(z)}{p(z)}\\,dz,</math>\n\nwhere ''c''(''r'') is the circle centered at 0 with radius ''r'' oriented counterclockwise; then the [[argument principle]] says that this number is the number ''N'' of zeros of ''p''(''z'') in the open ball centered at 0 with radius ''r'', which, since ''r''&nbsp;>&nbsp;''R'', is the total number of zeros of ''p''(''z''). On the other hand, the integral of ''n''/''z'' along ''c''(''r'') divided by 2π''i'' is equal to ''n''. But the difference between the two numbers is\n\n:<math>\\frac{1}{2\\pi i}\\int_{c(r)}\\left(\\frac{p'(z)}{p(z)}-\\frac{n}{z}\\right)dz=\\frac{1}{2\\pi i}\\int_{c(r)}\\frac{zp'(z)-np(z)}{zp(z)}\\,dz.</math>\n\nThe numerator of the rational expression being integrated has degree at most ''n''&nbsp;−&nbsp;1 and the degree of the denominator is ''n''&nbsp;+&nbsp;1. Therefore, the number above tends to 0 as ''r'' → +∞. But the number is also equal to ''N''&nbsp;−&nbsp;''n'' and so ''N''&nbsp;=&nbsp;''n''.\n\n'''Still another''' complex-analytic proof can be given by combining [[linear algebra]] with the [[Cauchy's integral theorem|Cauchy theorem]]. To establish that every complex polynomial of degree ''n''&nbsp;>&nbsp;0 has a zero, it suffices to show that every complex [[square matrix]] of size ''n''&nbsp;>&nbsp;0 has a (complex) [[eigenvalue]].<ref>A proof of the fact that this suffices can be seen [[Algebraically closed field#Every endomorphism of Fn has some eigenvector|here]].</ref> The proof of the latter statement is [[Proof by contradiction|by contradiction]].\n\nLet ''A'' be a complex square matrix of size ''n''&nbsp;>&nbsp;0 and let ''I<sub>n</sub>'' be the unit matrix of the same size. Assume ''A'' has no eigenvalues. Consider the [[resolvent formalism|resolvent]] function\n\n:<math> R(z)=(zI_n-A)^{-1},</math>\n\nwhich is a [[meromorphic function]] on the complex plane with values in the vector space of matrices. The eigenvalues of ''A'' are precisely the poles of ''R''(''z''). Since, by assumption, ''A'' has no eigenvalues, the function ''R''(''z'') is an [[entire function]] and [[Cauchy's integral theorem|Cauchy theorem]] implies that\n\n:<math> \\int_{c(r)} R(z) \\, dz =0.</math>\n\nOn the other hand, ''R''(''z'') expanded as a geometric series gives:\n\n:<math>R(z)=z^{-1}(I_n-z^{-1}A)^{-1}=z^{-1}\\sum_{k=0}^\\infty \\frac{1}{z^k}A^k\\cdot</math>\n\nThis formula is valid outside the closed [[disc (mathematics)|disc]] of radius <math>\\|A\\|</math> (the [[operator norm]] of ''A''). Let <math>r>\\|A\\|.</math> Then\n\n:<math>\\int_{c(r)}R(z)dz=\\sum_{k=0}^{\\infty}\\int_{c(r)}\\frac{dz}{z^{k+1}}A^k=2\\pi iI_n</math>\n\n(in which only the summand ''k''&nbsp;=&nbsp;0 has a nonzero integral). This is a contradiction, and so ''A'' has an eigenvalue.\n\n'''Finally''', [[Rouché's theorem]] gives perhaps the shortest proof of the theorem.\n\n===Topological proofs===\nSuppose the minimum of |''p''(''z'')| on the whole complex plane is achieved at ''z''<sub>0</sub>; it was seen at the proof which uses Liouville's theorem that such a number must exist. We can write ''p''(''z'') as a polynomial in ''z''&nbsp;−&nbsp;''z''<sub>0</sub>: there is some natural number ''k'' and there are some complex numbers ''c<sub>k</sub>'', ''c''<sub>''k''&nbsp;+&nbsp;1</sub>, ..., ''c<sub>n</sub>'' such that ''c<sub>k</sub>''&nbsp;≠&nbsp;0 and:\n\n:<math>p(z)=p(z_0)+c_k(z-z_0)^k+c_{k+1}(z-z_0)^{k+1}+ \\cdots +c_n(z-z_0)^n.</math>\n\nIf ''p''(''z''<sub>0</sub>) is nonzero, it follows that if ''a'' is a ''k''<sup>th</sup> root of −''p''(''z''<sub>0</sub>)/''c<sub>k</sub>'' and if ''t'' is positive and sufficiently small, then |''p''(''z''<sub>0</sub>&nbsp;+&nbsp;''ta'')|&nbsp;<&nbsp;|''p''(''z''<sub>0</sub>)|, which is impossible, since |''p''(''z''<sub>0</sub>)| is the minimum of |''p''| on ''D''.\n\nFor another topological proof by contradiction, suppose that the polynomial ''p''(''z'') has no roots, and consequently is never equal to 0. Think of the polynomial as a map from the complex plane into the complex plane. It maps any circle |''z''|&nbsp;=&nbsp;''R'' into a closed loop, a curve ''P''(''R''). We will consider what happens to the [[winding number]] of ''P''(''R'') at the extremes when ''R'' is very large and when ''R'' = 0. When ''R'' is a sufficiently large number, then the leading term ''z<sup>n</sup>'' of ''p''(''z'') dominates all other terms combined; in other words, \n\n:<math>\\left | z^n \\right | > \\left | a_{n-1} z^{n-1} + \\cdots + a_0 \\right |.</math>\n\nWhen ''z'' traverses the circle <math>Re^{i\\theta}</math> once counter-clockwise <math>(0\\leq \\theta \\leq 2\\pi),</math> then <math>z^n=Re^{in\\theta}</math> winds ''n'' times counter-clockwise <math>(0\\leq \\theta \\leq 2\\pi n)</math> around the origin (0,0), and ''P''(''R'') likewise. At the other extreme, with |''z''|&nbsp;=&nbsp;0, the curve ''P''(0) is merely the single point ''p''(0), which must be nonzero because ''p''(''z'') is never zero. Thus ''p''(0) must be distinct from the origin (0,0), which denotes 0 in the complex plane. The winding number of ''P''(0) around the origin (0,0) is thus 0. Now changing ''R'' continuously will [[homotopy | deform the loop continuously]]. At some ''R'' the winding number must change. But that can only happen if the curve ''P''(''R'') includes the origin (0,0) for some ''R''. But then for some ''z'' on that circle |''z''|&nbsp;=&nbsp;''R'' we have ''p''(''z'') = 0, contradicting our original assumption. Therefore, ''p''(''z'') has at least one zero.\n\n===Algebraic proofs===\nThese proofs use two facts about real numbers that require only a small amount of analysis (more precisely, the [[intermediate value theorem]]):\n* every polynomial with odd degree and real coefficients has some real root;\n* every non-negative real number has a square root.\n\nThe second fact, together with the [[quadratic formula]], implies the theorem for real quadratic polynomials. In other words, algebraic proofs of the fundamental theorem actually show that if ''R'' is any [[real-closed field]], then its extension ''C'' = ''R''({{radic|−1}}) is algebraically closed.\n\nAs mentioned above, it suffices to check the statement \"every non-constant polynomial ''p''(''z'') with real coefficients has a complex root\". This statement can be proved by induction on the greatest non-negative integer ''k'' such that 2<sup>''k''</sup> divides the degree ''n'' of ''p''(''z''). Let ''a'' be the coefficient of ''z<sup>n</sup>'' in ''p''(''z'') and let ''F'' be a [[splitting field]] of ''p''(''z'') over ''C''; in other words, the field ''F'' contains ''C'' and there are elements ''z''<sub>1</sub>, ''z''<sub>2</sub>, ..., ''z<sub>n</sub>'' in ''F'' such that\n\n:<math>p(z)=a(z-z_1)(z-z_2) \\cdots (z-z_n).</math>\n\nIf ''k''&nbsp;=&nbsp;0, then ''n'' is odd, and therefore ''p''(''z'') has a real root. Now, suppose that ''n''&nbsp;=&nbsp;2''<sup>k</sup>m'' (with ''m'' odd and ''k''&nbsp;>&nbsp;0) and that the theorem is already proved when the degree of the polynomial has the form 2<sup>''k''&nbsp;−&nbsp;1</sup>''m''′ with ''m''′ odd. For a real number ''t'', define:\n\n:<math>q_t(z)=\\prod_{1\\le i<j\\le n}\\left(z-z_i-z_j-tz_iz_j\\right).</math>\n\nThen the coefficients of ''q<sub>t</sub>''(''z'') are [[symmetric polynomial]]s in the ''z<sub>i</sub>'' with real coefficients. Therefore, they can be expressed as polynomials with real coefficients in the [[elementary symmetric polynomial]]s, that is, in −''a''<sub>1</sub>, ''a''<sub>2</sub>, ..., (−1)''<sup>n</sup>a<sub>n</sub>''. So ''q<sub>t</sub>''(''z'') has in fact ''real'' coefficients. Furthermore, the degree of ''q<sub>t</sub>''(''z'') is ''n''(''n''&nbsp;−&nbsp;1)/2&nbsp;=&nbsp;2<sup>''k''−1</sup>''m''(''n''&nbsp;−&nbsp;1), and ''m''(''n''&nbsp;−&nbsp;1) is an odd number. So, using the induction hypothesis, ''q<sub>t</sub>'' has at least one complex root; in other words, ''z<sub>i</sub>''&nbsp;+&nbsp;''z<sub>j</sub>''&nbsp;+&nbsp;''tz<sub>i</sub>z<sub>j</sub>'' is complex for two distinct elements ''i'' and ''j'' from {1, ..., ''n''}. Since there are more real numbers than pairs (''i'', ''j''), one can find distinct real numbers ''t'' and ''s'' such that ''z<sub>i</sub>''&nbsp;+&nbsp;''z<sub>j</sub>''&nbsp;+&nbsp;''tz<sub>i</sub>z<sub>j</sub>'' and ''z<sub>i</sub>''&nbsp;+&nbsp;''z<sub>j</sub>''&nbsp;+&nbsp;''sz<sub>i</sub>z<sub>j</sub>'' are complex (for the same ''i'' and ''j''). So, both ''z<sub>i</sub>''&nbsp;+&nbsp;''z<sub>j</sub>'' and ''z<sub>i</sub>z<sub>j</sub>'' are complex numbers. It is easy to check that every complex number has a complex square root, thus every complex polynomial of degree 2 has a complex root by the quadratic formula. It follows that ''z<sub>i</sub>'' and ''z<sub>j</sub>'' are complex numbers, since they are roots of the quadratic polynomial ''z''<sup>2</sup>&nbsp;−&nbsp; (''z<sub>i</sub>''&nbsp;+&nbsp;''z<sub>j</sub>'')''z''&nbsp;+&nbsp;''z<sub>i</sub>z<sub>j</sub>''.\n\nJoseph Shipman showed in 2007 that the assumption that odd degree polynomials have roots is stronger than necessary; any field in which polynomials of prime degree have roots is algebraically closed (so \"odd\" can be replaced by \"odd prime\" and furthermore this holds for fields of all characteristics). For axiomatization of algebraically closed fields, this is the best possible, as there are counterexamples if a single prime is excluded. However, these counterexamples rely on −1 having a square root. If we take a field where −1 has no square root, and every polynomial of degree ''n''&nbsp;∈&nbsp;''I'' has a root, where ''I'' is any fixed infinite set of odd numbers, then every polynomial ''f''(''x'') of odd degree has a root (since {{nowrap|(''x''<sup>2</sup> + 1)<sup>''k''</sup>''f''(''x'')}} has a root, where ''k'' is chosen so that {{nowrap|deg(''f'') + 2''k'' ∈ ''I''}}). Mohsen Aliabadi generalized Shipman's result for any field in 2013, proving that the sufficient condition for an arbitrary field (of any characteristic) to be algebraically closed is having a root for any polynomial of prime degree.<ref>M. Aliabadi, M. R. Darafsheh, [https://arxiv.org/pdf/1508.00937 On maximal and minimal linear matching property], ''Algebra and discrete mathematics'', Volume 15 (2013). Number 2. pp. 174–178</ref>\n\nAnother algebraic proof of the fundamental theorem can be given using [[Galois theory]]. It suffices to show that '''C''' has no proper finite [[field extension]].<ref>A proof of the fact that this suffices can be seen [[Algebraically closed field#The field has no proper finite extension|here]].</ref> Let ''K''/'''C''' be a finite extension. Since the [[Normal extension#Normal closure|normal closure]] of ''K'' over '''R''' still has a finite degree over '''C''' (or '''R'''), we may assume [[without loss of generality]] that ''K'' is a [[normal extension]] of '''R''' (hence it is a [[Galois extension]], as every algebraic extension of a field of [[characteristic (algebra)|characteristic]] 0 is [[separable extension|separable]]). Let ''G'' be the [[Galois group]] of this extension, and let ''H'' be a [[Sylow theorems|Sylow]] 2-subgroup of ''G'', so that the [[order (group theory)|order]] of ''H'' is a power of 2, and the [[index of a subgroup|index]] of ''H'' in ''G'' is odd. By the [[fundamental theorem of Galois theory]], there exists a subextension ''L'' of ''K''/'''R''' such that Gal(''K''/''L'')&nbsp;=&nbsp;''H''. As [''L'':'''R''']&nbsp;=&nbsp;[''G'':''H''] is odd, and there are no nonlinear irreducible real polynomials of odd degree, we must have ''L''&nbsp;= '''R''', thus [''K'':'''R'''] and [''K'':'''C'''] are powers of 2. Assuming by way of contradiction that [''K'':'''C''']&nbsp;>&nbsp;1, we conclude that the [[p-group|2-group]] Gal(''K''/'''C''') contains a subgroup of index 2, so there exists a subextension ''M'' of '''C''' of degree&nbsp;2. However, '''C''' has no extension of degree&nbsp;2, because every quadratic complex polynomial has a complex root, as mentioned above. This shows that [''K'':'''C'''] = 1, and therefore ''K'' = '''C''', which completes the proof.\n\n===Geometric proofs===\nThere exists still another way to approach the fundamental theorem of algebra, due to J. M. Almira and A. Romero: by [[Riemannian geometry|Riemannian geometric]] arguments. The main idea here is to prove that the existence of a non-constant polynomial ''p''(''z'') without zeros implies the existence of a [[Flat manifold|flat Riemannian metric]] over the sphere '''S'''<sup>2</sup>. This leads to a contradiction, since the sphere is not flat.\n\nA Riemannian surface (''M'', ''g'') is said to be flat if its Gaussian curvature, which we denote by ''K<sub>g</sub>'', is identically null. Now, [[Gauss–Bonnet theorem]], when applied to the sphere '''S'''<sup>2</sup>, claims that\n\n:<math>\\int_{\\mathbf{S}^2}K_g=4\\pi,</math>\n\nwhich proves that the sphere is not flat.\n\nLet us now assume that ''n'' > 0 and \n\n:<math>p(z) = a_0 + a_1 z + \\cdots + a_n z^n \\neq 0</math>\n\nfor each complex number ''z''. Let us define \n\n:<math>p^*(z) = z^n p \\left ( \\tfrac{1}{z} \\right ) = a_0 z^n + a_1 z^{n-1} + \\cdots + a_n.</math>\n\nObviously, ''p*''(''z'')&nbsp;≠ 0 for all ''z'' in '''C'''. Consider the polynomial ''f''(''z'')&nbsp;=&nbsp;''p''(''z'')''p*''(''z''). Then ''f''(''z'')&nbsp;≠ 0 for each ''z'' in '''C'''. Furthermore,\n\n:<math>f(\\tfrac{1}{w}) = p \\left (\\tfrac{1}{w} \\right )p^* \\left (\\tfrac{1}{w} \\right ) = w^{-2n}p^*(w)p(w) = w^{-2n}f(w).</math>\n\nWe can use this functional equation to prove that ''g'', given by\n\n:<math>g=\\frac{1}{|f(w)|^{\\frac{2}{n}}}\\,|dw|^2 </math>\n\nfor ''w'' in '''C''', and\n\n:<math>g=\\frac{1}{\\left |f\\left (\\tfrac{1}{w} \\right ) \\right |^{\\frac{2}{n}}}\\left |d\\left (\\tfrac{1}{w} \\right ) \\right |^2 </math>\n\nfor ''w''&nbsp;∈&nbsp;'''S'''<sup>2</sup>\\{0}, is a well defined Riemannian metric over the sphere '''S'''<sup>2</sup> (which we identify with the extended complex plane '''C'''&nbsp;∪&nbsp;{∞}).\n\nNow, a simple computation shows that\n\n:<math>\\forall w\\in\\mathbf{C}: \\qquad  \\frac{1}{|f(w)|^{\\frac{1}{n}}} K_g=\\frac{1}{n}\\Delta \\log|f(w)|=\\frac{1}{n}\\Delta \\text{Re}(\\log f(w))=0,</math>\n\nsince the real part of an analytic function is harmonic. This proves that ''K<sub>g</sub>''&nbsp;=&nbsp;0.\n\n==Corollaries==\nSince the fundamental theorem of algebra can be seen as the statement that the field of complex numbers is [[algebraically closed field|algebraically closed]], it follows that any theorem concerning algebraically closed fields applies to the field of complex numbers. Here are a few more consequences of the theorem, which are either about the field of real numbers or about the relationship between the field of real numbers and the field of complex numbers:\n\n* The field of complex numbers is the [[algebraic closure]] of the field of real numbers.\n\n* Every polynomial in one variable ''z'' with complex coefficients is the product of a complex constant and polynomials of the form ''z''&nbsp;+&nbsp;''a'' with ''a'' complex.\n\n* Every polynomial in one variable ''x'' with real coefficients can be uniquely written as the product of a constant, polynomials of the form ''x''&nbsp;+&nbsp;''a'' with ''a'' real, and polynomials of the form ''x''<sup>2</sup>&nbsp;+&nbsp;''ax''&nbsp;+&nbsp;''b'' with ''a'' and ''b'' real and ''a''<sup>2</sup>&nbsp;−&nbsp;4''b''&nbsp;<&nbsp;0 (which is the same thing as saying that the polynomial ''x''<sup>2</sup>&nbsp;+&nbsp;''ax''&nbsp;+&nbsp;''b'' has no real roots). (By the [[Abel–Ruffini theorem]], the real numbers ''a'' and ''b'' are not necessarily expressible in terms of the coefficients of the polynomial, the basic arithmetic operations and the extraction of ''n''-th roots.) This implies that the number of non-real complex roots is always even and remains even when counted with their multiplicity.\n\n* Every [[rational function]] in one variable ''x'', with real coefficients, can be written as the sum of a polynomial function with rational functions of the form ''a''/(''x''&nbsp;−&nbsp;''b'')<sup>''n''</sup> (where ''n'' is a natural number, and ''a'' and ''b'' are real numbers), and rational functions of the form (''ax''&nbsp;+&nbsp;''b'')/(''x''<sup>2</sup>&nbsp;+&nbsp;''cx''&nbsp;+&nbsp;''d'')<sup>''n''</sup> (where ''n'' is a natural number, and ''a'', ''b'', ''c'', and ''d'' are real numbers such that ''c''<sup>2</sup>&nbsp;−&nbsp;4''d''&nbsp;<&nbsp;0). A [[corollary]] of this is that every rational function in one variable and real coefficients has an [[elementary function (differential algebra)|elementary]] [[Antiderivative|primitive]].\n\n* Every [[algebraic extension]] of the real field is isomorphic either to the real field or to the complex field.\n\n==Bounds on the zeros of a polynomial==\n{{main|Properties of polynomial roots}}\nWhile the fundamental theorem of algebra states a general existence result, it is of some interest, both from the theoretical and from the practical point of view, to have information on the location of the zeros of a given polynomial. The simpler result in this direction is a bound on the modulus: all zeros ζ of a monic polynomial <math>z^n+a_{n-1}z^{n-1}+\\cdots+a_1z +a_0</math> satisfy an inequality |ζ| ≤ ''R''<sub>∞</sub>, where\n\n:<math>R_{\\infty}:= 1+\\max\\{|a_0|,\\ldots,|a_{n-1}|\\}. </math>\n\nNotice that, as stated, this is not yet an existence result but rather an example of what is called an [[a priori and a posteriori|a priori]] bound: it says that ''if there are solutions'' then they lie inside the closed disk of center the origin and radius ''R''<sub>∞</sub>. However, once coupled with the fundamental theorem of algebra it says that the disk contains in fact at least one solution. More generally, a bound can be given directly in terms of any [[p-norm]] of the ''n''-vector of coefficients <math>a:=( a_0, a_1, \\ldots, a_{n-1}),</math> that is |ζ| ≤ ''R<sub>p</sub>'', where ''R<sub>p</sub>'' is precisely the ''q''-norm of the 2-vector <math>(1, \\|a\\|_p),</math> ''q'' being the conjugate exponent of ''p'', <math>\\tfrac{1}{p} + \\tfrac{1}{q} =1,</math> for any 1 ≤ ''p'' ≤ ∞. Thus, the modulus of any solution is also bounded by\n\n:<math> R_1:= \\max\\left \\{ 1 , \\sum_{0\\leq k<n} |a_k|\\right \\},</math>\n:<math> R_p:= \\left[ 1 + \\left(\\sum_{0\\leq k<n}|a_k|^p\\right )^{\\frac{q}{p}}\\right ]^{\\frac{1}{q}},</math>\n\nfor 1 < ''p'' < ∞, and in particular\n\n:<math> R_2:= \\sqrt{\\sum_{0\\leq k\\leq n} |a_k|^2 }</math>\n\n(where we define ''a<sub>n</sub>'' to mean 1, which is reasonable since 1 is indeed the ''n''-th coefficient of our polynomial). The case of a generic polynomial of degree ''n'', \n\n:<math>P(z):= a_n z^n+a_{n-1}z^{n-1}+\\cdots+a_1z +a_0,</math>\n\nis of course reduced to the case of a monic, dividing all coefficients by ''a<sub>n</sub>'' ≠ 0. Also, in case that 0 is not a root, i.e. ''a''<sub>0</sub> ≠ 0, bounds from below on the roots ζ follow immediately as bounds from above on <math>\\tfrac{1}{\\zeta}</math>, that is, the roots of \n\n:<math>a_0 z^n+a_1z^{n-1}+\\cdots+a_{n-1}z +a_n.</math> \n\nFinally, the distance <math>|\\zeta-\\zeta_0|</math> from the roots ζ to any point <math>\\zeta_0</math> can be estimated from below and above, seeing <math>\\zeta-\\zeta_0</math> as zeros of the polynomial <math>P(z+\\zeta_0)</math>, whose coefficients are the [[Taylor expansion]] of ''P''(''z'') at <math>z=\\zeta_0.</math>\n\nLet ζ be a root of the polynomial \n\n:<math>z^n+a_{n-1}z^{n-1}+\\cdots+a_1z +a_0;</math>\n\nin order to prove the inequality |ζ| ≤ ''R<sub>p</sub>'' we can assume, of course, |ζ| > 1. Writing the equation as \n\n:<math>-\\zeta^n=a_{n-1}\\zeta^{n-1}+\\cdots+a_1\\zeta+a_0,</math> \n\nand using the [[Hölder's inequality]] we find \n\n:<math>|\\zeta|^n\\leq \\|a\\|_p \\left \\| \\left (\\zeta^{n-1},\\ldots,\\zeta, 1 \\right ) \\right \\|_q.</math>\n\nNow, if ''p'' = 1, this is \n\n:<math>|\\zeta|^n\\leq\\|a\\|_1\\max \\left \\{|\\zeta|^{n-1},\\ldots,|\\zeta|,1 \\right \\} =\\|a\\|_1|\\zeta|^{n-1},</math>\n\nthus \n\n:<math>|\\zeta|\\leq \\max\\{1, \\|a\\|_1\\}.</math> \n\nIn the case 1 < ''p'' ≤ ∞, taking into account the summation formula for a [[geometric progression]], we have\n\n:<math>|\\zeta|^n\\leq \\|a\\|_p \\left(|\\zeta|^{q(n-1)}+\\cdots+|\\zeta|^q +1\\right)^{\\frac{1}{q}}=\\|a\\|_p \\left(\\frac{|\\zeta|^{qn}-1}{|\\zeta|^q-1}\\right)^{\\frac{1}{q}}\\leq\\|a\\|_p \\left(\\frac{|\\zeta|^{qn}}{|\\zeta|^q-1}\\right)^{\\frac{1}{q}},</math>\n\nthus \n\n:<math>|\\zeta|^{nq}\\leq \\|a\\|_p^q \\frac{|\\zeta|^{qn}}{|\\zeta|^q-1}</math> \n\nand simplifying, \n\n:<math>|\\zeta|^q\\leq 1+\\|a\\|_p^q.</math> \n\nTherefore \n\n:<math>|\\zeta|\\leq \\left \\| \\left (1,\\|a\\|_p \\right ) \\right \\|_q=R_p </math> \n\nholds, for all 1 ≤ ''p'' ≤ ∞.\n\n==References==\n{{Reflist}}\n\n===Historic sources===\n*{{Citation|last = Cauchy|first = Augustin-Louis|author-link = Augustin-Louis Cauchy|publication-date = 1992|year = 1821|title = Cours d'Analyse de l'École Royale Polytechnique, 1<sup>ère</sup> partie: Analyse Algébrique|url = http://gallica.bnf.fr/ark:/12148/bpt6k29058v|place = Paris|publisher = Éditions Jacques Gabay|isbn = 978-2-87647-053-8}} (tr. Course on Analysis of the [[École Polytechnique|Royal Polytechnic Academy]], part 1: Algebraic Analysis)\n* {{citation|last = Euler|first = Leonhard|author-link = Leonhard Euler|year = 1751|title = Recherches sur les racines imaginaires des équations|periodical = Histoire de l'Académie Royale des Sciences et des Belles-Lettres de Berlin|location = Berlin|volume = 5|pages = 222–288|url = http://bibliothek.bbaw.de/bbaw/bibliothek-digital/digitalequellen/schriften/anzeige/index_html?band=02-hist/1749&seite:int=228}}. English translation: {{citation|last = Euler|first = Leonhard|author-link = Leonhard Euler|year = 1751|title = Investigations on the Imaginary Roots of Equations|periodical = Histoire de l'Académie Royale des Sciences et des Belles-Lettres de Berlin|location = Berlin|volume = 5|pages = 222–288|url = http://eulerarchive.maa.org/docs/translations/E170en.pdf}}\n* {{citation|last = Gauss|first = Carl Friedrich|author-link = Carl Friedrich Gauss|year = 1799|title = Demonstratio nova theorematis omnem functionem algebraicam rationalem integram unius variabilis in factores reales primi vel secundi gradus resolvi posse|place = [[Helmstedt]]|publisher = C.&nbsp;G.&nbsp;Fleckeisen}} (tr. New proof of the theorem that every integral rational algebraic function of one variable can be resolved into real factors of the first or second degree).\n* {{Citation|last=Gauss|first=Carl Friedrich|year=1866|title=Carl Friedrich Gauss Werke|publisher=Königlichen Gesellschaft der Wissenschaften zu Göttingen|volume=Band III|url={{Google books|WFxYAAAAYAAJ|Werke: Analysis|plainurl=yes}}}}\n*#{{Google books|WFxYAAAAYAAJ|Demonstratio nova theorematis omnem functionem algebraicam rationalem integram unius variabilis in factores reales primi vel secundi gradus resolvi posse (1799), pp. 1–31.|page=1}} – first proof.\n*#{{Google books|WFxYAAAAYAAJ|Demonstratio nova altera theorematis omnem functionem algebraicam rationalem integram unius variabilis in factores reales primi vel secundi gradus resolvi posse (1815 Dec), pp. 32–56.|page=32}} – second proof.\n*#{{Google books|WFxYAAAAYAAJ|Theorematis de resolubilitate functionum algebraicarum integrarum in factores reales demonstratio tertia Supplementum commentationis praecedentis (1816 Jan), pp. 57–64.|page=57}} – third proof.\n*#{{Google books|WFxYAAAAYAAJ|Beiträge zur Theorie der algebraischen Gleichungen (1849 Juli), pp. 71–103.|page=71}} – fourth proof.\n* {{citation|last = Kneser|first = Hellmuth|author-link = Hellmuth Kneser|year = 1940|title = Der Fundamentalsatz der Algebra und der Intuitionismus|url = http://www-gdz.sub.uni-goettingen.de/cgi-bin/digbib.cgi?PPN266833020_0046|periodical = Mathematische Zeitschrift|volume = 46|pages = 287–302|issn = 0025-5874|doi = 10.1007/BF01181442}} (The Fundamental Theorem of Algebra and [[Intuitionism]]).\n* {{citation|last = Kneser|first = Martin|year = 1981|title = Ergänzung zu einer Arbeit von Hellmuth Kneser über den Fundamentalsatz der Algebra|url = http://www-gdz.sub.uni-goettingen.de/cgi-bin/digbib.cgi?PPN266833020_0177|periodical = Mathematische Zeitschrift|volume = 177|pages = 285–287|issn = 0025-5874|doi = 10.1007/BF01214206|issue = 2}} (tr. An extension of a work of [[Hellmuth Kneser]] on the Fundamental Theorem of Algebra).\n* {{citation|last = Ostrowski|first = Alexander | author-link = Alexander Ostrowski | year = 1920 | chapter = Über den ersten und vierten Gaußschen Beweis des Fundamental-Satzes der Algebra | title = Carl Friedrich Gauss ''Werke'' Band X Abt. 2 | chapter-url = http://gdz.sub.uni-goettingen.de/dms/load/img/?PPN=PPN236019856&DMDID=dmdlog53}} (tr. On the first and fourth Gaussian proofs of the Fundamental Theorem of Algebra).\n* {{cite conference|last=Weierstraß|first= Karl|authorlink=Karl Weierstrass|title=Neuer Beweis des Satzes, dass jede ganze rationale Function einer Veränderlichen dargestellt werden kann als ein Product aus linearen Functionen derselben Veränderlichen|booktitle=Sitzungsberichte der königlich preussischen Akademie der Wissenschaften zu Berlin|pages = 1085–1101|year=1891|url=http://bibliothek.bbaw.de/bibliothek-digital/digitalequellen/schriften/anzeige?band=10-sitz/1891-2&seite:int=00000565}} (tr. New proof of the theorem that every integral rational function of one variable can be represented as a product of linear functions of the same variable).\n\n===Recent literature===\n* {{citation|last = Almira|first = J.M.|last2 = Romero|first2 = A. |year = 2007|title = Yet another application of the Gauss-Bonnet Theorem for the sphere|periodical = [[Bulletin of the Belgian Mathematical Society]]|volume = 14|pages = 341–342| url = http://projecteuclid.org/DPubS/Repository/1.0/Disseminate?handle=euclid.bbms/1179839226&view=body&content-type=pdf_1}}\n* {{citation|last = Almira|first = J.M.|last2 = Romero|first2 = A. |year = 2012|title = Some Riemannian geometric proofs of the Fundamental Theorem of Algebra|periodical = Differential Geometry – Dynamical Systems|volume = 14|pages = 1–4| url = http://www.mathem.pub.ro/dgds/v14/D14-al.pdf}}\n* {{citation|last = de Oliveira|first = O.R.B.|year = 2011|title = The Fundamental Theorem of Algebra: an elementary and direct proof|periodical = Mathematical Intelligencer|volume = 33|issue = 2|pages = 1–2|doi=10.1007/s00283-011-9199-2}}\n* {{citation|last = de Oliveira|first = O.R.B.|year = 2012|title = The Fundamental Theorem of Algebra: from the four basic operations|periodical = American Mathematical Monthly|volume = 119|issue = 9|pages = 753–758|doi=10.4169/amer.math.monthly.119.09.753|arxiv = 1110.0165}}\n* {{citation|last = Fine|first = Benjamin|last2 = Rosenberger|first2 = Gerhard|title = The Fundamental Theorem of Algebra|publisher = [[Springer Science+Business Media|Springer-Verlag]]|place = Berlin|year = 1997|isbn = 978-0-387-94657-3|series = [[Undergraduate Texts in Mathematics]]|mr = 1454356}}\n* {{citation|last = Gersten|first = S.M.|last2 = Stallings|first2 = John R.|year = 1988|title = On Gauss's First Proof of the Fundamental Theorem of Algebra|jstor = 2047574|periodical = Proceedings of the AMS|volume = 103|issue = 1|pages = 331–332|issn = 0002-9939|doi = 10.2307/2047574}}\n* {{citation|last = Gilain|first = Christian|year = 1991|title = Sur l'histoire du théorème fondamental de l'algèbre: théorie des équations et calcul intégral|periodical = Archive for History of Exact Sciences|volume = 42|issue = 2|pages = 91–136|issn = 0003-9519|doi = 10.1007/BF00496870}} (tr. On the history of the fundamental theorem of algebra: [[theory of equations]] and [[integral calculus]].)\n* {{citation|last = Netto|first = Eugen|last2 = Le Vavasseur|first2 = Raymond|author-link = Eugen Netto|year = 1916|chapter = Les fonctions rationnelles §80–88: Le théorème fondamental|editor-last = Meyer|editor-first = François|editor2-last = Molk|editor2-first = Jules|title = Encyclopédie des Sciences Mathématiques Pures et Appliquées, tome&nbsp;I, vol.&nbsp;2|publication-date = 1992|publisher = Éditions Jacques Gabay|isbn = 978-2-87647-101-6}} (tr. The rational functions §80–88: the fundamental theorem).\n* {{citation|last = Remmert|first = Reinhold|author-link = Reinhold Remmert|year = 1991|chapter = The Fundamental Theorem of Algebra|editor-last = Ebbinghaus|editor-first = Heinz-Dieter|editor2-last = Hermes|editor2-first = Hans|editor3-last = Hirzebruch|editor3-first = Friedrich|title = Numbers|series = Graduate Texts in Mathematics 123|editor3-link = Friedrich Hirzebruch|place = Berlin|publisher = [[Springer Science+Business Media|Springer-Verlag]]|isbn = 978-0-387-97497-2}}\n* {{citation|last = Shipman|first = Joseph|year = 2007|title = Improving the Fundamental Theorem of Algebra|periodical = Mathematical Intelligencer|volume = 29|issue = 4|pages = 9–14|doi=10.1007/BF02986170|issn = 0343-6993}}\n* {{citation|last = Smale|first = Steve|year = 1981|title=The Fundamental Theorem of Algebra and Complexity Theory|author-link = Stephen Smale|periodical = Bulletin (New Series) of the American Mathematical Society|volume = 4 | issue = 1}} [http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.bams/1183547848]\n* {{citation|last= Smith|first = David Eugene|author-link = David Eugene Smith|title = A Source Book in Mathematics|publisher = [[Dover Publications|Dover]]|isbn = 978-0-486-64690-9|year = 1959}}\n* {{citation|last = Smithies|first = Frank|year = 2000|title = A forgotten paper on the fundamental theorem of algebra|periodical = Notes & Records of the Royal Society|volume = 54|issue = 3|pages = 333–341|issn = 0035-9149|doi = 10.1098/rsnr.2000.0116}}\n* {{citation|last = Taylor|first = Paul|date = 2 June 2007|title = Gauss's second proof of the fundamental theorem of algebra|url = http://www.paultaylor.eu/misc/gauss-web.php}} – English translation of Gauss's second proof.\n* {{citation | last = van der Waerden | first = Bartel Leendert | author-link = Bartel Leendert van der Waerden | title = Algebra | volume = I | edition = 7th | year = 2003 | publisher = [[Springer Science+Business Media|Springer-Verlag]] | isbn = 978-0-387-40624-4}}\n\n{{Wikisourcelang|la|Demonstratio nova theorematis omnem functionem algebraicam rationalem integram unius variabilis in factores reales primi vel secundi gradus resolvi posse|Gauss's first proof}}\n\n==See also==\n\n==External links==\n* [http://www.encyclopediaofmath.org/index.php/Algebra,_fundamental_theorem_of ''Algebra, fundamental theorem of'' at Encyclopaedia of Mathematics]\n* [http://www.cut-the-knot.org/do_you_know/fundamental2.shtml Fundamental Theorem of Algebra]&nbsp;— a collection of proofs\n* D. J. Velleman: ''The Fundamental Theorem of Algebra: A Visual Approach'', [http://www.cs.amherst.edu/~djv/ PDF (unpublished paper)], visualisation of d'Alembert's, Gauss's and the winding number proofs\n* [http://www.ams.org/notices/200806/tx080600666p.pdf ''From the Fundamental Theorem of Algebra to Astrophysics: A \"Harmonious\" Path'']\n* {{Google books|g3VaAAAAcAAJ|Gauss's first proof (in Latin)}}\n* {{Google books|Svc7AQAAMAAJ|Gauss's first proof (in Latin)}}\n* [[Mizar system]] proof: http://mizar.org/version/current/html/polynom5.html#T74\n\n{{Fundamental theorems}}\n\n{{DEFAULTSORT:Fundamental Theorem Of Algebra}}\n[[Category:Articles containing proofs]]\n[[Category:Field theory]]\n[[Category:Fundamental theorems|Algebra]]\n[[Category:Theorems in algebra]]\n[[Category:Theorems in complex analysis]]"
    },
    {
      "title": "Fundamental theorem of finitely generated abelian groups",
      "url": "https://en.wikipedia.org/wiki/Fundamental_theorem_of_finitely_generated_abelian_groups",
      "text": "#REDIRECT [[Finitely generated abelian group#Classification]]\n\n{{Redirect category shell|1=\n{{R to section}}\n}}\n\n[[Category:Theorems in algebra]]"
    },
    {
      "title": "Fundamental theorem of Galois theory",
      "url": "https://en.wikipedia.org/wiki/Fundamental_theorem_of_Galois_theory",
      "text": "{{Unreferenced|date=September 2015}}\nIn [[mathematics]], the '''fundamental theorem of [[Galois theory]]''' is a result that describes the structure of certain types of [[field extension]]s.\n\nIn its most basic form, the theorem asserts that given a field extension ''E''/''F'' that is [[finite extension|finite]] and [[Galois extension|Galois]], there is a [[one-to-one correspondence]] between its [[intermediate field]]s and [[subgroup]]s of its [[Galois group]]. ([[Intermediate field]]s are fields ''K'' satisfying ''F'' ⊆ ''K'' ⊆ ''E''; they are also called ''subextensions'' of ''E''/''F''.)\n\n==Explicit description of the correspondence==\nFor finite extensions, the correspondence can be described explicitly as follows.\n* For any subgroup ''H'' of Gal(''E''/''F''), the corresponding [[Fixed-point subring|fixed field]], denoted ''E<sup>H</sup>'', is the set of those elements of ''E'' which are fixed by every automorphism in ''H''.\n* For any intermediate field ''K'' of ''E''/''F'', the corresponding subgroup is Aut(''E''/''K''), that is, the set of those automorphisms in Gal(''E''/''F'') which fix every element of ''K''.\n\nThe fundamental theorem says that this correspondence is a one-to-one correspondence if (and only if) ''E''/''F'' is a [[Galois extension]].\nFor example, the topmost field ''E'' corresponds to the trivial subgroup of Gal(''E''/''F''), and the base field ''F'' corresponds to the whole group Gal(''E''/''F'').\n\nThe notation Gal(''E''/''F'') is only used for [[Galois extension]]s. If ''E''/''F'' is Galois, then Gal(''E''/''F'') = Aut(''E''/''F''). If ''E''/''F'' is not Galois, then the \"correspondence\" gives only an injective (but not surjective) map from <math>\\{</math>subgroups of Aut(''E''/''F'')<math>\\}</math> to <math>\\{</math>subfields of ''E''/''F''<math>\\}</math>, and a surjective (but not injective) map in the reverse direction. In particular, if ''E''/''F'' is not Galois, then ''F'' is not the fixed field of any subgroup of Aut(''E''/''F'').\n\n==Properties of the correspondence==\nThe correspondence has the following useful properties.\n\n* It is ''inclusion-reversing''. The inclusion of subgroups ''H<sub>1</sub> &sube; H<sub>2</sub>'' holds if and only if the inclusion of fields ''E<sup>H<sub>1</sub></sup>'' ⊇ ''E<sup>H<sub>2</sub></sup>'' holds.\n* Degrees of extensions are related to orders of groups, in a manner consistent with the inclusion-reversing property. Specifically, if ''H'' is a subgroup of Gal(''E''/''F''), then |''H''| = [''E'':''E<sup>H</sup>''] and |Gal(''E''/''F'')|/|''H''| = [''E<sup>H</sup>'':''F''].\n* The field ''E<sup>H</sup>'' is a [[normal extension]] of ''F'' (or, equivalently, Galois extension, since any subextension of a separable extension is separable) if and only if ''H'' is a [[normal subgroup]] of Gal(''E''/''F''). In this case, the restriction of the elements of Gal(''E''/''F'') to ''E<sup>H</sup>'' induces an [[group isomorphism|isomorphism]] between Gal(''E<sup>H</sup>''/''F'') and the [[quotient group]] Gal(''E''/''F'')/''H''.\n\n==Example 1==\n[[File:Lattice diagram of Q adjoin the positive square roots of 2 and 3, its subfields, and Galois groups.svg|thumb|600px|[[Lattice of subgroups]] and subfields]]\nConsider the field\n\n:<math>K = \\Q\\left (\\sqrt{2}, \\sqrt{3} \\right) = \\left [\\Q(\\sqrt{2}) \\right ](\\sqrt{3}).</math>\n\nSince {{math|''K''}} is first determined by adjoining {{math|{{sqrt|2}}}}, then {{math|{{sqrt|3}}}}, each element of {{math|''K''}} can be written as:\n\n:<math>\\left ( a + b \\sqrt{2} \\right ) +  \\left ( c + d \\sqrt{2} \\right ) \\sqrt{3},\\qquad a,b,c,d \\in \\Q.</math>\n\nIts Galois group <math>G = \\text{Gal}(K/\\Q)</math> can be determined by examining the automorphisms of {{math|''K''}} which fix {{math|''a''}}. Each such automorphism must send {{math|{{sqrt|2}}}} to either {{math|{{sqrt|2}}}} or {{math|–{{sqrt|2}}}}, and must send {{math|{{sqrt|3}}}} to either {{math|{{sqrt|3}}}} or {{math|–{{sqrt|3}}}} since the permutations in a Galois group can only permute the roots of an irreducible polynomial. Suppose that {{math|''f''}} exchanges {{math|{{sqrt|2}}}} and {{math|–{{sqrt|2}}}}, so\n\n:<math>f\\left((a+b\\sqrt{2})+(c+d\\sqrt{2})\\sqrt{3}\\right)=(a-b\\sqrt{2})+(c-d\\sqrt{2})\\sqrt{3}=a-b\\sqrt{2}+c\\sqrt{3}-d\\sqrt{6},</math>\n\nand {{math|''g''}} exchanges {{math|{{sqrt|3}}}} and {{math|–{{sqrt|3}}}},  so\n\n:<math>g\\left((a+b\\sqrt{2})+(c+d\\sqrt{2})\\sqrt{3}\\right)=(a+b\\sqrt{2})-(c+d\\sqrt{2})\\sqrt{3}=a+b\\sqrt{2}-c\\sqrt{3}-d\\sqrt{6}.</math>\n\nThese are clearly automorphisms of {{math|''K''}}. There is also the identity automorphism {{math|''e''}} which does not change anything, and the composition of {{math|''f''}} and {{math|''g''}} which changes the signs on ''both'' radicals:\n\n:<math>(fg)\\left((a+b\\sqrt{2})+(c+d\\sqrt{2})\\sqrt{3}\\right)=(a-b\\sqrt{2})-(c-d\\sqrt{2})\\sqrt{3}=a-b\\sqrt{2}-c\\sqrt{3}+d\\sqrt{6}.</math>\n\nTherefore,\n\n:<math>G = \\left\\{1, f, g, fg\\right\\},</math>\n\nand {{math|''G''}} is isomorphic to the [[Klein four-group]]. It has five subgroups, each of which correspond via the theorem to a subfield of {{math|''K''}}. \n* The trivial subgroup (containing only the identity element) corresponds to all of {{math|''K''}}.\n* The entire group {{math|''G''}} corresponds to the base field <math>\\Q.</math>\n* The two-element subgroup {{math|{1, ''f''} }}corresponds to the subfield <math>\\Q(\\sqrt{3}),</math> since {{math|''f''}} fixes {{math|{{sqrt|3}}}}.\n* The two-element subgroup {{math|{1, ''g''} }}corresponds to the subfield <math>\\Q(\\sqrt{2}),</math> again since {{math|''g''}} fixes {{math|{{sqrt|2}}}}.\n* The two-element subgroup {{math|{1, ''fg''} }}corresponds to the subfield <math>\\Q(\\sqrt{6}),</math> since {{math|''fg''}} fixes {{math|{{sqrt|6}}}}.\n\n==Example 2==\n[[File:Lattice diagram of Q adjoin a cube root of 2 and a primitive cube root of 1, its subfields, and Galois groups.svg|thumb|600px|[[Lattice of subgroups]] and subfields]]\nThe following is the simplest case where the Galois group is not abelian.\n\nConsider the [[splitting field]] ''K'' of the polynomial <math>x^3-2</math> over <math>\\Q;</math> that is, <math>K = \\Q(\\omega, \\theta)</math><!-- aargggghhhh can't decide whether to assume that ''K'' is a subfield of '''C''' or whether to do it the pure algebraic way :-) - Dmharvey --> where θ is a cube root of 2, and ω is a cube root of 1 (but not 1 itself). For example, if we imagine ''K'' to be inside the field of complex numbers, we may take θ to be the real cube root of 2, and ω to be\n\n:<math>\\omega = \\frac{-1}2 + i\\frac{\\sqrt3}2.</math>\n\nIt can be shown that the Galois group <math>G=\\text{Gal}(K/\\Q)</math> has six elements, and is isomorphic to the group of permutations of three objects. It is generated by (for example) two automorphisms, say ''f'' and ''g'', which are determined by their effect on θ and ω,\n\n:<math>f(\\theta) = \\omega \\theta, \\quad f(\\omega) = \\omega,</math>\n:<math>g(\\theta) = \\theta, \\quad g(\\omega) = \\omega^2,</math>\n<!-- hmmm might be nice to explain a little more clearly the effect of these automorphisms - Dmharvey -->\nand then\n\n:<math>G = \\left\\{ 1, f, f^2, g, gf, gf^2 \\right\\}.</math>\n\nThe subgroups of ''G'' and corresponding subfields are as follows:\n\n* As usual, the entire group ''G'' corresponds to the base field <math>\\Q</math> and the trivial group {1} corresponds to the whole field ''K''.\n* There is a unique subgroup of order 3, namely <math>\\{1, f, f^2\\}.</math> The corresponding subfield is <math>\\Q(\\omega),</math> which has degree 2 over <math>\\Q</math> (the [[minimal polynomial (field theory)|minimal polynomial]] of ω is <math>x^2+x+1</math>), corresponding to the fact that the subgroup has [[Index of a subgroup|index]] two in ''G''. Also, this subgroup is normal, corresponding to the fact that the subfield is normal over <math>\\Q.</math>\n* There are three subgroups of order 2, namely <math>\\{1, g\\}, \\{1, gf\\}</math> and <math>\\{1, gf^2\\},</math> corresponding respectively to the three subfields <math>\\Q(\\theta), \\Q(\\omega \\theta), \\Q(\\omega^2\\theta ).</math> These subfields have degree 3 over <math>\\Q,</math> again corresponding to the subgroups having [[Index of a subgroup|index]] 3 in ''G''. Note that the subgroups are ''not'' [[normal subgroup|normal]] in ''G'', and this corresponds to the fact that the subfields are ''not'' Galois over <math>\\Q.</math> For example, <math>\\Q(\\theta)</math> contains only a single root of the polynomial <math>x^3-2,</math> so it cannot be [[normal extension|normal]] over <math>\\Q.</math>\n<!--would be nice to have a diagram of subgroups and subfields somewhere here!!! - and for the previous example too! - Dmharvey -->\n\n==Example 3==\nLet <math>E=\\Q(\\lambda)</math> be the field of rational functions in <math>\\lambda</math> and let\n\n:<math>G = \\left\\lbrace { \\lambda, \\frac{1}{1-\\lambda}, \\frac{\\lambda-1}{\\lambda}, \\frac{1}{\\lambda}, \\frac{\\lambda}{\\lambda-1}, 1-\\lambda } \\right\\rbrace \\subset {\\rm Aut}(E)</math>\n\nwhich is a group under composition, isomorphic to <math>S_3</math> (see: [[Cross ratio#Six cross-ratios as M.C3.B6bius transformations|six cross-ratios]]).\nLet <math>F</math> be the fixed field of <math>G</math>, then <math>{\\rm Gal}(E/F) = G</math>.\n\nIf <math>H</math> is a subgroup of <math>G</math> then the coefficients of the following polynomial\n\n: <math>P(T) := \\prod_{h \\in H} (T - h) \\in E[T]</math>\n\ngenerate the fixed field of <math>H</math>. Galois correspondence means that every subfield of <math>E/F</math> can be constructed this way. For example, if <math>H = \\{\\lambda, 1-\\lambda\\}</math> then the fixed field is <math>\\Q( \\lambda(1-\\lambda))</math> and if <math>H = \\{\\lambda, 1/\\lambda\\}</math> then the fixed field is <math>\\Q(\\lambda + 1/\\lambda)</math>. Likewise, one can write <math>F</math>, the fixed field of <math>G</math>, as <math>\\Q(j),</math> where {{mvar|j}} is the [[j-invariant|{{mvar|j}}-invariant]].\n\nSimilar examples can be constructed for each of the [[Platonic solid#Symmetry groups|symmetry groups of the platonic solids]] as these also have faithful actions on the [[projective line]] <math>\\mathbb{P}^1(\\Complex)</math> and hence on <math>\\Complex(x)</math>.\n\n==Applications==\nThe theorem classifies the intermediate fields of ''E''/''F'' in terms of [[finite group|group theory]]. This translation between intermediate fields and subgroups is key\nto showing  that the [[general quintic equation]] is not [[solvable by radicals]] (see [[Abel–Ruffini theorem]]). One first determines the Galois groups of [[radical extension]]s (extensions of the form ''F''(α) where α is an ''n''-th root of some element of ''F''), and then uses the fundamental theorem to show that solvable extensions correspond to [[solvable group]]s.\n\nTheories such as [[Kummer theory]] and [[class field theory]] are predicated on the fundamental theorem.\n\n==Infinite case==\nThere is also a version of the fundamental theorem that applies to infinite [[algebraic extension]]s, which are [[normal extension|normal]] and [[separable extension|separable]]. It involves defining a certain [[topological structure]], the [[Krull topology]], on the Galois group; only subgroups that are also [[closed set]]s are relevant in the correspondence.\n<!--\n==References==\n{{reflist}}-->\n\n==External links==\n*{{Commonscat-inline}}\n\n{{Fundamental theorems}}\n\n{{DEFAULTSORT:Fundamental Theorem Of Galois Theory}}\n[[Category:Field theory]]\n[[Category:Group theory]]\n[[Category:Galois theory]]\n[[Category:Theorems in algebra]]\n[[Category:Fundamental theorems|Galois theory]]"
    },
    {
      "title": "Gershgorin circle theorem",
      "url": "https://en.wikipedia.org/wiki/Gershgorin_circle_theorem",
      "text": "In [[mathematics]], the '''Gershgorin circle theorem''' may be used to bound the [[Eigenvalues and eigenvectors|spectrum]] of a square [[matrix (mathematics)|matrix]]. It was first published by the Soviet mathematician [[Semyon Aronovich Gershgorin]] in 1931.  Gershgorin's name has been transliterated in several different ways, including Geršgorin, Gerschgorin, Gershgorin, Hershhorn, and Hirschhorn.\n\n==Statement and proof==\nLet <math>A</math> be a [[complex number|complex]] <math>n\\times n</math> matrix, with entries <math>a_{ij}</math>. For <math>i \\in\\{1,\\dots,n\\}</math> let <math>R_i = \\sum_{j\\neq{i}} \\left|a_{ij}\\right|</math> be the sum of the [[absolute value]]s of the non-diagonal entries in the <math>i</math>-th row.  Let <math>D(a_{ii}, R_i ) \\subseteq \\C</math> be a closed [[disc (mathematics)|disc]] centered at <math>a_{ii}</math> with radius <math>R_i</math>. Such a disc is called a '''Gershgorin disc'''.\n\n'''Theorem''': Every [[eigenvalue]] of <math> A </math> lies within at least one of the Gershgorin discs \n<math> D(a_{ii},R_i). </math>\n\n''Proof'': Let <math>\\lambda</math> be an eigenvalue of <math>A</math>. Choose a corresponding eigenvector <math>x = (x_j)</math> so that one component <math>x_i</math> is equal to <math>1</math> and the others are of absolute value less than or equal to <math>1</math>: <math>x_i = 1</math> and <math>|x_j|\\le 1</math> for <math>j\\ne i</math>. There always is such an <math>x</math>, which can be obtained simply by dividing any eigenvector by its component with largest modulus. Since <math>Ax=\\lambda x</math>, in particular\n\n: <math> \\sum_j a_{ij} x_j = \\lambda x_i = \\lambda. </math>\n\nSo, splitting the sum and taking into account once again that <math>x_i = 1</math>, we get\n\n: <math> \\sum_{j \\ne i} a_{ij} x_j + a_{ii}= \\lambda. </math>\n\nTherefore, applying the [[triangle inequality]],\n\n: <math> |\\lambda - a_{ii}| = \\left|\\sum_{j \\ne i} a_{ij} x_j\\right| \\le \\sum_{j \\ne i} |a_{ij}| |x_j| \\le \\sum_{j \\ne i} |a_{ij}| = R_i. </math>\n\n'''Corollary''': The eigenvalues of ''A'' must also lie within the Gershgorin discs ''C''<sub>''j''</sub> corresponding to the columns of ''A''.\n\n''Proof'': Apply the Theorem to ''A''<sup>T</sup>.\n\n'''Example''' For a [[diagonal matrix]], the Gershgorin discs coincide with the spectrum. Conversely, if the Gershgorin discs coincide with the spectrum, the matrix is diagonal.\n\n==Discussion==\nOne way to interpret this theorem is that if the off-diagonal entries of a square matrix over the complex numbers have small [[Norm_(mathematics)|norms]], the eigenvalues of the matrix cannot be \"far from\" the diagonal entries of the matrix. Therefore, by reducing the norms of off-diagonal entries one can attempt to approximate the eigenvalues of the matrix. Of course, diagonal entries may change in the process of minimizing off-diagonal entries.\n\nThe theorem does ''not'' claim that there is one disc for each eigenvalue; if anything, the discs rather correspond to the ''axes'' in <math>\\mathbb{C}^n</math>, and each expresses a bound on precisely those eigenvalues whose eigenspaces are closest to one particular axis. In the matrix\n: <math> \\begin{pmatrix} 3 & 2 & 2 \\\\ 1 & 1 & 0 \\\\ 1 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} a & 0 & 0 \\\\ 0 & b & 0 \\\\ 0 & 0 & c \\end{pmatrix} \\begin{pmatrix} 3 & 2 & 2 \\\\ 1 & 1 & 0 \\\\ 1 & 0 & 1 \\end{pmatrix}^{-1} = \\begin{pmatrix} -3a+2b+2c & 6a-2b-4c & 6a-4b-2c \\\\ b-a & a+(a-b) & 2(a-b) \\\\ c-a & 2(a-c) & a+(a-c) \\end{pmatrix} </math>\n— which by construction has eigenvalues <math>a</math>, <math>b</math>, and <math>c</math> with eigenvectors <math> \\left(\\begin{smallmatrix} 3 \\\\ 1 \\\\ 1 \\end{smallmatrix}\\right) </math>, <math> \\left(\\begin{smallmatrix} 2 \\\\ 1 \\\\ 0 \\end{smallmatrix}\\right) </math>, and <math> \\left(\\begin{smallmatrix} 2 \\\\ 0 \\\\ 1 \\end{smallmatrix}\\right) </math> — it is easy to see that the disc for row 2 covers <math>a</math> and <math>b</math> while the disc for row 3 covers <math>a</math> and <math>c</math>. This is however just a happy coincidence; if working through the steps of the proof one finds that it in each eigenvector is the first element that is the largest (every eigenspace is closer to the first axis than to any other axis), so the theorem only promises that the disc for row 1 (whose radius can be twice the ''sum'' of the other two radii) covers all three eigenvalues.\n\n==Strengthening of the theorem==\nIf one of the discs is disjoint from the others then it contains exactly one eigenvalue. If however it meets another disc it is possible that it contains no eigenvalue (for example, <math> A=\\begin{pmatrix}0&1\\\\4&0\\end{pmatrix} </math> or <math> A=\\begin{pmatrix}1&-2\\\\1&-1\\end{pmatrix} </math>). In the general case the theorem can be strengthened as follows:\n\n'''Theorem''': If the union of ''k'' discs is disjoint from the union of the other ''n''&nbsp;&minus;&nbsp;''k'' discs then the former union contains exactly ''k'' and the latter ''n''&nbsp;&minus;&nbsp;''k'' eigenvalues of ''A''.\n\n''Proof'': Let ''D'' be the diagonal matrix with entries equal to the diagonal entries of ''A'' and let\n\n: <math>B(t)=(1-t)D + tA.</math>\n\nWe will use the fact that the eigenvalues are continuous in <math>t</math>, and show that if any eigenvalue moves from one of the unions to the other, then it must be outside all the discs for some <math>t</math>, which is a contradiction.\n\nThe statement is true for <math>D = B(0)</math>. The diagonal entries of <math>B(t)</math> are equal to that of ''A'', thus the centers of the Gershgorin circles are the same, however their radii are ''t'' times that of A. Therefore the union of the corresponding ''k'' discs of <math>B(t)</math> is disjoint from the union of the remaining ''n-k'' for all <math>t \\in [0,1]</math>. The discs are closed, so the distance of the two unions for ''A'' is <math>d>0</math>. The distance for <math>B(t)</math> is a decreasing function of ''t'', so it is always at least ''d''. Since the eigenvalues of <math>B(t)</math> are a continuous function of ''t'', for any eigenvalue <math>\\lambda(t)</math> of <math>B(t)</math> in the union of the ''k'' discs its distance <math>d(t)</math> from the union of the other ''n-k'' discs is also continuous. Obviously <math>d(0)\\ge d</math>, and assume <math>\\lambda(1)</math> lies in the union of the ''n-k'' discs. Then <math>d(1)=0</math>, so there exists <math>0<t_0<1</math> such that <math>0<d(t_0)<d</math>. But this means <math>\\lambda(t_0)</math> lies outside the Gershgorin discs, which is impossible. Therefore <math>\\lambda(1)</math> lies in the union of the ''k'' discs, and the theorem is proven.\n\nRemarks:\n* The continuity of <math>\\lambda(t)</math> should be understand in the sense of [[Topology#Continuous_functions_and_homeomorphisms|topology]]. It is sufficient to show that the roots (as a point in space <math>\\mathbb{C}^n</math>) is continuous function of its coefficients. Note that the inverse map that maps roots to coefficients is described by [[Vieta's formulas]] (note for [[Characteristic polynomial]] <math>a_n\\equiv1</math>) which can be proved a [[open map]]. This proves the roots as a whole is a continuous function of its coefficients. Since composition of continuous functions is again continuous, the <math>\\lambda(t)</math> as a composition of roots solver and <math>B(t)</math> is also continuous.\n\n* Individual eigenvalue <math>\\lambda(t)</math> could merge with other eigenvalue(s) or appeared from a splitting of previous eigenvalue. This may confuse people and questioning the concept of continuous. However, when viewing from the space of eigenvalue set <math>\\mathbb{C}^n</math>, the trajectory is still a continuous curve although not necessarily smooth everywhere.\n\n* There are two types of continuity concerning eigenvalues: (1) each individual eigenvalue is a usual continuous function, (2) eigenvalues are continuous in the topological sense. Whichever continuity is used in a proof of the Gersgorin disk theorem, it should be justified that the sum of algebraic multiplicities of eigenvalues remains unchanged on each connected region. A proof using complex analysis (Argument Principle) is clear and mathematically sound. (See Horn and Johnson, Matrix Analysis, 2nd edition, Cambridge U Press.)\n\n==Application==\nThe Gershgorin circle theorem is useful in solving matrix equations of the form ''Ax'' = ''b'' for ''x'' where ''b'' is a vector and ''A'' is a matrix with a large [[condition number]].\n\nIn this kind of problem, the error in the final result is usually of the same [[order of magnitude]] as the error in the initial data multiplied by the condition number of ''A''. For instance, if ''b'' is known to six decimal places and the condition number of ''A'' is 1000 then we can only be confident that ''x'' is accurate to three decimal places. For very high condition numbers, even very small errors due to rounding can be magnified to such an extent that the result is meaningless.\n\nIt would be good to reduce the condition number of ''A''. This can be done by [[preconditioning]]: A matrix ''P'' such that ''P'' ≈ ''A''<sup>&minus;1</sup> is constructed, and then the equation ''PAx'' = ''Pb'' is solved for ''x''. Using the ''exact'' [[matrix inverse|inverse]] of ''A'' would be nice but finding the inverse of a matrix is something we want to avoid because of the computational expense. \n\nNow, since ''PA'' ≈ ''I'' where ''I'' is the identity matrix, the [[eigenvalue]]s of ''PA'' should all be close to 1. By the Gershgorin circle theorem, every eigenvalue of ''PA'' lies within a known area and so we can form a rough estimate of how good our choice of ''P'' was.\n\n== Example ==\nUse the Gershgorin circle theorem to estimate the eigenvalues of:\n[[File:Gershgorin Disk Theorem Example.svg|thumb|400px|right|This diagram shows the discs in yellow derived for the eigenvalues.\nThe first two disks overlap and their union contains two eigenvalues.  The third and fourth disks are disjoint from the others and contain one eigenvalue each.]]\n:<math> A =\n       \\begin{bmatrix}   10    &    -1    &    0   &    1\\\\\n                         0.2    &    8    &    0.2   &    0.2\\\\\n                         1    &    1    &    2   &    1\\\\\n                         -1    &    -1    &    -1   &    -11\\\\\n       \\end{bmatrix}.</math>\n\nStarting with row one, we take the element on the diagonal, ''a''<sub>''ii''</sub> as the center for the disc.  We then take the remaining elements in the row and apply the formula:\n\n: <math> \\sum_{j\\ne i} |a_{ij}| = R_i</math>\n\nto obtain the following four discs:\n: <math> D(10,2), \\; D(8,0.6), \\; D(2,3), \\; \\text{and} \\; D(-11,3). </math>\n\nNote that we can improve the accuracy of the last two discs by applying the formula to the corresponding columns of the matrix, obtaining <math> D(2,1.2) </math> and <math> D(-11,2.2) </math>.\n\nThe eigenvalues are  9.8218,  8.1478,  1.8995, -10.86\n\n==See also==\n* For matrices with non-negative entries, see [[Perron–Frobenius theorem]].\n* [[Doubly stochastic matrix]]\n* [[Hurwitz matrix]]\n* [[Joel Lee Brenner]]\n* [[Metzler matrix]]\n* [[Muirhead's inequality]]\n* [[Schur–Horn theorem]]\n\n==References==\n\n*{{citation|last=Gerschgorin|first=S.|authorlink=Semyon Aranovich Gershgorin|title=Über die Abgrenzung der Eigenwerte einer Matrix|journal=Izv. Akad. Nauk. USSR Otd. Fiz.-Mat. Nauk|volume=6|pages=749–754|year=1931|url=http://mi.mathnet.ru/eng/izv/y1931/i6/p749|language=German}}.\n*{{citation|authorlink=Richard S. Varga|last=Varga|first=Richard S.|title=Geršgorin and His Circles|location=Berlin|publisher=Springer-Verlag|year=2004|isbn=3-540-21100-4}}. ([http://www.math.kent.edu/~varga/pub/corrections.pdf Errata]).\n*{{citation|authorlink=Richard S. Varga|last=Varga|first=Richard S.|year=2002|title=Matrix Iterative Analysis|edition=2nd|publisher=Springer-Verlag}}. 1st ed., Prentice Hall, 1962.\n*{{citation|author1-link=Gene H. Golub|last1=Golub|first1=G. H.|author2-link=Charles F. Van Loan|last2=Van Loan|first2=C. F.|title=Matrix Computations |publisher=Johns Hopkins University Press |location=Baltimore |year=1996 |pages=320 |isbn=0-8018-5413-X}}.\n\n==External links==\n*{{planetmath reference|id=3709|title=Gershgorin's circle theorem}}\n* Eric W. Weisstein. \"[http://mathworld.wolfram.com/GershgorinCircleTheorem.html Gershgorin Circle Theorem].\" From MathWorld&mdash;A Wolfram Web Resource. \n* Semyon Aranovich Gershgorin biography at [http://www-history.mcs.st-andrews.ac.uk/Mathematicians/Gershgorin.html  MacTutor]\n\n[[Category:Theorems in algebra]]\n[[Category:Linear algebra]]\n[[Category:Matrix theory]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "Haran's diamond theorem",
      "url": "https://en.wikipedia.org/wiki/Haran%27s_diamond_theorem",
      "text": "In [[mathematics]], the '''Haran diamond theorem''' gives a general sufficient condition for a separable extension of a [[Hilbertian field]] to be Hilbertian. \n\n== Statement of the diamond theorem ==\n[[Image:Diamond_theorem.jpg|thumb|right|field diagram of the diamond theorem]]\nLet ''K'' be a [[Hilbertian field]] and ''L'' a separable extension of ''K''. Assume there exist two Galois extensions \n''N'' and ''M'' of ''K'' such that ''L'' is contained in the compositum ''NM'', but is contained in neither ''N'' nor ''M''. Then ''L'' is Hilbertian.\n\nThe name of the theorem comes from the pictured diagram of fields, and was coined by Jarden.\n\n== Some corollaries ==\n=== Weissauer's theorem ===\n\nThis theorem was firstly proved using non-standard methods by Weissauer. It was reproved by Fried using standard methods. The latter proof led Haran to his diamond theorem.  \n\n;Weissauer's theorem \nLet ''K'' be a Hilbertian field, ''N'' a Galois extension of ''K'', and ''L'' a finite proper extension of ''N''. Then ''L'' is Hilbertian. \n\n;Proof using the diamond theorem\n\nIf ''L'' is finite over ''K'', it is Hilbertian; hence we assume that ''L/K'' is infinite. Let ''x'' be a primitive element for ''L/N'', i.e., ''L'' = ''N''(''x''). \n\nLet ''M'' be the Galois closure of ''K''(''x''). Then all the assumptions of the diamond theorem are satisfied, hence ''L'' is Hilbertian.\n\n=== Haran&ndash;Jarden condition ===\n\nAnother, preceding to the diamond theorem, sufficient permanence condition was given by Haran&ndash;Jarden:\n'''Theorem.''' \nLet ''K'' be a Hilbertian field and ''N'', ''M'' two Galois extensions of ''K''. Assume that neither contains the other. Then their compositum ''NM'' is Hilbertian.\n\nThis theorem has a very nice consequence: Since the field of rational numbers, ''Q'' is Hilbertian ([[Hilbert's irreducibility theorem]]), we get that the algebraic closure of ''Q'' is not the compositum of two proper Galois extensions.\n\n== References ==\n*{{citation\n | last = Haran | first = Dan\n | doi = 10.1007/s002220050325\n | issue = 1\n | journal = Inventiones Mathematicae\n | mr = 1702139 | zbl=0933.12003 \n | pages = 113–126\n | title = Hilbertian fields under separable algebraic extensions\n | volume = 137\n | year = 1999}}.\n*{{citation\n | last1 = Fried | first1 = Michael D.\n | last2 = Jarden | first2 = Moshe\n | edition = 3rd revised\n | isbn = 978-3-540-77269-9\n | location = Berlin\n | mr = 2445111 | zbl=1145.12001\n | publisher = [[Springer-Verlag]]\n | series = Ergebnisse der Mathematik und ihrer Grenzgebiete.  3 Folge\n | title = Field Arithmetic\n | volume = 11\n | year = 2008}}.\n\n{{DEFAULTSORT:Haran's Diamond Theorem}}\n[[Category:Galois theory]]\n[[Category:Theorems in algebra]]\n[[Category:Number theory]]"
    },
    {
      "title": "Harish-Chandra isomorphism",
      "url": "https://en.wikipedia.org/wiki/Harish-Chandra_isomorphism",
      "text": "{{distinguish|Harish-Chandra homomorphism}}\nIn [[mathematics]], the '''Harish-Chandra isomorphism''', introduced by {{harvs|txt|last=[[Harish-Chandra]]|year=1951}},\nis an [[isomorphism]] of commutative rings constructed in the theory of [[Lie algebra]]s. The isomorphism maps the [[center (ring theory)|center]] ''Z''(''U''(''g'')) of the [[universal enveloping algebra]] ''U''(''g'') of a [[reductive Lie algebra]] ''g'' to the elements ''S''(''h'')<sup>''W''</sup> of the [[symmetric algebra]] ''S''(''h'') of a [[Cartan subalgebra]] ''h'' that are invariant under the [[Weyl group]] ''W''.\n\n==Fundamental invariants==\n\nLet ''n'' be the '''rank''' of ''g'', which is the dimension of the Cartan subalgebra ''h''. [[H. S. M. Coxeter]] observed that ''S''(''h'')<sup>''W''</sup> is a [[polynomial ring|polynomial algebra]] in ''n'' variables (see [[Chevalley–Shephard–Todd theorem]] for a more general statement). Therefore, the center of the universal enveloping algebra of a reductive Lie algebra is a polynomial algebra. The degrees of the generators are the degrees of the fundamental invariants given in the following table.\n{| class=\"wikitable\" style=\"text-align:center\"\n|-\n! Lie algebra || [[Coxeter number]] ''h'' || [[Dual Coxeter number]] || Degrees of fundamental invariants\n|-\n| '''R''' || 0 || 0 || 1\n|-\n| A<sub>''n''</sub> || ''n''&nbsp;+&nbsp;1 || ''n''&nbsp;+&nbsp;1 || 2, 3, 4, ..., ''n''&nbsp;+&nbsp;1\n|-\n| B<sub>''n''</sub> || 2''n'' || 2''n''&nbsp;&minus;&nbsp;1 || 2, 4, 6, ..., 2''n''\n|-\n| C<sub>''n''</sub> || 2''n'' || ''n''&nbsp;+&nbsp;1 || 2, 4, 6, ..., 2''n''\n|-\n| D<sub>''n''</sub> || 2''n''&nbsp;&minus;&nbsp;2 || 2''n''&nbsp;&minus;&nbsp;2 || ''n''; 2, 4, 6, ..., 2''n''&nbsp;&minus;&nbsp;2\n|-\n| E<sub>6</sub> || 12 || 12 || 2, 5, 6, 8, 9, 12\n|-\n| E<sub>7</sub> || 18 || 18 || 2, 6, 8, 10, 12, 14, 18\n|-\n| E<sub>8</sub> || 30 || 30 || 2, 8, 12, 14, 18, 20, 24, 30\n|-\n| F<sub>4</sub> || 12 || 9 || 2, 6, 8, 12\n|-\n| G<sub>2</sub> || 6 || 4 || 2, 6\n|}\n\nFor example, the center of the universal enveloping algebra of ''G''<sub>2</sub> is a polynomial algebra on generators of degrees 2 and 6.\n\n==Examples==\n*If ''g'' is the Lie algebra ''sl''(2, '''R'''), then the center of the universal enveloping algebra is generated by the [[Casimir invariant]] of degree 2, and the Weyl group acts on the Cartan subalgebra, which is isomorphic to '''R''', by negation, so the invariant of the Weyl group is simply the square of the generator of the Cartan subalgebra, which is also of degree 2.\n\n== Introduction and setting ==\nLet ''g'' be a [[semisimple Lie algebra]], ''h'' its [[Cartan subalgebra]] and λ, μ &isin; ''h''* be two elements of the [[weight space]] and assume that a set of [[Positive root#Positive roots and simple roots|positive roots]] Φ<sup>+</sup> have been fixed. Let ''V''<sub>λ</sub>, resp. ''V''<sub>μ</sub> be [[highest weight module]]s with highest weight λ, resp. μ.\n\n=== Central characters ===\nThe ''g''-modules ''V''<sub>λ</sub> and ''V''<sub>μ</sub> are representations of the [[universal enveloping algebra]] ''U''(''g'') and its [[center (algebra)|center]] acts on the modules by scalar multiplication (this follows from the fact that the modules are generated by a highest weight vector). So, for ''v'' in ''V''<sub>λ</sub> and ''x'' in ''Z''(''U''(''g'')),\n:<math>x\\cdot v:=\\chi_\\lambda(x)v</math>\nand similarly for ''V''<sub>μ</sub>.\n\nThe functions <math>\\chi_\\lambda, \\,\\chi_\\mu</math> are homomorphims to scalars called ''central characters''.\n\n==Statement of Harish-Chandra theorem==\nFor any  λ, μ &isin; ''h''*, the characters <math>\\chi_\\lambda=\\chi_\\mu</math> if and only if λ+δ and μ+δ are on the same [[orbit (group theory)|orbit]] of the [[Weyl group]] of ''h''*, where δ is the half-sum of the [[positive root]]s.<ref>Humphreys (1972), p.130</ref>\n\nAnother closely related formulation is that the [[Harish-Chandra homomorphism]] from the center of the [[universal enveloping algebra]] ''Z''(''U''(''g'')) to ''S''(''h'')<sup>''W''</sup> (the elements of the symmetric algebra of the Cartan subalgebra fixed by the Weyl group) is an [[isomorphism]].\n\n== Applications ==\nThe theorem may be used to obtain a simple algebraic proof of [[Weyl's character formula]] for finite-dimensional representations.\n\nFurther, it is a necessary condition for the existence of a nonzero homomorphism of some highest weight modules (a homomorphism of such modules preserves central character). A simple consequence is that for [[Verma module]]s or [[generalized Verma module]]s ''V''<sub>λ</sub> with highest weight λ, there exist only finitely many weights μ such that  a nonzero homomorphism ''V''<sub>λ</sub> → ''V''<sub>μ</sub> exists.\n\n==See also==\n* [[Translation functor]]\n* [[Infinitesimal character]]\n\n== Notes ==\n{{reflist}}\n\n==References==\n\n*{{Citation | last1=Harish-Chandra | title=On some applications of the universal enveloping algebra of a semisimple Lie algebra | jstor=1990524 | mr=0044515 | year=1951 | journal=[[Transactions of the American Mathematical Society]] | issn=0002-9947 | volume=70 | issue=1 | pages=28–96 | doi=10.2307/1990524}}\n*{{cite book |title=Introduction to Lie algebras and Representation Theory |last1=Humphreys |first1=James |authorlink1=James E. Humphreys |year=1972 |publisher=Springer |isbn=978-0387900537 }}\n*{{Citation | last1=Humphreys | first1=James E. | author1-link=James E. Humphreys | title=Representations of semisimple Lie algebras in the BGG category O  | publisher=AMS | year=2008 | isbn=978-0-8218-4678-0 | page=26}}\n*{{Citation | last1=Knapp | first1=Anthony W. | first2=David A. |last2=Vogan| title=Cohomological induction and unitary representations | publisher=[[Princeton University Press]] | series=Princeton Mathematical Series | isbn=978-0-691-03756-1 | mr=1330919 | year=1995 | volume=45}}\n*Knapp, Anthony, ''Lie groups beyond an introduction'', Second edition, pages 300–303.\n\n[[Category:Lie algebras]]\n[[Category:Representation theory of Lie algebras]]\n[[Category:Theorems in algebra]]"
    },
    {
      "title": "Haynsworth inertia additivity formula",
      "url": "https://en.wikipedia.org/wiki/Haynsworth_inertia_additivity_formula",
      "text": "In mathematics, the '''Haynsworth inertia additivity formula''', discovered by Emilie Virginia Haynsworth (1916–1985), concerns the number of positive, negative, and zero [[eigenvalue]]s of a [[Hermitian matrix]] and of [[block matrix|block matrices into which it is partitioned]].<ref>Haynsworth, E. V., \"Determination of the inertia of a partitioned Hermitian matrix\", ''[[Linear Algebra and its Applications]]'', volume 1 (1968), pages 73–81</ref>\n\nThe ''inertia'' of a Hermitian matrix ''H'' is defined as the ordered triple\n\n: <math> \\mathrm{In}(H) = \\left( \\pi(H), \\nu(H), \\delta(H) \\right) </math>\n\nwhose components are respectively the numbers of positive, negative, and zero eigenvalues of&nbsp;''H''.  Haynsworth considered a partitioned Hermitian matrix\n\n: <math> H = \\begin{bmatrix} H_{11} & H_{12} \\\\  H_{12}^\\ast & H_{22} \\end{bmatrix} </math>\n\nwhere ''H''<sub>11</sub> is [[nonsingular matrix|nonsingular]] and ''H''<sub>12</sub><sup>*</sup> is the [[conjugate transpose]] of&nbsp;''H''<sub>12</sub>.  The formula states:<ref>{{cite book  |title=The Schur Complement and Its Applications |page=15|first=Fuzhen |last=Zhang |year=2005 |publisher=Springer| isbn=0-387-24271-6 }}</ref><ref>{{Google books |id=Wjd8_AwjiIIC |page=15 |title=The Schur Complement and Its Applications }}</ref>\n\n: <math> \\mathrm{In} \\begin{bmatrix} H_{11} & H_{12} \\\\  H_{12}^\\ast & H_{22} \\end{bmatrix} = \\mathrm{In}(H_{11}) + \\mathrm{In}(H/H_{11}) </math>\n\nwhere ''H''/''H''<sub>11</sub> is the [[Schur complement]] of ''H''<sub>11</sub> in&nbsp;''H'':\n\n: <math> H/H_{11} = H_{22} - H_{12}^\\ast H_{11}^{-1}H_{12}. </math>\n\n== See also ==\n* [[Block matrix pseudoinverse]]\n\n== Notes and references ==\n{{reflist}}\n\n[[Category:Linear algebra]]\n[[Category:Matrix theory]]\n[[Category:Theorems in algebra]]"
    },
    {
      "title": "Hilbert–Burch theorem",
      "url": "https://en.wikipedia.org/wiki/Hilbert%E2%80%93Burch_theorem",
      "text": "In [[mathematics]], the '''Hilbert–Burch theorem''' describes the structure of some [[Resolution (algebra)|free resolutions]] of a [[Quotient ring|quotient]] of a [[Local ring|local]] or [[Graded ring|graded]] [[Ring (mathematics)|ring]] in the case that the quotient has [[projective dimension]]&nbsp;2. {{harvs|txt|last=Hilbert|authorlink=David Hilbert|year=1890}} proved a version of this theorem for [[polynomial ring]]s, and {{harvs|txt|last=Burch|year1=1968|loc=p.&nbsp;944}} proved a more general version. Several other authors later rediscovered and published variations of this theorem. {{harvtxt|Eisenbud|1995|loc=theorem 20.15}} gives a statement and proof.\n\n==Statement==\nIf ''R'' is a local ring with an [[ideal (ring theory)|ideal]] ''I'' and \n:<math> 0 \\rightarrow R^m\\stackrel{f}{\\rightarrow} R^n \\rightarrow R \\rightarrow R/I\\rightarrow 0</math> \nis a free resolution of the ''R''-[[module (mathematics)|module]] ''R''/''I'', then ''m''&nbsp;=&nbsp;''n''&nbsp;–&nbsp;1 and the ideal ''I'' is ''aJ'' where ''a'' is a [[zero divisor|regular]] element of ''R'' and ''J'', a depth-2 ideal, is the first [[Fitting ideal]] <math>\\operatorname{Fitt}_1 I</math> of ''I'', i.e., the ideal generated by the [[determinant]]s of the minors of size ''m'' of the matrix of ''f''.\n\n==References==\n\n*{{Citation | last1=Burch | first1=Lindsay | title=On ideals of finite homological dimension in local rings | doi=10.1017/S0305004100043620 | mr=0229634 | year=1968 | journal=Proc. Cambridge Philos. Soc. | volume=64 | pages=941–948 | zbl=0172.32302 | issn=0008-1981 }}\n*{{Citation | last1=Eisenbud | first1=David | author1-link=David Eisenbud | title=Commutative algebra. With a view toward algebraic geometry | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=[[Graduate Texts in Mathematics]] | isbn=3-540-94268-8 | mr=1322960 | year=1995 | volume=150 | zbl=0819.13001 }}\n*{{citation |authorlink=David Eisenbud |first=David |last=Eisenbud |title=The Geometry of Syzygies.  A second course in commutative algebra and algebraic geometry | series=[[Graduate Texts in Mathematics]] | volume=229 | year=2005 |location=New York, NY |publisher=[[Springer-Verlag]] | isbn=0-387-22215-4 | zbl=1066.14001 }}\n*{{Citation | last1=Hilbert | first1=David | author1-link=David Hilbert | title=Ueber die Theorie der algebraischen Formen | language=German | doi=10.1007/BF01208503 | year=1890 | journal=[[Mathematische Annalen]] | issn=0025-5831 | volume=36 | issue=4 | pages=473–534 | jfm=22.0133.01 }}\n\n{{DEFAULTSORT:Hilbert-Burch theorem}}\n[[Category:Commutative algebra]]\n[[Category:Theorems in algebra]]\n\n{{abstract-algebra-stub}}"
    },
    {
      "title": "Hilbert's irreducibility theorem",
      "url": "https://en.wikipedia.org/wiki/Hilbert%27s_irreducibility_theorem",
      "text": "{{more footnotes|date=March 2012}}\nIn [[number theory]], '''Hilbert's irreducibility theorem''', conceived by [[David Hilbert]], states that every finite number of [[irreducible polynomial]]s in a finite number of variables and having [[rational number]] coefficients admit a common specialization of a proper subset of the variables to rational numbers such that all the polynomials remain irreducible.  This theorem is a prominent theorem in number theory.\n\n== Formulation of the theorem ==\n'''Hilbert's irreducibility theorem.''' Let\n\n: <math>f_1(X_1,\\ldots, X_r, Y_1,\\ldots, Y_s), \\ldots, f_n(X_1,\\ldots, X_r, Y_1,\\ldots, Y_s) </math>\n\nbe irreducible polynomials in the ring\n\n:<math> \\mathbb{Q}[X_1,\\ldots, X_r, Y_1,\\ldots, Y_s]. </math>\n\nThen there exists an ''r''-tuple of rational numbers (''a''<sub>1</sub>,...,''a''<sub>''r''</sub>) such that\n\n: <math>f_1(a_1,\\ldots, a_r, Y_1,\\ldots, Y_s), \\ldots, f_n(a_1,\\ldots, a_r, Y_1,\\ldots, Y_s) </math>\n\nare irreducible in the ring\n\n:<math> \\mathbb{Q}[Y_1,\\ldots, Y_s]. </math>\n\n'''Remarks.'''\n* It follows from the theorem that there are infinitely many ''r''-tuples. In fact the set of all irreducible specializations, called Hilbert set, is large in many senses. For example, this set is [[Zariski topology|Zariski dense]] in <math>\\mathbb Q^r</math>.\n* There are always (infinitely many) integer specializations, i.e., the assertion of the theorem holds even if we demand  (''a''<sub>1</sub>,...,''a''<sub>''r''</sub>) to be integers.\n* There are many [[Hilbertian field]]s, i.e., fields satisfying Hilbert's irreducibility theorem. For example, [[global field]]s are Hilbertian.<ref name=L41>Lang (1997) p.41</ref>\n* The irreducible specialization property stated in the theorem is the most general. There are many reductions, e.g., it suffices to take <math>n=r=s=1</math> in the definition. A recent result of Bary-Soroker shows that for a field ''K'' to be Hilbertian it suffices to consider the case of <math>n=r=s=1</math> and <math> f=f_1</math> [[absolutely irreducible]], that is, irreducible in the ring ''K''<sup>alg</sup>[''X'',''Y''], where ''K''<sup>alg</sup> is the algebraic closure of ''K''.\n\n== Applications ==\nHilbert's irreducibility theorem has numerous applications in [[number theory]] and [[algebra]]. For example:\n\n* The [[inverse Galois problem]], Hilbert's original motivation. The theorem almost immediately implies that if a finite group ''G'' can be realized as the Galois group of a Galois extension ''N'' of\n:: <math>E=\\mathbb{Q}(X_1,\\ldots, X_r),</math>\n:then it can be specialized to a Galois extension ''N''<sub>0</sub> of the rational numbers with ''G'' as its Galois group.<ref name=L42>Lang (1997) p.42</ref>  (To see this, choose a monic irreducible polynomial ''f''(''X''<sub>1</sub>,…,''X<sub>n</sub>'',''Y'') whose root generates ''N'' over ''E''. If ''f''(''a''<sub>1</sub>,…,''a<sub>n</sub>'',''Y'') is irreducible for some ''a<sub>i</sub>'', then a root of it will generate the asserted ''N''<sub>0</sub>.)\n\n* Construction of elliptic curves with large rank.<ref name=L42/>\n* Hilbert's irreducibility theorem is used as a step in the [[Andrew Wiles]] proof of [[Fermat's last theorem]].\n*If a polynomial <math>g(x) \\in \\mathbb{Z}[x]</math> is a perfect square for all large integer values of ''x'', then ''g(x)'' is the square of a polynomial in <math>\\mathbb{Z}[x]</math>.  This follows from Hilbert's irreducibility theorem with <math>n=r=s=1</math> and\n: <math>f_1(X, Y)\\, = Y^2 - g(X)</math>.\n(More elementary proofs exist.)  The same result is true when \"square\" is replaced by \"cube\", \"fourth power\", etc.\n\n== Generalizations ==\n\nIt has been reformulated and generalized extensively, by using the language of [[algebraic geometry]]. See [[thin set (Serre)]].\n\n== References ==\n{{reflist}}\n* {{cite book | first=Serge | last=Lang | authorlink=Serge Lang | title=Survey of Diophantine Geometry | publisher=[[Springer-Verlag]] | year=1997 | isbn=3-540-61223-8 | zbl=0869.11051 }}\n*J. P. Serre, ''Lectures on The Mordell-Weil Theorem'', Vieweg, 1989.\n*M. D. Fried and M. Jarden, ''Field Arithmetic'', Springer-Verlag, Berlin, 2005.\n*H. Völklein, ''Groups as Galois Groups'', Cambridge University Press, 1996.\n*G. Malle and B. H. Matzat, ''Inverse Galois Theory'', Springer, 1999.\n\n[[Category:Theorems in number theory]]\n[[Category:Polynomials]]\n[[Category:Theorems in algebra]]\n[[Category:David Hilbert]]"
    },
    {
      "title": "Hochster–Roberts theorem",
      "url": "https://en.wikipedia.org/wiki/Hochster%E2%80%93Roberts_theorem",
      "text": "In [[algebra]], the '''Hochster–Roberts theorem''', introduced by {{harvs|txt|last1=Hochster|last2=Roberts|year=1974}},  \nstates that rings of invariants of linearly [[reductive group]]s acting on [[regular ring]]s are [[Cohen–Macaulay ring|Cohen–Macaulay]].\n\nIn other words,<ref>{{harvnb|Mumford|1994|loc=pg. 199}}</ref>\n:If ''V'' is a rational representation of a linearly reductive group ''G'' over a [[field (mathematics)|field]] ''k'', then there exist algebraically independent invariant homogeneous polynomials <math>f_1, \\cdots, f_d</math> such that <math>k[V]^G</math> is a free finite [[graded module]] over <math>k[f_1, \\cdots, f_d]</math>.\n\n{{harvtxt|Boutot|1987}} proved that if a variety over a field of characteristic 0 has [[rational singularity|rational singularities]] then so does its quotient by the action of a reductive group; this implies the Hochster–Roberts theorem in characteristic 0 as rational singularities are Cohen–Macaulay.\n\nIn characteristic ''p''>0, there are examples of groups that are reductive (or even finite) acting on polynomial rings whose rings of invariants are not Cohen–Macaulay.\n\n== References ==\n{{reflist}}\n\n*{{Citation | last1=Boutot | first1=Jean-François | title=Singularités rationnelles et quotients par les groupes réductifs | doi=10.1007/BF01405091 |mr=877006 | year=1987 | journal=[[Inventiones Mathematicae]] | issn=0020-9910 | volume=88 | issue=1 | pages=65–68}}\n*{{Citation | last1=Hochster | first1=Melvin | author1-link=Melvin Hochster | last2=Roberts | first2=Joel L. | title=Rings of invariants of reductive groups acting on regular rings are Cohen-Macaulay | doi=10.1016/0001-8708(74)90067-X |mr=0347810 | year=1974 | journal=Advances in Mathematics | issn=0001-8708 | volume=13 | issue=2 | pages=115–175}}\n* Mumford, D.; Fogarty, J.; Kirwan, F. ''Geometric invariant theory''. Third edition. [[Ergebnisse der Mathematik und ihrer Grenzgebiete]] (2) (Results in Mathematics and Related Areas (2)), 34. Springer-Verlag, Berlin, 1994. xiv+292 pp. {{MathSciNet|id=1304906}} {{ISBN|3-540-56963-4}}\n\n{{DEFAULTSORT:Hochster-Roberts theorem}}\n[[Category:Theorems in algebra]]\n\n\n{{algebra-stub}}"
    },
    {
      "title": "Hua's identity",
      "url": "https://en.wikipedia.org/wiki/Hua%27s_identity",
      "text": "{{for|Hua's identity in [[Jordan algebra]]s|Hua's identity (Jordan algebra)}}\nIn algebra, '''[[Hua Luogeng|Hua]]'s identity'''<ref>{{harvnb|Cohn|2003|loc=§9.1}}</ref> <!-- named after --> states that for any elements ''a'', ''b'' in a [[division ring]],\n:<math>a - (a^{-1} + (b^{-1} - a)^{-1})^{-1} = aba</math>\nwhenever <math>ab \\ne 0, 1</math>. Replacing <math>b</math> with <math>-b^{-1}</math> gives another equivalent form of the identity:\n:<math>(a+ab^{-1}a)^{-1} + (a+b)^{-1} =a^{-1}.</math>\n\nAn important application of the identity is a proof of '''Hua's theorem'''.<ref>{{harvnb|Cohn|2003|loc=Theorem 9.1.3}}</ref><ref>{{Cite web|url=http://math.stackexchange.com/questions/161301/is-this-map-of-domains-a-jordan-homomorphism|title=Is this map of domains a Jordan homomorphism?|website=math.stackexchange.com|access-date=2016-06-28}}</ref> The theorem says that if <math>\\sigma: K \\to L</math> is a [[function (mathematics)|function]] between division rings and if <math>\\sigma</math> satisfies:\n:<math>\\sigma(a + b) = \\sigma(a) + \\sigma(b), \\quad \\sigma(1) = 1, \\quad \\sigma(a^{-1}) = \\sigma(a)^{-1},</math>\nthen <math>\\sigma</math> is either a [[Ring homomorphism|homomorphism]] or an [[antihomomorphism]]. The theorem is important because of the connection to the [[fundamental theorem of projective geometry]].\n\n== Proof ==\n<math>(a - aba)(a^{-1} + (b^{-1} - a)^{-1}) = 1 - ab + ab(b^{-1} - a)(b^{-1} - a)^{-1} = 1.</math>\n\n(Note the proof is valid in any ring as long as <math>a, b, ab - 1</math> are [[unit (ring theory)|unit]]s.<ref>{{harvnb|Jacobson|loc=§ 2.2. Exercise 9.}}</ref>)\n\n== References ==\n{{reflist}}\n* {{cite book | first=Paul M. | last=Cohn | authorlink=Paul Cohn | edition=Revised ed. of Algebra, 2nd | title=Further algebra and applications | year=2003 | location=London | publisher=[[Springer-Verlag]] | isbn=1-85233-667-6 | zbl=1006.00001 }}\n* Jacobson, Nathan (2009), Basic Algebra 1 (2nd ed.), Dover, {{ISBN|978-0-486-47189-1}}\n\n[[Category:Theorems in algebra]]\n\n{{algebra-stub}}"
    },
    {
      "title": "Hudde's rules",
      "url": "https://en.wikipedia.org/wiki/Hudde%27s_rules",
      "text": "In [[mathematics]], '''Hudde's rules''' are two [[properties of polynomial roots]] described by [[Johann Hudde]].\n\n1. If ''r'' is a [[double root]] of the polynomial equation\n::<math>a_0x^n + a_1x^{n-1} + \\cdots + a_{n-1}x + a_n = 0 </math>\n:and if <math>b_0, b_1, \\dots, b_{n-1}, b_n</math> are numbers in [[arithmetic progression]], then ''r'' is also a [[root]] of\n::<math>a_0b_0x^n + a_1b_1x^{n-1} + \\cdots + a_{n-1}b_{n-1}x + a_nb_n = 0. </math>\n:This definition is a form of the modern [[theorem]] that if ''r'' is a double root of ''ƒ''(''x'')&nbsp;=&nbsp;0, then ''r'' is a root of ''ƒ''<nowiki>&nbsp;'</nowiki>(''x'')&nbsp;=&nbsp;0.\n\n2. If for ''x''&nbsp;=&nbsp;''a'' the polynomial\n::<math>a_0x^n + a_1x^{n-1} + \\cdots + a_{n-1}x + a_n </math>\n:takes on a relative [[maximum]] or [[minimum]] [[Value (mathematics)|value]], then ''a'' is a root of the equation\n::<math>na_0x^n + (n-1)a_1x^{n-1} + \\cdots + 2a_{n-2}x^2 + a_{n-1}x = 0. </math>\n:This definition is a modification of [[Fermat's theorem (stationary points)|Fermat's theorem]] in the form that if ''ƒ''(''a'') is a relative maximum or minimum value of a polynomial ''ƒ''(''x''), then ''ƒ''<nowiki>&nbsp;'</nowiki>(''a'')&nbsp;=&nbsp;0, where ''ƒ''<nowiki>&nbsp;'</nowiki> is the [[derivative]] of ''ƒ''.\n\nHudde was working with [[Frans van Schooten]] on a [[Latin]] edition of [[La Géométrie]] of [[René Descartes]]. In the 1659 edition of the translation, Hudde contributed two letters: \"Epistola prima de Redvctione Ǣqvationvm\" (pages 406 to 506), and \"Epistola secvnda de Maximus et Minimus\" (pages 507 to 16). These letters may be read by the Internet Archive link below.\n\n==References==\n* [[Carl B. Boyer]] (1991) ''A History of Mathematics'', 2nd edition, page 373, [[John Wiley & Sons]].\n* Robert Raymond Buss (1979) ''Newton's use of Hudde's Rule in his Development of the Calculus'', Ph.D. Thesis [[Saint Louis University]], [[ProQuest]] #302919262\n* [[René Descartes]] (1659) [https://archive.org/details/bub_gb_eslA5J_AJggC La Géométria, 2nd edition] via [[Internet Archive]]. \n* [[Kirsti Pedersen]] (1980) §5 \"Descartes’s method of determining the normal, and Hudde’s rule\", chapter 2: \"Techniques of the calculus, 1630-1660\", pages 16—19 in ''From the Calculus to Set Theory'' edited by [[Ivor Grattan-Guinness]] [[Duckworth Overlook]] {{ISBN|0-7156-1295-6}}\n\n\n[[Category:Rules]]\n[[Category:Theorems in algebra]]\n[[Category:Polynomials]]\n[[Category:Calculus]]"
    },
    {
      "title": "Jacobson–Bourbaki theorem",
      "url": "https://en.wikipedia.org/wiki/Jacobson%E2%80%93Bourbaki_theorem",
      "text": "In algebra, the '''Jacobson–Bourbaki theorem''' is a theorem used to  extend [[Galois theory]] to [[field extension]]s that need not be separable. It was introduced by {{harvs|txt|first=Nathan|last=Jacobson|authorlink=Nathan Jacobson|year=1944}} for commutative [[field (mathematics)|field]]s and extended to non-commutative fields by {{harvtxt|Jacobson|1947}}, and {{harvs|txt|last=Cartan|first=Henri|authorlink=Henri Cartan|year=1947}} who credited the result to unpublished work by [[Nicolas Bourbaki]]. The extension of Galois theory to [[normal extension]]s is called the '''Jacobson–Bourbaki correspondence''', which replaces the correspondence between some [[Field extension|subfields]] of a field and some subgroups of a [[Galois group]] by a correspondence between some sub division rings of a [[division ring]] and some [[subalgebra]]s of an algebra.\n\nThe Jacobson–Bourbaki theorem implies both the usual Galois correspondence for subfields of a Galois extension, and Jacobson's Galois correspondence for subfields of a [[purely inseparable extension]] of exponent at most 1.\n\n==Statement==\nSuppose that ''L'' is a [[division ring]].\nThe Jacobson–Bourbaki theorem states that there is a natural 1:1 correspondence between:\n*Division rings ''K'' in ''L'' of finite index ''n'' (in other words ''L'' is a finite-dimensional left vector space over ''K'').\n*Unital ''K''-algebras of finite dimension ''n'' (as ''K''-vector spaces) contained in the ring of endomorphisms of the additive group of ''K''.\n\nThe sub division ring and the corresponding subalgebra are each other's commutants.\n\n{{harvtxt|Jacobson|1956|loc=Chapter 7.2}} gave an extension to sub division rings that might have infinite index, which correspond to closed subalgebras in the finite topology.\n\n==References==\n*{{Citation | last=Cartan|first=Henri|authorlink=Henri Cartan | title=Les principaux théorèmes de la théorie de Galois pour les corps non nécessairement commutatifs |mr=0020983 | year=1947 | journal=[[Comptes rendus de l'Académie des Sciences]] | volume=224 | pages=249–251}}\n*{{Citation | last=Cartan|first=Henri|authorlink=Henri Cartan | title=Théorie de Galois pour les corps non commutatifs | url=http://www.numdam.org/item?id=ASENS_1947_3_64__59_0 |mr=0023237 | year=1947 | journal=[[Annales Scientifiques de l'École Normale Supérieure]] |series=Série 3 | issn=0012-9593 | volume=64 | pages=59–77}}\n*{{Citation | last1=Jacobson | first1=Nathan | author1-link=Nathan Jacobson | title=Galois theory of purely inseparable fields of exponent one | jstor= 2371772 |mr=0011079 | year=1944 | journal=[[American Journal of Mathematics]] | issn=0002-9327 | volume=66 | pages=645–648 | doi=10.2307/2371772}}\n*{{Citation | last1=Jacobson | first1=Nathan | author1-link=Nathan Jacobson | title=A note on division rings | jstor=2371651 |mr=0020981 | year=1947 | journal=[[American Journal of Mathematics]] | issn=0002-9327 | volume=69 | pages=27–36 | doi=10.2307/2371651}}\n*{{Citation | last1=Jacobson | first1=Nathan | author1-link=Nathan Jacobson | title=Structure of rings | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=American Mathematical Society, Colloquium Publications | isbn=978-0-8218-1037-8 |mr=0081264 | year=1956 | volume=37}}\n*{{Citation | last1=Jacobson | first1=Nathan | author1-link=Nathan Jacobson | title=Lectures in abstract algebra. Vol III: Theory of fields and Galois theory | publisher=D. Van Nostrand Co., Inc., Princeton, N.J.-Toronto, Ont.-London-New York | isbn=978-0-387-90168-8 |mr=0172871 | year=1964}}\n*{{eom|id=Jacobson-Bourbaki_theorem|first=F. |last=Kreimer }}\n\n{{DEFAULTSORT:Jacobson-Bourbaki theorem}}\n[[Category:Field theory]]\n[[Category:Theorems in algebra]]"
    },
    {
      "title": "Koecher–Vinberg theorem",
      "url": "https://en.wikipedia.org/wiki/Koecher%E2%80%93Vinberg_theorem",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|Theorem of operator algebra}}\nIn [[operator algebra]], the '''Koecher–Vinberg theorem''' is a reconstruction theorem for real [[Jordan algebra]]s. It was proved independently by [[Max Koecher]] in 1957<ref name=\"koecher\">{{cite journal|last=Koecher|first=Max|title=Positivitatsbereiche im R<sup>n</sup>|journal=American Journal of Mathematics|year=1957|volume=97|issue=3|pages=575–596|doi=10.2307/2372563}}</ref> and [[Ernest Vinberg]] in 1961.<ref>{{cite journal|last=Vinberg|first=E. B.|title=Homogeneous Cones|journal=Soviet Math. Dokl.|year=1961|volume=1|pages=787–790}}</ref> It provides a [[one-to-one correspondence]] between [[jordan algebra#Formally real Jordan algebras|formally real Jordan algebras]] and so-called domains of positivity. Thus it links [[operator algebra]]ic and [[Convex function|convex]] [[Order theory|order theoretic]] views on state spaces of physical systems.\n\n==Statement==\nA [[convex cone]] <math>C</math> is called ''regular'' if <math>a=0</math> whenever both <math>a</math> and <math>-a</math> are in the closure <math>\\overline{C}</math>.\n\nA convex cone <math>C</math> in a [[vector space]] <math>A</math> with an [[inner product]] has a ''dual cone'' <math>C^* = \\{ a \\in A \\colon \\forall b \\in C \\langle a,b\\rangle > 0 \\}</math>. The cone is called ''self-dual'' when <math>C=C^*</math>. It is called ''homogeneous'' when to any two points <math>a,b \\in C</math> there is a real [[linear transformation]] <math>T \\colon A \\to A</math> that restricts to a bijection <math>C \\to C</math> and satisfies <math>T(a)=b</math>.\n\nThe Koecher–Vinberg theorem now states that these properties precisely characterize the positive cones of Jordan algebras.\n\n'''Theorem''': There is a one-to-one correspondence between [[jordan algebra#Formally real Jordan algebras|formally real Jordan algebras]] and convex cones that are:\n* open;\n* regular;\n* homogeneous;\n* self-dual.\n\nConvex cones satisfying these four properties are called ''domains of positivity'' or ''symmetric cones''. The domain of positivity associated with a real Jordan algebra <math>A</math> is the interior of the 'positive' cone <math>A_+ = \\{ a^2 \\colon a \\in A \\}</math>.\n\n==Proof==\nFor a proof, see {{harvtxt|Koecher|1999}}<ref name=minnesota>{{cite book|last=Koecher|first=Max|title=The Minnesota Notes on Jordan Algebras and Their Applications|year=1999|publisher=Springer|isbn=3-540-66360-6|url=https://books.google.com/books?id=RHrdf06-vZ0C&printsec=frontcover#v=onepage&q&f=false|ref=harv}}\n</ref> or {{harvtxt|Faraut|Koranyi|1994}}.<ref name=farautkoranyi>{{cite book|first1=J.|last1=Faraut|author2-link=Ádám Korányi|first2=A.|last2=Koranyi|title=Analysis on Symmetric Cones|year=1994|publisher=Oxford University Press|ref=harv}}</ref>\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Koecher-Vinberg theorem}}\n[[Category:Non-associative algebras]]\n[[Category:Theorems in algebra]]"
    },
    {
      "title": "Krull–Akizuki theorem",
      "url": "https://en.wikipedia.org/wiki/Krull%E2%80%93Akizuki_theorem",
      "text": "In algebra, the '''Krull–Akizuki theorem''' states the following: let ''A'' be a one-dimensional [[Reduced ring|reduced]] [[noetherian ring]],<ref>In this article, a ring is commutative and has unity.</ref> ''K'' its [[total ring of fractions]]. If ''B'' is a subring of a finite extension ''L'' of ''K'' containing ''A'' and is not a field, then ''B'' is a one-dimensional noetherian ring. Furthermore, for every nonzero ideal ''I'' of ''B'', <math>B/I</math> is finite over ''A''.<ref>{{harvnb|Bourbaki|1989|loc=Ch VII, §2, no. 5, Proposition 5}}</ref>\n\nNote that the theorem does not say that ''B'' is finite over ''A''. The theorem does not extend to higher dimension. One important consequence of the theorem is that the [[integral closure]] of a [[Dedekind domain]] ''A'' in a finite extension of the field of fractions of ''A'' is again a Dedekind domain. This consequence does generalize to a higher dimension: the [[Mori–Nagata theorem]] states that the integral closure of a noetherian domain is a [[Krull domain]].\n\n== Proof ==\nHere, we give a proof when <math>L = K</math>. Let <math>\\mathfrak{p}_i</math> be minimal prime ideals of ''A''; there are finitely many of them. Let <math>K_i</math> be the field of fractions of <math>A/{\\mathfrak{p}_i}</math> and <math>I_i</math> the kernel of the natural map <math>B \\to K \\to K_i</math>. Then we have:\n:<math>A/{\\mathfrak{p}_i} \\subset B/{I_i} \\subset K_i</math>.\nNow, if the theorem holds when ''A'' is a domain, then this implies that ''B'' is a one-dimensional noetherian domain since each <math>B/{I_i}</math> is and since <math>B = \\prod B/{I_i}</math>. Hence, we reduced the proof to the case ''A'' is a domain. Let <math>0 \\ne I \\subset B</math> be an ideal and let ''a'' be a nonzero element in the nonzero ideal  <math>I \\cap A</math>. Set <math>I_n = a^nB \\cap A + aA</math>. Since <math>A/aA</math> is a zero-dim noetherian ring; thus, [[artinian ring|artinian]], there is an ''l'' such that <math>I_n = I_l</math> for all <math>n \\ge l</math>. We claim\n:<math>a^l B \\subset a^{l+1}B + A.</math>\nSince it suffices to establish the inclusion locally, we may assume ''A'' is a local ring with the maximal ideal <math>\\mathfrak{m}</math>. Let ''x'' be a nonzero element in ''B''. Then, since ''A'' is noetherian, there is an ''n'' such that <math>\\mathfrak{m}^{n+1} \\subset x^{-1} A</math> and so <math>a^{n+1}x \\in a^{n+1}B \\cap A \\subset I_{n+2}</math>. Thus,\n:<math>a^n x \\in a^{n+1} B \\cap A + A.</math>\nNow, assume ''n'' is a minimum integer such that <math>n \\ge l</math> and the last inclusion holds. If <math>n > l</math>, then we easily see that <math>a^n x \\in I_{n+1}</math>. But then the above inclusion holds for <math>n-1</math>, contradiction. Hence, we have <math>n = l</math> and this establishes the claim. It now follows:\n:<math>B/{aB} \\simeq a^l B/a^{l+1} B \\subset (a^{l +1}B + A)/a^{l+1} B \\simeq A/{a^{l +1}B \\cap A}.</math>\nHence, <math>B/{aB}</math> has finite length as ''A''-module. In particular, the image of ''I'' there is finitely generated and so ''I'' is finitely generated. Finally, the above shows that <math>B/{aB}</math> has zero dimension and so ''B'' has dimension one. <math>\\square</math>\n\n== References ==\n{{reflist}}\n*[[Nicolas Bourbaki]], ''Commutative algebra''\n\n{{DEFAULTSORT:Krull-Akizuki theorem}}\n[[Category:Theorems in algebra]]"
    },
    {
      "title": "Liouville's theorem (differential algebra)",
      "url": "https://en.wikipedia.org/wiki/Liouville%27s_theorem_%28differential_algebra%29",
      "text": "In [[mathematics]], '''Liouville's theorem''', originally formulated by [[Joseph Liouville]] in 1833 to 1841,<ref>{{harvnb|Liouville|1833a}}.</ref><ref>{{harvnb|Liouville|1833b}}.</ref><ref>{{harvnb|Liouville|1833c}}.</ref> places an important restriction on [[antiderivative]]s that can be expressed as elementary functions.\n\nThe antiderivatives of certain [[elementary function]]s cannot themselves be expressed as elementary functions.  A standard example of such a function is <math>e^{-x^2},</math> whose antiderivative is (with a multiplier of a constant) the [[error function]], familiar from [[statistics]].  Other examples include the functions <math> \\frac{ \\sin ( x ) }{ x }</math> and <math> x^x </math>.\n\nLiouville's theorem states that elementary antiderivatives, if they exist, must be in the same [[differential field]] as the function, plus possibly a finite number of logarithms.\n\n== Definitions ==\n\nFor any differential field ''F'', there is a subfield\n\n: Con(''F'') = {''f'' in ''F'' | ''Df'' = 0},\n\ncalled the [[Mathematical constant|constant]]s of ''F''. Given two differential fields ''F'' and ''G'', ''G'' is called a '''logarithmic extension''' of ''F'' if ''G'' is a [[field extension|simple transcendental extension]] of ''F'' (i.e. ''G'' = ''F''(''t'') for some [[Transcendental element|transcendental]] ''t'') such that\n\n: ''Dt'' = ''Ds''/''s'' for some ''s'' in ''F''.\n\nThis has the form of a [[logarithmic derivative]]. Intuitively, one may think of ''t'' as the [[logarithm]] of some element ''s'' of ''F'', in which case, this condition is analogous to the ordinary [[chain rule]].  However, ''F'' is not necessarily equipped with a unique logarithm; one might adjoin many \"logarithm-like\" extensions to ''F''.  Similarly, an '''exponential extension''' is a simple transcendental extension that satisfies\n\n: ''Dt'' = ''t''&nbsp;''Ds''.\n\nWith the above caveat in mind, this element may be thought of as an exponential of an element ''s'' of ''F''.  Finally, ''G'' is called an '''elementary differential extension''' of ''F'' if there is a finite chain of subfields from ''F'' to ''G'' where each extension in the chain is either algebraic, logarithmic, or exponential.\n\n== Basic theorem ==\n\nSuppose ''F'' and ''G'' are differential fields, with Con(''F'') = Con(''G''), and that ''G'' is an elementary differential extension of ''F''.  Let ''a'' be in ''F'', ''y'' in G, and suppose ''Dy'' = ''a'' (in words, suppose that ''G'' contains an antiderivative of ''a'').  Then there exist ''c''<sub>1</sub>, ..., ''c''<sub>''n''</sub> in Con(''F''), ''u''<sub>1</sub>, ..., ''u''<sub>''n''</sub>, ''v'' in ''F'' such that\n\n:<math>a = c_1\\frac{Du_1}{u_1}+\\dotsb+c_n\\frac{Du_n}{u_n} + Dv.</math>\n\nIn other words, the only functions that have \"elementary antiderivatives\" (i.e. antiderivatives living in, at worst, an elementary differential extension of ''F'') are those with this form.  Thus, on an intuitive level, the theorem states that the only elementary antiderivatives are the \"simple\" functions plus a finite number of logarithms of \"simple\" functions.\n\nA proof of Liouville's theorem can be found in section 12.4 of Geddes, et al.\n\n== Examples ==\n\nAs an example, the field '''C'''(''x'') of [[rational function]]s in a single variable has a derivation given by the standard [[derivative]] with respect to that variable.  The constants of this field are just the [[complex number]]s '''C'''.\n\nThe function <math> \\tfrac{1}{x} </math>, which exists in '''C'''(''x''), does not have an antiderivative in '''C'''(''x'').  Its antiderivatives ln&nbsp;''x''&nbsp;+&nbsp;''C'' do, however, exist in the logarithmic extension '''C'''(''x'',&nbsp;ln&nbsp;''x'').\n\nLikewise, the function <math>\\tfrac{1}{x^2+1}</math> does not have an antiderivative in '''C'''(''x'').  Its antiderivatives tan<sup>&minus;1</sup>(''x'')&nbsp;+&nbsp;''C'' do not seem to satisfy the requirements of the theorem, since they are not (apparently) sums of rational functions and logarithms of rational functions.  However, a calculation with [[Euler's formula]] <math>e^{i \\theta} = \\cos \\theta + i \\sin \\theta</math> shows that in fact the antiderivatives can be written in the required manner (as logarithms of rational functions).\n\n:<math>\n\\begin{align}\ne^{2i \\theta} & = \\frac{e^{i \\theta}}{e^{-i \\theta}} \n= \\frac{\\cos \\theta + i \\sin \\theta}{\\cos \\theta - i \\sin \\theta} \n= \\frac{1 + i \\tan \\theta}{1 - i \\tan \\theta} \\\\[8pt]\n\\theta & \n= \\frac{1}{2i} \\ln \\left( \\frac{1 + i \\tan \\theta}{1 - i \\tan \\theta} \\right) \\\\[8pt]\n\\tan^{-1} x & \n= \\frac{1}{2i} \\ln \\left(  \\frac{1+ix}{1-ix} \\right)\n\\end{align}\n</math>\n\n== Relationship with differential Galois theory ==\n\nLiouville's theorem is sometimes presented as a theorem in [[differential Galois theory]], but this is not strictly true.  The theorem can be proved without any use of Galois theory.  Furthermore, the Galois group of a simple antiderivative is either trivial (if no field extension is required to express it), or is simply the additive group of the constants (corresponding to the constant of integration).  Thus, an antiderivative's differential Galois group does not encode enough information to determine if it can be expressed using elementary functions, the major condition of Liouville's theorem.\n<!--\n== Example of theorem ==\nSuppose we want to know whether a function of the form f*e<sup>g</sup> has an elementary antiderivative, with ''f'' and ''g'' in ''C''(''x'') -->\n\n== See also ==\n{{Div col}}\n*[[Algebraic function]]\n*[[Differential Galois theory]]\n*[[Elementary function]]\n*[[Risch algorithm]]\n*[[Transcendental function]]\n{{Div col end}}\n\n==Notes==\n{{reflist|2}}\n\n==References==\n*{{Citation | last1=Bertrand | first1=D. | title=Review of \"Lectures on differential Galois theory\" | url=http://www.ams.org/bull/1996-33-02/S0273-0979-96-00652-0/S0273-0979-96-00652-0.pdf | year=1996 | journal=[[Bulletin of the American Mathematical Society]] | issn=0002-9904 | volume=33 | issue=2|doi=10.1090/s0273-0979-96-00652-0 }}\n* {{cite book | first1=Keith O. |last1=Geddes |first2=Stephen R. |last2=Czapor |first3=George |last3=Labahn | title=Algorithms for Computer Algebra | publisher=Kluwer Academic Publishers | year=1992 | isbn=0-7923-9259-0}}\n*{{Cite journal\n | last = Liouville\n | first = Joseph\n | author-link = Joseph Liouville\n | title = Premier mémoire sur la détermination des intégrales dont la valeur est algébrique\n | journal = Journal de l'École Polytechnique\n | year = 1833a\n | volume = tome XIV\n | pages = 124–148\n | url = http://gallica.bnf.fr/ark:/12148/bpt6k433678n/f127.item.r=Liouville\n | ref = harv\n}}\n*{{Cite journal\n | last = Liouville\n | first = Joseph\n | author-link = Joseph Liouville\n | title = Second mémoire sur la détermination des intégrales dont la valeur est algébrique\n | journal = Journal de l'École Polytechnique\n | year = 1833b\n | volume = tome XIV\n | pages = 149–193\n | url = http://gallica.bnf.fr/ark:/12148/bpt6k433678n/f152.item.r=Liouville\n | ref = harv\n}}\n*{{Cite journal\n | last = Liouville\n | first = Joseph\n | author-link = Joseph Liouville\n | title = Note sur la détermination des intégrales dont la valeur est algébrique\n | journal = [[Journal für die reine und angewandte Mathematik]]\n | year = 1833c\n | volume = 10\n | pages = 347–359\n | url = http://gdz.sub.uni-goettingen.de/en/dms/loader/img/?PID=GDZPPN002139332\n | ref = harv\n}}\n*{{Citation | last1=Magid | first1=Andy R. | title=Lectures on differential Galois theory | url=https://books.google.com/books?id=cJ9vByhPqQ8C | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=University Lecture Series | isbn=978-0-8218-7004-4 | mr=1301076 | year=1994 | volume=7}}\n*{{Citation | last1=Magid | first1=Andy R. | title=Differential Galois theory | url=http://www.ams.org/notices/199909/fea-magid.pdf | mr=1710665 | year=1999 | journal=[[Notices of the American Mathematical Society]] | issn=0002-9920 | volume=46 | issue=9 | pages=1041–1049}}\n*{{Citation | last1=van der Put | first1=Marius | last2=Singer | first2=Michael F. | title=Galois theory of linear differential equations | url=http://www4.ncsu.edu/~singer/ms_papers.html | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences] | isbn=978-3-540-44228-8 | mr=1960772 | year=2003 | volume=328}}\n\n==External links==\n* {{MathWorld|id=LiouvillesPrinciple|title=Liouville's Principle}}\n\n[[Category:Field theory]]\n[[Category:Differential algebra]]\n[[Category:Differential equations]]\n[[Category:Theorems in algebra]]"
    },
    {
      "title": "Mason–Stothers theorem",
      "url": "https://en.wikipedia.org/wiki/Mason%E2%80%93Stothers_theorem",
      "text": "The '''Mason–Stothers theorem''', or simply '''Mason's theorem''', is a mathematical [[theorem]] about [[polynomials]], analogous to the [[abc conjecture|''abc'' conjecture]] for integers. It is named after {{Interlanguage link multi|W. Wilson Stothers|nl|3=Walter Wilson Stothers}}, who published it in 1981,<ref>{{citation|first=W. W.|last=Stothers|title=Polynomial identities and hauptmoduln|journal=Quarterly J. Math. Oxford|series=2|volume=32|year=1981|pages=349–370 |doi=10.1093/qmath/32.3.349}}.</ref> and [[R. C. Mason]], who rediscovered it shortly thereafter.<ref>{{citation|last=Mason|first=R. C.|title=Diophantine Equations over Function Fields|location=Cambridge, England|publisher=Cambridge University Press|year=1984|series=London Mathematical Society Lecture Note Series|volume=96}}.</ref>\n\nThe theorem states:\n:Let {{math|''a''(''t'')}}, {{math|''b''(''t'')}}, and {{math|''c''(''t'')}} be [[relatively prime polynomials]] over a field such that {{math|1=''a'' + ''b'' = ''c''}} and such that not all of them have vanishing derivative. Then\n::<math>\\max\\{\\deg(a),\\deg(b),\\deg(c)\\} \\le \\deg(\\operatorname{rad}(abc))-1.</math>\nHere {{math|rad(''f'')}} is the product of the distinct irreducible factors of {{mvar|f}}. For algebraically closed fields it is the polynomial of minimum degree that has the same [[Root of a function|root]]s as {{mvar|f}}; in this case {{math|deg(rad(''f''))}} gives the number of distinct roots of {{mvar|f}}.<ref>{{cite book | author = Lang, Serge | authorlink = Serge Lang | title = Algebra| publisher = Springer-Verlag | location =New York, Berlin, Heidelberg | year = 2002 | isbn = 0-387-95385-X|page=194}}</ref>\n\n==Examples==\n\n*Over fields of characteristic 0 the condition that {{mvar|a}}, {{mvar|b}}, and {{mvar|c}} do not all have vanishing derivative is equivalent to the condition that they are not all constant. Over fields of characteristic {{math|''p'' > 0}} it is not enough to assume that they are not all constant. For example, the identity {{math|1=''t''<sup>''p''</sup> + 1 = (''t'' + 1)<sup>''p''</sup>}} gives an example where the maximum degree of the three polynomials ({{mvar|a}} and {{mvar|b}} as the summands on the left hand side, and {{mvar|c}} as the right hand side) is {{mvar|p}}, but the degree of the radical is only&nbsp;{{math|2}}.\n*Taking {{math|1=''a''(''t'') = ''t''<sup>''n''</sup>}} and {{math|1=''c''(''t'') = (''t''+1)<sup>''n''</sup>}} gives an example where equality holds in the Mason–Stothers theorem, showing that the inequality is in some sense the best possible.\n*A corollary of the Mason–Stothers theorem is the analog of [[Fermat's last theorem]] for function fields: if {{math|1= ''a''(''t'')<sup>''n''</sup> + ''b''(''t'')<sup>''n''</sup> = ''c''(''t'')<sup>''n''</sup>}} for {{mvar|a}}, {{mvar|b}}, {{mvar|c}} relatively prime polynomials over a field of characteristic not dividing {{mvar|n}}  and {{math|''n'' > 2}} then either at least one of {{mvar|a}}, {{mvar|b}}, or {{mvar|c}} is 0 or they are all constant.\n\n==Proof==\n{{harvtxt|Snyder|2000}} gave the following elementary proof of the Mason–Stothers theorem.<ref>{{citation|mr=1781918 |doi=10.1007/s000170050074\n|last=Snyder|first= Noah\n|title=An alternate proof of Mason's theorem\n|journal=Elemente der Mathematik |volume=55 |year=2000|issue=3|pages= 93–94|url=http://cr.yp.to/bib/2000/snyder.pdf}}.</ref>\n\nStep 1. The condition {{math|1=''a'' + ''b'' + ''c'' = 0}} implies that the [[Wronskian]]s {{math|1=''W''(''a'', ''b'') = ''ab''′ − ''a''′''b''}}, {{math|''W''(''b'', ''c'')}}, and {{math|''W''(''c'', ''a'')}} are all equal. Write {{mvar|W}} for their common value.\n\nStep 2. The condition that at least one of the derivatives {{math|''a''′}}, {{math|''b''′}}, or {{math|''c''′}} is nonzero and that {{mvar|a}}, {{mvar|b}}, and {{mvar|c}} are coprime is used to show that {{mvar|W}} is nonzero.\nFor example, if {{math|1=''W'' = 0}} then {{math|1=''ab''′ = ''a''′''b''}} so {{mvar|a}} divides {{math|''a''′}} (as {{mvar|a}} and {{mvar|b}} are coprime) so {{math|1=''a''′ = 0}} (as {{math|deg ''a'' > deg ''a''′}} unless {{mvar|a}} is constant).\n\nStep 3. {{mvar|W}} is divisible by each of the greatest common divisors {{math|(''a'', ''a''′)}}, {{math|(''b'', ''b''′)}}, and {{math|(''c'', ''c''′)}}. Since these are coprime it is divisible by their product, and since {{mvar|W}} is nonzero we get\n:{{math|deg (''a'', ''a''′) + deg (''b'', ''b''′) + deg (''c'', ''c''′) ≤ deg ''W''.}}\n\nStep 4. Substituting in the inequalities\n:{{math|deg (''a'', ''a''′) ≥ deg ''a''}} − (number of distinct roots of {{mvar|a}})\n:{{math|deg (''b'', ''b''′) ≥ deg ''b''}} − (number of distinct roots of {{mvar|b}})\n:{{math|deg (''c'', ''c''′) ≥ deg ''c''}} − (number of distinct roots of {{mvar|c}})\n(where the roots are taken in some algebraic closure) and\n:{{math|deg ''W'' ≤ deg ''a'' + deg ''b'' − 1 }}\nwe find that\n:{{math|deg ''c'' ≤ (number of distinct roots of ''abc'')  − 1}}\nwhich is what we needed to prove.\n\n==Generalizations==\nThere is a natural generalization in which the ring of polynomials is replaced by a one-dimensional [[Algebraic function field|function field]].\nLet {{mvar|k}} be an algebraically closed field of characteristic 0, let {{math|''C/k''}} be a [[Projective variety|smooth projective curve]]\nof [[Geometric genus|genus]] {{mvar|g}}, let\n::<math> a,b\\in k(C)</math> be rational functions on {{mvar|C}} satisfying <math>a+b=1</math>,\nand let\n{{mvar|S}} be a set of points in {{math|''C''(''k'')}} containing all of the zeros and poles of {{mvar|a}} and {{mvar|b}}.\nThen\n::<math> \\max\\bigl\\{ \\deg(a),\\deg(b) \\bigr\\} \\le \\max\\bigl\\{|S| + 2g - 2,0\\bigr\\}.</math>\nHere the degree of a function in {{math|''k''(''C'')}} is the degree of\nthe map it induces from {{mvar|C}} to '''P'''<sup>1</sup>.\nThis was proved by Mason, with an alternative short proof published the same year by [[Joseph H. Silverman|J. H. Silverman]]\n.<ref>{{citation|first=J. H.|last=Silverman|title=The S-unit equation over function fields|journal=Proc. Camb. Philos. Soc.|volume=95|year=1984|pages=3–4}}</ref>\n\nThere is a further generalization, due independently to [[José Felipe Voloch|J. F. Voloch]]<ref>{{citation|first=J. F.|last=Voloch|title=Diagonal equations over function fields|journal=Bol. Soc. Brasil. Mat.|volume=16|year=1985|pages=29–39}}</ref>\nand to\n[[W. Dale Brownawell|W. D. Brownawell]] and [[David Masser|D. W. Masser]],<ref>{{citation|first=W. D.|last=Brownawell|first2=D. W.|last2=Masser|title=Vanishing sums in function fields|journal=Math. Proc. Cambridge Philos. Soc.|volume=100|year=1986|pages=427–434}}</ref>\nthat gives an upper bound for  {{mvar|n}}-variable {{mvar|S}}-unit\nequations {{math|1=''a''<sub>1</sub> + ''a''<sub>2</sub> + ... + ''a''<sub>''n''</sub> = 1}} provided that\nno subset of the {{math|''a''<sub>''i''</sub>}} are {{mvar|k}}-linearly dependent. Under this assumption, they prove that\n::<math> \\max\\bigl\\{ \\deg(a_1),\\ldots,\\deg(a_n) \\bigr\\} \\le \\frac{1}{2}n(n-1)\\max\\bigl\\{|S| + 2g - 2,0\\bigr\\}.</math>\n\n==References==\n{{reflist}}\n\n==External links==\n*{{mathworld|urlname=MasonsTheorem|title=Mason's Theorem}}\n*[http://topologicalmusings.wordpress.com/2008/03/03/mason-stothers-theorem-and-the-abc-conjecture/ Mason-Stothers Theorem and the ABC Conjecture], Vishal Lama. A cleaned-up version of the proof from Lang's book.\n\n{{DEFAULTSORT:Mason-Stothers theorem}}\n[[Category:Theorems in algebra]]\n[[Category:Polynomials]]"
    },
    {
      "title": "Matlis duality",
      "url": "https://en.wikipedia.org/wiki/Matlis_duality",
      "text": "In [[algebra]], '''Matlis duality''' is a [[duality (mathematics)|duality]] between [[Artinian module|Artinian]] and [[Noetherian module|Noetherian]] [[module (mathematics)|modules]] over a complete Noetherian [[local ring]]. In the special case when the local ring has a field{{clarify|reason=As a subfield? As a module?|date=May 2014}} mapping to the [[residue field]] it is closely related to earlier work by [[Francis Sowerby Macaulay]] on [[polynomial ring]]s and is sometimes called '''Macaulay duality''', and the general case was introduced by {{harvs|txt|last=Matlis|authorlink=Eben Matlis|year=1958}}.\n\n==Statement==\n\nSuppose that ''R'' is a Noetherian complete local ring with residue field ''k'', and choose ''E'' to be an [[injective hull]] of ''k'' (sometimes called a '''Matlis module'''). The dual ''D''<sub>''R''</sub>(''M'') of a module ''M'' is defined to be Hom<sub>''R''</sub>(''M'',''E''). Then Matlis duality states that the duality functor ''D''<sub>''R''</sub> gives an anti-equivalence between the categories of Artinian and Noetherian ''R''-modules. In particular the duality functor gives an anti-equivalence from the category of finite-length modules to itself.\n\n==Examples==\n\nSuppose that the Noetherian complete local ring ''R'' has a subfield ''k'' that maps onto a subfield of finite index of its residue field ''R''/''m''. Then the Matlis dual of any ''R''-module is just its dual as a [[topological vector space]] over ''k'', if the module is given its ''m''-adic topology. In particular the dual of ''R'' as a topological vector space over ''k'' is a Matlis module. This case is closely related to work of Macaulay on graded polynomial rings and is sometimes called Macaulay duality.\n\nIf ''R'' is a [[discrete valuation ring]] with [[quotient field]] ''K'' then the Matlis module is ''K''/''R''. In the special case when ''R'' is the ring of [[p-adic number|''p''-adic numbers]], the Matlis dual of a [[finitely-generated module]] is the [[Pontryagin dual]] of it considered as a [[locally compact group|locally compact]] [[abelian group]].\n\nIf ''R'' is a Cohen–Macaulay local ring of dimension ''d'' with [[dualizing module]] Ω, then the Matlis module is given by the [[local cohomology]] group H{{su|p=''d''|b=''R''}}(Ω). In particular if ''R'' is an Artinian local ring then the Matlis module is the same as the dualizing module.\n\n==Explanation using adjoint functors==\nMatlis duality can be conceptually explained using the language of [[adjoint functor]]s and [[derived category|derived categories]]:<ref>[[Paul Balmer]], Ivo Dell'Ambrogio, and Beren Sanders.\n[https://arxiv.org/abs/1501.01999v1 ''Grothendieck-Neeman duality and the Wirthmüller isomorphism''], 2015. Example 7.2.</ref> the functor between the derived categories of ''R''- and ''k''-modules induced by regarding a ''k''-module as an ''R''-module, admits a right adjoint\n(derived [[internal Hom]])\n:<math>D(k) \\gets D(R) : R\\operatorname{Hom}_R(k, -).</math>\nThis right adjoint sends the injective hull <math>E(k)</math> mentioned above to ''k'', which is a [[dualizing object]] in <math>D(k)</math>. This abstract fact gives then rise to the above-mentioned equivalence.\n\n==See also==\n\n*[[Grothendieck local duality]]\n\n==References==\n{{Reflist}}\n\n*{{Citation | last1=Bruns | first1=Winfried | last2=Herzog | first2=Jürgen | title=Cohen-Macaulay rings | url=https://books.google.com/books?id=LF6CbQk9uScC | publisher=[[Cambridge University Press]] | series=Cambridge Studies in Advanced Mathematics | isbn=978-0-521-41068-7 |mr=1251956 | year=1993 | volume=39}}\n*{{Citation | last1=Matlis | first1=Eben | author1-link=Eben Matlis | title=Injective modules over Noetherian rings | url=https://projecteuclid.org/euclid.pjm/1103039896 | mr=0099360 | year=1958 | journal=[[Pacific Journal of Mathematics]] | issn=0030-8730 | volume=8 | pages=511–528 | doi=10.2140/pjm.1958.8.511 }}{{dead link|date=January 2018 |bot=InternetArchiveBot |fix-attempted=yes }}\n\n[[Category:Commutative algebra]]\n[[Category:Theorems in algebra]]"
    },
    {
      "title": "Mitchell's embedding theorem",
      "url": "https://en.wikipedia.org/wiki/Mitchell%27s_embedding_theorem",
      "text": "'''Mitchell's embedding theorem''', also known as the '''Freyd–Mitchell theorem''' or the '''full embedding theorem''', is a result about [[abelian category|abelian categories]]; it essentially states that these categories, while rather abstractly defined, are in fact [[concrete category|concrete categories]] of [[module (mathematics)|modules]]. This allows one to use element-wise [[diagram chasing]] proofs in these categories. The theorem is named after [[Barry Mitchell (mathematician)|Barry Mitchell]] and [[Peter Freyd]].\n\n==Details==\nThe precise statement is as follows: if '''A''' is a small abelian category, then there exists a [[ring (mathematics)|ring]] ''R'' (with&nbsp;1, not necessarily commutative) and a [[full functor|full]], [[faithful functor|faithful]] and [[exact functor|exact]] [[functor]] ''F'': '''A''' → ''R''-Mod (where the latter denotes the category of all [[module (mathematics)|left ''R''-modules]]).\n\nThe functor ''F'' yields an [[equivalence of categories|equivalence]] between '''A''' and a [[full subcategory]] of ''R''-Mod in such a way that [[kernel (category theory)|kernels]] and [[cokernel]]s computed in '''A''' correspond to the ordinary kernels and cokernels computed in ''R''-Mod. Such an equivalence is necessarily [[additive functor|additive]].\nThe theorem thus essentially says that the objects of '''A''' can be thought of as ''R''-modules, and the morphisms as ''R''-linear maps, with kernels, cokernels, [[exact sequence]]s and sums of morphisms being determined as in the case of modules. However, [[projective object|projective]] and [[injective object|injective]] objects in '''A''' do not necessarily correspond to projective and injective ''R''-modules.\n\n== Sketch of the proof ==\nLet <math>\\mathcal{L} \\subset \\operatorname{Fun}(\\mathcal{A}, Ab)</math> be the category of [[left exact functor]]s from the abelian category <math>\\mathcal{A}</math> to the [[category of abelian groups]] <math>Ab</math>. First we construct a [[Covariance and contravariance of functors|contravariant]] embedding <math>H:\\mathcal{A}\\to\\mathcal{L}</math> by <math>H(A) = h^A</math> for all <math>A\\in\\mathcal{A}</math>, where <math>h^A</math> is the covariant hom-functor, <math>h^A(X)=\\operatorname{Hom}_\\mathcal{A}(A,X)</math>. The [[Yoneda Lemma]] states that <math>H</math> is fully faithful and we also get the left exactness of <math>H</math> very easily because <math>h^A</math> is already left exact. The proof of the right exactness of <math>H</math> is harder and can be read in Swan, ''Lecture Notes in Mathematics 76''.\n\nAfter that we prove that <math>\\mathcal{L}</math> is an abelian category by using localization theory (also Swan). This is the hard part of the proof.\n\nIt is easy to check that the abelian category <math>\\mathcal{L}</math> is an [[AB5 category|AB5]] category with a [[Generator (category theory)|generator]] \n<math>\\bigoplus_{A\\in\\mathcal{A}} h^A</math>.\nIn other words it is a [[Grothendieck category]] and therefore has an injective cogenerator <math>I</math>.\n\nThe [[endomorphism ring]] <math>R := \\operatorname{Hom}_{\\mathcal{L}} (I,I)</math> is the ring we need for the category of ''R''-modules.\n\nBy <math>G(B) = \\operatorname{Hom}_{\\mathcal{L}} (B,I)</math> we get another contravariant, exact and fully faithful embedding <math>G:\\mathcal{L}\\to R\\operatorname{-Mod}.</math> The composition <math>GH:\\mathcal{A}\\to R\\operatorname{-Mod}</math> is the desired covariant exact and fully faithful embedding.\n\nNote that the proof of the [[Gabriel–Quillen embedding theorem]] for [[exact category|exact categories]] is almost identical.\n\n== References ==\n{{refbegin}}\n*{{cite book\n | author     = R. G. Swan\n | title      = Algebraic K-theory, Lecture Notes in Mathematics 76\n | year       = 1968\n | publisher  = Springer\n}}\n*{{cite book\n | author     = Peter Freyd\n | title      = Abelian categories\n | year       = 1964\n | publisher  = Harper and Row\n}}\n*{{cite book\n | author     = Barry Mitchell\n | title      = The full imbedding theorem\n | year       = 1964\n | publisher  = The Johns Hopkins University Press\n}}\n*{{cite book\n | author     = Charles A. Weibel\n | title      = An introduction to homological algebra\n | year       = 1993\n | publisher  = Cambridge Studies in Advanced Mathematics\n}}\n{{refend}}\n\n[[Category:Module theory]]\n[[Category:Additive categories]]\n[[Category:Theorems in algebra]]"
    },
    {
      "title": "Multinomial theorem",
      "url": "https://en.wikipedia.org/wiki/Multinomial_theorem",
      "text": "In [[mathematics]], the '''multinomial theorem''' describes how to expand a [[power (mathematics)|power]] of a sum in terms of powers of the terms in that sum. It is the generalization of the [[binomial theorem]] from binomials to multinomials.\n\n==Theorem==\nFor any positive integer ''m'' and any nonnegative integer ''n'', the multinomial formula tells us how a sum with ''m'' terms expands when raised to an arbitrary power ''n'':\n\n:<math>(x_1 + x_2  + \\cdots + x_m)^n\n = \\sum_{k_1+k_2+\\cdots+k_m=n} {n \\choose k_1, k_2, \\ldots, k_m}\n  \\prod_{t=1}^m x_t^{k_t}\\,,</math>\nwhere\n:<math> {n \\choose k_1, k_2, \\ldots, k_m}\n = \\frac{n!}{k_1!\\, k_2! \\cdots k_m!}</math>\nis a '''multinomial coefficient'''.  The sum is taken over all combinations of [[nonnegative]] [[integer]] indices ''k''<sub>1</sub> through ''k''<sub>''m''</sub> such that the sum of all ''k''<sub>i</sub> is ''n''.  That is, for each term in the expansion, the exponents of the ''x''<sub>''i''</sub> must add up to ''n''.  Also, as with the [[binomial theorem]], quantities of the form ''x''<sup>0</sup> that appear are taken to equal 1 (even when ''x'' equals zero).\n\nIn the case ''m'' = 2, this statement reduces to that of the binomial theorem.\n\n===Example===\nThe third power of the trinomial ''a'' + ''b'' + ''c'' is given by\n\n:<math>(a+b+c)^3 = a^3 + b^3 + c^3 + 3 a^2 b + 3 a^2 c + 3 b^2 a + 3 b^2 c + 3 c^2 a + 3 c^2 b + 6 a b c.</math>\n\nThis can be computed by hand using the distributive property of multiplication over addition, but it can also be done (perhaps more easily) with the multinomial theorem, which gives us a simple formula for any coefficient we might want. It is possible to \"read off\" the multinomial coefficients from the terms by using the multinomial coefficient formula. For example:\n\n:<math>a^2 b^0 c^1 </math> has the coefficient <math>{3 \\choose 2, 0, 1} = \\frac{3!}{2!\\cdot 0!\\cdot 1!} = \\frac{6}{2 \\cdot 1 \\cdot 1} = 3.</math>\n:<math>a^1 b^1 c^1</math> has the coefficient <math>{3 \\choose 1, 1, 1} = \\frac{3!}{1!\\cdot 1!\\cdot 1!} = \\frac{6}{1 \\cdot 1 \\cdot 1} = 6.</math>\n\n===Alternate expression===\nThe statement of the theorem can be written concisely using [[multiindices]]:\n\n:<math>(x_1+\\cdots+x_m)^n = \\sum_{|\\alpha|=n}{n \\choose \\alpha}x^\\alpha</math>\n\nwhere \n\n:<math>\n\\alpha=(\\alpha_1,\\alpha_2,\\dots,\\alpha_m)\n</math>\nand\n:<math>\nx^\\alpha=x_1^{\\alpha_1} x_2^{\\alpha_2} \\cdots x_m^{\\alpha_m}\n</math>\n\n===Proof===\nThis proof of the multinomial theorem uses the [[binomial theorem]] and [[Mathematical induction|induction]] on ''m''.\n\nFirst, for ''m''&nbsp;=&nbsp;1, both sides equal ''x''<sub>1</sub><sup>''n''</sup> since there is only one term ''k''<sub>1</sub>&nbsp;=&nbsp;''n'' in the sum. For the induction step, suppose the multinomial theorem holds for ''m''.  Then\n\n: <math>\n\\begin{align}\n& (x_1+x_2+\\cdots+x_m+x_{m+1})^n = (x_1+x_2+\\cdots+(x_m+x_{m+1}))^n \\\\[6pt]\n= {} & \\sum_{k_1+k_2+\\cdots+k_{m-1}+K=n}{n\\choose k_1,k_2,\\ldots,k_{m-1},K} x_1^{k_1} x_2^{k_2}\\cdots x_{m-1}^{k_{m-1}}(x_m+x_{m+1})^K\n\\end{align}\n</math>\n\nby the induction hypothesis.  Applying the binomial theorem to the last factor,\n\n:<math> = \\sum_{k_1+k_2+\\cdots+k_{m-1}+K=n}{n\\choose k_1,k_2,\\ldots,k_{m-1},K} x_1^{k_1}x_2^{k_2}\\cdots x_{m-1}^{k_{m-1}}\\sum_{k_m+k_{m+1}=K}{K\\choose k_m,k_{m+1}}x_m^{k_m}x_{m+1}^{k_{m+1}}</math>\n:<math> = \\sum_{k_1+k_2+\\cdots+k_{m-1}+k_m+k_{m+1}=n}{n\\choose k_1,k_2,\\ldots,k_{m-1},k_m,k_{m+1}} x_1^{k_1}x_2^{k_2}\\cdots x_{m-1}^{k_{m-1}}x_m^{k_m}x_{m+1}^{k_{m+1}}\n</math>\n\nwhich completes the induction.  The last step follows because\n\n:<math>{n\\choose k_1,k_2,\\ldots,k_{m-1},K}{K\\choose k_m,k_{m+1}} = {n\\choose k_1,k_2,\\ldots,k_{m-1},k_m,k_{m+1}},</math>\nas can easily be seen by writing the three coefficients using factorials as follows:\n\n:<math> \\frac{n!}{k_1! k_2! \\cdots k_{m-1}!K!} \\frac{K!}{k_m! k_{m+1}!}=\\frac{n!}{k_1! k_2! \\cdots k_{m+1}!}.</math>\n\n==Multinomial coefficients==\nThe numbers\n:<math> {n \\choose k_1, k_2, \\ldots, k_m}</math>\nappearing in the theorem are the [[Binomial coefficient#Generalization to multinomials|multinomial coefficients]].  They can be expressed in numerous ways, including as a product of [[binomial coefficient]]s or of [[factorial]]s:\n:<math>\n{n \\choose k_1, k_2, \\ldots, k_m} = \\frac{n!}{k_1!\\, k_2! \\cdots k_m!} = {k_1\\choose k_1}{k_1+k_2\\choose k_2}\\cdots{k_1+k_2+\\cdots+k_m\\choose k_m}\n </math>\n\n===Sum of all multinomial coefficients===\nThe substitution of ''x''<sub>''i''</sub>&nbsp;=&nbsp;1 for all ''i'' into the multinomial theorem\n:<math>\\sum_{k_1+k_2+\\cdots+k_m=n} {n \\choose k_1, k_2, \\ldots, k_m} x_1^{k_1} x_2^{k_2} \\cdots x_m^{k_m}\n= (x_1 + x_2  + \\cdots + x_m)^n</math>\ngives immediately that \n:<math>\n\\sum_{k_1+k_2+\\cdots+k_m=n} {n \\choose k_1, k_2, \\ldots, k_m} = m^n.\n</math>\n\n===Number of multinomial coefficients===\n\nThe number of terms in a multinomial sum, #<sub>''n'',''m''</sub>, is equal to the number of monomials of degree ''n'' on the variables ''x''<sub>1</sub>,&nbsp;…,&nbsp;''x''<sub>''m''</sub>:\n:<math>\n\\#_{n,m} = {n+m-1 \\choose m-1}.\n</math>\n\nThe count can be performed easily using the method of [[Stars and bars (combinatorics)|stars and bars]].\n\n===Valuation of multinomial coefficients===\nThe largest power of a prime <math>p</math> that divides a multinomial coefficient may be computed using a generalization of [[Kummer's theorem]].\n\n==Interpretations==\n\n===Ways to put objects into bins===\nThe multinomial coefficients have a direct combinatorial interpretation, as the number of ways of depositing ''n'' distinct objects into ''m'' distinct bins, with ''k''<sub>1</sub> objects in the first bin, ''k''<sub>2</sub> objects in the second bin, and so on.<ref>{{cite web |url=http://dlmf.nist.gov/ |title=NIST Digital Library of Mathematical Functions |author=[[National Institute of Standards and Technology]] |date=May 11, 2010 |at=[http://dlmf.nist.gov/26.4 Section 26.4] |accessdate=August 30, 2010}}</ref>\n\n===Number of ways to select according to a distribution===\nIn [[statistical mechanics]] and [[combinatorics]] if one has a number distribution of labels then the multinomial coefficients naturally arise from the binomial coefficients. Given a number distribution  {''n''<sub>''i''</sub>} on a set of ''N'' total items, ''n''<sub>''i''</sub> represents the number of items to be given the label ''i''.  (In statistical mechanics ''i'' is the label of the energy state.)\n\nThe number of arrangements is found by \n*Choosing ''n''<sub>1</sub> of the total ''N'' to be labeled 1.  This can be done <math>N\\choose n_1</math> ways.\n*From the remaining ''N''&nbsp;−&nbsp;''n''<sub>1</sub> items choose ''n''<sub>2</sub> to label 2.  This can be done <math>N-n_1 \\choose n_2</math> ways.\n*From the remaining ''N''&nbsp;−&nbsp;''n''<sub>1</sub>&nbsp;−&nbsp;''n''<sub>2</sub> items choose ''n''<sub>3</sub> to label 3.  Again, this can be done <math>N-n_1-n_2 \\choose n_3</math> ways.\n\nMultiplying the number of choices at each step results in:\n:<math>{N \\choose n_1}{N-n_1\\choose n_2}{N-n_1-n_2\\choose n_3}\\cdots=\\frac{N!}{(N-n_1)!n_1!} \\cdot \\frac{(N-n_1)!}{(N-n_1-n_2)!n_2!} \\cdot \\frac{(N-n_1-n_2)!}{(N-n_1-n_2-n_3)!n_3!}\\cdots.</math>\n\nUpon cancellation, we arrive at the formula given in the introduction.\n\n===Number of unique permutations of words===\nThe multinomial coefficient is also the number of distinct ways to [[permutation|permute]] a [[multiset]] of ''n'' elements, and ''k<sub>i</sub>'' are the [[Multiplicity (mathematics)|multiplicities]] of each of the distinct elements. For example, the number of distinct permutations of the letters of the word MISSISSIPPI, which has 1 M, 4 Is, 4 Ss, and 2 Ps is\n:<math>{11 \\choose 1, 4, 4, 2} = \\frac{11!}{1!\\, 4!\\, 4!\\, 2!} = 34650.</math>\n\n(This is just like saying that there are 11! ways to permute the letters—the common interpretation of [[factorial]] as the number of unique permutations. However, we created duplicate permutations, because some letters are the same, and must divide to correct our answer.)\n\n===Generalized Pascal's triangle===\nOne can use the multinomial theorem to generalize [[Pascal's triangle]] or [[Pascal's pyramid]] to [[Pascal's simplex]]. This provides a quick way to generate a lookup table for multinomial coefficients.\n\n==See also==\n* [[Multinomial distribution]]\n* [[Stars and bars (combinatorics)]]\n\n==References==\n{{Reflist}}\n\n[[Category:Factorial and binomial topics]]\n[[Category:Articles containing proofs]]\n[[Category:Theorems in algebra]]"
    },
    {
      "title": "Nagata's conjecture",
      "url": "https://en.wikipedia.org/wiki/Nagata%27s_conjecture",
      "text": "{{for|the conjecture about curves|Nagata's conjecture on curves}}\n{{Infobox mathematical statement\n| name = Nagata's conjecture\n| image =\n| caption =\n| type =\n| field = [[Algebraic geometry]]\n| conjectured by = [[Masayoshi Nagata]]\n| conjecture date = 1972\n| open problem =\n| first proof by = Ualbai Umirbaev and Ivan Shestakov\n| first proof date = 2004\n| implied by =\n| generalizations =\n| consequences =\n}}\n\nIn [[algebra]], '''Nagata's conjecture''' states that '''Nagata's automorphism''' of the polynomial ring ''k''[''x'',''y'',''z''] is [[Wild automorphism|wild]]. The conjecture was proposed by {{harvs|txt|last=Nagata|authorlink= Masayoshi Nagata|year=1972}} and proved by {{harvs|txt| last1=Umirbaev | first1=Ualbai U. | last2=Shestakov | first2=Ivan P. | title=The tame and the wild automorphisms of polynomial rings in three variables | url=https://dx.doi.org/10.1090/S0894-0347-03-00440-5 | doi=10.1090/S0894-0347-03-00440-5 |mr=2015334 | year=2004 | journal=[[Journal of the American Mathematical Society]] | issn=0894-0347 | volume=17 | issue=1 | pages=197–227}}.\n\nNagata's automorphism is given by\n:<math>(x,y,z)\\mapsto(x-2(xz+y^2)y-(xz+y^2)^2z,y+(xz+y^2)z,z).</math>\n\n==References==\n\n*{{Citation | last1=Nagata | first1=Masayoshi | author1-link=Masayoshi Nagata | title=<nowiki>On automorphism group of k[x,y]</nowiki> | url=https://books.google.com/books?id=qvruAAAAMAAJ | publisher=Kinokuniya Book-Store Co. Ltd. | location=Tokyo |mr=0337962 | year=1972}}\n*{{Citation | last1=Umirbaev | first1=Ualbai U. | last2=Shestakov | first2=Ivan P. | title=The tame and the wild automorphisms of polynomial rings in three variables | doi=10.1090/S0894-0347-03-00440-5 |mr=2015334 | year=2004 | journal=[[Journal of the American Mathematical Society]] | issn=0894-0347 | volume=17 | issue=1 | pages=197–227}}\n\n[[Category:Field theory]]\n[[Category:Theorems in algebra]]"
    },
    {
      "title": "Nielsen–Schreier theorem",
      "url": "https://en.wikipedia.org/wiki/Nielsen%E2%80%93Schreier_theorem",
      "text": "In [[group theory]], a branch of mathematics, the '''Nielsen–Schreier theorem''' states that every [[subgroup]] of a [[free group]] is itself free.<ref name=\"stillwell\"/><ref>{{harvnb|Magnus|Karass|Solitar|1976}}, Corollary 2.9, p. 95.</ref><ref name=\"j80\"/> It is named after [[Jakob Nielsen (mathematician)|Jakob Nielsen]] and [[Otto Schreier]].\n\n==Statement of the theorem==\nA free group may be defined from a [[Presentation of a group|group presentation]] consisting of a [[generating set of a group|set of generators]] with no relations. That is, every element is a product of some sequence of generators and their inverses, but these elements do not obey any equations except those trivially following from {{math|''gg''<sup>&minus;1</sup>}} = 1. The elements of a free group may be described as all possible [[Word (group theory)|reduced words]], those [[string (computer science)|strings]] of generators and their inverses in which no generator is adjacent to its own inverse. Two reduced words may be multiplied by [[concatenation|concatenating]] them and then removing any generator-inverse pairs that result from the concatenation.\n\nThe '''Nielsen–Schreier theorem''' states that if ''H'' is a subgroup of a free group ''G'', then ''H'' is itself [[group isomorphism|isomorphic]] to a free group. That is, there exists a set ''S'' of elements which generate ''H'', with no nontrivial relations among the elements of ''S''.\n\nThe '''Nielsen–Schreier formula''', or '''Schreier index formula''', quantifies the result in the case where the subgroup has finite index: if ''G'' is a free group of rank ''n'' (free on ''n'' generators), and ''H'' is a subgroup of finite [[Index of a subgroup|index]] [''G'' : ''H''] = ''e'', then ''H'' is free of rank <math> 1 + e(n{-}1) </math>.<ref>{{harvp|Fried|Jarden|2008|p=355}}</ref>\n\n==Example==\nLet ''G'' be the free group with two generators <math>a,b</math>, and let ''H'' be the subgroup consisting of all reduced words of even length (products of an even number of letters <math> a,b,a^{-1},b^{-1} </math>). Then ''H'' is generated by its six elements <math>p=aa,\\ q=ab,\\ r=ba,\\ s=bb,\\ t=ab^{-1},\\ u=a^{-1}b.</math> A factorization of any reduced word in ''H'' into these generators and their inverses may be constructed simply by taking consecutive pairs of letters in the reduced word. However, this is not a free presentation of ''H'' because the last three generators can be written in terms of the first three as <math>s=rp^{-1}q,\\ t=pr^{-1},\\ u=p^{-1}q</math>. Rather, ''H'' is generated as a free group by the three elements <math>p=aa,\\ q=ab,\\ r=ba,</math>which have no relations among them; or instead by several other triples of the six generators.<ref>{{harvtxt|Johnson|1997}}, ex. 15, p. 12.</ref> Further, ''G'' is free on ''n'' = 2 generators, ''H'' has index ''e'' = [''G'' : ''H''] = 2 in ''G'', and ''H'' is free on 1 +  ''e''(''n''–1) = 3 generators.  The Nielsen–Schreier theorem states that like ''H'', every subgroup of a free group can be generated as a free group, and if the index of ''H'' is finite, its rank is given by the index formula.\n\n== Proof ==\n[[File:Covering-Graph.png|thumb|300x300px|The free group ''G'' = π<sub>1</sub>(''X'') has ''n'' = 2 generators corresponding to loops ''a'',''b'' from the base point ''P'' in ''X''. The subgroup ''H'' of even-length words, with index ''e'' = [''G'' : ''H''] = 2, corresponds to the covering graph ''Y'' with two vertices corresponding to the cosets ''H'' and ''H''' = ''aH'' = ''bH'' = ''a''<sup>−1</sup>''H'' = ''b<sup>−</sup>''<sup>1</sup>''H'', and two lifted edges for each of the original loop-edges ''a'',''b''. Contracting one of the edges of ''Y'' gives a homotopy equivalence to a bouquet of three circles, so that ''H'' = π<sub>1</sub>(''Y'') is a free group on three generators, for example ''aa'', ''ab'', ''ba''.|alt=]]\nA short and intuitive proof of the Nielsen–Schreier theorem uses the [[algebraic topology]] of [[fundamental group]]s and [[covering space]]s.<ref name=\"stillwell\">{{harvtxt|Stillwell|1993}}, Section 2.2.4, The Nielsen–Schreier Theorem, pp. 103–104.</ref> A free group ''G'' on a set of generators is the fundamental group of a [[rose (topology)|bouquet of circles]], a [[topological graph]] ''X'' with a single vertex and with a loop-edge for each generator.<ref name=\"still-bouquet\"/> Any subgroup ''H'' of the fundamental group is itself the fundamental group of a connected covering space ''Y'' → ''X.'' The space ''Y'' is a (possibly infinite) topological graph, the [[Schreier coset graph]] having one vertex for each [[coset]] in ''G/H''.<ref>{{harvtxt|Stillwell|1993}}, Section 2.2.2, The Subgroup Property, pp. 100–101.</ref> In any connected topological graph, it is possible to shrink the edges of a [[spanning tree]] of the graph, producing a bouquet of circles that has the [[Homotopy#Homotopy equivalence|same fundamental group]] ''H''. Since ''H'' is the fundamental group of a bouquet of circles, it is itself free.<ref name=\"still-bouquet\">{{harvtxt|Stillwell|1993}}, Section 2.1.8, Freeness of the Generators, p. 97.</ref>\n\n[[Simplicial homology]] allows the computation of the rank of ''H'', which is equal to ''h''<sub>1</sub>(''Y''), the first [[Betti number]] of the covering space, the number of independent cycles. For ''G'' free of rank ''n'', the graph ''X'' has ''n'' edges and 1 vertex; assuming ''H'' has finite index [''G'' : ''H''] = ''e'', the covering graph ''Y'' has ''en'' edges and ''e'' vertices.  The first Betti number of a graph is equal to the number of edges, minus the number of vertices, plus the number of connected components; hence the rank of ''H'' is:\n\n<math>h_1(Y) \\,=\\, en-e+1 \\,=\\, 1+e(n{-}1).</math>\n\nThis proof is due to {{harvs|first1=Reinhold|last1=Baer|author1-link=Reinhold Baer|first2=Friedrich|last2=Levi|author2-link=Friedrich Wilhelm Levi|year=1936|txt}}; the original proof by Schreier forms the Schreier graph in a different way as a quotient of the [[Cayley graph]] of {{mvar|G}} modulo the action of {{mvar|H}}.<ref name=\":0\">{{Cite book|title=Modern Graph Theory|last=Bollobas|first=Bela|publisher=Springer Verlag|year=1998|isbn=978-0-387-98488-9|location=|pages=262|chapter=Chapter VIII.1}}</ref>\n\nAccording to [[Schreier's subgroup lemma]], a set of generators for a free presentation of {{mvar|H}} may be constructed from [[cycle (graph theory)|cycles]] in the covering graph formed by concatenating a spanning tree path from a base point (the coset of the identity) to one of the cosets, a single non-tree edge, and an inverse spanning tree path from the other endpoint of the edge back to the base point.<ref>{{harvtxt|Stillwell|1993}}, Section 2.2.6, Schreier Transversals, pp. 105–106.</ref><ref name=\":0\" />\n\n==Axiomatic foundations==\nAlthough several different proofs of the Nielsen–Schreier theorem are known, they all depend on the [[axiom of choice]]. In the proof based on fundamental groups of bouquets, for instance, the axiom of choice appears in the guise of the statement that every connected graph has a spanning tree. The use of this axiom is necessary, as there exist models of [[Zermelo–Fraenkel set theory]] in which the axiom of choice and the Nielsen–Schreier theorem are both false. The Nielsen–Schreier theorem in turn implies a weaker version of the axiom of choice, for finite sets.<ref>{{harvtxt|Läuchli|1962}}</ref><ref>{{harvtxt|Howard|1985}}.</ref>\n\n==History==\nThe Nielsen–Schreier theorem is a [[Nonabelian group|non-abelian]] analogue of an older result of [[Richard Dedekind]], that every subgroup of a [[free abelian group]] is free [[abelian group|abelian]].<ref name=\"j80\">{{harvtxt|Johnson|1980}}, Section 2, The Nielsen–Schreier Theorem, pp. 9–23.</ref>\n\n{{harvs|first=Jakob|last=Nielsen|year=1921|txt}} originally proved a restricted form of the theorem, stating that any finitely-generated subgroup of a free group is free. His proof involves performing a sequence of [[Nielsen transformation]]s on the subgroup's generating set that reduce their length (as reduced words in the free group from which they are drawn).<ref name=\"stillwell\"/><ref>{{harvnb|Magnus|Karass|Solitar|1976}}, Section 3.2, A Reduction Process, pp. 121–140.</ref> Otto Schreier proved the Nielsen–Schreier theorem in its full generality in his 1926 [[habilitation]] [[thesis]], ''Die Untergruppen der freien Gruppe'', also published in 1927 in ''Abh. math. Sem. Hamburg. Univ.''<ref>{{MacTutor|name=Otto Schreier|id=Schreier}}</ref><ref>{{citation|page=117|title=Jakob Nielsen, Collected Mathematical Papers: 1913-1932|first=Vagn Lundsgaard|last=Hansen|publisher=Birkhäuser|year=1986|isbn=978-0-8176-3140-6}}.</ref>\n\nThe topological proof based on fundamental groups of bouquets of circles is due to {{harvs|last1=Baer|first1=Reinhold|author1-link=Reinhold Baer|last2=Levi|first2=Friedrich|author2-link=Friedrich Wilhelm Levi|year=1936|txt}}. Another topological proof, based on the [[Bass–Serre theory]] of [[Group action (mathematics)|group action]]s on [[Real tree|trees]], was published by {{harvs|authorlink=Jean-Pierre Serre|first=Jean-Pierre|last=Serre|year=1970|txt}}.<ref name=\"rotman\">{{harvtxt|Rotman|1995}}, The Nielsen–Schreier Theorem, pp. 383–387.</ref>\n\n==See also==\n*[[Fundamental theorem of cyclic groups]], a similar result for [[cyclic group]]s that in the infinite case may be seen as a special case of the Nielsen–Schreier theorem\n\n==Notes==\n{{reflist|colwidth=40em}}\n\n==References==\n*{{citation|last1=Baer|first=Reinhold|authorlink=Reinhold Baer|title=Freie Produkte und ihre Untergruppen|first2=Friedrich|last2=Levi|author2-link=Friedrich Wilhelm Levi|journal=Compositio Mathematica |volume=3|year=1936|pages=391–398}}.\n*{{citation | last1=Fried | first1=Michael D. | author-link1=Michael D. Fried | last2=Jarden | first2=Moshe | author-link2=Moshe Jarden | title=Field arithmetic | edition=3rd | series=Ergebnisse der Mathematik und ihrer Grenzgebiete. 3. Folge | volume=11 | publisher=[[Springer-Verlag]] | year=2008 | isbn=978-3-540-77269-9 | zbl=1145.12001 | page=70 }}.\n*{{citation\n | last = Howard | first = Paul E.\n | doi = 10.2307/2274234\n | mr = 793126\n | issue = 2\n | journal = The Journal of Symbolic Logic\n | pages = 458–467\n | title = Subgroups of a free group and the axiom of choice\n | volume = 50\n | year = 1985}}.\n*{{citation|title=Topics in the Theory of Group Presentations|volume=42|series=London Mathematical Society lecture note series|first=D. L.|last=Johnson|publisher=Cambridge University Press|year=1980|isbn=978-0-521-23108-4}}.\n*{{citation|title=Presentations of Groups|volume=15|series=London Mathematical Society student texts|first=D. L.|last=Johnson|edition=2nd|publisher=Cambridge University Press|year=1997|isbn=978-0-521-58542-2}}.\n*{{citation\n | last = Läuchli | first = Hans\n | mr = 0143705\n | journal = Commentarii Mathematici Helvetici\n | pages = 1–18\n | title = Auswahlaxiom in der Algebra\n | volume = 37\n | year = 1962\n | doi=10.1007/bf02566957}}.\n*{{Citation | last1=Magnus | first1=Wilhelm | author1-link=Wilhelm Magnus | last2= Karrass | first2 =Abraham | first3=Donald |last3=Solitar | edition=2nd revised | title=Combinatorial Group Theory | publisher=[[Dover Publications]] | year=1976}}.\n*{{Citation | last1=Nielsen | first1=Jakob | author1-link=Jakob Nielsen (mathematician) | title=Om regning med ikke-kommutative faktorer og dens anvendelse i gruppeteorien | language=Danish | jfm=48.0123.03 | year=1921 | journal=Math. Tidsskrift B | volume=1921 | pages=78–94}}.\n*{{citation|title=An Introduction to the Theory of Groups|volume=148|series=Graduate Texts in Mathematics|first=Joseph J.|last=Rotman|author-link=Joseph J. Rotman|edition=4th|publisher=Springer-Verlag|year=1995|isbn=978-0-387-94285-8}}.\n*{{citation|first=J.-P.|last=Serre|authorlink=Jean-Pierre Serre|title=Groupes Discretes|series=Extrait de I'Annuaire du College de France|location=Paris|year=1970}}.\n*{{citation|first=J.-P.|last=Serre|authorlink=Jean-Pierre Serre|title=Trees|publisher=Springer-Verlag|year=1980|isbn=3-540-10103-9}}.\n*{{citation|title=Classical Topology and Combinatorial Group Theory|last=Stillwell|first=John|authorlink=John Stillwell|series=Graduate Texts in Mathematics|volume=72|edition=2nd|year=1993|publisher=Springer-Verlag}}.\n\n{{DEFAULTSORT:Nielsen-Schreier theorem}}\n[[Category:Properties of groups]]\n[[Category:Axiom of choice]]\n[[Category:Theorems in algebra]]"
    },
    {
      "title": "Niven's theorem",
      "url": "https://en.wikipedia.org/wiki/Niven%27s_theorem",
      "text": "In [[mathematics]], '''Niven's theorem''', named after [[Ivan Niven]], states that the only [[rational number|rational values]] of ''&theta;'' in the interval 0°&nbsp;≤&nbsp;''&theta;''&nbsp;≤&nbsp;90°  for which the sine of ''&theta;''&nbsp;degrees is also a rational number are:<ref>{{cite journal |last=Schaumberger |first=Norman |year=1974 |title=A Classroom Theorem on Trigonometric Irrationalities |journal=[[Two-Year College Mathematics Journal]] |jstor=3026991 |volume=5 |pages=73–76}}</ref>\n\n:<math>\n\\begin{align}\n\\sin 0^\\circ & = 0, \\\\[10pt]\n\\sin 30^\\circ & = \\frac 12, \\\\[10pt]\n\\sin 90^\\circ & = 1.\n\\end{align}\n</math>\n\nIn [[radian]]s, one would require that 0&nbsp;≤&nbsp;''x''&nbsp;≤&nbsp;{{pi}}/2, that ''x''/{{pi}} be rational, and that sin&nbsp;''x'' be rational.  The conclusion is then that the only such values are sin&nbsp;0&nbsp;=&nbsp;0, sin&nbsp;{{pi}}/6&nbsp;=&nbsp;1/2, and sin&nbsp;{{pi}}/2&nbsp;=&nbsp;1.\n\nThe theorem appears as Corollary 3.12 in Niven's book on [[irrational number]]s.<ref name=niven>{{cite book |last=Niven |first=Ivan |author-link=Ivan Niven |year=1956 |title=Irrational Numbers |series=The [[Carus Mathematical Monographs]] |number=11 |publisher=[[The Mathematical Association of America]] |mr=0080123 |page=41}}</ref>\n\nThe theorem extends to the other trigonometric functions as well.<ref name=niven/> For rational values of  &theta;, the only rational values of the sine or cosine are 0, ±1/2, and ±1; the only rational values of the secant or cosecant are ±1 and ±2; and the only rational values of the tangent or cotangent are 0 and ±1.<ref>A proof for the cosine case appears as Lemma 12 in {{cite journal\n | last1 = Bennett | first1 = Curtis D.\n | last2 = Glass | first2 = A. M. W.\n | last3 = Székely | first3 = Gábor J.\n | doi = 10.2307/4145241\n | issue = 4\n | journal = American Mathematical Monthly\n | mr = 2057186\n | pages = 322–329\n | title = Fermat's last theorem for rational exponents\n | volume = 111\n | year = 2004}}</ref>\n\n==See also==\n*[[Pythagorean triples]] form right triangles where the trigonometric functions will always take rational values, though the acute angles are not rational\n* [[Trigonometric functions]]\n* [[Trigonometric number]]\n\n==References==\n{{reflist}}\n\n==Further reading==\n* {{cite journal | last1=Olmsted |first1=J. M. H. |year=1945 \n|title=Rational values of trigonometric functions\n|journal=Amer. Math. Monthly\n| volume=52 | number=9 |jstor=2304540 | pages=507–508\n}}\n* {{cite journal |last=Lehmer |first=Derik H. |year=1933 \n|title=A note on trigonometric algebraic numbers \n|journal=Amer. Math. Monthly |jstor=2301023\n| volume = 40 |number=3 | pages=165–166}}\n\n==External links==\n* {{MathWorld |urlname=NivensTheorem |title=Niven's Theorem}}\n* {{ProofWiki |id=Niven's_Theorem |title=Niven's Theorem}}\n\n[[Category:Rational numbers]]\n[[Category:Trigonometry]]\n[[Category:Theorems in geometry]]\n[[Category:Theorems in algebra]]"
    },
    {
      "title": "Norm residue isomorphism theorem",
      "url": "https://en.wikipedia.org/wiki/Norm_residue_isomorphism_theorem",
      "text": "In [[mathematics]],  the '''norm residue isomorphism theorem''' is a long-sought result relating [[Milnor K-theory|Milnor ''K''-theory]] and [[Galois cohomology]]. The result has a relatively elementary formulation and at the same time represents the key juncture in the proofs of many seemingly unrelated theorems from abstract algebra, theory of quadratic forms, algebraic K-theory and the theory of motives. The theorem asserts that a certain statement  holds true for any prime <math>\\ell</math> and any natural number <math>n</math>. [[John Milnor]]<ref name=\"Milnor1970\">Milnor (1970)</ref> speculated that this theorem might be true for <math>\\ell=2</math> and all <math>n</math>, and this question became known as [[Milnor's conjecture]]. The general case was conjectured by [[Spencer Bloch]] and [[Kazuya Kato]] <ref>[http://www.numdam.org/item?id=PMIHES_1986__63__107_0 Bloch, Spencer and Kato, Kazuya, \"p-adic étale cohomology\", Inst. Hautes Études Sci. Publ. Math. No. 63 (1986), p. 118]</ref> and became known as the '''Bloch–Kato conjecture''' or  the '''motivic Bloch–Kato conjecture''' to distinguish it from the Bloch–Kato conjecture on [[special values of L-functions|values of ''L''-functions]].<ref>Bloch, Spencer and Kato, Kazuya, \"L-functions and Tamagawa numbers of motives\", The Grothendieck Festschrift, Vol. I, 333–400, Progr. Math., 86, Birkhäuser Boston, Boston, MA, 1990.</ref>  The norm residue isomorphism theorem was proved by [[Vladimir Voevodsky]] using a number of highly innovative results of [[Markus Rost]].\n\n==Statement==\nFor any integer ℓ invertible in a field <math>k</math> there is a map\n<math>\\partial : k^\\times \\rightarrow H^1(k, \\mu_\\ell) </math>\nwhere <math>\\mu_\\ell</math> denotes the Galois module of ℓ-th roots of unity in some separable closure of ''k''.  It induces an isomorphism <math>k^\\times/(k^\\times)^\\ell \\cong H^1(k, \\mu_\\ell)</math>.  The first hint that this is related to ''K''-theory is that <math>k^\\times</math> is the group ''K''<sub>1</sub>(''k'').  Taking the tensor products and applying the multiplicativity of étale cohomology yields an extension of the map <math>\\partial</math> to maps:\n:<math>\\partial^n : k^\\times \\otimes \\cdots \\otimes k^\\times \\rightarrow H^n_{\\rm\\acute et}(k, \\mu_\\ell^{\\otimes n}).</math>\nThese maps have the property that, for every element ''a'' in <math>k \\setminus \\{0,1\\}</math>, <math>\\partial^n(\\ldots,a,\\ldots,1-a,\\ldots)</math> vanishes.  This is the defining relation of Milnor ''K''-theory.  Specifically, Milnor ''K''-theory is defined to be the graded parts of the ring:\n:<math>K^M_*(k) = T(k^\\times)/(\\{a \\otimes (1-a) \\colon a \\in k \\setminus \\{0, 1\\}\\}),</math>\nwhere <math>T(k^\\times)</math> is the [[tensor algebra]] of the [[multiplicative group]] <math>k^\\times</math> and the quotient is by the [[two-sided ideal]] generated by all elements of the form <math>a \\otimes (1 - a)</math>.  Therefore the map <math>\\partial^n</math> factors through a map:\n:<math>\\partial^n \\colon K^M_n(k) \\to H^n_{\\rm\\acute et}(k, \\mu_\\ell^{\\otimes n}).</math>\nThis map is called the '''Galois symbol''' or '''norm residue''' map.<ref name=Sri146>Srinivas (1996) p.146</ref><ref name=GS108>Gille & Szamuely (2006) p.108</ref><ref name=Efr221>Efrat (2006) p.221</ref>  Because étale cohomology with mod-ℓ coefficients is an ℓ-torsion group, this map additionally factors through <math>K^M_n(k) / \\ell</math>.\n\nThe norm residue isomorphism theorem (or Bloch–Kato conjecture) states that for a field ''k'' and an integer ℓ that is invertible in ''k'', the norm residue map\n:<math>\\partial^n : K_n^M(k)/\\ell \\to H^n_{\\rm\\acute et}(k, \\mu_\\ell^{\\otimes n})</math>\nfrom [[Milnor K-theory]]  mod-ℓ to [[étale cohomology]] is an isomorphism.  The case {{nowrap|1=ℓ = 2}} is the [[Milnor conjecture]], and the case {{nowrap|1=''n'' = 2}} is the Merkurjev–Suslin theorem.<ref name=Efr221/><ref name=Sri>Srinivas (1996) pp.145-193</ref>\n\n==History==\n\nThe étale cohomology of a field is identical to [[Galois cohomology]], so the conjecture equates the ℓth cotorsion (the quotient by the subgroup of ℓ-divisible elements) of the Milnor ''K''-group of a [[field (mathematics)|field]] ''k'' with the [[Galois cohomology]] of ''k'' with coefficients in the Galois module of ℓth roots of unity.  The point of the conjecture is that there are properties that are easily seen for Milnor ''K''-groups but not for Galois cohomology, and vice versa; the norm residue isomorphism theorem makes it possible to apply techniques applicable to the object on one side of the isomorphism to the object on the other side of the isomorphism.\n\nThe case when ''n'' is 0 is trivial, and the case when {{nowrap|1=''n'' = 1}} follows easily from [[Hilbert's Theorem 90]].  The case {{nowrap|1=''n'' = 2}} and {{nowrap|1=ℓ = 2}} was proved by {{harv|Merkurjev|1981}}.  An important advance was the case {{nowrap|1=''n'' = 2}} and ℓ arbitrary.  This case was proved by {{harv|Merkurjev|Suslin|1982}} and is known as the '''Merkurjev–Suslin theorem'''.  Later, Merkurjev and Suslin, and independently, Rost, proved the case {{nowrap|1=''n'' = 3}} and {{nowrap|1=ℓ = 2}} {{harv|Merkurjev|Suslin|1991}} {{harv|Rost|1986}}.\n\nThe name \"norm residue\" originally referred to the [[Hilbert symbol]] <math>(a_1, a_2)</math>, which takes values in the [[Brauer group]] of ''k'' (when the field contains all ℓ-th roots of unity).  Its usage here is in analogy with standard [[local class field theory]] and is expected to be part of an (as yet undeveloped) \"higher\" class field theory.\n\nThe norm residue isomorphism theorem implies the [[Quillen–Lichtenbaum conjecture]].  It is equivalent to a theorem whose statement was once referred to as the [[Norm residue isomorphism theorem#Beilinson-Lichtenbaum conjecture|Beilinson–Lichtenbaum conjecture]].\n\n===History of the proof===\n\nMilnor's conjecture was proved by [[Vladimir Voevodsky]].<ref name=\"Voev1995\">{{cite web|url=http://www.math.uiuc.edu/K-theory/0076/|title=Voevodsky,Vladimir. \"Bloch-Kato conjecture for Z/2-coefficients and algebraic Morava K-theories\" (1995)|author=|date=|website=UIUC.edu|access-date=3 August 2017}}</ref><ref name=\"Voev1996\">{{cite web|url=http://www.math.uiuc.edu/K-theory/0170/|title=Voevodsky, Vladimir, \"The Milnor Conjecture\" (1996)|author=|date=|website=UIUC.edu|access-date=3 August 2017}}</ref><ref name=\"Voev2001\">{{cite web|url=http://www.math.uiuc.edu/K-theory/0502/|title=Voevodsky, Vladimir, \"On 2-torsion in motivic cohomology\" (2001)|author=|date=|website=UIUC.edu|access-date=3 August 2017}}</ref><ref name=\"Voev2003b\">Voevodsky, Vladimir, \"Motivic cohomology with Z/2-coefficients\", ''Publ. Math. Inst. Hautes Études Sci.'' No. 98 (2003), 59–104.</ref> \nLater Voevodsky proved the general Bloch–Kato conjecture.<ref name=\"Voev2008\">{{cite web|url=http://www.math.uiuc.edu/K-theory/0639/|title=Voevodsky, Vladimir, \"On motivic cohomology with Z/l-coefficients\" (2008)|author=|date=|website=UIUC.edu|access-date=3 August 2017}}</ref><ref name=\"Voev2010\">Voevodsky (2010)</ref>\n\nThe starting point for the proof is a series of conjectures due to {{harvtxt|Lichtenbaum|1983}} and {{harvtxt|Beilinson|1987}}. They conjectured the existence of ''motivic complexes'', complexes of sheaves whose cohomology was related to [[motivic cohomology]].  Among the conjectural properties of these complexes were three properties: one connecting their Zariski cohomology to Milnor's K-theory, one connecting their etale cohomology to cohomology with coefficients in the sheaves of roots of unity and one connecting their Zariski cohomology to their etale cohomology. These three properties implied, as a very special case, that the norm residue map should be an isomorphism. The essential characteristic of the proof is that it uses the induction on the \"weight\" (which equals the dimension of the cohomology group in the conjecture) where the inductive step requires knowing not only the statement of Bloch-Kato conjecture but the much more general statement that contains a large part of the Beilinson-Lichtenbaum conjectures. It often occurs in proofs by induction that the statement being proved has to be strengthened in order to prove the inductive step. In this case the strengthening that was needed required the development of a very large amount of new mathematics.\n\nThe earliest proof of Milnor's conjecture is contained in a 1995 preprint of Voevodsky<ref name=\"Voev1995\"/> and is inspired by the idea that there should be algebraic analogs of [[Morava K-theory|Morava ''K''-theory]] (these [[algebraic Morava K-theories]] were later constructed by [[Simone Borghesi]]<ref name=\"Borghesi2000\">Borghesi (2000)</ref>). In a 1996 preprint, Voevodsky was able to remove Morava ''K''-theory from the picture by introducing instead [[algebraic cobordism]]s  and using some of their properties that were not proved at that time (these properties were proved later). The constructions of 1995 and 1996 preprints are now known to be correct but the first completed proof of Milnor's conjecture used a somewhat different scheme.\n\nIt is also the scheme that the proof of the full Bloch–Kato conjecture follows. It was devised by Voevodsky a few months after the 1996 preprint appeared. Implementing this scheme required making substantial advances in the field of [[motivic homotopy theory]] as well as finding a way to build algebraic varieties with a specified list of properties.  From the motivic homotopy theory the proof required the following:\n#A construction of the motivic analog of the basic ingredient of the [[Spanier–Whitehead duality]] in the form of the motivic fundamental class as a morphism from the motivic sphere to the [[Thom space]] of the motivic normal bundle over a smooth projective algebraic variety.\n#A construction of the motivic analog of the [[Steenrod algebra]].\n#A proof of the proposition stating that over a field of characteristic zero the [[motivic Steenrod algebra]] characterizes all bi-stable cohomology operations in the motivic cohomology.\n\nThe first two constructions were developed by Voevodsky by 2003. Combined with the results that had been known since late 1980s, they were sufficient to reprove the [[Milnor conjecture]].\n\nAlso in 2003, Voevodsky published on the web a preprint that nearly contained a proof of the general theorem.  It followed the original scheme but was missing the proofs of three statements.  Two of these statements were related to the properties of the motivic Steenrod operations and required the third fact above, while the third one required then-unknown facts about \"norm varieties\". The properties that these varieties were required to have had been formulated by Voevodsky in 1997, and the varieties themselves had been constructed by Markus Rost in 1998–2003.  The proof that they have the required properties was completed by [[Andrei Suslin]] and [[Seva Joukhovitski]] in 2006.\n\nThe third fact above required the development of new techniques in motivic homotopy theory.  The goal was to prove that a functor, which was not assumed to commute with limits or colimits, preserved weak equivalences between objects of a certain form. One of the main difficulties there was that the standard approach to the study of weak equivalences is based on Bousfield–Quillen factorization systems and [[model category]] structures, and these were inadequate.  Other methods had to be developed, and this work was completed by Voevodsky only in 2008.{{citation needed|date=August 2017}}\n\nIn the course of developing these techniques, it became clear that the first statement used without proof in Voevodsky's 2003 preprint is false. The proof had to be modified slightly to accommodate the corrected form of that statement. While Voevodsky continued to work out the final details of the proofs of the main theorems about motivic [[Eilenberg–MacLane space]]s, [[Charles Weibel]] invented an approach to correct the place in the proof that had to modified. Weibel also published in 2009 a paper that contained a summary of the Voevodsky's constructions combined with the correction that he discovered.{{citation needed|date=August 2017}}\n\n==Beilinson–Lichtenbaum conjecture==\n{{see also|Quillen–Lichtenbaum conjecture}}\nLet ''X'' be a smooth variety over a field containing <math>1/\\ell</math>.  Beilinson and Lichtenbaum conjectured that the [[motivic cohomology]] group <math>H^{p,q}(X, \\mathbf{Z}/\\ell)</math> is isomorphic to the [[étale cohomology]] group <math>H^p_{\\rm\\acute et}(X, \\mu^{\\otimes q}_\\ell)</math> when ''p''&le;''q''.  This conjecture has now been proven, and is equivalent to the norm residue isomorphism theorem.\n\n==References==\n{{reflist|30em}}\n\n==Bibliography==\n* {{cite journal|last1=Bloch|first1=Spencer|last2=Kato|first2=Kazuya|title=p-adic etale cohomology|journal=Publications Mathématiques de l'IHÉS|date=1986|volume=63|pages=107–152|doi=10.1007/bf02831624}}\n* {{citation | last1=Borghesi | first1=Simone  | url=http://www.math.uiuc.edu/K-theory/0412 | title=Algebraic Morava K-theories and the higher degree formula, | year=2000 | series=Preprint}}\n* {{cite book | last=Efrat | first=Ido | title=Valuations, orderings, and Milnor ''K''-theory | series=Mathematical Surveys and Monographs | volume=124 | location=Providence, RI | publisher=[[American Mathematical Society]] | year=2006 | isbn=0-8218-4041-X | zbl=1103.12002 }}\n* {{cite book | last1=Gille | first1=Philippe | last2=Szamuely | first2=Tamás | title=Central simple algebras and Galois cohomology | series=Cambridge Studies in Advanced Mathematics | volume=101 | location=Cambridge | publisher=[[Cambridge University Press]] | year=2006 | isbn=0-521-86103-9 | zbl=1137.12001 }}\n* {{cite journal|last1=Milnor|first1=John|title=Algebraic K-theory and quadratic forms|journal=Inv. Math.|date=1970|volume=9|pages=318–344|doi=10.1007/bf01425486|bibcode=1970InMat...9..318M}}\n* {{Cite web | last1=Rost | first1=Markus | title=Chain lemma for splitting fields of symbols | url=http://www.math.uni-bielefeld.de/~rost/chain-lemma.html | year=1998}}\n* {{cite book | last=Srinivas | first=V. | title=Algebraic ''K''-theory | edition=Paperback reprint of the 1996 2nd | series=Modern Birkhäuser Classics | location=Boston, MA | publisher=[[Birkhäuser]] | year=2008 | isbn=978-0-8176-4736-0 | zbl=1125.19300 }}\n* {{citation | last1=Voevodsky | first1=Vladimir | author1-link=Vladimir Voevodsky  | url= http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.154.922| title=Bloch-Kato conjecture for Z/2-coefficients and algebraic Morava K-theories, | year=1995 | series=Preprint}}\n* {{citation | last1=Voevodsky | first1=Vladimir | author1-link=Vladimir Voevodsky  | url=http://www.math.uiuc.edu/K-theory/0170 | title=The Milnor Conjecture | year=1996 | series=Preprint}}\n* {{citation | last1=Voevodsky | first1=Vladimir | author1-link=Vladimir Voevodsky  | arxiv= math/0107110 | title=On 2-torsion in motivic cohomology | year=2001 | series=Preprint| bibcode=2001math......7110V}}\n* {{Citation | last1=Voevodsky | first1=Vladimir | author1-link=Vladimir Voevodsky | title=Reduced power operations in motivic cohomology | url=http://www.numdam.org/item?id=PMIHES_2003__98__1_0 | doi=10.1007/s10240-003-0009-z | mr=2031198 | year=2003a| journal=Institut des Hautes Études Scientifiques. Publications Mathématiques | issn=0073-8301 | issue=1 | pages=1–57 | volume=98| arxiv=math/0107109 }}\n* {{Citation | last1=Voevodsky | first1=Vladimir | author1-link=Vladimir Voevodsky | title=Motivic cohomology with Z/2-coefficients | url=https://www.researchgate.net/publication/225345330_Motivic_cohomology_with_Z2-coefficients| doi=10.1007/s10240-003-0010-6 | mr=2031199 | year=2003b | journal=Institut des Hautes Études Scientifiques. Publications Mathématiques | issn=0073-8301 | issue=1 | pages=59–104 | volume=98}}\n* {{Cite arXiv | last1=Voevodsky | first1=Vladimir | author1-link=Vladimir Voevodsky | title=On motivic cohomology with Z/l coefficients | eprint=0805.4430 | year=2008 | class=math.AG}}\n* {{Cite journal | last1=Weibel | first1=Charles | author1-link=Charles Weibel | title=The norm residue isomorphism theorem | doi=10.1112/jtopol/jtp013 | mr=2529300 | year=2009 | journal=[[Journal of Topology]] | volume=2 | issue=2 | pages=346–372}}\n* {{cite journal|last1=Voevodsky|first1=Vladimir|title=On motivic cohomology with Z/l-coefficients|journal=Annals of Mathematics|date=2011|volume=174|issue=1|pages=401–438|doi=10.4007/annals.2011.174.1.11|url=http://annals.math.princeton.edu/2011/174-1/p11|arxiv=0805.4430}}\n\n[[Category:Conjectures that have been proved]]\n[[Category:Algebraic K-theory]]\n[[Category:Theorems in algebraic topology]]\n[[Category:Theorems in algebra]]"
    },
    {
      "title": "Polynomial remainder theorem",
      "url": "https://en.wikipedia.org/wiki/Polynomial_remainder_theorem",
      "text": "{{short description|Value of the remainder of the division by a linear polynomial}}\n{{redirect|Little Bézout's theorem|the number of intersection points of two algebraic curves|Bézout's theorem|the relation between two numbers and their greatest common divisor|Bézout's identity}}\n\nIn [[algebra]], the '''polynomial remainder theorem''' or '''little Bézout's theorem''' (named after [[Étienne Bézout]])<ref>{{cite journal |author=Piotr Rudnicki |title=Little Bézout Theorem (Factor Theorem) |journal=Formalized Mathematics |volume=12 |issue=1 |year=2004 |pages=49–58 |url=http://mizar.org/fm/2004-12/pdf12-1/uproots.pdf}}</ref> is an application of [[Euclidean division of polynomials]]. It states that the remainder of the division of a [[polynomial]] <math>f(x)</math> by a [[linear polynomial]] <math>x-r</math> is equal to <math>f(r) .</math> In particular, <math>x-r</math> is a [[divisor]] of <math>f(x)</math> [[if and only if]] <math>f(r)=0,</math><ref>Larson, Ron (2014), College Algebra, Cengage Learning</ref> a property known as the [[factor theorem]].\n\n== Examples ==\n=== Example 1 ===\nLet <math>f(x) = x^3 - 12x^2 - 42</math>. Polynomial division of <math>f(x)</math> by <math>(x-3)</math> gives the quotient <math>x^2 - 9x - 27</math> and the remainder <math>-123</math>. Therefore, <math>f(3)=-123</math>.\n\n===Example 2===\nShow that the polynomial remainder theorem holds for an arbitrary second degree polynomial <math>f(x) = ax^2 + bx + c</math> by using algebraic manipulation:\n:<math>\n\\begin{align}\n\\frac{f(x)}{{x - r}} &= \\frac{{a{x^2} + bx + c}}{{x - r}} \\\\\n &= \\frac{{a{x^2} - arx + arx + bx + c}}{{x - r}} \\\\\n &= \\frac{{ax(x - r) + (b + ar)x + c}}{{x - r}} \\\\\n &= ax + \\frac{{(b + ar)(x - r) + c + r(b + ar)}}{{x - r}} \\\\\n &= ax + b + ar + \\frac{{c + r(b + ar)}}{{x - r}} \\\\\n &= ax + b + ar + \\frac{{a{r^2} + br + c}}{{x - r}}\n\\end{align}</math>\n\nMultiplying both sides by (''x''&nbsp;&minus;&nbsp;''r'') gives\n:<math>f(x) = ax^2 + bx + c = (ax + b + ar)(x - r) + {a{r^2} + br + c}</math>.\n\nSince <math>R = ar^2 + br + c</math> is the remainder, we have indeed shown that <math>f(r) = R</math>.\n\n== Proof ==\n\nThe polynomial remainder theorem follows from the theorem of [[Euclidean division of polynomials|Euclidean division]], which, given two polynomials {{math|''f''(''x'')}} (the dividend) and {{math|''g''(''x'')}} (the divisor), asserts the existence (and the uniqueness) of a quotient {{math|''Q''(''x'')}} and a remainder {{math|''R''(''x'')}} such that\n:<math>f(x)=Q(x)g(x) + R(x)\\quad \\text{and}\\quad R(x) = 0 \\ \\text{ or } \\deg(R)<\\deg(g).</math>\n\nIf the divisor is <math>g(x) = x-r,</math> then either {{math|1=''R''(''x'') = 0}} or its degree is zero; in both cases,  {{math|''R''(''x'')}} is a constant that is independent of {{math|''x''}}; that is \n:<math>f(x)=q(x)(x-r) + R.</math>\n\nSetting <math>x=r</math> in this formula, we obtain:\n:<math>f(r)=R.</math>\n\nA slightly different proof, which may appear to some people as more elementary, starts with an observation that <math>f(x)-f(r)</math> is a [[linear combination]] of terms of the form  <math>x^k-r^k,</math> each of which is divisible by <math>x-r</math> since <math>x^k-r^k=(x-r)(x^{k-1}+x^{k-2}r+\\dots+xr^{k-2}+r^{k-1}).</math>\n\n== Applications ==\n\nThe polynomial remainder theorem may be used to evaluate <math>f(r)</math> by calculating the remainder, <math>R</math>. Although [[polynomial long division]] is more difficult than evaluating the [[Function (mathematics)|function]] itself, [[synthetic division]] is computationally easier.  Thus, the function may be more \"cheaply\" evaluated using synthetic division and the polynomial remainder theorem.\n\nThe [[factor theorem]] is another application of the remainder theorem: if the remainder is zero, then the linear divisor is a factor. Repeated application of the factor theorem may be used to factorize the polynomial.<ref>Larson, Ron (2011), Precalculus with Limits, Cengage Learning</ref>\n\n== References ==\n{{reflist}}\n\n[[Category:Polynomials]]\n[[Category:Theorems in algebra]]"
    },
    {
      "title": "Posner's theorem",
      "url": "https://en.wikipedia.org/wiki/Posner%27s_theorem",
      "text": "In algebra, '''Posner's theorem''' states that given a [[prime ring|prime]] [[polynomial identity algebra]] ''A'' with center ''Z'', the ring <math>A \\otimes_Z Z_{(0)}</math> is a [[central simple algebra]] over <math>Z_{(0)}</math>, the [[field of fractions]] of ''Z''.<ref>{{harvnb|Artin|1999|loc=Theorem V. 8.1.}}</ref> It is named after [[Ed Posner]].\n\n== References ==\n{{reflist}}\n*{{cite web|last=Artin|first=Michael|title=Noncommutative Rings|url=http://math.mit.edu/~etingof/artinnotes.pdf|year=1999|location=Chapter V|ref=harv}}\n* {{cite book | last=Formanek | first=Edward |authorlink= Edward W. Formanek | title=The polynomial identities and invariants of ''n''×''n'' matrices | zbl=0714.16001 | series=Regional Conference Series in Mathematics | volume=78 | location=Providence, RI | publisher=[[American Mathematical Society]] | year=1991 | isbn=0-8218-0730-7 }}\n* [[Ed Posner|Edward C. Posner]], Prime rings satisfying a polynomial identity, Proc. Amer. Math. Soc. 11 (1960), pp. 180–183.\n\n{{algebra-stub}}\n\n[[Category:Ring theory]]\n[[Category:Theorems in algebra]]"
    },
    {
      "title": "Rational root theorem",
      "url": "https://en.wikipedia.org/wiki/Rational_root_theorem",
      "text": "In [[algebra]], the '''rational root theorem''' (or '''rational root test''', '''rational zero theorem''', '''rational zero test''' or '''''p''/''q'' theorem''') states a constraint on [[rational number|rational]] [[Equation solving|solutions]] of a [[polynomial equation]]\n\n:<math>a_nx^n+a_{n-1}x^{n-1}+\\cdots+a_0 = 0</math>\n\nwith [[integer]] coefficients <math>a_i\\in\\mathbb{Z}</math> and <math>a_0,a_n \\neq 0</math>. Solutions of the equation are also called [[root of a polynomial|roots]] or zeroes of the [[polynomial]] on the left side.\n\nThe theorem states that each [[rational number|rational]] solution ''x''&nbsp;=&nbsp;''p''/''q,'' written in lowest terms so that ''p'' and ''q'' are [[relatively prime]], satisfies:\n\n* ''p'' is an integer [[divisor|factor]] of the [[constant term]] ''a''<sub>0</sub>, and\n\n* ''q'' is an integer factor of the leading [[coefficient]] ''a<sub>n</sub>''.\n\nThe rational root theorem is a special case (for a single linear factor) of [[Gauss's lemma (polynomial)|Gauss's lemma]] on the factorization of polynomials. The '''integral root theorem''' is the special case of the rational root theorem when the leading coefficient is&nbsp;''a<sub>n</sub>''&nbsp;=&nbsp;1.\n\n==Application==\n\nThe theorem is used to find all rational roots of a polynomial, if any. It gives a finite number of possible fractions which can be checked to see if they are roots. If a rational root ''x'' = ''r'' is found, a linear polynomial (''x''-''r)'' can be factored out of the polynomial using [[polynomial long division]], resulting in a polynomial of lower degree whose roots are also roots of the original polynomial.\n\n===Cubic equation===\n\nThe general [[cubic equation]]\n\n:<math>ax^3+bx^2+cx+d=0</math>\n\nwith integer coefficients has three solutions in the [[complex plane]]. If the rational root test finds no rational solutions, then the only way to express the solutions [[algebraic expression|algebraically]] uses [[Cubic function|cube roots]]. But if the test finds a rational solution ''r'', then factoring out (''x'' – ''r'') leaves a [[quadratic polynomial]] whose two roots, found with the [[quadratic formula]], are the remaining two roots of the cubic, avoiding cube roots.\n\n==Proofs==\n\n===First proof===\n\nLet \n\n:<math>P(x) \\ =\\ a_n x^n + a_{n-1} x^{n-1} + \\cdots + a_1 x + a_0, \\qquad a_0, \\ldots a_n \\in \\mathbf{Z}.</math>\n\nSuppose ''P''(''p''/''q'') = 0 for some [[coprime]] ''p'', ''q'' ∈ '''Z''':\n\n:<math>P \\left(\\tfrac{p}{q} \\right) = a_n\\left(\\tfrac{p}{q}\\right)^n + a_{n-1}\\left(\\tfrac{p}{q}\\right)^{n-1} + \\cdots + a_1 \\left(\\tfrac{p}{q}\\right) + a_0 = 0.</math>\n\nIf we multiply both sides by ''q<sup>n</sup>'', shift the constant term to the right side, and factor out ''p'' on the left side, we get\n\n:<math>p \\left (a_np^{n-1} + a_{n-1}qp^{n-2} + \\cdots + a_1q^{n-1} \\right ) = -a_0q^n.</math>\n\nWe see that ''p'' divides ''a''<sub>0</sub>''q<sup>n</sup>''. But ''p'' is coprime to ''q'' and therefore to ''q<sup>n</sup>'', so by [[Euclid's lemma]] ''p'' must divide the remaining factor ''a''<sub>0</sub> of the product.\n\nIf we instead shift the leading term to the right side and factor out ''q'' on the left side, we get\n\n:<math>q \\left (a_{n-1}p^{n-1} + a_{n-2}qp^{n-2} + \\cdots + a_0q^{n-1} \\right ) = -a_np^n.</math>\n\nReasoning as before, we conclude that ''q'' divides ''a<sub>n</sub>''.<ref>{{cite book |first=D. |last=Arnold |first2=G. |last2=Arnold |title=Four unit mathematics |publisher=Edward Arnold |year=1993 |isbn=0-340-54335-3 |pages=120–121 }}</ref>\n\n===Proof using Gauss' lemma===\n\nShould there be a nontrivial factor dividing all the coefficients of the polynomial, then one can divide by the [[greatest common divisor]] of the coefficients so as to obtain a primitive polynomial in the sense of [[Gauss's lemma (polynomial)|Gauss's lemma]]; this does not alter the set of rational roots and only strengthens the divisibility conditions. That lemma says that if the polynomial factors in {{math|'''Q'''[''X'']}}, then it also factors in {{math|'''Z'''[''X'']}} as a product of primitive polynomials. Now any rational root {{math|''p''/''q''}} corresponds to a factor of degree 1 in {{math|'''Q'''[''X'']}} of the polynomial, and its primitive representative is then {{math|''qx'' − ''p''}}, assuming that ''p'' and ''q'' are coprime. But any multiple in {{math|'''Z'''[''X'']}} of {{math|''qx'' − ''p''}} has leading term divisible by ''q'' and constant term divisible by ''p'', which proves the statement. This argument shows that more generally, any irreducible factor of ''P'' can be supposed to have integer coefficients, and leading and constant coefficients dividing the corresponding coefficients of&nbsp;''P''.\n\n==Examples==\n\n===First===\n\nIn the polynomial\n\n:<math>2x^3+x-1,</math>\n\nany rational root fully reduced would have to have a numerator that divides evenly into 1 and a denominator that divides evenly into 2. Hence the only possible rational roots are ±1/2 and ±1; since neither of these equates the polynomial to zero, it has no rational roots.\n\n===Second===\nIn the polynomial\n\n:<math>x^3-7x+6</math>\n\nthe only possible rational roots would have a numerator that divides 6 and a denominator that divides 1, limiting the possibilities to ±1, ±2, ±3, and ±6. Of these, 1, 2, and –3 equate the polynomial to zero, and hence are its rational roots. (In fact these are its only roots since a cubic has only three roots; in general, a polynomial could have some rational and some [[irrational number|irrational]] roots.)\n\n===Third===\n\nEvery rational root of the polynomial\n:<math>3x^3 - 5x^2 + 5x - 2 </math>\nmust be among the numbers symbolically indicated by:\n: <math>\\pm\\tfrac{1,2}{1,3} \\ =\\  \\pm \\{1, 2, \\tfrac{1}{3}, \\tfrac{2}{3}\\} .</math>\nThese 8 root candidates ''x'' = ''r'' can be tested by evaluating ''P''(''r''), for example using [[Horner's method]]. It turns out there is exactly one with ''P''(''r'') = 0. \n\nThis process may be made more efficient: if ''P''(''r'') &ne; 0, it can be used to shorten the list of remaining candidates.<ref>{{cite journal |last=King |first=Jeremy D. |title=Integer roots of polynomials |journal=Mathematical Gazette |volume=90 |date= November 2006 |pages=455–456 }}</ref> For example, ''x''&nbsp;=&nbsp;1 does not work, as ''P''(1) = 1. Substituting ''x''&nbsp;=&nbsp;1&nbsp;+&nbsp;''t'' yields a polynomial in&nbsp;''t'' with constant term ''P''(1) = 1, while the coefficient of ''t''<sup>3</sup> remains the same as the coefficient of ''x''<sup>3</sup>. Applying the rational root theorem thus yields the  possible roots <math>t=\\pm\\tfrac{1}{1,3}</math>, so that \n\n:<math>x = 1+t = 2, 0, \\tfrac{4}{3}, \\tfrac{2}{3}.</math>\n\nTrue roots must occur on both lists, so list of rational root candidates has shrunk to just ''x'' = 2 and ''x'' = 2/3.\n\nIf ''k'' &ge; 1 rational roots are found, Horner's method will also yield a polynomial of degree ''n''&nbsp;−&nbsp;''k'' whose roots, together with the rational roots, are exactly the roots of the original polynomial. If none of the candidates is a solution, there can be no rational solution.\n\n==See also==\n*[[Integrally closed domain]]\n*[[Descartes' rule of signs]]\n*[[Gauss–Lucas theorem]]\n*[[Properties of polynomial roots]]\n*[[Content (algebra)]]\n*[[Eisenstein's criterion]]\n\n==Notes==\n{{Reflist}}\n\n==References==\n*Charles D. Miller, Margaret L. Lial, David I. Schneider: ''Fundamentals of College Algebra''. Scott & Foresman/Little & Brown Higher Education, 3rd edition 1990, {{isbn|0-673-38638-4}}, pp.&nbsp;216–221\n*Phillip S. Jones, Jack D. Bedient: ''The historical roots of elementary mathematics''. Dover Courier Publications 1998, {{isbn|0-486-25563-8}}, pp.&nbsp;116–117  ({{Google books|7xArILpcndYC|online copy|page=116}})\n*Ron Larson: ''Calculus: An Applied Approach''. Cengage Learning 2007, {{isbn|978-0-618-95825-2}}, pp.&nbsp;23–24  ({{Google books|bDG7V0OV34C|online copy|page=23}})\n\n==External links==\n*{{MathWorld|urlname=RationalZeroTheorem|title=Rational Zero Theorem}}\n*[http://planetmath.org/encyclopedia/RationalRootTheorem.html ''RationalRootTheorem''] at [[PlanetMath]]\n* [http://www.cut-the-knot.org/Generalization/RationalRootTheorem.shtml Another proof that n<sup>th</sup> roots of integers are irrational, except for perfect nth powers] by Scott E. Brodie\n*[http://www.purplemath.com/modules/rtnlroot.htm ''The Rational Roots Test''] at purplemath.com\n\n[[Category:Polynomials]]\n[[Category:Theorems in algebra]]\n[[Category:Root-finding algorithms]]"
    },
    {
      "title": "Segal's conjecture",
      "url": "https://en.wikipedia.org/wiki/Segal%27s_conjecture",
      "text": "'''Segal's Burnside ring conjecture''', or, more briefly, the '''Segal conjecture''', is a [[theorem]] in [[homotopy theory]], a branch of [[mathematics]]. The theorem relates the [[Burnside ring]] of a finite [[Group (mathematics)|group]] ''G'' to the [[cohomotopy group|stable cohomotopy]] of the [[classifying space]] ''BG''. The conjecture was made in the mid 1970s by [[Graeme Segal]] and proved in 1984 by [[Gunnar Carlsson]]. {{As of|2016}}, this statement is still commonly referred to as the Segal conjecture, even though it now has the status of a theorem.\n\n==Statement of the theorem==\nThe Segal conjecture has several different formulations, not all of which are equivalent. Here is a weak form: there exists, for every finite group ''G'', an isomorphism\n\n:<math>\\varprojlim \\pi_S^0 \\left( BG^{(k)}_+ \\right) \\to \\widehat{A}(G).</math>\n\nHere, lim denotes the [[inverse limit]], {{pi}}<sub>S</sub>* denotes the stable cohomotopy ring, ''B'' denotes the classifying space, the superscript ''k'' denotes the ''k''-[[CW-complex|skeleton]], and the subscript + denotes the addition of a disjoint basepoint. On the right-hand side, the hat denotes the [[topological ring#Completion|completion]] of the Burnside ring with respect to its [[augmentation ideal]].\n\n==The Burnside ring==\n{{Main|Burnside ring}}\n\nThe Burnside ring of a finite group ''G'' is constructed from the category of finite [[Group action (mathematics)|''G''-sets]] as a [[Grothendieck group]]. More precisely, let ''M''(''G'') be the commutative [[monoid]] of isomorphism classes of finite ''G''-sets, with addition the disjoint union of ''G''-sets and identity element the empty set (which is a ''G''-set in a unique way). Then ''A''(''G''), the Grothendieck group of ''M''(''G''), is an abelian group. It is in fact a [[free abelian group|free]] abelian group with basis elements represented by the ''G''-sets ''G''/''H'', where ''H'' varies over the subgroups of ''G''. (Note that ''H'' is not assumed here to be a normal subgroup of ''G'', for while ''G''/''H'' is not a group in this case, it is still a ''G''-set.) The [[ring (mathematics)|ring]] structure on ''A''(''G'') is induced by the direct product of ''G''-sets; the multiplicative identity is the (isomorphism class of any) one-point set, which becomes a ''G''-set in a unique way.\n\nThe Burnside ring is the analogue of the [[representation ring]] in the category of finite sets, as opposed to the category of finite-dimensional [[vector space]]s over a [[Field (mathematics)|field]] (see [[#Motivation and interpretation|motivation]] below). It has proven to be an important tool in the [[group representation|representation theory]] of finite groups.\n\n==The classifying space==\n{{Main|Classifying space}}\nFor any [[topological group]] ''G'' admitting the structure of a [[CW-complex]], one may consider the category of [[principal bundle|principal ''G''-bundles]]. One can define a [[functor]] from the category of CW-complexes to the category of sets by assigning to each CW-complex ''X'' the set of principal ''G''-bundles on ''X''. This functor descends to a functor on the homotopy category of CW-complexes, and it is natural to ask whether the functor so obtained is [[representable functor|representable]]. The answer is affirmative, and the representing object is called the classifying space of the group ''G'' and typically denoted ''BG''. If we restrict our attention to the homotopy category of CW-complexes, then ''BG'' is unique. Any CW-complex that is homotopy equivalent to ''BG'' is called a ''model'' for ''BG''.\n\nFor example, if ''G'' is the group of order 2, then a model for ''BG'' is infinite-dimensional real projective space. It can be shown that if ''G'' is finite, then any CW-complex modelling ''BG'' has cells of arbitrarily large dimension. On the other hand, if ''G'' = '''Z''', the integers, then the classifying space ''BG'' is homotopy equivalent to the circle ''S''<sup>1</sup>.\n\n==Motivation and interpretation==\nThe content of the theorem becomes somewhat clearer if it is placed in its historical context. In the theory of representations of finite groups, one can form an object <math>R[G]</math> called the representation ring of <math>G</math> in a way entirely analogous to the construction of the Burnside ring outlined above. The stable [[cohomotopy group|cohomotopy]] is in a sense the natural analog to complex [[K-theory]], which is denoted <math>KU^*</math>. Segal was inspired to make his conjecture after [[Michael Atiyah]] proved the existence of an isomorphism\n:<math>KU^0(BG) \\to \\widehat{R}[G]</math>\nwhich is a special case of the [[Atiyah–Segal completion theorem]].\n\n==References==\n*{{cite conference\n| first= J. Frank\n| last= Adams\n| authorlink=Frank Adams\n| title= Graeme Segal's Burnside ring conjecture \n| booktitle= Topology Symposium, Siegen 1979\n| year= 1980\n| publisher=Springer\n| series=Lecture Notes in Mathematics\n| volume=788\n| pages = 378–395\n| location=Berlin\n| mr = 585670}}\n*{{cite journal\n| first=Gunnar\n| last=Carlsson\n| authorlink=Gunnar Carlsson \n| title=Equivariant stable homotopy and Segal's Burnside ring conjecture\n| journal=[[Annals of Mathematics]]\n| year=1984\n| volume=120\n| issue=2\n| pages=189–224\n| doi=10.2307/2006940\n| jstor=2006940\n| mr=0763905}}\n\n[[Category:Representation theory of finite groups]]\n[[Category:Homotopy theory]]\n[[Category:Conjectures that have been proved]]\n[[Category:Theorems in algebra]]"
    },
    {
      "title": "Sinkhorn's theorem",
      "url": "https://en.wikipedia.org/wiki/Sinkhorn%27s_theorem",
      "text": "'''Sinkhorn's theorem''' states that every square matrix with positive entries can be written in a certain standard form.\n\n==Theorem==\nIf ''A'' is an [[Matrix (mathematics)#Square matrices|''n'' &times; ''n'' matrix]] with strictly positive elements, then there exist [[Diagonal matrix|diagonal matrices]] ''D''<sub>1</sub> and ''D''<sub>2</sub> with strictly positive diagonal elements such that ''D''<sub>1</sub>''AD''<sub>2</sub> is [[Doubly stochastic matrix|doubly stochastic]]. The matrices ''D''<sub>1</sub> and ''D''<sub>2</sub> are unique modulo multiplying the first matrix by a positive number and dividing the second one by the same number. \n<ref name=\"Sinkhorn\">Sinkhorn, Richard. (1964). \"A relationship between arbitrary positive matrices and doubly stochastic matrices.\" ''Ann. Math. Statist.'' '''35''', 876–879. {{doi|10.1214/aoms/1177703591}}</ref>\n<ref name=\"Marshall\">Marshall, A.W., & Olkin, I. (1967). \"Scaling of matrices to achieve specified row and column sums.\" ''Numerische Mathematik''. '''12(1)''', 83–90. {{doi|10.1007/BF02170999}}</ref>\n\n==Sinkhorn-Knopp algorithm==\nA simple iterative method to approach the double stochastic matrix is to alternately rescale all rows and all columns of ''A'' to sum to 1. Sinkhorn and Knopp presented this algorithm and analyzed its convergence.\n<ref name=\"SinkhornKnopp\">Sinkhorn, Richard, & Knopp, Paul. (1967). \"Concerning nonnegative matrices and doubly stochastic matrices\". ''Pacific J. Math.'' '''21''', 343–348.</ref>\n\n==Analogues and extensions==\nThe following analogue for unitary matrices is also true: for every [[unitary matrix]] ''U'' there exist two diagonal unitary matrices ''L'' and ''R'' such that ''LUR'' has each of its columns and rows summing to 1.<ref name=\"IdelWolf\">{{cite journal|last1=Idel|first1=Martin|last2=Wolf|first2=Michael M.|title=Sinkhorn normal form for unitary matrices|journal=Linear Algebra and its Applications|date=2015|volume=471|pages=76–84|doi=10.1016/j.laa.2014.12.031|arxiv=1408.5728}}</ref>\n\nThe following extension to maps between matrices is also true (see Theorem 5<ref name=\"GeoPav\">{{cite journal|last1=Georgiou|first1=Tryphon|last2=Pavon|first2=Michele|title=Positive contraction mappings for classical and quantum Schrödinger systems|journal=Journal of Mathematical Physics|date=2015|volume=56|pages=033301-1-24|doi=10.1063/1.4915289|arxiv=1405.6650|bibcode=2015JMP....56c3301G}}</ref> and also Theorem 4.7<ref name=\"Gur\">{{cite journal|last1=Gurvits|first1=Leonid|title=Classical complexity and quantum entanglement|journal=Journal of Computational Science|date=2004|volume=69|pages=448-484|doi=10.1016/j.jcss.2004.06.003}}</ref>): given a [[Quantum_operation|Kraus operator]]\nwhich represents the quantum operation Φ mapping a [[density matrix]] into another,\n:<math> S \\to \\Phi(S) = \\sum_i B_i S B_i^*, </math>\nthat is trace preserving,\n:<math> \\sum_i B_i^* B_i = I, </math>\nand, in addition, whose range is in the interior of the positive definite cone (strict positivity), there exist scalings ''x''<sub>''j''</sub>, for ''j'' in {0,1}, that are positive definite so that the rescaled [[Quantum_operation|Kraus operator]]\n:<math> S \\to x_1\\Phi(x_0^{-1}Sx_0^{-1})x_1 = \\sum_i (x_1B_ix_0^{-1}) S (x_1B_ix_0^{-1})^* </math>\nis doubly stochastic. In other words, it is such that both,\n:<math> x_1\\Phi(x_0^{-1}I x_0^{-1})x_1 = I, </math>\nas well as for the adjoint,\n:<math> x_0^{-1}\\Phi^*(x_1I x_1)x_0^{-1} = I, </math>\nwhere I denotes the identity operator.\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Sinkhorn's Theorem}}\n[[Category:Matrix theory]]\n[[Category:Theorems in algebra]]"
    },
    {
      "title": "Skolem–Noether theorem",
      "url": "https://en.wikipedia.org/wiki/Skolem%E2%80%93Noether_theorem",
      "text": "In [[ring theory]], a branch of mathematics, the '''Skolem–Noether theorem''' characterizes the [[automorphism]]s of [[simple ring]]s. It is a fundamental result in the theory of [[central simple algebra]]s.\n\nThe theorem was first published by [[Thoralf Skolem]] in 1927 in his paper ''Zur Theorie der assoziativen Zahlensysteme'' ([[German language|German]]: ''On the theory of associative number systems'') and later rediscovered by [[Emmy Noether]].\n\n== Statement ==\nIn a general formulation, let ''A'' and ''B'' be simple unitary rings, and let ''k'' be the centre of ''B''. Notice that ''k'' is a [[field (mathematics)|field]] since given ''x'' nonzero in ''k'', the simplicity of ''B'' implies that the nonzero two-sided ideal ''BxB = (x)'' is the whole of ''B'', and hence that ''x'' is a [[Unit (ring theory)|unit]]. If the [[dimension (vector space)|dimension]] of ''B'' over ''k'' is finite, i.e. if ''B'' is a [[central simple algebra]] of finite dimension, and ''A'' is also a ''k''-algebra, then given ''k''-algebra homomorphisms\n\n:''f'', ''g'' : ''A'' → ''B'',\n\nthere exists a unit ''b'' in ''B'' such that for all ''a'' in ''A''<ref>Lorenz (2008) p.173</ref><ref>{{cite book|last=Farb|first=Benson|title=Noncommutative Algebra|year=1993|publisher=Springer|isbn=9780387940571|author2=Dennis, R. Keith }}</ref>\n\n:''g''(''a'') = ''b'' · ''f''(''a'') · ''b''<sup>&minus;1</sup>.\n\nIn particular, every [[automorphism]] of a central simple ''k''-algebra is an [[inner automorphism]].<ref name=GS40>Gille & Szamuely (2006) p.40</ref><ref name=Lor174>Lorenz (2008) p.174</ref>\n\n== Proof ==\nFirst suppose <math>B = \\operatorname{M}_n(k) = \\operatorname{End}_k(k^n)</math>. Then ''f'' and ''g'' define the actions of ''A'' on <math>k^n</math>; let <math>V_f, V_g</math> denote the ''A''-modules thus obtained. Any two simple ''A''-modules are isomorphic and <math>V_f, V_g</math> are finite direct sums of simple ''A''-modules. Since they have the same dimension, it follows that there is an isomorphism of ''A''-modules <math>b: V_g \\to V_f</math>. But such ''b'' must be an element of <math>\\operatorname{M}_n(k) = B</math>. For the general case, note that <math>B \\otimes_k B^{\\text{op}}</math> is a matrix algebra and that <math>A \\otimes_k B^{\\text{op}}</math> is simple. By the first part applied to the maps <math>f \\otimes 1, g \\otimes1 : A \\otimes_k B^{\\text{op}} \\to B \\otimes_k B^{\\text{op}}</math>, there exists <math>b \\in B \\otimes_k B^{\\text{op}}</math> such that\n:<math>(f \\otimes 1)(a \\otimes z) = b (g \\otimes 1)(a \\otimes z) b^{-1}</math>\nfor all <math>a \\in A</math> and <math>z \\in B^{\\text{op}}</math>. Taking <math>a = 1</math>, we find\n:<math>1 \\otimes z = b (1\\otimes z) b^{-1}</math>\nfor all ''z''. That is to say, ''b'' is in <math>Z_{B \\otimes B^{\\text{op}}}(k \\otimes B^{\\text{op}}) = B \\otimes k</math> and so we can write <math>b = b' \\otimes 1</math>. Taking <math>z = 1</math> this time we find\n:<math>f(a)= b' g(a) {b'^{-1}}</math>,\nwhich is what was sought.\n\n== Notes ==\n{{reflist}}\n\n== References ==\n*{{cite journal | jfm=54.0154.02| journal=Skrifter Oslo | year=1927 | number=12 | pages=50 | language=German | first=Thoralf | last= Skolem | authorlink=Thoralf Skolem | title=Zur Theorie der assoziativen Zahlensysteme }}\n*A discussion in Chapter IV of [[James Milne (mathematician)|Milne]], class field theory [http://jmilne.org/math/CourseNotes/cft.html]\n* {{cite book | last1=Gille | first1=Philippe | last2=Szamuely | first2=Tamás | title=Central simple algebras and Galois cohomology | series=Cambridge Studies in Advanced Mathematics | volume=101 | location=Cambridge | publisher=[[Cambridge University Press]] | year=2006 | isbn=0-521-86103-9 | zbl=1137.12001 }}\n* {{cite book | first=Falko | last=Lorenz | title=Algebra. Volume II: Fields with Structure, Algebras and Advanced Topics | year=2008 | publisher=Springer | isbn=978-0-387-72487-4 | zbl=1130.12001 }}\n\n{{DEFAULTSORT:Skolem-Noether theorem}}\n[[Category:Ring theory]]\n[[Category:Theorems in algebra]]"
    },
    {
      "title": "Specht's theorem",
      "url": "https://en.wikipedia.org/wiki/Specht%27s_theorem",
      "text": "In mathematics, '''Specht's theorem''' gives a [[necessary and sufficient condition]] for two [[matrix (mathematics)|matrices]] to be [[Matrix_similarity|unitarily equivalent]]. It is named after [[Wilhelm Specht]], who proved the theorem in 1940.<ref>{{harvtxt|Specht|1940}}</ref>\n\nTwo matrices ''A'' and ''B'' are said to be ''unitarily equivalent'' if there exists a [[unitary matrix]] ''U'' such that ''B'' = ''U''&thinsp;*''AU''.<ref>{{harvtxt|Horn|Johnson|1985}}, Definition 2.2.1</ref> Two matrices which are unitarily equivalent are also [[similar matrices|similar]]. Two similar matrices represent the same [[linear map]], but with respect to a different [[basis of a vector space|basis]]; unitary equivalence corresponds to a change from an [[orthonormal basis]] to another orthonormal basis. \n\nIf ''A'' and ''B'' are unitarily equivalent, then tr ''AA''* = tr ''BB''*, where tr denotes the [[trace (linear algebra)|trace]] (in other words, the [[Frobenius norm]] is a unitary invariant). This follows from the cyclic invariance of the trace: if ''B'' = ''U''&thinsp;*''AU'', then tr ''BB''* = tr ''U''&thinsp;*''AUU''&thinsp;*''A''*''U'' = tr ''AUU''&thinsp;*''A''*''UU''&thinsp;* = tr ''AA''*, where the second equality is cyclic invariance.<ref>{{harvtxt|Horn|Johnson|1985}}, Theorem 2.2.2</ref> \n\nThus, tr ''AA''* = tr ''BB''* is a necessary condition for unitary equivalence, but it is not sufficient. Specht's theorem gives infinitely many necessary conditions which together are also sufficient. The formulation of the theorem uses the following definition. A [[word (mathematics)|word]] in two variables, say ''x'' and ''y'', is an expression of the form\n\n:<math>\nW(x,y) = x^{m_1} y^{n_1} x^{m_2} y^{n_2} \\cdots x^{m_p},\n</math>\n\nwhere ''m''<sub>1</sub>, ''n''<sub>1</sub>, ''m''<sub>2</sub>, ''n''<sub>2</sub>, …, ''m''<sub>''p''</sub> are non-negative integers. The ''degree'' of this word is\n\n:<math> \nm_1 + n_1 + m_2 + n_2 + \\cdots + m_p.\n</math>\n\n'''Specht's theorem:''' Two matrices ''A'' and ''B'' are unitarily equivalent if and only if tr ''W''(''A'', ''A''*) = tr ''W''(''B'', ''B''*) for all words ''W''.<ref>{{harvtxt|Horn|Johnson|1985}}, Theorem 2.2.6</ref> \n\nThe theorem gives an infinite number of trace identities, but it can be reduced to a finite subset. Let ''n'' denote the size of the matrices ''A'' and ''B''. For the case ''n'' = 2, the following three conditions are sufficient:<ref>{{harvtxt|Horn|Johnson|1985}}, Theorem 2.2.8</ref> \n\n:<math> \n\\operatorname{tr} \\, A = \\operatorname{tr} \\, B, \\quad \n\\operatorname{tr} \\, A^2 = \\operatorname{tr} \\, B^2, \\quad\\text{and}\\quad\n\\operatorname{tr} \\, AA^* = \\operatorname{tr} \\, BB^*.\n</math>\n\nFor ''n'' = 3, the following seven conditions are sufficient:\n\n:<math> \n\\begin{align}\n&\\operatorname{tr} \\, A = \\operatorname{tr} \\, B, \\quad \n\\operatorname{tr} \\, A^2 = \\operatorname{tr} \\, B^2, \\quad\n\\operatorname{tr} \\, AA^* = \\operatorname{tr} \\, BB^*, \\quad\n\\operatorname{tr} \\, A^3 = \\operatorname{tr} \\, B^3, \\\\\n&\\operatorname{tr} \\, A^2 A^* = \\operatorname{tr} \\, B^2 B^*, \\quad\n\\operatorname{tr} \\, A^2 (A^*)^2 = \\operatorname{tr} \\, B^2 (B^*)^2, \\quad\\text{and}\\quad\n\\operatorname{tr} \\, A^2 (A^*)^2 A A^* = \\operatorname{tr} \\, B^2 (B^*)^2 B B^*.\n\\end{align}\n</math> &nbsp;<ref>{{harvtxt|Sibirskiǐ|1976}}, p. 260, quoted by {{harvtxt|Đoković|Johnson|2007}}</ref> \nFor general ''n'', it suffices to show that tr ''W''(''A'', ''A''*) = tr ''W''(''B'', ''B''*) for all words of degree at most \n\n:<math>\nn \\sqrt{\\frac{2n^2}{n-1} + \\frac14} + \\frac{n}2 - 2.\n</math> &nbsp;<ref>{{harvtxt|Pappacena|1997}}, Theorem 4.3</ref> \n\nIt has been conjectured that this can be reduced to an expression linear in ''n''.<ref>{{harvtxt|Freedman|Gupta|Guralnick|1997}}, p. 160</ref>\n\n== Notes ==\n<references/>\n\n== References ==\n* {{Citation | last1=Đoković | first1=Dragomir Ž. | last2=Johnson | first2=Charles R. | title=Unitarily achievable zero patterns and traces of words in ''A'' and ''A''* | doi=10.1016/j.laa.2006.03.002 | year=2007 | journal=Linear Algebra and its Applications | issn=0024-3795 | volume=421 | issue=1 | pages=63–68}}.\n* {{Citation | last1=Freedman | first1=Allen R. | last2=Gupta | first2=Ram Niwas | last3=Guralnick | first3=Robert M. | title=Shirshov's theorem and representations of semigroups | year=1997 | journal=[[Pacific Journal of Mathematics]] | issn=0030-8730 | volume=181 | issue=3 | pages=159–176 | doi=10.2140/pjm.1997.181.159}}.\n* {{Citation | last1=Horn | first1=Roger A. | last2=Johnson | first2=Charles R. | title=Matrix Analysis | publisher=[[Cambridge University Press]] | isbn=978-0-521-38632-6 | year=1985}}. \n* {{Citation | last1=Pappacena | first1=Christopher J. | title=An upper bound for the length of a finite-dimensional algebra | doi=10.1006/jabr.1997.7140 | year=1997 | journal=Journal of Algebra | issn=0021-8693 | volume=197 | issue=2 | pages=535–545}}.\n* {{Citation | last1=Sibirskiǐ | first1=K. S. | title=Algebraic Invariants of Differential Equations and Matrices | publisher=Izdat. \"Štiinca\", Kishinev | language=Russian | year=1976}}.\n* {{Citation | last1=Specht | first1=Wilhelm | author1-link=Wilhelm Specht | title=Zur Theorie der Matrizen. II | url=http://gdz.sub.uni-goettingen.de/dms/load/toc/?PPN=PPN37721857X_0050&DMDID=dmdlog6 | year=1940 | journal=Jahresbericht der Deutschen Mathematiker-Vereinigung | issn=0012-0456 | volume=50 | pages=19–23}}.\n\n{{DEFAULTSORT:Specht's Theorem}}\n[[Category:Matrix theory]]\n[[Category:Combinatorics on words]]\n[[Category:Theorems in algebra]]"
    },
    {
      "title": "Stone's representation theorem for Boolean algebras",
      "url": "https://en.wikipedia.org/wiki/Stone%27s_representation_theorem_for_Boolean_algebras",
      "text": "{{no footnotes|date=June 2015}}\nIn [[mathematics]], '''Stone's representation theorem for Boolean algebras''' states that every [[Boolean algebra (structure)|Boolean algebra]] is [[isomorphic]] to a certain [[field of sets]]. The theorem is fundamental to the deeper understanding of [[Boolean logic|Boolean algebra]] that emerged in the first half of the 20th century. The theorem was first proved by [[Marshall H. Stone]] (1936). Stone was led to it by his study of the [[spectral theory]] of [[linear operator|operators]] on a [[Hilbert space]].\n\n==Stone spaces==\nEach [[Boolean algebra (structure)|Boolean algebra]] ''B'' has an associated topological space, denoted here ''S''(''B''), called its [[Stone space|'''Stone space'''.]] The points in ''S''(''B'') are the [[ultrafilter]]s on ''B'', or equivalently the homomorphisms from ''B'' to the [[two-element Boolean algebra]]. The topology on ''S''(''B'') is generated by a (closed) [[basis (topology)|basis]] consisting of all sets of the form\n:<math>\\{ x \\in S(B) \\mid b \\in x\\},</math>\nwhere ''b'' is an element of ''B''. This is the topology of pointwise convergence of nets of homomorphisms into the two-element Boolean algebra.\n\nFor every Boolean algebra ''B'', ''S''(''B'') is a [[compact space|compact]] [[totally disconnected]] [[Hausdorff space|Hausdorff]] space; such spaces are called '''Stone spaces''' (also ''profinite spaces''). Conversely, given any topological space ''X'', the collection of subsets of ''X'' that are [[clopen set|clopen]] (both closed and open) is a Boolean algebra.\n\n==Representation theorem==\n\nA simple version of '''Stone's representation theorem''' states that every Boolean algebra ''B'' is isomorphic to the algebra of clopen subsets of its Stone space ''S''(''B''). The isomorphism sends an element ''b''&isin;''B'' to the set of all ultrafilters that contain ''b''. This is a clopen set because of the choice of topology on ''S''(''B'') and because ''B'' is a Boolean algebra. \n\nRestating the theorem using the language of [[category theory]]; the theorem states that there is a [[duality of categories|duality]] between the [[category theory|category]] of [[Boolean algebra (structure)|Boolean algebra]]s  and the category of Stone spaces. This duality means that in addition to the correspondence between Boolean algebras and their Stone spaces, each homomorphism from a Boolean algebra ''A'' to a Boolean algebra ''B'' corresponds in a natural way to a continuous function from ''S''(''B'') to  ''S''(''A''). In other words, there is a [[contravariant functor]] that gives an [[equivalence (category theory)|equivalence]] between the categories. This was an early example of a nontrivial duality of categories.\n\nThe theorem is a special case of [[Stone duality]], a more general framework for dualities between [[topological space]]s and [[partially ordered set]]s.\n\nThe proof requires either the [[axiom of choice]] or a weakened form of it. Specifically, the theorem is equivalent to the [[Boolean prime ideal theorem]], a weakened choice principle that states that every Boolean algebra has a prime ideal.\n\nAn extension of the classical Stone duality to the category of Boolean spaces (= zero-dimensional locally compact Hausdorff spaces) and continuous maps (respectively, perfect maps) was obtained by G. D. Dimov (respectively, by H. P. Doctor) (see the references below).\n\n==See also==\n* [[Field of sets]]\n* [[List of Boolean algebra topics]]\n* [[Stonean space]]\n* [[Stone functor]]\n* [[Profinite group]]\n* [[Representation theorem]]\n\n==References==\n* [[Paul Halmos]], and Givant, Steven (1998) ''Logic as Algebra''. Dolciani Mathematical Expositions No. 21. [[The Mathematical Association of America]].\n* [[Peter T. Johnstone|Johnstone, Peter T.]] (1982) ''Stone Spaces''. Cambridge University Press. {{isbn|0-521-23893-5}}.\n* [[Marshall H. Stone]] (1936) \"[https://www.jstor.org/stable/1989664 The Theory of Representations of Boolean Algebras,]\" ''Transactions of the American Mathematical Society 40'': 37-111. \n* G. D. Dimov (2012) ''Some generalizations of the Stone Duality Theorem''. ''Publ. Math. Debrecen 80'': 255–293.\n* H. P. Doctor (1964) ''The categories of Boolean lattices, Boolean rings and Boolean spaces''. ''Canad. Math. Bulletin 7'': 245–252.\n* Burris, Stanley N., and H. P. Sankappanavar, H. P.(1981) ''[http://www.thoralf.uwaterloo.ca/htdocs/ualg.html A Course in Universal Algebra.]''  Springer-Verlag. {{isbn|3-540-90578-2}}.\n\n[[Category:General topology]]\n[[Category:Boolean algebra]]\n[[Category:Theorems in algebra]]\n[[Category:Categorical logic]]"
    },
    {
      "title": "Strassmann's theorem",
      "url": "https://en.wikipedia.org/wiki/Strassmann%27s_theorem",
      "text": "In [[mathematics]], '''Strassmann's theorem''' is a result in [[field theory (mathematics)|field theory]]. It states that, for suitable fields, suitable formal [[power series]] with coefficients in the [[valuation ring]] of the field have only finitely many zeroes.\n\n==History==\nIt was introduced by {{harvs|txt|last=Straßmann|first= Reinhold|year=1928}}.\n\n==Statement of the theorem==\nLet ''K'' be a field with a [[Absolute value (algebra)|non-Archimedean absolute value]] |&nbsp;·&nbsp;| and let ''R'' be the valuation ring of ''K''. Let ''f''(''x'') be a formal power series with coefficients in ''R'' other than the zero series, with coefficients ''a''<sub>''n''</sub> converging to zero with respect to |&nbsp;·&nbsp;|. Then ''f''(''x'') has only finitely many zeroes in ''R''. More precisely, the number of zeros is at most ''N'', where ''N'' is the largest index with |''a''<sub>''N''</sub>| = max  |''a''<sub>''n''</sub>|.\n\n==References==\n*{{cite book|last = Murty|first = M. Ram|authorlink = M. Ram Murty|title = Introduction to P-Adic Analytic Number Theory|publisher = American Mathematical Society|date = 2002|pages = 35|isbn = 978-0-8218-3262-2|url = https://books.google.com/books?id=xi3ueDVuO7sC}}\n*{{Citation | authorlink = Reinhold Strassmann | last1=Straßmann | first1=Reinhold | title=Über den Wertevorrat von Potenzreihen im Gebiet der p-adischen Zahlen. | url=http://gdz.sub.uni-goettingen.de/en/dms/load/img/?PPN=GDZPPN002170485 | language=German | doi=10.1515/crll.1928.159.13 |jfm=54.0162.06 | year=1928 | journal=[[Journal für die reine und angewandte Mathematik]] | issn=0075-4102 | volume=159 | pages=13–28}}\n\n==External links==\n* {{MathWorld |title=Strassman's Theorem |urlname=StrassmansTheorem}}\n\n[[Category:Field theory]]\n[[Category:Theorems in algebra]]"
    },
    {
      "title": "Subgroups of cyclic groups",
      "url": "https://en.wikipedia.org/wiki/Subgroups_of_cyclic_groups",
      "text": "In [[abstract algebra]], every [[subgroup]] of a [[cyclic group]] is cyclic. Moreover, for a [[finite group|finite]] cyclic group of order ''n'', every subgroup's order is a divisor of ''n'', and there is exactly one subgroup for each divisor.<ref>{{citation|title=The Theory of Groups|first=Marshall|last=Hall|authorlink=Marshall Hall (mathematician)|publisher=American Mathematical Society|year=1976|isbn=9780821819678|at=Theorem 3.1.1, pp. 35–36|url=https://books.google.com/books?id=oyxnWF9ssI8C&pg=PA35}}</ref><ref>{{citation|title=A Course in Algebra|volume=56|series=[[Graduate Studies in Mathematics]]|first=Ėrnest Borisovich|last=Vinberg|publisher=American Mathematical Society|year=2003|isbn=9780821834138|url=https://books.google.com/books?id=kd24d3mwaecC&pg=PA152|at=Theorem 4.50, pp. 152–153}}.</ref> This result has been called the '''fundamental theorem of cyclic groups'''.<ref>{{citation |page=77 |chapter=Fundamental Theorem of Cyclic Groups |title=Contemporary Abstract Algebra |author=Joseph A. Gallian |year=2010 |isbn=9780547165097}}</ref><ref>{{citation |page=110 |chapter= Cyclic Groups and the Order of an Element |title=Introduction To Abstract Algebra |author = W. Keith Nicholson |year = 1999 |isbn=0471331090}}</ref>\n\n==Finite cyclic groups==\nFor every finite group ''G'' of order ''n'', the following statements are equivalent:\n* ''G'' is cyclic.\n* For every divisor ''d'' of ''n'', ''G'' has exactly one subgroup of order ''d''.\n* For every divisor ''d'' of ''n'', ''G'' has at most one subgroup of order ''d''.\nThis statement is known by various names such as '''characterization by subgroups'''.<ref name=\"Roman2011\">{{cite book|author=Steven Roman|title=Fundamentals of Group Theory: An Advanced Approach|url=https://books.google.com/books?id=eWkqG0aiVsMC&pg=PA44|year=2011|publisher=Springer|isbn=978-0-8176-8300-9|page=44}}</ref><ref name=\"Balakrishnan1994\">{{cite book|author=V. K. Balakrishnan|title=Schaum's Outline of Combinatorics|url=https://books.google.com/books?id=di0WSsXV74wC&pg=PA155|year=1994|publisher=McGraw-Hill Prof Med/Tech|isbn=978-0-07-003575-1|page=155}}</ref><ref name=\"Stroppel2006\">{{cite book|author=Markus Stroppel|title=Locally Compact Groups|url=https://books.google.com/books?id=3_BPupMDRr8C&pg=PA64|year=2006|publisher=European Mathematical Society|isbn=978-3-03719-016-6|page=64}}</ref> (See also [[cyclic group]] for some characterization.)<!-- if I remember correctly, an abelian group is cyclic if every Sylow subgroup is cyclic. -->\n\nThere exist finite groups other than cyclic groups with the property that all proper subgroups are cyclic; the [[Klein group]] is an example. However, the Klein group has more than one subgroup of order 2, so it does not meet the conditions of the characterization.\n\n==The infinite cyclic group==\nThe infinite cyclic group is isomorphic to the additive subgroup '''Z''' of the integers. There is one subgroup ''d'''''Z''' for each integer ''d'' (consisting of the multiples of ''d''), and with the exception of the trivial group (generated by ''d''&nbsp;=&nbsp;0) every such subgroup is itself an infinite cyclic group. Because the infinite cyclic group is a [[free group]] on one generator (and the trivial group is a free group on no generators), this result can be seen as a special case of the [[Nielsen–Schreier theorem]] that every subgroup of a free group is itself free.<ref name=\"aluffi\">{{citation|title=Algebra, Chapter 0|volume=104|series=Graduate Studies in Mathematics|first=Paolo|last=Aluffi|publisher=American Mathematical Society|year=2009|isbn=9780821847817|url=https://books.google.com/books?id=deWkZWYbyHQC&pg=PA82|pages=82–84|contribution=6.4 Example: Subgroups of Cyclic Groups}}.</ref>\n\nThe fundamental theorem for finite cyclic groups can be established from the same theorem for the infinite cyclic groups, by viewing each finite cyclic group as a [[quotient group]] of the infinite cyclic group.<ref name=\"aluffi\"/>\n\n==Lattice of subgroups==\nIn both the finite and the infinite case, the [[lattice of subgroups]] of a cyclic group is isomorphic to the [[Duality (order theory)|dual]] of a [[divisibility]] lattice. In the finite case, the lattice of subgroups of a cyclic group of order ''n'' is isomorphic to the dual of the lattice of divisors of ''n'', with a subgroup of order ''n''/''d'' for each divisor ''d''. The subgroup of order ''n''/''d'' is a subgroup of the subgroup of order ''n''/''e'' if and only if ''e'' is a divisor of ''d''. The lattice of subgroups of the infinite cyclic group can be described in the same way, as the dual of the divisibility lattice of all positive integers. If the infinite cyclic group is represented as the additive group on the integers, then the subgroup generated by ''d'' is a subgroup of the subgroup generated by ''e'' if and only if ''e'' is a divisor of ''d''.<ref name=\"aluffi\"/>\n\nDivisibility lattices are [[distributive lattice]]s, and therefore so are the lattices of subgroups of cyclic groups. This provides another alternative characterization of the finite cyclic groups: they are exactly the finite groups whose lattices of subgroups are distributive. More generally, a [[finitely generated group]] is cyclic if and only if its lattice of subgroups is distributive and an arbitrary group is [[locally cyclic group|locally cyclic]] if and only its lattice of subgroups is distributive.<ref>{{citation\n | last = Ore | first = Øystein | author-link = Øystein Ore\n | doi = 10.1215/S0012-7094-38-00419-3\n | mr = 1546048\n | issue = 2\n | journal = Duke Mathematical Journal\n | pages = 247–269\n | title = Structures and group theory. II\n | volume = 4\n | year = 1938}}.</ref> The additive group of the [[rational number]]s provides an example of a group that is locally cyclic, and that has a distributive lattice of subgroups, but that is not itself cyclic.\n\n==References==\n{{reflist}}\n\n{{fundamental theorems}}\n\n[[Category:Theorems in algebra]]\n[[Category:Articles containing proofs]]\n[[Category:Fundamental theorems|Cyclic groups]]"
    },
    {
      "title": "Sylow theorems",
      "url": "https://en.wikipedia.org/wiki/Sylow_theorems",
      "text": "{{Group theory sidebar |Finite}}\n{{Inline citations|date=November 2018}}\n\nIn [[mathematics]], specifically in the field of [[finite group theory]], the '''Sylow theorems''' are a collection of [[theorem]]s named after the [[Norway|Norwegian]] [[mathematician]] [[Peter Ludwig Mejdell Sylow|Peter Ludwig Sylow]] ([[#{{harvid|Sylow|1872}}|1872]]) that give detailed information about the number of [[subgroup]]s of fixed [[order of a group|order]] that a given [[finite group]] contains. The Sylow theorems form a fundamental part of finite group theory and have very important applications in the [[classification of finite simple groups]].\n\nFor a [[prime number]] ''p'', a '''Sylow ''p''-subgroup''' (sometimes '''''p''-Sylow subgroup''') of a group ''G'' is a maximal ''p''-subgroup of ''G'', i.e., a subgroup of ''G'' that is a [[p-group|''p''-group]] (so that the [[order of a group element|order]] of every group element is a [[power (mathematics)|power]] of ''p'') that is not a proper subgroup of any other ''p''-subgroup of ''G''. The set of all Sylow ''p''-subgroups for a given prime ''p'' is sometimes written Syl<sub>''p''</sub>(''G'').\n\nThe Sylow theorems assert a partial converse to [[Lagrange's theorem (group theory)|Lagrange's theorem]]. Lagrange's theorem states that for any finite group ''G'' the order (number of elements) of every subgroup of ''G'' divides the order of ''G''.  The Sylow theorems state that for every [[prime factor]] ''p'' of the order of a finite group ''G'', there exists a Sylow ''p''-subgroup of ''G'' of order ''p<sup>n</sup>'', the highest power of ''p'' that divides the order of ''G''.  Moreover, every subgroup of order ''p<sup>n</sup>'' is a Sylow ''p''-subgroup of ''G'', and the Sylow ''p''-subgroups of a group (for a given prime ''p'') are [[Conjugacy class|conjugate]] to each other. Furthermore, the number of Sylow ''p''-subgroups of a group for a given prime ''p'' is congruent to {{nowrap|1 mod ''p''.}}\n\n== Theorems ==\n\nCollections of subgroups that are each maximal in one sense or another are common in group theory. The surprising result here is that in the case of Syl<sub>''p''</sub>(''G''), all members are actually [[group isomorphism|isomorphic]] to each other and have the largest possible order: if |''G''| = ''p<sup>n</sup>m'' with ''n'' > 0 where ''p'' does not divide ''m'', then every Sylow ''p''-subgroup ''P'' has order |''P''| = ''p<sup>n</sup>''. That is, ''P'' is a ''p''-group and ''gcd''(|''G'' : ''P''|, ''p'') = 1. These properties can be exploited to further analyze the structure of ''G''.\n\nThe following theorems were first proposed and proven by Ludwig Sylow in 1872, and published in ''[[Mathematische Annalen]]''.\n\n'''Theorem 1''': For every [[prime factor]] ''p'' with [[multiplicity of a prime factor|multiplicity]] ''n'' of the order of a finite group ''G'', there exists a Sylow ''p''-subgroup of ''G'', of order ''p<sup>n</sup>''.\n\nThe following weaker version of theorem 1 was first proved by [[Augustin-Louis Cauchy]], and is known as [[Cauchy's theorem (group theory)|Cauchy's theorem]].\n\n'''Corollary''': Given a finite group ''G'' and a prime number ''p'' dividing the order of ''G'', then there exists an element (and hence a subgroup) of order ''p'' in ''G''.<ref>Fraleigh, Victor J. Katz. A First Course In Abstract Algebra. p. 322. {{isbn|9788178089973}}</ref>\n\n'''Theorem 2''': Given a finite group ''G'' and a prime number ''p'', all Sylow ''p''-subgroups of ''G'' are [[conjugacy class|conjugate]] to each other, i.e. if ''H'' and ''K'' are Sylow ''p''-subgroups of ''G'', then there exists an element ''g'' in ''G'' with ''g''<sup>−1</sup>''Hg'' = ''K''.\n\n'''Theorem 3''': Let ''p'' be a prime factor with multiplicity ''n'' of the order of a finite group ''G'', so that the order of ''G'' can be written as {{nowrap|''p<sup>n</sup>m''}}, where {{nowrap|''n'' > 0}} and ''p'' does not divide ''m''. Let ''n<sub>p</sub>'' be the number of Sylow ''p''-subgroups of ''G''. Then the following hold:\n* ''n<sub>p</sub>'' divides ''m'', which is the [[index of a subgroup|index]] of the Sylow ''p''-subgroup in ''G''.\n* ''n<sub>p</sub>'' ≡ 1 (mod&nbsp;''p'').\n* ''n<sub>p</sub>'' = |''G'' : ''N<sub>G</sub>''(''P'')|, where ''P'' is any Sylow ''p''-subgroup of ''G'' and ''N<sub>G</sub>'' denotes the [[normalizer]].\n\n=== Consequences ===\n\nThe Sylow theorems imply that for a prime number ''p'' every Sylow ''p''-subgroup is of the same order, ''p<sup>n</sup>''. Conversely, if a subgroup has order ''p<sup>n</sup>'', then it is a Sylow ''p''-subgroup, and so is isomorphic to every other Sylow ''p''-subgroup. Due to the maximality condition, if ''H'' is any ''p''-subgroup of ''G'', then ''H'' is a subgroup of a ''p''-subgroup of order ''p<sup>n</sup>''.\n\nA very important consequence of Theorem 3 is that the condition ''n<sub>p</sub>'' = 1 is equivalent to saying that the Sylow ''p''-subgroup of ''G'' is a [[normal subgroup]]\n(there are groups that have normal subgroups but no normal Sylow subgroups, such as ''S''<sub>4</sub>).\n\n=== Sylow theorems for infinite groups ===\n\nThere is an analogue of the Sylow theorems for infinite groups.  We define a Sylow ''p''-subgroup in an infinite group to be a ''p''-subgroup (that is, every element in it has ''p''-power order) that is maximal for inclusion among all ''p''-subgroups in the group.  Such subgroups exist by [[Zorn's lemma]].\n\n'''Theorem''': If ''K'' is a Sylow ''p''-subgroup of ''G'', and ''n<sub>p</sub>'' = |Cl(''K'')| is finite, then every Sylow ''p''-subgroup is conjugate to ''K'', and ''n<sub>p</sub>'' ≡ 1 (mod&nbsp;''p''), where Cl(''K'') denotes the conjugacy class of ''K''.\n\n== Examples ==\n[[File:Labeled Triangle Reflections.svg|thumb|In ''D''<sub>6</sub> all reflections are conjugate, as reflections correspond to Sylow 2-subgroups.]]\n\nA simple illustration of Sylow subgroups and the Sylow theorems are the [[dihedral group]] of the ''n''-gon, ''D''<sub>2''n''</sub>. For ''n'' odd, 2 = 2<sup>1</sup> is the highest power of 2 dividing the order, and thus subgroups of order 2 are Sylow subgroups. These are the groups generated by a reflection, of which there are ''n'', and they are all conjugate under rotations; geometrically the axes of symmetry pass through a vertex and a side.\n\n[[File:Hexagon reflections.svg|thumb|left|In ''D''<sub>12</sub> reflections no longer correspond to Sylow 2-subgroups, and fall into two conjugacy classes.]]\nBy contrast, if ''n'' is even, then 4 divides the order of the group, and the subgroups of order 2 are no longer Sylow subgroups, and in fact they fall into two conjugacy classes, geometrically according to whether they pass through two vertices or two faces. These are related by an [[outer automorphism]], which can be represented by rotation through π/''n'', half the minimal rotation in the dihedral group.\n\n{{clear}}\nAnother example are the Sylow p-subgroups of ''GL''<sub>2</sub>(''F''<sub>''q''</sub>), where ''p'' and ''q'' are primes&nbsp;≥&nbsp;3 and ''p''&nbsp;≡&nbsp;1&nbsp;(mod&nbsp;''q'') , which are all [[Abelian group|abelian]]. The order of ''GL''<sub>2</sub>(''F''<sub>''q''</sub>) is (''q''<sup>2</sup>&nbsp;−&nbsp;1)(''q''<sup>2</sup>&nbsp;−&nbsp;''q'') = (''q'')(''q''&nbsp;+&nbsp;1)(''q''&nbsp;−&nbsp;1)<sup>2</sup>. Since ''q''&nbsp;=&nbsp;''p''<sup>''n''</sup>''m''&nbsp;+&nbsp;1, the order of ''GL''<sub>2</sub>(''F''<sub>''q''</sub>) =&nbsp;''p''<sup>2''n''</sup> ''m''&prime;. Thus by Theorem&nbsp;1, the order of the Sylow ''p''-subgroups is ''p''<sup>2''n''</sup>.\n\nOne such subgroup ''P'', is the set of diagonal matrices  <math>\\begin{bmatrix}x^{im} & 0 \\\\0 & x^{jm} \\end{bmatrix}</math>, ''x'' is any [[Primitive root modulo n|primitive root]] of  ''F''<sub>''q''</sub>. Since the order of ''F''<sub>''q''</sub> is ''q''&nbsp;−&nbsp;1, its primitive roots have order ''q'' − 1, which implies that ''x''<sup>(''q''&nbsp;−&nbsp;1)/''p''<sup>''n''</sup></sup> or ''x''<sup>''m''</sup> and all its powers have an order which is a power of&nbsp;''p''. So, ''P'' is a subgroup where all its elements have orders which are powers of&nbsp;''p''. There are ''p<sup>n</sup>'' choices for both ''a'' and ''b'', making |''P''| =&nbsp;''p''<sup>2''n''</sup>. This means ''P'' is a Sylow ''p''-subgroup, which is abelian, as all diagonal matrices commute, and because Theorem 2 states that all Sylow ''p''-subgroups are conjugate to each other, the Sylow ''p''-subgroups of ''GL''<sub>2</sub>(''F''<sub>''q''</sub>) are all abelian.\n\n==Example applications==\nSince Sylow's theorem ensures the existence of p-subgroups of a finite group, its worthwhile to study groups of prime power order more closely. Most of the examples use Sylow's theorem to prove that a group of a particular order is not [[Simple group|simple]]. For groups of small order, the congruence condition of Sylow's theorem is often sufficient to force the existence of a [[normal subgroup]]. \n;Example-1: Groups of order ''pq'', ''p'' and ''q'' primes with ''p''&nbsp;<&nbsp;''q''. \n;Example-2: Group of order 30, groups of order 20, groups of order ''p''<sup>2</sup>''q'', ''p'' and ''q'' distinct primes are some of the applications. \n;Example-3: (Groups of order 60): If the order |''G''|&nbsp;=&nbsp;60 and ''G'' has more than one Sylow 5-subgroup, then ''G'' is simple.\n\n=== Cyclic group orders ===\nSome non-prime numbers ''n'' are such that every group of order ''n'' is cyclic.  One can show that ''n'' = 15 is such a number using the Sylow theorems:  Let ''G'' be a group of order 15 = 3 · 5 and ''n''<sub>3</sub> be the number of Sylow 3-subgroups. Then ''n''<sub>3</sub> <math>\\mid</math> 5 and ''n''<sub>3</sub> ≡ 1 (mod 3). The only value satisfying these constraints is 1; therefore, there is only one subgroup of order 3, and it must be [[normal subgroup|normal]] (since it has no distinct conjugates). Similarly, ''n''<sub>5</sub> must divide 3, and ''n''<sub>5</sub> must equal 1 (mod 5); thus it must also have a single normal subgroup of order 5. Since 3 and 5 are [[coprime]], the intersection of these two subgroups is trivial, and so ''G'' must be the [[internal direct product]] of groups of order 3 and 5, that is the [[cyclic group]] of order 15. Thus, there is only one group of order 15 ([[up to]] isomorphism).\n\n=== Small groups are not simple ===\nA more complex example involves the order of the smallest [[simple group]] that is not [[cyclic group|cyclic]]. [[Burnside's theorem|Burnside's ''p<sup>a</sup> q<sup>b</sup>'' theorem]] states that if the order of a group is the product of one or two [[prime power]]s, then it is [[solvable group|solvable]], and so the group is not simple, or is of prime order and is cyclic. This rules out every group up to order 30 {{nowrap|({{=}} 2 · 3 · 5)}}.\n\nIf ''G'' is simple, and |''G''| = 30, then ''n''<sub>3</sub> must divide 10 ( = 2 · 5), and ''n''<sub>3</sub> must equal 1 (mod 3). Therefore, ''n''<sub>3</sub> = 10, since neither 4 nor 7 divides 10, and if ''n''<sub>3</sub> = 1 then, as above, ''G'' would have a normal subgroup of order 3, and could not be simple. ''G'' then has 10 distinct cyclic subgroups of order 3, each of which has 2 elements of order 3 (plus the identity). This means ''G'' has at least 20 distinct elements of order 3.\n\nAs well, ''n''<sub>5</sub> = 6, since ''n''<sub>5</sub> must divide 6 ( = 2 · 3), and ''n''<sub>5</sub> must equal 1 (mod 5). So ''G'' also has 24 distinct elements of order 5. But the order of ''G'' is only 30, so a simple group of order 30 cannot exist.\n\nNext, suppose |''G''| = 42 = 2 · 3 · 7. Here ''n''<sub>7</sub> must divide 6 ( =  2 · 3) and ''n''<sub>7</sub> must equal 1 (mod 7), so ''n''<sub>7</sub> = 1. So, as before, ''G'' can not be simple.\n\nOn the other hand, for |''G''| = 60 = 2<sup>2</sup> · 3 · 5, then ''n''<sub>3</sub> = 10 and ''n''<sub>5</sub> = 6 is perfectly possible. And in fact, the smallest simple non-cyclic group is ''A''<sub>5</sub>, the [[alternating group]] over 5 elements. It has order 60, and has 24 [[cyclic permutation]]s of order 5, and 20 of order 3.\n\n=== Wilson's theorem ===\nPart of [[Wilson's theorem]] states that\n\n:<math>(p-1)!\\ \\equiv\\ -1 \\pmod p</math>\n\nfor every prime ''p''. One may easily prove this theorem by Sylow's third theorem. Indeed,  \nobserve that the number ''n<sub>p</sub>'' of Sylow's ''p''-subgroups  \nin the symmetric group ''S<sub>p</sub>'' is (''p''&nbsp;&minus;&nbsp;2)!. On the other hand, ''n''<sub>''p''</sub> ≡ 1 (mod&nbsp;''p''). Hence, (''p''&nbsp;&minus;&nbsp;2)! ≡ 1 (mod&nbsp;''p''). So, (''p''&nbsp;&minus;&nbsp;1)! ≡ &minus;1 (mod&nbsp;''p'').\n\n=== Fusion results ===\n[[Frattini's argument]] shows that a Sylow subgroup of a normal subgroup provides a factorization of a finite group.  A slight generalization known as '''Burnside's fusion theorem''' states that if ''G'' is a finite group with Sylow ''p''-subgroup ''P'' and two subsets ''A'' and ''B'' normalized by ''P'', then ''A'' and ''B'' are ''G''-conjugate if and only if they are ''N<sub>G</sub>''(''P'')-conjugate.  The proof is a simple application of Sylow's theorem: If ''B''=''A<sup>g</sup>'', then the normalizer of ''B'' contains not only ''P'' but also ''P<sup>g</sup>'' (since ''P<sup>g</sup>'' is contained in the normalizer of ''A<sup>g</sup>'').  By Sylow's theorem ''P'' and ''P<sup>g</sup>'' are conjugate not only in ''G'', but in the normalizer of ''B''.  Hence ''gh''<sup>−1</sup> normalizes ''P'' for some ''h'' that normalizes ''B'', and then ''A''<sup>''gh''<sup>−1</sup></sup> = ''B''<sup>h<sup>−1</sup></sup> = ''B'', so that ''A'' and ''B'' are ''N<sub>G</sub>''(''P'')-conjugate.  Burnside's fusion theorem can be used to give a more powerful factorization called a [[semidirect product]]: if ''G'' is a finite group whose Sylow ''p''-subgroup ''P'' is contained in the center of its normalizer, then ''G'' has a normal subgroup ''K'' of order coprime to ''P'', ''G'' = ''PK'' and ''P''∩''K'' = {1}, that is, ''G'' is [[p-nilpotent group|''p''-nilpotent]].\n\nLess trivial applications of the Sylow theorems include the [[focal subgroup theorem]], which studies the control a Sylow ''p''-subgroup of the [[derived subgroup]] has on the structure of the entire group.  This control is exploited at several stages of the [[classification of finite simple groups]], and for instance defines the case divisions used in the [[Alperin–Brauer–Gorenstein theorem]] classifying finite [[simple group]]s whose Sylow 2-subgroup is a [[quasi-dihedral group]].  These rely on [[J. L. Alperin]]'s strengthening of the conjugacy portion of Sylow's theorem to control what sorts of elements are used in the conjugation.\n\n==Proof of the Sylow theorems==\n\nThe Sylow theorems have been proved in a number of ways, and the history of the proofs themselves is the subject of many papers including {{harv|Waterhouse|1980}}, {{harv|Scharlau|1988}}, {{harv|Casadio|Zappa|1990}}, {{harv|Gow|1994}}, and to some extent {{harv|Meo|2004}}.\n\nOne proof of the Sylow theorems exploits the notion of [[Group action (mathematics)|group action]] in various creative ways. The group ''G'' acts on itself or on the set of its ''p''-subgroups in various ways, and each such action can be exploited to prove one of the Sylow theorems.  The following proofs are based on combinatorial arguments of {{harv|Wielandt|1959}}. In the following, we use ''a'' <math>\\mid</math> ''b'' as notation for \"a divides b\" and ''a'' <math>\\nmid</math> ''b'' for the negation of this statement.\n\n<blockquote> '''Theorem 1''':  A finite group ''G'' whose order |''G''| is divisible by a prime power ''p<sup>k</sup>'' has a subgroup of order ''p<sup>k</sup>''.</blockquote>\n\nProof: Let |''G''| = ''p<sup>k</sup>m = p<sup>k+r</sup>u'' such that ''p'' does not divide ''u'', and let Ω denote the set of subsets of ''G'' of size ''p<sup>k</sup>''. ''G'' [[Group action (mathematics)|acts]] on Ω by left multiplication. The [[Group action (mathematics)#Orbits and stabilizers|orbits]] ''G''ω = {''g''ω | ''g'' ∈ ''G''} of the ω ∈ Ω are the [[equivalence class]]es under the action of ''G''.\n\nFor any ω ∈ Ω consider its [[Group action (mathematics)#Orbits and stabilizers|stabilizer subgroup]]  ''G''<sub>ω</sub> = {''g'' ∈ ''G'' | ''g''ω = ω}. For any fixed element α ∈ ω the function [''g'' ↦ ''g''α] maps ''G''<sub>ω</sub> to ω injectively: for any two ''g'', ''h'' ∈ ''G''<sub>ω</sub> we have that ''g''α = ''h''α implies ''g'' = ''h'', because α ∈ ω ⊆ ''G'' means that one may cancel on the right. Therefore,  '' p<sup>k</sup>'' = |ω| ≥ |''G''<sub>ω</sub>|.\n\nOn the other hand,\n\n:<math>|\\Omega | ={p^km \\choose p^k} = \\prod_{j=0}^{p^k - 1} \\frac{p^k m - j}{p^k - j} =  m\\prod_{j=1}^{p^{k} - 1} \\frac{p^{k - \\nu_p(j)} m - j/p^{\\nu_p(j)}}{p^{k - \\nu_p(j)} - j/p^{\\nu_p(j)}} </math>\n\nand no power of ''p'' remains in any of the factors inside the product on the right. Hence [[Additive p-adic valuation|''ν<sub>p</sub>'']](|Ω|) = ''ν<sub>p</sub>''(''m'') = ''r''.\nLet ''R'' ⊆ Ω be a complete representation of all the equivalence classes under the action of ''G''. Then,\n:<math>|\\Omega | =\\sum_{\\omega\\in R}|G\\omega|\\mathrm{.}</math>\nThus, there exists an element ω ∈ ''R'' such that ''s'' := ''ν<sub>p</sub>''(|''G''ω|) ≤ ''ν<sub>p</sub>''(|Ω|) = ''r''. Hence |''G''ω| = ''p<sup>s</sup>v'' where ''p'' does not divide ''v''. By the [[Group action (mathematics)#Orbits and stabilizers|orbit-stabilizer theorem]] we have |''G''<sub>ω</sub>| = |''G''| / |''G''ω| = ''p<sup>k+r-s</sup>u/v''. Therefore, ''p<sup>k</sup>'' <math>\\mid</math> |''G''<sub>ω</sub>|, so ''p<sup>k</sup>'' ≤ |''G''<sub>ω</sub>| and ''G''<sub>ω</sub>'' is the desired subgroup.\n\n<blockquote> '''Lemma''': Let ''G'' be a finite ''p''-group, let Ω be a finite set, let Ω<sub>G</sub> be the set generated by the action of ''G'' on all the elements of Ω, and let Ω<sub>0</sub> denote the set of points of Ω<sub>G</sub> that are fixed under the action of ''G''.  Then |Ω<sub>G</sub>| ≡ |Ω<sub>0</sub>| (mod&nbsp;''p''). </blockquote>\n\nProof: Write Ω<sub>G</sub> as a disjoint sum of its orbits under ''G''.  Any element ''x'' ∈ Ω<sub>G</sub> not fixed by ''G'' will lie in an orbit of order |''G''|/|''G<sub>x</sub>''| (where ''G<sub>x</sub>'' denotes the [[Group action (mathematics)#Orbits and stabilizers|stabilizer]]), which is a multiple of ''p'' by assumption.  The result follows immediately.\n\n<blockquote>'''Theorem 2''': If ''H'' is a ''p''-subgroup of ''G'' and ''P'' is a Sylow ''p''-subgroup of ''G'', then there exists an element ''g'' in ''G'' such that ''g''<sup>−1</sup>''Hg'' ≤ ''P''. In particular, all Sylow ''p''-subgroups of ''G'' are [[conjugacy class|conjugate]] to each other (and therefore [[isomorphism|isomorphic]]), that is, if ''H'' and ''K'' are Sylow ''p''-subgroups of ''G'', then there exists an element ''g'' in ''G'' with ''g''<sup>−1</sup>''Hg'' = ''K''.</blockquote>\n\nProof: Let Ω be the set of left [[coset]]s of ''P'' in ''G'' and let ''H'' act on Ω by left multiplication. Applying the Lemma to ''H'' on Ω, we see that |Ω<sub>0</sub>| ≡ |Ω| = [''G'' : ''P''] (mod&nbsp;''p'').  Now ''p'' <math>\\nmid</math> [''G'' : ''P''] by definition so ''p'' <math>\\nmid</math> |Ω<sub>0</sub>|, hence in particular |Ω<sub>0</sub>| ≠ 0 so there exists some ''gP'' ∈ Ω<sub>0</sub>.  It follows that for some ''g'' ∈ ''G'' and ∀ ''h'' ∈ ''H'' we have ''hgP'' = ''gP'' so ''g''<sup>−1</sup>''HgP'' = ''P'' and therefore ''g''<sup>−1</sup>''Hg'' ≤ ''P''. Now if ''H'' is a Sylow ''p''-subgroup, |''H''| = |''P''| = |''gPg''<sup>−1</sup>| so that ''H'' = ''gPg''<sup>−1</sup> for some ''g'' ∈ ''G''.\n\n<blockquote>'''Theorem 3''': Let ''q'' denote the order of any Sylow ''p''-subgroup ''P'' of a finite group ''G''.  Then ''n<sub>p</sub>'' = |''G'' : ''N<sub>G</sub>''(''P'')|, ''n<sub>p</sub>'' <math>\\mid</math> |''G''|/''q'' and ''n<sub>p</sub>'' ≡ 1 (mod&nbsp;''p'').</blockquote>\n\nProof: Let ''G'' act on ''P'', a Sylow p-subgroup, by conjugation. By the orbit-stabilizer theorem, ''n<sub>p</sub>'' = [''G'' : ''Stab''<sub>''G''</sub>(''P'')]. ''Stab''<sub>G</sub>(''P'') = { ''g'' ∈ ''G'' | ''gPg''<sup>−1</sup> = ''P'' }  = ''N''<sub>G</sub> (''P''), the normalizer of ''P'' in ''G''. Thus, ''n<sub>p</sub>'' = |''G'' : ''N<sub>G</sub>''(''P'')|, and it follows that this number is a divisor of |''G''|/''q''. Let Ω be the set of all Sylow ''p''-subgroups of ''G'', and let ''P'' act on Ω by conjugation. Let ''Q'' ∈ Ω<sub>0</sub> and observe that then ''Q'' = ''xQx''<sup>−1</sup> for all ''x'' ∈ ''P'' so that ''P'' ≤ ''N<sub>G</sub>''(''Q'').  By Theorem 2, ''P'' and ''Q'' are conjugate in ''N<sub>G</sub>''(''Q'') in particular, and ''Q'' is normal in ''N<sub>G</sub>''(''Q''), so then ''P'' = ''Q''.  It follows that Ω<sub>0</sub> = {''P''} so that, by the Lemma, |Ω| ≡ |Ω<sub>0</sub>| = 1 (mod&nbsp;''p'').\n\n== Algorithms ==\nThe problem of finding a Sylow subgroup of a given group is an important problem in [[computational group theory]].\n\nOne proof of the existence of Sylow ''p''-subgroups is constructive: if ''H'' is a ''p''-subgroup of ''G'' and the index [''G'':''H''] is divisible by ''p'', then the normalizer ''N'' = ''N<sub>G</sub>''(''H'') of ''H'' in ''G'' is also such that [''N'' : ''H''] is divisible by ''p''.  In other words, a polycyclic generating system of a Sylow ''p''-subgroup can be found by starting from any ''p''-subgroup ''H'' (including the identity) and taking elements of ''p''-power order contained in the normalizer of ''H'' but not in ''H'' itself.  The algorithmic version of this (and many improvements) is described in textbook form in {{harv|Butler|1991|loc=Chapter 16}}, including the algorithm described in {{harv|Cannon|1971}}.  These versions are still used in the [[GAP computer algebra system]].\n\nIn [[permutation group]]s, it has been proven in ({{harvs|nb|last=Kantor|year1=1985a|year2=1985b|year3=1990}}; {{harvnb|Kantor|Taylor|1988}}) that a Sylow ''p''-subgroup and its normalizer can be found in [[polynomial time]] of the input (the degree of the group times the number of generators).  These algorithms are described in textbook form in {{harv|Seress|2003}}, and are now becoming practical as the constructive recognition of finite simple groups becomes a reality.  In particular, versions of this algorithm are used in the [[Magma computer algebra system]].\n\n== See also ==\n{{Div col}}\n* [[Frattini's argument]]\n* [[Hall subgroup]]\n* [[Maximal subgroup]]\n* [[p-group]]\n{{Div col end}}\n\n== Notes ==\n{{Reflist}}\n\n==References==\n* {{Citation | first=L. | last=Sylow | author-link=Peter Ludwig Mejdell Sylow | title=Théorèmes sur les groupes de substitutions | language=French | journal=[[Mathematische Annalen|Math. Ann.]] | volume=5 | issue=4 | pages=584–594 | year=1872 | url=http://resolver.sub.uni-goettingen.de/purl?GDZPPN002242052 | doi=10.1007/BF01442913 | jfm=04.0056.02 }}\n\n=== Proofs ===\n* {{Citation | last1=Casadio | first1=Giuseppina | last2=Zappa | first2=Guido | title=History of the Sylow theorem and its proofs | language=Italian | mr=1096350 | zbl = 0721.01008 | year=1990 | journal=<abbr Title=\"Bollettino di Storia delle Scienze Matematiche\">Boll. Storia Sci. Mat.</abbr> | issn=0392-4432 | volume=10 | issue=1 | pages=29–75}}\n* {{Citation | last=Gow | first=Rod | title=Sylow's proof of Sylow's theorem | mr=1313412 | zbl = 0829.01011 | year=1994 | journal=<abbr Title=\"Irish Mathematical Society Bulletin\">Irish Math. Soc. Bull.</abbr> | issn=0791-5578 | issue=33 | pages=55–63}}\n* {{Citation | last1=Kammüller | first1=Florian | last2=Paulson | first2=Lawrence C. | title=A formal proof of Sylow's theorem. An experiment in abstract algebra with Isabelle HOL | url=http://www.cl.cam.ac.uk/users/lcp/papers/Kammueller/sylow.pdf | doi=10.1023/A:1006269330992 | mr=1721912 | zbl=0943.68149 | year=1999 | journal=<abbr Title=\"Journal of Automated Reasoning\">J. Automat. Reason.</abbr> | issn=0168-7433 | volume=23 | issue=3 | pages=235–264 | deadurl=yes | archiveurl=https://web.archive.org/web/20060103223152/http://www.cl.cam.ac.uk/users/lcp/papers/Kammueller/sylow.pdf | archivedate=2006-01-03 | df= }}\n*{{Citation | last=Meo | first=M. | title=The mathematical life of Cauchy's group theorem | doi=10.1016/S0315-0860(03)00003-X | mr=2055642 | zbl = 1065.01009 | year=2004 | journal=[[Historia Mathematica|Historia Math.]] | issn=0315-0860 | volume=31 | issue=2 | pages=196–221}}\n* {{Citation | last=Scharlau | first=Winfried | title=Die Entdeckung der Sylow-Sätze | language=German | doi=10.1016/0315-0860(88)90048-1 | mr=931678 | zbl = 0637.01006 | year=1988 | journal=[[Historia Mathematica|Historia Math.]] | issn=0315-0860 | volume=15 | issue=1 | pages=40–52}}\n* {{Citation | last=Waterhouse | first=William C. | authorlink = William C. Waterhouse | title=The early proofs of Sylow's theorem | doi=10.1007/BF00327877 | mr=575718 | zbl = 0436.01006 | year=1980 | journal=<abbr Title=\"Archive for History of Exact Sciences\">Arch. Hist. Exact Sci.</abbr> | issn=0003-9519 | volume=21 | issue=3 | pages=279–290}}\n* {{Citation | last=Wielandt | first=Helmut | author-link=:de:Helmut Wielandt | title=Ein Beweis für die Existenz der Sylowgruppen | language=German | doi=10.1007/BF01240818 | mr=0147529 | zbl = 0092.02403 | year=1959 | journal=<abbr Title=\"Archiv der Mathematik\">Arch. Math.</abbr> | issn=0003-9268 | volume=10 | issue=1 | pages=401–402}}\n\n=== Algorithms ===\n* {{Citation | last=Butler | first=G. | title=Fundamental Algorithms for Permutation Groups | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=[[Lecture Notes in Computer Science]] | isbn=978-3-540-54955-0 | mr=1225579 | zbl = 0785.20001 | year=1991 | volume=559 | doi = 10.1007/3-540-54955-2}}\n* {{Citation | last=Cannon | first=John J. | title=Computers in Algebra and Number Theory (<abbr title=\"Proceedings of a SIAM-AMS Symposium in Applied Mathematics\">Proc. SIAM-AMS Sympos. Appl. Math.</abbr>, New York, 1970) | series=<abbr title=\"SIAM-AMS Proceedings\">SIAM-AMS Proc.</abbr> | volume=4 | publisher=[[American Mathematical Society|AMS]] | location=Providence, RI | mr=0367027 | zbl = 0253.20027 | year=1971 | chapter=Computing local structure of large finite groups | pages=161–176 | issn=0160-7634}}\n*{{Citation | last=Kantor | first=William M. | title=Polynomial-time algorithms for finding elements of prime order and Sylow subgroups | mr=813589 | zbl = 0604.20001 | year=1985a | journal=<abbr Title=\"Journal of Algorithms\">J. Algorithms</abbr> | issn=0196-6774 | volume=6 | issue=4 | pages=478–514 | doi=10.1016/0196-6774(85)90029-X| url=http://uoregon.edu/~kantor/PAPERS/primeorder.pdf | citeseerx=10.1.1.74.3690 }}\n*{{Citation | last=Kantor | first=William M. | title=Sylow's theorem in polynomial time | doi=10.1016/0022-0000(85)90052-2 | mr=805654 | zbl = 0573.20022 | year=1985b | journal=<abbr Title=\"Journal of Computer and System Sciences\">J. Comput. Syst. Sci.</abbr> | issn=1090-2724 | volume=30 | issue=3 | pages=359–394}}\n*{{Citation | last1=Kantor | first1=William M. | last2=Taylor | first2=Donald E. | title=Polynomial-time versions of Sylow's theorem | mr=925595 | zbl = 0642.20019 | year=1988 | journal=<abbr Title=\"Journal of Algorithms\">J. Algorithms</abbr> | issn=0196-6774 | volume=9 | issue=1 | pages=1–17 | doi=10.1016/0196-6774(88)90002-8}}\n*{{Citation | last=Kantor | first=William M. | title=Finding Sylow normalizers in polynomial time | mr=1079450 | zbl = 0731.20005 | year=1990 | journal=<abbr Title=\"Journal of Algorithms\">J. Algorithms</abbr> | issn=0196-6774 | volume=11 | issue=4 | pages=523–563 | doi=10.1016/0196-6774(90)90009-4}}\n*{{Citation | last=Seress | first=Ákos | title=Permutation Group Algorithms | publisher=[[Cambridge University Press]] | series=Cambridge Tracts in Mathematics | isbn=978-0-521-66103-4 | mr=1970241 | zbl = 1028.20002 | year=2003 | volume=152}}\n\n== External links ==\n* {{springer|title=Sylow theorems|id=p/s091560}}\n* {{Wikibooks-inline|Abstract Algebra/Group Theory/The Sylow Theorems}}\n* {{MathWorld |title=Sylow p-Subgroup |id=Sylowp-Subgroup}}\n* {{MathWorld |title=Sylow Theorems |id=SylowTheorems}}\n\n[[Category:Finite groups]]\n[[Category:P-groups]]\n[[Category:Theorems in algebra]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "Sylvester's determinant identity",
      "url": "https://en.wikipedia.org/wiki/Sylvester%27s_determinant_identity",
      "text": "In [[matrix (mathematics)|matrix]] theory, '''Sylvester's determinant identity''' is an identity useful for evaluating certain types of [[determinant]]s. It is named after [[James Joseph Sylvester]], who stated this identity without proof in 1851.<ref>{{cite journal | last = Sylvester | first = James Joseph | title = On the relation between the minor determinants of linearly equivalent quadratic functions | journal = Philosophical Magazine | volume = 1 | year = 1851 | pages = 295–305}} <br> Cited in {{Cite journal | last1 = Akritas | first1 = A. G. | last2 = Akritas | first2 = E. K. | last3 = Malaschonok | first3 = G. I. | doi = 10.1016/S0378-4754(96)00035-3 | title = Various proofs of Sylvester's (determinant) identity | journal = Mathematics and Computers in Simulation | volume = 42 | issue = 4–6 | page = 585 | year = 1996 | pmid =  | pmc = }}</ref>\n\nGiven an {{math|''n '' × ''n''}}  matrix <math>A</math>, let <math>\\det(A)</math> denote its determinant. \nChoose a pair  \n\n:<math>u =(u_1, \\dots, u_m),  v =(v_1, \\dots, v) \\subset (1, \\cdots n)</math> \nof <math>m</math>-element ordered subsets of <math>(1, \\dots, n)</math>, <math> m\\le n</math>.\nLet <math>A^u_v </math> denote the <math>(n-m) \\times (n-m)</math> submatrix of  <math>A</math> obtained by deleting the rows in <math>u</math> and the columns in <math>v</math>. \nDefine the auxiliary <math>m \\times m</math> matrix <math>\\tilde{A}^u_v</math> whose elements are equal to the following determinants\n:<math>\n(\\tilde{A}^u_v)_{ij} := \\det(A^{u[\\hat{u}_i]}_{v[\\hat{v}_j}),\n</math>\nwhere <math>u[\\hat{u_i}]</math>, <math>v[\\hat{v_j}]</math> denote the <math>m-1</math> element subsets of <math>u</math> and <math>v</math> obtained by deleting the elements <math>u_i</math> and <math>v_j</math>, respectively. Then the following is '''Sylvester's determinantal identity''' (Sylvester, 1851)\n\n:<math> \\det(A)(\\det(A^u_v))^{m-1}=\\det(\\tilde{A}^u_v) </math>\n\nWhen <math>m=2</math>, this is the Desnanot-Jacobi identity (Jacobi, 1851)\n\n==References==\n{{reflist}}\n\n[[Category:Determinants]]\n[[Category:Matrix theory]]\n[[Category:Linear algebra]]\n[[Category:Theorems in algebra]]\n\n\n{{linear-algebra-stub}}"
    },
    {
      "title": "Sylvester's law of inertia",
      "url": "https://en.wikipedia.org/wiki/Sylvester%27s_law_of_inertia",
      "text": "'''Sylvester's law of inertia''' is a [[theorem]] in [[matrix algebra]] about certain properties of the [[coefficient matrix]] of a [[real number|real]] [[quadratic form]] that remain [[invariant (mathematics)|invariant]] under a [[change of basis]]. Namely, if ''A'' is the [[symmetric matrix]] that defines the quadratic form, and ''S'' is any invertible matrix such that ''D''&nbsp;=&nbsp;''SAS''<sup>T</sup> is diagonal, then the number of negative elements in the diagonal of ''D'' is always the same, for all such ''S''; and the same goes for the number of positive elements.\n\nThis property is named after [[James Joseph Sylvester]] who published its proof in 1852.<ref name=syl852>\n  {{cite journal|last=Sylvester|first=James Joseph|authorlink=James Joseph Sylvester | title=A demonstration of the theorem that every homogeneous quadratic polynomial is reducible by real orthogonal substitutions to the form of a sum of positive and negative squares | journal=Philosophical Magazine |series=4th Series| volume=4 | issue=23 | pages=138–142 | year=1852 |  url=http://www.maths.ed.ac.uk/~aar/sylv/inertia.pdf | doi= 10.1080/14786445208647087 | accessdate=2008-06-27}}\n</ref><ref name=norm>\n  {{cite book|last=Norman|first= C.W.| title=Undergraduate algebra | publisher=[[Oxford University Press]] | pages=360–361 | year=1986 |  isbn=978-0-19-853248-4 }}\n</ref>\n\n== Statement of the theorem ==\nLet ''A'' be a symmetric square matrix of order ''n'' with [[real number|real]] entries. Any [[non-singular matrix]] ''S'' of the same size is said to transform ''A'' into another symmetric matrix {{nowrap|1=''B'' = ''SAS''<sup>T</sup>}}, also of order ''n'', where ''S''<sup>T</sup> is the transpose of ''S''. It is also said that matrices ''A'' and ''B'' are congruent.  If ''A'' is the coefficient matrix of some quadratic form of '''R'''<sup>''n''</sup>, then ''B'' is the matrix for the same form after the change of basis defined by ''S''.\n\nA symmetric matrix ''A'' can always be transformed in this way into a [[diagonal matrix]] ''D'' which has only entries 0, +1 and −1 along the diagonal. Sylvester's law of inertia states that the number of diagonal entries of each kind is an invariant of ''A'', i.e. it does not depend on the matrix ''S'' used.\n\nThe number of +1s, denoted ''n''<sub>+</sub>, is called the '''positive index of inertia''' of ''A'', and the number of −1s, denoted ''n''<sub>−</sub>, is called the '''negative index of inertia'''. The number of 0s, denoted ''n''<sub>0</sub>, is the dimension of the [[kernel (linear algebra)|kernel]] of ''A'', and also the corank of ''A''.  These numbers satisfy an obvious relation\n\n: <math> n_0+n_{+}+n_{-}=n.</math>\n\nThe difference {{nowrap|1=sign(''A'') = ''n''<sub>+</sub> − ''n''<sub>−</sub>}} is usually called the '''signature''' of ''A''.  (However, some authors use that term for the triple {{nowrap|(''n''<sub>0</sub>, ''n''<sub>+</sub>, ''n''<sub>−</sub>)}} consisting of the corank and the positive and negative indices of inertia of ''A''; for a non-degenerate form of a given dimension these are equivalent data, but in general the triple yields more data.)\n\nIf the matrix ''A'' has the property that every principal upper left {{nowrap|''k'' × ''k''}} [[minor (determinant)|minor]] ''Δ''<sub>''k''</sub> is non-zero then the negative index of inertia is equal to the number of sign changes in the sequence\n\n: <math> \\Delta_0=1, \\Delta_1, \\ldots, \\Delta_n=\\det A. </math>\n\n==Statement in terms of eigenvalues==\nThe law can also be stated as follows: two symmetric square matrices of the same size have the same number of positive, negative and zero eigenvalues if and only if they are congruent<ref>{{cite book|last=Carrell|first=James B.|title=Groups, Matrices, and Vector Spaces: A Group Theoretic Approach to Linear Algebra|date=2017|publisher=Springer|isbn=978-0-387-79428-0|page=313}}</ref> (<math>S^{\\prime}=ASA^{T}</math>, <math>A</math> non-singular).\n\nThe positive and negative indices of a symmetric matrix ''A'' are also the number of positive and negative [[eigenvalue]]s of ''A''.  Any symmetric real matrix ''A'' has an [[eigendecomposition]] of the form ''QEQ''<sup>T</sup> where ''E'' is a diagonal matrix containing the eigenvalues of ''A'', and ''Q'' is an [[orthonormal]] square matrix containing the eigenvectors.  The matrix ''E'' can be written ''E''&nbsp;=&nbsp;''WDW''<sup>T</sup> where ''D'' is diagonal with entries&nbsp;0,&nbsp;+1, or&nbsp;&minus;1, and ''W'' is diagonal with ''W''<sub>''ii''</sub>&nbsp;=&nbsp;√|''E''<sub>''ii''</sub>|. The matrix ''S''&nbsp;=&nbsp;''QW'' transforms ''D'' to&nbsp;''A''.\n\n== Law of inertia for quadratic forms ==\nIn the context of [[quadratic form]]s, a real quadratic form ''Q'' in ''n'' variables (or on an ''n''-dimensional real vector space) can by a suitable change of basis (by non-singular linear transformation from x to y) be brought to the diagonal form\n\n: <math> Q(x_1,x_2,\\ldots,x_n)=\\sum_{i=1}^n a_i x_i^2 </math>\n\nwith each ''a''<sub>''i''</sub>&nbsp;∈&nbsp;{0,&nbsp;1,&nbsp;&minus;1}. Sylvester's law of inertia states that the number of coefficients of a given sign is an invariant of ''Q'', i.e., does not depend on a particular choice of diagonalizing basis. Expressed geometrically, the law of inertia says that all maximal subspaces on which the restriction of the quadratic form is [[definite bilinear form|positive definite]] (respectively, negative definite) have the same [[dimension (linear algebra)|dimension]]. These dimensions are the positive and negative indices of inertia.\n\n==Generalizations==\nSylvester's law of inertia is also valid if ''A'' and ''B'' have complex entries. In this case, it is said that ''A'' and ''B'' are *-congruent if and only if there exists a non-singular complex matrix ''S'' such that {{nowrap|1=''B'' = ''SAS''<sup>∗</sup>}}.\n\nIn the complex scenario, a way to state Sylvester's law of inertia is that if ''A'' and ''B'' are [[Hermitian matrix|Hermitian matrices]], then ''A'' and ''B'' are *-congruent if and only if they have the same inertia. A theorem due to Ikramov generalizes the law of inertia to any [[normal matrices]] ''A'' and ''B'':<ref>{{cite journal|last1=Ikramov|first1=Kh. D.|title=On the inertia law for normal matrices|journal=Doklady Mathematics|date=2001|volume=64|pages=141–142}}</ref>\n\nIf ''A'' and ''B'' are [[normal matrices]], then ''A'' and ''B'' are congruent if and only if they have the same number of eigenvalues on each open ray from the origin in the complex plane.\n\n==See also==\n*[[Metric signature]]\n*[[Morse theory]]\n*[[Cholesky decomposition]]\n*[[Haynsworth inertia additivity formula]]\n\n==References==\n{{reflist}}\n* {{cite book | last=Garling | first=D. J. H. | title=Clifford algebras. An introduction | series=London Mathematical Society Student Texts | volume=78 | location=Cambridge | publisher=[[Cambridge University Press]] | year=2011 | isbn=978-1-107-09638-7 | zbl=1235.15025 }}\n\n==External links==\n*[http://planetmath.org/encyclopedia/SylvestersLaw.html Sylvester's law] on [[PlanetMath]].\n*[http://www.njohnston.ca/2009/08/sylvesters-law-of-inertia-and-congruence/ Sylvester's law of inertia and *-congruence]\n\n[[Category:Linear algebra]]\n[[Category:Matrix theory]]\n[[Category:Quadratic forms]]\n[[Category:Theorems in algebra]]"
    },
    {
      "title": "Weil's conjecture on Tamagawa numbers",
      "url": "https://en.wikipedia.org/wiki/Weil%27s_conjecture_on_Tamagawa_numbers",
      "text": "In [[mathematics]], the '''Weil conjecture on Tamagawa numbers''' is the statement that the [[Tamagawa number]] <math>\\tau(G)</math> of a [[simply connected]] simple [[algebraic group]] defined over a number field is 1. In this case, ''simply connected'' means \"not having a proper ''algebraic'' covering\" in the algebraic [[group theory]] sense, which is not always the [[simply connected space|topologists' meaning]].\n\n==History==\n{{harvs|txt|authorlink=André Weil|last=Weil|year=1959}} calculated the Tamagawa number in many cases of [[classical group]]s and observed that it is an integer in all considered cases and that it was equal to 1 in the cases when the group is simply connected. The first observation does not hold for all groups: {{harvtxt|Ono|1963}} found examples  where the Tamagawa numbers are not integers. The second observation, that the Tamagawa numbers of simply connected semisimple groups seem to be 1, became known as the Weil conjecture.\n\n[[Robert Langlands]] (1966) introduced [[harmonic analysis]] methods to show it for [[Chevalley group]]s. K. F. Lai (1980) extended the class of known cases to [[quasisplit reductive group]]s. {{harvtxt|Kottwitz|1988}} proved it for all groups satisfying the [[Hasse principle]], which at the time was known for all groups without  [[E8 (group)|''E''<sub>8</sub>]] factors. V. I. Chernousov (1989) removed this restriction, by proving the Hasse principle for the resistant ''E''<sub>8</sub> case (see [[strong approximation in algebraic groups]]), thus completing the proof of Weil's conjecture. In 2011, [[Jacob Lurie]] and [[Dennis Gaitsgory]] announced a proof of the conjecture for algebraic groups over function fields over finite fields.{{sfn|Lurie|2014}}\n\n==Applications==\n{{harvtxt|Ono|1965}} used the Weil conjecture to calculate the Tamagawa numbers of all semisimple algebraic groups.\n\nFor [[spin group]]s, the conjecture implies the known [[Smith–Minkowski–Siegel mass formula]].{{sfn|Lurie|2014}}\n\n==See also==\n*[[Tamagawa number]]\n\n==References==\n{{reflist}}\n*{{Springer|id=T/t092060|title=Tamagawa number}}\n*{{citation|last= Chernousov|first= V. I. |title=The Hasse principle for groups of type E8 |journal=  Soviet Math. Dokl. |volume= 39  |year=1989|pages= 592–596|mr= 1014762}}\n*{{citation|last= Kottwitz|first= Robert E. |title=Tamagawa numbers |journal=  Ann. of Math. |series=   2 |volume= 127  |year=1988|issue=  3|pages=629–646|doi=10.2307/2007007|jstor=2007007|publisher=Annals of Mathematics|mr= 0942522}}.\n*{{citation|last=Lai|first= K. F. |title=Tamagawa number of reductive algebraic groups|journal= Compositio Mathematica|volume= 41 |issue= 2 |year=1980|pages= 153–188 |url= http://www.numdam.org/item?id=CM_1980__41_2_153_0|mr=581580}}\n*{{citation |last=Langlands|first= R. P. |chapter=The volume of the fundamental domain for some arithmetical subgroups of Chevalley groups|year=  1966 |title= Algebraic Groups and Discontinuous Subgroups |series=Proc. Sympos. Pure Math.|pages=  143–148 |publisher=Amer. Math. Soc.|publication-place= Providence, R.I. |mr=0213362}}\n*{{Citation | last1=Ono | first1=Takashi | authorlink=Takashi Ono (mathematician) | title=On the Tamagawa number of algebraic tori | jstor=1970502 |mr=0156851 | year=1963 | journal=[[Annals of Mathematics]] |series=Second Series | issn=0003-486X | volume=78 | pages=47–73 | doi=10.2307/1970502}}\n*{{Citation | last1=Ono | first1=Takashi | title=On the relative theory of Tamagawa numbers | jstor=1970563 |mr=0177991 | year=1965 | journal=[[Annals of Mathematics]] |series=Second Series | issn=0003-486X | volume=82 | pages=88–111 | doi=10.2307/1970563}}\n*{{Citation | last1=Tamagawa | first1=Tsuneo | title=Algebraic Groups and Discontinuous Subgroups | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Proc. Sympos. Pure Math. |mr=0212025 | year=1966 | volume=IX | chapter=Adèles | pages=113–121}}\n*{{citation|first=V. E.|last= Voskresenskii|title=Algebraic Groups and their Birational Invariants|series= AMS translation|year= 1991}}\n*{{Citation | last1=Weil | first1=André | author1-link=André Weil | title=Exp. No. 186, Adèles et groupes algébriques | url=http://www.numdam.org/item?id=SB_1958-1960__5__249_0 | series=Séminaire Bourbaki | year=1959 | volume=5 | pages=249–257}}\n*{{Citation | last1=Weil | first1=André | author1-link=André Weil | title=Adeles and algebraic groups | origyear=1961 | url=https://books.google.com/books/about/Adeles_and_algebraic_groups.html?id=vQvvAAAAMAAJ | publisher=Birkhäuser Boston | location=Boston, MA | series=Progress in Mathematics | isbn=978-3-7643-3092-7 |mr=670072 | year=1982 | volume=23}}\n*{{Citation | last=Lurie | first=Jacob  | author-link=Jacob Lurie | title=Tamagawa Numbers via Nonabelian Poincaré Duality | year=2014 | url=http://www.math.harvard.edu/~lurie/282y.html }}\n\n== Further reading ==\n*Aravind Asok, Brent Doran and Frances Kirwan, [https://arxiv.org/pdf/0801.4733v1.pdf \"Yang-Mills theory and Tamagawa Numbers: the fascination of unexpected links in mathematics\"], February 22, 2013\n*J. Lurie, [http://www.cornell.edu/video/jacob-lurie-the-siegel-mass-formula The Siegel Mass Formula, Tamagawa Numbers, and Nonabelian Poincaré Duality] posted June 8, 2012.\n\n[[Category:Conjectures]]\n[[Category:Theorems in algebra]]\n[[Category:Algebraic groups]]\n[[Category:Diophantine geometry]]"
    },
    {
      "title": "Weinstein–Aronszajn identity",
      "url": "https://en.wikipedia.org/wiki/Weinstein%E2%80%93Aronszajn_identity",
      "text": "In [[mathematics]], the '''Weinstein–Aronszajn identity'''  states that if <math>A</math> and <math>B</math> are matrices of size {{math|''m'' × ''n''}} and {{math|''n'' × ''m''}} respectively (either or both of which may be infinite) then,\nprovided <math>AB</math> is of [[trace class]] (and hence, so is <math>BA</math>),\n\n:<math>\\det(I_m + AB) = \\det(I_n + BA),</math>\n\nwhere <math> I_k </math> is the [[identity matrix]] of order {{math|''k''}}.\n\nIt is closely related to the [[Matrix determinant lemma]] and its generalization. It is the determinant analogue of the [[Woodbury matrix identity]] for matrix inverses.\n\n==Proof==\nThe identity may be proved as follows.<ref>{{citation|title=An Introduction to Grids, Graphs, and Networks|first=C.|last=Pozrikidis|publisher=Oxford University Press|year=2014|isbn=9780199996735|page=271|url=https://books.google.com/books?id=Ws_RAgAAQBAJ&pg=PA271}}</ref> \nLet <math> M</math> be a matrix comprising the four blocks <math> I_m </math>, <math> {-A} </math>, <math> B </math> and <math> I_n </math>.\n\n:<math>M = \\begin{pmatrix}I_m & -A \\\\ B & I_n \\end{pmatrix}. </math>\n\nBecause {{math|''I''<sub>''m''</sub>}} is [[Invertible matrix|invertible]], the formula for the determinant of a block matrix gives\n\n:<math>\\det\\begin{pmatrix}I_m& -A\\\\ B& I_n\\end{pmatrix} = \\det(I_m) \\det(I_n - B I_m^{-1} (-A)) = \\det(I_n + BA). </math>\n\nBecause {{math|'''I<sub>n</sub>'''}} is invertible, the formula for the determinant of a block matrix gives\n\n:<math>\\det\\begin{pmatrix}I_m& -A\\\\ B& I_n\\end{pmatrix} = \\det(I_n) \\det(I_m - (-A) I_n^{-1} B) = \\det(I_m + AB).</math>\n\nThus\n:<math>\\det(I_n + B A) = \\det(I_m + A B).</math>\n\n==Applications==\nThis identify is useful in developing a [[Bayes estimator]] for [[multivariate Gaussian distribution]]s.\n\nThe identity also finds applications in [[random matrix theory]] by relating determinants of large matrices to determinants of smaller ones.<ref>{{cite web|url=http://terrytao.wordpress.com/2010/12/17/the-mesoscopic-structure-of-gue-eigenvalues/ |title=The mesoscopic structure of GUE eigenvalues &#124; What's new |website=Terrytao.wordpress.com |date= |accessdate=2016-01-16}}</ref>\n\n==References==\n{{reflist}}\n\n[[Category:Determinants]]\n[[Category:Matrix theory]]\n[[Category:Linear algebra]]\n[[Category:Theorems in algebra]]\n\n\n{{linear-algebra-stub}}"
    },
    {
      "title": "Witt's theorem",
      "url": "https://en.wikipedia.org/wiki/Witt%27s_theorem",
      "text": ":''\"Witt's theorem\" or \"the Witt theorem\" may also refer to the [[Bourbaki–Witt theorem|Bourbaki–Witt fixed point theorem]] of order theory.''\n\nIn mathematics, '''Witt's theorem''', named after [[Ernst Witt]], is a basic result in the algebraic theory of [[quadratic form]]s:  any [[Isometry (quadratic forms)|isometry]] between two subspaces of a nonsingular [[quadratic space]] over a [[field (algebra)|field]] ''k'' may be extended to an isometry of the whole space.  An analogous statement holds also for skew-symmetric, Hermitian and skew-Hermitian [[bilinear form]]s over arbitrary fields. The theorem applies to classification of quadratic forms over ''k'' and in particular allows one to define the [[Witt group]] ''W''(''k'') which describes the \"stable\" theory of quadratic forms over the field ''k''.\n\n== Statement of the theorem ==\n\nLet {{nowrap|(''V'', ''b'')}} be a finite-dimensional vector space over a [[field (algebra)|field]] ''k'' of characteristic different from 2 together with a non-degenerate symmetric or skew-symmetric [[bilinear form]]. If {{nowrap|''f'' : ''U'' → ''U''′}} is an [[isometry]] between two subspaces of ''V'' then ''f'' extends to an isometry of ''V''.\n\nWitt's theorem implies that the dimension of a maximal [[totally isotropic|totally isotropic subspace]] (null space) of ''V'' is an invariant, called the '''index''' or '''{{visible anchor|Witt index}}''' of ''b'',{{sfn|Lam|2005|p=12}} and moreover, that the [[isometry group]] of {{nowrap|(''V'', ''b'')}} [[Group action (mathematics)|acts]] transitively on the set of maximal isotropic subspaces. This fact plays an important role in the structure theory and [[group representation|representation theory]] of the isometry group and in the theory of [[reductive dual pair]]s.\n\n== Witt's cancellation theorem ==\n\nLet {{nowrap|(''V'', ''q'')}}, {{nowrap|(''V''<sub>1</sub>, ''q''<sub>1</sub>)}}, {{nowrap|(''V''<sub>2</sub>, ''q''<sub>2</sub>)}} be three quadratic spaces over a field ''k''. Assume that\n\n: <math> (V_1,q_1)\\oplus(V,q) \\simeq (V_2,q_2)\\oplus(V,q).</math>\n\nThen the quadratic spaces {{nowrap|(''V''<sub>1</sub>, ''q''<sub>1</sub>)}} and {{nowrap|(''V''<sub>2</sub>, ''q''<sub>2</sub>)}} are isometric:\n\n: <math> (V_1,q_1)\\simeq (V_2,q_2).</math>\n\nIn other words, the direct summand {{nowrap|(''V'', ''q'')}} appearing in both sides  of an isomorphism between quadratic spaces may be \"cancelled\".\n\n== Witt's decomposition theorem ==\n\nLet {{nowrap|(''V'', ''q'')}} be a quadratic space over a field ''k''. Then\nit admits a '''Witt decomposition''':\n\n: <math>(V,q)\\simeq (V_0,0)\\oplus(V_a, q_a)\\oplus (V_h,q_h),</math>\n\nwhere {{nowrap|1=''V''<sub>0</sub> = ker ''q''}} is the [[Radical of a quadratic space|radical]] of ''q'', {{nowrap|(''V''<sub>''a''</sub>, ''q''<sub>''a''</sub>)}} is an [[anisotropic quadratic space]] and {{nowrap|(''V''<sub>''h''</sub>, ''q''<sub>''h''</sub>)}} is a [[split quadratic space]]. Moreover, the anisotropic summand, termed the '''core form''', and the hyperbolic summand in a Witt decomposition of {{nowrap|(''V'', ''q'')}} are determined uniquely up to isomorphism.{{sfn|Lorenz|2008|p=30}}\n\nQuadratic forms with the same core form are said to be ''similar'' or '''Witt equivalent'''.\n\n== Citations ==\n{{reflist}}\n\n== References ==\n* [[Emil Artin]] (1957) [[Geometric Algebra]], page 121\n* {{citation | first=Tsit-Yuen | last=Lam | authorlink=Tsit Yuen Lam | year=2005 | title=Introduction to Quadratic Forms over Fields | volume=67 | series=[[Graduate Studies in Mathematics]] | publisher=American Mathematical Society | isbn=0-8218-1095-2 | zbl=1068.11023 | mr=2104929 }}\n* {{citation | first=Falko | last=Lorenz | year=2008 | title=Algebra. Volume II: Fields with Structure, Algebras and Advanced Topics | publisher=[[Springer-Verlag]] | isbn=978-0-387-72487-4 | pages=15–27 | zbl=1130.12001 }}\n* {{citation | first=O. Timothy | last=O'Meara | authorlink=O. Timothy O'Meara | year=1973 | title=Introduction to Quadratic Forms | publisher=[[Springer-Verlag]] | series=Die Grundlehren der mathematischen Wissenschaften | volume=117 | zbl=0259.10018 }}\n\n\n[[Category:Theorems in algebra]]\n[[Category:Quadratic forms]]"
    },
    {
      "title": "Woodbury matrix identity",
      "url": "https://en.wikipedia.org/wiki/Woodbury_matrix_identity",
      "text": "{{Use American English|date = January 2019}}\n{{Short description|Theorem of matrix ranks}}\nIn [[mathematics]] (specifically [[linear algebra]]), the '''Woodbury matrix identity''', named after Max A. Woodbury<ref>Max A. Woodbury, ''Inverting modified matrices'', Memorandum Rept. 42, Statistical Research Group, Princeton University, Princeton, NJ, 1950, 4pp {{MR|38136}}</ref><ref>Max A. Woodbury, ''The Stability of Out-Input Matrices''. Chicago, Ill., 1949. 5 pp. {{MR|32564}}</ref> says that the inverse of a rank-''k'' correction of some [[matrix (mathematics)|matrix]] can be computed by doing a rank-''k'' correction to the inverse of the original matrix. Alternative names for this formula are the '''matrix inversion lemma''', '''Sherman–Morrison–Woodbury formula''' or just '''Woodbury formula'''. However, the identity appeared in several papers before the Woodbury report.<ref name=\"hager\">{{cite journal\n |first=William W. |last=Hager\n |title=Updating the inverse of a matrix\n |journal=SIAM Review\n |volume=31 |year=1989 |pages=221&ndash;239 |issue=2\n |doi=10.1137/1031049 |mr=997457 | jstor = 2030425\n}}</ref>\n\nThe Woodbury matrix identity is<ref name=\"higham\">{{Cite book | last1=Higham | first1=Nicholas | author1-link=Nicholas Higham | title=Accuracy and Stability of Numerical Algorithms | publisher=[[Society for Industrial and Applied Mathematics|SIAM]] | edition=2nd | isbn=978-0-89871-521-7 | year=2002 | page=258 |mr=1927606 | postscript=<!--None--> }}\n</ref>\n:<math> \\left(A + UCV \\right)^{-1} = A^{-1} - A^{-1}U \\left(C^{-1} + VA^{-1}U \\right)^{-1} VA^{-1}, </math>\n\nwhere ''A'', ''U'', ''C'' and ''V'' all denote matrices of the correct ([[conformable matrix|conformable]]) sizes.  Specifically, ''A'' is ''n''-by-''n'', ''U'' is ''n''-by-''k'', ''C'' is ''k''-by-''k'' and ''V'' is ''k''-by-''n''. This can be derived using [[Invertible matrix#Blockwise inversion|blockwise matrix inversion]].\n\nFor a more general formula for which the matrix ''C'' need not be [[invertible matrix|invertible]] or even [[square matrix|square]], see [[#Binomial inverse theorem|Binomial inverse theorem]].\n\n== Special cases ==\nWhen ''C'' is the 1-by-1 unit matrix, this identity reduces to the [[Sherman–Morrison formula]]. In the case when ''C'' is the identity matrix ''I'', the matrix <math>I+VA^{-1}U</math> is known in [[numerical linear algebra]] and [[numerical partial differential equations]] as the '''capacitance matrix'''.<ref name=\"hager\"/>  For mere numbers, the identity reduces to the following relation of fractions\n: <math>\\frac{1}{a + c} = \\frac{1}{a} - \\left(\\frac{1}{a}\\right)^2\\cdot\\left(\\frac{1}{c} + \\frac{1}{a}\\right)^{-1}.</math>\n\n== Push-through identity ==\n\nMultiplying on the right by <math>U</math> and simplifying yields the following '''push-through identity'''<ref name=\"HS\"/> \n: <math>\\left(A + UCV \\right)^{-1} U = A^{-1} U \\left(C^{-1} + V A^{-1} U \\right)^{-1} C^{-1}.</math>\n\nThe special case where <math> A = aI </math> and <math> C = cI </math> (where these two identity matrices have different sizes in general) illustrates the origin of the \"push-through\" name:\n: <math>\\left(aI + cUV\\right)^{-1} U = U \\left(aI + cVU\\right)^{-1}.</math>\n\n== Direct proof ==\nThe formula can be proven by checking that <math>(A + UCV)</math> times its alleged inverse on the right side of the Woodbury identity gives the identity matrix:\n\n: <math>\\begin{align}\n      & \\left(A + UCV \\right) \\left[ A^{-1} - A^{-1}U \\left(C^{-1} + VA^{-1}U \\right)^{-1} VA^{-1} \\right] \\\\\n  ={} & \\left\\{ I - U\\left(C^{-1} + VA^{-1}U \\right)^{-1}VA^{-1} \\right\\} + \\left\\{ UCVA^{-1} - UCVA^{-1}U \\left(C^{-1} + VA^{-1}U \\right)^{-1} VA^{-1} \\right\\} \\\\\n  ={} & \\left\\{ I + UCVA^{-1} \\right\\} - \\left\\{ U\\left(C^{-1} + VA^{-1}U \\right)^{-1}VA^{-1} + UCVA^{-1}U \\left(C^{-1} + VA^{-1}U \\right)^{-1} VA^{-1} \\right\\} \\\\\n  ={} & I + UCVA^{-1} - \\left(U + UCVA^{-1}U\\right) \\left(C^{-1} + VA^{-1}U\\right)^{-1}VA^{-1} \\\\\n  ={} & I + UCVA^{-1} - UC \\left(C^{-1} + VA^{-1}U\\right) \\left(C^{-1} + VA^{-1}U\\right)^{-1}VA^{-1} \\\\\n  ={} & I + UCVA^{-1} - UCVA^{-1} \\\\\n  ={} & I.\n\\end{align}</math>\n\n== Algebraic proof ==\nFirst consider these useful identities,\n: <math>\\begin{align}\n               U + UCV A^{-1} U &= \n               UC \\left(C^{-1} + V A^{-1} U\\right) = \\left(A + UCV\\right) A^{-1} U \\\\\n  \\left(A + UCV\\right)^{-1} U C &= A^{-1}U \\left(C^{-1} + VA^{-1} U\\right) ^{-1}\n\\end{align}\n</math>\n\nNow, \n: <math>\\begin{align}\n  A^{-1} &= \\left(A + UCV\\right)^{-1}\\left(A + UCV\\right) A^{-1}\\\\\n         &= \\left(A + UCV\\right)^{-1}\\left(I + UCVA^{-1}\\right) \\\\\n         &= \\left(A + UCV\\right)^{-1} + \\left(A + UCV\\right)^{-1} UCVA^{-1} \\\\ \n         &= \\left(A + UCV\\right)^{-1} + A^{-1} U \\left(C^{-1} + VA^{-1}U \\right)^{-1} VA^{-1}.\n\\end{align}</math>\n\n== Derivation via blockwise elimination ==\nDeriving the Woodbury matrix identity is easily done by solving the following block matrix inversion problem\n:<math>\n  \\begin{bmatrix} A & U \\\\ V & -C^{-1} \\end{bmatrix}\\begin{bmatrix} X \\\\ Y \\end{bmatrix} = \\begin{bmatrix} I \\\\ 0 \\end{bmatrix}.\n</math>\n\nExpanding, we can see that the above reduces to \n\n:<math>\\begin{cases} AX + UY = I \\\\ VX - C^{-1}Y = 0\\end{cases}</math>\n\nwhich is equivalent to <math>(A + UCV)X = I</math>. Eliminating the first equation, we find that <math>X = A^{-1}(I - UY)</math>, which can be substituted into the second to find <math>VA^{-1}(I - UY) = C^{-1}Y</math>. Expanding and rearranging, we have <math>VA^{-1} = \\left(C^{-1} + VA^{-1}U\\right)Y</math>, or <math>\\left(C^{-1} + VA^{-1}U\\right)^{-1}VA^{-1} = Y</math>. Finally, we substitute into our <math>AX + UY = I</math>, and we have <math>AX + U\\left(C^{-1} + VA^{-1}U\\right)^{-1}VA^{-1} = I</math>. Thus,\n:<math>(A + UCV)^{-1} = X = A^{-1} - A^{-1}U\\left(C^{-1} + VA^{-1}U\\right)^{-1}VA^{-1}.</math>\n\nWe have derived the Woodbury matrix identity.\n\n== Derivation from LDU decomposition ==\n\nWe start by the matrix\n:<math>\\begin{bmatrix} A & U \\\\ V & C \\end{bmatrix}</math>\nBy eliminating the entry under the ''A'' (given that ''A'' is invertible) we get\n\n:<math>\\begin{bmatrix} I & 0 \\\\ -VA^{-1} & I \\end{bmatrix} \n\\begin{bmatrix} A & U \\\\ V & C \\end{bmatrix} = \\begin{bmatrix} A & U \\\\ 0 & C - VA^{-1}U \\end{bmatrix}\n</math>\n\nLikewise, eliminating the entry above ''C'' gives\n\n:<math>\\begin{bmatrix} A & U \\\\ V & C \\end{bmatrix} \\begin{bmatrix} I & -A^{-1}U \\\\ 0 & I \\end{bmatrix} \n= \\begin{bmatrix} A & 0 \\\\ V & C-VA^{-1}U \\end{bmatrix}\n</math>\n\nNow combining the above two, we get\n\n:<math>\n  \\begin{bmatrix} I & 0 \\\\ -VA^{-1} & I \\end{bmatrix} \\begin{bmatrix} A & U \\\\ V & C \\end{bmatrix}\\begin{bmatrix} I & -A^{-1}U \\\\ 0 & I \\end{bmatrix} =\n  \\begin{bmatrix} A & 0 \\\\ 0 & C - VA^{-1}U \\end{bmatrix}\n</math>\n\nMoving to the right side gives\n\n:<math>\\begin{bmatrix} A & U \\\\ V & C \\end{bmatrix} = \\begin{bmatrix} I & 0 \\\\ VA^{-1} & I \\end{bmatrix} \\begin{bmatrix} A & 0 \\\\ 0 & C - VA^{-1}U \\end{bmatrix} \\begin{bmatrix} I & A^{-1}U \\\\ 0 & I \\end{bmatrix}</math>\n\nwhich is the LDU decomposition of the block matrix into an upper triangular, diagonal, and lower triangular matrices.\n\nNow inverting both sides gives\n\n: <math>\\begin{align}\n  \\begin{bmatrix} A & U \\\\ V & C \\end{bmatrix}^{-1} \n    &= \\begin{bmatrix} I & A^{-1}U \\\\ 0 & I \\end{bmatrix}^{-1} \\begin{bmatrix} A & 0 \\\\ 0 & C - VA^{-1}U \\end{bmatrix}^{-1} \\begin{bmatrix} I & 0 \\\\ VA^{-1} & I \\end{bmatrix}^{-1} \\\\[8pt]\n    &= \\begin{bmatrix} I & -A^{-1}U \\\\ 0 & I \\end{bmatrix} \\begin{bmatrix} A^{-1} & 0 \\\\ 0 & \\left(C - VA^{-1}U\\right)^{-1} \\end{bmatrix} \\begin{bmatrix} I & 0 \\\\ -VA^{-1} & I \\end{bmatrix} \\\\[8pt]\n    &= \\begin{bmatrix} A^{-1} + A^{-1}U\\left(C - VA^{-1}U\\right)^{-1}VA^{-1} & -A^{-1}U\\left(C - VA^{-1}U\\right)^{-1} \\\\ -\\left(C - VA^{-1}U\\right)^{-1}VA^{-1} & \\left(C - VA^{-1}U\\right)^{-1} \\end{bmatrix}  \\qquad\\mathrm{(1)}\n\\end{align}</math>\n\nWe could equally well have done it the other way (provided that ''C'' is invertible) i.e.\n\n: <math>\\begin{bmatrix} A & U \\\\ V & C \\end{bmatrix} = \\begin{bmatrix} I & UC^{-1} \\\\ 0 & I \\end{bmatrix} \\begin{bmatrix} A - UC^{-1}V & 0 \\\\ 0 & C \\end{bmatrix} \\begin{bmatrix} I & 0 \\\\ C^{-1}V  & I\\end{bmatrix}</math>\n\nNow again inverting both sides,\n: <math>\\begin{align}\n  \\begin{bmatrix} A & U \\\\ V & C \\end{bmatrix}^{-1}\n    &= \\begin{bmatrix} I & 0 \\\\ C^{-1}V  & I\\end{bmatrix}^{-1} \\begin{bmatrix} A - UC^{-1}V & 0 \\\\ 0 & C \\end{bmatrix}^{-1} \\begin{bmatrix} I & UC^{-1} \\\\ 0 & I \\end{bmatrix}^{-1} \\\\[8pt]\n    &= \\begin{bmatrix} I & 0 \\\\ -C^{-1}V  & I\\end{bmatrix} \\begin{bmatrix} \\left(A - UC^{-1}V\\right)^{-1} & 0 \\\\ 0 & C^{-1} \\end{bmatrix} \\begin{bmatrix} I & -UC^{-1} \\\\ 0 & I \\end{bmatrix} \\\\[8pt]\n    &= \\begin{bmatrix} \\left(A - UC^{-1}V\\right)^{-1} & -\\left(A - UC^{-1}V\\right)^{-1}UC^{-1} \\\\ -C^{-1}V\\left(A - UC^{-1}V\\right)^{-1} & C^{-1}V\\left(A - UC^{-1}V\\right)^{-1}UC^{-1} + C^{-1} \\end{bmatrix} \\qquad\\mathrm{(2)}\n\\end{align}</math>\n\nNow comparing elements (1, 1) of the RHS of (1) and (2) above gives the Woodbury formula\n:<math>\\left(A - UC^{-1}V\\right)^{-1} = A^{-1} + A^{-1}U\\left(C - VA^{-1}U\\right)^{-1}VA^{-1}.</math>\n\n== Applications ==\n\nThis identity is useful in certain numerical computations where ''A''<sup>&minus;1</sup> has already been computed and it is desired to compute (''A''&nbsp;+&nbsp;''UCV'')<sup>&minus;1</sup>.  With the inverse of ''A'' available, it is only necessary to find the inverse of ''C''<sup>−1</sup>&nbsp;+&nbsp;''VA''<sup>−1</sup>''U'' in order to obtain the result using the right-hand side of the identity.  If ''C'' has a much smaller dimension than ''A'', this is more efficient than inverting ''A''&nbsp;+&nbsp;''UCV'' directly. A common case is finding the inverse of a low-rank update ''A''&nbsp;+&nbsp;''UCV'' of ''A'' (where ''U'' only has a few columns and ''V'' only a few rows), or finding an approximation of the inverse of the matrix ''A''&nbsp;+&nbsp;''B'' where the matrix ''B'' can be approximated by a low-rank matrix ''UCV'', for example using the [[singular value decomposition]].\n\nThis is applied, e.g., in the [[Kalman filter]] and [[recursive least squares]] methods, to replace the [[parametric solution]], requiring inversion of a state vector sized matrix, with a condition equations based solution. In case of the Kalman filter this matrix has the dimensions of the vector of observations, i.e., as small as 1 in case only one new observation is processed at a time. This significantly speeds up the often real time calculations of the filter.\n\n==Binomial inverse theorem==\nThe '''binomial inverse theorem''' is a more general form of the Woodbury matrix identity.\n\nIf '''A''', '''U''', '''B''', '''V''' are matrices of sizes ''p''×''p'', ''p''×''q'', ''q''×''q'', ''q''×''p'', respectively, then\n:<math>\n  \\left(\\mathbf{A} + \\mathbf{UBV}\\right)^{-1} =\n  \\mathbf{A}^{-1} - \\mathbf{A}^{-1}\\mathbf{UB}\\left(\\mathbf{B}+\\mathbf{BVA}^{-1}\\mathbf{UB}\\right)^{-1}\\mathbf{BVA}^{-1}\n</math>\n\nprovided '''A''' and '''B''' + '''BVA'''<sup>−1</sup>'''UB''' are nonsingular. Nonsingularity of the latter requires that '''B'''<sup>−1</sup> exist since it equals {{nowrap|'''B'''('''I''' + '''VA'''<sup>−1</sup>'''UB''')}} and the rank of the latter cannot exceed the rank of '''B'''.<ref name=HS>{{cite journal | last1 = Henderson | first1 = H. V. | last2 = Searle | first2 = S. R. | year = 1981 | title = On deriving the inverse of a sum of matrices | url = http://ecommons.cornell.edu/bitstream/1813/32749/1/BU-647-M.pdf| journal = SIAM Review | volume = 23 | issue = | pages = 53–60 | doi = 10.1137/1023004 | jstor = 2029838 }}</ref>\n\nSince '''B''' is invertible, the two '''B''' terms flanking the parenthetical quantity inverse in the right-hand side can be replaced with {{nowrap|('''B'''<sup>−1</sup>)<sup>−1</sup>,}} which results in\n\n:<math>\n  \\left(\\mathbf{A} + \\mathbf{UBV}\\right)^{-1} =\n  \\mathbf{A}^{-1} - \\mathbf{A}^{-1}\\mathbf{U}\\left(\\mathbf{B}^{-1} + \\mathbf{VA}^{-1}\\mathbf{U}\\right)^{-1}\\mathbf{VA}^{-1}.\n</math>\n\nThis is the Woodbury matrix identity, which can also be derived using [[Invertible matrix#Blockwise inversion|matrix blockwise inversion]].\n\nA more general formula exists when '''B''' is singular and possibly even non-square:<ref name=HS/>\n\n:<math>(\\mathbf{A + UBV})^{-1} = \\mathbf{A}^{-1} - \\mathbf{A}^{-1}\\mathbf{U}(\\mathbf{I + BVA}^{-1}\\mathbf{U})^{-1}\\mathbf{BVA}^{-1}.</math>\n\nFormulas also exist for certain cases in which '''A''' is singular.<ref>Kurt S. Riedel, \"A Sherman–Morrison–Woodbury Identity for Rank Augmenting Matrices with Application to Centering\", ''SIAM Journal on Matrix Analysis and Applications'', 13 (1992)659-662, {{doi|10.1137/0613040}} [http://math.nyu.edu/mfdd/riedel/ranksiam.ps preprint] {{MR|1152773}}</ref>\n\n===Verification===\nFirst notice that \n:<math>\\left(\\mathbf{A} + \\mathbf{UBV}\\right) \\mathbf{A}^{-1}\\mathbf{UB} = \\mathbf{UB} + \\mathbf{UBVA}^{-1}\\mathbf{UB} = \\mathbf{U} \\left(\\mathbf{B} + \\mathbf{BVA}^{-1}\\mathbf{UB}\\right).</math>\n\nNow multiply the matrix we wish to invert by its alleged inverse:\n:<math>\\begin{align}\n      &\\left(\\mathbf{A} + \\mathbf{UBV}\\right) \\left( \\mathbf{A}^{-1} - \\mathbf{A}^{-1}\\mathbf{UB}\\left(\\mathbf{B} + \\mathbf{BVA}^{-1}\\mathbf{UB}\\right)^{-1}\\mathbf{BVA}^{-1} \\right) \\\\\n  ={} &\\mathbf{I}_p + \\mathbf{UBVA}^{-1} - \\mathbf{U} \\left(\\mathbf{B} + \\mathbf{BVA}^{-1}\\mathbf{UB}\\right) \\left(\\mathbf{B} + \\mathbf{BVA}^{-1}\\mathbf{UB}\\right)^{-1}\\mathbf{BVA}^{-1} \\\\\n  ={} &\\mathbf{I}_p + \\mathbf{UBVA}^{-1} - \\mathbf{U BVA}^{-1} = \\mathbf{I}_p\n\\end{align}</math>\n\nwhich verifies that it is the inverse.\n\nSo we get that if '''A'''<sup>−1</sup> and <math>\\left(\\mathbf{B} + \\mathbf{BVA}^{-1}\\mathbf{UB}\\right)^{-1}</math> exist, then <math>\\left(\\mathbf{A} + \\mathbf{UBV}\\right)^{-1}</math> exists and is given by the theorem above.<ref name=\"strang\">{{cite book | author = Gilbert Strang | title = Introduction to Linear Algebra | edition = 3rd | year = 2003 | publisher = Wellesley-Cambridge Press: Wellesley, MA | isbn = 0-9614088-9-8}}</ref>\n\n===Special cases===\n\n====First====\nIf ''p'' = ''q'' and '''U''' = '''V''' = '''I'''<sub>''p''</sub> is the identity matrix, then\n\n:<math>\\left(\\mathbf{A} + \\mathbf{B}\\right)^{-1} = \\mathbf{A}^{-1} - \\mathbf{A}^{-1}\\mathbf{B}\\left(\\mathbf{B} + \\mathbf{BA}^{-1}\\mathbf{B}\\right)^{-1}\\mathbf{BA}^{-1}.</math>\n\nRemembering the identity\n:<math>\\left(\\mathbf{C} \\mathbf{D}\\right)^{-1} = \\mathbf{D}^{-1} \\mathbf{C}^{-1} ,</math>\n\nwe can also express the previous equation in the simpler form as\n:<math>\\left(\\mathbf{A} + \\mathbf{B}\\right)^{-1} = \\mathbf{A}^{-1} - \\mathbf{A}^{-1}\\left(\\mathbf{I} + \\mathbf{B}\\mathbf{A}^{-1}\\right)^{-1}\\mathbf{B}\\mathbf{A}^{-1}.</math>\n\nContinuing with the merging of the terms of the far right-hand side of the above equation results in [[Hua's identity]]\n:<math>\\left(\\mathbf{A} + \\mathbf{B}\\right)^{-1} = \\mathbf{A}^{-1} - \\left(\\mathbf{A} + \\mathbf{A}\\mathbf{B}^{-1}\\mathbf{A}\\right)^{-1}.</math>\n\nAnother useful form of the same identity is\n:<math>\\left(\\mathbf{A} - \\mathbf{B}\\right)^{-1} = \\mathbf{A}^{-1} + \\mathbf{A}^{-1}\\mathbf{B}\\left(\\mathbf{A} - \\mathbf{B}\\right)^{-1},</math>\n\nwhich has a recursive structure that yields\n:<math>\\left(\\mathbf{A} - \\mathbf{B}\\right)^{-1} = \\sum_{k=0}^{\\infty} \\left(\\mathbf{A}^{-1}\\mathbf{B}\\right)^k\\mathbf{A}^{-1}.</math>\n\nThis form can be used in perturbative expansions where '''B''' is a perturbation of '''A'''.\n\n====Second====\n\nIf '''B''' = '''I'''<sub>''q''</sub> is the identity matrix and ''q'' = 1, then '''U''' is a column vector, written '''u''', and '''V''' is a row vector, written '''v'''<sup>T</sup>.  Then the theorem implies the [[Sherman-Morrison formula]]:\n\n:<math>\\left(\\mathbf{A} + \\mathbf{uv}^\\textsf{T}\\right)^{-1} = \\mathbf{A}^{-1} - \\frac{\\mathbf{A}^{-1}\\mathbf{uv}^\\textsf{T}\\mathbf{A}^{-1}}{1+\\mathbf{v}^\\textsf{T}\\mathbf{A}^{-1}\\mathbf{u}}.</math>\n\nThis is useful if one has a matrix '''A''' with a known inverse '''A'''<sup>−1</sup> and one needs to invert matrices of the form '''A''' + '''uv'''<sup>T</sup> quickly for various '''u''' and '''v'''.\n\n====Third====\n\nIf we set '''A''' = '''I'''<sub>''p''</sub> and '''B''' = '''I'''<sub>''q''</sub>, we get \n:<math>\\left(\\mathbf{I}_p + \\mathbf{UV}\\right)^{-1} = \\mathbf{I}_p - \\mathbf{U}\\left(\\mathbf{I}_q + \\mathbf{VU}\\right)^{-1}\\mathbf{V}.</math>\n\nIn particular, if ''q'' = 1, then\n:<math>\\left(\\mathbf{I} + \\mathbf{uv}^\\textsf{T}\\right)^{-1} = \\mathbf{I} - \\frac{\\mathbf{uv}^\\textsf{T}}{1 + \\mathbf{v}^\\textsf{T}\\mathbf{u}},</math>\n\nwhich is a particular case of the Sherman-Morrison formula given above.\n\n==See also==\n*[[Sherman–Morrison formula]]\n*[[Schur complement]]\n*[[Matrix determinant lemma]], formula for a rank-''k'' update to a [[determinant]]\n*[[Invertible matrix]]\n*[[Moore-Penrose pseudoinverse#Updating the pseudoinverse]]\n\n== Notes ==\n{{reflist}}\n*{{Citation|last1=Press|first1=WH|last2=Teukolsky|first2=SA|last3=Vetterling|first3=WT|last4=Flannery|first4=BP|year=2007|title=Numerical Recipes: The Art of Scientific Computing|edition=3rd|publisher=Cambridge University Press| publication-place=New York|isbn=978-0-521-88068-8|chapter=Section 2.7.3. Woodbury Formula|chapter-url=http://apps.nrbook.com/empanel/index.html?pg=80}}\n\n== External links ==\n* [http://www.ee.ic.ac.uk/hp/staff/dmb/matrix/identity.html Some matrix identities]\n* {{MathWorld|title=Woodbury formula|urlname=WoodburyFormula}}\n\n[[Category:Linear algebra]]\n[[Category:Matrices]]\n[[Category:Matrix theory]]\n[[Category:Theorems in algebra]]\n[[Category:Lemmas]]"
    },
    {
      "title": "Cayley–Hamilton theorem",
      "url": "https://en.wikipedia.org/wiki/Cayley%E2%80%93Hamilton_theorem",
      "text": "[[File:Arthur Cayley.jpg|225px|thumb|right|[[Arthur Cayley]], [[Fellow of the Royal Society|F.R.S.]] (1821–1895) is widely regarded as Britain's leading pure mathematician of the 19th century. Cayley in 1848 went to Dublin to attend lectures on [[quaternion]]s by Hamilton, their discoverer. Later Cayley impressed him by being the second to publish work on them.<ref name=Crilly_1>{{harvnb|Crilly|1998}}</ref>\nCayley proved the theorem for matrices of dimension 3 and less,  publishing proof for the two-dimensional case.<ref name=Cayley_1>{{harvnb|Cayley|1858|pp=17–37}}</ref><ref>{{harvnb|Cayley|1889|pp=475–496}}</ref> As for {{math|''n'' × ''n''}} matrices, Cayley stated “..., I have not thought it necessary to undertake the labor of a formal proof\nof the theorem in the general case of a matrix of any degree”.]]\n[[File:William Rowan Hamilton portrait oval combined.png|225px|thumb|right|[[William Rowan Hamilton]] (1805–1865), Irish physicist, astronomer, and mathematician, first foreign member of the American [[National Academy of Sciences]]. While maintaining opposing position about how geometry should be studied, Hamilton always remained on the best terms with Cayley.<ref name=Crilly_1/><br/><br/>Hamilton proved that for a linear function of [[quaternion]]s there exists a certain equation, depending on the linear function, that is satisfied by the linear function itself.<ref name=Hamilton_1864a/><ref name=Hamilton_1864b/><ref name=Hamilton_1862/>]]\n\nIn [[linear algebra]], the '''Cayley–Hamilton theorem''' (named after the mathematicians [[Arthur Cayley]] and [[William Rowan Hamilton]]) states that every [[square matrix]] over a [[commutative ring]] (such as the [[real number|real]] or [[complex number|complex]] [[field (mathematics)|field]]) satisfies its own [[Characteristic polynomial#Characteristic equation|characteristic equation]].\n\nIf {{mvar|A}} is a given {{math|''n''&times;''n''}} matrix and {{math|''I<sub>n</sub>&nbsp;''}} is the  {{math|''n''&times;''n''}} [[identity matrix]], then the  [[characteristic polynomial]] of {{mvar|A}} is defined as<ref>{{harvnb|Atiyah|MacDonald|1969}}</ref>\n::<math>p(\\lambda)=\\det(\\lambda I_n-A)~,</math>\n\nwhere {{math|det}} is the [[determinant]] operation and {{mvar|λ}} is a [[scalar (mathematics)|scalar]] element of the base ring. Since the entries of the matrix are (linear or constant) polynomials in {{mvar|λ}}, the determinant is also an {{mvar|n}}-th order [[monic polynomial]] in {{mvar|λ}}. The Cayley–Hamilton theorem states that substituting the matrix {{mvar|A}} for {{mvar|λ}} in this polynomial results in the [[zero matrix]],\n\n::<math>p(A)= 0.</math>\n\nThe powers of {{mvar|A}}, obtained by substitution from powers of {{mvar|λ}}, are defined by repeated matrix multiplication; the constant term of {{math| ''p''(''λ'')}} gives a multiple of the power {{mvar|A}}<sup>0</sup>, which is defined as the identity matrix.\nThe theorem allows {{mvar|A}}<sup>{{mvar|n}}</sup> to be expressed as a linear combination of the lower matrix powers of {{mvar|A}}. When the ring is a field, the Cayley–Hamilton theorem is equivalent to the statement that the [[Minimal polynomial (linear algebra)|minimal polynomial]] of a square matrix [[Polynomial division|divides]] its characteristic polynomial.\n\nThe theorem was first proved in 1853<ref name=Hamilton_1853>{{harvnb|Hamilton|1853|p=562}}</ref> in terms of inverses of linear functions of [[quaternion]]s, a ''non-commutative'' ring, by Hamilton.<ref name=Hamilton_1864a>{{harvnb|Hamilton|1864a}}</ref><ref name=Hamilton_1864b>{{harvnb|Hamilton|1864b}}</ref><ref name=Hamilton_1862>{{harvnb|Hamilton|1862}}</ref> This corresponds to the special case of certain {{math|4 × 4}} real or {{math|2 × 2}} complex matrices. The theorem holds for general quaternionic matrices.<ref>{{harvnb|Zhang|1997}}</ref><ref group=nb>Due to the non-commutative nature of the multiplication operation for quaternions and related constructions, care needs to be taken with definitions, most notably in this context, for the determinant. The theorem holds as well for the slightly less well-behaved [[split-quaternion]]s, see {{harvtxt|Alagös|Oral|Yüce|2012}}. The rings of quaternions and split-quaternions can both be represented by certain {{math|2 × 2}} complex matrices. (When restricted to unit norm, these are the groups {{math|SU(2)}} and {{math|SU(1, 1)}} respectively.) Therefore it is not surprising that the theorem holds.<br><br><br>There is no such matrix representation for the [[octonion]]s, since the multiplication operation is not associative in this case. However, a modified Cayley–Hamilton theorem still holds for the octonions, see {{harvtxt|Tian|2000}}.</ref> Cayley in 1858 stated it for {{math|3 × 3}} and smaller matrices, but only published a proof for the {{math|2 × 2}} case.<ref name=Cayley_1/> The general case was first proved by [[Ferdinand Georg Frobenius|Frobenius]] in 1878.<ref name=\"Frobenius 1878\">{{harvnb|Frobenius|1878}}</ref>\n\n== Examples ==\n\n=== {{math|1×1}} matrices ===\n\nFor a {{math|1×1}} matrix {{math|''A''&nbsp;{{=}}&nbsp;(''a''<sub>1,1</sub>)}}, the characteristic polynomial is given by {{math|''p''(λ)&nbsp;{{=}}&nbsp;''λ''&nbsp;−&nbsp;''a''}}, and so {{math|''p''(''A'')&nbsp;{{=}}&nbsp;(''a'')&nbsp;−&nbsp;''a''<sub>1,1</sub>&nbsp;{{=}}&nbsp;0}} is obvious.\n\n=== {{math|2×2}} matrices ===\n\nAs a concrete example, let\n:<math>A = \\begin{pmatrix}1&2\\\\3&4\\end{pmatrix}.</math>\nIts characteristic polynomial is given by\n:<math>p(\\lambda)=\\det(\\lambda I_2-A)=\\det\\begin{pmatrix}\\lambda-1&-2\\\\\n-3&\\lambda-4\\end{pmatrix}=(\\lambda-1)(\\lambda-4)-(-2)(-3)=\\lambda^2-5\\lambda-2.</math>\n\nThe Cayley–Hamilton theorem claims that, if we ''define''\n:<math>p(X)=X^2-5X-2I_2,</math>\nthen\n:<math>p(A)=A^2-5A-2I_2=\\begin{pmatrix}0&0\\\\0&0\\\\\\end{pmatrix}.</math>\nWe can verify by computation that indeed,\n:<math>A^2-5A-2I_2=\\begin{pmatrix}7&10\\\\15&22\\\\\\end{pmatrix}-\\begin{pmatrix}5&10\\\\15&20\\\\\\end{pmatrix}-\\begin{pmatrix}2&0\\\\0&2\\\\\\end{pmatrix}=\\begin{pmatrix}0&0\\\\0&0\\\\\\end{pmatrix}.</math>\n\nFor a generic {{math|2×2}} matrix,\n:<math>A=\\begin{pmatrix}a&b\\\\c&d\\\\\\end{pmatrix} ,</math>\n\nthe characteristic polynomial is given by {{math| ''p''(''λ'')&nbsp;{{=}}&nbsp;''λ''<sup>2</sup>&nbsp;−&nbsp;(''a''&nbsp;+&nbsp;''d'')''λ''&nbsp;+&nbsp;(''ad''&nbsp;−&nbsp;''bc'')}}, so the Cayley–Hamilton theorem states that\n:<math>p(A)=A^2-(a+d)A+(ad-bc)I_2=\\begin{pmatrix}0&0\\\\0&0\\\\\\end{pmatrix};</math>\nwhich is indeed always the case, evident by working out the entries of {{mvar|A}}<sup>2</sup>.\n\n== Applications==\n\n===Determinant and inverse matrix===\n{{see also|Determinant#Relation to eigenvalues and trace|Characteristic polynomial#Properties}}\nFor a general {{math|''n''×''n''}} [[invertible matrix]] {{mvar|A}}, i.e., one with nonzero determinant, {{mvar|A}}<sup>−1</sup> can thus be written as an {{nowrap|{{math|(''n''&nbsp;−&nbsp;1)}}-th}} order  [[polynomial expression]] in {{mvar|A}}:   As indicated,  the Cayley–Hamilton theorem amounts to  the identity {{Equation box 1\n|indent =::\n|equation =  <math>p(A)=A^n+c_{n-1}A^{n-1}+\\cdots+c_1A+(-1)^n\\det(A)I_n =O.</math>\n|cellpadding= 6\n|border\n|border colour = #0073CF\n|bgcolor=#F9FFF7}}\n\nThe coefficients {{math|''c''<sub>''i''</sub>}} are given by the [[elementary symmetric polynomials]] of the eigenvalues of {{mvar|A}}. Using [[Newton identities]], the elementary symmetric polynomials can in turn be expressed in terms of [[power sum symmetric polynomial]]s of the eigenvalues: \n:<math>s_k = \\sum_{i=1}^n \\lambda_i^k = \\operatorname{tr}(A^k),</math> \nwhere {{math|tr (''A''<sup>''k''</sup>)}} is the [[Trace (linear algebra)|trace]] of the matrix {{mvar|A<sup>''k''</sup>}}. Thus, we can express {{math|''c''<sub>''i''</sub>}} in terms of the trace of powers of {{mvar|A}}.\n\nIn general, the formula for the coefficients {{math|''c''<sub>''i''</sub>}} is given in terms of complete exponential [[Bell polynomials]] as  <ref group=nb>An explicit expression for these coefficients is\n:<math>c_i = \\sum_{ k_1,k_2,\\ldots ,k_n}\\prod_{l=1}^{n} \\frac{(-1)^{k_l+1}}{l^{k_l} k_l!}\\operatorname{tr}(A^l)^{k_l},</math>\nwhere the sum is taken over the sets of all integer partitions {{math|''k''<sub>''l''</sub> ≥ 0}} satisfying the equation \n:<math>\\sum_{l=1}^{n}lk_{l} = n-i.</math></ref> \n:<math> c_{n-k} = \\frac{(-1)^{k}}{k!} B_k(s_1, -1! s_2, 2! s_3, \\ldots, (-1)^{k-1}(k-1)! s_k).</math>\n\nIn particular, the determinant of {{math|''A''}} corresponds to  {{math|''c''<sub>0</sub>}}. Thus, the determinant can be written as a [[trace identity]]\n:<math>\\det(A) = \\frac{1}{n!} B_n(s_1, -1! s_2, 2! s_3, \\ldots, (-1)^{n-1}(n-1)! s_n).</math>\n\nLikewise, the characteristic polynomial can be written as\n:<math>-(-1)^n\\det(A)I_n = A(A^{n-1}+c_{n-1}A^{n-2}+\\cdots+c_{1}I_n),</math>\nand, by  multiplying both sides by {{math|''A''<sup>−1</sup>}} (note {{math|−(−1)<sup>''n''</sup> {{=}} (−1)<sup>''n''−1</sup>}}), one is led to an expression for the inverse of {{math|''A''}} as a trace identity,\n:<math> \n\\begin{align}\nA^{-1} & = \\frac{(-1)^{n-1}}{\\det A}(A^{n-1}+c_{n-1}A^{n-2}+\\cdots+c_{1}I_n), \\\\[5pt]\n & = \\frac{1}{\\det A}\\sum_{k=0}^{n-1} (-1)^{n+k-1}\\frac{A^{n-k-1}}{k!} B_k(s_1, -1! s_2, 2! s_3, \\ldots, (-1)^{k-1}(k-1)! s_k). \n\\end{align}\n</math>\n\nFor instance, the first few Bell polynomials are {{math|''B''<sub>0</sub>}} = 1, {{math|''B''<sub>1</sub>(''x''<sub>1</sub>) {{=}} ''x''<sub>1</sub>}}, {{math|''B''<sub>2</sub>(''x''<sub>1</sub>, ''x''<sub>2</sub>) {{=}} ''x''{{su|b=1|p=2}} + ''x''<sub>2</sub>}}, and {{math|''B''<sub>3</sub>(''x''<sub>1</sub>, ''x''<sub>2</sub>, ''x''<sub>3</sub>) {{=}} ''x''{{su|b=1|p=3}} + 3 ''x''<sub>1</sub>''x''<sub>2</sub> + ''x''<sub>3</sub>}}.\n\nUsing these to specify the coefficients {{math|''c<sub>i</sub>''}} of the characteristic polynomial of a {{math|2×2}} matrix yields\n\n:<math>\n\\begin{align}\nc_2 = B_0 = 1, \\\\[4pt]\nc_1 =  \\frac{-1}{1!} B_1(s_1) = - s_1 = - \\operatorname{tr}(A), \\\\[4pt]\nc_0 =  \\frac{1}{2!} B_2(s_1, -1! s_2) = \\frac{1}{2}(s_1^2 - s_2) = \\frac{1}{2}((\\operatorname{tr}(A))^2 - \\operatorname{tr}(A^2)).\n\\end{align}\n</math>\n\nThe coefficient {{math|''c''<sub>0</sub>}} gives the determinant of the {{math|2×2}} matrix, {{math|''c''<sub>1</sub>}} minus its trace, while its inverse is given by\n:<math> A^{-1} = \\frac{-1}{\\det A}(A + c_1 I_2) = \\frac{-2(A - \\operatorname{tr}(A) I_2)}{(\\operatorname{tr}(A))^2 - \\operatorname{tr}(A^2)}.</math>\n\nIt is apparent from the general formula for ''c<sub>n-k</sub>'', expressed in terms of Bell polynomials, that the expressions\n:<math>-\\operatorname{tr}(A)\\quad \\text{and} \\quad \\tfrac 1 2 (\\operatorname{tr}(A)^2 - \\operatorname{tr}(A^2))</math>\n\nalways give the coefficients {{math|''c''<sub>''n''−1</sub>}} of {{math|''λ''<sup>''n''−1</sup>}} and  {{math|''c''<sub>''n''−2</sub>}} of {{math|''λ''<sup>''n''−2</sup>}} in the characteristic polynomial of any {{math|''n''×''n''}} matrix, respectively. So, for a {{math|3×3}} matrix {{mvar|A}}, the statement of the Cayley–Hamilton theorem can also be written as\n:<math>A^3- (\\operatorname{tr}A)A^2+\\frac{1}{2}\\left((\\operatorname{tr}A)^2-\\operatorname{tr}(A^2)\\right)A-\\det(A)I_3=O,</math>\nwhere the right-hand side designates a {{math|3×3}} matrix with all entries reduced to zero. Likewise, this determinant in the {{math|''n''&nbsp;{{=}}&nbsp;3}} case, is  now\n:<math> \n\\begin{align}\n\\det(A) &= \\frac{1}{3!} B_3(s_1, -1! s_2, 2! s_3) =  \\frac{1}{6}(s_1^3 + 3 s_1 (-s_2) + 2 s_3) \\\\[5pt]\n &= \\tfrac{1}{6} \\left ( (\\operatorname{tr}A)^3-3\\operatorname{tr}(A^2)(\\operatorname{tr}A)+2\\operatorname{tr}(A^3) \\right ).\n\\end{align}\n</math>\nThis expression gives the negative of coefficient {{math|''c''<sub>''n''−3</sub>}} of {{math|''λ''<sup>''n''−3</sup>}} in the general case, as seen below.\n\nSimilarly, one can write for a {{math|4×4}} matrix {{mvar|A}},\n:<math> A^4-(\\operatorname{tr}A)A^3 + \\tfrac{1}{2}\\bigl((\\operatorname{tr}A)^2-\\operatorname{tr}(A^2)\\bigr)A^2 - \\tfrac{1}{6}\\bigl( (\\operatorname{tr}A)^3-3\\operatorname{tr}(A^2)(\\operatorname{tr}A)+2\\operatorname{tr}(A^3)\\bigr)A + \\det(A)I_4 = O,</math>\n\nwhere, now,  the determinant is  {{math|''c''<sub>''n''−4</sub>}}, \n\n:<math>\\tfrac{1}{24} \\left ( (\\operatorname{tr}A)^4-6 \\operatorname{tr}(A^2)(\\operatorname{tr}A)^2+3(\\operatorname{tr}(A^2))^2+8\\operatorname{tr}(A^3)\\operatorname{tr}(A) -6\\operatorname{tr}(A^4) \\right ),</math>\n\nand so on for larger matrices. The increasingly complex expressions for the coefficients  {{math|''c''<sub>''k''</sub>}} is deducible from [[Newton's identities]] or the [[Faddeev–LeVerrier algorithm]].\n\nAnother method for obtaining these coefficients  {{math|''c''<sub>''k''</sub>}} for a general {{math|''n''×''n''}} matrix, provided no root be zero, relies on the following alternative [[Matrix exponential#The determinant of the matrix exponential|expression for the determinant]],\n:<math> p(\\lambda)= \\det (\\lambda I_n -A) = \\lambda^n \\exp (\\operatorname{tr} (\\log (I_n - A/\\lambda))). </math>\n\nHence, by virtue of the [[Mercator series]],\n:<math>p(\\lambda)= \\lambda^n \\exp \\left( -\\operatorname{tr} \\sum_{m=1}^\\infty {({A\\over\\lambda})^m \\over m}  \\right),</math>\nwhere the exponential ''only'' needs be expanded to order  {{math|''λ''<sup>−''n''</sup>}}, since {{math|''p''(''λ'')}} is of order {{math|''n''}}, the net negative powers of {{math|''λ''}} automatically vanishing by the C–H theorem. (Again, this requires a ring containing the rational numbers.) The coefficients of {{mvar|λ}} can be directly written in terms of complete [[Bell polynomial]]s by comparing this expression with the generating function of the Bell polynomial.\n\nDifferentiation of this expression with respect to {{mvar|λ}} allows determination of the generic coefficients of the characteristic polynomial for general {{mvar|n}}, as\ndeterminants of {{math|''m''×''m''}} matrices,<ref group=nb>See, e.g., p.&nbsp;54 of  {{harvnb|Brown|1994}}, which solves [[Jacobi's formula]], \n:<math>\\partial p(\\lambda)  /\\partial \\lambda= p(\\lambda) \\sum^\\infty _{m=0}\\lambda ^{-(m+1)}  \\operatorname{tr}A^m =  p(\\lambda) ~  \\operatorname{tr} \\frac{I}{\\lambda I -A}\\equiv\\operatorname{tr} B~, </math>\nwhere {{mvar|B}} is the adjugate matrix of the next section.\n\nThere also exists an equivalent, related recursive algorithm introduced by [[Urbain Le Verrier]] and [[Dmitry Konstantinovich Faddeev]]—the [[Faddeev–LeVerrier algorithm]], which reads\n\n:<math> \\begin{align}\nM_0 &\\equiv O      & c_n &= 1                                                               \\qquad &(k=0) \\\\[5pt]\nM_k &\\equiv AM_{k-1}  -\\frac{1}{k-1}(\\operatorname{tr}(AM_{k-1})) I \\qquad \\qquad  & c_{n-k} &= -\\frac 1 k \\operatorname{tr}(AM_k) \\qquad &k=1,\\ldots ,n   ~.\n\\end{align}</math>\n\n(see, e.g., p 88 of {{harvnb|Gantmacher|1960}}.)  Observe {{math|''A''<sup>−1</sup> {{=}} −''M''<sub>''n''</sub> /''c''<sub>0</sub>}} as the recursion terminates.\nSee the algebraic proof in the following section, which relies on the modes of the adjugate, {{math|''B''<sub>''k''</sub> ≡ ''M''<sub>''n''−''k''</sub>}} .  &nbsp;\nSpecifically, <math>(\\lambda I-A) B = I p(\\lambda)</math> and the above derivative of {{mvar|p}}   when one traces it yields\n\n:<math>\\lambda p' -n p =\\operatorname{tr} (AB)~,</math> ({{harvnb|Hou|1998}}), and the above recursions, in turn.</ref>\n:<math>c_{n-m} = \\frac{(-1)^m}{m!}\n\\begin{vmatrix}  \\operatorname{tr}A  &   m-1 &0&\\cdots\\\\\n\\operatorname{tr}A^2  &\\operatorname{tr}A&   m-2 &\\cdots\\\\\n \\vdots & \\vdots & & & \\vdots    \\\\\n\\operatorname{tr}A^{m-1} &\\operatorname{tr}A^{m-2}& \\cdots & \\cdots & 1    \\\\\n\\operatorname{tr}A^m  &\\operatorname{tr}A^{m-1}& \\cdots & \\cdots & \\operatorname{tr}A \\end{vmatrix} ~.</math>\n\n===''n''-th Power of matrix===\nThe Cayley–Hamilton theorem always provides a relationship between the powers of {{mvar|A}} (though not always the simplest one), which allows one to simplify expressions involving such powers, and evaluate them without having to compute the power {{math|A<sup>''n''</sup>}} or any higher powers of {{mvar|A}}.\n\nAs an example, for <math>A = \\begin{pmatrix}1&2\\\\3&4\\end{pmatrix}</math> the theorem gives \n:<math>A^2=5A+2I_2\\,  .</math>\n\nThen, to calculate {{math|''A''<sup>4</sup>}}, observe\n:<math>A^3=(5A+2I_2)A=5A^2+2A=5(5A+2I_2)+2A=27A+10I_2,</math>\n:<math>A^4=A^3A=(27A+10I_2)A=27A^2+10A=27(5A+2I_2)+10A=145A+54I_2\\, .</math>\nLikewise,\n:<math>A^{-1}=\\frac{A-5I_2}{2}~.</math>\n\nNotice that we have been able to write the matrix power as the sum of two terms. In fact, matrix power of any order {{math|''k''}} can be written as a matrix polynomial of degree at most {{math|''n -'' 1}}, where {{math|''n''}} is the size of a square matrix. This is an instance where Cayley–Hamilton theorem can be used to express a matrix function, which we will discuss below systematically.\n\n===Matrix functions===\nGiven an analytic function \n:<math>f(x) = \\sum_{k=0}^\\infty a_k x^k</math>\nand the characteristic polynomial {{math|''p''(''x'')}}  of degree {{math|''n''}} of an {{math|''n × n''}} matrix {{mvar|A}}, the function can be expressed using long division as \n:<math>f(x) = q(x) p(x) + r(x),</math>\nwhere {{math|''q''(''x'')}} is some quotient polynomial and  {{math|''r''(''x'')}} is a remainder polynomial such that {{math|0 ≤ deg ''r''(''x'') < ''n''}}. By the Cayley–Hamilton theorem, replacing {{mvar|x}} by the matrix {{mvar|A}} gives {{math|1=''p''(''A'') = 0}}, so one has\n:<math>f(A) = r(A). </math>\n\nThus, the analytic function of matrix {{mvar|''A''}} can be expressed as a matrix polynomial of degree less than {{mvar|''n''}}.\n\nLet the remainder polynomial be\n:<math>r(x) = c_0 + c_1 x + \\cdots + c_{n-1} x^{n-1}.</math>\nSince {{math|1=''p''(''λ'') = 0}}, evaluating the function {{math|''f''(''x'')}} at the {{math|''n''}} eigenvalues of {{math|''A''}},  yields\n:<math> f(\\lambda_i) = r(\\lambda_i) = c_0 + c_1 \\lambda_i + \\cdots + c_{n-1} \\lambda_i^{n-1}, \\qquad \\mathrm{for} \\qquad i=1,2,...,n.</math>\nThis amounts to a system of {{math|''n''}} linear equations, which can be solved to determine the coefficients {{math|''c<sub>i</sub>''}}. Thus, one has \n:<math>f(A) = \\sum_{k=0}^{n-1} c_k A^k.</math>\n\nWhen the eigenvalues are repeated, that is {{math|1=''λ<sub>i</sub> = λ<sub>j</sub>''}} for some {{math|''i ≠ j''}}, two or more equations are identical; and hence the linear equations cannot be solved uniquely. For such cases, for an eigenvalue {{math|''λ''}} with multiplicity {{math|''m''}}, the first {{math|''m'' – 1}} derivative of {{math|''p(x)''}} vanishes at the eigenvalues. Thus, there are the extra {{math|''m'' – 1}} linearly independent solutions \n:<math>\\frac{\\mathrm{d}^k f(x)}{\\mathrm{d}x^k}\\Big|_{x=\\lambda} = \\frac{\\mathrm{d}^k r(x)}{\\mathrm{d}x^k}\\Big|_{x=\\lambda}\\qquad \\text{for} \\qquad k = 1, 2, \\ldots, m-1,</math>\nwhich, when combined with others, yield the required {{math|''n''}} equations to solve for {{math|''c<sub>i</sub>''}}.\n\nFinding a polynomial that passes through the points {{math|(''λ<sub>i</sub>'', ''f'' (''λ<sub>i</sub>''))}} is essentially an [[polynomial interpolation|interpolation problem]], and can be solved using [[Lagrange interpolation|Lagrange]] or [[Newton polynomial|Newton interpolation]] techniques,  leading to  [[Sylvester's formula]].\n\nFor example, suppose the task is to find the polynomial representation of \n:<math>f(A) = e^{At} \\qquad \\mathrm{where} \\qquad A = \\begin{pmatrix}1&2\\\\0&3\\end{pmatrix}.</math>\n\nThe characteristic polynomial is {{math|1=''p''(''x'') = (''x'' − 1)(''x'' − 3) = ''x''<sup>2</sup> − 4''x'' + 3}}, and the eigenvalues are {{math|1=''λ'' = 1, 3}}. Let {{math|1=''r''(''x'') = ''c''<sub>0</sub> + ''c''<sub>1</sub>''x''}}. Evaluating {{math|1=''f''(''λ) = ''r''(''λ'')''}} at the eigenvalues, one obtains two linear equations {{math|1=''e''<sup>''t''</sup> = ''c''<sub>0</sub> + ''c''<sub>1</sub>}} and {{math|1=''e''<sup>3''t''</sup> = ''c''<sub>0</sub> + 3''c''<sub>1</sub>}}. Solving the equations yields {{math|1=''c''<sub>0</sub> = (3''e''<sup>''t''</sup> − ''e''<sup>3''t''</sup>)/2}} and {{math|1=''c''<sub>1</sub> = (''e''<sup>3''t''</sup> − ''e''<sup>''t''</sup>)/2}}. Thus, it follows that\n\n:<math>e^{At} = c_0 I_2 + c_1 A = \\begin{pmatrix}c_0 + c_1 & 2 c_1\\\\ 0 & c_0 + 3 c_1\\end{pmatrix} = \\begin{pmatrix}e^{t} & e^{3t} - e^{t} \\\\ 0 & e^{3t}\\end{pmatrix}. </math>\n\nIf, instead, the function were {{math|1=''f''(''A'') = sin ''At''}}, then the coefficients would have been {{math|1=''c''<sub>0</sub> = (3 sin ''t'' − sin 3''t'')/2}} and {{math|1=''c''<sub>1</sub> = (sin 3''t'' - sin ''t'')/2}}; hence\n:<math>\\sin(At) = c_0 I_2 + c_1 A = \\begin{pmatrix}\\sin t & \\sin 3t - \\sin t \\\\ 0 & \\sin 3t\\end{pmatrix}.</math>\n\nAs a further example, when considering\n:<math>f(A) = e^{At} \\qquad \\mathrm{where} \\qquad A = \\begin{pmatrix}0 & 1\\\\-1 & 0\\end{pmatrix},</math>\nthen the characteristic polynomial is {{math|1=''p''(''x'') = ''x''<sup>2</sup> + 1}}, and the eigenvalues are {{math|1=''λ'' = ±''i''}}. As before, evaluating the function at the eigenvalues gives us the linear equations {{math|1=''e<sup>it</sup> = c<sub>0</sub> + i c<sub>1</sub>''}} and {{math|1=''e''<sup>−''it''</sup> = ''c''<sub>0</sub> − ''ic''<sub>1</sub>}}; the solution of which gives, {{math|1=''c''<sub>0</sub> = (''e''<sup>''it''</sup> + ''e''<sup>−''it''</sup>)/2 = cos ''t''}} and {{math|1=''c''<sub>1</sub> = (''e''<sup>''it''</sup> − ''e''<sup>−''it''</sup>)/2''i'' = sin ''t''}}. Thus, for this case,\n:<math>e^{At} = (\\cos t) I_2 + (\\sin t) A =  \\begin{pmatrix}\\cos t & \\sin t\\\\ -\\sin t & \\cos t \\end{pmatrix},</math>\nwhich is a [[rotation matrix]].\n\nStandard examples of such usage is the [[exponential map (Lie theory)|exponential map]] from the [[Lie algebra]] of a [[matrix Lie group]] into the group. It is given by a [[matrix exponential]],\n:<math>\\exp: \\mathfrak g \\rightarrow G;\n\\qquad tX \\mapsto e^{tX} = \\sum_{n=0}^\\infty \\frac{t^nX^n}{n!} = I + tX + \\frac{t^2X^2}{2} + \\cdots, t \\in \\mathbb R, X \\in \\mathfrak g .</math>\nSuch expressions have long been known for {{math|SU(2)}},\n:<math>e^{i(\\theta/2)(\\hat n \\cdot \\sigma)} = I_2 \\cos \\theta/2 + i(\\hat n \\cdot \\sigma) \\sin \\theta/2,</math>\nwhere the {{mvar|σ}} are the [[Pauli matrices]] and for {{math|SO(3)}},\n:<math>e^{i\\theta(\\hat n \\cdot \\mathbf J)} = I_3 +  i(\\hat n \\cdot \\mathbf J) \\sin \\theta  + (\\hat n \\cdot \\mathbf J)^2 (\\cos \\theta - 1),</math>\nwhich is [[Rodrigues' rotation formula]]. For the notation, see [[rotation group SO(3)#A note on Lie algebra]].\n\nMore recently, expressions have appeared for other groups, like the [[Lorentz group]] {{math|SO(3, 1)}},<ref>{{harvnb|Zeni|Rodrigues|1992}}</ref> {{math|O(4, 2)}}<ref>{{harvnb|Barut|Zeni|Laufer|1994a}}</ref> and {{math|SU(2, 2)}},<ref>{{harvnb|Barut|Zeni|Laufer|1994b}}</ref> as well as {{math|GL(''n'', '''R''')}}.<ref>{{harvnb|Laufer|1997}}</ref> The group {{math|O(4, 2)}} is the [[conformal group]] of [[spacetime]], {{math|SU(2, 2)}} its [[simply connected]] cover (to be precise, the simply connected cover of the [[connected component (topology)|connected component]] {{math|SO<sup>+</sup>(4, 2)}} of {{math|O(4, 2)}}). The expressions obtained apply to the standard representation of these groups. They require knowledge of (some of) the [[eigenvalue]]s of the matrix to exponentiate. For {{math|SU(2)}} (and hence for {{math|SO(3)}}), closed expressions have recently been obtained for all irreducible representations, i.e. of any spin.<ref>{{harvnb|Curtright|Fairlie|Zachos|2014}}</ref>\n\n[[File:GeorgFrobenius.jpg|220px|thumb|right|[[Ferdinand Georg Frobenius]] (1849–1917), German mathematician. His main interests were [[elliptic function]]s [[differential equation]]s, and later [[group theory]].<br/>In 1878 he gave the first full proof of the Cayley&ndash;Hamilton theorem.<ref name=\"Frobenius 1878\"/>]]\n\n===Algebraic number theory===\nThe Cayley–Hamilton theorem is an effective tool for computing the minimal polynomial of algebraic integers. For example, given a finite extension <math>\\mathbb{Q}[\\alpha_1,\\ldots,\\alpha_k]</math> of <math>\\mathbb{Q}</math> and an algebraic integer <math>\\alpha \\in \\mathbb{Q}[\\alpha_1,\\ldots,\\alpha_k]</math> which is a non-zero linear combination of the <math>\\alpha_1^{n_1}\\cdots\\alpha_k^{n_k}</math> we can compute the minimal polynomial of <math>\\alpha</math> by finding a matrix representing the <math>\\mathbb{Q}</math>-linear transformation\n:<math>\\cdot \\alpha : \\mathbb{Q}[\\alpha_1,\\ldots,\\alpha_k] \\to \\mathbb{Q}[\\alpha_1,\\ldots,\\alpha_k]</math>\nIf we call this transformation matrix <math>A</math>, then we can find the minimal polynomial by applying the Cayley–Hamilton theorem to <math>A</math>.<ref>{{cite book|last1=Stein|first1=William|title=Algebraic Number Theory, a Computational Approach|pages=29|url=http://wstein.org/books/ant/ant.pdf}}</ref>\n\n== Proving the theorem in general ==\nThe Cayley–Hamilton theorem is an immediate consequence of the existence of the [[Jordan normal form]] for matrices over [[algebraically closed field]]s. In this section direct proofs are presented.\n\nAs the examples above show, obtaining the statement of the Cayley–Hamilton theorem for an {{math|''n''×''n''}} matrix\n\n:<math>A=(a_{ij})_{i,j=1}^n</math>\nrequires two steps: first the coefficients {{math|''c''<sub>''i''</sub>}} of the characteristic polynomial are determined by development as a polynomial in {{math|''t''}} of the determinant\n\n: <math>\n\\begin{align}\np(t) & = \\det(t I_n - A) =\n\\begin{vmatrix}t-a_{1,1}&-a_{1,2}&\\cdots&-a_{1,n} \\\\\n-a_{2,1}&t-a_{2,2}&\\cdots&-a_{2,n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n-a_{n,1}&-a_{n,2}& \\cdots& t-a_{n,n} \\end{vmatrix} \\\\[5pt]\n& = t^n+c_{n-1}t^{n-1}+\\cdots+c_1t+c_0,\n\\end{align}\n</math>\n\nand then these coefficients are used in a linear combination of powers of {{math|''A''}} that is equated to the {{math|''n''×''n''}} null matrix:\n:<math>A^n+c_{n-1}A^{n-1} + \\cdots + c_1 A + c_0 I_n = \\begin{pmatrix} 0 & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & 0 \\end{pmatrix}.</math>\n\nThe left hand side can be worked out to an {{math|''n''×''n''}} matrix whose entries are (enormous) polynomial expressions in the set of entries {{math|''a''<sub>''i'',''j''</sub>}} of {{math|''A''}}, so the Cayley–Hamilton theorem states that each of these {{math|''n''<sup>2</sup>}} expressions are equal to {{math|0}}. For any fixed value of {{math|''n''}} these identities can be obtained by tedious but completely straightforward algebraic manipulations. None of these computations can show however why the Cayley–Hamilton theorem should be valid for matrices of all possible sizes {{math|''n''}}, so a uniform proof for all {{math|''n''}} is needed.\n\n=== Preliminaries ===\nIf a vector {{math|''v''}} of size {{math|''n''}} happens to be an [[eigenvector]] of {{math|''A''}} with eigenvalue {{math|''λ''}}, in other words if {{math|''A''⋅''v'' {{=}} ''λv''}}, then\n:<math>\\begin{align}\np(A)\\cdot v & = A^n\\cdot v+c_{n-1}A^{n-1}\\cdot v+\\cdots+c_1A\\cdot v+c_0I_n\\cdot v \\\\[6pt]\n& = \\lambda^nv+c_{n-1}\\lambda^{n-1}v+\\cdots+c_1\\lambda v+c_0 v=p(\\lambda)v,\n\\end{align}</math>\nwhich is the null vector since {{math|''p''(''λ'') {{=}} 0}} (the eigenvalues of {{math|''A''}} are precisely the [[root of a function|root]]s of {{math|''p''(''t'')}}). This holds for all possible eigenvalues {{math|''λ''}}, so the two matrices equated by the theorem certainly give the same (null) result when applied to any eigenvector. Now if {{math|''A''}} admits a [[basis (linear algebra)|basis]] of eigenvectors, in other words if {{math|''A''}} is [[diagonalizable]], then the Cayley–Hamilton theorem must hold for {{math|''A''}}, since two matrices that give the same values when applied to each element of a basis must be equal. \n:<math>A=XDX^{-1}, \\quad D=\\operatorname{diag}(\\lambda_i), \\quad i=1,2,...,n\n</math>\n:<math>p_A(\\lambda)=|\\lambda I-A|=</math> product  of  eigenvalues  of <math>\\lambda I -A=\\prod_{i=1}^n (\\lambda-\\lambda_i)\\equiv \\sum_{k=0}^n c_k\\lambda^k</math>\n:<math>p_A(A)=\\sum c_k A^k=X p_A(D)X^{-1}=X C X^{-1} </math>\n:<math>C_{ii}=\\sum_{k=0}^n c_k\\lambda_i^k=\\prod_{j=1}^n(\\lambda_i-\\lambda_j)=0, \\qquad C_{i,j\\neq i}=0</math>\n:<math>\\therefore p_A(A)=XCX^{-1}=O .</math>\n\nConsider now the function <math>e: M_n \\to M_n</math>which maps <math>n \\times n</math>matrices to <math>n \\times n</math>matrices given by the formula <math>e(A)=p_A(A)</math>, i.e. which takes a matrix <math>A</math>and plugs it into its own characteristic polynomial. Not all matrices are diagonalizable, but for matrices with complex coefficients many of them are: the set of <math>D</math> diagonalizable complex square matrices of a given size is [[dense set|dense]] in the set of all such square matrices<ref>{{harvnb|Bhatia|1997|p=7}}</ref> (for a matrix to be diagonalizable it suffices for instance that its characteristic polynomial not have any multiple roots).  Now viewed as a function  <math>e:\\mathbb{C}^{n^2}\\to \\mathbb {C} ^{n^{2}}</math>(since matrices have <math>n^2</math>entries) we see that this function is [[Continuous function|continuous]]. This is true because the entries of the image of a matrix are given by polynomials in the entries of the matrix. Since\n\n<math>e(D) = \\left\\{\\begin{pmatrix} 0 & \\cdots & 0 \\\\ \\vdots & \\ddots & \\vdots \\\\ 0 & \\cdots & 0 \\end{pmatrix}\\right\\}</math>\n\nand since <math>D</math>is dense, by continuity this function must map the entire set of <math>n \\times n</math>matrices to the zero matrix. Therefore the Cayley–Hamilton theorem is true for complex numbers, and must therefore also hold for <math>\\mathbb Q</math>or <math>\\mathbb R</math>valued matrices..\n\nWhile this provides a valid proof,  the argument is not very satisfactory, since the identities represented by the theorem do not in any way depend on the nature of the matrix (diagonalizable or not), nor on the kind of entries allowed (for matrices with real entries the diagonalizable ones do not form a dense set, and it seems strange one would have to consider complex matrices to see that the Cayley–Hamilton theorem holds for them). We shall therefore now consider only arguments that prove the theorem directly for any matrix using algebraic manipulations only; these also have the benefit of working for matrices with entries in any [[commutative ring]].\n\nThere is a great variety of such proofs of the Cayley–Hamilton theorem, of which several will be given here. They vary in the amount of abstract algebraic notions required to understand the proof. The simplest proofs use just those notions needed to formulate the theorem (matrices, polynomials with numeric entries, determinants), but involve technical computations that render somewhat mysterious the fact that they lead precisely to the correct conclusion. It is possible to avoid such details, but at the price of involving more subtle algebraic notions: polynomials with coefficients in a non-commutative ring, or matrices with unusual kinds of entries.\n\n==== Adjugate matrices ====\nAll proofs below use the notion of the [[adjugate matrix]] {{math|adj(''M'')}} of an {{math|''n''×''n''}} matrix {{math|''M''}}, the [[transpose]] of its [[Minor (linear algebra)|cofactor matrix]].\n\nThis is a matrix whose coefficients are given  by polynomial expressions in the coefficients of {{math|''M''}} (in fact, by certain {{math|(''n''&nbsp;−&nbsp;1)×(''n''&nbsp;−&nbsp;1)}} determinants), in such a way that \nthe following fundamental relations hold,\n:<math>\\operatorname{adj}(M)\\cdot M=\\det(M)I_n=M\\cdot\\operatorname{adj}(M)~.</math>\nThese relations are a direct consequence of the basic properties of determinants: evaluation of the {{math|(''i'',''j'')}} entry of the matrix product on the left gives the expansion by column {{math|''j''}} of the determinant of the matrix obtained from {{math|''M''}} by replacing column {{math|''i''}} by a copy of column {{math|''j''}}, which is {{math|det(''M'')}} if {{math|''i'' {{=}} ''j''}} and zero otherwise; the matrix product on the right is similar, but for expansions by rows.\n\nBeing a consequence of just algebraic expression manipulation, these relations are valid for matrices with entries in any commutative ring (commutativity must be assumed for determinants to be defined in the first place). This is important to note here, because these relations will be applied below for matrices with non-numeric entries such as polynomials.\n\n=== A direct algebraic proof ===\nThis proof uses just the kind of objects needed to formulate the Cayley–Hamilton theorem: matrices with polynomials as entries. The matrix {{math|''t I''<sub>''n''</sub> −''A''}} whose determinant is the characteristic polynomial of {{mvar|A}} is such a matrix, and since polynomials form a commutative ring, it has an [[Adjugate matrix|adjugate]]\n:<math>B=\\operatorname{adj}(tI_n-A).</math>\nThen, according to the right-hand fundamental relation of the adjugate, one has\n:<math>(t I_n - A)B = \\det(t I_n - A) I_n = p(t) I_n~.</math>\n\nSince {{math|''B''}} is also a matrix with polynomials in  {{math|''t''}} as entries, one can, for each  {{math|''i''}} , collect the coefficients of  {{math|''t<sup>i</sup>''}} in each entry to form a matrix  {{math|''B'' <sub>''i''</sub>}} of numbers, such that one has\n:<math>B = \\sum_{i = 0}^{n - 1} t^i B_i   ~.</math>\n(The way the entries of  {{math|''B''}} are defined makes clear that no powers higher than {{math|''t''<sup>''n''−1</sup>}} occur). While this ''looks'' like a polynomial with matrices as coefficients, we shall not consider such a notion; it is just a way to write a matrix with polynomial entries as a linear combination of {{mvar|n}} constant matrices, and the coefficient  {{math|''t <sup>i</sup>''}} has been written to the left of the matrix to stress this point of view.\n\nNow, one can expand the matrix product in our equation by bilinearity\n:<math>\\begin{align}\n p(t) I_n &= (t I_n - A)B \\\\\n &=(t I_n - A)\\sum_{i = 0}^{n - 1} t^i B_i  \\\\\n &=\\sum_{i = 0}^{n - 1} tI_n\\cdot t^i B_i - \\sum_{i = 0}^{n - 1} A\\cdot t^i B_i \\\\\n &=\\sum_{i = 0}^{n - 1} t^{i + 1}  B_i- \\sum_{i = 0}^{n - 1} t^i AB_i  \\\\\n &=t^n B_{n - 1} + \\sum_{i = 1}^{n - 1}  t^i(B_{i - 1} - AB_i) - AB_0~.\n\\end{align}</math>\n\nWriting\n:<math>p(t)I_n=t^nI_n+t^{n-1}c_{n-1}I_n+\\cdots+tc_1I_n+c_0I_n~,</math>\none obtains an equality of two matrices with polynomial entries, written as linear combinations of constant matrices with powers of  {{math|''t''}} as coefficients.\n\nSuch an equality can hold only if in any matrix position the entry that is multiplied by a given power {{math| ''t<sup>i</sup>''}}  is the same on both sides; it follows that the constant matrices with coefficient {{math|''t<sup>i</sup>''}} in both expressions must be equal. Writing these equations then for {{math|''i''}} from  {{math|''n''}} down to 0,  one finds\n:<math>B_{n - 1} = I_n, \\qquad B_{i - 1} - AB_i = c_i I_n\\quad \\text{for }1 \\leq i \\leq n-1, \\qquad -A B_0 = c_0 I_n~.</math>\n\nFinally, multiply the equation of the coefficients of  {{math|''t''<sup>''i''</sup>}} from the left by  {{math|''A''<sup>''i''</sup>}}, and sum up:\n\n<math display=\"inline\">A^n B_{n-1} + \\sum\\limits_{i=1}^{n-1}\\left( A^i B_{i-1} - A^{i+1}B_i\\right) -A B_0 =A^n+c_{n-1}A^{n-1}+\\cdots+c_1A+c_0I_n~. </math>\n\nThe left-hand sides form a [[telescoping sum]] and cancel completely; the right-hand sides add up to <math> p(A)</math>:\n:<math> 0= p(A)~.</math>\nThis completes the proof.\n\n=== A proof using polynomials with matrix coefficients ===\nThis proof is similar to the first one, but tries to give meaning to the notion of polynomial with matrix coefficients that was suggested by the expressions occurring in that proof. This requires considerable care, since it is somewhat unusual to consider polynomials with coefficients in a non-commutative ring, and not all reasoning that is valid for commutative polynomials can be applied in this setting.\n\nNotably, while arithmetic of polynomials over a commutative ring models the arithmetic of [[polynomial function]]s, this is not the case over a non-commutative ring (in fact there is no obvious notion of polynomial function in this case that is closed under multiplication). So when considering polynomials in {{mvar|t}} with matrix coefficients, the variable {{mvar|t}} must not be thought of as an \"unknown\", but as a formal symbol that is to be manipulated according to given rules; in particular one cannot just set  {{mvar|t}} to a specific value.\n:<math>(f+g)(x) = \\sum_i \\left (f_i+g_i \\right )x^i = \\sum_i{f_i x^i} + \\sum_i{g_i x^i} = f(x) + g(x).</math>\n\nLet ''M''(''n'', ''R'') be the ring of ''n''×''n'' matrices with entries in some ring ''R'' (such as the real or complex numbers) that has  {{mvar|A}}  as an element. Matrices with as coefficients polynomials in {{mvar|t}}, such as <math>tI_n - A</math> or its adjugate ''B'' in the first proof, are elements of ''M''(''n'', ''R''[''t'']).\n\nBy collecting like powers of {{mvar|t}}, such matrices can be written as \"polynomials\" in {{mvar|t}} with constant matrices as coefficients; write ''M''(''n'', ''R'')[''t''] for the set of such polynomials. Since this set is in bijection with ''M''(''n'', ''R''[''t'']), one defines arithmetic operations on it correspondingly, in particular multiplication is given by\n:<math>\\left (\\sum_iM_it^i \\right )\\left (\\sum_jN_jt^j \\right )=\\sum_{i,j}(M_i N_j)t^{i+j},</math>\nrespecting the order of the coefficient matrices from the two operands; obviously this gives a non-commutative multiplication.\n\nThus, the identity\n:<math>(t I_n - A)B = p(t) I_n.</math>\nfrom the first proof can be viewed as one involving a multiplication of elements in ''M''(''n'', ''R'')[''t''].\n\nAt this point, it is tempting to simply set {{mvar|t}} equal to the matrix  {{mvar|A}} , which makes the first factor on the left equal to the null matrix, and the right hand side equal to {{math|''p''(''A'')}}; however, this is not an allowed operation when coefficients do not commute. It is possible to define a \"right-evaluation map\" ev<sub>{{mvar|A}}</sub> : '''M'''[''t''] → '''M''', which replaces each ''t''<sup>''i''</sup> by the matrix power  {{mvar|A}}<sup>''i''</sup> of  {{mvar|A}} , where one stipulates that the power is always to be multiplied on the right to the corresponding coefficient.\n\nBut this map is not a ring homomorphism: the right-evaluation of a product differs in general from the product of the right-evaluations. This is so because multiplication of polynomials with matrix coefficients does not model multiplication of expressions containing unknowns: a product <math>Mt^i Nt^j = (M\\cdot N)t^{i+j}</math> is defined assuming that  {{mvar|t}} commutes with  {{mvar|N}}, but this may fail if {{mvar|t}} is replaced by the matrix  {{mvar|A}}.\n\nOne can work around this difficulty in the particular situation at hand, since the above right-evaluation map does become a ring homomorphism if the matrix  {{mvar|A}}  is in the [[center (algebra)|center]] of the ring of coefficients, so that it commutes with all the coefficients of the polynomials (the argument proving this is straightforward, exactly because commuting {{mvar|t}} with coefficients is now justified after evaluation).\n\nNow, {{mvar|A}}  is not always in the center of '''M''', but we may replace '''M''' with a smaller ring provided it contains all the coefficients of the polynomials in question: <math>I_n</math>,  {{mvar|A}}, and the coefficients <math>B_i</math> of the polynomial ''B''. The obvious choice for such a subring is the [[centralizer]] ''Z'' of  {{mvar|A}}, the subring of all matrices that commute with  {{mvar|A}}; by definition  {{mvar|A}}  is in the center of ''Z''.\n\nThis centralizer obviously contains <math>I_n</math>, and {{mvar|A}}, but one has to show that it contains the matrices <math>B_i</math>. To do this, one combines the two fundamental relations for adjugates, writing out the adjugate ''B'' as a polynomial:\n:<math>\\begin{align}\n  \\left(\\sum_{i = 0}^m B_i t^i\\right) (t I_n - A)&=(tI_n - A) \\sum_{i = 0}^m B_i t^i \\\\\n  \\sum_{i = 0}^m B_i t^{i + 1} - \\sum_{i = 0}^m B_i A t^i &= \\sum_{i = 0}^m B_i t^{i + 1} - \\sum_{i = 0}^m A B_i t^i \\\\\n \\sum_{i = 0}^m B_i A t^i &= \\sum_{i = 0}^m A B_i t^i .\n \\end{align}</math>\n\n[[Equating the coefficients]] shows that for each ''i'', we have  {{mvar|A}} ''B''<sub>''i''</sub> = ''B''<sub>''i''</sub>  {{mvar|A}}  as desired. Having found the proper setting in which ev<sub>{{mvar|A}}</sub> is indeed a homomorphism of rings, one can complete the proof as suggested above:\n:<math>\\begin{align}\n \\operatorname{ev}_A\\bigl(p(t)I_n\\bigr) &= \\operatorname{ev}_A((tI_n-A)B)  \\\\[5pt]\n  p(A)&= \\operatorname{ev}_A(tI_n - A)\\cdot \\operatorname{ev}_A(B) \\\\[5pt]\n  p(A) &= (AI_n-A) \\cdot \\operatorname{ev}_A(B) = O\\cdot\\operatorname{ev}_A(B)=O.\n \\end{align}</math>\nThis completes the proof.\n\n=== A synthesis of the first two  proofs ===\nIn the first proof, one was able to determine the coefficients  {{math|''B''<sub>''i''</sub>}} of  {{math|''B''}} based on the right-hand fundamental relation for the adjugate only. In fact the first {{math|''n''}} equations derived can be interpreted as determining the quotient {{math|''B''}} of the [[Euclidean division]] of the polynomial {{math|''p''(''t'')''I<sub>n</sub>''}} on the left by the [[monic polynomial]] {{math|''I<sub>n</sub>t'' − ''A''}}, while the final equation expresses the fact that the remainder is zero. This division is performed in the ring of polynomials with matrix coefficients. Indeed, even over a non-commutative ring, Euclidean division by a monic polynomial  {{math|''P''}} is defined, and always produces a unique quotient and remainder with the same degree condition as in the commutative case, provided it is specified at which side one wishes {{math|''P''}} to be a factor (here that is to the left).\n\nTo see that quotient and remainder are unique (which is the important part of the statement here), it suffices to write <math>PQ+r=PQ'+r'</math> as <math>P(Q-Q')=r'-r</math> and observe that since  {{math|''P''}} is monic, {{math|''P(Q−Q') ''}} cannot have a degree less than that of  {{math|''P''}}, unless {{math|''Q''{{=}}''Q' ''}}.\n\nBut the dividend {{math|''p''(''t'')''I<sub>n</sub>''}} and divisor {{math|''I<sub>n</sub>t''−''A''}} used here both lie in the subring  {{math|(''R''[''A''])[''t'']}}, where  {{math|''R''[''A'']}} is the subring of the matrix ring  {{math|''M''(''n'', ''R'')}} generated by  {{math|''A''}}: the  {{math|''R''}}-linear span of all powers of   {{math|''A''}}. Therefore, the Euclidean division can in fact be performed within that ''commutative'' polynomial ring, and of course it then gives the same quotient  {{math|''B''}} and remainder 0 as in the larger ring; in particular this shows that  {{math|''B''}} in fact lies in  {{math|(''R''[''A''])[''t'']}}.\n\nBut, in this commutative setting, it is valid to set   {{math|''t''}} to  {{math|''A''}} in the equation \n\n:<math>p(t)I_n=(tI_n-A)B;</math>\n\nin other words, to apply the evaluation map\n\n:<math>\\operatorname{ev}_A:(R[A])[t]\\to R[A]</math>\n\nwhich is a ring homomorphism, giving\n\n:<math>p(A)=0\\cdot\\operatorname{ev}_A(B)=0</math>\n\njust like in the second proof, as desired.\n\nIn addition to proving the theorem, the above argument tells us that the coefficients {{math| ''B<sub>i</sub>''}} of  {{math|''B''}} are polynomials in  {{math|''A''}}, while from the second proof we only knew that they lie in the centralizer  {{math|''Z''}} of  {{math|''A''}}; in general  {{math|''Z''}} is a larger subring than  {{math|''R''[''A'']}}, and not necessarily commutative. In particular the constant term {{math|''B''<sub>0</sub>{{=}} adj(−''A'')}} lies in  {{math|''R''[''A'']}}. Since  {{math|''A''}} is an arbitrary square matrix, this proves that {{math|adj(''A'')}} can always be expressed as a polynomial in  {{math|''A''}} (with coefficients that depend on  {{math|''A'')}}.\n\nIn fact, the equations found in the first proof allow successively expressing <math>B_{n-1}, \\ldots, B_1, B_0</math> as polynomials in  {{math|''A''}}, which leads to the identity\n{{Equation box 1\n|indent =::\n|equation =<math>\\operatorname{adj}(-A)=\\sum_{i=1}^nc_iA^{i-1},</math>\n|cellpadding= 6\n|border\n|border colour = #0070BF\n|bgcolor=#FAFFFB}}\nvalid for all  {{math|''n''×''n''}} matrices, where \n:<math>p(t)=t^n+c_{n-1}t^{n-1}+\\cdots+c_1t+c_0</math> \nis the characteristic polynomial of  {{math|''A''}}.\n\nNote that this identity also implies the statement of the Cayley–Hamilton theorem: one may move  {{math|adj(−''A'')}} to the right hand side, multiply the resulting equation (on the left or on the right) by  {{math|''A''}}, and use the fact that\n:<math>-A\\cdot \\operatorname{adj}(-A)=\\operatorname{adj}(-A)\\cdot (-A)=\\det(-A)I_n=c_0I_n.</math>\n\n{{see also|Faddeev–LeVerrier algorithm}}\n\n=== A proof using matrices of endomorphisms ===\nAs was mentioned above, the matrix ''p''(''A'') in statement of the theorem is obtained by first evaluating the determinant and then substituting the matrix ''A'' for ''t''; doing that substitution into the matrix <math>tI_n-A</math> before evaluating the determinant is not meaningful. Nevertheless, it is possible to give an interpretation where ''p''(''A'') is obtained directly as the value of a certain determinant, but this requires a more complicated setting, one of matrices over a ring in which one can interpret both the entries <math>A_{i,j}</math> of ''A'', and all of ''A'' itself. One could take for this the ring ''M''(''n'', ''R'') of ''n''×''n'' matrices over ''R'', where the entry <math>A_{i,j}</math> is realised as <math>A_{i,j}I_n</math>, and ''A'' as itself. But considering matrices with matrices as entries might cause confusion with [[block matrix|block matrices]], which is not intended, as that gives the wrong notion of determinant (recall that the determinant of a matrix is defined as a sum of products of its entries, and in the case of a block matrix this is generally not the same as the corresponding sum of products of its blocks!). It is clearer to distinguish ''A'' from the endomorphism ''φ'' of an ''n''-dimensional vector space ''V'' (or free ''R''-module if ''R'' is not a field) defined by it in a basis ''e''<sub>1</sub>, ..., ''e''<sub>''n''</sub>, and to take matrices over the ring End(''V'') of all such endomorphisms. Then ''φ'' ∈ End(''V'') is a possible matrix entry, while ''A'' designates the element of ''M''(''n'', End(''V'')) whose ''i'',''j'' entry is endomorphism of scalar multiplication by <math>A_{i,j}</math>; similarly ''I''<sub>''n''</sub> will be interpreted as element of ''M''(''n'', End(''V'')). However, since End(''V'') is not a commutative ring, no determinant is defined on ''M''(''n'', End(''V'')); this can only be done for matrices over a commutative subring of End(''V''). Now the entries of the matrix <math>\\varphi I_n-A</math> all lie in the subring ''R''[''φ''] generated by the identity and ''φ'', which is commutative. Then a determinant map ''M''(''n'', ''R''[''φ'']) → ''R''[''φ''] is defined, and <math>\\det(\\varphi I_n-A)</math> evaluates to the value ''p''(''φ'') of the characteristic polynomial of ''A'' at ''φ'' (this holds independently of the relation between ''A'' and ''φ''); the Cayley–Hamilton theorem states that ''p''(''φ'') is the null endomorphism.\n\nIn this form, the following proof can be obtained from that of {{Harvard citations|last1 = Atiyah|last2 = MacDonald|year = 1969|loc = Prop. 2.4}} (which in fact is the more general statement related to the [[Nakayama lemma]]; one takes for the ideal in that proposition the whole ring ''R''). The fact that ''A'' is the matrix of ''φ'' in the basis ''e''<sub>1</sub>, ..., ''e''<sub>''n''</sub> means that\n:<math>\\varphi(e_i) = \\sum_{j = 1}^n A_{j,i} e_j \\quad\\text{for }i=1,\\ldots,n.</math>\nOne can interpret these as ''n'' components of one equation in ''V''<sup>''n''</sup>, whose members can be written using the matrix-vector product ''M''(''n'', End(''V'')) × ''V<sup>n</sup>'' → ''V<sup>n</sup>'' that is defined as usual, but with individual entries ''ψ'' ∈ End(''V'') and ''v'' in ''V'' being \"multiplied\" by forming <math>\\psi(v)</math>; this gives:\n:<math>\\varphi I_n \\cdot E= A^\\operatorname{tr}\\cdot E,</math>\nwhere <math>E\\in V^n</math> is the element whose component ''i'' is ''e''<sub>''i''</sub> (in other words it is the basis ''e''<sub>1</sub>, ..., ''e''<sub>''n''</sub> of ''V'' written as a column of vectors). Writing this equation as\n:<math>(\\varphi I_n-A^\\operatorname{tr})\\cdot E=0\\in V^n</math>\none recognizes the [[transpose]] of the matrix <math>\\varphi I_n-A</math> considered above, and its determinant (as element of ''M''(''n'', ''R''[''φ''])) is also ''p''(''φ''). To derive from this equation that ''p''(''φ'') = 0 ∈ End(''V''), one left-multiplies by the [[adjugate matrix]] of <math>\\varphi I_n-A^\\operatorname{tr}</math>, which is defined in the matrix ring ''M''(''n'', ''R''[''φ'']), giving\n:<math>\\begin{align}\n 0&=\\operatorname{adj}(\\varphi I_n-A^\\operatorname{tr})\\cdot((\\varphi I_n-A^\\operatorname{tr})\\cdot E)\\\\\n  &= (\\operatorname{adj}(\\varphi I_n-A^\\operatorname{tr})\\cdot(\\varphi I_n-A^\\operatorname{tr}))\\cdot E\\\\\n  &= (\\det(\\varphi I_n-A^\\operatorname{tr})I_n)\\cdot E\\\\\n  &= (p(\\varphi)I_n)\\cdot E;\n\\end{align}</math>\nthe associativity of matrix-matrix and matrix-vector multiplication used in the first step is a purely formal property of those operations, independent of the nature of the entries. Now component ''i'' of this equation says that ''p''(''φ'')(''e<sub>i</sub>'') = 0 ∈ ''V''; thus ''p''(''φ'') vanishes on all ''e''<sub>''i''</sub>, and since these elements generate ''V'' it follows that ''p''(''φ'') = 0 ∈ End(''V''), completing the proof.\n\nOne additional fact that follows from this proof is that the matrix ''A'' whose characteristic polynomial is taken need not be identical to the value ''φ'' substituted into that polynomial; it suffices that ''φ'' be an endomorphism of ''V'' satisfying the initial equations\n\n:<math>\\varphi(e_i) = \\sum_j A_{j,i} e_j</math>\nfor ''some'' sequence of elements ''e''<sub>1</sub>,...,''e''<sub>''n''</sub> that generate ''V'' (which space might have smaller dimension than ''n'', or in case the ring ''R'' is not a field it might not be a [[free module]] at all).\n\n=== A bogus \"proof\": ''p''(''A'') = det(''AI''<sub>''n''</sub>&nbsp;−&nbsp;''A'') = det(''A''&nbsp;−&nbsp;''A'') = 0 ===\nOne persistent elementary but '''''incorrect''''' argument<ref>{{harvnb|Garrett|2007|p. 381}}</ref>  for the theorem is to \"simply\" take the definition\n:<math>p(\\lambda) = \\det(\\lambda I_n - A)</math>\nand substitute {{mvar|A}} for {{mvar|λ}}, obtaining\n:<math>p(A)=\\det(A I_n - A) = \\det(A - A) = 0~.</math>\n\nThere are many ways to see why this argument is wrong. First, in Cayley–Hamilton theorem, ''p''(''A'') is an ''n×n matrix''.  However, the right hand side of the above equation is the value of a determinant, which is a ''scalar''. So they cannot be equated unless ''n''&nbsp;=&nbsp;1 (i.e. ''A'' is just a scalar).  Second, in the expression <math>\\det(\\lambda I_n - A)</math>, the variable λ actually occurs at the diagonal entries of the matrix <math>\\lambda I_n - A</math>.  To illustrate, consider the characteristic polynomial in the previous example again:\n\n:<math>\\det\\begin{pmatrix}\\lambda-1&-2\\\\-3&\\lambda-4\\end{pmatrix}.</math>\n\nIf one substitutes the entire matrix ''A'' for ''λ'' in those positions, one obtains\n\n:<math>\\det\\begin{pmatrix} \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix} - 1 & -2 \\\\ -3 &\\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix} - 4\\end{pmatrix},</math>\n\nin which the \"matrix\" expression is simply not a valid one.  Note, however, that if scalar multiples of identity matrices\ninstead of scalars are subtracted in the above, i.e. if the substitution is performed as\n\n:<math> \\det \\begin{pmatrix} \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix} - I_2 & -2I_2 \\\\ -3I_2 &\\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix} - 4I_2 \\end{pmatrix},</math>\n\nthen the determinant is indeed zero, but the expanded matrix in question does not evaluate to <math>A I_n-A</math>; nor can its determinant (a scalar) be compared to ''p''(''A'') (a matrix).  So the argument that <math>p(A)=\\det(AI_n-A)=0</math> still does not apply.\n\nActually, if such an argument holds, it should also hold when other [[multilinear form]]s instead of determinant is used. For instance, if we consider the [[Permanent (mathematics)|permanent]] function and define <math>q(\\lambda) = \\operatorname{perm}(\\lambda I_n - A)</math>, then by the same argument, we should be able to \"prove\" that ''q''(''A'')&nbsp;=&nbsp;0.  But this statement is demonstrably wrong.  In the 2-dimensional case, for instance, the permanent of a matrix is given by\n\n:<math>\\operatorname{perm} \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} = ad + bc. </math>\n\nSo, for the matrix ''A'' in the previous example,\n\n:<math>\n\\begin{align}\nq(\\lambda) & = \\operatorname{perm} (\\lambda I_2 - A) = \\operatorname{perm} \\begin{pmatrix} \\lambda - 1 & -2  \\\\ -3 & \\lambda-4 \\end{pmatrix} \\\\[6pt]\n& = (\\lambda - 1)(\\lambda - 4) + (-2)(-3) = \\lambda^2 - 5\\lambda + 10.\n\\end{align}\n</math>\n\nYet one can verify that\n\n:<math>q(A)=A^2-5A+10I_2=12I_2\\not=0.</math>\n\nOne of the proofs for Cayley–Hamilton theorem above bears some similarity to the argument that <math>p(A)=\\det(AI_n-A)=0</math>. By introducing a matrix with non-numeric coefficients, one can actually let ''A'' live inside a matrix entry, but then <math>A I_n</math> is not equal to ''A'', and the conclusion is reached differently.\n==Proofs using methods of abstract algebra==\n\nBasic properties of [[Hasse–Schmidt derivation]]s on the [[exterior algebra]] <math>A = \\bigwedge M</math> of some ''B''-module ''M'' (supposed to be free and of finite rank) have been used by {{harvtxt|Gatto|Salehyan|2016|loc=§4}} to prove the Cayley–Hamilton theorem.\n==Abstraction and generalizations==\n\nThe above proofs show that the Cayley–Hamilton theorem holds for matrices with entries in any commutative ring ''R'', and that ''p''(''φ'') = 0 will hold whenever ''φ'' is an endomorphism of an ''R'' module generated by elements ''e''<sub>1</sub>,...,''e''<sub>''n''</sub> that satisfies\n\n:<math>\\varphi(e_j)=\\sum a_{ij}e_i, \\qquad j =1, \\ldots, n.</math>\n\nThis more general version of the theorem is the source of the celebrated [[Nakayama lemma]] in commutative algebra and algebraic geometry.\n\n==See also==\n* [[Companion matrix]]\n\n==Remarks==\n{{reflist|group=nb}}\n\n== Notes ==\n{{reflist|20em}}\n\n== References ==\n*{{cite journal|ref=harv|first1=Y.|last1=Alagös|first2=K.|last2=Oral|first3=S.|last3=Yüce|title=Split Quaternion Matrices|journal=Miskolc Mathematical Notes|issn=1787-2405|volume=13|issue=2|year=2012|pages=223&ndash;232|url=http://mat76.mat.uni-miskolc.hu/~mnotes/index.php?page=contents&volume=13&number=2|postscript=none}} (open access)\n*{{citation|last1 = Atiyah|first1 = M. F.|author1-link = M. F. Atiyah|last2 = MacDonald|first2 = I. G.|author2-link = I. G. Macdonald|year = 1969|title = Introduction to Commutative Algebra|publisher = Westview Press|isbn=978-0-201-40751-8}}\n*{{cite journal|ref=harv|first1=A. O.|last1=Barut|authorlink1=Asım Orhan Barut|first2=J. R.|last2=Zeni|first3=A.|last3=Laufer|journal=J. Phys. A: Math. Gen.|volume=27|pages=5239&ndash;5250|doi=10.1088/0305-4470/27/15/022|title=The exponential map for the conformal group O(2,4)|year=1994a|issue=15|url=http://iopscience.iop.org/0305-4470/27/15/022/|arxiv=hep-th/9408105|bibcode = 1994JPhA...27.5239B }}\n*{{cite journal|ref=harv|first1=A. O.|last1=Barut|authorlink1=Asım Orhan Barut|first2=J. R.|last2=Zeni|first3=A.|last3=Laufer|journal=J. Phys. A: Math. Gen.|volume=27|issue=20|pages=6799&ndash;6806|doi=10.1088/0305-4470/27/20/017|title=The exponential map for the unitary group SU(2,2)|year=1994b|url=http://iopscience.iop.org/0305-4470/27/20/017/|arxiv=hep-th/9408145|bibcode = 1994JPhA...27.6799B }}\n*{{cite book|ref=harv|last=Bhatia|first=R.|year=1997|title=Matrix Analysis|publisher=Springer|series=Graduate texts in mathematics|volume=169|isbn=978-0387948461}}\n*{{cite book|ref=harv|last= Brown|first= Lowell S.\n|title= Quantum Field Theory\n|publisher= [[Cambridge University Press]]\n|year= 1994|isbn= 978-0-521-46946-3 }}\n*{{cite journal|ref=harv|first=A.|last=Cayley|title=A Memoir on the Theory of Matrices|journal=Philos. Trans.|year=1858|volume=148|authorlink=Arthur Cayley}}\n*{{cite book|ref=harv|first=A.|last=Cayley|title=The Collected Mathematical Papers of Arthur Cayley|volume=2|publisher=Forgotten books|year=1889|asin=B008HUED9O|series=(Classic Reprint)}}\n*{{cite journal|ref=harv|last=Crilly|first=T.|journal=Notes Rec. R. Soc. Lond.|volume=52|year=1998|issue=2|pages=267–282|title=The young Arthur Cayley|doi=10.1098/rsnr.1998.0050}}\n*{{cite journal|ref=harv|last=Curtright|first = T L |last2=Fairlie|first2= D B|last3=Zachos|first3 = C K|year = 2014|title = A compact formula for rotations as spin matrix polynomials| journal =SIGMA| volume=10|issue=2014| page=084|doi=10.3842/SIGMA.2014.084|authorlink=David Fairlie|authorlink2=Thomas Curtright|authorlink3=Cosmas Zachos|arxiv =1402.3541|bibcode=2014SIGMA..10..084C}}\n*{{cite journal|ref=harv|first=G.|last=Frobenius|title=Ueber lineare Substutionen und bilineare Formen|journal=J. Reine Angew. Math. |volume=84|year=1878|pages=1–63|authorlink=Ferdinand Georg Frobenius}}\n*{{cite book|ref=harv| last= Gantmacher|first=F.R. | title=The Theory of Matrices  |year=1960| publisher= Chelsea Publishing|location= NY | isbn = 978-0-8218-1376-8 }}\n\n* {{Citation|author1=Gatto|first1=Letterio|author2=Salehyan|first2=Parham|title=Hasse–Schmidt derivations on Grassmann algebras|publisher=Springer|year=2016|isbn=978-3-319-31842-4|mr=3524604|doi=10.1007/978-3-319-31842-4}}\n\n*{{cite book|ref=harv| last= Garrett|first=Paul B. | title=Abstract Algebra  |year=2007| publisher= Chapman and Hall/CRC |location= NY | isbn = 978-1584886891 }}\n*{{cite book|ref=harv|first=W. R.|last=Hamilton|title=Lectures on Quaternions|location=Dublin|year=1853|authorlink=William Rowan Hamilton}}\n*{{cite journal|ref=harv|first=W. R.|last=Hamilton|title=On a New and General Method of Inverting a Linear and Quaternion Function of a Quaternion|journal=[[Proceedings of the Royal Irish Academy]]|volume=viii|year=1864a|pages=182&ndash;183}} (communicated on June 9, 1862)\n*{{cite journal|ref=harv|first=W. R.|last=Hamilton|title=On the Existence of a Symbolic and Biquadratic Equation, which is satisfied by the Symbol of Linear Operation in Quaternions|journal=[[Proceedings of the Royal Irish Academy]]|volume=viii|year=1864b|pages=190&ndash;101}} (communicated on June 23, 1862)\n*{{cite journal|ref=harv|first=S. H.|last=Hou|title=Classroom Note: A Simple Proof of the Leverrier--Faddeev Characteristic Polynomial Algorithm|journal=SIAM Review|doi=10.1137/S003614459732076X| volume=40 | issue=3|year=1998|pages=706–709|bibcode=1998SIAMR..40..706H}} [http://epubs.siam.org/doi/pdf/10.1137/S003614459732076X  \"Classroom Note: A Simple Proof of the Leverrier--Faddeev Characteristic Polynomial Algorithm\"] \n*{{cite journal|ref=harv|first=W. R.|last=Hamilton|title=On the Existence of a Symbolic and Biquadratic Equation which is satisfied by the Symbol of Linear or Distributive Operation on a Quaternion|journal=[[Philosophical Magazine|The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science]]|issn=1478-6435|volume=24|series= series '''iv'''|year=1862|pages=127&ndash;128|url=http://zs.thulb.uni-jena.de/rsc/viewer/jportal_derivate_00126615/PMS_1862_Bd24_%200135.tif|access-date=2015-02-14}}\n*{{cite book|ref=harv|first=Alston S.|last=Householder|title=The Theory of Matrices in Numerical Analysis |publisher=Dover Books on Mathematics|year=2006|authorlink=Alston Scott Householder | isbn=978-0486449722}}\n*{{cite journal|ref=harv|first=A.|last=Laufer|journal=J. Phys. A: Math. Gen.|volume=30|issue=15|pages=5455&ndash;5470|doi=10.1088/0305-4470/30/15/029|title=The exponential map of GL(N)|year=1997|url=http://iopscience.iop.org/0305-4470/30/15/029/|arxiv=hep-th/9604049|bibcode = 1997JPhA...30.5455L }}\n*{{cite journal|ref=harv|last=Tian|first=Y.|title=Matrix representations of octonions and their application|doi=10.1007/BF03042010|journal=[[Advances in Applied Clifford Algebras]]|issn=0188-7009|volume=10|issue=1|year=2000|pages=61&ndash;90|arxiv=math/0003166|citeseerx=10.1.1.237.2217}}\n*{{cite journal|ref=harv|last1=Zeni|first1=J. R.|first2=W.A.|last2=Rodrigues|title=A thoughful study of Lorentz transformations by Clifford algebras|journal=Int. J. Mod. Phys. A|volume=7|issue=8|pages=1793 pp|year=1992|doi= 10.1142/S0217751X92000776|bibcode = 1992IJMPA...7.1793Z }}\n*{{cite journal|ref=harv|last=Zhang|first=F.|title=Quaternions and matrices of quaternions|journal=Linear Algebra and its Applications|issn=0024-3795|volume=251|year=1997|pages=21&ndash;57|        doi=10.1016/0024-3795(95)00543-9|url=http://www.sciencedirect.com/science/article/pii/0024379595005439|via=[[ScienceDirect]]|postscript=none}} (open archive).\n\n==External links==\n*{{springer|title=Cayley–Hamilton theorem|id=p/c120080}}\n*[http://planetmath.org/?op=getobj&from=objects&id=7308 A proof from PlanetMath.]\n*[http://www.mathpages.com/home/kmath640/kmath640.htm The Cayley–Hamilton theorem at MathPages]\n\n{{DEFAULTSORT:Cayley-Hamilton Theorem}}\n[[Category:Theorems in linear algebra]]\n[[Category:Articles containing proofs]]\n[[Category:Matrix theory]]\n[[Category:William Rowan Hamilton]]"
    },
    {
      "title": "Chebotarev theorem on roots of unity",
      "url": "https://en.wikipedia.org/wiki/Chebotarev_theorem_on_roots_of_unity",
      "text": "{{Distinguish|Chebotarev's density theorem}}\n{{Orphan|date=May 2014}}\n\nThe '''Chebotarev theorem on roots of unity''' was originally a conjecture made by [[Alexander Ostrowski|Ostrowski]] in the context of [[Lacunary function|lacunary series]]. \n\n[[Nikolai Chebotaryov|Chebotarev]] was the first to prove it, in the 1930s. This proof involves tools from [[Galois extension|Galois theory]] and pleased [[Alexander Ostrowski|Ostrowski]], who made comments arguing that it \"''does meet the requirements of mathematical esthetics''\".<ref>Stevenhagen et al., 1996</ref>\nSeveral proofs have been proposed since,<ref>P.E. Frenkel, 2003</ref> and it has even been discovered independently by [[Jean Dieudonné|Dieudonné]].<ref>J. Dieudonné, 1970</ref>\n\n== Statement ==\n\nLet <math>\\Omega </math> be a matrix with entries <math> a_{ij} =\\omega^{ij},1\\leq i,j\\leq n </math>, where <math>\\omega =e^{2i\\pi / n},n\\in \\mathbb{N}</math>. \nIf <math>n</math> is prime then any minor of <math> \\Omega </math> is non-zero.\n\nEquivalently, all [[Submatrix|submatrices]] of a [[DFT matrix]] of prime length are invertible.\n\n== Applications ==\nIn [[signal processing]],<ref>Candès, Romberg, Tao, 2006</ref> the theorem was used by [[Terence Tao|T. Tao]] to extend the [[Uncertainty principle#Signal processing|uncertainty principle]].<ref>T. Tao, 2003</ref>\n\n== Notes ==\n{{reflist}}\n\n== References ==\n\n*{{cite journal\n | title=Chebotarev and his density theorem\n |author1=Stevenhagen, Peter  |author2=Lenstra, Hendrik W\n | journal=The Mathematical Intelligencer\n | volume=18 |issue=2 |pages=26–37\n | year=1996\n | doi =10.1007/BF03027290\n}}\n*{{cite arXiv\n | title=Simple proof of Chebotarev's theorem on roots of unity\n | author=Frenkel, PE\n | year=2003\n | arxiv=math/0312398\n}}\n\n*{{cite arXiv\n | title=An uncertainty principle for cyclic groups of prime order\n | author=Tao, Terence\n | year=2003\n | arxiv=math/0308286 \n}}\n*{{cite journal\n | title= Une propriété des racines de l'unité\n | author=Dieudonné,Jean\n | journal=Collection of articles dedicated to Alberto González Domınguez on his sixty-fifth birthday\n | year=1970\n}}\n\n*{{cite journal\n | title=Stable signal recovery from incomplete and inaccurate measurements\n |author1=Candes, Emmanuel J |author2=Romberg Justin K |author3=Tao, Terence | journal=Communications on Pure and Applied Mathematics\n | volume=59 |issue=8\n | pages=1207–1223\n | year=2006\n | arxiv=math/0503066\n | bibcode=2005math......3066C\n | doi=10.1002/cpa.20124\n}}\n<!-- This will add a notice to the bottom of the page and won't blank it! The new template which says that your draft is waiting for a review will appear at the bottom; simply ignore the old (grey) drafted templates and the old (red) decline templates. A bot will update your article submission. Until then, please don't change anything in this text box and press \"Save page\". -->\n\n[[Category:Theorems in linear algebra]]\n[[Category:Theorems in algebraic number theory]]"
    },
    {
      "title": "Crouzeix's theorem",
      "url": "https://en.wikipedia.org/wiki/Crouzeix%27s_theorem",
      "text": "#redirect [[Crouzeix's conjecture]]\n\n[[Category:Theorems in linear algebra]]"
    },
    {
      "title": "Dimension theorem for vector spaces",
      "url": "https://en.wikipedia.org/wiki/Dimension_theorem_for_vector_spaces",
      "text": "In [[mathematics]], the '''dimension theorem for vector spaces''' states that all [[Basis (linear algebra)|bases]] of a [[vector space]] have equally many elements. This number of elements may be finite or infinite (in the latter case, it is a [[cardinal number]]), and defines the [[Dimension (vector space)|dimension]] of the vector space.\n\nFormally, the dimension theorem for vector spaces states that\n\n:Given a vector space {{math|''V''}}, any two bases have the same [[cardinality]].\n\nAs a basis is a [[generating set]] that is [[linearly independent]], the theorem is a consequence of the following theorem, which is also useful:\n\n:In a vector space {{math|''V''}}, if {{mvar|G}} is a generating set, and {{mvar|I}} is a linearly independent set, then the cardinality of {{mvar|I}} is not larger than the cardinality of {{mvar|G}}.\n\nIn particular if {{math|''V''}} is [[finitely generated module|finitely generated]], then all its bases are finite and have the  same number of elements.\n\nWhile the proof of the existence of a basis for any vector space in the general case requires [[Zorn's lemma]] and is in fact equivalent to the [[axiom of choice]], the uniqueness of the cardinality of the basis requires only the [[ultrafilter lemma]],<ref>Howard, P., [[Jean E. Rubin|Rubin, J.]]: \"Consequences of the axiom of choice\" - Mathematical Surveys and Monographs, vol 59 (1998) {{issn|0076-5376}}.</ref> which is strictly weaker (the proof given below, however, assumes [[trichotomy (mathematics)|trichotomy]], i.e., that all [[cardinal number]]s are comparable, a statement which is also equivalent to the axiom of choice). The theorem can be generalized to arbitrary [[module (mathematics)|{{math|''R''}}-modules]] for rings {{math|''R''}} having [[invariant basis number]].\n\nIn the finitely generated case the proof uses only elementary arguments of [[algebra]], and does not require the axiom of choice nor its weaker variants.\n\n==Proof==\nLet {{mvar|V}} be a vector space, {{math|{''a''<sub>''i''</sub>: ''i'' ∈ ''I'' }}} be a [[linearly independent]] set of elements of {{mvar|V}}, and {{math|{''b''<sub>''j''</sub>: ''j'' ∈ ''J'' }}} be a [[generating set]]. One has to prove that the [[cardinality]] of {{mvar|I}} is not larger than that of {{mvar|J}}.\n\nIf {{mvar|J}} is finite, this results from the [[Steinitz exchange lemma]]. (Indeed, the [[Steinitz exchange lemma]] implies every finite subset of {{mvar|I}} has cardinality not larger than that of {{mvar|J}}, hence {{mvar|I}} is finite with cardinality not larger than that of {{mvar|J}}.) If {{mvar|J}} is finite, a proof based on matrix theory is also possible.<ref>Hoffman, K., Kunze, R., \"Linear Algebra\", 2nd ed., 1971, Prentice-Hall. (Theorem 4 of Chapter 2).</ref> \n\nAssume that {{math|''J''}} is infinite. If {{mvar|I}} is finite, there is nothing to prove. Thus, we may assume that {{mvar|I}} is also infinite. Let us suppose that the cardinality of {{mvar|I}} is larger than that of {{mvar|J}}.<ref group=note>This uses the axiom of choice.</ref> We have to prove that this leads to a contradiction. \n\nBy [[Zorn's lemma]], every linearly independent set is contained in a maximal linearly independent set {{mvar|K}}. This maximality implies that {{mvar|K}} spans {{mvar|V}} and is therefore a basis (the maximality implies that every element of {{mvar|V}} is linearly dependent from the elements of {{mvar|K}}, and therefore is a linear combination of elements of {{mvar|K}}. As the cardinality of {{mvar|K}} is greater or equal with the cardinality of {{mvar|I}}, one may replace {{mvar|I}} with {{mvar|K}}, that is, one may suppose, without loss of generality, that {{mvar|I}} is a basis. \n\nThus, every {{math|''b''<sub>''j''</sub>}} can be written as a finite sum\n:<math>\\textstyle b_j = \\sum_{i\\in E_j} \\lambda_{i,j} a_i, </math>\nwhere <math>E_j</math> is a finite subset of <math>I</math>.\nAs {{mvar|J}} is infinite, <math>\\textstyle\\bigcup_{j\\in J} E_j</math> has the same cardinality as {{mvar|J}}.<ref group=note>This uses the axiom of choice.</ref> Therefore <math>\\textstyle\\bigcup_{j\\in J} E_j</math> has cardinality smaller than that of {{mvar|I}}. \nSo there is some <math>i_0\\in I</math> which does not appear\nin any <math>E_j</math>.  The corresponding <math>a_{i_0}</math> can be expressed as a finite linear combination of <math>b_j</math>'s, which in turn can be expressed as finite linear combination of <math> a_i</math>'s, not involving <math>a_{i_0}</math>. Hence <math> a_{i_0}</math> is linearly dependent on the other <math>a_i</math>'s, which provides the desired contradiction.\n\n==Kernel extension theorem for vector spaces==\nThis application of the dimension theorem is sometimes itself called the ''dimension theorem''. Let\n\n:''T'': ''U'' → ''V''\n\nbe a [[linear transformation]]. Then\n\n:''dim''(''range''(''T'')) + ''dim''(''kernel''(''T'')) = ''dim''(''U''),\n\nthat is, the dimension of ''U'' is equal to the dimension of the transformation's [[Range (mathematics)|range]] plus the dimension of the [[Kernel (algebra)|kernel]]. See [[rank–nullity theorem]] for a fuller discussion.\n\n==Notes==\n{{reflist|group=note}}\n\n==References==\n<references />\n\n{{DEFAULTSORT:Dimension Theorem For Vector Spaces}}\n[[Category:Theorems in abstract algebra]]\n[[Category:Theorems in linear algebra]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "Fundamental theorem of linear algebra",
      "url": "https://en.wikipedia.org/wiki/Fundamental_theorem_of_linear_algebra",
      "text": "{{multiple issue|\n{{technical|date=February 2016}}\n{{inline|date=February 2016}}\n}}\nIn [[mathematics]], the '''[[fundamental theorem]] of [[linear algebra]]''' makes several statements regarding [[vector space]]s. Those statements may be given concretely in terms of the [[rank (linear algebra)|rank]] ''r'' of an {{nowrap|''m'' × ''n''}} [[matrix (mathematics)|matrix]] ''A'' and its [[singular value decomposition]]:\n\n:<math>A=U\\mathit\\Sigma V^\\mathrm{T}</math>\n\nFirst, each matrix <math>A \\in \\mathbb{R}^{m \\times n}</math> (<math> A </math> has <math>m</math> rows and <math>n</math> columns) induces four ''fundamental [[linear subspace|subspaces]]''. These ''fundamental subspaces'' are as follows:\n\n{| class=\"wikitable\"\n|-\n!name of subspace\n!definition\n!containing space\n![[Hamel dimension|dimension]]\n![[basis (linear algebra)|basis]]\n|-\n|[[column space]], range or [[image (mathematics)|image]]\n|<math>\\operatorname{im}(A)</math> or <math>\\operatorname{range} (A)</math>\n|<math>\\mathbb{R}^m</math>\n|<math>r</math> ([[Rank (linear algebra)|rank]])\n|The first <math>r</math> columns of <math>U</math>\n|-\n|[[nullspace]] or [[Kernel (matrix)|kernel]]\n|<math>\\ker(A)</math> or <math>\\operatorname{null} (A)</math>\n|<math>\\mathbb{R}^n</math>\n|<math>n - r</math> (nullity)\n|The last <math>(n - r)</math> columns of <math>V</math>\n|-\n|[[row space]] or [[coimage]]\n|<math>\\operatorname{im}(A^\\mathrm{T})</math> or <math>\\operatorname{range} (A^\\mathrm{T})</math>\n|<math>\\mathbb{R}^n</math>\n|<math>r</math> ([[Rank (linear algebra)|rank]])\n|The first <math>r</math> columns of <math>V</math>\n|-\n|[[left nullspace#Left null space|left nullspace]] or [[cokernel]]\n|<math>\\ker(A^\\mathrm{T})</math> or <math>\\operatorname{null} (A^\\mathrm{T})</math>\n|<math>\\mathbb{R}^m</math>\n|<math>m - r</math> (corank)\n|The last <math>(m - r)</math> columns of <math>U</math>\n|}\n\nSecondly:\n\n# In <math>\\mathbb{R}^n</math>, <math>\\ker(A) = (\\operatorname{im}(A^\\mathrm{T}))^\\perp</math>, that is, the nullspace is the [[orthogonal complement]] of the row space\n# In <math>\\mathbb{R}^m</math>, <math>\\ker(A^\\mathrm{T}) = (\\operatorname{im}(A))^\\perp</math>, that is, the left nullspace is the orthogonal complement of the column space.\n[[File:The four subspaces.svg|center|600px|thumb|The four subspaces associated to a matrix ''A''.]]\n\nThe dimensions of the subspaces are related by the [[rank–nullity theorem]], and follow from the above theorem.\n\nFurther, all these spaces are intrinsically defined—they do not require a choice of basis—in which case one rewrites this in terms of abstract vector spaces, operators, and the [[dual space]]s as <math>A\\colon V \\to W</math> and <math>A^* \\colon W^* \\to V^*</math>: the kernel and image of <math>A^*</math> are the cokernel and coimage of <math>A</math>.\n\n== See also ==\n* [[Rank–nullity theorem]]\n* [[Closed range theorem]]\n\n==References==\n* [[Gilbert Strang|Strang, Gilbert]]. ''Linear Algebra and Its Applications''. 3rd ed. Orlando: Saunders, 1988.\n*{{Citation\n| title = The fundamental theorem of linear algebra\n| url = http://www.eng.iastate.edu/~julied/classes/CE570/Notes/strangpaper.pdf <!-- if that goes dead try one of these:\nhttp://ocw.nctu.edu.tw/upload/fourier/supplement/fund%20linear%20algebra.pdf\nhttp://www.dm.unibo.it/~regonati/ad0708/strang-FTLA.pdf\n-->\n| year = 1993\n| author = Strang, Gilbert\n| journal = American Mathematical Monthly\n| volume = 100\n| issue = 9\n| pages = 848–855\n| doi = 10.2307/2324660\n| jstor = 2324660\n| citeseerx = 10.1.1.384.2309\n}}\n* {{Citation | last = Banerjee | first = Sudipto | last2 = Roy | first2 = Anindya | date = 2014 | title = Linear Algebra and Matrix Analysis for Statistics | series = Texts in Statistical Science | publisher = Chapman and Hall/CRC | edition =  1st | isbn =  978-1420095388}}\n\n==External links==\n*{{Commonscat-inline}}\n*{{aut|[[Gilbert Strang]]}}, [http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/lecture-10-the-four-fundamental-subspaces MIT Linear Algebra Lecture on the Four Fundamental Subspaces] on YouTube, from [[MIT OpenCourseWare]]\n\n{{Fundamental theorems}}\n{{Linear algebra}}\n\n[[Category:Theorems in linear algebra]]\n[[Category:Isomorphism theorems]]\n[[Category:Fundamental theorems|Linear algebra]]"
    },
    {
      "title": "Gerbaldi's theorem",
      "url": "https://en.wikipedia.org/wiki/Gerbaldi%27s_theorem",
      "text": "In [[linear algebra]] and [[projective geometry]], '''Gerbaldi's theorem''', proved by {{harvs|txt|last=Gerbaldi|authorlink=Francesco Gerbaldi|year=1882}}, states that one can find six pairwise apolar [[linearly independent]] nondegenerate ternary [[quadratic form]]s.  These are permuted by the [[Valentiner group]].\n\n==References==\n\n*{{Citation | last1=Gerbaldi | first1=Francesco | title= Sui gruppi di sei coniche in involuzione | language=Italian  | jfm=14.0537.02 | year=1882 | journal=Torino Atti | volume=XVII | pages=566–580}}\n\n[[Category:Quadratic forms]]\n[[Category:Theorems in linear algebra]]\n[[Category:Theorems in projective geometry]]"
    },
    {
      "title": "Goddard–Thorn theorem",
      "url": "https://en.wikipedia.org/wiki/Goddard%E2%80%93Thorn_theorem",
      "text": "{{Expert needed|Physics|reason=Too complicated, jargon needs explaining. Technical details should be explained later|date=June 2017}}\nIn [[mathematics]], and in particular, in the mathematical background of [[string theory]], the '''Goddard–Thorn theorem''' (also called the '''no-ghost theorem''') is a theorem describing properties of a [[functor]] that quantizes [[bosonic strings]]. It is named after [[Peter Goddard (physicist)|Peter Goddard]] and [[Charles Thorn]].\n\nThe name \"no-ghost theorem\" stems from the fact that in the original statement of the theorem, the natural [[inner product]] induced on the output vector space is positive definite. Thus, there were no so-called [[Ghosts (physics)|ghosts]] ([[Pauli–Villars ghost]]s), or vectors of negative norm. The name \"no-ghost theorem\" is also a word play on the [[no-go theorem]] of quantum mechanics.\n\n== Formalism ==\nThere are two naturally isomorphic functors that are typically used to quantize bosonic strings.  In both cases, one starts with positive-energy representations of the [[Virasoro algebra]] of central charge 26, equipped with Virasoro-invariant bilinear forms, and ends up with vector spaces equipped with bilinear forms.  Here, \"Virasoro-invariant\" means ''L<sub>n</sub>'' is adjoint to ''L''<sub>−''n''</sub> for all integers ''n''.\n\nThe first functor historically is \"old canonical quantization\", and it is given by taking the quotient of the weight 1 primary subspace by the radical of the bilinear form.  Here, \"primary subspace\" is the set of vectors annihilated by ''L<sub>n</sub>'' for all strictly positive ''n'', and \"weight 1\" means ''L''<sub>0</sub> acts by identity.  A second, naturally isomorphic functor, is given by degree 1 BRST cohomology.  Older treatments of BRST cohomology often have a shift in the degree due to a change in choice of BRST charge, so one may see degree −1/2 cohomology in papers and texts from before 1995.  A proof that the functors are naturally isomorphic can be found in Section 4.4 of Polchinski's ''String Theory'' text.\n\nThe Goddard–Thorn theorem amounts to the assertion that this quantization functor more or less cancels the addition of two free bosons, as conjectured by Lovelace in 1971.  Lovelace's precise claim was that at critical dimension 26, Virasoro-type Ward identities cancel two full sets of oscillators.  Mathematically, this is the following claim:\n\nLet ''V'' be a unitarizable Virasoro representation of central charge 24 with Virasoro-invariant bilinear form, and let π<sup>1,1</sup><sub>λ</sub> be the irreducible module of the '''R'''<sup>1,1</sup> Heisenberg Lie algebra attached to a nonzero vector λ in '''R'''<sup>1,1</sup>.  Then the image of ''V'' ⊗ π<sup>1,1</sup><sub>λ</sub> under quantization is canonically isomorphic to the subspace of V on which ''L''<sub>0</sub> acts by 1-(λ,λ).\n\nThe no-ghost property follows immediately, since the positive-definite Hermitian structure of ''V'' is transferred to the image under quantization.\n\n==Applications==\nThe bosonic string quantization functors described here can be applied to any conformal vertex algebra of central charge 26, and the output naturally has a Lie algebra structure.  The Goddard–Thorn theorem can then be applied to concretely describe the Lie algebra in terms of the input vertex algebra.\n\nPerhaps the most spectacular case of this application is Borcherds's proof of the [[Monstrous Moonshine]] conjecture, where the unitarizable Virasoro representation is the [[Monster vertex algebra]] (also called \"Moonshine module\") constructed by Frenkel, Lepowsky, and Meurman.  By taking a tensor product with the vertex algebra attached to a rank 2 hyperbolic lattice, and applying quantization, one obtains the [[monster Lie algebra]], which is a [[generalized Kac–Moody algebra]] graded by the lattice.  By using the Goddard–Thorn theorem, Borcherds showed that the homogeneous pieces of the Lie algebra are naturally isomorphic to graded pieces of the Moonshine module, as representations of the [[monster simple group]].\n\nEarlier applications include Frenkel's determination of upper bounds on the root multiplicities of the Kac-Moody Lie algebra whose Dynkin diagram is the [[Leech lattice]], and Borcherds's construction of a generalized Kac-Moody Lie algebra that contains Frenkel's Lie algebra and saturates Frenkel's 1/∆ bound.\n\n== References ==\n\n* R. Borcherds, ''The Monster Lie algebra'' Adv. Math. 83 no. 1 (1990) 30–47.\n* R. Borcherds, ''[http://math.berkeley.edu/%7Ereb/papers/monster/monster.pdf Monstrous moonshine and monstrous Lie superalgebras]'' Invent. Math. 109 (1992), 405–444.\n* I. Frenkel, ''Representations of Kac-Moody algebras and dual resonance models'' Applications of group theory in theoretical physics, Lect. Appl. Math. 21 A.M.S. (1985) 325–353.\n* P. Goddard and C. B. Thorn, ''[http://preprints.cern.ch/cgi-bin/setlink?base=preprint&categ=CM-P&id=CM-P00058839 Compatibility of the dual Pomeron with unitarity and the absence of ghosts in the dual resonance model]'', Phys. Lett., B 40, No. 2 (1972), 235-238.\n* C. Lovelace ''Pomeron form factors and dual regge cuts'' Phys. Lett. 34B (1971) 500–506.\n* J. Polchinski, ''String Theory'' Cambridge University Press, Cambridge, UK (1998).\n\n{{DEFAULTSORT:Goddard-Thorn theorem}}\n[[Category:Theorems in linear algebra]]\n[[Category:String theory]]\n[[Category:Theorems in mathematical physics]]"
    },
    {
      "title": "Hawkins–Simon condition",
      "url": "https://en.wikipedia.org/wiki/Hawkins%E2%80%93Simon_condition",
      "text": "The '''Hawkins–Simon condition''' refers to a result in [[mathematical economics]], attributed to [[David Hawkins (philosopher)|David Hawkins]] and [[Herbert A. Simon]],<ref>{{cite journal |title=Some Conditions of Macroeconomic Stability |first=David |last=Hawkins |first2=Herbert A. |last2=Simon |journal=[[Econometrica]] |volume=17 |issue=3/4 |year=1949 |pages=245–248 |jstor=1905526 }}</ref> that guarantees the existence of a non-negative output vector that solves the [[Economic equilibrium|equilibrium]] relation in the [[input–output model]] where [[Supply and demand|demand equals supply]]. More precisely, it states a condition for <math>[\\mathbf{I} - \\mathbf{A}]</math> under which the input–output system\n:<math>[\\mathbf{I} - \\mathbf{A}] \\cdot \\mathbf{x} = \\mathbf{d}</math>\nhas a solution <math>\\mathbf{\\hat{x}} \\geq 0</math> for any <math>\\mathbf{d} \\geq 0</math>. Here <math>\\mathbf{I}</math> is the [[identity matrix]] and <math>\\mathbf{A}</math> is called the ''input–output matrix'' or ''Leontief matrix'' after [[Wassily Leontief]], who empirically estimated it in the 1940s.<ref>{{cite book |first=Wassily |last=Leontief |title=Input-Output Economics |location=New York |publisher=Oxford University Press |edition=2nd |year=1986 |isbn=0-19-503525-9 }}</ref> Together, they describe a system in which\n:<math>\\sum_{j=1}^{n} a_{ij} x_{j} + d_{i} = x_{i} \\quad i = 1, 2, \\ldots, n</math>\nwhere <math>a_{ij}</math> is the amount of the ''i''th good used to produce one unit of the ''j''th good, <math>x_{j}</math> is the amount of the ''j''th good produced, and <math>d_{i}</math> is the amount of final demand for good ''i''. Rearranged and written in vector notation, this gives the first equation.\n\nDefine <math>[\\mathbf{I} - \\mathbf{A}] = \\mathbf{B}</math>, where <math>\\mathbf{B} = \\left[ b_{ij} \\right]</math> is an <math>n \\times n</math> matrix with <math>b_{ij} \\leq 0, i \\neq j</math>   <ref>{{cite book |first=Hukukane |last=Nikaido |title=Convex Structures and Economic Theory |location= |publisher=Academic Press |year=1968 |isbn= |pages=90–92 |url=https://books.google.com/books?id=NMVgDAAAQBAJ&pg=PA90 }}</ref>. Then the '''Hawkins–Simon theorem''' states that the following two conditions are equivalent\n:(i) There exists an <math>\\mathbf{x} \\geq 0</math> such that <math>\\mathbf{B} \\cdot \\mathbf{x} > 0</math>.\n:(ii) All the successive [[Minor (linear algebra)|leading principal minors]] of <math>\\mathbf{B}</math> are positive, that is\n::<math>b_{11} > 0, \\begin{vmatrix} b_{11} & b_{12} \\\\ b_{21} & b_{22} \\end{vmatrix} > 0, \\ldots, \\begin{vmatrix} b_{11} & b_{12} & \\dots & b_{1n} \\\\ b_{21} & b_{22} & \\dots & b_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ b_{n1} & b_{n2} & \\dots & b_{nn} \\end{vmatrix} > 0</math>\n\nFor a proof, see [[Michio Morishima|Morishima]] (1964),<ref>{{cite book |first=Michio |last=Morishima |title=Equilibrium, Stability, and Growth: A Multi-sectoral Analysis |location=London |publisher=Oxford University Press |year=1964 |isbn= |pages=15–17 |url=https://books.google.com/books?id=m-cjAAAAMAAJ&pg=PA15 }}</ref> [[Hukukane Nikaido|Nikaido]] (1968),<ref>{{cite book |first=Hukukane |last=Nikaido |title=Convex Structures and Economic Theory |location= |publisher=Academic Press |year=1968 |isbn= |pages=90–92 |url=https://books.google.com/books?id=NMVgDAAAQBAJ&pg=PA90 }}</ref> or Murata (1977).<ref>{{cite book |first=Yasuo |last=Murata |title=Mathematics for Stability and Optimization of Economic Systems |location=New York |publisher=Academic Press |year=1977 |pages=52–53 |url=https://books.google.com/books?id=5DOjBQAAQBAJ&pg=PA52 }}</ref> Condition (ii) is known as '''Hawkins–Simon condition'''. This theorem was [[Multiple discovery|independently discovered]] by [[David Kotelyanskiĭ]],<ref>{{cite journal |first=D. M. |last=Kotelyanskiĭ |title=О некоторых свойствах матриц с положительными элементами  |trans-title=On Some Properties of Matrices with Positive Elements |journal=[[Matematicheskii Sbornik|Mat. Sb.]] |series=N.S. |year=1952 |volume=31 |issue=3 |pages=497–506 |url=http://www.mathnet.ru/links/75fc598ee896f36e1c2671f38c1e778d/sm5545.pdf }}</ref> as it is referred to by [[Felix Gantmacher]] as '''Kotelyanskiĭ lemma'''.<ref>{{cite book |first=Felix |last=Gantmacher |title=The Theory of Matrices |volume=2 |location=New York |publisher=Chelsea |year=1959 |pages=71–73 |url=https://books.google.com/books?id=cyX32q8ZP5cC&pg=PA71 }}</ref>\n\n== See also ==\n* [[Diagonally dominant matrix]]\n* [[Perron–Frobenius theorem]]\n\n== References ==\n{{Reflist}}\n\n== Further reading ==\n* {{cite book |first=Lionel |last=McKenzie |authorlink=Lionel McKenzie |chapter=Matrices with Dominant Diagonals and Economic Theory |title=Mathematical Methods in the Social Sciences |editor-first=Kenneth J. |editor-last=Arrow |editorlink=Kenneth Arrow |editor2-first=Samuel |editor2-last=Karlin |editor2link=Samuel Karlin |editor3-first=Patrick |editor3-last=Suppes |editor3link=Patrick Suppes |publisher=Stanford University Press |year=1960 |pages=47–62 |oclc=25792438 }}\n* {{cite book |first=Akira |last=Takayama |chapter=Frobenius Theorems, Dominant Diagonal Matrices, and Applications |title=Mathematical Economics |location=New York |publisher=Cambridge University Press |edition=Second |year=1985 |pages=359–409 |chapterurl=https://books.google.com/books?id=j6PLOBFotPQC&pg=PA359 }}\n\n{{DEFAULTSORT:Hawkins-Simon condition}}\n[[Category:Theorems in linear algebra]]"
    },
    {
      "title": "MacMahon Master theorem",
      "url": "https://en.wikipedia.org/wiki/MacMahon_Master_theorem",
      "text": "In mathematics, the '''MacMahon Master theorem''' ('''MMT''') is a result in [[enumerative combinatorics]] and [[linear algebra]].  It was discovered by [[Percy Alexander MacMahon|Percy MacMahon]] and proved in his monograph ''Combinatory analysis'' (1916).  It is often used to derive binomial identities, most notably [[Dixon's identity]].\n\n== Background ==\nIn the monograph, MacMahon found so many applications of his result, he called it \"a master theorem in the Theory of Permutations.\"  He explained the title as follows: \"a Master Theorem from the masterly and rapid fashion in which it deals with various questions otherwise troublesome to solve.\"\n\nThe result was re-derived (with attribution) a number of times, most notably by  [[I. J. Good]] who derived it from his multilinear generalization of the [[Lagrange inversion theorem]].  MMT was also popularized by [[Leonard Carlitz|Carlitz]] who found an [[Exponential generating function#Exponential generating function|exponential]] [[power series]]  version.  In 1962, Good found a short proof of Dixon's identity from MMT.  In 1969, [[Pierre Cartier (mathematician)|Cartier]] and [[Dominique Foata|Foata]] found a new proof of MMT by combining [[algebra]]ic and [[bijective proof|bijective]] ideas (built on Foata's thesis) and further applications to [[combinatorics on words]], introducing the concept of [[trace monoid|trace]]s.  Since then, MMT has become a standard tool in enumerative combinatorics.\n\nAlthough various ''q''-Dixon identities have been known for decades, except for a Krattenthaler–Schlosser extension (1999), the proper [[q-analog]] of MMT remained elusive.  After Garoufalidis–Lê–Zeilberger's [[Quantum algebra|quantum]] extension (2006), a number of [[Noncommutative geometry|noncommutative]] extensions were developed by Foata–Han, Konvalinka–Pak, and Etingof–Pak.  Further connections to [[Koszul algebra]] and [[quasideterminant]]s were also found by Hai–Lorentz, Hai–Kriegk–Lorenz, Konvalinka–Pak, and others.\n\nFinally, according to J. D. Louck, [[theoretical physicist]] [[Julian Schwinger]] re-discovered the MMT in the context of his [[generating function]] approach to the [[angular momentum]] theory of [[many-particle system]]s.  Louck writes:\n\n{{quote|It is the MacMahon Master Theorem that unifies the angular momentum properties of composite systems in the binary build-up of such systems from more elementary constituents.<ref>{{cite book|last1=Louck|first1=James D.|title=Unitary symmetry and combinatorics|date=2008|publisher=World Scientific|location=Singapore|isbn=978-981-281-472-2|pages=viii}}</ref>}}\n\n== Precise statement ==\nLet <math>A = (a_{ij})_{m\\times m}</math> be a complex matrix, and let <math>x_1,\\ldots,x_m</math> be formal variables.  Consider a [[coefficient]]\n:<math>\nG(k_1,\\dots,k_m) \\, = \\, \\bigl[x_1^{k_1}\\cdots x_m^{k_m}\\bigr] \\,\n\\prod_{i=1}^m \\bigl(a_{i1}x_1 + \\dots + a_{im}x_m \\bigl)^{k_i}.\n</math>\n(Here the notation <math>[f]g</math> means \"the coefficient of monomial <math>f</math> in <math>g</math>\".)  Let <math>t_1,\\ldots,t_m</math> be another set of formal variables, and let <math>T = (\\delta_{ij}t_i)_{m\\times m}</math> be a [[diagonal matrix]].  Then\n:<math>\n\\sum_{(k_1,\\dots,k_m)} G(k_1,\\dots,k_m) \\, t_1^{k_1}\\cdots t_m^{k_m} \\, = \\,\n\\frac{1}{\\det (I_m - TA)},\n</math>\nwhere the sum runs over all nonnegative integer vectors <math>(k_1,\\dots,k_m)</math>,\nand <math>I_m</math> denotes the [[identity matrix]] of size <math>m</math>.\n\n== Derivation of Dixon's identity ==\nConsider a matrix\n:<math>\nA = \\begin{pmatrix}\n0 & 1 & -1 \\\\\n-1 & 0 & 1 \\\\\n1 & -1 & 0\n\\end{pmatrix}.\n</math>\nCompute the coefficients ''G''(2''n'',&nbsp;2''n'',&nbsp;2''n'') directly from the definition:\n\n:<math>\n\\begin{align}\nG(2n,2n,2n) & = \\bigl[x_1^{2n}x_2^{2n}x_3^{2n}\\bigl] (x_2 - x_3)^{2n} (x_3 - x_1)^{2n} (x_1 - x_2)^{2n} \\\\[6pt]\n& = \\, \\sum_{k=0}^{2n} (-1)^k \\binom{2n}{k}^3,\n\\end{align}\n</math>\n\nwhere the last equality follows from the fact that on the right-hand side we have the product of the following coefficients:\n:<math>[x_2^k x_3^{2n-k}](x_2 - x_3)^{2n}, \\ \\  [x_3^k x_1^{2n-k}](x_3 - x_1)^{2n}, \\ \\  [x_1^k x_2^{2n-k}](x_1 - x_2)^{2n},</math>\nwhich are computed from the [[binomial theorem]]. On the other hand, we can compute the [[determinant]] explicitly:\n:<math>\n\\det(I - TA) \\, = \\, \\det \\begin{pmatrix}\n1 & -t_1 & t_1 \\\\\nt_2 & 1 & -t_2 \\\\\n-t_3 & t_3 & 1\n\\end{pmatrix}  \\, = \\, 1 + \\bigl(t_1 t_2 + t_1 t_3 +t_2t_3\\bigr).\n</math>\nTherefore, by the MMT, we have a new formula for the same coefficients:\n\n: <math>\n\\begin{align}\nG(2n,2n,2n) & = \\bigl[t_1^{2n}t_2^{2n}t_3^{2n}\\bigl] (-1)^{3n} \\bigl(t_1 t_2 + t_1 t_3 +t_2t_3\\bigr)^{3n} \\\\[6pt]\n& = (-1)^{n} \\binom{3n}{n,n,n},\n\\end{align}\n</math>\n\nwhere the last equality follows from the fact that we need to use an equal number of times all three terms in the power.  Now equating the two formulas for coefficients ''G''(2''n'',&nbsp;2''n'',&nbsp;2''n'') we obtain an equivalent version of Dixon's identity:\n\n:<math> \\sum_{k=0}^{2n} (-1)^k \\binom{2n}{k}^3 = (-1)^{n} \\binom{3n}{n,n,n}.\n</math>\n\n==See also==\n*[[Permanent (mathematics)|Permanent]]\n\n== References ==\n{{Reflist}}\n* P.A. MacMahon, ''[http://www.hti.umich.edu/cgi/t/text/text-idx?c=umhistmath;idno=ABU9009 Combinatory analysis]'', vols 1 and 2, Cambridge University Press, 1915–16.\n* {{cite journal | zbl=0108.25104  | authorlink=I. J. Good | first=I.J. | last=Good | title=A short proof of MacMahon's ‘Master Theorem’ | journal=[[Proc. Cambridge Philos. Soc.]] | volume=58 | year=1962 | page=160 }}\n* {{cite journal | zbl=0108.25105  | authorlink=I. J. Good | first=I.J. | last=Good | title=Proofs of some `binomial' identities by means of MacMahon's ‘Master Theorem’ | journal=[[Proc. Cambridge Philos. Soc.]] | volume=58 | year=1962 | pages=161–162 }}\n* [[Pierre Cartier (mathematician)|P. Cartier]] and D. Foata, [http://www.mat.univie.ac.at/~slc/books/cartfoa.html Problèmes combinatoires de commutation et réarrangements], ''Lecture Notes in Mathematics'', no. 85, Springer, Berlin, 1969.\n* [[Leonard Carlitz|L. Carlitz]], An Application of MacMahon's Master Theorem, ''SIAM Journal on Applied Mathematics'' 26 (1974), 431–436.\n* I.P. Goulden and [[David M. Jackson|D. M. Jackson]], ''Combinatorial Enumeration'', John Wiley, New York, 1983.\n* C. Krattenthaler and M. Schlosser, [http://radon.mat.univie.ac.at/users/kratt/public_html/artikel/minv.ps.gz A new multidimensional matrix inverse with applications to multiple ''q''-series], ''Discrete Math.'' 204 (1999), 249–279.\n* S. Garoufalidis, T. T. Q. Lê and [[Doron Zeilberger|D. Zeilberger]], [http://www.pnas.org/content/103/38/13928.full The Quantum MacMahon Master Theorem], ''Proc. Natl. Acad. of Sci.'' 103  (2006),  no. 38, 13928–13931 ([https://arxiv.org/abs/math/0303319 eprint]).\n* M. Konvalinka and [[Igor Pak|I. Pak]], Non-commutative extensions of the MacMahon Master Theorem, ''Adv. Math.'' 216 (2007), no. 1. ([https://arxiv.org/abs/math/0607737 eprint]).\n* D. Foata and G.-N. Han, A new proof of the Garoufalidis-Lê-Zeilberger Quantum MacMahon Master Theorem,  ''J. Algebra''  307  (2007),  no. 1, 424–431 ([https://arxiv.org/abs/math/0603464 eprint]).\n* D. Foata and G.-N. Han, Specializations and extensions of the quantum MacMahon Master Theorem, ''Linear Algebra Appl'' 423  (2007),  no. 2–3, 445–455 ([https://arxiv.org/abs/math.CO/0603466 eprint]).\n* P.H. Hai and M. Lorenz, Koszul algebras and the quantum MacMahon master theorem,  ''Bull. Lond. Math. Soc.''  39  (2007),  no. 4, 667–676. ([https://arxiv.org/abs/math/0603169 eprint]).\n* P. Etingof and I. Pak, An algebraic extension of the MacMahon master theorem,  ''Proc. Amer. Math. Soc.''  136  (2008),  no. 7, 2279–2288 ([https://arxiv.org/abs/math/0608005  eprint]).\n* P.H. Hai, B. Kriegk and M. Lorenz, ''N''-homogeneous superalgebras, ''J. Noncommut. Geom.'' 2 (2008) 1–51 ([https://arxiv.org/abs/0704.1888 eprint]).\n* J.D. Louck, ''Unitary symmetry and combinatorics'', World Sci., Hackensack, NJ, 2008.\n\n[[Category:Enumerative combinatorics]]\n[[Category:Factorial and binomial topics]]\n[[Category:Articles containing proofs]]\n[[Category:Theorems in combinatorics]]\n[[Category:Theorems in linear algebra]]"
    },
    {
      "title": "Perron–Frobenius theorem",
      "url": "https://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem",
      "text": "In [[linear algebra]], the '''Perron–Frobenius theorem''', proved by {{harvs|txt|authorlink=Oskar Perron|first=Oskar|last= Perron|year=1907}} and {{harvs|txt|authorlink=Georg Frobenius|first=Georg |last=Frobenius|year=1912}}, asserts that a [[real square matrix]] with positive entries has a unique largest real [[eigenvalue]] and that the corresponding [[eigenvector]] can be chosen to have strictly positive components, and also asserts a similar statement for certain classes of nonnegative matrices. This theorem has important applications to probability theory ([[ergodicity]] of [[Markov chain]]s); to the theory of [[dynamical systems]] ([[subshifts of finite type]]); to economics ([[Okishio's theorem]],<ref>{{Cite journal|last=Bowles|first=Samuel|date=1981-06-01|title=Technical change and the profit rate: a simple proof of the Okishio theorem|url=https://academic.oup.com/cje/article/1699192/Technical|journal=Cambridge Journal of Economics|language=en|volume=5|issue=2|pages=183–186|doi=10.1093/oxfordjournals.cje.a035479|issn=0309-166X}}</ref> [[Hawkins–Simon condition]]<ref name=\"Meyer681\">{{harvnb|Meyer|2000|pp=[http://www.matrixanalysis.com/Chapter8.pdf 8.3.6 p. 681] {{cite web |url=http://www.matrixanalysis.com/Chapter8.pdf |title=Archived copy |accessdate=2010-03-07 |deadurl=yes |archiveurl=https://web.archive.org/web/20100307021652/http://www.matrixanalysis.com/Chapter8.pdf |archivedate=March 7, 2010 |df= }}}}</ref>);\nto demography ([[Leslie matrix|Leslie population age distribution model]]);<ref name=\"Meyer683\">{{harvnb|Meyer|2000|pp=[http://www.matrixanalysis.com/Chapter8.pdf 8.3.7 p. 683] {{cite web |url=http://www.matrixanalysis.com/Chapter8.pdf |title=Archived copy |accessdate=2010-03-07 |deadurl=yes |archiveurl=https://web.archive.org/web/20100307021652/http://www.matrixanalysis.com/Chapter8.pdf |archivedate=March 7, 2010 |df= }}}}</ref> \nto social networks ([[DeGroot learning|DeGroot learning process]]), to  [[PageRank|Internet search engines]]<ref name=\"LangvilleMeyer167\">{{harvnb|Langville|Meyer|2006|p=[https://books.google.com/books?id=hxvB14-I0twC&lpg=PP1&dq=isbn%3A0691122024&pg=PA167#v=onepage&q&f=false 15.2 p. 167]}} {{cite book |url=https://books.google.com/books?id=hxvB14-I0twC&lpg=PP1&dq=isbn%3A0691122024&pg=PA167 |title=Google's PageRank and Beyond: The Science of Search Engine Rankings |accessdate=2016-10-31 |deadurl=bot: unknown |archiveurl=https://web.archive.org/web/20140710041730/https://books.google.com/books?id=hxvB14-I0twC&lpg=PP1&dq=isbn%3A0691122024&pg=PA167#v=onepage&q&f=false |archivedate=July 10, 2014 |df= |isbn=978-0691122021 |last1=Langville |first1=Amy N. |last2=Langville |first2=Amy N. |last3=Meyer |first3=Carl D. |date=2006-07-23 }}</ref> and even to ranking of football\nteams.<ref name=\"Keener80\">{{harvnb|Keener|1993|p=[https://www.jstor.org/stable/2132526 p. 80]}}</ref> The first to discuss the ordering of players within tournaments using Perron–Frobenius eigenvectors is [[Edmund Landau]].<ref>{{citation | title = Zur relativen Wertbemessung der Turnierresultaten| pages = 366–369| volume = XI | number =  |  journal = Deutsches Wochenschach | year=1895 | first1=Edmund | last1=Landau |  url = | doi= }}</ref>\n<ref>{{citation | title = Über Preisverteilung bei Spielturnieren| pages = 192–202| volume = 63 | number =  |  journal =Zeitschrift für Mathematik und Physik | year=1915 | first1=Edmund | last1=Landau |  url = http://iris.univ-lille1.fr/handle/1908/2031 | doi= }}</ref>\n\n==Statement==\nLet '''positive''' and '''non-negative''' respectively describe [[matrix (mathematics)|matrices]] with exclusively [[positive number|positive]] real numbers as elements and matrices with exclusively non-negative real numbers as elements. The [[eigenvalue]]s of a real [[square matrix]] ''A'' are [[complex numbers]] that make up the [[spectrum of a matrix|spectrum]] of the matrix. The [[exponential growth|exponential growth rate]] of the matrix powers ''A''<sup>''k''</sup> as ''k'' → ∞ is controlled by the eigenvalue of ''A'' with the largest [[absolute value]] ([[Absolute value|modulus]]). The Perron–Frobenius theorem describes the properties of the leading eigenvalue and of the corresponding eigenvectors when ''A'' is a non-negative real square matrix. Early results were due to {{harvs|txt|authorlink=Oskar Perron|first=Oskar|last= Perron|year=1907}} and concerned positive matrices. Later, {{harvs|txt|authorlink=Georg Frobenius|first=Georg |last=Frobenius|year=1912}} found their extension to certain classes of non-negative matrices.\n\n===Positive matrices===\nLet <math>A = (a_{ij}) </math> be an <math> n \\times n </math> positive matrix: <math> a_{ij} > 0 </math> for <math> 1 \\le i,j \\le n </math>. Then the following statements hold.\n\n# There is a positive real number ''r'', called the '''Perron root''' or the '''Perron–Frobenius eigenvalue''' (also called the '''leading eigenvalue''' or '''dominant eigenvalue'''), such that ''r'' is an eigenvalue of ''A'' and any other eigenvalue ''λ'' (possibly, [[complex number|complex]]) in [[absolute value]] is strictly smaller than ''r'' , |''λ''| < ''r''. Thus, the [[spectral radius]] <math>\\rho(A) </math> is equal to ''r''. If the matrix coefficients are algebraic, this implies that the eigenvalue is a [[Perron number]].\n# The Perron–Frobenius eigenvalue is simple: ''r'' is a simple root of the [[characteristic polynomial]] of ''A''. Consequently, the [[eigenspace]] associated to ''r'' is one-dimensional. (The same is true for the left eigenspace, i.e., the eigenspace for ''A<sup>T</sup>'', the transpose of ''A''.)\n# There exists an eigenvector ''v'' = (''v''<sub>1</sub>,…,''v''<sub>''n''</sub>) of ''A'' with eigenvalue ''r'' such that all components of ''v'' are positive: ''A v'' = ''r v'', ''v''<sub>''i''</sub> > 0 for 1 ≤ ''i'' ≤ ''n''. (Respectively, there exists a positive left eigenvector ''w'' : ''w<sup>T</sup> A'' = ''r w<sup>T</sup>'', ''w''<sub>''i''</sub> > 0.) It is known in the literature under many variations as the '''Perron vector''', '''Perron eigenvector''', '''Perron-Frobenius eigenvector''', '''leading eigenvector''', or '''dominant eigenvector'''. \n# There are no other positive (moreover non-negative) eigenvectors except positive multiples of ''v'' (respectively, left eigenvectors except ''w''), i.e., all other eigenvectors must have at least one negative or non-real component.\n# <math> \\lim_{k \\rightarrow \\infty} A^k/r^k = v w^T</math>, where the left and right eigenvectors for ''A'' are normalized so that ''w<sup>T</sup>v'' = 1. Moreover, the matrix ''v w<sup>T</sup>'' is the [[Jordan canonical form#Invariant subspace decompositions|projection onto the eigenspace]] corresponding to&nbsp;''r''. This projection is called the '''Perron projection'''.\n# '''[[Lothar Collatz|Collatz]]–Wielandt formula''': for all non-negative non-zero vectors ''x'', let ''f''(''x'') be the minimum value of [''Ax'']<sub>''i''</sub> / ''x''<sub>''i''</sub> taken over all those ''i'' such that ''x<sub>i</sub>'' ≠ 0. Then ''f'' is a real valued function whose [[maximum]] over all non-negative non-zero vectors ''x'' is the Perron–Frobenius eigenvalue.\n# A \"Min-max\" Collatz–Wielandt formula takes a form similar to the one above: for all strictly positive vectors ''x'', let ''g''(''x'') be the maximum value of [''Ax'']<sub>''i''</sub> / ''x''<sub>''i''</sub> taken over ''i''. Then ''g'' is a real valued function whose [[minimum]] over all strictly positive vectors ''x'' is the Perron–Frobenius eigenvalue.\n# '''[[Monroe D. Donsker|Donsker]]–[[S. R. Srinivasa Varadhan|Varadhan]]–Friedland formula''': Let ''p'' be a probability vector and ''x'' a strictly positive vector.  Then <math>r = \\sup_p \\inf_{x>0} \\sum_{i=1}^n p_i[Ax]_i/x_i.</math> <ref>Donsker, M.D. and Varadhan, S.S., 1975. On a variational formula for the principal eigenvalue for operators with maximum principle. Proceedings of the National Academy of Sciences, 72(3), pp.780-783.</ref><ref>Friedland, S., 1981. Convex spectral functions. Linear and multilinear algebra, 9(4), pp.299-316.</ref>\n# The Perron–Frobenius eigenvalue satisfies the inequalities\n::<math>\\min_i \\sum_{j} a_{ij} \\le r \\le \\max_i \\sum_{j} a_{ij}.</math>\n\nThese claims can be found in Meyer<ref name=\"Meyer\"/> [https://web.archive.org/web/20100307021652/http://www.matrixanalysis.com/Chapter8.pdf chapter 8] claims 8.2.11–15 page 667 and exercises 8.2.5,7,9 pages 668–669.\n\nThe left and right eigenvectors ''w'' and ''v'' are sometimes normalized so that the sum of their components is equal to 1; in this case, they are sometimes called '''stochastic eigenvectors'''.  Often they are normalized so that the right eigenvector ''v'' sums to one, while <math>w^T v=1</math>.\n\n===Non-negative matrices===\nAn extension of the theorem to matrices with non-negative entries is also available. In order to highlight the similarities and differences between the two cases the following points are to be noted: every non-negative matrix can be obtained as a limit of positive matrices, thus one obtains the existence of an eigenvector with non-negative components; the corresponding eigenvalue will be non-negative and greater than '''or equal''', in absolute value, to all other eigenvalues.<ref>{{harvnb|Meyer|2000|pp=[http://www.matrixanalysis.com/Chapter8.pdf chapter 8.3 page 670] {{cite web |url=http://www.matrixanalysis.com/Chapter8.pdf |title=Archived copy |accessdate=2010-03-07 |deadurl=yes |archiveurl=https://web.archive.org/web/20100307021652/http://www.matrixanalysis.com/Chapter8.pdf |archivedate=March 7, 2010 |df= }}}}</ref><ref>{{harvnb|Gantmacher|2000|p=[https://books.google.com/books?id=cyX32q8ZP5cC&lpg=PA178&vq=preceding%20section&pg=PA66#v=onepage&q&f=false  chapter XIII.3 theorem 3 page 66]}}</ref> However, the simple examples\n: <math>\n\\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix},\n\\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}</math>\nshow that for non-negative matrices there may exist eigenvalues of the same absolute value as the maximal one ((1) and (−1) – eigenvalues of the first matrix); moreover the maximal eigenvalue may not be a simple root of the characteristic polynomial, can be zero and the corresponding eigenvector (1,0) is not strictly positive (second example). So it may seem that most properties are broken for non-negative matrices, however Frobenius found the right way to generalize to this case.\n\nThe key feature of theory in the non-negative case is to find some special subclass of non-negative matrices— '''irreducible''' matrices— for which a non-trivial generalization is possible. Namely, although the eigenvalues attaining the maximal absolute value may not be unique, the structure of maximal eigenvalues is under control: they have the form ''e''<sup>''i2πl/h''</sup>''r'', where ''h'' is an integer called the [[iterated function|period]] of matrix, ''r'' is a real strictly positive eigenvalue, and ''l''&nbsp;=&nbsp;0,&nbsp;1,&nbsp;...,&nbsp;''h''&nbsp;−&nbsp;1.\nThe eigenvector corresponding to ''r'' has strictly positive components (in contrast with the general case of non-negative matrices, where components are only non-negative). Also all such eigenvalues are simple roots of the characteristic polynomial. Further properties are described below.\n\n====Classification of matrices====\nLet ''A'' be a square matrix (not necessarily positive or even real).\nThe matrix ''A'' is '''irreducible''' if any of the following equivalent properties\nholds.\n\n'''Definition 1 :''' ''A'' does not have non-trivial invariant '''coordinate''' subspaces.\nHere a non-trivial coordinate subspace means a [[linear subspace]] spanned by any [[proper subset]] of standard basis vectors of ''R''<sup>''n''</sup>. More explicitly, for any linear subspace spanned by standard basis vectors ''e''<sub>''i''<sub>1</sub> </sub>, ...,\n''e''<sub>''i''<sub>k</sub></sub>, 0&nbsp;< ''k''&nbsp;<&nbsp;''n'' its image under the action of ''A'' is not contained in the same subspace.\n\n'''Definition 2:''' ''A'' cannot be conjugated into block upper triangular form by a [[permutation matrix]] ''P'':\n: <math>PAP^{-1} \\ne\n\\begin{pmatrix} E & F \\\\ 0 & G \\end{pmatrix},</math>\nwhere ''E'' and ''G'' are non-trivial (i.e. of size greater than zero) square matrices.\n\nIf ''A'' is non-negative other definitions exist:\n\n'''Definition 3:''' For every pair of indices ''i'' and ''j'', there exists a natural number ''m'' such that (''A''<sup>''m''</sup>)<sub>''ij''</sub> is positive.\n\n'''Definition 4:''' One can associate with a matrix ''A'' a certain [[directed graph]] ''G''<sub>''A''</sub>. It has exactly ''n'' vertices, where ''n'' is size of ''A'', and there is an edge from vertex ''i'' to vertex ''j'' precisely when ''A''<sub>''ij''</sub> > 0. Then the matrix ''A'' is irreducible if and only if its associated graph ''G''<sub>''A''</sub> is [[strongly connected component|strongly connected]].\n\nThis notion is somewhat reminiscent of that of a free action of a group; if one could somehow build a group out of ''A'', then the space ''R''<sup>''n''</sup> would be an irreducible representation. (One can build a group by  considering the exponential <math> \\left\\lbrace \\exp(tA), t\\in\\mathbb{R}\\right\\rbrace </math>.) However, the notion of an irreducible matrix is fundamentally easier to satisfy than an irreducible representation, because only coordinate subspaces are considered.\n\nA matrix is '''reducible''' if it is not irreducible.\n\nLet ''A'' be non-negative. Fix an index ''i'' and define the '''period of index ''i'' ''' to be the [[greatest common divisor]] of all natural numbers ''m'' such that (''A''<sup>''m''</sup>)<sub>''ii''</sub> > 0. When ''A'' is irreducible, the period of every index is the same and is called the '''period of ''A''. ''' In fact, when ''A'' is irreducible, the period can be defined as the greatest common divisor of the lengths of the closed directed paths in ''G''<sub>''A''</sub> (see Kitchens<ref name=\"Kitchens\"/> page 16). The period is also called the index of imprimitivity\n(Meyer<ref name=\"Meyer\"/> page 674) or the order of cyclicity.\n\nIf the period is 1, ''A'' is '''aperiodic'''.\n\nA matrix ''A'' is '''primitive''' if it is non-negative and its ''m''th power is positive for some natural number ''m'' (i.e. the same ''m'' works for all pairs of indices). It can be proved that primitive matrices  are the same as irreducible aperiodic non-negative matrices.\n\nA positive square matrix is primitive and a primitive matrix is irreducible. All statements of the Perron–Frobenius theorem for positive matrices remain true for primitive matrices. However, a general non-negative irreducible matrix ''A'' may possess several eigenvalues whose absolute value is equal to the spectral radius of ''A'', so the statements need to be correspondingly modified. Actually the number of such eigenvalues is exactly equal to the period. Results for non-negative matrices were first obtained by Frobenius in 1912.\n\n====Perron–Frobenius theorem for irreducible matrices====\n\nLet ''A'' be an irreducible non-negative ''n''&nbsp;×&nbsp;''n'' matrix with period ''h'' and [[spectral radius]] ''ρ''(''A'')&nbsp;=&nbsp;''r''. Then the following statements hold.\n\n# The number ''r'' is a positive real number and it is an eigenvalue of the matrix ''A'', called the '''Perron–Frobenius eigenvalue'''.\n# The Perron–Frobenius eigenvalue ''r'' is simple. Both right and left eigenspaces associated with ''r'' are one-dimensional.\n# ''A'' has a right eigenvector ''v'' with eigenvalue ''r'' whose components are all positive.\n# Likewise, ''A'' has a left eigenvector ''w'' with eigenvalue ''r'' whose components are all positive.\n# The only eigenvectors whose components are all positive are those associated with the eigenvalue ''r''.\n# The matrix ''A'' has exactly ''h'' (where ''h'' is the '''period''') complex eigenvalues with absolute value ''r''. Each of them is a simple root of the characteristic polynomial and is the product of ''r'' with an ''h''th [[root of unity]].\n# Let ''ω'' = 2π/''h''. Then the matrix ''A'' is [[similar matrix|similar]] to ''e''<sup>''iω''</sup>''A'', consequently the spectrum of ''A'' is invariant under multiplication by ''e''<sup>''iω''</sup> (corresponding to the rotation of the complex plane by the angle ''ω'').\n# If ''h'' > 1 then there exists a permutation matrix ''P'' such that\n::<math>PAP^{-1}=\n\\begin{pmatrix}\n0 & A_1 & 0 & 0 & \\ldots & 0 \\\\\n0 & 0 & A_2 & 0 & \\ldots & 0 \\\\\n\\vdots & \\vdots &\\vdots & \\vdots & & \\vdots \\\\\n0 & 0 & 0 & 0 & \\ldots & A_{h-1} \\\\\nA_h & 0 & 0 & 0 & \\ldots & 0\n\\end{pmatrix},\n</math>\n::where the blocks along the main diagonal are zero square matrices.\n\n:9. '''[[Lothar Collatz|Collatz]]–Wielandt formula''': for all non-negative non-zero vectors ''x'' let ''f''(''x'') be the minimum value of [''Ax'']<sub>''i''</sub> / ''x''<sub>''i''</sub> taken over all those ''i'' such that ''x<sub>i</sub>'' ≠ 0. Then ''f'' is a real valued function whose [[maximum]] is the Perron–Frobenius eigenvalue.\n\n:10. The Perron–Frobenius eigenvalue satisfies the inequalities\n::<math>\\min_i \\sum_{j} a_{ij} \\le r \\le \\max_i \\sum_{j} a_{ij}.</math>\n\nThe matrix <math>\\begin{pmatrix}0 & 0 & 1 \\\\0 & 0 & 1 \\\\1 & 1 & 0 \\\\\\end{pmatrix}</math> shows that the (square) zero-matrices along the diagonal may be of different sizes, the blocks ''A''<sub>''j''</sub> need not be square, and ''h'' need not divide&nbsp;''n''.\n\n===Further properties===\nLet ''A'' be an irreducible non-negative matrix, then:\n# (I+''A'')<sup>''n''−1</sup> is a positive matrix. (Meyer<ref name=\"Meyer\"/> [https://web.archive.org/web/20100307021652/http://www.matrixanalysis.com/Chapter8.pdf claim 8.3.5 p. 672]).\n# Wielandt's theorem.{{clarify|reason=What are the restrictions on ''B''?|date=March 2015}} If |''B''|<''A'', then ''ρ''(''B'')≤''ρ''(''A''). If equality holds (i.e. if ''μ=ρ(A)e<sup>iφ</sup>'' is eigenvalue for ''B''), then ''B'' = ''e''<sup>''iφ''</sup> ''D AD''<sup>−1</sup> for some diagonal unitary matrix ''D'' (i.e. diagonal elements of ''D'' equals to ''e''<sup>''iΘ''<sub>''l''</sub></sup>, non-diagonal are zero).<ref name=\"Meyer675\">{{harvnb|Meyer|2000|pp=[http://www.matrixanalysis.com/Chapter8.pdf claim 8.3.11 p. 675] {{cite web |url=http://www.matrixanalysis.com/Chapter8.pdf |title=Archived copy |accessdate=2010-03-07 |deadurl=yes |archiveurl=https://web.archive.org/web/20100307021652/http://www.matrixanalysis.com/Chapter8.pdf |archivedate=March 7, 2010 |df= }}}}</ref>\n# If some power ''A<sup>q</sup>'' is reducible, then it is completely reducible, i.e. for some permutation matrix ''P'', it is true that: <math>\nP A^q P^{-1}= \\begin{pmatrix}\nA_1 & 0 & 0 & \\dots & 0 \\\\\n0 & A_2 & 0 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\vdots & & \\vdots \\\\\n0 & 0 & 0 & \\dots & A_d \\\\\n\\end{pmatrix}\n</math>, where ''A<sub>i</sub>'' are irreducible matrices having the same maximal eigenvalue. The number of these matrices ''d'' is the greatest common divisor of ''q'' and ''h'', where ''h'' is period of ''A''.<ref>{{harvnb|Gantmacher|2000|p=section XIII.5 theorem 9}}</ref>\n# If ''c(x)=x<sup>n</sup>+c<sub>k<sub>1</sub></sub> x<sup>n-k<sub>1</sub></sup> +c<sub>k<sub>2</sub></sub> x<sup>n-k<sub>2</sub></sup> + ... + c<sub>k<sub>s</sub></sub> x<sup>n-k<sub>s</sub></sup>'' is the characteristic polynomial of ''A'' in which the only non-zero coefficients are listed, then the period of ''A'' equals to the greatest common divisor for ''k<sub>1</sub>, k<sub>2</sub>, ... , k<sub>s</sub>''.<ref>{{harvnb|Meyer|2000|pp=[http://www.matrixanalysis.com/Chapter8.pdf page 679] {{cite web |url=http://www.matrixanalysis.com/Chapter8.pdf |title=Archived copy |accessdate=2010-03-07 |deadurl=yes |archiveurl=https://web.archive.org/web/20100307021652/http://www.matrixanalysis.com/Chapter8.pdf |archivedate=March 7, 2010 |df= }}}}</ref>\n# [[Cesàro summation|Cesàro]] [[summability theory|averages]]: <math> \\lim_{k \\rightarrow \\infty} 1/k\\sum_{i=0,...,k} A^i/r^i = ( v w^T),</math> where the left and right eigenvectors for ''A'' are normalized so that ''w''<sup>''T''</sup>''v'' = 1. Moreover, the matrix ''v w<sup>T</sup>'' is the [[spectral projection]] corresponding to ''r'' - Perron projection.<ref>{{harvnb|Meyer|2000|pp=[http://www.matrixanalysis.com/Chapter8.pdf example 8.3.2 p. 677] {{cite web |url=http://www.matrixanalysis.com/Chapter8.pdf |title=Archived copy |accessdate=2010-03-07 |deadurl=yes |archiveurl=https://web.archive.org/web/20100307021652/http://www.matrixanalysis.com/Chapter8.pdf |archivedate=March 7, 2010 |df= }}}}</ref>\n# Let ''r'' be the Perron–Frobenius eigenvalue, then the adjoint matrix for (r-''A'') is positive.<ref>{{harvnb|Gantmacher|2000|p=[https://books.google.com/books?id=cyX32q8ZP5cC&lpg=PA178&vq=preceding%20section&pg=PA62#v=onepage&q&f=true section XIII.2.2 page 62]}}</ref>\n# If ''A'' has at least one non-zero diagonal element, then ''A'' is primitive.<ref>{{harvnb|Meyer|2000|pp= [http://www.matrixanalysis.com/Chapter8.pdf example 8.3.3 p. 678] {{cite web |url=http://www.matrixanalysis.com/Chapter8.pdf |title=Archived copy |accessdate=2010-03-07 |deadurl=yes |archiveurl=https://web.archive.org/web/20100307021652/http://www.matrixanalysis.com/Chapter8.pdf |archivedate=March 7, 2010 |df= }}}}</ref>\n# If 0 ≤ ''A'' < ''B'', then ''r''<sub>''A''</sub> ≤ ''r''<sub>''B.''</sub> Moreover, if ''B'' is irreducible, then the inequality is strict: ''r<sub>A</sub> < r<sub>B</sub>''.\n\nOne of the definitions of primitive matrix requires ''A'' to be non-negative and there exists ''m'', such that ''A<sup>m</sup>'' is positive. One may one wonder how big ''m'' can be, depending on the size of ''A''. The following answers this question.\n* Assume ''A'' is non-negative primitive matrix of size ''n'', then ''A''<sup>''n''<sup>2</sup>&nbsp;−&nbsp;2''n''&nbsp;+&nbsp;2</sup> is positive. Moreover, if ''n'' > 1, there exists a matrix ''M'' given below, such that ''M<sup>k</sup>'' is not positive (but of course still non-negative) for all ''k'' < ''n''<sup>2</sup>&nbsp;−&nbsp;2''n''&nbsp;+&nbsp;2, in particular (''M''<sup>''n''<sup>2</sup>&nbsp;−&nbsp;2''n''+1</sup>)<sub>11</sub> = 0.\n:<math>M=\n\\begin{pmatrix}\n0 & 1 & 0 & 0 & \\cdots & 0 \\\\\n0 & 0 & 1 & 0 & \\cdots & 0 \\\\\n0 & 0 & 0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & & \\vdots \\\\\n0 & 0 & 0 & 0 & \\cdots & 1 \\\\\n1 & 1 & 0 & 0 & \\cdots & 0\n\\end{pmatrix}\n</math><ref>{{harvnb|Meyer|2000|pp=[http://www.matrixanalysis.com/Chapter8.pdf chapter 8 example 8.3.4 page 679 and exercise 8.3.9 p.&nbsp;685] {{cite web |url=http://www.matrixanalysis.com/Chapter8.pdf |title=Archived copy |accessdate=2010-03-07 |deadurl=yes |archiveurl=https://web.archive.org/web/20100307021652/http://www.matrixanalysis.com/Chapter8.pdf |archivedate=March 7, 2010 |df= }}}}</ref>\n\n==Applications==\nNumerous books have been written on the subject of non-negative matrices, and Perron–Frobenius theory is invariably a central feature. The following examples given below only scratch the surface of its vast application domain.\n\n===Non-negative matrices===\nThe Perron–Frobenius theorem does not apply directly to non-negative matrices. Nevertheless, any reducible square matrix ''A'' may be written in upper-triangular block form (known as the '''normal form of a reducible matrix''')<ref>{{harvnb|Varga|2002|p=2.43 (page 51)}}</ref>\n::::''PAP''<sup>−1</sup> = <math> \\left( \\begin{smallmatrix}\nB_1 & * & * & \\cdots & * \\\\\n0 & B_2 & * & \\cdots & * \\\\\n\\vdots & \\vdots & \\vdots & & \\vdots \\\\\n0 & 0 & 0 & \\cdots & * \\\\\n0 & 0 & 0 & \\cdots & B_h\n\\end{smallmatrix}\n\\right)</math>\n\nwhere ''P'' is a permutation matrix and each ''B<sub>i</sub>'' is a square matrix that is either irreducible or zero. Now if ''A'' is\nnon-negative then so too is each block of ''PAP''<sup>−1</sup>, moreover the spectrum of ''A'' is just the union of the spectra of the\n''B<sub>i</sub>''.\n\nThe invertibility of ''A'' can also be studied. The inverse of ''PAP''<sup>−1</sup> (if it exists) must have diagonal blocks of the form ''B<sub>i</sub>''<sup>−1</sup> so if any\n''B<sub>i</sub>'' isn't invertible then neither is ''PAP''<sup>−1</sup> or ''A''.\nConversely let ''D'' be the block-diagonal matrix corresponding to ''PAP''<sup>−1</sup>, in other words ''PAP''<sup>−1</sup> with the\nasterisks zeroised. If each ''B<sub>i</sub>'' is invertible then so is ''D'' and ''D''<sup>−1</sup>(''PAP''<sup>−1</sup>) is equal to the\nidentity plus a nilpotent matrix. But such a matrix is always invertible (if ''N<sup>k</sup>'' = 0 the inverse of 1 − ''N'' is\n1 + ''N'' + ''N''<sup>2</sup> + ... + ''N''<sup>''k''−1</sup>) so ''PAP''<sup>−1</sup> and ''A'' are both invertible.\n\nTherefore, many of the spectral properties of ''A'' may be deduced by applying the theorem to the irreducible ''B<sub>i</sub>''. For example, the Perron root is the maximum of the ρ(''B<sub>i</sub>''). While there will still be eigenvectors with non-negative components it is quite possible\nthat none of these will be positive.\n\n===Stochastic matrices===\nA row (column) [[stochastic matrix]] is a square matrix each of whose rows (columns) consists of non-negative real numbers whose sum is unity. The theorem cannot be applied directly to such matrices because they need not be irreducible.\n\nIf ''A'' is row-stochastic then the column vector with each entry 1 is an eigenvector corresponding to the eigenvalue 1, which is also ρ(''A'') by the remark above. It might not be the only eigenvalue on the unit circle: and the associated eigenspace can be multi-dimensional. If ''A'' is row-stochastic and irreducible then the Perron projection is also row-stochastic and all its rows are equal.\n\n===Algebraic graph theory===\nThe theorem has particular use in [[algebraic graph theory]]. The \"underlying graph\" of a nonnegative ''n''-square matrix is the graph with vertices numbered 1, ..., ''n'' and arc ''ij'' if and only if ''A<sub>ij</sub>'' ≠ 0. If the underlying graph of such a matrix is strongly connected, then the matrix is irreducible, and thus the theorem applies. In particular, the [[adjacency matrix]] of a [[strongly connected component|strongly connected graph]] is irreducible.<ref>{{cite book |authorlink=Richard A. Brualdi |first=Richard A. |last=Brualdi |authorlink2=H. J. Ryser |first2=Herbert J. |last2=Ryser |title=Combinatorial Matrix Theory |location=Cambridge |publisher=Cambridge UP |year=1992 |isbn=978-0-521-32265-2 }}</ref><ref>{{cite book |authorlink=Richard A. Brualdi |first=Richard A. |last=Brualdi |first2=Dragos |last2=Cvetkovic |title=A Combinatorial Approach to Matrix Theory and Its Applications |publisher=CRC Press |location=Boca Raton, FL |year=2009 |isbn=978-1-4200-8223-4 }}</ref>\n\n===Finite Markov chains===\nThe theorem has a natural interpretation in the theory of finite [[Markov chain]]s (where it is the matrix-theoretic equivalent of the convergence of an irreducible finite Markov chain to its stationary distribution, formulated in terms of the transition matrix of the chain; see, for example, the article on the [[subshift of finite type]]).<!--which article?-->\n\n===Compact operators===\n{{main|Krein–Rutman theorem}}\nMore generally, it can be extended to the case of non-negative [[compact operator]]s, which, in many ways, resemble finite-dimensional matrices. These are commonly studied in physics, under the name of [[transfer operator]]s, or sometimes '''Ruelle–Perron–Frobenius operators''' (after [[David Ruelle]]). In this case, the leading eigenvalue corresponds to the [[thermodynamic equilibrium]] of a [[dynamical system]], and the lesser eigenvalues to the decay modes of a system that is not in equilibrium. Thus, the theory offers a way of discovering the [[arrow of time]] in what would otherwise appear to be reversible, deterministic dynamical processes, when examined from the point of view of [[point-set topology]].<ref>{{cite book |first=Michael C. |last=Mackey |title=Time's Arrow: The origins of thermodynamic behaviour |location=New York |publisher=Springer-Verlag |year=1992 |isbn=978-0-387-97702-7 }}</ref>\n\n==Proof methods==\nA common thread in many proofs is the [[Brouwer fixed point theorem]]. Another popular method is that of Wielandt (1950). He used the [[Lothar Collatz|Collatz]]–Wielandt formula described above to extend and clarify Frobenius's work.<ref>{{harvnb|Gantmacher|2000|p=[https://books.google.ru/books?id=cyX32q8ZP5cC&lpg=PR5&dq=Applications%20of%20the%20theory%20of%20matrices&pg=PA54#v=onepage&q&f=false section XIII.2.2 page 54]}}</ref> Another proof is based on the [[spectral theory]]<ref name=\"Smith\"/> from which part of the arguments are borrowed.\n\n===Perron root is strictly maximal eigenvalue for positive (and primitive) matrices===\nIf ''A'' is a positive (or more generally primitive) matrix, then there exists a real positive eigenvalue ''r'' (Perron–Frobenius eigenvalue or Perron root), which is strictly greater in absolute value than all other eigenvalues, hence ''r'' is the [[spectral radius]] of ''A''.\n\nThis statement does not hold for general non-negative irreducible matrices, which have ''h'' eigenvalues with the same absolute eigenvalue as ''r'', where ''h'' is the period of ''A''.\n\n====Proof for positive matrices====\nLet ''A'' be a positive matrix, assume that its spectral radius ρ(''A'') = 1 (otherwise consider ''A/ρ(A)''). Hence, there exists an eigenvalue λ on the unit circle, and all the other eigenvalues are less or equal 1 in absolute value. Assume that λ ≠ 1. Then there exists a positive integer ''m'' such that ''A<sup>m</sup>'' is a positive matrix and the real part of λ''<sup>m</sup>'' is negative. Let ε be half the smallest diagonal entry of ''A<sup>m</sup>'' and set ''T'' = ''A<sup>m</sup>''&nbsp;−&nbsp;''ε'' which is yet another positive matrix. Moreover, if ''Ax'' = ''λx'' then ''A<sup>m</sup>x'' = ''λ<sup>m</sup>x'' thus ''λ''<sup>''m''</sup>&nbsp;−&nbsp;''ε'' is an eigenvalue of ''T''. Because of the choice of ''m'' this point lies outside the unit disk consequently ''ρ''(''T'') > 1. On the other hand, all the entries in ''T'' are positive and less than or equal to those in ''A<sup>m</sup>'' so by [[spectral radius|Gelfand's formula]] ''ρ''(''T'') ≤ ''ρ''(''A<sup>m</sup>'') ≤ ''ρ''(''A'')<sup>''m''</sup> = 1. This contradiction means that λ=1 and there can be no other eigenvalues on the unit circle.\n\nAbsolutely the same arguments can be applied to the case of primitive matrices; we just need to mention the following simple lemma, which clarifies the properties of primitive matrices.\n\n====Lemma====\nGiven a non-negative ''A'', assume there exists ''m'', such that ''A<sup>m</sup>'' is positive, then ''A''<sup>''m''+1</sup>, ''A''<sup>''m''+2</sup>, ''A''<sup>''m''+3</sup>,... are all positive.\n\n''A''<sup>''m''+1</sup> = ''AA''<sup>''m''</sup>, so it can have zero element only if some row of ''A'' is entirely zero, but in this case the same row of ''A<sup>m</sup>'' will be zero.\n\nApplying the same arguments as above for primitive matrices, prove the main claim.\n\n===Power method and the positive eigenpair===\nFor a positive (or more generally irreducible non-negative) matrix ''A'' the dominant [[eigenvector]] is real and strictly positive (for non-negative ''A'' respectively non-negative.)\n\nThis can be established using the [[power method]], which states that for a sufficiently generic (in the sense below) matrix ''A'' the sequence of vectors  ''b''<sub>''k''+1</sub> = ''Ab''<sub>''k''</sub> / | ''Ab''<sub>''k''</sub> | converges to the [[eigenvector]] with the maximum [[eigenvalue]]. (The initial vector ''b''<sub>0</sub> can be chosen arbitrarily except for some measure zero set). Starting with a non-negative vector ''b''<sub>0</sub> produces the sequence of non-negative vectors ''b<sub>k</sub>''. Hence the limiting vector is also non-negative. By the power method this limiting vector is the dominant eigenvector for ''A'', proving the assertion. The corresponding eigenvalue is non-negative.\n\nThe proof requires two additional arguments. First, the power method converges for matrices which do not have several eigenvalues of the same absolute value as the maximal one. The previous section's argument guarantees this.\n\nSecond, to ensure strict positivity of all of the components of the eigenvector for the case of irreducible matrices. This follows from the following fact, which is of independent interest:\n\n:Lemma: given a positive (or more generally irreducible non-negative) matrix ''A'' and ''v'' as any non-negative eigenvector for ''A'', then it<!--the eigenvector?--> is necessarily strictly positive and the corresponding eigenvalue is also strictly positive.\n\nProof. One of the definitions of irreducibility for non-negative matrices is that for all indexes ''i,j'' there exists ''m'', such that (''A''<sup>''m''</sup>)<sub>''ij''</sub> is strictly positive. Given a non-negative eigenvector ''v'', and that at least one of its components say ''j''-th is strictly positive, the corresponding eigenvalue is strictly positive, indeed, given ''n'' such that (''A''<sup>''n''</sup>)<sub>''ii''</sub> >0, hence:  ''r''<sup>''n''</sup>''v''<sub>''i''</sub> =\n''A''<sup>''n''</sup>''v''<sub>''i''</sub> ≥\n(''A''<sup>''n''</sup>)<sub>''ii''</sub>''v''<sub>''i''</sub>\n>0. Hence ''r'' is strictly positive. The eigenvector is strict positivity. Then given ''m'', such that (''A''<sup>''m''</sup>)<sub>''ij''</sub> >0, hence:  ''r''<sup>''m''</sup>''v''<sub>''j''</sub> =\n(''A''<sup>''m''</sup>''v'')<sub>''j''</sub> ≥\n(''A''<sup>''m''</sup>)<sub>''ij''</sub>''v''<sub>''i''</sub> >0, hence\n''v''<sub>''j''</sub> is strictly positive, i.e., the eigenvector is strictly positive.\n\n===Multiplicity one===\nThis section proves that the Perron–Frobenius eigenvalue is a simple root of the characteristic polynomial of the matrix. Hence the eigenspace associated to Perron–Frobenius eigenvalue  ''r'' is one-dimensional. The arguments here are close to those in Meyer.<ref name=\"Meyer\">{{harvnb|Meyer|2000|pp=[http://www.matrixanalysis.com/Chapter8.pdf chapter 8 page 665] {{cite web |url=http://www.matrixanalysis.com/Chapter8.pdf |title=Archived copy |accessdate=2010-03-07 |deadurl=yes |archiveurl=https://web.archive.org/web/20100307021652/http://www.matrixanalysis.com/Chapter8.pdf |archivedate=March 7, 2010 |df= }}}}</ref>\n\nGiven a strictly positive eigenvector ''v'' corresponding to ''r'' and another eigenvector ''w'' with the same eigenvalue. (Vector ''w'' can be chosen to be real, because ''A'' and ''r'' are both real, so the null space of ''A-r'' has a basis consisting of real vectors). Assuming at least one of the components of ''w'' is positive (otherwise multiply ''w'' by −1). Given  maximal possible ''α'' such that ''u=v- α w'' is non-negative, then one of the components of ''u'' is zero, otherwise ''α'' is not maximum. Vector ''u'' is an eigenvector. It is non-negative, hence by the lemma described in the [[Perron–Frobenius theorem#Power method and the positive eigenpair|previous section]] non-negativity implies strict positivity for any eigenvector. On the other hand, as above at least one component of ''u'' is zero. The contradiction implies that ''w'' does not exist.\n\nCase: There are no Jordan cells corresponding to the Perron–Frobenius eigenvalue ''r'' and all other eigenvalues which have the same absolute value.\n\nIf there is a Jordan cell, then the [[Matrix norm#Induced norm|infinity norm]]\n(A/r)<sup>k</sup><sub>∞</sub> tends to infinity for ''k → ∞ '',\nbut that contradicts the existence of the positive eigenvector.\n\nGiven ''r'' = 1, or ''A/r''. Letting ''v'' be a Perron–Frobenius strictly positive eigenvector, so ''Av=v'', then:\n\n<math> \\|v\\|_{\\infty}= \\|A^k v\\|_{\\infty} \\ge \\|A^k\\|_{\\infty} \\min_i (v_i), ~~\\Rightarrow~~ \\|A^k\\|_{\\infty} \\le \\|v\\|/\\min_i (v_i) </math>\nSo ''A<sup>k</sup>''<sub>∞</sub> is bounded for all ''k''. This gives another proof that there are no eigenvalues which have greater absolute value than Perron–Frobenius one. It also contradicts the existence of the Jordan cell for any eigenvalue which has absolute value equal to 1 (in particular for the Perron–Frobenius one), because existence of the Jordan cell implies that ''A<sup>k</sup>''<sub>∞</sub> is unbounded. For a two by two matrix:\n: <math>\nJ^k= \\begin{pmatrix} \\lambda & 1 \\\\ 0 & \\lambda \\end{pmatrix} ^k\n=\n\\begin{pmatrix} \\lambda^k & k\\lambda^{k-1} \\\\ 0 & \\lambda^k \\end{pmatrix},\n</math>\nhence ''J''<sup>''k''</sup><sub>∞</sub> = |''k'' + ''λ''| (for |''λ''| = 1), so it tends to infinity when ''k'' does so. Since ''J<sup>k</sup>'' = ''C''<sup>−1</sup> ''A''<sup>''k''</sup>''C'', then ''A''<sup>''k''</sup> ≥ ''J''<sup>''k''</sup>/ (''C''<sup>−1</sup> ''C'' ), so it also tends to infinity. The resulting contradiction implies that there are no Jordan cells for the corresponding eigenvalues.\n\nCombining the two claims above reveals that the Perron–Frobenius eigenvalue ''r'' is simple root of the characteristic polynomial. In the case of nonprimitive matrices, there exist other eigenvalues which have the same absolute value as ''r''.  The same claim is true for them, but requires more work.\n\n===No other non-negative eigenvectors===\nGiven positive (or more generally irreducible non-negative matrix) ''A'', the Perron–Frobenius eigenvector is the only (up to multiplication by constant) non-negative eigenvector for ''A''.\n\nOther eigenvectors must contain negative or complex components since eigenvectors for different eigenvalues are orthogonal in some sense, but two positive eigenvectors cannot be orthogonal, so they must correspond to the same eigenvalue, but the eigenspace for the Perron–Frobenius is one-dimensional.\n\nAssuming there exists an eigenpair (''λ'', ''y'') for ''A'', such that vector ''y'' is positive, and given (''r'', ''x''), where ''x'' – is the left Perron–Frobenius eigenvector for ''A'' (i.e. eigenvector for ''A<sup>T</sup>''), then\n''rx''<sup>''T''</sup>''y'' = (''x''<sup>''T''</sup> ''A'') ''y'' = ''x''<sup>''T''</sup> (''Ay'') = ''λx''<sup>''T''</sup>''y'', also ''x''<sup>''T''</sup> ''y'' > 0, so one has: ''r'' = ''λ''. Since the eigenspace for the Perron–Frobenius eigenvalue ''r'' is one-dimensional, non-negative eigenvector ''y'' is a multiple of the Perron–Frobenius one.<ref>{{harvnb|Meyer|2000|pp=[http://www.matrixanalysis.com/Chapter8.pdf chapter 8 claim 8.2.10 page 666] {{cite web |url=http://www.matrixanalysis.com/Chapter8.pdf |title=Archived copy |accessdate=2010-03-07 |deadurl=yes |archiveurl=https://web.archive.org/web/20100307021652/http://www.matrixanalysis.com/Chapter8.pdf |archivedate=March 7, 2010 |df= }}}}</ref>\n\n===Collatz–Wielandt formula===\nGiven a positive (or more generally irreducible non-negative matrix) ''A'', for all non-negative non-zero vectors ''x'' and ''f''(''x'') as the minimum value of [''Ax'']<sub>''i''</sub> / ''x''<sub>''i''</sub> taken over all those ''i'' such that ''x<sub>i</sub>'' ≠ 0, then ''f'' is a real valued function whose [[maximum]] is the Perron–Frobenius eigenvalue ''r''.\n\nHere, ''r'' is attained for ''x'' taken to be the Perron–Frobenius eigenvector ''v''. The proof requires that values ''f'' on the other vectors are less or equal. Given a vector ''x''. Let ''ξ=f(x)'', so ''0≤ξx≤Ax'' and  ''w'' to be the right eigenvector for ''A'', then ''w<sup>T</sup> ξx ≤ w<sup>T</sup> (Ax) = (w<sup>T</sup> A)x = r w<sup>T</sup> x ''. Hence ξ≤r.<ref>{{harvnb|Meyer|2000|pp=[http://www.matrixanalysis.com/Chapter8.pdf chapter 8 page 666] {{cite web |url=http://www.matrixanalysis.com/Chapter8.pdf |title=Archived copy |accessdate=2010-03-07 |deadurl=yes |archiveurl=https://web.archive.org/web/20100307021652/http://www.matrixanalysis.com/Chapter8.pdf |archivedate=March 7, 2010 |df= }}}}</ref>\n\n===Perron projection as a limit: ''A''<sup>''k''</sup>/''r''<sup>''k''</sup>===\nLet ''A'' be a positive (or more generally, primitive) matrix, and let ''r'' be its Perron–Frobenius eigenvalue.\n# There exists a limit ''A<sup>k</sup>/r<sup>k</sup>'' for ''k → ∞'', denote it by ''P''.\n# ''P'' is a [[Projection (linear algebra)|projection operator]]: ''P''<sup>2</sup> = ''P'', which commutes with ''A'': ''AP'' = ''PA''.\n# The image of ''P'' is one-dimensional and spanned by the Perron–Frobenius eigenvector ''v'' (respectively for ''P<sup>T</sup>''—by the Perron–Frobenius eigenvector ''w'' for ''A<sup>T</sup>'').\n# ''P'' = ''vw''<sup>''T''</sup>, where ''v,w'' are normalized such that ''w''<sup>''T''</sup> ''v'' = 1.\n# Hence ''P'' is a positive operator.\n\nHence ''P'' is a [[spectral projection]] for the Perron–Frobenius eigenvalue ''r'', and is called the Perron projection. The above assertion is not true for general non-negative irreducible matrices.\n\nActually the claims above (except claim 5) are valid for any matrix ''M'' such that there exists an eigenvalue ''r'' which is strictly greater than the other eigenvalues in absolute value and is the simple root of the characteristic [[polynomial]]. (These requirements hold for primitive matrices as above).\n\nGiven that ''M'' is diagonalizable, ''M'' is conjugate to a diagonal matrix with eigenvalues ''r''<sub>1</sub>, ... , ''r''<sub>''n''</sub> on the diagonal (denote ''r''<sub>1</sub> = ''r''). The matrix ''M''<sup>''k''</sup>/''r''<sup>''k''</sup> will be conjugate (1, (''r''<sub>2</sub>/''r'')<sup>''k''</sup>, ... , (''r''<sub>''n''</sub>/''r'')<sup>''k''</sup>), which tends to (1,0,0,...,0), for ''k → ∞'', so the limit exists. The same method works for general ''M'' (without assuming that ''M'' is diagonalizable).\n\nThe projection and commutativity properties are elementary corollaries of the definition: ''MM''<sup>''k''</sup>/''r''<sup>''k''</sup> =  ''M''<sup>''k''</sup>/''r''<sup>''k''</sup> ''M'' ; ''P''<sup>2</sup> = lim ''M''<sup>2''k''</sup>/''r''<sup>2''k''</sup> = ''P''. The third fact is also elementary: ''M''(''Pu'') = ''M'' lim ''M''<sup>''k''</sup>/''r''<sup>''k''</sup> ''u'' = lim ''rM''<sup>''k''+1</sup>/''r''<sup>''k''+1</sup>''u'', so taking the limit yields ''M(''Pu'') = ''r''(''Pu''), so image of ''P'' lies in the ''r''-eigenspace for ''M'', which is one-dimensional by the assumptions.\n\nDenoting by ''v'', ''r''-eigenvector for ''M'' (by ''w'' for ''M<sup>T</sup>''). Columns of ''P'' are multiples of ''v'', because the image of ''P'' is spanned by it. Respectively, rows of ''w''. So ''P'' takes a form ''(a v w<sup>T</sup>)'', for some ''a''. Hence its trace equals to ''(a w<sup>T</sup> v)''. Trace of projector equals the dimension of its image. It was proved before that it is not more than one-dimensional. From the definition one sees that ''P'' acts identically on the ''r''-eigenvector for ''M''. So it is one-dimensional. So choosing (''w''<sup>''T''</sup>''v'') = 1, implies ''P'' = ''vw''<sup>''T''</sup>.\n\n===Inequalities for Perron–Frobenius eigenvalue===\nFor any non-nonegative matrix ''A'' its Perron–Frobenius eigenvalue ''r'' satisfies the inequality:\n\n:<math> r \\; \\le \\; \\max_i \\sum_j A_{ij}.</math>\n\nThis is not specific to non-negative matrices: for any matrix ''A'' with an eigenvalue <math>\\scriptstyle\\lambda</math> it is true\nthat <math>\\scriptstyle |\\lambda| \\; \\le \\; \\max_i \\sum_j |A_{ij}|</math>. This is an immediate corollary of the\n[[Gershgorin circle theorem]]. However another proof is more direct:\n\nAny [[Matrix norm#Induced norm|matrix induced norm]] satisfies the inequality <math>\\scriptstyle\\|A\\| \\ge |\\lambda|</math> for any eigenvalue <math>\\scriptstyle\\lambda</math> because, if <math>\\scriptstyle x</math> is a corresponding eigenvector, <math>\\scriptstyle\\|A\\| \\ge |Ax|/|x| = |\\lambda x|/|x| = |\\lambda|</math>. The [[Matrix norm#Induced norm|infinity norm]] of a matrix is the maximum of row sums: <math>\\scriptstyle \\left \\| A \\right \\| _\\infty = \\max \\limits _{1 \\leq i \\leq m} \\sum _{j=1} ^n | A_{ij} |. </math> Hence the desired inequality is exactly  <math>\\scriptstyle\\|A\\|_\\infty \\ge |\\lambda|</math> applied to the non-negative matrix ''A''.\n\nAnother inequality is:\n\n:<math>\\min_i \\sum_j A_{ij} \\; \\le \\; r .</math>\n\nThis fact is specific to non-negative matrices; for general matrices there is nothing similar. Given  that ''A'' is positive (not just non-negative), then there exists a positive eigenvector ''w'' such that ''Aw'' = ''rw'' and the smallest component of ''w'' (say ''w<sub>i</sub>'') is 1. Then ''r'' = (''Aw'')<sub>''i''</sub> ≥ the sum of the numbers in row ''i'' of ''A''. Thus the minimum row sum gives a lower bound for ''r'' and this observation can be extended to all non-negative matrices by continuity.\n\nAnother way to argue it is via the [[Lothar Collatz|Collatz]]-Wielandt formula. One takes the vector ''x''&nbsp;=&nbsp;(1,&nbsp;1,&nbsp;...,&nbsp;1) and immediately obtains the inequality.\n\n===Further proofs===\n\n====Perron projection====\nThe proof now proceeds using [[spectral decomposition]]. The trick here is to split the Perron root from the other eigenvalues. The spectral projection associated with the Perron root is called the Perron projection and it enjoys the following property:\n\nThe Perron projection of an irreducible non-negative square matrix is a positive matrix.\n\nPerron's findings and also (1)–(5) of the theorem are corollaries of this result. The key point is that a positive projection always has rank one. This means that if ''A'' is an irreducible non-negative square matrix then the algebraic and geometric multiplicities of its Perron root are both one. Also if ''P'' is its Perron projection then ''AP'' = ''PA'' = ρ(''A'')''P'' so every column of ''P'' is a positive right eigenvector of ''A'' and every row is a positive left eigenvector. Moreover, if ''Ax'' = λ''x'' then ''PAx'' = λ''Px'' = ρ(''A'')''Px'' which means ''Px'' = 0 if λ ≠ ρ(''A''). Thus the only positive eigenvectors are those associated with ρ(''A''). If ''A'' is a primitive matrix with ρ(''A'') = 1 then it can be decomposed as ''P'' ⊕ (1&nbsp;−&nbsp;''P'')''A'' so that ''A<sup>n</sup>'' = ''P'' + (1&nbsp;−&nbsp;''P'')''A''<sup>''n''</sup>. As ''n'' increases the second of these terms decays to zero leaving ''P'' as the limit of ''A<sup>n</sup>'' as ''n''&nbsp;→&nbsp;∞.\n\nThe power method is a convenient way to compute the Perron projection of a primitive matrix. If ''v'' and ''w'' are the positive row and column vectors that it generates then the Perron projection is just ''wv''/''vw''. It should be noted that the spectral projections aren't neatly blocked as in the Jordan form. Here they are overlaid and each generally has complex entries extending to all four corners of the square matrix. Nevertheless, they retain their mutual orthogonality which is what facilitates the decomposition.\n\n====Peripheral projection====\nThe analysis when ''A'' is irreducible and non-negative is broadly similar. The Perron projection is still positive but there may now be other eigenvalues of modulus ρ(''A'') that negate use of the power method and prevent the powers of (1&nbsp;−&nbsp;''P'')''A'' decaying as in the primitive case whenever ρ(''A'') = 1. So we consider the '''peripheral projection''', which is the spectral projection of ''A'' corresponding to all the eigenvalues that have modulus ''ρ''(''A''). It may then be shown that the peripheral projection of an irreducible non-negative square matrix is a non-negative matrix with a positive diagonal.\n\n====Cyclicity====\nSuppose in addition that ρ(''A'') = 1 and ''A'' has ''h'' eigenvalues on the unit circle. If ''P'' is the peripheral projection then the matrix ''R'' = ''AP'' = ''PA'' is non-negative and irreducible, ''R<sup>h</sup>'' = ''P'', and the cyclic group ''P'', ''R'', ''R''<sup>2</sup>, ...., ''R''<sup>''h''−1</sup> represents the harmonics of ''A''. The spectral projection of ''A'' at the eigenvalue λ on the unit circle is given by the formula <math>\\scriptstyle h^{-1}\\sum^h_1\\lambda^{-k}R^k</math>. All of these projections (including the Perron projection) have the same positive diagonal, moreover choosing any one of them and then taking the modulus of every entry invariably yields the Perron projection. Some donkey work is still needed in order to establish the cyclic properties (6)–(8) but it's essentially just a matter of turning the handle. The spectral decomposition of ''A'' is given by ''A''&nbsp;=&nbsp;''R''&nbsp;⊕&nbsp;(1&nbsp;−&nbsp;''P'')''A'' so the difference between ''A<sup>n</sup>'' and ''R<sup>n</sup>'' is ''A<sup>n</sup>''&nbsp;−&nbsp;''R<sup>n</sup>'' =&nbsp;(1&nbsp;−&nbsp;''P'')''A''<sup>''n''</sup> representing the transients of ''A<sup>n</sup>'' which eventually decay to zero. ''P'' may be computed as the limit of ''A<sup>nh</sup>'' as ''n''&nbsp;→&nbsp;∞.\n\n==Caveats==\nThe matrices ''L'' = <math>\\left(\n\\begin{smallmatrix}\n1 & 0 & 0 \\\\\n1 & 0 & 0 \\\\\n1 & 1 & 1\n\\end{smallmatrix}\n\\right)</math>, ''P'' = <math>\\left(\n\\begin{smallmatrix}\n\\;\\;1 & 0 & 0 \\\\\n\\;\\;1 & 0 & 0 \\\\\n-1 & 1 & 1\n\\end{smallmatrix}\n\\right)</math>, ''T'' = <math>\\left(\n\\begin{smallmatrix}\n0 & 1 & 1 \\\\\n1 & 0 & 1 \\\\\n1 & 1 & 0\n\\end{smallmatrix}\n\\right)</math>, ''M'' = <math>\\left(\n\\begin{smallmatrix}\n0 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0 & 0\n\\end{smallmatrix}\n\\right)</math> provide simple examples of what can go wrong if the necessary conditions are not met. It is easily seen that the Perron and peripheral projections of ''L'' are both equal to ''P'', thus when the original matrix is reducible the projections may lose non-negativity and there is no chance of expressing them as limits of its powers. The matrix ''T'' is an example of a primitive matrix with zero diagonal. If the diagonal of an irreducible non-negative square matrix is non-zero then the matrix must be primitive but this example demonstrates that the converse is false. ''M'' is an example of a matrix with several missing spectral teeth. If ω = e<sup>iπ/3</sup> then ω<sup>6</sup> = 1 and the eigenvalues of ''M'' are {1,ω<sup>2</sup>,ω<sup>3</sup>,ω<sup>4</sup>} so ω and ω<sup>5</sup> are both absent.{{Citation needed|date=January 2012}}\n\n==Terminology==\nA problem that causes confusion is a lack of standardisation in the definitions. For example, some authors use the terms ''strictly positive'' and ''positive'' to mean > 0 and ≥ 0 respectively. In this article ''positive'' means > 0 and ''non-negative'' means ≥ 0. Another vexed area concerns ''decomposability'' and ''reducibility'': ''irreducible'' is an overloaded term. For avoidance of doubt a non-zero non-negative square matrix ''A'' such that 1&nbsp;+&nbsp;''A'' is primitive is sometimes said to be ''connected''. Then irreducible non-negative square matrices and connected matrices are synonymous.<ref>For surveys of results on irreducibility, see [[Olga Taussky-Todd]] and [[Richard A. Brualdi]].</ref>\n\nThe nonnegative eigenvector is often normalized so that the sum of its components is equal to unity; in this case, the eigenvector is the vector of a [[probability distribution]] and is sometimes called a ''stochastic eigenvector''.\n\n''Perron–Frobenius eigenvalue'' and ''dominant eigenvalue'' are alternative names for the Perron root. Spectral projections are also known as ''spectral projectors'' and ''spectral idempotents''. The period is sometimes referred to as the ''index of imprimitivity'' or the ''order of cyclicity''.\n\n==See also==\n\n* [[Z-matrix (mathematics)]]\n* [[M-matrix]]\n* [[P-matrix]]\n* [[Hurwitz matrix]]\n* [[Metzler matrix]] ([[Quasipositive matrix]])\n* [[Positive operator]]\n\n==Notes==\n{{reflist|3|refs=\n\n<ref name=\"Kitchens\">{{citation |  title=Symbolic dynamics: one-sided, two-sided and countable state markov shifts. | year=1998 | first1=Bruce | last1=Kitchens | url = https://books.google.com/?id=mCcdC_5crpoC&lpg=PA195&dq=kitchens%20perron%20frobenius&pg=PA16#v=onepage&q&f=false|publisher = Springer\n| isbn=9783540627388 }}</ref>\n<ref name=\"Smith\">{{citation | title = A Spectral Theoretic Proof of Perron–Frobenius| pages = 29–35| volume = 102 | number = 1 |  journal =Mathematical Proceedings of the Royal Irish Academy | year=2006 | first1=Roger  | last1=Smith |  url = ftp://emis.maths.adelaide.edu.au/pub/EMIS/journals/MPRIA/2002/pa102i1/pdf/102a102.pdf | doi=10.3318/PRIA.2002.102.1.29\n}}</ref>\n}}\n\n==References==\n\n===Original papers===\n*{{Citation | last1=Perron | first1=Oskar | title=Zur Theorie der Matrices | doi=10.1007/BF01449896 | year=1907 | journal= [[Mathematische Annalen]] | volume=64 | issue=2 | pages=248–263 | author-link = Oskar Perron | url=http://gdz.sub.uni-goettingen.de/dms/load/toc/?PID=PPN235181684_0064 }}\n*{{citation|first=Georg|last= Frobenius|title=Ueber Matrizen aus nicht negativen Elementen|journal= [[:wikisource:de:Sitzungsberichte der Königlich Preußischen Akademie der Wissenschaften zu Berlin|Sitzungsberichte der Königlich Preussischen Akademie der Wissenschaften]] |date=May 1912|pages=456–477 | author-link = Georg Frobenius }}\n*{{citation|first=Georg|last= Frobenius|title=Über Matrizen aus positiven Elementen, 1 |journal= Sitzungsberichte der Königlich Preussischen Akademie der Wissenschaften   |year=1908|pages=471–476 | author-link = Georg Frobenius}}\n*{{citation|first=Georg|last= Frobenius|title=Über Matrizen aus positiven Elementen, 2 |journal= Sitzungsberichte der Königlich Preussischen Akademie der Wissenschaften   |year=1909|pages=514–518 | author-link = Georg Frobenius }}\n* {{citation |  title= The Theory of Matrices, Volume 2  | origyear = 1959|  year= 2000 | first1=Felix | last1=Gantmacher | publisher = AMS Chelsea Publishing| author1-link=Felix Gantmacher| isbn= 978-0-8218-2664-5|\nurl = https://books.google.com/books?id=cyX32q8ZP5cC&lpg=PA178&vq=preceding%20section&pg=PA53#v=onepage&q&f=true}} (1959 edition had different title: \"Applications of the theory of matrices\". Also the numeration of chapters is different in the two editions.)\n\n*{{citation|title=Google page rank and beyond |year=2006 |first1=Amy |last1=Langville |first2=Carl |last2=Meyer |publisher=Princeton University Press |isbn=978-0-691-12202-1 |doi=10.1007/s10791-008-9063-y }}\n*{{citation |  title=The Perron–Frobenius theorem and the ranking of football teams | year=1993 | first1=James | last1=Keener | jstor = 2132526| journal =  SIAM Review| volume = 35| number =1| pages = 80–93 | doi=10.1137/1035004\n}}\n\n*{{citation\n |title       = Matrix analysis and applied linear algebra\n |year        = 2000\n |first1      = Carl\n |last1       = Meyer\n |url         = http://www.matrixanalysis.com/Chapter8.pdf\n |publisher   = SIAM\n |isbn        = 978-0-89871-454-8\n |deadurl     = yes\n |archiveurl  = https://web.archive.org/web/20100307021652/http://www.matrixanalysis.com/Chapter8.pdf\n |archivedate = 2010-03-07\n |df          = \n}}\n*{{citation|first=V.|last= Romanovsky|title=Sur les zéros des matrices stocastiques\n|journal= Bulletin de la Société Mathématique de France |year=1933|pages=213–219\n| volume = 61 | url = http://www.numdam.org/item?id=BSMF_1933__61__213_0  }}\n\n*{{citation|first=Lothar |last= Collatz| author-link = Lothar Collatz|title=Einschließungssatz für die charakteristischen Zahlen von Matrizen\n|journal= Mathematische Zeitschrift |year=1942|pages=221–226\n|doi = 10.1007/BF01180013| volume = 48 | number =1  }}\n\n*{{citation|first=Helmut|last= Wielandt|title=Unzerlegbare, nicht negative Matrizen\n|journal= Mathematische Zeitschrift |year=1950|pages=642–648\n|doi = 10.1007/BF02230720| volume = 52 | number =1  }}\n\n===Further reading===\n\n* Abraham Berman, [[Robert J. Plemmons]], ''Nonnegative Matrices in the Mathematical Sciences'', 1994, SIAM. {{isbn|0-89871-321-8}}.\n* [[Chris Godsil]] and [[Gordon Royle]], ''Algebraic Graph Theory'', Springer, 2001.\n* A. Graham, ''Nonnegative Matrices and Applicable Topics in Linear Algebra'', John Wiley&Sons, New York, 1987.\n* R. A. Horn and C.R. Johnson, ''Matrix Analysis'', Cambridge University Press, 1990\n* Bas Lemmens and Roger Nussbaum, ''Nonlinear Perron-Frobenius Theory'', Cambridge Tracts in Mathematics 189, Cambridge Univ. Press, 2012.\n* S. P. Meyn and R. L. Tweedie, [https://web.archive.org/web/20100619010320/https://netfiles.uiuc.edu/meyn/www/spm_files/book.html ''Markov Chains and Stochastic Stability''] London: Springer-Verlag, 1993. {{isbn|0-387-19832-6}} (2nd edition, Cambridge University Press, 2009)\n*Henryk Minc, ''Nonnegative matrices'', John Wiley&Sons, New York, 1988, {{isbn|0-471-83966-3}}\n* Seneta, E. ''Non-negative matrices and Markov chains''. 2nd rev. ed., 1981, XVI, 288 p., Softcover Springer Series in Statistics. (Originally published by Allen & Unwin Ltd., London, 1973) {{isbn|978-0-387-29765-1}}\n*{{eom|id=P/p072350|first=D.A. |last=Suprunenko}}  (The claim that ''A''<sub>''j''</sub> has order ''n''/''h'' at the end of the statement of the theorem is incorrect.)\n* [[Richard S. Varga]], ''Matrix Iterative Analysis'', 2nd ed., Springer-Verlag, 2002. \n\n{{DEFAULTSORT:Perron-Frobenius Theorem}}\n[[Category:Matrix theory]]\n[[Category:Theorems in linear algebra]]\n[[Category:Markov processes]]"
    },
    {
      "title": "Principal axis theorem",
      "url": "https://en.wikipedia.org/wiki/Principal_axis_theorem",
      "text": "In the [[mathematics|mathematical]] fields of [[geometry]] and [[linear algebra]], a '''principal axis''' is a certain line in a [[Euclidean space]] associated with an [[ellipsoid]] or [[hyperboloid]], generalizing the major and minor [[rotational symmetry|axes]] of an [[ellipse]] or [[hyperbola]]. The '''principal axis theorem''' states that the principal axes are perpendicular, and gives a constructive procedure for finding them.\n\nMathematically, the principal axis theorem is a generalization of the method of [[completing the square]] from [[elementary algebra]].  In [[linear algebra]] and [[functional analysis]], the principal axis theorem is a geometrical counterpart of the [[spectral theorem]].  It has applications to the [[statistics]] of [[principal components analysis]] and the [[singular value decomposition]].  In [[physics]], the theorem is fundamental to the study of [[angular momentum]].\n\n==Motivation==\nThe equations in the [[Cartesian plane]] '''R'''<sup>2</sup>:\n:<math>\\frac{x^2}{9}+\\frac{y^2}{25}=1</math>\n:<math>{}\\frac{x^2}{9}-\\frac{y^2}{25}=1</math>\ndefine, respectively, an ellipse and a hyperbola.  In each case, the ''x'' and ''y'' axes are the principal axes.  This is easily seen, given that there are no ''cross-terms'' involving products ''xy'' in either expression.  However, the situation is more complicated for equations like\n:<math>5x^2+8xy+5y^2=1.</math>\nHere some method is required to determine whether this is an ellipse or a hyperbola.  The basic observation is that if, by completing the square, the quadratic expression can be reduced to a sum of two squares then the equation defines an ellipse, whereas if it reduces to a difference of two squares then the equation represents a hyperbola:\n:<math>u(x,y)^2+v(x,y)^2=1\\qquad\\text{(ellipse)}</math>\n:<math>u(x,y)^2-v(x,y)^2=1\\qquad\\text{(hyperbola)}.</math>\nThus, in our example expression, the problem is how to absorb the coefficient of the cross-term 8''xy'' into the functions ''u'' and ''v''.  Formally, this problem is similar to the problem of [[matrix diagonalization]], where one tries to find a suitable coordinate system in which the matrix of a linear transformation is diagonal.  The first step is to find a matrix in which the technique of diagonalization can be applied.\n\nThe trick is to write the quadratic form as\n:<math>5x^2+8xy+5y^2=\n\\begin{bmatrix}\nx&y\n\\end{bmatrix}\n\\begin{bmatrix}\n5&4\\\\4&5\n\\end{bmatrix}\n\\begin{bmatrix}\nx\\\\y\n\\end{bmatrix}\n=\\mathbf{x}^TA\\mathbf{x} \n</math>\nwhere the cross-term has been split into two equal parts.  The matrix ''A'' in the above decomposition is a [[symmetric matrix]].  In particular, by the [[spectral theorem]], it has [[real numbers|real]] [[eigenvalues]] and is [[diagonalizable]] by an [[orthogonal matrix]] (''orthogonally diagonalizable'').\n\nTo orthogonally diagonalize ''A'', one must first find its eigenvalues, and then find an [[orthonormal]] [[eigenbasis]].  Calculation reveals that the eigenvalues of ''A'' are\n:<math>\\lambda_1 = 1,\\quad \\lambda_2 = 9</math>\nwith corresponding eigenvectors\n:<math>\\mathbf{v}_1 = \\begin{bmatrix}1\\\\-1\\end{bmatrix},\\quad \\mathbf{v}_2=\\begin{bmatrix}1\\\\1\\end{bmatrix}.</math>\nDividing these by their respective lengths yields an orthonormal eigenbasis:\n:<math>\\mathbf{u}_1 = \\begin{bmatrix}1/\\sqrt{2}\\\\-1/\\sqrt{2}\\end{bmatrix},\\quad \\mathbf{u}_2=\\begin{bmatrix}1/\\sqrt{2}\\\\1/\\sqrt{2}\\end{bmatrix}.</math>\n\nNow the matrix ''S'' = ['''u'''<sub>1</sub> '''u'''<sub>2</sub>] is an orthogonal matrix, since it has orthonormal columns, and ''A'' is diagonalized by:\n:<math>A = SDS^{-1} = SDS^T =\n\\begin{bmatrix}\n1/\\sqrt{2}&1/\\sqrt{2}\\\\\n-1/\\sqrt{2}&1/\\sqrt{2}\n\\end{bmatrix}\n\\begin{bmatrix}\n1&0\\\\\n0&9\n\\end{bmatrix}\n\\begin{bmatrix}\n1/\\sqrt{2}&-1/\\sqrt{2}\\\\\n1/\\sqrt{2}&1/\\sqrt{2}\n\\end{bmatrix}.\n</math>\n\nThis applies to the present problem of \"diagonalizing\" the quadratic form through the observation that\n:<math>5x^2+8xy+5y^2=\\mathbf{x}^TA\\mathbf{x}=\\mathbf{x}^T(SDS^T)\\mathbf{x}= (S^T\\mathbf{x})^TD(S^T\\mathbf{x})=1\\left(\\frac{x-y}{\\sqrt{2}}\\right)^2+9\\left(\\frac{x+y}{\\sqrt{2}}\\right)^2.</math>\nThus, the equation <math>5x^2+8xy+5y^2=1</math> is that of an ellipse, since the left side can be written as the sum of two squares.\n\nIt is tempting to simplify this expression by pulling out factors of 2. However, it is important ''not'' to do this.  The quantities\n:<math>c_1=\\frac{x-y}{\\sqrt{2}},\\quad c_2=\\frac{x+y}{\\sqrt{2}}</math>\nhave a geometrical meaning.  They determine an ''orthonormal coordinate system'' on '''R'''<sup>2</sup>.  In other words, they are obtained from the original coordinates by the application of a rotation (and possibly a reflection).  Consequently, one may use the ''c''<sub>1</sub> and ''c''<sub>2</sub> coordinates to make statements about ''length and angles'' (particularly length), which would otherwise be more difficult in a different choice of coordinates (by rescaling them, for instance).  For example, the maximum distance from the origin on the ellipse ''c''<sub>1</sub><sup>2</sup> + 9''c''<sub>2</sub><sup>2</sup> = 1 occurs when ''c''<sub>2</sub>=0, so at the points ''c''<sub>1</sub>=±1.  Similarly, the minimum distance is where ''c''<sub>2</sub>=±1/3.\n\nIt is possible now to read off the major and minor axes of this ellipse.  These are precisely the individual eigenspaces of the matrix ''A'', since these are where ''c''<sub>2</sub> = 0 or ''c''<sub>1</sub>=0.  Symbolically, the principal axes are\n:<math>\nE_1 = \\text{span}\\left(\\begin{bmatrix}1/\\sqrt{2}\\\\-1/\\sqrt{2}\\end{bmatrix}\\right),\\quad\nE_2 = \\text{span}\\left(\\begin{bmatrix}1/\\sqrt{2}\\\\1/\\sqrt{2}\\end{bmatrix}\\right).\n</math>\nTo summarize:\n* The equation is for an ellipse, since both eigenvalues are positive.  (Otherwise, if one were positive and the other negative, it would be a hyperbola.)\n* The principal axes are the lines spanned by the eigenvectors.\n* The minimum and maximum distances to the origin can be read off the equation in diagonal form.\nUsing this information, it is possible to attain a clear geometrical picture of the ellipse: to graph it, for instance.\n\n==Formal statement==\nThe '''principal axis theorem''' concerns  [[quadratic forms]] in '''R'''<sup>n</sup>, which are [[homogeneous polynomial]]<nowiki/>s of degree 2.  Any quadratic form may be represented as\n:<math>Q(\\mathbf{x})=\\mathbf{x}^TA\\mathbf{x}</math>\nwhere ''A'' is a symmetric matrix.\n\nThe first part of the theorem is contained in the following statements guaranteed by the spectral theorem:\n* The eigenvalues of ''A'' are real.\n* ''A'' is diagonalizable, and the eigenspaces of ''A'' are mutually orthogonal.\nIn particular, ''A'' is ''orthogonally diagonalizable'', since one may take a basis of each eigenspace and apply the [[Gram-Schmidt process]] separately within the eigenspace to obtain an orthonormal eigenbasis.\n\nFor the second part, suppose that the eigenvalues of ''A'' are &lambda;<sub>1</sub>, ..., &lambda;<sub>n</sub> (possibly repeated according to their algebraic multiplicities) and the corresponding orthonormal eigenbasis is '''u'''<sub>1</sub>,...,'''u'''<sub>n</sub>.  Then\n* <math>Q(\\mathbf{x}) = \\lambda_1c_1^2+\\lambda_2c_2^2+\\dots+\\lambda_nc_n^2,</math>\nwhere the ''c''<sub>i</sub> are the coordinates with respect to the given eigenbasis.  Furthermore,\n* The ''i''-th '''principal axis''' is the line determined by the ''n''-1 equations ''c''<sub>j</sub> = 0, ''j'' &ne; ''i''.  This axis is the span of the vector '''u'''<sub>i</sub>.\n\n==See also==\n* [[Sylvester's law of inertia]]\n\n==References==\n* {{cite book|authorlink=Gilbert Strang|first=Gilbert|last=Strang|title=Introduction to Linear Algebra|publisher=Wellesley-Cambridge Press|year=1994|isbn=0-9614088-5-5}}\n\n[[Category:Theorems in geometry]]\n[[Category:Theorems in linear algebra]]"
    },
    {
      "title": "Rank–nullity theorem",
      "url": "https://en.wikipedia.org/wiki/Rank%E2%80%93nullity_theorem",
      "text": "{{redirect|Rank theorem|the rank theorem of multivariable calculus|constant rank theorem}}\n\n[[File:Rank-nullity.svg|thumb|260px|Rank–nullity theorem]]\n\nThe '''rank-nullity theorem''' is a fundamental theorem in [[linear algebra]] which relates the dimensions of a [[linear map]]'s [[Kernel_(linear_algebra)|kernel]] and [[Image_(mathematics)|image]] with the dimension of its [[Domain_of_a_function|domain]].\n==Stating the theorem==\nLet <math>V</math>, <math>W</math> be vector spaces, where <math>V</math> is finite dimensional.  Let <math>T \\colon V \\to W</math> be a linear transformation. Then<ref>{{cite book |last1=Friedberg |last2=Insel |last3=Spence |title=Linear Algebra |publisher=Pearson |isbn=9780321998897 |pages=70}}</ref>\n:<math>\\operatorname{Rank}(T) + \\operatorname{Nullity}(T) = \\dim V</math>,\n\nwhere\n:<math>\\operatorname{Rank}(T) := \\dim\\operatorname{Im} T</math> and <math>\\operatorname{Nullity}(T) := \\dim\\operatorname{Ker} T</math>.\nOne can refine this theorem via the [[splitting lemma]] to be a statement about an [[isomorphism]] of spaces, not just dimensions.\n\n=== Matrices ===\nSince <math>\\operatorname{Mat}_{m \\times n}(\\mathbb{F}) \\cong \\operatorname{Hom}(\\mathbb{F}^n, \\mathbb{F}^m)</math><ref>{{cite book |last1=Friedberg |last2=Insel |last3=Spence |title=Linear Algebra |isbn=9780321998897 |pages=103-104}}</ref>, [[Matrix_(mathematics)|matrices]] immediately come to mind when discussing linear maps. In the case of an <math>m\\times n</math> matrix, the dimension of the domain is <math>n</math>, the number of columns in the matrix. Thus the Rank-Nullity Theorem for a given matrix <math>M \\in \\operatorname{Mat}_{m \\times n}(\\mathbb{F})</math> immediately becomes\n:<math>\\operatorname{Rank}(M) + \\operatorname{Nullity}(M) = n </math>.\n\n== Proofs ==\nHere we provide two proofs. The first<ref>{{cite book |last1=Friedberg |last2=Insel |last3=Spence |title=Linear Algebra |publisher=Pearson |isbn=9780321998897 |pages=70}}</ref> operates in the general case, using linear maps. The second proof<ref>{{Citation | last = Banerjee | first = Sudipto | last2 = Roy | first2 = Anindya | date = 2014 | title = Linear Algebra and Matrix Analysis for Statistics | series = Texts in Statistical Science | publisher = Chapman and Hall/CRC | edition =  1st | isbn =  978-1420095388}}</ref> looks at the homogeneous system <math>\\mathbf{Ax} = \\mathbf{0}</math> for <math>\\mathbf{A} \\in \\operatorname{Mat}_{m \\times n}(\\mathbb{F})</math> with [[rank (linear algebra)|rank]] <math>r</math> and shows explicitly that there exists a set of <math>n-r</math> [[linearly independent]] solutions that span the kernel of <math>\\mathbf{A}</math>.\n\nWhile the theorem requires that the domain of the linear map be finite-dimensional, there is no such assumption on the codomain. This means that there are linear maps not given by matrices for which the theorem applies. Despite this, the first proof is not actually more general than the second: since the image of the linear map is finite-dimensional, we can represent the map from its domain to its image by a matrix, prove the theorem for that matrix, then compose with the inclusion of the image into the full codomain.\n\n===First proof===\nLet <math>V, W</math> be vector spaces over some field <math>\\mathbb{F}</math> and <math>T</math> defined as in the statement of the theorem with <math>\\dim V = n</math>.\n\nAs <math>\\operatorname{Ker}T \\subset V</math> is a [[Linear_subspace|subspace]], there exists a basis for it. \nSuppose <math>\\dim\\operatorname{Ker}T = k</math> and let\n:<math>\\mathcal{K} := \\{v_1, \\ldots, v_k\\} \\subset \\operatorname{Ker}(T)</math>\nbe such a basis.\n\nWe may now, by the [[Steinitz exchange lemma]], extend <math>\\mathcal{K}</math> with <math>n-k</math> linearly independent vectors <math>w_1, \\ldots, w_{n-k}</math> to form a full basis of <math>V</math>.\n\nLet\n:<math> \\mathcal{S} := \\{w_1, \\ldots, w_{n-k}\\} \\subset V \\setminus \\operatorname{Ker}(T) </math>\nsuch that\n:<math> \\mathcal{B} := \\mathcal{K} \\cup \\mathcal{S} = \\{v_1, \\ldots, v_k, w_1, \\ldots, w_{n-k}\\} \\subset V </math>\nis a basis for <math>V</math>.\n\nFrom this, we know that\n:<math>\\operatorname{Im} T = \\operatorname{Span}T(\\mathcal{B}) = \\operatorname{Span}\\{T(v_1), \\ldots, T(v_k), T(w_1), \\ldots, T(w_{n-k})\\} = \\operatorname{Span}\\{T(w_1), \\ldots, T(w_{n-k})\\} = \\operatorname{Span}T(\\mathcal{S}) </math>.\nWe now claim that <math>T(\\mathcal{S})</math> is a basis for <math>\\operatorname{Im} T</math>.\nBy definition, <math>T(\\mathcal{S})</math> is generating; it remains to be shown that it is also linearly independent to conclude that it is a basis.\n\n:Let \n::<math> \\sum_{j=1}^{n-k} \\alpha _j T(w_j) = 0_W </math>\n:for some <math>\\alpha _j \\in \\mathbb{F}</math>.\n:Thus, owing to the linearity of <math>T</math>, it follows that\n::<math> T \\left(\\sum_{j=1}^{n-k} \\alpha _j w_j \\right) = 0_W  \\implies \\left(\\sum_{j=1}^{n-k} \\alpha _j w_j \\right) \\in \\operatorname{Ker} T = \\operatorname{Span} \\mathcal{K} \\subset V</math>.\n:This is a contradiction to <math>\\mathcal{B}</math> being a basis, unless all <math>\\alpha _j</math> are equal to zero. This shows that <math>T(\\mathcal{S})</math> is linearly independent, and more specifically that it is a basis for <math>\\operatorname{Im}T</math>.\n\nTo summarise, we have <math>\\mathcal{K}</math>, a basis for <math>\\operatorname{Ker}T</math>, and <math>T(\\mathcal{S})</math>, a basis for <math>\\operatorname{Im}T</math>.\n\nFinally we may state that\n:<math> \\operatorname{Rank}(T) + \\operatorname{Nullity}(T) =  \\dim \\operatorname{Im} T + \\dim \\operatorname{Ker}T = |T(\\mathcal{S})| + |\\mathcal{K}| = (n-k) + k = n = \\dim V</math>.\nThis concludes our proof.\n\n===Second proof===\n\nLet <math>\\mathbf{A} \\in \\operatorname{Mat}_{m \\times n}(\\mathbb{F})</math> with <math>r</math> [[linearly independent]] columns (i.e. <math>\\operatorname{Rank}(\\mathbf{A}) = r</math>). We will show that:\n{{ordered list|type=lower-roman\n  | There exists a set of <math>n-r</math> linearly independent solutions to the homogeneous system <math>\\mathbf{Ax} = \\mathbf{0}</math>.\n  | That every other solution is a linear combination of these <math>n-r</math> solutions.\n}}\n\nTo do this, we will produce a matrix <math>\\mathbf{X} \\in \\operatorname{Mat}_{n \\times (n-r)}(\\mathbb{F})</math> whose columns form a [[basis (linear algebra)|basis]] of the null space of <math>\\mathbf{A}</math>.\n\nWithout loss of generality, assume that the first <math>r</math> columns of <math>\\mathbf{A}</math> are linearly independent. So, we can write\n:<math>\\mathbf{A} = \\begin{pmatrix} \\mathbf{A}_1 & \\mathbf{A}_2\\end{pmatrix}</math>,\nwhere\n:<math>\\mathbf{A}_1 \\in \\operatorname{Mat}_{m \\times r}(\\mathbb{F}) </math> with <math>r</math> linearly independent column vectors, and\n:<math>\\mathbf{A}_2 \\in \\operatorname{Mat}_{m \\times (n-r)}(\\mathbb{F})</math>, each of whose <math>n-r</math> columns are linear combinations of the columns of <math>\\mathbf{A}_1</math>.\n\nThis means that <math>\\mathbf{A}_2 = \\mathbf{A}_1\\mathbf{B}</math> for some <math>\\mathbf{B} \\in \\operatorname{Mat} _{r \\times (n-r)}</math> (see [[rank factorization]]) and, hence,\n:<math>\\mathbf{A} = \\begin{pmatrix} \\mathbf{A}_1 & \\mathbf{A}_1\\mathbf{B}\\end{pmatrix}</math>.\n\nLet\n:<math>\\mathbf{X} = \\begin{pmatrix} -\\mathbf{B} \\\\ \\mathbf{I}_{n-r} \\end{pmatrix} </math>,\nwhere <math>\\mathbf{I}_{n-r}</math> is the <math>(n-r)\\times (n-r)</math> [[identity matrix]]. We note that <math>\\mathbf{X} \\in \\operatorname{Mat}_{n \\times (n-r)}(\\mathbb{F})</math> satisfies\n:<math> \\mathbf{A}\\mathbf{X} = \\begin{pmatrix}\\mathbf{A}_1 & \\mathbf{A}_1\\mathbf{B} \\end{pmatrix}\\begin{pmatrix} -\\mathbf{B} \\\\ \\mathbf{I}_{n-r} \\end{pmatrix} = -\\mathbf{A}_1\\mathbf{B} + \\mathbf{A}_1\\mathbf{B} = \\mathbf{0}_{m \\times (n-r)}. </math>\n\nTherefore, each of the <math>n-r</math> columns of <math>\\mathbf{X}</math> are particular solutions of <math>\\mathbf{Ax} = \\mathbf{0}_{\\mathbb{F}^{m}}</math>. \n\nFurthermore, the <math>n-r</math> columns of <math>\\mathbf{X}</math> are [[linearly independent]] because <math>\\mathbf{Xu} = \\mathbf{0}_{\\mathbb{F}^{n}}</math> will imply <math>\\mathbf{u} = \\mathbf{0}_{\\mathbb{F}^{n}}</math> for <math>\\mathbf{u} \\in \\mathbb{F}^{n-r}</math>:\n:<math> \\mathbf{X}\\mathbf{u} = \\mathbf{0}_{\\mathbb{F}^{n}} \\implies \\begin{pmatrix}\n-\\mathbf{B} \\\\\n \\mathbf{I}_{n-r} \n\\end{pmatrix}\\mathbf{u} = \\mathbf{0}_{\\mathbb{F}^{n}} \\implies \\begin{pmatrix}\n-\\mathbf{B}\\mathbf{u} \\\\\n \\mathbf{u} \n\\end{pmatrix} = \\begin{pmatrix}\n\\mathbf{0}_{\\mathbb{F}^{r}} \\\\\n \\mathbf{0}_{\\mathbb{F}^{n-r}}\n\\end{pmatrix} \\implies \\mathbf{u} = \\mathbf{0}_{\\mathbb{F}^{n-r}}.</math>\nTherefore, the column vectors of <math>\\mathbf{X}</math> constitute a set of <math>n-r</math> linearly independent solutions for <math>\\mathbf{Ax} = \\mathbf{0}_{\\mathbb{F}^{m}}</math>.\n\nWe next prove that ''any'' solution of <math>\\mathbf{Ax} = \\mathbf{0}_{\\mathbb{F}^{m}}</math> must be a [[linear combination]] of the columns of <math>\\mathbf{X}</math>.\n\nFor this, let\n:<math>\\mathbf{u} = \\begin{pmatrix}\n \\mathbf{u}_1 \\\\\n \\mathbf{u}_2 \n\\end{pmatrix} \\in \\mathbb{F}^{n}</math>\n\nbe any vector such that <math>\\mathbf{Au} = \\mathbf{0}_{\\mathbb{F}^{m}}</math>. Note that since the columns of <math>\\mathbf{A}_1</math> are linearly independent, <math>\\mathbf{A}_1\\mathbf{x} = \\mathbf{0}_{\\mathbb{F}^{m}}</math> implies <math>\\mathbf{x} = \\mathbf{0}_{\\mathbb{F}^{r}}</math>.\n\nTherefore,\n:<math>\n\\begin{array}{rcl}\n\\mathbf{A}\\mathbf{u} & = & \\mathbf{0}_{\\mathbb{F}^{m}} \\\\\n\\implies \\begin{pmatrix}\\mathbf{A}_1 & \\mathbf{A}_1\\mathbf{B}\\end{pmatrix} \\begin{pmatrix} \\mathbf{u}_1 \\\\ \\mathbf{u}_2 \\end{pmatrix} & = &  \\mathbf{A}_1\\mathbf{u}_1 + \\mathbf{A}_1\\mathbf{B}\\mathbf{u}_2 & = & \\mathbf{A}_1(\\mathbf{u}_1 + \\mathbf{B}\\mathbf{u}_2) & = & \\mathbf{0}_{\\mathbb{F}^{m}} \\\\\n\\implies \\mathbf{u}_1 + \\mathbf{B}\\mathbf{u}_2 & = & \\mathbf{0}_{\\mathbb{F}^{r}} \\\\\n\\implies \\mathbf{u}_1 & = & -\\mathbf{B}\\mathbf{u}_2\n\\end{array}\n</math>\n\n\n: <math> \\implies \\mathbf{u} = \\begin{pmatrix}\n \\mathbf{u}_1 \\\\\n \\mathbf{u}_2 \n\\end{pmatrix} = \\begin{pmatrix}\n -\\mathbf{B} \\\\\n  \\mathbf{I}_{n-r} \n\\end{pmatrix}\\mathbf{u}_2 = \\mathbf{X}\\mathbf{u}_2. </math>\n\nThis proves that any vector <math>\\mathbf{u}</math> that is a solution of <math>\\mathbf{Ax} = \\mathbf{0}</math> must be a linear combination of the <math>n-r</math> special solutions given by the columns of <math>\\mathbf{X}</math>. And we have already seen that the columns of <math>\\mathbf{X}</math> are linearly independent. Hence, the columns of <math>\\mathbf{X}</math> constitute a basis for the [[null space]] of <math>\\mathbf{A}</math>. Therefore, the [[kernel (matrix)|nullity]] of <math>\\mathbf{A}</math> is <math>n-r</math>. Since <math>r</math> equals rank of <math>\\mathbf{A}</math>, it follows that <math>\\operatorname{Rank}(\\mathbf{A}) + \\operatorname{Nullity}(\\mathbf{A}) = n</math>. This concludes our proof.\n\n== Reformulations and generalizations ==\nThis theorem is a statement of the [[first isomorphism theorem]] of algebra for the case of vector spaces; it generalizes to the [[splitting lemma]].\n\nIn more modern language, the theorem can also be phrased as follows: if\n:0 → ''U'' → ''V'' → ''R'' → 0\nis a [[short exact sequence]] of vector spaces, then\n:<math>\\dim(U) + \\dim(R) = \\dim(V)</math>.\nHere ''R'' plays the role of im ''T'' and ''U'' is ker ''T'', i.e.\n: <math> 0 \\rightarrow \\ker T~\\overset{Id}{\\rightarrow}~V~\\overset{T}{\\rightarrow}~\\operatorname{im} T \\rightarrow 0</math>\n\nIn the finite-dimensional case, this formulation is susceptible to a generalization: if \n:0 → ''V''<sub>1</sub> → ''V''<sub>2</sub> → ... → ''V''<sub>''r''</sub> → 0\nis an [[exact sequence]] of finite-dimensional vector spaces, then\n:<math>\\sum_{i=1}^r (-1)^i\\dim(V_i) = 0.</math><ref>{{cite web|last1=Zaman|first1=Ragib|title=Dimensions of vector spaces in an exact sequence|url=http://math.stackexchange.com/questions/255384/dimensions-of-vector-spaces-in-an-exact-sequence|website=Mathematics Stack Exchange|accessdate=27 October 2015|ref=DimVS}}</ref>\nThe rank–nullity theorem for finite-dimensional vector spaces may also be formulated in terms of the ''index'' of a linear map. The index of a linear map <math>T \\in \\operatorname{Hom}(V,W)</math>, where <math>V</math> and <math>W</math> are finite-dimensional, is defined by\n:<math> \\operatorname{index} T = \\dim \\operatorname{Ker}(T) - \\dim \\operatorname{Coker} T</math>.\n\nIntuitively, <math>\\dim \\operatorname{Ker} T</math> is the number of independent solutions <math>v</math> of the equation <math>Tv = 0</math>, and <math>\\dim \\operatorname{Coker} T </math> is the number of independent restrictions that have to be put on <math>w</math> to make <math>Tv = w </math> solvable. The rank–nullity theorem for finite-dimensional vector spaces is equivalent to the statement\n\n:<math> \\operatorname{index} T = \\dim V - \\dim W </math>.\n\nWe see that we can easily read off the index of the linear map <math>T</math> from the involved spaces, without any need to analyze <math>T</math> in detail. This effect also occurs in a much deeper result: the [[Atiyah–Singer index theorem]] states that the index of certain differential operators can be read off the geometry of the involved spaces.\n\n== Notes ==\n<references/>\n\n== References ==\n* {{Citation | last = Banerjee | first = Sudipto | last2 = Roy | first2 = Anindya | date = 2014 | title = Linear Algebra and Matrix Analysis for Statistics | series = Texts in Statistical Science | publisher = Chapman and Hall/CRC | edition =  1st | isbn =  978-1420095388}}\n* {{Citation | last1=Meyer | first1=Carl D. | title=Matrix Analysis and Applied Linear Algebra | url=http://www.matrixanalysis.com/ | publisher=[[Society for Industrial and Applied Mathematics|SIAM]] | isbn=978-0-89871-454-8 | year=2000}}.\n\n{{DEFAULTSORT:Rank-nullity theorem}}\n[[Category:Theorems in linear algebra]]\n[[Category:Isomorphism theorems]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "Rouché–Capelli theorem",
      "url": "https://en.wikipedia.org/wiki/Rouch%C3%A9%E2%80%93Capelli_theorem",
      "text": "{{short description|Existence of solutions for a system of linear equations in terms of matrix ranks}}\n{{distinguish|Rouché's theorem}}\nThe '''[[Eugène Rouché|Rouché]]–[[Alfredo Capelli|Capelli]] theorem''' is a theorem in [[linear algebra]] that determines the number of [[Solution set|solutions]] for a [[system of linear equations]], given the [[Rank (linear algebra)|rank]] of its [[augmented matrix]] and [[coefficient matrix]]. The theorem is variously known as the:\n* '''[[Leopold Kronecker|Kronecker]]–Capelli theorem''' in [[Poland]], [[Romania]] and [[Russia]];\n* '''Rouché–Capelli theorem''' in [[Italy]];\n* '''Rouché–Fontené theorem''' in [[France]];\n* '''Rouché–[[Ferdinand Georg Frobenius|Frobenius]] theorem''' in [[Spain]] and many countries in [[Latin America]];\n* '''Frobenius theorem''' in the [[Czech Republic]] and in [[Slovakia]].\n\n== Formal statement ==\nA system of linear equations with ''n'' variables has a solution [[if and only if]] the [[Rank (linear algebra)|rank]] of its [[coefficient matrix]] ''A'' is equal to the rank of its augmented matrix&nbsp;[''A''|''b''].<ref>{{Cite book |url = https://books.google.com/books?id=6Pp2-DTOKWIC&pg=PA56 |title = Linear Algebra and Geometry |last = Shafarevich|first = Igor R. |last2 = Remizov |first2 = Alexey |date = 2012-08-23 |publisher = Springer Science & Business Media |isbn = 9783642309946 |page = 56 |language = en}}</ref> If there are solutions, they form an [[affine subspace]] of <math>\\mathbb{R}^n</math> of dimension ''n''&nbsp;−&nbsp;rank(''A''). In particular:\n* if ''n''&nbsp;=&nbsp;rank(''A''), the solution is unique,\n* otherwise there are infinitely many solutions.\n\n==Example==\nConsider the system of equations\n\n: ''x'' + ''y'' + 2''z'' = 3,\n: ''x'' + ''y'' + ''z'' = 1,\n: 2''x'' + 2''y'' + 2''z'' = 2.\n\nThe coefficient matrix is\n\n:<math>\nA = \n  \\begin{bmatrix}\n    1 & 1 & 2 \\\\\n    1 & 1 & 1 \\\\\n    2 & 2 & 2 \\\\\n  \\end{bmatrix},\n</math>\n\nand the augmented matrix is\n\n:<math>\n(A|B) = \n  \\left[\\begin{array}{ccc|c}\n    1 & 1 & 2 & 3\\\\\n    1 & 1 & 1 & 1 \\\\\n    2 & 2 & 2 & 2\n  \\end{array}\\right].\n</math>\n\nSince both of these have the same rank, namely 2, there exists at least one solution; and since their rank is less than the number of unknowns, the latter being 3, there are infinitely many solutions.\n\nIn contrast, consider the system\n\n: ''x'' + ''y'' + 2''z'' = 3,\n: ''x'' + ''y'' + ''z'' = 1,\n: 2''x'' + 2''y'' + 2''z'' = 5.\n\nThe coefficient matrix is\n\n:<math>\nA = \n  \\begin{bmatrix}\n    1 & 1 & 2 \\\\\n    1 & 1 & 1 \\\\\n    2 & 2 & 2 \\\\\n  \\end{bmatrix},\n</math>\n\nand the augmented matrix is\n\n:<math>\n(A|B) = \n  \\left[\\begin{array}{ccc|c}\n    1 & 1 & 2 & 3\\\\\n    1 & 1 & 1 & 1 \\\\\n    2 & 2 & 2 & 5\n  \\end{array}\\right].\n</math>\n\nIn this example the coefficient matrix has rank 2, while the augmented matrix has rank 3; so this system of equations has no solution. Indeed, an increase in the number of linearly independent rows has made the system of equations '''inconsistent'''.\n\n==See also==\n*[[Gaussian elimination]]\n\n==References==\n{{Reflist}}\n* {{cite book | author=A. Carpinteri | title=Structural mechanics | page=74 | publisher=Taylor and Francis | isbn=0-419-19160-7 | year=1997 }}\n\n{{DEFAULTSORT:Rouche-Capelli theorem}}\n[[Category:Theorems in linear algebra]]\n[[Category:Matrix theory]]\n\n[[cs:Soustava lineárních rovnic#Frobeniova věta]]"
    },
    {
      "title": "Schur–Horn theorem",
      "url": "https://en.wikipedia.org/wiki/Schur%E2%80%93Horn_theorem",
      "text": "In [[mathematics]], particularly [[linear algebra]], the '''Schur–Horn theorem''', named after [[Issai Schur]] and [[Alfred Horn]], characterizes the diagonal of a [[Hermitian matrix]] with given [[eigenvalues]]. It has inspired investigations and substantial generalizations in the setting of symplectic geometry. A few important generalizations are [[Kostant's convexity theorem]], [[Atiyah–Guillemin–Sternberg convexity theorem]], [[Kirwan convexity theorem]].\n\n== Statement ==\n\n'''Theorem.''' Let <math>\\mathbf{d}=\\{d_i\\}_{i=1}^N</math> and <math>\\mathbf{\\lambda}=\\{\\lambda_i\\}_{i=1}^N</math> be vectors in <math>\\mathbb{R}^N</math> such that their entries are in non-increasing order. There is a [[Hermitian matrix]] with diagonal values <math>\\{d_i\\}_{i=1}^N</math> and eigenvalues <math>\\{\\lambda_i\\}_{i=1}^N</math> if and only if\n\n: <math>\\sum_{i=1}^n d_i \\leq \\sum_{i=1}^n \\lambda_i \\qquad n=1,2,\\ldots,N</math>\n\nand\n\n: <math>\\sum_{i=1}^N d_i= \\sum_{i=1}^N \\lambda_i.</math>\n\n==Polyhedral geometry perspective==\n\n===Permutation polytope generated by a vector===\nThe '''permutation polytope''' generated by <math>\\tilde{x} = (x_1, x_2,\\ldots, x_n) \\in \\mathbb{R}^n</math> denoted by <math>\\mathcal{K}_{\\tilde{x}}</math> is defined as the convex hull of the set <math>\\{ (x_{\\pi(1)},x_{\\pi(2)},\\ldots,x_{\\pi(n)}) \\in \\mathbb{R}^n : \\pi \\in S_n  \\}</math>. Here <math>S_n</math> denotes the [[symmetric group]] on <math>\\{1, 2, \\ldots, n\\}</math>. The following lemma characterizes the permutation polytope of a vector in <math>\\mathbb{R}^n</math>.\n\n'''Lemma'''.<ref>[[Kadison, R. V.]], Lemma 5, ''The Pythagorean Theorem: I. The finite case'', Proc. Natl. Acad. Sci. USA, vol. 99 no. 7 (2002):4178–4184 (electronic)</ref><ref>[[Kadison, R. V.]]; [[Pedersen, G. K.]], Lemma 13, ''Means and Convex Combinations of Unitary Operators'', Math. Scand. 57 (1985),249–266</ref> If <math>x_1 \\ge x_2 \\ge \\cdots \\ge x_n, y_1 \\ge y_2 \\ge \\cdots \\ge y_n</math>, and <math>x_1 + x_2 + \\cdots + x_n = y_1 + y_2 + \\cdots + y_n,</math> then the following are equivalent :\n\n(i) <math>(y_1, y_2, \\cdots, y_n)(= \\tilde{y}) \\in \\mathcal{K}_{\\tilde{x}}</math>.\n\n(ii) <math>y_1 \\le x_1, y_1 + y_2 \\le x_1 + x_2 , \\ldots, y_1 + y_2 + \\cdots + y_{n-1} \\le x_1 + x_2 + \\cdots + x_{n-1}</math>\n\n(iii) There are points <math>(x_1^{(1)}, x_2 ^{(1)}, \\cdots, x_n ^{(1)})(=\\tilde{x}_1), \\ldots, (x_1^{(n)}, x_2 ^{(n)}, \\ldots, x_n^{(n)})(=\\tilde{x}_n)</math> in <math>\\mathcal{K}_{\\tilde{x}}</math> such that <math>\\tilde{x}_1=\\tilde{x}, \\tilde{x}_n=\\tilde{y},</math> and <math>\\tilde{x}_{k+1} = t \\tilde{x}_k + (1-t) \\tau(\\tilde{x_k})</math> for each <math>k</math> in <math>\\{1, 2, \\ldots, n-1 \\}</math>, some transposition <math>\\tau</math> in <math>S_n</math>, and some <math>t</math> in <math>[0,1]</math>, depending on <math>k</math>.\n\n===Reformulation of Schur–Horn theorem===\nIn view of the equivalence of (i) and (ii) in the lemma mentioned above, one may reformulate the theorem in the following manner.\n\n'''Theorem.''' Let <math>\\mathbf{d}=\\{d_i\\}_{i=1}^N</math> and <math>\\mathbf{\\lambda}=\\{\\lambda_i\\}_{i=1}^N</math> be real vectors. There is a [[Hermitian matrix]] with diagonal entries <math>\\{d_i\\}_{i=1}^N</math> and eigenvalues <math>\\{\\lambda_i\\}_{i=1}^N</math> if and only if the vector <math>(d_1, d_2, \\ldots, d_n)</math> is in the permutation polytope generated by <math>(\\lambda_1, \\lambda_2, \\ldots, \\lambda_n)</math>.\n\nNote that in this formulation, one does not need to impose any ordering on the entries of the vectors <math>\\mathbf{d}</math> and <math>\\mathbf{\\lambda}</math>.\n\n==Proof of the Schur–Horn theorem==\nLet <math>A(=a_{jk})</math> be a <math>n \\times n</math> Hermitian matrix with eigenvalues <math>\\{ \\lambda_i \\}_{i=1}^{n}</math>, counted with multiplicity. Denote the diagonal of <math>A</math> by <math>\\tilde{a}</math>, thought of as a vector in <math>\\mathbb{R}^n</math>, and the vector <math>(\\lambda_1, \\lambda_2, \\ldots, \\lambda_n)</math> by <math>\\tilde{\\lambda}</math>. Let <math>\\Lambda</math> be the diagonal matrix having <math>\\lambda_1, \\lambda_2, \\ldots, \\lambda_n</math> on its diagonal.\n\n(<math>\\Rightarrow</math>) <math>A</math> may be written in the form <math>U\\Lambda U^{-1}</math>, where <math>U</math> is a unitary matrix. Then\n:<math>a_{ii} = \\sum_{j=1}^n \\lambda_j |u_{ij}|^2, \\; i = 1, 2, \\ldots, n</math>\n\nLet <math>S = (s_{ij})</math> be the matrix defined by <math>s_{ij} = |u_{ij}|^2</math>. Since <math>U</math> is a unitary matrix, <math>S</math> is a [[doubly stochastic matrix]] and we have <math>\\tilde{a} = S\\tilde{\\lambda}</math>. By the [[Birkhoff–von Neumann theorem]], <math>S</math> can be written as a convex combination of permutation matrices. Thus <math>\\tilde{a}</math> is in the permutation polytope generated by <math>\\tilde{\\lambda}</math>. This proves Schur's theorem.\n\n(<math>\\Leftarrow</math>) If <math>\\tilde{a}</math> occurs as the diagonal of a Hermitian matrix with eigenvalues <math>\\{ \\lambda_i \\}_{i=1}^n</math>, then <math>t\\tilde{a} + (1-t)\\tau(\\tilde{a})</math> also occurs as the diagonal of some Hermitian matrix with the same set of eigenvalues, for any transposition <math>\\tau</math> in <math>S_n</math>. One may prove that in the following manner.\n\nLet <math>\\xi</math> be a complex number of modulus <math>1</math> such that <math>\\overline{\\xi a_{jk}} = - \\xi a_{jk}</math> and <math>U</math> be a unitary matrix with <math>\\xi \\sqrt{t}, \\sqrt{t}</math> in the <math>j,j</math> and <math>k,k</math> entries, respectively, <math>-\\sqrt{1-t^2}, \\xi \\sqrt{1-t^2}</math> at the <math>j,k</math> and <math>k,j</math> entries, respectively, <math>1</math> at all diagonal entries other than <math>j,j</math> and <math>k,k</math>, and <math>0</math> at all other entries. Then <math>UAU^{-1}</math>\nhas <math>ta_{jj} + (1-t)a_{kk}</math> at the <math>j,j</math> entry, <math>(1-t)a_{jj} + ta_{kk}</math> at the <math>k,k</math> entry, and <math>a_{ll}</math> at the <math>l,l</math> entry where <math>l \\ne j,k</math>. Let <math>\\tau</math> be the transposition of <math>\\{1, 2, \\ldots, n\\}</math> that interchanges <math>j</math> and <math>k</math>.\n\nThen the diagonal of <math>UAU^{-1}</math> is <math>t\\tilde{a} + (1-t)\\tau(\\tilde{a})</math>.\n\n<math>\\Lambda</math> is a Hermitian matrix with eigenvalues <math>\\{ \\lambda_i \\}_{i=1}^n</math>. Using the equivalence of (i) and (iii) in the lemma mentioned above, we see that any vector in the permutation polytope generated by <math>(\\lambda_1, \\lambda_2, \\ldots, \\lambda_n)</math>, occurs as the diagonal of a Hermitian matrix with the prescribed eigenvalues. This proves Horn's theorem.\n\n==Symplectic geometry perspective==\nThe Schur–Horn theorem may be viewed as a corollary of the [[Atiyah–Guillemin–Sternberg convexity theorem]] in the following manner. Let <math>\\mathcal{U}(n)</math> denote the group of <math>n \\times n</math> unitary matrices. Its Lie algebra, denoted by <math>\\mathfrak{u}(n)</math>, is the set of [[skew-Hermitian]] matrices. One may identify the dual space <math>\\mathfrak{u}(n)^*</math> with the set of Hermitian matrices <math>\\mathcal{H}(n)</math> via the linear isomorphism <math>\\Psi : \\mathcal{H}(n) \\rightarrow \\mathfrak{u}(n)^*</math> defined by <math>\\Psi(A)(B) = \\mathrm{tr}(iAB)</math> for <math>A \\in \\mathcal{H}(n), B \\in \\mathfrak{u}(n)</math>. The unitary group <math>\\mathcal{U}(n)</math> acts on <math>\\mathcal{H}(n)</math> by conjugation and acts on <math>\\mathfrak{u}(n)^*</math> by the [[coadjoint action]]. Under these actions, <math>\\Psi</math> is an <math>\\mathcal{U}(n)</math>-equivariant map i.e. for every <math>U \\in \\mathcal{U}(n)</math> the following diagram commutes,\n\n[[Image:U(n)-equivariance of isomorphism.png|200px]]\n\nLet <math>\\tilde{\\lambda} = (\\lambda_1, \\lambda_2, \\ldots, \\lambda_n) \\in \\mathbb{R}^n</math> and <math>\\Lambda \\in \\mathcal{H}(n)</math> denote the diagonal matrix with entries given by <math>\\tilde{\\lambda}</math>. Let <math>\\mathcal{O}_{\\tilde{\\lambda}}</math> denote the orbit of <math>\\Lambda</math> under the <math>\\mathcal{U}(n)</math>-action i.e. conjugation. Under the <math>\\mathcal{U}(n)</math>-equivariant isomorphism <math>\\Psi</math>, the symplectic structure on the corresponding coadjoint orbit may be brought onto <math>\\mathcal{O}_{\\tilde{\\lambda}}</math>. Thus <math>\\mathcal{O}_{\\tilde{\\lambda}}</math> is a Hamiltonian <math>\\mathcal{U}(n)</math>-manifold.\n\nLet <math>\\mathbb{T}</math> denote the [[Cartan subgroup]] of <math>\\mathcal{U}(n)</math> which consists of diagonal complex matrices with diagonal entries of modulus <math>1</math>. The Lie algebra <math>\\mathfrak{t}</math> of <math>\\mathbb{T}</math> consists of diagonal skew-Hermitian matrices and the dual space <math>\\mathfrak{t}^*</math> consists of diagonal Hermitian matrices, under the isomorphism <math>\\Psi</math>. In other words, <math>\\mathfrak{t}</math> consists of diagonal matrices with purely imaginary entries and <math>\\mathfrak{t}^*</math> consists of diagonal matrices with real entries. The inclusion map <math>\\mathfrak{t} \\hookrightarrow \\mathfrak{u}(n)</math> induces a map <math>\\Phi : \\mathcal{H}(n) \\cong \\mathfrak{u}(n)^* \\rightarrow \\mathfrak{t}^*</math>, which projects a matrix <math>A</math> to the diagonal matrix with the same diagonal entries as <math>A</math>. The set <math>\\mathcal{O}_{\\tilde{\\lambda}}</math> is a Hamiltonian <math>\\mathbb{T}</math>-manifold, and the restriction of <math>\\Phi</math> to this set is a [[moment map]] for this action.\n\nBy the Atiyah–Guillemin–Sternberg theorem, <math>\\Phi(\\mathcal{O}_{\\tilde{\\lambda}})</math> is a convex polytope. A matrix <math>A \\in \\mathcal{H}(n)</math> is fixed under conjugation by every element of <math>\\mathbb{T}</math> if and only if <math>A</math> is diagonal. The only diagonal matrices in <math>\\mathcal{O}_{\\tilde{\\lambda}}</math> are the ones with diagonal entries <math>\\lambda_1, \\lambda_2, \\ldots, \\lambda_n</math> in some order. Thus, these matrices generate the convex polytope <math>\\Phi(\\mathcal{O}_{\\tilde{\\lambda}})</math>. This is exactly the statement of the Schur–Horn theorem.\n\n==Notes==\n{{Reflist}}\n\n== References ==\n* [[Schur, Issai]], ''Über eine Klasse von Mittelbildungen mit Anwendungen auf die Determinantentheorie'', Sitzungsber. Berl. Math. Ges. 22 (1923), 9–20.\n* [[Horn, Alfred]], ''Doubly stochastic matrices and the diagonal of a rotation matrix,'' American Journal of Mathematics 76 (1954), 620–630.\n* [[Kadison, R. V.]]; [[Pedersen, G. K.]], ''Means and Convex Combinations of Unitary Operators'', Math. Scand. 57 (1985),249–266.\n* [[Kadison, R. V.]], ''The Pythagorean Theorem: I. The finite case'', Proc. Natl. Acad. Sci. USA, vol. 99 no. 7 (2002):4178–4184 (electronic)\n\n== External links ==\n* [http://mathworld.wolfram.com/HornsTheorem.html MathWorld]\n\n{{DEFAULTSORT:Schur-Horn theorem}}\n[[Category:Order theory]]\n[[Category:Theorems in linear algebra]]\n[[Category:Matrix theory]]\n[[Category:Spectral theory]]"
    },
    {
      "title": "Schur's theorem",
      "url": "https://en.wikipedia.org/wiki/Schur%27s_theorem",
      "text": "In [[discrete mathematics]], '''Schur's theorem''' is any of several theorems of the [[mathematician]] [[Issai Schur]]. In [[differential geometry]], '''Schur's theorem''' is a theorem of [[:de:Axel Schur|Axel Schur]]. In [[functional analysis]], '''Schur's theorem''' is often called [[Schur's property]], also due to Issai Schur.\n\n== Ramsey theory ==\n{{wikibooks|Combinatorics|Schur's Theorem|Proof of Schur's theorem}}\nIn [[Ramsey theory]], '''Schur's theorem''' states that for any [[Partition of a set|partition]] of the [[positive integer]]s into a finite number of parts, one of the parts contains three integers ''x'', ''y'', ''z'' with \n\n:<math>x + y = z.</math>\n\nMoreover, for every positive integer ''c'', there exists a number ''S''(''c''), called '''Schur's number''', such that for every partition of the integers \n\n:<math>\\{1,\\ldots, S(c)\\}</math>\n\ninto ''c'' parts, one of the parts contains integers ''x'', ''y'', and ''z'' with \n\n:<math>x + y = z.</math>\n\n[[Folkman's theorem]] generalizes Schur's theorem by stating that there exist arbitrarily large sets of integers, all of whose nonempty sums belong to the same part.\n\nUsing this definition, the first few Schur numbers are {{nobr|''S''(1) {{eq}} 2}}, 5, 14, 45, 161, ... ({{oeis|A030126}}) The proof that {{nobr|''S''(5) {{eq}} 161}} was announced in 2017 and took up 2 [[petabyte]]s of space.<ref>{{cite paper |last=Heule |first=Marijn J. H. |title=Schur Number Five |arxiv=1711.08076 |date=2017 }}</ref>\n\n== Combinatorics ==\nIn [[combinatorics]], '''Schur's theorem''' tells the number of ways for expressing a given number as a (non-negative, integer) linear combination of a fixed set of relatively prime numbers. In particular, if <math>\\{a_1,\\ldots,a_n\\}</math> is a set of integers such that <math> \\gcd(a_1,\\ldots,a_n)=1 </math>, the number of different tuples of non-negative integer numbers <math>(c_1,\\ldots,c_n)</math> such that <math>x=c_1a_1 + \\cdots + c_na_n</math> when <math>x</math> goes to infinity is:\n\n:<math>\\frac{x^{n-1}}{(n-1)!a_1\\cdots a_n}(1+o(1)).</math>\n\nAs a result, for every set of relatively prime numbers <math>\\{a_1,\\ldots,a_n\\}</math> there exists a value of <math>x</math> such that every larger number is representable as a linear combination of <math>\\{a_1,\\ldots,a_n\\}</math> in at least one way. This consequence of the theorem can be recast in a familiar context considering the problem of changing an amount using a set of coins. If the denominations of the coins are relatively prime numbers (such as 2 and 5) then any sufficiently large amount can be changed using only these coins. (See [[Coin problem]].)\n\n== Differential geometry ==\nIn [[differential geometry]], '''Schur's theorem''' compares the distance between the endpoints of a space curve <math>C^*</math> to the distance between the endpoints of a corresponding plane curve <math>C</math> of less curvature.\n\nSuppose <math>C(s)</math> is a plane curve with curvature <math>\\kappa(s)</math> which makes a convex curve when closed by the chord connecting its endpoints, and <math>C^*(s)</math> is a curve of the same length with curvature <math>\\kappa^*(s)</math>. Let <math>d</math> denote the distance between the endpoints of <math>C</math> and <math>d^*</math> denote the distance between the endpoints of <math>C^*</math>. If <math>\\kappa^*(s) \\leq \\kappa(s)</math> then <math>d^* \\geq d</math>.\n\n'''Schur's theorem''' is usually stated for <math>C^2</math> curves, but [[John M. Sullivan (mathematician)|John M. Sullivan]] has observed that Schur's theorem applies to curves of finite total curvature (the statement is slightly different).\n\n==  Linear algebra ==\n{{main|Schur decomposition}}\nIn [[linear algebra]] Schur’s theorem is referred to as either the triangularization of a square matrix with complex entries, or of a square matrix with real entries and real eigenvalues.\n\n==Functional analysis==\nIn [[functional analysis]] and the study of [[Banach space]]s, Schur's theorem, due to [[J. Schur]], often refers to [[Schur's property]], that for certain spaces, [[weak topology|weak convergence]] implies convergence in the norm.\n\n== Number theory ==\nIn [[number theory]], Issai Schur showed in 1912 that for every nonconstant polynomial ''p''(''x'') with integer coefficients, if ''S'' is the set of all nonzero values <math>\\begin{Bmatrix} p(n) \\neq 0 : n \\in \\mathbb{N} \\end{Bmatrix}</math>, then the set of primes that divide some member of ''S'' is infinite.\n\n==See also==\n\n*[[Schur's lemma (from Riemannian geometry)]]\n\n==References==\n{{reflist}}\n* Herbert S. Wilf (1994). [http://www.cs.utsa.edu/~wagner/CS3343/resources/gfology.pdf generatingfunctionology]. Academic Press.\n* [[Shiing-Shen Chern]] (1967). Curves and Surfaces in Euclidean Space. In ''Studies in Global Geometry and Analysis.'' Prentice-Hall.\n* Issai Schur (1912). Über die Existenz unendlich vieler Primzahlen in einigen speziellen arithmetischen Progressionen, Sitzungsberichte der Berliner Math.\n\n==Further reading==\n* Dany Breslauer and Devdatt P. Dubhashi (1995). [http://www.brics.dk/LS/95/4/BRICS-LS-95-4/BRICS-LS-95-4.html Combinatorics for Computer Scientists]\n* [[John M. Sullivan (mathematician)|John M. Sullivan]] (2006). [https://arxiv.org/pdf/math.GT/0606007 Curves of Finite Total Curvature]. arXiv.\n\n[[Category:Theorems in discrete mathematics]]\n[[Category:Ramsey theory]]\n[[Category:Additive combinatorics]]\n[[Category:Theorems in combinatorics]]\n[[Category:Theorems in differential geometry]]\n[[Category:Theorems in linear algebra]]\n[[Category:Theorems in functional analysis]]"
    },
    {
      "title": "Spectral theorem",
      "url": "https://en.wikipedia.org/wiki/Spectral_theorem",
      "text": "{{Short description|result about when a matrix can be diagonalized}}\nIn [[mathematics]], particularly [[linear algebra]] and [[functional analysis]], a '''spectral theorem''' is a result about when a [[linear operator]] or [[matrix (mathematics)|matrix]] can be [[Diagonalizable matrix|diagonalized]] (that is, represented as a [[diagonal matrix]] in some basis). This is extremely useful because computations involving a diagonalizable matrix can often be reduced to much simpler computations involving the corresponding diagonal matrix. The concept of diagonalization is relatively straightforward for operators on finite-dimensional vector spaces but requires some modification for operators on infinite-dimensional spaces. In general, the spectral theorem identifies a class of [[linear operator]]s that can be modeled by [[multiplication operator]]s, which are as simple as one can hope to find. In more abstract language, the spectral theorem is a statement about commutative [[C*-algebra]]s. See also [[spectral theory]] for a historical perspective.\n\nExamples of operators to which the spectral theorem applies are [[self-adjoint operator]]s or more generally [[normal operator]]s on [[Hilbert space]]s.\n\nThe spectral theorem also provides a [[canonical form|canonical]] decomposition, called the '''spectral decomposition''', '''eigenvalue decomposition''', or '''[[eigendecomposition of a matrix|eigendecomposition]]''', of the underlying vector space on which the operator acts.\n\n[[Augustin-Louis Cauchy]] proved the spectral theorem for [[Hermitian matrix|self-adjoint matrices]], i.e., that every real, symmetric matrix is diagonalizable. In addition, Cauchy was the first to be systematic about determinants.<ref>{{cite journal|url = http://www.sciencedirect.com/science/article/pii/0315086075900324 | doi=10.1016/0315-0860(75)90032-4 | volume=2 | title=Cauchy and the spectral theory of matrices | year=1975 | journal=Historia Mathematica | pages=1–29 | last1 = Hawkins | first1 = Thomas}}</ref><ref>[http://www.mathphysics.com/opthy/OpHistory.html A Short History of Operator Theory by Evans M. Harrell II]</ref> The spectral theorem as generalized by [[John von Neumann]] is today perhaps the most important result of operator theory.\n\nThis article mainly focuses on the simplest kind of spectral theorem, that for a [[self-adjoint]] operator on a Hilbert space. However, as noted above, the spectral theorem also holds for normal operators on a Hilbert space.\n\n== Finite-dimensional case ==<!-- This section is linked from [[Singular value decomposition]] -->\n\n=== Hermitian maps and Hermitian matrices ===\nWe begin by considering a [[Hermitian matrix]] on <math>\\mathbb{C}^n</math> (but the following discussion will be adaptable to the more restrictive case of [[symmetric matrix|symmetric matrices]] on  <math>\\mathbb{R}^n</math>). We consider a [[Hermitian operator|Hermitian map]] {{math|''A''}} on a finite-dimensional [[complex number|complex]] [[inner product space]] {{math|''V''}} endowed with a [[Definite bilinear form|positive definite]] [[sesquilinear form|sesquilinear]] [[inner product]] <math>\\langle\\cdot,\\cdot\\rangle</math>. The Hermitian condition on <math>A</math> means that for all {{math|''x'', ''y'' ∈ ''V''}},\n\n:<math> \\langle A x ,\\, y \\rangle =  \\langle x ,\\, A y \\rangle .</math>\n\n(An equivalent condition is that {{math|1=''A''<sup>∗</sup> = ''A''}}, where {{math|''A''<sup>∗</sup>}} is the [[hermitian conjugate]] of {{math|''A''}}.) In the case that {{math|''A''}} is identified with a Hermitian matrix, the matrix of {{math|''A''<sup>∗</sup>}} can be identified with its [[conjugate transpose]]. (If {{math|''A''}} is a [[real matrix]], this is equivalent to {{math|1=''A''<sup>T</sup> = ''A''}}, that is, {{math|''A''}} is a [[symmetric matrix]].)\n\nThis condition implies that all eigenvalues of a Hermitian map are real: it is enough to apply it to the case when {{math|1=''x'' = ''y''}} is an eigenvector. (Recall that an [[eigenvector]] of a linear map {{math|''A''}} is a (non-zero) vector {{math|''x''}} such that {{math|1=''Ax'' = ''λx''}} for some scalar {{math|''λ''}}. The value {{math|''λ''}} is the corresponding [[eigenvalue]]. Moreover, the [[eigenvalues]] are roots of the [[characteristic polynomial]].)\n\n'''Theorem'''. If {{math|''A''}} is Hermitian, there exists an [[orthonormal basis]] of {{math|''V''}} consisting of eigenvectors of {{math|''A''}}. Each eigenvalue is real.\n\nWe provide a sketch of a proof for the case where the underlying field of scalars is the [[complex number]]s.\n\nBy the [[fundamental theorem of algebra]], applied to the [[characteristic polynomial]] of {{math|''A''}}, there is at least one eigenvalue {{math|''λ''<sub>1</sub>}} and eigenvector {{math|''e''<sub>1</sub>}}. Then since \n:<math>\\lambda_1 \\langle e_1, e_1 \\rangle = \\langle A (e_1), e_1 \\rangle = \\langle e_1, A(e_1) \\rangle = \\bar\\lambda_1 \\langle e_1, e_1 \\rangle </math> \nwe find that {{math|''λ''<sub>1</sub>}} is real. Now consider the space {{math|1=''K'' = span{''e''<sub>1</sub>}<sup>⊥</sup>}}, the [[orthogonal complement]] of {{math|''e''<sub>1</sub>}}. By Hermiticity, {{math|''K''}} is an [[invariant subspace]] of {{math|''A''}}. Applying the same argument to {{math|''K''}} shows that {{math|''A''}} has an eigenvector {{math|''e''<sub>2</sub> ∈ ''K''}}. Finite induction then finishes the proof.\n\nThe spectral theorem holds also for symmetric maps on finite-dimensional real inner product spaces, but the existence of an eigenvector does not follow immediately from the  [[fundamental theorem of algebra]]. To prove this, consider {{math|''A''}} as a Hermitian matrix and use the fact that all eigenvalues of a Hermitian matrix are real.\n\nThe matrix representation of {{math|''A''}} in a basis of eigenvectors is diagonal, and by the construction the proof gives a basis of mutually orthogonal eigenvectors; by choosing them to be unit vectors one obtains  an orthonormal basis of eigenvectors. {{math|''A''}} can be written as a linear combination of pairwise orthogonal projections, called its '''spectral decomposition'''. Let\n\n:<math> V_\\lambda = \\{\\,v \\in V: A v = \\lambda v\\,\\}</math>\n\nbe the eigenspace corresponding to an eigenvalue {{math|''λ''}}. Note that the definition does not depend on any choice of specific eigenvectors. {{math|''V''}} is the orthogonal direct sum of the spaces {{math|''V''<sub>''λ''</sub>}} where the index ranges over eigenvalues. \n\nIn other words, if {{math|''P''<sub>''λ''</sub>}} denotes the [[Orthogonal projection#Orthogonal projections|orthogonal projection]] onto {{math|''V''<sub>''λ''</sub>}}, and {{math|''λ''<sub>1</sub>, ..., ''λ''<sub>''m''</sub>}} are the eigenvalues of {{math|''A''}}, then the spectral decomposition may be written as\n:<math>A =\\lambda_1 P_{\\lambda_1} +\\cdots+\\lambda_m P_{\\lambda_m}.</math>\n\nIf the spectral decomposition of A is <math>A =\\lambda_1 P_1 +\\cdots+\\lambda_m P_m</math>, then <math>A^2 =(\\lambda_1)^2 P_1 +\\cdots+ (\\lambda_m)^2 P_m</math> and <math>\\mu A = \\mu \\lambda_1 P_1 +\\cdots+ \\mu \\lambda_m P_m</math> for any scalar <math>\\mu.</math> It follows that for any polynomial {{mvar|f}} one has\n:<math>f(A) = f(\\lambda_1) P_1 +\\cdots+ f(\\lambda_m) P_m.</math>\n\nThe spectral decomposition is a special case of both the [[Schur decomposition]] and the [[singular value decomposition]].\n\n=== Normal matrices ===\n{{main|Normal matrix}}\nThe spectral theorem extends to a more general class of matrices. Let {{math|''A''}} be an operator on a finite-dimensional inner product space. {{math|''A''}} is said to be [[normal matrix|normal]]  if {{math|1=''A''<sup>∗</sup>''A'' = ''AA''<sup>∗</sup>}}. One can show that {{math|''A''}} is normal if and only if it is unitarily diagonalizable. Proof: By the [[Schur decomposition]], we can write any matrix as {{math|1=''A'' = ''UTU''<sup>∗</sup>}}, where {{math|''U''}} is unitary and {{math|''T''}} is upper-triangular.\nIf {{math|''A''}} is normal, one sees that {{math|1=''TT''<sup>∗</sup> = ''T''<sup>*</sup>''T''}}. Therefore, {{math|''T''}} must be diagonal since a normal upper triangular matrix is diagonal (see [[normal matrix#Consequences|normal matrix]]). The converse is obvious.\n\nIn other words, {{math|''A''}} is normal if and only if there exists a [[unitary matrix]] {{math|''U''}} such that\n\n:<math>A=U D U^*,</math>\n\nwhere {{math|''D''}} is a [[diagonal matrix]]. Then, the entries of the diagonal of {{math|''D''}} are the [[eigenvalue]]s of {{math|''A''}}. The column vectors of {{math|''U''}} are the eigenvectors of {{math|''A''}} and they are orthonormal. Unlike the Hermitian case, the entries of {{math|''D''}} need not be real.\n\n== Compact self-adjoint operators ==\n{{main|Compact operator on Hilbert space}}\nIn the more general setting of Hilbert spaces, which may have an infinite dimension, the statement of the spectral theorem for [[compact operator|compact]] [[self-adjoint operators]] is virtually the same as in the finite-dimensional case.\n\n'''Theorem'''. Suppose {{math|''A''}} is a compact self-adjoint operator on a (real or complex) Hilbert space {{math|''V''}}. Then there is an [[orthonormal basis]] of {{math|''V''}} consisting of eigenvectors of {{math|''A''}}. Each eigenvalue is real.\n\nAs for Hermitian matrices, the key point is to prove the existence of at least one nonzero eigenvector. One cannot rely on determinants to show existence of eigenvalues, but one can use a maximization argument analogous to the variational characterization of eigenvalues. \n\nIf the compactness assumption is removed, it is ''not'' true that every self-adjoint operator has eigenvectors.\n\n== Bounded self-adjoint operators ==\n{{See also|Eigenfunction|Self-adjoint operator#Spectral theorem}}\n\n===Possible absence of eigenvectors===\n\nThe next generalization we consider is that of [[bounded operator|bounded]] self-adjoint operators on a Hilbert space. Such operators may have no eigenvalues: for instance let {{math|''A''}} be the operator of multiplication by {{math|''t''}} on {{math|''L''<sup>2</sup>[0, 1]}}, that is,<ref>{{harvnb|Hall|2013}} Section 6.1</ref>\n\n:<math> [A \\varphi](t) = t \\varphi(t). \\;</math>\n\nNow, a physicist would say that <math>A</math> ''does'' have eigenvectors, namely the <math>\\varphi(t)=\\delta(t-t_0)</math>, where <math>\\delta</math> is a Dirac delta-function. A delta-function, however, is not a normalizable function; that is, it is not actually in the Hilbert space {{math|''L''<sup>2</sup>[0, 1]}}. Thus, the delta-functions are \"generalized eigenvectors\" but not eigenvectors in the strict sense.\n\n===Spectral subspaces and projection-valued measures===\n\nIn the absence of (true) eigenvectors, one can look for subspaces consisting of ''almost eigenvectors''. In the above example, for example, we might consider the subspace of functions supported on a small interval <math>[a,a+\\epsilon]</math> inside <math>[0,1]</math>. This space is invariant under <math>A</math> and for any <math>\\varphi</math> in this subspace, <math>A\\varphi</math> is very close to <math>a\\varphi</math>. In this approach to the spectral theorem, if <math>A</math> is a bounded self-adjoint operator, one looks for large families of such \"spectral subspaces\".<ref>{{harvnb|Hall|2013}} Theorem 7.2.1</ref> Each subspace, in turn, is encoded by the associated projection operator, and the collection of all the subspaces is then represented by a [[projection-valued measure]]. \n\nOne formulation of the spectral theorem expresses the operator {{math|''A''}} as an integral of the coordinate function over the operator's [[Eigenvector#Infinite dimensions|spectrum]] with respect to a projection-valued measure.<ref>{{harvnb|Hall|2013}} Theorem 7.12</ref>\n\n: <math> A = \\int_{\\sigma(A)} \\lambda \\, d E_{\\lambda} .</math>\n\nWhen the self-adjoint operator in question is [[compact operator|compact]], this version of the spectral theorem reduces to something similar to the finite-dimensional spectral theorem above, except that the operator is expressed as a finite or countably infinite linear combination of projections, that is, the measure consists only of atoms.\n\n===Multiplication operator version===\n\nAn alternative formulation of the spectral theorem says that every bounded self-adjoint operator is unitarily equivalent to a multiplication operator. The significance of this result is that multiplication operators are in many ways easy to understand.\n\n'''Theorem'''.<ref>{{harvnb|Hall|2013}} Theorem 7.20</ref> Let {{math|''A''}}  be a bounded self-adjoint operator on a Hilbert space {{math|''H''}}.  Then there is a [[measure space]] {{math|(''X'', Σ, ''μ'')}} and a real-valued [[ess sup|essentially bounded]] measurable function {{math|''f''}} on {{math|''X''}} and a [[unitary operator]] {{math|''U'':''H'' → ''L''<sup>2</sup><sub>''μ''</sub>(''X'')}} such that\n\n::<math> U^* T U = A,</math>\n\n:where {{math|''T''}} is the [[multiplication operator]]:\n\n::<math> [T \\varphi](x) = f(x) \\varphi(x).</math>\n\n:and <math>\\|T\\| = \\|f\\|_\\infty</math>\n\nThe spectral theorem is the beginning of the vast research area of functional analysis called [[operator theory]]; see also the [[spectral measure#Spectral measure|spectral measure]].\n\nThere is also an analogous spectral theorem for bounded [[normal operator]]s on Hilbert spaces.  The only difference in the conclusion is that now {{math|''f''}} may be complex-valued.\n\n===Direct integrals===\nThere is also a formulation of the spectral theorem in terms of direct integrals. It is similar to the multiplication-operator formulation, but more canonical.\n\nLet <math>A</math> be a bounded self-adjoint operator and let <math>\\sigma (A)</math> be the spectrum of <math>A</math>. The direct-integral formulation of the spectral theorem associates two quantities to <math>A</math>. First, a measure <math>\\mu</math> on <math>\\sigma (A)</math>, and second, a family of Hilbert spaces <math>\\{H_{\\lambda}\\},\\,\\,\\lambda\\in\\sigma (A).</math> We then form the direct integral Hilbert space\n::<math> \\int_\\mathbf{R}^\\oplus H_{\\lambda}\\, d \\mu(\\lambda). </math>\nThe elements of this space are functions (or \"sections\") <math>s(\\lambda),\\,\\,\\lambda\\in\\sigma(A),</math> such that <math>s(\\lambda)\\in H_{\\lambda}</math> for all <math>\\lambda</math>. \nThe direct-integral version of the spectral theorem may be expressed as follows:<ref>{{harvnb|Hall|2013}} Theorem 7.19</ref>\n:'''Theorem.''' If <math>A</math> is a bounded self-adjoint operator, then <math>A</math> is unitarily equivalent to the \"multiplication by <math>\\lambda</math>\" operator on \n::<math> \\int_\\mathbf{R}^\\oplus H_{\\lambda}\\, d \\mu(\\lambda) </math>\nfor some measure <math>\\mu</math> and some family <math>\\{H_{\\lambda}\\}</math> of Hilbert spaces. The measure <math>\\mu</math> is uniquely determined by <math>A</math> up to measure-theoretic equivalence; that is, any two measure associated to the same <math>A</math> have the same sets of measure zero. The dimensions of the Hilbert spaces <math>H_{\\lambda}</math> are uniquely determined by <math>A</math> up to a set of <math>\\mu</math>-measure zero.\n\nThe spaces <math>H_{\\lambda}</math> can be thought of as something like \"eigenspaces\" for <math>A</math>. Note, however, that unless the one-element set <math>{\\lambda}</math> has positive measure, the space <math>H_{\\lambda}</math> is not actually a subspace of the direct integral. Thus, the <math>H_{\\lambda}</math>'s should be thought of as \"generalized eigenspace\"—that is, the elements of <math>H_{\\lambda}</math> are \"eigenvectors\" that do not actually belong to the Hilbert space.\n\nAlthough both the multiplication-operator and direct integral formulations of the spectral theorem express a self-adjoint operator as unitarily equivalent to a multiplication operator, the direct integral approach is more canonical. First, the set over which the direct integral takes place (the spectrum of the operator) is canonical. Second, the function we are multiplying by is canonical in the direct-integral approach: Simply the function <math>\\lambda\\mapsto\\lambda</math>.\n\n===Cyclic vectors and simple spectrum===\nA vector <math>\\varphi</math> is called a '''cyclic vector''' for <math>A</math> if the vectors <math>\\varphi,A\\varphi,A^2\\varphi,\\ldots</math> span a dense subspace of the Hilbert space. Suppose <math>A</math> is a bounded self-adjoint operator for which a cyclic vector exists. In that case, there is no distinction between the direct-integral and multiplication-operator formulations of the spectral theorem. Indeed, in that case, there is a measure <math>\\mu</math> on the spectrum <math>\\sigma(A)</math> of <math>A</math> such that <math>A</math> is unitarily equivalent to the \"multiplication by <math>\\lambda</math>\" operator on <math>L^2(\\sigma(A),\\mu)</math>.<ref>{{harvnb|Hall|2013}} Lemma 8.11</ref> This result represents <math>A</math> simultaneously a multiplication operator ''and'' as a direct integral, since <math>L^2(\\sigma(A),\\mu)</math> is just a direct integral in which each Hilbert space <math>H_{\\lambda}</math> is just <math>\\mathbb{C}</math>.\n\nNot every bounded self-adjoint operator admits a cyclic vector; indeed, by the uniqueness in the direct integral decomposition, this can occur only when all the <math>H_{\\lambda}</math>'s have dimension one. When this happens, we say that <math>A</math> has \"simple spectrum\" in the sense of [[Self-adjoint_operator#Spectral_multiplicity_theory|spectral multiplicity theory]]. That is, a bounded self-adjoint operator that admits a cyclic vector should be thought of as the infinite-dimensional generalization of a self-adjoint matrix with distinct eigenvalues (i.e., each eigenvalue has multiplicity one).\n\nAlthough not every <math>A</math> admits a cyclic vector,  it is easy to see that we can decompose the Hilbert space as a direct sum of invariant subspaces on which <math>A</math> has a cyclic vector. This observation is the key to the proofs of the multiplication-operator and direct-integral forms of the spectral theorem.\n\n===Functional calculus===\nOne important application of the spectral theorem (in whatever form) is the idea of defining a [[functional calculus]]. That is, given a function <math>f</math> defined on the spectrum of <math>A</math>, we wish to define an operator <math>f(A)</math>. If <math>f</math> is simply a positive power, <math>f(x)=x^n</math>, then <math>f(A)</math> is just the <math>n\\mathrm{th}</math> power of <math>A</math>, <math>A^n</math>. The interesting cases are where <math>f</math> is a nonpolynomial function such as a square root or an exponential. Either of the versions of the spectral theorem provides such a functional calculus.<ref>E.g., {{harvnb|Hall|2013}} Definition 7.13</ref> In the direct-integral version, for example, <math>f(A)</math> acts as the \"multiplication by <math>f</math>\" operator in the direct integral:\n:<math>[f(A)s](\\lambda)=f(\\lambda)s(\\lambda)</math>.\nThat is to say, each space <math>H_{\\lambda}</math> in the direct integral is a (generalized) eigenspace for <math>f(A)</math> with eigenvalue <math>f(\\lambda)</math>.\n\n== General self-adjoint operators ==\nMany important linear operators which occur in [[Mathematical analysis|analysis]], such as [[differential operators]], are unbounded. There is also a spectral theorem for [[self-adjoint operator]]s that applies in these cases.  To give an example, every constant-coefficient differential operator is unitarily equivalent to a multiplication operator. Indeed, the unitary operator that implements this equivalence is the [[Fourier transform]]; the multiplication operator is a type of [[Multiplier (Fourier analysis)|Fourier multiplier]].\n\nIn general, spectral theorem for self-adjoint operators may take several equivalent forms.<ref>See Section 10.1 of {{harvnb|Hall|2013}}</ref> Notably, all of the formulations given in the previous section for bounded self-adjoint operators—the projection-valued measure version, the multiplication-operator version, and the direct-integral version—continue to hold for unbounded self-adjoint operators, with small technical modifications to deal with domain issues.\n\n== See also ==\n* [[Borel functional calculus]]\n* [[Spectral theory]]\n* [[Matrix decomposition]]\n* [[Canonical form]]\n* [[Jordan normal form|Jordan decomposition]], of which the spectral decomposition is a special case.\n* [[Singular value decomposition]], a generalisation of spectral theorem to arbitrary matrices.\n* [[Eigendecomposition of a matrix]]\n\n== Notes ==\n{{reflist}}\n\n== References ==\n* [[Sheldon Axler]], ''Linear Algebra Done Right'', Springer Verlag, 1997\n*{{citation | last = Hall |first = B.C. |title = Quantum Theory for Mathematicians|series=Graduate Texts in Mathematics|volume=267 | year = 2013 |publisher = Springer|isbn=978-1461471158}}\n* [[Paul Halmos]], [https://www.jstor.org/stable/2313117 \"What Does the Spectral Theorem Say?\"], ''American Mathematical Monthly'', volume 70, number 3 (1963), pages 241–247 [http://www.math.wsu.edu/faculty/watkins/Math502/pdfiles/spectral.pdf Other link]\n* [[Michael C. Reed|M. Reed]] and [[Barry Simon|B. Simon]], ''Methods of Mathematical Physics'', vols I–IV, Academic Press 1972.\n* [[Gerald Teschl|G. Teschl]], ''Mathematical Methods in Quantum Mechanics with Applications to Schrödinger Operators'', http://www.mat.univie.ac.at/~gerald/ftp/book-schroe/, American Mathematical Society, 2009.\n\n\n{{Functional Analysis}}\n\n[[Category:Spectral theory|*]]\n[[Category:Linear algebra]]\n[[Category:Matrix theory]]\n[[Category:Singular value decomposition]]\n[[Category:Theorems in functional analysis]]\n[[Category:Theorems in linear algebra]]"
    },
    {
      "title": "Borel–Weil–Bott theorem",
      "url": "https://en.wikipedia.org/wiki/Borel%E2%80%93Weil%E2%80%93Bott_theorem",
      "text": "In [[mathematics]], the '''Borel–Weil–Bott theorem''' is a basic result in the [[representation theory]] of [[Lie group]]s, showing how a family of representations can be obtained from holomorphic sections of certain complex [[vector bundle]]s, and, more generally, from higher [[sheaf cohomology]] groups associated to such bundles. It is built on the earlier '''Borel–Weil theorem''' of [[Armand Borel]] and [[André Weil]], dealing just with the space of sections (the zeroth cohomology group), the extension to higher cohomology groups being provided by [[Raoul Bott]]. One can equivalently, through Serre's [[GAGA]], view this as a result in [[complex algebraic geometry]] in the [[Zariski topology]].\n\n==Formulation==\nLet {{mvar|G}} be a [[Semisimple algebraic group|semisimple]] Lie group or [[Linear algebraic group|algebraic group]] over <math>\\mathbb C</math>, and fix a [[maximal torus]] {{mvar|T}} along with a [[Borel subgroup]] {{mvar|B}} which contains {{mvar|T}}. Let {{mvar|λ}} be an [[weight (representation theory)|integral weight]] of {{mvar|T}}; {{mvar|λ}} defines in a natural way a one-dimensional representation {{math|''C''<sub>''λ''</sub>}} of {{mvar|B}}, by pulling back the representation on {{math|1=''T'' = ''B''[[quotient group|/]]''U''}}, where  {{mvar|U}} is the [[unipotent radical]] of {{mvar|B}}. Since we can think of the projection map {{math|''G'' → ''G''/''B''}} as a [[Principal bundle|principal {{mvar|B}}-bundle]], for each {{math|''C''<sub>''λ''</sub>}} we get an [[associated fiber bundle]] {{math|''L''<sub>−λ</sub>}} on {{math|''G''/''B''}} (note the sign), which is obviously a [[line bundle]]. Identifying {{math|''L''<sub>''λ''</sub>}} with its [[sheaf (mathematics)|sheaf]] of holomorphic sections, we consider the [[sheaf cohomology]] groups <math>H^i( G/B, \\, L_\\lambda )</math>. Since {{mvar|G}} acts on the total space of the bundle <math>L_\\lambda</math> by bundle automorphisms, this action naturally gives a {{mvar|G}}-module structure on these groups; and the Borel–Weil–Bott theorem gives an explicit description of these groups as {{mvar|G}}-modules.\n\nWe first need to describe the [[Weyl group]] action centered at <math> - \\rho </math>. For any integral weight {{mvar|λ}} and {{mvar|w}} in the Weyl group {{mvar|W}}, we set <math>w*\\lambda :=  w( \\lambda + \\rho ) - \\rho \\,</math>, where {{mvar|ρ}} denotes the half-sum of positive roots of {{mvar|G}}. It is straightforward to check that this defines a group action, although this action is ''not'' linear, unlike the usual Weyl group action. Also, a weight {{mvar|μ}} is said to be ''dominant'' if <math>\\mu( \\alpha^\\vee ) \\geq 0</math> for all simple roots {{mvar|α}}. Let {{mvar|{{ell}}}} denote the [[Weyl group#Coxeter group structure|length function]] on {{mvar|W}}.\n\nGiven an integral weight {{mvar|λ}}, one of two cases occur:\n# There is no <math>w \\in W</math> such that <math>w*\\lambda</math> is dominant, equivalently, there exists a nonidentity <math>w \\in W</math> such that <math>w * \\lambda = \\lambda</math>; or\n# There is a ''unique'' <math>w \\in W</math> such that <math>w * \\lambda</math> is dominant.\nThe theorem states that in the first case, we have\n\n:<math>H^i( G/B, \\, L_\\lambda ) = 0</math> for all {{mvar|i}};\n\nand in the second case, we have\n\n:<math>H^i( G/B, \\, L_\\lambda ) = 0</math> for all <math>i \\neq \\ell(w)</math>, while\n\n:<math>H^{ \\ell(w) }( G/B, \\, L_\\lambda )</math> is the dual of the irreducible highest-weight representation of {{mvar|G}} with highest weight <math> w * \\lambda</math>.\n\nIt is worth noting that case (1) above occurs if and only if <math>\\lambda( \\beta^\\vee ) = 0</math> for some positive root {{mvar|β}}. Also, we obtain the classical '''Borel–Weil theorem''' as a special case of this theorem by taking {{mvar|λ}} to be dominant and {{mvar|w}} to be the identity element <math>e \\in W</math>.\n\n==Example==\nFor example, consider {{math|1=''G'' = [[SL2(C)|SL<sub>2</sub>('''C''')]]}}, for which {{math|''G''/''B''}} is the [[Riemann sphere]], an integral weight is specified simply by an integer {{mvar|n}}, and {{math|1=''ρ'' = 1}}. The line bundle {{math|''L''<sub>''n''</sub>}} is [[canonical bundle|<math>{\\mathcal O}(n)</math>]], whose [[section (fiber bundle)|sections]] are the [[homogeneous polynomial]]s of degree {{mvar|n}} (i.e. the ''binary forms''). As a representation of {{mvar|G}}, the sections can be written as {{math|Sym<sup>''n''</sup>('''C'''<sup>2</sup>)*}}, and is canonically isomorphic{{how|date=March 2014}} to {{math|Sym<sup>''n''</sup>('''C'''<sup>2</sup>)}}. \n\nThis gives us at a stroke the representation theory of <math>\\mathfrak{sl}_2(\\mathbf{C})</math>: <math>\\Gamma({\\mathcal O}(1))</math> is the standard representation, and <math>\\Gamma({\\mathcal O}(n))</math> is its {{mvar|n}}th [[symmetric power]]. We even have a unified description of the action of the Lie algebra, derived from its realization as vector fields on the Riemann sphere: if {{mvar|H}}, {{mvar|X}}, {{mvar|Y}} are the standard generators of <math>\\mathfrak{sl}_2(\\mathbf{C})</math>, then \n\n: <math>\n\\begin{align}\nH & = x\\frac{d}{dx}-y\\frac{d}{dy}, \\\\[5pt]\nX & = x\\frac{d}{dy}, \\\\[5pt]\nY & = y\\frac{d}{dx}.\n\\end{align}\n</math>\n\n{{further information|Jordan map}}\n\n==Positive characteristic==\nOne also has a weaker form of this theorem in positive characteristic. Namely, let {{mvar|G}} be a semisimple algebraic group over an [[algebraically closed field]] of characteristic <math>p > 0</math>. Then it remains true that <math>H^i( G/B, \\, L_\\lambda ) = 0</math> for all {{mvar|i}} if {{mvar|λ}} is a weight such that <math>w*\\lambda</math> is non-dominant for all <math>w \\in W</math> as long as {{mvar|λ}} is \"close to zero\".<ref name=\"Jantzen\">{{cite book|last=Jantzen|first=Jens Carsten|title=Representations of algebraic groups|edition=second|year=2003|publisher=American Mathematical Society|isbn=978-0-8218-3527-2}}</ref> This is known as the [[Kempf vanishing theorem]]. However, the other statements of the theorem do not remain valid in this setting.\n\nMore explicitly, let {{mvar|λ}} be a dominant integral weight; then it is still true that <math>H^i( G/B, \\, L_\\lambda ) = 0</math> for all <math>i > 0</math>, but it is no longer true that this {{mvar|G}}-module is simple in general, although it does contain the unique highest weight module of highest weight {{mvar|λ}} as a {{mvar|G}}-submodule. If {{mvar|λ}} is an arbitrary integral weight, it is in fact a large unsolved problem in representation theory to describe the cohomology modules <math>H^i( G/B, \\, L_\\lambda )</math> in general. Unlike over <math>\\mathbb{C}</math>, Mumford gave an example showing that it need not be the case for a fixed {{mvar|λ}} that these modules are all zero except in a single degree {{mvar|i}}.\n\n==Borel–Weil theorem==\nThe Borel–Weil theorem provides a concrete model for [[irreducible representation]]s of [[compact Lie group]]s and irreducible holomorphic representations of [[complex numbers|complex]] [[semisimple Lie group]]s. These representations are realized in the spaces of global [[Section (fiber bundle)|sections]] of [[holomorphic line bundle]]s on the [[flag manifold]] of the group. The Borel–Weil–Bott theorem is its generalization to higher cohomology spaces.  The theorem dates back to the early 1950s and can be found in {{harvtxt|Serre|1951-4}} and {{harvtxt|Tits|1955}}.\n\n=== Statement of the theorem ===\nThe theorem can be stated either for a complex semisimple Lie group {{math|''G''}} or for its [[Real form (Lie theory)|compact form]] {{math|''K''}}. Let {{math|''G''}} be a [[connected space|connected]] complex semisimple Lie group, {{math|''B''}} a [[Borel subgroup]] of {{math|''G''}}, and {{math|''X'' {{=}} ''G''/''B''}} the [[flag variety]]. In this scenario, {{math|''X''}} is a [[complex manifold]] and a nonsingular algebraic {{nowrap|{{math|''G''}}-variety}}. The flag variety can also be described as a compact [[homogeneous space]] {{math|''K''/''T''}}, where {{math|''T'' {{=}} ''K'' &cap; ''B''}} is a (compact) [[Cartan subgroup]] of {{math|''K''}}. An [[Weight (representation theory)#Integral weight|integral weight]] {{math|''λ''}} determines a {{nowrap|{{math|''G''}}-equivariant}} holomorphic line bundle {{math|''L''<sub>''λ''</sub>}} on {{math|''X''}} and the group {{math|''G''}} acts on its space of global sections,\n\n:<math>\\Gamma(G/B,L_\\lambda).\\ </math>\n\nThe Borel–Weil theorem states that if {{math|''λ''}} is a ''dominant'' integral weight then this representation is a ''holomorphic'' irreducible [[highest weight representation]] of {{math|''G''}} with highest weight {{math|''λ''}}. Its restriction to {{math|''K''}} is an [[irreducible unitary representation]] of {{math|''K''}} with highest weight {{math|''λ''}}, and each irreducible unitary representations of {{math|''K''}} is obtained in this way for a unique value of {{math|''λ''}}. (A holomorphic representation of a complex Lie group is one for which the corresponding Lie algebra representation is ''complex'' linear.)\n\n=== Concrete description ===\nThe weight {{math|''λ''}} gives rise to a character (one-dimensional representation) of the Borel subgroup {{math|''B''}}, which is denoted {{math|''&chi;''<sub>''λ''</sub>}}. Holomorphic sections of the holomorphic line bundle {{math|''L''<sub>''λ''</sub>}} over {{math|''G''/''B''}} may be described more concretely as [[holomorphic map]]s\n\n:<math> f: G\\to \\mathbb{C}_{\\lambda}: f(gb)=\\chi_{\\lambda}(b)f(g)</math>\n\nfor all {{math|''g'' &isin; ''G''}} and {{math|''b'' &isin; ''B''}}.\n\nThe action of {{math|''G''}} on these sections is given by \n: <math>g\\cdot f(h)=f(g^{-1}h)</math>\n\nfor {{math|''g'', ''h'' &isin; ''G''}}.\n\n=== Example ===\nLet {{math|''G''}} be the complex [[special linear group]] {{math|SL(2, '''C''')}}, with a Borel subgroup consisting of upper triangular matrices with determinant one. Integral weights for {{math|''G''}} may be identified with [[integer]]s, with dominant weights corresponding to nonnegative integers, and the corresponding characters {{math|''&chi;''<sub>''n''</sub>}} of {{math|''B''}} have the form\n\n: <math> \\chi_n\n\\begin{pmatrix}\na & b\\\\\n0 & a^{-1}\n\\end{pmatrix}=a^n.\n</math>\n\nThe flag variety {{math|''G''/''B''}} may be identified with the [[complex projective line]] {{math|'''CP'''<sup>1</sup>}} with [[homogeneous coordinates]] {{math|''X'', ''Y''}} and the space of the global sections of the line bundle {{math|''L''<sub>''n''</sub>}} is identified with the space of homogeneous polynomials of degree {{math|''n''}} on {{math|'''C'''<sup>''2''</sup>}}. For {{math|''n'' &ge; 0}}, this space has dimension {{math|''n'' + 1}} and forms an irreducible representation under the standard action of {{math|''G''}} on the polynomial algebra {{math|'''C'''[''X'', ''Y'']}}. Weight vectors are given by monomials\n\n: <math>  X^i Y^{n-i}, \\quad 0\\leq i\\leq n </math>\n\nof weights {{math|2''i'' &minus; ''n''}}, and the highest weight vector {{math|''X''<sup>''n''</sup>}} has weight {{math|''n''}}.\n\n== See also ==\n*[[Theorem of the highest weight]]\n\n==Notes==\n<references />\n\n==References==\n* {{Fulton-Harris}}.\n* {{citation|first1=Robert J.|last1=Baston|first2=Michael G.|last2=Eastwood|authorlink2=Michael Eastwood|title=The Penrose Transform: its Interaction with Representation Theory|publisher=Oxford University Press|year=1989}}. ([http://store.doverpublications.com/0486797295.html reprinted] by Dover)\n* {{Springer|id=b/b120400|title=Bott–Borel–Weil theorem}}\n*[http://www.math.harvard.edu/~lurie/papers/bwb.pdf A Proof of the Borel–Weil–Bott Theorem], by [[Jacob Lurie]]. Retrieved on Jul. 13, 2014.\n*{{citation\n | last = Serre | first = Jean-Pierre | authorlink = Jean-Pierre Serre\n | title = Représentations linéaires et espaces homogènes kählériens des groupes de Lie compacts (d'après Armand Borel et André Weil)\n | journal = Séminaire Bourbaki\n | volume = 2 | issue = 100 | pages = 447–454 | year = 1954 | orig-year=1951}}. In French; translated title: “Linear representations and Kähler homogeneous spaces of compact Lie groups (after Armand Borel and André Weil).”\n*{{citation\n | last = Tits | first = Jacques | authorlink = Jacques Tits\n | title = Sur certaines classes d'espaces homogènes de groupes de Lie\n | series = Acad. Roy. Belg. Cl. Sci. Mém. Coll.\n | volume = 29 | year = 1955}} In French.\n*{{citation\n | last = Sepanski | first = Mark R.\n | title = Compact Lie groups.\n | series = Graduate Texts in Mathematics | volume = 235\n | publisher = Springer | location = New York | year = 2007|isbn=9780387302638}}.\n*{{citation\n | last = Knapp | first = Anthony W.\n | title = Representation theory of semisimple groups: An overview based on examples\n | series = Princeton Landmarks in Mathematics\n | publisher = Princeton University Press | location = Princeton, NJ | year = 2001}}. Reprint of the 1986 original.\n\n==Further reading==\n* {{cite journal|last=Teleman |first=Constantin| authorlink=Constantin Teleman| title=Borel–Weil–Bott theory on the moduli stack of ''G''-bundles over a curve|journal=[[Inventiones Mathematicae]]| volume=134 |year=1998| issue=1|pages= 1–57| doi=10.1007/s002220050257| mr=1646586}}\n\n{{PlanetMath attribution|id=4585|title=Borel–Bott–Weil theorem}}\n\n{{DEFAULTSORT:Borel-Weil-Bott Theorem}}\n[[Category:Representation theory of Lie groups]]\n[[Category:Theorems in representation theory]]"
    },
    {
      "title": "Brauer's theorem on induced characters",
      "url": "https://en.wikipedia.org/wiki/Brauer%27s_theorem_on_induced_characters",
      "text": "'''Brauer's theorem on induced characters''', often known as '''Brauer's induction theorem''', and named after [[Richard Brauer]], is a basic result in the branch of [[mathematics]] known as [[character theory]], which is, in turn, part of the [[representation theory of a finite group]]. Let ''G'' be a [[finite group]] and let Char(''G'') denote the subring of the ring of complex-valued [[class function]]s of ''G'' consisting of integer combinations of [[Character theory|irreducible characters]]. Char(''G'') is known as the '''character ring''' of ''G'', and its elements are known as '''virtual characters''' (alternatively, as '''generalized characters''', or sometimes '''difference characters'''). It is a ring by virtue of the fact that the product of characters of ''G'' is again a character of ''G.'' Its multiplication is given by the elementwise product of class functions.\n\nBrauer's induction theorem shows that the character ring can be generated (as an [[abelian group]]) by   [[Character theory|induced character]]s of the form <math>\\lambda^{G}_{H}</math>, where ''H'' ranges over [[subgroup]]s of ''G'' and &lambda; ranges over [[Character theory|linear character]]s (having degree 1) of ''H''.\n\nIn fact, Brauer showed that the subgroups ''H'' could be chosen from a very\nrestricted collection, now called [[elementary group|'''Brauer elementary'''\nsubgroups]]. These are direct products of cyclic groups and groups whose order is a power of a prime.\n\nUsing [[Character theory|Frobenius reciprocity]], Brauer's induction theorem leads easily to his fundamental '''characterization of characters''', which asserts that a complex-valued class function of ''G'' is a virtual character if and only if its restriction to each Brauer elementary subgroup of ''G'' is a virtual character. This result, together with the fact that a virtual character &theta; is an irreducible character\nif and only if &theta;(1) ''> 0'' and <math>\\langle \\theta,\\theta \\rangle =1 </math> (where  <math>\\langle,\\rangle</math> is the usual [[Class function#Inner products|inner product on the ring of complex-valued class functions]]) gives\na means of constructing irreducible characters without explicitly constructing the associated representations.\n\nAn initial motivation for Brauer's induction theorem was application to [[Artin L-function]]s. It shows that those are built up from [[Dirichlet L-function]]s, or more general [[Hecke character|Hecke L-function]]s. Highly significant for that application is whether each character of ''G'' is a ''non-negative'' integer combination of characters induced from linear characters of subgroups. In general, this is not the case. In fact, by a theorem of Taketa, if all characters of ''G'' are so expressible, then ''G'' must be a  [[group theory|solvable group]] (although solvability alone does not guarantee such expressions- for example, the solvable group ''SL(2,3)'' has an irreducible complex character of degree 2 which is not expressible as a non-negative integer combination of characters induced from linear characters of subgroups). An ingredient of the proof of Brauer's induction theorem is that when ''G'' is a finite [[nilpotent group]], every complex irreducible character of ''G'' is induced from a linear character of some subgroup.\n\nA precursor to Brauer's induction theorem was [[Emil Artin|Artin]]'s induction theorem, which states that  |''G''| times the trivial character of ''G'' is an integer combination of characters which are each induced from trivial characters of cyclic subgroups of ''G.'' Brauer's theorem removes the factor |''G''|, \nbut at the expense of expanding the collection of subgroups used. Some years after the proof of Brauer's theorem appeared, [[James Alexander Green|J.A. Green]] showed (in 1955) that no such induction theorem (with integer combinations of characters induced from linear characters) could be proved with a collection of subgroups smaller than the Brauer elementary subgroups.\n\nThe proof of Brauer's induction theorem exploits the ring structure of Char(''G'') (most proofs also make use of a slightly larger ring, Char*(G), which consists of <math>\\mathbb{Z}[\\omega]</math>-combinations of irreducible characters, where &omega; is a primitive complex |''G''|-th root of unity). The set of integer combinations of characters induced from linear characters of Brauer  elementary subgroups is an ideal ''I''(''G'') of Char(''G''), so the proof reduces to showing that the trivial character is in ''I''(''G''). Several proofs of the theorem, beginning with a proof  due to Brauer and [[John Tate]], show that the trivial character is in the analogously defined ideal ''I''*(''G'') of Char*(''G'') by concentrating attention on one prime ''p'' at a time, and constructing integer-valued elements of ''I''*(''G'') which differ (elementwise) from the trivial character by (integer multiples of) a sufficiently high power of ''p.'' Once this is achieved for every prime divisor of |''G''|, some manipulations with congruences\nand [[algebraic integer]]s, again exploiting the fact that ''I''*(''G'') is an ideal of Ch*(''G''), place the trivial character in ''I''(''G''). An auxiliary result here is that a <math>\\mathbb{Z}[\\omega]</math>-valued class function lies in the ideal ''I''*(''G'') if its values are all divisible (in <math>\\mathbb{Z}[\\omega]</math>) by |''G''|.\n\nBrauer's induction theorem was proved in 1946, and there are now many alternative proofs. In 1986, Victor Snaith gave a proof by a radically different approach, topological in nature (an application of the [[Lefschetz fixed-point theorem]]). There has been related recent work on the question of finding natural and explicit forms of Brauer's theorem, notably by [[Robert Boltje]].\n\n==References==\n\n* {{cite book | author=Isaacs, I.M. | title=Character Theory of Finite Groups | publisher=Dover | year=1994 | origyear=1976 | isbn=0-486-68014-2 | zbl=0849.20004 }} Corrected reprint of the 1976 original, published by Academic Press. {{Zbl|0337.20005}}\n\n==Further reading==\n* {{cite book | title=Explicit Brauer Induction: With Applications to Algebra and Number Theory | volume=40 | series=Cambridge Studies in Advanced Mathematics | first=V. P. | last=Snaith | authorlink= | publisher=[[Cambridge University Press]] | year=1994 | isbn=0-521-46015-8 | zbl=0991.20005 }}\n\n[[Category:Representation theory of finite groups]]\n[[Category:Theorems in representation theory]]"
    },
    {
      "title": "Brauer's three main theorems",
      "url": "https://en.wikipedia.org/wiki/Brauer%27s_three_main_theorems",
      "text": "'''Brauer's main theorems''' are three theorems in [[representation theory of finite groups]] linking the [[modular representation theory|blocks]] of a [[finite group]] (in characteristic ''p'') with those of its [[p-local subgroup|''p''-local subgroups]], that is to say, the [[normalizer]]s of its non-trivial ''p''-subgroups.\n\nThe second and third main theorems allow refinements of orthogonality relations for [[character theory|ordinary character]]s which may be applied in finite [[group theory]]. These do not presently admit a proof purely in terms of ordinary characters. \nAll three main theorems are stated in terms of the '''Brauer correspondence'''.\n\n==Brauer correspondence==\nThere are many ways to extend the definition which follows, but this is close to the early treatments \nby Brauer. Let ''G'' be a finite group, ''p'' be a prime, ''F'' be a ''field'' of characteristic ''p''.\nLet ''H'' be a subgroup of ''G'' which contains\n\n:<math>QC_G(Q)</math>\n\nfor some ''p''-subgroup ''Q''\nof ''G,'' and is contained in the [[normalizer]]\n\n:<math>N_G(Q)</math>,\n\nwhere <math>C_G(Q)</math> is the [[centralizer]] of ''Q'' in ''G''.\n\nThe '''Brauer homomorphism''' (with respect to ''H'') is a linear map from the center of the group algebra of ''G'' over ''F'' to the corresponding algebra for ''H''. Specifically, it is the restriction to \n<math>Z(FG)</math> of the (linear) projection from <math>FG</math> to <math>FC_G(Q)</math> whose\nkernel is spanned by the elements of ''G'' outside <math>C_G(Q)</math>. The image of this map is contained in \n<math>Z(FH)</math>, and it transpires that the map is also a ring homomorphism.\n\nSince it is a [[ring homomorphism]], for any block ''B'' of ''FG'', the Brauer homomorphism \nsends the identity element of ''B'' either to ''0'' or to an idempotent element. In the latter case, \nthe idempotent may be decomposed as a sum of (mutually orthogonal) [[primitive idempotent]]s of ''Z(FH).'' \nEach of these primitive idempotents is the multiplicative identity of some block of ''FH.'' The block ''b'' of ''FH'' is said to be a '''Brauer correspondent''' of ''B'' if its identity element occurs\nin this decomposition of the image of the identity of ''B'' under the Brauer homomorphism.\n\n==Brauer's first main theorem==\n\nBrauer's first main theorem {{harvs|last=Brauer|year1=1944|year2=1956|year3=1970}} states that if <math>G</math> is a finite group a <math>D</math> is a <math>p</math>-subgroup of <math>G</math>, then there is a [[bijection]] between the set of \n(characteristic ''p'') blocks of <math>G</math> with defect group <math>D</math> and blocks of the normalizer <math>N_G(D)</math> with \ndefect group ''D''. This bijection arises because when <math>H  = N_G(D)</math>, each block of ''G''\nwith defect group ''D'' has a unique Brauer correspondent block of ''H'', which also has defect \ngroup ''D''.\n\n==Brauer's second main theorem==\n\nBrauer's second main theorem {{harvs|last=Brauer|year1=1944|year2=1959}} gives, for an element ''t'' whose order is a power of a prime ''p'', a criterion for a (characteristic ''p'') block of <math>C_G(t)</math> to correspond to a given block of <math>G</math>, via ''generalized decomposition numbers''. These are the coefficients which occur when the restrictions of ordinary characters of <math>G</math> (from the given block) to elements of the form ''tu'', where ''u'' ranges over elements of order prime to ''p'' in <math>C_G(t)</math>, are written as linear combinations of the irreducible [[modular representation theory|Brauer character]]s of <math>C_G(t)</math>. The content of the theorem is that it is only necessary to use Brauer characters from blocks of <math>C_G(t)</math> which are Brauer correspondents of the chosen block of ''G''.\n\n==Brauer's third main theorem==\n\nBrauer's third main theorem {{harv|Brauer|1964|loc=theorem3}} states that when ''Q'' is a ''p''-subgroup of the finite group ''G'',\nand ''H'' is a subgroup of ''G,'' containing <math>QC_G(Q)</math>, and contained in <math>N_G(Q)</math>,\nthen the [[modular representation theory|principal block]] of ''H'' is the only Brauer correspondent of the principal block of ''G'' (where the blocks referred to are calculated in characteristic ''p'').\n\n==References==\n\n*{{Citation | last1=Brauer | first1=R. | author1-link=Richard Brauer | title=On the arithmetic in a group ring | jstor=87919 | mr=0010547 | year=1944 | journal=[[Proceedings of the National Academy of Sciences|Proceedings of the National Academy of Sciences of the United States of America]] | issn=0027-8424 | volume=30 | pages=109–114 | doi=10.1073/pnas.30.5.109| pmid=16578120 }}\n*{{Citation | last1=Brauer | first1=R. | author1-link=Richard Brauer | title=On blocks of characters of groups of finite order I | jstor=87578 | mr=0016418 | year=1946 | journal=[[Proceedings of the National Academy of Sciences|Proceedings of the National Academy of Sciences of the United States of America]] | issn=0027-8424 | volume=32 | pages=182–186 | doi=10.1073/pnas.32.6.182| pmc=1078910 }}\n*{{Citation | last1=Brauer | first1=R. | author1-link=Richard Brauer | title=On blocks of characters of groups of finite order. II | jstor=87838 | mr=0017280 | year=1946 | journal=[[Proceedings of the National Academy of Sciences|Proceedings of the National Academy of Sciences of the United States of America]] | issn=0027-8424 | volume=32 | pages=215–219 | doi=10.1073/pnas.32.8.215| pmc=1078924 }}\n*{{Citation | last1=Brauer | first1=R. | author1-link=Richard Brauer | title=Zur Darstellungstheorie der Gruppen endlicher Ordnung | doi=10.1007/BF01187950 | mr=0075953 | year=1956 | journal=[[Mathematische Zeitschrift]] | issn=0025-5874 | volume=63 | pages=406–444}}\n*{{Citation | last1=Brauer | first1=R. | author1-link=Richard Brauer | title=Zur Darstellungstheorie der Gruppen endlicher Ordnung. II | doi=10.1007/BF01162934 | mr=0108542 | year=1959 | journal=[[Mathematische Zeitschrift]] | issn=0025-5874 | volume=72 | pages=25–46}}\n*{{Citation | last1=Brauer | first1=R. | author1-link=Richard Brauer | title=Some applications of the theory of blocks of characters of finite groups. I | doi=10.1016/0021-8693(64)90031-6 | mr=0168662 | year=1964 | journal=[[Journal of Algebra]] | issn=0021-8693 | volume=1 | pages=152–167}}\n*{{Citation | last1=Brauer | first1=R. | author1-link=Richard Brauer | title=On the first main theorem on blocks of characters of finite groups. | url=http://projecteuclid.org/euclid.ijm/1256053174 | mr=0267010 | year=1970 | journal=Illinois Journal of Mathematics | issn=0019-2082 | volume=14 | pages=183–187}}\n*{{Citation | last1=Dade | first1=Everett C. | author1-link=Everett C. Dade | editor1-last=Powell | editor1-first=M. B. | editor2-last=Higman | editor2-first=Graham | editor2-link=Graham Higman | title=Finite simple groups. Proceedings of an Instructional Conference organized by the London Mathematical Society (a NATO Advanced Study Institute), Oxford, September 1969. | publisher=[[Academic Press]] | location=Boston, MA | isbn=978-0-12-563850-0 | mr=0360785 | year=1971 | chapter=Character theory pertaining to finite simple groups | pages=249–327}} gives a detailed proof of the Brauer's main theorems.\n*{{eom|id=b/b120440|first=H.|last= Ellers|title=Brauer's first main theorem}}\n*{{eom|id=b/b120450|first=H.|last= Ellers|title=Brauer height-zero conjecture}}\n*{{eom|id=b/b120460|first=H.|last= Ellers|title=Brauer's second main theorem}}\n*{{eom|id=b/b120470|first=H.|last= Ellers|title=Brauer's third main theorem}}\n* [[Walter Feit]], ''The representation theory of finite groups.'' North-Holland Mathematical Library, 25. North-Holland Publishing Co., Amsterdam-New York, 1982. xiv+502 pp.&nbsp;{{ISBN|0-444-86155-6}}\n\n[[Category:Representation theory of finite groups]]\n[[Category:Theorems in representation theory]]"
    },
    {
      "title": "Commutation theorem",
      "url": "https://en.wikipedia.org/wiki/Commutation_theorem",
      "text": "In [[mathematics]], a  '''commutation theorem''' explicitly identifies the [[commutant]] of a specific [[von Neumann algebra]] acting on a [[Hilbert space]] in the presence of a [[Von Neumann algebra#Weights, states, and traces|trace]]. The first such result was proved by [[F.J. Murray]] and [[John von Neumann]] in the 1930s and applies to the von Neumann algebra generated by a [[discrete group]] or by the [[dynamical system]] associated with a\n[[ergodic theory|measurable transformation]] preserving a [[probability measure]]. Another important application is in the theory of [[unitary representation]]s of [[Haar measure|unimodular]] [[locally compact group]]s, where the theory has been applied to the [[regular representation]] and other closely related representations. In particular this framework led to an abstract version of the [[Plancherel theorem]] for unimodular locally compact groups due to [[Irving Segal]] and Forrest Stinespring and an abstract [[Plancherel theorem for spherical functions]] associated with a [[Gelfand pair]] due to [[Roger Godement]]. Their work was put in final form in the 1950s  by [[Jacques Dixmier]] as part of the theory of '''Hilbert algebras'''. It was not until the late 1960s, prompted partly by results in [[algebraic quantum field theory]] and [[quantum statistical mechanics]] due to the school of [[Rudolf Haag]], that the more general non-tracial [[Tomita–Takesaki theory]] was developed, heralding a new era in the theory of von Neumann algebras.\n\n==Commutation theorem for finite traces==\nLet ''H'' be a [[Hilbert space]] and ''M'' a [[von Neumann algebra]] on ''H'' with a unit vector Ω such that\n\n* ''M'' Ω  is dense in ''H''\n* ''M'' ' Ω  is dense in ''H'', where ''M'' ' denotes the [[commutant]] of ''M''\n* (''ab''Ω, Ω) = (''ba''Ω, Ω) for all ''a'', ''b'' in ''M''.\n\nThe vector Ω is called a ''cyclic-separating trace vector''. It is called a trace vector because the last condition means that the [[matrix coefficient]] corresponding to Ω defines a tracial [[state (functional analysis)|state]] on ''M''. It is called cyclic since Ω generates ''H'' as a topological ''M''-module. It is called separating\nbecause if ''a''Ω = 0 for ''a'' in ''M'', then ''aM'''Ω= (0), and hence ''a'' = 0.\n\nIt follows that the map\n\n:<math>Ja\\Omega=a^*\\Omega</math>\n\nfor ''a'' in ''M'' defines a conjugate-linear isometry of ''H'' with square the identity ''J''<sup>2</sup> = ''I''. The operator ''J'' is usually called the '''modular conjugation operator'''.\n\nIt is immediately verified that ''JMJ'' and ''M'' commute on the subspace ''M'' Ω, so that\n\n:<math>JMJ\\subseteq M^\\prime.</math>\n\nThe '''commutation theorem''' of Murray and von Neumann states that\n\n:{| border=\"1\" cellspacing=\"0\" cellpadding=\"5\"\n|<math>JMJ=M^\\prime</math>\n|}\n\nOne of the easiest ways to see this<ref name=\"rieffel\">{{harvnb|Rieffel|van Daele|1977}}</ref> is to introduce ''K'', the closure of the real\nsubspace ''M''<sub>sa</sub> Ω, where ''M''<sub>sa</sub> denotes the self-adjoint elements in ''M''. It follows that\n\n:<math> H=K\\oplus iK,</math>\n\nan orthogonal direct sum for the real part of inner product. This is just the real orthogonal decomposition for the ±1 eigenspaces of ''J''.\nOn the other hand for ''a'' in  ''M''<sub>sa</sub> and ''b'' in ''M'''<sub>sa</sub>, the inner product (''ab''Ω, Ω) is real, because ''ab'' is self-adjoint. Hence ''K'' is unaltered if ''M'' is replaced by ''M'' '.\n\nIn particular Ω is a trace vector for ''M''' and ''J'' is unaltered if ''M'' is replaced by ''M'' '. So the opposite inclusion\n\n:<math>JM^\\prime J\\subseteq M</math>\n \nfollows by reversing the roles of ''M'' and ''M'''.\n\n===Examples===\n* One of the simplest cases of the commutation theorem, where it can easily be seen directly, is that of a [[finite group]] Γ acting on the finite-dimensional [[inner product space]] <math>\\ell^2(\\Gamma)</math> by the left and right [[regular representation]]s λ and ρ. These [[unitary representation]]s are given by the formulas\n\n::<math>(\\lambda(g) f)(x)=f(g^{-1}x),\\,\\,(\\rho(g)f)(x)=f(xg)</math>\n\n:for ''f'' in <math>\\ell^2(\\Gamma)</math> and the commutation theorem implies that\n\n::<math>\\lambda(\\Gamma)^{\\prime\\prime}=\\rho(\\Gamma)^\\prime, \\,\\, \\rho(\\Gamma)^{\\prime\\prime}=\\lambda(\\Gamma)^\\prime.</math>\n\n:The operator ''J'' is given by the formula\n\n::<math> Jf(g)=\\overline{f(g^{-1})}.</math>\n\n:Exactly the same results remain true if Γ is allowed to be any [[countable]] [[discrete group]].<ref name=\"dixmier57\">{{harvnb|Dixmier|1957}}</ref> The von Neumann algebra λ(Γ)' ' is usually called the '''''group von Neumann algebra''''' of Γ.\n\n* Another important example is provided by a [[probability space]] (''X'', μ). The [[Abelian von Neumann algebra]] ''A'' = ''L''<sup>∞</sup>(''X'', μ) acts by [[multiplication operator]]s on ''H'' = ''L''<sup>2</sup>(''X'', μ) and the constant function 1 is a cyclic-separating trace vector. It follows that\n\n::<math>A^{\\prime}=A,</math>\n\n:so that ''A'' is a '''''maximal Abelian subalgebra''''' of ''B''(''H''), the von Neumann algebra of all [[bounded operator]]s on ''H''.\n\n* The third class of examples combines the above two. Coming from [[ergodic theory]], it was one of von Neumann's original motivations for studying von Neumann algebras. Let (''X'', μ) be a probability space and let Γ be a countable discrete group of measure-preserving transformations of  (''X'', μ). The group therefore acts unitarily on the Hilbert space ''H'' = ''L''<sup>2</sup>(''X'', μ) according to the formula\n\n::<math>U_g f(x) = f(g^{-1}x),</math>\n\n:for ''f'' in ''H'' and normalises the Abelian von Neumann  algebra ''A'' = ''L''<sup>∞</sup>(''X'', μ). Let\n\n::<math>H_1=H\\otimes \\ell^2(\\Gamma),</math>\n\n:a [[tensor product]] of Hilbert spaces.<ref>''H''<sub>1</sub> can be identified with the space of square integrable functions on ''X'' x  Γ with respect to the [[product measure]].</ref> The '''''group–measure space construction''''' or [[crossed product]] von Neumann algebra\n\n::<math> M = A \\rtimes \\Gamma</math>\n\n:is defined to be the von Neumann algebra on ''H''<sub>1</sub> generated by the algebra <math>A\\otimes I</math> and the normalising operators <math>U_g\\otimes \\lambda(g)</math>.<ref>It should not be confused with the von Neumann algebra on ''H''  generated by ''A'' and the operators  ''U''<sub>''g''</sub>.</ref>\n\n:The vector <math>\\Omega=1\\otimes \\delta_1</math> is a cyclic-separating trace vector. Moreover the modular conjugation operator ''J'' and commutant ''M'' ' can be explicitly identified.\n\nOne of the most important cases of the group–measure space construction is when Γ is the group of integers '''Z''', i.e. the case of a single invertible\nmeasurable transformation ''T''. Here ''T'' must preserve the probability measure μ. Semifinite traces are required to handle the case when ''T'' (or more generally  Γ) only preserves an infinite [[equivalence of measures|equivalent]] measure; and the full force of the [[Tomita–Takesaki theory]] is required when there is no invariant measure in the equivalence class, even though the equivalence class of the measure is preserved by ''T'' (or Γ).<ref>{{harvnb|Connes|1979}}</ref><ref name=\"Takesaki 2002\">{{harvnb|Takesaki|2002}}</ref>\n\n==Commutation theorem for semifinite traces==\nLet ''M'' be a von Neumann algebra and ''M''<sub>+</sub> the set of [[positive operator]]s in ''M''. By definition,<ref name=\"dixmier57\" /> a '''semifinite trace''' (or sometimes just '''trace''') on ''M'' is a functional τ from ''M''<sub>+</sub> into [0, ∞] such that\n\n# <math>\\tau(\\lambda a + \\mu b) = \\lambda \\tau(a) + \\mu \\tau(b)</math> for ''a'', ''b'' in ''M''<sub>+</sub> and λ, μ ≥ 0 (''{{visible anchor|semilinearity}}'');\n# <math>\\tau\\left(uau^*\\right) = \\tau(a)</math> for ''a'' in ''M''<sub>+</sub> and ''u'' a [[unitary operator]] in ''M'' (''unitary invariance'');\n# τ is completely additive on orthogonal families of projections in ''M'' (''normality'');\n# each projection in ''M'' is as orthogonal direct sum of projections with finite trace (''semifiniteness'').\n\nIf in addition τ is non-zero on every non-zero projection, then   τ is called a '''faithful trace'''.\n\nIf τ is a faithful trace on ''M'', let ''H'' = ''L''<sup>2</sup>(''M'', τ) be the Hilbert space completion of the inner product space\n\n:<math>M_0 = \\left\\{a \\in M \\mid \\tau\\left(a^*a\\right) < \\infty\\right\\}</math>\n\nwith respect to the inner product\n\n:<math>(a, b) = \\tau\\left(b^*a\\right).</math>\n\nThe von Neumann algebra ''M'' acts by left multiplication on ''H'' and can be identified with its image. Let\n\n:<math>Ja = a^*</math>\n\nfor ''a'' in ''M''<sub>0</sub>. The operator ''J'' is again called the ''modular conjugation operator'' and extends to a conjugate-linear isometry of ''H'' satisfying ''J''<sup>2</sup> = I. The commutation theorem of Murray and von Neumann\n\n:{| border=\"1\" cellspacing=\"0\" cellpadding=\"5\"\n|<math>JMJ = M^\\prime</math>\n|}\n\nis again valid in this case. This result can be proved directly by a variety of methods,<ref name=\"dixmier57\" /> but follows immediately from the result for finite traces, by repeated use of the following elementary fact:\n\n:''If'' ''M''<sub>1</sub> ⊇ ''M''<sub>2</sub> ''are two von Neumann algebras such that'' ''p''<sub>''n''</sub> ''M''<sub>1</sub> = ''p''<sub>''n''</sub> ''M''<sub>2</sub> ''for a family of projections'' ''p''<sub>''n''</sub> ''in the commutant of'' ''M''<sub>1</sub> ''increasing to'' ''I'' ''in the [[strong operator topology]], then'' ''M''<sub>1</sub> = ''M''<sub>2</sub>.\n\n==Hilbert algebras==\n{{see also|Tomita–Takesaki theory}}\nThe theory of Hilbert algebras was introduced by Godement (under the name \"unitary algebras\"), Segal and Dixmier to formalize the classical method of defining the trace for [[trace class operator]]s starting from [[Hilbert–Schmidt operator]]s.<ref>{{harvnb|Simon|1979}}</ref> Applications in the [[Unitary representation|representation theory of groups]] naturally lead to examples of Hilbert algebras. Every von Neumann algebra endowed with a semifinite trace has a canonical \"completed\"<ref>Dixmier uses the adjectives ''achevée'' or ''maximale''.</ref> or \"full\" Hilbert algebra associated with it; and conversely a completed Hilbert algebra of exactly this form can be canonically associated with every Hilbert algebra. The theory of Hilbert algebras can be used to deduce the commutation theorems of Murray and von Neumann; equally well the main results on Hilbert algebras can also be deduced directly from the commutation theorems for traces. The theory of Hilbert algebras was generalised by Takesaki<ref name=\"Takesaki 2002\"/> as a tool for proving commutation theorems for semifinite weights in [[Tomita–Takesaki theory]]; they can be dispensed with when dealing with states.<ref name =\"rieffel\" /><ref>{{harvnb|Pedersen|1979}}</ref><ref>{{harvnb|Bratteli|Robinson|1987}}</ref>\n\n===Definition===\nA '''Hilbert algebra'''<ref name=\"dixmier57\" /><ref>{{harvnb|Dixmier|1977}}, Appendix A54–A61.</ref><ref>{{harvnb|Dieudonné|1976}}</ref> is an algebra <math>\\mathfrak{A}</math> with involution ''x''→''x''* and an inner product (,) such that\n\n# (''a'', ''b'') = (''b''*, ''a''*) for ''a'', ''b'' in <math>\\mathfrak{A}</math>;\n# left multiplication by a fixed ''a'' in  <math>\\mathfrak{A}</math> is a bounded operator;\n# * is the adjoint, in other words (''xy'', ''z'') = (''y'', ''x''*''z'');\n# the linear span of all products ''xy'' is dense in <math>\\mathfrak{A}</math>.\n\n===Examples===\n* The Hilbert–Schmidt operators on an infinite-dimensional Hilbert space form a Hilbert algebra with inner product (''a'', ''b'') = Tr (''b''*''a'').\n* If (''X'', μ) is an infinite measure space, the algebra ''L''<sup>∞</sup> (''X'') <math>\\cap</math> ''L''<sup>2</sup>(''X'') is a Hilbert algebra with the usual inner product from ''L''<sup>2</sup>(''X'').\n* If ''M'' is a von Neumann algebra with faithful semifinite trace τ, then the *-subalgebra ''M''<sub>0</sub> defined above is a Hilbert algebra with inner product (''a'', '' b'') =  τ(''b''*''a'').\n* If ''G'' is a [[Haar measure|unimodular]] [[locally compact group]], the convolution algebra ''L''<sup>1</sup>(''G'')<math>\\cap</math>''L''<sup>2</sup>(''G'') is a Hilbert algebra with the usual inner product from ''L''<sup>2</sup>(''G'').\n* If (''G'', ''K'') is a [[Gelfand pair]], the convolution algebra ''L''<sup>1</sup>(''K''\\''G''/''K'')<math>\\cap</math>''L''<sup>2</sup>(''K''\\''G''/''K'') is a Hilbert algebra with the usual inner product from ''L''<sup>2</sup>(''G''); here ''L''<sup>''p''</sup>(''K''\\''G''/''K'') denotes the closed subspace of ''K''-biinvariant functions in ''L''<sup>''p''</sup>(''G'').\n* Any dense *-subalgebra of a Hilbert algebra is also a Hilbert algebra.\n\n===Properties===\nLet ''H'' be the Hilbert space completion of <math>\\mathfrak{A}</math> with respect to the inner product and let ''J'' denote the extension of the involution to a conjugate-linear involution of ''H''. Define a representation λ and an anti-representation ρ of \n<math>\\mathfrak{A}</math> on itself by left and right multiplication:\n\n:<math>\\lambda(a)x = ax,\\,\\, \\rho(a)x = xa.</math>\n\nThese actions extend continuously to actions on ''H''. In this case the commutation theorem for Hilbert algebras states that\n\n:{| border=\"1\" cellspacing=\"0\" cellpadding=\"5\"\n|<math>\\lambda(\\mathfrak{A})^{\\prime\\prime} = \\rho(\\mathfrak{A})^\\prime</math>\n|}\n\nMoreover if\n \n:<math>M = \\lambda(\\mathfrak{A})^{\\prime\\prime},</math>\n\nthe von Neumann algebra generated by the operators  λ(''a''), then\n\n:{| border=\"1\" cellspacing=\"0\" cellpadding=\"5\"\n|<math>JMJ = M^\\prime</math>\n|}\n\nThese results were proved independently by {{harvtxt|Godement|1954}} and {{harvtxt|Segal|1953}}.\n\nThe proof relies on the notion of \"bounded elements\" in the Hilbert space completion ''H''.\n\nAn element of ''x'' in ''H'' is said to be '''bounded''' (relative to <math>\\mathfrak{A}</math>) if the map ''a'' → ''xa'' of <math>\\mathfrak{A}</math> into ''H'' extends to a \nbounded operator on ''H'', denoted by λ(''x'').  In this case it is straightforward to prove that:<ref>{{harvnb|Godement|1954|pp=52–53}}</ref>\n\n* ''Jx'' is also a bounded element, denoted ''x''*, and λ(''x''*) = λ(''x'')*;\n* ''a'' → ''ax'' is given by the bounded operator ρ(''x'') = ''J''λ(''x''*)''J'' on ''H'';\n* ''M'' ' is generated by the ρ(''x'')'s with ''x'' bounded;\n*   λ(''x'') and ρ(''y'') commute for ''x'', ''y'' bounded.\n\nThe commutation theorem follows immediately from the last assertion. In particular\n\n* ''M'' =  λ(<math>\\mathfrak{B}</math>)\".\n\nThe space of all bounded elements <math>\\mathfrak{B}</math> forms a Hilbert algebra containing <math>\\mathfrak{A}</math> as a dense *-subalgebra. It is said to be  '''completed''' or '''full''' because any element in ''H'' bounded relative to <math>\\mathfrak{B}</math>must actually already lie in <math>\\mathfrak{B}</math>. The functional τ on ''M''<sub>+</sub> defined by\n\n:<math> \\tau(x) = (a,a)</math>\n\nif ''x'' = λ(a)*λ(a) and ∞ otherwise, yields a faithful semifinite trace on ''M'' with\n\n:<math>M_0 = \\mathfrak{B}.</math>\n\nThus:\n\n:{| border=\"1\" cellspacing=\"0\" cellpadding=\"5\"\n|'''''There is a one-one correspondence between von Neumann algebras on H with faithful semifinite trace and full Hilbert algebras with Hilbert space completion H.'''''\n|}\n\n==See also==\n* [[von Neumann algebra]]\n* [[Affiliated operator]]\n* [[Tomita–Takesaki theory]]\n\n==Notes==\n{{reflist|2}}\n\n==References==\n*{{citation|first=O.|last=Bratteli|first2=D.W.|last2=Robinson|title=Operator Algebras and Quantum Statistical Mechanics 1, Second Edition|publisher=Springer-Verlag|year=1987|isbn=3-540-17093-6}}\n*{{citation|first=A.|last=Connes|authorlink=Alain Connes|title=Sur la théorie non commutative de l’intégration|series=Lecture Notes in Mathematics|volume=(Algèbres d'Opérateurs)|publisher=Springer-Verlag|year=1979|pages=19–143|isbn=978-3-540-09512-5}}\n*{{citation|first=J.|last = Dieudonné|authorlink=Jean Dieudonné|title=Treatise on Analysis, Vol. II |year=1976|publisher=Academic Press|isbn=0-12-215502-5}}\n*{{citation|first=J.|last= Dixmier|authorlink=Jacques Dixmier|title=Les algèbres d'opérateurs dans l'espace hilbertien: algèbres de von Neumann|publisher= Gauthier-Villars  |year=1957}}\n*{{citation|first=J.|last= Dixmier|authorlink=Jacques Dixmier|title=Von Neumann algebras|publisher=North Holland| isbn=0-444-86308-7 |year=1981}} (English translation)\n*{{citation|first=J.|last= Dixmier|authorlink=Jacques Dixmier|title=Les C*-algèbres et leurs représentations|publisher= Gauthier-Villars|year=1969|isbn= 0-7204-0762-1}}\n*{{citation|first=J.|last=Dixmier|authorlink=Jacques Dixmier|title=C* algebras|publisher=North Holland|year=1977|isbn=0-7204-0762-1}} (English translation)\n*{{citation|first=R.|last=Godement|authorlink=Roger Godement|title=Mémoire sur la théorie des caractères dans les groupes localement compacts unimodulaires|journal=J. Math. Pures Appl.|volume= 30|year=1951|pages=1–110}}\n*{{citation|first=R.|last=Godement|authorlink=Roger Godement|title=Théorie des caractères. I. Algèbres unitaires|journal=Ann. of Math.|volume= 59|year=1954|pages=47–62|doi=10.2307/1969832|issue=1|publisher=Annals of Mathematics|jstor=1969832}}\n*{{citation|first=F.J.|last= Murray|authorlink1=F.J. Murray|first2=   J. |last2=von Neumann   |authorlink2=John von Neumann\n|title=On rings of operators| journal= Ann. of Math. |series=  2 |volume= 37  |year=1936|pages=116–229|doi=10.2307/1968693|jstor=1968693|issue=1|publisher=Annals of Mathematics}}\n*{{citation|first=F.J.|last= Murray|authorlink1=F.J. Murray|first2=   J. |last2=von Neumann|authorlink2=John von Neumann\n|title=On rings of operators II|journal= Trans. Amer. Math. Soc. |volume= 41  |year=1937|pages= 208–248|doi=10.2307/1989620|issue=2|jstor=1989620|publisher=American Mathematical Society}}\n*{{citation|first=F.J.|last= Murray|authorlink1=F.J. Murray|first2=   J. |last2=von Neumann |authorlink2=John von Neumann\n|title=On rings of operators IV|journal= Ann. of Math. |series=  2 |volume= 44  |year=1943|pages= 716–808|doi=10.2307/1969107|jstor=1969107|issue=4|publisher=Annals of Mathematics}}\n*{{citation|last=Pedersen|first=G.K.|title=C* algebras and their automorphism groups|series=London Mathematical Society Monographs|volume=14|year=1979|\npublisher=Academic Press|isbn=0-12-549450-5}}\n*{{citation|last=Rieffel|first= M.A.|last2= van Daele|first2=A.|title=A bounded operator approach to Tomita–Takesaki theory|journal=Pacific J. Math.|volume= 69 |year=1977|pages= 187–221|doi=10.2140/pjm.1977.69.187}}\n*{{citation|last=Segal|first=I.E.| authorlink=Irving Segal|title=A non-commutative extension of abstract integration|journal=Ann. of Math. |volume=57|year=1953|pages= 401–457|doi=10.2307/1969729|issue=3|publisher=Annals of Mathematics|jstor=1969729}} (Section 5)\n*{{citation|last=Simon|first= B.|authorlink=Barry Simon|title=Trace ideals and their applications|series=London Mathematical Society Lecture Note Series|volume= 35|publisher= Cambridge University Press|year= 1979|isbn = 0-521-22286-9}}\n*{{citation|first=M. |last=Takesaki |title=Theory of Operator Algebras II|publisher=Springer-Verlag|isbn= 3-540-42914-X|year=2002}}\n\n{{DEFAULTSORT:Commutation Theorem}}\n[[Category:Von Neumann algebras]]\n[[Category:Representation theory of groups]]\n[[Category:Ergodic theory]]\n[[Category:Theorems in functional analysis]]\n[[Category:Theorems in representation theory]]"
    },
    {
      "title": "Engel's theorem",
      "url": "https://en.wikipedia.org/wiki/Engel%27s_theorem",
      "text": "In [[representation theory]], a branch of mathematics, '''Engel's theorem''' is one of the basic theorems in the theory of [[Lie algebra]]s; it asserts that for a Lie algebra two concepts of [[nilpotent|nilpotency]] are identical. A useful form of the theorem says that if a Lie algebra '''L''' of matrices consists of nilpotent matrices, then they can all be simultaneously brought to a [[strictly upper triangular]] form. The theorem is named after the mathematician [[Friedrich Engel (mathematician)|Friedrich Engel]], who sketched a proof of it in a letter to [[Wilhelm Killing]] dated 20 July 1890 {{harv|Hawkins|2000|loc=p. 176}}.  Engel's student K.A. Umlauf gave a complete proof in his 1891 dissertation, reprinted as {{harv|Umlauf|2010}}.\n\nA linear operator ''T'' on a vector space ''V'' is defined to be '''nilpotent''' if there is a positive integer  ''k'' such that ''T''<sup>''k''</sup> = 0. For example, any operator given by a matrix whose entries are zero on and below its diagonal, such as \n\n:<math>  \n\\begin{bmatrix}\n0      & a_{1 2}& a_{1 3} & \\cdots & a_{1 n}  \\\\\n0      & 0      & a_{2 3} & \\cdots & a_{2 n}  \\\\\n\\vdots & \\vdots & \\ddots  & \\ddots & \\vdots   \\\\\n0      &   0    &         & \\ddots & a_{n-1 n}\\\\\n0      &   0    & \\cdots  & \\cdots &   0     \n\\end{bmatrix},\n</math>\nis nilpotent. An element ''x'' of a Lie algebra '''L''' is called ad-nilpotent if and only if the linear operator on '''L''' defined by \n\n:<math> \\operatorname{ad}_x (y) = [x,y]\\ </math>\n\nis nilpotent.  Note that in the Lie algebra ''L''(''V'') of linear operators on ''V'', the identity operator I<sub>''V''</sub> is ad-nilpotent (because <math> \\operatorname{ad}_{I_{V}} = 0 : L(V) \\rightarrow L(V) </math>) but is not a nilpotent operator.\n\nA Lie algebra '''L''' is defined to be nilpotent if and only if the [[lower central series]] defined [[recursion|recursively]] by\n\n:<math> \\mathbf{L}^0 =  \\mathbf{L}, \\quad \\mathbf{L}^{i+1} = [\\mathbf{L}, \\mathbf{L}^i]\\ </math>\n\neventually reaches {0}. \n\n'''Theorem'''.  A finite-dimensional Lie algebra '''L''' is nilpotent if and only if every element of '''L''' is ad-nilpotent.\n\nNote that no assumption on the underlying base field is required.\n\nThe key lemma in the proof of Engel's theorem is the following fact\nabout Lie algebras of linear operators on finite dimensional vector spaces which is useful in its own right:\n\nLet '''L''' be a Lie subalgebra of ''L''(''V''), ''n''=''dim(V)''.  Then '''L''' consists of nilpotent operators if and only if there is a sequence\n\n:<math> V_0 \\subsetneq V_1 \\subsetneq \\cdots \\subsetneq V_n\\ </math>\n\nof subspaces of ''V'' such that <math>V_0 = 0</math>, <math>V_n = V</math> and\n\n: <math> \\mathbf{L} \\, V_{i+1} \\subseteq V_i, \\quad \\forall i \\leq n-1.\\ </math>\n\nThus Lie algebras of nilpotent operators are simultaneously strictly upper-triangulizable.\n\n== See also ==\n* [[Lie's theorem]]\n\n==References==\n* [[Karin Erdmann|Erdmann, Karin]] & Wildon, Mark. ''Introduction to Lie Algebras'', 1st edition, Springer, 2006. {{ISBN|1-84628-040-0}}\n*{{Citation | last1=Hawkins | first1=Thomas | title=Emergence of the theory of Lie groups | url=https://books.google.com/books?isbn=978-0-387-98963-1 | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Sources and Studies in the History of Mathematics and Physical Sciences | isbn=978-0-387-98963-1 | mr=1771134 | year=2000}}\n* G. Hochschild, ''The Structure of Lie Groups'', Holden Day, 1965.\n* J. Humphreys, ''Introduction to Lie Algebras and Representation Theory'', Springer, 1972.\n*{{Citation | last1=Umlauf | first1=Karl Arthur | title=Über Die Zusammensetzung Der Endlichen Continuierlichen Transformationsgruppen, Insbesondre Der Gruppen Vom Range Null | origyear=1891 | url=https://books.google.com/books?isbn=978-1141588893 | publisher=Nabu Press | language=German | series=Inaugural-Dissertation, Leipzig | isbn=978-1-141-58889-3 | year=2010}}\n\n[[Category:Theorems in representation theory]]\n[[Category:Representation theory of Lie algebras]]"
    },
    {
      "title": "Frobenius reciprocity",
      "url": "https://en.wikipedia.org/wiki/Frobenius_reciprocity",
      "text": "In [[mathematics]], and in particular [[representation theory]], '''Frobenius reciprocity''' is a theorem expressing a [[Duality (mathematics)|duality]] between the process of [[Restricted representation|restricting]] and [[Induced representation|inducting]]. It can be used to leverage knowledge about representations of a subgroup to find and classify representations of \"large\" groups that contain them. It is named for [[Ferdinand Georg Frobenius]], the inventor of the [[representation theory of finite groups]].\n\n== Statement ==\n\n=== Character theory ===\n\nThe theorem was originally stated in terms of [[character theory]]. Let {{var|G}} be a finite [[Group (mathematics)|group]] with a [[subgroup]] {{var|H}}, let <math>\\operatorname{Res}_H^G</math> denote the restriction of a character, or more generally, [[Class function (algebra)|class function]] of {{var|G}} to {{var|H}}, and let <math>\\operatorname{Ind}_H^G</math> denote the [[Induced representation|induced class function]] of a given class function on {{var|H}}. For any finite group {{var|A}}, there is an [[Inner product space|inner product]] <math>\\langle -,-\\rangle_A</math> on the [[vector space]] of class functions <math>A\\to\\mathbb{C}</math> (described in detail in the article [[Schur orthogonality relations]]). Now, for any class functions <math>\\psi:H\\to\\mathbb{C}</math> and <math>\\varphi:G\\to\\mathbb{C}</math>, the following equality holds:\n\n:<math display=\"block\">\\langle\\operatorname{Ind}_H^G\\psi, \\varphi\\rangle_G=\\langle\\psi,\\operatorname{Res}_H^G\\varphi\\rangle_H</math>.{{sfn|Serre|1977|p=56}}{{sfn|Sengupta|2012|p=246}}\n\nIn other words, <math>\\operatorname{Ind}_H^G</math> and <math>\\operatorname{Res}_H^G</math> are [[Hermitian adjoint]].\n\n{{Collapse top|title=Proof of Frobenius reciprocity for class functions}}\nLet <math>\\psi:H\\to\\mathbb{C}</math> and <math>\\varphi:G\\to\\mathbb{C}</math> be class functions.\n\n'''Proof.''' Every class function can be written as a [[linear combination]] of irreducible characters. As <math>\\langle\\cdot,\\cdot\\rangle</math> is a [[bilinear form]], we can, without loss of generality, assume <math>\\psi</math> and <math>\\varphi</math> to be characters of irreducible representations of <math>H</math> in <math>W</math> and of <math>G</math> in <math>V,</math> respectively.\nWe define <math> \\psi(s)=0</math> for all <math>s\\in G\\setminus H.</math> Then we have\n:<math display=\"block\"> \\begin{align}  \n\\langle \\text{Ind}(\\psi), \\varphi\\rangle_G &= \\frac{1}{|G|} \\sum_{t\\in G} \\text{Ind}(\\psi)(t) \\varphi(t^{-1}) \\\\\n&= \\frac{1}{|G|} \\sum_{t\\in G} \\frac{1}{|H|}\\sum_{s\\in G \\atop s^{-1}ts \\in H} \\psi(s^{-1}ts) \\varphi(t^{-1}) \\\\\n&= \\frac{1}{|G|} \\frac{1}{|H|}\\sum_{t\\in G} \\sum_{s\\in G} \\psi(s^{-1}ts) \\varphi((s^{-1}ts)^{-1}) \\\\\n&= \\frac{1}{|G|} \\frac{1}{|H|}\\sum_{t\\in G} \\sum_{s\\in G} \\psi(t) \\varphi(t^{-1})\\\\\n&= \\frac{1}{|H|}\\sum_{t\\in G} \\psi(t) \\varphi(t^{-1})\\\\\n&= \\frac{1}{|H|}\\sum_{t\\in H} \\psi(t) \\varphi(t^{-1})\\\\\n&= \\frac{1}{|H|}\\sum_{t\\in H} \\psi(t) \\text{Res}(\\varphi)(t^{-1})\\\\\n&= \\langle \\psi, \\text{Res}(\\varphi)\\rangle_H \n\\end{align} </math>\n\nIn the course of this sequence of equations we used only the definition of induction on class functions and the [[#Properties character|properties of characters]]. <math>\\Box</math>\n\n'''Alternative proof.''' In terms of the group algebra, i.e. by the alternative description of the induced representation, the Frobenius reciprocity is a special case of a general equation for a change of rings:\n:<math>\\text{Hom}_{\\Complex [H]}(W,U)=\\text{Hom}_{\\Complex [G]}(\\Complex [G]\\otimes_{\\Complex [H]}W, U).</math>\nThis equation is by definition equivalent to\n:<math>\\langle W,\\text{Res}(U)\\rangle_H=\\langle W,U\\rangle_H=\\langle \\text{Ind}(W),U\\rangle_G.</math>\nAs this bilinear form tallies the bilinear form on the corresponding characters, the theorem follows without calculation. <math>\\Box</math>\n{{Collapse bottom}}\n\n=== Module theory ===\n{{See also|Change of rings#Relation between the extension of scalars and the restriction of scalars}}\n\nAs explained in the section [[Representation theory of finite groups#Representations, modules and the convolution algebra]], the theory of the representations of a group {{var|G}} over a field {{var|K}} is, in a certain sense, equivalent to the theory of [[Module (mathematics)|modules]] over the [[group algebra]] {{var|K}}[{{var|G}}].<ref>Specifically, there is an [[isomorphism of categories]] between '''{{var|K}}[{{var|G}}]-Mod''' and '''Rep<sub>{{var|G}}</sub><sup>{{var|K}}</sup>''', as described on the pages [[Isomorphism of categories#Category of representations]] and [[Representation theory of finite groups#Representations, modules and the convolution algebra]].</ref> Therefore, there is a corresponding Frobenius reciprocity theorem for {{var|K}}[{{var|G}}]-modules.\n\nLet {{var|G}} be a group with subgroup {{var|H}}, let {{var|M}} be an {{var|H}}-module, and let {{var|N}} be a {{var|G}}-module. In the language of module theory, the [[induced module]] <math>K[G]\\otimes_{K[H]} M</math> corresponds to the induced representation <math>\\operatorname{Ind}_H^G</math>, whereas the [[Change of rings#Restriction of scalars|restriction of scalars]] <math>{_{K[H]}}N</math> corresponds to the restriction <math>\\operatorname{Res}_H^G</math>. Accordingly, the statement is as follows: The following sets of module homomorphisms are in bijective correspondence:\n\n:<math display=\"block\">\\operatorname{Hom}_{K[G]}(K[G]\\otimes_{K[H]} M,N)\\cong \\operatorname{Hom}_{K[H]}(M,{_{K[H]}}N)</math>.<ref>{{Cite book|url=https://www.worldcat.org/oclc/52220683|title=Representations and characters of groups|last=James|first=Gordon Douglas|date=1945–2001|publisher=Cambridge University Press|others=Liebeck, M. W. (Martin W.)|isbn=9780521003926|edition=2nd|location=Cambridge, UK|oclc=52220683}}</ref>{{sfn|Sengupta|2012|p=245}}\n\nAs noted below in the section on category theory, this result applies to modules over all rings, not just modules over group algebras.\n\n=== Category theory ===\n\nLet {{var|G}} be a group with a subgroup {{var|H}}, and let <math>\\operatorname{Res}_H^G,\\operatorname{Ind}_H^G</math> be defined as above. For any group {{math|A}} and [[Field (mathematics)|field]] {{math|K}} let <math>\\textbf{Rep}_A^K</math> denote the [[Category (mathematics)|category]] of linear representations of {{var|A}} over {{var|K}}. There is a [[forgetful functor]]\n\n:<math display=\"block\">\\begin{align}\n  \\operatorname{ResF}_H^G:\\textbf{Rep}_G&\\longrightarrow\\textbf{Rep}_H \\\\\n  (V,\\rho) &\\longmapsto \\operatorname{Res}_G^H(V,\\rho)\n\\end{align}</math>\n\nThis functor acts as the [[Identity function|identity]] on [[morphism]]s. There is a functor going in the opposite direction:\n\n:<math display=\"block\">\\begin{align}\n  \\operatorname{IndF}_H^G:\\textbf{Rep}_H &\\longrightarrow\\textbf{Rep}_G \\\\\n  (W,\\tau) &\\longmapsto \\operatorname{Ind}_H^G(W,\\tau)\n\\end{align}</math>\n\nThese functors form an [[Adjoint functors|adjoint pair]] <math>\\operatorname{IndF}_H^G\\dashv\\operatorname{ResF}_H^G</math>.<ref>{{Cite web|url=http://planetmath.org/frobeniusreciprocity|title=Frobenius reciprocity on planetmath.org|website=planetmath.org|language=en|access-date=2017-11-02}}</ref><ref>{{Cite web|url=https://ncatlab.org/nlab/show/Frobenius+reciprocity#InRepresentationTheory|title=Frobenius reciprocity in nLab|website=ncatlab.org|access-date=2017-11-02}}</ref> In the case of finite groups, they are actually both left- and right-adjoint to one another. This adjunction gives rise to a [[universal property]] for the induced representation (for details, see [[Induced representation#Properties]]).\n\nIn the language of module theory, the corresponding adjunction is an instance of the more general [[Change of rings#Relation between the extension of scalars and the restriction of scalars|relationship between restriction and extension of scalars]].\n\n== See also ==\n{{Portal|Algebra}}\n* See [[Restricted representation]] and [[Induced representation]] for definitions of the processes to which this theorem applies.\n* See [[Representation theory of finite groups]] for a broad overview of the subject of group representations.\n* See [[Selberg trace formula]] and the [[Arthur-Selberg trace formula]] for generalizations to discrete cofinite subgroups of certain locally compact groups.\n\n== Notes ==\n{{Reflist}}\n\n== References ==\n{{Refbegin}}\n* {{Cite book|url=https://www.worldcat.org/oclc/2202385|title=Linear representations of finite groups|last=Serre|first=Jean-Pierre|date=1926–1977|publisher=Springer-Verlag|isbn=0387901906|location=New York|oclc=2202385}}\n* {{Cite book|url=https://www.worldcat.org/oclc/769756134|title=Representing finite groups : a semisimple introduction|last=Sengupta|first=Ambar|year=2012|doi=10.1007/978-1-4614-1231-1_8|isbn=9781461412304|location=New York|oclc=769756134}}\n* {{Cite web|url=http://mathworld.wolfram.com/InducedRepresentation.html|title=Induced Representation|last=Weisstein|first=Eric|website=mathworld.wolfram.com|language=en|access-date=2017-11-02}}\n{{Refend}}\n\n[[Category:Representation theory of finite groups]]\n[[Category:Theorems in representation theory]]\n[[Category:Adjoint functors]]"
    },
    {
      "title": "Gabriel's theorem",
      "url": "https://en.wikipedia.org/wiki/Gabriel%27s_theorem",
      "text": "In mathematics, '''Gabriel's theorem''', proved by [[Pierre Gabriel]],  classifies the [[quiver (mathematics)|quivers]] of finite type in terms of [[Dynkin diagram]]s.\n\n==Statement==\n\nA quiver is of '''finite type''' if it has only finitely many isomorphism classes of indecomposable representations. {{harvtxt|Gabriel|1972}} classified all quivers of finite type, and also their indecomposable representations. More precisely, Gabriel's theorem states that:\n\n# A (connected) quiver is of finite type if and only if its underlying graph (when the directions of the arrows are ignored) is one of the [[ADE classification|ADE]] [[Dynkin diagram]]s: <math>A_n</math>, <math>D_n</math>, <math>E_6</math>, <math>E_7</math>, <math>E_8</math>.\n# The indecomposable representations are in a one-to-one correspondence with the positive roots of the [[root system]] of the Dynkin diagram.\n\n{{harvtxt|Dlab|Ringel|1973}} found a generalization of Gabriel's theorem in which all Dynkin diagrams of finite-dimensional semisimple Lie algebras occur.\n\n==References==\n\n*{{Citation | last1=Bernšteĭn | first1=I. N. | last2=Gelfand | first2=I. M. | last3=Ponomarev | first3=V. A. | title=Coxeter functors, and Gabriel's theorem | doi=10.1070/RM1973v028n02ABEH001526 | mr=0393065 | year=1973 | journal=Russian Mathematical Surveys | issn=0042-1316 | volume=28 | issue=2 | pages=17–32| citeseerx=10.1.1.642.2527 }}\n*{{Citation | last1=Dlab | first1=Vlastimil | last2=Ringel | first2=Claus Michael | title=On algebras of finite representation type | url=https://books.google.com/books?id=_JrnAAAAMAAJ | publisher=Department of Mathematics, Carleton Univ., Ottawa, Ont. | series=Carleton mathematical lecture notes | mr=0347907 | year=1973 | volume=2}}\n*{{Citation | last1=Gabriel | first1=Peter | title=Unzerlegbare Darstellungen. I | doi=10.1007/BF01298413 | mr=0332887 | year=1972 | journal=Manuscripta Mathematica | issn=0025-2611 | volume=6 | pages=71–103}}\n\n[[Category:Theorems in representation theory]]"
    },
    {
      "title": "Haboush's theorem",
      "url": "https://en.wikipedia.org/wiki/Haboush%27s_theorem",
      "text": "In [[mathematics]] '''Haboush's theorem''', often still referred to as the '''Mumford conjecture''', states that for any [[Semisimple algebraic group|semisimple]] [[algebraic group]] ''G'' over a [[field (mathematics)|field]] ''K'', and for any linear representation ρ of ''G'' on a ''K''-[[vector space]] ''V'', given ''v''&nbsp;≠&nbsp;0 in ''V'' that is fixed by the action of ''G'', there is a [[G-invariant|''G''-invariant]] [[polynomial]] ''F'' on ''V'', without constant term,  such that\n\n:''F''(''v'') &ne; 0.\n\nThe polynomial can be taken to be [[homogeneous polynomial|homogeneous]], in other words an element of a symmetric power of the dual of ''V'', and if the characteristic is ''p''>0 the degree of the polynomial can be taken to be a power of ''p''.  \nWhen ''K'' has characteristic 0 this was well known; in fact Weyl's theorem on the complete reducibility of the representations of ''G'' implies that ''F'' can even be taken to be linear. Mumford's conjecture about the extension to prime characteristic ''p'' was proved  by W. J. {{harvtxt|Haboush|1975}}, about a decade after the problem had been posed by [[David Mumford]], in the introduction to the first edition of his book ''Geometric Invariant Theory''.\n\n==Applications==\nHaboush's theorem can be used to generalize  results of [[geometric invariant theory]] from characteristic 0, where they were already known, to characteristic ''p''>0. In particular Nagata's earlier results together with Haboush's theorem show that if a reductive group (over an algebraically closed field) acts on a finitely generated algebra then the fixed subalgebra is also finitely generated.\n\nHaboush's theorem implies that if ''G'' is a reductive algebraic group acting regularly on an affine algebraic variety, then disjoint closed invariant sets ''X'' and ''Y'' can be separated by an invariant function ''f'' (this means that ''f'' is 0 on ''X'' and 1 on ''Y'').\n\nC.S. Seshadri (1977) extended Haboush's theorem to reductive groups over schemes.\n\nIt follows from the work of {{harvtxt|Nagata|1963}}, Haboush, and Popov that the following conditions are equivalent for an affine algebraic group ''G'' over a field ''K'':\n*''G'' is reductive (its unipotent radical is trivial).\n*For any non-zero invariant vector in a rational representation of ''G'', there is an invariant homogeneous polynomial that does not vanish on it.\n*For any finitely generated ''K'' algebra on which ''G'' act rationally, the algebra of fixed elements is finitely generated.\n\n==Proof==\nThe theorem is proved in several steps as follows:\n*We can assume that the group is defined over an [[algebraically closed]] field ''K'' of characteristic ''p''>0.\n*Finite groups are easy to deal with as one can just take a product over all elements, so one can reduce to the case of '''connected''' reductive groups (as the connected component has finite index). By taking a central extension which is harmless one can also assume the group ''G'' is '''simply connected'''.\n*Let ''A''(''G'') be the coordinate ring of ''G''. This is a representation of ''G'' with ''G'' acting by left translations. Pick an element ''v&prime;'' of the dual of ''V'' that has value 1 on the invariant vector ''v''. The map ''V'' to ''A''(''G'') by sending ''w''∈''V'' to the element ''a''∈''A''(''G'') with ''a''(''g'') = ''v''&prime;(''g''(''w'')). This sends ''v'' to 1∈''A''(''G''), so we can assume that ''V''⊂''A''(''G'') and ''v''=1.\n*The structure of the representation ''A''(''G'') is given as follows. Pick a maximal torus ''T'' of ''G'', and let it act on  ''A''(''G'') by right translations (so that it commutes with the action of ''G''). Then ''A''(''G'') splits as a sum over characters λ of ''T'' of the subrepresentations ''A''(''G'')<sup>λ</sup> of elements transforming according to λ. So we can assume that ''V'' is contained in the ''T''-invariant subspace ''A''(''G'')<sup>λ</sup> of ''A''(''G'').\n*The representation ''A''(''G'')<sup>λ</sup> is an increasing union of subrepresentations of the form ''E''<sub>λ+''n''ρ</sub>⊗''E''<sub>''n''ρ</sub>, where ρ is the Weyl vector for a choice of simple roots of ''T'', ''n'' is a positive integer, and ''E''<sub>μ</sub> is the space of sections of the [[line bundle]] over ''G''/''B'' corresponding to a character μ of ''T'', where ''B'' is a [[Borel subgroup]] containing ''T''.\n*If ''n'' is sufficiently large then ''E''<sub>''n''ρ</sub> has dimension (''n''+1)<sup>''N''</sup> where ''N'' is the number of positive roots. This is because in characteristic 0 the corresponding module has this dimension by the [[Weyl character formula]], and for ''n'' large enough that the line bundle over ''G''/''B'' is [[very ample]], ''E''<sub>''n''ρ</sub> has the same dimension as in characteristic 0.\n*If ''q''=''p''<sup>''r''</sup>  for a positive integer ''r'', and ''n''=''q''&minus;1, then ''E''<sub>''n''ρ</sub> contains the [[Steinberg representation]] of ''G''('''F'''<sub>''q''</sub>) of dimension ''q''<sup>''N''</sup>. (Here '''F'''<sub>''q''</sub> ⊂ ''K'' is the finite field of order ''q''.) The Steinberg representation is an irreducible representation of ''G''('''F'''<sub>''q''</sub>) and therefore of ''G''(''K''), and for ''r'' large enough it has the same dimension as ''E''<sub>''n''ρ</sub>, so there are infinitely many values of ''n'' such that ''E''<sub>''n''ρ</sub> is irreducible.\n*If ''E''<sub>''n''ρ</sub> is irreducible it is isomorphic to its dual, so ''E''<sub>''n''ρ</sub>⊗''E''<sub>''n''ρ</sub> is isomorphic to End(''E''<sub>''n''ρ</sub>). Therefore, the ''T''-invariant subspace ''A''(''G'')<sup>λ</sup> of ''A''(''G'') is an increasing union of subrepresentations of the form End(''E'') for  representations ''E'' (of the form ''E''<sub>(''q''&minus;1)ρ</sub>)). However, for  representations of the form End(''E'')  an invariant polynomial that separates 0 and 1 is given by the determinant. This completes the sketch of the proof of Haboush's theorem.\n\n==References==\n*{{citation|mr=0444786\n|last=Demazure|first= Michel|authorlink=Michel Demazure\n|chapter=Démonstration de la conjecture de Mumford (d'après W. Haboush)|title= Séminaire Bourbaki (1974/1975: Exposés Nos. 453--470)|pages= 138–144|series= Lecture Notes in Math.|volume= 514|publisher= Springer|place= Berlin|year= 1976|doi=10.1007/BFb0080063|isbn=978-3-540-07686-5}} \n*{{citation|authorlink=William Haboush|first=W. J. |last=Haboush|title=Reductive groups are geometrically reductive|journal=Ann. of Math. |volume=102|year=1975|pages=67–83|doi=10.2307/1970974|issue=1|publisher=The Annals of Mathematics, Vol. 102, No. 1|jstor=1970974}}\n*Mumford, D.; Fogarty, J.; Kirwan, F. ''Geometric invariant theory''. Third edition. [[Ergebnisse der Mathematik und ihrer Grenzgebiete]] (2) (Results in Mathematics and Related Areas (2)), 34. Springer-Verlag, Berlin, 1994. xiv+292 pp. {{MathSciNet|id=1304906}} {{ISBN|3-540-56963-4}}\n*{{Citation | last1=Nagata | first1=Masayoshi | author1-link=Masayoshi Nagata | title=Invariants of a group in an affine ring | url=http://projecteuclid.org/euclid.kjm/1250524787 | year=1963  | journal=Journal of Mathematics of Kyoto University | issn=0023-608X | volume=3 | pages=369–377 | mr=0179268 | doi=10.1215/kjm/1250524787}}\n*{{cite journal | last1 = Nagata | first1 = M. | last2 = Miyata | first2 = T. | year = 1964 | title = Note on semi-reductive groups | url = | journal = J. Math. Kyoto Univ. | volume = 3 | issue = | pages = 379–382 | doi=10.1215/kjm/1250524788}}\n*{{springer|id=M/m065570|first=V.L. |last=Popov|authorlink=Vladimir L. Popov|title=Mumford hypothesis}}\n*{{cite journal | last1 = Seshadri | first1 = C.S. | year = 1977 | title = Geometric reductivity over arbitrary base | url = | journal = Adv. Math. | volume = 26 | issue = | pages = 225–274 | doi=10.1016/0001-8708(77)90041-x}}\n\n[[Category:Representation theory of algebraic groups]]\n[[Category:Invariant theory]]\n[[Category:Theorems in representation theory]]\n[[Category:Conjectures that have been proved]]"
    },
    {
      "title": "Harish-Chandra's regularity theorem",
      "url": "https://en.wikipedia.org/wiki/Harish-Chandra%27s_regularity_theorem",
      "text": "{{no footnotes|date=September 2014}}\nIn mathematics, '''Harish-Chandra's regularity theorem''', introduced by {{harvs|txt|last=Harish-Chandra|authorlink=Harish-Chandra|year=1963}}, states that every invariant eigendistribution on a [[semisimple Lie group]], and in particular every character of an [[irreducible unitary representation]] on a [[Hilbert space]], is given by a [[locally integrable function]]. {{harvs|txt|last=Harish-Chandra|year1=1978|year2=1999}} proved a similar theorem for semisimple ''p''-adic groups.\n\n{{harvs|txt|last=Harish-Chandra|year1=1955|year2=1956}} had previously shown that any invariant eigendistribution is analytic on the regular elements of the group, by showing that on these elements it is a solution of an elliptic [[differential equation]]. The problem is that it may have singularities on the singular elements of the group; the regularity theorem implies that these singularities are not too severe.\n\n==Statement==\nA distribution on a group ''G'' or its Lie algebra is called '''invariant''' if it is invariant under conjugation by ''G''.\n\nA distribution on a group ''G'' or its Lie algebra is called an '''eigendistribution''' if it is an eigenvector of the center of the universal enveloping algebra of ''G'' (identified with the left and right invariant differential operators of ''G''.\n\nHarish-Chandra's regularity theorem states that any invariant eigendistribution on a semisimple group or Lie algebra is a locally integrable function. \nThe condition that it is an eigendistribution can be relaxed slightly to the condition that its image under the center of the universal enveloping algebra is finite-dimensional. The regularity theorem also implies that on each Cartan subalgebra the distribution can be written as a finite sum of exponentials divided by a function Δ that closely resembles the denominator of the [[Weyl character formula]].\n\n==Proof==\nHarish-Chandra's original proof of the regularity theorem is given in a sequence of five papers {{harvs|last=Harish-Chandra|year1=1964a|year2=1964b|year3=1964c|year4=1965a|year5=1965b}}.\n{{harvtxt|Atiyah|1988}} gave an exposition of the proof of  Harish-Chandra's regularity theorem for the case of SL<sub>2</sub>('''R'''), and sketched its generalization to higher rank groups.\n\nMost proofs can be broken up into several steps as follows.\n\n*Step 1. If Θ is an invariant eigendistribution then it is analytic on the regular elements of ''G''. This follows from [[elliptic regularity]], by showing that the center of the universal enveloping algebra has an element that is \"elliptic transverse to an orbit of G\" for any regular orbit.\n*Step 2.  If Θ is an invariant eigendistribution then its restriction to the regular elements of ''G'' is locally integrable on ''G''. (This makes sense as the non-regular elements of ''G'' have measure zero.) This follows by showing that ΔΘ on each Cartan subalgebra is a finite sum of exponentials, where Δ is essentially the denominator of the Weyl denominator formula, with 1/Δ locally integrable.\n*Step 3. By steps 1 and 2, the invariant eigendistribution Θ is a sum ''S''+''F'' where ''F'' is a locally integrable function and ''S'' has support on the singular elements of ''G''. The problem is to show that ''S'' vanishes. This is done by stratifying the set of  singular elements of ''G'' as a union of locally closed submanifolds of ''G'' and using induction on the codimension of the strata. While it is possible for an eigenfunction of a differential equation to be of the form ''S''+''F'' with ''F'' locally integrable and ''S'' having singular support on a submanifold, this is only possible if the differential operator satisfies some restrictive conditions. One can then check that the Casimir operator of ''G'' does not satisfy these conditions on the strata of the singular set, which forces ''S'' to vanish.\n\n==References==\n*{{Citation | last1=Atiyah | first1=Michael | author1-link=Michael Atiyah | title=Collected works. Vol. 4 | publisher=The Clarendon Press Oxford University Press | series=Oxford Science Publications | isbn=978-0-19-853278-1 | mr=951895  | year=1988|chapter=Characters of semi-simple Lie groups|pages=491–557}}\n*{{Citation | last1=Harish-Chandra | title=On the characters of a semisimple Lie group | doi=10.1090/S0002-9904-1955-09935-X  | mr=0071715  | year=1955 | journal=[[Bulletin of the American Mathematical Society]] | issn=0002-9904 | volume=61 | issue=5 | pages=389–396}}\n*{{Citation | last1=Harish-Chandra | title=The characters of semisimple Lie groups | jstor=1992907 | mr=0080875  | year=1956 | journal=[[Transactions of the American Mathematical Society]] | issn=0002-9947 | volume=83 | pages=98–163 | doi=10.2307/1992907}}\n*{{Citation | last1=Harish-Chandra | title=Invariant eigendistributions on semisimple Lie groups | doi=10.1090/S0002-9904-1963-10889-7  | mr=0145006  | year=1963 | journal=[[Bulletin of the American Mathematical Society]] | issn=0002-9904 | volume=69 | pages=117–123}}\n*{{Citation | last1=Harish-Chandra | title=Invariant distributions on Lie algebras | jstor=2373165 | mr=0161940  | year=1964a | journal=[[American Journal of Mathematics]] | issn=0002-9327 | volume=86 | pages=271–309 | doi=10.2307/2373165}}\n*{{Citation | last1=Harish-Chandra | title=Invariant differential operators and distributions on a semisimple Lie algebra | jstor=2373023 | mr=0180628  | year=1964b | journal=[[American Journal of Mathematics]] | issn=0002-9327 | volume=86 | pages=534–564 | doi=10.2307/2373023}}\n*{{Citation | last1=Harish-Chandra | title=Some results on an invariant integral on a semisimple Lie algebra | jstor=1970664 | mr=0180629  | year=1964c | journal=[[Annals of Mathematics]] |series=Second Series | issn=0003-486X | volume=80 | pages=551–593 | doi=10.2307/1970664}}\n*{{Citation | last1=Harish-Chandra | title=Invariant eigendistributions on a semisimple Lie algebra | url=http://www.numdam.org/item?id=PMIHES_1965__27__5_0 | mr=0180630  | year=1965a | journal=[[Publications Mathématiques de l'IHÉS]] | issn=1618-1913 | issue=27 | pages=5–54}}\n*{{Citation | last1=Harish-Chandra | title=Invariant eigendistributions on a semisimple Lie group | jstor=1994080 | mr=0180631  | year=1965b | journal=[[Transactions of the American Mathematical Society]] | issn=0002-9947 | volume=119 | pages=457–508 | doi=10.2307/1994080}}\n*{{Citation | last1=Harish-Chandra | editor1-last=Rossmann | editor1-first=Wulf | title=Lie theories and their applications (proceedings of the 1977 annual seminar of the Canadian mathematical congress, Queen's University in Kingston, Ontario, 1977) | publisher=Queen's Univ. | location=Kingston, Ont. | series=Queen's Papers in Pure Appl. Math. | mr=0579175 |id= Reprinted in volume 4 of his collected works. | year=1978 | volume=48 | chapter=Admissible invariant distributions on reductive p-adic groups | pages=281–347}}\n*{{Citation | last1=Harish-Chandra | editor1-last=DeBacker | editor1-first=Stephen | editor2-last=Sally | editor2-first=Paul J. Jr. | title=Admissible invariant distributions on reductive p-adic groups | url=https://books.google.com/books?id=QhweZD8wGHEC | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=University Lecture Series | isbn=978-0-8218-2025-4 | mr=1702257  | year=1999 | volume=16}}\n\n[[Category:Theorems in representation theory]]"
    },
    {
      "title": "Lafforgue's theorem",
      "url": "https://en.wikipedia.org/wiki/Lafforgue%27s_theorem",
      "text": "In [[mathematics]], '''Lafforgue's theorem''', due to [[Laurent Lafforgue]], completes the [[Langlands program]] for [[general linear group]]s over [[algebraic function field]]s, by giving a correspondence between [[automorphic form]]s on these groups and representations of [[Galois group]]s. \n\nThe Langlands conjectures were introduced by {{harvs|txt|last=Langlands|year1=1967|year2=1970}} and  describe a correspondence between representations of the [[Weil group]] of an [[algebraic function field]] and representations of [[algebraic group]]s over the function field, generalizing [[class field theory]] of function fields from abelian Galois groups to non-abelian Galois groups.\n\n==Langlands conjectures for GL<sub>1</sub>==\nThe  Langlands conjectures for GL<sub>1</sub>(''K'') follow from (and are essentially equivalent to)   [[class field theory]]. More precisely the [[Artin map]] gives a map from the idele class group  to the abelianization of the Weil group.\n\n==Automorphic representations of GL<sub>''n''</sub>(''F'')==\nThe representations of GL<sub>''n''</sub>(''F'') appearing in the Langlands correspondence are automorphic representations.\n==Lafforgue's theorem for GL<sub>''n''</sub>(''F'')==\nHere ''F'' is a global field of some positive characteristic ''p'', and ℓ is some prime not equal to ''p''.\n\nLafforgue's theorem states that there is a bijection σ between:\n*Equivalence classes of cuspidal representations π of GL<sub>''n''</sub>(''F''), and\n*Equivalence classes of irreducible ℓ-adic representations σ(π) of dimension ''n'' of the absolute Galois group of ''F''\nthat preserves the ''L''-function at every place of ''F''.\n\nThe proof of Lafforgue's theorem involves constructing a representation  σ(π) of the absolute Galois group for each cuspidal representation π. The idea of doing this is to look in the [[ℓ-adic cohomology]] of the moduli stack of [[shtuka]]s of rank ''n'' that have compatible [[level structure (algebraic geometry)|level ''N'' structures]] for all ''N''. The cohomology contains subquotients of the form \n:π⊗σ(π)⊗σ(π)<sup>&or;</sup>\nwhich can be used to construct σ(π) from π. A major problem is that the moduli stack is not of finite type, which means that there are formidable technical difficulties in studying its cohomology.\n\n==Applications==\nLafforgue's theorem implies the [[Ramanujan–Petersson conjecture]] that if an automorphic form for GL<sub>''n''</sub>(''F'') has central character of finite order, then the corresponding Hecke eigenvalues at every unramified place have absolute value 1.\n\nLafforgue's theorem implies the conjecture of  {{harvtxt|Deligne|1980|loc=1.2.10}} that an irreducible finite-dimensional ''l''-adic representation of the absolute Galois group with determinant character of finite order is pure of weight 0.\n\n==See also==\n*[[Local Langlands conjectures]]\n\n==References==\n{{no footnotes|date=March 2016}}\n*{{Citation | last1=Borel | first1=Armand | author1-link=Armand Borel | editor1-last=Borel | editor1-first=Armand | editor1-link=Armand Borel | editor2-last=Casselman | editor2-first=W. | title=Automorphic forms, representations and L-functions (Proc. Sympos. Pure Math., Oregon State Univ., Corvallis, Ore., 1977), Part 2 | url=http://www.ams.org/publications/online-books/pspum332-index | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Proc. Sympos. Pure Math. | isbn=978-0-8218-1437-6  | mr=546608 | year=1979 | volume= XXXIII | chapter=Automorphic L-functions | pages=27–61}}\n*{{Citation | last1=Deligne | first1=Pierre | author1-link=Pierre Deligne | title=La conjecture de Weil. II | url=http://www.numdam.org/item?id=PMIHES_1980__52__137_0 | mr=601520 | year=1980 | journal=[[Publications Mathématiques de l'IHÉS]] | issn=1618-1913 | issue=52 | pages=137–252}}\n*{{Citation | last1=Gelfand | first1=I. M. | last2=Graev | first2=M. I. | last3=Pyatetskii-Shapiro | first3=I. I. | title=Representation theory and automorphic functions | origyear=1966 | url=https://books.google.com/books?id=_p5qAAAAMAAJ | publisher=W. B. Saunders Co. | location=Philadelphia, Pa. | series=Generalized functions | isbn=978-0-12-279506-0  | mr=0220673 | year=1969 | volume=6}}\n*{{Citation | last1=Lafforgue | first1=Laurent | title=Chtoucas de Drinfeld et applications | language=fr | trans-title=Drinfelʹd shtukas and applications | url=http://www.math.uni-bielefeld.de/documenta/xvol-icm/07/Lafforgue.MAN.html | mr=1648105 | year=1998 | journal=Documenta Mathematica | issn=1431-0635 | volume=II | pages=563–570}}\n* Lafforgue, Laurent (2002), [https://arxiv.org/abs/math.NT/0212399 \"Chtoucas de Drinfeld, formule des traces d'Arthur-Selberg et correspondance de Langlands.\"] (Drinfeld shtukas, Arthur-Selberg trace formula and Langlands correspondence)  Proceedings of the International Congress of Mathematicians, Vol. I (Beijing, 2002), 383–400, Higher Ed. Press, Beijing, 2002.\n*{{Citation | author2-link=Robert Langlands | last1=Jacquet | first1=H. | last2=Langlands | first2=Robert P. | title=Automorphic forms on  GL(2) | url=http://www.sunsite.ubc.ca/DigitalMathArchive/Langlands/JL.html#book | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Lecture Notes in Mathematics | doi=10.1007/BFb0058988 | mr=0401654 | year=1970 | volume=114}}\n*{{citation|last=Langlands|first=Robert|title=Letter to Prof. Weil|year=1967|url=http://publications.ias.edu/rpl/section/21}}\n*{{Citation | last1=Langlands | first1=R. P. | title=Lectures in modern analysis and applications, III | url=http://publications.ias.edu/rpl/section/21 | publisher=[[Springer-Verlag]] | location=Berlin, New York | series= Lecture Notes in Math | isbn=978-3-540-05284-5 | doi=10.1007/BFb0079065 | mr=0302614 | year=1970 | volume=170 | chapter=Problems in the theory of automorphic forms | pages=18–61}}\n*Gérard Laumon (2002), [https://arxiv.org/abs/math.NT/0212417 \"The work of Laurent Lafforgue\"], Proceedings of the ICM, Beijing 2002, vol. 1, 91–97,\n*G. Laumon (2000), [https://arxiv.org/abs/math.AG/0003131 \"La correspondance de Langlands sur les corps de fonctions (d'après Laurent Lafforgue)\"] (The Langlands correspondence over function fields (according to Laurent Lafforgue)), Séminaire Bourbaki, 52e année, 1999–2000, no. 873.\n\n==External links==\n*[http://www.ihes.fr/~lafforgue/publications.html Lafforgue's publications]\n*[http://publications.ias.edu/rpl/ The work of Robert Langlands ]\n*{{citation|last=Rapoport|title=The work of Laurent Lafforgue|url=http://www.math.uni-bonn.de/ag/alggeom/preprints/lafforgue.pdf}}\n\n[[Category:Theorems in algebraic number theory]]\n[[Category:Representation theory of Lie groups]]\n[[Category:Automorphic forms]]\n[[Category:Conjectures]]\n[[Category:Class field theory]]\n[[Category:Langlands program]]\n[[Category:Theorems in representation theory]]"
    },
    {
      "title": "Lie–Kolchin theorem",
      "url": "https://en.wikipedia.org/wiki/Lie%E2%80%93Kolchin_theorem",
      "text": "In [[mathematics]], the '''Lie–Kolchin theorem''' is a theorem in the [[representation theory]] of [[linear algebraic group]]s; '''Lie's theorem''' is the analog for [[linear Lie algebra]]s.\n\nIt states that if ''G'' is a [[connected space|connected]] and [[solvable group|solvable]] [[linear algebraic group]] defined over an [[algebraically closed]] [[field (mathematics)|field]] and\n\n:<math>\\rho\\colon G \\to GL(V)</math>\n\na [[group representation|representation]] on a nonzero finite-dimensional [[vector space]] ''V'', then there is a one-dimensional linear subspace ''L'' of ''V'' such that\n\n: <math>\\rho(G)(L) = L.</math>\n\nThat is, ρ(''G'') has an invariant line ''L'', on which ''G'' therefore acts through a one-dimensional representation. This is equivalent to the statement that ''V'' contains a nonzero vector ''v'' that is a common (simultaneous) eigenvector for all <math> \\rho(g), \\,\\, g \\in G </math>.\n\nIt follows directly that every [[irreducible representation|irreducible]] finite-dimensional representation of a connected and solvable linear algebraic group ''G'' has dimension one. In fact, this is another way to state the Lie–Kolchin theorem.\n\nLie's theorem states that any nonzero representation of a solvable Lie algebra on a finite dimensional vector space over an algebraically closed field of characteristic 0 has a one-dimensional invariant subspace.\n\nThe result  for Lie algebras was proved by {{harvs|txt|authorlink=Sophus Lie|first=Sophus |last=Lie|year=1876}} and for algebraic groups was proved by {{harvs|txt|authorlink=Ellis Kolchin|first=Ellis|last= Kolchin|year=1948|loc=p.19}}.\n\nThe [[Borel fixed point theorem]] generalizes the Lie–Kolchin theorem.\n\n== Triangularization ==\nSometimes the theorem is also referred to as the ''Lie–Kolchin triangularization theorem'' because by induction it implies that with respect to a suitable basis of ''V'' the image <math>\\rho(G)</math> has a ''triangular shape''; in other words, the image group <math>\\rho(G)</math> is conjugate in GL(''n'',''K'') (where ''n'' = dim ''V'') to a subgroup of the group T of [[upper triangular]] matrices, the standard [[Borel subgroup]] of GL(''n'',''K''): the image is [[simultaneously triangularizable]].\n\nThe theorem applies in particular to a [[Borel subgroup]] of a [[semisimple algebraic group|semisimple]] [[linear algebraic group]] ''G''.\n\n==Lie's theorem==\n\nLie's theorem states that if ''V'' is a finite dimensional vector space over an algebraically closed field of characteristic 0, then for any solvable Lie algebra of endomorphisms of ''V'' there is a vector that is an eigenvector for every element of the Lie algebra.\n\nApplying this result repeatedly shows that there is a basis for ''V'' such that all elements of the Lie algebra are represented by upper triangular matrices. \nThis is a generalization of the result of Frobenius that [[commuting matrices]] are simultaneously upper triangularizable, as commuting matrices form an [[abelian Lie algebra]], which is a fortiori solvable.\n\nA consequence of Lie's theorem  is that any finite dimensional solvable Lie algebra over a field of characteristic 0 has a nilpotent derived algebra.\n\n== Counter-examples ==\n\nIf the field ''K'' is not algebraically closed, the theorem can fail. The standard [[unit circle]], viewed as the set of [[complex number]]s <math> \\{ x+iy \\in \\mathbb{C} \\mid x^2+y^2=1 \\} </math> of absolute value one is a one-dimensional commutative (and therefore solvable) [[linear algebraic group]] over the real numbers which has a two-dimensional representation into the [[special orthogonal group]] SO(2) without an invariant (real) line.  Here the image <math> \\rho(z)</math>  of <math> z=x+iy </math> is the [[orthogonal matrix]]\n\n: <math> \\begin{pmatrix} x & y \\\\ -y & x \\end{pmatrix}.</math>\n\nFor algebraically closed fields of characteristic ''p''>0 Lie's theorem holds provided the dimension of the representation is less than ''p'', but can fail for representations of dimension ''p''. An example is given by the 3-dimensional nilpotent Lie algebra spanned by 1, ''x'', and ''d''/''dx'' acting on the ''p''-dimensional vector space ''k''[''x'']/(''x''<sup>''p''</sup>), which has no eigenvectors. Taking the semidirect product of this 3-dimensional Lie algebra by the ''p''-dimensional representation (considered as an abelian Lie algebra) gives a solvable Lie algebra whose derived algebra is not nilpotent.\n\n==References==\n\n*{{eom|first=V.V.|last= Gorbatsevich|id=l/l058710}}\n*{{Citation | last1=Kolchin | first1=E. R. | title=Algebraic matric groups and the Picard-Vessiot theory of homogeneous linear ordinary differential equations | jstor=1969111 | mr=0024884 | zbl=0037.18701 | year=1948 | journal=[[Annals of Mathematics]] |series=Second Series | issn=0003-486X | volume=49 | pages=1–42 | doi=10.2307/1969111}}\n*{{Citation | last1=Lie | first1=Sophus | author1-link=Sophus Lie | title=Theorie der Transformationsgruppen. Abhandlung II | url=https://archive.org/details/archivformathem02sarsgoog | year=1876 | journal=Archiv for Mathematik og Naturvidenskab | volume=1 | pages=152–193}}\n*[[William C. Waterhouse]], ''Introduction to Affine Group Schemes'', Graduate Texts in Mathematics vol. 66, Springer Verlag New York, 1979 (chapter 10, in particular section 10.2).\n\n{{DEFAULTSORT:Lie-Kolchin theorem}}\n[[Category:Lie algebras]]\n[[Category:Representation theory of algebraic groups]]\n[[Category:Theorems in representation theory]]"
    },
    {
      "title": "Maschke's theorem",
      "url": "https://en.wikipedia.org/wiki/Maschke%27s_theorem",
      "text": "In mathematics, '''Maschke's theorem''',<ref>{{cite journal |last=Maschke |first=Heinrich |date=1898-07-22 |title=Ueber den arithmetischen Charakter der Coefficienten der Substitutionen endlicher linearer Substitutionsgruppen |language=German |trans-title=On the arithmetical character of the coefficients of the substitutions of finite linear substitution groups |journal=<abbr Title=\"Mathematische Annalen\">Math. Ann.</Abbr> |volume=50 |issue=4 |pages=492–498 |url=http://resolver.sub.uni-goettingen.de/purl?GDZPPN002256975 |jfm=29.0114.03 | mr = 1511011 |doi=10.1007/BF01444297 }}</ref><ref>{{cite journal |last=Maschke |first=Heinrich |date=1899-07-27 |title=Beweis des Satzes, dass diejenigen endlichen linearen Substitutionsgruppen, in welchen einige durchgehends verschwindende Coefficienten auftreten, intransitiv sind |language=German |trans-title=Proof of the theorem that those finite linear substitution groups, in which some everywhere vanishing coefficients appear, are intransitive |journal=<abbr Title=\"Mathematische Annalen\">Math. Ann.</Abbr> |volume=52 |issue=2–3 |pages=363–368 |url=http://resolver.sub.uni-goettingen.de/purl?GDZPPN002257599 |jfm=30.0131.01 | mr = 1511061 |doi=10.1007/BF01476165 }}</ref> named after [[Heinrich Maschke]],<ref>{{MacTutor|id=Maschke|title=Heinrich Maschke}}</ref> is a theorem in [[group representation]] theory that concerns the decomposition of representations of a [[finite group]] into [[irreducible representation|irreducible]] pieces. Maschke's theorem allow one to make general conclusions about representations of a finite group ''G'' without actually computing them. It reduces the task of classifying all representations to a more manageable task of classifying [[irreducible representations]], since when the theorem applies, any representation is a direct sum of irreducible pieces (constituents). Moreover, it follows from the [[Jordan–Hölder theorem]] that, while the decomposition into a direct sum of irreducible subrepresentations may not be unique, the irreducible pieces have well-defined [[Multiplicity (mathematics)|multiplicities]]. In particular, a representation of a finite group over a field of characteristic zero is determined up to isomorphism by its [[character theory|character]].\n\n== Formulations ==\n\nMaschke's theorem addresses the question: when is a general (finite-dimensional) representation built from irreducible [[subrepresentation]]s using the [[direct sum of representations|direct sum]] operation? This question (and its answer) are formulated differently for different perspectives on group representation theory.\n\n=== Group-theoretic ===\n\nMaschke's theorem is commonly formulated as a [[corollary]] to the following result:\n<blockquote>\n'''Theorem.''' If {{var|V}} is a complex representation of a finite group {{var|G}} with a subrepresentation {{var|W}}, then there is another subrepresentation {{var|U}} of {{var|V}} such that {{var|V}}={{var|W}}⊕{{var|U}}.{{sfn|Fulton|Harris|loc=Proposition 1.5}}{{sfn|Serre|loc=Theorem 1}}\n</blockquote>\nThen the corollary is\n<blockquote>\n'''Corollary (Maschke's theorem).''' Every representation of a finite group {{var|G}} over a field {{var|F}} with [[Characteristic (algebra)|characteristic]] not dividing the order of {{var|G}} is a direct sum of irreducible representations.{{sfn|Fulton|Harris|loc=Corollary 1.6}}{{sfn|Serre|loc=Theorem 2}}\n</blockquote>\n\nThe [[vector space]] of [[Complex number|complex-valued]] [[Class function (algebra)|class functions]] of a group {{var|G}} has a natural {{var|G}}-invariant inner product structure, described in the article [[Schur orthogonality relations]]. Maschke's theorem was originally proved for the case of representations over <math>\\mathbb{C}</math> by constructing {{var|U}} as the [[orthogonal complement]] of {{var|W}} under this inner product.\n\n=== Module-theoretic ===\nOne of the approaches to representations of finite groups is through [[module theory]]. ''Representations'' of a group ''G'' are replaced by ''modules'' over its [[group algebra]]&nbsp;''K''[''G''] (to be precise, there is an [[isomorphism of categories]] between '''''K''[''G'']-Mod''' and '''Rep<sub>''G''</sub>''', the [[category of representations]] of ''G'').  Irreducible representations correspond to [[simple module]]s. In the module-theoretic language, Maschke's theorem asks: is an arbitrary module [[semisimple module|semisimple]]? In this context, the theorem can be reformulated as follows:\n\n<blockquote>'''Maschke's Theorem.''' Let ''G'' be a finite group and ''K'' a field whose characteristic does not divide the order of ''G''. Then ''K''[''G''], the group algebra of ''G'', is [[semisimple algebra|semisimple]].<ref>It follows that every module over ''K''[''G''] is a semisimple module.</ref><ref>The converse statement also holds: if the characteristic of the field divides the order of the group (the ''modular case''), then the group algebra is not semisimple.</ref>\n</blockquote>\nThe importance of this result stems from the well developed theory of semisimple rings, in particular, the [[Artin–Wedderburn theorem]] (sometimes referred to as Wedderburn's Structure Theorem). When ''K'' is the field of complex numbers, this shows that the algebra ''K''[''G''] is a product of several copies of complex [[matrix (mathematics)|matrix algebras]], one for each irreducible representation.<ref>The number of the summands can be computed, and turns out to be equal to the number of the [[conjugacy class]]es of the group.</ref> If the field ''K'' has characteristic zero, but is not [[algebraically closed]], for example, ''K'' is a field of [[real number|real]] or [[rational number|rational]] numbers, then a somewhat more complicated statement holds: the group algebra ''K''[''G''] is a product of matrix algebras over [[division ring]]s over ''K''. The summands correspond to irreducible representations of ''G'' over ''K''.<ref>One must be careful, since a representation may decompose differently over different fields: a representation may be irreducible over the real numbers but not over the complex numbers.</ref>\n\n=== Category-theoretic ===\n\nReformulated in the language of [[Semi-simplicity|semi-simple categories]], Maschke's theorem states\n\n<blockquote>\n'''Maschke's theorem.''' If {{var|G}} is a group and {{var|F}} is a field with characteristic not dividing the order of {{var|G}}, then the [[category of representations]] of {{var|G}} over {{var|F}} is semi-simple.\n</blockquote>\n\n== Proofs ==\n\n=== Module-theoretic ===\nLet ''V'' be a ''K''[''G'']-submodule. We will prove that ''V'' is a direct summand. Let ''π'' be any ''K''-linear projection of ''K''[''G''] onto ''V''. Consider\nthe map <math>\\varphi:K[G]\\to V</math> given by <math>\\varphi(x)=\\frac{1}{\\#G}\\sum_{s \\in G} s\\cdot \\pi(s^{-1} \\cdot x).</math>\nThen ''φ'' is again a projection: it is clearly ''K''-linear, maps ''K''[''G''] onto ''V'', and induces the identity on ''V''. Moreover we have\n\n:<math>\\begin{align}\n\\varphi(t\\cdot x) &= \\frac{1}{\\#G}\\sum_{s \\in G} s\\cdot \\pi(s^{-1}\\cdot t\\cdot x)\\\\\n{} &= \\frac{1}{\\#G}\\sum_{u \\in G} t\\cdot u\\cdot \\pi(u^{-1}\\cdot x)\\\\\n{} &= t\\cdot\\varphi(x),\n\\end{align}</math>\n\nso ''φ'' is in fact ''K''[''G'']-linear. By the [[splitting lemma]], <math>K[G]=V \\oplus \\ker \\varphi</math>. This proves that every submodule is a direct summand, that is, ''K''[''G''] is semisimple.\n\n== Converse statement ==\nThe above proof depends on the fact that #''G'' is invertible in ''K''. This might lead one to ask if the converse of Maschke's theorem also holds: if the characteristic of ''K'' divides the order of ''G'', does it follow that ''K''[''G''] is not semisimple? The answer is ''yes''.{{sfn|Serre|loc=Exercise 6.1}}\n\n'''Proof.''' For <math>x=\\sum\\lambda_gg\\in K[G]</math> define <math>\\epsilon(x)=\\sum\\lambda_g</math>. Let <math>I=\\ker\\epsilon</math>. Then ''I'' is a ''K''[''G'']-submodule. We will prove that for every nontrivial submodule ''V'' of ''K''[''G''], <math>I\\cap V\\neq0</math>. Let ''V'' be given, and let <math>v=\\sum\\mu_gg</math> be any nonzero element of ''V''. If <math>\\epsilon(v)=0</math>, the claim is immediate. Otherwise, let <math>s=\\sum1g</math>. Then <math>\\epsilon(s)=\\#G\\cdot1=0</math> so <math>s\\in I</math> and <math>sv=\\left(\\sum1g\\right)\\left(\\sum\\mu_gg\\right)=\\sum\\epsilon(v)g=\\epsilon(v)s</math> so that <math>sv</math> is an element of both ''I'' and ''V''. This proves that ''V'' is not a direct complement of ''I'' for all ''V'', so ''K''[''G''] is not semisimple.\n\n== Notes ==\n<references/>\n\n==References==\n*{{cite book\n   | last = Lang\n   | first = Serge\n   | authorlink = Serge Lang\n   | title = Algebra\n   | edition = Revised 3rd\n   | series = [[Graduate Texts in Mathematics]], '''211'''\n   | publisher = [[Springer-Verlag]]\n   | location = New York\n   | date = 2002-01-08\n   | isbn = 978-0-387-95385-4\n   | mr = 1878556 | zbl = 0984.00001\n}}\n*{{cite book\n   | first = Jean-Pierre\n   | last = Serre\n   | authorlink = Jean-Pierre Serre\n   | title = Linear Representations of Finite Groups\n   | series = [[Graduate Texts in Mathematics]], '''42'''\n   | publisher = [[Springer-Verlag]]\n   | location = New York–Heidelberg\n   | date = 1977-09-01\n   | isbn = 978-0-387-90190-9\n   | mr = 0450380 | zbl = 0355.20006\n}}\n*{{Fulton-Harris}}\n\n{{DEFAULTSORT:Maschke's Theorem}}\n[[Category:Representation theory of finite groups]]\n[[Category:Theorems in group theory]]\n[[Category:Theorems in representation theory]]"
    },
    {
      "title": "Mautner's lemma",
      "url": "https://en.wikipedia.org/wiki/Mautner%27s_lemma",
      "text": "In [[mathematics]], '''Mautner's lemma''' in [[representation theory]] states that if ''G'' is a [[topological group]] and π a [[unitary representation]] of ''G'' on a [[Hilbert space]] ''H'', then for any ''x'' in ''G'', which has [[Glossary of Riemannian and metric geometry#C|conjugate]]s \n\n:''yxy''<sup>&minus;1</sup>\n\nconverging to the [[identity element]] ''e'', for a [[net (mathematics)|net]] of elements ''y'', then any vector ''v'' of ''H'' invariant under all the π(''y'') is also invariant under π(''x'').\n\n==References==\n*[[Friederich Ignaz Mautner|F. Mautner]], ''Geodesic flows on symmetric Riemannian spaces'' (1957),  Ann. Math. 65, 416-430\n\n[[Category:Unitary representation theory]]\n[[Category:Topological groups]]\n[[Category:Theorems in representation theory]]\n[[Category:Lemmas]]\n\n{{algebra-stub}}"
    },
    {
      "title": "Multiplicity-one theorem",
      "url": "https://en.wikipedia.org/wiki/Multiplicity-one_theorem",
      "text": "In the mathematical theory of [[automorphic representation]]s, a '''multiplicity-one theorem''' is a result about the [[representation theory]] of an [[adelic algebraic group|adelic]] [[reductive algebraic group]]. The multiplicity in question is the number of times a given abstract [[group representation]] is realised in a certain space, of [[square-integrable function]]s, given in a concrete way.\n\nA multiplicity one theorem may also refer to a result about the [[Restricted representation|restriction]] of a [[Group representation|representation]] of a [[Group (mathematics)|group]] ''G'' to a [[subgroup]]&nbsp;''H''. In that context, the pair (''G'',&nbsp;''H'') is called a strong [[Gelfand pair]]. \n\n==Definition==\nLet ''G'' be a reductive algebraic group over a [[number field]] ''K'' and let '''A''' denote the [[adele ring|adele]]s of ''K''. Let ''Z'' denote the [[center of a group|centre]] of ''G'' and let {{math|''ω''}} be a [[continuous (mathematics)|continuous]] [[character (mathematics)|unitary character]] from ''Z''(''K'')\\Z('''A''')<sup>&times;</sup> to '''C'''<sup>&times;</sup>. Let ''L''<sup>2</sup><sub>0</sub>(''G''(''K'')/''G''('''A'''),&nbsp;{{math|''ω''}}) denote the [[cuspidal representation|space of cusp forms with central character &omega;]] on ''G''('''A'''). This space decomposes into a [[direct sum of Hilbert spaces]]\n:<math>L^2_0(G(K)\\backslash G(\\mathbf{A}),\\omega)=\\widehat{\\bigoplus}_{(\\pi,V_\\pi)}m_\\pi V_\\pi</math>\nwhere the sum is over [[irreducible representation|irreducible]] [[subrepresentation]]s and ''m''<sub>{{pi}}</sub> are non-negative [[integer]]s.\n\nThe group of adelic points of ''G'', ''G''('''A'''), is said to satisfy the '''multiplicity-one property''' if any [[smooth representation|smooth]] irreducible [[admissible representation]] of ''G''('''A''') occurs with multiplicity at most one in the space of [[cusp form]]s of central character&nbsp;{{math|''ω''}}, i.e. ''m''<sub>{{pi}}</sub> is 0 or 1 for all such&nbsp;{{pi}}.\n\n==Results==\nThe fact that the [[general linear group]], ''GL''(''n''), has the multiplicity-one property was proved by {{harvtxt|Jacquet|Langlands|1970}} for ''n''&nbsp;=&nbsp;2 and independently by {{harvtxt|Piatetski-Shapiro|1979}} and {{harvs|txt|authorlink=Joseph Shalika|last=Shalika|year=1974}} for ''n''&nbsp;>&nbsp;2 using the uniqueness of the [[Whittaker model]]. Multiplicity-one also holds for [[Special linear group|''SL''(2)]], but not for ''SL''(''n'') for ''n''&nbsp;>&nbsp;2 {{harv|Blasius|1994}}.\n\n==Strong multiplicity one theorem==\n\nThe strong multiplicity one theorem of {{harvtxt|Piatetski-Shapiro|1979}} and {{harvtxt|Jacquet|Shalika|1981}} states that two cuspidal automorphic representations of the general linear group are isomorphic if their local components are isomorphic for all but a finite number of places.\n\n==References==\n\n*{{Citation | last1=Blasius | first1=Don | title=On multiplicities for SL(''n'') | doi=10.1007/BF02937513 | mr=1303497 | year=1994 | journal=Israel Journal of Mathematics | issn=0021-2172 | volume=88 | issue=1 | pages=237–251}}\n*{{Citation | last1=Cogdell | first1=James W. | editor1-last=Cogdell | editor1-first=James W. | editor2-last=Kim | editor2-first=Henry H. | editor3-last=Murty | editor3-first=Maruti Ram | title=Lectures on automorphic L-functions | url=https://books.google.com/books?id=jb3ZCp0-MQsC | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Fields Inst. Monogr. | isbn=978-0-8218-3516-6  | mr=2071506 | year=2004 | volume=20 | chapter=Lectures on L-functions, converse theorems, and functoriality for GL<sub>''n''</sub> | chapterurl=http://www.math.osu.edu/~cogdell/ | pages=1–96}}\n*{{Citation\n| last=Jacquet\n| first=Hervé\n| last2=Langlands\n| first2=Robert\n| title=Automorphic forms on GL(2)\n| series=Lecture Notes in Mathematics\n| publisher=Springer-Verlag\n| volume=114\n| year=1970\n}}\n*{{Citation | last1=Jacquet | first1=H. | last2=Shalika | first2=J. A. | title=On Euler products and the classification of automorphic representations. I | doi=10.2307/2374103 | mr=618323 | year=1981 | journal=[[American Journal of Mathematics]] | issn=0002-9327 | volume=103 | issue=3 | pages=499–558}} {{Citation | last1=Jacquet | first1=H. | last2=Shalika | first2=J. A. | title=On Euler products and the classification of automorphic representations. II | jstor=2374050 | mr=618323 | year=1981 | journal=[[American Journal of Mathematics]] | issn=0002-9327 | volume=103 | issue=4 | pages=777–815 | doi=10.2307/2374050}}\n*{{Citation | last1=Piatetski-Shapiro | first1=I. I. | editor1-last=Borel | editor1-first=Armand | editor1-link=Armand Borel | editor2-last=Casselman. | editor2-first=W. | title=Automorphic forms, representations and L-functions (Proc. Sympos. Pure Math., Oregon State Univ., Corvallis, Ore., 1977), Part 1 | url=http://www.ams.org/publications/online-books/pspum331-index | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Proc. Sympos. Pure Math., XXXIII | isbn=978-0-8218-1435-2  | mr=546599 | year=1979 | chapter=Multiplicity one theorems | pages=209–212}}\n*{{Citation | last1=Shalika | first1=J. A. | title=The multiplicity one theorem for GL<sub>''n''</sub> | jstor=1971071 | mr=0348047 | year=1974 | journal=[[Annals of Mathematics]] |series=Second Series | issn=0003-486X | volume=100 | pages=171–193 | doi=10.2307/1971071}}\n\n[[Category:Representation theory of groups]]\n[[Category:Automorphic forms]]\n[[Category:Theorems in number theory]]\n[[Category:Theorems in representation theory]]"
    },
    {
      "title": "Peter–Weyl theorem",
      "url": "https://en.wikipedia.org/wiki/Peter%E2%80%93Weyl_theorem",
      "text": "In [[mathematics]], the '''Peter–Weyl theorem''' is a basic result in the theory of [[harmonic analysis]], applying to [[topological group]]s that are [[Compact group|compact]], but are not necessarily [[Abelian group|abelian]]. It was initially proved by [[Hermann Weyl]], with his student [[Fritz Peter]], in the setting of a compact [[topological group]] ''G'' {{harv|Peter|Weyl|1927}}. The theorem is a collection of results generalizing the significant facts about the decomposition of the [[regular representation]] of any [[finite group]], as discovered by [[Ferdinand Georg Frobenius]] and [[Issai Schur]].\n\nThe theorem has three parts.  The first part states that the matrix coefficients of [[irreducible representation]]s of ''G'' are dense in the space ''C''(''G'') of continuous [[complex-valued function]]s on ''G'', and thus also in the space ''L''<sup>2</sup>(''G'') of [[square-integrable function]]s.  The second part asserts the complete reducibility of [[unitary representation]]s of ''G''.  The third part then asserts that the regular representation of ''G'' on ''L''<sup>2</sup>(''G'') decomposes as the direct sum of all irreducible unitary representations.  Moreover, the matrix coefficients of the irreducible unitary representations form an [[orthonormal basis]] of ''L''<sup>2</sup>(''G''). In the case that ''G'' is the group of unit complex numbers, this last result is simply a standard result from Fourier series.\n\n==Matrix coefficients==\nA '''[[matrix coefficient]]''' of the group ''G'' is a complex-valued function φ on ''G'' given as the composition\n\n:<math>\\varphi = L\\circ \\pi</math>\n\nwhere π&nbsp;:&nbsp;''G''&nbsp;→&nbsp;GL(''V'') is a finite-dimensional ([[continuous function|continuous]]) [[group representation]] of ''G'', and ''L'' is a [[linear functional]] on the vector space of [[endomorphism]]s of ''V'' (e.g. trace), which contains GL(''V'') as an open subset.  Matrix coefficients are continuous, since representations are by definition continuous, and linear functionals on finite-dimensional spaces are also continuous.\n\nThe first part of the Peter–Weyl theorem asserts ({{harvnb|Bump|2004|loc=§4.1}}; {{harvnb|Knapp|1986|loc=Theorem 1.12}}):\n\n<blockquote>'''Peter–Weyl Theorem (Part I).''' The set of matrix coefficients of ''G'' is [[dense set|dense]] in the space of [[continuous functions on a compact Hausdorff space|continuous complex functions]] C(''G'') on ''G'', equipped with the [[uniform norm]].</blockquote>\n\nThis first result resembles the [[Stone–Weierstrass theorem]] in that it indicates the density of a set of functions in the space of all continuous functions, subject only to an ''algebraic'' characterization.  In fact, the matrix coefficients of tensor product form a unital algebra invariant under complex conjugation because the product of two matrix coefficients is a matrix coefficient of the tensor product representation, and the complex conjugate is a matrix coefficient of the dual representation. Hence the theorem follows directly from the Stone–Weierstrass theorem if the matrix coefficients separate points, which is obvious if ''G'' is a [[matrix group]] {{harv|Knapp|1986|p=17}}. Conversely, it is a consequence of the theorem that any compact [[Lie group]] is isomorphic to a matrix group {{harv|Knapp|1986|loc=Theorem 1.15}}.\n\nA corollary of this result is that the matrix coefficients of ''G'' are dense in ''L''<sup>2</sup>(''G'').\n\n==Decomposition of a unitary representation==\nThe second part of the theorem gives the existence of a decomposition of a [[unitary representation]] of ''G'' into finite-dimensional representations. Now, intuitively groups were conceived as rotations on geometric objects, so it is only natural to study representations which essentially arise from continuous '''[[Group action (mathematics)|actions]]''' on Hilbert spaces. (For those who were first introduced to dual groups consisting of characters which are the continuous homomorphisms into the [[circle group]], this approach is similar except that the circle group is (ultimately) generalised to the group of unitary operators on a given Hilbert space.)\n\nLet ''G'' be a topological group and ''H'' a complex Hilbert space.\n\nA continuous action ∗ : ''G'' × ''H'' → ''H'', gives rise to a continuous map ρ<sub>∗</sub> : ''G'' → ''H''<sup>''H''</sup> (functions from ''H'' to ''H'' with the [[Strong operator topology|strong topology]]) defined by: ρ<sub>∗</sub>(''g'')(''v'') = ''∗(g,v)''. This map is clearly an homomorphism from ''G'' into GL(''H''), the homeomorphic automorphisms of ''H''. Conversely, given such a map, we can uniquely recover the action in the obvious way.\n\nThus we define the '''representations of ''G'' on a Hilbert space ''H''''' to be those [[group homomorphisms]], ρ, which arise from continuous actions of ''G'' on ''H''. We say that a representation ρ is '''unitary''' if ρ(''g'') is a [[unitary operator]] for all ''g''&nbsp;∈&nbsp;''G''; i.e., <math>\\langle \\rho(g)v,\\rho(g)w\\rangle = \\langle v,w\\rangle</math> for all ''v'', ''w''&nbsp;∈&nbsp;''H''. (I.e. it is unitary if ρ : ''G'' → U(''H''). Notice how this generalises the special case of the one-dimensional Hilbert space, where U('''C''') is just the circle group.)\n\nGiven these definitions, we can state the second part of the Peter–Weyl theorem {{harv|Knapp|1986|loc=Theorem 1.12}}:\n\n<blockquote>'''Peter–Weyl Theorem (Part II).''' Let ρ be a unitary representation of a compact group ''G'' on a complex Hilbert space ''H''.  Then ''H'' splits into an orthogonal [[direct sum of representations|direct sum]] of irreducible finite-dimensional unitary representations of ''G''.</blockquote>\n\n==Decomposition of square-integrable functions==\nTo state the third and final part of the theorem, there is a natural Hilbert space over ''G'' consisting of [[square-integrable function]]s, [[Lp space|<math>L^2(G)</math>]]; this makes sense because [[Haar measure]] exists on ''G''. The group ''G'' has a [[unitary representation]] ρ on <math>L^2(G)</math> given by [[Group action (mathematics)|acting]] on the left, via\n\n:<math>\\rho(h)f(g) = f(h^{-1}g).</math>\n\nThe final statement of the Peter–Weyl theorem {{harv|Knapp|1986|loc=Theorem 1.12}} gives an explicit [[orthonormal basis]] of <math>L^2(G)</math>.  Roughly it asserts that the matrix coefficients for ''G'', suitably renormalized, are an [[orthonormal basis]] of ''L''<sup>2</sup>(''G'').  In particular, <math>L^2(G)</math> decomposes into an orthogonal direct sum of all the irreducible unitary representations, in which the multiplicity of each irreducible representation is equal to its degree (that is, the dimension of the underlying space of the representation).  Thus,\n\n:<math>L^2(G) = \\underset{\\pi\\in\\Sigma}{\\widehat{\\bigoplus}} E_\\pi^{\\oplus\\dim E_\\pi}</math>\n\nwhere Σ denotes the set of (isomorphism classes of) irreducible unitary representations of ''G'', and the summation denotes the [[closure (topology)|closure]] of the direct sum of the total spaces ''E''<sub>π</sub> of the representations π.\n\nWe may also regard <math>L^2(G)</math> as a representation of the direct product group <math>G\\times G</math>, with the two factors acting by translation on the left and the right, respectively. Fix a representation <math>(\\pi,E_\\pi)</math> of <math>G</math>. The space of matrix coefficients for the representation may be identified with <math>\\operatorname{End}(E_\\pi)</math>, the space of linear maps of <math>E_\\pi</math> to itself. The natural left and right action of <math>G\\times G</math> on the matrix coefficients corresponds to the action on <math>\\operatorname{End}(E_\\pi)</math> given by\n:<math>(g,h)\\cdot A=\\pi(g)A\\pi(h)^{-1}.</math>\nThen we may decompose <math>L^2(G)</math> as unitary representation of <math>G\\times G</math> in the form\n\n:<math>L^2(G) = \\underset{\\pi\\in\\Sigma}{\\widehat{\\bigoplus}} E_\\pi\\otimes E_\\pi^*.</math>\n\nFinally, we may form an orthonormal basis for <math>L^2(G)</math> as follows. Suppose that a representative π is chosen for each isomorphism class of irreducible unitary representation, and denote the collection of all such π by Σ. Let <math>\\scriptstyle{u_{ij}^{(\\pi)}}</math> be the matrix coefficients of π in an orthonormal basis, in other words\n\n:<math>u^{(\\pi)}_{ij}(g) = \\langle \\pi(g)e_i, e_j\\rangle.</math>\n\nfor each ''g''&nbsp;∈&nbsp;''G''.  Finally, let ''d''<sup>(π)</sup> be the degree of the representation π. The theorem now asserts that the set of functions\n\n:<math>\\left\\{\\sqrt{d^{(\\pi)}}u^{(\\pi)}_{ij}\\mid\\, \\pi\\in\\Sigma,\\,\\, 1\\le i,j\\le d^{(\\pi)}\\right\\}</math>\n\nis an orthonormal basis of <math>L^2(G).</math>\n\n===Restriction to class functions===\nA function <math>f</math> on ''G'' is called a ''class function'' if <math>f(hgh^{-1})=f(g)</math> for all <math>g</math> and <math>h</math> in ''G''. The space of class functions forms a closed subspace of <math>L^2(G)</math>, and therefore a Hilbert space in its own right. Within the space of matrix coefficients for a fixed representation <math>\\pi</math> is the [[Character_(mathematics)|character]] <math>\\chi_\\pi</math> of <math>\\pi</math>, defined by\n:<math>\\chi_\\pi(g)=\\operatorname{trace}(\\pi(g)).</math>\nIn the notation above, the character is the sum of the diagonal matrix coefficients:\n:<math>\\chi_\\pi=\\sum_{i=1}^{d^{(\\pi)}}u_{ii}^{(\\pi)}.</math>\nAn important consequence of the preceding result is the following:\n:'''Theorem''': The characters of the irreducible representations of ''G'' form an orthonormal basis for the space of square-integrable class functions on ''G''.\nThis result plays an important part in Weyl's classification of the [[Compact_group#Representation_theory_of_a_connected_compact_Lie_group|representations of a connected compact Lie group]].<ref>{{harvnb|Hall|2015}} Chapter 12</ref>\n\n===An example: <math>S^1</math>===\nA simple but helpful example is the case of the group of complex numbers of magnitude 1, <math>G=S^1</math>. In this case, the irreducible representations are one-dimensional and given by\n:<math>\\pi_n(e^{i\\theta})=e^{in\\theta}.</math>\nThere is then a single matrix coefficient for each representation, the function\n:<math>u_n(e^{i\\theta})=e^{in\\theta}.</math>\nThe last part of the Peter–Weyl theorem then asserts in this case that these functions form an orthonormal basis for <math>L^2(S^1)</math>. In this case, the theorem is simply a standard result from the theory of Fourier series. \n\nFor any compact group ''G'', we can regard the decomposition of <math>L^2(G)</math> in terms of matrix coefficients as a generalization of the theory of Fourier series. Indeed, this decomposition is often referred to as a Fourier series.\n\n===An example: SU(2)===\nWe use the standard representation of the group [[Special_unitary_group#n_.3D_2|SU(2)]] as\n:<math> \\operatorname{SU}(2) = \\left \\{ \\begin{pmatrix} \\alpha&-\\overline{\\beta}\\\\ \\beta & \\overline{\\alpha} \\end{pmatrix}: \\ \\ \\alpha,\\beta\\in\\mathbb{C},\\, |\\alpha|^2 + |\\beta|^2 = 1\\right \\}  ~,</math>\nThus, SU(2) is represented as the 3-sphere <math>S^3</math> sitting inside <math>\\mathbb{C}^2=\\mathbb{R}^4</math>.\nThe irreducible representations of SU(2), meanwhile, are labeled by a non-negative integer <math>m</math> and can be realized as the natural action of SU(2) on the space of homogeneous polynomials of degree <math>m</math> in two complex variables.<ref>{{harvnb|Hall|2015}} Example 4.10</ref> The matrix coefficients of the <math>m</math>th representation are [[Spherical_harmonics#Higher_dimensions|hyperspherical harmonics]] of degree <math>m</math>, that is, the restrictions to <math>S^3</math> of homogeneous harmonic polynomials of degree <math>m</math> in <math>\\alpha</math> and <math>\\beta</math>. The key to verifying this claim is to compute that for any two complex numbers <math>z_1</math> and <math>z_2</math>, the function \n:<math>(\\alpha,\\beta)\\mapsto (z_1\\alpha+z_2\\beta)^m</math> \nis harmonic as a function of <math>(\\alpha,\\beta)\\in\\mathbb{C}^2=\\mathbb{R}^4</math>.\n\nIn this case, finding an orthonormal basis for <math>L^2(\\operatorname{SU}(2))=L^2(S^3)</math> consisting of matrix coefficients amounts to finding an orthonormal basis consisting of hyperspherical harmonics, which is a standard construction in analysis on spheres.\n\n==Consequences==\n===Representation theory of connected compact Lie groups===\nThe Peter–Weyl theorem—specifically the assertion that the characters form an orthonormal ''basis'' for the space of square-integrable class functions—plays a key role in the [[Compact_group#Representation_theory_of_a_connected_compact_Lie_group|classification]] of the irreducible representations of a connected compact Lie group.<ref>{{harvnb|Hall|2015}} Section 12.5</ref> The argument also depends on the [[Maximal_torus#Weyl_integral_formula|Weyl integral formula]] (for class functions) and the [[Weyl character formula]].\n\nAn outline of the argument may be found [[Compact_group#An_outline_of_the_proof|here]].\n\n===Linearity of compact Lie groups===\nOne important consequence of the Peter–Weyl theorem is the following:<ref>{{harvnb|Knapp|2002|loc=Corollary IV.4.22}}</ref>\n:'''Theorem''': Every compact Lie group has a faithful finite-dimensional representation and is therefore isomorphic to a closed subgroup of <math>\\operatorname{GL}(n;\\mathbb{C})</math> for some <math>n</math>.\n\n===Structure of compact topological groups===\nFrom the Peter–Weyl theorem, one can deduce a significant general structure theorem. Let ''G'' be a compact topological group, which we assume [[Hausdorff space|Hausdorff]]. For any finite-dimensional ''G''-invariant subspace ''V'' in ''L''<sup>2</sup>(''G''), where ''G'' [[Group action (mathematics)|acts]] on the left, we consider the image of ''G'' in GL(''V''). It is closed, since ''G'' is compact, and a subgroup of the [[Lie group]] GL(''V''). It follows by a [[Closed subgroup theorem|theorem]] of [[Élie Cartan]] that the image of ''G'' is a Lie group also.\n\nIf we now take the [[Limit (category theory)|limit]] (in the sense of [[category theory]]) over all such spaces ''V'', we get a result about ''G'': Because ''G'' acts faithfully on ''L''<sup>2</sup>(''G''), ''G'' is an ''inverse limit of Lie groups''. It may of course not itself be a Lie group: it may for example be a [[profinite group]].\n\n==See also==\n* [[Pontryagin duality]]\n\n==References==\n* {{citation|first1=F.|last1=Peter|first2=H.|last2=Weyl|title=Die Vollständigkeit der primitiven Darstellungen einer geschlossenen kontinuierlichen Gruppe|journal=Math. Ann.|volume=97|year=1927|pages=737–755|doi=10.1007/BF01447892}}.\n\n* {{citation|first=Daniel|last=Bump|title=Lie groups|publisher=Springer|year=2004|isbn=0-387-21154-3}}.\n* {{citation|first=Brian C.|last=Hall|title=Lie Groups, Lie Algebras, and Representations: An Elementary Introduction|edition= 2nd|series=Graduate Texts in Mathematics|volume=222 |publisher=Springer|year=2015|isbn=978-3319134666}}.\n* {{springer|title=Peter-Weyl theorem|id=p/p072440}}\n* {{citation|last=Knapp|first=Anthony|authorlink=Anthony Knapp|title=Representation theory of semisimple groups|publisher=Princeton University Press|year=1986|isbn=0-691-09089-0}}.\n* {{citation|last=Knapp|first=Anthony W.|authorlink=Anthony Knapp|title=Lie Groups Beyond an Introduction|edition= 2nd|series=Progress in Mathematics|volume=140|publisher=Birkhäuser|place= Boston|year= 2002|isbn=0-8176-4259-5}}.\n* {{citation|first=George D.|last=Mostow|authorlink=George Mostow|title=Cohomology of topological groups and solvmanifolds|journal=[[Annals of Mathematics]]| volume=73|issue=1|year=1961|pages=20–48|doi=10.2307/1970281|jstor=1970281|publisher=Princeton University Press}}\n* {{citation|first1=Richard S.|last1=Palais|authorlink1=Richard Palais| first2=T. E.|last2=Stewart|title=The cohomology of differentiable transformation groups|journal=[[American Journal of  Mathematics]]|volume=83|issue=4|year=1961|pages=623–644|doi=10.2307/2372901|jstor=2372901|publisher=The Johns Hopkins University Press}}.\n\n;Specific\n<references />\n\n{{DEFAULTSORT:Peter-Weyl theorem}}\n[[Category:Unitary representation theory]]\n[[Category:Topological groups]]\n[[Category:Theorems in harmonic analysis]]\n[[Category:Theorems in representation theory]]\n[[Category:Theorems in group theory]]"
    },
    {
      "title": "Wigner–Eckart theorem",
      "url": "https://en.wikipedia.org/wiki/Wigner%E2%80%93Eckart_theorem",
      "text": "The '''Wigner–Eckart theorem''' is a [[theorem]] of [[representation theory]] and [[quantum mechanics]]. It states that [[Matrix (mathematics)|matrix]] elements of [[spherical tensor operator|spherical tensor]] [[Operator (physics)|operator]]s in the basis of [[angular momentum]] [[eigenstate]]s can be expressed as the product of two factors, one of which is independent of angular momentum orientation, and the other a [[Clebsch–Gordan coefficient]]. The name derives from physicists [[Eugene Wigner]] and [[Carl Eckart]], who developed the formalism as a link between the symmetry transformation groups of space (applied to the Schrödinger equations) and the laws of conservation of energy, momentum, and angular momentum.<ref name=\"Eckart Biography\">[http://orsted.nap.edu/openbook.php?record_id=571&page=194 Eckart Biography] – The National Academies Press.</ref>\n\nMathematically, the Wigner–Eckart theorem is generally stated in the following way.  Given a tensor operator <math>T^{(k)}</math> and two states of angular momenta <math>j</math> and <math>j'</math>, there exists a constant <math>\\langle j \\| T^{(k)} \\| j' \\rangle</math> such that for all <math>m</math>, <math>m'</math>, and <math>q</math>, the following equation is satisfied:\n\n:<math>\n    \\langle j \\, m | T^{(k)}_q | j' \\, m'\\rangle\n  = \\langle j' \\, m' \\, k \\, q | j \\, m \\rangle \\langle j \\| T^{(k)} \\| j'\\rangle,\n</math>\n\nwhere\n*<math>T^{(k)}_q</math> is the {{math|''q''}}-th component of the spherical tensor operator <math>T^{(k)}</math> of rank {{math|''k''}},<ref>The parenthesized superscript {{math|(''k'')}} provides a reminder of its rank.  However, unlike {{math|''q''}}, it need not be an actual index.</ref> \n* <math>|j m\\rangle</math> denotes an eigenstate of total angular momentum {{math|''J''<sup>2</sup>}} and its ''z'' component {{math|''J''<sub>z</sub>}},\n* <math>\\langle j' m' k q | j m\\rangle</math> is the [[Clebsch–Gordan coefficient]] for coupling {{math|''j''′}} with {{math|''k''}} to get {{math|''j''}},\n* <math>\\langle j \\| T^{(k)} \\| j' \\rangle</math> denotes<ref>This is a special notation specific to the Wigner–Eckart theorem.</ref> some value that does not depend on {{math|''m''}}, {{math|''m''′}}, nor {{math|''q''}} and is referred to as the '''reduced matrix element'''.\n\nThe Wigner–Eckart theorem states indeed that operating with a spherical tensor operator of rank {{math|''k''}} on an angular momentum eigenstate is like adding a state with angular momentum ''k'' to the state. The matrix element one finds for the spherical tensor operator is proportional to a Clebsch–Gordan coefficient, which arises when considering adding two angular momenta. When stated another way, one can say that the Wigner–Eckart theorem is a theorem that tells how vector operators behave in a subspace. Within a given subspace, a component of a vector operator will behave in a way proportional to the same component of the angular momentum operator.  This definition is given in the book ''Quantum Mechanics'' by Cohen–Tannoudji, Diu and Laloe.\n\n==Background and overview==\n\n===Motivating example: position operator matrix elements for 4d → 2p transition===\n\nLet's say we want to calculate [[transition dipole moment]]s for an electron transition from a 4d to a 2p [[atomic orbital|orbital]] of a hydrogen atom, i.e. the matrix elements of the form <math>\\langle 2p,m_1 | r_i | 4d,m_2 \\rangle</math>, where ''r''<sub>''i''</sub> is either the ''x'', ''y'', or ''z'' component of the [[position operator]], and ''m''<sub>1</sub>, ''m''<sub>2</sub> are the [[magnetic quantum number]]s that distinguish different orbitals within the 2p or 4d [[Electron shell#Subshells|subshell]]. If we do this directly, it involves calculating 45 different integrals: there are 3 possibilities for ''m''<sub>1</sub> (−1, 0, 1), 5 possibilities for ''m''<sub>2</sub> (−2, −1, 0, 1, 2), and 3 possibilities for ''i'', so the total is 3 × 5 × 3 = 45.\n\nThe Wigner–Eckart theorem allows one to obtain the same information after evaluating just ''one'' of those 45 integrals (''any'' of them can be used, as long as it is nonzero). Then the other 44 integrals can be inferred from that first one—without the need to write down any wavefunctions or evaluate any integrals—with the help of [[Clebsch–Gordan coefficient]]s, which can be easily looked up in a table or computed by hand or computer.\n\n===Qualitative summary of proof===\n\nThe Wigner–Eckart theorem works because all 45 of these different calculations are related to each other by rotations. If an electron is in one of the 2p orbitals, rotating the system will generally move it into a ''different'' 2p orbital (usually it will wind up in a [[quantum superposition]] of all three basis states, ''m'' = +1, 0, −1). Similarly, if an electron is in one of the 4d orbitals, rotating the system will move it into a different 4d orbital. Finally, an analogous statement is true for the position operator: when the system is rotated, the three different components of the position operator are effectively interchanged or mixed.\n\nIf we start by knowing just one of the 45 values (say, we know that <math>\\langle 2p,m_1 | r_i | 4d,m_2 \\rangle = K</math>) and then we rotate the system, we can infer that ''K'' is also the matrix element between the rotated version of <math>\\langle 2p,m_1 |</math>, the rotated version of <math>r_i</math>, and the rotated version of <math>| 4d,m_2 \\rangle</math>. This gives an algebraic relation involving ''K'' and some or all of the 44 unknown matrix elements. Different rotations of the system lead to different algebraic relations, and it turns out that there is enough information to figure out all of the matrix elements in this way.\n\n(In practice, when working through this math, we usually apply [[angular momentum operator]]s to the states, rather than rotating the states. But this is fundamentally the same thing, because of the close mathematical [[Angular momentum operator#Angular momentum as the generator of rotations|relation between rotations and angular momentum operators]].)\n\n===In terms of representation theory===\n\nTo state these observations more precisely and to prove them, it helps to invoke the mathematics of [[representation theory]]. For example, the set of all possible 4d orbitals (i.e., the 5 states ''m'' = −2, −1, 0, 1, 2 and their [[quantum superposition]]s) form a 5-dimensional abstract [[vector space]]. Rotating the system transforms these states into each other, so this is an example of a \"group representation\", in this case, the 5-dimensional [[irreducible representation]] (\"irrep\") of the rotation group [[angular momentum operator#SU(2), SO(3), and 360° rotations|SU(2) or SO(3)]], also called the \"spin-2 representation\". Similarly, the 2p quantum states form a 3-dimensional irrep (called \"spin-1\"), and the components of the position operator also form the 3-dimensional \"spin-1\" irrep.\n\nNow consider the matrix elements <math>\\langle 2p,m_1 | r_i | 4d,m_2 \\rangle</math>. It turns out that these are transformed by rotations according to the [[direct product]] of those three representations, i.e. the spin-1 representation of the 2p orbitals, the spin-1 representation of the components of '''r''', and the spin-2 representation of the 4d orbitals. This direct product, a 45-dimensional representation of SU(2), is ''not'' an [[irreducible representation]], instead it is the [[direct sum]] of a spin-4 representation, two spin-3 representations, three spin-2 representations, two spin-1 representations, and a spin-0 (i.e. trivial) representation. The nonzero matrix elements can only come from the spin-0 subspace. The Wigner–Eckart theorem works because the direct product decomposition contains one and only one spin-0 subspace, which implies that all the matrix elements are determined by a single scale factor.\n\nApart from the overall scale factor, calculating the matrix element <math>\\langle 2p,m_1 | r_i | 4d,m_2 \\rangle</math> is equivalent to calculating the [[projection (linear algebra)|projection]] of the corresponding abstract vector (in 45-dimensional space) onto the spin-0 subspace. The results of this calculation are the [[Clebsch–Gordan coefficient]]s. The key qualitative aspect of the Clebsch–Gordan decomposition that makes the argument work is that in the decomposition of the tensor product of two irreducible representations, each irreducible representation occurs only once. This allows [[Schur's lemma]] to be used.<ref>{{harvnb|Hall|2015}} Appendix C.</ref>\n\n==Proof==\n\nStarting with the definition of a [[spherical tensor operator]], we have\n\n:<math>[J_\\pm, T^{(k)}_q] = \\hbar \\sqrt{(k \\mp q)(k \\pm q + 1)}T_{q\\pm 1}^{(k)},</math>\n\nwhich we use to then calculate\n\n:<math>\n\\begin{align}\n    &\\langle j \\, m | [J_\\pm, T^{(k)}_q] | j' \\, m' \n\\rangle = \\hbar \\sqrt{(k \\mp q) (k \\pm q + 1)} \\, \n    \\langle j \\, m | T^{(k)}_{q \\pm 1} | j' \\, m' \\rangle.\n\\end{align}\n</math>\n\nIf we expand the commutator on the LHS by calculating the action of the {{math|''J''<sub>±</sub>}} on the bra and ket, then we get\n\n:<math>\n\\begin{align} \n    \\langle j \\, m | [J_\\pm, T^{(k)}_q] | j' \\, m' \n\\rangle = &\\hslash\\sqrt{(j \\pm m) (j \\mp m + 1)} \\, \\langle j \\, (m \\mp 1) | T^{(k)}_q | j' \\, m' \\rangle \\\\\n   &-\\hslash\\sqrt{(j' \\mp m')(j' \\pm m' + 1)} \\, \\langle j \\, m | T^{(k)}_q | j' \\, (m' \\pm 1) \\rangle.\n\\end{align}\n</math>\n\nWe may combine these two results to get\n\n:<math>\n\\begin{align} \n    \\sqrt{(j \\pm m) (j \\mp m + 1)} \\langle j \\, (m \\mp 1) | T^{(k)}_q | j' \\, m' \n\\rangle = &\\sqrt{(j' \\mp m') (j' \\pm m' + 1)} \\, \\langle j \\, m | T^{(k)}_q | j' \\, (m' \\pm 1) \\rangle \\\\\n   &+\\sqrt{(k \\mp q) (k \\pm q + 1)} \\, \\langle j \\, m | T^{(k)}_{q \\pm 1} | j' \\, m' \\rangle.\n\\end{align}\n</math>\n\nThis recursion relation for the matrix elements closely resembles that of the [[Clebsch–Gordan coefficient]].  In fact, both are of the form {{math|∑<sub>''c''</sub> ''a''<sub>''b'', ''c''</sub> ''x''<sub>''c''</sub> {{=}} 0}}. We therefore have two sets of linear homogeneous equations:\n\n:<math>\n\\begin{align}\n  \\sum_c a_{b, c} x_c &= 0, &\n  \\sum_c a_{b, c} y_c &= 0.\n\\end{align}\n</math>\n\none for the Clebsch–Gordan coefficients ({{math|''x<sub>c</sub>''}}) and one for the matrix elements ({{math|''y<sub>c</sub>''}}). It is not possible to exactly solve for {{math|''x<sub>c</sub>''}}. We can only say that the ratios are equal, that is\n\n:<math>\\frac{x_c}{x_d} = \\frac{y_c}{y_d}</math>\n\nor that {{math|''x<sub>c</sub>'' ∝ ''y<sub>c</sub>''}}, where the coefficient of proportionality is independent of the indices. Hence, by comparing recursion relations, we can identify the Clebsch–Gordan coefficient {{math|⟨''j''<sub>1</sub> ''m''<sub>1</sub> ''j''<sub>2</sub> (''m''<sub>2</sub> ± 1){{!}}''j m''⟩}} with the matrix element {{math|⟨''j''′ ''m''′{{!}}''T''<sup>(''k'')</sup><sub>''q'' ± 1</sub>{{!}}''j'' ''m''⟩}}, then we may write\n\n:<math>\n  \\langle j' \\, m' | T^{(k)}_{q \\pm 1} | j \\, m\\rangle\n  \\propto \\langle j \\, m \\, k \\, (q \\pm 1) | j' \\, m' \\rangle.\n</math>\n\n== Alternative conventions ==\n\nThere are different conventions for the reduced matrix elements.  One convention, used by Racah<ref>{{Cite journal |first=G. |last=Racah |title=Theory of Complex Spectra II |journal=[[Physical Review]] |volume=62 |issue=9–10 |pages=438–462 |year=1942 |doi=10.1103/PhysRev.62.438 |bibcode = 1942PhRv...62..438R }}</ref> and Wigner,<ref>{{cite book |last=Wigner |first=E. P. |editor1-last=Wightman |editor1-first=Arthur S. |date=1951 |chapter=On the Matrices Which Reduce the Kronecker Products of Representations of S. R. Groups |title=The Collected Works of Eugene Paul Wigner |volume=3 |pages=614 |doi=10.1007/978-3-662-02781-3_42 }}</ref> includes an additional phase and normalization factor,\n\n:<math>\n    \\langle j \\, m | T^{(k)}_q | j' \\, m'\\rangle\n  = \\frac{(-1)^{2 k} \\langle j' \\, m' \\, k \\, q | j \\, m \\rangle \\langle j \\| T^{(k)} \\| j'\\rangle_{\\mathrm{R}}}{\\sqrt{2 j + 1}}\n  = (-1)^{j - m}\n    \\begin{pmatrix}\n      j & k &  j' \\\\\n      -m & q & m'\n    \\end{pmatrix} \\langle j \\| T^{(k)} \\| j'\\rangle_{\\mathrm{R}}.\n</math>\nwhere the {{math|2 × 3}} array denotes the [[3-j symbol]].  (Since in practice {{math|''k''}} is often integral, the {{math|(−1)<sup>2 ''k''</sup>}} factor is sometimes omitted in literature.)  With this choice of normalization, the reduced matrix element satisfies the relation:\n\n:<math>\\langle j \\| T^{\\dagger (k)} \\| j'\\rangle_{\\mathrm{R}} = (-1)^{k + j' - j} \\langle j' \\| T^{(k)} \\| j\\rangle_{\\mathrm{R}}^*,</math>\n\nwhere the [[Hermitian adjoint]] is defined with the [[Tensor operator#Spherical tensor operators|{{math|''k'' − ''q''}} convention]].  Although this relation is not affected by the presence or absence of the {{math|(−1)<sup>2 ''k''</sup>}} phase factor in the definition of the reduced matrix element, it is affected by the phase convention for the Hermitian adjoint.\n\nAnother convention for reduced matrix elements is that of Sakurai's ''Modern Quantum Mechanics'':\n\n:<math>\n    \\langle j \\, m | T^{(k)}_q | j' \\, m'\\rangle\n  = \\frac{\\langle j' \\, m' \\, k \\, q | j \\, m \\rangle \\langle j \\| T^{(k)} \\| j'\\rangle_{\\mathrm{S}}}{\\sqrt{2 j' + 1}}.\n</math>\n\n== Example ==\n\nConsider the position expectation value {{math|⟨''n j m''{{!}}''x''{{!}}''n j m''⟩}}. This matrix element is the expectation value of a Cartesian operator in a spherically symmetric hydrogen-atom-eigenstate [[Basis (linear algebra)|basis]], which is a nontrivial problem. However, the Wigner–Eckart theorem simplifies the problem. (In fact, we could obtain the solution quickly using [[Parity (physics)|parity]], although a slightly longer route will be taken.)\n\nWe know that {{math|''x''}} is one component of {{math|'''r'''}}, which is a vector.  Since vectors are rank-1 spherical tensor operators, it follows that {{math|''x''}} must be some linear combination of a rank-1 spherical tensor {{math|''T''<sup>(1)</sup><sub>''q''</sub>}} with {{math|''q'' ∈ {−1, 0, 1}}}.  In fact, it can be shown that\n\n:<math>x = \\frac{T^{(1)}_{-1} - T^{(1)}_1}{\\sqrt{2}},</math>\n\nwhere we define the spherical tensors as<ref name=\"J. Sakurai 1994\">J. J. Sakurai: \"Modern quantum mechanics\" (Massachusetts, 1994, Addison-Wesley).</ref>\n\n:<math>T^{(1)}_{q} = \\sqrt{\\frac{4 \\pi}{3}} r Y_1^q</math>\n\nand {{math|''Y''<sub>''l''</sub><sup>''m''</sup>}} are [[spherical harmonic]]s, which themselves are also spherical tensors of rank {{math|''l''}}.  Additionally, {{math|''T''<sup>(1)</sup><sub>0</sub> {{=}} ''z''}}, and\n\n:<math>T^{(1)}_{\\pm 1} = \\mp \\frac{x \\pm i y}{\\sqrt{2}}.</math>\n\nTherefore,\n\n:<math>\n\\begin{align}\n    &\\langle n \\, j \\, m | x | n' \\, j' \\, m'\n\\rangle = \\\\\n  & = \\left\\langle n \\, j \\, m \\left| \\frac{T^{(1)}_{-1} - T^{(1)}_1}{\\sqrt{2}} \\right| n' \\, j' \\, m'\n\\right\\rangle = \\\\\n  & = \\frac{1}{\\sqrt{2}} \\langle n \\, j \\| T^{(1)} \\| n' \\, j'\\rangle \\,\n    \\big(\\langle j' \\, m' \\, 1 \\, (-1) | j \\, m \\rangle - \\langle j' \\, m' \\, 1 \\, 1 | j \\, m \\rangle\\big).\n\\end{align}\n</math>\n\nThe above expression gives us the matrix element for {{math|''x''}} in the {{math|{{!}}''n j m''⟩}} basis.  To find the expectation value, we set {{math|''n''′ {{=}} ''n''}}, {{math|''j''′ {{=}} ''j''}}, and {{math|''m''′ {{=}} ''m''}}.  The selection rule for {{math|''m''′}} and {{math|''m''}} is {{math|''m'' ± 1 {{=}} ''m''′}} for the {{math|''T''<sup>(1)</sup><sub>±1</sub>}} spherical tensors.  As we have {{math|''m''′ {{=}} ''m''}}, this makes the Clebsch–Gordan Coefficients zero, leading to the expectation value to be equal to zero.\n\n==See also==\n\n*[[Tensor operator]]\n*[[Landé g-factor]]\n\n==References==\n\n<references/>\n\n=== General ===\n\n* {{Citation| last=Hall|first=Brian C.|title=Lie groups, Lie algebras, and representations: An elementary introduction|edition=2nd|series=Graduate Texts in Mathematics|volume=222|publisher=Springer|year=2015|isbn=978-3319134666}}\n\n==External links==\n*J. J. Sakurai, (1994). \"Modern Quantum Mechanics\", Addison Wesley, {{ISBN|0-201-53929-2}}.\n*{{mathworld|urlname=Wigner-EckartTheorem|title= Wigner–Eckart theorem}}\n*[http://electron6.phys.utk.edu/qm2/modules/m4/wigner.htm Wigner–Eckart theorem]\n*[http://galileo.phys.virginia.edu/classes/752.mf1i.spring03/TensorOperators.htm Tensor Operators]\n\n{{DEFAULTSORT:Wigner-Eckart theorem}}\n[[Category:Quantum mechanics]]\n[[Category:Representation theory of Lie groups]]\n[[Category:Theorems in quantum physics]]\n[[Category:Theorems in representation theory]]"
    },
    {
      "title": "Abhyankar's conjecture",
      "url": "https://en.wikipedia.org/wiki/Abhyankar%27s_conjecture",
      "text": "{{distinguish|Abhyankar's lemma}}\n\nIn [[abstract algebra]], '''Abhyankar's conjecture''' is a 1957 [[conjecture]] of [[Shreeram Abhyankar]], on the [[Galois group]]s of [[algebraic function field]]s of [[Characteristic (algebra)|characteristic]] ''p''.<ref>{{Citation |last=Abhyankar |first=Shreeram |year=1957 |title=Coverings of Algebraic Curves |journal=[[American Journal of Mathematics]] |volume=79 |issue=4 |pages=825–856 |doi=10.2307/2372438 }}.</ref> The soluble case was solved by Serre in 1990<ref>{{citation | last=Serre | first=Jean-Pierre | authorlink=Jean-Pierre Serre | title=Construction de revêtements étales de la droite affine en caractéristique p | language=French | zbl=0726.14021 | journal=Comptes Rendus de l'Académie des Sciences, Série I | volume=311 | number=6 | pages=341–346 | year=1990 }}</ref> and the full conjecture was proved in 1994 by work of [[Michel Raynaud]] and [[David Harbater]].<ref>{{Citation |first=Michel |last=Raynaud |title=Revêtements de la droite affine en caractéristique p > 0 |journal=Inventiones Mathematicae |volume=116 |issue=1 |year=1994 |pages=425–462 | doi=10.1007/BF01231568 | zbl=0798.14013 |bibcode=1994InMat.116..425R }}.</ref><ref>{{Citation |first=David |last=Harbater |year=1994 |title=Abhyankar's conjecture on Galois groups over curves |journal=Inventiones Mathematicae |volume=117 |issue=1 |pages=1–25 | doi=10.1007/BF01232232 | zbl=0805.14014 |bibcode=1994InMat.117....1H }}.</ref><ref name=FJ70>{{citation | last1=Fried | first1=Michael D. | last2=Jarden | first2=Moshe | title=Field arithmetic | edition=3rd | series=Ergebnisse der Mathematik und ihrer Grenzgebiete. 3. Folge | volume=11 | publisher=[[Springer-Verlag]] | year=2008 | isbn=978-3-540-77269-9 | zbl=1145.12001 | page=70 }}</ref>\n\nThe problem involves a [[finite group]] ''G'', a [[prime number]] ''p'', and the [[function field of an algebraic variety|function field]] ''K(C)'' of a nonsingular integral [[algebraic curve]] ''C'' defined over an [[algebraically closed]] [[Field (mathematics)|field]] ''K'' of characteristic ''p''.\n\nThe question addresses the existence of a [[Galois extension]] ''L'' of ''K''(''C''), with ''G'' as Galois group, and with specified [[Ramification (mathematics)|ramification]]. From a geometric point of view, ''L'' corresponds to another curve ''C''&prime;, together with\na [[morphism]]\n\n:π : ''C''&prime; → ''C''.\n\nGeometrically, the assertion that π is ramified at a finite set ''S'' of points on ''C''\nmeans that π restricted to the complement of ''S'' in ''C'' is an [[étale morphism]].\nThis is in analogy with the case of [[Riemann surface]]s.\nIn Abhyankar's conjecture, ''S'' is fixed, and the question is what ''G'' can be. This is therefore a special type of [[inverse Galois problem]].\n\nThe subgroup ''p''(''G'') is defined to be the subgroup generated by all the [[Sylow subgroup]]s of ''G'' for the prime number ''p''. This is a [[normal subgroup]], and the parameter ''n'' is defined as the minimum number of generators of\n\n:''G''/''p''(''G'').\n\nThen for the case of ''C'' the [[projective line]] over ''K'', the conjecture states that ''G'' can be realised as a Galois group of ''L'', unramified outside ''S'' containing ''s'' + 1 points,  if and only if\n\n:''n'' ≤ ''s''.\n\nThis was proved by Raynaud.\n\nFor the general case, proved by Harbater, let ''g'' be the [[Genus (mathematics)|genus]] of ''C''. Then ''G'' can be realised if and only if\n\n:''n'' ≤ ''s'' + 2 ''g''.\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* {{MathWorld|urlname=AbhyankarsConjecture|title=Abhyankar's conjecture}}\n* [http://www.math.purdue.edu/about/purview/spring95/conjecture.html A layman's perspective of Abhyankar's conjecture] from [[Purdue University]]\n\n[[Category:Algebraic curves]]\n[[Category:Galois theory]]\n[[Category:Theorems in abstract algebra]]\n[[Category:Conjectures that have been proved]]"
    },
    {
      "title": "Abhyankar's inequality",
      "url": "https://en.wikipedia.org/wiki/Abhyankar%27s_inequality",
      "text": "'''Abhyankar's inequality''' is an inequality involving extensions of [[valued field]]s in [[algebra]], introduced by {{harvs|txt|authorlink=Shreeram Shankar Abhyankar|last=Abhyankar|year=1956}}.\n\nIf ''K''/''k'' is an extension of [[valued field]]s, then Abhyankar's inequality states that the [[transcendence degree]] of ''K''/''k'' is at least the [[transcendence degree]] of the [[residue field]] extension plus the [[Q-rank|''Q''-rank]] of the [[quotient]] of the [[valuation group]]s.\n\n==References==\n\n*{{Citation | last1=Abhyankar | first1=Shreeram | title=On the valuations centered in a local domain | jstor=2372519 | mr=0082477  | year=1956 | journal=[[American Journal of Mathematics]] | issn=0002-9327 | volume=78 | pages=321–348 | doi=10.2307/2372519}}\n\n[[Category:Field theory]]\n[[Category:Commutative algebra]]\n[[Category:Theorems in abstract algebra]]\n\n{{abstract-algebra-stub}}"
    },
    {
      "title": "Abhyankar's lemma",
      "url": "https://en.wikipedia.org/wiki/Abhyankar%27s_lemma",
      "text": "{{dablink|Abhyankar's lemma is not directly related to [[Abhyankar's conjecture]].}}\nIn [[mathematics]], '''Abhyankar's lemma''' (named after [[Shreeram Shankar Abhyankar]]) allows one to kill [[tame ramification]] by taking an extension of a base field. \n\nMore precisely, Abhyankar's lemma states that if ''A'', ''B'', ''C'' are [[local field]]s such that ''A'' and ''B'' are [[finite extension]]s of ''C'', with [[ramification index|ramification indices]] ''a'' and ''b'', and ''B'' is tamely ramified over ''C'' and ''b'' divides ''a'', then the [[compositum]]\n''AB'' is an unramified extension of ''A''.\n\n==References==\n*{{citation|first=Gary|last=Cornell|jstor=1998895|title=On the Construction of Relative Genus Fields|journal=[[Transactions of the American Mathematical Society]]|volume=271|issue=2|year=1982|pages=501–511|doi=10.2307/1998895}}. Theorem 3, page 504.\n*{{citation|last1=Gold|first1=Robert|last2=Madan|first2=M. L.|title=Some applications of Abhyankar's lemma|journal=[[Mathematische Nachrichten]]|volume=82|year=1978|pages=115–119|doi=10.1002/mana.19780820112}}.\n*{{citation|first=A.|last=Grothendieck|authorlink=Alexander Grothendieck|arxiv=math.AG/0206203|title=Revêtements étales et groupe fondamental (SGA 1, Séminaire de Géométrie Algébriques du Bois-Marie 1960/61)|series=Lecture Notes in Mathematics|volume=224|publisher=Springer-Verlag|year=1971}}, [https://web.archive.org/web/20060906082030/http://modular.fas.harvard.edu/sga/sga/1/1t_279.html p. 279].\n* {{citation | last=Narkiewicz | first=Władysław | title=Elementary and analytic theory of algebraic numbers | edition=3rd | zbl=1159.11039 | series=Springer Monographs in Mathematics | location=Berlin | publisher=[[Springer-Verlag]] | isbn=3-540-21902-1 | year=2004 | page=229 }}.\n\n[[Category:Theorems in algebraic geometry]]\n[[Category:Lemmas]]\n[[Category:Algebraic number theory]]\n[[Category:Theorems in abstract algebra]]\n\n\n{{algebra-stub}}"
    },
    {
      "title": "Ado's theorem",
      "url": "https://en.wikipedia.org/wiki/Ado%27s_theorem",
      "text": "In [[abstract algebra]], '''Ado's theorem''' is a theorem characterizing finite-dimensional [[Lie algebra]]s.\n\n==Statement==\n\nAdo's theorem states that every finite-dimensional [[Lie algebra]] ''L'' over a [[field (mathematics)|field]] ''K'' of [[characteristic zero]] can be viewed as a Lie algebra of [[square matrices]] under the [[commutator bracket]]. More precisely, the theorem states that ''L'' has a [[linear representation]] ρ over ''K'', on a [[finite-dimensional vector space]] ''V'', that is a [[faithful representation]], making ''L'' isomorphic to a subalgebra of the [[endomorphism]]s of ''V''.\n\n==History==\nThe theorem was proved in 1935 by Igor Dmitrievich Ado of [[Kazan State University]], a student of [[Nikolai Chebotaryov]].\n\nThe restriction on the characteristic was later removed by [[Kenkichi Iwasawa]] (see also the below [[Gerhard Hochschild]] paper for a proof).\n\n==Implications==\nWhile for the Lie algebras associated to [[classical group]]s there is nothing new in this, the general case is a deeper result. Applied to the real Lie algebra of a [[Lie group]] ''G'', it does not imply that ''G'' has a faithful linear representation (which is not true in general), but rather that ''G'' always has a linear representation that is a [[local isomorphism]] with a [[linear group]].\n\n==References==\n* {{Citation | last=Ado | first=Igor D. | title=Note on the representation of finite continuous groups by means of linear substitutions|journal= Izv. Fiz.-Mat. Obsch. (Kazan')|volume= 7  |year=1935|pages=1–43}}.  (Russian language)\n*{{Citation | last=Ado | first=Igor D. | title=The representation of Lie algebras by matrices | url=http://mi.mathnet.ru/eng/umn/v2/i6/p159 | language=Russian | mr=0027753 | year=1947 | journal=Akademiya Nauk SSSR i Moskovskoe Matematicheskoe Obshchestvo. Uspekhi Matematicheskikh Nauk | issn=0042-1316 | volume=2 | issue=6 | pages=159–173}} translation in {{Citation | last1=Ado | first1=Igor D. | title=The representation of Lie algebras by matrices | mr=0030946 | year=1949 | journal=American Mathematical Society Translations | issn=0065-9290 | volume=1949 | issue=2 | pages=21}}\n*{{Citation | last1=Iwasawa | first1=Kenkichi | authorlink=Kenkichi Iwasawa| title=On the representation of Lie algebras | mr=0032613 | year=1948 | journal=Japanese Journal of Mathematics | volume=19 | pages=405–426}}\n*{{Citation | last=Harish-Chandra | authorlink=Harish-Chandra | title=Faithful representations of Lie algebras | jstor=1969352 | mr=0028829 | year=1949 | journal=[[Annals of Mathematics]] |series=Second Series | issn=0003-486X | volume=50 | pages=68–76 | doi=10.2307/1969352}}\n* {{Citation | last=Hochschild | first=Gerhard |authorlink=Gerhard Hochschild| title=An addition to Ado's theorem | year=1966 | journal=[[Proceedings of the American Mathematical Society]] | volume=17 | pages=531–533 | url=http://www.ams.org/journals/proc/1966-017-02/S0002-9939-1966-0194482-0/home.html | doi=10.1090/s0002-9939-1966-0194482-0}}\n* [[Nathan Jacobson]], ''Lie Algebras'', pp.&nbsp;202–203\n\n==External links==\n*[http://terrytao.wordpress.com/2011/05/10/ados-theorem/ Ado’s theorem], comments and a proof of Ado's theorem in [[Terence Tao]]'s blog ''What's new''.\n\n[[Category:Lie algebras]]\n[[Category:Theorems in abstract algebra]]"
    },
    {
      "title": "Andreotti–Grauert theorem",
      "url": "https://en.wikipedia.org/wiki/Andreotti%E2%80%93Grauert_theorem",
      "text": "In mathematics, the '''Andreotti–Grauert theorem''', introduced by {{harvs|txt|last=Andreotti|last2=Grauert|author1-link=Aldo Andreotti|author2-link=Hans Grauert|year=1962}},  gives conditions for [[cohomology group]]s of [[coherent sheaf|coherent sheaves]] over [[complex manifold]]s to vanish or to be finite-dimensional.\n\n==References==\n\n*{{Citation | last1=Andreotti | first1=Aldo | last2=Grauert | first2=Hans | title=Théorème de finitude pour la cohomologie des espaces complexes | url=http://www.numdam.org/item?id=BSMF_1962__90__193_0 | mr=0150342  | year=1962 | journal=Bulletin de la Société Mathématique de France | issn=0037-9484 | volume=90 | pages=193–259}}\n\n{{DEFAULTSORT:Andreotti-Grauert theorem}}\n[[Category:Complex manifolds]]\n[[Category:Theorems in abstract algebra]]\n\n{{abstract-algebra-stub}}"
    },
    {
      "title": "Artin approximation theorem",
      "url": "https://en.wikipedia.org/wiki/Artin_approximation_theorem",
      "text": "In [[mathematics]], the '''Artin approximation theorem''' is a fundamental result of {{harvs|last=Artin|first=Michael|txt|authorlink=Michael Artin|year=1969}} in [[deformation theory]] which implies that [[formal power series]] with coefficients in a [[field (mathematics)|field]] ''k'' are well-approximated by the [[algebraic function]]s on ''k''.\n\nMore precisely, Artin proved two such theorems: one, in 1968, on approximation of complex analytic solutions by formal solutions (in the case k = C); and an algebraic version of this theorem in 1969.\n\n==Statement of the theorem==\nLet \n\n:'''x''' = ''x''<sub>1</sub>, …, ''x''<sub>''n''</sub>\n\ndenote a collection of ''n'' [[indeterminate (variable)|indeterminate]]s,\n\n''k'''''<nowiki>[[x]]</nowiki>''' the [[ring (mathematics)|ring]] of formal power series with indeterminates '''x''' over a field ''k'', and\n\n: '''y''' = ''y''<sub>1</sub>, …, ''y''<sub>''m''</sub>\n\na different set of indeterminates. Let \n\n:''f''('''x''', '''y''') = 0\n\nbe a system of [[polynomial equation]]s in ''k''['''x''', '''y'''], and ''c'' a positive [[integer]]. Then given a formal power series solution '''ŷ'''('''x''') ∈ ''k'''''<nowiki>[[x]]</nowiki>''' there is an algebraic solution '''y'''('''x''') consisting of [[algebraic function]]s (more precisely, algebraic power series) such that \n\n:'''ŷ'''('''x''') ≡ '''y'''('''x''') mod ('''x''')<sup>''c''</sup>.\n\n==Discussion==\nGiven any desired positive integer ''c'', this theorem shows that one can find an algebraic solution approximating a formal power series solution up to the degree specified by ''c''. This leads to theorems that deduce the existence of certain [[formal moduli space]]s of deformations as [[scheme (mathematics)|scheme]]s. See also: [[Artin's criterion]].\n\n==Alternative statement==\nThe following alternative statement is given in Theorem 1.12 of {{harvs|last=Artin|first=Michael|txt|authorlink=Michael Artin|year=1969}}.\n\nLet ''R'' be a field or an excellent discrete valuation ring, let ''A'' be the henselization of an ''R''-algebra of finite type at a prime ideal, let ''m'' be a proper ideal of ''A'', let <math> \\hat{A}</math> be the ''m''-adic completion of ''A'', and let \n\n:''F'': (''A''-algebras) → (sets), \n\nbe a functor sending filtered colimits to filtered colimits (Artin calls such a functor locally of finite presentation).\n\nThen for any integer ''c'' and any <math> \\overline{\\xi} \\in F(\\hat{A})</math> there is a <math> \\xi \\in F(A)</math> such that \n\n:<math>\\overline{\\xi}</math> ≡ <math>\\xi</math> mod ''m''<sup>''c''</sup>.\n\n\n== See also ==\n*[[Ring with the approximation property]]\n*[[Popescu's theorem]]\n\n==References==\n*{{Citation | last1=Artin | first1=Michael | author1-link=Michael Artin | title=Algebraic approximation of structures over complete local rings | url=http://www.numdam.org/item?id=PMIHES_1969__36__23_0 | mr=0268188  | year=1969 | journal=[[Publications Mathématiques de l'IHÉS]] | issue=36 | pages=23–58}}\n*Artin, Michael. ''Algebraic Spaces''. Yale University Press, 1971.\n\n[[Category:Moduli theory]]\n[[Category:Commutative algebra]]\n[[Category:Theorems in abstract algebra]]"
    },
    {
      "title": "Artin–Tate lemma",
      "url": "https://en.wikipedia.org/wiki/Artin%E2%80%93Tate_lemma",
      "text": "In algebra, the '''Artin–Tate lemma''', named after [[Emil Artin]] and [[John Tate]], states:<ref>{{harvnb|Eisenbud|loc=Exercise 4.32}}</ref>\n:Let ''A'' be a commutative [[Noetherian ring]] and <math>B \\sub C</math> [[Associative algebra|algebras over ''A'']]. If ''C'' is of finite type over ''A'' and if ''C'' is finite over ''B'', then ''B'' is of finite type over ''A''.\n(Here, \"of finite type\" means \"[[finitely generated algebra]]\" and \"finite\" means \"[[finitely generated module]]\".) The lemma was introduced by E. Artin and J. Tate in 1951<ref>E Artin, J.T Tate, \"A note on finite ring extensions,\" J. Math. Soc Japan, Volume 3, 1951, pp. 74–77</ref> to give a proof of [[Hilbert's Nullstellensatz]].\n\n== Proof ==\n\nThe following proof can be found in  Atiyah–MacDonald.<ref>{{harvnb|Atiyah–MacDonald|1969|loc=Proposition 7.9}}</ref> Let <math> x_1,\\ldots, x_m</math> generate <math>C</math> as an <math>A</math>-algebra and let <math>y_1, \\ldots, y_n</math> generate <math>C</math> as a <math>B</math>-module. Then we can write \n\n:<math>x_i = \\sum_j b_{ij}y_j \\quad \\text{and} \\quad y_iy_j = \\sum_{k}b_{ijk}y_k</math> \n\nwith <math>b_{ij},b_{ijk} \\in B</math>. Then <math>C</math> is finite over the <math>A</math>-algebra <math>B_0</math> generated by the <math>b_{ij},b_{ijk}</math>. Using that <math>A</math> and hence <math>B_0</math> is Noetherian, also <math>B</math> is finite over <math>B_0</math>. Since <math>B_0</math> is a finitely generated <math>A</math>-algebra, also <math>B</math> is a finitely generated <math>A</math>-algebra.\n\n== Noetherian necessary ==\nWithout the assumption that ''A'' is Noetherian, the statement of the Artin-Tate lemma is no longer true. Indeed, for any [[Noetherian ring|non-Noetherian ring]] ''A'' we can define an ''A''-algebra structure on <math>C = A\\oplus A</math> by declaring <math>(a,x)(b,y) = (ab,bx+ay)</math>. Then for any ideal <math>I \\subset A</math> which is not finitely generated, <math>B = A \\oplus I \\subset C</math> is not of finite type over ''A'', but all conditions as in the lemma are satisfied.\n\n== References ==\n{{reflist}}\n* [[David Eisenbud|Eisenbud, David]], ''Commutative Algebra with a View Toward Algebraic Geometry'', Graduate Texts in Mathematics, 150, Springer-Verlag, 1995, {{ISBN|0-387-94268-8}}.\n*[[Michael Atiyah|M. Atiyah]], [[Ian G. Macdonald|I.G. Macdonald]], ''Introduction to Commutative Algebra'', [[Addison–Wesley]], 1994. {{ISBN|0-201-40751-5}}\n\n== External links ==\n*http://commalg.subwiki.org/wiki/Artin-Tate_lemma\n\n{{DEFAULTSORT:Artin-Tate lemma}}\n[[Category:Theorems in abstract algebra]]\n[[Category:Lemmas]]\n[[Category:Commutative algebra]]\n\n\n{{algebra-stub}}"
    },
    {
      "title": "Artin–Wedderburn theorem",
      "url": "https://en.wikipedia.org/wiki/Artin%E2%80%93Wedderburn_theorem",
      "text": "In [[algebra]], the '''Artin–Wedderburn theorem''' is a [[classification theorem]] for [[semisimple ring]]s and [[semisimple algebra]]s.  The theorem states that an (Artinian) <ref>[[Semisimple ring]]s are necessarily [[Artinian ring]]s. Some authors use \"semisimple\" to mean the ring has a trivial [[Jacobson radical]]. For Artinian rings, the two notions are equivalent, so \"Artinian\" is included here to eliminate that ambiguity.</ref> semisimple ring ''R'' is isomorphic to a [[Product of rings|product]] of finitely many ''n<sub>i</sub>''-by-''n<sub>i</sub>'' [[matrix ring]]s over [[division ring]]s ''D<sub>i</sub>'', for some integers ''n<sub>i</sub>'', both of which are uniquely determined up to permutation of the index ''i''.  In particular, any [[simple ring|simple]] left or right [[Artinian ring]] is isomorphic to an ''n''-by-''n'' [[matrix ring]] over a [[division ring]] ''D'', where both ''n'' and ''D'' are uniquely determined.<ref name=\"Beachy1999\">{{cite book|author=John A. Beachy|title=Introductory Lectures on Rings and Modules|url=https://books.google.com/books?id=rnNzivBfgOoC&pg=PA156|year=1999|publisher=Cambridge University Press|isbn=978-0-521-64407-5|page=156}}</ref>\n\nAs a direct corollary, the Artin–Wedderburn theorem implies that every simple ring that is finite-dimensional over a division ring (a '''simple algebra''') is a [[matrix ring]].  This is [[Joseph Wedderburn]]'s original result.  [[Emil Artin]] later generalized it to the case of Artinian rings.\n\nNote that if ''R'' is a finite-dimensional simple algebra over a division ring ''E'', ''D'' need not be contained in ''E''.  For example, matrix rings over the [[complex number]]s are finite-dimensional simple algebras over the [[real number]]s.\n\nThe Artin–Wedderburn theorem reduces classifying simple rings over a division ring to classifying division rings that contain a given division ring.  This in turn can be simplified:  The [[center (ring theory)|center]] of ''D'' must be a [[field (mathematics)|field]] K. Therefore, ''R'' is a ''K''-algebra, and itself has ''K'' as its center.  A finite-dimensional simple algebra ''R'' is thus a [[central simple algebra]] over K.  Thus the Artin–Wedderburn theorem reduces the problem of classifying finite-dimensional central simple algebras to the problem of classifying division rings with given center.\n\n==Examples==\nLet '''R''' be the field of real numbers, '''C''' be the field of complex numbers, and '''H''' the [[quaternion]]s.\n\n* Every finite-dimensional [[simple algebra]] over '''R''' is isomorphic to a matrix ring over '''R''', '''C''', or '''H'''.  Every [[central simple algebra]] over '''R''' is isomorphic to a matrix ring over '''R''' or '''H'''.  These results follow from the [[Frobenius theorem (real division algebras)|Frobenius theorem]].\n* Every finite-dimensional simple algebra over '''C''' is a central simple algebra, and is isomorphic to a matrix ring over '''C'''.\n* Every finite-dimensional central simple algebra over a [[finite field]] is isomorphic to a matrix ring over that field.\n* For a [[commutative ring]], the four following properties are equivalent: being a [[semisimple ring]]; being [[Artinian ring|Artinian]] and [[reduced ring|reduced]]; being a [[reduced ring|reduced]] [[Noetherian ring]] of [[Krull dimension]] 0; being isomorphic to a finite direct product of fields.\n* The Artin–Wedderburn theorem implies that a [[semisimple algebra]] that is finite-dimensional over a field <math> k</math> is isomorphic to a finite product <math> \\prod M_{n_i}(D_i) </math> where the <math> n_i </math> are natural numbers, the <math> D_i </math> are finite dimensional [[division algebra]]s over <math>k </math> (possibly [[finite field extension|finite extension field]]s of {{mvar|k}}), and <math> M_{n_i}(D_i) </math> is the algebra of <math> n_i \\times n_i </math> matrices over <math> D_i</math>.  Again, this product is unique up to permutation of the factors.\n\n==See also==\n* [[Maschke's theorem]]\n* [[Brauer group]]\n* [[Jacobson density theorem]]\n* [[Hypercomplex number]]\n\n==References==\n{{Reflist}}\n* [[P. M. Cohn]] (2003) ''Basic Algebra: Groups, Rings, and Fields'', pages 137&ndash;9.\n* {{cite journal |author=J.H.M. Wedderburn |author-link=Joseph Wedderburn | title=On Hypercomplex Numbers |journal=Proceedings of the London Mathematical Society |volume=6 | pages=77–118 |year=1908 | doi= 10.1112/plms/s2-6.1.77 }}\n* {{cite journal|last=Artin|first=E.|title=Zur Theorie der hyperkomplexen Zahlen|year=1927|volume=5|pages=251–260}}\n\n{{DEFAULTSORT:Artin-Wedderburn Theorem}}\n[[Category:Ring theory]]\n[[Category:Theorems in abstract algebra]]"
    },
    {
      "title": "Artin–Zorn theorem",
      "url": "https://en.wikipedia.org/wiki/Artin%E2%80%93Zorn_theorem",
      "text": "In [[mathematics]], the '''Artin–Zorn theorem''', named after [[Emil Artin]] and [[Max Zorn]], states that any finite [[alternative ring|alternative]] [[division ring]] is necessarily a [[finite field]].  It was first published in 1930 by Zorn, but in his publication Zorn credited it to Artin.<ref>{{citation|first=M.|last=Zorn|authorlink=Max Zorn|title=Theorie der alternativen Ringe|journal=[[Abhandlungen aus dem Mathematischen Seminar der Universität Hamburg]]|volume=8|year=1930|pages=123–147}}.</ref><ref>{{citation|first=Heinz|last=Lüneburg|contribution=On the early history of Galois fields|pages=341–355|mr=1849100|title=Finite fields and applications: proceedings of the Fifth International Conference on Finite Fields and Applications Fq5, held at the University of Augsburg, Germany, August 2–6, 1999|editor1-first=Dieter|editor1-last=Jungnickel|editor1-link=Dieter Jungnickel|editor2-first=Harald|editor2-last=Niederreiter|editor2-link= Harald Niederreiter |publisher=Springer-Verlag|year=2001|isbn=978-3-540-41109-3}}.</ref>\n\nThe Artin–Zorn theorem is a generalization of the [[Wedderburn theorem]], which states that finite associative division rings are fields. As a geometric consequence, every finite [[Moufang plane]] is the classical projective plane over a finite field.<ref>{{citation|title=Points and Lines: Characterizing the Classical Geometries|series=Universitext|first=Ernest|last=Shult|publisher=Springer-Verlag|year=2011|isbn=978-3-642-15626-7|page=123}}.</ref><ref>{{citation|title=A taste of Jordan algebras|series=Universitext|publisher=Springer-Verlag|first=Kevin|last=McCrimmon|year=2004|isbn=978-0-387-95447-9|page=34}}.</ref>\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Artin-Zorn theorem}}\n[[Category:Ring theory]]\n[[Category:Theorems in abstract algebra]]\n\n\n{{Abstract-algebra-stub}}"
    },
    {
      "title": "Auslander–Buchsbaum formula",
      "url": "https://en.wikipedia.org/wiki/Auslander%E2%80%93Buchsbaum_formula",
      "text": "{{distinguish|Auslander–Buchsbaum theorem}}\nIn [[commutative algebra]], the '''Auslander–Buchsbaum formula''', introduced by {{harvs|txt|last=Auslander|author1-link=Maurice Auslander|last2=Buchsbaum|author2-link=David Buchsbaum|year=1957|loc=theorem 3.7}},  states that if ''R'' is a commutative [[Noetherian ring|Noetherian]] [[local ring]] and ''M'' is a non-zero [[finitely generated module|finitely generated]] ''R''-module of finite [[projective dimension]], then\n\n: <math> \\mathrm{pd}_R(M) + \\mathrm{depth}(M) = \\mathrm{depth}(R).</math>\n\nHere pd stands for the projective dimension of a module, and depth for the [[depth (ring theory)|depth]] of a module.\n\n==Applications==\n\nThe Auslander–Buchsbaum formula implies that a Noetherian local ring is [[regular local ring|regular]] if, and only if, it has finite [[global dimension]]. In turn this implies that the [[localization of a ring|localization]] of a regular local ring is regular.\n\nIf ''A'' is a local finitely generated [[algebra (ring theory)|''R''-algebra]] (over a regular local ring ''R''), then the Auslander–Buchsbaum formula implies that ''A'' is [[Cohen–Macaulay ring|Cohen–Macaulay]] if, and only if, pd<sub>''R''</sub>''A'' = codim<sub>''R''</sub>''A''.\n\n==References==\n\n*{{Citation | last1=Auslander | first1=Maurice | last2=Buchsbaum | first2=David A. | title=Homological dimension in local rings | jstor=1992937 | mr=0086822 | year=1957 | journal=[[Transactions of the American Mathematical Society]] | issn=0002-9947 | volume=85 | pages=390–405 | doi=10.2307/1992937}}\n*Chapter 19 of {{Citation\n| last=Eisenbud\n| first=David\n| author-link=David Eisenbud\n| title=Commutative algebra with a view toward algebraic geometry\n| publisher=[[Springer-Verlag]]\n| location=Berlin, New York\n| series=[[Graduate Texts in Mathematics]]\n| isbn=978-0-387-94269-8\n| mr=1322960\n| year=1995\n| volume=150\n}}\n\n{{DEFAULTSORT:Auslander-Buchsbaum formula}}\n[[Category:Commutative algebra]]\n[[Category:Theorems in abstract algebra]]\n\n{{abstract-algebra-stub}}"
    },
    {
      "title": "Auslander–Buchsbaum theorem",
      "url": "https://en.wikipedia.org/wiki/Auslander%E2%80%93Buchsbaum_theorem",
      "text": "{{distinguish|Auslander–Buchsbaum formula}}\nIn [[commutative algebra]], the '''Auslander–Buchsbaum theorem''' states that [[regular local ring]]s are [[unique factorization domain]]s.\n\nThe theorem was first proved by {{harvs|txt|first=Maurice|last=Auslander|authorlink=Maurice Auslander|first2=David|last2=Buchsbaum|author2-link=David Buchsbaum|year=1959}}. They showed that regular [[local ring]]s of dimension 3 are unique factorization domains, and {{harvs|txt|first1=Masayoshi | author1-link=Masayoshi Nagata |last=Nagata|year=1958}} had previously shown that this implies that all regular local rings are unique factorization domains.\n\n==References==\n\n*{{Citation | last1=Auslander | first1=Maurice | last2=Buchsbaum | first2=D. A. | title=Unique factorization in regular local rings | jstor=90213 | mr=0103906 | year=1959 | journal=[[Proceedings of the National Academy of Sciences|Proceedings of the National Academy of Sciences of the United States of America]] | issn=0027-8424 | volume=45 | pages=733–734 | doi=10.1073/pnas.45.5.733 | pmid=16590434 | pmc=222624}}\n*{{Citation | last1=Nagata | first1=Masayoshi | author1-link=Masayoshi Nagata | title=A general theory of algebraic geometry over Dedekind domains. II. Separably generated extensions and regular local rings | jstor=2372791 | mr=0094344 | year=1958 | journal=[[American Journal of Mathematics]] | issn=0002-9327 | volume=80 | pages=382–420 | doi=10.2307/2372791}}\n\n{{DEFAULTSORT:Auslander-Buchsbaum theorem}}\n[[Category:Commutative algebra]]\n[[Category:Theorems in abstract algebra]]\n\n\n{{abstract-algebra-stub}}"
    },
    {
      "title": "Beauville–Laszlo theorem",
      "url": "https://en.wikipedia.org/wiki/Beauville%E2%80%93Laszlo_theorem",
      "text": "In [[mathematics]], the '''Beauville–Laszlo theorem''' is a result in [[commutative algebra]] and [[algebraic geometry]] that allows one to \"glue\" two [[sheaf (mathematics)|sheaves]] over an infinitesimal neighborhood of a point on an [[algebraic curve]].  It was proved by {{Harvard citations|last=Beauville|first=Arnaud|author-link=Arnaud Beauville|last2=Laszlo|first2=Yves|author2-link=Yves Laszlo|year=1995|txt=yes}}.\n\n==The theorem==\nAlthough it has implications in algebraic geometry, the theorem is a [[local property|local]] result and is stated in its most primitive form for [[commutative rings]].  If ''A'' is a ring and ''f'' is a nonzero element of A, then we can form two derived rings: the [[localization of a ring|localization]] at ''f'', ''A''<sub>''f''</sub>, and the [[completion (ring theory)|completion]] at ''Af'', ''Â''; both are ''A''-[[algebra (ring theory)|algebra]]s. In the following we assume that ''f'' is a non-zero divisor.  Geometrically, ''A'' is viewed as a [[Scheme (mathematics)|scheme]] ''X'' = Spec ''A'' and ''f'' as a [[divisor (algebraic geometry)|divisor]] (''f'') on Spec ''A''; then ''A''<sub>''f''</sub> is its complement ''D''<sub>''f''</sub> = Spec ''A''<sub>''f''</sub>, the [[Zariski topology#Affine varieties|principal open set]] determined by ''f'', while ''Â'' is an \"infinitesimal neighborhood\" ''D'' = Spec ''Â'' of (''f'').  The intersection of ''D''<sub>''f''</sub> and Spec ''Â'' is a \"punctured infinitesimal neighborhood\" ''D''<sup>0</sup> about (''f''), equal to Spec ''Â'' ⊗<sub>''A''</sub> ''A''<sub>''f''</sub> = Spec ''Â''<sub>''f''</sub>.\n\nSuppose now that we have an ''A''-[[module (mathematics)|module]] ''M''; geometrically, ''M'' is a [[sheaf (mathematics)|sheaf]] on Spec ''A'', and we can restrict it to both the principal open set ''D''<sub>''f''</sub> and the infinitesimal neighborhood Spec ''Â'', yielding an ''A''<sub>''f''</sub>-module ''F'' and an ''Â''-module ''G''.  Algebraically,\n:<math>F = M \\otimes_A A_f = M_f \\qquad G = M \\otimes_A \\hat{A}.</math>\n(Despite the notational temptation to write <span style=\"vertical-align:33%;\"><math>G = \\widehat{M}</math></span>, meaning the completion of the ''A''-module ''M'' at the ideal ''Af'', unless ''A'' is [[noetherian]] and ''M'' is finitely-generated, the two are not in fact equal.  This phenomenon is the main reason that the theorem bears the names of Beauville and Laszlo; in the noetherian, finitely-generated case, it is, as noted by the authors, a special case of Grothendieck's [[faithfully flat descent]].)  ''F'' and ''G'' can both be further restricted to the punctured neighborhood ''D''<sup>0</sup>, and since both restrictions are ultimately derived from ''M'', they are isomorphic: we have an isomorphism\n:<math>\\phi \\colon G_f \\xrightarrow{\\sim} F \\otimes_{A_f} \\hat{A}_f = F \\otimes_A \\hat{A}.</math>\n\nNow consider the converse situation: we have a ring ''A'' and an element ''f'', and two modules: an ''A''<sub>''f''</sub>-module ''F'' and an ''Â''-module ''G'', together with an isomorphism ''&phi;'' as above.  Geometrically, we are given a scheme ''X'' and both an open set ''D''<sub>''f''</sub> and a \"small\" neighborhood ''D'' of its closed complement (''f''); on ''D''<sub>''f''</sub> and ''D'' we are given two sheaves which agree on the intersection ''D''<sup>0</sup> = ''D''<sub>''f''</sub> &cap; ''D''.  If ''D'' were an open set in the Zariski topology we could glue the sheaves; the content of the Beauville&ndash;Laszlo theorem is that, under one technical assumption on ''f'', the same is true for the infinitesimal neighborhood ''D'' as well.\n\n'''Theorem''': Given ''A'', ''f'', ''F'', ''G'', and ''&phi;'' as above, if ''G'' has no ''f''-torsion, then there exist an ''A''-module ''M'' and isomorphisms\n:<math>\\alpha \\colon M_f \\xrightarrow{\\sim} F \\qquad \\beta \\colon M \\otimes_A \\hat{A} \\xrightarrow{\\sim} G</math>\nconsistent with the isomorphism ''&phi;'': ''&phi;'' is equal to the composition\n:<math>G_f = G \\otimes_A A_f \\xrightarrow{\\beta^{-1} \\otimes 1} M \\otimes_A \\hat{A} \\otimes_A A_f = M_f \\otimes_A \\hat{A} \\xrightarrow{\\alpha \\otimes 1} F \\otimes_A \\hat{A}.</math>\n\nThe technical condition that ''G'' has no ''f''-torsion is referred to by the authors as \"''f''-regularity\".  In fact, one can state a stronger version of this theorem.  Let '''M'''(''A'') be the category of ''A''-modules (whose morphisms are ''A''-module homomorphisms) and let '''M'''<sub>''f''</sub>(''A'') be the [[full subcategory]] of ''f''-regular modules.  In this notation, we obtain a [[commutative diagram]] of categories (note '''M'''<sub>''f''</sub>(''A''<sub>''f''</sub>) = '''M'''(''A''<sub>''f''</sub>)):\n:<math>\\begin{array}{ccc}\n \\mathbf{M}_f(A) & \\longrightarrow & \\mathbf{M}_f(\\hat{A}) \\\\\n \\downarrow & & \\downarrow \\\\\n \\mathbf{M}(A_f) & \\longrightarrow & \\mathbf{M}(\\hat{A}_f)\n\\end{array}</math>\nin which the arrows are the base-change maps; for example, the top horizontal arrow acts on objects by ''M'' &rarr; ''M'' &otimes;<sub>''A''</sub> ''Â''.\n\n'''Theorem''': The above diagram is a [[cartesian diagram]] of categories.\n\n==Global version==\nIn geometric language, the Beauville&ndash;Laszlo theorem allows one to glue [[sheaf (mathematics)|sheaves]] on a one-dimensional [[affine scheme]] over an infinitesimal neighborhood of a point.  Since sheaves have a \"local character\" and since any scheme is locally affine, the theorem admits a global statement of the same nature.  The version of this statement that the authors found noteworthy concerns [[vector bundles]]:\n\n'''Theorem''': Let ''X'' be an [[algebraic curve]] over a field ''k'', ''x'' a ''k''-[[rational point|rational]] [[Singular point of an algebraic variety|smooth point]] on ''X'' with infinitesimal neighborhood ''D'' = Spec ''k''<nowiki>[[</nowiki>''t''<nowiki>]]</nowiki>, ''R'' a ''k''-algebra, and ''r'' a positive integer.  Then the category '''Vect'''<sub>''r''</sub>(''X''<sub>''R''</sub>) of rank-''r'' vector bundles on the curve ''X''<sub>''R''</sub> = ''X'' &times;<sub>Spec ''k''</sub> Spec ''R'' fits into a cartesian diagram:\n:<math>\\begin{array}{ccc}\n \\mathbf{Vect}_r(X_R) & \\longrightarrow & \\mathbf{Vect}_r(D_R) \\\\\n \\downarrow & & \\downarrow \\\\\n \\mathbf{Vect}_r((X \\setminus x)_R) & \\longrightarrow & \\mathbf{Vect}_r(D_R^0)\n\\end{array}</math>\n\nThis entails a corollary stated in the paper:\n\n'''Corollary''': With the same setup, denote by '''Triv'''(''X''<sub>''R''</sub>) the set of triples (''E'', ''&tau;'', ''&sigma;''), where ''E'' is a vector bundle on ''X''<sub>''R''</sub>, ''&tau;'' is a trivialization of ''E'' over (''X'' \\ ''x'')<sub>''R''</sub> (i.e., an isomorphism with the trivial bundle ''O''<sub>(''X'' - ''x'')<sub>''R''</sub></sub>), and ''&sigma;'' a trivialization over ''D''<sub>''R''</sub>.  Then the maps in the above diagram furnish a bijection between '''Triv'''(''X''<sub>''R''</sub>) and ''GL''<sub>''r''</sub>(''R''((''t''))) (where ''R''((''t'')) is the [[formal Laurent series]] ring).\n\nThe corollary follows from the theorem in that the triple is associated with the unique matrix which, viewed as a \"transition function\" over ''D''<sup>0</sup><sub>''R''</sub> between the trivial bundles over (''X'' \\ ''x'')<sub>''R''</sub> and over ''D''<sub>''R''</sub>, allows gluing them to form ''E'', with the natural trivializations of the glued bundle then being identified with ''&sigma;'' and ''&tau;''.  The importance of this corollary is that it shows that the [[affine Grassmannian]] may be formed either from the data of bundles over an infinitesimal disk, or bundles on an entire algebraic curve.\n\n==References==\n* {{Citation\n | last=Beauville\n | first=Arnaud\n | author-link=Arnaud Beauville\n | last2=Laszlo\n | first2=Yves\n | author2-link=Yves Laszlo\n | title=Un lemme de descente\n | year=1995\n | journal=Comptes Rendus de l'Académie des Sciences, Série I\n | volume=320\n | issue=3\n | pages=335–340\n | issn=0764-4442\n | url=http://math1.unice.fr/~beauvill/pubs/descente.pdf\n | accessdate=2008-04-08\n}}\n\n{{DEFAULTSORT:Beauville-Laszlo theorem}}\n[[Category:Vector bundles]]\n[[Category:Module theory]]\n[[Category:Theorems in algebraic geometry]]\n[[Category:Theorems in abstract algebra]]"
    },
    {
      "title": "Brauer–Nesbitt theorem",
      "url": "https://en.wikipedia.org/wiki/Brauer%E2%80%93Nesbitt_theorem",
      "text": "In [[mathematics]], the '''Brauer–Nesbitt theorem''' can refer to  several different theorems proved by [[Richard Brauer]] and [[Cecil J. Nesbitt]] in the [[representation theory]] of [[finite group]]s.\n\nIn [[modular representation theory]],\nthe '''Brauer–Nesbitt theorem on blocks of defect zero''' states that a character whose order is divisible by the highest power of a prime ''p'' dividing the order of a finite group remains irreducible when reduced mod ''p'' and vanishes on all elements whose order is divisible by ''p''. Moreover, it belongs to a [[block (representation theory)|block]] of [[defect zero]]. A block of defect zero contains only one [[ordinary character]] and only one [[modular character]].\n\nAnother version states that if ''k'' is a field of characteristic zero, ''A'' is a ''k''-algebra, ''V'', ''W'' are semisimple ''A''-modules which are finite dimensional over ''k'', and Tr<sub>''V''</sub> = Tr<sub>''W''</sub> as elements of Hom<sub>k</sub>(''A'',k), then ''V'' and ''W'' are isomorphic as ''A''-modules.\n\n==References==\n*Curtis, Reiner, ''Representation theory of finite groups and associative algebras'', Wiley 1962. \n*Brauer, R.; Nesbitt, C. ''On the modular characters of groups.'' Ann. of Math. (2) 42, (1941). 556-590.\n\n{{DEFAULTSORT:Brauer-Nesbitt theorem}}\n[[Category:Representation theory of finite groups]]\n[[Category:Theorems in abstract algebra]]\n\n\n{{Abstract-algebra-stub}}"
    }
  ]
}