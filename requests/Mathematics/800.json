{
  "pages": [
    {
      "title": "Birkhoff's representation theorem",
      "url": "https://en.wikipedia.org/wiki/Birkhoff%27s_representation_theorem",
      "text": "This is about lattice theory. For other similarly named results, see Birkhoff's theorem (disambiguation).\nIn mathematics, Birkhoff's representation theorem for distributive lattices states that the elements of any finite distributive lattice can be represented as finite sets, in such a way that the lattice operations correspond to unions and intersections of sets. The theorem can be interpreted as providing a one-to-one correspondence between distributive lattices and partial orders, between quasi-ordinal knowledge spaces and preorders, or between finite topological spaces and preorders. It is named after Garrett Birkhoff, who published a proof of it in 1937..\n\nThe name “Birkhoff's representation theorem” has also been applied to two other results of Birkhoff, one from 1935 on the representation of Boolean algebras as families of sets closed under union, intersection, and complement (so-called fields of sets, closely related to the rings of sets used by Birkhoff to represent distributive lattices), and Birkhoff's HSP theorem representing algebras as products of irreducible algebras. Birkhoff's representation theorem has also been called the fundamental theorem for finite distributive lattices..\n\n Understanding the theorem \nMany lattices can be defined in such a way that the elements of the lattice are represented by sets, the join operation of the lattice is represented by set union, and the meet operation of the lattice is represented by set intersection. For instance, the Boolean lattice defined from the family of all subsets of a finite set has this property. More generally any finite topological space has a lattice of sets as its family of open sets. Because set unions and intersections obey the distributive law, any lattice defined in this way is a distributive lattice. Birkhoff's theorem states that in fact all finite distributive lattices can be obtained this way, and later generalizations of Birkhoff's theorem state a similar thing for infinite distributive lattices.\n\nExamples\nthumb|upright=1.8|The distributive lattice of divisors of 120, and its representation as sets of prime powers.\nConsider the divisors of some composite number, such as (in the figure) 120,  partially ordered by divisibility. Any two divisors of 120, such as 12 and 20, have a unique greatest common factor 12 ∧ 20 = 4, the largest number that divides both of them, and a unique least common multiple 12 ∨ 20 = 60; both of these numbers are also divisors of 120. These two operations ∨ and ∧ satisfy the distributive law, in either of two equivalent forms: (x ∧ y) ∨ z = (x ∨ z) ∧ (y ∨ z) and (x ∨ y) ∧ z = (x ∧ z) ∨ (y ∧ z), for all x, y, and z. Therefore, the divisors form a finite distributive lattice.\n\nOne may associate each divisor with the set of prime powers that divide it: thus, 12 is associated with the set {2,3,4}, while 20 is associated with the set {2,4,5}. Then 12 ∧ 20 = 4 is associated with the set {2,3,4} ∩ {2,4,5} = {2,4}, while 12 ∨ 20 = 60 is associated with the set {2,3,4} ∪ {2,4,5} = {2,3,4,5}, so the join and meet operations of the lattice correspond to union and intersection of sets. \n\nThe prime powers 2, 3, 4, 5, and 8 appearing as elements in these sets may themselves be partially ordered by divisibility; in this smaller partial order, 2 ≤ 4 ≤ 8 and there are no order relations between other pairs. The 16 sets that are associated with divisors of 120 are the lower sets of this smaller partial order, subsets of elements such that if x ≤ y and y belongs to the subset, then x must also belong to the subset. From any lower set L, one can recover the associated divisor by computing the least common multiple of the prime powers in L. Thus, the partial order on the five prime powers 2, 3, 4, 5, and 8 carries enough information to recover the entire original 16-element divisibility lattice.\n\nBirkhoff's theorem states that this relation between the operations ∧ and ∨ of the lattice of divisors and the operations ∩ and ∪ of the associated sets of prime powers is not coincidental, and not dependent on the specific properties of prime numbers and divisibility: the elements of any finite distributive lattice may be associated with lower sets of a partial order in the same way.\n\nAs another example, the application of Birkhoff's theorem to the family of subsets of an n-element set, partially ordered by inclusion, produces the free distributive lattice with n generators. The number of elements in this lattice is given by the Dedekind numbers.\n\nThe partial order of join-irreducibles\nIn a lattice, an element x is join-irreducible if x is not the join of a finite set of other elements. Equivalently, x is join-irreducible if it is neither the bottom element of the lattice (the join of zero elements) nor the join of any two smaller elements. For instance, in the lattice of divisors of 120, there is no pair of elements whose join is 4, so 4 is join-irreducible. An element x is join-prime if, whenever x ≤ y ∨ z, either x ≤ y or x ≤ z. In the same lattice, 4 is join-prime: whenever lcm(y,z) is divisible by 4, at least one of y and z must itself be divisible by 4.\n\nIn any lattice, a join-prime element must be join-irreducible. Equivalently, an element that is not join-irreducible is not join-prime. For, if an element x is not join-irreducible, there exist smaller y and z such that x = y ∨ z. But then x ≤ y ∨ z, and x is not less than or equal to either y or z, showing that it is not join-prime.\n\nThere exist lattices in which the join-prime elements form a proper subset of the join-irreducible elements, but in a distributive lattice the two types of elements coincide. For, suppose that x is join-irreducible, and that x ≤ y ∨ z. This inequality is equivalent to the statement that x = x ∧ (y ∨ z), and by the distributive law x = (x ∧ y) ∨ (x ∧ z). But since x is join-irreducible, at least one of the two terms in this join must be x itself, showing that either x = x ∧ y (equivalently x ≤ y) or x = x ∧ z (equivalently x ≤ z).\n\nThe lattice ordering on the subset of join-irreducible elements forms a partial order; Birkhoff's theorem states that the lattice itself can be recovered from the lower sets of this partial order.\n\nBirkhoff's theorem\nthumb|upright=1.2|Distributive example lattice, with join-irreducible elements a,...,g (shadowed nodes). The lower set a node corresponds to by Birkhoff's isomorphism is shown in blue.\nIn any partial order, the lower sets form a lattice in which the lattice's partial ordering is given by set inclusion, the join operation corresponds to set union, and the meet operation corresponds to set intersection, because unions and intersections preserve the property of being a lower set. Because set unions and intersections obey the distributive law, this is a distributive lattice. Birkhoff's theorem states that any finite distributive lattice can be constructed in this way.\n\nTheorem. Any finite distributive lattice L is isomorphic to the lattice of lower sets of the partial order of the join-irreducible elements of L.\n\nThat is, there is a one-to-one order-preserving correspondence between elements of L and lower sets of the partial order. The lower set corresponding to an element x of L is simply the set of join-irreducible elements of L that are less than or equal to x, and the element of L corresponding to a lower set S of join-irreducible elements is the join of S.\n\nFor any lower set S of join-irreducible elements, let x be the join of S, and let T be the  lower set of the join-irreducible elements less than or equal to x. Then S = T. For, every element of S clearly belongs to T, and any join-irreducible element less than or equal to x must (by join-primality) be less than or equal to one of the members of S, and therefore must (by the assumption that S is a lower set) belong to S itself. Conversely, for any element x of L, let S be the join-irreducible elements less than or equal to x, and let y be the join of S. Then x = y. For, as a join of elements less than or equal to x, y can be no greater than x itself, but if x is join-irreducible then x belongs to S while if x is the join of two or more join-irreducible items then they must again belong to S, so y ≥ x. Therefore, the correspondence is one-to-one and the theorem is proved.\n\nRings of sets and preorders\n defined a ring of sets to be a family of sets that is closed under the operations of set unions and set intersections; later, motivated by applications in mathematical psychology,  called the same structure a quasi-ordinal knowledge space. If the sets in a ring of sets are ordered by inclusion, they form a distributive lattice. The elements of the sets may be given a preorder in which x ≤ y whenever some set in the ring contains x but not y. The ring of sets itself is then the family of lower sets of this preorder, and any preorder gives rise to a ring of sets in this way.\n\nFunctoriality\nBirkhoff's theorem, as stated above, is a correspondence between individual partial orders and distributive lattices. However, it can also be extended to a correspondence between order-preserving functions of partial orders and bounded homomorphisms of the corresponding distributive lattices. The direction of these maps is reversed in this correspondence.\n\nLet 2 denote the partial order on the two-element set {0, 1}, with the order relation 0 < 1, and (following Stanley) let J(P) denote the distributive lattice of lower sets of a finite partial order P. Then the elements of J(P) correspond one-for-one to the order-preserving functions from P to 2. For, if ƒ is such a function, ƒ−1(0) forms a lower set, and conversely if L is a lower set one may define an order-preserving function ƒL that maps L to 0 and that maps the remaining elements of P to 1. If g is any order-preserving function from Q to P, one may define a function g* from J(P) to J(Q) that uses the composition of functions to map any element L of J(P) to  ƒL ∘ g. This composite function maps Q to 2 and therefore corresponds to an element g*(L) = (ƒL ∘ g)−1(0) of J(Q). Further, for any x and y in J(P), g*(x ∧ y) = g*(x) ∧ g*(y) (an element of Q is mapped by g to the lower set x ∩ y if and only if belongs both to the set of elements mapped to x and the set of elements mapped to y) and symmetrically g*(x ∨ y) = g*(x) ∨ g*(y). Additionally, the bottom element of J(P) (the function that maps all elements of P to 0) is mapped by g* to the bottom element of J(Q), and the top element of J(P) is mapped by g* to the top element of J(Q). That is, g* is a homomorphism of bounded lattices.\n\nHowever, the elements of P themselves correspond one-for-one with bounded lattice homomorphisms from J(P) to 2. For, if x is any element of P, one may define a bounded lattice homomorphism jx that maps all lower sets containing x to 1 and all other lower sets to 0. And, for any lattice homomorphism from J(P) to 2, the elements of J(P) that are mapped to 1 must have a unique minimal element x (the meet of all elements mapped to 1), which must be join-irreducible (it cannot be the join of any set of elements mapped to 0), so every lattice homomorphism has the form jx for some x. Again, from any bounded lattice homomorphism h from J(P) to J(Q) one may use  composition of functions to define an order-preserving map h* from Q to P. It may be verified that g** = g for any order-preserving map g from Q to P and that and h** = h for any bounded lattice homomorphism h from J(P) to J(Q).\n\nIn category theoretic terminology, J is a contravariant hom-functor J = Hom(—,2) that defines a duality of categories between, on the one hand, the category of finite partial orders and order-preserving maps, and on the other hand the category of finite distributive lattices and bounded lattice homomorphisms.\n\nGeneralizations\nIn an infinite distributive lattice, it may not be the case that the lower sets of the join-irreducible elements are in one-to-one correspondence with lattice elements. Indeed, there may be no join-irreducibles at all. This happens, for instance, in the lattice of all natural numbers, ordered with the reverse of the usual divisibility ordering (so x ≤ y when y divides x): any number x can be expressed as the join of numbers xp and xq where p and q are distinct prime numbers. However, elements in infinite distributive lattices may still be represented as sets via Stone's representation theorem for distributive lattices, a form of Stone duality in which each lattice element corresponds to a compact open set in a certain topological space. This generalized representation theorem can be expressed as a category-theoretic duality between distributive lattices and spectral spaces (sometimes called coherent spaces, but not the same as the coherent spaces in linear logic), topological spaces in which the compact open sets are closed under intersection and form a base for the topology.. Hilary Priestley showed that Stone's representation theorem could be interpreted as an extension of the idea of representing lattice elements by lower sets of a partial order, using Nachbin's idea of ordered topological spaces. Stone spaces with an additional partial order linked with the topology via Priestley separation axiom can also be used to represent bounded distributive lattices. Such spaces are known as Priestley spaces. Further, certain bitopological spaces, namely pairwise Stone spaces, generalize Stone's original approach by utilizing two topologies on a set to represent an abstract distributive lattice. Thus, Birkhoff's representation theorem extends to the case of infinite (bounded) distributive lattices in at least three different ways, summed up in duality theory for distributive lattices.\n\nBirkhoff's representation theorem may also be generalized to finite structures other than distributive lattices. In a distributive lattice, the self-dual median operation.\n\ngives rise to a median algebra, and the covering relation of the lattice forms a median graph. Finite median algebras and median graphs have a dual structure\nas the set of solutions of a 2-satisfiability instance;  formulate this structure equivalently as the family of initial stable sets in a mixed graph.A minor difference between the 2-SAT and initial stable set formulations is that the latter presupposes the choice of a fixed base point from the median graph that corresponds to the empty initial stable set. For a distributive lattice, the corresponding mixed graph has no undirected edges, and the initial stable sets are just the lower sets of the transitive closure of the graph. Equivalently, for a distributive lattice, the implication graph of the 2-satisfiability instance can be partitioned into two connected components, one on the positive variables of the instance and the other on the negative variables; the transitive closure of the positive component is the underlying partial order of the distributive lattice.\n\nAnother result analogous to Birkhoff's representation theorem, but applying to a broader class of lattices, is the theorem of  that any finite join-distributive lattice may be represented as an antimatroid, a family of sets closed under unions but in which closure under intersections has been replaced by the property that each nonempty set has a removable element.\n\nNotes\n\nReferences\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\nCategory:Lattice theory\nCategory:Theorems in algebra"
    },
    {
      "title": "Boolean prime ideal theorem",
      "url": "https://en.wikipedia.org/wiki/Boolean_prime_ideal_theorem",
      "text": "In mathematics, the Boolean prime ideal theorem states that ideals in a Boolean algebra can be extended to  prime ideals.  A variation of this statement for filters on sets is known as the ultrafilter lemma.  Other theorems are obtained by considering different mathematical structures with appropriate notions of ideals, for example, rings and prime ideals (of ring theory), or distributive lattices and maximal ideals (of order theory).  This article focuses on prime ideal theorems from order theory.\n\nAlthough the various prime ideal theorems may appear simple and intuitive, they cannot be deduced in general from the axioms of Zermelo–Fraenkel set theory without the axiom of choice  (abbreviated ZF). Instead, some of the statements turn out to be equivalent to the axiom of choice (AC), while others—the Boolean prime ideal theorem, for instance—represent a property that is strictly weaker than AC. It is due to this intermediate status between ZF and ZF + AC (ZFC) that the Boolean prime ideal theorem is often taken as an axiom of set theory. The abbreviations BPI or PIT (for Boolean algebras) are sometimes used to refer to this additional axiom.\n\nPrime ideal theorems\nAn order ideal is a (non-empty) directed lower set. If the considered partially ordered set (poset) has binary suprema (a.k.a. joins), as do the posets within this article, then this is equivalently characterized as a non-empty lower set I that is closed for binary suprema (i.e. x, y in I imply xy in I). An ideal I is prime if its set-theoretic complement in the poset is a filter. Ideals are proper if they are not equal to the whole poset.\n\nHistorically, the first statement relating to later prime ideal theorems was in fact referring to filters—subsets that are ideals with respect to the dual order. The ultrafilter lemma states that every filter on a set is contained within some maximal (proper) filter—an ultrafilter. Recall that filters on sets are proper filters of the Boolean algebra of its powerset. In this special case, maximal filters (i.e. filters that are not strict subsets of any proper filter) and prime filters (i.e. filters that with each union of subsets X  and Y contain also X or Y) coincide. The dual of this statement thus assures that every ideal of a powerset is contained in a prime ideal.\n\nThe above statement led to various generalized prime ideal theorems, each of which exists in a weak and in a strong form. Weak prime ideal theorems state that every non-trivial algebra of a certain class has at least one prime ideal. In contrast, strong prime ideal theorems require that every ideal that is disjoint from a given filter can be extended to a prime ideal that is still disjoint from that filter. In the case of algebras that are not posets, one uses different substructures instead of filters. Many forms of these theorems are actually known to be equivalent, so that the assertion that \"PIT\" holds is usually taken as the assertion that the corresponding statement for Boolean algebras (BPI) is valid.\n\nAnother variation of similar theorems is obtained by replacing each occurrence of prime ideal by maximal ideal. The corresponding maximal ideal theorems (MIT) are often—though not always—stronger than their PIT equivalents.\n\n Boolean prime ideal theorem \nThe Boolean prime ideal theorem is the strong prime ideal theorem for Boolean algebras. Thus the formal statement is:\n\n Let B be a Boolean algebra, let I be an ideal and let F be a filter of B, such that I and F are disjoint. Then I is contained in some prime ideal of B that is disjoint from F.\n\nThe weak prime ideal theorem for Boolean algebras simply states:\n\n Every Boolean algebra contains a prime ideal.\n\nWe refer to these statements as the weak and strong BPI.  The two are equivalent, as the strong BPI clearly implies the weak BPI, and the reverse implication can be achieved by using the weak BPI to find prime ideals in the appropriate quotient algebra.\n\nThe BPI can be expressed in various ways. For this purpose, recall the following theorem:\n\nFor any ideal I of a Boolean algebra B, the following are equivalent:\n I is a prime ideal.\n I is a maximal ideal, i.e. for any proper ideal J, if I is contained in J then I = J.\n For every element a of B, I contains exactly one of {a, ¬a}.\nThis theorem is a well-known fact for Boolean algebras. Its dual establishes the equivalence of prime filters and ultrafilters. Note that the last property is in fact self-dual—only the prior assumption that I is an ideal gives the full characterization. All of the implications within this theorem can be proven in ZF.\n\nThus the following (strong) maximal ideal theorem (MIT) for Boolean algebras is equivalent to BPI:\n\nLet B be a Boolean algebra, let I be an ideal and let F be a filter of B, such that I and F are disjoint. Then I is contained in some maximal ideal of B that is disjoint from F.\n\nNote that one requires \"global\" maximality, not just maximality with respect to being disjoint from F. Yet, this variation yields another equivalent characterization of BPI:\n\nLet B be a Boolean algebra, let I be an ideal and let F be a filter of B, such that I and F are disjoint. Then I is contained in some ideal of B that is maximal among all ideals disjoint from F.\n\nThe fact that this statement is equivalent to BPI is easily established by noting the following theorem: For any distributive lattice L, if an ideal I is maximal among all ideals of L that are disjoint to a given filter F, then I is a prime ideal. The proof for this statement (which can again be carried out in ZF set theory) is included in the article on ideals. Since any Boolean algebra is a distributive lattice, this shows the desired implication.\n\nAll of the above statements are now easily seen to be equivalent. Going even further, one can exploit the fact the dual orders of Boolean algebras are exactly the Boolean algebras themselves. Hence, when taking the equivalent duals of all former statements, one ends up with a number of theorems that equally apply to Boolean algebras, but where every occurrence of ideal is replaced by filter. It is worth noting that for the special case where the Boolean algebra under consideration is a powerset with the subset ordering, the \"maximal filter theorem\" is called the ultrafilter lemma.\n\nSumming up, for Boolean algebras, the weak and strong MIT, the weak and strong PIT, and these statements with filters in place of ideals are all equivalent. It is known that all of these statements are consequences of the Axiom of Choice, AC, (the easy proof makes use of Zorn's lemma), but cannot be proven in ZF (Zermelo-Fraenkel set theory without AC), if ZF is consistent. Yet, the BPI is strictly weaker than the axiom of choice, though the proof of this statement, due to  J. D. Halpern and Azriel Lévy is rather non-trivial.\n\n Further prime ideal theorems \nThe prototypical properties that were discussed for Boolean algebras in the above section can easily be modified to include more general lattices, such as distributive lattices or Heyting algebras. However, in these cases maximal ideals are different from prime ideals, and the relation between PITs and MITs is not obvious.\n\nIndeed, it turns out that the MITs for distributive lattices and even for Heyting algebras are equivalent to the axiom of choice. On the other hand, it is known that the strong PIT for distributive lattices is equivalent to BPI (i.e. to the MIT and PIT for Boolean algebras). Hence this statement is strictly weaker than the axiom of choice. Furthermore, observe that Heyting algebras are not self dual, and thus using filters in place of ideals yields different theorems in this setting. Maybe surprisingly, the MIT for the duals of Heyting algebras is not stronger than BPI, which is in sharp contrast to the abovementioned MIT for Heyting algebras.\n\nFinally, prime ideal theorems do also exist for other (not order-theoretical) abstract algebras. For example, the MIT for rings implies the axiom of choice. This situation requires to replace the order-theoretic term \"filter\" by other concepts—for rings a \"multiplicatively closed subset\" is appropriate.\n\n The ultrafilter lemma \nA filter on a set X is a nonempty collection of nonempty subsets of X that is closed under finite intersection and under superset.  An ultrafilter is a maximal filter.   The ultrafilter lemma states that every filter  on a set X is a subset of some ultrafilter on X..  This lemma is most often used in the study of topology.  An ultrafilter that does not contain finite sets is called \"non-principal\".The ultrafilter lemma, and in particular the existence of non-principal ultrafilters (consider the filter of all sets with finite complements), follows easily from Zorn's lemma.\n\nThe ultrafilter lemma is equivalent to the Boolean prime ideal theorem, with the equivalence provable in ZF set theory without the axiom of choice.  The idea behind the proof is that the subsets of any set form a Boolean algebra partially ordered by inclusion, and any Boolean algebra is representable as an algebra of sets by Stone's representation theorem.\n\n Applications \nIntuitively, the Boolean prime ideal theorem states that there are \"enough\" prime ideals in a Boolean algebra in the sense that we can extend every ideal to a maximal one. This is of practical importance for proving Stone's representation theorem for Boolean algebras, a special case of Stone duality, in which one equips the set of all prime ideals with a certain topology and can indeed regain the original Boolean algebra (up to isomorphism) from this data. Furthermore, it turns out that in applications one can freely choose either to work with prime ideals or with prime filters, because every ideal uniquely determines a filter: the set of all Boolean complements of its elements. Both approaches are found in the literature.\n\nMany other theorems of general topology that are often said to rely on the axiom of choice are in fact equivalent to BPI. For example, the theorem that a product of compact Hausdorff spaces is compact is equivalent to it. If we leave out \"Hausdorff\" we get a theorem equivalent to the full axiom of choice.\n\nIn graph theory, the de Bruijn–Erdős theorem is another equivalent to BPI. It states that, if a given infinite graph requires at least some finite number  in any graph coloring, then it has a finite subgraph that also requires ..\n\nA not too well known application of the Boolean prime ideal theorem is the existence of a non-measurable set (the example usually given is the Vitali set, which requires the axiom of choice).  From this and the fact that the BPI is strictly weaker than the axiom of choice, it follows that the existence of non-measurable sets is strictly weaker than the axiom of choice.\n\nIn linear algebra, the Boolean prime ideal theorem can be used to prove that any two bases of a given vector space have the same cardinality.\n\nSee also\n List of Boolean algebra topics\n\nNotes\n\nReferences\n\n.\n An easy to read introduction, showing the equivalence of PIT for Boolean algebras and distributive lattices.\n\n.\n The theory in this book often requires choice principles. The notes on various chapters discuss the general relation of the theorems to PIT and MIT for various structures (though mostly lattices) and give pointers to further literature.\n\n.\n Discusses the status of the ultrafilter lemma.\n\n.\n Gives many equivalent statements for the BPI, including prime ideal theorems for other algebraic structures. PITs are considered as special instances of separation lemmas.\n\nCategory:Theorems in algebra\nCategory:Boolean algebra\nCategory:Order theory\nCategory:Axiom of choice"
    },
    {
      "title": "Cartan–Brauer–Hua theorem",
      "url": "https://en.wikipedia.org/wiki/Cartan%E2%80%93Brauer%E2%80%93Hua_theorem",
      "text": "In abstract algebra, the Cartan–Brauer–Hua theorem (named after Richard Brauer, Élie Cartan, and Hua Luogeng) is a theorem pertaining to division rings. It says that given two division rings  such that xKx−1 is contained in K for every x not equal to 0 in D, either K is contained in the center of D, or .  In other words, if the unit group of K is a normal subgroup of the unit group of D, then either  or K is central .\n\nReferences\n\n \n\nCategory:Ring theory\nCategory:Theorems in algebra"
    },
    {
      "title": "Chevalley–Warning theorem",
      "url": "https://en.wikipedia.org/wiki/Chevalley%E2%80%93Warning_theorem",
      "text": "In number theory, the  Chevalley–Warning theorem implies that certain polynomial equations in sufficiently many variables over a finite field have solutions. It was proved by  and a slightly weaker form of the theorem, known as Chevalley's theorem, was proved by . Chevalley's theorem implied Artin's and Dickson's conjecture that finite fields are quasi-algebraically closed fields .\n\n Statement of the theorems \n\nLet  be a finite field and  be a set of polynomials such that the number of variables satisfies \n\nwhere  is the total degree of . The theorems are statements about the solutions of the following system of polynomial equations\n\n Chevalley–Warning theorem states that the number of common solutions  is divisible by the characteristic  of . Or in other words, the cardinality of the vanishing set of  is  modulo .\n Chevalley's theorem states that if the system has the trivial solution , i.e. if the polynomials have no constant terms, then the system also has a non-trivial solution . \n\nChevalley's theorem is an immediate consequence of the Chevalley–Warning theorem since  is at least 2. \n\nBoth theorems are best possible in the sense that, given any , the list  has total degree  and only the trivial solution. Alternatively, using just one polynomial, we can take f1 to be the degree n polynomial given by the norm of x1a1 + ... + xnan where the elements a form a basis of the finite field of order pn.\n\nWarning proved another theorem, known as Warning's second theorem, which states that if the system of polynomial equations has the trivial solution, then it has at least  solutions where  is the size of the finite field and . Chevalley's theorem also follows directly from this.\n\nProof of Warning's theorem\nRemark: If  then \n\nso the sum over  of any polynomial in  of degree less than  also vanishes.\n\nThe total number of common solutions modulo  of  is equal to\n\nbecause each term is 1 for a solution and 0 otherwise.\nIf the sum of the degrees of the polynomials  is less than n then this vanishes by the remark above.\n\n Artin's conjecture \n\nIt is a consequence of Chevalley's theorem that finite fields are quasi-algebraically closed. This had been conjectured by Emil Artin in 1935. The motivation behind Artin's conjecture was his observation that quasi-algebraically closed fields have trivial Brauer group, together with the fact that finite fields have trivial Brauer group by Wedderburn's theorem.\n\n The Ax–Katz theorem \n\nThe Ax–Katz theorem, named after James Ax and Nicholas Katz, determines more accurately a power  of the cardinality  of  dividing the number of solutions; here, if  is the largest of the , then the exponent  can be taken as the ceiling function of \n\n \n\nThe Ax–Katz result has an interpretation in étale cohomology as a divisibility result for the (reciprocals of) the zeroes and poles of the local zeta-function. Namely, the same power of  divides each of these algebraic integers.\n\n See also \n Combinatorial Nullstellensatz\n\nReferences\n\nExternal links\n\nCategory:Finite fields\nCategory:Diophantine geometry\nCategory:Theorems in algebra"
    },
    {
      "title": "Classification of finite simple groups",
      "url": "https://en.wikipedia.org/wiki/Classification_of_finite_simple_groups",
      "text": "In mathematics, the classification of the finite simple groups is a theorem stating that every finite simple group belongs to one of four broad classes described below. These groups can be seen as the basic building blocks of all finite groups, in a way reminiscent of the way the prime numbers are the basic building blocks of the natural numbers.  The Jordan–Hölder theorem is a more precise way of stating this fact about finite groups. However, a significant difference from  integer factorization is that such \"building blocks\" do not necessarily determine  a unique group, since there might be many non-isomorphic groups with the same composition series or, put in another way, the extension problem does not have a unique solution.\n\nGroup theory is central to many areas of pure and applied mathematics and the classification theorem is one of the great achievements of modern mathematics. The proof consists of tens of thousands of pages in several hundred journal articles written by about 100 authors, published mostly between 1955 and 2004. Gorenstein (d.1992), Lyons, and Solomon are gradually publishing a simplified and revised version of the proof.\n\nStatement of the classification theorem\n\nThe classification theorem has applications in many branches of mathematics, as questions about the structure of finite groups (and their action on other mathematical objects) can sometimes be reduced to questions about finite simple groups. Thanks to the classification theorem, such questions can sometimes be answered by checking each family of simple groups and each sporadic group.\n\nDaniel Gorenstein announced in 1983 that the finite simple groups had all been classified, but this was premature as he had been misinformed about the proof of the classification of quasithin groups. The completed proof of the classification was announced by  after Aschbacher and Smith published a 1221-page proof for the missing quasithin case.\n\nOverview of the proof of the classification theorem\n wrote two volumes outlining the low rank and odd characteristic part of the proof, and \nwrote a 3rd volume covering the remaining characteristic 2 case. The proof can be broken up into several major pieces as follows:\n\nGroups of small 2-rank\nThe simple groups of low 2-rank are mostly groups of Lie type of small rank over fields of odd characteristic, together with five alternating and seven characteristic 2 type and nine sporadic groups.\n\nThe simple groups of small 2-rank include:\nGroups of 2-rank 0, in other words groups of odd order, which are all solvable by the Feit–Thompson theorem.\nGroups of 2-rank 1. The Sylow 2-subgroups are either cyclic, which is easy to handle using the transfer map, or generalized quaternion, which are handled with the Brauer–Suzuki theorem: in particular there are no simple groups of 2-rank 1.\nGroups of 2-rank 2. Alperin showed that the Sylow subgroup must be dihedral, quasidihedral, wreathed, or a Sylow 2-subgroup of U3(4). The first case was done by the Gorenstein–Walter theorem which showed that the only simple groups are isomorphic to L2(q) for q odd or A7, the second and third cases were done by the Alperin–Brauer–Gorenstein theorem which implies that the only simple groups are isomorphic to L3(q) or U3(q) for q odd or M11, and the last case was done by Lyons who showed that U3(4) is the only simple possibility.\nGroups of sectional 2-rank at most 4, classified by the Gorenstein–Harada theorem.\nThe classification of groups of small 2-rank, especially ranks at most 2, makes heavy use of ordinary and modular character theory, which is almost never directly used elsewhere in the classification.\n\nAll groups not of small 2 rank can be split into two major classes: groups of component type and groups of characteristic 2 type. This is because if a group has sectional 2-rank at least 5 then MacWilliams showed that its Sylow 2-subgroups are connected, and the balance theorem implies that any simple group with connected Sylow 2-subgroups is either of component type or characteristic 2 type. (For groups of low 2-rank the proof of this breaks down, because theorems such as the signalizer functor theorem only work for groups with elementary abelian subgroups of rank at least 3.)\n\nGroups of component type\nA group is said to be of component type if for some centralizer C of an involution, C/O(C) has a component (where O(C) is the core of C, the maximal normal subgroup of odd order).\nThese are more or less the groups of Lie type of odd characteristic of large rank, and alternating groups, together with some sporadic groups.\nA major step in this case is to eliminate the obstruction of the core of an involution. This is accomplished by the B-theorem, which states that every component of C/O(C) is the image of a component of C.\n\nThe idea is that these groups have a centralizer of an involution with a component that is a smaller quasisimple group, which can be assumed to be already known by induction. So to classify these groups one takes every central extension of every known finite simple group, and finds all simple groups with a centralizer of involution with this as a component. This gives a rather large number of different cases to check: there are not only 26 sporadic groups and 16 families of groups of Lie type and the alternating groups, but also many of the groups of small rank or over small fields behave differently from the general case and have to be treated separately, and the groups of Lie type of even and odd characteristic are also quite different.\n\nGroups of characteristic 2 type\n\nA group is of characteristic 2 type if the generalized Fitting subgroup F*(Y) of every 2-local subgroup Y is a 2-group.\nAs the name suggests these are roughly the groups of Lie type over fields of characteristic 2, plus a handful of others that are alternating or sporadic or of odd characteristic. Their classification is divided into the small and large rank cases, where the rank is the largest rank of an odd abelian subgroup normalizing a nontrivial 2-subgroup, which is often (but not always) the same as the rank of a Cartan subalgebra when the group is a group of Lie type in characteristic 2.\n\nThe rank 1 groups are the thin groups, classified by Aschbacher, and the rank 2 ones are the notorious quasithin groups, classified by Aschbacher and Smith. These correspond roughly to groups of Lie type of ranks 1 or 2 over fields of characteristic 2.\n\nGroups of rank at least 3 are further subdivided into 3 classes by the trichotomy theorem, proved by Aschbacher for rank 3 and by Gorenstein and Lyons for rank at least 4.\nThe three classes are groups of GF(2) type (classified mainly by Timmesfeld), groups of \"standard type\" for some odd prime (classified by the Gilman–Griess theorem and work by several others), and groups of uniqueness type, where a result of Aschbacher implies that there are no simple groups.\nThe general higher rank case consists mostly of the groups of Lie type over fields of characteristic 2 of rank at least 3 or 4.\n\nExistence and uniqueness of the simple groups\n\nThe main part of the classification produces a characterization of each simple group. It is then necessary to check that there exists a simple group for each characterization and that it is unique. This gives a large number of separate problems; for example, the original proofs of existence and uniqueness of the monster group totaled about 200 pages, and the identification of the Ree groups by Thompson and Bombieri was one of the hardest parts of the classification. Many of the existence proofs and some of the uniqueness proofs for the sporadic groups originally used computer calculations, most of which have since been replaced by shorter hand proofs.\n\nHistory of the proof\n\nGorenstein's program\nIn 1972  announced a program for completing the classification of finite simple groups, consisting of the following 16 steps:\n Groups of low 2-rank. This was essentially done by Gorenstein and Harada, who classified the groups with sectional 2-rank at most 4. Most of the cases of 2-rank at most 2 had been done by the time Gorenstein announced his program.\n The semisimplicity of 2-layers. The problem is to prove that the 2-layer of the centralizer of an involution in a simple group is semisimple.\n Standard form in odd characteristic. If a group has an involution with a 2-component that is a group of Lie type of odd characteristic, the goal is to show that it has a centralizer of involution in \"standard form\" meaning that a centralizer of involution has a component that is of Lie type in odd characteristic and also has a centralizer of 2-rank 1.\n Classification of groups of odd type. The problem is to show that if a group has a centralizer of involution in \"standard form\" then it is a group of Lie type of odd characteristic. This was solved by Aschbacher's classical involution theorem.\n Quasi-standard form\n Central involutions\n Classification of alternating groups.\n Some sporadic groups\n Thin groups. The simple thin finite groups, those with 2-local p-rank at most 1 for odd primes p, were classified by Aschbacher in 1978\n Groups with a strongly p-embedded subgroup for p odd\n The signalizer functor method for odd primes. The main problem is to prove a signalizer functor theorem for nonsolvable signalizer functors. This was solved by McBride in 1982.\n Groups of characteristic p type. This is the problem of groups with a strongly p-embedded 2-local subgroup with p odd, which was handled by Aschbacher.\n Quasithin groups. A quasithin group is one whose 2-local subgroups have p-rank at most 2 for all odd primes p, and the problem is to classify the simple ones of characteristic 2 type. This was completed by Aschbacher and Smith in 2004.\n Groups of low 2-local 3-rank. This was essentially solved by Aschbacher's trichotomy theorem for groups with e(G)=3. The main change is that 2-local 3-rank is replaced by 2-local p-rank for odd primes.\n Centralizers of 3-elements in standard form. This was essentially done by the Trichotomy theorem.\n Classification of simple groups of characteristic 2 type. This was handled by the Gilman–Griess theorem, with 3-elements replaced by p-elements for odd primes.\n\nTimeline of the proof\n\nMany of the items in the list below are taken from . The date given is usually the publication date of the complete proof of a result, which is sometimes several years later than the proof or first announcement of the result, so some of the items appear in the \"wrong\" order.\nPublication date1832Galois introduces normal subgroups and finds the simple groups An (n ≥ 5) and PSL2(Fp) (p ≥ 5)1854Cayley defines abstract groups1861Mathieu describes the first two Mathieu groups M11, M12, the first sporadic simple groups, and announces the existence of M24.1870Jordan lists some simple groups: the alternating and projective special linear ones, and emphasizes the importance of the simple groups.1872Sylow proves the Sylow theorems1873Mathieu introduces three more Mathieu groups M22, M23, M24.1892Otto Hölder proves that the order of any nonabelian finite simple group must be a product of at least four (not necessarily distinct) primes, and asks for a classification of finite simple groups.1893Cole classifies simple groups of order up to 6601896Frobenius and Burnside begin the study of character theory of finite groups.1899Burnside classifies the simple groups such that the centralizer of every involution is a non-trivial elementary abelian 2-group.1901Frobenius proves that a Frobenius group has a Frobenius kernel, so in particular is not simple.1901Dickson defines classical groups over arbitrary finite fields, and exceptional groups of type G2 over fields of odd characteristic.1901Dickson introduces the exceptional finite simple groups of type E6.1904Burnside uses character theory to prove Burnside's theorem that the order of any non-abelian finite simple group must be divisible by at least 3 distinct primes.1905Dickson introduces simple groups of type G2 over fields of even characteristic 1911Burnside conjectures that every non-abelian finite simple group has even order1928Philip Hall proves the existence of Hall subgroups of solvable groups1933Hall begins his study of p-groups1935Brauer begins the study of modular characters.1936Zassenhaus classifies finite sharply 3-transitive permutation groups1938Fitting introduces the Fitting subgroup and proves Fitting's theorem that for solvable groups the Fitting subgroup contains its centralizer.1942Brauer describes the modular characters of a group divisible by a prime to the first power.1954Brauer classifies simple groups with GL2(Fq) as the centralizer of an involution.1955The Brauer–Fowler theorem implies that the number of finite simple groups with given centralizer of involution is finite, suggesting an attack on the classification using centralizers of involutions.1955Chevalley introduces the Chevalley groups, in particular introducing exceptional simple groups of types F4, E7, and E8.1956Hall–Higman theorem1957 Suzuki shows that all finite simple CA groups of odd order are cyclic.1958The Brauer–Suzuki–Wall theorem characterizes the projective special linear groups of rank 1, and classifies the simple CA groups.1959Steinberg introduces the Steinberg groups, giving some new finite simple groups, of types 3D4 and 2E6 (the latter were independently found at about the same time by Jacques Tits).1959The Brauer–Suzuki theorem about groups with generalized quaternion Sylow 2-subgroups shows in particular that none of them are simple.1960Thompson proves that a group with a fixed-point-free automorphism of prime order is nilpotent.1960 Feit, Marshall Hall, and Thompson show that all finite simple CN groups of odd order are cyclic.1960Suzuki introduces the Suzuki groups, with types 2B2.1961Ree introduces the Ree groups, with types 2F4 and  2G2. 1963 Feit and Thompson prove the odd order theorem.1964Tits introduces BN pairs for groups of Lie type and finds the Tits group1965The Gorenstein–Walter theorem classifies groups with a dihedral Sylow 2-subgroup.1966Glauberman proves the Z* theorem1966Janko introduces the Janko group J1, the first new sporadic group for about a century.1968Glauberman proves the ZJ theorem1968Higman and Sims introduce the Higman–Sims group1968Conway introduces the Conway groups1969Walter's theorem classifies groups with abelian Sylow 2-subgroups1969Introduction of the Suzuki sporadic group, the Janko group J2, the Janko group J3, the McLaughlin group, and the Held group.1969Gorenstein introduces signalizer functors based on Thompson's ideas.1970MacWilliams shows that the 2-groups with no normal abelian subgroup of rank 3 have sectional 2-rank at most 4. (The simple groups with Sylow subgroups satisfying the latter condition were later classified by Gorenstein and Harada.)1970Bender introduced the generalized Fitting subgroup1970The Alperin–Brauer–Gorenstein theorem classifies groups with quasi-dihedral or wreathed Sylow 2-subgroups, completing the classification of the simple groups of 2-rank at most 21971Fischer introduces the three Fischer groups1971Thompson classifies quadratic pairs1971Bender classifies group with a strongly embedded subgroup1972Gorenstein proposes a 16-step program for classifying finite simple groups; the final classification follows his outline quite closely.1972Lyons introduces the Lyons group1973Rudvalis introduces the Rudvalis group1973Fischer discovers the baby monster group (unpublished), which Fischer and Griess use to discover the monster group, which in turn leads Thompson to the Thompson sporadic group and Norton to the Harada–Norton group (also found in a different way by Harada).1974Thompson classifies N-groups, groups all of whose local subgroups are solvable.1974The Gorenstein–Harada theorem classifies the simple groups of sectional 2-rank at most 4, dividing the remaining finite simple groups into those of component type and those of characteristic 2 type.1974Tits shows that groups with BN pairs of rank at least 3 are groups of Lie type1974Aschbacher classifies the groups with a proper 2-generated core1975Gorenstein and Walter prove the L-balance theorem1976Glauberman proves the solvable signalizer functor theorem1976Aschbacher proves the component theorem, showing roughly that groups of odd type satisfying some conditions have a component in standard form. The groups with a component of standard form were classified in a large collection of papers by many authors.1976O'Nan introduces the O'Nan group1976Janko introduces the Janko group J4, the last sporadic group to be discovered1977Aschbacher characterizes the groups of Lie type of odd characteristic in his classical involution theorem. After this theorem, which in some sense deals with \"most\" of the simple groups, it was generally felt that the end of the classification was in sight.1978Timmesfeld proves the O2 extraspecial theorem, breaking the classification of groups of GF(2)-type into several smaller problems.1978Aschbacher classifies the thin finite groups, which are mostly rank 1 groups of Lie type over fields of even characteristic.1981Bombieri uses elimination theory to complete Thompson's work on the characterization of Ree groups, one of the hardest steps of the classification.1982McBride proves the signalizer functor theorem for all finite groups.1982Griess constructs the monster group by hand1983 The Gilman–Griess theorem classifies groups of characteristic 2 type and rank at least 4 with standard components, one of the three cases of the trichotomy theorem.1983Aschbacher proves that no finite group satisfies the hypothesis of the uniqueness case, one of the three cases given by the trichotomy theorem for groups of characteristic 2 type.1983Gorenstein and Lyons prove the trichotomy theorem for groups of characteristic 2 type and rank at least 4, while Aschbacher does the case of rank 3. This divides these groups into 3 subcases: the uniqueness case, groups of GF(2) type, and groups with a standard component. 1983 Gorenstein announces the proof of the classification is complete, somewhat prematurely as the proof of the quasithin case was incomplete.1994 Gorenstein, Lyons, and Solomon begin publication of the revised classification2004Aschbacher and Smith publish their work on quasithin groups (which are mostly groups of Lie type of rank at most 2 over fields of even characteristic), filling the last gap in the classification known at that time.2008Harada and Solomon fill a minor gap in the classification by describing groups with a standard component that is a cover of the Mathieu group M22, a case that was accidentally omitted from the proof of the classification due to an error in the calculation of the Schur multiplier of M22.2012Georges Gonthier and collaborators announce a computer-checked version of the Feit–Thompson theorem using the Coq proof assistant.\n\nSecond-generation classification\nThe proof of the theorem, as it stood around 1985 or so, can be called first generation. Because of the extreme length of the first generation proof, much effort has been devoted to finding a simpler proof, called a second-generation classification proof. This effort, called \"revisionism\", was originally led by Daniel Gorenstein.\n\n, eight volumes of the second generation proof have been published (Gorenstein, Lyons & Solomon 1994, 1996, 1998, 1999, 2002, 2005, 2018a, 2018b). In 2012 Solomon estimated that the project would need another 5 volumes, but said that progress on them was slow. It is estimated that the new proof will eventually fill approximately 5,000 pages. (This length stems in part from the second generation proof being written in a more relaxed style.) Aschbacher and Smith wrote their two volumes devoted to the quasithin case in such a way that those volumes can be part of the second generation proof.\n\nGorenstein and his collaborators have given several reasons why a simpler proof is possible.\n The most important is that the correct, final statement of the theorem is now known. Simpler techniques can be applied that are known to be adequate for the types of groups we know to be finite simple. In contrast, those who worked on the first generation proof did not know how many sporadic groups there were, and in fact some of the sporadic groups (e.g., the Janko groups) were discovered while proving other cases of the classification theorem. As a result, many of the pieces of the theorem were proved using techniques that were overly general.\nBecause the conclusion was unknown, the first generation proof consists of many stand-alone theorems, dealing with important special cases. Much of the work of proving these theorems was devoted to the analysis of numerous special cases. Given a larger, orchestrated proof, dealing with many of these special cases can be postponed until the most powerful assumptions can be applied. The price paid under this revised strategy is that these first generation theorems no longer have comparatively short proofs, but instead rely on the complete classification.\nMany first generation theorems overlap, and so divide the possible cases in inefficient ways. As a result, families and subfamilies of finite simple groups were identified multiple times. The revised proof eliminates these redundancies by relying on a different subdivision of cases.\nFinite group theorists have more experience at this sort of exercise, and have new techniques at their disposal.\n\n has called the work on the classification problem by Ulrich Meierfrankenfeld, Bernd Stellmacher, Gernot Stroth, and a few others, a third generation program. One goal of this is to treat all groups in characteristic 2 uniformly using the amalgam method.\n\nWhy is the proof so long?\n\nGorenstein has discussed some of the reasons why there might not be a short proof of the classification similar to the classification of compact Lie groups.\n\nThe most obvious reason is that the list of simple groups is quite complicated: with 26 sporadic groups there are likely to be many special cases that have to be considered in any proof. So far no one has yet found a clean uniform description of the finite simple groups similar to the parameterization of the compact Lie groups by Dynkin diagrams.\nAtiyah and others have suggested that the classification ought to be simplified by constructing some geometric object that the groups act on and then classifying these geometric structures. The problem is that no one has been able to suggest an easy way to find such a geometric structure associated to a simple group. In some sense the classification does work by finding geometric structures such as BN-pairs, but this only comes at the end of a very long and difficult analysis of the structure of a finite simple group.\nAnother suggestion for simplifying the proof is to make greater use of representation theory. The problem here is that representation theory seems to require very tight control over the subgroups of a group in order to work well. For groups of small rank one has such control and representation theory works very well, but for groups of larger rank no-one has succeeded in using it to simplify the classification. In the early days of the classification there was considerable effort made to use representation theory, but this never achieved much success in the higher rank case.\n\nConsequences of the classification\n\nThis section lists some results that have been proved using the classification of finite simple groups.\n\nThe Schreier conjecture\nThe Signalizer functor theorem\nThe B conjecture\nThe Schur–Zassenhaus theorem for all groups (though this only uses the Feit–Thompson theorem).\nA transitive permutation group on a finite set with more than 1 element has a fixed-point-free element of prime power order.\nThe classification of 2-transitive permutation groups.\nThe classification of rank 3 permutation groups.\nThe Sims conjecture\nFrobenius's conjecture on the number of solutions of .\n\nSee also\n\nO'Nan–Scott theorem\n\nReferences\n\n \n\n \n\n Daniel Gorenstein (1985), \"The Enormous Theorem\", Scientific American, December 1, 1985, vol. 253, no. 6, pp. 104–115.\n\n Mark Ronan, Symmetry and the Monster, , Oxford University Press, 2006. (Concise introduction for lay reader)\nMarcus du Sautoy, Finding Moonshine, Fourth Estate, 2008,  (another introduction for the lay reader)\n Ron Solomon (1995) \"On Finite Simple Groups and their Classification,\" Notices of the American Mathematical Society. (Not too technical and good on history)\n  – article won Levi L. Conant prize for exposition\n\nExternal links\n ATLAS of Finite Group Representations. Searchable database of representations and other data for many finite simple groups.\n Elwes, Richard, \"An enormous theorem: the classification of finite simple groups,\" Plus Magazine, Issue 41, December 2006. For laypeople.\n Madore, David (2003) Orders of nonabelian simple groups. Includes a list of all nonabelian simple groups up to order 1010.\n http://mathoverflow.net/questions/180355/in-what-sense-is-the-classification-of-all-finite-groups-impossible\n\nCategory:Group theory\n*\nCategory:Finite groups\nCategory:Theorems in algebra\nCategory:2004 in science\nCategory:History of mathematics\nCategory:Classification systems"
    },
    {
      "title": "Cohn's irreducibility criterion",
      "url": "https://en.wikipedia.org/wiki/Cohn%27s_irreducibility_criterion",
      "text": "Arthur Cohn's irreducibility criterion is a sufficient condition for a polynomial to be irreducible in —that is, for it to be unfactorable into the product of lower-degree polynomials with integer coefficients.\n\nThe criterion is often stated as follows:\nIf a prime number  is expressed in base 10 as  (where ) then the polynomial\n\nis irreducible in .\n\nThe theorem can be generalized to other bases as follows:\nAssume that  is a natural number and  is a polynomial such that . If  is a prime number then  is irreducible in .\n\nThe base-10 version of the theorem is attributed to Cohn by Pólya and Szegő in one of their books English translation in:  while the generalization to any baseb is due to Brillhart, Filaseta, and Odlyzko.\n\nIn 2002, Ram Murty gave a simplified proof as well as some history of the theorem in a paper that is available online. (dvi file)\n\nThe converse of this criterion is that, if p is an irreducible polynomial with integer coefficients that have greatest common divisor 1, then there exists a base such that the coefficients of p form the representation of a prime number in that base; this is the Bunyakovsky conjecture and its truth or falsity remains an open question.\n\nHistorical notes\nPolya and Szegő gave their own generalization but it has many side conditions (on the locations of the roots, for instance) so it lacks the elegance of Brillhart's, Filaseta's, and Odlyzko's generalization.\nIt is clear from context that the \"A. Cohn\" mentioned by Polya and Szegő is Arthur Cohn (1894–1940), a student of Issai Schur who was awarded his doctorate from Frederick William University in 1921.Arthur Cohn's entry at the Mathematics Genealogy Project\n\nSee also\n\nEisenstein's criterion\n\nReferences\n\nExternal links\n\nCategory:Polynomials\nCategory:Theorems in algebra"
    },
    {
      "title": "Complex conjugate root theorem",
      "url": "https://en.wikipedia.org/wiki/Complex_conjugate_root_theorem",
      "text": "In mathematics, the complex conjugate root theorem states that if P is a polynomial in one variable with real coefficients, and a + bi is a root of P with a and b real numbers, then its complex conjugate a − bi is also a root of P. Preview available at Google books\n\nIt follows from this (and the fundamental theorem of algebra), that if the degree of a real polynomial is odd, it must have at least one real root.  That fact can also be proven by using the intermediate value theorem.\n\n Examples and consequences \n The polynomial x2 + 1 = 0 has roots ±i.\n Any real square matrix of odd degree has at least one real eigenvalue. For example, if the matrix is orthogonal, then 1 or −1 is an eigenvalue.\n The polynomial\n\nhas roots\n\nand thus can be factored as\n\nIn computing the product of the last two factors, the imaginary parts cancel, and we get\n\nThe non-real factors come in pairs which when multiplied give quadratic polynomials with real coefficients. Since every polynomial with complex coefficients can be factored into 1st-degree factors (that is one way of stating the fundamental theorem of algebra), it follows that every polynomial with real coefficients can be factored into factors of degree no higher than 2: just 1st-degree and quadratic factors.\n If the roots are  and , they form a quadratic\n.\nIf the third root is , this becomes\n\n.\n\n Corollary on odd-degree polynomials \nIt follows from the present theorem and the fundamental theorem of algebra that if the degree of a real polynomial is odd, it must have at least one real root.\n\nThis can be proved as follows.\nSince non-real complex roots come in conjugate pairs, there are an even number of them;\nBut a polynomial of odd degree has an odd number of roots;\nTherefore some of them must be real.\n \nThis requires some care in the presence of multiple roots; but a complex root and its conjugate do have the same multiplicity (and this lemma is not hard to prove). It can also be worked around by considering only irreducible polynomials; any real polynomial of odd degree must have an irreducible factor of odd degree, which (having no multiple roots) must have a real root by the reasoning above.\n\nThis corollary can also be proved directly by using the intermediate value theorem.\n\n Proof \nOne proof of the theorem is as follows:\n\nConsider the polynomial\n\n \n\nwhere all ar are real.  Suppose some complex number ζ is a root of P, that is P(ζ) = 0. It needs to be shown that \n \n\nas well.\n\nIf P(ζ) = 0, then\n\n \n\nwhich can be put as\n\n \n\nNow\n \n\nand given the properties of complex conjugation,\n\n \n\nSince,\n \n\nit follows that\n\n \n\nThat is,\n\n \n\nNote that this works only because the ar are real, that is, . If any of the coefficients were nonreal, the roots would not necessarily come in conjugate pairs.\n\n Notes \n\nCategory:Theorems in complex analysis\nCategory:Polynomials\nCategory:Theorems in algebra\nCategory:Articles containing proofs"
    },
    {
      "title": "Cramer's rule",
      "url": "https://en.wikipedia.org/wiki/Cramer%27s_rule",
      "text": "In linear algebra, Cramer's rule is an explicit formula for the solution of a system of linear equations with as many equations as unknowns, valid whenever the system has a unique solution. It expresses the solution in terms of the determinants of the (square) coefficient matrix and of matrices obtained from it by replacing one column by the column vector of right-hand-sides of the equations. It is named after Gabriel Cramer (1704–1752), who published the rule for an arbitrary number of unknowns in 1750, although Colin Maclaurin also published special cases of the rule in 1748 (and possibly knew of it as early as 1729).\n\nCramer's rule implemented in a naïve way is computationally inefficient for systems of more than two or three equations. In the case of  equations in  unknowns, it requires computation of  determinants, while Gaussian elimination produces the result with the same computational complexity as the computation of a single determinant. Cramer's rule can also be numerically unstable even for 2×2 systems.  However, it has recently been shown that Cramer's rule can be implemented in O(n3) time, which is comparable to more common methods of solving systems of linear equations, such as Gaussian elimination (consistently requiring 2.5 times as many arithmetic operations for all matrix sizes), while exhibiting comparable numeric stability in most cases.\n\nGeneral case\nConsider a system of  linear equations for  unknowns, represented in matrix multiplication form as follows:\n\nwhere the  matrix  has a nonzero determinant, and the vector  is the column vector of the variables. Then the theorem states that in this case the system has a unique solution, whose individual values for the unknowns are given by:\n\nwhere  is the matrix formed by replacing the -th column of  by the column vector .\n\nA more general version of Cramer's rule considers the matrix equation\n\nwhere the  matrix  has a nonzero determinant, and ,  are  matrices. Given sequences  and , let  be the  submatrix of  with rows in  and columns in . Let  be the  matrix formed by replacing the  column of  by the  column of , for all . Then\n\nIn the case , this reduces to the normal Cramer's rule.\n\nThe rule holds for systems of equations with coefficients and unknowns in any field, not just in the real numbers.\n\nProof\nThe proof for Cramer's rule uses just two properties of determinants: linearity with respect to any given column (taking for that column a linear combination of column vectors produces as determinant the corresponding linear combination of their determinants), and the fact that the determinant is zero whenever two columns are equal (which is implied by the basic property that the sign of the determinant flips if you switch two columns).\n\nFix the index j of a column. Linearity means that if we consider only column j as variable (fixing the others arbitrarily), the resulting function  (assuming matrix entries are in ) can be given by a matrix, with one row and n columns, that acts on column j. In fact this is precisely what Laplace expansion does, writing  for certain coefficients C1, ..., Cn that depend on the columns of  other than column j (the precise expression for these cofactors is not important here). The value  is then the result of applying the one-line matrix  to column j of . If  is applied to any other column k of , then the result is the determinant of the matrix obtained from  by replacing column j by a copy of column k, so the resulting determinant is 0 (the case of two equal columns).\n\nNow consider a system of  linear equations in  unknowns , whose coefficient matrix is , with det(A) assumed to be nonzero:\n\nIf one combines these equations by taking C1 times the first equation, plus C2 times the second, and so forth until Cn times the last, then the coefficient of  will become , while the coefficients of all other unknowns become 0; the left hand side becomes simply det(A)xj. The right hand side is , which is  applied to the column vector b of the right hand side . In fact what has been done here is multiply the matrix equation  on the left by . Dividing by the nonzero number det(A) one finds the following equation, necessary to satisfy the system:\n\nBut by construction the numerator is the determinant of the matrix obtained from  by replacing column j by b, so we get the expression of Cramer's rule as a necessary condition for a solution. The same procedure can be repeated for other values of j to find values for the other unknowns.\n\nThe only point that remains to prove is that these values for the unknowns, the only possible ones, do indeed together form a solution. But if the matrix  is invertible with inverse , then  will be a solution, thus showing its existence. To see that  is invertible when det(A) is nonzero, consider the  matrix M obtained by stacking the one-line matrices  on top of each other for j = 1, ..., n (this gives the adjugate matrix for ). It was shown that  where  appears at the position j; from this it follows that . Therefore,\n\ncompleting the proof.\n\nFor other proofs, see below.\n\nFinding inverse matrix\n\nLet  be an  matrix. Then\n\nwhere adj(A) denotes the adjugate matrix of ,  is the determinant, and I is the identity matrix.  If det(A) is invertible in R, then the inverse matrix of  is\n\nIf R is a field (such as the field of real numbers), then this gives a formula for the inverse of , provided . In fact, this formula will work whenever R is a commutative ring, provided that det(A) is a unit. If det(A) is not a unit, then  is not invertible.\n\nApplications\n\nExplicit formulas for small systems\nConsider the linear system\n\nwhich in matrix format is\n\nAssume  nonzero. Then, with help of determinants,  and  can be found with Cramer's rule as\n\nThe rules for  matrices are similar.  Given\n\nwhich in matrix format is\n\nThen the values of  and  can be found as follows:\n\nDifferential geometry\n\nRicci calculus\nCramer's rule is used in the Ricci calculus in various calculations involving the Christoffel symbols of the first and second kind.\n\nIn particular, Cramer's rule can be used to prove that the divergence operator on a Riemannian manifold is invariant with respect to change of coordinates. We give a direct proof, suppressing the role of the Christoffel symbols.\nLet  be a Riemannian manifold equipped with local coordinates . Let  be a vector field.  We use the summation convention throughout.\n\nTheorem.\nThe divergence of ,  \n\nis invariant under change of coordinates.\n\nLet  be a coordinate transformation with non-singular Jacobian.  Then the classical transformation laws imply that  where .  Similarly, if , then .  \nWriting this transformation law in terms of matrices yields , which implies .\n\nNow one computes\n\nIn order to show that this equals \n,\nit is necessary and sufficient to show that \n\nwhich is equivalent to\n\nCarrying out the differentiation on the left-hand side, we get:\n\nwhere  denotes the matrix obtained from  by deleting the th row and th column.\nBut Cramer's Rule says that \n\nis the th entry of the matrix .\nThus\n\ncompleting the proof.\n\nComputing derivatives implicitly\nConsider the two equations  and .  When u and v are independent variables, we can define  and \n\nFinding an equation for  is a trivial application of Cramer's rule.\n\nFirst, calculate the first derivatives of F, G, x, and y:\n\nSubstituting dx, dy into dF and dG, we have:\n\nSince u, v are both independent, the coefficients of du, dv must be zero.  So we can write out equations for the coefficients:\n\nNow, by Cramer's rule, we see that:\n\nThis is now a formula in terms of two Jacobians:\n\nSimilar formulas can be derived for \n\nInteger programming\nCramer's rule can be used to prove that an integer programming problem whose constraint matrix is totally unimodular and whose right-hand side is integer, has integer basic solutions.  This makes the integer program substantially easier to solve.\n\nOrdinary differential equations\nCramer's rule is used to derive the general solution to an inhomogeneous linear differential equation by the method of variation of parameters.\n\nGeometric interpretation\nthumb|400px|Geometric interpretation of Cramer's rule. The areas of the second and third shaded parallelograms are the same and the second is  times the first. From this equality Cramer's rule follows.\nCramer's rule has a geometric interpretation that can be considered also a proof or simply giving insight about its geometric nature. These geometric arguments work in general and not only in the case of two equations with two unknowns presented here.\n\nGiven the system of equations\n\nit can be considered as an equation between vectors\n\nThe area of the parallelogram determined by  and  is given by the determinant of the system of equations:\n\nIn general, when there are more variables and equations, the determinant of  vectors of length  will give the volume of the parallelepiped determined by those vectors in the -th dimensional Euclidean space.\n\nTherefore, the area of the parallelogram determined by  and  has to be  times the area of the first one since one of the sides has been multiplied by this factor. Now, this last parallelogram, by Cavalieri's principle, has the same area as the parallelogram determined by  and \n\nEquating the areas of this last and the second parallelogram gives the equation\n\nfrom which Cramer's rule follows.\n\nOther proofs\n\nA proof by abstract linear algebra \n\nThis is a restatement of the proof above in abstract language.\n\nConsider the map  where  is the matrix  with  substituted in the th column, as in Cramer's rule. Because of linearity of determinant in every column, this map is linear. Observe that it sends the \nth column of  to the th basis vector  (with 1 in the th  place), because determinant of a matrix with a repeated column is 0. So we have a linear map which agrees with the inverse of  on the column space; hence it agrees with     on the span of the column space. Since  is invertible, the column vectors span all of , so our map really is the inverse of . Cramer's rule follows.\n\nA short proof\nA short proof of Cramer's rule  can be given by noticing that  is the determinant of the matrix\n\nOn the other hand, assuming that our original matrix  is invertible, this matrix  has columns , where  is the n-th column of the matrix . Recall that the matrix  has columns . Hence we have\n\nThe proof for other  is similar.\n\n Proof using Clifford algebra \nConsider the system of three scalar equations in three unknown scalars \n\nand assign an orthonormal vector basis  for  as\n\nLet the vectors\n\nAdding the system of equations, it is seen that\n\nUsing the exterior product, each unknown scalar  can be solved as\n\nFor  equations in  unknowns, the solution for the -th unknown  generalizes to\n\nIf  are linearly independent, then the  can be expressed in determinant form identical to Cramer’s Rule as\n\nwhere  denotes the substitution of vector  with vector  in the -th numerator position.\n\nIncompatible and indeterminate cases\nA system of equations is said to be incompatible or inconsistent when there are no solutions and it is called indeterminate when there is more than one solution. For linear equations, an indeterminate system will have infinitely many solutions (if it is over an infinite field), since the solutions can be expressed in terms of one or more parameters that can take arbitrary values.\n\nCramer's rule applies to the case where the coefficient determinant is nonzero. In the 2×2 case, if the coefficient determinant is zero, then the system is incompatible if the numerator determinants are nonzero, or indeterminate if the numerator determinants are zero.\n\nFor 3×3 or higher systems, the only thing one can say when the coefficient determinant equals zero is that if any of the numerator determinants are nonzero, then the system must be incompatible. However, having all determinants zero does not imply that the system is indeterminate. A simple example where all determinants vanish (equal zero) but the system is still incompatible is the 3×3 system x+y+z=1, x+y+z=2, x+y+z=3.\n\nReferences\n\nExternal links\n\n Proof of Cramer's Rule\n WebApp descriptively solving systems of linear equations with Cramer's Rule\n Online Calculator of System of linear equations\n\nCategory:Linear algebra\nCategory:Theorems in algebra\nCategory:Determinants\nCategory:1750 in science"
    },
    {
      "title": "Crystallographic restriction theorem",
      "url": "https://en.wikipedia.org/wiki/Crystallographic_restriction_theorem",
      "text": "The crystallographic restriction theorem in its basic form was based on the observation that the rotational symmetries of a crystal are usually limited to 2-fold, 3-fold, 4-fold, and 6-fold. However, quasicrystals can occur with other diffraction pattern symmetries, such as 5-fold; these were not discovered until 1982 by Dan Shechtman.Shechtman et al (1982)\n\nCrystals are modeled as discrete lattices, generated by a list of independent finite translations . Because discreteness requires that the spacings between lattice points have a lower bound, the group of rotational symmetries of the lattice at any point must be a finite group (alternatively, the point is the only system allowing for infinite rotational symmetry). The strength of the theorem is that not all finite groups are compatible with a discrete lattice; in any dimension, we will have only a finite number of compatible groups.\n\nDimensions 2 and 3\n\nThe special cases of 2D (wallpaper groups) and 3D (space groups) are most heavily used in applications, and they can be treated together.\n\n Lattice proof \n\nA rotation symmetry in dimension 2 or 3 must move a lattice point to a succession of other lattice points in the same plane, generating a regular polygon of coplanar lattice points. We now confine our attention to the plane in which the symmetry acts , illustrated with lattice vectors in the figure.\n\nright|280px|thumb|Lattices restrict polygons\n Compatible: 6-fold (3-fold), 4-fold (2-fold)\n Incompatible: 8-fold, 5-fold\n\nNow consider an 8-fold rotation, and the displacement vectors between adjacent points of the polygon. If a displacement exists between any two lattice points, then that same displacement is repeated everywhere in the lattice. So collect all the edge displacements to begin at a single lattice point. The edge vectors become radial vectors, and their 8-fold symmetry implies a regular octagon of lattice points around the collection point. But this is impossible, because the new octagon is about 80% as large as the original. The significance of the shrinking is that it is unlimited. The same construction can be repeated with the new octagon, and again and again until the distance between lattice points is as small as we like; thus no discrete lattice can have 8-fold symmetry. The same argument applies to any k-fold rotation, for k greater than 6.\n\nA shrinking argument also eliminates 5-fold symmetry. Consider a regular pentagon of lattice points. If it exists, then we can take every other edge displacement and (head-to-tail) assemble a 5-point star, with the last edge returning to the starting point. The vertices of such a star are again vertices of a regular pentagon with 5-fold symmetry, but about 60% smaller than the original.\n\nThus the theorem is proved.\n\nThe existence of quasicrystals and Penrose tilings shows that the assumption of a linear translation is necessary.  Penrose tilings may have 5-fold rotational symmetry and a discrete lattice, and any local neighborhood of the tiling is repeated infinitely many times, but there is no linear translation for the tiling as a whole.  And without the discrete lattice assumption, the above construction not only fails to reach a contradiction, but produces a (non-discrete) counterexample.  Thus 5-fold rotational symmetry cannot be eliminated by an argument missing either of those assumptions.  A Penrose tiling of the whole (infinite) plane can only have exact 5-fold rotational symmetry (of the whole tiling) about a single point, however, whereas the 4-fold and 6-fold lattices have infinitely many centres of rotational symmetry.\n\n Trigonometry proof \n\nConsider two lattice points A and B separated by a translation vector r. Consider an angle α such that a rotation of angle α about any lattice point is a symmetry of the lattice. Rotating about point B by α maps point A to a new point A'.  Similarly, rotating about point A by α maps B to a point B'.  Since both rotations mentioned are symmetry operations, A' and B' must both be lattice points.  Due to periodicity of the crystal, the new vector r' which connects them must be equal to an integer multiple of r:\n\n \n\nwith  integer. The four translation vectors, three of length  and one, connecting A' and B', of length , form a trapezium.  Therefore, the length of r' is also given by:\n\n \n\nCombining the two equations gives:\n\n \n\nwhere  is also an integer. Bearing in mind that  we have allowed integers . Solving for possible values of  reveals that the only values in the 0° to 180° range are 0°, 60°, 90°, 120°, and 180°.  In radians, the only allowed rotations consistent with lattice periodicity are given by 2π/n, where n = 1, 2, 3, 4, 6.  This corresponds to 1-, 2-, 3-, 4-, and 6-fold symmetry, respectively, and therefore excludes the possibility of 5-fold or greater than 6-fold symmetry.\n\n Short trigonometry proof \nthumb|right\nthumb|right\n\nConsider a line of atoms A-O-B, separated by distance a. Rotate the entire row by θ = +2π/n and θ = −2π/n, with point O kept fixed. After the rotation by +2π/n, A is moved to the lattice point C and after the rotation by -2π/n, B is moved to the lattice point D. Due to the assumed periodicity of the lattice, the two lattice points C and D will be also in a line directly below the initial row; moreover C and D will be separated by r = ma, with m an integer. But by the geometry, the separation between these points is: \n\n . \n\nEquating the two relations gives: \n\n \n\nThis is satisfied by only n = 1, 2, 3, 4, 6.\n\n Matrix proof \n\nFor an alternative proof, consider matrix properties. The sum of the diagonal elements of a matrix is called the trace of the matrix. In 2D and 3D every rotation is a planar rotation, and the trace is a function of the angle alone. For a 2D rotation, the trace is 2 cos θ; for a 3D rotation, 1 + 2 cos θ.\n\nExamples\nConsider a 60° (6-fold) rotation matrix with respect to an orthonormal basis in 2D.\n\nThe trace is precisely 1, an integer.\nConsider a 45° (8-fold) rotation matrix.\n\nThe trace is 2/, not an integer.\n\nSelecting a basis formed from vectors that spans the lattice, neither orthogonality nor unit length is guaranteed, only linear independence. However the trace of the rotation matrix is the same with respect to any basis. The trace is a similarity invariant under linear transformations. In the lattice basis, the rotation operation must map every lattice point into an integer number of lattice vectors, so the entries of the rotation matrix in the lattice basis — and hence the trace — are necessarily integers. Similar as in other proofs, this implies that the only allowed rotational symmetries correspond to 1,2,3,4 or 6-fold invariance.  For example, wallpapers and crystals cannot be rotated by 45° and remain invariant, the only possible angles are: 360°, 180°, 120°, 90° or 60°.\n\nExample\nConsider a 60° (360°/6) rotation matrix with respect to the oblique lattice basis for a tiling by equilateral triangles.\n\nThe trace is still 1. The determinant (always +1 for a rotation) is also preserved.\n\nThe general crystallographic restriction on rotations does not guarantee that a rotation will be compatible with a specific lattice. For example, a 60° rotation will not work with a square lattice; nor will a 90° rotation work with a rectangular lattice.\n\nHigher dimensions\nWhen the dimension of the lattice rises to four or more, rotations need no longer be planar; the 2D proof is inadequate. However, restrictions still apply, though more symmetries are permissible. For example, the hypercubic lattice has an eightfold rotational symmetry, corresponding to an eightfold rotational symmetry of the hypercube. This is of interest, not just for mathematics, but for the physics of quasicrystals under the cut-and-project theory. In this view, a 3D quasicrystal with 8-fold rotation symmetry might be described as the projection of a slab cut from a 4D lattice.\n\nThe following 4D rotation matrix is the aforementioned eightfold symmetry of the hypercube (and the cross-polytope):\n\nTransforming this matrix to the new coordinates given by \n will produce:\n\nThis third matrix then corresponds to a rotation both by 45° (in the first two dimensions) and by 135° (in the last two). Projecting a slab of hypercubes along the first two dimensions of the new coordinates produces an Ammann–Beenker tiling (another such tiling is produced by projecting along the last two dimensions), which therefore also has 8-fold rotational symmetry on average.\n\nThe A4 lattice and F4 lattice have order 10 and order 12 rotational symmetries, respectively.\n\nTo state the restriction for all dimensions, it is convenient to shift attention away from rotations alone and concentrate on the integer matrices . We say that a matrix A has order k when its k-th power (but no lower), Ak, equals the identity. Thus a 6-fold rotation matrix in the equilateral triangle basis is an integer matrix with order 6. Let OrdN denote the set of integers that can be the order of an N×N integer matrix. For example, Ord2 = {1, 2, 3, 4, 6}. We wish to state an explicit formula for OrdN.\n\nDefine a function ψ based on Euler's totient function φ; it will map positive integers to non-negative integers. For an odd prime, p, and a positive integer, k, set ψ(pk) equal to the totient function value, \nφ(pk), which in this case is pk−pk−1. Do the same for ψ(2k) when k > 1. Set ψ(2) and ψ(1) to 0. Using the fundamental theorem of arithmetic, we can write any other positive integer uniquely as a product of prime powers, m = ∏α pαk α; set ψ(m) = ∑α ψ(pα<span>k α</span>). This differs from the totient itself, because it is a sum instead of a product.\n\nThe crystallographic restriction in general form states that OrdN consists of those positive integers m such that ψ(m) ≤ N.\n\n{| class=\"wikitable\"\n|+ Smallest dimension for a given order\n|- align=\"center\"\n| m || 1 || 2 || 3 || 4 || 5 || 6 || 7 || 8 || 9 || 10 || 11 || 12 || 13 || 14 || 15 || 16 || 17 || 18 || 19 || 20 || 21 || 22 || 23 || 24 || 25 || 26 || 27 || 28 || 29 || 30 || 31\n|- align=\"center\"\n| ψ(m) || 0 || 0 || 2 || 2 || 4 || 2 || 6 || 4 || 6 ||  4 || 10 ||  4 || 12 ||  6 || 6 || 8 || 16 || 6 || 18 || 6 || 8 || 10 || 22 || 6 || 20 || 12 || 18 || 8 || 28 || 6 || 30\n|}\n\nFor m>2, the values of ψ(m) are equal to twice the algebraic degree of cos(2π/m); therefore, ψ(m) is strictly less than m and reaches this maximum value if and only if m is a prime.\n\nNote that these additional symmetries do not allow a planar slice to have, say, 8-fold rotation symmetry. In the plane, the 2D restrictions still apply. Thus the cuts used to model quasicrystals necessarily have thickness.\n\nInteger matrices are not limited to rotations; for example, a reflection is also a symmetry of order 2. But by insisting on determinant +1, we can restrict the matrices to proper rotations.\n\nFormulation in terms of isometries\n\nThe crystallographic restriction theorem can be formulated in terms of isometries of Euclidean space. A set of isometries can form a group. By a discrete isometry group we will mean an isometry group that maps every point to a discrete subset of RN, i.e. a set of isolated points. With this terminology, the crystallographic restriction theorem in two and three dimensions can be formulated as follows.\n\nFor every discrete isometry group in two- and three-dimensional space which includes translations spanning the whole space, all isometries of finite order are of order 1, 2, 3, 4 or 6.\n\nNote that isometries of order n include, but are not restricted to, n-fold rotations. The theorem also excludes S8, S12, D4d, and D6d (see point groups in three dimensions), even though they have 4- and 6-fold rotational symmetry only.\n\nNote also that rotational symmetry of any order about an axis is compatible with translational symmetry along that axis.\n\nThe result in the table above implies that for every discrete isometry group in four- and five-dimensional space which includes translations spanning the whole space, all isometries of finite order are of order 1, 2, 3, 4, 5, 6, 8, 10, or 12.\n\nAll isometries of finite order in six- and seven-dimensional space are of order 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 15, 18, 20, 24 or 30 .\n\nSee also\nCrystallographic point group\nCrystallography\n\nNotes\n\n References \n \n \n \n \n \n\nExternal links\nThe crystallographic restriction\n\nCategory:Crystallography\nCategory:Group theory\nCategory:Theorems in algebra\nCategory:Articles containing proofs"
    },
    {
      "title": "Descartes' rule of signs",
      "url": "https://en.wikipedia.org/wiki/Descartes%27_rule_of_signs",
      "text": "In mathematics, Descartes' rule of signs, first described by René Descartes in his work La Géométrie, is a technique for getting information on the number of positive real roots of a polynomial. It asserts that the number of positive roots is at most the number of sign changes in the sequence of polynomial's coefficients (omitting the zero coefficients), and that the difference between these two numbers is always even. This implies in particular that, if this difference is zero or one, then there is exactly zero or one positive root, respectively.\n\nBy a homographic transformation of the variable, one may use Descartes' rule of signs for getting a similar information on the number of roots in any interval. This is the basic idea of Budan's theorem and Budan–Fourier theorem. By repeating the division of an interval into two intervals, one gets eventually a list of disjoints intervals containing together all real roots of the polynomial, and containing each exactly one real root. Descartes rule of signs and homographic transformations of the variable are, nowadays, the basis of the fastest algorithms for computer computation of real roots of polynomials (see Real-root isolation).\n\nDescartes himself used the transformation  for using his rule for getting information of the number of negative roots.\n\nDescartes' rule of signs\nPositive roots\nThe rule states that if the terms of a single-variable polynomial with real coefficients are ordered by descending variable exponent, then the number of positive roots of the polynomial is either equal to the number of sign differences between consecutive nonzero coefficients, or is less than it by an even number. Multiple roots of the same value are counted separately.\n\nNegative roots\nAs a corollary of the rule, the number of negative roots is the number of sign changes after multiplying the coefficients of odd-power terms by −1, or fewer than it by an even number. This procedure is equivalent to substituting the negation of the variable for the variable itself.\nFor example, to find the number of negative roots of , we equivalently ask how many positive roots there are for  in \n \nUsing Descartes' rule of signs on  gives the number of positive roots  of g, and since  it gives the number of positive roots  of f, which is the same as the number of negative roots  of f.\n\nExample: real roots\nThe polynomial\n\nhas one sign change between the second and third terms (the sequence of pairs of successive signs is + → +, + → −, − → − ). Therefore it has exactly one positive root. Note that the sign of the leading coefficient needs to be considered.\nTo find the number of negative roots, change the signs of the coefficients of the terms with odd exponents, i.e., apply Descartes' rule of signs to the polynomial , to obtain a second polynomial\n\nThis polynomial has two sign changes (the sequence of pairs of successive signs is − → +, + → +, + → − ), meaning that this second polynomial has two or zero positive roots; thus the original polynomial has two or zero negative roots.\n\nIn fact, the factorization of the first polynomial is\n\t\n\n\t \t\t\nso the roots are −1 (twice) and +1 (once).\n\nThe factorization of the second polynomial is\n\nSo here, the roots are +1 (twice) and −1 (once), the negation of the roots of the original polynomial.\n\nNonreal roots\n\nAny nth degree polynomial has exactly n  roots in the complex plane, if counted according to multiplicity. So if f(x) is a polynomial which does not have a root at 0 (which can be determined by inspection) then the minimum number of nonreal roots is equal to\n\nwhere p denotes the maximum number of positive roots, q denotes the maximum number of negative roots (both of which can be found using Descartes' rule of signs), and n denotes the degree of the equation.\n\nExample: zero coefficients, nonreal roots\n\nThe polynomial\n\nhas one sign change, so the maximum number of positive real roots is 1. From\n\nwe can tell that the polynomial has no negative real roots. So the minimum number of nonreal roots is\n\nSince nonreal roots of a polynomial with real coefficients must occur in conjugate pairs, we can see that  has exactly 2 nonreal roots and 1 real (and positive) root.\n\nSpecial case\nThe subtraction of only multiples of 2 from the maximal number of positive roots occurs because the polynomial may have nonreal roots, which always come in pairs since the rule applies to polynomials whose coefficients are real. Thus if the polynomial is known to have all real roots, this rule allows one to find the exact number of positive and negative roots. Since it is easy to determine the multiplicity of zero as a root, the sign of all roots can be determined in this case.\n\nGeneralizations\n\nIf the real polynomial P has k real positive roots counted with multiplicity, then for every a > 0 there are at least k changes of sign in the sequence of coefficients of the Taylor series of the function eaxP(x). For a sufficiently large, there are exactly k such changes of sign.D. R. Curtiss, Recent extensions of Descartes' rule of signs, Annals of Mathematics., Vol. 19, No. 4, 1918, pp. 251–278.Vladimir P. Kostov, A mapping defined by the Schur–Szegő composition, Comptes Rendus Acad. Bulg. Sci. tome 63, No. 7, 2010, pp. 943–952.\n\nIn the 1970s Askold Georgevich Khovanskiǐ developed the theory of fewnomials that generalises Descartes' rule. The rule of signs can be thought of as stating that the number of real roots of a polynomial is dependent on the polynomial's complexity, and that this complexity is proportional to the number of monomials it has, not its degree. Khovanskiǐ showed that this holds true not just for polynomials but for algebraic combinations of many transcendental functions, the so-called Pfaffian functions.\n\nSee also\nSturm's theorem\nRational root theorem\nProperties of polynomial roots\nGauss–Lucas theorem\n\nNotes\n\nExternal links\n\nDescartes' Rule of Signs – Proof of the rule\nDescartes' Rule of Signs – Basic explanation\n\nCategory:Polynomials\nCategory:Theorems in algebra"
    },
    {
      "title": "Factor theorem",
      "url": "https://en.wikipedia.org/wiki/Factor_theorem",
      "text": "In algebra, the factor theorem is a theorem linking factors and zeros of a polynomial. It is a special case of the polynomial remainder theorem..\n\nThe factor theorem states that a polynomial  has a factor  if and only if  (i.e.  is a root)..\n\nFactorization of polynomials\n\nTwo problems where the factor theorem is commonly applied are those of factoring a polynomial and finding the roots of a polynomial equation; it is a direct consequence of the theorem that these problems are essentially equivalent.\n\nThe factor theorem is also used to remove known zeros from a polynomial while leaving all unknown zeros intact, thus producing a lower degree polynomial whose zeros may be easier to find. Abstractly, the method is as follows:.\n \"Guess\" a zero  of the polynomial . (In general, this can be very hard, but maths textbook problems that involve solving a polynomial equation are often designed so that some roots are easy to discover.)\n Use the factor theorem to conclude that  is a factor of .\n Compute the polynomial , for example using polynomial long division or synthetic division.\n Conclude that any root  of  is a root of . Since the polynomial degree of  is one less than that of , it is \"simpler\" to find the remaining zeros by studying .\n\nExample\nFind the factors of\n \n\nTo do this one would use trial and error (or the rational root theorem) to find the first x value that causes the expression to equal zero.  To find out if  is a factor, substitute  into the polynomial above:\n \n \n \n\nAs this is equal to 18 and not 0 this means  is not a factor of . So, we next try  (substituting  into the polynomial):\n \n\nThis is equal to . Therefore , which is to say , is a factor, and  is a root of \n\nThe next two roots can be found by algebraically dividing  by  to get a quadratic:\n\n \n\nand therefore  and  are factors of  Of these the quadratic factor can be further factored using the quadratic formula, which gives as roots of the quadratic  Thus the three irreducible factors of the original polynomial are   and \nReferences\n\n \n\nCategory:Theorems in algebra"
    },
    {
      "title": "Frobenius determinant theorem",
      "url": "https://en.wikipedia.org/wiki/Frobenius_determinant_theorem",
      "text": "In mathematics, the Frobenius determinant theorem was a conjecture made in 1896 by the mathematician Richard Dedekind, who wrote a letter to F. G. Frobenius about it (reproduced in , with an English translation in ).\n\nIf one takes the multiplication table of a finite group G and replaces each entry g with the variable xg, and subsequently takes the determinant, then the determinant factors as a product of n irreducible polynomials, where n is the number of conjugacy classes.  Moreover, each polynomial is raised to a power equal to its degree.  Frobenius proved this surprising conjecture, and it became known as the Frobenius determinant theorem.\n\nFormal statement\nLet a finite group  have elements , and let  be associated with each element of . Define the matrix  with entries . Then\n\n \n\nwhere r is the number of conjugacy classes of G.\n\nReferences\n\n  Review\n\nEtingof, Pavel. Lectures on Representation Theory. \n\nCategory:Theorems in algebra\nCategory:Determinants\nCategory:Group theory\nCategory:Matrix theory"
    },
    {
      "title": "Frobenius reciprocity theorem",
      "url": "https://en.wikipedia.org/wiki/Frobenius_reciprocity_theorem",
      "text": "REDIRECT Frobenius reciprocity\nCategory:Theorems in algebra"
    },
    {
      "title": "Fundamental lemma (Langlands program)",
      "url": "https://en.wikipedia.org/wiki/Fundamental_lemma_%28Langlands_program%29",
      "text": "In the mathematical theory of automorphic forms, the  fundamental lemma relates orbital integrals on a reductive group over a local field to stable orbital integrals on its endoscopic groups. It was conjectured by  in the course of developing the Langlands program.  The fundamental lemma was proved by Gérard Laumon and Ngô Bảo Châu in the case of unitary groups and then by Ngô for general reductive groups, building on a series of important reductions made by Jean-Loup Waldspurger to the case of Lie algebras. Time magazine placed Ngô's proof on the list of the \"Top 10 scientific discoveries of 2009\".Top 10 Scientific Discoveries of 2009, Time In 2010, Ngô was awarded the Fields medal for this proof.\n\n Motivation and history \n\nRobert Langlands outlined a strategy for proving local and global Langlands conjectures using the Arthur–Selberg trace formula, but in order for this approach to work, the geometric sides of the trace formula for different groups must be related in a particular way. This relationship takes the form of identities between orbital integrals on reductive groups G and H over a nonarchimedean local field F, where the group H, called an endoscopic group of G, is constructed from G and some additional data.\n\nThe first case considered was G = SL2 .  then developed the general framework for the theory of endoscopic transfer and formulated specific conjectures. However, during the next two decades only partial progress was made towards proving the fundamental lemma.Kottwitz and Rogawski for U3, Wadspurger for SLn,  Hales and Weissauer for Sp4.Fundamental Lemma and Hitchin Fibration, Gérard Laumon, May 13, 2009 Harris called it a \"bottleneck limiting progress on a host of arithmetic questions\".INTRODUCTION TO “THE STABLE TRACE FORMULA, SHIMURA VARIETIES, AND ARITHMETIC APPLICATIONS”, p. 1., Michael Harris Langlands himself, writing on the origins of endoscopy, commented:\n\nStatement\n\nThe fundamental lemma states that an orbital integral O for a group G is equal to a stable orbital integral SO for an endoscopic group H, up to a transfer factor Δ :\n\nwhere\nF is a local field\nG is an unramified group defined over F, in other words a quasi-split reductive group defined over F that splits over an unramified extension of F\nH is an unramified endoscopic group of G associated to κ\nKG and KH are hyperspecial maximal compact subgroups of G and H, which means roughly that they are the subgroups of points with coefficients in the ring of integers of F.\n1KG and 1KH are the characteristic functions of KG and KH.\nΔ(γH,γG) is a transfer factor, a certain elementary expression depending on γH and γG\nγH and  γG are elements of G and H representing stable conjugacy classes, such that the stable conjugacy class of G is the transfer of the stable conjugacy class of H.\nκ is a character of the group of conjugacy classes in the stable conjugacy class of γG\nSO and O are stable orbital integrals and orbital integrals depending on their parameters.\n\n Approaches \n\n proved the fundamental lemma for Archimedean fields.\n\n verified the fundamental lemma for general linear groups.\n\n and  verified some cases of the fundamental lemma for 3-dimensional unitary groups.\n\n and  verified the fundamental lemma for the symplectic and general symplectic groups Sp4, GSp4.\n\nA paper of George Lusztig and David Kazhdan pointed out that orbital integrals could be interpreted as counting points on certain algebraic varieties over finite fields. Further, the integrals in question can be computed in a way that depends only on the residue field of F; and the issue can be reduced to the Lie algebra version of the orbital integrals. Then the problem was restated in terms of the Springer fiber of algebraic groups.The Fundamental Lemma for Unitary Groups , at p. 12., Gérard Laumon The circle of ideas was connected to a purity conjecture; Laumon gave a conditional proof based on such a conjecture, for unitary groups.  then proved the fundamental lemma for unitary groups, using  Hitchin fibration introduced by , which is an abstract geometric analogue of the Hitchin system of complex algebraic geometry.\n showed for Lie algebras that the function field case implies the fundamental lemma over all local fields, and  showed that the fundamental lemma for Lie algebras implies the fundamental lemma for groups.\n\nNotes\n\n References \n\n External links \n Gerard Laumon lecture on the fundamental lemma for unitary groups\n \n\nCategory:Algebraic groups\nCategory:Automorphic forms\nCategory:Theorems in algebra\nCategory:Theorems in number theory\nCategory:Langlands program"
    },
    {
      "title": "Fundamental theorem of algebra",
      "url": "https://en.wikipedia.org/wiki/Fundamental_theorem_of_algebra",
      "text": "The fundamental theorem of algebra states that every non-constant single-variable polynomial with complex coefficients has at least one complex root. This includes polynomials with real coefficients, since every real number can be considered a complex number with its imaginary part equal to zero.\n\nEquivalently (by definition), the theorem states that the field of complex numbers is algebraically closed.\n\nThe theorem is also stated as follows: every non-zero, single-variable, degree n polynomial with complex coefficients has, counted with multiplicity, exactly n complex roots. The equivalence of the two statements can be proven through the use of successive polynomial division.\n\nIn spite of its name, there is no purely algebraic proof of the theorem, since any proof must use some form of completeness, which is not an algebraic concept.Even the proof that the equation  has a solution involves the definition of the real numbers through some form of completeness. Additionally, it is not fundamental for modern algebra; its name was given at a time when algebra was synonymous with theory of equations.\n\nHistory\nPeter Roth, in his book Arithmetica Philosophica (published in 1608, at Nürnberg, by Johann Lantzenberger),Rare books wrote that a polynomial equation of degree n (with real coefficients) may have n solutions. Albert Girard, in his book L'invention nouvelle en l'Algèbre (published in 1629), asserted that a polynomial equation of degree n has n solutions, but he did not state that they had to be real numbers. Furthermore, he added that his assertion holds \"unless the equation is incomplete\", by which he meant that no coefficient is equal to 0. However, when he explains in detail what he means, it is clear that he actually believes that his assertion is always true; for instance, he shows that the equation  although incomplete, has four solutions (counting multiplicities): 1 (twice),  and \n\nAs will be mentioned again below, it follows from the fundamental theorem of algebra that every non-constant polynomial with real coefficients can be written as a product of polynomials with real coefficients whose degrees are either 1 or 2. However, in 1702 Leibniz said that no polynomial of the type x4 + a4 (with a real and distinct from 0) can be written in such a way. Later, Nikolaus Bernoulli made the same assertion concerning the polynomial x4 − 4x3 + 2x2 + 4x + 4, but he got a letter from Euler in 1742See section Le rôle d'Euler in C. Gilain's article Sur l'histoire du théorème fondamental de l'algèbre: théorie des équations et calcul intégral. in which he was told that his polynomial happened to be equal to\n\nAlso, Euler mentioned that\n\nA first attempt at proving the theorem was made by d'Alembert in 1746, but his proof was incomplete. Among other problems, it assumed implicitly a theorem (now known as Puiseux's theorem) which would not be proved until more than a century later, and furthermore the proof assumed the fundamental theorem of algebra. Other attempts were made by Euler (1749), de Foncenex (1759), Lagrange (1772), and Laplace (1795). These last four attempts assumed implicitly Girard's assertion; to be more precise, the existence of solutions was assumed and all that remained to be proved was that their form was a + bi for some real numbers a and b. In modern terms, Euler, de Foncenex, Lagrange, and Laplace were assuming the existence of a splitting field of the polynomial p(z).\n\nAt the end of the 18th century, two new proofs were published which did not assume the existence of roots, but neither of which was complete. One of them, due to James Wood and mainly algebraic, was published in 1798 and it was totally ignored. Wood's proof had an algebraic gap.Concerning Wood's proof, see the article A forgotten paper on the fundamental theorem of algebra, by Frank Smithies. The other one was published by Gauss in 1799 and it was mainly geometric, but it had a topological gap, filled by Alexander Ostrowski in 1920, as discussed in Smale 1981  (Smale writes, \"...I wish to point out what an immense gap Gauss' proof contained. It is a subtle point even today that a real algebraic plane curve cannot enter a disk without leaving. In fact even though Gauss redid this proof 50 years later, the gap remained. It was not until 1920 that Gauss' proof was completed. In the reference Gauss, A. Ostrowski has a paper which does this and gives an excellent discussion of the problem as well...\"). A rigorous proof was first published by Argand in 1806 (and revisited in 1813);  it was here that, for the first time, the fundamental theorem of algebra was stated for polynomials with complex coefficients, rather than just real coefficients. Gauss produced two other proofs in 1816 and another version of his original proof in 1849.\n\nThe first textbook containing a proof of the theorem was Cauchy's Cours d'analyse de l'École Royale Polytechnique (1821). It contained Argand's proof, although Argand is not credited for it.\n\nNone of the proofs mentioned so far is constructive. It was Weierstrass who raised for the first time, in the middle of the 19th century, the problem of finding a constructive proof of the fundamental theorem of algebra. He presented his solution, that amounts in modern terms to a combination of the Durand–Kerner method with the homotopy continuation principle, in 1891. Another proof of this kind was obtained by Hellmuth Kneser in 1940 and simplified by his son Martin Kneser in 1981.\n\nWithout using countable choice, it is not possible to constructively prove the fundamental theorem of algebra for complex numbers based on the Dedekind real numbers (which are not constructively equivalent to the Cauchy real numbers without countable choiceFor the minimum necessary to prove their equivalence, see Bridges, Schuster, and Richman; 1998; A weak countable choice principle; available from .). However, Fred Richman proved a reformulated version of the theorem that does work.See Fred Richman; 1998; The fundamental theorem of algebra: a constructive development without choice; available from .\n\nProofs\nAll proofs below involve some analysis, or at least the topological concept of continuity of real or complex functions. Some also use differentiable or even analytic functions. This fact has led to the remark that the Fundamental Theorem of Algebra is neither fundamental, nor a theorem of algebra.\n\nSome proofs of the theorem only prove that any non-constant polynomial with real coefficients has some complex root. This is enough to establish the theorem in the general case because, given a non-constant polynomial p(z) with complex coefficients, the polynomial\n\nhas only real coefficients and, if z is a zero of q(z), then either z or its conjugate is a root of p(z).\n\nA large number of non-algebraic proofs of the theorem use the fact (sometimes called \"growth lemma\") that an n-th degree polynomial function p(z) whose dominant coefficient is 1 behaves like zn when |z| is large enough. A more precise statement is: there is some positive real number R such that:\n\nwhen |z| > R.\n\nComplex-analytic proofs\nFind a closed disk D of radius r centered at the origin such that |p(z)| > |p(0)| whenever |z| ≥ r. The minimum of |p(z)| on D, which must exist since D is compact, is therefore achieved at some point z0 in the interior of D, but not at any point of its boundary. The Maximum modulus principle (applied to 1/p(z)) implies then that p(z0) = 0. In other words, z0 is a zero of p(z).\n\nA variation of this proof does not require the use of the maximum modulus principle (in fact, the same argument with minor changes also gives a proof of the maximum modulus principle for holomorphic functions). If we assume by contradiction that a := p(z0) ≠ 0, then, expanding p(z) in powers of z − z0 we can write\n\nHere, the cj are simply the coefficients of the polynomial z → p(z + z0), and we let k be the index of the first coefficient following the constant term that is non-zero. But now we see that for z sufficiently close to z0 this has behavior asymptotically similar to the simpler polynomial , \n\nin the sense that (as is easy to check) the function\n\n \n\nis bounded by some positive constant M in some neighborhood of z0. Therefore if we define  and let , then for any sufficiently small positive number r (so that the bound M mentioned above holds), using the triangle inequality we see that\n\nWhen r is sufficiently close to 0 this upper bound for |p(z)| is strictly smaller than |a|, in contradiction to the definition of z0. (Geometrically, we have found an explicit direction θ0 such that if one approaches z0 from that direction one can obtain values p(z) smaller in absolute value than |p(z0)|.)\n\nAnother analytic proof can be obtained along this line of thought observing that, since |p(z)| > |p(0)| outside D, the minimum of |p(z)| on the whole complex plane is achieved at z0. If |p(z0)| > 0, then 1/p is a bounded holomorphic function in the entire complex plane since, for each complex number z, |1/p(z)| ≤ |1/p(z0)|. Applying Liouville's theorem, which states that a bounded entire function must be constant, this would imply that 1/p is constant and therefore that p is constant. This gives a contradiction, and hence p(z0) = 0.\n\nYet another analytic proof uses the argument principle. Let R be a positive real number large enough so that every root of p(z) has absolute value smaller than R; such a number must exist because every non-constant polynomial function of degree n has at most n zeros. For each r > R, consider the number\n\nwhere c(r) is the circle centered at 0 with radius r oriented counterclockwise; then the argument principle says that this number is the number N of zeros of p(z) in the open ball centered at 0 with radius r, which, since r > R, is the total number of zeros of p(z). On the other hand, the integral of n/z along c(r) divided by 2πi is equal to n. But the difference between the two numbers is\n\nThe numerator of the rational expression being integrated has degree at most n − 1 and the degree of the denominator is n + 1. Therefore, the number above tends to 0 as r → +∞. But the number is also equal to N − n and so N = n.\n\nStill another complex-analytic proof can be given by combining linear algebra with the Cauchy theorem. To establish that every complex polynomial of degree n > 0 has a zero, it suffices to show that every complex square matrix of size n > 0 has a (complex) eigenvalue.A proof of the fact that this suffices can be seen here. The proof of the latter statement is by contradiction.\n\nLet A be a complex square matrix of size n > 0 and let In be the unit matrix of the same size. Assume A has no eigenvalues. Consider the resolvent function\n\nwhich is a meromorphic function on the complex plane with values in the vector space of matrices. The eigenvalues of A are precisely the poles of R(z). Since, by assumption, A has no eigenvalues, the function R(z) is an entire function and Cauchy theorem implies that\n\nOn the other hand, R(z) expanded as a geometric series gives:\n\nThis formula is valid outside the closed disc of radius  (the operator norm of A). Let  Then\n\n(in which only the summand k = 0 has a nonzero integral). This is a contradiction, and so A has an eigenvalue.\n\nFinally, Rouché's theorem gives perhaps the shortest proof of the theorem.\n\nTopological proofs\nSuppose the minimum of |p(z)| on the whole complex plane is achieved at z0; it was seen at the proof which uses Liouville's theorem that such a number must exist. We can write p(z) as a polynomial in z − z0: there is some natural number k and there are some complex numbers ck, ck + 1, ..., cn such that ck ≠ 0 and:\n\nIf p(z0) is nonzero, it follows that if a is a kth root of −p(z0)/ck and if t is positive and sufficiently small, then |p(z0 + ta)| < |p(z0)|, which is impossible, since |p(z0)| is the minimum of |p| on D.\n\nFor another topological proof by contradiction, suppose that the polynomial p(z) has no roots, and consequently is never equal to 0. Think of the polynomial as a map from the complex plane into the complex plane. It maps any circle |z| = R into a closed loop, a curve P(R). We will consider what happens to the winding number of P(R) at the extremes when R is very large and when R = 0. When R is a sufficiently large number, then the leading term zn of p(z) dominates all other terms combined; in other words, \n\nWhen z traverses the circle  once counter-clockwise  then  winds n times counter-clockwise  around the origin (0,0), and P(R) likewise. At the other extreme, with |z| = 0, the curve P(0) is merely the single point p(0), which must be nonzero because p(z) is never zero. Thus p(0) must be distinct from the origin (0,0), which denotes 0 in the complex plane. The winding number of P(0) around the origin (0,0) is thus 0. Now changing R continuously will  deform the loop continuously. At some R the winding number must change. But that can only happen if the curve P(R) includes the origin (0,0) for some R. But then for some z on that circle |z| = R we have p(z) = 0, contradicting our original assumption. Therefore, p(z) has at least one zero.\n\nAlgebraic proofs\nThese proofs use two facts about real numbers that require only a small amount of analysis (more precisely, the intermediate value theorem):\n every polynomial with odd degree and real coefficients has some real root;\n every non-negative real number has a square root.\n\nThe second fact, together with the quadratic formula, implies the theorem for real quadratic polynomials. In other words, algebraic proofs of the fundamental theorem actually show that if R is any real-closed field, then its extension C = R() is algebraically closed.\n\nAs mentioned above, it suffices to check the statement \"every non-constant polynomial p(z) with real coefficients has a complex root\". This statement can be proved by induction on the greatest non-negative integer k such that 2k divides the degree n of p(z). Let a be the coefficient of zn in p(z) and let F be a splitting field of p(z) over C; in other words, the field F contains C and there are elements z1, z2, ..., zn in F such that\n\nIf k = 0, then n is odd, and therefore p(z) has a real root. Now, suppose that n = 2km (with m odd and k > 0) and that the theorem is already proved when the degree of the polynomial has the form 2k − 1m′ with m′ odd. For a real number t, define:\n\nThen the coefficients of qt(z) are symmetric polynomials in the zi with real coefficients. Therefore, they can be expressed as polynomials with real coefficients in the elementary symmetric polynomials, that is, in −a1, a2, ..., (−1)nan. So qt(z) has in fact real coefficients. Furthermore, the degree of qt(z) is n(n − 1)/2 = 2k−1m(n − 1), and m(n − 1) is an odd number. So, using the induction hypothesis, qt has at least one complex root; in other words, zi + zj + tzizj is complex for two distinct elements i and j from {1, ..., n}. Since there are more real numbers than pairs (i, j), one can find distinct real numbers t and s such that zi + zj + tzizj and zi + zj + szizj are complex (for the same i and j). So, both zi + zj and zizj are complex numbers. It is easy to check that every complex number has a complex square root, thus every complex polynomial of degree 2 has a complex root by the quadratic formula. It follows that zi and zj are complex numbers, since they are roots of the quadratic polynomial z2 −  (zi + zj)z + zizj.\n\nJoseph Shipman showed in 2007 that the assumption that odd degree polynomials have roots is stronger than necessary; any field in which polynomials of prime degree have roots is algebraically closed (so \"odd\" can be replaced by \"odd prime\" and furthermore this holds for fields of all characteristics). For axiomatization of algebraically closed fields, this is the best possible, as there are counterexamples if a single prime is excluded. However, these counterexamples rely on −1 having a square root. If we take a field where −1 has no square root, and every polynomial of degree n ∈ I has a root, where I is any fixed infinite set of odd numbers, then every polynomial f(x) of odd degree has a root (since  has a root, where k is chosen so that ). Mohsen Aliabadi generalized Shipman's result for any field in 2013, proving that the sufficient condition for an arbitrary field (of any characteristic) to be algebraically closed is having a root for any polynomial of prime degree.M. Aliabadi, M. R. Darafsheh, On maximal and minimal linear matching property, Algebra and discrete mathematics, Volume 15 (2013). Number 2. pp. 174–178\n\nAnother algebraic proof of the fundamental theorem can be given using Galois theory. It suffices to show that C has no proper finite field extension.A proof of the fact that this suffices can be seen here. Let K/C be a finite extension. Since the normal closure of K over R still has a finite degree over C (or R), we may assume without loss of generality that K is a normal extension of R (hence it is a Galois extension, as every algebraic extension of a field of characteristic 0 is separable). Let G be the Galois group of this extension, and let H be a Sylow 2-subgroup of G, so that the order of H is a power of 2, and the index of H in G is odd. By the fundamental theorem of Galois theory, there exists a subextension L of K/R such that Gal(K/L) = H. As [L:R] = [G:H] is odd, and there are no nonlinear irreducible real polynomials of odd degree, we must have L = R, thus [K:R] and [K:C] are powers of 2. Assuming by way of contradiction that [K:C] > 1, we conclude that the 2-group Gal(K/C) contains a subgroup of index 2, so there exists a subextension M of C of degree 2. However, C has no extension of degree 2, because every quadratic complex polynomial has a complex root, as mentioned above. This shows that [K:C] = 1, and therefore K = C, which completes the proof.\n\nGeometric proofs\nThere exists still another way to approach the fundamental theorem of algebra, due to J. M. Almira and A. Romero: by Riemannian geometric arguments. The main idea here is to prove that the existence of a non-constant polynomial p(z) without zeros implies the existence of a flat Riemannian metric over the sphere S2. This leads to a contradiction, since the sphere is not flat.\n\nA Riemannian surface (M, g) is said to be flat if its Gaussian curvature, which we denote by Kg, is identically null. Now, Gauss–Bonnet theorem, when applied to the sphere S2, claims that\n\nwhich proves that the sphere is not flat.\n\nLet us now assume that n > 0 and \n\nfor each complex number z. Let us define \n\nObviously, p*(z) ≠ 0 for all z in C. Consider the polynomial f(z) = p(z)p*(z). Then f(z) ≠ 0 for each z in C. Furthermore,\n\nWe can use this functional equation to prove that g, given by\n\nfor w in C, and\n\nfor w ∈ S2\\{0}, is a well defined Riemannian metric over the sphere S2 (which we identify with the extended complex plane C ∪ {∞}).\n\nNow, a simple computation shows that\n\nsince the real part of an analytic function is harmonic. This proves that Kg = 0.\n\nCorollaries\nSince the fundamental theorem of algebra can be seen as the statement that the field of complex numbers is algebraically closed, it follows that any theorem concerning algebraically closed fields applies to the field of complex numbers. Here are a few more consequences of the theorem, which are either about the field of real numbers or about the relationship between the field of real numbers and the field of complex numbers:\n\n The field of complex numbers is the algebraic closure of the field of real numbers.\n\n Every polynomial in one variable z with complex coefficients is the product of a complex constant and polynomials of the form z + a with a complex.\n\n Every polynomial in one variable x with real coefficients can be uniquely written as the product of a constant, polynomials of the form x + a with a real, and polynomials of the form x2 + ax + b with a and b real and a2 − 4b < 0 (which is the same thing as saying that the polynomial x2 + ax + b has no real roots). (By the Abel–Ruffini theorem, the real numbers a and b are not necessarily expressible in terms of the coefficients of the polynomial, the basic arithmetic operations and the extraction of n-th roots.) This implies that the number of non-real complex roots is always even and remains even when counted with their multiplicity.\n\n Every rational function in one variable x, with real coefficients, can be written as the sum of a polynomial function with rational functions of the form a/(x − b)n (where n is a natural number, and a and b are real numbers), and rational functions of the form (ax + b)/(x2 + cx + d)n (where n is a natural number, and a, b, c, and d are real numbers such that c2 − 4d < 0). A corollary of this is that every rational function in one variable and real coefficients has an elementary primitive.\n\n Every algebraic extension of the real field is isomorphic either to the real field or to the complex field.\n\nBounds on the zeros of a polynomial\n\nWhile the fundamental theorem of algebra states a general existence result, it is of some interest, both from the theoretical and from the practical point of view, to have information on the location of the zeros of a given polynomial. The simpler result in this direction is a bound on the modulus: all zeros ζ of a monic polynomial  satisfy an inequality |ζ| ≤ R∞, where\n\nNotice that, as stated, this is not yet an existence result but rather an example of what is called an a priori bound: it says that if there are solutions then they lie inside the closed disk of center the origin and radius R∞. However, once coupled with the fundamental theorem of algebra it says that the disk contains in fact at least one solution. More generally, a bound can be given directly in terms of any p-norm of the n-vector of coefficients  that is |ζ| ≤ Rp, where Rp is precisely the q-norm of the 2-vector  q being the conjugate exponent of p,  for any 1 ≤ p ≤ ∞. Thus, the modulus of any solution is also bounded by\n\nfor 1 < p < ∞, and in particular\n\n(where we define an to mean 1, which is reasonable since 1 is indeed the n-th coefficient of our polynomial). The case of a generic polynomial of degree n, \n\nis of course reduced to the case of a monic, dividing all coefficients by an ≠ 0. Also, in case that 0 is not a root, i.e. a0 ≠ 0, bounds from below on the roots ζ follow immediately as bounds from above on , that is, the roots of \n\n \n\nFinally, the distance  from the roots ζ to any point  can be estimated from below and above, seeing  as zeros of the polynomial , whose coefficients are the Taylor expansion of P(z) at \n\nLet ζ be a root of the polynomial \n\nin order to prove the inequality |ζ| ≤ Rp we can assume, of course, |ζ| > 1. Writing the equation as \n\n \n\nand using the Hölder's inequality we find \n\nNow, if p = 1, this is \n\nthus \n\n \n\nIn the case 1 < p ≤ ∞, taking into account the summation formula for a geometric progression, we have\n\nthus \n\n \n\nand simplifying, \n\n \n\nTherefore \n\n \n\nholds, for all 1 ≤ p ≤ ∞.\n\nReferences\n\nHistoric sources\n (tr. Course on Analysis of the Royal Polytechnic Academy, part 1: Algebraic Analysis)\n . English translation: \n  (tr. New proof of the theorem that every integral rational algebraic function of one variable can be resolved into real factors of the first or second degree).\n \n – first proof.\n – second proof.\n – third proof.\n – fourth proof.\n  (The Fundamental Theorem of Algebra and Intuitionism).\n  (tr. An extension of a work of Hellmuth Kneser on the Fundamental Theorem of Algebra).\n  (tr. On the first and fourth Gaussian proofs of the Fundamental Theorem of Algebra).\n  (tr. New proof of the theorem that every integral rational function of one variable can be represented as a product of linear functions of the same variable).\n\nRecent literature\n \n \n \n \n \n \n  (tr. On the history of the fundamental theorem of algebra: theory of equations and integral calculus.)\n  (tr. The rational functions §80–88: the fundamental theorem).\n \n \n  \n \n \n  – English translation of Gauss's second proof.\n \n\nSee also\n\nExternal links\n Algebra, fundamental theorem of at Encyclopaedia of Mathematics\n Fundamental Theorem of Algebra — a collection of proofs\n D. J. Velleman: The Fundamental Theorem of Algebra: A Visual Approach, PDF (unpublished paper), visualisation of d'Alembert's, Gauss's and the winding number proofs\n From the Fundamental Theorem of Algebra to Astrophysics: A \"Harmonious\" Path\n \n \n Mizar system proof: http://mizar.org/version/current/html/polynom5.html#T74\n\nCategory:Articles containing proofs\nCategory:Field theory\nAlgebra\nCategory:Theorems in algebra\nCategory:Theorems in complex analysis"
    },
    {
      "title": "Fundamental theorem of finitely generated abelian groups",
      "url": "https://en.wikipedia.org/wiki/Fundamental_theorem_of_finitely_generated_abelian_groups",
      "text": "REDIRECT Finitely generated abelian group#Classification\n\nCategory:Theorems in algebra"
    },
    {
      "title": "Fundamental theorem of Galois theory",
      "url": "https://en.wikipedia.org/wiki/Fundamental_theorem_of_Galois_theory",
      "text": "In mathematics, the fundamental theorem of Galois theory is a result that describes the structure of certain types of field extensions.\n\nIn its most basic form, the theorem asserts that given a field extension E/F that is finite and Galois, there is a one-to-one correspondence between its intermediate fields and subgroups of its Galois group. (Intermediate fields are fields K satisfying F ⊆ K ⊆ E; they are also called subextensions of E/F.)\n\nExplicit description of the correspondence\nFor finite extensions, the correspondence can be described explicitly as follows.\n For any subgroup H of Gal(E/F), the corresponding fixed field, denoted EH, is the set of those elements of E which are fixed by every automorphism in H.\n For any intermediate field K of E/F, the corresponding subgroup is Aut(E/K), that is, the set of those automorphisms in Gal(E/F) which fix every element of K.\n\nThe fundamental theorem says that this correspondence is a one-to-one correspondence if (and only if) E/F is a Galois extension.\nFor example, the topmost field E corresponds to the trivial subgroup of Gal(E/F), and the base field F corresponds to the whole group Gal(E/F).\n\nThe notation Gal(E/F) is only used for Galois extensions. If E/F is Galois, then Gal(E/F) = Aut(E/F). If E/F is not Galois, then the \"correspondence\" gives only an injective (but not surjective) map from subgroups of Aut(E/F) to subfields of E/F, and a surjective (but not injective) map in the reverse direction. In particular, if E/F is not Galois, then F is not the fixed field of any subgroup of Aut(E/F).\n\nProperties of the correspondence\nThe correspondence has the following useful properties.\n\n It is inclusion-reversing. The inclusion of subgroups H1 ⊆ H2 holds if and only if the inclusion of fields EH1 ⊇ EH2 holds.\n Degrees of extensions are related to orders of groups, in a manner consistent with the inclusion-reversing property. Specifically, if H is a subgroup of Gal(E/F), then |H| = [E:EH] and |Gal(E/F)|/|H| = [EH:F].\n The field EH is a normal extension of F (or, equivalently, Galois extension, since any subextension of a separable extension is separable) if and only if H is a normal subgroup of Gal(E/F). In this case, the restriction of the elements of Gal(E/F) to EH induces an isomorphism between Gal(EH/F) and the quotient group Gal(E/F)/H.\n\nExample 1\nthumb|600px|Lattice of subgroups and subfields\nConsider the field\n\nSince  is first determined by adjoining , then , each element of  can be written as:\n\nIts Galois group  can be determined by examining the automorphisms of  which fix . Each such automorphism must send  to either  or , and must send  to either  or  since the permutations in a Galois group can only permute the roots of an irreducible polynomial. Suppose that  exchanges  and , so\n\nand  exchanges  and ,  so\n\nThese are clearly automorphisms of . There is also the identity automorphism  which does not change anything, and the composition of  and  which changes the signs on both radicals:\n\nTherefore,\n\nand  is isomorphic to the Klein four-group. It has five subgroups, each of which correspond via the theorem to a subfield of . \n The trivial subgroup (containing only the identity element) corresponds to all of .\n The entire group  corresponds to the base field \n The two-element subgroup corresponds to the subfield  since  fixes .\n The two-element subgroup corresponds to the subfield  again since  fixes .\n The two-element subgroup corresponds to the subfield  since  fixes .\n\nExample 2\nthumb|600px|Lattice of subgroups and subfields\nThe following is the simplest case where the Galois group is not abelian.\n\nConsider the splitting field K of the polynomial  over  that is,  where θ is a cube root of 2, and ω is a cube root of 1 (but not 1 itself). For example, if we imagine K to be inside the field of complex numbers, we may take θ to be the real cube root of 2, and ω to be\n\nIt can be shown that the Galois group  has six elements, and is isomorphic to the group of permutations of three objects. It is generated by (for example) two automorphisms, say f and g, which are determined by their effect on θ and ω,\n\nand then\n\nThe subgroups of G and corresponding subfields are as follows:\n\n As usual, the entire group G corresponds to the base field  and the trivial group {1} corresponds to the whole field K.\n There is a unique subgroup of order 3, namely  The corresponding subfield is  which has degree 2 over  (the minimal polynomial of ω is ), corresponding to the fact that the subgroup has index two in G. Also, this subgroup is normal, corresponding to the fact that the subfield is normal over \n There are three subgroups of order 2, namely  and  corresponding respectively to the three subfields  These subfields have degree 3 over  again corresponding to the subgroups having index 3 in G. Note that the subgroups are not normal in G, and this corresponds to the fact that the subfields are not Galois over  For example,  contains only a single root of the polynomial  so it cannot be normal over \n\nExample 3\nLet  be the field of rational functions in  and let\n\nwhich is a group under composition, isomorphic to  (see: six cross-ratios).\nLet  be the fixed field of , then .\n\nIf  is a subgroup of  then the coefficients of the following polynomial\n\n \n\ngenerate the fixed field of . Galois correspondence means that every subfield of  can be constructed this way. For example, if  then the fixed field is  and if  then the fixed field is . Likewise, one can write , the fixed field of , as  where  is the -invariant.\n\nSimilar examples can be constructed for each of the symmetry groups of the platonic solids as these also have faithful actions on the projective line  and hence on .\n\nApplications\nThe theorem classifies the intermediate fields of E/F in terms of group theory. This translation between intermediate fields and subgroups is key\nto showing  that the general quintic equation is not solvable by radicals (see Abel–Ruffini theorem). One first determines the Galois groups of radical extensions (extensions of the form F(α) where α is an n-th root of some element of F), and then uses the fundamental theorem to show that solvable extensions correspond to solvable groups.\n\nTheories such as Kummer theory and class field theory are predicated on the fundamental theorem.\n\nInfinite case\nThere is also a version of the fundamental theorem that applies to infinite algebraic extensions, which are normal and separable. It involves defining a certain topological structure, the Krull topology, on the Galois group; only subgroups that are also closed sets are relevant in the correspondence.\n\nExternal links\n\nCategory:Field theory\nCategory:Group theory\nCategory:Galois theory\nCategory:Theorems in algebra\nGalois theory"
    },
    {
      "title": "Gershgorin circle theorem",
      "url": "https://en.wikipedia.org/wiki/Gershgorin_circle_theorem",
      "text": "In mathematics, the Gershgorin circle theorem may be used to bound the spectrum of a square matrix. It was first published by the Soviet mathematician Semyon Aronovich Gershgorin in 1931.  Gershgorin's name has been transliterated in several different ways, including Geršgorin, Gerschgorin, Gershgorin, Hershhorn, and Hirschhorn.\n\nStatement and proof\nLet  be a complex  matrix, with entries . For  let  be the sum of the absolute values of the non-diagonal entries in the -th row.  Let  be a closed disc centered at  with radius . Such a disc is called a Gershgorin disc.\n\nTheorem: Every eigenvalue of  lies within at least one of the Gershgorin discs \n\nProof: Let  be an eigenvalue of . Choose a corresponding eigenvector  so that one component  is equal to  and the others are of absolute value less than or equal to :  and  for . There always is such an , which can be obtained simply by dividing any eigenvector by its component with largest modulus. Since , in particular\n\n \n\nSo, splitting the sum and taking into account once again that , we get\n\n \n\nTherefore, applying the triangle inequality,\n\n \n\nCorollary: The eigenvalues of A must also lie within the Gershgorin discs Cj corresponding to the columns of A.\n\nProof: Apply the Theorem to AT.\n\nExample For a diagonal matrix, the Gershgorin discs coincide with the spectrum. Conversely, if the Gershgorin discs coincide with the spectrum, the matrix is diagonal.\n\nDiscussion\nOne way to interpret this theorem is that if the off-diagonal entries of a square matrix over the complex numbers have small norms, the eigenvalues of the matrix cannot be \"far from\" the diagonal entries of the matrix. Therefore, by reducing the norms of off-diagonal entries one can attempt to approximate the eigenvalues of the matrix. Of course, diagonal entries may change in the process of minimizing off-diagonal entries.\n\nThe theorem does not claim that there is one disc for each eigenvalue; if anything, the discs rather correspond to the axes in , and each expresses a bound on precisely those eigenvalues whose eigenspaces are closest to one particular axis. In the matrix\n \n— which by construction has eigenvalues , , and  with eigenvectors , , and  — it is easy to see that the disc for row 2 covers  and  while the disc for row 3 covers  and . This is however just a happy coincidence; if working through the steps of the proof one finds that it in each eigenvector is the first element that is the largest (every eigenspace is closer to the first axis than to any other axis), so the theorem only promises that the disc for row 1 (whose radius can be twice the sum of the other two radii) covers all three eigenvalues.\n\nStrengthening of the theorem\nIf one of the discs is disjoint from the others then it contains exactly one eigenvalue. If however it meets another disc it is possible that it contains no eigenvalue (for example,  or ). In the general case the theorem can be strengthened as follows:\n\nTheorem: If the union of k discs is disjoint from the union of the other n − k discs then the former union contains exactly k and the latter n − k eigenvalues of A.\n\nProof: Let D be the diagonal matrix with entries equal to the diagonal entries of A and let\n\n \n\nWe will use the fact that the eigenvalues are continuous in , and show that if any eigenvalue moves from one of the unions to the other, then it must be outside all the discs for some , which is a contradiction.\n\nThe statement is true for . The diagonal entries of  are equal to that of A, thus the centers of the Gershgorin circles are the same, however their radii are t times that of A. Therefore the union of the corresponding k discs of  is disjoint from the union of the remaining n-k for all . The discs are closed, so the distance of the two unions for A is . The distance for  is a decreasing function of t, so it is always at least d. Since the eigenvalues of  are a continuous function of t, for any eigenvalue  of  in the union of the k discs its distance  from the union of the other n-k discs is also continuous. Obviously , and assume  lies in the union of the n-k discs. Then , so there exists  such that . But this means  lies outside the Gershgorin discs, which is impossible. Therefore  lies in the union of the k discs, and the theorem is proven.\n\nRemarks:\n The continuity of  should be understand in the sense of topology. It is sufficient to show that the roots (as a point in space ) is continuous function of its coefficients. Note that the inverse map that maps roots to coefficients is described by Vieta's formulas (note for Characteristic polynomial ) which can be proved a open map. This proves the roots as a whole is a continuous function of its coefficients. Since composition of continuous functions is again continuous, the  as a composition of roots solver and  is also continuous.\n\n Individual eigenvalue  could merge with other eigenvalue(s) or appeared from a splitting of previous eigenvalue. This may confuse people and questioning the concept of continuous. However, when viewing from the space of eigenvalue set , the trajectory is still a continuous curve although not necessarily smooth everywhere.\n\n There are two types of continuity concerning eigenvalues: (1) each individual eigenvalue is a usual continuous function, (2) eigenvalues are continuous in the topological sense. Whichever continuity is used in a proof of the Gersgorin disk theorem, it should be justified that the sum of algebraic multiplicities of eigenvalues remains unchanged on each connected region. A proof using complex analysis (Argument Principle) is clear and mathematically sound. (See Horn and Johnson, Matrix Analysis, 2nd edition, Cambridge U Press.)\n\nApplication\nThe Gershgorin circle theorem is useful in solving matrix equations of the form Ax = b for x where b is a vector and A is a matrix with a large condition number.\n\nIn this kind of problem, the error in the final result is usually of the same order of magnitude as the error in the initial data multiplied by the condition number of A. For instance, if b is known to six decimal places and the condition number of A is 1000 then we can only be confident that x is accurate to three decimal places. For very high condition numbers, even very small errors due to rounding can be magnified to such an extent that the result is meaningless.\n\nIt would be good to reduce the condition number of A. This can be done by preconditioning: A matrix P such that P ≈ A−1 is constructed, and then the equation PAx = Pb is solved for x. Using the exact inverse of A would be nice but finding the inverse of a matrix is something we want to avoid because of the computational expense. \n\nNow, since PA ≈ I where I is the identity matrix, the eigenvalues of PA should all be close to 1. By the Gershgorin circle theorem, every eigenvalue of PA lies within a known area and so we can form a rough estimate of how good our choice of P was.\n\n Example \nUse the Gershgorin circle theorem to estimate the eigenvalues of:\nthumb|400px|right|This diagram shows the discs in yellow derived for the eigenvalues.\nThe first two disks overlap and their union contains two eigenvalues.  The third and fourth disks are disjoint from the others and contain one eigenvalue each.\n\nStarting with row one, we take the element on the diagonal, aii as the center for the disc.  We then take the remaining elements in the row and apply the formula:\n\n \n\nto obtain the following four discs:\n \n\nNote that we can improve the accuracy of the last two discs by applying the formula to the corresponding columns of the matrix, obtaining  and .\n\nThe eigenvalues are  9.8218,  8.1478,  1.8995, -10.86\n\nSee also\n For matrices with non-negative entries, see Perron–Frobenius theorem.\n Doubly stochastic matrix\n Hurwitz matrix\n Joel Lee Brenner\n Metzler matrix\n Muirhead's inequality\n Schur–Horn theorem\n\nReferences\n\n.\n. (Errata).\n. 1st ed., Prentice Hall, 1962.\n.\n\nExternal links\n\n Eric W. Weisstein. \"Gershgorin Circle Theorem.\" From MathWorld—A Wolfram Web Resource. \n Semyon Aranovich Gershgorin biography at  MacTutor\n\nCategory:Theorems in algebra\nCategory:Linear algebra\nCategory:Matrix theory\nCategory:Articles containing proofs"
    },
    {
      "title": "Haran's diamond theorem",
      "url": "https://en.wikipedia.org/wiki/Haran%27s_diamond_theorem",
      "text": "In mathematics, the Haran diamond theorem gives a general sufficient condition for a separable extension of a Hilbertian field to be Hilbertian. \n\n Statement of the diamond theorem \nthumb|right|field diagram of the diamond theorem\nLet K be a Hilbertian field and L a separable extension of K. Assume there exist two Galois extensions \nN and M of K such that L is contained in the compositum NM, but is contained in neither N nor M. Then L is Hilbertian.\n\nThe name of the theorem comes from the pictured diagram of fields, and was coined by Jarden.\n\n Some corollaries \n Weissauer's theorem \n\nThis theorem was firstly proved using non-standard methods by Weissauer. It was reproved by Fried using standard methods. The latter proof led Haran to his diamond theorem.  \n\nWeissauer's theorem \nLet K be a Hilbertian field, N a Galois extension of K, and L a finite proper extension of N. Then L is Hilbertian. \n\nProof using the diamond theorem\n\nIf L is finite over K, it is Hilbertian; hence we assume that L/K is infinite. Let x be a primitive element for L/N, i.e., L = N(x). \n\nLet M be the Galois closure of K(x). Then all the assumptions of the diamond theorem are satisfied, hence L is Hilbertian.\n\n Haran–Jarden condition \n\nAnother, preceding to the diamond theorem, sufficient permanence condition was given by Haran–Jarden:\nTheorem. \nLet K be a Hilbertian field and N, M two Galois extensions of K. Assume that neither contains the other. Then their compositum NM is Hilbertian.\n\nThis theorem has a very nice consequence: Since the field of rational numbers, Q is Hilbertian (Hilbert's irreducibility theorem), we get that the algebraic closure of Q is not the compositum of two proper Galois extensions.\n\n References \n.\n.\n\nCategory:Galois theory\nCategory:Theorems in algebra\nCategory:Number theory"
    },
    {
      "title": "Harish-Chandra isomorphism",
      "url": "https://en.wikipedia.org/wiki/Harish-Chandra_isomorphism",
      "text": "In mathematics, the Harish-Chandra isomorphism, introduced by ,\nis an isomorphism of commutative rings constructed in the theory of Lie algebras. The isomorphism maps the center Z(U(g)) of the universal enveloping algebra U(g) of a reductive Lie algebra g to the elements S(h)W of the symmetric algebra S(h) of a Cartan subalgebra h that are invariant under the Weyl group W.\n\nFundamental invariants\n\nLet n be the rank of g, which is the dimension of the Cartan subalgebra h. H. S. M. Coxeter observed that S(h)W is a polynomial algebra in n variables (see Chevalley–Shephard–Todd theorem for a more general statement). Therefore, the center of the universal enveloping algebra of a reductive Lie algebra is a polynomial algebra. The degrees of the generators are the degrees of the fundamental invariants given in the following table.\n Lie algebra  Coxeter number h  Dual Coxeter number  Degrees of fundamental invariants R  0  0  1 An  n + 1  n + 1  2, 3, 4, ..., n + 1 Bn  2n  2n − 1  2, 4, 6, ..., 2n Cn  2n  n + 1  2, 4, 6, ..., 2n Dn  2n − 2  2n − 2  n; 2, 4, 6, ..., 2n − 2 E6  12  12  2, 5, 6, 8, 9, 12 E7  18  18  2, 6, 8, 10, 12, 14, 18 E8  30  30  2, 8, 12, 14, 18, 20, 24, 30 F4  12  9  2, 6, 8, 12 G2  6  4  2, 6\n\nFor example, the center of the universal enveloping algebra of G2 is a polynomial algebra on generators of degrees 2 and 6.\n\nExamples\nIf g is the Lie algebra sl(2, R), then the center of the universal enveloping algebra is generated by the Casimir invariant of degree 2, and the Weyl group acts on the Cartan subalgebra, which is isomorphic to R, by negation, so the invariant of the Weyl group is simply the square of the generator of the Cartan subalgebra, which is also of degree 2.\n\n Introduction and setting \nLet g be a semisimple Lie algebra, h its Cartan subalgebra and λ, μ ∈ h* be two elements of the weight space and assume that a set of positive roots Φ+ have been fixed. Let Vλ, resp. Vμ be highest weight modules with highest weight λ, resp. μ.\n\n Central characters \nThe g-modules Vλ and Vμ are representations of the universal enveloping algebra U(g) and its center acts on the modules by scalar multiplication (this follows from the fact that the modules are generated by a highest weight vector). So, for v in Vλ and x in Z(U(g)),\n\nand similarly for Vμ.\n\nThe functions  are homomorphims to scalars called central characters.\n\nStatement of Harish-Chandra theorem\nFor any  λ, μ ∈ h*, the characters  if and only if λ+δ and μ+δ are on the same orbit of the Weyl group of h*, where δ is the half-sum of the positive roots.Humphreys (1972), p.130\n\nAnother closely related formulation is that the Harish-Chandra homomorphism from the center of the universal enveloping algebra Z(U(g)) to S(h)W (the elements of the symmetric algebra of the Cartan subalgebra fixed by the Weyl group) is an isomorphism.\n\n Applications \nThe theorem may be used to obtain a simple algebraic proof of Weyl's character formula for finite-dimensional representations.\n\nFurther, it is a necessary condition for the existence of a nonzero homomorphism of some highest weight modules (a homomorphism of such modules preserves central character). A simple consequence is that for Verma modules or generalized Verma modules Vλ with highest weight λ, there exist only finitely many weights μ such that  a nonzero homomorphism Vλ → Vμ exists.\n\nSee also\n Translation functor\n Infinitesimal character\n\n Notes \n\nReferences\n\nKnapp, Anthony, Lie groups beyond an introduction, Second edition, pages 300–303.\n\nCategory:Lie algebras\nCategory:Representation theory of Lie algebras\nCategory:Theorems in algebra"
    },
    {
      "title": "Haynsworth inertia additivity formula",
      "url": "https://en.wikipedia.org/wiki/Haynsworth_inertia_additivity_formula",
      "text": "In mathematics, the Haynsworth inertia additivity formula, discovered by Emilie Virginia Haynsworth (1916–1985), concerns the number of positive, negative, and zero eigenvalues of a Hermitian matrix and of block matrices into which it is partitioned.Haynsworth, E. V., \"Determination of the inertia of a partitioned Hermitian matrix\", Linear Algebra and its Applications, volume 1 (1968), pages 73–81\n\nThe inertia of a Hermitian matrix H is defined as the ordered triple\n\n \n\nwhose components are respectively the numbers of positive, negative, and zero eigenvalues of H.  Haynsworth considered a partitioned Hermitian matrix\n\n \n\nwhere H11 is nonsingular and H12* is the conjugate transpose of H12.  The formula states:\n\n \n\nwhere H/H11 is the Schur complement of H11 in H:\n\n \n\n See also \n Block matrix pseudoinverse\n\n Notes and references \n\nCategory:Linear algebra\nCategory:Matrix theory\nCategory:Theorems in algebra"
    },
    {
      "title": "Hilbert–Burch theorem",
      "url": "https://en.wikipedia.org/wiki/Hilbert%E2%80%93Burch_theorem",
      "text": "In mathematics, the Hilbert–Burch theorem describes the structure of some free resolutions of a quotient of a local or graded ring in the case that the quotient has projective dimension 2.  proved a version of this theorem for polynomial rings, and  proved a more general version. Several other authors later rediscovered and published variations of this theorem.  gives a statement and proof.\n\nStatement\nIf R is a local ring with an ideal I and \n \nis a free resolution of the R-module R/I, then m = n – 1 and the ideal I is aJ where a is a regular element of R and J, a depth-2 ideal, is the first Fitting ideal  of I, i.e., the ideal generated by the determinants of the minors of size m of the matrix of f.\n\nReferences\n\nCategory:Commutative algebra\nCategory:Theorems in algebra"
    },
    {
      "title": "Hilbert's irreducibility theorem",
      "url": "https://en.wikipedia.org/wiki/Hilbert%27s_irreducibility_theorem",
      "text": "In number theory, Hilbert's irreducibility theorem, conceived by David Hilbert, states that every finite number of irreducible polynomials in a finite number of variables and having rational number coefficients admit a common specialization of a proper subset of the variables to rational numbers such that all the polynomials remain irreducible.  This theorem is a prominent theorem in number theory.\n\n Formulation of the theorem \nHilbert's irreducibility theorem. Let\n\n \n\nbe irreducible polynomials in the ring\n\nThen there exists an r-tuple of rational numbers (a1,...,ar) such that\n\n \n\nare irreducible in the ring\n\nRemarks.\n It follows from the theorem that there are infinitely many r-tuples. In fact the set of all irreducible specializations, called Hilbert set, is large in many senses. For example, this set is Zariski dense in .\n There are always (infinitely many) integer specializations, i.e., the assertion of the theorem holds even if we demand  (a1,...,ar) to be integers.\n There are many Hilbertian fields, i.e., fields satisfying Hilbert's irreducibility theorem. For example, global fields are Hilbertian.Lang (1997) p.41\n The irreducible specialization property stated in the theorem is the most general. There are many reductions, e.g., it suffices to take  in the definition. A recent result of Bary-Soroker shows that for a field K to be Hilbertian it suffices to consider the case of  and  absolutely irreducible, that is, irreducible in the ring Kalg[X,Y], where Kalg is the algebraic closure of K.\n\n Applications \nHilbert's irreducibility theorem has numerous applications in number theory and algebra. For example:\n\n The inverse Galois problem, Hilbert's original motivation. The theorem almost immediately implies that if a finite group G can be realized as the Galois group of a Galois extension N of\n \nthen it can be specialized to a Galois extension N0 of the rational numbers with G as its Galois group.Lang (1997) p.42  (To see this, choose a monic irreducible polynomial f(X1,…,Xn,Y) whose root generates N over E. If f(a1,…,an,Y) is irreducible for some ai, then a root of it will generate the asserted N0.)\n\n Construction of elliptic curves with large rank.\n Hilbert's irreducibility theorem is used as a step in the Andrew Wiles proof of Fermat's last theorem.\nIf a polynomial  is a perfect square for all large integer values of x, then g(x) is the square of a polynomial in .  This follows from Hilbert's irreducibility theorem with  and\n .\n(More elementary proofs exist.)  The same result is true when \"square\" is replaced by \"cube\", \"fourth power\", etc.\n\n Generalizations \n\nIt has been reformulated and generalized extensively, by using the language of algebraic geometry. See thin set (Serre).\n\n References \n\n \nJ. P. Serre, Lectures on The Mordell-Weil Theorem, Vieweg, 1989.\nM. D. Fried and M. Jarden, Field Arithmetic, Springer-Verlag, Berlin, 2005.\nH. Völklein, Groups as Galois Groups, Cambridge University Press, 1996.\nG. Malle and B. H. Matzat, Inverse Galois Theory, Springer, 1999.\n\nCategory:Theorems in number theory\nCategory:Polynomials\nCategory:Theorems in algebra\nCategory:David Hilbert"
    },
    {
      "title": "Hochster–Roberts theorem",
      "url": "https://en.wikipedia.org/wiki/Hochster%E2%80%93Roberts_theorem",
      "text": "In algebra, the Hochster–Roberts theorem, introduced by ,  \nstates that rings of invariants of linearly reductive groups acting on regular rings are Cohen–Macaulay.\n\nIn other words,\nIf V is a rational representation of a linearly reductive group G over a field k, then there exist algebraically independent invariant homogeneous polynomials  such that  is a free finite graded module over .\n\n proved that if a variety over a field of characteristic 0 has rational singularities then so does its quotient by the action of a reductive group; this implies the Hochster–Roberts theorem in characteristic 0 as rational singularities are Cohen–Macaulay.\n\nIn characteristic p>0, there are examples of groups that are reductive (or even finite) acting on polynomial rings whose rings of invariants are not Cohen–Macaulay.\n\n References \n\n Mumford, D.; Fogarty, J.; Kirwan, F. Geometric invariant theory. Third edition. Ergebnisse der Mathematik und ihrer Grenzgebiete (2) (Results in Mathematics and Related Areas (2)), 34. Springer-Verlag, Berlin, 1994. xiv+292 pp.  \n\nCategory:Theorems in algebra"
    },
    {
      "title": "Hua's identity",
      "url": "https://en.wikipedia.org/wiki/Hua%27s_identity",
      "text": "In algebra, Hua's identity  states that for any elements a, b in a division ring,\n\nwhenever . Replacing  with  gives another equivalent form of the identity:\n\nAn important application of the identity is a proof of Hua's theorem. The theorem says that if  is a function between division rings and if  satisfies:\n\nthen  is either a homomorphism or an antihomomorphism. The theorem is important because of the connection to the fundamental theorem of projective geometry.\n\n Proof \n\n(Note the proof is valid in any ring as long as  are units.)\n\n References \n\n \n Jacobson, Nathan (2009), Basic Algebra 1 (2nd ed.), Dover, \n\nCategory:Theorems in algebra"
    },
    {
      "title": "Hudde's rules",
      "url": "https://en.wikipedia.org/wiki/Hudde%27s_rules",
      "text": "In mathematics, Hudde's rules are two properties of polynomial roots described by Johann Hudde.\n\n1. If r is a double root of the polynomial equation\n\nand if  are numbers in arithmetic progression, then r is also a root of\n\nThis definition is a form of the modern theorem that if r is a double root of ƒ(x) = 0, then r is a root of ƒ '(x) = 0.\n\n2. If for x = a the polynomial\n\ntakes on a relative maximum or minimum value, then a is a root of the equation\n\nThis definition is a modification of Fermat's theorem in the form that if ƒ(a) is a relative maximum or minimum value of a polynomial ƒ(x), then ƒ '(a) = 0, where ƒ ' is the derivative of ƒ.\n\nHudde was working with Frans van Schooten on a Latin edition of La Géométrie of René Descartes. In the 1659 edition of the translation, Hudde contributed two letters: \"Epistola prima de Redvctione Ǣqvationvm\" (pages 406 to 506), and \"Epistola secvnda de Maximus et Minimus\" (pages 507 to 16). These letters may be read by the Internet Archive link below.\n\nReferences\n Carl B. Boyer (1991) A History of Mathematics, 2nd edition, page 373, John Wiley & Sons.\n Robert Raymond Buss (1979) Newton's use of Hudde's Rule in his Development of the Calculus, Ph.D. Thesis Saint Louis University, ProQuest #302919262\n René Descartes (1659) La Géométria, 2nd edition via Internet Archive. \n Kirsti Pedersen (1980) §5 \"Descartes’s method of determining the normal, and Hudde’s rule\", chapter 2: \"Techniques of the calculus, 1630-1660\", pages 16—19 in From the Calculus to Set Theory edited by Ivor Grattan-Guinness Duckworth Overlook \n\nCategory:Rules\nCategory:Theorems in algebra\nCategory:Polynomials\nCategory:Calculus"
    },
    {
      "title": "Jacobson–Bourbaki theorem",
      "url": "https://en.wikipedia.org/wiki/Jacobson%E2%80%93Bourbaki_theorem",
      "text": "In algebra, the Jacobson–Bourbaki theorem is a theorem used to  extend Galois theory to field extensions that need not be separable. It was introduced by  for commutative fields and extended to non-commutative fields by , and  who credited the result to unpublished work by Nicolas Bourbaki. The extension of Galois theory to normal extensions is called the Jacobson–Bourbaki correspondence, which replaces the correspondence between some subfields of a field and some subgroups of a Galois group by a correspondence between some sub division rings of a division ring and some subalgebras of an algebra.\n\nThe Jacobson–Bourbaki theorem implies both the usual Galois correspondence for subfields of a Galois extension, and Jacobson's Galois correspondence for subfields of a purely inseparable extension of exponent at most 1.\n\nStatement\nSuppose that L is a division ring.\nThe Jacobson–Bourbaki theorem states that there is a natural 1:1 correspondence between:\nDivision rings K in L of finite index n (in other words L is a finite-dimensional left vector space over K).\nUnital K-algebras of finite dimension n (as K-vector spaces) contained in the ring of endomorphisms of the additive group of K.\n\nThe sub division ring and the corresponding subalgebra are each other's commutants.\n\n gave an extension to sub division rings that might have infinite index, which correspond to closed subalgebras in the finite topology.\n\nReferences\n\nCategory:Field theory\nCategory:Theorems in algebra"
    },
    {
      "title": "Koecher–Vinberg theorem",
      "url": "https://en.wikipedia.org/wiki/Koecher%E2%80%93Vinberg_theorem",
      "text": "In operator algebra, the Koecher–Vinberg theorem is a reconstruction theorem for real Jordan algebras. It was proved independently by Max Koecher in 1957 and Ernest Vinberg in 1961. It provides a one-to-one correspondence between formally real Jordan algebras and so-called domains of positivity. Thus it links operator algebraic and convex order theoretic views on state spaces of physical systems.\n\nStatement\nA convex cone  is called regular if  whenever both  and  are in the closure .\n\nA convex cone  in a vector space  with an inner product has a dual cone . The cone is called self-dual when . It is called homogeneous when to any two points  there is a real linear transformation  that restricts to a bijection  and satisfies .\n\nThe Koecher–Vinberg theorem now states that these properties precisely characterize the positive cones of Jordan algebras.\n\nTheorem: There is a one-to-one correspondence between formally real Jordan algebras and convex cones that are:\n open;\n regular;\n homogeneous;\n self-dual.\n\nConvex cones satisfying these four properties are called domains of positivity or symmetric cones. The domain of positivity associated with a real Jordan algebra  is the interior of the 'positive' cone .\n\nProof\nFor a proof, see  or .\n\nReferences\n\nCategory:Non-associative algebras\nCategory:Theorems in algebra"
    },
    {
      "title": "Krull–Akizuki theorem",
      "url": "https://en.wikipedia.org/wiki/Krull%E2%80%93Akizuki_theorem",
      "text": "In algebra, the Krull–Akizuki theorem states the following: let A be a one-dimensional reduced noetherian ring,In this article, a ring is commutative and has unity. K its total ring of fractions. If B is a subring of a finite extension L of K containing A and is not a field, then B is a one-dimensional noetherian ring. Furthermore, for every nonzero ideal I of B,  is finite over A.\n\nNote that the theorem does not say that B is finite over A. The theorem does not extend to higher dimension. One important consequence of the theorem is that the integral closure of a Dedekind domain A in a finite extension of the field of fractions of A is again a Dedekind domain. This consequence does generalize to a higher dimension: the Mori–Nagata theorem states that the integral closure of a noetherian domain is a Krull domain.\n\n Proof \nHere, we give a proof when . Let  be minimal prime ideals of A; there are finitely many of them. Let  be the field of fractions of  and  the kernel of the natural map . Then we have:\n.\nNow, if the theorem holds when A is a domain, then this implies that B is a one-dimensional noetherian domain since each  is and since . Hence, we reduced the proof to the case A is a domain. Let  be an ideal and let a be a nonzero element in the nonzero ideal  . Set . Since  is a zero-dim noetherian ring; thus, artinian, there is an l such that  for all . We claim\n\nSince it suffices to establish the inclusion locally, we may assume A is a local ring with the maximal ideal . Let x be a nonzero element in B. Then, since A is noetherian, there is an n such that  and so . Thus,\n\nNow, assume n is a minimum integer such that  and the last inclusion holds. If , then we easily see that . But then the above inclusion holds for , contradiction. Hence, we have  and this establishes the claim. It now follows:\n\nHence,  has finite length as A-module. In particular, the image of I there is finitely generated and so I is finitely generated. Finally, the above shows that  has zero dimension and so B has dimension one. \n\n References \n\nNicolas Bourbaki, Commutative algebra\n\nCategory:Theorems in algebra"
    },
    {
      "title": "Liouville's theorem (differential algebra)",
      "url": "https://en.wikipedia.org/wiki/Liouville%27s_theorem_%28differential_algebra%29",
      "text": "In mathematics, Liouville's theorem, originally formulated by Joseph Liouville in 1833 to 1841,... places an important restriction on antiderivatives that can be expressed as elementary functions.\n\nThe antiderivatives of certain elementary functions cannot themselves be expressed as elementary functions.  A standard example of such a function is  whose antiderivative is (with a multiplier of a constant) the error function, familiar from statistics.  Other examples include the functions  and .\n\nLiouville's theorem states that elementary antiderivatives, if they exist, must be in the same differential field as the function, plus possibly a finite number of logarithms.\n\n Definitions \n\nFor any differential field F, there is a subfield\n\n Con(F) = {f in F | Df = 0},\n\ncalled the constants of F. Given two differential fields F and G, G is called a logarithmic extension of F if G is a simple transcendental extension of F (i.e. G = F(t) for some transcendental t) such that\n\n Dt = Ds/s for some s in F.\n\nThis has the form of a logarithmic derivative. Intuitively, one may think of t as the logarithm of some element s of F, in which case, this condition is analogous to the ordinary chain rule.  However, F is not necessarily equipped with a unique logarithm; one might adjoin many \"logarithm-like\" extensions to F.  Similarly, an exponential extension is a simple transcendental extension that satisfies\n\n Dt = t Ds.\n\nWith the above caveat in mind, this element may be thought of as an exponential of an element s of F.  Finally, G is called an elementary differential extension of F if there is a finite chain of subfields from F to G where each extension in the chain is either algebraic, logarithmic, or exponential.\n\n Basic theorem \n\nSuppose F and G are differential fields, with Con(F) = Con(G), and that G is an elementary differential extension of F.  Let a be in F, y in G, and suppose Dy = a (in words, suppose that G contains an antiderivative of a).  Then there exist c1, ..., cn in Con(F), u1, ..., un, v in F such that\n\nIn other words, the only functions that have \"elementary antiderivatives\" (i.e. antiderivatives living in, at worst, an elementary differential extension of F) are those with this form.  Thus, on an intuitive level, the theorem states that the only elementary antiderivatives are the \"simple\" functions plus a finite number of logarithms of \"simple\" functions.\n\nA proof of Liouville's theorem can be found in section 12.4 of Geddes, et al.\n\n Examples \n\nAs an example, the field C(x) of rational functions in a single variable has a derivation given by the standard derivative with respect to that variable.  The constants of this field are just the complex numbers C.\n\nThe function , which exists in C(x), does not have an antiderivative in C(x).  Its antiderivatives ln x + C do, however, exist in the logarithmic extension C(x, ln x).\n\nLikewise, the function  does not have an antiderivative in C(x).  Its antiderivatives tan−1(x) + C do not seem to satisfy the requirements of the theorem, since they are not (apparently) sums of rational functions and logarithms of rational functions.  However, a calculation with Euler's formula  shows that in fact the antiderivatives can be written in the required manner (as logarithms of rational functions).\n\n Relationship with differential Galois theory \n\nLiouville's theorem is sometimes presented as a theorem in differential Galois theory, but this is not strictly true.  The theorem can be proved without any use of Galois theory.  Furthermore, the Galois group of a simple antiderivative is either trivial (if no field extension is required to express it), or is simply the additive group of the constants (corresponding to the constant of integration).  Thus, an antiderivative's differential Galois group does not encode enough information to determine if it can be expressed using elementary functions, the major condition of Liouville's theorem.\n\n See also \n\nAlgebraic function\nDifferential Galois theory\nElementary function\nRisch algorithm\nTranscendental function\n\nNotes\n\nReferences\n\n \n\nExternal links\n \n\nCategory:Field theory\nCategory:Differential algebra\nCategory:Differential equations\nCategory:Theorems in algebra"
    },
    {
      "title": "Mason–Stothers theorem",
      "url": "https://en.wikipedia.org/wiki/Mason%E2%80%93Stothers_theorem",
      "text": "The Mason–Stothers theorem, or simply Mason's theorem, is a mathematical theorem about polynomials, analogous to the abc conjecture for integers. It is named after , who published it in 1981,. and R. C. Mason, who rediscovered it shortly thereafter..\n\nThe theorem states:\nLet , , and  be relatively prime polynomials over a field such that  and such that not all of them have vanishing derivative. Then\n\nHere  is the product of the distinct irreducible factors of . For algebraically closed fields it is the polynomial of minimum degree that has the same roots as ; in this case  gives the number of distinct roots of .\n\nExamples\n\nOver fields of characteristic 0 the condition that , , and  do not all have vanishing derivative is equivalent to the condition that they are not all constant. Over fields of characteristic  it is not enough to assume that they are not all constant. For example, the identity  gives an example where the maximum degree of the three polynomials ( and  as the summands on the left hand side, and  as the right hand side) is , but the degree of the radical is only .\nTaking  and  gives an example where equality holds in the Mason–Stothers theorem, showing that the inequality is in some sense the best possible.\nA corollary of the Mason–Stothers theorem is the analog of Fermat's last theorem for function fields: if  for , ,  relatively prime polynomials over a field of characteristic not dividing   and  then either at least one of , , or  is 0 or they are all constant.\n\nProof\n gave the following elementary proof of the Mason–Stothers theorem..\n\nStep 1. The condition  implies that the Wronskians , , and  are all equal. Write  for their common value.\n\nStep 2. The condition that at least one of the derivatives , , or  is nonzero and that , , and  are coprime is used to show that  is nonzero.\nFor example, if  then  so  divides  (as  and  are coprime) so  (as  unless  is constant).\n\nStep 3.  is divisible by each of the greatest common divisors , , and . Since these are coprime it is divisible by their product, and since  is nonzero we get\n\nStep 4. Substituting in the inequalities\n − (number of distinct roots of )\n − (number of distinct roots of )\n − (number of distinct roots of )\n(where the roots are taken in some algebraic closure) and\n\nwe find that\n\nwhich is what we needed to prove.\n\nGeneralizations\nThere is a natural generalization in which the ring of polynomials is replaced by a one-dimensional function field.\nLet  be an algebraically closed field of characteristic 0, let  be a smooth projective curve\nof genus , let\n be rational functions on  satisfying ,\nand let\n be a set of points in  containing all of the zeros and poles of  and .\nThen\n\nHere the degree of a function in  is the degree of\nthe map it induces from  to P1.\nThis was proved by Mason, with an alternative short proof published the same year by J. H. Silverman\n.\n\nThere is a further generalization, due independently to J. F. Voloch\nand to\nW. D. Brownawell and D. W. Masser,\nthat gives an upper bound for  -variable -unit\nequations  provided that\nno subset of the  are -linearly dependent. Under this assumption, they prove that\n\nReferences\n\nExternal links\n\nMason-Stothers Theorem and the ABC Conjecture, Vishal Lama. A cleaned-up version of the proof from Lang's book.\n\nCategory:Theorems in algebra\nCategory:Polynomials"
    },
    {
      "title": "Matlis duality",
      "url": "https://en.wikipedia.org/wiki/Matlis_duality",
      "text": "In algebra, Matlis duality is a duality between Artinian and Noetherian modules over a complete Noetherian local ring. In the special case when the local ring has a field mapping to the residue field it is closely related to earlier work by Francis Sowerby Macaulay on polynomial rings and is sometimes called Macaulay duality, and the general case was introduced by .\n\nStatement\n\nSuppose that R is a Noetherian complete local ring with residue field k, and choose E to be an injective hull of k (sometimes called a Matlis module). The dual DR(M) of a module M is defined to be HomR(M,E). Then Matlis duality states that the duality functor DR gives an anti-equivalence between the categories of Artinian and Noetherian R-modules. In particular the duality functor gives an anti-equivalence from the category of finite-length modules to itself.\n\nExamples\n\nSuppose that the Noetherian complete local ring R has a subfield k that maps onto a subfield of finite index of its residue field R/m. Then the Matlis dual of any R-module is just its dual as a topological vector space over k, if the module is given its m-adic topology. In particular the dual of R as a topological vector space over k is a Matlis module. This case is closely related to work of Macaulay on graded polynomial rings and is sometimes called Macaulay duality.\n\nIf R is a discrete valuation ring with quotient field K then the Matlis module is K/R. In the special case when R is the ring of p-adic numbers, the Matlis dual of a finitely-generated module is the Pontryagin dual of it considered as a locally compact abelian group.\n\nIf R is a Cohen–Macaulay local ring of dimension d with dualizing module Ω, then the Matlis module is given by the local cohomology group H(Ω). In particular if R is an Artinian local ring then the Matlis module is the same as the dualizing module.\n\nExplanation using adjoint functors\nMatlis duality can be conceptually explained using the language of adjoint functors and derived categories:Paul Balmer, Ivo Dell'Ambrogio, and Beren Sanders.\nGrothendieck-Neeman duality and the Wirthmüller isomorphism, 2015. Example 7.2. the functor between the derived categories of R- and k-modules induced by regarding a k-module as an R-module, admits a right adjoint\n(derived internal Hom)\n\nThis right adjoint sends the injective hull  mentioned above to k, which is a dualizing object in . This abstract fact gives then rise to the above-mentioned equivalence.\n\nSee also\n\nGrothendieck local duality\n\nReferences\n\nCategory:Commutative algebra\nCategory:Theorems in algebra"
    },
    {
      "title": "Mitchell's embedding theorem",
      "url": "https://en.wikipedia.org/wiki/Mitchell%27s_embedding_theorem",
      "text": "Mitchell's embedding theorem, also known as the Freyd–Mitchell theorem or the full embedding theorem, is a result about abelian categories; it essentially states that these categories, while rather abstractly defined, are in fact concrete categories of modules. This allows one to use element-wise diagram chasing proofs in these categories. The theorem is named after Barry Mitchell and Peter Freyd.\n\nDetails\nThe precise statement is as follows: if A is a small abelian category, then there exists a ring R (with 1, not necessarily commutative) and a full, faithful and exact functor F: A → R-Mod (where the latter denotes the category of all left R-modules).\n\nThe functor F yields an equivalence between A and a full subcategory of R-Mod in such a way that kernels and cokernels computed in A correspond to the ordinary kernels and cokernels computed in R-Mod. Such an equivalence is necessarily additive.\nThe theorem thus essentially says that the objects of A can be thought of as R-modules, and the morphisms as R-linear maps, with kernels, cokernels, exact sequences and sums of morphisms being determined as in the case of modules. However, projective and injective objects in A do not necessarily correspond to projective and injective R-modules.\n\n Sketch of the proof \nLet  be the category of left exact functors from the abelian category  to the category of abelian groups . First we construct a contravariant embedding  by  for all , where  is the covariant hom-functor, . The Yoneda Lemma states that  is fully faithful and we also get the left exactness of  very easily because  is already left exact. The proof of the right exactness of  is harder and can be read in Swan, Lecture Notes in Mathematics 76.\n\nAfter that we prove that  is an abelian category by using localization theory (also Swan). This is the hard part of the proof.\n\nIt is easy to check that the abelian category  is an AB5 category with a generator \n.\nIn other words it is a Grothendieck category and therefore has an injective cogenerator .\n\nThe endomorphism ring  is the ring we need for the category of R-modules.\n\nBy  we get another contravariant, exact and fully faithful embedding  The composition  is the desired covariant exact and fully faithful embedding.\n\nNote that the proof of the Gabriel–Quillen embedding theorem for exact categories is almost identical.\n\n References \n\nCategory:Module theory\nCategory:Additive categories\nCategory:Theorems in algebra"
    },
    {
      "title": "Multinomial theorem",
      "url": "https://en.wikipedia.org/wiki/Multinomial_theorem",
      "text": "In mathematics, the multinomial theorem describes how to expand a power of a sum in terms of powers of the terms in that sum. It is the generalization of the binomial theorem from binomials to multinomials.\n\nTheorem\nFor any positive integer m and any nonnegative integer n, the multinomial formula tells us how a sum with m terms expands when raised to an arbitrary power n:\n\nwhere\n\nis a multinomial coefficient.  The sum is taken over all combinations of nonnegative integer indices k1 through km such that the sum of all ki is n.  That is, for each term in the expansion, the exponents of the xi must add up to n.  Also, as with the binomial theorem, quantities of the form x0 that appear are taken to equal 1 (even when x equals zero).\n\nIn the case m = 2, this statement reduces to that of the binomial theorem.\n\nExample\nThe third power of the trinomial a + b + c is given by\n\nThis can be computed by hand using the distributive property of multiplication over addition, but it can also be done (perhaps more easily) with the multinomial theorem, which gives us a simple formula for any coefficient we might want. It is possible to \"read off\" the multinomial coefficients from the terms by using the multinomial coefficient formula. For example:\n\n has the coefficient \n has the coefficient \n\nAlternate expression\nThe statement of the theorem can be written concisely using multiindices:\n\nwhere \n\nand\n\nProof\nThis proof of the multinomial theorem uses the binomial theorem and induction on m.\n\nFirst, for m = 1, both sides equal x1n since there is only one term k1 = n in the sum. For the induction step, suppose the multinomial theorem holds for m.  Then\n\n \n\nby the induction hypothesis.  Applying the binomial theorem to the last factor,\n\nwhich completes the induction.  The last step follows because\n\nas can easily be seen by writing the three coefficients using factorials as follows:\n\nMultinomial coefficients\nThe numbers\n\nappearing in the theorem are the multinomial coefficients.  They can be expressed in numerous ways, including as a product of binomial coefficients or of factorials:\n\nSum of all multinomial coefficients\nThe substitution of xi = 1 for all i into the multinomial theorem\n\ngives immediately that \n\nNumber of multinomial coefficients\n\nThe number of terms in a multinomial sum, #n,m, is equal to the number of monomials of degree n on the variables x1, …, xm:\n\nThe count can be performed easily using the method of stars and bars.\n\nValuation of multinomial coefficients\nThe largest power of a prime  that divides a multinomial coefficient may be computed using a generalization of Kummer's theorem.\n\nInterpretations\n\nWays to put objects into bins\nThe multinomial coefficients have a direct combinatorial interpretation, as the number of ways of depositing n distinct objects into m distinct bins, with k1 objects in the first bin, k2 objects in the second bin, and so on.\n\nNumber of ways to select according to a distribution\nIn statistical mechanics and combinatorics if one has a number distribution of labels then the multinomial coefficients naturally arise from the binomial coefficients. Given a number distribution  {ni} on a set of N total items, ni represents the number of items to be given the label i.  (In statistical mechanics i is the label of the energy state.)\n\nThe number of arrangements is found by \nChoosing n1 of the total N to be labeled 1.  This can be done  ways.\nFrom the remaining N − n1 items choose n2 to label 2.  This can be done  ways.\nFrom the remaining N − n1 − n2 items choose n3 to label 3.  Again, this can be done  ways.\n\nMultiplying the number of choices at each step results in:\n\nUpon cancellation, we arrive at the formula given in the introduction.\n\nNumber of unique permutations of words\nThe multinomial coefficient is also the number of distinct ways to permute a multiset of n elements, and ki are the multiplicities of each of the distinct elements. For example, the number of distinct permutations of the letters of the word MISSISSIPPI, which has 1 M, 4 Is, 4 Ss, and 2 Ps is\n\n(This is just like saying that there are 11! ways to permute the letters—the common interpretation of factorial as the number of unique permutations. However, we created duplicate permutations, because some letters are the same, and must divide to correct our answer.)\n\nGeneralized Pascal's triangle\nOne can use the multinomial theorem to generalize Pascal's triangle or Pascal's pyramid to Pascal's simplex. This provides a quick way to generate a lookup table for multinomial coefficients.\n\nSee also\n Multinomial distribution\n Stars and bars (combinatorics)\n\nReferences\n\nCategory:Factorial and binomial topics\nCategory:Articles containing proofs\nCategory:Theorems in algebra"
    },
    {
      "title": "Nagata's conjecture",
      "url": "https://en.wikipedia.org/wiki/Nagata%27s_conjecture",
      "text": "In algebra, Nagata's conjecture states that Nagata's automorphism of the polynomial ring k[x,y,z] is wild. The conjecture was proposed by  and proved by .\n\nNagata's automorphism is given by\n\nReferences\n\nCategory:Field theory\nCategory:Theorems in algebra"
    },
    {
      "title": "Nielsen–Schreier theorem",
      "url": "https://en.wikipedia.org/wiki/Nielsen%E2%80%93Schreier_theorem",
      "text": "In group theory, a branch of mathematics, the Nielsen–Schreier theorem states that every subgroup of a free group is itself free., Corollary 2.9, p. 95. It is named after Jakob Nielsen and Otto Schreier.\n\nStatement of the theorem\nA free group may be defined from a group presentation consisting of a set of generators with no relations. That is, every element is a product of some sequence of generators and their inverses, but these elements do not obey any equations except those trivially following from  = 1. The elements of a free group may be described as all possible reduced words, those strings of generators and their inverses in which no generator is adjacent to its own inverse. Two reduced words may be multiplied by concatenating them and then removing any generator-inverse pairs that result from the concatenation.\n\nThe Nielsen–Schreier theorem states that if H is a subgroup of a free group G, then H is itself isomorphic to a free group. That is, there exists a set S of elements which generate H, with no nontrivial relations among the elements of S.\n\nThe Nielsen–Schreier formula, or Schreier index formula, quantifies the result in the case where the subgroup has finite index: if G is a free group of rank n (free on n generators), and H is a subgroup of finite index [G : H] = e, then H is free of rank .\n\nExample\nLet G be the free group with two generators , and let H be the subgroup consisting of all reduced words of even length (products of an even number of letters ). Then H is generated by its six elements  A factorization of any reduced word in H into these generators and their inverses may be constructed simply by taking consecutive pairs of letters in the reduced word. However, this is not a free presentation of H because the last three generators can be written in terms of the first three as . Rather, H is generated as a free group by the three elements which have no relations among them; or instead by several other triples of the six generators., ex. 15, p. 12. Further, G is free on n = 2 generators, H has index e = [G : H] = 2 in G, and H is free on 1 +  e(n–1) = 3 generators.  The Nielsen–Schreier theorem states that like H, every subgroup of a free group can be generated as a free group, and if the index of H is finite, its rank is given by the index formula.\n\n Proof \n[[File:Covering-Graph.png|thumb|300x300px|The free group G = π1(X) has n = 2 generators corresponding to loops a,b from the base point P in X. The subgroup H of even-length words, with index e = [G : H] = 2, corresponds to the covering graph Y with two vertices corresponding to the cosets H and H''' = aH = bH = a−1H = b−1H, and two lifted edges for each of the original loop-edges a,b. Contracting one of the edges of Y gives a homotopy equivalence to a bouquet of three circles, so that H = π1(Y) is a free group on three generators, for example aa, ab, ba.|alt=]]\nA short and intuitive proof of the Nielsen–Schreier theorem uses the algebraic topology of fundamental groups and covering spaces., Section 2.2.4, The Nielsen–Schreier Theorem, pp. 103–104. A free group G on a set of generators is the fundamental group of a bouquet of circles, a topological graph X with a single vertex and with a loop-edge for each generator. Any subgroup H of the fundamental group is itself the fundamental group of a connected covering space Y → X. The space Y is a (possibly infinite) topological graph, the Schreier coset graph having one vertex for each coset in G/H., Section 2.2.2, The Subgroup Property, pp. 100–101. In any connected topological graph, it is possible to shrink the edges of a spanning tree of the graph, producing a bouquet of circles that has the same fundamental group H. Since H is the fundamental group of a bouquet of circles, it is itself free., Section 2.1.8, Freeness of the Generators, p. 97.\n\nSimplicial homology allows the computation of the rank of H, which is equal to h1(Y), the first Betti number of the covering space, the number of independent cycles. For G free of rank n, the graph X has n edges and 1 vertex; assuming H has finite index [G : H] = e, the covering graph Y has en edges and e vertices.  The first Betti number of a graph is equal to the number of edges, minus the number of vertices, plus the number of connected components; hence the rank of H is:\n\nThis proof is due to ; the original proof by Schreier forms the Schreier graph in a different way as a quotient of the Cayley graph of  modulo the action of .\n\nAccording to Schreier's subgroup lemma, a set of generators for a free presentation of  may be constructed from cycles in the covering graph formed by concatenating a spanning tree path from a base point (the coset of the identity) to one of the cosets, a single non-tree edge, and an inverse spanning tree path from the other endpoint of the edge back to the base point., Section 2.2.6, Schreier Transversals, pp. 105–106.\n\nAxiomatic foundations\nAlthough several different proofs of the Nielsen–Schreier theorem are known, they all depend on the axiom of choice. In the proof based on fundamental groups of bouquets, for instance, the axiom of choice appears in the guise of the statement that every connected graph has a spanning tree. The use of this axiom is necessary, as there exist models of Zermelo–Fraenkel set theory in which the axiom of choice and the Nielsen–Schreier theorem are both false. The Nielsen–Schreier theorem in turn implies a weaker version of the axiom of choice, for finite sets..\n\nHistory\nThe Nielsen–Schreier theorem is a non-abelian analogue of an older result of Richard Dedekind, that every subgroup of a free abelian group is free abelian., Section 2, The Nielsen–Schreier Theorem, pp. 9–23.\n\n originally proved a restricted form of the theorem, stating that any finitely-generated subgroup of a free group is free. His proof involves performing a sequence of Nielsen transformations on the subgroup's generating set that reduce their length (as reduced words in the free group from which they are drawn)., Section 3.2, A Reduction Process, pp. 121–140. Otto Schreier proved the Nielsen–Schreier theorem in its full generality in his 1926 habilitation thesis, Die Untergruppen der freien Gruppe, also published in 1927 in Abh. math. Sem. Hamburg. Univ.''.\n\nThe topological proof based on fundamental groups of bouquets of circles is due to . Another topological proof, based on the Bass–Serre theory of group actions on trees, was published by ., The Nielsen–Schreier Theorem, pp. 383–387.\n\nSee also\nFundamental theorem of cyclic groups, a similar result for cyclic groups that in the infinite case may be seen as a special case of the Nielsen–Schreier theorem\n\nNotes\n\nReferences\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n\nCategory:Properties of groups\nCategory:Axiom of choice\nCategory:Theorems in algebra"
    },
    {
      "title": "Niven's theorem",
      "url": "https://en.wikipedia.org/wiki/Niven%27s_theorem",
      "text": "In mathematics, Niven's theorem, named after Ivan Niven, states that the only rational values of θ in the interval 0° ≤ θ ≤ 90°  for which the sine of θ degrees is also a rational number are:\n\nIn radians, one would require that 0 ≤ x ≤ /2, that x/ be rational, and that sin x be rational.  The conclusion is then that the only such values are sin 0 = 0, sin /6 = 1/2, and sin /2 = 1.\n\nThe theorem appears as Corollary 3.12 in Niven's book on irrational numbers.\n\nThe theorem extends to the other trigonometric functions as well. For rational values of  θ, the only rational values of the sine or cosine are 0, ±1/2, and ±1; the only rational values of the secant or cosecant are ±1 and ±2; and the only rational values of the tangent or cotangent are 0 and ±1.A proof for the cosine case appears as Lemma 12 in \n\nSee also\nPythagorean triples form right triangles where the trigonometric functions will always take rational values, though the acute angles are not rational\n Trigonometric functions\n Trigonometric number\n\nReferences\n\nFurther reading\n \n \n\nExternal links\n \n \n\nCategory:Rational numbers\nCategory:Trigonometry\nCategory:Theorems in geometry\nCategory:Theorems in algebra"
    },
    {
      "title": "Norm residue isomorphism theorem",
      "url": "https://en.wikipedia.org/wiki/Norm_residue_isomorphism_theorem",
      "text": "In mathematics,  the norm residue isomorphism theorem is a long-sought result relating Milnor K-theory and Galois cohomology. The result has a relatively elementary formulation and at the same time represents the key juncture in the proofs of many seemingly unrelated theorems from abstract algebra, theory of quadratic forms, algebraic K-theory and the theory of motives. The theorem asserts that a certain statement  holds true for any prime  and any natural number . John MilnorMilnor (1970) speculated that this theorem might be true for  and all , and this question became known as Milnor's conjecture. The general case was conjectured by Spencer Bloch and Kazuya Kato Bloch, Spencer and Kato, Kazuya, \"p-adic étale cohomology\", Inst. Hautes Études Sci. Publ. Math. No. 63 (1986), p. 118 and became known as the Bloch–Kato conjecture or  the motivic Bloch–Kato conjecture to distinguish it from the Bloch–Kato conjecture on values of L-functions.Bloch, Spencer and Kato, Kazuya, \"L-functions and Tamagawa numbers of motives\", The Grothendieck Festschrift, Vol. I, 333–400, Progr. Math., 86, Birkhäuser Boston, Boston, MA, 1990.  The norm residue isomorphism theorem was proved by Vladimir Voevodsky using a number of highly innovative results of Markus Rost.\n\nStatement\nFor any integer ℓ invertible in a field  there is a map\n\nwhere  denotes the Galois module of ℓ-th roots of unity in some separable closure of k.  It induces an isomorphism .  The first hint that this is related to K-theory is that  is the group K1(k).  Taking the tensor products and applying the multiplicativity of étale cohomology yields an extension of the map  to maps:\n\nThese maps have the property that, for every element a in ,  vanishes.  This is the defining relation of Milnor K-theory.  Specifically, Milnor K-theory is defined to be the graded parts of the ring:\n\nwhere  is the tensor algebra of the multiplicative group  and the quotient is by the two-sided ideal generated by all elements of the form .  Therefore the map  factors through a map:\n\nThis map is called the Galois symbol or norm residue map.Srinivas (1996) p.146Gille & Szamuely (2006) p.108Efrat (2006) p.221  Because étale cohomology with mod-ℓ coefficients is an ℓ-torsion group, this map additionally factors through .\n\nThe norm residue isomorphism theorem (or Bloch–Kato conjecture) states that for a field k and an integer ℓ that is invertible in k, the norm residue map\n\nfrom Milnor K-theory  mod-ℓ to étale cohomology is an isomorphism.  The case  is the Milnor conjecture, and the case  is the Merkurjev–Suslin theorem.Srinivas (1996) pp.145-193\n\nHistory\n\nThe étale cohomology of a field is identical to Galois cohomology, so the conjecture equates the ℓth cotorsion (the quotient by the subgroup of ℓ-divisible elements) of the Milnor K-group of a field k with the Galois cohomology of k with coefficients in the Galois module of ℓth roots of unity.  The point of the conjecture is that there are properties that are easily seen for Milnor K-groups but not for Galois cohomology, and vice versa; the norm residue isomorphism theorem makes it possible to apply techniques applicable to the object on one side of the isomorphism to the object on the other side of the isomorphism.\n\nThe case when n is 0 is trivial, and the case when  follows easily from Hilbert's Theorem 90.  The case  and  was proved by .  An important advance was the case  and ℓ arbitrary.  This case was proved by  and is known as the Merkurjev–Suslin theorem.  Later, Merkurjev and Suslin, and independently, Rost, proved the case  and   .\n\nThe name \"norm residue\" originally referred to the Hilbert symbol , which takes values in the Brauer group of k (when the field contains all ℓ-th roots of unity).  Its usage here is in analogy with standard local class field theory and is expected to be part of an (as yet undeveloped) \"higher\" class field theory.\n\nThe norm residue isomorphism theorem implies the Quillen–Lichtenbaum conjecture.  It is equivalent to a theorem whose statement was once referred to as the Beilinson–Lichtenbaum conjecture.\n\nHistory of the proof\n\nMilnor's conjecture was proved by Vladimir Voevodsky.Voevodsky, Vladimir, \"Motivic cohomology with Z/2-coefficients\", Publ. Math. Inst. Hautes Études Sci. No. 98 (2003), 59–104. \nLater Voevodsky proved the general Bloch–Kato conjecture.Voevodsky (2010)\n\nThe starting point for the proof is a series of conjectures due to  and . They conjectured the existence of motivic complexes, complexes of sheaves whose cohomology was related to motivic cohomology.  Among the conjectural properties of these complexes were three properties: one connecting their Zariski cohomology to Milnor's K-theory, one connecting their etale cohomology to cohomology with coefficients in the sheaves of roots of unity and one connecting their Zariski cohomology to their etale cohomology. These three properties implied, as a very special case, that the norm residue map should be an isomorphism. The essential characteristic of the proof is that it uses the induction on the \"weight\" (which equals the dimension of the cohomology group in the conjecture) where the inductive step requires knowing not only the statement of Bloch-Kato conjecture but the much more general statement that contains a large part of the Beilinson-Lichtenbaum conjectures. It often occurs in proofs by induction that the statement being proved has to be strengthened in order to prove the inductive step. In this case the strengthening that was needed required the development of a very large amount of new mathematics.\n\nThe earliest proof of Milnor's conjecture is contained in a 1995 preprint of Voevodsky and is inspired by the idea that there should be algebraic analogs of Morava K-theory (these algebraic Morava K-theories were later constructed by Simone BorghesiBorghesi (2000)). In a 1996 preprint, Voevodsky was able to remove Morava K-theory from the picture by introducing instead algebraic cobordisms  and using some of their properties that were not proved at that time (these properties were proved later). The constructions of 1995 and 1996 preprints are now known to be correct but the first completed proof of Milnor's conjecture used a somewhat different scheme.\n\nIt is also the scheme that the proof of the full Bloch–Kato conjecture follows. It was devised by Voevodsky a few months after the 1996 preprint appeared. Implementing this scheme required making substantial advances in the field of motivic homotopy theory as well as finding a way to build algebraic varieties with a specified list of properties.  From the motivic homotopy theory the proof required the following:\nA construction of the motivic analog of the basic ingredient of the Spanier–Whitehead duality in the form of the motivic fundamental class as a morphism from the motivic sphere to the Thom space of the motivic normal bundle over a smooth projective algebraic variety.\nA construction of the motivic analog of the Steenrod algebra.\nA proof of the proposition stating that over a field of characteristic zero the motivic Steenrod algebra characterizes all bi-stable cohomology operations in the motivic cohomology.\n\nThe first two constructions were developed by Voevodsky by 2003. Combined with the results that had been known since late 1980s, they were sufficient to reprove the Milnor conjecture.\n\nAlso in 2003, Voevodsky published on the web a preprint that nearly contained a proof of the general theorem.  It followed the original scheme but was missing the proofs of three statements.  Two of these statements were related to the properties of the motivic Steenrod operations and required the third fact above, while the third one required then-unknown facts about \"norm varieties\". The properties that these varieties were required to have had been formulated by Voevodsky in 1997, and the varieties themselves had been constructed by Markus Rost in 1998–2003.  The proof that they have the required properties was completed by Andrei Suslin and Seva Joukhovitski in 2006.\n\nThe third fact above required the development of new techniques in motivic homotopy theory.  The goal was to prove that a functor, which was not assumed to commute with limits or colimits, preserved weak equivalences between objects of a certain form. One of the main difficulties there was that the standard approach to the study of weak equivalences is based on Bousfield–Quillen factorization systems and model category structures, and these were inadequate.  Other methods had to be developed, and this work was completed by Voevodsky only in 2008.\n\nIn the course of developing these techniques, it became clear that the first statement used without proof in Voevodsky's 2003 preprint is false. The proof had to be modified slightly to accommodate the corrected form of that statement. While Voevodsky continued to work out the final details of the proofs of the main theorems about motivic Eilenberg–MacLane spaces, Charles Weibel invented an approach to correct the place in the proof that had to modified. Weibel also published in 2009 a paper that contained a summary of the Voevodsky's constructions combined with the correction that he discovered.\n\nBeilinson–Lichtenbaum conjecture\n\nLet X be a smooth variety over a field containing .  Beilinson and Lichtenbaum conjectured that the motivic cohomology group  is isomorphic to the étale cohomology group  when p≤q.  This conjecture has now been proven, and is equivalent to the norm residue isomorphism theorem.\n\nReferences\n\nBibliography\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\nCategory:Conjectures that have been proved\nCategory:Algebraic K-theory\nCategory:Theorems in algebraic topology\nCategory:Theorems in algebra"
    },
    {
      "title": "Polynomial remainder theorem",
      "url": "https://en.wikipedia.org/wiki/Polynomial_remainder_theorem",
      "text": "In algebra, the polynomial remainder theorem or little Bézout's theorem (named after Étienne Bézout) is an application of Euclidean division of polynomials. It states that the remainder of the division of a polynomial  by a linear polynomial  is equal to  In particular,  is a divisor of  if and only if Larson, Ron (2014), College Algebra, Cengage Learning a property known as the factor theorem.\n\n Examples \n Example 1 \nLet . Polynomial division of  by  gives the quotient  and the remainder . Therefore, .\n\nExample 2\nShow that the polynomial remainder theorem holds for an arbitrary second degree polynomial  by using algebraic manipulation:\n\nMultiplying both sides by (x − r) gives\n.\n\nSince  is the remainder, we have indeed shown that .\n\n Proof \n\nThe polynomial remainder theorem follows from the theorem of Euclidean division, which, given two polynomials  (the dividend) and  (the divisor), asserts the existence (and the uniqueness) of a quotient  and a remainder  such that\n\nIf the divisor is  then either  or its degree is zero; in both cases,   is a constant that is independent of ; that is \n\nSetting  in this formula, we obtain:\n\nA slightly different proof, which may appear to some people as more elementary, starts with an observation that  is a linear combination of terms of the form   each of which is divisible by  since \n\n Applications \n\nThe polynomial remainder theorem may be used to evaluate  by calculating the remainder, . Although polynomial long division is more difficult than evaluating the function itself, synthetic division is computationally easier.  Thus, the function may be more \"cheaply\" evaluated using synthetic division and the polynomial remainder theorem.\n\nThe factor theorem is another application of the remainder theorem: if the remainder is zero, then the linear divisor is a factor. Repeated application of the factor theorem may be used to factorize the polynomial.Larson, Ron (2011), Precalculus with Limits, Cengage Learning\n\n References \n\nCategory:Polynomials\nCategory:Theorems in algebra"
    },
    {
      "title": "Posner's theorem",
      "url": "https://en.wikipedia.org/wiki/Posner%27s_theorem",
      "text": "In algebra, Posner's theorem states that given a prime polynomial identity algebra A with center Z, the ring  is a central simple algebra over , the field of fractions of Z. It is named after Ed Posner.\n\n References \n\n \n Edward C. Posner, Prime rings satisfying a polynomial identity, Proc. Amer. Math. Soc. 11 (1960), pp. 180–183.\n\nCategory:Ring theory\nCategory:Theorems in algebra"
    },
    {
      "title": "Rational root theorem",
      "url": "https://en.wikipedia.org/wiki/Rational_root_theorem",
      "text": "In algebra, the rational root theorem (or rational root test, rational zero theorem, rational zero test or p/q theorem) states a constraint on rational solutions of a polynomial equation\n\nwith integer coefficients  and . Solutions of the equation are also called roots or zeroes of the polynomial on the left side.\n\nThe theorem states that each rational solution x = p/q, written in lowest terms so that p and q are relatively prime, satisfies:\n\n p is an integer factor of the constant term a0, and\n\n q is an integer factor of the leading coefficient an.\n\nThe rational root theorem is a special case (for a single linear factor) of Gauss's lemma on the factorization of polynomials. The integral root theorem is the special case of the rational root theorem when the leading coefficient is an = 1.\n\nApplication\n\nThe theorem is used to find all rational roots of a polynomial, if any. It gives a finite number of possible fractions which can be checked to see if they are roots. If a rational root x = r is found, a linear polynomial (x-r) can be factored out of the polynomial using polynomial long division, resulting in a polynomial of lower degree whose roots are also roots of the original polynomial.\n\nCubic equation\n\nThe general cubic equation\n\nwith integer coefficients has three solutions in the complex plane. If the rational root test finds no rational solutions, then the only way to express the solutions algebraically uses cube roots. But if the test finds a rational solution r, then factoring out (x – r) leaves a quadratic polynomial whose two roots, found with the quadratic formula, are the remaining two roots of the cubic, avoiding cube roots.\n\nProofs\n\nFirst proof\n\nLet \n\nSuppose P(p/q) = 0 for some coprime p, q ∈ Z:\n\nIf we multiply both sides by qn, shift the constant term to the right side, and factor out p on the left side, we get\n\nWe see that p divides a0qn. But p is coprime to q and therefore to qn, so by Euclid's lemma p must divide the remaining factor a0 of the product.\n\nIf we instead shift the leading term to the right side and factor out q on the left side, we get\n\nReasoning as before, we conclude that q divides an.\n\nProof using Gauss' lemma\n\nShould there be a nontrivial factor dividing all the coefficients of the polynomial, then one can divide by the greatest common divisor of the coefficients so as to obtain a primitive polynomial in the sense of Gauss's lemma; this does not alter the set of rational roots and only strengthens the divisibility conditions. That lemma says that if the polynomial factors in , then it also factors in  as a product of primitive polynomials. Now any rational root  corresponds to a factor of degree 1 in  of the polynomial, and its primitive representative is then , assuming that p and q are coprime. But any multiple in  of  has leading term divisible by q and constant term divisible by p, which proves the statement. This argument shows that more generally, any irreducible factor of P can be supposed to have integer coefficients, and leading and constant coefficients dividing the corresponding coefficients of P.\n\nExamples\n\nFirst\n\nIn the polynomial\n\nany rational root fully reduced would have to have a numerator that divides evenly into 1 and a denominator that divides evenly into 2. Hence the only possible rational roots are ±1/2 and ±1; since neither of these equates the polynomial to zero, it has no rational roots.\n\nSecond\nIn the polynomial\n\nthe only possible rational roots would have a numerator that divides 6 and a denominator that divides 1, limiting the possibilities to ±1, ±2, ±3, and ±6. Of these, 1, 2, and –3 equate the polynomial to zero, and hence are its rational roots. (In fact these are its only roots since a cubic has only three roots; in general, a polynomial could have some rational and some irrational roots.)\n\nThird\n\nEvery rational root of the polynomial\n\nmust be among the numbers symbolically indicated by:\n \nThese 8 root candidates x = r can be tested by evaluating P(r), for example using Horner's method. It turns out there is exactly one with P(r) = 0. \n\nThis process may be made more efficient: if P(r) ≠ 0, it can be used to shorten the list of remaining candidates. For example, x = 1 does not work, as P(1) = 1. Substituting x = 1 + t yields a polynomial in t with constant term P(1) = 1, while the coefficient of t3 remains the same as the coefficient of x3. Applying the rational root theorem thus yields the  possible roots , so that \n\nTrue roots must occur on both lists, so list of rational root candidates has shrunk to just x = 2 and x = 2/3.\n\nIf k ≥ 1 rational roots are found, Horner's method will also yield a polynomial of degree n − k whose roots, together with the rational roots, are exactly the roots of the original polynomial. If none of the candidates is a solution, there can be no rational solution.\n\nSee also\nIntegrally closed domain\nDescartes' rule of signs\nGauss–Lucas theorem\nProperties of polynomial roots\nContent (algebra)\nEisenstein's criterion\n\nNotes\n\nReferences\nCharles D. Miller, Margaret L. Lial, David I. Schneider: Fundamentals of College Algebra. Scott & Foresman/Little & Brown Higher Education, 3rd edition 1990, , pp. 216–221\nPhillip S. Jones, Jack D. Bedient: The historical roots of elementary mathematics. Dover Courier Publications 1998, , pp. 116–117  ()\nRon Larson: Calculus: An Applied Approach. Cengage Learning 2007, , pp. 23–24  ()\n\nExternal links\n\nRationalRootTheorem at PlanetMath\n Another proof that nth roots of integers are irrational, except for perfect nth powers by Scott E. Brodie\nThe Rational Roots Test at purplemath.com\n\nCategory:Polynomials\nCategory:Theorems in algebra\nCategory:Root-finding algorithms"
    },
    {
      "title": "Segal's conjecture",
      "url": "https://en.wikipedia.org/wiki/Segal%27s_conjecture",
      "text": "Segal's Burnside ring conjecture, or, more briefly, the Segal conjecture, is a theorem in homotopy theory, a branch of mathematics. The theorem relates the Burnside ring of a finite group G to the stable cohomotopy of the classifying space BG. The conjecture was made in the mid 1970s by Graeme Segal and proved in 1984 by Gunnar Carlsson. , this statement is still commonly referred to as the Segal conjecture, even though it now has the status of a theorem.\n\nStatement of the theorem\nThe Segal conjecture has several different formulations, not all of which are equivalent. Here is a weak form: there exists, for every finite group G, an isomorphism\n\nHere, lim denotes the inverse limit, S* denotes the stable cohomotopy ring, B denotes the classifying space, the superscript k denotes the k-skeleton, and the subscript + denotes the addition of a disjoint basepoint. On the right-hand side, the hat denotes the completion of the Burnside ring with respect to its augmentation ideal.\n\nThe Burnside ring\n\nThe Burnside ring of a finite group G is constructed from the category of finite G-sets as a Grothendieck group. More precisely, let M(G) be the commutative monoid of isomorphism classes of finite G-sets, with addition the disjoint union of G-sets and identity element the empty set (which is a G-set in a unique way). Then A(G), the Grothendieck group of M(G), is an abelian group. It is in fact a free abelian group with basis elements represented by the G-sets G/H, where H varies over the subgroups of G. (Note that H is not assumed here to be a normal subgroup of G, for while G/H is not a group in this case, it is still a G-set.) The ring structure on A(G) is induced by the direct product of G-sets; the multiplicative identity is the (isomorphism class of any) one-point set, which becomes a G-set in a unique way.\n\nThe Burnside ring is the analogue of the representation ring in the category of finite sets, as opposed to the category of finite-dimensional vector spaces over a field (see motivation below). It has proven to be an important tool in the representation theory of finite groups.\n\nThe classifying space\n\nFor any topological group G admitting the structure of a CW-complex, one may consider the category of principal G-bundles. One can define a functor from the category of CW-complexes to the category of sets by assigning to each CW-complex X the set of principal G-bundles on X. This functor descends to a functor on the homotopy category of CW-complexes, and it is natural to ask whether the functor so obtained is representable. The answer is affirmative, and the representing object is called the classifying space of the group G and typically denoted BG. If we restrict our attention to the homotopy category of CW-complexes, then BG is unique. Any CW-complex that is homotopy equivalent to BG is called a model for BG.\n\nFor example, if G is the group of order 2, then a model for BG is infinite-dimensional real projective space. It can be shown that if G is finite, then any CW-complex modelling BG has cells of arbitrarily large dimension. On the other hand, if G = Z, the integers, then the classifying space BG is homotopy equivalent to the circle S1.\n\nMotivation and interpretation\nThe content of the theorem becomes somewhat clearer if it is placed in its historical context. In the theory of representations of finite groups, one can form an object  called the representation ring of  in a way entirely analogous to the construction of the Burnside ring outlined above. The stable cohomotopy is in a sense the natural analog to complex K-theory, which is denoted . Segal was inspired to make his conjecture after Michael Atiyah proved the existence of an isomorphism\n\nwhich is a special case of the Atiyah–Segal completion theorem.\n\nReferences\n\nCategory:Representation theory of finite groups\nCategory:Homotopy theory\nCategory:Conjectures that have been proved\nCategory:Theorems in algebra"
    },
    {
      "title": "Sinkhorn's theorem",
      "url": "https://en.wikipedia.org/wiki/Sinkhorn%27s_theorem",
      "text": "Sinkhorn's theorem states that every square matrix with positive entries can be written in a certain standard form.\n\nTheorem\nIf A is an n × n matrix with strictly positive elements, then there exist diagonal matrices D1 and D2 with strictly positive diagonal elements such that D1AD2 is doubly stochastic. The matrices D1 and D2 are unique modulo multiplying the first matrix by a positive number and dividing the second one by the same number. \nSinkhorn, Richard. (1964). \"A relationship between arbitrary positive matrices and doubly stochastic matrices.\" Ann. Math. Statist. 35, 876–879. \nMarshall, A.W., & Olkin, I. (1967). \"Scaling of matrices to achieve specified row and column sums.\" Numerische Mathematik. 12(1), 83–90. \n\nSinkhorn-Knopp algorithm\nA simple iterative method to approach the double stochastic matrix is to alternately rescale all rows and all columns of A to sum to 1. Sinkhorn and Knopp presented this algorithm and analyzed its convergence.\nSinkhorn, Richard, & Knopp, Paul. (1967). \"Concerning nonnegative matrices and doubly stochastic matrices\". Pacific J. Math. 21, 343–348.\n\nAnalogues and extensions\nThe following analogue for unitary matrices is also true: for every unitary matrix U there exist two diagonal unitary matrices L and R such that LUR has each of its columns and rows summing to 1.\n\nThe following extension to maps between matrices is also true (see Theorem 5 and also Theorem 4.7): given a Kraus operator\nwhich represents the quantum operation Φ mapping a density matrix into another,\n\nthat is trace preserving,\n\nand, in addition, whose range is in the interior of the positive definite cone (strict positivity), there exist scalings xj, for j in {0,1}, that are positive definite so that the rescaled Kraus operator\n\nis doubly stochastic. In other words, it is such that both,\n\nas well as for the adjoint,\n\nwhere I denotes the identity operator.\n\nReferences\n\nCategory:Matrix theory\nCategory:Theorems in algebra"
    },
    {
      "title": "Skolem–Noether theorem",
      "url": "https://en.wikipedia.org/wiki/Skolem%E2%80%93Noether_theorem",
      "text": "In ring theory, a branch of mathematics, the Skolem–Noether theorem characterizes the automorphisms of simple rings. It is a fundamental result in the theory of central simple algebras.\n\nThe theorem was first published by Thoralf Skolem in 1927 in his paper Zur Theorie der assoziativen Zahlensysteme (German: On the theory of associative number systems) and later rediscovered by Emmy Noether.\n\n Statement \nIn a general formulation, let A and B be simple unitary rings, and let k be the centre of B. Notice that k is a field since given x nonzero in k, the simplicity of B implies that the nonzero two-sided ideal BxB = (x) is the whole of B, and hence that x is a unit. If the dimension of B over k is finite, i.e. if B is a central simple algebra of finite dimension, and A is also a k-algebra, then given k-algebra homomorphisms\n\nf, g : A → B,\n\nthere exists a unit b in B such that for all a in ALorenz (2008) p.173\n\ng(a) = b · f(a) · b−1.\n\nIn particular, every automorphism of a central simple k-algebra is an inner automorphism.Gille & Szamuely (2006) p.40Lorenz (2008) p.174\n\n Proof \nFirst suppose . Then f and g define the actions of A on ; let  denote the A-modules thus obtained. Any two simple A-modules are isomorphic and  are finite direct sums of simple A-modules. Since they have the same dimension, it follows that there is an isomorphism of A-modules . But such b must be an element of . For the general case, note that  is a matrix algebra and that  is simple. By the first part applied to the maps , there exists  such that\n\nfor all  and . Taking , we find\n\nfor all z. That is to say, b is in  and so we can write . Taking  this time we find\n,\nwhich is what was sought.\n\n Notes \n\n References \n\nA discussion in Chapter IV of Milne, class field theory \n \n \n\nCategory:Ring theory\nCategory:Theorems in algebra"
    },
    {
      "title": "Specht's theorem",
      "url": "https://en.wikipedia.org/wiki/Specht%27s_theorem",
      "text": "In mathematics, Specht's theorem gives a necessary and sufficient condition for two matrices to be unitarily equivalent. It is named after Wilhelm Specht, who proved the theorem in 1940.\n\nTwo matrices A and B are said to be unitarily equivalent if there exists a unitary matrix U such that B = U *AU., Definition 2.2.1 Two matrices which are unitarily equivalent are also similar. Two similar matrices represent the same linear map, but with respect to a different basis; unitary equivalence corresponds to a change from an orthonormal basis to another orthonormal basis. \n\nIf A and B are unitarily equivalent, then tr AA* = tr BB*, where tr denotes the trace (in other words, the Frobenius norm is a unitary invariant). This follows from the cyclic invariance of the trace: if B = U *AU, then tr BB* = tr U *AUU *A*U = tr AUU *A*UU * = tr AA*, where the second equality is cyclic invariance., Theorem 2.2.2 \n\nThus, tr AA* = tr BB* is a necessary condition for unitary equivalence, but it is not sufficient. Specht's theorem gives infinitely many necessary conditions which together are also sufficient. The formulation of the theorem uses the following definition. A word in two variables, say x and y, is an expression of the form\n\nwhere m1, n1, m2, n2, …, mp are non-negative integers. The degree of this word is\n\nSpecht's theorem: Two matrices A and B are unitarily equivalent if and only if tr W(A, A*) = tr W(B, B*) for all words W., Theorem 2.2.6 \n\nThe theorem gives an infinite number of trace identities, but it can be reduced to a finite subset. Let n denote the size of the matrices A and B. For the case n = 2, the following three conditions are sufficient:, Theorem 2.2.8 \n\nFor n = 3, the following seven conditions are sufficient:\n\n  , p. 260, quoted by  \nFor general n, it suffices to show that tr W(A, A*) = tr W(B, B*) for all words of degree at most \n\n  , Theorem 4.3 \n\nIt has been conjectured that this can be reduced to an expression linear in n., p. 160\n\n Notes \n\n References \n .\n .\n . \n .\n .\n .\n\nCategory:Matrix theory\nCategory:Combinatorics on words\nCategory:Theorems in algebra"
    },
    {
      "title": "Stone's representation theorem for Boolean algebras",
      "url": "https://en.wikipedia.org/wiki/Stone%27s_representation_theorem_for_Boolean_algebras",
      "text": "In mathematics, Stone's representation theorem for Boolean algebras states that every Boolean algebra is isomorphic to a certain field of sets. The theorem is fundamental to the deeper understanding of Boolean algebra that emerged in the first half of the 20th century. The theorem was first proved by Marshall H. Stone (1936). Stone was led to it by his study of the spectral theory of operators on a Hilbert space.\n\nStone spaces\nEach Boolean algebra B has an associated topological space, denoted here S(B), called its Stone space. The points in S(B) are the ultrafilters on B, or equivalently the homomorphisms from B to the two-element Boolean algebra. The topology on S(B) is generated by a (closed) basis consisting of all sets of the form\n\nwhere b is an element of B. This is the topology of pointwise convergence of nets of homomorphisms into the two-element Boolean algebra.\n\nFor every Boolean algebra B, S(B) is a compact totally disconnected Hausdorff space; such spaces are called Stone spaces (also profinite spaces). Conversely, given any topological space X, the collection of subsets of X that are clopen (both closed and open) is a Boolean algebra.\n\nRepresentation theorem\n\nA simple version of Stone's representation theorem states that every Boolean algebra B is isomorphic to the algebra of clopen subsets of its Stone space S(B). The isomorphism sends an element b∈B to the set of all ultrafilters that contain b. This is a clopen set because of the choice of topology on S(B) and because B is a Boolean algebra. \n\nRestating the theorem using the language of category theory; the theorem states that there is a duality between the category of Boolean algebras  and the category of Stone spaces. This duality means that in addition to the correspondence between Boolean algebras and their Stone spaces, each homomorphism from a Boolean algebra A to a Boolean algebra B corresponds in a natural way to a continuous function from S(B) to  S(A). In other words, there is a contravariant functor that gives an equivalence between the categories. This was an early example of a nontrivial duality of categories.\n\nThe theorem is a special case of Stone duality, a more general framework for dualities between topological spaces and partially ordered sets.\n\nThe proof requires either the axiom of choice or a weakened form of it. Specifically, the theorem is equivalent to the Boolean prime ideal theorem, a weakened choice principle that states that every Boolean algebra has a prime ideal.\n\nAn extension of the classical Stone duality to the category of Boolean spaces (= zero-dimensional locally compact Hausdorff spaces) and continuous maps (respectively, perfect maps) was obtained by G. D. Dimov (respectively, by H. P. Doctor) (see the references below).\n\nSee also\n Field of sets\n List of Boolean algebra topics\n Stonean space\n Stone functor\n Profinite group\n Representation theorem\n\nReferences\n Paul Halmos, and Givant, Steven (1998) Logic as Algebra. Dolciani Mathematical Expositions No. 21. The Mathematical Association of America.\n Johnstone, Peter T. (1982) Stone Spaces. Cambridge University Press. .\n Marshall H. Stone (1936) \"The Theory of Representations of Boolean Algebras,\" Transactions of the American Mathematical Society 40: 37-111. \n G. D. Dimov (2012) Some generalizations of the Stone Duality Theorem. Publ. Math. Debrecen 80: 255–293.\n H. P. Doctor (1964) The categories of Boolean lattices, Boolean rings and Boolean spaces. Canad. Math. Bulletin 7: 245–252.\n Burris, Stanley N., and H. P. Sankappanavar, H. P.(1981) A Course in Universal Algebra.  Springer-Verlag. .\n\nCategory:General topology\nCategory:Boolean algebra\nCategory:Theorems in algebra\nCategory:Categorical logic"
    },
    {
      "title": "Strassmann's theorem",
      "url": "https://en.wikipedia.org/wiki/Strassmann%27s_theorem",
      "text": "In mathematics, Strassmann's theorem is a result in field theory. It states that, for suitable fields, suitable formal power series with coefficients in the valuation ring of the field have only finitely many zeroes.\n\nHistory\nIt was introduced by .\n\nStatement of the theorem\nLet K be a field with a non-Archimedean absolute value | · | and let R be the valuation ring of K. Let f(x) be a formal power series with coefficients in R other than the zero series, with coefficients an converging to zero with respect to | · |. Then f(x) has only finitely many zeroes in R. More precisely, the number of zeros is at most N, where N is the largest index with |aN| = max  |an|.\n\nReferences\n\nExternal links\n \n\nCategory:Field theory\nCategory:Theorems in algebra"
    },
    {
      "title": "Subgroups of cyclic groups",
      "url": "https://en.wikipedia.org/wiki/Subgroups_of_cyclic_groups",
      "text": "In abstract algebra, every subgroup of a cyclic group is cyclic. Moreover, for a finite cyclic group of order n, every subgroup's order is a divisor of n, and there is exactly one subgroup for each divisor.. This result has been called the fundamental theorem of cyclic groups.\n\nFinite cyclic groups\nFor every finite group G of order n, the following statements are equivalent:\n G is cyclic.\n For every divisor d of n, G has exactly one subgroup of order d.\n For every divisor d of n, G has at most one subgroup of order d.\nThis statement is known by various names such as characterization by subgroups. (See also cyclic group for some characterization.)\n\nThere exist finite groups other than cyclic groups with the property that all proper subgroups are cyclic; the Klein group is an example. However, the Klein group has more than one subgroup of order 2, so it does not meet the conditions of the characterization.\n\nThe infinite cyclic group\nThe infinite cyclic group is isomorphic to the additive subgroup Z of the integers. There is one subgroup dZ for each integer d (consisting of the multiples of d), and with the exception of the trivial group (generated by d = 0) every such subgroup is itself an infinite cyclic group. Because the infinite cyclic group is a free group on one generator (and the trivial group is a free group on no generators), this result can be seen as a special case of the Nielsen–Schreier theorem that every subgroup of a free group is itself free..\n\nThe fundamental theorem for finite cyclic groups can be established from the same theorem for the infinite cyclic groups, by viewing each finite cyclic group as a quotient group of the infinite cyclic group.\n\nLattice of subgroups\nIn both the finite and the infinite case, the lattice of subgroups of a cyclic group is isomorphic to the dual of a divisibility lattice. In the finite case, the lattice of subgroups of a cyclic group of order n is isomorphic to the dual of the lattice of divisors of n, with a subgroup of order n/d for each divisor d. The subgroup of order n/d is a subgroup of the subgroup of order n/e if and only if e is a divisor of d. The lattice of subgroups of the infinite cyclic group can be described in the same way, as the dual of the divisibility lattice of all positive integers. If the infinite cyclic group is represented as the additive group on the integers, then the subgroup generated by d is a subgroup of the subgroup generated by e if and only if e is a divisor of d.\n\nDivisibility lattices are distributive lattices, and therefore so are the lattices of subgroups of cyclic groups. This provides another alternative characterization of the finite cyclic groups: they are exactly the finite groups whose lattices of subgroups are distributive. More generally, a finitely generated group is cyclic if and only if its lattice of subgroups is distributive and an arbitrary group is locally cyclic if and only its lattice of subgroups is distributive.. The additive group of the rational numbers provides an example of a group that is locally cyclic, and that has a distributive lattice of subgroups, but that is not itself cyclic.\n\nReferences\n\nCategory:Theorems in algebra\nCategory:Articles containing proofs\nCyclic groups"
    },
    {
      "title": "Sylow theorems",
      "url": "https://en.wikipedia.org/wiki/Sylow_theorems",
      "text": "In mathematics, specifically in the field of finite group theory, the Sylow theorems are a collection of theorems named after the Norwegian mathematician Peter Ludwig Sylow (1872) that give detailed information about the number of subgroups of fixed order that a given finite group contains. The Sylow theorems form a fundamental part of finite group theory and have very important applications in the classification of finite simple groups.\n\nFor a prime number p, a Sylow p-subgroup (sometimes p-Sylow subgroup) of a group G is a maximal p-subgroup of G, i.e., a subgroup of G that is a p-group (so that the order of every group element is a power of p) that is not a proper subgroup of any other p-subgroup of G. The set of all Sylow p-subgroups for a given prime p is sometimes written Sylp(G).\n\nThe Sylow theorems assert a partial converse to Lagrange's theorem. Lagrange's theorem states that for any finite group G the order (number of elements) of every subgroup of G divides the order of G.  The Sylow theorems state that for every prime factor p of the order of a finite group G, there exists a Sylow p-subgroup of G of order pn, the highest power of p that divides the order of G.  Moreover, every subgroup of order pn is a Sylow p-subgroup of G, and the Sylow p-subgroups of a group (for a given prime p) are conjugate to each other. Furthermore, the number of Sylow p-subgroups of a group for a given prime p is congruent to \n\n Theorems \n\nCollections of subgroups that are each maximal in one sense or another are common in group theory. The surprising result here is that in the case of Sylp(G), all members are actually isomorphic to each other and have the largest possible order: if |G| = pnm with n > 0 where p does not divide m, then every Sylow p-subgroup P has order |P| = pn. That is, P is a p-group and gcd(|G : P|, p) = 1. These properties can be exploited to further analyze the structure of G.\n\nThe following theorems were first proposed and proven by Ludwig Sylow in 1872, and published in Mathematische Annalen.\n\nTheorem 1: For every prime factor p with multiplicity n of the order of a finite group G, there exists a Sylow p-subgroup of G, of order pn.\n\nThe following weaker version of theorem 1 was first proved by Augustin-Louis Cauchy, and is known as Cauchy's theorem.\n\nCorollary: Given a finite group G and a prime number p dividing the order of G, then there exists an element (and hence a subgroup) of order p in G.Fraleigh, Victor J. Katz. A First Course In Abstract Algebra. p. 322. \n\nTheorem 2: Given a finite group G and a prime number p, all Sylow p-subgroups of G are conjugate to each other, i.e. if H and K are Sylow p-subgroups of G, then there exists an element g in G with g−1Hg = K.\n\nTheorem 3: Let p be a prime factor with multiplicity n of the order of a finite group G, so that the order of G can be written as , where  and p does not divide m. Let np be the number of Sylow p-subgroups of G. Then the following hold:\n np divides m, which is the index of the Sylow p-subgroup in G.\n np ≡ 1 (mod p).\n np = |G : NG(P)|, where P is any Sylow p-subgroup of G and NG denotes the normalizer.\n\n Consequences \n\nThe Sylow theorems imply that for a prime number p every Sylow p-subgroup is of the same order, pn. Conversely, if a subgroup has order pn, then it is a Sylow p-subgroup, and so is isomorphic to every other Sylow p-subgroup. Due to the maximality condition, if H is any p-subgroup of G, then H is a subgroup of a p-subgroup of order pn.\n\nA very important consequence of Theorem 3 is that the condition np = 1 is equivalent to saying that the Sylow p-subgroup of G is a normal subgroup\n(there are groups that have normal subgroups but no normal Sylow subgroups, such as S4).\n\n Sylow theorems for infinite groups \n\nThere is an analogue of the Sylow theorems for infinite groups.  We define a Sylow p-subgroup in an infinite group to be a p-subgroup (that is, every element in it has p-power order) that is maximal for inclusion among all p-subgroups in the group.  Such subgroups exist by Zorn's lemma.\n\nTheorem: If K is a Sylow p-subgroup of G, and np = |Cl(K)| is finite, then every Sylow p-subgroup is conjugate to K, and np ≡ 1 (mod p), where Cl(K) denotes the conjugacy class of K.\n\n Examples \nthumb|In D6 all reflections are conjugate, as reflections correspond to Sylow 2-subgroups.\n\nA simple illustration of Sylow subgroups and the Sylow theorems are the dihedral group of the n-gon, D2n. For n odd, 2 = 21 is the highest power of 2 dividing the order, and thus subgroups of order 2 are Sylow subgroups. These are the groups generated by a reflection, of which there are n, and they are all conjugate under rotations; geometrically the axes of symmetry pass through a vertex and a side.\n\nthumb|left|In D12 reflections no longer correspond to Sylow 2-subgroups, and fall into two conjugacy classes.\nBy contrast, if n is even, then 4 divides the order of the group, and the subgroups of order 2 are no longer Sylow subgroups, and in fact they fall into two conjugacy classes, geometrically according to whether they pass through two vertices or two faces. These are related by an outer automorphism, which can be represented by rotation through π/n, half the minimal rotation in the dihedral group.\n\nAnother example are the Sylow p-subgroups of GL2(Fq), where p and q are primes ≥ 3 and p ≡ 1 (mod q) , which are all abelian. The order of GL2(Fq) is (q2 − 1)(q2 − q) = (q)(q + 1)(q − 1)2. Since q = pnm + 1, the order of GL2(Fq) = p2n m′. Thus by Theorem 1, the order of the Sylow p-subgroups is p2n.\n\nOne such subgroup P, is the set of diagonal matrices  , x is any primitive root of  Fq. Since the order of Fq is q − 1, its primitive roots have order q − 1, which implies that x(q − 1)/pn or xm and all its powers have an order which is a power of p. So, P is a subgroup where all its elements have orders which are powers of p. There are pn choices for both a and b, making |P| = p2n. This means P is a Sylow p-subgroup, which is abelian, as all diagonal matrices commute, and because Theorem 2 states that all Sylow p-subgroups are conjugate to each other, the Sylow p-subgroups of GL2(Fq) are all abelian.\n\nExample applications\nSince Sylow's theorem ensures the existence of p-subgroups of a finite group, its worthwhile to study groups of prime power order more closely. Most of the examples use Sylow's theorem to prove that a group of a particular order is not simple. For groups of small order, the congruence condition of Sylow's theorem is often sufficient to force the existence of a normal subgroup. \nExample-1 Groups of order pq, p and q primes with p < q. \nExample-2 Group of order 30, groups of order 20, groups of order p2q, p and q distinct primes are some of the applications. \nExample-3 (Groups of order 60): If the order |G| = 60 and G has more than one Sylow 5-subgroup, then G is simple.\n\n Cyclic group orders \nSome non-prime numbers n are such that every group of order n is cyclic.  One can show that n = 15 is such a number using the Sylow theorems:  Let G be a group of order 15 = 3 · 5 and n3 be the number of Sylow 3-subgroups. Then n3  5 and n3 ≡ 1 (mod 3). The only value satisfying these constraints is 1; therefore, there is only one subgroup of order 3, and it must be normal (since it has no distinct conjugates). Similarly, n5 must divide 3, and n5 must equal 1 (mod 5); thus it must also have a single normal subgroup of order 5. Since 3 and 5 are coprime, the intersection of these two subgroups is trivial, and so G must be the internal direct product of groups of order 3 and 5, that is the cyclic group of order 15. Thus, there is only one group of order 15 (up to isomorphism).\n\n Small groups are not simple \nA more complex example involves the order of the smallest simple group that is not cyclic. Burnside's pa qb theorem states that if the order of a group is the product of one or two prime powers, then it is solvable, and so the group is not simple, or is of prime order and is cyclic. This rules out every group up to order 30 .\n\nIf G is simple, and |G| = 30, then n3 must divide 10 ( = 2 · 5), and n3 must equal 1 (mod 3). Therefore, n3 = 10, since neither 4 nor 7 divides 10, and if n3 = 1 then, as above, G would have a normal subgroup of order 3, and could not be simple. G then has 10 distinct cyclic subgroups of order 3, each of which has 2 elements of order 3 (plus the identity). This means G has at least 20 distinct elements of order 3.\n\nAs well, n5 = 6, since n5 must divide 6 ( = 2 · 3), and n5 must equal 1 (mod 5). So G also has 24 distinct elements of order 5. But the order of G is only 30, so a simple group of order 30 cannot exist.\n\nNext, suppose |G| = 42 = 2 · 3 · 7. Here n7 must divide 6 ( =  2 · 3) and n7 must equal 1 (mod 7), so n7 = 1. So, as before, G can not be simple.\n\nOn the other hand, for |G| = 60 = 22 · 3 · 5, then n3 = 10 and n5 = 6 is perfectly possible. And in fact, the smallest simple non-cyclic group is A5, the alternating group over 5 elements. It has order 60, and has 24 cyclic permutations of order 5, and 20 of order 3.\n\n Wilson's theorem \nPart of Wilson's theorem states that\n\nfor every prime p. One may easily prove this theorem by Sylow's third theorem. Indeed,  \nobserve that the number np of Sylow's p-subgroups  \nin the symmetric group Sp is (p − 2)!. On the other hand, np ≡ 1 (mod p). Hence, (p − 2)! ≡ 1 (mod p). So, (p − 1)! ≡ −1 (mod p).\n\n Fusion results \nFrattini's argument shows that a Sylow subgroup of a normal subgroup provides a factorization of a finite group.  A slight generalization known as Burnside's fusion theorem states that if G is a finite group with Sylow p-subgroup P and two subsets A and B normalized by P, then A and B are G-conjugate if and only if they are NG(P)-conjugate.  The proof is a simple application of Sylow's theorem: If B=Ag, then the normalizer of B contains not only P but also Pg (since Pg is contained in the normalizer of Ag).  By Sylow's theorem P and Pg are conjugate not only in G, but in the normalizer of B.  Hence gh−1 normalizes P for some h that normalizes B, and then Agh−1 = Bh−1 = B, so that A and B are NG(P)-conjugate.  Burnside's fusion theorem can be used to give a more powerful factorization called a semidirect product: if G is a finite group whose Sylow p-subgroup P is contained in the center of its normalizer, then G has a normal subgroup K of order coprime to P, G = PK and P∩K = {1}, that is, G is p-nilpotent.\n\nLess trivial applications of the Sylow theorems include the focal subgroup theorem, which studies the control a Sylow p-subgroup of the derived subgroup has on the structure of the entire group.  This control is exploited at several stages of the classification of finite simple groups, and for instance defines the case divisions used in the Alperin–Brauer–Gorenstein theorem classifying finite simple groups whose Sylow 2-subgroup is a quasi-dihedral group.  These rely on J. L. Alperin's strengthening of the conjugacy portion of Sylow's theorem to control what sorts of elements are used in the conjugation.\n\nProof of the Sylow theorems\n\nThe Sylow theorems have been proved in a number of ways, and the history of the proofs themselves is the subject of many papers including , , , , and to some extent .\n\nOne proof of the Sylow theorems exploits the notion of group action in various creative ways. The group G acts on itself or on the set of its p-subgroups in various ways, and each such action can be exploited to prove one of the Sylow theorems.  The following proofs are based on combinatorial arguments of . In the following, we use a  b as notation for \"a divides b\" and a  b for the negation of this statement.\n\n Theorem 1:  A finite group G whose order |G| is divisible by a prime power pk has a subgroup of order pk.\n\nProof: Let |G| = pkm = pk+ru such that p does not divide u, and let Ω denote the set of subsets of G of size pk. G acts on Ω by left multiplication. The orbits Gω = {gω | g ∈ G} of the ω ∈ Ω are the equivalence classes under the action of G.\n\nFor any ω ∈ Ω consider its stabilizer subgroup  Gω = {g ∈ G | gω = ω}. For any fixed element α ∈ ω the function [g ↦ gα] maps Gω to ω injectively: for any two g, h ∈ Gω we have that gα = hα implies g = h, because α ∈ ω ⊆ G means that one may cancel on the right. Therefore,   pk = |ω| ≥ |Gω|.\n\nOn the other hand,\n\nand no power of p remains in any of the factors inside the product on the right. Hence νp(|Ω|) = νp(m) = r.\nLet R ⊆ Ω be a complete representation of all the equivalence classes under the action of G. Then,\n\nThus, there exists an element ω ∈ R such that s := νp(|Gω|) ≤ νp(|Ω|) = r. Hence |Gω| = psv where p does not divide v. By the orbit-stabilizer theorem we have |Gω| = |G| / |Gω| = pk+r-su/v. Therefore, pk  |Gω|, so pk ≤ |Gω| and Gω is the desired subgroup.\n\n Lemma: Let G be a finite p-group, let Ω be a finite set, let ΩG be the set generated by the action of G on all the elements of Ω, and let Ω0 denote the set of points of ΩG that are fixed under the action of G.  Then |ΩG| ≡ |Ω0| (mod p). \n\nProof: Write ΩG as a disjoint sum of its orbits under G.  Any element x ∈ ΩG not fixed by G will lie in an orbit of order |G|/|Gx| (where Gx denotes the stabilizer), which is a multiple of p by assumption.  The result follows immediately.\n\nTheorem 2: If H is a p-subgroup of G and P is a Sylow p-subgroup of G, then there exists an element g in G such that g−1Hg ≤ P. In particular, all Sylow p-subgroups of G are conjugate to each other (and therefore isomorphic), that is, if H and K are Sylow p-subgroups of G, then there exists an element g in G with g−1Hg = K.\n\nProof: Let Ω be the set of left cosets of P in G and let H act on Ω by left multiplication. Applying the Lemma to H on Ω, we see that |Ω0| ≡ |Ω| = [G : P] (mod p).  Now p  [G : P] by definition so p  |Ω0|, hence in particular |Ω0| ≠ 0 so there exists some gP ∈ Ω0.  It follows that for some g ∈ G and ∀ h ∈ H we have hgP = gP so g−1HgP = P and therefore g−1Hg ≤ P. Now if H is a Sylow p-subgroup, |H| = |P| = |gPg−1| so that H = gPg−1 for some g ∈ G.\n\nTheorem 3: Let q denote the order of any Sylow p-subgroup P of a finite group G.  Then np = |G : NG(P)|, np  |G|/q and np ≡ 1 (mod p).\n\nProof: Let G act on P, a Sylow p-subgroup, by conjugation. By the orbit-stabilizer theorem, np = [G : StabG(P)]. StabG(P) = { g ∈ G | gPg−1 = P }  = NG (P), the normalizer of P in G. Thus, np = |G : NG(P)|, and it follows that this number is a divisor of |G|/q. Let Ω be the set of all Sylow p-subgroups of G, and let P act on Ω by conjugation. Let Q ∈ Ω0 and observe that then Q = xQx−1 for all x ∈ P so that P ≤ NG(Q).  By Theorem 2, P and Q are conjugate in NG(Q) in particular, and Q is normal in NG(Q), so then P = Q.  It follows that Ω0 = {P} so that, by the Lemma, |Ω| ≡ |Ω0| = 1 (mod p).\n\n Algorithms \nThe problem of finding a Sylow subgroup of a given group is an important problem in computational group theory.\n\nOne proof of the existence of Sylow p-subgroups is constructive: if H is a p-subgroup of G and the index [G:H] is divisible by p, then the normalizer N = NG(H) of H in G is also such that [N : H] is divisible by p.  In other words, a polycyclic generating system of a Sylow p-subgroup can be found by starting from any p-subgroup H (including the identity) and taking elements of p-power order contained in the normalizer of H but not in H itself.  The algorithmic version of this (and many improvements) is described in textbook form in , including the algorithm described in .  These versions are still used in the GAP computer algebra system.\n\nIn permutation groups, it has been proven in (; ) that a Sylow p''-subgroup and its normalizer can be found in polynomial time of the input (the degree of the group times the number of generators).  These algorithms are described in textbook form in , and are now becoming practical as the constructive recognition of finite simple groups becomes a reality.  In particular, versions of this algorithm are used in the Magma computer algebra system.\n\n See also \n\n Frattini's argument\n Hall subgroup\n Maximal subgroup\n p-group\n\n Notes \n\nReferences\n \n\n Proofs \n \n \n \n\n \n \n \n\n Algorithms \n \n \n\n External links \n \n \n \n \n\nCategory:Finite groups\nCategory:P-groups\nCategory:Theorems in algebra\nCategory:Articles containing proofs"
    },
    {
      "title": "Sylvester's determinant identity",
      "url": "https://en.wikipedia.org/wiki/Sylvester%27s_determinant_identity",
      "text": "In matrix theory, Sylvester's determinant identity is an identity useful for evaluating certain types of determinants. It is named after James Joseph Sylvester, who stated this identity without proof in 1851.  Cited in \n\nGiven an   matrix , let  denote its determinant. \nChoose a pair  \n\n \nof -element ordered subsets of , .\nLet  denote the  submatrix of   obtained by deleting the rows in  and the columns in . \nDefine the auxiliary  matrix  whose elements are equal to the following determinants\n\nwhere ,  denote the  element subsets of  and  obtained by deleting the elements  and , respectively. Then the following is Sylvester's determinantal identity (Sylvester, 1851)\n\nWhen , this is the Desnanot-Jacobi identity (Jacobi, 1851)\n\nReferences\n\nCategory:Determinants\nCategory:Matrix theory\nCategory:Linear algebra\nCategory:Theorems in algebra"
    },
    {
      "title": "Sylvester's law of inertia",
      "url": "https://en.wikipedia.org/wiki/Sylvester%27s_law_of_inertia",
      "text": "Sylvester's law of inertia is a theorem in matrix algebra about certain properties of the coefficient matrix of a real quadratic form that remain invariant under a change of basis. Namely, if A is the symmetric matrix that defines the quadratic form, and S is any invertible matrix such that D = SAST is diagonal, then the number of negative elements in the diagonal of D is always the same, for all such S; and the same goes for the number of positive elements.\n\nThis property is named after James Joseph Sylvester who published its proof in 1852.    \n\n Statement of the theorem \nLet A be a symmetric square matrix of order n with real entries. Any non-singular matrix S of the same size is said to transform A into another symmetric matrix , also of order n, where ST is the transpose of S. It is also said that matrices A and B are congruent.  If A is the coefficient matrix of some quadratic form of Rn, then B is the matrix for the same form after the change of basis defined by S.\n\nA symmetric matrix A can always be transformed in this way into a diagonal matrix D which has only entries 0, +1 and −1 along the diagonal. Sylvester's law of inertia states that the number of diagonal entries of each kind is an invariant of A, i.e. it does not depend on the matrix S used.\n\nThe number of +1s, denoted n+, is called the positive index of inertia of A, and the number of −1s, denoted n−, is called the negative index of inertia. The number of 0s, denoted n0, is the dimension of the kernel of A, and also the corank of A.  These numbers satisfy an obvious relation\n\n \n\nThe difference  is usually called the signature of A.  (However, some authors use that term for the triple  consisting of the corank and the positive and negative indices of inertia of A; for a non-degenerate form of a given dimension these are equivalent data, but in general the triple yields more data.)\n\nIf the matrix A has the property that every principal upper left  minor Δk is non-zero then the negative index of inertia is equal to the number of sign changes in the sequence\n\n \n\nStatement in terms of eigenvalues\nThe law can also be stated as follows: two symmetric square matrices of the same size have the same number of positive, negative and zero eigenvalues if and only if they are congruent (,  non-singular).\n\nThe positive and negative indices of a symmetric matrix A are also the number of positive and negative eigenvalues of A.  Any symmetric real matrix A has an eigendecomposition of the form QEQT where E is a diagonal matrix containing the eigenvalues of A, and Q is an orthonormal square matrix containing the eigenvectors.  The matrix E can be written E = WDWT where D is diagonal with entries 0, +1, or −1, and W is diagonal with Wii = √|Eii|. The matrix S = QW transforms D to A.\n\n Law of inertia for quadratic forms \nIn the context of quadratic forms, a real quadratic form Q in n variables (or on an n-dimensional real vector space) can by a suitable change of basis (by non-singular linear transformation from x to y) be brought to the diagonal form\n\n \n\nwith each ai ∈ {0, 1, −1}. Sylvester's law of inertia states that the number of coefficients of a given sign is an invariant of Q, i.e., does not depend on a particular choice of diagonalizing basis. Expressed geometrically, the law of inertia says that all maximal subspaces on which the restriction of the quadratic form is positive definite (respectively, negative definite) have the same dimension. These dimensions are the positive and negative indices of inertia.\n\nGeneralizations\nSylvester's law of inertia is also valid if A and B have complex entries. In this case, it is said that A and B are *-congruent if and only if there exists a non-singular complex matrix S such that .\n\nIn the complex scenario, a way to state Sylvester's law of inertia is that if A and B are Hermitian matrices, then A and B are *-congruent if and only if they have the same inertia. A theorem due to Ikramov generalizes the law of inertia to any normal matrices A and B:\n\nIf A and B are normal matrices, then A and B are congruent if and only if they have the same number of eigenvalues on each open ray from the origin in the complex plane.\n\nSee also\nMetric signature\nMorse theory\nCholesky decomposition\nHaynsworth inertia additivity formula\n\nReferences\n\n \n\nExternal links\nSylvester's law on PlanetMath.\nSylvester's law of inertia and *-congruence\n\nCategory:Linear algebra\nCategory:Matrix theory\nCategory:Quadratic forms\nCategory:Theorems in algebra"
    },
    {
      "title": "Weil's conjecture on Tamagawa numbers",
      "url": "https://en.wikipedia.org/wiki/Weil%27s_conjecture_on_Tamagawa_numbers",
      "text": "In mathematics, the Weil conjecture on Tamagawa numbers is the statement that the Tamagawa number  of a simply connected simple algebraic group defined over a number field is 1. In this case, simply connected means \"not having a proper algebraic covering\" in the algebraic group theory sense, which is not always the topologists' meaning.\n\nHistory\n calculated the Tamagawa number in many cases of classical groups and observed that it is an integer in all considered cases and that it was equal to 1 in the cases when the group is simply connected. The first observation does not hold for all groups:  found examples  where the Tamagawa numbers are not integers. The second observation, that the Tamagawa numbers of simply connected semisimple groups seem to be 1, became known as the Weil conjecture.\n\nRobert Langlands (1966) introduced harmonic analysis methods to show it for Chevalley groups. K. F. Lai (1980) extended the class of known cases to quasisplit reductive groups.  proved it for all groups satisfying the Hasse principle, which at the time was known for all groups without  E8 factors. V. I. Chernousov (1989) removed this restriction, by proving the Hasse principle for the resistant E8 case (see strong approximation in algebraic groups), thus completing the proof of Weil's conjecture. In 2011, Jacob Lurie and Dennis Gaitsgory announced a proof of the conjecture for algebraic groups over function fields over finite fields.\n\nApplications\n used the Weil conjecture to calculate the Tamagawa numbers of all semisimple algebraic groups.\n\nFor spin groups, the conjecture implies the known Smith–Minkowski–Siegel mass formula.\n\nSee also\nTamagawa number\n\nReferences\n\n.\n\n Further reading \nAravind Asok, Brent Doran and Frances Kirwan, \"Yang-Mills theory and Tamagawa Numbers: the fascination of unexpected links in mathematics\", February 22, 2013\nJ. Lurie, The Siegel Mass Formula, Tamagawa Numbers, and Nonabelian Poincaré Duality posted June 8, 2012.\n\nCategory:Conjectures\nCategory:Theorems in algebra\nCategory:Algebraic groups\nCategory:Diophantine geometry"
    },
    {
      "title": "Weinstein–Aronszajn identity",
      "url": "https://en.wikipedia.org/wiki/Weinstein%E2%80%93Aronszajn_identity",
      "text": "In mathematics, the Weinstein–Aronszajn identity  states that if  and  are matrices of size  and  respectively (either or both of which may be infinite) then,\nprovided  is of trace class (and hence, so is ),\n\nwhere  is the identity matrix of order .\n\nIt is closely related to the Matrix determinant lemma and its generalization. It is the determinant analogue of the Woodbury matrix identity for matrix inverses.\n\nProof\nThe identity may be proved as follows. \nLet  be a matrix comprising the four blocks , ,  and .\n\nBecause  is invertible, the formula for the determinant of a block matrix gives\n\nBecause  is invertible, the formula for the determinant of a block matrix gives\n\nThus\n\nApplications\nThis identify is useful in developing a Bayes estimator for multivariate Gaussian distributions.\n\nThe identity also finds applications in random matrix theory by relating determinants of large matrices to determinants of smaller ones.\n\nReferences\n\nCategory:Determinants\nCategory:Matrix theory\nCategory:Linear algebra\nCategory:Theorems in algebra"
    },
    {
      "title": "Witt's theorem",
      "url": "https://en.wikipedia.org/wiki/Witt%27s_theorem",
      "text": "\"Witt's theorem\" or \"the Witt theorem\" may also refer to the Bourbaki–Witt fixed point theorem of order theory.\n\nIn mathematics, Witt's theorem, named after Ernst Witt, is a basic result in the algebraic theory of quadratic forms:  any isometry between two subspaces of a nonsingular quadratic space over a field k may be extended to an isometry of the whole space.  An analogous statement holds also for skew-symmetric, Hermitian and skew-Hermitian bilinear forms over arbitrary fields. The theorem applies to classification of quadratic forms over k and in particular allows one to define the Witt group W(k) which describes the \"stable\" theory of quadratic forms over the field k.\n\n Statement of the theorem \n\nLet  be a finite-dimensional vector space over a field k of characteristic different from 2 together with a non-degenerate symmetric or skew-symmetric bilinear form. If  is an isometry between two subspaces of V then f extends to an isometry of V.\n\nWitt's theorem implies that the dimension of a maximal totally isotropic subspace (null space) of V is an invariant, called the index or  of b, and moreover, that the isometry group of  acts transitively on the set of maximal isotropic subspaces. This fact plays an important role in the structure theory and representation theory of the isometry group and in the theory of reductive dual pairs.\n\n Witt's cancellation theorem \n\nLet , ,  be three quadratic spaces over a field k. Assume that\n\n \n\nThen the quadratic spaces  and  are isometric:\n\n \n\nIn other words, the direct summand  appearing in both sides  of an isomorphism between quadratic spaces may be \"cancelled\".\n\n Witt's decomposition theorem \n\nLet  be a quadratic space over a field k. Then\nit admits a Witt decomposition:\n\n \n\nwhere  is the radical of q,  is an anisotropic quadratic space and  is a split quadratic space. Moreover, the anisotropic summand, termed the core form, and the hyperbolic summand in a Witt decomposition of  are determined uniquely up to isomorphism.\n\nQuadratic forms with the same core form are said to be similar or Witt equivalent.\n\n Citations \n\n References \n Emil Artin (1957) Geometric Algebra, page 121\n \n \n \n\nCategory:Theorems in algebra\nCategory:Quadratic forms"
    },
    {
      "title": "Woodbury matrix identity",
      "url": "https://en.wikipedia.org/wiki/Woodbury_matrix_identity",
      "text": "In mathematics (specifically linear algebra), the Woodbury matrix identity, named after Max A. WoodburyMax A. Woodbury, Inverting modified matrices, Memorandum Rept. 42, Statistical Research Group, Princeton University, Princeton, NJ, 1950, 4pp Max A. Woodbury, The Stability of Out-Input Matrices. Chicago, Ill., 1949. 5 pp.  says that the inverse of a rank-k correction of some matrix can be computed by doing a rank-k correction to the inverse of the original matrix. Alternative names for this formula are the matrix inversion lemma, Sherman–Morrison–Woodbury formula or just Woodbury formula. However, the identity appeared in several papers before the Woodbury report.\n\nThe Woodbury matrix identity is\n\nwhere A, U, C and V all denote matrices of the correct (conformable) sizes.  Specifically, A is n-by-n, U is n-by-k, C is k-by-k and V is k-by-n. This can be derived using blockwise matrix inversion.\n\nFor a more general formula for which the matrix C need not be invertible or even square, see Binomial inverse theorem.\n\n Special cases \nWhen C is the 1-by-1 unit matrix, this identity reduces to the Sherman–Morrison formula. In the case when C is the identity matrix I, the matrix  is known in numerical linear algebra and numerical partial differential equations as the capacitance matrix.  For mere numbers, the identity reduces to the following relation of fractions\n \n\n Push-through identity \n\nMultiplying on the right by  and simplifying yields the following push-through identity \n \n\nThe special case where  and  (where these two identity matrices have different sizes in general) illustrates the origin of the \"push-through\" name:\n \n\n Direct proof \nThe formula can be proven by checking that  times its alleged inverse on the right side of the Woodbury identity gives the identity matrix:\n\n \n\n Algebraic proof \nFirst consider these useful identities,\n \n\nNow, \n \n\n Derivation via blockwise elimination \nDeriving the Woodbury matrix identity is easily done by solving the following block matrix inversion problem\n\nExpanding, we can see that the above reduces to \n\nwhich is equivalent to . Eliminating the first equation, we find that , which can be substituted into the second to find . Expanding and rearranging, we have , or . Finally, we substitute into our , and we have . Thus,\n\nWe have derived the Woodbury matrix identity.\n\n Derivation from LDU decomposition \n\nWe start by the matrix\n\nBy eliminating the entry under the A (given that A is invertible) we get\n\nLikewise, eliminating the entry above C gives\n\nNow combining the above two, we get\n\nMoving to the right side gives\n\nwhich is the LDU decomposition of the block matrix into an upper triangular, diagonal, and lower triangular matrices.\n\nNow inverting both sides gives\n\n \n\nWe could equally well have done it the other way (provided that C is invertible) i.e.\n\n \n\nNow again inverting both sides,\n \n\nNow comparing elements (1, 1) of the RHS of (1) and (2) above gives the Woodbury formula\n\n Applications \n\nThis identity is useful in certain numerical computations where A−1 has already been computed and it is desired to compute (A + UCV)−1.  With the inverse of A available, it is only necessary to find the inverse of C−1 + VA−1U in order to obtain the result using the right-hand side of the identity.  If C has a much smaller dimension than A, this is more efficient than inverting A + UCV directly. A common case is finding the inverse of a low-rank update A + UCV of A (where U only has a few columns and V only a few rows), or finding an approximation of the inverse of the matrix A + B where the matrix B can be approximated by a low-rank matrix UCV, for example using the singular value decomposition.\n\nThis is applied, e.g., in the Kalman filter and recursive least squares methods, to replace the parametric solution, requiring inversion of a state vector sized matrix, with a condition equations based solution. In case of the Kalman filter this matrix has the dimensions of the vector of observations, i.e., as small as 1 in case only one new observation is processed at a time. This significantly speeds up the often real time calculations of the filter.\n\nBinomial inverse theorem\nThe binomial inverse theorem is a more general form of the Woodbury matrix identity.\n\nIf A, U, B, V are matrices of sizes p×p, p×q, q×q, q×p, respectively, then\n\nprovided A and B + BVA−1UB are nonsingular. Nonsingularity of the latter requires that B−1 exist since it equals  and the rank of the latter cannot exceed the rank of B.\n\nSince B is invertible, the two B terms flanking the parenthetical quantity inverse in the right-hand side can be replaced with  which results in\n\nThis is the Woodbury matrix identity, which can also be derived using matrix blockwise inversion.\n\nA more general formula exists when B is singular and possibly even non-square:\n\nFormulas also exist for certain cases in which A is singular.Kurt S. Riedel, \"A Sherman–Morrison–Woodbury Identity for Rank Augmenting Matrices with Application to Centering\", SIAM Journal on Matrix Analysis and Applications, 13 (1992)659-662,  preprint \n\nVerification\nFirst notice that \n\nNow multiply the matrix we wish to invert by its alleged inverse:\n\nwhich verifies that it is the inverse.\n\nSo we get that if A−1 and  exist, then  exists and is given by the theorem above.\n\nSpecial cases\n\nFirst\nIf p = q and U = V = Ip is the identity matrix, then\n\nRemembering the identity\n\nwe can also express the previous equation in the simpler form as\n\nContinuing with the merging of the terms of the far right-hand side of the above equation results in Hua's identity\n\nAnother useful form of the same identity is\n\nwhich has a recursive structure that yields\n\nThis form can be used in perturbative expansions where B is a perturbation of A.\n\nSecond\n\nIf B = Iq is the identity matrix and q = 1, then U is a column vector, written u, and V is a row vector, written vT.  Then the theorem implies the Sherman-Morrison formula:\n\nThis is useful if one has a matrix A with a known inverse A−1 and one needs to invert matrices of the form A + uvT quickly for various u and v.\n\nThird\n\nIf we set A = Ip and B = Iq, we get \n\nIn particular, if q = 1, then\n\nwhich is a particular case of the Sherman-Morrison formula given above.\n\nSee also\nSherman–Morrison formula\nSchur complement\nMatrix determinant lemma, formula for a rank-k update to a determinant\nInvertible matrix\nMoore-Penrose pseudoinverse#Updating the pseudoinverse\n\n Notes \n\n External links \n Some matrix identities\n \n\nCategory:Linear algebra\nCategory:Matrices\nCategory:Matrix theory\nCategory:Theorems in algebra\nCategory:Lemmas"
    },
    {
      "title": "Cayley–Hamilton theorem",
      "url": "https://en.wikipedia.org/wiki/Cayley%E2%80%93Hamilton_theorem",
      "text": "225px|thumb|right|Arthur Cayley, F.R.S. (1821–1895) is widely regarded as Britain's leading pure mathematician of the 19th century. Cayley in 1848 went to Dublin to attend lectures on quaternions by Hamilton, their discoverer. Later Cayley impressed him by being the second to publish work on them.\nCayley proved the theorem for matrices of dimension 3 and less,  publishing proof for the two-dimensional case. As for  matrices, Cayley stated “..., I have not thought it necessary to undertake the labor of a formal proof\nof the theorem in the general case of a matrix of any degree”.\n225px|thumb|right|William Rowan Hamilton (1805–1865), Irish physicist, astronomer, and mathematician, first foreign member of the American National Academy of Sciences. While maintaining opposing position about how geometry should be studied, Hamilton always remained on the best terms with Cayley.Hamilton proved that for a linear function of quaternions there exists a certain equation, depending on the linear function, that is satisfied by the linear function itself.\n\nIn linear algebra, the Cayley–Hamilton theorem (named after the mathematicians Arthur Cayley and William Rowan Hamilton) states that every square matrix over a commutative ring (such as the real or complex field) satisfies its own characteristic equation.\n\nIf  is a given  matrix and  is the   identity matrix, then the  characteristic polynomial of  is defined as\n\nwhere  is the determinant operation and  is a scalar element of the base ring. Since the entries of the matrix are (linear or constant) polynomials in , the determinant is also an -th order monic polynomial in . The Cayley–Hamilton theorem states that substituting the matrix  for  in this polynomial results in the zero matrix,\n\nThe powers of , obtained by substitution from powers of , are defined by repeated matrix multiplication; the constant term of  gives a multiple of the power 0, which is defined as the identity matrix.\nThe theorem allows  to be expressed as a linear combination of the lower matrix powers of . When the ring is a field, the Cayley–Hamilton theorem is equivalent to the statement that the minimal polynomial of a square matrix divides its characteristic polynomial.\n\nThe theorem was first proved in 1853 in terms of inverses of linear functions of quaternions, a non-commutative ring, by Hamilton. This corresponds to the special case of certain  real or  complex matrices. The theorem holds for general quaternionic matrices.Due to the non-commutative nature of the multiplication operation for quaternions and related constructions, care needs to be taken with definitions, most notably in this context, for the determinant. The theorem holds as well for the slightly less well-behaved split-quaternions, see . The rings of quaternions and split-quaternions can both be represented by certain  complex matrices. (When restricted to unit norm, these are the groups  and  respectively.) Therefore it is not surprising that the theorem holds.There is no such matrix representation for the octonions, since the multiplication operation is not associative in this case. However, a modified Cayley–Hamilton theorem still holds for the octonions, see . Cayley in 1858 stated it for  and smaller matrices, but only published a proof for the  case. The general case was first proved by Frobenius in 1878.\n\n Examples \n\n  matrices \n\nFor a  matrix , the characteristic polynomial is given by , and so  is obvious.\n\n  matrices \n\nAs a concrete example, let\n\nIts characteristic polynomial is given by\n\nThe Cayley–Hamilton theorem claims that, if we define\n\nthen\n\nWe can verify by computation that indeed,\n\nFor a generic  matrix,\n\nthe characteristic polynomial is given by , so the Cayley–Hamilton theorem states that\n\nwhich is indeed always the case, evident by working out the entries of 2.\n\n Applications\n\nDeterminant and inverse matrix\n\nFor a general  invertible matrix , i.e., one with nonzero determinant, −1 can thus be written as an  order  polynomial expression in :   As indicated,  the Cayley–Hamilton theorem amounts to  the identity \n\nThe coefficients  are given by the elementary symmetric polynomials of the eigenvalues of . Using Newton identities, the elementary symmetric polynomials can in turn be expressed in terms of power sum symmetric polynomials of the eigenvalues: \n \nwhere  is the trace of the matrix . Thus, we can express  in terms of the trace of powers of .\n\nIn general, the formula for the coefficients  is given in terms of complete exponential Bell polynomials as  An explicit expression for these coefficients is\n\nwhere the sum is taken over the sets of all integer partitions  satisfying the equation  \n\nIn particular, the determinant of  corresponds to  . Thus, the determinant can be written as a trace identity\n\nLikewise, the characteristic polynomial can be written as\n\nand, by  multiplying both sides by  (note ), one is led to an expression for the inverse of  as a trace identity,\n\nFor instance, the first few Bell polynomials are  = 1, , , and .\n\nUsing these to specify the coefficients  of the characteristic polynomial of a  matrix yields\n\nThe coefficient  gives the determinant of the  matrix,  minus its trace, while its inverse is given by\n\nIt is apparent from the general formula for cn-k, expressed in terms of Bell polynomials, that the expressions\n\nalways give the coefficients  of  and   of  in the characteristic polynomial of any  matrix, respectively. So, for a  matrix , the statement of the Cayley–Hamilton theorem can also be written as\n\nwhere the right-hand side designates a  matrix with all entries reduced to zero. Likewise, this determinant in the  case, is  now\n\nThis expression gives the negative of coefficient  of  in the general case, as seen below.\n\nSimilarly, one can write for a  matrix ,\n\nwhere, now,  the determinant is  , \n\nand so on for larger matrices. The increasingly complex expressions for the coefficients   is deducible from Newton's identities or the Faddeev–LeVerrier algorithm.\n\nAnother method for obtaining these coefficients   for a general  matrix, provided no root be zero, relies on the following alternative expression for the determinant,\n\nHence, by virtue of the Mercator series,\n\nwhere the exponential only needs be expanded to order  , since  is of order , the net negative powers of  automatically vanishing by the C–H theorem. (Again, this requires a ring containing the rational numbers.) The coefficients of  can be directly written in terms of complete Bell polynomials by comparing this expression with the generating function of the Bell polynomial.\n\nDifferentiation of this expression with respect to  allows determination of the generic coefficients of the characteristic polynomial for general , as\ndeterminants of  matrices,See, e.g., p. 54 of  , which solves Jacobi's formula, \n\nwhere  is the adjugate matrix of the next section.\n\nThere also exists an equivalent, related recursive algorithm introduced by Urbain Le Verrier and Dmitry Konstantinovich Faddeev—the Faddeev–LeVerrier algorithm, which reads\n\n(see, e.g., p 88 of .)  Observe  as the recursion terminates.\nSee the algebraic proof in the following section, which relies on the modes of the adjugate,  .   \nSpecifically,  and the above derivative of    when one traces it yields\n\n (), and the above recursions, in turn.\n\nn-th Power of matrix\nThe Cayley–Hamilton theorem always provides a relationship between the powers of  (though not always the simplest one), which allows one to simplify expressions involving such powers, and evaluate them without having to compute the power  or any higher powers of .\n\nAs an example, for  the theorem gives \n\nThen, to calculate , observe\n\nLikewise,\n\nNotice that we have been able to write the matrix power as the sum of two terms. In fact, matrix power of any order  can be written as a matrix polynomial of degree at most , where  is the size of a square matrix. This is an instance where Cayley–Hamilton theorem can be used to express a matrix function, which we will discuss below systematically.\n\nMatrix functions\nGiven an analytic function \n\nand the characteristic polynomial   of degree  of an  matrix , the function can be expressed using long division as \n\nwhere  is some quotient polynomial and   is a remainder polynomial such that . By the Cayley–Hamilton theorem, replacing  by the matrix  gives , so one has\n\nThus, the analytic function of matrix  can be expressed as a matrix polynomial of degree less than .\n\nLet the remainder polynomial be\n\nSince , evaluating the function  at the  eigenvalues of ,  yields\n\nThis amounts to a system of  linear equations, which can be solved to determine the coefficients . Thus, one has \n\nWhen the eigenvalues are repeated, that is  for some , two or more equations are identical; and hence the linear equations cannot be solved uniquely. For such cases, for an eigenvalue  with multiplicity , the first  derivative of  vanishes at the eigenvalues. Thus, there are the extra  linearly independent solutions \n\nwhich, when combined with others, yield the required  equations to solve for .\n\nFinding a polynomial that passes through the points  is essentially an interpolation problem, and can be solved using Lagrange or Newton interpolation techniques,  leading to  Sylvester's formula.\n\nFor example, suppose the task is to find the polynomial representation of \n\nThe characteristic polynomial is , and the eigenvalues are . Let . Evaluating  at the eigenvalues, one obtains two linear equations  and . Solving the equations yields  and . Thus, it follows that\n\nIf, instead, the function were , then the coefficients would have been  and ; hence\n\nAs a further example, when considering\n\nthen the characteristic polynomial is , and the eigenvalues are . As before, evaluating the function at the eigenvalues gives us the linear equations  and ; the solution of which gives,  and . Thus, for this case,\n\nwhich is a rotation matrix.\n\nStandard examples of such usage is the exponential map from the Lie algebra of a matrix Lie group into the group. It is given by a matrix exponential,\n\nSuch expressions have long been known for ,\n\nwhere the  are the Pauli matrices and for ,\n\nwhich is Rodrigues' rotation formula. For the notation, see rotation group SO(3)#A note on Lie algebra.\n\nMore recently, expressions have appeared for other groups, like the Lorentz group ,  and , as well as . The group  is the conformal group of spacetime,  its simply connected cover (to be precise, the simply connected cover of the connected component  of ). The expressions obtained apply to the standard representation of these groups. They require knowledge of (some of) the eigenvalues of the matrix to exponentiate. For  (and hence for ), closed expressions have recently been obtained for all irreducible representations, i.e. of any spin.\n\n220px|thumb|right|Ferdinand Georg Frobenius (1849–1917), German mathematician. His main interests were elliptic functions differential equations, and later group theory.In 1878 he gave the first full proof of the Cayley–Hamilton theorem.\n\nAlgebraic number theory\nThe Cayley–Hamilton theorem is an effective tool for computing the minimal polynomial of algebraic integers. For example, given a finite extension  of  and an algebraic integer  which is a non-zero linear combination of the  we can compute the minimal polynomial of  by finding a matrix representing the -linear transformation\n\nIf we call this transformation matrix , then we can find the minimal polynomial by applying the Cayley–Hamilton theorem to .\n\n Proving the theorem in general \nThe Cayley–Hamilton theorem is an immediate consequence of the existence of the Jordan normal form for matrices over algebraically closed fields. In this section direct proofs are presented.\n\nAs the examples above show, obtaining the statement of the Cayley–Hamilton theorem for an  matrix\n\nrequires two steps: first the coefficients  of the characteristic polynomial are determined by development as a polynomial in  of the determinant\n\n \n\nand then these coefficients are used in a linear combination of powers of  that is equated to the  null matrix:\n\nThe left hand side can be worked out to an  matrix whose entries are (enormous) polynomial expressions in the set of entries  of , so the Cayley–Hamilton theorem states that each of these  expressions are equal to . For any fixed value of  these identities can be obtained by tedious but completely straightforward algebraic manipulations. None of these computations can show however why the Cayley–Hamilton theorem should be valid for matrices of all possible sizes , so a uniform proof for all  is needed.\n\n Preliminaries \nIf a vector  of size  happens to be an eigenvector of  with eigenvalue , in other words if , then\n\nwhich is the null vector since  (the eigenvalues of  are precisely the roots of ). This holds for all possible eigenvalues , so the two matrices equated by the theorem certainly give the same (null) result when applied to any eigenvector. Now if  admits a basis of eigenvectors, in other words if  is diagonalizable, then the Cayley–Hamilton theorem must hold for , since two matrices that give the same values when applied to each element of a basis must be equal. \n\n product  of  eigenvalues  of \n\nConsider now the function which maps matrices to matrices given by the formula , i.e. which takes a matrix and plugs it into its own characteristic polynomial. Not all matrices are diagonalizable, but for matrices with complex coefficients many of them are: the set of  diagonalizable complex square matrices of a given size is dense in the set of all such square matrices (for a matrix to be diagonalizable it suffices for instance that its characteristic polynomial not have any multiple roots).  Now viewed as a function  (since matrices have entries) we see that this function is continuous. This is true because the entries of the image of a matrix are given by polynomials in the entries of the matrix. Since\n\nand since is dense, by continuity this function must map the entire set of matrices to the zero matrix. Therefore the Cayley–Hamilton theorem is true for complex numbers, and must therefore also hold for or valued matrices..\n\nWhile this provides a valid proof,  the argument is not very satisfactory, since the identities represented by the theorem do not in any way depend on the nature of the matrix (diagonalizable or not), nor on the kind of entries allowed (for matrices with real entries the diagonalizable ones do not form a dense set, and it seems strange one would have to consider complex matrices to see that the Cayley–Hamilton theorem holds for them). We shall therefore now consider only arguments that prove the theorem directly for any matrix using algebraic manipulations only; these also have the benefit of working for matrices with entries in any commutative ring.\n\nThere is a great variety of such proofs of the Cayley–Hamilton theorem, of which several will be given here. They vary in the amount of abstract algebraic notions required to understand the proof. The simplest proofs use just those notions needed to formulate the theorem (matrices, polynomials with numeric entries, determinants), but involve technical computations that render somewhat mysterious the fact that they lead precisely to the correct conclusion. It is possible to avoid such details, but at the price of involving more subtle algebraic notions: polynomials with coefficients in a non-commutative ring, or matrices with unusual kinds of entries.\n\n Adjugate matrices \nAll proofs below use the notion of the adjugate matrix  of an  matrix , the transpose of its cofactor matrix.\n\nThis is a matrix whose coefficients are given  by polynomial expressions in the coefficients of  (in fact, by certain  determinants), in such a way that \nthe following fundamental relations hold,\n\nThese relations are a direct consequence of the basic properties of determinants: evaluation of the  entry of the matrix product on the left gives the expansion by column  of the determinant of the matrix obtained from  by replacing column  by a copy of column , which is  if  and zero otherwise; the matrix product on the right is similar, but for expansions by rows.\n\nBeing a consequence of just algebraic expression manipulation, these relations are valid for matrices with entries in any commutative ring (commutativity must be assumed for determinants to be defined in the first place). This is important to note here, because these relations will be applied below for matrices with non-numeric entries such as polynomials.\n\n A direct algebraic proof \nThis proof uses just the kind of objects needed to formulate the Cayley–Hamilton theorem: matrices with polynomials as entries. The matrix  whose determinant is the characteristic polynomial of  is such a matrix, and since polynomials form a commutative ring, it has an adjugate\n\nThen, according to the right-hand fundamental relation of the adjugate, one has\n\nSince  is also a matrix with polynomials in   as entries, one can, for each   , collect the coefficients of   in each entry to form a matrix   of numbers, such that one has\n\n(The way the entries of   are defined makes clear that no powers higher than  occur). While this looks like a polynomial with matrices as coefficients, we shall not consider such a notion; it is just a way to write a matrix with polynomial entries as a linear combination of  constant matrices, and the coefficient   has been written to the left of the matrix to stress this point of view.\n\nNow, one can expand the matrix product in our equation by bilinearity\n\nWriting\n\none obtains an equality of two matrices with polynomial entries, written as linear combinations of constant matrices with powers of   as coefficients.\n\nSuch an equality can hold only if in any matrix position the entry that is multiplied by a given power   is the same on both sides; it follows that the constant matrices with coefficient  in both expressions must be equal. Writing these equations then for  from   down to 0,  one finds\n\nFinally, multiply the equation of the coefficients of   from the left by  , and sum up:\n\nThe left-hand sides form a telescoping sum and cancel completely; the right-hand sides add up to :\n\nThis completes the proof.\n\n A proof using polynomials with matrix coefficients \nThis proof is similar to the first one, but tries to give meaning to the notion of polynomial with matrix coefficients that was suggested by the expressions occurring in that proof. This requires considerable care, since it is somewhat unusual to consider polynomials with coefficients in a non-commutative ring, and not all reasoning that is valid for commutative polynomials can be applied in this setting.\n\nNotably, while arithmetic of polynomials over a commutative ring models the arithmetic of polynomial functions, this is not the case over a non-commutative ring (in fact there is no obvious notion of polynomial function in this case that is closed under multiplication). So when considering polynomials in  with matrix coefficients, the variable  must not be thought of as an \"unknown\", but as a formal symbol that is to be manipulated according to given rules; in particular one cannot just set   to a specific value.\n\nLet M(n, R) be the ring of n×n matrices with entries in some ring R (such as the real or complex numbers) that has    as an element. Matrices with as coefficients polynomials in , such as  or its adjugate B in the first proof, are elements of M(n, R[t]).\n\nBy collecting like powers of , such matrices can be written as \"polynomials\" in  with constant matrices as coefficients; write M(n, R)[t] for the set of such polynomials. Since this set is in bijection with M(n, R[t]), one defines arithmetic operations on it correspondingly, in particular multiplication is given by\n\nrespecting the order of the coefficient matrices from the two operands; obviously this gives a non-commutative multiplication.\n\nThus, the identity\n\nfrom the first proof can be viewed as one involving a multiplication of elements in M(n, R)[t].\n\nAt this point, it is tempting to simply set  equal to the matrix   , which makes the first factor on the left equal to the null matrix, and the right hand side equal to ; however, this is not an allowed operation when coefficients do not commute. It is possible to define a \"right-evaluation map\" ev : M[t] → M, which replaces each ti by the matrix power  i of   , where one stipulates that the power is always to be multiplied on the right to the corresponding coefficient.\n\nBut this map is not a ring homomorphism: the right-evaluation of a product differs in general from the product of the right-evaluations. This is so because multiplication of polynomials with matrix coefficients does not model multiplication of expressions containing unknowns: a product  is defined assuming that   commutes with  , but this may fail if  is replaced by the matrix  .\n\nOne can work around this difficulty in the particular situation at hand, since the above right-evaluation map does become a ring homomorphism if the matrix    is in the center of the ring of coefficients, so that it commutes with all the coefficients of the polynomials (the argument proving this is straightforward, exactly because commuting  with coefficients is now justified after evaluation).\n\nNow,   is not always in the center of M, but we may replace M with a smaller ring provided it contains all the coefficients of the polynomials in question: ,  , and the coefficients  of the polynomial B. The obvious choice for such a subring is the centralizer Z of  , the subring of all matrices that commute with  ; by definition    is in the center of Z.\n\nThis centralizer obviously contains , and , but one has to show that it contains the matrices . To do this, one combines the two fundamental relations for adjugates, writing out the adjugate B as a polynomial:\n\nEquating the coefficients shows that for each i, we have   Bi = Bi    as desired. Having found the proper setting in which ev is indeed a homomorphism of rings, one can complete the proof as suggested above:\n\nThis completes the proof.\n\n A synthesis of the first two  proofs \nIn the first proof, one was able to determine the coefficients   of   based on the right-hand fundamental relation for the adjugate only. In fact the first  equations derived can be interpreted as determining the quotient  of the Euclidean division of the polynomial  on the left by the monic polynomial , while the final equation expresses the fact that the remainder is zero. This division is performed in the ring of polynomials with matrix coefficients. Indeed, even over a non-commutative ring, Euclidean division by a monic polynomial   is defined, and always produces a unique quotient and remainder with the same degree condition as in the commutative case, provided it is specified at which side one wishes  to be a factor (here that is to the left).\n\nTo see that quotient and remainder are unique (which is the important part of the statement here), it suffices to write  as  and observe that since   is monic,  cannot have a degree less than that of  , unless .\n\nBut the dividend  and divisor  used here both lie in the subring  , where   is the subring of the matrix ring   generated by  : the  -linear span of all powers of   . Therefore, the Euclidean division can in fact be performed within that commutative polynomial ring, and of course it then gives the same quotient   and remainder 0 as in the larger ring; in particular this shows that   in fact lies in  .\n\nBut, in this commutative setting, it is valid to set    to   in the equation \n\nin other words, to apply the evaluation map\n\nwhich is a ring homomorphism, giving\n\njust like in the second proof, as desired.\n\nIn addition to proving the theorem, the above argument tells us that the coefficients  of   are polynomials in  , while from the second proof we only knew that they lie in the centralizer   of  ; in general   is a larger subring than  , and not necessarily commutative. In particular the constant term  lies in  . Since   is an arbitrary square matrix, this proves that  can always be expressed as a polynomial in   (with coefficients that depend on  .\n\nIn fact, the equations found in the first proof allow successively expressing  as polynomials in  , which leads to the identity\n\nvalid for all   matrices, where \n \nis the characteristic polynomial of  .\n\nNote that this identity also implies the statement of the Cayley–Hamilton theorem: one may move   to the right hand side, multiply the resulting equation (on the left or on the right) by  , and use the fact that\n\n A proof using matrices of endomorphisms \nAs was mentioned above, the matrix p(A) in statement of the theorem is obtained by first evaluating the determinant and then substituting the matrix A for t; doing that substitution into the matrix  before evaluating the determinant is not meaningful. Nevertheless, it is possible to give an interpretation where p(A) is obtained directly as the value of a certain determinant, but this requires a more complicated setting, one of matrices over a ring in which one can interpret both the entries  of A, and all of A itself. One could take for this the ring M(n, R) of n×n matrices over R, where the entry  is realised as , and A as itself. But considering matrices with matrices as entries might cause confusion with block matrices, which is not intended, as that gives the wrong notion of determinant (recall that the determinant of a matrix is defined as a sum of products of its entries, and in the case of a block matrix this is generally not the same as the corresponding sum of products of its blocks!). It is clearer to distinguish A from the endomorphism φ of an n-dimensional vector space V (or free R-module if R is not a field) defined by it in a basis e1, ..., en, and to take matrices over the ring End(V) of all such endomorphisms. Then φ ∈ End(V) is a possible matrix entry, while A designates the element of M(n, End(V)) whose i,j entry is endomorphism of scalar multiplication by ; similarly In will be interpreted as element of M(n, End(V)). However, since End(V) is not a commutative ring, no determinant is defined on M(n, End(V)); this can only be done for matrices over a commutative subring of End(V). Now the entries of the matrix  all lie in the subring R[φ] generated by the identity and φ, which is commutative. Then a determinant map M(n, R[φ]) → R[φ] is defined, and  evaluates to the value p(φ) of the characteristic polynomial of A at φ (this holds independently of the relation between A and φ); the Cayley–Hamilton theorem states that p(φ) is the null endomorphism.\n\nIn this form, the following proof can be obtained from that of  (which in fact is the more general statement related to the Nakayama lemma; one takes for the ideal in that proposition the whole ring R). The fact that A is the matrix of φ in the basis e1, ..., en means that\n\nOne can interpret these as n components of one equation in Vn, whose members can be written using the matrix-vector product M(n, End(V)) × Vn → Vn that is defined as usual, but with individual entries ψ ∈ End(V) and v in V being \"multiplied\" by forming ; this gives:\n\nwhere  is the element whose component i is ei (in other words it is the basis e1, ..., en of V written as a column of vectors). Writing this equation as\n\none recognizes the transpose of the matrix  considered above, and its determinant (as element of M(n, R[φ])) is also p(φ). To derive from this equation that p(φ) = 0 ∈ End(V), one left-multiplies by the adjugate matrix of , which is defined in the matrix ring M(n, R[φ]), giving\n\nthe associativity of matrix-matrix and matrix-vector multiplication used in the first step is a purely formal property of those operations, independent of the nature of the entries. Now component i of this equation says that p(φ)(ei) = 0 ∈ V; thus p(φ) vanishes on all ei, and since these elements generate V it follows that p(φ) = 0 ∈ End(V), completing the proof.\n\nOne additional fact that follows from this proof is that the matrix A whose characteristic polynomial is taken need not be identical to the value φ substituted into that polynomial; it suffices that φ be an endomorphism of V satisfying the initial equations\n\nfor some sequence of elements e1,...,en that generate V (which space might have smaller dimension than n, or in case the ring R is not a field it might not be a free module at all).\n\n A bogus \"proof\": p(A) = det(AIn − A) = det(A − A) = 0 \nOne persistent elementary but incorrect argument  for the theorem is to \"simply\" take the definition\n\nand substitute  for , obtaining\n\nThere are many ways to see why this argument is wrong. First, in Cayley–Hamilton theorem, p(A) is an n×n matrix.  However, the right hand side of the above equation is the value of a determinant, which is a scalar. So they cannot be equated unless n = 1 (i.e. A is just a scalar).  Second, in the expression , the variable λ actually occurs at the diagonal entries of the matrix .  To illustrate, consider the characteristic polynomial in the previous example again:\n\nIf one substitutes the entire matrix A for λ in those positions, one obtains\n\nin which the \"matrix\" expression is simply not a valid one.  Note, however, that if scalar multiples of identity matrices\ninstead of scalars are subtracted in the above, i.e. if the substitution is performed as\n\nthen the determinant is indeed zero, but the expanded matrix in question does not evaluate to ; nor can its determinant (a scalar) be compared to p(A) (a matrix).  So the argument that  still does not apply.\n\nActually, if such an argument holds, it should also hold when other multilinear forms instead of determinant is used. For instance, if we consider the permanent function and define , then by the same argument, we should be able to \"prove\" that q(A) = 0.  But this statement is demonstrably wrong.  In the 2-dimensional case, for instance, the permanent of a matrix is given by\n\nSo, for the matrix A in the previous example,\n\nYet one can verify that\n\nOne of the proofs for Cayley–Hamilton theorem above bears some similarity to the argument that . By introducing a matrix with non-numeric coefficients, one can actually let A live inside a matrix entry, but then  is not equal to A, and the conclusion is reached differently.\nProofs using methods of abstract algebra\n\nBasic properties of Hasse–Schmidt derivations on the exterior algebra  of some B-module M (supposed to be free and of finite rank) have been used by  to prove the Cayley–Hamilton theorem.\nAbstraction and generalizations\n\nThe above proofs show that the Cayley–Hamilton theorem holds for matrices with entries in any commutative ring R, and that p(φ) = 0 will hold whenever φ is an endomorphism of an R module generated by elements e1,...,en that satisfies\n\nThis more general version of the theorem is the source of the celebrated Nakayama lemma in commutative algebra and algebraic geometry.\n\nSee also\n Companion matrix\n\nRemarks\n\n Notes \n\n References \n (open access)\n\n \n\n (communicated on June 9, 1862)\n (communicated on June 23, 1862)\n  \"Classroom Note: A Simple Proof of the Leverrier--Faddeev Characteristic Polynomial Algorithm\" \n\n (open archive).\n\nExternal links\n\nA proof from PlanetMath.\nThe Cayley–Hamilton theorem at MathPages\n\nCategory:Theorems in linear algebra\nCategory:Articles containing proofs\nCategory:Matrix theory\nCategory:William Rowan Hamilton"
    },
    {
      "title": "Chebotarev theorem on roots of unity",
      "url": "https://en.wikipedia.org/wiki/Chebotarev_theorem_on_roots_of_unity",
      "text": "The Chebotarev theorem on roots of unity was originally a conjecture made by Ostrowski in the context of lacunary series. \n\nChebotarev was the first to prove it, in the 1930s. This proof involves tools from Galois theory and pleased Ostrowski, who made comments arguing that it \"does meet the requirements of mathematical esthetics\".Stevenhagen et al., 1996\nSeveral proofs have been proposed since,P.E. Frenkel, 2003 and it has even been discovered independently by Dieudonné.J. Dieudonné, 1970\n\n Statement \n\nLet  be a matrix with entries , where . \nIf  is prime then any minor of  is non-zero.\n\nEquivalently, all submatrices of a DFT matrix of prime length are invertible.\n\n Applications \nIn signal processing,Candès, Romberg, Tao, 2006 the theorem was used by T. Tao to extend the uncertainty principle.T. Tao, 2003\n\n Notes \n\n References \n\nCategory:Theorems in linear algebra\nCategory:Theorems in algebraic number theory"
    },
    {
      "title": "Crouzeix's theorem",
      "url": "https://en.wikipedia.org/wiki/Crouzeix%27s_theorem",
      "text": "redirect Crouzeix's conjecture\n\nCategory:Theorems in linear algebra"
    },
    {
      "title": "Dimension theorem for vector spaces",
      "url": "https://en.wikipedia.org/wiki/Dimension_theorem_for_vector_spaces",
      "text": "In mathematics, the dimension theorem for vector spaces states that all bases of a vector space have equally many elements. This number of elements may be finite or infinite (in the latter case, it is a cardinal number), and defines the dimension of the vector space.\n\nFormally, the dimension theorem for vector spaces states that\n\nGiven a vector space , any two bases have the same cardinality.\n\nAs a basis is a generating set that is linearly independent, the theorem is a consequence of the following theorem, which is also useful:\n\nIn a vector space , if  is a generating set, and  is a linearly independent set, then the cardinality of  is not larger than the cardinality of .\n\nIn particular if  is finitely generated, then all its bases are finite and have the  same number of elements.\n\nWhile the proof of the existence of a basis for any vector space in the general case requires Zorn's lemma and is in fact equivalent to the axiom of choice, the uniqueness of the cardinality of the basis requires only the ultrafilter lemma,Howard, P., Rubin, J.: \"Consequences of the axiom of choice\" - Mathematical Surveys and Monographs, vol 59 (1998) . which is strictly weaker (the proof given below, however, assumes trichotomy, i.e., that all cardinal numbers are comparable, a statement which is also equivalent to the axiom of choice). The theorem can be generalized to arbitrary -modules for rings  having invariant basis number.\n\nIn the finitely generated case the proof uses only elementary arguments of algebra, and does not require the axiom of choice nor its weaker variants.\n\nProof\nLet  be a vector space, } be a linearly independent set of elements of , and } be a generating set. One has to prove that the cardinality of  is not larger than that of .\n\nIf  is finite, this results from the Steinitz exchange lemma. (Indeed, the Steinitz exchange lemma implies every finite subset of  has cardinality not larger than that of , hence  is finite with cardinality not larger than that of .) If  is finite, a proof based on matrix theory is also possible.Hoffman, K., Kunze, R., \"Linear Algebra\", 2nd ed., 1971, Prentice-Hall. (Theorem 4 of Chapter 2). \n\nAssume that  is infinite. If  is finite, there is nothing to prove. Thus, we may assume that  is also infinite. Let us suppose that the cardinality of  is larger than that of .This uses the axiom of choice. We have to prove that this leads to a contradiction. \n\nBy Zorn's lemma, every linearly independent set is contained in a maximal linearly independent set . This maximality implies that  spans  and is therefore a basis (the maximality implies that every element of  is linearly dependent from the elements of , and therefore is a linear combination of elements of . As the cardinality of  is greater or equal with the cardinality of , one may replace  with , that is, one may suppose, without loss of generality, that  is a basis. \n\nThus, every  can be written as a finite sum\n\nwhere  is a finite subset of .\nAs  is infinite,  has the same cardinality as .This uses the axiom of choice. Therefore  has cardinality smaller than that of . \nSo there is some  which does not appear\nin any .  The corresponding  can be expressed as a finite linear combination of 's, which in turn can be expressed as finite linear combination of 's, not involving . Hence  is linearly dependent on the other 's, which provides the desired contradiction.\n\nKernel extension theorem for vector spaces\nThis application of the dimension theorem is sometimes itself called the dimension theorem. Let\n\nT: U → V\n\nbe a linear transformation. Then\n\ndim(range(T)) + dim(kernel(T)) = dim(U),\n\nthat is, the dimension of U is equal to the dimension of the transformation's range plus the dimension of the kernel. See rank–nullity theorem for a fuller discussion.\n\nNotes\n\nReferences\n\nCategory:Theorems in abstract algebra\nCategory:Theorems in linear algebra\nCategory:Articles containing proofs"
    },
    {
      "title": "Fundamental theorem of linear algebra",
      "url": "https://en.wikipedia.org/wiki/Fundamental_theorem_of_linear_algebra",
      "text": "In mathematics, the fundamental theorem of linear algebra makes several statements regarding vector spaces. Those statements may be given concretely in terms of the rank r of an  matrix A and its singular value decomposition:\n\nFirst, each matrix  ( has  rows and  columns) induces four fundamental subspaces. These fundamental subspaces are as follows:\n\nname of subspacedefinitioncontaining spacedimensionbasiscolumn space, range or image or  (rank)The first  columns of nullspace or kernel or  (nullity)The last  columns of row space or coimage or  (rank)The first  columns of left nullspace or cokernel or  (corank)The last  columns of \n\nSecondly:\n\n In , , that is, the nullspace is the orthogonal complement of the row space\n In , , that is, the left nullspace is the orthogonal complement of the column space.\ncenter|600px|thumb|The four subspaces associated to a matrix A.\n\nThe dimensions of the subspaces are related by the rank–nullity theorem, and follow from the above theorem.\n\nFurther, all these spaces are intrinsically defined—they do not require a choice of basis—in which case one rewrites this in terms of abstract vector spaces, operators, and the dual spaces as  and : the kernel and image of  are the cokernel and coimage of .\n\n See also \n Rank–nullity theorem\n Closed range theorem\n\nReferences\n Strang, Gilbert. Linear Algebra and Its Applications. 3rd ed. Orlando: Saunders, 1988.\n\n \n\nExternal links\n\n, MIT Linear Algebra Lecture on the Four Fundamental Subspaces on YouTube, from MIT OpenCourseWare\n\nCategory:Theorems in linear algebra\nCategory:Isomorphism theorems\nLinear algebra"
    },
    {
      "title": "Gerbaldi's theorem",
      "url": "https://en.wikipedia.org/wiki/Gerbaldi%27s_theorem",
      "text": "In linear algebra and projective geometry, Gerbaldi's theorem, proved by , states that one can find six pairwise apolar linearly independent nondegenerate ternary quadratic forms.  These are permuted by the Valentiner group.\n\nReferences\n\nCategory:Quadratic forms\nCategory:Theorems in linear algebra\nCategory:Theorems in projective geometry"
    },
    {
      "title": "Goddard–Thorn theorem",
      "url": "https://en.wikipedia.org/wiki/Goddard%E2%80%93Thorn_theorem",
      "text": "In mathematics, and in particular, in the mathematical background of string theory, the Goddard–Thorn theorem (also called the no-ghost theorem) is a theorem describing properties of a functor that quantizes bosonic strings. It is named after Peter Goddard and Charles Thorn.\n\nThe name \"no-ghost theorem\" stems from the fact that in the original statement of the theorem, the natural inner product induced on the output vector space is positive definite. Thus, there were no so-called ghosts (Pauli–Villars ghosts), or vectors of negative norm. The name \"no-ghost theorem\" is also a word play on the no-go theorem of quantum mechanics.\n\n Formalism \nThere are two naturally isomorphic functors that are typically used to quantize bosonic strings.  In both cases, one starts with positive-energy representations of the Virasoro algebra of central charge 26, equipped with Virasoro-invariant bilinear forms, and ends up with vector spaces equipped with bilinear forms.  Here, \"Virasoro-invariant\" means Ln is adjoint to L−n for all integers n.\n\nThe first functor historically is \"old canonical quantization\", and it is given by taking the quotient of the weight 1 primary subspace by the radical of the bilinear form.  Here, \"primary subspace\" is the set of vectors annihilated by Ln for all strictly positive n, and \"weight 1\" means L0 acts by identity.  A second, naturally isomorphic functor, is given by degree 1 BRST cohomology.  Older treatments of BRST cohomology often have a shift in the degree due to a change in choice of BRST charge, so one may see degree −1/2 cohomology in papers and texts from before 1995.  A proof that the functors are naturally isomorphic can be found in Section 4.4 of Polchinski's String Theory text.\n\nThe Goddard–Thorn theorem amounts to the assertion that this quantization functor more or less cancels the addition of two free bosons, as conjectured by Lovelace in 1971.  Lovelace's precise claim was that at critical dimension 26, Virasoro-type Ward identities cancel two full sets of oscillators.  Mathematically, this is the following claim:\n\nLet V be a unitarizable Virasoro representation of central charge 24 with Virasoro-invariant bilinear form, and let π1,1λ be the irreducible module of the R1,1 Heisenberg Lie algebra attached to a nonzero vector λ in R1,1.  Then the image of V ⊗ π1,1λ under quantization is canonically isomorphic to the subspace of V on which L0 acts by 1-(λ,λ).\n\nThe no-ghost property follows immediately, since the positive-definite Hermitian structure of V is transferred to the image under quantization.\n\nApplications\nThe bosonic string quantization functors described here can be applied to any conformal vertex algebra of central charge 26, and the output naturally has a Lie algebra structure.  The Goddard–Thorn theorem can then be applied to concretely describe the Lie algebra in terms of the input vertex algebra.\n\nPerhaps the most spectacular case of this application is Borcherds's proof of the Monstrous Moonshine conjecture, where the unitarizable Virasoro representation is the Monster vertex algebra (also called \"Moonshine module\") constructed by Frenkel, Lepowsky, and Meurman.  By taking a tensor product with the vertex algebra attached to a rank 2 hyperbolic lattice, and applying quantization, one obtains the monster Lie algebra, which is a generalized Kac–Moody algebra graded by the lattice.  By using the Goddard–Thorn theorem, Borcherds showed that the homogeneous pieces of the Lie algebra are naturally isomorphic to graded pieces of the Moonshine module, as representations of the monster simple group.\n\nEarlier applications include Frenkel's determination of upper bounds on the root multiplicities of the Kac-Moody Lie algebra whose Dynkin diagram is the Leech lattice, and Borcherds's construction of a generalized Kac-Moody Lie algebra that contains Frenkel's Lie algebra and saturates Frenkel's 1/∆ bound.\n\n References \n\n R. Borcherds, The Monster Lie algebra Adv. Math. 83 no. 1 (1990) 30–47.\n R. Borcherds, Monstrous moonshine and monstrous Lie superalgebras Invent. Math. 109 (1992), 405–444.\n I. Frenkel, Representations of Kac-Moody algebras and dual resonance models Applications of group theory in theoretical physics, Lect. Appl. Math. 21 A.M.S. (1985) 325–353.\n P. Goddard and C. B. Thorn, Compatibility of the dual Pomeron with unitarity and the absence of ghosts in the dual resonance model, Phys. Lett., B 40, No. 2 (1972), 235-238.\n C. Lovelace Pomeron form factors and dual regge cuts Phys. Lett. 34B (1971) 500–506.\n J. Polchinski, String Theory Cambridge University Press, Cambridge, UK (1998).\n\nCategory:Theorems in linear algebra\nCategory:String theory\nCategory:Theorems in mathematical physics"
    },
    {
      "title": "Hawkins–Simon condition",
      "url": "https://en.wikipedia.org/wiki/Hawkins%E2%80%93Simon_condition",
      "text": "The Hawkins–Simon condition refers to a result in mathematical economics, attributed to David Hawkins and Herbert A. Simon, that guarantees the existence of a non-negative output vector that solves the equilibrium relation in the input–output model where demand equals supply. More precisely, it states a condition for  under which the input–output system\n\nhas a solution  for any . Here  is the identity matrix and  is called the input–output matrix or Leontief matrix after Wassily Leontief, who empirically estimated it in the 1940s. Together, they describe a system in which\n\nwhere  is the amount of the ith good used to produce one unit of the jth good,  is the amount of the jth good produced, and  is the amount of final demand for good i. Rearranged and written in vector notation, this gives the first equation.\n\nDefine , where  is an  matrix with    . Then the Hawkins–Simon theorem states that the following two conditions are equivalent\n(i) There exists an  such that .\n(ii) All the successive leading principal minors of  are positive, that is\n\nFor a proof, see Morishima (1964), Nikaido (1968), or Murata (1977). Condition (ii) is known as Hawkins–Simon condition. This theorem was independently discovered by David Kotelyanskiĭ, as it is referred to by Felix Gantmacher as Kotelyanskiĭ lemma.\n\n See also \n Diagonally dominant matrix\n Perron–Frobenius theorem\n\n References \n\n Further reading \n \n \n\nCategory:Theorems in linear algebra"
    },
    {
      "title": "MacMahon Master theorem",
      "url": "https://en.wikipedia.org/wiki/MacMahon_Master_theorem",
      "text": "In mathematics, the MacMahon Master theorem (MMT) is a result in enumerative combinatorics and linear algebra.  It was discovered by Percy MacMahon and proved in his monograph Combinatory analysis (1916).  It is often used to derive binomial identities, most notably Dixon's identity.\n\n Background \nIn the monograph, MacMahon found so many applications of his result, he called it \"a master theorem in the Theory of Permutations.\"  He explained the title as follows: \"a Master Theorem from the masterly and rapid fashion in which it deals with various questions otherwise troublesome to solve.\"\n\nThe result was re-derived (with attribution) a number of times, most notably by  I. J. Good who derived it from his multilinear generalization of the Lagrange inversion theorem.  MMT was also popularized by Carlitz who found an exponential power series  version.  In 1962, Good found a short proof of Dixon's identity from MMT.  In 1969, Cartier and Foata found a new proof of MMT by combining algebraic and bijective ideas (built on Foata's thesis) and further applications to combinatorics on words, introducing the concept of traces.  Since then, MMT has become a standard tool in enumerative combinatorics.\n\nAlthough various q-Dixon identities have been known for decades, except for a Krattenthaler–Schlosser extension (1999), the proper q-analog of MMT remained elusive.  After Garoufalidis–Lê–Zeilberger's quantum extension (2006), a number of noncommutative extensions were developed by Foata–Han, Konvalinka–Pak, and Etingof–Pak.  Further connections to Koszul algebra and quasideterminants were also found by Hai–Lorentz, Hai–Kriegk–Lorenz, Konvalinka–Pak, and others.\n\nFinally, according to J. D. Louck, theoretical physicist Julian Schwinger re-discovered the MMT in the context of his generating function approach to the angular momentum theory of many-particle systems.  Louck writes:\n\n Precise statement \nLet  be a complex matrix, and let  be formal variables.  Consider a coefficient\n\n(Here the notation  means \"the coefficient of monomial  in \".)  Let  be another set of formal variables, and let  be a diagonal matrix.  Then\n\nwhere the sum runs over all nonnegative integer vectors ,\nand  denotes the identity matrix of size .\n\n Derivation of Dixon's identity \nConsider a matrix\n\nCompute the coefficients G(2n, 2n, 2n) directly from the definition:\n\nwhere the last equality follows from the fact that on the right-hand side we have the product of the following coefficients:\n\nwhich are computed from the binomial theorem. On the other hand, we can compute the determinant explicitly:\n\nTherefore, by the MMT, we have a new formula for the same coefficients:\n\n \n\nwhere the last equality follows from the fact that we need to use an equal number of times all three terms in the power.  Now equating the two formulas for coefficients G(2n, 2n, 2n) we obtain an equivalent version of Dixon's identity:\n\nSee also\nPermanent\n\n References \n\n P.A. MacMahon, Combinatory analysis, vols 1 and 2, Cambridge University Press, 1915–16.\n \n \n P. Cartier and D. Foata, Problèmes combinatoires de commutation et réarrangements, Lecture Notes in Mathematics, no. 85, Springer, Berlin, 1969.\n L. Carlitz, An Application of MacMahon's Master Theorem, SIAM Journal on Applied Mathematics 26 (1974), 431–436.\n I.P. Goulden and D. M. Jackson, Combinatorial Enumeration, John Wiley, New York, 1983.\n C. Krattenthaler and M. Schlosser, A new multidimensional matrix inverse with applications to multiple q-series, Discrete Math. 204 (1999), 249–279.\n S. Garoufalidis, T. T. Q. Lê and D. Zeilberger, The Quantum MacMahon Master Theorem, Proc. Natl. Acad. of Sci. 103  (2006),  no. 38, 13928–13931 (eprint).\n M. Konvalinka and I. Pak, Non-commutative extensions of the MacMahon Master Theorem, Adv. Math. 216 (2007), no. 1. (eprint).\n D. Foata and G.-N. Han, A new proof of the Garoufalidis-Lê-Zeilberger Quantum MacMahon Master Theorem,  J. Algebra  307  (2007),  no. 1, 424–431 (eprint).\n D. Foata and G.-N. Han, Specializations and extensions of the quantum MacMahon Master Theorem, Linear Algebra Appl 423  (2007),  no. 2–3, 445–455 (eprint).\n P.H. Hai and M. Lorenz, Koszul algebras and the quantum MacMahon master theorem,  Bull. Lond. Math. Soc.  39  (2007),  no. 4, 667–676. (eprint).\n P. Etingof and I. Pak, An algebraic extension of the MacMahon master theorem,  Proc. Amer. Math. Soc.  136  (2008),  no. 7, 2279–2288 ( eprint).\n P.H. Hai, B. Kriegk and M. Lorenz, N-homogeneous superalgebras, J. Noncommut. Geom. 2 (2008) 1–51 (eprint).\n J.D. Louck, Unitary symmetry and combinatorics, World Sci., Hackensack, NJ, 2008.\n\nCategory:Enumerative combinatorics\nCategory:Factorial and binomial topics\nCategory:Articles containing proofs\nCategory:Theorems in combinatorics\nCategory:Theorems in linear algebra"
    },
    {
      "title": "Perron–Frobenius theorem",
      "url": "https://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem",
      "text": "In linear algebra, the Perron–Frobenius theorem, proved by  and , asserts that a real square matrix with positive entries has a unique largest real eigenvalue and that the corresponding eigenvector can be chosen to have strictly positive components, and also asserts a similar statement for certain classes of nonnegative matrices. This theorem has important applications to probability theory (ergodicity of Markov chains); to the theory of dynamical systems (subshifts of finite type); to economics (Okishio's theorem, Hawkins–Simon condition);\nto demography (Leslie population age distribution model); \nto social networks (DeGroot learning process), to  Internet search engines  and even to ranking of football\nteams. The first to discuss the ordering of players within tournaments using Perron–Frobenius eigenvectors is Edmund Landau.\n\nStatement\nLet positive and non-negative respectively describe matrices with exclusively positive real numbers as elements and matrices with exclusively non-negative real numbers as elements. The eigenvalues of a real square matrix A are complex numbers that make up the spectrum of the matrix. The exponential growth rate of the matrix powers Ak as k → ∞ is controlled by the eigenvalue of A with the largest absolute value (modulus). The Perron–Frobenius theorem describes the properties of the leading eigenvalue and of the corresponding eigenvectors when A is a non-negative real square matrix. Early results were due to  and concerned positive matrices. Later,  found their extension to certain classes of non-negative matrices.\n\nPositive matrices\nLet  be an  positive matrix:  for . Then the following statements hold.\n\n There is a positive real number r, called the Perron root or the Perron–Frobenius eigenvalue (also called the leading eigenvalue or dominant eigenvalue), such that r is an eigenvalue of A and any other eigenvalue λ (possibly, complex) in absolute value is strictly smaller than r , |λ| < r. Thus, the spectral radius  is equal to r. If the matrix coefficients are algebraic, this implies that the eigenvalue is a Perron number.\n The Perron–Frobenius eigenvalue is simple: r is a simple root of the characteristic polynomial of A. Consequently, the eigenspace associated to r is one-dimensional. (The same is true for the left eigenspace, i.e., the eigenspace for AT, the transpose of A.)\n There exists an eigenvector v = (v1,…,vn) of A with eigenvalue r such that all components of v are positive: A v = r v, vi > 0 for 1 ≤ i ≤ n. (Respectively, there exists a positive left eigenvector w : wT A = r wT, wi > 0.) It is known in the literature under many variations as the Perron vector, Perron eigenvector, Perron-Frobenius eigenvector, leading eigenvector, or dominant eigenvector. \n There are no other positive (moreover non-negative) eigenvectors except positive multiples of v (respectively, left eigenvectors except w), i.e., all other eigenvectors must have at least one negative or non-real component.\n , where the left and right eigenvectors for A are normalized so that wTv = 1. Moreover, the matrix v wT is the projection onto the eigenspace corresponding to r. This projection is called the Perron projection.\n Collatz–Wielandt formula: for all non-negative non-zero vectors x, let f(x) be the minimum value of [Ax]i / xi taken over all those i such that xi ≠ 0. Then f is a real valued function whose maximum over all non-negative non-zero vectors x is the Perron–Frobenius eigenvalue.\n A \"Min-max\" Collatz–Wielandt formula takes a form similar to the one above: for all strictly positive vectors x, let g(x) be the maximum value of [Ax]i / xi taken over i. Then g is a real valued function whose minimum over all strictly positive vectors x is the Perron–Frobenius eigenvalue.\n Donsker–Varadhan–Friedland formula: Let p be a probability vector and x a strictly positive vector.  Then  Donsker, M.D. and Varadhan, S.S., 1975. On a variational formula for the principal eigenvalue for operators with maximum principle. Proceedings of the National Academy of Sciences, 72(3), pp.780-783.Friedland, S., 1981. Convex spectral functions. Linear and multilinear algebra, 9(4), pp.299-316.\n The Perron–Frobenius eigenvalue satisfies the inequalities\n\nThese claims can be found in Meyer chapter 8 claims 8.2.11–15 page 667 and exercises 8.2.5,7,9 pages 668–669.\n\nThe left and right eigenvectors w and v are sometimes normalized so that the sum of their components is equal to 1; in this case, they are sometimes called stochastic eigenvectors.  Often they are normalized so that the right eigenvector v sums to one, while .\n\nNon-negative matrices\nAn extension of the theorem to matrices with non-negative entries is also available. In order to highlight the similarities and differences between the two cases the following points are to be noted: every non-negative matrix can be obtained as a limit of positive matrices, thus one obtains the existence of an eigenvector with non-negative components; the corresponding eigenvalue will be non-negative and greater than or equal, in absolute value, to all other eigenvalues. However, the simple examples\n \nshow that for non-negative matrices there may exist eigenvalues of the same absolute value as the maximal one ((1) and (−1) – eigenvalues of the first matrix); moreover the maximal eigenvalue may not be a simple root of the characteristic polynomial, can be zero and the corresponding eigenvector (1,0) is not strictly positive (second example). So it may seem that most properties are broken for non-negative matrices, however Frobenius found the right way to generalize to this case.\n\nThe key feature of theory in the non-negative case is to find some special subclass of non-negative matrices— irreducible matrices— for which a non-trivial generalization is possible. Namely, although the eigenvalues attaining the maximal absolute value may not be unique, the structure of maximal eigenvalues is under control: they have the form ei2πl/hr, where h is an integer called the period of matrix, r is a real strictly positive eigenvalue, and l = 0, 1, ..., h − 1.\nThe eigenvector corresponding to r has strictly positive components (in contrast with the general case of non-negative matrices, where components are only non-negative). Also all such eigenvalues are simple roots of the characteristic polynomial. Further properties are described below.\n\nClassification of matrices\nLet A be a square matrix (not necessarily positive or even real).\nThe matrix A is irreducible if any of the following equivalent properties\nholds.\n\nDefinition 1 : A does not have non-trivial invariant coordinate subspaces.\nHere a non-trivial coordinate subspace means a linear subspace spanned by any proper subset of standard basis vectors of Rn. More explicitly, for any linear subspace spanned by standard basis vectors ei1 , ...,\neik, 0 < k < n its image under the action of A is not contained in the same subspace.\n\nDefinition 2: A cannot be conjugated into block upper triangular form by a permutation matrix P:\n \nwhere E and G are non-trivial (i.e. of size greater than zero) square matrices.\n\nIf A is non-negative other definitions exist:\n\nDefinition 3: For every pair of indices i and j, there exists a natural number m such that (Am)ij is positive.\n\nDefinition 4: One can associate with a matrix A a certain directed graph GA. It has exactly n vertices, where n is size of A, and there is an edge from vertex i to vertex j precisely when Aij > 0. Then the matrix A is irreducible if and only if its associated graph GA is strongly connected.\n\nThis notion is somewhat reminiscent of that of a free action of a group; if one could somehow build a group out of A, then the space Rn would be an irreducible representation. (One can build a group by  considering the exponential .) However, the notion of an irreducible matrix is fundamentally easier to satisfy than an irreducible representation, because only coordinate subspaces are considered.\n\nA matrix is reducible if it is not irreducible.\n\nLet A be non-negative. Fix an index i and define the period of index i  to be the greatest common divisor of all natural numbers m such that (Am)ii > 0. When A is irreducible, the period of every index is the same and is called the period of A.  In fact, when A is irreducible, the period can be defined as the greatest common divisor of the lengths of the closed directed paths in GA (see Kitchens page 16). The period is also called the index of imprimitivity\n(Meyer page 674) or the order of cyclicity.\n\nIf the period is 1, A is aperiodic.\n\nA matrix A is primitive if it is non-negative and its mth power is positive for some natural number m (i.e. the same m works for all pairs of indices). It can be proved that primitive matrices  are the same as irreducible aperiodic non-negative matrices.\n\nA positive square matrix is primitive and a primitive matrix is irreducible. All statements of the Perron–Frobenius theorem for positive matrices remain true for primitive matrices. However, a general non-negative irreducible matrix A may possess several eigenvalues whose absolute value is equal to the spectral radius of A, so the statements need to be correspondingly modified. Actually the number of such eigenvalues is exactly equal to the period. Results for non-negative matrices were first obtained by Frobenius in 1912.\n\nPerron–Frobenius theorem for irreducible matrices\n\nLet A be an irreducible non-negative n × n matrix with period h and spectral radius ρ(A) = r. Then the following statements hold.\n\n The number r is a positive real number and it is an eigenvalue of the matrix A, called the Perron–Frobenius eigenvalue.\n The Perron–Frobenius eigenvalue r is simple. Both right and left eigenspaces associated with r are one-dimensional.\n A has a right eigenvector v with eigenvalue r whose components are all positive.\n Likewise, A has a left eigenvector w with eigenvalue r whose components are all positive.\n The only eigenvectors whose components are all positive are those associated with the eigenvalue r.\n The matrix A has exactly h (where h is the period) complex eigenvalues with absolute value r. Each of them is a simple root of the characteristic polynomial and is the product of r with an hth root of unity.\n Let ω = 2π/h. Then the matrix A is similar to eiωA, consequently the spectrum of A is invariant under multiplication by eiω (corresponding to the rotation of the complex plane by the angle ω).\n If h > 1 then there exists a permutation matrix P such that\n\nwhere the blocks along the main diagonal are zero square matrices.\n\n9. Collatz–Wielandt formula: for all non-negative non-zero vectors x let f(x) be the minimum value of [Ax]i / xi taken over all those i such that xi ≠ 0. Then f is a real valued function whose maximum is the Perron–Frobenius eigenvalue.\n\n10. The Perron–Frobenius eigenvalue satisfies the inequalities\n\nThe matrix  shows that the (square) zero-matrices along the diagonal may be of different sizes, the blocks Aj need not be square, and h need not divide n.\n\nFurther properties\nLet A be an irreducible non-negative matrix, then:\n (I+A)n−1 is a positive matrix. (Meyer claim 8.3.5 p. 672).\n Wielandt's theorem. If |B|<A, then ρ(B)≤ρ(A). If equality holds (i.e. if μ=ρ(A)eiφ is eigenvalue for B), then B = eiφ D AD−1 for some diagonal unitary matrix D (i.e. diagonal elements of D equals to eiΘl, non-diagonal are zero).\n If some power Aq is reducible, then it is completely reducible, i.e. for some permutation matrix P, it is true that: , where Ai are irreducible matrices having the same maximal eigenvalue. The number of these matrices d is the greatest common divisor of q and h, where h is period of A.\n If c(x)=xn+ck1 xn-k1 +ck2 xn-k2 + ... + cks xn-ks is the characteristic polynomial of A in which the only non-zero coefficients are listed, then the period of A equals to the greatest common divisor for k1, k2, ... , ks.\n Cesàro averages:  where the left and right eigenvectors for A are normalized so that wTv = 1. Moreover, the matrix v wT is the spectral projection corresponding to r - Perron projection.\n Let r be the Perron–Frobenius eigenvalue, then the adjoint matrix for (r-A) is positive.\n If A has at least one non-zero diagonal element, then A is primitive.\n If 0 ≤ A < B, then rA ≤ rB. Moreover, if B is irreducible, then the inequality is strict: rA < rB.\n\nOne of the definitions of primitive matrix requires A to be non-negative and there exists m, such that Am is positive. One may one wonder how big m can be, depending on the size of A. The following answers this question.\n Assume A is non-negative primitive matrix of size n, then An2 − 2n + 2 is positive. Moreover, if n > 1, there exists a matrix M given below, such that Mk is not positive (but of course still non-negative) for all k < n2 − 2n + 2, in particular (Mn2 − 2n+1)11 = 0.\n\nApplications\nNumerous books have been written on the subject of non-negative matrices, and Perron–Frobenius theory is invariably a central feature. The following examples given below only scratch the surface of its vast application domain.\n\nNon-negative matrices\nThe Perron–Frobenius theorem does not apply directly to non-negative matrices. Nevertheless, any reducible square matrix A may be written in upper-triangular block form (known as the normal form of a reducible matrix)\nPAP−1 = \n\nwhere P is a permutation matrix and each Bi is a square matrix that is either irreducible or zero. Now if A is\nnon-negative then so too is each block of PAP−1, moreover the spectrum of A is just the union of the spectra of the\nBi.\n\nThe invertibility of A can also be studied. The inverse of PAP−1 (if it exists) must have diagonal blocks of the form Bi−1 so if any\nBi isn't invertible then neither is PAP−1 or A.\nConversely let D be the block-diagonal matrix corresponding to PAP−1, in other words PAP−1 with the\nasterisks zeroised. If each Bi is invertible then so is D and D−1(PAP−1) is equal to the\nidentity plus a nilpotent matrix. But such a matrix is always invertible (if Nk = 0 the inverse of 1 − N is\n1 + N + N2 + ... + Nk−1) so PAP−1 and A are both invertible.\n\nTherefore, many of the spectral properties of A may be deduced by applying the theorem to the irreducible Bi. For example, the Perron root is the maximum of the ρ(Bi). While there will still be eigenvectors with non-negative components it is quite possible\nthat none of these will be positive.\n\nStochastic matrices\nA row (column) stochastic matrix is a square matrix each of whose rows (columns) consists of non-negative real numbers whose sum is unity. The theorem cannot be applied directly to such matrices because they need not be irreducible.\n\nIf A is row-stochastic then the column vector with each entry 1 is an eigenvector corresponding to the eigenvalue 1, which is also ρ(A) by the remark above. It might not be the only eigenvalue on the unit circle: and the associated eigenspace can be multi-dimensional. If A is row-stochastic and irreducible then the Perron projection is also row-stochastic and all its rows are equal.\n\nAlgebraic graph theory\nThe theorem has particular use in algebraic graph theory. The \"underlying graph\" of a nonnegative n-square matrix is the graph with vertices numbered 1, ..., n and arc ij if and only if Aij ≠ 0. If the underlying graph of such a matrix is strongly connected, then the matrix is irreducible, and thus the theorem applies. In particular, the adjacency matrix of a strongly connected graph is irreducible.\n\nFinite Markov chains\nThe theorem has a natural interpretation in the theory of finite Markov chains (where it is the matrix-theoretic equivalent of the convergence of an irreducible finite Markov chain to its stationary distribution, formulated in terms of the transition matrix of the chain; see, for example, the article on the subshift of finite type).\n\nCompact operators\n\nMore generally, it can be extended to the case of non-negative compact operators, which, in many ways, resemble finite-dimensional matrices. These are commonly studied in physics, under the name of transfer operators, or sometimes Ruelle–Perron–Frobenius operators (after David Ruelle). In this case, the leading eigenvalue corresponds to the thermodynamic equilibrium of a dynamical system, and the lesser eigenvalues to the decay modes of a system that is not in equilibrium. Thus, the theory offers a way of discovering the arrow of time in what would otherwise appear to be reversible, deterministic dynamical processes, when examined from the point of view of point-set topology.\n\nProof methods\nA common thread in many proofs is the Brouwer fixed point theorem. Another popular method is that of Wielandt (1950). He used the Collatz–Wielandt formula described above to extend and clarify Frobenius's work. Another proof is based on the spectral theory from which part of the arguments are borrowed.\n\nPerron root is strictly maximal eigenvalue for positive (and primitive) matrices\nIf A is a positive (or more generally primitive) matrix, then there exists a real positive eigenvalue r (Perron–Frobenius eigenvalue or Perron root), which is strictly greater in absolute value than all other eigenvalues, hence r is the spectral radius of A.\n\nThis statement does not hold for general non-negative irreducible matrices, which have h eigenvalues with the same absolute eigenvalue as r, where h is the period of A.\n\nProof for positive matrices\nLet A be a positive matrix, assume that its spectral radius ρ(A) = 1 (otherwise consider A/ρ(A)). Hence, there exists an eigenvalue λ on the unit circle, and all the other eigenvalues are less or equal 1 in absolute value. Assume that λ ≠ 1. Then there exists a positive integer m such that Am is a positive matrix and the real part of λm is negative. Let ε be half the smallest diagonal entry of Am and set T = Am − ε which is yet another positive matrix. Moreover, if Ax = λx then Amx = λmx thus λm − ε is an eigenvalue of T. Because of the choice of m this point lies outside the unit disk consequently ρ(T) > 1. On the other hand, all the entries in T are positive and less than or equal to those in Am so by Gelfand's formula ρ(T) ≤ ρ(Am) ≤ ρ(A)m = 1. This contradiction means that λ=1 and there can be no other eigenvalues on the unit circle.\n\nAbsolutely the same arguments can be applied to the case of primitive matrices; we just need to mention the following simple lemma, which clarifies the properties of primitive matrices.\n\nLemma\nGiven a non-negative A, assume there exists m, such that Am is positive, then Am+1, Am+2, Am+3,... are all positive.\n\nAm+1 = AAm, so it can have zero element only if some row of A is entirely zero, but in this case the same row of Am will be zero.\n\nApplying the same arguments as above for primitive matrices, prove the main claim.\n\nPower method and the positive eigenpair\nFor a positive (or more generally irreducible non-negative) matrix A the dominant eigenvector is real and strictly positive (for non-negative A respectively non-negative.)\n\nThis can be established using the power method, which states that for a sufficiently generic (in the sense below) matrix A the sequence of vectors  bk+1 = Abk / | Abk | converges to the eigenvector with the maximum eigenvalue. (The initial vector b0 can be chosen arbitrarily except for some measure zero set). Starting with a non-negative vector b0 produces the sequence of non-negative vectors bk. Hence the limiting vector is also non-negative. By the power method this limiting vector is the dominant eigenvector for A, proving the assertion. The corresponding eigenvalue is non-negative.\n\nThe proof requires two additional arguments. First, the power method converges for matrices which do not have several eigenvalues of the same absolute value as the maximal one. The previous section's argument guarantees this.\n\nSecond, to ensure strict positivity of all of the components of the eigenvector for the case of irreducible matrices. This follows from the following fact, which is of independent interest:\n\nLemma: given a positive (or more generally irreducible non-negative) matrix A and v as any non-negative eigenvector for A, then it is necessarily strictly positive and the corresponding eigenvalue is also strictly positive.\n\nProof. One of the definitions of irreducibility for non-negative matrices is that for all indexes i,j there exists m, such that (Am)ij is strictly positive. Given a non-negative eigenvector v, and that at least one of its components say j-th is strictly positive, the corresponding eigenvalue is strictly positive, indeed, given n such that (An)ii >0, hence:  rnvi =\nAnvi ≥\n(An)iivi\n>0. Hence r is strictly positive. The eigenvector is strict positivity. Then given m, such that (Am)ij >0, hence:  rmvj =\n(Amv)j ≥\n(Am)ijvi >0, hence\nvj is strictly positive, i.e., the eigenvector is strictly positive.\n\nMultiplicity one\nThis section proves that the Perron–Frobenius eigenvalue is a simple root of the characteristic polynomial of the matrix. Hence the eigenspace associated to Perron–Frobenius eigenvalue  r is one-dimensional. The arguments here are close to those in Meyer.\n\nGiven a strictly positive eigenvector v corresponding to r and another eigenvector w with the same eigenvalue. (Vector w can be chosen to be real, because A and r are both real, so the null space of A-r has a basis consisting of real vectors). Assuming at least one of the components of w is positive (otherwise multiply w by −1). Given  maximal possible α such that u=v- α w is non-negative, then one of the components of u is zero, otherwise α is not maximum. Vector u is an eigenvector. It is non-negative, hence by the lemma described in the previous section non-negativity implies strict positivity for any eigenvector. On the other hand, as above at least one component of u is zero. The contradiction implies that w does not exist.\n\nCase: There are no Jordan cells corresponding to the Perron–Frobenius eigenvalue r and all other eigenvalues which have the same absolute value.\n\nIf there is a Jordan cell, then the infinity norm\n(A/r)k∞ tends to infinity for k → ∞ ,\nbut that contradicts the existence of the positive eigenvector.\n\nGiven r = 1, or A/r. Letting v be a Perron–Frobenius strictly positive eigenvector, so Av=v, then:\n\nSo Ak∞ is bounded for all k. This gives another proof that there are no eigenvalues which have greater absolute value than Perron–Frobenius one. It also contradicts the existence of the Jordan cell for any eigenvalue which has absolute value equal to 1 (in particular for the Perron–Frobenius one), because existence of the Jordan cell implies that Ak∞ is unbounded. For a two by two matrix:\n \nhence Jk∞ = |k + λ| (for |λ| = 1), so it tends to infinity when k does so. Since Jk = C−1 AkC, then Ak ≥ Jk/ (C−1 C ), so it also tends to infinity. The resulting contradiction implies that there are no Jordan cells for the corresponding eigenvalues.\n\nCombining the two claims above reveals that the Perron–Frobenius eigenvalue r is simple root of the characteristic polynomial. In the case of nonprimitive matrices, there exist other eigenvalues which have the same absolute value as r.  The same claim is true for them, but requires more work.\n\nNo other non-negative eigenvectors\nGiven positive (or more generally irreducible non-negative matrix) A, the Perron–Frobenius eigenvector is the only (up to multiplication by constant) non-negative eigenvector for A.\n\nOther eigenvectors must contain negative or complex components since eigenvectors for different eigenvalues are orthogonal in some sense, but two positive eigenvectors cannot be orthogonal, so they must correspond to the same eigenvalue, but the eigenspace for the Perron–Frobenius is one-dimensional.\n\nAssuming there exists an eigenpair (λ, y) for A, such that vector y is positive, and given (r, x), where x – is the left Perron–Frobenius eigenvector for A (i.e. eigenvector for AT), then\nrxTy = (xT A) y = xT (Ay) = λxTy, also xT y > 0, so one has: r = λ. Since the eigenspace for the Perron–Frobenius eigenvalue r is one-dimensional, non-negative eigenvector y is a multiple of the Perron–Frobenius one.\n\nCollatz–Wielandt formula\nGiven a positive (or more generally irreducible non-negative matrix) A, for all non-negative non-zero vectors x and f(x) as the minimum value of [Ax]i / xi taken over all those i such that xi ≠ 0, then f is a real valued function whose maximum is the Perron–Frobenius eigenvalue r.\n\nHere, r is attained for x taken to be the Perron–Frobenius eigenvector v. The proof requires that values f on the other vectors are less or equal. Given a vector x. Let ξ=f(x), so 0≤ξx≤Ax and  w to be the right eigenvector for A, then wT ξx ≤ wT (Ax) = (wT A)x = r wT x . Hence ξ≤r.\n\nPerron projection as a limit: Ak/rk\nLet A be a positive (or more generally, primitive) matrix, and let r be its Perron–Frobenius eigenvalue.\n There exists a limit Ak/rk for k → ∞, denote it by P.\n P is a projection operator: P2 = P, which commutes with A: AP = PA.\n The image of P is one-dimensional and spanned by the Perron–Frobenius eigenvector v (respectively for PT—by the Perron–Frobenius eigenvector w for AT).\n P = vwT, where v,w are normalized such that wT v = 1.\n Hence P is a positive operator.\n\nHence P is a spectral projection for the Perron–Frobenius eigenvalue r, and is called the Perron projection. The above assertion is not true for general non-negative irreducible matrices.\n\nActually the claims above (except claim 5) are valid for any matrix M such that there exists an eigenvalue r which is strictly greater than the other eigenvalues in absolute value and is the simple root of the characteristic polynomial. (These requirements hold for primitive matrices as above).\n\nGiven that M is diagonalizable, M is conjugate to a diagonal matrix with eigenvalues r1, ... , rn on the diagonal (denote r1 = r). The matrix Mk/rk will be conjugate (1, (r2/r)k, ... , (rn/r)k), which tends to (1,0,0,...,0), for k → ∞, so the limit exists. The same method works for general M (without assuming that M is diagonalizable).\n\nThe projection and commutativity properties are elementary corollaries of the definition: MMk/rk =  Mk/rk M ; P2 = lim M2k/r2k = P. The third fact is also elementary: M(Pu) = M lim Mk/rk u = lim rMk+1/rk+1u, so taking the limit yields M(Pu) = r(Pu), so image of P lies in the r-eigenspace for M, which is one-dimensional by the assumptions.\n\nDenoting by v, r-eigenvector for M (by w for MT). Columns of P are multiples of v, because the image of P is spanned by it. Respectively, rows of w. So P takes a form (a v wT), for some a. Hence its trace equals to (a wT v). Trace of projector equals the dimension of its image. It was proved before that it is not more than one-dimensional. From the definition one sees that P acts identically on the r-eigenvector for M. So it is one-dimensional. So choosing (wTv) = 1, implies P = vwT.\n\nInequalities for Perron–Frobenius eigenvalue\nFor any non-nonegative matrix A its Perron–Frobenius eigenvalue r satisfies the inequality:\n\nThis is not specific to non-negative matrices: for any matrix A with an eigenvalue  it is true\nthat . This is an immediate corollary of the\nGershgorin circle theorem. However another proof is more direct:\n\nAny matrix induced norm satisfies the inequality  for any eigenvalue  because, if  is a corresponding eigenvector, . The infinity norm of a matrix is the maximum of row sums:  Hence the desired inequality is exactly   applied to the non-negative matrix A.\n\nAnother inequality is:\n\nThis fact is specific to non-negative matrices; for general matrices there is nothing similar. Given  that A is positive (not just non-negative), then there exists a positive eigenvector w such that Aw = rw and the smallest component of w (say wi) is 1. Then r = (Aw)i ≥ the sum of the numbers in row i of A. Thus the minimum row sum gives a lower bound for r and this observation can be extended to all non-negative matrices by continuity.\n\nAnother way to argue it is via the Collatz-Wielandt formula. One takes the vector x = (1, 1, ..., 1) and immediately obtains the inequality.\n\nFurther proofs\n\nPerron projection\nThe proof now proceeds using spectral decomposition. The trick here is to split the Perron root from the other eigenvalues. The spectral projection associated with the Perron root is called the Perron projection and it enjoys the following property:\n\nThe Perron projection of an irreducible non-negative square matrix is a positive matrix.\n\nPerron's findings and also (1)–(5) of the theorem are corollaries of this result. The key point is that a positive projection always has rank one. This means that if A is an irreducible non-negative square matrix then the algebraic and geometric multiplicities of its Perron root are both one. Also if P is its Perron projection then AP = PA = ρ(A)P so every column of P is a positive right eigenvector of A and every row is a positive left eigenvector. Moreover, if Ax = λx then PAx = λPx = ρ(A)Px which means Px = 0 if λ ≠ ρ(A). Thus the only positive eigenvectors are those associated with ρ(A). If A is a primitive matrix with ρ(A) = 1 then it can be decomposed as P ⊕ (1 − P)A so that An = P + (1 − P)An. As n increases the second of these terms decays to zero leaving P as the limit of An as n → ∞.\n\nThe power method is a convenient way to compute the Perron projection of a primitive matrix. If v and w are the positive row and column vectors that it generates then the Perron projection is just wv/vw. It should be noted that the spectral projections aren't neatly blocked as in the Jordan form. Here they are overlaid and each generally has complex entries extending to all four corners of the square matrix. Nevertheless, they retain their mutual orthogonality which is what facilitates the decomposition.\n\nPeripheral projection\nThe analysis when A is irreducible and non-negative is broadly similar. The Perron projection is still positive but there may now be other eigenvalues of modulus ρ(A) that negate use of the power method and prevent the powers of (1 − P)A decaying as in the primitive case whenever ρ(A) = 1. So we consider the peripheral projection, which is the spectral projection of A corresponding to all the eigenvalues that have modulus ρ(A). It may then be shown that the peripheral projection of an irreducible non-negative square matrix is a non-negative matrix with a positive diagonal.\n\nCyclicity\nSuppose in addition that ρ(A) = 1 and A has h eigenvalues on the unit circle. If P is the peripheral projection then the matrix R = AP = PA is non-negative and irreducible, Rh = P, and the cyclic group P, R, R2, ...., Rh−1 represents the harmonics of A. The spectral projection of A at the eigenvalue λ on the unit circle is given by the formula . All of these projections (including the Perron projection) have the same positive diagonal, moreover choosing any one of them and then taking the modulus of every entry invariably yields the Perron projection. Some donkey work is still needed in order to establish the cyclic properties (6)–(8) but it's essentially just a matter of turning the handle. The spectral decomposition of A is given by A = R ⊕ (1 − P)A so the difference between An and Rn is An − Rn = (1 − P)An representing the transients of An which eventually decay to zero. P may be computed as the limit of Anh as n → ∞.\n\nCaveats\nThe matrices L = , P = , T = , M =  provide simple examples of what can go wrong if the necessary conditions are not met. It is easily seen that the Perron and peripheral projections of L are both equal to P, thus when the original matrix is reducible the projections may lose non-negativity and there is no chance of expressing them as limits of its powers. The matrix T is an example of a primitive matrix with zero diagonal. If the diagonal of an irreducible non-negative square matrix is non-zero then the matrix must be primitive but this example demonstrates that the converse is false. M is an example of a matrix with several missing spectral teeth. If ω = eiπ/3 then ω6 = 1 and the eigenvalues of M are {1,ω2,ω3,ω4} so ω and ω5 are both absent.\n\nTerminology\nA problem that causes confusion is a lack of standardisation in the definitions. For example, some authors use the terms strictly positive and positive to mean > 0 and ≥ 0 respectively. In this article positive means > 0 and non-negative means ≥ 0. Another vexed area concerns decomposability and reducibility: irreducible is an overloaded term. For avoidance of doubt a non-zero non-negative square matrix A such that 1 + A is primitive is sometimes said to be connected. Then irreducible non-negative square matrices and connected matrices are synonymous.For surveys of results on irreducibility, see Olga Taussky-Todd and Richard A. Brualdi.\n\nThe nonnegative eigenvector is often normalized so that the sum of its components is equal to unity; in this case, the eigenvector is the vector of a probability distribution and is sometimes called a stochastic eigenvector.Perron–Frobenius eigenvalue and dominant eigenvalue are alternative names for the Perron root. Spectral projections are also known as spectral projectors and spectral idempotents. The period is sometimes referred to as the index of imprimitivity or the order of cyclicity.\n\nSee also\n\n Z-matrix (mathematics)\n M-matrix\n P-matrix\n Hurwitz matrix\n Metzler matrix (Quasipositive matrix)\n Positive operator\n\nNotes\n\nReferences\n\nOriginal papers\n\n  (1959 edition had different title: \"Applications of the theory of matrices\". Also the numeration of chapters is different in the two editions.)\n\nFurther reading\n\n Abraham Berman, Robert J. Plemmons, Nonnegative Matrices in the Mathematical Sciences, 1994, SIAM. .\n Chris Godsil and Gordon Royle, Algebraic Graph Theory, Springer, 2001.\n A. Graham, Nonnegative Matrices and Applicable Topics in Linear Algebra, John Wiley&Sons, New York, 1987.\n R. A. Horn and C.R. Johnson, Matrix Analysis, Cambridge University Press, 1990\n Bas Lemmens and Roger Nussbaum, Nonlinear Perron-Frobenius Theory, Cambridge Tracts in Mathematics 189, Cambridge Univ. Press, 2012.\n S. P. Meyn and R. L. Tweedie, Markov Chains and Stochastic Stability London: Springer-Verlag, 1993.  (2nd edition, Cambridge University Press, 2009)\nHenryk Minc, Nonnegative matrices, John Wiley&Sons, New York, 1988, \n Seneta, E. Non-negative matrices and Markov chains. 2nd rev. ed., 1981, XVI, 288 p., Softcover Springer Series in Statistics. (Originally published by Allen & Unwin Ltd., London, 1973) \n  (The claim that Aj has order n/h at the end of the statement of the theorem is incorrect.)\n Richard S. Varga, Matrix Iterative Analysis'', 2nd ed., Springer-Verlag, 2002. \n\nCategory:Matrix theory\nCategory:Theorems in linear algebra\nCategory:Markov processes"
    },
    {
      "title": "Principal axis theorem",
      "url": "https://en.wikipedia.org/wiki/Principal_axis_theorem",
      "text": "In the mathematical fields of geometry and linear algebra, a principal axis is a certain line in a Euclidean space associated with an ellipsoid or hyperboloid, generalizing the major and minor axes of an ellipse or hyperbola. The principal axis theorem states that the principal axes are perpendicular, and gives a constructive procedure for finding them.\n\nMathematically, the principal axis theorem is a generalization of the method of completing the square from elementary algebra.  In linear algebra and functional analysis, the principal axis theorem is a geometrical counterpart of the spectral theorem.  It has applications to the statistics of principal components analysis and the singular value decomposition.  In physics, the theorem is fundamental to the study of angular momentum.\n\nMotivation\nThe equations in the Cartesian plane R2:\n\ndefine, respectively, an ellipse and a hyperbola.  In each case, the x and y axes are the principal axes.  This is easily seen, given that there are no cross-terms involving products xy in either expression.  However, the situation is more complicated for equations like\n\nHere some method is required to determine whether this is an ellipse or a hyperbola.  The basic observation is that if, by completing the square, the quadratic expression can be reduced to a sum of two squares then the equation defines an ellipse, whereas if it reduces to a difference of two squares then the equation represents a hyperbola:\n\nThus, in our example expression, the problem is how to absorb the coefficient of the cross-term 8xy into the functions u and v.  Formally, this problem is similar to the problem of matrix diagonalization, where one tries to find a suitable coordinate system in which the matrix of a linear transformation is diagonal.  The first step is to find a matrix in which the technique of diagonalization can be applied.\n\nThe trick is to write the quadratic form as\n\nwhere the cross-term has been split into two equal parts.  The matrix A in the above decomposition is a symmetric matrix.  In particular, by the spectral theorem, it has real eigenvalues and is diagonalizable by an orthogonal matrix (orthogonally diagonalizable).\n\nTo orthogonally diagonalize A, one must first find its eigenvalues, and then find an orthonormal eigenbasis.  Calculation reveals that the eigenvalues of A are\n\nwith corresponding eigenvectors\n\nDividing these by their respective lengths yields an orthonormal eigenbasis:\n\nNow the matrix S = [u1 u2] is an orthogonal matrix, since it has orthonormal columns, and A is diagonalized by:\n\nThis applies to the present problem of \"diagonalizing\" the quadratic form through the observation that\n\nThus, the equation  is that of an ellipse, since the left side can be written as the sum of two squares.\n\nIt is tempting to simplify this expression by pulling out factors of 2. However, it is important not to do this.  The quantities\n\nhave a geometrical meaning.  They determine an orthonormal coordinate system on R2.  In other words, they are obtained from the original coordinates by the application of a rotation (and possibly a reflection).  Consequently, one may use the c1 and c2 coordinates to make statements about length and angles (particularly length), which would otherwise be more difficult in a different choice of coordinates (by rescaling them, for instance).  For example, the maximum distance from the origin on the ellipse c12 + 9c22 = 1 occurs when c2=0, so at the points c1=±1.  Similarly, the minimum distance is where c2=±1/3.\n\nIt is possible now to read off the major and minor axes of this ellipse.  These are precisely the individual eigenspaces of the matrix A, since these are where c2 = 0 or c1=0.  Symbolically, the principal axes are\n\nTo summarize:\n The equation is for an ellipse, since both eigenvalues are positive.  (Otherwise, if one were positive and the other negative, it would be a hyperbola.)\n The principal axes are the lines spanned by the eigenvectors.\n The minimum and maximum distances to the origin can be read off the equation in diagonal form.\nUsing this information, it is possible to attain a clear geometrical picture of the ellipse: to graph it, for instance.\n\nFormal statement\nThe principal axis theorem concerns  quadratic forms in Rn, which are homogeneous polynomials of degree 2.  Any quadratic form may be represented as\n\nwhere A is a symmetric matrix.\n\nThe first part of the theorem is contained in the following statements guaranteed by the spectral theorem:\n The eigenvalues of A are real.\n A is diagonalizable, and the eigenspaces of A are mutually orthogonal.\nIn particular, A is orthogonally diagonalizable, since one may take a basis of each eigenspace and apply the Gram-Schmidt process separately within the eigenspace to obtain an orthonormal eigenbasis.\n\nFor the second part, suppose that the eigenvalues of A are λ1, ..., λn (possibly repeated according to their algebraic multiplicities) and the corresponding orthonormal eigenbasis is u1,...,un.  Then\n \nwhere the ci are the coordinates with respect to the given eigenbasis.  Furthermore,\n The i-th principal axis is the line determined by the n-1 equations cj = 0, j ≠ i.  This axis is the span of the vector ui.\n\nSee also\n Sylvester's law of inertia\n\nReferences\n \n\nCategory:Theorems in geometry\nCategory:Theorems in linear algebra"
    },
    {
      "title": "Rank–nullity theorem",
      "url": "https://en.wikipedia.org/wiki/Rank%E2%80%93nullity_theorem",
      "text": "thumb|260px|Rank–nullity theorem\n\nThe rank-nullity theorem is a fundamental theorem in linear algebra which relates the dimensions of a linear map's kernel and image with the dimension of its domain.\nStating the theorem\nLet ,  be vector spaces, where  is finite dimensional.  Let  be a linear transformation. Then\n,\n\nwhere\n and .\nOne can refine this theorem via the splitting lemma to be a statement about an isomorphism of spaces, not just dimensions.\n\n Matrices \nSince , matrices immediately come to mind when discussing linear maps. In the case of an  matrix, the dimension of the domain is , the number of columns in the matrix. Thus the Rank-Nullity Theorem for a given matrix  immediately becomes\n.\n\n Proofs \nHere we provide two proofs. The first operates in the general case, using linear maps. The second proof looks at the homogeneous system  for  with rank  and shows explicitly that there exists a set of  linearly independent solutions that span the kernel of .\n\nWhile the theorem requires that the domain of the linear map be finite-dimensional, there is no such assumption on the codomain. This means that there are linear maps not given by matrices for which the theorem applies. Despite this, the first proof is not actually more general than the second: since the image of the linear map is finite-dimensional, we can represent the map from its domain to its image by a matrix, prove the theorem for that matrix, then compose with the inclusion of the image into the full codomain.\n\nFirst proof\nLet  be vector spaces over some field  and  defined as in the statement of the theorem with .\n\nAs  is a subspace, there exists a basis for it. \nSuppose  and let\n\nbe such a basis.\n\nWe may now, by the Steinitz exchange lemma, extend  with  linearly independent vectors  to form a full basis of .\n\nLet\n\nsuch that\n\nis a basis for .\n\nFrom this, we know that\n.\nWe now claim that  is a basis for .\nBy definition,  is generating; it remains to be shown that it is also linearly independent to conclude that it is a basis.\n\nLet \n\nfor some .\nThus, owing to the linearity of , it follows that\n.\nThis is a contradiction to  being a basis, unless all  are equal to zero. This shows that  is linearly independent, and more specifically that it is a basis for .\n\nTo summarise, we have , a basis for , and , a basis for .\n\nFinally we may state that\n.\nThis concludes our proof.\n\nSecond proof\n\nLet  with  linearly independent columns (i.e. ). We will show that:\n\nTo do this, we will produce a matrix  whose columns form a basis of the null space of .\n\nWithout loss of generality, assume that the first  columns of  are linearly independent. So, we can write\n,\nwhere\n with  linearly independent column vectors, and\n, each of whose  columns are linear combinations of the columns of .\n\nThis means that  for some  (see rank factorization) and, hence,\n.\n\nLet\n,\nwhere  is the  identity matrix. We note that  satisfies\n\nTherefore, each of the  columns of  are particular solutions of . \n\nFurthermore, the  columns of  are linearly independent because  will imply  for :\n\nTherefore, the column vectors of  constitute a set of  linearly independent solutions for .\n\nWe next prove that any solution of  must be a linear combination of the columns of .\n\nFor this, let\n\nbe any vector such that . Note that since the columns of  are linearly independent,  implies .\n\nTherefore,\n\n \n\nThis proves that any vector  that is a solution of  must be a linear combination of the  special solutions given by the columns of . And we have already seen that the columns of  are linearly independent. Hence, the columns of  constitute a basis for the null space of . Therefore, the nullity of  is . Since  equals rank of , it follows that . This concludes our proof.\n\n Reformulations and generalizations \nThis theorem is a statement of the first isomorphism theorem of algebra for the case of vector spaces; it generalizes to the splitting lemma.\n\nIn more modern language, the theorem can also be phrased as follows: if\n0 → U → V → R → 0\nis a short exact sequence of vector spaces, then\n.\nHere R plays the role of im T and U is ker T, i.e.\n \n\nIn the finite-dimensional case, this formulation is susceptible to a generalization: if \n0 → V1 → V2 → ... → Vr → 0\nis an exact sequence of finite-dimensional vector spaces, then\n\nThe rank–nullity theorem for finite-dimensional vector spaces may also be formulated in terms of the index of a linear map. The index of a linear map , where  and  are finite-dimensional, is defined by\n.\n\nIntuitively,  is the number of independent solutions  of the equation , and  is the number of independent restrictions that have to be put on  to make  solvable. The rank–nullity theorem for finite-dimensional vector spaces is equivalent to the statement\n\n.\n\nWe see that we can easily read off the index of the linear map  from the involved spaces, without any need to analyze  in detail. This effect also occurs in a much deeper result: the Atiyah–Singer index theorem states that the index of certain differential operators can be read off the geometry of the involved spaces.\n\n Notes \n\n References \n \n .\n\nCategory:Theorems in linear algebra\nCategory:Isomorphism theorems\nCategory:Articles containing proofs"
    },
    {
      "title": "Rouché–Capelli theorem",
      "url": "https://en.wikipedia.org/wiki/Rouch%C3%A9%E2%80%93Capelli_theorem",
      "text": "The Rouché–Capelli theorem is a theorem in linear algebra that determines the number of solutions for a system of linear equations, given the rank of its augmented matrix and coefficient matrix. The theorem is variously known as the:\n Kronecker–Capelli theorem in Poland, Romania and Russia;\n Rouché–Capelli theorem in Italy;\n Rouché–Fontené theorem in France;\n Rouché–Frobenius theorem in Spain and many countries in Latin America;\n Frobenius theorem in the Czech Republic and in Slovakia.\n\n Formal statement \nA system of linear equations with n variables has a solution if and only if the rank of its coefficient matrix A is equal to the rank of its augmented matrix [A|b]. If there are solutions, they form an affine subspace of  of dimension n − rank(A). In particular:\n if n = rank(A), the solution is unique,\n otherwise there are infinitely many solutions.\n\nExample\nConsider the system of equations\n\n x + y + 2z = 3,\n x + y + z = 1,\n 2x + 2y + 2z = 2.\n\nThe coefficient matrix is\n\nand the augmented matrix is\n\nSince both of these have the same rank, namely 2, there exists at least one solution; and since their rank is less than the number of unknowns, the latter being 3, there are infinitely many solutions.\n\nIn contrast, consider the system\n\n x + y + 2z = 3,\n x + y + z = 1,\n 2x + 2y + 2z = 5.\n\nThe coefficient matrix is\n\nand the augmented matrix is\n\nIn this example the coefficient matrix has rank 2, while the augmented matrix has rank 3; so this system of equations has no solution. Indeed, an increase in the number of linearly independent rows has made the system of equations inconsistent.\n\nSee also\nGaussian elimination\n\nReferences\n\n \n\nCategory:Theorems in linear algebra\nCategory:Matrix theory\n\ncs:Soustava lineárních rovnic#Frobeniova věta"
    },
    {
      "title": "Schur–Horn theorem",
      "url": "https://en.wikipedia.org/wiki/Schur%E2%80%93Horn_theorem",
      "text": "In mathematics, particularly linear algebra, the Schur–Horn theorem, named after Issai Schur and Alfred Horn, characterizes the diagonal of a Hermitian matrix with given eigenvalues. It has inspired investigations and substantial generalizations in the setting of symplectic geometry. A few important generalizations are Kostant's convexity theorem, Atiyah–Guillemin–Sternberg convexity theorem, Kirwan convexity theorem.\n\n Statement \n\nTheorem. Let  and  be vectors in  such that their entries are in non-increasing order. There is a Hermitian matrix with diagonal values  and eigenvalues  if and only if\n\n \n\nand\n\n \n\nPolyhedral geometry perspective\n\nPermutation polytope generated by a vector\nThe permutation polytope generated by  denoted by  is defined as the convex hull of the set . Here  denotes the symmetric group on . The following lemma characterizes the permutation polytope of a vector in .\n\nLemma.Kadison, R. V., Lemma 5, The Pythagorean Theorem: I. The finite case, Proc. Natl. Acad. Sci. USA, vol. 99 no. 7 (2002):4178–4184 (electronic)Kadison, R. V.; Pedersen, G. K., Lemma 13, Means and Convex Combinations of Unitary Operators, Math. Scand. 57 (1985),249–266 If , and  then the following are equivalent :\n\n(i) .\n\n(ii) \n\n(iii) There are points  in  such that  and  for each  in , some transposition  in , and some  in , depending on .\n\nReformulation of Schur–Horn theorem\nIn view of the equivalence of (i) and (ii) in the lemma mentioned above, one may reformulate the theorem in the following manner.\n\nTheorem. Let  and  be real vectors. There is a Hermitian matrix with diagonal entries  and eigenvalues  if and only if the vector  is in the permutation polytope generated by .\n\nNote that in this formulation, one does not need to impose any ordering on the entries of the vectors  and .\n\nProof of the Schur–Horn theorem\nLet  be a  Hermitian matrix with eigenvalues , counted with multiplicity. Denote the diagonal of  by , thought of as a vector in , and the vector  by . Let  be the diagonal matrix having  on its diagonal.\n\n()  may be written in the form , where  is a unitary matrix. Then\n\nLet  be the matrix defined by . Since  is a unitary matrix,  is a doubly stochastic matrix and we have . By the Birkhoff–von Neumann theorem,  can be written as a convex combination of permutation matrices. Thus  is in the permutation polytope generated by . This proves Schur's theorem.\n\n() If  occurs as the diagonal of a Hermitian matrix with eigenvalues , then  also occurs as the diagonal of some Hermitian matrix with the same set of eigenvalues, for any transposition  in . One may prove that in the following manner.\n\nLet  be a complex number of modulus  such that  and  be a unitary matrix with  in the  and  entries, respectively,  at the  and  entries, respectively,  at all diagonal entries other than  and , and  at all other entries. Then \nhas  at the  entry,  at the  entry, and  at the  entry where . Let  be the transposition of  that interchanges  and .\n\nThen the diagonal of  is .\n\n is a Hermitian matrix with eigenvalues . Using the equivalence of (i) and (iii) in the lemma mentioned above, we see that any vector in the permutation polytope generated by , occurs as the diagonal of a Hermitian matrix with the prescribed eigenvalues. This proves Horn's theorem.\n\nSymplectic geometry perspective\nThe Schur–Horn theorem may be viewed as a corollary of the Atiyah–Guillemin–Sternberg convexity theorem in the following manner. Let  denote the group of  unitary matrices. Its Lie algebra, denoted by , is the set of skew-Hermitian matrices. One may identify the dual space  with the set of Hermitian matrices  via the linear isomorphism  defined by  for . The unitary group  acts on  by conjugation and acts on  by the coadjoint action. Under these actions,  is an -equivariant map i.e. for every  the following diagram commutes,\n\n200px\n\nLet  and  denote the diagonal matrix with entries given by . Let  denote the orbit of  under the -action i.e. conjugation. Under the -equivariant isomorphism , the symplectic structure on the corresponding coadjoint orbit may be brought onto . Thus  is a Hamiltonian -manifold.\n\nLet  denote the Cartan subgroup of  which consists of diagonal complex matrices with diagonal entries of modulus . The Lie algebra  of  consists of diagonal skew-Hermitian matrices and the dual space  consists of diagonal Hermitian matrices, under the isomorphism . In other words,  consists of diagonal matrices with purely imaginary entries and  consists of diagonal matrices with real entries. The inclusion map  induces a map , which projects a matrix  to the diagonal matrix with the same diagonal entries as . The set  is a Hamiltonian -manifold, and the restriction of  to this set is a moment map for this action.\n\nBy the Atiyah–Guillemin–Sternberg theorem,  is a convex polytope. A matrix  is fixed under conjugation by every element of  if and only if  is diagonal. The only diagonal matrices in  are the ones with diagonal entries  in some order. Thus, these matrices generate the convex polytope . This is exactly the statement of the Schur–Horn theorem.\n\nNotes\n\n References \n Schur, Issai, Über eine Klasse von Mittelbildungen mit Anwendungen auf die Determinantentheorie, Sitzungsber. Berl. Math. Ges. 22 (1923), 9–20.\n Horn, Alfred, Doubly stochastic matrices and the diagonal of a rotation matrix, American Journal of Mathematics 76 (1954), 620–630.\n Kadison, R. V.; Pedersen, G. K., Means and Convex Combinations of Unitary Operators, Math. Scand. 57 (1985),249–266.\n Kadison, R. V., The Pythagorean Theorem: I. The finite case, Proc. Natl. Acad. Sci. USA, vol. 99 no. 7 (2002):4178–4184 (electronic)\n\n External links \n MathWorld\n\nCategory:Order theory\nCategory:Theorems in linear algebra\nCategory:Matrix theory\nCategory:Spectral theory"
    },
    {
      "title": "Schur's theorem",
      "url": "https://en.wikipedia.org/wiki/Schur%27s_theorem",
      "text": "In discrete mathematics, Schur's theorem is any of several theorems of the mathematician Issai Schur. In differential geometry, Schur's theorem is a theorem of Axel Schur. In functional analysis, Schur's theorem is often called Schur's property, also due to Issai Schur.\n\n Ramsey theory \n\nIn Ramsey theory, Schur's theorem states that for any partition of the positive integers into a finite number of parts, one of the parts contains three integers x, y, z with \n\nMoreover, for every positive integer c, there exists a number S(c), called Schur's number, such that for every partition of the integers \n\ninto c parts, one of the parts contains integers x, y, and z with \n\nFolkman's theorem generalizes Schur's theorem by stating that there exist arbitrarily large sets of integers, all of whose nonempty sums belong to the same part.\n\nUsing this definition, the first few Schur numbers are , 5, 14, 45, 161, ... () The proof that  was announced in 2017 and took up 2 petabytes of space.\n\n Combinatorics \nIn combinatorics, Schur's theorem tells the number of ways for expressing a given number as a (non-negative, integer) linear combination of a fixed set of relatively prime numbers. In particular, if  is a set of integers such that , the number of different tuples of non-negative integer numbers  such that  when  goes to infinity is:\n\nAs a result, for every set of relatively prime numbers  there exists a value of  such that every larger number is representable as a linear combination of  in at least one way. This consequence of the theorem can be recast in a familiar context considering the problem of changing an amount using a set of coins. If the denominations of the coins are relatively prime numbers (such as 2 and 5) then any sufficiently large amount can be changed using only these coins. (See Coin problem.)\n\n Differential geometry \nIn differential geometry, Schur's theorem compares the distance between the endpoints of a space curve  to the distance between the endpoints of a corresponding plane curve  of less curvature.\n\nSuppose  is a plane curve with curvature  which makes a convex curve when closed by the chord connecting its endpoints, and  is a curve of the same length with curvature . Let  denote the distance between the endpoints of  and  denote the distance between the endpoints of . If  then .\n\nSchur's theorem is usually stated for  curves, but John M. Sullivan has observed that Schur's theorem applies to curves of finite total curvature (the statement is slightly different).\n\n  Linear algebra \n\nIn linear algebra Schur’s theorem is referred to as either the triangularization of a square matrix with complex entries, or of a square matrix with real entries and real eigenvalues.\n\nFunctional analysis\nIn functional analysis and the study of Banach spaces, Schur's theorem, due to J. Schur, often refers to Schur's property, that for certain spaces, weak convergence implies convergence in the norm.\n\n Number theory \nIn number theory, Issai Schur showed in 1912 that for every nonconstant polynomial p(x) with integer coefficients, if S is the set of all nonzero values , then the set of primes that divide some member of S is infinite.\n\nSee also\n\nSchur's lemma (from Riemannian geometry)\n\nReferences\n\n Herbert S. Wilf (1994). generatingfunctionology. Academic Press.\n Shiing-Shen Chern (1967). Curves and Surfaces in Euclidean Space. In Studies in Global Geometry and Analysis. Prentice-Hall.\n Issai Schur (1912). Über die Existenz unendlich vieler Primzahlen in einigen speziellen arithmetischen Progressionen, Sitzungsberichte der Berliner Math.\n\nFurther reading\n Dany Breslauer and Devdatt P. Dubhashi (1995). Combinatorics for Computer Scientists\n John M. Sullivan (2006). Curves of Finite Total Curvature. arXiv.\n\nCategory:Theorems in discrete mathematics\nCategory:Ramsey theory\nCategory:Additive combinatorics\nCategory:Theorems in combinatorics\nCategory:Theorems in differential geometry\nCategory:Theorems in linear algebra\nCategory:Theorems in functional analysis"
    },
    {
      "title": "Spectral theorem",
      "url": "https://en.wikipedia.org/wiki/Spectral_theorem",
      "text": "In mathematics, particularly linear algebra and functional analysis, a spectral theorem is a result about when a linear operator or matrix can be diagonalized (that is, represented as a diagonal matrix in some basis). This is extremely useful because computations involving a diagonalizable matrix can often be reduced to much simpler computations involving the corresponding diagonal matrix. The concept of diagonalization is relatively straightforward for operators on finite-dimensional vector spaces but requires some modification for operators on infinite-dimensional spaces. In general, the spectral theorem identifies a class of linear operators that can be modeled by multiplication operators, which are as simple as one can hope to find. In more abstract language, the spectral theorem is a statement about commutative C*-algebras. See also spectral theory for a historical perspective.\n\nExamples of operators to which the spectral theorem applies are self-adjoint operators or more generally normal operators on Hilbert spaces.\n\nThe spectral theorem also provides a canonical decomposition, called the spectral decomposition, eigenvalue decomposition, or eigendecomposition, of the underlying vector space on which the operator acts.\n\nAugustin-Louis Cauchy proved the spectral theorem for self-adjoint matrices, i.e., that every real, symmetric matrix is diagonalizable. In addition, Cauchy was the first to be systematic about determinants.A Short History of Operator Theory by Evans M. Harrell II The spectral theorem as generalized by John von Neumann is today perhaps the most important result of operator theory.\n\nThis article mainly focuses on the simplest kind of spectral theorem, that for a self-adjoint operator on a Hilbert space. However, as noted above, the spectral theorem also holds for normal operators on a Hilbert space.\n\n Finite-dimensional case \n\n Hermitian maps and Hermitian matrices \nWe begin by considering a Hermitian matrix on  (but the following discussion will be adaptable to the more restrictive case of symmetric matrices on  ). We consider a Hermitian map  on a finite-dimensional complex inner product space  endowed with a positive definite sesquilinear inner product . The Hermitian condition on  means that for all ,\n\n(An equivalent condition is that , where  is the hermitian conjugate of .) In the case that  is identified with a Hermitian matrix, the matrix of  can be identified with its conjugate transpose. (If  is a real matrix, this is equivalent to , that is,  is a symmetric matrix.)\n\nThis condition implies that all eigenvalues of a Hermitian map are real: it is enough to apply it to the case when  is an eigenvector. (Recall that an eigenvector of a linear map  is a (non-zero) vector  such that  for some scalar . The value  is the corresponding eigenvalue. Moreover, the eigenvalues are roots of the characteristic polynomial.)\n\nTheorem. If  is Hermitian, there exists an orthonormal basis of  consisting of eigenvectors of . Each eigenvalue is real.\n\nWe provide a sketch of a proof for the case where the underlying field of scalars is the complex numbers.\n\nBy the fundamental theorem of algebra, applied to the characteristic polynomial of , there is at least one eigenvalue  and eigenvector . Then since \n \nwe find that  is real. Now consider the space , the orthogonal complement of . By Hermiticity,  is an invariant subspace of . Applying the same argument to  shows that  has an eigenvector . Finite induction then finishes the proof.\n\nThe spectral theorem holds also for symmetric maps on finite-dimensional real inner product spaces, but the existence of an eigenvector does not follow immediately from the  fundamental theorem of algebra. To prove this, consider  as a Hermitian matrix and use the fact that all eigenvalues of a Hermitian matrix are real.\n\nThe matrix representation of  in a basis of eigenvectors is diagonal, and by the construction the proof gives a basis of mutually orthogonal eigenvectors; by choosing them to be unit vectors one obtains  an orthonormal basis of eigenvectors.  can be written as a linear combination of pairwise orthogonal projections, called its spectral decomposition. Let\n\nbe the eigenspace corresponding to an eigenvalue . Note that the definition does not depend on any choice of specific eigenvectors.  is the orthogonal direct sum of the spaces  where the index ranges over eigenvalues. \n\nIn other words, if  denotes the orthogonal projection onto , and  are the eigenvalues of , then the spectral decomposition may be written as\n\nIf the spectral decomposition of A is , then  and  for any scalar  It follows that for any polynomial  one has\n\nThe spectral decomposition is a special case of both the Schur decomposition and the singular value decomposition.\n\n Normal matrices \n\nThe spectral theorem extends to a more general class of matrices. Let  be an operator on a finite-dimensional inner product space.  is said to be normal  if . One can show that  is normal if and only if it is unitarily diagonalizable. Proof: By the Schur decomposition, we can write any matrix as , where  is unitary and  is upper-triangular.\nIf  is normal, one sees that . Therefore,  must be diagonal since a normal upper triangular matrix is diagonal (see normal matrix). The converse is obvious.\n\nIn other words,  is normal if and only if there exists a unitary matrix  such that\n\nwhere  is a diagonal matrix. Then, the entries of the diagonal of  are the eigenvalues of . The column vectors of  are the eigenvectors of  and they are orthonormal. Unlike the Hermitian case, the entries of  need not be real.\n\n Compact self-adjoint operators \n\nIn the more general setting of Hilbert spaces, which may have an infinite dimension, the statement of the spectral theorem for compact self-adjoint operators is virtually the same as in the finite-dimensional case.\n\nTheorem. Suppose  is a compact self-adjoint operator on a (real or complex) Hilbert space . Then there is an orthonormal basis of  consisting of eigenvectors of . Each eigenvalue is real.\n\nAs for Hermitian matrices, the key point is to prove the existence of at least one nonzero eigenvector. One cannot rely on determinants to show existence of eigenvalues, but one can use a maximization argument analogous to the variational characterization of eigenvalues. \n\nIf the compactness assumption is removed, it is not true that every self-adjoint operator has eigenvectors.\n\n Bounded self-adjoint operators \n\nPossible absence of eigenvectors\n\nThe next generalization we consider is that of bounded self-adjoint operators on a Hilbert space. Such operators may have no eigenvalues: for instance let  be the operator of multiplication by  on , that is, Section 6.1\n\nNow, a physicist would say that  does have eigenvectors, namely the , where  is a Dirac delta-function. A delta-function, however, is not a normalizable function; that is, it is not actually in the Hilbert space . Thus, the delta-functions are \"generalized eigenvectors\" but not eigenvectors in the strict sense.\n\nSpectral subspaces and projection-valued measures\n\nIn the absence of (true) eigenvectors, one can look for subspaces consisting of almost eigenvectors. In the above example, for example, we might consider the subspace of functions supported on a small interval  inside . This space is invariant under  and for any  in this subspace,  is very close to . In this approach to the spectral theorem, if  is a bounded self-adjoint operator, one looks for large families of such \"spectral subspaces\". Theorem 7.2.1 Each subspace, in turn, is encoded by the associated projection operator, and the collection of all the subspaces is then represented by a projection-valued measure. \n\nOne formulation of the spectral theorem expresses the operator  as an integral of the coordinate function over the operator's spectrum with respect to a projection-valued measure. Theorem 7.12\n\n \n\nWhen the self-adjoint operator in question is compact, this version of the spectral theorem reduces to something similar to the finite-dimensional spectral theorem above, except that the operator is expressed as a finite or countably infinite linear combination of projections, that is, the measure consists only of atoms.\n\nMultiplication operator version\n\nAn alternative formulation of the spectral theorem says that every bounded self-adjoint operator is unitarily equivalent to a multiplication operator. The significance of this result is that multiplication operators are in many ways easy to understand.\n\nTheorem. Theorem 7.20 Let   be a bounded self-adjoint operator on a Hilbert space .  Then there is a measure space  and a real-valued essentially bounded measurable function  on  and a unitary operator  such that\n\nwhere  is the multiplication operator:\n\nand \n\nThe spectral theorem is the beginning of the vast research area of functional analysis called operator theory; see also the spectral measure.\n\nThere is also an analogous spectral theorem for bounded normal operators on Hilbert spaces.  The only difference in the conclusion is that now  may be complex-valued.\n\nDirect integrals\nThere is also a formulation of the spectral theorem in terms of direct integrals. It is similar to the multiplication-operator formulation, but more canonical.\n\nLet  be a bounded self-adjoint operator and let  be the spectrum of . The direct-integral formulation of the spectral theorem associates two quantities to . First, a measure  on , and second, a family of Hilbert spaces  We then form the direct integral Hilbert space\n\nThe elements of this space are functions (or \"sections\")  such that  for all . \nThe direct-integral version of the spectral theorem may be expressed as follows: Theorem 7.19\nTheorem. If  is a bounded self-adjoint operator, then  is unitarily equivalent to the \"multiplication by \" operator on \n\nfor some measure  and some family  of Hilbert spaces. The measure  is uniquely determined by  up to measure-theoretic equivalence; that is, any two measure associated to the same  have the same sets of measure zero. The dimensions of the Hilbert spaces  are uniquely determined by  up to a set of -measure zero.\n\nThe spaces  can be thought of as something like \"eigenspaces\" for . Note, however, that unless the one-element set  has positive measure, the space  is not actually a subspace of the direct integral. Thus, the 's should be thought of as \"generalized eigenspace\"—that is, the elements of  are \"eigenvectors\" that do not actually belong to the Hilbert space.\n\nAlthough both the multiplication-operator and direct integral formulations of the spectral theorem express a self-adjoint operator as unitarily equivalent to a multiplication operator, the direct integral approach is more canonical. First, the set over which the direct integral takes place (the spectrum of the operator) is canonical. Second, the function we are multiplying by is canonical in the direct-integral approach: Simply the function .\n\nCyclic vectors and simple spectrum\nA vector  is called a cyclic vector for  if the vectors  span a dense subspace of the Hilbert space. Suppose  is a bounded self-adjoint operator for which a cyclic vector exists. In that case, there is no distinction between the direct-integral and multiplication-operator formulations of the spectral theorem. Indeed, in that case, there is a measure  on the spectrum  of  such that  is unitarily equivalent to the \"multiplication by \" operator on . Lemma 8.11 This result represents  simultaneously a multiplication operator and as a direct integral, since  is just a direct integral in which each Hilbert space  is just .\n\nNot every bounded self-adjoint operator admits a cyclic vector; indeed, by the uniqueness in the direct integral decomposition, this can occur only when all the 's have dimension one. When this happens, we say that  has \"simple spectrum\" in the sense of spectral multiplicity theory. That is, a bounded self-adjoint operator that admits a cyclic vector should be thought of as the infinite-dimensional generalization of a self-adjoint matrix with distinct eigenvalues (i.e., each eigenvalue has multiplicity one).\n\nAlthough not every  admits a cyclic vector,  it is easy to see that we can decompose the Hilbert space as a direct sum of invariant subspaces on which  has a cyclic vector. This observation is the key to the proofs of the multiplication-operator and direct-integral forms of the spectral theorem.\n\nFunctional calculus\nOne important application of the spectral theorem (in whatever form) is the idea of defining a functional calculus. That is, given a function  defined on the spectrum of , we wish to define an operator . If  is simply a positive power, , then  is just the  power of , . The interesting cases are where  is a nonpolynomial function such as a square root or an exponential. Either of the versions of the spectral theorem provides such a functional calculus.E.g.,  Definition 7.13 In the direct-integral version, for example,  acts as the \"multiplication by \" operator in the direct integral:\n.\nThat is to say, each space  in the direct integral is a (generalized) eigenspace for  with eigenvalue .\n\n General self-adjoint operators \nMany important linear operators which occur in analysis, such as differential operators, are unbounded. There is also a spectral theorem for self-adjoint operators that applies in these cases.  To give an example, every constant-coefficient differential operator is unitarily equivalent to a multiplication operator. Indeed, the unitary operator that implements this equivalence is the Fourier transform; the multiplication operator is a type of Fourier multiplier.\n\nIn general, spectral theorem for self-adjoint operators may take several equivalent forms.See Section 10.1 of  Notably, all of the formulations given in the previous section for bounded self-adjoint operators—the projection-valued measure version, the multiplication-operator version, and the direct-integral version—continue to hold for unbounded self-adjoint operators, with small technical modifications to deal with domain issues.\n\n See also \n Borel functional calculus\n Spectral theory\n Matrix decomposition\n Canonical form\n Jordan decomposition, of which the spectral decomposition is a special case.\n Singular value decomposition, a generalisation of spectral theorem to arbitrary matrices.\n Eigendecomposition of a matrix\n\n Notes \n\n References \n Sheldon Axler, Linear Algebra Done Right, Springer Verlag, 1997\n\n Paul Halmos, \"What Does the Spectral Theorem Say?\", American Mathematical Monthly, volume 70, number 3 (1963), pages 241–247 Other link\n M. Reed and B. Simon, Methods of Mathematical Physics, vols I–IV, Academic Press 1972.\n G. Teschl, Mathematical Methods in Quantum Mechanics with Applications to Schrödinger Operators, http://www.mat.univie.ac.at/~gerald/ftp/book-schroe/, American Mathematical Society, 2009.\n\n*\nCategory:Linear algebra\nCategory:Matrix theory\nCategory:Singular value decomposition\nCategory:Theorems in functional analysis\nCategory:Theorems in linear algebra"
    },
    {
      "title": "Borel–Weil–Bott theorem",
      "url": "https://en.wikipedia.org/wiki/Borel%E2%80%93Weil%E2%80%93Bott_theorem",
      "text": "In mathematics, the Borel–Weil–Bott theorem is a basic result in the representation theory of Lie groups, showing how a family of representations can be obtained from holomorphic sections of certain complex vector bundles, and, more generally, from higher sheaf cohomology groups associated to such bundles. It is built on the earlier Borel–Weil theorem of Armand Borel and André Weil, dealing just with the space of sections (the zeroth cohomology group), the extension to higher cohomology groups being provided by Raoul Bott. One can equivalently, through Serre's GAGA, view this as a result in complex algebraic geometry in the Zariski topology.\n\nFormulation\nLet  be a semisimple Lie group or algebraic group over , and fix a maximal torus  along with a Borel subgroup  which contains . Let  be an integral weight of ;  defines in a natural way a one-dimensional representation  of , by pulling back the representation on , where   is the unipotent radical of . Since we can think of the projection map  as a principal -bundle, for each  we get an associated fiber bundle  on  (note the sign), which is obviously a line bundle. Identifying  with its sheaf of holomorphic sections, we consider the sheaf cohomology groups . Since  acts on the total space of the bundle  by bundle automorphisms, this action naturally gives a -module structure on these groups; and the Borel–Weil–Bott theorem gives an explicit description of these groups as -modules.\n\nWe first need to describe the Weyl group action centered at . For any integral weight  and  in the Weyl group , we set , where  denotes the half-sum of positive roots of . It is straightforward to check that this defines a group action, although this action is not linear, unlike the usual Weyl group action. Also, a weight  is said to be dominant if  for all simple roots . Let  denote the length function on .\n\nGiven an integral weight , one of two cases occur:\n There is no  such that  is dominant, equivalently, there exists a nonidentity  such that ; or\n There is a unique  such that  is dominant.\nThe theorem states that in the first case, we have\n\n for all ;\n\nand in the second case, we have\n\n for all , while\n\n is the dual of the irreducible highest-weight representation of  with highest weight .\n\nIt is worth noting that case (1) above occurs if and only if  for some positive root . Also, we obtain the classical Borel–Weil theorem as a special case of this theorem by taking  to be dominant and  to be the identity element .\n\nExample\nFor example, consider , for which  is the Riemann sphere, an integral weight is specified simply by an integer , and . The line bundle  is , whose sections are the homogeneous polynomials of degree  (i.e. the binary forms). As a representation of , the sections can be written as , and is canonically isomorphic to . \n\nThis gives us at a stroke the representation theory of :  is the standard representation, and  is its th symmetric power. We even have a unified description of the action of the Lie algebra, derived from its realization as vector fields on the Riemann sphere: if , ,  are the standard generators of , then \n\n \n\nPositive characteristic\nOne also has a weaker form of this theorem in positive characteristic. Namely, let  be a semisimple algebraic group over an algebraically closed field of characteristic . Then it remains true that  for all  if  is a weight such that  is non-dominant for all  as long as  is \"close to zero\". This is known as the Kempf vanishing theorem. However, the other statements of the theorem do not remain valid in this setting.\n\nMore explicitly, let  be a dominant integral weight; then it is still true that  for all , but it is no longer true that this -module is simple in general, although it does contain the unique highest weight module of highest weight  as a -submodule. If  is an arbitrary integral weight, it is in fact a large unsolved problem in representation theory to describe the cohomology modules  in general. Unlike over , Mumford gave an example showing that it need not be the case for a fixed  that these modules are all zero except in a single degree .\n\nBorel–Weil theorem\nThe Borel–Weil theorem provides a concrete model for irreducible representations of compact Lie groups and irreducible holomorphic representations of complex semisimple Lie groups. These representations are realized in the spaces of global sections of holomorphic line bundles on the flag manifold of the group. The Borel–Weil–Bott theorem is its generalization to higher cohomology spaces.  The theorem dates back to the early 1950s and can be found in  and .\n\n Statement of the theorem \nThe theorem can be stated either for a complex semisimple Lie group  or for its compact form . Let  be a connected complex semisimple Lie group,  a Borel subgroup of , and  the flag variety. In this scenario,  is a complex manifold and a nonsingular algebraic . The flag variety can also be described as a compact homogeneous space , where  is a (compact) Cartan subgroup of . An integral weight  determines a  holomorphic line bundle  on  and the group  acts on its space of global sections,\n\nThe Borel–Weil theorem states that if  is a dominant integral weight then this representation is a holomorphic irreducible highest weight representation of  with highest weight . Its restriction to  is an irreducible unitary representation of  with highest weight , and each irreducible unitary representations of  is obtained in this way for a unique value of . (A holomorphic representation of a complex Lie group is one for which the corresponding Lie algebra representation is complex linear.)\n\n Concrete description \nThe weight  gives rise to a character (one-dimensional representation) of the Borel subgroup , which is denoted . Holomorphic sections of the holomorphic line bundle  over  may be described more concretely as holomorphic maps\n\nfor all  and .\n\nThe action of  on these sections is given by \n \n\nfor .\n\n Example \nLet  be the complex special linear group , with a Borel subgroup consisting of upper triangular matrices with determinant one. Integral weights for  may be identified with integers, with dominant weights corresponding to nonnegative integers, and the corresponding characters  of  have the form\n\n \n\nThe flag variety  may be identified with the complex projective line  with homogeneous coordinates  and the space of the global sections of the line bundle  is identified with the space of homogeneous polynomials of degree  on . For , this space has dimension  and forms an irreducible representation under the standard action of  on the polynomial algebra . Weight vectors are given by monomials\n\n \n\nof weights , and the highest weight vector  has weight .\n\n See also \nTheorem of the highest weight\n\nNotes\n\nReferences\n .\n . (reprinted by Dover)\n \nA Proof of the Borel–Weil–Bott Theorem, by Jacob Lurie. Retrieved on Jul. 13, 2014.\n. In French; translated title: “Linear representations and Kähler homogeneous spaces of compact Lie groups (after Armand Borel and André Weil).”\n In French.\n.\n. Reprint of the 1986 original.\n\nFurther reading\n \n\nCategory:Representation theory of Lie groups\nCategory:Theorems in representation theory"
    },
    {
      "title": "Brauer's theorem on induced characters",
      "url": "https://en.wikipedia.org/wiki/Brauer%27s_theorem_on_induced_characters",
      "text": "Brauer's theorem on induced characters, often known as Brauer's induction theorem, and named after Richard Brauer, is a basic result in the branch of mathematics known as character theory, which is, in turn, part of the representation theory of a finite group. Let G be a finite group and let Char(G) denote the subring of the ring of complex-valued class functions of G consisting of integer combinations of irreducible characters. Char(G) is known as the character ring of G, and its elements are known as virtual characters (alternatively, as generalized characters, or sometimes difference characters). It is a ring by virtue of the fact that the product of characters of G is again a character of G. Its multiplication is given by the elementwise product of class functions.\n\nBrauer's induction theorem shows that the character ring can be generated (as an abelian group) by   induced characters of the form , where H ranges over subgroups of G and λ ranges over linear characters (having degree 1) of H.\n\nIn fact, Brauer showed that the subgroups H could be chosen from a very\nrestricted collection, now called Brauer elementary\nsubgroups. These are direct products of cyclic groups and groups whose order is a power of a prime.\n\nUsing Frobenius reciprocity, Brauer's induction theorem leads easily to his fundamental characterization of characters, which asserts that a complex-valued class function of G is a virtual character if and only if its restriction to each Brauer elementary subgroup of G is a virtual character. This result, together with the fact that a virtual character θ is an irreducible character\nif and only if θ(1) > 0 and  (where   is the usual inner product on the ring of complex-valued class functions) gives\na means of constructing irreducible characters without explicitly constructing the associated representations.\n\nAn initial motivation for Brauer's induction theorem was application to Artin L-functions. It shows that those are built up from Dirichlet L-functions, or more general Hecke L-functions. Highly significant for that application is whether each character of G is a non-negative integer combination of characters induced from linear characters of subgroups. In general, this is not the case. In fact, by a theorem of Taketa, if all characters of G are so expressible, then G must be a  solvable group (although solvability alone does not guarantee such expressions- for example, the solvable group SL(2,3) has an irreducible complex character of degree 2 which is not expressible as a non-negative integer combination of characters induced from linear characters of subgroups). An ingredient of the proof of Brauer's induction theorem is that when G is a finite nilpotent group, every complex irreducible character of G is induced from a linear character of some subgroup.\n\nA precursor to Brauer's induction theorem was Artin's induction theorem, which states that  |G| times the trivial character of G is an integer combination of characters which are each induced from trivial characters of cyclic subgroups of G. Brauer's theorem removes the factor |G|, \nbut at the expense of expanding the collection of subgroups used. Some years after the proof of Brauer's theorem appeared, J.A. Green showed (in 1955) that no such induction theorem (with integer combinations of characters induced from linear characters) could be proved with a collection of subgroups smaller than the Brauer elementary subgroups.\n\nThe proof of Brauer's induction theorem exploits the ring structure of Char(G) (most proofs also make use of a slightly larger ring, Char*(G), which consists of -combinations of irreducible characters, where ω is a primitive complex |G|-th root of unity). The set of integer combinations of characters induced from linear characters of Brauer  elementary subgroups is an ideal I(G) of Char(G), so the proof reduces to showing that the trivial character is in I(G). Several proofs of the theorem, beginning with a proof  due to Brauer and John Tate, show that the trivial character is in the analogously defined ideal I*(G) of Char*(G) by concentrating attention on one prime p at a time, and constructing integer-valued elements of I*(G) which differ (elementwise) from the trivial character by (integer multiples of) a sufficiently high power of p. Once this is achieved for every prime divisor of |G|, some manipulations with congruences\nand algebraic integers, again exploiting the fact that I*(G) is an ideal of Ch*(G), place the trivial character in I(G). An auxiliary result here is that a -valued class function lies in the ideal I*(G) if its values are all divisible (in ) by |G|.\n\nBrauer's induction theorem was proved in 1946, and there are now many alternative proofs. In 1986, Victor Snaith gave a proof by a radically different approach, topological in nature (an application of the Lefschetz fixed-point theorem). There has been related recent work on the question of finding natural and explicit forms of Brauer's theorem, notably by Robert Boltje.\n\nReferences\n\n  Corrected reprint of the 1976 original, published by Academic Press. \n\nFurther reading\n \n\nCategory:Representation theory of finite groups\nCategory:Theorems in representation theory"
    },
    {
      "title": "Brauer's three main theorems",
      "url": "https://en.wikipedia.org/wiki/Brauer%27s_three_main_theorems",
      "text": "Brauer's main theorems are three theorems in representation theory of finite groups linking the blocks of a finite group (in characteristic p) with those of its p-local subgroups, that is to say, the normalizers of its non-trivial p-subgroups.\n\nThe second and third main theorems allow refinements of orthogonality relations for ordinary characters which may be applied in finite group theory. These do not presently admit a proof purely in terms of ordinary characters. \nAll three main theorems are stated in terms of the Brauer correspondence.\n\nBrauer correspondence\nThere are many ways to extend the definition which follows, but this is close to the early treatments \nby Brauer. Let G be a finite group, p be a prime, F be a field of characteristic p.\nLet H be a subgroup of G which contains\n\nfor some p-subgroup Q\nof G, and is contained in the normalizer\n\n,\n\nwhere  is the centralizer of Q in G.\n\nThe Brauer homomorphism (with respect to H) is a linear map from the center of the group algebra of G over F to the corresponding algebra for H. Specifically, it is the restriction to \n of the (linear) projection from  to  whose\nkernel is spanned by the elements of G outside . The image of this map is contained in \n, and it transpires that the map is also a ring homomorphism.\n\nSince it is a ring homomorphism, for any block B of FG, the Brauer homomorphism \nsends the identity element of B either to 0 or to an idempotent element. In the latter case, \nthe idempotent may be decomposed as a sum of (mutually orthogonal) primitive idempotents of Z(FH). \nEach of these primitive idempotents is the multiplicative identity of some block of FH. The block b of FH is said to be a Brauer correspondent of B if its identity element occurs\nin this decomposition of the image of the identity of B under the Brauer homomorphism.\n\nBrauer's first main theorem\n\nBrauer's first main theorem  states that if  is a finite group a  is a -subgroup of , then there is a bijection between the set of \n(characteristic p) blocks of  with defect group  and blocks of the normalizer  with \ndefect group D. This bijection arises because when , each block of G\nwith defect group D has a unique Brauer correspondent block of H, which also has defect \ngroup D.\n\nBrauer's second main theorem\n\nBrauer's second main theorem  gives, for an element t whose order is a power of a prime p, a criterion for a (characteristic p) block of  to correspond to a given block of , via generalized decomposition numbers. These are the coefficients which occur when the restrictions of ordinary characters of  (from the given block) to elements of the form tu, where u ranges over elements of order prime to p in , are written as linear combinations of the irreducible Brauer characters of . The content of the theorem is that it is only necessary to use Brauer characters from blocks of  which are Brauer correspondents of the chosen block of G.\n\nBrauer's third main theorem\n\nBrauer's third main theorem  states that when Q is a p-subgroup of the finite group G,\nand H is a subgroup of G, containing , and contained in ,\nthen the principal block of H is the only Brauer correspondent of the principal block of G (where the blocks referred to are calculated in characteristic p).\n\nReferences\n\n gives a detailed proof of the Brauer's main theorems.\n\n Walter Feit, The representation theory of finite groups. North-Holland Mathematical Library, 25. North-Holland Publishing Co., Amsterdam-New York, 1982. xiv+502 pp. \n\nCategory:Representation theory of finite groups\nCategory:Theorems in representation theory"
    },
    {
      "title": "Commutation theorem",
      "url": "https://en.wikipedia.org/wiki/Commutation_theorem",
      "text": "In mathematics, a  commutation theorem explicitly identifies the commutant of a specific von Neumann algebra acting on a Hilbert space in the presence of a trace. The first such result was proved by F.J. Murray and John von Neumann in the 1930s and applies to the von Neumann algebra generated by a discrete group or by the dynamical system associated with a\nmeasurable transformation preserving a probability measure. Another important application is in the theory of unitary representations of unimodular locally compact groups, where the theory has been applied to the regular representation and other closely related representations. In particular this framework led to an abstract version of the Plancherel theorem for unimodular locally compact groups due to Irving Segal and Forrest Stinespring and an abstract Plancherel theorem for spherical functions associated with a Gelfand pair due to Roger Godement. Their work was put in final form in the 1950s  by Jacques Dixmier as part of the theory of Hilbert algebras. It was not until the late 1960s, prompted partly by results in algebraic quantum field theory and quantum statistical mechanics due to the school of Rudolf Haag, that the more general non-tracial Tomita–Takesaki theory was developed, heralding a new era in the theory of von Neumann algebras.\n\nCommutation theorem for finite traces\nLet H be a Hilbert space and M a von Neumann algebra on H with a unit vector Ω such that\n\n M Ω  is dense in H\n M ' Ω  is dense in H, where M ' denotes the commutant of M\n (abΩ, Ω) = (baΩ, Ω) for all a, b in M.\n\nThe vector Ω is called a cyclic-separating trace vector. It is called a trace vector because the last condition means that the matrix coefficient corresponding to Ω defines a tracial state on M. It is called cyclic since Ω generates H as a topological M-module. It is called separating\nbecause if aΩ = 0 for a in M, then aMΩ= (0), and hence a = 0.\n\nIt follows that the map\n\nfor a in M defines a conjugate-linear isometry of H with square the identity J2 = I. The operator J is usually called the modular conjugation operator.\n\nIt is immediately verified that JMJ and M commute on the subspace M Ω, so that\n\nThe commutation theorem of Murray and von Neumann states that\n\n{| border=\"1\" cellspacing=\"0\" cellpadding=\"5\"\n|\n|}\n\nOne of the easiest ways to see this is to introduce K, the closure of the real\nsubspace Msa Ω, where Msa denotes the self-adjoint elements in M. It follows that\n\nan orthogonal direct sum for the real part of inner product. This is just the real orthogonal decomposition for the ±1 eigenspaces of J.\nOn the other hand for a in  Msa and b in Msa, the inner product (abΩ, Ω) is real, because ab is self-adjoint. Hence K is unaltered if M is replaced by M '.\n\nIn particular Ω is a trace vector for M and J is unaltered if M is replaced by M '. So the opposite inclusion\n\n \nfollows by reversing the roles of M and M.\n\nExamples\n One of the simplest cases of the commutation theorem, where it can easily be seen directly, is that of a finite group Γ acting on the finite-dimensional inner product space  by the left and right regular representations λ and ρ. These unitary representations are given by the formulas\n\nfor f in  and the commutation theorem implies that\n\nThe operator J is given by the formula\n\nExactly the same results remain true if Γ is allowed to be any countable discrete group. The von Neumann algebra λ(Γ)' ' is usually called the group von Neumann algebra of Γ.\n\n Another important example is provided by a probability space (X, μ). The Abelian von Neumann algebra A = L∞(X, μ) acts by multiplication operators on H = L2(X, μ) and the constant function 1 is a cyclic-separating trace vector. It follows that\n\nso that A is a maximal Abelian subalgebra of B(H), the von Neumann algebra of all bounded operators on H.\n\n The third class of examples combines the above two. Coming from ergodic theory, it was one of von Neumann's original motivations for studying von Neumann algebras. Let (X, μ) be a probability space and let Γ be a countable discrete group of measure-preserving transformations of  (X, μ). The group therefore acts unitarily on the Hilbert space H = L2(X, μ) according to the formula\n\nfor f in H and normalises the Abelian von Neumann  algebra A = L∞(X, μ). Let\n\na tensor product of Hilbert spaces.H1 can be identified with the space of square integrable functions on X x  Γ with respect to the product measure. The group–measure space construction or crossed product von Neumann algebra\n\nis defined to be the von Neumann algebra on H1 generated by the algebra  and the normalising operators .It should not be confused with the von Neumann algebra on H  generated by A and the operators  Ug.\n\nThe vector  is a cyclic-separating trace vector. Moreover the modular conjugation operator J and commutant M ' can be explicitly identified.\n\nOne of the most important cases of the group–measure space construction is when Γ is the group of integers Z, i.e. the case of a single invertible\nmeasurable transformation T. Here T must preserve the probability measure μ. Semifinite traces are required to handle the case when T (or more generally  Γ) only preserves an infinite equivalent measure; and the full force of the Tomita–Takesaki theory is required when there is no invariant measure in the equivalence class, even though the equivalence class of the measure is preserved by T (or Γ).\n\nCommutation theorem for semifinite traces\nLet M be a von Neumann algebra and M+ the set of positive operators in M. By definition, a semifinite trace (or sometimes just trace) on M is a functional τ from M+ into [0, ∞] such that\n\n  for a, b in M+ and λ, μ ≥ 0 ();\n  for a in M+ and u a unitary operator in M (unitary invariance);\n τ is completely additive on orthogonal families of projections in M (normality);\n each projection in M is as orthogonal direct sum of projections with finite trace (semifiniteness).\n\nIf in addition τ is non-zero on every non-zero projection, then   τ is called a faithful trace.\n\nIf τ is a faithful trace on M, let H = L2(M, τ) be the Hilbert space completion of the inner product space\n\nwith respect to the inner product\n\nThe von Neumann algebra M acts by left multiplication on H and can be identified with its image. Let\n\nfor a in M0. The operator J is again called the modular conjugation operator and extends to a conjugate-linear isometry of H satisfying J2 = I. The commutation theorem of Murray and von Neumann\n\n{| border=\"1\" cellspacing=\"0\" cellpadding=\"5\"\n|\n|}\n\nis again valid in this case. This result can be proved directly by a variety of methods, but follows immediately from the result for finite traces, by repeated use of the following elementary fact:\n\nIf M1 ⊇ M2 are two von Neumann algebras such that pn M1 = pn M2 for a family of projections pn in the commutant of M1 increasing to I in the strong operator topology, then M1 = M2.\n\nHilbert algebras\n\nThe theory of Hilbert algebras was introduced by Godement (under the name \"unitary algebras\"), Segal and Dixmier to formalize the classical method of defining the trace for trace class operators starting from Hilbert–Schmidt operators. Applications in the representation theory of groups naturally lead to examples of Hilbert algebras. Every von Neumann algebra endowed with a semifinite trace has a canonical \"completed\"Dixmier uses the adjectives achevée or maximale. or \"full\" Hilbert algebra associated with it; and conversely a completed Hilbert algebra of exactly this form can be canonically associated with every Hilbert algebra. The theory of Hilbert algebras can be used to deduce the commutation theorems of Murray and von Neumann; equally well the main results on Hilbert algebras can also be deduced directly from the commutation theorems for traces. The theory of Hilbert algebras was generalised by Takesaki as a tool for proving commutation theorems for semifinite weights in Tomita–Takesaki theory; they can be dispensed with when dealing with states.\n\nDefinition\nA Hilbert algebra, Appendix A54–A61. is an algebra  with involution x→x* and an inner product (,) such that\n\n (a, b) = (b*, a*) for a, b in ;\n left multiplication by a fixed a in   is a bounded operator;\n * is the adjoint, in other words (xy, z) = (y, x*z);\n the linear span of all products xy is dense in .\n\nExamples\n The Hilbert–Schmidt operators on an infinite-dimensional Hilbert space form a Hilbert algebra with inner product (a, b) = Tr (b*a).\n If (X, μ) is an infinite measure space, the algebra L∞ (X)  L2(X) is a Hilbert algebra with the usual inner product from L2(X).\n If M is a von Neumann algebra with faithful semifinite trace τ, then the *-subalgebra M0 defined above is a Hilbert algebra with inner product (a,  b) =  τ(b*a).\n If G is a unimodular locally compact group, the convolution algebra L1(G)L2(G) is a Hilbert algebra with the usual inner product from L2(G).\n If (G, K) is a Gelfand pair, the convolution algebra L1(K\\G/K)L2(K\\G/K) is a Hilbert algebra with the usual inner product from L2(G); here Lp(K\\G/K) denotes the closed subspace of K-biinvariant functions in Lp(G).\n Any dense *-subalgebra of a Hilbert algebra is also a Hilbert algebra.\n\nProperties\nLet H be the Hilbert space completion of  with respect to the inner product and let J denote the extension of the involution to a conjugate-linear involution of H. Define a representation λ and an anti-representation ρ of \n on itself by left and right multiplication:\n\nThese actions extend continuously to actions on H. In this case the commutation theorem for Hilbert algebras states that\n\n{| border=\"1\" cellspacing=\"0\" cellpadding=\"5\"\n|\n|}\n\nMoreover if\n \n\nthe von Neumann algebra generated by the operators  λ(a), then\n\n{| border=\"1\" cellspacing=\"0\" cellpadding=\"5\"\n|\n|}\n\nThese results were proved independently by  and .\n\nThe proof relies on the notion of \"bounded elements\" in the Hilbert space completion H.\n\nAn element of x in H is said to be bounded (relative to ) if the map a → xa of  into H extends to a \nbounded operator on H, denoted by λ(x).  In this case it is straightforward to prove that:\n\n Jx is also a bounded element, denoted x*, and λ(x*) = λ(x)*;\n a → ax is given by the bounded operator ρ(x) = Jλ(x*)J on H;\n M ' is generated by the ρ(x)'s with x bounded;\n   λ(x) and ρ(y) commute for x, y bounded.\n\nThe commutation theorem follows immediately from the last assertion. In particular\n\n M =  λ()\".\n\nThe space of all bounded elements  forms a Hilbert algebra containing  as a dense *-subalgebra. It is said to be  completed or full because any element in H bounded relative to must actually already lie in . The functional τ on M+ defined by\n\nif x = λ(a)*λ(a) and ∞ otherwise, yields a faithful semifinite trace on M with\n\nThus:\n\n{| border=\"1\" cellspacing=\"0\" cellpadding=\"5\"\n|There is a one-one correspondence between von Neumann algebras on H with faithful semifinite trace and full Hilbert algebras with Hilbert space completion H.\n|}\n\nSee also\n von Neumann algebra\n Affiliated operator\n Tomita–Takesaki theory\n\nNotes\n\nReferences\n\n (English translation)\n\n (English translation)\n\n (Section 5)\n\nCategory:Von Neumann algebras\nCategory:Representation theory of groups\nCategory:Ergodic theory\nCategory:Theorems in functional analysis\nCategory:Theorems in representation theory"
    },
    {
      "title": "Engel's theorem",
      "url": "https://en.wikipedia.org/wiki/Engel%27s_theorem",
      "text": "In representation theory, a branch of mathematics, Engel's theorem is one of the basic theorems in the theory of Lie algebras; it asserts that for a Lie algebra two concepts of nilpotency are identical. A useful form of the theorem says that if a Lie algebra L of matrices consists of nilpotent matrices, then they can all be simultaneously brought to a strictly upper triangular form. The theorem is named after the mathematician Friedrich Engel, who sketched a proof of it in a letter to Wilhelm Killing dated 20 July 1890 .  Engel's student K.A. Umlauf gave a complete proof in his 1891 dissertation, reprinted as .\n\nA linear operator T on a vector space V is defined to be nilpotent if there is a positive integer  k such that Tk = 0. For example, any operator given by a matrix whose entries are zero on and below its diagonal, such as \n\nis nilpotent. An element x of a Lie algebra L is called ad-nilpotent if and only if the linear operator on L defined by \n\nis nilpotent.  Note that in the Lie algebra L(V) of linear operators on V, the identity operator IV is ad-nilpotent (because ) but is not a nilpotent operator.\n\nA Lie algebra L is defined to be nilpotent if and only if the lower central series defined recursively by\n\neventually reaches {0}. \n\nTheorem.  A finite-dimensional Lie algebra L is nilpotent if and only if every element of L is ad-nilpotent.\n\nNote that no assumption on the underlying base field is required.\n\nThe key lemma in the proof of Engel's theorem is the following fact\nabout Lie algebras of linear operators on finite dimensional vector spaces which is useful in its own right:\n\nLet L be a Lie subalgebra of L(V), n=dim(V).  Then L consists of nilpotent operators if and only if there is a sequence\n\nof subspaces of V such that ,  and\n\n \n\nThus Lie algebras of nilpotent operators are simultaneously strictly upper-triangulizable.\n\n See also \n Lie's theorem\n\nReferences\n Erdmann, Karin & Wildon, Mark. Introduction to Lie Algebras, 1st edition, Springer, 2006. \n\n G. Hochschild, The Structure of Lie Groups, Holden Day, 1965.\n J. Humphreys, Introduction to Lie Algebras and Representation Theory, Springer, 1972.\n\nCategory:Theorems in representation theory\nCategory:Representation theory of Lie algebras"
    },
    {
      "title": "Frobenius reciprocity",
      "url": "https://en.wikipedia.org/wiki/Frobenius_reciprocity",
      "text": "In mathematics, and in particular representation theory, Frobenius reciprocity is a theorem expressing a duality between the process of restricting and inducting. It can be used to leverage knowledge about representations of a subgroup to find and classify representations of \"large\" groups that contain them. It is named for Ferdinand Georg Frobenius, the inventor of the representation theory of finite groups.\n\n Statement \n\n Character theory \n\nThe theorem was originally stated in terms of character theory. Let  be a finite group with a subgroup , let  denote the restriction of a character, or more generally, class function of  to , and let  denote the induced class function of a given class function on . For any finite group , there is an inner product  on the vector space of class functions  (described in detail in the article Schur orthogonality relations). Now, for any class functions  and , the following equality holds:\n\n.\n\nIn other words,  and  are Hermitian adjoint.\n\nLet  and  be class functions.\n\nProof. Every class function can be written as a linear combination of irreducible characters. As  is a bilinear form, we can, without loss of generality, assume  and  to be characters of irreducible representations of  in  and of  in  respectively.\nWe define  for all  Then we have\n\nIn the course of this sequence of equations we used only the definition of induction on class functions and the properties of characters. \n\nAlternative proof. In terms of the group algebra, i.e. by the alternative description of the induced representation, the Frobenius reciprocity is a special case of a general equation for a change of rings:\n\nThis equation is by definition equivalent to\n\nAs this bilinear form tallies the bilinear form on the corresponding characters, the theorem follows without calculation. \n\n Module theory \n\nAs explained in the section Representation theory of finite groups#Representations, modules and the convolution algebra, the theory of the representations of a group  over a field  is, in a certain sense, equivalent to the theory of modules over the group algebra [].Specifically, there is an isomorphism of categories between []-Mod and Rep, as described on the pages Isomorphism of categories#Category of representations and Representation theory of finite groups#Representations, modules and the convolution algebra. Therefore, there is a corresponding Frobenius reciprocity theorem for []-modules.\n\nLet  be a group with subgroup , let  be an -module, and let  be a -module. In the language of module theory, the induced module  corresponds to the induced representation , whereas the restriction of scalars  corresponds to the restriction . Accordingly, the statement is as follows: The following sets of module homomorphisms are in bijective correspondence:\n\n.\n\nAs noted below in the section on category theory, this result applies to modules over all rings, not just modules over group algebras.\n\n Category theory \n\nLet  be a group with a subgroup , and let  be defined as above. For any group  and field  let  denote the category of linear representations of  over . There is a forgetful functor\n\nThis functor acts as the identity on morphisms. There is a functor going in the opposite direction:\n\nThese functors form an adjoint pair . In the case of finite groups, they are actually both left- and right-adjoint to one another. This adjunction gives rise to a universal property for the induced representation (for details, see Induced representation#Properties).\n\nIn the language of module theory, the corresponding adjunction is an instance of the more general relationship between restriction and extension of scalars.\n\n See also \n\n See Restricted representation and Induced representation for definitions of the processes to which this theorem applies.\n See Representation theory of finite groups for a broad overview of the subject of group representations.\n See Selberg trace formula and the Arthur-Selberg trace formula for generalizations to discrete cofinite subgroups of certain locally compact groups.\n\n Notes \n\n References \n\n \n \n \n\nCategory:Representation theory of finite groups\nCategory:Theorems in representation theory\nCategory:Adjoint functors"
    },
    {
      "title": "Gabriel's theorem",
      "url": "https://en.wikipedia.org/wiki/Gabriel%27s_theorem",
      "text": "In mathematics, Gabriel's theorem, proved by Pierre Gabriel,  classifies the quivers of finite type in terms of Dynkin diagrams.\n\nStatement\n\nA quiver is of finite type if it has only finitely many isomorphism classes of indecomposable representations.  classified all quivers of finite type, and also their indecomposable representations. More precisely, Gabriel's theorem states that:\n\n A (connected) quiver is of finite type if and only if its underlying graph (when the directions of the arrows are ignored) is one of the ADE Dynkin diagrams: , , , , .\n The indecomposable representations are in a one-to-one correspondence with the positive roots of the root system of the Dynkin diagram.\n\n found a generalization of Gabriel's theorem in which all Dynkin diagrams of finite-dimensional semisimple Lie algebras occur.\n\nReferences\n\nCategory:Theorems in representation theory"
    },
    {
      "title": "Haboush's theorem",
      "url": "https://en.wikipedia.org/wiki/Haboush%27s_theorem",
      "text": "In mathematics Haboush's theorem, often still referred to as the Mumford conjecture, states that for any semisimple algebraic group G over a field K, and for any linear representation ρ of G on a K-vector space V, given v ≠ 0 in V that is fixed by the action of G, there is a G-invariant polynomial F on V, without constant term,  such that\n\nF(v) ≠ 0.\n\nThe polynomial can be taken to be homogeneous, in other words an element of a symmetric power of the dual of V, and if the characteristic is p>0 the degree of the polynomial can be taken to be a power of p.  \nWhen K has characteristic 0 this was well known; in fact Weyl's theorem on the complete reducibility of the representations of G implies that F can even be taken to be linear. Mumford's conjecture about the extension to prime characteristic p was proved  by W. J. , about a decade after the problem had been posed by David Mumford, in the introduction to the first edition of his book Geometric Invariant Theory.\n\nApplications\nHaboush's theorem can be used to generalize  results of geometric invariant theory from characteristic 0, where they were already known, to characteristic p>0. In particular Nagata's earlier results together with Haboush's theorem show that if a reductive group (over an algebraically closed field) acts on a finitely generated algebra then the fixed subalgebra is also finitely generated.\n\nHaboush's theorem implies that if G is a reductive algebraic group acting regularly on an affine algebraic variety, then disjoint closed invariant sets X and Y can be separated by an invariant function f (this means that f is 0 on X and 1 on Y).\n\nC.S. Seshadri (1977) extended Haboush's theorem to reductive groups over schemes.\n\nIt follows from the work of , Haboush, and Popov that the following conditions are equivalent for an affine algebraic group G over a field K:\nG is reductive (its unipotent radical is trivial).\nFor any non-zero invariant vector in a rational representation of G, there is an invariant homogeneous polynomial that does not vanish on it.\nFor any finitely generated K algebra on which G act rationally, the algebra of fixed elements is finitely generated.\n\nProof\nThe theorem is proved in several steps as follows:\nWe can assume that the group is defined over an algebraically closed field K of characteristic p>0.\nFinite groups are easy to deal with as one can just take a product over all elements, so one can reduce to the case of connected reductive groups (as the connected component has finite index). By taking a central extension which is harmless one can also assume the group G is simply connected.\nLet A(G) be the coordinate ring of G. This is a representation of G with G acting by left translations. Pick an element v′ of the dual of V that has value 1 on the invariant vector v. The map V to A(G) by sending w∈V to the element a∈A(G) with a(g) = v′(g(w)). This sends v to 1∈A(G), so we can assume that V⊂A(G) and v=1.\nThe structure of the representation A(G) is given as follows. Pick a maximal torus T of G, and let it act on  A(G) by right translations (so that it commutes with the action of G). Then A(G) splits as a sum over characters λ of T of the subrepresentations A(G)λ of elements transforming according to λ. So we can assume that V is contained in the T-invariant subspace A(G)λ of A(G).\nThe representation A(G)λ is an increasing union of subrepresentations of the form Eλ+nρ⊗Enρ, where ρ is the Weyl vector for a choice of simple roots of T, n is a positive integer, and Eμ is the space of sections of the line bundle over G/B corresponding to a character μ of T, where B is a Borel subgroup containing T.\nIf n is sufficiently large then Enρ has dimension (n+1)N where N is the number of positive roots. This is because in characteristic 0 the corresponding module has this dimension by the Weyl character formula, and for n large enough that the line bundle over G/B is very ample, Enρ has the same dimension as in characteristic 0.\nIf q=pr  for a positive integer r, and n=q−1, then Enρ contains the Steinberg representation of G(Fq) of dimension qN. (Here Fq ⊂ K is the finite field of order q.) The Steinberg representation is an irreducible representation of G(Fq) and therefore of G(K), and for r large enough it has the same dimension as Enρ, so there are infinitely many values of n such that Enρ is irreducible.\nIf Enρ is irreducible it is isomorphic to its dual, so Enρ⊗Enρ is isomorphic to End(Enρ). Therefore, the T-invariant subspace A(G)λ of A(G) is an increasing union of subrepresentations of the form End(E) for  representations E (of the form E(q−1)ρ)). However, for  representations of the form End(E)  an invariant polynomial that separates 0 and 1 is given by the determinant. This completes the sketch of the proof of Haboush's theorem.\n\nReferences\n \n\nMumford, D.; Fogarty, J.; Kirwan, F. Geometric invariant theory. Third edition. Ergebnisse der Mathematik und ihrer Grenzgebiete (2) (Results in Mathematics and Related Areas (2)), 34. Springer-Verlag, Berlin, 1994. xiv+292 pp.  \n\nCategory:Representation theory of algebraic groups\nCategory:Invariant theory\nCategory:Theorems in representation theory\nCategory:Conjectures that have been proved"
    },
    {
      "title": "Harish-Chandra's regularity theorem",
      "url": "https://en.wikipedia.org/wiki/Harish-Chandra%27s_regularity_theorem",
      "text": "In mathematics, Harish-Chandra's regularity theorem, introduced by , states that every invariant eigendistribution on a semisimple Lie group, and in particular every character of an irreducible unitary representation on a Hilbert space, is given by a locally integrable function.  proved a similar theorem for semisimple p-adic groups.\n\n had previously shown that any invariant eigendistribution is analytic on the regular elements of the group, by showing that on these elements it is a solution of an elliptic differential equation. The problem is that it may have singularities on the singular elements of the group; the regularity theorem implies that these singularities are not too severe.\n\nStatement\nA distribution on a group G or its Lie algebra is called invariant if it is invariant under conjugation by G.\n\nA distribution on a group G or its Lie algebra is called an eigendistribution if it is an eigenvector of the center of the universal enveloping algebra of G (identified with the left and right invariant differential operators of G.\n\nHarish-Chandra's regularity theorem states that any invariant eigendistribution on a semisimple group or Lie algebra is a locally integrable function. \nThe condition that it is an eigendistribution can be relaxed slightly to the condition that its image under the center of the universal enveloping algebra is finite-dimensional. The regularity theorem also implies that on each Cartan subalgebra the distribution can be written as a finite sum of exponentials divided by a function Δ that closely resembles the denominator of the Weyl character formula.\n\nProof\nHarish-Chandra's original proof of the regularity theorem is given in a sequence of five papers .\n gave an exposition of the proof of  Harish-Chandra's regularity theorem for the case of SL2(R), and sketched its generalization to higher rank groups.\n\nMost proofs can be broken up into several steps as follows.\n\nStep 1. If Θ is an invariant eigendistribution then it is analytic on the regular elements of G. This follows from elliptic regularity, by showing that the center of the universal enveloping algebra has an element that is \"elliptic transverse to an orbit of G\" for any regular orbit.\nStep 2.  If Θ is an invariant eigendistribution then its restriction to the regular elements of G is locally integrable on G. (This makes sense as the non-regular elements of G have measure zero.) This follows by showing that ΔΘ on each Cartan subalgebra is a finite sum of exponentials, where Δ is essentially the denominator of the Weyl denominator formula, with 1/Δ locally integrable.\nStep 3. By steps 1 and 2, the invariant eigendistribution Θ is a sum S+F where F is a locally integrable function and S has support on the singular elements of G. The problem is to show that S vanishes. This is done by stratifying the set of  singular elements of G as a union of locally closed submanifolds of G and using induction on the codimension of the strata. While it is possible for an eigenfunction of a differential equation to be of the form S+F with F locally integrable and S having singular support on a submanifold, this is only possible if the differential operator satisfies some restrictive conditions. One can then check that the Casimir operator of G does not satisfy these conditions on the strata of the singular set, which forces S to vanish.\n\nReferences\n\nCategory:Theorems in representation theory"
    },
    {
      "title": "Lafforgue's theorem",
      "url": "https://en.wikipedia.org/wiki/Lafforgue%27s_theorem",
      "text": "In mathematics, Lafforgue's theorem, due to Laurent Lafforgue, completes the Langlands program for general linear groups over algebraic function fields, by giving a correspondence between automorphic forms on these groups and representations of Galois groups. \n\nThe Langlands conjectures were introduced by  and  describe a correspondence between representations of the Weil group of an algebraic function field and representations of algebraic groups over the function field, generalizing class field theory of function fields from abelian Galois groups to non-abelian Galois groups.\n\nLanglands conjectures for GL1\nThe  Langlands conjectures for GL1(K) follow from (and are essentially equivalent to)   class field theory. More precisely the Artin map gives a map from the idele class group  to the abelianization of the Weil group.\n\nAutomorphic representations of GLn(F)\nThe representations of GLn(F) appearing in the Langlands correspondence are automorphic representations.\nLafforgue's theorem for GLn(F)\nHere F is a global field of some positive characteristic p, and ℓ is some prime not equal to p.\n\nLafforgue's theorem states that there is a bijection σ between:\nEquivalence classes of cuspidal representations π of GLn(F), and\nEquivalence classes of irreducible ℓ-adic representations σ(π) of dimension n of the absolute Galois group of F\nthat preserves the L-function at every place of F.\n\nThe proof of Lafforgue's theorem involves constructing a representation  σ(π) of the absolute Galois group for each cuspidal representation π. The idea of doing this is to look in the ℓ-adic cohomology of the moduli stack of shtukas of rank n that have compatible level N structures for all N. The cohomology contains subquotients of the form \nπ⊗σ(π)⊗σ(π)∨\nwhich can be used to construct σ(π) from π. A major problem is that the moduli stack is not of finite type, which means that there are formidable technical difficulties in studying its cohomology.\n\nApplications\nLafforgue's theorem implies the Ramanujan–Petersson conjecture that if an automorphic form for GLn(F) has central character of finite order, then the corresponding Hecke eigenvalues at every unramified place have absolute value 1.\n\nLafforgue's theorem implies the conjecture of   that an irreducible finite-dimensional l-adic representation of the absolute Galois group with determinant character of finite order is pure of weight 0.\n\nSee also\nLocal Langlands conjectures\n\nReferences\n\n Lafforgue, Laurent (2002), \"Chtoucas de Drinfeld, formule des traces d'Arthur-Selberg et correspondance de Langlands.\" (Drinfeld shtukas, Arthur-Selberg trace formula and Langlands correspondence)  Proceedings of the International Congress of Mathematicians, Vol. I (Beijing, 2002), 383–400, Higher Ed. Press, Beijing, 2002.\n\nGérard Laumon (2002), \"The work of Laurent Lafforgue\", Proceedings of the ICM, Beijing 2002, vol. 1, 91–97,\nG. Laumon (2000), \"La correspondance de Langlands sur les corps de fonctions (d'après Laurent Lafforgue)\" (The Langlands correspondence over function fields (according to Laurent Lafforgue)), Séminaire Bourbaki, 52e année, 1999–2000, no. 873.\n\nExternal links\nLafforgue's publications\nThe work of Robert Langlands \n\nCategory:Theorems in algebraic number theory\nCategory:Representation theory of Lie groups\nCategory:Automorphic forms\nCategory:Conjectures\nCategory:Class field theory\nCategory:Langlands program\nCategory:Theorems in representation theory"
    },
    {
      "title": "Lie–Kolchin theorem",
      "url": "https://en.wikipedia.org/wiki/Lie%E2%80%93Kolchin_theorem",
      "text": "In mathematics, the Lie–Kolchin theorem is a theorem in the representation theory of linear algebraic groups; Lie's theorem is the analog for linear Lie algebras.\n\nIt states that if G is a connected and solvable linear algebraic group defined over an algebraically closed field and\n\na representation on a nonzero finite-dimensional vector space V, then there is a one-dimensional linear subspace L of V such that\n\n \n\nThat is, ρ(G) has an invariant line L, on which G therefore acts through a one-dimensional representation. This is equivalent to the statement that V contains a nonzero vector v that is a common (simultaneous) eigenvector for all .\n\nIt follows directly that every irreducible finite-dimensional representation of a connected and solvable linear algebraic group G has dimension one. In fact, this is another way to state the Lie–Kolchin theorem.\n\nLie's theorem states that any nonzero representation of a solvable Lie algebra on a finite dimensional vector space over an algebraically closed field of characteristic 0 has a one-dimensional invariant subspace.\n\nThe result  for Lie algebras was proved by  and for algebraic groups was proved by .\n\nThe Borel fixed point theorem generalizes the Lie–Kolchin theorem.\n\n Triangularization \nSometimes the theorem is also referred to as the Lie–Kolchin triangularization theorem because by induction it implies that with respect to a suitable basis of V the image  has a triangular shape; in other words, the image group  is conjugate in GL(n,K) (where n = dim V) to a subgroup of the group T of upper triangular matrices, the standard Borel subgroup of GL(n,K): the image is simultaneously triangularizable.\n\nThe theorem applies in particular to a Borel subgroup of a semisimple linear algebraic group G.\n\nLie's theorem\n\nLie's theorem states that if V is a finite dimensional vector space over an algebraically closed field of characteristic 0, then for any solvable Lie algebra of endomorphisms of V there is a vector that is an eigenvector for every element of the Lie algebra.\n\nApplying this result repeatedly shows that there is a basis for V such that all elements of the Lie algebra are represented by upper triangular matrices. \nThis is a generalization of the result of Frobenius that commuting matrices are simultaneously upper triangularizable, as commuting matrices form an abelian Lie algebra, which is a fortiori solvable.\n\nA consequence of Lie's theorem  is that any finite dimensional solvable Lie algebra over a field of characteristic 0 has a nilpotent derived algebra.\n\n Counter-examples \n\nIf the field K is not algebraically closed, the theorem can fail. The standard unit circle, viewed as the set of complex numbers  of absolute value one is a one-dimensional commutative (and therefore solvable) linear algebraic group over the real numbers which has a two-dimensional representation into the special orthogonal group SO(2) without an invariant (real) line.  Here the image   of  is the orthogonal matrix\n\n \n\nFor algebraically closed fields of characteristic p>0 Lie's theorem holds provided the dimension of the representation is less than p, but can fail for representations of dimension p. An example is given by the 3-dimensional nilpotent Lie algebra spanned by 1, x, and d/dx acting on the p-dimensional vector space k[x]/(xp), which has no eigenvectors. Taking the semidirect product of this 3-dimensional Lie algebra by the p-dimensional representation (considered as an abelian Lie algebra) gives a solvable Lie algebra whose derived algebra is not nilpotent.\n\nReferences\n\nWilliam C. Waterhouse, Introduction to Affine Group Schemes, Graduate Texts in Mathematics vol. 66, Springer Verlag New York, 1979 (chapter 10, in particular section 10.2).\n\nCategory:Lie algebras\nCategory:Representation theory of algebraic groups\nCategory:Theorems in representation theory"
    },
    {
      "title": "Maschke's theorem",
      "url": "https://en.wikipedia.org/wiki/Maschke%27s_theorem",
      "text": "In mathematics, Maschke's theorem, named after Heinrich Maschke, is a theorem in group representation theory that concerns the decomposition of representations of a finite group into irreducible pieces. Maschke's theorem allow one to make general conclusions about representations of a finite group G without actually computing them. It reduces the task of classifying all representations to a more manageable task of classifying irreducible representations, since when the theorem applies, any representation is a direct sum of irreducible pieces (constituents). Moreover, it follows from the Jordan–Hölder theorem that, while the decomposition into a direct sum of irreducible subrepresentations may not be unique, the irreducible pieces have well-defined multiplicities. In particular, a representation of a finite group over a field of characteristic zero is determined up to isomorphism by its character.\n\n Formulations \n\nMaschke's theorem addresses the question: when is a general (finite-dimensional) representation built from irreducible subrepresentations using the direct sum operation? This question (and its answer) are formulated differently for different perspectives on group representation theory.\n\n Group-theoretic \n\nMaschke's theorem is commonly formulated as a corollary to the following result:\nTheorem. If  is a complex representation of a finite group  with a subrepresentation , then there is another subrepresentation  of  such that =⊕.\nThen the corollary is\nCorollary (Maschke's theorem). Every representation of a finite group  over a field  with characteristic not dividing the order of  is a direct sum of irreducible representations.\n\nThe vector space of complex-valued class functions of a group  has a natural -invariant inner product structure, described in the article Schur orthogonality relations. Maschke's theorem was originally proved for the case of representations over  by constructing  as the orthogonal complement of  under this inner product.\n\n Module-theoretic \nOne of the approaches to representations of finite groups is through module theory. Representations of a group G are replaced by modules over its group algebra K[G] (to be precise, there is an isomorphism of categories between K[G]-Mod and RepG, the category of representations of G).  Irreducible representations correspond to simple modules. In the module-theoretic language, Maschke's theorem asks: is an arbitrary module semisimple? In this context, the theorem can be reformulated as follows:\n\nMaschke's Theorem. Let G be a finite group and K a field whose characteristic does not divide the order of G. Then K[G], the group algebra of G, is semisimple.It follows that every module over K[G] is a semisimple module.The converse statement also holds: if the characteristic of the field divides the order of the group (the modular case), then the group algebra is not semisimple.\nThe importance of this result stems from the well developed theory of semisimple rings, in particular, the Artin–Wedderburn theorem (sometimes referred to as Wedderburn's Structure Theorem). When K is the field of complex numbers, this shows that the algebra K[G] is a product of several copies of complex matrix algebras, one for each irreducible representation.The number of the summands can be computed, and turns out to be equal to the number of the conjugacy classes of the group. If the field K has characteristic zero, but is not algebraically closed, for example, K is a field of real or rational numbers, then a somewhat more complicated statement holds: the group algebra K[G] is a product of matrix algebras over division rings over K. The summands correspond to irreducible representations of G over K.One must be careful, since a representation may decompose differently over different fields: a representation may be irreducible over the real numbers but not over the complex numbers.\n\n Category-theoretic \n\nReformulated in the language of semi-simple categories, Maschke's theorem states\n\nMaschke's theorem. If  is a group and  is a field with characteristic not dividing the order of , then the category of representations of  over  is semi-simple.\n\n Proofs \n\n Module-theoretic \nLet V be a K[G]-submodule. We will prove that V is a direct summand. Let π be any K-linear projection of K[G] onto V. Consider\nthe map  given by \nThen φ is again a projection: it is clearly K-linear, maps K[G] onto V, and induces the identity on V. Moreover we have\n\nso φ is in fact K[G]-linear. By the splitting lemma, . This proves that every submodule is a direct summand, that is, K[G] is semisimple.\n\n Converse statement \nThe above proof depends on the fact that #G is invertible in K. This might lead one to ask if the converse of Maschke's theorem also holds: if the characteristic of K divides the order of G, does it follow that K[G] is not semisimple? The answer is yes.\n\nProof. For  define . Let . Then I is a K[G]-submodule. We will prove that for every nontrivial submodule V of K[G], . Let V be given, and let  be any nonzero element of V. If , the claim is immediate. Otherwise, let . Then  so  and  so that  is an element of both I and V. This proves that V is not a direct complement of I for all V, so K[G] is not semisimple.\n\n Notes \n\nReferences\n\nCategory:Representation theory of finite groups\nCategory:Theorems in group theory\nCategory:Theorems in representation theory"
    },
    {
      "title": "Mautner's lemma",
      "url": "https://en.wikipedia.org/wiki/Mautner%27s_lemma",
      "text": "In mathematics, Mautner's lemma in representation theory states that if G is a topological group and π a unitary representation of G on a Hilbert space H, then for any x in G, which has conjugates \n\nyxy−1\n\nconverging to the identity element e, for a net of elements y, then any vector v of H invariant under all the π(y) is also invariant under π(x).\n\nReferences\nF. Mautner, Geodesic flows on symmetric Riemannian spaces (1957),  Ann. Math. 65, 416-430\n\nCategory:Unitary representation theory\nCategory:Topological groups\nCategory:Theorems in representation theory\nCategory:Lemmas"
    },
    {
      "title": "Multiplicity-one theorem",
      "url": "https://en.wikipedia.org/wiki/Multiplicity-one_theorem",
      "text": "In the mathematical theory of automorphic representations, a multiplicity-one theorem is a result about the representation theory of an adelic reductive algebraic group. The multiplicity in question is the number of times a given abstract group representation is realised in a certain space, of square-integrable functions, given in a concrete way.\n\nA multiplicity one theorem may also refer to a result about the restriction of a representation of a group G to a subgroup H. In that context, the pair (G, H) is called a strong Gelfand pair. \n\nDefinition\nLet G be a reductive algebraic group over a number field K and let A denote the adeles of K. Let Z denote the centre of G and let  be a continuous unitary character from Z(K)\\Z(A)× to C×. Let L20(G(K)/G(A), ) denote the space of cusp forms with central character ω on G(A). This space decomposes into a direct sum of Hilbert spaces\n\nwhere the sum is over irreducible subrepresentations and m are non-negative integers.\n\nThe group of adelic points of G, G(A), is said to satisfy the multiplicity-one property if any smooth irreducible admissible representation of G(A) occurs with multiplicity at most one in the space of cusp forms of central character , i.e. m is 0 or 1 for all such .\n\nResults\nThe fact that the general linear group, GL(n), has the multiplicity-one property was proved by  for n = 2 and independently by  and  for n > 2 using the uniqueness of the Whittaker model. Multiplicity-one also holds for SL(2), but not for SL(n) for n > 2 .\n\nStrong multiplicity one theorem\n\nThe strong multiplicity one theorem of  and  states that two cuspidal automorphic representations of the general linear group are isomorphic if their local components are isomorphic for all but a finite number of places.\n\nReferences\n\n \n\nCategory:Representation theory of groups\nCategory:Automorphic forms\nCategory:Theorems in number theory\nCategory:Theorems in representation theory"
    },
    {
      "title": "Peter–Weyl theorem",
      "url": "https://en.wikipedia.org/wiki/Peter%E2%80%93Weyl_theorem",
      "text": "In mathematics, the Peter–Weyl theorem is a basic result in the theory of harmonic analysis, applying to topological groups that are compact, but are not necessarily abelian. It was initially proved by Hermann Weyl, with his student Fritz Peter, in the setting of a compact topological group G . The theorem is a collection of results generalizing the significant facts about the decomposition of the regular representation of any finite group, as discovered by Ferdinand Georg Frobenius and Issai Schur.\n\nThe theorem has three parts.  The first part states that the matrix coefficients of irreducible representations of G are dense in the space C(G) of continuous complex-valued functions on G, and thus also in the space L2(G) of square-integrable functions.  The second part asserts the complete reducibility of unitary representations of G.  The third part then asserts that the regular representation of G on L2(G) decomposes as the direct sum of all irreducible unitary representations.  Moreover, the matrix coefficients of the irreducible unitary representations form an orthonormal basis of L2(G). In the case that G is the group of unit complex numbers, this last result is simply a standard result from Fourier series.\n\nMatrix coefficients\nA matrix coefficient of the group G is a complex-valued function φ on G given as the composition\n\nwhere π : G → GL(V) is a finite-dimensional (continuous) group representation of G, and L is a linear functional on the vector space of endomorphisms of V (e.g. trace), which contains GL(V) as an open subset.  Matrix coefficients are continuous, since representations are by definition continuous, and linear functionals on finite-dimensional spaces are also continuous.\n\nThe first part of the Peter–Weyl theorem asserts (; ):\n\nPeter–Weyl Theorem (Part I). The set of matrix coefficients of G is dense in the space of continuous complex functions C(G) on G, equipped with the uniform norm.\n\nThis first result resembles the Stone–Weierstrass theorem in that it indicates the density of a set of functions in the space of all continuous functions, subject only to an algebraic characterization.  In fact, the matrix coefficients of tensor product form a unital algebra invariant under complex conjugation because the product of two matrix coefficients is a matrix coefficient of the tensor product representation, and the complex conjugate is a matrix coefficient of the dual representation. Hence the theorem follows directly from the Stone–Weierstrass theorem if the matrix coefficients separate points, which is obvious if G is a matrix group . Conversely, it is a consequence of the theorem that any compact Lie group is isomorphic to a matrix group .\n\nA corollary of this result is that the matrix coefficients of G are dense in L2(G).\n\nDecomposition of a unitary representation\nThe second part of the theorem gives the existence of a decomposition of a unitary representation of G into finite-dimensional representations. Now, intuitively groups were conceived as rotations on geometric objects, so it is only natural to study representations which essentially arise from continuous actions on Hilbert spaces. (For those who were first introduced to dual groups consisting of characters which are the continuous homomorphisms into the circle group, this approach is similar except that the circle group is (ultimately) generalised to the group of unitary operators on a given Hilbert space.)\n\nLet G be a topological group and H a complex Hilbert space.\n\nA continuous action ∗ : G × H → H, gives rise to a continuous map ρ∗ : G → HH (functions from H to H with the strong topology) defined by: ρ∗(g)(v) = ∗(g,v). This map is clearly an homomorphism from G into GL(H), the homeomorphic automorphisms of H. Conversely, given such a map, we can uniquely recover the action in the obvious way.\n\nThus we define the representations of G on a Hilbert space H to be those group homomorphisms, ρ, which arise from continuous actions of G on H. We say that a representation ρ is unitary if ρ(g) is a unitary operator for all g ∈ G; i.e.,  for all v, w ∈ H. (I.e. it is unitary if ρ : G → U(H). Notice how this generalises the special case of the one-dimensional Hilbert space, where U(C) is just the circle group.)\n\nGiven these definitions, we can state the second part of the Peter–Weyl theorem :\n\nPeter–Weyl Theorem (Part II). Let ρ be a unitary representation of a compact group G on a complex Hilbert space H.  Then H splits into an orthogonal direct sum of irreducible finite-dimensional unitary representations of G.\n\nDecomposition of square-integrable functions\nTo state the third and final part of the theorem, there is a natural Hilbert space over G consisting of square-integrable functions, ; this makes sense because Haar measure exists on G. The group G has a unitary representation ρ on  given by acting on the left, via\n\nThe final statement of the Peter–Weyl theorem  gives an explicit orthonormal basis of .  Roughly it asserts that the matrix coefficients for G, suitably renormalized, are an orthonormal basis of L2(G).  In particular,  decomposes into an orthogonal direct sum of all the irreducible unitary representations, in which the multiplicity of each irreducible representation is equal to its degree (that is, the dimension of the underlying space of the representation).  Thus,\n\nwhere Σ denotes the set of (isomorphism classes of) irreducible unitary representations of G, and the summation denotes the closure of the direct sum of the total spaces Eπ of the representations π.\n\nWe may also regard  as a representation of the direct product group , with the two factors acting by translation on the left and the right, respectively. Fix a representation  of . The space of matrix coefficients for the representation may be identified with , the space of linear maps of  to itself. The natural left and right action of  on the matrix coefficients corresponds to the action on  given by\n\nThen we may decompose  as unitary representation of  in the form\n\nFinally, we may form an orthonormal basis for  as follows. Suppose that a representative π is chosen for each isomorphism class of irreducible unitary representation, and denote the collection of all such π by Σ. Let  be the matrix coefficients of π in an orthonormal basis, in other words\n\nfor each g ∈ G.  Finally, let d(π) be the degree of the representation π. The theorem now asserts that the set of functions\n\nis an orthonormal basis of \n\nRestriction to class functions\nA function  on G is called a class function if  for all  and  in G. The space of class functions forms a closed subspace of , and therefore a Hilbert space in its own right. Within the space of matrix coefficients for a fixed representation  is the character  of , defined by\n\nIn the notation above, the character is the sum of the diagonal matrix coefficients:\n\nAn important consequence of the preceding result is the following:\nTheorem: The characters of the irreducible representations of G form an orthonormal basis for the space of square-integrable class functions on G.\nThis result plays an important part in Weyl's classification of the representations of a connected compact Lie group. Chapter 12\n\nAn example: \nA simple but helpful example is the case of the group of complex numbers of magnitude 1, . In this case, the irreducible representations are one-dimensional and given by\n\nThere is then a single matrix coefficient for each representation, the function\n\nThe last part of the Peter–Weyl theorem then asserts in this case that these functions form an orthonormal basis for . In this case, the theorem is simply a standard result from the theory of Fourier series. \n\nFor any compact group G, we can regard the decomposition of  in terms of matrix coefficients as a generalization of the theory of Fourier series. Indeed, this decomposition is often referred to as a Fourier series.\n\nAn example: SU(2)\nWe use the standard representation of the group SU(2) as\n\nThus, SU(2) is represented as the 3-sphere  sitting inside .\nThe irreducible representations of SU(2), meanwhile, are labeled by a non-negative integer  and can be realized as the natural action of SU(2) on the space of homogeneous polynomials of degree  in two complex variables. Example 4.10 The matrix coefficients of the th representation are hyperspherical harmonics of degree , that is, the restrictions to  of homogeneous harmonic polynomials of degree  in  and . The key to verifying this claim is to compute that for any two complex numbers  and , the function \n \nis harmonic as a function of .\n\nIn this case, finding an orthonormal basis for  consisting of matrix coefficients amounts to finding an orthonormal basis consisting of hyperspherical harmonics, which is a standard construction in analysis on spheres.\n\nConsequences\nRepresentation theory of connected compact Lie groups\nThe Peter–Weyl theorem—specifically the assertion that the characters form an orthonormal basis for the space of square-integrable class functions—plays a key role in the classification of the irreducible representations of a connected compact Lie group. Section 12.5 The argument also depends on the Weyl integral formula (for class functions) and the Weyl character formula.\n\nAn outline of the argument may be found here.\n\nLinearity of compact Lie groups\nOne important consequence of the Peter–Weyl theorem is the following:\nTheorem: Every compact Lie group has a faithful finite-dimensional representation and is therefore isomorphic to a closed subgroup of  for some .\n\nStructure of compact topological groups\nFrom the Peter–Weyl theorem, one can deduce a significant general structure theorem. Let G be a compact topological group, which we assume Hausdorff. For any finite-dimensional G-invariant subspace V in L2(G), where G acts on the left, we consider the image of G in GL(V). It is closed, since G is compact, and a subgroup of the Lie group GL(V). It follows by a theorem of Élie Cartan that the image of G is a Lie group also.\n\nIf we now take the limit (in the sense of category theory) over all such spaces V, we get a result about G: Because G acts faithfully on L2(G), G is an inverse limit of Lie groups. It may of course not itself be a Lie group: it may for example be a profinite group.\n\nSee also\n Pontryagin duality\n\nReferences\n .\n\n .\n .\n \n .\n .\n \n .\n\nSpecific\n\nCategory:Unitary representation theory\nCategory:Topological groups\nCategory:Theorems in harmonic analysis\nCategory:Theorems in representation theory\nCategory:Theorems in group theory"
    },
    {
      "title": "Wigner–Eckart theorem",
      "url": "https://en.wikipedia.org/wiki/Wigner%E2%80%93Eckart_theorem",
      "text": "The Wigner–Eckart theorem is a theorem of representation theory and quantum mechanics. It states that matrix elements of spherical tensor operators in the basis of angular momentum eigenstates can be expressed as the product of two factors, one of which is independent of angular momentum orientation, and the other a Clebsch–Gordan coefficient. The name derives from physicists Eugene Wigner and Carl Eckart, who developed the formalism as a link between the symmetry transformation groups of space (applied to the Schrödinger equations) and the laws of conservation of energy, momentum, and angular momentum.Eckart Biography – The National Academies Press.\n\nMathematically, the Wigner–Eckart theorem is generally stated in the following way.  Given a tensor operator  and two states of angular momenta  and , there exists a constant  such that for all , , and , the following equation is satisfied:\n\nwhere\n is the -th component of the spherical tensor operator  of rank ,The parenthesized superscript  provides a reminder of its rank.  However, unlike , it need not be an actual index. \n  denotes an eigenstate of total angular momentum  and its z component ,\n  is the Clebsch–Gordan coefficient for coupling  with  to get ,\n  denotesThis is a special notation specific to the Wigner–Eckart theorem. some value that does not depend on , , nor  and is referred to as the reduced matrix element.\n\nThe Wigner–Eckart theorem states indeed that operating with a spherical tensor operator of rank  on an angular momentum eigenstate is like adding a state with angular momentum k to the state. The matrix element one finds for the spherical tensor operator is proportional to a Clebsch–Gordan coefficient, which arises when considering adding two angular momenta. When stated another way, one can say that the Wigner–Eckart theorem is a theorem that tells how vector operators behave in a subspace. Within a given subspace, a component of a vector operator will behave in a way proportional to the same component of the angular momentum operator.  This definition is given in the book Quantum Mechanics by Cohen–Tannoudji, Diu and Laloe.\n\nBackground and overview\n\nMotivating example: position operator matrix elements for 4d → 2p transition\n\nLet's say we want to calculate transition dipole moments for an electron transition from a 4d to a 2p orbital of a hydrogen atom, i.e. the matrix elements of the form , where ri is either the x, y, or z component of the position operator, and m1, m2 are the magnetic quantum numbers that distinguish different orbitals within the 2p or 4d subshell. If we do this directly, it involves calculating 45 different integrals: there are 3 possibilities for m1 (−1, 0, 1), 5 possibilities for m2 (−2, −1, 0, 1, 2), and 3 possibilities for i, so the total is 3 × 5 × 3 = 45.\n\nThe Wigner–Eckart theorem allows one to obtain the same information after evaluating just one of those 45 integrals (any of them can be used, as long as it is nonzero). Then the other 44 integrals can be inferred from that first one—without the need to write down any wavefunctions or evaluate any integrals—with the help of Clebsch–Gordan coefficients, which can be easily looked up in a table or computed by hand or computer.\n\nQualitative summary of proof\n\nThe Wigner–Eckart theorem works because all 45 of these different calculations are related to each other by rotations. If an electron is in one of the 2p orbitals, rotating the system will generally move it into a different 2p orbital (usually it will wind up in a quantum superposition of all three basis states, m = +1, 0, −1). Similarly, if an electron is in one of the 4d orbitals, rotating the system will move it into a different 4d orbital. Finally, an analogous statement is true for the position operator: when the system is rotated, the three different components of the position operator are effectively interchanged or mixed.\n\nIf we start by knowing just one of the 45 values (say, we know that ) and then we rotate the system, we can infer that K is also the matrix element between the rotated version of , the rotated version of , and the rotated version of . This gives an algebraic relation involving K and some or all of the 44 unknown matrix elements. Different rotations of the system lead to different algebraic relations, and it turns out that there is enough information to figure out all of the matrix elements in this way.\n\n(In practice, when working through this math, we usually apply angular momentum operators to the states, rather than rotating the states. But this is fundamentally the same thing, because of the close mathematical relation between rotations and angular momentum operators.)\n\nIn terms of representation theory\n\nTo state these observations more precisely and to prove them, it helps to invoke the mathematics of representation theory. For example, the set of all possible 4d orbitals (i.e., the 5 states m = −2, −1, 0, 1, 2 and their quantum superpositions) form a 5-dimensional abstract vector space. Rotating the system transforms these states into each other, so this is an example of a \"group representation\", in this case, the 5-dimensional irreducible representation (\"irrep\") of the rotation group SU(2) or SO(3), also called the \"spin-2 representation\". Similarly, the 2p quantum states form a 3-dimensional irrep (called \"spin-1\"), and the components of the position operator also form the 3-dimensional \"spin-1\" irrep.\n\nNow consider the matrix elements . It turns out that these are transformed by rotations according to the direct product of those three representations, i.e. the spin-1 representation of the 2p orbitals, the spin-1 representation of the components of r, and the spin-2 representation of the 4d orbitals. This direct product, a 45-dimensional representation of SU(2), is not an irreducible representation, instead it is the direct sum of a spin-4 representation, two spin-3 representations, three spin-2 representations, two spin-1 representations, and a spin-0 (i.e. trivial) representation. The nonzero matrix elements can only come from the spin-0 subspace. The Wigner–Eckart theorem works because the direct product decomposition contains one and only one spin-0 subspace, which implies that all the matrix elements are determined by a single scale factor.\n\nApart from the overall scale factor, calculating the matrix element  is equivalent to calculating the projection of the corresponding abstract vector (in 45-dimensional space) onto the spin-0 subspace. The results of this calculation are the Clebsch–Gordan coefficients. The key qualitative aspect of the Clebsch–Gordan decomposition that makes the argument work is that in the decomposition of the tensor product of two irreducible representations, each irreducible representation occurs only once. This allows Schur's lemma to be used. Appendix C.\n\nProof\n\nStarting with the definition of a spherical tensor operator, we have\n\nwhich we use to then calculate\n\nIf we expand the commutator on the LHS by calculating the action of the  on the bra and ket, then we get\n\nWe may combine these two results to get\n\nThis recursion relation for the matrix elements closely resembles that of the Clebsch–Gordan coefficient.  In fact, both are of the form . We therefore have two sets of linear homogeneous equations:\n\none for the Clebsch–Gordan coefficients () and one for the matrix elements (). It is not possible to exactly solve for . We can only say that the ratios are equal, that is\n\nor that , where the coefficient of proportionality is independent of the indices. Hence, by comparing recursion relations, we can identify the Clebsch–Gordan coefficient  with the matrix element , then we may write\n\n Alternative conventions \n\nThere are different conventions for the reduced matrix elements.  One convention, used by Racah and Wigner, includes an additional phase and normalization factor,\n\nwhere the  array denotes the 3-j symbol.  (Since in practice  is often integral, the  factor is sometimes omitted in literature.)  With this choice of normalization, the reduced matrix element satisfies the relation:\n\nwhere the Hermitian adjoint is defined with the  convention.  Although this relation is not affected by the presence or absence of the  phase factor in the definition of the reduced matrix element, it is affected by the phase convention for the Hermitian adjoint.\n\nAnother convention for reduced matrix elements is that of Sakurai's Modern Quantum Mechanics:\n\n Example \n\nConsider the position expectation value . This matrix element is the expectation value of a Cartesian operator in a spherically symmetric hydrogen-atom-eigenstate basis, which is a nontrivial problem. However, the Wigner–Eckart theorem simplifies the problem. (In fact, we could obtain the solution quickly using parity, although a slightly longer route will be taken.)\n\nWe know that  is one component of , which is a vector.  Since vectors are rank-1 spherical tensor operators, it follows that  must be some linear combination of a rank-1 spherical tensor  with }.  In fact, it can be shown that\n\nwhere we define the spherical tensors asJ. J. Sakurai: \"Modern quantum mechanics\" (Massachusetts, 1994, Addison-Wesley).\n\nand  are spherical harmonics, which themselves are also spherical tensors of rank .  Additionally, , and\n\nTherefore,\n\nThe above expression gives us the matrix element for  in the  basis.  To find the expectation value, we set , , and .  The selection rule for  and  is  for the  spherical tensors.  As we have , this makes the Clebsch–Gordan Coefficients zero, leading to the expectation value to be equal to zero.\n\nSee also\n\nTensor operator\nLandé g-factor\n\nReferences\n\n General \n\n \n\nExternal links\nJ. J. Sakurai, (1994). \"Modern Quantum Mechanics\", Addison Wesley, .\n\nWigner–Eckart theorem\nTensor Operators\n\nCategory:Quantum mechanics\nCategory:Representation theory of Lie groups\nCategory:Theorems in quantum physics\nCategory:Theorems in representation theory"
    },
    {
      "title": "Abhyankar's conjecture",
      "url": "https://en.wikipedia.org/wiki/Abhyankar%27s_conjecture",
      "text": "In abstract algebra, Abhyankar's conjecture is a 1957 conjecture of Shreeram Abhyankar, on the Galois groups of algebraic function fields of characteristic p.. The soluble case was solved by Serre in 1990 and the full conjecture was proved in 1994 by work of Michel Raynaud and David Harbater...\n\nThe problem involves a finite group G, a prime number p, and the function field K(C) of a nonsingular integral algebraic curve C defined over an algebraically closed field K of characteristic p.\n\nThe question addresses the existence of a Galois extension L of K(C), with G as Galois group, and with specified ramification. From a geometric point of view, L corresponds to another curve C′, together with\na morphism\n\nπ : C′ → C.\n\nGeometrically, the assertion that π is ramified at a finite set S of points on C\nmeans that π restricted to the complement of S in C is an étale morphism.\nThis is in analogy with the case of Riemann surfaces.\nIn Abhyankar's conjecture, S is fixed, and the question is what G can be. This is therefore a special type of inverse Galois problem.\n\nThe subgroup p(G) is defined to be the subgroup generated by all the Sylow subgroups of G for the prime number p. This is a normal subgroup, and the parameter n is defined as the minimum number of generators of\n\nG/p(G).\n\nThen for the case of C the projective line over K, the conjecture states that G can be realised as a Galois group of L, unramified outside S containing s + 1 points,  if and only if\n\nn ≤ s.\n\nThis was proved by Raynaud.\n\nFor the general case, proved by Harbater, let g be the genus of C. Then G can be realised if and only if\n\nn ≤ s + 2 g.\n\n References \n\n External links \n \n A layman's perspective of Abhyankar's conjecture from Purdue University\n\nCategory:Algebraic curves\nCategory:Galois theory\nCategory:Theorems in abstract algebra\nCategory:Conjectures that have been proved"
    },
    {
      "title": "Abhyankar's inequality",
      "url": "https://en.wikipedia.org/wiki/Abhyankar%27s_inequality",
      "text": "Abhyankar's inequality is an inequality involving extensions of valued fields in algebra, introduced by .\n\nIf K/k is an extension of valued fields, then Abhyankar's inequality states that the transcendence degree of K/k is at least the transcendence degree of the residue field extension plus the Q-rank of the quotient of the valuation groups.\n\nReferences\n\nCategory:Field theory\nCategory:Commutative algebra\nCategory:Theorems in abstract algebra"
    },
    {
      "title": "Abhyankar's lemma",
      "url": "https://en.wikipedia.org/wiki/Abhyankar%27s_lemma",
      "text": "In mathematics, Abhyankar's lemma (named after Shreeram Shankar Abhyankar) allows one to kill tame ramification by taking an extension of a base field. \n\nMore precisely, Abhyankar's lemma states that if A, B, C are local fields such that A and B are finite extensions of C, with ramification indices a and b, and B is tamely ramified over C and b divides a, then the compositum\nAB is an unramified extension of A.\n\nReferences\n. Theorem 3, page 504.\n.\n, p. 279.\n .\n\nCategory:Theorems in algebraic geometry\nCategory:Lemmas\nCategory:Algebraic number theory\nCategory:Theorems in abstract algebra"
    },
    {
      "title": "Ado's theorem",
      "url": "https://en.wikipedia.org/wiki/Ado%27s_theorem",
      "text": "In abstract algebra, Ado's theorem is a theorem characterizing finite-dimensional Lie algebras.\n\nStatement\n\nAdo's theorem states that every finite-dimensional Lie algebra L over a field K of characteristic zero can be viewed as a Lie algebra of square matrices under the commutator bracket. More precisely, the theorem states that L has a linear representation ρ over K, on a finite-dimensional vector space V, that is a faithful representation, making L isomorphic to a subalgebra of the endomorphisms of V.\n\nHistory\nThe theorem was proved in 1935 by Igor Dmitrievich Ado of Kazan State University, a student of Nikolai Chebotaryov.\n\nThe restriction on the characteristic was later removed by Kenkichi Iwasawa (see also the below Gerhard Hochschild paper for a proof).\n\nImplications\nWhile for the Lie algebras associated to classical groups there is nothing new in this, the general case is a deeper result. Applied to the real Lie algebra of a Lie group G, it does not imply that G has a faithful linear representation (which is not true in general), but rather that G always has a linear representation that is a local isomorphism with a linear group.\n\nReferences\n .  (Russian language)\n translation in \n\n \n Nathan Jacobson, Lie Algebras, pp. 202–203\n\nExternal links\nAdo’s theorem, comments and a proof of Ado's theorem in Terence Tao's blog What's new.\n\nCategory:Lie algebras\nCategory:Theorems in abstract algebra"
    },
    {
      "title": "Andreotti–Grauert theorem",
      "url": "https://en.wikipedia.org/wiki/Andreotti%E2%80%93Grauert_theorem",
      "text": "In mathematics, the Andreotti–Grauert theorem, introduced by ,  gives conditions for cohomology groups of coherent sheaves over complex manifolds to vanish or to be finite-dimensional.\n\nReferences\n\nCategory:Complex manifolds\nCategory:Theorems in abstract algebra"
    },
    {
      "title": "Artin approximation theorem",
      "url": "https://en.wikipedia.org/wiki/Artin_approximation_theorem",
      "text": "In mathematics, the Artin approximation theorem is a fundamental result of  in deformation theory which implies that formal power series with coefficients in a field k are well-approximated by the algebraic functions on k.\n\nMore precisely, Artin proved two such theorems: one, in 1968, on approximation of complex analytic solutions by formal solutions (in the case k = C); and an algebraic version of this theorem in 1969.\n\nStatement of the theorem\nLet \n\nx = x1, …, xn\n\ndenote a collection of n indeterminates,\n\nk[[x]] the ring of formal power series with indeterminates x over a field k, and\n\n y = y1, …, ym\n\na different set of indeterminates. Let \n\nf(x, y) = 0\n\nbe a system of polynomial equations in k[x, y], and c a positive integer. Then given a formal power series solution ŷ(x) ∈ k[[x]] there is an algebraic solution y(x) consisting of algebraic functions (more precisely, algebraic power series) such that \n\nŷ(x) ≡ y(x) mod (x)c.\n\nDiscussion\nGiven any desired positive integer c, this theorem shows that one can find an algebraic solution approximating a formal power series solution up to the degree specified by c. This leads to theorems that deduce the existence of certain formal moduli spaces of deformations as schemes. See also: Artin's criterion.\n\nAlternative statement\nThe following alternative statement is given in Theorem 1.12 of .\n\nLet R be a field or an excellent discrete valuation ring, let A be the henselization of an R-algebra of finite type at a prime ideal, let m be a proper ideal of A, let  be the m-adic completion of A, and let \n\nF: (A-algebras) → (sets), \n\nbe a functor sending filtered colimits to filtered colimits (Artin calls such a functor locally of finite presentation).\n\nThen for any integer c and any  there is a  such that \n\n ≡  mod mc.\n\n See also \nRing with the approximation property\nPopescu's theorem\n\nReferences\n\nArtin, Michael. Algebraic Spaces. Yale University Press, 1971.\n\nCategory:Moduli theory\nCategory:Commutative algebra\nCategory:Theorems in abstract algebra"
    },
    {
      "title": "Artin–Tate lemma",
      "url": "https://en.wikipedia.org/wiki/Artin%E2%80%93Tate_lemma",
      "text": "In algebra, the Artin–Tate lemma, named after Emil Artin and John Tate, states:\nLet A be a commutative Noetherian ring and  algebras over A. If C is of finite type over A and if C is finite over B, then B is of finite type over A.\n(Here, \"of finite type\" means \"finitely generated algebra\" and \"finite\" means \"finitely generated module\".) The lemma was introduced by E. Artin and J. Tate in 1951E Artin, J.T Tate, \"A note on finite ring extensions,\" J. Math. Soc Japan, Volume 3, 1951, pp. 74–77 to give a proof of Hilbert's Nullstellensatz.\n\n Proof \n\nThe following proof can be found in  Atiyah–MacDonald. Let  generate  as an -algebra and let  generate  as a -module. Then we can write \n\n \n\nwith . Then  is finite over the -algebra  generated by the . Using that  and hence  is Noetherian, also  is finite over . Since  is a finitely generated -algebra, also  is a finitely generated -algebra.\n\n Noetherian necessary \nWithout the assumption that A is Noetherian, the statement of the Artin-Tate lemma is no longer true. Indeed, for any non-Noetherian ring A we can define an A-algebra structure on  by declaring . Then for any ideal  which is not finitely generated,  is not of finite type over A, but all conditions as in the lemma are satisfied.\n\n References \n\n Eisenbud, David, Commutative Algebra with a View Toward Algebraic Geometry, Graduate Texts in Mathematics, 150, Springer-Verlag, 1995, .\nM. Atiyah, I.G. Macdonald, Introduction to Commutative Algebra, Addison–Wesley, 1994. \n\n External links \nhttp://commalg.subwiki.org/wiki/Artin-Tate_lemma\n\nCategory:Theorems in abstract algebra\nCategory:Lemmas\nCategory:Commutative algebra"
    },
    {
      "title": "Artin–Wedderburn theorem",
      "url": "https://en.wikipedia.org/wiki/Artin%E2%80%93Wedderburn_theorem",
      "text": "In algebra, the Artin–Wedderburn theorem is a classification theorem for semisimple rings and semisimple algebras.  The theorem states that an (Artinian) Semisimple rings are necessarily Artinian rings. Some authors use \"semisimple\" to mean the ring has a trivial Jacobson radical. For Artinian rings, the two notions are equivalent, so \"Artinian\" is included here to eliminate that ambiguity. semisimple ring R is isomorphic to a product of finitely many ni-by-ni matrix rings over division rings Di, for some integers ni, both of which are uniquely determined up to permutation of the index i.  In particular, any simple left or right Artinian ring is isomorphic to an n-by-n matrix ring over a division ring D, where both n and D are uniquely determined.\n\nAs a direct corollary, the Artin–Wedderburn theorem implies that every simple ring that is finite-dimensional over a division ring (a simple algebra) is a matrix ring.  This is Joseph Wedderburn's original result.  Emil Artin later generalized it to the case of Artinian rings.\n\nNote that if R is a finite-dimensional simple algebra over a division ring E, D need not be contained in E.  For example, matrix rings over the complex numbers are finite-dimensional simple algebras over the real numbers.\n\nThe Artin–Wedderburn theorem reduces classifying simple rings over a division ring to classifying division rings that contain a given division ring.  This in turn can be simplified:  The center of D must be a field K. Therefore, R is a K-algebra, and itself has K as its center.  A finite-dimensional simple algebra R is thus a central simple algebra over K.  Thus the Artin–Wedderburn theorem reduces the problem of classifying finite-dimensional central simple algebras to the problem of classifying division rings with given center.\n\nExamples\nLet R be the field of real numbers, C be the field of complex numbers, and H the quaternions.\n\n Every finite-dimensional simple algebra over R is isomorphic to a matrix ring over R, C, or H.  Every central simple algebra over R is isomorphic to a matrix ring over R or H.  These results follow from the Frobenius theorem.\n Every finite-dimensional simple algebra over C is a central simple algebra, and is isomorphic to a matrix ring over C.\n Every finite-dimensional central simple algebra over a finite field is isomorphic to a matrix ring over that field.\n For a commutative ring, the four following properties are equivalent: being a semisimple ring; being Artinian and reduced; being a reduced Noetherian ring of Krull dimension 0; being isomorphic to a finite direct product of fields.\n The Artin–Wedderburn theorem implies that a semisimple algebra that is finite-dimensional over a field  is isomorphic to a finite product  where the  are natural numbers, the  are finite dimensional division algebras over  (possibly finite extension fields of ), and  is the algebra of  matrices over .  Again, this product is unique up to permutation of the factors.\n\nSee also\n Maschke's theorem\n Brauer group\n Jacobson density theorem\n Hypercomplex number\n\nReferences\n\n P. M. Cohn (2003) Basic Algebra: Groups, Rings, and Fields, pages 137–9.\n \n \n\nCategory:Ring theory\nCategory:Theorems in abstract algebra"
    },
    {
      "title": "Artin–Zorn theorem",
      "url": "https://en.wikipedia.org/wiki/Artin%E2%80%93Zorn_theorem",
      "text": "In mathematics, the Artin–Zorn theorem, named after Emil Artin and Max Zorn, states that any finite alternative division ring is necessarily a finite field.  It was first published in 1930 by Zorn, but in his publication Zorn credited it to Artin...\n\nThe Artin–Zorn theorem is a generalization of the Wedderburn theorem, which states that finite associative division rings are fields. As a geometric consequence, every finite Moufang plane is the classical projective plane over a finite field...\n\nReferences\n\nCategory:Ring theory\nCategory:Theorems in abstract algebra"
    },
    {
      "title": "Auslander–Buchsbaum formula",
      "url": "https://en.wikipedia.org/wiki/Auslander%E2%80%93Buchsbaum_formula",
      "text": "In commutative algebra, the Auslander–Buchsbaum formula, introduced by ,  states that if R is a commutative Noetherian local ring and M is a non-zero finitely generated R-module of finite projective dimension, then\n\n \n\nHere pd stands for the projective dimension of a module, and depth for the depth of a module.\n\nApplications\n\nThe Auslander–Buchsbaum formula implies that a Noetherian local ring is regular if, and only if, it has finite global dimension. In turn this implies that the localization of a regular local ring is regular.\n\nIf A is a local finitely generated R-algebra (over a regular local ring R), then the Auslander–Buchsbaum formula implies that A is Cohen–Macaulay if, and only if, pdRA = codimRA.\n\nReferences\n\nChapter 19 of \n\nCategory:Commutative algebra\nCategory:Theorems in abstract algebra"
    },
    {
      "title": "Auslander–Buchsbaum theorem",
      "url": "https://en.wikipedia.org/wiki/Auslander%E2%80%93Buchsbaum_theorem",
      "text": "In commutative algebra, the Auslander–Buchsbaum theorem states that regular local rings are unique factorization domains.\n\nThe theorem was first proved by . They showed that regular local rings of dimension 3 are unique factorization domains, and  had previously shown that this implies that all regular local rings are unique factorization domains.\n\nReferences\n\nCategory:Commutative algebra\nCategory:Theorems in abstract algebra"
    },
    {
      "title": "Beauville–Laszlo theorem",
      "url": "https://en.wikipedia.org/wiki/Beauville%E2%80%93Laszlo_theorem",
      "text": "In mathematics, the Beauville–Laszlo theorem is a result in commutative algebra and algebraic geometry that allows one to \"glue\" two sheaves over an infinitesimal neighborhood of a point on an algebraic curve.  It was proved by .\n\nThe theorem\nAlthough it has implications in algebraic geometry, the theorem is a local result and is stated in its most primitive form for commutative rings.  If A is a ring and f is a nonzero element of A, then we can form two derived rings: the localization at f, Af, and the completion at Af, Â; both are A-algebras. In the following we assume that f is a non-zero divisor.  Geometrically, A is viewed as a scheme X = Spec A and f as a divisor (f) on Spec A; then Af is its complement Df = Spec Af, the principal open set determined by f, while Â is an \"infinitesimal neighborhood\" D = Spec Â of (f).  The intersection of Df and Spec Â is a \"punctured infinitesimal neighborhood\" D0 about (f), equal to Spec Â ⊗A Af = Spec Âf.\n\nSuppose now that we have an A-module M; geometrically, M is a sheaf on Spec A, and we can restrict it to both the principal open set Df and the infinitesimal neighborhood Spec Â, yielding an Af-module F and an Â-module G.  Algebraically,\n\n(Despite the notational temptation to write , meaning the completion of the A-module M at the ideal Af, unless A is noetherian and M is finitely-generated, the two are not in fact equal.  This phenomenon is the main reason that the theorem bears the names of Beauville and Laszlo; in the noetherian, finitely-generated case, it is, as noted by the authors, a special case of Grothendieck's faithfully flat descent.)  F and G can both be further restricted to the punctured neighborhood D0, and since both restrictions are ultimately derived from M, they are isomorphic: we have an isomorphism\n\nNow consider the converse situation: we have a ring A and an element f, and two modules: an Af-module F and an Â-module G, together with an isomorphism φ as above.  Geometrically, we are given a scheme X and both an open set Df and a \"small\" neighborhood D of its closed complement (f); on Df and D we are given two sheaves which agree on the intersection D0 = Df ∩ D.  If D were an open set in the Zariski topology we could glue the sheaves; the content of the Beauville–Laszlo theorem is that, under one technical assumption on f, the same is true for the infinitesimal neighborhood D as well.\n\nTheorem: Given A, f, F, G, and φ as above, if G has no f-torsion, then there exist an A-module M and isomorphisms\n\nconsistent with the isomorphism φ: φ is equal to the composition\n\nThe technical condition that G has no f-torsion is referred to by the authors as \"f-regularity\".  In fact, one can state a stronger version of this theorem.  Let M(A) be the category of A-modules (whose morphisms are A-module homomorphisms) and let Mf(A) be the full subcategory of f-regular modules.  In this notation, we obtain a commutative diagram of categories (note Mf(Af) = M(Af)):\n\nin which the arrows are the base-change maps; for example, the top horizontal arrow acts on objects by M → M ⊗A Â.\n\nTheorem: The above diagram is a cartesian diagram of categories.\n\nGlobal version\nIn geometric language, the Beauville–Laszlo theorem allows one to glue sheaves on a one-dimensional affine scheme over an infinitesimal neighborhood of a point.  Since sheaves have a \"local character\" and since any scheme is locally affine, the theorem admits a global statement of the same nature.  The version of this statement that the authors found noteworthy concerns vector bundles:\n\nTheorem: Let X be an algebraic curve over a field k, x a k-rational smooth point on X with infinitesimal neighborhood D = Spec k[[t]], R a k-algebra, and r a positive integer.  Then the category Vectr(XR) of rank-r vector bundles on the curve XR = X ×Spec k Spec R fits into a cartesian diagram:\n\nThis entails a corollary stated in the paper:\n\nCorollary: With the same setup, denote by Triv(XR) the set of triples (E, τ, σ), where E is a vector bundle on XR, τ is a trivialization of E over (X \\ x)R (i.e., an isomorphism with the trivial bundle O(X - x)R), and σ a trivialization over DR.  Then the maps in the above diagram furnish a bijection between Triv(XR) and GLr(R((t))) (where R((t)) is the formal Laurent series ring).\n\nThe corollary follows from the theorem in that the triple is associated with the unique matrix which, viewed as a \"transition function\" over D0R between the trivial bundles over (X \\ x)R and over DR, allows gluing them to form E, with the natural trivializations of the glued bundle then being identified with σ and τ.  The importance of this corollary is that it shows that the affine Grassmannian may be formed either from the data of bundles over an infinitesimal disk, or bundles on an entire algebraic curve.\n\nReferences\n \n\nCategory:Vector bundles\nCategory:Module theory\nCategory:Theorems in algebraic geometry\nCategory:Theorems in abstract algebra"
    },
    {
      "title": "Brauer–Nesbitt theorem",
      "url": "https://en.wikipedia.org/wiki/Brauer%E2%80%93Nesbitt_theorem",
      "text": "In mathematics, the Brauer–Nesbitt theorem can refer to  several different theorems proved by Richard Brauer and Cecil J. Nesbitt in the representation theory of finite groups.\n\nIn modular representation theory,\nthe Brauer–Nesbitt theorem on blocks of defect zero states that a character whose order is divisible by the highest power of a prime p dividing the order of a finite group remains irreducible when reduced mod p and vanishes on all elements whose order is divisible by p. Moreover, it belongs to a block of defect zero. A block of defect zero contains only one ordinary character and only one modular character.\n\nAnother version states that if k is a field of characteristic zero, A is a k-algebra, V, W are semisimple A-modules which are finite dimensional over k, and TrV = TrW as elements of Homk(A,k), then V and W are isomorphic as A-modules.\n\nReferences\nCurtis, Reiner, Representation theory of finite groups and associative algebras, Wiley 1962. \nBrauer, R.; Nesbitt, C. On the modular characters of groups. Ann. of Math. (2) 42, (1941). 556-590.\n\nCategory:Representation theory of finite groups\nCategory:Theorems in abstract algebra"
    }
  ]
}