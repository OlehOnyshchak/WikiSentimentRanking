{
  "pages": [
    {
      "title": "Range Software",
      "url": "https://en.wikipedia.org/wiki/Range_Software",
      "text": "{{Infobox software\n| name = Range Software\n| logo = Range-logo-128.png\n| screenshot = Range3-CFD.png\n| developer = [[Tomáš Šoltys]]\n| latest release version = 3.1\n| latest release date = 2018\n| operating_system = [[Windows]], [[Linux]]\n| genre = [[Finite element analysis]], [[3D computer graphics|3D Modeling]], [[Computer-aided design|CAD]]\n| license = [[GPL]]v3\n| website = {{URL|range-software.com}}\n}}\n\n'''Range Software''' is [[finite element analysis]] software package.\n\n== Analysis types ==\n\n* Steady-state and transient [[heat transfer]] + [[thermal radiation]]\n* [[Stress analysis]]\n* [[Soft body dynamics]]\n* [[Modal analysis]]\n* [[Computational fluid dynamics]] + [[heat transfer]], contaminant dispersion\n* [[Electrostatics]]\n* [[Magnetostatics]]\n* Coupled [[multiphysics]]\n* [[Shape optimization]]\n\n== External links ==\n* {{Official website|range-software.com}}\n\n[[Category:Computer-aided engineering software]]\n[[Category:Engineering software companies]]\n[[Category:Finite element software]]\n[[Category:Numerical software]]\n[[Category:Science software]]\n[[Category:Scientific simulation software]]\n[[Category:Simulation software]]\n[[Category:Specific models]]\n[[Category:Finite element software for Linux]]\n\n{{Simulation-software-stub}}"
    },
    {
      "title": "RFEM",
      "url": "https://en.wikipedia.org/wiki/RFEM",
      "text": "{{infobox software\n| name                    = RFEM\n| logo                    = \n| screenshot              = Screenshot from RFEM.png\n| caption                 = Visualized deformations of an office block in RFEM \n| collapsible             = \n| author                  = \n| developer               = {{URL|www.dlubal.com|Dlubal Software}}\n| released                = \n| discontinued            = \n| latest release version  = 5.xx\n| latest release date     = \n| latest preview version  = \n| latest preview date     = <!-- {{Start date and age|YYYY|MM|DD}} -->\n| operating system        = [[Microsoft Windows|Windows]]\n| genre                   = [[Computer-aided design|CAD]]/[[Computer-aided engineering|CAE]]/[[Finite element method|FEM]]\n| website                 = {{URL|dlubal.com/en/products/rfem-fea-software}}\n}}\n'''RFEM''' is a [[Three-dimensional space|3D]] [[finite element analysis]] software working under [[Microsoft Windows]] computer [[operating system]]s. RFEM can be used for [[structural analysis]] and [[Structural design|design]] of [[steel]], [[concrete]], [[timber]], [[glass]], [[membrane]] and [[tensile]] structures as well as for [[Physical plant|plant]] and [[mechanical engineering]] or [[Seismic analysis|dynamic analysis]].\n\nRFEM is used by more than 6,000 companies, 25,000 users <ref>{{cite web|url=https://www.dlubal.com/en/company/about-us/history-and-facts |title=Company in Facts and Figures - Dlubal Software |publisher=dlubal.com |date=2016-03-18 |accessdate=2016-04-20}}</ref> and many universities<ref>{{cite web|url=https://www.dlubal.com/en/education/schools/reference-list-of-schools-and-universities |title=Schools Using Dlubal Software for Structural Engineering - Dlubal Software |publisher=dlubal.com |date=2013-09-30 |accessdate=2013-10-09}}</ref> in 71 countries.\n\n== BIM Integration ==\nRFEM offers numerous [[Interface_(computing)#Software_interfaces|interfaces]] for data exchange within the [[BIM]] process. All relevant building data is digitally maintained within a three-dimensional model, which then is used throughout all of the planning stages. As a result, the various CAD and structural analysis programs are using the same model, which is directly transferred between the programs.\n\nBesides direct interfaces to [[AutoCAD|Autodesk AutoCAD]], [[Autodesk Revit|Autodesk Revit Structure]], Autodesk Structural Detailing, [[Bentley Systems]] applications (ISM) and [[Tekla Structures]], RFEM has interfaces for [[Industry Foundation Classes]], CIS/2 and others.\n\n== Materials and cross-section libraries ==\nRFEM's library of materials includes various types of [[concrete]], [[metal]], [[timber]], [[glass]], [[foil (metal)|foil]], [[gas]] and [[soil]].\n\nRFEM's [[Cross section (geometry)|cross-section]] library includes rolled, built-up, thin-walled, and thick-walled cross-sections for steel, concrete, and timber.\n\n== References ==\n{{reflist}}\n\n[[Category:Building information modeling]]\n[[Category:Computer-aided design software]]\n[[Category:Finite element software]]\n[[Category:Computer-aided design software for Windows]]"
    },
    {
      "title": "Safehull",
      "url": "https://en.wikipedia.org/wiki/Safehull",
      "text": "{{Multiple issues|\n{{orphan|date=February 2009}}\n{{no footnotes|date=April 2008}}\n}}\n\n'''Safehull''' is a software package developed by the [[American Bureau of Shipping|ABS]] in September 1993. It enables the design and evaluation of ship structures, thanks to [[finite element]] analysis tools. Possible analyses include:\n* [[Dynamic load]] determination\n* [[Fatigue analysis]]\n\n==Users==\nThis software is used by [[Harland and Wolff|Harland and Wolff Heavy Industries]] and Viking Systems (a ship structural design company), among others.\n\n==References==\n* [http://www.eagle.org/ ABS Homepage]\n* [http://www.eagle.org/prodserv/applications/safehull/SHTechIn.pdf ABS SafeHull Technical Information]\n* [https://web.archive.org/web/20080908102800/http://www.harland-wolff.com/facilities.asp Harland and Wolff Engineering Design Resources]\n* [http://www.viking-systems.net Viking Systems]\n\n[[Category:Finite element software]]"
    },
    {
      "title": "SDC Verifier",
      "url": "https://en.wikipedia.org/wiki/SDC_Verifier",
      "text": "{{Infobox Software\n| logo                   = SDC_Verifier_logo.svg\n| logo caption           = \n| screenshot             =\n| name = '''SDC Verifier'''\n|developer = SDC Veifier\n| latest release version = 5.1\n|operating_system = [[Microsoft Windows|Windows]]\n|genre = [[Computer-aided engineering]], [[Finite Element Analysis]]\n|website = {{URL|sdcverifier.com}}\n}}\n\n'''SDC Verifier''' ('''''S'''tructural '''D'''esign '''C'''odes Verifier'') is a commercial [[finite element analysis]] post-processor software with a calculation core for checking [[structural analysis|structures]] according to different standards, either predefined or self programmed, and final report generation with all checks. The goal is to automate routine work and speed up a verification of the engineering projects. It works as an addon for popular [[List of finite element software packages|FEA software]] [[Ansys]]<ref>{{cite web|url=https://www.infinite.nl/ansys/sdc-verifier/|title=SDC Verifier for ANSYS|website=Infinite - Simulation Systems}}</ref>, [[Femap]] and [[Siemens PLM Software|Simcenter 3D]]<ref>{{cite web|url=https://www.femto.eu/sdc-verifier/|title=SDC Verifier - Check out Purchase & Product Info!|publisher=}}</ref>.\n\nIt is possible to apply complicated [[Structural load|loads]]: buoyancy, tank ballast and wind. Automatic recognition of joints, welds and panels.\n\n\n==Implemented Standards==\nThe rules for popular design standards are predefined in SDC Verifier<ref>{{cite web|url=http://sdcverifier.com/software/faq/implemented-standards/|title=Implemented standards - SDC Verifier|publisher=}}</ref>. The open structure of the standard makes all checks customizable. The Custom standard can be saved and used for other models, password protected and added to the custom library. This standard can be shared between other users.\n\n*[[American Bureau of Shipping|ABS]] 2004: Guide for buckling and ultimate strength assessment for offshore structures;\n*[[American Bureau of Shipping|ABS]] 2014: Rules for building and classing (floating production installations);\n*[[American Institute of Steel Construction|AISC]] ASD 9th edition (July 1989);\n*[[American Institute of Steel Construction|AISC]] 360-10, 14th edition (2010);\n*API RP 2A LRFD, 1st edition (1993);\n*API RP 2A WSD 21st edition (2007);\n*DIN 15018 (1984);\n*[[DNV GL|DNV]] OS C101 LRFD (April 2011)\n*[[DNV GL|DNV]] OS C201 WSD (April 2011)\n*[[DNV GL|DNV]] Classification Notes NO. 30.1 ([[Buckling]] Strength Analysis, July 1995);\n*[[DNV GL|DNV]] RP C201: Buckling Strength of Plated Structures (Recommended Practice, October 2010);\n*FEM 1.001, 3rd edition (1998);\n*[[Eurocode 3: Design of steel structures|Eurocode 3]], Part 1-9: [[Fatigue (material)|Fatigue]] (2006);\n*[[Eurocode 3: Design of steel structures|Eurocode 3]], Part 1-1: Member checks (2005);\n*[[Eurocode 3: Design of steel structures|Eurocode 3]], Part 1-8: Weld Strength;\n*ISO 19902, 1th edition (2007);\n*Norsok N004, Rev. 3 (2013);\n==Alternative software==\n*GENIE\n*Midas\n*nCode\n*SACS\n*Skyciv\n\n==References==\n{{Reflist}}\n\n==External links==\n*[https://www.femto.eu/sdc-verifier/ Femap and SDC Verifier]\n*[https://www.plm.automation.siemens.com/en/products/femap/applications/sdc-verifier.shtml Siemens and SDC Verifier]\n*[http://www.slideshare.net/SDC_Verifier/sdc-verifier-mainpresentation SDC Verifier Presentation]\n*[http://sdcverifier.com/software/sdc-verifier/ SDC Verifier new version]\n\n[[Category:Computer-aided engineering software]]\n[[Category:Finite element software]]\n[[Category:Product lifecycle management]]\n[[Category:Siemens software products]]"
    },
    {
      "title": "SESAM (FEM)",
      "url": "https://en.wikipedia.org/wiki/SESAM_%28FEM%29",
      "text": "'''Sesam''' is a software suite for structural and hydrodynamic analysis of ships and offshore structures.<ref name=\"Impact\">{{cite book |title=Impact - 50 years of DNV research and innovation |url= |location= |publisher=Det Norske Veritas, DNV Research |page=150 |date=2005 |isbn=82 515 0303 5 }}</ref> It is based on the displacement formulation of the [[Finite Element Method]].\n\nThe first version of Sesam was developed at NTH, now [[Norwegian University of Science and Technology|Norges Teknisk-Naturvitenskapelige Universitet]] (''NTNU Trondheim''), in the mid-1960s.<ref name=\"BuildingTrust\">{{cite book |author1=Gard Paulsen |author2=Håkon With Andersen |author3=John Petter Collett |author4=Iver Tangen Stensrud |title=Building Trust, The history of DNV 1864-2014 |url= |location=Lysaker, Norway |publisher=Dinamo Forlag A/S |pages=121, 436 |date=2014 |isbn=978-82-8071-256-1 }}</ref> Sesam was bought by Det Norske Veritas, now [[DNV GL]], in 1968 and commercialized under the name SESAM-69 in 1970. Sesam was thus one of the first major structural analysis tools based on the [[Finite Element Method]] available and when it came to capability of analysing large and complex structures it outclassed all.<ref name=\"BuildingTrust\" /> In the beginning it was used for analysis of ships, in particular oil tankers (for which a comparison of analysis results with measurements on the real ship was made to confirm the accuracy of the method and tool <ref>{{cite journal |last=Araldsen |first=Per O. |date=January 1974 |title=Computers & Structures, Volume 4, Issue 1: An example of large-scale structural analysis. Comparison between finite element calculation and full scale measurements on the oil tanker Esso Norway |journal=Computers & Structures |volume=4 |url=http://www.sciencedirect.com/science/article/pii/0045794974900777 |pages=69–93 |isbn= |access-date=30 June 2015 |doi=10.1016/0045-7949(74)90077-7 }}</ref>) and liquefied natural gas ([[LNG]]) carriers.<ref>{{cite book |date=2004 |title=Moss Rosenberg Verfts LNG-skip, Andre gass- og kjemikalietankere |trans-title=LNG Carriers of Moss Rosenberg Verft, Other Gas- and Chemical Tankers |url=http://www.museumstavanger.no/Portals/48/Biblioteket/Aarbokartikler%202000-2010/2003%20Moss%20Rosenberg%20Verfts%20LNG-skip.pdf |language=Norwegian |location= |publisher=Stavanger Museums Årbok, Årg. 113 (2003), s. 99-120, 2004 |isbn= |access-date=30 June 2015 }}</ref>\n\nWith the development of offshore oil fields in the North Sea in the 1970s the use of Sesam for [[Fixed platform|fixed offshore platforms]] grew. Examples of such use are the Ekofisk concrete tank of the [[Ekofisk]] oil field, the [[Condeep]] [[Offshore concrete structure|concrete gravity base structures]] and the Kvitebjørn jacket in the North Sea.\n\nIn the late 1970s development of a completely new version of Sesam started.<ref>{{cite book |last=Means |first=Kaia |date=2009 |title=Celebrating Sesam's first 40 years |url=http://www.dnv.com/services/software/publications/2009/no_1/celebratingsesam40.asp |publisher=DNV GL |access-date=2 July 2015 }}</ref> This version was released in the mid-1980s under the name SESAM'80 and is the basis for today's Sesam. During the 1990s Sesam was further enhanced with a high-level concept modelling technique together with a design-oriented and unified user interface. Analysis features for mooring systems and flexible risers were also added. The software name was at the same time simplified to merely \"Sesam\".\n\nThe development of the recent years with frequent new releases is focused on improving Sesam as a tool for all phases of offshore and maritime structures from design, through transportation, installation, operation and modification to life extension, requalification and finally decommissioning.\n\nSesam consists of several modules of which the most important are:\n\n'''GeniE''' for modelling, analysis and code checking of beam, plate and shell structures like offshore platforms and ships.\n\n'''HydroD''' for hydrodynamic and hydrostatic analysis of fixed and floating structures like offshore platforms and ships.\n\n'''Sima''' for simulation of marine operations like lifting and lowering large objects in a marine environment.\n\n'''DeepC''' for mooring and riser design as well marine operations of offshore floating structures.\n\nSesam is developed in Norway by [[DNV GL]] with focus on solution of structural and hydrodynamic engineering problems within the offshore and maritime industries. It has been used by the offshore and maritime industries world-wide for more than 45 years.\n\n== References ==\n{{reflist}}\n\n==External links==\n* {{Official website|www.dnvgl.com/software/index.html}}\n* [http://www.dnv.com/services/software/publications/2009/no_1/celebratingsesam40.asp Celebrating Sesam's first 40 years] - A brief view on the history of Sesam from 1969 to 2009.\n\n[[Category:Structural engineering]]\n[[Category:Structural analysis]]\n[[Category:Computer-aided engineering software]]\n[[Category:Finite element software]]\n[[Category:Fracture mechanics]]\n[[Category:Earthquake engineering]]\n[[Category:Fluid dynamics]]\n[[Category:Hydrostatics]]"
    },
    {
      "title": "SimScale",
      "url": "https://en.wikipedia.org/wiki/SimScale",
      "text": "{{Infobox software\n| name = SimScale\n| logo = SimScale official logo.png\n| logo alt = \n| caption = \n| collapsible = \n| author = \n| developer = SimScale GmbH\n| released = {{Start date and age|2013}}\n| discontinued = \n| latest release version = \n| latest preview version = \n| programming language = \n| operating system = \n| platform = Web browser\n| size = \n| language = \n| language count = <!-- Number only -->\n| language footnote = \n| genre = [[Computer-aided engineering]]\n| license = \n| alexa = \n| website = {{url|simscale.com}}\n| standard = \n| AsOf = \n}}\n\n'''SimScale''' is a [[computer-aided engineering]] (CAE) software product based on [[cloud computing]]. SimScale was developed by SimScale GmbH and allows [[Computational fluid dynamics|Computational Fluid Dynamics]], [[Finite Element Analysis]] and [[Thermodynamics|Thermal]] simulations.<ref name=Enggarticle01>{{cite web|url=http://www.engineering.com/DesignSoftware/DesignSoftwareArticles/ArticleID/11121/SimScale-Brings-the-Price-of-Computer-Aided-Engineering-Down-to-Zero.aspx|title=SimScale Brings the Price of Computer-Aided Engineering Down to Zero|publisher=engineering.com|date=9 December 2015|last=Wasserman|first=Shawn}}</ref><ref>{{cite web|url=http://www.engineering.com/DesignSoftware/DesignSoftwareArticles/ArticleID/12409/Be-Warned-The-CAE-World-Is-About-to-Shift.aspx|title=Be Warned: The CAE World Is About to Shift|publisher=engineering.com|date=16 June 2016|last=Tara|first=Roopinder}}</ref> The backend of the platform uses [[Open-source software|open source]] codes:\n* [[Finite element method|FEA]]: [[Code_Aster]] and [[Calculix|CalculiX]]\n* [[Computational fluid dynamics|CFD]]:  [[OpenFOAM]]\nThe cloud-based platform of SimScale allows users to run more simulations, and in turn iterate more design changes, compared to traditional local computer-based systems.<ref name=\"digitaleng.news\">{{Cite web|url=http://www.digitaleng.news/de/enhancing-cycling-performance-via-simulation/|title=Enhancing Cycling Performance via Simulation|last=|first=|date=April 2016|website=|access-date=}}</ref>\n\n== History ==\nSimScale GmbH was founded in 2012 by five graduates of [[TU Munich]], David Heiny, Vincenz Dölle, Alexander Fischer, Johannes Probst, and Anatol Dammer<ref>{{cite web|url=https://www.crunchbase.com/organization/simscale#/entity|title=SimScale|publisher=CrunchBase}}</ref> with the goal of bringing CAE solutions to the mass market and provide an alternative to the traditional on-premises solutions which were the industry standard at the time. After a beta phase, the SimScale platform was launched in the second half of 2013.<ref>{{cite web|url=http://www.engineering.com/DesignSoftware/DesignSoftwareArticles/ArticleID/6214/Cloud-Based-Simulation.aspx/|title=Cloud-Based Simulation|last=Schmitz|first=Barb|date=26 August 2013|publisher=engineering.com}}</ref>{{verification failed|date=October 2018}}\n\nOn 2 December 2015, a community plan was announced making the platform accessible for free,<ref>{{cite web|url=https://www.nafems.org/about/media/news/industrynews1003/simscale_announces_free_access_to_simulation_technology_as_part_of_its_new_community_plan/|title=SimScale announces free access to simulation technology as a part of its new community plan|date=2 December 2015|publisher=NAFEMS|type=Press release}}</ref><ref>{{cite web|url=https://www.heise.de/make/meldung/Mit-SimScale-und-Make-gratis-simulieren-lernen-wie-die-Profis-3175633.html/|title=Mit SimScale und Make gratis simulieren lernen wie die Profis|last=König|first=Peter|date=15 April 2016|publisher=MAKE|type=Press release |archive-url=https://web.archive.org/web/20171005232739/https://www.heise.de/make/meldung/Mit-SimScale-und-Make-gratis-simulieren-lernen-wie-die-Profis-3175633.html|archive-date=5 October 2017|df=}}</ref> as part of the initiative to democratize [[Computer-aided engineering|CAE]] and expand their user base of professional engineers and CAE experts to include small and medium scale enterprises, as well as students and individual product designers.<ref>{{cite web|url=http://www.manufacturingglobal.com/technology/593/SimScale-to-bring-simulation-technology-to-small-and-medium-businesses|title=SimScale to bring simulation technology to small and medium businesses|date=8 December 2015|publisher=Global Manufacturing}}</ref><ref>{{cite web|url=http://www.engineering.com/DesignSoftware/DesignSoftwareArticles/ArticleID/10045/Is-Cloud-Based-Simulation-Affordable-Enough-to-Dominate-the-Start-Up-Market.aspx|title=Is Cloud-Based Simulation Affordable Enough to Dominate the Start-Up Market?|last=Wasserman|first=Shawn|date=30 April 2015|publisher=Engineering.com}}</ref>\n\n== Features ==\n\n=== Finite element analysis module ===\nThe FEA module of SimScale uses the open-source codes / solvers Code_Aster and [[Calculix|CalculiX]]. These codes allow [[Linearity|linear]] and nonlinear [[Static analysis|static]]/[[Dynamic scoring|dynamic analysis]] of structures. Code_Aster is used simulations involving [[Fatigue (material)|fatigue]], damage, [[Fracture mechanics|fracture]], [[Contact mechanics|contact]], geomaterials, [[Porous medium|porous media]], multi-physics coupling and more. [[Calculix|CalculiX]] has similar functionalities allowing users to build, calculate and post-process [[Finite element model data post-processing|finite element models]].\n\n=== Computational fluid dynamics module ===\nThe CFD module of SimScale uses [[OpenFOAM]] for [[Fluid dynamics|fluid flow]] simulations. Both [[steady state]] and [[Transient state|transient analysis]] for the below types are possible. The following analysis types are possible in SimScale.\n\n=== Thermal module ===\nThe Thermal module of SimScale uses [[OpenFOAM]] for solid-solid and fluid-solid thermal interaction problems. For thermo-structural analysis, SimScale uses Code_Aster and [[Calculix|CalculiX]]. At present, SimScale allows uncoupled [[Thermomechanical analysis|thermo-mechanical]] simulations,<ref>{{cite web|url=http://www.engineering.com/DesignSoftware/DesignSoftwareArticles/ArticleID/11332/Transient-Heating-and-Thermal-Shock-Analysis-for-Free.aspx/|title=Transient Heating and Thermal Shock Analysis for Free|publisher=engineering.com|date=19 January 2015|last=Wasserman|first=Shawn}}</ref> conjugate [[heat transfer]]<ref>{{cite web|url=http://www.engineering.com/DesignSoftware/DesignSoftwareArticles/ArticleID/12159/Freemium-Simulation-Software-Now-Includes-Conjugate-Heat-Transfer.aspx|title=Freemium Simulation Software Now Includes Conjugate Heat Transfer|publisher=engineering.com|date=19 May 2016|last=Wasserman|first=Shawn}}</ref> and convective heat transfer analysis. Both [[Steady state|steady-state]] and [[Transient state|transient]] simulations are possible. In addition, fluid simulations also allow usage of [[Turbulence]] models. Types of analysis possible using SimScale include:\n\nConjugate [[heat transfer]] (CHT), simulates the thermal energy transfer between a solid and a fluid, was added most recently to SimScale physics portfolio. It is most commonly used in the design of heat exchangers, heaters, coolers, electronic components and other heat sources.<ref>{{Cite web|url=http://www.engineering.com/DesignSoftware/DesignSoftwareArticles/ArticleID/12159/Freemium-Simulation-Software-Now-Includes-Conjugate-Heat-Transfer.aspx|title=Freemium Simulation Software Now Includes Conjugate Heat Transfer|last=|first=|date=May 2016|website=|access-date=}}</ref>\n\n== File format ==\nSimScale allows import of geometry in [[ISO 10303-21|STEP]], [[IGES]], [[BREP]], [[Rhinoceros 3D]], [[Autodesk Inventor]], [[SolidWorks]], [[Parasolid]], [[ACIS]] and [[STL (file format)|STL]] formats; mesh in [[OpenFOAM]], UNV, EGC, MED, [[CGNS]] formats. In addition, the geometry can be directly imported from their partner CAD platform, namely [[Onshape]].\n\n== Industrial applications ==\nJapan-based Tokyowheel — a company that engineers technical carbon fiber racing wheels for competitive cyclists — used SimScale's [[Computational fluid dynamics|CFD]] software component to determine the most aerodynamic wheel profile.<ref name=\"digitaleng.news\"/> QRC Technologies performed [[Thermal analysis|thermal simulations]] on SimScale to test multiple variations of their RF tester.<ref>{{Cite web|url=http://www.engineering.com/DesignSoftware/DesignSoftwareArticles/ArticleID/14299/Simulation-Experts-Save-Electronics-from-Thermal-Damage.aspx|title=Simulation Experts Save Electronics from Thermal Damage|last=|first=|date=February 2017|website=|access-date=}}</ref>\n\n== SimScale community ==\nThe SimScale Community Plan was announced on 2 December 2015 based on new investment round led by [[Union Square Ventures]] (USV).<ref>{{cite web|url= http://tech.eu/brief/usj-invests-in-simscale|title= Union Square Ventures invests in Munich-based startup SimScale|date=2 December 2015|website=Tech.eu}}</ref> The Community Plan is free and includes 3000 computation hours and 500&nbsp;GB of storage per year for any registered user. Simulations/Projects created by a user registered under the \"Community plan\" are accessible to all other users within the SimScale public project library].<ref name=\"Enggarticle01\" />´\n\n== SimScale outreach program ==\nSimScale has also organized several free webinars as a part of its outreach program to make simulation technologies more popular among hobbyists and designers. Webinars organized by SimScale include:\n* 3D Printer Workshop<ref>{{cite web|url=https://3dprint.com/119142/simscale-3d-printing-workshops/|title=SimScale Offers Three Workshops to Teach 3D Printing|date=11 February 2016|publisher=3Dprint.com}}</ref>\n* F1 Aerodynamics Workshop<ref>{{cite web|url=http://insidehpc.com/2016/03/f1-aerodynamics-workshop-organized-by-simscale|title=SimScale Offers Online F1 Aerodynamics Workshop|date=11 March 2016|publisher=Inside HPC}}</ref>\n* Simulation in Biomedical Engineering Workshop<ref>{{cite web|url=http://www.engineering.com/Education/EducationArticles/ArticleID/12942/SimScale-Offers-Training-on-Using-Simulation-in-Biomedical-Engineering.aspx|title=SimScale Offers Training on Using Simulation in Biomedical Engineering|date=19 August 2016|publisher=Engineering.com}}</ref>\n\n== References ==\n{{reflist}}\n\n[[Category:Cloud platforms]]\n[[Category:Computer-aided engineering software for Linux]]\n[[Category:Finite element software]]\n[[Category:Simulation software]]"
    },
    {
      "title": "STRAND7",
      "url": "https://en.wikipedia.org/wiki/STRAND7",
      "text": "{{Orphan|date=December 2012}}\n\n{{Infobox Software |\n name = Strand7 |\n screenshot = <!-- Image with unknown copyright status removed: [[Image:Runnertop.jpg|400px]]  -->|\n caption = A Strand7 model of the 'runner' structure placed on Sydney Tower during the 2000 Olympics|\n developer = [[Strand7 Pty. Ltd.]] |\n latest_release_version = 2.4.6 |\n latest_release_date = 5-Jan-2015 |\n operating_system = [[Microsoft Windows|Windows]] |\n genre = [[Finite Element Analysis]] Simulator |\n license = [[Proprietary software|Proprietary]] |\n website = [http://www.strand7.com] |\n}}\n\nStrand7 is a [[Finite Element Analysis| Finite Element Analysis (FEA)]] software product developed by the company with the same name.\n\n== History ==\n\nThe Strand computer software was first developed by a group of academics from the [[University of Sydney]] and the [[University of New South Wales]]. Further to this early research work, an independent company called G+D Computing was established in 1988 to develop an FEA program that could be used commercially for industrial applications. Between 1988 and 1996 the company researched, developed and marketed a series of DOS and Unix based FEA programs, most notably its STRAND6 program. In 1996 the company commenced work on a completely new software development specifically for the Windows platform.<ref>[http://www.strand7.com/html/specifications.htm 'Strand7 Specification']</ref> This product was first released in 2000 and was named Strand7.  In 2005 the company also changed its name to Strand7 to better reflect its primary focus.<ref>[http://www.strand7.com/pressrel/2005-01-07/rebrand.htm 'Strand7 Press Release'] {{webarchive|url=https://web.archive.org/web/20071018235843/http://strand7.com/pressrel/2005-01-07/rebrand.htm |date=2007-10-18 }}</ref>\n\n== Application ==\n\nSome high-profile applications of Strand7 include the optimisation of the \"Water Cube\" [[Beijing National Aquatics Center]] for the [[2008 Summer Olympics|2008 Beijing Olympics]],<ref>[http://www.aecmag.com/index.php?option=com_content&task=view&id=36&Itemid=37  'Water Cube' structure]</ref> the \"Runner\" sculpture that was placed on top of [[Sydney Tower]] during the [[2000 Sydney Olympics]]<ref>[http://www.strand7.com/html/olympicsculptures.htm Olympic Sculptures]</ref> and the Terminal 2E roof, [[Charles de Gaulle Airport]].\n\n== Analysis Capabilities ==\n\nStrand7 is most commonly used for the construction and mechanical engineering sectors, but also has seen use in other areas of engineering including aeronautical, marine and mining.\n\nStrand7 includes the following solvers:\n* Linear static \n* Natural frequency \n* Buckling \n* Nonlinear static \n* Linear and nonlinear transient dynamic \n* Spectral and harmonic response \n* Linear and nonlinear steady-state heat transfer \n* Linear and nonlinear transient heat transfer\n\n== External links ==\n*[http://www.strand7.com/ The Strand7 website]\n*[http://cad-reviews.com/20000305-strand7-review-1.html Review of Strand7 by CAD-Reviews in 2000]\n*[http://www.cogit-composites.com/ COGIT Composites : Advanced french user website]\n* http://www.et-global.com/index.php?option=com_content&view=category&layout=blog&id=4&Itemid=5\n\n==References==\n{{Reflist|2}}\n\n[[Category:Finite element software]]"
    },
    {
      "title": "StressCheck",
      "url": "https://en.wikipedia.org/wiki/StressCheck",
      "text": "{{Infobox_Software\n|name = StressCheck\n|screenshot = StressCheck Global to Local Detail Stress Analysis.png\n|caption = Global-local stress analysis including multi-body contact effects\n|developer = ESRD, Inc.\n|latest_release_version = V10.4\n|latest_release_date = {{Start date and age|2018|04}}\n|operating_system = [[Microsoft Windows|Windows 7, 8 and 10]]\n|genre = [[Computer aided engineering|Computer Aided Engineering (CAE)]] software\n|license =\n|website = {{URL|www.esrd.com}}\n}}\n{{notable|date=September 2018}}\n'''StressCheck''' is a [[finite element]] analysis software product developed and supported by [[ESRD, Inc.]] of [[St. Louis, Missouri]].  It is one of the first commercially available FEA products to utilize the [[P-FEM|p-version]] of the [[finite element method]] and support the requirements of [[Simulation Governance]].\n\n==History==\nDevelopment of StressCheck software began shortly after the founding in 1989 of ESRD, Inc. by Dr. [[Barna Szabó]], Dr. [[Ivo Babuška]], and Mr. [[Kent Myers]].  The principals have been performing research and development related to p-version finite element analysis for more than 20 years.  Close ties to the [[Washington_University_School_of_Engineering_and_Applied_Science|Washington University Center for Computational Mechanics]] facilitates incorporation of the latest research results into StressCheck.\n\n==Capabilities==\nStressCheck is a complete 3D finite element analysis tool with an integrated pre- and post-processor, a suite of analysis modules supporting advanced solutions in [[Elasticity_(physics)|elasticity]] and heat transfer, and utility modules that offer functionality to import CAD models and perform 2D and 3D automatic meshing.<ref>[https://www.esrd.com/stresscheck-professional \"StressCheck Professional Software\"]</ref>  Below is an abbreviated summary of current analysis modules and general capabilities.\n\n===Pre-Processing===\n* Fully [[Solid_modeling#Parametric_and_Feature_based_modeling|parametric]] modeling capability, including parameter- or formula-controlled:\n** Geometric dimensioning\n** Meshing parameters\n** Material properties\n** Boundary conditions (loads and constraints)\n*** [[Stress (mechanics)#Euler.E2.80.93Cauchy_stress_principle|Tractions]] can be directly applied to the model to meet the requirements of [[Elasticity_(physics)|elasticity]]\n*** Intrinsic capability to represent sinusoidal bearing loads\n*** Support for residual stress (RS) inputs (bulk residual stress or machining induced residual stress)\n*** Support for a wide variety of constraint conditions\n** Solution settings\n** Extraction settings\n* Geometric (blended) [[mapping (mathematics)|mapping]] capability for higher-order approximation of geometry<ref name=\"szabobook\">Barna Szabó and [[Ivo Babuška]], Finite element analysis, John Wiley & Sons, Inc., New York, 1991. {{ISBN|0-471-50273-1}}</ref>\n** This is critical for detailed stress analysis and composite modeling\n* [[Automatic meshing]] capability in 2D and 3D\n** Advanced boundary layer meshing and crack insertion for [[fracture mechanics]] problems\n* Handmeshing capability in 2D and 3D for improved [[discretization]]\n** H-discretization capability to automatically refine hand mesh\n* [[Global-local]] capability, i.e. importing structural nodal loads from a global model into a local StressCheck detail model\n** TLAP ('''T'''otal '''L'''oad '''A'''t a '''P'''oint) bearing and traction capability converts discrete point loads/moments into statically equivalent, smooth stress distributions\n* Advanced [[laminated]] composite modeling capability\n** Support for high aspect ratio elements (200:1 and larger) for representation of individual plies\n** Automatic lamination capability for [[discretization]] of a simple mesh to a ply-by-ply or [[homogenized]] representation\n** Ability to use geometry for laminate directions\n* [[Standardization]] via Handbook Solutions and Toolkit FEA\n** Each installation of StressCheck contains a library of pre-built [[Handbook]] models to encompass a variety of commonly solved engineering problems.\n** Users may generate organization-specific Handbook solutions for standardization purposes.\n\n===Finite Element Solver Features===\n* Linear [[Elasticity (physics)|Elasticity]], including multi-body contact analysis of [[metal]]lic and [[Composite material|composite]] structures\n* Nonlinear [[Elasticity (physics)|Elasticity]], including material (i.e. [[plasticity (physics)|plasticity]]) and geometric nonlinearities\n* [[Modal_analysis_using_FEM|Modal]]/[[Buckling]] analysis, including pre-stress buckling\n* Steady-State Conduction [[Heat Transfer]], including radiation and convection boundary conditions\n* [[64-bit]] [[Windows]] [[batch solver]] capability for solutions with extraordinary [[Degrees of freedom (physics and chemistry)|degrees of freedom]] (DOF)\n\n===Post-Processing===\n* Inherent [[verification and validation|verification]] capability for identifying and controlling [[discretization]] errors\n** Extract any FEA data of interest (i.e. [[Stress (mechanics)|stress]], [[Deformation (mechanics)|strain]], etc.) and [[convergence (mathematics)|convergence]] information for that data at any location in the model domain\n* Parameter and formula based post-processing\n* [[Fracture mechanics]] extractions, including [[Stress_intensity_factor|stress intensity factors]] and [[J integral]] computation of separated energy release rates (i.e. J1, J2, and J3) with or without residual stress effects\n* Ply by Ply extraction capability for [[laminated]] composite materials\n\n===Interfacing with External Tools===\n* [[Component_Object_Model|COM]] [[API]], allowing the ability to create or load models, solve them, and extract solution data using external programs such as [[AFGROW]], [[Microsoft Excel]], [[MATLAB]] and [[Visual Basic .NET]]\n** Allows for the development of custom applications\n** Optimization programs can interface with parametric modeling capability\n** Automation scripts can be written to update and solve multiple models\n\n==Technology==\nStressCheck uses the [[P-FEM|p-version]] of the [[finite element method]].  The utilization of the p-version in finite element analysis was pioneered by [[Dr. Barna Szabó]] during his tenure at [[Washington University in St. Louis]].  The [[P-FEM|p-version finite element method]] spans a space of high order polynomials by nodeless basis functions, chosen approximately orthogonal for numerical stability.  Since not all interior basis functions need to be present, the p-version finite element method can create a space that contains all polynomials up to a given degree with many fewer degrees of freedom.<ref name=\"szabobook\" />\n\nIn practice, the name p-version means that accuracy is increased by increasing the order of the approximating polynomials (thus, p) rather than decreasing the mesh size, h.  Thus, to check for solution convergence by increasing the number of degrees of freedom in a given model, the shape function polynomial level is increased rather than remeshing with more elements, which is the standard FEA tool method.  In StressCheck the maximum p-level is set to eight (8).\n\n==Application==\nStressCheck is used in a variety of industries, notably [[aerospace]], and for a range of applications such as aircraft [[damage tolerance]] assessment and analysis of [[composite materials]] for which high order elements are particularly useful.<ref>[https://www.youtube.com/watch?v=D6UenYtUd3Y \"Applications of advanced fracture mechanics utilizing StressCheck and AFGROW\"]</ref>\n\n==References==\n{{reflist}}\n;Notes\n* Barna Szabó and [[Ivo Babuška]], Introduction to Finite Element Analysis: Formulation, Verification and Validation, John Wiley & Sons, Inc., United Kingdom, 2011. {{ISBN|978-0-470-97728-6}}. http://www.wiley.com//legacy/wileychi/szabo/\n\n==See also==\n*[[p-FEM]]\n*[[hp-FEM]]\n* [[Spectral element method]]\n* [[List of finite element software packages]]\n\n{{CAE software}}\n{{DEFAULTSORT:Stresscheck}}\n[[Category:Finite element software]]"
    },
    {
      "title": "Wolfram Language",
      "url": "https://en.wikipedia.org/wiki/Wolfram_Language",
      "text": "{{Missing information|the programming language's syntax and examples|date=April 2017}}\n{{Infobox programming language\n| name                   = Wolfram Language\n| logo                   = Wolfram Language Logo 2016.svg\n| paradigm               = [[Programming paradigm#Multi-paradigm programming language|Multi-paradigm]]: [[term-rewriting]], [[Functional programming|functional]], [[Procedural programming|procedural]], [[Array programming|array]]\n| year                   = 1988\n| latest_test_date       =\n| typing                 = [[dynamic typing|Dynamic]], [[strong typing|strong]]\n| designer               = [[Stephen Wolfram]]\n| developer              = [[Wolfram Research]]\n| latest release version = 12.0<ref>{{cite web |url=https://blog.stephenwolfram.com/2019/04/version-12-launches-today-big-jump-for-wolfram-language-and-mathematica/ |title=Version 12 Launches Today! (And It’s a Big Jump for Wolfram Language and Mathematica) |last=Wolfram |first=Stephen |date=2018-03-08 |website=Wolfram Blog |publisher=Wolfram Research |access-date=2019-04-16 |archive-url=https://web.archive.org/web/20190416163117/https://blog.stephenwolfram.com/2019/04/version-12-launches-today-big-jump-for-wolfram-language-and-mathematica/ |archive-date=2019-04-16}}</ref>\n| latest release date    = {{start date and age|2019|04|16}}\n| implementations        = [[Wolfram Mathematica|Mathematica]], [http://www.wolfram.com/wolfram-one/ Wolfram&#124;One], [https://github.com/mathics/Mathics Mathics], [https://github.com/corywalker/expreduce Expreduce], [https://web.archive.org/web/20160119224638/http://www.cs.berkeley.edu/~fateman/lisp/mma4max/ MockMMA]\n| dialects               = \n| genre                  = [[Computer algebra system|Computer algebra]], [[List of numerical analysis software|numerical computations]], [[information visualization]], [[List of statistical packages|statistics]], [[Graphical user interface|user interface creation]]\n| influenced_by          = {{startflatlist}}\n*[[APL (programming language)|APL]]\n*[[C (programming language)|C]]\n*[[C++]]\n*[[FORTRAN]]\n*[[Lisp (programming language)|Lisp]]\n*[[Pascal (programming language)|Pascal]] \n*[[Prolog]]\n*[[Simula]] \n*[[Smalltalk]]<ref name=\"Maeder\">{{cite book |first=Roman E. |last=Maeder |title=The Mathematica® Programmer |publisher=Academic Press, Inc. |year=1994 |isbn=978-1-48321-415-3 |page=6}}</ref>\n*[[Symbolic Manipulation Program|SMP]]<ref name=\"Q&A\">{{cite web |url=https://www.wolfram.com/language/faq/  |title=Wolfram Language Q&A |publisher=Wolfram Research |accessdate=2016-12-05}}</ref>{{endflatlist}}\n| influenced             = [[Julia (programming language)|Julia]]<ref name=\"Julia\">{{cite web |url=http://julialang.org/blog/2012/02/why-we-created-julia |first1=Jeff |last1=Bezanson |first2=Stefan |last2=Karpinski |first3=Viral |last3=Shah |first4=Alan |last4=Edelman |title=Why We Created Julia  |publisher=Julia Language |date=2012-02-14 |accessdate=2016-12-01}}</ref>\n| operating_system       = [[Cross-platform]]\n| license                = [[Proprietary license|Proprietary]] (available at no-cost for some platforms)<ref>[http://bits.blogs.nytimes.com/2015/12/14/stephen-wolfram-seeks-to-democratize-his-software/?smid=tw-nytimesbits&smtyp=cur&_r=0 Stephen Wolfram Aims to Democratize His Software] by Steve Lohr, The New York Times, December 14, 2015</ref>\n| website                = {{URL|https://www.wolfram.com/language/}}\n| file_ext               = .nb, .m, .wl}}\n\nThe '''Wolfram Language''' is a general [[multi-paradigm programming language|multi-paradigm computational language]]<ref>{{cite web|url=http://www.wolfram.com/language/for-experts/ |title=Notes for Programming Language Experts about Wolfram Language |publisher=Wolfram.com |date= |accessdate=2015-11-05}}</ref> developed by [[Wolfram Research]] and is the programming language of the mathematical symbolic computation program [[Mathematica]]<ref name=\"25anv\">{{cite web|url=http://blog.wolfram.com/2013/06/23/celebrating-mathematicas-first-quarter-century/ |title=Celebrating Mathematica’s First Quarter Century—Wolfram Blog |publisher=Blog.wolfram.com |date= |accessdate=2015-11-05}}</ref> and the Wolfram Programming Cloud. It emphasizes [[symbolic computation]], [[functional programming]], and [[rule-based programming]]<ref name=\"LangName\">{{cite web|url=http://blog.stephenwolfram.com/2013/02/what-should-we-call-the-language-of-mathematica/ |title=What Should We Call the Language of Mathematica?—Stephen Wolfram Blog |publisher=Blog.stephenwolfram.com |date=2013-02-12 |accessdate=2015-11-05}}</ref> and can employ arbitrary [[data structure|structures]] and data.<ref name=\"LangName\"/>\n\nIt includes built-in functions for generating and running [[Turing machines]], creating graphics and audio, analyzing 3D models, [[Matrix (mathematics)|matrix]] manipulations, and solving [[differential equation]]s. It is extensively documented.<ref>{{cite web|url=http://reference.wolfram.com/language/ |title=Wolfram Language & System Documentation Center |publisher=Reference.wolfram.com |date= |accessdate=2015-11-05}}</ref>\n\nWolfram Language's core principles that differentiate it from other programming languages includes a built-in knowledgebase, automation in the form of meta-algorithms and [[superfunction|superfunctions]], a coherently elegant design and structure, built-in natural language understanding, and representation of everything as a symbolic expression.<ref>https://www.wolfram.com/language/principles/</ref>\n\nThe Wolfram language was released for the [[Raspberry Pi]] in 2013 with the goal of making it free for all Raspberry Pi users.<ref>{{cite web|url=http://blog.wolfram.com/2013/11/21/putting-the-wolfram-language-and-mathematica-on-every-raspberry-pi/ |title=Putting the Wolfram Language (and Mathematica) on Every Raspberry Pi—Wolfram Blog |publisher=Blog.wolfram.com |date= |accessdate=2015-11-05}}</ref> It was included in the recommended software bundle that the [[Raspberry Pi Foundation]] provides for beginners, which caused some controversy due to the Wolfram language's [[Non-free software|proprietary]] nature.<ref>{{cite web|last=Sherr |first=Ian |url=http://news.cnet.com/8301-1001_3-57613462-92/premium-mathematica-software-free-on-budget-raspberry-pi/ |title=Premium Mathematica software free on budget Raspberry Pi - CNET |publisher=News.cnet.com |date=2013-11-22 |accessdate=2015-11-05}}</ref><ref>{{cite web|url=https://www.gadgetdaily.xyz/raspberry-pi-gets-computer-based-mathematics-software/|title=Eben Upton comments on open source Pi concerns|first=Gavin|last=Thomas|publisher=Gadget Daily|year=2014|accessdate=2017-04-11}}</ref> Plans to port the Wolfram language to the [[Intel Edison]] were announced after the board's introduction at [[CES 2014]].<ref>{{cite web|author=Daniel AJ Sokolov |url=http://www.heise.de/newsticker/meldung/Intels-Edison-Pentium-System-im-Format-einer-SD-Karte-2076917.html |title=Intels Edison: Pentium-System im Format einer SD-Karte &#124; heise online |publisher=Heise.de |date=2014-11-22 |accessdate=2015-11-05}}</ref> In 2019, a link was added to make Wolfram libraries compatible with the [[Unity (game engine)|Unity]] game engine, giving game developers access to the language's high level functions.<ref>{{cite web|url=http://gamasutra.com/view/news/212709/The_Wolfram_Language_will_soon_be_integrated_into_Unity |title=The Wolfram Language will soon be integrated into Unity |publisher=Gamasutra |date=2014-03-10 |accessdate=2015-11-05}}</ref><ref>{{cite web|url=http://community.wolfram.com/groups/-/m/t/312155|title=Is there a way to use Wolfram Language in Unity3D?|publisher=Wolfram|year=2017|accessdate=2017-04-11}}</ref>\n\n==Naming==\nThe language was officially named in June 2013 although, as the programming language of Mathematica, it has been in use in various forms for over 30 years since Mathematica's initial release.<ref name=\"25anv\"/><ref>{{cite web|url=http://readwrite.com/2013/03/11/stephen-wolfram-has-an-algorithm-for-everything-literally#awesm=~oekpXL21gq1fST |title=Stephen Wolfram Says He Has An Algorithm For Everything — Literally |publisher=Readwrite.com |accessdate=2015-11-05}}</ref> Before 2013, it was internally referred to by several names, such as \"M\" and \"Wolfram Language.\" Other possible names Wolfram Research considered include \"Lingua\" and \"Express.\"<ref name=\"LangName\"/>\n\n==In popular culture==\nBoth [[Stephen Wolfram]] and his son Christopher Wolfram were involved in helping create the alien language for the film ''[[Arrival (film)|Arrival]]'', for which they used the Wolfram Language.<ref>[https://www.wired.com/2016/11/arrivals-designers-crafted-mesmerizing-alien-alphabet/ How Arrival's Designers Crafted a Mesmerizing Language], Margaret Rhodes,  Wired, November 16, 2016.</ref>  They were given portions of the written language, and used Wolfram Language to analyze the images and attempt to interpret them.  This served as the model for how the characters approached the problem in the film.\n\nBeginning in 2017, Wolfram began to [[Live streaming|Live stream]] internal Wolfram Language development meetings. During these meetings, viewers are encouraged to submit questions and comments related to the development of the [[programming language]]. Viewers have been known to suggest new functions that they would like to see developed, name new functions, and help solve complex issues faced by Stephen and the [[Wolfram Research]] development team. These live streamed meetings can be viewed on [[Twitch.tv]], [[YouTube|YouTube Live]], and [[Facebook|Facebook Live]].\n\n==See also==\n{{Portal|Computer programming|Mathematics}}\n* [[Stephen Wolfram]]\n* [[Wolfram Mathematica (software)|Wolfram Mathematica]]\n* [[Notebook interface]]\n* [[Wolfram Research]]\n* [[Wolfram Alpha]]\n* [[Wolfram Demonstrations Project]]\n\n==References==\n{{Reflist|30em}}\n\n==External links==\n* [http://reference.wolfram.com/language/ Documentation for the Wolfram Language]\n* [http://www.wolfram.com/language/elementary-introduction/ An Elementary Introduction to the Wolfram Language]\n* [http://wolframcloud.com The Wolfram Programming Cloud]\n* [http://www.wolframlanguage.org WolframLanguage.org]: a guide to community resources about Wolfram Language\n* [http://blog.stephenwolfram.com/2013/11/something-very-big-is-coming-our-most-important-technology-project-yet/ Something Very Big Is Coming: Our Most Important Technology Project Yet]: first announcement of the Wolfram Language in Stephen Wolfram's blog\n* [http://mathematica.stackexchange.com/a/4456/11992 A list of open-source implementations of the Wolfram language]\n\n{{Wolfram Research}}\n{{authority control}}\n\n[[Category:Array programming languages]]\n[[Category:Audio programming languages]]\n[[Category:Cloud platforms]]\n[[Category:Computational notebook]]\n[[Category:Computer algebra systems]]\n[[Category:Computer vision software]]\n[[Category:Concatenative programming languages]]\n[[Category:Constructed languages]]\n[[Category:Cross-platform software]]\n[[Category:Data mining and machine learning software]]\n[[Category:Data visualization software]]\n[[Category:Data-centric programming languages]]\n[[Category:Declarative programming languages]]\n[[Category:Dynamically typed programming languages]]\n[[Category:Educational programming languages]]\n[[Category:Finite element software]]\n[[Category:Formula editors]]\n[[Category:Formula manipulation languages]]\n[[Category:Functional languages]]\n[[Category:Functional programming]]\n[[Category:High-level programming languages]]\n[[Category:Homoiconic programming languages]]\n[[Category:Image processing software]]\n[[Category:Linear algebra]]\n[[Category:Literate programming]]\n[[Category:Multi-paradigm programming languages]]\n[[Category:Neural network software]]\n[[Category:Numerical linear algebra]]\n[[Category:Numerical programming languages]]\n[[Category:Object-oriented programming languages]]\n[[Category:Ontology languages]]\n[[Category:Parallel computing]]\n[[Category:Pattern matching programming languages]]\n[[Category:Programming languages created in 1988]]\n[[Category:Simulation programming languages]]\n[[Category:Social network analysis software]]\n[[Category:Software modeling language]]\n[[Category:SQL data access]]\n[[Category:Statistical programming languages]]\n[[Category:Technical analysis software]]\n[[Category:Term-rewriting programming languages]]\n[[Category:Theorem proving software systems]]\n[[Category:Wolfram Research]]"
    },
    {
      "title": "XFdtd",
      "url": "https://en.wikipedia.org/wiki/XFdtd",
      "text": "{{Multiple issues|{{refimprove|date=July 2015}}{{notability|date=July 2015}}}}\n\n'''XFdtd''' is [[electromagnetic]] simulation software with a very wide variety of applications in [[RF circuit]], antenna, military/defense, medical EM, [[photonics]], [[radar]], component, metamaterial, and related fields.  It originally stood for X (Window System) Finite Difference Time Domain and was first developed in the mid 1990s by Remcom Incorporated <ref>[http://www.remcom.com/xf7 Remcom Incorporated]</ref> of State College, PA in the United States.  XFdtd includes full wave ([[FDTD]]), [[electrostatic]], thermal-biological, circuit, and 2D Eigen solver and integrates with PO/MEC, and GTD/UTD method solvers.\n\n==See also==\n*[[Computational electromagnetics]]\n\n==References==\n{{Reflist}}\n\n[[Category:Finite element software]]\n[[Category:Electronic design automation software]]\n[[Category:Electromagnetic simulation software]]\n[http://www.microwavejournal.com/articles/24648-d-electromagnetic-simulation-vs-planar-mom 3D Electromagnetic Simulation vs. Planar MoM]\n\n{{engineering-stub}}"
    },
    {
      "title": "Agros2D",
      "url": "https://en.wikipedia.org/wiki/Agros2D",
      "text": "{{Infobox software\n| name                   = Agros2D\n| title                  = Agros2D\n| logo                   = Agros2D logo.png\n| logo caption           = \n| logo_size              = \n| logo_alt               = \n| screenshot             = Heat transfer in Agros2D.png\n| caption                = Heat transfer simulated by the Agros2D\n| screenshot_size        = \n| screenshot_alt         = \n| collapsible            = \n| author                 = \n| developer              = [[University of West Bohemia]]\n| released               = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| discontinued           = \n| latest release version = 3.2\n| latest release date    = {{Start date and age|2014|03|03}}\n| latest preview version = \n| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| status                 = \n| programming language   = \n| operating system       = [[Linux]], [[Microsoft Windows|Windows]]\n| platform               = \n| size                   = \n| language               = C++, Python\n| language count         = <!-- DO NOT include this parameter unless you know what it does -->\n| language footnote      = \n| genre                  = Scientific simulation software\n| license                = [[GNU General Public License]]\n| alexa                  = \n| website                = {{URL|www.agros2d.org}}\n| standard               = \n| AsOf                   = \n}}\n\n'''Agros2D''' is an open-source code for numerical solutions of 2D coupled problems in technical disciplines. Its principal part is a user interface serving for complete preprocessing and postprocessing of the tasks (it contains sophisticated tools for building geometrical models and input of data, [[Mesh generation|generators of meshes]], tables of [[Weak formulation|weak forms]] for the [[partial differential equation]]s and tools for evaluating results and drawing graphs and maps). The processor is based on the library [[Hermes Project|Hermes]] containing the most advanced numerical algorithms for monolithic and fully adaptive solution of systems of generally nonlinear and nonstationary partial differential equations (PDEs) based on [[hp-FEM]] (adaptive [[finite element method]] of higher order of accuracy). Both parts of the code are written in [[C++]].<ref name=\"karban2013\">Karban, P., Mach, F., Kůs, P., Pánek, D., Doležel, I.: Numerical solution of coupled problems using code Agros2D, Computing, 2013, Volume 95, Issue 1 Supplement, pp 381-408</ref>\n\n== Features ==\n* Coupled Fields - With coupled field feature you can blend two or more physical fields in one problem. Weak or hard coupling options are available.\n* [[Nonlinear system|Nonlinear Problems]] - Simulation and analysis of nonlinear problems are available. Agros2D now implements both Newton’s and Pickard’s methods.\n* Automatic space and time adaptivity - One of the main strengths of the Hermes library is an automatic space adaptivity algorithm. With Agros2D is also possible use adaptive time stepping for transient phenomena analysis. It can significantly improve solution speed without decreasing accuracy.\n* [[Curvilinear coordinates|Curvilinear Elements]] - Curvilinear elements is an effective feature for meshing curved geometries and leads to faster and more accurate calculations.\n* [[Types of mesh#Quadrilateral|Quadrilateral Meshing]] - Quadrilateral meshing can be very useful for some types of problem geometry such as compressible and incompressible flow.\n* [[Single particle tracking|Particle Tracing]]—Powerful environment for computing the trajectory of charged particles in electromagnetic field, including the drag force or their reflection on the boundaries.\n\n== Highlights of capabilities ==\n* Higher-order finite element method ([[Hp-FEM|''hp''-FEM]]) with ''h'', ''p'' and ''hp'' adaptivity based on reference solution and local projections\n* Time-adaptive capabilities for transient problems\n* Multimesh assembling over component-specific meshes without projections or interpolations in multi-physics problems\n* Parallelization on single machine using [[OpenMP]]\n* Large range of linear algebra libraries ([[MUMPS (software)|MUMPS]], [[UMFPACK]], PARALUTION, [[Trilinos]])\n* Support for scripting in [[Python (programming language)|Python]] (advanced IDE PythonLab)\n\n== Physical Fields ==\n* [[Electrostatics]]\n* [[Electric current]]s (steady state and harmonic)\n* [[Magnetic field]] (steady state, harmonic and transient)\n* [[Heat transfer]] (steady state and transient)\n* [[Structural mechanics]] and thermoelasticity\n* [[Acoustics]] (harmonic and transient)\n* [[Incompressible flow]] (steady state and transient)\n* [[Radio frequency|RF]] field (TE and TM vawes)\n* [[Richards equation]] (steady state and transient)\n\n=== Couplings ===\n* Current field as a source for heat transfer through Joule losses\n* Magnetic field as a source for heat transfer through Joule losses\n* Heat distribution as a source for thermoelastic field\n\n== History ==\nThe software started from work at the ''hp''-FEM Group at [[University of West Bohemia]] in 2009. The first public version was released at the beginning of year 2010. Agros2D has been used in many publications.<ref>Dolezel, I., Karban, P., Mach, F., & Ulrych, B. (2011, July). Advanced adaptive algorithms in finite element method of higher order of accuracy. In Nonlinear Dynamics and Synchronization (INDS) & 16th Int'l Symposium on Theoretical Electrical Engineering (ISTET), 2011 Joint 3rd Int'l Workshop on (pp. 1-4). IEEE.</ref><ref>Polcar, P. (2012, May). Magnetorheological brake design and experimental verification. In ELEKTRO, 2012 (pp. 448-451). IEEE.</ref><ref>Lev, J., Mayer, P., Prosek, V., & Wohlmuthova, M. (2012). The Mathematical Model of Experimental Sensor for Detecting of Plant Material Distribution on the Conveyor. Main Thematic Areas, 97.</ref><ref>Kotlan, V., Voracek, L., & Ulrych, B. (2013). Experimental calibration of numerical model of thermoelastic actuator. Computing, 95(1), 459-472.</ref><ref>Vlach, F., & Jelínek, P. (2014). Determination of linear thermal transmittance for curved detail. Advanced Materials Research, 899, 112-115.</ref><ref>Kyncl, J., Doubek, J., & Musálek, L. (2014). Modeling of Dielectric Heating within Lyophilization Process. Mathematical Problems in Engineering, 2014.</ref><ref>De, P. R., Mukhopadhyay, S., & Layek, G. C. (2012). Analysis of fluid flow and heat transfer over a symmetric porous wedge. Acta Technica CSAV, 57(3), 227-237.</ref>\n\n== See also ==\n* [[Hermes Project|Hermes]]\n* [[List of numerical analysis software]]\n* [[List of finite element software packages]]\n* [[hp-FEM#Open source hp-FEM codes|Open source hp-FEM codes]]\n\n== References ==\n{{reflist}}\n\n== External links ==\n*{{Official website|www.agros2d.org}}\n* [http://www.hpfem.org/ Group's website]\n\n[[Category:Numerical software]]\n[[Category:Scientific simulation software]]\n[[Category:Finite element software for Linux]]\n[[Category:Computer-aided engineering software for Linux]]\n[[Category:Engineering_software_that_uses_Qt]]\n[[Category:University of West Bohemia]]"
    },
    {
      "title": "Calculix",
      "url": "https://en.wikipedia.org/wiki/Calculix",
      "text": "{{Infobox software\n| name                   = CalculiX\n| title                  = \n| logo                   = \n| logo caption           = \n| screenshot             = lav.png\n| caption                = Compressor of a turbocharger\n| collapsible            = \n| author                 = Guido Dhondt, Klaus Wittig\n| developer              = \n| released               = <!-- {{Start date|YYYY|MM|DD|df=yes/no}} -->\n| discontinued           = \n| latest release version = 2.15\n| latest release date    = {{Start date and age|2018|12|15|df=yes/no}}\n| latest preview version = \n| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| programming language   = \n| operating system       = [[Linux]], [[Microsoft Windows|Windows]]\n| platform               = \n| size                   = \n| language               = \n| language count         = <!-- DO NOT include this parameter unless you know what it does -->\n| language footnote      = \n| status                 = \n| genre                  = [[Finite element analysis]]\n| license                = [[GNU General Public Licence|GPL]] ([[free software]])\n| alexa                  = \n| website                = {{URL|www.calculix.de}}\n}}\n'''CalculiX''' is a [[free and open-source]] [[finite-element analysis]] application that uses an input format similar to [[Abaqus]]. It has an implicit and explicit solver (CCX) written by Guido Dhondt and a pre- and post-processor (CGX) written by Klaus Wittig.<ref name=\"CalculiX website\">[http://www.calculix.de CalculiX website].</ref> The original software was written for the Linux<ref>[http://www.libremechanics.com/?q=node/9 How To Install CalculiX 2.6 multi-thread under Ubuntu 11.04 and later].</ref> operating system. [[Convergent Mechanical]] has ported the application to the Windows operating system.<ref name=\"Convergent Mechanical's website\">[http://www.bConverged.com Convergent Mechanical's website].</ref>\n\nThe pre-processor component of CalculiX can generate grid data for the [[computational fluid dynamics]] programs duns, ISAAC and [[OpenFOAM]].  It can also generate input data for the commercial FEM programs [[Nastran]], [[Ansys]] and [[Abaqus]].<ref>[http://imechanica.org/node/1628 CalculiX Review by iMechanica].</ref>  The pre-processor can also generate mesh data from STL files.\n<ref name=\"CGX Documentation\">[http://bconverged.com/calculix/doc/cgx/html/cgx.html CGX Documentation].</ref>\n\nThere is an active online community that provides support via a Yahoo! discussion group.<ref name=\"CalculiX Yahoo! Group\">[https://groups.yahoo.com/group/calculix/ CalculiX Yahoo! Group].</ref>  Convergent Mechanical also provides installation support for their extended version of CalculiX for Windows.<ref name=\"Convergent Mechanical's website\"/>\n\nThere is a friendly CalculiX Launcher <ref name=\"CalculiX Launcher sourceforge\">[https://sourceforge.net/projects/calculixforwin/ CalculiX Launcher (SourceForge)].</ref> with CCX wizard for both Windows and Linux.\n<ref name=\"CalculiX Launcher\">[http://calculixforwin.blogspot.mx/2015/05/calculix-launcher.html CalculiX Launcher].</ref>\n\nAlso possible is the Installation in Windows 10 Fall Creator (1709) with the new Linux Subsystem WSL.<ref>https://carlomonjaraztec.wordpress.com/2017/07/10/ccx_in_win10/</ref>\n\nThe CalculiX solver is available on the [[Sun Grid]].<ref>[http://blogs.sun.com/hardik/entry/new_application_on_sun_grid New application on Sun Grid: Calculix] {{Webarchive|url=https://web.archive.org/web/20070528020616/http://blogs.sun.com/hardik/entry/new_application_on_sun_grid |date=2007-05-28 }}, May 2007.</ref>\n\nA [[Python (programming language)|Python]] library, pycalculix,<ref name=\"pycalculix website\">[http://justinablack.com/pycalculix/ pycalculix website].</ref> was written to automate the creation of CalculiX models in the [[Python (programming language)|Python]] programming language. The library provides Python access to building, loading, meshing, solving, and querying CalculiX results for 2D models. Pycalculix was written by Justin Black. Examples and tutorials are available on the pycalculix site.<ref name=\"pycalculix website\"/>\n\nFreeCAD <ref name=\"FreeCAD website\">[https://freecadweb.org/ FreeCAD website].</ref> has developed a FEM workbench which automates the creation of Calculix models.\n\n== Literature ==\n* Guido Dhondt: ''\"The Finite Element Method for Three-Dimensional Thermomechanical Applications\"''. Wiley, Hoboken 2004, {{ISBN|0-470-85752-8}}\n* [http://bconverged.com/calculix/doc/ccx/html/ccx.html Current CCX documentation]\n* [http://bconverged.com/calculix/doc/cgx/html/cgx.html Current CGX documentation]\n* [http://www.bconverged.com/content/calculix/doc/GettingStarted.pdf Getting Started Guide]\n* [https://freecadweb.org/wiki/FEM_Module FreeCAD FEM workbench for CalCulix]\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* {{official website|www.calculix.de}}\n* [http://www.feacluster.com/calculix.php#1 Searchable online documentation]\n* [http://bConverged.com/calculix CalculiX for Windows]\n* [https://groups.yahoo.com/group/calculix/ CalculiX Discussion Group]\n* [http://justinablack.com/pycalculix/ pycalculix website]\n* [https://forum.freecadweb.org/viewtopic.php?f=18&t=12212 FreeCAD FEM workbench]\n* [http://tfel.sourceforge.net/calculix.html MFront code generator for CalculiX]\n\n{{CAE software}}\n\n[[Category:Computer-aided engineering software]]\n[[Category:Free computer-aided design software]]\n[[Category:Free simulation software]]\n[[Category:Finite element software for Linux]]"
    },
    {
      "title": "Deal.II",
      "url": "https://en.wikipedia.org/wiki/Deal.II",
      "text": "{{lowercase title}}\n{{Infobox software\n| name                   = deal.II\n| title                  = \n| logo                   = <!-- Image name is enough -->\n| logo caption           = \n| logo size              = \n| logo alt               = \n| screenshot             = <!-- Image name is enough -->\n| caption                = \n| screenshot size        = \n| screenshot alt         = \n| collapsible            = \n| author                 = \n| developer              = Wolfgang Bangerth, Timo Heister, Guido Kanschat, Matthias Maier et al.\n| released               = {{Start date and age|2000|df=yes}}\n| discontinued           = \n| latest release version = 9.1.1\n| latest release date    = {{Start date and age|2019|05|27|df=yes}}\n| latest preview version = \n| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| programming language   = [[C++]]\n| operating system       = [[Linux]], [[macOS]], [[Microsoft Windows]]\n| platform               = \n| language               = \n| language count         = <!-- Number only -->\n| language footnote      = \n| genre                  = [[Finite element analysis]]\n| license                = [[GNU Lesser General Public License]] 2.1 or later\n| alexa                  = \n| website                = {{URL|dealii.org}}\n| standard               = \n| AsOf                   = \n}}\n\n'''deal.II''' is a free, [[open-source software|open-source]] library to solve [[partial differential equation]]s using the [[finite element method]].<ref>{{cite journal|last=Bangerth|first=W|author2=Hartmann, R |author3=Kanschat, G. |title=deal.II - a general purpose object oriented finite element library|journal=ACM Trans. Math. Softw.|year=2007|volume=33}}</ref><ref>{{cite web |title=deal.II Homepage |url=http://www.dealii.org |work=deal.II Homepage |accessdate=5 August 2012 |deadurl=yes |archiveurl=https://web.archive.org/web/20120608055415/http://www.dealii.org/ |archivedate=8 June 2012 |df= }}</ref>&nbsp; The current release is version 9.1.1 released in May 2019. In 2007 the authors won the [[J. H. Wilkinson Prize for Numerical Software]] for deal.II.<ref>{{cite web|title=Developers of Finite Element Library Receive Wilkinson Prize for Numerical Software|url=http://www.siam.org/news/news.php?id=1241|accessdate=5 August 2012}}</ref>\n\n== Features ==\nThe library features\n* dimension independent programming using [[Template (C++)|C++ templates]] on [[Adaptive mesh refinement|locally adapted meshes]],\n* a large collection of different finite elements of any order: continuous and discontinuous Lagrange elements, Nedelec elements, Raviart-Thomas elements, and combinations,\n* parallelization using multithreading through [[Threading Building Blocks|TBB]] and massively parallel using [[Message Passing Interface|MPI]]. deal.II has been shown to scale to at least 16,000 processors<ref>{{cite journal|last=Bangerth|first=W.|author2=Burstedde, C. |author3=Heister, T. |author4= Kronbichler, M. |title=Algorithms and Data Structures for Massively Parallel Generic Finite Element Codes|journal=ACM Trans. Math. Softw.|year=2011|volume=38}}</ref> and has been used in applications on up to 300,000 processor cores.\n* [[multigrid method]] with local smoothing on adaptively refined meshes<ref>{{cite journal|last=Janssen|first=B.|author2=Kanschat, G.|title=Adaptive multilevel methods with local smoothing for H<sup>1</sup>- and H<sup>curl</sup>-conforming high order finite element methods|journal=SIAM J. Sci. Comput.|year=2011|volume=33 | issue = 4 }}</ref><ref>{{cite journal|last=Kanschat|first=G.|title=Multi-level methods for discontinuous Galerkin FEM on locally refined meshes|journal=Computers & Structures|year=2004|volume=82}}</ref>\n* [[hp-FEM]]\n* extensive documentation and tutorial programs,\n* interfaces to several libraries including [[Gmsh]], [[PETSc]], [[Trilinos]], [[METIS]], [[VTK]], [[p4est]], [[BLAS]], [[LAPACK]], [[HDF5]], [[NetCDF]], and [[Open Cascade Technology]].\n\n== History and Impact ==\nThe software started from work at the Numerical Methods Group at [[Heidelberg University]] in Germany in 1998. The first public release was version 3.0.0 in 2000. Since then deal.II has gotten contributions from several hundred authors<ref>{{cite web|title=deal.II Authors|url=http://dealii.org/authors.html|accessdate=14 June 2019}}</ref> and has been used in more than a thousand of research publications.<ref>{{cite web|title=List of Publications|url=http://dealii.org/publications.html|accessdate=14 June 2019}}</ref>\n\nThe primary maintainers, coordinating the worldwide development of the library, are today located at [[Colorado State University]], [[University of Utah]], [[Heidelberg University]], [[Texas A&M University]], [[Oak Ridge National Laboratory]] and a number of other institutions. It is developed as a worldwide community of contributors through [[github]]<ref>{{cite web|title=deal.II github page|url=http://github.com/dealii/dealii|accessdate=14 June 2019}}</ref> that incorporates several hundred changes by dozens of authors every month.\n\n== See also ==\n* [[List of finite element software packages]]\n* [[List of numerical analysis software]]\n\n==References==\n{{reflist}}\n\n== External links ==\n*{{Official website|http://www.dealii.org/}}\n*[https://github.com/dealii/dealii Source Code on Github]\n*[https://dealii.org/publications.html List of Scientific publications]\n\n[[Category:Free computer libraries]]\n[[Category:Differential calculus]]\n[[Category:Finite element software for Linux]]\n[[Category:C++ numerical libraries]]"
    },
    {
      "title": "Dune (software)",
      "url": "https://en.wikipedia.org/wiki/Dune_%28software%29",
      "text": "{{ infobox software\n| name                   = DUNE\n| logo                   = Dune-logo6.png\n| latest_release_version = 2.4.1\n| latest_release_date    = {{Start date and age|2016|02|29}}\n| operating_system       = [[Linux]], [[Unix]], [[OS X]]\n| programming_language   = [[C++]]\n| genre                  = [[Finite element analysis]]\n| license                = [[GNU General Public Licence|GPL (version 2)]] with \"runtime exception\"\n| website                = {{URL|www.dune-project.org}}\n}}\n\n'''DUNE''' ('''''D'''istributed and '''U'''nified '''N'''umerics '''E'''nvironment'') is a modular [[C++]] library for the solution of [[partial differential equations]] using grid-based methods.\n\nThe DUNE library is divided into modules.  In version 2.4 are the modules\n* general classes and infrastructure: ''dune-common'',\n* geometry classes: ''dune-geometry'',\n* grid interface: ''dune-grid'',\n* linear algebra classes: ''dune-istl'',\n* local [[ansatz]] functions: ''dune-localfunctions'',\nand a documentation module available. In addition there are several further modules, including some which have been developed by third parties.\n\n== History ==\n\nThe development of DUNE started in 2002 on the initiative of Prof. Bastian (then [[Heidelberg University]]), Dr. Ohlberger (during his habilitation at the [[University of Freiburg]]), and Prof. Rumpf (then [[University of Duisburg-Essen]]). The aim was a development model which was not attached to a single university, in order to make the project attractive for a wide audience. For the same reason a license was chosen which allows DUNE together with proprietary libraries. While most of developers still have a university background, others are providing commercial support for DUNE.<ref>[http://www.dune-project.org/people.html DUNE - People<!-- Bot generated title -->]</ref>\n\n== Goals ==\n\nRight from the start the main design goal of DUNE was to allow the coupling of new and legacy codes efficiently.  This is what sets DUNE apart from other finite element programs.\n\nDUNE is primarily a set of [[interface (object-oriented programming)|abstract interfaces]], which embody concepts from [[scientific computing]].   These are mainly intended to be used in [[finite element]] and [[finite volume]] applications, but also [[finite differences|finite difference methods]] are possible.  The central interface is the grid interface.  It describes structured and unstructured grids of arbitrary dimension, both with manifold and non-manifold structure.  Also, functionality for parallel programming is described.  Seven different implementations of the grid interface exist.  Four of these are encapsulations of existing grid managers.  It is hence possible to directly compare different grid implementations.\n\n== Implementation ==\n\nVarious C++ techniques such as [[Template (programming)|template programming]], [[generic programming]], [[Template metaprogramming|C++ template metaprogramming]], and static polymorphism are used.  These are well known in other areas of software development and are slowly making their way into [[scientific computing]].  They allow the [[compiler]] to eliminate most of the overhead introduced by the extra layer of abstraction.  A high level of standard conformance is required for this from the compiler.\n\n== References ==\n{{reflist}}\n\n== External links ==\n* [http://www.dune-project.org DUNE webpage].\n* Scientific [http://www.dune-project.org/publications.html publications] about DUNE.\n\n[[Category:Numerical software]]\n[[Category:Numerical linear algebra]]\n[[Category:Scientific simulation software]]\n[[Category:C++ libraries]]\n[[Category:Finite element software for Linux]]\n[[Category:Free software programmed in C++]]"
    },
    {
      "title": "Elmer FEM solver",
      "url": "https://en.wikipedia.org/wiki/Elmer_FEM_solver",
      "text": "{{Infobox software\n| name = Elmer FEM solver\n| logo = elmerlogo.png\n| screenshot = Elmer-pump-heatequation.png\n| caption = One of the simpler examples provided with Elmer, a thermal model of a pump casing, as visualised using the ElmerPost tool\n| latest release version = 8.4\n| latest release date = {{Start date and age|2018|12|19}}<ref name=\"Releases\">{{cite web |url=https://github.com/ElmerCSC/elmerfem/releases|title=Releases|publisher=github.com/ElmerCSC|accessdate=23 December 2018}}</ref>\n| repo = {{URL|https://github.com/ElmerCSC/elmerfem}}\n| programming language = [[Fortran 90]], [[C (programming language)|C]] and [[C++]]\n| operating system = [[Linux]], [[Microsoft Windows]], [[MacOS]]\n| platform = command line /GUI [[Qt (software)|Qt]] v4/v5\n| size = \n| language = \n| genre = [[Computer-aided engineering|CAE]]\n| license = [[GNU General Public License]]\n| website = {{URL|www.csc.fi/elmer}}\n}}\n\n'''Elmer''' is computational tool for multi-[[physics]] problems. It has been developed by [[CSC – IT Center for Science Ltd|CSC]]<ref name=\"website\"/> in collaboration with [[Finland|Finnish]] universities, research laboratories and industry. Elmer FEM solver is [[free and open-source software]], subject to the requirements of the [[GNU General Public License]] (GPL), version 2 or any later.<ref name=\"raback2007\"/>\n\nElmer includes physical models of [[fluid dynamics]], [[structural mechanics]], [[electromagnetics]], [[heat transfer]] and [[acoustics]], for example.<ref name=\"raback2007\"/> These are described by partial differential equations which Elmer solves by the [[Finite Element Method]] (FEM).\n\nElmer comprises several different parts:<ref name=\"raback2015\"/>\n* ElmerGrid – A mesh conversion tool, which can be used to convert differing mesh formats into Elmer-suitable meshes.\n* ElmerGUI – A graphical interface which can be used on an existing mesh to assign physical models, this generates a \"case file\" which describes the problem to be solved. Does not show the whole ElmerSolver functionality in GUI.\n* ElmerSolver – The numerical solver which performs the  finite element calculations, using the mesh and case files. \n* ElmerPost – A post-processing/visualisation module. (Development stopped in favour of other post-processing tools such as ParaView, VisIt, etc.)\n\nThe different parts of Elmer software may be used independently. Whilst the main module is the ElmerSolver tool, which includes many sophisticated features for physical model solving, the additional components are required to create a full workflow. For pre- and post-processing other tools, such as [[Paraview]] can be used to visualise the output.\n\nThe software runs on Unix and Windows platforms and can be compiled on a large variety of compilers, using the [[CMake]] building tool. The solver can also be used in a multi-host parallel mode on platforms that support [[Message Passing Interface|MPI]]. Elmer's parallelisation capability is one of the strongest sides of this solver.\n\n== External links ==\n* {{Official website|www.csc.fi/elmer}}\n\n== See also ==\n{{Portal|Free and open-source software|Electronics}}\n* [[Finite Element Method]]\n* [[List of finite element software packages|List of finite element packages]]\n\n== References ==\n<references>\n<ref name=\"website\">{{cite web\n    | url = http://www.csc.fi/english/pages/elmer\n    | title = Elmer – CSC\n    | publisher = CSC — IT Center for Science Ltd\n    | accessdate = 2010-06-24}}\n</ref>\n<ref name=\"raback2007\">{{cite journal\n    | last1 = Råback\n    | first1 = Peter\n    | last2 = Forsström\n    | first2 = Pirjo-Leena\n    | last3 = Lyly\n    | first3 = Mikko\n    | last4 = Gröhn\n    | first4 = Matti\n    | title = Elmer-finite element package for the solution of partial differential equations\n    | journal = EGEE User Forum\n    | year = 2007}}\n</ref>\n<ref name=\"raback2015\">{{cite journal\n    | first1 = Peter\n    | last1 = Råback\n    | first2 = Mika\n    | last2 = Malinen\n    | title = Overview of Elmer\n    | year = 2016\n    | journal = CSC – IT Center for Science\n    | url = ftp://lukusali.nic.funet.fi/ftp/index/Science/physics/elmer/doc/ElmerOverview.pdf}}\n</ref>\n</references>\n\n{{CAE software}}\n\n[[Category:Numerical software]]\n[[Category:Free computer-aided design software]]\n[[Category:Finite element software for Linux]]\n[[Category:Free software programmed in Fortran]]\n[[Category:Free science software]]\n[[Category:Computational physics]]\n[[Category:Engineering software that uses Qt]]\n[[Category:Computer-aided engineering software for Linux]]\n[[Category:Software that uses Tk]]"
    },
    {
      "title": "FreeFem++",
      "url": "https://en.wikipedia.org/wiki/FreeFem%2B%2B",
      "text": "{{Infobox software\n| name = FreeFem++\n| screenshot = FreeFEM++ CS Example - Dirichlet.png\n| caption = FreeFem++-cs\n| developer = [[Université Pierre et Marie Curie]] and [[Laboratoire Jacques-Louis Lions]]\n| released = {{Start date and age|1987}}\n| latest release version = 3.61\n| latest release date = {{Start date and age|2018|06|20}}\n| latest preview date = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| repo = {{URL|https://github.com/FreeFem/FreeFem-sources}}\n| programming language = [[C++]]\n| operating system = [[Linux]], [[macOS]], [[Microsoft Windows]], [[Solaris (operating system)|Solaris]]\n| license = [[LGPL]] version 2.1 or later\n| website = {{URL|https://freefem.org}}\n}}\n\n'''FreeFem++''' is a [[programming language]] and a software focused on solving [[partial differential equations]] using the [[finite element method]]. FreeFem++ is written in [[C++]] and developed and maintained by [[Université Pierre et Marie Curie]] and [[Laboratoire Jacques-Louis Lions]]. It runs on [[Linux]], [[Solaris (operating system)|Solaris]], [[macOS]] and [[MS Windows]] systems. FreeFem++ is [[free software]] ([[LGPL]]).,<ref name=\"hecht2012\"/><ref name=\"website\"/>\n\nFreeFem++ language is inspired by C++. There is an [[Integrated development environment|IDE]] called FreeFem++-cs.\n\n== History ==\nThe first version was created in 1987 by Olivier Pironneau and was named MacFem (it only worked on Macintosh); PCFem appeared some time later. Both were written in [[Pascal (programming language)|Pascal]].\n\nIn 1992 it was re-written in C++ and named FreeFem. Later versions, FreeFem+ (1996) and FreeFem++ (1998), used that programming language too.\n\n== Other versions ==\n* FreeFem++ includes versions for console mode and [[Message Passing Interface|MPI]]\n* FreeFem3D\n\nDeprecated versions:\n* FreeFem+\n* FreeFem\n\n== See also ==\n* [[List of finite element software packages]]\n\n== References ==\n<references>\n<ref name=\"hecht2012\">{{cite journal\n    | author = Hecht, Frédéric\n    | title = New development in FreeFem++\n    | journal = Journal of Numerical Mathematics\n    | year = 2012\n    | volume = 20\n    | issue = 3-4\n    | pages = 251–266}}\n</ref>\n<ref name=\"website\">{{cite web\n    | url = https://freefem.org\n    | title = FreeFem++ website\n    | accessdate = November 27, 2018}}\n</ref>\n</references>\n\n== External links ==\n* {{Official website|https://freefem.org}}\n\n\n{{Numerical analysis software}}\n{{CAE software}}\n\n[[Category:Computational physics]]\n[[Category:Free science software]]\n[[Category:Computer-aided engineering software for Linux]]\n[[Category:Engineering software that uses Qt]]\n[[Category:Numerical analysis software for Linux]]\n[[Category:Finite element software for Linux]]"
    },
    {
      "title": "OpenSees",
      "url": "https://en.wikipedia.org/wiki/OpenSees",
      "text": "{{main|Earthquake engineering}}\n{{Infobox software\n| name                   = \n| title                  = \n| logo                   = <!-- Image name is enough -->\n| logo caption           = \n| logo_size              = \n| logo_alt               = \n| screenshot             = <!-- Image name is enough -->\n| caption                = \n| screenshot_size        = \n| screenshot_alt         = \n| collapsible            = \n| author                 = \n| developer              = \n| released               = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| discontinued           = \n| latest release version = \n| latest release date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| latest preview version = \n| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| status                 = \n| programming language   = [[C++]]\n| operating system       = [[Linux]], Windows and Unix-like\n| platform               = \n| size                   = \n| language               = \n| language count         = <!-- DO NOT include this parameter unless you know what it does -->\n| language footnote      = \n| genre                  = \n| license                = http://opensees.berkeley.edu/OpenSees/copyright.php\n| website                = {{URL|http://opensees.berkeley.edu/}}\n}}\n'''OpenSees''' (the Open System for [[Earthquake engineering|Earthquake Engineering]] Simulation) is a [[proprietary license|proprietary]] [[object-oriented]], software framework created at the [[National Science Foundation]]-sponsored Pacific Earthquake Engineering\n([http://peer.berkeley.edu PEER]) Center. It allows users to create [[finite element]] applications for simulating the response of structural and geotechnical systems subjected to [[earthquakes]]. This framework was developed by Frank McKenna and [[Gregory L. Fenves]] with significant contributions from Michael H. Scott, Terje Haukaas, Armen Der Kiureghian, Remo M. de Souza, Filip C. Filippou, Silvia Mazzoni, and Boris Jeremic.  OpenSees is primarily written in [[C++]] and uses several [[Fortran]] numerical libraries for linear equation solving.\n\n==Licensing==\nThe license permits use, reproduction, modification, and distribution by educational, research, and non-profit entities for noncommercial purposes only. Use, reproduction and modification by other entities is allowed for internal purposes only. The [[UC Regents]] hold the copyright for OpenSees. <ref>{{cite web | url = http://opensees.berkeley.edu/OpenSees/copyright.php | title = Copyright | publisher = [[University of California, Berkeley]] | accessdate = 2014-05-16}}</ref>\n\n==Usage==\nUsers of OpenSees create applications by writing scripts in the [[Tcl]] programming language.  The ''TclModelBuilder'' class in the OpenSees framework extends an instance of the Tcl interpreter with commands for finite element model building and analysis.\n\nOpenSees developers access the source code using [[Apache Subversion]]. Although anyone may check-out the source code anonymously, only a handful of individuals have check-in access.\n\n==Acronym==\nThe proper acronym capitalization for the \"Open System for Earthquake Engineering Simulation\" is OpenSees, as opposed to OpenSEES.  This reflects the same unconventional capitalization of [[Tcl]].\n\n==History==\nPrior to taking on the name \"OpenSees,\" the framework was simply called \"G3\" in reference to the name of the [http://peer.berkeley.edu PEER] research group tasked with simulation development.\nThe doctoral thesis of Frank McKenna on parallel object-oriented structural analysis formed the basis for \"G3.\"\n\n==References==\n{{Reflist}}\n\n==External links==\n*[http://opensees.berkeley.edu/ OpenSees Webpage]\n*[http://opensees.berkeley.edu/OpenSees/manuals/usermanual/index.html OpenSees Manual]\n*[http://www.inrisk.ubc.ca/ Infrastructure Risk Research Project at The University of British Columbia, Vancouver, Canada]\n\n[[Category:Numerical software]]\n[[Category:Earthquake engineering]]\n[[Category:Finite element software for Linux]]"
    },
    {
      "title": "Bogacki–Shampine method",
      "url": "https://en.wikipedia.org/wiki/Bogacki%E2%80%93Shampine_method",
      "text": "The '''Bogacki–Shampine method''' is a method for the [[numerical ordinary differential equations|numerical solution of ordinary differential equations]], that was proposed by Przemyslaw Bogacki and Lawrence F. Shampine in 1989 {{harv|Bogacki|Shampine|1989}}. The Bogacki–Shampine method is a [[Runge–Kutta method]] of order three with four stages with the First Same As Last (FSAL) property, so that it uses approximately three function evaluations per step. It has an embedded second-order method which can be used to implement [[adaptive step size]]. The Bogacki–Shampine method is implemented in the <code>ode23</code> function in [[MATLAB]] {{harv|Shampine|Reichelt|1997}}. \n\nLow-order methods are more suitable than higher-order methods like the [[Dormand–Prince method]] of order five, if only a crude approximation to the solution is required. Bogacki and Shampine argue that their method outperforms other third-order methods with an embedded method of order two.\n\nThe [[Runge–Kutta method#Explicit Runge–Kutta methods|Butcher tableau]] for the Bogacki–Shampine method is:\n{| cellpadding=3px cellspacing=0px\n|width=\"20px\"| || style=\"border-right:1px solid;\" | 0\n|- \n||| style=\"border-right:1px solid;\" | 1/2 || 1/2\n|- \n||| style=\"border-right:1px solid;\" | 3/4 || 0 || 3/4\n|- \n||| style=\"border-right:1px solid; border-bottom:1px solid;\" | 1 || style=\"border-bottom:1px solid;\" | 2/9 || style=\"border-bottom:1px solid;\" | 1/3 || style=\"border-bottom:1px solid;\" | 4/9 || style=\"border-bottom:1px solid;\" | \n|- \n||| style=\"border-right:1px solid;\" | || 2/9 || 1/3 || 4/9 || 0\n|-\n||| style=\"border-right:1px solid;\" | || 7/24 || 1/4 || 1/3 || 1/8\n|}\n\nFollowing the standard notation, the differential equation to be solved is <math>y'=f(t,y)</math>. Furthermore, <math>y_n</math> denotes the numerical solution at time <math>t_n</math> and <math>h_n</math> is the step size, defined by <math>h_n = t_{n+1}-t_n</math>. Then, one step of the Bogacki–Shampine method is given by:\n:<math> \\begin{align}\nk_1 &= f(t_n, y_n) \\\\\nk_2 &= f(t_n + \\tfrac12 h_n, y_n + \\tfrac12 h_n k_1) \\\\\nk_3 &= f(t_n + \\tfrac34 h_n, y_n + \\tfrac34 h_n k_2) \\\\\ny_{n+1} &= y_n + \\tfrac29 h_n k_1 + \\tfrac13 h_n k_2 + \\tfrac49 h_n k_3 \\\\\nk_4 &= f(t_n + h_n, y_{n+1}) \\\\\nz_{n+1} &= y_n + \\tfrac7{24} h_n k_1 + \\tfrac14 h_n k_2 + \\tfrac13 h_n k_3 + \\tfrac18 h_n k_4.\n\\end{align} </math>\n\nHere, <math>z_{n+1}</math> is a second-order approximation to the exact solution. The method for calculating <math>y_{n+1}</math> is due to {{harvtxt|Ralston|1965}}. On the other hand, <math>y_{n+1}</math> is a third-order approximation, so the difference between <math>y_{n+1}</math> and <math>z_{n+1}</math> can be used to [[adaptive stepsize|adapt the step size]]. The FSAL—first same as last—property is that the stage value <math>k_4</math> in one step equals <math>k_1</math> in the next step; thus, only three function evaluations are needed per step.\n\n== References ==\n* {{Citation | last1=Bogacki | first1=Przemyslaw | last2=Shampine | first2=Lawrence F. | title=A 3(2) pair of Runge–Kutta formulas | doi=10.1016/0893-9659(89)90079-7 | year=1989 | journal=Applied Mathematics Letters | issn=0893-9659 | volume=2 | issue=4 | pages=321–325}}.\n* {{Citation | last1=Ralston | first1=Anthony | title=A First Course in Numerical Analysis | publisher=[[McGraw-Hill]] | location=New York | year=1965}}.\n* {{Citation | last1=Shampine | first1=Lawrence F. | last2=Reichelt | first2=Mark W. | title=The Matlab ODE Suite | doi=10.1137/S1064827594276424 | year=1997 | journal=[[SIAM Journal on Scientific Computing]] | issn=1064-8275 | volume=18 | issue=1 | pages=1–22}}.\n\n{{DEFAULTSORT:Bogacki-Shampine method}}\n[[Category:Runge–Kutta methods]]"
    },
    {
      "title": "Gauss–Legendre method",
      "url": "https://en.wikipedia.org/wiki/Gauss%E2%80%93Legendre_method",
      "text": "In [[numerical analysis]] and [[scientific computing]], the '''Gauss–Legendre methods''' are a family of [[numerical methods for ordinary differential equations]]. Gauss–Legendre methods are implicit [[Runge–Kutta methods]]. More specifically, they are [[collocation method]]s based on the points of [[Gauss–Legendre quadrature]]. The Gauss–Legendre method based on ''s'' points has order 2''s''.<ref>{{harvnb|Iserles|1996|p=47}}</ref>\n\nAll Gauss–Legendre methods are [[A-stability|A-stable]].<ref>{{harvnb|Iserles|1996|p=63}}</ref>\n\nThe Gauss–Legendre method of order two is the [[midpoint method|implicit midpoint rule]]. Its [[Butcher tableau]] is:\n\n:{| cellpadding=3px cellspacing=0px style=\"text-align: center;\"\n| style=\"border-right:1px solid; border-bottom:1px solid;\" | 1/2 || style=\"border-bottom:1px solid;\" | 1/2\n|-\n| style=\"border-right:1px solid;\" | || 1\n|}\n\nThe Gauss–Legendre method of order four has Butcher tableau:\n\n:{| cellpadding=3px cellspacing=0px style=\"text-align: center;\"\n| style=\"border-right:1px solid;\" | <math> \\tfrac12 - \\tfrac16 \\sqrt3 </math> || <math> \\tfrac14 </math> || <math> \\tfrac14 - \\tfrac16 \\sqrt3 </math>\n|-\n| style=\"border-right:1px solid; border-bottom:1px solid;\" | <math> \\tfrac12 + \\tfrac16 \\sqrt3 </math> || style=\"border-bottom:1px solid;\" | <math> \\tfrac14 + \\tfrac16 \\sqrt3 </math> || style=\"border-bottom:1px solid;\" | <math> \\tfrac14 </math> \n|-\n| style=\"border-right:1px solid;\" | || <math> \\tfrac12 </math> || <math> \\tfrac12 </math>\n|}\n\nThe Gauss–Legendre method of order six has Butcher tableau:\n\n:{| cellpadding=3px cellspacing=0px style=\"text-align: center;\"\n| style=\"border-right:1px solid;\" | <math> \\tfrac12 - \\tfrac1{10} \\sqrt{15} </math> || <math> \\tfrac5{36} </math> || <math> \\tfrac29 - \\tfrac1{15} \\sqrt{15} </math> || <math> \\tfrac5{36} - \\tfrac1{30} \\sqrt{15} </math>\n|-\n| style=\"border-right:1px solid;\" | <math> \\tfrac12 </math> || <math> \\tfrac5{36} + \\tfrac1{24} \\sqrt{15} </math> || <math> \\tfrac29 </math> || <math> \\tfrac5{36} - \\tfrac1{24} \\sqrt{15} </math>\n|-\n| style=\"border-right:1px solid; border-bottom:1px solid;\" | <math> \\tfrac12 + \\tfrac1{10} \\sqrt{15} </math> || style=\"border-bottom:1px solid;\" | <math> \\tfrac5{36} + \\tfrac1{30} \\sqrt{15} </math> || style=\"border-bottom:1px solid;\" | <math> \\tfrac29 + \\tfrac1{15} \\sqrt{15} </math> || style=\"border-bottom:1px solid;\" | <math> \\tfrac5{36} </math>\n|-\n| style=\"border-right:1px solid;\" | || <math> \\tfrac5{18} </math> || <math> \\tfrac49 </math> || <math> \\tfrac5{18} </math>\n|}\n\nThe computational cost of higher-order Gauss–Legendre methods is usually too high, and thus, they are rarely used.<ref>{{harvnb|Iserles|1996|p=47}}</ref>\n\n== Notes ==\n{{reflist}}\n\n== References ==\n* {{Citation | last1=Iserles | first1=Arieh | author1-link=Arieh Iserles | title=A First Course in the Numerical Analysis of Differential Equations | publisher=[[Cambridge University Press]] | isbn=978-0-521-55655-2 | year=1996}}.\n\n{{DEFAULTSORT:Gauss-Legendre method}}\n[[Category:Runge–Kutta methods]]"
    },
    {
      "title": "Trapezoidal rule (differential equations)",
      "url": "https://en.wikipedia.org/wiki/Trapezoidal_rule_%28differential_equations%29",
      "text": "In [[numerical analysis]] and [[scientific computing]], the '''trapezoidal rule''' is a [[numerical methods for ordinary differential equations|numerical method to solve ordinary differential equations]] derived from the [[trapezoidal rule]] for computing integrals. The trapezoidal rule is an [[implicit and explicit methods|implicit]] second-order method, which can be considered as both a [[Runge–Kutta method]] and a [[linear multistep method]].\n\n== Method ==\n\nSuppose that we want to solve the differential equation\n:<math> y' = f(t,y). </math>\nThe trapezoidal rule is given by the formula\n:<math> y_{n+1} = y_n + \\tfrac12 h \\Big( f(t_n,y_n) + f(t_{n+1},y_{n+1}) \\Big), </math>\nwhere <math> h = t_{n+1} - t_n </math> is the step size.<ref>{{harvnb|Iserles|1996|p=8}}; {{harvnb|Süli|Mayers|2003|p=324}}</ref>\n\nThis is an implicit method: the value <math> y_{n+1} </math> appears on both sides of the equation, and to actually calculate it, we have to solve an equation which will usually be nonlinear. One possible method for solving this equation is [[Newton's method]]. We can use the [[Euler method]] to get a fairly good estimate for the solution, which can be used as the initial guess of Newton's method.<ref>{{harvnb|Süli|Mayers|2003|p=324}}</ref>\n\n== Motivation ==\n\nIntegrating the differential equation from <math> t_n </math> to <math> t_{n+1} </math>, we find that\n:<math> y(t_{n+1}) - y(t_n) = \\int_{t_n}^{t_{n+1}} f(t,y(t)) \\,\\mathrm{d}t. </math>\nThe [[trapezoidal rule]] states that the integral on the right-hand side can be approximated as\n:<math> \\int_{t_n}^{t_{n+1}} f(t,y(t)) \\,\\mathrm{d}t \\approx \\tfrac12 h \\Big( f(t_n,y(t_n)) + f(t_{n+1},y(t_{n+1})) \\Big). </math>\nNow combine both formulas and use that <math> y_n \\approx y(t_n) </math> and <math> y_{n+1} \\approx y(t_{n+1}) </math> to get the trapezoidal rule for solving ordinary differential equations.<ref>{{harvnb|Iserles|1996|p=8}}; {{harvnb|Süli|Mayers|2003|p=324}}</ref>\n\n== Error analysis ==\n\nIt follows from the error analysis of the trapezoidal rule for quadrature that the [[local truncation error]] <math> \\tau_n </math> of the trapezoidal rule for solving differential equations can be bounded as:\n:<math> |\\tau_n| \\le \\tfrac1{12} h^3 \\max_t |y'''(t)|. </math>\nThus, the trapezoidal rule is a third-order method. This result can be used to show that the global error is <math> O(h^3) </math> as the step size <math> h </math> tends to zero (see [[big O notation]] for the meaning of this).<ref>{{harvnb|Iserles|1996|p=9}}; {{harvnb|Süli|Mayers|2003|p=325}}</ref>\n\n== Stability ==\n\n[[Image:Stability region for trapezoidal method.svg|thumb|The pink region is the stability region for the trapezoidal method.]]\nThe [[stiff equation|region of absolute stability]] for the trapezoidal rule is\n:<math> \\{ z \\in \\mathbb{C} \\mid \\operatorname{Re}(z) < 0 \\}. </math>\nThis includes the left-half plane, so the trapezoidal rule is A-stable. The second Dahlquist barrier states that the trapezoidal rule is the most accurate amongst the A-stable linear multistep methods. More precisely, a linear multistep method that is A-stable has at most order two, and the error constant of a second-order A-stable linear multistep method cannot be better than the error constant of the trapezoidal rule.<ref>{{harvnb|Süli|Mayers|2003|p=324}}</ref>\n\nIn fact, the region of absolute stability for the trapezoidal rule is precisely the left-half plane. This means that if the trapezoidal rule is applied to the linear test equation ''y''' = λ''y'', the numerical solution decays to zero if and only if the exact solution does. \n\n== Notes ==\n{{reflist}}\n\n== References ==\n* {{Citation | last1=Iserles | first1=Arieh | author1-link=Arieh Iserles | title=A First Course in the Numerical Analysis of Differential Equations | publisher=[[Cambridge University Press]] | isbn=978-0-521-55655-2 | year=1996}}.\n* {{Citation | last1=Süli | first1=Endre | last2=Mayers | first2=David | title=An Introduction to Numerical Analysis | publisher=[[Cambridge University Press]] | isbn=0521007941 | year=2003}}.\n\n== See also ==\n*[[Crank–Nicolson method]]\n\n\n{{Numerical integrators}}\n\n[[Category:Runge–Kutta methods]]"
    },
    {
      "title": "ABS methods",
      "url": "https://en.wikipedia.org/wiki/ABS_methods",
      "text": "'''ABS methods''', where the acronym contains the initials of Jozsef Abaffy, [[Charles George Broyden|Charles G. Broyden]] and [[Emilio Spedicato]], have been developed since 1981 to generate a large class of [[algorithm]]s for the following applications:\n* solution of general linear algebraic systems, determined or underdetermined, \n* full or deficient rank; \n* solution of [[linear Diophantine system]]s, i.e. equation systems where the coefficient matrix and the right hand side are integer valued and an integer solution is sought; this is a special but important case of [[Hilbert's tenth problem]], the only one in practice soluble; \n* solution of nonlinear [[algebraic equation]]s;\n* solution of continuous unconstrained or [[constrained optimization]].\n\nAt the beginning of 2007 ABS literature consisted of over 400 papers and reports and two monographs, one due to Abaffy and Spedicato and published in 1989, one due to Xia and Zhang and published, in Chinese, in 1998. Moreover three conferences had been organized in China.\n\nResearch on ABS methods has been the outcome of an international collaboration  coordinated by Spedicato of University of [[Bergamo]], Italy.  It has involved over forty mathematicians from Hungary, UK, China, Iran and other countries.\n\nThe central element in such methods is the use of a special matrix  transformation due essentially to the Hungarian mathematician [[Jenő Egerváry]], who investigated its main properties in some papers that went unnoticed. \nFor the basic problem of solving a linear system of ''m'' equations in ''n'' variables, where <math>\\scriptstyle m \\,\\leq\\, n</math>, ABS methods use the following simple geometric idea:\n\n# Given an arbitrary initial estimate of the solution, find one of the infinite solutions, defining a [[linear variety]] of dimension ''n''&nbsp;−&nbsp;1, of the first equation.\n# Find a solution of the second equation that is also a solution of the first, i.e. find a solution lying in the intersection of the linear varieties of the solutions of the first two equations considered separately.\n# By iteration of the above approach after ''m''' steps one gets a solution of the last equation that is also a solution of the previous equations, hence of the full system. Moreover it is possible to detect equations that are either redundant or incompatible.\n\nAmong the main results obtained so far:\n* unification of algorithms for linear, nonlinear algebraic equations and for linearly constrained nonlinear optimization, including the [[LP problem]] as a special case;\n* the method of [[Carl Gauss|Gauss]] has been improved by reducing the required memory and eliminating the need for pivoting;\n* new methods for nonlinear systems with convergence properties better than for Newton method;\n* derivation of a general algorithm for Hilbert tenth problem, linear case, with the extension of a classic Euler theorem from one equation to a system;\n* solvers have been obtained that are more stable than classical ones, especially for the problem arising in primal-dual interior point method;\n* ABS methods are usually faster on vector or parallel machines;\n* ABS methods provide a simpler approach for teaching for a variety of classes of problems, since particular methods are obtained just by specific parameter choices.\n\nKnowledge of ABS methods is still quite limited among mathematicians, but they have great potential for improving the methods currently in use.\n\n== Bibliography ==\n* Jozsef Abaffy, Emilio Spedicato (1989): ''ABS Projection Algorithms: Mathematical Techniques for Linear and Nonlinear Algebraic Equations'', Ellis Horwood, Chichester. &nbsp; ''The first monograph on the subject''\n* Jozsef Abaffy, Charles G. Broyden, Emilio Spedicato (1984): ''A class of direct methods for linear equations'', Numerische Mathematik '''45''', 361-376. ''Paper introducing ABS methods for continuous linear systems''.\n* H. Esmaeili, N. Mahdavi-Amiri, Emilio Spedicato: ''A class of ABS algorithms for Diophantine linear systems'', Numerische Mathematik '''90''', 101-115.  ''Paper introducing ABS methods for integer linear systems''.\n\n{{DEFAULTSORT:Abs Methods}}\n[[Category:Diophantine equations]]\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "Armadillo (C++ library)",
      "url": "https://en.wikipedia.org/wiki/Armadillo_%28C%2B%2B_library%29",
      "text": "{{Infobox software\n| name                   = Armadillo C++ Library\n| logo                   =\n| screenshot             =\n| caption                =\n| developer              =\n| latest release version = 9.200\n| latest release date    = {{Start date and age|2018|11|6}}<ref name=\"sf-news\">{{cite web|url=https://sourceforge.net/p/arma/news/|title=Armadillo C++ matrix library / News: Recent posts|accessdate=14 November 2018|via=[[SourceForge]]}}</ref>\n| latest preview version = 9.300-RC1<ref name=\"sf-news\" />\n| latest preview date    = {{Start date and age|2019|03|14}}<ref name=\"sf-news-2019-03-14\">{{cite web|url=https://sourceforge.net/p/arma/news/2019/03/armadillo-c-linear-algebra-version-9300-rc1-release-candidate-1/|title=Armadillo C++ linear algebra: version 9.300-RC1 (Release Candidate 1)|date=14 March 2019|last=Sanderson|first=Conrad|accessdate=14 March 2019|via=[[SourceForge]]}}</ref>\n| programming language   = [[C++]]\n| operating system       = [[Cross-platform]]\n| language               = [[English language|English]]\n| genre                  = [[library (computing)|Software library]]\n| license                = [[Apache License|Apache 2.0]] ([[Open-source software|open source]])\n| website                = {{URL|arma.sourceforge.net}}\n}}\n'''Armadillo''' is a [[linear algebra]] software library for the [[C++|C++ programming language]].  It aims to provide efficient and streamlined base calculations, while at the same time having a straightforward and easy-to-use interface.  Its intended target users are scientists and engineers.\n\nIt supports integer, floating point ([[single precision|single]] and [[double precision floating-point format|double]] precision), [[complex number]]s, and a subset of [[Trigonometric functions|trigonometric]] and [[statistics]] functions. Dense and [[Sparse matrix|sparse matrices]] are supported<ref>{{cite conference |url=https://doi.org/10.1007/978-3-319-96418-8_50 |title=A User-Friendly Hybrid Sparse Matrix Class in C++ |author=Conrad Sanderson and Ryan Curtin |conference=Lecture Notes in Computer Science (LNCS), Vol. 10931, pp. 422-430 |year=2018}}</ref>. Various [[matrix decomposition]]s are provided through optional integration with Linear Algebra PACKage ([[LAPACK]]) and [[Automatically Tuned Linear Algebra Software]] (ATLAS) libraries.<ref>{{cite journal |title=Armadillo: a template-based C++ library for linear algebra |author=Conrad Sanderson and Ryan Curtin|journal=Journal of Open Source Software |volume=1 |pages=26 |year=2016}}</ref><ref>{{cite journal |url=http://jmlr.org/papers/v14/curtin13a.html |title=MLPACK: A Scalable C++ Machine Learning Library |author=Ryan Curtin |journal=Journal of Machine Learning Research (JMLR) |volume=14 |issue=Mar |pages= 801–805 |year=2013 |display-authors=etal}}</ref> High-performance BLAS/LAPACK replacement libraries such as [[OpenBLAS]] and [[Math Kernel Library|Intel MKL]] can also be used.\n\nThe library employs a [[Lazy evaluation|delayed-evaluation]] approach (during [[compile time]]) to combine several operations into one and reduce (or eliminate) the need for temporaries. Where applicable, the order of operations is optimised. Delayed evaluation and optimisation are achieved through [[template metaprogramming]].\n\nArmadillo is related to the [[Boost (C++ libraries)|Boost]] Basic Linear Algebra Subprograms (uBLAS) library, which also uses [[template metaprogramming]]. However, Armadillo builds upon [[Automatically Tuned Linear Algebra Software|ATLAS]] and [[LAPACK]] libraries, thereby providing machine-dependent optimisations and functions not present in uBLAS.\n\nIt is [[open-source software]] distributed under the permissive [[Apache License]], making it applicable for the [[Software development|development]] of both [[Open-source software|open source]] and [[Proprietary software|proprietary]] software. The project is supported by the [[NICTA]] research centre in Australia.\n\n==Example in C++ 11==\nHere is a trivial example demonstrating Armadillo functionality:\n\n<source lang=\"cpp\">\n// Compile with:\n// $ g++ -std=c++11 main.cpp -o file_name -O2 -larmadillo\n\n#include <iostream>\n#include <armadillo>\n#include <cmath>\n\nint main()\n{\n                                                //    ^\n  // Position of a particle                     //    |\n  arma::vec Pos = {{0},                         //    | (0,1)\n                   {1}};                        //    +---x-->\n\n  // Rotation matrix \n  double phi = -3.1416/2; \n  arma::mat RotM = {{+cos(phi), -sin(phi)},\n                    {+sin(phi), +cos(phi)}};\n\n  Pos.print(\"Current position of the particle:\");\n  std::cout << \"Rotating the point \" << phi*180/3.1416 << \" deg\" << std::endl;\n\n  Pos = RotM*Pos;\n\n  Pos.print(\"New position of the particle:\");   //    ^\n                                                //    x (1,0)\n                                                //    | \n                                                //    +------>\n\n  return 0;\n}\n</source>\n\n==Example in C++ 98==\nHere is another trivial example in C++ 98:\n\n<source lang=\"cpp\">\n#include <iostream>\n#include <armadillo>\n\nint main()\n{\n  arma::vec b;\n  b << 2.0 << 5.0 << 2.0;\n\n  // arma::endr represents the end of a row in a matrix\n  arma::mat A;\n  A << 1.0 << 2.0 << arma::endr\n    << 2.0 << 3.0 << arma::endr\n    << 1.0 << 3.0 << arma::endr;\n\n  std::cout << \"Least squares solution:\\n\";\n  std::cout << arma::solve(A,b) << '\\n';\n\n  return 0;\n}\n</source>\n\n==See also==\n{{Portal|Free and open-source software}}\n* [[mlpack]]\n* [[List of numerical analysis software]]\n* [[List of numerical libraries]]\n* [[Numerical linear algebra]]\n* [[Scientific computing]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* {{Official website}}\n\n{{DEFAULTSORT:Armadillo (C++ library)}}\n[[Category:Articles with example C++ code]]\n[[Category:C++ numerical libraries]]\n[[Category:Free computer libraries]]\n[[Category:Free mathematics software]]\n[[Category:Free science software]]\n[[Category:Free software programmed in C++]]\n[[Category:Free statistical software]]\n[[Category:Numerical linear algebra]]\n[[Category:Software using the Apache license]]"
    },
    {
      "title": "Arnoldi iteration",
      "url": "https://en.wikipedia.org/wiki/Arnoldi_iteration",
      "text": "In [[Numerical analysis|numerical]] [[linear algebra]], the '''Arnoldi iteration''' is an [[eigenvalue algorithm]] and an important example of an [[iterative method]].  Arnoldi finds an approximation to the [[eigenvalue]]s and [[eigenvector]]s of general (possibly non-[[Hermitian matrix|Hermitian]]) [[Matrix (mathematics)|matrices]] by constructing an orthonormal basis of the [[Krylov subspace]], which makes it particularly useful when dealing with large [[sparse matrix|sparse matrices]].\n\nThe Arnoldi method belongs to a class of linear algebra algorithms that give a partial result after a small number of iterations, in contrast to so-called ''direct methods'' which must complete to give any useful results (see for example, [[Householder transformation]]). The partial result in this case being the first few vectors of the basis the algorithm is building.\n\nWhen applied to Hermitian matrices it reduces to the [[Lanczos algorithm]].  The Arnoldi iteration was invented by [[W. E. Arnoldi]] in 1951.<ref>W. E. Arnoldi, \"The principle of minimized iterations in the solution of the matrix eigenvalue problem,\" ''Quarterly of  Applied Mathematics'', volume 9, pages 17–29, 1951</ref>\n\n==Krylov subspaces and the power iteration==\n\nAn intuitive method for finding the largest (in absolute value) eigenvalue of a given ''m'' × ''m'' matrix <math>A</math> is the [[power iteration]]:  starting with an arbitrary initial [[vector space|vector]] <var>b</var>, calculate ''Ab'', ''A''<sup>2</sup>''b'', ''A''<sup>3</sup>''b'',… normalizing the result after every application of the matrix ''A''.\n\nThis sequence converges to the [[eigenvector]] corresponding to the eigenvalue with the largest absolute value, <math>\\lambda_{1}</math>. However, much potentially useful computation is wasted by using only the final result, <math>A^{n-1}b</math>. This suggests that instead, we form the so-called ''Krylov matrix'':\n:<math>K_{n} = \\begin{bmatrix}b & Ab & A^{2}b & \\cdots & A^{n-1}b \\end{bmatrix}.</math>\n\nThe columns of this matrix are not in general [[orthogonal]], but we can extract an orthogonal [[basis (linear algebra)|basis]], via a method such as [[Gram–Schmidt process|Gram–Schmidt orthogonalization]].  The resulting set of vectors is thus an orthogonal basis of the ''[[Krylov subspace]]'', <math>\\mathcal{K}_{n}</math>.  We may expect the vectors of this basis to give good approximations of the eigenvectors corresponding to the <math>n</math> largest eigenvalues, for the same reason that <math>A^{n-1}b</math> approximates the dominant eigenvector.\n\n==The Arnoldi iteration==\n\nThe Arnoldi iteration uses the [[Gram-Schmidt#Numerical stability|stabilized Gram–Schmidt process]] to produce a sequence of orthonormal vectors, ''q''<sub>1</sub>, ''q''<sub>2</sub>, ''q''<sub>3</sub>, …, called the ''Arnoldi vectors'', such that for every ''n'', the vectors ''q''<sub>1</sub>, …, ''q''<sub>''n''</sub> span the Krylov subspace <math>\\mathcal{K}_n</math>. Explicitly, the algorithm is as follows:\n\n* Start with an arbitrary vector ''q''<sub>1</sub> with norm 1.\n* Repeat for ''k'' = 2, 3, …\n** <math> q_k \\leftarrow Aq_{k-1} \\,</math>\n** '''for''' ''j'' from 1 to ''k'' − 1\n*** <math> h_{j,k-1} \\leftarrow q_j^* q_k \\, </math>\n*** <math> q_k \\leftarrow q_k - h_{j,k-1} q_j \\, </math>\n** <math> h_{k,k-1} \\leftarrow \\|q_k\\| \\, </math>\n** <math> q_k \\leftarrow \\frac{q_k}{h_{k,k-1}} \\, </math>\n\nThe ''j''-loop projects out the component of <math>q_k</math> in the directions of <math>q_1,\\dots,q_{k-1}</math>.  This ensures the orthogonality of all the generated vectors.\n\nThe algorithm breaks down when ''q''<sub>''k''</sub> is the zero vector. This happens when the [[Minimal polynomial (linear algebra)|minimal polynomial]] of ''A'' is of degree ''k''. In most applications of the Arnoldi iteration, including the eigenvalue algorithm below and [[GMRES]], the algorithm has converged at this point.\n\nEvery step of the ''k''-loop takes one matrix-vector product and approximately 4''mk'' floating point operations.\n\nIn the programming language python:<syntaxhighlight lang=\"python3\" line=\"1\">\nimport numpy as np\ndef arnoldi_iteration(A, b, n):\n    \"\"\"Computes a basis of the (n + 1)-Krylov subspace of A: the space\n    spanned by {b, Ab, ..., A^n b}.\n\n    Input\n    A: m x m array\n    b: initial vector (length m)\n    n: dimension of Krylov subspace, must be >=1\n    \n    Returns Q, h\n    Q: m x (n + 1) array, the columns are an orthonormal basis of the\n    Krylov subspace.\n    h: (n + 1) x n array, A on basis Q. It is upper Hessenberg.  \n    \"\"\"\n    m = A.shape[0]\n\n    h = np.zeros((n + 1, n))\n    Q = np.zeros((m, n + 1))\n\n    q = b / np.linalg.norm(b)  # Normalize the input vector\n    Q[:, 0] = q  # Use it as the first Krylov vector\n\n    for k in range(n):\n        v = A.dot(q)  # Generate a new candidate vector\n        for j in range(k + 1):  # Subtract the projections on previous vectors\n            h[j, k] = np.dot(Q[:, j].conj(), v)\n            v = v - h[j, k] * Q[:, j]\n\n        h[k + 1, k] = np.linalg.norm(v)\n        eps = 1e-12  # If v is shorter than this threshold it is the zero vector\n        if h[k + 1, k] > eps:  # Add the produced vector to the list, unless\n            q = v / h[k + 1, k]  #   the zero vector is produced.\n            Q[:, k + 1] = q\n        else:  # If that happens, stop iterating.\n            return Q, h\n    return Q, h\n</syntaxhighlight>\n\n==Properties of the Arnoldi iteration==\n\nLet ''Q''<sub>''n''</sub> denote the ''m''-by-''n'' matrix formed by the first ''n'' Arnoldi vectors ''q''<sub>1</sub>, ''q''<sub>2</sub>, …, ''q''<sub>''n''</sub>, and let ''H''<sub>''n''</sub> be the (upper [[Hessenberg matrix|Hessenberg]]) matrix formed by the numbers ''h''<sub>''j'',''k''</sub> computed by the algorithm:\n:<math> H_n = Q_n^* A Q_n. \\, </math>\nWe then have\n:<math> H_n = \\begin{bmatrix}\n   h_{1,1} & h_{1,2} & h_{1,3} & \\cdots  & h_{1,n} \\\\\n   h_{2,1} & h_{2,2} & h_{2,3} & \\cdots  & h_{2,n} \\\\\n   0       & h_{3,2} & h_{3,3} & \\cdots  & h_{3,n} \\\\\n   \\vdots  & \\ddots  & \\ddots  & \\ddots  & \\vdots  \\\\\n   0       & \\cdots  & 0     & h_{n,n-1} & h_{n,n}\n\\end{bmatrix}. </math>\nThis yields an alternative interpretation of the Arnoldi iteration as a (partial) orthogonal reduction of ''A'' to Hessenberg form. The matrix ''H''<sub>''n''</sub> can be viewed as the representation in the basis formed by the Arnoldi vectors of the orthogonal projection of ''A'' onto the Krylov subspace <math>\\mathcal{K}_n</math>.\n\nThe matrix ''H''<sub>''n''</sub> can be characterized by the following optimality condition. The [[characteristic polynomial]] of ''H''<sub>''n''</sub> minimizes ||''p''(''A'')''q''<sub>1</sub>||<sub>2</sub> among all [[monic polynomial]]s of degree ''n''. This optimality problem has a unique solution if and only if the Arnoldi iteration does not break down.\n\nThe relation between the ''Q'' matrices in subsequent iterations is given by\n:<math> A Q_n = Q_{n+1} \\tilde{H}_n </math>\nwhere\n:<math> \\tilde{H}_n = \\begin{bmatrix}\n   h_{1,1} & h_{1,2} & h_{1,3} & \\cdots  & h_{1,n} \\\\\n   h_{2,1} & h_{2,2} & h_{2,3} & \\cdots  & h_{2,n} \\\\\n   0       & h_{3,2} & h_{3,3} & \\cdots  & h_{3,n} \\\\\n   \\vdots  & \\ddots  & \\ddots  & \\ddots  & \\vdots  \\\\\n   \\vdots  &         & 0       & h_{n,n-1} & h_{n,n} \\\\\n   0       & \\cdots  & \\cdots  & 0       & h_{n+1,n}\n\\end{bmatrix} </math>\nis an (''n''+1)-by-''n'' matrix formed by adding an extra row to ''H''<sub>''n''</sub>.\n\n==Finding eigenvalues with the Arnoldi iteration==\n\nThe idea of the Arnoldi iteration as an [[eigenvalue algorithm]] is to compute the eigenvalues of the orthogonal projection of ''A'' onto the Krylov subspace. This projection is represented by ''H''<sub>''n''</sub>. The eigenvalues of ''H''<sub>''n''</sub> are called the ''Ritz eigenvalues''. Since ''H''<sub>''n''</sub> is a Hessenberg matrix of modest size, its eigenvalues can be computed efficiently, for instance with the [[QR algorithm]]. This is an example of the [[Rayleigh-Ritz method]].\n\nIt is often observed in practice that some of the Ritz eigenvalues converge to eigenvalues of ''A''. Since ''H''<sub>''n''</sub> is ''n''-by-''n'', it has at most ''n'' eigenvalues, and not all eigenvalues of ''A'' can be approximated. Typically, the Ritz eigenvalues converge to the extreme eigenvalues of ''A''. This can be related to the characterization of ''H''<sub>''n''</sub> as the matrix whose characteristic polynomial minimizes ||''p''(''A'')''q''<sub>1</sub>|| in the following way. A good way to get ''p''(''A'') small is to choose the polynomial ''p'' such that ''p''(''x'') is small whenever ''x'' is an eigenvalue of ''A''. Hence, the zeros of ''p'' (and thus the Ritz eigenvalues) will be close to the eigenvalues of ''A''.\n\nHowever, the details are not fully understood yet. This is in contrast to the case where ''A'' is [[symmetric matrix|symmetric]]. In that situation, the Arnoldi iteration becomes the [[Lanczos algorithm|Lanczos iteration]], for which the theory is more complete.\n\n==Implicitly restarted Arnoldi method (IRAM)==\nDue to practical storage consideration, common implementations of Arnoldi methods typically restart after some number of iterations. One major innovation in restarting was due to Lehoucq and Sorensen who proposed the Implicitly Restarted Arnoldi Method.<ref>{{cite web |author1=R. B. Lehoucq  |author2=D. C. Sorensen  |lastauthoramp=yes |year=1996 |title=Deflation Techniques for an Implicitly Restarted Arnoldi  Iteration |publisher=SIAM |doi=10.1137/S0895479895281484 }}</ref>  They also implemented the algorithm in a freely available software package called [[ARPACK]].<ref>{{cite web |author1=R. B. Lehoucq |author2=D. C. Sorensen |author3=C. Yang |last-author-amp=yes |year=1998 |title=ARPACK Users Guide: Solution of Large-Scale Eigenvalue Problems with Implicitly Restarted Arnoldi Methods |publisher=SIAM |url=http://www.ec-securehost.com/SIAM/SE06.html |access-date=2007-06-30 |archive-url=https://web.archive.org/web/20070626080708/http://www.ec-securehost.com/SIAM/SE06.html |archive-date=2007-06-26 |dead-url=yes }}</ref>  This has spurred a number of other variations including Implicitly Restarted Lanczos method.<ref>{{cite web |author1=D. CALVETTI |author2=L. REICHEL |author3=D.C. SORENSEN  |last-author-amp=yes |year=1994 |title=An Implicitly Restarted Lanczos Method for Large Symmetric Eigenvalue Problems |publisher=ETNA |url=http://etna.mcs.kent.edu/vol.2.1994/pp1-21.dir/pp1-21.ps}}</ref><ref>{{cite web |author1=E. Kokiopoulou |author2=C. Bekas |author3=E. Gallopoulos  |last-author-amp=yes |year=2003 |title=An Implicitly Restarted Lanczos Bidiagonalization Method for Computing Smallest Singular Triplets |publisher=SIAM |url=http://www.siam.org/meetings/la03/proceedings/LA03proc.pdf }}</ref><ref>{{cite web |author=Zhongxiao Jia |year=2002 |title=The refined harmonic Arnoldi method and an implicitly restarted refined algorithm for computing interior eigenpairs of large matrices |publisher=Appl. Numer. Math. |doi=10.1016/S0168-9274(01)00132-5 }}</ref>  It also influenced how other restarted methods are analyzed.<ref>{{cite web |author=Andreas Stathopoulos and Yousef Saad and Kesheng Wu |year=1998 |title=Dynamic Thick Restarting of the Davidson,  and the Implicitly Restarted Arnoldi Methods |publisher=SIAM |doi=10.1137/S1064827596304162 }}</ref>\nTheoretical results have shown that convergence improves with an increase in the Krylov subspace dimension ''n''. However, an a-priori value of ''n'' which would lead to optimal convergence is not known. Recently a dynamic switching strategy<ref>{{cite web |author1=K.Dookhitram, R. Boojhawon |author2=M. Bhuruth  |lastauthoramp=yes |year=2009 |title=A New Method For Accelerating Arnoldi Algorithms For Large Scale Eigenproblems|publisher=Math. Comput. Simulat. |doi=10.1016/j.matcom.2009.07.009}}</ref> has been proposed which fluctuates the dimension ''n'' before each restart and thus leads to acceleration in the rate of convergence.\n\n==See also==\n\nThe [[generalized minimal residual method]] (GMRES) is a method for solving ''Ax'' = ''b'' based on Arnoldi iteration.\n\n==References==\n{{Reflist}}\n* W. E. Arnoldi, \"The principle of minimized iterations in the solution of the matrix eigenvalue problem,\" ''Quarterly of  Applied Mathematics'', volume 9, pages 17–29, 1951.\n* [[Yousef Saad]], ''Numerical Methods for Large Eigenvalue Problems'', Manchester University Press, 1992. {{ISBN|0-7190-3386-1}}.\n* Lloyd N. Trefethen and David Bau, III, ''Numerical Linear Algebra'', Society for Industrial and Applied Mathematics, 1997. {{ISBN|0-89871-361-7}}.\n* Jaschke, Leonhard: ''Preconditioned Arnoldi Methods for Systems of Nonlinear Equations''. (2004). {{ISBN|2-84976-001-3}}\n* Implementation: [[Matlab]] comes with ARPACK built-in. Both stored and implicit matrices can be analyzed through the [http://www.mathworks.com/help/techdoc/ref/eigs.html eigs()] function.\n\n{{Numerical linear algebra}}\n\n{{DEFAULTSORT:Arnoldi Iteration}}\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "Walter Edwin Arnoldi",
      "url": "https://en.wikipedia.org/wiki/Walter_Edwin_Arnoldi",
      "text": "{{more citations needed|date=June 2011}}\n'''Walter Edwin Arnoldi''' ([[New York City|New York]], December 14, 1917 – October 5, 1995) was an [[United States|American]] [[engineer]] mainly known for the [[Arnoldi iteration]], an [[eigenvalue algorithm]] used in [[numerical linear algebra]]. Arnoldi graduated in [[mechanical engineering]] at the [[Stevens Institute of Technology]] in 1937 and attended a [[Master of Science]] course at [[Harvard]]. He worked at [[United Aircraft Corp.]] from 1939 to 1977. His main research interests included modelling vibrations, [[acoustics]] and [[aerodynamics]] of [[aircraft]] [[propeller]]s. His 1951 paper ''The principle of minimized iterations in the solution of the eigenvalue problem'' is one of the most cited papers in numerical linear algebra.<ref>{{cite book|last=Meyer|first=Carl|title=Matrix analysis and applied linear algebra|publisher=SIAM|location=Philadelphia, Pa.|isbn=0-89871-454-0|pages=653}}</ref>\n\n==References==\n{{reflist}}\n\n{{authority control}}\n\n{{DEFAULTSORT:Arnoldi, Walter Edwin}}\n[[Category:20th-century American mathematicians]]\n[[Category:Numerical linear algebra]]\n[[Category:1917 births]]\n[[Category:1995 deaths]]\n[[Category:American mechanical engineers]]\n[[Category:Engineers from New York (state)]]\n[[Category:Stevens Institute of Technology alumni]]\n[[Category:Harvard University alumni]]\n[[Category:20th-century American engineers]]\n\n\n{{NewYork-bio-stub}}\n{{US-engineer-stub}}"
    },
    {
      "title": "Automatically Tuned Linear Algebra Software",
      "url": "https://en.wikipedia.org/wiki/Automatically_Tuned_Linear_Algebra_Software",
      "text": "{{Primary sources|date=November 2012}}\n{{Infobox software\n| name    = ATLAS\n| genre   = [[Software library]]\n| license = [[BSD License]]\n| website = {{URL|http://math-atlas.sourceforge.net}}\n}}\n\n'''Automatically Tuned Linear Algebra Software''' ('''ATLAS''') is a [[Library (computer science)|software library]] for [[linear algebra]]. It provides a mature [[Open-source software|open source]] implementation of [[Basic Linear Algebra Subprograms|BLAS]] [[application programming interface|APIs]] for [[C programming language|C]] and [[Fortran|Fortran77]].\n\nATLAS is often recommended as a way to automatically generate an [[Optimization (computer science)|optimized]] BLAS library. While its performance often trails that of specialized libraries written for one specific [[platform (computing)|hardware platform]], it is often the first or even only optimized BLAS implementation available on new systems and is a large improvement over the generic BLAS available at [[Netlib]]. For this reason, ATLAS is sometimes used as a performance baseline for comparison with other products.\n\nATLAS runs on most [[Unix]]-like operating systems and on [[Microsoft Windows]] (using [[Cygwin]]). It is released under a [[BSD licenses|BSD-style license]] without advertising clause, and many well-known mathematics applications including [[MATLAB]], [[Mathematica]], [[Scilab]], [[SageMath]], and some builds of [[GNU Octave]] may use it.\n\n==Functionality==\nATLAS provides a full implementation of the BLAS APIs as well as some additional functions from [[LAPACK]], a higher-level library built on top of BLAS. In BLAS, functionality is divided into three groups called levels 1, 2 and 3.\n\n* Level 1 contains ''vector operations'' of the form\n\n::<math>\\mathbf{y} \\leftarrow \\alpha \\mathbf{x} + \\mathbf{y} \\!</math>\n\n:as well as scalar [[dot product]]s and [[norm (mathematics)|vector norm]]s, among other things.\n\n* Level 2 contains ''matrix-vector operations'' of the form\n\n::<math>\\mathbf{y} \\leftarrow \\alpha A \\mathbf{x} + \\beta \\mathbf{y} \\!</math>\n\n:as well as solving <math>T \\mathbf{x} = \\mathbf{y}</math> for <math>\\mathbf{x}</math> with <math>T</math> being triangular, among other things.\n\n* Level 3 contains ''matrix-matrix operations'' such as the widely used [[General Matrix Multiply]] (GEMM) operation\n\n::<math>C \\leftarrow \\alpha A B + \\beta C \\!</math>\n\n:as well as solving <math>B \\leftarrow \\alpha T^{-1} B</math> for triangular matrices <math>T</math>, among other things.\n\n==Optimization approach==\nThe [[Optimization (computer science)|optimization]] approach is called Automated Empirical Optimization of Software (AEOS), which identifies four fundamental approaches to computer assisted optimization of which ATLAS employs three:<ref>{{cite journal\n |author1=R. Clint Whaley |author2=Antoine Petitet |author3=Jack J. Dongarra  |last-author-amp=yes | title = Automated Empirical Optimization of Software and the ATLAS Project\n | journal = Parallel Computing\n | volume = 27\n |issue=1–2 | pages = 3–35\n | year = 2001\n | doi = 10.1016/S0167-8191(00)00087-9\n | url = http://www.netlib.org/lapack/lawnspdf/lawn147.pdf\n | accessdate = 2006-10-06\n|citeseerx=10.1.1.35.2297 }}</ref>\n\n# [[Parameter (computer science)|Parameter]]ization—searching over the parameter space of a function, used for blocking factor, cache edge, etc.\n# Multiple implementation—searching through various approaches to implementing the same function, e.g., for [[Streaming SIMD Extensions|SSE]] support before intrinsics made them available in C code\n# [[Automatic programming|Code generation]]—programs that write programs incorporating what knowledge they can about what will produce the best performance for the system\n\n* Optimization of the level 1 BLAS uses parameterization and multiple implementation\n: Every ATLAS level 1 BLAS function has its own kernel. Since it would be difficult to maintain thousands of cases in ATLAS there is little architecture specific optimization for Level 1 BLAS. Instead multiple implementation is relied upon to allow for [[compiler optimization]] to produce high performance implementation for the system.\n\n* Optimization of the level 2 BLAS uses parameterization and multiple implementation\n: With <math>N^2</math> data and <math>N^2</math> operations to perform the function is usually limited by bandwidth to memory, and thus there is not much opportunity for optimization\n: All routines in the ATLAS level 2 BLAS are built from two Level 2 BLAS kernels:\n** GEMV—matrix by vector multiply update:\n::<math>\\mathbf{y} \\leftarrow \\alpha A \\mathbf{x} + \\beta \\mathbf{y} \\!</math>\n** GER—general rank 1 update from an outer product:\n::<math>A \\leftarrow \\alpha \\mathbf{x} \\mathbf{y}^T + A \\! </math>\n\n* Optimization of the level 3 BLAS uses code generation and the other two techniques\n: Since we have <math>N^3</math> ops with only <math>N^2</math> data, there are many opportunities for optimization\n\n==Level 3 BLAS==\nMost of the Level 3 BLAS is derived from [[General Matrix Multiply|GEMM]], so that is the primary focus of the optimization.\n\n:<math>O(n^3)</math> operations vs. <math>O(n^2)</math> data\n\nThe intuition that the <math>n^3</math> operations will dominate over the <math>n^2</math> data accesses only works for roughly square matrices.\nThe real measure should be some kind of surface area to volume.\nThe difference becomes important for very non-square matrices.\n\n===Can it afford to copy?===\nCopying the inputs allows the data to be arranged in a way that provides optimal access for the kernel functions, \nbut this comes at the cost of allocating temporary space, and an extra read and write of the inputs.\n\nSo the first question GEMM faces is, can it afford to copy the inputs?\n\nIf so, \n* Put into block major format with good alignment\n* Take advantage of user contributed kernels and cleanup\n* Handle the transpose cases with the copy: make everything into TN (transpose - no-transpose)\n* Deal with &alpha; in the copy\n\nIf not,\n* Use the nocopy version\n* Make no assumptions on the stride of matrix ''A'' and ''B'' in memory\n* Handle all transpose cases explicitly\n* No guarantee about alignment of data\n* Support &alpha; specific code\n* Run the risk of [[Translation Lookaside Buffer|TLB]] issues, bad strides, etc.\n\nThe actual decision is made through a simple [[Heuristic (computer science)|heuristic]] which checks for \"skinny cases\".\n\n===Cache edge===\nFor 2nd Level Cache blocking a single cache edge parameter is used.\nThe high level choose an order to traverse the blocks: ''ijk, jik, ikj, jki, kij, kji''. \nThese need not be the same order as the product is done within a block.\n\nTypically chosen orders are ''ijk'' or ''jik''.\nFor ''jik'' the ideal situation would be to copy ''A'' and the ''NB'' wide panel of ''B''. \nFor ''ijk'' swap the role of ''A'' and ''B''.\n\nChoosing the bigger of ''M'' or ''N'' for the outer loop reduces the footprint of the copy.\nBut for large ''K'' ATLAS does not even allocate such a large amount of memory.\nInstead it defines a parameter, ''Kp'', to give best use of the L2 cache. \nPanels are limited to ''Kp'' in length.\nIt first tries to allocate (in the ''jik'' case) <math>M\\cdot p + NB\\cdot Kp + NB\\cdot NB</math>.\nIf that fails it tries <math>2\\cdot Kp\\cdot NB + NB\\cdot NB</math>.\n(If that fails it uses the no-copy version of GEMM, but this case is unlikely for reasonable choices of cache edge.)\n''Kp'' is a function of cache edge and ''NB''.\n\n==LAPACK==\nWhen integrating the ATLAS BLAS with [[LAPACK]] an important consideration is the choice of blocking factor for LAPACK. If the ATLAS blocking factor is small enough the blocking factor of LAPACK could be set to match that of ATLAS.\n\nTo take advantage of recursive factorization, ATLAS provides replacement routines for some LAPACK routines. These simply overwrite the corresponding LAPACK routines from [[Netlib]].\n\n==Need for installation==\nInstalling ATLAS on a particular platform is a challenging process which is typically done by a system vendor or a local expert and made available to a wider audience.\n\nFor many systems, architectural default parameters are available; these are essentially saved searches plus the results of hand tuning. \nIf the arch defaults work they will likely get 10-15% better performance than the install search. On such systems the installation process is greatly simplified.\n\n==References==\n{{Reflist}}\n\n==External links==\n*{{sourceforge|math-atlas}}\n*[http://math-atlas.sourceforge.net/devel/atlas_contrib/ User contribution to ATLAS]\n*[http://math-atlas.sourceforge.net/devel/atlas_devel/ A Collaborative guide to ATLAS Development]\n*The [http://math-atlas.sourceforge.net/faq.html#doc FAQ] has links to the Quick reference guide to BLAS and Quick reference to ATLAS LAPACK API reference\n*[http://www.terborg.net/research/kml/installation.html Microsoft Visual C++ Howto] for ATLAS\n\n{{Numerical linear algebra}}\n\n[[Category:C libraries]]\n[[Category:Fortran libraries]]\n[[Category:Numerical linear algebra]]\n[[Category:Numerical software]]\n[[Category:Software using the BSD license]]"
    },
    {
      "title": "Backfitting algorithm",
      "url": "https://en.wikipedia.org/wiki/Backfitting_algorithm",
      "text": "In [[statistics]], the '''backfitting algorithm''' is a simple iterative procedure used to fit a [[generalized additive model]]. It was introduced in 1985 by Leo Breiman and Jerome Friedman along with generalized additive models. In most cases, the backfitting algorithm is equivalent to the [[Gauss&ndash;Seidel|Gauss&ndash;Seidel method]] algorithm for solving a certain linear system of equations.\n\n==Algorithm==\nAdditive models are a class of non-parametric regression models of the form:\n\n: <math> Y_i  = \\alpha + \\sum_{j=1}^p f_j(X_{ij}) + \\epsilon_i </math>\n\nwhere each <math>X_1, X_2, \\ldots, X_p </math> is a variable in our <math>p</math>-dimensional predictor <math>X</math>, and <math>Y</math> is our outcome variable. <math>\\epsilon</math> represents our inherent error, which is assumed to have mean zero. The <math>f_j</math> represent unspecified smooth functions of a single <math>X_j</math>. Given the flexibility in the <math>f_j</math>, we typically do not have a unique solution: <math>\\alpha</math> is left unidentifiable as one can add any constants to any of the <math>f_j</math> and subtract this value from <math>\\alpha</math>. It is common to rectify this by constraining\n\n: <math>\\sum_{i = 1}^N f_j(X_{ij}) = 0</math> for all <math>j</math>\n\nleaving\n\n: <math>\\alpha = 1/N \\sum_{i = 1}^N y_i</math>\n\nnecessarily.\n\nThe backfitting algorithm is then:\n    \t\n    '''Initialize''' <math>\\hat{\\alpha} = 1/N \\sum_{i = 1}^N y_i, \\hat{f_j} \\equiv 0</math>,<math> \\forall j</math>\n    '''Do''' until <math>\\hat{f_j}</math> converge:\n        '''For''' each predictor ''j'':\n            '''(a)''' <math> \\hat{f_j} \\leftarrow \\text{Smooth}[\\lbrace y_i - \\hat{\\alpha} - \\sum_{k \\neq j} \\hat{f_k}(x_{ik}) \\rbrace_1^N ]</math> (backfitting step)\n            '''(b)''' <math> \\hat{f_j} \\leftarrow \\hat{f_j} - 1/N \\sum_{i=1}^N \\hat{f_j}(x_{ij})</math> (mean centering of estimated function)\n\nwhere <math>\\text{Smooth}</math> is our smoothing operator. This is typically chosen to be a [[Smoothing spline|cubic spline smoother]] but can be any other appropriate fitting operation, such as:\n\n* local [[polynomial regression]]\n* [[kernel smoothing]] methods\n* more complex operators, such as surface smoothers for second and higher-order interactions\n\nIn theory, step '''(b)''' in the algorithm is not needed as the function estimates are constrained to sum to zero. However, due to numerical issues this might become a problem in practice.<ref>[[Trevor Hastie|Hastie, Trevor]], [[Robert Tibshirani]] and Jerome Friedman (2001). ''The Elements of Statistical Learning: Data Mining, Inference, and Prediction''. Springer, {{ISBN|0-387-95284-5}}.</ref>\n\n==Motivation==\nIf we consider the problem of minimizing the expected squared error:\n\n: <math>\\min E[Y - (\\alpha + \\sum_{j=1}^p f_j(X_j))]^2</math>\n\nThere exists a unique solution by the theory of projections given by:\n\n: <math>f_i(X_i) = E[Y - (\\alpha + \\sum_{j \\neq i}^p f_j(X_j)) | X_i]</math>\n\nfor ''i''&nbsp;=&nbsp;1,&nbsp;2,&nbsp;...,&nbsp;''p''.\n\nThis gives the matrix interpretation:\n\n: <math>\n\n\\begin{pmatrix}\nI & P_1 & \\cdots  & P_1 \\\\\nP_2 & I &  \\cdots  & P_2 \\\\\n\\vdots &  &  \\ddots & \\vdots \\\\\nP_p & \\cdots  & P_p & I \n\\end{pmatrix}\n\n\\begin{pmatrix}\nf_1(X_1)\\\\\nf_2(X_2)\\\\\n\\vdots \\\\\nf_p(X_p)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nP_1 Y\\\\\nP_2 Y\\\\\n\\vdots \\\\\nP_p Y\n\\end{pmatrix}\n</math>\n\nwhere <math>P_i(\\cdot) = E(\\cdot|X_i)</math>. In this context we can imagine a smoother matrix, <math>S_i</math>, which approximates our <math>P_i</math> and gives an estimate, <math>S_i Y</math>, of <math>E(Y|X)</math>\n\n: <math>\n\\begin{pmatrix}\nI & S_1 & \\cdots  & S_1 \\\\\nS_2 & I &  \\cdots  & S_2 \\\\\n\\vdots &  &  \\ddots & \\vdots \\\\\nS_p & \\cdots  & S_p & I \n\\end{pmatrix}\n\n\\begin{pmatrix}\nf_1\\\\\nf_2\\\\\n\\vdots \\\\\nf_p\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nS_1 Y\\\\\nS_2 Y\\\\\n\\vdots \\\\\nS_p Y\n\\end{pmatrix}\n\n</math>\n\nor in abbreviated form\n\n: <math> \\hat{S}f = QY \\, </math>\n\nAn exact solution of this is infeasible to calculate for large np, so the iterative technique of backfitting is used. We take initial guesses <math>f_i^{(0)}</math> and update each <math>f_i^{(j)}</math> in turn to be the smoothed fit for the residuals of all the others:\n\n: <math> \\hat{f_i}^{(j)} \\leftarrow \\text{Smooth}[\\lbrace y_i - \\hat{\\alpha} - \\sum_{k \\neq j} \\hat{f_k}(x_{ik}) \\rbrace_1^N ]</math>\n\nLooking at the abbreviated form it is easy to see the backfitting algorithm as equivalent to the [[Gauss&ndash;Seidel|Gauss&ndash;Seidel method]] for linear smoothing operators ''S''.\n\n==Explicit derivation for two dimensions==\n\nFor the two dimensional case, we can formulate the backfitting algorithm explicitly. We have:\n\n: <math> f_1 = S_1(Y-f_2), f_2 = S_2(Y-f_1) </math>\n\nIf we denote <math> \\hat{f}_1^{(i)} </math> as the estimate of <math>f_1</math> in the ''i''th updating step, the backfitting steps are\n\n: <math> \\hat{f}_1^{(i)} = S_1[Y - \\hat{f}_2^{(i-1)}], \\hat{f}_2^{(i)} = S_2[Y - \\hat{f}_1^{(i-1)}] </math>\n\nBy induction we get\n\n: <math> \\hat{f}_1^{(i)} = Y - \\sum_{\\alpha = 0}^{i-1}(S_1 S_2)^\\alpha(I-S_1)Y - (S_1 S_2)^{i -1} S_1\\hat{f}_2^{(0)} </math>\n\nand\n\n: <math> \\hat{f}_2^{(i)} = S_2 \\sum_{\\alpha = 0}^{i-1}(S_1 S_2)^\\alpha(I-S_1)Y + S_2(S_1 S_2)^{i -1} S_1\\hat{f}_2^{(0)} </math>\n\nIf we assume our constant <math>\\alpha</math> is zero and we set <math> \\hat{f}_2^{(0)}= 0</math> then we get\n\n: <math> \\hat{f}_1^{(i)} = [I - \\sum_{\\alpha = 0}^{i-1}(S_1 S_2)^\\alpha(I-S_1)]Y </math>\n\n: <math> \\hat{f}_2^{(i)} = [S_2 \\sum_{\\alpha = 0}^{i-1}(S_1 S_2)^\\alpha(I-S_1)]Y </math>\n\nThis converges if <math> \\|S_1 S_2\\| < 1 </math>.\n\n==Issues==\nThe choice of when to stop the algorithm is arbitrary and it is hard to know a priori how long reaching a specific convergence threshold will take. Also, the final model depends on the order in which the predictor variables <math>X_i</math> are fit.\n\nAs well, the solution found by the backfitting procedure is non-unique. If <math>b</math> is a vector such that <math>\\hat{S}b = 0</math> from above, then if <math>\\hat{f}</math> is a solution then so is <math>\\hat{f} + \\alpha b</math> is also a solution for any <math> \\alpha \\in \\mathbb{R}</math>. A modification of the backfitting algorithm involving projections onto the eigenspace of ''S'' can remedy this problem.\n\n==Modified algorithm==\nWe can modify the backfitting algorithm to make it easier to provide a unique solution. Let <math> \\mathcal{V}_1(S_i) </math> be the space spanned by all the eigenvectors of ''S''<sub>i</sub> that correspond to eigenvalue 1. Then any ''b'' satisfying <math>\\hat{S}b = 0</math> has <math> b_i \\in \\mathcal{V}_1(S_i) \\forall i=1,\\dots,p</math> and <math> \\sum_{i=1}^p b_i = 0.</math> Now if we take <math> A </math> to be a matrix that projects orthogonally onto <math> \\mathcal{V}_1(S_1) + \\dots + \\mathcal{V}_1(S_p) </math>, we get the following modified backfitting algorithm:\n\n    '''Initialize''' <math>\\hat{\\alpha} = 1/N \\sum_1^N y_i, \\hat{f_j} \\equiv 0</math>,<math> \\forall i, j</math>, <math>\\hat{f_+} = \\alpha + \\hat{f_1} + \\dots + \\hat{f_p} </math>\n    '''Do''' until <math>\\hat{f_j}</math> converge:\n        Regress <math> y - \\hat{f_+} </math> onto the space <math> \\mathcal{V}_1(S_i) + \\dots + \\mathcal{V}_1(S_p) </math>, setting <math> a = A(Y- \\hat{f_+})</math>\n        '''For''' each predictor ''j'':\n            Apply backfitting update to <math>(Y - a)</math> using the smoothing operator <math>(I - A_i)S_i</math>, yielding new estimates for <math>\\hat{f_j}</math>\n\n{{more footnotes|date=December 2009}}\n\n==References==\n{{Reflist}}<!--added under references heading by script-assisted edit-->\n* {{cite journal\n | doi=10.2307/2288473\n | title=Estimating optimal transformations for multiple regression and correlations (with discussion)\n | jstor=2288473\n |author1=Breiman, L.  |author2=Friedman, J. H.\n  |lastauthoramp=yes | journal=Journal of the American Statistical Association\n | volume=80 | issue = 391 \n | pages=580–619\n | year=1985\n}}\n* {{cite journal\n | title=Generalized Additive Models\n |author1=Hastie, T. J.  |author2=Tibshirani, R. J.\n  |lastauthoramp=yes | journal=Monographs on Statistics and Applied Probability\n | volume=43\n | year=1990\n}}\n* {{cite web\n |url=http://sfb649.wiwi.hu-berlin.de/fedc_homepage/xplore/ebooks/html/spm/spmhtmlnode37.html \n |title=Backfitting \n |author=Härdle, Wolfgang \n |date=June 9, 2004 \n |accessdate=2015-08-19 \n |display-authors=etal \n |deadurl=yes \n |archiveurl=https://web.archive.org/web/20150510225240/http://sfb649.wiwi.hu-berlin.de/fedc_homepage/xplore/ebooks/html/spm/spmhtmlnode37.html \n |archivedate=2015-05-10 \n |df= \n}}\n\n==External links==\n*[https://archive.is/20121211125906/http://rss.acs.unt.edu/Rdoc/library/gam/html/gam.html R Package for GAM backfitting]\n*[https://web.archive.org/web/20061121130651/http://pbil.univ-lyon1.fr/library/mda/html/bruto.html R Package for BRUTO backfitting]\n\n[[Category:Numerical linear algebra]]\n[[Category:Generalized linear models]]"
    },
    {
      "title": "Bartels–Stewart algorithm",
      "url": "https://en.wikipedia.org/wiki/Bartels%E2%80%93Stewart_algorithm",
      "text": "{{short description|Algorithm in numerical linear algebra}}\nIn [[numerical linear algebra]], the '''Bartels–Stewart algorithm''' is used to numerically solve the [[Sylvester equation|Sylvester matrix equation]] <math> AX - XB = C</math>. Developed by R.H. Bartels and G.W. Stewart in 1971<ref name=\":0\">{{Cite journal|last=Bartels|first=R. H.|last2=Stewart|first2=G. W.|date=1972|title=Solution of the matrix equation AX + XB = C [F4]|url=http://dl.acm.org/citation.cfm?id=361573.361582|journal=Communications of the ACM|volume=15|issue=9|pages=820–826|doi=10.1145/361573.361582|issn=0001-0782}}</ref>, it was the first [[numerical stability|numerically stable]] method that could by systematically applied to solve such equations. The algorithm works by using the [[Schur decomposition|real Schur decompositions]] of <math>A</math> and <math>B</math> to transform <math> AX - XB = C</math> into a triangular system that can then be solved using forward or backward substitution. In 1979, [[Gene H. Golub|G. Golub]], [[Charles F. Van Loan|C. Van Loan]] and S. Nash introduced an improved version of the algorithm<ref name=\":1\">{{Cite journal|last=Golub|first=G.|last2=Nash|first2=S.|last3=Loan|first3=C. Van|date=1979|title=A Hessenberg–Schur method for the problem AX + XB= C|url=https://ieeexplore.ieee.org/document/1102170/|journal=IEEE Transactions on Automatic Control|volume=24|issue=6|pages=909–913|doi=10.1109/TAC.1979.1102170|issn=0018-9286|hdl=1813/7472}}</ref>, known as the Hessenberg–Schur algorithm. It remains a standard approach for solving [[Sylvester equation| Sylvester equations]] when <math>X</math> is of small to moderate size.\n\n== The algorithm ==\nLet <math>X, C \\in \\mathbb{R}^{m \\times n}</math>, and assume that the eigenvalues of <math>A</math> are distinct from the eigenvalues of <math>B</math>. Then, the matrix equation <math> AX - XB = C</math> has a unique solution. The Bartels–Stewart algorithm computes <math>X</math> by applying the following steps<ref name=\":1\" />: \n\n1.Compute the [[Schur decomposition|real Schur decompositions]]\n\n: <math>R = U^TAU,</math>\n\n: <math>S = V^TB^TV.</math>\n\nThe matrices <math>R</math> and <math>S</math> are block-upper triangular matrices, with square blocks of size no greater than <math>2</math>.\n\n2. Set <math>F = U^TCV.</math>\n\n3. Solve the  simplified system <math>RY - YS^T = F</math>, where <math>Y = U^TXV</math>. This can be done using forward substitution on the blocks. Specifically, if <math>s_{k-1, k} = 0</math>, then\n\n: <math>(R - s_{kk}I)y_k = f_{k} + \\sum_{j = k+1}^n s_{kj}y_j,</math>\n\nwhere <math>y_k</math>is the <math>k</math>th column of <math>Y</math>. When <math>s_{k-1, k} \\neq 0</math>, columns <math>[ y_{k-1} \\mid y_{k}]</math>  should be concatenated and solved for simultaneously. \n\n4. Set <math>X = UYV^T.</math>\n\n=== Computational cost ===\nUsing the [[QR algorithm]], the [[Schur decomposition| real Schur decompositions]] in step 1 require approximately <math>10(m^3 + n^3)</math> flops, so that the overall computational cost is  <math>10(m^3 + n^3) + 2.5(mn^2 + nm^2)</math><ref name=\":1\" />. \n\n=== Simplifications and special cases ===\nIn the special case where <math>B=-A^T</math> and <math>C</math> is symmetric, the solution <math>X</math> will also be symmetric. This symmetry can be exploited so that <math>Y</math> is found more efficiently in step 3 of the algorithm<ref name=\":0\" />. \n\n== The Hessenberg–Schur algorithm ==\nThe Hessenberg–Schur algorithm<ref name=\":1\" /> replaces the decomposition <math>R = U^TAU</math> in step 1 with the decomposition <math>H = Q^TAQ</math>, where <math>H</math> is an [[Hessenberg matrix| upper-Hessenberg matrix]]. This leads to a system of the form <math> HY - YS^T = F</math> that can be solved using forward substitution. The advantage of this approach is that <math>H = Q^TAQ</math> can be found using [[Householder transformation| Householder reflections]] at a cost of <math>(5/3)m^3</math> flops, compared to the <math>10m^3</math> flops required to compute the real Schur decomposition of <math>A</math>. \n\n== Software and implementation ==\nThe subroutines required for the Hessenberg-Schur variant of the Bartels–Stewart  algorithm are implemented in the SLICOT library. These are used in the MATLAB control system toolbox.\n\n== Alternative approaches ==\nFor large systems, the <math>\\mathcal{O}(m^3 + n^3)</math> cost of the Bartels–Stewart algorithm can be prohibitive. When <math>A</math> and <math>B</math> are sparse or structured, so that linear solves and matrix vector multiplies involving them are efficient, iterative algorithms can potentially perform better. These include projection-based methods, which use [[Krylov subspace method|Krylov subspace]] iterations, methods based on the [[Alternating direction implicit method|alternating direction implicit]] (ADI) iteration, and hybridizations that involve both projection and ADI<ref>{{Cite journal|last=Simoncini|first=V.|date=2016|title=Computational Methods for Linear Matrix Equations|journal=SIAM Review|language=en-US|volume=58|issue=3|pages=377–441|doi=10.1137/130912839|issn=0036-1445}}</ref>.  Iterative methods can also be used to directly construct [[Low-rank approximation|low rank approximations]] to <math>X</math> when solving <math>AX-XB = C</math>. \n\n== References ==\n{{Reflist}}\n\n{{Numerical linear algebra}}\n\n[[Category:Algorithms]]\n[[Category:Control theory]]\n[[Category:Matrices]]\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "Basic Linear Algebra Subprograms",
      "url": "https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms",
      "text": "{{Infobox software\n| name                   = BLAS\n| logo                   = <!-- Image name is enough -->\n| logo alt               = \n| screenshot             = <!-- Image name is enough -->\n| caption                = \n| screenshot alt         = \n| collapsible            = \n| author                 = \n| developer              = \n| released               = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| discontinued           = \n| latest release version = 3.8.0\n| latest release date    = {{Start date and age|2017|11|12|df=yes}}\n| latest preview version = \n| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| status                 = \n| programming language   = [[Fortran]]\n| operating system       = \n| platform               = [[Cross-platform]]\n| size                   = \n| language               = \n| language count         = <!-- Number only -->\n| language footnote      = \n| genre                  = [[Library (computing)|Library]]\n| license                = \n| alexa                  = \n| website                = {{URL|http://www.netlib.org/blas/}}\n| standard               = \n| AsOf                   = \n}}\n\n'''Basic Linear Algebra Subprograms''' ('''BLAS''') is a [[specification (technical standard)|specification]] that prescribes a set of low-level routines for performing common [[linear algebra]] operations such as [[vector space|vector]] addition, [[scalar multiplication]], [[dot product]]s, linear combinations, and [[matrix multiplication]]. They are the ''[[de facto]]'' standard low-level routines for linear algebra libraries; the routines have bindings for both [[C (programming language)|C]] and [[Fortran]]. Although the BLAS specification is general, BLAS implementations are often optimized for speed on a particular machine, so using them can bring substantial performance benefits. BLAS implementations will take advantage of special floating point hardware such as vector registers or [[SIMD]] instructions.\n\nIt originated as a Fortran library in 1979<ref name=\"lawson79\">*{{cite journal |last1=Lawson |first1=C. L. |last2=Hanson |first2=R. J. |last3=Kincaid |first3=D. |last4=Krogh |first4=F. T. |title=Basic Linear Algebra Subprograms for FORTRAN usage |journal=ACM Trans. Math. Softw. |volume=5 |issue=3 |pages=308–323 |year=1979 |id=Algorithm 539 |doi=10.1145/355841.355847 |ref=harv}}</ref> and its interface was standardized by the BLAS Technical (BLAST) Forum, whose latest BLAS report can be found on the [[netlib]] website.<ref>{{Cite web|url=http://netlib.org/blas/blast-forum|title=BLAS Technical Forum|website=netlib.org|access-date=2017-07-07}}</ref> This Fortran library is known as the ''[[reference implementation]]'' (sometimes confusingly referred to as ''the'' BLAS library) and is not optimized for speed but is in the [[public domain]].<ref>[http://www.lahey.com/docs/blaseman_lin62.pdf blaseman] {{webarchive |url=https://web.archive.org/web/20161012014431/http://www.lahey.com/docs/blaseman_lin62.pdf |date=October 12, 2016 }} ''\"The products are the implementations of the public domain BLAS (Basic Linear Algebra Subprograms) and LAPACK (Linear Algebra PACKage), which have been developed by groups of people such as Prof. Jack Dongarra, University of Tennessee, USA and all published on the WWW (URL: http://www.netlib.org/).\"''{{dead link|date=October 2016 |bot=InternetArchiveBot |fix-attempted=yes }}</ref><ref>{{cite web|url=http://www.netlib.org/utk/people/JackDongarra/PAPERS/netlib-history6.pdf|title=Netlib and NA-Net: building a scientific computing community |author=Jack Dongarra |author2=Gene Golub |author3=Eric Grosse |author4=Cleve Moler |author5=Keith Moore |quote=''The Netlib software repository was created in 1984 to facilitate quick distribution of public domain software routines for use in scientific computation.'' |publisher=netlib.org |date=|accessdate=2016-02-13}}</ref>\n\nMost libraries that offer linear algebra routines conform to the BLAS interface, allowing library users to develop programs that are agnostic of the BLAS library being used. Examples of BLAS libraries include: [[AMD Core Math Library]] (ACML), [[Automatically Tuned Linear Algebra Software|ATLAS]], [[Intel Math Kernel Library]] (MKL), and [[OpenBLAS]]. ACML is no longer supported by its producer.<ref>{{cite web |year=2013 |title=ACML – AMD Core Math Library |publisher=[[AMD]] |url=http://developer.amd.com/tools-and-sdks/archive/amd-core-math-library-acml/ |accessdate=26 August 2015 |deadurl=yes |archiveurl=https://web.archive.org/web/20150905190558/http://developer.amd.com/tools-and-sdks/archive/amd-core-math-library-acml/ |archivedate=5 September 2015 |df= }}</ref> ATLAS is a portable library that automatically optimizes itself for an arbitrary architecture. MKL is a freeware<ref name=\"MKLfree\">{{cite web |year=2015 |title=No Cost Options for Intel Math Kernel Library (MKL), Support yourself, Royalty-Free |publisher=[[Intel]] |url=http://software.intel.com/articles/free_mkl |accessdate=31 August 2015}}</ref> and proprietary<ref name=\"MKLintel\">{{cite web |year=2015 |title=Intel® Math Kernel Library (Intel® MKL) |publisher=[[Intel]] |url=http://software.intel.com/intel-mkl |accessdate=25 August 2015}}</ref> vendor library optimized for x86 and x86-64 with a performance emphasis on [[Intel]] processors.<ref name=\"optnotice\">{{cite web |year=2012 |title=Optimization Notice |publisher=[[Intel]] |url=http://software.intel.com/articles/optimization-notice |accessdate=10 April 2013}}</ref> OpenBLAS is an open-source library that is hand-optimized for many of the popular architectures. The [[LINPACK benchmarks]] rely heavily on the BLAS routine <code>[[General Matrix Multiply|gemm]]</code> for its performance measurements.\n\nMany numerical software applications use BLAS-compatible libraries to do linear algebra computations, including [[Armadillo (C++ library)|Armadillo]], [[LAPACK]], [[LINPACK]], [[GNU Octave]], [[Mathematica]],<ref>{{cite journal|author=Douglas Quinney |year=2003 |title=So what's new in Mathematica 5.0? |journal=MSOR Connections |volume=3 |number=4 |publisher=The Higher Education Academy |url=http://78.158.56.101/archive/msor/headocs/34mathematica5.pdf |deadurl=yes |archiveurl=https://web.archive.org/web/20131029204826/http://78.158.56.101/archive/msor/headocs/34mathematica5.pdf |archivedate=2013-10-29 |df= }}</ref> [[MATLAB]],<ref>{{cite web |author=Cleve Moler |year=2000 |title=MATLAB Incorporates LAPACK |publisher=[[MathWorks]] |url=http://www.mathworks.com/company/newsletters/articles/matlab-incorporates-lapack.html |accessdate=26 October 2013}}</ref> [[NumPy]],<ref name=\"cise\">{{cite journal |title=The NumPy array: a structure for efficient numerical computation |author=Stéfan van der Walt |author2=S. Chris Colbert |author3=Gaël Varoquaux |last-author-amp=yes |year=2011 |journal=Computing in Science and Engineering |volume=13 |issue=2 |pages=22–30 |arxiv=1102.1523|bibcode=2011arXiv1102.1523V |doi=10.1109/MCSE.2011.37 }}</ref> [[R (programming language)|R]], and [[Julia (programming language)|Julia]].\n\n==Background==\nWith the advent of numerical programming, sophisticated subroutine libraries became useful.  These libraries would contain subroutines for common high-level mathematical operations such as root finding, matrix inversion, and solving systems of equations. The language of choice was [[FORTRAN]]. The most prominent numerical programming library was [[IBM]]'s [[Scientific Subroutine Package]] (SSP).<ref>{{Cite journal | last1 = Boisvert | first1 = Ronald F. | year = 2000 | title = Mathematical software: past, present, and future | journal = Mathematics and Computers in Simulation | volume = 54 | issue = 4–5 | pages = 227–241 | publisher =  | jstor =  | doi = 10.1016/S0378-4754(00)00185-3 | url =  | arxiv = cs/0004004}}</ref>  These subroutine libraries allowed programmers to concentrate on their specific problems and avoid re-implementing well-known algorithms.  The library routines would also be better than average implementations; matrix algorithms, for example, might use full pivoting to get better numerical accuracy. The library routines would also have more efficient routines. For example, a library may include a program to solve a matrix that is upper triangular. The libraries would include single-precision and double-precision versions of some algorithms.\n\nInitially, these subroutines used hard-coded loops for their low-level operations. For example, if a subroutine need to perform a matrix multiplication, then the subroutine would have three nested loops. Linear algebra programs have many common low-level operations (the so-called \"kernel\" operations, not related to [[kernel (operating system)|operating system]]s).<ref>Even the SSP (which appeared around 1966) had some basic routines such as RADD (add rows), CADD (add columns), SRMA (scale row and add to another row), and RINT (row interchange). These routines apparently were not used as kernel operations to implement other routines such as matrix inversion.  See {{Citation |last=IBM |title=System/360 Scientific Subroutine Package, Version III, Programmer's Manual |edition=5th |publisher=International Business Machines |year=1970 |id=GH20-0205-4}}.</ref> Between 1973 and 1977, several of these kernel operations were identified.{{sfn|BLAST Forum|2001|p=1}}  These kernel operations became defined subroutines that math libraries could call.  The kernel calls had advantages over hard-coded loops: the library routine would be more readable, there were fewer chances for bugs, and the kernel implementation could be optimized for speed.  A specification for these kernel operations using [[scalar (mathematics)|scalars]] and [[vector space|vector]]s, the level-1 Basic Linear Algebra Subroutines (BLAS), was published in 1979.{{sfn|Lawson|Hanson|Kincaid|Krogh|1979}}  BLAS was used to implement the linear algebra subroutine library [[LINPACK]].\n\nThe BLAS abstraction allows customization for high performance. For example, LINPACK is a general purpose library that can be used on many different machines without modification. LINPACK could use a generic version of BLAS. To gain performance, different machines might use tailored versions of BLAS. As computer architectures became more sophisticated, [[vector processor|vector machines]] appeared. BLAS for a vector machine could use the machine's fast vector operations.  (While vector processors eventually fell out of favor, vector instructions in modern CPUs are essential for optimal performance in BLAS routines.)\n\nOther machine features became available and could also be exploited.  Consequently, BLAS was augmented from 1984 to 1986 with level-2 kernel operations that concerned vector-matrix operations.  Memory hierarchy was also recognized as something to exploit. Many computers have [[cache memory]] that is much faster than main memory; keeping matrix manipulations localized allows better usage of the cache. In 1987 and 1988, the level 3 BLAS were identified to do matrix-matrix operations. The level 3 BLAS encouraged block-partitioned algorithms. The [[LAPACK]] library uses level 3 BLAS.{{sfn|BLAST Forum|2001|pp=1&ndash;2}}\n\nThe original BLAS concerned only densely stored vectors and matrices. Further extensions to BLAS, such as for sparse matrices, have been addressed.{{sfn|BLAST Forum|2001|p=2}}\n\n===ATLAS===\n[[Automatically Tuned Linear Algebra Software]] (ATLAS) attempts to make a BLAS implementation with higher performance.  ATLAS defines many BLAS operations in terms of some core routines and then tries to automatically tailor the core routines to have good performance. A search is performed to choose good block sizes. The block sizes may depend on the computer's cache size and architecture.  Tests are also made to see if copying arrays and vectors improves performance. For example, it may be advantageous to copy arguments so that they are cache-line aligned so user-supplied routines can use [[SIMD]] instructions.\n\n==Functionality==\nBLAS functionality is categorized into three sets of routines called \"levels\", which correspond to both the chronological order of definition and publication, as well as the degree of the polynomial in the complexities of algorithms; Level 1 BLAS operations typically take [[linear time]], {{math|''O''(''n'')}}, Level 2 operations quadratic time and Level 3 operations cubic time.{{r|level3}} Modern BLAS implementations typically provide all three levels.\n\n===Level 1===\nThis level consists of all the routines described in the original presentation of BLAS (1979),<ref name=\"lawson79\"/> which defined only ''vector operations'' on [[Stride of an array|strided arrays]]: [[dot product]]s, [[norm (mathematics)|vector norms]], a generalized vector addition of the form\n\n:<math>\\boldsymbol{y} \\leftarrow \\alpha \\boldsymbol{x} + \\boldsymbol{y}</math>\n\n(called \"axpy\") and several other operations.\n\n===Level 2===\nThis level contains ''matrix-vector operations'' including, among other things, a generalized matrix-vector multiplication ({{mono|gemv}}):\n\n:<math>\\boldsymbol{y} \\leftarrow \\alpha \\boldsymbol{A} \\boldsymbol{x} + \\beta \\boldsymbol{y}</math>\n\nas well as a solver for {{math|'''''x'''''}} in the linear equation\n\n:<math>\\boldsymbol{T} \\boldsymbol{x} = \\boldsymbol{y}</math>\n\nwith {{math|'''''T'''''}} being triangular. Design of the Level 2 BLAS started in 1984, with results published in 1988.<ref name=\"dongarra88\"/> The Level 2 subroutines are especially intended to improve performance of programs using BLAS on [[vector processor]]s, where Level 1 BLAS are suboptimal \"because they hide the matrix-vector nature of the operations from the compiler.\"<ref name=\"dongarra88\">{{cite journal |first1=Jack J. |last1=Dongarra |first2=Jeremy |last2=Du Croz |first3=Sven |last3=Hammarling |first4=Richard J. |last4=Hanson |title=An extended set of FORTRAN Basic Linear Algebra Subprograms |journal=ACM Trans. Math. Softw. |volume=14 |year=1988 |pages=1–17 |doi=10.1145/42288.42291|citeseerx=10.1.1.17.5421 }}</ref>\n\n===Level 3===\nThis level, formally published in 1990,<ref name=\"level3\">{{Cite journal |last1=Dongarra |first1=Jack J. |last2=Du Croz |first2=Jeremy |last3=Hammarling |first3=Sven |last4=Duff |first4=Iain S. |title=A set of level 3 basic linear algebra subprograms |doi=10.1145/77626.79170 |year=1990 |journal=[[ACM Transactions on Mathematical Software]] |issn=0098-3500 |volume=16 |issue=1 |pages=1–17}}</ref> contains ''matrix-matrix operations'', including a \"general [[matrix multiplication]]\" (<code>gemm</code>), of the form\n\n:<math>\\boldsymbol{C} \\leftarrow \\alpha \\boldsymbol{A} \\boldsymbol{B} + \\beta \\boldsymbol{C}</math>\n\nwhere {{math|'''''A'''''}} and {{math|'''''B'''''}} can optionally be [[transpose]]d or [[hermitian conjugate|hermitian-conjugated]] inside the routine and all three matrices may be strided. The ordinary matrix multiplication {{math|'''''A B'''''}} can be performed by setting {{math|''α''}} to one and {{math|'''''C'''''}} to an all-zeros matrix of the appropriate size.\n\nAlso included in Level 3 are routines for solving\n\n:<math>\\boldsymbol{B} \\leftarrow \\alpha \\boldsymbol{T}^{-1} \\boldsymbol{B}</math>\n\nwhere {{math|'''''T'''''}} is a triangular matrix, among other functionality.\n\nDue to the ubiquity of matrix multiplications in many scientific applications, including for the implementation of the rest of Level 3 BLAS,<ref>{{cite journal |last1=Goto |first1=Kazushige |first2=Robert |last2=van de Geijn |title=High-performance implementation of the level-3 BLAS |journal=ACM Transactions on Mathematical Software |volume=35 |pages=1–14 |number=1 |year=2008 |doi=10.1145/1377603.1377607 |url=ftp://ftp.cs.utexas.edu/pub/techreports/tr06-23.pdf}}</ref> and because faster algorithms exist beyond the obvious repetition of matrix-vector multiplication, <code>gemm</code> is a prime target of optimization for BLAS implementers. E.g., by decomposing one or both of {{math|'''''A'''''}}, {{math|'''''B'''''}} into [[Block matrix|block matrices]], <code>gemm</code> can be [[Matrix multiplication algorithm#Divide and conquer algorithm|implemented recursively]]. This is one of the motivations for including the {{math|''β''}} parameter,{{dubious|Reason for beta parameter|date=January 2015}} so the results of previous blocks can be accumulated. Note that this decomposition requires the special case {{math|''β'' {{=}} 1}} which many implementations optimize for, thereby eliminating one multiplication for each value of {{math|'''''C'''''}}. This decomposition allows for better [[locality of reference]] both in space and time of the data used in the product. This, in turn, takes advantage of the [[CPU cache|cache]] on the system.<ref>{{Citation | last1=Golub | first1=Gene H. | author1-link=Gene H. Golub | last2=Van Loan | first2=Charles F. | author2-link=Charles F. Van Loan | title=Matrix Computations | publisher=Johns Hopkins | edition=3rd | isbn=978-0-8018-5414-9 | year=1996}}</ref> For systems with more than one level of cache, the blocking can be applied a second time to the order in which the blocks are used in the computation. Both of these levels of optimization are used in implementations such as [[Automatically Tuned Linear Algebra Software|ATLAS]]. More recently, implementations by [[Kazushige Goto]] have shown that blocking only for the [[L2 cache]], combined with careful [[amortized analysis|amortizing]] of copying to contiguous memory to reduce [[translation lookaside buffer|TLB]] misses, is superior to [[Automatically Tuned Linear Algebra Software|ATLAS]].<ref>{{Cite journal |title=Anatomy of High-performance Matrix Multiplication |doi=10.1145/1356052.1356053 |journal=ACM Trans. Math. Softw. |year=2008 |issn=0098-3500 |pages=12:1–12:25 |volume=34 |issue=3 |first1=Kazushige |last1=Goto |first2=Robert A. |last2=van de Geijn|citeseerx=10.1.1.111.3873 }}</ref> A highly tuned implementation based on these ideas is part of the [[GotoBLAS]], [[OpenBLAS]] and [[BLIS (software)|BLIS]].\n\n==Implementations==\n;Accelerate: [[Apple Inc.|Apple]]'s framework for [[macOS]] and [[IOS (Apple)|iOS]], which includes tuned versions of BLAS and LAPACK.<ref>{{Cite web|url=https://developer.apple.com/library/mac/#releasenotes/Performance/RN-vecLib/|title=Guides and Sample Code|website=developer.apple.com|access-date=2017-07-07}}</ref><ref>{{Cite web|url=https://developer.apple.com/library/ios/#documentation/Accelerate/Reference/AccelerateFWRef/|title=Guides and Sample Code|website=developer.apple.com|access-date=2017-07-07}}</ref>\n;ACML: The [[AMD Core Math Library]], supporting the AMD [[Athlon]] and [[Opteron]] CPUs under [[Linux]] and [[Microsoft Windows|Windows]].<ref>{{cite web |url=http://developer.amd.com/acml.aspx |title=Archived copy |accessdate=2005-10-26 |deadurl=yes |archiveurl=https://web.archive.org/web/20051130022536/http://developer.amd.com/acml.aspx |archivedate=2005-11-30 |df= }}</ref>\n;C++ AMP BLAS: The [[C++ AMP]] BLAS Library is an [[Open-source software|open source]] implementation of BLAS for Microsoft's AMP language extension for Visual C++.<ref>{{Cite web|url=http://ampblas.codeplex.com/|title=C&#43;&#43; AMP BLAS Library|website=CodePlex|language=en|access-date=2017-07-07}}</ref>\n;ATLAS: [[Automatically Tuned Linear Algebra Software]], an [[Open-source software|open source]] implementation of BLAS [[application programming interface|API]]s for [[C (programming language)|C]] and [[Fortran|Fortran 77]].<ref>{{Cite web|url=http://math-atlas.sourceforge.net/|title=Automatically Tuned Linear Algebra Software (ATLAS)|website=math-atlas.sourceforge.net|access-date=2017-07-07}}</ref>\n;BLIS:  BLAS-like Library Instantiation Software framework for rapid instantiation.<ref>{{Citation|title=blis: BLAS-like Library Instantiation Software Framework|date=2017-06-30|url=https://github.com/flame/blis|publisher=flame|accessdate=2017-07-07}}</ref>\n;cuBLAS:Optimized BLAS for NVIDIA based GPU cards, requiring few additional library calls.<ref>{{Cite news|url=http://developer.nvidia.com/cublas|title=cuBLAS|date=2013-07-29|work=NVIDIA Developer|access-date=2017-07-07|language=en}}</ref>\n;NVBLAS:Optimized BLAS for NVIDIA based GPU cards, providing only Level 3 functions, but as direct drop-in replacement for other BLAS libraries.<ref>{{Cite news|url=https://docs.nvidia.com/cuda/nvblas/index.htmls|title=NVBLAS|date=2018-05-15|work=NVIDIA Developer|access-date=2018-05-15|language=en}}</ref>\n;clBLAS: An [[OpenCL]] implementation of BLAS.<ref name=\"github.com\">{{Citation|title=clBLAS: a software library containing BLAS functions written in OpenCL|date=2017-07-03|url=https://github.com/clMathLibraries/clBLAS|publisher=clMathLibraries|accessdate=2017-07-07}}</ref>\n;clBLAST: A tuned [[OpenCL]] implementation of BLAS.<ref name=\"https://github.com/CNugteren/CLBlast\">{{Citation|last=Nugteren|first=Cedric|title=CLBlast: Tuned OpenCL BLAS|date=2017-07-05|url=https://github.com/CNugteren/CLBlast|accessdate=2017-07-07}}</ref>\n;Eigen BLAS: A [[Fortran|Fortran 77]] and [[C (programming language)|C]] BLAS library implemented on top of the [[Mozilla license|MPL]]-licensed [[Eigen (C++ library)|Eigen library]], supporting [[x86]], [[x86 64]], [[ARM architecture|ARM (NEON)]], and [[PowerPC]] architectures.[http://eigen.tuxfamily.org] (Note: as of Eigen 3.0.3, the BLAS interface is not built by default and the documentation refers to it as \"a work in progress which is far to be ready for use\".)\n;ESSL: [[IBM]]'s Engineering and Scientific Subroutine Library, supporting the [[PowerPC]] architecture under [[AIX operating system|AIX]] and [[Linux]].<ref>http://publib.boulder.ibm.com/infocenter/clresctr/index.jsp?topic=/com.ibm.cluster.essl.doc/esslbooks.html{{dead link|date=July 2017 |bot=InternetArchiveBot |fix-attempted=yes }}</ref>\n;[[GotoBLAS]]: [[Kazushige Goto]]'s BSD-licensed implementation of BLAS, tuned in particular for [[Intel]] [[Nehalem (microarchitecture)|Nehalem]]/[[Intel Atom|Atom]], [[VIA Technologies|VIA]] [[VIA Nano|Nanoprocessor]], [[AMD]] [[Opteron]].<ref>{{cite web |url=http://www.tacc.utexas.edu/tacc-projects/gotoblas2/ |title=Archived copy |accessdate=2012-05-24 |deadurl=yes |archiveurl=https://web.archive.org/web/20120517132718/http://www.tacc.utexas.edu/tacc-projects/gotoblas2 |archivedate=2012-05-17 |df= }}</ref>\n;HP MLIB: [[Hewlett-Packard|HP]]'s Math library supporting [[IA-64]], [[PA-RISC]], [[x86]] and [[Opteron]] architecture under [[HPUX]] and [[Linux]].\n;Intel MKL: The [[Intel]] [[Math Kernel Library]], supporting x86 32-bits and 64-bits, available free from [[Intel]].<ref name=\"MKLfree\" /> Includes optimizations for Intel [[Pentium (brand)|Pentium]], [[Intel Core|Core]] and Intel [[Xeon]] CPUs and Intel [[Xeon Phi]]; support for [[Linux]], [[Microsoft Windows|Windows]] and [[macOS]].<ref>{{Cite web|url=http://software.intel.com/en-us/intel-mkl/|title=Intel® Math Kernel Library (Intel® MKL) {{!}} Intel® Software|website=software.intel.com|language=en|access-date=2017-07-07}}</ref>\n;MathKeisan: [[NEC Corporation|NEC]]'s math library, supporting [[NEC SX architecture]] under [[SUPER-UX]], and [[Itanium]] under [[Linux]]<ref>{{Cite web|url=http://www.mathkeisan.com/|title=MathKeisan|last=Mathkeisan|first=NEC|website=www.mathkeisan.com|language=en|access-date=2017-07-07}}</ref>\n;Netlib BLAS: The official reference implementation on [[Netlib]], written in [[Fortran|Fortran 77]].<ref>{{Cite web|url=http://www.netlib.org/blas/|title=BLAS (Basic Linear Algebra Subprograms)|website=www.netlib.org|access-date=2017-07-07}}</ref>\n;Netlib CBLAS: Reference [[C (programming language)|C]] interface to the BLAS. It is also possible (and popular) to call the Fortran BLAS from C.<ref>{{Cite web|url=http://www.netlib.org/blas|title=BLAS (Basic Linear Algebra Subprograms)|website=www.netlib.org|access-date=2017-07-07}}</ref>\n;[[OpenBLAS]]: Optimized BLAS based on GotoBLAS, supporting [[x86]], [[x86-64]], [[MIPS architecture|MIPS]] and [[ARM architecture|ARM]] processors.<ref>{{Cite web|url=http://www.openblas.net/|title=OpenBLAS : An optimized BLAS library|website=www.openblas.net|access-date=2017-07-07}}</ref>\n;PDLIB/SX: [[NEC Corporation|NEC]]'s Public Domain Mathematical Library for the NEC [[NEC SX architecture|SX-4]] system.<ref>{{cite web |url=http://www.nec.co.jp/hpc/mediator/sxm_e/software/61.html |title=Archived copy |accessdate=2007-05-20 |deadurl=yes |archiveurl=https://web.archive.org/web/20070222154031/http://www.nec.co.jp/hpc/mediator/sxm_e/software/61.html |archivedate=2007-02-22 |df= }}</ref>\n;SCSL: [[Silicon Graphics|SGI]]'s Scientific Computing Software Library contains BLAS and LAPACK implementations for SGI's [[Irix]] workstations.<ref>{{cite web |url=http://www.sgi.com/products/software/scsl.html |title=Archived copy |accessdate=2007-05-20 |deadurl=yes |archiveurl=https://web.archive.org/web/20070513173030/http://www.sgi.com/products/software/scsl.html |archivedate=2007-05-13 |df= }}</ref>\n;Sun Performance Library: Optimized BLAS and LAPACK for [[SPARC]], [[Intel Core|Core]] and [[AMD64]] architectures under Solaris 8, 9, and 10 as well as Linux.<ref>{{Cite web|url=http://www.oracle.com/technetwork/server-storage/solarisstudio/overview/index.html|title=Oracle Developer Studio|website=www.oracle.com|access-date=2017-07-07}}</ref>\n\n==Similar libraries but not compatible with BLAS==\n;Armadillo: [[Armadillo (C++ library)|Armadillo]] is a C++ linear algebra library aiming towards a good balance between speed and ease of use. It employs template classes, and has optional links to BLAS/ATLAS and LAPACK. It is sponsored by NICTA (in Australia) and is licensed under a free license.<ref>{{Cite web|url=http://arma.sourceforge.net/|title=Armadillo: C++ linear algebra library|website=arma.sourceforge.net|access-date=2017-07-07}}</ref>\n;ACL:  AMD Compute Libraries <ref>{{cite web |url=http://developer.amd.com/tools-and-sdks/opencl-zone/acl-amd-compute-libraries/ |title=Archived copy |accessdate=2016-10-25 |deadurl=yes |archiveurl=https://web.archive.org/web/20161116145528/http://developer.amd.com/tools-and-sdks/opencl-zone/acl-amd-compute-libraries/ |archivedate=2016-11-16 |df= }}</ref>\n* clBLAS: complete set of BLAS level 1, 2 & 3 routines <ref name=\"github.com\"/> \n* clSparse:<ref>{{Citation|title=clSPARSE: a software library containing Sparse functions written in OpenCL|date=2017-07-03|url=https://github.com/clMathLibraries/clSPARSE|publisher=clMathLibraries|accessdate=2017-07-07}}</ref>  Routines for thin Matrix\n* clFFT:<ref>{{Citation|title=clFFT: a software library containing FFT functions written in OpenCL|date=2017-07-06|url=https://github.com/clMathLibraries/clFFT|publisher=clMathLibraries|accessdate=2017-07-07}}</ref> Fast Fourier Transform\n* clRNG:<ref>{{Citation|title=clRNG: an OpenCL based software library containing random number generation functions|date=2017-06-25|url=https://github.com/clMathLibraries/clRNG|publisher=clMathLibraries|accessdate=2017-07-07}}</ref> Random Generators MRG31k3p, MRG32k3a, LFSR113 und Philox-4×32-10\n\n;CUDA SDK: The NVIDIA [[CUDA]] SDK includes BLAS functionality for writing C programs that runs on [[GeForce 8 Series]] (Tesla-Architecture) or newer graphics cards. The library cuBLAS has been designed with the purpose of implementing the BLAS capabilities using the CUDA SDK.\n;Eigen: The Eigen template library provides an easy to use highly generic C++98 template interface to matrix/vector operations and related algorithms like solving algorithms, decompositions etc. It uses vector capabilities and is optimized for both fixed size and dynamic sized and sparse matrices.<ref>{{Cite web|url=http://eigen.tuxfamily.org|title=Eigen|website=eigen.tuxfamily.org|language=en|access-date=2017-07-07}}</ref>\n;Elemental: Elemental is an open source software for [[distributed memory|distributed-memory]] dense and sparse-direct linear algebra and optimization.<ref>{{Cite web|url=http://libelemental.org/|title=Elemental: distributed-memory dense and sparse-direct linear algebra and optimization — Elemental|website=libelemental.org|access-date=2017-07-07}}</ref>\n;GSL: The [[GNU Scientific Library]] Contains a multi-platform implementation in C which is distributed under the [[GNU]] [[General Public License]].\n;HASEM: is a C++ template library, being able to solve linear equations and to compute eigenvalues.  It is licensed under BSD License.<ref>{{Cite web|url=http://sourceforge.net/projects/hasem/|title=HASEM|website=SourceForge|language=en|access-date=2017-07-07}}</ref>\n;LAMA: The Library for Accelerated Math Applications ([[Library for Accelerated Math Applications|LAMA]]) is a C++ template library for writing numerical solvers targeting various kinds of hardware (e.g. [[GPU]]s through [[CUDA]] or [[OpenCL]]) on [[distributed memory]] systems, hiding the hardware specific programming from the program developer\n;Libflame: FLAME project implementation of dense linear algebra library<ref>{{cite web |url=http://z.cs.utexas.edu/wiki/flame.wiki/FrontPage |title=Archived copy |accessdate=2011-02-21 |deadurl=yes |archiveurl=https://web.archive.org/web/20100803003649/http://z.cs.utexas.edu/wiki/flame.wiki/FrontPage |archivedate=2010-08-03 |df= }}</ref>\n;MAGMA: Matrix Algebra on GPU and Multicore Architectures (MAGMA) project develops a dense linear algebra library similar to LAPACK but for heterogeneous and hybrid architectures including multicore systems accelerated with [[general-purpose computing on graphics processing units]].<ref>http://icl.eecs.utk.edu/magma/</ref>\n;Mir\n: An [[LLVM]]-accelerated generic numerical library for science and machine learning written in [[D (programming language)|D]]. It provides generic linear algebra subprograms (GLAS).<ref>{{Cite web|url=https://github.com/libmir|title= Dlang Numerical and System Libraries|last=|first=|date=|website=|publisher=|access-date=}}</ref>\n;MTL4: The [[Matrix Template Library]] version 4 is a generic [[C++]] template library providing sparse and dense BLAS functionality. MTL4 establishes an intuitive interface (similar to [[MATLAB]]) and broad applicability thanks to [[generic programming]].\n;PLASMA: [[The Parallel Linear Algebra for Scalable Multi-core Architectures]] (PLASMA) project is a modern replacement of LAPACK for multi-core architectures. PLASMA is a software framework for development of asynchronous operations and features out of order scheduling with a runtime scheduler called QUARK that may be used for any code that expresses its dependencies with a [[directed acyclic graph]].<ref>{{Cite web|url=http://icl.eecs.utk.edu/|title=ICL|website=icl.eecs.utk.edu|language=en|access-date=2017-07-07}}</ref>\n;uBLAS: A generic [[C++]] template class library providing BLAS functionality. Part of the [[Boost library]]. It provides bindings to many hardware-accelerated libraries in a unifying notation. Moreover, uBLAS focuses on correctness of the algorithms using advanced C++ features.<ref>{{Cite web|url=http://www.boost.org/doc/libs/1_60_0/libs/numeric/ublas/doc/index.html|title=Boost Basic Linear Algebra - 1.60.0|website=www.boost.org|access-date=2017-07-07}}</ref>\n\n==Sparse BLAS==\nSeveral extensions to BLAS for handling [[Sparse matrix|sparse matrices]] have been suggested over the course of the library's history; a small set of sparse matrix kernel routines were finally standardized in 2002.<ref>{{cite journal |first1=Iain S. |last1=Duff |first2=Michael A. |last2=Heroux |first3=Roldan |last3=Pozo |title=An Overview of the Sparse Basic Linear Algebra Subprograms: The New Standard from the BLAS Technical Forum |journal=TOMS |year=2002 |volume=28 |issue=2 |pages=239–267 |doi=10.1145/567806.567810}}</ref>\n\n==See also==\n*[[List of numerical libraries]]\n*[[Math Kernel Library]], math library optimized for the [[Intel]] architecture; includes BLAS, LAPACK\n*[[Numerical linear algebra]], the type of problem BLAS solves\n\n==References==\n{{reflist|30em}}\n*{{Citation |author=BLAST Forum |title=Basic Linear Algebra Subprograms Technical (BLAST) Forum Standard |date=21 August 2001 |publisher=University of Tennessee |location=Knoxville, TN |url= |doi= }}\n*{{Citation |last1=Dodson |first1= D. S. |last2=Grimes |first2=R. G. |title=Remark on algorithm 539: Basic Linear Algebra Subprograms for Fortran usage |journal=ACM Trans. Math. Softw. |volume=8 |issue= 4 |pages=403&ndash;404 |year=1982 |doi= 10.1145/356012.356020}}\n*{{Citation |last=Dodson |first=D. S. |title=Corrigendum: Remark on \"Algorithm 539: Basic Linear Algebra Subroutines for FORTRAN usage\" |journal=ACM Trans. Math. Softw. |volume=9 |page=140 |year=1983 |doi= 10.1145/356022.356032}}\n*J. J. Dongarra, J. Du Croz, S. Hammarling, and R. J. Hanson, Algorithm 656: An extended set of FORTRAN Basic Linear Algebra Subprograms, ACM Trans. Math. Softw., 14 (1988), pp.&nbsp;18&ndash;32.\n*J. J. Dongarra, J. Du Croz, I. S. Duff, and S. Hammarling, A set of Level 3 Basic Linear Algebra Subprograms, ACM Trans. Math. Softw., 16 (1990), pp.&nbsp;1&ndash;17.\n*J. J. Dongarra, J. Du Croz, I. S. Duff, and S. Hammarling, Algorithm 679: A set of Level 3 Basic Linear Algebra Subprograms, ACM Trans. Math. Softw., 16 (1990), pp.&nbsp;18&ndash;28.\n\n;New BLAS\n*L. S. Blackford, J. Demmel, J. Dongarra, I. Duff, S. Hammarling, G. Henry, M. Heroux, L. Kaufman, A. Lumsdaine, A. Petitet, R. Pozo, K. Remington, R. C. Whaley, An Updated Set of Basic Linear Algebra Subprograms (BLAS), ACM Trans. Math. Softw., 28-2 (2002), pp.&nbsp;135&ndash;151.\n*J. Dongarra, Basic Linear Algebra Subprograms Technical Forum Standard, International Journal of High Performance Applications and Supercomputing, 16(1) (2002), pp.&nbsp;1&ndash;111, and International Journal of High Performance Applications and Supercomputing, 16(2) (2002), pp.&nbsp;115&ndash;199.\n\n==External links==\n*[http://www.netlib.org/blas/ BLAS homepage] on Netlib.org\n*[http://www.netlib.org/blas/faq.html BLAS FAQ]\n*[http://www.netlib.org/lapack/lug/node145.html BLAS Quick Reference Guide] from LAPACK Users' Guide\n* [https://web.archive.org/web/20061009230911/http://history.siam.org/oralhistories/lawson.htm Lawson Oral History] One of the original authors of the BLAS discusses its creation in an oral history interview. Charles L. Lawson Oral history interview by Thomas Haigh, 6 and 7 November 2004, San Clemente, California. Society for Industrial and Applied Mathematics, Philadelphia, PA.\n* [https://web.archive.org/web/20061009230904/http://history.siam.org/oralhistories/dongarra.htm Dongarra Oral History] In an oral history interview, Jack Dongarra explores the early relationship of BLAS to LINPACK, the creation of higher level BLAS versions for new architectures, and his later work on the ATLAS system to automatically optimize BLAS for particular machines. Jack Dongarra, Oral history interview by Thomas Haigh, 26 April 2005, University of Tennessee, Knoxville TN. Society for Industrial and Applied Mathematics, Philadelphia, PA\n*[https://stackoverflow.com/questions/1303182/how-does-blas-get-such-extreme-performance How does BLAS get such extreme performance?] Ten naive 1000&times;1000 matrix multiplications (10<sup>10</sup> floating point multiply-adds) takes 15.77&nbsp;seconds on 2.6&nbsp;GHz processor; BLAS implementation takes 1.32&nbsp;seconds.\n* An Overview of the Sparse Basic Linear Algebra Subprograms: The New Standard from the BLAS Technical Forum {{doi-inline|10.1145/567806.567810}}\n\n{{Numerical linear algebra}}\n{{Linear algebra}}\n\n[[Category:Numerical linear algebra]]\n[[Category:Numerical software]]\n[[Category:Public-domain software with source code]]"
    },
    {
      "title": "Bidiagonalization",
      "url": "https://en.wikipedia.org/wiki/Bidiagonalization",
      "text": "'''Bidiagonalization''' is one of unitary (orthogonal) [[matrix decomposition]]s such that '''U'''* '''A''' '''V''' = '''B''', where '''U''' and '''V''' are [[unitary matrix|unitary]] ([[orthogonal matrix|orthogonal]]) matrices; * denotes [[Hermitian transpose]]; and '''B''' is upper [[Bidiagonal matrix|bidiagonal]]. '''A''' is allowed to be rectangular.\n\nFor [[dense matrix|dense matrices]], the left and right unitary matrices are obtained by a series of [[Householder reflection]]s alternately applied from the left and right. This is known as Golub-Kahan bidiagonalization. For large matrices, they are calculated iteratively by using [[Lanczos algorithm|Lanczos method]], referred to as Golub-Kahan-Lanczos method.\n\nBidiagonalization has a very similar structure to the [[singular value decomposition]] (SVD). However, it is computed within finite operations, while SVD requires iterative schemes to find singular values. It is because the squared singular values are the roots of [[characteristic polynomial]]s of '''A'''* '''A''', where '''A''' is assumed to be tall.\n\n==References==\n<references/>\n* {{Citation | last1=Golub | first1=Gene H. | author1-link=Gene H. Golub | last2=Van Loan | first2=Charles F. | author2-link=Charles F. Van Loan | title=Matrix Computations | publisher=Johns Hopkins | edition=3rd | isbn=978-0-8018-5414-9 | year=1996}}.\n==External links==\n*[http://www.netlib.org/utk/people/JackDongarra/etemplates/node198.html Golub-Kahan-Lanczos Bidiagonalization Procedure]\n\n{{Numerical linear algebra}}\n\n[[Category:Matrix theory]]\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "BLIS (software)",
      "url": "https://en.wikipedia.org/wiki/BLIS_%28software%29",
      "text": "{{Infobox software\n| name = BLIS\n| logo = \n| screenshot = \n| caption = \n| collapsible = \n| author = Science of High-Performance Computing (SHPC) group, UT-Austin\n| developer = Field Van Zee\n| released = {{release date and age|2016|10|5}}\n| latest release version = 0.5.2\n| latest release date = {{release date and age|2019|3|19}}<ref>[https://github.com/flame/blis/releases Releases · flame/blis – GitHub]</ref>\n| latest preview version = \n| latest preview date = \n| programming language = \n| operating system = [[Linux]]<br>[[Microsoft Windows]]<br>[[macOS]]<br>[[FreeBSD]]\n| platform = [[x86-64]]<br>[[ARM architecture|ARM]]<br>ARM64\n| size = \n| language = \n| genre = Linear algebra library; implementation of [[Basic Linear Algebra Subprograms|BLAS]]\n| license = new/modified/3-clause [[BSD License]]\n}}\n\nIn [[Computational science|scientific computing]], the '''BLIS''' (BLAS-like Library Instantiation Software)\n<ref name=\"BLIS1\">{{cite journal |last1=Van Zee |first1=Field |last2=van de Geijn |first2=Robert |title={BLIS}: A Framework for Rapidly Instantiating BLAS Functionality |journal=ACM Transactions on Mathematical Software |date=2015 |volume=41 | issue=2 | pages=14:1–14:33 |doi=10.1145/2764454}}</ref>\n<ref name=\"BLIS2\">{{cite journal |last1=Van Zee |first1=Field |last2=Smith |first2=Tyler |last3=Igual |first3=Francisco |last4=Smelyanskiy |first4=Mikhail |last5=Zhang |first5=Xiangyi |last6=Kistler |first6=Michael |last7=Austel |first7=Vernon |last8=Gunnels |first8=John |last9=Low |first9=Tze Meng |last10=Marker |first10=Bryan |last11=Killough |first11=Lee |last12=van de Geijn |first12=Robert |title=The BLIS Framework: Experiments in Portability |journal=ACM Transactions on Mathematical Software |date=2016 |volume=42 |issue=2 |pages=12:1–12:19 |doi=10.1145/2755561}}</ref>\n<ref name=\"BLIS3\">{{cite journal |last1=Smith |first1=Tyler |last2=van de Geijn |first2=Robert |last3=Smelyanskiy |first3=Mikhael |last4=Hammond |first4=Jeff |last5=Van Zee |first5=Field |title=Anatomy High-Performance . Many-Threaded Matrix Multiplication |journal=28th IEEE International Parallel & Distributed Processing Symposium   IPDPS 2014) |date=2014}}</ref>\n<ref>{{cite journal |last1=Low |first1=Tze Meng |last2=Igual |first2=Francisco |last3=Smith |first3=Tyler |last4=Quintana |first4=Enrique |title=Analytical Modeling is Enough for High-Performance BLIS |journal=ACM Transactions on Mathematical Software |date=2016 |volume=43 |issue=2 |pages=12:1–12:18 |doi=10.1145/2925987}}</ref>\nis an [[Open source software|open source]] framework for implementing a superset of [[BLAS]] (Basic Linear Algebra Subprograms) functionality for specific [[Central processing unit|processor]] types.  It exposes that functionality through the traditional BLAS interface as well as a native typed (BLAS-like) interface and an object interface.\nIt is developed and supported by the Science of High-Performance Computing (SHPC) group of the Oden [[Institute for Computational Engineering and Sciences]] at [[The University of Texas at Austin]].\n\nBLIS supports most conventional CPUs through a generic (lower performing) implementation and many current such processors via optimized low-level kernels.  It provides highly competitive performance on those architectures for which it has been optimized.\n\nBLIS is a refactoring of [[GotoBLAS]]2, which was created by [[Kazushige Goto]] at the [[Texas Advanced Computing Center]].<ref>{{cite journal |last1=Goto |first1=Kazushige |last2=van de Geijn |first2=Robert |title=Anatomy of high-performance matrix multiplication |journal=ACM Transactions on Mathematical Software |date=2008 |volume=34 |issue=3 |pages=Article No. 12}}</ref>\n\n==See also==\n* [[Automatically Tuned Linear Algebra Software]] (ATLAS)\n* [[OpenBLAS]] \n* [[Math Kernel Library|Intel Math Kernel Library]] (MKL)\n\n==References==\n{{reflist|1}}\n\n==External links==\n* {{official website|www.github.com/flame/blis}}\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]\n[[Category:Numerical software]]"
    },
    {
      "title": "Block Lanczos algorithm",
      "url": "https://en.wikipedia.org/wiki/Block_Lanczos_algorithm",
      "text": "In [[computer science]], the '''block Lanczos algorithm''' is an [[algorithm]] for finding the [[nullspace]] of a [[Matrix (mathematics)|matrix]] over a [[finite field]], using only multiplication of the matrix by long, thin matrices. Such matrices are considered as vectors of [[tuple]]s of finite-field entries, and so tend to be called 'vectors' in descriptions of the algorithm.\n\nThe block Lanczos algorithm is amongst the most efficient methods known for finding nullspaces, which is the final stage in [[integer factorization]] algorithms such as the [[quadratic sieve]] and [[number field sieve]], and its development has been entirely driven by this application.\n\n== Parallelization issues ==\n\nThe algorithm is essentially not parallel:  it is of course possible to distribute the matrix–'vector' multiplication, but the whole vector must be available for the combination step at the end of each iteration, so all the machines involved in the calculation must be on the same fast network.  In particular, it is not possible to widen the vectors and distribute slices of vectors to different independent machines.\n\nThe [[block Wiedemann algorithm]] is more useful in contexts where several systems each large enough to hold the entire matrix are available, since in that algorithm the systems can run independently until a final stage at the end.\n\n== History ==\nThe block Lanczos algorithm was developed by [[Peter Montgomery (mathematician)|Peter Montgomery]] and published in 1995;<ref>{{cite conference |last=Montgomery |first=P L |authorlink=Peter Montgomery (mathematician) |year=1995 |title=A Block Lanczos Algorithm for Finding Dependencies over GF(2) |conference=EUROCRYPT '95 |booktitle=Lecture Notes in Computer Science |volume=921 |pages=106–120 |publisher=Springer-Verlag|doi=10.1007/3-540-49264-X_9 }}</ref> it is based on, and bears a strong resemblance to, the [[Lanczos algorithm]] for finding [[eigenvalue]]s of large sparse real matrices.\n\n== References ==\n{{reflist}}\n\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "Block matrix pseudoinverse",
      "url": "https://en.wikipedia.org/wiki/Block_matrix_pseudoinverse",
      "text": "{{refimprove|date=December 2010}}\nIn [[mathematics]], a '''block matrix pseudoinverse''' is a formula for the [[pseudoinverse]] of a [[partitioned matrix]]. This is useful for decomposing or approximating many algorithms updating parameters in [[signal processing]], which are based on the [[least squares]] method.\n\n== Derivation ==\nConsider a column-wise partitioned matrix:\n:<math>\n  \\begin{bmatrix}\\mathbf A & \\mathbf B\\end{bmatrix},\\quad\n  \\mathbf A \\in \\reals^{m \\times n},\\quad\n  \\mathbf B \\in \\reals^{m \\times p},\\quad\n  m \\geq n + p.\n</math>\n\nIf the above matrix is full rank, the [[Moore–Penrose inverse]] matrices of it and its transpose are\n:<math>\\begin{align}\n  \\begin{bmatrix}\\mathbf A & \\mathbf B\\end{bmatrix}^+ &=\n  \\left(\n    \\begin{bmatrix}\\mathbf A & \\mathbf B\\end{bmatrix}^\\textsf{T}\n    \\begin{bmatrix}\\mathbf A & \\mathbf B\\end{bmatrix}\n  \\right)^{-1} \\begin{bmatrix}\\mathbf A & \\mathbf B\\end{bmatrix}^\\textsf{T}, \\\\\n\n  \\begin{bmatrix}\n    \\mathbf A^\\textsf{T} \\\\\n    \\mathbf B^\\textsf{T}\n  \\end{bmatrix}^+ &=\n  \\begin{bmatrix}\\mathbf A & \\mathbf B\\end{bmatrix} \\left(\n    \\begin{bmatrix}\\mathbf A & \\mathbf B\\end{bmatrix}^\\textsf{T}\n    \\begin{bmatrix}\\mathbf A & \\mathbf B\\end{bmatrix}\n  \\right)^{-1}.\n\\end{align}</math>\n\nThis computation of the pseudoinverse requires (''n''&nbsp;+&nbsp;''p'')-square matrix inversion and does not take advantage of the block form.\n\nTo reduce computational costs to ''n''- and ''p''-square matrix inversions and to introduce parallelism, treating the blocks separately, one derives <ref name=Baksalary>{{cite journal|author=[[Jerzy Baksalary|J.K. Baksalary]] and O.M. Baksalary|title=Particular formulae for the Moore–Penrose inverse of a columnwise partitioned matrix|journal=Linear Algebra Appl.|volume=421|date=2007|pages=16–23|doi=10.1016/j.laa.2006.03.031}}</ref>\n:<math>\\begin{align}\n  \\begin{bmatrix}\\mathbf A & \\mathbf B\\end{bmatrix}^+ &=\n  \\begin{bmatrix}\n    \\mathbf P_B^\\perp \\mathbf A\\left(\\mathbf A^\\textsf{T} \\mathbf P_B^\\perp \\mathbf A\\right)^{-1} \\\\\n    \\mathbf P_A^\\perp \\mathbf B\\left(\\mathbf B^\\textsf{T} \\mathbf P_A^\\perp \\mathbf B\\right)^{-1}\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    \\left(\\mathbf P_B^\\perp\\mathbf A\\right)^+ \\\\\n    \\left(\\mathbf P_A^\\perp\\mathbf B\\right)^+\n  \\end{bmatrix}, \\\\\n\n  \\begin{bmatrix}\n    \\mathbf A^\\textsf{T} \\\\\n    \\mathbf B^\\textsf{T}\n  \\end{bmatrix}^+ &=\n  \\begin{bmatrix}\n    \\mathbf P_B^\\perp \\mathbf A\\left(\\mathbf A^\\textsf{T} \\mathbf P_B^\\perp \\mathbf A\\right)^{-1},\\quad\n    \\mathbf P_A^\\perp \\mathbf B\\left(\\mathbf B^\\textsf{T} \\mathbf P_A^\\perp \\mathbf B\\right)^{-1}\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    \\left(\\mathbf A^\\textsf{T} \\mathbf P_B^\\perp\\right)^+ &\n    \\left(\\mathbf B^\\textsf{T} \\mathbf P_A^\\perp\\right)^+\n  \\end{bmatrix},\n\\end{align}</math>\n\nwhere [[orthogonal projection]] matrices are defined by\n::<math>\\begin{align}\n  \\mathbf P_A^\\perp &= \\mathbf I - \\mathbf A \\left(\\mathbf A^\\textsf{T} \\mathbf A\\right)^{-1} \\mathbf A^\\textsf{T}, \\\\\n  \\mathbf P_B^\\perp &= \\mathbf I - \\mathbf B \\left(\\mathbf B^\\textsf{T} \\mathbf B\\right)^{-1} \\mathbf B^\\textsf{T}.\n\\end{align}</math>\n\nThe above formulas are not necessarily valid if <math>\\begin{bmatrix}\\mathbf A & \\mathbf B\\end{bmatrix}</math> does not have full rank – for example, if <math>\\mathbf A \\neq 0</math>, then\n:<math>\n  \\begin{bmatrix}\\mathbf A & \\mathbf A\\end{bmatrix}^+ =\n  \\frac{1}{2}\\begin{bmatrix}\n    \\mathbf A^+ \\\\\n    \\mathbf A^+\n  \\end{bmatrix} \\neq\n  \\begin{bmatrix}\n    \\left(\\mathbf P_A^\\perp\\mathbf A\\right)^+ \\\\\n    \\left(\\mathbf P_A^\\perp\\mathbf A\\right)^+\n  \\end{bmatrix} =\n  0\n</math>\n\n== Application to least squares problems ==\n\nGiven the same matrices as above, we consider the following least squares problems, which\nappear as multiple objective optimizations or constrained problems in signal processing.\nEventually, we can implement a parallel algorithm for least squares based on the following results.\n\n=== Column-wise partitioning in over-determined least squares ===\n\nSuppose a solution <math>\n  \\mathbf x = \\begin{bmatrix}\n    \\mathbf x_1 \\\\\n    \\mathbf x_2 \\\\\n  \\end{bmatrix}\n</math> solves an over-determined system:\n:<math> \n  \\begin{bmatrix}\n    \\mathbf A, & \\mathbf B \n  \\end{bmatrix}\n  \\begin{bmatrix}\n    \\mathbf x_1 \\\\\n    \\mathbf x_2 \\\\\n  \\end{bmatrix} =\n  \\mathbf d,\\quad\n  \\mathbf d \\in \\reals^{m \\times 1}.\n</math>\n\nUsing the block matrix pseudoinverse, we have\n:<math>\\mathbf x = \n  \\begin{bmatrix}\n    \\mathbf A, & \\mathbf B\n  \\end{bmatrix}^+\\,\\mathbf d =\n  \\begin{bmatrix}\n    \\left(\\mathbf P_B^\\perp \\mathbf A\\right)^+ \\\\\n    \\left(\\mathbf P_A^\\perp \\mathbf B\\right)^+ \n  \\end{bmatrix}\\mathbf d.\n</math>\n\nTherefore, we have a decomposed solution:\n:<math>\n  \\mathbf x_1 = \\left(\\mathbf P_B^\\perp \\mathbf A\\right)^+\\,\\mathbf d,\\quad\n  \\mathbf x_2 = \\left(\\mathbf P_A^\\perp \\mathbf B\\right)^+\\,\\mathbf d.\n</math>\n\n=== Row-wise partitioning in under-determined least squares ===\n\nSuppose a solution <math>\\mathbf x</math> solves an under-determined system:\n:<math>\n  \\begin{bmatrix}\n    \\mathbf A^\\textsf{T} \\\\\n    \\mathbf B^\\textsf{T} \n  \\end{bmatrix}\\mathbf x =\n  \\begin{bmatrix}\n    \\mathbf e \\\\\n    \\mathbf f \n  \\end{bmatrix},\\quad\n  \\mathbf e \\in \\reals^{n \\times 1},\\quad\n  \\mathbf f \\in \\reals^{p \\times 1}.\n</math>\n\nThe minimum-norm solution is given by\n:<math>\\mathbf x =\n  \\begin{bmatrix}\n    \\mathbf A^\\textsf{T} \\\\\n    \\mathbf B^\\textsf{T} \n  \\end{bmatrix}^+\\,\n  \\begin{bmatrix}\n    \\mathbf e \\\\\n    \\mathbf f \n  \\end{bmatrix}.\n</math>\n\nUsing the block matrix pseudoinverse, we have\n:<math>\n  \\mathbf x = \\begin{bmatrix}\n    \\left(\\mathbf A^\\textsf{T}\\mathbf P_B^\\perp\\right)^+ &\n    \\left(\\mathbf B^\\textsf{T}\\mathbf P_A^\\perp\\right)^+\n  \\end{bmatrix} \\begin{bmatrix}\n    \\mathbf e \\\\\n    \\mathbf f \n  \\end{bmatrix} =\n  \\left(\\mathbf A^\\textsf{T}\\mathbf P_B^\\perp\\right)^+\\,\\mathbf e +\n    \\left(\\mathbf B^\\textsf{T}\\mathbf P_A^\\perp\\right)^+\\,\\mathbf f.\n</math>\n\n== Comments on matrix inversion ==\n\nInstead of <math>\\mathbf \\left(\\begin{bmatrix}\\mathbf A & \\mathbf B\\end{bmatrix}^\\textsf{T} \\begin{bmatrix}\\mathbf A & \\mathbf B\\end{bmatrix}\\right)^{-1}</math>, we need to calculate directly or indirectly{{citation needed|date=December 2010}}{{original research?|date=December 2010}}\n\n:<math> \n  \\left(\\mathbf A^\\textsf{T} \\mathbf A\\right)^{-1},\\quad\n  \\left(\\mathbf B^\\textsf{T} \\mathbf B\\right)^{-1},\\quad\n  \\left(\\mathbf A^\\textsf{T} \\mathbf P_B^\\perp \\mathbf A\\right)^{-1},\\quad\n  \\left(\\mathbf B^\\textsf{T} \\mathbf P_A^\\perp \\mathbf B\\right)^{-1}.\n</math>\n\nIn a dense and small system, we can use [[singular value decomposition]], [[QR decomposition]], or [[Cholesky decomposition]] to replace the matrix inversions with numerical routines. In a large system, we may employ [[iterative methods]] such as Krylov subspace methods.\n\nConsidering [[parallel algorithms]], we can compute <math>\\left(\\mathbf A^\\textsf{T} \\mathbf A\\right)^{-1}</math> and <math>\\left(\\mathbf B^\\textsf{T} \\mathbf B\\right)^{-1}</math> in parallel. Then, we finish to compute <math>\\left(\\mathbf A^\\textsf{T} \\mathbf P_B^\\perp \\mathbf A\\right)^{-1}</math> and <math>\\left(\\mathbf B^\\textsf{T} \\mathbf P_A^\\perp \\mathbf B\\right)^{-1}</math> also in parallel.\n\n== See also ==\n* [[Invertible matrix#Blockwise inversion]]\n\n==References ==\n{{Reflist}}\n\n== External links ==\n* [http://www.ee.ic.ac.uk/hp/staff/dmb/matrix/intro.html The Matrix Reference Manual] by [http://www.ee.ic.ac.uk/hp/staff/dmb/dmb.html Mike Brookes]\n* [https://web.archive.org/web/20060414125709/http://www.csit.fsu.edu/~burkardt/papers/linear_glossary.html Linear Algebra Glossary] by [http://www.csit.fsu.edu/~burkardt/ John Burkardt]\n* [http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf The Matrix Cookbook] by [http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=3274/ Kaare Brandt Petersen]\n* [http://see.stanford.edu/materials/lsoeldsee263/08-min-norm.pdf Lecture 8: Least-norm solutions of undetermined equations] by [http://www.s\\right]tanford.edu/~boyd/ Stephen P. Boyd]\n\n{{Numerical linear algebra}}\n\n{{DEFAULTSORT:Block Matrix Pseudoinverse}}\n[[Category:Numerical linear algebra]]\n[[Category:Matrix theory]]"
    },
    {
      "title": "Block Wiedemann algorithm",
      "url": "https://en.wikipedia.org/wiki/Block_Wiedemann_algorithm",
      "text": "The '''block Wiedemann algorithm''' for computing kernel vectors of a [[matrix (mathematics)|matrix]] over a finite field is a generalisation of an algorithm due to [[Don Coppersmith]].\n\n== Coppersmith's algorithm ==\n\nLet <math>M</math> be an <math>n\\times n</math> [[square matrix]] over some [[finite field]] F, let <math>x_{\\mathrm {base}}</math> be a random vector of length <math>n</math>, and let <math>x = M x_{\\mathrm {base}}</math>. Consider the sequence of vectors <math>S = \\left[x, Mx, M^2x, \\ldots\\right]</math> obtained by repeatedly multiplying the vector by the matrix <math>M</math>; let <math>y</math> be any other vector of length <math>n</math>, and consider the sequence of finite-field elements <math>S_y = \\left[y \\cdot x, y \\cdot Mx, y \\cdot M^2x \\ldots\\right]</math>\n\nWe know that the matrix <math>M</math> has a [[Minimal_polynomial_(linear_algebra)|minimal polynomial]]; by the [[Cayley–Hamilton theorem]] we know that this polynomial is of degree (which we will call <math>n_0</math>) no more than <math>n</math>. Say <math>\\sum_{r=0}^{n_0} p_rM^r = 0</math>.  Then <math>\\sum_{r=0}^{n_0} y \\cdot (p_r (M^r x)) = 0</math>; so the minimal polynomial of the matrix annihilates the sequence <math>S</math> and hence <math>S_y</math>.\n\nBut the [[Berlekamp–Massey algorithm]] allows us to calculate relatively efficiently some sequence <math>q_0 \\ldots q_L</math> with <math>\\sum_{i=0}^L q_i S_y[{i+r}]=0 \\;\\forall \\; r</math>. Our hope is that this sequence, which by construction annihilates <math>y \\cdot S</math>, actually annihilates <math>S</math>; so we have <math>\\sum_{i=0}^L q_i M^i x = 0</math>. We then take advantage of the initial definition of <math>x</math> to say <math>M \\sum_{i=0}^L q_i M^i x_{\\mathrm {base}} = 0</math> and so <math>\\sum_{i=0}^L q_i M^i x_{\\mathrm {base}}</math> is a hopefully non-zero kernel vector of <math>M</math>.\n\n== The block Wiedemann algorithm ==\n\nThe natural implementation of sparse matrix arithmetic on a computer makes it easy to compute the sequence ''S'' in parallel for a number of vectors equal to the width of a machine word – indeed, it will normally take no longer to compute for that many vectors than for one.  If you have several processors, you can compute the sequence S for a different set of random vectors in parallel on all the computers.\n\nIt turns out, by a generalization of the Berlekamp–Massey algorithm to provide a sequence of small matrices, that you can take the sequence produced for a large number of vectors and generate a kernel vector of the original large matrix.  You need to compute <math> y_i \\cdot M^t x_j</math> for some <math>i = 0 \\ldots i_\\max, j=0 \\ldots j_\\max, t = 0 \\ldots t_\\max</math> where <math>i_\\max, j_\\max, t_\\max</math> need to satisfy <math>t_\\max > \\frac{d}{i_\\max} + \\frac{d}{j_\\max} + O(1)</math> and <math>y_i</math> are a series of vectors of length n; but in practice you can take <math>y_i</math> as a sequence of unit vectors and simply write out the first <math>i_\\max</math> entries in your vectors at each time ''t''.\n\n== References ==\n\nVillard's 1997 research report '[http://citeseer.ist.psu.edu/cache/papers/cs/4204/ftp:zSzzSzftp.imag.frzSzpubzSzCALCUL_FORMELzSzRAPPORTzSz1997zSzRR975.pdf A study of Coppersmith's block Wiedemann algorithm using matrix polynomials]' (the cover material is in French but the content in English) is a reasonable description.\n\nThomé's paper '[http://hal.archives-ouvertes.fr/docs/00/10/34/17/PDF/jsc.pdf Subquadratic computation of vector generating polynomials and improvement of the block Wiedemann algorithm]' uses a more sophisticated [[Fast Fourier transform|FFT]]-based algorithm for computing the vector generating polynomials, and describes a practical implementation with ''i''<sub>max</sub>&nbsp;=&nbsp;''j''<sub>max</sub>&nbsp;=&nbsp;4 used to compute a kernel vector of a 484603×484603 matrix of entries modulo 2<sup>607</sup>−1, and hence to compute discrete logarithms in the field ''GF''(2<sup>607</sup>).\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "Chebyshev iteration",
      "url": "https://en.wikipedia.org/wiki/Chebyshev_iteration",
      "text": "In [[numerical linear algebra]], the '''Chebyshev iteration''' is an\n[[Iterative method#Linear systems|iterative method]] for determining the solutions of a [[system of linear equations]]. The method is named after [[Russia]]n mathematician [[Pafnuty Chebyshev]].\n\nChebyshev iteration avoids the computation of [[inner product]]s as is necessary for the other nonstationary methods. For some distributed-memory architectures these inner products are a bottleneck with respect to efficiency. The price one pays for avoiding inner products is that the method requires enough knowledge about spectrum of the coefficient matrix&nbsp;''A'', that is an upper estimate for the upper [[eigenvalue]] and lower estimate for the lower eigenvalue.  There are modifications of the method for nonsymmetric matrices&nbsp;''A''.\n\n==Example code in [[MatLab]]==\n\n<source lang=\"matlab\">\nfunction [x] =  SolChebyshev002(A,b,x0,iterNum,lMax,lMin)\n\n  d=(lMax+lMin)/2;\n  c=(lMax-lMin)/2;\n  preCond=eye(size(A)); %preconditioner\n  x=x0;\n  r=b-A*x;\n\n  for i = 1:iterNum % size(A,1)\n      z = linsolve(preCond,r);\n      if (i==1)\n          p=z;\n          alpha=1/d;\n      else if (i==2)\n          beta=(1/2)*(c*alpha)^2\n          alpha=1/(d - beta/alpha);\n          p=z+beta*p;\n      else\n          beta=(c*alpha/2)^2;\n          alpha=1/(d - beta/alpha);\n          p=z+beta*p;\n      end;\n\n      x=x+alpha*p;\n      r=b-A*x; %(=r-alpha*A*p)\n      if (norm(r)<1e-15), break; end; %stop if necessary\n  end;\nend\n</source>\nCode translated from \n<ref>{{cite journal|last1=Barrett|first1=Richard|last2=Michael|first2=Berry|last3=Tony|first3=Chan|first4=James|last4=Demmel|first5=June\n|last5=Donato|first6=Jack|last6=Dongarra|first7=Victor|last7=Eijkhout|first8=Roldan|last8=Pozo|first9=Charles|last9=Romine\n|first10=Henk|last10=Van der Vorst|title=Templates for the solution of linear systems: building blocks for iterative methods|date=1993|publisher=SIAM|volume=43|url=http://www.netlib.org/linalg/html_templates/Templates.html}}</ref> \nand \n.<ref>{{cite journal|last1=Gutknecht|first1=Martin|last2=Röllin|first2=Stefan|title=The Chebyshev iteration revisited|journal=Parallel Computing|date=2002|volume=28|issue=2|pages=263–283|doi=10.1016/S0167-8191(01)00139-9}}</ref>\n\n==See also==\n* [[Iterative method#Linear systems|Iterative method. Linear systems]]\n* [[List of numerical analysis topics#Solving systems of linear equations|List of numerical analysis topics. Solving systems of linear equations]]\n* [[Jacobi iteration]]\n* [[Gauss–Seidel method]]\n* [[Modified Richardson iteration]]\n* [[Successive over-relaxation]]\n* [[Conjugate gradient method]]\n* [[Generalized minimal residual method]]\n* [[Biconjugate gradient method]]\n* [[Iterative Template Library]]\n* [[IML++]]\n\n<ref>[https://link.springer.com/article/10.1007%2Fs00025-015-0490-y On the Convergence of Chebyshev’s Method for Multiple Polynomial Zeros]</ref>\n\n==References==\n* {{springer|title=Chebyshev iteration method|id=p/c021900}}\n{{reflist}}\n\n== External links ==\n* [http://www.netlib.org/linalg/html_templates/Templates.html Templates for the Solution of Linear Systems]\n* [http://mathworld.wolfram.com/ChebyshevIteration.html Chebyshev Iteration. From MathWorld]\n* [https://github.com/peterborodatyy/chebyshev-iteration Chebyshev Iteration. Implementation on Go language]\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "Cholesky decomposition",
      "url": "https://en.wikipedia.org/wiki/Cholesky_decomposition",
      "text": "In [[linear algebra]], the '''Cholesky decomposition''' or '''Cholesky factorization''' (pronounced /ʃo-LESS-key/) is a [[matrix decomposition|decomposition]] of a [[Hermitian matrix|Hermitian]], [[positive-definite matrix]] into the product of a [[lower triangular matrix]] and its [[conjugate transpose]], which is useful for efficient numerical solutions, e.g., [[Monte Carlo simulation]]s. It was discovered by [[André-Louis Cholesky]] for real matrices. When it is applicable, the Cholesky decomposition is roughly twice as efficient as the [[LU decomposition]] for solving [[System of linear equations|systems of linear equations]].<ref name=\"NR\">{{cite book|last=Press|first=William H.|author2=Saul A. Teukolsky |author3=William T. Vetterling |author4=Brian P. Flannery |title=Numerical Recipes in C: The Art of Scientific Computing |edition=second|publisher=Cambridge University England EPress|year=1992|page=994|url=http://www.nr.com/|isbn=0-521-43108-5}}</ref>\n\n== Statement ==\nThe Cholesky decomposition of a [[Hermitian matrix|Hermitian]] [[positive-definite matrix]] '''A''' is a decomposition of the form\n\n: <math>\\mathbf{A} = \\mathbf{L L}^*,</math>\n\nwhere '''L''' is a [[lower triangular matrix]] with real and positive diagonal entries, and '''L'''* denotes the [[conjugate transpose]] of '''L'''. Every Hermitian positive-definite matrix (and thus also every real-valued symmetric positive-definite matrix) has a unique Cholesky decomposition.<ref>{{harvtxt|Golub|Van Loan|1996|p=143}}, {{harvtxt|Horn|Johnson|1985|p=407}}, {{harvtxt|Trefethen|Bau|1997|p=174}}.</ref>\n\nIf the matrix '''A''' is Hermitian and positive semi-definite, then it still has a decomposition of the form '''A''' = '''LL'''* if the diagonal entries of '''L''' are allowed to be zero.<ref>{{harvtxt|Golub|Van Loan|1996|p=147}}.</ref>\n\nWhen '''A''' has only real entries, '''L''' has only real entries as well, and the factorization may be written '''A''' = '''LL'''<sup>T</sup>.<ref>{{harvtxt|Horn|Johnson|1985|p=407}}.</ref>\n\nThe Cholesky decomposition is unique when '''A''' is [[positive definite matrix|positive definite]]; there is only one lower triangular matrix '''L''' with strictly positive diagonal entries such that '''A''' = '''LL'''*. However, the decomposition need not be unique when '''A''' is positive semidefinite.\n\nThe converse holds trivially: if '''A''' can be written as '''LL'''* for some invertible '''L''', lower triangular or otherwise, then '''A''' is Hermitian and positive definite.\n\n== LDL decomposition ==\n\nA closely related variant of the classical Cholesky decomposition is the LDL decomposition,\n\n: <math>\\mathbf{A} = \\mathbf{L D L}^*,</math>\n\nwhere '''L''' is a [[Unitriangular matrix|lower unit triangular (unitriangular)]] matrix, and '''D''' is a [[diagonal matrix|diagonal]] matrix.\n\nThis decomposition is related to the classical Cholesky decomposition of the form '''LL'''* as follows:\n\n: <math>\\mathbf{A} = \\mathbf{L D L}^* = \\mathbf L \\mathbf D^{1/2} (\\mathbf D^{1/2})^* \\mathbf L^* =\n\\mathbf L \\mathbf D^{1/2} (\\mathbf L \\mathbf D^{1/2})^*.</math>\n\nOr, given the classical Cholesky decomposition <math>\\mathbf L^\\text{Cholesky}</math>, the <math>\\mathbf L \\mathbf D \\mathbf L^*</math> form can be found by using the property that the diagonal of '''L''' must be 1 and that both the Cholesky and the <math>\\mathbf L \\mathbf D \\mathbf L^\\mathrm{T}</math> form are lower triangles,<ref>[http://stats.stackexchange.com/questions/208599/ldlt-decomposition-from-cholesky-decomposition variance – '''LDL'''<sup>T</sup> decomposition from Cholesky decomposition – Cross Validated]. Stats.stackexchange.com (2016-04-21). Retrieved on 2016-11-02.</ref>\nif '''S''' is a diagonal matrix that contains the main diagonal of <math>\\mathbf L^\\text{Cholesky}</math>, then\n: <math> \\mathbf D  = \\mathbf S^2, </math>\n: <math> \\mathbf L  = \\mathbf L^\\text{Cholesky} \\mathbf S^{-1}. </math>\n\nThe '''LDL''' variant, if efficiently implemented, requires the same space and computational complexity to construct and use but avoids extracting square roots.<ref name=\"kri\">{{cite journal|last=Krishnamoorthy|first=Aravindh|author2=Menon, Deepak|title=Matrix Inversion Using Cholesky Decomposition|volume=1111|page=4144|year=2011|bibcode=2011arXiv1111.4144K|arxiv=1111.4144}}</ref> Some indefinite matrices for which no Cholesky decomposition exists have an LDL decomposition with negative entries in '''D'''. For these reasons, the LDL decomposition may be preferred.\nFor real matrices, the factorization has the form '''A''' = '''LDL'''<sup>T</sup> and is often referred to as '''LDLT decomposition''' (or LDL<sup>T</sup> decomposition, or LDL′).  It is closely related to the [[eigendecomposition of a matrix#Real symmetric matrices|eigendecomposition of real symmetric matrices]], '''A''' = '''QΛQ'''<sup>T</sup>.\n\n== Example ==\n\nHere is the Cholesky decomposition of a symmetric real matrix:\n\n: <math>\\begin{align}\n\\left(\n  \\begin{array}{*{3}{r}}\n      4 &  12 & -16 \\\\\n     12 &  37 & -43 \\\\\n    -16 & -43 &  98 \\\\\n  \\end{array}\n\\right)\n=\n\\left(\n  \\begin{array}{*{3}{r}}\n     2 &  0 &  0 \\\\\n     6 &  1 &  0 \\\\\n    -8 &  5 &  3 \\\\\n  \\end{array}\n\\right)\n\\left(\n  \\begin{array}{*{3}{r}}\n     2 &  6 & -8 \\\\\n     0 &  1 &  5 \\\\\n     0 &  0 &  3 \\\\\n  \\end{array}\n\\right).\n\\end{align}</math>\n\nAnd here is its LDL<sup>T</sup> decomposition:\n\n: <math>\\begin{align}\n\\left(\n  \\begin{array}{*{3}{r}}\n      4 &  12 & -16 \\\\\n     12 &  37 & -43 \\\\\n    -16 & -43 &  98 \\\\\n  \\end{array}\n\\right)\n& =\n\\left(\n  \\begin{array}{*{3}{r}}\n     1 &  0 &  0 \\\\\n     3 &  1 &  0 \\\\\n    -4 &  5 &  1 \\\\\n  \\end{array}\n\\right)\n\\left(\n  \\begin{array}{*{3}{r}}\n     4 &  0 &  0 \\\\\n     0 &  1 &  0 \\\\\n     0 &  0 &  9 \\\\\n  \\end{array}\n\\right)\n\\left(\n  \\begin{array}{*{3}{r}}\n     1 &  3 & -4 \\\\\n     0 &  1 &  5 \\\\\n     0 &  0 &  1 \\\\\n  \\end{array}\n\\right).\n\\end{align}</math>\n\n== Applications ==\nThe Cholesky decomposition is mainly used for the numerical solution of [[system of linear equations|linear equations]] <math>\\mathbf{Ax} = \\mathbf{b}</math>. If '''A''' is symmetric and positive definite, then we can solve  <math>\\mathbf{Ax} = \\mathbf{b}</math> by first computing the Cholesky decomposition  <math>\\mathbf{A} = \\mathbf{LL}^\\mathrm{*}</math>, then solving <math>\\mathbf{Ly} = \\mathbf{b}</math> for '''y''' by [[forward substitution]], and finally solving <math>\\mathbf{L^*x} = \\mathbf{y}</math> for '''x''' by [[back substitution]].\n\nAn alternative way to eliminate taking square roots in the <math>\\mathbf{LL}^\\mathrm{*}</math> decomposition is to compute the Cholesky decomposition <math>\\mathbf{A} = \\mathbf{LDL}^\\mathrm{*}</math>, then solving <math>\\mathbf{Ly} = \\mathbf{b}</math> for '''y''', and finally solving <math>\\mathbf{DL}^\\mathrm{*}\\mathbf{x} = \\mathbf{y}</math>.\n\nFor linear systems that can be put into symmetric form, the Cholesky decomposition (or its LDL variant) is the method of choice, for superior efficiency and numerical stability. Compared to the [[LU decomposition]], it is roughly twice as efficient.<ref name=\"NR\"/>\n\n===Linear least squares===\nSystems of the form '''Ax''' = '''b''' with '''A''' symmetric and positive definite arise quite often in applications. For instance, the normal equations in [[linear least squares (mathematics)|linear least squares]] problems are of this form. It may also happen that matrix '''A''' comes from an energy functional, which must be positive from physical considerations; this happens frequently in the numerical solution of [[partial differential equation]]s.\n\n===Non-linear optimization===\nNon-linear multi-variate functions may be minimized over their parameters using variants of [[Newton's method]] called ''quasi-Newton'' methods.  At iteration k, the search steps in a direction <math> p_k </math> defined by solving <math> B_k p_k </math> = <math> -g_k </math> for <math> p_k </math>, where <math> p_k </math> is the step direction, <math> g_k </math> is the [[Gradient|gradient]], and <math> B_k </math> is an approximation to the [[Hessian matrix]] formed by repeating rank-1 updates at each iteration.  Two well-known update formulas are called [[Davidon–Fletcher–Powell]] (DFP) and [[BFGS method|Broyden–Fletcher–Goldfarb–Shanno]] (BFGS). Loss of the positive-definite condition through round-off error is avoided if rather than updating an approximation to the inverse of the Hessian, one updates the Cholesky decomposition of an approximation of the Hessian matrix itself.{{Citation needed|date=February 2011}}\n\n===Monte Carlo simulation===\nThe Cholesky decomposition is commonly used in the [[Monte Carlo method]] for simulating systems with multiple correlated variables. The [[covariance matrix]] is decomposed to give the lower-triangular '''L'''. Applying this to a vector of uncorrelated samples '''u''' produces a sample vector '''Lu''' with the covariance properties of the system being modeled.<ref name=\"Matlab documentation\">[http://www.mathworks.com/help/techdoc/ref/randn.html Matlab randn documentation]. mathworks.com.</ref>\n\nThe following simplified example shows the economy one gets from the Cholesky decomposition:  suppose the goal is to generate two correlated normal variables <math>x_1</math> and <math>x_2</math> with given correlation coefficient <math>\\rho</math>. To accomplish that, it is necessary to first generate two uncorrelated Gaussian random variables <math>z_1</math> and <math>z_2</math>, which can be done using a [[Box–Muller transform]].  Given the  required correlation coefficient <math>\\rho</math>, the correlated normal variables can be obtained via the transformations <math>x_1 = z_1</math> and <math>x_2 =  \\rho z_1 + \\sqrt{1 - \\rho^2} z_2</math>.\n\n===Kalman filters===\n[[Unscented Kalman filter]]s commonly use the Cholesky decomposition to choose a set of so-called sigma points.  The Kalman filter tracks the average state of a system as a vector '''x''' of length ''N'' and covariance as an ''N'' × ''N'' matrix '''P'''.  The matrix '''P''' is always positive semi-definite and can be decomposed into '''LL'''<sup>T</sup>.  The columns of '''L''' can be added and subtracted from the mean '''x''' to form a set of 2''N'' vectors called ''sigma points''.  These sigma points completely capture the mean and covariance of the system state.\n\n===Matrix inversion===\nThe explicit [[inverse matrix|inverse]] of a Hermitian matrix can be computed by Cholesky decomposition, in a manner similar to solving linear systems, using <math>n^3</math> operations (<math>\\tfrac{1}{2} n^3</math> multiplications).<ref name=\"kri\"/> The entire inversion can even be efficiently performed in-place.\n\nA non-Hermitian matrix '''B''' can also be inverted using the following identity, where '''BB'''* will always be Hermitian:\n\n: <math>\\mathbf{B}^{-1} = \\mathbf{B}^* (\\mathbf{B B}^*)^{-1}.</math>\n\n== Computation ==\nThere are various methods for calculating the Cholesky decomposition. The computational complexity of commonly used algorithms is ''O''(''n''<sup>3</sup>) in general.{{Citation needed|date=June 2011}} The algorithms described below all involve about ''n''<sup>3</sup>/3 [[FLOP]]s (''n''<sup>3</sup>/6 multiplications and the same number of additions), where ''n'' is the size of the matrix '''A'''. Hence, they have half the cost of the [[LU decomposition]], which uses 2''n''<sup>3</sup>/3 FLOPs (see Trefethen and Bau 1997).\n\nWhich of the algorithms below is faster depends on the details of the implementation. Generally, the first algorithm will be slightly slower because it accesses the data in a less regular manner.\n\n=== The Cholesky algorithm ===\nThe '''Cholesky algorithm''', used to calculate the decomposition matrix ''L'', is a modified version of [[Gaussian elimination]].\n\nThe recursive algorithm starts with ''i'' := 1 and\n:'''A'''<sup>(1)</sup> := '''A'''.\n\nAt step ''i'', the matrix '''A'''<sup>(''i'')</sup> has the following form:\n:<math>\\mathbf{A}^{(i)}=\n\\begin{pmatrix}\n\\mathbf{I}_{i-1} & 0              & 0 \\\\\n0                & a_{i,i}        & \\mathbf{b}_{i}^{*} \\\\\n0                & \\mathbf{b}_{i} & \\mathbf{B}^{(i)}\n\\end{pmatrix},\n</math>\nwhere '''I'''<sub>''i''−1</sub> denotes the [[identity matrix]] of dimension ''i'' − 1.\n\nIf we now define the matrix '''L'''<sub>''i''</sub> by\n:<math>\\mathbf{L}_{i}:=\n\\begin{pmatrix}\n\\mathbf{I}_{i-1} & 0                                  & 0 \\\\\n0                & \\sqrt{a_{i,i}}           & 0 \\\\\n0                & \\frac{1}{\\sqrt{a_{i,i}}} \\mathbf{b}_{i} & \\mathbf{I}_{n-i}\n\\end{pmatrix},\n</math>\nthen we can write '''A'''<sup>(''i'')</sup> as\n:<math>\\mathbf{A}^{(i)} = \\mathbf{L}_{i} \\mathbf{A}^{(i+1)} \\mathbf{L}_{i}^{*}</math>\nwhere\n:<math>\\mathbf{A}^{(i+1)}=\n\\begin{pmatrix}\n\\mathbf{I}_{i-1} & 0 & 0 \\\\\n0                & 1 & 0 \\\\\n0                & 0 & \\mathbf{B}^{(i)} - \\frac{1}{a_{i,i}} \\mathbf{b}_{i} \\mathbf{b}_{i}^{*}\n\\end{pmatrix}.</math>\nNote that '''b'''<sub>''i''</sub> '''b'''*<sub>''i''</sub> is an [[outer product]], therefore this algorithm is called the ''outer-product version'' in (Golub & Van Loan).\n\nWe repeat this for ''i'' from 1 to ''n''. After ''n'' steps, we get '''A'''<sup>(''n''+1)</sup> = '''I'''. Hence, the lower triangular matrix ''L'' we are looking for is calculated as\n\n:<math>\\mathbf{L} := \\mathbf{L}_{1} \\mathbf{L}_{2} \\dots \\mathbf{L}_{n}.</math>\n\n=== The Cholesky–Banachiewicz and Cholesky–Crout algorithms ===\n[[File:Chol.gif|thumb|Access pattern (white) and writing pattern (yellow) for the in-place Cholesky—Banachiewicz algorithm on a 5×5 matrix]]\nIf we write out the equation\n:<math>\\begin{align}\n\\mathbf{A} = \\mathbf{LL}^T & =\n\\begin{pmatrix}   L_{11} & 0 & 0 \\\\\n   L_{21} & L_{22} & 0 \\\\\n   L_{31} & L_{32} & L_{33}\\\\\n\\end{pmatrix}\n\\begin{pmatrix}   L_{11} & L_{21} & L_{31} \\\\\n   0 & L_{22} & L_{32} \\\\\n   0 & 0 & L_{33}\n\\end{pmatrix} \\\\\n& =\n\\begin{pmatrix}   L_{11}^2 &   &(\\text{symmetric})   \\\\\n   L_{21}L_{11} & L_{21}^2 + L_{22}^2& \\\\\n   L_{31}L_{11} & L_{31}L_{21}+L_{32}L_{22} & L_{31}^2 + L_{32}^2+L_{33}^2\n\\end{pmatrix},\n\\end{align}</math>\n\nwe obtain the following:\n\n:<math>\\begin{align}\n\\mathbf{L} = \n\\begin{pmatrix} \\sqrt{A_{11}} &  0 & 0  \\\\\nA_{21}/L_{11} & \\sqrt{A_{22} - L_{21}^2} & 0 \\\\\nA_{31}/L_{11} &  \\left( A_{32} - L_{31}L_{21} \\right) /L_{22}  &\\sqrt{A_{33}- L_{31}^2 - L_{32}^2}\n\\end{pmatrix}\n\\end{align}</math>\n\nand therefore the following formulas for the entries of '''L''':\n\n:<math> L_{j,j} = \\sqrt{ A_{j,j} - \\sum_{k=1}^{j-1} L_{j,k}^2 }, </math>\n:<math> L_{i,j} = \\frac{1}{L_{j,j}} \\left( A_{i,j} - \\sum_{k=1}^{j-1} L_{i,k} L_{j,k} \\right) \\quad \\text{for } i>j. </math>\n\nThe expression under the [[square root]] is always positive if '''A''' is real and positive-definite.\n\nFor complex Hermitian matrix, the following formula applies:\n\n:<math> L_{j,j} = \\sqrt{ A_{j,j} - \\sum_{k=1}^{j-1} L_{j,k}L_{j,k}^* }, </math>\n:<math> L_{i,j} = \\frac{1}{L_{j,j}} \\left( A_{i,j} - \\sum_{k=1}^{j-1} L_{i,k} L_{j,k}^* \\right) \\quad \\text{for } i>j. </math>\n\nSo we can compute the (''i'', ''j'') entry if we know the entries to the left and above. The computation is usually arranged in either of the following orders:\n* The '''Cholesky–Banachiewicz algorithm''' starts from the upper left corner of the matrix ''L'' and proceeds to calculate the matrix row by row.\n* The '''Cholesky–Crout algorithm''' starts from the upper left corner of the matrix ''L'' and proceeds to calculate the matrix column by column.\n\nEither pattern of access allows the entire computation to be performed in-place if desired.\n\n=== Stability of the computation ===\nSuppose that we want to solve a [[condition number|well-conditioned]] system of linear equations. If the LU decomposition is used, then the algorithm is unstable unless we use some sort of pivoting strategy. In the latter case, the error depends on the so-called growth factor of the matrix, which is usually (but not always) small.\n\nNow, suppose that the Cholesky decomposition is applicable. As mentioned above, the algorithm will be twice as fast. Furthermore, no pivoting is necessary, and the error will always be small. Specifically, if we want to solve '''Ax''' = '''b''', and '''y''' denotes the computed solution, then '''y''' solves the perturbed system ('''A''' + '''E''')'''y''' = '''b''', where\n:<math> \\|\\mathbf{E}\\|_2 \\le c_n \\varepsilon \\|\\mathbf{A}\\|_2. </math>\nHere ||·||<sub>2</sub> is the [[matrix norm|matrix 2-norm]], ''c<sub>n</sub>'' is a small constant depending on ''n'', and ε denotes the [[unit round-off]].\n\nOne concern with the Cholesky decomposition to be aware of is the use of square roots. If the matrix being factorized is positive definite as required, the numbers under the square roots are always positive ''in exact arithmetic''. Unfortunately, the numbers can become negative because of [[round-off error]]s, in which case the algorithm cannot continue. However, this can only happen if the matrix is very ill-conditioned. One way to address this is to add a diagonal correction matrix to the matrix being decomposed in an attempt to promote the positive-definiteness.<ref>{{cite journal|last=Fang|first=Haw-ren|author2=O'Leary, Dianne P.|author2-link= Dianne P. O'Leary |title=Modified Cholesky Algorithms: A Catalog with New Approaches|date=8 August 2006|url=http://www.cs.umd.edu/~oleary/tr/tr4807.pdf}}</ref> While this might lessen the accuracy of the decomposition, it can be very favorable for other reasons; for example, when performing [[Newton's method in optimization]], adding a diagonal matrix can improve stability when far from the optimum.\n\n=== LDL decomposition ===\nAn alternative form, eliminating the need to take square roots, is the symmetric indefinite factorization<ref>{{cite book |first=D. |last=Watkins |year=1991 |title=Fundamentals of Matrix Computations |location=New York |publisher=Wiley |page=84 |isbn=0-471-61414-9 }}</ref>\n:<math>\n\\begin{align}\n\\mathbf{A} = \\mathbf{LDL}^\\mathrm{T} & =\n\\begin{pmatrix}   1 & 0 & 0 \\\\\n   L_{21} & 1 & 0 \\\\\n   L_{31} & L_{32} & 1\\\\\n\\end{pmatrix}\n\\begin{pmatrix}   D_1 & 0 & 0 \\\\\n   0 & D_2 & 0 \\\\\n   0 & 0 & D_3\\\\\n\\end{pmatrix}\n\\begin{pmatrix}   1 & L_{21} & L_{31} \\\\\n   0 & 1 & L_{32} \\\\\n   0 & 0 & 1\\\\\n\\end{pmatrix} \\\\\n& = \\begin{pmatrix}   D_1 &   &(\\mathrm{symmetric})   \\\\\n   L_{21}D_1 & L_{21}^2D_1 + D_2& \\\\\n   L_{31}D_1 & L_{31}L_{21}D_{1}+L_{32}D_2 & L_{31}^2D_1 + L_{32}^2D_2+D_3.\n\\end{pmatrix}.\n\\end{align}\n</math>\n\nIf '''A''' is real, the following recursive relations apply for the entries of '''D''' and '''L''':\n:<math> D_j = A_{jj} - \\sum_{k=1}^{j-1} L_{jk}^2 D_k, </math>\n:<math> L_{ij} = \\frac{1}{D_j} \\left( A_{ij} - \\sum_{k=1}^{j-1} L_{ik} L_{jk} D_k \\right) \\quad \\text{for } i>j. </math>\n\nFor complex Hermitian matrix '''A''', the following formula applies:\n:<math> D_{j} = A_{jj} - \\sum_{k=1}^{j-1} L_{jk}L_{jk}^* D_k, </math>\n:<math> L_{ij} = \\frac{1}{D_j} \\left( A_{ij} - \\sum_{k=1}^{j-1} L_{ik} L_{jk}^* D_k \\right) \\quad \\text{for } i>j. </math>\n\nAgain, the pattern of access allows the entire computation to be performed in-place if desired.\n\n===Block variant===\nWhen used on indefinite matrices, the '''LDL'''* factorization is known to be unstable without careful pivoting;<ref>{{cite book|last=Nocedal|first=Jorge|title=Numerical Optimization|year=2000|publisher=Springer}}</ref> specifically, the elements of the factorization can grow arbitrarily. A possible improvement is to perform the factorization on block sub-matrices, commonly 2 × 2:<ref>{{cite journal|last=Fang|first=Haw-ren|title=Analysis of Block LDLT Factorizations for Symmetric Indefinite Matrices|date=24 August 2007}}</ref>\n\n:<math>\\begin{align}\n\\mathbf{A} = \\mathbf{LDL}^\\mathrm{T} & =\n\\begin{pmatrix}\n \\mathbf I & 0 & 0 \\\\\n \\mathbf L_{21} & \\mathbf I & 0 \\\\\n \\mathbf L_{31} & \\mathbf L_{32} & \\mathbf I\\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n \\mathbf D_1 & 0 & 0 \\\\\n 0 & \\mathbf D_2 & 0 \\\\\n 0 & 0 & \\mathbf D_3\\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n \\mathbf I & \\mathbf L_{21}^\\mathrm T & \\mathbf L_{31}^\\mathrm T \\\\\n 0 & \\mathbf I & \\mathbf L_{32}^\\mathrm T \\\\\n 0 & 0 & \\mathbf I\\\\\n\\end{pmatrix} \\\\\n& = \\begin{pmatrix}\n \\mathbf D_1 &   &(\\mathrm{symmetric})   \\\\\n \\mathbf L_{21} \\mathbf D_1 & \\mathbf L_{21} \\mathbf D_1 \\mathbf L_{21}^\\mathrm T + \\mathbf D_2& \\\\\n \\mathbf L_{31} \\mathbf D_1 & \\mathbf  L_{31} \\mathbf D_{1} \\mathbf L_{21}^\\mathrm T + \\mathbf  L_{32} \\mathbf D_2 & \\mathbf L_{31} \\mathbf D_1 \\mathbf L_{31}^\\mathrm T + \\mathbf L_{32} \\mathbf D_2 \\mathbf L_{32}^\\mathrm T + \\mathbf D_3\n\\end{pmatrix},\n\\end{align}\n</math>\n\nwhere every element in the matrices above is a square submatrix. From this, these analogous recursive relations follow:\n\n:<math>\\mathbf D_j = \\mathbf A_{jj} - \\sum_{k=1}^{j-1} \\mathbf L_{jk} \\mathbf D_k \\mathbf L_{jk}^\\mathrm T,</math>\n:<math>\\mathbf L_{ij} = \\left(\\mathbf A_{ij} - \\sum_{k=1}^{j-1} \\mathbf L_{ik} \\mathbf D_k \\mathbf L_{jk}^\\mathrm T\\right) \\mathbf D_j^{-1}.</math>\n\nThis involves matrix products and explicit inversion, thus limiting the practical block size.\n\n===Updating the decomposition===\nA task that often arises in practice is that one needs to update a Cholesky decomposition. In more details, one has already computed the Cholesky decomposition <math>\\mathbf{A} = \\mathbf{L}\\mathbf{L}^*</math> of some matrix <math>\\mathbf{A}</math>, then one changes the matrix <math>\\mathbf{A}</math> in some way into another matrix, say <math> \\tilde{\\mathbf{A}} </math>, and one wants to compute the Cholesky decomposition of the updated matrix: <math> \\tilde{\\mathbf{A}} = \\tilde{\\mathbf{L}} \\tilde{\\mathbf{L}}^* </math>. The question is now whether one can use the Cholesky decomposition of <math>\\mathbf{A}</math> that was computed before to compute the Cholesky decomposition of <math> \\tilde{\\mathbf{A}} </math>.\n\n==== Rank-one update ====\nThe specific case, where the updated matrix <math> \\tilde{\\mathbf{A}} </math> is related to the matrix <math>\\mathbf{A}</math> by <math> \\tilde{\\mathbf{A}} = \\mathbf{A} + \\mathbf{x} \\mathbf{x}^* </math>, is known as a ''rank-one update''.\n\nHere is a little function<ref>Based on: {{cite book|last=Stewart|first=G. W.|title=Basic decompositions|year=1998|publisher=Soc. for Industrial and Applied Mathematics|location=Philadelphia|isbn=0-89871-414-1}}</ref> written in [[Matlab]] syntax that realizes a rank-one update:\n<syntaxhighlight lang=\"matlab\">\nfunction [L] = cholupdate(L, x)\n    n = length(x);\n    for k = 1:n\n        r = sqrt(L(k, k)^2 + x(k)^2);\n        c = r / L(k, k);\n        s = x(k) / L(k, k);\n        L(k, k) = r;\n        if k < n\n            L((k+1):n, k) = (L((k+1):n, k) + s * x((k+1):n)) / c;\n            x((k+1):n) = c * x((k+1):n) - s * L((k+1):n, k);\n        end\n    end\nend\n</syntaxhighlight>\n\n==== Rank-one downdate ====\nA ''rank-one downdate'' is similar to a rank-one update, except that the addition is replaced by subtraction: <math> \\tilde{\\mathbf{A}} = \\mathbf{A} - \\mathbf{x} \\mathbf{x}^* </math>. This only works if the new matrix <math> \\tilde{\\mathbf{A}} </math> is still positive definite.\n\nThe code for the rank-one update shown above can easily be adapted to do a rank-one downdate: one merely needs to replace the two additions in the assignment to <code> r </code> and <code> L((k+1):n, k) </code> by subtractions.\n\n==== Adding and removing rows and columns ====\nIf we have a symmetric and positive definite matrix <math> \\mathbf A </math> represented in block form as\n\n:<math>\n\\mathbf{A} = \n\\begin{pmatrix}\n \\mathbf A_{11} & \\mathbf A_{13} \\\\\n \\mathbf A_{13}^{\\mathrm{T}} & \\mathbf A_{33} \\\\\n\\end{pmatrix}\n</math>\n\nand its upper Cholesky factor\n:<math>\n\\mathbf{L} = \n\\begin{pmatrix}\n \\mathbf L_{11} &  \\mathbf L_{13} \\\\\n  0      & \\mathbf L_{33} \\\\\n\\end{pmatrix},\n</math>\n\nthen for a new matrix <math> \\tilde{\\mathbf{A}} </math>, which is the same as <math> \\mathbf A </math> but with the insertion of new rows and columns,\n:<math>\\begin{align}\n\\tilde{\\mathbf{A}} &= \n\\begin{pmatrix}\n \\mathbf A_{11} & \\mathbf A_{12} & \\mathbf A_{13} \\\\\n \\mathbf A_{12}^{\\mathrm{T}} & \\mathbf A_{22} & \\mathbf A_{23} \\\\\n \\mathbf A_{13}^{\\mathrm{T}} & \\mathbf A_{23}^{\\mathrm{T}} &  \\mathbf A_{33} \\\\\n\\end{pmatrix}\n\\end{align}\n</math>\n\nwe are interested in finding the Cholesky factorisation of <math> \\tilde{\\mathbf{A}} </math>, which we call <math> \\tilde{\\mathbf S} </math>, without directly computing the entire decomposition. \n:<math>\\begin{align}\n\\tilde{\\mathbf{S}} &= \n\\begin{pmatrix}\n \\mathbf S_{11} & \\mathbf S_{12} & \\mathbf S_{13} \\\\\n 0 & \\mathbf S_{22} & \\mathbf S_{23} \\\\\n 0 & 0 &  \\mathbf S_{33} \\\\\n\\end{pmatrix}.\n\\end{align}\n</math>\n\nWriting <math> \\mathbf A \\setminus \\mathbf{b}</math> for the solution of <math> \\mathbf A \\mathbf x = \\mathbf b</math>, which can be found easily for triangular matrices, and <math> \\text{chol} (\\mathbf M)</math> for the Cholesky decomposition of <math> \\mathbf M </math>, the following relations can be found:\n:<math>\\begin{align}\n\\mathbf S_{11} &= \\mathbf L_{11}, \\\\\n\\mathbf S_{12} &= \\mathbf L_{11}^{\\mathrm{T}} \\setminus \\mathbf A_{12}, \\\\\n\\mathbf S_{13} &= \\mathbf L_{13}, \\\\\n\\mathbf S_{22} &= \\text{chol} (\\mathbf A_{22}  - \\mathbf S_{12}^{\\mathrm{T}} \\mathbf S_{12}), \\\\\n\\mathbf S_{23} &= \\mathbf S_{22}^{\\mathrm{T}} \\setminus (\\mathbf A_{23} - \\mathbf S_{12}^{\\mathrm{T}} \\mathbf S_{13}), \\\\\n\\mathbf S_{33} &= \\text{chol} (\\mathbf L_{33}^{\\mathrm{T}} \\mathbf L_{33} - \\mathbf S_{23}^{\\mathrm{T}} \\mathbf S_{23}). \\\\\n\\end{align}\n</math>\n\nThese formulas may be used to determine the Cholesky factor after the insertion of rows or columns in any position, if we set the row and column dimensions appropriately (including to zero). The inverse problem, when we have\n\n:<math>\\begin{align}\n\\tilde{\\mathbf{A}} &= \n\\begin{pmatrix}\n \\mathbf A_{11} & \\mathbf A_{12} & \\mathbf A_{13} \\\\\n \\mathbf A_{12}^{\\mathrm{T}} & \\mathbf A_{22} & \\mathbf A_{23} \\\\\n \\mathbf A_{13}^{\\mathrm{T}} & \\mathbf A_{23}^{\\mathrm{T}} &  \\mathbf A_{33} \\\\\n\\end{pmatrix}\n\\end{align}\n</math>\nwith known Cholesky decomposition \n:<math>\\begin{align}\n\\tilde{\\mathbf{S}} &= \n\\begin{pmatrix}\n \\mathbf S_{11} & \\mathbf S_{12} & \\mathbf S_{13} \\\\\n 0 & \\mathbf S_{22} & \\mathbf S_{23} \\\\\n 0 & 0 &  \\mathbf S_{33} \\\\\n\\end{pmatrix}\n\\end{align}\n</math>\n\nand wish to determine the Cholesky factor\n:<math>\\begin{align}\n\\mathbf{L} &= \n\\begin{pmatrix}\n \\mathbf L_{11} &  \\mathbf L_{13} \\\\\n  0      & \\mathbf L_{33} \\\\\n\\end{pmatrix}\n\\end{align}\n</math>\n\nof the matrix <math> \\mathbf A </math> with rows and columns removed,\n:<math>\\begin{align}\n\\mathbf{A} &= \n\\begin{pmatrix}\n \\mathbf A_{11} & \\mathbf A_{13} \\\\\n \\mathbf A_{13}^{\\mathrm{T}} & \\mathbf A_{33} \\\\\n\\end{pmatrix},\n\\end{align}\n</math>\n\nyields the following rules:\n:<math>\\begin{align}\n\\mathbf L_{11} &= \\mathbf S_{11}, \\\\\n\\mathbf L_{13} &= \\mathbf S_{13}, \\\\\n\\mathbf L_{33} &= \\text{chol} (\\mathbf S_{33}^{\\mathrm{T}} \\mathbf S_{33} + \\mathbf S_{23}^{\\mathrm{T}} \\mathbf S_{23}).\n\\end{align}\n</math>\n\nNotice that the equations above that involve finding the Cholesky decomposition of a new matrix are all of the form <math> \\tilde{\\mathbf{A}} = \\mathbf{A} \\pm \\mathbf{x} \\mathbf{x}^* </math>, which allows them to be efficiently calculated using the update and downdate procedures detailed in the previous section.<ref>Osborne, M. (2010), Appendix B.</ref>\n\n== Proof for positive semi-definite matrices ==\n\n=== Proof by limiting argument ===\nThe above algorithms show that every positive definite matrix <math> \\mathbf{A} </math> has a Cholesky decomposition. This result can be extended to the positive semi-definite case by a limiting argument. The argument is not fully constructive, i.e., it gives no explicit numerical algorithms for computing Cholesky factors.\n\nIf <math> \\mathbf{A} </math> is an <math> n \\times n </math> [[Positive-definite matrix|positive semi-definite matrix]], then the sequence <math> \\left(\\mathbf{A}_k\\right)_k := \\left(\\mathbf{A} + \\frac{1}{k} \\mathbf{I}_n\\right)_k </math> consists of [[Positive-definite matrix|positive definite matrices]]. (This is an immediate consequence of, for example, the spectral mapping theorem for the polynomial functional calculus.) Also,\n:<math> \n\\mathbf{A}_k \\rightarrow \\mathbf{A}\n\\quad \\text{for} \\quad\nk \\rightarrow \\infty\n</math> \nin [[operator norm]]. From the positive definite case, each <math> \\mathbf{A}_k </math> has Cholesky decomposition <math> \\mathbf{A}_k = \\mathbf{L}_k\\mathbf{L}_k^* </math>. By property of the operator norm,\n\n:<math>\\| \\mathbf{L}_k \\|^2 \\geq \\| \\mathbf{L}_k \\mathbf{L}_k^* \\| = \\| \\mathbf{A}_k \\| \\,.</math>\n\nSo <math> \\left(\\mathbf{L}_k \\right)_k</math> is a bounded set in the [[Banach space]] of operators, therefore [[relatively compact]] (because the underlying vector space is finite-dimensional). \nConsequently, it has a convergent subsequence, also denoted by <math> \\left( \\mathbf{L}_k \\right)_k</math>, with limit <math> \\mathbf{L}</math>. \nIt can be easily checked that this <math> \\mathbf{L}</math> has the desired properties, i.e. <math> \\mathbf{A} = \\mathbf{L}\\mathbf{L}^* </math>, and <math> \\mathbf{L}</math> is lower triangular with non-negative diagonal entries: for all <math> x</math> and <math> y</math>,\n\n:<math> \n\\langle \\mathbf{A} x, y \\rangle \n= \\left\\langle \\lim \\mathbf{A}_k x, y \\right\\rangle \n= \\langle \\lim \\mathbf{L}_k \\mathbf{L}_k^* x, y \\rangle \n= \\langle \\mathbf{L} \\mathbf{L}^*x, y \\rangle \\,. \n</math>\n\nTherefore, <math> \\mathbf{A} = \\mathbf{L}\\mathbf{L}^* </math>. \nBecause the underlying vector space is finite-dimensional, all topologies on the space of operators are equivalent. \nSo <math> \\left( \\mathbf{L}_k \\right)_k</math> tends to <math> \\mathbf{L}</math> in norm means <math> \\left( \\mathbf{L}_k \\right)_k</math> tends to <math> \\mathbf{L}</math> entrywise. \nThis in turn implies that, since each <math> \\mathbf{L}_k</math> is lower triangular with non-negative diagonal entries, <math> \\mathbf{L}</math> is also.\n\n=== Proof by QR decomposition ===\n\nLet <math>\\mathbf{A}</math> be a [[Positive-definite matrix|positive semi-definite matrix]] Hermitian matrix. Then it can be written as a product of its [[Square root of a matrix|square root matrix]], <math>\\mathbf{A} = \\mathbf{B} \\mathbf{B}^*</math>. Now [[QR decomposition]] can be applied to <math>\\mathbf{B}^*</math>, resulting in <math>\\mathbf{B}^* = \\mathbf{Q}\\mathbf{R}</math>\n, where <math>\\mathbf{Q}</math> is unitary and <math>\\mathbf{R}</math> is upper triangular. Inserting the decomposition into the original equality yields <math>A = \\mathbf{B} \\mathbf{B}^* = (\\mathbf{QR})^*\\mathbf{QR} = \\mathbf{R}^*\\mathbf{Q}^*\\mathbf{QR} = \\mathbf{R}^*\\mathbf{R}</math>. Setting <math>\\mathbf{L} = \\mathbf{R}^*</math> completes the proof.\n\n== Generalization ==\nThe Cholesky factorization can be generalized {{Citation needed|date=October 2016}} to (not necessarily finite) matrices with operator entries. Let <math>\\{\\mathcal{H}_n \\}</math> be a sequence of [[Hilbert spaces]]. Consider the operator matrix\n\n:<math>\n\\mathbf{A} =\n\\begin{bmatrix}\n\\mathbf{A}_{11}   & \\mathbf{A}_{12}   & \\mathbf{A}_{13} & \\; \\\\\n\\mathbf{A}_{12}^* & \\mathbf{A}_{22}   & \\mathbf{A}_{23} & \\; \\\\\n\\mathbf{A} _{13}^* & \\mathbf{A}_{23}^* & \\mathbf{A}_{33} & \\; \\\\\n\\;       & \\;       & \\;     & \\ddots\n\\end{bmatrix}\n</math>\n\nacting on the direct sum\n\n:<math>\\mathcal{H} = \\oplus _n  \\mathcal{H}_n,</math>\n\nwhere each\n\n:<math>\\mathbf{A}_{ij} : \\mathcal{H}_j \\rightarrow \\mathcal{H} _i</math>\n\nis a [[bounded operator]]. If '''A''' is positive (semidefinite) in the sense that for all finite ''k'' and for any\n\n:<math>h \\in \\oplus _{n = 1 }^k \\mathcal{H}_k ,</math>\n\nwe have <math>\\langle h, \\mathbf{A} h\\rangle \\ge 0</math>, then there exists a lower triangular operator matrix '''L''' such that '''A''' = '''LL'''*. One can also take the diagonal entries of '''L''' to be positive.\n\n== Implementations in programming libraries ==\n* [[C programming language]]: the [[GNU Scientific Library]] provides several implementations of Cholesky decomposition.\n* [[Maxima (software)|Maxima]] computer algebra system: function ''cholesky'' computes Cholesky decomposition.\n* [[GNU Octave]] numerical computations system provides several functions to calculate, update, and apply a Cholesky decomposition.\n* The [[LAPACK]] library provides a high performance implementation of the Cholesky decomposition that can be accessed from Fortran, C and most languages.\n* In Python, the function \"cholesky\" from the numpy.linalg module performs Cholesky decomposition.\n* In Matlab and R, the \"chol\" function gives the Cholesky decomposition.. \n* In [[Julia (programming language)|Julia]], the \"cholesky\" function from the LinearAlgebra package gives the Cholesky decomposition.\n* In [[Mathematica]], the function \"CholeskyDecomposition\" can be applied to a matrix.\n* In C++, the command \"chol\" from the armadillo library performs Cholesky decomposition.  The [[Eigen (C++ library)|Eigen library]] supplies Cholesky factorizations for both sparse and dense matrices.\n* In the [[ROOT]] package, the TDecompChol class is available. \n* In [[Analytica (software)|Analytica]], the function Decompose gives the Cholesky decomposition.\n* The [http://commons.apache.org/proper/commons-math/javadocs/api-3.4/org/apache/commons/math3/linear/CholeskyDecomposition.html Apache Commons Math library has an implementation] which can be used in Java, Scala and any other JVM language.\n\n==See also==\n* [[Cycle rank]]\n* [[Incomplete Cholesky factorization]]\n* [[Matrix decomposition]]\n* [[Minimum degree algorithm]]\n* [[Square root of a matrix]]\n* [[Sylvester's law of inertia]]\n* [[Symbolic Cholesky decomposition]]\n\n==Notes==\n{{Reflist}}\n\n==References==\n* {{Cite conference\n | last1 = Dereniowski\n | first1 = Dariusz\n | last2 = Kubale\n | first2 = Marek\n | contribution = Cholesky Factorization of Matrices in Parallel and Ranking of Graphs\n | doi = 10.1007/978-3-540-24669-5_127\n | pages = 985–992\n | publisher = Springer-Verlag\n | series = Lecture Notes on Computer Science\n | title = 5th International Conference on Parallel Processing and Applied Mathematics\n | url = http://www.eti.pg.gda.pl/katedry/kams/wwwkams/pdf/Cholesky_fmprg.pdf\n | volume = 3019\n | year = 2004\n | isbn = 978-3-540-21946-0\n | deadurl = yes\n | archiveurl = https://web.archive.org/web/20110716060800/http://www.eti.pg.gda.pl/katedry/kams/wwwkams/pdf/Cholesky_fmprg.pdf\n | archivedate = 2011-07-16\n | df = \n }}\n* {{Cite book| first1=Gene H. | last1=Golub | author1-link=Gene H. Golub | first2=Charles F. | last2=Van Loan | author2-link=Charles F. Van Loan | year=1996 | title=Matrix Computations | edition=3rd | publisher=Johns Hopkins | place=Baltimore | isbn=978-0-8018-5414-9| ref=harv }}\n* {{Cite book| first1=Roger A. | last1=Horn | first2=Charles R. | last2=Johnson | year=1985 | title=Matrix Analysis | publisher=Cambridge University Press | isbn=0-521-38632-2 | ref=harv }}\n* S. J. Julier and J. K. Uhlmann. \"A General Method for Approximating Nonlinear Transformations of ProbabilityDistributions\".\n* S. J. Julier and J. K. Uhlmann, \"A new extension of the Kalman filter to nonlinear systems\", in Proc. AeroSense: 11th Int. Symp. Aerospace/Defence Sensing, Simulation and Controls, 1997, pp.&nbsp;182–193.\n* {{Cite book| last1=Trefethen | first1=Lloyd N. | author1-link=Lloyd N. Trefethen | last2=Bau | first2=David | title=Numerical linear algebra | publisher=Society for Industrial and Applied Mathematics | location=Philadelphia | isbn=978-0-89871-361-9 | year=1997| ref=harv }}\n* {{cite thesis |last= Osborne|first= Michael|date= 2010|title= Bayesian Gaussian Processes for Sequential Prediction, Optimisation and Quadrature|type= thesis|chapter= |publisher= University of Oxford|docket= |oclc= |url= http://www.robots.ox.ac.uk/~mosb/public/pdf/2160/full_thesis.pdf |access-date=}}\n\n== External links ==\n\n===History of science===\n* ''Sur la résolution numérique des systèmes d'équations linéaires'', Cholesky's 1910 manuscript, online and analyzed on [http://bibnum.education.fr/mathematiques/algebre/sur-la-resolution-numerique-des-systemes-d-equations-lineaires BibNum] {{fr icon}} {{en icon}} <small>[for English, click 'A télécharger']</small>\n\n===Information===\n* {{springer|title=Cholesky factorization|id=p/c120160}}\n* {{planetmath reference|id=1287|title=Cholesky Decomposition}}\n* [https://web.archive.org/web/20060518112024/http://rkb.home.cern.ch/rkb/AN16pp/node33.html#SECTION000330000000000000000 Cholesky Decomposition], The Data Analysis BriefBook\n* [http://www.math-linux.com/spip.php?article43 Cholesky Decomposition] on www.math-linux.com\n* [http://sciencemeanderthal.wordpress.com/2012/06/28/cholesky-decomposition-of-variance-covariance-matrices-in-the-classic-twin-study/ Cholesky Decomposition Made Simple] on Science Meanderthal\n\n===Computer code===\n* [http://netlib.org/lapack/ LAPACK] is a collection of FORTRAN subroutines for solving dense linear algebra problems\n* [http://www.alglib.net/ ALGLIB] includes a partial port of the LAPACK to C++, C#, Delphi, Visual Basic, etc.\n* [http://www.cs.utexas.edu/users/flame/ libflame] is a C library with LAPACK functionality.\n* [http://www.cs.utexas.edu/users/flame/Movies.html#Chol Notes and video on high-performance implementation of Cholesky factorization] at The University of Texas at Austin.\n* [http://upcommons.upc.edu/pfc/handle/2099.1/10988/ Cholesky : TBB + Threads + SSE] is a book explaining the implementation of the CF with TBB, threads and SSE (in Spanish).\n* [http://ceres-solver.org/ library \"Ceres Solver\" ] by Google.\n* [http://infohost.nmt.edu/~borchers/ldlt.html LDL decomposition] routines in Matlab.\n* [http://arma.sourceforge.net/download.html Armadillo] is a C++ linear algebra package\n\n===Use of the matrix in simulation===\n* [http://www.columbia.edu/~mh2078/MonteCarlo/MCS_Generate_RVars.pdf Generating Correlated Random Variables and Stochastic Processes], Martin Haugh, [[Columbia University]]\n\n===Online calculators===\n* [http://www.bluebit.gr/matrix-calculator/  Online Matrix Calculator] Performs Cholesky decomposition of matrices online.\n\n{{Numerical linear algebra}}\n\n[[Category:Operator theory]]\n[[Category:Matrix decompositions]]\n[[Category:Numerical linear algebra]]\n<!-- Dummy edit -->"
    },
    {
      "title": "Circulant matrix",
      "url": "https://en.wikipedia.org/wiki/Circulant_matrix",
      "text": "{{short description|Matrix in which each row is rotated one position to the right from the previous row}}\n{{For|the symmetric graphs|Circulant graph}}\n\nIn [[linear algebra]], a '''circulant matrix''' is a special kind of [[Toeplitz matrix]] where each [[row vector]] is rotated one element to the right relative to the preceding row vector. In [[numerical analysis]], circulant matrices are important because they are diagonalized by a [[discrete Fourier transform]], and hence [[linear equation]]s that contain them may be quickly solved using a [[fast Fourier transform]].<ref>[[Philip J. Davis|Davis, Philip J.]], Circulant Matrices, Wiley, New York, 1970 {{ISBN|0471057711}}</ref>  They can be [[#Analytic interpretation|interpreted analytically]] as the [[integral kernel]] of a [[convolution operator]] on the [[cyclic group]] <math>C_n</math> and hence frequently appear in formal descriptions of spatially invariant linear operations.\n\nIn [[cryptography]], a circulant matrix is used in the [[Rijndael MixColumns|MixColumns]] step of the [[Advanced Encryption Standard]].\n\n==Definition==\n\nAn <math>n\\times n</math> circulant matrix <math>C</math> takes the form\n\n:<math>\nC=\n\\begin{bmatrix}\nc_0     & c_{n-1} & \\dots  & c_{2} & c_{1}  \\\\\nc_{1} & c_0    & c_{n-1} &         & c_{2}  \\\\\n\\vdots  & c_{1}& c_0    & \\ddots  & \\vdots   \\\\\nc_{n-2}  &        & \\ddots & \\ddots  & c_{n-1}   \\\\\nc_{n-1}  & c_{n-2} & \\dots  & c_{1} & c_0 \\\\\n\\end{bmatrix}.\n</math>\n\nA circulant matrix is fully specified by one vector, <math>c</math>, which appears as the first column of <math>C</math>.  The remaining columns of <math>C</math> are each [[cyclic permutation]]s of the vector <math>c</math> with offset equal to the column index.  The last row of <math>C</math> is the vector <math>c</math> in reverse order, and the remaining rows are each [[cyclic permutation]]s of the last row.  Note that different sources define the circulant matrix in different ways, for example with the coefficients corresponding to the first row rather than the first column of the matrix, or with a different direction of shift.\n\nThe polynomial <math> f(x) = c_0 + c_1 x + \\dots + c_{n-1} x^{n-1} </math> is called the ''associated polynomial'' of matrix <math>C</math>.\n\n== Properties ==\n\n=== Eigenvectors and eigenvalues ===\n\nThe normalized [[eigenvector]]s of a circulant matrix are given by\n\n:<math>v_j=\\frac{1}{\\sqrt{n}} (1, \\omega_j, \\omega_j^2, \\ldots, \\omega_j^{n-1}),\\quad j= 0, 1,\\ldots, n-1,</math>\nwhere <math>\\omega_j=\\exp \\left(i \\tfrac{2\\pi j}{n}\\right)</math> are the <math>n</math>-th [[roots of unity]] and <math>i</math> is the [[imaginary unit]].\n\nThe corresponding eigenvalues are then given by\n\n:<math>\\lambda_j = c_0+c_{n-1} \\omega_j + c_{n-2} \\omega_j^2 + \\ldots + c_{1} \\omega_j^{n-1}, \\qquad j=0,1,\\ldots, n-1.</math>\n\n=== Determinant ===\n\nAs a consequence of the explicit formula for the eigenvalues above, \nthe [[determinant]] of circulant matrix can be computed as:\n:<math>\n\\det(C) \n= \\prod_{j=0}^{n-1} (c_0 + c_{n-1} \\omega_j + c_{n-2} \\omega_j^2 + \\dots + c_1\\omega_j^{n-1}).</math>\nSince taking transpose does not change the eigenvalues of a matrix, an equivalent formulation is\n:<math>\n\\det(C)=\\prod_{j=0}^{n-1} (c_0 + c_1 \\omega_j + c_2 \\omega_j^2 + \\dots + c_{n-1}\\omega_j^{n-1}) = \\prod_{j=0}^{n-1} f(\\omega_j).\n</math>\n\n=== Rank ===\n\nThe [[Rank (linear algebra)|rank]] of a circulant matrix <math> C </math> is equal to <math> n - d </math>, where <math> d </math> is the [[degree of a polynomial|degree]] of <math> \\gcd( f(x), x^n - 1) </math>.<ref>{{cite journal |author=A. W. Ingleton |title=The Rank of Circulant Matrices |journal=J. London Math. Soc. |year=1956 |volume=s1-31 |issue=4 |pages=445–460 |doi=10.1112/jlms/s1-31.4.445}}</ref>\n\n=== Other properties ===\n\n* We have\n::<math> C=c_0I+c_1P+c_2P^2+\\ldots+c_{n-1}P^{n-1}=f(P).</math>\n:where <math>P</math> is the 'cyclic permutation' matrix, a specific [[permutation matrix]] given by\n::<math>P=\n\\begin{bmatrix}\n 0&0&\\ldots&0&1\\\\\n 1&0&\\ldots&0&0\\\\\n 0&\\ddots&\\ddots&\\vdots&\\vdots\\\\\n \\vdots&\\ddots&\\ddots&0&0\\\\\n 0&\\ldots&0&1&0\n\\end{bmatrix}.</math>\n\n* The set of <math>n\\times n</math> circulant matrices forms an <math>n</math>-[[dimensional]] [[vector space]] with respect to their standard addition and scalar multiplication.  This space can be interpreted as the space of functions on the [[cyclic group]] of order ''n'', <math>C_n</math>, or equivalently as the [[group ring]] of <math>C_n</math>.\n* Circulant matrices form a [[commutative algebra]], since for any two given circulant matrices <math>A</math> and <math>B</math>, the sum <math>A + B</math> is circulant, the product <math>AB</math> is circulant, and <math>AB = BA</math>.\n* The matrix <math>U</math> that is composed of the [[eigenvectors]] of a circulant matrix is related to the [[Discrete Fourier transform#The unitary DFT|discrete Fourier transform]] and its inverse transform: \n::<math> U_n^* = \\frac{1}{\\sqrt{n}} F_n, \\quad\\text{and}\\quad U_n = \\sqrt{n} F_n^{-1}, \\quad\\text{where}\\quad F_n = (f_{jk}) \\quad\\text{with}\\quad f_{jk} = e^{-2jk\\pi i/n},  \\quad\\text{for}\\quad  0\\leq j,k<n.</math>\n:Consequently the matrix <math>U_n</math> [[diagonalizable matrix|diagonalizes]] <math>C</math>. In fact, we have\n::<math> C = U_n \\operatorname{diag}(F_n c) U_n^* = F_n^{-1} \\operatorname{diag}(F_n c) F_n, </math>\n:where <math>c</math> is the first column of <math>C</math>. The eigenvalues of <math>C</math> are given by the product <math>F_n c</math>. This product can be readily calculated by a [[fast Fourier transform]].<ref>{{Citation | last1=Golub | first1=Gene H. | author1-link=Gene H. Golub | last2=Van Loan | first2=Charles F. | author2-link=Charles F. Van Loan | title=Matrix Computations | chapter=§4.7.7 Circulant Systems | publisher=Johns Hopkins | edition=3rd | isbn=978-0-8018-5414-9 | year=1996}}</ref>\n*Let <math>p</math> be the monic characteristic polynomial of an <math>n\\times n</math> circulant matrix <math>C</math>, and let <math>p'</math> be the derivative of <math>p</math>. Then the polynomial <math>\\frac{1}{n}p'</math> is the monic characteristic polynomial of the following <math>(n-1)\\times(n-1)</math> submatrix of <math>C</math>:\n:<math>\nC_{n-1}=\n\\begin{bmatrix}\n c_0     & c_{n-1} & \\dots   & c_{3}   & c_{2}   \\\\\n c_{1}   & c_0     & c_{n-1} &         & c_{3}   \\\\\n \\vdots  & c_{1}   & c_0     & \\ddots  & \\vdots  \\\\\n c_{n-3} &         & \\ddots  & \\ddots  & c_{n-1} \\\\\n c_{n-2} & c_{n-3} & \\dots   & c_{1}   & c_0     \\\\\n\\end{bmatrix}\n</math>\n(see<ref>{{Citation | last1=Kushel | first1=Olga | last2=Tyaglov | first2=Mikhail | title=Circulants and critical points of polynomials |url = http://www.sciencedirect.com/science/article/pii/S0022247X16002237|journal = Journal of Mathematical Analysis and Applications| date=July 15, 2016| issn=0022-247X| pages=634–650|volume=439|issue=2| doi= 10.1016/j.jmaa.2016.03.005|arxiv=1512.07983}}</ref>  for proof).\n\n==Analytic interpretation==\nCirculant matrices can be interpreted geometrically, which explains the connection with the discrete Fourier transform.\n\nConsider vectors in <math>\\mathbf{R}^n</math> as functions on the integers with period <math>n</math>, (i.e., as periodic bi-infinite sequences: <math>\\dots,a_0,a_1,\\dots,a_{n-1},a_0,a_1,\\dots</math>) or equivalently, as functions on the [[cyclic group]] of order <math>n</math> (<math>C_n</math> or <math>\\mathbf{Z}/n\\mathbf{Z}</math>) geometrically, on (the vertices of) the regular <math>n</math>-gon: this is a discrete analog to periodic functions on the real line or circle.\n\nThen, from the perspective of [[operator theory]], a circulant matrix is the kernel of a discrete [[integral transform]], namely the [[convolution operator]] for the function <math>(c_0,c_1,\\dots,c_{n-1})</math>; this is a discrete [[circular convolution]]. The formula for the convolution of the functions <math>(b_i) := (c_i) * (a_i)</math> is\n:<math>b_k = \\sum_{i=0}^{n-1} a_i c_{k-i}</math> (recall that the sequences are periodic)\nwhich is the product of the vector <math>(a_i)</math> by the circulant matrix for <math>(c_i)</math>.\n\nThe discrete Fourier transform then converts convolution into multiplication, which in the matrix setting corresponds to diagonalization.\n\nThe <math>C^*</math>-algebra of all circulant matrices with complex entries is isomorphic to the group <math>C^*</math>-algebra of <math>\\mathbf{Z}/n\\mathbf{Z}</math>.\n\n==Symmetric circulant matrices==\nFor a symmetric circulant matrix <math>C</math> one has the extra condition that <math>c_{n-i}=c_i</math>. \nThus it is defined by <math>\\lfloor n/2\\rfloor + 1</math> elements. \n:<math>\nC=\n\\begin{bmatrix}\nc_0     & c_1 & \\dots  & c_{2} & c_{1}  \\\\\nc_{1} & c_0    & c_1 &         & c_{2}  \\\\\n\\vdots  & c_{1}& c_0    & \\ddots  & \\vdots   \\\\\nc_2  &        & \\ddots & \\ddots  & c_1   \\\\\nc_1  & c_2 & \\dots  & c_{1} & c_0 \\\\\n\\end{bmatrix}.\n</math>\n\nThe eigenvalues of any real symmetric matrix are real.\nThe corresponding eigenvalues become:\n:<math>\n\\lambda_j = c_0 + 2 c_1 \\Re \\omega_j + 2 c_2 \\Re \\omega_j^2 +  \\ldots + 2c_{n/2-1} \\Re \\omega_j^{n/2-1}  + c_{n/2} \\omega_j^{n/2} </math>\nfor <math>n</math> even, and\n:<math>\n\\lambda_j = c_0 + 2 c_1 \\Re \\omega_j + 2 c_2 \\Re \\omega_j^2 +  \\ldots + 2c_{(n-1)/2} \\Re \\omega_j^{(n-1)/2}\n </math> \nfor odd <math>n</math>.\nThis can be further simplified by using that <math>\\Re \\omega_j^k= \\cos(2\\pi j k/n)</math>.\n\n== Applications ==\n\n===In linear equations===\n\nGiven a matrix equation\n:<math>\\mathbf{C} \\mathbf{x} = \\mathbf{b},</math>\nwhere <math>C</math> is a circulant square matrix of size <math>n</math> we can write the equation as the [[circular convolution]]\n:<math>\\mathbf{c} \\star \\mathbf{x} = \\mathbf{b},</math>\nwhere <math>c</math> is the first column of <math>C</math>, and the vectors <math>c</math>, <math>x</math> and <math>b</math> are cyclically extended in each direction. Using the [[discrete Fourier transform#Circular convolution theorem and cross-correlation theorem|circular convolution theorem]], we can use the [[discrete Fourier transform]] to transform the cyclic convolution into component-wise multiplication\n\n:<math>\\mathcal{F}_{n}(\\mathbf{c} \\star \\mathbf{x}) = \\mathcal{F}_{n}(\\mathbf{c}) \\mathcal{F}_{n}(\\mathbf{x}) = \\mathcal{F}_{n}(\\mathbf{b})</math>\n\nso that\n\n:<math>\\mathbf{x} = \\mathcal{F}_{n}^{-1} \n\\left [ \n\\left (\n\\frac{(\\mathcal{F}_n(\\mathbf{b}))_{\\nu}}\n{(\\mathcal{F}_n(\\mathbf{c}))_{\\nu}} \n\\right )_{\\!\\nu \\in \\mathbf{Z}}\n\\right ]^{\\rm T}.\n</math>\n\nThis algorithm is much faster than the standard [[Gaussian elimination]], especially if a [[fast Fourier transform]] is used.\n\n=== In graph theory ===\n\nIn [[graph theory]], a [[Graph (discrete mathematics)|graph]] or [[Directed graph|digraph]] whose [[adjacency matrix]] is circulant is called a [[circulant graph]] (or digraph).  Equivalently, a graph is circulant if its [[automorphism group]] contains a full-length cycle. The [[Möbius ladder]]s are examples of circulant graphs, as are the [[Paley graph]]s for fields of prime order.\n\n==References==\n{{reflist}}\n\n==External links==\n* R. M. Gray, [http://www-ee.stanford.edu/~gray/toeplitz.pdf Toeplitz and Circulant Matrices: A Review]\n* {{MathWorld|id=CirculantMatrix|title=Circulant Matrix}}\n* [https://github.com/MMesch/toeplitz_spectrum/blob/master/toeplitz_spectrum.ipynb IPython Notebook demonstrating properties of circulant matrices]\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]\n[[Category:Matrices]]\n[[Category:Latin squares]]\n[[Category:Determinants]]"
    },
    {
      "title": "Conjugate residual method",
      "url": "https://en.wikipedia.org/wiki/Conjugate_residual_method",
      "text": "The '''conjugate residual method''' is an iterative [[numeric method]] used for solving [[systems of linear equations]]. It's a [[Krylov subspace method]] very similar to the much more popular [[conjugate gradient method]], with similar construction and convergence properties.\n\nThis method is used to solve linear equations of the form\n\n:<math>\\mathbf A \\mathbf x = \\mathbf b</math>\n\nwhere '''A''' is an invertible and [[Hermitian matrix]], and '''b''' is nonzero.\n\nThe conjugate residual method differs from the closely related [[conjugate gradient method]] primarily in that it involves more numerical operations and requires more storage, but the system matrix is only required to be Hermitian, not symmetric positive definite.\n\nGiven an (arbitrary) initial estimate of the solution <math>\\mathbf x_0</math>, the method is outlined below:\n\n:<math>\n\\begin{align}\n& \\mathbf{x}_0 := \\text{Some initial guess} \\\\\n& \\mathbf{r}_0 := \\mathbf{b} - \\mathbf{A x}_0 \\\\\n& \\mathbf{p}_0 := \\mathbf{r}_0 \\\\\n& \\text{Iterate, with } k \\text{ starting at } 0:\\\\\n& \\qquad \\alpha_k := \\frac{\\mathbf{r}_k^\\mathrm{T} \\mathbf{A r}_k}{(\\mathbf{A p}_k)^\\mathrm{T} \\mathbf{A p}_k} \\\\\n& \\qquad \\mathbf{x}_{k+1} := \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k \\\\\n& \\qquad \\mathbf{r}_{k+1} := \\mathbf{r}_k - \\alpha_k \\mathbf{A p}_k \\\\\n& \\qquad \\beta_k := \\frac{\\mathbf{r}_{k+1}^\\mathrm{T} \\mathbf{A r}_{k+1}}{\\mathbf{r}_k^\\mathrm{T} \\mathbf{A r}_k} \\\\\n& \\qquad \\mathbf{p}_{k+1} := \\mathbf{r}_{k+1} + \\beta_k \\mathbf{p}_k \\\\\n& \\qquad \\mathbf{A p}_{k + 1} := \\mathbf{A r}_{k+1} + \\beta_k \\mathbf{A p}_k \\\\\n& \\qquad k := k + 1  \n\\end{align}\n</math>\n\nthe iteration may be stopped once <math>\\mathbf x_k</math> has been deemed converged. The only difference between this and the conjugate gradient method is the calculation of <math>\\alpha_k</math> and <math>\\beta_k</math> (plus the optional incremental calculation of <math>\\mathbf{A p}_k</math> at the end).\n\nNote: the above algorithm can be transformed so to make only one symmetric matrix-vector multiplication in each iteration.\n\n==Preconditioning==\n\nBy making a few substitutions and variable changes, a preconditioned conjugate residual method may be derived in the same way as done for the conjugate gradient method:\n\n:<math>\n\\begin{align}\n& \\mathbf x_0 := \\text{Some initial guess} \\\\\n& \\mathbf r_0 := \\mathbf M^{-1}(\\mathbf b - \\mathbf{A x}_0) \\\\\n& \\mathbf p_0 := \\mathbf r_0 \\\\\n& \\text{Iterate, with } k \\text{ starting at } 0: \\\\\n& \\qquad \\alpha_k := \\frac{\\mathbf r_k^\\mathrm{T} \\mathbf A \\mathbf r_k}{(\\mathbf{A p}_k)^\\mathrm{T} \\mathbf M^{-1} \\mathbf{A p}_k}  \\\\\n& \\qquad \\mathbf x_{k+1} := \\mathbf x_k + \\alpha_k \\mathbf{p}_k \\\\\n& \\qquad \\mathbf r_{k+1} := \\mathbf r_k - \\alpha_k \\mathbf M^{-1} \\mathbf{A p}_k \\\\\n& \\qquad \\beta_k := \\frac{\\mathbf r_{k + 1}^\\mathrm{T} \\mathbf A \\mathbf r_{k + 1}}{\\mathbf r_k^\\mathrm{T} \\mathbf A \\mathbf r_k} \\\\\n& \\qquad \\mathbf p_{k+1} := \\mathbf r_{k+1} + \\beta_k \\mathbf{p}_k \\\\\n& \\qquad \\mathbf{A p}_{k + 1} := \\mathbf A \\mathbf r_{k+1} + \\beta_k \\mathbf{A p}_k \\\\\n& \\qquad k := k + 1 \\\\\n\\end{align}\n</math>\n\nThe [[preconditioner]] <math>\\mathbf M^{-1}</math> must be symmetric positive definite. Note that the residual vector here is different from the residual vector without preconditioning.\n\n==References==\n* [[Yousef Saad]], ''Iterative methods for sparse linear systems'' (2nd ed.), page 194, SIAM. {{ISBN|978-0-89871-534-7}}.\n\n[[Category:Numerical linear algebra]]\n[[Category:Articles with example pseudocode]]"
    },
    {
      "title": "Coppersmith–Winograd algorithm",
      "url": "https://en.wikipedia.org/wiki/Coppersmith%E2%80%93Winograd_algorithm",
      "text": "{{Use dmy dates|date=July 2013}}\nIn [[linear algebra]], the '''Coppersmith–Winograd algorithm''', named after [[Don Coppersmith]] and [[Shmuel Winograd]], was the asymptotically fastest known [[matrix multiplication algorithm]] from 1990 until 2010. It can multiply two <math>n \\times n</math> matrices in <math>\\mathcal{O}(n^{2.375477})</math> time <ref  name=\"coppersmith\">{{Citation|doi=10.1016/S0747-7171(08)80013-2|title=Matrix multiplication via arithmetic progressions|url=http://www.cs.umd.edu/~gasarch/TOPICS/ramsey/matrixmult.pdf|year=1990|last1=Coppersmith|first1=Don|last2=Winograd|first2=Shmuel|journal=Journal of Symbolic Computation|volume=9|issue=3|pages=251}}</ref> (see [[Big O notation]]). \nThis is an improvement over the naïve <math>\\mathcal{O}(n^3)</math> time algorithm and the <math>\\mathcal{O}(n^{2.807355})</math> time [[Strassen algorithm]]. Algorithms with better asymptotic running time than the Strassen algorithm are rarely used in practice, because the large constant factors in their running times make them impractical.<ref>{{citation\n | last = Le Gall | first = F.\n | arxiv = 1204.1111\n | contribution = Faster algorithms for rectangular matrix multiplication\n | doi = 10.1109/FOCS.2012.80\n | pages = 514–523\n | title = Proceedings of the 53rd Annual IEEE Symposium on Foundations of Computer Science (FOCS 2012)\n | year = 2012}}.</ref>\nIt is possible to improve the exponent further; however, the exponent must be at least 2 (because an <math>n \\times n</math> matrix has <math>n^2</math> values, and all of them have to be read at least once to calculate the exact result).\n\nIn 2010, Andrew Stothers gave an improvement to the algorithm, <math>\\mathcal{O}(n^{2.374}).</math><ref>{{Citation | last1=Stothers | first1=Andrew | title=On the Complexity of Matrix Multiplication | url=https://www.era.lib.ed.ac.uk/handle/1842/4734 | year=2010}}.</ref><ref>{{Citation | last1=Davie | first1=A.M. | last2=Stothers | first2=A.J. | title=Improved bound for complexity of matrix multiplication|journal=Proceedings of the Royal Society of Edinburgh|volume=143A|pages=351–370|year=2013|doi=10.1017/S0308210511001648}}</ref> In 2011, [[Virginia Vassilevska Williams]] combined a mathematical short-cut from Stothers' paper with her own insights and automated optimization on computers, improving the bound to <math>\\mathcal{O}(n^{2.3728642}).</math><ref>{{Citation | last1=Williams | first1=Virginia Vassilevska| title=Breaking the Coppersmith-Winograd barrier | url=http://theory.stanford.edu/~virgi/matrixmult-f.pdf | year=2011}}</ref> In 2014, François Le Gall simplified the methods of Williams and obtained an improved bound of <math>\\mathcal{O}(n^{2.3728639}).</math><ref>\"Even if someone manages to prove one of the conjectures—thereby demonstrating that ω = 2—the wreath product approach is unlikely to be applicable to the large matrix problems that arise in practice. (...) the input matrices must be astronomically large for the difference in time to be apparent.\"{{Citation | last1=Le Gall | first1=François | contribution=Powers of tensors and fast matrix multiplication | year = 2014 | arxiv=1401.7714 | title = Proceedings of the 39th International Symposium on Symbolic and Algebraic Computation ([[ISSAC]] 2014)| bibcode=2014arXiv1401.7714L }}</ref>\n\nThe Coppersmith–Winograd algorithm is frequently used as a building block in other algorithms to prove theoretical time bounds. \nHowever, unlike the Strassen algorithm, it is not used in practice (making it a [[galactic algorithm]]) because it only provides an advantage for matrices so large that they cannot be processed by modern hardware.<ref>{{Citation | last1=Robinson | first1=Sara | title=Toward an Optimal Algorithm for Matrix Multiplication | url=https://archive.siam.org/pdf/news/174.pdf | year=2005 | journal=SIAM News | volume=38 | issue=9}}</ref>\n\n[[Henry Cohn]], [[Robert Kleinberg]], [[Balázs Szegedy]] and [[Chris Umans]] have re-derived the Coppersmith–Winograd algorithm using a [[group theory|group-theoretic]] construction. They also showed that either of two different conjectures would imply that the optimal exponent of matrix multiplication is 2, as has long been suspected. However, they were not able to formulate a specific solution leading to a better running-time than Coppersmith–Winograd.<ref>{{Cite book | last1 = Cohn | first1 = H. | last2 = Kleinberg | first2 = R. | last3 = Szegedy | first3 = B. | last4 = Umans | first4 = C. | chapter = Group-theoretic Algorithms for Matrix Multiplication | doi = 10.1109/SFCS.2005.39 | title = 46th Annual IEEE Symposium on Foundations of Computer Science (FOCS'05) | pages = 379 | year = 2005 | isbn = 0-7695-2468-0 | pmid =  | pmc = }}</ref> Several of their conjectures have since been disproven by Blasiak, Cohn, Church, Grochow, Naslund, Sawin, and Umans using the Slice Rank method.<ref>{{Cite book | last1 = Blasiak | first1 = J. | last2 = Cohn | first2 = H. | last3 = Church | first3 = T. | last4 = Grochow | first4 = J. | last5 = Naslund | first5= E. | last6 = Sawin | first6 = W. | last7=Umans | first7= C.| chapter= On cap sets and the group-theoretic approach to matrix multiplication | doi = 10.19086/da.1245 | title = Discrete Analysis | url = http://discreteanalysisjournal.com/article/1245-on-cap-sets-and-the-group-theoretic-approach-to-matrix-multiplication}}</ref>\n\n== See also ==\n* [[Computational complexity of mathematical operations]]\n* [[Gauss–Jordan elimination]]\n* [[Salem–Spencer set]]\n* [[Strassen algorithm]]\n\n== References ==\n{{Reflist}}\n\n== Further reading ==\n*{{cite book |first=P. |last=Bürgisser |first2=M. |last2=Clausen |first3=M. A. |last3=Shokrollahi |title=Algebraic Complexity Theory |series=Grundlehren der mathematischen Wissenschaften |volume=315 |publisher=Springer Verlag |year=1997 }}\n\n{{Numerical linear algebra}}\n\n{{DEFAULTSORT:Coppersmith-Winograd Algorithm}}\n[[Category:Numerical linear algebra]]\n[[Category:Matrix theory]]\n[[Category:Matrix multiplication algorithms]]"
    },
    {
      "title": "DADiSP",
      "url": "https://en.wikipedia.org/wiki/DADiSP",
      "text": "{{Infobox software\n| name                   = DADiSP\n| developer              = DSP Development Corporation\n| released               = {{Start date and age|1987|df=yes}}\n| latest release version = DADiSP 6.7 B02\n| latest release date    = {{Start date and age|2017|1|17}}\n| status                 = Active\n| programming language   = [[C (programming language)|C]], [[C++]], SPL\n| operating_system       = [[Microsoft Windows]]\n| platform               = [[IA-32]], [[x86-64]]\n| genre                  = [[List of numerical analysis software|Technical computing]]\n| license                = [[Proprietary software|Proprietary]] [[commercial software]]\n| website                = [http://www.dadisp.com DADiSP]\n}}\n\n{{Infobox programming language\n| name                   = SPL\n| paradigm               = [[multi-paradigm programming language|multi-paradigm]]: [[Imperative programming|imperative]], [[Procedural programming|procedural]], [[Object-oriented programming|object-oriented]], [[Array programming|array]]\n| family                 =\n| year                   = late 1990s\n| designer               = Randy Race\n| developer              = DSP Development Corporation\n| latest release version = 6.7\n| latest release date    = 2017\n| typing                 = [[dynamic typing|Dynamic]], [[weak typing|weak]]\n| scope                  =\n| implementations        =\n| dialects               =\n| influenced by          = [[APL (programming language)|APL]], [[C (programming language)|C]], [[C++]]\n| influenced             =\n| operating_system       = [[Microsoft Windows]]\n| license                =\n| file_ext               = .spl\n| website                =\n}}\n\n'''DADiSP''' (Data Analysis and Display, pronounced day-disp) is a [[Numerical analysis|numerical computing]] environment developed by DSP Development Corporation which allows one to display and manipulate data series, [[matrix (math)|matrices]] and [[image]]s with an interface similar to a [[spreadsheet]]. DADiSP is used in the study of [[signal processing]],<ref>{{cite web|title=Real-Time Digital Signal Processing Design Projects in an Undergraduate DSP Course and Laboratory |url=http://www.texas-instruments.com/sc/docs/general/dsp/fest99/edu_trackpm/06nahvi.pdf|author=Mahmood Nahvi|publisher=Texas Instruments DSPS Fest, 1999}}</ref> [[numerical analysis]], [[statistical]] and [[physiological]] data processing.<ref>{{cite web|title=User Interactive Software for Analysis of Human Physiological Data|url=http://www.techbriefs.com/component/content/article/1265|publisher=Nasa Tech Briefs, December 2006}}</ref>\n\n== Interface ==\n\nDADiSP is designed to perform technical data analysis in a [[spreadsheet]] like environment. However, unlike a typical business spreadsheet that operates on a table of cells each of which contain single [[Variable (computing)|scalar]] values, a DADiSP Worksheet consists of multiple interrelated windows where each window contains an entire [[Time series|series]] or multi-column [[Matrix (mathematics)|matrix]]. A window not only stores the data, but also displays the data in several interactive forms, including 2D graphs, XYZ plots, 3D surfaces, images and numeric tables. Like a traditional spreadsheet, the windows are linked such that a change to the data in one window automatically updates all dependent windows both numerically and graphically.<ref>{{cite web|title=DADiSP Makes Complex Data Analysis Faster and Easier |url=http://www.dadisp.com/aboutdad.htm |publisher=DSP Development Corp |accessdate=March 3, 2014}}</ref><ref name=\"sca1\">{{cite web|title=DADiSP 2002 Escape from the cell block |url=http://www.scientific-computing.com/scwjulaug03review_dadisp.html|publisher=Scientific Computing World|accessdate=March 3, 2014}}</ref>\nUsers manipulate data primarily through windows. A DADiSP window is normally referred to by the letter \"W\" followed by a window number, as in \"W1\". For example, the formula <code>W1: 1..3</code> assigns the series values {1, 2, 3} to \"W1\". The formula <code>W2: W1*W1</code> sets a second window to compute the square of each value in \"W1\" such that \"W2\" will contain the series {1, 4, 9}. If the values of \"W1\" change to {3, 5, 2, 4}, the values of \"W2\" automatically update to {9, 25, 4, 16}.\n\n== Programming language ==\n\nDADiSP includes a series based [[programming language]] called SPL (Series Processing Language)<ref>{{cite web|title=DADiSP SPL vs. MATLAB |url=http://www.dadisp.com/matlab.htm |publisher=DSP Development Corp |accessdate=March 3, 2014}}</ref> used to implement custom [[algorithm]]s. SPL has a [[C (programming language)|C]]/[[C++]] like syntax and is incrementally compiled into intermediate [[bytecode]], which is executed by a [[virtual machine]]. SPL supports both standard variables assigned with <code>=</code> and \"hot\" variables assigned with <code>:=</code>. For example, the statement <code>A = 1..3</code> assigns the series {1, 2, 3} to the standard variable \"A\". The square of the values can be assigned with <code>B = A * A</code>. Variable \"B\" contains the series {1, 3, 9}. If \"A\" changes, \"B\" does ''not'' change because \"B\" preserves the values as assigned without regard to the future state of \"A\". However, the statement <code>A := 1..3</code> creates a \"hot\" variable. A hot variable is analogous to a window, except hot variables do not display their data. The assignment <code>B := A * A</code> computes the square of the values of \"A\" as before, but now if \"A\" changes, \"B\" automatically updates. Setting <code>A = {3, 5, 2, 4}</code> causes \"B\" to automatically update with {9, 25, 4, 16}.\n\n== History ==\n\nDADiSP was originally developed in the early 1980s as part of a research project at [[Massachusetts Institute of Technology|MIT]] to explore the aerodynamics of [[Formula One]] racing cars.<ref name=\"sca1\" /> The original goal of the project was to enable researchers to quickly explore data analysis algorithms without the need for traditional programming.\n\n== Version history ==\n* DADiSP 6.7 B02,<ref>{{cite web|title=DADiSP 6.7 B02 Release Notes |url=http://www.dadisp.com/sls67quick.htm |publisher=DSP Development Corp |accessdate=January 18, 2017}}</ref> Jan 2017\n* DADiSP 6.7 B01,<ref>{{cite web|title=DADiSP 6.7 B01 Release Notes |url=http://www.dadisp.com/sls67b01.htm |publisher=DSP Development Corp |accessdate=October 30, 2015 |f= }}</ref> Oct 2015\n* DADiSP 6.5 B05,<ref>{{cite web|title=DADiSP 6.5 B05 Release Notes |url=http://www.dadisp.com/sls65quick.htm |publisher=DSP Development Corp |accessdate=March 3, 2014 }}</ref> Dec 2012\n* DADiSP 6.5,<ref>{{cite web|title=DADiSP 6.5 |url=http://www.scientific-computing.com/products/product_details.php?product_id=844|publisher=Scientific Computing World|accessdate=June 1, 2010}}</ref> May 2010 \n* DADiSP 6.0, Sep 2002\n* DADiSP 5.0, Oct 2000\n* DADiSP 4.1, Dec 1997\n* DADiSP 4.0, Jul 1995\n* DADiSP 3.01, Feb 1993\n* DADiSP 2.0,<ref>{{cite journal | doi = 10.1111/j.0033-0124.1992.00103.x | title = DADiSP 2.0 | volume=44 | year=1992 | journal=The Professional Geographer | pages=103–108}}</ref> Feb 1992\n* DADiSP 1.05, May 1989\n* DADiSP 1.03, Apr 1987\n\n== See also ==\n\n* [[List of numerical analysis software]]\n* [[Comparison of numerical analysis software]]\n\n== References ==\n\n{{reflist}}\n\n== Further reading ==\n\n* Allen Brown, Zhang Jun: ''First Course In Digital Signal Processing Using DADiSP'', Abramis, {{ISBN|9781845495022}}\n* Charles Stephen Lessard: ''Signal Processing of Random Physiological Signals (Google eBook)'', Morgan & Claypool Publishers\n\n== External links ==\n* [http://www.dadisp.com DSP Development Corporation (DADiSP vendor)]\n* [http://www.dadisp.com/webhelp/dsphelp.htm DADiSP Online Help]\n* [http://www.dadisp.com/flashdemos1.htm DADiSP Tutorials]\n* [http://www.dadisp.com/files/getstart.pdf Getting Started with DADiSP]\n* [http://civil.eng.buffalo.edu/cie616/3-HANDOUTS/DADISP/101108-Introduction%20to%20Dadisp.pdf Introduction to DADiSP]\n\n{{-}}\n\n{{Numerical analysis software}}\n\n[[Category:Data analysis software]]\n[[Category:Data-centric programming languages]]\n[[Category:Data mining and machine learning software]]\n[[Category:Numerical linear algebra]]\n[[Category:Data visualization software]]\n[[Category:Statistical programming languages]]\n[[Category:C software]]\n[[Category:Software modeling language]]"
    },
    {
      "title": "Data Analytics Acceleration Library",
      "url": "https://en.wikipedia.org/wiki/Data_Analytics_Acceleration_Library",
      "text": "{{Infobox software\n| name                   = Data Analytics Acceleration Library\n| logo                   =\n| screenshot             =\n| caption                =\n| collapsible            =\n| author                 =\n| developer              = [[Intel]]\n| released               = {{Start date and age|2015|08|25}}\n| latest release version = 2018 Update 1\n| latest release date    = November 17, 2017<ref>{{cite web |title= Intel® Data Analytics Acceleration Library 2018 Release Notes|url= https://software.intel.com/en-us/articles/intel-daal-2018-release-notes}}</ref>\n| latest preview version = \n| latest preview date    = \n| programming language   = [[C++]], [[Java (programming language)|Java]], [[Python (programming language)|Python]]<ref name=\"homepage\"/>\n| operating system       = [[Microsoft Windows]], [[Linux]], [[macOS]]<ref name=\"homepage\"/>\n| platform               = [[Intel Atom]], [[Intel Core]], [[Intel Xeon]], [[Intel Xeon Phi]]<ref name=\"homepage\"/>\n| size                   =\n| language               =\n| status                 =\n| genre                  = [[Library (computing)|Library]] or [[Software framework|framework]]\n| license = [[Apache License]] 2.0<ref name=\"freedaal\">{{cite web|title=Open Source Project: Intel Data Analytics Acceleration Library (DAAL)|url=https://software.intel.com/articles/opendaal}}</ref>\n| website                = {{URL|software.intel.com/intel-daal}}\n}}\n\n[[Intel]] '''Data Analytics Acceleration Library''' (Intel '''DAAL''') is a [[Library (computer science)|library]] of optimized algorithmic building blocks for [[data analysis]] stages most commonly associated with solving [[Big Data]] problems.<ref name=\"githubdaal\">{{cite web |title=DAAL github|url=https://github.com/01org/daal}}</ref><ref>{{cite web |title=Intel Updates Developer Toolkit with Data Analytics Acceleration Library |url=http://insidehpc.com/2015/08/intel-updates-developer-toolkit-with-data-analytics-acceleration-library/}}</ref><ref>{{cite web |title=Intel adds big data functions to math libraries |url=https://www.theregister.co.uk/2015/08/26/intel_adds_big_data_functions_to_math_libraries/}}</ref><ref>{{cite web |title=Intel Leverages HPC Core for Analytics Tooling Push |url=http://www.nextplatform.com/2015/08/25/intel-leverages-hpc-core-for-analytics-tooling-push/|work=nextplatform.com|date=2015-08-25}}</ref>\n\nThe library supports Intel processors and is available for [[Windows]], [[Linux]] and [[macOS]] [[operating system]]s.<ref name=\"homepage\">[https://software.intel.com/intel-daal Intel® Data Analytics Acceleration Library (Intel® DAAL) | Intel® Software]</ref> The library is designed for use popular data platforms including [[Hadoop]], [[Apache Spark|Spark]], [[R (programming language)|R]], and [[Matlab]].<ref name=\"githubdaal\"/><ref name=\"dicedaal\">{{cite web |title=Try Out Intel DAAL to Process Big Data|url=http://insights.dice.com/2016/01/13/try-out-intel-daal-to-process-big-data/}}</ref>\n\n==History==\nIntel launched the Data Analytics Acceleration Library on August 25, 2015 and called it Intel Data Analytics Acceleration Library 2016 (Intel DAAL 2016).<ref>{{cite web |title=Intel Data Analytics Acceleration Library |url=https://software.intel.com/daal}}</ref> DAAL is bundled with [[Intel Parallel Studio XE]] as a commercial product. A standalone version is available commercially or freely,<ref name=\"freedaal\"/><ref name=\"commlic\">{{cite web |title=Community Licensing of Intel Performance Libraries|url=https://software.intel.com/nest}}</ref> the only difference being support and maintenance related.\n\n==License==\n[[Apache License]] 2.0\n\n==Details==\n\n===Functional categories===\nIntel DAAL has the following algorithms:<ref>[https://software.intel.com/sites/products/documentation/doclib/daal/daal-user-and-reference-guides/index.htm Developer Guide and Reference for Intel(R) Data Analytics Acceleration Library 2017]</ref><ref name=\"githubdaal\"/><ref name=\"colfaxdaal\">{{cite web |title=Introduction to Intel DAAL, Part 1: Polynomial Regression with Batch Mode Computation|url=http://colfaxresearch.com/intro-to-daal-1/}}</ref>\n*'''Analysis'''\n**'''Low Order Moments:''' Includes computing min, max, mean, standard deviation, variance, etc. for a dataset.\n**'''Quantiles:''' splitting observations into equal-sized groups defined by quantile orders.\n**'''Correlation matrix and variance-covariance matrix:''' A basic tool in understanding statistical dependence among variables. The degree of correlation indicates the tendency of one change to indicate the likely change in another.\n**'''Cosine distance matrix:''' Measuring pairwise distance using cosine distance.\n**'''Correlation distance matrix:''' Measuring pairwise distance between items using correlation distance.\n**'''Clustering:''' Grouping data into unlabeled groups. This is a typical technique used in “unsupervised learning” where there is not established model to rely on. Intel DAAL provides 2 algorithms for clustering: K-Means and “EM for GMM.”\n**'''Principal Component Analysis (PCA):''' the most popular algorithm for dimensionality reduction.\n**'''Association rules mining:''' Detecting co-occurrence patterns. Commonly known as “shopping basket mining.”\n**'''Data transformation through matrix decomposition:''' DAAL provides Cholesky, QR, and SVD decomposition algorithms.\n**'''Outlier detection:''' Identifying observations that are abnormally distant from typical distribution of other observations.\n*'''Training and Prediction'''\n**'''Regression'''\n***'''Linear regression:''' The simplest regression method. Fitting a linear equation to model the relationship between dependent variables (things to be predicted) and explanatory variables (things known).\n**'''Classification:''' Building a model to assign items into different labeled groups. DAAL provides multiple algorithms in this area, including Naïve Bayes classifier, Support Vector Machine, and multi-class classifiers.\n**'''Recommendation systems'''\n**'''Neural networks'''\n\nIntel DAAL supported three processing modes:\n*'''Batch processing:''' When all data fits in the memory, a function is called to process the data all at once.\n*'''Online processing (also called Streaming):''' when all data does not fit in memory. Intel® DAAL can process data chunks individually and combine all partial results at the finalizing stage.\n*'''Distributed processing:''' DAAL supports a model similar to MapReduce. Consumers in a cluster process local data (map stage), and then the Producer process collects and combines partial results from Consumers (reduce stage). Intel DAAL offers flexibility in this mode by leaving the communication functions completely to the developer. Developers can choose to use the data movement in a framework such as Hadoop or Spark, or explicitly coding communications most likely with MPI.\n\n==References==\n{{reflist}}\n\n==External links==\n*{{github|01org/daal}}\n*[https://software.intel.com/intel-daal DAAL Official Product Website]\n*[https://software.intel.com/en-us/intel-daal-support DAAL Support]\n*[https://software.intel.com/en-us/intel-daal-support/training DAAL User Forum]\n*[http://premier.intel.com/ DAAL Support Channel]\n\n{{Intel software}}\n{{Numerical linear algebra}}\n\n[[Category:Intel software]]\n[[Category:Numerical software]]\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "Diagonally dominant matrix",
      "url": "https://en.wikipedia.org/wiki/Diagonally_dominant_matrix",
      "text": "In mathematics, a square [[matrix (mathematics)|matrix]] is said to be '''diagonally dominant''' if, for every row of the matrix, the magnitude of the diagonal entry in a row is larger than or equal to the sum of the magnitudes of all the other (non-diagonal) entries in that row. More precisely, the matrix ''A'' is diagonally dominant if\n:<math>|a_{ii}| \\geq \\sum_{j\\neq i} |a_{ij}| \\quad\\text{for all } i, \\,</math>\nwhere ''a''<sub>''ij''</sub> denotes the entry in the ''i''th row and ''j''th column.\n\nNote that this definition uses a weak inequality, and is therefore sometimes called ''weak diagonal dominance''.  If a strict inequality (>) is used, this is called ''strict diagonal dominance''.  The unqualified term ''diagonal dominance'' can mean both strict and weak diagonal dominance, depending on the context.<ref>For instance, Horn and Johnson (1985, p.&nbsp;349) use it to mean weak diagonal dominance.</ref>\n\n==Variations==\nThe definition in the first paragraph sums entries across rows.  It is therefore sometimes called ''row diagonal dominance''.  If one changes the definition to sum down columns, this is called ''column diagonal dominance''.\n\nAny strictly diagonally dominant matrix is trivially a [[weakly chained diagonally dominant matrix]]. Weakly chained diagonally dominant matrices are nonsingular and include the family of ''irreducibly diagonally dominant'' matrices. These are [[irreducible (mathematics)|irreducible]] matrices that are weakly diagonally dominant, but strictly diagonally dominant in at least one row.\n\n==Examples==\nThe matrix\n\n:<math> A = \\begin{bmatrix}\n3 & -2 & 1\\\\\n1 & -3 & 2\\\\\n-1 & 2 & 4\\end{bmatrix}\n</math>\n\nis diagonally dominant because\n\n:<math>|a_{11}| \\ge |a_{12}| + |a_{13}|</math> &nbsp; since &nbsp; <math>|+3| \\ge |-2| + |+1|</math>\n\n:<math>|a_{22}| \\ge |a_{21}| + |a_{23}|</math> &nbsp; since &nbsp; <math>|-3| \\ge |+1| + |+2|</math>\n\n:<math>|a_{33}| \\ge |a_{31}| + |a_{32}|</math> &nbsp; since &nbsp; <math>|+4| \\ge |-1| + |+2|</math>.\n\nThe matrix\n\n:<math> B = \\begin{bmatrix}\n-2 & 2 & 1\\\\\n1 & 3 & 2\\\\\n1 & -2 & 0\\end{bmatrix}\n</math>\n\nis ''not'' diagonally dominant because\n\n:<math>|b_{11}| < |b_{12}| + |b_{13}|</math> &nbsp; since &nbsp; <math>|-2| < |+2| + |+1|</math>\n\n:<math>|b_{22}| \\ge |b_{21}| + |b_{23}|</math> &nbsp; since &nbsp; <math>|+3| \\ge |+1| + |+2|</math>\n\n:<math>|b_{33}| < |b_{31}| + |b_{32}|</math> &nbsp; since &nbsp; <math>|+0| < |+1| + |-2|</math>.\n\nThat is, the first and third rows fail to satisfy the diagonal dominance condition.\n\nThe matrix\n\n:<math> C = \\begin{bmatrix}\n-4 & 2 & 1\\\\\n1 & 6 & 2\\\\\n1 & -2 & 5\\end{bmatrix}\n</math>\n\nis ''strictly'' diagonally dominant because\n\n:<math>|c_{11}| > |c_{12}| + |c_{13}|</math> &nbsp; since &nbsp; <math>|-4| > |+2| + |+1|</math>\n\n:<math>|c_{22}| > |c_{21}| + |c_{23}|</math> &nbsp; since &nbsp; <math>|+6| > |+1| + |+2|</math>\n\n:<math>|c_{33}| > |c_{31}| + |c_{32}|</math> &nbsp; since &nbsp; <math>|+5| > |+1| + |-2|</math>.\n\n==Applications and properties==\nA strictly diagonally dominant matrix (or an irreducibly diagonally dominant matrix<ref>Horn and Johnson, Thm 6.2.27.</ref>) is [[singular matrix|non-singular]]. This result is known as the Levy–Desplanques theorem.<ref>Horn and Johnson, Thm 6.1.10.  This result has been independently rediscovered dozens of times.  A few notable ones are Lévy (1881), Desplanques (1886), Minkowski (1900), Hadamard (1903), Schur, Markov (1908), Rohrbach (1931), Gershgorin (1931), Artin (1932), Ostrowski (1937), and Furtwängler (1936).  For a history of this \"recurring theorem\" see: {{cite journal | last=Taussky | first=Olga | authorlink=Olga Taussky-Todd | year=1949 | title=A recurring theorem on determinants | journal=[[American Mathematical Monthly]] | volume=56 | pages=672–676 | doi=10.2307/2305561 | issue=10 | publisher=The American Mathematical Monthly, Vol. 56, No. 10 | jstor=2305561}}  Another useful history is in: {{cite journal | last=Schneider | first=Hans | year=1977 | title=Olga Taussky-Todd's influence on matrix theory and matrix theorists | journal=Linear and Multilinear Algebra | volume=5 | issue=3 | pages=197–224 | doi=10.1080/03081087708817197}}</ref> This can be proved, for strictly diagonal dominant matrices, using the [[Gershgorin circle theorem]].\n\nA [[hermitian matrix|Hermitian]] diagonally dominant matrix <math> A </math> with real non-negative diagonal entries is [[positive semidefinite matrix|positive semidefinite]].\n\n''Proof'': Let the diagonal matrix <math> D </math> contain the diagonal entries of <math> A </math>. Connect <math> A </math> and <math>D+I</math> via a segment of matrices <math> M(t)=(1-t)(D+I)+tA </math>. This segment consists of strictly diagonally dominant (thus nonsingular) matrices, except maybe for <math>A</math>. This shows that <math> \\mathrm{det}(A) \\ge 0</math>. Applying this argument to the [[minor (linear algebra)|principal minors]] of <math> A </math>, the positive semidefiniteness follows by [[Sylvester's criterion]].\n\nIf the symmetry requirement is eliminated, such a matrix is not necessarily positive semidefinite. For example, consider\n:<math> \\begin{pmatrix}-2&2&1\\end{pmatrix}\\begin{pmatrix}\n1&1&0\\\\\n1&1&0\\\\\n1&0&1\\end{pmatrix}\\begin{pmatrix}-2\\\\2\\\\1\\end{pmatrix}<0.</math>\nHowever, the real parts of its eigenvalues remain non-negative by the [[Gershgorin circle theorem]].\n\nSimilarly, an Hermitian strictly diagonally dominant matrix with real positive diagonal entries is [[positive definite matrix|positive definite]], as it equals to the sum of some Hermitian diagonally dominant matrix <math>A</math> with real non-negative diagonal entries (which is positive semidefinite) and <math>xI</math> for some positive real number <math>x</math> (which is positive definite).\n\nNo (partial) [[Pivot element|pivoting]] is necessary for a strictly column diagonally dominant matrix when performing [[Gaussian elimination]] (LU factorization).\n\nThe [[Jacobi method|Jacobi]] and [[Gauss–Seidel method]]s for solving a linear system converge if the matrix is strictly (or irreducibly) diagonally dominant.\n\nMany matrices that arise in [[finite element method]]s are diagonally dominant.\n\nA slight variation on the idea of diagonal dominance is used to prove that the pairing on diagrams without loops in the [[Temperley–Lieb algebra]] is nondegenerate.<ref>{{cite journal | author = K.H. Ko and L. Smolinski | title = A combinatorial matrix in 3-manifold theory | journal = [[Pacific J. Math.]] | volume = 149 | year = 1991 | pages = 319–336}}</ref> For a matrix with polynomial entries, one sensible definition of diagonal dominance is if the highest power of <math>q</math> appearing in each row appears only on the diagonal. (The evaluations of such a matrix at large values of <math>q</math> are diagonally dominant in the above sense.)\n\n==Notes==\n<references/>\n\n==References==\n*{{cite book |first=Gene H. |last=Golub |authorlink=Gene Golub |first2=Charles F. |last2=Van Loan |title=Matrix Computations |year=1996 |isbn=0-8018-5414-8 }}\n*{{cite book |first=Roger A. |last=Horn |first2=Charles R. |last2=Johnson |title=Matrix Analysis |location= |publisher=Cambridge University Press |year=1985 |isbn=0-521-38632-2 |edition=Paperback }}\n\n==External links==\n* [http://planetmath.org/?op=getobj&from=objects&id=4512 PlanetMath: Diagonal dominance definition]\n* [http://planetmath.org/?op=getobj&from=objects&id=7483 PlanetMath: Properties of diagonally dominant matrices]\n* [http://mathworld.wolfram.com/DiagonallyDominantMatrix.html Mathworld]\n\n{{Numerical linear algebra}}\n{{Matrix classes}}\n\n[[Category:Numerical linear algebra]]\n[[Category:Matrices]]"
    },
    {
      "title": "DIIS",
      "url": "https://en.wikipedia.org/wiki/DIIS",
      "text": "'''DIIS''' ('''direct inversion in the iterative subspace''' or '''direct inversion of the iterative subspace'''), also known as '''Pulay mixing''', is an [[extrapolation]] technique. DIIS was developed by [[Peter Pulay]] in the field of computational [[quantum chemistry]] with the intent to accelerate and stabilize the [[convergence (mathematics)|convergence]] of the [[Hartree–Fock]] self-consistent field method.<ref>{{cite journal|last=Pulay|first=Péter |year=1980|title=Convergence acceleration of iterative sequences. the case of SCF iteration|journal=Chemical Physics Letters|volume=73|issue=2|pages=393–398|doi=10.1016/0009-2614(80)80396-4|bibcode=1980CPL....73..393P}}</ref><ref>{{cite journal|last=Pulay|first=Péter |year=1982|title=Improved SCF Convergence Acceleration|journal=Journal of Computational Chemistry|volume=3|issue=4|pages=556–560|doi=10.1002/jcc.540030413}}</ref><ref>{{Cite journal|doi=10.1080/00268970701691611|title=Some comments on the DIIS method|journal=Molecular Physics|volume=105|issue=19–22|pages=2839–2848|year=2010|last1=Shepard|first1=Ron|last2=Minkoff|first2=Michael|bibcode=2007MolPh.105.2839S}}</ref>\n\nAt a given iteration, the approach constructs a [[linear combination]] of approximate error vectors from previous iterations. The coefficients of the linear combination are determined so to best approximate, in a [[least squares]] sense, the [[null vector]]. The newly determined coefficients are then used to extrapolate the function variable for the next iteration. \n\n== Details ==\n\nAt each iteration, an approximate error vector, {{math|'''e'''<sub>''i''</sub>}}, corresponding to the variable value, {{math|'''p'''<sub>''i''</sub>}} is determined. After sufficient iterations, a linear combination of {{math|''m''}} previous error vectors is constructed\n\n:<math>\\mathbf e_{m+1}=\\sum_{i = 1}^m\\ c_i\\mathbf e_i.</math>\n\nThe DIIS method seeks to minimize the norm of {{math|'''e'''<sub>''m''+1</sub>}} under the constraint that the coefficients sum to one. The reason why the coefficients must sum to one can be seen if we write the trial vector as the sum of the exact solution ({{math|'''p'''<sup>f</sup>}}) and an error vector. In the DIIS approximation, we get:\n:<math>\n\\begin{align}\n\\mathbf p &= \\sum_i c_i \\left( \\mathbf p^\\text{f} + \\mathbf e_i \\right) \\\\\n  &= \\mathbf p^\\text{f} \\sum_i c_i + \\sum_i c_i \\mathbf e_i\n\\end{align}\n</math>\nWe minimize the second term while it is clear that the sum coefficients must be equal to one if we want to find the exact solution.\nThe minimization is done by a [[Lagrange multiplier]] technique. Introducing an undetermined multiplier {{math|''λ''}}, a Lagrangian is constructed as\n\n:<math>\n\\begin{align}\nL&=\\left\\|\\mathbf e_{m+1}\\right\\|^2-2\\lambda\\left(\\sum_i\\ c_i-1\\right),\\\\\n&=\\sum_{ij}c_jB_{ji}c_i-2\\lambda\\left(\\sum_i\\ c_i-1\\right),\\text{ where } B_{ij}=\\langle\\mathbf e_j, \\mathbf e_i\\rangle.\n\\end{align}\n</math>\n\nEquating zero to the derivatives of {{math|''L''}} with respect to the coefficients and the multiplier leads to a system of {{math|(''m'' + 1)}} [[linear equation]]s to be solved for the {{math|''m''}} coefficients (and the Lagrange multiplier). \n\n:<math>\\begin{bmatrix} \nB_{11} & B_{12} & B_{13} & ... & B_{1m} & -1 \\\\\nB_{21} & B_{22} & B_{23} & ... & B_{2m} & -1 \\\\ \nB_{31} & B_{32} & B_{33} & ... & B_{3m} & -1 \\\\ \n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\nB_{m1} & B_{m2} & B_{m3} & ... & B_{mm} & -1 \\\\\n1      & 1      & 1      & ... & 1      & 0\n\\end{bmatrix} \\begin{bmatrix} c_1 \\\\ c_2 \\\\ c_3 \\\\ \\vdots \\\\ c_m \\\\ \\lambda \\end{bmatrix}=\n\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\\\ 1 \\end{bmatrix}\n</math> \n\nMoving the minus sign to {{math|''λ''}}, results in an equivalent symmetric problem.\n:<math>\\begin{bmatrix} \nB_{11} & B_{12} & B_{13} & ... & B_{1m} & 1 \\\\\nB_{21} & B_{22} & B_{23} & ... & B_{2m} & 1 \\\\ \nB_{31} & B_{32} & B_{33} & ... & B_{3m} & 1 \\\\ \n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\nB_{m1} & B_{m2} & B_{m3} & ... & B_{mm} & 1 \\\\\n1      & 1      & 1      & ... & 1      & 0\n\\end{bmatrix} \\begin{bmatrix} c_1 \\\\ c_2 \\\\ c_3 \\\\ \\vdots \\\\ c_m \\\\ -\\lambda \\end{bmatrix}=\n\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\\\ 1 \\end{bmatrix}\n</math> \nThe coefficients are then used to update the variable as\n\n:<math>\\mathbf p_{m+1}=\\sum_{i = 1}^m c_i\\mathbf p_i.</math>\n\n== Relevant work ==\n\n* LCIIS\n* EDIIS\n* CDIIS\n\n== Citations ==\n{{reflist}}\n\n== References ==\n* {{cite journal|last1=Garza|first1=Alejandro J.|last2=Scuseria|first2=Gustavo E.|year=2012|title=Comparison of self-consistent field convergence acceleration techniques|journal=Journal of Chemical Physics|volume=173|issue=5|page=054110|doi=10.1063/1.4740249|pmid=22894335|bibcode=2012JChPh.137e4110G}}\n* {{Cite journal|doi=10.1007/s10910-011-9863-y|title=An analysis for the DIIS acceleration method used in quantum chemistry calculations|year=2011|last1=Rohwedder|first1=Thorsten|last2=Schneider|first2=Reinhold|journal=Journal of Mathematical Chemistry|volume=49|issue=9|pages=1889|citeseerx=10.1.1.461.1285}}\n\n== External links ==\n* [http://vergil.chemistry.gatech.edu/notes/diis/node2.html The Mathematics of DIIS]\n\n[[Category:Quantum chemistry]]\n[[Category:Computational chemistry]]\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "Divide-and-conquer eigenvalue algorithm",
      "url": "https://en.wikipedia.org/wiki/Divide-and-conquer_eigenvalue_algorithm",
      "text": "'''Divide-and-conquer eigenvalue algorithms''' are a class of [[eigenvalue algorithm]]s for [[Hermitian matrix|Hermitian]] or [[real number|real]] [[Symmetric matrix|symmetric matrices]] that have recently (circa 1990s) become competitive in terms of [[Numerical stability|stability]] and [[Computational complexity theory|efficiency]] with more traditional algorithms such as the [[QR algorithm]].  The basic concept behind these algorithms is the [[Divide and conquer algorithm|divide-and-conquer]] approach from [[computer science]].  An [[eigenvalue]] problem is divided into two problems of roughly half the size, each of these are solved [[Recursion|recursively]], and the eigenvalues of the original problem are computed from the results of these smaller problems.\n\nHere we present the simplest version of a divide-and-conquer algorithm, similar to the one originally proposed by Cuppen in 1981.  Many details that lie outside the scope of this article will be omitted; however, without considering these details, the algorithm is not fully stable.\n\n==Background==\nAs with most eigenvalue algorithms for Hermitian matrices, divide-and-conquer begins with a reduction to [[Tridiagonal matrix|tridiagonal]] form.  For an <math>m \\times m</math> matrix, the standard method for this, via [[Householder reflection]]s, takes <math>\\frac{4}{3}m^{3}</math> [[flops]], or <math>\\frac{8}{3}m^{3}</math> if [[eigenvector]]s are needed as well.  There are other algorithms, such as the [[Arnoldi iteration]], which may do better for certain classes of matrices; we will not consider this further here.\n\nIn certain cases, it is possible to ''deflate'' an eigenvalue problem into smaller problems.  Consider a [[block diagonal matrix]]\n:<math>T = \\begin{bmatrix} T_{1} & 0 \\\\ 0 & T_{2}\\end{bmatrix}.</math>\nThe eigenvalues and eigenvectors of <math>T</math> are simply those of <math>T_{1}</math> and <math>T_{2}</math>, and it will almost always be faster to solve these two smaller problems than to solve the original problem all at once.  This technique can be used to improve the efficiency of many eigenvalue algorithms, but it has special significance to divide-and-conquer.\n\nFor the rest of this article, we will assume the input to the divide-and-conquer algorithm is an <math>m \\times m</math> real symmetric tridiagonal matrix <math>T</math>.  Although the algorithm can be modified for Hermitian matrices, we do not give the details here.\n\n==Divide==\n\nThe ''divide'' part of the divide-and-conquer algorithm comes from the realization that a tridiagonal matrix is \"almost\" block diagonal.\n<!-- For original TeX, see image description page -->\n:[[Image:Almost block diagonal.png]]\n\nThe size of submatrix <math>T_{1}</math> we will call <math>n \\times n</math>, and then <math>T_{2}</math> is <math>(m - n) \\times (m - n)</math>.  Note that the remark about <math>T</math> being almost block diagonal is true regardless of how <math>n</math> is chosen (i.e., there are many ways to so decompose the matrix).  However, it makes sense, from an efficiency standpoint, to choose <math>n \\approx m/2</math>. \n\nWe write <math>T</math> as a block diagonal matrix, plus a [[Rank (linear algebra)|rank-1]] correction:\n<!-- For original TeX, see image description page -->\n:[[Image:Block diagonal plus correction.png]]\n\nThe only difference between <math>T_{1}</math> and <math>\\hat{T}_{1}</math> is that the lower right entry <math>t_{nn}</math> in <math>\\hat{T}_{1}</math> has been replaced with <math>t_{nn} - \\beta</math> and similarly, in <math>\\hat{T}_{2}</math> the top left entry <math>t_{n+1,n+1}</math> has been replaced with <math>t_{n+1,n+1} - \\beta</math>.\n\nThe remainder of the divide step is to solve for the eigenvalues (and if desired the eigenvectors) of <math>\\hat{T}_{1}</math> and <math>\\hat{T}_{2}</math>, that is to find the [[diagonalizable matrix|diagonalization]]s <math>\\hat{T}_{1} = Q_{1} D_{1} Q_{1}^{T}</math> and <math>\\hat{T}_{2} = Q_{2} D_{2} Q_{2}^{T}</math>.  This can be accomplished with recursive calls to the divide-and-conquer algorithm, although practical implementations often switch to the QR algorithm for small enough submatrices.\n\n==Conquer==\n\nThe ''conquer'' part of the algorithm is the unintuitive part.  Given the diagonalizations of the submatrices, calculated above, how do we find the diagonalization of the original matrix?\n\nFirst, define <math>z^{T} = (q_{1}^{T},q_{2}^{T})</math>, where <math>q_{1}^{T}</math> is the last row of <math>Q_{1}</math> and <math>q_{2}^{T}</math> is the first row of <math>Q_{2}</math>.  It is now elementary to show that\n:<math>T = \\begin{bmatrix} Q_{1} & \\\\ & Q_{2} \\end{bmatrix} \\left( \\begin{bmatrix} D_{1} & \\\\ & D_{2} \\end{bmatrix} + \\beta z z^{T} \\right) \\begin{bmatrix} Q_{1}^{T} & \\\\ & Q_{2}^{T} \\end{bmatrix}</math>\n\nThe remaining task has been reduced to finding the eigenvalues of a diagonal matrix plus a rank-one correction.  Before showing how to do this, let us simplify the notation.  We are looking for the eigenvalues of the matrix <math>D + w w^{T}</math>, where <math>D</math> is diagonal with distinct entries and <math>w</math> is any vector with nonzero entries.\n\nIf w<sub>i</sub> is zero, (<math>e_i</math>,d<sub>i</sub>) is an eigenpair of <math>D + w w^{T}</math> since\n<math>(D + w w^{T})e_i = De_i = d_i e_i</math>.\n\nIf <math>\\lambda</math> is an eigenvalue, we have:\n:<math>(D + w w^{T})q = \\lambda q</math>\nwhere <math>q</math> is the corresponding eigenvector.  Now\n:<math>(D - \\lambda I)q + w(w^{T}q) = 0</math>\n:<math>q + (D - \\lambda I)^{-1} w(w^{T}q) = 0</math>\n:<math>w^{T}q + w^{T}(D - \\lambda I)^{-1} w(w^{T}q) = 0</math>\nKeep in mind that <math>w^{T}q</math> is a nonzero scalar. Neither <math>w</math> nor <math>q</math> are zero. If <math>w^{T}q</math> were to be zero, <math>q</math> would be an eigenvector of <math>D</math> by <math>(D + w w^{T})q = \\lambda q</math>. If that were the case, <math>q</math> would contain only one nonzero position since <math>D</math> is distinct diagonal and thus the inner product <math>w^{T}q</math> can not be zero after all. Therefore, we have:\n:<math>1 + w^{T}(D - \\lambda I)^{-1} w = 0</math>\nor written as a scalar equation,\n:<math>1 + \\sum_{j=1}^{m} \\frac{w_{j}^{2}}{d_{j} - \\lambda} = 0.</math>\nThis equation is known as the ''secular equation''. The problem has therefore been reduced to finding the roots of the [[rational function]] defined by the left-hand side of this equation.\n\nAll general eigenvalue algorithms must be iterative, and the divide-and-conquer algorithm is no different.  Solving the [[nonlinear]] secular equation requires an iterative technique, such as the [[Newton's method|Newton–Raphson method]].  However, each root can be found in [[Big O notation|O]](1) iterations, each of which requires <math>\\Theta(m)</math> flops (for an <math>m</math>-degree rational function), making the cost of the iterative part of this algorithm <math>\\Theta(m^{2})</math>.\n\n==Analysis==\n\nAs is common for divide and conquer algorithms, we will use the [[Master theorem (analysis of algorithms)|master theorem for divide-and-conquer recurrences]] to analyze the running time.  Remember that above we stated we choose <math>n \\approx m/2</math>.  We can write the [[recurrence relation]]:\n:<math>T(m) = 2 \\times T\\left(\\frac{m}{2}\\right) + \\Theta(m^{2})</math>\nIn the notation of the Master theorem, <math>a = b = 2</math> and thus <math>\\log_{b} a = 1</math>.  Clearly, <math>\\Theta(m^{2}) = \\Omega(m^{1})</math>, so we have\n:<math>T(m) = \\Theta(m^{2})</math>\n\nRemember that above we pointed out that reducing a Hermitian matrix to tridiagonal form takes <math>\\frac{4}{3}m^{3}</math> flops.  This dwarfs the running time of the divide-and-conquer part, and at this point it is not clear what advantage the divide-and-conquer algorithm offers over the QR algorithm (which also takes <math>\\Theta(m^{2})</math> flops for tridiagonal matrices).\n\nThe advantage of divide-and-conquer comes when eigenvectors are needed as well.  If this is the case, reduction to tridiagonal form takes <math>\\frac{8}{3}m^{3}</math>, but the second part of the algorithm takes <math>\\Theta(m^{3})</math> as well.  For the QR algorithm with a reasonable target precision, this is <math>\\approx 6 m^{3}</math>, whereas for divide-and-conquer it is <math>\\approx \\frac{4}{3}m^{3}</math>.  The reason for this improvement is that in divide-and-conquer, the <math>\\Theta(m^{3})</math> part of the algorithm (multiplying <math>Q</math> matrices) is separate from the iteration, whereas in QR, this must occur in every iterative step.  Adding the <math>\\frac{8}{3}m^{3}</math> flops for the reduction, the total improvement is from <math>\\approx 9 m^{3}</math> to <math>\\approx 4 m^{3}</math> flops.\n\nPractical use of the divide-and-conquer algorithm has shown that in most realistic eigenvalue problems, the algorithm actually does better than this.  The reason is that very often the matrices <math>Q</math> and the vectors <math>z</math> tend to be ''numerically sparse'', meaning that they have many entries with values smaller than the [[floating point]] precision, allowing for ''numerical deflation'', i.e. breaking the problem into uncoupled subproblems.\n\n==Variants and implementation==\n\nThe algorithm presented here is the simplest version.  In many practical implementations, more complicated rank-1 corrections are used to guarantee stability; some variants even use rank-2 corrections.{{Citation needed|date=September 2011}}\n\nThere exist specialized root-finding techniques for rational functions that may do better than the Newton-Raphson method in terms of both performance and stability.  These can be used to improve the iterative part of the divide-and-conquer algorithm.\n\nThe divide-and-conquer algorithm is readily [[Parallel algorithm|parallelized]], and [[linear algebra]] computing packages such as [[LAPACK]] contain high-quality parallel implementations.\n\n==References==\n*{{citation\n | last = Demmel | first = James W. | authorlink = James Demmel\n | mr = 1463942\n | isbn = 0-89871-389-7\n | location = Philadelphia, PA\n | publisher = [[Society for Industrial and Applied Mathematics]]\n | title = Applied Numerical Linear Algebra\n | year = 1997}}.\n* {{cite journal |first1=J.J.M. |last1=Cuppen |title=A Divide and Conquer Method for the Symmetric Tridiagonal Eigenproblem |journal=[[Numerische Mathematik]] |volume=36 |pages=177–195 |date=1981 }}\n\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "Eigenmode expansion",
      "url": "https://en.wikipedia.org/wiki/Eigenmode_expansion",
      "text": "{{Short description|computational electrodynamics technique}}\n'''Eigenmode expansion''' ('''EME''') is a computational electrodynamics modelling technique. It is also referred to as the '''mode matching technique'''<ref name=\"mmt\" /> or the '''bidirectional eigenmode propagation method''' ('''BEP method''').<ref name=\"bep\" /> Eigenmode expansion is a linear frequency-domain method.\n\nIt offers very strong benefits compared with [[FDTD]], [[Finite element method|FEM]] and the [[beam propagation method]] for the modelling of [[Waveguide (optics)|optical waveguides]],<ref name=\"phot_cad\" /> and it is a popular tool for the modelling linear effects in fiber optics and silicon photonics devices.\n\n==Principles of the EME method==\n\nEigenmode expansion is a rigorous technique to simulate electromagnetic propagation which relies on the decomposition of the electromagnetic fields into a basis set of local [[eigenmodes]] that exists in the cross section of the device. The eigenmodes are found by solving [[Maxwell's equations]] in each local cross-section. The method can be fully vectorial provided that the mode solvers themselves are fully vectorial.\n\nIn a typical waveguide, there are a few guided modes (which propagate without coupling along the waveguide) and an infinite number of radiation modes (which carry optical power away from the waveguide). The guided and radiation modes together form a complete basis set. Many problems can be resolved by considering only a modest number of modes, making EME a very powerful method.\n\nAs can be seen from the mathematical formulation, the algorithm is inherently bi-directional. It uses the scattering matrix (S-matrix) technique to join different sections of the waveguide or to model nonuniform structures. For structures that vary continuously along the z-direction, a form of z-discretisation is required. Advanced algorithms have been developed for the modelling of optical tapers.\n\n==Mathematical formulation==\n\nIn a structure where the optical refractive index does not vary in the z direction, the solutions of Maxwell's equations take the form of a plane wave:\n\n: <math>\\textstyle E(x,y,z)=E(x,y)e^{(i \\beta z)}</math>\n\nWe assume here a single wavelength and time dependence of the form <math>\\scriptstyle \\exp(i \\omega t) </math>.\n\nMathematically <math>\\textstyle E(x,y)e^{(i \\beta z)}</math> and <math>\\scriptstyle\\beta</math>  are the eigenfunction and eigenvalues of maxwell's equations for conditions with simple harmonic z-dependence.\n\nWe can express any solution of Maxwell's equations in terms of a superposition of the forward and backward propagating modes:\n\n: <math>E(x,y,z)= \\sum_{k=1}^M {(a_k e^{(i \\beta_k z)}+ b_k e^{(-i \\beta_k z)})E_k(x,y)}</math>\n\n: <math>H(x,y,z)= \\sum_{k=1}^M {(a_k e^{(i \\beta_k z)}- b_k e^{(-i \\beta_k z)})H_k(x,y)}</math>\n\nThese equations provide a rigorous solution of Maxwell's equations in a linear medium, the only limitation being the finite number of modes.\n\nWhen there is a change in the structure along the z-direction, the coupling between the different input and output modes can be obtained in the form of a scattering matrix. The scattering matrix of a discrete step can be obtained rigorously by applying the boundary conditions of Maxwell's equations at the interface; this requires to calculate the modes on both sides of the interface and their overlaps. For continuously varying structures (e.g. tapers), the scattering matrix can be obtained by discretising the structure along the z-axis.\n\n==Strengths of the EME method==\n\n* The EME method is ideal for the modelling of guided optical components, for fibre and integrated geometries. The mode calculation can take advantage of symmetries of the structure; for instance cylindrically symmetric structures can be modelled very efficiently.\n* The method is fully vectorial (provided that it relies on a fully vectorial mode solver) and fully bidirectional.\n* As it relies on a scattering matrix approach, all reflections are taken into account.\n* Unlike the beam propagation method, which is only valid under the [[slowly varying envelope approximation]], eigenmode expansion provides a rigorous solution to Maxwell's equations.\n* It is generally much more efficient than [[FDTD]] or [[Finite element method|FEM]] as it does not require fine discretisation (i.e. on the scale of the wavelength) along the direction of propagation.\n* The scattering matrix approach provides a flexible calculation framework, potentially allowing users to only re-calculate modified parts of the structure when performing parameter scan studies.\n* It is an excellent technique to model long devices or devices composed of metals.\n* Fully analytical solutions can be obtained for the modelling of 1D+Z structures.\n\n==Limitations of the EME method==\n\n* EME is limited to linear problems; nonlinear problems may be modelled using iterative techniques.\n* EME may be inefficient to model structures requiring a very large number of modes, which limits the size of the cross-section for 3D problems.\n\n==See also==\n* [[Computational electromagnetics]]\n\n==References==\n\n{{reflist|refs=\n<ref name=\"mmt\">\n{{cite journal\n| author= G.V. Eleftheriades\n| url=http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=320771\n| title=Some important properties of waveguide junction generalized scattering matrices in the context of the mode matching technique\n| year=1994\n}}</ref>\n\n<ref name=\"bep\">\n{{cite journal\n| author= J. Petracek\n| url=http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5971039\n| title=Bidirectional eigenmode propagation algorithm for 3D waveguide structures\n| year=2011\n}}</ref>\n\n<ref name=\"phot_cad\">\n{{cite journal\n| author= D. Gallagher\n| url=http://www.photond.com/files/docs/leos_newsletter_feb08_article.pdf\n| title=Photonics CAD Matures\n| journal=LEOS Newsletter\n| year=2008\n}}</ref>\n}}\n\n==External links==\n*[http://www.jpier.org/PIERB/pierb35/13.11083107.pdf Improved Formulation of Scattering Matrices for Semi-Analytical Methods That is Consistent with Convention]\n*[https://web.archive.org/web/20140302111344/http://emlab.utep.edu/ee5390cem.htm See Lectures 19-22 on rigorous coupled-wave analysis and method of lines]\n\n[[Category:Electrodynamics]]\n[[Category:Computational electromagnetics]]\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "Eigenvalue algorithm",
      "url": "https://en.wikipedia.org/wiki/Eigenvalue_algorithm",
      "text": "In [[numerical analysis]], one of the most important problems is designing efficient and [[Numerical stability|stable]] [[algorithm]]s for finding the [[eigenvalue]]s of a [[Matrix (mathematics)|matrix]].  These '''eigenvalue algorithms''' may also find eigenvectors.\n\n==Eigenvalues and eigenvectors==\n{{main|Eigenvalues and eigenvectors|Generalized eigenvector}}\nGiven an {{math|''n'' &times; ''n''}} [[Square matrix#Square matrices|square matrix]] {{math|''A''}} of [[Real number|real]] or [[Complex number|complex]] numbers, an ''[[eigenvalue]]'' {{math|λ}} and its associated ''[[generalized eigenvector]]'' {{math|'''v'''}} are a pair obeying the relation<ref name=\"Axler\">{{Citation\n | last = Axler\n | first = Sheldon\n | author-link = Sheldon Axler\n | title = Down with Determinants!\n | journal = American Mathematical Monthly \n | volume = 102\n | issue = 2\n | pages = 139–154\n | url = http://www.axler.net/DwD.pdf\n | year = 1995\n | doi=10.2307/2975348| jstor = 2975348\n }}</ref>\n\n:<math>\\left(A - \\lambda I\\right)^k {\\mathbf v} = 0,</math>\n\nwhere {{math|'''v'''}} is an nonzero {{math|''n'' &times; 1}} column vector, {{math|''I''}} is the {{math|''n'' &times; ''n''}} [[identity matrix]], {{math|''k''}} is a positive integer, and both {{math|λ}} and {{math|'''v'''}} are allowed to be complex even when {{math|''A''}} is real. When {{math|1=''k'' = 1}}, the vector is called simply an ''[[eigenvector]]'', and the pair is called an ''eigenpair''. In this case, {{math|1=''A'''''v''' = λ'''v'''}}.  Any eigenvalue {{math|λ}} of {{math|''A''}} has ordinary<ref group=\"note\">The term \"ordinary\" is used here only to emphasize the distinction between \"eigenvector\" and \"generalized eigenvector\".</ref> eigenvectors associated to it, for if {{math|''k''}} is the smallest integer such that {{math|1=(''A'' - λ''I'')<sup>''k''</sup> '''v''' = 0}} for a generalized eigenvector {{math|'''v'''}}, then {{math|1=(''A'' - λ''I'')<sup>''k''-1</sup> '''v'''}} is an ordinary eigenvector. The value {{math|''k''}} can always be taken as less than or equal to {{math|''n''}}. In particular, {{math|1=(''A'' - λ''I'')<sup>''n''</sup> '''v''' = 0}} for all generalized eigenvectors {{math|'''v'''}} associated with {{math|λ.}}\n\nFor each eigenvalue {{math|λ}} of {{math|''A''}}, the [[kernel (matrix)|kernel]] {{math|ker(''A'' - λ''I'')}} consists of all eigenvectors associated with {{math|λ}} (along with 0), called the ''[[eigenspace]]'' of {{math|λ}}, while the vector space {{math|ker((''A'' - λ''I'')<sup>''n''</sup>)}} consists of all generalized eigenvectors, and is called the ''[[generalized eigenspace]]''. The ''[[geometric multiplicity]]'' of {{math|λ}} is the dimension of its eigenspace. The ''[[algebraic multiplicity]]'' of {{math|λ}} is the dimension of its generalized eigenspace. The latter terminology is justified by the equation\n\n:<math>p_A\\left(z\\right) = {\\rm det}\\left( zI - A \\right) = \\prod_{i=1}^k (z - \\lambda_i)^{\\alpha_i},</math>\n\nwhere {{math|det}} is the [[determinant]] function, the {{math|λ<sub>''i''</sub>}} are all the distinct eigenvalues of {{math|''A''}} and the {{math|α<sub>''i''</sub>}} are the corresponding algebraic multiplicities. The function {{math|1=''p<sub>A</sub>''(''z'')}} is the ''[[characteristic polynomial]]'' of {{math|''A''}}. So the algebraic multiplicity is the multiplicity of the eigenvalue as a [[Properties of polynomial roots|zero]] of the characteristic polynomial. Since any eigenvector is also a generalized eigenvector, the geometric multiplicity is less than or equal to the algebraic multiplicity. The algebraic multiplicities sum up to {{math|''n''}}, the degree of the characteristic polynomial. The equation {{math|1=''p<sub>A</sub>''(''z'') = 0}} is called the ''characteristic equation'', as its roots are exactly the eigenvalues of {{math|''A''}}. By the [[Cayley–Hamilton theorem]], {{math|''A''}} itself obeys the same equation: {{math|1=''p<sub>A</sub>''(''A'') = 0.}}<ref group=\"note\">where the constant term is multiplied by the identity matrix {{math|''I''}}.</ref> As a consequence, the columns of the matrix <math>\\textstyle \\prod_{i \\ne j} (A - \\lambda_iI)^{\\alpha_i}</math> must be either 0 or generalized eigenvectors of the eigenvalue {{math|λ<sub>''j''</sub>}}, since they are annihilated by <math>\\textstyle (A - \\lambda_jI)^{\\alpha_j}.</math>  In fact, the [[column space]] is the generalized eigenspace of {{math|λ<sub>''j''</sub>.}}\n\nAny collection of generalized eigenvectors of distinct eigenvalues is linearly independent, so a basis for all of {{math|'''''C'''<sup> n</sup>''}} can be chosen consisting of generalized eigenvectors. More particularly, this basis {{math|{{(}}'''v'''<sub>''i''</sub>{{)}}{{Sup sub|''n''|''i''{{=}}1}}}} can be chosen and organized so that\n:* if {{math|'''v'''<sub>''i''</sub>}} and {{math|'''v'''<sub>''j''</sub>}} have the same eigenvalue, then so does {{math|'''v'''<sub>''k''</sub>}} for each {{math|''k''}} between {{math|''i''}} and {{math|''j''}}, and\n:* if {{math|'''v'''<sub>''i''</sub>}} is not an ordinary eigenvector, and if {{math|λ<sub>''i''</sub>}} is its eigenvalue, then {{math|1=(''A'' - λ<sub>''i''</sub>''I'' )'''v'''<sub>''i''</sub> = '''v'''<sub>''i''-1</sub>}} (in particular, {{math|'''v'''<sub>1</sub>}} must be an ordinary eigenvector).\nIf these basis vectors are placed as the column vectors of a matrix {{math|''V'' {{=}} [ '''v'''<sub>1</sub>  '''v'''<sub>2</sub>  ... '''v'''<sub>''n''</sub> ]}}, then {{math|''V''}} can be used to convert {{math|''A''}} to its [[Jordan normal form]]:\n:<math>V^{-1}AV = \\begin{bmatrix} \\lambda_1 & \\beta_1 & 0 & \\ldots & 0 \\\\ 0 & \\lambda_2 & \\beta_2 & \\ldots & 0 \\\\ 0 & 0 & \\lambda_3 & \\ldots & 0 \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & 0 & \\ldots & \\lambda_n \\end{bmatrix},</math>\n\nwhere the {{math|λ<sub>''i''</sub>}} are the eigenvalues, {{math|1=''β''<sub>''i''</sub> = 1}} if {{math|1=(''A'' - λ<sub>''i''+1</sub>)'''v'''<sub>''i''+1</sub> = '''v'''<sub>''i''</sub>}} and {{math|1=''β''<sub>''i''</sub> = 0}} otherwise.\n\nMore generally, if {{math|''W''}} is any invertible matrix, and {{math|λ}} is an eigenvalue of {{math|''A''}} with generalized eigenvector {{math|'''v'''}}, then {{math|1=(''W''<sup>−1</sup>''AW'' - λ''I'' )<sup>''k''</sup> ''W''<sup>−''k''</sup>'''v''' = 0}}. Thus {{math|λ}} is an eigenvalue of {{math|''W''<sup>−1</sup>''AW''}} with generalized eigenvector {{math|''W''<sup>−''k''</sup>'''v'''}}. That is, [[similar matrices]] have the same eigenvalues.\n\n===Normal, Hermitian, and real-symmetric matrices===\n{{main|Adjoint matrix|Normal matrix|Hermitian matrix}}\n\nThe [[conjugate transpose|adjoint]] {{math|''M''<sup>*</sup>}} of a complex matrix {{math|''M''}} is the transpose of the conjugate of {{math|''M''}}: {{math|1=''M'' <sup>*</sup> = {{overline|''M''}} <sup>T</sup>}}. A square matrix {{math|''A''}} is called ''[[Normal matrix|normal]]'' if it commutes with its adjoint: {{math|1=''A''<sup>*</sup>''A'' = ''AA''<sup>*</sup>}}. It is called ''[[Hermitian matrix|hermitian]]'' if it is equal to its adjoint: {{math|1=''A''<sup>*</sup> = ''A''}}. All hermitian matrices are normal. If {{math|''A''}} has only real elements, then the adjoint is just the transpose, and {{math|''A''}} is hermitian if and only if it is [[symmetric matrix|symmetric]]. When applied to column vectors, the adjoint can be used to define the canonical inner product on {{math|'''''C'''<sup>n</sup>''}}: {{math|1='''w''' · '''v''' = '''w'''<sup>*</sup> '''v'''}}.<ref group=\"note\">This ordering of the inner product (with the conjugate-linear position on the left), is preferred by physicists. Algebraists often place the conjugate-linear position on the right: {{math|1='''w''' · '''v''' = '''v'''<sup>*</sup> '''w'''}}.</ref> Normal, hermitian, and real-symmetric matrices have several useful properties:\n:* Every generalized eigenvector of a normal matrix is an ordinary eigenvector.\n:* Any normal matrix is similar to a diagonal matrix, since its Jordan normal form is diagonal.\n:* Eigenvectors of distinct eigenvalues of a normal matrix are orthogonal. \n:* The null space and the image (or column space) of a normal matrix are orthogonal to each other. \n:* For any normal matrix {{math|''A''}}, {{math|'''''C'''<sup> n</sup>''}} has an orthonormal basis consisting of eigenvectors of {{math|''A''}}. The corresponding matrix of eigenvectors is [[Unitary matrix|unitary]].\n:* The eigenvalues of a hermitian matrix are real, since {{math|1=({{overline|λ}} − λ)'''v''' = (''A''<sup>*</sup> − ''A'')'''v''' = (''A'' − ''A'')'''v''' = 0}} for a non-zero eigenvector {{math|'''v'''}}.\n:* If {{math|''A''}} is real, there is an orthonormal basis for {{math|'''''R'''<sup>n</sup>''}} consisting of eigenvectors of {{math|''A''}} if and only if {{math|''A''}} is symmetric.\n\nIt is possible for a real or complex matrix to have all real eigenvalues without being hermitian. For example, a real [[triangular matrix]] has its eigenvalues along its diagonal, but in general is not symmetric.\n\n==Condition number==\n\nAny problem of numeric calculation can be viewed as the evaluation of some function &fnof; for some input {{math|''x''}}. The [[condition number]] {{math|''κ''(&fnof;, ''x'')}} of the problem is the ratio of the relative error in the function's output to the relative error in the input, and varies with both the function and the input. The condition number describes how error grows during the calculation. Its base-10 logarithm tells how many fewer digits of accuracy exist in the result than existed in the input. The condition number is a best-case scenario. It reflects the instability built into the problem, regardless of how it is solved. No algorithm can ever produce more accurate results than indicated by the condition number, except by chance. However, a poorly designed algorithm may produce significantly worse results. For example, as mentioned below, the problem of finding eigenvalues for normal matrices is always well-conditioned. However, the problem of finding the roots of a polynomial can be [[Wilkinson's polynomial|very ill-conditioned]]. Thus eigenvalue algorithms that work by finding the roots of the characteristic polynomial can be ill-conditioned even when the problem is not.\n\nFor the problem of solving the linear equation {{math|1=''A'''''v''' = '''b'''}} where {{math|''A''}} is invertible, the condition number {{math|1=''κ''(''A''<sup>−1</sup>, '''b''')}} is given by {{math|1={{!!}}''A''{{!!}}<sub>op</sub>{{!!}}''A''<sup>−1</sup>{{!!}}<sub>op</sub>}}, where {{nowrap|{{!!}}  {{!!}}<sub>op</sub>}}  is the [[operator norm]] subordinate to the normal [[Norm (mathematics)#Euclidean norm|Euclidean norm]] on {{math|'''''C'''<sup> n</sup>''}}. Since this number is independent of {{math|'''b'''}} and is the same for {{math|''A''}} and {{math|''A''<sup>−1</sup>}}, it is usually just called the condition number {{math|''κ''(''A'')}} of the matrix {{math|''A''}}. This value {{math|''κ''(''A'')}} is also the absolute value of the ratio of the largest eigenvalue of {{math|''A''}} to its smallest. If {{math|''A''}} is [[Unitary matrix|unitary]], then {{math|1={{!!}}''A''{{!!}}<sub>op</sub> = {{!!}}''A''<sup>−1</sup>{{!!}}<sub>op</sub> = 1}}, so {{math|1=''κ''(''A'') = 1}}. For general matrices, the operator norm is often difficult to calculate. For this reason, other [[matrix norms]] are commonly used to estimate the condition number.\n\nFor the eigenvalue problem, [[Bauer–Fike theorem|Bauer and Fike proved]] that if {{math|λ}} is an eigenvalue for a [[Diagonalizable matrix|diagonalizable]] {{math|''n'' &times; ''n''}} matrix {{math|''A''}} with eigenvector matrix {{math|''V''}}, then the absolute error in calculating {{math|λ}} is bounded by the product of {{math|''κ''(''V'')}} and the absolute error in {{math|''A''}}.<ref>{{Citation\n | author = F. L. Bauer\n | author2 = C. T. Fike\n | title = Norms and exclusion theorems\n | journal = Numer. Math. \n | volume = 2\n | pages = 137–141\n | year = 1960\n | doi=10.1007/bf01386217}}</ref> [[Bauer-Fike theorem#Corollary|As a result]], the condition number for finding {{math|λ}} is {{math|1=''κ''(λ, ''A'') = ''κ''(''V'') = {{!!}}''V'' {{!!}}<sub>op</sub> {{!!}}''V'' <sup>−1</sup>{{!!}}<sub>op</sub>}}. If {{math|''A''}} is normal, then {{math|''V''}} is unitary, and {{math|1=''κ''(λ, ''A'') = 1}}. Thus the eigenvalue problem for all normal matrices is well-conditioned.\n\nThe condition number for the problem of finding the eigenspace of a normal matrix {{math|''A''}} corresponding to an eigenvalue {{math|λ}} has been shown to be inversely proportional to the minimum distance between {{math|λ}} and the other distinct eigenvalues of {{math|''A''}}.<ref>{{Citation\n | author = S.C. Eisenstat\n | author2 = I.C.F. Ipsen\n | title = Relative Perturbation Results for Eigenvalues and Eigenvectors of Diagonalisable Matrices\n | journal = BIT\n | volume = 38\n | issue = 3\n | pages = 502–9\n | year = 1998\n | doi=10.1007/bf02510256}}</ref> In particular, the eigenspace problem for normal matrices is well-conditioned for isolated eigenvalues. When eigenvalues are not isolated, the best that can be hoped for is to identify the span of all eigenvectors of nearby eigenvalues.\n\n==Algorithms==\n\nAny monic polynomial is the characteristic polynomial of its [[companion matrix]]. Therefore, a general algorithm for finding eigenvalues could also be used to find the roots of polynomials. The [[Abel–Ruffini theorem]] shows that any such algorithm for dimensions greater than 4 must either be infinite, or involve functions of greater complexity than elementary arithmetic operations and fractional powers. For this reason algorithms that exactly calculate eigenvalues in a finite number of steps only exist for a few special classes of matrices. For general matrices, algorithms are [[Iterative method|iterative]], producing better approximate solutions with each iteration.\n\nSome algorithms produce every eigenvalue, others will produce a few, or only one. However, even the latter algorithms can be used to find all eigenvalues. Once an eigenvalue {{math|λ}} of a matrix {{math|''A''}} has been identified, it can be used to either direct the algorithm towards a different solution next time, or to reduce the problem to one that no longer has {{math|λ}} as a solution.\n\nRedirection is usually accomplished by shifting: replacing {{math|''A''}} with {{math|''A'' - μ''I''}} for some constant {{math|μ}}. The eigenvalue found for {{math|''A'' - μ''I''}} must have {{math|μ}} added back in to get an eigenvalue for {{math|''A''}}. For example, for [[power iteration]], {{math|1=μ = λ}}. Power iteration finds the largest eigenvalue in absolute value, so even when {{math|λ}} is only an approximate eigenvalue, power iteration is unlikely to find it a second time. Conversely, [[inverse iteration]] based methods find the lowest eigenvalue, so {{math|μ}} is chosen well away from {{math|λ}} and hopefully closer to some other eigenvalue.\n\nReduction can be accomplished by restricting {{math|''A''}} to the column space of the matrix {{math|''A'' - λ''I''}}, which {{math|''A''}} carries to itself. Since {{math|''A'' - λ''I''}} is singular, the column space is of lesser dimension. The eigenvalue algorithm can then be applied to the restricted matrix. This process can be repeated until all eigenvalues are found.\n\nIf an eigenvalue algorithm does not produce eigenvectors, a common practice is to use an inverse iteration based algorithm with {{math|μ}} set to a close approximation to the eigenvalue. This will quickly converge to the eigenvector of the closest eigenvalue to {{math|μ}}. For small matrices, an alternative is to look at the column space of the product of {{math|''A'' - λ{{'}}''I''}} for each of the other eigenvalues {{math|λ{{'}}.}}\n\n==Hessenberg and tridiagonal matrices==\n\n{{main|Hessenberg matrix}}\n\nBecause the eigenvalues of a triangular matrix are its diagonal elements, for general matrices there is no finite method like [[gaussian elimination]] to convert a matrix to triangular form while preserving eigenvalues. But it is possible to reach something close to triangular. An [[Hessenberg matrix|upper Hessenberg matrix]] is a square matrix for which all entries below the [[subdiagonal]] are zero. A lower Hessenberg matrix is one for which all entries above the [[superdiagonal]] are zero. Matrices that are both upper and lower Hessenberg are [[Tridiagonal matrix|tridiagonal]]. Hessenberg and tridiagonal matrices are the starting points for many eigenvalue algorithms because the zero entries reduce the complexity of the problem. Several methods are commonly used to convert a general matrix into a Hessenberg matrix with the same eigenvalues. If the original matrix was symmetric or hermitian, then the resulting matrix will be tridiagonal.\n\nWhen only eigenvalues are needed, there is no need to calculate the similarity matrix, as the transformed matrix has the same eigenvalues. If eigenvectors are needed as well, the similarity matrix may be needed to transform the eigenvectors of the Hessenberg matrix back into eigenvectors of the original matrix.\n\n{| class=\"wikitable\" style=\"text-align: center\"\n|-\n! Method !! Applies to !! Produces !! Cost without similarity matrix !! Cost with similarity matrix !! Description\n|-\n| [[Householder transformation]]s || General || Hessenberg || {{math|{{frac|2''n''<sup>3</sup>|3}} + ''O''(''n''<sup>2</sup>)}}<ref name=NumericalRecipes>{{cite book\n| last1 = Press\n| first1 = William H.\n| last2 = Teukolsky\n| first2 = Saul A.\n| last3 = Vetterling\n| first3 = William T.\n| last4 = Flannery\n| first4 = Brian P.\n| title = Numerical Recipes in C\n| edition = 2nd\n| year = 1992\n| publisher = Cambridge University Press \n| isbn = 978-0-521-43108-8\n}}</ref>{{rp|page=474}} || {{math|{{frac|4''n''<sup>3</sup>|3}} + ''O''(''n''<sup>2</sup>)}}<ref name=NumericalRecipes />{{rp|page=474}} || align=\"left\" | Reflect each column through a subspace to zero out its lower entries.\n|-\n| [[Givens rotation]]s || General  || Hessenberg || {{math|{{frac|4''n''<sup>3</sup>|3}} + ''O''(''n''<sup>2</sup>)}}<ref name=NumericalRecipes />{{rp|page=470}} ||  || align=\"left\" | Apply planar rotations to zero out individual entries. Rotations are ordered so that later ones do not cause zero entries to become non-zero again. \n|-\n| [[Arnoldi iteration]] || General || Hessenberg ||  || || align=\"left\" | Perform Gram–Schmidt orthogonalization on Krylov subspaces.\n|-\n| [[Lanczos algorithm]] || Hermitian || Tridiagonal ||  ||  || align=\"left\" | Arnoldi iteration for hermitian matrices, with shortcuts. \n|}\n\nFor tridiagonal eigenvalue problems all eigenvalues (without eigenvectors) can be computed numerically in O(n),{{dubious|Tridiagonal eigenvalue complexity|date=June 2018}} using bisection on the characteristic polynomial.\n\n==Iterative algorithms==\nIterative algorithms solve the eigenvalue problem by producing sequences that converge to the eigenvalues. Some algorithms also produce sequences of vectors that converge to the eigenvectors. Most commonly, the eigenvalue sequences are expressed as sequences of similar matrices which converge to a triangular or diagonal form, allowing the eigenvalues to be read easily. The eigenvector sequences are expressed as the corresponding similarity matrices.\n\n{| class=\"wikitable\" style=\"text-align: center\"\n|-\n! Method !! Applies to !! Produces !!Cost per step !! Convergence !! Description\n|-\n| [[Power iteration]] || general || eigenpair with largest value || {{math|''O''(''n''<sup>2</sup>)}} || linear || align=\"left\" |Repeatedly applies the matrix to an arbitrary starting vector and renormalizes.\n|-\n| [[Inverse iteration]] || general  || {{nowrap|eigenpair with value closest to μ}} ||  || linear ||  align=\"left\" |Power iteration for {{math|(''A'' − μ''I'' )<sup>−1</sup>}}\n|-\n| [[Rayleigh quotient iteration]] || hermitian || any eigenpair ||  || cubic ||  align=\"left\" |Power iteration for {{math|(''A'' − μ<sub>''i''</sub>''I'' )<sup>−1</sup>,}} where {{math|μ<sub>''i''</sub>}} for each iteration is the Rayleigh quotient of the previous iteration.\n|-\n| width=\"200\" | [[Preconditioned inverse iteration]]<ref>{{Citation\n| last=Neymeyr\n| first=K.\n| title=A geometric theory for preconditioned inverse iteration IV: On the fastest convergence cases.\n| journal=Linear Algebra Appl.\n| volume=415\n| issue=1\n| pages=114–139\n| year=2006\n| doi=10.1016/j.laa.2005.06.022\n}}</ref> or [[LOBPCG|LOBPCG algorithm]] || [[Positive-definite matrix|positive-definite]] real symmetric || eigenpair with value closest to μ ||  ||  || align=\"left\" | Inverse iteration using a [[preconditioner]] (an approximate inverse to {{math|''A''}}).\n|-\n| [[Bisection eigenvalue algorithm|Bisection method]] || real symmetric tridiagonal  || any eigenvalue ||  || linear || align=\"left\" | Uses the [[bisection method]] to find roots of the characteristic polynomial, supported by the Sturm sequence.\n|-\n| [[Laguerre iteration]] || real symmetric tridiagonal  || any eigenvalue || || cubic<ref>{{Citation\n| last1=Li\n| first1=T. Y.\n| last2=Zeng\n| first2=Zhonggang\n| title=Laguerre's Iteration In Solving The Symmetric Tridiagonal Eigenproblem - Revisited\n| journal=[[SIAM Journal on Scientific Computing]]\n| year=1992\n}}</ref> || align=\"left\" | Uses [[Laguerre's method]] to find roots of the characteristic polynomial, supported by the Sturm sequence.\n|-\n| rowspan=\"2\" | [[QR algorithm]] ||rowspan=\"2\" | Hessenberg|| all eigenvalues ||  {{math|''O''(''n''<sup>2</sup>)}} ||rowspan=\"2\" | cubic || align=\"left\" rowspan=\"2\" | Factors ''A'' = ''QR'', where ''Q'' is orthogonal and ''R'' is triangular, then applies the next iteration to ''RQ''.\n|-\n| all eigenpairs || {{math|6''n''<sup>3</sup> + ''O''(''n''<sup>2</sup>)}}\n|-\n| [[Jacobi eigenvalue algorithm]] || real symmetric || all eigenvalues ||{{math|''O''(''n''<sup>3</sup>)}} || quadratic || align=\"left\" | Uses Givens rotations to attempt clearing all off-diagonal entries. This fails, but strengthens the diagonal.\n|-\n| rowspan=\"2\" | [[Divide-and-conquer eigenvalue algorithm|Divide-and-conquer]] || rowspan=\"2\" | Hermitian Tridiagonal || all eigenvalues || {{math|''O''(''n''<sup>2</sup>)}} || rowspan=\"2\" | || align=\"left\" rowspan=\"2\" | Divides the matrix into submatrices that are diagonalized then recombined.\n|-\n| all eigenpairs || {{math|({{frac|4|3}})''n''<sup>3</sup> + ''O''(''n''<sup>2</sup>)}}\n|-\n| [[Homotopy method]] || real symmetric tridiagonal || all eigenpairs || {{math|''O''(''n''<sup>2</sup>)<ref>{{Citation\n| last=Chu\n| first=Moody T.\n| title=A Note on the Homotopy Method for Linear Algebraic Eigenvalue Problems\n| journal=Linear Algebra Appl.\n| volume=105\n| pages=225–236\n| year=1988\n| doi=10.1016/0024-3795(88)90015-8\n}}</ref>}} ||  || align=\"left\" | Constructs a computable homotopy path from a diagonal eigenvalue problem.\n|-\n| [[Folded spectrum method]] || real symmetric || eigenpair with value closest to μ ||  ||  || align=\"left\" | Preconditioned inverse iteration applied to {{math|(''A'' − μ''I'' )<sup>2</sup>}}\n|-\n| [[MRRR|MRRR algorithm]]<ref>{{Citation\n| last1=Dhillon\n| first1=Inderjit S.\n| last2=Parlett\n| first2=Beresford N.\n| last3=Vömel\n| first3=Christof\n| title=The Design and Implementation of the MRRR Algorithm\n| journal=[[ACM Transactions on Mathematical Software]]\n| volume=32\n| issue=4\n| pages=533–560\n| year=2006\n| doi=10.1145/1186785.1186788\n}}</ref> || real symmetric tridiagonal || some or all eigenpairs || {{math|''O''(''n''<sup>2</sup>)}} ||  || align=\"left\" | \"Multiple relatively robust representations\" – performs inverse iteration on a [[Cholesky decomposition|''LDL''<sup>T</sup> decomposition]] of the shifted matrix.\n\t\n|}\n\n==Direct calculation==\n\nWhile there is no simple algorithm to directly calculate eigenvalues for general matrices, there are numerous special classes of matrices where eigenvalues can be directly calculated. These include:\n\n===Triangular matrices===\n\nSince the determinant of a [[triangular matrix]] is the product of its diagonal entries, if ''T'' is triangular, then <math>\\textstyle \\det(\\lambda I - T) = \\prod_i (\\lambda - T_{ii})</math>. Thus the eigenvalues of ''T'' are its diagonal entries.\n\n===Factorable polynomial equations===\n\nIf {{math|''p''}} is any polynomial and {{math|1=''p''(''A'') = 0,}} then the eigenvalues of {{math|''A''}} also satisfy the same equation. If {{math|''p''}} happens to have a known factorization, then the eigenvalues of {{math|''A''}} lie among its roots.\n\nFor example, a [[Projection (linear algebra)|projection]] is a square matrix {{math|''P''}} satisfying {{math|1=''P''<sup>2</sup> = ''P''}}. The roots of the corresponding scalar polynomial equation, {{math|1=λ<sup>2</sup> = λ}}, are 0 and 1. Thus any projection has 0 and 1 for its eigenvalues. The multiplicity of 0 as an eigenvalue is the [[Kernel (linear algebra)#Representation as matrix multiplication|nullity]] of {{math|''P''}}, while the multiplicity of 1 is the rank of {{math|''P''}}.\n\nAnother example is a matrix {{math|''A''}} that satisfies {{math|1=''A''<sup>2</sup> = α<sup>2</sup>''I''}} for some scalar {{math|α}}. The eigenvalues must be {{math|±α}}. The projection operators\n:<math>P_+=\\frac{1}{2}\\left(I+\\frac{A}{\\alpha}\\right)</math>\n\n:<math>P_-=\\frac{1}{2}\\left(I-\\frac{A}{\\alpha}\\right)</math>\nsatisfy\n:<math>AP_+=\\alpha P_+ \\quad AP_-=-\\alpha P_-</math>\nand\n:<math>P_+P_+=P_+ \\quad P_-P_-=P_- \\quad P_+P_-=P_-P_+=0.</math>\n\nThe [[column space]]s of {{math|''P''<sub>+</sub>}} and {{math|''P''<sub>−</sub>}} are the eigenspaces of {{math|''A''}} corresponding to {{math|+α}} and {{math|-α}}, respectively.\n\n===2×2 matrices===\n\nFor dimensions 2 through 4, formulas involving radicals exist that can be used to find the eigenvalues. While a common practice for 2×2 and 3×3 matrices, for 4×4 matrices the increasing complexity of the [[Quartic function#Ferrari's solution|root formulas]] makes this approach less attractive.\n\nFor the 2×2 matrix\n\n:<math>A = \\begin{bmatrix} a  & b \\\\ c & d \\end{bmatrix},</math>\n\nthe characteristic polynomial is\n\n:<math>{\\rm det} \\begin{bmatrix} \\lambda - a & -b \\\\ -c & \\lambda - d \\end{bmatrix} = \\lambda^2\\, -\\, \\left( a + d \\right )\\lambda\\, +\\, \\left ( ad - bc \\right ) = \\lambda^2\\, -\\, \\lambda\\, {\\rm tr}(A)\\, +\\, {\\rm det}(A).</math>\n\nThus the eigenvalues can be found by using the [[quadratic formula]]:\n\n:<math>\\lambda = \\frac{{\\rm tr}(A) \\pm \\sqrt{{\\rm tr}^2 (A) - 4 {\\rm det}(A)}}{2}.</math>\n\nDefining <math>\\textstyle {\\rm gap}\\left ( A \\right ) = \\sqrt{{\\rm tr}^2 (A) - 4 {\\rm det}(A)}</math>  to be the distance between the two eigenvalues, it is straightforward to calculate\n\n:<math>\\frac{\\partial\\lambda}{\\partial a} = \\frac{1}{2}\\left ( 1 \\pm \\frac{a - d}{{\\rm gap}(A)} \\right ),\\qquad \\frac{\\partial\\lambda}{\\partial b} =  \\frac{\\pm c}{{\\rm gap}(A)}</math>\n\nwith similar formulas for {{math|''c''}} and {{math|''d''}}. From this it follows that the calculation is well-conditioned if the eigenvalues are isolated.\n\nEigenvectors can be found by exploiting the [[Cayley–Hamilton theorem]]. If {{math|λ<sub>1</sub>, λ<sub>2</sub>}} are the eigenvalues, then {{math|1=(''A'' - λ<sub>1</sub>''I'' )(''A'' - λ<sub>2</sub>''I'' ) = (''A'' - λ<sub>2</sub>''I'' )(''A'' - λ<sub>1</sub>''I'' ) = 0}}, so the columns of {{math|(''A'' - λ<sub>2</sub>''I'' )}} are annihilated by {{math|(''A'' - λ<sub>1</sub>''I'' )}} and vice versa. Assuming neither matrix is zero, the columns of each must include eigenvectors for the other eigenvalue. (If either matrix is zero, then {{math|''A''}} is a multiple of the identity and any non-zero vector is an eigenvector.)\n\nFor example, suppose\n\n:<math>A = \\begin{bmatrix} 4 & 3 \\\\ -2 & -3 \\end{bmatrix},</math>\n\nthen {{math|1=tr(''A'') = 4 - 3 = 1}} and {{math|1=det(''A'') = 4(-3) - 3(-2) = -6}}, so the characteristic equation is\n\n:<math> 0 = \\lambda^2 - \\lambda - 6 = (\\lambda - 3)(\\lambda + 2),</math>\n\nand the eigenvalues are 3 and -2. Now,\n\n:<math>A - 3I = \\begin{bmatrix} 1 & 3 \\\\ -2 & -6 \\end{bmatrix}, \\qquad  A + 2I = \\begin{bmatrix} 6 & 3 \\\\ -2 & -1 \\end{bmatrix}.</math>\n\nIn both matrices, the columns are multiples of each other, so either column can be used. Thus, {{math|(1, -2)}} can be taken as an eigenvector associated with the eigenvalue -2, and {{math|(3, -1)}} as an eigenvector associated with the eigenvalue 3, as can be verified by multiplying them by {{math|''A''}}.\n\n===3×3 matrices===\n\nIf {{math|''A''}} is a 3×3 matrix, then its characteristic equation can be expressed as:\n\n:<math>{\\rm det} \\left( \\alpha I - A \\right) = \\alpha^3 - \\alpha^2 {\\rm tr}(A) - \\alpha \\frac{1}{2}\\left( {\\rm tr}(A^2) - {\\rm tr}^2(A) \\right) - {\\rm det}(A) = 0.</math>\n\nThis equation may be solved using the methods of [[Cubic function#Cardano's method|Cardano]] or [[Cubic function#Lagrange's method|Lagrange]], but an affine change to {{math|''A''}} will simplify the expression considerably, and lead directly to a [[Cubic function#Trigonometric and hyperbolic solutions|trigonometric solution]]. If {{math|1=''A'' = ''pB'' + ''qI''}}, then {{math|''A''}} and {{math|''B''}} have the same eigenvectors, and {{math|''β''}} is an eigenvalue of {{math|''B''}} if and only if {{math|1=''α'' = ''pβ'' + ''q''}} is an eigenvalue of {{math|''A''}}. Letting <math>\\textstyle q = {\\rm tr}(A)/3</math> and <math>\\textstyle p =\\left({\\rm tr}\\left((A - qI)^2\\right)/ 6\\right)^{1/2}</math>, gives\n\n:<math>{\\rm det} \\left( \\beta I - B \\right) = \\beta^3 - 3 \\beta - {\\rm det}(B) = 0.</math>\n\nThe substitution {{math|1=''β'' = 2cos ''θ''}} and some simplification using the identity {{math|1=cos 3''θ'' = 4cos<sup>3</sup> ''θ'' - 3cos ''θ''}} reduces the equation to {{math|1=cos 3''θ'' = det(''B'') / 2}}. Thus\n\n:<math>\\beta = 2{\\rm cos}\\left(\\frac{1}{3}{\\rm arccos}\\left( {\\rm det}(B)/2 \\right) + \\frac{2k\\pi}{3}\\right), \\quad k = 0, 1, 2.</math>\n\nIf {{math|det(''B'')}} is complex or is greater than 2 in absolute value, the arccosine should be taken along the same branch for all three values of {{math|''k''}}. This issue doesn't arise when {{math|''A''}} is real and symmetric, resulting in a simple algorithm:<ref name=Smith>{{Citation |last=Smith |first=Oliver K. |title=Eigenvalues of a symmetric 3 × 3 matrix. |journal=[[Communications of the ACM]] |volume=4 |issue=4 |date=April 1961 |page=168 |doi=10.1145/355578.366316}}</ref>\n\n<source lang=\"matlab\">\n% Given a real symmetric 3x3 matrix A, compute the eigenvalues\n% Note that acos and cos operate on angles in radians\n\np1 = A(1,2)^2 + A(1,3)^2 + A(2,3)^2\nif (p1 == 0) \n   % A is diagonal.\n   eig1 = A(1,1)\n   eig2 = A(2,2)\n   eig3 = A(3,3)\nelse\n   q = trace(A)/3               % trace(A) is the sum of all diagonal values\n   p2 = (A(1,1) - q)^2 + (A(2,2) - q)^2 + (A(3,3) - q)^2 + 2 * p1\n   p = sqrt(p2 / 6)\n   B = (1 / p) * (A - q * I)    % I is the identity matrix\n   r = det(B) / 2\n\n   % In exact arithmetic for a symmetric matrix  -1 <= r <= 1\n   % but computation error can leave it slightly outside this range.\n   if (r <= -1) \n      phi = pi / 3\n   elseif (r >= 1)\n      phi = 0\n   else\n      phi = acos(r) / 3\n   end\n\n   % the eigenvalues satisfy eig3 <= eig2 <= eig1\n   eig1 = q + 2 * p * cos(phi)\n   eig3 = q + 2 * p * cos(phi + (2*pi/3))\n   eig2 = 3 * q - eig1 - eig3     % since trace(A) = eig1 + eig2 + eig3\nend\n</source>\n\nOnce again, the eigenvectors of {{math|''A''}} can be obtained by recourse to the [[Cayley–Hamilton theorem]]. If {{math|''α''<sub>1</sub>, ''α''<sub>2</sub>, ''α''<sub>3</sub>}} are distinct eigenvalues of {{math|''A''}}, then {{math|1=(''A'' - ''α''<sub>1</sub>''I'')(''A'' - ''α''<sub>2</sub>''I'')(''A'' - ''α''<sub>3</sub>''I'') = 0}}. Thus the columns of the product of any two of these matrices will contain an eigenvector for the third eigenvalue. However, if {{math|1=''α''<sub>3</sub> = ''α''<sub>1</sub>}}, then {{math|1=(''A'' - ''α''<sub>1</sub>''I'')<sup>2</sup>(''A'' - ''α''<sub>2</sub>''I'') = 0}} and {{math|1=(''A'' - ''α''<sub>2</sub>''I'')(''A'' - ''α''<sub>1</sub>''I'')<sup>2</sup> = 0}}. Thus the ''generalized'' eigenspace of {{math|''α''<sub>1</sub>}} is spanned by the columns of {{math|''A'' - ''α''<sub>2</sub>''I''}} while the ordinary eigenspace is spanned by the columns of {{math|1=(''A'' - ''α''<sub>1</sub>''I'')(''A'' - ''α''<sub>2</sub>''I'')}}.  The ordinary eigenspace of {{math|''α''<sub>2</sub>}} is spanned by the columns of {{math|(''A'' - ''α''<sub>1</sub>''I'')<sup>2</sup>}}.\n\nFor example, let\n\n:<math>A = \\begin{bmatrix} 3 & 2 & 6 \\\\ 2 & 2 & 5 \\\\ -2 & -1 & -4 \\end{bmatrix}.</math>\n\nThe characteristic equation is\n\n:<math> 0 = \\lambda^3 - \\lambda^2 - \\lambda + 1 = (\\lambda - 1)^2(\\lambda + 1),</math>\n\nwith eigenvalues 1 (of multiplicity 2) and -1. Calculating,\n\n:<math>A - I = \\begin{bmatrix} 2 & 2 & 6 \\\\ 2 & 1 & 5 \\\\ -2 & -1 & -5 \\end{bmatrix}, \\qquad A + I = \\begin{bmatrix} 4 & 2 & 6 \\\\ 2 & 3 & 5 \\\\ -2 & -1 & -3 \\end{bmatrix}</math>\n\nand\n\n:<math>(A - I)^2 = \\begin{bmatrix} -4 & 0 & -8 \\\\ -4 & 0 & -8 \\\\ 4 & 0 & 8 \\end{bmatrix}, \\qquad (A - I)(A + I) = \\begin{bmatrix} 0 & 4 & 4 \\\\ 0 & 2 & 2 \\\\ 0 & -2 & -2 \\end{bmatrix}</math>\n\nThus {{math|(-4, -4, 4)}} is an eigenvector for -1, and {{math|(4, 2, -2)}} is an eigenvector for 1. {{math|(2, 3, -1)}} and {{math|(6, 5, -3)}} are both generalized eigenvectors associated with 1, either one of which could be combined with {{math|(-4, -4, 4)}} and {{math|(4, 2, -2)}} to form a basis of generalized eigenvectors of {{math|''A''}}. Once found, the eigenvectors can be normalized if needed.\n\n==== Eigenvectors of normal 3×3 matrices ====\nIf a 3×3 matrix <math>A</math> is normal, then the cross-product can be used to find eigenvectors. If <math>\\lambda</math> is an eigenvalue of <math>A</math>, then the null space of <math>A - \\lambda I</math> is perpendicular to its column space, The cross product of two independent columns of  <math>A - \\lambda I</math> will be in the null space. I.e., it will be an eigenvector associated with <math>\\lambda</math>. Since the column space is two dimensional in this case, the eigenspace must be one dimensional, so any other eigenvector will be parallel to it.\n\nIf <math>A - \\lambda I</math> does not contain two independent columns but is not {{math|'''0'''}}, the cross-product can still be used. In this case <math>\\lambda</math> is an eigenvalue of multiplicity 2, so any vector perpendicular to the column space will be an eigenvector. Suppose <math>\\mathbf v</math> is a non-zero column of <math>A - \\lambda I</math>. Choose an arbitrary vector <math>\\mathbf u</math> not parallel to <math>\\mathbf v</math>. Then <math>\\mathbf v\\times \\mathbf u</math>  and  <math>(\\mathbf v\\times \\mathbf u)\\times \\mathbf v</math> will be perpendicular to <math>\\mathbf v</math> and thus will be eigenvectors of  <math>\\lambda</math>.\n\nThis does not work when <math>A</math> is not normal, as the null space and column space do not need to be perpendicular for such matrices.\n\n==See also==\n* [[List of numerical analysis topics#Eigenvalue algorithms|List of eigenvalue algorithms]]\n\n==Notes==\n{{reflist|group=\"note\"}}\n\n==References==\n{{reflist}}\n\n==Further reading==\n*{{cite journal\n | last = Bojanczyk\n | first = Adam W. \n | authorlink =\n |author2=Adam Lutoborski\n | title = Computation of the Euler angles of a symmetric 3X3 matrix\n | journal = SIAM Journal on Matrix Analysis and Applications\n | volume = 12\n | issue = 1\n | pages = 41–48\n | publisher =\n | location =\n | date = Jan 1991\n | url = http://cacm.acm.org/magazines/1961/4/14532-eigenvalues-of-a-symmetric-3-%C3%83-3-matrix/abstract\n | jstor =\n | issn =\n | doi = 10.1137/0612005}}\n\n{{Numerical linear algebra}}\n\n{{DEFAULTSORT:Eigenvalue Algorithm}}\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "EISPACK",
      "url": "https://en.wikipedia.org/wiki/EISPACK",
      "text": "'''EISPACK''' is a [[software library]] for [[numerical computation]] of [[eigenvalues]] and [[eigenvectors]] of matrices, written in [[FORTRAN]]. It contains subroutines for calculating the eigenvalues of nine classes of [[matrix (Mathematics)|matrices]]:  complex general, complex [[Hermitian matrix|Hermitian]], real general, real symmetric, real symmetric [[band matrix|banded]], real symmetric tridiagonal, special real tridiagonal, generalized real, and generalized real symmetric matrices.\nIn addition it includes subroutines to perform a [[singular value decomposition]].\n\nOriginally written around 1972–1973,<ref>{{cite journal | last1 = Garbow | first1 = Burton S. | year = 1974 | title = EISPACK — A package of matrix eigensystem routines | doi = 10.1016/0010-4655(74)90086-1 | journal = Computer Physics Communications | volume = 7 | issue = | pages = 179–184 }}</ref> EISPACK, like [[LINPACK]] and [[MINPACK]], originated from  [[Argonne National Laboratory]], has always been free, and aims to be [[Software portability|portable]], robust and reliable. The library drew heavily on algorithms developed by [[James H. Wilkinson|James Wilkinson]], which were originally implemented in [[ALGOL]].  Brian Smith led a team at Argonne developing EISPACK, initially by translating these algorithms into FORTRAN. [[Jack Dongarra]] joined the team as an undergraduate intern at Argonne, and later went on to create [[LAPACK]], which has largely superseded EISPACK and LINPACK.\n\n==Documentation==\n*Brian Smith, James Boyle, Jack Dongarra, Burton Garbow, Y Ikebe, V Klema, Cleve Moler, Matrix Eigensystem Routines, EISPACK Guide, [[Lecture Notes in Computer Science]], Volume 6, Springer Verlag, 1976. As of 2012, it has been cited more than 1200 times in the scholarly literature.\n*Burton Garbow et al., Matrix eigensystem routines: EISPACK guide extension, Lecture Notes in Computer Science, Volume 51, Springer Verlag, 1977.\n\n==References==\n{{reflist}}\n\n==External links==\n*[[Netlib]] [http://www.netlib.org/eispack/ download site for EISPACK]\n*[http://history.siam.org/oralhistories/dongarra.htm Interview] with [[Jack Dongarra]] about EISPACK.\n\n{{Numerical linear algebra}}\n\n[[Category:Fortran libraries]]\n[[Category:Numerical linear algebra]]\n[[Category:Numerical software]]"
    },
    {
      "title": "Fangcheng (mathematics)",
      "url": "https://en.wikipedia.org/wiki/Fangcheng_%28mathematics%29",
      "text": "'''Fangcheng''' (sometimes written as '''fang-cheng''' or  '''fang cheng''') ({{zh|c=方程|p=fāng chéng}}) is the title of the eighth chapter of the [[Chinese mathematics|Chinese mathematical]] classic [[Jiuzhang suanshu]]  (The Nine Chapters on the Mathematical Art) composed by several generations of scholars who flourished during the period from the 10th to the 2nd century BC. This text is one of the earliest surviving mathematical texts from China. Several historians of Chinese mathematics have observed that the term ''fangcheng'' is not easy to translate exactly.<ref name=\"Hist01\">{{cite book |author=Jean-Clause Martzloff |title=A History of Chinese Mathematics |date=2006 |publisher=Springer |page=250}}</ref><ref name=\"Hart01\">{{cite book |author=Roger Hart |title=The Chinese Roots of Linear Algebra |date=2011 |publisher=The Johns Hopkins University Press |url=https://muse.jhu.edu/chapter/322683 |accessdate=6 December 2016}}</ref> However, as a first approximation it has been translated as \"[[Matrix (mathematics)|rectangular arrays]]\" or \"square arrays\".<ref name=Hist01/> The term is also used to refer to a particular procedure for solving a certain class of problems discussed in the Chapter 8 of The Nine Chapters book.<ref name=Hart01/>\n\nThe procedure referred to by the term ''fangcheng'' and explained in the eighth chapter of The Nine Chapters, is essentially a procedure to find the solution of systems of ''n'' equations in ''n'' unknowns and is equivalent to certain similar procedures in modern [[linear algebra]]. The earliest recorded ''fangcheng'' procedure is similar to what we now call [[Gaussian elimination]].\n\nThe ''fangcheng'' procedure was popular in ancient China and was transmitted to [[Japan]]. It is possible that this procedure was transmitted to [[Europe]] also and served as precursors of the modern theory of [[Matrix (mathematics)|matrices]], [[Gaussian elimination]], and [[determinant]]s.<ref name=\"Hart02\"/> It is well known that there was not much work on linear algebra in [[Greece]] or [[Europe]] prior to [[Gottfried Leibniz]]'s studies of [[Elimination theory|elimination]] and determinants, beginning in 1678. Moreover Leibniz was a [[Sinophile]] and was interested in the translations of such Chinese texts as were available to him.<ref name=\"Hart02\">{{cite book |author=Roger Hart|title=The Chinese Roots of Linear Algebra |date=2011 |publisher=The Johns Hopkins University Press |url=https://muse.jhu.edu/chapter/322679 |accessdate=6 December 2016}}</ref>\n\n==On the meaning of ''fangcheng''==\n\nThere is no ambiguity in the meaning of the first character ''fang''. It means \"rectangle\" or \"square.\" But different interpretations are given to the second character ''cheng'':<ref name=Hart01/>\n\n#The earliest extant commentary, by [[Liu Hui]], dated 263 CE defines ''cheng'' as \"measures,\" citing the non-mathematical term ''kecheng'', which means \"collecting taxes according to tax rates.\" Liu then defines ''fangcheng'' as a \"rectangle of measures.\" The term ''kecheng'', however, is not a mathematical term and it appears nowhere else in the Nine Chapters.  Outside of mathematics, ''kecheng'' is a term most commonly used for collecting taxes. \n#Li Ji's \"Nine Chapters on the Mathematical Arts: Pronunciations and Meanings\" also glosses ''cheng'' as \"measure,\" again using a nonmathematical term, ''kelü'', commonly used for taxation. This is how Li Ji defines ''fangcheng'': \"''Fang'' means [on the] left and right. ''Cheng'' means terms of a ratio. Terms of a ratio [on the] left and right, combining together numerous objects, therefore [it] is called a \"rectangular array\".\" \n#[[Yang Hui]]'s \"Nine Chapters on the Mathematical Arts with Detailed Explanations\" defines ''cheng'' as a general term for measuring weight, height, and length. Detailed Explanations states:  What is called \"rectangular\" (''fang'') is the shape of the numbers; \"measure\" (''cheng'') is the general term for [all forms of] measurement, also a method for equating weights, lengths, and volumes, especially referring to measuring clearly and distinctly the greater and lesser.\n\nSince the end of the 19th century, in Chinese mathematical literature the term ''fangcheng'' has been used to denote an \"equation.\" However, as already been noted, the traditional meaning of the term is very different from \"equation.\"\n\n==Contents of the chapter titled ''Fangcheng''==\n\nThe eighth chapter titled ''Fangcheng'' of the ''Nine Chapters'' book contains 18 problems. (There are a total of 288 problems in the whole book.) Each of these 18 problems reduces to a problem of solving a system of simultaneous linear equations. Except for one problem, namely Problem 13, all the problems are determinate in the sense that the number of unknowns is same as the number of equations.  There are problems involving 2, 3, 4 and 5 unknowns. The table below shows how many unknowns are there in the various problems:\n\n<center>\n'''Table showing the number of unknowns and number of equations <br> in the various problems in Chapter 8 of ''Nine Chapters'' '''\n{| class=\"wikitable\" style=\"Text-align: center\"\n|-\n! Number of unknowns<br> in the problem !! Number of equations<br> in the problem !! Serial numbers of problems !! Number of problems || Determinacy\n|-\n| 2 || 2 ||2, 4, 5, 6, 7, 9, 10, 11  || 8 ||Determinate\n|-\n| 3 || 3 || 1, 3, 8, 12, 15, 16 || 6 ||Determinate\n|-\n| 4 || 4 || 14, 17 || 2 ||Determinate\n|-\n| 5 || 5 || 18 || 1 ||Determinate\n|-\n| 6 || 5 || 13 || 1 ||[[Indeterminate system|Indeterminate]]\n|-\n|  || || Total || 18\n|}\n</center>\n\nThe presentations of all the 18  problems  (except Problem 1 and Problem 3) follow a common pattern:\n\n*First the problem is stated.\n*Then the answer to the problem is given.\n*Finally the method of obtaining the answer is indicated.\n\n===On Problem 1 ===\n\n* Problem:\n** 3 bundles of high-quality rice straws, 2 bundles of mid-quality rice straws and 1 bundle of low-quality rice straw produce 39 units of rice\n** 2 bundles of high-quality rice straws, 3 bundles of mid-quality rice straws and 1 bundle of low-quality rice straw produce 34 units of rice\n** 1 bundles of high-quality rice straw, 2 bundles of mid-quality rice straws and 3 bundle of low-quality rice straws produce 26 units of rice\n** Question: how many units of rice can high, mid and low quality rice straw produce respectively?\n\n* Solution:\n** High-quality rice straw each produces 9 + 1/4 units of rice\n** Mid-quality rice straw each produces 4 + 1/4 units of rice\n** Low-quality rice straw each produces 2 + 3/4 units of rice\n\nThe presentation of Problem 1 contains a description (not a crisp indication)  of the procedure for obtaining the solution. The procedure has been referred to as ''fangcheng shu'', which means \"''fangcheng'' procedure.\" The remaining problems all give the instruction \"follow the ''fangcheng''\" procedure sometimes followed by the instruction to use the \"procedure for positive and negative numbers\".\n\n===On Problem 3===\nThere is also a special procedure, called \"procedure for positive and negative numbers\" (''zheng fu shu'') for handling negative numbers. This procedure is explained as part of the method for solving Problem 3.\n\n===On Problem 13=== \nIn the collection of these 18 problems Problem 13 is very special. In it there are 6 unknowns but only 5 equations and so Problem 13 is indeterminate and does not have a unique solution.  This is the earliest known reference to a system of linear equations in which the number of unknowns exceeds the number of equations. As per a suggestion of Jean-Claude Martzloff, a historian of Chinese mathematics, Roger Hart has named this problem \"the well problem.\"\n\n==References==\n{{reflist}}\n\n==Further reading==\n*{{cite journal|author=Christine Andrews-Larson|title=Roots of Linear Algebra: An Historical Exploration of Linear Systems|journal=PRIMUS|date=2015|volume=25|issue=6|pages=507–528|doi=10.1080/10511970.2015.1027975}}\n*{{cite book|author=Kangshen Shen|author2=John N. Crossley|author3=Anthony Wah-Cheung Lun, Hui Liu|title=The Nine Chapters on the Mathematical Art: Companion and Commentary|date=1999|publisher=Oxford University Press|isbn=9780198539360|pages=386–440|url=https://books.google.com/?id=eiTJHRGTG6YC&pg=PA1&lpg=PA1&dq=nine+chapters+of+mathematical+art#v=onepage&q=nine%20chapters%20of%20mathematical%20art&f=false|accessdate=7 December 2016}}\n*For an investigation into the possibility of teaching ''fangcheng'' to European children: {{cite journal|author=Cecília Costa|title=Potentialities on the Western Education of the Ancient Chinese Method to Solve Linear Systems of Equations|journal=Applied Mathematical Sciences|date=2014|volume=8|issue=36|pages=1789–1798|url=http://www.m-hikari.com/ams/ams-2014/ams-33-36-2014/costaAMS33-36-2014.pdf|accessdate=15 December 2016|doi=10.12988/ams.2014.42118}}\n\n[[Category:Chinese mathematics]]\n[[Category:Linear algebra]]\n[[Category:Numerical linear algebra]]\n[[Category:Han dynasty texts]]"
    },
    {
      "title": "Folded spectrum method",
      "url": "https://en.wikipedia.org/wiki/Folded_spectrum_method",
      "text": "In [[mathematics]], the '''folded spectrum method (FSM)''' is an [[iterative method]] for solving large [[eigenvalue]] problems.\nHere you always find a vector with an eigenvalue close to a search-value <math>\\varepsilon</math>.  This means you can get a vector <math>\\Psi</math> in the middle of the spectrum without solving the matrix.\n\n<math>\\Psi_{i+1}= \\Psi_i-\\alpha( H- \\varepsilon \\mathbf{1} )^2 \\Psi_i</math>, with <math>0<\\alpha^{\\,}<1</math> and <math>\\mathbf{1}</math> the [[Identity matrix]].\n\nIn contrast to the [[Conjugate gradient method]], here the gradient calculates by twice multiplying matrix <math>H:\\;G\\sim H\\rightarrow G\\sim H^2.</math>\n\n== Literature ==\n* J. K. L. MacDonald, Phys. Rev. 46, 828 - 828 (1934)\n* W. Wang and A. Zunger, J. Phys. Chem. 98, 2158 (1994)\n* W. Wang and A. Zunger, J. Chem. Phys. 100, 2394 (1994)\n* [http://www.sst.nrel.gov/topics/nano/escan.html http://www.sst.nrel.gov/topics/nano/escan.html]\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]\n\n\n{{Linear-algebra-stub}}"
    },
    {
      "title": "Frontal solver",
      "url": "https://en.wikipedia.org/wiki/Frontal_solver",
      "text": "A '''frontal solver''', conceived by [[Bruce Irons (engineer)|Bruce Irons]],<ref>{{cite journal |title=A frontal solution program for finite element analysis |last=Irons |first=Bruce M. | journal=International Journal for Numerical Methods in Engineering |volume=2 | year=1970 |issue=January/March |pages=5–32 |doi=10.1002/nme.1620020104}}</ref> is an approach to solving [[sparse matrix|sparse linear systems]] which is used extensively in [[finite element analysis]].<ref>Renaud Sizaire, keyFE2 User Manual, 2005, Sec. I.4.2 Solving_linear_system [http://users.skynet.be/keyFE2/manual/I_4_2_Solving_linear_system.html online]  {{webarchive |url=https://web.archive.org/web/20061008212805/http://users.skynet.be/keyFE2/manual/I_4_2_Solving_linear_system.html |date=October 8, 2006 }}</ref> It is a variant of [[Gauss elimination]] that automatically avoids a large number of operations involving zero terms.<ref>Hayrettin Kardestuncer, Ed. ''Finite Element Handbook''.</ref>\n\nA frontal solver builds a [[LU decomposition|LU]] or [[Cholesky decomposition]] of a sparse matrix given as the assembly of element matrices by assembling the matrix and eliminating equations only on a subset of elements at a time. However, elements can be stored in-core in a clique sequence as recently proposed by Areias.<ref>P. Areias, T. Rabczuk, J.I. Barbosa, The extended unsymmetric frontal solution for multiple-point constraints, Engineering Computations, v.31 n.7, pp. 1582–1607, Nov. 2014 [https://dx.doi.org/10.1108/EC-10-2013-0263]</ref> This subset is called the front and it is essentially the transition region between the part of the system already finished and the part not touched yet. The whole sparse matrix is never created explicitly. Only parts of the matrix are assembled as they enter the front. Processing the front involves [[dense matrix]] operations, which use the CPU efficiently. In a typical implementation, only the front is in [[computer memory|memory]], while the factors in the decomposition are written into [[computer file|files]]. The element matrices are read from files or created as needed and discarded.\n\nA '''multifrontal solver''' of [[Iain S. Duff|Duff]] and [[John K. Reid|Reid]]<ref>I. S. Duff , J. K. Reid, The Multifrontal Solution of Indefinite Sparse Symmetric Linear, ACM Transactions on Mathematical Software (TOMS), v.9 n.3, p.302-325, Sept. 1983  [https://dx.doi.org/10.1145/356044.356047 DOI 10.1145/356044.356047]</ref> is an improvement of the frontal solver that uses several independent fronts at the same time. The fronts can be worked on by different [[CPU|processors]], which enables [[parallel computing]].\n\nSee<ref>Iain S Duff , Albert M Erisman , John K Reid, Direct methods for sparse matrices, Oxford University Press, Inc., New York, NY, 1986</ref> for a monograph exposition.\n\n==See also==\n* [[MUMPS (software)|MUMPS]]\n* [[Skyline matrix]]\n* [[Banded matrix]]\n\n==References==\n<References/>\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]\n[[Category:Numerical software]]\n\n\n{{engineering-stub}}\n{{science-software-stub}}\n{{mathapplied-stub}}"
    },
    {
      "title": "Generalized minimal residual method",
      "url": "https://en.wikipedia.org/wiki/Generalized_minimal_residual_method",
      "text": "In mathematics, the '''generalized minimal residual method (GMRES)''' is an [[iterative method]] for the [[numerical analysis|numerical]] solution of a nonsymmetric [[system of linear equations]]. The method approximates the solution by the vector in a [[Krylov subspace]] with minimal [[residual (numerical analysis)|residual]]. The [[Arnoldi iteration]] is used to find this vector.\n\nThe GMRES method was developed by [[Yousef Saad]] and Martin H. Schultz in 1986.<ref>Y. Saad and M.H. Schultz</ref>\nGMRES is a generalization of the [[MINRES]] method developed by Chris Paige and Michael Saunders in 1975. GMRES also is a special case of the [[DIIS]] method developed by Peter Pulay in 1980. DIIS is also applicable to non-linear systems.\n\n== The method ==\n\nDenote the [[Euclidean norm]] of any vector  '''v ''' by <math>\\|v\\|</math>. Denote the (square) system of linear equations to be solved by\n:<math> Ax = b. \\, </math>\nThe matrix  ''A'' is assumed to be [[invertible matrix|invertible]] of size ''m''-by-''m''. Furthermore, it is assumed that '''b''' is normalized, i.e., that <math>\\|b\\| = 1</math>.\n\nThe ''n''-th [[Krylov sequence|Krylov subspace]] for this problem is\n:<math> K_n = K_n(A,b) = \\operatorname{span} \\, \\{ b, Ab, A^2b, \\ldots, A^{n-1}b \\}. \\, </math>\nGMRES approximates the exact solution of <math>Ax = b</math> by the vector <math>x_n \\in K_n </math> that minimizes the Euclidean norm of the [[Residual (numerical analysis)|residual]] <math>r_n= Ax_n-b</math>.\n\nThe vectors <math>b,Ab,\\ldots A^{n-1}b</math> might be close to [[linear independence|linearly dependent]], so instead of this basis, the [[Arnoldi iteration]] is used to find orthonormal vectors <math> q_1, q_2, \\ldots, q_n \\, </math> which form a basis for <math>K_n</math>. Hence, the vector <math>x_n \\in K_n </math> can be written as <math>x_n = Q_n y_n </math> with <math> y_n \\in \\mathbb{R}^n </math>, where <math> Q_n </math> is the ''m''-by-''n'' matrix formed by <math> q_1,\\ldots,q_n </math>.\n\nThe Arnoldi process also produces an (<math>n+1</math>)-by-<math>n</math> upper [[Hessenberg matrix]] <math>\\tilde{H}_n</math> with\n:<math> AQ_n = Q_{n+1} \\tilde{H}_n. \\, </math>\nBecause columns of <math>Q_n</math> are orthonormal, we have\n:<math> \\| Ax_n - b \\| = \\| \\tilde{H}_n y_n - Q_{n+1}^T b \\| =  \\| \\tilde{H}_ny_n - \\beta e_1 \\|, \\, </math>\nwhere\n:<math> e_1 = (1,0,0,\\ldots,0)^T \\, </math>\nis the first vector in the [[standard basis]] of <math>\\mathbb{R}^{n+1} </math>, and \n:<math> \\beta = \\|b-Ax_0\\| \\, ,</math> \n<math>x_0</math> being the first trial vector (usually zero). Hence, <math>x_n</math> can be found by minimizing the Euclidean norm of the residual\n:<math> r_n = \\tilde{H}_n y_n - \\beta e_1. </math>\nThis is a [[linear least squares (mathematics)|linear least squares]] problem of size ''n''.\n\nThis yields the GMRES method. On the <math>n</math>-th iteration:\n# calculate <math> q_n </math> with the Arnoldi method;\n# find the <math> y_n </math> which minimizes <math>\\|r_n\\|</math>;\n# compute <math> x_n = Q_n y_n </math>;\n# repeat if the residual is not yet small enough.\nAt every iteration, a matrix-vector product <math>A q_n </math> must be computed. This costs about <math>2m^2 </math> [[floating point|floating-point operations]] for general dense matrices of size <math>m</math>, but the cost can decrease to <math>O(m)</math> for [[sparse matrix|sparse matrices]]. In addition to the matrix-vector product, <math>O(nm)</math> floating-point operations must be computed at the ''n'' -th iteration.\n\n== Convergence ==\n\nThe ''n''th iterate minimizes the residual in the Krylov subspace ''K''<sub>''n''</sub>. Since every subspace is contained in the next subspace, the residual does not increase. After ''m'' iterations, where ''m'' is the size of the matrix ''A'', the Krylov space ''K''<sub>''m''</sub> is the whole of '''R'''<sup>''m''</sup> and hence the GMRES method arrives at the exact solution. However, the idea is that after a small number of iterations (relative to ''m''), the vector ''x''<sub>''n''</sub> is already a good approximation to the \nexact solution.\n\nThis does not happen in general. Indeed, a theorem of Greenbaum, Pták and Strakoš states that for every nonincreasing sequence ''a''<sub>1</sub>, …, ''a''<sub>''m''&minus;1</sub>, ''a''<sub>''m''</sub> = 0, one can find a matrix ''A'' such that the ||''r''<sub>''n''</sub>|| = ''a''<sub>''n''</sub> for all ''n'', where ''r''<sub>''n''</sub> is the residual defined above. In particular, it is possible to find a matrix for which the residual stays constant for ''m''&nbsp;&minus;&nbsp;1 iterations, and only drops to zero at the last iteration.\n\nIn practice, though, GMRES often performs well. This can be proven in specific situations. If the symmetric part of ''A'', that is <math>(A^T + A)/2</math>, is [[positive-definite matrix|positive definite]], then\n:<math> \\|r_n\\| \\leq \\left( 1-\\frac{\\lambda_{\\min}^2(1/2(A^T + A))}{ \\lambda_{\\max}(A^T A)} \\right)^{n/2} \\|r_0\\|, </math>\nwhere <math>\\lambda_{\\mathrm{min}}(M)</math> and <math>\\lambda_{\\mathrm{max}}(M)</math> denote the smallest and largest [[eigenvalue]] of the matrix <math>M</math>, respectively.<ref>Eisenstat, Elman & Schultz, Thm 3.3. NB all results for GCR also hold for GMRES, cf. Saad & Schultz</ref>\n\nIf ''A'' is [[symmetric matrix|symmetric]] and positive definite, then we even have\n:<math> \\|r_n\\| \\leq \\left( \\frac{\\kappa_2(A)^2-1}{\\kappa_2(A)^2} \\right)^{n/2} \\|r_0\\|. </math>\nwhere <math>\\kappa_2(A)</math> denotes the [[condition number]] of ''A'' in the Euclidean norm.\n\nIn the general case, where ''A'' is not positive definite, we have\n:<math> \\frac{\\|r_n\\|}{\\|b\\|} \\le \\inf_{p \\in P_n} \\|p(A)\\| \\le \\kappa_2(V) \\inf_{p \\in P_n} \\max_{\\lambda \\in \\sigma(A)} |p(\\lambda)|, \\, </math>\nwhere ''P''<sub>''n''</sub> denotes the set of polynomials of degree at most ''n'' with ''p''(0) = 1, ''V'' is the matrix appearing in the [[spectral decomposition]] of ''A'', and ''σ''(''A'') is the [[spectrum of a matrix|spectrum]] of ''A''. Roughly speaking, this says that fast convergence occurs when the eigenvalues of ''A'' are clustered away from the origin and ''A'' is not too far from [[normal matrix|normality]].<ref>Trefethen & Bau, Thm 35.2</ref>\n\nAll these inequalities bound only the residuals instead of the actual error, that is, the distance between the current iterate ''x''<sub>''n''</sub> and the exact solution.\n\n== Extensions of the method ==\n\nLike other iterative methods, GMRES is usually combined with a [[preconditioning]] method in order to speed up convergence.\n\nThe cost of the iterations grow as O(''n''<sup>2</sup>), where ''n'' is the iteration number. Therefore, the method is sometimes restarted after a number, say ''k'', of iterations, with ''x''<sub>''k''</sub> as initial guess. The resulting method is called GMRES(''k'') or Restarted GMRES. This methods suffers from stagnation in convergence as the restarted subspace is often close to the earlier subspace.\n\nThe shortcomings of GMRES and restarted GMRES are addressed by the recycling of Krylov subspace in the GCRO type methods such as GCROT and GCRODR.<ref>{{Cite journal|doi=10.1016/j.jcp.2015.09.040|title=Recycling Krylov subspaces for CFD applications and a new hybrid recycling solver|year=2015|last1=Amritkar|first1=Amit|last2=de Sturler|first2=Eric|last3=Świrydowicz|first3=Katarzyna|last4=Tafti|first4=Danesh|last5=Ahuja|first5=Kapil|journal=Journal of Computational Physics|volume=303|page=222|arxiv=1501.03358|bibcode=2015JCoPh.303..222A}}</ref>\nRecycling of Krylov subspaces in GMRES can also speed up convergence when sequences of linear systems need to be solved.<ref>{{Cite thesis |type=Ph.D. |doi=10.14279/depositonce-4147 |title=Recycling Krylov subspace methods for sequences of linear systems |year=2014 |last=Gaul |first=André |publisher=TU Berlin}}</ref>\n\n== Comparison with other solvers ==\n\nThe Arnoldi iteration reduces to the [[Lanczos iteration]] for symmetric matrices. The corresponding [[Krylov subspace]] method is the minimal residual method (MinRes) of Paige and Saunders. Unlike the unsymmetric case, the MinRes method is given by a three-term [[recurrence relation]]. It can be shown that there is no Krylov subspace method for general matrices, which is given by a short recurrence relation and yet minimizes the norms of the residuals, as GMRES does.\n\nAnother class of methods builds on the [[unsymmetric Lanczos iteration]], in particular the [[Biconjugate gradient method|BiCG method]]. These use a three-term recurrence relation, but they do not attain the minimum residual, and hence the residual does not decrease monotonically for these methods. Convergence is not even guaranteed.\n\nThe third class is formed by methods like [[Conjugate gradient squared method|CGS]] and [[Biconjugate gradient stabilized method|BiCGSTAB]]. These also work with a three-term recurrence relation (hence, without optimality) and they can even terminate prematurely without achieving convergence. The idea behind these methods is to choose the generating polynomials of the iteration sequence suitably.\n\nNone of these three classes is the best for all matrices; there are always examples in which one class outperforms the other. Therefore, multiple solvers are tried in practice to see which one is the best for a given problem.\n\n== Solving the least squares problem ==\n\nOne part of the GMRES method is to find the vector <math>y_n</math> which minimizes\n:<math> \\| \\tilde{H}_n y_n - \\beta e_1 \\|. \\, </math>\nNote that <math>\\tilde{H}_n</math> is an (''n''&nbsp;+&nbsp;1)-by-''n'' matrix, hence it gives an over-constrained linear system of ''n''+1 equations for ''n'' unknowns.\n\nThe minimum can be computed using a [[QR decomposition]]: find an (''n''&nbsp;+&nbsp;1)-by-(''n''&nbsp;+&nbsp;1) [[orthogonal matrix]] &Omega;<sub>''n''</sub> and an (''n''&nbsp;+&nbsp;1)-by-''n'' upper [[triangular matrix]] <math>\\tilde{R}_n</math> such that\n:<math> \\Omega_n \\tilde{H}_n = \\tilde{R}_n. </math>\nThe triangular matrix has one more row than it has columns, so its bottom row consists of zero. Hence, it can be decomposed as\n:<math> \\tilde{R}_n = \\begin{bmatrix} R_n \\\\ 0 \\end{bmatrix}, </math>\nwhere <math>R_n</math> is an ''n''-by-''n'' (thus square) triangular matrix.\n\nThe QR decomposition can be updated cheaply from one iteration to the next, because the Hessenberg matrices differ only by a row of zeros and a column:\n:<math>\\tilde{H}_{n+1} = \\begin{bmatrix} \\tilde{H}_n & h_{n+1} \\\\ 0 & h_{n+2,n+1} \\end{bmatrix}, </math> \nwhere ''h''<sub>''n+1''</sub> = (''h''<sub>1,''n+1''</sub>, &hellip;, ''h''<sub>''n+1,n+1''</sub>)<sup>T</sup>. This implies that premultiplying the Hessenberg matrix with &Omega;<sub>''n''</sub>, augmented with zeroes and a row with multiplicative identity, yields almost a triangular matrix:\n:<math> \\begin{bmatrix} \\Omega_n & 0 \\\\ 0 & 1 \\end{bmatrix} \\tilde{H}_{n+1} = \\begin{bmatrix} R_n & r_{n+1} \\\\ 0 & \\rho \\\\ 0 & \\sigma \\end{bmatrix} </math>\nThis would be triangular if &sigma; is zero. To remedy this, one needs the [[Givens rotation]]\n:<math> G_n = \\begin{bmatrix} I_{n} & 0 & 0 \\\\ 0 & c_n & s_n \\\\ 0 & -s_n & c_n \\end{bmatrix} </math>\nwhere\n:<math> c_n = \\frac{\\rho}{\\sqrt{\\rho^2+\\sigma^2}} \\quad\\mbox{and}\\quad s_n = \\frac{\\sigma}{\\sqrt{\\rho^2+\\sigma^2}}. </math>\nWith this Givens rotation, we form\n:<math> \\Omega_{n+1} = G_n \\begin{bmatrix} \\Omega_n & 0 \\\\ 0 & 1 \\end{bmatrix}. </math>\nIndeed,\n:<math> \\Omega_{n+1} \\tilde{H}_{n+1} = \\begin{bmatrix} R_n & r_{n+1} \\\\ 0 & r_{n+1,n+1} \\\\ 0 & 0 \\end{bmatrix} \\quad\\text{with}\\quad r_{n+1,n+1} = \\sqrt{\\rho^2+\\sigma^2} </math>\nis a triangular matrix.\n\nGiven the QR decomposition, the minimization problem is easily solved by noting that\n:<math> \\| \\tilde{H}_n y_n - \\beta e_1 \\| = \\| \\Omega_n (\\tilde{H}_n y_n - \\beta e_1) \\| = \\| \\tilde{R}_n y_n - \\beta \\Omega_n e_1 \\|. </math>\nDenoting the vector <math>\\beta\\Omega_ne_1</math> by\n:<math> \\tilde{g}_n = \\begin{bmatrix} g_n \\\\ \\gamma_n \\end{bmatrix} </math>\nwith ''g''<sub>''n''</sub> &isin; '''R'''<sup>''n''</sup> and &gamma;<sub>''n''</sub> &isin; '''R''', this is\n:<math> \\| \\tilde{H}_n y_n - \\beta e_1 \\| = \\| \\tilde{R}_n y_n - \\beta \\Omega_n e_1 \\| = \\left\\| \\begin{bmatrix} R_n \\\\ 0 \\end{bmatrix} y_n - \\begin{bmatrix} g_n \\\\ \\gamma_n \\end{bmatrix} \\right\\|. </math>\nThe vector ''y'' that minimizes this expression is given by \n:<math> y_n = R_n^{-1} g_n. </math>\nAgain, the vectors <math>g_n</math> are easy to update.<ref>Stoer and Bulirsch, §8.7.2</ref>\n\n==Example code==\n===Regular GMRES (MATLAB / GNU Octave)===\n\n<source lang=\"matlab\">\n\nfunction [x, e] = gmres( A, b, x, max_iterations, threshold)\n  n = length(A);\n  m = max_iterations;\n  \n  %use x as the initial vector\n  r=b-A*x;\n\n  b_norm = norm(b);\n  error = norm(r)/b_norm;\n\n  %initialize the 1D vectors\n  sn = zeros(m,1);\n  cs = zeros(m,1);\n  e1 = zeros(n,1);\n  e1(1) = 1;\n  e=[error];\n  r_norm=norm(r);\n  Q(:,1) = r/r_norm;\n  beta = r_norm*e1;\n  for k = 1:m                                   \n    \n    %run arnoldi\n    [H(1:k+1,k) Q(:,k+1)] = arnoldi(A, Q, k);\n    \n    %eliminate the last element in H ith row and update the rotation matrix\n    [H(1:k+1,k) cs(k) sn(k)] = apply_givens_rotation(H(1:k+1,k), cs, sn, k);\n    \n    %update the residual vector\n    beta(k+1) = -sn(k)*beta(k);\n    beta(k)   = cs(k)*beta(k);\n    error  = abs(beta(k+1)) / b_norm;\n    \n    %save the error\n    e=[e; error];\n    \n    if ( error <= threshold)\n      break;\n    end\n  end\n\n  %calculate the result\n  y = H(1:k,1:k) \\ beta(1:k);\n  x = x + Q(:,1:k)*y; \nend\n\n%----------------------------------------------------%\n%                  Arnoldi Function                  %\n%----------------------------------------------------%\nfunction [h, q] = arnoldi(A, Q, k)\n  q = A*Q(:,k);\n  for i = 1:k\n    h(i)= q'*Q(:,i);\n    q = q - h(i)*Q(:,i);\n  end\n  h(k+1) = norm(q);\n  q = q / h(k+1);\nend\n\n%---------------------------------------------------------------------%\n%                  Applying Givens Rotation to H col                  %\n%---------------------------------------------------------------------%\nfunction [h, cs_k, sn_k] = apply_givens_rotation(h, cs, sn, k)\n  %apply for ith column\n  for i = 1:k-1                              \n    temp   =  cs(i)*h(i) + sn(i)*h(i+1);\n    h(i+1) = -sn(i)*h(i) + cs(i)*h(i+1);\n    h(i)   = temp;\n  end\n  \n  %update the next sin cos values for rotation\n  [cs_k sn_k] = givens_rotation(h(k), h(k+1));\n  \n  %eliminate H(i+1,i)\n  h(k) = cs_k*h(k) + sn_k*h(k+1);\n  h(k+1) = 0.0;\nend\n\n%%----Calculate the Given rotation matrix----%%\nfunction [cs, sn] = givens_rotation(v1, v2)\n  if (v1==0)\n    cs = 0;\n    sn = 1;\n  else\n    t=sqrt(v1^2+v2^2);\n    cs = abs(v1) / t;\n    sn = cs * v2 / v1;\n  end\nend  \n\n</source>\n\n== See also==\n* [[Biconjugate gradient method]]\n\n== References ==\n<references/>\n\n== Notes ==\n* A. Meister, ''Numerik linearer Gleichungssysteme'', 2nd edition, Vieweg 2005, {{ISBN|978-3-528-13135-7}}.\n* Y. Saad, ''Iterative Methods for Sparse Linear Systems'', 2nd edition, [[Society for Industrial and Applied Mathematics]], 2003. {{ISBN|978-0-89871-534-7}}.\n* Y. Saad and M.H. Schultz, \"GMRES: A generalized minimal residual algorithm for solving nonsymmetric linear systems\", ''SIAM J. Sci. Stat. Comput.'', '''7''':856–869, 1986. {{doi|10.1137/0907058}}.\n* S. C. Eisenstat, H.C. Elman and M.H. Schultz, \"Variational iterative methods for nonsymmetric systems of linear equations\", ''SIAM Journal on Numerical Analysis'', 20(2), 345–357, 1983. \n* J. Stoer and R. Bulirsch, ''Introduction to numerical analysis'', 3rd edition, Springer, New York, 2002. {{ISBN|978-0-387-95452-3}}. \n* Lloyd N. Trefethen and David Bau, III, ''Numerical Linear Algebra'', Society for Industrial and Applied Mathematics, 1997. {{ISBN|978-0-89871-361-9}}.\n* [http://www.netlib.org/linalg/html_templates/node29.html#SECTION00734000000000000000|J. Dongarra et al. , ''Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods''], 2nd Edition, SIAM, Philadelphia, 1994\n* Amritkar, Amit; de Sturler, Eric; Świrydowicz, Katarzyna; Tafti, Danesh; Ahuja, Kapil (2015). \"Recycling Krylov subspaces for CFD applications and a new hybrid recycling solver\". Journal of Computational Physics 303: 222. doi:10.1016/j.jcp.2015.09.040\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]\n[[Category:Articles with example pseudocode]]"
    },
    {
      "title": "GotoBLAS",
      "url": "https://en.wikipedia.org/wiki/GotoBLAS",
      "text": "{{Infobox software\n| name                   = GotoBLAS\n| logo                   = \n| screenshot             =\n| caption                =\n| collapsible            =\n| author                 = [[Kazushige Goto]]\n| released               =\n| latest release version = 2-1.13\n| latest release date    = {{release date and age|2010|2|5}}\n| latest preview version =\n| latest preview date    =\n| programming language   =\n| operating system       =\n| platform               =\n| size                   =\n| language               =\n| status                 = Unmaintained\n| genre                  = Linear algebra library; implementation of [[Basic Linear Algebra Subprograms|BLAS]]\n| license                = [[BSD License]]\n| website                = {{url|http://www.tacc.utexas.edu/tacc-software/gotoblas2}}\n}}\n\nIn [[Computational science|scientific computing]], '''GotoBLAS''' and '''GotoBLAS2''' are [[Open source software|open source]] implementations of the [[BLAS]] (Basic Linear Algebra Subprograms) [[Application programming interface|API]] with many hand-crafted optimizations for specific [[Central processing unit|processor]] types. GotoBLAS was developed by [[Kazushige Goto]] at the [[Texas Advanced Computing Center]]. {{As of|2003}}, it was used in seven of the world's ten fastest supercomputers.<ref name=\"nyt\"/>\n\nGotoBLAS remains available, but development ceased with a final version touting optimal performance on Intel's [[Nehalem (microarchitecture)|Nehalem]] architecture (contemporary in 2008).<ref>{{cite web |title=GotoBlas2 |url=http://www.tacc.utexas.edu/tacc-software/gotoblas2 |accessdate=28 August 2013}}</ref>\n[[OpenBLAS]] is an actively maintained fork of GotoBLAS, developed at the Lab of Parallel Software and Computational Science, [[Institute of Software, Chinese Academy of Sciences|ISCAS]].\n\nGotoBLAS was written by Goto during his [[sabbatical]] leave from the [[Japan Patent Office]] in 2002. It was initially optimized for the [[Pentium 4]] processor and managed to immediately boost the performance of a [[supercomputer]] based on that CPU from 1.5 [[TFLOPS]] to 2 TFLOPS.<ref name=\"nyt\">{{cite newspaper |title=Writing the fastest code, by hand, for fun |newspaper=New York Times |author=John Markoff |date=28 November 2005 |url=https://www.nytimes.com/2005/11/28/technology/28super.html}}</ref> {{As of|2005}}, the library was available at no cost for noncommercial use.<ref name=\"nyt\"/> A later open source version was released under the terms of the [[BSD license]].\n\nGotoBLAS's [[Matrix multiplication algorithm|matrix-matrix multiplication routine]], called GEMM in BLAS terms, is highly tuned for the [[x86]] and [[x86-64|AMD64]] processor architectures by means of handcrafted [[assembly language|assembly code]].<ref name=\"anatomy\"/> It follows a similar decomposition into smaller \"kernel\" routines that other BLAS implementations use, but where earlier implementations streamed data from the [[CPU cache|L1 processor cache]], GotoBLAS uses the [[L2 cache]].<ref name=\"anatomy\">{{cite journal | last1=Goto | first1=Kazushige | last2=van de Geijn | first2 = Robert A. | title = Anatomy of High-Performance Matrix Multiplication | year = 2008 | journal = [[ACM Transactions on Mathematical Software]] | volume = 34 | issue = 3 | pages = Article 12, 25 pages | doi=10.1145/1356052.1356053| citeseerx=10.1.1.111.3873 }}</ref>\nThe kernel used for GEMM is a routine called GEBP, for \"General block-times-panel multiply\",<ref name=\"level3\"/> which was experimentally found to be \"inherently superior\" over several other kernels that were considered in the design.<ref name=\"anatomy\"/>\n\nSeveral other BLAS routines are, as is customary in BLAS libraries, implemented in terms of GEMM.<ref name=\"level3\">{{cite journal |last1=Goto | first1=Kazushige | last2=van de Geijn | first2 = Robert A. |title=High-performance implementation of the level-3 BLAS |journal=ACM Transactions on Mathematical Software |volume=35 |issue=1 | pages=1–14 |year=2008 | doi = 10.1145/1377603.1377607}}</ref>\n\n==See also==\n* [[Automatically Tuned Linear Algebra Software]] (ATLAS)\n* [[Math Kernel Library|Intel Math Kernel Library]] (MKL)\n\n==References==\n{{reflist|2}}\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]\n[[Category:Numerical software]]\n[[Category:Software using the BSD license]]"
    },
    {
      "title": "Householder operator",
      "url": "https://en.wikipedia.org/wiki/Householder_operator",
      "text": "In [[Linear algebra|linear algebra]], the '''Householder [[Operator (mathematics)|operator]] ''' is defined as follows. Let <math> V\\, </math> be a finite dimensional [[inner product space]] with [[inner product]] <math> \\langle \\cdot, \\cdot \\rangle </math> and [[unit vector]] <math> u\\in V</math>. Then\n:<math> H_u : V \\to V\\,</math> \nis defined by\n:<math> H_u(x) = x - 2\\,\\langle x,u \\rangle\\,u\\,</math>\n\nThis operator reflects the vector <math>x</math> across a plane given by the normal vector <math>u</math>.<ref>{{cite book|title=Methods of Applied Mathematics for Engineers and Scientist|publisher=Cambridge University Press|isbn=9781107244467|pages=Section E.4.11|url=https://books.google.com/books?id=nQIlAAAAQBAJ}}</ref>\n\nIt is also common to choose a non-unit vector <math>q \\in V</math>, and normalize it directly in the Householder operator's expression\n:<math>H_q \\left ( x \\right ) = x - 2\\, \\frac{\\langle x, q \\rangle}{\\langle q, q \\rangle}\\, q \\,</math>\n\n== Properties ==\nThe Householder operator verifies the following properties:\n\n* it is [[Linear operator|linear]] ; if <math>V</math> is a vector space over a field <math>K</math>, then\n:<math>\\forall \\left ( \\lambda, \\mu \\right ) \\in K^2, \\, \\forall \\left ( x, y \\right ) \\in V^2, \\,  H_q \\left ( \\lambda x + \\mu y \\right ) = \\lambda \\ H_q \\left ( x \\right ) + \\mu \\ H_q \\left ( y \\right )</math>\n* [[self-adjoint]]\n* if <math>K = \\mathbb{R}</math>, it is [[Orthogonal matrix|orthogonal]] ; otherwise, if <math>K = \\mathbb{C}</math> it is [[Unitary matrix|unitary]].\n\n== Special cases ==\n\nOver a real or complex [[vector space]], the Householder operator is also known as the [[Householder transformation]].\n\n==References==\n{{reflist}}\n\n[[Category:Numerical linear algebra]]\n\n\n{{Linear-algebra-stub}}"
    },
    {
      "title": "Householder transformation",
      "url": "https://en.wikipedia.org/wiki/Householder_transformation",
      "text": "In [[linear algebra]], a '''Householder transformation''' (also known as a '''Householder reflection''' or '''elementary reflector''') is a [[linear transformation]] that describes a [[reflection (mathematics)|reflection]] about a [[plane (mathematics)|plane]] or [[hyperplane]] containing the origin. The Householder transformation was introduced in 1958 by [[Alston Scott Householder]].<ref>{{cite journal\n |first=A. S. |last=Householder |authorlink=Alston Scott Householder\n |title=Unitary Triangularization of a Nonsymmetric Matrix\n |journal=[[Journal of the ACM]]\n |volume=5 |issue=4 |year=1958 |pages=339&ndash;342\n |doi=10.1145/320941.320947 |mr=0111128\n}}</ref>\n\nIts analogue over general [[inner product spaces]] is the [[Householder operator]].\n\n==Definition==\n\n===Transformation===\n\nThe reflection hyperplane can be defined by a [[unit vector]] <math display=\"inline\">v</math> (a vector with length <math display=\"inline\">1</math>) which is [[orthogonal]] to the hyperplane. The reflection of a [[Point (geometry)|point]] <math display=\"inline\">x</math> about this hyperplane is the [[linear transformation]]:\n\n: <math>x - 2\\langle x, v\\rangle v = x - 2v\\left(v^\\textsf{H} x\\right), </math>\n\nwhere <math display=\"inline\">v</math> is given as a column unit vector with [[Hermitian transpose]] <math display=\"inline\">v^\\textsf{H}</math>.\n\n===Householder matrix===\n\nThe matrix constructed from this transformation can be expressed in terms of an [[outer product]] as:\n\n: <math>P = I - 2(v \\otimes v) = I - 2vv^\\textsf{H}</math>\n\nis known as the '''Householder matrix''', where <math display=\"inline\">I</math> is the [[identity matrix]]\n\n====Properties====\n\nThe Householder matrix has the following properties:\n* it is [[Hermitian matrix|Hermitian]]: <math display=\"inline\">P = P^\\textsf{H}</math>,\n* it is [[unitary matrix|unitary]]: <math display=\"inline\">P^{-1} = P^\\textsf{H}</math>,\n* hence it is [[involutory matrix|involutory]]: <math display=\"inline\">P^2 = I </math>.\n* A Householder matrix has eigenvalues <math display=\"inline\">\\pm 1</math>.  To see this, notice that if <math display=\"inline\">u</math> is orthogonal to the vector <math display=\"inline\">v</math> which was used to create the reflector, then <math display=\"inline\">Pu = u</math>, i.e., <math display=\"inline\">1</math> is an eigenvalue of multiplicity <math display=\"inline\">n - 1</math>, since there are <math display=\"inline\">n - 1</math> independent vectors orthogonal to <math display=\"inline\">v</math>.  Also, notice <math display=\"inline\">Pv = -v</math>, and so <math display=\"inline\">-1</math> is an eigenvalue with multiplicity <math display=\"inline\">1</math>.\n* The [[determinant]] of a Householder reflector is <math display=\"inline\">-1</math>, since the determinant of a matrix is the product of its eigenvalues, in this case one of which is <math display=\"inline\">-1</math> with the remainder being <math display=\"inline\">1</math> (as in the previous point).\n\n==Applications==\n\n===Geometric optics===\n\nIn geometric optics, [[specular reflection]] can be expressed in terms of the Householder matrix ([[specular reflection#Vector formulation]]).\n\n===Numerical linear algebra===\n\nHouseholder transformations are widely used in [[numerical linear algebra]], to perform [[QR decomposition]]s and is the first step of the [[QR algorithm]]. They are also widely used for [[tridiagonal]]ization of symmetric matrices and for transforming non-symmetric matrices to a [[Hessenberg matrix|Hessenberg]] form.\n\n====QR decomposition====\n\nHouseholder reflections can be used to calculate [[QR decomposition]]s by reflecting first one column of a matrix onto a multiple of a standard basis vector, calculating the transformation matrix, multiplying it with the original matrix and then recursing down the <math display=\"inline\">(i, i)</math> [[minor (linear algebra)|minor]]s of that product.\n\n==== Tridiagonalization ====\n{{main|Tridiagonal matrix}}\nThis procedure is taken from the book: Numerical Analysis, Burden and Faires, 8th Edition.\nIn the first step, to form the Householder matrix in each step we need to determine <math display=\"inline\">\\alpha</math> and <math display=\"inline\">r</math>, which are:\n:<math>\\begin{align}\n  \\alpha &= -\\operatorname{sgn}\\left(a_{21}\\right)\\sqrt{\\sum_{j=2}^n a_{j1}^2}; \\\\\n       r &= \\sqrt{\\frac{1}{2}\\left(\\alpha^2 - a_{21}\\alpha\\right)};\n\\end{align}</math>\n\nFrom <math display=\"inline\">\\alpha</math> and <math display=\"inline\">r</math>, construct vector <math display=\"inline\">v</math>:\n\n:<math>v^{(1)} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix},</math>\n\nwhere <math display=\"inline\">v_1 = 0</math>, <math display=\"inline\">v_2 = \\frac{a_{21} - \\alpha}{2r}</math>, and \n:<math>v_k = \\frac{a_{k1}}{2r}</math> for each <math>k = 3, 4 \\ldots n</math>\n\nThen compute:\n:<math>\\begin{align}\n      P^1 &= I - 2v^{(1)} \\left(v^{(1)}\\right)^\\textsf{T} \\\\\n  A^{(2)} &= P^1 AP^1\n\\end{align}</math>\n\nHaving found <math display=\"inline\">P^1</math> and computed <math display=\"inline\">A^{(2)}</math> the process is repeated for <math display=\"inline\">k = 2, 3, \\ldots, n - 2</math> as follows:\n\n:<math>\\begin{align}\n     \\alpha &= -\\operatorname{sgn}\\left(a^k_{k+1,k}\\right)\\sqrt{\\sum_{j=k+1}^n \\left(a^k_{jk}\\right)^2} \\\\\n          r &= \\sqrt{\\frac{1}{2}\\left(\\alpha^2 - a^k_{k+1,k}\\alpha\\right)} \\\\\n      v^k_1 &= v^k_2 = \\cdots = v^k_k = 0 \\\\\n  v^k_{k+1} &= \\frac{a^k_{k+1,k} - \\alpha}{2r} \\\\\n      v^k_j &= \\frac{a^k_{jk}}{2r} \\text{ for } j = k + 2,\\ k + 3,\\ \\ldots,\\ n \\\\\n        P^k &= I - 2v^{(k)} \\left(v^{(k)}\\right)^\\textsf{T} \\\\\n  A^{(k+1)} &= P^k A^{(k)}P^k\n\\end{align}</math>\n\nContinuing in this manner, the tridiagonal and symmetric matrix is formed.\n\n====Examples====\n\nThis example is taken from the book \"Numerical Analysis\" by Richard L. Burden (Author), J. Douglas Faires.  In this example, the given matrix is transformed to the similar tridiagonal matrix A<sub>3</sub> by using the Householder method.\n\n: <math>\\mathbf{A} = \\begin{bmatrix}\n   4 & 1 & -2 &  2 \\\\\n   1 & 2 &  0 &  1 \\\\\n  -2 & 0 &  3 & -2 \\\\\n   2 & 1 & -2 & -1\n\\end{bmatrix},</math>\n\nFollowing those steps in the Householder method, we have:\n\nThe first Householder matrix:\n: <math>\\begin{align}\n  Q_1 &= \\begin{bmatrix}\n      1 &  0   & 0   &  0   \\\\\n      0 & -1/3 & 2/3 & -2/3 \\\\\n      0 &  2/3 & 2/3 &  1/3 \\\\\n      0 & -2/3 & 1/3 &  2/3\n    \\end{bmatrix}, \\\\\n  A_2 = Q_1 A Q_1 &= \\begin{bmatrix}\n       4 & -3   &  0   &  0 \\\\\n      -3 & 10/3 &  1   &  4/3 \\\\\n       0 &  1   &  5/3 & -4/3 \\\\\n       0 &  4/3 & -4/3 & -1\n    \\end{bmatrix},\n\\end{align}</math>\n\nUsed <math display=\"inline\">A_2</math> to form\n: <math>\\begin{align}\n  Q_2 &= \\begin{bmatrix}\n    1 & 0 &  0   &  0   \\\\\n    0 & 1 &  0   &  0   \\\\\n    0 & 0 & -3/5 & -4/5 \\\\\n    0 & 0 & -4/5 &  3/5\n  \\end{bmatrix}, \\\\\n  A_3 = Q_2 A_2 Q_2 &= \\begin{bmatrix}\n     4 & -3   &   0    &   0    \\\\\n    -3 & 10/3 &  -5/3  &   0    \\\\\n     0 & -5/3 & -33/25 &  68/75 \\\\\n     0 &  0   &  68/75 & 149/75\n  \\end{bmatrix},\n\\end{align}</math>\n\nAs we can see, the final result is a tridiagonal symmetric matrix which is similar to the original one. The process is finished after two steps.\n\n== Computational and theoretical relationship to other unitary transformations ==\n{{see also|Rotation (mathematics)}}\nThe Householder transformation is a reflection about a hyperplane with unit normal vector <math display=\"inline\">v</math>, as stated earlier. An <math display=\"inline\">N</math>-by-<math display=\"inline\">N</math> [[unitary transformation]] <math display=\"inline\">U</math> satisfies <math display=\"inline\">UU^\\textsf{H} = I</math>. Taking the determinant (<math display=\"inline\">N</math>-th power of the geometric mean) and trace (proportional to arithmetic mean) of a unitary matrix  reveals that its eigenvalues <math display=\"inline\">\\lambda_i</math> have unit modulus. This can be seen directly and swiftly:\n:<math>\n  \\frac{\\operatorname{Trace}\\left(UU^\\textsf{H}\\right)}{N} =\n  \\frac{\\sum_{j=2}^N\\left|\\lambda_j\\right|^2}{N} = 1,\\quad\n  \\operatorname{det}\\left(UU^\\textsf{H}\\right) =\n  \\prod_{j=1}^N \\left|\\lambda_j\\right|^2 = 1.\n</math>\n\nSince arithmetic and geometric means are equal if the variables are constant (see [[inequality of arithmetic and geometric means]]), we establish the claim of unit modulus.\n\nFor the case of real valued unitary matrices we obtain [[orthogonal matrices]], <math display=\"inline\">UU^\\textsf{T} = I</math>. It follows rather readily (see [[orthogonal matrix]]) that any orthogonal matrix can be [[QR decomposition#Using Givens rotations|decomposed]] into a product of 2 by 2 rotations, called [[Givens rotation|Givens Rotations]], and Householder reflections. This is appealing intuitively since multiplication of a vector by an orthogonal matrix preserves the length of that vector, and rotations and reflections exhaust the set of (real valued) geometric operations that render invariant a vector's length.\n\nThe Householder transformation was shown to have a one-to-one relationship with the canonical coset decomposition of unitary matrices defined in group theory, which can be used to parametrize unitary operators in a very efficient manner.<ref>{{cite journal\n |author1=Renan Cabrera |author2=Traci Strohecker |author3=Herschel Rabitz |title= The canonical coset decomposition of unitary matrices through Householder transformations\n |journal=[[Journal of Mathematical Physics]]\n |volume=51 |issue=8 |year=2010 \n |doi=10.1063/1.3466798 \n |arxiv=1008.2477|bibcode=2010JMP....51h2101C\n}}</ref>\n\nFinally we note that a single Householder transform, unlike a solitary Givens transform, can act on all columns of a matrix, and as such exhibits the lowest computational cost for QR decomposition and tridiagonalization. The penalty for this \"computational optimality\" is, of course, that Householder operations cannot be as deeply or efficiently parallelized. As such Householder is preferred for dense matrices on sequential machines, whilst Givens is preferred on sparse matrices, and/or parallel machines.\n\n==References==\n<references />\n* {{cite journal\n |first=C.D. |last=LaBudde\n |title=The reduction of an arbitrary real square matrix to tridiagonal form using similarity transformations\n |journal=[[Mathematics of Computation]] \n |volume=17 |issue=84 |year=1963 |pages=433&ndash;437\n |mr=0156455 |doi=10.2307/2004005\n |jstor=2004005\n |publisher=American Mathematical Society\n}}\n* {{cite journal\n |first=D.D. |last=Morrison\n |title=Remarks on the Unitary Triangularization of a Nonsymmetric Matrix\n |journal=[[Journal of the ACM]]\n |volume=7 |issue=2 |year=1960 |pages=185&ndash;186\n |doi=10.1145/321021.321030 |mr=0114291\n}}\n* {{cite journal\n|last= Cipra  |first= Barry A.  |author-link = Barry A. Cipra\n|title=The Best of the 20th Century: Editors Name Top 10 Algorithms\n|volume=33 | issue=4 | year= 2000 | page= 1| url=https://archive.siam.org/news/news.php?id=637}} (Herein Householder Transformation is cited as a top 10 algorithm of this century)\n* {{Cite book | last1=Press | first1=WH | last2=Teukolsky | first2=SA | last3=Vetterling | first3=WT | last4=Flannery | first4=BP | year=2007 | title=Numerical Recipes: The Art of Scientific Computing | edition=3rd | publisher=Cambridge University Press |  publication-place=New York | isbn=978-0-521-88068-8 | chapter=Section 11.3.2. Householder Method | chapter-url=http://apps.nrbook.com/empanel/index.html#pg=578 | postscript=<!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->{{inconsistent citations}}}}\n\n{{Numerical linear algebra}}\n\n[[Category:Transformation (function)]]\n[[Category:Matrices]]\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "Hypre",
      "url": "https://en.wikipedia.org/wiki/Hypre",
      "text": "{{infobox software\n| name                   = HYPRE\n| latest_release_version = 2.11.2\n| latest_release_date    = 2016/06/09\n| operating_system       = [[Linux]], [[Unix]]\n| language               = C (main language),C++, [[FORTRAN]]\n| genre                  = High-performance Parallel Software for linear systems and eigenvalue problems\n| license                = [[LGPL]] (version 2.1)\n| website                = https://computation.llnl.gov/casc/hypre/software.html\n}}\n\nThe Parallel '''High Performance Preconditioners''' ('''hypre''') is a library of [[computer code|routines]] for [[scalability|scalable]] ([[parallel computing|parallel]]) solution of linear systems. The built-in [[BLOPEX]] package in addition allows solving [[eigenvalue]] problems. The main strength of '''Hypre''' is availability of high performance parallel [[multigrid]] [[preconditioning|preconditioners]] for both structured and unstructured grid problems, see (Falgout et al., 2005, 2006).\n\nCurrently, '''Hypre''' supports only [[real number|real]] [[double-precision]] arithmetic. '''Hypre''' uses the [[Message Passing Interface]]  (MPI) standard for all message-passing communication.  The current beta version of '''Hypre''' is 2.10.0b; released 2015/01/22. [[PETSc]] has an interface to call '''Hypre''' [[preconditioning|preconditioners]].\n\n'''Hypre''' is being developed and is supported by members of the Scalable Linear Solvers project within the [[Lawrence Livermore National Laboratory]].\n\n== Features ==\n\n'''hypre''' provides the following features:\n* Parallel vectors and  matrices, using several different interfaces\n* Scalable parallel [[preconditioning|preconditioners]]\n* Built-in [[BLOPEX]]\n\n== References ==\n* {{cite journal\n  | last = Falgout\n  | first = R.D.\n  | author-link =\n  | last2 = Jones\n  | first2 = J.E.\n  | author2-link =\n  | last3 = Yang\n  | first3 = U.M.\n  | author3-link =\n  | title = Pursuing scalability for hypre's conceptual interfaces\n  | journal = ACM Transactions on Mathematical Software\n  | volume = 31\n  | issue = 3\n  | pages = 326–350\n  | date = 2005  | url =\n  | doi =10.1145/1089014.1089018}}\n* {{cite book\n  | last = Falgout\n  | first = R.D.\n  | author-link =\n  | last2 = Jones\n  | first2 = J.E.\n  | author2-link =\n  | last3 = Yang\n  | first3 = U.M.\n  | author3-link =\n  | chapter = The Design and Implementation of hypre, a Library of Parallel High Performance Preconditioners\n  | title = Numerical Solution of Partial Differential Equations on Parallel Computers\n  |editor1-first= A. M. |editor1-last= Bruaset\n  |editor2-first= A.  |editor2-last=Tveito\n  |publisher= Springer-Verlag\n  |series=Lecture Notes in Computational Science and Engineering \n  | volume = 51\n  | issue =\n  | pages = 267–294\n  | date =\n  | year = 2006\n  | url =\n  |isbn=978-3-540-29076-6\n  | doi =10.1007/3-540-31619-1_8\n  | id =  }}\n\n== External links ==\n* [https://computation.llnl.gov/casc/hypre/software.html Hypre home page]\n* [http://www.mcs.anl.gov/petsc/ PETSc home page]\n* [http://code.google.com/p/blopex/ BLOPEX home page]\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical libraries]]\n[[Category:Numerical linear algebra]]\n[[Category:Scientific simulation software]]"
    },
    {
      "title": "ILNumerics",
      "url": "https://en.wikipedia.org/wiki/ILNumerics",
      "text": "{{unreliable sources|date=November 2015}}\n{{Infobox software\n| name = ILNumerics\n| screenshot = \n| caption = \n| developer = ILNumerics\n| latest release version = v5.01\n| latest release date = {{Start date and age|2018|09|01}}\n| operating system = [[.NET Framework]] / [[Mono (software)|Mono]], [[Microsoft Windows|Windows]]\n| genre = [[List of numerical analysis software|Technical computing]], [[Mathematical software]]\n| license = [[Proprietary software|Proprietary]]\n| website = {{URL|ilnumerics.net}}\n| platform = [[IA-32]], [[x86-64]], AnyCPU\n| programming language = [[C Sharp (programming language)|C#]], [[Visual Basic .NET|Visual Basic]]\n}}\n'''ILNumerics''' is a mathematical [[Library (computing)|class library]] for [[Common Language Infrastructure]] (CLI) developers and a [[Domain-specific language|domain specific language (DSL)]] for the [[implementation]] of [[numerical analysis|numerical]] [[algorithm]]s on the [[Dotnet framework|.NET platform]]. While [[computer algebra system|algebra systems]] with [[graphical user interface]]s focus on [[Prototype-based programming|prototyping]] of algorithms, implementation of such algorithms into distribution-ready [[Application software|applications]] is done using \n[[Integrated development environment|development environments]] and [[General-purpose programming language|general purpose programming languages (GPL)]]. ILNumerics is an extension to [[Visual Studio Debugger|Visual Studio]] and aims at supporting the creation of technical applications based on .NET.\n\n==History==\nILNumerics started in 2006 as an open source project, originating from the Technical University of Berlin.<ref>{{cite web|url=https://www.entrepreneurship.tu-berlin.de/menue/start-ups_events/gruenderstories_start-ups/steckbriefe/steckbrief_ilnumerics |title=Centre for Entrepreneurship: Steckbrief ILNumerics |publisher=Entrepreneurship.tu-berlin.de |date= |accessdate=2016-01-28}}</ref> In 2007 ILNumerics won the [[BASTA! Innovation Awards 2007]]<ref>[http://basta-award.de BASTA! Innovation Award 2007]</ref> as most innovative .NET project in [[Germany]], [[Switzerland]] and [[Austria]]. After 6 years of open source development, the project added a closed source, proprietary license in 2011, aiming business and academic developers at the same time. The project quickly gained popularity (download numbers and engagement at stackoverflow.com,<ref>{{cite web|author=ilnumerics |url=https://www.nuget.org/packages/ILNumerics/ |title=NuGet Gallery &#124; ILNumerics 4.8.0 |publisher=Nuget.org |date= |accessdate=2016-01-28}}</ref><ref>{{cite web|url=https://stackoverflow.com/questions/tagged/ilnumerics |title=Newest 'ilnumerics' Questions |publisher=Stack Overflow |date= |accessdate=2016-01-28}}</ref><ref>{{cite web|url=https://visualstudiogallery.msdn.microsoft.com/9a67b49d-b0b2-4073-97fd-3f18ba08cc61 |title=ILNumerics Ultimate VS extension |publisher=Visualstudiogallery.msdn.microsoft.com |date= |accessdate=2016-01-28}}</ref> download counts from website not available).\nThe [[.NET Framework version history|.NET framework]] was selected as a [[Managed code|managed]] foundation, since earlier attempts on the [[Java Platform|Java platform]] had been abandoned due to technical limitations. Similarly, the .NET framework has not been designed with the focus on requirements of technical application development. ILNumerics added interfaces to popular codes ([[LAPACK]], [[FFTW]]), [[complex number]]s and [[Parametric polymorphism|generic]] [[Array data structure#Multidimensional arrays|mult-dimensional array]] classes. In 2010 graphical capabilities have been added. Efforts to increase the performance of the technology were introduced in 2011. At the same time, a company was founded to continue the development. The technological goal is to establish the .NET framework as a feasible alternative to unmanaged [[Programming language C|languages]] for [[Mathematics and Computing Engineering|numeric computing]].\n\n==Syntax==\nILNumerics implements base functionality frequently needed for application development in technical areas: N-dimensional [[Array data structure|arrays]], [[complex number]]s, [[linear algebra]], [[Fast Fourier transform|FFT]] and [[Chart|plotting]] [[GUI widget|controls]] (2D and [[3D computer graphics|3D]]). The array classes are fully compatible with the array features of [[MATLAB|Matlab]]<small><sup>(R)</sup></small> and [[NumPy|numpy]], including internal storage order, subarray creation, expansion, and advanced indexing. Higher level functionality is provided by toolboxes for [[interpolation]], [[Mathematical optimization|optimization]], [[statistics]], [[Hierarchical Data Format|HDF5]] and [[machine learning]]. The ILNumerics DSL is embedded into .NET. Computational algorithms are formulated using any [[List of CLI languages|CLI language]]. However, only [[C Sharp (programming language)|C#]] and [[Visual Basic .NET|Visual Basic]] are officially supported. Due to the [[type safety|strong type system]] of the .NET framework algorithms created with ILNumerics are strongly typed. This deviates from the syntax of [[Matlab|alternatives]], which are often weakly typed and therefore easier to adopt.\n\n==Graphics== \nA [[scene graph]] is used in ILNumerics to realize graphical output. Interactive 2D and 3D plots are used in Windows Forms applications. Hardware accelerated drawing is available via [[OpenGL]]. A software renderer is provided for legacy hardware, based on GDI+ and [[Scalable Vector Graphics|SVG]].\n\n==IDE integration==\nILNumerics is distributed as an extension to [[Visual Studio]]. It adds a tool window to the IDE for the graphical inspection of mathematical objects while stepping through user code.\n\n==Performance==\nSince ILNumerics comes as a [[Assembly (CLI)|CLI assembly]], it targets [[Common Language Infrastructure]] (CLI) applications. Just like [[Java (programming language)|Java]] - those frameworks are often criticized for not being suitable for numerical computations. Reasons are the [[Random access memory|memory]] management by a [[Garbage collection (computer science)|garbage collector]], the [[Common Intermediate Language|intermediate language]] execution and deficient optimizations by the [[Compiler optimizations|compilers]] involved. ILNumerics approaches these limitations by performing [[loop unrolling]], [[Bounds-checking elimination|removal of bound checks]] on array accesses and [[Cache-oblivious algorithm|cache optimizations]]. Further speed-up is gained by the auto-management of the memory of large array objects. Numerical operations are [[Parallel computing|parallelized]] on [[Multicore programming|multicore]] systems. Linear algebra routines rely on processor specific optimized versions of [[LAPACK]] and [[BLAS]].\n\nILNumerics arrays utilize the [[Virtual memory|unmanaged heap]] for storing data. This way, the [[Big data|size]] of ILNumerics arrays is not [https://blogs.msdn.microsoft.com/joshwil/2005/08/10/bigarrayt-getting-around-the-2gb-array-size-limit/ limited by the CLR] and [[P/invoke|interoperability]] with 3rd party libraries is improved. \n\n==See also==\n*[[Comparison of numerical analysis software]]\n*[[List of numerical analysis software]]\n*[[List of numerical libraries]]\n\n==References==\n{{reflist}}\n\n==External links==\n*{{Official website|ilnumerics.net}} \n*[https://msdn.microsoft.com/library/hh304368%28v=vs.100%29.aspx Article about Math Libraries for .NET at MSDN]\n\n{{Numerical analysis software}}\n{{Statistical software}}\n\n{{DEFAULTSORT:Ilnumerics.Net}}\n[[Category:3D graphics software]]\n[[Category:3D scenegraph APIs]]\n[[Category:Array programming languages]]\n[[Category:C Sharp libraries]]\n[[Category:Computer vision software]]\n[[Category:Data analysis software]]\n[[Category:Data visualization software]]\n[[Category:Mathematical software]]\n[[Category:Numerical analysis software for Linux]]\n[[Category:Numerical analysis software for MacOS]]\n[[Category:Numerical analysis software for Windows]]\n[[Category:Numerical linear algebra]]\n[[Category:Numerical programming languages]]\n[[Category:Object-oriented programming languages]]\n[[Category:OpenGL]]\n[[Category:Parallel computing]]\n[[Category:Science software]]\n[[Category:Unix programming tools]]"
    },
    {
      "title": "In-place matrix transposition",
      "url": "https://en.wikipedia.org/wiki/In-place_matrix_transposition",
      "text": "'''In-place matrix transposition''', also called '''in-situ matrix transposition''', is the problem of [[transpose|transposing]] an ''N''×''M'' [[matrix (mathematics)|matrix]] [[in-place]] in [[computer memory]], ideally with [[Big O notation|''O''(1)]] (bounded) additional storage, or at most with additional storage much less than ''NM''.  Typically, the matrix is assumed to be stored in [[row-major order]] or [[column-major order]] (i.e., contiguous rows or columns, respectively, arranged consecutively).\n\nPerforming an in-place transpose (in-situ transpose) is most difficult when ''N'' ≠ ''M'', i.e. for a non-square (rectangular) matrix, where it involves a complicated [[permutation]] of the data elements, with many [[cyclic permutation|cycle]]s of length greater than 2.  In contrast, for a square matrix (''N'' = ''M''), all of the cycles are of length 1 or 2, and the transpose can be achieved by a simple loop to swap the upper triangle of the matrix with the lower triangle.  Further complications arise if one wishes to maximize [[memory locality]] in order to improve [[cache line]] utilization or to operate [[out-of-core]] (where the matrix does not fit into main memory), since transposes inherently involve non-consecutive memory accesses.\n\nThe problem of non-square in-place transposition has been studied since at least the late 1950s, and several algorithms are known, including several which attempt to optimize locality for cache, out-of-core, or similar memory-related contexts.\n\n==Background==\n\nOn a [[computer]], one can often avoid explicitly transposing a matrix in [[Random access memory|memory]] by simply accessing the same data in a different order.  For example, [[software libraries]] for [[linear algebra]], such as [[BLAS]], typically provide options to specify that certain matrices are to be interpreted in transposed order to avoid data movement.\n\nHowever, there remain a number of circumstances in which it is necessary or desirable to physically reorder a matrix in memory to its transposed ordering.  For example, with a matrix stored in [[row-major order]], the rows of the matrix are contiguous in memory and the columns are discontiguous.  If repeated operations need to be performed on the columns, for example in a [[fast Fourier transform]] algorithm (e.g. Frigo & Johnson, 2005), transposing the matrix in memory (to make the columns contiguous) may improve performance by increasing [[memory locality]].  Since these situations normally coincide with the case of very large matrices (which exceed the cache size), performing the transposition in-place with minimal additional storage becomes desirable.\n\nAlso, as a purely mathematical problem, in-place transposition involves a number of interesting [[number theory]] puzzles that have been worked out over the course of several decades.\n\n==Example==\n\nFor example, consider the 2×4 matrix:\n\n:<math>\\begin{bmatrix} 11 & 12 & 13 & 14 \\\\ 21 & 22 & 23 & 24\\end{bmatrix}.</math>\n\nIn row-major format, this would be stored in computer memory as the sequence (11, 12, 13, 14, 21, 22, 23, 24), i.e. the two rows stored consecutively. If we transpose this, we obtain the 4×2 matrix:\n\n:<math>\\begin{bmatrix} 11 & 21 \\\\ 12 & 22 \\\\ 13 & 23 \\\\ 14 & 24\\end{bmatrix}</math>\n\nwhich is stored in computer memory as the sequence (11, 21, 12, 22, 13, 23, 14, 24).\n\n{| class=\"wikitable infobox\" style=\"color:black\"\n! style=\"text-align:right;\" |Position\n! style=\"background:#ccffff\"|0\n! style=\"background:#ffcccc\"|1\n! style=\"background:#ffcccc\"|2\n! style=\"background:#ffffcc\"|3\n! style=\"background:#ffcccc\"|4\n! style=\"background:#ffffcc\"|5\n! style=\"background:#ffffcc\"|6\n! style=\"background:#ccffcc\"|7\n|-\n| Original storage\n! style=\"background:#ccffff\"|11\n! style=\"background:#ffcccc\"|12\n! style=\"background:#ffcccc\"|13\n! style=\"background:#ffffcc\"|14\n! style=\"background:#ffcccc\"|21\n! style=\"background:#ffffcc\"|22\n! style=\"background:#ffffcc\"|23\n! style=\"background:#ccffcc\"|24\n|-\n| Transposed storage\n! style=\"background:#ccffff\"|11\n! style=\"background:#ffcccc\"|21\n! style=\"background:#ffcccc\"|12\n! style=\"background:#ffffcc\"|22\n! style=\"background:#ffcccc\"|13\n! style=\"background:#ffffcc\"|23\n! style=\"background:#ffffcc\"|14\n! style=\"background:#ccffcc\"|24\n|}\nIf we number the storage locations 0 to 7, from left to right, then this permutation consists of four cycles:\n\n:(0), (1 2 4), (3 6 5), (7)\n\nThat is, the value in position 0 goes to position 0 (a cycle of length 1, no data motion). Next, the value in position 1 (in the original storage: 11, '''12''', 13, 14, 21, 22, 23, 24) goes to position 2 (in the transposed storage 11, 21, '''12''', 22, 13, 23, 14, 24), while the value in position 2 (11, 12, '''13''', 14, 21, 22, 23, 24) goes to position 4 (11, 21, 12, 22, '''13''', 23, 14, 24), and position 4 (11, 12, 13, 14, '''21''', 22, 23, 24) goes back to position 1 (11, '''21''', 12, 22, 13, 23, 14, 24). Similarly for the values in position 7 and positions (3 6 5).\n\n==Properties of the permutation==\n\nIn the following, we assume that the ''N''×''M'' matrix is stored in row-major order with zero-based indices.  This means that the (''n'',''m'') element, for ''n'' = 0,&#8230;,''N''&minus;1 and ''m'' = 0,&#8230;,''M''&minus;1, is stored at an address ''a'' = ''Mn'' + ''m'' (plus some offset in memory, which we ignore).  In the transposed ''M''×''N'' matrix, the corresponding (''m'',''n'') element is stored at the address ''a' '' = ''Nm'' + ''n'', again in row-major order.  We define the ''transposition permutation'' to be the function ''a' '' = ''P''(''a'') such that:\n:<math>Nm + n = P(Mn + m) \\,</math> for all <math>(n,m) \\in [0,N-1]\\times[0,M-1] \\,.</math>\nThis defines a permutation on the numbers <math>a = 0,\\ldots,MN-1</math>.\n\nIt turns out that one can define simple formulas for ''P'' and its inverse (Cate & Twigg, 1977).  First:\n\n:<math>P(a) = \\begin{cases}\nMN - 1 & \\text{if } a = MN - 1, \\\\\nNa \\bmod MN - 1 & \\text{otherwise},\n\\end{cases}\n</math>\n\nwhere \"mod\" is the [[modulo operation]]. \n{{cot|Proof}} \nIf 0 ≤ ''a'' = ''Mn'' + ''m'' < ''MN'' &minus; 1, then ''Na'' mod (''MN''&minus;1) = ''MN'' ''n'' + ''Nm'' mod (''MN'' &minus; 1) = ''n'' + ''Nm''.  <ref group=\"ProofNote\"> ''MN'' ''x'' mod (''MN''&minus;1) = (''MN'' &minus; 1) ''x''  + ''x'' mod (''MN''&minus;1) = ''x'' for 0 ≤ ''x'' < ''MN'' &minus; 1.</ref><ref group=\"ProofNote\">The first (''a'' = 0) and last (''a'' = ''MN''&minus;1) elements are always left invariant under transposition. </ref>\n{{cob}}  \nSecond, the inverse permutation is given by:\n\n:<math>P^{-1}(a') = \\begin{cases}\nMN - 1 & \\text{if } a' = MN - 1, \\\\\nMa' \\bmod MN - 1 & \\text{otherwise}.\n\\end{cases}\n</math>\n\n(This is just a consequence of the fact that the inverse of an ''N''×''M'' transpose is an ''M''×''N'' transpose, although it is also easy to show explicitly that ''P''<sup>&minus;1</sup> composed with ''P'' gives the identity.)\n\nAs proved by Cate & Twigg (1977), the number of [[fixed point (mathematics)|fixed points]] (cycles of length 1) of the permutation is precisely {{math|1&nbsp;+&nbsp;gcd(''N''&minus;1,''M''&minus;1)}}, where gcd is the [[greatest common divisor]].  For example, with ''N'' = ''M'' the number of fixed points is simply ''N'' (the diagonal of the matrix).  If {{math|''N''&nbsp;&minus;&nbsp;1}} and {{math|''M''&nbsp;&minus;&nbsp;1}} are [[coprime]], on the other hand, the only two fixed points are the upper-left and lower-right corners of the matrix.\n\nThe number of cycles of any length ''k''&gt;1 is given by (Cate & Twigg, 1977):\n\n:<math>\\frac{1}{k} \\sum_{d | k} \\mu(k/d) \\gcd(N^d - 1, MN - 1) ,</math>\n\nwhere μ is the [[Möbius function]] and the sum is over the [[divisor]]s ''d'' of ''k''.\n\nFurthermore, the cycle containing ''a''=1 (i.e. the second element of the first row of the matrix) is always a cycle of maximum length ''L'', and the lengths ''k'' of all other cycles must be divisors of ''L''  (Cate & Twigg, 1977).\n\nFor a given cycle ''C'', every element <math>x \\in C</math> has the same greatest common divisor <math>d = \\gcd(x, MN - 1)</math>.  \n{{cot|Proof (Brenner, 1973)}}  \nLet ''s'' be the smallest element of the cycle, and <math>d = \\gcd(s, MN - 1)</math>.  From the definition of the permutation ''P'' above, every other element ''x'' of the cycle is obtained by repeatedly multiplying ''s'' by ''N'' modulo ''MN''&minus;1, and therefore every other element is divisible by ''d''.  But, since ''N'' and {{math|''MN''&nbsp;&minus;&nbsp;1}} are coprime, ''x'' cannot be divisible by any factor of {{math|''MN''&nbsp;&minus;&nbsp;1}} larger than ''d'', and hence <math>d = \\gcd(x, MN - 1)</math>.\n{{cob}}  \nThis theorem is useful in searching for cycles of the permutation, since an efficient search can look only at multiples of divisors of ''MN''&minus;1 (Brenner, 1973).\n\nLaflin & Brebner (1970) pointed out that the cycles often come in pairs, which is exploited by several algorithms that permute pairs of cycles at a time.  In particular, let ''s'' be the smallest element of some cycle ''C'' of length ''k''.  It follows that ''MN''&minus;1&minus;''s'' is also an element of a cycle of length ''k'' (possibly the same cycle).  \n{{cot|Proof by the definition of ''P'' above}} \nThe length ''k'' of the cycle containing ''s'' is the smallest ''k'' &gt; 0 such that <math>s N^k = s \\bmod (MN - 1)</math>.  Clearly, this is the same as the smallest ''k''&gt;0 such that <math>(-s) N^k = -s \\bmod (MN - 1)</math>, since we are just multiplying both sides by &minus;1, and <math>MN-1-s = -s \\bmod (MN - 1)</math>. \n{{cob}}\n\n{{cot|Note of proofs}}\n<references group=\"ProofNote\"/>\n{{cob}}\n\n==Algorithms==\n\nThe following briefly summarizes the published algorithms to perform in-place matrix transposition.  [[Source code]] implementing some of these algorithms can be found in the references, below.\n===Accessor Transpose===\nBecause physically transposing a matrix is computationally expensive, instead of moving values in memory, the access path may be transposed instead. It is trivial to perform this operation for CPU access, as the access paths of [[iterators]] must simply be exchanged<ref>{{cite web |title=numpy.swapaxes — NumPy v1.15 Manual |url=https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.swapaxes.html |website=docs.scipy.org |accessdate=22 January 2019}}</ref>, however hardware acceleration may require that still be physically realigned.<ref>{{cite web |last1=Harris |first1=Mark |title=An Efficient Matrix Transpose in CUDA C/C++ |url=https://devblogs.nvidia.com/efficient-matrix-transpose-cuda-cc/ |website=NVIDIA Developer Blog |date=18 February 2013}}</ref>\n===Square matrices===\n\nFor a square ''N''×''N'' matrix ''A''<sub>''n'',''m''</sub> = ''A''(''n'',''m''), in-place transposition is easy because all of the cycles have length 1 (the diagonals ''A''<sub>''n'',''n''</sub>) or length 2 (the upper triangle is swapped with the lower triangle).  [[Pseudocode]] to accomplish this (assuming zero-based [[array data structure|array]] indices) is:\n\n '''for''' n = 0 to N - 2\n     '''for''' m = n + 1 to N - 1\n         swap A(n,m) with A(m,n)\n\nThis type of implementation, while simple, can exhibit poor performance due to poor cache-line utilization, especially when ''N'' is a [[power of two]] (due to cache-line conflicts in a [[CPU cache]] with limited associativity).  The reason for this is that, as ''m'' is incremented in the inner loop, the memory address corresponding to ''A''(''n'',''m'') or ''A''(''m'',''n'') jumps discontiguously by ''N'' in memory (depending on whether the array is in column-major or row-major format, respectively).  That is, the algorithm does not exploit [[locality of reference]].\n\nOne solution to improve the cache utilization is to \"block\" the algorithm to operate on several numbers at once, in blocks given by the cache-line size; unfortunately, this means that the algorithm depends on the size of the cache line (it is \"cache-aware\"), and on a modern computer with multiple levels of cache it requires multiple levels of machine-dependent blocking. Instead, it has been suggested (Frigo ''et al.'', 1999) that better performance can be obtained by a [[recursion|recursive]] algorithm: divide the matrix into four submatrices of roughly equal size, transposing the two submatrices along the diagonal recursively and transposing and swapping the two submatrices above and below the diagonal.  (When ''N'' is sufficiently small, the simple algorithm above is used as a base case, as naively recurring all the way down to ''N''=1 would have excessive function-call overhead.)  This is a [[cache-oblivious]] algorithm, in the sense that it can exploit the cache line without the cache-line size being an explicit parameter.\n\n===Non-square matrices: Following the cycles===\n\nFor non-square matrices, the algorithms are more complicated.  Many of the algorithms prior to 1980 could be described as \"follow-the-cycles\" algorithms.  That is, they loop over the cycles, moving the data from one location to the next in the cycle.  In pseudocode form:\n\n '''for each''' length&gt;1 cycle ''C'' of the permutation\n     pick a starting address ''s'' in ''C''\n     let ''D'' = data at ''s''\n     let ''x'' = predecessor of ''s'' in the cycle\n     '''while''' ''x'' ≠ ''s''\n         move data from ''x'' to successor of ''x''\n         let ''x'' = predecessor of ''x''\n     move data from ''D'' to successor of ''s''\n\nThe differences between the algorithms lie mainly in how they locate the cycles, how they find the starting addresses in each cycle, and how they ensure that each cycle is moved exactly once.  Typically, as discussed above, the cycles are moved in pairs, since ''s'' and ''MN''&minus;1&minus;''s'' are in cycles of the same length (possibly the same cycle).  Sometimes, a small scratch array, typically of length ''M''+''N'' (e.g. Brenner, 1973; Cate & Twigg, 1977) is used to keep track of a subset of locations in the array that have been visited, to accelerate the algorithm.\n\nIn order to determine whether a given cycle has been moved already, the simplest scheme would be to use ''O''(''MN'') auxiliary storage, one [[bit]] per element, to indicate whether a given element has been moved.  To use only ''O''(''M''+''N'') or even {{math|''O''(log&nbsp;''MN'')}} auxiliary storage, more complicated algorithms are required, and the known algorithms have a worst-case [[linearithmic]] computational cost of {{math|''O''(''MN''&nbsp;log&nbsp;''MN'')}} at best, as first proved by [[Donald Knuth|Knuth]] (Fich ''et al.'', 1995; Gustavson & Swirszcz, 2007).\n\nSuch algorithms are designed to move each data element exactly once.  However, they also involve a considerable amount of arithmetic to compute the cycles, and require heavily non-consecutive memory accesses since the adjacent elements of the cycles differ by multiplicative factors of ''N'', as discussed above.\n\n===Improving memory locality at the cost of greater total data movement===\n\nSeveral algorithms have been designed to achieve greater memory locality at the cost of greater data movement, as well as slightly greater storage requirements.  That is, they may move each data element more than once, but they involve more consecutive memory access (greater spatial locality), which can improve performance on modern CPUs that rely on caches, as well as on [[SIMD]] architectures optimized for processing consecutive data blocks.  The oldest context in which the spatial locality of transposition seems to have been studied is for out-of-core operation (by Alltop, 1975), where the matrix is too large to fit into main memory (\"[[Magnetic-core memory|core]]\").\n\nFor example, if ''d'' = [[greatest common divisor|gcd]](''N'',''M'') is not small, one can perform the transposition using a small amount (''NM''/''d'') of additional storage, with at most three passes over the array (Alltop, 1975; Dow, 1995).  Two of the passes involve a sequence of separate, small transpositions (which can be performed efficiently out of place using a small buffer) and one involves an in-place ''d''&times;''d'' square transposition of <math>NM/d^2</math> blocks (which is efficient since the blocks being moved are large and consecutive, and the cycles are of length at most 2). This is further simplified if N is a multiple of M (or vice versa), since only one of the two out-of-place passes is required.\n\nAnother algorithm for non-[[coprime]] dimensions, involving multiple subsidiary transpositions, was described by Catanzaro et al. (2014).  For the case where {{math|{{abs|''N''&nbsp;&minus;&nbsp;''M''}}}} is small, Dow (1995) describes another algorithm requiring {{math|{{abs|''N''&nbsp;&minus;&nbsp;''M''}} ⋅ min(''N'',''M'')}} additional storage, involving a {{math|min(''N'',&nbsp;''M'') ⋅ min(''N'',&nbsp;''M'')}} square transpose preceded or followed by a small out-of-place transpose.  Frigo & Johnson (2005) describe the adaptation of these algorithms to use cache-oblivious techniques for general-purpose CPUs relying on cache lines to exploit spatial locality.\n\nWork on out-of-core matrix transposition, where the matrix does not fit in main memory and must be stored largely on a [[hard disk]], has focused largely on the ''N'' = ''M'' square-matrix case, with some exceptions (e.g. Alltop, 1975).  Recent reviews of out-of-core algorithms, especially as applied to [[parallel computing]], can be found in e.g. Suh & Prasanna (2002) and Krishnamoorth et al. (2004).\n\n==References==\n{{refbegin}}\n* P. F. Windley, \"Transposing matrices in a digital computer,\" ''Computer Journal'' '''2''', p.&nbsp;47-48 (1959).\n* G. Pall, and E. Seiden, \"A problem in Abelian Groups, with application to the transposition of a matrix on an electronic computer,\" ''Math. Comp.'' '''14''', p.&nbsp;189-192 (1960).\n* J. Boothroyd, \"[http://portal.acm.org/citation.cfm?id=363304&dl=GUIDE&coll=GUIDE&CFID=436989&CFTOKEN=18491885 Algorithm 302: Transpose vector stored array],\" ''ACM Transactions on Mathematical Software'' '''10''' (5), p.&nbsp;292-293 (1967). \n* Susan Laflin and M. A. Brebner, \"[http://portal.acm.org/citation.cfm?id=362368&dl=GUIDE&coll=GUIDE&CFID=436989&CFTOKEN=18491885 Algorithm 380: in-situ transposition of a rectangular matrix],\" ''ACM Transactions on Mathematical Software'' '''13''' (5), p.&nbsp;324-326 (1970).  [http://www.netlib.org/toms/380 Source code].\n* Norman Brenner, \"[http://portal.acm.org/citation.cfm?id=362542&dl=GUIDE&coll=GUIDE&CFID=436989&CFTOKEN=18491885 Algorithm 467: matrix transposition in place],\" ''ACM Transactions on Mathematical Software'' '''16''' (11), p.&nbsp;692-694 (1973). [http://www.netlib.org/toms/467 Source code].\n* W. O. Alltop, \"A computer algorithm for transposing nonsquare matrices,\" ''IEEE Trans. Comput.'' '''24''' (10), p.&nbsp;1038-1040 (1975).\n* Esko G. Cate and David W. Twigg, \"[http://portal.acm.org/citation.cfm?id=355719.355729&coll=GUIDE&dl=GUIDE&CFID=436989&CFTOKEN=18491885 Algorithm 513: Analysis of In-Situ Transposition],\" ''ACM Transactions on Mathematical Software'' '''3''' (1), p.&nbsp;104-110 (1977). [http://www.netlib.org/toms/513 Source code].\n* Bryan Catanzaro, Alexander Keller, and Michael Garland, [A decomposition for in-place matrix transposition http://dl.acm.org/citation.cfm?id=2555253],  Proceedings of the 19th ACM SIGPLAN symposium on Principles and practice of parallel programming (PPoPP '14), pp.&nbsp;193–206 (2014).\n* Murray Dow, \"Transposing a matrix on a vector computer,\" ''Parallel Computing'' '''21''' (12), p.&nbsp;1997-2005 (1995).\n* Donald E. Knuth, ''[[The Art of Computer Programming]] Volume 1: Fundamental Algorithms'', third edition, section 1.3.3 exercise 12 (Addison-Wesley: New York, 1997).\n* M. Frigo, C. E. Leiserson, H. Prokop, and S. Ramachandran, \"[https://www.cs.cmu.edu/~guyb/realworld/slidesF10/FLP99.pdf Cache-oblivious algorithms],\" in ''Proceedings of the 40th IEEE Symposium on Foundations of Computer Science'' (FOCS 99), p.&nbsp;285-297 (1999). [http://ieeexplore.ieee.org/iel5/6604/17631/00814600.pdf?arnumber=814600 Extended abstract at IEEE], [http://citeseer.ist.psu.edu/307799.html at Citeseer].\n* J. Suh and V. K. Prasanna, \"[https://dx.doi.org/10.1109/12.995452 An efficient algorithm for out-of-core matrix transposition],\" ''IEEE Trans. Computers'' '''51''' (4), p.&nbsp;420-438 (2002).\n* S. Krishnamoorthy, G. Baumgartner, D. Cociorva, C.-C. Lam, and P. Sadayappan, \"[http://csc.lsu.edu/~gb/TCE//Publications/ParTranspose2.pdf Efficient parallel out-of-core matrix transposition],\" ''International Journal of High Performance Computing and Networking'' '''2''' (2-4), p.&nbsp;110-119 (2004).\n* M. Frigo and S. G. Johnson, \"[http://fftw.org/fftw-paper-ieee.pdf The Design and Implementation of FFTW3],\" ''Proceedings of the IEEE'' '''93''' (2), 216–231 (2005). [http://www.fftw.org Source code] of the [[FFTW]] library, which includes optimized serial and [[parallel computing|parallel]] square and non-square transposes, in addition to [[Fast Fourier transform|FFT]]s.\n* [[Faith Ellen|Faith E. Fich]], J. Ian Munro, and Patricio V. Poblete, \"Permuting in place,\" ''SIAM Journal on Computing'' '''24''' (2), p.&nbsp;266-278 (1995).\n* Fred G. Gustavson and Tadeusz Swirszcz, \"In-place transposition of rectangular matrices,\" ''Lecture Notes in Computer Science'' '''4699''', p.&nbsp;560-569 (2007), from the Proceedings of the 2006 Workshop on State-of-the-Art <nowiki>[</nowiki>''sic''<nowiki>]</nowiki> in Scientific and Parallel Computing (PARA 2006) (Umeå, Sweden, June 2006).\n*{{Cite OEIS|sequencenumber=A093055|name=Number of non-singleton cycles in the in-situ transposition of a rectangular j X k matrix}}\n*{{Cite OEIS|sequencenumber=A093056|name=Length of the longest cycle in the in-situ transposition of a rectangular j X k matrix}}\n*{{Cite OEIS|sequencenumber=A093057|name=Number of matrix elements remaining at fixed position in the in-situ transposition of a rectangular j X k matrix}}\n{{refend}}\n\n==External links==\n\n===Source code===\n* [http://romo661.free.fr/offt.html OFFT] - recursive block in-place transpose of square matrices, in Fortran\n* [https://groups.google.com/group/sci.math.num-analysis/msg/680211b3fbac30c4?hl=en Jason Stratos Papadopoulos], blocked in-place transpose of square matrices, in [[C (programming language)|C]], ''sci.math.num-analysis'' newsgroup (April 7, 1998).\n* See \"Source code\" links in the references section above, for additional code to perform in-place transposes of both square and non-square matrices.\n* [https://bitbucket.org/ijsung/libmarshal/wiki/Home libmarshal] Blocked in-place transpose of rectangular matrices for the GPUs.\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]\n[[Category:Permutations]]\n[[Category:Articles with example pseudocode]]"
    },
    {
      "title": "Incomplete Cholesky factorization",
      "url": "https://en.wikipedia.org/wiki/Incomplete_Cholesky_factorization",
      "text": "In [[numerical analysis]], an '''incomplete Cholesky factorization''' of a symmetric [[positive definite matrix]] is a [[sparse matrix|sparse]] approximation of the [[Cholesky factorization]]. An incomplete Cholesky factorization is often used as a [[preconditioner]] for algorithms like the [[conjugate gradient method]].\n\nThe Cholesky factorization of a positive definite matrix ''A'' is ''A'' = ''LL''* where ''L'' is a [[lower triangular matrix]]. An incomplete Cholesky factorization is given by a sparse lower triangular matrix ''K'' that is in some sense close to ''L''. The corresponding preconditioner is ''KK''*. \n\nOne popular way to find such a matrix ''K'' is to use the algorithm for finding the exact Cholesky decomposition, except that any entry is set to zero if the corresponding entry in ''A'' is also zero. This gives an incomplete Cholesky factorization which is as sparse as the matrix ''A''.\n\n== Algorithm ==\nFor <math>i</math> from <math>1</math> to <math>N</math>:\n:<math>\nL_{ii}  = \\left( {a_{ii}  - \\sum\\limits_{k = 1}^{i - 1} {L_{ik}^2 } } \\right)^{{1 \\over 2}} \n</math>\n:For <math>j</math> from <math>i+1</math> to <math>N</math>:\n::<math>\nL_{ji}  = {1 \\over {L_{ii} }}\\left( {a_{ji}  - \\sum\\limits_{k = 1}^{i - 1} {L_{ik} L_{jk} } } \\right)\n</math>\n\n== Implementation ==\n\nImplementation of the incomplete Cholesky factorization in the Octave scripting language. The factorization is stored as a lower triangular matrix, with the elements in the upper triangle set to zero.\n\n<source lang=\"octave\">\nfunction a = ichol(a)\n\tn = size(a,1);\n\n\tfor k=1:n\n\t\ta(k,k) = sqrt(a(k,k));\n\t\tfor i=(k+1):n\n\t\t    if (a(i,k)!=0)\n\t\t        a(i,k) = a(i,k)/a(k,k);            \n\t\t    endif\n\t\tendfor\n\t\tfor j=(k+1):n\n\t\t    for i=j:n\n\t\t        if (a(i,j)!=0)\n\t\t            a(i,j) = a(i,j)-a(i,k)*a(j,k);  \n\t\t        endif\n\t\t    endfor\n\t\tendfor\n\tendfor\n\n    for i=1:n\n        for j=i+1:n\n            a(i,j) = 0;\n        endfor\n    endfor            \nendfunction\n</source>\n\n==References==\n* [http://www.cfd-online.com/Wiki/Incomplete_Cholesky_factorization Incomplete Cholesky factorization] at CFD Online wiki\n* {{Citation | last1=Golub | first1=Gene H. | author1-link=Gene H. Golub | last2=Van Loan | first2=Charles F. | author2-link=Charles F. Van Loan | title=Matrix Computations | publisher=Johns Hopkins | edition=3rd | isbn=978-0-8018-5414-9 | year=1996}}. See Section 10.3.2.\n\n{{mathapplied-stub}}\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "Incomplete LU factorization",
      "url": "https://en.wikipedia.org/wiki/Incomplete_LU_factorization",
      "text": "In [[numerical linear algebra]], an '''incomplete LU factorization''' (abbreviated as '''ILU''') of a [[matrix (mathematics)|matrix]] is a [[sparse matrix|sparse]] approximation of the [[LU factorization]] often used as a [[preconditioner]].\n\n== Introduction ==\nConsider a sparse linear system <math>Ax = b</math>. These are often solved by computing the factorization <math>A = LU</math>, with ''L'' [[Triangular_matrix#Unitriangular_matrix|lower unitriangular]] and ''U'' [[Triangular_matrix|upper triangular]].\nOne then solves <math>Ly = b</math>, <math>Ux = y</math>, which can be done efficiently because the matrices are triangular.\n\nFor a typical sparse matrix, the LU factors can be much less sparse than the original matrix &mdash; a phenomenon called ''fill-in''. \nThe memory requirements for using a direct solver can then become a bottleneck in solving linear systems. One can combat this problem by using fill-reducing reorderings of the matrix's unknowns, such as the [[Cuthill-McKee algorithm|Cuthill-McKee ordering]].\n\nAn incomplete factorization instead seeks triangular matrices ''L'', ''U'' such that <math>A \\approx LU</math> rather than <math>A = LU</math>. Solving for <math>LUx = b</math> can be done quickly but does not yield the exact solution to <math>Ax = b</math>. So, we instead use the matrix <math>M = LU</math> as a preconditioner in another iterative solution algorithm such as the [[conjugate gradient method]] or [[GMRES]].\n\n== Definition ==\nFor a given matrix <math> A \\in \\R^{n \\times n} </math> one defines the graph <math> G(A) </math> as\n:<math>\n  G(A) := \\left\\lbrace (i,j) \\in \\N^2 : A_{ij} \\neq 0 \\right\\rbrace \\,,\n</math>\nwhich is used to define the conditions a ''sparsity patterns'' <math> S </math> needs to fulfill\n:<math>\n  S \\subset \\left\\lbrace 1, \\dots , n \\right\\rbrace^2\n  \\,, \\quad\n  \\left\\lbrace (i,i) : 1 \\leq i \\leq n \\right\\rbrace \\subset S\n  \\,, \\quad\n  G(A) \\subset S \n  \\,.\n</math>\n\nA decomposition of the form <math> A = LU - R </math> where the following hold\n* <math> L \\in \\R^{n \\times n} </math> is a [[Triangular_matrix#Unitriangular_matrix|lower unitriangular]] matrix \n* <math> U \\in \\R^{n \\times n} </math> is an [[Triangular_matrix|upper triangular]] matrix \n* <math> L,U </math> are zero outside of the sparsity pattern: <math> L_{ij}=U_{ij}=0 \\quad \\forall \\; (i,j) \\notin S </math>\n* <math> R \\in \\R^{n \\times n} </math> is zero within the sparsity pattern: <math> R_{ij}=0 \\quad \\forall \\; (i,j) \\in S </math>\nis called an '''incomplete LU decomposition''' (w.r.t. the sparsity pattern <math> S </math>).\n\nThe sparsity pattern of ''L'' and ''U'' is often chosen to be the same as the sparsity pattern of the original matrix ''A''. If the underlying matrix structure can be referenced by pointers instead of copied, the only extra memory required is for the entries of ''L'' and ''U''. This preconditioner is called ILU(0).\n\n== Stability ==\nConcerning the stability of the ILU the following theorem was proven by Meijerink and van der Vorst<ref>{{Cite journal|last=Meijerink|first=J. A.|last2=Vorst|first2=Van Der|last3=A|first3=H.|date=1977|title=An iterative solution method for linear systems of which the coefficient matrix is a symmetric 𝑀-matrix|url=http://www.ams.org/home/page/|journal=Mathematics of Computation|language=en-US|volume=31|issue=137|pages=148–162|doi=10.1090/S0025-5718-1977-0438681-4|issn=0025-5718}}</ref>.\n\nLet <math> A </math> be an [[M-matrix]], the (complete) LU decomposition given by <math> A=\\hat{L} \\hat{U} </math>, and the ILU by <math> A=LU-R </math>.\nThen\n:<math>\n  |L_{ij}| \\leq |\\hat{L}_{ij}|\n  \\quad \\forall \\; i,j\n</math>\nholds.\nThus, the ILU is at least as stable as the (complete) LU decomposition.\n\n== Generalizations ==\nOne can obtain a more accurate preconditioner by allowing some level of extra fill in the factorization. A common choice is to use the sparsity pattern of ''A<sup>2</sup>'' instead of ''A''; this matrix is appreciably more dense than ''A'', but still sparse over all. This preconditioner is called ILU(1). One can then generalize this procedure; the ILU(k) preconditioner of a matrix ''A'' is the incomplete LU factorization with the sparsity pattern of the matrix ''A<sup>k+1</sup>''.\n\nMore accurate ILU preconditioners require more memory, to such an extent that eventually the running time of the algorithm increases even though the total number of iterations decreases. Consequently, there is a cost/accuracy trade-off that users must evaluate, typically on a case-by-case basis depending on the family of linear systems to be solved.\n\nThe ILU factorization can be performed as a [[fixed-point iteration]] in a highly parallel way.<ref>{{cite journal|last1=Chow|first1=Edmond|last2=Patel|first2=Aftab|title=Fine-grained parallel incomplete LU factorization|journal=SIAM Journal on Scientific Computing|date=2015|volume=37|issue=2|page=C169-C193|ref=iterativeILU}}</ref>\n\n== See also ==\n* [[Incomplete Cholesky factorization]]\n\n==References==\n* {{Citation | last1=Saad | first1=Yousef | authorlink=Yousef Saad |title=Iterative methods for sparse linear systems | publisher=PWS | location=Boston | edition=1st | isbn=978-0-534-94776-7 | year=1996}}. See Section 10.3 and further.\n<references />\n\n\n==External links==\n* [http://www.cfd-online.com/Wiki/Incomplete_LU_factorization_-_ILU Incomplete LU Factorization on CFD Wiki]\n\n{{mathapplied-stub}}\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "Interpolative decomposition",
      "url": "https://en.wikipedia.org/wiki/Interpolative_decomposition",
      "text": "{{no footnotes|date=November 2016}}\n\nIn [[numerical analysis]], '''interpolative decomposition (ID)''' factors a [[matrix (mathematics)|matrix]] as the product of two matrices, one of which contains selected columns from the original matrix, and the other of which has a subset of columns consisting of the [[identity matrix]] and all its values are no greater than&nbsp;2 in absolute value.\n\n==Definition ==\nLet <math> A </math> be an <math> m \\times n </math> matrix of [[Rank (linear algebra)|rank]] <math> r </math>.  The matrix <math> A </math> can be written as\n:<math> A = A_{(:,J)} X , \\, </math>\nwhere\n* <math> J </math> is a subset of <math> r </math> indices from <math>\\{ 1 ,\\ldots, n  \\};</math>\n* The <math> m \\times r </math> matrix <math> A_{(:,J)} </math> represents <math> J</math>'s columns of <math> A;</math>\n* <math> X </math> is an <math> r \\times n </math> matrix, all of whose values are less than 2 in magnitude. <math> X </math>  has an <math> r \\times r </math> identity submatrix.\n\nNote that a similar decomposition can be done using the rows of <math> A </math> instead of its columns.\n\n== Example ==\nLet <math> A </math> be the <math> 3 \\times 3 </math> matrix of rank 2:\n\n:<math>\nA = \n    \\begin{bmatrix}\n        34  &  58  &  52 \\\\\n        59  &  89  &  80 \\\\\n        17  &  29  &  26\n    \\end{bmatrix}.\n</math>\n\nIf\n:<math>\nJ = \\begin{bmatrix}\n       2  & 1\n    \\end{bmatrix},\n</math>\n\nthen \n\n:<math>\nA = \n    \\begin{bmatrix}\n       58  & 34 \\\\\n       89  & 59 \\\\\n       29  & 17\n    \\end{bmatrix}  \n    \\begin{bmatrix}\n        0  &  1  &  \\frac{29}{33} \\\\\n        1  &  0  &  \\frac{1}{33}\n    \\end{bmatrix} \\approx\n    \\begin{bmatrix}\n       58  & 34 \\\\\n       89  & 59 \\\\\n       29  & 17\n    \\end{bmatrix}  \n    \\begin{bmatrix}\n        0  &  1  &  0.8788 \\\\\n        1  &  0  &  0.0303\n    \\end{bmatrix}.  \n</math>\n\n==Notes==\n<references/>\n\n== References ==\n* Cheng, Hongwei, Zydrunas Gimbutas, Per-Gunnar Martinsson, and Vladimir Rokhlin. \"[https://amath.colorado.edu/faculty/martinss/Pubs/2004_skeletonization.pdf On the compression of low rank matrices.]\" SIAM Journal on Scientific Computing 26, no. 4 (2005): 1389–1404.\n* Liberty, E., Woolfe, F., Martinsson, P. G., Rokhlin, V., & Tygert, M. (2007). [http://www.pnas.org/content/104/51/20167.full.pdf+html Randomized algorithms for the low-rank approximation of matrices]. Proceedings of the National Academy of Sciences, 104(51), 20167–20172.\n\n[[Category:Matrix decompositions]]\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "Inverse iteration",
      "url": "https://en.wikipedia.org/wiki/Inverse_iteration",
      "text": "In [[numerical analysis]], '''inverse iteration''' (also known as the ''inverse power method'') is an [[Iterative method|iterative]] [[eigenvalue algorithm]]. It allows one to find an approximate\n[[eigenvector]] when an approximation to a corresponding [[eigenvalue]] is already known.\nThe method is conceptually similar to  the [[power method]].\nIt appears to have originally been developed to compute resonance frequencies in the field of structural mechanics.\n<ref name=Pohlhausen>Ernst Pohlhausen, ''Berechnung der Eigenschwingungen statisch-bestimmter Fachwerke'', ZAMM - Zeitschrift für Angewandte\nMathematik und Mechanik 1, 28-42 (1921).</ref>\n\nThe inverse power iteration algorithm starts with an approximation <math>\\mu</math> for the [[eigenvalue]] corresponding to the desired [[eigenvector]] and a vector <math>b_0</math>, either a randomly selected vector or an approximation to the eigenvector. The method is described by the iteration\n\n:<math> b_{k+1} = \\frac{(A - \\mu I)^{-1}b_k}{C_k}, </math>\n\nwhere <math>C_k</math> are some constants usually chosen as <math>C_k= \\|(A - \\mu I)^{-1}b_k \\|. </math> Since eigenvectors are defined up to multiplication by constant, the choice of <math>C_k</math> can be arbitrary in theory; practical aspects of the choice of <math>C_k</math> are discussed below.\n\nAt every iteration, the vector <math>b_k</math> is multiplied by the matrix <math>(A - \\mu I)^{-1}</math> and normalized.\nIt is exactly the same formula as in the [[power method]], except replacing the matrix <math>A</math> by <math>(A - \\mu I)^{-1}. </math>\nThe closer the approximation <math>\\mu</math> to the eigenvalue is chosen, the faster the algorithm converges; however, incorrect choice of  <math>\\mu</math> can lead to slow convergence or to the convergence to an eigenvector other than the one desired. In practice, the method is used when a good approximation for the eigenvalue is known, and hence one needs only few (quite often just one) iterations.\n\n== Theory and convergence ==\n\nThe basic idea of the [[power iteration]] is choosing an initial vector <math>b</math> (either an [[eigenvector]] approximation or a [[random]] vector) and iteratively calculating <math>Ab, A^{2}b, A^{3}b,...</math>.  Except for a set of zero [[Measure (mathematics)|measure]], for any initial vector, the result will converge to an [[eigenvector]] corresponding to the dominant [[eigenvalue]].\n\nThe inverse iteration does the same for the matrix  <math>(A - \\mu I)^{-1}</math>, so it converges to the eigenvector corresponding to the dominant eigenvalue of the matrix <math>(A - \\mu I)^{-1}</math>. \nEigenvalues of this matrix are  <math>(\\lambda_1 - \\mu)^{-1},...,(\\lambda_n - \\mu)^{-1}, </math> where  <math> \\lambda_i </math> are eigenvalues of <math>A</math>.\nThe largest of these numbers corresponds to the smallest of <math>(\\lambda_1 - \\mu),...,(\\lambda_n - \\mu). </math> The eigenvectors of <math>A</math> and of <math>(A - \\mu I)^{-1}</math> are the same, since\n\n<math>\nAv=\\lambda v \\Leftrightarrow\n(A-\\mu I)v = \\lambda v - \\mu v \\Leftrightarrow\n(\\lambda - \\mu)^{-1} v = (A-\\mu I)^{-1} v\n</math>\n\n'''Conclusion''': The method converges to the eigenvector of the matrix <math>A</math> corresponding to the closest eigenvalue to  <math>\\mu .</math>\n\nIn particular, taking <math>\\mu=0</math> we see that  <math>(A)^{-1}b_k </math>\nconverges to the eigenvector corresponding to the eigenvalue of <math>A</math> with the smallest absolute value {{clarify|date=October 2016}}.\n\n=== Speed of  convergence  ===\n\nLet us analyze the [[rate of convergence]] of the method.\n\nThe [[power method]] is known to [[Rate of convergence#Convergence speed for iterative methods|converge linearly]] to the limit, more precisely:\n\n<math> \\mathrm{Distance}( b^\\mathrm{ideal},  b^{k}_\\mathrm{Power~Method})=O \\left(   \\left| \\frac{\\lambda_\\mathrm{subdominant} }{\\lambda_\\mathrm{dominant} } \\right|^k \\right), </math>\n\nhence for the inverse iteration method similar result sounds as:\n\n<math> \\mathrm{Distance}( b^\\mathrm{ideal},  b^{k}_\\mathrm{Inverse~iteration})=O \\left(   \\left| \\frac{\\mu -\\lambda_{\\mathrm{closest~ to~ }\\mu}   }{\\mu - \\lambda_{\\mathrm{second~ closest~ to~} \\mu} } \\right|^k \\right). </math>\n\nThis is a key formula for understanding the method's convergence. It shows that if <math>\\mu</math> is chosen close enough to some  eigenvalue  <math>\\lambda </math>, for example <math> \\mu- \\lambda = \\epsilon </math> each iteration will improve the accuracy <math> |\\epsilon| /|\\lambda +\\epsilon - \\lambda_{\\mathrm{closest~ to~} \\lambda} |  </math>  times. (We use that for small enough <math>\\epsilon</math> \"closest to <math>\\mu</math>\" and \"closest to <math>\\lambda </math>\" is the same.) For small enough  <math> |\\epsilon|</math> it is approximately the same as  <math> |\\epsilon| /|\\lambda  - \\lambda_{\\mathrm{closest~ to~} \\lambda}|  </math>. Hence if one is able to find <math>\\mu </math>, such that the \n<math>\\epsilon </math> will be small enough, then very few iterations may be satisfactory.\n\n=== Complexity ===\n\nThe inverse iteration algorithm requires solving a [[System of linear equations|linear system]] or calculation of the inverse matrix.\nFor non-structured matrices (not sparse, not Toeplitz,...) this requires <math>O(n^{3})</math> operations.\n\n== Implementation options ==\n\nThe method is defined by the formula:\n\n:<math> b_{k+1} = \\frac{(A - \\mu I)^{-1}b_k}{C_k}, </math>\n\nThere are, however, multiple options for its implementation.\n\n=== Calculate inverse matrix or solve system of linear equations ===\n\nWe can rewrite the formula in the following way:\n\n:<math> (A - \\mu I) b_{k+1} = \\frac{b_k}{C_k}, </math>\n\nemphasizing that to find the next approximation  <math> b_{k+1} </math> we may solve a system of linear equations. \nThere are two options: one may choose an algorithm that solves a linear system, or one may calculate the inverse <math>(A - \\mu I)^{-1}</math> and then apply it to the vector.\nBoth options have complexity ''O(n<sup>3</sup>)'', the exact number depends on the chosen method.\n\nThe choice depends also on the number of iterations. Naively, if at each iteration one solves a linear system, the complexity will be ''k*O(n<sup>3</sup>)'', where ''k'' is number of iterations; similarly, calculating the inverse matrix and applying it at each iteration is of complexity ''k*O(n<sup>3</sup>)''.\nNote, however, that if the eigenvalue estimate <math>\\mu</math> remains constant, then we may reduce the complexity to ''O(n<sup>3</sup>) + k*O(n<sup>2</sup>)'' with either method.\nCalculating the inverse matrix once, and storing it to apply at each iteration is of complexity ''O(n<sup>3</sup>) + k*O(n<sup>2</sup>)''.\nStoring an [[LU decomposition]] of <math>(A - \\mu I)</math> and using [[Triangular matrix#Forward and back substitution|forward and back substitution]] to solve the system of equations at each iteration is also of complexity ''O(n<sup>3</sup>) + k*O(n<sup>2</sup>)''.\n\nInverting the matrix will typically have a greater initial cost, but lower cost at each iteration. Conversely, solving systems of linear equations will typically have a lesser initial cost, but require more operations for each iteration.\n\n=== Tridiagonalization, [[Hessenberg form]] ===\n\nIf it is necessary to perform many iterations (or few iterations, but for many eigenvectors), then it might be wise to bring the matrix to the \nupper [[Hessenberg form]] first (for symmetric matrix this will be [[tridiagonal matrix|tridiagonal form]]).  Which costs <math>\\begin{matrix}\\frac{10}{3}\\end{matrix} n^3 + O(n^2)</math> arithmetic operations using a technique based on [[Householder transformation|Householder reduction]]), with a finite sequence of orthogonal similarity transforms, somewhat like a two-sided QR decomposition.<ref name=Demmel>{{citation\n | last = Demmel | first = James W. | authorlink = James Demmel\n | mr = 1463942\n | isbn = 0-89871-389-7\n | location = Philadelphia, PA\n | publisher = [[Society for Industrial and Applied Mathematics]]\n | title = Applied Numerical Linear Algebra\n | year = 1997}}.</ref><ref name=Trefethen>Lloyd N. Trefethen and David Bau, ''Numerical Linear Algebra'' (SIAM, 1997).</ref>  (For QR decomposition, the Householder rotations are multiplied only on the left, but for the Hessenberg case they are multiplied on both left and right.) For  [[symmetric matrix|symmetric matrices]] this procedure costs <math>\\begin{matrix}\\frac{4}{3}\\end{matrix} n^3 + O(n^2)</math> arithmetic operations using  a technique based on Householder reduction.<ref name=Demmel/><ref name=Trefethen/>\n\nSolution of the system of linear equations for the [[tridiagonal matrix]]\ncosts <math>O(n)</math> operations, so the complexity grows like <math>O(n^3) + kO(n)</math>, where <math>k</math> is the iteration number, which is better than for the direct inversion. However, for few iterations such transformation may not be practical.\n\nAlso transformation to the [[Hessenberg form]] involves square roots and the division operation, which are not universally supported by hardware.\n\n=== Choice of the normalization constant <math>C_k</math> ===\n\nOn general purpose processors (e.g. produced by Intel) the execution time of addition, multiplication and division is approximately equal. But on embedded and/or low energy consuming hardware ([[digital signal processor]]s, [[FPGA]], [[Application-specific integrated circuit|ASIC]]) division may not be supported by hardware, and so should be avoided. \nChoosing <math>C_k=2^{n_k}</math> allows fast division without explicit hardware support, as division by a power of 2 may be implemented as either a [[Bitwise operation#Bit shifts|bit shift]] (for [[fixed-point arithmetic]]) or subtraction of <math>k</math> from the exponent (for [[Floating point|floating-point arithmetic]]).\n\nWhen implementing the algorithm using [[fixed-point arithmetic]], the choice of the constant <math>C_k</math> is especially important. Small values will lead to fast growth of the norm of <math>b_k</math> and to [[Integer overflow|overflow]]; large values of <math>C_k</math> will cause the vector <math>b_k</math> to tend toward zero.\n\n==  Usage   ==\n\nThe main application of the method is the situation when an approximation to an eigenvalue is found and one needs to find the corresponding approximate eigenvector. In such a situation the inverse iteration is the main and probably the only method to use.\n\n=== Methods to find approximate eigenvalues ===\n\nSo typically the method is used in combination with some other method which finds approximate eigenvalues: the standard example is the [[bisection eigenvalue algorithm]], another example is the [[Rayleigh quotient iteration]], which is actually the same inverse iteration with the choice of the approximate eigenvalue as the [[Rayleigh quotient]] corresponding to the vector obtained on the previous step of the iteration.\n\nThere are some situations where the method can be used by itself, however they are quite marginal.\n\n=== Norm of matrix as approximation to the ''dominant'' eigenvalue ===\n\nThe dominant eigenvalue can be easily estimated for any matrix. \nFor any [[Matrix norm#Induced norm|induced norm]] it is true that\n<math>\\left \\| A \\right \\| \\ge |\\lambda| , </math> for any eigenvalue <math>\\lambda</math>. \nSo taking the norm of the matrix as an approximate eigenvalue one can see that the method will converge to the dominant eigenvector.\n\n=== Estimates based on statistics ===\n\nIn some real-time applications one needs to find  eigenvectors  for matrices with  a speed of millions of matrices per second. In such applications, typically the statistics of matrices is known in advance and one can take as an approximate eigenvalue the average eigenvalue for some large matrix sample.\nBetter, one may calculate the mean ratio of the eigenvalues to the trace or the norm of the  matrix and estimate the average eigenvalue as the trace or norm multiplied by the average value of that ratio.  Clearly such a method can be used only with discretion and only when high precision is not critical. \nThis approach of estimating an average eigenvalue can be combined with other methods to avoid excessively large error.\n\n== See also ==\n*[[Power iteration]]\n*[[Rayleigh quotient iteration]]\n* [[List of numerical analysis topics#Eigenvalue algorithms|List of eigenvalue algorithms]]\n\n==References==\n{{reflist}}\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "Iterative refinement",
      "url": "https://en.wikipedia.org/wiki/Iterative_refinement",
      "text": "{{about|iterative refinement in mathematics|iterative refinement in software development|Iterative and incremental development}}\n'''Iterative refinement''' is an [[iterative method]] proposed by James H. Wilkinson to improve the accuracy of numerical solutions to [[systems of linear equations]].\n\nWhen solving a linear system {{math|'''<var>Ax</var>''' {{=}} '''<var>b</var>'''}}, due to the presence of [[Round-off error|rounding error]]s, the computed solution {{math|'''<var>x</var>&#x302;'''}} may sometimes deviate from the exact solution {{math|'''<var>x</var>'''<sup>*</sup>}}. Starting with {{math|'''<var>x</var>'''<sub>1</sub> {{=}} '''<var>x</var>&#x302;'''}}, iterative refinement computes a sequence {{math|{{mset|'''<var>x</var>'''<sub>1</sub>,'''<var>x</var>'''<sub>2</sub>,'''<var>x</var>'''<sub>3</sub>,…}}}} which converges to {{math|'''<var>x</var>'''<sup>*</sup>}} when certain assumptions are met.\n\n==Description==\nFor {{math|<var>m</var> {{=}} 1,2,&hellip;}}, the <math>m</math>th iteration of iterative refinement consists of three steps:\n# Compute the residual<br>{{math|'''<var>r</var>'''<sub><var>m</var></sub> {{=}} '''<var>b</var>''' &minus; '''<var>Ax</var>'''<sub><var>m</var></sub>}}\n# Solve the system<br>{{math|'''<var>Ad</var>'''<sub><var>m</var></sub> {{=}} '''<var>r</var>'''<sub><var>m</var></sub>}}\n# Add the correction<br>{{math|'''<var>x</var>'''<sub><var>m</var>+1</sub> {{=}} '''<var>x</var>'''<sub><var>m</var></sub> + '''<var>d</var>'''<sub><var>m</var></sub>}}\n\n==Error analysis==\nAs a rule of thumb, iterative refinement for [[Gaussian elimination]] produces a solution correct to working precision if double the working precision is used in the computation of {{math|<var>r</var>}}, e.g. by using [[Quadruple-precision floating-point format|quad]] or [[extended precision|double extended]] precision [[IEEE 754]] [[floating point]], and if {{math|<var>A</var>}} is not too ill-conditioned (and the iteration  and the rate of convergence are determined by the condition number of {{math|<var>A</var>}}).<ref>{{cite book|first=Nicholas | last=Higham |title=Accuracy and Stability of Numerical Algorithms (2 ed)| publisher=SIAM|year=2002 | pages=232  }}</ref>\n\nMore formally, assuming that each solve step is reasonably accurate, i.e., in mathematical terms, for every {{math|<var>m</var>}}, we have\n\n:{{math|'''<var>A</var>'''('''<var>I</var>''' + <var>'''F'''<sub>m</sub></var>)<var>'''d'''<sub>m</sub></var> {{=}} <var>'''r'''<sub>m</sub></var>}}\n\nwhere {{math|&#x2016;<var>'''F'''<sub>m</sub></var>&#x2016;<sub>&infin;</sub> < 1}}, the [[Approximation error|relative error]] in the <math>m</math>th iterate of iterative refinement satisfies\n\n:<math>\\frac{\\lVert\\boldsymbol{x}_m-\\boldsymbol{x}^\\ast\\rVert_\\infty}{\\lVert\\boldsymbol{x}^\\ast\\rVert_\\infty}\\leq\\bigl(\\sigma\\kappa_\\infty(\\boldsymbol{A})\\varepsilon_1\\bigr)^m+\\mu_1\\varepsilon_1+\\mu_2n\\kappa_\\infty(\\boldsymbol{A})\\varepsilon_2</math>\n\nwhere\n* {{math|&#x2016;&middot;&#x2016;<sub>&infin;</sub>}} denotes the [[Uniform norm|{{math|&infin;}}-norm]] of a vector,\n* {{math|<var>&kappa;</var><sub>&infin;</sub>('''<var>A</var>''')}} is the {{math|&infin;}}-[[condition number]] of {{math|'''<var>A</var>'''}},\n* <math>n</math> is the order of {{math|'''<var>A</var>'''}},\n* {{math|<var>&epsilon;</var><sub>1</sub>}} and {{math|<var>&epsilon;</var><sub>2</sub>}} are [[Machine epsilon|unit round-offs]] of [[Floating point|floating-point]] arithmetic operations,\n* {{math|<var>&sigma;</var>}}, {{math|<var>&mu;</var><sub>1</sub>}} and {{math|<var>&mu;</var><sub>2</sub>}} are constants depending on {{math|'''<var>A</var>'''}}, {{math|<var>&epsilon;</var><sub>1</sub>}} and {{math|<var>&epsilon;</var><sub>2</sub>}}\nif {{math|'''<var>A</var>'''}} is “not too badly conditioned”, which in this context means\n\n:{{math|0 &lt; <var>&sigma;&kappa;</var><sub>&infin;</sub>('''<var>A</var>''')<var>&epsilon;</var><sub>1</sub> &#x226a; 1}}\n\nand implies that {{math|<var>&mu;</var><sub>1</sub>}} and {{math|<var>&mu;</var><sub>2</sub>}} are of order unity.\n\nThe distinction of {{math|<var>&epsilon;</var><sub>1</sub>}} and {{math|<var>&epsilon;</var><sub>2</sub>}} is intended to allow mixed-precision evaluation of {{math|'''<var>r</var>'''<sub><var>m</var></sub>}} where intermediate results are computed with unit round-off {{math|<var>&epsilon;</var><sub>2</sub>}} before the final result is rounded (or truncated) with unit round-off {{math|<var>&epsilon;</var><sub>1</sub>}}. All other computations are assumed to be carried out with unit round-off {{math|<var>&epsilon;</var><sub>1</sub>}}.\n\n==Notes==\n{{Reflist}}\n\n==References==\n*{{cite book|title=Rounding Errors in Algebraic Processes|first=James H.|last=Wilkinson|publisher=[[Prentice Hall]]|location= Englewood Cliffs, NJ|year=1963}}\n*{{cite journal|title=Iterative Refinement in Floating Point|first=Cleve B.|last=Moler|publisher=[[Association for Computing Machinery]]|location=New York, NY|date=April 1967|journal=[[Journal of the ACM]]|volume=14|issue=2|pages=316&ndash;321|url=http://portal.acm.org/citation.cfm?id=321394|doi=10.1145/321386.321394}}\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]\n\n\n{{comp-sci-stub}}"
    },
    {
      "title": "Jacobi eigenvalue algorithm",
      "url": "https://en.wikipedia.org/wiki/Jacobi_eigenvalue_algorithm",
      "text": "In [[numerical linear algebra]], the '''Jacobi eigenvalue algorithm''' is an [[iterative method]] for the calculation of the [[eigenvalue]]s and [[eigenvector]]s of a [[real number|real]] [[symmetric matrix]] (a process known as [[Matrix diagonalization#Diagonalization|diagonalization]]). It is named after [[Carl Gustav Jacob Jacobi]], who first proposed the method in 1846,<ref>{{cite journal\n |last=Jacobi |first=C.G.J. |authorlink=Carl Gustav Jacob Jacobi\n |url=http://gdz.sub.uni-goettingen.de/dms/load/img/?PID=GDZPPN002144522\n |title=Über ein leichtes Verfahren, die in der Theorie der Säkularstörungen vorkommenden Gleichungen numerisch aufzulösen\n |journal=[[Crelle's Journal]]\n |volume=30 |issue=30 |year=1846 |pages=51–94\n |language=German\n|doi=10.1515/crll.1846.30.51 }}</ref> but only became widely used in the 1950s with the advent of computers.<ref>{{cite journal\n |first=G.H. |last=Golub \n |first2=H.A. |last2=van der Vorst\n |title=Eigenvalue computation in the 20th century\n |journal=Journal of Computational and Applied Mathematics\n |volume=123 |issue=1–2 |year=2000 |pages=35&ndash;65\n |doi=10.1016/S0377-0427(00)00413-1\n}}</ref>\n\n== Description ==\nLet <math>S</math> be a symmetric matrix, and <math>G=G(i,j,\\theta)</math> be a [[Givens rotation|Givens rotation matrix]]. Then:\n\n:<math>S'=G S G^\\top \\, </math>\n\nis symmetric and [[similar (linear algebra)|similar]] to <math>S</math>.\n\nFurthermore, <math>S^\\prime</math> has entries:\n\n:<math>\\begin{align}\n S'_{ii} &= c^2\\, S_{ii}  -  2\\, s c \\,S_{ij}  +  s^2\\, S_{jj} \\\\\n S'_{jj} &= s^2 \\,S_{ii}  +  2 s c\\, S_{ij}  +  c^2 \\, S_{jj} \\\\\n S'_{ij} &= S'_{ji} = (c^2 - s^2 ) \\, S_{ij}  +  s c \\, (S_{ii} - S_{jj} ) \\\\\n S'_{ik} &= S'_{ki} = c \\, S_{ik}  -  s \\, S_{jk} & k \\ne i,j \\\\\n S'_{jk} &= S'_{kj} = s \\, S_{ik}  + c \\, S_{jk} & k \\ne i,j \\\\\n S'_{kl} &= S_{kl} &k,l \\ne i,j\n\\end{align}</math>\n\nwhere <math>s=\\sin(\\theta)</math> and <math>c=\\cos(\\theta)</math>.\n\nSince <math>G</math> is orthogonal, <math>S</math> and <math>S^\\prime</math> have the same [[Frobenius norm]] <math>||\\cdot||_F</math> (the square-root sum of squares of all components), however we can choose <math>\\theta</math> such that <math>S^\\prime_{ij}=0</math>, in which case <math>S^\\prime</math> has a larger sum of squares on the diagonal:\n\n:<math> S'_{ij} = \\cos(2\\theta) S_{ij} + \\tfrac{1}{2} \\sin(2\\theta) (S_{ii} - S_{jj}) </math>\n\nSet this equal to 0, and rearrange:\n\n:<math> \\tan(2\\theta) = \\frac{2 S_{ij}}{S_{jj} - S_{ii}} </math>\n\nif <math>  S_{jj} = S_{ii} </math>\n\n:<math> \\theta = \\frac{\\pi} {4}  </math>\n\nIn order to optimize this effect, ''S''<sub>''ij''</sub> should be the [[off-diagonal element]] with the largest absolute value, called the ''pivot''.\n\nThe Jacobi eigenvalue method repeatedly [[Jacobi rotation|performs rotations]] until the matrix becomes almost diagonal. Then the elements in the diagonal are approximations of the (real) eigenvalues of ''S''.\n\n== Convergence ==\n\nIf  <math> p = S_{kl} </math>  is a pivot element, then by definition  <math> |S_{ij} | \\le |p| </math>  for  <math> 1 \\le i, j \\le n, i \\ne j</math> . Since <math>S</math> has exactly  <math> 2N := n(n-1) </math>  off-diag elements, we have  <math>  p^2 \\le \\Gamma(S )^2 \\le 2 N p^2 </math>  or <math>  2 p^2 \\ge \\Gamma(S )^2 / N </math> .  This implies\n\t<math>  \\Gamma(S^J )^2  \\le  (1 - 1 / N ) \\Gamma (S )^2  </math>   or  <math>  \\Gamma (S^ J )  \\le (1 - 1 / N )^{1 / 2} \\Gamma(S ) \t</math>,\ni.e. the sequence of Jacobi rotations converges at least linearly by a factor  <math>  (1 - 1 / N )^{1 / 2} </math>  to a diagonal matrix.\n\nA number of <math> N </math> Jacobi rotations is called a sweep; let <math> S^{\\sigma} </math> denote the result. The previous estimate yields\n: <math>  \\Gamma(S^{\\sigma} )  \\le  \\left(1 - \\frac{1}{N} \\right)^{N / 2} \\Gamma(S )  </math>,\ni.e. the sequence of sweeps converges at least linearly with a factor  ≈ <math>  e ^{1 / 2}</math> .\n\nHowever the following result of [[Arnold Schönhage|Schönhage]]<ref>{{cite journal\n |last=Schönhage |first=A.\n |title=Zur quadratischen Konvergenz des Jacobi-Verfahrens\n |journal=Numerische Mathematik\n |volume=6 |issue=1 |year=1964 |pages=410–412\n |language=German\n |doi=10.1007/BF01386091 |mr=174171\n}}</ref> yields locally quadratic convergence. To this end let ''S'' have ''m'' distinct eigenvalues  <math>  \\lambda_1, ... , \\lambda_m </math>  with multiplicities  <math>  \\nu_1, ... , \\nu_m </math>  and let  ''d'' > 0 be the smallest distance of two different eigenvalues. Let us call a number of\n\n: <math>  N_S := \\frac{n (n - 1)}{2}  - \\sum_{\\mu = 1}^{m} \\frac{1}{2} \\nu_{\\mu} (\\nu_{\\mu} - 1) \\le N  </math>\n\nJacobi rotations a Schönhage-sweep. If  <math> S^ s </math>  denotes the result then\n: <math>  \\Gamma(S^ s ) \\le\\sqrt{\\frac{n}{2} - 1} \\left(\\frac{\\gamma^2}{d - 2\\gamma}\\right), \\quad \\gamma :=  \\Gamma(S )  </math>  .\n\nThus convergence becomes quadratic as soon as \n<math> \\Gamma(S ) < \\frac{d}{2 + \\sqrt{\\frac{n}{2} - 1}} </math>\n\n== Cost ==\n\nEach Jacobi rotation can be done in O(''n'') steps when the pivot element ''p'' is known. However the search for ''p'' requires inspection of all ''N''&nbsp;≈&nbsp;½&nbsp;''n''<sup>2</sup> off-diagonal elements. We can reduce this to O(''n'') complexity too if we introduce an additional index array  <math> m_1, \\, \\dots \\, , \\, m_{n - 1} </math>  with the property that <math> m_i </math>  is the index of the largest element in row ''i'', (''i'' = 1, …, ''n''&nbsp;&minus;&nbsp;1) of the current ''S''. Then the indices of the pivot (''k'', ''l'')  must be one of the pairs <math>  (i, m_i) </math>. Also the updating of the index array can be done in O(''n'') average-case complexity: First, the maximum entry in the updated rows ''k'' and ''l'' can be found in O(''n'') steps. In the other rows ''i'', only the entries in columns ''k'' and ''l'' change. Looping over these rows, if <math>  m_i </math> is neither ''k'' nor ''l'', it suffices to compare the old maximum at <math>  m_i </math> to the new entries and update <math>  m_i </math> if necessary. If <math>  m_i </math> should be equal to ''k'' or ''l'' and the corresponding entry decreased during the update, the maximum over row ''i'' has to be found from scratch in O(''n'') complexity. However, this will happen on average only once per rotation. Thus, each rotation has  O(''n'') and one sweep O(''n''<sup>3</sup>) average-case complexity, which is equivalent to one matrix multiplication. Additionally the  <math> m_i </math>  must be initialized before the process starts, which can be done in ''n''<sup>2</sup> steps.\n\nTypically the Jacobi method converges within numerical precision after a small number of sweeps. Note that multiple eigenvalues reduce the number of iterations since <math>N_S < N</math>.\n\n== Algorithm ==\n\nThe following algorithm is a description of the Jacobi method in math-like notation.\nIt calculates a vector ''e'' which contains the eigenvalues and a matrix ''E'' which contains the corresponding eigenvectors, i.e. <math> e_i </math>  is an eigenvalue and the column <math> E_i </math>  an orthonormal eigenvector for <math> e_i </math>,  ''i'' = 1, …, ''n''.\n\n '''procedure''' jacobi(''S'' ∈ '''R'''<sup>''n''×''n''</sup>; '''out''' ''e'' ∈ '''R'''<sup>''n''</sup>; '''out''' ''E'' ∈ '''R'''<sup>''n''×''n''</sup>)\n   '''var'''\n     ''i'', ''k'', ''l'', ''m'', ''state'' ∈ '''N'''\n     ''s'', ''c'', ''t'', ''p'', ''y'', ''d'', ''r'' ∈ '''R'''\n     ''ind'' ∈ '''N'''<sup>''n''</sup>\n     ''changed'' ∈ '''L'''<sup>''n''</sup>\n \n   '''function''' maxind(''k'' ∈ '''N''') ∈ '''N''' ! ''index of largest off-diagonal element in row k''\n     ''m'' := ''k''+1\n     '''for''' ''i'' := ''k''+2 '''to''' ''n'' '''do'''\n       '''if''' │''S''<sub>''ki''</sub>│ > │''S''<sub>''km''</sub>│ '''then''' ''m'' := ''i'' '''endif'''\n     '''endfor'''\n     '''return''' ''m''\n   '''endfunc'''\n \n   '''procedure''' update(''k'' ∈ '''N'''; ''t'' ∈ '''R''') ! ''update e<sub>k</sub> and its status''\n     ''y'' := ''e''<sub>''k''</sub>; ''e''<sub>''k''</sub> := ''y''+''t''\n     '''if''' ''changed''<sub>''k''</sub> and (''y''=''e''<sub>''k''</sub>) '''then''' ''changed''<sub>''k''</sub> := false; ''state'' := ''state''−1\n     '''elsif''' (not ''changed''<sub>''k''</sub>) and (''y''≠''e''<sub>''k''</sub>) '''then''' ''changed''<sub>''k''</sub> := true; ''state'' := ''state''+1\n     '''endif'''\n   '''endproc'''\n \n   '''procedure''' rotate(''k'',''l'',''i'',''j'' ∈ '''N''') ! ''perform rotation of S<sub>ij</sub>, S<sub>kl</sub>''\n     ┌ <sub>  </sub>┐    ┌     ┐┌ <sub>  </sub>┐\n     │''S''<sub>''kl''</sub>│    │''c''  −''s''││''S''<sub>''kl''</sub>│\n     │ <sub>  </sub>│ := │     ││ <sub>  </sub>│\n     │''S''<sub>''ij''</sub>│    │''s''   ''c''││''S''<sub>''ij''</sub>│\n     └ <sub>  </sub>┘    └     ┘└ <sub>  </sub>┘\n   '''endproc'''\n \n   ! ''init e, E, and arrays ind, changed''\n   ''E'' := ''I''; ''state'' := ''n''\n   '''for''' ''k'' := 1 '''to''' ''n'' '''do''' ''ind''<sub>''k''</sub> := maxind(''k''); ''e''<sub>''k''</sub> := ''S''<sub>''kk''</sub>; ''changed''<sub>''k''</sub> := true '''endfor'''\n   '''while''' ''state''≠0 '''do''' ! ''next rotation''\n     ''m'' := 1 ! ''find index (k,l) of pivot p''\n     '''for''' ''k'' := 2 '''to''' ''n''−1 '''do'''\n       '''if''' │''S''<sub>''k''&nbsp;''ind''</sub><sub><sub>''k''</sub></sub>│ > │''S''<sub>''m''&nbsp;''ind''</sub><sub><sub>''m''</sub></sub>│ '''then''' ''m'' := ''k'' '''endif'''\n     '''endfor'''\n     ''k'' := ''m''; ''l'' := ''ind''<sub>''m''</sub>; ''p'' := ''S''<sub>''kl''</sub>\n     ! ''calculate c = cos &phi;, s = sin &phi;''\n     ''y'' := (''e''<sub>''l''</sub>−''e''<sub>''k''</sub>)/2; ''d'' := │''y''│+√(''p''<sup>2</sup>+''y''<sup>2</sup>)\n     ''r'' := √(''p''<sup>2</sup>+''d''<sup>2</sup>); ''c'' := ''d''/''r''; ''s'' := ''p''/''r''; ''t'' := ''p''<sup>2</sup>/''d''\n     '''if''' ''y''<0 '''then''' ''s'' := −''s''; ''t'' := −''t'' '''endif'''\n     ''S''<sub>''kl''</sub> := 0.0; update(''k'',−''t''); update(''l'',''t'')\n     ! ''rotate rows and columns k and l\n     '''for''' ''i'' := 1 '''to''' ''k''−1 '''do''' rotate(''i'',''k'',''i'',''l'') '''endfor'''\n     '''for''' ''i'' := ''k''+1 '''to''' ''l''−1 '''do''' rotate(''k'',''i'',''i'',''l'') '''endfor'''\n     '''for''' ''i'' := ''l''+1 '''to''' ''n'' '''do''' rotate(''k'',''i'',''l'',''i'') '''endfor'''\n     ! ''rotate eigenvectors''\n     '''for''' ''i'' := 1 '''to''' ''n'' '''do'''\n       ┌ <sub>  </sub>┐    ┌     ┐┌ <sub>  </sub>┐\n       │''E''<sub>''ik''</sub>│    │''c''  −''s''││''E''<sub>''ik''</sub>│\n       │ <sub>  </sub>│ := │     ││ <sub>  </sub>│\n       │''E''<sub>''il''</sub>│    │''s''   ''c''││''E''<sub>''il''</sub>│\n       └ <sub>  </sub>┘    └     ┘└ <sub>  </sub>┘\n     '''endfor'''\n     ! ''rows k, l have changed, update rows ind<sub>k</sub>, ind<sub>l</sub>''\n     ''ind''<sub>''k''</sub> := maxind(''k''); ''ind''<sub>''l''</sub> := maxind(''l'')\n   '''loop'''\n '''endproc'''\n\n=== Notes ===\n\n1. The logical array ''changed'' holds the status of each eigenvalue. If the numerical value of <math> e_k </math>  or <math> e_l </math>  changes during an iteration, the corresponding component of  ''changed'' is set to ''true'', otherwise to ''false''. The integer ''state''  counts the number of components of ''changed'' which have the value ''true''. Iteration stops as soon as ''state'' = 0. This means that none of the approximations <math> e_1,\\, ...\\, , e_n </math>  has recently changed its value and thus it is not very likely that this will happen if iteration continues. Here it is assumed that floating point operations are optimally rounded to the nearest floating point number.\n\n2. The upper triangle of the matrix ''S'' is destroyed while the lower triangle and the diagonal are unchanged. Thus it is possible to restore ''S'' if necessary according to\n\n '''for''' ''k'' := 1 '''to''' ''n''−1 '''do''' ! ''restore matrix S''\n   '''for''' ''l'' := ''k''+1 '''to''' ''n'' '''do''' ''S''<sub>''kl''</sub> := ''S''<sub>''lk''</sub> '''endfor'''\n '''endfor'''\n\n3. The eigenvalues are not necessarily in descending order. This can be achieved by a simple sorting algorithm.\n\n '''for''' ''k'' := 1 '''to''' ''n''−1 '''do'''\n   ''m'' := ''k''\n   '''for''' ''l'' := ''k''+1 '''to''' ''n'' '''do'''\n     '''if''' ''e''<sub>''l''</sub> > ''e''<sub>''m''</sub> '''then''' ''m'' := ''l'' '''endif'''\n   '''endfor'''\n   '''if''' ''k'' ≠ ''m'' '''then''' swap ''e''<sub>''m''</sub>,''e''<sub>''k''</sub>; swap ''E''<sub>''m''</sub>,''E''<sub>''k''</sub> '''endif'''\n '''endfor'''\n\n4. The algorithm is written using matrix notation (1 based arrays instead of 0 based).\n\n5. When implementing the algorithm, the part specified using matrix notation must be performed simultaneously.\n\n6. This implementation does not correctly account for the case in which one dimension is an independent subspace.  For example, if given a diagonal matrix, the above implementation will never terminate, as none of the eigenvalues will change.  Hence, in real implementations, extra logic must be added to account for this case.\n\n=== Example ===\n\nLet \n<math>\n\tS = \\begin{pmatrix} 4 & -30 & 60 & -35 \\\\ -30 & 300 & -675 & 420 \\\\ 60 & -675 & 1620 & -1050 \\\\ -35 & 420 & -1050 & 700 \\end{pmatrix}\n</math>\n\nThen ''jacobi'' produces the following eigenvalues and eigenvectors after 3 sweeps (19 iterations) :\n\n<math>\n\te_1 = 2585.25381092892231\n</math>\n\n<math>\n\tE_1 = \\begin{pmatrix}0.0291933231647860588\\\\ -0.328712055763188997\\\\ 0.791411145833126331\\\\ -0.514552749997152907\\end{pmatrix}\n</math>\n\n<math>\n\te_2 = 37.1014913651276582\n</math>\n\n<math>\n\tE_2 = \\begin{pmatrix}-0.179186290535454826\\\\ 0.741917790628453435\\\\ -0.100228136947192199\\\\ -0.638282528193614892\\end{pmatrix}\n</math>\n\n<math>\n\te_3 = 1.4780548447781369\n</math>\n\n<math>\n\tE_3 = \\begin{pmatrix}-0.582075699497237650\\\\ 0.370502185067093058\\\\ 0.509578634501799626\\\\ 0.514048272222164294\\end{pmatrix}\n</math>\n\n<math>\n\te_4 = 0.1666428611718905\n</math>\n\n<math>\n\tE_4 = \\begin{pmatrix}0.792608291163763585\\\\ 0.451923120901599794\\\\ 0.322416398581824992\\\\ 0.252161169688241933\\end{pmatrix}\n</math>\n\n== Applications for real symmetric matrices ==\n\nWhen the eigenvalues (and eigenvectors) of a symmetric matrix are known, the following\nvalues are easily calculated.\n\n;Singular values\n:The singular values of a (square) matrix ''A'' are the square roots of the (non-negative) eigenvalues of <math> A^T A </math>. In case of a symmetric matrix ''S'' we have of <math> S^T S = S^2 </math>, hence the singular values of ''S'' are the absolute values of the eigenvalues of ''S''\n\n;2-norm and spectral radius\n:The 2-norm of a matrix ''A'' is the norm based on the Euclidean vectornorm, i.e. the largest value <math> \\| A x\\|_2 </math> when x runs through all vectors with <math> \\|x\\|_2 = 1 </math>. It is the largest singular value of ''A''. In case of a symmetric matrix it is largest absolute value of its eigenvectors and thus equal to its spectral radius.\n\n;Condition number\n:The condition number of a nonsingular matrix ''A'' is defined as  <math> \\mbox{cond} (A) = \\| A \\|_2 \\| A^{-1}\\|_2 </math>. In case of a symmetric matrix it is the absolute value of the quotient of the largest and smallest eigenvalue. Matrices with large condition numbers can cause numerically unstable results: small perturbation can result in large errors. [[Hilbert matrix|Hilbert matrices]] are the most famous ill-conditioned matrices. For example, the fourth-order Hilbert matrix has a condition of 15514, while for order 8 it is 2.7&nbsp;&times;&nbsp;10<sup>8</sup>.\n\n;Rank\n:A matrix ''A'' has rank ''r'' if it has ''r'' columns that are linearly independent while the remaining columns are linearly dependent on these. Equivalently, ''r'' is the dimension of the range of&nbsp;''A''. Furthermore it is the number of nonzero singular values.\n:In case of a symmetric matrix r is the number of nonzero eigenvalues. Unfortunately because of rounding errors numerical approximations of zero eigenvalues may not be zero (it may also happen that a numerical approximation is zero while the true value is not). Thus one can only calculate the ''numerical'' rank by making a decision which of the eigenvalues are close enough to zero.\n\n;Pseudo-inverse\n:The pseudo inverse of a matrix ''A'' is the unique matrix <math> X = A^+ </math> for which ''AX'' and ''XA'' are symmetric and for which ''AXA = A,  XAX = X''  holds. If ''A'' is nonsingular, then '<math> A^+ = A^{-1} </math>.\n:When procedure jacobi (S, e, E) is called, then the relation <math> S = E^T \\mbox{Diag} (e) E </math> holds where Diag(''e'')  denotes the diagonal matrix with vector ''e'' on the diagonal. Let <math> e^+ </math> denote the vector where  <math> e_i </math> is replaced by <math> 1/e_i </math> if  <math> e_i \\le 0 </math> and by 0  if  <math> e_i </math> is (numerically close to) zero. Since matrix ''E'' is orthogonal, it follows that the pseudo-inverse of S is given by   <math> S^+ = E^T \\mbox{Diag} (e^+) E </math>.\n\n;Least squares solution\n:If matrix ''A''  does not have full rank, there may not be a solution of the linear system  ''Ax = b''. However one can look for a vector x for which  <math> \\| Ax - b \\|_2 </math> is minimal. The solution is <math> x = A^+ b </math>. In case of a symmetric matrix ''S'' as before, one has <math> x = S^+ b = E^T \\mbox{Diag} (e^+) E b </math>.\n\n;Matrix exponential\n:From <math> S = E^T \\mbox{Diag} (e) E </math> one finds <math> \\exp S = E^T \\mbox{Diag} (\\exp e) E </math> where  exp&nbsp;''e''  is the vector where <math> e_i </math> is replaced by <math> \\exp e_i </math>. In the same way, ''f''(''S'')  can be calculated in an obvious way for any (analytic) function ''f''.\n\n;Linear differential equations\n:The differential equation  ''x'&nbsp;'' =&nbsp;''Ax'', ''x''(0) = ''a'' has the solution  ''x''(''t'') = exp(''t&nbsp;A'')&nbsp;''a''. For a symmetric matrix ''S'', it follows that  <math> x(t) = E^T \\mbox{Diag} (\\exp t e) E a </math>. If  <math> a = \\sum_{i = 1}^n a_i E_i </math> is the expansion of ''a''  by the eigenvectors of ''S'', then <math> x(t) = \\sum_{i = 1}^n a_i \\exp(t e_i) E_i </math>.\n:Let  <math> W^s </math> be the vector space spanned by the eigenvectors of ''S'' which correspond to a negative eigenvalue and <math> W^u </math> analogously for the positive eigenvalues. If  <math> a \\in W^s </math> then <math> \\mbox{lim}_{t \\ \\infty} x(t) = 0 </math> i.e. the equilibrium point 0 is attractive to ''x''(''t''). If <math> a \\in W^u </math> then <math> \\mbox{lim}_{t \\ \\infty} x(t) = \\infty </math>, i.e. 0 is repulsive to  ''x''(''t''). <math> W^s </math> and <math> W^u </math> are called ''stable'' and ''unstable'' manifolds for ''S''. If ''a'' has components in both manifolds, then one component is attracted and one component is repelled. Hence ''x''(''t'') approaches <math> W^u </math> as <math> t \\to \\infty </math>.\n\n==  Generalizations  ==\n\nThe Jacobi Method has been generalized to [[Jacobi method for complex Hermitian matrices|complex Hermitian matrices]], general nonsymmetric real and complex matrices as well as block matrices.\n\nSince singular values of a real matrix are the square roots of the eigenvalues of the symmetric matrix <math> S = A^T A</math> it can also be used for the calculation of these values. For this case, the method is modified in such a way that ''S'' must not be explicitly calculated which reduces the danger of [[round-off error]]s. Note that <math> J S J^T = J A^T A J^T = J A^T J^T J A J^T = B^T B  </math>  with  <math> B \\, := J A J^T </math> .\n\nThe Jacobi Method is also well suited for parallelism.\n\n==  References  ==\n{{reflist}}\n\n== Further reading ==\n{{refbegin}}\n*{{Citation | last1=Press | first1=WH | last2=Teukolsky | first2=SA | last3=Vetterling | first3=WT | last4=Flannery | first4=BP | year=2007 | title=Numerical Recipes: The Art of Scientific Computing | edition=3rd | publisher=Cambridge University Press |  location=New York | isbn=978-0-521-88068-8 | chapter=Section 11.1. Jacobi Transformations of a Symmetric Matrix | chapter-url=http://apps.nrbook.com/empanel/index.html#pg=570}}\n* {{cite journal\n |last=Rutishauser |first=H.\n |title=Handbook Series Linear Algebra: The Jacobi method for real symmetric matrices. \n |journal=Numerische Mathematik\n |volume=9 |issue=1 |year=1966 |pages=1–10\n |doi=10.1007/BF02165223 |mr=1553948\n}}\n* {{cite journal\n |last=Sameh |first=A.H.\n |title=On Jacobi and Jacobi-like algorithms for a parallel computer\n |journal=[[Mathematics of Computation]]\n |volume=25 |issue=115 |year=1971 |pages=579–590\n |mr=297131 | jstor = 2005221 | doi = 10.1090/s0025-5718-1971-0297131-6 \n}}\n* {{cite journal\n |last=Shroff |first=Gautam M.\n |title=A parallel algorithm for the eigenvalues and eigenvectors of a general complex matrix\n |journal=Numerische Mathematik\n |volume=58 |issue=1 |year=1991 |pages=779–805\n |doi=10.1007/BF01385654 |mr=1098865\n|citeseerx=10.1.1.134.3566\n }}\n* {{cite journal\n |last=Veselić |first=K.\n |title=On a class of Jacobi-like procedures for diagonalising arbitrary real matrices\n |journal=Numerische Mathematik\n |volume=33 |issue=2 |year=1979 |pages=157–172\n |doi=10.1007/BF01399551 |mr=549446\n}}\n* {{cite journal\n |last=Veselić |first=K.\n |last2=Wenzel |first2=H. J.\n |title=A quadratically convergent Jacobi-like method for real matrices with complex eigenvalues\n |journal=Numerische Mathematik\n |volume=33 |issue=4 |year=1979 |pages=425–435\n |doi=10.1007/BF01399324 |mr=553351\n}}\n\n{{refend}}\n\n== External links ==\n*[https://groups.google.com/group/sci.math.num-analysis/msg/8282d0d412f72d2e Matlab implementation of Jacobi algorithm that avoids trigonometric functions]\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]\n[[Category:Articles with example pseudocode]]"
    },
    {
      "title": "Jacobi method for complex Hermitian matrices",
      "url": "https://en.wikipedia.org/wiki/Jacobi_method_for_complex_Hermitian_matrices",
      "text": "In mathematics, the '''Jacobi method for complex [[Hermitian matrices]]''' is a generalization of the [[Jacobi eigenvalue algorithm|Jacobi iteration method]].  The [[Jacobi eigenvalue algorithm|Jacobi iteration method]] is also explained in \"Introduction to Linear Algebra\" by {{harvtxt|Strang|1993}}.\n\n== Derivation ==\nThe complex [[unitary matrices|unitary]] [[Rotation matrix|rotation]] matrices ''R''<sub>''pq''</sub> can be used for [[Jacobi eigenvalue algorithm|Jacobi iteration]] of complex [[Hermitian matrices]] in order to find a numerical estimation of their eigenvectors and eigenvalues simultaneously.\n\nSimilar to the [[Givens rotation|Givens rotation matrices]], ''R''<sub>''pq''</sub> are defined as:\n:<math>  \n\\begin{align}\n(R_{pq})_{m,n} & = \\delta_{m,n}  & \\qquad m,n \\ne p,q, \\\\[10pt]\n(R_{pq})_{p,p} & = \\frac{+1}{\\sqrt{2}} e^{-i\\theta},  \\\\[10pt]\n(R_{pq})_{q,p} & = \\frac{+1}{\\sqrt{2}} e^{-i\\theta}, \\\\[10pt]\n(R_{pq})_{p,q} & = \\frac{-1}{\\sqrt{2}} e^{+i\\theta}, \\\\[10pt]\n(R_{pq})_{q,q} & = \\frac{+1}{\\sqrt{2}} e^{+i\\theta}\n\\end{align}\n</math>\n\nEach rotation matrix, ''R''<sub>''pq''</sub>, will modify only the ''p''th and ''q''th rows or columns of a matrix ''M'' if it is applied from left or right, respectively:\n\n:<math>\n\\begin{align}\n(R_{pq} M)_{m,n} & =\n\\begin{cases}\nM_{m,n} & m \\ne p,q \\\\[8pt]\n\\frac{1}{\\sqrt{2}} (M_{p,n} e^{-i\\theta} - M_{q,n} e^{+i\\theta}) & m = p \\\\[8pt]\n\\frac{1}{\\sqrt{2}} (M_{p,n} e^{-i\\theta} + M_{q,n} e^{+i\\theta}) & m = q\n\\end{cases} \\\\[8pt]\n(MR_{pq}^\\dagger)_{m,n} & =\n\\begin{cases}\nM_{m,n} & n \\ne p,q \\\\\n\\frac{1}{\\sqrt{2}} (M_{m,p} e^{+i\\theta} - M_{m,q} e^{-i\\theta}) & n = p \\\\[8pt]\n\\frac{1}{\\sqrt{2}} (M_{m,p} e^{+i\\theta} + M_{m,q} e^{-i\\theta}) & n = q\n\\end{cases}\n\\end{align}\n</math>\n\nA [[Hermitian matrix]], ''H'' is defined by the conjugate transpose symmetry property:\n\n:<math>  H^\\dagger = H \\ \\Leftrightarrow\\ H_{i,j} = H^{*}_{j,i} </math>\n\nBy definition, the complex conjugate of a complex [[unitary matrices|unitary]] [[Rotation matrix|rotation]] matrix, ''R'' is its inverse and also a complex  [[unitary matrices|unitary]] [[Rotation matrix|rotation]] matrix:\n\n:<math>\n\\begin{align}\nR^\\dagger_{pq} & = R^{-1}_{pq} \\\\[6pt]\n\\Rightarrow\\ R^{\\dagger^\\dagger}_{pq} & = R^{-1^\\dagger}_{pq} = R^{-1^{-1}}_{pq} = R_{pq}.\n\\end{align}\n</math>\n\nHence, the complex equivalent [[Givens rotation|Givens transformation]] <math>T</math> of a [[Hermitian matrices|Hermitian matrix]] ''H'' is also a [[Hermitian matrices|Hermitian matrix]] similar to ''H'':\n\n:<math>  \n\\begin{align}\nT & \\equiv R_{pq} H R^\\dagger_{pq}, & & \\\\[6pt]\nT^\\dagger & = (R_{pq} H R^\\dagger_{pq})^\\dagger = R^{\\dagger^\\dagger}_{pq} H^\\dagger R^\\dagger_{pq} = R_{pq} H R^\\dagger_{pq} = T\n\\end{align}\n</math>\n\nThe elements of ''T'' can be calculated by the relations above. The important elements for the [[Jacobi eigenvalue algorithm|Jacobi iteration]] are the following four:\n\n:<math>  \n\\begin{array}{clrcl}\nT_{p,p} & = & & \\frac{H_{p,p} + H_{q,q}}{2} & - \\ \\ \\ \\mathrm{Re}\\{H_{p,q} e^{-2i\\theta}\\}, \\\\[8pt]\nT_{p,q} & = & & \\frac{H_{p,p} - H_{q,q}}{2} & + \\ i \\ \\mathrm{Im}\\{H_{p,q} e^{-2i\\theta}\\}, \\\\[8pt]\nT_{q,p} & = & & \\frac{H_{p,p} - H_{q,q}}{2} & - \\ i \\ \\mathrm{Im}\\{H_{p,q} e^{-2i\\theta}\\}, \\\\[8pt]\nT_{q,q} & = & & \\frac{H_{p,p} + H_{q,q}}{2} & + \\ \\ \\ \\mathrm{Re}\\{H_{p,q} e^{-2i\\theta}\\}. \n\\end{array}\n</math>\n\nEach [[Jacobi eigenvalue algorithm|Jacobi iteration]] with ''R''<sup>''J''</sup><sub>''pq''</sub> generates a transformed matrix, ''T''<sup>''J''</sup>, with ''T''<sup>''J''</sup><sub>''p'',''q''</sub>&nbsp;=&nbsp;0.  The rotation matrix ''R''<sup>''J''</sup><sub>''p'',''q''</sub> is defined as a product of two complex [[unitary matrices|unitary]] [[Givens rotation|rotation]] matrices.\n\n:<math>  \n\\begin{align}\nR^J_{pq} & \\equiv R_{pq}(\\theta_2)\\, R_{pq}(\\theta_1),\\text{ with} \\\\[8pt]\n\\theta_1 & \\equiv \\frac{2\\phi_1 - \\pi}{4} \\text{ and } \\theta_2 \\equiv \\frac{\\phi_2}{2},\n\\end{align}\n</math>\n\nwhere the phase terms, <math>\\phi_1</math> and <math>\\phi_2</math> are given by:\n\n:<math>\n\\begin{align}\n\\tan \\phi_1 & = \\frac{\\mathrm{Im}\\{H_{p,q}\\}}{\\mathrm{Re}\\{H_{p,q}\\}}, \\\\[8pt]\n\\tan \\phi_2 & = \\frac{2 |H_{p,q}|}{H_{p,p} - H_{q,q}}.\n\\end{align}\n</math>\n\nFinally, it is important to note that the product of two complex rotation matrices for given angles ''&theta;''<sub>1</sub> and ''&theta;''<sub>2</sub> cannot be transformed into a single complex unitary rotation matrix ''R''<sub>''pq''</sub>(''&theta;''). The product of two complex rotation matrices are given by:\n\n:<math>\n\\begin{align}\n\\left[ R_{pq}(\\theta_2)\\, R_{pq}(\\theta_1) \\right]_{m,n} =\n\\begin{cases}\n\\ \\ \\ \\ \\delta_{m,n}                        &  m,n \\ne p,q, \\\\[8pt]\n-i e^{-i\\theta_1}\\, \\sin{\\theta_2}          & m = p \\text{ and } n = p, \\\\[8pt]\n- e^{+i\\theta_1}\\, \\cos{\\theta_2}          & m = p \\text{ and } n = q, \\\\[8pt]\n\\ \\ \\ \\ e^{-i\\theta_1}\\, \\cos{\\theta_2}     & m = q \\text{ and } n = p, \\\\[8pt]\n+i e^{+i\\theta_1}\\, \\sin{\\theta_2}          & m = q \\text{ and } n = q. \n\\end{cases}\n\\end{align}\n</math>\n\n== References ==\n\n* {{Citation | last=Strang | first=G. | authorlink=Gilbert Strang | title=Introduction to Linear Algebra | publisher=Wellesley Cambridge Press | location = MA | year=1993}}.\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "Jacobi rotation",
      "url": "https://en.wikipedia.org/wiki/Jacobi_rotation",
      "text": "In [[numerical linear algebra]], a '''Jacobi rotation''' is a [[rotation (mathematics)|rotation]], ''Q''<sub>''k''ℓ</sub>, of a 2-dimensional linear subspace of an ''n-''dimensional [[inner product space]], chosen to zero a symmetric pair of off-[[main diagonal|diagonal]] entries of an ''n''×''n'' [[real number|real]] [[symmetric matrix]], ''A'', when applied as a [[Similar matrix|similarity transformation]]:\n\n: <math> A \\mapsto Q_{k\\ell}^T A Q_{k\\ell} = A' . \\,\\! </math>\n\n: <math>\n\\begin{bmatrix}\n {*} &   &   & \\cdots &   &   & * \\\\\n   & \\ddots &   &   &   &   &   \\\\\n   &   & a_{kk} & \\cdots & a_{k\\ell} &   &   \\\\\n \\vdots &   & \\vdots & \\ddots & \\vdots &   & \\vdots \\\\\n   &   & a_{\\ell k} & \\cdots & a_{\\ell\\ell} &   &   \\\\\n   &    &   &   &   & \\ddots &   \\\\\n {*} &   &   & \\cdots &   &   & *\n\\end{bmatrix}\n\\to\n\\begin{bmatrix}\n {*} &   &   & \\cdots &   &   & * \\\\\n   & \\ddots &   &   &   &   &   \\\\\n   &   & a'_{kk} & \\cdots & 0 &   &   \\\\\n \\vdots &   & \\vdots & \\ddots & \\vdots &   & \\vdots \\\\\n   &   & 0 & \\cdots & a'_{\\ell\\ell} &   &   \\\\\n   &    &   &   &   & \\ddots &   \\\\\n {*} &   &   & \\cdots &   &   & *\n\\end{bmatrix}.\n</math>\n\nIt is the core operation in the [[Jacobi eigenvalue algorithm]], which is [[numerically stable]] and well-suited to implementation on [[parallel processor]]s {{Fact|date=March 2013}}.\n\nOnly rows ''k'' and ℓ and columns ''k'' and ℓ of ''A'' will be affected, and that ''A''&prime; will remain symmetric. Also, an explicit matrix for ''Q''<sub>''k''ℓ</sub> is rarely computed; instead, auxiliary values are computed and ''A'' is updated in an efficient and numerically stable way. However, for reference, we may write the matrix as\n\n: <math>\nQ_{k\\ell} = \n\\begin{bmatrix}\n 1 &   &   &   &   &   &   \\\\\n   & \\ddots &   &   &   & 0 &   \\\\\n   &   & c & \\cdots & -s &   &   \\\\\n   &   & \\vdots & \\ddots & \\vdots &   &  \\\\\n   &   & s & \\cdots & c &   &   \\\\\n   & 0 &   &   &   & \\ddots &   \\\\\n   &   &   &   &   &   & 1\n\\end{bmatrix} .\n</math>\n\nThat is, ''Q''<sub>''k''ℓ</sub> is an identity matrix except for four entries, two on the diagonal (''q''<sub>''kk''</sub> and ''q''<sub>ℓℓ</sub>, both equal to ''c'') and two symmetrically placed off the diagonal (''q''<sub>''k''ℓ</sub> and ''q''<sub>ℓ''k''</sub>, equal to ''s'' and −''s'', respectively). Here ''c''&nbsp;= cos&nbsp;ϑ and ''s''&nbsp;= sin&nbsp;ϑ for some angle ϑ; but to apply the rotation, the angle itself is not required. Using [[Kronecker delta]] notation, the matrix entries can be written\n\n: <math> q_{ij} = \n\\delta_{ij} + (\\delta_{ik}\\delta_{jk} \n+ \\delta_{i\\ell}\\delta_{j\\ell})(c-1) + (\\delta_{ik}\\delta_{j\\ell} \n- \\delta_{i\\ell}\\delta_{jk})s . \\,\\!\n</math>\n\nSuppose ''h'' is an index other than ''k'' or ℓ (which must themselves be distinct). Then the similarity update produces, algebraically,\n\n: <math> a'_{hk} = a'_{kh} = c a_{hk} - s a_{h\\ell} \\,\\! </math>\n: <math> a'_{h\\ell} = a'_{\\ell h} = c a_{h\\ell} + s a_{hk} \\,\\! </math>\n\n: <math> a'_{k\\ell} = a'_{\\ell k} = (c^2-s^2)a_{k\\ell} + sc (a_{kk} - a_{\\ell\\ell}) = 0 \\,\\! </math>\n\n: <math> a'_{kk} = c^2 a_{kk} + s^2 a_{\\ell\\ell} - 2 s c a_{k\\ell} \\,\\! </math>\n: <math> a'_{\\ell\\ell} = s^2 a_{kk} + c^2 a_{\\ell\\ell} + 2 s c a_{k\\ell}. \\,\\! </math>\n\n== Numerically stable computation ==\nTo determine the quantities needed for the update, we must solve the off-diagonal equation for zero {{Harv|Golub|Van Loan|1996|loc=§8.4}}. This implies that\n\n: <math> \\frac{c^2-s^2}{sc} = \\frac{a_{\\ell\\ell} - a_{kk}}{a_{k\\ell}} . </math>\n\nSet β to half of this quantity,\n\n: <math> \\beta = \\frac{a_{\\ell\\ell} - a_{kk}}{2 a_{k\\ell}} . </math>\n\nIf ''a''<sub>''k''ℓ</sub> is zero we can stop without performing an update, thus we never divide by zero. Let ''t'' be tan&nbsp;ϑ. Then with a few trigonometric identities we reduce the equation to\n\n: <math> t^2 + 2\\beta t - 1 = 0 . \\,\\! </math>\n\nFor stability we choose the solution\n\n: <math> t = \\frac{\\sgn(\\beta)}{|\\beta|+\\sqrt{\\beta^2+1}} . </math>\n\nFrom this we may obtain ''c'' and ''s'' as\n\n: <math> c = \\frac{1}{\\sqrt{t^2+1}} \\,\\! </math>\n: <math> s = c t \\,\\! </math>\n\nAlthough we now could use the algebraic update equations given previously, it may be preferable to rewrite them. Let\n\n: <math> \\rho= \\frac{s}{1+c} , </math>\n\nso that ρ&nbsp;= tan(ϑ/2). Then the revised update equations are\n\n: <math> a'_{hk} = a'_{kh} = a_{hk} - s (a_{h\\ell} + \\rho a_{hk}) \\,\\! </math>\n: <math> a'_{h\\ell} = a'_{\\ell h} = a_{h\\ell} + s (a_{hk} - \\rho a_{h\\ell}) \\,\\! </math>\n\n: <math> a'_{k\\ell} = a'_{\\ell k} = 0 \\,\\! </math>\n\n: <math> a'_{kk} = a_{kk} - t a_{k \\ell} \\,\\! </math>\n: <math> a'_{\\ell\\ell} = a_{\\ell\\ell} + t a_{k \\ell} \\,\\! </math>\n\nAs previously remarked, we need never explicitly compute the rotation angle ϑ. In fact, we can reproduce the symmetric update determined by ''Q''<sub>''k''ℓ</sub> by retaining only the three values ''k'', ℓ, and ''t'', with ''t'' set to zero for a null rotation.\n\n== See also ==\n* [[Givens rotation]]\n\n== References ==\n* {{citation\n  | last1 = Golub\n  | first1 = Gene H.\n  | author1-link = Gene H. Golub\n  | last2 = Van Loan\n  | first2 = Charles F.\n  | author2-link = Charles F. Van Loan\n  | title = Matrix Computations\n  | edition = 3rd\n  | publisher = Johns Hopkins University Press\n  | date = 1996\n  | location = Baltimore\n  | isbn = 978-0-8018-5414-9 }}\n\n<!-- The main source for this presentation was [http://beige.ucs.indiana.edu/B673/node24.html]. -->\n\n{{Numerical linear algebra}}\n\n{{DEFAULTSORT:Jacobi Rotation}}\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "Julia (programming language)",
      "url": "https://en.wikipedia.org/wiki/Julia_%28programming_language%29",
      "text": "{{Use dmy dates|date=October 2015}}\n{{Infobox programming language\n| name = Julia\n| logo = Julia prog language.svg\n| logo caption = \n| screenshot = <!-- filename is enough -->\n| screenshot caption = \n| paradigm = [[multi-paradigm programming language|Multi-paradigm]]: [[multiple dispatch]], [[procedural programming|procedural]], [[functional programming|functional]], [[metaprogramming|meta]], [[multistaged programming|multistaged]]<ref>{{cite web |url=https://medium.com/@acidflask/smoothing-data-with-julia-s-generated-functions-c80e240e05f3#.615wk3dle |title=Smoothing data with Julia's @generated functions |quote=Julia's generated functions are closely related to the multistaged programming (MSP) paradigm popularized by Taha and Sheard, which generalizes the compile time/run time stages of program execution by allowing for multiple stages of delayed code execution. |date=5 November 2015 |accessdate=9 December 2015}}</ref>\n| released = {{Start date and age|2012}}<ref name=\"announcement\"/>\n| designer = Jeff Bezanson, [[Alan Edelman]], [[Stefan Karpinski]], [[Viral B. Shah]]\n| developer = Jeff Bezanson, [[Stefan Karpinski]], [[Viral B. Shah]], and other contributors<ref name=\"license\">{{cite web\n  | url = https://github.com/JuliaLang/julia/blob/master/LICENSE.md\n  | title = LICENSE.md\n  | publisher = [[GitHub]]\n  }}</ref><ref>{{cite web\n  | url = https://github.com/JuliaLang/julia/graphs/contributors\n  | title = Contributors to JuliaLang/julia\n  | publisher = [[GitHub]]\n  }}</ref>\n| latest release version = 1.1.1<ref>{{cite web|author= |url=https://github.com/JuliaLang/julia/releases/tag/v1.1.1 |title=v1.1.1 |website=Github.com |date=2019-05-16 |accessdate=2019-05-16}}</ref>\n| latest release date = {{Start date and age|2019|05|16|df=yes}}\n| latest preview version = 1.2.0-rc1<ref>{{Cite web|url=https://github.com/JuliaLang/julia/releases/tag/v1.2.0-rc1|title=v1.2.0-rc1|website=GitHub|language=en|access-date=2019-05-30}}</ref> / 1.3.0-DEV<ref>{{Cite web|url=https://github.com/JuliaLang/julia/pull/31660|title=Set VERSION to 1.3-DEV by ararslan · Pull Request #31660 · JuliaLang/julia|website=GitHub|language=en|access-date=2019-04-10}}</ref>\n| latest preview date = {{Start date and age|2019|05|30|df=yes}} / daily updates\n| typing = [[dynamic programming language|Dynamic]], [[nominal type system|nominative]], [[parametric polymorphism|parametric]], [[optional typing|optional]]\n| implementations = \n| dialects = \n| influenced by = {{startflatlist}}\n* [[C (programming language)|C]]<ref name=\"announcement\"/>\n* [[Lisp (programming language)|Lisp]]<ref name=\"announcement\"/><!--only the parser implemented by femtolisp, a [[Scheme (programming language)|Scheme]] implementation (see: \"unsupported\" \"julia --lisp\" that invokes it, but its only influence on the language/syntax – Lisp-like macros? Maybe not even Lisp's CLOS \"multimethods\"? as Dylan with \"multiple dispatch\" was cut from the list-->\n* [[Lua (programming language)|Lua]]<ref name=\"Introduction\">{{Cite web|url=https://docs.julialang.org/en/stable/|title=<!--Chapter: Introduction under --> Home · The Julia Language|website=docs.julialang.org|language=en|access-date=2018-08-15}}</ref>\n* [[Mathematica]]<ref name=\"announcement\"/> (strictly its [[Wolfram Language]]<ref name=\"announcement\"/><ref>{{cite web |url=https://fatiherikli.github.io/programming-language-network/ |title=Programming Language Network |publisher=GitHub |accessdate=6 December 2016}}</ref>)\n* [[MATLAB]]<ref name=\"announcement\"/>\n* [[Perl]]<ref name=\"Introduction\"/>\n* [[Python (programming language)|Python]]<ref name=\"Introduction\"/>\n* [[R (programming language)|R]]<ref name=\"announcement\"/>\n* [[Ruby (programming language)|Ruby]]<ref name=\"Introduction\"/>\n* [[Scheme (programming language)|Scheme]]<ref name=\"JuliaCon2016\">{{cite web |url=http://www.juliacon.org |title=JuliaCon 2016 |publisher=JuliaCon |quote=\"He has co-designed the programming language Scheme, which has greatly influenced the design of Julia\"|accessdate=6 December 2016}}</ref>\n{{endflatlist}}\n| influenced = \n| programming language = Julia, [[C (programming language)|C]],<!--\"Remove when C11 is required for C code\" https://github.com/JuliaLang/julia/search?utf8=%E2%9C%93&q=C11&type=\n\n\"These roughly follows the c11/c++11 memory model [..] The only exception is the GC safepoint and GC state transitions [..] We use the compiler intrinsics to implement a similar API to the c11/c++11\" https://github.com/JuliaLang/julia/blob/44f3d7c921cbe50105a54258db2febb65a4b9d44/src/julia_threads.h\n--> [[C++]], [[Scheme (programming language)|Scheme]], [[LLVM]]<ref name=\"JuliaHome\">{{cite web |url=https://julialang.org/ |title=Julia |author=<!--Staff writer(s); no by-line.--> |date=<!--None listed.--> |website=Julia |publisher=NumFocus project |access-date=9 December 2016 |quote=Julia's Base library, largely written in Julia itself, also integrates mature, best-of-breed open source C and Fortran libraries for ...}}</ref>\n| platform = Tier 1: [[x86-64]], [[IA-32]], [[CUDA]]<br> Tier 2 and 3: [[AArch32|ARM]] and [[PowerPC]]\n| operating system = [[Linux]], [[macOS]], [[Microsoft Windows|Windows]] and <!-- no longer just community support for --> [[FreeBSD]]\n| license = [[MIT License|MIT]] (core),<ref name=\"license\"/><!--for core language while, by default, \"the environment, which consists of the language, user interfaces, and libraries, is under the GPL\"--> [[General Public Licence|GPL v2]];<ref name=\"JuliaHome\" /><ref>{{cite newsgroup|url=https://groups.google.com/forum/#!topic/julia-users/v4OjEK7azBs |title=Non-GPL Julia? |website=Groups.google.com |date= |accessdate=2017-05-31}}</ref><!--as some few libraries used by the standard library (that can be excluded) are [[copyleft]]--> a [[makefile]] option omits GPL libraries<ref>{{cite web |url=https://github.com/JuliaLang/julia/pull/10870 |title=Introduce USE_GPL_LIBS Makefile flag to build Julia without GPL libraries |quote=Note that this commit does not remove GPL utilities such as git and busybox that are included in the Julia binary installers on Mac and Windows.<!--dropped as of 0.5 then only libgit2--> It allows building from source with no GPL library dependencies.}}</ref>\n| file ext = .jl\n| website = {{Official website|https://JuliaLang.org|name=JuliaLang.org}}\n}}\n\n'''Julia''' is a [[high-level programming language|high-level]] [[general-purpose programming language|general-purpose]]<ref>{{cite web\n  | title      = The Julia Language\n  | type       = official website\n  | url        = https://julialang.org/\n  | quote      = General Purpose [..] Julia lets you write UIs, statically compile your code, or even deploy it on a webserver.\n  }}</ref> [[dynamic programming language]] designed for high-performance [[numerical analysis]] and [[computational science]].<ref>{{cite web\n  | date       = 15 October 2012\n  | last       = Bryant\n  | first      = Avi\n  | title      = Matlab, R, and Julia: Languages for data analysis\n  | url        = https://strata.oreilly.com/2012/10/matlab-r-julia-languages-for-data-analysis.html\n  | archiveurl = https://web.archive.org/web/20140426110631/https://strata.oreilly.com/2012/10/matlab-r-julia-languages-for-data-analysis.html\n  | archivedate= 2014-04-26\n  | publisher  = O'Reilly Strata\n  }}</ref><ref>{{cite web\n  | date       = 23 August 2015\n  | last       = Singh\n  | first      = Vicky\n  | title      = Julia Programming Language – A True Python Alternative\n  | url        = https://www.technotification.com/2018/08/julia-programming-language.html\n  | publisher  = Technotification\n  }}</ref><ref>{{cite web\n  | last       = Krill\n  | first      = Paul\n  | title      = New Julia language seeks to be the C for scientists\n  | url        = https://www.infoworld.com/d/application-development/new-julia-language-seeks-be-the-c-scientists-190818\n  | publisher  = InfoWorld\n  | date       = 18 April 2012\n  }}</ref><ref>{{cite web\n  | last       = Finley\n  | first      = Klint\n  | title      = Out in the Open: Man Creates One Programming Language to Rule Them All\n  | url        = https://www.wired.com/2014/02/julia/\n  | publisher  = Wired\n  | date       = 3 February 2014\n  }}</ref>  It is also useful for low-level [[system programming|systems programming]],<ref name=\"LowLevelSystemsProgrammingInJulia\">{{cite web |url=http://juliacon.org/2018/talks_workshops/42/\n|archiveurl=https://web.archive.org/web/20181105083419/http://juliacon.org/2018/talks_workshops/42/\n|archivedate=5 November 2018\n|first=Todd |last=Green\n|title=Low-Level Systems Programming in High-Level Julia\n|date=10 August 2018\n|accessdate=5 November 2018\n}}</ref> as a [[specification language]],<ref name=\"spec\">{{cite web |url=http://juliacon.org/talks.html#friday\n|archiveurl=https://web.archive.org/web/20150701182804/http://juliacon.org/talks.html\n|archivedate=1 July 2015\n|first=Robert |last=Moss\n|title=Using Julia as a Specification Language for the Next-Generation Airborne Collision Avoidance System\n|quote=[[Airborne collision avoidance system]]\n|date=26 June 2015\n|accessdate=29 June 2015\n}}</ref> with work being done on client<ref>{{Citation|last=Fischer|first=Keno|title=Running julia on wasm.|date=2019-02-03|url=https://github.com/Keno/julia-wasm|access-date=2019-02-09}}</ref> and server web use.<ref name=\"node-js\">{{cite web|url=https://node-julia.readme.io/|title=Getting Started with Node Julia · Node Julia|date=|website=Node-julia.readme.io|accessdate=2017-05-31}}</ref> \n\nDistinctive aspects of Julia's design include a type system with [[parametric polymorphism]] and types in a fully [[dynamic programming language]] and [[multiple dispatch]] as its core [[programming paradigm]]. It allows [[Concurrent computing|concurrent]], [[parallel computing|parallel]] and [[distributed computing]], and [[foreign function interface|direct calling]] of [[C (programming language)|C]] and [[Fortran]] libraries without [[Adapter pattern#gluecode|glue code]]. A [[Just-in-time compilation|just-in-time]] compiler that is referred to as \"just-[[Ahead-of-time compilation|ahead-of-time]]\"<ref>{{cite web |last1=Fischer |first1=Keno |last2=Nash |first2=Jameson |title=Growing a Compiler - Getting to Machine Learning from a General Purpose Compiler |url=https://juliacomputing.com/blog/2019/02/19/growing-a-compiler.html |website=Julia Computing Blog |accessdate=11 April 2019}}</ref> in the Julia community is used.\n\nJulia is [[Garbage collection (computer science)|garbage-collected]],<ref>{{cite newsgroup|url=https://groups.google.com/forum/#!topic/julia-users/6_XvoLBzN60 |title=Suspending Garbage Collection for Performance...good idea or bad idea? |website=Groups.google.com |date= |accessdate=2017-05-31}}</ref> uses [[eager evaluation]], and includes efficient libraries for [[floating-point]] calculations, [[linear algebra]], [[random number generation]], and [[regular expression]] matching. Many libraries are available, including some (e.g., for [[fast Fourier transform]]s) that were previously bundled with Julia and are now separate.<ref>(now available with <code>using [[FFTW]]</code> in current versions; that dependency is one of many moved out of the standard library to a package because it is GPL licensed, and thus is not included in Julia 1.0 by default.) {{Cite web|url=https://github.com/JuliaLang/julia/pull/21956|title=Remove the FFTW bindings from Base by ararslan · Pull Request #21956 · JuliaLang/julia|website=GitHub|language=en|access-date=2018-03-01}}</ref>\n\nTools available for Julia include [[integrated development environment|IDEs]]; with integrated tools, e.g. a [[lint (software)|linter]],<ref>{{Cite web|url=https://discourse.julialang.org/t/ann-linter-julia-plugin-for-atom-juno/2118|title=ANN: linter-julia plugin for Atom / Juno|date=2017-02-15|website=JuliaLang|language=en|access-date=2019-04-10}}</ref> debugger,<ref>{{Cite web|url=https://julialang.org/blog/2019/03/debuggers|title=A Julia interpreter and debugger|website=julialang.org|access-date=2019-04-10}}</ref> and the Rebugger.jl package \"supports [[interactive programming|repeated-execution debugging]]\"{{Efn|<!--quote= --> [With Rebugger.jl] you can:\n* test different modifications to the code or arguments as many times as you want; you are never forced to exit “debug mode” and save your file\n* run the same chosen block of code repeatedly (perhaps trying out different ways of fixing a bug) without needing to repeat any of the “setup” work that might have been necessary to get to some deeply nested method in the original call stack.<ref name=\"Rebugger\">{{Cite web|url=https://discourse.julialang.org/t/ann-rebugger-interactive-debugging-for-julia-0-7-1-0/13843|title=[ANN] Rebugger: interactive debugging for Julia 0.7/1.0|quote=<!--Better in the \"Efn\" footnote above?-->\n|date=2018-08-21|website=JuliaLang|language=en|access-date=2019-04-10}}</ref>}} and more.<ref>{{Cite web|url=https://timholy.github.io/Rebugger.jl/dev/|title=Home · Rebugger.jl|website=timholy.github.io|access-date=2019-04-10}}</ref>\n\n==History==\nWork on Julia was started in 2009, by Jeff Bezanson, [[Stefan Karpinski]], [[Viral B. Shah]], and [[Alan Edelman]], who set out to create a free language that was both high-level and fast. On 14 February 2012 the team launched a website with a blog post explaining the language's mission.<ref>{{cite web|last1=Jeff Bezanson, Stefan Karpinski, Viral Shah, Alan Edelman|title=Why We Created Julia|url=https://julialang.org/blog/2012/02/why-we-created-julia|website=JuliaLang.org|accessdate=5 June 2017}}</ref>  In an interview with InfoWorld in April 2012, Karpinski said of the name \"Julia\": \"There's no good reason, really. It just seemed like a pretty name.\"<ref>[[Stefan Karpinski]], [https://www.infoworld.com/article/2616709/application-development/new-julia-language-seeks-to-be-the-c-for-scientists.html New Julia language seeks to be the C for scientists], [[InfoWorld]], 18 April 2012</ref> Bezanson said he chose the name on the recommendation of a friend.<ref>{{cite web |last1=Torre |first1=Charles |title=Stefan Karpinski and Jeff Bezanson on Julia |url=https://channel9.msdn.com/Blogs/Charles/Stefan-Karpinski-and-Jeff-Bezanson-Julia-Programming-Language |website=Channel 9 |publisher=MSDN |accessdate=4 December 2018}}</ref>\n\nSince the 2012 launch, the Julia community has grown, with over 3,200,000 downloads as of January 2019<ref>{{Cite web|url=https://juliacomputing.com/blog/2019/01/04/january-newsletter.html |title=Julia Computing Newsletter, Growth Metrics|website=juliacomputing.com|access-date=2019-02-11}}</ref>. The Official Julia Docker images have seen over 1,000,000 downloads.<ref>https://hub.docker.com/_/julia</ref>. The JuliaCon<ref>{{cite web|url=http://juliacon.org/ |title=JuliaCon website |website=juliacon.org |accessdate=2018-05-10}}</ref> [[academic conference]] for Julia users and developers has been held annually since 2014.\n\nVersion 0.3 was released in August 2014, version 0.4 in October 2015, and version 0.5 in October 2016.<ref>[https://julialang.org/blog/ The Julia Blog]</ref>  Julia 0.6 was released in June 2017,<ref>https://julialang.org/blog/2017/06/julia-0.6-release</ref> and was the stable release version until 8 August 2018. Both Julia 0.7 (a useful release for testing packages, and knowing how to upgrade them for 1.0<ref>{{Cite web|url=https://discourse.julialang.org/t/what-is-julia-0-7-how-does-it-relate-to-1-0/9994|title=What is Julia 0.7? How does it relate to 1.0?|website=JuliaLang|language=en|access-date=2018-10-17}}</ref>) and  version 1.0 were released on 8 August 2018. Work on Julia 0.7 was a \"huge undertaking\" (e.g., because of \"entirely new optimizer\"), and some changes were made to the syntax (with the syntax now stable, and same for 1.x and 0.7) and semantics; the [[iteration]] interface was simplified.<ref>{{Cite web|url=https://julialang.org/blog/2018/07/iterators-in-julia-0.7 |title=Writing Iterators in Julia 0.7 |authors=Eric Davies |website=julialang.org |access-date=2018-08-05}}</ref>\n\nThe release candidate for Julia 1.0 (Julia 1.0.0-rc1) was released on 7 August 2018, and the final version a day later. Julia 1.1 was released on 21 January 2019 with, e.g., a new \"exception stack\" language feature. Bugfix releases are expected roughly monthly, for Julia 1.1.x and 1.0.x (1.0.x currently has [[long-term support]]; for at least a year) and Julia 1.0.1, 1.0.2, and 1.0.3 have followed that schedule (no such bugfix releases in the pipeline for 0.7-release). Julia 1.2 was due 15 March 2019;<ref>{{Citation|title=The Julia Language: A fresh approach to technical computing.: JuliaLang/julia|date=2019-01-22|url=https://github.com/JuliaLang/julia|publisher=The Julia Language|access-date=2019-01-22}}</ref> it's currently in feature freeze,<ref>{{Cite web|url=https://discourse.julialang.org/t/julia-1-2-feature-freeze-april-4th/22478/3|title=Julia 1.2 feature freeze April 4th|date=2019-04-03|website=JuliaLang|language=en|access-date=2019-05-06}}</ref> and Julia 1.3 is due July 15, 2019.<ref>{{Cite web|url=https://github.com/JuliaLang/julia/milestone/33|title=Julia Language 1.3 Milestone|access-date=2019-05-06}}</ref>\n\n===Notable uses===\nJulia has attracted some high-profile users, from investment manager [[BlackRock]], which uses it for [[time-series analytics]], to the British insurer [[Aviva]], which uses it for [[risk calculations]]. In 2015, the [[Federal Reserve Bank of New York]] used Julia to make models of the US economy, noting that the language made model estimation \"about 10 times faster\" than its previous [[MATLAB]] implementation. Julia's co-founders established Julia Computing in 2015 to provide paid support, training, and consulting services to clients, though Julia itself remains free to use. At the 2017 JuliaCon<ref>{{cite web|url=http://juliacon.org/2017/ |title=JuliaCon 2017 |website=juliacon.org |accessdate=2017-06-04}}</ref> conference, Jeffrey Regier, [[Keno Fischer]] and others announced<ref>{{cite web|last1=Fisher|first1=Keno|title=The Celeste Project|url=https://juliacon2017.sched.com/speaker/thecelesteproject|website=juliacon.org|accessdate=24 June 2017}}</ref> that the Celeste project<ref>{{cite arxiv|last1=Regier|first1=Jeffrey|last2=Pamnany|first2=Kiran|last3=Giordano|first3=Ryan|last4=Thomas|first4=Rollin|last5=Schlegel|first5=David|last6=McAulife|first6=Jon|last7=Prabat|title=Learning an Astronomical Catalog of the Visible Universe through Scalable Bayesian Inference|eprint=1611.03404|class=cs.DC|year=2016}}</ref> used Julia to achieve \"peak performance of 1.54&nbsp;[[FLOPS|petaFLOPS]] using 1.3 million threads\"<ref>{{cite press |url=https://juliacomputing.com/press/2017/09/12/julia-joins-petaflop-club.html |title=Julia Joins Petaflop Club |date=12 September 2017 | first=Andrew |last=Claster |work=Julia Computing |quote=Celeste is written entirely in Julia, and the Celeste team loaded an aggregate of 178 terabytes of image data to produce the most accurate catalog of 188 million [[astronomical object]]s in just 14.6 minutes [..] a performance improvement of 1,000x in single-threaded execution.}}</ref> on 9300 <!-- on 9,300 (vs. 8192 Xeon cores\") possibly \"optional nVidia\" GPU cards of Cori are used (or not), unclear conflated with lower numbers? --> [[Knights Landing (microarchitecture)|Knights Landing]] (KNL) nodes of the [[Cray XC40#United States|Cori (Cray XC40)]] supercomputer (the 5th fastest in the world at the time; by November 2017 was 8th [[TOP500#Top 10 ranking|fastest]]). Julia thus joins C, C++, and Fortran as high-level languages in which petaFLOPS computations have been achieved.\n\nThree of the Julia co-creators are the recipients of the 2019 [[J. H. Wilkinson Prize for Numerical Software|James H. Wilkinson Prize for Numerical Software]] (awarded every four years) \"for the creation of Julia, an innovative environment for the creation of high-performance tools that enable the analysis and solution of computational science problems.\"<ref>{{Cite web|url=https://news.mit.edu/2018/julia-language-co-creators-win-james-wilkinson-prize-numerical-software-1226|title=Julia language co-creators win James H. Wilkinson Prize for Numerical Software|website=MIT News|access-date=2019-01-22}}</ref>\n\n===Sponsors===\nJulia has received contributions from 800 developers worldwide.<ref>{{Citation|title=The Julia Language: A fresh approach to technical computing.: JuliaLang/julia|date=2019-01-26|url=https://github.com/JuliaLang/julia|publisher=The Julia Language|access-date=2019-01-26}}</ref> Dr. Jeremy Kepner at [[MIT Lincoln Laboratory]] was the founding sponsor of the Julia project in its early days. In addition, funds from the [[Gordon and Betty Moore Foundation]], the [[Alfred P. Sloan Foundation]], [[Intel]], and agencies such as [[National Science Foundation|NSF]], [[DARPA]], [[National Institutes of Health|NIH]], [[NASA]], and [[Federal Aviation Administration|FAA]] have been essential to the development of Julia.<ref>{{Cite web|url=https://julialang.org/publications/|title=Julia Sponsors, Research, and Publications|last=|first=|date=|website=|archive-url=|archive-date=|dead-url=|access-date=}}</ref>\n\n==Language features==\nAccording to the official website, the main features of the language are:\n\n* [[Multiple dispatch]]: providing ability to define function behavior across many combinations of argument types\n* [[Dynamic type]] system: types for documentation, optimization, and dispatch\n* Good performance, approaching that of [[Type system|statically-typed]] languages like C\n* A built-in [[package manager]]\n* [[Lisp (programming language)|Lisp]]-like macros and other [[metaprogramming]] facilities\n* Call [[Python (programming language)|Python]] functions: use the PyCall package{{Efn|For calling the newer [[Python 3]] (the older default to call Python 2, is also still supported)<ref>{{cite web |url=https://github.com/JuliaPy/PyCall.jl |title=PyCall.jl |work=stevengj |publisher=github.com}}</ref><!--omit?, no need to convince Python 3 works (except maybe for NumPy, also with Python 3), former link actually says Python 3 (and NumPy): --><ref>{{cite newsgroup |url=https://groups.google.com/forum/#!topic/julia-users/lDM7-YXT2LU |title=Using PyCall in julia on Ubuntu with python3 |work=julia-users at Google Groups|quote=to import modules (e.g., python3-numpy)}}</ref> (and [[PyPy]]<ref name=\"Polyglot\">{{cite web |url=https://github.com/wavexx/Polyglot.jl |title=Polyglot.jl |work=wavexx |publisher=github.com}}</ref><!--or [[Jython]] PyPy should work (supported by python-bond that Polyglot.jl is based on), Jython is not confirmed, but since JavaCall.jl allows calling Java (and thus Scala) I assume also Jython.-->) and calling in the other direction, from Python to Julia, is also supported with ''pyjulia''.<ref>{{cite web |url=https://github.com/JuliaPy/pyjulia |title=python interface to julia}}</ref> <!--\nIt's not like Polyglot.jl doesn't support this for older Julia (and also interesting project to support other languages, e.g., PHP), but it hasn't been upgraded for Julia 1.0:\n Even calling [[recursive]]ly (back and forth) between these languages is possible, without (or with) using ''Polyglot.jl'',<ref name=\"Polyglot\"/> that supports additional languages to Python. -->\n}}\n* Call [[C (programming language)|C]] functions directly: no wrappers or special [[application programming interface|APIs]]\n* Powerful [[Shell (computing)|shell]]-like abilities to manage other processes\n* Designed for [[Parallel computing|parallel]] and [[distributed computing]]\n* Coroutines: lightweight [[Green threads|''green'' threading]]\n* User-defined types are as fast and compact as built-ins\n* Automatic generation of efficient, specialized code for different argument types\n* Elegant and extensible conversions and promotions for numeric and other types\n* Efficient support for [[Unicode]], including but not limited to [[UTF-8]]\n<!-- |source=JuliaLang.org}} Only part missing it the MIT part, that is better explained elsewhere.-->\n\nMultiple dispatch (also termed [[multimethod]]s in Lisp) is a [[generalization]] of [[single dispatch]]{{snd}} the [[Polymorphism (computer science)|polymorphic mechanism]] used in common [[object-oriented programming]] (OOP) languages{{snd}} that uses [[Inheritance (object-oriented programming)|inheritance]]. In Julia, all concrete types are [[subtyping|subtypes]] of abstract types, directly or indirectly subtypes of the ''Any'' type, which is the top of the type hierarchy. Concrete types can not be subtyped, but composition is used over inheritance, that is used by traditional object-oriented languages (see also [[Inheritance (object-oriented programming)#Inheritance vs subtyping|inheritance vs subtyping]]).\n\nJulia draws significant inspiration from various dialects of Lisp<!-- Karpinski: \"In short, I would describe it as a Lisp with Matlab-like syntax\" https://www.reddit.com/r/programming/comments/pv3k9/why_we_created_julia_a_new_programming_language/ -->, including [[Scheme (programming language)|Scheme]] and [[Common Lisp]], and it shares many features with [[Dylan (programming language)|Dylan]], also a multiple-dispatch-oriented dynamic language (which features an [[ALGOL]]-like [[Free-form language|free-form]] [[Infix notation|infix]] syntax rather than a Lisp-like prefix syntax, while in Julia \"everything\"<ref name=\"Learn Julia in Y\">{{cite web|url=https://learnxinyminutes.com/docs/julia/ |title=Learn Julia in Y Minutes |website=Learnxinyminutes.com |date= |accessdate=2017-05-31}}</ref> is an [[Expression (computer science)|expression]]), and with [[Fortress (programming language)|Fortress]], another numerical programming language (which features multiple dispatch and a sophisticated parametric type system). While [[Common Lisp Object System]] (CLOS) adds multiple dispatch to Common Lisp, not all functions are generic functions.\n\nIn Julia, Dylan, and Fortress extensibility is the default, and the system's built-in functions are all generic and extensible. In Dylan, multiple dispatch is as fundamental as it is in Julia: all user-defined functions and even basic built-in operations like <code>+</code> are generic. Dylan's type system, however, does not fully support parametric types, which are more typical of the [[Generational list of programming languages#ML based|ML lineage of languages]]. By default, CLOS does not allow for dispatch on Common Lisp's parametric types; such extended dispatch semantics can only be added as an extension through the [[Common Lisp Object System#Metaobject Protocol|CLOS Metaobject Protocol]]. By convergent design, Fortress also features multiple dispatch on parametric types; unlike Julia, however, Fortress is statically rather than dynamically typed, with separate compiling and executing phases. The language features are summarized in the following table:\n\n{| class=\"wikitable\"\n|-\n! [[Programming language|Language]] !! [[Type system]] !! [[Generic function]]s !! [[Parametric polymorphism|Parametric types]]\n|-\n| Julia || Dynamic || Default || Yes\n|-\n| [[Common Lisp]] || Dynamic || Opt-in || Yes (but no dispatch)\n|-\n| [[Dylan (programming language)|Dylan]] || Dynamic || Default || Partial (no dispatch)\n|-\n| [[Fortress (programming language)|Fortress]] || Static || Default || Yes\n|}\n\nBy default, the Julia runtime must be pre-installed as user-provided source code is run, while another way is possible, where a standalone [[executable]] can be made that needs no Julia source code built with ''ApplicationBuilder.jl''<ref>{{Citation|last=Daly|first=Nathan|title=Compile, bundle, and release julia software. Contribute to NHDaly/ApplicationBuilder.jl development by creating an account on GitHub|date=2019-02-13|url=https://github.com/NHDaly/ApplicationBuilder.jl|access-date=2019-02-15}}</ref> and ''PackageCompiler.jl''.<ref>{{Citation|title=Compile your Julia Package. Contribute to JuliaLang/PackageCompiler.jl development by creating an account on GitHub|date=2019-02-14|url=https://github.com/JuliaLang/PackageCompiler.jl|publisher=The Julia Language|access-date=2019-02-15}}</ref><!--\n<ref>{{cite web |url=https://github.com/dhoegh/BuildExecutable.jl |title=Build a standalone executables from a Julia script}}</ref><ref>{{cite newsgroup|url=https://groups.google.com/forum/#!topic/julia-users/MtF4wHc77sw |title=.jl to .exe |website=Groups.google.com |date= |accessdate=2017-05-31}}</ref>  -->\n\nJulia's [[Macro (computer science)#Syntactic macros|syntactic macros]] (used for [[metaprogramming]]), like Lisp macros, are more powerful and different from [[Macro (computer science)#Text-substitution macros|text-substitution macros]] used in the [[preprocessor]] of some other languages such as C, because they work at the level of [[abstract syntax tree]]s (ASTs). Julia's macro system is [[hygienic macro|hygienic]], but also supports deliberate capture when desired (like for [[anaphoric macro]]s) using the <code>esc</code> construct.\n\n==Interaction==\nThe Julia official distribution includes an interactive session shell, called Julia's [[read–eval–print loop]] (REPL), which can be used to experiment and test code quickly.<ref>{{Cite web|url=https://docs.julialang.org/en/stable/manual/getting-started/|title=Getting Started · The Julia Language|website=docs.julialang.org|language=en|access-date=2018-08-15}}</ref> The following fragment represents a sample session example where strings are concatenated automatically by println:<ref>See also: https://docs.julialang.org/en/stable/manual/strings/ for string interpolation and the <code>string(greet, \", \", whom, \".\\n\")</code> example for preferred ways to concatenate strings. <!--While the <code>+</code> operator is not used for string concatenation, it could easily be defined to do so.--> Julia has the println and print functions, but also a @printf macro (i.e., not in function form) to eliminate run-time overhead of formatting (unlike the same function in C).</ref>\n\n<source lang=\"jlcon\">\njulia> p(x) = 2x^2 + 1; f(x, y) = 1 + 2p(x)y\njulia> println(\"Hello world!\", \" I'm on cloud \", f(0, 4), \" as Julia supports recognizable syntax!\")\nHello world! I'm on cloud 9 as Julia supports recognizable syntax!\n</source>\n\nThe REPL gives user access to the system shell and to help mode, by pressing <code>;</code> or <code>?</code> after the prompt (preceding each command), respectively. It also keeps the history of commands, including between sessions.<ref>{{cite web|title=Julia Documentation|url=https://docs.julialang.org|website=JuliaLang.org|accessdate=18 November 2014}}</ref> Code that can be tested inside the Julia's interactive section or saved into a file with a <code>.jl</code> extension and run from the command line by typing:<ref name=\"Learn Julia in Y\"/>\n\n<source lang=\"console\">\n $ julia <filename>\n</source>\n\nJulia is supported by [[Jupyter]], an online interactive \"notebooks\" environment.<ref>{{cite web |url=https://jupyter.org/ |title=Project Jupyter}}</ref>\n\n===Use with other languages===\nJulia's <source lang=\"julia\" inline>ccall</source> keyword is used to call C-exported or Fortran shared library functions individually.\n\nJulia has [[Unicode 11.0]] support (12.0 in Julia 1.2 and Unicode 12.1.0<ref>{{Cite web|url=https://github.com/JuliaLang/julia/pull/32002|title=support for Unicode 12.1.0 by stevengj · Pull Request #32002 · JuliaLang/julia|website=GitHub|language=en|access-date=2019-05-12}}</ref> in Julia 1.3), with [[UTF-8]] used for strings (by default) and for Julia source code, meaning also allowing as an option common math symbols for many operators, such as ∈ for the <code>in</code> operator.\n\nJulia has packages supporting markup languages such as [[HTML]] (and also for [[HTTP]]), [[XML]], [[JSON]] and [[BSON]], and for [[database]]s and web use in general.\n\n==Implementation==\nJulia's core is implemented in Julia and [[C (programming language)|C]]<!--C99, except for 0.4 needing C11 because of static asserts-->, together with [[C++]] for the [[LLVM]] dependency. The parsing and code-lowering are implemented in [[Scheme (programming language)|Scheme]].<ref name=\"JeffBezanson 2019\">{{cite web | author=JeffBezanson | title=JeffBezanson/femtolisp | website=GitHub | date=June 6, 2019 | url=https://github.com/JeffBezanson/femtolisp | access-date=June 16, 2019}}</ref> The LLVM compiler infrastructure project is used as the [[Compiler#Back end|back end]] for generation of [[64-bit computing|64-bit]] or [[32-bit]] optimized [[machine code]] depending on the platform Julia runs on. With some exceptions (e.g., [[PCRE]]), the [[standard library]] is implemented in Julia itself. The most notable aspect of Julia's implementation is its speed, which is often within a factor of two relative to fully optimized C code (and thus often an order of magnitude faster than Python or [[R (programming language)|R]]).<ref name=\"Julia-TR\">{{cite web\n  | format       = PDF\n  | title        = Julia: A Fast Dynamic Language for Technical Computing\n  | url          = https://julialang.org/images/julia-dynamic-2012-tr.pdf\n  | year         = 2012\n  }}</ref><ref>{{cite web\n  | title        = How To Make Python Run As Fast As Julia\n  | url          = https://www.ibm.com/developerworks/community/blogs/jfp/entry/Python_Meets_Julia_Micro_Performance?lang=en\n  | year         = 2015\n  }}</ref><ref>{{cite web\n  | title        = Basic Comparison of Python, Julia, R, Matlab and IDL\n  | url          = https://modelingguru.nasa.gov/docs/DOC-2625\n  | year         = 2015\n  }}</ref> Development of Julia began in 2009 and an [[Open-source software|open-source]] version was publicized in February 2012.<ref name=\"announcement\">{{cite web\n  | title        = Why We Created Julia\n  | date         = February 2012\n  | website      = Julia website\n  | url          = https://julialang.org/blog/2012/02/why-we-created-julia\n  | accessdate   = 7 February 2013\n  }}</ref><ref>{{cite web\n  | last         = Gibbs\n  | first        = Mark\n  | title        = Pure and Julia are cool languages worth checking out\n  | url          = https://www.networkworld.com/columnists/2013/010913-gearhead.html\n  | type         = column\n  | work         = Network World\n  | date         = 9 January 2013\n  | accessdate   = 7 February 2013\n}}</ref>\n\n===Current and future platforms===\nWhile Julia uses JIT<ref>{{cite web |url=https://github.com/JuliaLang/julia/pull/5208\n|title=Support MCJIT\n|website=Github.com\n|accessdate=26 May 2015 <!--quote=Small LLVM 3.4 fix-->\n}}</ref> (MCJIT<ref>{{cite web |url=http://blog.llvm.org/2013/07/using-mcjit-with-kaleidoscope-tutorial.html\n|title=Using MCJIT with the Kaleidoscope Tutorial\n|date=22 July 2013 |accessdate=26 May 2015\n|quote=The older implementation (llvm::JIT) is a sort of ad hoc implementation that brings together various pieces of the LLVM code generation and adds its own glue to get dynamically generated code into memory one function at a time.  The newer implementation (llvm::MCJIT) is heavily based on the core MC library and emits complete object files into memory then prepares them for execution.}}\n</ref> from LLVM<!-- 3.4, or version >3.9 see https://github.com/JuliaLang/julia/commit/ae29df5df3c883b60dbb11c7d322a57977f69ae3 -->){{snd}} Julia generates native machine code directly, before a function is first run (not [[bytecode]]s that are run on a [[virtual machine]] (VM) or translated as the bytecode is running, as with, ''e.g.'', Java; the [[JVM]] or [[Dalvik (software)|Dalvik]] in Android).\n\nJulia has four support tiers,<ref>{{Cite web|url=https://julialang.org/downloads/#support-tiers|title=Julia Downloads|website=julialang.org|access-date=2019-05-17}}</ref> and currently supports all [[x86-64]] processors, that are [[64-bit computing|64-bit]] (and is more [[program optimization|optimized]] for the latest generations) and all [[IA-32]] (\"x86\") processors except for decades old ones, i.e., in [[32-bit]] mode (\"i686\", excepting CPUs from the pre-[[Pentium 4]]-era); and supports more in lower tiers, e.g., tier 2: \"fully supports ARMv8 (AArch64) processors, and supports ARMv7 and ARMv6 (AArch32) with some caveats.\"<ref>{{Citation|title=julia: The Julia Language: A fresh approach to technical computing|date=2018-02-01|url=https://github.com/JuliaLang/julia|publisher=The Julia Language|quote=A list of [https://github.com/JuliaLang/julia/labels/arm known issues] for ARM is available.|accessdate=2018-02-01}}</ref> [[CUDA]] (i.e. \"Nvidia PTX\") has tier 1 support, with the help of an [https://github.com/JuliaGPU/CUDAnative.jl external package].\n\nAt least some platforms may need to be compiled from [[source code]] (e.g., the original [[Raspberry Pi]]), with options changed, while the download page has otherwise [[executable]]s (and the source) available. Julia has been \"successfully built\" <!--on the following ARMv8 devices:\n* [https://www.nvidia.com/object/embedded-systems-dev-kits-modules.html nVidia Jetson TX1 & TX2];\n* [https://www.apm.com/products/data-center/x-gene-family/x-gene/ X-Gene 1];\n* [https://softiron.com/products/overdrive-3000/ Overdrive 3000];\n* [https://www.cavium.com/ThunderX_ARM_Processors.html Cavium ThunderX]-->\non several ARM platforms, up to, e.g., \"ARMv8 Data Center & Cloud Processors\", such as [https://www.cavium.com/ThunderX_ARM_Processors.html Cavium ThunderX] (first ARM with 48 cores). ARM v7 (32-bit) has tier 2 support and binaries (first to get after x86), while ARM v8 ([[64-bit computing|64-bit]]) and PTX (64-bit) (meaning Nvidia's CUDA on GPUs) has \"[https://github.com/JuliaGPU/CUDAnative.jl External]\" support. PowerPC (64-bit) has tier 3 support \"may or may not build\".\n<!--\n Support for ARM, [[AArch64]], and [[Power Architecture|POWER8]] (little-endian) has been added recently as of 0.5.1 is available too.<ref>{{cite web|author= |url=https://github.com/JuliaLang/julia/blob/v0.5.2/README.md |title=julia/README.md at v0.5.2 · JuliaLang/julia · GitHub |website=Github.com |date=2017-05-03 |accessdate=2017-05-31}}</ref>\n\nand in 0.5.x:\n\n\"work in progress\" text dropped with (not yet backported (possibly the text, only docs left behind?) to 0.6.x): https://github.com/JuliaLang/julia/commit/dcffef03594779402bb5c2666fbcf24b4438adba#diff-8b8b297c5626992d7377a6bbb3aadceb\n\n\"only supports ARMv7\" text dropped on master (may apply on older, meaning possibly only for ARMv6?:\n\n\"[[Nightly build]]s are available for ARMv7-A. [..] Note that OpenBLAS only supports ARMv7. For older ARM variants, using the reference BLAS may be the simplest thing to do. [..] Note: These [Raspberry Pi] chips use ARMv6, which is not well supported at the moment. However it is possible to get a working Julia build. [e.g., supported] [[Tegra#Tegra P1|nVidia Jetson TX2]] [with] CUDA functionality\"<ref>{{cite web|author=JuliaLang |url=https://github.com/JuliaLang/julia/blob/v0.6.2/README.arm.md |title=julia/README.arm.md at v0.5.2 · JuliaLang/julia · GitHub |publisher=Github.com |date= |accessdate=2017-05-31}}</ref>\n\nThe [[Raspberry Pi]] support also includes limited support for [[Raspberry Pi]] 1 (since it has [[ARMv6]]),<ref>{{cite web |url=https://github.com/JuliaLang/julia/issues/10488\n|title=Cross-compiling for ARMv6\n|quote=I believe #10917 should fix this. The CPU used there <code>arm1176jzf-s</code>. Please reopen if it does not.\n|accessdate=16 May 2015}}\n</ref><ref>\n{{cite web |url=https://github.com/JuliaLang/julia/issues/10235\n|title=ARM build failing during bootstrap on Raspberry Pi 2\n|quote=I can confirm (FINALLY) that it works on the Raspberry Pi 2 [..] I guess we can announce alpha support for arm in 0.4 as well. |accessdate=16 May 2015}}</ref>\n\n-->\nJulia is now supported in [[Raspbian]]<ref>{{cite web |url=https://julialang.org/blog/2017/05/raspberry-pi-julia |title=Julia available in Raspbian on the Raspberry Pi |quote=Julia works on all the Pi variants, we recommend using the Pi 3.}}</ref> while support is better for newer, e.g., ARMv7 Pis; the Julia support is promoted by the [[Raspberry Pi Foundation]].<ref>{{cite web |url=https://www.raspberrypi.org/blog/julia-language-raspberry-pi/ |title=Julia language for Raspberry Pi |work=[[Raspberry Pi Foundation]]}}</ref>\n<!--\nJulia supports [[64-bit computing|64-bit]] [[ARM architecture|ARM]] and [[PowerPC]] and \"fully supports [[ARMv8]] ([[AArch64]]) processors, and supports ARMv7 and ARMv6 (AArch32) with some caveats\"<ref>https://github.com/JuliaLang/julia/blob/master/README.arm.md</ref><ref>https://github.com/JuliaLang/julia/issues/10791#issuecomment-91735439</ref> and [[PowerPC]] being worked on, with almost no open specific issues,<ref>https://github.com/JuliaLang/julia/labels/Power</ref><ref>{{cite web |url=https://groups.google.com/forum/#!topic/julia-dev/BYVCyUlNR8c |title=Porting Julia to PowerPC |quote=Wow, the latest git allows me to build to completion. |accessdate=9 May 2015}}</ref> with [[Executable|binaries]] available for [[POWER7]] (\"due to some small support from [[IBM]]\") and [[POWER8]], that are expected to have official beta support as of 0.5 (at least for non-parallel support).<ref>{{cite web |url=https://groups.google.com/forum/#!topic/julia-users/xB0k7XMBNqM |title=IBM Power port |quote=I am hoping we can have beta support from the 0.5 release onwards for sequential julia. We were able to do this work due to some small support from IBM.}}</ref>\n-->\n<!--\nSupport for [[GNU/kFreeBSD]] and [[GNU Hurd]] is being worked on (in JuliaLang's [[C mathematical functions#libm|openlibm]] dependency project).<ref>{{cite web|url=https://github.com/JuliaLang/openlibm/pull/129 |title=Fix building tests on GNU/kFreeBSD and GNU/Hurd by ginggs · Pull Request #129 · JuliaLang/openlibm |website=Github.com |date= |accessdate=2017-05-31}}</ref>\nYes, openlibm, is not Julia, but is a dependency of it-->\n<!--\nAn unofficial Julia-lite,<ref>https://github.com/ScottPJones/julia/tree/spj/lite</ref>a trimmed down fork (by now outdated) is a available. Officially Julia has been dropping dependencies and features from the standard library, to make GPL-free (one left), so it's also the official plan.\n-->\n\n==Julia Computing company==\nJulia Computing, Inc. was founded in 2015 by [[Viral B. Shah]], Deepak Vinchhi, [[Alan Edelman]], Jeff Bezanson, [[Stefan Karpinski]] and Keno Fischer.<ref>{{Cite web|url=https://juliacomputing.com/about-us|title=About Us – Julia Computing|website=juliacomputing.com|access-date=2017-09-12}}</ref>\n\nIn June 2017, Julia Computing raised $4.6M in seed funding from [[General Catalyst]] and [[Founder Collective]].<ref>https://juliacomputing.com/communication/2017/06/19/seed-funding.html</ref>\n\n==See also==\n* [[Comparison of numerical analysis software]]\n\n==Notes==\n{{Notelist|80em}}\n\n==References==\n{{Reflist|30em}}\n\n==External links==\n{{wikibook|Introducing Julia}}\n* {{Official website|https://julialang.org}}\n* [https://github.com/JuliaLang/julia Source code]\n\n\n{{Programming languages}}\n{{FOSS}}\n{{Numerical analysis software}}\n{{Mathematical optimization software}}\n{{Statistical software}}\n\n[[Category:2012 software]]\n[[Category:Array programming languages]]\n[[Category:Computational notebook]]\n[[Category:Cross-platform software]]\n[[Category:Data mining and machine learning software]]\n[[Category:Data-centric programming languages]]\n[[Category:Dynamically typed programming languages]]\n[[Category:Free compilers and interpreters]]<!-- note there's also a Julia interpreter (not on by default); beside FemptoLisp-->\n[[Category:Free computer libraries]]\n[[Category:Free data analysis software]]\n[[Category:Free data visualization software]]\n[[Category:Free software projects]]\n[[Category:Free software programmed in C]]\n[[Category:Free statistical software]]\n[[Category:Functional languages]]\n[[Category:High-level programming languages]]\n[[Category:Lisp programming language_family]]\n[[Category:Multi-paradigm programming languages]]<!-- ok? Mostly only using the main paradigm multiple-dispatch, but Julia enables using more, e.g. implementing class-based OO-->\n[[Category:Numerical analysis software for Linux]]\n[[Category:Numerical analysis software for MacOS]]\n[[Category:Numerical analysis software for Windows]]\n[[Category:Numerical libraries]] <!--for Julia's standard library: seems ok, as e.g. part of (and all planned) C's libm has been rewritten in Julia-->\n[[Category:Numerical linear algebra]]\n[[Category:Numerical programming languages]]\n[[Category:Object-oriented programming languages]]\n[[Category:Parallel computing]]\n[[Category:Procedural programming languages]]\n[[Category:Programming languages]]\n[[Category:Programming languages created in 2012]]\n[[Category:Software using the MIT license]]\n<!-- [[Category:Scripting languages]] ? -->\n[[Category:Statistical programming languages]]\n[[Category:Text-oriented programming languages]]\n[[Category:Homoiconic programming languages]]"
    },
    {
      "title": "Kaczmarz method",
      "url": "https://en.wikipedia.org/wiki/Kaczmarz_method",
      "text": "The '''Kaczmarz method''' or '''Kaczmarz's algorithm''' is an [[iterative algorithm]] for solving [[linear equation system]]s <math> A x = b </math>. It was first discovered by the Polish mathematician [[Stefan Kaczmarz]],<ref>{{harvcoltxt|Kaczmarz|1937}}</ref> and was rediscovered in the field of image reconstruction from projections by [[Richard Gordon (theoretical biologist)|Richard Gordon]], Robert Bender, and [[Gabor Herman]] in 1970, where it is called the [[Algebraic reconstruction technique|Algebraic Reconstruction Technique]] (ART).<ref>{{harvcoltxt|Gordon|Bender|Herman|1970}}</ref> ART includes the positivity constraint, making it nonlinear.<ref>{{harvcoltxt|Gordon|2011}}</ref>\n\nThe Kaczmarz method is applicable to any linear system of equations, but its computational advantage relative to other methods depends on the system being [[Sparse matrix|sparse]]. It has been demonstrated to be superior, in some biomedical imaging applications, to other methods such as the [[filtered backprojection]] method.<ref name=\"Herman2009\">{{harvcoltxt|Herman|2009}}</ref>\n\nIt has many applications ranging from [[computed tomography]] (CT) to [[signal processing]]. It can be obtained also by applying to the hyperplanes, described by the linear system, the method of successive [[projections onto convex sets]] (POCS).<ref>{{harvcoltxt|Censor|Zenios|1997}}</ref><ref>{{harvcoltxt|Aster|Borchers|Thurber|2004}}</ref>\n\n==Algorithm 1: Kaczmarz algorithm==\nLet <math>Ax = b</math> be a [[system of linear equations]], let <math> m </math> the number of rows of ''A'', <math>a_{i}</math> be the <math>i</math>th row of [[complex number|complex]]-valued [[matrix (mathematics)|matrix]] <math>A</math>, and let <math>x^{0}</math> be arbitrary complex-valued initial approximation to the solution of <math> Ax=b </math>. For <math> k=0,1,\\ldots </math> compute:\n\n{{NumBlk|:| <math>x^{k+1} = x^{k} + \\frac{b_{i} - \\langle a_{i}, x^{k} \\rangle}{\\| a_{i} \\|^2} \\overline{a_{i}}</math> |{{EquationRef|1}}}}\n\nwhere <math> i = k \\bmod m , i= 1,2, \\ldots m </math> and <math>\\overline{a_i}</math> denotes [[complex conjugate|complex conjugation]] of <math>a_i</math>.\n\nIf the system is consistent, <math> x^k </math> converges to the minimum-[[norm (mathematics)|norm]] solution, provided that the iterations start with the zero vector.\n\nA more general algorithm can be defined using a [[Relaxation (iterative method)|relaxation]] parameter <math> \\lambda^k </math>\n\n: <math> x^{k+1} = x^{k} + \\lambda^k \\frac{b_{i} - \\langle a_{i}, x^{k} \\rangle}{\\| a_{i} \\|^2} \\overline{a_{i}}</math>\n\nThere are versions of the method that converge to a regularized weighted least squares solution when applied to a system of inconsistent equations and, at least as far as initial behavior is concerned, at a lesser cost than other iterative methods, such as the [[conjugate gradient method]].<ref>See {{harvcoltxt|Herman|2009}} and references therein.</ref>\n\n==Algorithm 2: Randomized Kaczmarz algorithm==\nRecently, a randomized version of the Kaczmarz method for [[overdetermined system|overdetermined]] linear systems was introduced by Thomas Strohmer and Roman Vershynin<ref name=\"Strohmer_Vershynin_2009\">{{harvcoltxt|Strohmer|Vershynin|2009}}</ref> in which the ''i''-th equation is selected randomly with probability proportional to <math>\\|a_i \\|^2.</math>\n\nThis method can be seen as a particular case of [[stochastic gradient descent]].<ref name = \"Needell Srebro Ward 2014\">{{harvcoltxt|Needell|Srebro|Ward|2009}}</ref>\n\nUnder such circumstances <math> x_{k} </math> converges exponentially fast to the solution of <math> Ax=b,</math> and the rate of convergence depends only on the scaled [[condition number]] <math> \\kappa(A) </math>.\n\n:'''Theorem.''' Let <math>x</math> be the solution of <math>Ax=b.</math> Then Algorithm 2 converges to <math>x</math> in expectation, with the average error:\n::<math> E \\|x_k-x \\|^2 \\leq \\left (1-\\kappa(A)^{-2} \\right )^{k} \\cdot \\| x_0-x \\|^2. </math>\n\n===Proof===\nWe have\n\n{{NumBlk|:| <math>\\forall z \\in \\Complex^n: \\quad \\sum_{j=1}^{m}|\\langle z,a_j \\rangle|^2 \\geq \\frac{\\| z \\|^2}{\\| A^{-1} \\|^2}</math> |{{EquationRef|2}}}}\n\nUsing\n\n:<math>\\| A \\|^2=\\sum_{j=1}^{m} \\| a_j \\|^2 </math>\n\nwe can write ({{EquationNote|2}}) as\n\n{{NumBlk|:| <math>\\forall z \\in \\Complex^n: \\quad \\sum_{j=1}^{m} \\frac{\\| a_j \\|^2}{\\| A \\|^2}\\left|\\left\\langle z,\\frac {a_j}{\\| a_j \\| }\\right\\rangle \\right|^2 \\geq \\kappa(A)^{-2}{\\| z \\|^2}</math> |{{EquationRef|3}}}}\n\nThe main point of the proof is to view the left hand side in ({{EquationNote|3}}) as an expectation of some random variable. Namely, recall that the solution space of the <math>j-th</math> equation of <math> Ax=b </math> is the hyperplane\n\n:<math>\\{y : \\langle y,a_j \\rangle = b_j\\},</math>\n\nwhose normal is <math>\\tfrac{a_j}{\\| a_j \\|^2}.</math> Define a random vector ''Z'' whose values are the normals to all the equations of <math>Ax=b </math>, with probabilities as in our algorithm:\n\n:<math> Z=\\frac {a_j}{\\| a_j \\| } </math> with probability <math> \\frac{\\| a_j \\|^2}{\\| A \\|^2} \\qquad\\qquad\\qquad j=1,\\ldots,m </math>\n\nThen ({{EquationNote|3}}) says that\n\n{{NumBlk|:| <math>\\forall z \\in \\Complex^n: \\quad \\mathbb E|\\langle z,Z\\rangle|^2 \\geq\\kappa(A)^{-2}{\\| z \\|^2}</math> |{{EquationRef|4}}}}\n\nThe orthogonal projection <math>P</math> onto the solution space of a random equation of <math> Ax=b </math> is given by <math> Pz= z-\\langle z-x, Z\\rangle Z.</math>\n\nNow we are ready to analyze our algorithm. We want to show that the error <math>{\\| x_k-x \\|^2}</math> reduces at each step in average (conditioned on the previous steps) by at least the factor of <math> (1-\\kappa(A)^{-2}). </math> The next approximation <math> x_k </math> is computed from <math> x_{k-1} </math> as <math> x_k= P_kx_{k-1}, </math> where <math> P_1,P_2,\\ldots </math> are independent realizations of the random projection <math>P.</math> The vector <math> x_{k-1}-x_k </math> is in the kernel of <math>P_k.</math> It is orthogonal to the solution space of the equation onto which <math> P_k </math> projects, which contains the vector <math> x_k-x </math> (recall that <math> x </math> is the solution to all equations). The orthogonality of these two vectors then yields\n\n:<math>\\| x_k-x \\|^2=\\| x_{k-1}-x \\|^2-\\| x_{k-1}-x_k \\|^2.</math>\n\nTo complete the proof, we have to bound <math>\\| x_{k-1}-x_k \\|^2</math> from below. By the definition of <math> x_k </math>, we have\n\n:<math>\\| x_{k-1}-x_k \\|=\\langle x_{k-1}-x,Z_k\\rangle </math>\n\nwhere <math> Z_1,Z_2,\\ldots</math> are independent realizations of the random vector <math> Z. </math>\n\nThus\n\n:<math> \\| x_k-x \\|^2 = \\left(1-\\left|\\left\\langle\\frac{x_{k-1}-x}{\\| x_{k-1}-x \\| }, Z_k\\right\\rangle\\right|^2\\right){\\| x_{k-1}-x \\|^2}. </math>\n\nNow we take the expectation of both sides conditional upon the choice of the random vectors <math> Z_1,\\ldots,Z_{k-1} </math> (hence we fix the choice of the random projections <math> P_1,\\ldots,P_{k-1} </math> and thus the random vectors <math> x_1,\\ldots,x_{k-1} </math> and we average over the random vector <math> Z_k </math>). Then\n\n:<math> \\mathbb E_{Z_1,\\ldots,Z_{k-1}}{\\| x_k-x \\|^2} = \\left(1-\\mathbb E_{Z_1,\\ldots,Z_{k-1}}\\left|\\left\\langle\\frac{x_{k-1}-x}{\\| x_{k-1}-x \\| },Z_k\\right\\rangle\\right|^2\\right){\\| x_{k-1}-x \\|^2}.</math>\n\nBy ({{EquationNote|4}}) and the independence,\n\n:<math> \\mathbb E_{Z_1,\\ldots,Z_{k-1}}{\\| x_k-x \\|^2} \\leq (1-\\kappa(A)^{-2}){\\| x_{k-1}-x \\|^2}. </math>\n\nTaking the full expectation of both sides, we conclude that\n\n:<math> \\mathbb E \\| x_k-x \\|^2 \\leq (1-\\kappa(A)^{-2})\\mathbb E{\\| x_{k-1}-x \\|^2}.\\blacksquare  </math>\n\nThe superiority of this selection was illustrated with the reconstruction of a bandlimited function from its nonuniformly spaced sampling values. However, it has been pointed out<ref name=\"Censor_Herman_Jiang_2009\">{{harvcoltxt|Censor|Herman|Jiang|2009}}</ref> that the reported success by Strohmer and Vershynin depends on the specific choices that were made there in translating the underlying problem, whose geometrical nature is to ''find a common point of a set of hyperplanes'', into a system of algebraic equations. There will always be legitimate algebraic representations of the underlying problem for which the selection method in<ref name=\"Strohmer_Vershynin_2009\"/> will perform in an inferior manner.<ref name=\"Strohmer_Vershynin_2009\"/><ref name=\"Censor_Herman_Jiang_2009\"/><ref>{{harvcoltxt|Strohmer|Vershynin|2009b}}</ref>\n\nThe Kaczmarz iteration ({{EquationNote|1}}) has a purely geometric interpretation: the algorithm successively projects the current iterate onto the hyperplane defined by the next equation. Hence, any scaling of the equations is irrelevant; it can also be seen from ({{EquationNote|1}}) that any (nonzero) scaling of the equations cancels out. Thus, in RK, one can use <math>\\| a_i \\| </math> or any other weights that may be relevant. Specifically, in the above-mentioned reconstruction example, the equations were chosen with probability proportional to the average distance of each sample point from its two nearest neighbors — a concept introduced by [[Hans Georg Feichtinger|Feichtinger]] and [[:de:Karlheinz Gröchenig|Gröchenig]]. For additional progress on this topic, see,<ref>{{harvcoltxt|Bass|Gröchenig|2013}}</ref><ref>{{harvcoltxt|Gordon|2017}}</ref> and the references therein.\n\n== Algorithm 3: Gower-Richtarik algorithm ==\n\nIn 2015, Robert M. Gower and [[Peter Richtarik]]<ref name=\"Gower_Richtarik_2015\">{{harvcoltxt|Gower|Richtarik|2015}}</ref> developed a versatile randomized iterative method for solving a consistent system of linear equations <math> Ax = b </math> which includes the randomized Kaczmarz algorithm as a special case. Other special cases include randomized coordinate descent, randomized Gaussian descent and randomized Newton method. Block versions and versions with importance sampling of all these methods also arise as special cases. The method is shown to enjoy exponential rate decay (in expectation) - also known as linear convergence, under very mild conditions on the way randomness enters the algorithm. The Gower-Richtarik method is the first algorithm uncovering a \"sibling\" relationship between these methods, some of which were independently proposed before, while many of which were new.\n\n===Insights about Randomized Kaczmarz===\nInteresting new insights about the randomized Kaczmarz method that can be gained from the analysis of the method include:\n\n* The general rate of the Gower-Richtarik algorithm precisely recovers the rate of the randomized Kaczmarz method in the special case when it reduced to it.\n* The choice of probabilities for which the randomized Kaczmarz algorithm was originally formulated and analyzed (probabilities proportional to the squares of the row norms) is not optimal. Optimal probabilities are the solution of a certain semidefinite program. The theoretical complexity of randomized Kaczmarz with the optimal probabilities can be arbitrarily better than the complexity for the standard probabilities. However, the amount by which it is better depends on the matrix <math>A</math>. There are problems for which the standard probabilities are optimal.\n* When applied to a system with matrix <math>A</math> which is positive definite, Randomized Kaczmarz method is equivalent to the Stochastic Gradient Descent (SGD) method (with a very special stepsize) for minimizing the strongly convex quadratic function <math> f(x) = \\tfrac{1}{2}x^T A x - b^T x.</math> Note that since <math>f</math> is convex, the minimizers of <math>f</math> must satisfy <math> \\nabla f(x) = 0 </math>, which is equivalent to <math> Ax = b.</math> The \"special stepsize\" is the stepsize which leads to a point which in the one-dimensional line spanned by the stochastic gradient minimizes the Euclidean distance from the unknown(!) minimizer of <math> f </math>, namely, from <math>x^* = A^{-1}b.</math> This insight is gained from a dual view of the iterative process (below described as \"Optimization Viewpoint: Constrain and Approximate\").\n\n===Six Equivalent Formulations===\nThe Gower-Richtarik method enjoys six seemingly different but equivalent formulations, shedding additional light on how to interpret it (and, as a consequence, how to interpret its many variants, including randomized Kaczmarz):\n\n* 1. Sketching viewpoint: Sketch & Project\n* 2. Optimization viewpoint: Constrain and Approximate\n* 3. Geometric viewpoint: Random Intersect\n* 4. Algebraic viewpoint 1: Random Linear Solve\n* 5. Algebraic viewpoint 2: Random Update\n* 6. Analytic viewpoint: Random Fixed Point\n\nWe now describe some of these viewpoints. The method depends on 2 parameters:\n\n* a positive definite matrix <math>B</math> giving rise to a weighted Euclidean inner product <math> \\langle x,y \\rangle _B := x^T B y </math> and the induced norm \n::<math> \\|x\\|_B = \\left (\\langle x,x \\rangle _B \\right )^{\\frac{1}{2}},</math>\n\n* and a random matrix <math>S</math> with as many rows as <math>A</math> (and possibly random number of columns).\n\n===1. Sketch and Project===\n\nGiven previous iterate <math>x^k,</math> the new point <math>x^{k+1}</math> is computed by drawing a random matrix <math>S</math> (in an iid fashion from some fixed distribution), and setting\n\n:<math>x^{k+1} = \\underset x \\operatorname{arg\\ min} \\| x - x^k \\|_B \\text{ subject to } S^T A x = S^T b.</math>\n\nThat is, <math>x^{k+1}</math> is obtained as the projection of <math>x^k </math> onto the randomly sketched system <math>S^T Ax = S^T b</math>. The idea behind this method is to pick <math>S</math> in such a way that a projection onto the sketched system is substantially simpler than the solution of the original system <math>Ax=b</math>. Randomized Kaczmarz method is obtained by picking <math>B</math> to be the identity matrix, and <math>S</math> to be the <math>i^{th}</math> unit coordinate vector with probability <math>p_i = \\|a_i\\|^2_2/\\|A\\|_F^2.</math> Different choices of <math>B</math> and <math>S</math> lead to different variants of the method.\n\n===2. Constrain and Approximate===\n\nA seemingly different but entirely equivalent formulation of the method (obtained via Lagrangian duality) is\n\n:<math> x^{k+1} = \\underset x \\operatorname{arg\\ min} \\left \\|x - x^* \\right \\|_B \\text{ subject to } x = x^k + B^{-1}A^T S y,</math>\n\nwhere <math>y</math> is also allowed to vary, and where <math>x^*</math> is any solution of the system <math>Ax=b.</math> Hence, <math>x^{k+1}</math> is obtained by first constraining the update to the linear subspace spanned by the columns of the random matrix <math> B^{-1}A^T S </math>, i.e., to\n\n:<math> \\left \\{ h  : h = B^{-1} A^T S y, \\quad y \\text{ can vary } \\right \\}, </math>\n\nand then choosing the point <math>x</math> from this subspace which best approximates <math>x^* </math>. This formulation may look surprising as it seems impossible to perform the approximation step due to the fact that <math>x^*</math> is not known (after all, this is what we are trying the compute!). However, it is still possible to do this, simply because <math>x^{k+1}</math> computed this way is the same as <math>x^{k+1}</math> computed via the sketch and project formulation and since <math>x^*</math> does not appear there.\n\n===5. Random Update===\n\nThe update can also be written explicitly as\n\n:<math> x^{k+1} = x^k - B^{-1}A^T S \\left (S^T A B^{-1}A^T S \\right )^{\\dagger} S^T \\left (Ax^k - b \\right ), </math>\n\nwhere by <math>M^\\dagger</math> we denote the Moore-Penrose pseudo-inverse of matrix <math>M</math>. Hence, the method can be written in the form <math>x^{k+1}=x^k + h^k </math>, where <math> h^k </math> is a '''random update''' vector.\n\nLetting <math> M = S^T A B^{-1}A^T S,</math> it can be shown that the system <math>M y = S^T (Ax^k - b)</math> always has a solution <math>y^k</math>, and that for all such solutions the vector <math>x^{k+1} - B^{-1} A^T S y^k </math> is the same. Hence, it does not matter which of these solutions is chosen, and the method can be also written as <math>x^{k+1} = x^k - B^{-1}A^T S y^k </math>. The pseudo-inverse leads just to one particular solution. The role of the pseudo-inverse is twofold:\n\n* It allows the method to be written in the explicit \"random update\" form as above,\n* It makes the analysis simple through the final, sixth, formulation.\n\n=== 6. Random Fixed Point ===\n\nIf we subtract <math>x^*</math> from both sides of the random update formula, denote\n\n:<math> Z := A^T S \\left (S^T A B^{-1} A^T S \\right )^\\dagger S^T A,</math>\n\nand use the fact that <math> Ax^* = b,</math> we arrive at the last formulation:\n\n:<math> x^{k+1} - x^* = \\left (I - B^{-1}Z \\right ) \\left (x^k - x^* \\right ),</math>\n\nwhere <math>I</math> is the identity matrix. The iteration matrix, <math> I- B^{-1}Z,</math> is random, whence the name of this formulation.\n\n===Convergence===\n\nBy taking conditional expectations in the 6th formulation (conditional on <math>x^k</math>), we obtain\n\n:<math> \\mathbb{E} \\left. \\left  [x^{k+1}-x^* \\right | x^k \\right ] = \\left (I - B^{-1}\\mathbb{E}[Z] \\right ) \\left [x^k - x^* \\right ]. </math>\n\nBy taking expectation again, and using the tower property of expectations, we obtain\n\n:<math> \\mathbb{E} \\left [x^{k+1}-x^* \\right ] = (I - B^{-1}\\mathbb{E}[Z]) \\mathbb{E}\\left [x^k - x^* \\right ]. </math>\n\nGower and Richtarik<ref name=\"Gower_Richtarik_2015\"/> show that\n\n:<math>\\rho: = \\left \\|I-B^{-\\frac{1}{2}}\\mathbb{E}[Z]B^{-\\frac{1}{2}} \\right \\|_B = \\lambda_{\\max} \\left (I - B^{-1}\\mathbb{E}[Z] \\right ), </math>\n\nwhere the matrix norm is defined by\n\n:<math> \\|M\\|_B := \\max_{x\\neq 0} \\frac{\\|Mx\\|_B}{\\|x\\|_B}.</math>\n\nMoreover, without any assumptions on <math>S</math> one has <math>0\\leq \\rho \\leq 1.</math> By taking norms and unrolling the recurrence, we obtain\n\n===Theorem [Gower & Richtarik 2015]===\n\n:<math> \\left \\| \\mathbb{E} \\left [x^{k}-x^* \\right ] \\right \\|_B \\leq \\rho^k \\| x^0 - x^* \\|_B. </math>\n\n''Remark''. A sufficient condition for the expected residuals to converge to 0 is <math>\\rho<1.</math> This can be achieved if <math>A</math> has a full column rank and under very mild conditions on <math>S.</math> Convergence of the method can be established also without the full column rank assumption in a different way.<ref name=\"gower-richtarik2015.06890\">{{cite arxiv |title=Stochastic dual ascent for solving linear systems |last1= Gower| first1= Robert M. |first2= Peter|last2= Richtarik | year=2015 |eprint = 1512.06890|class= math.NA }}</ref>\n\nIt is also possible to show a stronger result:\n\n===Theorem [Gower & Richtarik 2015]===\nThe '''expected squared norms''' (rather than norms of expectations) converge at the same rate:\n\n:<math> \\mathbb{E} \\left \\| \\left  [x^{k}-x^* \\right ] \\right \\|^2_B \\leq \\rho^k \\left \\|x^0 - x^* \\right \\|^2_B.</math>\n\n''Remark''. This second type of convergence is '''stronger''' due to the following identity<ref name=\"Gower_Richtarik_2015\"/> which holds for any random vector <math>x</math> and any fixed vector <math>x^*</math>:\n\n:<math> \\left\\|\\mathbb{E} \\left [x - x^* \\right ] \\right \\|^2 = \\mathbb{E}\\left [ \\left \\|x-x^* \\right \\|^2 \\right ] - \\mathbb{E} \\left [\\|x-\\mathbb{E}[x]\\|^2 \\right ].</math>\n\n===Convergence of Randomized Kaczmarz===\n\nWe have seen that the randomized Kaczmarz method appears as a special case of the Gower-Richtarik method for <math>B=I</math> and <math>S</math> being the <math>i^{th}</math> unit coordinate vector with probability <math>p_i = \\|a_i\\|_2^2/\\|A\\|_F^2,</math> where <math>a_i</math> is the <math>i^{th}</math> row of <math>A.</math> It can be checked by direct calculation that\n\n:<math>\\rho = \\|I-B^{-1}\\mathbb{E}[Z]\\|_B = 1 - \\frac{\\lambda_{\\min}(A^T A)}{\\|A\\|_F^2}.</math>\n\n===Further Special Cases===\n\n==Notes==\n{{reflist|2}}\n\n==References==\n* {{citation |first=Stefan |last=Kaczmarz |author-link=Stefan Kaczmarz |title=Angenäherte Auflösung von Systemen linearer Gleichungen |work=''Bulletin International de l'Académie Polonaise des Sciences et des Lettres''. Classe des Sciences Mathématiques et Naturelles. Série A, Sciences Mathématiques |volume=35 |pages=355–357 |url=http://jasonstockmann.com/Jason_Stockmann/Welcome_files/kaczmarz_english_translation_1937.pdf |year=1937 |format=PDF}}\n* {{citation |title=An Introduction to Optimization |first=Edwin K. P. |last=Chong |first2=Stanislaw H.|last2=Zak |year=2008 |publisher=John Wiley & Sons |pages=226–230 |edition=3rd}}\n* {{citation |first=Richard |last=Gordon |author-link=Richard Gordon (theoretical biologist) |first2=Robert |last2=Bender |author2-link=Robert Bender |first3=Gabor |last3=Herman |author3-link=Gabor Herman |title=Algebraic reconstruction techniques (ART) for threedimensional electron microscopy and x-ray photography |journal=Journal of Theoretical Biology |volume=29 |issue=3 |pages=471–481 |year=1970 |doi=10.1016/0022-5193(70)90109-8 |pmid=5492997}}\n* {{citation |first=Richard |last=Gordon |author-link=Richard Gordon (theoretical biologist) |title=Stop breast cancer now! Imagining imaging pathways towards search, destroy, cure and watchful waiting of premetastasis breast cancer. In: Breast Cancer - A Lobar Disease, editor: Tibor Tot |publisher=Springer |pages=167–203 |year=2011}}\n*{{citation |first=Gabor |last=Herman |author-link=Gabor Herman |title=Fundamentals of computerized tomography: Image reconstruction from projection |edition=2nd |publisher=Springer |year=2009}}\n* {{citation |first=Yair |last=Censor |author-link=Yair Censor |first2=S.A. |last2=Zenios |title=Parallel optimization: theory, algorithms, and applications |publisher=Oxford University Press |location=New York |year=1997}}\n* {{citation |first=Richard |last=Aster |first2=Brian |last2=Borchers |first3=Clifford |last3=Thurber |title=Parameter Estimation and Inverse Problems |publisher=Elsevier |year=2004}}\n* {{citation |first=Thomas |last=Strohmer |first2=Roman |last2=Vershynin |title=A randomized Kaczmarz algorithm for linear systems with exponential convergence |journal=Journal of Fourier Analysis and Applications |volume=15 |issue=2 |pages=262–278 |year=2009 |url=http://www.eecs.berkeley.edu/~brecht/cs294docs/week1/09.Strohmer.pdf |format=PDF |doi=10.1007/s00041-008-9030-4}}\n* {{citation |first=Deanna |last=Needell |first2=Rachel |last2=Ward|first3=Nati|last3 =Srebro |title=Stochastic gradient descent, weighted sampling, and the randomized Kaczmarz algorithm |journal=Mathematical Programming |volume=155 |pages=549–573 |arxiv=1310.5715 |doi=10.1007/s10107-015-0864-7|year=2015 }}\n* {{citation |first=Yair |last=Censor |first2=Gabor |last2=Herman |author2-link=Gabor Herman |first3=M. |last3=Jiang |title=A note on the behavior of the randomized Kaczmarz algorithm of Strohmer and Vershynin |journal=Journal of Fourier Analysis and Applications |volume=15 |issue=4 |pages=431–436 |year=2009 |doi=10.1007/s00041-009-9077-x|pmid=20495623 |pmc=2872793}}\n* {{citation |first=Thomas |last=Strohmer |first2=Roman |last2=Vershynin |title=Comments on the randomized Kaczmarz method |journal=Journal of Fourier Analysis and Applications |volume=15 |issue=4 |pages=437–440 |year=2009b |doi=10.1007/s00041-009-9082-0}}\n* {{citation |last1=Bass|first1=Richard F.|author1-link=Richard F. Bass|last2=Gröchenig|first2=Karlheinz|title=Relevant sampling of band-limited functions|journal=Illinois Journal of Mathematics|date=2013|volume=57|issue=1|pages=43–58}}\n* {{citation |last=Gordon|first=Dan|title=A derandomization approach to recovering bandlimited signals across a wide range of random sampling rates|journal=Numerical Algorithms|year=2017|doi=10.1007/s11075-017-0356-3}}\n* {{citation |first=Quang |last=Vinh Nguyen |first2=Ford |last2=Lumban Gaol |title=Proceedings of the 2011 2nd International Congress on Computer Applications and Computational Science |publisher=Springer |volume=2 |pages=465–469 |year=2011}}\n* {{citation |first=Robert |last=Gower |first2 = Peter|last2 = Richtarik|title=Randomized iterative methods for linear systems |journal=SIAM Journal on Matrix Analysis and Applications| volume= 36| number =4 | pages=1660–1690 |arxiv=1506.03296 |year=2015 |doi=10.1137/15M1025487}}\n* {{cite arxiv |first=Robert |last=Gower |first2 = Peter|last2 = Richtarik|title=Stochastic dual ascent for solving linear systems |eprint=1512.06890 |year=2015 |mode=cs2|class=math.NA }}\n\n==External links==\n*[http://www.eecs.berkeley.edu/~brecht/cs294docs/week1/09.Strohmer.pdf] A randomized Kaczmarz algorithm with exponential convergence\n*[http://www-personal.umich.edu/~romanv/papers/kaczmarz-comments.pdf] Comments on the randomized Kaczmarz method\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]\n[[Category:Medical imaging]]\n[[Category:Signal processing]]"
    },
    {
      "title": "Kernel (linear algebra)",
      "url": "https://en.wikipedia.org/wiki/Kernel_%28linear_algebra%29",
      "text": "{{other uses|Kernel (disambiguation)}}\n\nIn [[mathematics]], and more specifically in [[linear algebra]] and [[functional analysis]], the '''kernel''' (also known as '''null space''' or '''nullspace''') of a [[linear map]] {{nowrap|''L'' : ''V'' → ''W''}} between two [[vector space]]s ''V'' and ''W'', is the set of all elements '''v''' of ''V'' for which {{nowrap|1=''L''('''v''') = '''0'''}}, where '''0''' denotes the [[zero vector]] in ''W''.  That is,\n\n:<math>\\ker(L) = \\left\\{ \\mathbf{v}\\in V \\mid L(\\mathbf{v})=\\mathbf{0} \\right\\}\\text{.}</math>\n\n==Properties==\n[[File:KerIm 2015Joz L2.png|thumb|Kernel and image of a map ''L''.|346x346px]]\nThe kernel of ''L'' is a [[linear subspace]] of the [[Domain of a function|domain]] ''V''.<ref name=\"textbooks\">Linear algebra, as discussed in this article, is a very well established mathematical discipline for which there are many sources. Almost all of the material in this article can be found in {{harvnb|Lay|2005}}, {{harvnb|Meyer|2001}}, and Strang's lecture.</ref>\nIn the linear map {{nowrap|''L'' : ''V'' → ''W''}}, two elements of ''V'' have the same [[image (mathematics)|image]] in ''W'' if and only if their difference lies in the kernel of ''L'':\n:<math>L(\\mathbf{v}_1) = L(\\mathbf{v}_2)\\;\\Leftrightarrow\\;L(\\mathbf{v}_1-\\mathbf{v}_2)=\\mathbf{0}\\text{.} </math>\nIt follows that the image of ''L'' is [[isomorphism|isomorphic]] to the [[quotient space (linear algebra)|quotient]] of ''V'' by the kernel:\n:<math>\\mathop{\\mathrm{im}}(L) \\cong V / \\ker(L)\\text{.}</math>\n{{anchor|nullity}}This implies the [[rank–nullity theorem]]:\n:<math>\\dim(\\ker L) + \\dim(\\mathop{\\mathrm{im}} L) = \\dim(V)\\text{.}\\,</math>\nwhere, by ''rank'' we mean the dimension of the image of ''L'', and by ''nullity'' that of the kernel of ''L''.\n\nWhen ''V'' is an [[inner product space]], the quotient {{nowrap|''V'' / ker(''L'')}} can be identified with the [[orthogonal complement]] in ''V'' of ker(''L'').  This is the generalization to linear operators of the [[row space]], or coimage, of a matrix.\n\n==Application to modules==\n\n{{main|module (mathematics)}}\nThe notion of kernel applies to the [[homomorphism]]s of modules, the latter being a generalization of the vector space over a [[Field (mathematics)|field]] to that over a [[ring (mathematics)|ring]].\nThe domain of the mapping is a module, and the kernel constitutes a \"[[submodule]]\". Here, the concepts of rank and nullity do not necessarily apply.\n\n==In functional analysis==\n\n{{main|topological vector space}}\nIf ''V'' and ''W'' are [[topological vector space]]s (and ''W'' is finite-dimensional) then a linear operator ''L'':&nbsp;''V''&nbsp;→&nbsp;''W'' is [[continuous linear operator|continuous]] if and only if the kernel of ''L'' is a [[closed set|closed]] subspace of ''V''.\n\n==Representation as matrix multiplication==\nConsider a linear map represented as a ''m'' × ''n'' matrix ''A'' with coefficients in a [[field (mathematics)|field]] ''K'' (typically the field of the [[real number]]s or of the [[complex number]]s) and operating on column vectors ''x'' with ''n'' components over ''K''.\nThe kernel of this linear map is the set of solutions to the equation {{nowrap|1=''A'' '''x''' = '''0'''}}, where '''0''' is understood as the [[zero vector]]. The [[dimension (vector space)|dimension]] of the kernel of ''A'' is called the '''nullity''' of ''A''. In [[set-builder notation]],\n:<math>\\operatorname{N}(A)=\\operatorname{Null}(A)=\\operatorname{ker}(A) = \\left\\{ \\mathbf{x}\\in K^n | A\\mathbf{x} = \\mathbf{0} \\right\\}.</math>\nThe matrix equation is equivalent to a homogeneous [[system of linear equations]]:\n:<math>A\\mathbf{x}=\\mathbf{0} \\;\\;\\Leftrightarrow\\;\\;\n\\begin{alignat}{7}\na_{11} x_1 &&\\; + \\;&& a_{12} x_2 &&\\; + \\;\\cdots\\; + \\;&& a_{1n} x_n &&\\; = \\;&&& 0      \\\\\na_{21} x_1 &&\\; + \\;&& a_{22} x_2 &&\\; + \\;\\cdots\\; + \\;&& a_{2n} x_n &&\\; = \\;&&& 0      \\\\\n\\vdots\\;\\;\\; &&     && \\vdots\\;\\;\\; &&              && \\vdots\\;\\;\\; &&     &&& \\;\\vdots \\\\\na_{m1} x_1 &&\\; + \\;&& a_{m2} x_2 &&\\; + \\;\\cdots\\; + \\;&& a_{mn} x_n &&\\; = \\;&&& 0\\text{.}      \\\\\n\\end{alignat}</math>\nThus the kernel of ''A'' is the same as the solution set to the above homogeneous equations.\n\n===Subspace properties===\nThe kernel of an {{nowrap|''m'' × ''n''}} matrix ''A'' over a field ''K'' is a [[linear subspace]] of '''K'''<sup>''n''</sup>. That is, the kernel of ''A'', the set Null(''A''), has the following three properties:\n# Null(''A'') always contains the [[zero vector]], since {{nowrap|1=''A'''''0''' = '''0'''}}.\n# If {{nowrap|'''x''' ∈ Null(''A'')}} and {{nowrap|'''y''' ∈ Null(''A'')}}, then {{nowrap|'''x''' + '''y''' ∈ Null(''A'')}}. This follows from the distributivity of matrix multiplication over addition.\n# If {{nowrap|'''x''' ∈ Null(''A'')}} and ''c'' is a [[scalar (mathematics)|scalar]] {{nowrap|''c'' ∈ ''K''}}, then {{nowrap|''c'''''x''' ∈ Null(''A'')}}, since {{nowrap|1=''A''(''c'''''x''') = ''c''(''A'''''x''') = ''c'''''0''' = '''0'''}}.\n\n===The row space of a matrix===\n{{main|Rank–nullity theorem}}\nThe product ''A'''''x''' can be written in terms of the [[dot product]] of vectors as follows:\n:<math>A\\mathbf{x} = \\begin{bmatrix} \\mathbf{a}_1 \\cdot \\mathbf{x} \\\\ \\mathbf{a}_2 \\cdot \\mathbf{x} \\\\ \\vdots \\\\ \\mathbf{a}_m \\cdot \\mathbf{x} \\end{bmatrix}.</math>\nHere '''a'''<sub>1</sub>, ... , '''a'''<sub>''m''</sub> denote the rows of the matrix ''A''. It follows that '''x''' is in the kernel of ''A'' if and only if '''x''' is [[orthogonality|orthogonal]] (or perpendicular) to each of the row vectors of ''A'' (because when the dot product of two vectors is equal to zero, they are by definition orthogonal).\n\nThe [[row space]], or coimage, of a matrix ''A'' is the [[linear span|span]] of the row vectors of ''A''. By the above reasoning, the kernel of ''A'' is the [[orthogonal complement]] to the row space. That is, a vector '''x''' lies in the kernel of ''A'' if and only if it is perpendicular to every vector in the row space of ''A''.\n\nThe dimension of the row space of ''A'' is called the [[rank (linear algebra)|rank]] of ''A'', and the dimension of the kernel of ''A'' is called the '''nullity''' of ''A''. These quantities are related by the [[rank–nullity theorem]]\n:<math>\\operatorname{rank}(A) + \\operatorname{nullity}(A) = n.</math>\n\n===[[cokernel|Left null space]]===\nThe '''left null space''', or cokernel, of a matrix ''A'' consists of all vectors '''x''' such that '''x'''<sup>T</sup>''A''&nbsp;=&nbsp;'''0'''<sup>T</sup>, where T denotes the [[transpose]] of a column vector.  The left null space of ''A'' is the same as the kernel of ''A''<sup>T</sup>.  The left null space of ''A'' is the orthogonal complement to the [[column space]] of ''A'', and is dual to the [[cokernel]] of the associated linear transformation.  The kernel, the row space, the column space, and the left null space of ''A'' are the [[Fundamental theorem of linear algebra|four fundamental subspaces]] associated to the matrix ''A''.\n\n===Nonhomogeneous systems of linear equations===\nThe kernel also plays a role in the solution to a nonhomogeneous system of linear equations:\n:<math>A\\mathbf{x}=\\mathbf{b}\\;\\;\\;\\;\\;\\;\\text{or}\\;\\;\\;\\;\\;\\;\\begin{alignat}{7}\na_{11} x_1 &&\\; + \\;&& a_{12} x_2 &&\\; + \\;\\cdots\\; + \\;&& a_{1n} x_n &&\\; = \\;&&& b_1      \\\\\na_{21} x_1 &&\\; + \\;&& a_{22} x_2 &&\\; + \\;\\cdots\\; + \\;&& a_{2n} x_n &&\\; = \\;&&& b_2      \\\\\n\\vdots\\;\\;\\; &&     && \\vdots\\;\\;\\; &&              && \\vdots\\;\\;\\; &&     &&& \\;\\vdots \\\\\na_{m1} x_1 &&\\; + \\;&& a_{m2} x_2 &&\\; + \\;\\cdots\\; + \\;&& a_{mn} x_n &&\\; = \\;&&& b_m      \\\\\n\\end{alignat}</math>\nIf '''u''' and '''v''' are two possible solutions to the above equation, then\n:<math>A(\\mathbf{u}-\\mathbf{v}) = A\\mathbf{u} - A\\mathbf{v} = \\mathbf{b} - \\mathbf{b} = \\mathbf{0}\\,</math>\nThus, the difference of any two solutions to the equation ''A'''''x'''&nbsp;=&nbsp;'''b''' lies in the kernel of ''A''.\n\nIt follows that any solution to the equation ''A'''''x'''&nbsp;=&nbsp;'''b''' can be expressed as the sum of a fixed solution '''v''' and an arbitrary element of the kernel.  That is, the solution set to the equation ''A'''x'''&nbsp;=&nbsp;'''b''' is\n:<math>\\left\\{ \\mathbf{v}+\\mathbf{x} \\mid A \\mathbf{v}=\\mathbf{b} \\land \\mathbf{x}\\in\\operatorname{Null}(A) \\right\\},</math>\nGeometrically, this says that the solution set to ''A'''''x'''&nbsp;=&nbsp;'''b''' is the [[translation (geometry)|translation]] of the kernel of ''A'' by the vector '''v'''. See also [[Fredholm alternative]] and [[flat (geometry)]].\n\n==Illustration==\nWe give here a simple illustration of computing the kernel of a matrix (see {{slink||Computation by Gaussian elimination}}, below for methods better suited to more complex calculations.) We also touch on the row space and its relation to the kernel.\n\nConsider the matrix\n:<math>A = \\begin{bmatrix}\\,\\,\\,2 & 3 & 5 \\\\ -4 & 2 & 3\\end{bmatrix}.</math>\nThe kernel of this matrix consists of all vectors (''x'', ''y'', ''z'')&nbsp;∈&nbsp;[[real coordinate space|'''R'''<sup>3</sup>]] for which\n:<math>\\begin{bmatrix}\\,\\,\\,2 & 3 & 5 \\\\ -4 & 2 & 3\\end{bmatrix}\\begin{bmatrix} x \\\\ y \\\\ z\\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},</math>\nwhich can be expressed as a homogeneous [[system of linear equations]] involving ''x'', ''y'', and ''z'':\n:<math>\\begin{alignat}{7}\n 2x &&\\; + \\;&& 3y &&\\; + \\;&& 5z &&\\; = \\;&& 0, \\\\\n-4x &&\\; + \\;&& 2y &&\\; + \\;&& 3z &&\\; = \\;&& 0,\\\\\n\\end{alignat}</math>\nwhich can be written in matrix form as:\n:<math>\n  \\left[\\begin{array}{ccc|c}\n    2 & 3 & 5 & 0 \\\\\n    -4 & 2 & 3 & 0\n  \\end{array}\\right].\n</math>\n[[Gauss–Jordan elimination]] reduces this to:\n:<math>\n  \\left[\\begin{array}{ccc|c}\n    1 & 0 & 1/16 & 0 \\\\\n    0 & 1 & 13/8 & 0\n  \\end{array}\\right].\n</math>\nRewriting yields:\n:<math>\\begin{alignat}{7}\nx = \\;&& -\\frac{1}{16}z\\,\\,\\, \\\\\ny = \\;&& -\\frac{13}8z.\n\\end{alignat}</math>\nNow we can express an element of the kernel:\n:<math>\\begin{bmatrix} x \\\\ y \\\\ z\\end{bmatrix} = c \\begin{bmatrix} -1/16 \\\\ -13/8 \\\\ 1\\end{bmatrix}.</math>\nfor ''c'' a [[scalar (mathematics)|scalar]].\n\nSince ''c'' is a [[free variable]], this can be expressed equally well as,\n:<math>\n\t\\begin{bmatrix}\n\t\tx\\\\\n\t\ty\\\\\n\t\tz\n\t\\end{bmatrix} = \n        c \\begin{bmatrix}\n\t\t -1\\\\\n\t\t-26\\\\\n\t\t 16\n\t\\end{bmatrix}.\n</math>\nThe kernel of ''A'' is precisely the solution set to these equations (in this case, a [[line (geometry)|line]] through the origin in '''R'''<sup>3</sup>); the vector (−1,−26,16)<sup>T</sup> constitutes a [[Basis (linear algebra)|basis]] of the kernel of ''A''.\nThus, the nullity of ''A'' is 1.\n\nNote also that the following dot products are zero:\n:<math>\n \\left[\\begin{array}{ccc}\n    2 & 3 & 5\n  \\end{array}\\right]\n \\cdot\n \\begin{bmatrix}\n\t\t -1\\\\\n\t\t-26\\\\\n\t\t 16\n \\end{bmatrix}\n= 0\n\\quad\\mathrm{and}\\quad\n \\left[\\begin{array}{ccc}\n    -4 & 2 & 3\n \\end{array}\\right]\n \\cdot\n \\begin{bmatrix}\n\t\t -1\\\\\n\t\t-26\\\\\n\t\t 16\n \\end{bmatrix}\n= 0\\mathrm{,}\n</math>\nwhich illustrates that vectors in the kernel of A are orthogonal to each of the row vectors of A.\n\nThese two (linearly independent) row vectors span the row space of ''A'', a plane orthogonal to the vector (−1,−26,16)<sup>T</sup>.\n\nWith the rank 2 of ''A'', the nullity 1 of ''A'', and the dimension 3 of ''A'', we have an illustration of the rank-nullity theorem.\n\n==Examples==\n\n* If ''L'':&nbsp;'''R'''<sup>''m''</sup>&nbsp;→&nbsp;'''R'''<sup>''n''</sup>, then the kernel of ''L'' is the solution set to a homogeneous [[system of linear equations]].  As in the above illustration, if ''L'' is the operator:\n::<math>L(x_1,x_2,x_3) = (2x_1 + 3x_2 + 5x_3,\\; -4x_1 + 2x_2 + 3x_3)</math>\n:then the kernel of ''L'' is the set of solutions to the equations\n::<math>\\begin{alignat}{7}\n    2x_1 &\\;+\\;& 3x_2 &\\;+\\;& 5x_3 &\\;=\\;& 0 \\\\\n   -4x_1 &\\;+\\;& 2x_2 &\\;+\\;& 3x_3 &\\;=\\;& 0\n\\end{alignat}</math>\n* Let ''C''[0,1] denote the [[vector space]] of all continuous real-valued functions on the interval [0,1], and define ''L'':&nbsp;''C''[0,1]&nbsp;→&nbsp;'''R''' by the rule\n::<math>L(f) = f(0.3)\\text{.}\\,</math>\n:Then the kernel of ''L'' consists of all functions ''f''&nbsp;∈&nbsp;''C''[0,1] for which ''f''(0.3)&nbsp;=&nbsp;0.\n* Let ''C''<sup>∞</sup>('''R''') be the vector space of all infinitely differentiable functions '''R'''&nbsp;→&nbsp;'''R''', and let ''D'':&nbsp;''C''<sup>∞</sup>('''R''')&nbsp;→&nbsp;''C''<sup>∞</sup>('''R''') be the [[differential operator|differentiation operator]]:\n::<math>D(f) = \\frac{df}{dx}\\text{.}</math>\n:Then the kernel of ''D'' consists of all functions in ''C''<sup>∞</sup>('''R''') whose derivatives are zero, i.e. the set of all [[constant function]]s.\n* Let '''R'''<sup>∞</sup> be the [[direct product]] of infinitely many copies of '''R''', and let ''s'':&nbsp;'''R'''<sup>∞</sup>&nbsp;→&nbsp;'''R'''<sup>∞</sup> be the [[shift operator]]\n::<math>s(x_1,x_2,x_3,x_4,\\ldots) = (x_2,x_3,x_4,\\ldots)\\text{.}</math>\n:Then the kernel of ''s'' is the one-dimensional subspace consisting of all vectors (''x''<sub>1</sub>,&nbsp;0,&nbsp;0,&nbsp;...).\n* If ''V'' is an [[inner product space]] and ''W'' is a subspace, the kernel of the [[projection (linear algebra)|orthogonal projection]] ''V''&nbsp;→&nbsp;''W'' is the [[orthogonal complement]] to ''W'' in ''V''.\n\n==Computation by Gaussian elimination==\nA [[Basis (linear algebra)|basis]] of the kernel of a matrix may be computed by [[Gaussian elimination]].\n\nFor this purpose, given an ''m'' × ''n'' matrix ''A'', we construct first the row [[augmented matrix]] <math> \\left[\\begin{array}{c}A\\\\\\hline I\\end{array}\\right],</math> where {{math|''I''}}<!-- necessary to ensure a serif font --> is the ''n'' × ''n'' [[identity matrix]].\n\nComputing its [[column echelon form]] by Gaussian elimination (or any other suitable method), we get a matrix <math> \\left[\\begin{array}{c}B\\\\\\hline C\\end{array}\\right].</math> A basis of the kernel of ''A'' consists in the non-zero columns of ''C'' such that the corresponding column of ''B'' is a [[zero matrix|zero column]].\n\nIn fact, the computation may be stopped as soon as the upper matrix is in column echelon form: the remainder of the computation consists in changing the basis of the vector space generated by the columns whose upper part is zero.\n\nFor example, suppose that\n:<math>A=\\left[ \\begin{array}{cccccc}\n1 & 0 & -3 & 0 &  2 & -8 \\\\\n0 & 1 &  5 & 0 & -1 & 4 \\\\\n0 & 0 &  0 & 1 & 7 & -9 \\\\\n0 & 0 & 0 & 0 & 0 & 0 \\end{array} \\,\\right]. </math>\nThen\n:<math> \\left[\\begin{array}{c}A\\\\\\hline I\\end{array}\\right]=\n\\left[\\begin{array}{cccccc}\n1 & 0 & -3 & 0 &  2 & -8 \\\\\n0 & 1 &  5 & 0 & -1 & 4 \\\\\n0 & 0 &  0 & 1 & 7 & -9 \\\\\n0 & 0 & 0 & 0 & 0 & 0 \\\\\n\\hline\n1 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1\n\\end{array}\\right]. </math>\n\nPutting the upper part in column echelon form by column operations on the whole matrix gives\n:<math> \\left[\\begin{array}{c}B\\\\\\hline C\\end{array}\\right]=\n\\left[\\begin{array}{cccccc}\n1 & 0 &  0 & 0 &  0 & 0 \\\\\n0 & 1 &  0 & 0 &  0 & 0 \\\\\n0 & 0 &  1 & 0 &  0 & 0 \\\\\n0 & 0 &  0 & 0 &  0 & 0 \\\\\n\\hline\n1 & 0 &  0 & 3 & -2 & 8 \\\\\n0 & 1 &  0 & -5 & 1 & -4 \\\\\n0 & 0 &  0 & 1 & 0 & 0 \\\\\n0 & 0 &  1 & 0 & -7 & 9 \\\\\n0 & 0 &  0 & 0 & 1 & 0 \\\\\n0 & 0 &  0 & 0 & 0 & 1\n\\end{array}\\right]. </math>\n\nThe last three columns of ''B'' are zero columns. Therefore, the three last vectors of ''C'',\n:<math>\\left[\\!\\! \\begin{array}{r} 3 \\\\ -5 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{array} \\right],\\;\n\\left[\\!\\! \\begin{array}{r} -2 \\\\ 1 \\\\ 0 \\\\ -7 \\\\ 1 \\\\ 0 \\end{array} \\right],\\;\n\\left[\\!\\! \\begin{array}{r} 8 \\\\ -4 \\\\ 0 \\\\ 9 \\\\ 0 \\\\ 1 \\end{array} \\right] </math>\nare a basis of the kernel of ''A''.\n\nProof that the method computes the kernel: Since column operations correspond to post-multiplication by invertible matrices, the fact that <math> \\left[\\begin{array}{c}A\\\\\\hline I\\end{array}\\right]</math> reduces to <math> \\left[\\begin{array}{c}B\\\\\\hline C\\end{array}\\right]</math> means that there exists an invertible matrix <math>P</math> such that <math> \\left[\\begin{array}{c}A\\\\\\hline I\\end{array}\\right] P = \\left[\\begin{array}{c}B\\\\\\hline C\\end{array}\\right], </math> with <math>B</math> in column echelon form. Thus <math>AP=B,</math> <math>IP=C, </math> and <math> AC=B. </math> A column vector <math>v</math> belongs to the kernel of <math>A</math> (that is <math>Av=0</math>) if and only <math>Bw=0,</math> where <math>w=P^{-1}v=C^{-1}v.</math> As <math>B</math> is in column echelon form, <math>Bw=0,</math> if and only if the nonzero entries of <math>w</math> correspond to the zero columns of <math>B.</math> By multiplying by <math>C</math>, one may deduce that this is the case if and only if <math>v=Cw</math> is a linear combination of the corresponding columns of <math>C.</math>\n\n==Numerical computation==\nThe problem of computing the kernel on a computer depends on the nature of the coefficients.\n\n===Exact coefficients===\nIf the coefficients of the matrix are exactly given numbers, the [[column echelon form]] of the matrix may be computed by [[Bareiss algorithm]] more efficiently than with Gaussian elimination. It is even more efficient to use [[modular arithmetic]] and [[Chinese remainder theorem]], which reduces the problem to several similar ones over [[finite field]]s (this avoids the overhead induced by the non-linearity of the [[computational complexity]] of integer multiplication).{{Citation needed|date=October 2014}}\n\nFor coefficients in a finite field, Gaussian elimination works well, but for the large matrices that occur in [[cryptography]] and [[Gröbner basis]] computation, better algorithms are known, which have roughly the same [[Analysis of algorithms|computational complexity]], but are faster and behave better with modern [[computer hardware]].{{Citation needed|date=October 2014}}\n\n===Floating point computation===\nFor matrices whose entries are [[floating-point number]]s, the problem of computing the kernel makes sense only for matrices such that the number of rows is equal to their rank: because of the [[rounding error]]s, a floating-point matrix has almost always a [[full rank]], even when it is an approximation of a matrix of a much smaller rank. Even for a full-rank matrix, it is possible to compute its kernel only if it is [[well-conditioned problem|well conditioned]], i.e. it has a low [[condition number]].<ref>https://www.math.ohiou.edu/courses/math3600/lecture11.pdf</ref>\n\nEven for a well conditioned full rank matrix, Gaussian elimination does not behave correctly: it introduces rounding errors that are too large for getting a significant result. As the computation of the kernel of a matrix is a special instance of solving a homogeneous system of linear equations, the kernel may be computed by any of the various algorithms designed to solve homogeneous systems. A state of the art software for this purpose is the [[Lapack]] library.{{Citation needed|date=October 2014}}\n\n==See also==\n{{Div col|colwidth=20em}}\n* [[Kernel (algebra)]]\n* [[Zero set]]\n* [[System of linear equations]]\n* [[Row and column spaces]]\n* [[Row reduction]]\n* [[Four fundamental subspaces]]\n* [[Vector space]]\n* [[Linear subspace]]\n* [[Linear operator]]\n* [[Function space]]\n* [[Fredholm alternative]]\n{{div col end}}\n\n==Notes==\n{{reflist}}\n\n==References==\n{{see also|Linear algebra#Further reading}}\n* {{Citation\n | last = Axler\n | first = Sheldon Jay\n | year = 1997\n | title = Linear Algebra Done Right\n | publisher = Springer-Verlag\n | edition = 2nd\n | isbn = 0-387-98259-0\n | postscript = .\n}}\n* {{Citation\n | last = Lay\n | first = David C.\n | year = 2005\n | title = Linear Algebra and Its Applications\n | publisher = Addison Wesley\n | edition = 3rd\n | isbn = 978-0-321-28713-7\n | postscript = .\n}}\n* {{Citation\n |last=Meyer \n |first=Carl D. \n |year=2001 \n |title=Matrix Analysis and Applied Linear Algebra \n |publisher=Society for Industrial and Applied Mathematics (SIAM) \n |isbn=978-0-89871-454-8 \n |url=http://www.matrixanalysis.com/DownloadChapters.html \n |postscript=. \n |deadurl=yes \n |archiveurl=https://web.archive.org/web/20091031193126/http://matrixanalysis.com/DownloadChapters.html \n |archivedate=2009-10-31 \n |df= \n}}\n* {{Citation\n | last = Poole\n | first = David\n | year = 2006\n | title = Linear Algebra: A Modern Introduction\n | publisher = Brooks/Cole\n | edition = 2nd\n | isbn = 0-534-99845-3\n | postscript = .\n}}\n* {{Citation\n | last = Anton\n | first = Howard\n | year = 2005\n | title = Elementary Linear Algebra (Applications Version)\n | publisher = Wiley International\n | edition = 9th\n | postscript = .\n}}\n* {{Citation\n | last = Leon\n | first = Steven J.\n | year = 2006\n | title = Linear Algebra With Applications\n | publisher = Pearson Prentice Hall\n | edition = 7th\n | postscript = .\n}}\n* {{Cite book\n | ref = harv\n | first = Serge\n | last = Lang\n | author-link = Serge Lang\n | title = Linear Algebra\n | publisher = Springer\n | year = 1987\n | isbn = 9780387964126\n}}\n* {{Citation\n | first1 = Lloyd N.\n | last1 = Trefethen\n | first2 = David III\n | last2 = Bau\n | title = Numerical Linear Algebra\n | publisher = SIAM\n | year = 1997\n | isbn = 978-0-89871-361-9\n | url = http://web.comlab.ox.ac.uk/oucl/work/nick.trefethen/text.html\n | postscript = .\n}}\n\n==External links==\n{{wikibooks|Linear Algebra/Null Spaces}}\n* {{springer|title=Kernel of a matrix|id=p/k110090}}\n* [[Khan Academy]], [http://www.khanacademy.org/video/introduction-to-the-null-space-of-a-matrix Introduction to the Null Space of a Matrix]\n\n{{linear algebra}}\n\n{{DEFAULTSORT:Kernel (linear algebra)}}\n[[Category:Linear algebra]]\n[[Category:Functional analysis]]\n[[Category:Matrices]]\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "Krylov subspace",
      "url": "https://en.wikipedia.org/wiki/Krylov_subspace",
      "text": "In [[linear algebra]], the order-''r'' '''Krylov subspace''' generated by an ''n''-by-''n'' matrix ''A'' and a vector ''b'' of dimension ''n'' is the [[linear subspace]] [[Linear span|spanned]] by the images of ''b'' under the first ''r'' powers of ''A'' (starting from <math>A^0=I</math>), that is,\n::<math>\\mathcal{K}_r(A,b) = \\operatorname{span} \\, \\{ b, Ab, A^2b, \\ldots, A^{r-1}b \\}. \\, </math>  \n\n==Background==\nThe concept is named after Russian applied mathematician and naval engineer [[Alexei Krylov]], who published a paper about it in 1931.\n\n==Properties==\n* <math>\\mathcal{K}_r(A,b),A\\mathcal{K}_r(A,b)\\subset \\mathcal{K}_{r+1}(A,b)</math>.\n* Vectors <math>\\{ b, Ab, A^2b, \\ldots, A^{r-1}b \\}</math> are linearly independent until <math>r>r_0</math>, and <math>\\mathcal{K}_r(A,b) \\subset \\mathcal{K}_{r_0}(A,b)</math>. <math>r_0</math> is the maximal dimension of a Krylov subspace.\n* Such <math>r_0\\leq \\mathrm{rank} A\\leq n</math>, more exactly <math>r_0\\leq \\partial p(A)</math>, where <math>p(A)</math> is the minimal polynomial of <math>A</math>.\n* There exists <math>b</math> that <math>r_0 = \\partial p(A)</math>.\n* <math>\\mathcal{K}_r(A,b) </math> is a cyclic submodule generated by <math>b</math> of the [[Torsion_(algebra)|torsion]] <math>k[x]</math>-module <math>(k^n)^A</math>, where <math>k^n</math> is the linear space on <math>k</math>.\n* <math>k^n</math> can be decomposed as the direct sum of Krylov subspaces.\n\n==Use==\nModern [[iterative method]]s for finding one (or a few) eigenvalues of large [[sparse matrix|sparse matrices]] or solving large systems of linear equations avoid matrix-matrix operations, but rather multiply vectors by the matrix and work with the resulting vectors. Starting with a vector, ''b'', one computes <math>A b</math>, then one multiplies that vector by <math>A</math> to find <math>A^2 b</math> and so on.  All algorithms that work this way are referred to as Krylov subspace methods; they are among the most successful methods currently available in numerical linear algebra.  \n\n==Issues==\nBecause the vectors usually soon become almost [[linear independence|linearly dependent]] due to the properties of [[power iteration]], methods relying on Krylov subspace frequently involve some [[orthogonalization]] scheme, such as [[Lanczos iteration]] for [[Hermitian matrix|Hermitian matrices]] or [[Arnoldi iteration]] for more general matrices.\n\n==Existing methods==\nThe best known Krylov subspace methods are the [[Arnoldi iteration|Arnoldi]], [[Lanczos iteration|Lanczos]], [[Conjugate gradient]], [[IDR(s)]] (Induced dimension reduction), [[GMRES]] (generalized minimum residual), [[BiCGSTAB]] (biconjugate gradient stabilized), [[quasi minimal residual|QMR]] (quasi minimal residual), [[TFQMR]] (transpose-free QMR), and [[MINRES]] (minimal residual) methods.\n\n\n== See also ==\n* [[Iterative method]], which has a section on Krylov subspace methods\n\n==References==\n{{reflist}}\n\n==Bibliography==\n* '''Key reference paper''': A N Krylov. ''“О численном решении уравнения, которым в технических вопросах определяются частоты малых колебаний материальных систем”.'' Izvestija AN SSSR (News of Academy of Sciences of the USSR), Otdel. mat. i estest. nauk, 1931, VII, Nr.4, 491-539 '''(in Russian)'''. Transl. as ''“On the Numerical Solution of Equation by Which are Determined in Technical Problems the Frequencies of Small Vibrations of Material Systems”'', or ''\"On the numerical solution of the equation by which in technical questions frequencies of small oscillations of material systems are determined,\"''; according to Grigorian, A. T. (2008) and Botchev (2002) respectively - see below.\n* {{cite book|last=Nevanlinna|first=Olavi|title=Convergence of iterations for linear equations|series=Lectures in Mathematics ETH Zürich|publisher=Birkhäuser Verlag|location=Basel|year=1993|pages=viii+177 pp.|isbn=3-7643-2865-7| mr=1217705}}\n* {{cite book | first=Yousef | last=Saad | authorlink=Yousef Saad |title=Iterative methods for sparse linear systems | edition=2nd| year=2003 | publisher=[[Society for Industrial and Applied Mathematics|SIAM]] | isbn=0-89871-534-2 | oclc=51266114 |url=http://www-users.cs.umn.edu/~saad/IterMethBook_2ndEd.pdf }}\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]\n[[Category:Invariant subspaces]]\n[[Category:Operator theory]]"
    },
    {
      "title": "Lanczos algorithm",
      "url": "https://en.wikipedia.org/wiki/Lanczos_algorithm",
      "text": "{{for|the null space-finding algorithm|block Lanczos algorithm}}\n{{for|the interpolation method|Lanczos resampling}}\n\nThe '''Lanczos algorithm''' is a [[iterative method|direct algorithm]] devised by [[Cornelius Lanczos]] that is an adaptation of [[power iteration|power methods]] to find the <math>m</math> most useful [[eigenvalues and eigenvectors]] of an <math>n \\times n</math> [[Hermitian matrix]], where <math> m </math> is often but not necessarily much smaller than <math> n </math>.<ref>{{cite journal |last=Lanczos |first=C. |title=An iteration method for the solution of the eigenvalue problem of linear differential and integral operators |journal=J. Res. Nat’l Bur. Std. |volume=45 |issue= |pages=255–282 |year=1950 |url=https://www.cs.umd.edu/~oleary/lanczos1950.pdf }}</ref> Although computationally efficient in principle, the method as initially formulated was not useful, due to its [[Numerical stability|numerical instability]].\n\nIn 1970, Ojalvo and Newman showed how to make the method numerically stable and applied it to the solution of very large engineering structures subjected to dynamic loading.<ref name=\":0\">{{cite journal |last=Ojalvo |first=I. U. |last2=Newman |first2=M. |title=Vibration modes of large structures by an automatic matrix-reduction method |journal=[[AIAA Journal]] |volume=8 |issue=7 |pages=1234–1239 |year=1970 |doi=10.2514/3.5878 }}</ref> This was achieved using a method for purifying the Lanczos vectors (i.e. by repeatedly reorthogonalizing each newly generated vector with '''all''' previously generated ones)<ref name=\":0\" /> to any degree of accuracy, which when not performed, produced a series of vectors that were highly contaminated by those associated with the lowest natural frequencies.\n\nIn their original work, these authors also suggested how to select a starting vector (i.e. use a random-number generator to select each element of the starting vector) and suggested an empirically determined method for determining <math> m </math>, the reduced number of vectors (i.e. it should be selected to be approximately 1.5 times the number of accurate eigenvalues desired). Soon thereafter their work was followed by Paige, who also provided an error analysis.<ref>{{cite thesis |last=Paige |first=C. C. |title=The computation of eigenvalues and eigenvectors of very large sparse matrices |publisher=U. of London |type=Ph.D. thesis |date=1971 |oclc=654214109 }}</ref><ref>{{cite journal |last=Paige |first=C. C. |title=Computational Variants of the Lanczos Method for the Eigenproblem |journal=J. Inst. Maths Applics |volume=10 |issue=3 |pages=373–381 |year=1972 |doi=10.1093/imamat/10.3.373 }}</ref> In 1988, Ojalvo produced a more detailed history of this algorithm and an efficient eigenvalue error test.<ref>{{cite book |last=Ojalvo |first=I. U. |chapter=Origins and advantages of Lanczos vectors for large dynamic systems |title=Proc. 6th Modal Analysis Conference (IMAC), Kissimmee, FL |pages=489–494 |year=1988 }}</ref>\n\nCurrently,{{when|date=June 2019}} the method is widely used in a variety of technical fields and has seen a number of variations.\n\n==The algorithm==\n:'''Input''' a [[Hermitian matrix]] <math>A</math> of size <math>n \\times n</math>, and optionally a number of iterations <math>m</math> (as default, let <math>m=n</math>).\n:* Strictly speaking, the algorithm does not need access to the explicit matrix, but only a function <math>v \\mapsto A v</math> that computes the product of the matrix by an arbitrary vector. This function is called at most <math>m</math> times.\n:'''Output''' an <math>n \\times m</math> matrix <math>V</math> with [[Orthonormality|orthonormal]] columns and a [[Tridiagonal matrix|tridiagonal]] real symmetric matrix <math>T = V^* A V</math> of size <math>m \\times m</math>. If <math>m=n</math>, then <math>V</math> is [[unitary matrix|unitary]], and <math> A = V T V^* </math>.\n:'''Warning''' The Lanczos iteration is prone to numerical instability. When executed in non-exact arithmetic, additional measures (as outlined in later sections) should be taken to ensure validity of the results.\n:# Let <math>v_1 \\in \\mathbb{C}^n</math> be an arbitrary vector with [[Euclidean norm]] <math>1</math>.\n:# Abbreviated initial iteration step:\n:## Let <math> w_1' = A v_1 </math>.\n:## Let <math> \\alpha_1 =  w_1'^* v_1 </math>.\n:## Let <math> w_1 = w_1' - \\alpha_1 v_1 </math>.\n:# For <math> j=2,\\dots,m </math> do:\n:## Let <math> \\beta_j = \\| w_{j-1} \\| </math> (also [[Euclidean norm]]).\n:## If <math> \\beta_j \\neq 0 </math>, then let <math> v_j = w_{j-1} / \\beta_j </math>,\n:##: else pick as <math>v_j</math> an arbitrary vector with Euclidean norm <math>1</math> that is orthogonal to all of <math> v_1,\\dots,v_{j-1} </math>.\n:## Let <math> w_j' = A v_j </math>.\n:## Let <math> \\alpha_j = w_j'^* v_j </math>.\n:## Let <math> w_j = w_j' - \\alpha_j v_j   - \\beta_j v_{j-1} </math>.\n:# Let <math>V</math> be the matrix with columns <math> v_1,\\dots,v_m </math>. Let <math>T = \\begin{pmatrix}\n\\alpha_1 & \\beta_2  &          &             &              & 0 \\\\\n\\beta_2  & \\alpha_2 & \\beta_3  &             &              & \\\\\n         & \\beta_3  & \\alpha_3 & \\ddots      &              & \\\\\n         &          & \\ddots   & \\ddots      & \\beta_{m-1}  & \\\\\n         &          &          & \\beta_{m-1} & \\alpha_{m-1} & \\beta_m \\\\\n0        &          &          &             & \\beta_m      & \\alpha_m \\\\\n\\end{pmatrix}</math>.\n:'''Note''' <math> A v_j = w_j' = \\beta_{j+1} v_{j+1} + \\alpha_j v_j + \\beta_j v_{j-1} </math> for <math> 1 < j < m </math>.\n\nThere are in principle four ways to write the iteration procedure. Paige and other works show that the above order of operations is the most numerically stable.<ref name=\"CW1985\">{{Cite book|last1=Cullum |last2= Willoughby|title=Lanczos Algorithms for Large Symmetric Eigenvalue Computations|volume= 1| isbn= 0-8176-3058-9}}</ref><ref name=\"Saad1992\">{{Cite book|author=[[Yousef Saad]]|title=Numerical Methods for Large Eigenvalue Problems|  isbn= 0-470-21820-7|url= http://www-users.cs.umn.edu/~saad/books.html}}</ref>\nIn practice the initial vector <math>v_1</math> may be taken as another argument of the procedure, with <math>\\beta_j=0</math> and indicators of numerical imprecision being included as additional loop termination conditions.\n\nNot counting the matrix–vector multiplication, each iteration does <math>O(n)</math> arithmetical operations. If <math>d</math> is the average number of nonzero elements in a row of <math>A</math>, then the matrix–vector multiplication can be done in <math>O(dn)</math> arithmetical operations. Total complexity is thus <math>O(dmn)</math>, or <math>O(dn^2)</math> if <math>m=n</math>; the Lanczos algorithm can be really fast for sparse matrices. Schemes for improving numerical stability are typically judged against this high performance.\n\nThe vectors <math>v_j</math> are called ''Lanczos vectors''. \nThe vector <math> w_j' </math> is not used after <math> w_j </math> is computed, and the vector <math> w_j </math> is not used after <math> v_{j+1} </math> is computed. Hence one may use the same storage for all three. Likewise, if only the tridiagonal matrix <math>T</math> is sought, then the raw iteration does not need <math> v_{j-1} </math> after having computed <math> w_j </math>, although some schemes for improving the numerical stability would need it later on. Sometimes the subsequent Lanczos vectors are recomputed from <math>v_1</math> when needed.\n\n=== Application to the eigenproblem ===\nThe Lanczos algorithm is most often brought up in the context of finding the [[eigenvalue]]s and [[eigenvector]]s of a matrix, but whereas an ordinary [[Matrix diagonalization|diagonalization of a matrix]] would make eigenvectors and eigenvalues apparent from inspection, the same is not true for the tridiagonalization performed by the Lanczos algorithm; nontrivial additional steps are needed to compute even a single eigenvalue or eigenvector. Nonetheless, applying the Lanczos algorithm is often a significant step forward in computing the eigendecomposition.\nFirst observe that when <math>T</math> is <math>n \\times n</math>, it is [[Matrix similarity|similar]] to <math>A</math>: if <math>\\lambda</math> is an eigenvalue of <math>T</math> then it is also an eigenvalue of <math>A</math>, and if <math> T x = \\lambda x </math> (<math>x</math> is an eigenvector of <math>T</math>) then <math> y = V x </math> is the corresponding eigenvector of <math>A</math> (since <math> A y = A V x = V T V^* V x = V T I x = V T x = V (\\lambda x) = \\lambda V x = \\lambda y</math>). Thus the Lanczos algorithm transforms the eigendecomposition problem for <math>A</math> into the eigendecomposition problem for <math>T</math>.\n# For tridiagonal matrices, there exist a number of specialised algorithms, often with better computational complexity than general-purpose algorithms. For example, if <math>T</math> is an <math>m \\times m</math> tridiagonal symmetric matrix then:\n#* The [[Tridiagonal matrix#Determinant|continuant recursion]] allows computing the [[characteristic polynomial]] in <math>O(m^2)</math> operations, and evaluating it at a point in <math>O(m)</math> operations.\n#* The [[divide-and-conquer eigenvalue algorithm]] can be used to compute the entire eigendecomposition of <math>T</math> in <math>O(m^2)</math> operations.\n#* The Fast Multipole Method<ref>{{cite journal|last1=Coakley|first1=Ed S.|last2=Rokhlin|first2=Vladimir|title=A fast divide-and-conquer algorithm for computing the spectra of real symmetric tridiagonal matrices|journal=Applied and Computational Harmonic Analysis|date=2013|volume=34|issue=3|pages=379–414|doi=10.1016/j.acha.2012.06.003}}</ref> can compute all eigenvalues in just <math>O(m \\log m)</math> operations.\n# Some general eigendecomposition algorithms, notably the [[QR algorithm]], are known to converge faster for tridiagonal matrices than for general matrices. Asymptotic complexity of tridiagonal QR is <math>O(m^2)</math> just as for the divide-and-conquer algorithm (though the constant factor may be different); since the eigenvectors together have <math>m^2</math> elements, this is asymptotically optimal.\n# Even algorithms whose convergence rates are unaffected by unitary transformations, such as the [[power method]] and [[inverse iteration]], may enjoy low-level performance benefits from being applied to the tridiagonal matrix <math>T</math> rather than the original matrix <math>A</math>. Since <math>T</math> is very sparse with all nonzero elements in highly predictable positions, it permits compact storage with excellent performance vis-à-vis [[Cache (computing)|caching]]. Likewise, <math>T</math> is a [[real number|real]] matrix with all eigenvectors and eigenvalues real, whereas <math>A</math> in general may have complex elements and eigenvectors, so real arithmetic is sufficient for finding the eigenvectors and eigenvalues of <math>T</math>.\n# If <math>n</math> is very large, then reducing <math>m</math> so that <math>T</math> is of a manageable size will still allow finding the more extreme eigenvalues and eigenvectors of <math>A</math>; in the <math>m \\ll n</math> region, the Lanczos algorithm can be viewed as a [[lossy compression]] scheme for Hermitian matrices, that emphasises preserving the extreme eigenvalues.\nThe combination of good performance for sparse matrices and the ability to compute several (without computing all) eigenvalues are the main reasons for choosing to use the Lanczos algorithm.\n\n=== Application to tridiagonalization ===\nThough the eigenproblem is often the motivation for applying the Lanczos algorithm, the operation the algorithm primarily performs is tridiagonalization of a matrix, for which numerically stable [[Householder transformation]]s have been favoured since the 1950s.  During the 1960s the Lanczos algorithm was disregarded. Interest in it was rejuvenated by the Kaniel–Paige convergence theory and the development of methods to prevent numerical instability, but the Lanczos algorithm remains the alternative algorithm that one tries only if Householder is not satisfactory.<ref name=\"GolubVanLoan\">{{cite book|last1=Golub|first1=Gene H.|last2=Van Loan|first2=Charles F.|title=Matrix computations|date=1996|publisher=Johns Hopkins Univ. Press|location=Baltimore|isbn=0-8018-5413-X|edition=3.}}</ref>\n\nAspects in which the two algorithms differ include:\n* Lanczos takes advantage of <math>A</math> being a sparse matrix, whereas Householder does not, and will generate [[Sparse matrix#Reducing fill-in|fill-in]].\n* Lanczos works throughout with the original matrix <math>A</math> (and has no problem with it being known only implicitly), whereas raw Householder wants to modify the matrix during the computation (although that can be avoided).\n* Each iteration of the Lanczos algorithm produces another column of the final transformation matrix <math>V</math>, whereas an iteration of Householder produces another factor in a unitary factorisation <math> Q_1 Q_2 \\dots Q_n</math> of <math>V</math>. Each factor is however determined by a single vector, so the storage requirements are the same for both algorithms, and <math>V = Q_1 Q_2 \\dots Q_n</math> can be computed in <math>O(n^3)</math> time.\n* Householder is numerically stable, whereas raw Lanczos is not.\n* Lanczos is highly parallel, with only <math>O(n)</math> points of [[Synchronization (computer science)|synchronisation]] (the computations of <math>\\alpha_j</math> and <math>\\beta_j</math>). Householder is less parallel, having a sequence of <math>O(n^2)</math> scalar quantities computed that each depend on the previous quantity in the sequence.\n\n==Derivation of the algorithm==\nThere are several lines of reasoning which lead to the Lanczos algorithm.\n\n===A more provident power method===\n{{Main|Power iteration}}\nThe power method for finding the largest eigenvalue and a corresponding eigenvector of a matrix <math>A</math> is roughly\n:# Pick a random vector <math>u_1 \\neq 0</math>.\n:# For <math> j \\geqslant 1 </math> (until the direction of <math>u_j</math> has converged) do:\n:## Let <math> u_{j+1}' = A u_j.</math>\n:## Let <math> u_{j+1} = u_{j+1}' / \\| u_{j+1}' \\|.</math>\n:* In the large <math>j</math> limit, <math>u_j</math> approaches the normed eigenvector corresponding to the largest magnitude eigenvalue.\nA critique that can be raised against this method is that it is wasteful: it spends a lot of work (the matrix–vector products in step 2.1) extracting information from the matrix <math>A</math>, but pays attention only to the very last result; implementations typically use the same variable for all the vectors <math>u_j</math>, having each new iteration overwrite the results from the previous one. What if we instead kept all the intermediate results and organised their data?\n\nOne piece of information that trivially is available from the vectors <math>u_j</math> is a chain of [[Krylov subspace]]s. One way of stating that without introducing sets into the algorithm is to claim that it computes\n\n:a subset <math>\\{v_j\\}_{j=1}^m</math> of a basis of <math>\\Complex^n</math> such that <math>Ax \\in \\operatorname{span}(v_1,\\dotsc,v_{j+1}) </math> for every <math>x \\in \\operatorname{span}(v_1,\\dotsc,v_j)</math> and all <math>1 \\leqslant j < m;</math>\n\nthis is trivially satisfied by <math>v_j = u_j</math> as long as <math>u_j</math> is linearly independent of <math>u_1,\\dotsc,u_{j-1}</math> (and in the case that there is such a dependence then one may continue the sequence by picking as <math>v_j</math> an arbitrary vector linearly independent of <math>u_1,\\dotsc,u_{j-1}</math>). A basis containing the <math>u_j</math> vectors is however likely to be numerically [[ill-conditioned]], since this sequence of vectors is by design meant to converge to an eigenvector of <math>A</math>. To avoid that, one can combine the power iteration with a [[Gram–Schmidt process]], to instead produce an orthonormal basis of these Krylov subspaces. \n:# Pick a random vector <math>u_1</math> of Euclidean norm <math>1</math>. Let <math>v_1 = u_1</math>.\n:# For <math>j = 1,\\dotsc,m-1</math> do:\n:## Let <math> u_{j+1}' = A u_j </math>.\n:## For all <math> k = 1, \\dotsc, j </math> let <math>g_{k,j} = v_k^* u_{j+1}'</math>. (These are the coordinates of <math> A u_j = u_{j+1}' </math> with respect to the basis vectors <math>v_1,\\dotsc,v_j</math>.)\n:## Let <math> w_{j+1} = u_{j+1}' - \\sum_{k=1}^j g_{k,j} v_k</math>. (Cancel the component of <math>u_{j+1}'</math> that is in <math>\\operatorname{span}(v_1,\\dotsc,v_j)</math>.)\n:## If <math>w_{j+1} \\neq 0</math> then let <math> u_{j+1} = u_{j+1}' / \\| u_{j+1}' \\| </math> and <math> v_{j+1} = w_{j+1} / \\| w_{j+1} \\| </math>,\n:##: otherwise pick as <math> u_{j+1} = v_{j+1} </math> an arbitrary vector of Euclidean norm <math>1</math> that is orthogonal to all of <math>v_1,\\dotsc,v_j</math>.\nThe relation between the power iteration vectors <math>u_j</math> and the orthogonal vectors <math>v_j</math> is that\n: <math> A u_j = \\| u_{j+1}'\\| u_{j+1} = u_{j+1}' = w_{j+1} + \\sum_{k=1}^j g_{k,j} v_k = \\| w_{j+1}\\| v_{j+1} + \\sum_{k=1}^j g_{k,j} v_k </math>.\nHere it may be observed that we do not actually need the <math>u_j</math> vectors to compute these <math>v_j</math>, because <math>u_j - v_j \\in \\operatorname{span}(v_1,\\dotsc,v_{j-1})</math> and therefore the difference between <math> u_{j+1}' = A u_j </math> and <math> w_{j+1}' = A v_j </math> is in <math>\\operatorname{span}(v_1,\\dotsc,v_j)</math>, which is cancelled out by the orthogonalisation process. Thus the same basis for the chain of Krylov subspaces is computed by\n:# Pick a random vector <math>v_1</math> of Euclidean norm <math>1</math>.\n:# For <math>j = 1,\\dotsc,m-1</math> do:\n:## Let <math> w_{j+1}' = A v_j </math>.\n:## For all <math> k = 1, \\dotsc, j </math> let <math>h_{k,j} = v_k^* w_{j+1}'</math>.\n:## Let <math> w_{j+1} = w_{j+1}' - \\sum_{k=1}^j h_{k,j} v_k </math>.\n:## Let <math> h_{j+1,j} = \\| w_{j+1} \\| </math>.\n:## If <math>h_{j+1,j} \\neq 0</math> then let <math> v_{j+1} = w_{j+1} / h_{j+1,j} </math>,\n:##: otherwise pick as <math> v_{j+1} </math> an arbitrary vector of Euclidean norm <math>1</math> that is orthogonal to all of <math>v_1,\\dotsc,v_j</math>.\nA priori the coefficients <math>h_{k,j}</math> satisfy\n: <math> A v_j = \\sum_{k=1}^{j+1} h_{k,j} v_k </math> for all <math> j < m </math>;\nthe definition <math> h_{j+1,j} = \\| w_{j+1} \\| </math> may seem a bit odd, but fits the general pattern <math> h_{k,j} = v_k^* w_{j+1}' </math> since \n\n:<math> v_{j+1}^* w_{j+1}' = v_{j+1}^* w_{j+1} = \\| w_{j+1} \\| v_{j+1}^* v_{j+1} = \\| w_{j+1} \\|.</math>\n\nBecause the power iteration vectors <math>u_j</math> that were eliminated from this recursion satisfy <math>u_j \\in \\operatorname{span}(v_1,\\ldots,v_j),</math> the vectors <math>\\{v_j\\}_{j=1}^m</math> and coefficients <math>h_{k,j}</math> contain enough information from <math>A</math> that all of <math>u_1,\\ldots,u_m</math> can be computed, so nothing was lost by switching vectors. (Indeed, it turns out that the data collected here give significantly better approximations of the largest eigenvalue than one gets from an equal number of iterations in the power method, although that is not necessarily obvious at this point.)\n\nThis last procedure is the [[Arnoldi iteration]]. The Lanczos algorithm then arises as the simplification one gets from eliminating calculation steps that turn out to be trivial when <math>A</math> is Hermitian—in particular most of the <math>h_{k,j}</math> coefficients turn out to be zero.\n\nElementarily, if <math>A</math> is Hermitian then \n\n:<math> h_{k,j} = v_k^* w_{j+1}' = v_k^* A v_j = v_k^* A^* v_j = (A v_k)^* v_j.</math>\n\nFor <math> k < j-1 </math> we know that <math> A v_k \\in \\operatorname{span}(v_1,\\ldots,v_{j-1}) </math>, and since <math> v_j </math> by construction is orthogonal to this subspace, this inner product must be zero. (This is essentially also the reason why sequences of orthogonal polynomials can always be given a [[Orthogonal polynomials#Recurrence relation|three-term recurrence relation]].) For <math> k = j-1 </math> one gets \n\n:<math> h_{j-1,j} = (A v_{j-1})^* v_j = \\overline{v_j^* A v_{j-1} } = \\overline{ h_{j,j-1} } = h_{j,j-1} </math> \n\nsince the latter is real on account of being the norm of a vector. For <math> k = j </math> one gets \n\n:<math> h_{j,j} = (A v_j)^* v_j = \\overline{v_j^* A v_j } = \\overline{h_{j,j}},</math>\n\nmeaning this is real too.\n\nMore abstractly, if <math>V</math> is the matrix with columns <math>v_1,\\ldots,v_m</math> then the numbers <math>h_{k,j}</math> can be identified as elements of the matrix <math>H = V^*AV</math>, and <math>h_{k,j} = 0</math> for <math>k > j+1;</math> the matrix <math>H</math> is [[Hessenberg matrix|upper Hessenberg]]. Since \n\n:<math> H^* = \\left (V^* A V \\right )^* = V^* A^* V = V^* A V = H </math> \n\nthe matrix <math>H</math> is Hermitian. This implies that <math>H</math> is also lower Hessenberg, so it must in fact be tridiagional. Being Hermitian, its main diagonal is real, and since its first subdiagonal is real by construction, the same is true for its first superdiagonal. Therefore, <math>H</math> is a real, symmetric matrix—the matrix <math>T</math> of the Lanczos algorithm specification.\n\n===Simultaneous approximation of extreme eigenvalues===\nOne way of characterising the eigenvectors of a Hermitian matrix <math>A</math> is as [[stationary point]]s of the [[Rayleigh quotient]]\n\n: <math>r(x) = \\frac{x^* A x} {x^* x}, \\qquad x \\in\\Complex^n.</math>\n\nIn particular, the largest eigenvalue <math>\\lambda_\\max</math> is the global maximum of <math>r</math> and the smallest eigenvalue <math>\\lambda_\\min</math> is the global minimum of <math>r</math>.\n\nWithin a low-dimensional subspace <math>\\mathcal{L}</math> of <math>\\Complex^n</math> it can be feasible to locate the maximum <math>x</math> and minimum <math>y</math> of <math>r</math>. Repeating that for an increasing chain <math>\\mathcal{L}_1 \\subset \\mathcal{L}_2 \\subset \\cdots</math> produces two sequences of vectors: <math>x_1, x_2, \\ldots</math> and <math>y_1, y_2, \\dotsc </math> such that <math>x_j, y_j \\in \\mathcal{L}_j </math> and  \n\n:<math>\\begin{align}\nr(x_1) &\\leqslant r(x_2) \\leqslant \\cdots \\leqslant \\lambda_\\max \\\\\nr(y_1) &\\geqslant r(y_2) \\geqslant \\cdots \\geqslant \\lambda_\\min \n\\end{align}</math>\n\nThe question then arises how to choose the subspaces so that these sequences converge at optimal rate.\n\nFrom <math>x_j</math>, the optimal direction in which to seek larger values of <math>r</math> is that of the [[gradient]] <math>\\nabla r(x_j)</math>, and likewise from <math>y_j</math> the optimal direction in which to seek smaller values of <math>r</math> is that of the negative gradient <math>-\\nabla r(y_j)</math>. In general\n\n: <math>\\nabla r(x) = \\frac{2}{x^* x} ( A x - r(x) x ) </math>,\n\nso the directions of interest are easy enough to compute in matrix arithmetic, but if one wishes to improve on both <math>x_j</math> and <math>y_j</math> then there are two new directions to take into account: <math>Ax_j</math> and <math>Ay_j;</math> since <math>x_j</math> and <math>y_j</math> can be linearly independent vectors (indeed, are close to orthogonal), one cannot in general expect <math>Ax_j</math> and <math>Ay_j</math> to be parallel. Is it therefore necessary to increase the dimension of <math>\\mathcal{L}_j</math> by <math>2</math> on every step? Not if <math>\\{\\mathcal{L}_j\\}_{j=1}^m</math> are taken to be Krylov subspaces, because then <math>Az \\in \\mathcal{L}_{j+1}</math> for all <math>z \\in \\mathcal{L}_j,</math> thus in particular for both <math>z = x_j</math> and <math>z = y_j.</math>.\n\nIn other words, we can start with some arbitrary initial vector <math>x_1 = y_1,</math> construct the vector spaces\n\n:<math> \\mathcal{L}_j = \\operatorname{span}( x_1, A x_1, \\ldots, A^{j-1} x_1 ) </math>\n\nand then seek <math> x_j, y_j \\in \\mathcal{L}_j </math> such that \n\n:<math> r(x_j) = \\max_{z \\in \\mathcal{L}_j} r(z) \\qquad \\text{and} \\qquad  r(y_j) = \\min_{z \\in \\mathcal{L}_j} r(z).</math>\n\nSince the <math>j</math>th power method iterate <math>u_j</math> belongs to <math>\\mathcal{L}_j,</math> it follows that an iteration to produce the <math>x_j</math> and <math>y_j</math> cannot converge slower than that of the power method, and will achieve more by approximating both eigenvalue extremes. For the subproblem of optimising <math>r</math> on some <math>\\mathcal{L}_j </math>, it is convenient to have an orthonormal basis <math>\\{ v_1, \\ldots, v_j \\}</math> for this vector space. Thus we are again led to the problem of iteratively computing such a basis for the sequence of Krylov subspaces.\n\n==Convergence and other dynamics==\nWhen analysing the dynamics of the algorithm, it is convenient to take the eigenvalues and eigenvectors of <math>A</math> as given, even though they are not explicitly known to the user. To fix notation, let <math>\\lambda_1 \\geqslant \\lambda_2 \\geqslant \\dotsb \\geqslant \\lambda_n</math> be the eigenvalues (these are known to all be real, and thus possible to order) and let <math>z_1,\\dotsc,z_n</math> be an orthonormal set of eigenvectors such that <math>A z_k = \\lambda_k z_k</math> for all <math>k=1,\\dotsc,n</math>.\n\nIt is also convenient to fix a notation for the coefficients of the initial Lanczos vector <math>v_1</math> with respect to this eigenbasis; let <math>d_k = z_k^* v_1</math> for all <math>k=1,\\dotsc,n</math>, so that <math> \\textstyle v_1 = \\sum_{k=1}^n d_k z_k</math>. A starting vector <math>v_1</math> depleted of some eigenvalue will delay convergence to the corresponding eigenvalue, and even though this just comes out as a constant factor in the error bounds, depletion remains undesirable. One common technique for avoiding being consistently hit by it is to pick <math>v_1</math> by first drawing the elements randomly according to the same [[normal distribution]] with mean <math>0</math> and then rescale the vector to norm <math>1</math>. Prior to the rescaling, this causes the coefficients <math>d_k</math> to also be independent normally distributed stochastic variables from the same normal distribution (since the change of coordinates is unitary), and after rescaling the vector <math>(d_1,\\dotsc,d_n)</math> will have a [[uniform distribution (continuous)|uniform distribution]] on the unit sphere in <math>\\mathbb{C}^n</math>. This makes it possible to bound the probability that for example <math>|d_1| < \\varepsilon</math>.\n\nThe fact that the Lanczos algorithm is coordinate-agnostic – operations only look at inner products of vectors, never at individual elements of vectors – makes it easy to construct examples with known eigenstructure to run the algorithm on: make <math>A</math> a diagonal matrix with the desired eigenvalues on the diagonal; as long as the starting vector <math>v_1</math> has enough nonzero elements, the algorithm will output a general tridiagonal symmetric matrix as <math>T</math>.\n\n===Kaniel–Paige convergence theory===\nAfter <math>m</math> iteration steps of the Lanczos algorithm, <math>T</math> is an <math>m \\times m</math> real symmetric matrix, that similarly to the above has <math>m</math> eigenvalues <math>\\theta_1 \\geqslant \\theta_2 \\geqslant \\dots \\geqslant \\theta_m.</math> By convergence is primarily understood the convergence of <math>\\theta_1</math> to <math>\\lambda_1</math> (and the symmetrical convergence of <math>\\theta_m</math> to <math>\\lambda_n</math>) as <math>m</math> grows, and secondarily the convergence of some range <math>\\theta_1, \\ldots, \\theta_k</math> of eigenvalues of <math>T</math> to their counterparts <math>\\lambda_1, \\ldots, \\lambda_k</math> of <math>A</math>. The convergence for the Lanczos algorithm is often orders of magnitude faster than that for the power iteration algorithm.{{r|GolubVanLoan|p=477}}\n\nThe bounds for <math>\\theta_1</math> come from the above interpretation of eigenvalues as extreme values of the Rayleigh quotient <math>r(x)</math>. Since <math>\\lambda_1</math> is a priori the maximum of <math>r</math> on the whole of <math>\\Complex^n,</math> whereas <math>\\theta_1</math> is merely the maximum on an <math>m</math>-dimensional Krylov subspace, we trivially get <math> \\lambda_1 \\geqslant \\theta_1</math>. Conversely, any point <math>x</math> in that Krylov subspace provides a lower bound <math>r(x)</math> for <math>\\theta_1</math>, so if a point can be exhibited for which <math>\\lambda_1 - r(x)</math> is small then this provides a tight bound on <math>\\theta_1</math>.\n\nThe dimension <math>m</math> Krylov subspace is \n\n:<math>\\operatorname{span} \\left \\{v_1, A v_1, A^2 v_1, \\ldots, A^{m-1} v_1 \\right \\},</math> \n\nso any element of it can be expressed as <math>p(A) v_1</math> for some polynomial <math>p</math> of degree at most <math>m-1</math>; the coefficients of that polynomial are simply the coefficients in the linear combination of the vectors <math>v_1, A v_1, A^2 v_1, \\ldots, A^{m-1} v_1 </math>. The polynomial we want will turn out to have real coefficients, but for the moment we should allow also for complex coefficients, and we will write <math>p^*</math> for the polynomial obtained by complex conjugating all coefficients of <math>p</math>. In this parametrisation of the Krylov subspace, we have\n\n:<math>r(p(A)v_1) = \\frac{(p(A) v_1)^* A p(A) v_1}{(p(A) v_1)^* p(A) v_1} = \\frac{v_1^* p(A)^* A p(A) v_1}{v_1^* p(A)^* p(A) v_1} = \\frac{v_1^* p^*(A^*) A p(A) v_1 }{v_1^* p^*(A^*) p(A) v_1} = \\frac{ v_1^* p^*(A) A p(A) v_1 }{v_1^* p^*(A) p(A) v_1}</math>\n\nUsing now the expression for <math>v_1</math> as a linear combination of eigenvectors, we get\n\n:<math> A v_1 = A \\sum_{k=1}^n d_k z_k = \\sum_{k=1}^n d_k \\lambda_k z_k</math> \n\nand more generally \n\n:<math>q(A) v_1 = \\sum_{k=1}^n d_k q(\\lambda_k) z_k </math> \n\nfor any polynomial <math>q</math>.\n\nThus\n\n:<math>\\lambda_1 - r(p(A)v_1) = \\lambda_1 - \\frac{v_1^* \\sum_{k=1}^n d_k p^*(\\lambda_k) \\lambda_k p(\\lambda_k) z_k}{v_1^* \\sum_{k=1}^n d_k p^*(\\lambda_k) p(\\lambda_k) z_k} = \\lambda_1 - \\frac{\\sum_{k=1}^n |d_k|^2 \\lambda_k p(\\lambda_k)^* p(\\lambda_k)}{\\sum_{k=1}^n |d_k|^2 p(\\lambda_k)^* p(\\lambda_k)} = \\frac{\\sum_{k=1}^n |d_k|^2 (\\lambda_1-\\lambda_k) \\left| p(\\lambda_k) \\right|^2 }{\\sum_{k=1}^n |d_k|^2 \\left| p(\\lambda_k) \\right|^2 }.</math>\n\nA key difference between numerator and denominator here is that the <math>k=1</math> term vanishes in the numerator, but not in the denominator. Thus if one can pick <math>p</math> to be large at <math>\\lambda_1</math> but small at all other eigenvalues, one will get a tight bound on the error <math>\\lambda_1-\\theta_1</math>.\n\nSince <math>A</math> has many more eigenvalues than <math>p</math> has coefficients, this may seem a tall order, but one way to meet it is to use [[Chebyshev polynomial]]s. Writing <math>c_k</math> for the degree <math>k</math> Chebyshev polynomial of the first kind (that which satisfies <math>c_k(\\cos x) = \\cos(kx)</math> for all <math>x</math>), we have a polynomial which stays in the range <math>[-1,1]</math> on the known interval <math>[-1,1]</math> but grows rapidly outside it. With some scaling of the argument, we can have it map all eigenvalues except <math>\\lambda_1</math> into <math>[-1,1]</math>. Let\n\n:<math> p(x) = c_{m-1}\\left( \\frac{2x - \\lambda_2 - \\lambda_n}{\\lambda_2 - \\lambda_n} \\right)</math> \n\n(in case <math>\\lambda_2=\\lambda_1</math>, use instead the largest eigenvalue strictly less than <math>\\lambda_1</math>), then the maximal value of <math>| p(\\lambda_k)|^2</math> for <math>k \\geqslant 2</math> is <math>1</math> and the minimal value is <math>0</math>, so\n\n:<math>\\lambda_1 - \\theta_1 \\leqslant \\lambda_1 - r(p(A) v_1) = \\frac{\\sum_{k=2}^n |d_k|^2 (\\lambda_1-\\lambda_k) |p(\\lambda_k)|^2 }{ \\sum_{k=1}^n |d_k|^2 |p(\\lambda_k)|^2} \\leqslant \\frac{\\sum_{k=2}^n |d_k|^2 (\\lambda_1-\\lambda_k)}{|d_1|^2 |p(\\lambda_1)|^2 } \\leqslant \\frac{(\\lambda_1-\\lambda_n) \\sum_{k=2}^n |d_k|^2 }{|p(\\lambda_1)|^2 |d_1|^2 }.</math>\n\nFurthermore\n\n:<math> p(\\lambda_1) = c_{m-1}\\left( \\frac{2\\lambda_1 - \\lambda_2 - \\lambda_n}{\\lambda_2 - \\lambda_n} \\right) = c_{m-1}\\left( 2\\frac{\\lambda_1 - \\lambda_2}{\\lambda_2 - \\lambda_n} + 1 \\right);</math>\n\nthe quantity \n\n:<math> \\rho = \\frac{\\lambda_1 - \\lambda_2}{\\lambda_2 - \\lambda_n}</math> \n\n(i.e., the ratio of the first [[eigengap]] to the diameter of the rest of the [[spectrum of a matrix|spectrum]]) is thus of key importance for the convergence rate here. Also writing \n\n:<math> R = e^{\\operatorname{arcosh}(1+2\\rho)} = 1 + 2\\rho + 2\\sqrt{\\rho^2+\\rho},</math>\n\nwe may conclude that\n\n: <math>\\begin{align}\n\\lambda_1 - \\theta_1 &\\leqslant \\frac{(\\lambda_1-\\lambda_n) \\left(1 - |d_1|^2 \\right )}{c_{m-1}(2\\rho+1)^2 |d_1|^2} \\\\[6pt]\n &= \\frac{1 - |d_1|^2}{|d_1|^2} (\\lambda_1-\\lambda_n) \\frac{1}{\\cosh^2 ((m-1) \\operatorname{arcosh}(1+2\\rho))} \\\\[6pt]\n &= \\frac{1 - |d_1|^2}{|d_1|^2} (\\lambda_1-\\lambda_n) \\frac{4}{\\left (R^{m-1} + R^{-(m-1)} \\right )^2} \\\\[6pt]\n &\\leqslant 4 \\frac{1 - |d_1|^2}{|d_1|^2} (\\lambda_1-\\lambda_n) R^{-2(m-1)}\n\\end{align}</math>\n\nThe convergence rate is thus controlled chiefly by <math>R</math>, since this bound shrinks by a factor <math>R^{-2}</math> for each extra iteration.\n\nFor comparison, one may consider how the convergence rate of the power method depends on <math>\\rho</math>, but since the power method primarily is sensitive to the quotient between absolute values of the eigenvalues, we need <math>|\\lambda_n| \\leqslant |\\lambda_2|</math> for the eigengap between <math>\\lambda_1</math> and <math>\\lambda_2</math> to be the dominant one. Under that constraint, the case that most favours the power method is that <math>\\lambda_n = -\\lambda_2</math>, so consider that. Late in the power method, the iteration vector:\n\n:<math> u = (1-t^2)^{1/2} z_1 + t z_2 \\approx z_1 + t z_2,</math>{{notetag|The coefficients need not both be real, but the phase is of little importance. Nor need the composants for other eigenvectors have completely disappeared, but they shrink at least as fast as that for <math>z_2</math>, so <math> u \\approx z_1 + t z_2 </math> describes the worst case.}} \n\nwhere each new iteration effectively multiplies the <math>z_2</math>-amplitude <math>t</math> by\n\n:<math>\\frac{\\lambda_2}{\\lambda_1} = \\frac{\\lambda_2}{\\lambda_2 + (\\lambda_1-\\lambda_2)} = \\frac{1}{1 + \\frac{\\lambda_1-\\lambda_2}{\\lambda_2}} = \\frac{1}{1 + 2\\rho}.</math>\n\nThe estimate of the largest eigenvalue is then \n\n:<math> u^*Au = (1-t^2)\\lambda_1 + t^2\\lambda_2,</math>\n\nso the above bound for the Lanczos algorithm convergence rate should be compared to \n\n:<math>\\lambda_1 - u^*Au = (\\lambda_1-\\lambda_2) t^2,</math>\n\nwhich shrinks by a factor of <math>(1+2\\rho)^{-2}</math> for each iteration. The difference thus boils down to that between <math> 1+2\\rho </math> and <math>R = 1 + 2\\rho + 2\\sqrt{\\rho^2+\\rho}</math>. In the <math>\\rho \\gg 1</math> region, the latter is more like <math> 1+4\\rho </math>, and performs like the power method would with an eigengap twice as large; a notable improvement. The more challenging case is however that of <math> \\rho \\ll 1,</math> in which <math>R \\approx 1 + 2\\sqrt{\\rho} </math> is an even larger improvement on the eigengap; the <math> \\rho \\gg 1 </math> region is where the Lanczos algorithm convergence-wise makes the ''smallest'' improvement on the power method.\n\n==Numerical stability==\nStability means how much the algorithm will be affected (i.e. will it produce the approximate result close to the original one) if there are small numerical errors introduced and accumulated. Numerical stability is the central criterion for judging the usefulness of implementing an algorithm on a computer with roundoff.\n\nFor the Lanczos algorithm, it can be proved that with ''exact arithmetic'', the set of vectors <math>v_1, v_2, \\cdots, v_{m+1}</math> constructs an ''orthonormal'' basis, and the eigenvalues/vectors solved are good approximations to those of the original matrix. However, in practice (as the calculations are performed in floating point arithmetic where inaccuracy is inevitable), the orthogonality is quickly lost and in some cases the new vector could even be linearly dependent on the set that is already constructed. As a result, some of the eigenvalues of the resultant tridiagonal matrix may not be approximations to the original matrix. Therefore, the Lanczos algorithm is not very stable.\n\nUsers of this algorithm must be able to find and remove those \"spurious\" eigenvalues. Practical implementations of the Lanczos algorithm go in three directions to fight this stability issue:<ref name=\"CW1985\"/><ref name=\"Saad1992\"/>\n\n# Prevent the loss of orthogonality,\n# Recover the orthogonality after the basis is generated.\n# After the good and \"spurious\" eigenvalues are all identified, remove the spurious ones.\n\n==Variations==\nVariations on the Lanczos algorithm exist where the vectors involved are tall, narrow matrices instead of vectors and the normalizing constants are small square matrices. These are called \"block\" Lanczos algorithms and can be much faster on computers with large numbers of registers and long memory-fetch times.\n\nMany implementations of the Lanczos algorithm restart after a certain number of iterations.  One of the most influential restarted variations is the implicitly restarted Lanczos method,<ref>{{cite journal |author1=D. Calvetti |author2=L. Reichel |author3=D.C. Sorensen |year=1994 |title=An Implicitly Restarted Lanczos Method for Large Symmetric Eigenvalue Problems|url=http://etna.mcs.kent.edu/vol.2.1994/pp1-21.dir/pp1-21.ps|\n    journal = [[Electronic Transactions on Numerical Analysis]]|\n    volume = 2|\n    pages = 1–21\n\n}}</ref> which is implemented in [[ARPACK]].<ref>{{cite book |author1=R. B. Lehoucq |author2=D. C. Sorensen |author3=C. Yang |year=1998 |title=ARPACK Users Guide: Solution of Large-Scale Eigenvalue Problems with Implicitly Restarted Arnoldi Methods |publisher=SIAM |doi=10.1137/1.9780898719628 }}</ref>  This has led into a number of other restarted variations such as restarted Lanczos bidiagonalization.<ref>{{cite journal |author1=E. Kokiopoulou |author2=C. Bekas |author3=E. Gallopoulos |year=2004 |title=Computing smallest singular triplets with implicitly restarted Lanczos bidiagonalization |journal=Appl. Numer. Math. |doi=10.1016/j.apnum.2003.11.011 |volume=49 |url=https://infoscience.epfl.ch/record/87150/files/Kokiopoulou2004_1356.pdf|pages=39–61}}</ref>  Another successful restarted variation is the Thick-Restart Lanczos method,<ref>{{cite journal |author1=Kesheng Wu |author2=Horst Simon |year=2000 |title=Thick-Restart Lanczos Method for Large Symmetric  Eigenvalue Problems |publisher=SIAM |doi=10.1137/S0895479898334605 |journal=SIAM Journal on Matrix Analysis and Applications |volume=22 |issue=2 |pages=602–616 |url=https://zenodo.org/record/1236144/files/article.pdf }}</ref> which has been implemented in a software package called TRLan.<ref>{{cite web |author1=Kesheng Wu |author2=Horst Simon |year=2001 |title=TRLan software package |publisher= |url=http://crd.lbl.gov/~kewu/trlan.html }}</ref>\n\n===Nullspace over a finite field===\n{{Main|Block Lanczos algorithm}}\n\nIn 1995, [[Peter Montgomery (mathematician)|Peter Montgomery]] published an algorithm, based on the Lanczos algorithm, for finding elements of the [[kernel (matrix)|nullspace]] of a large sparse matrix over [[GF(2)]]; since the set of people interested in large sparse matrices over finite fields and the set of people interested in large eigenvalue problems scarcely overlap, this is often also called the ''block Lanczos algorithm'' without causing unreasonable confusion.{{Citation needed|date=June 2011}}\n\n==Applications==\nLanczos algorithms are very attractive because the multiplication by <math>A\\,</math> is the only large-scale linear operation. Since weighted-term text retrieval engines implement just this operation, the Lanczos algorithm can be applied efficiently to text documents (see [[Latent Semantic Indexing]]). Eigenvectors are also important for large-scale ranking methods such as the [[HITS algorithm]] developed by [[Jon Kleinberg]], or the [[PageRank]] algorithm used by Google.\n\nLanczos algorithms are also used in [[Condensed Matter Physics]] as a method for solving [[Hamiltonian matrix|Hamiltonians]] of [[Strongly correlated material|strongly correlated electron systems]],<ref>{{cite journal|last=Chen|first=HY|author2=Atkinson, W.A. |author3=Wortis, R. |title=Disorder-induced zero-bias anomaly in the Anderson-Hubbard model: Numerical and analytical calculations|journal=Physical Review B|date=July 2011|volume=84|issue=4|doi=10.1103/PhysRevB.84.045113|arxiv=1012.1031|bibcode=2011PhRvB..84d5113C}}</ref> as well as in [[nuclear shell model|shell model]] codes in [[nuclear physics]].<ref>{{cite arxiv|last=Shimizu|first=Noritaka|title=Nuclear shell-model code for massive parallel computation, \"KSHELL\"|eprint=1310.5431|date=21 October 2013|class=nucl-th}}</ref>\n\n==Implementations==\nThe [[NAG Numerical Library|NAG Library]] contains several routines<ref>{{ cite web | author = The Numerical Algorithms Group  | title = Keyword Index: Lanczos | work = NAG Library Manual, Mark 23 | url = http://www.nag.co.uk/numeric/fl/nagdoc_fl23/html/INDEXES/KWIC/lanczos.html | accessdate = 2012-02-09 }}</ref> for the solution of large scale linear systems and eigenproblems which use the Lanczos algorithm.\n\n[[MATLAB]] and [[GNU Octave]] come with ARPACK built-in. Both stored and implicit matrices can be analyzed through the ''eigs()'' function ([http://www.mathworks.com/help/techdoc/ref/eigs.html Matlab]/[https://www.gnu.org/software/octave/doc/interpreter/Sparse-Linear-Algebra.html#doc_002deigs Octave]).\n\nA Matlab implementation of the Lanczos algorithm (note precision issues) is available as a part of the [http://www.cs.cmu.edu/~bickson/gabp/#download Gaussian Belief Propagation Matlab Package]. The [[GraphLab]]<ref>[http://www.graphlab.ml.cmu.edu/pmf.html GraphLab] {{webarchive|url=https://web.archive.org/web/20110314171151/http://www.graphlab.ml.cmu.edu/pmf.html |date=2011-03-14 }}</ref> collaborative filtering library incorporates a large scale parallel implementation of the Lanczos algorithm (in C++) for multicore.\n\nThe [http://www.cs.wm.edu/~andreas/software/ PRIMME] library also implements a Lanczos like algorithm.\n\n==Notes==\n{{reflist|group=note}}\n\n==References==\n{{reflist|30em}}\n\n==Further reading==\n* {{cite book |first=Gene H. |last=Golub |authorlink=Gene H. Golub |first2=Charles F. |last2=Van Loan |authorlink2=Charles F. Van Loan |title=Matrix Computations |location=Baltimore |publisher=Johns Hopkins University Press |year=1996 |isbn=0-8018-5414-8 |pages=470–507 |chapter=Lanczos Methods |chapterurl=https://books.google.com/books?id=mlOa7wPX6OYC&pg=PA470 }}\n* {{cite paper |first=Andrew Y. |last=Ng |authorlink=Andrew Ng |first2=Alice X. |last2=Zheng |first3=Michael I. |last3=Jordan |authorlink3=Michael I. Jordan |title=Link Analysis, Eigenvectors and Stability |work=IJCAI'01 Proceedings of the 17th international joint conference on Artificial intelligence |volume=Volume 2 |pages=903–910 |date=2001 |url=https://ai.stanford.edu/~ang/papers/ijcai01-linkanalysis.pdf }}\n\n{{Numerical linear algebra}}\n\n{{DEFAULTSORT:Lanczos Algorithm}}\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "LAPACK",
      "url": "https://en.wikipedia.org/wiki/LAPACK",
      "text": "{{Infobox software\n| name                       = LAPACK\n| logo                       = LAPACK logo.svg\n| logo size                  = 120px\n| screenshot                 =\n| caption                    =\n| collapsible                =\n| author                     =\n| developer                  =\n| released                   = {{Start date and age|1992}}\n| latest release version     = 3.8.0\n| latest release date        = {{Start date and age|2017|11|12|df=yes}}\n| latest preview version     =\n| latest preview date        =\n| frequently updated         =\n| programming language       = [[Fortran 90]]\n| operating system           =\n| platform                   =\n| size                       =\n| language                   =\n| status                     =\n| genre                      = [[Software library]]\n| license                    = [[BSD-new]]\n| website                    = {{URL|http://www.netlib.org/lapack/}}\n}}\n'''LAPACK''' ('''L'''inear '''A'''lgebra '''Pack'''age) is a standard [[software library]] for [[numerical linear algebra]]. It provides [[subroutine|routines]] for solving [[systems of linear equations]] and [[numerical methods for linear least squares|linear least squares]], [[eigendecomposition of a matrix|eigenvalue problems]], and [[singular value decomposition]]. It also includes routines to implement the associated [[matrix factorization]]s such as [[LU decomposition|LU]], [[QR decomposition|QR]], [[Cholesky decomposition|Cholesky]] and [[Schur decomposition]]. LAPACK was originally written in [[FORTRAN 77]], but moved to [[Fortran 90]] in version 3.2 (2008).<ref>{{cite web|url=http://www.netlib.org/lapack/lapack-3.2.html|title=LAPACK 3.2 Release Notes|date=16 November 2008}}</ref> The routines handle both [[real number|real]] and [[complex number|complex]] matrices in both [[single precision|single]] and [[double precision]].\n\nLAPACK was designed as the successor to the linear equations and linear least-squares routines of [[LINPACK]] and  the eigenvalue routines of [[EISPACK]]. [[LINPACK]], written in the 1970s and 1980s, was designed to run on the then-modern [[vector processor|vector computer]]s with shared memory.  LAPACK, in contrast, was designed to effectively exploit the [[CPU cache|cache]]s on modern cache-based architectures, and thus can run orders of magnitude faster than LINPACK on such machines, given a well-tuned [[Basic Linear Algebra Subprograms|BLAS]] implementation. LAPACK has also been extended to run on [[distributed memory]] systems in later packages such as [[ScaLAPACK]] and PLAPACK.<ref>{{cite web|accessdate=20 April 2017| date=12 June 2007| title=PLAPACK: Parallel Linear Algebra Package| url=https://www.cs.utexas.edu/users/plapack/| website=www.cs.utexas.edu| publisher=[[University of Texas at Austin]]}}</ref>\n\nLAPACK is licensed under a three-clause [[BSD licenses|BSD style]] license, a [[permissive free software license]] with few restrictions.\n\n==Naming scheme==\nSubroutines in LAPACK have a naming convention which makes the identifiers very compact. This was necessary as the first [[Fortran]] standards only supported identifiers up to six characters long, so the names had to be shortened to fit into this limit.\n\nA LAPACK subroutine name is in the form <code>pmmaaa</code>, where:\n* <code>p</code> is a one-letter code denoting the type of numerical constants used. <code>S</code>, <code>D</code> stand for real [[floating point]] arithmetic respectively in single and double precision, while <code>C</code> and <code>Z</code> stand for [[complex number|complex arithmetic]] with respectively single and double precision. The newer version, LAPACK95, uses [[generic function|generic]] subroutines in order to overcome the need to explicitly specify the data type.\n* <code>mm</code> is a two-letter code denoting the kind of matrix expected by the algorithm. The codes for the different kind of matrices are reported below; the actual data are stored in a different format depending on the specific kind; e.g., when the code <code>DI</code> is given, the subroutine expects a vector of length <code>n</code> containing the elements on the diagonal, while when the code <code>GE</code> is given, the subroutine expects an {{math|''n''×''n''}} array containing the entries of the matrix.\n* <code>aaa</code> is a one- to three-letter code describing the actual algorithm implemented in the subroutine, e.g. <code>SV</code> denotes a subroutine to solve [[linear system]], while <code>R</code> denotes a rank-1 update.\n\nFor example, the subroutine to solve a linear system with a general (non-structured) matrix using real double-precision arithmetic is called <code>DGESV</code>.\n\n{|class=\"wikitable\"\n|+Matrix types in the LAPACK naming scheme\n|-\n!Name\n!Description\n|-\n|BD\n|[[Bidiagonal matrix]]\n|-\n|DI\n|[[Diagonal matrix]]\n|-\n|GB\n|[[Band matrix]]\n|-\n|GE\n|[[matrix (mathematics)|Matrix]] (i.e., [[Symmetric matrix|unsymmetric]], in some cases rectangular)\n|-\n|GG\n|general matrices, generalized problem (i.e., a pair of general matrices)\n|-\n|GT\n|[[Tridiagonal Matrix]] General Matrix\n|-\n|HB\n|([[Complex number|complex]]) [[Hermitian matrix]] [[Band matrix]]\n|-\n|HE\n|([[Complex number|complex]]) [[Hermitian matrix]]\n|-\n|HG\n|[[upper Hessenberg matrix]], generalized problem (i.e. a Hessenberg and a [[Triangular matrix]])\n|-\n|HP\n|([[Complex number|complex]]) [[Hermitian matrix]], [[Packed storage matrix]]\n|-\n|HS\n|[[upper Hessenberg matrix]]\n|-\n|OP\n|([[Real number|real]]) [[Orthogonal matrix]], [[Packed storage matrix]]\n|-\n|OR\n|([[Real number|real]]) [[Orthogonal matrix]]\n|-\n|PB\n|[[Symmetric matrix]] or [[Hermitian matrix]] [[positive definite matrix|positive definite]] band\n|-\n|PO\n|[[Symmetric matrix]] or [[Hermitian matrix]] [[positive definite matrix|positive definite]]\n|-\n|PP\n|[[Symmetric matrix]] or [[Hermitian matrix]] [[positive definite matrix|positive definite]], [[Packed storage matrix]]\n|-\n|PT\n|[[Symmetric matrix]] or [[Hermitian matrix]] [[positive definite matrix|positive definite]] [[Tridiagonal matrix]]\n|-\n|SB\n|([[Real number|real]]) [[Symmetric matrix]] [[Band matrix]]\n|-\n|SP\n|[[Symmetric matrix]], [[Packed storage matrix]]\n|-\n|ST\n|([[Real number|real]]) [[Symmetric matrix]] [[Tridiagonal matrix]]\n|-\n|SY\n|[[Symmetric matrix]]\n|-\n|TB\n|[[Triangular matrix]] [[Band matrix]]\n|-\n|TG\n|[[Triangular matrix|triangular matrices]], generalized problem (i.e., a pair of [[triangular matrix|triangular matrices]])\n|-\n|TP\n|[[Triangular matrix]], [[Packed storage matrix]]\n|-\n|TR\n|[[Triangular matrix]] (or in some cases quasi-triangular)\n|-\n|TZ\n|[[Trapezoidal matrix]]\n|-\n|UN\n|([[Complex number|complex]]) [[Unitary matrix]]\n|-\n|UP\n|([[Complex number|complex]]) [[Unitary matrix]], [[Packed storage matrix]]\n|}\nDetails on this scheme can be found in the [http://www.netlib.org/lapack/lug/node24.html Naming scheme] section in LAPACK Users' Guide.\n\n==Use with other programming languages==\nMany programming environments today support the use of libraries with [[C (programming language)|C]] binding. The LAPACK routines can be used like C functions if a few restrictions are observed.\n\nSeveral alternative [[language binding]]s are also available:\n* [[Armadillo (C++ library)|Armadillo]] for [[C++]]\n* [[IT++]] for C++\n* [[LAPACK++]] for C++\n* Lacaml for [[OCaml]]\n* CLapack for [[C (programming language)|C]]\n* [[SciPy]] for [[Python (programming language)|Python]]\n\n==See also==\n{{Portal|Free and open-source software}}\n*[[List of numerical libraries]]\n*[[Math Kernel Library]] (MKL)\n*[[NAG Numerical Library]]\n*[[SLATEC]], a FORTRAN 77 library of mathematical and statistical routines\n*[[QUADPACK]], a FORTRAN 77 library for numerical integration\n\n==References==\n{{reflist}}\n\n==Further reading==\n*{{cite book|last1=Anderson|first1=E.|last2=Bai|first2=Z.|last3=Bischof|first3=C.|last4=Blackford|first4=S.|last5=Demmel|first5=J.|author-link5=James Demmel|last6=Dongarra|first6=J.|author-link6=Jack Dongarra|last7=Du Croz|first7=J.|last8=Greenbaum|first8=A.|author-link8=Anne Greenbaum|last9=Hammarling|first9=S.|last10=McKenney|first10=A.|last11=Sorensen|first11=D.\n| title =LAPACK Users' Guide\n| edition = Third\n| publisher = Society for Industrial and Applied Mathematics\n|year = 1999\n| location = Philadelphia, PA\n| isbn = 0-89871-447-8\n| url = http://www.netlib.org/lapack/lug/\n}}\n\n==External links==\n* {{Official website|http://www.netlib.org/lapack/}} on Netlib.org\n\n{{Numerical linear algebra}}\n\n{{DEFAULTSORT:Lapack}}\n[[Category:Fortran libraries]]\n[[Category:Free software programmed in Fortran]]\n[[Category:Numerical linear algebra]]\n[[Category:Numerical software]]\n[[Category:Software using the BSD license]]"
    },
    {
      "title": "Librsb",
      "url": "https://en.wikipedia.org/wiki/Librsb",
      "text": "{{Infobox software\n| name                   = LIBRSB\n| title                  = \n| logo                   = \n| logo caption           = \n| screenshot             = \n[[File:rsb-audikw_1-2.png|Example rendering of RSB matrix created with librsb.|160px]]\n| caption                = \n| collapsible            = \n| author                 = Michele Martone\n| developer              = \n| released               = \n| discontinued           = \n| latest release version = 1.2.0\n| latest release date    = {{Start date|2016|09}}\n| latest preview version = \n| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| frequently updated     = <!-- DO NOT include this parameter unless you know what it does -->\n| language               = [[C (programming language)|C]], [[C++]], [[Fortran]]\n| operating system       = [[Cross-platform]]\n| platform               = \n| size                   = \n| status                 = Active\n| genre                  = [[Software library]]\n| license                = [[GPL License]]\n| alexa                  = \n| website                = [http://librsb.sf.net http://librsb.sf.net]\n}}\n\n'''librsb''' is an open-source [[parallel computing|parallel]] library for [[Sparse Matrix|sparse matrix]] computations using the ''Recursive Sparse Blocks'' (RSB) matrix format.\n                          \n'''librsb''' provides [[Cache (computing)|cache]] efficient [[Thread (computing)|multi-threaded]] '''Sparse BLAS''' operations via [[OpenMP]], and is best suited to large [[Sparse Matrix|sparse matrices]].\n\n== Features ==\n'''librsb''' provides:\n*Conversion from/to COO, CSR, CSC sparse matrix formats.\n*Support for the four BLAS types.\n*Support for general, symmetric, hermitian matrices.\n*Parallel threaded, eventually strided:\n**Sparse matrix-vector multiplication.                                                                                         \n**Sparse matrix-dense matrix multiplication.\n**Sparse matrix-vector triangular solve.\n**Sparse matrix-dense matrix triangular solve.\n*Sparse matrix-sparse matrix multiplication.\n*Elemental sparse matrix operations (scaling, add, etc).\n*Row-wise or column-wise scaling.\n*Rows / columns extraction.\n*An online empirical autotuning function.\n*File input/output in the Matrix Market format.\n*Rendering of the RSB structure into EPS (Encapsulated Postscript) figures.\n*A program for benchmarking / testing performance.\n*Implements the Sparse [[BLAS]] standard, as specified in the BLAS Technical Forum.<ref>[http://www.netlib.org/blas/blast-forum/ BLAS Technical Forum]</ref> documents.\n\n== System requirements ==        \n'''librsb''' can be used from:\n\n*C and C++ (''rsb.h'' interface)\n*Fortran 90/95/2003 (''module rsb'')\n*GNU Octave (sparsersb package for GNU Octave) [[GNU Octave]] package <ref>[http://octave.sourceforge.net/sparsersb/  sparsersb: \"Interface to the librsb package implementing the RSB sparse matrix format for fast shared-memory sparse matrix computations.\" ]</ref>\n\n== References ==\n{{reflist}}\n* [https://dx.doi.org/10.1016/j.parco.2014.03.008  Martone, M. Efficient multithreaded untransposed, transposed or symmetric sparse matrix-vector multiplication with the Recursive Sparse Blocks format. ''Parallel Computing 40(7): 251-270 (2014)']\n\n== External links ==\n* {{official website|http://librsb.sf.net/|librsb.sf.net}}\n* [http://librsb.sourceforge.net/ librsb.sourceforge.net]\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]\n[[Category:C libraries]]\n[[Category:Fortran libraries]]\n[[Category:Free software programmed in C]]\n[[Category:Numerical libraries]]\n[[Category:Scientific simulation software]]\n[[Category:Free simulation software]]\n[[Category:Free software programmed in Fortran]]"
    },
    {
      "title": "LINPACK",
      "url": "https://en.wikipedia.org/wiki/LINPACK",
      "text": "{{Other uses|LINPACK (disambiguation)}}\n{{Infobox software\n| name                   = Lunpack\n| title                  = LINPACK\n| logo                   = <!-- [[File: ]] -->\n| screenshot             = <!-- [[File: ]] -->\n| caption                = \n| collapsible            = \n| author                 = [[Jack Dongarra]], Jim Bunch, [[Cleve Moler]], and Gilbert Stewart\n| developer              = \n| released               = <!-- {{Start date|YYYY|MM|DD|df=yes/no}} -->\n| discontinued           = \n| latest release version = \n| latest release date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| latest preview version = \n| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| frequently updated     = <!-- DO NOT include this parameter unless you know what it does -->\n| programming language   = [[Fortran]]\n| operating system       = \n| platform               = \n| size                   = \n| language               = \n| status                 = \n| genre                  = [[Library (computer science)|Library]]\n| license                = \n| website                = {{URL|http://www.netlib.org/linpack/}}\n}}\n'''LINPACK''' is a software [[Library (computer science)|library]] for performing numerical [[linear algebra]] on [[digital computers]].  It was written in [[Fortran]] by [[Jack Dongarra]], Jim Bunch, [[Cleve Moler]], and Gilbert Stewart, and was intended for use on [[supercomputer]]s in the 1970s and early 1980s.<ref>\n{{cite news\n|title=Sidebar: The Linpack Benchmark\n|first=Jan\n|last=Matlis\n|date=2005-05-30\n|newspaper=ComputerWorld\n|url=http://www.computerworld.com/s/article/102050/Sidebar_The_Linpack_Benchmark\n}}</ref><ref>\n{{ cite news\n|newspaper=New York Times\n|title=Technology; Measuring How Fast Computers Really Are\n|first=John\n|last=Markoff\n|date=1991-09-22\n|url=https://select.nytimes.com/gst/abstract.html?res=F20616FF39550C718EDDA00894D9494D81\n}}</ref>  It has been largely superseded by [[LAPACK]], which runs more efficiently on modern architectures.\n\nLINPACK makes use of the [[Basic Linear Algebra Subprograms|BLAS]] (Basic Linear Algebra Subprograms) libraries for performing basic vector and matrix operations.\n\nThe [[LINPACK benchmarks]] appeared initially as part of the LINPACK user's manual. The parallel LINPACK benchmark implementation called HPL (High Performance Linpack) is used to benchmark and rank supercomputers for the [[TOP500]] list.\n\n==References==\n{{reflist}}\n\n==External links==\n* http://www.netlib.org/linpack/\n* http://www.netlib.org/blas/\n\n{{Numerical linear algebra}}\n{{Benchmark}}\n\n{{DEFAULTSORT:Linpack}}\n[[Category:Benchmarks (computing)]]\n[[Category:Fortran libraries]]\n[[Category:Numerical linear algebra]]\n[[Category:Numerical software]]\n\n\n{{compu-eng-stub}}"
    },
    {
      "title": "Lis (linear algebra library)",
      "url": "https://en.wikipedia.org/wiki/Lis_%28linear_algebra_library%29",
      "text": "{{Infobox software\n| name                   = Lis\n| title                  = \n| logo                   = Irises screen 1.jpg\n| logo size              = 160px\n| logo caption           = \n| screenshot             = <!-- [[File: ]] -->\n| caption                = \n| collapsible            = \n| author                 = \n| developer              = \n| released               = \n| discontinued           = \n| latest release version = 2.0.19\n| latest release date    = {{Start date|2019|05|22}}\n| latest preview version = \n| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| language               = [[C (programming language)|C]], [[Fortran]]\n| operating system       = [[Cross-platform]]\n| platform               = \n| size                   = \n| genre                  = [[Software library]]\n| license                = [[New BSD License]]\n| alexa                  = \n| website                = [https://www.ssisc.org/lis/index.en.html www.ssisc.org/lis/]\n}}\n'''Lis''' ('''Library of Iterative Solvers''' for linear systems, pronounced [lis]) is a [[scalability|scalable]] [[parallel computing|parallel]] software library for solving [[system of linear equations|linear equation]]s and [[Eigenvalue, eigenvector and eigenspace|eigenvalue problems]] that arise in the [[numerical analysis|numerical solution]] of [[partial differential equation]]s by using [[iterative method]]s.<ref>{{Cite book\n|author=Akira Nishida \n|year=2010\n|chapter=Experience in Developing an Open Source Scalable Software Infrastructure in Japan\n|title=Computational Science and Its Applications – ICCSA 2010 \n|series=Lecture Notes in Computer Science 6017\n|pages=87–98\n|publisher=Springer\n|doi=10.1007/978-3-642-12165-4_36\n|isbn=3-642-12164-0 \n}}</ref><ref>{{Cite book\n|author1=Hisashi Kotakemori |author2=Hidehiko Hasegawa |author3=Tamito Kajiyama |author4=Akira Nukada |author5=Reiji Suda |author6=Akira Nishida  |last-author-amp=yes |year=2008\n|chapter=Performance Evaluation of Parallel Sparse Matrix-Vector Products on SGI Altix 3700\n|title=OpenMP Shared Memory Parallel Programming \n|series=Lecture Notes in Computer Science 4315\n|pages=153–163\n|publisher=Springer\n|doi=10.1007/978-3-540-68555-5_13 \n|isbn=3-540-68554-5 \n}}</ref><ref>{{Cite book\n|author1=Hisashi Kotakemori |author2=Hidehiko Hasegawa |author3=Akira Nishida  |last-author-amp=yes |year=2005\n|chapter=Performance Evaluation of a Parallel Iterative Method Library using OpenMP\n|title=Proceedings of the 8th International Conference on High Performance Computing in Asia Pacific Region (HPC Asia 2005)\n|pages=432–436\n|publisher=IEEE\n|doi=10.1109/HPCASIA.2005.74\n|isbn=0-7695-2486-9\n}}</ref> Although it is designed mainly for parallel computers, the library can be used without being conscious of [[parallel computing|parallel processing]].\n\n== Features ==\n'''Lis''' provides facilities for:\n* [[Configure script|Automatic program configuration]]\n* [[Non-Uniform Memory Access|NUMA]] aware hybrid implementation with [[Message Passing Interface|MPI]] and [[OpenMP]]\n* Exchangeable dense and [[sparse matrix]] [[data structure|storage format]]s\n* Basic [[linear algebra]] operations for dense and sparse matrices\n* Parallel [[iterative method]]s for [[system of linear equations|linear equation]]s and [[Eigenvalue, eigenvector and eigenspace|eigenvalue problems]]\n* Parallel [[preconditioner]]s for iterative methods\n* [[Quadruple-precision floating-point format|Quadruple precision]] [[floating point]] operations\n* [[profiling (computer programming)|Performance analysis]]\n* [[Command-line interface]] to solvers and benchmarks\n\n== Example ==\nA [[C (programming language)|C]] program to solve the linear equation <math>Ax=b</math> is written as follows:\n<source lang=c>\n#include <stdio.h>\n#include \"lis_config.h\"\n#include \"lis.h\"\n\nLIS_INT main(LIS_INT argc, char* argv[])\n{\n  LIS_MATRIX  A;\n  LIS_VECTOR  b, x;\n  LIS_SOLVER  solver;\n  LIS_INT     iter;\n  double      time;\n\n  lis_initialize(&argc, &argv);\n\n  lis_matrix_create(LIS_COMM_WORLD, &A);\n  lis_vector_create(LIS_COMM_WORLD, &b);\n  lis_vector_create(LIS_COMM_WORLD, &x);\n\n  lis_input_matrix(A, argv[1]);\n  lis_input_vector(b, argv[2]);\n  lis_vector_duplicate(A, &x);\n\n  lis_solver_create(&solver);\n  lis_solver_set_optionC(solver);\n  lis_solve(A, b, x, solver);\n\n  lis_solver_get_iter(solver, &iter);\n  lis_solver_get_time(solver, &time);\n  printf(\"number of iterations = %d\\n\", iter);\n  printf(\"elapsed time = %e\\n\", time);\n\n  lis_output_vector(x, LIS_FMT_MM, argv[3]);\n\n  lis_solver_destroy(solver);\n  lis_matrix_destroy(A);\n  lis_vector_destroy(b);\n  lis_vector_destroy(x);\n\n  lis_finalize();\n\n  return 0;\n}\n</source>\n\n== System requirements ==\nThe installation of '''Lis''' requires a C compiler. The Fortran interface requires a Fortran compiler, and the [[Multigrid method|algebraic multigrid]] preconditioner requires a Fortran 90 compiler.<ref>{{Cite book\n|author1=Akihiro Fujii |author2=Akira Nishida |author3=Yoshio Oyanagi  |last-author-amp=yes |year=2005\n|chapter=Evaluation of Parallel Aggregate Creation Orders : Smoothed Aggregation Algebraic Multigrid Method\n|title=High Performance Computational Science and Engineering\n|pages=99–122\n|publisher=Springer\n|doi=10.1007/0-387-24049-7_6  \n|isbn=1-4419-3684-X \n}}</ref> \nFor parallel computing environments, an OpenMP or MPI library is used. Both the [[Harwell-Boeing file format|Harwell-Boeing]] and [[Matrix Market exchange formats|Matrix Market]] formats are supported to import and export user data.\n\n== Packages that use Lis ==\n* [[Gerris (software)|Gerris]]\n* [[OpenModelica]]\n* [http://www.opengeosys.org/ OpenGeoSys]\n* [http://www.sicopolis.net/ SICOPOLIS]\n* [http://stomp.pnnl.gov/ STOMP]\n* [http://dl.acm.org/citation.cfm?id=2634291 Diablo]\n* [https://kiva.readthedocs.io/en/latest/ Kiva]\n* [https://notus-cfd.org/ Notus]\n* [http://www.hamady.org/comet.html Comet]\n\n== See also ==\n{{Portal|Information technology|Free and open-source software}}\n* [[List of numerical libraries]]\n\n== References ==\n{{reflist}}\n\n== External links ==\n* {{Official website|https://www.ssisc.org/lis/index.en.html}}\n* [https://github.com/anishida/lis/tree/develop Development repository on GitHub]\n* [http://www.netlib.org/utk/people/JackDongarra/la-sw.html Prof. Jack Dongarra's freely available linear algebra software page]\n* [http://www.netlib.org/linalg/lis Netlib repository] (Courtesy of [[Netlib|Netlib Project]])\n* [http://apps.fedoraproject.org/packages/lis Fedora packages] (Courtesy of [[Fedora Project]])\n* [http://packages.gentoo.org/package/sci-libs/lis Gentoo packages] (Courtesy of [[Gentoo Linux|Gentoo Linux Project]])\n* [https://svnweb.freebsd.org/ports/head/math/lis/ FreeBSD packages] (Courtesy of [[FreeBSD|FreeBSD Project]])\n* [http://sourceforge.net/projects/whpc/files/ Packages for Windows] (Courtesy of WHPC Project)\n* [https://github.com/brewsci/homebrew-science/blob/master/Formula/lis.rb Packages for Mac OS X] (Courtesy of [[Homebrew (package management software)|Homebrew Project]])\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical libraries]]\n[[Category:Numerical linear algebra]]\n[[Category:Scientific simulation software]]\n[[Category:C libraries]]\n[[Category:Fortran libraries]]\n[[Category:Free simulation software]]\n[[Category:Free software programmed in C]]\n[[Category:Free software programmed in Fortran]]"
    },
    {
      "title": "LOBPCG",
      "url": "https://en.wikipedia.org/wiki/LOBPCG",
      "text": "'''Locally Optimal Block Preconditioned Conjugate Gradient''' ('''LOBPCG''') is a [[matrix-free methods|matrix-free method]] for finding the largest (or smallest) [[eigenvalues]] and the corresponding [[eigenvectors]] of a symmetric positive definite [[generalized eigenvalue problem]]\n\n:<math>A x= \\lambda B x,</math>\n\nfor a given pair <math>(A, B)</math> of complex [[Hermitian matrix|Hermitian]] or real [[Symmetric matrix|symmetric]] matrices, where\nthe matrix <math>B</math> is also assumed [[positive-definite matrix|positive-definite]].\n\n==Background==\n[[Kantorovich]] in 1948 proposed calculating the smallest [[eigenvalue]] <math>\\lambda_1</math> of a symmetric matrix <math>A</math> by [[steepest descent]] using a direction <math>r = Ax-\\lambda (x) x</math> of a scaled [[gradient]] of a [[Rayleigh quotient]] <math>\\lambda(x) = (x, Ax)/(x, x)</math> in a [[scalar product]] <math>(x, y) = x'y</math>, with the step size computed by minimizing the Rayleigh quotient in the [[linear span]] of the vectors <math>x</math> and <math>w</math>, i.e. in a locally optimal manner. Samokish<ref name=\"S58\">{{Cite journal| title = The steepest descent method for an eigenvalue problem with semi-bounded operators| journal = Izvestiya Vuzov, Math.| issue = 5| pages = 105–114| year = 1958| last1 = Samokish| first1 = B.A. }}</ref> proposed applying a [[preconditioner]] <math>T</math> to the residual vector <math>r</math> to generate the preconditioned direction <math>w = T r</math> and derived asymptotic, as <math>x</math> approaches the [[eigenvector]], convergence rate bounds. D'yakonov suggested<ref name=\"D\">{{cite book |title= Optimization in solving elliptic problems |last=  D'yakonov |first= E. G. |year= 1996 |publisher= CRC-Press |isbn= 978-0-8493-2872-5 |pages=592}}</ref> spectrally equivalent [[preconditioning]] and derived non-asymptotic convergence rate bounds. Block locally optimal multi-step steepest descent for eigenvalue problems was described in.<ref name=\"CW\">{{Cite book| title = Lanczos algorithms for large symmetric eigenvalue computations. Vol. 1 (Reprint of the 1985 original)| year = 2002| last1 = Cullum| first1 = Jane K.| last2 = Willoughby| first2 = Ralph A.| publisher = [[Society for Industrial and Applied Mathematics]]}}</ref> Local minimization of the Rayleigh quotient on the subspace spanned by the current approximation, the current residual and the previous approximation, as well as its block version, appeared in.<ref name=\"K87\">{{Cite journal| title = Convergence rate estimates for iterative methods for mesh symmetric eigenvalue  problem| journal = Soviet J. Numerical Analysis and Math. Modelling| volume = 2| number =5| pages = 371–396| year = 1987| last1 = Knyazev | first1 = Andrew V. }}</ref> The preconditioned version was analyzed in <ref name=\"K91\">{{Cite journal| title = A preconditioned conjugate gradient method  for eigenvalue problems and its implementation in a subspace| journal = International  Ser. Numerical Mathematics, v. 96, Eigenwertaufgaben in Natur- und Ingenieurwissenschaften und  ihre numerische  Behandlung,  Oberwolfach 1990, Birkhauser| pages = 143–154| year = 1991| last1 = Knyazev | first1 = Andrew V. }}</ref> and.<ref name=\"K98\">{{Cite journal| title = Preconditioned eigensolvers - an oxymoron?| journal = [[Electronic Transactions on Numerical Analysis]]| volume = 7| pages = 104–123| year = 1998| last1 = Knyazev | first1 = Andrew V. }}</ref>\n\n==Main features<ref name=\"K2017\">{{cite arxiv| title = Recent implementations, applications, and extensions of the Locally Optimal Block Preconditioned Conjugate Gradient method (LOBPCG)| eprint = 1708.08354| year = 2017| last1 = Knyazev | first1 = Andrew| class = cs.NA}}</ref>==\n* LOBPCG is [[Matrix-free methods|matrix-free]], i.e. does not require storing the coefficient matrix explicitly, but can accesses the matrix by evaluating matrix-vector products.\n* The costs per iteration and the memory use in LOBPCG are competitive with those of the [[Lanczos method]], computing a single extreme eigenpair.\n* Linear convergence is theoretically guaranteed and practically observed, since local optimality implies that LOBPCG converges at least as fast as the [[gradient descent method]]. In numerical tests, LOBPCG typically shows no super-linear convergence.\n* LOBPCG blocking allows utilizing highly efficient matrix-matrix operations, e.g., [[BLAS]] 3.\n* LOBPCG can directly take advantage of [[preconditioning]], in contrast to the [[Lanczos method]]. LOBPCG allows variable and non-symmetric as well as fixed and positive definite [[preconditioning]].\n* LOBPCG allows warm starts and computes an approximation to the eigenvector on every iteration. It has no numerical stability issues similar to those of the Lanczos method.\n* LOBPCG is reasonably easy to implement, so many implementations have appeared.\n* LOBPCG general technology can also be viewed as a particular case of generalized block Davidson diagonalization methods with thick restart, or accelerated block [[gradient descent]] with plane-search. \n* Very large block sizes in LOBPCG become expensive to deal with due to orthogonalizations and the use of the [[Rayleigh-Ritz method]] on every iteration.\n\n==Algorithm==\n===Single-vector version===\nThe method performs an [[iterative]] maximization (or minimization) of the generalized [[Rayleigh quotient]]\n\n:<math>\\rho(x) := \\rho(A,B; x) :=\\frac{x^T A x}{x^T B x},</math>\n\nwhich results in finding largest (or smallest) eigenpairs of <math>A x= \\lambda B x.</math>\n\nThe direction of the steepest ascent, which is the [[gradient]], of the generalized [[Rayleigh quotient]] is positively proportional to the vector\n\n:<math>r := Ax - \\rho(x) Bx,</math>\n\ncalled the eigenvector [[Residual (numerical analysis)|residual]]. If a [[preconditioner]] <math>T</math> is available, it is applied to the residual and gives the vector\n\n:<math>w := Tr,</math>\n\ncalled the preconditioned residual. Without preconditioning, we set \n<math>T := I</math> and so <math>w := r</math>. An iterative method\n\n:<math>x^{i+1} := x^i + \\alpha^i T(Ax^i - \\rho(x^i) Bx^i),</math>\n\nor, in short,\n\n:<math>x^{i+1} := x^i + \\alpha^i w^i,\\,</math>\n:<math>w^i := Tr^i,\\,</math>\n:<math>r^i := Ax^i - \\rho(x^i) Bx^i,</math>\n\nis known as preconditioned [[steepest ascent]] (or descent), where the scalar \n<math>\\alpha^i</math> is called the step size. The optimal step size can be determined by maximizing the Rayleigh quotient, i.e.,\n\n:<math>x^{i+1} := \\arg\\max_{y\\in span\\{x^i,w^i\\}} \\rho(y)</math>\n\n(or <math>\\arg\\min</math> in case of minimizing), \nin which case the method is called locally optimal. To further accelerate the convergence of the locally optimal preconditioned steepest ascent (or descent), one can add one extra vector to the two-term [[recurrence relation]] to make it three-term:\n\n:<math>x^{i+1} := \\arg\\max_{y\\in span\\{x^i,w^i,x^{i-1}\\}} \\rho(y)</math>\n\n(use <math>\\arg\\min</math> in case of minimizing). The maximization/minimization of the Rayleigh quotient in a 3-dimensional subspace can be performed numerically by the [[Rayleigh–Ritz method]].\nAs the iterations converge, the vectors <math>x^i</math> and <math>x^{i-1}</math> become nearly linearly dependent, making the [[Rayleigh–Ritz method]] numerically unstable in the presence of round-off errors. It is possible to substitute the vector <math>x^{i-1}</math> with an explicitly computed difference <math>p^i=x^{i-1}-x^i</math> making the [[Rayleigh–Ritz method]] more stable; see.<ref name=\"AK2001\">{{Cite journal| doi = 10.1137/S1064827500366124| title = Toward the Optimal Preconditioned Eigensolver: Locally Optimal Block Preconditioned Conjugate Gradient Method| journal = [[SIAM Journal on Scientific Computing]]| volume = 23| issue = 2| pages = 517–541| year = 2001| last1 = Knyazev | first1 = Andrew V. }}</ref>\n\nThis is a single-vector version of the LOBPCG method—one of possible generalization of the [[Preconditioner|preconditioned]] [[Conjugate gradient method|conjugate gradient]] linear solvers to the case of symmetric [[eigenvalue]] problems.<ref name=\"AK2001\" /> Even in the trivial case <math>T=I</math> and <math>B=I</math> the resulting approximation with <math>i>3</math> will be different from that obtained by the [[Lanczos algorithm]], although both approximations will belong to the same [[Krylov subspace]].\n\n===Block version===\nIterating several approximate [[eigenvectors]] together in a block in a similar locally optimal fashion, gives the full block version of the LOBPCG.<ref name=\"AK2001\" /> It allows robust computation of eigenvectors corresponding to nearly-multiple eigenvalues.\n\n==Convergence theory and practice==\nLOBPCG by construction is guaranteed<ref name=\"AK2001\" /> to minimize the [[Rayleigh quotient]] not slower than the block steepest [[gradient descent]], which has a comprehensive convergence theory. Every [[eigenvector]] is a stationary point of the [[Rayleigh quotient]], where the [[gradient]] vanishes. Thus, the [[gradient descent]] may slow down in a vicinity of any [[eigenvector]], however, it is guaranteed to either converge to the eigenvector with a linear convergence rate or, if this eigenvector is a [[saddle point]], the iterative [[Rayleigh quotient]] is more likely to drop down below the corresponding eigenvalue and start converging linearly to the next eigenvalue below. The worst value of the linear linear convergence rate has been determined<ref name=\"AK2001\" /> and depends on the relative gap between the eigenvalue and the rest of the matrix [[spectrum]] and the quality of the [[preconditioner]], if present. \n\nFor a general matrix, there is evidently no way to predict the eigenvectors and thus generate the initial approximations that always work well. The iterative solution by LOBPCG may be sensitive to the initial eigenvectors approximations, e.g., taking longer to converge slowing down as passing intermediate eigenpairs. Moreover, in theory, one cannot guarantee convergence necessarily to the smallest eigenpair, although the probability of the miss is zero. A good quality [[random]] [[Gaussian]] function with the zero [[mean]] is commonly the default in LOBPCG to generate the initial approximations. To fix the initial approximations, one can select a fixed seed for the [[random number generator]]. \n\nIn contrast to the [[Lanczos method]], LOBPCG rarely exhibits asymptotic [[superlinear convergence]] in practice.\n\n==Partial [[Principal component analysis]] (PCA) and [[Singular Value Decomposition]] (SVD)==\nLOBPCG can be trivially adopted for computing several largest [[singular values]] and the corresponding singular vectors (partial SVD), e.g., for [[Principal_component_analysis#Iterative_computation| iterative computation of PCA]], for a data matrix {{math|'''D'''}} with zero mean, without explicitly computing the [[covariance]] matrix {{math|'''D<sup>T</sup>D'''}}, i.e. in [[matrix-free methods | matrix-free fashion]]. The main calculation is evaluation of a function of the product {{math|'''D<sup>T</sup>(D X)'''}} of the covariance matrix {{math|'''D<sup>T</sup>D'''}} and the block-vector {{math|'''X'''}} that iteratively approximates the desired singular vectors. PCA needs the largest eigenvalues of the covariance matrix, while LOBPCG is typically implemented to calculate the smallest ones. A simple work-around is to negate the function, substituting {{math|'''-D<sup>T</sup>(D X)'''}} for {{math|'''D<sup>T</sup>(D X)'''}} and thus reversing the order of the eigenvalues, since LOBPCG does not care if the matrix of the eigenvalue problem is positive definite or not.<ref>[https://www.mathworks.com/matlabcentral/fileexchange/48-lobpcg-m LOBPCG] at [[Mathworks]]</ref>\n\n==General software implementations==\nLOBPCG's inventor, [[Andrei Knyazev (mathematician)|Andrew Knyazev]], published a reference implementation called Block Locally Optimal Preconditioned Eigenvalue Xolvers (BLOPEX)<ref>{{Cite journal | doi = 10.1137/060661624| title = Block Locally Optimal Preconditioned Eigenvalue Xolvers (BLOPEX) in Hypre and PETSc| journal = SIAM Journal on Scientific Computing| volume = 29| issue = 5| pages = 2224| year = 2007| last1 = Knyazev | first1 = A. V.| last2 = Argentati | first2 = M. E.| last3 = Lashuk | first3 = I.| last4 = Ovtchinnikov | first4 = E. E.| arxiv = 0705.2626}}</ref> with interfaces to [[Portable, Extensible Toolkit for Scientific Computation|PETSc]], [[hypre]], and Parallel Hierarchical Adaptive MultiLevel method (PHAML)<ref>PHAML [https://math.nist.gov/~WMitchell/phaml/phaml.html#goals BLOPEX interface to LOBPCG]</ref>. Other implementations are available in, e.g., [[Octave]]<ref>[https://octave.sourceforge.io/linear-algebra/function/lobpcg.html Octave linear-algebra function lobpcg]</ref>, [[MATLAB]]<ref>[[MATLAB]] [https://www.mathworks.com/matlabcentral/fileexchange/48-lobpcg-m File Exchange function LOBPCG]</ref>, [[Java (programming language)|Java]]<ref>[https://code.google.com/archive/p/sparse-eigensolvers-java/ Java LOBPCG] at [[Google Code]]</ref>, Anasazi ([[Trilinos]])<ref>[https://github.com/trilinos/Trilinos/blob/master/packages/anasazi/src/AnasaziLOBPCG.hpp Anasazi Trilinos LOBPCG] at [[github]]</ref>, [[SLEPc]]<ref>[http://slepc.upv.es/documentation/current/src/eps/impls/cg/lobpcg/lobpcg.c.html Native SLEPc LOBPCG]</ref><ref>[[SLEPc]] [[BLOPEX]] [https://bitbucket.org/joseroman/blopex interface to LOBPCG]</ref>, [[SciPy]]<ref>[[SciPy]] [https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.sparse.linalg.lobpcg.html sparse linear algebra function lobpcg] </ref>,  [[Julia (programming language)|Julia]]<ref> [[Julia (programming language)|Julia]] [https://github.com/JuliaMath/IterativeSolvers.jl/blob/master/src/lobpcg.jl LOBPCG] at [[github]]</ref>, MAGMA<ref>{{Cite journal | url = http://dl.acm.org/citation.cfm?id=2872609 | title = Accelerating the LOBPCG method on GPUs using a blocked sparse matrix vector product| journal = Proceedings of the Symposium on High Performance Computing (HPC '15). Society for Computer Simulation International, San Diego, CA, USA| pages = 75–82| year = 2015| last1 = Anzt| first1 = Hartwig| last2 = Tomov | first2 =  Stanimir| last3 = Dongarra | first3 = Jack}}</ref> and [[NVIDIA]] AMGX.<ref>[[NVIDIA]] AMGX [https://github.com/NVIDIA/AMGX/blob/master/eigen_examples/LOBPCG LOBPCG] at [[github]]</ref>\n\n==Applications==\n===[[Material Sciences]]===\nLOBPCG is implemented in [[ABINIT]]<ref>[https://docs.abinit.org/variables/dev/#wfoptalg ABINIT Docs: WaveFunction OPTimisation ALGorithm]</ref> (including [[CUDA]] version) and [[Octopus (software)|Octopus]].<ref>[http://octopus-code.org/wiki/Developers_Manual:LOBPCG Octopus Developers Manual:LOBPCG]</ref> It has been used for multi-billion size matrices by [[Gordon Bell Prize]] finalists, on the [[Earth Simulator]] [[supercomputer]] in Japan.<ref>{{Cite conference| doi = 10.1109/SC.2005.1| title = 16.447 TFlops and 159-Billion-dimensional Exact-diagonalization for Trapped Fermion-Hubbard Model on the Earth Simulator| work = Proc. ACM/IEEE Conference on Supercomputing (SC'05)| pages = 44| year = 2005| last1 = Yamada | first1 = S.| last2 = Imamura | first2 = T.| last3 = Machida | first3 = M.| isbn = 1-59593-061-2}}</ref><ref>{{Cite conference| doi = 10.1145/1188455.1188504| title = Gordon Bell finalists I—High-performance computing for exact numerical approaches to quantum many-body problems on the earth simulator| conference = Proc. ACM/IEEE conference on Supercomputing (SC '06)| pages = 47| year = 2006| last1 = Yamada | first1 = S. | last2 = Imamura | first2 = T. | last3 = Kano | first3 = T. | last4 = Machida | first4 = M. | isbn = 0769527000}}</ref> Recent implementations include TTPY,<ref>{{Cite journal | doi = 10.1063/1.4962420| title = Calculating vibrational spectra of molecules using tensor train decomposition| journal =  J. Chem. Phys. | volume = 145| year = 2016| issue = 145| pages = 124101| last1 =  Rakhuba| first1 = Maxim | last2 =  Oseledets | first2 =   Ivan| bibcode = 2016JChPh.145l4101R| arxiv =1605.08422}}</ref> Platypus‐QM,<ref>{{Cite journal | doi = 10.1002/jcc.24318 | title = Development of massive multilevel molecular dynamics simulation program, platypus (PLATform for dYnamic protein unified simulation), for the elucidation of protein functions| journal = J. Comput. Chem.| volume = 37| issue = 12| pages = 1125–1132| year = 2016| last1 =  Takano| first1 = Yu | last2 =  Nakata | first2 =  Kazuto| last3 =  Yonezawa | first3 =  Yasushige| last4 =  Nakamura | first4 =  Haruki| pmc =4825406}}</ref> and MFDn.<ref>{{Cite journal|arxiv=1609.01689 | title = Accelerating Nuclear Configuration Interaction Calculations through a Preconditioned Block Iterative Eigensolver| journal = Computer Physics Communications| volume = 222| issue = 1| pages = 1–13| year = 2018| last1 =  Shao| first1 = Meiyue | display-authors =  etal| doi = 10.1016/j.cpc.2017.09.004}}</ref> \n[[Hubbard model]] for strongly-correlated electron systems to understand the mechanism behind the [[superconductivity]] uses LOBPCG to calculate the [[ground state]] of the [[Hamiltonian (quantum mechanics)|Hamiltonian]] on the [[K computer]].<ref>{{Cite conference| doi = 10.1007/978-3-319-69953-0_14 | conference = Asian Conference on Supercomputing Frontiers | title = High Performance LOBPCG Method for Solving Multiple Eigenvalues of Hubbard Model: Efficiency of Communication Avoiding Neumann Expansion Preconditioner | work = Yokota R., Wu W. (eds) Supercomputing Frontiers. SCFA 2018. Lecture Notes in Computer Science, vol 10776. Springer, Cham| pages =  243–256| year = 2018| last1 = Yamada | first1 = S.| last2 = Imamura | first2 = T.| last3 = Machida | first3 = M.}}</ref>\n\n===[[Mechanics]] and [[Fluids]]===\nLOBPCG from BLOPEX is used for [[preconditioner]] setup in Multilevel [[Balancing Domain Decomposition by Constraints]] (BDDC) solver library BDDCML, which is incorporated into OpenFTL (Open [[Finite element]] Template Library) and Flow123d simulator of underground water flow, solute and [[heat transport]] in fractured [[porous media]].\n\n===[[Maxwell's equations]]===\nLOBPCG is one of core eigenvalue solvers in PYFEMax and high performance multiphysics [[finite element]] software Netgen/NGSolve. LOBPCG from [[hypre]] is incorporated into [[open source]] lightweight scalable [[C++]] library for [[finite element]] methods [[MFEM]], which is used in many projects, including [[BLAST]], XBraid, [[VisIt]], xSDK, the FASTMath institute in [[SciDAC]], and the co-design Center for Efficient Exascale Discretizations (CEED) in the [[Exascale computing]] Project.\n\n===[[Denoising]]===\nIterative LOBPCG-based approximate [[low-pass filter]] can be used for [[denoising]]; see,<ref>{{Cite conference | url = http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7324315&isnumber=7324166 |  \nfirst1 = A. | last1 =Knyazev | first2 = A. | last2 =Malyshev | title = Accelerated graph-based spectral polynomial filters | conference = 2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP), Boston, MA | year = 2015 | pages = 1–6 | doi = 10.1109/MLSP.2015.7324315| arxiv = 1509.02468 }}</ref> e.g., to accelerate [[total variation denoising]].\n\n===[[image segmentation | Image Segmentation]]===\n[[Hypre]] implementation of LOBPCG with [[multigrid]] [[preconditioning]] has been applied to [[image segmentation]] in <ref>{{Cite conference | url = http://math.ucdenver.edu/~aknyazev/research/conf/ICDM03.pdf | title =  Modern preconditioned eigensolvers for spectral image segmentation and graph bisection | conference = Clustering Large Data Sets; Third IEEE International Conference on Data Mining (ICDM 2003) Melbourne, Florida: IEEE Computer Society| editor = Boley| editor2 = Dhillon| editor3 = Ghosh| editor4 = Kogan | pages = 59–62| year = 2003| last1 =  Knyazev| first1 = Andrew V.}}</ref> via [[spectral]] [[graph partitioning]] using the [[graph Laplacian]] for the [[bilateral filter]].\n\n===[[Data Mining]]===\nSoftware packages [[scikit-learn]] and Megaman<ref>{{Cite journal | url = http://jmlr.org/papers/v17/16-109.html | title = Megaman: Scalable Manifold Learning in Python| journal = Journal of Machine Learning Research| volume = 17| issue = 148| pages = 1–5| year = 2016| last1 =  McQueen| first1 = James | display-authors =  etal}}</ref> use LOBPCG to scale [[spectral clustering]]<ref>http://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html</ref> and [[manifold learning]]<ref>http://scikit-learn.org/stable/modules/generated/sklearn.manifold.spectral_embedding.html</ref> via [[Nonlinear_dimensionality_reduction#Laplacian_eigenmaps | Laplacian eigenmaps]] to large data sets. [[NVIDIA]] has implemented<ref>{{Cite journal | url = https://devblogs.nvidia.com/fast-spectral-graph-partitioning-gpus/ | title = Fast Spectral Graph Partitioning on GPUs| journal = NVIDIA Developer Blog| year = 2016| last1 =  Naumov| first1 = Maxim}}</ref> LOBPCG in its [[nvGRAPH]] library introduced in [[CUDA]] 8.\n\n==References==\n<references/>\n\n==External links==\n*[http://www.mathworks.com/matlabcentral/fileexchange/48-lobpcg-m LOBPCG] in [[MATLAB]]\n*[http://octave.sourceforge.net/linear-algebra/function/lobpcg.html LOBPCG] in [[Octave]]\n*[http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lobpcg.html LOBPCG] in [[SciPy]]\n*[https://code.google.com/p/sparse-eigensolvers-java/ LOBPCG] in [[Java (programming language)|Java]] at [[Google Code]]\n*[https://github.com/lobpcg/blopex LOBPCG in Block Locally Optimal Preconditioned Eigenvalue Xolvers (BLOPEX)] at [[GitHub]] and [https://code.google.com/p/blopex/ archived] at [[Google Code]]\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]\n[[Category:Scientific simulation software]]"
    },
    {
      "title": "LU decomposition",
      "url": "https://en.wikipedia.org/wiki/LU_decomposition",
      "text": "In [[numerical analysis]] and [[linear algebra]], '''lower–upper''' ('''LU''') '''decomposition''' or '''factorization''' factors a [[matrix (mathematics)|matrix]] as the product of a lower [[triangular matrix]] and an upper triangular matrix. The product sometimes includes a [[permutation matrix]] as well. LU decomposition can be viewed as the matrix form of [[Gaussian elimination]]. Computers usually solve square [[system of linear equations|systems of linear equations]] using LU decomposition, and it is also a key step when inverting a matrix or computing the [[determinant]] of a matrix. LU decomposition was introduced by mathematician [[Tadeusz Banachiewicz]] in 1938.<ref name=\"Schwarzenberg\">{{Cite journal |last1=Schwarzenberg-Czerny |first1=A. |title=On matrix factorization and efficient least squares solution |journal=Astronomy and Astrophysics Supplement Series |volume=110 |pages=405 |url=http://adsabs.harvard.edu/full/1995A%26AS..110..405S| bibcode=1995A&AS..110..405S |year=1995}}</ref>\n\n== Definitions ==\n\n[[File:LDU decomposition of Walsh 16.svg|thumb|300px|LDU decomposition of a [[Walsh matrix]]]]\n\nLet ''A'' be a square matrix. An '''LU factorization''' refers to the factorization of ''A'', with proper row and/or column orderings or permutations, into two factors – a lower triangular matrix ''L'' and an upper triangular matrix ''U'':\n:<math> A = LU. </math>\n\nIn the lower triangular matrix all elements above the diagonal are zero, in the upper triangular matrix, all the elements below the diagonal are zero.  For example, for a 3 × 3 matrix ''A'', its LU decomposition looks like this:\n:<math>\n  \\begin{bmatrix}\n    a_{11} & a_{12} & a_{13} \\\\\n    a_{21} & a_{22} & a_{23} \\\\\n    a_{31} & a_{32} & a_{33}\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    l_{11} &      0 & 0      \\\\\n    l_{21} & l_{22} & 0      \\\\\n    l_{31} & l_{32} & l_{33}\n  \\end{bmatrix}\n  \\begin{bmatrix}\n    u_{11} & u_{12} & u_{13} \\\\\n         0 & u_{22} & u_{23} \\\\\n         0 &      0 & u_{33}\n  \\end{bmatrix}.\n</math>\n\nWithout a proper ordering or permutations in the matrix, the factorization may fail to materialize.  For example, it is easy to verify (by expanding the matrix multiplication) that <math display=\"inline\">a_{11} = l_{11} u_{11}</math>. If <math display=\"inline\">a_{11} = 0</math>, then at least one of <math display=\"inline\">l_{11}</math> and <math display=\"inline\">u_{11}</math> has to be zero, which implies that either ''L'' or ''U'' is [[Singular matrix|singular]]. This is impossible if ''A'' is nonsingular (invertible). This is a procedural problem. It can be removed by simply reordering the rows of ''A'' so that the first element of the permuted matrix is nonzero. The same problem in subsequent factorization steps can be removed the same way; see the basic procedure below.\n\n=== LU factorization with partial pivoting ===\n\nIt turns out that a proper permutation in rows (or columns) is sufficient for LU factorization. '''LU factorization with partial pivoting''' (LUP) refers often to LU factorization with row permutations only:\n:<math> PA = LU, </math>\n\nwhere ''L'' and ''U'' are again lower and upper triangular matrices, and ''P'' is a [[permutation matrix]], which, when left-multiplied to ''A'', reorders the rows of ''A''. It turns out that all square matrices can be factorized in this form,<ref name=okunev-cor3>{{harvtxt|Okunev|Johnson|1997}}, Corollary 3.</ref> and the factorization is numerically stable in practice.<ref>{{harvtxt|Trefethen|Bau|1997}}, p. 166.</ref> This makes LUP decomposition a useful technique in practice.\n\n=== LU factorization with full pivoting ===\n\nAn '''LU factorization with full pivoting''' involves both row and column permutations:\n:<math> PAQ = LU, </math>\n\nwhere ''L'', ''U'' and ''P'' are defined as before, and ''Q'' is a permutation matrix that reorders the columns of ''A''.<ref>{{harvtxt|Trefethen|Bau|1997}}, p. 161.</ref>\n\n=== LDU decomposition===\n\nAn '''LDU decomposition''' is a decomposition of the form\n:<math> A = LDU, </math>\n\nwhere ''D'' is a [[diagonal matrix]], and ''L'' and ''U'' are ''unit'' triangular matrices, meaning that all the entries on the diagonals of ''L'' and ''U'' are one.\n\nAbove we required that ''A'' be a square matrix, but these decompositions can all be generalized to rectangular matrices as well.  In that case, ''L'' and ''D'' are square matrices both of which have the same number of rows as ''A'', and ''U'' has exactly the same dimensions as ''A''.  ''Upper triangular'' should be interpreted as having only zero entries below the main diagonal, which starts at the upper left corner.\n\n== Example ==\n\nWe factorize the following 2-by-2 matrix:\n: <math>\n  \\begin{bmatrix}\n    4 & 3 \\\\\n    6 & 3\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    l_{11} & 0      \\\\\n    l_{21} & l_{22}\n  \\end{bmatrix}\n  \\begin{bmatrix}\n    u_{11} & u_{12} \\\\\n         0 & u_{22}\n  \\end{bmatrix}.\n</math>\n\nOne way to find the LU decomposition of this simple matrix would be to simply solve the linear equations by inspection. Expanding the matrix multiplication gives\n\n:<math>\\begin{align}\n            l_{11} \\cdot u_{11} + 0 \\cdot 0 &= 4 \\\\\n       l_{11} \\cdot u_{12} + 0 \\cdot u_{22} &= 3 \\\\\n       l_{21} \\cdot u_{11} + l_{22} \\cdot 0 &= 6 \\\\\n  l_{21} \\cdot u_{12} + l_{22} \\cdot u_{22} &= 3.\n\\end{align}</math>\n\nThis system of equations is [[Underdetermined system|underdetermined]]. In this case any two non-zero elements of ''L'' and ''U'' matrices are parameters of the solution and can be set arbitrarily to any non-zero value. Therefore, to find the unique LU decomposition, it is necessary to put some restriction on ''L'' and ''U'' matrices. For example, we can conveniently require the lower triangular matrix ''L'' to be a unit triangular matrix (i.e. set all the entries of its main diagonal to ones). Then the system of equations has the following solution:\n\n:<math>\\begin{align}\n  l_{21} &=  1.5 \\\\\n  u_{11} &=  4   \\\\\n  u_{12} &=  3   \\\\\n  u_{22} &= -1.5\n\\end{align}</math>\n\nSubstituting these values into the LU decomposition above yields\n\n:<math>\n  \\begin{bmatrix}\n    4 & 3 \\\\\n    6 & 3\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    1   & 0 \\\\\n    1.5 & 1\n  \\end{bmatrix}\n  \\begin{bmatrix}\n    4 &  3 \\\\\n    0 & -1.5\n  \\end{bmatrix}.\n</math>\n\n== Existence and uniqueness ==\n\n=== Square matrices ===\n\nAny square matrix <math display=\"inline\"> A </math> admits an ''LUP'' factorization.<ref name=okunev-cor3 /> If <math display=\"inline\"> A </math> is [[invertible matrix|invertible]], then it admits an ''LU'' (or ''LDU'') factorization [[if and only if]] all its leading principal [[minor (linear algebra)|minors]] are nonzero.<ref name=horn-cor355>{{harvtxt|Horn|Johnson|1985}}, Corollary 3.5.5</ref> If <math display=\"inline\"> A </math> is a singular matrix of rank <math display=\"inline\"> k </math>, then it admits an ''LU'' factorization if the first <math display=\"inline\"> k </math> leading principal minors are nonzero, although the converse is not true.<ref>{{harvtxt|Horn|Johnson|1985}}, Theorem 3.5.2</ref>\n\nIf a square, invertible matrix has an ''LDU'' factorization with all diagonal entries of ''L'' and ''U'' equal to 1, then the factorization is unique.<ref name=horn-cor355 /> In that case, the ''LU'' factorization is also unique if we require that the diagonal of <math display=\"inline\"> L </math> (or <math display=\"inline\"> U </math>) consists of ones.\n\n=== Symmetric positive definite matrices ===\n\nIf ''A'' is a symmetric (or [[Hermitian matrix|Hermitian]], if ''A'' is complex) [[Positive-definite matrix|positive definite]] matrix, we can arrange matters so that ''U'' is the [[conjugate transpose]] of ''L''. That is, we can write ''A'' as\n: <math> A = LL^*. \\, </math>\n\nThis decomposition is called the [[Cholesky decomposition]]. The Cholesky decomposition always exists and is unique — provided the matrix is positive definite. Furthermore, computing the Cholesky decomposition is more efficient and [[numerical stability|numerically more stable]] than computing some other LU decompositions.\n\n=== General matrices ===\n\nFor a (not necessarily invertible) matrix over any field, the exact necessary and sufficient conditions under which it has an LU factorization are known. The conditions are expressed in terms of the ranks of certain submatrices. The Gaussian elimination algorithm for obtaining LU decomposition has also been extended to this most general case.<ref>{{harvtxt|Okunev|Johnson|1997}}</ref>#\n\n== Algorithms ==\n\nLU decomposition is basically a modified form of [[Gaussian elimination]]. We transform the matrix ''A'' into an upper triangular matrix ''U'' by eliminating the entries below the main diagonal. The Doolittle algorithm does the elimination column-by-column, starting from the left, by multiplying ''A'' to the left with atomic lower triangular matrices. It results in a ''unit lower triangular'' matrix and an upper triangular matrix. The Crout algorithm is slightly different and constructs a lower triangular matrix and a ''unit upper triangular'' matrix.\n\nComputing an LU decomposition using either of these algorithms requires 2''n''<sup>3</sup>/3 floating-point operations, ignoring lower-order terms. Partial [[Pivot element|pivoting]] adds only a quadratic term; this is not the case for full pivoting.<ref>{{harvtxt|Golub|Van Loan|1996}}, p. 112, 119.</ref>\n\n=== Closed formula ===\n\nWhen an LDU factorization exists and is unique, there is a closed (explicit) formula for the elements of ''L'', ''D'', and ''U'' in terms of ratios of determinants of certain submatrices of the original matrix ''A''.<ref>{{harvtxt|Householder|1975}}</ref> In particular, <math display=\"inline\">D_1 = A_{1,1}</math>, and for <math display=\"inline\">i = 2, \\ldots, n</math>, <math display=\"inline\">D_i</math> is the ratio of the <math display=\"inline\">i</math>-th principal submatrix to the <math display=\"inline\">(i - 1)</math>-th principal submatrix. Computation of the determinants is [[computational complexity|computationally expensive]], so this explicit formula is not used in practice.\n\n=== Doolittle algorithm ===\nGiven an ''N'' × ''N'' matrix\n\n:<math>A = (a_{i,j})_{1 \\leq i,j \\leq N},</math>\n\nwe define\n\n:<math> A^{(0)} := A.</math>\n\nWe eliminate the matrix elements below the main diagonal in the ''n''-th column of ''A''<sup>(''n''&nbsp;−&nbsp;1)</sup> by adding to the ''i''-th row of this matrix the ''n''-th row multiplied by\n\n:<math>-l_{i,n} := -\\frac{a_{i,n}^{(n-1)}}{a_{n,n}^{(n-1)}}</math>\n\nfor <math display=\"inline\">i = n+1, \\ldots, N</math>. This can be done by multiplying ''A''<sup>(''n''&nbsp;−&nbsp;1)</sup> to the left with the lower triangular matrix\n\n:<math>L_n =\n \\begin{pmatrix}\n  1      & 0      &            & \\dots &        & 0      \\\\\n  0      & \\ddots & \\ddots     &       &        &        \\\\\n         &        & 1          &       &        &        \\\\\n  \\vdots &        & -l_{n+1,n} &       &        & \\vdots \\\\\n         &        & \\vdots     &       & \\ddots & 0      \\\\\n  0      &        & -l_{N,n}   &       &        & 1\n \\end{pmatrix}.\n</math>\n\nWe set\n\n:<math> A^{(n)} := L_n A^{(n-1)}.</math>\n\nAfter ''N''&nbsp;−&nbsp;1 steps, we eliminated all the matrix elements below the main diagonal, so we obtain an upper triangular matrix ''A''<sup>(''N''&nbsp;−&nbsp;1)</sup>. We find the decomposition\n\n:<math>\n  A = L_1^{-1} L_1 A^{(0)}\n    = L_1^{-1} A^{(1)}\n    = L_1^{-1} L_2^{-1} L_2 A^{(1)}\n    = L_1^{-1} L_2^{-1} A^{(2)}\n    = \\dotsb\n    = L_1^{-1} \\ldots L_{N-1}^{-1} A^{(N-1)}.\n</math>\n\nDenote the upper triangular matrix ''A''<sup>(''N''&nbsp;−&nbsp;1)</sup> by ''U'', and <math display=\"inline\">L = L_1^{-1} \\ldots L_{N-1}^{-1}</math>. Because the inverse of a lower triangular matrix ''L''<sub>''n''</sub> is again a lower triangular matrix, and the multiplication of two lower triangular matrices is again a lower triangular matrix, it follows that ''L'' is a lower triangular matrix. Moreover, it can be seen that\n\n:<math>L =\n  \\begin{pmatrix}\n          1 &    0   &    \\dots  &        &            & 0    \\\\\n    l_{2,1} & \\ddots &    \\ddots &        &            &      \\\\\n            &        &         1 &        &            &      \\\\\n     \\vdots &        & l_{n+1,n} & \\ddots &            &\\vdots\\\\\n            &        &    \\vdots &        &       1    & 0    \\\\\n    l_{N,1} &        & l_{N,n}   &        & l_{N,N-1}  & 1\n  \\end{pmatrix}.\n</math>\n\nWe obtain <math display=\"inline\">A = LU</math>.\n\nIt is clear that in order for this algorithm to work, one needs to have <math display=\"inline\">a_{n,n}^{(n-1)} \\neq 0</math> at each step (see the definition of <math display=\"inline\">l_{i,n}</math>). If this assumption fails at some point, one needs to interchange ''n''-th row with another row below it before continuing. This is why an  LU decomposition in general looks like <math display=\"inline\">P^{-1}A = L U </math>.\n\n=== Crout and LUP algorithms ===\n\nThe LUP decomposition algorithm by Cormen et al. generalizes [[Crout matrix decomposition]]. It can be described as follows.\n\n# If <math display=\"inline\">A</math> has a nonzero entry in its first row, then take a permutation matrix <math display=\"inline\">P_1</math> such that <math display=\"inline\">A P_1</math> has a nonzero entry in its upper left corner. Otherwise, take for <math display=\"inline\">P_1</math> the identity matrix. Let <math display=\"inline\">A_1 = A P_1</math>.\n# Let <math display=\"inline\">A_2</math> be the matrix that one gets from <math display=\"inline\">A_1</math> by deleting both the first row and the first column. Decompose <math display=\"inline\">A_2 = L_2 U_2 P_2</math> recursively. Make <math display=\"inline\">L</math> from <math display=\"inline\">L_2</math> by first adding a zero row above and then adding the first column of <math display=\"inline\">A_1</math> at the left.\n# Make <math display=\"inline\">U_3</math> from <math display=\"inline\">U_2</math> by first adding a zero row above and a zero column at the left and then replacing the upper left entry (which is 0 at this point) by 1. Make <math display=\"inline\">P_3</math> from <math display=\"inline\">P_2</math> in a similar manner and define <math display=\"inline\">A_3 = A_1 / P_3 = A P_1 / P_3</math>. Let <math display=\"inline\">P</math> be the inverse of <math display=\"inline\">P_1 / P_3</math>.\n# At this point, <math display=\"inline\">A_3</math> is the same as <math display=\"inline\">L U_3</math>, except (possibly) at the first row. If the first row of <math display=\"inline\">A</math> is zero, then <math display=\"inline\">A_3 =  L U_3</math>, since both have first row zero, and <math display=\"inline\">A = L U_3 P</math> follows, as desired. Otherwise, <math display=\"inline\">A_3</math> and <math display=\"inline\">L U_3</math> have the same nonzero entry in the upper left corner, and <math display=\"inline\">A_3 = L U_3 U_1</math> for some upper triangular square matrix <math display=\"inline\">U_1</math> with ones on the diagonal (<math display=\"inline\">U_1</math> clears entries of <math display=\"inline\">L U_3</math> and adds entries of <math display=\"inline\">A_3</math> by way of the upper left corner). Now <math display=\"inline\">A = L U_3 U_1 P</math> is a decomposition of the desired form.\n\n=== Randomized algorithm ===\nIt is possible to find a low rank approximation to an LU decomposition using a randomized algorithm. Given an input matrix <math display=\"inline\">A</math> and a desired low rank <math display=\"inline\">k</math>, the randomized LU returns permutation matrices <math display=\"inline\">P, Q</math> and lower/upper trapezoidal matrices <math display=\"inline\">L, U</math> of size <math display=\"inline\">m \\times k </math> and <math display=\"inline\">k \\times n</math> respectively, such that with high probability <math display=\"inline\">\\Vert PAQ-LU \\Vert_2 \\le C\\sigma_{k+1}</math>, where <math display=\"inline\">C</math> is a constant that depends on the parameters of the algorithm and <math display=\"inline\">\\sigma_{k+1}</math> is the <math display=\"inline\">(k+1)</math>th singular value of the input matrix <math display=\"inline\">A</math>.<ref>{{cite journal|last1=Shabat|first1=Gil|last2=Shmueli|first2=Yaniv|last3=Aizenbud|first3=Yariv|last4=Averbuch|first4=Amir|title=Randomized LU Decomposition|journal=Applied and Computational Harmonic Analysis|volume=44|issue=2|pages=246–272|date=2016|doi=10.1016/j.acha.2016.04.006|arxiv=1310.7202}}</ref>\n\n=== Theoretical complexity ===\nIf two matrices of order ''n'' can be multiplied in time ''M''(''n''), where ''M''(''n'') ≥ ''n''<sup>''a''</sup> for some ''a'' > 2, then an LU decomposition can be computed in time O(''M''(''n'')).<ref>{{harvtxt|Bunch|Hopcroft|1974}}</ref> This means, for example, that an O(''n''<sup>2.376</sup>) algorithm exists based on the [[Coppersmith–Winograd algorithm]].\n\n=== Sparse-matrix decomposition ===\nSpecial algorithms have been developed for factorizing large [[sparse matrix|sparse matrices]]. These algorithms attempt to find sparse factors ''L'' and ''U''. Ideally, the cost of computation is determined by the number of nonzero entries, rather than by the size of the matrix.\n\nThese algorithms use the freedom to exchange rows and columns to minimize fill-in (entries that change from an initial zero to a non-zero value during the execution of an algorithm).\n\nGeneral treatment of orderings that minimize fill-in can be addressed using [[graph theory]].\n\n== Applications ==\n\n=== Solving linear equations ===\nGiven a system of linear equations in matrix form\n\n:<math>Ax = b,</math>\n\nwe want to solve the equation for ''x'', given ''A'' and ''b''. Suppose we have already obtained the LUP decomposition of ''A'' such that <math display=\"inline\">PA = LU</math>, so <math display=\"inline\">LUx = Pb</math>.\n\nIn this case the solution is done in two logical steps:\n# First, we solve the equation <math display=\"inline\">Ly = Pb</math> for ''y''.\n# Second, we solve the equation <math display=\"inline\">Ux = y</math> for ''x''.\n\nNote that in both cases we are dealing with triangular matrices (''L'' and ''U''), which can be solved directly by [[Triangular matrix#Forward and back substitution|forward and backward substitution]] without using the [[Gaussian elimination]] process (however we do need this process or equivalent to compute the ''LU'' decomposition itself).\n\nThe above procedure can be repeatedly applied to solve the equation multiple times for different ''b''. In this case it is faster (and more convenient) to do an LU decomposition of the matrix ''A'' once and then solve the triangular matrices for the different ''b'', rather than using Gaussian elimination each time. The matrices ''L'' and ''U'' could be thought to have \"encoded\" the Gaussian elimination process.\n\nThe cost of solving a system of linear equations is approximately <math display=\"inline\">\\frac{2}{3} n^3</math> floating-point operations if the matrix <math display=\"inline\">A</math> has size <math display=\"inline\">n</math>. This makes it twice as fast as algorithms based on [[QR decomposition]], which costs about <math display=\"inline\">\\frac{4}{3} n^3</math> floating-point operations when [[Householder reflection]]s are used. For this reason, LU decomposition is usually preferred.<ref>{{harvtxt|Trefethen|Bau|1997}}, p. 152.</ref>\n\n=== Inverting a matrix ===\nWhen solving systems of equations, ''b'' is usually treated as a vector with a length equal to the height of matrix ''A''. In matrix inversion however, instead of vector ''b'', we have matrix ''B'', where ''B'' is an ''n''-by-''p'' matrix, so that we are trying to find a matrix ''X'' (also a ''n''-by-''p'' matrix):\n\n:<math>AX = LUX = B.</math>\n\nWe can use the same algorithm presented earlier to solve for each column of matrix ''X''. Now suppose that ''B'' is the identity matrix of size ''n''. It would follow that the result ''X'' must be the inverse of ''A''.<ref>{{harvtxt|Golub|Van Loan|1996}}, p. 121</ref> An implementation of this methodology in [[the C programming language]] can be found [http://chandraacads.blogspot.jp/2015/12/c-program-for-matrix-inversion.html here].\n\n=== Computing the determinant ===\nGiven the LUP decomposition <math display=\"inline\">A = P^{-1} LU</math> of a square matrix ''A'', the determinant of ''A'' can be computed straightforwardly as\n\n:<math>\\det(A) = \\det(P^{-1}) \\det(L) \\det(U) = (-1)^S \\left( \\prod_{i=1}^n l_{ii} \\right) \\left( \\prod_{i=1}^n u_{ii} \\right) .</math>\n\nThe second equation follows from the fact that the determinant of a triangular matrix is simply the product of its diagonal entries, and that the determinant of a permutation matrix is equal to (−1)<sup>''S''</sup> where ''S'' is the number of row exchanges in the decomposition.\n\nIn the case of LU decomposition with full pivoting, <math display=\"inline\">\\det(A)</math> also equals the right-hand side of the above equation, if we let ''S'' be the total number of row and column exchanges.\n\nThe same method readily applies to LU decomposition by setting ''P'' equal to the identity matrix.\n\n=== C code examples ===\n<source lang=\"cpp\">\n/* INPUT: A - array of pointers to rows of a square matrix having dimension N\n *        Tol - small tolerance number to detect failure when the matrix is near degenerate\n * OUTPUT: Matrix A is changed, it contains both matrices L-E and U as A=(L-E)+U such that P*A=L*U.\n *        The permutation matrix is not stored as a matrix, but in an integer vector P of size N+1 \n *        containing column indexes where the permutation matrix has \"1\". The last element P[N]=S+N, \n *        where S is the number of row exchanges needed for determinant computation, det(P)=(-1)^S    \n */\nint LUPDecompose(double **A, int N, double Tol, int *P) {\n\n    int i, j, k, imax; \n    double maxA, *ptr, absA;\n\n    for (i = 0; i <= N; i++)\n        P[i] = i; //Unit permutation matrix, P[N] initialized with N\n\n    for (i = 0; i < N; i++) {\n        maxA = 0.0;\n        imax = i;\n\n        for (k = i; k < N; k++)\n            if ((absA = fabs(A[k][i])) > maxA) { \n                maxA = absA;\n                imax = k;\n            }\n\n        if (maxA < Tol) return 0; //failure, matrix is degenerate\n\n        if (imax != i) {\n            //pivoting P\n            j = P[i];\n            P[i] = P[imax];\n            P[imax] = j;\n\n            //pivoting rows of A\n            ptr = A[i];\n            A[i] = A[imax];\n            A[imax] = ptr;\n\n            //counting pivots starting from N (for determinant)\n            P[N]++;\n        }\n\n        for (j = i + 1; j < N; j++) {\n            A[j][i] /= A[i][i];\n\n            for (k = i + 1; k < N; k++)\n                A[j][k] -= A[j][i] * A[i][k];\n        }\n    }\n\n    return 1;  //decomposition done \n}\n\n/* INPUT: A,P filled in LUPDecompose; b - rhs vector; N - dimension\n * OUTPUT: x - solution vector of A*x=b\n */\nvoid LUPSolve(double **A, int *P, double *b, int N, double *x) {\n\n    for (int i = 0; i < N; i++) {\n        x[i] = b[P[i]];\n\n        for (int k = 0; k < i; k++)\n            x[i] -= A[i][k] * x[k];\n    }\n\n    for (int i = N - 1; i >= 0; i--) {\n        for (int k = i + 1; k < N; k++)\n            x[i] -= A[i][k] * x[k];\n\n        x[i] = x[i] / A[i][i];\n    }\n}\n\n/* INPUT: A,P filled in LUPDecompose; N - dimension\n * OUTPUT: IA is the inverse of the initial matrix\n */\nvoid LUPInvert(double **A, int *P, int N, double **IA) {\n  \n    for (int j = 0; j < N; j++) {\n        for (int i = 0; i < N; i++) {\n            if (P[i] == j) \n                IA[i][j] = 1.0;\n            else\n                IA[i][j] = 0.0;\n\n            for (int k = 0; k < i; k++)\n                IA[i][j] -= A[i][k] * IA[k][j];\n        }\n\n        for (int i = N - 1; i >= 0; i--) {\n            for (int k = i + 1; k < N; k++)\n                IA[i][j] -= A[i][k] * IA[k][j];\n\n            IA[i][j] = IA[i][j] / A[i][i];\n        }\n    }\n}\n\n/* INPUT: A,P filled in LUPDecompose; N - dimension. \n * OUTPUT: Function returns the determinant of the initial matrix\n */\ndouble LUPDeterminant(double **A, int *P, int N) {\n\n    double det = A[0][0];\n\n    for (int i = 1; i < N; i++)\n        det *= A[i][i];\n\n    if ((P[N] - N) % 2 == 0)\n        return det; \n    else\n        return -det;\n}\n</source>\n\n=== C# code examples ===\n<source lang=\"c#\">\npublic class SystemOfLinearEquations\n    {\n        public double[] SolveUsingLU(double[,] matrix, double[] rightPart, int n)\n        {\n            // decomposition of matrix\n            double[,] lu = new double[n, n];\n            double sum = 0;\n            for (int i = 0; i < n; i++)\n            {\n                for (int j = i; j < n; j++)\n                {\n                    sum = 0;\n                    for (int k = 0; k < i; k++)\n                        sum += lu[i, k] * lu[k, j];\n                    lu[i, j] = matrix[i, j] - sum;\n                }\n                for (int j = i + 1; j < n; j++)\n                {\n                    sum = 0;\n                    for (int k = 0; k < i; k++)\n                        sum += lu[j, k] * lu[k, i];\n                    lu[j, i] = (1 / lu[i, i]) * (matrix[j, i] - sum);\n                }\n            }\n            \n            // lu = L+U-I\n            // find solution of Ly = b\n            double[] y = new double[n];\n            for (int i = 0; i < n; i++)\n            {\n                sum = 0;\n                for (int k = 0; k < i; k++)\n                    sum += lu[i, k] * y[k];\n                y[i] = rightPart[i] - sum;\n            }\n            // find solution of Ux = y\n            double[] x = new double[n];\n            for (int i = n - 1; i >= 0; i--)\n            {\n                sum = 0;\n                for (int k = i + 1; k < n; k++)\n                    sum += lu[i, k] * x[k];\n                x[i] = (1 / lu[i, i]) * (y[i] - sum);\n            }\n            return x;\n        }\n}\n</source>\n\n=== MATLAB code examples ===\n<source lang=\"matlab\">\nfunction x = SolveLinearSystem(A, b)\n    n = length(b);\n    x = zeros(n, 1);\n    y = zeros(n, 1);\n    % decomposition of matrix, Doolittle’s Method\n    for i = 1:1:n\n        for j = 1:1:(i - 1)\n            alpha = A(i,j);\n            for k = 1:1:(j - 1)\n                alpha = alpha - A(i,k)*A(k,j);\n            end\n            A(i,j) = alpha/A(j,j);\n        end\n        for j = i:1:n\n            alpha = A(i,j);\n            for k = 1:1:(i - 1)\n                alpha = alpha - A(i,k)*A(k,j);\n            end\n            A(i,j) = alpha;\n        end\n    end\n    %A = L+U-I\n    % find solution of Ly = b\n    for i = 1:1:n\n        alpha = 0;\n        for k = 1:1:i\n            alpha = alpha + A(i,k)*y(k);\n        end\n        y(i) = b(i) - alpha;\n    end\n    % find solution of Ux = y\n    for i = n:(-1):1\n        alpha = 0;\n        for k = (i + 1):1:n\n            alpha = alpha + A(i,k)*x(k);\n        end\n        x(i) = (y(i) - alpha)/A(i, i);\n    end    \nend\n</source>\n\n== See also ==\n*[[Block LU decomposition]]\n*[[Bruhat decomposition]]\n*[[Cholesky decomposition]]\n*[[Incomplete LU factorization]]\n*[[LU Reduction]]\n*[[Matrix decomposition]]\n*[[QR decomposition]]\n\n== Notes ==\n{{Reflist|30em}}\n\n== References ==\n\n* {{Citation | last1=Bunch | first1=James R. | last2=Hopcroft | first2=John | author2-link=John Hopcroft | title=Triangular factorization and inversion by fast matrix multiplication | year=1974 | journal=[[Mathematics of Computation]] | issn=0025-5718 | volume=28 | issue=125 | pages=231–236 | doi=10.2307/2005828| jstor=2005828 }}.\n* {{Citation | last1=Cormen | first1=Thomas H. | author1-link=Thomas H. Cormen | last2=Leiserson | first2=Charles E. | author2-link=Charles E. Leiserson | last3=Rivest | first3=Ronald L. | author3-link=Ronald L. Rivest | last4=Stein | first4=Clifford | author4-link=Clifford Stein | title=Introduction to Algorithms | publisher=MIT Press and McGraw-Hill | isbn=978-0-262-03293-3 | year=2001 | title-link=Introduction to Algorithms }}.\n* {{citation | first1=Gene H. | last1=Golub | author1-link=Gene H. Golub | first2=Charles F. | last2=Van Loan | author2-link=Charles F. Van Loan | year=1996 | title=Matrix Computations | edition=3rd | publisher=Johns Hopkins | place=Baltimore | isbn=978-0-8018-5414-9}}.\n* {{citation | first1=Roger A. | last1=Horn | first2=Charles R. | last2=Johnson | year=1985 | title=Matrix Analysis | publisher=Cambridge University Press | isbn=978-0-521-38632-6 }}. See Section 3.5. ''N''&nbsp;−&nbsp;1\n* {{Citation | last1=Householder | first1=Alston S. | title=The Theory of Matrices in Numerical Analysis | publisher=[[Dover Publications]] | location=New York |mr=0378371 | year=1975}}.\n* {{citation | first1=Pavel | last1=Okunev | first2=Charles R. | last2=Johnson | year=1997| title=Necessary And Sufficient Conditions For Existence of the LU Factorization of an Arbitrary Matrix | arxiv=math.NA/0506382 }}.\n* {{Citation | first1=David | last1=Poole | year=2006 | title=Linear Algebra: A Modern Introduction | edition=2nd | publisher=Thomson Brooks/Cole | place=Canada | isbn=978-0-534-99845-5}}.\n* {{Citation |last1=Press|first1=WH|last2=Teukolsky|first2=SA|last3=Vetterling|first3=WT|last4=Flannery|first4=BP|year=2007|title=Numerical Recipes: The Art of Scientific Computing|edition=3rd|publisher=Cambridge University Press| location=New York|isbn=978-0-521-88068-8|chapter=Section 2.3|chapter-url=http://apps.nrbook.com/empanel/index.html?pg=48}}.\n* {{Citation | last1=Trefethen | first1=Lloyd N. | author1-link=Lloyd Nicholas Trefethen | last2=Bau | first2=David | title=Numerical linear algebra | publisher=Society for Industrial and Applied Mathematics | location=Philadelphia | isbn=978-0-89871-361-9 | year=1997}}.\n\n== External links ==\n'''References'''\n* [http://mathworld.wolfram.com/LUDecomposition.html LU decomposition] on ''MathWorld''.\n* [http://www.math-linux.com/spip.php?article51 LU decomposition] on ''Math-Linux''.\n* [http://numericalmethods.eng.usf.edu/topics/lu_decomposition.html LU decomposition] at ''Holistic Numerical Methods Institute''\n* [http://www.mathworks.com/help/techdoc/ref/lu.html LU matrix factorization]. MATLAB reference.\n\n'''Computer code'''\n* [http://www.netlib.org/lapack/ LAPACK] is a collection of FORTRAN subroutines for solving dense linear algebra problems\n* [http://www.alglib.net/ ALGLIB] includes a partial port of the LAPACK to C++, C#, Delphi, etc.\n* [http://www.johnloomis.org/ece538/notes/Matrix/ludcmp.html C++ code], Prof. J. Loomis, [[University of Dayton]]\n* [http://www.mymathlib.com/matrices/linearsystems/doolittle.html C code], Mathematics Source Library\n* [http://docs.codehaus.org/display/XTENLANG/LU LU in X10]\n* [https://www.researchgate.net/publication/301478425_Lu_decomposition_to_solve_n_linear_equations_in_n_unknowns?ev=prf_pub ]   Lu in C++ Anil Pedgaonkar\n* [https://sites.google.com/site/gilgils/downloads Randomized LU MATLAB Code]\n\n'''Online resources'''\n* [http://sole.ooz.ie/ WebApp descriptively solving systems of linear equations with LU Decomposition]\n* [http://www.bluebit.gr/matrix-calculator/ Matrix Calculator], bluebit.gr\n* [http://www.uni-bonn.de/~manfear/matrix_lu.php LU Decomposition Tool], uni-bonn.de\n* [http://demonstrations.wolfram.com/LUDecomposition/ LU Decomposition] by [[Ed Pegg, Jr.]], [[The Wolfram Demonstrations Project]], 2007.\n\n{{Numerical linear algebra}}\n\n[[Category:Matrix decompositions]]\n[[Category:Numerical linear algebra]]\n[[Category:Articles with example C code]]\n[[Category:Articles with example C Sharp code]]\n\n[[de:Gaußsches Eliminationsverfahren#LR-Zerlegung]]"
    },
    {
      "title": "LU reduction",
      "url": "https://en.wikipedia.org/wiki/LU_reduction",
      "text": "'''LU reduction''' is an [[algorithm]] related to [[LU decomposition]]. This term is usually used in the context of [[super computing]] and highly [[parallel computing]]. In this context it is used as a [[benchmarking]] algorithm, i.e. to provide a comparative measurement of speed for different computers. LU reduction is a special parallelized version of an LU decomposition algorithm, an example can be found in (Guitart 2001). The parallelized version usually distributes the work for a matrix row to a single processor and synchronizes the result with the whole matrix (Escribano 2000).\n\n== Sources ==\n* J. Oliver, J. Guitart, E. Ayguadé, N. Navarro and J. Torres. Strategies for Efficient Exploitation of Loop-level Parallelism in Java. Concurrency and Computation: Practice and Experience(Java Grande 2000 Special Issue), Vol.13 (8-9), pp. 663–680. ISSN 1532-0634, July 2001, [https://web.archive.org/web/20071021195955/http://www.bsc.es/media/396.pdf], last retrieved on Sept. 14 2007\n* J. Guitart, X. Martorell, J. Torres, and E. Ayguadé, Improving Java Multithreading Facilities: the Java Nanos Environment, Research Report UPC-DAC-2001-8, Computer Architecture Department, Technical University of Catalonia, March 2001, [http://elio.ac.upc.edu/~dacsecre/reports/2001/8/contents.ps.Z]{{dead link|date=December 2017 |bot=InternetArchiveBot |fix-attempted=yes }}.\n* Arturo González-Escribano, Arjan J. C. van Gemund, Valentín Cardeñoso-Payo et al., Measuring the Performance Impact of SP-Restricted Programming in Shared-Memory Machines, In Vector and Parallel Processing — VECPAR 2000, Springer Verlag, pp. 128–141, {{ISBN|978-3-540-41999-0}}, 2000, [https://web.archive.org/web/20060621203923/http://www.infor.uva.es/~arturo/Ftp/p-vecpar2000lncs.ps.gz]\n\n{{algorithm-stub}}\n{{mathapplied-stub}}\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]\n[[Category:Supercomputers]]"
    },
    {
      "title": "Matrix-free methods",
      "url": "https://en.wikipedia.org/wiki/Matrix-free_methods",
      "text": "In [[computational mathematics]], a '''matrix-free method''' is an algorithm for solving a [[linear system of equations]] or an [[eigenvalue]] problem that does not store the coefficient [[matrix (mathematics)|matrix]] explicitly, but accesses the matrix by evaluating matrix-vector products.<ref>{{Citation | last1=Langville | first1=Amy N. | last2=Meyer | first2=Carl D. | title=Google's PageRank and beyond: the science of search engine rankings | publisher=[[Princeton University Press]] | isbn=978-0-691-12202-1 | year=2006 | page=40}}</ref> Such methods can be preferable when the matrix is so big that storing and manipulating it would cost a lot of memory and computer time, even with the use of methods for [[sparse matrix|sparse matrices]]. Many [[iterative method]]s allow for a matrix-free implementation, including:\n* the [[power method]],\n* the [[Lanczos algorithm]],<ref>{{Citation |doi=10.1016/0024-3795(93)90235-G |title=Solving linear equations over GF(2): Block Lanczos algorithm |year=1993 |last1=Coppersmith |first1=Don |journal=Linear Algebra and its Applications |volume=192 |pages=33–60}}</ref>\n* Locally Optimal Block Preconditioned Conjugate Gradient Method ([[LOBPCG]]),<ref>{{Cite journal| doi = 10.1137/S1064827500366124| title = Toward the Optimal Preconditioned Eigensolver: Locally Optimal Block Preconditioned Conjugate Gradient Method| journal = [[SIAM Journal on Scientific Computing]]| volume = 23| issue = 2| pages = 517–541| year = 2001| last1 = Knyazev | first1 = Andrew V. | citeseerx = 10.1.1.34.2862}}</ref>  \n* Wiedemann's coordinate recurrence algorithm,<ref>{{Citation |url=http://www.enseignement.polytechnique.fr/profs/informatique/Francois.Morain/Master1/Crypto/projects/Wiedemann86.pdf|doi=10.1109/TIT.1986.1057137 |title=Solving sparse linear equations over finite fields |year=1986 |last1=Wiedemann |first1=D. |journal=IEEE Transactions on Information Theory |volume=32 |pages=54–62}}</ref> and\n* the [[conjugate gradient method]].<ref>{{Citation | doi=10.1007/3-540-38424-3_8 | chapter=Solving Large Sparse Linear Systems Over Finite Fields | title=Advances in Cryptology-CRYPT0' 90 | series=Lecture Notes in Computer Science | year=1991 | last1=Lamacchia | first1=B. A. | last2=Odlyzko | first2=A. M. | isbn=978-3-540-54508-8 | volume=537 | pages=109}}</ref>\n\nDistributed solutions have also been explored using coarse-grain parallel software systems to achieve homogeneous solutions of linear systems.<ref>{{Citation | last1=Kaltofen | first1=E. | last2=Lobo | first2=A. | title=Distributed Matrix-Free Solution of Large Sparse Linear Systems over Finite Fields | year=1996 | periodical=Algorithmica | volume=24 | pages=311–348 |doi=10.1007/PL00008266 | issue=3–4| citeseerx=10.1.1.17.7470 }}</ref>\n\nIt is generally used in solving non-linear equations like Euler's equations in [[Computational Fluid Dynamics]]. Solving these equations requires the calculation of the [[Jacobian matrix and determinant|jacobian]] which is costly in terms of CPU time and storage. To avoid this expense, matrix free methods are employed. In order to remove the need to calculate the jacobian, the jacobian vector product is formed instead, which is in fact a vector itself. Manipulating and calculating this vector is easier than working with a large matrix or linear system.\n\n== References ==\n{{Reflist}}\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]\n\n\n{{mathapplied-stub}}"
    },
    {
      "title": "Method of Four Russians",
      "url": "https://en.wikipedia.org/wiki/Method_of_Four_Russians",
      "text": "In [[computer science]], the '''Method of Four Russians''' is a technique for speeding up [[algorithm]]s involving [[Boolean matrix|Boolean matrices]], or more generally algorithms involving matrices in which each cell may take on only a bounded number of possible values.\n\n== Idea ==\n\nThe main idea of the method is to partition the matrix into small square blocks of size {{math|''t'' &times; ''t''}} for some parameter {{mvar|t}}, and to use a [[lookup table]] to perform the algorithm quickly within each block. The index into the lookup table encodes the values of the matrix cells on the upper left of the block boundary prior to some operation of the algorithm, and the result of the lookup table encodes the values of the boundary cells on the lower right of the block after the operation. Thus, the overall algorithm may be performed by operating on only {{math|(''n''/''t'')<sup>2</sup>}} blocks instead of on {{math|''n''<sup>2</sup>}} matrix cells, where {{mvar|n}} is the side length of the matrix. In order to keep the size of the lookup tables (and the time needed to initialize them) sufficiently small, {{mvar|t}} is typically chosen to be {{math|''O''(log ''n'')}}.\n\n== Applications ==\n\nAlgorithms to which the Method of Four Russians may be applied include:\n* computing the [[transitive closure]] of a graph, \n* Boolean [[matrix multiplication]], \n* [[edit distance]] calculation,\n* [[sequence alignment]],\n* index calculation for [[binary jumbled pattern matching]].\nIn each of these cases it speeds up the algorithm by one or two [[logarithm]]ic factors.\n\nThe Method of Four Russians matrix inversion algorithm published by Bard is implemented in M4RI library for fast arithmetic with dense matrices over ''F''<sub>2</sub>. M4RI is used by [[SageMath]] and the PolyBoRi library.<ref>[https://malb.bitbucket.io/m4ri/ M4RI - Main Page]</ref>\n\n== History ==\n\nThe algorithm was introduced by [[Vladimir Arlazarov|V. L. Arlazarov]], E. A. Dinic, [[Alexander Kronrod|M. A. Kronrod]], and I. A. Faradžev in 1970.{{sfn|Arlazarov|Dinic|Kronrod|Faradzev|1970}} The origin of the name is unknown; {{Harvtxt|Aho|Hopcroft|Ullman|1974}} explain:\n:The second method, often called the \"Four Russians'\" algorithm, after the cardinality and nationality of its inventors, is somewhat more \"practical\" than the algorithm in Theorem 6.9.{{sfn|Aho|Hopcraft|Ullman|1974|p=243}}\nAll four authors worked in [[Moscow|Moscow, Russia]] at the time.<ref>[http://www.mathnet.ru/php/archive.phtml?wshow=paper&jrnid=dan&paperid=35675&option_lang=eng Author affiliations] on MathNet.ru.</ref>\n\n==Notes==\n{{Reflist}}\n\n==References==\n*{{Citation |first=V. |last=Arlazarov |authorlink=Vladimir Arlazarov |first2=E. |last2=Dinic |first3=M. |last3=Kronrod |first4=I. |last4=Faradžev |year=1970 |title=On economical construction of the transitive closure of a directed graph |journal=Dokl. Akad. Nauk SSSR |volume=194 |issue=11 }}. Original title: \"Об экономном построении транзитивного замыкания ориентированного графа\", published in ''[[Proceedings of the USSR Academy of Sciences|Доклады Академии Наук СССР]]'' '''134''' (3), 1970.\n*{{Citation |first=Alfred V. |last=Aho |first2=John E. |last2=Hopcroft |first3=Jeffrey D. |last3=Ullman |year=1974 |title=The design and analysis of computer algorithms |publisher=Addison-Wesley}}\n*{{Citation |last=Bard |first=Gregory V. |year=2009 |title=Algebraic Cryptanalysis |publisher=Springer |isbn=978-0-387-88756-2}}\n\n[[Category:Numerical linear algebra]]\n\n\n{{mathapplied-stub}}"
    },
    {
      "title": "Minimum degree algorithm",
      "url": "https://en.wikipedia.org/wiki/Minimum_degree_algorithm",
      "text": "In [[numerical analysis]] the '''minimum degree algorithm''' is an [[algorithm]] used to permute the rows and columns of a [[symmetric matrix|symmetric]] [[sparse matrix]] before applying the [[Cholesky decomposition]], to reduce the number of non-zeros in the Cholesky factor.\nThis results in reduced storage requirements and means that the Cholesky factor can be applied with fewer arithmetic operations. (Sometimes it may also pertain to an incomplete Cholesky factor used as a preconditioner, for example in the preconditioned conjugate gradient algorithm.)\n\nMinimum degree algorithms are often used in the [[finite element method]] where the reordering of nodes can be carried out depending only on the topology of the mesh, rather than the coefficients in the partial differential equation, resulting in efficiency savings when the same mesh is used for a variety of coefficient values.\n\nGiven a linear system\n:<math> \\mathbf{A}\\mathbf{x} = \\mathbf{b}</math>\nwhere '''A''' is an <math>n \\times n</math> real symmetric sparse square matrix the Cholesky factor '''L''' will typically suffer 'fill in', that is have more non-zeros than the upper triangle of '''A'''. We seek a [[permutation matrix]] '''P''', so that the matrix\n<math>\\mathbf{P}^T\\mathbf{A}\\mathbf{P}</math>, which is also symmetric, has the least possible fill in its Cholesky factor. We solve the  reordered system\n:<math> \\left(\\mathbf{P}^T\\mathbf{A}\\mathbf{P}\\right) \\left(\\mathbf{P}^T\\mathbf{x}\\right) = \\mathbf{P}^T\\mathbf{b}.</math>\nThe problem of finding the best ordering is an [[NP-complete]] problem and is thus intractable, so heuristic methods are used instead.  The minimum degree algorithm is derived from a method first proposed by [[Harry Markowitz|Markowitz]] in 1959 for non-symmetric [[linear programming]] problems, which is loosely described as follows. At each step in [[Gaussian elimination]] row and column permutations are performed so as to minimize the number of off diagonal non-zeros in the pivot row and column. A symmetric version\nof Markowitz method was described by Tinney and Walker in 1967 and Rose later derived a graph theoretic version of the algorithm where the factorization is only simulated, and this was named the minimum degree algorithm. The graph referred to is the graph with ''n'' vertices, with vertices ''i'' and ''j'' connected by an edge when <math>a_{ij} \\ne 0</math>, and the ''degree'' is the degree of the vertices. A crucial aspect of such algorithms is a tie breaking strategy when there is a  choice of renumbering resulting in the same degree.\n\nA version of the minimum degree algorithm was implemented in the [[MATLAB]] function '''symmmd''' (where MMD stands for multiple minimum degree), but has now been superseded by a symmetric approximate multiple minimum degree function '''symamd''', which is faster.  This is confirmed by theoretical analysis, which shows that for graphs on ''n'' vertices and ''m'' edges, MMD has a tight [[Big O notation|upper bound]] of <math>O(n^2m)</math> on its run time, whereas for AMD a tight bound of <math>O(nm)</math> holds.\n\n==References==\n*{{cite journal |authorlink=Harry Markowitz |first=H. M. |last=Markowitz |title=The elimination form of the inverse and its application to linear programming |journal=[[Management Science: A Journal of the Institute for Operations Research and the Management Sciences|Management Science]] |volume=3 |issue=3 |pages=255–269 |year=1957 |jstor=2627454 |doi=10.1287/mnsc.3.3.255|url=http://www.dtic.mil/get-tr-doc/pdf?AD=AD0604711 }}\n*{{cite journal |first=Alan |last=George |first2=Joseph |last2=Liu |title=The evolution of the Minimum Degree Ordering Algorithm |journal=[[SIAM Review]] |volume=31 |issue=1 |pages=1–19 |year=1989 |jstor=2030845 |doi=10.1137/1031001}}\n*{{cite journal |first=W. F. |last=Tinney |first2=J. W. |last2=Walker |title=Direct solution of sparse network equations by optimally ordered triangular factorization |journal=[[Proceedings of the IEEE|Proc. IEEE]] |volume=55 |issue=11 |pages=1801–1809 |year=1967 |doi=10.1109/PROC.1967.6011 }}\n*{{cite book |first=D. J. |last=Rose |chapter=A graph-theoretic study of the numerical solution of sparse positive definite systems of linear equations |title=Graph Theory and Computing |editor-first=R. C. |editor-last=Read |location=New York |publisher=Academic Press |year=1972 |pages=183–217 |isbn=0-12-583850-6 }}\n*{{citation|first=P.|last=Heggernes | author1-link = Pinar Heggernes |first2=S. C. |last2=Eisenstat |first3=G. |last3= Kumfert| first4=A. |last4=Pothen |date=December 2001 |title=The Computational Complexity of the Minimum Degree Algorithm |type= Technical report |url=https://www.cs.purdue.edu/homes/apothen/Papers/md-conf.pdf |publisher=Institute for Computer Applications in Science and Engineering}}\n\n{{Numerical linear algebra}}\n\n{{DEFAULTSORT:Minimum Degree Algorithm}}\n[[Category:Numerical linear algebra]]\n[[Category:Matrix theory]]"
    },
    {
      "title": "Modified Richardson iteration",
      "url": "https://en.wikipedia.org/wiki/Modified_Richardson_iteration",
      "text": "'''Modified Richardson iteration''' is an [[iterative method]] for solving a [[system of linear equations]]. Richardson iteration was proposed by [[Lewis Richardson]] in his work dated 1910. It is similar to the [[Jacobi method|Jacobi]] and [[Gauss–Seidel method]].\n\nWe seek the solution to a set of linear equations, expressed in matrix terms as\n\n:<math> A x = b.\\, </math>\n\nThe Richardson iteration is\n\n:<math> \nx^{(k+1)}  = x^{(k)} + \\omega \\left( b - A x^{(k)} \\right),\n</math>\n\nwhere <math>\\omega</math> is a scalar parameter that has to be chosen such that the sequence <math>x^{(k)}</math> converges.\n\nIt is easy to see that the method has the correct [[Fixed point (mathematics)|fixed points]], because if it converges, then <math>x^{(k+1)} \\approx x^{(k)}</math> and <math>x^{(k)}</math> has to approximate a solution of <math>A x = b</math>.\n\n== Convergence ==\n\nSubtracting the exact solution <math>x</math>, and introducing the notation for the error <math>e^{(k)} = x^{(k)}-x</math>, we get the equality for the errors\n\n:<math> \ne^{(k+1)}  = e^{(k)} - \\omega A e^{(k)} = (I-\\omega A) e^{(k)}. \n</math>\n\nThus,\n\n:<math> \n\\|e^{(k+1)}\\| = \\|(I-\\omega A) e^{(k)}\\|\\leq  \\|I-\\omega A\\| \\|e^{(k)}\\|, \n</math>\n\nfor any vector norm and the corresponding induced matrix norm. Thus, if <math>\\|I-\\omega A\\|<1</math>, the method converges.\n\nSuppose that <math>A</math> is [[Symmetric positive definite|symmetric positive definite]] and that <math>(\\lambda_j)_j</math> are the [[eigenvalues]] of <math>A</math>. The error converges to <math>0</math> if <math>| 1 - \\omega \\lambda_j |< 1</math> for all eigenvalues <math>\\lambda_j</math>. If, e.g., all eigenvalues are positive, this can be guaranteed if <math>\\omega</math> is chosen such that <math>0 < \\omega < \\omega_\\text{max}\\,, \\ \\omega_\\text{max}:= 2/\\lambda_{\\text{max}}(A)</math>. The optimal choice, minimizing all <math>| 1 - \\omega \\lambda_j |</math>, is <math>\\omega_\\text{opt} := 2/(\\lambda_\\text{min}(A)+\\lambda_\\text{max}(A))</math>, which gives the simplest [[Chebyshev iteration]]. This optimal choice yields a spectral radius of \n:<math>\n\\min_{\\omega\\in (0,\\omega_\\text{max}) } \\rho (I-\\omega A)\n  = \\rho (I-\\omega_\\text{opt} A)\n  = 1 - \\frac{2}{\\kappa(A)+1} \\,,\n</math>\nwhere <math>\\kappa(A)</math> is the [[Condition number|condition number]].\n\nIf there are both positive and negative eigenvalues, the method will diverge for any <math>\\omega</math> if the initial error <math>e^{(0)}</math> has nonzero components in the corresponding [[eigenvectors]].\n\n== Equivalence to [[gradient descent]] ==\nConsider minimizing the function <math>F(x) = \\frac{1}{2}\\|\\tilde{A}x-\\tilde{b}\\|_2^2</math>. Since this is a [[convex function]], a sufficient condition for optimality is that the [[gradient]] is zero (<math>\\nabla F(x) = 0</math>) which gives rise to the equation\n:<math>\\tilde{A}^T\\tilde{A}x = \\tilde{A}^T\\tilde{b}.</math>\n\nDefine <math>A=\\tilde{A}^T\\tilde{A}</math> and <math>b=\\tilde{A}^T\\tilde{b}</math>.\nBecause of the form of ''A'', it is a [[positive semi-definite matrix]], so it has no negative eigenvalues.\n\nA step of gradient descent is\n:<math> x^{(k+1)} = x^{(k)} - t \\nabla F(x^{(k)}) = x^{(k)} - t( Ax^{(k)} - b )</math>\nwhich is equivalent to the Richardson iteration by making <math>t=\\omega</math>.\n\n<references/>\n\n==See also==\n* [[Richardson extrapolation]]\n\n== References ==\n\n*{{cite journal\n | last = Richardson\n | first = L.F.\n | year = 1910\n | journal = [[Philosophical Transactions of the Royal Society A]]\n | title = The approximate arithmetical solution by finite differences of physical problems involving differential equations, with an application to the stresses in a masonry dam\n  | volume = 210\n | pages = 307–357\n | jstor = 90994\n | doi=10.1098/rsta.1911.0009\n}}\n* {{Cite web\n | url = http://eom.springer.de/c/c021900.htm\n | title = Chebyshev iteration method\n | last = [[Vyacheslav Ivanovich Lebedev]]\n | publisher= [[Springer Science+Business Media|Springer]]\n | year=2002\n | accessdate= 2010-05-25\n}} Appeared in Encyclopaedia of Mathematics (2002), Ed. by [[Michiel Hazewinkel]], Kluwer - {{isbn|1-4020-0609-8}}\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "Moore–Penrose inverse",
      "url": "https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse",
      "text": "In [[mathematics]], and in particular [[linear algebra]], a '''pseudoinverse''' {{math|''A''<sup>+</sup>}} of a  [[matrix (mathematics)|matrix]] {{math|''A''}} is a [[generalization]] of the [[inverse matrix]].{{sfn|Ben-Israel|Greville|2003}}  The most widely known type of matrix pseudoinverse is the '''Moore–Penrose inverse''',{{sfn|Ben-Israel|Greville|2003|p=7}}{{sfn|Campbell|Meyer, Jr.|1991|p=10}}{{sfn|Nakamura|1991|p=42}}{{sfn|Rao|Mitra|1971|p=50–51}} which was independently described by [[E. H. Moore]]<ref name=\"Moore1920\">{{cite journal | last=Moore | first=E. H. | authorlink=E. H. Moore | title=On the reciprocal of the general algebraic matrix | journal=[[Bulletin of the American Mathematical Society]] | volume=26 |issue=9| pages=394–95 | year=1920 | url =http://projecteuclid.org/euclid.bams/1183425340 | doi = 10.1090/S0002-9904-1920-03322-7 }}</ref> in 1920, [[Arne Bjerhammar]]<ref name=\"Bjerhammar1951\">{{cite journal | last=Bjerhammar| first=Arne| authorlink=Arne Bjerhammar | title=Application of calculus of matrices to method of least squares; with special references to geodetic calculations| journal=Trans. Roy. Inst. Tech. Stockholm | year=1951 | volume = 49}}</ref> in 1951, and [[Roger Penrose]]<ref name=\"Penrose1955\">{{cite journal | last=Penrose | first=Roger | authorlink=Roger Penrose | title=A generalized inverse for matrices | journal=[[Proceedings of the Cambridge Philosophical Society]] | volume=51 | issue=3 | pages=406–13 | year=1955 | doi=10.1017/S0305004100030401}}</ref> in 1955. Earlier, [[Erik Ivar Fredholm]] had introduced the concept of a pseudoinverse of [[integral operator]]s in 1903. When referring to a matrix, the term pseudoinverse, without further specification, is often used to indicate the Moore–Penrose inverse. The term [[generalized inverse]] is sometimes used as a synonym for pseudoinverse.\n\nA common use of the pseudoinverse is to compute a \"best fit\" ([[Ordinary least squares|least squares]]) solution to a [[system of linear equations]] that lacks a unique solution (see below under [[#Applications|§ Applications]]).\nAnother use is to find the minimum ([[Euclidean norm|Euclidean]]) norm solution to a system of linear equations with multiple solutions. The pseudoinverse facilitates the statement and proof of results in linear algebra.\n\nThe pseudoinverse is defined and unique for all matrices whose entries are [[Real number|real]] or [[Complex number|complex]] numbers. It can be computed using the [[singular value decomposition]].\n\n==Notation==\nIn the following discussion, the following conventions are adopted.\n\n* <math>K</math> will denote one of the [[field (mathematics)|fields]] of real or complex numbers, denoted <math>\\mathbb{R}</math>, <math>\\mathbb{C}</math>, respectively. The vector space of <math>m \\times n</math> matrices over <math>K</math> is denoted by <math>K^{m\\times n}</math>.\n* For <math>A \\in K^{m\\times n}</math>, <math>A^\\mathrm{T}</math> and <math>A^*</math> denote the transpose and Hermitian transpose (also called [[conjugate transpose]]) respectively. If <math>K = \\mathbb{R}</math>, then <math>A^* = A^\\mathrm{T}</math>.\n* For <math>A \\in K^{m\\times n}</math>, <math>\\operatorname{im}(A)</math> denotes the [[column space|range]] (image) of <math>A</math> (the space spanned by the column vectors of <math>A</math>) and <math>\\operatorname{ker}(A)</math> denotes the [[Kernel (linear algebra)|kernel]] (null space) of <math>A</math>.\n* Finally, for any positive integer <math>n</math>, <math>I_n \\in K^{n\\times n}</math> denotes the <math>n \\times n</math> [[identity matrix]].\n\n==Definition==\nFor <math> A \\in K^{m\\times n} </math>, a pseudoinverse of <math> A </math> is defined as a matrix <math> A^+ \\in K^{n\\times m}</math> satisfying all of the following four criteria, known as the Moore–Penrose conditions:<ref name=\"Penrose1955\"/><ref name=\"GvL1996\">{{cite book | last=Golub | first=Gene H. | authorlink=Gene H. Golub |author2=[[Charles F. Van Loan]]  | title=Matrix computations | edition=3rd | publisher=Johns Hopkins | location=Baltimore | year=1996 | isbn=978-0-8018-5414-9 | pages = 257–258}}</ref>\n# <math>A A^+ A = A</math> &emsp; ({{math|''AA''<sup>+</sup>}} need not be the general identity matrix, but it maps all column vectors of {{math|''A''}} to themselves);\n# <math>A^+ A A^+ = A^+</math> &emsp; ({{math|''A''<sup>+</sup>}} is a [[weak inverse]] for the multiplicative [[semigroup]]);\n# <math>(AA^+)^* = AA^+</math> &emsp; ({{math|''AA''<sup>+</sup>}} is [[Hermitian matrix|Hermitian]]);\n# <math>(A^+ A)^* = A^+ A</math> &emsp; ({{math|''A''<sup>+</sup>''A''}} is also Hermitian).\n\n<math> A^+ </math> exists for any matrix <math> A </math>, but when the latter has full [[rank (linear algebra)|rank]], <math> A^+ </math> can be expressed as a simple algebraic formula.\n\nIn particular, when <math>A</math> has linearly independent columns (and thus matrix <math> A^* A </math> is invertible), <math> A^+ </math> can be computed as\n: <math> A^+ = (A^* A)^{-1} A^*.</math>\n\nThis particular pseudoinverse constitutes a ''left inverse'', since, in this case, <math> A^+A = I </math>.\n\nWhen <math>A</math> has linearly independent rows (matrix <math> A A^* </math> is invertible), <math> A^+ </math> can be computed as\n: <math> A^+ = A^* (A A^*)^{-1}.</math>\n\nThis is a ''right inverse'', as <math> A A^+ = I</math>.\n\n==Properties==\n{{hatnote|Proofs for some of these facts may be found on a separate page, [[Proofs involving the Moore–Penrose inverse]].}}\n\n===Existence and uniqueness===\nThe pseudoinverse exists and is unique: for any matrix <math>A</math>, there is precisely one matrix <math>A^+</math>, that satisfies the four properties of the definition.<ref name=\"GvL1996\"/>\n\nA matrix satisfying the first condition of the definition is known as a [[generalized inverse]]. If the matrix also satisfies the second definition, it is called a [[generalized inverse#Types of generalized inverses|generalized ''reflexive'' inverse]]. Generalized inverses always exist but are not in general unique. Uniqueness is a consequence of the last two conditions.\n\n===Basic properties===\n* If <math>A</math> has real entries, then so does <math>A^+</math>.\n* If <math>A</math> is [[invertible matrix|invertible]], its pseudoinverse is its inverse.  That is: <math>A^+ = A^{-1}</math>.<ref name=\"SB2002\">{{Cite book | last1=Stoer | first1=Josef | last2=Bulirsch | first2=Roland | title=Introduction to Numerical Analysis | publisher=[[Springer-Verlag]] | location=Berlin, New York | edition=3rd | isbn=978-0-387-95452-3 | year=2002}}.</ref>{{rp|243}}\n* The pseudoinverse of a [[zero matrix]] is its transpose.\n* The pseudoinverse of the pseudoinverse is the original matrix: <math>\\left(A^+\\right)^+ = A</math>.<ref name=\"SB2002\" />{{rp|245}}\n* Pseudoinversion commutes with transposition, conjugation, and taking the conjugate transpose:<ref name=\"SB2002\"/>{{rp|245}} <!-- reference only mentions the last bit -->\n*: <math>\\left(A^\\mathrm{T}\\right)^+ = \\left(A^+\\right)^\\mathrm{T}</math>, <math>\\left(\\overline{A}\\right)^+ = \\overline{A^+}</math>, <math>\\left(A^*\\right)^+ = \\left(A^+\\right)^*</math>.\n* The pseudoinverse of a scalar multiple of {{math|''A''}} is the reciprocal multiple of {{math|''A''<sup>+</sup>}}:\n*: <math>\\left(\\alpha A\\right)^+ = \\alpha^{-1} A^+</math> for <math>\\alpha \\neq 0</math>.\n\n====Identities====\nThe following identities can be used to cancel certain subexpressions or expand expressions involving pseudoinverses. Proofs for these properties can be found in the [[Proofs involving the Moore–Penrose inverse|proofs subpage]].\n: <math>\\begin{alignat}{3}\n  A^+ ={}& A^+    && A^{+*} && A^* \\\\\n      ={}& A^*    && A^{+*} && A^+, \\\\\n  A   ={}& A^{+*} && A^*    && A       \\\\\n      ={}& A      && A^*    && A^{+*}, \\\\\n  A^* ={}& A^*    && A      && A^+ \\\\\n      ={}& A^+    && A      && A^*.\n\\end{alignat}</math>\n\n===Reduction to Hermitian case===\nThe computation of the pseudoinverse is reducible to its construction in the Hermitian case. This is possible through the equivalences:\n: <math>A^+ = \\left(A^*A\\right)^+ A^*,</math>\n: <math>A^+ = A^* \\left(AA^*\\right)^+,</math>\n\nas <math>A^*A</math> and <math>AA^*</math> are Hermitian.\n\n===Products===\nIf <math>A \\in K^{m\\times n},\\ B \\in K^{n\\times p}</math>, and if\n: <math>A</math> has orthonormal columns (i.e., <math>A^*A = I_n</math>), or\n: <math>B</math> has orthonormal rows (i.e., <math>BB^* = I_n</math>), or\n: <math>A</math> has all columns linearly independent (full column rank) and <math>B</math> has all rows linearly independent (full row rank), or\n: <math>B = A^*</math> (i.e., <math>B</math> is the conjugate transpose of <math>A</math>),\n\nthen\n: <math>(AB)^+ \\equiv B^+ A^+.</math>\n\nThe last property yields the equivalences\n: <math>\\begin{align}\n  \\left(A A^*\\right)^+ &\\equiv A^{+*} A^+, \\\\\n  \\left(A^* A\\right)^+ &\\equiv A^+ A^{+*}.\n\\end{align}</math>\n\n===Projectors===\n<math>P = AA^+\\,\\!</math> and <math>Q = A^+A\\,\\!</math> are [[projection (linear algebra)|orthogonal projection operators]] – that is, they are  Hermitian (<math>P = P^*\\,\\!</math>, <math>Q = Q^*\\,\\!</math>) and idempotent (<math>P^2 = P\\,\\!</math> and <math>Q^2 = Q\\,\\!</math>). The following hold:\n* <math>PA = AQ = A\\,\\!</math> and <math>A^+ P = QA^+ = A^+\\,\\!</math>\n* <math>P\\,\\!</math> is the [[orthogonal projector]] onto the [[range (mathematics)|range]] of <math>A\\,\\!</math> (which equals the [[orthogonal complement]] of the kernel of <math>A^*\\,\\!</math>).\n* <math>Q\\,\\!</math> is the orthogonal projector onto the [[range (mathematics)|range]] of <math>A^*\\,\\!</math> (which equals the orthogonal complement of the kernel of <math>A\\,\\!</math>).\n* <math>(I - Q) = \\left(I - A^+A\\right)\\,\\!</math> is the orthogonal projector onto the kernel of <math>A\\,\\!</math>.\n* <math>(I - P) = \\left(I - AA^+\\right)\\,\\!</math> is the orthogonal projector onto the kernel of <math>A^*\\,\\!</math>.<ref name=\"GvL1996\"/>\n\nThe last two properties imply the following identities:\n* <math>A\\,\\ \\left(I - A^+ A\\right)= \\left(I - AA^+\\right)A\\ \\ = 0</math>\n* <math>A^*\\left(I - AA^+\\right) = \\left(I - A^+A\\right)A^* = 0</math>\n\nAnother property is the following: if <math>A \\in R^{n\\times n}</math> is Hermitian and idempotent (true if and only if it represents an orthogonal projection), then, for any matrix <math>B\\in R^{m\\times n}</math> the following equation holds:<ref>{{cite journal|first=Anthony A.|last=Maciejewski|first2=Charles A.|last2=Klein|title=Obstacle Avoidance for Kinematically Redundant Manipulators in Dynamically Varying Environments|journal=International Journal of Robotics Research|volume=4|issue=3|pages=109–117|year=1985|doi=10.1177/027836498500400308}}</ref>\n: <math> A(BA)^+ = (BA)^+</math>\n\nThis can be proven by defining matrices <math>C = BA</math>, <math>D = A(BA)^+</math>, and checking that <math>D</math> is indeed a pseudoinverse for <math>C</math> by verifying that the defining properties of the pseudoinverse hold, when <math>A</math> is Hermitian and idempotent.\n\nFrom the last property it follows that, if <math>A \\in R^{n\\times n}</math> is Hermitian and idempotent, for any matrix <math>B \\in R^{n\\times m}</math>\n:<math>(AB)^+A = (AB)^+</math>\n\nFinally, it should be noted that if <math>A</math> is an orthogonal projection matrix, then its pseudoinverse trivially coincides with the matrix itself, i.e. <math>A^+ = A\\,\\!</math>.\n\n===Geometric construction===\nIf we view the matrix as a linear map <math>A:K^n \\to K^m</math> over a field <math>K</math> then <math>A^+: K^m \\to K^n </math> can be decomposed as follows. We write <math>\\oplus</math> for the [[direct sum of modules|direct sum]], <math>\\perp</math> for the [[orthogonal complement]], <math>\\operatorname{ker}</math> for the [[kernel (linear algebra)|kernel]] of a map, and <math>\\operatorname{ran}</math> for the [[image (mathematics)|image]] of a map. Notice that <math>K^n = \\left(\\operatorname{ker} A\\right)^\\perp \\oplus \\operatorname{ker} A</math> and <math>K^m = \\operatorname{ran} A \\oplus \\left(\\operatorname{ran} A\\right)^\\perp</math>. The restriction <math> A: \\left(\\operatorname{ker} A\\right)^\\perp \\to \\operatorname{ran} A</math> is then an isomorphism. These imply that <math>A^+</math> is defined on <math>\\operatorname{ran} A</math> to be the inverse of this isomorphism, and on <math>\\left(\\operatorname{ran} A\\right)^\\perp</math> to be zero.\n\nIn other words: To find <math>A^+b</math> for given {{math|''b''}} in {{math|''K''<sup>''m''</sup>}}, first project {{math|''b''}} orthogonally onto the range of {{math|''A''}}, finding a point {{math|''p''(''b'')}} in the range. Then form {{math|''A''<sup>−1</sup>({''p''(''b'')})}}, i.e. find those vectors in {{math|''K''<sup>''n''</sup>}} that {{math|''A''}} sends to {{math|''p''(''b'')}}. This will be an affine subspace of {{math|''K''<sup>''n''</sup>}} parallel to the kernel of {{math|''A''}}. The element of this subspace that has the smallest length (i.e. is closest to the origin) is the answer <math>A^+b</math> we are looking for. It can be found by taking an arbitrary member of {{math|''A''<sup>−1</sup>({''p''(''b'')})}} and projecting it orthogonally onto the orthogonal complement of the kernel of {{math|''A''}}.\n\nThis description is closely related to the [[Moore–Penrose inverse#Minimum norm solution to a linear system|Minimum norm solution to a linear system]].\n\n===Subspaces===\n: <math>\\begin{align}\n  \\operatorname{ker}\\left(A^+\\right) &= \\operatorname{ker}\\left(A^*\\right) \\\\\n  \\operatorname{ran}\\left(A^+\\right) &= \\operatorname{ran}\\left(A^*\\right)\n\\end{align}</math>\n\n===Limit relations===\nThe pseudoinverse are limits:\n: <math>A^+ = \\lim_{\\delta \\searrow 0} \\left(A^* A + \\delta I\\right)^{-1} A^*\n            = \\lim_{\\delta \\searrow 0} A^* \\left(A A^* + \\delta I\\right)^{-1}\n</math>\n: (see [[Tikhonov regularization]]). These limits exist even if <math>\\left(AA^*\\right)^{-1}\\,\\!</math>  or <math>\\left(A^*A\\right)^{-1}\\,\\!</math> do not exist.<ref name=\"GvL1996\"/>{{rp|263}}\n\n===Continuity===\nIn contrast to ordinary matrix inversion, the process of taking pseudoinverses is not [[continuous function|continuous]]: if the sequence <math>\\left(A_n\\right)</math> converges to the matrix {{math|''A''}} (in the [[matrix norm|maximum norm or Frobenius norm]], say), then {{math|(''A<sub>n</sub>'')<sup>+</sup>}} need not converge to {{math|''A''<sup>+</sup>}}. However, if all the matrices have the same rank, {{math|(''A<sub>n</sub>'')<sup>+</sup>}} will converge to {{math|''A''<sup>+</sup>}}.<ref name=\"rakocevic1997\">{{cite journal | last=Rakočević | first=Vladimir | title=On continuity of the Moore–Penrose and Drazin inverses | journal=Matematički Vesnik | volume=49 | pages=163–72 | year=1997 | url =http://elib.mi.sanu.ac.rs/files/journals/mv/209/mv973404.pdf }}</ref>\n\n===Derivative===\nThe derivative of a real valued pseudoinverse matrix which has constant rank at a point <math>x</math> may be calculated in terms of the derivative of the original matrix:<ref>{{cite journal|title=The Differentiation of Pseudo-Inverses and Nonlinear Least Squares Problems Whose Variables Separate|first1=G. H.|last1=Golub|first2=V.|last2=Pereyra|journal=SIAM Journal on Numerical Analysis|volume=10|number=2|date=April 1973|pages=413–32|jstor=2156365|doi=10.1137/0710036}}</ref>\n: <math>\n  \\frac{\\mathrm d}{\\mathrm d x} A^+(x) =\n    -A^+ \\left( \\frac{\\mathrm d}{\\mathrm d x} A \\right) A^+ ~+~\n     A^+ A^{+\\text{T}} \\left( \\frac{\\mathrm d}{\\mathrm d x} A^\\text{T} \\right) \\left(I - A A^+\\right) ~+~\n     \\left(I - A^+ A\\right) \\left( \\frac{\\text{d}}{\\text{d} x} A^\\text{T} \\right) A^{+\\text{T}} A^+\n</math>\n\n==Examples==\n\nSince for invertible matrices the pseudoinverse equals the usual inverse, only examples of non-invertible matrices are considered below.\n\n* For <math>\\mathbf{A} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix},\\,</math> the pseudoinverse is <!--\n--> <math>\\mathbf{A^+} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}.</math> (Generally, the pseudoinverse of a zero matrix is its transpose.) The uniqueness of this pseudoinverse can be seen from the requirement <math>A^+ = A^+ A A^+</math>, since multiplication by a zero matrix would always produce a zero matrix.\n\n* For <math>\\mathbf{A} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 0 \\end{pmatrix},\\,</math> the pseudoinverse is <!--\n--> <math>\\mathbf{A^+} = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} \\\\ 0 & 0 \\end{pmatrix}.</math> <!--\n--> <br> Indeed, <!--\n--> <math>\\mathbf{A\\,A^+} = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} \\\\ \\frac{1}{2} & \\frac{1}{2}  \\end{pmatrix},\\,</math> and thus <!--\n--> <math>\\mathbf{A\\,A^+A} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 0\\end{pmatrix} = A.</math> <!--\n--> <br> Similarly, <!--\n--> <math>\\mathbf{A^+A} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix},\\,</math> <!--\n--> and thus <!--\n--> <math>\\mathbf{A^+A\\,A^+} = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} \\\\ 0 & 0 \\end{pmatrix} = A^+.</math>\n\n* For <!--\n--> <math>\\mathbf{A} = \\begin{pmatrix} 1 & 0 \\\\ -1 & 0 \\end{pmatrix},\\ </math> <!--\n--> <math>\\mathbf{A^+} = \\begin{pmatrix} \\frac{1}{2} & -\\frac{1}{2} \\\\ 0 & 0 \\end{pmatrix}.</math>\n\n* For <!--\n--> <math>\\mathbf{A} = \\begin{pmatrix} 1 & 0 \\\\ 2 & 0 \\end{pmatrix},\\ </math> <!--\n--> <math>\\mathbf{A^+} = \\begin{pmatrix} \\frac{1}{5} & \\frac{2}{5} \\\\ 0 & 0 \\end{pmatrix}.</math> (The denominators are <math>5 = 1^2 + 2^2</math>.)\n\n* For<!--\n--> <math>\\mathbf{A} = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix},\\ </math> <!--\n--> <math>\\mathbf{A^+} = \\begin{pmatrix} \\frac{1}{4} & \\frac{1}{4} \\\\ \\frac{1}{4} & \\frac{1}{4} \\end{pmatrix}.</math>\n\n* For <math>\\mathbf{A} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 1 \\end{pmatrix},\\,</math> the pseudoinverse is <!--\n--> <math>\\mathbf{A^+} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & \\frac{1}{2} & \\frac{1}{2} \\end{pmatrix}.</math> <!--\n--> <br> Note that for this matrix, the [[inverse element#Matrices|left inverse]] exists and thus equals <math>\\mathbf{A^+}</math>, <!--\n--> indeed, <math>\\mathbf{A^+A} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}.</math>\n\n==Special cases==\n\n===Scalars===\nIt is also possible to define a pseudoinverse for scalars and vectors. This amounts to treating these as matrices. The pseudoinverse of a scalar {{math|''x''}} is zero if {{math|''x''}} is zero and the reciprocal of {{math|''x''}} otherwise:\n:<math>x^+ = \\begin{cases}\n  0,      & \\mbox{if }x = 0; \\\\ \n  x^{-1}, & \\mbox{otherwise}.\n\\end{cases}</math>\n\n===Vectors===\nThe pseudoinverse of the null (all zero) vector is the transposed null vector. The pseudoinverse of a non-null vector is the conjugate transposed vector divided by its squared magnitude:\n:<math>x^+ = \\begin{cases}\n  0^\\mathrm{T},      & \\mbox{if }x = 0; \\\\\n  {x^* \\over x^* x}, & \\mbox{otherwise}.\n\\end{cases}</math>\n\n===Linearly independent columns===\nIf the '''columns''' of <math>A\\,\\!</math> are [[linear independence|linearly independent]]\n(so that <math>m \\ge n</math>), then <math>A^*A\\,\\!</math> is invertible. In this case, an explicit formula is:{{sfn|Ben-Israel|Greville|2003}}\n:<math>A^+ = \\left(A^*A\\right)^{-1}A^*\\,\\!</math>.\n\nIt follows that <math>A^+\\,\\!</math> is then a left inverse of <math>A\\,\\!</math>: &nbsp; <math>A^+ A = I_n\\,\\!</math>.\n\n===Linearly independent rows===\nIf the '''rows''' of <math>A\\,\\!</math> are linearly independent (so that <math>m \\le n</math>), then\n<math>AA^*</math> is invertible. In this case, an explicit formula is:\n:<math>A^+ = A^*\\left(AA^*\\right)^{-1}\\,\\!</math>.\n\nIt follows that <math>A^+\\,\\!</math> is a right inverse of <math>A\\,\\!</math>: &nbsp; <math>A A^+ = I_m\\,\\!</math>.\n\n===Orthonormal columns or rows===\nThis is a special case of either full column rank or full row rank (treated above). If <math>A\\,\\!</math> has orthonormal columns (<math>A^*A = I_n\\,\\!</math>) or orthonormal rows (<math>AA^* = I_m\\,\\!</math>), then:\n:<math>A^+ = A^*\\,\\!</math>.\n\n===Orthogonal projection matrices===\nIf <math>A\\,\\!</math> is an orthogonal projection matrix, i.e. <math>A = A^*</math> and <math>A^2 = A</math>, then the pseudoinverse trivially coincides with the matrix itself:\n:<math>A^+ = A\\,\\!</math>.\n\n===Circulant matrices===\nFor a [[circulant matrix]] <math>C\\,\\!</math>, the singular value decomposition is given by the [[Fourier transform]], that is the singular values are the Fourier coefficients. Let <math>\\mathcal{F}</math> be the [[DFT matrix|Discrete Fourier Transform (DFT) matrix]], then<ref name=\"Stallings1972\">{{cite journal | last=Stallings | first=W. T. | authorlink=W. T. Stallings | title=The Pseudoinverse of an ''r''-Circulant Matrix | journal=[[Proceedings of the American Mathematical Society]] | volume=34 | issue=2 | pages=385–88 | year=1972 | doi=10.2307/2038377 | last2=Boullion | first2=T. L.| jstor=2038377 }}</ref>\n:<math>\\begin{align}\n    C &= \\mathcal{F}\\cdot\\Sigma\\cdot\\mathcal{F}^* \\\\\n  C^+ &= \\mathcal{F}\\cdot\\Sigma^+\\cdot\\mathcal{F}^*\n\\end{align}</math>\n\n==Construction==\n\n===Rank decomposition===\nLet <math>r \\le \\min(m, n)</math> denote the [[rank (matrix theory)|rank]] of <math>A \\in K^{m\\times n}\\,\\!</math>. Then <math>A\\,\\!</math> can be [[rank factorization|(rank) decomposed]] as\n<math>A = BC\\,\\!</math> where <math>B \\in K^{m\\times r}\\,\\!</math> and <math>C \\in K^{r\\times n}\\,\\!</math> are of rank <math>r</math>. Then <math>A^+ = C^+B^+ = C^*\\left(CC^*\\right)^{-1}\\left(B^*B\\right)^{-1}B^*\\,\\!</math>.\n\n===The QR method===\nFor <math>K = \\mathbb{R}\\,\\!</math> or <math>K = \\mathbb{C}\\,\\!</math> computing the product <math>AA^*</math> or <math>A^*A</math> and their inverses explicitly is often a source of numerical rounding errors and computational cost in practice. An alternative approach using the [[QR decomposition]] of <math>A\\,\\!</math> may be used instead.\n\nConsider the case when <math>A\\,\\!</math> is of full column rank, so that <math>A^+ = (A^*A)^{-1}A^*\\,\\!</math>. Then the [[Cholesky decomposition]] <math>A^*A = R^*R\\,\\!</math>, where <math>R\\,\\!</math> is an [[upper triangular matrix]], may be used. Multiplication by the inverse is then done easily by solving a system with multiple right-hand sides,\n: <math>A^+ = (A^*A)^{-1}A^*  \\quad \\Leftrightarrow \\quad  (A^*A)A^+ = A^*  \\quad \\Leftrightarrow \\quad R^*RA^+ = A^* </math>\n\nwhich may be solved by [[forward substitution]] followed by [[back substitution]].\n\nThe Cholesky decomposition may be computed without forming <math>A^*A\\,\\!</math> explicitly, by alternatively using the [[QR decomposition]] of <math> A = QR\\,\\!</math>, where <math>Q\\,\\,\\!</math> has orthonormal columns, <math> Q^*Q = I </math>, and <math>R\\,\\!</math> is upper triangular. Then\n: <math> A^*A \\,=\\, (QR)^*(QR) \\,=\\, R^*Q^*QR \\,=\\, R^*R</math>,\n\nso {{math|''R''}} is the Cholesky factor of <math>A^*A</math>.\n\nThe case of full row rank is treated similarly by using the formula <math>A^+ = A^*(AA^*)^{-1}\\,\\!</math> and using a similar argument, swapping the roles of <math>A</math> and <math>A^*</math>.\n\n===Singular value decomposition (SVD)===\nA computationally simple and accurate way to compute the pseudoinverse is by using the [[singular value decomposition]].{{sfn|Ben-Israel|Greville|2003}}<ref name=\"GvL1996\"/><ref name=\"SLEandPI\">[http://websites.uwlax.edu/twill/svd/systems/index.html Linear Systems & Pseudo-Inverse]</ref>  If <math>A = U\\Sigma V^*</math> is the singular value decomposition of {{math|''A''}}, then <math>A^+ = V\\Sigma^+ U^*</math>. For a [[rectangular diagonal matrix]] such as <math>\\Sigma</math>, we get the pseudoinverse by taking the reciprocal of each non-zero element on the diagonal, leaving the zeros in place, and then transposing the matrix. In numerical computation, only elements larger than some small tolerance are taken to be nonzero, and the others are replaced by zeros. For example, in the [[MATLAB]], [[GNU Octave]], or [[NumPy]] function <tt>pinv</tt>, the tolerance is taken to be {{math|''t'' {{=}} ε⋅max(''m'', ''n'')⋅max(Σ)}}, where ε is the [[machine epsilon]].\n\nThe computational cost of this method is dominated by the cost of computing the SVD, which is several times higher than matrix–matrix multiplication, even if a state-of-the art implementation (such as that of [[LAPACK]]) is used.\n\nThe above procedure shows why taking the pseudoinverse is not a continuous operation: if the original matrix {{math|''A''}} has a singular value 0 (a diagonal entry of the matrix <math>\\Sigma</math> above), then modifying {{math|''A''}} slightly may turn this zero into a tiny positive number, thereby affecting the pseudoinverse dramatically as we now have to take the reciprocal of a tiny number.\n\n===Block matrices===\n[[Block matrix pseudoinverse|Optimized approaches]] exist for calculating the pseudoinverse of block structured matrices.\n\n===The iterative method of Ben-Israel and Cohen===\nAnother method for computing the pseudoinverse (cf. [[Drazin inverse]]) uses the recursion\n:<math> A_{i+1} = 2A_i - A_i A A_i, \\, </math>\n\nwhich is sometimes referred to as hyper-power sequence. This recursion produces a sequence converging quadratically to the pseudoinverse of <math>A</math> if it is started with an appropriate <math>A_0</math> satisfying <math>A_0 A = \\left(A_0 A\\right)^*</math>. The choice <math>A_0 = \\alpha A^*</math> (where <math>0 < \\alpha < 2/\\sigma^2_1(A)</math>, with <math>\\sigma_1(A)</math> denoting the largest singular value of <math>A</math>) <ref>{{cite journal | last1=Ben-Israel | first1=Adi | last2=Cohen | first2=Dan | title=On Iterative Computation of Generalized Inverses and Associated Projections | journal=SIAM Journal on Numerical Analysis | volume=3 | issue=3 | pages=410–19 | year=1966 | jstor=2949637 | doi=10.1137/0703035 }}[http://benisrael.net/COHEN-BI-ITER-GI.pdf pdf]</ref> has been argued not to be competitive to the method using the SVD mentioned above, because even for moderately ill-conditioned matrices it takes a long time before <math>A_i</math> enters the region of quadratic convergence.<ref>{{cite journal | last1=Söderström | first1=Torsten | last2=Stewart | first2=G. W. | title=On the Numerical Properties of an Iterative Method for Computing the Moore–Penrose Generalized Inverse | journal=SIAM Journal on Numerical Analysis | volume=11 | issue=1 | pages=61–74 | year=1974 | jstor=2156431 | doi=10.1137/0711008 }}</ref> However, if started with <math>A_0</math> already close to the Moore–Penrose inverse and <math>A_0 A = \\left(A_0 A\\right)^*</math>, for example <math>A_0 := \\left(A^* A + \\delta I\\right)^{-1} A^*</math>, convergence is fast (quadratic).\n\n===Updating the pseudoinverse===\n\nFor the cases where {{math|''A''}} has full row or column rank, and the inverse of the correlation matrix (<math>AA^*</math> for {{math|''A''}} with full row rank or <math>A^*A</math> for full column rank) is already known, the pseudoinverse for matrices related to <math>A</math> can be computed by applying the [[Sherman–Morrison–Woodbury formula]] to update the inverse of the correlation matrix, which may need less work. In particular, if the related matrix differs from the original one by only a changed, added or deleted row or column, incremental algorithms<ref name=\"G1992\">{{Cite journal |author= Tino Gramß |title= Worterkennung mit einem künstlichen neuronalen Netzwerk |version= |publisher= Georg-August-Universität zu Göttingen |year = 1992 | url = }}</ref><ref name=\"EMTIYAZ2008\">, Mohammad Emtiyaz, \"Updating Inverse of a Matrix When a Column is Added/Removed\"[http://www.cs.ubc.ca/~emtiyaz/Writings/OneColInv.pdf]</ref> exist that exploit the relationship.\n\nSimilarly, it is possible to update the Cholesky factor when a row or column is added, without creating the inverse of the correlation matrix explicitly. However, updating the pseudoinverse in the general rank-deficient case is much more complicated.<ref>{{cite journal|last=Meyer, Jr.|first=Carl D.|title=Generalized inverses and ranks of block matrices|journal=SIAM J. Appl. Math.|volume=25|issue=4|date=1973|pages=597–602|doi=10.1137/0125057}}</ref><ref>{{cite journal|last=Meyer, Jr.|first=Carl D.|title=Generalized inversion of modified matrices|journal=SIAM J. Appl. Math.|volume=24|issue=3|date=1973|pages=315–23|doi=10.1137/0124033}}</ref>\n\n===Software libraries===\nThe Python package [[NumPy]] provides a pseudoinverse calculation through its functions <code>matrix.I</code> and <code>linalg.pinv</code>; its <code>pinv</code> uses the SVD-based algorithm. [[SciPy]] adds a function <code>scipy.linalg.pinv</code> that uses a least-squares solver. High-quality implementations of SVD, QR, and back substitution are available in [[Singular value decomposition#Implementations|standard libraries]], such as [[LAPACK]].  Writing one's own implementation of SVD is a major programming project that requires a significant [[Floating point#Accuracy problems|numerical expertise]]. In special circumstances, such as [[parallel computing]] or [[embedded computing]], however, alternative implementations by QR or even the use of an explicit inverse might be preferable, and custom implementations may be unavoidable.\n\nThe MASS package for [[R (programming language)|R]] provides a calculation of the Moore–Penrose inverse through the <code>ginv</code> function.<ref>{{cite web |url=https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/ginv.html |title=R: Generalized Inverse of a Matrix}}</ref> The <code>ginv</code> function calculates a pseudoinverse using the singular value decomposition provided by the <code>svd</code> function in the base R package. An alternative is to employ the <code>pinv</code> function available in the pracma package.\n\nThe [[GNU Octave|Octave programming language]] provides a pseudoinverse through the standard package function <code>pinv</code> and the <code>pseudo_inverse()</code> method.\n\n==Applications==\n\n===Linear least-squares===\n{{See also|Linear least squares (mathematics)}}\n\nThe pseudoinverse provides a [[linear least squares (mathematics)|least squares]] solution to a [[system of linear equations]].<ref name=\"Penrose1956\">{{cite journal | last=Penrose | first=Roger | title=On best approximate solution of linear matrix equations | journal=[[Proceedings of the Cambridge Philosophical Society]] | volume=52 | pages=17–19 | year=1956 | doi=10.1017/S0305004100030929}}</ref>\nFor <math>A \\in K^{m\\times n}\\,\\!</math>, given a system of linear equations\n:<math>A x = b,\\,</math>\n\nin general, a vector <math>x</math> that solves the system may not exist, or if one does exist, it may not be unique. The pseudoinverse solves the \"least-squares\" problem as follows:\n\n* <math>\\forall x \\in K^n\\,\\!</math>, we have <math>\\|Ax - b\\|_2 \\ge \\|Az - b\\|_2</math> where <math>z = A^+b</math> and <math>\\|\\cdot\\|_2</math> denotes the [[Euclidean norm]].  This weak inequality holds with equality if and only if <math>x = A^+b + \\left(I - A^+A\\right)w</math>  for any vector ''w''; this provides an infinitude of minimizing solutions unless ''A'' has full column rank, in which case <math>\\left(I - A^+A\\right)</math>  is a zero matrix.<ref name=Planitz>{{cite journal|last=Planitz|first=M.|title=Inconsistent systems of linear equations|journal=Mathematical Gazette|volume=63|issue=425|date=October 1979|pages=181–85|doi=10.2307/3617890|jstor=3617890}}</ref> The solution with minimum Euclidean norm is <math>z.</math><ref name=Planitz/>\n\nThis result is easily extended to systems with multiple right-hand sides, when the Euclidean norm is replaced by the Frobenius norm. Let <math> B \\in K^{m\\times p}</math>.\n\n* <math>\\forall X \\in K^{n\\times p}\\,\\!</math>, we have <math> \\|AX - B\\|_{\\mathrm{F}} \\ge \\|AZ -B\\|_{\\mathrm{F}}</math> where <math>Z = A^+B</math> and <math>\\|\\cdot\\|_{\\mathrm{F}} </math> denotes the [[Frobenius norm]].\n\n===Obtaining all solutions of a linear system===\n\nIf the linear system\n\n:<math>A x = b\\,</math>\n\nhas any solutions, they are all given by<ref name=James>{{cite journal|last=James|first=M.|title=The generalised inverse|journal=Mathematical Gazette|volume=62|issue=420|date=June 1978|pages=109–14|doi=10.1017/S0025557200086460}}</ref>\n\n:<math>x = A^+ b + \\left[I - A^+ A\\right]w</math>\n\nfor arbitrary vector <math>w</math>. Solution(s) exist if and only if <math>AA^+ b = b</math>.<ref name=James/>  If the latter holds, then the solution is unique if and only if ''A'' has full column rank, in which case <math>\\left[I - A^+ A\\right]</math> is a zero matrix. If solutions exist but ''A'' does not have full column rank, then we have an [[indeterminate system]], all of whose infinitude of solutions are given by this last equation.\n\n===Minimum norm solution to a linear system===\nFor linear systems <math>Ax = b,\\,</math> with non-unique solutions (such as under-determined systems), the pseudoinverse may be used to construct the solution of minimum [[Euclidean norm]]\n<math>\\|x\\|_2</math> among all solutions.\n\n* If <math>Ax = b\\,</math> is satisfiable, the vector <math>z = A^+b</math> is a solution, and satisfies <math>\\|z\\|_2 \\le \\|x\\|_2</math> for all solutions.\n\nThis result is easily extended to systems with multiple right-hand sides, when the Euclidean norm is replaced by the Frobenius norm. Let <math>B \\in K^{m\\times p}\\,\\!</math>.\n\n* If <math>AX = B\\,</math> is satisfiable, the matrix <math>Z = A^+B</math> is a solution, and satisfies <math>\\|Z\\|_{\\mathrm{F}} \\le \\|X\\|_{\\mathrm{F}}</math> for all solutions.\n\n===Condition number===\nUsing the pseudoinverse and a [[matrix norm]], one can define a [[condition number]] for any matrix:\n:<math>\\mbox{cond}(A) = \\|A\\| \\left\\|A^+\\right\\|.\\ </math>\n\nA large condition number implies that the problem of finding least-squares solutions to the corresponding system of linear equations is ill-conditioned in the sense that small errors in the entries of {{math|''A''}} can lead to huge errors in the entries of the solution.<ref name=hagen/>\n\n==Generalizations==\nIn order to solve more general least-squares problems, one can define Moore–Penrose inverses for all continuous linear operators {{math|''A'' : ''H''<sub>1</sub> → ''H''<sub>2</sub>}} between two [[Hilbert space]]s {{math|''H''<sub>1</sub>}} and {{math|''H''<sub>2</sub>}}, using the same four conditions as in our definition above. It turns out that not every continuous linear operator has a continuous linear pseudoinverse in this sense.<ref name=hagen>{{cite book|first1=Roland|last1=Hagen|first2=Steffen|last2=Roch|first3=Bernd|last3=Silbermann|title=C*-algebras and Numerical Analysis|publisher=CRC Press|year=2001|chapter=Section 2.1.2}}</ref> Those that do are precisely the ones whose range is [[closed set|closed]] in {{math|''H''<sub>2</sub>}}.\n\nIn [[abstract algebra]], a Moore–Penrose inverse may be defined on a [[*-regular semigroup]]. This abstract definition coincides with the one in linear algebra.\n\n==See also==\n* [[Proofs involving the Moore–Penrose inverse]]\n* [[Drazin inverse]]\n* [[Hat matrix]]\n* [[Inverse element]]\n* [[Linear least squares (mathematics)]]\n* [[Pseudo-determinant]]\n* [[Von Neumann regular ring]]\n\n==Notes==\n{{Reflist|30em}}\n\n==References==\n* {{cite book| first1 = Adi | last1 = Ben-Israel |author1-link=Adi Ben-Israel| first2 = Thomas N.E. | last2 = Greville |author2-link=Thomas N. E. Greville|title=Generalized inverses: Theory and applications|\nedition= 2nd |location= New York, NY | publisher =  Springer |year=2003| isbn = 978-0-387-00293-4 |ref=harv| doi = 10.1007/b97366 }}\n* {{cite book| first1 = S. L. | last1 = Campbell | first2 = C. D. | last2 = Meyer, Jr. | title= Generalized Inverses of Linear Transformations|\npublisher=Dover |year=1991 | isbn = 978-0-486-66693-8 |ref=harv}}\n* {{cite book| first = Yoshihiko | last = Nakamura  | title= Advanced Robotics: Redundancy and Optimization|\npublisher=Addison-Wesley  |year= 1991 | isbn = 978-0201151985 |ref=harv}}\n* {{cite book| first1 = C. Radhakrishna | last1 = Rao | first2 = Sujit Kumar | last2 = Mitra| title = Generalized Inverse of Matrices and its Applications | publisher= John Wiley & Sons |location = New York |year= 1971 | page=240 | isbn = 978-0-471-70821-6 |ref=harv}}\n\n==External links==\n* [http://planetmath.org/Pseudoinverse Pseudoinverse on PlanetMath]\n* [http://people.revoledu.com/kardi/tutorial/LinearAlgebra/MatrixGeneralizedInverse.html Interactive program & tutorial of Moore–Penrose Pseudoinverse]\n* {{planetmath reference|id=6067|title=Moore–Penrose inverse}}\n* {{MathWorld|urlname=Pseudoinverse|title=Pseudoinverse}}\n* {{MathWorld|urlname=Moore-PenroseMatrixInverse|title=Moore–Penrose Inverse}}\n* [https://arxiv.org/abs/1110.6882 The Moore–Penrose Pseudoinverse. A Tutorial Review of the Theory]\n* [http://engineerjs.com/doc/ejs/engine/linalg-1/_pinv.html Online Moore-Penrose Inverse calculator]\n\n{{Numerical linear algebra}}\n\n{{DEFAULTSORT:Moore-Penrose Pseudoinverse}}\n[[Category:Matrix theory]]\n[[Category:Singular value decomposition]]\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "Nested dissection",
      "url": "https://en.wikipedia.org/wiki/Nested_dissection",
      "text": "In [[numerical analysis]], '''nested dissection''' is a [[Divide and conquer algorithm|divide and conquer]] [[heuristic]] for the solution of [[sparse matrix|sparse]] [[symmetric matrix|symmetric]] [[System of linear equations|systems of linear equations]] based on [[graph partitioning]]. Nested dissection was introduced by {{harvtxt|George|1973}}; the name was suggested by [[Garrett Birkhoff]].<ref>{{harvtxt|George|1973}}.</ref>\n\nNested dissection consists of the following steps:\n*Form an [[undirected graph]] in which the vertices represent rows and columns of the system of linear equations, and an edge represents a nonzero entry in the [[sparse matrix]] representing the system.\n*[[Recursion|Recursively]] partition the graph into [[Glossary of graph theory#Subgraphs|subgraphs]] using [[planar separator theorem|separators]], small subsets of vertices the removal of which allows the graph to be partitioned into subgraphs with at most a constant fraction of the number of vertices.\n*Perform [[Cholesky decomposition]] (a variant of [[Gaussian elimination]] for symmetric matrices), ordering the elimination of the variables by the recursive structure of the partition: each of the two subgraphs formed by removing the separator is eliminated first, and then the separator vertices are eliminated.\n\nAs a consequence of this algorithm, the fill-in (the set of nonzero matrix entries created in the Cholesky decomposition that are not part of the input matrix structure) is limited to at most the square of the separator size at each level of the recursive partition. In particular, for planar graphs (frequently arising in the solution of sparse linear systems derived from two-dimensional [[finite element method]] meshes) the resulting matrix has O(''n''&nbsp;log&nbsp;''n'') nonzeros, due to the [[planar separator theorem]] guaranteeing separators of size O({{radic|''n''}}).<ref>{{harvtxt|Lipton|Rose|Tarjan|1979}}; {{harvtxt|Gilbert|Tarjan|1986}}.</ref> For arbitrary graphs there is a nested dissection that guarantees fill-in within a <math>O(\\min\\{\\sqrt{d}\\log^4 n, m^{1/4}\\log^{3.5} n\\})</math> factor of optimal, where ''d'' is the maximum degree and ''m'' is the number of non-zeros. <ref>{{harvtxt|Agrawal|Klein|Ravi|1993}}.</ref>\n\n==See also==\n*[[Cycle rank]] of a graph, or a symmetric Boolean matrix, measures the minimum parallel time needed to perform Cholesky decomposition  \n*[[Vertex separator]]\n\n==Notes==\n{{reflist}}\n\n==References==\n*{{citation\n | last = George | first = J. Alan | authorlink = J. Alan George\n | doi = 10.1137/0710032\n | issue = 2\n | journal = SIAM Journal on Numerical Analysis\n | pages = 345–363\n | title = Nested dissection of a regular finite element mesh\n | volume = 10\n | year = 1973\n | jstor = 2156361}}.\n*{{citation\n | last = Gilbert | first = John R.\n | doi = 10.1016/0020-0190(88)90191-3\n | issue = 6\n | journal = Information Processing Letters\n | pages = 325–328\n | title = Some nested dissection order is nearly optimal\n | url = http://www.ecommons.cornell.edu/handle/1813/6607\n | volume = 26\n | year = 1988}}.\n*{{citation\n | last1 = Gilbert | first1 = John R.\n | last2 = Tarjan | first2 = Robert E. | author2-link = Robert Tarjan\n | doi = 10.1007/BF01396660\n | issue = 4\n | journal = Numerische Mathematik\n | pages = 377–404\n | title = The analysis of a nested dissection algorithm\n | volume = 50\n | year = 1986}}.\n*{{citation\n | last1 = Lipton | first1 = Richard J. | author1-link = Richard J. Lipton\n | last2 = Rose | first2 = Donald J.\n | last3 = Tarjan | first3 = Robert E. | author3-link = Robert Tarjan\n | doi = 10.1137/0716027\n | journal = SIAM Journal on Numerical Analysis\n | pages = 346–358\n | title = Generalized nested dissection\n | year = 1979\n | jstor = 2156840\n | volume = 16\n | issue = 2}}.\n*{{citation\n | last1 = Agrawal | first1 = Ajit\n | last2 = Klein | first2 = Philip\n | last3 = Ravi | first3 = R.\n | doi = 10.1007/978-1-4613-8369-7_2\n | pages = 31-55\n | chapter = Cutting down on Fill Using Nested Dissection: Provably Good Elimination Orderings\n | title = Graph Theory and Sparse Matrix Computation\n | publication-date = 1993\n | publisher = Springer New York\n | isbn = 978-1-4613-8371-0}}.\n\n[[Category:Sparse matrices]]\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "Numerical linear algebra",
      "url": "https://en.wikipedia.org/wiki/Numerical_linear_algebra",
      "text": "'''Numerical linear algebra''' is the study of how matrix operations can be used to create [[algorithms|computer algorithms]] which [[Algorithmic efficiency|efficiently]] and accurately provide approximate answers to mathematical questions. It is a subfield of [[numerical analysis]], and a type of [[linear algebra]]. Because [[computer|computers]] use [[floating-point arithmetic]], they cannot exactly represent [[Irrational number|irrational]] data, and many algorithms [[Propagation of uncertainty|increase that imprecision]] when implemented by a computer. Numerical linear algebra uses properties of vectors and matrices to develop computer algorithms that minimize computer error while retaining efficiency and precision.\n\nNumerical linear algebra aims to solve problems of continuous [[mathematics]] using finite precision computers, so its applications to the [[Natural science|natural]] and [[Social science|social sciences]] are as vast as the applications of continuous mathematics. It is often a fundamental part of [[engineering]] and [[computational science]] problems, such as [[image processing|image]] and [[signal processing]], [[telecommunication]], [[computational finance]], [[materials science]] simulations, [[structural biology]], [[data mining]], [[bioinformatics]], and [[fluid dynamics]]. Matrix methods are particularly used in [[finite difference method|finite difference methods]], [[finite element method|finite element methods]], and the modeling of [[Differential equation|differential equations]]. Noting the broad applications of numerical linear algebra, [[Lloyd N. Trefethen]] and David Bau, III argue that it is \"as fundamental to the mathematical sciences as calculus and differential equations\",<ref name = \"tb397\">{{cite book | last1 = Trefethen | first = Lloyd | last2 = Bau III | first2 = David | location=Philadelphia | isbn=978-0-89871-361-9 | year = 1997 | title = Numerical Linear Algebra | publisher = SIAM | edition =  1st}}</ref>{{rp|x}} even though it is a comparatively small field.<ref name = \"golubhist\">{{cite web |url=https://www.stat.uchicago.edu/~lekheng/courses/302/slides0.pdf |title=A History of Modern Numerical Linear Algebra |last=Golub |first=Gene  H. |website=University of Chicago Statistics Department |access-date=February 17, 2019}}</ref>\n\nNumerical linear algebra has also been called \"applied linear algebra\", and because many properties of matrices and vectors also apply to functions and operators, it can also be viewed as a type of [[functional analysis]] that has a particular emphasis on practical algorithms.<ref name=tb397/>{{rp|ix}}\n\nCommon problems in numerical linear algebra include obtaining matrix decompositions like the singular value decomposition, the QR factorization, the LU factorization, or the eigendecomposition, which can then be used to answer common linear algebraic problems like solving linear systems of equations, locating eigenvalues, or least squares optimisation. Numerical linear algebra is centrally concerned with developing algorithms that do not introduce errors when applied to real data on a finite precision computer. Often this is achieved by iterative methods rather than direct methods.\n\n==History==\nNumerical linear algebra was developed by computer pioneers like [[John von Neumann]], [[Alan Turing]], [[James H. Wilkinson]], [[Alston Scott Householder]], [[George Forsythe]], and [[Heinz Rutishauser]], in order to apply the earliest computers to problems in continuous mathematics, such as ballistics problems and the solutions to systems of PDEs.<ref name=golubhist/> The first serious attempt to minimize computer error in the application of algorithms to real data is John von Neumann and [[Herman Goldstine]]'s work in 1947.<ref>{{cite journal |last1=von Neumann|first1=John |last2=Goldstine |first2=Herman H. |date=1947 |title=Numerical inverting of matrices of high order |url=https://pdfs.semanticscholar.org/503b/f08383134ce107d870982fc50f96b80881f7.pdf |journal=Bulletin of the American Mathematical Society |volume=53 |issue=11 |pages=1021-1099 |access-date=February 17, 2019 }}</ref> The field has grown as technology has increasingly enabled researchers to solve complex problems on extremely large high-precision matrices, and some numerical algorithms have grown in prominence as technologies like parallel computing have made them practical approaches to scientific problems.<ref name=golubhist/>\n\n==Matrix decompositions==\n===Partitioned matrices===\n{{Main|Block matrix}}\nFor many problems in applied linear algebra, it is useful to adopt the perspective of a matrix as being a concatenation of columns vectors. For example, when solving the linear system <math>x = A^{-1}b</math>, rather than understanding ''x'' as the product of <math>A^{-1}</math> with ''b'', it is helpful to think of ''x'' as the vector of [[Coefficient|coefficients]] in the linear expansion of <i>b</i> in the [[Basis (linear algebra)|basis]] formed by the columns of ''A''.<ref name=tb397/>{{rp|8}} Thinking of matrices as a concatenation of columns is also a practical approach for the purposes of matrix algorithms. This is because matrix algorithms frequently contain two nested loops: one over the columns of a matrix ''A'', and another over the rows of ''A''. For example, for matrices <math>A^{m \\times n}</math> and vectors <math>x^{n \\times 1}</math> and <math>y^{m \\times 1}</math>, we could use the column partitioning perspective to compute ''Ax'' + ''y'' as\n\n<syntaxhighlight lang=\"Fortran\">\nfor j = 1:n\n  for i = 1:m\n    y(i) = A(i,j)x(j) + y(i)\n  end\nend\n</syntaxhighlight>\n\n===Singular value decomposition===\n{{Main|singular value decomposition}}\nThe singular value decomposition of a matrix <math>A^{m \\times n}</math> is <math>A = U \\Sigma V^\\ast</math> where ''U'' and ''V'' are [[Unitary matrix|unitary]], and <math>\\Sigma</math> is [[Diagonal matrix|diagonal]]. The diagonal entries of <math>\\Sigma</math> are called the [[singular values]] of ''A''. Because singular values are the square roots of the [[eigenvalues]], there is a tight connection between the singular value decomposition and eigenvalue decompositions. This means that most methods for computing the singular value decomposition are similar to eigenvalue methods;<ref name=tb397/>{{rp|36}} perhaps the most common method involves [[Householder transformation|Householder procedures]].<ref name=tb397/>{{rp|253}}\n\n===QR factorization===\n{{Main|QR decomposition}}\nThe QR factorization of a matrix <math>A^{m \\times n}</math> is a matrix <math>Q^{m \\times m}</math> and a matrix <math>R^{m \\times n}</math> so that ''A = QR'', where ''Q'' is [[Orthogonal matrix|orthogonal]] and ''R'' is [[Triangular matrix|upper triangular]].<ref name=tb397/>{{rp|50}}<ref name = \"gvl96\">{{cite book | last1 = Golub | first = Gene H. | last2 = Van Loan | first2 = Charles F. | location=Baltimore | isbn=0-8018-5413-X | year = 1996 | title = Matrix Computations | publisher = The Johns Hopkins University Press | edition =  3rd}}</ref>{{rp|223}} The two main algorithms for computing QR factorizations are the [[Gram–Schmidt process]] and the [[Householder transformation]], which is the first step in the [[QR algorithm]].\n\n===LU factorization===\n{{Main|LU decomposition}}\nAn LU factorization of a matrix ''A'' consists of a lower triangular matrix ''L'' and an upper triangular matrix ''M'' so that ''A = LU''. The matrix ''U'' is found by an upper triangularization procedure which involves left-multiplying ''A'' by a series of matrices <math>M_1,\\ldots,M_{n-1}</math> to form the product <math>M_{n-1} \\cdots M_1 A = U</math>, so that equivalently <math>L = M_1^{-1} \\cdots M_{n-1}^{-1}</math>.<ref name=tb397/>{{rp|147}}<ref name=gvl96/>{{rp|96}}\n\n===Eigenvalue decomposition===\n{{Main|Eigendecomposition of a matrix}}\nThe eigenvalue decomposition of a matrix <math>A^{m \\times m}</math> is <math>A = X \\Lambda X^{-1}</math>, where the columns of ''X'' are the eigenvectors of ''A'', and <math>\\Lambda</math> is a diagonal matrix the diagonal entries of which are the eigenvalues of ''A''.<ref name=tb397/>{{rp|33}} There is no direct method for finding the eigenvalue decomposition of an arbitrary matrix. Because it is not possible to write a program that finds the exact roots of an arbitrary polynomial in finite time, any general eigenvalue solver must necessarily be iterative.<ref name=tb397/>{{rp|192}}\n\n==Algorithms==\n===Gaussian elimination===\n{{Main|Gaussian elimination}}\nFrom the numerical linear algebra perspective, Gaussian elimination is a procedure for factoring a matrix ''A'' into its ''LU'' factorization, which Gaussian elimination accomplishes by left-multiplying ''A'' by a succession of matrices <math>L_{m-1} \\cdots L_2 L_1 A = U</math> until ''U'' is upper triangular and ''L'' is lower triangular, where <math>L \\equiv L_1^{-1}L_2^{-1} \\cdots L_{m-1}^{-1}</math>.<ref name=tb397/>{{rp|148}} Naive programs for Gaussian elimination are notoriously highly unstable, and produce huge errors when applied to matrices with many significant digits.<ref name=golubhist/> The simplest solution is to introduce [[Pivot element|pivoting]], which produces a modified Gaussian elimination algorithm that is stable.<ref name=tb397/>{{rp|151}}\n\n===Solutions of linear systems===\n{{Main|System of linear equations}}\nNumerical linear algebra characteristically approaches matrices as a concatenation of columns vectors. In order to solve the linear system <math>x = A^{-1}b</math>, the traditional algebraic approach is to understand ''x'' as the product of <math>A^{-1}</math> with ''b''. Numerical linear algebra instead interprets ''x'' as the vector of coefficients of the linear expansion of ''b'' in the basis formed by the columns of ''A''.<ref name=tb397/>{{rp|8}}\n\nMany different decompositions can be used to solve the linear problem, depending on the characteristics of the matrix ''A'' and the vectors ''x'' and ''b'', which may make one factorization much easier to obtain than others. If ''A'' = ''QR'' is a QR factorization of ''A'', then equivalently <math>Rx = Q^\\ast b</math>. This is easy to compute as a matrix factorization.<ref name=tb397/>{{rp|54}} If <math>A = X\\Lambda X^{-1}</math> is an eigendecomposition ''A'', and we seek to find ''b'' so that ''b'' = ''Ax'', with <math>b^\\prime = X^{-1}b</math> and <math>x^\\prime = X^{-1}x</math>, then we have <math>b^\\prime = \\Lambda x^\\prime</math>.<ref name=tb397/>{{rp|33}} This is closely related to the solution to the linear system using the singular value decomposition, because singular values of a matrix are the square roots of its eigenvalues. And if ''A'' = ''LU'' is an ''LU'' factorization of ''A'', then ''Ax'' = ''b'' can be solved using the triangular matrices ''Ly'' = ''b'' and ''Ux'' = ''y''.<ref name=tb397/>{{rp|147}}<ref name=gvl96/>{{rp|99}}\n\n===Least squares optimisation===\n{{Main|Numerical methods for linear least squares}}\nMatrix decompositions suggest a number of ways to solve the linear system ''r'' = ''b'' − ''Ax'' where we seek to minimize ''r'', as in the [[Linear regression|regression problem]]. The QR algorithm solves this problem by first defining ''y'' ''= Ax'', and then computing the reduced QR factorization of ''A'' and rearranging to obtain <math>\\widehat{R}x = \\widehat{Q}^\\ast b</math>. This upper triangular system can then be solved for ''b''. The SVD also suggests an algorithm for obtaining linear least squares. By computing the reduced SVD decomposition <math>A = \\widehat{U}\\widehat{\\Sigma}V^\\ast</math> and then computing the vector <math>\\widehat{U}^\\ast b</math>, we reduce the least squares problem to a simple diagonal system.<ref name=tb397/>{{rp|84}} The fact that least squares solutions can be produced by the QR and SVD factorizations means that, in addition to the classical [[Numerical methods for linear least squares#Inverting the matrix of the normal equations|normal equations]] method for solving least squares problems, these problems can also be solved by methods that include the Gram-Schmidt algorithm and Householder methods.\n\n==Conditioning and stability==\n{{Main|Numerical analysis#Numerical stability and well-posed problems}}\nAllow that a problem is a function <math>f: X \\to Y</math>, where ''X'' is a normed vector space of data and ''Y'' is a normed vector space of solutions. For some data point <math>x \\in X</math>, the problem is said to be ill-conditioned if a small perturbation in ''x'' produces a large change in the value of ''f(x)''. We can quantify this by defining a [[condition number]] which represents how well-conditioned a problem is, defined as\n\n: <math>\\widehat{\\kappa} = \\lim_{\\delta \\to 0} \\sup_{\\| \\delta x \\| \\leq \\delta} \\frac{\\| \\delta f \\|}{\\| \\delta x \\|}</math>\n\nInstability is the tendency of computer algorithms, which depend on [[floating-point arithmetic]], to produce results that differ dramatically from the exact mathematical solution to a problem. When a matrix contains real data with many [[significant digits]], many algorithms for solving problems like linear systems of equation or least squares optimisation may produce highly inaccurate results. Creating stable algorithms for ill-conditioned problems is a central concern in numerical linear algebra. One example is that the stability of householder triangularization makes it a particularly robust solution method for linear systems, whereas the instability of the normal equations method for solving least squares problems is a reason to favour matrix decomposition methods like using the singular value decomposition. Some matrix decomposition methods may be unstable, but have straightforward modifications that make them stable; one example is the unstable Gram–Schmidt, which can easily be changed to produce the stable [[Gram–Schmidt process#Numerical stability|modified Gram–Schmidt]].<ref name=tb397/>{{rp|140}} Another classical problem in numerical linear algebra is the finding that Gaussian elimination is unstable, but becomes stable with the introduction of pivoting.\n\n==Iterative methods==\n{{Main|Iterative methods}}\nThere are two reasons that iterative algorithms are an important part of numerical linear algebra. First, many important numerical problems have no direct solution; in order to find the eigenvalues and eigenvectors of an arbitrary matrix, we can only adopt an iterative approach. Second, noniterative algorithms for an arbitrary <math>m \\times m</math> matrix require <math>O(m^3)</math> time, which is a surprisingly high floor given that matrices contain only <math>m^2</math> numbers. Iterative approaches can take advantage of several features of some matrices to reduce this time. For example, when a matrix is [[Sparse matrix|sparse]], an iterative algorithm can skip many of the steps that a direct approach would necessarily follow, even if they are redundant steps given a highly structured matrix.\n\nThe core of many iterative methods in numerical linear algebra is the projection of a matrix onto a lower dimensional [[Krylov subspace]], which allows features of a high-dimensional matrix to be approximated by iteratively computing the equivalent features of similar matrices starting in a low dimension space and moving to successively higher dimensions. When ''A'' is symmetric and we wish to solve the linear problem ''Ax'' = ''b'', the classical iterative approach is the [[conjugate gradient method]]. If ''A'' is not symmetric, then examples of iterative solutions to the linear problem are the [[generalized minimal residual method]] and [[Conjugate gradient method#Conjugate gradient on the normal equations|CGN]]. If ''A'' is symmetric, then to solve the eigenvalue and eigenvector problem we can use the [[Lanczos algorithm]], and if ''A'' is non-symmetric, then we can use [[Arnoldi iteration]].\n\n==Software==\n{{Main|List of numerical analysis software}}\nSeveral programming languages use numerical linear algebra optimisation techniques and are designed to implement numerical linear algebra algorithms. These languages include [[MATLAB]], [[Analytica (software)|Analytica]], [[Maple]], and [[Mathematica]]. Other programming languages which are not explicitly designed for numerical linear algebra have libraries that provide numerical linear algebra routines and optimisation; [[C (programming language)|C]] and [[Fortran]] have packages like [[Basic Linear Algebra Subprograms]] and [[LAPACK]], [[Python (programming language)|python]] has the library [[pandas (software)|pandas]], and [[Perl]] has the [[Perl Data Language]]. Many numerical linear algebra commands in [[R (programming language)|R]] rely on these more fundamental libraries like [[LAPACK]].<ref>{{cite web |url=https://www.r-bloggers.com/r-and-linear-algebra/ |title=R and Linear Algebra |last=Rickert |first=Joseph |website=R-bloggers |date=August 29, 2013 |access-date=February 17, 2019}}</ref> More libraries can be found on the [[List of numerical libraries]].\n\n== References ==\n{{Reflist|30em}}\n\n==External links==\n{{Commonscat}}\n*[http://www.netlib.org/utk/people/JackDongarra/la-sw.html Freely available software for numerical algebra on the web], composed by Jack Dongarra and Hatem Ltaief, University of Tennessee\n*[http://www.nag.co.uk/numeric/fl/nagdoc_fl24/html/F/fconts.html NAG Library of numerical linear algebra routines]\n\n{{Numerical linear algebra}}\n{{DEFAULTSORT:Numerical Linear Algebra}}\n[[Category:Numerical linear algebra]]\n[[Category:Computational fields of study]]"
    },
    {
      "title": "OpenBLAS",
      "url": "https://en.wikipedia.org/wiki/OpenBLAS",
      "text": "{{Infobox software\n| name = OpenBLAS\n| logo = \n| screenshot = \n| caption = \n| collapsible = \n| author = [[Kazushige Goto]]\n| developer = Zhang Xianyi, Wang Qian, Werner Saar\n| released = {{release date and age|2011|3|22}}\n| latest release version = 0.3.6\n| latest release date = {{release date and age|2019|04|29}}<ref>[https://github.com/xianyi/OpenBLAS/releases Releases · xianyi/OpenBLAS - GitHub]</ref>\n| latest preview version = \n| latest preview date = \n| programming language = \n| operating system = [[Linux]]<br>[[Microsoft Windows]]<br>[[macOS]]<br>[[FreeBSD]]\n| platform = [[x86]]<br>[[x86-64]]<br>[[MIPS architecture|MIPS]]<br>[[ARM architecture|ARM]]<br>ARM64<br>[[IBM POWER instruction set architecture|POWER]]<ref>{{Cite web|url=https://github.com/xianyi/OpenBLAS/blob/develop/Changelog.txt|title=xianyi/OpenBLAS|website=GitHub|language=en|access-date=2018-04-03}}</ref><br>[[IBM Z]]<br>[[SPARC]]\n| size = \n| language = \n| genre = Linear algebra library; implementation of [[Basic Linear Algebra Subprograms|BLAS]]\n| license = [[BSD License]]\n}}\n\nIn [[Computational science|scientific computing]], '''OpenBLAS''' is an [[Open source software|open source]] implementation of the [[BLAS]] (Basic Linear Algebra Subprograms) [[Application programming interface|API]] with many hand-crafted optimizations for specific [[Central processing unit|processor]] types.  It is developed at the Lab of Parallel Software and Computational Science, [[Institute of Software, Chinese Academy of Sciences|ISCAS]].\n\nOpenBLAS adds optimized implementations of linear algebra kernels for several processor architectures, including Intel [[Sandy Bridge]]<ref>{{cite conference |author1=Wang Qian |author2=Zhang Xianyi |author3=Zhang Yunquan |author4=Qing Yi |title=AUGEM: Automatically Generate High Performance Dense Linear Algebra Kernels on x86 CPUs |conference=Int'l Conf. on High Performance Computing, Networking, Storage and Analysis |year=2013 |url=https://xianyi.github.io/paper/augem_SC13.pdf}}</ref>\nand [[Loongson]].<ref>{{cite conference |author1=Zhang Xianyi |author2=Wang Qian|author3=Zhang Yunquan |title=Model-driven Level 3 BLAS Performance Optimization on Loongson 3A Processor |conference=IEEE 18th Int'l Conf. on Parallel and Distributed Systems (ICPADS) |year=2012}}</ref>  It claims to achieve performance comparable to the [[Math_Kernel_Library|Intel MKL]].\n\nOpenBLAS is a fork of [[GotoBLAS]]2, which was created by [[Kazushige Goto]] at the [[Texas Advanced Computing Center]].\n\n==See also==\n* [[Automatically Tuned Linear Algebra Software]] (ATLAS)\n* [[BLIS (software)|BLIS]] (BLAS-like Library Instantiation Software) \n* [[Math Kernel Library|Intel Math Kernel Library]] (MKL)\n\n==References==\n{{reflist|2}}\n\n==External links==\n* {{official website}}\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]\n[[Category:Numerical software]]\n[[Category:Software using the BSD license]]"
    },
    {
      "title": "Portable, Extensible Toolkit for Scientific Computation",
      "url": "https://en.wikipedia.org/wiki/Portable%2C_Extensible_Toolkit_for_Scientific_Computation",
      "text": "{{Infobox software\n| name                   = PETSc\n| latest_release_version = 3.10\n| latest release date    = {{Start date and age|2018|09|12|df=yes/no}}\n| operating_system       = [[Linux]], [[Unix]], [[Mac OS X]], [[Microsoft Windows|Windows]]\n| license                = [[BSD_licenses#2-clause license (\"Simplified BSD License\" or \"FreeBSD License\")|BSD 2-clause license]]\n| language               = C, Python\n| genre                  = Scientific simulation software\n| website                = {{URL|http://www.mcs.anl.gov/petsc/}}\n}}\n\nThe '''Portable, Extensible Toolkit for Scientific Computation''' ('''PETSc''', pronounced PET-see; the S is silent), is a suite of [[data structures]] and [[computer code|routines]] developed by [[Argonne National Laboratory]] for the [[scalability|scalable]] ([[parallel computing|parallel]]) solution of [[scientific computing|scientific applications]] modeled by [[partial differential equations]].  It employs the [[Message Passing Interface]]  (MPI) standard for all message-passing communication.  PETSc is the world’s most widely used parallel numerical software library for [[partial differential equations]] and [[sparse matrix]] computations. PETSc received an R&D 100 Award in 2009.<ref>http://www.anl.gov/sites/anl.gov/files/Argonne_strategic_plan_0.pdf</ref><ref>{{cite web|url=https://www.alcf.anl.gov/articles/petsc-wins-2009-rd-100-award |title=PETSc Wins 2009 R&D 100 Award &#124; Argonne Leadership Computing Facility |publisher=Alcf.anl.gov |date=2009-07-21 |accessdate=2013-05-01}}</ref><ref>{{cite web|author=Thu, 07/30/2009 - 5:23am |url=http://www.rdmag.com/award-winners/2009/07/petsc-release-30-expands-capabilities |title=PETSc Release 3.0 expands capabilities |publisher=Rdmag.com |date=2009-07-30 |accessdate=2013-05-01}}</ref> The PETSc Core Development Group won the SIAM/ACM Prize in Computational Science and Engineering for 2015.<ref>{{cite web|url=http://www.siam.org/prizes/sponsored/cse.php|title=SIAM/ACM Prize in Computational Science and Engineering|publisher=siam.org|date=2015-03-18|accessdate=2015-04-19}}</ref>\n \nPETSc is intended for use in [[scale (computing)|large-scale]] application projects, many ongoing computational science projects  are built around the PETSc [[library (computer science)|libraries]]. Its careful design allows advanced users to have detailed control over the solution process. PETSc includes a large suite of parallel [[linear algebra|linear]] and [[nonlinearity|nonlinear]] [[system of linear equations|equation solvers]] that are easily used in application codes written in [[C (programming language)|C]], [[C++]], [[Fortran]] and now [[Python (programming language)|Python]]. PETSc provides many of the mechanisms needed within parallel application code, such as simple parallel [[matrix (mathematics)|matrix]] and [[vector space|vector]] assembly routines that allow the overlap of [[computer networking|communication]] and [[computation]]. In addition, PETSc includes support for parallel distributed [[array data structure|arrays]] useful for [[finite difference]] methods.<ref>http://www.mcs.anl.gov/petsc/petsc-dev/docs/manual.pdf</ref>\n\n== Components ==\n[[File:Petsc-components.svg|frame|none|Major components of the PETSc software package as of version 3.5]]\n\nPETSc consists of a variety of components consisting of major [[class (computer science)|classes]] and supporting infrastructure.  Users typically interact with [[object (computer science)|objects]] of the highest level classes relevant to their application, essential lower level objects such as vectors, and may customize or extend any others.  All major components of PETSc have an extensible plugin architecture.\n\n== Features and modules ==\n'''PETSc''' provides many features for parallel computation, broken into several modules:\n\n* Index sets, including [[permutations]], for [[Index mapping|index]]ing into vectors, renumbering, etc.\n* Parallel [[vector (geometric)|vectors]]; and [[Matrix (mathematics)|matrices]] (generally [[sparse matrix|sparse]])\n* [[Parallel computing|Scatters]] (handles communicating ghost point information) and [[Parallel computing|gathers]] (the opposite of scatters)\n* Data management for parallel [[Structured grid|structured]] and [[Unstructured grid|unstructured meshes]]\n* Several [[sparse matrix|sparse]] [[computer storage|storage]] [[file format|formats]]\n* Scalable parallel [[preconditioning|preconditioners]], including [[multigrid]] and sparse direct solvers\n* [[Krylov subspace]] methods\n* Parallel nonlinear solvers, such as [[Newton's method]] and nonlinear GMRES\n* Parallel [[Numerical methods for ordinary differential equations|time-stepping]] ([[Ordinary differential equation|ODE]] and [[Differential algebraic equation|DAE]]) solvers\n* Automatic profiling of [[floating point]] and [[Memory (computers)|memory]] usage\n* Consistent [[Interface (computer science)|interface]]\n* Intensive error checking\n* Portable to [[UNIX]], [[Mac OS X]], and [[Microsoft Windows|Windows]]\n\n== Notes ==\n{{reflist}}\n\n== Bibliography ==\n* [http://www.mcs.anl.gov/petsc/petsc-current/docs/manual.pdf ''PETSc Users Manual''], Satish Balay, Shrirang Abhyankar, Mark F. Adams, Jed Brown, Peter Brune, Kris Buschelman, Victor Eijkhout, William D. Gropp, Dinesh Kaushik, Matthew G. Knepley, [[Lois Curfman McInnes]], Karl Rupp, Barry F. Smith, and Hong Zhang, ANL-95/11 Revision 3.5, Argonne National Laboratory, June, 2014.\n* ''Efficient Management of Parallelism in Object Oriented Numerical Software Libraries'', Satish Balay, William D. Gropp, Lois Curfman McInnes, Barry F. Smith, Modern Software Tools in Scientific Computing, ed. Bruaset et al., pp.&nbsp;163–202, 1997.\n* [https://dx.doi.org/10.1016/j.pepi.2007.04.016 ''Numerical simulation of geodynamic processes with the Portable Extensible Toolkit for Scientific Computation'', R.F. Katz, M.G. Knepley, B. Smith, M. Spiegelman, and E.T. Coon, Physics of The Earth and Planetary Interiors, 163, pp. 52-68, 2007.]\n\n== See also ==\n* [[list of numerical libraries]]\n\n== External links ==\n* [http://www.mcs.anl.gov/petsc The Official PETSc web site]\n* [http://www.netlib.org/utk/people/JackDongarra/la-sw.html Jack Dongarra's Linear Algebra Software Page]\n\n[[Category:Numerical libraries]]\n[[Category:Numerical linear algebra]]\n[[Category:Scientific simulation software]]\n[[Category:C++ numerical libraries]]"
    },
    {
      "title": "Power iteration",
      "url": "https://en.wikipedia.org/wiki/Power_iteration",
      "text": "In [[mathematics]], '''power iteration''' (also known as the ''power method'') is an [[eigenvalue algorithm]]: given a [[diagonalizable]] [[matrix (mathematics)|matrix]] <math>A</math>, the algorithm will produce a number <math>\\lambda</math>, which is the greatest (in absolute value) [[eigenvalue]] of <math>A</math>, and a nonzero vector <math>v</math>, the{{clarify|There may be more than one corresponding eigenvector.|date=May 2019}} corresponding [[eigenvector]] of <math>\\lambda</math>, such that <math>Av = \\lambda v</math>.\nThe algorithm is also known as the Von Mises iteration.<ref name=VonMises>[[Richard von Mises]] and H. Pollaczek-Geiringer,\n''Praktische Verfahren der Gleichungsauflösung'', ZAMM - Zeitschrift für Angewandte Mathematik und Mechanik 9, 152-164 (1929).</ref>\n\nPower iteration is a very simple algorithm, but it may converge slowly. It does not compute a [[matrix decomposition]], and hence it can be used when <math>A</math> is a very large [[sparse matrix]].{{Clarify|what's the relation between a matrix decomposition and a matrix sparsity|date=October 2016}}\n\n==The method==\nThe power iteration algorithm starts with a vector <math>b_0</math>, which may be an approximation to the dominant eigenvector or a random vector. The method is described by the [[recurrence relation]]\n:<math> b_{k+1} = \\frac{Ab_k}{\\|Ab_k\\|} </math>\nSo, at every iteration, the vector <math>b_k</math> is multiplied by the matrix <math>A</math> and normalized.\n\nIf we assume <math>A</math> has an eigenvalue that is strictly greater in magnitude than its other eigenvalues and the starting vector <math>b_0</math> has a nonzero component in the direction of an eigenvector associated with the dominant eigenvalue, then a subsequence <math>\\left( b_{k} \\right)</math> converges to an eigenvector associated with the dominant eigenvalue.\n\nWithout the two assumptions above, the sequence <math>\\left( b_{k} \\right)</math> does not necessarily converge. In this sequence,\n: <math>b_k = e^{i \\phi_k} v_1 + r_k</math>,\nwhere <math>v_1</math> is an eigenvector associated with the dominant eigenvalue, and <math> \\| r_{k} \\| \\rightarrow 0</math>. The presence of the term <math>e^{i \\phi_{k}}</math> implies that <math>\\left( b_{k} \\right) </math> does not converge unless <math>e^{i \\phi_{k}} = 1</math>. Under the two assumptions listed above, the sequence <math>\\left( \\mu_{k} \\right)</math> defined by\n: <math>\\mu_{k} = \\frac{b_{k}^{*}Ab_{k}}{b_{k}^{*}b_{k}}</math>\nconverges to the dominant eigenvalue.{{Clarify|what's the relation between this sequence and the power method|date=October 2016}}\n\nOne may compute this with the following algorithm (shown in Python with NumPy):\n\n<source lang=\"python\">\n#!/usr/bin/python\n\nimport numpy as np\n\n\ndef power_iteration(A, num_simulations):\n    # Ideally choose a random vector\n    # To decrease the chance that our vector\n    # Is orthogonal to the eigenvector\n    b_k = np.random.rand(A.shape[1])\n\n    for _ in range(num_simulations):\n        # calculate the matrix-by-vector product Ab\n        b_k1 = np.dot(A, b_k)\n\n        # calculate the norm\n        b_k1_norm = np.linalg.norm(b_k1)\n\n        # re normalize the vector\n        b_k = b_k1 / b_k1_norm\n\n    return b_k\n\npower_iteration(np.array([[0.5, 0.5], [0.2, 0.8]]), 10)\n</source>\nThe vector <math>b_k</math> to an associated eigenvector. Ideally, one should use the [[Rayleigh quotient]] in order to get the associated eigenvalue.\n\nThis algorithm is the one used to calculate such things as the ''Google [[PageRank]]''.\n\nThe method can also be used to calculate the [[spectral radius]] (the largest eigenvalue of a matrix) by computing the Rayleigh quotient\n:<math> \\frac{b_k^\\top A b_k}{b_k^\\top b_k} = \\frac{b_{k+1}^\\top b_k}{b_k^\\top b_k}. </math>\n\n==Analysis==\nLet <math>A</math> be decomposed into its [[Jordan canonical form]]: <math>A=VJV^{-1}</math>, where the first column of <math>V</math> is an eigenvector of <math>A</math> corresponding to the dominant eigenvalue <math>\\lambda_{1}</math>. Since the dominant eigenvalue of <math>A</math> is unique, the first Jordan block of <math>J</math> is the <math>1 \\times 1</math> matrix <math>[\\lambda_1],</math> where <math>\\lambda_{1}</math> is the largest eigenvalue of ''A'' in magnitude. The starting vector <math>b_0</math> can be written as a linear combination of the columns of ''V'':\n\n:<math>b_{0} = c_{1}v_{1} + c_{2}v_{2} + \\cdots + c_{n}v_{n}.</math>\n\nBy assumption, <math>b_{0}</math> has a nonzero component in the direction of the dominant eigenvalue, so <math>c_{1} \\ne 0</math>.\n\nThe computationally useful [[recurrence relation]] for <math>b_{k+1}</math> can be rewritten as:\n\n:<math>b_{k+1}=\\frac{Ab_{k}}{\\|Ab_{k}\\|}=\\frac{A^{k+1}b_{0}}{\\|A^{k+1}b_{0}\\|},</math>\n\nwhere the expression: <math>\\frac{A^{k+1}b_{0}}{\\|A^{k+1}b_{0}\\|}</math> is more amenable to the following analysis.\n\n:<math>\\begin{align}\nb_k &= \\frac{A^{k}b_{0}}{\\| A^{k} b_{0} \\|} \\\\\n    &= \\frac{\\left( VJV^{-1} \\right)^{k} b_{0}}{\\|\\left( VJV^{-1} \\right)^{k}b_{0}\\|} \\\\\n    &= \\frac{ VJ^{k}V^{-1} b_{0}}{\\| V J^{k} V^{-1} b_{0}\\|} \\\\\n    &= \\frac{ VJ^{k}V^{-1} \\left( c_{1}v_{1} + c_{2}v_{2} + \\cdots + c_{n}v_{n} \\right)}{\\| V J^{k} V^{-1} \\left( c_{1}v_{1} + c_{2}v_{2} + \\cdots + c_{n}v_{n} \\right)\\|} \\\\\n    &= \\frac{ VJ^{k}\\left( c_{1}e_{1} + c_{2}e_{2} + \\cdots + c_{n}e_{n} \\right)}{\\| V J^{k} \\left( c_{1}e_{1} + c_{2}e_{2} + \\cdots + c_{n}e_{n} \\right) \\|} \\\\\n    &= \\left( \\frac{\\lambda_{1}}{|\\lambda_{1}|} \\right)^{k} \\frac{c_{1}}{|c_{1}|} \\frac{ v_{1} + \\frac{1}{c_{1}} V \\left( \\frac{1}{\\lambda_1} J \\right)^{k} \\left( c_{2}e_{2} +  \\cdots + c_{n}e_{n} \\right)}{ \\left \\| v_{1} + \\frac{1}{c_{1}} V \\left( \\frac{1}{\\lambda_1} J \\right)^{k} \\left( c_{2}e_{2} +  \\cdots + c_{n}e_{n} \\right) \\right \\| }\n\\end{align}</math>\n\nThe expression above simplifies as <math>k \\to \\infty </math>\n\n:<math>\\left( \\frac{1}{\\lambda_{1}} J \\right)^{k} = \n\\begin{bmatrix}\n[1] & & & & \\\\\n& \\left( \\frac{1}{\\lambda_{1}} J_{2} \\right)^{k}& & & \\\\\n& & \\ddots & \\\\\n& & & \\left( \\frac{1}{\\lambda_{1}} J_{m} \\right)^{k} \\\\\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\n1 & & & & \\\\\n& 0 & & & \\\\\n& & \\ddots & \\\\\n& & & 0 \\\\\n\\end{bmatrix} \\quad \\text{as} \\quad k \\to \\infty.</math>\n\nThe limit follows from the fact that the eigenvalue of <math> \\frac{1}{\\lambda_{1}} J_{i} </math> is less than 1 in magnitude, so\n\n:<math>\\left( \\frac{1}{\\lambda_{1}} J_{i} \\right)^{k} \\to 0 \\quad \\text{as} \\quad k \\to \\infty.</math>\n\nIt follows that:\n\n:<math>\\frac{1}{c_{1}} V \\left( \\frac{1}{\\lambda_1} J \\right)^{k} \\left( c_{2}e_{2} + \\cdots + c_{n}e_{n} \\right) \\to 0 \\quad \\text{as} \\quad k \\to \\infty</math>\n\nUsing this fact, <math>b_{k}</math> can be written in a form that emphasizes its relationship with <math>v_{1}</math> when ''k'' is large:\n\n:<math>\\begin{align}\nb_k &= \\left( \\frac{\\lambda_{1}}{|\\lambda_{1}|} \\right)^{k} \\frac{c_{1}}{|c_{1}|} \\frac{v_{1} + \\frac{1}{c_{1}} V \\left( \\frac{1}{\\lambda_1} J \\right)^{k} \\left( c_{2}e_{2} +  \\cdots + c_{n}e_{n} \\right)}{\\left \\| v_{1} + \\frac{1}{c_{1}} V \\left( \\frac{1}{\\lambda_1} J \\right)^{k} \\left( c_{2}e_{2} +  \\cdots + c_{n}e_{n} \\right) \\right \\| } \\\\[6pt]\n    &= e^{i \\phi_{k}} \\frac{c_{1}}{|c_{1}|} \\frac{v_{1}}{\\|v_{1}\\|} + r_{k}\n\\end{align}</math>\n\nwhere <math>e^{i \\phi_{k}} = \\left( \\lambda_{1} / |\\lambda_{1}| \\right)^{k} </math> and <math> \\| r_{k} \\| \\to 0 </math> as <math>k \\to \\infty </math> \n\nThe sequence <math> \\left( b_{k} \\right)</math> is bounded, so it contains a convergent subsequence. Note that the eigenvector corresponding to the dominant eigenvalue is only unique up to a scalar, so although the sequence <math>\\left(b_{k}\\right)</math> may not converge,\n<math>b_{k}</math> is nearly an eigenvector of ''A'' for large ''k''.\n\nAlternatively, if ''A'' is [[diagonalizable]], then the following proof yields the same result\n\nLet λ<sub>1</sub>, λ<sub>2</sub>, ..., λ<sub>''m''</sub> be the <var>m</var> eigenvalues (counted with multiplicity) of <var>A</var> and let ''v''<sub>1</sub>, ''v''<sub>2</sub>, ..., ''v''<sub>''m''</sub> be the corresponding eigenvectors.  Suppose that <math>\\lambda_1</math> is the dominant eigenvalue, so that <math>|\\lambda_1| > |\\lambda_j|</math> for <math>j>1</math>.\n\nThe initial vector <math>b_0</math> can be written:\n\n:<math>b_0 = c_{1}v_{1} + c_{2}v_{2} + \\cdots + c_{m}v_{m}.</math>\n\nIf <math>b_0</math> is chosen randomly (with uniform probability), then ''c''<sub>1</sub> ≠ 0 with [[Almost surely|probability 1]].  Now,\n\n:<math>\\begin{align}\nA^{k}b_0 &= c_{1}A^{k}v_{1} + c_{2}A^{k}v_{2} + \\cdots + c_{m}A^{k}v_{m} \\\\\n&= c_{1}\\lambda_{1}^{k}v_{1} + c_{2}\\lambda_{2}^{k}v_{2} + \\cdots + c_{m}\\lambda_{m}^{k}v_{m} \\\\\n&= c_{1}\\lambda_{1}^{k} \\left( v_{1} + \\frac{c_{2}}{c_{1}}\\left(\\frac{\\lambda_{2}}{\\lambda_{1}}\\right)^{k}v_{2} + \\cdots + \\frac{c_{m}}{c_{1}}\\left(\\frac{\\lambda_{m}}{\\lambda_{1}}\\right)^{k}v_{m}\\right) \\\\\n&\\to c_{1}\\lambda_{1}^{k} v_1 && \\left |\\frac{\\lambda_j}{\\lambda_1} \\right | < 1 \\text{ for } j>1\n\\end{align}</math>\n\nOn the other hand:\n\n:<math> b_k = \\frac{A^k b_0}{\\|A^kb_0\\|}. </math>\n\nTherefore, <math>b_k</math> converges to (a multiple of) the eigenvector <math>v_1</math>. The convergence is [[geometric sequence|geometric]], with ratio\n\n:<math> \\left| \\frac{\\lambda_2}{\\lambda_1} \\right|, </math>\n\nwhere <math>\\lambda_2</math> denotes the second dominant eigenvalue. Thus, the method converges slowly if there is an eigenvalue close in magnitude to the dominant eigenvalue.\n\n==Applications==\nAlthough the power iteration method approximates only one eigenvalue of a matrix, it remains useful for certain [[computational problem]]s. For instance, [[Google]] uses it to calculate the [[PageRank]] of documents in their search engine,<ref>{{cite news|author=[[Ilse Ipsen|Ipsen, Ilse]], and Rebecca M. Wills| url=http://www4.ncsu.edu/~ipsen/ps/slides_imacs.pdf|title=7th IMACS International Symposium on Iterative Methods in Scientific Computing|location=Fields Institute, Toronto, Canada|date=5–8 May 2005}}</ref> and [[Twitter]] uses it to show users recommendations of who to follow.<ref name=\"twitterwtf\">Pankaj Gupta, Ashish Goel, Jimmy Lin, Aneesh Sharma, Dong Wang, and Reza Bosagh Zadeh [http://dl.acm.org/citation.cfm?id=2488433 WTF: The who-to-follow system at Twitter], Proceedings of the 22nd international conference on World Wide Web</ref> The power iteration method is especially suitable for [[Sparse matrix|sparse matrices]], such as the web matrix, or as the [[Matrix-free_methods|matrix-free method]] that does not require storing the coefficient matrix <math>A</math> explicitly, but can instead access a function evaluating matrix-vector products <math>Ax</math>. For non-symmetric matrices that are [[Condition number#matrices|well-conditioned]] the power iteration method can outperform more complex [[Arnoldi iteration]]. For symmetric matrices, the power iteration method is rarely used, since its convergence speed can be easily increased without sacrificing the small cost per iteration; see, e.g., [[Lanczos iteration]] and [[LOBPCG]].  \n\nSome of the more advanced eigenvalue algorithms can be understood as variations of the power iteration. For instance, the [[inverse iteration]] method applies power iteration to the matrix <math>A^{-1}</math>. Other algorithms look at the whole subspace generated by the vectors <math>b_k</math>. This subspace is known as the [[Krylov subspace]]. It can be computed by [[Arnoldi iteration]] or [[Lanczos iteration]].\n\n==See also==\n* [[Rayleigh quotient iteration]]\n* [[Inverse iteration]]\n\n==References==\n{{Reflist}}\n\n\n{{Numerical linear algebra}}\n{{Use dmy dates|date=September 2010}}\n\n{{DEFAULTSORT:Power Iteration}}\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "Preconditioner",
      "url": "https://en.wikipedia.org/wiki/Preconditioner",
      "text": "{{redirect|Preconditioning}}\n{{no footnotes|date=February 2013}}\nIn [[mathematics]], '''preconditioning''' is the application of a transformation, called the '''preconditioner''',  that conditions a given problem into a form that is more suitable for numerical solving methods. Preconditioning is typically related to reducing a [[condition number]] of the problem. The preconditioned problem is then usually solved by an [[iterative method]].\n\n== Preconditioning for linear systems ==\n\nIn [[linear algebra]] and [[numerical analysis]], a '''preconditioner''' <math>P</math> of a matrix <math>A</math> is a matrix such that <math> P^{-1}A</math> has a smaller [[condition number]] than <math>A</math>. It is also common to call <math>T=P^{-1}</math> the preconditioner, rather than <math>P</math>, since <math>P</math> itself is rarely explicitly available. In modern preconditioning, the application of  <math>T=P^{-1}</math>, i.e., multiplication of a column vector, or a block of column vectors, by <math>T=P^{-1}</math>, is commonly performed by rather sophisticated computer software packages in a [[Matrix-free methods|matrix-free fashion]], i.e., where neither <math>P</math>, nor  <math>T=P^{-1}</math> (and often not even <math>A</math>) are explicitly available in a matrix form. \n\nPreconditioners are useful in [[iterative methods]] to solve a linear system  <math>Ax=b</math> for <math>x</math> since the [[rate of convergence]] for most iterative linear solvers increases because the [[condition number]] of a matrix decreases as a result of preconditioning. Preconditioned iterative solvers typically outperform direct solvers, e.g., [[Gaussian elimination]], for large, especially for [[sparse matrix|sparse]], matrices. Iterative solvers can be used as [[matrix-free methods]], i.e. become the only choice if the coefficient matrix <math>A</math> is not stored explicitly, but is accessed by evaluating matrix-vector products.\n\n=== Description ===\n\nInstead of solving the original linear system above, one may solve the right preconditioned system:\n\n: <math> AP^{-1}Px = b</math>\n\nvia solving \n\n: <math>AP^{-1}y=b</math>\n\nfor <math>y</math> and \n\n: <math>Px=y</math>\n\nfor <math>x</math>.\n\nAlternatively, one may solve the left preconditioned system:\n\n: <math> P^{-1}(Ax-b)=0 .</math>\n\nBoth systems give the same solution as the original system as long as the preconditioner matrix <math>P</math> is [[nonsingular]]. The left preconditioning is more common.\n\nThe goal of this preconditioned system is to reduce the [[condition number]] of the left or right preconditioned system matrix <math>P^{-1}A</math> or <math>AP^{-1},</math> respectively. The preconditioned matrix <math>P^{-1}A</math> or <math>AP^{-1}</math> is almost never explicitly formed. Only the action of applying the preconditioner solve operation  <math>P^{-1}</math> to a given vector need to be computed in iterative methods.\n\nTypically there is a trade-off in the choice of <math>P</math>.  Since the operator <math>P^{-1}</math> must be applied at each step of the iterative linear solver, it should have a small cost (computing time) of applying the  <math>P^{-1}</math> operation.  The cheapest preconditioner would therefore be <math>P=I</math> since then <math>P^{-1}=I.</math> Clearly, this results in the original linear system and the preconditioner does nothing.  At the other extreme, the choice  <math>P=A</math> gives <math>P^{-1}A = AP^{-1} = I,</math> which has optimal [[condition number]] of 1, requiring a single iteration for convergence; however in this case <math>P^{-1}=A^{-1},</math> and applying the preconditioner is as difficult as solving the original system.  One therefore chooses  <math>P</math>  as somewhere between these two extremes, in an attempt to achieve a minimal number of linear iterations while keeping the operator  <math>P^{-1}</math>  as simple as possible.  Some examples of typical preconditioning approaches are detailed below.\n\n===Preconditioned iterative methods===\nPreconditioned iterative methods for <math>Ax-b=0</math> are, in most cases, mathematically equivalent to standard iterative methods applied to the preconditioned system <math>P^{-1}(Ax-b)=0.</math> For example, the standard [[Richardson iteration]] for solving <math>Ax-b=0</math> is\n\n:<math>\\mathbf{x}_{n+1}=\\mathbf{x}_n-\\gamma_n (A\\mathbf{x}_n-\\mathbf{b}),\\ n \\ge 0.</math>\n\nApplied to the preconditioned system <math>P^{-1}(Ax-b)=0,</math> it turns into a preconditioned method\n\n:<math>\\mathbf{x}_{n+1}=\\mathbf{x}_n-\\gamma_n P^{-1}(A\\mathbf{x}_n-\\mathbf{b}),\\ n \\ge 0.</math>\n\nExamples of popular preconditioned [[iterative methods]] for linear systems include the [[preconditioned conjugate gradient method]], the [[biconjugate gradient method]], and [[generalized minimal residual method]]. Iterative methods, which use scalar products to compute the iterative parameters, require corresponding changes in the scalar product together with substituting <math>P^{-1}(Ax-b)=0</math> for <math>Ax-b=0.</math>\n\n==== Linear preconditioning ====\nLet a [[Iterative_method#Stationary_iterative_methods|linear iterative method]] be given by the matrix splitting <math> A=M-N </math>, and iteration matrix <math> C=I-M^{-1}A </math>.\n\nAssume the following\n* the system matrix <math> A </math> is [[Symmetric_matrix|symmetric]] [[Positive-definite_matrix|positive-definite]]\n* the splitting matrix <math> M </math> is [[Symmetric_matrix|symmetric]] [[Positive-definite_matrix|positive-definite]]\n* the linear iterative method is convergent: <math> \\rho(C)<1 </math>.\nThen, the reduction of the system's [[condition number]] <math> \\kappa(M^{-1}A) </math> can be bounded from above by\n:<math>\n  \\kappa(M^{-1}A) \\leq \\frac{1+\\rho(C)}{1-\\rho(C)} \\,.\n</math>\n\n===Geometric interpretation===\nFor a [[Symmetric matrix|symmetric]] [[Positive-definite matrix|positive definite]] matrix <math>A</math> the preconditioner <math>P</math> is typically chosen to be symmetric positive definite as well. The preconditioned operator <math>P^{-1}A</math> is then also symmetric positive definite, but with respect to the <math>P</math>-based [[scalar product]]. In this case, the desired effect in applying a preconditioner is to make the [[quadratic form]] of the preconditioned operator <math>P^{-1}A</math> with respect to the <math>P</math>-based [[scalar product]] to be nearly spherical [http://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf].\n\n=== Variable and non-linear preconditioning ===\nDenoting <math>T=P^{-1}</math>, we highlight that preconditioning is practically implemented as multiplying some vector <math>r</math> by <math>T</math>, i.e., computing the product <math>Tr.</math> In many applications, <math>T</math> is not given as a matrix, but rather as an operator <math>T(r)</math> acting on the vector <math>r</math>. Some popular preconditioners, however, change with <math>r</math> and the dependence on <math>r</math> may not be linear. Typical examples involve using non-linear [[iterative methods]], e.g., the [[conjugate gradient method]], as a part of the preconditioner construction. Such preconditioners may be practically very efficient, however, their behavior is hard to predict theoretically.\n\n===Spectrally equivalent preconditioning===\nThe most common use of preconditioning is for iterative solution of linear systems resulting from approximations of [[partial differential equations]]. The better the approximation quality, the larger the matrix size is. In such a case, the goal of optimal preconditioning is, on the one side, to make the spectral condition number of <math> P^{-1}A</math> to be bounded from above by a constant independent of the matrix size, which is called ''spectrally equivalent'' preconditioning by [[Evgenii Georgievich D'yakonov|D'yakonov]]. On the other hand, the cost of application of the  <math> P^{-1}</math> should ideally be proportional (also independent of the matrix size) to the cost of multiplication of  <math>A</math> by a vector.\n\n===Examples===\n\n====Jacobi (or diagonal) preconditioner====\nThe '''Jacobi preconditioner''' is one of the simplest forms of preconditioning, in which the preconditioner is chosen to be the diagonal of the matrix <math> P = \\mathrm{diag}(A).</math> Assuming <math>A_{ii} \\neq 0, \\forall i </math>, we get <math>P^{-1}_{ij} = \\frac{\\delta_{ij}}{A_{ij}}.</math> It is efficient for [[Diagonally dominant matrix|diagonally dominant matrices]] <math> A</math>.\n\n====SPAI====\nThe '''Sparse Approximate Inverse''' preconditioner minimises <math>\\|AT-I\\|_F,</math> where <math>\\|\\cdot\\|_F</math> is the [[Frobenius norm]] and <math>T = P^{-1}</math> is from some suitably constrained set of [[sparse matrices]]. Under the Frobenius norm, this reduces to solving numerous independent least-squares problems (one for every column). The entries in <math>T</math> must be restricted to some sparsity pattern or the problem remains as difficult and time-consuming as finding the exact inverse of <math>A</math>. The method was introduced by M.J. Grote and T. Huckle together with an approach to selecting sparsity patterns.<ref>M.J. Grote & T. Huckle (1997) \"Parallel pre-conditioning with sparse approximate inverses\", [[SIAM_Journal_on_Scientific_Computing]] 18:838–53</ref>\n\n==== Other preconditioners ====\n* [[Incomplete Cholesky factorization]]\n* [[Incomplete LU factorization]]\n* [[Successive over-relaxation]]\n** [[Symmetric successive over-relaxation]]\n* [[Multigrid_method#Multigrid_preconditioning|Multigrid preconditioning]]\n\n===External links===\n* [http://www.math-linux.com/spip.php?article55 Preconditioned Conjugate Gradient] – math-linux.com\n* [http://www.netlib.org/linalg/html_templates/Templates.html Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods]\n\n== Preconditioning for eigenvalue problems ==\nEigenvalue problems can be framed in several alternative ways, each leading to its own preconditioning. The traditional preconditioning is based on the so-called ''spectral transformations.'' Knowing (approximately) the targeted eigenvalue, one can compute the corresponding eigenvector by solving the related homogeneous linear system, thus allowing to use preconditioning for linear system. Finally, formulating the eigenvalue problem as optimization of the [[Rayleigh quotient]] brings preconditioned optimization techniques to the scene.<ref name=\"K98\">{{Cite journal| title = Preconditioned eigensolvers - an oxymoron?| journal = [[Electronic Transactions on Numerical Analysis]]| volume = 7| | pages = 104-123| year = 1998| last1 = Knyazev | first1 = Andrew V. }}</ref>\n\n===Spectral transformations===\nBy analogy with linear systems, for an [[eigenvalue]] problem <math> Ax = \\lambda x</math> one may be tempted to replace the matrix <math>A</math> with the matrix <math>P^{-1}A</math> using a preconditioner <math>P</math>. However, this makes sense only if the seeking [[eigenvectors]] of  <math>A</math> and <math>P^{-1}A</math> are the same. This is the case for spectral transformations. \n\nThe most popular spectral transformation is the so-called ''shift-and-invert'' transformation, where for a given scalar <math>\\alpha</math>, called the ''shift'', the original eigenvalue problem <math> Ax = \\lambda x</math> is replaced with the shift-and-invert problem <math> (A-\\alpha I)^{-1}x = \\mu x</math>. The eigenvectors are preserved, and one can solve the shift-and-invert problem by an iterative solver, e.g., the [[power iteration]]. This gives the [[Inverse iteration]], which normally converges to the eigenvector, corresponding to the eigenvalue closest to the shift <math>\\alpha</math>. The [[Rayleigh quotient iteration]] is a shift-and-invert method with a variable shift. \n\nSpectral transformations are specific for eigenvalue problems and have no analogs for linear systems. They require accurate numerical calculation of the transformation involved, which becomes the main bottleneck for large problems.\n\n===General preconditioning===\nTo make a close connection to linear systems, let us suppose that the targeted eigenvalue <math>\\lambda_\\star</math> is known (approximately). Then one can compute the corresponding eigenvector from the homogeneous linear system <math>(A-\\lambda_\\star I)x=0</math>. Using the concept of left preconditioning for linear systems, we obtain <math>T(A-\\lambda_\\star I)x=0</math>, where  <math>T</math> is the preconditioner, which we can try to solve using the [[Richardson iteration]]\n\n:<math>\\mathbf{x}_{n+1}=\\mathbf{x}_n-\\gamma_n T(A-\\lambda_\\star I))\\mathbf{x}_n,\\ n \\ge 0.</math>\n\n====The ''ideal'' preconditioning<ref name=\"K98\" />====\nThe [[Moore–Penrose pseudoinverse]] <math>T=(A-\\lambda_\\star I)^+</math> is the preconditioner, which makes the [[Richardson iteration]] above converge in one step with <math>\\gamma_n=1</math>, since <math>I-(A-\\lambda_\\star I)^+(A-\\lambda_\\star I)</math>, denoted by <math>P_\\star</math>, is the orthogonal projector on the eigenspace, corresponding to <math>\\lambda_\\star</math>. The choice  <math>T=(A-\\lambda_\\star I)^+</math> is impractical for three independent reasons. First, <math>\\lambda_\\star</math> is actually not known, although it can be replaced with its approximation <math>\\tilde\\lambda_\\star</math>. Second, the exact [[Moore–Penrose pseudoinverse]] requires the knowledge of the eigenvector, which we are trying to find. This can be somewhat circumvented by the use of the [[Jacobi–Davidson preconditioner]] <math>T=(I-\\tilde P_\\star)(A-\\tilde\\lambda_\\star I)^{-1}(I-\\tilde P_\\star)</math>, where <math>\\tilde P_\\star</math> approximates <math>P_\\star</math>. Last, but not least, this approach requires accurate numerical solution of linear system with the system matrix <math>(A-\\tilde\\lambda_\\star I)</math>, which becomes as expensive for large problems as the shift-and-invert method above. If the solution is not accurate enough, step two may be redundant.\n\n====Practical preconditioning====\nLet us first replace the theoretical value <math>\\lambda_\\star</math> in the [[Richardson iteration]] above with its current approximation <math>\\lambda_n</math> to obtain a practical algorithm\n\n:<math>\\mathbf{x}_{n+1}=\\mathbf{x}_n-\\gamma_n T(A-\\lambda_n I)\\mathbf{x}_n,\\ n \\ge 0.</math>\n\nA popular choice is <math>\\lambda_n=\\rho(x_n)</math> using the [[Rayleigh quotient]] function <math>\\rho(\\cdot)</math>. Practical preconditioning may be as trivial as just using <math>T=(diag(A))^{-1}</math> or <math>T=(diag(A-\\lambda_n I))^{-1}.</math> For some classes of eigenvalue problems the efficiency of <math>T\\approx A^{-1}</math> has been demonstrated, both numerically and theoretically. The choice <math>T\\approx A^{-1}</math> allows one to easily utilize for eigenvalue problems the vast variety of preconditioners developed for linear systems. \n\nDue to the changing value <math>\\lambda_n</math>, a comprehensive theoretical convergence analysis is much more difficult, compared to the linear systems case, even for the simplest methods, such as the [[Richardson iteration]].\n\n===External links===\n* [http://www.cs.ucdavis.edu/~bai/ET/contents.html Templates for the Solution of Algebraic Eigenvalue Problems: a Practical Guide]\n\n== Preconditioning in optimization ==\n[[File:gradient descent.png|thumb|right|350px|Illustration of gradient descent]]\nIn [[optimization (mathematics)|optimization]], preconditioning is typically used to accelerate [[First-order approximation|first-order]] [[optimization (mathematics)|optimization]] [[algorithms]].\n\n=== Description ===\nFor example, to find a [[local minimum]] of a real-valued function <math>F(\\mathbf{x})</math> using [[gradient descent]], one takes steps proportional to the ''negative'' of the [[gradient]] <math>-\\nabla F(\\mathbf{a})</math>  (or of the approximate gradient) of the function at the current point:\n\n:<math>\\mathbf{x}_{n+1}=\\mathbf{x}_n-\\gamma_n \\nabla F(\\mathbf{x}_n),\\ n \\ge 0.</math>\n\nThe preconditioner is applied to the gradient: \n\n:<math>\\mathbf{x}_{n+1}=\\mathbf{x}_n-\\gamma_n P^{-1} \\nabla F(\\mathbf{x}_n),\\ n \\ge 0.</math>\n\nPreconditioning here can be viewed as changing the geometry of the vector space with the goal to make the level sets look like circles. In this case the preconditioned gradient aims closer to the point of the extrema as on the figure, which speeds up the convergence.\n\n===Connection to linear systems===\nThe minimum of a quadratic function \n\n:<math>F(\\mathbf{x})= \\frac{1}{2}\\mathbf{x}^TA\\mathbf{x}-\\mathbf{x}^T\\mathbf{b}</math>,\n\nwhere <math>\\mathbf{x}</math> and <math>\\mathbf{b}</math> are real column-vectors and <math>A</math> is a real [[Symmetric matrix|symmetric]] [[positive-definite matrix]], is exactly the solution of the linear equation <math>A\\mathbf{x}=\\mathbf{b}</math>. Since <math>\\nabla F(\\mathbf{x})=A\\mathbf{x}-\\mathbf{b}</math>, the preconditioned [[gradient descent]] method of minimizing  <math>F(\\mathbf{x})</math> is \n\n:<math>\\mathbf{x}_{n+1}=\\mathbf{x}_n-\\gamma_n P^{-1}(A\\mathbf{x}_n-\\mathbf{b}),\\ n \\ge 0.</math>\n\nThis is the preconditioned [[Richardson iteration]] for solving a [[system of linear equations]].\n\n===Connection to eigenvalue problems===\n\nThe minimum of the [[Rayleigh quotient]]\n\n:<math>\\rho(\\mathbf{x})= \\frac{\\mathbf{x}^TA\\mathbf{x}}{\\mathbf{x}^T\\mathbf{x}},</math>\n\nwhere <math>\\mathbf{x}</math> is a real non-zero column-vector and <math>A</math> is a real [[Symmetric matrix|symmetric]] [[positive-definite matrix]], is the smallest [[eigenvalue]] of <math>A</math>, while the minimizer is the corresponding [[eigenvector]]. Since <math>\\nabla \\rho(\\mathbf{x})</math> is proportional to <math>A\\mathbf{x}-\\rho(\\mathbf{x})\\mathbf{x}</math>, the preconditioned [[gradient descent]] method of minimizing  <math>\\rho(\\mathbf{x})</math> is \n\n:<math>\\mathbf{x}_{n+1}=\\mathbf{x}_n-\\gamma_n P^{-1}(A\\mathbf{x}_n-\\rho(\\mathbf{x_n})\\mathbf{x_n}),\\ n \\ge 0.</math>\n\nThis is an analog of preconditioned [[Richardson iteration]] for solving eigenvalue problems.\n\n=== Variable preconditioning ===\nIn many cases, it may be beneficial to change the preconditioner at some or even every step of an [[iterative algorithm]] in order to accommodate for a changing shape of the level sets, as in \n\n:<math>\\mathbf{x}_{n+1}=\\mathbf{x}_n-\\gamma_n P_n^{-1} \\nabla F(\\mathbf{x}_n),\\ n \\ge 0.</math>\n\nOne should have in mind, however, that constructing an efficient preconditioner is very often computationally expensive. The increased cost of updating the preconditioner can easily override the positive effect of faster convergence.\n\n==References==\n{{Reflist}}\n*{{cite book\n |title= Iterative Solution Methods\n |last=  Axelsson|first= Owe  |year= 1996 |publisher= Cambridge University Press |isbn= 978-0-521-55569-2 |pages=6722}}\n*{{cite book\n |title= Optimization in solving elliptic problems \n |last=  D'yakonov |first= E. G. |year= 1996 |publisher= CRC-Press |isbn= 978-0-8493-2872-5 |pages=592}}\n* [[Yousef Saad]] & [[Henk van der Vorst]] (2001) \"Iterative solution of linear systems in the 20th century\", §8 Preconditioning methods, pp 193–8 of ''Numerical Analysis: Historical Developments in the 20th Century'', C. Brezinski & L Wuytack editors, [[Elsevier Science Publishers]] {{ISBN|0-444-50617-9}}.\n*{{Cite book\n  | first = H. A.|last= van der Vorst\n  | title = Iterative Krylov Methods for Large Linear systems\n  | publisher = Cambridge University Press, Cambridge\n  | year = 2003\n  | isbn = 0-521-81828-1\n}}\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "QR algorithm",
      "url": "https://en.wikipedia.org/wiki/QR_algorithm",
      "text": "In [[numerical linear algebra]], the '''QR algorithm''' is an [[eigenvalue algorithm]]: that is, a procedure to calculate the [[eigenvalues and eigenvectors]] of a [[Matrix (mathematics)|matrix]]. The QR algorithm was developed in the late 1950s by [[John G. F. Francis]] and by [[Vera N. Kublanovskaya]], working independently.<ref>J.G.F. Francis, \"The QR Transformation, I\", ''[[The Computer Journal]]'', '''4'''(3), pages 265&ndash;271 (1961, received October 1959). [[doi:10.1093/comjnl/4.3.265]]</ref><ref>{{cite journal |first=J. G. F. |last=Francis |title=The QR Transformation, II |journal=The Computer Journal |volume=4 |issue=4 |pages=332–345 |year=1962 |doi=10.1093/comjnl/4.4.332 }}</ref><ref>\nVera N. Kublanovskaya, \"On some algorithms for the solution of the complete eigenvalue problem,\" ''USSR Computational Mathematics and Mathematical Physics'', vol. 1, no. 3, pages 637–657 (1963, received Feb 1961). Also published in: ''Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki'', vol.1, no. 4, pages 555–570 (1961). [[doi:10.1016/0041-5553(63)90168-X]]</ref>  The basic idea is to perform a [[QR decomposition]], writing the matrix as a product of an [[orthogonal matrix]] and an upper [[triangular matrix]], multiply the factors in the reverse order, and iterate.\n\n==The practical QR algorithm==\nFormally, let ''A'' be a real matrix of which we want to compute the eigenvalues, and let ''A''<sub>0</sub>:=''A''. At the ''k''-th step (starting with ''k'' = 0), we compute the [[QR decomposition]] ''A''<sub>''k''</sub>=''Q''<sub>''k''</sub>''R''<sub>''k''</sub> where ''Q''<sub>''k''</sub> is an [[orthogonal matrix]] (i.e., ''Q''<sup>''T''</sup> = ''Q''<sup>−1</sup>) and ''R''<sub>''k''</sub> is an upper triangular matrix. We then form ''A''<sub>''k''+1</sub> = ''R''<sub>''k''</sub>''Q''<sub>''k''</sub>. Note that\n:<math> A_{k+1} = R_k Q_k = Q_k^{-1} Q_k R_k Q_k = Q_k^{-1} A_k Q_k = Q_k^{\\mathsf{T}} A_k Q_k, </math>\nso all the ''A''<sub>''k''</sub> are [[Similar matrix|similar]] and hence they have the same eigenvalues. The algorithm is [[numerical stability|numerically stable]] because it proceeds by ''orthogonal'' similarity transforms.\n\nUnder certain conditions,<ref name=\"golubvanloan\">{{cite book |last=Golub |first=G. H. |last2=Van Loan |first2=C. F. |title=Matrix Computations |edition=3rd |publisher=Johns Hopkins University Press |location=Baltimore |year=1996 |isbn=0-8018-5414-8 }}</ref> the matrices ''A''<sub>''k''</sub> converge to a triangular matrix, the [[Schur form]] of ''A''. The eigenvalues of a triangular matrix are listed on the diagonal, and the eigenvalue problem is solved. In testing for convergence it is impractical to require exact zeros, but the [[Gershgorin circle theorem]] provides a bound on the error.\n\nIn this crude form the iterations are relatively expensive. This can be mitigated by first bringing the matrix ''A'' to upper [[Hessenberg form]] (which costs <math>\\begin{matrix}\\frac{10}{3}\\end{matrix} n^3 + \\mathcal{O}(n^2)</math> arithmetic operations using a technique based on [[Householder transformation|Householder reduction]]), with a finite sequence of orthogonal similarity transforms, somewhat like a two-sided QR decomposition.<ref name=Demmel>{{cite book |first=James W. |last=Demmel |authorlink=James W. Demmel |title=Applied Numerical Linear Algebra |location= |publisher=SIAM |year=1997 }}</ref><ref name=Trefethen>{{cite book |first=Lloyd N. |last=Trefethen |authorlink=Lloyd N. Trefethen |first2=David |last2=Bau |title=Numerical Linear Algebra |location= |publisher=SIAM |year=1997 }}</ref>  (For QR decomposition, the Householder reflectors are multiplied only on the left, but for the Hessenberg case they are multiplied on both left and right.) Determining the QR decomposition of an upper Hessenberg matrix costs <math>6 n^2 + \\mathcal{O}(n)</math> arithmetic operations.  Moreover, because the Hessenberg form is already nearly upper-triangular (it has just one nonzero entry below each diagonal), using it as a starting point reduces the number of steps required for convergence of the QR algorithm.\n\nIf the original matrix is [[symmetric matrix|symmetric]], then the upper Hessenberg matrix is also symmetric and thus [[tridiagonal matrix|tridiagonal]], and so are all the ''A''<sub>''k''</sub>. This procedure costs <math>\\begin{matrix}\\frac{4}{3}\\end{matrix} n^3 + \\mathcal{O}(n^2)</math> arithmetic operations using  a technique based on Householder reduction.<ref name=Demmel/><ref name=Trefethen/> Determining the QR decomposition of a symmetric tridiagonal matrix costs <math>\\mathcal{O}(n)</math> operations.<ref>{{cite journal |first=James M. |last=Ortega |first2=Henry F. |last2=Kaiser |title=The ''LL<sup>T</sup>'' and ''QR'' methods for symmetric tridiagonal matrices |journal=The Computer Journal |volume=6 |issue=1 |pages=99–101 |year=1963 |doi=10.1093/comjnl/6.1.99 }}</ref>\n\nThe rate of convergence depends on the separation between eigenvalues, so a practical algorithm will use shifts, either explicit or implicit, to increase separation and accelerate convergence. A typical symmetric QR algorithm isolates each eigenvalue (then reduces the size of the matrix) with only one or two iterations, making it efficient as well as robust.{{clarify|date=June 2012}}\n\n== The implicit QR algorithm ==\nIn modern computational practice, the QR algorithm is performed in an implicit version which makes the use of multiple shifts easier to introduce.<ref name=\"golubvanloan\" /> The matrix is first brought to upper Hessenberg form <math>A_0=QAQ^{\\mathsf{T}}</math> as in the explicit version; then, at each step, the first column of <math>A_k</math> is transformed via a small-size Householder similarity transformation to the first column of <math>p(A_k)</math> (or <math>p(A_k)e_1</math>), where <math>p(A_k)</math>, of degree <math>r</math>, is the polynomial that defines the shifting strategy (often <math>p(x)=(x-\\lambda)(x-\\bar\\lambda)</math>, where <math>\\lambda</math> and <math>\\bar\\lambda</math> are the two eigenvalues of the trailing <math>2 \\times 2</math> principal submatrix of <math>A_k</math>, the so-called ''implicit double-shift''). Then successive Householder transformations of size <math>r+1</math> are performed in order to return the working matrix <math>A_k</math> to upper Hessenberg form. This operation is known as ''bulge chasing'', due to the peculiar shape of the non-zero entries of the matrix along the steps of the algorithm. As in the first version,  deflation is performed as soon as one of the sub-diagonal entries of <math>A_k</math> is sufficiently small.\n\n===Renaming proposal===\nSince in the modern implicit version of the procedure no [[QR decomposition]]s are explicitly performed, some authors, for instance Watkins,<ref>{{cite book |last=Watkins |first=David S. |title=The Matrix Eigenvalue Problem: GR and Krylov Subspace Methods |publisher=SIAM |location=Philadelphia, PA |year=2007 |isbn=978-0-89871-641-2 }}</ref> suggested changing its name to ''Francis algorithm''. [[Gene H. Golub|Golub]] and [[Charles F. Van Loan|Van Loan]] use the term ''Francis QR step''.\n\n== Interpretation and convergence ==\nThe QR algorithm can be seen as a more sophisticated variation of the basic \"power\" [[eigenvalue algorithm]]. Recall that the power algorithm repeatedly multiplies ''A'' times a single vector, normalizing after each iteration. The vector converges to an eigenvector of the largest eigenvalue. Instead, the QR algorithm works with a complete basis of vectors, using QR decomposition to renormalize (and orthogonalize). For a symmetric matrix ''A'', upon convergence, ''AQ'' = ''Q&Lambda;'', where ''&Lambda;'' is the diagonal matrix of eigenvalues to which ''A'' converged, and where ''Q'' is a composite of all the orthogonal similarity transforms required to get there. Thus the columns of ''Q'' are the eigenvectors.\n\n== History ==\nThe QR algorithm was preceded by the ''LR algorithm'', which uses the [[LU decomposition]] instead of the QR decomposition. The QR algorithm is more stable, so the LR algorithm is rarely used nowadays. However, it represents an important step in the development of the QR algorithm.\n\nThe LR algorithm was developed in the early 1950s by [[Heinz Rutishauser]], who worked at that time as a research assistant of [[Eduard Stiefel]] at [[ETH Zurich]]. Stiefel suggested that Rutishauer use the sequence of moments ''y''<sub>0</sub><sup>T</sup> ''A''<sup>''k''</sup> ''x''<sub>0</sub>, ''k'' = 0, 1, … (where ''x''<sub>0</sub> and ''y''<sub>0</sub> are arbitrary vectors) to find the eigenvalues of ''A''. Rutishauer took an algorithm of [[Alexander Aitken]] for this task and developed it into the ''quotient&ndash;difference algorithm'' or ''qd algorithm''. After arranging the computation in a suitable shape, he discovered that the qd algorithm is in fact the iteration ''A''<sub>''k''</sub> = ''L''<sub>''k''</sub>''U''<sub>''k''</sub> (LU decomposition), ''A''<sub>''k''+1</sub> = ''U''<sub>''k''</sub>''L''<sub>''k''</sub>, applied on a tridiagonal matrix, from which the LR algorithm follows.<ref>{{Citation | last1=Parlett | first1=Beresford N. | last2=Gutknecht | first2=Martin H. | title=From qd to LR, or, how were the qd and LR algorithms discovered? | doi=10.1093/imanum/drq003 | year=2011 | journal=IMA Journal of Numerical Analysis | issn=0272-4979 | volume=31 | issue=3 | pages=741–754| url=http://doc.rero.ch/record/299139/files/drq003.pdf }}</ref>\n\n== Other variants ==\nOne variant of the ''QR algorithm'', ''the Golub-Kahan-Reinsch'' algorithm starts with reducing a general matrix into a bidiagonal one.<ref>Bochkanov Sergey Anatolyevich. ALGLIB User Guide - General Matrix operations - Singular value decomposition . ALGLIB Project. 2010-12-11. URL:http://www.alglib.net/matrixops/general/svd.php.{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }} Accessed: 2010-12-11. (Archived by WebCite at https://www.webcitation.org/5utO4iSnR)</ref> This variant of the ''QR algorithm'' for the computation of [[singular values]] was first described by {{harvtxt|Golub|Kahan|1965}}. The [[LAPACK]] subroutine [http://www.netlib.org/lapack/double/dbdsqr.f DBDSQR] implements this iterative method, with some modifications to cover the case where the singular values are very small {{harv|Demmel|Kahan|1990}}. Together with a first step using Householder reflections and, if appropriate, [[QR decomposition]], this forms the [http://www.netlib.org/lapack/double/dgesvd.f  DGESVD] routine for the computation of the [[singular value decomposition]].\n\n== References ==\n{{Reflist|30em}}\n\n== External links ==\n* {{planetmath reference|id=1474|title=Eigenvalue problem}}\n* [http://www-users.math.umn.edu/~olver/aims_/qr.pdf Notes on orthogonal bases and the workings of the QR algorithm] by [[Peter J. Olver]]\n* [https://web.archive.org/web/20081209042103/http://math.fullerton.edu/mathews/n2003/QRMethodMod.html Module for the QR Method]\n* [https://github.com/nom-de-guerre/Matrices C++ Library]\n\n{{Numerical linear algebra}}\n\n{{DEFAULTSORT:Qr Algorithm}}\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "QR decomposition",
      "url": "https://en.wikipedia.org/wiki/QR_decomposition",
      "text": "In [[linear algebra]], a '''QR decomposition''', also known as a '''QR factorization''', is a [[matrix decomposition|decomposition]] of a [[matrix (mathematics)|matrix]] ''A'' into a product ''A''&nbsp;=&nbsp;''QR'' of an [[orthogonal matrix]] ''Q'' and an [[upper triangular matrix]] ''R''. QR decomposition is often used to solve the [[linear least squares (mathematics)|linear least squares]] problem and is the basis for a particular [[eigenvalue algorithm]], the [[QR algorithm]].\n\n==Cases and definitions==\n\n===Square matrix===\nAny real [[square matrix]] ''A'' may be decomposed as\n: <math> A = QR, \\, </math>\n\nwhere ''Q'' is an [[orthogonal matrix]] (its columns are [[orthogonal]] [[unit vector]]s meaning <math>Q^\\textsf{T} Q = QQ^\\textsf{T} = I</math>) and ''R'' is an upper [[triangular matrix]] (also called right triangular matrix). If ''A'' is [[invertible matrix|invertible]], then the factorization is unique if we require the diagonal elements of ''R'' to be positive.\n\nIf instead ''A'' is a complex square matrix, then there is a decomposition ''A'' = ''QR'' where ''Q'' is a [[unitary matrix]] (so <math>Q^* Q = QQ^* = I</math>).\n\nIf ''A'' has ''n'' [[linearly independent]] columns, then the first ''n'' columns of ''Q'' form an [[orthonormal basis]] for the [[column space]] of ''A''. More generally, the first ''k'' columns of ''Q'' form an orthonormal basis for the [[linear span|span]] of the first ''k'' columns of ''A'' for any 1&nbsp;≤&nbsp;''k''&nbsp;≤&nbsp;''n''.<ref name=Trefethen>L. N. Trefethen and D. Bau, ''Numerical Linear Algebra'' (SIAM, 1997).</ref>  The fact that any column ''k'' of ''A'' only depends on the first ''k'' columns of ''Q'' is responsible for the triangular form of&nbsp;''R''.<ref name=Trefethen/>\n\n===Rectangular matrix===\nMore generally, we can factor a complex ''m''×''n'' matrix ''A'', with ''m''&nbsp;≥&nbsp;''n'', as the product of an ''m''×''m'' [[unitary matrix]] ''Q'' and an ''m''×''n'' upper triangular matrix ''R''.  As the bottom (''m''−''n'') rows of an ''m''×''n'' upper triangular matrix consist entirely of zeroes, it is often useful to partition ''R'', or both ''R'' and ''Q'':\n:<math>\n  A = QR = Q \\begin{bmatrix} R_1 \\\\ 0 \\end{bmatrix}\n    = \\begin{bmatrix} Q_1, Q_2 \\end{bmatrix} \\begin{bmatrix} R_1 \\\\ 0 \\end{bmatrix}\n    = Q_1 R_1,\n</math>\n\nwhere ''R''<sub>1</sub> is an ''n''×''n'' upper triangular matrix, ''0'' is an (''m''&nbsp;−&nbsp;''n'')×''n'' zero matrix, ''Q''<sub>1</sub> is ''m''×''n'', ''Q''<sub>2</sub> is ''m''×(''m''&nbsp;−&nbsp;''n''), and ''Q''<sub>1</sub> and ''Q''<sub>2</sub> both have orthogonal columns.\n\n{{harvtxt|Golub|Van Loan|1996|loc=§5.2}} call ''Q''<sub>1</sub>''R''<sub>1</sub> the ''thin QR factorization'' of ''A''; Trefethen and Bau call this the ''reduced QR factorization''.<ref name=Trefethen/> If ''A'' is of full [[matrix rank|rank]] ''n'' and we require that the diagonal elements of ''R''<sub>1</sub> are positive then ''R''<sub>1</sub> and ''Q''<sub>1</sub> are unique, but in general ''Q''<sub>2</sub> is not. ''R''<sub>1</sub> is then equal to the upper triangular factor of the [[Cholesky decomposition]] of ''A''{{starred}} ''A'' (=&nbsp;''A''<sup>T</sup>''A'' if ''A'' is real).\n\n===QL, RQ and LQ decompositions===\nAnalogously, we can define QL, RQ, and LQ decompositions, with ''L'' being a ''lower'' triangular matrix.\n\n==Computing the QR decomposition==\nThere are several methods for actually computing the QR decomposition, such as by means of the [[Gram–Schmidt process]], [[Householder transformation]]s, or [[Givens rotation]]s. Each has a number of advantages and disadvantages.\n\n===Using the Gram–Schmidt process===\n{{details|Gram–Schmidt#Numerical stability}}\nConsider the [[Gram–Schmidt process]] applied to the columns of the full column rank matrix <math>A = \\left[\\mathbf{a}_1, \\cdots, \\mathbf{a}_n\\right]</math>, with [[inner product]] <math>\\langle\\mathbf{v}, \\mathbf{w}\\rangle = \\mathbf{v}^\\textsf{T} \\mathbf{w}</math> (or <math>\\langle\\mathbf{v}, \\mathbf{w}\\rangle = \\mathbf{v}^* \\mathbf{w}</math> for the complex case).\n\nDefine the [[vector projection|projection]]:\n:<math>\\operatorname{proj}_{\\mathbf{u}}\\mathbf{a} =\n  \\frac{\\left\\langle\\mathbf{u}, \\mathbf{a}\\right\\rangle}{\\left\\langle\\mathbf{u}, \\mathbf{u}\\right\\rangle}{\\mathbf{u}}\n</math>\n\nthen:\n:<math>\\begin{align}\n  \\mathbf{u}_1 &= \\mathbf{a}_1, &\n    \\mathbf{e}_1 &= {\\mathbf{u}_1 \\over \\|\\mathbf{u}_1\\|} \\\\\n  \\mathbf{u}_2 &= \\mathbf{a}_2 - \\operatorname{proj}_{\\mathbf{u}_1}\\,\\mathbf{a}_2, &\n    \\mathbf{e}_2 &= {\\mathbf{u}_2 \\over \\|\\mathbf{u}_2\\|} \\\\\n  \\mathbf{u}_3 &= \\mathbf{a}_3 - \\operatorname{proj}_{\\mathbf{u}_1}\\,\\mathbf{a}_3 - \\operatorname{proj}_{\\mathbf{u}_2}\\,\\mathbf{a}_3, &\n    \\mathbf{e}_3 &= {\\mathbf{u}_3 \\over \\|\\mathbf{u}_3\\|} \\\\\n                 & \\vdots &\n                   & \\vdots \\\\\n  \\mathbf{u}_k &= \\mathbf{a}_k - \\sum_{j=1}^{k-1}\\operatorname{proj}_{\\mathbf{u}_j}\\,\\mathbf{a}_k,&\n    \\mathbf{e}_k &= {\\mathbf{u}_k \\over \\|\\mathbf{u}_k\\|}\n\\end{align}</math>\n\nWe can now express the <math>\\mathbf{a}_i</math>s over our newly computed orthonormal basis:\n:<math>\\begin{align}\n   \\mathbf{a}_1 &= \\langle\\mathbf{e}_1, \\mathbf{a}_1\\rangle \\mathbf{e}_1 \\\\\n   \\mathbf{a}_2 &= \\langle\\mathbf{e}_1, \\mathbf{a}_2\\rangle \\mathbf{e}_1\n                 + \\langle\\mathbf{e}_2, \\mathbf{a}_2\\rangle \\mathbf{e}_2 \\\\\n   \\mathbf{a}_3 &= \\langle\\mathbf{e}_1, \\mathbf{a}_3\\rangle \\mathbf{e}_1\n                 + \\langle\\mathbf{e}_2, \\mathbf{a}_3\\rangle \\mathbf{e}_2\n                 + \\langle\\mathbf{e}_3, \\mathbf{a}_3\\rangle \\mathbf{e}_3 \\\\\n                &\\vdots \\\\\n   \\mathbf{a}_k &= \\sum_{j=1}^k \\langle \\mathbf{e}_j, \\mathbf{a}_k \\rangle \\mathbf{e}_j\n\\end{align}</math>\n\nwhere <math>\\left\\langle\\mathbf{e}_i, \\mathbf{a}_i\\right\\rangle = \\left\\|\\mathbf{u}_i\\right\\|</math>. This can be written in matrix form:\n:<math>A = QR</math>\n\nwhere:\n:<math>Q = \\left[\\mathbf{e}_1, \\cdots, \\mathbf{e}_n\\right]</math>\n\nand\n:<math>R = \\begin{pmatrix}\n  \\langle\\mathbf{e}_1, \\mathbf{a}_1\\rangle &\n  \\langle\\mathbf{e}_1, \\mathbf{a}_2\\rangle &\n  \\langle\\mathbf{e}_1, \\mathbf{a}_3\\rangle & \\ldots \\\\\n                                         0 &\n  \\langle\\mathbf{e}_2, \\mathbf{a}_2\\rangle &\n  \\langle\\mathbf{e}_2, \\mathbf{a}_3\\rangle & \\ldots \\\\\n                                         0 &\n                                         0 &\n  \\langle\\mathbf{e}_3, \\mathbf{a}_3\\rangle & \\ldots \\\\\n                                    \\vdots &\n                                    \\vdots &\n                                    \\vdots &\n                                    \\ddots\n\\end{pmatrix}.</math>\n\n====Example====\nConsider the decomposition of\n: <math>A = \\begin{pmatrix}\n  12 & -51 &   4 \\\\\n   6 & 167 & -68 \\\\\n  -4 &  24 & -41\n\\end{pmatrix}.</math>\n\nRecall that an orthonormal matrix <math>Q</math> has the property\n: <math>Q^\\textsf{T} \\,Q = I.</math>\n\nThen, we can calculate <math>Q</math> by means of Gram–Schmidt as follows:\n: <math>\\begin{align}\n  U = \\begin{pmatrix}\n        \\mathbf u_1 & \\mathbf u_2 & \\mathbf u_3\n      \\end{pmatrix} &=\n      \\begin{pmatrix}\n        12 & -69 & -58/5 \\\\\n         6 & 158 &   6/5 \\\\\n        -4 &  30 & -33\n      \\end{pmatrix}; \\\\\n  Q = \\begin{pmatrix}\n        \\frac{\\mathbf u_1}{\\|\\mathbf u_1\\|} &\n        \\frac{\\mathbf u_2}{\\|\\mathbf u_2\\|} &\n        \\frac{\\mathbf u_3}{\\|\\mathbf u_3\\|}\n      \\end{pmatrix} &=\n      \\begin{pmatrix}\n         6/7 & -69/175 & -58/175 \\\\\n         3/7 & 158/175 &   6/175 \\\\\n        -2/7 &   6/35  & -33/35\n      \\end{pmatrix}.\n\\end{align}</math>\n\nThus, we have\n: <math>\\begin{align}\n  Q^\\textsf{T} A &= Q^\\textsf{T}Q\\,R = R; \\\\\n               R &= Q^\\textsf{T}A =\n    \\begin{pmatrix}\n      14 &  21 & -14 \\\\\n       0 & 175 & -70 \\\\\n       0 &   0 &  35\n    \\end{pmatrix}.\n\\end{align}</math>\n\n====Relation to RQ decomposition====\nThe RQ decomposition transforms a matrix ''A'' into the product of an upper triangular matrix ''R'' (also known as right-triangular) and an orthogonal matrix ''Q''. The only difference from QR decomposition is the order of these matrices.\n\nQR decomposition is Gram–Schmidt orthogonalization of columns of ''A'', started from the first column.\n\nRQ decomposition is Gram–Schmidt orthogonalization of rows of ''A'', started from the last row.\n\n====Advantages and disadvantages====\n\nThe Gram-Schmidt process is inherently numerically unstable. While the application of the projections has an appealing geometric analogy to orthogonalization, the orthogonalization itself is prone to numerical error. A significant advantage however is the ease of implementation, which makes this a useful algorithm to use for prototyping if a pre-built linear algebra library is unavailable.\n\n===Using Householder reflections===\n[[File:Householder.svg|thumb|Householder reflection for QR-decomposition: The goal is to find a linear transformation that changes the vector <math>x</math> into a vector of same length which is collinear to <math>e_1</math>. We could use an orthogonal projection (Gram-Schmidt) but this will be numerically unstable if the vectors <math>x</math> and <math>e_1</math> are close to orthogonal. Instead, the Householder reflection reflects through the dotted line (chosen to bisect the angle between <math>x</math> and <math>e_1</math>). The maximum angle with this transform is 45 degrees.]]\n\nA [[Householder reflection]] (or ''Householder transformation'') is a transformation that takes a vector and reflects it about some [[plane (mathematics)|plane]] or [[hyperplane]]. We can use this operation to calculate the ''QR'' factorization of an ''m''-by-''n'' matrix <math>A</math> with ''m''&nbsp;≥&nbsp;''n''.\n\n''Q'' can be used to reflect a vector in such a way that all coordinates but one disappear.\n\nLet <math>\\mathbf{x}</math> be an arbitrary real ''m''-dimensional column vector of <math>A</math> such that <math>\\|\\mathbf{x}\\| = |\\alpha|</math> for a scalar ''α''. If the algorithm is implemented using [[floating-point arithmetic]], then ''α'' should get the opposite sign as the ''k''-th coordinate of <math>\\mathbf{x}</math>, where <math>x_k</math> is to be the pivot coordinate after which all entries are 0 in matrix ''A''{{'}}s final upper triangular form, to avoid [[loss of significance]]. In the complex case, set\n:<math>\\alpha = -e^{i \\arg x_k} \\|\\mathbf{x}\\|</math>\n{{harv|Stoer|Bulirsch|2002|p=225}} and substitute transposition by conjugate transposition in the construction of ''Q'' below.\n\nThen, where <math>\\mathbf{e}_1</math> is the vector (1 0 … 0)<sup>T</sup>, ||·|| is the [[Euclidean space#Euclidean structure|Euclidean norm]] and <math>I</math> is an ''m''-by-''m'' identity matrix, set\n: <math>\\begin{align}\n  \\mathbf{u} &= \\mathbf{x} - \\alpha\\mathbf{e}_1, \\\\\n  \\mathbf{v} &= {\\mathbf{u} \\over \\|\\mathbf{u}\\|}, \\\\\n           Q &= I - 2 \\mathbf{v}\\mathbf{v}^\\textsf{T}.\n\\end{align}</math>\n\nOr, if <math>A</math> is complex\n: <math>Q = I - 2\\mathbf{v}\\mathbf{v}^*.</math>\n\n<math>Q</math> is an ''m''-by-''m'' Householder matrix and\n: <math>Q\\mathbf{x} = \\begin{pmatrix} \\alpha & 0 & \\cdots & 0 \\end{pmatrix}^\\textsf{T}.\\,</math>\n\nThis can be used to gradually transform an ''m''-by-''n'' matrix ''A'' to upper [[triangular]] form. First, we multiply ''A'' with the Householder matrix ''Q''<sub>1</sub> we obtain when we choose the first matrix column for '''x'''. This results in a matrix ''Q''<sub>1</sub>''A'' with zeros in the left column (except for the first row).\n: <math>Q_1A = \\begin{bmatrix}\n  \\alpha_1 & \\star & \\dots & \\star \\\\\n         0 &       &       &       \\\\\n    \\vdots &       &    A' &       \\\\\n         0 &       &       &\n\\end{bmatrix}</math>\n\nThis can be repeated for ''A''′ (obtained from ''Q''<sub>1</sub>''A'' by deleting the first row and first column), resulting in a Householder matrix ''Q''′<sub>2</sub>. Note that ''Q''′<sub>2</sub> is smaller than ''Q''<sub>1</sub>. Since we want it really to operate on ''Q''<sub>1</sub>''A'' instead of ''A''′ we need to expand it to the upper left, filling in a 1, or in general:\n:<math>Q_k = \\begin{pmatrix}\n  I_{k-1} & 0    \\\\\n       0  & Q_k'\n\\end{pmatrix}.</math>\n\nAfter <math>t</math> iterations of this process, <math>t = \\min(m - 1, n)</math>,\n:<math>R = Q_t \\cdots Q_2 Q_1 A</math>\n\nis an upper triangular matrix. So, with\n:<math>Q = Q_1^\\textsf{T} Q_2^\\textsf{T} \\cdots Q_t^\\textsf{T},</math>\n\n<math>A = QR</math> is a QR decomposition of <math>A</math>.\n\nThis method has greater [[numerical stability]] than the Gram–Schmidt method above.<!--See the below example, and compare above-->\n\nThe following table gives the number of operations in the ''k''-th step of the QR-decomposition by the Householder transformation, assuming a square matrix with size ''n''.\n{| class=\"wikitable\"\n|-\n! Operation\n! Number of operations in the ''k''-th step\n|-\n| Multiplications\n| <math>2(n - k + 1)^2</math>\n|-\n| Additions\n| <math>(n - k + 1)^2 + (n - k + 1)(n - k) + 2 </math>\n|-\n| Division\n| <math>1</math>\n|-\n| Square root\n| <math>1</math>\n|}\n\nSumming these numbers over the ''n''&nbsp;−&nbsp;1 steps (for a square matrix of size ''n''), the complexity of the algorithm (in terms of floating point multiplications) is given by\n:<math>\\frac{2}{3}n^3 + n^2 + \\frac{1}{3}n - 2 = O\\left(n^3\\right).</math>\n\n====Example====\nLet us calculate the decomposition of\n: <math>A = \\begin{pmatrix}\n  12 & -51 &   4 \\\\\n   6 & 167 & -68 \\\\\n  -4 &  24 & -41\n\\end{pmatrix}.</math>\n\nFirst, we need to find a reflection that transforms the first column of matrix ''A'', vector <math>\\mathbf{a}_1 = \\begin{pmatrix} 12 & 6 & -4 \\end{pmatrix}^\\textsf{T}</math>, into <math>\\left\\|\\mathbf{a}_1\\right\\| \\;\\mathbf{e}_1 = \\begin{pmatrix} 1 & 0 & 0\\end{pmatrix}^\\textsf{T}.</math>\n\nNow,\n: <math>\\mathbf{u} = \\mathbf{x} - \\alpha\\mathbf{e}_1,</math>\n\nand\n: <math>\\mathbf{v} = {\\mathbf{u} \\over \\|\\mathbf{u}\\|}.</math>\n\nHere,\n: <math>\\alpha = 14</math> and <math>\\mathbf{x} = \\mathbf{a}_1 = \\begin{pmatrix} 12 & 6 & -4 \\end{pmatrix}^\\textsf{T}</math>\n\nTherefore\n: <math>\\mathbf{u} = \\begin{pmatrix} -2 & 6 & -4 \\end{pmatrix}^\\textsf{T} = \\begin{pmatrix} 2 \\end{pmatrix}\\begin{pmatrix} -1 & 3 & -2 \\end{pmatrix}^\\textsf{T}</math> and <math>\\mathbf{v} = {1 \\over \\sqrt{14}}\\begin{pmatrix} -1 & 3 & -2 \\end{pmatrix}^\\textsf{T}</math>, and then\n: <math>\\begin{align}\n      Q_1\n  ={} &I - {2 \\over \\sqrt{14}\\sqrt{14}}\n         \\begin{pmatrix} -1 \\\\ 3 \\\\ -2 \\end{pmatrix}\n         \\begin{pmatrix} -1 &  3 &  -2 \\end{pmatrix} \\\\\n  ={} &I - {1 \\over 7}\\begin{pmatrix}\n          1 & -3 &  2 \\\\\n         -3 &  9 & -6 \\\\\n          2 & -6 &  4\n       \\end{pmatrix} \\\\\n  ={} &\\begin{pmatrix}\n          6/7 &  3/7 & -2/7 \\\\\n          3/7 & -2/7 &  6/7 \\\\\n         -2/7 &  6/7 &  3/7 \\\\\n       \\end{pmatrix}.\n\\end{align}</math>\n\nNow observe:\n:<math>Q_1A = \\begin{pmatrix}\n  14 &  21 & -14 \\\\\n   0 & -49 & -14 \\\\\n   0 & 168 & -77\n\\end{pmatrix},</math>\n\nso we already have almost a triangular matrix. We only need to zero the (3, 2) entry.\n\nTake the (1, 1) [[minor (linear algebra)|minor]], and then apply the process again to\n:<math>A' = M_{11} = \\begin{pmatrix}\n  -49 & -14 \\\\\n  168 & -77\n\\end{pmatrix}.</math>\n\nBy the same method as above, we obtain the matrix of the Householder transformation\n:<math>Q_2 = \\begin{pmatrix}\n  1 &     0 &  0 \\\\\n  0 & -7/25 & 24/25 \\\\\n  0 & 24/25 &  7/25\n\\end{pmatrix}</math>\n\nafter performing a direct sum with 1 to make sure the next step in the process works properly.\n\nNow, we find\n:<math>Q = Q_1^\\textsf{T} Q_2^\\textsf{T} = \\begin{pmatrix}\n   6/7 & -69/175 & 58/175 \\\\\n   3/7 & 158/175 & -6/175 \\\\\n  -2/7 &   6/35  & 33/35\n\\end{pmatrix} </math>\n\nOr, to four decimal digits,\n:<math>\\begin{align}\n  Q &= Q_1^\\textsf{T} Q_2^\\textsf{T} = \\begin{pmatrix}\n     0.8571 & -0.3943 &  0.3314 \\\\\n     0.4286 &  0.9029 & -0.0343 \\\\\n    -0.2857 &  0.1714 &  0.9429\n  \\end{pmatrix} \\\\\n  R &= Q_2 Q_1 A = Q^\\textsf{T} A = \\begin{pmatrix}\n    14 &  21 & -14 \\\\\n     0 & 175 & -70 \\\\\n     0 &   0 & -35\n  \\end{pmatrix}.\n\\end{align}</math>\n\nThe matrix ''Q'' is orthogonal and ''R'' is upper triangular, so ''A'' = ''QR'' is the required QR-decomposition.\n\n====Advantages and disadvantages====\n\nThe use of Householder transformations is inherently the most simple of the numerically stable QR decomposition algorithms due to the use of reflections as the mechanism for producing zeroes in the ''R'' matrix. However, the Householder reflection algorithm is bandwidth heavy and not parallelizable, as every reflection that produces a new zero element changes the entirety of both ''Q'' and ''R'' matrices.\n\n===Using Givens rotations===\n''QR'' decompositions can also be computed with a series of [[Givens rotation]]s.  Each rotation zeroes an element in the subdiagonal of the matrix, forming the ''R'' matrix.  The concatenation of all the Givens rotations forms the orthogonal ''Q'' matrix.\n\nIn practice, Givens rotations are not actually performed by building a whole matrix and doing a matrix multiplication.  A Givens rotation procedure is used instead which does the equivalent of the sparse Givens matrix multiplication, without the extra work of handling the sparse elements.  The Givens rotation procedure is useful in situations where only a relatively few off diagonal elements need to be zeroed, and is more easily parallelized than [[Householder transformation]]s.\n\n====Example====\nLet us calculate the decomposition of\n: <math>A = \\begin{pmatrix}\n  12 & -51 &   4 \\\\\n   6 & 167 & -68 \\\\\n  -4 &  24 & -41\n\\end{pmatrix}.</math>\n\nFirst, we need to form a rotation matrix that will zero the lowermost left element, <math>\\mathbf{a}_{31} = -4</math>.  We form this matrix using the Givens rotation method, and call the matrix <math>G_1</math>.  We will first rotate the vector <math>\\begin{pmatrix} 12 & -4 \\end{pmatrix}</math>, to point along the ''X'' axis.  This vector has an angle <math>\\theta = \\arctan\\left({-(-4) \\over 12}\\right)</math>.  We create the orthogonal Givens rotation matrix, <math>G_1</math>:\n\n:<math>\\begin{align}\n  G_1 &= \\begin{pmatrix}\n    \\cos(\\theta) & 0 & -\\sin(\\theta) \\\\\n               0 & 1 &             0 \\\\\n    \\sin(\\theta) & 0 &  \\cos(\\theta)\n  \\end{pmatrix} \\\\\n      &\\approx \\begin{pmatrix}\n    0.94868 & 0 & -0.31622 \\\\\n    0       & 1 &  0       \\\\\n    0.31622 & 0 &  0.94868 \n  \\end{pmatrix}\n\\end{align}</math>\n\nAnd the result of <math>G_1A</math> now has a zero in the <math>\\mathbf{a}_{31}</math> element.\n:<math>G_1A \\approx \\begin{pmatrix}\n  12.64911 & -55.97231 &  16.76007 \\\\\n   6       & 167       & -68       \\\\\n   0       &   6.64078 & -37.6311 \n\\end{pmatrix}</math>\n\nWe can similarly form Givens matrices <math>G_2</math> and <math>G_3</math>, which will zero the sub-diagonal elements <math>a_{21}</math> and <math>a_{32}</math>, forming a triangular matrix <math>R</math>.  The orthogonal matrix <math>Q^\\textsf{T}</math> is formed from the product of all the Givens matrices <math>Q^\\textsf{T} = G_3 G_2 G_1</math>.  Thus, we have <math>G_3 G_2 G_1 A = Q^\\textsf{T} A = R</math>, and the ''QR'' decomposition is <math>A = QR</math>.\n\n====Advantages and disadvantages====\n\nThe QR decomposition via Givens rotations is the most involved to implement, as the ordering of the rows required to fully exploit the algorithm is not trivial to determine. However, it has a significant advantage in that each new zero element <math>a_{ij}</math> affects only the row with the element to be zeroed (i) and a row above (j). This makes the Givens rotation algorithm more bandwidth efficient and parallelisable than the Householder reflection technique.\n\n==Connection to a determinant or a product of eigenvalues==\nWe can use QR decomposition to find the absolute value of the [[determinant]] of a square matrix. Suppose a matrix is decomposed as <math>A = QR</math>. Then we have\n:<math>\\det(A) = \\det(Q) \\cdot \\det(R).</math>\n\nSince ''Q'' is unitary, <math>|\\det(Q)| = 1</math>. Thus,\n:<math>\\left|\\det(A)\\right| = \\left|\\det(R)\\right| = \\left|\\prod_{i} r_{ii}\\right|,</math>\n\nwhere <math>r_{ii}</math> are the entries on the diagonal of ''R''.\n\nFurthermore, because the determinant equals the product of the eigenvalues, we have\n: <math>\\left|\\prod_{i} r_{ii}\\right| = \\left|\\prod_{i} \\lambda_{i}\\right|,</math>\n\nwhere <math>\\lambda_i</math> are eigenvalues of <math>A</math>.\n\nWe can extend the above properties to non-square complex matrix <math>A</math> by introducing the definition of QR-decomposition for non-square complex matrix and replacing eigenvalues with singular values.\n\nSuppose a QR decomposition for a non-square matrix ''A'':\n: <math>A = Q \\begin{pmatrix} R \\\\ O \\end{pmatrix}, \\qquad Q^* Q = I,</math>\n\nwhere <math>O</math> is a zero matrix and <math>Q</math> is a unitary matrix.\n\nFrom the properties of [[singular value decomposition|SVD]] and determinant of matrix, we have\n:<math>\\left|\\prod_i r_{ii}\\right| = \\prod_i\\sigma_{i},</math>\n\nwhere <math>\\sigma_i</math> are singular values of <math>A</math>.\n\nNote that the singular values of <math>A</math> and <math>R</math> are identical, although their complex eigenvalues may be different.  However, if ''A'' is square, the following is true:\n:<math>{\\prod_i \\sigma_i} = \\left|{\\prod_i \\lambda_i}\\right|.</math>\n\nIn conclusion, QR decomposition can be used efficiently to calculate the product of the eigenvalues or singular values of a matrix.\n\n==Column pivoting==\n{{Expand section|date=December 2009}}\nQR decomposition with column pivoting introduces a [[permutation matrix]] ''P'':\n:<math>AP = QR\\quad \\iff\\quad A = QRP^\\textsf{T}</math>\n\nColumn pivoting is useful when ''A'' is (nearly) [[rank deficient]], or is suspected of being so. It can also improve numerical accuracy. ''P'' is usually chosen so that the diagonal elements of ''R'' are non-increasing: <math>\\left|r_{11}\\right| \\ge \\left|r_{22}\\right| \\ge \\ldots \\ge \\left|r_{nn}\\right|</math>. This can be used to find the (numerical) rank of ''A'' at lower computational cost than a [[singular value decomposition]], forming the basis of so-called [[rank-revealing QR algorithm]]s.\n\n==Using for solution to linear inverse problems==\nCompared to the direct matrix inverse, inverse solutions using QR decomposition are more numerically stable as evidenced by their reduced [[condition number]]s [Parker, Geophysical Inverse Theory, Ch1.13].\n\nTo solve the underdetermined (<math>m < n</math>) linear problem <math>Ax = b</math> where the matrix A has dimensions <math>m \\times n</math> and rank <math>m</math>, first find the QR factorization of the transpose of A: <math>A^\\textsf{T} = QR</math>, where Q is an orthogonal matrix (i.e. <math>Q^\\textsf{T} = Q^{-1}</math>), and R has a special form: <math>R = \\begin{bmatrix} R_1 \\\\ 0 \\end{bmatrix}</math>. Here <math>R_1</math> is a square <math>m \\times m</math> right triangular matrix, and the zero matrix has dimension <math>(n-m) \\times m</math>. After some algebra, it can be shown that a solution to the inverse problem can be expressed as: <math>\nx = Q \\begin{bmatrix}\n    \\left(R_1^\\textsf{T}\\right)^{-1}b \\\\\n                           0\n  \\end{bmatrix}\n</math> where one may either find <math>R_1^{-1}</math> by [[Gaussian elimination]] or compute <math>\\left(R_1^\\textsf{T}\\right)^{-1} b</math> directly by [[triangular matrix#Forward and back substitution|forward substitution]]. The latter technique enjoys greater numerical accuracy and lower computations.\n\nTo find a solution, <math>\\hat{x}</math>, to the overdetermined (<math>m \\geq n</math>) problem <math>Ax = b</math> which minimizes the norm <math>\\|A \\hat{x} - b\\|</math>, first find the QR factorization of A: <math>A = QR</math>. The solution can then be expressed as <math>\\hat{x} = R_1^{-1} \\left(Q_1^\\textsf{T} b\\right) </math>, where <math>Q_1</math> is an <math>m \\times n</math> matrix containing the first <math>n</math> columns of the full orthonormal basis <math>Q</math> and where <math>R_1</math> is as before. Equivalent to the underdetermined case, [[triangular matrix#Forward and back substitution|back substitution]] can be used to quickly and accurately find this <math>\\hat x</math> without explicitly inverting <math>R_1</math>. (<math>Q_1</math> and <math>R_1</math> are often provided by numerical libraries as an \"economic\" QR decomposition.)\n\n==Generalizations==\n[[Iwasawa decomposition]] generalizes QR decomposition to semi-simple Lie groups.\n\n==See also==\n* [[Polar decomposition]]\n* [[Eigenvalue decomposition]]\n* [[Spectral decomposition]]\n* [[LU decomposition]]\n* [[Singular value decomposition]]\n\n==References==\n<references />\n\n==Further reading==\n* {{Citation | last1=Golub | first1=Gene H. | author1-link=Gene H. Golub | last2=Van Loan | first2=Charles F. | author2-link=Charles F. Van Loan | title=Matrix Computations | publisher=Johns Hopkins | edition=3rd | isbn=978-0-8018-5414-9 | year=1996}}.\n* {{citation | first1=Roger A. | last1=Horn | first2=Charles R. | last2=Johnson | year=1985 | title=Matrix Analysis | publisher=Cambridge University Press | isbn=0-521-38632-2 }}. Section 2.8.\n* {{Citation |last1=Press|first1=WH|last2=Teukolsky|first2=SA|last3=Vetterling|first3=WT|last4=Flannery|first4=BP|year=2007|title=Numerical Recipes: The Art of Scientific Computing|edition=3rd|publisher=Cambridge University Press| publication-place=New York|isbn=978-0-521-88068-8|chapter=Section 2.10. QR Decomposition|chapter-url=http://apps.nrbook.com/empanel/index.html?pg=102}}\n* {{citation | first1=Josef | last1=Stoer | first2=Roland | last2=Bulirsch | year=2002 | title=Introduction to Numerical Analysis | edition=3rd | publisher=Springer | isbn=0-387-95452-X }}.\n\n==External links==\n*[https://web.archive.org/web/20081212221215/http://www.bluebit.gr/matrix-calculator/ Online Matrix Calculator] Performs QR decomposition of matrices.\n*[http://netlib.org/lapack/lug/node39.html LAPACK users manual] gives details of subroutines to calculate the QR decomposition\n*[https://web.archive.org/web/20061122183956/http://documents.wolfram.com/mathematica/functions/QRDecomposition Mathematica users manual] gives details and examples of routines to calculate QR decomposition\n*[http://www.alglib.net/ ALGLIB] includes a partial port of the LAPACK to C++, C#, Delphi, etc.\n*[http://eigen.tuxfamily.org/dox-devel/group__QR__Module.html Eigen::QR] Includes C++ implementation of QR decomposition.\n\n{{Numerical linear algebra}}\n\n[[Category:Matrix decompositions]]\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "Rayleigh quotient iteration",
      "url": "https://en.wikipedia.org/wiki/Rayleigh_quotient_iteration",
      "text": "'''Rayleigh quotient iteration''' is an [[eigenvalue algorithm]] which extends the idea of the [[inverse iteration]] by using the [[Rayleigh quotient]] to obtain increasingly accurate [[eigenvalue]] estimates.\n\nRayleigh quotient iteration is an [[iterative method]], that is, it delivers a sequence of approximate solutions that [[Limit of a sequence|converges]] to a true solution in the limit (this is true for all algorithms that compute eigenvalues: since eigenvalues can be irrational numbers, there can be no general method for computing them in a finite number of steps). Very rapid convergence is guaranteed and no more than a few iterations are needed in practice to obtain a reasonable approximation.  The Rayleigh quotient iteration algorithm [[rate of convergence|converges cubically]] for Hermitian or symmetric matrices, given an initial vector that is sufficiently close to an [[EigenVector|eigenvector]] of the [[Matrix (mathematics)|matrix]] that is being analyzed.\n\n== Algorithm ==\n\nThe algorithm is very similar to inverse iteration, but replaces the estimated eigenvalue at the end of each iteration with the Rayleigh quotient. Begin by choosing some value <math>\\mu_0</math> as an initial eigenvalue guess for the Hermitian matrix <math>A</math>. An initial vector <math>b_0</math> must also be supplied as initial eigenvector guess.\n\nCalculate the next approximation of the eigenvector <math>b_{i+1}</math> by\n\n<math>\nb_{i+1} = \\frac{(A-\\mu_i I)^{-1}b_i}{||(A-\\mu_i I)^{-1}b_i||},\n</math><br>\nwhere <math>I</math> is the identity matrix,\nand set the next approximation of the eigenvalue to the Rayleigh quotient of the current iteration equal to<br>\n<math>\n\\mu_i = \\frac{b^*_i A b_i}{b^*_i b_i}.\n</math>\n\nTo compute more than one eigenvalue, the algorithm can be combined with a deflation technique.\n\nNote that for very small problems it is beneficial to replace the [[matrix inverse]] with the [[adjugate matrix|adjugate]], which will yield the same iteration because it is equal to the inverse up to an irrelevant scale (the inverse of the determinant, specifically). The adjugate is easier to compute explicitly than the inverse (though the inverse is easier to apply to a vector for problems that aren't small), and is more numerically sound because it remains well defined as the eigenvalue converges.\n\n== Example ==\n\nConsider the matrix\n\n:<math>\nA = \n\\left[\\begin{matrix}\n1 & 2 & 3\\\\\n1 & 2 & 1\\\\\n3 & 2 & 1\\\\\n\\end{matrix}\\right]\n</math>\n\nfor which the exact eigenvalues are <math>\\lambda_1 = 3+\\sqrt5</math>, <math>\\lambda_2 = 3-\\sqrt5</math> and <math>\\lambda_3 = -2</math>, with corresponding eigenvectors\n\n:<math>v_1 = \\left[\n\\begin{matrix}\n   1 \\\\\n   \\varphi-1 \\\\\n   1 \\\\\n\\end{matrix}\\right]</math>,  <math>v_2 = \\left[\n\\begin{matrix}\n   1 \\\\\n   -\\varphi \\\\\n   1 \\\\\n\\end{matrix}\\right]</math> and <math>v_3 = \\left[\n\\begin{matrix}\n   1 \\\\\n   0 \\\\\n   1 \\\\\n\\end{matrix}\\right]</math>.\n\n(where <math>\\textstyle\\varphi=\\frac{1+\\sqrt5}2</math> is the golden ratio).\n\nThe largest eigenvalue is <math>\\lambda_1 \\approx 5.2361</math> and corresponds to any eigenvector proportional to <math>v_1 \\approx \\left[\n\\begin{matrix}\n   1 \\\\\n   0.6180 \\\\\n   1 \\\\\n\\end{matrix}\\right].\n</math>\n\nWe begin with an initial eigenvalue guess of\n\n:<math>b_0 =\n\\left[\\begin{matrix}\n  1 \\\\\n  1 \\\\\n  1 \\\\\n\\end{matrix}\\right], ~\\mu_0 = 200</math>.\n\nThen, the first iteration yields\n\n:<math>b_1 \\approx \n\\left[\\begin{matrix}\n  -0.57927 \\\\\n  -0.57348 \\\\\n  -0.57927 \\\\\n\\end{matrix}\\right], ~\\mu_1 \\approx 5.3355\n</math>\n\nthe second iteration,\n\n:<math>b_2 \\approx \n\\left[\\begin{matrix}\n   0.64676 \\\\\n   0.40422 \\\\\n   0.64676 \\\\\n\\end{matrix}\\right], ~\\mu_2 \\approx 5.2418\n</math>\n\nand the third,\n\n:<math>b_3 \\approx \n\\left[\\begin{matrix}\n  -0.64793 \\\\\n  -0.40045 \\\\\n  -0.64793 \\\\\n\\end{matrix}\\right], ~\\mu_3 \\approx 5.2361\n</math>\n\nfrom which the cubic convergence is evident.\n\n== Octave Implementation ==\n\nThe following is a simple implementation of the algorithm in [[GNU Octave|Octave]].\n\n<source lang=\"matlab\">\nfunction x = rayleigh(A, epsilon, mu, x)\n  x = x / norm(x);\n  % the backslash operator in Octave solves a linear system\n  y = (A - mu * eye(rows(A))) \\ x; \n  lambda = y' * x;\n  mu = mu + 1 / lambda\n  err = norm(y - lambda * x) / norm(y)\n\n  while err > epsilon\n    x = y / norm(y);\n    y = (A - mu * eye(rows(A))) \\ x;\n    lambda = y' * x;\n    mu = mu + 1 / lambda\n    err = norm(y - lambda * x) / norm(y)\n  end\n\nend\n</source>\n\n== See also ==\n* [[Power iteration]]\n* [[Inverse iteration]]\n\n==References==\n* Lloyd N. Trefethen and David Bau, III, ''Numerical Linear Algebra'', Society for Industrial and Applied Mathematics, 1997. {{isbn|0-89871-361-7}}.\n* Rainer Kress, \"Numerical Analysis\", Springer, 1991. {{isbn|0-387-98408-9}}\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]\n[[Category:Articles with example MATLAB/Octave code]]"
    },
    {
      "title": "Row echelon form",
      "url": "https://en.wikipedia.org/wiki/Row_echelon_form",
      "text": "\nIn [[linear algebra]], a [[matrix (mathematics)|matrix]] is in '''echelon form''' if it has the shape resulting from a [[Gaussian elimination]]. '''Row echelon form''' means that Gaussian elimination has operated on the rows and\n'''column echelon form''' means that Gaussian elimination has operated on the columns. In other words, a matrix is in column echelon form if its [[transpose]] is in row echelon form. Therefore, only row echelon forms are considered in the remainder of this article. The similar properties of column echelon form are easily deduced by transposing all the matrices. \n\nSpecifically, a matrix is in '''row echelon form''' if\n* all nonzero rows (rows with at least one nonzero element) are above any rows of all zeroes (all zero rows, if any, belong at the bottom of the matrix), and\n* the [[Leading coefficient#Linear algebra|leading coefficient]] (the first nonzero number from the left, also called the [[pivot element|pivot]]) of a nonzero row is always strictly to the right of the leading coefficient of the row above it (some texts add the condition that the leading coefficient must be 1<ref>See, for instance, {{harvtxt|Leon|2009|p=13}}</ref>). \n\nThese two conditions imply that all entries in a column below a leading coefficient are zeros.<ref>{{harvnb|Meyer|2000|p=44}}</ref>\n\nThis is an example of a 3×5 matrix in row echelon form, which is not in ''reduced'' row echelon form (see below):\n\n: <math>\n\\left[ \\begin{array}{ccccc}\n1 & a_0 & a_1 & a_2 & a_3 \\\\\n0 & 0 & 2 & a_4 & a_5 \\\\\n0 & 0 & 0 & 1 & a_6\n\\end{array} \\right]\n</math>\n\nMany properties of matrices may be easily deduced from their row echelon form, such as the [[rank (linear algebra)|rank]] and the [[kernel (linear algebra)|kernel]].\n\n=={{anchor|rref}}Reduced row echelon form==\nA matrix is in '''reduced row echelon form''' (also called '''row canonical form''') if it satisfies the following conditions:<ref>{{harvnb|Meyer|2000|p=48}}</ref>\n* It is in row echelon form.\n* The leading entry in each nonzero row is a 1 (called a leading 1).\n* Each column containing a leading 1 has zeros everywhere else.\n\nThe reduced row echelon form of a matrix may be computed by [[Gauss–Jordan elimination]]. Unlike the row echelon form, the reduced row echelon form of a matrix is unique and does not depend on the algorithm used to compute it.<ref name=\":0\">{{Cite book|url=https://books.google.com/books?id=loRbAgAAQBAJ|title=Elementary Linear Algebra: Applications Version, 11th Edition|last=Anton|first=Howard|last2=Rorres|first2=Chris|date=2013-10-23|publisher=Wiley Global Education|isbn=9781118879160|page=21|language=en}}</ref> For a given matrix, despite the row echelon form not being unique, all row echelon forms and the reduced row echelon form have the same number of zero rows and the pivots are located in the same indices.<ref name=\":0\" />\n\nThis is an example of a matrix in reduced row echelon form, which shows that the left of the matrix is not always an [[identity matrix]]:\n\n: <math>\n\\left[ \\begin{array}{ccccc}\n1 & 0 & a_1 & 0 & b_1 \\\\\n0 & 1 & a_2 & 0 & b_2 \\\\\n0 & 0 & 0 & 1 & b_3\n\\end{array} \\right]\n</math>\n\nFor matrices with integer coefficients, the [[Hermite normal form]] is a row echelon form that may be calculated using [[Euclidean division]] and without introducing any [[rational number]] or denominator. On the other hand, the reduced echelon form of a matrix with integer coefficients generally contains non-integer coefficients.\n\n== Transformation to row echelon form ==\n\nBy means of a finite sequence of [[elementary row operations]], called [[Gaussian elimination]], any matrix can be transformed to row echelon form.  Since elementary row operations preserve the [[row space]] of the matrix, the row space of the row echelon form is the same as that of the original matrix.\n\nThe resulting echelon form is not unique; any matrix that is in echelon form can be put in an ([[row equivalence|equivalent]]) echelon form by adding a scalar multiple of a row to one of the above rows, for example:\n: <math> \\begin{bmatrix} 1 & 3 & -1 \\\\ 0 & 1 & 7 \\\\ \\end{bmatrix} \n\\xrightarrow{\\text{add row 2 to row 1}}\n\\begin{bmatrix} 1 & 4 & 6 \\\\ 0 & 1 & 7 \\\\ \\end{bmatrix}. </math>\nHowever, every matrix has a unique ''reduced'' row echelon form. In the above example, the reduced row echelon form can be found as\n: <math> \\begin{bmatrix} 1 & 3 & -1 \\\\ 0 & 1 & 7 \\\\ \\end{bmatrix} \n\\xrightarrow{\\text{subtract 3 × (row 2) from row 1}}\n\\begin{bmatrix} 1 & 0 & -22 \\\\ 0 & 1 & 7 \\\\ \\end{bmatrix}. </math>\nThis means that the nonzero rows of the reduced row echelon form are the unique reduced row echelon generating set for the row space of the original matrix.\n\n== Systems of linear equations ==\n\nA [[system of linear equations]] is said to be in ''row echelon form'' if its [[augmented matrix]] is in row echelon form. Similarly, a system of equations is said to be in ''reduced row echelon form'' or in ''canonical form'' if its augmented matrix is in reduced row echelon form.\n\nThe canonical form may be viewed as an explicit solution of the linear system. In fact, the system is [[System of linear equations#Consistency|inconsistent]] if and only if one of the equations of the canonical form is reduced to 0 = 1.<ref>{{Cite book|url=https://books.google.com/books?id=S0imN2tl1qwC|title=Linear Algebra: Theory and Applications|last=Cheney|first=Ward|last2=Kincaid|first2=David R.|date=2010-12-29|publisher=Jones & Bartlett Publishers|isbn=9781449613525|pages=47–50|language=en}}</ref> Otherwise, regrouping in the right hand side all the terms of the equations but the leading ones, expresses the variables corresponding to the pivots as constants or linear functions of the other variables, if any.\n\n==Notes==\n{{reflist}}\n\n==References==\n* {{Citation | last1 = Leon | first1 = Steve | title = Linear Algebra with Applications | isbn=978-0136009290 | edition = 8th | year = 2009 | publisher = Pearson }}.\n* {{Citation | last1=Meyer | first1=Carl D. | title=Matrix Analysis and Applied Linear Algebra | url=http://www.matrixanalysis.com/ | publisher=[[Society for Industrial and Applied Mathematics|SIAM]] | isbn=978-0-89871-454-8 | year=2000}}.\n\n==External links==\n{{wikibooks|Linear Algebra|Row Reduction and Echelon Forms}}\n\n*[http://people.revoledu.com/kardi/tutorial/LinearAlgebra/RREF.html Interactive Row Echelon Form with rational output]\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]\n\n[[de:Lineares Gleichungssystem#Stufenform, Treppenform]]"
    },
    {
      "title": "RRQR factorization",
      "url": "https://en.wikipedia.org/wiki/RRQR_factorization",
      "text": "An '''RRQR factorization''' or '''rank-revealing QR factorization''' is a [[matrix decomposition]] algorithm based on the [[QR decomposition|QR factorization]] which can be used to determine the [[rank (linear algebra)|rank]] of a matrix.<ref name=GuSciComput1996>{{cite journal|last=Gu|first=Ming|author2=Stanley C. Eisenstat |title=Efficient algorithms for computing a strong rank-revealing QR factorization|journal=SIAM Journal on Scientific Computing|date=July 1996|volume=17|issue=4|pages=848–869|doi=10.1137/0917055|url=http://math.berkeley.edu/~mgu/MA273/Strong_RRQR.pdf|accessdate=22 September 2014}}</ref> The SVD can be used to generate an RRQR, but it is not an efficient method to do so.<ref name=HongPan92>{{cite journal|last=Hong|first=Y.P.|author2=C.-T. Pan |title=Rank-Revealing QR Factorizations and the Singular Value Decomposition|journal=Mathematics of Computation|date=Jan 1992|volume=58|issue=197|pages=213–232|jstor=2153029|doi=10.2307/2153029}}</ref> An RRQR implementation is available in MATLAB.<ref name=\"RRQR Factorization MATLAB Docs\">{{cite web|title=RRQR Factorization|url=http://www.mpi-magdeburg.mpg.de/mpcsc/downloads/rrqr/Readme.pdf|accessdate=2 April 2011|date=29 March 2007}}</ref>\n\n== References ==\n{{Reflist}}\n\n{{Numerical linear algebra}}\n{{Use dmy dates|date=April 2011}}\n\n[[Category:Matrix decompositions]]\n[[Category:Numerical linear algebra]]\n\n\n{{Linear-algebra-stub}}\n{{Algorithm-stub}}"
    },
    {
      "title": "Rybicki Press algorithm",
      "url": "https://en.wikipedia.org/wiki/Rybicki_Press_algorithm",
      "text": "{{Orphan|date=May 2018}}\n\n[[File:Extended_Sparse_Matrix.png|thumb|Extended Sparse Matrix arising from a <math>10 \\times 10</math> semi-separable matrix whose semi-separable rank is <math>4</math>.]]\nThe '''Rybicki–Press algorithm''' is a fast direct algorithm for inverting a matrix, whose entries are given by <math>A(i,j) = \\exp(-a \\vert t_i - t_j \\vert)</math>, where <math>a \\in \\mathbb{R}</math>.<ref>{{citation\n |last1 = Rybicki|first1 = George B.|last2 = Press|first2 = William H.|arxiv = comp-gas/9405004|doi = 10.1103/PhysRevLett.74.1060|journal = Physical Review Letters|title = Class of fast methods for processing Irregularly sampled or otherwise inhomogeneous one-dimensional data|volume = 74|issue = 7|pages = 1060–1063|year = 1995|bibcode = 1995PhRvL..74.1060R|pmid=10058924}} {{Open access}}</ref>  It is a computational optimization of a general set of statistical methods developed to determine whether two noisy, irregularly sampled data sets are, in fact, dimensionally shifted representations of the same underlying function.<ref>{{Cite journal|url = |title = Interpolation, realization, and reconstruction of noisy, irregularly sampled data|last = Rybicki|first = George B.|date = October 1992|journal = The Astrophysical Journal|doi = 10.1086/171845|pmid = |last2 = Press|first2 = William H.|bibcode = 1992ApJ...398..169R|volume=398|page=169}}{{Open access}}</ref><ref name=\":0\">{{Cite journal|last=MacLeod|first=C. L.|last2=Brooks|first2=K.|last3=Ivezic|first3=Z.|last4=Kochanek|first4=C. S.|last5=Gibson|first5=R.|last6=Meisner|first6=A.|last7=Kozlowski|first7=S.|last8=Sesar|first8=B.|last9=Becker|first9=A. C.|date=2011-02-10|title=Quasar Selection Based on Photometric Variability|journal=The Astrophysical Journal|volume=728|issue=1|pages=26|doi=10.1088/0004-637X/728/1/26|issn=0004-637X|arxiv=1009.2081|bibcode=2011ApJ...728...26M}}</ref>  The most common use of the algorithm is in the detection of periodicity in astronomical observations.<ref name=\":0\" />\n\nRecently, this method has been extended ('''Generalized Rybicki Press algorithm''') for inverting matrices whose entries of the form <math>A(i,j) = \\sum_{k=1}^p a_k \\exp(-\\beta_k \\vert t_i - t_j \\vert)</math>.<ref>{{Cite journal|last=Ambikasaran|first=Sivaram|date=2015-12-01|title=Generalized Rybicki Press algorithm|journal=Numerical Linear Algebra with Applications|language=en|volume=22|issue=6|pages=1102–1114|doi=10.1002/nla.2003|issn=1099-1506|arxiv=1409.7852}}</ref> The key observation in the Generalized Rybicki Press (GPP) algorithm is that the matrix <math>A</math> is a semi-separable matrix with rank <math>p</math>. More precisely, if the matrix <math>A \\in \\mathbb{R}^{n\\times n}</math> has a semi-separable rank is <math>p</math>, the cost for solving the linear system <math>Ax=b</math> and obtaining the determinant of the matrix scales as <math>\\mathcal{O}\\left(p^2n \\right)</math>, thereby making it extremely attractive for large matrices. This implementation of the GPP algorithm can be found here.<ref>{{Cite web|url=https://github.com/sivaramambikasaran/ESS|title=sivaramambikasaran/ESS|website=GitHub|language=en|access-date=2018-04-05}}</ref> The key idea is that the dense matrix <math>A</math> can be converted into a sparser matrix of a larger size (see figure on the right), whose sparsity structure can be leveraged to reduce the computational complexity.\n\nThe fact that matrix <math>A</math> is a semi-separable matrix also forms the basis for celerite<ref>{{Cite web|url=https://celerite.readthedocs.io/en/stable/|title=celerite — celerite 0.3.0 documentation|website=celerite.readthedocs.io|language=en|access-date=2018-04-05}}</ref> library, which is a library for fast and scalable Gaussian Process (GP) Regression in one dimension<ref name=\":1\">{{Cite journal|last=Foreman-Mackey|first=Daniel|last2=Agol|first2=Eric|last3=Ambikasaran|first3=Sivaram|last4=Angus|first4=Ruth|date=2017|title=Fast and Scalable Gaussian Process Modeling with Applications to Astronomical Time Series|url=http://stacks.iop.org/1538-3881/154/i=6/a=220|journal=The Astronomical Journal|language=en|volume=154|issue=6|pages=220|doi=10.3847/1538-3881/aa9332|issn=1538-3881|arxiv=1703.09710|bibcode=2017AJ....154..220F}}</ref> with implementations in C++, Python, and Julia. The celerite method<ref name=\":1\" /> also provides an algorithm for generating samples from a high-dimensional distribution. The method has found attractive applications in a wide range of fields, especially in astronomical data analysis.<ref>{{Cite journal|last=Foreman-Mackey|first=Daniel|date=2018|title=Scalable Backpropagation for Gaussian Processes using Celerite|url=http://stacks.iop.org/2515-5172/2/i=1/a=31|journal=Research Notes of the AAS|language=en|volume=2|issue=1|pages=31|doi=10.3847/2515-5172/aaaf6c|issn=2515-5172|arxiv=1801.10156|bibcode=2018RNAAS...2a..31F}}</ref><ref>{{Cite book|title=Handbook of Exoplanets|last=Parviainen|first=Hannu|date=2018|publisher=Springer, Cham|isbn=9783319306483|pages=1–24|language=en|doi=10.1007/978-3-319-30648-3_149-1|chapter = Bayesian Methods for Exoplanet Science|arxiv = 1711.03329}}</ref>\n\n==References==\n{{Reflist}}.\n\n[[Category:Numerical linear algebra]]\n\n\n{{Signal-processing-stub}}"
    },
    {
      "title": "Samuelson–Berkowitz algorithm",
      "url": "https://en.wikipedia.org/wiki/Samuelson%E2%80%93Berkowitz_algorithm",
      "text": "In mathematics, the '''Samuelson–Berkowitz algorithm''' efficiently computes the [[characteristic polynomial]] of an <math>n\\times n</math> matrix who entries may be elements of any unital [[commutative ring]] without [[zero divisor]]s.\n\n==Description of the algorithm==\n\nThe Samuelson–Berkowitz algorithm applied to a matrix <math>A</math> produces a vector whose entries are the coefficient of the characteristic polynomial of <math>A</math>.  It computes this coefficients vector as a recursive product of Toeplitz matrices based on the principal submatrices of <math>A</math>\n\nLet <math>A_0</math> be an <math>n\\times n</math> matrix partitioned so that\n\n:<math> A_0 = \\left[ \\begin{array}{c|c}\n                    a_{1,1} & R \\\\ \\hline\n                    C & A_1\n                    \\end{array}  \\right] </math>\n\nThe <i>first principal submatrix</i> of <math>A_0</math> is the <math>(n-1)\\times(n-1)</math> matrix <math>A_1</math>.  Associate with <math>A_0</math> the <math>(n+1)\\times n</math> [[Toeplitz matrix]] <math>T_0</math>\ndefined by\n\n:<math> T_0 = \\left[ \\begin{array}{c}\n                    1 \\\\ \n                    -a_{1,1}\n                    \\end{array}  \\right] </math>\nif <math>A_0</math> is <math>1\\times 1</math>,\n\n:<math> T_0 = \\left[ \\begin{array}{c c}\n                    1 &0\\\\ \n                    -a_{1,1} & 1\\\\\n                    -RC & -a_{1,1}\n                    \\end{array}  \\right] </math>\nif <math>A_0</math> is <math>2\\times 2</math>,\nand in general\n\n:<math> T_0 = \\left[ \\begin{array}{c c c c c}\n                    1 &0&0&0&\\cdots\\\\ \n                    -a_{1,1} & 1&0&0&\\cdots\\\\\n                    -RC & -a_{1,1}&1 &0&\\cdots\\\\\n-RA_1C & -RC &-a_{1,1}&1 &\\cdots\\\\\n-RA_1^2C & -RA_1C &-RC &-a_{1,1}&\\cdots\\\\\n\\vdots &\\vdots&\\vdots &\\vdots&\\ddots\n                    \\end{array}  \\right] </math>\n\nThat is, all super diagonals of <math>T_0</math> consist of zeros, the main diagonal consists of <math>1</math>s, the first subdiagonal consists of <math>-a_{1,1}</math> and the <math>k</math>th subdiagonal\nconsists of <math>-RA_1^{k-2}C</math>.\n\nThe Toeplitz matrix <math>T_1</math> is the Toeplitz matrix associated with the first principal submatrix <math>A_1</math>, and so on.  The Samuelson–Berkowitz algorithm then states that the vector <math>v</math> defined by\n:<math> v=T_0T_1T_2\\cdots T_{n-1} </math>\ncontains the coefficients of the characteristic polynomial of <math>A_0</math>.\n\n==References==\n{{reflist}}\n\n<ref>Cook, Stephen and Soltys, Michael.  The Proof Complexity of Linear Algebra. 1993</ref>\n<ref>S.J. Berkowitz, On computing the determinant in small parallel time using a small number of processors, ACM, Information Processing Letters 18, 1984, pp. 147–150</ref>\n<ref>M. Keber, Division-Free computation of sub-resultants using Bezout matrices, Tech. Report MPI-I-2006-1-006, Saarbrucken, 2006</ref>\n\n[[Category:Linear algebra]]\n[[Category:Polynomials]]\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "SequenceL",
      "url": "https://en.wikipedia.org/wiki/SequenceL",
      "text": "{{Infobox programming language\n| name = SequenceL\n| logo = <!-- (filename) -->\n| logo caption = \n| screenshot = <!-- (filename) -->\n| screenshot caption = \n| paradigms = [[Parallel computing]], [[Functional programming|Functional]], [[Purely functional programming|Purely functional]], [[Declarative programming]]\n| family =  \n| designers = Dr. Daniel Cooke,<br/>Dr. Nelson Rushton,<br/>Dr. Brad Nemanich\n| developers = Texas Tech University,<br/>Texas Multicore Technologies\n| released = {{Start date and age|1989}}\n| latest release version = \n| latest release date = <!-- {{Start date and age|2016|MM|DD|df=yes/no}} -->\n| latest preview version = \n| latest preview date = <!-- {{Start date and age|2016|MM|DD|df=yes/no}} -->\n| typing = [[Type system#Static type checking|Static]], [[type inference]]\n| scope = \n| programming language = \n| platform = [[x86]], [[IBM POWER microprocessors|POWER]], [[ARM architecture|ARM]]\n| operating system = [[Microsoft Windows|Windows]], [[macOS]], [[Linux]]\n| file ext = \n| file format = <!-- or: | file formats = -->\n| website = {{URL|texasmulticore.com}}{{Dead link|date=May 2018}}\n| implementations = \n| dialects = \n| influenced by = \n| influenced = \n| license = [[Proprietary software|Proprietary]]<ref name=Licensing>{{cite web |url=https://texasmulticore.com/products/sequencel-licenses/ |title=SequenceL Licensing |access-date=2017-01-26 |archive-url=https://web.archive.org/web/20170202045317/https://texasmulticore.com/products/sequencel-licenses/ |archive-date=2017-02-02 |dead-url=yes |df= }}</ref>\n}}\n'''SequenceL''' is a general purpose [[functional programming]] language and [[automatic parallelization|auto-parallelizing]] ([[Parallel computing]]) compiler and tool set, whose primary design objectives are performance on [[multi-core processor]] hardware, ease of programming, platform portability/optimization, and code clarity and readability.  Its main advantage is that it can be used to write straightforward code that automatically takes full advantage of all the processing power available, without [[programmer]]s needing to be concerned with identifying [[Parallel computing|parallelisms]], specifying [[Automatic vectorization|vectorization]], avoiding [[race condition]]s, and other challenges of manual [[Directive pragma|directive-based programming]] approaches such as [[OpenMP]].\n\nPrograms written in SequenceL can be compiled to [[Thread (computing)|multithreaded]] code that runs in parallel, with no explicit indications from a programmer of how or what to parallelize. {{As of|2015}}, versions of the SequenceL [[compiler]] generate parallel code in [[C++]] and [[OpenCL]], which allows it to work with most popular programming languages, including [[C (programming language)|C]], C++, [[C Sharp (programming language)|C#]], [[Fortran]], [[Java (programming language)|Java]], and [[Python (programming language)|Python]].  A platform-specific runtime manages the threads safely, automatically providing parallel performance according to the number of cores available, currently supporting [[x86]], [[POWER8]], and [[ARM architecture|ARM]] platforms.\n\n==History==\nSequenceL was initially developed over a 20-year period starting in 1989, mostly at [[Texas Tech University]].  Primary funding was from [[NASA]], which originally wanted to develop a specification language which was \"self-verifying\"; that is, once written, the requirements could be ''executed'', and the results verified against the desired outcome.\n\nThe principal researcher on the project was initially Dr. Daniel Cooke,<ref>{{Cite web |url=http://www.texasmulticoretechnologies.com/about/inventors/ |title=Dr. Daniel Cooke at Texas Multicore Technologies |access-date=2016-02-24 |archive-url=https://web.archive.org/web/20160304111952/http://www.texasmulticoretechnologies.com/about/inventors/ |archive-date=2016-03-04 |dead-url=yes |df= }}</ref> who was soon joined by Dr. Nelson Rushton (another Texas Tech professor) and later Dr. Brad Nemanich (then a PhD student under Cooke). The goal of creating a language that was simple enough to be readable, but unambiguous enough to be executable, drove the inventors to settle on a [[Functional programming|functional]], [[Declarative programming|declarative]] language approach, where a programmer describes desired results, rather than the means to achieve them.  The language is then free to solve the problem in the most efficient manner that it can find.\n\nAs the language evolved, the researchers developed new computational approaches, including ''consume-simplify-produce'' (CSP).<ref>{{Cite web |url=https://texasmulticore.com/wp-content/uploads/2016/05/2004-ttudamp.pdf |title=Consume-simplify-produce (CSP) |access-date=2017-01-26 |archive-url=https://web.archive.org/web/20170202050125/https://texasmulticore.com/wp-content/uploads/2016/05/2004-ttudamp.pdf |archive-date=2017-02-02 |dead-url=yes |df= }}</ref>  In 1998, research began to apply SequenceL to [[parallel computing]].  This culminated in 2004 when it took its more complete form with the addition of the ''normalize-transpose'' (NT) semantic,<ref>{{Citation |last1=Nemanich |first1=Brad |last2=Cooke |first2=Daniel |last3=Rushton |first3=Nelson |title=SequenceL: Transparency And Multi-Core Parallelisms |series=DAMP '10 Proceedings of the 5th ACM SIGPLAN workshop on Declarative Aspects of Multicore Programming |publisher=ACM |year=2010 |pages=45–52 |location=New York, NY, US |url=https://texasmulticore.com/wp-content/uploads/2016/05/2004-ttudamp.pdf |access-date=2017-01-26 |archive-url=https://web.archive.org/web/20170202050125/https://texasmulticore.com/wp-content/uploads/2016/05/2004-ttudamp.pdf |archive-date=2017-02-02 |dead-url=yes |df= }}</ref><ref>{{Citation |last1=Cooke |first1=Daniel |last2=Rushton |first2=Nelson |last3=Nemanich |first3=Brad |last4=Watson |first4=Robert G. |last5=Andersen |first5=Per |title=Normalize, Transpose, and Distribute: An Automatic Approach for Handling Nonscalars |journal=ACM Transactions on Programming Languages and Systems |volume=30 |issue=2 |pages=1–49 |date=March 2008 |doi=10.1145/1330017.1330020 |url=http://dl.acm.org/citation.cfm?id=1330020&dl=ACM&coll=DL&CFID=893264698&CFTOKEN=93567450}}</ref> which coincided with the major vendors of [[central processing unit]]s (CPUs) making a major shift to [[multi-core processor]]s rather than continuing to increase clock speeds. NT is the semantic work-horse, being used to simplify and decompose structures, based on a [[dataflow]]-like execution strategy similar to GAMMA<ref>{{Citation |last1=Banater |first1=J-P |last2=Le Metayer |first2=D. |title=Programming by Multiset Transformation |journal=Communications of the ACM |date=January 1993 |volume=36 |issue=1 |pages=98–111 |doi= 10.1145/151233.151242}}</ref> and NESL.<ref>{{Citation |last1=Blelloch |first1=Guy |title=Programming Parallel Algorithms |journal=Communications of the ACM |date=March 1996 |volume=39 |issue= 3 |pages=85–97 |doi=10.1145/227234.227246|citeseerx=10.1.1.141.5884 }}</ref> The NT semantic achieves a goal similar to that of the Lämmel and Peyton-Jones’ boilerplate elimination.<ref>{{Citation |last1=Lämmel |first1=Ralf |last2=Peyton-Jones |first2=Simon |title=Scrap your boilerplate: a practical design pattern for generic programming |journal=Proceedings of TLDI 2003 |year=2003}}</ref><ref>{{Citation |last1=Lämmel |first1=Ralf |last2=Peyton-Jones |first2=Simon |title=Scrap more boilerplate: reflection, zips, and generalised casts |journal=Proceedings of ICFP 2004 |year=2004}}</ref> \nAll other features of the language are definable from these two laws - including [[Recursion (computer science)|recursion]], subscripting structures, function references, and evaluation of function bodies.<ref>{{Citation |last1=Cooke |first1=Daniel |last2=Rushton |first2=Nelson |title=Iterative and Parallel Algorithm Design from High Level Language Traces |journal=ICCS'05 Proceedings of the 5th International Conference on Computational Science |date=January 1993 |volume=Part III |pages=891–894 |doi=10.1007/11428862_132 |isbn=978-3-540-26044-8 |url=https://texasmulticore.com/wp-content/uploads/2016/05/2005-Iterative-and-Parallel-Algorithm-Design-from-High.pdf |access-date=2017-01-26 |archive-url=https://web.archive.org/web/20170202044854/https://texasmulticore.com/wp-content/uploads/2016/05/2005-Iterative-and-Parallel-Algorithm-Design-from-High.pdf |archive-date=2017-02-02 |dead-url=yes |df= }}</ref><ref>{{Citation |last1=Cooke |first1=Daniel |last2=Rushton |first2=Nelson |title=SequenceL – An Overview of a Simple Language |journal=Proceedings of the 2005 International Conference on Programming Languages and Compilers, PLC 2005 |date=June 27–30, 2005 }}</ref>\n\nThough it was not the original intent, these new approaches allowed the language to parallelize a large fraction of the operations it performed, transparently to the programmer. In 2006, a prototype auto-parallelizing compiler was developed at Texas Tech University. In 2009, Texas Tech licensed the intellectual property to Texas Multicore Technologies (TMT),<ref>[http://www.texasmulticoretechnologies.com Texas Multicore Technologies, Inc.]</ref> for follow-on commercial development. In January 2017 TMT released v3, which includes a free Community Edition for download in addition to the commercial Professional Edition.\n\n==Design==\nSequenceL is designed to be as simple as possible to learn and use, focusing on algorithmic code where it adds value, e.g., the inventors chose not to reinvent I/O since C handled that well. As a result, the full [https://web.archive.org/web/20170202050741/https://texasmulticore.com/documentation/3.0/0710language_ref.html language reference for SequenceL] is only 40 pages, with copious examples, and its formal grammar has around 15 production rules.<ref>{{Citation |last1=Nemanich |first1=Brad |last2=Cooke |first2=Daniel |last3=Rushton |first3=Nelson |title=SequenceL: Transparency And Multi-Core Parallelisms |series=DAMP '10 Proceedings of the 5th ACM SIGPLAN workshop on Declarative Aspects of Multicore Programming |publisher=ACM |year=2010 |pages=45–52 |location=New York, NY, US |url=https://texasmulticore.com/wp-content/uploads/2016/05/2004-ttudamp.pdf |access-date=2017-01-26 |archive-url=https://web.archive.org/web/20170202050125/https://texasmulticore.com/wp-content/uploads/2016/05/2004-ttudamp.pdf |archive-date=2017-02-02 |dead-url=yes |df= }}</ref>\n\nSequenceL is strictly evaluated (like [[Lisp (programming language)|Lisp]]), statically typed with [[type inference]] (like [[Haskell (programming language)|Haskell]]), and uses a combination of infix and prefix operators that resemble standard, informal mathematical notation (like [[C (programming language)|C]], [[Pascal (programming language)|Pascal]], [[Python (programming language)|Python]], etc.).  It is a purely declarative language, meaning that a programmer defines functions, in the mathematical sense, without giving instructions for their implementation. For example, the mathematical definition of matrix multiplication is as follows:\n\n:The product of the ''m''×''p'' matrix ''A'' with the ''p''×''n'' matrix ''B'' is the ''m''×''n'' matrix whose (''i'',''j'')'th entry is\n::<math>\\sum_{k=1}^p A(i,k)B(k,j)</math>\n\nThe SequenceL definition mirrors that definition more or less exactly:\n    matmul(A(2), B(2)) [i,j] := \n        let k := 1...size(B); \n        in  sum( A[i,k] * B[k,j] );\n\nThe subscripts following each parameter ''A'' and ''B'' on the left hand side of the definition indicate that ''A'' and ''B'' are depth-2 structures (i.e., lists of lists of scalars), which are here thought of as matrices. From this formal definition, SequenceL infers the dimensions of the defined product from the formula for its (''i'', ''j'')'th entry (as the set of pairs (''i'', ''j'') for which the right hand side is defined) and computes each entry by the same formula as in the informal definition above. Notice there are no explicit instructions for iteration in this definition, or for the order in which operations are to be carried out. Because of this, the SequenceL compiler can perform operations in any order (including parallel order) which satisfies the defining equation.  In this example, computation of coordinates in the product will be parallelized in a way that, for large matrices, scales linearly with the number of processors.\n\nAs noted above, SequenceL has no built-in constructs for [[input/output]] (I/O) since it was designed to work in an additive manner with other programming languages. The decision to compile to multithreaded C++ and support the 20+ Simplified Wrapper and Interface Generator ([[SWIG]]) languages (C, C++, C#, Java, Python, etc.) means it easily fits into extant design flows, training, and tools. It can be used to enhance extant applications, create multicore libraries, and even create standalone applications by linking the resulting code with other code which performs I/O tasks. SequenceL functions can also be queried from an [[Interpreter (computing)|interpreter]] with given inputs, like Python and other interpreted languages.\n\n==Normalize–transpose==\nThe main non-scalar construct of SequenceL is the sequence, which is essentially a list. Sequences may be nested to any level. To avoid the routine use of recursion common in many purely functional languages, SequenceL uses a technique termed ''normalize–transpose'' (NT), in which scalar operations are automatically distributed over elements of a sequence.<ref>{{Citation |last1=Cooke |first1=Daniel |last2=Rushton |first2=Nelson |title=SequenceL – An Overview of a Simple Language |journal=Proceedings of the 2005 International Conference on Programming Languages and Compilers, PLC 2005 |date=June 27–30, 2005 }}</ref> For example, in SequenceL we have\n:<math>[1,2,3] + 10 == [11,12,13]</math>\nThis results not from overloading the '+' operator, but from the effect of NT that extends to all operations, both built-in and user-defined.\nAs another example, if f() is a 3-argument function whose arguments are scalars, then for any appropriate x and z we will have\n:<math>f(x,[1,2,3],z) == [f(x,1,z), f(x,2,z), f(x,3,z)]</math>\nThe NT construct can be used for multiple arguments at once, as in, for example\n:<math>[1,2,3] + [10,20,30] == [11,22,33]</math>\nIt also works when the expected argument is a non-scalar of any type T, and the actual argument is a list of objects of type T (or, in greater generality, any data structure whose coordinates are of type T). For example, if '''''A''''' is a matrix and '''''X<sub>s</sub>''''' is a list of matrices [X<sub>1</sub>, ..., X<sub>n</sub>], and given the above definition of matrix multiply, in SequenceL we would have\n\n    matmul(A,X<sub>s</sub>) = [matmul(A,X<sub>1</sub>),...,matmul(A,X<sub>n</sub>)]\n\nAs a rule, NTs eliminate the need for iteration, recursion, or high level functional operators to \n#do the same things to every member of a data structure, or to\n#process corresponding parts of similarly shaped structures together.\nThis tends to account for most uses of iteration and recursion.\n\n==Example: prime numbers==\nA good example that demonstrates the above concepts would be in finding prime numbers.  A [[prime number]] is defined as\n\n:''An integer greater than 1, with no positive divisors other than itself and 1.''\n\nSo a positive integer ''z'' is prime if no numbers from 2 through ''z''-1, inclusive, divide evenly.  SequenceL allows this problem to be programmed by literally transcribing the above definition into the language.\n\nIn SequenceL, a sequence of the numbers from 2 through ''z''-1, inclusive, is just (2...(''z''-1)), so a program to find all of the primes between 100 and 200 can be written:\n\n    prime(z) := z when none(z mod (2...(z-1)) = 0);\n\nWhich, in English just says,\n\n:''...return the argument if none of the numbers between 2, and 1 less than the argument itself, divide evenly into it.''\n\nIf that condition isn’t met, the function returns nothing.  As a result, running this program yields\n\n    cmd:>prime(17)\n    17\n    cmd:>prime(18)\n    empty\n\nThe string \"between 100 and 200\" doesn’t appear in the program.  Rather, a programmer will typically pass that part in as the argument.  Since the program expects a scalar as an argument, passing it a sequence of numbers instead will cause SequenceL to perform the operation on each member of the sequence automatically.  Since the function returns empty for failing values, the result will be the input sequence, but filtered to return only those numbers that satisfy the criteria for primes:\n\n    cmd:>prime(100...200)\n    [101,103,107,109,113,127,131,137,139,149,151,157,163,167,173,179,181,191,193,197,199]\n\nIn addition to solving this problem with a very short and readable program, SequenceL’s evaluation of the nested sequences would all be performed in parallel.\n\n==Components==\nThe following software components are available and supported by TMT for use in writing SequenceL code.  All components are available on [[x86]] platforms running [[Microsoft Windows|Windows]], [[macOS]], and most varieties of [[Linux]] (including [[CentOS]], [[RedHat]], [[OpenSUSE]], and [[Ubuntu (operating system)|Ubuntu]]), and on [[ARM architecture|ARM]] and [[IBM POWER microprocessors|IBM POWER]] platforms running most varieties of [[Linux]].\n\n===Interpreter===\nA [[Command-line interface|command-line]] [[Interpreter (computing)|interpreter]] allows writing code directly into a command shell, or loading code from prewritten text files.  This code can be executed, and the results evaluated, to assist in checking code correctness, or finding a quick answer. It is also available via the popular [[Eclipse (software)|Eclipse]] [[integrated development environment]] (IDE). Code executed in the interpreter does not run in parallel; it executes in one thread.\n\n===Compiler===\nA command-line [[compiler]] reads SequenceL code and generates highly parallelized, [[Automatic vectorization|vectorized]], C++, and optionally OpenCL, which must be linked with the SequenceL runtime library to execute.\n\n===Runtime===\nThe runtime environment is a pre-compiled set of libraries which works with the compiled parallelized C++ code to execute optimally on the target platform.  It builds on Intel Threaded Building Blocks (TBB)<ref>[https://www.threadingbuildingblocks.org/ Intel Threaded Building Blocks (TBB)]</ref> and handles things such as cache optimization, memory management, work queues-stealing, and performance monitoring.\n\n===Eclipse IDE plug-in with debugger===\nAn [[Eclipse (software)|Eclipse]] [[integrated development environment]] [[Plug-in (computing)|plug-in]] provides standard editing abilities (function rollup, chromacoding, etc.), and a SequenceL debugging environment.  This plug-in runs against the SequenceL Interpreter, so cannot be used to debug the multithreaded code; however, by providing automatic parallelization, debugging of parallel SequenceL code is really verifying correctness of sequential SequenceL code.  That is, if it runs correctly sequentially, it should run correctly in parallel – so debugging in the interpreter is sufficient.\n\n===Libraries===\nVarious math and other standard function libraries are included as SequenceL source code to streamline the programming process and serve as best practice examples.  These may be imported, in much the same way that C or C++ libraries are #included.\n\n==See also==\n* [[Parallel computing]]\n* [[Automatic parallelization tool]]\n* [[Multi-core processor]]\n* [[Multiprocessing]]\n* [[Functional programming]]\n* [[Purely functional programming]]\n* [[Declarative programming]]\n* [[Comparison of programming paradigms]]\n* [[Automatic vectorization]]\n* [[Simon Peyton Jones]]\n* [[Rosetta Code]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* {{Official website|texasmulticore.com}}{{Dead link|date=May 2018}} Texas Multicore Technologies\n* [https://web.archive.org/web/20170415165711/https://texasmulticore.com/technology/multicore-performance/ Why SequenceL Works]\n* [https://web.archive.org/web/20170415181140/https://texasmulticore.com/technology/openmp-comparison/ OpenMP compared to SequenceL]\n* [https://web.archive.org/web/20170412062102/https://texasmulticore.com/products/features/ SequenceL Features]\n* [https://web.archive.org/web/20170412063032/https://texasmulticore.com/technology/patented-auto-parallel/ Overview: Patented Automatic Parallelization in SequenceL]\n* [https://www.youtube.com/channel/UCb6JyUsAuS_vmBAE3gXVKzQ YouTube: Texas Multicore Technologies]\n* [https://web.archive.org/web/20170107005833/https://texasmulticore.com/resources/downloads/ Free Downloads]\n* [https://web.archive.org/web/20170424091304/https://texasmulticore.com/documentation/ Programmer Resources and Education]\n* [https://web.archive.org/web/20170116162400/https://texasmulticore.com/wp-content/uploads/2016/05/2008-ACM-paper_NTD-Auto-Approach-for-Handling-Nonscalars-1.pdf Normalize, Transpose and Distribute: An Automatic Approach for Handling Nonscalars]\n* [http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO1&Sect2=HITOFF&d=PALL&p=1&u=%2Fnetahtml%2FPTO%2Fsrchnum.htm&r=1&f=G&l=50&s1=8839212.PN.&OS=PN/8839212&RS=PN/8839212/ US Patent 8,839,212, Method, apparatus and computer program product for automatically generating a computer program using consume, simplify and produce semantics with normalize, transpose and distribute operations]\n* [https://rosettacode.org/wiki/Category:SequenceL SequenceL examples on Rosetta Code wiki]\n\n[[Category:High-level programming languages]]\n[[Category:Parallel computing]]\n[[Category:Array programming languages]]\n[[Category:Cross-platform software]]\n[[Category:Declarative programming languages]]\n[[Category:Functional programming]]\n[[Category:Functional languages]]\n[[Category:Statically typed programming languages]]\n[[Category:Heterogeneous computing]]\n[[Category:Concurrent programming languages]]\n[[Category:Mathematical software]]\n[[Category:Numerical analysis software for Windows]]\n[[Category:Numerical analysis software for MacOS]]\n[[Category:Numerical analysis software for Linux]]\n[[Category:Numerical linear algebra]]\n[[Category:Numerical programming languages]]\n[[Category:Numerical software]]\n[[Category:Science software for Windows]]\n[[Category:Science software for MacOS]]\n[[Category:Science software for Linux]]\n[[Category:GPGPU]]"
    },
    {
      "title": "Singular value decomposition",
      "url": "https://en.wikipedia.org/wiki/Singular_value_decomposition",
      "text": "{{short description|Matrix decomposition}}\n{{Use dmy dates|date=August 2012}}\n[[File:Singular value decomposition.gif|thumb|right|280px|Visualization of the SVD of a 2D, real [[Shear mapping|shearing matrix]] {{math|'''M'''}}. First, we see the [[unit disc]] in blue together with the two [[standard basis|canonical unit vectors]]. We then see the action of {{math|'''M'''}}, which distorts the disk to an [[ellipse]]. The SVD decomposes {{math|'''M'''}} into three simple transformations: an initial [[Rotation matrix|rotation]] {{math|'''V'''<sup>∗</sup>}}, a [[Scaling matrix|scaling]] {{math|'''Σ'''}} along the coordinate axes, and a final rotation {{math|'''U'''}}. The lengths {{math|''σ''<sub>1</sub>}} and {{math|''σ''<sub>2</sub>}} of the [[Ellipse#Elements of an ellipse|semi-axes]] of the ellipse are the [[singular value]]s of {{math|'''M'''}}, namely {{math|'''Σ'''<sub>1,1</sub>}} and {{math|'''Σ'''<sub>2,2</sub>}}.]]\n[[File:Singular_value_decomposition_visualisation.svg|thumb|Visualisation of the matrix multiplications in singular value decomposition]]\n\nIn [[linear algebra]], the '''singular value decomposition''' ('''SVD''') is a [[Matrix decomposition|factorization]] of a [[real number|real]] or [[complex number|complex]] [[matrix (mathematics)|matrix]].  It is the generalization of the [[eigendecomposition]] of a [[positive-semidefinite matrix|positive semidefinite]] [[normal matrix]] (for example, a [[symmetric matrix]] with positive eigenvalues) to any <math>m \\times n</math> matrix via an extension of the [[polar decomposition#Matrix polar decomposition|polar decomposition]].  It has many useful applications in [[signal processing]] and [[statistics]].\n\nFormally, the singular value decomposition of an <math>m \\times n</math> real or complex matrix <math>\\mathbf{M}</math> is a factorization of the form <math>\\mathbf{U\\Sigma V^*}</math>, where <math>\\mathbf{U}</math> is an <math>m \\times m</math> real or complex [[unitary matrix]], <math>\\mathbf{\\Sigma}</math> is an <math>m \\times n</math> [[rectangular diagonal matrix]] with non-negative real numbers on the diagonal, and <math>\\mathbf{V}</math> is an <math>n \\times n</math> real or complex unitary matrix. The diagonal entries <math>\\sigma_i</math> of <math>\\mathbf{\\Sigma}</math> are known as the '''[[singular value]]s''' of <math>\\mathbf{M}</math>. The columns of <math>\\mathbf{U}</math> and the columns of <math>\\mathbf{V}</math> are called the '''left-singular vectors''' and '''right-singular vectors''' of <math>\\mathbf{M}</math>, respectively.\n\nThe singular value decomposition can be computed using the following observations:\n* The left-singular vectors of {{math|'''M'''}} are a set of [[orthonormal]] [[eigenvectors]] of {{math|'''MM'''<sup>∗</sup>}}.\n* The right-singular vectors of {{math|'''M'''}} are a set of orthonormal eigenvectors of {{math|'''M'''<sup>∗</sup>'''M'''}}.\n* The non-zero singular values of {{math|'''M'''}} (found on the diagonal entries of {{math|'''Σ'''}}) are the square roots of the non-zero [[eigenvalues]] of both {{math|'''M'''<sup>∗</sup>'''M'''}} and {{math|'''MM'''<sup>∗</sup>}}.\n\nApplications that employ the SVD include computing the [[Moore–Penrose pseudoinverse|pseudoinverse]], [[least squares]] fitting of data, multivariable control, matrix approximation, and determining the [[rank of a matrix|rank]], [[range of a matrix|range]], and [[kernel (matrix)|null space]] of a matrix.\n\n== Statement of the theorem ==\nSuppose {{math|'''M'''}} is an {{math|''m'' × ''n''}} [[matrix (mathematics)|matrix]] whose entries come from the [[field (mathematics)|field]] {{mvar|K}}, which is either the field of [[real number]]s or the field of [[complex number]]s. Then the singular value decomposition of {{math|'''M'''}} exists, and is a factorization of the form\n\n: <math>\\mathbf{M} = \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^*</math>\n\nwhere \n* {{math|'''U'''}} is an {{math|''m'' × ''m''}} [[unitary matrix]] over {{mvar|K}} (if {{math|''K'' {{=}} }}<math>\\mathbb{R}</math>, unitary matrices are [[orthogonal matrix|orthogonal matrices]]),\n* {{math|'''Σ'''}} is a [[rectangular diagonal matrix|diagonal]] {{math|''m'' × ''n''}} matrix with non-negative real numbers on the diagonal,\n* {{math|'''V'''}} is an {{math|''n'' × ''n''}} [[unitary matrix]] over {{mvar|K}}, and {{math|'''V'''<sup>∗</sup>}} is the [[conjugate transpose]] of {{math|'''V'''}}.\n\nThe diagonal entries {{mvar|σ<sub>i</sub>}} of {{math|'''Σ'''}} are known as the '''[[singular value]]s''' of {{math|'''M'''}}. A common convention is to list the singular values in descending order. In this case, the diagonal matrix, {{math|'''Σ'''}}, is uniquely determined by {{math|'''M'''}} (though not the matrices {{math|'''U'''}} and {{math|'''V'''}} if {{math|'''M'''}} is not square, see below).\n\n== Intuitive interpretations ==\n[[File:Singular-Value-Decomposition.svg|thumb|{{ubl\n| '''Upper left:''' The unit disc with the two canonical unit vectors.\n| '''Upper right:''' Unit disc transformed with M and singular values {{math|''σ''<sub>1</sub>}} and {{math|''σ''<sub>2</sub>}} indicated.\n| '''Lower left:''' The action of {{math|'''V'''<sup>∗</sup>}} on the unit disc. This is just a rotation.\n| '''Lower right:''' The action of {{math|'''ΣV'''<sup>∗</sup>}} on the unit disc. &Sigma; scales in vertically and horizontally.\n}}{{paragraph}}\nIn this special case, the singular values are ''&phi;'' and 1/''&phi;'' where ''&phi;'' is the [[golden ratio]]. {{math|'''V'''<sup>∗</sup>}} is a (counter clockwise) rotation by an angle alpha where alpha satisfies tan(''&alpha;'') = −''&phi;''. {{math|'''U'''}} is a rotation by an angle beta with tan(''&beta;'') = ''&phi;''&nbsp;−&nbsp;1\n]]\n\n=== Rotation, scaling ===\nIn the special, yet common case when {{math|'''M'''}} is an {{math|''m'' × ''m''}} real [[square matrix]] with positive [[determinant]]: {{math|'''U''', '''V'''<sup>∗</sup>}}, and {{math|'''Σ'''}} are real {{math|''m'' × ''m''}} matrices as well. {{math|'''Σ'''}} can be regarded as a [[scaling matrix]], and {{math|'''U''', '''V'''<sup>∗</sup>}} can be viewed as [[rotation matrix|rotation matrices]]. Thus the expression {{math|'''UΣV'''<sup>∗</sup>}} can be intuitively interpreted as a [[Function composition|composition]] of three geometrical [[Transformation (geometry)|transformations]]: a [[Coordinate rotations and reflections|rotation or reflection]], a [[Scaling (geometry)|scaling]], and another rotation or reflection. For instance, the figure explains how a [[shear matrix]] can be described as such a sequence.\n\nUsing the [[polar decomposition#Matrix polar decomposition|polar decomposition]] theorem, we can also consider {{math|'''M''' {{=}} '''RP'''}} as the composition of a stretch (positive definite matrix {{math|'''P''' {{=}} '''VΣV'''<sup>∗</sup>}}) with eigenvalue scale factors {{math|σ<sub>''i''</sub>}} along the orthogonal eigenvectors {{math|'''V'''<sub>''i''</sub>}} of {{math|'''P'''}}, followed by a single rotation (unitary matrix {{math|'''R''' {{=}} '''UV'''<sup>∗</sup>}}).  If the rotation is done first, {{math|'''M''' {{=}} '''P'''{{'}}'''R'''}}, then {{math|'''R'''}} is the same and {{math|'''P'''{{'}} {{=}} '''UΣU'''<sup>∗</sup>}} has the same eigenvalues, but is stretched along different (post-rotated) directions.  This shows that the SVD is a generalization of the eigenvalue decomposition of pure stretches in orthogonal directions (symmetric matrix {{math|'''P'''}}) to arbitrary matrices ({{math|'''M''' {{=}} '''RP'''}}) which both stretch and rotate.\n\n=== Singular values as semiaxis of an ellipse or ellipsoid ===\nAs shown in the figure, the [[singular values]] can be interpreted as the semiaxis of an [[ellipse]] in 2D. This concept can be generalized to {{mvar|n}}-dimensional [[Euclidean space]], with the singular values of any {{math|''n'' × ''n''}} [[square matrix]] being viewed as the semiaxis of an {{mvar|n}}-dimensional [[ellipsoid]]. Similarly, the singular values of any {{math|''m'' × ''n''}} matrix can be viewed as the semiaxis of an {{mvar|n}}-dimensional [[ellipsoid]] in {{mvar|m}}-dimensional space, for example as an ellipse in a (tilted) 2D plane in a 3D space. See [[#Geometric meaning|below]] for further details.\n\n=== The columns of ''U'' and ''V'' are orthonormal bases ===\nSince {{math|'''U'''}} and {{math|'''V'''<sup>∗</sup>}} are unitary, the columns of each of them form a set of [[orthonormal vectors]], which can be regarded as [[basis vectors]]. The matrix {{math|'''M'''}} maps the basis vector {{math|'''V'''<sub>i</sub>}} to the stretched unit vector {{math|σ<sub>i</sub> '''U'''<sub>i</sub>}} (see [[#Geometric meaning|below]] for further details).  By the definition of a unitary matrix, the same is true for their conjugate transposes {{math|'''U'''<sup>∗</sup>}} and {{math|'''V'''}}, except the geometric interpretation of the singular values as stretches is lost. In short, the columns of {{math|'''U''', '''U'''<sup>∗</sup>, '''V'''}}, and {{math|'''V'''<sup>∗</sup>}} are [[Orthonormal basis|orthonormal bases]]. When the <math>\\mathbf{M}</math> is a [[normal matrix]], {{math|'''U'''}} and {{math|'''V'''<sup>∗</sup>}} reduce to the unitary used to diagonalize <math>\\mathbf{M}</math>. However, when <math>\\mathbf{M}</math> is not normal but still [[diagonalizable]], its [[eigendecomposition]] and singular value decomposition are distinct.\n\n=== Geometric meaning ===\nBecause {{math|'''U'''}} and {{math|'''V'''}} are unitary, we know that the columns {{math|'''U'''<sub>1</sub>, ..., '''U'''<sub>''m''</sub>}} of {{math|'''U'''}} yield an [[orthonormal basis]] of {{mvar|K<sup>m</sup>}} and the columns {{math|'''V'''<sub>1</sub>, ..., '''V'''<sub>''n''</sub>}} of {{math|'''V'''}} yield an orthonormal basis of {{mvar|K<sup>n</sup>}} (with respect to the standard [[scalar product]]s on these spaces).\n\nThe [[linear transformation]]\n\n:<math>\\begin{cases} T : K^n \\to K^m \\\\ x \\mapsto \\mathbf{M}x \\end{cases}</math>\n\nhas a particularly simple description with respect to these orthonormal bases: we have\n\n:<math>T(\\mathbf{V}_i) = \\sigma_i \\mathbf{U}_i, \\qquad i = 1, \\ldots, \\min(m, n),</math>\n\nwhere {{mvar|σ<sub>i</sub>}} is the {{mvar|i}}-th diagonal entry of {{math|'''Σ'''}}, and {{math|''T''('''V'''<sub>''i''</sub>) {{=}} 0}} for {{math|''i'' > min(''m'',''n'')}}.\n\nThe geometric content of the SVD theorem can thus be summarized as follows: for every linear map {{math|''T'' : ''K<sup>n</sup>'' → ''K<sup>m</sup>''}} one can find orthonormal bases of {{mvar|K<sup>n</sup>}} and {{mvar|K<sup>m</sup>}} such that {{mvar|T}} maps the {{mvar|i}}-th basis vector of {{mvar|K<sup>n</sup>}} to a non-negative multiple of the {{mvar|i}}-th basis vector of {{mvar|K<sup>m</sup>}}, and sends the left-over basis vectors to zero. With respect to these bases, the map {{mvar|T}} is therefore represented by a diagonal matrix with non-negative real diagonal entries.\n\nTo get a more visual flavour of singular values and SVD factorization — at least when working on real vector spaces — consider the sphere {{mvar|S}} of radius one in {{math|'''R'''<sup>''n''</sup>}}. The linear map {{mvar|T}} maps this sphere onto an [[ellipsoid]] in {{math|'''R'''<sup>''m''</sup>}}. Non-zero singular values are simply the lengths of the [[Semi-minor axis|semi-axes]] of this ellipsoid. Especially when {{math|''n'' {{=}} ''m''}}, and all the singular values are distinct and non-zero, the SVD of the linear map {{mvar|T}} can be easily analysed as a succession of three consecutive moves: consider the ellipsoid {{math|''T''(''S'')}} and specifically its axes; then consider the directions in {{math|'''R'''<sup>''n''</sup>}} sent by {{mvar|T}} onto these axes. These directions happen to be mutually orthogonal. Apply first an isometry {{math|'''V'''<sup>∗</sup>}} sending these directions to the coordinate axes of {{math|'''R'''<sup>''n''</sup>}}. On a second move, apply an [[endomorphism]] {{math|'''D'''}} diagonalized along the coordinate axes and stretching or shrinking in each direction, using the semi-axes lengths of {{math|''T''(''S'')}} as stretching  coefficients. The composition {{math|'''D''' ∘ '''V'''<sup>∗</sup>}} then sends the unit-sphere onto an ellipsoid isometric to {{math|''T''(''S'')}}. To define the third and last move {{math|'''U'''}}, apply an isometry to this ellipsoid so as to carry it over {{math|''T''(''S'')}}. As can be easily checked, the composition {{math|'''U''' ∘ '''D''' ∘ '''V'''<sup>∗</sup>}} coincides with {{mvar|T}}.\n\n== Example ==\nConsider the {{math|4 × 5}} matrix\n\n:<math>\\mathbf{M} = \\begin{bmatrix}\n                      1 & 0 & 0 & 0 & 2 \\\\\n                      0 & 0 & 3 & 0 & 0 \\\\\n                      0 & 0 & 0 & 0 & 0 \\\\\n                      0 & 2 & 0 & 0 & 0\n                    \\end{bmatrix}\n</math>\n\nA singular value decomposition of this matrix is given by {{math|'''UΣV'''<sup>∗</sup>}}\n\n:<math>\\begin{align}\n  \\mathbf{U} &= \\begin{bmatrix}\n                  \\color{Green}0 & \\color{Blue}0 & \\color{Cyan}1 & \\color{Emerald}0 \\\\\n                  \\color{Green}0 & \\color{Blue}1 & \\color{Cyan}0 & \\color{Emerald}0 \\\\\n                  \\color{Green}0 & \\color{Blue}0 & \\color{Cyan}0 & \\color{Emerald}-1 \\\\\n                  \\color{Green}1 & \\color{Blue}0 & \\color{Cyan}0 & \\color{Emerald}0\n                \\end{bmatrix} \\\\[6pt]\n\n  \\boldsymbol{\\Sigma} &= \\begin{bmatrix}\n                           2 & 0 &        0 &                    0  & \\color{Gray}\\mathit{0} \\\\\n                           0 & 3 &        0 &                    0  & \\color{Gray}\\mathit{0} \\\\\n                           0 & 0 & \\sqrt{5} &                    0  & \\color{Gray}\\mathit{0} \\\\\n                           0 & 0 &        0 & \\color{Red}\\mathbf{0} & \\color{Gray}\\mathit{0}\n                         \\end{bmatrix} \\\\[6pt]\n\n \\mathbf{V}^* &= \\begin{bmatrix}\n                   \\color{Violet}0 & \\color{Plum}1 & \\color{Magenta}0 & \\color{Orchid}0 &          \\color{Purple}0 \\\\\n                   \\color{Violet}0 & \\color{Plum}0 & \\color{Magenta}1 & \\color{Orchid}0 &          \\color{Purple}0 \\\\\n                   \\color{Violet}\\sqrt{0.2} & \\color{Plum}0 & \\color{Magenta}0 & \\color{Orchid}0 & \\color{Purple}\\sqrt{0.8} \\\\\n                   \\color{Violet}0 & \\color{Plum}0 & \\color{Magenta}0 & \\color{Orchid}1 &          \\color{Purple}0 \\\\\n                   \\color{Violet} - \\sqrt{0.8} & \\color{Plum}0 & \\color{Magenta}0 & \\color{Orchid}0 & \\color{Purple}\\sqrt{0.2}\n                 \\end{bmatrix}\n\\end{align}</math>\n\nNotice {{math|'''Σ'''}} is zero outside of the diagonal (grey italics) and one diagonal element is zero (red bold). Furthermore, because the matrices {{math|'''U'''}} and {{math|'''V'''<sup>∗</sup>}} are [[unitary matrix|unitary]], multiplying by their respective conjugate transposes yields [[identity matrix|identity matrices]], as shown below.  In this case, because {{math|'''U'''}} and {{math|'''V'''<sup>∗</sup>}} are real valued, each is an [[orthogonal matrix]].\n\n:<math>\\begin{align}\n  \\mathbf{U} \\mathbf{U}^\\textsf{T} &=\n  \\begin{bmatrix}\n    \\color{Green}0 & \\color{Blue}0 & \\color{Cyan}1 &  \\color{Emerald}0 \\\\\n    \\color{Green}0 & \\color{Blue}1 & \\color{Cyan}0 &  \\color{Emerald}0 \\\\\n    \\color{Green}0 & \\color{Blue}0 & \\color{Cyan}0 & \\color{Emerald}-1 \\\\\n    \\color{Green}1 & \\color{Blue}0 & \\color{Cyan}0 &  \\color{Emerald}0\n  \\end{bmatrix} \\cdot\n  \\begin{bmatrix}\n    \\color{Green}0 & \\color{Green}0 &  \\color{Green}0 & \\color{Green}1 \\\\\n    \\color{Blue}0 & \\color{Blue}1 &  \\color{Blue}0 & \\color{Blue}0 \\\\\n    \\color{Cyan}1 & \\color{Cyan}0 &  \\color{Cyan}0 & \\color{Cyan}0 \\\\\n    \\color{Emerald}0 & \\color{Emerald}0 & \\color{Emerald}-1 & \\color{Emerald}0\n  \\end{bmatrix}  = \n  \\begin{bmatrix}\n    1 & 0 & 0 & 0 \\\\\n    0 & 1 & 0 & 0 \\\\\n    0 & 0 & 1 & 0 \\\\\n    0 & 0 & 0 & 1\n  \\end{bmatrix} = \\mathbf{I}_4 \\\\[6pt]\n  \\mathbf{V} \\mathbf{V}^\\textsf{T} &=\n  \\begin{bmatrix}\n    \\color{Violet}0  &  \\color{Violet}0 & \\color{Violet}\\sqrt{0.2} &  \\color{Violet}0 & \\color{Violet}-\\sqrt{0.8} \\\\\n       \\color{Plum}1 &    \\color{Plum}0 &            \\color{Plum}0 &    \\color{Plum}0 &           \\color{Plum}0 \\\\\n    \\color{Magenta}0 & \\color{Magenta}1 &         \\color{Magenta}0 & \\color{Magenta}0 &           \\color{Magenta}0 \\\\\n    \\color{Orchid}0  &  \\color{Orchid}0 &          \\color{Orchid}0 &  \\color{Orchid}1 &           \\color{Orchid}0 \\\\\n    \\color{Purple}0  &  \\color{Purple}0 & \\color{Purple}\\sqrt{0.8} &  \\color{Purple}0 & \\color{Purple}\\sqrt{0.2}\n  \\end{bmatrix} \\cdot\n  \\begin{bmatrix}\n                \\color{Violet}0 & \\color{Plum}1 & \\color{Magenta}0 & \\color{Orchid}0 &          \\color{Purple}0 \\\\\n                \\color{Violet}0 & \\color{Plum}0 & \\color{Magenta}1 & \\color{Orchid}0 &          \\color{Purple}0 \\\\\n       \\color{Violet}\\sqrt{0.2} & \\color{Plum}0 & \\color{Magenta}0 & \\color{Orchid}0 & \\color{Purple}\\sqrt{0.8} \\\\\n                \\color{Violet}0 & \\color{Plum}0 & \\color{Magenta}0 & \\color{Orchid}1 &          \\color{Purple}0 \\\\\n    \\color{Violet} - \\sqrt{0.8} & \\color{Plum}0 & \\color{Magenta}0 & \\color{Orchid}0 & \\color{Purple}\\sqrt{0.2}\n  \\end{bmatrix}  =\n  \\begin{bmatrix}\n    1 & 0 & 0 & 0 & 0 \\\\\n    0 & 1 & 0 & 0 & 0 \\\\\n    0 & 0 & 1 & 0 & 0 \\\\\n    0 & 0 & 0 & 1 & 0 \\\\\n    0 & 0 & 0 & 0 & 1\n  \\end{bmatrix} = \\mathbf{I}_5\n\\end{align}</math>\n\nThis particular singular value decomposition is not unique.  Choosing <math>V</math> such that\n:<math>\\mathbf{V}^* = \\begin{bmatrix}\n            \\color{Violet}0 & \\color{Plum}1 & \\color{Magenta}0 &          \\color{Orchid}0 &           \\color{Purple}0 \\\\\n            \\color{Violet}0 & \\color{Plum}0 & \\color{Magenta}1 &          \\color{Orchid}0 &           \\color{Purple}0 \\\\\n   \\color{Violet}\\sqrt{0.2} & \\color{Plum}0 & \\color{Magenta}0 &          \\color{Orchid}0 &  \\color{Purple}\\sqrt{0.8} \\\\\n   \\color{Violet}\\sqrt{0.4} & \\color{Plum}0 & \\color{Magenta}0 & \\color{Orchid}\\sqrt{0.5} & \\color{Purple}-\\sqrt{0.1} \\\\\n  \\color{Violet}-\\sqrt{0.4} & \\color{Plum}0 & \\color{Magenta}0 & \\color{Orchid}\\sqrt{0.5} &  \\color{Purple}\\sqrt{0.1}\n\\end{bmatrix}</math>\n\nis also a valid singular value decomposition.\n\n== SVD and spectral decomposition ==\n=== Singular values, singular vectors, and their relation to the SVD ===\nA non-negative real number {{mvar|σ}} is a '''[[singular value]]''' for {{math|'''M'''}} if and only if there exist unit-length vectors <math>\\vec{u}</math> in ''K<sup>m</sup>'' and <math>\\vec{v}</math> in ''K<sup>n</sup>'' such that\n\n:<math>\\mathbf{M}\\vec{v} = \\sigma \\vec{u} \\,\\text{ and } \\mathbf{M}^*\\vec{u} = \\sigma \\vec{v}</math>\n\nThe vectors <math>\\vec{u}</math> and <math>\\vec{v}</math> are called '''left-singular''' and '''right-singular vectors''' for {{mvar|σ}}, respectively.\n\nIn any singular value decomposition\n\n:<math>\\mathbf{M} = \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^*</math>\n\nthe diagonal entries of {{math|'''Σ'''}} are equal to the singular values of {{math|'''M'''}}. The first {{math|''p'' {{=}} min(''m'', ''n'')}} columns of {{math|'''U'''}} and {{math|'''V'''}} are, respectively, left- and right-singular vectors for the corresponding singular values.  Consequently, the above theorem implies that:\n* An {{math|''m'' × ''n''}} matrix {{math|'''M'''}} has at most {{math|''p''}} distinct singular values.\n* It is always possible to find a [[orthogonal basis|unitary basis]] {{math|'''U'''}} for {{mvar|K<sup>m</sup>}} with a subset of basis vectors spanning the left-singular vectors of each singular value of {{math|'''M'''}}.\n* It is always possible to find a unitary basis {{math|'''V'''}} for {{mvar|K<sup>n</sup>}} with a subset of basis vectors spanning the right-singular vectors of each singular value of {{math|'''M'''}}.\n\nA singular value for which we can find two left (or right) singular vectors that are linearly independent is called ''degenerate''.  If <math>\\vec{u_1}</math> and <math>\\vec{u_2}</math> are two left-singular vectors which both correspond to the singular value σ, then any normalized linear combination of the two vectors is also a left-singular vector corresponding to the singular value σ.  The similar statement is true for right-singular vectors.  The number of independent left and right-singular vectors coincides, and these singular vectors appear in the same columns of {{math|'''U'''}} and {{math|'''V'''}} corresponding to diagonal elements of {{math|'''Σ'''}} all with the same value σ.\n\nAs an exception, the left and right-singular vectors of singular value 0 comprise all unit vectors in the [[Kernel (linear algebra)|kernel]] and [[cokernel]], respectively, of {{math|'''M'''}}, which by the [[rank–nullity theorem]] cannot be the same dimension if {{math|m ≠ n}}.  Even if all singular values are nonzero, if {{math|''m'' > ''n''}} then the cokernel is nontrivial, in which case {{math|'''U'''}} is padded with {{math|''m'' − ''n''}} orthogonal vectors from the cokernel.  Conversely, if {{math|''m'' < ''n''}}, then {{math|'''V'''}} is padded by {{math|''n'' − ''m''}} orthogonal vectors from the kernel.  However, if the singular value of 0 exists, the extra columns of {{math|'''U'''}} or {{math|'''V'''}} already appear as left or right-singular vectors.\n\nNon-degenerate singular values always have unique left- and right-singular vectors, up to multiplication by a unit-phase factor ''e''<sup>'''i'''''φ''</sup> (for the real case up to a sign).  Consequently, if all singular values of a square matrix {{math|'''M'''}} are non-degenerate and non-zero, then its singular value decomposition is unique, up to multiplication of a column of {{math|'''U'''}} by a unit-phase factor and simultaneous multiplication of the corresponding column of {{math|'''V'''}} by the same unit-phase factor.\nIn general, the SVD is unique up to arbitrary unitary transformations applied uniformly to the column vectors of both {{math|'''U'''}} and {{math|'''V'''}} spanning the subspaces of each singular value, and up to arbitrary unitary transformations on vectors of {{math|'''U'''}} and {{math|'''V'''}} spanning the kernel and cokernel, respectively, of {{math|'''M'''}}.\n\n=== Relation to eigenvalue decomposition ===\nThe singular value decomposition is very general in the sense that it can be applied to any {{math|''m'' × ''n''}} matrix whereas [[eigenvalue decomposition]] can only be applied to [[Diagonalizable matrix|diagonalizable matrices]]. Nevertheless, the two decompositions are related.\n\nGiven an SVD of {{math|'''M'''}}, as described above, the following two relations hold:\n\n:<math>\\begin{align}\n\\mathbf{M}^* \\mathbf{M} &= \\mathbf{V} \\boldsymbol{\\Sigma}^* \\mathbf{U}^*\\, \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^* = \\mathbf{V} (\\boldsymbol{\\Sigma}^* \\boldsymbol{\\Sigma}) \\mathbf{V}^* \\\\\n\\mathbf{M} \\mathbf{M}^* &= \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^*\\, \\mathbf{V} \\boldsymbol{\\Sigma}^* \\mathbf{U}^* = \\mathbf{U} (\\boldsymbol{\\Sigma} \\boldsymbol{\\Sigma}^*) \\mathbf{U}^*\n\\end{align}</math>\n\nThe right-hand sides of these relations describe the eigenvalue decompositions of the left-hand sides.  Consequently:\n\n:* The columns of {{math|'''V'''}} (right-singular vectors) are [[eigenvectors]] of {{math|'''M'''<sup>∗</sup>'''M'''}}.\n:* The columns of {{math|'''U'''}} (left-singular vectors) are eigenvectors of {{math|'''MM'''<sup>∗</sup>}}.\n:* The non-zero elements of {{math|'''Σ'''}} (non-zero singular values) are the square roots of the non-zero [[eigenvalues]] of {{math|'''M'''<sup>∗</sup>'''M'''}} or {{math|'''MM'''<sup>∗</sup>}}.\n\nIn the special case that {{math|'''M'''}} is a [[normal matrix]], which by definition must be square, the [[Spectral theorem#Finite-dimensional case|spectral theorem]] says that it can be [[Unitary transform|unitarily]] [[Diagonalizable matrix|diagonalized]] using a basis of [[eigenvector]]s, so that it can be written {{math|'''M''' {{=}} '''UDU'''<sup>∗</sup>}} for a unitary matrix {{math|'''U'''}} and a diagonal matrix {{math|'''D'''}}.  When {{math|'''M'''}} is also [[Positive-definite matrix|positive semi-definite]], the decomposition {{math|'''M''' {{=}} '''UDU'''<sup>∗</sup>}} is also a singular value decomposition.  Otherwise, it can be recast as an SVD by moving the phase of each {{mvar|σ<sub>i</sub>}} to either its corresponding {{math|'''V'''<sub>i</sub>}} or {{math|'''U'''<sub>i</sub>}}.  The natural connection of the SVD to non-normal matrices is through the [[polar decomposition]] theorem:  {{math|'''M'''&nbsp;{{=}}&nbsp;'''SR'''}}, where {{math|'''S'''&nbsp;{{=}}&nbsp;'''UΣU'''<sup>*</sup>}} is positive semidefinite and normal, and  {{math|'''R'''&nbsp;{{=}}&nbsp;'''UV'''<sup>*</sup>}} is unitary.\n\nThus while related, the eigenvalue decomposition and SVD differ except for positive semi-definite normal matrices {{math|'''M'''}}: the eigenvalue decomposition is {{math|'''M''' {{=}} '''UDU'''<sup>−1</sup>}} where {{math|'''U'''}} is not necessarily unitary and {{math|'''D'''}} is not necessarily positive semi-definite, while the SVD is {{math|'''M''' {{=}} '''UΣV'''<sup>∗</sup>}} where {{math|'''Σ'''}} is diagonal and positive semi-definite, and {{math|'''U'''}} and {{math|'''V'''}} are unitary matrices that are not necessarily related except through the matrix {{math|'''M'''}}.  While only [[defective matrix|non-defective]] square matrices have an eigenvalue decomposition, any <math>m \\times n</math> matrix has a SVD.\n\n==Applications of the SVD==\n\n===Pseudoinverse===\nThe singular value decomposition can be used for computing the [[Moore–Penrose pseudoinverse|pseudoinverse]] of a matrix. Indeed, the pseudoinverse of the matrix {{math|'''M'''}} with singular value decomposition {{math|'''M''' {{=}} '''UΣV'''<sup>∗</sup>}} is\n\n:<math>\\mathbf{M}^+ = \\mathbf{V} \\boldsymbol{\\Sigma}^+ \\mathbf{U}^*</math>\n\nwhere {{math|'''Σ'''<sup>+</sup>}} is the pseudoinverse of {{math|'''Σ'''}}, which is formed by replacing every non-zero diagonal entry by its [[Multiplicative inverse|reciprocal]] and transposing the resulting matrix. The pseudoinverse is one way to solve [[linear least squares (mathematics)|linear least squares]] problems.\n\n===Solving homogeneous linear equations===\nA set of [[homogeneous linear equation]]s can be written as {{math|'''Ax''' {{=}} '''0'''}} for a matrix {{math|'''A'''}} and vector {{math|'''x'''}}. A typical situation is that {{math|'''A'''}} is known and a non-zero {{math|'''x'''}} is to be determined which satisfies the equation. Such an {{math|'''x'''}} belongs to {{math|'''A'''}}'s [[Kernel (matrix)|null space]] and is sometimes called a (right) null vector of {{math|'''A'''}}. The vector {{math|'''x'''}} can be characterized as a right-singular vector corresponding to a singular value of {{math|'''A'''}} that is zero. This observation means that if {{math|'''A'''}} is a [[square matrix]] and has no vanishing singular value, the equation has no non-zero {{math|'''x'''}} as a solution.  It also means that if there are several vanishing singular values, any linear combination of the corresponding right-singular vectors is a valid solution. Analogously to the definition of a (right) null vector, a non-zero {{math|'''x'''}} satisfying {{math|'''x'''<sup>∗</sup>'''A''' {{=}} '''0'''}}, with {{math|'''x'''<sup>∗</sup>}} denoting the conjugate transpose of {{math|'''x'''}}, is called a left null vector of {{math|'''A'''}}.\n\n===Total least squares minimization===\nA [[total least squares]] problem refers to determining the vector {{math|'''x'''}} which minimizes the [[Vector norm#p-norm|2-norm]] of a vector {{math|'''Ax'''}} under the constraint {{math|{{!!}}'''x'''{{!!}} {{=}} 1}}. The solution turns out to be the right-singular vector of {{math|'''A'''}} corresponding to the smallest singular value.\n\n===Range, null space and rank===\nAnother application of the SVD is that it provides an explicit representation of the [[Column space|range]] and [[null space]] of a matrix {{math|'''M'''}}. The right-singular vectors corresponding to vanishing singular values of {{math|'''M'''}} span the null space of {{math|'''M'''}} and the left-singular vectors corresponding to the non-zero singular values of {{math|'''M'''}} span the range of {{math|'''M'''}}. E.g., in the above [[Singular value decomposition#Example|example]] the null space is spanned by the last two columns of {{math|'''V'''}} and the range is spanned by the first three columns of {{math|'''U'''}}.\n\nAs a consequence, the [[rank of a matrix|rank]] of {{math|'''M'''}} equals the number of non-zero singular values which is the same as the number of non-zero diagonal elements in {{math|'''Σ'''}}. In numerical linear algebra the singular values can be used to determine the ''effective rank'' of a matrix, as [[rounding error]] may lead to small but non-zero singular values in a rank deficient matrix. Singular values beyond a significant gap are assumed to be numerically equivalent to zero.\n\n===Low-rank matrix approximation===\nSome practical applications need to solve the problem of approximating a matrix {{math|'''M'''}} with another matrix <math>\\tilde{\\mathbf{M}}</math>, said [[#Truncated SVD|truncated]], which has a specific rank {{mvar|r}}. In the case that the approximation is based on minimizing the [[Frobenius norm]] of the difference between {{math|'''M'''}} and <math>\\tilde{\\mathbf{M}}</math> under the constraint that <math>\\operatorname{rank}\\left(\\tilde{\\mathbf{M}}\\right) = r</math> it turns out that the solution is given by the SVD of {{math|'''M'''}}, namely\n:<math>\\tilde{\\mathbf{M}} = \\mathbf{U} \\tilde{\\boldsymbol{\\Sigma}} \\mathbf{V}^*</math>\n\nwhere <math>\\tilde{\\boldsymbol{\\Sigma}}</math> is the same matrix as {{math|'''Σ'''}} except that it contains only the {{mvar|r}} largest singular values (the other singular values are replaced by zero). This is known as the '''Eckart–Young theorem''', as it was proved by those two authors in 1936 (although it was later found to have been known to earlier authors; see {{harvnb|Stewart|1993}}).\n\n===Separable models===\nThe SVD can be thought of as decomposing a matrix into a weighted, ordered sum of separable matrices. By separable, we mean that a matrix {{math|'''A'''}} can be written as an [[outer product]] of two vectors {{math|'''A''' {{=}} '''u''' ⊗ '''v'''}}, or, in coordinates, <math>A_{ij} = u_i v_j</math>. Specifically, the matrix {{math|'''M'''}} can be decomposed as:\n:<math>\\mathbf{M} = \\sum_i \\mathbf{A}_i = \\sum_i \\sigma_i \\mathbf U_i \\otimes \\mathbf V_i</math>\n\nHere {{math|'''U'''<sub>''i''</sub>}} and {{math|'''V'''<sub>''i''</sub>}} are the {{mvar|i}}-th columns of the corresponding SVD matrices, {{mvar|σ<sub>i</sub>}} are the ordered singular values, and each {{math|'''A'''<sub>''i''</sub>}} is separable. The SVD can be used to find the decomposition of an image processing filter into separable horizontal and vertical filters. Note that the number of non-zero {{mvar|σ<sub>i</sub>}} is exactly the rank of the matrix.\n\nSeparable models often arise in biological systems, and the SVD factorization is useful to analyze such systems. For example, some visual area V1 simple cells' receptive fields can be well described<ref>{{cite journal |doi=10.1016/0166-2236(95)94496-R |last1=DeAngelis|first1=G. C.|last2=Ohzawa|first2=I.|last3=Freeman|first3=R. D. |title=Receptive-field dynamics in the central visual pathways |journal=Trends Neurosci. |volume=18 |issue=10 |pages=451–8 |date=October 1995 |pmid=8545912 |url=http://linkinghub.elsevier.com/retrieve/pii/0166-2236(95)94496-R |ref=harv}}</ref> by a [[Gabor filter]] in the space domain multiplied by a modulation function in the time domain. Thus, given a linear filter evaluated through, for example, [[Spike-triggered average|reverse correlation]], one can rearrange the two spatial dimensions into one dimension, thus yielding a two-dimensional filter (space, time) which can be decomposed through SVD. The first column of {{math|'''U'''}} in the SVD factorization is then a Gabor while the first column of {{math|'''V'''}} represents the time modulation (or vice versa). One may then define an index of separability,\n\n:<math>\\alpha = \\frac{\\sigma_1^2}{\\sum_i \\sigma_i^2},</math>\n\nwhich is the fraction of the power in the matrix M which is accounted for by the first separable matrix in the decomposition.<ref>{{cite journal |last1=Depireux|first1=D. A.|last2=Simon|first2=J. Z.|last3=Klein|first3=D. J.|last4=Shamma|first4=S. A. |title=Spectro-temporal response field characterization with dynamic ripples in ferret primary auditory cortex |journal=J. Neurophysiol. |volume=85 |issue=3 |pages=1220–34 |date=March 2001 |pmid=11247991 |ref=harv |doi=10.1152/jn.2001.85.3.1220}}</ref>\n\n===Nearest orthogonal matrix===\nIt is possible to use the SVD of a square matrix {{math|'''A'''}} to determine the [[orthogonal matrix]] {{math|'''O'''}} closest to {{math|'''A'''}}. The closeness of fit is measured by the [[Frobenius norm]] of {{math|'''O''' − '''A'''}}. The solution is the product {{math|'''UV'''<sup>∗</sup>}}.<ref>[http://www.wou.edu/~beavers/Talks/Willamette1106.pdf The Singular Value Decomposition in Symmetric (Lowdin) Orthogonalization and Data Compression]</ref> This intuitively makes sense because an orthogonal matrix would have the decomposition {{math|'''UIV'''<sup>∗</sup>}} where {{math|'''I'''}} is the identity matrix, so that if {{math|'''A''' {{=}} '''UΣV'''<sup>∗</sup>}} then the product {{math|'''A''' {{=}} '''UV'''<sup>∗</sup>}} amounts to replacing the singular values with ones.\n\nA similar problem, with interesting applications in [[shape analysis (digital geometry)|shape analysis]], is the [[orthogonal Procrustes problem]], which consists of finding an orthogonal matrix {{math|'''O'''}} which most closely maps {{math|'''A'''}} to {{math|'''B'''}}. Specifically,\n\n:<math>\\mathbf{O} = \\underset\\Omega\\operatorname{argmin} \\|\\mathbf{A}\\boldsymbol{\\Omega} - \\mathbf{B}\\|_F \\quad\\text{subject to}\\quad \\boldsymbol{\\Omega}^\\textsf{T}\\boldsymbol{\\Omega} = \\mathbf{I}</math>\n\nwhere <math>\\| \\cdot \\|_F</math> denotes the Frobenius norm.\n\nThis problem is equivalent to finding the nearest orthogonal matrix to a given matrix {{math|'''M''' {{=}} '''A'''<sup>{{font|T}}</sup>'''B'''}}.\n\n===The Kabsch algorithm===\nThe [[Kabsch algorithm]] (called [[Wahba's problem]] in other fields) uses SVD to compute the optimal rotation (with respect to least-squares minimization) that will align a set of points with a corresponding set of points. It is used, among other applications, to compare the structures of molecules.\n\n===Signal processing===\nThe SVD and pseudoinverse have been successfully applied to signal [[processing]],<ref>{{cite journal |last=Sahidullah |first=Md. |author2=Kinnunen, Tomi |title=Local spectral variability features for speaker verification |journal=Digital Signal Processing |date=March 2016 |volume=50 |pages=1–11 |doi=10.1016/j.dsp.2015.10.011 }}</ref> [[image processing]]<ref>{{cite journal |last= Rowayda A. Sadek |title= SVD Based Image Processing Applications: State of The Art, Contributions and Research Challenges | journal = International Journal of Advanced Computer Science and Applications|date=2012|volume=3|issue=7|pages=26–34| doi = 10.14569/IJACSA.2012.030703 |arxiv=1211.7102|bibcode= 2012arXiv1211.7102S }}</ref> and [[big data]], e.g., in genomic signal processing.<ref>{{Cite journal\n | author = O. Alter, P. O. Brown and D. Botstein\n | title = Singular Value Decomposition for Genome-Wide Expression Data Processing and Modeling\n | journal = PNAS\n | volume = 97\n | issue = 18\n | pages = 10101–10106\n | date = September 2000\n | doi = 10.1073/pnas.97.18.10101\n | pmid = 10963673\n | pmc = 27718\n | bibcode = 2000PNAS...9710101A\n }}</ref><ref>{{Cite journal\n | author1=O. Alter |author2=G. H. Golub\n | title = Integrative Analysis of Genome-Scale Data by Using Pseudoinverse Projection Predicts Novel Correlation Between DNA Replication and RNA Transcription\n | journal = PNAS\n | volume = 101\n | issue = 47\n | pages = 16577–16582\n | date = November 2004\n | doi = 10.1073/pnas.0406767101\n | pmid=15545604\n | pmc=534520\n | bibcode=2004PNAS..10116577A}}</ref><ref>{{Cite journal\n | author1=O. Alter  |author2=G. H. Golub\n | title = Singular Value Decomposition of Genome-Scale mRNA Lengths Distribution Reveals Asymmetry in RNA Gel Electrophoresis Band Broadening\n | journal = PNAS\n | volume = 103\n | issue = 32\n | pages = 11828–11833\n | date = August 2006\n | doi = 10.1073/pnas.0604756103\n | pmid=16877539\n | pmc=1524674\n | bibcode=2006PNAS..10311828A}}</ref><ref>{{Cite journal\n | first1 = N. M.\n | last1 = Bertagnolli\n | first2 = J. A.\n | last2 = Drake\n | first3 = J. M.\n | last3 = Tennessen\n | first4 = O.\n | last4 = Alter\n | title = SVD Identifies Transcript Length Distribution Functions from DNA Microarray Data and Reveals Evolutionary Forces Globally Affecting GBM Metabolism\n | journal = PLOS ONE\n | volume = 8\n | issue = 11\n | pages = e78913\n | date = November 2013\n | doi = 10.1371/journal.pone.0078913\n | id = [http://www.alterlab.org/research/highlights/pone.0078913_Highlight.pdf Highlight]\n | pmid=24282503\n | pmc=3839928\n | bibcode = 2013PLoSO...878913B\n }}</ref>\n\n===Other examples===\nThe SVD is also applied extensively to the study of linear [[inverse problem]]s, and is useful in the analysis of regularization methods such as that of [[Tikhonov regularization|Tikhonov]]. It is widely used in statistics where it is related to [[principal component analysis]] and to [[Correspondence analysis]], and in [[signal processing]] and [[pattern recognition]]. It is also used in output-only [[modal analysis]], where the non-scaled [[mode shape]]s can be determined from the singular vectors. Yet another usage is [[latent semantic indexing]] in natural language text processing.\n\nThe SVD also plays a crucial role in the field of [[quantum information]], in a form often referred to as the [[Schmidt decomposition]]. Through it, states of two quantum systems are naturally decomposed, providing a necessary and sufficient condition for them to be [[Quantum entanglement|entangled]]: if the rank of the {{math|'''Σ'''}} matrix is larger than one.\n\nOne application of SVD to rather large matrices is in [[numerical weather prediction]], where [[Lanczos algorithm|Lanczos method]]s are used to estimate the most linearly quickly growing few perturbations to the central numerical weather prediction over a given initial forward time period; i.e., the singular vectors corresponding to the largest singular values of the linearized propagator for the global weather over that time interval. \nThe output singular vectors in this case are entire weather systems. These perturbations are then run through the full nonlinear model to generate an [[ensemble forecasting|ensemble forecast]], giving a handle on some of the uncertainty that should be allowed for around the current central prediction.\n\nSVD has also been applied to reduced order modelling. The aim of reduced order modelling is to reduce the number of degrees of freedom in a complex system which is to be modelled. SVD was coupled with [[radial basis functions]] to interpolate solutions to three-dimensional unsteady flow problems.<ref>{{cite journal | last1 = Walton | first1 = S. | last2 = Hassan | first2 = O. | last3 = Morgan | first3 = K. | year = 2013| title = Reduced order modelling for unsteady fluid flow using proper orthogonal decomposition and radial basis functions | journal = Applied Mathematical Modelling | volume =  37| issue = 20–21| pages = 8930–8945| doi=10.1016/j.apm.2013.04.025}}</ref>\n\nInterestingly, SVD has been used to improve gravitational waveform modeling by the ground based gravitational wave interferometer aLIGO.<ref>{{cite journal | last1 = Setyawati | first1 = Y. | last2 = Ohme | first2 = F. | last3 = Khan | first3 = S. | year = 2019| title = Enhancing gravitational waveform model through dynamic calibration | journal = Physical Review D | volume =  99| issue =2 | pages = 024010| doi=10.1103/PhysRevD.99.024010| bibcode = 2019PhRvD..99b4010S }}</ref> SVD can help to increase the accuracy and speed of waveform generation to support gravitational waves searches and update two different waveform models.\n\nSingular value decomposition is used in [[recommender systems]] to predict people's item ratings.<ref>{{cite journal |last=Sarwar |first=Badrul |last2=Karypis |first2=George |last3=Konstan |first3=Joseph A. |last4=Riedl |first4=John T. |lastauthoramp=yes |year=2000 |title=Application of Dimensionality Reduction in Recommender System -- A Case Study |url=http://files.grouplens.org/papers/webKDD00.pdf |accessdate=May 26, 2014 |publisher=[[University of Minnesota]]}}</ref> Distributed algorithms have been developed for the purpose of calculating the SVD on clusters of commodity machines.<ref>{{cite journal|last1=Bosagh Zadeh|first1=Reza|last2=Carlsson|first2=Gunnar|title=Dimension Independent Matrix Square Using MapReduce|url=http://stanford.edu/~rezab/papers/dimsum.pdf|accessdate=12 July 2014|bibcode=2013arXiv1304.1467B|year=2013|arxiv=1304.1467}}</ref>\n\nAnother code implementation of the Netflix Recommendation Algorithm SVD (the third optimal algorithm in the competition conducted by Netflix to find the best collaborative filtering techniques for predicting user ratings for films based on previous reviews) in platform Apache Spark is available in the following GitHub repository <ref>{{Cite web | url=https://github.com/it21208/SVDMovie-Lens-Parallel-Apache-Spark |title = Contribute to it21208/SVDMovie-Lens-Parallel-Apache-Spark development by creating an account on GitHub|date = 28 January 2019}}</ref> implemented by Alexandros Ioannidis. The original SVD algorithm,<ref>http://www.timelydevelopment.com/demos/NetflixPrize.aspx</ref> which in this case is executed in parallel encourages users of the GroupLens website, by consulting proposals for monitoring new films tailored to the needs of each user.\n\nLow-rank SVD has been applied for hotspot detection from spatiotemporal data with application to disease [[outbreak]] detection \n.<ref>{{Cite journal\n |author1=Hadi Fanaee-T  |author2=João Gama\n  |lastauthoramp=yes | title = Eigenspace method for spatiotemporal hotspot detection\n | journal = Expert Systems\n |volume=32\n  |issue=3\n  | pages = 454–464\n | date = September 2014\n | doi = 10.1111/exsy.12088\n |arxiv=1406.3506|bibcode=2014arXiv1406.3506F\n }}</ref> A combination of SVD and [[Higher-order singular value decomposition|higher-order SVD]] also has been applied for real time event detection from complex data streams (multivariate data with space and time dimensions) in [[Disease surveillance]].<ref>{{Cite journal\n |author1=Hadi Fanaee-T  |author2=João Gama\n  |lastauthoramp=yes | title = EigenEvent: An Algorithm for Event Detection from Complex Data Streams in Syndromic Surveillance\n | journal = Intelligent Data Analysis\n | volume = 19\n | issue = 3\n |pages=597–616\n  | date = May 2015\n | arxiv = 1406.3496\n | doi=10.3233/IDA-150734}}</ref>\n\n== Existence proofs ==\nAn eigenvalue {{mvar|λ}} of a matrix {{math|'''M'''}} is characterized by the algebraic relation {{math|'''M'''''u'' {{=}} ''λu''}}. When {{math|'''M'''}} is [[Hermitian matrix|Hermitian]], a variational characterization is also available. Let {{math|'''M'''}} be a real {{math|''n'' × ''n''}} [[symmetric matrix]]. Define\n\n:<math>\\begin{cases} f : \\mathbb{R}^n \\to \\mathbb{R} \\\\ f(x) = x^\\textsf{T} \\mathbf{M} x \\end{cases}</math>\n\nBy the [[extreme value theorem]], this continuous function attains a maximum at some ''u'' when restricted to the closed unit sphere {||''x''|| ≤ 1}. By the [[Lagrange multipliers]] theorem, ''u'' necessarily satisfies\n\n:<math>\\nabla f = \\nabla x^\\textsf{T} \\mathbf{M} x - \\lambda \\cdot \\nabla x^\\textsf{T} x = 2(\\mathbf{M}-\\lambda \\mathbf{I})x = 0</math>\n\nwhere the nabla symbol, {{math|∇}}, is the [[del]] operator.\n\nA short calculation shows the above leads to {{math|'''M'''''u'' {{=}} ''λu''}} (symmetry of {{math|'''M'''}} is needed here). Therefore, {{mvar|λ}} is the largest eigenvalue of {{math|'''M'''}}. The same calculation performed on the orthogonal complement of ''u'' gives the next largest eigenvalue and so on. The complex Hermitian case is similar; there ''f''(''x'') = ''x* M x'' is a real-valued function of {{math|2''n''}} real variables.\n\nSingular values are similar in that they can be described algebraically or from variational principles. Although, unlike the eigenvalue case, Hermiticity, or symmetry, of {{math|'''M'''}} is no longer required.\n\nThis section gives these two arguments for existence of singular value decomposition.\n\n=== Based on the spectral theorem ===\nLet <math>\\mathbf{M}</math> be an {{math|''m'' × ''n''}} complex matrix. Since <math>\\mathbf{M}^* \\mathbf{M}</math> is positive semi-definite and Hermitian, by the [[spectral theorem]], there exists an {{math|''n'' × ''n''}} unitary matrix <math>\\mathbf{V}</math> such that\n\n:<math>\\mathbf{V}^* \\mathbf{M}^* \\mathbf{M} \\mathbf{V} = \\bar\\mathbf{D} = \\begin{bmatrix} \\mathbf{D} & 0 \\\\ 0 & 0\\end{bmatrix},</math>\n\nwhere <math>\\mathbf{D}</math> is diagonal and positive definite, of dimension <math>\\ell\\times \\ell</math>, with <math>\\ell</math> the number of non-zero eigenvalues of <math>\\mathbf{M}^* \\mathbf{M}</math> (which can be shown to verify <math>\\ell\\le\\min(n,m)</math>). Note that <math>\\mathbf{V}</math> is here by definition a matrix whose <math>i</math>-th row is the <math>i</math>-th eigenvector of <math>\\mathbf{M}^* \\mathbf{M}</math>, corresponding to the eigenvalue <math>\\bar{\\mathbf{D}}_{ii}</math>. Moreover, the <math>j</math>-th row of <math>\\mathbf{V}</math>, for <math>j>\\ell</math>, is an eigenvector of <math>\\mathbf{M}^* \\mathbf{M}</math> with eigenvalue <math>\\bar{\\mathbf{D}}_{jj}=0</math>. This can be expressed by writing <math>\\mathbf{V}</math>  as <math>\\mathbf{V}=\\begin{bmatrix}\\mathbf{V}_1 &\\mathbf{V}_2\\end{bmatrix}</math>, where the rows of <math>\\mathbf{V}_1</math> and <math>\\mathbf{V}_2</math> therefore contain the eigenvectors of <math>\\mathbf{M}^* \\mathbf{M}</math> corresponding to non-zero and zero eigenvalues, respectively. Using this rewriting of <math>\\mathbf{V}</math>, the equation becomes:\n\n:<math>\\begin{bmatrix} \\mathbf{V}_1^* \\\\ \\mathbf{V}_2^* \\end{bmatrix} \\mathbf{M}^* \\mathbf{M} \\begin{bmatrix} \\mathbf{V}_1 & \\mathbf{V}_2 \\end{bmatrix} = \\begin{bmatrix} \\mathbf{V}_1^* \\mathbf{M}^* \\mathbf{M} \\mathbf{V}_1 & \\mathbf{V}_1^* \\mathbf{M}^* \\mathbf{M} \\mathbf{V}_2 \\\\ \\mathbf{V}_2^* \\mathbf{M}^* \\mathbf{M} \\mathbf{V}_1 & \\mathbf{V}_2^* \\mathbf{M}^* \\mathbf{M} \\mathbf{V}_2 \\end{bmatrix} = \\begin{bmatrix} \\mathbf{D} & 0 \\\\ 0 & 0 \\end{bmatrix}.</math>\n\nThis implies that\n\n:<math>\\mathbf{V}_1^* \\mathbf{M}^* \\mathbf{M} \\mathbf{V}_1 = \\mathbf{D}, \\qquad \\mathbf{V}_2^* \\mathbf{M}^* \\mathbf{M} \\mathbf{V}_2 = \\mathbf{0}.</math>\n\nMoreover, the second equation implies <math>\\mathbf{M}\\mathbf{V}_2=\\mathbf{0}</math>.<ref>To see this, we just have to notice that <math>trace(\\mathbf{V}_2^* \\mathbf{M}^* \\mathbf{M} \\mathbf{V}_2)=\\|\\mathbf{M} \\mathbf{V}_2\\|^2</math>, and remember that <math>\\|A\\|=0\\Longleftrightarrow A=0</math></ref> Finally, the unitarity of <math>\\mathbf{V}</math> translates, in terms of <math>\\mathbf{V}_1</math> and <math>\\mathbf{V}_2</math>, into the following conditions:\n\n:<math>\\begin{align} \n\\mathbf{V}_1^* \\mathbf{V}_1 &= \\mathbf{I_1}, \\\\\n\\mathbf{V}_2^* \\mathbf{V}_2 &= \\mathbf{I_2}, \\\\\n\\mathbf{V}_1 \\mathbf{V}_1^* + \\mathbf{V}_2 \\mathbf{V}_2^* &= \\mathbf{I_{12}},\n\\end{align}</math>\n\nwhere the subscripts on the identity matrices are used to remark that they are of different dimensions.\n\nLet us now define\n\n:<math>\\mathbf{U}_1 = \\mathbf{M} \\mathbf{V}_1 \\mathbf{D}^{-\\frac{1}{2}}.</math>\n\nThen,\n\n:<math>\\mathbf{U}_1 \\mathbf{D}^\\frac{1}{2} \\mathbf{V}_1^* = \\mathbf{M} \\mathbf{V}_1 \\mathbf{D}^{-\\frac{1}{2}} \\mathbf{D}^\\frac{1}{2} \\mathbf{V}_1^* = \\mathbf{M} (\\mathbf{I} - \\mathbf{V}_2\\mathbf{V}_2^*) = \\mathbf{M} - (\\mathbf{M}\\mathbf{V}_2)\\mathbf{V}_2^* = \\mathbf{M},</math>\n\nsince <math>\\mathbf{M}\\mathbf{V}_2 = \\mathbf{0}. </math> This can be also seen as immediate consequence of the fact that <math>\\mathbf{M}\\mathbf{V}_1\\mathbf{V}_1^* = \\mathbf{M}</math>. Note how this is equivalent to the observation that, if <math>\\{\\boldsymbol v_i\\}_{i=1}^l</math> is the set of eigenvectors of <math>\\mathbf{M}^* \\mathbf{M}</math> corresponding to non-vanishing eigenvalues, then <math>\\{\\mathbf M \\boldsymbol v_i\\}_{i=1}^l</math> is a set of orthogonal vectors, and <math>\\{\\lambda^{-1/2}\\mathbf M \\boldsymbol v_i\\}_{i=1}^l</math> a (generally not complete) set of ''orthonormal'' vectors. This matches with the matrix formalism used above denoting with <math>\\mathbf{V}_1</math> the matrix whose columns are <math>\\{\\boldsymbol v_i\\}_{i=1}^l</math>, with <math>\\mathbf{V}_2</math> the matrix whose columns are the eigenvectors of <math>\\mathbf{M}^* \\mathbf{M}</math> which vanishing eigenvalue, and <math>\\mathbf{U}_1</math> the matrix whose columns are the vectors <math>\\{\\lambda^{-1/2}\\mathbf M \\boldsymbol v_i\\}_{i=1}^l</math>.\n\nWe see that this is almost the desired result, except that <math>\\mathbf{U}_1</math> and <math>\\mathbf{V}_1</math> are in general not unitary, since they might not be square. However, we do know that the number of rows of <math>\\mathbf{U}_1</math> is no smaller than the number of columns, since the dimensions of <math>\\mathbf{D}</math> is no greater than <math>m</math> and <math>n</math>. Also, since\n\n:<math>\\mathbf{U}_1^*\\mathbf{U}_1 = \\mathbf{D}^{-\\frac{1}{2}}\\mathbf{V}_1^*\\mathbf{M}^*\\mathbf{M} \\mathbf{V}_1 \\mathbf{D}^{-\\frac{1}{2}}=\\mathbf{D}^{-\\frac{1}{2}}\\mathbf{D}\\mathbf{D}^{-\\frac{1}{2}} = \\mathbf{I_1}</math>\n\nthe columns in <math>\\mathbf{U}_1</math> are orthonormal and can be extended to an orthonormal basis. This means that we can choose <math>\\mathbf{U}_2</math> such that <math>\\mathbf{U} = \\begin{bmatrix} \\mathbf{U}_1 & \\mathbf{U}_2 \\end{bmatrix}</math> is unitary.\n\nFor {{math|'''V'''<sub>1</sub>}} we already have {{math|'''V'''<sub>2</sub>}} to make it unitary. Now, define\n\n:<math>\\boldsymbol{\\Sigma} =\n  \\begin{bmatrix}\n    \\begin{bmatrix}\n      \\mathbf{D}^\\frac{1}{2} & 0 \\\\\n      0                      & 0\n    \\end{bmatrix} \\\\\n    0\n  \\end{bmatrix}</math>\n\nwhere extra zero rows are added '''or removed''' to make the number of zero rows equal the number of columns of {{math|'''U'''<sub>2</sub>}}, and hence the overall dimensions of <math>\\boldsymbol{\\Sigma}</math> equal to <math>m\\times n</math>. Then\n\n:<math> \\begin{bmatrix}\n    \\mathbf{U}_1 & \\mathbf{U}_2\n  \\end{bmatrix}\n  \\begin{bmatrix}\n    \\begin{bmatrix}\n      \\mathbf{}D^\\frac{1}{2} & 0 \\\\\n      0                      & 0\n    \\end{bmatrix} \\\\\n    0\n  \\end{bmatrix}\n  \\begin{bmatrix}\n    \\mathbf{V}_1 & \\mathbf{V}_2\n  \\end{bmatrix}^* =\n  \\begin{bmatrix}\n    \\mathbf{U}_1 & \\mathbf{U}_2\n  \\end{bmatrix}\n  \\begin{bmatrix} \\mathbf{D}^\\frac{1}{2} \\mathbf{V}_1^* \\\\ 0 \\end{bmatrix} =\n\\mathbf{U}_1 \\mathbf{D}^\\frac{1}{2} \\mathbf{V}_1^* = \\mathbf{M} </math>\n\nwhich is the desired result:\n\n:<math>\\mathbf{M} = \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^*</math>\n\nNotice the argument could begin with diagonalizing {{math|'''MM'''<sup>∗</sup>}} rather than {{math|'''M'''<sup>∗</sup>'''M'''}} (This shows directly that {{math|'''MM'''<sup>∗</sup>}} and {{math|'''M'''<sup>∗</sup>'''M'''}} have the same non-zero eigenvalues).\n\n===  Based on variational characterization ===\n{{anchor|vch}}The singular values can also be characterized as the maxima of {{math|'''u'''<sup>{{font|T}}</sup>'''Mv'''}}, considered as a function of {{math|'''u'''}} and {{math|'''v'''}}, over particular subspaces. The singular vectors are the values of {{math|'''u'''}} and {{math|'''v'''}} where these maxima are attained.\n\nLet {{math|'''M'''}} denote an {{math|''m'' × ''n''}} matrix with real entries. Let {{math|''S''<sup>''k''−1</sup>}} be the unit <math>(k-1)</math>-sphere in <math> \\mathbb{R}^k </math>, and define <math>\\sigma(\\mathbf{u},\\mathbf{v}) = \\mathbf{u}^\\textsf{T} \\mathbf{M} \\mathbf{v}, \\qquad \\mathbf{u} \\in S^{m-1}, \\mathbf{v} \\in S^{n-1}.</math>\n\nConsider the function {{mvar|σ}} restricted to {{math|''S''<sup>''m''−1</sup> × ''S''<sup>''n''−1</sup>}}. Since both {{math|''S''<sup>''m''−1</sup>}} and {{math|''S''<sup>''n''−1</sup>}} are [[compact space|compact]] sets, their [[Product topology|product]] is also compact.  Furthermore, since {{mvar|σ}} is continuous, it attains a largest value for at least one pair of vectors {{math|'''u''' ∈ ''S''<sup>''m''−1</sup>}} and {{math|'''v''' ∈ ''S''<sup>''n''−1</sup>}}. This largest value is denoted {{math|''σ''<sub>1</sub>}} and the corresponding vectors are denoted {{math|'''u'''<sub>1</sub>}} and {{math|'''v'''<sub>1</sub>}}. Since {{math|''σ''<sub>1</sub>}} is the largest value of {{math|''σ''('''u''', '''v''')}} it must be non-negative. If it were negative, changing the sign of either {{math|'''u'''<sub>1</sub>}} or {{math|'''v'''<sub>1</sub>}} would make it positive and therefore larger.\n\n'''Statement.''' {{math|'''u'''<sub>1</sub>, '''v'''<sub>1</sub>}} are left and right-singular vectors of {{math|'''M'''}} with corresponding singular value ''σ''<sub>1</sub>.\n\n'''Proof.''' Similar to the eigenvalues case, by assumption the two vectors satisfy the Lagrange multiplier equation:\n\n:<math>\\nabla \\sigma = \\nabla \\mathbf{u}^\\textsf{T} \\mathbf{M} \\mathbf{v} - \\lambda_1 \\cdot \\nabla \\mathbf{u}^\\textsf{T} \\mathbf{u} - \\lambda_2 \\cdot \\nabla \\mathbf{v}^\\textsf{T} \\mathbf{v}</math>\n\nAfter some algebra, this becomes\n\n:<math>\\begin{align}\n             \\mathbf{M} \\mathbf{v}_{1} &= 2 \\lambda_1 \\mathbf{u}_1 + 0 \\\\\n  \\mathbf{M}^\\textsf{T} \\mathbf{u}_{1} &= 0 + 2 \\lambda_2 \\mathbf{v}_1\n\\end{align}</math>\n\nMultiplying the first equation from left by <math>\\mathbf{u}_1^\\textsf{T}</math> and the second equation from left by <math>\\mathbf{v}_1^\\textsf{T}</math> and taking {{math|{{!!}}'''u'''{{!!}} {{=}} {{!!}}'''v'''{{!!}} {{=}} 1}}  into account gives\n\n:<math>\\sigma_1 = 2\\lambda_1 = 2\\lambda_2.</math>\n\nPlugging this into the pair of equations above, we have\n\n:<math>\\begin{align}\n             \\mathbf{M} \\mathbf{v}_1 &= \\sigma_1 \\mathbf{u}_1\\\\\n  \\mathbf{M}^\\textsf{T} \\mathbf{u}_1 &= \\sigma_1 \\mathbf{v}_1\n\\end{align}</math>\n\nThis proves the statement.\n\nMore singular vectors and singular values can be found by maximizing {{math|''σ''('''u''', '''v''')}} over normalized {{math|'''u''', '''v'''}} which are orthogonal to {{math|'''u'''<sub>1</sub>}} and {{math|'''v'''<sub>1</sub>}}, respectively.\n\nThe passage from real to complex is similar to the eigenvalue case.\n\n== Calculating the SVD ==\n\n=== Numerical approach ===\nThe SVD of a matrix {{math|'''M'''}} is typically computed by a two-step procedure. In the first step, the matrix is reduced to a [[bidiagonal matrix]]. This takes [[big O notation|O]](''mn''<sup>2</sup>) floating-point operations (flop), assuming that ''m'' ≥ ''n''. The second step is to compute the SVD of the bidiagonal matrix. This step can only be done with an [[iterative method]] (as with [[eigenvalue algorithm]]s). However, in practice it suffices to compute the SVD up to a certain precision, like the [[machine epsilon]]. If this precision is considered constant, then the second step takes O(''n'') iterations, each costing O(''n'') flops. Thus, the first step is more expensive, and the overall cost is O(''mn''<sup>2</sup>) flops {{harv|Trefethen|Bau III|1997|loc=Lecture 31}}.\n\nThe first step can be done using [[Householder reflection]]s for a cost of 4''mn''<sup>2</sup> − 4''n''<sup>3</sup>/3 flops, assuming that only the singular values are needed and not the singular vectors. If ''m'' is much larger than ''n'' then it is advantageous to first reduce the matrix ''M'' to a triangular matrix with the [[QR decomposition]] and then use Householder reflections to further reduce the matrix to bidiagonal form; the combined cost is 2''mn''<sup>2</sup> + 2''n''<sup>3</sup> flops {{harv|Trefethen|Bau III|1997|loc=Lecture 31}}.\n\nThe second step can be done by a variant of the [[QR algorithm]] for the computation of eigenvalues, which was first described by {{harvtxt|Golub|Kahan|1965}}. The [[LAPACK]] subroutine DBDSQR<ref>[http://www.netlib.org/lapack/double/dbdsqr.f Netlib.org]</ref> implements this iterative method, with some modifications to cover the case where the singular values are very small {{harv|Demmel|Kahan|1990}}. Together with a first step using Householder reflections and, if appropriate, QR decomposition, this forms the DGESVD<ref>[http://www.netlib.org/lapack/double/dgesvd.f Netlib.org]</ref> routine for the computation of the singular value decomposition.\n\nThe same algorithm is implemented in the [[GNU Scientific Library]] (GSL). The GSL also offers an alternative method, which uses a one-sided [[Jacobi orthogonalization]] in step 2 {{harv|GSL Team|2007}}. This method computes the SVD of the bidiagonal matrix by solving a sequence of 2 × 2 SVD problems, similar to how the [[Jacobi eigenvalue algorithm]] solves a sequence of 2 × 2 eigenvalue methods {{harv|Golub|Van Loan|1996|loc=§8.6.3}}. Yet another method for step 2 uses the idea of [[divide-and-conquer eigenvalue algorithm]]s {{harv|Trefethen|Bau III|1997|loc=Lecture 31}}.\n\nThere is an alternative way which is not explicitly using the eigenvalue decomposition.<ref>[http://www.mathworks.co.kr/matlabcentral/fileexchange/12674-simple-svd mathworks.co.kr/matlabcentral/fileexchange/12674-simple-svd]</ref> Usually the singular value problem of a matrix {{math|'''M'''}} is converted into an equivalent symmetric eigenvalue problem such as {{math|'''M M'''<sup>*</sup>}}, {{math|'''M'''<sup>*</sup>'''M'''}}, or \n:<math> \\begin{pmatrix} \\mathbf{O} & \\mathbf{M} \\\\ \\mathbf{M}^* & \\mathbf{O} \\end{pmatrix}. </math>\nThe approaches using eigenvalue decompositions are based on [[QR algorithm]] which is well-developed to be stable and fast. \nNote that the singular values are real and right- and left- singular vectors are not required to form any similarity transformation. Alternating [[QR decomposition]] and [[LQ decomposition]] can be claimed to use iteratively to find the real diagonal matrix with [[Hermitian matrix|Hermitian matrices]]. [[QR decomposition]] gives {{math|'''M''' ⇒ '''Q''' '''R'''}} and [[LQ decomposition]] of {{math|'''R'''}} gives {{math|'''R''' ⇒ '''L''' '''P'''<sup>*</sup>}}. Thus, at every iteration, we have {{math|'''M''' ⇒ '''Q''' '''L''' '''P'''<sup>*</sup>}}, update {{math|'''M''' ⇐ '''L'''}} and repeat the orthogonalizations.\nEventually, [[QR decomposition]] and [[LQ decomposition]] iteratively provide unitary matrices for left- and right- singular matrices, respectively. \nThis approach does not come with any acceleration method such as spectral shifts and deflation as in QR algorithm. It is because the shift method is not easily defined without using similarity transformation.  But it is very simple to implement where the speed does not matter. Also it give us a good interpretation that only orthogonal/unitary transformations can obtain SVD as the [[QR algorithm]] can calculate the [[eigenvalue decomposition]].\n\n=== Analytic result of 2 × 2 SVD ===\nThe singular values of a 2 × 2 matrix can be found analytically. Let the matrix be\n<math>\\mathbf{M} = z_0\\mathbf{I} + z_1\\sigma_1 + z_2\\sigma_2 + z_3\\sigma_3</math>\n\nwhere <math>z_i \\in \\mathbb{C}</math> are complex numbers that parameterize the matrix, {{math|'''I'''}} is the identity matrix, and <math>\\sigma_i</math> denote the [[Pauli matrices]]. Then its two singular values are given by\n:<math>\\begin{align}\n\\sigma_\\pm &= \\sqrt{|z_0|^2 + |z_1|^2 + |z_2|^2 + |z_3|^2 \\pm \\sqrt{(|z_0|^2 + |z_1|^2 + |z_2|^2 + |z_3|^2)^2 - |z_0^2 - z_1^2 - z_2^2 - z_3^2|^2}} \\\\\n&= \\sqrt{|z_0|^2 + |z_1|^2 + |z_2|^2 + |z_3|^2 \\pm 2\\sqrt{(\\operatorname{Re}z_0z_1^*)^2 + (\\operatorname{Re}z_0z_2^*)^2 + (\\operatorname{Re}z_0z_3^*)^2 + (\\operatorname{Im}z_1z_2^*)^2 + (\\operatorname{Im}z_2z_3^*)^2 + (\\operatorname{Im}z_3z_1^*)^2}}\n\\end{align}</math>\n\n==Reduced SVDs==\nIn applications it is quite unusual for the full SVD, including a full unitary decomposition of the null-space of the matrix, to be required.  Instead, it is often sufficient (as well as faster, and more economical for storage) to compute a reduced version of the SVD.  The following can be distinguished for an ''m''×''n'' matrix ''M'' of rank ''r'':\n\n===Thin SVD===\n:<math>\\mathbf{M} = \\mathbf{U}_n \\boldsymbol{\\Sigma}_n \\mathbf{V}^*</math>\n\nOnly the ''n'' column vectors of ''U'' corresponding to the row vectors of ''V*'' are calculated. The remaining column vectors of ''U'' are not calculated. This is significantly quicker and more economical than the full SVD if ''n''&nbsp;≪&nbsp;''m''. The matrix ''U''<sub>'''n''</sub> is thus ''m''×''n'', Σ<sub>''n''</sub> is ''n''×''n'' diagonal, and ''V'' is ''n''×''n''.\n\nThe first stage in the calculation of a thin SVD will usually be a [[QR decomposition]] of ''M'', which can make for a significantly quicker calculation if&nbsp;''n''&nbsp;≪&nbsp;''m''.\n\n===Compact SVD===\n:<math>\\mathbf{M} = \\mathbf{U}_r \\boldsymbol{\\Sigma}_r \\mathbf{V}_r^*</math>\n\nOnly the ''r'' column vectors of ''U'' and ''r'' row vectors of ''V*'' corresponding to the non-zero singular values Σ<sub>''r''</sub> are calculated. The remaining vectors of ''U'' and ''V*'' are not calculated. This is quicker and more economical than the thin SVD if ''r''&nbsp;≪&nbsp;''n''. The matrix ''U''<sub>''r''</sub> is thus ''m''×''r'', Σ<sub>''r''</sub> is ''r''×''r'' diagonal, and ''V''<sub>''r''</sub>* is ''r''×''n''.\n\n===Truncated SVD===\n:<math>\\tilde{\\mathbf{M}} = \\mathbf{U}_t \\boldsymbol{\\Sigma}_t \\mathbf{V}_t^*</math>\n\nOnly the ''t'' column vectors of ''U'' and ''t'' row vectors of ''V*'' corresponding to the ''t'' largest singular values Σ<sub>''t''</sub> are calculated. The rest of the matrix is discarded. This can be much quicker and more economical than the compact SVD if ''t''≪''r''. The matrix ''U''<sub>''t''</sub> is thus ''m''×''t'', Σ<sub>''t''</sub> is ''t''×''t'' diagonal, and ''V''<sub>''t''</sub>* is ''t''×''n''.\n\nOf course the truncated SVD is no longer an exact decomposition of the original matrix ''M'', but as discussed [[#Low-rank matrix approximation|above]], the approximate matrix <math>\\tilde{\\mathbf{M}}</math> is in a very useful sense the closest approximation to ''M'' that can be achieved by a matrix of rank&nbsp;''t''.\n\n==Norms==\n\n=== Ky Fan norms ===\nThe sum of the ''k'' largest singular values of ''M'' is a [[matrix norm]], the [[Ky Fan]] ''k''-norm of ''M''.<ref>{{Cite journal|last=Fan|first=Ky.|date=1951|title=Maximum properties and inequalities for the eigenvalues of completely continuous operators|journal=Proceedings of the National Academy of Sciences of the United States of America|volume=37|issue=11|pages=760–766|doi=10.1073/pnas.37.11.760|pmid=16578416|pmc=1063464|bibcode=1951PNAS...37..760F}}</ref>\n\nThe first of the Ky Fan norms, the Ky Fan 1-norm, is the same as the [[operator norm]] of ''M'' as a linear operator with respect to the Euclidean norms of ''K''<sup>''m''</sup> and ''K''<sup>''n''</sup>. In other words, the Ky Fan 1-norm is the operator norm induced by the standard ''ℓ''<sup>2</sup> Euclidean inner product. For this reason, it is also called the operator 2-norm. One can easily verify the relationship between the Ky Fan 1-norm and singular values. It is true in general, for a bounded operator ''M'' on (possibly infinite-dimensional) Hilbert spaces\n\n:<math>\\| \\mathbf{M} \\| = \\| \\mathbf{M}^* \\mathbf{M} \\|^\\frac{1}{2}</math>\n\nBut, in the matrix case, (''M* M'')<sup>½</sup> is a [[normal matrix]], so ||''M* M''||<sup>½</sup> is the largest eigenvalue of (''M* M'')<sup>½</sup>, i.e. the largest singular value of ''M''.\n\nThe last of the Ky Fan norms, the sum of all singular values, is the [[trace class|trace norm]] (also known as the 'nuclear norm'), defined by ||''M''|| = Tr[(''M* M'')<sup>½</sup>] (the eigenvalues of ''M* M'' are the squares of the singular values).\n\n=== Hilbert–Schmidt norm{{Anchor|Hilbert–Schmidt norm|Hilbert-Schmidt norm|Hilbert–Schmidt|Hilbert-Schmidt}} ===\nThe singular values are related to another norm on the space of operators. Consider the [[Hilbert–Schmidt operator|Hilbert–Schmidt]] inner product on the {{math|''n'' × ''n''}} matrices, defined by\n\n:<math>\\langle \\mathbf{M}, \\mathbf{N} \\rangle = \\operatorname{trace} \\left (\\mathbf{N}^*\\mathbf{M} \\right ).</math>\n\nSo the induced norm is\n\n:<math>\\| \\mathbf{M} \\| = \\sqrt{\\langle \\mathbf{M}, \\mathbf{M}\\rangle} = \\sqrt{\\operatorname{trace} \\left (\\mathbf{M}^*\\mathbf{M} \\right )}.</math>\n\nSince the trace is invariant under unitary equivalence, this shows\n\n:<math>\\| \\mathbf{M} \\| = \\sqrt{\\sum_i \\sigma_i ^2}</math>\n\nwhere {{mvar|σ<sub>i</sub>}} are the singular values of {{math|'''M'''}}. This is called the '''[[Frobenius norm]]''', '''Schatten 2-norm''', or '''Hilbert–Schmidt norm''' of {{math|'''M'''}}. Direct calculation shows that the Frobenius norm of {{math|'''M''' {{=}} (''m''<sub>''ij''</sub>)}} coincides with:\n\n:<math>\\sqrt{\\sum_{ij} | m_{ij} |^2}.</math>\n\nIn addition, the Frobenius norm and the trace norm (the nuclear norm) are special cases of the [[Schatten norm]].\n\n== Variations and generalizations ==\n\n=== Mode-''k'' representation ===\n<math>M = U S V^\\textsf{T}</math> can be represented using [[mode-k multiplication|mode-''k'' multiplication]] of matrix <math>S</math> applying <math>\\times_1 U</math> then <math>\\times_2 V</math> on the result; that is <math>M = S \\times_1 U\\times_2 V</math>.<ref>{{Cite journal|last=De Lathauwer|first=L.|last2=De Moor|first2=B.|last3=Vandewalle|first3=J.|date=2000-01-01|title=A Multilinear Singular Value Decomposition|journal=SIAM Journal on Matrix Analysis and Applications|volume=21|issue=4|pages=1253–1278|doi=10.1137/S0895479896305696|issn=0895-4798|citeseerx=10.1.1.102.9135}}</ref>\n\n=== Tensor SVD ===\nTwo types of tensor decompositions exist, which generalise the SVD to multi-way arrays. One of them decomposes a tensor into a sum of rank-1 tensors, which is called a [[tensor rank decomposition]]. The second type of decomposition computes the orthonormal subspaces associated with the different factors appearing in the tensor product of vector spaces in which the tensor lives. This decomposition is referred to in the literature as the [[Higher-order singular value decomposition|higher-order SVD]] (HOSVD) or [[Tucker decomposition|Tucker3/TuckerM]]. In addition, [[multilinear principal component analysis]] in [[multilinear subspace learning]] involves the same mathematical operations as Tucker decomposition, being used in a different context of [[dimensionality reduction]].\n\n=== Scale-invariant SVD ===\n\nThe SVD singular values of a matrix ''A'' are unique and are invariant with respect to left and/or right unitary transformations of  ''A''. In other words, the singular values of ''UAV'', for unitary ''U'' and ''V'', are equal to the singular values of ''A''. This is an important property for applications in which it is necessary to preserve Euclidean distances, and invariance with respect to rotations.\n\nThe Scale-Invariant SVD, or SI-SVD,<ref>{{citation|last=Uhlmann |first=Jeffrey |title=A Generalized Matrix Inverse that is Consistent with Respect to Diagonal Transformations |series=SIAM Journal on Matrix Analysis |year=2018 |volume=239:2 |pages=781–800 |url=http://faculty.missouri.edu/uhlmannj/UC-SIMAX-Final.pdf}}</ref> is analogous to the conventional SVD except that its uniquely-determined singular values are invariant with respect to diagonal transformations of  ''A''. In other words, the singular values of ''DAE'', for nonsingular diagonal matrices ''D'' and ''E'', are  equal to the singular values of ''A''. This is an important property for applications for which invariance to the choice of units on variables (e.g., metric versus imperial units) is needed.\n\n=== HOSVD of functions – numerical reconstruction – TP model transformation ===\nTP model transformation numerically reconstruct the HOSVD of functions. For further details please visit:\n\n* [[HOSVD-based canonical form of TP functions and qLPV models]]\n* [[Tensor product model transformation]]\n* [[TP model transformation in control theory]]\n\n=== Bounded operators on Hilbert spaces ===\nThe factorization {{math|'''M''' {{=}} '''UΣV'''<sup>∗</sup>}} can be extended to a [[bounded operator]] ''M'' on a separable Hilbert space ''H''. Namely, for any bounded operator ''M'', there exist a [[partial isometry]] ''U'', a unitary ''V'', a measure space (''X'',&nbsp;''μ''), and a non-negative measurable ''f'' such that\n\n:<math>\\mathbf{M} = \\mathbf{U} T_f \\mathbf{V}^*</math>\n\nwhere <math>T_f</math> is the [[multiplication operator|multiplication by ''f'']] on ''L''<sup>2</sup>(''X'', ''μ'').\n\nThis can be shown by mimicking the linear algebraic argument for the matricial case above. ''VT''<sub>''f''</sub> V* is the unique positive square root of ''M*M'', as given by the [[Borel functional calculus]] for [[self adjoint operator]]s. The reason why ''U'' need not be unitary is because, unlike the finite-dimensional case, given an isometry ''U''<sub>1</sub> with nontrivial kernel, a suitable ''U''<sub>2</sub> may not be found such that\n\n:<math>\\begin{bmatrix} U_1 \\\\ U_2 \\end{bmatrix}</math>\n\nis a unitary operator.\n\nAs for matrices, the singular value factorization is equivalent to the [[polar decomposition]] for operators: we can simply write\n\n:<math>\\mathbf{M} = \\mathbf{U} \\mathbf{V}^* \\cdot \\mathbf{V} T_f \\mathbf{V}^*</math>\n\nand notice that ''U V*'' is still a partial isometry while ''VT''<sub>''f''</sub> ''V''* is positive.\n\n=== Singular values and compact operators ===\nThe notion of singular values and left/right-singular vectors can be extended to [[compact operator on Hilbert space]] as they have a discrete spectrum. If {{mvar|T}} is compact, every non-zero {{mvar|λ}} in its spectrum is an eigenvalue. Furthermore, a compact self adjoint operator can be diagonalized by its eigenvectors. If {{math|'''M'''}} is compact, so is {{math|'''M'''<sup>∗</sup>'''M'''}}. Applying the diagonalization result, the unitary image of its positive square root {{mvar|T<sub>f</sub>&nbsp;}} has a set of orthonormal eigenvectors {{math|{''e<sub>i</sub>''} }} corresponding to strictly positive eigenvalues {{math|{''σ''<sub>''i''</sub>}.}} For any {{math|''ψ'' ∈ ''H''}},\n\n:<math>\\mathbf{M} \\psi = \\mathbf{U} T_f \\mathbf{V}^* \\psi = \\sum_i \\left \\langle \\mathbf{U} T_f \\mathbf{V}^* \\psi, \\mathbf{U} e_i \\right \\rangle \\mathbf{U} e_i = \\sum_i \\sigma_i \\left \\langle \\psi, \\mathbf{V} e_i \\right \\rangle \\mathbf{U} e_i</math>\n\nwhere the series converges in the norm topology on {{mvar|H}}. Notice how this resembles the expression from the finite-dimensional case. {{mvar|σ<sub>i</sub>}} are called the singular values of {{math|'''M'''}}. {{math|{'''U'''''e<sub>i</sub>''} }} (resp. {{math|{'''V'''''e<sub>i</sub>''} }}) can be considered the left-singular (resp. right-singular) vectors of {{math|'''M'''}}.\n\nCompact operators on a Hilbert space are the closure of [[finite-rank operator]]s in the uniform operator topology. The above series expression gives an explicit such representation. An immediate consequence of this is:\n\n:'''Theorem.''' {{math|'''M'''}} is compact if and only if {{math|'''M'''<sup>∗</sup>'''M'''}} is compact.\n\n==History==\nThe singular value decomposition was originally developed by [[differential geometry|differential geometers]], who wished to determine whether a real [[bilinear form]] could be made equal to another by independent orthogonal transformations of the two spaces it acts on. [[Eugenio Beltrami]] and [[Camille Jordan]] discovered independently, in 1873 and 1874 respectively, that the singular values of the bilinear forms, represented as a matrix, form a [[Complete set of invariants|complete set]] of [[invariant (mathematics)|invariant]]s for bilinear forms under orthogonal substitutions. [[James Joseph Sylvester]] also arrived at the singular value decomposition for real square matrices in 1889, apparently independently of both Beltrami and Jordan. Sylvester called the singular values the ''canonical multipliers'' of the matrix ''A''. The fourth mathematician to discover the singular value decomposition independently is [[Léon Autonne|Autonne]] in 1915, who arrived at it via the [[polar decomposition]]. The first proof of the singular value decomposition for rectangular and complex matrices seems to be by [[Carl Eckart]] and Gale Young in 1936;<ref>{{Cite journal\n |last1=Eckart |first1=C.|authorlink1=Carl Eckart |last2=Young |first2=G. |year=1936  |title=The approximation of one matrix by another of lower rank |journal=[[Psychometrika]] |volume=1 |issue=3 |pages=211–8  |doi=10.1007/BF02288367  |ref=harv }}</ref> they saw it as a generalization of the [[Principal axis theorem|principal axis]] transformation for [[Hermitian matrix|Hermitian matrices]].\n\nIn 1907, [[Erhard Schmidt]] defined an analog of singular values for [[integral operator]]s (which are compact, under some weak technical assumptions); it seems he was unaware of the parallel work on singular values of finite matrices. This theory was further developed by [[Émile Picard]] in 1910, who is the first to call the numbers <math>\\sigma_k</math> ''singular values'' (or in French, ''valeurs singulières'').\n\nPractical methods for computing the SVD date back to [[Ervand Kogbetliantz|Kogbetliantz]] in 1954, 1955 and [[Magnus Hestenes|Hestenes]] in 1958.<ref>{{Cite journal |first=M. R. |last=Hestenes |authorlink=Magnus Hestenes\n |title=Inversion of Matrices by Biorthogonalization and Related Results\n |journal=Journal of the Society for Industrial and Applied Mathematics\n |year=1958 |volume=6 |issue=1 |pages=51–90\n |doi=10.1137/0106005 |mr=0092215 | jstor = 2098862\n |ref=harv\n }}</ref> resembling closely the [[Jacobi eigenvalue algorithm]], which uses plane rotations or [[Givens rotation]]s. However, these were replaced by the method of  [[Gene H. Golub|Gene Golub]] and [[William Kahan]] published in 1965,<ref>{{Cite journal\n | last1=Golub | first1=G. H. | author1-link=Gene H. Golub\n | last2=Kahan | first2=W. | author2-link=William Kahan\n | title=Calculating the singular values and pseudo-inverse of a matrix\n | year=1965\n | journal=Journal of the Society for Industrial and Applied Mathematics, Series B: Numerical Analysis\n | volume=2 | issue=2 | pages=205–224\n | doi=10.1137/0702016 |mr=0183105 | jstor = 2949777\n | ref=harv\n| bibcode=1965SJNA....2..205G }}</ref> which uses [[Householder transformation]]s or reflections.\nIn 1970, Golub and Christian Reinsch<ref>{{Cite journal\n |title=Singular value decomposition and least squares solutions\n |first1=G. H. |last1=Golub |authorlink1=Gene H. Golub\n |first2=C. |last2=Reinsch\n |year=1970\n |journal=Numerische Mathematik\n |volume=14 |issue=5 |pages=403–420\n |doi=10.1007/BF02163027 |mr=1553974\n |ref=harv\n }}</ref> published a variant of the Golub/Kahan algorithm that is still the one most-used today.\n\n==See also==\n{{columns-list|colwidth=22em|\n*[[Canonical correlation]]\n*[[Canonical form]]\n*[[Correspondence analysis]] (CA)\n*[[Curse of dimensionality]]\n*[[Digital signal processing]]\n*[[Dimensionality reduction]]\n*[[Eigendecomposition of a matrix]]\n*[[Empirical orthogonal functions]] (EOFs)\n*[[Fourier analysis]]\n*[[Generalized singular value decomposition]]\n*[[Singular value#Inequalities about singular values|Inequalities about singular values]]\n*[[K-SVD]]\n*[[Latent semantic analysis]]\n*[[Latent semantic indexing]]\n*[[Linear least squares (mathematics)|Linear least squares]]\n*[[List of Fourier-related transforms]]\n*[[Locality-sensitive hashing]]\n*[[Low-rank approximation]]\n*[[Matrix decomposition]]\n*[[Multilinear principal component analysis]] (MPCA)\n*[[Nearest neighbor search]]\n*[[Non-linear iterative partial least squares]]\n*[[Polar decomposition]]\n*[[Principal component analysis]] (PCA)\n*[[Schmidt decomposition]]\n*[[Singular value]]\n*[[Time series]]\n*[[Two-dimensional singular-value decomposition]] (2DSVD)\n*[[von Neumann's trace inequality]]\n*[[Wavelet compression]]\n}}\n\n==Notes==\n{{Reflist}}\n\n==References==\n* {{Citation | last = Banerjee | first = Sudipto | last2 = Roy | first2 = Anindya | date = 2014 | title = Linear Algebra and Matrix Analysis for Statistics | series = Texts in Statistical Science | publisher = Chapman and Hall/CRC | edition =  1st | isbn =  978-1420095388}}\n* {{Cite book | last2=Bau III | first2=David | last1=Trefethen | first1=Lloyd N. | author1-link = Lloyd N. Trefethen | title=Numerical linear algebra | publisher=Society for Industrial and Applied Mathematics | location=Philadelphia | isbn=978-0-89871-361-9 | year=1997 |ref=harv}}\n* {{Cite journal | last1=Demmel | first1=James | author1-link = James Demmel | last2=Kahan | first2=William | author2-link=William Kahan | title=Accurate singular values of bidiagonal matrices | doi=10.1137/0911052 | year=1990 | journal= SIAM Journal on Scientific and Statistical Computing| volume=11 | issue=5 | pages=873–912 | citeseerx=10.1.1.48.3740 }}\n* {{Cite journal | last1=Golub | first1=Gene H. | author1-link=Gene H. Golub | last2=Kahan | first2=William | author2-link=William Kahan | title=Calculating the singular values and pseudo-inverse of a matrix | jstor=2949777 | year=1965 | journal=Journal of the Society for Industrial and Applied Mathematics, Series B: Numerical Analysis | volume=2 | issue=2 | pages=205–224 | doi=10.1137/0702016 | bibcode=1965SJNA....2..205G }}\n* {{Cite book | last1=Golub | first1=Gene H. | author1-link=Gene H. Golub | last2=Van Loan | first2=Charles F. | author2-link=Charles F. Van Loan | title=Matrix Computations | publisher=Johns Hopkins | edition=3rd | isbn=978-0-8018-5414-9 | year=1996 }}\n* {{Cite book | last1=GSL Team | title=GNU Scientific Library. Reference Manual | year=2007 | chapter=§14.4 Singular Value Decomposition | chapterurl=https://www.gnu.org/software/gsl/manual/html_node/Singular-Value-Decomposition.html }}\n* Halldor, Bjornsson and Venegas, Silvia A. (1997). [http://brunnur.vedur.is/pub/halldor/TEXT/eofsvd.html \"A manual for EOF and SVD analyses of climate data\"]. McGill University, CCGCR Report No. 97-1, Montréal, Québec, 52pp.\n* {{Cite journal | doi = 10.1007/BF01937276 | last1 = Hansen | first1 = P. C. | year = 1987 | title = The truncated SVD as a method for regularization | journal = BIT | volume = 27 | issue = 4 | pages = 534–553 | ref = harv }}\n*{{cite book |author1=Horn, Roger A. |author2=Johnson, Charles R. |title=Matrix Analysis |publisher=Cambridge University Press |year=1985 |isbn=978-0-521-38632-6 |chapter=Section 7.3 }}\n*{{cite book |author1=Horn, Roger A. |author2=Johnson, Charles R. |title=Topics in Matrix Analysis |publisher=Cambridge University Press |year=1991 |isbn=978-0-521-46713-1 |chapter=Chapter 3 }}\n*{{cite book |author=Samet, H. |title=Foundations of Multidimensional and Metric Data Structures |publisher=Morgan Kaufmann |year=2006 |isbn=978-0-12-369446-1 }}\n*{{cite book |author=Strang G. |title=Introduction to Linear Algebra |publisher=Wellesley-Cambridge Press |year=1998 |isbn=978-0-9614088-5-5 |edition=3rd |chapter=Section 6.7 }}\n* {{Cite journal | last1=Stewart | first1=G. W. | title=On the Early History of the Singular Value Decomposition | url=http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.23.1831 | doi=10.1137/1035134 | jstor=2132388|year=1993 | journal=SIAM Review | volume=35 | issue=4 | pages=551–566 |ref=harv | citeseerx=10.1.1.23.1831 }}\n*{{cite book |last1=Wall|first1 = Michael E.|first2 = Andreas|last2 = Rechtsteiner|first3 = Luis M.|last3 = Rocha |chapter=Singular value decomposition and principal component analysis |chapterurl=http://public.lanl.gov/mewall/kluwer2002.html |editor1=D.P. Berrar |editor2=W. Dubitzky |editor3=M. Granzow |title=A Practical Approach to Microarray Data Analysis |publisher=Kluwer |location=Norwell, MA |year=2003 |pages=91–109 }}\n*{{Citation|last1=Press|first1=WH|last2=Teukolsky|first2=SA|last3=Vetterling|first3=WT|last4=Flannery|first4=BP|year=2007|title=Numerical Recipes: The Art of Scientific Computing|edition=3rd|publisher=Cambridge University Press| location=New York|isbn=978-0-521-88068-8|chapter=Section 2.6|chapter-url=http://apps.nrbook.com/empanel/index.html?pg=65}}\n\n== External links ==\n*[http://engineerjs.com/doc/ejs/engine/linalg-1/_svd.html Online SVD calculator]\n\n{{Numerical linear algebra}}\n{{Functional Analysis}}\n\n{{DEFAULTSORT:Singular Value Decomposition}}\n[[Category:Singular value decomposition| ]]\n[[Category:Linear algebra]]\n[[Category:Numerical linear algebra]]\n[[Category:Matrix theory]]\n[[Category:Matrix decompositions]]\n[[Category:Functional analysis]]"
    },
    {
      "title": "SLEPc",
      "url": "https://en.wikipedia.org/wiki/SLEPc",
      "text": "{{ infobox software\n| name                   = SLEPc\n| latest_release_version = 3.11\n| latest_release_date    = 29 Mar 2019\n| operating_system       = [[Linux]], [[Unix]], [[Mac OS X]], [[Microsoft Windows|Windows]]\n| license                = [[BSD_licenses#2-clause license (\"Simplified BSD License\" or \"FreeBSD License\")|BSD 2-clause license]]\n| language               = C (main language),C++, FORTRAN\n| genre                   = Scientific simulation software\n| website                = http://slepc.upv.es\n}}\n\n'''SLEPc'''<ref>{{cite web |author1=V. Hernandez |author2=J. E. Roman |author3=V. Vidal  |last-author-amp=yes |year=2005 |title=SLEPc: A Scalable and Flexible Toolkit for the Solution of Eigenvalue Problems|publisher=ACM Trans. Math. Softw. |doi=10.1145/1089014.1089019|url=http://dl.acm.org/citation.cfm?doid=1089014.1089019}}</ref> is a [[software library]] for the parallel computation of [[eigenvalues and eigenvectors]] of large, sparse matrices. It can be seen as a module of [[Portable, Extensible Toolkit for Scientific Computation|PETSc]] that provides solvers for different types of eigenproblems, including linear (standard and generalized) and nonlinear ([[quadratic eigenvalue problem|quadratic]], polynomial and [[nonlinear eigenproblem|general]]), as well as the [[singular value decomposition|SVD]]. Recent versions also include support for [[matrix function|matrix functions]]. It uses the [[Message Passing Interface|MPI]] standard for parallelization. Both real and complex arithmetic are supported, with single, double and quadruple precision.\n\nWhen using SLEPc, the application programmer can use any of the PETSc's data structures and solvers. Other PETSc features are incorporated into SLEPc as well, such as command-line option setting, automatic profiling, error checking, portability to virtually all computing platforms, etc.\n\n== Components ==\n'''EPS''' provides iterative algorithms for linear eigenvalue problems.\n* Krylov methods such as Krylov-Schur, [[Arnoldi iteration|Arnoldi]] and [[Lanczos algorithm|Lanczos]].\n* Davidson methods such as Generalized Davidson and Jacobi-Davidson.\n* Conjugate gradient methods such as LOBPCG.\n* A contour integral solver (CISS).\n* Interface to some external eigensolvers, such as [[ARPACK]] and [[BLOPEX]].\n* Customization options include: number of wanted eigenvalues, tolerance, size of the employed subspaces, part of the spectrum of interest.\n\n'''ST''' encapsulates [[Preconditioner#Spectral transformations|spectral transformations]] and other [[preconditioners]] for eigenvalue problems.\n* Shift-and-invert and Cayley spectral transformations.\n* Support for preconditioned eigensolvers (such as Jacobi-Davidson) by using the preconditioners provided by PETSc.\n\n'''SVD''' contains solvers for the [[singular value decomposition]].\n* Solvers based on the cross-product matrix or the cyclic matrix, that rely on EPS solvers.\n* Specific solvers based on [[bidiagonalization]] such as Golub-Kahan-Lanczos and a thick-restarted variant.\n\n'''PEP''' is intended for polynomial eigenproblems, including the [[quadratic eigenvalue problem]].\n* Solvers based on explicit linearization, that rely on EPS solvers.\n* Solvers that perform the linearization implicitly in a memory-efficient way, such as Q-Arnoldi.\n* A Jacobi-Davidson solver for PEP.\n\n'''NEP''' provides functionality for the solution of the [[nonlinear eigenproblem]].\n* Basic solvers such as residual inverse iteration and successive linear problems.\n* A solver based on polynomial interpolation that relies on PEP solvers.\n* A solver based on rational interpolation (NLEIGS).\n* A contour integral solver (CISS).\n\n'''MFN''' can be used to compute the action of a [[matrix function]] on a vector.\n* A restarted Krylov solver.\n\n== See also ==\n* [[Portable, Extensible Toolkit for Scientific Computation]] (PETSc)\n* [[List of numerical libraries]]\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [http://slepc.upv.es The Official SLEPc web site]\n\n[[Category:Numerical libraries]]\n[[Category:Numerical linear algebra]]\n[[Category:Scientific simulation software]]"
    },
    {
      "title": "Sparse approximation",
      "url": "https://en.wikipedia.org/wiki/Sparse_approximation",
      "text": "'''Sparse Approximation''' (also known as '''Sparse Representation''') theory deals with [[Sparsity|sparse]] solutions for [[system of linear equations|systems of linear equations]]. Techniques for finding these solutions and exploiting them in applications have found wide use in [[image processing]], [[signal processing]], [[machine learning]], [[medical imaging]], and more.\n\n== Sparse decomposition ==\n\n=== Noiseless observations ===\nConsider a [[System of linear equations|linear system of equations]] <math>x = D\\alpha</math>, where <math>D</math> is an [[underdetermined system|underdetermined]] <math>m\\times p</math> [[matrix (mathematics)|matrix]] <math>(m < p)</math> and <math>x \\in \\mathbb{R}^m,\\alpha \\in \\mathbb{R}^p</math>. The matrix <math>D</math> (typically assumed to be full-rank) is referred to as the dictionary, and <math>x</math> is a signal of interest. The core sparse representation problem is defined as the quest for the sparsest possible representation <math>\\alpha</math> satisfying <math>x = D\\alpha</math>. Due to the underdetermined nature of <math>D</math>, this linear system admits in general infinitely many possible solutions, and among these we seek the one with the fewest non-zeros. Put formally, we solve \n:<math>\n\\min_{\\alpha \\in \\mathbb{R}^p} \\|\\alpha\\|_0 \\text{ subject to } x = D\\alpha,\n</math>\nwhere <math>\\|\\alpha\\|_0 = \\#\\{ i : \\alpha_i \\neq 0, \\, i=1,\\ldots,p \\}</math> is the <math>\\ell_0</math> pseudo-norm, which counts the [[Cardinality|number]] of non-zero components of <math>\\alpha</math>. This problem is known to be NP-Hard with a reduction to NP-complete subset selection problems in [[combinatorial optimization]].\n\nSparsity of <math>\\alpha</math> implies that only a few (<math>k \\ll m < p</math>) components in it are non-zero. The underlying motivation for such a sparse decomposition is the desire to provide the simplest possible explanation of <math>x</math> as a linear combination of as few as possible columns from <math>D</math>, also referred to as atoms. As such, the signal <math>x</math> can be viewed as a molecule composed of a few fundamental elements taken from <math>D</math>.\n\nWhile the above posed problem is indeed NP-Hard, its solution can often be found using approximation algorithms. One such option is a convex [[Relaxation (approximation)|relaxation]] of the problem, obtained by using the <math>\\ell_1</math>-norm instead of <math>\\ell_0</math>, where <math>\\|\\alpha\\|_1</math> simply sums the absolute values of the entries in <math>\\alpha</math>. This is known as the [[basis pursuit|Basis Pursuit]] (BP) algorithm, which can be handled using any [[linear programming]] solver. An alternative approximation method is a greedy technique, such as the [[matching pursuit|Matching Pursuit]] (MP), which finds the location of the non-zeros one at a time.\n\nSurprisingly, under mild conditions on <math>D</math> (using the [[spark (mathematics)|Spark]], the [[Mutual coherence (linear algebra)|Mutual Coherence]] or the [[restricted isometry property|Restricted Isometry Property]]) and the level of sparsity in the solution, <math>k</math>, the sparse representation problem can be shown to have a unique solution, and BP and MP are guaranteed to find it perfectly.<ref name=\"donohoelad2003\">{{cite journal\n | author = Donoho, D.L. and Elad, M. \n | year = 2003\n | title =  Optimally sparse representation in general (nonorthogonal) dictionaries via L1 minimization\n | journal = Proceedings of the National Academy of Sciences\n | volume = 100\n | issue = 5\n | pages = 2197–2202\n | url = http://www.cs.technion.ac.il/~elad/publications/journals/2003/15_New_Sparseness_IT.pdf\n | doi = 10.1073/pnas.0437847100 \n| pmid = 16576749\n | pmc = 153464\n | bibcode = 2003PNAS..100.2197D\n }}</ref>\n<ref name=\"Tropp2004\">{{cite journal\n | author = Tropp, J.A.\n | year = 2004\n | title =  Greed is good: Algorithmic results for sparse approximation\n | journal = IEEE Transactions on Information Theory\n | volume = 50\n | number = 10\n | pages = 2231–2242\n | url = https://authors.library.caltech.edu/9035/1/TROieeetit04a.pdf\n | doi = 10.1109/TIT.2004.834793\n| citeseerx = 10.1.1.321.1443\n }}</ref>\n<ref name=\"donoho2006most\">{{cite journal\n | author = Donoho, D.L.\n | year = 2006\n | title = For most large underdetermined systems of linear equations the minimal l1-norm solution is also the sparsest solution\n | journal = Communications on Pure and Applied Mathematics\n | volume = 56\n | number = 6\n | pages = 797–829\n | url = http://www-stat.stanford.edu/~donoho/Reports/2004/l1l0approx.pdf\n | doi = 10.1002/cpa.20132\n}}</ref>\n\n=== Noisy observations ===\nOften the observed signal <math>x</math> is noisy. By relaxing the equality constraint and imposing an [[L2 norm#Euclidean norm|<math>\\ell_2</math>]]-norm on the data-fitting term, the sparse decomposition problem becomes \n:<math>\n\\min_{\\alpha \\in \\mathbb{R}^p} \\|\\alpha\\|_0 \\text{ subject to } \\|x - D\\alpha \\|_2^2 \\le \\epsilon^2, \n</math>\nor put in a Lagrangian form, \n:<math>\n\\min_{\\alpha \\in \\mathbb{R}^p} \\lambda \\|\\alpha\\|_0 + \\frac{1}{2}\\|x - D\\alpha \\|_2^2,   \n</math>\nwhere <math>\\lambda</math> is replacing the <math>\\epsilon</math>.\n\nJust as in the noiseless case, these two problems are NP-Hard in general, but can be approximated using pursuit algorithms. More specifically, changing the <math>\\ell_0</math> to an <math>\\ell_1</math>-norm, we obtain \n:<math>\n\\min_{\\alpha \\in \\mathbb{R}^p} \\lambda \\|\\alpha\\|_1 + \\frac{1}{2} \\|x - D\\alpha \\|_2^2, \n</math>\nwhich is known as the [[basis pursuit denoising]]. Similarly, [[matching pursuit]] can be used for approximating the solution of the above problems, finding the locations of the non-zeros one at a time until the error threshold is met. Here as well, theoretical guarantees suggest that BP and MP lead to nearly optimal solutions depending on the properties of <math>D</math> and the cardinality of the solution <math>k</math>.\n<ref name=\"Elad2010\">{{cite book\n | author = Elad, M. \n | year = 2010\n | publisher = Springer\n | title = Sparse and Redundant Representations: From Theory to Applications in Signal and Image Processing\n | doi =  10.1007/978-1-4419-7011-4\n| isbn = 978-1441970107\n | citeseerx = 10.1.1.331.8963\n }}</ref>\n<ref name=\"DonohoEladTem2006\">{{cite journal\n | author = Donoho, D.L., Elad, M. and Templyakov, V. \n | year = 2006\n | title = Stable recovery of sparse overcomplete representations in the presence of noise\n | journal = IEEE Transactions on Information Theory\n | volume = 52\n | number = 1\n | pages = 6–18\n | url = http://www.cs.technion.ac.il/~elad/publications/journals/2004/23_Stability_IEEE_TIT.pdf\n | doi =  10.1109/TIT.2005.860430\n| citeseerx = 10.1.1.125.5610\n }}</ref>\n<ref name=\"Tropp2006\">{{cite journal\n | author = Tropp, J.A. \n | year = 2006\n | title = Just relax: Convex programming methods for identifying sparse signals in noise\n | journal = IEEE Transactions on Information Theory\n | volume = 52\n | number = 3\n | pages = 1030–1051\n | url = https://authors.library.caltech.edu/9040/1/TROieeetit06.pdf\n | doi =  10.1109/TIT.2005.864420\n| citeseerx = 10.1.1.184.2957\n }}</ref>\nAnother interesting theoretical result refers to the case in which <math>D</math> is unitary. Under this assumption, the problems posed above (with either <math>\\ell_0</math> or <math>\\ell_1</math>) admit closed-form solutions in the form of non-linear shrinkage.<ref name=\"Elad2010\"/>\n\n== Variations ==\nThere are several variations to the basic sparse approximation problem.\n\n'''Structured sparsity''': In the original version of the problem, any of the atoms in the dictionary can be picked. In the structured (block) sparsity model, instead of picking atoms individually, groups of them are to be picked. These groups can be overlapping and of varying size. The objective is to represent <math>x</math> such that it is sparse while forcing this block-structure.<ref>{{cite journal\n | author = Eldar, Y.C, Kuppinger, P. and Bolcskei, H. \n | year = 2009\n | title =  Block-sparse signals: Uncertainty relations and efficient recovery\n | journal = IEEE Transactions on Signal Processing \n | volume = 58\n | issue = 6\n | pages = 3042–3054\n | doi = 10.1109/TSP.2010.2044837\n| arxiv = 0906.3173\n | bibcode = 2010ITSP...58.3042E\n }}</ref>\n\n'''Collaborative (joint) sparse coding''': The original version of the problem is defined for a single signal <math>x</math>. In the collaborative (joint) sparse coding model, a set of signals is available, each believed to emerge from (nearly) the same set of atoms from <math>D</math>. In this case, the pursuit task aims to recover a set of sparse representations that best describe the data while forcing them to share the same (or close-by) support.<ref>{{cite journal\n | author = Tropp, J.A., Gilbert, A.C. and Strauss, M.J. \n | year = 2006\n | title =  Algorithms for simultaneous sparse approximation. Part I: Greedy pursuit\n | journal = Signal Processing \n | volume = 86\n | issue = 3\n | pages = 572–588\n | url = http://www.sciencedirect.com/science/article/pii/S0165168405002227\n | doi = 10.1016/j.sigpro.2005.05.030\n}}</ref>\n\n'''Other structures''': More broadly, the sparse approximation problem can be cast while forcing a specific desired structure on the pattern of non-zero locations in <math>\\alpha</math>. Two cases of interest that have been extensively studied are tree-based structure, and more generally, a Boltzmann distributed support.<ref>{{cite journal\n | author = Peleg, T. Eldar, Y.C. and Elad, M. \n | year = 2012\n | title =  Exploiting Statistical Dependencies in Sparse Representations for Signal Recovery\n | journal = IEEE Transactions on Signal Processing \n | volume = 60\n | issue = 5\n | pages = 2286–2303\n | doi =  10.1109/TSP.2012.2188520\n| arxiv = 1010.5734| bibcode = 2012ITSP...60.2286P\n }}</ref>\n\n== Algorithms ==\n\nAs already mentioned above, there are various approximation (also referred to as '''pursuit''') algorithms that have been developed for addressing the sparse representation problem:\n:<math>\n\\min_{\\alpha \\in \\mathbb{R}^p} \\|\\alpha\\|_0 \\text{ subject to } \\|x - D\\alpha \\|_2^2 \\le \\epsilon^2.\n</math>\nWe mention below a few of these main methods.\n\n* [[Matching pursuit]] is a greedy iterative algorithm for approximately solving the above problem. It works by gradually finding the locations of the non-zeros in <math>\\alpha</math> one at a time. The core idea is to find in each step the column (atom) in <math>D</math> that best correlates with the current residual (initialized to <math>x</math>), and then updating this residual to take the new atom and its coefficient into account. Matching pursuit might pick the same atom multiple times.\n* Orthogonal Matching Pursuit is very similar to Matching Pursuit, with one major difference: in each of the algorithm's step, all the non-zero coefficients are updated by a [[least squares|Least-Squares]]. As a consequence, the residual is orthogonal to the already chosen atoms, and thus an atom cannot be picked more than once.\n* Stage-wise Greedy Methods: Improved variations over the above are algorithms that operate greedily while adding two critical features: (i) the ability to add groups of non-zeros at a time (instead of one non-zero per round); and (ii) including a pruning step in each round in which several of the atoms are discarded from the support. Representatives of this approach are the Subspace-Pursuit algorithm and the CoSaMP.<ref>{{cite journal\n | author = Needell, D. and Tropp, J.A. \n | year = 2009\n | title =  CoSaMP: Iterative signal recovery from incomplete and inaccurate samples\n | journal = Applied and Computational Harmonic Analysis\n | volume = 26\n | issue = 3\n | pages = 301–321\n | url = http://www.sciencedirect.com/science/article/pii/S1063520308000638\n | doi = 10.1016/j.acha.2008.07.002\n| bibcode = 2010A&CHA..28..171W\n | arxiv = 0803.2392\n }}</ref>\n\n* [[basis pursuit denoising|Basis Pursuit]] solves a convex relaxed version of the problem by replacing the <math>\\ell_0</math> by an <math>\\ell_1</math>-norm. Note that this only defines a new objective, while leaving open the question of the algorithm to use for getting the desired solution. Commonly considered such algorithms are the [[Iteratively reweighted least squares|IRLS]], [[Least-angle regression|LARS]], and iterative soft-shrinkage methods.<ref>{{cite journal\n | author = Zibulevsky, M. and Elad, M. \n | year = 2010\n | title =  L1-L2 optimization in signal and image processing\n | journal = IEEE Signal Processing Magazine \n | volume = 27\n | issue = 3\n | pages = 76–88\n | url = http://www.cs.technion.ac.il/~elad/publications/journals/2010/L1L2_IEEE_SPM_2010.pdf\n | doi = 10.1109/MSP.2010.936023\n| bibcode = 2010ISPM...27...76Z\n }}</ref>\n\n* There are several other methods for solving sparse decomposition problems: Homotopy method, [[Coordinate descent]], Iterative Hard-Thresholding, First order [[Proximal gradient method|proximal methods]], which are related to the above-mentioned iterative soft-shrinkage algorithms, and Dantzig selector.\n\n== Applications ==\nSparse approximation ideas and algorithms have been extensively used in [[signal processing]], [[image processing]], [[machine learning]], [[medical imaging]], [[array processing]], [[data mining]], and more. In most of these applications, the unknown signal of interest is modeled as a sparse combination of a few atoms from a given dictionary, and this is used as the [[Regularization (mathematics)|regularization]] of the problem. These problems are typically accompanied by a [[dictionary learning]] mechanism that aims to fit <math>D</math> to best match the model to the given data. The use of sparsity-inspired models has led to state-of-the-art results in a wide set of applications.<ref name=\"survey1\">{{cite journal\n | author = Baraniuk, R.G. Candes, E. Elad, M. and Ma, Y.\n | year = 2010\n | title =  Applications of sparse representation and compressive sensing\n | journal = Proceedings of the IEEE\n | volume = 98\n | number = 6\n | pages = 906–909\n | url = http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5466604\n | doi =  10.1109/JPROC.2010.2047424 \n}}</ref><ref name=\"survey2\">{{cite journal\n | author = Elad, M. Figueiredo, M.A.T., and Ma, Y.\n | year = 2010\n | title =  On the role of sparse and redundant representations in image processing\n | journal = Proceedings of the IEEE\n | volume = 98\n | number = 6\n | pages = 972–982\n | url = https://pdfs.semanticscholar.org/5393/46f7f35f83875d9e86851010ee35d1a47e69.pdf\n | doi = 10.1109/JPROC.2009.2037655 \n| citeseerx = 10.1.1.160.465\n }}</ref><ref name=\"survey3\">{{cite journal\n | author = Plumbley, M.D. Blumensath, T. Daudet, L. Gribonval, R. and Davies, M.E.\n | year = 2010\n | title =  Sparse representations in audio and music: From coding to source separation\n | journal = Proceedings of the IEEE\n | volume = 98\n | number = 6\n | pages = 995–1005\n | url = http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5332363\n | doi = 10.1109/JPROC.2009.2030345\n| citeseerx = 10.1.1.160.1607\n }}</ref> Recent work suggests that there is a tight connection between sparse representation modeling and deep-learning.<ref name=\"deeplearning\">{{cite journal\n | author = Papyan, V. Romano, Y. and Elad, M.\n | year = 2017\n | title =  Convolutional Neural Networks Analyzed via Convolutional Sparse Coding\n | journal = Journal of Machine Learning Research\n | volume = 18\n | number = 83\n | pages = 1–52\n | url = http://jmlr.org/papers/volume18/16-505/16-505.pdf\n| bibcode = 2016arXiv160708194P\n | arxiv = 1607.08194\n }}</ref>\n\n== See also ==\n*[[Compressed sensing]]\n*[[Sparse dictionary learning]]\n*[[K-SVD]]\n*[[Lasso (statistics)]]\n*[[Regularization (disambiguation)]] and [[inverse problems]]\n\n== References ==\n<references />\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "Speakeasy (computational environment)",
      "url": "https://en.wikipedia.org/wiki/Speakeasy_%28computational_environment%29",
      "text": "{{Infobox software\n| name                   = Speakeasy\n| logo                   = Speakeasy Red Logo.png\n| screenshot             =\n| caption                =\n| developer              = Speakeasy Computing Corporation\n| latest_release_version = IV Iota\n| latest_release_date    = 2006\n| programming language  = [[Mortran]], [[FORTRAN]], [[C (programming language)|C]]{{Citation needed|date=August 2013}}\n| operating_system       = [[Windows]], [[macOS]], [[RedHat Linux]], [[SUSE Linux]], [[Mandrake Linux]], [[Debian]], [[Solaris (operating system)|Solaris]], [[HP-UX]]<ref>[http://www.speakeasy.com/about_speakeasy.htm#Platforms Supported platforms]</ref>\n| genre                  = [[List of numerical analysis software|Technical computing]]\n| license                = [[Trialware]]\n| website                = {{URL|speakeasy.com}}\n}}\n{{Infobox programming language\n| name                   = Speakeasy (the interpreted programming language)\n| logo                   =\n| paradigm               = [[Imperative programming|imperative]]\n| year                   = 1964\n| designer               = [[Stanley Cohen (physicist)|Stanley Cohen]]\n| developer              = Speakeasy Computing Corporation\n| latest_release_version =\n| latest_release_date    =\n| latest_test_version    =\n| latest_test_date       =\n| typing                 = [[dynamic typing|dynamic]]\n| implementations        =\n| dialects               =\n| influenced_by          = [[APL (programming language)|APL]]<ref>{{cite thesis |url=http://www.cs.nyu.edu/media/publications/rubinsteyn_alex.pdf |type=Ph.D. |first=Alex |last=Rubinsteyn |title=Runtime Compilation of Array-Oriented Python Programs |publisher=New York University |year=2014 |quote=APL directly inspired Speakeasy}}</ref>\n| influenced             = [[MATLAB]]<ref>{{cite web |url=http://archive.computerhistory.org/resources/access/text/2013/12/102746804-05-01-acc.pdf |title=An interview with CLEVE MOLER Conducted by Thomas Haigh On 8 and 9 March, 2004 Santa Barbara, California |publisher=Computer History Museum |quote=So APL, Speakeasy, LINPACK, EISPACK, and PL0 were the predecessors to MATLAB. |accessdate=2016-12-06}}</ref>\n| operating_system       = \n| license                =\n| website                =\n}}\n\n'''Speakeasy''' is a [[Numerical analysis|numerical computing]] interactive environment also featuring an interpreted [[programming language]]. It was initially developed for internal use at the Physics Division of [[Argonne National Laboratory]] by the theoretical physicist [[Stanley Cohen (physicist)|Stanley Cohen]].<ref>[https://www.amazon.co.uk/An-introduction-SPEAKEASY-Informal-report/dp/B0006WLCUE \"An introduction to Speakeasy - Informal report]</ref> He eventually founded Speakeasy Computing Corporation to make the program available commercially.\n\nSpeakeasy is a very long-lasting numerical package. In fact, the original version of the environment was built around a core dynamic data repository called \"Named storage\" developed in the early 1960s,<ref>[https://books.google.com/books/about/Named_storage.html?id=UIYL2MSgPSwC \"Named storage: a dynamic storage-allocation scheme with manipulative routines\", ''AEC research and development report - Volume 7021 ANL (Series)'' - Stanley Cohen, Physics Division, U.S. Atomic Energy Commission, Argonne National Laboratory, 1965.]</ref><ref>[http://www.osti.gov/bridge/servlets/purl/4292135-TkCmlk/4292135.pdf \"Speakeasy - An evolutionary system\", S. Cohen,  ''Proceedings of the ACM SIGPLAN symposium on Very high level languages'' (March 1974)]</ref> while the most recent version has been released in 2006.\n\nSpeakeasy was aimed to make the computational work of the physicists at the Argonne National Laboratory easier.<ref>{{cite journal|url = http://www.sciencedirect.com/science/article/pii/0010465571900087 | doi=10.1016/0010-4655(71)90008-7 | volume=2 | title=The Delphi-speakeasy system. I. Overall description | year=1971 | journal=Computer Physics Communications | pages=1–10 | last1 = Cohen | first1 = Stanley}}</ref> It was initially conceived to work on [[mainframes]] (the only kind of computers at that time), and was subsequently ported to new platforms ([[minicomputers]], [[personal computers]]) as they became available. The porting of the same code on different platforms was made easier by using [[Mortran]] metalanguage macros to face systems dependencies and compilers deficiencies and differences.<ref>\"Using Mortran to translate Fortran programs from one machine to another\"\nSteven C. Pieper, ''Argonne National Laboratory'', 1976</ref> Speakeasy is currently available on several platforms: PCs running [[Windows]], [[macOS]], [[Linux]], departmental computers and workstations running several flavors of Linux, [[AIX]] or [[Solaris (operating system)|Solaris]].\n\nSpeakeasy was also among the first{{Citation needed|date=October 2009}} interactive numerical computing environments, having been implemented in such a way on a [[CDC 3600]] system, and later on [[Time Sharing Option|IBM TSO]] machines as one was in beta-testing at the Argonne National Laboratory at the time.\n\nAlmost since the beginning (as the dynamic linking functionality was made available in the operating systems) Speakeasy features the capability of expanding its operational vocabulary using separated modules, dynamically linked to the core processor as they are needed. For that reason such modules  were called \"linkules\" (LINKable-modULES).<ref>[http://portal.acm.org/citation.cfm?id=810231&dl=GUIDE&coll=GUIDE&CFID=55845887&CFTOKEN=77421645 \"Speakeasy linkules - plug compatible software\" ''ACM - Proceedings of the 1977 annual conference'']</ref> They are functions with a generalized interface, which can be written in  [[FORTRAN]] or in [[C (programming language)|C]].{{citation needed|date=August 2012}}\nThe independence of each of the new modules from the others and from the main processor is of great help in improving the system, especially it was in the old days.\n\nThis easy way of expanding the functionalities of the main processor was often exploited by the users to develop their own specialized packages. Besides the programs, functions and subroutines the user can write in the Speakeasy's own interpreted language, linkules add functionalities carried out with the typical performances of compiled programs.\n\nAmong the packages developed by the users, one of the most important is \"Modeleasy\", originally developed as \"FEDeasy\"<ref>\"Econometric models via SPEAKEASY/FEDEASY\", James M. Condie, John W. Davison, 1975</ref> in the early 1970s at the research department of the [[Federal Reserve Board]] of Governors in Washington D.C..\nModeleasy implements special objects and functions for large econometric models estimation and simulation.\nIts evolution led eventually to its distribution as an independent product.\n\n== Syntax  ==\n\nThe symbol ''':_''' (colon+underscore) is both the Speakeasy logo and the prompt of the interactive session.\n\nThe dollar sign is used for delimiting comments; the ampersand is used to continue a statement on the following physical line, in which case the prompt becomes ''':&''' (colon+ampersand); a semicolon can separate statements written on the same physical line.\n $ suppose you have a very long statement, \n $ you can write it on multiple physical lines using \"&\" \n $ at the end of the line to be continued:\n \n :_ the_return_value = this_is_a_function_with_many_arguments(argument_1, argument_2, &\n :&                             argument_3, argument_4, argument_5, argument_6)\n \n $ on the other hand, you can collect several short statements \n $ on a single physical line using \";\"\n :_ a=1; b=2; c=3; d=4 \nAs its own name tells, Speakeasy was aimed to expose a syntax as friendly as possible to the user, and as close as possible to the spoken language. The best example of that is given by the set of commands for reading/writing data from/to the permanent storage. E.g. (the languages keywords are in upper case to clarify the point):\n :_ GET my_data FROM LIBRARY my_project\n :_ KEEP my_data AS a_new_name_for_mydata IN LIBRARY other_project \nVariables (i.e. Speakeasy objects) are given a name up to 255 character long, when LONGNAME option is ON, up to 8 characters otherwise (for backward compatibility). They are dynamically typed, depending on the value assigned to them.\n :_ a=1\n :_ whatis a\n A is a REAL SCALAR.\n :_ a=\"now a character array\"\n :_ whatis a\n A is a 21 element CHARACTER ARRAY.\nArguments of functions are usually not required to be surrounded by parenthesis or separated by commas, provided that the context remains clear and unambiguous. For example:\n :_ sin(grid(-pi,pi,pi/32))    $ fully specified syntax\ncan be written :\n :_ sin grid(-pi,pi,pi/32)     $ the argument of function sin is not surrounded by parenthesis\nor even \n :_ sin grid(-pi pi pi/32)     $ the arguments of function grid can be separated by spaces\nMany other syntax simplifications are possible; for example, to define an object named 'a' valued to a ten-elements array of zeroes, one can write any of the following statements:\n :_ a=array(10:0,0,0,0,0,0,0,0,0,0)\n :_ a=0,0,0,0,0,0,0,0,0,0\n :_ a=0 0 0 0 0 0 0 0 0 0\n :_ a=ints(10)*0\n :_ a=10:\nSpeakeasy is a '''''vector-oriented''''' language: giving a structured argument to a function of a scalar, the result is usually an object with the same structure of the argument, in which each element is the result of the function applied to the corresponding element of the argument. In the example given above, the result of function '''sin''' applied to the array (let us call it '''x''') generated by the function '''grid''' is the array '''answer''' whose element '''answer'''(i) equals '''sin'''('''x'''(i)) for each i from 1 to '''noels'''(x) (the number of elements of '''x'''). In other words, the statement\n :_ a=sin(grid(-pi pi pi/32))\nis equivalent to the following fragment of program: \n x=grid(-pi pi pi/32) $ generates an array of real numbers from -pi to pi, stepping by pi/32\n for i = 1,noels(x)   $ loops on the elements of x\n   a(i) = sin(x(i))   $ evaluates the i-th element of a\n next i               $ increment the loop index\nThe  '''''vector-oriented''''' statements avoid writing programs for such loops and are much faster than them.\n\n==Work area and objects==\nBy the very first statement of the session, the user can define the size of the \"named storage\" (or \"work area\", or \"allocator\"), which is allocated once and for all at the beginning of the session. Within this fixed-size work area, the Speakeasy processor dynamically creates and destroys the work objects as needed. A user-tunable <ref>The user can decide how often the garbage collections occur, in terms of number of objects created between two of them. This feature (SLOSH command) is actually aimed to linkules debugging.</ref> garbage collection mechanism is provided to maximize the size of the free block in the work area, packing the defined objects in the low end or in the high end of the allocator. At any time, the user can ask about used or remaining space in the work area.\n :_ SIZE 100M $ very first statement: the work area will be 100MB\n <nowiki>:</nowiki>_ SIZE      $ returns the size of the work area in the current session\n <nowiki>:</nowiki>_ SPACELEFT $ returns the amount of data storage space currently unused\n <nowiki>:</nowiki>_ SPACENOW  $ returns the amount of data storage space currently used\n <nowiki>:</nowiki>_ SPACEPEAK $ returns the maximum amount of data storage space used in the current session\n\n==Raw object orientation==\nWithin reasonable conformity and compatibility constraints, the Speakeasy objects can be operated on using the same algebraic syntax.\n\nFrom this point of view, and considering the dynamic and structured nature of the data held in the \"named storage\", it is possible to say that Speakeasy since the beginning implemented  a very raw form of operator overloading, and a pragmatic approach to some features of what was later called \"[[Object Oriented Programming]]\", although it did not evolve further in that direction.\n<center>\n{| cellpadding=\"0\" cellspacing=\"0\"\n| colspan=\"2\" |\n $ The following example shows how a Matrix-family object and an Array-family object\n $ with the same structure and values are operated on differently although using the \n $ same \"*\" and \"/\" operator: in the first case using the matrix algebra and in the \n $ second case operating on an element-by-element basis.\n|-\n|\n :_ a='''matrix'''(2,2:1,2,3,4) ; a\n   A (A 2 by 2 '''Matrix''')\n   1  2\n   3  4\n :_ a'''*'''a\n   A*A (A 2 by 2 '''Matrix''')\n   7   10\n   15  22\n :_ a'''/'''a\n   A/A (A 2 by 2 '''Matrix''')\n   1  0\n   0  1\n|\n :_ aa='''array'''(2,2:1,2,3,4) ; aa\n   AA (A 2 by 2 '''Array''')\n   1  2\n   3  4\n :_ aa'''*'''aa\n   AA*AA (A 2 by 2 '''Array''')\n   1   4\n   9   16\n :_ aa'''/'''aa\n   AA/AA (A 2 by 2 '''Array''')\n   1  1\n   1  1\n|}\n</center>\n\n== The object families ==\nSpeakeasy provides a bunch of predefined \"families\" of data objects: scalars, arrays (up to 15 dimensions), matrices, sets, time series.\n\nThe elemental data can be of kind real (8-bytes), complex (2x8-bytes), character-literal or name-literal ( matrices elements can be real or complex, time series values can only be real ).\n\n=== Missing values ===\n\nFor [[time series]] processing, five types of [[missing values]] are provided. They are denoted by N.A. (not available), N.C. (not computable), N.D. (not defined), along with  N.B. and N.E. the meaning of which is not predetermined and is left available for the linkules developer. They are internally represented by specific (and very small) numeric values, acting as codes.\n\nAll the time series operations take care of the presence of missing values, propagating them appropriately in the results.\n\nDepending on a specific setting, missing values can be represented by the above notation, by a question mark symbol, or a blank (useful in tables). When used in input the question mark is interpreted as an N.A. missing value.\n :_ b=timeseries(1,2,3,4 : 2010 1 4)\n :_ b\n   B (A Time Series with 4 Components)\n   1  2  3  4\n :_ b(2010 3) = ? \n :_ showmval qmark\n :_ b\n   B (A Time Series with 4 Components)\n   1  2  ?  4\n :_ 1/b\n   1/B (A Time Series with 4 Components)\n   1    .5   ?    .25\n :_ showmval explain\n :_ b\n   B (A Time Series with 4 Components)\n   1     2     N.A.  4\n :_ 1/b\n   1/B (A Time Series with 4 Components)\n   1     .5    N.C.  .25\n\nIn numerical objects other than time series, the concept of \"missing values\" is meaningless, and the numerical operations on them use the actual numeric values regardless they correspond to \"missing values codes\" or not (although \"missing values codes\" can be input and shown as such).\n  :_ 1+?\n   1+? =  1.00\n  :_ 1/?\n   1/? =  5.3033E36\n  :_ 1*?\n   1*? = ?\n\nNote that, in other contexts, a question mark may have a different meaning: for example, when used as the first (and possibly only) character of a command line, it means the request to show more pieces of a long error message (which ends with a \"+\" symbol).\n :_ a=array(10000,10000:)\n ARRAY(10000,10000:) In line \"A=ARRAY(10000,10000:)\"  Too much data.+\n :_ ?\n Allocator size must be at least     859387 kilobytes.+\n :_ ?\n Use FREE to remove no longer needed data\n or\n use CHECKPOINT to save allocator for later restart.+\n :_ ?\n Use NAMES to see presently defined names.\n Use SIZE & RESTORE to restart with a larger allocator.\n :_ ?\n NO MORE INFORMATION AVAILABLE.\n\n=== Logical values ===\nSome support is provided for logical values, relational operators (the [[Fortran]] syntax can be used) and logical expressions.\n\nLogical values are stored actually as numeric values: with 0 meaning false and non-zero (1 on output) meaning true.\n :_ a = 1 2 3 4 5\n :_ b = 1 3 2 5 4\n :_ a>b\n   A>B (A 5 Component Array)\n   0  0  1  0  1\n :_ a<=b\n   A<=B (A 5 Component Array)\n   1  1  0  1  0\n :_ a.eq.b\n   A.EQ.B (A 5 Component Array)\n   1  0  0  0  0\n :_ logical(2) $ this changes the way logical values are shown\n :_ a>b; a<=b; a.eq.b\n   A>B (A 5 Component Array)\n  F F T F T\n   A<=B (A 5 Component Array)\n  T T F T F\n   A.EQ.B (A 5 Component Array)\n  T F F F F\n\n== Programming ==\nSpecial objects such as \"PROGRAM\", \"SUBROUTINE\" and \"FUNCTION\" objects (collectively referred to as ''procedures'') can be defined for operations automation. Another way for running several instructions with a single command is to store them into a use-file and make the processor read them by mean of the USE command.\n\n=== Use-files ===\n\"USEing\" a use-file is the simplest way for performing several instruction with minimal typed input. (This operation roughly corresponds to what \"source-ing\" a file is in other scripting languages.)\n\nA use-file is an alternate input source to the standard console and can contain all the commands a user can input by the keyboard (hence no multi-line flow control construct is allowed). The processor reads and executes use-files one line at a time.\n\nUse-file execution can be concatenated but not nested, i.e. the control does not return to the caller at the completion of the called use-file.\n\n=== Procedures ===\nFull programming capability is achieved using \"procedures\". They are actually Speakeasy objects, which must be defined in the work area to be executed. An option is available in order to make the procedures being automatically retrieved and loaded from the external storage as they are needed.\n\nProcedures can contain any of the execution flow control constructs available in the Speakeasy programming language.\n\n==== Programs ====\nA program can be run simply invoking its name or using it as the argument of the command EXECUTE. In the latter case, a further argument can identify a label from which the execution will begin.\nSpeakeasy programs differs from the other procedures for being executed at the same scoping \"level\" they are referenced to, hence they have full visibility of all the objects defined at that level, and all the objects created during their execution will be left there for subsequent uses. For that reason no argument list is needed.\n\n==== Subroutines and functions ====\nSubroutines and Functions are executed at a new scoping level, which is removed when they finish. The communication with the calling scoping level is carried out through the argument list (in both directions). This implements data hiding, i.e. objects created within a Subroutine or a Function are not visible to other Subroutine and Functions but through argument lists.\n\nA global level is available for storing object which must be visible from within any procedure, e.g. the procedures themselves.\n\nThe Functions differ from the Subroutines because they also return a functional value; reference to them can be part of more complex statement and are replaced by the returned functional value when evaluating the statement.\n\nIn some extent, Speakeasy Subroutines and Functions are very similar to the Fortran procedures of the same name.\n\n==== Flow control ====\nAn IF-THEN-ELSE construct is available for conditional execution and two forms of FOR-NEXT construct are provided for looping.\n<center>\n{| cellpadding=\"0\" border=\"0\" cellspacing=20\n|\n IF (''logical-expression'') THEN\n    ''true-block''\n [ELSE\n    ''false-block'']\n END IF\n|\n FOR ''index'' = ''min'', ''max'' [, ''step'']\n    ''loop-block''\n NEXT ''index''\n|\n FOR ''value'' IN ''set-of-values''\n    ''loop-block''\n NEXT ''value''\n|}\n</center>\n\nA \"GO TO ''label''\" statement is provided for jumping, while a Fortran-like computed GO TO statement can be used fort multiple branching.\n<center>\n{| cellpadding=\"0\" border=\"0\" cellspacing=20\n|\n ...\n IF (''logical-expression'') GO TO ''label''\n ...\n ''label'':\n ...\n|\n $ In the following statement \n $ ''selector'' must be >= 1 and <= N\n \n GO TO ''label1'', ''label2'', ..., ''labelN'' : ''selector'' \n ...\n ''label1'':\n ...\n ''label2'':\n ...\n ...\n ''labelN'':\n ...\n|}\n</center>\n\nAn ON ERROR mechanism, with several options, provides a means for error handling.\n\n== Linkule writing ==\nLinkules are functions usually written in Fortran (or, unsupportedly, in C). With the aid of [[Mortran]] or C macros and an API library, they can interface the Speakeasy workarea for retrieving, defining, manipulating any Speakeasy object.\n\nMost of the Speakeasy operational vocabulary is implemented via linkules. They can be statically linked to the core engine, or dynamically loaded as they are needed, provided they are properly compiled as shared objects (unix) or dll (windows).\n\n==Notes==\n{{Reflist}}\n\n==External links==\n* [https://web.archive.org/web/20180804160022/http://www.speakeasy.com Official website] (via [[Wayback Machine]])\n* {{YouTube|user=gSpeakeasy|title=The Speakasy Computing Corporation}}\n* [http://www.emcc.com The Econometric Modeling & Computing Corporation web site.]\n* [http://sci.tech-archive.net/Archive/sci.stat.math/2006-11/msg00064.html An interesting conversation with Stan Cohen.]\n\n{{Numerical analysis software}}\n\n[[Category:Data analysis software]]\n[[Category:Mathematical software]]\n[[Category:Physics software]]\n[[Category:Proprietary cross-platform software]]\n[[Category:Numerical analysis software for Linux]]\n[[Category:Numerical analysis software for MacOS]]\n[[Category:Numerical analysis software for Windows]]\n[[Category:Computer algebra system software for Windows]]\n[[Category:Computer algebra system software for MacOS]]\n[[Category:Computer algebra system software for Linux]]\n[[Category:Array programming languages]]\n[[Category:Numerical programming languages]]\n[[Category:Numerical linear algebra]]\n[[Category:Statistical programming languages]]\n[[Category:Simulation programming languages]]\n[[Category:Programming languages created in 1964]]"
    },
    {
      "title": "SPIKE algorithm",
      "url": "https://en.wikipedia.org/wiki/SPIKE_algorithm",
      "text": "The '''SPIKE algorithm''' is a hybrid [[Parallel computing|parallel]] solver for [[Band matrix|banded]] [[System of linear equations|linear systems]] developed by Eric Polizzi and [[Ahmed Sameh]] \n{{ref|Spike_Polizzi1}} {{note|Spike_Polizzi2}}{{ref|PSPIKE}}\n\n==Overview==\nThe SPIKE algorithm deals with a linear system {{math|'''<var>AX</var>''' {{=}} '''<var>F</var>'''}}, where {{math|'''<var>A</var>'''}} is a banded <math>n\\times n</math> matrix of [[matrix bandwidth|bandwidth]] much less than <math>n</math>, and {{math|'''<var>F</var>'''}} is an <math>n\\times s</math> matrix containing <math>s</math> right-hand sides. It is divided into a preprocessing stage and a postprocessing stage.\n\n===Preprocessing stage===\nIn the preprocessing stage, the linear system {{math|'''<var>AX</var>''' {{=}} '''<var>F</var>'''}} is partitioned into a [[Block matrix#Block tridiagonal matrices|block tridiagonal]] form\n:<math>\n\\begin{bmatrix}\n\\boldsymbol{A}_1 & \\boldsymbol{B}_1\\\\\n\\boldsymbol{C}_2 & \\boldsymbol{A}_2 & \\boldsymbol{B}_2\\\\\n& \\ddots & \\ddots & \\ddots\\\\\n& & \\boldsymbol{C}_{p-1} & \\boldsymbol{A}_{p-1} & \\boldsymbol{B}_{p-1}\\\\\n& & & \\boldsymbol{C}_p & \\boldsymbol{A}_p\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boldsymbol{X}_1\\\\\n\\boldsymbol{X}_2\\\\\n\\vdots\\\\\n\\boldsymbol{X}_{p-1}\\\\\n\\boldsymbol{X}_p\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\boldsymbol{F}_1\\\\\n\\boldsymbol{F}_2\\\\\n\\vdots\\\\\n\\boldsymbol{F}_{p-1}\\\\\n\\boldsymbol{F}_p\n\\end{bmatrix}.\n</math>\n\nAssume, for the time being, that the diagonal blocks {{math|'''<var>A</var>'''<sub><var>j</var></sub>}} ({{math|<var>j</var> {{=}} 1,&hellip;,<var>p</var>}} with {{math|<var>p</var> &ge; 2}}) are [[Invertible matrix|nonsingular]]. Define a [[Block matrix#Block diagonal matrices|block diagonal]] matrix\n:{{math|'''<var>D</var>''' {{=}} diag('''<var>A</var>'''<sub>1</sub>,…,'''<var>A</var>'''<sub><var>p</var></sub>)}},\nthen {{math|'''<var>D</var>'''}} is also nonsingular. Left-multiplying {{math|'''<var>D</var>'''<sup>−1</sup>}} to both sides of the system gives\n::<math>\n\\begin{bmatrix}\n\\boldsymbol{I} & \\boldsymbol{V}_1\\\\\n\\boldsymbol{W}_2 & \\boldsymbol{I} & \\boldsymbol{V}_2\\\\\n& \\ddots & \\ddots & \\ddots\\\\\n& & \\boldsymbol{W}_{p-1} & \\boldsymbol{I} & \\boldsymbol{V}_{p-1}\\\\\n& & & \\boldsymbol{W}_p & \\boldsymbol{I}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boldsymbol{X}_1\\\\\n\\boldsymbol{X}_2\\\\\n\\vdots\\\\\n\\boldsymbol{X}_{p-1}\\\\\n\\boldsymbol{X}_p\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\boldsymbol{G}_1\\\\\n\\boldsymbol{G}_2\\\\\n\\vdots\\\\\n\\boldsymbol{G}_{p-1}\\\\\n\\boldsymbol{G}_p\n\\end{bmatrix},\n</math>\n\nwhich is to be solved in the postprocessing stage. Left-multiplication by {{math|'''<var>D</var>'''<sup>−1</sup>}} is equivalent to solving <math>p</math> systems of the form\n:{{math|'''<var>A</var>'''<sub><var>j</var></sub>['''<var>V</var>'''<sub><var>j</var></sub> '''<var>W</var>'''<sub><var>j</var></sub> '''<var>G</var>'''<sub><var>j</var></sub>] {{=}} ['''<var>B</var>'''<sub><var>j</var></sub> '''<var>C</var>'''<sub><var>j</var></sub> '''<var>F</var>'''<sub><var>j</var></sub>]}}\n(omitting {{math|'''<var>W</var>'''<sub>1</sub>}} and {{math|'''<var>C</var>'''<sub>1</sub>}} for <math>j=1</math>, and {{math|'''<var>V</var>'''<sub><var>p</var></sub>}} and {{math|'''<var>B</var>'''<sub><var>p</var></sub>}} for <math>j=p</math>), which can be carried out in parallel.\n\nDue to the banded nature of {{math|'''<var>A</var>'''}}, only a few leftmost columns of each {{math|'''<var>V</var>'''<sub><var>j</var></sub>}} and a few rightmost columns of each {{math|'''<var>W</var>'''<sub><var>j</var></sub>}} can be nonzero. These columns are called the ''spikes''.\n\n===Postprocessing stage===\n[[Without loss of generality]], assume that each spike contains exactly <math>m</math> columns (<math>m</math> is much less than <math>n</math>) (pad the spike with columns of zeroes if necessary). Partition the spikes in all {{math|'''<var>V</var>'''<sub><var>j</var></sub>}} and {{math|'''<var>W</var>'''<sub><var>j</var></sub>}} into\n\n:<math>\n\\begin{bmatrix}\n\\boldsymbol{V}_j^{(t)}\\\\\n\\boldsymbol{V}_j'\\\\\n\\boldsymbol{V}_j^{(b)}\n\\end{bmatrix}\n</math> and <math>\n\\begin{bmatrix}\n\\boldsymbol{W}_j^{(t)}\\\\\n\\boldsymbol{W}_j'\\\\\n\\boldsymbol{W}_j^{(b)}\\\\\n\\end{bmatrix}\n</math>\n\nwhere {{math|{{SubSup|'''<var>V</var>'''|<var>j</var>|(''t'')}}}}, {{math|{{SubSup|'''<var>V</var>'''|<var>j</var>|(''b'')}}}}, {{math|{{SubSup|'''<var>W</var>'''|<var>j</var>|(''t'')}}}} and {{math|{{SubSup|'''<var>W</var>'''|<var>j</var>|(''b'')}}}} are of dimensions <math>m\\times m</math>. Partition similarly all {{math|'''<var>X</var>'''<sub><var>j</var></sub>}} and {{math|'''<var>G</var>'''<sub><var>j</var></sub>}} into\n\n:<math>\n\\begin{bmatrix}\n\\boldsymbol{X}_j^{(t)}\\\\\n\\boldsymbol{X}_j'\\\\\n\\boldsymbol{X}_j^{(b)}\n\\end{bmatrix}\n</math> and <math>\n\\begin{bmatrix}\n\\boldsymbol{G}_j^{(t)}\\\\\n\\boldsymbol{G}_j'\\\\\n\\boldsymbol{G}_j^{(b)}\\\\\n\\end{bmatrix}.\n</math>\n\nNotice that the system produced by the preprocessing stage can be reduced to a block [[Pentadiagonal matrix|pentadiagonal]] system of much smaller size (recall that <math>m</math> is much less than <math>n</math>)\n\n:<math>\n\\begin{bmatrix}\n\\boldsymbol{I}_m & \\boldsymbol{0} & \\boldsymbol{V}_1^{(t)}\\\\\n\\boldsymbol{0} & \\boldsymbol{I}_m & \\boldsymbol{V}_1^{(b)} & \\boldsymbol{0}\\\\\n\\boldsymbol{0} & \\boldsymbol{W}_2^{(t)} & \\boldsymbol{I}_m & \\boldsymbol{0} & \\boldsymbol{V}_2^{(t)}\\\\\n& \\boldsymbol{W}_2^{(b)} & \\boldsymbol{0} & \\boldsymbol{I}_m & \\boldsymbol{V}_2^{(b)} & \\boldsymbol{0} \\\\\n& & \\ddots & \\ddots & \\ddots & \\ddots & \\ddots\\\\\n& & & \\boldsymbol{0} & \\boldsymbol{W}_{p-1}^{(t)} & \\boldsymbol{I}_m & \\boldsymbol{0} & \\boldsymbol{V}_{p-1}^{(t)}\\\\\n& & & & \\boldsymbol{W}_{p-1}^{(b)} & \\boldsymbol{0} & \\boldsymbol{I}_m & \\boldsymbol{V}_{p-1}^{(b)} & \\boldsymbol{0}\\\\\n& & & & & \\boldsymbol{0} & \\boldsymbol{W}_p^{(t)} & \\boldsymbol{I}_m & \\boldsymbol{0}\\\\\n& & & & & & \\boldsymbol{W}_p^{(b)} & \\boldsymbol{0} & \\boldsymbol{I}_m\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boldsymbol{X}_1^{(t)}\\\\\n\\boldsymbol{X}_1^{(b)}\\\\\n\\boldsymbol{X}_2^{(t)}\\\\\n\\boldsymbol{X}_2^{(b)}\\\\\n\\vdots\\\\\n\\boldsymbol{X}_{p-1}^{(t)}\\\\\n\\boldsymbol{X}_{p-1}^{(b)}\\\\\n\\boldsymbol{X}_p^{(t)}\\\\\n\\boldsymbol{X}_p^{(b)}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\boldsymbol{G}_1^{(t)}\\\\\n\\boldsymbol{G}_1^{(b)}\\\\\n\\boldsymbol{G}_2^{(t)}\\\\\n\\boldsymbol{G}_2^{(b)}\\\\\n\\vdots\\\\\n\\boldsymbol{G}_{p-1}^{(t)}\\\\\n\\boldsymbol{G}_{p-1}^{(b)}\\\\\n\\boldsymbol{G}_p^{(t)}\\\\\n\\boldsymbol{G}_p^{(b)}\n\\end{bmatrix}\\text{,}\n</math>\n\nwhich we call the ''reduced system'' and denote by {{math|'''<var>S&#x303;X&#x303;</var>''' {{=}} '''<var>G&#x303;</var>'''}}.\n\nOnce all {{math|{{SubSup|'''<var>X</var>'''|<var>j</var>|(''t'')}}}} and {{math|{{SubSup|'''<var>X</var>'''|<var>j</var>|(''b'')}}}} are found, all {{math|'''<var>X</var>'''′<sub><var>j</var></sub>}} can be recovered with perfect parallelism via\n\n:<math>\n\\begin{cases}\n\\boldsymbol{X}_1'=\\boldsymbol{G}_1'-\\boldsymbol{V}_1'\\boldsymbol{X}_2^{(t)}\\text{,}\\\\\n\\boldsymbol{X}_j'=\\boldsymbol{G}_j'-\\boldsymbol{V}_j'\\boldsymbol{X}_{j+1}^{(t)}-\\boldsymbol{W}_j'\\boldsymbol{X}_{j-1}^{(b)}\\text{,} & j=2,\\ldots,p-1\\text{,}\\\\\n\\boldsymbol{X}_p'=\\boldsymbol{G}_p'-\\boldsymbol{W}_p\\boldsymbol{X}_{p-1}^{(b)}\\text{.}\n\\end{cases}\n</math>\n\n==SPIKE as a polyalgorithmic banded linear system solver==\nDespite being logically divided into two stages, computationally, the SPIKE algorithm comprises three stages:\n# [[Matrix decomposition|factorizing]] the diagonal blocks,\n# computing the spikes,\n# solving the reduced system.\nEach of these stages can be accomplished in several ways, allowing a multitude of variants. Two notable variants are the ''recursive SPIKE'' algorithm for non-[[Diagonally dominant matrix|diagonally-dominant]] cases and the ''truncated SPIKE'' algorithm for diagonally-dominant cases. Depending on the variant, a system can be solved either exactly or approximately. In the latter case, SPIKE is used as a preconditioner for iterative schemes like [[Iterative method|Krylov subspace method]]s and [[iterative refinement]].\n\n===Recursive SPIKE===\n\n====Preprocessing stage====\nThe first step of the preprocessing stage is to factorize the diagonal blocks {{math|'''<var>A</var>'''<sub><var>j</var></sub>}}. For numerical stability, one can use [[LAPACK]]'s <code>XGBTRF</code> routines to [[LU decomposition|LU factorize]] them with partial pivoting. Alternatively, one can also factorize them without partial pivoting but with a \"diagonal boosting\" strategy. The latter method tackles the issue of singular diagonal blocks.\n\nIn concrete terms, the diagonal boosting strategy is as follows. Let {{math|0<sub><var>&epsilon;</var></sub>}} denote a configurable \"machine zero\". In each step of LU factorization, we require that the pivot satisfy the condition\n\n:{{math|&#x7c;pivot&#x7c; &gt; 0<sub><var>&epsilon;</var></sub>&#x2016;'''<var>A</var>'''&#x2016;<sub>1</sub>}}.\n\nIf the pivot does not satisfy the condition, it is then boosted by\n\n:<math>\n\\mathrm{pivot}=\n\\begin{cases}\n\\mathrm{pivot}+\\epsilon\\lVert\\boldsymbol{A}_j\\rVert_1 & \\text{if }\\mathrm{pivot}\\geq 0\\text{,}\\\\\n\\mathrm{pivot}-\\epsilon\\lVert\\boldsymbol{A}_j\\rVert_1 & \\text{if }\\mathrm{pivot}<0\n\\end{cases}\n</math>\n\nwhere {{math|<var>&epsilon;</var>}} is a positive parameter depending on the machine's [[Machine epsilon|unit roundoff]], and the factorization continues with the boosted pivot. This can be achieved by modified versions of [[ScaLAPACK]]'s <code>XDBTRF</code> routines. After the diagonal blocks are factorized, the spikes are computed and passed on to the postprocessing stage.\n\n====Postprocessing stage====\n\n=====The two-partition case=====\nIn the two-partition case, i.e., when {{math|<var>p</var> {{=}} 2}}, the reduced system {{math|'''<var>S&#x303;X&#x303;</var>''' {{=}} '''<var>G&#x303;</var>'''}} has the form\n\n:<math>\n\\begin{bmatrix}\n\\boldsymbol{I}_m & \\boldsymbol{0} & \\boldsymbol{V}_1^{(t)}\\\\\n\\boldsymbol{0} & \\boldsymbol{I}_m & \\boldsymbol{V}_1^{(b)} & \\boldsymbol{0}\\\\\n\\boldsymbol{0} & \\boldsymbol{W}_2^{(t)} & \\boldsymbol{I}_m & \\boldsymbol{0}\\\\\n& \\boldsymbol{W}_2^{(b)} & \\boldsymbol{0} & \\boldsymbol{I}_m\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boldsymbol{X}_1^{(t)}\\\\\n\\boldsymbol{X}_1^{(b)}\\\\\n\\boldsymbol{X}_2^{(t)}\\\\\n\\boldsymbol{X}_2^{(b)}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\boldsymbol{G}_1^{(t)}\\\\\n\\boldsymbol{G}_1^{(b)}\\\\\n\\boldsymbol{G}_2^{(t)}\\\\\n\\boldsymbol{G}_2^{(b)}\n\\end{bmatrix}\\text{.}\n</math>\n\nAn even smaller system can be extracted from the center:\n\n:<math>\n\\begin{bmatrix}\n\\boldsymbol{I}_m & \\boldsymbol{V}_1^{(b)}\\\\\n\\boldsymbol{W}_2^{(t)} & \\boldsymbol{I}_m\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boldsymbol{X}_1^{(b)}\\\\\n\\boldsymbol{X}_2^{(t)}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\boldsymbol{G}_1^{(b)}\\\\\n\\boldsymbol{G}_2^{(t)}\n\\end{bmatrix}\\text{,}\n</math>\n\nwhich can be solved using the [[Block LU decomposition|block LU factorization]]\n\n:<math>\n\\begin{bmatrix}\n\\boldsymbol{I}_m & \\boldsymbol{V}_1^{(b)}\\\\\n\\boldsymbol{W}_2^{(t)} & \\boldsymbol{I}_m\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\boldsymbol{I}_m\\\\\n\\boldsymbol{W}_2^{(t)} & \\boldsymbol{I}_m\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boldsymbol{I}_m & \\boldsymbol{V}_1^{(b)}\\\\\n& \\boldsymbol{I}_m-\\boldsymbol{W}_2^{(t)}\\boldsymbol{V}_1^{(b)}\n\\end{bmatrix}\\text{.}\n</math>\n\nOnce {{math|{{SubSup|'''<var>X</var>'''|1|(''b'')}}}} and {{math|{{SubSup|'''<var>X</var>'''|2|(''t'')}}}} are found, {{math|{{SubSup|'''<var>X</var>'''|1|(''t'')}}}} and {{math|{{SubSup|'''<var>X</var>'''|2|(''b'')}}}} can be computed via\n\n:{{math|{{SubSup|'''<var>X</var>'''|1|(''t'')}} {{=}} {{SubSup|'''<var>G</var>'''|1|(''t'')}} &minus; {{SubSup|'''<var>V</var>'''|1|(''t'')}}{{SubSup|'''<var>X</var>'''|2|(''t'')}}}},\n:{{math|{{SubSup|'''<var>X</var>'''|2|(''b'')}} {{=}} {{SubSup|'''<var>G</var>'''|2|(''b'')}} &minus; {{SubSup|'''<var>W</var>'''|2|(''b'')}}{{SubSup|'''<var>X</var>'''|1|(''b'')}}}}.\n\n=====The multiple-partition case=====\nAssume that {{math|<var>p</var>}} is a power of two, i.e., {{math|<var>p</var> {{=}} 2<sup><var>d</var></sup>}}. Consider a block diagonal matrix\n\n:{{math|'''<var>D&#x303;</var>'''<sub>1</sub> {{=}} diag({{SubSup|'''<var>D&#x303;</var>'''|1|[1]}},&hellip;,{{SubSup|'''<var>D&#x303;</var>'''|<var>p</var>/2|[1]}})}}\n\nwhere\n\n:<math>\n\\boldsymbol{\\tilde{D}}_k^{[1]}=\n\\begin{bmatrix}\n\\boldsymbol{I}_m & \\boldsymbol{0} & \\boldsymbol{V}_{2k-1}^{(t)}\\\\\n\\boldsymbol{0} & \\boldsymbol{I}_m & \\boldsymbol{V}_{2k-1}^{(b)} & \\boldsymbol{0}\\\\\n\\boldsymbol{0} & \\boldsymbol{W}_{2k}^{(t)} & \\boldsymbol{I}_m & \\boldsymbol{0}\\\\\n& \\boldsymbol{W}_{2k}^{(b)} & \\boldsymbol{0} & \\boldsymbol{I}_m\n\\end{bmatrix}\n</math>\n\nfor {{math|<var>k</var> {{=}} 1,&hellip;,<var>p</var>/2}}. Notice that {{math|'''<var>D&#x303;</var>'''<sub>1</sub>}} essentially consists of diagonal blocks of order {{math|4<var>m</var>}} extracted from {{math|'''<var>S&#x303;</var>'''}}. Now we factorize {{math|'''<var>S&#x303;</var>'''}} as\n\n:{{math|'''<var>S&#x303;</var>''' {{=}} '''<var>D&#x303;</var>'''<sub>1</sub>'''<var>S&#x303;</var>'''<sub>2</sub>}}.\n\nThe new matrix {{math|'''<var>S&#x303;</var>'''<sub>2</sub>}} has the form\n\n:<math>\n\\begin{bmatrix}\n\\boldsymbol{I}_{3m} & \\boldsymbol{0} & \\boldsymbol{V}_1^{[2](t)}\\\\\n\\boldsymbol{0} & \\boldsymbol{I}_m & \\boldsymbol{V}_1^{[2](b)} & \\boldsymbol{0}\\\\\n\\boldsymbol{0} & \\boldsymbol{W}_2^{[2](t)} & \\boldsymbol{I}_m & \\boldsymbol{0} & \\boldsymbol{V}_2^{[2](t)}\\\\\n& \\boldsymbol{W}_2^{[2](b)} & \\boldsymbol{0} & \\boldsymbol{I}_{3m} & \\boldsymbol{V}_2^{[2](b)} & \\boldsymbol{0} \\\\\n& & \\ddots & \\ddots & \\ddots & \\ddots & \\ddots\\\\\n& & & \\boldsymbol{0} & \\boldsymbol{W}_{p/2-1}^{[2](t)} & \\boldsymbol{I}_{3m} & \\boldsymbol{0} & \\boldsymbol{V}_{p/2-1}^{[2](t)}\\\\\n& & & & \\boldsymbol{W}_{p/2-1}^{[2](b)} & \\boldsymbol{0} & \\boldsymbol{I}_m & \\boldsymbol{V}_{p/2-1}^{[2](b)} & \\boldsymbol{0}\\\\\n& & & & & \\boldsymbol{0} & \\boldsymbol{W}_{p/2}^{[2](t)} & \\boldsymbol{I}_m & \\boldsymbol{0}\\\\\n& & & & & & \\boldsymbol{W}_{p/2}^{[2](b)} & \\boldsymbol{0} & \\boldsymbol{I}_{3m}\n\\end{bmatrix}\\text{.}\n</math>\n\nIts structure is very similar to that of {{math|'''<var>S&#x303;</var>'''<sub>2</sub>}}, only differing in the number of spikes and their height (their width stays the same at {{math|<var>m</var>}}). Thus, a similar factorization step can be performed on {{math|'''<var>S&#x303;</var>'''<sub>2</sub>}} to produce\n\n:{{math|'''<var>S&#x303;</var>'''<sub>2</sub> {{=}} '''<var>D&#x303;</var>'''<sub>2</sub>'''<var>S&#x303;</var>'''<sub>3</sub>}}\n\nand\n\n:{{math|'''<var>S&#x303;</var>''' {{=}} '''<var>D&#x303;</var>'''<sub>1</sub>'''<var>D&#x303;</var>'''<sub>2</sub>'''<var>S&#x303;</var>'''<sub>3</sub>}}.\n\nSuch factorization steps can be performed recursively. After {{math|<var>d</var> &minus; 1}} steps, we obtain the factorization\n\n:{{math|'''<var>S&#x303;</var>''' {{=}} '''<var>D&#x303;</var>'''<sub>1</sub>'''&#x22ef;<var>D&#x303;</var>'''<sub><var>d</var>&minus;1</sub>'''<var>S&#x303;</var>'''<sub><var>d</var></sub>}},\n\nwhere {{math|'''<var>S&#x303;</var>'''<sub><var>d</var></sub>}} has only two spikes. The reduced system will then be solved via\n\n:{{math|'''<var>X&#x303;</var>''' {{=}} {{SubSup|'''<var>S&#x303;</var>'''|<var>d</var>|&minus;1}}{{SubSup|'''<var>D&#x303;</var>'''|<var>d</var>&minus;1|&minus;1}}&#x22ef;{{SubSup|'''<var>D&#x303;</var>'''|1|&minus;1}}'''<var>G&#x303;</var>'''}}.\n\nThe block LU factorization technique in the two-partition case can be used to handle the solving steps involving {{math|'''<var>D&#x303;</var>'''<sub>1</sub>}}, …, {{math|'''<var>D&#x303;</var>'''<sub><var>d</var>&minus;1</sub>}} and {{math|'''<var>S&#x303;</var>'''<sub><var>d</var></sub>}} for they essentially solve multiple independent systems of generalized two-partition forms.\n\nGeneralization to cases where {{math|<var>p</var>}} is not a power of two is almost trivial.\n\n===Truncated SPIKE===\nWhen {{math|'''<var>A</var>'''}} is diagonally-dominant, in the reduced system\n\n:<math>\n\\begin{bmatrix}\n\\boldsymbol{I}_m & \\boldsymbol{0} & \\boldsymbol{V}_1^{(t)}\\\\\n\\boldsymbol{0} & \\boldsymbol{I}_m & \\boldsymbol{V}_1^{(b)} & \\boldsymbol{0}\\\\\n\\boldsymbol{0} & \\boldsymbol{W}_2^{(t)} & \\boldsymbol{I}_m & \\boldsymbol{0} & \\boldsymbol{V}_2^{(t)}\\\\\n& \\boldsymbol{W}_2^{(b)} & \\boldsymbol{0} & \\boldsymbol{I}_m & \\boldsymbol{V}_2^{(b)} & \\boldsymbol{0} \\\\\n& & \\ddots & \\ddots & \\ddots & \\ddots & \\ddots\\\\\n& & & \\boldsymbol{0} & \\boldsymbol{W}_{p-1}^{(t)} & \\boldsymbol{I}_m & \\boldsymbol{0} & \\boldsymbol{V}_{p-1}^{(t)}\\\\\n& & & & \\boldsymbol{W}_{p-1}^{(b)} & \\boldsymbol{0} & \\boldsymbol{I}_m & \\boldsymbol{V}_{p-1}^{(b)} & \\boldsymbol{0}\\\\\n& & & & & \\boldsymbol{0} & \\boldsymbol{W}_p^{(t)} & \\boldsymbol{I}_m & \\boldsymbol{0}\\\\\n& & & & & & \\boldsymbol{W}_p^{(b)} & \\boldsymbol{0} & \\boldsymbol{I}_m\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boldsymbol{X}_1^{(t)}\\\\\n\\boldsymbol{X}_1^{(b)}\\\\\n\\boldsymbol{X}_2^{(t)}\\\\\n\\boldsymbol{X}_2^{(b)}\\\\\n\\vdots\\\\\n\\boldsymbol{X}_{p-1}^{(t)}\\\\\n\\boldsymbol{X}_{p-1}^{(b)}\\\\\n\\boldsymbol{X}_p^{(t)}\\\\\n\\boldsymbol{X}_p^{(b)}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\boldsymbol{G}_1^{(t)}\\\\\n\\boldsymbol{G}_1^{(b)}\\\\\n\\boldsymbol{G}_2^{(t)}\\\\\n\\boldsymbol{G}_2^{(b)}\\\\\n\\vdots\\\\\n\\boldsymbol{G}_{p-1}^{(t)}\\\\\n\\boldsymbol{G}_{p-1}^{(b)}\\\\\n\\boldsymbol{G}_p^{(t)}\\\\\n\\boldsymbol{G}_p^{(b)}\n\\end{bmatrix}\\text{,}\n</math>\n\nthe blocks {{math|{{SubSup|'''<var>V</var>'''|<var>j</var>|(''t'')}}}} and {{math|{{SubSup|'''<var>W</var>'''|<var>j</var>|(''b'')}}}} are often negligible. With them omitted, the reduced system becomes block diagonal\n\n:<math>\n\\begin{bmatrix}\n\\boldsymbol{I}_m\\\\\n& \\boldsymbol{I}_m & \\boldsymbol{V}_1^{(b)}\\\\\n& \\boldsymbol{W}_2^{(t)} & \\boldsymbol{I}_m\\\\\n& & & \\boldsymbol{I}_m & \\boldsymbol{V}_2^{(b)}\\\\\n& & & \\ddots & \\ddots & \\ddots\\\\\n& & & & \\boldsymbol{W}_{p-1}^{(t)} & \\boldsymbol{I}_m\\\\\n& & & & & & \\boldsymbol{I}_m & \\boldsymbol{V}_{p-1}^{(b)}\\\\\n& & & & & & \\boldsymbol{W}_p^{(t)} & \\boldsymbol{I}_m\\\\\n& & & & & & & & \\boldsymbol{I}_m\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boldsymbol{X}_1^{(t)}\\\\\n\\boldsymbol{X}_1^{(b)}\\\\\n\\boldsymbol{X}_2^{(t)}\\\\\n\\boldsymbol{X}_2^{(b)}\\\\\n\\vdots\\\\\n\\boldsymbol{X}_{p-1}^{(t)}\\\\\n\\boldsymbol{X}_{p-1}^{(b)}\\\\\n\\boldsymbol{X}_p^{(t)}\\\\\n\\boldsymbol{X}_p^{(b)}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\boldsymbol{G}_1^{(t)}\\\\\n\\boldsymbol{G}_1^{(b)}\\\\\n\\boldsymbol{G}_2^{(t)}\\\\\n\\boldsymbol{G}_2^{(b)}\\\\\n\\vdots\\\\\n\\boldsymbol{G}_{p-1}^{(t)}\\\\\n\\boldsymbol{G}_{p-1}^{(b)}\\\\\n\\boldsymbol{G}_p^{(t)}\\\\\n\\boldsymbol{G}_p^{(b)}\n\\end{bmatrix}\n</math>\n\nand can be easily solved in parallel {{ref|Truncated_Spike}}.\n\nThe truncated SPIKE algorithm can be wrapped inside some outer iterative scheme (e.g., [[Biconjugate gradient stabilized method|BiCGSTAB]] or [[iterative refinement]]) to improve the accuracy of the solution.\n\n==SPIKE for tridiagonal systems==\nThe first SPIKE partitioning and algorithm was presented in {{ref|KuckSameh78}} and was designed as the means to improve the stability properties of a parallel Givens rotations-based solver for tridiagonal systems. A version of the algorithm, termed g-Spike, that is based on serial Givens rotations applied independently on each block was designed for the NVIDIA GPU {{ref|Venetis_gSpike_GPU}}.  A SPIKE-based algorithm for the GPU that is based on a special  block diagonal pivoting strategy is described in {{ref|Chang_tridiagonal_GPU}}.\n\n==SPIKE as a preconditioner==\nThe SPIKE algorithm can also function as a preconditioner for iterative methods for solving linear systems. To solve a linear system {{math|'''<var>Ax</var>''' {{=}} '''<var>b</var>'''}} using a SPIKE-preconditioned iterative solver, one extracts center bands from {{math|'''<var>A</var>'''}} to form a banded preconditioner {{math|'''<var>M</var>'''}} and solves linear systems involving {{math|'''<var>M</var>'''}} in each iteration with the SPIKE algorithm.\n\nIn order for the preconditioner to be effective, row and/or column permutation is usually necessary to move “heavy” elements of {{math|'''<var>A</var>'''}} close to the diagonal so that they are covered by the preconditioner. This can be accomplished by computing the [[Algebraic connectivity#The Fiedler vector|weighted spectral reordering]] of {{math|'''<var>A</var>'''}}.\n\nThe SPIKE algorithm can be generalized by not restricting the preconditioner to be strictly banded. In particular, the diagonal block in each partition can be a general matrix and thus handled by a direct general linear system solver rather than a banded solver. This enhances the preconditioner, and hence allows better chance of convergence and reduces the number of iterations.\n\n==Implementations==\n[[Intel]] offers an implementation of the SPIKE algorithm under the name ''Intel Adaptive Spike-Based Solver'' {{ref|1}}. Tridiagonal solvers have also been developed for the NVIDIA GPU \n{{ref|Venetis_gSpike_GPU}}{{ref|Chang_tridiagonal_GPU}} and the Xeon Phi co-processors. The method in  {{ref|Chang_tridiagonal_GPU}} is the basis for a tridiagonal solver in the cuSPARSE library.<ref>NVIDIA, Accessed October 28, 2014. CUDA Toolkit Documentation v. 6.5: cuSPARSE, http://docs.nvidia.com/cuda/cusparse.</ref> The Givens rotations based solver was also implemented for the \nGPU and the Intel Xeon Phi.<ref>https://www.researchgate.net/publication/282286515_A_general_tridiagonal_solver_for_coprocessors_Adapting_g-Spike_for_the_Intel_Xeon_Phi</ref>\n\n{{reflist}}\n\n==References==\n# {{note|Spike_Polizzi1}}{{Cite journal| last2 = Sameh| first1 = E.| first2 = A. H.| title = A parallel hybrid banded system solver: the SPIKE algorithm| journal = Parallel Computing| volume = 32| issue = 2| pages = 177–194| year = 2006| last1 = Polizzi| doi = 10.1016/j.parco.2005.07.005}}\n# {{note|Spike_Polizzi2}}{{Cite journal| last2 = Sameh| first1 = E.| first2 = A. H.| title = SPIKE: A parallel environment for solving banded linear systems| journal = Computers & Fluids| volume = 36| pages = 113–141| year = 2007| last1 = Polizzi| doi = 10.1016/j.compfluid.2005.07.005}}\n# {{note|Truncated_Spike}}{{Cite journal | first1 = C. C. K. | last2 = Manguoglu | first2 = M. | title = Analysis of the Truncated SPIKE Algorithm| last1 = Mikkelsen | journal = [[SIAM Journal on Matrix Analysis and Applications|SIAM J. Matrix Anal. Appl.]] | volume = 30| issue = 4 | pages = 1500–1519| year = 2008 | doi = 10.1137/080719571| citeseerx = 10.1.1.514.8748}}\n#  {{note|PSPIKE}}{{cite journal | last1 = Manguoglu | first1 = M. | last2 = Sameh | first2 = A. H. | last3 = Schenk | first3 = O. | year = 2009 | title = PSPIKE: A parallel hybrid sparse linear system solver | journal = Lecture Notes in Computer Science | volume = 5704 | issue =  | pages = 797–808 | bibcode = 2009LNCS.5704..797M | doi = 10.1007/978-3-642-03869-3_74 }}\n# {{note|1}}{{cite web|url=http://software.intel.com/en-us/articles/intel-adaptive-spike-based-solver/|title=Intel Adaptive Spike-Based Solver - Intel Software Network|accessdate=2009-03-23}}\n# {{note|KuckSameh78}}{{Cite journal| doi = 10.1145/322047.322054| title = On Stable Parallel Linear System Solvers| year = 1978| last1 = Sameh | first1 = A. H.| last2 = Kuck | first2 = D. J.| journal = Journal of the ACM| volume = 25| pages = 81 }}\n# {{note|Venetis_gSpike_GPU}}{{Cite journal| title = A direct tridiagonal solver based on Givens rotations for GPU architectures| year = 2015| last1 = Venetis|first1=I.E.|last2=Kouris|first2=A.|last3=Sobczyk|first3=A.|last4=Gallopoulos|first4=E.|last5= Sameh | first5 = A. H.| journal = Parallel Computing| volume = 25| pages = 101–116|doi=10.1016/j.parco.2015.03.008 }}\n# {{note|Chang_tridiagonal_GPU}}{{Cite journal| title = A scalable, numerically stable, high-performance tridiagonal solver using GPUs| year = 2012| last1 = Chang|first1=L.-W.|last2=Stratton|first2=J.|last3=Kim|first3=H.|last4=Hwu|first4=W.-M.| journal = Proc. Int’l. Conf. High Performance Computing, Networking Storage and Analysis (SC'12)| pages =  27:1–27:11|location=Los Alamitos, CA, USA|publisher=IEEE Computer Soc. Press|isbn= 978-1-4673-0804-5 }}\n\n==See also ==\n\n# {{cite book |last1=Gallopoulos |first1=E. |last2= Philippe | first2=B.| last3=Sameh |first3= A.H.|date= 2015|title=Parallelism in Matrix Computations |url= https://www.springer.com/in/book/9789401771870 |publisher=Springer |isbn=978-94-017-7188-7}}\n\n{{Numerical linear algebra}}\n\n{{DEFAULTSORT:Spike Algorithm}}\n[[Category:Numerical linear algebra]]"
    }
  ]
}