{
  "pages": [
    {
      "title": "Polynomial-time approximation scheme",
      "url": "https://en.wikipedia.org/wiki/Polynomial-time_approximation_scheme",
      "text": "In [[computer science]], a '''polynomial-time approximation scheme''' ('''PTAS''') is a type of [[approximation algorithm]] for [[optimization problem]]s (most often, [[NP-hard]] optimization problems).\n\nA PTAS is an algorithm which takes an instance of an optimization problem and a parameter ε&nbsp;>&nbsp;0 and, in polynomial time, produces a solution that is within a factor 1 + ε of being optimal (or 1 &minus; ε for maximization problems). For example, for the Euclidean [[traveling salesman problem]], a PTAS would produce a tour with length at most (1&nbsp;+&nbsp;ε)''L'', with ''L'' being the length of the shortest tour.<ref>[[Sanjeev Arora]], Polynomial-time Approximation Schemes for Euclidean TSP and other Geometric Problems, Journal of the ACM 45(5) 753–782, 1998.</ref> There exists also PTAS for the class of all dense constraint satisfaction problems (CSPs).<ref>{{citation\n|last1=Arora|first1=S.\n|last2=Karger|first2=D.\n|last3=Karpinski|first3=M.\n|journal=Journal of Computer and System Sciences\n|volume=58|issue=1|year=1999|pages=193–210\n|title=Polynomial Time Approximation Schemes for Dense Instances of NP-Hard Problems\n|doi=10.1006/jcss.1998.1605\n}}</ref>{{Clarify|date=October 2017}}\n\nThe running time of a PTAS is required to be polynomial in ''n'' for every fixed ε but can be different for different ε. Thus an algorithm running in time ''[[Big O notation|O]]''(''n''<sup>1/ε</sup>) or even ''O''(''n''<sup>exp(1/ε)</sup>) counts as a PTAS.\n\n==Variants==\n===Deterministic===\nA practical problem with PTAS algorithms is that the exponent of the polynomial could increase dramatically as ε shrinks, for example if the runtime is O(''n''<sup>(1/ε)!</sup>). One way of addressing this is to define the '''efficient polynomial-time approximation scheme''' or '''EPTAS''', in which the running time is required to be ''O''(''n''<sup>''c''</sup>) for a constant ''c'' independent of ε. This ensures that an increase in problem size has the same relative effect on runtime regardless of what ε is being used; however, the constant under the [[Big O notation|big-O]] can still depend on ε arbitrarily. Even more restrictive, and useful in practice, is the '''fully polynomial-time approximation scheme''' or '''FPTAS''', which requires the algorithm to be polynomial in both the problem size ''n'' and 1/ε. All problems in FPTAS are [[fixed-parameter tractable]]. Both the [[knapsack problem]] and [[bin packing problem]] admit an FPTAS.<ref>{{Cite book|url=https://www.worldcat.org/oclc/47097680|title=Approximation algorithms|last=Vazirani|first=Vijay|date=|publisher=Springer|year=2001|isbn=3540653678|location=Berlin|pages=74-83|oclc=47097680}}</ref>\n\nAny [[Strongly NP-complete|strongly NP-hard]] optimization problem with a polynomially bounded objective function cannot have an FPTAS unless P=NP.<ref name=\"vvv\"/> However, the converse fails: e.g. if P does not equal NP,  [[List_of_knapsack_problems#Multiple_constraints|knapsack with two constraints]] is not strongly NP-hard, but has no FPTAS even when the optimal objective is polynomially bounded.<ref>{{cite book|authors= H. Kellerer and U. Pferschy and D. Pisinger| title = Knapsack Problems | publisher = Springer | year = 2004}}</ref>\n\nUnless [[P = NP problem|P = NP]], it holds that  FPTAS&nbsp;⊊&nbsp;PTAS&nbsp;⊊&nbsp;[[APX]].<ref name=Jansen>{{citation|first=Thomas|last=Jansen|contribution=Introduction to the Theory of Complexity and Approximation Algorithms|pages=5–28|title=Lectures on Proof Verification and Approximation Algorithms|editor1-first=Ernst W.|editor1-last=Mayr|editor2-first=Hans Jürgen|editor2-last=Prömel|editor3-first=Angelika|editor3-last=Steger|editor3-link=Angelika Steger|publisher=Springer|year=1998|isbn=9783540642015|doi=10.1007/BFb0053011}}. See discussion following Definition 1.30 on [https://books.google.com/books?id=_C8Ly1ya4cgC&pg=PA20 p.&nbsp;20].</ref> Consequently, under this assumption, APX-hard problems do not have PTASs.\n\nAnother deterministic variant of the PTAS is the '''quasi-polynomial-time approximation scheme''' or '''QPTAS'''. A QPTAS has time complexity <math>n^{\\operatorname{polylog}(n)}</math> for each fixed <math>\\varepsilon >0</math>.\n\n===Randomized===\nSome problems which do not have a PTAS may admit a [[randomized algorithm]] with similar properties, a '''polynomial-time randomized approximation scheme''' or '''PRAS'''.  A PRAS is an algorithm which takes an instance of an optimization or counting problem and a parameter ε&nbsp;>&nbsp;0 and, in polynomial time, produces a solution that has a ''high probability'' of being within a factor ε of optimal.  Conventionally, \"high probability\" means probability greater than 3/4, though as with most probabilistic complexity classes the definition is robust to variations in this exact value (the bare minimum requirement is generally greater than 1/2).  Like a PTAS, a PRAS must have running time polynomial in ''n'', but not necessarily in ε; with further restrictions on the running time in ε, one can define an '''efficient polynomial-time randomized approximation scheme''' or '''EPRAS''' similar to the EPTAS, and a '''fully polynomial-time randomized approximation scheme''' or '''FPRAS''' similar to the FPTAS.<ref name=\"vvv\">{{cite book\n  | last = Vazirani\n  | first = Vijay V.\n  | title = Approximation Algorithms\n  | publisher = Springer\n  | year = 2003\n  | pages = 294–295\n  | location = Berlin\n  | isbn = 3-540-65367-8\n}}</ref>\n\n==As a complexity class==\nThe term PTAS may also be used to refer to the class of optimization problems that have a PTAS.  PTAS is a subset of [[APX]], and unless [[P = NP problem|P = NP]], it is a strict subset. <ref name=Jansen></ref>\n\nMembership in PTAS can be shown using a [[PTAS reduction]], [[L-reduction]], or [[Approximation-preserving reduction#A-reduction and P-reduction|P-reduction]], all of which preserve PTAS membership, and these may also be used to demonstrate PTAS-completeness.  On the other hand, showing non-membership in PTAS (namely, the nonexistence of a PTAS), may be done by showing that the problem is APX-hard, after which the existence of a PTAS would show P = NP.  APX-hardness is commonly shown via PTAS reduction or [[Approximation-preserving reduction#AP-reduction|AP-reduction]].\n\n==References==\n<references/>\n\n==External links==\n*Complexity Zoo: [https://complexityzoo.uwaterloo.ca/Complexity_Zoo:P#ptas PTAS], [https://complexityzoo.uwaterloo.ca/Complexity_Zoo:E#eptas EPTAS], [https://complexityzoo.uwaterloo.ca/Complexity_Zoo:F#fptas FPTAS]\n*Pierluigi Crescenzi, Viggo Kann, Magnús Halldórsson, [[Marek Karpinski]], and [[Gerhard J. Woeginger|Gerhard Woeginger]], [http://www.nada.kth.se/~viggo/wwwcompendium/ ''A compendium of NP optimization problems''] – list which NP optimization problems have PTAS.\n\n{{DEFAULTSORT:Polynomial-Time Approximation Scheme}}\n[[Category:Approximation algorithms]]\n[[Category:Complexity classes]]"
    },
    {
      "title": "Property testing",
      "url": "https://en.wikipedia.org/wiki/Property_testing",
      "text": "In [[computer science]], a '''property testing''' algorithm for a [[decision problem]] is an [[algorithm]] whose [[query complexity]] to its input is much smaller than the [[Computational complexity theory#Measuring the size of an instance|instance size]] of the problem.  Typically property testing algorithms are used to decide if some mathematical object (such as a [[Graph (discrete mathematics)|graph]] or a [[boolean function]]) has a \"global\" property, or is \"far\" from having this property, using only a small number of \"local\" queries to the object.\n\nFor example, the following [[promise problem]] admits an algorithm whose query complexity is independent of the instance size (for an arbitrary constant ε > 0):\n\n:\"Given a graph ''G'' on ''n'' vertices, decide if ''G'' is [[Bipartite graph|bipartite]], or ''G'' cannot be made bipartite even after removing an arbitrary subset of at most <math>\\epsilon\\tbinom n2</math> edges of ''G''.\"\n\nProperty testing algorithms are central to the definition of [[probabilistically checkable proof]]s, as a probabilistically checkable proof is essentially a proof that can be verified by a property testing algorithm.\n\n==Definition and variants==\n\nFormally, a '''property testing algorithm''' with query complexity ''q''(''n'') and ''proximity parameter'' ε for a decision problem ''L''  is a [[randomized algorithm]] that, on input ''x'' (an instance of ''L'') makes at most ''q''(|''x''|) queries to ''x'' and behaves as follows:\n* If ''x'' is in ''L'', the algorithm accepts ''x'' with probability at least ⅔.\n* If ''x'' is ε-far from ''L'', the algorithm rejects ''x'' with probability at least ⅔.\n\nHere, \"''x'' is ε-far from ''L''\" means that the Hamming distance between ''x'' and any string in ''L'' is at least ε|''x''|.\n\nA property testing algorithm is said to have ''one-sided error'' if it satisfies the stronger condition that the accepting probability for instances ''x ∈ L'' is 1 instead of ⅔.\n\nA property testing algorithm is said be ''non-adaptive'' if it performs all its queries before it \"observes\" any answers to previous queries. Such an algorithm can be viewed as operating in the following manner.  First the algorithm receives its input.  Before looking at the input, using its internal randomness, the algorithm decides which symbols of the input are to be queried.  Next, the algorithm observes these symbols.  Finally, without making any additional queries (but possibly using its randomness), the algorithm decides whether to accept or reject the input.\n\n==Features and limitations==\n\nThe main efficiency parameter of a property testing algorithm is its query complexity, which is the maximum number of input symbols inspected over all inputs of a given length (and all random choices made by the algorithm).  One is interested in designing algorithms whose query complexity is as small as possible. In many cases the running time of property testing algorithms is [[Time complexity#Sub-linear time|sublinear]] in the instance length.  Typically, the goal is first to make the query complexity as small as possible as a function of the instance size ''n'', and then study the dependency on the proximity parameter ε.\n\nUnlike other complexity-theoretic settings, the asymptotic query complexity of property testing algorithms is affected dramatically by the representation of instances.  For example, when ε = 0.01, the problem of testing bipartiteness of ''dense graphs'' (which are represented by their adjacency matrix) admits an algorithm of constant query complexity. In contrast, sparse graphs on ''n'' vertices (which are represented by their adjacency list) require property testing algorithms of query complexity <math>\\Omega(\\sqrt{n})</math>.\n\nThe query complexity of property testing algorithms grows as the proximity parameter ε becomes smaller for all non-trivial properties.  This dependence on ε is necessary as a change of fewer than ε symbols in the input cannot be detected with constant probability using fewer than O(1/ε) queries.  Many interesting properties of dense graphs can be tested using query complexity that depends only on ε and not on the graph size ''n''.  However, the query complexity can grow enormously fast as a function of ε.  For example, for a long time the best known algorithm for testing if a graph does not [[Triangle-free graph|contain any triangle]] had a query complexity which is a [[Tetration|tower function]] of ''poly''(1/ε), and only in 2010 this has been improved to a tower function of ''log''(1/ε).  One of the reasons for this enormous growth in bounds is that many of the positive results for property testing of graphs are established using the [[Szemerédi regularity lemma]], which also has tower-type bounds in its conclusions.\n\n==References==\n* {{cite techreport | first=Dana | last=Ron | authorlink=Dana Ron | title=Property Testing | year=2000 }}\n* {{cite journal | last=Rubinfeld | first=Ronitt | last2=Shapira | first2=Asaf | title=Sublinear Time Algorithms | journal=SIAM Journal on Discrete Mathematics | volume=25 | number=4 | pages=1562–1588 | year=2011 | doi=10.1137/100791075 | citeseerx=10.1.1.221.1797 }}\n* {{cite journal | title=Combinatorial Property Testing (a survey) | last=Goldreich | first=Oded | authorlink=Oded Goldreich | journal=Randomization Methods in Algorithm Design | volume=43 | pages=45–59 | year=1999 | isbn=0821870874 | url=http://www.wisdom.weizmann.ac.il/~oded/PS/testSU.ps }}\n* {{cite arXiv | last=Fox | first=Jacob | eprint=1006.1300 | title= A new proof of the graph removal lemma |date=2010 }}\n\n[[Category:Approximation algorithms]]\n[[Category:Randomized algorithms]]\n[[Category:Theoretical computer science]]"
    },
    {
      "title": "PTAS reduction",
      "url": "https://en.wikipedia.org/wiki/PTAS_reduction",
      "text": "In [[computational complexity theory]], a '''PTAS reduction''' is an [[approximation-preserving reduction]] that is often used to perform [[Reduction (complexity)|reductions]] between solutions to [[optimization problem]]s. It preserves the property that a problem has a [[polynomial time approximation scheme]] (PTAS) and is used to define [[complete (complexity)|completeness]] for certain classes of optimization problems such as [[APX]]. Notationally, if there is a PTAS reduction from a problem A to a problem B, we write <math>\\text{A} \\leq_{\\text{PTAS}} \\text{B}</math>.\n\nWith ordinary [[polynomial-time many-one reduction]]s, if we can describe a [[reduction (complexity)|reduction]] from a problem A to a problem B, then any polynomial-time solution for B can be composed with that reduction to obtain a polynomial-time solution for the problem A. Similarly, our goal in defining PTAS reductions is so that given a PTAS reduction from an optimization problem A to a problem B, a PTAS for B can be composed with the reduction to obtain a PTAS for the problem A.\n\n==Definition==\nFormally, we define a PTAS reduction from A to B using three polynomial-time computable functions, ''f'', ''g'', and ''&alpha;'', with the following properties:\n\n* ''f'' maps instances of problem A to instances of problem B.\n* ''g'' takes an instance ''x'' of problem A, an approximate solution to the corresponding problem <math>f(x)</math> in B, and an error parameter &epsilon; and produces an approximate solution to ''x''.\n* ''&alpha;'' maps error parameters for solutions to instances of problem A to error parameters for solutions to problem B.\n* If the solution ''y'' to <math>f(x)</math> (an instance of problem B) is at most <math>1 + \\alpha(\\epsilon)</math> times worse than the optimal solution, then the corresponding solution <math>g(x,y,\\epsilon)</math> to ''x'' (an instance of problem A) is at most <math>1 + \\epsilon</math> times worse than the optimal solution.\n\n==Properties==\n\nFrom the definition it is straightforward to show that:\n* <math>\\text{A} \\leq_{\\text{PTAS}} \\text{B}</math> and <math>\\text{B} \\in \\text{PTAS} \\implies \\text{A} \\in \\text{PTAS}</math>\n* <math>\\text{A} \\leq_{\\text{PTAS}} \\text{B}</math> and <math>\\text{A} \\not\\in \\text{PTAS} \\implies \\text{B} \\not\\in \\text{PTAS}</math>\n\n[[L-reduction]]s imply PTAS reductions.  As a result, one may show the existence of a PTAS reduction via a L-reduction instead.<ref name=crescenzi>{{cite journal|last1=Crescenzi|first1=Pierluigi|title=A Short Guide To Approximation Preserving Reductions|journal=Proceedings of the 12th Annual IEEE Conference on Computational Complexity|date=1997|pages=262–|url=http://dl.acm.org/citation.cfm?id=792302|publisher=IEEE Computer Society|location=Washington, D.C.}}</ref>\n\nPTAS reductions are used to define completeness in [[APX]], the class of optimization problems with constant-factor approximation algorithms.\n\n== See also ==\n* [[Approximation-preserving reduction]]\n* [[L-reduction]]\n* [[APX]]\n\n== References ==\n{{Reflist}}\n\n* Ingo Wegener. Complexity Theory: Exploring the Limits of Efficient Algorithms. {{isbn|3-540-21045-8}}. Chapter 8, pp.&nbsp;110&ndash;111. [https://books.google.com/books?vid=ISBN3540210458 Google Books preview]\n\n[[Category:Reduction (complexity)]]\n[[Category:Approximation algorithms]]"
    },
    {
      "title": "Set cover problem",
      "url": "https://en.wikipedia.org/wiki/Set_cover_problem",
      "text": "The '''set cover problem''' is a classical question in [[combinatorics]], [[computer science]], [[operations research]], and [[Computational complexity theory|complexity theory]]. It is one of [[Karp's 21 NP-complete problems]] shown to be [[NP-complete]] in 1972.\n\nIt is a problem \"whose study has led to the development of fundamental techniques for the entire field\" of [[approximation algorithms]].<ref>{{harvtxt|Vazirani|2001|p=15}}</ref>\n\nGiven a set of elements <math>\\{1,2,...,n\\}</math> (called the [[universe (mathematics)|universe]]) and a collection <math>S</math> of <math>m</math> sets whose [[union (set theory)|union]] equals the universe, the set cover problem is to identify the smallest sub-collection of <math>S</math> whose union equals the universe.  For example, consider the universe <math>U = \\{1, 2, 3, 4, 5\\}</math> and the collection of sets <math>S = \\{\\{1, 2, 3\\}, \\{2, 4\\}, \\{3, 4\\}, \\{4, 5\\}\\}</math>. Clearly the union of <math>S</math> is <math>U</math>. However, we can cover all of the elements with the following, smaller number of sets: <math>\\{\\{1, 2, 3\\}, \\{4, 5\\}\\}</math>.\n\nMore formally, given a universe <math>\\mathcal{U}</math> and a family <math>\\mathcal{S}</math> of subsets of <math>\\mathcal{U}</math>,\na ''cover'' is a subfamily <math>\\mathcal{C}\\subseteq\\mathcal{S}</math> of sets whose union is <math>\\mathcal{U}</math>. In the set covering [[decision problem]], the input is a pair <math>(\\mathcal{U},\\mathcal{S})</math> and an integer <math>k</math>; the question is whether\nthere is a set covering of size <math>k</math> or less. In the set covering [[optimization problem]], the input is a pair <math>(\\mathcal{U},\\mathcal{S})</math>, and the task is to find a set covering that uses the fewest sets.\n\nThe decision version of set covering is [[NP-complete]], and the optimization/search version of set cover is [[NP-hard]].{{sfn |Korte|Vygen|2012|p=414}}\n\nIf each set is assigned a cost, it becomes a ''weighted'' set cover problem.\n\n{{Covering-Packing Problem Pairs}}\n\n==Integer linear program formulation==\nThe minimum set cover problem can be formulated as the following [[integer linear program]] (ILP).<ref>{{harvtxt|Vazirani|2001|p=108}}</ref>\n{|\n| minimize\n| <math>\\sum_{S \\in \\mathcal S} x_S</math>\n|\n| (minimize the number of sets)\n|-\n| subject to\n| <math>\\sum_{S\\colon e \\in S} x_S \\geqslant 1 </math>\n| for all <math>e\\in \\mathcal U</math>\n| (cover every element of the universe)\n|-\n|\n| <math>x_S \\in \\{0,1\\}</math>\n| for all <math>S\\in \\mathcal S</math>.\n| (every set is either in the set cover or not)\n|}\nThis ILP belongs to the more general class of ILPs for [[covering problem]]s.\nThe [[Linear programming relaxation#Approximation and integrality gap|integrality gap]] of this ILP is at most <math>\\scriptstyle \\log n</math>, so its [[Linear programming relaxation|relaxation]] gives a factor-<math>\\scriptstyle \\log n</math> [[approximation algorithm]] for the minimum set cover problem (where <math>\\scriptstyle n</math> is the size of the universe).<ref>{{harvtxt|Vazirani|2001|pp=110–112}}</ref>\n\nIn weighted set cover, the sets are assigned weights. Denote the weight of set <math>S\\in \\mathcal{U}</math> by <math>w_{S}</math>. Then the integer linear program describing weighted set cover is identical to the one given above, except that the objective function to minimize is <math>\\sum_{S \\in \\mathcal S} w_S x_S</math>.\n\n== Hitting set formulation ==\nSet covering is equivalent to the '''hitting set problem'''. That is seen by observing that an instance of set covering can\nbe viewed as an arbitrary [[bipartite graph]], with sets represented by vertices on the left, the universe represented by vertices on the\nright, and edges representing the inclusion of elements in sets. The task is then to find a minimum cardinality subset of left-vertices which covers all of the right-vertices. In the Hitting set problem, the objective is to cover the left-vertices using a minimum subset of the right vertices. Converting from one problem to the other is therefore achieved by interchanging the two sets of vertices.\n\n== Greedy algorithm ==\n\nThere is a [[greedy algorithm]] for polynomial time approximation of set covering that chooses sets according to one rule: at each stage, choose the set that contains the largest number of uncovered elements. It can be shown<ref>Chvatal, V. [https://www.jstor.org/stable/3689577 A Greedy Heuristic for the Set-Covering Problem]. Mathematics of Operations Research\nVol. 4, No. 3 (Aug., 1979), pp. 233-235</ref> that this algorithm achieves an approximation ratio of <math>H(s)</math>, where <math>s</math> is the size of the set to be covered. In other words, it finds a covering that may be <math>H(n)</math> times as large as the minimum one, where <math>H(n)</math> is the <math>n</math>-th [[harmonic number]]:\n\n:<math> H(n) = \\sum_{k=1}^{n} \\frac{1}{k} \\le \\ln{n} +1</math>\n\nThis greedy algorithm actually achieves an approximation ratio of <math>H(s^\\prime)</math> where <math>s^\\prime</math> is the maximum cardinality set of <math>S</math>. For <math>\\delta-</math>dense instances, however, there exists a <math>c \\ln{m}</math>-approximation algorithm for every <math>c > 0</math>.<ref>\n{{harvnb|Karpinski|Zelikovsky|1998}}</ref>\n\n[[Image:SetCoverGreedy.gif|frame|Tight example for the greedy algorithm with k=3]]\n\nThere is a standard example on which the greedy algorithm achieves an approximation ratio of <math>\\log_2(n)/2</math>.\nThe universe consists of <math>n=2^{(k+1)}-2</math> elements. The set system consists of <math>k</math> pairwise disjoint sets \n<math>S_1,\\ldots,S_k</math> with sizes <math>2,4,8,\\ldots,2^k</math> respectively, as well as two additional disjoint sets <math>T_0,T_1</math>,\neach of which contains half of the elements from each <math>S_i</math>. On this input, the greedy algorithm takes the sets\n<math>S_k,\\ldots,S_1</math>, in that order, while the optimal solution consists only of <math>T_0</math> and <math>T_1</math>.\nAn example of such an input for <math>k=3</math> is pictured on the right.\n\nInapproximability results show that the greedy algorithm is essentially the best-possible polynomial time approximation algorithm for set cover up to lower order terms\n(see [[Set cover problem#Inapproximability results|Inapproximability results]] below), under plausible complexity assumptions. A tighter analysis for the greedy algorithm shows that the approximation ratio is exactly <math>\\ln{n} - \\ln{\\ln{n}} + \\Theta(1)</math>.<ref>Slavík Petr  [http://dl.acm.org/citation.cfm?id=237991 A tight analysis of the greedy algorithm for set cover]. STOC'96, Pages 435-441, doi:10.1145/237814.237991</ref>\n\n== Low-frequency systems ==\n\nIf each element occurs in at most {{var|f}} sets, then a solution can be found in polynomial time that approximates the optimum to within a factor of {{var|f}} using [[Linear programming relaxation|LP relaxation]].\n\nIf the constraint <math>x_S\\in\\{0,1\\}</math> is replaced by <math>x_S \\geq 0</math> for all {{var|S}} in <math>\\mathcal{S}</math> in the integer linear program shown [[#Integer linear program formulation|above]], then it becomes a (non-integer) linear program {{var|L}}. The algorithm can be described as follows:\n# Find an optimal solution {{var|O}} for the program {{var|L}} using some polynomial-time method of solving linear programs.\n# Pick all sets {{var|S}} for which the corresponding variable {{var|x}}<sub>{{var|S}}</sub> has value at least 1/{{var|f}} in the solution {{var|O}}.<ref>{{harvtxt|Vazirani|2001|pp=118–119}}</ref>\n\n== Inapproximability results ==\n\nWhen <math> n</math> refers to the size of the universe, {{harvtxt |Lund|Yannakakis|1994}} showed that set covering cannot be approximated in polynomial time to within a factor of <math>\\tfrac{1}{2}\\log_2{n} \\approx 0.72\\ln{n}</math>, unless '''NP''' has [[quasi-polynomial time]] algorithms. [[Uriel Feige|Feige]] (1998) improved this lower bound to <math>\\bigl(1-o(1)\\bigr)\\cdot\\ln{n}</math> under the same assumptions, which essentially matches the approximation ratio achieved by the greedy algorithm. {{harvtxt |Raz|Safra|1997}} established a lower bound\nof <math>c\\cdot\\ln{n}</math>, where <math>c</math> is a certain constant, under the weaker assumption that '''P'''<math>\\not=</math>'''NP'''.\nA similar result with a higher value of <math>c</math> was recently proved by {{harvtxt |Alon|Moshkovitz|Safra|2006}}. {{harvtxt |Dinur|Steurer|2013}} showed optimal inapproximability by proving that it cannot be approximated to <math>\\bigl(1 - o(1)\\bigr) \\cdot \\ln{n}</math> unless '''P'''<math>=</math>'''NP'''.\n\n== Weighted set cover ==\n{{Expand section|date=November 2017}}\n\n[[Linear programming relaxation|Relaxing]] the integer linear program for weighted set cover stated [[#Integer linear program formulation|above]], one may use [[randomized rounding]] to get an <math>O(\\log n)</math>-factor approximation. The corresponding analysis for nonweighted set cover is outlined in [[Randomized rounding#Randomized-rounding algorithm for set cover]] and can be adapted to the weighted case.<ref>{{harvtxt|Vazirani|2001|loc=Chapter 14}}</ref>\n\n== Related problems ==\n* Hitting set is an equivalent reformulation of Set Cover.\n* [[Vertex cover problem|Vertex cover]] is a special case of Hitting Set.\n* [[Edge cover problem|Edge cover]] is a special case of Set Cover.\n* [[Geometric Set Cover Problem|Geometric set cover]] is a special case of Set Cover when the universe is a set of points in <math>\\mathbb{R}^d</math> and the sets are induced by the intersection of the universe and geometric shapes (e.g., disks, rectangles).\n* [[Set packing]]\n* [[Maximum coverage problem]] is to choose at most k sets to cover as many elements as possible.\n* [[Dominating set]] is the problem of selecting a set of vertices (the dominating set) in a graph such that all other vertices are adjacent to at least one vertex in the dominating set. The Dominating set problem was shown to be NP complete through a reduction from Set cover.\n* [[Exact cover problem]] is to choose a set cover with no element included in more than one covering set.\n* [[Closest pair of points problem]]\n* [[Nearest neighbor search]]\n\n==Notes==\n\n{{reflist}}\n\n== References ==\n\n* {{Citation | last1=Alon | first1=Noga | author1-link=Noga Alon | last2=Moshkovitz | first2=Dana | last3=Safra | first3=Shmuel | author3-link=Shmuel Safra | title=Algorithmic construction of sets for k-restrictions | year=2006 | journal=ACM Trans. Algorithms | issn=1549-6325 | volume=2 | issue=2 | pages=153–177 | doi=10.1145/1150334.1150336| citeseerx=10.1.1.138.8682 }}.\n* {{Citation\n | last=Cormen     | first=Thomas H.   | authorlink=Thomas H. Cormen\n | last2=Leiserson | first2=Charles E. | authorlink2=Charles E. Leiserson\n | last3=Rivest    | first3=Ronald L.  | authorlink3=Ronald L. Rivest\n | last4=Stein     | first4=Clifford   | authorlink4=Clifford Stein\n | title=Introduction to Algorithms | year=2001 | publisher=MIT Press and McGraw-Hill | location=Cambridge, Mass.\n | isbn=978-0-262-03293-3 | pages=1033–1038}}\n* {{Citation | last1=Feige | first1=Uriel | author1-link=Uriel Feige | title=A threshold of ln n for approximating set cover | year=1998 | journal=[[Journal of the ACM]] | issn=0004-5411 | volume=45 | issue=4 | pages=634–652 | doi=10.1145/285055.285059| citeseerx=10.1.1.70.5014 }}.\n* {{Citation\n | last1=Karpinski | first1=Marek | last2=Zelikovsky|first2=Alexander\n | year=1998\n | title=Approximating dense cases of covering problems\n | booktitle=Proceedings of the DIMACS Workshop on Network Design: Connectivity and Facilities Location\n | volume=40\n | pages=169–178\n | url=https://books.google.com/?id=IMmuF0RZk1MC&pg=PA169&dq=karpinski+zelikovsky+cover+dense#v=onepage&q=karpinski%20zelikovsky%20cover%20dense&f=false\n| isbn=9780821870846 }}\n* {{Citation | last1=Lund | first1=Carsten | author1-link=Carsten Lund | last2=Yannakakis | first2=Mihalis | author2-link=Mihalis Yannakakis | title=On the hardness of approximating minimization problems | year=1994 | journal=[[Journal of the ACM]] | issn=0004-5411 | volume=41 | issue=5 | pages=960–981 | doi=10.1145/185675.306789}}.\n* {{Citation | last1=Raz | first1=Ran | author1-link=Ran Raz | last2=Safra | first2=Shmuel | author2-link=Shmuel Safra | title=STOC '97: Proceedings of the twenty-ninth annual ACM symposium on Theory of computing | publisher=ACM | isbn=978-0-89791-888-6 | year=1997 | chapter=A sub-constant error-probability low-degree test, and a sub-constant error-probability PCP characterization of NP | pages=475–484}}.\n* {{Citation | last1=Dinur | first1=Irit | author1-link=Irit Dinur | last2=Steurer | first2=David | title=STOC '14: Proceedings of the forty-sixth annual ACM symposium on Theory of computing | publisher=ACM | year=2013 | chapter=Analytical approach to parallel repetition | pages=624–633}}.\n* {{Citation| last = Vazirani | first = Vijay V.\n | authorlink = Vijay Vazirani\n | title = Approximation Algorithms\n | year = 2001\n | publisher = Springer-Verlag\n | isbn = 978-3-540-65367-7\n | url = http://www.cc.gatech.edu/fac/Vijay.Vazirani/book.pdf\n | ref = harv\n| postscript = \n }}\n* {{Citation| last1 = Korte | first1 = Bernhard\n | last2 = Vygen | first2 = Jens\n | title = Combinatorial Optimization: Theory and Algorithms\n | year = 2012\n | isbn = 978-3-642-24487-2\n | edition = 5\n | publisher = Springer\n | ref = harv\n| postscript = \n }}\n* {{Citation| last1 = Cardoso | first1 = Nuno | last2 = Abreu | first2 = Rui | title = An Efficient Distributed Algorithm for Computing Minimal Hitting Sets | year = 2014 | booktitle=[[Diagnosis (artificial intelligence)|Proceedings of the 25th International Workshop on Principles of Diagnosis]] (DX'14, best paper award)  |location=Graz, Austria | url = http://dx-2014.ist.tugraz.at/papers/DX14_Mon_PM_S1_paper1.pdf | doi=10.5281/zenodo.10037 | ref = harv}}\n\n== External links ==\n{{commons category}}\n* [http://www.nlsde.buaa.edu.cn/~kexu/benchmarks/set-benchmarks.htm Benchmarks with Hidden Optimum Solutions for Set Covering, Set Packing and Winner Determination]\n* [http://www.csc.kth.se/~viggo/wwwcompendium/node146.html A compendium of NP optimization problems - Minimum Set Cover]\n\n{{DEFAULTSORT:Set Cover Problem}}\n[[Category:Set families]]\n[[Category:NP-complete problems]]\n[[Category:Linear programming]]\n[[Category:Approximation algorithms]]"
    },
    {
      "title": "Shortest common supersequence problem",
      "url": "https://en.wikipedia.org/wiki/Shortest_common_supersequence_problem",
      "text": "In [[computer science]], the '''shortest common supersequence''' of two sequences '''X''' and '''Y''' is the shortest sequence which has '''X''' and '''Y''' as [[subsequence]]s. This is a problem closely related to the [[longest common subsequence problem]]. Given two sequences '''X''' = < x<sub>1</sub>,...,x<sub>m</sub> > and '''Y''' = < y<sub>1</sub>,...,y<sub>n</sub> >, a sequence '''U''' = < u<sub>1</sub>,...,u<sub>k</sub> > is a common supersequence of '''X''' and '''Y''' if items can be removed from '''U''' to produce '''X''' or '''Y'''.\n\nA shortest common supersequence (SCS) is a common supersequence of minimal length. In the shortest common supersequence problem, the two sequences '''X''' and '''Y''' are given and the task is to find a shortest possible common supersequence of these sequences. In general, an SCS is not unique.\n\nFor two input sequences, an SCS can be formed from a [[longest common subsequence]] (LCS) easily. For example, if '''X'''<math>[1..m] = abcbdab</math> and '''Y'''<math>[1..n] = bdcaba</math>, the lcs is '''Z'''<math>[1..r] = bcba</math>. By inserting the non-lcs symbols while preserving the symbol order, we get the SCS: '''U'''<math>[1..t] = abdcabdab</math>.\n\nIt is quite clear that <math> r + t = m + n </math> for two input sequences. However, for three or more input sequences this does not hold. Note also, that the LCS and the SCS problems are not [[dual problem]]s.\n\n== Shortest common superstring ==\n\nThe closely related problem of finding a string which is a superstring of a finite set of strings {{var|S}} = { {{var|s}}<sub>1</sub>,{{var|s}}<sub>2</sub>,...,{{var|s}}<sub>n</sub> } is NP-Complete.<ref>{{cite journal \n |title=The shortest common supersequence problem over binary alphabet is NP-complete\n |author=Kari-Jouko Räihä, Esko Ukkonen\n |journal=Theoretical Computer Science\n |volume=16\n |number=2\n |date=1981\n |pages=187–198\n |doi=10.1016/0304-3975(81)90075-x\n |url=http://www.sciencedirect.com/science/article/pii/030439758190075X\n}}</ref>  Also, good (constant factor) approximations have been found for the average case but not for the worst case.<ref>{{cite journal\n |title=On the Approximation of Shortest Common Supersequences and Longest Common Subsequences\n |author=Tao Jiang and Ming Li\n |journal=SIAM Journal on Computing\n |volume=24\n |number=5\n |date=1994\n |pages=1122–1139\n |doi=10.1137/s009753979223842x\n |url=http://epubs.siam.org/doi/abs/10.1137/S009753979223842X\n}}</ref><ref>\n{{cite journal\n |title=On Improved Inapproximability Results for the Shortest Superstring and Related Problems\n |author=Marek Karpinski and Richard Schmied\n |journal=Proceedings of 19th CATS CRPIT\n |volume=141\n |date=2013\n |pages=27–36\n |url=http://crpit.com/abstracts/CRPITV141Karpinski.html\n }}\n</ref> However, it can be formulated as an instance of [[Set cover problem#Weighted set cover|weighted set cover]] in such a way that the weight of the optimal solution to the set cover instance is less than twice the length of the shortest superstring {{var|S}}. One can then use the [[Approximation algorithm#Performance guarantees|O(log({{var|n}}))-approximation]] for weighted set-cover to obtain an O(log({{var|n}}))-approximation for the shortest superstring (note that this is ''not'' a constant factor approximation).\n\nFor any string {{var|x}} in this alphabet, define {{var|P}}({{var|x}}) to be the set of all strings which are substrings of {{var|x}}. The instance {{var|I}} of set cover is formulated as follows: \n* Let {{var|M}} be empty.\n* For each pair of strings {{var|s}}<sub>{{var|i}}</sub> and {{var|s}}<sub>{{var|j}}</sub>, if the last {{var|k}} symbols of {{var|s}}<sub>{{var|i}}</sub> are the same as the first {{var|k}} symbols of {{var|s}}<sub>{{var|j}}</sub>, then add a string to {{var|M}} that consists of the concatenation with maximal overlap of {{var|s}}<sub>{{var|i}}</sub> with {{var|s}}<sub>{{var|j}}</sub>. \n* Define the universe <math>\\mathcal U</math> of the set cover instance to be {{var|S}}\n* Define the set of subsets of the universe to be { {{var|P}}({{var|x}}) | {{var|x}} ∈ {{var|S}} ∪ {{var|M}} }\n* Define the cost of each subset {{var|P}}(x) to be |{{var|x}}|, the length of {{var|x}}.\n\nThe instance {{var|I}} can then be solved using an algorithm for weighted set cover, and the algorithm can output an arbitrary concatenation of the strings {{var|x}} for which the weighted set cover algorithm outputs {{var|P}}({{var|x}}).{{sfn|Vazirani|p=20}}\n\n=== Example ===\n\nConsider the set {{var|S}} = { abc, cde, fab }, which becomes the universe of the weighted set cover instance. In this case, {{var|M}} = { abcde, fabc }. Then the set of subsets of the universe is\n:<math displaystyle=\"block\">\n\\begin{align}\n  \\{ P(x) | x\\in S\\cup M \\} \n  &= \\{ P(x) | x\\in \\{ abc, cde, fab, abcde, fabc \\} \\} \\\\\n  &= \\{ P(abc), P(cde), P(fab), P(abcde), P(fabc) \\} \\} \\\\\n  &= \\{ \\{a,b,c,ab,bc,abc\\}, \\{c,d,e,cd,de,cde\\},\\ldots, \\{f,a,b,c,fa,ab,bc,fab,abc,fabc\\} \\} \\} \\\\\n\\end{align}\n</math>\nwhich have costs 3, 3, 3, 5, and 4, respectively.\n\n== References ==\n{{Reflist}}\n\n* {{cite book | first1=Michael R. | last1=Garey | author1-link=Michael R. Garey | first2=David S. | last2=Johnson | author2-link=David S. Johnson | year = 1979 | title = [[Computers and Intractability: A Guide to the Theory of NP-Completeness]] | publisher = W.H. Freeman | isbn = 0-7167-1045-5 | zbl=0411.68039 | at=p. 228 A4.2: SR8 }}\n* {{cite book | last=Szpankowski | first=Wojciech | authorlink = Wojciech Szpankowski | title=Average case analysis of algorithms on sequences | others=With a foreword by Philippe Flajolet | series=Wiley-Interscience Series in Discrete Mathematics and Optimization | location=Chichester | publisher=Wiley | year=2001 | isbn=0-471-24063-X | zbl=0968.68205 }}\n* {{Citation| last = Vazirani | first = Vijay V.\n | authorlink = Vijay Vazirani\n | title = Approximation Algorithms\n | year = 2001\n | publisher = Springer-Verlag\n | isbn = 3-540-65367-8\n | url = http://www.cc.gatech.edu/fac/Vijay.Vazirani/book.pd\n }}\n\n==External links==\n* [http://nist.gov/dads/HTML/shortestCommonSuperseq.html Dictionary of Algorithms and Data Structures: shortest common supersequence]\n\n[[Category:Problems on strings]]\n[[Category:Combinatorics]]\n[[Category:Formal languages]]\n[[Category:Dynamic programming]]\n[[Category:NP-complete problems]]\n[[Category:Approximation algorithms]]"
    },
    {
      "title": "Token reconfiguration",
      "url": "https://en.wikipedia.org/wiki/Token_reconfiguration",
      "text": "In [[computational complexity theory]] and [[combinatorics]], the '''token reconfiguration problem''' is an optimization problem on a graph with both an initial and desired state for tokens.\n\nGiven a graph <math>G</math>, an initial state of tokens is defined by a subset <math>V \\subset V(G)</math> of the vertices of the graph; let <math>n = |V|</math>.  Moving a token from vertex <math>v_1</math> to vertex <math>v_2</math> is '''valid''' if <math>v_1</math> and <math>v_2</math> are joined by a path in <math>G</math> that does not contain any other tokens; note that the distance traveled within the graph is inconsequential, and moving a token across multiple edges sequentially is considered a single move.  A desired end state is defined as another subset <math>V' \\subset V(G)</math>.  The goal is to minimize the number of valid moves to reach the end state from the initial state.<ref name=\"d\">{{cite web|first1=Erik|last1=Demaine|date=Fall 2014|title=Algorithmic Lower Bounds: Fun with Hardness Proofs Lecture 11 Notes|url=http://courses.csail.mit.edu/6.890/fall14/scribe/lec11.pdf}}</ref>\n\n== Motivation ==\n\nThe problem is motivated by so-called [[sliding puzzle]]s, which are in fact a variant of this problem, often restricted to rectangular grid graphs with no holes.  The most famous such puzzle, the 15 puzzle, is a variant of this problem on a 4 by 4 grid graph such that <math>n = |V(G)| - 1</math>.  One key difference between sliding block puzzles and the token reconfiguration problem is that in the original token reconfiguration problem, the tokens are indistinguishable.  As a result, if the graph is connected, the token reconfiguration problem is always solvable; this is not necessarily the case for sliding block puzzles.\n\n== Complexity ==\n\nCalinescu, Dumitrescu, and Pach have shown several results regarding both the optimization and approximation of this problem on various types of graphs.<ref name=calinescu>{{cite book|last1=Calinescu|first1=Gruia|last2=Dumitrescu|first2=Adrian|last3=Pach|first3=János|title=Reconfigurations in Graphs and Grids|journal=LATIN 2006: Theoretical Informatics, 7th Latin American Symposium, Valdivia, Chile, March 20–24, 2006, Proceedings|volume=3887|date=2006|pages=262–273|doi=10.1007/11682462_27|series=Lecture Notes in Computer Science|isbn=978-3-540-32755-4}}</ref>\n\n=== Optimization ===\nFirstly, reducing to the case of trees, there is always a solution in at most <math>n</math> moves, with at most one move per token.  Furthermore, an optimal solution can be found in time linear in the size of the tree.  Clearly, the first result extends to arbitrary graphs; the latter does not.\n\nA sketch of the optimal algorithm for trees is as follows.  First, we obtain an algorithm that moves each node exactly once, which may not be optimal.  Do this recursively: consider any leaf of the smallest tree in the graph containing both the initial and desired sets.  If a leaf of this tree is in both, remove it and recurse down.  If a leaf is in the initial set only, find a path from it to a vertex in the desired set that does not pass through any other vertices in the desired set.  Remove this path (it'll be the last move), and recurse down.  The other case, where the leaf is in the desired set only, is symmetric.  To extend to an algorithm that achieves the optimum, consider any token in both the initial and desired sets.  If removing it would split the graph into subtrees, all of which have the same number of elements from the initial and desired sets, then do so and recurse.  If there is no such token, then each token must move exactly once, and so the solution that moves all tokens exactly once must be optimal.\n\nWhile the algorithm for finding the optimum on trees is linear time, finding the optimum for general graphs is NP-complete, a leap up in difficulty.  It is in NP; the certificate is a sequence of moves, which is at most linear size, so it remains to show the problem is NP-hard as well.  This is done via [[reduction (complexity)|reduction]] from [[set cover]].\n\nConsider an instance of set cover, where we wish to cover all elements <math>v_1, v_2, \\ldots, v_n</math> in a universe <math>U</math> using subsets <math>S_1, S_2, \\ldots, S_m</math> of <math>U</math> using the minimum number of subsets.  Construct a graph as follows:\n\nMake a vertex for each of the elements in the universe and each of the subsets.  Connect a subset vertex to an element vertex if the subset contains that element.  Create a long path of size <math>n</math>, and attach one end to every subset vertex.  The initial set is the added path plus every subset vertex, and the final set is every subset vertex plus every element vertex.\n\nTo see why this is a reduction, consider the selection of which subset vertex tokens to move.  Clearly, we must open up paths to each of the element vertices, and we do so by moving some of the subset vertex tokens.  After doing so, each token on the long path must move once.  Thus, the optimum cost is equal to the number of selected subsets plus the number of elements (the latter of which is notably a constant).  So we have a polynomial-time reduction from set cover, which is NP-complete, to token reconfiguration.  Thus token reconfiguration is also NP-complete on general graphs.\n\n=== Approximation ===\nThe token reconfiguration problem is [[APX#Related complexity classes|APX-complete]], meaning that in some sense, it is as hard to approximate as any problem that has a constant-factor [[approximation algorithm]].  The reduction is the same one as above, from set cover.  However, the set cover problem is restricted to subsets of size at most 3, which is an APX-hard problem.<ref>{{cite journal|last1=Papadimitriou|first1=Christos H.|last2=Yannakakis|first2=Mihailis|title=Optimization, Approximation, and Complexity Classes|journal=Journal of Computer and System Sciences|date=1991|volume=43|issue=3|pages=425–440|doi=10.1016/0022-0000(91)90023-X}}</ref>\n\nUsing exactly the same structure as above, we obtain an [[L-reduction]], as the distance of any solution from optimum is equal between the set cover instance and the transformed token reconfiguration problem.  The only change is the addition of the number of elements in the universe.  Furthermore, the set cover optimum is at least 1/3 of the number of elements, due to the bounded subset size.  Thus, the constants from the [[L-reduction]] are <math>\\alpha = 1/4, \\beta = 1</math>.\n\nOne can, in fact, modify the reduction to work for labeled token reconfiguration as well.  To do so, attach a new vertex to each of the subset vertices, which is neither an initial nor desired vertex.  Label the vertices on the long path 1 through <math>n</math>, and do the same for the element vertices.  Now, the solution consists of 'moving aside' each chosen subset vertex token, correctly placing the labeled vertices from the path, and returning the subset vertex tokens to the initial locations.  This is an L-reduction with <math>\\alpha = 1/5, \\beta = 2</math>.\n\nCalinescu, Dumitrescu, and Pach have also shown that there exists a 3-approximation for unlabeled token reconfiguration, so the problem is in APX as well and thus APX-complete.  The proof is much more complicated and omitted here.\n\n== References ==\n{{reflist}}\n\n[[Category:NP-complete problems]]\n[[Category:Computational problems in graph theory]]\n[[Category:Approximation algorithms]]"
    },
    {
      "title": "Unique games conjecture",
      "url": "https://en.wikipedia.org/wiki/Unique_games_conjecture",
      "text": "{{unsolved|computer science|Prove or disprove the Unique Games Conjecture.}}\nIn [[computational complexity theory]], the '''unique games conjecture''' (often referred to as '''UGC''') is a conjecture made by [[Subhash Khot]] in 2002.<ref name=\"klarreich\">{{cite news\n | url=http://simonsfoundation.org/features/feature-articles/mathematics-and-physical-science/approximately-hard-the-unique-games-conjecture/\n | title=Approximately Hard: The Unique Games Conjecture\n | work=Simons Foundation\n | author=Erica Klarreich\n | date=2011-10-06\n | accessdate=2012-10-29 }}\n</ref><ref name=\"lipton\">{{cite web\n | url=http://rjlipton.wordpress.com/2010/05/05/unique-games-a-three-act-play/\n | title=Unique Games: A Three Act Play\n | work=Gödel’s Lost Letter and P=NP\n | author=Dick Lipton\n | date=2010-05-05\n | accessdate=2012-10-29 }}\n</ref><ref name=\"khot02onthepower\"/> The conjecture postulates that the problem of determining the approximate ''value'' of a certain type of game, known as a ''unique game'', has [[NP-hard]] [[Computational complexity theory|algorithmic complexity]].  It has broad applications in the theory of [[hardness of approximation]]. If it is true, then for many important problems it is not only impossible to get an exact solution in [[polynomial time]] (as postulated by the [[P versus NP problem]]), but also impossible to get a good polynomial-time approximation. The problems for which such an inapproximability result would hold include [[constraint satisfaction problem]]s, which crop up in a wide variety of disciplines.\n\nThe conjecture is unusual in that the academic world seems about evenly divided on whether it is true or not.<ref name=\"klarreich\"/>\n\n==Formulations==\n\nThe unique games conjecture can be stated in a number of equivalent ways.\n\n===Unique label cover===\nThe following formulation of the unique games conjecture is often used in [[hardness of approximation]]. The conjecture postulates the [[NP-hard]]ness of the following [[promise problem]] known as ''label cover with unique constraints''. For each edge, the colors on the two vertices are restricted to some particular ordered pairs. ''Unique'' constraints means that for each edge none of the ordered pairs have the same color for the same node.\n\nThis means that an instance of label cover with unique constraints over an alphabet of size ''k'' can be represented as a [[directed graph]] together with a collection of [[permutation]]s π<sub>''e''</sub>: [''k''] → [''k''], one for each edge ''e'' of the graph.  An assignment to a label cover instance gives to each vertex of ''G'' a value in the set [''k''] = {1, 2, ... k}, often called “colours.”\n\n<gallery widths=\"256px\" perrow=2>\n\nImage:Unique label cover yes-instance.svg|An instance of unique label cover. The 4 vertices may be assigned the colors red, blue, and green while satisfying the constraints at each edge.\n\nImage:Unique label cover yes-instance with assignment.svg|A solution to the unique label cover instance.\n\n</gallery>\n\nSuch instances are strongly constrained in the sense that the colour of a vertex uniquely defines the colours of its neighbours, and hence for its entire connected component. Thus, if the input instance admits a valid assignment, then such an assignment can be found efficiently by iterating over all colours of a single node. In particular, the problem of deciding if a given instance admits a satisfying assignment can be solved in polynomial time.\n\n<gallery widths=\"256px\" perrow=2>\nImage:Unique label cover no-instance.svg|An instance of unique label cover that does not allow a satisfying assignment.\n\nImage:Unique label cover no-instance with assignment.svg|An assignment that satisfies all edges except the thick edge. Thus, this instance has value 3/4.\n</gallery>\n\nThe ''value'' of a unique label cover instance is the fraction of constraints that can be satisfied by any assignment. For satisfiable instances, this value is 1 and is easy to find. On the other hand, it seems to be very difficult to determine the value of an unsatisfiable game, even approximately. The unique games conjecture formalises this difficulty.\n\nMore formally, the (''c'', ''s'')-gap label-cover problem with unique constraints is the following promise problem (''L''<sub>yes</sub>, ''L''<sub>no</sub>):\n* ''L''<sub>yes</sub> = {''G'': Some assignment satisfies at least a ''c''-fraction of constraints in ''G''}\n* ''L''<sub>no</sub> = {''G'': Every assignment satisfies at most an ''s''-fraction of constraints in ''G''}\nwhere ''G'' is an instance of the label cover problem with unique constraints.\n\nThe unique games conjecture states that for every sufficiently small pair of constants ''ε'',&nbsp;''δ''&nbsp;>&nbsp;0, there exists a constant ''k'' such that the (1&nbsp;−&nbsp;''δ'',&nbsp;''ε'')-gap label-cover problem with unique constraints over alphabet of size ''k'' is [[NP-hard]].\n\nInstead of graphs, the label cover problem can be formulated in terms of linear equations. For example, suppose that we have a system of linear equations over the integers modulo 7:\n\n: <math>\n\\begin{align}\nx_1 & \\equiv 2\\cdot x_2 \\pmod 7, \\\\\nx_2 & \\equiv 4\\cdot x_5 \\pmod 7, \\\\\n& {}\\  \\  \\vdots \\\\\nx_1 & \\equiv 2\\cdot x_7 \\pmod 7.\n\\end{align}\n</math>\n\nThis is an instance of the label cover problem with unique constraints.  For example, the first equation corresponds to the permutation {{pi}}<sub>(1, 2)</sub> where {{pi}}<sub>(1,&nbsp;2)</sub>(''x''<sub>1</sub>) =&nbsp;2''x''<sub>2</sub>&nbsp;modulo&nbsp;7.\n\n===Two-prover proof systems===\n\nA '''unique game''' is a special case of a ''two-prover one-round (2P1R) game''.  A two-prover one-round game has two players (also known as provers) and a referee.  The referee sends each player a question drawn from a known [[probability distribution]], and the players each have to send an answer. The answers come from a set of fixed size.  The game is specified by a predicate that depends on the questions sent to the players and the answers provided by them.\n\nThe players may decide on a strategy beforehand, although they cannot communicate with each other during the game.  The players win if the predicate is satisfied by their questions and their answers.\n\nA two-prover one-round game is called a ''unique game'' if for every pair of questions and every answer to the first question, there is exactly one answer to the second question that results in a win for the players, and vice versa. The ''value'' of a game is the maximum winning probability for the players over all strategies.\n\nThe '''unique games conjecture''' states that for every sufficiently small pair of constants ''ε'',&nbsp;''δ''&nbsp;>&nbsp;0, there exists a constant ''k'' such that the following [[promise problem]] (''L''<sub>yes</sub>, ''L''<sub>no</sub>) is [[NP-hard]]:\n* ''L''<sub>yes</sub> = {''G'': the value of ''G'' is at least 1&nbsp;&minus;&nbsp;δ}\n* ''L''<sub>no</sub> = {''G'': the value of ''G'' is at most&nbsp;ε}\nwhere ''G'' is a unique game whose answers come from a set of size&nbsp;''k''.\n\n===Probabilistically checkable proofs===\nAlternatively, the unique games conjecture postulates the existence of a certain type of [[probabilistically checkable proof]] for problems in '''[[NP (complexity)|NP]]'''.\n\nA unique game can be viewed as a special kind of nonadaptive probabilistically checkable proof with query complexity 2, where for each pair of possible queries of the verifier and each possible answer to the first query, there is exactly one possible answer to the second query that makes the verifier accept, and vice versa.\n\nThe unique games conjecture states that for every sufficiently small pair of constants ''ε'',&nbsp;''δ''&nbsp;>&nbsp;0 there is a constant ''K'' such that every problem in '''[[NP (complexity)|NP]]''' has a probabilistically checkable proof over an alphabet of size ''K'' with completeness 1&nbsp;−&nbsp;''δ'', soundness ''ε'' and randomness complexity O(log(''n'')) which is a unique game.\n\n==Relevance==\n{| class=\"wikitable\" style=\"float: right; margin-left: 1em;\" border=\"1\"\n|+ '''Approximability results assuming P ≠ NP versus the UGC'''\n! Problem || Poly.-time approx. || NP hardness || UG hardness\n|-\n| [[2-satisfiability|Max 2-Sat]] || 0.940...<ref name=\"FG95approximating\"/> || 0.954...&nbsp;+&nbsp;ε<ref name=\"hastad99someoptimal\"/> || 0.9439...&nbsp;+&nbsp;ε<ref name=\"KKMO07optimal\"/>\n|-\n| [[Maximum cut|Max Cut]] || 0.878...<ref name=\"GW95improved\"/> || 0.941...&nbsp;+&nbsp;ε<ref name=\"hastad99someoptimal\"/> || 0.878...&nbsp;+&nbsp;ε<ref name=\"KKMO07optimal\"/>\n|-\n| [[Vertex cover|Min Vertex Cover]] || 2 || 1.360...&nbsp;&minus;&nbsp;ε<ref name=\"IS05onthehardness\"/> || 2-''ε''<ref name=\"KR03vertex\"/>\n|-\n| [[Betweenness]] || 1/3 || 47/48<ref>{{citation\n | last1 = Chor | first1 = Benny\n | last2 = Sudan | first2 = Madhu | author2-link = Madhu Sudan\n | doi = 10.1137/S0895480195296221\n | issue = 4\n | journal = [[SIAM Journal on Discrete Mathematics]]\n | mr = 1640920\n | pages = 511–523 (electronic)\n | title = A geometric approach to betweenness\n | volume = 11\n | year = 1998}}.</ref> || 1/3&nbsp;+&nbsp;ε<ref>{{citation\n | last1 = Charikar | first1 = Moses | author1-link = Moses Charikar\n | last2 = Guruswami | first2 = Venkatesan | author2-link = Venkatesan Guruswami\n | last3 = Manokaran | first3 = Rajsekar\n | contribution = Every permutation CSP of arity 3 is approximation resistant\n | doi = 10.1109/CCC.2009.29\n | mr = 2932455\n | pages = 62–73\n | title = 24th Annual IEEE Conference on Computational Complexity\n | year = 2009}}.</ref>\n|}\n\n{{pull quote|text=Some very natural, intrinsically interesting statements about things like voting and foams just popped out of studying the UGC.... Even if the UGC turns out to be false, it has inspired a lot of interesting math research.|author=Ryan O’Donnell|source=<ref name=\"klarreich\"/>}}\nThe unique games conjecture was introduced by [[Subhash Khot]] in 2002 in order to make progress on certain questions in the theory of [[hardness of approximation]].\n\nThe truth of the unique games conjecture would imply the optimality of many known [[approximation algorithms]] (assuming '''P'''&nbsp;≠&nbsp;'''NP''').  For example, the approximation ratio achieved by the [[semidefinite programming#Examples|algorithm of Goemans and Williamson]] for approximating the [[maximum cut]] in a [[Graph (discrete mathematics)|graph]] is optimal to within any additive constant assuming the unique games conjecture and '''P'''&nbsp;≠&nbsp;'''NP'''.\n\nA list of results that the unique games conjecture is known to imply is shown in the adjacent table together with the corresponding best results for the weaker assumption P&nbsp;≠&nbsp;NP. A constant of ''c''&nbsp;+&nbsp;ε or ''c''&nbsp;&minus;&nbsp;ε means that the result holds for every ''constant'' (with respect to the problem size) strictly greater than or less than ''c'', respectively.\n\n==Discussion and alternatives==\nCurrently there is no consensus regarding the truth of the unique games conjecture.  Certain stronger forms of the conjecture have been disproved.\n\nA different form of the conjecture postulates that distinguishing the case when the value of a unique game is at least 1&nbsp;&minus;&nbsp;δ from the case when the value is at most ε is impossible for [[polynomial time|polynomial-time algorithms]] (but perhaps not NP-hard).  This form of the conjecture would still be useful for applications in hardness of approximation. On the other hand, distinguishing instances with value at most 3/8&nbsp;+&nbsp;δ from instances with value at least 1/2 is known to be NP-hard.<ref>{{citation\n | last1 = O'Donnell | first1 = Ryan\n | last2 = Wright | first2 = John\n | contribution = A new point of NP-hardness for unique games\n | doi = 10.1145/2213977.2214005\n | mr = 2961512\n | pages = 289–306\n | publisher = ACM | location = New York\n | title = Proceedings of the 2012 ACM Symposium on Theory of Computing (STOC'12)\n | year = 2012}}.</ref>\n\nThe constant δ&nbsp;>&nbsp;0 in the above formulations of the conjecture is necessary unless '''P'''&nbsp;=&nbsp;'''NP'''.  If the uniqueness requirement is removed the corresponding statement is known to be true by the [[parallel repetition theorem]], even when&nbsp;δ&nbsp;=&nbsp;0.\n\n[[Marek Karpinski]] and Warren Schudy<ref name=\"karpinskischudy\" /> constructed linear time approximation schemes for dense instances of unique games problem.\n\nIn 2008, Prasad Raghavendra has shown that if the UGC is true, then for every [[constraint satisfaction problem]] (CSP) the best approximation ratio is given by a certain simple [[semidefinite programming]] (SDP) instance, which is in particular polynomial\n[https://people.eecs.berkeley.edu/~prasad/Files/extabstract.pdf].\n\nIn 2010, Prasad Raghavendra and David Steurer defined the \"Gap-Small-Set Expansion\" problem, and conjectured that it is NP-hard. This conjecture implies the unique games conjecture.<ref>{{citation\n | last1 = Raghavendra | first1 = Prasad\n | last2 = Steurer | first2 = David\n | contribution = Graph expansion and the unique games conjecture\n | contribution-url = https://people.eecs.berkeley.edu/~prasad/Files/expansion.pdf\n | doi = 10.1145/1806689.1806792\n | mr = 2743325\n | pages = 755–764\n | publisher = ACM, New York\n | title = STOC'10—Proceedings of the 2010 ACM International Symposium on Theory of Computing\n | year = 2010}}</ref> It has also been used to prove strong [[hardness of approximation]] results for finding [[complete bipartite graph|complete bipartite subgraphs]].<ref>{{citation\n | last = Manurangsi | first = Pasin\n | editor1-last = Chatzigiannakis | editor1-first = Ioannis\n | editor2-last = Indyk | editor2-first = Piotr\n | editor3-last = Kuhn | editor3-first = Fabian\n | editor4-last = Muscholl | editor4-first = Anca\n | contribution = Inapproximability of Maximum Edge Biclique, Maximum Balanced Biclique and Minimum ''k''-Cut from the Small Set Expansion Hypothesis\n | doi = 10.4230/LIPIcs.ICALP.2017.79\n | isbn = 978-3-95977-041-5\n | location = Dagstuhl, Germany\n | pages = 79:1–79:14\n | publisher = Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik\n | series = Leibniz International Proceedings in Informatics (LIPIcs)\n | title = 44th International Colloquium on Automata, Languages, and Programming (ICALP 2017)\n | volume = 80\n | year = 2017}}</ref>\n\nIn 2010, [[Sanjeev Arora]], Boaz Barak and David Steurer found a subexponential time approximation algorithm for the unique games problem.<ref>{{citation\n | last1 = Arora | first1 = Sanjeev | author1-link = Sanjeev Arora\n | last2 = Barak | first2 = Boaz\n | last3 = Steurer | first3 = David\n | doi = 10.1145/2775105\n | issue = 5\n | journal = [[Journal of the ACM]]\n | mr = 3424199\n | page = Art. 42, 25\n | title = Subexponential algorithms for unique games and related problems\n | volume = 62\n | year = 2015}}. Previously announced at FOCS 2010.</ref>\n\nIn 2018, after a series of papers, a weaker version of the conjecture, called the 2-2 games conjecture, was proven. In a certain sense, this proves \"a half\" of the original conjecture \n[https://www.quantamagazine.org/computer-scientists-close-in-on-unique-games-conjecture-proof-20180424/],[https://windowsontheory.org/2018/01/10/unique-games-conjecture-halfway-there/].\n\n==Notes==\n{{reflist|30em|refs=\n<ref name=\"hastad99someoptimal\">{{Citation\n |author-link=Johan Håstad\n |first=Johan\n |last=Håstad\n |title=Some Optimal Inapproximability Results\n |journal=Journal of the ACM\n |year=1999\n |url=http://www.nada.kth.se/~johanh/optimalinap.ps\n |postscript=.\n |doi=10.1145/502090.502098\n}}</ref>\n<ref name=\"FG95approximating\">{{citation\n|first1=Uriel|last1=Feige\n|first2=Michel X.|last2=Goemans\n|contribution=Approximating the value of two prover proof systems, with applications to MAX 2SAT and MAX DICUT\n|title=Proc. 3rd Israel Symp. Theory of Computing and Systems\n|publisher=IEEE Computer Society Press\n|year=1995\n|pages=182–189\n}}</ref>\n<ref name=\"GW95improved\">{{citation\n |first1=Michel X.\n |last1=Goemans | author1-link = Michel Goemans\n |first2=David P.\n |last2=Williamson | author2-link = David P. Williamson\n |title=Improved Approximation Algorithms for Maximum Cut and Satisfiability Problems Using Semidefinite Programming\n |journal=Journal of the ACM\n |year=1995\n |doi= 10.1145/227683.227684\n}}</ref>\n<ref name=\"IS05onthehardness\">{{Citation\n | first1=Irit | last1=Dinur\n | first2=Samuel | last2=Safra | authorlink2=Shmuel Safra\n | title=On the hardness of approximating minimum vertex cover\n | journal=[[Annals of Mathematics]]\n | volume=162 | issue=1\n | pages=439–485\n | year=2005\n | doi=10.4007/annals.2005.162.439\n | url=http://www.wisdom.weizmann.ac.il/~dinuri/mypapers/vc.pdf\n | ref=harv\n | accessdate=2010-03-05\n | postscript=.\n}}</ref>\n<ref name=\"khot02onthepower\">{{citation\n | author-link = Subhash Khot\n | last = Khot | first = Subhash\n | contribution = On the power of unique 2-prover 1-round games\n | doi = 10.1145/509907.510017\n | pages = 767–775\n | title = Proceedings of the thirty-fourth annual ACM symposium on Theory of computing\n | isbn = 1-58113-495-9\n | year = 2002\n}}</ref>\n<ref name=\"KKMO07optimal\">{{citation\n | author1-link = Subhash Khot\n | last1 = Khot | first1 = Subhash\n | last2 = Kindler | first2 = Guy\n | last3 = Mossel | first3 = Elchanan\n | last4 = O'Donnell | first4 = Ryan\n | doi = 10.1137/S0097539705447372\n | issue = 1\n | journal = [[SIAM Journal on Computing]]\n | pages = 319–357\n | title = Optimal inapproximability results for MAX-CUT and other two-variable CSPs?\n | url = http://www.cs.cornell.edu/~abrahao/tdg/papers/KKMO-maxcut.pdf\n | volume = 37\n | year = 2007\n}}</ref>\n<ref name=\"KR03vertex\">{{citation\n | author1-link = Subhash Khot\n | first1 = Subhash | last1 = Khot\n | author2-link = Oded Regev\n | first2 = Oded | last2 = Regev\n | title = Vertex cover might be hard to approximate to within 2&nbsp;−&nbsp;''ε''\n | journal = IEEE Conference on Computational Complexity\n | year = 2003\n | pages = 379–\n}}</ref>\n<ref name=\"karpinskischudy\">{{citation\n | title=Linear time approximation schemes for the Gale–Berlekamp game and related minimization problems\n | last1=Karpinski | first1=Marek | last2=Schudy | first2=Warren\n | journal=Proceedings of the forty-first annual ACM symposium on Theory of computing\n | pages=313–322,\n | year=2009\n | doi=10.1145/1536414.1536458\n| arxiv=0811.3244}}\n</ref>\n}}\n\n==References==\n* {{citation\n | author-link = Subhash Khot\n | last = Khot | first = Subhash\n | contribution = On the Unique Games Conjecture\n | title = Proc. 25th IEEE Conference on Computational Complexity\n | url = http://cs.nyu.edu/~khot/papers/UGCSurvey.pdf\n | year = 2010\n | pages = 99–121\n | doi = 10.1109/CCC.2010.19\n}}.\n\n{{Computational hardness assumptions}}\n\n[[Category:2002 in computer science]]\n[[Category:Approximation algorithms]]\n[[Category:Computational complexity theory]]\n[[Category:Computational hardness assumptions]]\n[[Category:Unsolved problems in computer science]]\n[[Category:Conjectures]]"
    },
    {
      "title": "Vertex k-center problem",
      "url": "https://en.wikipedia.org/wiki/Vertex_k-center_problem",
      "text": "{{orphan|date=November 2018}}\n{{DISPLAYTITLE:Vertex ''k''-center problem}}\n\nThe '''vertex ''k''-center problem''' is a classical [[NP-hardness|NP-hard]] problem in [[computer science]]. It has application in [[Facility location problem|facility location]] and [[Cluster analysis|clustering]].<ref>{{Cite journal|last=Pacheco|first=Joaquín A.|last2=Casado|first2=Silvia|date=December 2005|title=Solving two location models with few facilities by using a hybrid heuristic: a real health resources case|journal=Computers & Operations Research|volume=32|issue=12|pages=3075–3091|doi=10.1016/j.cor.2004.04.009|issn=0305-0548}}</ref><ref>{{Cite journal|last=Kaveh|first=A.|last2=Nasr|first2=H.|date=August 2011|title=Solving the conditional and unconditional -center problem with modified harmony search: A real case study|journal=Scientia Iranica|volume=18|issue=4|pages=867–877|doi=10.1016/j.scient.2011.07.010|issn=1026-3098}}</ref> Basically, the vertex ''k''-center problem models the following real problem: given a city with <math>n</math> facilities, find the best <math>k</math> facilities where to build fire stations. Since firemen must attend any emergency as quickly as possible, the distance from the farthests facility to its nearest fire station has to be as small as possible. In other words, the position of the fire stations must be such that every possible fire is attended as quickly as possible.\n\n== Formal definition ==\nThe vertex ''k''-center problem is a classical [[NP-hardness|NP-Hard]] problem in [[computer science]]. It was first proposed by Hakimi in 1964.<ref>{{Cite journal|last=Hakimi|first=S. L.|date=1964|title=Optimum Locations of Switching Centers and the Absolute Centers and Medians of a Graph|journal=Operations Research|volume=12|issue=3|pages=450–459|jstor=168125|doi=10.1287/opre.12.3.450}}</ref> Formally, the vertex ''k''-center problem consists in: given a complete undirected [[Graph (discrete mathematics)|graph]] <math>G=(V,E)</math>  in a [[metric space]], and a positive integer <math>k</math>, find a subset <math>C \\subseteq V</math> such that <math>|C|\\le k</math>  and the objective function <math>r(C)=\\max_{v \\in V}\\{d(v,C)\\}</math>  is minimized. The distance <math>d(v,C)</math>  is defined as the distance from the vertex <math>v</math>  to its nearest center in <math>C</math>.\n\n==[[Approximation algorithm]]s==\nIf <math>P \\neq NP</math>, the vertex ''k''-center problem can not be (optimally) solved in polynomial time. However, there are some polynomial time algorithms that get near optimal solutions. Specifically, 2-approximated solutions. Actually, if <math>P \\neq NP</math> the best possible solution that can be achieved by a polynomial time algorithm is a 2-approximated one.<ref>{{Cite journal|last=Kariv|first=O.|last2=Hakimi|first2=S. L.|date=December 1979|title=An Algorithmic Approach to Network Location Problems. I: The p-Centers|journal=SIAM Journal on Applied Mathematics|volume=37|issue=3|pages=513–538|doi=10.1137/0137040|issn=0036-1399}}</ref><ref name=\"Gonzalez 1985\">{{Cite journal|last=Gonzalez|first=Teofilo F.|date=1985|title=Clustering to minimize the maximum intercluster distance|journal=Theoretical Computer Science|volume=38|pages=293–306|doi=10.1016/0304-3975(85)90224-5|issn=0304-3975}}</ref><ref name=\"Dyer 1985\">{{Cite journal|last=Dyer|first=M.E|last2=Frieze|first2=A.M|date=February 1985|title=A simple heuristic for the p-centre problem|journal=Operations Research Letters|volume=3|issue=6|pages=285–288|doi=10.1016/0167-6377(85)90002-1|issn=0167-6377}}</ref><ref name=\"Hochbaum 1985\">{{Cite journal|last=Hochbaum|first=Dorit S.|last2=Shmoys|first2=David B.|date=May 1985|title=A Best Possible Heuristic for the ''k''-Center Problem|journal=Mathematics of Operations Research|volume=10|issue=2|pages=180–184|doi=10.1287/moor.10.2.180|issn=0364-765X}}</ref> In the context of a minimization problem, such as the vertex ''k''-center problem, a 2-approximated solution is any solution <math>C'</math>  such that <math>r(C') \\le 2 \\times r(\\text{OPT})</math>, where <math>r(\\text{OPT})</math>  is an optimal solution. An algorithm that guarantees to generate 2-approximated solutions is known as a 2-approximation algorithm. The main 2-approximated algorithms for the vertex ''k''-center problem reported in the literature are the Sh algorithm,<ref name=\"Shmoys 1995\">{{Cite book|last=Shmoys|first=David B.|date=1995|title=Computing Near-Optimal Solutions to Combinatorial Optimization Problems|journal=In Combinatorial Optimization, Dimacs Series in Discrete Mathematics and Theoretical Computer Science|volume=20|pages=355––397|citeseerx=10.1.1.33.1719|doi=10.1090/dimacs/020/07|series=DIMACS Series in Discrete Mathematics and Theoretical Computer Science|isbn=9780821802397}}</ref> the HS algorithm,<ref name=\"Hochbaum 1985\" /> and the Gon algorithm.<ref name=\"Gonzalez 1985\" /><ref name=\"Dyer 1985\" /> Even though these algorithms are the (polynomial) best possible ones, their performance on most benchmark datasets is very deficient. Because of this, many [[Heuristic (computer science)|heuristics]] and [[metaheuristic]]s have been developed through the time. Contrary to common sense, one of the most practical (polynomial) heuristics for the vertex ''k''-center problem is based on the CDS algorithm, which is a 3-approximation algorithm<ref name=\"Garcia-Diaz 2017\">{{Cite journal|last=Garcia-Diaz|first=Jesus|last2=Sanchez-Hernandez|first2=Jairo|last3=Menchaca-Mendez|first3=Ricardo|last4=Menchaca-Mendez|first4=Rolando|date=2017-07-01|title=When a worse approximation factor gives better performance: a 3-approximation algorithm for the vertex ''k''-center problem|journal=Journal of Heuristics|volume=23|issue=5|pages=349–366|doi=10.1007/s10732-017-9345-x|issn=1381-1231}}</ref>\n\n=== The Sh algorithm ===\nFormally characterized by [[David Shmoys]] in 1995,<ref name=\"Shmoys 1995\" /> the Sh algorithm takes as input a complete undirected graph <math>G=(V,E)</math>, a positive integer <math>k</math>, and an assumption <math>r</math>  on what the optimal solution size is. The Sh algorithm works as follows: selects the first center <math>c_1</math>  at random. So far, the solution consists of only one vertex, <math>C=\\{c_1\\}</math>. Next, selects center <math>c_2</math>  at random from the set containing all the vertices whose distance from <math>C</math>  is greater than <math>2 \\times r</math>. At this point, <math>C=\\{c_1,c_2\\}</math>. Finally, selects the remaining <math>k-2</math>  centers the same way <math>c_2</math>  was selected. The complexity of the Sh algorithm is <math>O(kn)</math>, where <math>n</math> is the number of vertices.\n\n=== The HS algorithm ===\nProposed by [[Dorit S. Hochbaum|Dorit Hochbaum]] and [[David Shmoys]] in 1985, the HS algorithm takes the Sh algorithm as basis.<ref name=\"Hochbaum 1985\" /> By noticing that the value of <math>r(\\text{OPT})</math> must equals the cost of some edge in <math>E</math>, and since there are <math>O(n^2)</math> edges in <math>E</math>, the HS algorithm basically repeats the Sh algorithm with every edge cost. The complexity of the HS algorithm is <math>O(n^4)</math>. However, by running a [[Binary search algorithm|binary search]] over the ordered set of edge costs, its complexity is reduced to <math>O(n^2 \\log n)</math>.\n\n=== The Gon algorithm ===\nProposed independently by [[Teofilo F. Gonzalez|Teofilo Gonzalez]],<ref name=\"Gonzalez 1985\" /> and by Martin Dyer and [[Alan M. Frieze|Alan Frieze]]<ref name=\"Dyer 1985\" /> in 1985, the Gon algorithm is basically a more powerful version of the Sh algorithm. While the Sh algorithm requires a guess <math>r</math> on <math>r(\\text{OPT})</math>, the Gon algorithm prescinds from such guess by noticing that if any set of vertices at distance greater than <math>2 \\times r(\\text{OPT})</math> exists, then the farthest vertex must be inside such set. Therefore, instead of computing at each iteration the set of vertices at distance greater than <math>2 \\times r</math> and then selecting a random vertex, the Gon algorithm simply selects the farthest vertex from every partial solution <math>C'</math>. The complexity of the Gon algorithm is <math>O(kn)</math>, where <math>n</math> is the number of vertices.\n\n=== The CDS algorithm ===\nProposed by García Díaz et al. in 2017,<ref name=\"Garcia-Diaz 2017\" /> the CDS algorithm is a 3-approximation algorithm that takes ideas from the Gon algorithm (farthest point heuristic), the HS algorithm (parametric pruning), and the relationship between the vertex ''k''-center problem and the [[Dominating set|Dominating Set]] problem. The CDS algorithm has a complexity of <math>O(n^4)</math>. However, by performing a binary search over the ordered set of edge costs, a more efficiente heuristic named CDSh is proposed. The CDSh algorithm complexity is <math>O(n^2 \\log n)</math>. Despite the suboptimal performance of the CDS algorithm, and the heuristic performance of CDSh, both present a much better performance than the Sh, HS, and Gon algorithms.\n\n=== Experimental comparison ===\nSome of the most widely used benchmark datasets for the vertex ''k''-center problem are the pmed instances from OR-Lib.,<ref>{{Cite journal|last=Beasley|first=J. E.|date=1990|title=OR-Library: Distributing Test Problems by Electronic Mail|journal=The Journal of the Operational Research Society|volume=41|issue=11|pages=1069–1072|doi=10.2307/2582903|jstor=2582903}}</ref> and some instances from TSP-Lib.<ref>{{Cite journal|last=Reinelt|first=Gerhard|date=November 1991|title=TSPLIB—A Traveling Salesman Problem Library|journal=ORSA Journal on Computing|volume=3|issue=4|pages=376–384|doi=10.1287/ijoc.3.4.376|issn=0899-1499}}</ref> Table 1 shows the mean and standard deviation of the experimental approximation factors of the solutions generated by each algorithm over the 40 pmed instances from OR-Lib<ref name=\"Garcia-Diaz 2017\" />\n{| class=\"wikitable\"\n|+Table 1. Experimental approximation factor over pmed instances from OR-Lib\n!Algorithm\n!<math>\\mu</math>\n!<math>\\sigma</math>\n!Complexity\n|-\n|HS\n|1.532\n|0.175\n|<math>O(n^2 \\log n)</math>\n|-\n|Gon\n|1.503\n|0.122\n|<math>O(kn)</math>\n|-\n|CDSh\n|1.035\n|0.031\n|<math>O(n^2 \\log n)</math>\n|-\n|CDS\n|1.020\n|0.027\n|<math>O(n^4)</math>\n|}\n{| class=\"wikitable\"\n|+Table 2. Experimental approximation factor over instances from TSP-Lib\n!Algorithm\n!<math>\\mu</math>\n!<math>\\sigma</math>\n!Algorithm\n|-\n|Gon\n|1.396\n|0.091\n|<math>O(kn)</math>\n|-\n|HS\n|1.318\n|0.108\n|<math>O(n^2 \\log n)</math>\n|-\n|CDSh\n|1.124\n|0.065\n|<math>O(n^2 \\log n)</math>\n|-\n|CDS\n|1.042\n|0.038\n|<math>O(n^4)</math>\n|}\n\n== Polynomial heuristics ==\n\n=== Greedy pure algorithm ===\nThe greedy pure algorithm (or Gr) follows the core idea of [[greedy algorithm]]s: to take optimal local decisions. In the case of the vertex ''k''-center problem, the optimal local decision consists in selecting each center in such a way that the size of the solution (covering radius) is minimum at each iteration. In other words, the first center selected is the one that solves the 1-center problem. The second center selected is the one that, along with the previous center, generates a solution with minimum covering radius. The remaining centers are selected the same way. The complexity of the Gr algorithm is <math>O(kn^2)</math>.<ref>{{Cite book|last=Rana|first=Rattan|last2=Garg|first2=Deepak|date=March 2009|title=Heuristic Approaches for ''k''-Center Problem|journal=2009 IEEE International Advance Computing Conference|publisher=IEEE|doi=10.1109/iadcc.2009.4809031|isbn=9781424429271}}</ref> The empirical performance of the Gr algorithm is poor on most benchmark instances.\n\n=== Scoring algorithm ===\nThe Scoring algorithm (or Scr) was introduced by Jurij Mihelič and Borut Robič in 2005.<ref>{{Cite journal|last=Mihelič |first=Jurij |last2=Robič |first2=Borut |date=2005 |title=Solving the ''k''-center Problem Efficiently with a Dominating Set Algorithm |journal=Journal of Computing and Information Technology |volume=13 |issue=3 |pages=225 |doi=10.2498/cit.2005.03.05 |issn=1330-1136 }}</ref> This algorithm takes advantage of the reduction from the vertex ''k''-center problem to the minimum dominating set problem. The problem is solved by pruning the input graph with every possible value of the optimal solution size and then solving the minimum dominating set problem heuristically. This heuristic follows the ''lazy principle,'' which takes every decision as slow as possible (opossed to the greedy strategy). The complexity of the Scr algorithm is <math>O(n^4)</math>. The empirical performance of the Scr algorithm is very good on most benchmark instances. However, its running time rapidly becomes unpractical as the input grows. So, it seems to be a good algorithm only for small instances.\n\n== References ==\n<!-- Inline citations added to your article will automatically display here. See https://en.wikipedia.org/wiki/WP:REFB for instructions on how to add citations. -->\n{{reflist}}\n\n[[Category:NP-hard problems]]\n[[Category:Graph theory]]\n[[Category:Combinatorial optimization]]\n[[Category:Approximation algorithms]]"
    },
    {
      "title": "Approximation theory",
      "url": "https://en.wikipedia.org/wiki/Approximation_theory",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|Theory of getting acceptably close inexact mathematical calculations}}\nIn [[mathematics]], '''approximation theory''' is concerned with how [[function (mathematics)|function]]s can best be [[approximation|approximated]] with simpler [[function (mathematics)|functions]], and with [[Quantitative property|quantitative]]ly [[characterization (mathematics)|characterizing]] the [[approximation error|errors]] introduced thereby. Note that what is meant by ''best'' and ''simpler'' will depend on the application.\n\nA closely related topic is the approximation of functions by [[generalized Fourier series]], that is, approximations based upon summation of a series of terms based upon [[orthogonal polynomials]].\n\nOne problem of particular interest is that of approximating a function in a [[computer]] mathematical library, using operations that can be performed on the computer or calculator (e.g. addition and multiplication), such that the result is as close to the actual function as possible.  This is typically done with [[polynomial]] or [[Rational function|rational]] (ratio of polynomials) approximations.\n\nThe objective is to make the approximation as close as possible to the actual function, typically with an accuracy close to that of the underlying computer's [[floating point]] arithmetic.  This is accomplished by using a polynomial of high [[Degree of a polynomial|degree]], and/or narrowing the domain over which the polynomial has to approximate the function.\nNarrowing the domain can often be done through the use of various addition or scaling formulas for the function being approximated.  Modern mathematical libraries often reduce the domain into many tiny segments and use a low-degree polynomial for each segment.\n\n{|style=\"float:right\"\n| [[Image:Logerror.png|thumb|300px|Error between optimal polynomial and log(x) (red), and Chebyshev approximation and log(x) (blue) over the interval [2, 4].  Vertical divisions are 10<sup>−5</sup>.  Maximum error for the optimal polynomial is 6.07 x 10<sup>−5</sup>.]]\n| [[Image:Experror.png|thumb|300px|Error between optimal polynomial and exp(x) (red), and Chebyshev approximation and exp(x) (blue) over the interval [−1, 1].  Vertical divisions are 10<sup>−4</sup>.  Maximum error for the optimal polynomial is 5.47 x 10<sup>−4</sup>.]]\n|}\n\n==Optimal polynomials==\nOnce the domain (typically an interval) and degree of the polynomial are chosen, the polynomial itself is chosen in such a way as to minimize the worst-case error.  That is, the goal is to minimize the maximum value of <math>\\mid P(x)-f(x)\\mid</math>, where ''P''(''x'') is the approximating polynomial, ''f''(''x'') is the actual function, and ''x'' varies over the chosen interval. For well-behaved functions, there exists an ''N''<span style=\"padding-left:0.1em;\">th</span>-degree polynomial that will lead to an error curve that oscillates back and forth between <math>+\\varepsilon</math> and <math>-\\varepsilon</math> a total of ''N''+2 times, giving a worst-case error of <math>\\varepsilon</math>. It is seen that there exists an ''N''<span style=\"padding-left:0.1em;\">th</span>-degree polynomial can interpolate ''N''+1 points in a curve. Such a polynomial is always optimal. It is possible to make contrived functions ''f''(''x'') for which no such polynomial exists, but these occur rarely in practice.\n\nFor example, the graphs shown to the right show the error in approximating log(x) and exp(x) for ''N''&nbsp;=&nbsp;4. The red curves, for the optimal polynomial, are '''level''', that is, they oscillate between <math>+\\varepsilon</math> and <math>-\\varepsilon</math> exactly. Note that, in each case, the number of extrema is ''N''+2, that is, 6.  Two of the extrema are at the end points of the interval, at the left and right edges of the graphs.\n\n[[Image:Impossibleerror.png|thumb|right|300px|Error ''P''(''x'')&nbsp;&minus;&nbsp;''f''(''x'') for level polynomial (red), and for purported better polynomial (blue)]] To prove this is true in general, suppose ''P'' is a polynomial of degree ''N'' having the property described, that is, it gives rise to an error function that has ''N''&nbsp;+&nbsp;2 extrema, of alternating signs and equal magnitudes. The red graph to the right shows what this error function might look like for ''N''&nbsp;=&nbsp;4. Suppose ''Q''(''x'') (whose error function is shown in blue to the right) is another ''N''-degree polynomial that is a better approximation to ''f'' than ''P''. In particular, ''Q'' is closer to ''f'' than ''P'' for each value ''x<sub>i</sub>'' where an extreme of ''P''−''f'' occurs, so\n:<math>|Q(x_i)-f(x_i)|<|P(x_i)-f(x_i)|.</math>\nWhen a maximum of ''P''−''f'' occurs at ''x<sub>i</sub>'', then\n:<math>Q(x_i)-f(x_i)\\le|Q(x_i)-f(x_i)|<|P(x_i)-f(x_i)|=P(x_i)-f(x_i),</math>\nAnd when a minimum of ''P''−''f'' occurs at ''x<sub>i</sub>'', then\n:<math>f(x_i)-Q(x_i)\\le|Q(x_i)-f(x_i)|<|P(x_i)-f(x_i)|=f(x_i)-P(x_i).</math>\nSo, as can be seen in the graph, [''P''(''x'')&nbsp;−&nbsp;''f''(''x'')]&nbsp;−&nbsp;[''Q''(''x'')&nbsp;−&nbsp;''f''(''x'')] must alternate in sign for the ''N''&nbsp;+&nbsp;2 values of ''x<sub>i</sub>''. But [''P''(''x'')&nbsp;−&nbsp;''f''(''x'')]&nbsp;−&nbsp;[''Q''(''x'')&nbsp;−&nbsp;''f''(''x'')] reduces to ''P''(''x'')&nbsp;−&nbsp;''Q''(''x'') which is a polynomial of degree ''N''. This function changes sign at least ''N''+1 times so, by the [[Intermediate value theorem]], it has ''N''+1 zeroes, which is impossible for a polynomial of degree ''N''.\n\n==Chebyshev approximation==\nOne can obtain polynomials very close to the optimal one by expanding the given function in terms of [[Chebyshev polynomials]] and then cutting off the expansion at the desired degree.\nThis is similar to the [[Harmonic analysis|Fourier analysis]] of the function, using the Chebyshev polynomials instead of the usual trigonometric functions.\n\nIf one calculates the coefficients in the Chebyshev expansion for a function:\n\n:<math>f(x) \\sim \\sum_{i=0}^\\infty c_i T_i(x)</math>\n\nand then cuts off the series after the <math>T_N</math> term, one gets an ''N''<span style=\"padding-left:0.1em;\">th</span>-degree polynomial approximating ''f''(''x'').\n\nThe reason this polynomial is nearly optimal is that, for functions with rapidly converging power series, if the series is cut off after some term, the total error arising from the cutoff is close to the first term after the cutoff.  That is, the first term after the cutoff dominates all later terms.  The same is true if the expansion is in terms of Chebyshev polynomials.  If a Chebyshev expansion is cut off after <math>T_N</math>, the error will take a form close to a multiple of <math>T_{N+1}</math>.  The Chebyshev polynomials have the property that they are level – they oscillate between +1 and −1 in the interval [−1, 1].  <math>T_{N+1}</math> has ''N''+2 level extrema.  This means that the error between ''f''(''x'') and its Chebyshev expansion out to <math>T_N</math> is close to a level function with ''N''+2 extrema, so it is close to the optimal ''N''<span style=\"padding-left:0.1em;\">th</span>-degree polynomial.\n\nIn the graphs above, note that the blue error function is sometimes better than (inside of) the red function, but sometimes worse, meaning that it is not quite the optimal polynomial.  Note also that the discrepancy is less serious for the exp function, which has an extremely rapidly converging power series, than for the log function.\n\nChebyshev approximation is the basis for [[Clenshaw–Curtis quadrature]], a [[numerical integration]] technique.\n\n==Remez's algorithm==\n\nThe [[Remez algorithm]] (sometimes spelled Remes) is used to produce an optimal polynomial ''P''(''x'') approximating a given function ''f''(''x'') over a given interval.  It is an iterative algorithm that converges to a polynomial that has an error function with ''N''+2 level extrema.  By the theorem above, that polynomial is optimal.\n\nRemez's algorithm uses the fact that one can construct an ''N''<span style=\"padding-left:0.1em;\">th</span>-degree polynomial that leads to level and alternating error values, given ''N''+2 test points.\n\nGiven ''N''+2 test points <math>x_1</math>, <math>x_2</math>, ... <math>x_{N+2}</math> (where <math>x_1</math> and <math>x_{N+2}</math> are presumably the end points of the interval of approximation), these equations need to be solved:\n\n:<math>P(x_1) - f(x_1) = + \\varepsilon\\,</math>\n:<math>P(x_2) - f(x_2) = - \\varepsilon\\,</math>\n:<math>P(x_3) - f(x_3) = + \\varepsilon\\,</math>\n:<math>\\vdots</math>\n:<math>P(x_{N+2}) - f(x_{N+2}) = \\pm \\varepsilon.\\,</math>\n\nThe right-hand sides alternate in sign.\n\nThat is,\n\n:<math>P_0 + P_1 x_1 + P_2 x_1^2 + P_3 x_1^3 + \\dots + P_N x_1^N - f(x_1) = + \\varepsilon\\,</math>\n:<math>P_0 + P_1 x_2 + P_2 x_2^2 + P_3 x_2^3 + \\dots + P_N x_2^N - f(x_2) = - \\varepsilon\\,</math>\n:<math>\\vdots</math>\n\nSince <math>x_1</math>, ..., <math>x_{N+2}</math> were given, all of their powers are known, and <math>f(x_1)</math>, ..., <math>f(x_{N+2})</math> are also known.  That means that the above equations are just ''N''+2 linear equations in the ''N''+2 variables <math>P_0</math>, <math>P_1</math>, ..., <math>P_N</math>, and <math>\\varepsilon</math>.  Given the test points <math>x_1</math>, ..., <math>x_{N+2}</math>, one can solve this system to get the polynomial ''P'' and the number <math>\\varepsilon</math>.\n\nThe graph below shows an example of this, producing a fourth-degree polynomial approximating <math>e^x</math> over [−1, 1].  The test points were set at\n−1, −0.7, −0.1, +0.4, +0.9, and 1.  Those values are shown in green.  The resultant value of <math>\\varepsilon</math> is 4.43 x 10<sup>−4</sup>\n\n[[Image:Remesdemo.png|thumb|center|300px|Error of the polynomial produced by the first step of Remez's algorithm, approximating e<sup>x</sup> over the interval [−1, 1].  Vertical divisions are 10<sup>−4</sup>.]]\n\nNote that the error graph does indeed take on the values <math>\\pm \\varepsilon</math> at the six test points, including the end points, but that those points are not extrema.  If the four interior test points had been extrema (that is, the function ''P''(''x'')''f''(''x'') had maxima or minima there), the polynomial would be optimal.\n\nThe second step of Remez's algorithm consists of moving the test points to the approximate locations where the error function had its actual local maxima or minima.  For example, one can tell from looking at the graph that the point at −0.1 should have been at about −0.28.  The way to do this in the algorithm is to use a single round of [[Newton's method]].  Since one knows the first and second derivatives of ''P''(''x'')−''f''(''x''), one can calculate approximately how far a test point has to be moved so that the derivative will be zero.\n\nCalculating the derivatives of a polynomial is straightforward.  One must also be able to calculate the first and second derivatives of ''f''(''x'').  Remez's algorithm requires an ability to calculate <math>f(x)\\,</math>, <math>f'(x)\\,</math>, and <math>f''(x)\\,</math> to extremely high precision.  The entire algorithm must be carried out to higher precision than the desired precision of the result.\n\nAfter moving the test points, the linear equation part is repeated, getting a new polynomial, and Newton's method is used again to move the test points again.  This sequence is continued until the result converges to the desired accuracy.  The algorithm converges very rapidly.\nConvergence is quadratic for well-behaved functions—if the test points are within <math>10^{-15}</math> of the correct result, they will be approximately within <math>10^{-30}</math> of the correct result after the next round.\n\nRemez's algorithm is typically started by choosing the extrema of the Chebyshev polynomial <math>T_{N+1}</math> as the initial points, since the final error function will be similar to that polynomial.\n\n==Main journals==\n\n* [[Journal of Approximation Theory]]\n* [[Constructive Approximation]]\n* [[East Journal on Approximations]]\n\n==See also==\n*[[Chebyshev polynomials]]\n*[[Estimation theory]]\n*[[Generalized Fourier series]]\n*[[Orthogonal polynomials]]\n*[[Orthonormal basis]]\n*[[Fourier series]]\n*[[Schauder basis]]\n*[[Padé approximant]]\n*[[Function approximation]]\n\n==References==\n* N. I. Achiezer (Akhiezer), Theory of approximation, Translated by Charles J. Hyman Frederick Ungar Publishing Co., New York 1956 x+307 pp.\n* A. F. Timan, ''Theory of approximation of functions of a real variable'', 1963 {{ISBN|0-486-67830-X}}\n* C. Hastings, Jr. ''Approximations for Digital Computers''. Princeton University Press, 1955.\n* J. F. Hart, E. W. Cheney, C. L. Lawson, H. J. Maehly, C. K. Mesztenyi, [[John R. Rice (professor)|J. R. Rice]], H. C. Thacher Jr., C. Witzgall, ''Computer Approximations''. Wiley, 1968, Lib. Cong. 67-23326.\n* L. Fox and I. B. Parker. \"Chebyshev Polynomials in Numerical Analysis.\" Oxford University Press London, 1968.\n* {{Citation | last1=Press | first1=WH | last2=Teukolsky | first2=SA | last3=Vetterling | first3=WT | last4=Flannery | first4=BP | year=2007 | title=Numerical Recipes: The Art of Scientific Computing | edition=3rd | publisher=Cambridge University Press |  publication-place=New York | isbn=978-0-521-88068-8 | chapter=Section 5.8. Chebyshev Approximation | chapter-url=http://apps.nrbook.com/empanel/index.html?pg=233}}\n* W. J. Cody Jr., W. Waite, ''Software Manual for the Elementary Functions''. Prentice-Hall, 1980, {{ISBN|0-13-822064-6}}.\n* E. Remes [Remez], \"Sur le calcul effectif des polynomes d'approximation de Tschebyscheff\". 1934 ''C. R. Acad. Sci.'', Paris, '''199''', 337-340.\n* K.-G. Steffens, \"The History of Approximation Theory: From Euler to Bernstein,\" Birkhauser, Boston 2006 {{ISBN|0-8176-4353-2}}.\n* [[Tamas Erdelyi (mathematician)|T. Erdélyi]], \"Extensions of the Bloch-Pólya theorem on the number of distinct real zeros of polynomials\", ''Journal de théorie des nombres de Bordeaux'' '''20''' (2008), 281–287.\n* T. Erdélyi, \"The Remez inequality for linear combinations of shifted Gaussians\", ''Math. Proc. Camb. Phil. Soc.'' '''146''' (2009), 523–530.\n* L. N. Trefethen, \"Approximation theory and approximation practice\", SIAM 2013. [https://web.archive.org/web/20131002133817/http://www2.maths.ox.ac.uk/chebfun/ATAP/]\n\n==External links==\n*[http://www.math.technion.ac.il/hat/ History of Approximation Theory (HAT)]\n*[http://www.emis.de/journals/SAT/ Surveys in Approximation Theory (SAT)]\n\n[[Category:Approximation theory|*]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Baskakov operator",
      "url": "https://en.wikipedia.org/wiki/Baskakov_operator",
      "text": "In [[functional analysis]], a branch of [[mathematics]], the '''Baskakov operators''' are generalizations of [[Bernstein polynomials]], [[Szász–Mirakyan operator]]s, and [[Lupas operator]]s. They are defined by\n:<math>[\\mathcal{L}_n(f)](x) = \\sum_{k=0}^\\infty {(-1)^k \\frac{x^k}{k!} \\phi_n^{(k)}(x) f\\left(\\frac{k}{n}\\right)}</math>\nwhere <math>x\\in[0,b)\\subset\\mathbb{R}</math> (<math>b</math> can be <math>\\infty</math>), <math>n\\in\\mathbb{N}</math>, and <math>(\\phi_n)_{n\\in\\mathbb{N}}</math> is a sequence of functions defined on <math>[0,b]</math> that have the following properties for all <math>n,k\\in\\mathbb{N}</math>:\n#<math>\\phi_n\\in\\mathcal{C}^\\infty[0,b]</math>. Alternatively, <math>\\phi_n</math> has a [[Taylor series]] on <math>[0,b)</math>.\n#<math>\\phi_n(0) = 1</math>\n#<math>\\phi_n</math> is completely monotone, i.e. <math>(-1)^k\\phi_n^{(k)}\\geq 0</math>.\n#There is an integer <math>c</math> such that <math>\\phi_n^{(k+1)} = -n\\phi_{n+c}^{(k)}</math> whenever <math>n>\\max\\{0,-c\\}</math>\nThey are named after V. A. Baskakov, who studied their convergence to bounded, continuous functions.<ref name=\"Agrawal\">{{cite encyclopedia|last=Agrawal|first=P. N.|editor=[[Michiel Hazewinkel]]|year=2001|title=Baskakov operators|encyclopedia=Encyclopaedia of Mathematics|publisher=Springer|isbn=1-4020-0609-8|url=http://eom.springer.de/b/b110150.htm}}</ref>\n\n==Basic results==\nThe Baskakov operators are linear and positive.<ref name=\"Agrawal2\">{{cite encyclopedia|last=Agrawal|first=P. N.|author2=T. A. K. Sinha|editor=[[Michiel Hazewinkel]]|year=2001|title=Bernstein–Baskakov–Kantorovich operator|encyclopedia=Encyclopaedia of Mathematics|publisher=Springer|isbn=1-4020-0609-8|url=http://eom.springer.de/b/b110350.htm}}</ref>\n\n==References==\n*{{cite journal| last=Baskakov | first=V. A. | year=1957 | script-title=ru:Пример последовательности линейных положительных операторов в пространстве непрерывных функций |trans-title=An example of a sequence of linear positive operators in the space of continuous functions | journal=[[Doklady Akademii Nauk SSSR]] | language=Russian | volume=113 | pages=249–251}}\n\n===Footnotes===\n<references/>\n\n{{mathanalysis-stub}}\n[[Category:Approximation theory]]"
    },
    {
      "title": "Bernstein's theorem (polynomials)",
      "url": "https://en.wikipedia.org/wiki/Bernstein%27s_theorem_%28polynomials%29",
      "text": "{{See also|Bernstein's inequality (mathematical analysis)}}\n'''Bernstein's theorem''' is an inequality relating the maximum modulus of a complex [[polynomial]] function on the unit disk with the maximum modulus of its [[derivative]] on the unit disk. It was proven by [[Sergei Bernstein]] while he was working on [[approximation theory]].<ref>R. P. Boas, Jr., Inequalities for the derivatives of polynomials, Math. Mag. 42 (1969), 165–174. </ref>\n\n== Statement ==\nLet <math>\\max_{|z| = 1} |f(z)|</math> denote the maximum modulus of an arbitrary\nfunction <math>f(z)</math> on <math>|z|=1</math>, and let <math>f'(z)</math> denote its derivative.\nThen for every polynomial <math>P(z)</math> of degree <math>n</math> we have\n\n: <math>\\max_{|z| = 1} |P'(z)| \\le n \\max_{|z| = 1} |P(z)|</math>.\n\nThe inequality is best possible with equality holding if and only if\n\n: <math> P(z) =  \\alpha z^n,\\ |\\alpha| = \\max_{|z| = 1} |P(z)|</math>.\n<ref>M. A. Malik, M. C. Vong, Inequalities concerning the derivative of polynomials, Rend. Circ. Mat. Palermo (2) 34 (1985), 422–426.</ref>\n\n=== Proof ===\nLet <math>P(z)</math> be a polynomial of degree <math>n</math>, and let <math>Q(z)</math> be another polynomial of the same degree with no zeros in <math>|z| \\ge 1</math>. We show first that if <math>|P(z)| < |Q(z)|</math> on <math>|z| = 1</math>, then <math>|P'(z)| < |Q'(z)|</math> on <math>|z| \\ge 1</math>. \n\nBy [[Rouché's theorem]], <math>P(z) + \\varepsilon\\ Q(z)</math> with <math>|\\varepsilon| \\geq 1</math> has all\nits zeros in <math>|z| < 1</math>. By virtue of the [[Gauss–Lucas theorem]],\n<math>P'(z) + \\varepsilon\\ Q'(z)</math> has all its zeros in <math>|z| < 1</math> as well.\nIt follows that <math>|P'(z)| < |Q'(z)|</math> on <math>|z| \\geq 1</math>,\notherwise we could choose an <math>\\varepsilon</math> with <math>|\\varepsilon| \\geq 1</math> such that\n<math>P'(z) + \\varepsilon Q'(z)</math> has a zero in <math>|z|\\geq 1</math>.\n\nFor an arbitrary polynomial <math>P(z)</math> of degree <math>n</math>, we obtain Bernstein's Theorem by applying the above result to the polynomials <math>Q(z) =  C z^n</math>, where <math>C</math> is an arbitrary constant exceeding <math>\\max_{|z|=1}|P(z)|</math>.\n\n== Similar results ==\n[[Paul Erdős]] conjectured that if <math>P(z)</math> has no zeros in <math>|z|<1</math>, then <math>\\max_{|z| = 1} |P'(z)| \\le \\frac{n}{2} \\max_{|z| = 1} |P(z)|</math>. This was proved by [[Peter Lax]].<ref>P. D. Lax, Proof of a conjecture of P. Erdös on the derivative of a polynomial, Bull. Amer. Math. Soc. 50 (1944), 509–513.</ref>\n\nM. A. Malik showed that if <math>P(z)</math> has no zeros in <math>|z|<k</math> for a given <math>k \\ge 1</math>, then <math>\\max_{|z| = 1} |P'(z)| \\le \\frac{n}{1+k} \\max_{|z| = 1}|P(z)|</math>.<ref>M. A. Malik, On the derivative of a polynomial J. London Math. Soc (2) 1 (1969), 57–60.</ref>\n\n== References ==\n{{Reflist}}\n\n[[Category:Approximation theory]]\n[[Category:Polynomials]]"
    },
    {
      "title": "Bramble–Hilbert lemma",
      "url": "https://en.wikipedia.org/wiki/Bramble%E2%80%93Hilbert_lemma",
      "text": "In [[mathematics]], particularly [[numerical analysis]], the '''Bramble–Hilbert [[lemma (mathematics)|lemma]]''', named after [[James H. Bramble]] and [[Stephen Hilbert]], bounds the [[approximation error|error]] of an [[approximation]] of a [[function (mathematics)|function]] <math>\\textstyle u</math> by a [[polynomial]] of order at most <math>\\textstyle m-1</math> in terms of [[derivative (mathematics)|derivatives]] of <math>\\textstyle u</math> of order <math>\\textstyle m</math>. Both the error of the approximation and the derivatives of <math>\\textstyle u</math> are measured by [[Lp space|<math>\\textstyle L^{p}</math> norms]] on a [[Bounded set|bounded]] [[Domain (mathematical analysis)|domain]] in <math>\\textstyle \\mathbb{R}^{n}</math>. This is similar to classical numerical analysis, where, for example, the error of [[linear interpolation]] <math>\\textstyle u</math> can be bounded using the second derivative of <math>\\textstyle u</math>. However, the Bramble–Hilbert lemma applies in any number of dimensions, not just one dimension, and the approximation error and the derivatives of <math>\\textstyle u</math> are measured by more general norms involving averages, not just the [[maximum norm]].\n\nAdditional assumptions on the domain are needed for the Bramble–Hilbert lemma to hold. Essentially, the [[Boundary (topology)|boundary]] of the domain must be \"reasonable\". For example, domains that have a spike or a slit with zero angle at the tip are excluded. [[Lipschitz domain]]s are reasonable enough, which includes [[Convex set|convex]] domains and domains with [[continuously differentiable]] boundary. \n\nThe main use of the Bramble–Hilbert lemma is to prove bounds on the error of interpolation of function <math>\\textstyle u</math> by an operator that preserves polynomials of order up to <math>\\textstyle m-1</math>, in terms of the derivatives of <math>\\textstyle u</math> of order <math>\\textstyle m</math>. This is an essential step in error estimates for the [[finite element method]]. The Bramble–Hilbert lemma is applied there on the domain consisting of one element (or, in some [[superconvergence]] results, a small number of elements).\n\n==The one-dimensional case==\n\nBefore stating the lemma in full generality, it is useful to look at some simple special cases. In one dimension and for a function <math>\\textstyle u</math> that has <math>\\textstyle m</math> derivatives on interval <math>\\textstyle \\left(  a,b\\right)  </math>, the lemma reduces to\n\n:<math> \\inf_{v\\in P_{m-1}}\\bigl\\Vert u^{\\left(  k\\right)  }-v^{\\left(  k\\right) }\\bigr\\Vert_{L^{p}\\left(  a,b\\right)  }\\leq C\\left(  m\\right)  \\left( b-a\\right)  ^{m-k}\\bigl\\Vert u^{\\left(  m\\right)  }\\bigr\\Vert_{L^{p}\\left( a,b\\right)  }, </math>\n\nwhere <math>\\textstyle P_{m-1}</math> is the space of all polynomials of order at most <math>\\textstyle m-1</math>.\n\nIn the case when <math>\\textstyle p=\\infty</math>, <math>\\textstyle m=2</math>, <math>\\textstyle k=0</math>, and <math>\\textstyle u</math> is twice differentiable, this means that there exists a polynomial <math>\\textstyle v</math> of degree one such that for all <math>\\textstyle x\\in\\left(  a,b\\right)  </math>,\n\n:<math> \\left\\vert u\\left(  x\\right)  -v\\left(  x\\right)  \\right\\vert \\leq C\\left( b-a\\right)  ^{2}\\sup_{\\left(  a,b\\right)  }\\left\\vert u^{\\prime\\prime }\\right\\vert. </math>\n\nThis inequality also follows from the well-known error estimate for linear interpolation by choosing <math>\\textstyle v</math> as the linear interpolant of <math>\\textstyle u</math>.\n\n==Statement of the lemma==\n{{Dubious|Dependence of the constant|date=October 2011}}\nSuppose <math>\\textstyle \\Omega</math> is a bounded domain in <math>\\textstyle \\mathbb{R}^n</math>, <math>\\textstyle n\\geq1</math>, with boundary <math>\\textstyle \\partial\\Omega</math> and [[diameter]] <math>\\textstyle d</math>. <math>\\textstyle W_p^k(\\Omega)</math> is the [[Sobolev space]] of all function <math>\\textstyle u</math> on <math>\\textstyle \\Omega</math> with [[weak derivative]]s <math>\\textstyle D^\\alpha u</math> of order <math>\\textstyle \\left\\vert \\alpha\\right\\vert </math> up to <math>\\textstyle k</math> in <math>\\textstyle L^p(\\Omega)</math>. Here, <math>\\textstyle \\alpha=\\left(  \\alpha_1,\\alpha_2,\\ldots,\\alpha_n\\right)  </math> is a [[multiindex]], <math>\\textstyle \\left\\vert \\alpha\\right\\vert =</math> <math>\\textstyle \\alpha_1+\\alpha_2+\\cdots+\\alpha_n</math> and <math>\\textstyle D^\\alpha</math> denotes the derivative <math>\\textstyle \\alpha_1</math> times with respect to <math>\\textstyle x_1</math>, <math>\\textstyle \\alpha_2</math> times with respect to <math>\\textstyle x_2</math>, and so on. The Sobolev seminorm on <math>\\textstyle W_p^m(\\Omega)</math> consists of the <math>\\textstyle L^p</math> norms of the highest order derivatives,\n\n:<math> \\left\\vert u\\right\\vert _{W_p^m(\\Omega)}=\\left(  \\sum_{\\left\\vert \\alpha\\right\\vert =m}\\left\\Vert D^\\alpha  u\\right\\Vert_{L^p(\\Omega)}^p\\right)  ^{1/p}\\text{ if }1\\leq p<\\infty </math>\n\nand\n\n:<math> \\left\\vert u\\right\\vert _{W_\\infty^{m}(\\Omega)}=\\max_{\\left\\vert \\alpha\\right\\vert =m}\\left\\Vert D^{\\alpha}u\\right\\Vert _{L^\\infty(\\Omega)}</math>\n\n<math>\\textstyle P_k</math> is the space of all polynomials of order up to <math>\\textstyle k</math> on <math>\\textstyle \\mathbb{R}^n</math>. Note that <math>\\textstyle D^{\\alpha}v=0</math> for all <math>\\textstyle v\\in P_{m-1}</math> and <math>\\textstyle \\left\\vert \\alpha\\right\\vert =m</math>, so <math>\\textstyle \\left\\vert u+v\\right\\vert _{W_p^m(\\Omega)}</math> has the same value for any <math>\\textstyle v\\in P_{m-1}</math>.\n\n'''Lemma''' (Bramble and Hilbert) Under additional assumptions on the domain <math>\\textstyle \\Omega</math>, specified below, there exists a constant <math>\\textstyle C=C\\left( m,\\Omega\\right)  </math> independent of <math>\\textstyle p</math> and <math>\\textstyle u</math> such that for any <math>\\textstyle u\\in W_p^m(\\Omega)</math> there exists a polynomial <math>\\textstyle v\\in P_{m-1}</math> such that for all <math>\\textstyle k=0,\\ldots,m,</math>\n\n:<math> \\left\\vert u-v\\right\\vert _{W_p^k(\\Omega)}\\leq Cd^{m-k}\\left\\vert u\\right\\vert _{W_p^m(\\Omega)}. </math>\n\n==The original result==\n\nThe lemma was proved by Bramble and Hilbert <ref name=\"Bramble-1970-ELF\">J. H. Bramble and S. R. Hilbert. Estimation of linear functionals on Sobolev spaces with application to Fourier transforms and spline interpolation. ''SIAM J. Numer. Anal.'', 7:112–124, 1970.\n\n</ref> under the assumption that <math>\\textstyle \\Omega</math> satisfies the [[strong cone property]]; that is, there exists a finite open covering <math>\\textstyle \\left\\{  O_{i}\\right\\}  </math> of <math>\\textstyle \\partial\\Omega</math> and corresponding cones <math>\\textstyle \\{C_{i}\\}</math> with vertices at the origin such that <math>\\textstyle x+C_{i}</math> is contained in <math>\\textstyle \\Omega</math> for any <math>\\textstyle x</math> <math>\\textstyle \\in\\Omega\\cap O_{i}</math>.\n\nThe statement of the lemma here is a simple rewriting of the right-hand inequality stated in Theorem 1 in.<ref name=\"Bramble-1970-ELF\"/> The actual statement in <ref name=\"Bramble-1970-ELF\"/> is that the norm of the factorspace <math>\\textstyle W_{p}^{m}(\\Omega)/P_{m-1}</math> is equivalent to the <math>\\textstyle W_{p}^{m}(\\Omega)</math> seminorm. The <math>\\textstyle W_{p}^{m}(\\Omega)</math> norm is not the usual one but the terms are scaled with <math>\\textstyle d</math> so that the right-hand inequality in the equivalence of the seminorms comes out exactly as in the statement here.\n\nIn the original result, the choice of the polynomial is not specified, and the value of constant and its dependence on the domain <math>\\textstyle \\Omega</math> cannot be determined from the proof.\n\n==A constructive form==\n\nAn alternative result was given by Dupont and Scott <ref name=\"Dupont-1980-PAF\">Todd Dupont and Ridgway Scott. Polynomial approximation of functions in Sobolev spaces. ''Math. Comp.'', 34(150):441–463, 1980.</ref> under the assumption that the domain <math>\\textstyle \\Omega</math> is [[star-shaped]]; that is, there exists a ball <math>\\textstyle B</math> such that for any <math>\\textstyle x\\in\\Omega</math>, the closed [[convex hull]] of <math>\\textstyle \\left\\{  x\\right\\}  \\cup B</math> is a subset of <math>\\textstyle \\Omega</math>. Suppose that <math>\\textstyle \\rho _\\max</math> is the supremum of the diameters of such balls. The ratio <math>\\textstyle \\gamma=d/\\rho_\\max</math> is called the chunkiness of <math>\\textstyle \\Omega</math>.\n\nThen the lemma holds with the constant <math>\\textstyle C=C\\left(  m,n,\\gamma\\right)  </math>, that is, the constant depends on the domain <math>\\textstyle \\Omega</math> only through its chunkiness <math>\\textstyle \\gamma</math> and the dimension of the space <math>\\textstyle n</math>. In addition, <math>v</math> can be chosen as <math>v=Q^m u</math>, where <math>\\textstyle Q^m u</math> is the averaged [[Taylor polynomial]], defined as\n\n:<math> Q^{m}u=\\int_B T_y^mu\\left(  x\\right)  \\psi\\left(  y\\right) \\, dx, </math>\n\nwhere\n\n:<math> T_y^m u\\left(  x\\right)  =\\sum\\limits_{k=0}^{m-1}\\sum\\limits_{\\left\\vert \\alpha\\right\\vert =k}\\frac{1}{\\alpha!}D^\\alpha u\\left(  y\\right)  \\left( x-y\\right)^\\alpha</math>\n\nis the Taylor polynomial of degree at most <math>\\textstyle m-1</math> of <math>\\textstyle u</math> centered at <math>\\textstyle y</math> evaluated at <math>\\textstyle x</math>, and <math>\\textstyle \\psi\\geq0</math> is a function that has derivatives of all orders, equals to zero outside of <math>\\textstyle B</math>, and such that\n\n:<math> \\int_B\\psi \\, dx=1. </math>\n\nSuch function <math>\\textstyle \\psi</math> always exists.\n\nFor more details and a tutorial treatment, see the monograph by [[Susanne Brenner|Brenner]] and Scott.<ref name=\"Brenner-2002-MTF\">[[Susanne Brenner|Susanne C. Brenner]] and L. Ridgway Scott. ''The mathematical theory of finite element methods'', volume 15 of ''Texts in Applied Mathematics''. Springer-Verlag, New York, second edition, 2002. {{ISBN|0-387-95451-1}}\n\n</ref> The result can be extended to the case when the domain <math>\\textstyle \\Omega</math> is the union of a finite number of star-shaped domains, which is slightly more general than the strong cone property, and other polynomial spaces than the space of all polynomials up to a given degree.<ref name=\"Dupont-1980-PAF\"/>\n\n==Bound on linear functionals==\n\nThis result follows immediately from the above lemma, and it is also called sometimes the Bramble–Hilbert lemma, for example by [[Philippe G. Ciarlet|Ciarlet]].<ref name=\"Ciarlet-2002-FEM\">[[Philippe G. Ciarlet]]. ''The finite element method for elliptic problems'', volume 40 of ''Classics in Applied Mathematics''. Society for Industrial and Applied Mathematics (SIAM), Philadelphia, PA, 2002. Reprint of the 1978 original [North-Holland, Amsterdam]. {{ISBN|0-89871-514-8}}\n\n</ref> It is essentially Theorem 2 from.<ref name=\"Bramble-1970-ELF\"/>\n\n'''Lemma''' Suppose that <math>\\textstyle \\ell</math> is a [[continuous linear functional]] on <math>\\textstyle W_{p}^{m}(\\Omega)</math> and <math>\\textstyle \\left\\Vert \\ell\\right\\Vert _{W_{p}^{m}(\\Omega )^{^{\\prime}}}</math> its [[dual norm]]. Suppose that <math>\\textstyle \\ell\\left(  v\\right)  =0</math> for all <math>\\textstyle v\\in P_{m-1}</math>. Then there exists a constant <math>\\textstyle C=C\\left(  \\Omega\\right)  </math> such that\n\n:<math> \\left\\vert \\ell\\left(  u\\right)  \\right\\vert \\leq C\\left\\Vert \\ell\\right\\Vert _{W_{p}^{m}(\\Omega)^{^{\\prime}}}\\left\\vert u\\right\\vert _{W_{p}^{m}(\\Omega)}. </math>\n\n==References==\n<!-- this 'empty' section displays references defined elsewhere -->\n{{reflist}}\n\n==External links==\n* {{Springer|id=B/b130220|author=Raytcho D. Lazarov|title=Bramble–Hilbert lemma}}\n* https://arxiv.org/abs/0710.5148 – Jan Mandel: The Bramble–Hilbert Lemma\n\n{{DEFAULTSORT:Bramble-Hilbert Lemma}}\n[[Category:Lemmas]]\n[[Category:Approximation theory]]\n[[Category:Finite element method]]"
    },
    {
      "title": "Elliott Ward Cheney Jr.",
      "url": "https://en.wikipedia.org/wiki/Elliott_Ward_Cheney_Jr.",
      "text": "{{Infobox scientist\n|name              = Elliott Ward Cheney Jr.\n|image             = Elliott-Ward-Cheney-Jr.jpg\n|caption           =\n|birth_date        = {{Birth date|1929|06|28}}\n|birth_place       = [[Gettysburg, Pennsylvania]]\n|death_date        = {{Death date and age|2016|07|13|1929|06|28}} \n|workplaces        = [[University of Texas at Austin]]\n|alma_mater        = [[Lehigh University]]<br />[[University of Kansas]]\n|doctoral_advisor  = [[Robert Schatten]]\n|known_for         = [[approximation theory]] research, mathematics textbooks\n}}\n'''Elliott Ward Cheney Jr.''' (June 28, 1929 – July 13, 2016) was an American mathematician and an Emeritus Professor at the [[University of Texas at Austin]].<ref name=\"UTemeritus\">{{Cite web |url=http://www.ma.utexas.edu/people/emeritus_faculty |title=Emeritus Faculty, UT Austin |access-date=2013-01-01 |archive-url=https://web.archive.org/web/20130102010355/http://www.ma.utexas.edu/people/emeritus_faculty/ |archive-date=2013-01-02 |dead-url=yes }}</ref> Known to his friends and colleagues as '''Ward Cheney''', he was one of the pioneers in the fields of approximation theory and numerical analysis.<ref name=\"segenealogy\">[http://people.engr.ncsu.edu/txie/sefamily.htm Software Engineering Academic Genealogy]</ref>  His 1966 book, ''[[An Introduction to Approximation Theory]]'', remains in print and is \"highly respected and well known\", \"a small book almost encyclopedic in character\", and \"is a classic with few competitors\".<ref>[http://www.ams.org/bookstore-getitem/item=CHEL-317-H ''Mathematical Reviews'' / ''Computer Reviews'']</ref><ref>{{cite web|title=ELLIOTT WARD CHENEY JR|url=http://www.legacy.com/obituaries/statesman/obituary.aspx?pid=180693139|publisher=Austin American-Statesman|accessdate=6 May 2017}}</ref>\n\n==Education and personal life==\nThe second of two children of E. W. Cheney, Sr., and Carleton (Pratt) Cheney, Elliott Ward Cheney Jr., was born in [[Gettysburg, Pennsylvania]], and grew up in [[Washington, D.C.]], [[New Jersey]], and [[Bethlehem, Pennsylvania]].<ref name=\"ewc-bio\">[http://www.ma.utexas.edu/CNA/DMY/ewc-bio.pdf EWC Biographical Data]</ref>{{rp|1}} Ward began clarinet studies at age ten and would play in chamber music groups throughout his life.<ref>[http://www.ma.utexas.edu/CNA/DMY/ewc.html]</ref>\n\nWard Cheney was a 1947 graduate of Fountain Hill High School, Bethlehem, Pennsylvania.  In 1951, he earned his bachelor's degree in mathematics from [[Lehigh University]], where his father was a physics professor. During undergraduate summers, Ward worked for the [[United States Forest Service]], where he met Elizabeth Jean \"Beth\", whom he married in 1952. The young couple resided in [[Lawrence, Kansas]], while Ward studied and served as a mathematics instructor at [[The University of Kansas]], earning his [[Ph.D.]] in 1957.<ref name=\"mathgenealogy\">{{mathgenealogy|id=13161|name=Elliott Ward Cheney Jr.}}</ref>  Ward had three children with his first wife, Beth, all of whom earned doctorates: daughter Margaret is a professor of mathematics,<ref>{{cite web|url=http://www.math.colostate.edu/~cheney|title=Margaret Cheney|work=Department of Mathematics, College of Natural Sciences|publisher=[[Colorado State University]]|accessdate=2013-06-14}}</ref> son David is a manager of international research projects,<ref>{{cite web|url=http://csted.sri.com/people/david-w-cheney |title=David W. Cheney |work=Center for Science, Technology & Economic Development |publisher=[[SRI International]] |accessdate=2013-06-14 |deadurl=yes |archiveurl=https://web.archive.org/web/20140728023253/http://csted.sri.com/people/david-w-cheney |archivedate=2014-07-28 |df= }}</ref> and son Elliott is a professional cellist.<ref>{{cite web|url=http://cheneycellists.com/elliottwardcheney.html|title=Elliott Ward Cheney|publisher=Cheney Cellists|accessdate=2013-06-14}}</ref> \nTheir mother Beth remarried in 1975 and died in 1991. Ward and his  wife Victoria had been together since 1983.\n\n==Professional background==\nFollowing the launch of [[Sputnik 1]] by the [[Soviet Union]] in 1957, the United States intensified its focus on their aerospace program.  Cheney became a research scientist at [[Convair]] Astronautics in [[San Diego, California]], where his mathematical team worked on calculations for the [[Atlas rocket]]—which would take [[John Glenn]] into space.\n\nCheney also worked for [[Space Technology]], near Los Angeles, and taught at [[UCLA]], with visiting positions at [[Michigan State University]] and [[Iowa State University]]. In addition, Ward was a consultant and/or a guest worker at [[Boeing]] Scientific Research Laboratories, [[The Aerospace Corporation]], and [[IBM Research]] Laboratory.  In the summers of 1961–63, he was director of the NSF Summer Institute in Numerical Analysis at the [[University of California, Los Angeles]].<ref name=\"ewc-bio\"/>{{rp|1}}\n\nIn 1964, Ward joined the mathematics faculty of [[The University of Texas at Austin]], where he taught for the next 41 years, until his retirement at age 76.<ref name=\"UTemeritus\"/>\n\n==Career and travel==\nCheney served continuously on the editorial board of the [[Journal of Approximation Theory]] from its inception in 1968 until sometime after the start of 2015, and published 14 papers there.<ref>[http://www.sciencedirect.com/science/journal/00219045]</ref><ref>[http://www.journals.elsevier.com/journal-of-approximation-theory/editorial-board Journal of Approximation Theory Editorial Board]</ref>\n\nProfessor Cheney supervised 17 PhD students,<ref name=\"mathgenealogy\"/> 35 Masters students, and worked with three post-doctoral students.<ref name=\"ewc-bio\"/>{{rp|12–13}} He was Associate Editor for ten mathematical journals as well as referee and reviewer for many other journals.<ref name=\"ewc-bio\"/>{{rp|11–12}} Cheney had over a 100 published papers and was the author of two dozen mathematical textbooks, with several having multiple editions.<ref name=\"ewc-bio\"/>{{rp|2–6}} A reviewer wrote of his 1986 monograph: \"Cheney's book scores highly on all ... points\". Ward Cheney and Will Light wrote two graduate level books. Ward Cheney and David Kincaid co-authored two undergraduate textbooks, and a graduate textbook.\n\nDuring his career, Ward frequently spent his summers and a sabbatical semester in England at [[Lancaster University]] and at [[Leicester University]].  Also, he held a visiting professorship at [[Lund University]], Sweden. Ward was a world-wide traveler and was frequently invited to give lecturers on approximation theory at universities wherever he went.<ref name=\"ewc-bio\"/>{{rp|6–10}}\n\n==Teaching and research==\nWard was an inspirational teacher and a superb lecturer who presented over 165 invited lectures and colloquium talks at universities and conferences around the world.<ref name=\"ewc-bio\"/>{{rp|6–10}}  Special honors include an invited lectures at the ''1963 National Meeting of the Society for Industrial and Applied Mathematics (SIAM)'', in [[Denver]], Colorado, and at the ''1974 [[International Congress of Mathematicians]]'', in [[Vancouver]], Canada.  Moreover, Professor Cheney was the honoree at the ''1995 International Conference on Approximation Theory'', [[College Station, Texas]].<ref name=\"ewc-bio\"/>{{rp|11}} For over 40 years, this has been the main general conference on approximation theory with presentations by international mathematician from academia, industry, and government.<ref>[http://www.math.vanderbilt.edu/~at14 14th International Conference on Approximation Theory]</ref>\n\nCheney was awarded grants for his research on approximation theory from the [[National Science Foundation]], [[United States Air Force]] and [[United States Army]] as well as the [[UK Research Councils]] and the [[National Research Council (Italy)|Italian Scientific Research Council]], among others.<ref name=\"ewc-bio\"/>{{rp|11}}\n\nIn 2012, Ward became a Fellow of the [[American Mathematical Society]], which was the first year the honor was awarded.<ref>[http://www.ams.org/profession/fellows-list List of Fellows of the American Mathematical Society]</ref>\n\nCheney died in July 2016, after having had [[Alzheimer's disease]] for several years.{{cn|date=February 2017}}\n\n==Books==\n* An Introduction to Approximation Theory, Ed. 2, American Mathematical Society Chelsea, 1982. {{ISBN|978-0-8218-1374-4}}\n* Approximation Theory in Tensor Product Spaces (with Will Light), Springer, 1985. {{ISBN|978-3-540-16057-1}}; {{cite book|title=2006 reprint|postscript=; pbk, 158 pages|url=https://books.google.com/books/about/Approximation_Theory_in_Tensor_Product_S.html?id=vep7CwAAQBAJ|isbn=9783540397410|last1=Light|first1=William A.|last2=Cheney|first2=Elliot W.|date=2006-11-14}}\n* Multivariate Approximation Theory: Selected Topics, SIAM, 1986. (CBMS-NSF Regional Conference Series in Applied Mathematics 51) {{ISBN|978-0-89871-207-0}}\n* A Course in Approximation Theory (with Will Light), American Mathematical Society, 2000. {{ISBN|0-8218-4798-8}}, {{ISBN|978-0-8218-4798-5}}; {{cite book|title=2009 reprint|url=https://books.google.com/books?id=II6DAwAAQBAJ|postscript=; pbk, 359 pages|isbn = 9780821847985|last1 = Cheney|first1 = Elliott Ward|last2 = Light|first2 = William Allan|date = 2009-01-13}}\n* Analysis for Applied Mathematics, Springer Science+Business Media, New York, 2001. {{ISBN|978-1-4757-3559-8}} (eBook)\n* Numerical Analysis: Mathematics of Scientific Computing, Ed. 3 (with David Kincaid), American Mathematical Society, 2002. {{ISBN|0-534-38905-8}}; 1st edition, 1992<ref>{{cite journal|title=Reviews and Descriptions of Tables and Books (Review of ''Numerical Analysis'' by David Kincaid and Elliott Ward Cheney, pp. 297–298)|journal=Math. Comp.|volume=59|issue=199|year=1992|pages=297–308|doi=10.1090/S0025-5718-92-99743-2}}</ref>\n\n==Further reading==\n* Cheney, E.W., \"Biographical Data File\" [http://www.ma.utexas.edu/CNA/DMY/ewc-bio.pdf] \n* Cheney, E.W., \"On Gauge Functions\", PhD thesis, Kansas University, 1957. [http://www.ams.org/bull/1958-64-03/.../S0002-9904-1958-/0187-1.pdf]{{dead link|date=November 2017 |bot=InternetArchiveBot |fix-attempted=yes }}\n* Cheney, E.W., P.C. Curtis, \"Research Problem 33\", Bulletin American Mathematical Society, Vol. 68, No. 4, p.&nbsp;305, 1962.[http://projecteuclid.org/DPubS?verb=Display&version=1.0&service=U|&handle=euclid.bams/1183524655&page=record]\n* E.W. Cheney Papers, 1933–2005, [[Archives of American Mathematics]], Dolph Briscoe Center for American History, University of Texas at Austin. [http://www.lib.utexas.edu/taro/utcah/00487/cah-00487.html]\n* Cheney, E.W., and J. Levesley, ''In Memoriam: William Allan Light (1950–2002)'', '''J. Approx. Theory''', 123, 1–12, 2003. [http://www.math.technion.ac.il/hat/people/obits/light.pdf]\n* \"Pi Mu Epsilon Journal\", Vol. 1, No. 10, 1949,Syracuse University, April 1954.  [The Official Publication of the Honorary Mathematics Fraternity, p.&nbsp;416:  National Meeting, Johns Hopkins University, Baltimore, Maryland, Dec. 28, 1953] [http://www.pme-math.org/journal/issues/PMEJ.Vol.1.No.10.pdf]\n* [[Griffith Baley Price|Price, Griffith Baley]], \"History of the Department of Mathematics of The University of Kansas, 1866–1970\", ''Kansas University Endowment Association,'' University of Kansas, 1976. (p.&nbsp;647) [https://books.google.com/books/about/History_of_the_Department_of_Mathematics.html?id=1UIhAQAAMAAJ]\n\n==References==\n{{Reflist}}\n\n==External links==\n* [http://www.ma.utexas.edu/CNA/DMY/ewc.html Cheney's website at The University of Texas at Austin]\n* [http://www.legacy.com/obituaries/name/elliott-cheney-obituary?pid=1000000180678176 Cheney's obituary]\n\n{{Authority control}}\n\n{{DEFAULTSORT:Cheney, Elliott Ward Jr.}}\n[[Category:2016 deaths]]\n[[Category:20th-century American mathematicians]]\n[[Category:21st-century American mathematicians]]\n[[Category:University of Texas at Austin faculty]]\n[[Category:Approximation theory]]\n[[Category:University of Kansas alumni]]\n[[Category:Lehigh University alumni]]\n[[Category:Fellows of the American Mathematical Society]]\n[[Category:1929 births]]"
    },
    {
      "title": "Constructive Approximation",
      "url": "https://en.wikipedia.org/wiki/Constructive_Approximation",
      "text": "{{Infobox Journal\n| cover\t=\t\n| discipline\t=\t[[mathematics]]\n| abbreviation\t=\tConstr. Approx.\n| publisher\t=\t[[Springer Science+Business Media|Springer]]\n| country\t=\t[[United States|U.S.]]\n| ISSN\t\t=\t0176-4276\n| eISSN\t\t=\t1432-0940\t\n| CODEN\t\t=\t\n| history\t=\t1985–present\n| website\t=\thttp://www.math.vanderbilt.edu/~ca/\n}}\n\n'''''Constructive Approximation''''' is \"an international mathematics journal dedicated to Approximations and Expansions and related research in computation, function theory, functional analysis, interpolation spaces and interpolation of operators, numerical analysis, space of functions, special functions, and applications.\"<ref>{{cite web | url = https://www.springer.com/journal/00365/about | title = Constructive Approximation| accessdate = 2007-04-30}}</ref>\n\n== References ==\n{{reflist}}\n\n== External links ==\n* [https://web.archive.org/web/20070425012247/http://www.math.vanderbilt.edu/~ca/ Constructive Approximation web site]\n\n[[Category:Mathematics journals]]\n[[Category:Approximation theory]]\n[[Category:English-language journals]]\n[[Category:Publications established in 1985]]\n[[Category:Springer Science+Business Media academic journals]]\n[[Category:Bimonthly journals]]\n\n\n{{math-journal-stub}}"
    },
    {
      "title": "Constructive function theory",
      "url": "https://en.wikipedia.org/wiki/Constructive_function_theory",
      "text": "In [[mathematical analysis]], '''constructive function theory''' is a field which studies the connection between the smoothness of a [[Function (mathematics)|function]] and its degree of [[approximation theory|approximation]].<ref>{{cite web|url=http://encyclopedia2.thefreedictionary.com/Constructive+Theory+of+Functions|title=Constructive Theory of Functions}}</ref><ref>{{SpringerEOM|id=Constructive_theory_of_functions|title=Constructive theory of functions|first=S.A.|last=Telyakovskii}}</ref> It is closely related to [[approximation theory]]. The term was coined by [[Sergei Bernstein]].\n\n==Example==\n\nLet ''f'' be a 2''π''-periodic function. Then ''f'' is ''α''-[[Hölder condition|Hölder]] for some 0&nbsp;<&nbsp;''α''&nbsp;<&nbsp;1 if and only if for every natural ''n'' there exists a [[trigonometric polynomial]] ''P<sub>n</sub>'' of degree ''n'' such that\n: <math> \\max_{0 \\leq x \\leq 2\\pi} | f(x) - P_n(x) | \\leq \\frac{C(f)}{n^\\alpha}, </math>\nwhere ''C''(''f'') is a positive number depending on ''f''. The \"only if\" is due to [[Dunham Jackson]], see  [[Jackson's inequality]]; the \"if\" part is due to [[Sergei Bernstein]], see [[Bernstein's theorem (approximation theory)]].\n\n==Notes==\n{{Reflist}}\n\n==References==\n\n* {{Cite book |first=N. I. |last=Achiezer |author-link=Naum Akhiezer|title=Theory of approximation |translator=Charles J. Hyman |publisher=Frederick Ungar Publishing |location=New York |year=1956 }}\n* {{cite book|mr=0196340|last=Natanson|first=I. P.|author-link=Isidor Natanson|title=Constructive function theory. Vol. I. Uniform approximation|publisher=Frederick Ungar Publishing Co.|location=New York|year=1964}}\n: {{cite book|mr=0196341|last=Natanson|first=I. P.|author-link=Isidor Natanson|title=Constructive function theory. Vol. II. Approximation in mean|publisher=Frederick Ungar Publishing Co.|location=New York|year=1965}}\n: {{cite book|mr=0196342|last=Natanson|first=I. P.|author-link=Isidor Natanson|title=Constructive function theory. Vol. III. Interpolation and approximation quadratures|publisher=Ungar Publishing Co.|location=New York|year=1965}}\n\n[[Category:Approximation theory]]\n[[Category:Smooth functions]]"
    },
    {
      "title": "Dirichlet kernel",
      "url": "https://en.wikipedia.org/wiki/Dirichlet_kernel",
      "text": "In [[mathematical analysis]], the '''Dirichlet kernel''' is the collection of functions\n\n:<math>D_n(x)=\\frac{1}{2\\pi}\\sum_{k=-n}^n\ne^{ikx}=\\frac{1}{2\\pi}\\left(1+2\\sum_{k=1}^n\\cos(kx)\\right)=\\frac{\\sin\\left(\\left(n +1/2\\right) x \\right)}{2\\pi\\sin(x/2)}.</math>\n\nIt is named after [[Peter Gustav Lejeune Dirichlet]].\n\n[[file:Dirichlet kernel anime.gif|thumb|300px|Plot of the first few Dirichlet kernels showing its convergence to the [[Dirac delta function|Dirac delta]] distribution.]]\n\nThe importance of the Dirichlet kernel comes from its relation to [[Fourier series]]. The [[convolution]] of ''D<sub>n</sub>''(''x'') with any function ''ƒ'' of period 2{{pi}} is the ''n''th-degree Fourier series approximation to ''ƒ'', i.e., we have\n\n:<math>(D_n*f)(x)=\\int_{-\\pi}^\\pi f(y)D_n(x-y)\\,dy=\\sum_{k=-n}^n \\hat{f}(k)e^{ikx},</math>\n\nwhere\n\n:<math>\\widehat{f}(k)=\\frac 1 {2\\pi}\\int_{-\\pi}^\\pi f(x)e^{-ikx}\\,dx</math>\n\nis the ''k''th Fourier coefficient of&nbsp;''ƒ''. This implies that in order to study convergence of Fourier series it is enough to study properties of the Dirichlet kernel. \n\n[[File:Dirichlet kernels.svg|thumb|300px|Plot of the first few Dirichlet kernels]]\n\n==''L''<sup>1</sup> norm of the kernel function==\n\nOf particular importance is the fact that the [[Lp space|''L''<sup>1</sup>]] norm of ''D<sub>n</sub>'' on <math>[0, 2\\pi]</math> diverges to infinity as ''n'' → ∞. One can estimate that\n\n: <math>\\| D_n \\| _{L^1} = \\Omega(\\log n). \\, </math>\n\nBy using a Riemann-sum argument to estimate the contribution in the largest neighbourhood of zero in which <math>D_n</math> is positive, and the Jensen's inequality for the remaining part, it is also possible to show that:\n\n: <math>\\| D_n \\|_{L^1} \\geq 4\\operatorname{Si}(\\pi)+\\frac 8 \\pi \\log n.</math>\n\nThis lack of uniform integrability is behind many divergence phenomena for the Fourier series. For example, together with the [[uniform boundedness principle]], it can be used to show that the Fourier series of a [[continuous function]] may fail to converge pointwise, in rather dramatic fashion. See [[convergence of Fourier series]] for further details. \n\nA precise proof of the first result that <math>\\| D_n \\| _{L^1[0,2\\pi]} = \\Omega(\\log n)</math> is given by \n\n:<math>\\begin{align} \n\\int_0^{2\\pi} |D_n(x)| \\, dx & \\geq \\int_0^\\pi \\frac{|\\sin[(2n+1)x]|}{x} \\, dx \\\\[5pt]\n     & \\geq \\sum_{k=0}^{2n} \\int_{k\\pi}^{(k+1)\\pi} \\frac{|\\sin(s)|}{s} \\, ds \\\\[5pt]\n     & \\geq \\left|\\sum_{k=0}^{2n} \\int_0^{\\pi} \\frac{\\sin(s)}{(k+1)\\pi} \\, ds\\right| \\\\[5pt]\n     & = \\frac{2}{\\pi} H_{2n+1} \\\\[5pt]\n     & \\geq \\frac{2}{\\pi} \\log(2n+1),\n\\end{align}\n</math> \n\nwhere we have used the Taylor series identity that <math>2/x \\leq 1 / |\\sin(x/2)|</math> and where <math>H_n</math> are the first-order [[harmonic number]]s.\n\n==Relation to the delta function==\n<!--I am not sure this is useful here.\n\nTo understand the definition, one can see that it is 2{{pi}} times the ''n''th-degree Fourier series approximation to a \"function\" with period 2{{pi}} given by\n\n:<math>\\delta_p(x)=\\sum_{k=-\\infty}^\\infty\\delta(x-2\\pi k)</math>\nwhere &delta;-->\n\nTake the [[periodic function|periodic]] [[Dirac delta function]],{{what|reason=Dirac delta function is not periodic|date=December 2017}} which is not a function of a real variable, but rather a \"[[generalized function]]\", also called a \"distribution\", and multiply by&nbsp;2{{pi}}. We get the [[identity element]] for convolution on functions of period&nbsp;2{{pi}}. In other words, we have\n\n:<math>f*(2\\pi \\delta)=f </math>\n\nfor every function ''ƒ'' of period&nbsp;2{{pi}}. The Fourier series representation of this \"function\" is\n\n:<math>2\\pi \\delta(x)\\sim\\sum_{k=-\\infty}^\\infty e^{ikx}= \\left(1 + 2\\sum_{k=1}^\\infty \\cos(kx)\\right).</math>\n\nTherefore the Dirichlet kernel, which is just the sequence of partial sums of this series, can be thought of as an ''[[approximate identity]]''. Abstractly speaking it is not however an approximate identity of ''positive'' elements (hence the failures mentioned above).\n\n==Proof of the trigonometric identity==\n\nThe [[trigonometric identity]]\n\n:<math>\\sum_{k=-n}^n e^{ikx} = \\frac{\\sin((n+1/2)x)}{\\sin(x/2)}</math>\n\ndisplayed at the top of this article may be established as follows.  First recall that the sum of a finite [[geometric series]] is\n\n:<math>\\sum_{k=0}^n a r^k=a\\frac{1-r^{n+1}}{1-r}.</math>\n\nIn particular, we have\n\n:<math>\\sum_{k=-n}^n r^k=r^{-n}\\cdot\\frac{1-r^{2n+1}}{1-r}.</math>\n\nMultiply both the numerator and the denominator by <math>r^{-1/2}</math>, getting\n\n:<math>\\frac{r^{-n-1/2}}{r^{-1/2}}\\cdot\\frac{1-r^{2n+1}}{1-r} =\\frac{r^{-n-1/2}-r^{n+1/2}}{r^{-1/2}-r^{1/2}}.</math>\n\nIn the case <math>r = e^{ix}</math> we have\n\n:<math>\\sum_{k=-n}^n e^{ikx}=\\frac{e^{-(n+1/2)ix}-e^{(n+1/2)ix}}{e^{-ix/2}-e^{ix/2}} =\\frac{-2i\\sin((n+1/2)x)}{-2i\\sin(x/2)} = \\frac{\\sin((n+1/2)x)}{\\sin(x/2)}</math>\n\nas required.\n\n===Alternative proof of the trigonometric identity===\n\nStart with the series\n\n:<math>f(x) = \\frac 1 2 + \\sum_{k=1}^n\\cos(kx).</math>\n\nMultiply both sides of the above by\n\n:<math>2\\sin(x/2)</math>\n\nand use the trigonometric identity\n\n:<math>\\cos(a)\\sin(b) = \\frac{\\sin(a + b) - \\sin(a - b)} 2</math>\n\nto reduce the right-hand side to\n\n:<math>\\sin((n + 1/2)x).</math>\n\n== Variant of identity ==\nIf the sum is only over non negative integers (which may arise when computing a DFT that is not centered), then using similar techniques we can show the following identity:\n: <math>\\sum_{k=0}^{N-1} e^{ikx} = e^{i(N-1)x/2}\\frac{\\sin(N \\, x/2)}{\\sin(x/2)}</math>\n\n== See also ==\n* [[Fejér kernel]]\n\n==References==\n* Andrew M. Bruckner, Judith B. Bruckner, Brian S. Thomson: ''Real Analysis''. ClassicalRealAnalysis.com 1996, {{ISBN|0-13-458886-X}}, S.620 ([https://books.google.com/books?id=1WY6u0C_jEsC vollständige Online-Version (Google Books)])\n* Podkorytov, A. N. (1988), \"Asymptotic behavior of the Dirichlet kernel of Fourier sums with respect to a polygon\". ''Journal of Soviet Mathematics'', 42(2): 1640–1646. doi: 10.1007/BF01665052\n* [[Howard Levi|Levi, H.]] (1974), \"A geometric construction of the Dirichlet kernel\". ''Transactions of the New York Academy of Sciences'', 36: 640–643. doi: 10.1111/j.2164-0947.1974.tb03023.x\n* {{springer|title=Dirichlet kernel|id=p/d032880}}\n* [http://planetmath.org/encyclopedia/DirichletKernel.html Dirichlet-Kernel at [[PlanetMath]]]{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}\n\n[[Category:Mathematical analysis]]\n[[Category:Fourier series]]\n[[Category:Approximation theory]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "Favard operator",
      "url": "https://en.wikipedia.org/wiki/Favard_operator",
      "text": "In [[functional analysis]], a branch of [[mathematics]], the '''Favard operators''' are defined by:\n\n:<math>[\\mathcal{F}_n(f)](x) = \\frac{1}{\\sqrt{n\\pi}} \\sum_{k=-\\infty}^\\infty {\\exp{\\left({-n {\\left({\\frac{k}{n}-x}\\right)}^2 }\\right)} f\\left(\\frac{k}{n}\\right)}</math>\n\nwhere <math>x\\in\\mathbb{R}</math>, <math>n\\in\\mathbb{N}</math>. They are named after [[Jean Favard]].\n\n==Generalizations==\nA common generalization is:\n:<math>[\\mathcal{F}_n(f)](x) = \\frac{1}{n\\gamma_n\\sqrt{2\\pi}} \\sum_{k=-\\infty}^\\infty {\\exp{\\left({\\frac{-1}{2\\gamma_n^2} {\\left({\\frac{k}{n}-x}\\right)}^2 }\\right)} f\\left(\\frac{k}{n}\\right)}</math>\n\nwhere <math>(\\gamma_n)_{n=1}^\\infty</math> is a positive sequence that [[Limit of a sequence|converges]] to 0.<ref name=\"Nowak\">{{cite journal|last=Nowak|first=Grzegorz|author2=Aneta Sikorska-Nowak|date=14 November 2007|title=On the generalized Favard–Kantorovich and Favard–Durrmeyer operators in exponential function spaces|url=https://eudml.org/doc/128662|journal=Journal of Inequalities and Applications|volume=2007|pages=1|doi=10.1155/2007/75142}}</ref> This reduces to the classical Favard operators when <math>\\gamma_n^2=1/(2n)</math>.\n\n==References==\n*{{cite journal| last=Favard | first=Jean | authorlink=Jean Favard | year=1944 | title=Sur les multiplicateurs d'interpolation | journal=Journal de Mathématiques Pures et Appliquées | volume=23 | issue=9 | pages=219–247|language=fr}} This paper also discussed [[Szász–Mirakyan operator]]s, which is why Favard is sometimes credited with their development (e.g. Favard–Szász operators).[http://users.utcluj.ro/~holhosadi/oldies.php]\n\n===Footnotes===\n<references/>\n\n{{DEFAULTSORT:Favard Operator}}\n[[Category:Approximation theory]]\n\n\n{{mathanalysis-stub}}"
    },
    {
      "title": "Fekete problem",
      "url": "https://en.wikipedia.org/wiki/Fekete_problem",
      "text": "In [[mathematics]], the '''Fekete problem''' is, given a natural number ''N'' and a real ''s''&nbsp;&ge;&nbsp;0, to find the points ''x''<sub>1</sub>,...,''x''<sub>''N''</sub> on the [[2-sphere]]  for which the ''s''-energy, defined by\n\n:<math> \\sum_{1 \\leq i < j \\leq N} \\|x_i - x_j \\|^{-s} </math>\n\nfor ''s''&nbsp;>&nbsp;0 and by\n\n:<math> \\sum_{1 \\leq i < j \\leq N} \\log \\|x_i - x_j \\|^{-1} </math>\n\nfor ''s''&nbsp;=&nbsp;0, is minimal. For ''s''&nbsp;>&nbsp;0, such points are called ''s''-''Fekete points'', and for ''s''&nbsp;=&nbsp;0, ''logarithmic Fekete points'' (see {{harvtxt|Saff|Kuijlaars|1997}}).\nMore generally, one can consider the same problem on the ''d''-dimensional sphere, or on a [[Riemannian manifold]] (in which case ||''x''<sub>''i''</sub>&nbsp;&minus;''x''<sub>''j''</sub>|| is replaced with the Riemannian distance between ''x''<sub>''i''</sub> and ''x''<sub>''j''</sub>).\n\nThe problem originated in the paper by {{harvs|txt|first=Michael|last=Fekete||authorlink=Michael Fekete|year=1923}} who considered the one-dimensional, ''s''&nbsp;=&nbsp;0 case, answering a question of [[Issai Schur]].\n\nAn algorithmic version of the Fekete problem is number 7 on the list of problems discussed by {{harvtxt|Smale|1998}}.\n\n==References==\n*{{Citation | last1=Bendito | first1=E. | last2=Carmona | first2=A. | last3=Encinas | first3=A. M. | last4=Gesto | first4=J. M. | last5=Gómez | first5=A. | last6=Mouriño | first6=C. | last7=Sánchez | first7=M. T. | title=Computational cost of the Fekete problem. I. The forces method on the 2-sphere | doi=10.1016/j.jcp.2009.01.021 | mr=2513833 | year=2009 | journal=Journal of Computational Physics | issn=0021-9991 | volume=228 | issue=9 | pages=3288–3306}}\n*{{Citation | last1=Fekete | first1=M. | author-link=Michael Fekete|title=Über die Verteilung der Wurzeln bei gewissen algebraischen Gleichungen mit ganzzahligen Koeffizienten | doi=10.1007/BF01504345 | mr=1544613 | year=1923 | journal=[[Mathematische Zeitschrift]] | issn=0025-5874 | volume=17 | issue=1 | pages=228–249}}\n*{{cite journal|ref=harv|mr=1439152|last1=Saff|first1=E. B.|last2=Kuijlaars|first2=A. B. J.|title=Distributing many points on a sphere|journal=Math. Intelligencer|volume=19|year=1997|issue=1|pages=5&ndash;11|doi=10.1007/BF03024331}}\n*{{Citation | last1=Smale | first1=Stephen | authorlink = Stephen Smale | title=Mathematical problems for the next century | doi=10.1007/BF03025291 | mr=1631413 | year=1998 | journal=[[The Mathematical Intelligencer]] | issn=0343-6993 | volume=20 | issue=2 | pages=7–15}}\n\n[[Category:Mathematical analysis]]\n[[Category:Approximation theory]]"
    },
    {
      "title": "Haar space",
      "url": "https://en.wikipedia.org/wiki/Haar_space",
      "text": "{{one source|date=February 2017}}\nIn [[approximation theory]], a '''Haar space''' or '''Chebyshev space''' is a finite-dimensional subspace <math>V</math> of <math>\\mathcal C(X, \\mathbb K)</math>, where <math>X</math> is a [[compact space]] and <math>\\mathbb K</math> either the [[real numbers]] or the [[complex numbers]], such that for any given <math>f \\in \\mathcal C(X, \\mathbb K)</math> there is exactly one element of <math>V</math> that approximates <math>f</math> \"best\", i.e. with minimum distance to <math>f</math> in [[supremum norm]].<ref name=\"shapiro\" />\n\n== References ==\n<references>\n<ref name=\"shapiro\">{{cite book |last=Shapiro |first=Harold |year=1971 |title=Topics in Approximation Theory |publisher=Springer |pages=19–22 |isbn=3-540-05376-X}}</ref>\n</references>\n\n[[Category:Approximation theory]]\n\n\n{{mathanalysis-stub}}"
    },
    {
      "title": "Hilbert matrix",
      "url": "https://en.wikipedia.org/wiki/Hilbert_matrix",
      "text": "In [[linear algebra]], a '''Hilbert matrix''', introduced by {{harvs|txt|last=Hilbert|year=1894|authorlink=David Hilbert}},  is a [[square matrix]] with entries being the [[unit fraction]]s\n\n:<math> H_{ij} = \\frac{1}{i+j-1}. </math>\n\nFor example, this is the 5 &times; 5 Hilbert matrix:\n\n:<math>H = \\begin{bmatrix} \n1 & \\frac{1}{2} & \\frac{1}{3} & \\frac{1}{4} & \\frac{1}{5} \\\\[4pt]\n\\frac{1}{2} & \\frac{1}{3} & \\frac{1}{4} & \\frac{1}{5} & \\frac{1}{6} \\\\[4pt]\n\\frac{1}{3} & \\frac{1}{4} & \\frac{1}{5} & \\frac{1}{6} & \\frac{1}{7} \\\\[4pt]\n\\frac{1}{4} & \\frac{1}{5} & \\frac{1}{6} & \\frac{1}{7} & \\frac{1}{8} \\\\[4pt]\n\\frac{1}{5} & \\frac{1}{6} & \\frac{1}{7} & \\frac{1}{8} & \\frac{1}{9} \\end{bmatrix}.</math>\n\nThe Hilbert matrix can be regarded as derived from the integral\n\n:<math> H_{ij} = \\int_{0}^{1} x^{i+j-2} \\, dx, </math>\n\nthat is, as a [[Gramian matrix]] for powers of ''x''. It arises in the [[least squares]] approximation of arbitrary functions by [[polynomial]]s.\n\nThe Hilbert matrices are canonical examples of [[ill-conditioned]] matrices, making them notoriously difficult to use in numerical computation.  For example, the 2-norm [[condition number]] of the matrix above is about 4.8 · 10<sup>5</sup>.\n\n==Historical note==\n\n{{harvtxt|Hilbert|1894}} introduced the Hilbert matrix to  study the following question in [[approximation theory]]: \"Assume that   {{nowrap|''I'' {{=}} [''a'', ''b'']}}, is a real interval.  Is it then possible to find a non-zero polynomial ''P'' with integral coefficients, such that the integral\n\n:<math>\\int_{a}^b P(x)^2 dx</math>\n\nis smaller than any given bound ''&epsilon;''&nbsp;> 0, taken arbitrarily small?\" To answer this question, Hilbert derives an exact formula for the [[determinant]] of the Hilbert matrices and investigates their asymptotics. He concludes that the answer to his question is positive if the length {{nowrap|''b'' &minus; ''a''}} of the interval is smaller than&nbsp;4.\n\n==Properties==\n\nThe Hilbert matrix is [[symmetric matrix|symmetric]] and [[positive-definite matrix|positive definite]]. The Hilbert matrix is also [[totally positive]] (meaning the determinant of every [[submatrix]] is positive).\n\nThe Hilbert matrix is an example of a [[Hankel matrix]]. It is also a specific example of a [[Cauchy matrix|Cauchy Matrix]].\n\nThe determinant can be expressed in [[closed-form expression|closed form]], as a special case of the [[Cauchy determinant]]. The determinant of the ''n'' &times; ''n'' Hilbert matrix is\n\n:<math>\\det(H)={{c_n^{\\;4}}\\over {c_{2n}}}</math>\n\nwhere\n\n:<math>c_n = \\prod_{i=1}^{n-1} i^{n-i}=\\prod_{i=1}^{n-1} i!.\\,</math>\n\nHilbert already mentioned the curious fact that the determinant of the Hilbert matrix is the reciprocal of an integer (see sequence {{OEIS2C|A005249}} in the [[OEIS]]) which also follows from the identity\n: <math>{1 \\over \\det (H)}={{c_{2n}}\\over {c_n^{\\;4}}}=n!\\cdot \\prod_{i=1}^{2n-1} {i \\choose [i/2]}.\n</math>\n\nUsing [[Stirling's approximation]] of the [[factorial]] one can establish the following asymptotic result:\n\n:<math>\\det(H)=a_n\\, n^{-1/4}(2\\pi)^n \\,4^{-n^2}</math>\n\nwhere ''a''<sub>''n''</sub> converges to the constant <math>e^{1/4} 2^{1/12} A^{ - 3} \\approx 0.6450 </math> as <math>n\\rightarrow\\infty</math>, where A is the [[Glaisher–Kinkelin constant]].\n\nThe [[matrix inverse|inverse]] of the Hilbert matrix can be expressed in closed form using [[binomial coefficient]]s; its entries are\n\n:<math>(H^{-1})_{ij}=(-1)^{i+j}(i+j-1){n+i-1 \\choose n-j}{n+j-1 \\choose n-i}{i+j-2 \\choose i-1}^2</math>\n\nwhere ''n'' is the order of the matrix.<ref>{{Cite journal|last=Choi|first=Man-Duen|date=1983|title=Tricks or Treats with the Hilbert Matrix|jstor=2975779|journal=The American Mathematical Monthly|volume=90|issue=5|pages=301–312|doi=10.2307/2975779}}</ref> It follows that the entries of the inverse matrix are all integers, and that the signs form a checkerboard pattern, being positive on the principal diagonal.\n\nThe condition number of the ''n''-by-''n'' Hilbert matrix grows as <math> O((1+\\sqrt{2})^{4n}/\\sqrt{n})</math>.\n\n== References ==\n<references />\n\n== Further reading ==\n\n*{{Citation | last1=Hilbert | first1=David | author1-link=David Hilbert | title=Ein Beitrag zur Theorie des Legendre'schen Polynoms | doi=10.1007/BF02418278 | jfm=25.0817.02 | year=1894 | journal=[[Acta Mathematica]] | issn=0001-5962 | volume=18 | pages=155–159}}. Reprinted in  {{cite book|first=David|last= Hilbert|title=Collected papers|volume= II|chapter= article 21}}\n* {{cite journal|doi=10.1007/PL00005392|last=Beckermann|first=Bernhard|title=The condition number of real Vandermonde, Krylov and positive definite Hankel matrices|journal= Numerische Mathematik|volume=85|issue=4|pages= 553–577|year= 2000|citeseerx=10.1.1.23.5979}}\n* {{cite journal|doi=10.2307/2975779|last=Choi|first= M.-D.|title= Tricks or Treats with the Hilbert Matrix|journal=American Mathematical Monthly|volume=90|issue=5|pages=301–312|year= 1983|jstor=2975779}}\n* {{cite journal|last=Todd|first= John|title=The Condition Number of the Finite Segment of the Hilbert Matrix|journal=National Bureau of Standards, Applied Mathematics Series|volume=39|pages= 109–116|year=1954}}\n* {{Cite book|last=Wilf|first=H. S.|title=Finite Sections of Some Classical Inequalities|location= Heidelberg|publisher= Springer|year= 1970|isbn=978-3-540-04809-1}}\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]\n[[Category:Approximation theory]]\n[[Category:Matrices]]\n[[Category:Determinants]]"
    },
    {
      "title": "Journal of Approximation Theory",
      "url": "https://en.wikipedia.org/wiki/Journal_of_Approximation_Theory",
      "text": "{{Infobox Journal\n| title\t=\tJournal of Approximation Theory\n| cover\t=\tJournal of Approximation Theory.gif\n| discipline\t=\t[[Approximation theory]]\n| abbreviation\t=\tJ. Approx. Theory\n| publisher\t=\t[[Elsevier]]\n| country\t=\t[[United States|U.S.]]\n| ISSN\t=\t0021-9045\n| CODEN\t=\tJAXTAZ\n| history\t=\t1968 to present\n| website\t= http://www.journals.elsevier.com/journal-of-approximation-theory\n}}\n\nThe '''''Journal of Approximation Theory''''' is \"devoted to advances in pure and applied approximation theory and related areas.\"<ref>{{cite web | url = http://www.elsevier.com/wps/find/journaldescription.cws_home/622853/description#description | title = Journal of Approximation Theory - Elsevier | accessdate = 2007-04-30}}</ref>\n\n== References ==\n<references />\n\n==External links==\n* [http://www.math.ohio-state.edu/JAT/ ''Journal of Approximation Theory'' web site]\n* [http://www.journals.elsevier.com/journal-of-approximation-theory ''Journal of Approximation Theory'' home page at Elsevier]\n\n[[Category:Mathematics journals]]\n[[Category:Approximation theory]]\n[[Category:Publications established in 1968]]\n[[Category:Elsevier academic journals]]\n[[Category:English-language journals]]\n[[Category:Monthly journals]]\n\n\n{{math-journal-stub}}"
    },
    {
      "title": "Kolmogorov–Arnold representation theorem",
      "url": "https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Arnold_representation_theorem",
      "text": "In [[real analysis]] and [[approximation theory]], the '''Kolmogorov–Arnold representation theorem''' (or '''superposition theorem''') states that every [[Multivariable calculus|multivariate]] [[Continuity (mathematics)|continuous]] [[Function (mathematics)|function]] can be represented as a superposition of continuous functions of one variable. It solved a more general form of [[Hilbert's thirteenth problem]].<ref>{{cite book|author1=[[Boris Khesin|Boris A. Khesin]]|author2=[[Sergei Tabachnikov|Serge L. Tabachnikov]]|title=Arnold: Swimming Against the Tide|url=https://books.google.com/books?id=aBWHBAAAQBAJ&pg=PA165|year=2014|publisher=[[American Mathematical Society]]|isbn=978-1-4704-1699-7|page=165}}</ref><ref>Shigeo Akashi (2001). \"Application of ϵ-entropy theory to Kolmogorov—Arnold representation theorem\", ''[[Reports on Mathematical Physics]]'', v. 48, pp. 19–26 doi:10.1016/S0034-4877(01)80060-4</ref>\n\nThe works of [[Andrey Kolmogorov]] and [[Vladimir Arnold]] established that if ''f'' is a multivariate continuous function, then ''f'' can be written as a finite [[Function composition|composition]] of continuous functions of a single variable and the [[binary operation]] of [[addition]].<ref name=\"h13\">{{cite web|last=Bar-Natan|first=Dror|authorlink=Dror Bar-Natan|title=Dessert: Hilbert's 13th Problem, in Full Colour|url=http://www.math.toronto.edu/drorbn/Talks/Fields-0911/}}</ref> More specifically,\n\n:<math> f(\\mathbf x) = f(x_1,\\ldots ,x_n) = \\sum_{q=0}^{2n} \\Phi_{q}\\left(\\sum_{p=1}^{n} \\phi_{q,p}(x_{p})\\right) </math>.\n\nConstructive proofs, and even more specific constructions can be found in <ref>Jürgen Braun and Michael Griebel. \"On a constructive proof of Kolmogorov’s superposition theorem\", http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.91.5436&rep=rep1&type=pdf</ref>.\n\nIn a sense, they showed that the only true multivariate function is the sum, since every other function can be written using [[univariate]] functions and summing.<ref name=\"dia\">[[Persi Diaconis]] and Mehrdad Shahshahani, ''On Linear Functions of Linear Combinations'' (1984) p. 180 ([http://www-stat.stanford.edu/~cgates/PERSI/papers/nonlin_func.pdf link])</ref>\n\n==History==\nThe Kolmogorov–Arnold representation theorem is closely related to [[Hilbert's 13th problem]]. In his [[Paris]] lecture at the [[International Congress of Mathematicians]] in 1900, [[David Hilbert]] formulated [[Hilbert's problems|23 problems]] which in his opinion were important for the further development of mathematics.<ref>{{cite journal|first=David|last=Hilbert|authorlink=David Hilbert|title= Mathematical problems|journal=Bulletin of the American Mathematical Society|volume=8|year=1902|pages=461–462}}</ref> The 13th of these problems dealt with the solution of general equations of higher degrees. It is known that for algebraic equations of degree 4 the solution can be computed by formulae that only contain radicals and arithmetic operations. For higher orders, [[Galois theory]] shows us that the solutions of algebraic equations cannot be expressed in terms of basic algebraic operations. It follows from the so called [[Tschirnhaus transformation]] that the general algebraic equation \n:<math>x^{n}+a_{n-1}x^{n-1}+\\cdots +a_{0}=0</math>\ncan be translated to the form <math> y^{n}+b_{n-4}y^{n-4}+\\cdots +b_{1}y+1=0</math>. The Tschirnhaus transformation is given by a formula containing only radicals and arithmetic operations and transforms. Therefore, the solution of an algebraic equation of degree <math>n</math>  can be represented as a superposition of functions of two variables if <math>n<7</math> and as a superposition of functions of <math>n-4</math> variables if <math>n\\geq 7</math>. For <math>n=7</math>  the solution is a superposition of arithmetic operations, radicals, and the solution of the equation <math>y^{7}+b_{3}y^{3}+b_{2}y^{2}+b_{1}y+1=0</math>. \n\nA further simplification with algebraic transformations seems to be impossible which led to Hilbert's conjecture that \"A solution of the general equation of degree 7 cannot be represented as a superposition of continuous functions of two variables\". This explains the relation of [[Hilbert's thirteenth problem]] to the representation of a higher-dimensional function as superposition of lower-dimensional functions. In this context, it has stimulated many studies in the theory of functions and other related problems by different authors.<ref>Jürgen Braun, On Kolmogorov's Superposition Theorem and Its Applications, SVH Verlag, 2010, 192 pp.</ref>\n\n==Variants of the Kolmogorov–Arnold representation theorem==\nA variant of Kolmogorov's theorem that reduces the number of\nouter functions <math>\\Phi_{q}</math> is due to George Lorentz.<ref>{{cite journal|first=George |last=Lorentz|title=Metric entropy, widths, and superpositions of functions|journal=[[American Mathematical Monthly]]|volume=69 |year=1962|pages=469–485}}</ref> He showed in 1962  that the outer functions <math>\\Phi_{q}</math> can be replaced by a single function <math>\\Phi</math>. More precisely, Lorentz proved the existence of functions <math>\\phi _{q,p}</math>, <math>q=0,1,\\ldots, 2n</math>, <math>p=1,\\ldots,n,</math> such that \n\n:<math> f(\\mathbf x) = \\sum_{q=0}^{2n} \\Phi\\left(\\sum_{p=1}^{n} \\phi_{q,p}(x_{p})\\right)</math>.\n\nSprecher <ref>David A. Sprecher, ''On the structure of continuous functions of several variables'', [[Transactions of the American Mathematical Society]], '''115''' (1965), pp. 340–355.</ref> replaced the inner functions <math>\\phi_{q,p}</math> by one single inner function with an appropriate shift in its argument. He proved that there exist real values <math>\\eta, \\lambda_1,\\ldots,\\lambda_n</math>, a continuous function <math>\\Phi\\colon \\mathbb{R} \\rightarrow \\R</math>, and a real increasing continuous function <math>\\phi\\colon [0,1] \\rightarrow [0,1]</math> with <math>\\phi \\in \\operatorname{Lip}(\\ln 2/\\ln (2N+2))</math>, for <math>N \\geq n \\geq 2</math>, such that\n\n:<math> f(\\mathbf x) = \\sum_{q=0}^{2n} \\Phi\\left(\\sum_{p=1}^{n} \\lambda_p \\phi(x_{p}+\\eta q)+q \\right)</math>.\n\nPhillip A. Ostrand <ref>{{cite journal|first=Phillip A.|last= Ostrand|title=Dimension of metric spaces and Hilbert's problem 13|journal= [[Bulletin of the American Mathematical Society]]|volume= 71 |year=1965|pages= 619–622}}</ref> generalized the Kolmogorov superposition theorem to compact metric spaces. For <math>p=1,\\ldots,m</math> let <math>X_p</math> be compact metric spaces of finite dimension <math>n_p</math> and let <math>n = \\sum_{p=1}^{m} n_p</math>. Then there exists continuous functions <math>\\phi_{q,p}\\colon X_p \\rightarrow [0,1], q=0,\\ldots,2n, p=1,\\ldots,m</math> and continuous functions <math>G_q\\colon [0,1] \\rightarrow \\R, q=0,\\ldots,2n</math> such that any continuous function <math>f\\colon X_1 \\times \\dots \\times X_m \\rightarrow \\mathbb{R}</math> is representable in the form \n\n:<math> f(x_1,\\ldots,x_m) = \\sum_{q=0}^{2n} G_{q}\\left(\\sum_{p=1}^{m} \\phi_{q,p}(x_{p})\\right) </math>.\n\n==Original references==\n*[[Andrey Kolmogorov]], \"On the representation of continuous functions of several variables by superpositions of continuous functions of a smaller number of variables\", ''[[Proceedings of the USSR Academy of Sciences]]'', 108 (1956), pp.&nbsp;179–182; English translation: ''Amer. Math. Soc. Transl.'', 17 (1961), pp.&nbsp;369–373.\n*[[Vladimir Arnold]], \"On functions of three variables\", ''Proceedings of the USSR Academy of Sciences'', 114 (1957), pp.&nbsp;679–681; English translation: ''Amer. Math. Soc. Transl.'', 28 (1963), pp.&nbsp;51–54.\n\n==Further reading==\n*S. Ya. Khavinson, ''Best Approximation by Linear Superpositions (Approximate Nomography)'', AMS Translations of Mathematical Monographs (1997)\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Kolmogorov-Arnold representation theorem}}\n[[Category:Real analysis]]\n[[Category:Theorems]]\n[[Category:Functions and mappings]]\n[[Category:Approximation theory]]"
    },
    {
      "title": "Least-squares function approximation",
      "url": "https://en.wikipedia.org/wiki/Least-squares_function_approximation",
      "text": "In [[mathematics]], '''least squares function approximation''' applies the principle of [[least squares]] to [[function approximation]], by means of a weighted sum of other functions. The best approximation can be defined as that which minimises the difference between the original function and the approximation; for a least-squares approach the quality of the approximation is measured in terms of the squared differences between the two.\n\n==Functional analysis==\n{{See also|Fourier series|Generalized Fourier series}}\n\nA generalization to approximation of a data set is the approximation of a function by a sum of other functions, usually an [[orthogonal functions|orthogonal set]]:<ref name=Lanczos>\n\n{{cite book |title=Applied analysis |author=Cornelius Lanczos |pages =212–213 |isbn=0-486-65656-X |publisher=Dover Publications |year=1988 |edition=Reprint of 1956 Prentice–Hall |url=https://books.google.com/books?id=6E85hExIqHYC&pg=PA212}}\n</ref> \n\n:<math>f(x) \\approx f_n (x) = a_1 \\phi _1 (x) + a_2 \\phi _2(x) + \\cdots + a_n \\phi _n (x), \\ </math>\n\nwith the set of functions {<math>\\  \\phi _j (x) </math>} an [[Orthonormal_set#Real-valued_functions|orthonormal set]] over the interval of interest, {{nowrap|say [a, b]}}: see also [[Fejér's theorem]]. The coefficients {<math>\\ a_j </math>} are selected to make the magnitude of the difference ||{{nowrap|''f'' − ''f''<sub>''n''</sub>}}||<sup>2</sup> as small as possible. For example, the magnitude, or norm, of a function {{nowrap|''g'' (''x'' )}} over the {{nowrap|interval [a, b]}} can be defined by:<ref name=Folland>\n\n{{cite book |title=Fourier analysis and its application |page =69 |chapter=Equation 3.14 |author=Gerald B Folland |url=https://books.google.com/books?id=ix2iCQ-o9x4C&pg=PA69 |isbn=0-8218-4790-2 |publisher=American Mathematical Society Bookstore |year=2009 |edition=Reprint of Wadsworth and Brooks/Cole 1992}}\n\n</ref>\n\n:<math> \\|g\\| = \\left(\\int_a^b g^*(x)g(x) \\, dx \\right)^{1/2} </math>\n\nwhere the ‘*’ denotes [[complex conjugate]] in the case of complex functions. The extension of Pythagoras' theorem in this manner leads to [[function space]]s and the notion of [[Lebesgue measure]], an idea of “space” more general than the original basis of Euclidean geometry. The {{nowrap|{ <math>\\phi_j (x)\\  </math> } }} satisfy   [[Orthogonal#Orthogonal_functions|orthonormality relations]]:<ref name=Folland2>\n{{cite book |title=Fourier Analysis and Its Applications|page =69 |first1=Gerald B | last1= Folland|url=https://books.google.com/books?id=ix2iCQ-o9x4C&pg=PA69 |isbn=0-8218-4790-2 |year=2009 |publisher=American Mathematical Society}}\n</ref>\n\n:<math> \\int_a^b \\phi _i^* (x)\\phi _j (x) \\, dx =\\delta_{ij},</math>\n\nwhere ''δ''<sub>''ij''</sub> is the [[Kronecker delta]]. Substituting function {{nowrap|''f''<sub>''n''</sub>}} into these equations then leads to\nthe ''n''-dimensional [[Pythagorean theorem]]:<ref name=Wood>\n\n{{cite book |title=Statistical methods: the geometric approach |author= David J. Saville, Graham R. Wood |chapter=§2.5 Sum of squares |page=30 |url=https://books.google.com/books?id=8ummgMVRev0C&pg=PA30 |isbn=0-387-97517-9 |year=1991 |edition=3rd |publisher=Springer}}\n</ref>\n\n:<math>\\|f_n\\|^2 = |a_1|^2 + |a_2|^2 + \\cdots + |a_n|^2. \\,  </math>\n\nThe coefficients {''a''<sub>''j''</sub>} making {{nowrap begin}}||''f'' − ''f''<sub>''n''</sub>||<sup>2</sup>{{nowrap end}} as small as possible are found to be:<ref name=Lanczos/>\n\n:<math>a_j = \\int_a^b \\phi _j^* (x)f (x) \\, dx. </math>\n\nThe generalization of the ''n''-dimensional Pythagorean theorem to ''infinite-dimensional&thinsp;'' [[real number|real]] inner product spaces is known as [[Parseval's identity]] or Parseval's equation.<ref name=Folland3>\n\n{{cite book |title=cited work |page =77 |chapter=Equation 3.22 |author=Gerald B Folland |url=https://books.google.com/books?id=ix2iCQ-o9x4C&pg=PA77 |isbn=0-8218-4790-2 |date=2009-01-13 }}\n\n</ref> Particular examples of such a representation of a function are the [[Fourier series]] and the [[generalized Fourier series]].\n\n==Further discussion==\n===Using linear algebra===\nIt follows that one can find a \"best\" approximation of another function by minimizing the area between two functions, a continuous function <math>f</math> on <math>[a,b]</math> and a function <math>g\\in W</math> where <math>W</math> is a subspace of <math>C[a,b]</math>:\n:<math>\\text{Area} = \\int_a^b \\left\\vert f(x) - g(x)\\right\\vert \\, dx,</math>\nall within the subspace <math>W</math>. Due to the frequent difficulty of evaluating integrands involving absolute value, one can instead define\n:<math>\\int_a^b [ f(x) - g(x) ] ^2\\, dx</math>\nas an adequate criterion for obtaining the least squares approximation, function <math>g</math>, of <math>f</math> with respect to the inner product space <math>W</math>.\n\nAs such, <math>\\lVert f-g \\rVert ^2</math> or, equivalently, <math>\\lVert f-g \\rVert</math>, can thus be written in vector form:\n\n:<math>\\int_a^b [ f(x)-g(x) ]^2\\, dx = \\left\\langle f-g , f-g\\right\\rangle = \\lVert f-g\\rVert^2.</math>\n\nIn other words, the least squares approximation of <math>f</math> is the function <math>g\\in \\text{ subspace } W</math> closest to <math>f</math> in terms of the inner product <math>\\left \\langle f,g \\right \\rangle</math>. Furthermore, this can be applied with a theorem:\n:Let <math>f</math> be continuous on <math>[ a,b ]</math>, and let <math>W</math> be a finite-dimensional subspace of <math>C[a,b]</math>. The least squares approximating function of <math>f</math> with respect to <math>W</math> is given by\n::<math>g = \\left \\langle f,\\vec w_1 \\right \\rangle \\vec w_1 + \\left \\langle f,\\vec w_2 \\right \\rangle \\vec w_2 + \\cdots + \\left \\langle f,\\vec w_n \\right \\rangle \\vec w_n,</math>\n:where <math>B = \\{\\vec w_1 , \\vec w_2 , \\dots , \\vec w_n \\}</math> is an orthonormal basis for <math>W</math>.\n\n==References==\n{{reflist}}\n\n[[Category:Least squares]]\n[[Category:Approximation theory]]"
    },
    {
      "title": "Lebesgue's lemma",
      "url": "https://en.wikipedia.org/wiki/Lebesgue%27s_lemma",
      "text": "{{Unreferenced|date=December 2009}}\n''For Lebesgue's lemma for open covers of compact spaces in topology see [[Lebesgue's number lemma]]''\n\nIn [[mathematics]], '''Lebesgue's lemma''' is an important statement in [[approximation theory]].  It provides a bound for the projection error.\n\n==Statement==\nLet (''V'', ||·||) be a [[normed vector space]], ''U'' a subspace of ''V'', and ''P'' a [[projection (linear algebra)|linear projector]] on ''U''.  Then for each ''v'' in ''V'':\n:<math> \\|v-Pv\\|\\leq (1+\\|P\\|)\\inf_{u\\in U}\\|v-u\\|.</math>\n\n==See also==\n* [[Lebesgue constant (interpolation)]]\n\n{{DEFAULTSORT:Lebesgue's Lemma}}\n[[Category:Lemmas]]\n[[Category:Approximation theory]]"
    },
    {
      "title": "Modulus of continuity",
      "url": "https://en.wikipedia.org/wiki/Modulus_of_continuity",
      "text": "In [[mathematical analysis]], a '''modulus of continuity''' is a function ω : [0, ∞] → [0, ∞] used to measure quantitatively the [[uniform continuity]] of functions. So, a function ''f'' : ''I'' → '''R''' admits ω as a modulus of continuity if and only if\n:<math>|f(x)-f(y)|\\leq\\omega(|x-y|),</math>\nfor all ''x'' and ''y'' in the domain of ''f''. Since moduli of continuity are required to be infinitesimal at 0, a function turns out to be uniformly continuous if and only if it admits a modulus of continuity. Moreover, relevance to the notion is given by the fact that sets of functions sharing the same modulus of continuity are exactly [[equicontinuity|equicontinuous families]]. For instance, the modulus ω(''t'') := ''kt'' describes the k-[[Lipschitz functions]], the moduli ω(''t'') := ''kt''<sup>α</sup> describe the [[Hölder continuity]], the modulus ω(''t'') := ''kt''(|log(''t'')|+1) describes the '''almost Lipschitz''' class, and so on. In general, the role of ω is to fix some explicit functional dependence of ε on δ in the [[(ε, δ)-definition of limit#Uniform continuity|(ε, δ) definition of uniform continuity]]. The same notions generalize naturally to functions between [[metric space]]s. Moreover, a suitable local version of these notions allows to describe quantitatively the continuity at a point in terms of moduli of continuity.\n\nA special role is played by concave moduli of continuity, especially in connection with extension properties, and with approximation of uniformly continuous functions. For a function between metric spaces, it is equivalent to admit a modulus of continuity that is either concave, or subadditive, or uniformly continuous, or sublinear (in the sense of [[linear growth|growth]]). Actually, the existence of such special moduli of continuity for a uniformly continuous function is always ensured whenever the domain is either a compact, or a convex subset of a normed space. However, a uniformly continuous function on a general metric space admits a concave modulus of continuity if and only if the ratios\n\n:<math>\\frac{d_Y(f(x),f(x'))}{d_X(x,x')}</math>\n\nare uniformly bounded for all pairs (''x'', ''x''′) bounded away from the diagonal of ''X x X''. The functions with the latter property constitute a special subclass of the uniformly continuous functions, that in the following we refer to as the ''special uniformly continuous'' functions. Real-valued special uniformly continuous functions on the metric space ''X'' can also be characterized as the set of all functions that are restrictions to ''X'' of uniformly continuous functions over any normed space isometrically containing ''X''. Also, it can be characterized as the uniform closure of the Lipschitz functions on ''X''.\n\n==Formal definition==\nFormally, a modulus of continuity is any real-extended valued function ω : [0, ∞] → [0, ∞], vanishing at 0 and continuous at 0, that is\n:<math>\\lim_{t\\to0}\\omega(t)=\\omega(0)=0.</math>\n\nModuli of continuity are mainly used to give a quantitative account both of the continuity at a point, and of the uniform continuity, for functions between metric spaces, according to the following definitions.\n\nA function ''f'' : (''X'', ''d<sub>X</sub>'') → (''Y'', ''d<sub>Y</sub>'') admits ω as (local) modulus of continuity at the point ''x'' in ''X'' if and only if,\n:<math>\\forall x'\\in X: d_Y(f(x),f(x'))\\leq\\omega(d_X(x,x')).</math>\nAlso, ''f'' admits ω as (global) modulus of continuity if and only if,\n:<math>\\forall x,x'\\in X: d_Y(f(x),f(x'))\\leq\\omega(d_X(x,x')).</math>\nOne equivalently says that ω is a modulus of continuity (resp., at ''x'') for ''f'', or shortly, ''f'' is ω-continuous (resp., at ''x''). Here, we mainly treat the global notion.\n\n===Elementary facts===\n*If ''f'' has ω as modulus of continuity and ω<sub>1</sub> ≥ ω, then, obviously, ''f'' admits ω<sub>1</sub> too as modulus of continuity.\n*If ''f'' : ''X'' → ''Y'' and ''g'' : ''Y'' → ''Z'' are functions between metric spaces with increasing moduli respectively ω<sub>1</sub> and ω<sub>2</sub> then the composition map <math>g\\circ f:X\\to Z</math> has modulus of continuity <math>\\omega_2\\circ\\omega_1</math>.\n*If ''f'' and ''g'' are functions from the metric space X to the Banach space ''Y'', with moduli respectively ω<sub>1</sub> and ω<sub>2</sub>, then any linear combination ''af''+''bg'' has modulus of continuity |''a''|ω<sub>1</sub>+|''b''|ω<sub>2</sub>. In particular, the set of all functions from ''X'' to ''Y'' that have ω as a modulus of continuity is a convex subset of the vector space ''C''(''X'', ''Y''), closed under [[pointwise convergence]].\n*If ''f'' and ''g'' are bounded real-valued functions on the metric space ''X'', with moduli respectively ω<sub>1</sub> and ω<sub>2</sub>, then the pointwise product ''fg'' has modulus of continuity <math>\\|g\\|_\\infty\\omega_1+\\|f\\|_\\infty \\omega_2</math>.\n*If <math>\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}</math> is a family of real-valued functions on the metric space ''X'' with common modulus of continuity ω, then the inferior envelope <math>\\inf_{\\lambda\\in\\Lambda}f_\\lambda</math>, respectively, the superior envelope <math>\\sup_{\\lambda\\in\\Lambda}f_\\lambda</math>, is a real-valued function with modulus of continuity ω, provided it is finite valued at every point. If ω is real-valued, it is sufficient that the envelope be finite at one point of ''X'' at least.\n\n===Remarks===\n*Some authors require additional properties such as ω being increasing, or continuous. However, if f admits a modulus of continuity in the weaker definition above, it also admits a modulus of continuity which is increasing and infinitely differentiable in ]0, ∞[. For instance,\n::<math>\\omega_1(t):=\\sup_{s\\leq t}\\omega(s)</math> is increasing, and ω<sub>1</sub> ≥ ω;\n::<math>\\omega_2(t):=\\frac{1}{t}\\int_t^{2t}\\omega_1(s)ds</math> is also continuous, and ω<sub>2</sub> ≥ ω<sub>1</sub>,\n:and a suitable variant of the preceding definition also makes ω<sub>2</sub> infinitely differentiable in ]0, ∞[.\n*Any uniformly continuous function admits a minimal modulus of continuity ω<sub>''f''</sub>, that is sometimes referred to as ''the'' (optimal) modulus of continuity of ''f'':\n::<math>\\omega_f(t):=\\sup\\{ d_Y(f(x),f(x')):x\\in X,x'\\in X,d_X(x,x')=t \\} ,\\quad\\forall t\\geq0.</math>\n:Similarly, any function continuous at the point ''x'' admits a minimal modulus of continuity at ''x'', ω<sub>''f''</sub>(''t''; ''x'') (''the'' (optimal) modulus of continuity of ''f'' at ''x'') :\n::<math>\\omega_f(t;x):=\\sup\\{ d_Y(f(x),f(x')): x'\\in X,d_X(x,x')= t \\},\\quad\\forall t\\geq0.</math>\n:However, these restricted notions are not as relevant, for in most cases the optimal modulus of ''f'' could not be computed explicitly, but only bounded from above (by ''any'' modulus of continuity of f). Moreover, the main properties of moduli of continuity concern directly the unrestricted definition.\n*In general, the modulus of continuity of a uniformly continuous function on a metric space needs to take the value +∞. For instance, the function ''f'' : '''N''' → '''N''' such that ''f''(''n'') := ''n''<sup>2</sup> is uniformly continuous with respect to the [[discrete metric]] on '''N''', and its minimal modulus of continuity is ω<sub>''f''</sub>(''t'') = +∞ for any positive integer ''t'', and ω<sub>''f''</sub>(''t'') = 0 otherwise. However, the situation is different for uniformly continuous functions defined on compact or convex subsets of normed spaces.\n\n==Special moduli of continuity==\nSpecial moduli of continuity also reflect certain global properties of functions such as extendibility and uniform approximation. In this section we mainly deal with moduli of continuity that are [[concave function|concave]], or [[subadditive]], or uniformly continuous, or sublinear. These properties are essentially equivalent in that, for a modulus ω (more precisely, its restriction on [0, ∞[) each of the following implies the next:\n*ω is concave;\n*ω is subadditive;\n*ω is uniformly continuous;\n*ω is sublinear, that is, there are constants ''a'' and ''b'' such that ω(''t'') ≤ ''at''+''b'' for all ''t'';\n*ω is dominated by a concave modulus, that is, there exists a concave modulus of continuity <math>\\tilde\\omega</math> such that <math>\\omega(t)\\leq \\tilde\\omega(t)</math> for all ''t''.\n\nThus, for a function ''f'' between metric spaces it is equivalent to admit a modulus of continuity which is either concave, or subadditive, or uniformly continuous, or sublinear. In this case, the function ''f'' is sometimes called a ''special uniformly continuous'' map. This is always true in case of either compact or convex domains. Indeed, a uniformly continuous map ''f'' : ''C'' → ''Y'' defined on a [[convex set]] ''C'' of a normed space ''E'' always admits a [[subadditive]] modulus of continuity; in particular, real-valued as a function ω : [0, ∞[ → [0, ∞[. Indeed, it is immediate to check that the optimal modulus of continuity ω<sub>''f''</sub> defined above is subadditive if the domain of ''f'' is convex: we have, for all ''s'' and ''t'':\n\n:<math>\\begin{align}\n\\omega_f(s+t) &=\\sup_{|x-x'|=t+s} d_Y(f(x),f(x')) \\\\\n&\\leq \\sup_{|x-x'|=t+s}\\left\\{d_Y\\left( f(x), f\\left(x +t\\frac{x-x'}{|x-x'|}\\right)\\right) + d_Y\\left( f\\left(x +s\\frac{x-x'}{|x-x'|}\\right), f(x')\\right )\\right\\} \\\\\n&\\leq \\omega_f(t)+\\omega_f(s).\n\\end{align}</math>\n\nNote that as an immediate consequence, any uniformly continuous function on a convex subset of a normed space has a sublinear growth: there are constants ''a'' and ''b'' such that |''f''(''x'')| ≤ ''a''|''x''|+''b'' for all ''x''. However, a uniformly continuous function on a general metric space admits a concave modulus of continuity if and only if the ratios <math>d_Y(f(x),f(x'))/d_X(x,x')</math> are uniformly bounded for all pairs (''x'', ''x''′) bounded away from the diagonal of ''X x X''; this condition is certainly satisfied by any bounded uniformly continuous function; hence in particular, by any continuous function on a compact metric space.\n\n===Sublinear moduli, and bounded perturbations from Lipschitz===\nA sublinear modulus of continuity can easily be found for any uniformly function which is a bounded perturbation of a Lipschitz function: if ''f'' is a uniformly continuous function with modulus of continuity ω, and ''g'' is a ''k'' Lipschitz function with uniform distance ''r'' from ''f'', then ''f'' admits the sublinear module of continuity min{ω(''t''), 2''r''+''kt''}. Conversely, at least for real-valued functions, any special uniformly continuous function is a bounded, uniformly continuous perturbation of some Lipschitz function; indeed more is true as shown below (Lipschitz approximation).\n\n===Subadditive moduli, and extendibility===\nThe above property for uniformly continuous function on convex domains admits a sort of converse at least in the case of real-valued functions: that is, every special uniformly continuous real-valued function ''f'' : ''X'' → '''R''' defined on a subset ''X'' of a normed space ''E'' admits extensions over ''E'' that preserves any subadditive modulus ω of ''f''. The least and the greatest of such extensions are respectively:\n\n:<math>\\begin{align}\nf_*(x) &:=\\sup_{y\\in X}\\left\\{f(y)-\\omega(|x-y|)\\right\\}, \\\\\nf^*(x) &:=\\inf_{y\\in X}\\left\\{f(y)+\\omega(|x-y|)\\right\\}.\n\\end{align}</math>\n\nAs remarked, any subadditive modulus of continuity is uniformly continuous: in fact, it admits itself as a modulus of continuity. Therefore, ''f''<sub>∗</sub> and ''f*'' are respectively inferior and superior envelopes of ω-continuous families; hence still ω-continuous. Incidentally, by the [[Kuratowski embedding]] any metric space is isometric to a subset of a normed space. Hence, special uniformly continuous real-valued functions are essentially the restrictions of uniformly continuous functions on normed spaces. In particular, this construction provides a quick proof of the [[Tietze extension theorem]] on compact metric spaces. However, for mappings with values in more general Banach spaces than '''R''', the situation is quite more complicated; the first non-trivial result in this direction is the [[Kirszbraun theorem]].\n\n===Concave moduli and Lipschitz approximation===\nEvery special uniformly continuous real-valued function ''f'' : ''X'' → '''R''' defined on the metric space ''X'' is [[uniform convergence|uniformly]] approximable by means of Lipschitz functions. Moreover, the speed of convergence in terms of the Lipschitz constants of the approximations is strictly related to the modulus of continuity of ''f''. Precisely, let ω be the minimal concave modulus of continuity of ''f'', which is\n:<math>\\omega(t)=\\inf\\big\\{at+b\\, :\\, a>0,\\, b>0,\\, \\forall x\\in X,\\, \\forall x'\\in X\\,\\,  |f(x)-f(x')|\\leq a|x-x'|+b\\big\\}.</math>\nLet δ(''s'') be the uniform [[metric spaces#Distance between points and sets; Hausdorff distance and Gromov metric|distance]] between the function ''f'' and the set Lip<sub>''s''</sub> of all Lipschitz real-valued functions on ''C'' having Lipschitz constant ''s'' :\n:<math>\\delta(s):=\\inf\\big\\{\\|f-u\\|_{\\infty,X}\\,:\\, u\\in \\mathrm{Lip}_s\\big\\}\\leq+\\infty.</math>\nThen the functions ω(''t'') and δ(''s'') can be related with each other via a [[Legendre transformation]]: more precisely, the functions 2δ(''s'') and −ω(−''t'') (suitably extended to +∞ outside their domains of finiteness) are a pair of conjugated convex functions<sup>[http://mathoverflow.net/questions/194863/legendre-transform-and-lipschitz-approximation/194890#194890]</sup>, for\n\n:<math>2\\delta(s)=\\sup_{t\\geq0}\\left\\{\\omega(t)-st\\right\\},</math>\n:<math>\\omega(t)=\\inf_{s\\geq0}\\left\\{2\\delta(s)+st\\right\\}.</math>\n\nSince ω(''t'') = o(1) for ''t'' → 0<sup>+</sup>, it follows that δ(''s'') = o(1) for ''s'' → +∞, that exactly means that ''f'' is uniformly approximable by Lipschitz functions. Correspondingly, an optimal approximation is given by the functions\n:<math>f_s:=\\delta(s)+\\inf_{y\\in X}\\{f(y)+sd(x,y)\\}, \\quad  \\mathrm{for} \\ s\\in\\mathrm{dom}(\\delta):</math>\neach function ''f<sub>s</sub>'' has Lipschitz constant ''s'' and\n:<math>\\|f-f_s\\|_{\\infty,X}=\\delta(s);</math>\nin fact, it is the greatest ''s''-Lipschitz function that realize the distance δ(''s''). For example, the α-Hölder real-valued functions on a metric space are characterized as those functions that can be uniformly approximated by ''s''-Lipschitz functions with speed of convergence <math>O(s^{-\\frac{\\alpha}{1-\\alpha}}),</math> while the almost Lipschitz functions are characterized by an exponential speed of convergence <math>O(e^{-as}).</math>\n\n==Examples of use==\n*Let ''f'' : [''a'', ''b''] → '''R''' a continuous function. In the proof that ''f'' is [[Riemann integrable]], one usually  bounds the distance between the upper and lower [[Riemann sums]] with respect to the Riemann partition ''P'' := {''t''<sub>0</sub>, ..., ''t<sub>n</sub>''} in terms of the modulus of continuity of ''f'' and the [[Riemann_integrable#Definition|mesh]] of the partition ''P'' (which  is the number <math> \\scriptstyle |P|:=\\max_{0\\le i<n} (t_{i+1}-t_i)\\quad </math>)\n::<math>S^*(f;P)-S_*(f;P)\\leq(b-a)\\omega(|P|).</math>\n\n*For an example of use in the Fourier series, see [[Dini test]].\n\n==History==\nSteffens (2006, p.&nbsp;160) attributes the first usage of omega for the modulus of continuity to [[Lebesgue]] (1909, p.&nbsp;309/p.&nbsp;75) where omega refers to the oscillation of a Fourier transform. [[De la Vallée Poussin]] (1919, pp.&nbsp;7-8) mentions both names (1) \"modulus of continuity\" and (2) \"modulus of oscillation\" and then concludes \"but we choose (1) to draw attention to the usage we will make of it\".\n\n==The translation group of ''L<sup>p</sup>'' functions, and moduli of continuity ''L<sup>p</sup>''.==\nLet 1 ≤ ''p''; let ''f'' : '''R'''<sup>''n''</sup> → '''R''' a function of class ''L<sup>p</sup>'', and let ''h'' ∈ '''R'''<sup>''n''</sup>. The ''h''-[[Translation (geometry)|translation]] of ''f'', the function defined by (τ<sub>''h''</sub>''f'')(''x'') := ''f''(''x''−''h''), belongs to the ''L<sup>p</sup>'' class; moreover, if 1 ≤ ''p'' < ∞, then as ǁ''h''ǁ → 0 we have:\n\n:<math>\\|\\tau_h f - f\\|_p=o(1).</math>\n\nTherefore, since translations are in fact  linear isometries, also\n\n:<math>\\|\\tau_{v+h} f - \\tau_v f\\|_p=o(1),</math>\n\nas ǁ''h''ǁ → 0, uniformly on ''v'' ∈ '''R'''<sup>''n''</sup>.\n\nIn other words, the map ''h'' → τ<sub>''h''</sub> defines a strongly continuous group of linear isometries of ''L<sup>p</sup>''. In the case ''p'' = ∞ the above property does not hold in general: actually, it exactly reduces to the uniform continuity, and defines the uniform continuous functions. This leads to the following definition, that generalizes the notion of a modulus of continuity of the uniformly continuous functions: a modulus of continuity ''L<sup>p</sup>'' for a measurable function ''f'' : ''X'' → '''R''' is a modulus of continuity ω : [0, ∞] → [0, ∞] such that\n\n:<math>\\|\\tau_h f - f\\|_p\\leq \\omega(h).</math>\n\nThis way, moduli of continuity also give a quantitative account of the continuity property shared by all ''L<sup>p</sup>'' functions.\n\n==Modulus of continuity of higher orders==\nIt can be seen that formal definition of the modulus uses notion of [[finite difference]] of first order:\n\n:<math>\\omega_f(\\delta)=\\omega(f, \\delta)=\\sup\\limits_{x; |h|<\\delta;}\\left|\\Delta_h(f,x)\\right|.</math>\n\nIf we replace that difference with a [[finite difference#Higher-order differences|difference of order ''n'']], we get a modulus of continuity of order ''n'':\n\n:<math>\\omega_n(f, \\delta)=\\sup\\limits_{x; |h|<\\delta;}\\left|\\Delta^n_h(f,x)\\right|.</math>\n\n==See also==\n* [[Constructive analysis]]\n* [[Modulus of convergence]]\n\n==References==\n*{{cite book |first=G. |last=Choquet |title=Cours D'Analyse. Tome II, Topologie |publisher=Masson et C<sup>ie</sup> |location=Paris |year=1964 |language=fr}}\n*{{cite book |first=A. V. |last=Efimov |chapterurl=http://eom.springer.de/c/c025580.htm |chapter=Modulus of continuity |title=Encyclopaedia of Mathematics |publisher=Springer |year=2001 |isbn=1-4020-0609-8 }}\n*{{cite book |first=H. |last=Lebesgue |chapter=Sur les intégrales singulières |series=Ann. Fac. Sci. Univ. Toulouse |volume=3 |issue=1 |year=1909 |pages=25–117 }} Reproduced in: {{cite book |first=Henri |last=Lebesgue |title=Œuvres scientifiques |volume=3 |pages=259–351 |language=fr}}\n*{{cite book |first=Ch. de la Vallée |last=Poussin |title=L'approximation des fonctions d'une variable réelle |publisher=Gauthier-Villars |location=Paris |year=1952 |edition=Reprint of 1919 |isbn= |language=fr}}\n*{{cite book |first1=Y| last1=Benyamini|first2=J| last2= Lindenstrauss |title=Geometric Nonlinear Functional Analysis: Volume 1 |publisher=American Mathematical Soc. |location=Providence, RI  |year=1998 |edition=Colloquium Publications, Vol. 48 |isbn= |language=en}}\n*{{cite book |first=K.-G. |last=Steffens |title=The History of Approximation Theory |publisher=Birkhäuser |location=Boston |year=2006 |isbn=0-8176-4353-2 }}\n\n[[Category:Lipschitz maps]]\n[[Category:Approximation theory]]\n[[Category:Constructivism (mathematics)]]\n[[Category:Fourier analysis]]"
    },
    {
      "title": "Modulus of smoothness",
      "url": "https://en.wikipedia.org/wiki/Modulus_of_smoothness",
      "text": "{{Multiple issues|\n{{one source|date=March 2015}}\n{{technical|date=March 2015}}\n}}\n\nIn [[mathematics]], '''moduli of smoothness''' are used to quantitatively measure smoothness of functions. Moduli of smoothness generalise [[modulus of continuity]] and are used in [[approximation theory]] and [[numerical analysis]] to estimate errors of approximation by [[polynomial]]s and [[spline (mathematics)|spline]]s.\n \n==Moduli of smoothness==\n\nThe modulus of smoothness of order <math>n</math> <ref>DeVore, Ronald A., Lorentz, George G., Constructive approximation, Springer-Verlag, 1993.</ref>\nof a function <math>f\\in C[a,b]</math> is the function <math>\\omega_n:[0,\\infty)\\to\\R</math> defined by\n\n:<math>\\omega_n(t,f,[a,b])=\\sup_{h\\in[0,t]}\\sup_{x\\in[a,b-nh]} \\left |\\Delta_h^n(f,x) \\right | \\qquad \\text{for} \\quad 0\\le t\\le \\frac{b-a} n,</math>\n\nand\n\n:<math>\\omega_n(t,f,[a,b])=\\omega_n\\left(\\frac{b-a}{n},f,[a,b]\\right) \\qquad \\text{for} \\quad t>\\frac{b-a}{n},</math>\n\nwhere the [[finite difference]] (''n''-th order forward difference) is defined as\n\n:<math>\\Delta_h^n(f,x_0)=\\sum_{i=1}^n(-1)^{n-i}\\binom{n}{i} f(x_0+ih).</math>\n\n==Properties==\n\n*<math>\\omega_n(0)=0, \\omega_n(0+)=0.</math>\n\n*<math>\\omega_n</math> is non-decreasing on <math>[0,\\infty).</math>\n\n*<math>\\omega_n</math> is continuous on <math>[0,\\infty).</math>\n\n*For <math>m\\in\\N, t\\geq 0</math> we have:\n::<math>\\omega_n(mt)\\leq m^n\\omega_n(t).</math>\n\n*<math>\\omega_n(f,\\lambda t)\\leq (\\lambda +1)^n\\omega_n(f,t),</math> for <math>\\lambda>0.</math>\n\n*For <math>r\\in \\N</math> let <math>W^r</math> denote the space of continuous function on <math>[-1,1]</math> that have <math>(r-1)</math>-st absolutely continuous derivative on <math>[-1,1]</math> and \n::<math>\\left \\|f^{(r)} \\right \\|_{L_{\\infty}[-1,1]}<+\\infty.</math> \n:If <math>f\\in W^r,</math> then \n::<math>\\omega_r(t,f,[-1,1])\\leq t^r \\left \\|f^{(r)} \\right \\|_{L_{\\infty}[-1,1]}, t\\geq 0,</math> \n:where <math>\\|g(x)\\|_{L_{\\infty}[-1,1]}={\\mathrm{ess} \\sup}_{x\\in [-1,1]}|g(x)|.</math>\n\n==Applications==\n\nModuli of smoothness can be used to prove estimates on the error of approximation. Due to property (6), moduli of smoothness provide more general estimates than the estimates in terms of derivatives.\n\nFor example, moduli of smoothness are used in [[Whitney inequality]] to estimate the error of local polynomial approximation. Another application is given by the following more general version of [[Jackson inequality]]:\n\nFor every natural number <math>n</math>, if <math>f</math> is <math>2\\pi</math>-periodic continuous function, there exists a [[trigonometric polynomial]] <math>T_n</math> of degree <math>\\le n</math> such that\n\n:<math>\\left |f(x)-T_n(x \\right )|\\leq c(k)\\omega_k\\left(\\frac{1}{n},f\\right),\\quad x\\in[0,2\\pi],</math>\n\nwhere the constant <math>c(k)</math> depends on <math>k\\in\\N.</math>\n\n==References==\n{{reflist}}\n\n[[Category:Approximation theory]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Remez algorithm",
      "url": "https://en.wikipedia.org/wiki/Remez_algorithm",
      "text": "{{Short description|Algorithm to approximate functions}}\nThe '''Remez algorithm'''  or '''Remez exchange algorithm''', published by [[Evgeny Yakovlevich Remez]] in 1934, is an iterative algorithm used to find simple approximations to functions, specifically, approximations by functions in a [[Chebyshev space]] that are the best in the [[uniform norm]] ''L''<sub>∞</sub> sense.<ref>E. Ya. Remez, \"Sur la détermination des polynômes d'approximation de degré donnée\", Comm. Soc. Math. Kharkov '''10''', 41 (1934);<br>\"Sur un procédé convergent d'approximations successives pour déterminer les polynômes d'approximation, Compt. Rend. Acad. Sc. '''198''', 2063 (1934);<br>\"Sur le calcul effectiv des polynômes d'approximation des Tschebyscheff\", Compt. Rend. Acade. Sc. '''199''', 337 (1934).</ref>\n\nA typical example of a Chebyshev space is the subspace of [[Chebyshev polynomials]] of order ''n'' in the [[Vector space|space]] of real [[continuous function]]s on an [[interval (mathematics)|interval]], ''C''[''a'', ''b''].  The polynomial of best approximation within a given subspace is defined to be the one that minimizes the maximum [[absolute difference]] between the polynomial and the function. In this case, the form of the solution is precised by the [[equioscillation theorem]].\n\n==Procedure==\nThe Remez algorithm starts with the function ''f'' to be approximated and a set ''X'' of <math>n + 2</math> sample points <math> x_1, x_2, ...,x_{n+2}</math> in the approximation interval, usually the extrema of Chebyshev polynomial linearly mapped to the interval. The steps are:\n\n* Solve the linear system of equations\n:<math> b_0 + b_1 x_i+ ... +b_n x_i ^ n + (-1)^ i E = f(x_i) </math> (where <math> i=1, 2, ... n+2 </math>),\n:for the unknowns <math>b_0, b_1...b_n</math> and ''E''.\n* Use the <math> b_i </math> as coefficients to form a polynomial <math>P_n</math>.\n* Find the set ''M'' of points of local maximum error <math>|P_n(x) - f(x)| </math>.\n* If the errors at every <math> m \\in M </math> are of equal magnitude and alternate in sign, then <math>P_n</math> is the minimax approximation polynomial. If not, replace ''X'' with ''M'' and repeat the steps above.\n\nThe result is called the polynomial of best approximation or the [[minimax approximation algorithm]].\n\nA review of technicalities in implementing the Remez algorithm is given by W. Fraser.<ref>{{cite journal |doi=10.1145/321281.321282 |first=W. |last=Fraser |title=A Survey of Methods of Computing Minimax and Near-Minimax Polynomial Approximations for Functions of a Single Independent Variable |journal=J. ACM |volume=12 |issue= |pages=295 |year=1965 }}</ref>\n\n===On the choice of initialization===\nThe Chebyshev nodes are a common choice for the initial approximation because of their role in the theory of polynomial interpolation. For the initialization of the optimization problem for function ''f'' by the Lagrange interpolant ''L''<sub>n</sub>(''f''), it can be shown that this initial approximation is bounded by\n\n:<math>\\lVert f - L_n(f)\\rVert_\\infty \\le (1 + \\lVert L_n\\rVert_\\infty) \\inf_{p \\in P_n} \\lVert f - p\\rVert</math>\n\nwith the norm or [[Lebesgue constant (interpolation)|Lebesgue constant]] of the Lagrange interpolation operator ''L''<sub>''n''</sub> of the nodes (''t''<sub>1</sub>, ..., ''t''<sub>''n''&nbsp;+&nbsp;1</sub>) being\n\n:<math>\\lVert L_n\\rVert_\\infty = \\overline{\\Lambda}_n(T) = \\max_{-1 \\le x \\le 1} \\lambda_n(T; x),</math>\n\n''T'' being the zeros of the Chebyshev polynomials, and the Lebesgue functions being\n\n:<math>\\lambda_n(T; x) = \\sum_{j = 1}^{n + 1} \\left| l_j(x) \\right|, \\quad l_j(x) = \\prod_{\\stackrel{i = 1}{i \\ne j}}^{n + 1} \\frac{(x - t_i)}{(t_j - t_i)}.</math>\n\nTheodore A. Kilgore,<ref>{{cite journal |doi=10.1016/0021-9045(78)90013-8 |first=T. A. |last=Kilgore |title=A characterization of the Lagrange interpolating projection with minimal Tchebycheff norm |journal=J. Approx. Theory |volume=24 |issue= |pages=273 |year=1978 }}</ref> Carl de Boor, and Allan Pinkus<ref>{{cite journal |doi=10.1016/0021-9045(78)90014-X |first=C. |last=de Boor |first2=A. |last2=Pinkus |title=Proof of the conjectures of Bernstein and Erdös concerning the optimal nodes for polynomial interpolation |journal=[[Journal of Approximation Theory]] |volume=24 |issue= |pages=289 |year=1978 }}</ref> proved that there exists a unique ''t''<sub>''i''</sub> for each ''L''<sub>''n''</sub>, although not known explicitly for (ordinary) polynomials. Similarly, <math>\\underline{\\Lambda}_n(T) = \\min_{-1 \\le x \\le 1} \\lambda_n(T; x)</math>, and the optimality of a choice of nodes can be expressed as <math>\\overline{\\Lambda}_n - \\underline{\\Lambda}_n \\ge 0.</math>\n\nFor Chebyshev nodes, which provides a suboptimal, but analytically explicit choice, the asymptotic behavior is known as<ref>{{cite journal |first=F. W. |last=Luttmann |first2=T. J. |last2=Rivlin |title=Some numerical experiments in the theory of polynomial interpolation |journal=IBM J. Res. Dev. |volume=9 |issue= |pages=187 |year=1965 |doi= 10.1147/rd.93.0187}}</ref>\n\n:<math>\\overline{\\Lambda}_n(T) = \\frac{2}{\\pi} \\log(n + 1) + \\frac{2}{\\pi}\\left(\\gamma + \\log\\frac{8}{\\pi}\\right) + \\alpha_{n + 1}</math>\n\n({{math|''γ''}} being the [[Euler–Mascheroni constant]]) with\n\n:<math>0 < \\alpha_n < \\frac{\\pi}{72 n^2}</math> for <math>n \\ge 1,</math>\n\nand upper bound<ref>T. Rivlin, \"The Lebesgue constants for polynomial interpolation\", in ''Proceedings of the Int. Conf. on Functional Analysis and Its Application'', edited by H. G. Garnier ''et al.'' (Springer-Verlag, Berlin, 1974), p. 422; ''The Chebyshev polynomials'' (Wiley-Interscience, New York, 1974).</ref>\n\n:<math>\\overline{\\Lambda}_n(T) \\le \\frac{2}{\\pi} \\log(n + 1) + 1</math>\n\nLev Brutman<ref>{{cite journal |doi=10.1137/0715046 |first=L. |last=Brutman |title=On the Lebesgue Function for Polynomial Interpolation |journal=SIAM J. Numer. Anal. |volume=15 |issue= |pages=694 |year=1978 }}</ref> obtained the bound for <math>n \\ge 3</math>, and <math>\\hat{T}</math> being the zeros of the expanded Chebyshev polynomials:\n\n:<math>\\overline{\\Lambda}_n(\\hat{T}) - \\underline{\\Lambda}_n(\\hat{T}) < \\overline{\\Lambda}_3 - \\frac{1}{6} \\cot \\frac{\\pi}{8} + \\frac{\\pi}{64} \\frac{1}{\\sin^2(3\\pi/16)} - \\frac{2}{\\pi}(\\gamma - \\log\\pi)\\approx 0.201.</math>\n\nRüdiger Günttner<ref>{{cite journal |doi=10.1137/0717043 |first=R. |last=Günttner |title=Evaluation of Lebesgue Constants |journal=SIAM J. Numer. Anal. |volume=17 |issue= |pages=512 |year=1980 }}</ref> obtained from a sharper estimate for <math>n \\ge 40</math>\n\n:<math>\\overline{\\Lambda}_n(\\hat{T}) - \\underline{\\Lambda}_n(\\hat{T}) < 0.0196.</math>\n\n==Detailed discussion==\nThis section provides more information on the steps outlined above. In this section, the index ''i'' runs from 0 to ''n''+1.\n\n'''Step 1:''' Given <math>x_0, x_1, ... x_{n+1}</math>, solve the linear system of ''n''+2 equations\n:<math> b_0 + b_1 x_i+ ... +b_n x_i ^ n + (-1) ^ i E = f(x_i) </math> (where <math> i=0, 1, ... n+1 </math>),\n:for the unknowns <math>b_0, b_1, ...b_n</math> and ''E''.\n\nIt should be clear that <math>(-1)^i E</math> in this equation makes sense only if the nodes <math>x_0, ...,x_{n+1}</math> are ''ordered'', either strictly increasing or strictly decreasing. Then this linear system has a unique solution.  (As is well known, not every linear system has a solution.) Also, the solution can be obtained with only <math>O(n^2)</math> arithmetic operations while a standard solver from the library would take <math>O(n^3)</math> operations.  Here is the simple proof:\n\nCompute the standard ''n''-th degree interpolant <math>p_1(x)</math> to <math>f(x)</math> at the first ''n''+1 nodes and also the standard ''n''-th degree interpolant\n<math>p_2(x)</math> to the ordinates <math>(-1)^i</math>\n:<math>p_1(x_i) = f(x_i), p_2(x_i) = (-1)^i, i = 0, ..., n.</math>\nTo this end, use each time Newton's interpolation formula with the divided\ndifferences of order <math>0, ...,n</math> and <math>O(n^2)</math> arithmetic operations.\n\nThe polynomial <math>p_2(x)</math> has its ''i''-th zero between <math>x_{i-1}</math> and <math>x_i,\\ i=1, ...,n</math>, and thus no further zeroes between <math>x_n</math> and <math>x_{n+1}</math>: <math>p_2(x_n)</math> and <math>p_2(x_{n+1})</math> have the same sign <math>(-1)^n</math>.\n\nThe linear combination\n<math>p(x) := p_1 (x) - p_2(x)\\!\\cdot\\!E</math> is also a polynomial of degree ''n'' and\n:<math>p(x_i) = p_1(x_i) - p_2(x_i)\\!\\cdot\\! E \\ = \\ f(x_i) - (-1)^i E,\\ \\ \\ \\  i =0, \\ldots, n.</math>\nThis is the same as the equation above for <math>i = 0, ... ,n</math> and for any choice of ''E''.\nThe same equation for ''i'' = ''n''+1 is\n:<math>p(x_{n+1}) \\ = \\ p_1(x_{n+1}) - p_2(x_{n+1})\\!\\cdot\\!E \\ = \\ f(x_{n+1}) - (-1)^{n+1} E</math> and needs special reasoning:  solved for the variable ''E'', it is the ''definition'' of ''E'':\n:<math>E \\ := \\ \\frac{p_1(x_{n+1}) - f(x_{n+1})}{p_2(x_{n+1}) + (-1)^n}.</math>\nAs mentioned above, the two terms in the denominator have same sign:\n''E'' and thus <math>p(x) \\equiv b_0 + b_1x + \\ldots + b_nx^n</math> are always well-defined.\n\nThe error at the given ''n''+2 ordered nodes is positive and negative in turn because\n:<math>p(x_i) - f(x_i) \\ = \\ -(-1)^i E,\\ \\ i = 0, ... , n\\!+\\!1. </math>\n\nThe theorem of ''de La Vallée Poussin'' states that under this condition no polynomial of degree ''n'' exists with error less than ''E''. Indeed, if such a polynomial existed, call it <math>\\tilde p(x)</math>, then the difference\n<math>p(x)-\\tilde p(x) = (p(x) - f(x)) - (\\tilde p(x) - f(x))</math> would still be positive/negative at the ''n''+2 nodes <math>x_i</math> and therefore have at least ''n''+1 zeros which is impossible for a polynomial of degree ''n''.\nThus, this ''E'' is a lower bound for the minimum error which can be achieved with polynomials of degree ''n''.\n\n'''Step 2''' changes the notation from\n<math>b_0 + b_1x + ... + b_nx^n</math> to <math>p(x)</math>.\n\n'''Step 3''' improves upon the input nodes <math>x_0, ..., x_{n+1}</math> and their errors <math>\\pm E</math> as follows.\n\nIn each P-region, the current node <math>x_i</math> is replaced with the local maximizer <math>\\bar{x}_i</math> and in each N-region <math>x_i</math> is replaced with the local minimizer.  (Expect <math>\\bar{x}_0</math> at ''A'', the <math>\\bar {x}_i</math> near <math>x_i</math>, and <math>\\bar{x}_{n+1}</math> at ''B''.) No high precision is required here,\nthe standard ''line search'' with a couple of ''quadratic fits''  should suffice. (See <ref>David G. Luenberger: ''Introduction to Linear and Nonlinear Programming'', Addison-Wesley Publishing Company 1973.</ref>)\n\nLet <math>z_i := p(\\bar{x}_i) - f(\\bar{x}_i)</math>. Each amplitude <math>|z_i|</math> is greater than or equal to ''E''. The Theorem of ''de La Vallée Poussin'' and its proof also\napply to <math>z_0, ... ,z_{n+1}</math> with <math>\\min\\{|z_i|\\} \\geq E</math> as the new\nlower bound for the best error possible with polynomials of degree ''n''.\n\nMoreover, <math>\\max\\{|z_i|\\}</math> comes in handy as an obvious upper bound for that best possible error.\n\n'''Step 4:''' With <math>\\min\\,\\{|z_i|\\}</math> and <math>\\max\\,\\{|z_i|\\}</math> as lower and upper bound for the best possible approximation error, one has a reliable stopping criterion: repeat the steps until <math>\\max\\{|z_i|\\} - \\min\\{|z_i|\\}</math> is sufficiently small or no longer decreases. These bounds indicate the progress.\n\n==Variants==\nSometimes more than one sample point is replaced at the same time with the locations of nearby maximum absolute differences.\n\nSometimes all of the sample points are replaced in a single iteration with the locations of all, alternating sign, maximum differences.<ref name=toobs>2/73 \"The Optimization of Bandlimited Systems\" – G. C. Temes, F. C. Marshall and V. Barcilon.  Proceedings IEEE.</ref>\n\nSometimes [[relative error]] is used to measure the difference between the approximation and the function, especially if the approximation will be used to compute the function on a computer which uses [[floating point]] arithmetic.\n\nSometimes zero-error point constraints are included in a Modified Remez Exchange Algorithm.<ref name=toobs/>\n\n==See also==\n* [[Approximation theory]]\n\n==References==\n{{reflist}}\n\n==External links==\n*[http://www.bores.com/courses/intro/filters/4_equi.htm Intro to DSP]\n*{{MathWorld|urlname=RemezAlgorithm|title=Remez Algorithm|author=Aarts, Ronald M.; Bond, Charles; Mendelsohn, Phil; and Weisstein, Eric W.}}\n\n[[Category:Polynomials]]\n[[Category:Approximation theory]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Semi-infinite programming",
      "url": "https://en.wikipedia.org/wiki/Semi-infinite_programming",
      "text": "In [[optimization (mathematics)|optimization theory]], '''semi-infinite programming''' ('''SIP''') is an [[optimization problem]] with a finite number of variables and an infinite number of constraints, or an infinite number of variables and a finite number of constraints. In the former case the constraints are typically parameterized.<ref>\n* {{cite book|last1=Bonnans|first1=J.&nbsp;Frédéric|last2=Shapiro|first2=Alexander|chapter=5.4 and&nbsp;7.4.4 Semi-infinite programming|title=Perturbation analysis of optimization problems|series=Springer Series in Operations Research|publisher=Springer-Verlag|location=New York|year=2000|pages=496–526 and&nbsp;581|isbn=978-0-387-98705-7|mr=1756264}}\n* M. A. Goberna and M. A. López, ''Linear Semi-Infinite Optimization'', Wiley, 1998.\n* {{cite article|last1=Hettich|first1=R.|last2=Kortanek|first2=K.&nbsp;O.|title=Semi-infinite programming: Theory, methods, and applications|journal=SIAM Review|volume=35|year=1993|number=3|pages=380–429|doi=10.1137/1035089|mr=1234637 | jstor = 2132425}}\n</ref>\n\n==Mathematical formulation of the problem==\nThe problem can be stated simply as:\n:<math> \\min_{x \\in X}\\;\\; f(x) </math>\n\n:<math> \\text{subject to: }</math>\n\n::<math> g(x,y) \\le 0, \\;\\;  \\forall y \\in Y </math>\n\nwhere\n:<math>f: R^n \\to R</math>\n:<math>g: R^n \\times R^m \\to R</math>\n:<math>X \\subseteq R^n</math>\n:<math>Y \\subseteq R^m.</math>\n\nSIP can be seen as a special case of [[bilevel program]]s in which the lower-level variables do not participate in the objective function.\n\n==Methods for solving the problem==\n{{Empty section|date=July 2010}}\n\nIn the meantime, see external links below for a complete tutorial.\n\n==Examples==\n{{Empty section|date=July 2010}}\n\nIn the meantime, see external links below for a complete tutorial.\n\n==See also==\n* [[optimization (mathematics)|Optimization]]\n* [[Generalized semi-infinite programming|Generalized semi-infinite programming (GSIP)]]\n\n==References==\n<references/>\n\n* Edward J. Anderson and Peter Nash, ''Linear Programming in Infinite-Dimensional Spaces'', Wiley, 1987.\n* {{cite book|last1=Bonnans|first1=J.&nbsp;Frédéric|last2=Shapiro|first2=Alexander|chapter=5.4 and&nbsp;7.4.4 Semi-infinite programming|title=Perturbation analysis of optimization problems|series=Springer Series in Operations Research|publisher=Springer-Verlag|location=New York|year=2000|pages=496–526 and&nbsp;581|isbn=978-0-387-98705-7|mr=1756264}}\n* M. A. Goberna and M. A. López, ''Linear Semi-Infinite Optimization'', Wiley, 1998.\n* {{cite article|last1=Hettich|first1=R.|last2=Kortanek|first2=K.&nbsp;O.|title=Semi-infinite programming: Theory, methods, and applications|journal=SIAM Review|volume=35|year=1993|number=3|pages=380–429|doi=10.1137/1035089|mr=1234637 | jstor = 2132425}}\n* David Luenberger (1997). ''Optimization by Vector Space Methods.'' John Wiley & Sons.  {{isbn|0-471-18117-X}}.\n* Rembert Reemtsen and Jan-J. Rückmann (Editors), ''Semi-Infinite Programming (Nonconvex Optimization and Its Applications)''. Springer, 1998, {{isbn|0-7923-5054-5}}, 1998\n\n==External links==\n* [https://glossary.informs.org/ver2/mpgwiki/index.php?title=Semi-infinite_program Description of semi-infinite programming from INFORMS (Institute for Operations Research and Management Science)].\n* [http://www.sciencedirect.com/science/article/pii/S0377042707000982# A complete, free, open source Semi Infinite Programming Tutorial is available here from Elsevier as a pdf download from their Journal of Computational and Applied Mathematics, Volume 217, Issue 2, 1 August 2008, Pages 394–419]\n\n[[Category:Optimization in vector spaces]]\n[[Category:Approximation theory]]\n[[Category:Numerical analysis]]\n\n\n{{Mathapplied-stub}}"
    },
    {
      "title": "Szász–Mirakjan–Kantorovich operator",
      "url": "https://en.wikipedia.org/wiki/Sz%C3%A1sz%E2%80%93Mirakjan%E2%80%93Kantorovich_operator",
      "text": "In [[functional analysis]], a discipline within [[mathematics]], the '''Szász–Mirakjan–Kantorovich operators''' are defined by\n\n:<math>[\\mathcal{T}_n(f)](x)=ne^{-nx}\\sum_{k=0}^\\infty{\\frac{(nx)^k}{k!}\\int_{k/n}^{(k+1)/n}f(t)\\,dt}</math>\n\nwhere <math>x\\in[0,\\infty)\\subset\\mathbb{R}</math> and <math>n\\in\\mathbb{N}</math>.<ref name=\"Walczak\">{{cite journal|last=Walczak|first=Zbigniew|year=2002|title=On approximation by modified Szasz–Mirakyan operators|journal=Glasnik Matematički|volume=37|issue=57|pages=303–319|url=http://web.math.hr/glasnik/vol_37/no2_08.html}}</ref>\n\n==See also==\n*[[Szász–Mirakyan operator]]\n\n==Notes==\n<references/>\n\n==References==\n*{{cite journal|last=Totik|first=V.|authorlink=Vilmos Totik|date=June 1983|title=Approximation by Szász–Mirakjan–-Kantorovich operators in ''L''<sup>''p''</sup> (''p''&nbsp;>&nbsp;1)|journal=Analysis Mathematica|volume=9|issue=2|pages=147–167|doi=10.1007/BF01982010|mr=720083| zbl = 0513.41012|language=ru}}\n\n{{DEFAULTSORT:Szasz-Mirakjan-Kantorovich Operator}}\n[[Category:Approximation theory]]\n\n\n{{mathanalysis-stub}}"
    },
    {
      "title": "Szász–Mirakyan operator",
      "url": "https://en.wikipedia.org/wiki/Sz%C3%A1sz%E2%80%93Mirakyan_operator",
      "text": "In [[functional analysis]], a discipline within [[mathematics]], the '''Szász–Mirakyan operators''' (also spelled \"Mirakjan\" and \"Mirakian\") are generalizations of [[Bernstein polynomials]] to infinite intervals, introduced by [[Otto Szász]] in 1950  and [[G. M. Mirakjan]] in 1941. They are defined by\n:<math>\\left[\\mathcal{S}_n(f)\\right](x)</math>=<math>e^{-nx}\\sum_{k=0}^\\infty{\\frac{(nx)^k}{k!}f\\left(\\frac{k}{n}\\right)}</math>\nwhere <math>x\\in[0,\\infty)\\subset\\mathbb{R}</math> and <math>n\\in\\mathbb{N}</math>.<ref name=\"Szász\">{{cite journal| last=Szász | first=Otto | year=1950 | title=Generalizations of S. Bernstein's polynomials to the infinite interval | journal=Journal of Research of the National Bureau of Standards | volume=45 | issue=3 | pages=239–245 | url=http://nvl.nist.gov/pub/nistpubs/jres/045/3/V45.N03.A09.pdf | doi=10.6028/jres.045.024}}</ref><ref name=\"Walczak\">{{cite journal| last=Walczak | first=Zbigniew | year=2003 | title=On modified Szasz–Mirakyan operators | journal=Novi Sad Journal of Mathematics | volume=33 | issue=1 | pages=93–107 | url=http://www.emis.de/journals/NSJOM/33_1/rad-08.pdf}}</ref>\n\n==Basic results==\nIn 1964, Cheney and Sharma showed that if <math>f</math> is convex and non-linear, the sequence <math>(\\mathcal{S}_n(f))_{n\\in\\mathbb{N}}</math> decreases with <math>n</math> (<math>\\mathcal{S}_n(f)\\geq f</math>).<ref name=\"Cheney\">{{cite journal|last=Cheney|first=Edward W.|author2=A. Sharma |year=1964|title=Bernstein power series|journal=Canadian Journal of Mathematics|volume=16|issue=[https://books.google.com/books?id=RSNqggY5Q5cC&dq 2]|pages=241–252|doi=10.4153/cjm-1964-023-1 }}</ref> They also showed that if <math>f</math> is a polynomial of degree <math>\\leq m</math>, then so is <math>\\mathcal{S}_n(f)</math> for all <math>n</math>.\n\nA converse of the first property was shown by Horová in 1968 (Altomare & Campiti 1994:350).\n\n==Theorem on convergence==\nIn Szász's original paper, he proved the following:\n:: If <math>f</math> is [[Continuous function|continuous]] on <math>(0,\\infty)</math>, having a finite limit at infinity, then <math>\\mathcal{S}_n(f)</math> [[Uniform convergence|converges uniformly]] to <math>f</math> as <math>n\\rightarrow\\infty</math>.<ref name=\"Szász\"/>\nThis is analogous to [[Bernstein polynomial#Approximating continuous functions|a theorem stating that Bernstein polynomials approximate continuous functions on [0,1]]].\n\n==Generalizations==\nA [[Leonid Kantorovich|Kantorovich]]-type generalization is sometimes discussed in the literature. These generalizations are also called the [[Szász–Mirakjan–Kantorovich operator]]s.\n\nIn 1976, C. P. May showed that the [[Baskakov operators]] can reduce to the Szász–Mirakyan operators.<ref name=\"May\">{{cite journal|last=May|first=C. P.|year=1976|title=Saturation and inverse theorems for combinations of a class of exponential-type operators|journal=Canadian Journal of Mathematics|volume=28|issue=6|pages=1224–1250|url=https://books.google.com/books?hl=en&lr=&id=irg7sNuSTT8C&oi=fnd&pg=PA1224&ots=cdhSVISxAs&sig=OW7T5zOvK9PSaEsmT9_40BG0Pdc|doi=10.4153/cjm-1976-123-8}}</ref>\n\n==References==\n*{{cite book|last=Altomare|first=Francesco|author2=Michele Campiti |year=1994|title=Korovkin-Type Approximation Theory and Its Applications|publisher=Walter de Gruyter|isbn=3-11-014178-7}}\n*{{cite journal| last=Favard | first=Jean | authorlink=Jean Favard | year=1944 | title=Sur les multiplicateurs d'interpolation | language=French | journal=Journal de Mathématiques Pures et Appliquées | volume=23 | issue=9 | pages=219–247}} (See also: [[Favard operators]])\n*{{cite journal|last=Horová|first=Ivana|year=1968|title=Linear positive operators of convex functions|journal=Mathematica (Cluj)|volume=10|issue=33|pages=275–283|zbl=0186.11101}}\n*{{cite journal| last=Kac | first=Mark | authorlink=Mark Kac | year=1938 | title=Une remarque sur les polynomes de M. S. Bernstein | language=French | journal=Studia Mathematica | volume=7 | pages=49–51 | zbl=0018.20704 | url=http://matwbn.icm.edu.pl/ksiazki/sm/sm7/sm715.pdf}}\n*{{cite journal| last=Kac | first=M. | authorlink=Mark Kac | year=1939 | title=Reconnaissance de priorité relative à ma note 'Une remarque sur les polynomes de M. S. Bernstein' | language=French | journal=Studia Mathematica | volume=8 | pages=170 | jfm=65.0248.03 | url=http://matwbn.icm.edu.pl/ksiazki/sm/sm8/sm8111.pdf}}\n*{{cite journal| last=Mirakjan | first=G. M. | year=1941 |trans-title=Approximation of continuous functions with the aid of polynomials  of the form <math>e^{-nx}\\sum_{k=0}^{m_n}{C_{k,n}x^k}</math> | title=Approximation des fonctions continues au moyen de polynômes de la forme <math>e^{-nx}\\sum_{k=0}^{m_n}{C_{k,n}x^k}</math> | language=French | journal=[[Comptes rendus de l'Académie des sciences de l'URSS]] | volume=31 | pages=201–205 | jfm=67.0216.03}}\n*{{cite journal|last=Wood|first=B.|date=July 1969|title=Generalized Szasz operators for the approximation in the complex domain|journal=SIAM Journal on Applied Mathematics|volume=17|pages=790–801| zbl=0182.08801|issue=4|doi=10.1137/0117071|jstor=2099320}}\n\n===Footnotes===\n<references/>\n\n{{DEFAULTSORT:Szasz-Mirakyan Operator}}\n[[Category:Approximation theory]]"
    },
    {
      "title": "Unisolvent functions",
      "url": "https://en.wikipedia.org/wiki/Unisolvent_functions",
      "text": "\n{{refimprove|date=February 2009}}\n\nIn mathematics, a set of ''n'' [[function (mathematics)|function]]s ''f''<sub>1</sub>, ''f''<sub>2</sub>, ..., ''f''<sub>''n''</sub> is '''unisolvent''' (meaning \"uniquely solvable\") on a [[domain (mathematics)|domain]] Ω if the [[vector (mathematics)|vector]]s\n\n: <math>\\begin{bmatrix}f_1(x_1) \\\\ f_1(x_2) \\\\ \\vdots \\\\ f_1(x_n)\\end{bmatrix}, \\begin{bmatrix}f_2(x_1) \\\\ f_2(x_2) \\\\ \\vdots \\\\ f_2(x_n)\\end{bmatrix}, \\dots, \\begin{bmatrix}f_n(x_1) \\\\ f_n(x_2) \\\\ \\vdots \\\\ f_n(x_n)\\end{bmatrix}</math>\n\nare [[linearly independent]] for any choice of ''n'' distinct points ''x''<sub>1</sub>, ''x''<sub>2</sub> ... ''x''<sub>''n''</sub> in Ω. Equivalently, the collection is unisolvent if the [[matrix (mathematics)|matrix]] ''F'' with entries ''f''<sub>''i''</sub>(''x''<sub>''j''</sub>) has nonzero [[determinant]]: det(''F'') ≠ 0 for any choice of distinct ''x''<sub>''j''</sub>'s in Ω. Unisolvency is a property of [[vector space]]s, not just particular sets of functions. That is, a vector space of functions of dimension ''n'' is unisolvent if given any [[Basis (linear algebra)|basis]] (equivalently, a linearly independent set of ''n'' functions), the basis is unisolvent (as a set of functions). This is because any two bases are related by an invertible matrix (the change of basis matrix), so one basis is unisolvent if and only if any other basis is unisolvent.\n\nUnisolvent systems of functions are widely used in [[interpolation]] since they guarantee a unique solution to the interpolation problem. The set of [[polynomial]]s of degree at most {{tmath|d}} (which form a vector space of dimension {{tmath|d + 1}}) are unisolvent by the [[unisolvence theorem]].\n\n==Examples==\n* 1, ''x'', ''x''<sup>2</sup> is unisolvent on any interval by the unisolvence theorem\n* 1, ''x''<sup>2</sup> is unisolvent on [0,&nbsp;1], but not unisolvent on [&minus;1,&nbsp;1]\n* 1, cos(''x''), cos(2''x''), ..., cos(''nx''), sin(''x''), sin(2''x''), ..., sin(''nx'') is unisolvent on [&minus;''π'',&nbsp;''π'']\n* In science, unisolvent functions are used in [[linear inverse problem]]s.\n\n==Dimensions==\nSystems of unisolvent functions are much more common in 1&nbsp;dimension than in higher dimensions. In dimension ''d'' = 2 and higher (Ω&nbsp;⊂&nbsp;'''R'''<sup>''d''</sup>), the functions ''f''<sub>1</sub>, ''f''<sub>2</sub>, ..., ''f''<sub>''n''</sub> cannot be unisolvent on Ω if there exists a single open set on which they are all continuous. To see this, consider moving points ''x''<sub>1</sub> and ''x''<sub>2</sub> along continuous paths in the open set until they have switched positions, such that ''x''<sub>1</sub> and ''x''<sub>2</sub> never intersect each other or any of the other ''x''<sub>''i''</sub>. The determinant of the resulting system (with ''x''<sub>1</sub> and ''x''<sub>2</sub> swapped) is the negative of the determinant of the initial system. Since the functions ''f''<sub>''i''</sub> are continuous, the [[intermediate value theorem]] implies that some intermediate configuration has determinant zero, hence the functions cannot be unisolvent.\n\n==See also==\n* [[Inverse problem]]\n\n==References==\n* [[Philip J. Davis]]: ''Interpolation and Approximation'' pp.&nbsp;31&ndash;32\n\n[[Category:Interpolation]]\n[[Category:Inverse problems]]\n[[Category:Numerical analysis]]\n[[Category:Approximation theory]]"
    },
    {
      "title": "Unisolvent point set",
      "url": "https://en.wikipedia.org/wiki/Unisolvent_point_set",
      "text": "\n{{unreferenced|date=August 2012}}\nIn [[approximation theory]], a finite collection of points <math>X \\subset R^n</math> is often called '''unisolvent''' for a space <math>W</math> if any element <math>w \\in W</math> is uniquely determined by its values on <math>X</math>.\n<br />\n<math>X</math> is unisolvent for <math>\\Pi^m_n</math> (polynomials in n variables of degree at most m) if there exists a unique [[polynomial]] in <math>\\Pi^m_n</math> of lowest possible degree which [[interpolate]]s the data <math>X</math>.\n\nSimple examples in <math>R</math> would be the fact that two distinct points determine a line, three points determine a parabola, etc. It is clear that over <math>R</math>, any collection of ''k''&nbsp;+&nbsp;1 distinct points will uniquely determine a polynomial of lowest possible degree in <math>\\Pi^k</math>.\n\n==See also==\n*[[Padua points]]\n\n==External links==\n*[http://en.wikibooks.org/wiki/Numerical_Methods/Interpolation Numerical Methods / Interpolation]\n\n[[Category:Approximation theory]]\n\n\n{{Mathanalysis-stub}}"
    },
    {
      "title": "Universal differential equation",
      "url": "https://en.wikipedia.org/wiki/Universal_differential_equation",
      "text": "A '''universal differential equation''' ('''UDE''') is a non-trivial [[differential algebraic equation]] with the property that its solutions can [[Approximation theory|approximate]] any [[continuous function]] on any interval of the real line to any desired level of accuracy. \n\n== External links ==\n*[http://mathworld.wolfram.com/UniversalDifferentialEquation.html Wolfram Mathworld page on UDEs]\n\n[[Category:Differential equations]]\n[[Category:Approximation theory]]\n{{mathanalysis-stub}}"
    },
    {
      "title": "Whitney inequality",
      "url": "https://en.wikipedia.org/wiki/Whitney_inequality",
      "text": "In [[mathematics]], the '''Whitney inequality''' gives an upper bound for the error of best approximation of a function by [[polynomial]]s in terms of the [[modulus of smoothness|moduli of smoothness]]. It was first proved by [[Hassler Whitney]] in 1957,<ref name=Whitneypaper>{{cite journal|last1=Hassler|first1=Whitney|title=On Functions with Bounded nth Differences|journal=J. Math. Pures Appl.|date=1957|volume=36|issue=IX|pages=67–95}}</ref> and is an important tool in the field of [[approximation theory]] for obtaining upper estimates on the errors of best approximation.\n\n==Statement of the theorem==\n\nDenote the value of the best uniform approximation of a [[Function (mathematics)|function]] <math>f\\in C([a,b])</math> by algebraic [[polynomials]] <math>P_n</math> of degree <math>\\leq n</math> by\n\n: <math> E_n(f)_{[a,b]} := \\inf_{P_n}{\\| f-P_n \\|_{C([a,b])}} </math>\n\nThe [[modulus of smoothness|moduli of smoothness]] of order <math>k</math> of a [[Function (mathematics)|function]] <math>f\\in C([a,b])</math> are defined as:\n\n: <math>\\omega_k(t):=\\omega_k(t;f;[a,b]) :=\\sup_{h\\in [0,t]} \\|\\Delta_h^k(f;\\cdot)\\|_{C([a,b-kh])} \\quad \\text{ for } \\quad t\\in [0,(b-a)/k],</math>\n\n: <math>\\omega_k(t):=\\omega_k((b-a)/k)\\quad \\text{ for} \\quad t>(b-a)/k ,</math>\n\nwhere <math>\\Delta_h^k</math> is the [[finite difference]] of order <math>k</math>.\n\n'''Theorem:''' <ref name=UnifApprox>{{cite book|last1=Dzyadyk|first1=Vladislav K.|last2=Shevchuk|first2=Igor A.|title=Theory of Uniform Approximation of Functions by Polynomials|publisher=Walter de Gruyter|location=Berlin, Germany|isbn=978-3-11-020147-5|edition=1st|chapter=3.6|pages=231&ndash;233}}</ref> [Whitney, 1957] If <math>f\\in C([a,b])</math>, then\n\n: <math>E_{k-1}(f)_{[a,b]}\\leq W_k \\omega_k\\left(\\frac{b-a}{k};f;[a,b]\\right) </math>\n\nwhere <math>W_k</math> is a constant depending only on <math>k</math>. The Whitney constant <math>W(k)</math> is the smallest value of <math>W_k</math> for which the above inequality holds. The theorem is particularly useful when applied on intervals of small length, leading to good estimates on the error of [[spline (mathematics)|spline]] approximation.\n\n==Proof==\nThe original proof given by Whitney follows an analytic argument which utilizes the properties of [[modulus of smoothness|moduli of smoothness]]. However, it can also be proved in a much shorter way using Peetre's K-functionals.<ref name=DeLo>{{cite book|last1=Devore|first1=R. A. K.|last2=Lorentz|first2=G. G.|title=Constructive Approximation, Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences]|publisher=Springer-Verlag|location=Berlin, Germany|isbn=978-3540506270|edition=1st|chapter=6, Theorem 4.2}}</ref>\n\nLet:\n\n: <math>x_0:=a, \\quad h:=\\frac{b-a}{k}, \\quad x_j:=x+0+jh, \\quad F(x) = \\int_a^x f(u) \\, du,</math>\n\n: <math>G(x):=F(x)-L(x;F;x_0,\\ldots,x_k), \\quad g(x):=G'(x),</math>\n\n: <math>\\omega_k(t):=\\omega_k(t;f;[a,b])\\equiv \\omega_k(t;g;[a,b])</math>\n\nwhere <math>L(x;F;x_0,\\ldots,x_k)</math> is the [[Lagrange polynomial]] for <math>F</math> at the nodes <math>\\{x_0,\\ldots,x_k\\}</math>.\n\nNow fix some <math>x\\in [a,b]</math> and choose <math>\\delta</math> for which <math>(x+k\\delta)\\in [a,b]</math>. Then:\n\n: <math>\\int_0^1 \\Delta_{t\\delta}^k (g;x) \\, dt =(-1)^kg(x)+\\sum_{j=1}^k (-1)^{k-j} \\binom{k}{j} \\int_0^1 g(x+jt\\delta) \\, dt</math>\n\n: <math>=(-1)^kg(x)+\\sum_{j=1}^k{(-1)^{k-j} \\binom{k}{j} \\frac{1}{j\\delta}(G(x+j\\delta)-G(x))}, </math>\n\nTherefore:\n\n: <math>|g(x)| \\leq \\int_0^1 |\\Delta_{t\\delta}^k(g;x)| \\, dt + \\frac{2}{|\\delta|}\\|G\\|_{C([a,b])} \\sum_{j=1}^k \\binom{k}{j}\\frac{1}{j} \\leq \\omega_k(|\\delta|)+\\frac{1}{|\\delta|}2^{k+1}\\|G\\|_{C([a,b])} </math>\n\nAnd since we have <math>\\|G\\|_{C([a,b])}\\leq h\\omega_k(h)</math>, (a property of [[modulus of smoothness|moduli of smoothness]])\n\n: <math>E_{k-1}(f)_{[a,b]}\\leq \\|g\\|_{C([a,b])} \\leq \\omega_k(|\\delta|) +\\frac{1}{|\\delta|} h 2^{k+1}\\omega_k(h). </math> \n\nSince <math>\\delta</math> can always be chosen in such a way that <math>h\\geq |\\delta| \\geq h/2</math>, this completes the proof.\n\n==Whitney constants and Sendov's conjecture==\n\nIt is important to have sharp estimates of the Whitney constants. It is easily shown that <math>W(1)=1/2</math>, and it was first proved by [[John Charles Burkill|Burkill]] (1952) that <math>W(2)\\leq 1</math>, who conjectured that <math>W(k)\\leq 1</math> for all <math>k</math>. [[Hassler Whitney|Whitney]] was also able to prove that <ref name=UnifApprox />\n\n: <math>W(2)=\\frac{1}{2}, \\quad \\frac{8}{15}\\leq W(3) \\leq 0.7 \\quad W(4)\\leq 3.3 \\quad W(5)\\leq 10.4</math> \n\nand\n\n: <math>W(k)\\geq \\frac{1}{2},\\quad k\\in\\mathbb{N} </math>\n\nIn 1964, Brudnyi was able to obtain the estimate <math>W(k)=O(k^{2k})</math>, and in 1982, Sendov proved that <math>W(k)\\leq (k+1)k^k</math>.  Then, in 1985, Ivanov and Takev proved that <math>W(k)=O(k\\ln k)</math>, and Binev proved that <math>W(k)=O(k)</math>. Sendov conjectured that <math>W(k)\\leq 1</math> for all <math>k</math>, and in 1985 was able to prove that the Whitney constants are bounded above by an absolute constant, that is, <math>W(k)\\leq 6</math> for all <math>k</math>. Kryakin, Gilewicz, and Shevchuk (2002)<ref>{{cite journal|last1=Gilewicz|first1=J.|last2=Kryakin|first2=Yu. V.|last3=Shevchuk|first3=I. A.|title=Boundedness by 3 of the Whitney Interpolation Constant|journal=Journal of Approximation Theory|date=2002|volume=119|issue=2|pages=271–290}}</ref> were able to show that <math>W(k)\\leq 2</math> for <math>k \\leq 82000</math>, and that <math>W(k)\\leq 2+\\frac{1}{e^2}</math> for all <math>k</math>.\n\n==References==\n{{reflist}}\n\n[[Category:Approximation theory]]\n[[Category:Numerical analysis]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "Arakelyan's theorem",
      "url": "https://en.wikipedia.org/wiki/Arakelyan%27s_theorem",
      "text": "In mathematics, '''Arakelyan's theorem''' is a generalization of [[Mergelyan's theorem]] from compact subsets of an open subset of the complex plane to relatively closed subsets of an open subset.\n\n== Theorem ==\nLet Ω be an open subset of ℂ and ''E'' a relatively closed subset of Ω. By Ω<sup>*</sup> is denoted the [[Alexandroff compactification]] of ''Ω''.\n\nArakelyan's theorem states that for every ''f'' continuous in ''E'' and holomorphic in the interior of ''E'' and for every ''ε'' > 0 there exists ''g'' holomorphic in Ω such that |''g''&nbsp;&minus;&nbsp;''f''|&nbsp;<&nbsp;''ε'' on ''E'' if and only if Ω<sup>*</sup>&nbsp;\\&nbsp;''E'' is connected and locally connected.<ref>{{cite book|last1=Gardiner|first1=Stephen J.|title=Harmonic approximation|date=1995|publisher=Cambridge University Press|location=Cambridge|isbn=9780521497992|page=39}}</ref>\n\n== See also ==\n* [[Runge's theorem]]\n* [[Mergelyan's theorem]]\n\n== References ==\n{{reflist}}\n* {{cite journal|last1=Arakeljan|first1=N. U.|authorlink=Norair Arakelian|title=Uniform and tangential approximations by analytic functions|journal=Izv. Akad. Nauk Armjan. SSR Ser. Mat|date=1968|volume=3|pages=273–286}}\n* {{cite book|last1=Arakeljan|first1=N. U|title=Actes, Congrès intern. Math.|date=1970|volume=2|pages=595–600}}\n* {{cite journal|last1=Rosay|first1=Jean-Pierre|last2=Rudin|first2=Walter|title=Arakelian's Approximation Theorem|journal=The American Mathematical Monthly|date=May 1989|volume=96|issue=5|pages=432|doi=10.2307/2325151}}\n\n[[Category:Theorems in complex analysis]]\n[[Category:Theorems in approximation theory]]"
    },
    {
      "title": "Bernstein's theorem (approximation theory)",
      "url": "https://en.wikipedia.org/wiki/Bernstein%27s_theorem_%28approximation_theory%29",
      "text": "In [[approximation theory]], '''Bernstein's theorem'''  is a converse to [[Jackson's inequality|Jackson's theorem]].<ref>{{cite book|last=Achieser|first=N.I.|author-link=Naum Akhiezer|title=Theory of Approximation|year=1956|publisher=Frederick Ungar Publishing Co|location=New York}}</ref> The first results of this type were proved by [[Sergei Bernstein]] in 1912.<ref>{{cite book|last=Bernstein|first=S.N.|author-link=Sergey Bernstein|title=Collected works, 1|year=1952|location=Moscow|pages=11&ndash;104}}</ref>\n\nFor approximation by [[trigonometric polynomials]], the result is as follows:\n\nLet ''f'': [0, 2π]  → '''C''' be a 2''π''-[[periodic function]], and assume ''r'' is a natural number, and 0 < ''α'' < 1. If there exists a number ''C''(''f'') > 0 and a sequence of [[trigonometric polynomial]]s {''P''<sub>''n''</sub>}<sub>''n'' ≥ ''n''<sub>0</sub></sub> such that\n: <math> \\deg\\, P_n = n~, \\quad \\sup_{0 \\leq x \\leq 2\\pi} |f(x) - P_n(x)| \\leq \\frac{C(f)}{n^{r + \\alpha}}~,</math>\nthen ''f'' = ''P''<sub>''n''<sub>0</sub></sub> + ''φ'', where ''φ'' has a bounded ''r''-th derivative which is [[Hölder condition|α-Hölder continuous]].\n\n==See also==\n* [[Bernstein's lethargy theorem]]\n* [[Constructive function theory]]\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Bernstein's Theorem (Approximation Theory)}}\n[[Category:Theorems in approximation theory]]"
    },
    {
      "title": "Carleman's condition",
      "url": "https://en.wikipedia.org/wiki/Carleman%27s_condition",
      "text": "In mathematics, particularly, in [[mathematical analysis|analysis]], '''Carleman's condition'''  gives a sufficient condition for the determinacy of the [[moment problem]]. That is, if a [[measure (mathematics)|measure]] ''&mu;'' satisfies Carleman's condition, there is no other measure ''&nu;'' having the same [[moment (mathematics)|moments]] as ''&mu;''. The condition was discovered by [[Torsten Carleman]] in 1922.<ref>{{harvtxt|Akhiezer|1965}}</ref>\n\n==Hamburger moment problem==\n\nFor the [[Hamburger moment problem]] (the moment problem on the whole real line), the theorem states the following:\n\nLet ''&mu;'' be a [[Measure (mathematics)|measure]] on '''R''' such that all the moments\n\n:<math>m_n = \\int_{-\\infty}^{+\\infty} x^n \\, d\\mu(x)~, \\quad n = 0,1,2,\\cdots</math>\n\nare finite. If\n\n:<math>\\sum_{n=1}^\\infty m_{2n}^{-\\frac{1}{2n}} = + \\infty,</math>\n\nthen the moment problem for ''m''<sub>''n''</sub> is ''determinate''; that is, ''μ'' is the only measure on '''R''' with (''m''<sub>''n''</sub>) as its sequence of moments.\n\n==Stieltjes moment problem==\n\nFor the [[Stieltjes moment problem]], the sufficient condition for determinacy is\n\n:<math>\\sum_{n=1}^\\infty m_{n}^{-\\frac{1}{2n}} = + \\infty. </math>\n\n==Notes==\n{{Reflist}}\n\n==References==\n\n* {{Cite book |first=N. I. |last=Akhiezer |title=The Classical Moment Problem and Some Related Questions in Analysis |publisher=Oliver & Boyd |year=1965|ref=harv }}\n\n[[Category:Mathematical analysis]]\n[[Category:Probability theory]]\n[[Category:Theorems in approximation theory]]"
    },
    {
      "title": "Fejér's theorem",
      "url": "https://en.wikipedia.org/wiki/Fej%C3%A9r%27s_theorem",
      "text": "{{Disputed|date=August 2018}} \nIn mathematics, '''Fejér's theorem''', named for [[Hungary|Hungarian]] [[mathematician]] [[Lipót Fejér]], states that if ''f'':'''R'''&nbsp;→&nbsp;'''C''' is a [[continuous function]] with [[periodic function|period]] 2π, then the [[sequence]] (σ<sub>''n''</sub>) of [[Cesàro mean]]s of the sequence (''s''<sub>''n''</sub>) of [[partial sum]]s of the [[Fourier series]] of ''f'' [[Uniform convergence|converges uniformly]] to ''f'' on [-π,π].\n\nExplicitly,\n:<math>s_n(x)=\\sum_{k=-n}^nc_ke^{ikx},</math>\nwhere\n:<math>c_k=\\frac{1}{2\\pi}\\int_{-\\pi}^\\pi f(t)e^{-ikt}dt,</math>\nand\n:<math>\\sigma_n(x)=\\frac{1}{n}\\sum_{k=0}^{n-1}s_k(x)=\\frac{1}{2\\pi}\\int_{-\\pi}^\\pi f(x-t)F_n(t)dt,</math>\nwith ''F''<sub>''n''</sub> being the ''n''th order [[Fejér kernel]].\n\nA more general form of the theorem applies to functions which are not necessarily continuous {{harv|Zygmund|1968|loc=Theorem III.3.4}}.  Suppose that ''f'' is in ''L''<sup>1</sup>(-π,π).  If the left and right limits ''f''(''x''<sub>0</sub>±0) of ''f''(''x'') exist at ''x''<sub>0</sub>, or if both limits are infinite of the same sign, then\n\n:<math>\\sigma_n(x_0) \\to \\frac{1}{2}\\left(f(x_0+0)+f(x_0-0)\\right).</math>\n\nExistence or divergence to infinity of the Cesàro mean is also implied.  By a theorem of [[Marcel Riesz]], Fejér's theorem holds precisely as stated if the (C, 1) mean σ<sub>''n''</sub> is replaced with [[Cesàro summation|(C, &alpha;) mean]] of the Fourier series {{harv|Zygmund|1968|loc=Theorem III.5.1}}.\n\n==References==\n* {{citation|title=Trigonometric series|first=Antoni|last=Zygmund|authorlink=Antoni Zygmund|publisher=Cambridge University Press|year=1968|publication-date=1988|isbn=978-0-521-35885-9|edition=2nd}}.\n\n{{DEFAULTSORT:Fejer's theorem}}\n[[Category:Fourier series]]\n[[Category:Theorems in approximation theory]]"
    },
    {
      "title": "Hartogs–Rosenthal theorem",
      "url": "https://en.wikipedia.org/wiki/Hartogs%E2%80%93Rosenthal_theorem",
      "text": "In [[mathematics]], the '''Hartogs–Rosenthal theorem''' is a classical result in [[complex analysis]] on the [[uniform approximation]] of continuous functions on compact subsets of the [[complex plane]] by [[rational function]]s. The theorem was proved in 1931 by the German mathematicians [[Friedrich Hartogs]] and [[Arthur Rosenthal]] and has been widely applied, particularly in [[operator theory]].\n\n==Statement of theorem==\nThe Hartogs–Rosenthal theorem states that if ''K'' is a compact subset of the complex plane with [[Lebesgue measure]] zero, then any continuous complex-valued function on ''K'' can be uniformly approximated by rational functions.\n\n==Proof of theorem==\nBy the [[Stone–Weierstrass theorem]] any complex-valued continuous function on ''K'' can be uniformly approximated by a polynomial in <math>z</math> and <math>\\overline{z}</math>.\n\nSo it suffices to show that <math>\\overline{z}</math> can be uniformly approximated by a rational function on ''K''.\n\nLet ''g(z)'' be a [[smooth function]] of compact support on '''C''' equal to 1 on ''K'' and set\n\n:<math> f(z)=g(z)\\cdot \\overline{z}.</math>\n\nBy the [[Cauchy_integral_formula#Smooth_functions|generalized Cauchy integral formula]]\n\n:<math>f(z) =  \\frac{1}{2\\pi i}\\iint_{{C\\backslash K}} \\frac{\\partial f}{\\partial \\bar{w}}\\frac{dw\\wedge d\\bar{w}}{w-z}, </math>\n\nsince ''K'' has measure zero.\n\nRestricting ''z'' to ''K'' and taking [[Riemann sum|Riemann approximating sums]] for the integral on the right hand side yields the required uniform approximation of <math>\\bar{z}</math> by a rational function.<ref>{{harvnb|Conway|2000}}</ref>\n\n==See also==\n*[[Runge's theorem]]\n*[[Mergelyan's theorem]]\n\n==Notes==\n{{reflist}}\n\n==References==\n*{{citation|title=Functions of one complex variable II|series=[[Graduate Texts in Mathematics]]|volume=159|first=John B.|last= Conway|publisher=Springer|year= 1995|isbn=0387944605|page=197}}\n*{{citation|title=A course in operator theory|volume=21|series=[[Graduate Studies in Mathematics]]|first=John B.|last= Conway|publisher=[[American Mathematical Society]]|year= 2000|isbn=0821820656|pages=175–176}}\n*{{citation|first=Theodore W.|last=Gamelin|title=Uniform algebras|edition=2nd|publisher=[[American Mathematical Society]]|year= 2005|\nisbn=0821840495|pages=46–47}}\n*{{citation|first=Friedrichs|last=Hartogs|first2=Arthur|last2=Rosenthal|year=1931|title=Über Folgen analytischer Funktionen\n|journal=[[Mathematische Annalen]]|volume=104|pages=606–610|doi=10.1007/bf01457959 |url=http://gdz.sub.uni-goettingen.de/en/dms/load/img/?PPN=GDZPPN002274736}}\n\n{{DEFAULTSORT:Hartogs-Rosenthal theorem}}\n[[Category:Rational functions]]\n[[Category:Theorems in approximation theory]]\n[[Category:Theorems in complex analysis]]"
    },
    {
      "title": "Krein's condition",
      "url": "https://en.wikipedia.org/wiki/Krein%27s_condition",
      "text": "In [[mathematical analysis]], '''Krein's condition''' provides a necessary and sufficient condition for exponential sums\n\n:<math> \\left\\{ \\sum_{k=1}^n a_k \\exp(i \\lambda_k x), \n\\quad  a_k \\in \\mathbb{C}, \\, \\lambda_k \\geq 0 \\right\\},</math>\n\nto be [[dense (topology)|dense]] in a [[Lp-space#Weighted Lp spaces|weighted L<sub>2</sub> space]] on the real line. It was discovered by [[Mark Krein]] in the 1940s.<ref>{{cite journal|last=Krein|first=M.G.|author-link=Mark Krein|title=On  an  extrapolation  problem  due  to  Kolmogorov|journal=[[Doklady Akademii Nauk SSSR]]|volume= 46|year=1945|pages=306&ndash;309}}</ref> A corollary, also called Krein's condition, provides a sufficient condition for the indeterminacy of the [[moment problem]].<ref>{{eom|id=Krein_condition|first=J.|last=Stoyanov}}</ref><ref>{{cite journal|last=Berg|first=Ch.|title=Indeterminate moment problems and the theory of entire functions|doi=10.1016/0377-0427(95)00099-2|journal=J. Comput. Appl. Math.|volume=65|year=1995|pages=1&ndash;3, 27&ndash;55|mr=1379118}}</ref>\n\n==Statement==\n\nLet ''&mu;'' be an [[Absolutely continuous#Absolute continuity of measures|absolutely continuous]] [[measure (mathematics)|measure]] on the real line, d''&mu;''(''x'') = ''f''(''x'')&nbsp;d''x''. The exponential sums\n\n:<math> \\sum_{k=1}^n a_k \\exp(i \\lambda_k x), \n\\quad a_k \\in \\mathbb{C}, \\, \\lambda_k \\geq 0 </math>\n\nare dense in ''L''<sub>2</sub>(''&mu;'') if and only if\n\n:<math> \\int_{-\\infty}^\\infty \\frac{- \\ln f(x)}{1 + x^2} \\, dx = \\infty. </math>\n\n==Indeterminacy of the moment problem==\n\nLet ''&mu;'' be  as above; assume that all the [[moment (mathematics)|moments]]\n\n:<math> m_n = \\int_{-\\infty}^\\infty x^n d\\mu(x), \\quad n = 0,1,2,\\ldots</math>\n\nof ''&mu;'' are finite. If\n\n:<math> \\int_{-\\infty}^\\infty \\frac{- \\ln f(x)}{1 + x^2} \\, dx < \\infty </math>\n\nholds, then the [[Hamburger moment problem]] for ''&mu;'' is indeterminate; that is, there exists another measure ''&nu;''&nbsp;≠&nbsp;''&mu;'' on '''R''' such that\n\n:<math> m_n = \\int_{-\\infty}^\\infty x^n \\, d\\nu(x), \\quad n = 0,1,2,\\ldots</math>\n\nThis can be derived from the \"only if\" part of Krein's theorem above.<ref>{{Cite book |first=N. I. |last=Akhiezer |author-link=Naum Akhiezer|title=The Classical Moment Problem and Some Related Questions in Analysis |location= |publisher=Oliver & Boyd |year=1965 }}</ref>\n\n===Example===\n\nLet\n\n:<math> f(x) = \\frac{1}{\\sqrt{\\pi}} \\exp \\left\\{ - \\ln^2 x \\right\\};</math>\n\nthe measure d''&mu;''(''x'')&nbsp;=&nbsp;''f''(''x'') d''x'' is called the [[Stieltjes–Wigert polynomials|Stieltjes–Wigert measure]]. Since\n\n:<math> \n\\int_{-\\infty}^\\infty \\frac{- \\ln f(x)}{1+x^2} dx\n = \\int_{-\\infty}^\\infty \\frac{\\ln^2 x + \\ln \\sqrt{\\pi}}{1 + x^2} \\, dx < \\infty, </math>\n\nthe Hamburger moment problem for ''&mu;'' is indeterminate.\n\n==References==\n{{Reflist}}\n\n[[Category:Theorems in analysis]]\n[[Category:Theorems in approximation theory]]"
    },
    {
      "title": "Lethargy theorem",
      "url": "https://en.wikipedia.org/wiki/Lethargy_theorem",
      "text": "In [[mathematics]], a '''lethargy theorem''' is a statement about the distance of points in a metric space from members of a sequence of subspaces; one application in [[numerical analysis]] is to [[approximation theory]], where such theorems quantify the difficulty of approximating general functions by functions of special form, such as [[polynomial]]s.  In more recent work, the convergence of a sequence of operators is studied: these operators generalise the projections of the earlier work.\n\n==Bernstein's lethargy theorem==\n\nLet <math> V_1 \\subset V_2 \\subset \\ldots </math> be a strictly ascending sequence of finite-dimensional linear subspaces of a [[Banach space]] ''X'', and let <math>\\epsilon_1 \\ge \\epsilon_2 \\ge \\ldots</math> be a decreasing sequence of real numbers tending to zero.  Then there exists a point ''x'' in ''X'' such that the distance of ''x'' to ''V''<sub>''i''</sub> is exactly <math>\\epsilon_i</math>.\n\n==See also==\n* [[Bernstein's theorem (approximation theory)]]\n\n==References==\n* {{cite journal | author=S.N. Bernstein | authorlink=Sergei Natanovich Bernstein | title=On the inverse problem of the theory of the best approximation of continuous functions | journal=Sochinenya | volume=II | year=1938 | pages=292–294 }}\n* {{cite book | author=Elliott Ward Cheney | title=Introduction to Approximation Theory | publisher=American Mathematical Society | edition=2nd | year=1982 | isbn=978-0-8218-1374-4 }}\n* {{cite book | editor1-first=Heinz H. | editor1-last=Bauschke | editor2-first=Regina S. | editor2-last=Burachik |editor2-link=Regina Burachik | editor3-first=Patrick L. | editor3-last=Combettes | editor4-first=Veit | editor4-last=Elser | editor5-first=D. Russell | editor5-last=Luke | editor6-first=Henry | editor6-last=Wolkowicz | series=Springer Optimization and Its Applications | number=49 | title=Fixed-Point Algorithms for Inverse Problems in Science and Engineering | year=2011 | isbn=9781441995681 | doi=10.1007/978-1-4419-9569-8 }}\n* {{cite journal | title=Slow convergence of sequences of linear operators I: almost arbitrarily slow convergence | author1=Frank Deutsch | author2=Hein Hundal | journal=[[Journal of Approximation Theory]] | volume=162 | year=2010 | number=9 | pages=1701–1716 | mr=2718892 | doi=10.1016/j.jat.2010.05.001}}\n* {{cite journal | title=Slow convergence of sequences of linear operators II: arbitrarily slow convergence | author1=Frank Deutsch | author2=Hein Hundal | journal=Journal of Approximation Theory | volume=162 | year=2010 | number=9 | pages=1717–1738 | mr=2718893 | doi=10.1016/j.jat.2010.05.002}}\n* {{cite journal | title=The Rate of Convergence in the Method of Alternating Projections | author1=Catalin Badea| author2=Sophie Grivaux| author3=Vladimir MÄuller| year=2011  }}\n** [http://wayback.archive-it.org/all/20110401104837/http://www.math.cas.cz/preprint/pre-211.pdf Preprint]\n\n[[Category:Theorems in approximation theory]]"
    },
    {
      "title": "Mergelyan's theorem",
      "url": "https://en.wikipedia.org/wiki/Mergelyan%27s_theorem",
      "text": "'''Mergelyan's theorem''' is a famous result from [[complex analysis]] proved by the [[Armenian SSR|Armenian]] [[mathematician]] [[Sergey Mergelyan|Sergei Nikitovich Mergelyan]] in 1951. It states the following:\n\nLet ''K'' be a [[compact set|compact subset]] of the [[complex plane]] '''C''' such that '''C'''∖''K'' is [[connected set|connected]]. Then, every [[continuous function]] ''f'' : ''K''<math>\\to</math> '''C''', such that the [[Restriction (mathematics)|restriction]] ''f'' to int(''K'') is [[holomorphic function|holomorphic]], can be approximated [[uniform convergence|uniformly]] on ''K'' with [[polynomial]]s. Here, int(''K'') denotes the [[interior_(topology)|interior]] of ''K''.\n\nMergelyan's theorem is the ultimate development and generalization of the [[Weierstrass approximation theorem]] and [[Runge's theorem]]. It gives the complete solution of the classical problem of approximation by polynomials.\n\nIn the case that '''C'''∖''K'' is ''not'' connected, in the initial approximation problem the polynomials have to be replaced by [[rational function]]s. An important step of the solution of this further [[Padé approximant|rational approximation]] problem was also suggested by Mergelyan in 1952. Further deep results on rational approximation are due to, in particular, [[A. G. Vitushkin]].\n\nWeierstrass and Runge's theorems were put forward in 1885, while Mergelyan's theorem dates from 1951. This rather large time difference is not surprising, as the proof of Mergelyan's theorem is based on a new powerful method created by Mergelyan. After Weierstrass and Runge, many mathematicians (in particular [[Joseph Leonard Walsh|Walsh]], [[Mstislav Keldysh|Keldysh]], and [[Mikhail Lavrentyev|Lavrentyev]]) had been working on the same problem. The method of the proof suggested by Mergelyan is constructive, and remains the only known constructive proof of the result.\n\n==See also==\n* [[Arakelyan's theorem]]\n* [[Hartogs–Rosenthal theorem]]\n\n==References==\n* [[Lennart Carleson]], ''Mergelyan's theorem on uniform polynomial approximation'', Math. Scand., V. 15, (1964) 167–175.\n* Dieter Gaier, ''Lectures on Complex Approximation'', Birkhäuser Boston, Inc. (1987), {{ISBN|0-8176-3147-X}}.\n* W. Rudin, '' Real and Complex Analysis'', McGraw–Hill Book Co., New York, (1987), {{ISBN|0-07-054234-1}}.\n* A. G. Vitushkin, ''Half a century as one day'',  Mathematical events of the twentieth century,  449–473, Springer, Berlin, (2006), {{ISBN|3-540-23235-4}}/hbk.\n\n==External links==\n* {{springer|title=Mergelyan theorem|id=p/m063450}}\n\n[[Category:Theorems in complex analysis]]\n[[Category:Theorems in approximation theory]]"
    },
    {
      "title": "Müntz–Szász theorem",
      "url": "https://en.wikipedia.org/wiki/M%C3%BCntz%E2%80%93Sz%C3%A1sz_theorem",
      "text": "The '''Müntz–Szász theorem''' is a basic result of [[approximation theory]], proved by [[Herman Müntz]] in 1914 and [[Otto Szász]] (1884–1952) in 1916. Roughly speaking, the theorem shows to what extent the [[Weierstrass theorem on polynomial approximation]] can have holes dug into it, by restricting certain coefficients in the polynomials to be zero. The form of the result had been conjectured by [[Sergei Bernstein]] before it was proved.\n\nThe theorem, in a special case, states that a necessary and sufficient condition for the [[monomial]]s\n\n:<math>x^n,\\quad n\\in S\\subset\\mathbb N</math>\n\nto span a [[dense subset]] of the [[Banach space]] ''C''[''a'',''b''] of all [[continuous function]]s with [[complex number]] values on the [[closed interval]] [''a'',''b''] with ''a'' > 0, with the [[uniform norm]], is that the sum\n\n:<math>\\sum_{n\\in S}\\frac{1}{n}\\ </math>\n\nof the reciprocals, taken over ''S'', should [[divergent series|diverge]], i.e. ''S'' is a [[Large set (combinatorics)|large set]]. For an interval [0, ''b''], the [[constant function]]s are necessary: assuming therefore that 0 is in ''S'', the condition on the other exponents is as before.\n\nMore generally, one can take exponents from any [[strictly increasing]] sequence of positive real numbers, and the same result holds. Szász showed that for complex number exponents, the same condition applied to the sequence of [[real part]]s.\n\nThere are also versions for the [[Lp space|''L''<sub>''p''</sub> spaces]].\n\n==References==\n\n*Müntz, Ch. H., ''Über den Approximationssatz von Weierstrass'', (1914) in H. A. Schwarz's Festschrift, Berlin, pp.&nbsp;303–312. [http://quod.lib.umich.edu/u/umhistmath/aca0698.0001.001/316?view=image&size=125 Scanned at University of Michigan]\n*Szász, O.,'' Über die Approximation stetiger Funktionen durch lineare Aggregate von Potenzen'', Math. Ann., 77 (1916), pp.&nbsp;482–496. [http://www.digizeitschriften.de/dms/resolveppn/?PPN=GDZPPN002266563 Scanned at digizeitschriften.de]\n\n{{DEFAULTSORT:Muntz-Szasz Theorem}}\n[[Category:Functional analysis]]\n[[Category:Theorems in approximation theory]]"
    },
    {
      "title": "Stone–Weierstrass theorem",
      "url": "https://en.wikipedia.org/wiki/Stone%E2%80%93Weierstrass_theorem",
      "text": "In [[mathematical analysis]], the '''Weierstrass approximation theorem''' states that every [[continuous function]] defined on a closed [[interval (mathematics)|interval]] {{math|[''a'', ''b'']}} can be [[uniform convergence|uniformly approximated]] as closely as desired by a [[polynomial]] function. Because polynomials are among the simplest functions, and because computers can directly evaluate polynomials, this theorem has both practical and theoretical relevance, especially in [[polynomial interpolation]]. The original version of this result was established by [[Karl Weierstrass]] in [[#Historical works|1885]] using the [[Weierstrass transform]].\n\n[[Marshall H. Stone]] considerably generalized the theorem {{harv|Stone|1937}} and simplified the proof {{harv|Stone|1948}}.  His result is known as the '''Stone–Weierstrass theorem'''. The Stone–Weierstrass theorem generalizes the Weierstrass approximation theorem in two directions both regressive and progressive: instead of the real interval {{math|[''a'', ''b'']}}, an arbitrary [[compact space|compact]] [[Hausdorff space]] {{mvar|X}} is considered, and instead of the [[Algebra over a field|algebra]] of polynomial functions, approximation with elements from more general subalgebras of {{math|C(''X'')}} is investigated.  The Stone–Weierstrass theorem is a vital result in the study of the algebra of [[continuous functions on a compact Hausdorff space]].\n\nFurther, there is a generalization of the Stone–Weierstrass theorem to noncompact [[Tychonoff space]]s, namely, any continuous function on a Tychonoff space is approximated [[compact-open topology|uniformly on compact sets]] by algebras of the type appearing in the Stone–Weierstrass theorem and described below.\n\nA different generalization of Weierstrass' original theorem is [[Mergelyan's theorem]], which generalizes it to functions defined on certain subsets of the [[complex plane]].\n\n== Weierstrass approximation theorem ==\nThe statement of the approximation theorem as originally discovered by Weierstrass is as follows:\n\n:'''Weierstrass Approximation Theorem.''' Suppose {{math| ''f'' }} is a continuous real-valued function defined on the real interval {{math|[''a'', ''b'']}}. For every {{math|''ε'' > 0}}, there exists a polynomial {{math|''p''}} such that for all {{mvar|x}} in {{math|[''a'', ''b'']}}, we have {{math|{{!}} ''f'' (''x'') − ''p''(''x''){{!}} < ''ε''}}, or equivalently, the [[supremum norm]] {{math|{{!!}} ''f''  − ''p''{{!!}} < ''ε''}}.\n\nA constructive proof of this theorem using [[Bernstein polynomial]]s is outlined on that page.\n\n=== Applications ===\nAs a consequence of the Weierstrass approximation theorem, one can show that the space {{math|C[''a'', ''b'']}} is [[separable space|separable]]: the polynomial functions are dense, and each polynomial function can be uniformly approximated by one with [[rational number|rational]] coefficients; there are only [[countable|countably many]] polynomials with rational coefficients. Since {{math|C[''a'', ''b'']}} is [[Hausdorff space|Hausdorff]] and separable it follows that {{math|C[''a'', ''b'']}} has [[cardinality]] equal to {{math|2<sup>ℵ<sub>0</sub></sup>}} — the same cardinality as the [[cardinality of the continuum|cardinality of the reals]].  (Remark: This cardinality result also follows from the fact that a continuous function on the reals is uniquely determined by its restriction to the rationals.)\n\n== Stone–Weierstrass theorem, real version ==\nThe set {{math|C[''a'', ''b'']}} of continuous real-valued functions on {{math|[''a'', ''b'']}}, together with the supremum norm {{math|{{!!}} ''f'' {{!!}} {{=}} sup<sub>''a'' ≤ ''x'' ≤ ''b''</sub> {{!}} ''f'' (''x''){{!}}}}, is a [[Banach algebra]], (that is, an [[associative algebra]] and a [[Banach space]] such that {{math|{{!!}} ''fg''{{!!}} ≤ {{!!}} ''f'' {{!!}}·{{!!}}''g''{{!!}}}} for all {{math| ''f'', ''g''}}). The set of all polynomial functions forms a subalgebra of {{math|C[''a'', ''b'']}} (that is, a [[linear subspace|vector subspace]] of {{math|C[''a'', ''b'']}} that is closed under multiplication of functions), and the content of the Weierstrass approximation theorem is that this subalgebra is [[Topology Glossary|dense]] in {{math|C[''a'', ''b'']}}.\n\nStone starts with an arbitrary compact Hausdorff space {{mvar|X}} and considers the algebra {{math|C(''X'', '''R''')}} of real-valued continuous functions on {{mvar|X}}, with the topology of [[uniform convergence]]. He wants to find subalgebras of {{math|C(''X'', '''R''')}} which are dense. It turns out that the crucial property that a subalgebra must satisfy is that it ''[[separating set|separates points]]'': a set {{mvar|A}} of functions defined on {{mvar|X}} is said to separate points if, for every two different points {{mvar|x}} and {{mvar|y}} in {{mvar|X}} there exists a function {{mvar|p}} in {{mvar|A}} with {{math|''p''(''x'') ≠ ''p''(''y'')}}. Now we may state:\n\n:'''Stone–Weierstrass Theorem (real numbers).''' Suppose {{mvar|X}} is a compact Hausdorff space and {{mvar|A}} is a subalgebra of {{math|C(''X'', '''R''')}} which contains a non-zero constant function. Then {{mvar|A}} is dense in {{math|C(''X'', '''R''')}} [[if and only if]] it separates points.\n\nThis implies Weierstrass’ original statement since the polynomials on {{math|[''a'', ''b'']}} form a subalgebra of {{math|C[''a'', ''b'']}} which contains the constants and separates points.\n\n=== Locally compact version ===\nA version of the Stone–Weierstrass theorem is also true when {{mvar|X}} is only [[locally compact]]. Let {{math|C<sub>0</sub>(''X'', '''R''')}} be the space of real-valued continuous functions on {{mvar|X}} which [[vanish at infinity]]; that is, a continuous function {{math| ''f'' }} is in {{math|C<sub>0</sub>(''X'', '''R''')}} if, for every {{math|''ε'' > 0}}, there exists a compact set {{math|''K'' ⊂ ''X''}} such that {{math| {{!}} ''f'' {{!}}  < ''ε''}} on {{math|''X''&nbsp;\\&nbsp;''K''}}. Again, {{math|C<sub>0</sub>(''X'', '''R''')}} is a [[Banach algebra]] with the [[supremum norm]]. A subalgebra {{mvar|A}} of {{math|C<sub>0</sub>(''X'', '''R''')}} is said to '''vanish nowhere''' if not all of the elements of {{mvar|A}} simultaneously vanish at a point; that is, for every {{mvar|x}} in {{mvar|X}}, there is some {{math| ''f'' }} in {{mvar|A}} such that {{math| ''f'' (''x'') ≠ 0}}. The theorem generalizes as follows:\n\n:'''Stone–Weierstrass Theorem (locally compact spaces).''' Suppose {{mvar|X}} is a ''locally compact'' Hausdorff space and {{mvar|A}} is a subalgebra of {{math|C<sub>0</sub>(''X'', '''R''')}}. Then {{mvar|A}} is dense in {{math|C<sub>0</sub>(''X'', '''R''')}} (given the topology of [[uniform convergence]]) if and only if it separates points and vanishes nowhere.\n\nThis version clearly implies the previous version in the case when {{mvar|X}} is compact, since in that case {{math|C<sub>0</sub>(''X'', '''R''') {{=}} C(''X'', '''R''')}}. There are also more general versions of the Stone–Weierstrass that weaken the assumption of local compactness.<ref name=Willard>{{cite book |first=Stephen |last=Willard |title=General Topology |page=293 |publisher=Addison-Wesley |year=1970 |isbn=0-486-43479-6 }}</ref>\n\n=== Applications ===\nThe Stone–Weierstrass theorem can be used to prove the following two statements which go beyond Weierstrass's result.\n\n* If {{math| ''f'' }} is a continuous real-valued function defined on the set {{math|[''a'', ''b''] × [''c'', ''d'']}} and {{math|''ε'' > 0}}, then there exists a polynomial function {{mvar|p}} in two variables such that {{math|{{!}} ''f'' (''x'', ''y'') − ''p''(''x'', ''y'') {{!}} < ''ε''}} for all {{mvar|x}} in {{math|[''a'', ''b'']}} and {{mvar|y}} in {{math|[''c'', ''d'']}}.{{Citation needed|date=July 2018}}\n* If {{mvar|X}} and {{mvar|Y}} are two compact Hausdorff spaces and {{math|''f'' : ''X'' × ''Y'' → '''R'''}} is a continuous function, then for every {{math|''ε'' > 0}} there exist {{math|''n'' > 0}} and continuous functions {{math| ''f''<sub>1</sub>, ...,  ''f<sub>n</sub>'' }} on {{mvar|X}} and continuous functions {{math|''g''<sub>1</sub>, ..., ''g<sub>n</sub>''}} on {{mvar|Y}} such that {{math|{{!!}} ''f'' − ∑&nbsp;''f<sub>i</sub> g<sub>i</sub>'' {{!!}} < ''ε''}}. {{Citation needed|date=July 2018}}\n\nThe theorem has many other applications to analysis, including:\n\n* [[Fourier series]]: The set of linear combinations of functions {{math|''e<sub>n</sub>''(''x'') {{=}} ''e''<sup>2''πinx''</sup>, ''n'' ∈ '''Z'''}} is dense in {{math|C([0, 1]/{0, 1})}}, where we identify the endpoints of the interval {{math|[0, 1]}} to obtain a circle.  An important consequence of this is that the {{math|''e<sub>n</sub>''}} are an [[orthonormal basis]] of the space [[Lp space|{{math|L<sup>2</sup>([0, 1])}}]] of [[square-integrable function]]s on {{math|[0, 1]}}.\n\n== Stone–Weierstrass theorem, complex version ==\nSlightly more general is the following theorem, where we consider the algebra {{math|C(''X'', '''C''')}} of complex-valued continuous functions on the compact space {{mvar|X}}, again with the topology of uniform convergence. This is a [[C*-algebra]] with the *-operation given by pointwise [[complex conjugation]].\n\n:'''Stone–Weierstrass Theorem (complex numbers).''' Let {{mvar|X}} be a compact Hausdorff space and let {{mvar|S}} be a subset of {{math|C(''X'', '''C''')}} which [[separating set|separates points]]. Then the complex [[unital algebra|unital]] [[*-algebra]] generated by {{mvar|S}} is dense in {{math|C(''X'', '''C''')}}.\n\nThe complex unital *-algebra generated by {{mvar|S}} consists of all those functions that can be obtained from the elements of {{mvar|S}} by throwing in the constant function {{math|1}} and adding them, multiplying them, conjugating them, or multiplying them with complex scalars, and repeating finitely many times.\n\nThis theorem implies the real version, because if a sequence of complex-valued functions uniformly approximate a given function {{math| ''f'' }}, then the real parts of those functions uniformly approximate the real part of {{math| ''f'' }}.  As in the real case, an analog of this theorem is true for locally compact Hausdorff spaces.\n\n== Stone–Weierstrass theorem, quaternion version ==\nFollowing {{harvtxt|John C.Holladay|1957}} : consider the algebra {{math|C(''X'', '''H''')}} of quaternion-valued continuous functions on the compact space {{mvar|X}}, again with the topology of uniform convergence. If a quaternion ''q'' is written in the form ''q''&nbsp;=&nbsp;''a''&nbsp;+&nbsp;''ib'';+&nbsp;''jc''&nbsp;+&nbsp;''kd'' then the '''scalar part''' a is the '''real number''' (''q''&nbsp;−&nbsp;''iqi''&nbsp;−&nbsp;''jqj''&nbsp;−&nbsp;''kqk'')/4. Likewise being the '''scalar part''' of −''qi'', −''qj'' and −''qk'' : b,c and d are respectively the '''real numbers''' (−''qi''&nbsp;−&nbsp;''iq''&nbsp;+&nbsp;''jqk''&nbsp;−&nbsp;''kqj'')/4,\n(−''qj''&nbsp;−&nbsp;''iqk''&nbsp;−&nbsp;''jq''&nbsp;+&nbsp;''kqi'')/4 and (−''qk''&nbsp;+&nbsp;''iqj''&nbsp;−&nbsp;''jqk''&nbsp;−&nbsp;''kq'')/4.  Then we may state :\n\n:'''Stone–Weierstrass Theorem (quaternion numbers).''' Suppose {{mvar|X}} is a compact Hausdorff space and {{mvar|A}} is a subalgebra of {{math|C(''X'', '''H''')}} which contains a non-zero constant function. Then {{mvar|A}} is dense in {{math|C(''X'', '''H''')}} if and only if it [[separating set|separates points]].\n\n== Stone–Weierstrass theorem, C*-algebra version ==\nThe space of complex-valued continuous functions on a compact Hausdorff space ''X'' i.e. C(''X'', '''C''') is the canonical example of a unital [[C*-algebra#Commutative C.2A-algebras|commutative C*-algebra]] <math>\\mathfrak{A}</math>. The space ''X'' may be viewed as the space of pure states on <math>\\mathfrak{A}</math>, with the weak-* topology. Following the above cue, a non-commutative extension of the Stone–Weierstrass theorem, which has remain unsolved, is as follows:\n\n:'''Conjecture.''' If a unital [[C*-algebra]] <math>\\mathfrak{A}</math> has a C*-subalgebra <math>\\mathfrak{B}</math> which separates the pure states of <math>\\mathfrak{A}</math>, then <math>\\mathfrak{A} = \\mathfrak{B}</math>.\n\nIn 1960, Jim Glimm proved a weaker version of the above conjecture.\n\n:'''Stone–Weierstrass theorem (C*-algebras).'''<ref>{{cite journal |first=James |last=Glimm |authorlink=James Glimm |title=A Stone–Weierstrass Theorem for C*-algebras |journal=[[Annals of Mathematics]] |series=Second Series |volume=72 |issue=2 |year=1960 |pages=216–244 [Theorem 1] |jstor=1970133 |doi=10.2307/1970133}}</ref> If a unital C*-algebra <math>\\mathfrak{A}</math> has a C*-subalgebra <math>\\mathfrak{B}</math> which separates the pure state space (i.e. the weak-* closure of the pure states) of <math>\\mathfrak{A}</math>, then <math> \\mathfrak{A}= \\mathfrak{B}</math>.\n\n== Lattice versions ==\nLet {{mvar|X}} be a compact Hausdorff space. Stone's original proof of the theorem used the idea of [[lattice (order)|lattices]] in {{math|C(''X'', '''R''')}}.  A subset {{mvar|L}} of {{math|C(''X'', '''R''')}} is called a [[lattice (order)|lattice]] if for any two elements {{math| ''f'', ''g'' ∈ ''L''}}, the functions {{math|max{ ''f'', ''g''}, min{ ''f'', ''g''} }}also belong to {{mvar|L}}. The lattice version of the Stone–Weierstrass theorem states:\n\n:'''Stone–Weierstrass Theorem (lattices).''' Suppose {{mvar|X}} is a compact Hausdorff space with at least two points and {{mvar|L}} is a lattice in {{math|C(''X'', '''R''')}} with the property that for any two distinct elements {{mvar|x}} and {{mvar|y}} of {{mvar|X}} and any two real numbers {{mvar|a}} and {{mvar|b}} there exists an element {{math| ''f''  ∈ ''L''}} with {{math| ''f'' (''x'') {{=}} ''a''}} and {{math| ''f'' (''y'') {{=}} ''b''}}. Then {{mvar|L}} is dense in {{math|C(''X'', '''R''')}}.\n\nThe above versions of Stone–Weierstrass can be proven from this version once one realizes that the lattice property can also be formulated using the [[absolute value]] {{math|{{!}} ''f'' {{!}}}} which in turn can be approximated by polynomials in {{math| ''f'' }}. A variant of the theorem applies to linear subspaces of {{math|C(''X'', '''R''')}} closed under max {{harv|Hewitt|Stromberg|1965|loc=Theorem 7.29}}:\n\n:'''Stone–Weierstrass Theorem.''' Suppose {{mvar|X}} is a compact Hausdorff space and {{mvar|B}} is a family of functions in {{math|C(''X'', '''R''')}} such that\n:# {{mvar|B}} separates points.\n:# {{mvar|B}} contains the constant function 1.\n:# If {{math| ''f''  ∈ ''B''}} then {{math|''af''  ∈ ''B''}} for all constants {{math|''a'' ∈ '''R'''}}.\n:# If {{math| ''f'',  ''g'' ∈ ''B''}}, then {{math| ''f''  + ''g'', max{ ''f'', ''g''} ∈ ''B''}}.\n:Then {{mvar|B}} is dense in {{math|C(''X'', '''R''')}}.\n\nMore precise information is available:\n\n:Suppose {{mvar|X}} is a compact Hausdorff space with at least two points and {{mvar|L}} is a lattice in {{math|C(''X'', '''R''')}}. The function {{math|''φ'' ∈ C(''X'', '''R''')}} belongs to the [[closure (topology)|closure]] of {{mvar|L}} if and only if for each pair of distinct points ''x'' and ''y'' in {{mvar|X}} and for each {{math|''ε'' > 0}} there exists some {{math| ''f''  ∈ ''L''}} for which {{math|{{!}} ''f'' (''x'') − ''φ''(''x''){{!}} < ''ε''}} and {{math|{{!}} ''f'' (''y'') − ''φ''(''y''){{!}} < ''ε''}}.\n\n== Bishop's theorem ==\nAnother generalization of the Stone–Weierstrass theorem is due to [[Errett Bishop]].  Bishop's theorem is as follows {{harv|Bishop|1961}}:\n\n:Let {{mvar|A}} be a closed subalgebra of the [[Banach space]] {{math|C(''X'', '''C''')}} of continuous complex-valued functions on a compact Hausdorff space {{mvar|X}}.  Suppose that {{math| ''f''  ∈ C(''X'', '''C''')}} has the following property:\n:: {{math| ''f'' {{!}}<sub>''S''</sub> ∈ ''A<sub>S</sub>''}} for every maximal set {{math|''S'' ⊂ ''X''}} such that all real functions of {{math|''A<sub>S</sub>''}} are constant.\n:Then {{math| ''f''  ∈ ''A''}}.\n\n{{harvtxt|Glicksberg|1962}} gives a short proof of Bishop's theorem using the [[Krein–Milman theorem]] in an essential way, as well as the [[Hahn–Banach theorem]] : the process of {{harvtxt|Louis de Branges|1959}}.  See also {{harvtxt|Rudin|1973|loc=§5.7}}.\n\n== Nachbin's theorem ==\nNachbin's theorem gives an analog for Stone–Weierstrass theorem for algebras of complex valued smooth functions on a smooth manifold {{harv|Nachbin|1949}}. Nachbin's theorem is as follows {{harv|Llavona|1986}}:\n\n:Let {{mvar|A}} be a subalgebra of the algebra {{math|C<sup>∞</sup>(''M'')}} of smooth functions on a finite dimensional smooth manifold {{mvar|M}}. Suppose that {{mvar|A}} separates the points of {{mvar|M}} and also separates the tangent vectors of {{mvar|M}}: for each point ''m'' ∈ ''M'' and tangent vector ''v'' at the tangent space at ''m'', there is a ''f'' ∈ {{mvar|A}} such that d''f''(''x'')(''v'') ≠ 0. Then {{mvar|A}} is dense in {{math|C<sup>∞</sup>(''M'')}}.\n\n== See also ==\n*[[Müntz–Szász theorem]].\n* [[Bernstein polynomial]].\n* [[Runge's phenomenon]] shows that finding a polynomial {{mvar|P}} such that {{math| ''f'' (''x'') {{=}} ''P''(''x'')}} for some finely spaced {{math|''x'' {{=}} ''x<sub>n</sub>''}} is a bad way to attempt to find a polynomial approximating {{math| ''f'' }} uniformly.  However, as is shown in [[Walter Rudin]]'s Principles of Mathematical Analysis, one can easily find a polynomial {{mvar|P}} uniformly approximating {{math| ''f'' }} by convolving {{math| ''f'' }} with a polynomial kernel.\n* [[Mergelyan's theorem]], concerning polynomial approximations of complex functions.\n\n== Notes ==\n{{Reflist}}\n\n== References ==\n* {{citation | last =John C.Holladay| title =The Stone–Weierstrass theorem for quaternions| journal = Proc. Amer. Math. Soc. | volume =8| year =1957|url =http://www.ams.org/journals/proc/1957-008-04/S0002-9939-1957-0087047-7/S0002-9939-1957-0087047-7.pdf| doi=10.1090/S0002-9939-1957-0087047-7| pages=656}}.\n* {{citation | last =Louis de Branges |authorlink=Louis de Branges | title =The Stone–Weierstrass theorem| journal = Proc. Amer. Math. Soc. | volume =10| year =1959| pages =822–824 | doi=10.1090/s0002-9939-1959-0113131-7}}.\n* [[Jan Brinkhuis]] & Vladimir Tikhomirov (2005) ''Optimization: Insights and Applications'', [[Princeton University Press]] {{isbn|978-0-691-10287-0}} {{mr|id=2168305}}.\n* {{citation|first=James|last=Glimm|title=A Stone–Weierstrass Theorem for C*-algebras|jstor=1970133|journal=Annals of Mathematics |series=Second Series|volume=72|issue=2|year=1960|pages=216–244|doi=10.2307/1970133}}\n* {{citation|first=Errett|last=Bishop|title=A generalization of the Stone–Weierstrass theorem|journal=Pacific Journal of Mathematics|url=http://projecteuclid.org/DPubS/Repository/1.0/Disseminate?handle=euclid.pjm/1103037116&view=body&content-type=pdf_1|year=1961|volume=11|issue=3|pages=777–783|doi=10.2140/pjm.1961.11.777}}.\n* {{citation|last=Glicksberg|first=Irving|title=Measures Orthogonal to Algebras and Sets of Antisymmetry|journal=Transactions of the American Mathematical Society|year=1962|volume=105|issue=3|pages=415–435|doi=10.2307/1993729|publisher=Transactions of the American Mathematical Society, Vol. 105, No. 3|jstor=1993729}}.\n* {{citation|first1=E|last1=Hewitt|first2=K|last2=Stromberg|title=Real and abstract analysis|year=1965|publisher=Springer-Verlag}}.\n* {{citation|first=Walter|last=Rudin|authorlink=Walter Rudin|title=Principles of mathematical analysis|publisher=McGraw-Hill|year=1976|isbn=978-0-07-054235-8|edition=3rd.}}.\n* {{citation|first=Walter|last=Rudin|authorlink=Walter Rudin|title=Functional analysis|publisher=McGraw-Hill|year=1973|isbn=0-07-054236-8}}.\n* {{citation|first=L.|last=Nachbin|title=Sur les algèbres denses de fonctions diffèrentiables sur une variété|journal=C. R. Acad. Sci. Paris|date=1949|volume=228|pages=1549–1551}}\n* {{citation|first=José G.|last=Llavona|title=Approximation of continuously differentiable functions|date=1986|publisher=North-Holland|location=Amsterdam|isbn=9780080872414}}\n* {{citation | last =JG Burkill| title =Lectures On Approximation By Polynomials| url =http://www.math.tifr.res.in/~publ/ln/tifr16.pdf| }}.\n\n=== Historical works ===\nThe historical publication of Weierstrass (in [[German language]]) is freely available from the digital online archive of the ''[http://bibliothek.bbaw.de/ Berlin Brandenburgische Akademie der Wissenschaften]'':\n\n* K. Weierstrass (1885). Über die analytische Darstellbarkeit sogenannter willkürlicher Functionen einer reellen Veränderlichen. ''Sitzungsberichte der Königlich Preußischen Akademie der Wissenschaften zu Berlin'', 1885 (II).\n:[http://bibliothek.bbaw.de/bibliothek-digital/digitalequellen/schriften/anzeige/index_html?band=10-sitz/1885-2&seite:int=109 Erste Mitteilung] (part 1) pp. 633–639, [http://bibliothek.bbaw.de/bibliothek-digital/digitalequellen/schriften/anzeige/index_html?band=10-sitz/1885-2&seite:int=272 Zweite Mitteilung] (part 2) pp. 789–805.\n\nImportant historical works of Stone include:\n\n* {{citation|first=M. H.|last=Stone|year=1937|authorlink=Marshall Stone|title=Applications of the Theory of Boolean Rings to General Topology|journal=Transactions of the American Mathematical Society|volume=41|issue=3|pages=375–481|doi=10.2307/1989788|publisher=Transactions of the American Mathematical Society, Vol. 41, No. 3|jstor=1989788}}.\n* {{citation|doi=10.2307/3029750|first=M. H.|last=Stone|year=1948|authorlink=Marshall Stone|title=The Generalized Weierstrass Approximation Theorem|journal=Mathematics Magazine|volume=21|issue=4|pages=167–184|jstor=3029750}}; '''21''' (5), 237–254.\n\n== External links ==\n* {{springer|title=Stone–Weierstrass theorem|id=p/s090370}}\n{{authority control}}\n\n{{DEFAULTSORT:Stone-Weierstrass Theorem}}\n[[Category:Continuous mappings]]\n[[Category:Theorems in analysis]]\n[[Category:Theorems in approximation theory]]\n[[Category:1885 in mathematics]]\n[[Category:1937 in mathematics]]"
    },
    {
      "title": "Walsh–Lebesgue theorem",
      "url": "https://en.wikipedia.org/wiki/Walsh%E2%80%93Lebesgue_theorem",
      "text": "The '''Walsh–Lebesgue theorem''' is a famous result from [[harmonic analysis]] proved by the American mathematician [[Joseph L. Walsh]] in 1929, using results proved by [[Henri Lebesgue|Lebesgue]] in 1907.<ref>{{cite journal|author=Walsh, J. L.|title=Über die Entwicklung einer harmonischen Funktion nach harmonischen Polynomen|journal=J. Reine Angew. Math.|year=1928|volume=159|pages=197–209|url=http://eudml.org/doc/149665}}</ref><ref>{{cite journal|author=Walsh, J. L.|title=The approximation of harmonic functions by harmonic polynomials and by harmonic rational functions|journal=Bull. Amer. Math. Soc.|year=1929|volume=35|issue=2|pages=499–544|doi=10.1090/S0002-9947-1929-1501495-4}}</ref><ref>{{cite journal|author=Lebesgue, H.|title=Sur le probléme de Dirichlet|journal=Rendiconti del Circolo Matematico di Palermo|volume=24|issue=1|year=1907|pages=371–402|doi=10.1007/BF03015070}}</ref> The theorem states the following:\n\nLet ''{{math|K}}'' be a [[compact set|compact subset]] of the [[Euclidean plane]] {{math|ℝ<sup>2</sup>}} such the [[Complement (set theory)|relative complement]] of <math>K</math> with respect to {{math|ℝ<sup>2</sup>}} is [[connected set|connected]]. Then, every real-valued [[continuous function]] on <math>\\partial{K}</math> (''i.e.'' the [[Boundary (topology)|boundary]] of ''{{math|K}}'') can be [[Uniform convergence|approximated uniformly]] on <math>\\partial{K}</math> by (real-valued) [[harmonic polynomial]]s in the real variables {{math|x}} and {{math|y}}.<ref>{{cite book|author=Gamelin, Theodore W.|authorlink=Theodore Gamelin|chapter=3.3 Theorem (Walsh-Lebesgue Theorem)|title=Uniform Algebras|year=1984|pages=36–37|publisher=American Mathematical Society|chapter-url=https://books.google.com/books?id=2-K2A7cdORoC&pg=PA36}}</ref>\n\n==Generalizations==\nThe Walsh–Lebesgue theorem has been generalized to [[Riemann surface]]s<ref>{{cite book|author=Bagby, T.|author2=Gauthier, P. M.|chapter=Uniform approximation by global harmonic functions|title=Approximations by solutions of partial differential equations|publisher=Springer|location=Dordrecht|year=1992|chapter-url=https://books.google.com/books?id=vzrsCAAAQBAJ&pg=PA20|pages=15–26 (p. 20)}}</ref> and to [[Real coordinate space|{{math|ℝ<sup>n</sup>}}]].\n\n{{blockquote|This Walsh-Lebesgue theorem has also served as a catalyst for entire chapters in the theory of [[function algebra]]s such as the theory of [[Dirichlet algebra]]s and logmodular algebras.<ref>{{cite book|author=Walsh, J. L.|editor=Rivlin, Theodore J.|editorlink=Theodore J. Rivlin|editor2=Saff, Edward B.|editorlink2=Edward B. Saff|title=Joseph L. Walsh. Selected papers|year=2000|publisher=Springer|pages=249–250|url=https://books.google.com/books?id=Zm6ahqIyF5QC&pg=PA249|isbn=978-0-387-98782-8}}</ref>}}\n\nIn 1974 Anthony G. O'Farrell gave a generalization of the Walsh–Lebesgue theorem by means of the 1964 Browder–Wermer theorem<ref>{{cite journal|author=Browder, A.|authorlink=Andrew Browder|author2=Wermer, J.|authorlink2=John Wermer|title=A method for constructing Dirichlet algebras|journal=Proceedings of the American Mathematical Society|volume=15|issue=4|date=August 1964|pages=546–552|doi=10.2307/2034745|jstor=2034745}}</ref> with related techniques.<ref>{{cite journal|url=http://archive.maths.nuim.ie/staff/aofarrell/preprint/1974agwlt.pdf |doi=10.1017/S0308210500016395|title=A Generalised Walsh-Lebesgue Theorem|journal=Proceedings of the Royal Society of Edinburgh A|volume=73|pages=231–234|year=2012|last1=O'Farrell|first1=A. G}}</ref><ref>{{cite journal|author=O'Farrell, A. G.|title=Five Generalisations of the Weierstrass Approximation Theorem|journal=Proceedings of the Royal Irish Academy, Section A |volume=81|issue=1|year=1981|pages=65–69|url=http://archive.maths.nuim.ie/staff/aof/preprint/19815gotwat.pdf}}</ref><ref>{{cite book|chapter=Theorems of Walsh-Lebesgue Type |first=A. G. |last=O'Farrell |title=Aspects of Contemporary Complex Analysis |editors=D. A. Brannan, J. Clunie |year=1980|pages=461–467|publisher=Academic Press|chapter-url=http://archive.maths.nuim.ie/staff/aof/preprint/1980towlt.pdf}}</ref>\n\n==References==\n<references/>\n\n[[Category:Theorems in harmonic analysis]]\n[[Category:Theorems in approximation theory]]"
    },
    {
      "title": "Wirtinger's representation and projection theorem",
      "url": "https://en.wikipedia.org/wiki/Wirtinger%27s_representation_and_projection_theorem",
      "text": "{{multiple issues|\n{{Expert-subject|Mathematics|date=May 2010}}\n{{Technical|date=May 2010}}\n}}\n\nIn mathematics, '''Wirtinger's representation and projection theorem''' is a [[theorem]] proved by [[Wilhelm Wirtinger]] in 1932 in connection with some problems of [[approximation theory]]. This theorem gives the representation formula for the [[Holomorphic function|holomorphic]] [[Linear subspace|subspace]] <math>\\left.\\right. H_2 </math> of the simple, unweighted holomorphic [[Hilbert space]] <math>\\left.\\right. L^2 </math> of functions [[square-integrable]] over the surface of the unit disc <math>\\left.\\right.\\{z:|z|<1\\} </math> of the [[complex plane]], along with a form of the [[orthogonal projection]] from <math>\\left.\\right. L^2 </math> to <math>\\left.\\right. H_2 </math>.\n\nWirtinger's paper <ref>{{Cite journal\n|first=W. |last= Wirtinger\n|title=Uber eine Minimumaufgabe im Gebiet der analytischen Functionen\n|journal=Monatshefte fur Math. und Phys.\n|volume=39\n|pages=377–384\n|year=1932\n |doi=10.1007/bf01699078}}</ref> contains the following theorem presented also in [[Joseph L. Walsh]]'s well-known monograph\n<ref>{{Cite journal\n|first=J. L.|last= Walsh\n|title=Interpolation and Approximation by Rational Functions in the Complex Domain\n|journal=Amer. Math. Soc. Coll. Publ. XX\n|publisher=Edwards Brothers, Inc.\n|location=Ann Arbor, Michigan\n|year=1956}}</ref>\n(p.&nbsp;150) with a different proof. ''If''  <math>\\left.\\right.\\left. F(z)\\right.</math> ''is of the class'' <math>\\left.\\right. L^2 </math> on <math>\\left.\\right. |z|<1 </math>, ''i.e.\n\n: <math> \\iint_{|z|<1}|F(z)|^2 \\, dS<+\\infty,</math>\n\n''where <math>\\left.\\right. dS </math> is the [[area element]], then the unique function <math>\\left.\\right. f(z)</math> of the holomorphic subclass <math> H_2\\subset L^2 </math>, such that''\n\n: <math> \\iint_{|z|<1}|F(z)-f(z)|^2 \\, dS </math>\n\n''is least, is given by\n\n: <math> f(z)=\\frac1\\pi\\iint_{|\\zeta|<1}F(\\zeta)\\frac{dS}{(1-\\overline\\zeta z)^2},\\quad |z|<1. </math>\n\nThe last formula gives a form for the orthogonal projection from <math>\\left.\\right. L^2 </math> to <math>\\left.\\right. H_2 </math>. Besides, replacement of <math> \\left.\\right. F(\\zeta) </math> by <math>\\left.\\right. f(\\zeta) </math> makes it Wirtinger's representation for all <math>f(z)\\in H_2 </math>. This is an analog of the well-known [[Cauchy integral formula]] with the square of the Cauchy kernel. Later, after the 1950s, a degree of the Cauchy kernel was called [[reproducing kernel]], and the notation <math>\\left.\\right. A^2_0</math> became common for the class <math>\\left.\\right. H_2</math>.\n\nIn 1948 [[Mkhitar Djrbashian]]<ref>{{Cite journal\n|first=M. M.|last=Djrbashian|authorlink=Mkhitar Djrbashian\n|title=On the Representability Problem of Analytic Functions\n|journal=Soobsch. Inst. Matem. i Mekh. Akad. Nauk Arm. SSR\n|volume=2\n|pages=3–40\n|year=1948\n|url=http://math.sci.am/upload/file/ArmenJerbashian/1945-1948.pdf}}</ref> extended Wirtinger's representation and projection to the wider, weighted Hilbert spaces <math>\\left.\\right. A^2_\\alpha </math> of functions <math>\\left.\\right. f(z)</math> holomorphic in <math> \\left.\\right.|z|<1</math>, which satisfy the condition\n\n: <math>\\|f\\|_{A^2_\\alpha}=\\left\\{\\frac1\\pi\\iint_{|z|<1}|f(z)|^2(1-|z|^2)^{\\alpha-1} \\, dS\\right\\}^{1/2}<+\\infty\\text{ for some }\\alpha\\in(0,+\\infty),</math>\n\nand also to some Hilbert spaces of entire functions. The extensions of these results to some weighted <math>\\left.\\right. A^2_\\omega</math> spaces of functions holomorphic in <math>\\left.\\right. |z|<1</math> and similar spaces of entire functions, the unions of which respectively coincide with ''all'' functions holomorphic in  <math>\\left.\\right. |z|<1</math> and the ''whole'' set of entire functions can be seen in.<ref>{{Cite journal\n|first=A. M.|last=Jerbashian\n|title=On the Theory of Weighted Classes of Area Integrable Regular Functions\n|journal=Complex Variables\n|volume=50\n|pages=155–183\n|year=2005\n|doi=10.1080/02781070500032846}}</ref>\n\n==See also==\n* {{Cite journal\n|first=A. M.|last=Jerbashian\n|author2=V. S. Zakaryan\n |title=The Contemporary Development in M. M. Djrbashian Factorization Theory and Related Problems of Analysis\n|journal=Izv. NAN of Armenia, Matematika (English translation: Journal of Contemporary Mathematical Analysis)\n|volume=44|number=6|year=2009}}\n\n==References==\n{{Reflist}}\n\n[[Category:Theorems in complex analysis]]\n[[Category:Theorems in functional analysis]]\n[[Category:Theorems in approximation theory]]"
    },
    {
      "title": "Diophantine approximation",
      "url": "https://en.wikipedia.org/wiki/Diophantine_approximation",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|approximating real numbers with rational numbers}}\nIn [[number theory]], the study of '''Diophantine approximation''' deals with the approximation of [[real number]]s by [[rational number]]s.  It is named after [[Diophantus of Alexandria]].\n\nThe first problem was to know how well a real number can be approximated by rational numbers. For this problem, a rational number ''a''/''b'' is a \"good\" approximation of a real number ''α'' if the absolute value of the difference between ''a''/''b'' and ''α'' may not decrease if ''a''/''b'' is replaced by another rational number with a smaller denominator. This problem was solved during the 18th century by means of [[continued fraction]]s.\n\nKnowing the \"best\" approximations of a given number, the main problem of the field is to find sharp [[upper and lower bounds]] of the above difference, expressed as a function of the [[denominator]].\n\nIt appears that these bounds depend on the nature of the real numbers to be approximated: the lower bound for the approximation of a rational number by another rational number is larger than the lower bound for [[algebraic number]]s, which is itself larger than the lower bound for all real numbers. Thus a real number that may be better approximated than the bound for algebraic numbers is certainly a [[transcendental number]]. This allowed [[Joseph Liouville|Liouville]], in 1844, to produce the first explicit transcendental number. Later, the proofs that [[Pi|{{pi}}]] and ''[[e (mathematical constant)|e]]'' are transcendental were obtained with a similar method.\n\nThus Diophantine approximations and [[transcendental number theory]] are very close areas that share many theorems and methods. Diophantine approximations also have important applications in the study of [[Diophantine equation]]s.\n\n== Best Diophantine approximations of a real number ==\n{{main|Continued fraction#Best rational approximations}}\n\nGiven a real number {{math|''α''}}, there are two ways to define a best Diophantine approximation of {{math|''α''}}. For the first definition,<ref name=\"Khinchin 1997 p.21\">{{harvnb|Khinchin|1997|p=21}}</ref> the rational number {{math|''p''/''q''}} is a ''best Diophantine approximation'' of {{math|''α''}} if\n:<math>\\left|\\alpha -\\frac{p}{q}\\right | < \\left|\\alpha -\\frac{p'}{q'}\\right |,</math>\nfor every rational number {{math|''p'''/''q' ''}} different from {{math|''p''/''q''}} such that {{math|0 < ''q''&prime;&nbsp;≤&nbsp;''q''}}.\n\nFor the second definition,<ref>{{harvnb|Cassels|1957|p=2}}</ref><ref name=Lang9>{{harvnb|Lang|1995|p=9}}</ref> the above inequality is replaced by\n:<math>\\left|q\\alpha -p\\right| < \\left|q^\\prime\\alpha - p^\\prime\\right|.</math>\n\nA best approximation for the second definition is also a best approximation for the first one, but the converse is false.<ref name=Khinchin24>{{harvnb|Khinchin|1997|p=24}}</ref>\n\nThe theory of [[continued fraction]]s allows us to compute the best approximations of a real number: for the second definition, they are the [[convergent (continued fraction)|convergents]] of its expression as a regular continued fraction.<ref name=Lang9/><ref name=Khinchin24/><ref>{{harvnb|Cassels|1957|pp=5–8}}</ref>  For the first definition, one has to consider also the [[Continued fraction#Semiconvergents|semiconvergents]].<ref name=\"Khinchin 1997 p.21\"/>\n\nFor example, the constant ''e'' = 2.718281828459045235... has the (regular) continued fraction representation\n\n:<math>[2;1,2,1,1,4,1,1,6,1,1,8,1,\\ldots\\;].</math>\n\nIts best approximations for the second definition are\n:<math> 3, \\tfrac{8}{3}, \\tfrac{11}{4}, \\tfrac{19}{7}, \\tfrac{87}{32}, \\ldots\\, ,</math>\nwhile, for the first definition, they are\n:<math>3, \\tfrac{5}{2}, \\tfrac{8}{3}, \\tfrac{11}{4}, \\tfrac{19}{7},\n\\tfrac{49}{18}, \\tfrac{68}{25}, \\tfrac{87}{32}, \\tfrac{106}{39}, \\ldots\\, .</math>\n\n==Measure of the accuracy of approximations ==\n\nThe obvious measure of the accuracy of a Diophantine approximation of a real number {{math|''α''}} by a rational number {{math|''p''/''q''}} is <math>\\left|\\alpha-\\frac{p}{q}\\right|.</math> However, this quantity can always be made arbitrarily small by increasing the absolute values of {{math|''p''}} and {{math|''q''}}; thus the accuracy of the approximation is usually estimated by comparing this quantity to some function {{math|''φ''}} of the denominator {{math|''q''}}, typically a negative power of it.\n\nFor such a comparison, one may want upper bounds or lower bounds of the accuracy. A lower bound is typically described by a theorem like \"for every element {{math|''α''}} of some subset of the real numbers and every rational number {{math|''p''/''q''}}, we have <math>\\left|\\alpha-\\frac{p}{q}\\right|>\\phi(q)</math> \". In some cases, \"every rational number\" may be replaced by \"all rational numbers except a finite number of them\", which amounts to multiplying {{math|''φ''}} by some constant depending on {{math|''α''}}.\n\nFor upper bounds, one has to take into account that not all the \"best\" Diophantine approximations provided by the convergents may have the desired accuracy. Therefore, the theorems take the form \"for every element {{math|''α''}} of some subset of the real numbers, there are infinitely many rational numbers {{math|''p''/''q''}} such that <math>\\left|\\alpha-\\frac{p}{q}\\right|<\\phi(q)</math> \".\n\n===Badly approximable numbers===\nA '''badly approximable number''' is an ''x'' for which there is a positive constant ''c'' such that for all rational ''p''/''q'' we have\n\n:<math>\\left|{ x - \\frac{p}{q} }\\right| > \\frac{c}{q^2} \\ . </math>\n\nThe badly approximable numbers are precisely those with [[Restricted partial quotients|bounded partial quotients]].<ref name=Bug245>{{harvnb|Bugeaud|2012|p=245}}</ref>\n\n== Lower bounds for Diophantine approximations ==\n\n=== Approximation of a rational by other rationals ===\nA rational number <math>\\alpha =\\frac{a}{b}</math> may be obviously and perfectly approximated by <math>\\tfrac{p_i}{q_i} = \\tfrac{i\\,a}{i \\,b}</math> for every positive integer ''i''.\n\nIf <math>\\tfrac{p}{q} \\not= \\alpha = \\tfrac{a}{b}\\,,</math> we have\n:<math> \\left|\\frac{a}{b} - \\frac{p}{q}\\right| = \\left|\\frac{aq-bp}{bq}\\right| \\ge \\frac{1}{bq},</math>\nbecause <math>|aq-bp|</math> is a positive integer and is thus not lower than 1. Thus the accuracy of the approximation is bad relative to irrational numbers (see next sections).\n\nIt may be remarked that the preceding proof uses a variant of the [[pigeon hole principle]]: a non-negative integer that is not 0 is not smaller than 1. This apparently trivial remark is used in almost every proof of lower bounds for Diophantine approximations, even the most sophisticated ones.\n\nIn summary, a rational number is perfectly approximated by itself, but is badly approximated by any other rational number.\n\n=== Approximation of algebraic numbers, Liouville's result ===\n{{main|Liouville number}}\n\nIn the 1840s, [[Joseph Liouville]] obtained the first lower bound for the approximation of [[algebraic number]]s: If ''x'' is an irrational algebraic number of degree ''n'' over the rational numbers, then there exists a constant {{nowrap|''c''(''x'') > 0}} such that\n\n:<math> \\left| x- \\frac{p}{q} \\right| > \\frac{c(x)}{q^{n}}</math>\n\nholds for all integers ''p'' and ''q'' where {{nowrap|''q'' > 0}}.\n\nThis result allowed him to produce the first proven example of a transcendental number, the [[Liouville constant]]\n:<math>\n\\sum_{j=1}^\\infty 10^{-j!} = 0.110001000000000000000001000\\ldots\\,,\n</math>\nwhich does not satisfy Liouville's theorem, whichever degree ''n'' is chosen.\n\nThis link between Diophantine approximations and transcendental number theory continues to the present day. Many of the proof techniques are shared between the two areas.\n\n=== Approximation of algebraic numbers, Thue–Siegel–Roth theorem ===\n{{main|Thue–Siegel–Roth theorem}}\n\nOver more than a century, there were many efforts to improve Liouville's theorem: every improvement of the bound enables us to prove that more numbers are transcendental. The main improvements are due to {{harvs|first=Axel|last=Thue|authorlink=Axel Thue|year=1909|txt}}, {{harvs|frst=Carl Ludwig|last=Siegel|authorlink=Carl Ludwig Siegel|year=1921|txt}}, {{harvs|first=Freeman|last=Dyson|authorlink=Freeman Dyson|year=1947|txt}}, and {{harvs|first=Klaus|last=Roth|authorlink=Klaus Roth|year=1955|txt}}, leading finally to the Thue–Siegel–Roth theorem: If {{math|''x''}} is an irrational algebraic number and {{math|''ε''}} a (small) positive real number, then there exists a  positive constant {{math|''c''(''x'', ''ε'')}} such that\n:<math>\n    \\left| x- \\frac{p}{q} \\right|>\\frac{c(x, \\varepsilon)}{q^{2+\\varepsilon}}\n</math>\nholds for every integer {{math|''p''}} and {{math|''q''}} such that {{math|''q'' > 0}}.\n\nIn some sense, this result is optimal, as the theorem would be false with ''ε''=0. This is an immediate consequence of the upper bounds described below.\n\n=== Simultaneous approximations of algebraic numbers ===\n{{main|Subspace theorem}}\nSubsequently, [[Wolfgang M. Schmidt]] generalized this to the case of simultaneous approximations, proving that: If {{math|''x''<sub>1</sub>, ..., ''x''<sub>''n''</sub>}} are algebraic numbers such that {{math|1, ''x''<sub>1</sub>, ..., ''x''<sub>''n''</sub>}} are linearly independent over the rational numbers and {{math|''ε''}} is any given positive real number, then there are only finitely many rational {{math|''n''}}-tuples {{math|(''p''<sub>1</sub>/''q'', ..., ''p''<sub>''n''</sub>/''q'')}} such that\n:<math>|x_i-p_i/q|<q^{-(1+1/n+\\varepsilon)},\\quad i=1,\\ldots,n.</math>\n\nAgain, this result is optimal in the sense that one may not remove {{math|''ε''}} from the exponent.\n\n=== Effective bounds ===\nAll preceding lower bounds are not [[effective results in number theory|effective]], in the sense that the proofs do not provide any way to compute the constant implied in the statements. This means that one cannot use the results or their proofs to obtain bounds on the size of solutions of related Diophantine equations. However, these techniques and results can often be used to bound the number of solutions of such equations.\n\nNevertheless, a refinement of [[Baker's theorem]] by Feldman provides an effective bound: if ''x'' is an algebraic number of degree ''n'' over the rational numbers, then there exist effectively computable constants ''c''(''x'')&nbsp;>&nbsp;0 and 0&nbsp;<&nbsp;''d''(''x'')&nbsp;<&nbsp;''n'' such that\n\n:<math>\\left| x- \\frac{p}{q} \\right|>\\frac{c(x)}{|q|^{d(x)}} </math>\n\nholds for all rational integers.\n\nHowever, as for every effective version of Baker's theorem, the constants ''d'' and 1/''c'' are so large that this effective result cannot be used in practice.\n\n== Upper bounds for Diophantine approximations ==\n\n===General upper bound ===\n{{main | Hurwitz's theorem (number theory)}}\n\nThe first important result about upper bounds for Diophantine approximations is [[Dirichlet's approximation theorem]], which implies that, for every irrational number {{math|''α''}}, there are infinitely many fractions <math>\\tfrac{p}{q}\\;</math> such that\n: <math>\\left|\\alpha-\\frac{p}{q}\\right| < \\frac{1}{q^2}\\,.</math>\n\nThis implies immediately that one cannot suppress the {{math|''ε''}} in the statement of Thue-Siegel-Roth theorem.\n\nOver the years, this theorem has been improved until the following theorem of [[Émile Borel]] (1903).<ref>{{harvnb|Perron|1913|loc=Chapter 2, Theorem 15}}</ref> For every irrational number {{math|''α''}}, there are infinitely many fractions <math>\\tfrac{p}{q}\\;</math> such that\n: <math>\\left|\\alpha-\\frac{p}{q}\\right| < \\frac{1}{\\sqrt{5}q^2}\\,.</math>\n\nTherefore, <math>\\frac{1}{\\sqrt{5}\\, q^2}</math> is an upper bound for the Diophantine approximations of any irrational number.\nThe constant in this result may not be further improved without excluding some irrational numbers (see below).\n\n=== Equivalent real numbers ===\n\n'''Definition''': Two real numbers <math>x,y</math> are called  ''equivalent''<ref>{{harvnb|Hurwitz|1891|p=284}}</ref><ref>{{harvnb|Hardy|Wright|1979|loc=Chapter 10.11}}</ref> if there are integers <math>a,b,c,d\\;</math> with <math>ad-bc = \\pm 1\\;</math> such that:\n:<math>y = \\frac{ax+b}{cx+d}\\, .</math>\n\nSo equivalence is defined by an integer [[Möbius transformation]] on the real numbers, or by a member of the [[Modular group]] <math>\\text{SL}_2^{\\pm}(\\Z)</math>, the set of invertible 2 &times; 2 matrices over the integers. Each rational number is equivalent to 0; thus the rational numbers are an [[equivalence class]] for this relation.\n\nThe equivalence may be read on the regular continued fraction representation, as shown by the following theorem of [[Serret]]:\n\n'''Theorem''': Two irrational numbers ''x'' and ''y'' are equivalent if and only there exist two positive integers ''h'' and ''k'' such that the regular [[continued fraction]] representations of ''x'' and ''y''\n:<math>x=[u_0; u_1, u_2, \\ldots]\\, ,</math>\n:<math>y=[v_0; v_1, v_2, \\ldots]\\, ,</math>\nverify\n:<math>u_{h+i}=v_{k+i}</math>\nfor every non negative integer ''i''.<ref>See {{harvnb|Perron|1929|loc=Chapter 2, Theorem 23, p. 63}}</ref>\n\nThus, except for a finite initial sequence, equivalent numbers have the same continued fraction representation.\n\n===Lagrange spectrum ===\n{{main|Markov spectrum}}\nAs said above, the constant in Borel's theorem may not improved, as shown by [[Adolf Hurwitz]] in 1891.<ref>{{harvnb|Hardy|Wright|1979|p=164}}</ref>\nLet <math>\\phi = \\tfrac{1+\\sqrt{5}}{2}</math> be the [[golden ratio]].\nThen for any real constant ''c'' with <math>c > \\sqrt{5}\\;</math> there are only a finite number of rational numbers {{math|''p''/''q''}} such that \n:<math>\\left|\\phi-\\frac{p}{q}\\right| < \\frac{1}{c\\, q^2}.</math>\n\nHence an improvement can only be achieved, if the numbers which are equivalent to <math>\\phi</math> are excluded. More precisely:<ref>{{harvnb|Cassels|1957|p=11}}</ref><ref>{{harvnb|Hurwitz|1891}}</ref>\nFor every irrational number <math>\\alpha</math>, which is not equivalent to <math>\\phi</math>, there are infinite many fractions <math>\\tfrac{p}{q}\\;</math> such that\n\n: <math>\\left|\\alpha-\\frac{p}{q}\\right| < \\frac{1}{\\sqrt{8} q^2}.</math>\n\nBy successive exclusions — next one must exclude the numbers equivalent to <math>\\sqrt 2</math> — of more and more classes of equivalence, the lower bound can be further enlarged.\nThe values which may be generated in this way are ''Lagrange numbers'', which are part of the [[Markov spectrum|Lagrange spectrum]]. \nThey converge to the number 3 and are related to the [[Markov number]]s.<ref>{{harvnb|Cassels|1957|p=18}}</ref><ref>See [http://www.math.jussieu.fr/~miw/articles/pdf/IntroductionDiophantineMethods.pdf Michel Waldschmidt: ''Introduction to Diophantine methods irrationality and transcendence''], pp 24–26.</ref>\n\n== Khinchin's theorem and extensions == <!-- [[Khinchin's theorem on Diophantine approximations]] links here -->\n\nLet <math>\\psi</math> be a non-increasing function from the positive integers to the positive real numbers. A real number ''x'' (not necessarily algebraic) is called <math>\\psi</math>-''approximable'' if there exist infinitely many rational numbers  ''p''/''q'' such that\n\n:<math>\\left| x- \\frac{p}{q} \\right| < \\frac{\\psi(q)}{|q|}.</math>\n\n[[Aleksandr Khinchin]] proved in 1926 that if the series <math>\\sum_{q} \\psi(q) </math> diverges, then almost every real number (in the sense of [[Lebesgue measure]]) is <math>\\psi</math>-approximable, and if the series converges, then almost every real number is not <math>\\psi</math>-approximable.\n\n{{harvtxt|Duffin|Schaeffer|1941}} proved a more general theorem that implies Khinchin's result, and made a conjecture now known by their name as the [[Duffin–Schaeffer conjecture]]. {{harvtxt|Beresnevich|Velani|2006}} proved that a [[Hausdorff measure]] analogue of the Duffin–Schaeffer conjecture is equivalent to the original Duffin–Schaeffer conjecture, which is a priori weaker.\n\n=== Hausdorff dimension of exceptional sets ===\n\nAn important example of a function <math>\\psi</math> to which Khinchin's theorem can be applied is the function <math>\\psi_c(q) = q^{-c}</math>, where ''c''&nbsp;>&nbsp;1 is a real number. For this function, the relevant series converges and so Khinchin's theorem tells us that almost every point is not <math>\\psi_c</math>-approximable. Thus, the set of numbers which are <math>\\psi_c</math>-approximable forms a subset of the real line of Lebesgue measure zero. The Jarník-Besicovitch theorem, due to [[Vojtech Jarnik|V. Jarník]] and [[Abram Samoilovitch Besicovitch|A. S. Besicovitch]], states that the [[Hausdorff dimension]] of this set is equal to <math>1/c</math>.<ref>{{harvnb|Bernik|Beresnevich|Götze|Kukso|2013|p=24}}</ref> In particular, the set of numbers which are <math>\\psi_c</math>-approximable for some <math>c > 1</math> (known as the set of ''very well approximable numbers'') has Hausdorff dimension one, while the set of numbers which are <math>\\psi_c</math>-approximable for all <math>c > 1</math> (known as the set of [[Liouville number]]s) has Hausdorff dimension zero.\n\nAnother important example is the function <math>\\psi_\\epsilon(q) = \\epsilon q^{-1}</math>, where <math>\\epsilon > 0</math> is a real number. For this function, the relevant series diverges and so Khinchin's theorem tells us that almost every number is <math>\\psi_\\epsilon</math>-approximable. This is the same as saying that every such number is ''well approximable'', where a number is called well approximable if it is not badly approximable. So an appropriate analogue of the Jarník-Besicovitch theorem should concern the Hausdorff dimension of the set of badly approximable numbers. And indeed, V. Jarník proved that the Hausdorff dimension of this set is equal to one. This result was improved by [[Wolfgang M. Schmidt|W. M. Schmidt]], who showed that the set of badly approximable numbers is ''incompressible'', meaning that if <math>f_1,f_2,\\ldots</math> is a sequence of [[Lipschitz continuity#Lipschitz manifolds|bi-Lipschitz]] maps, then the set of numbers ''x'' for which <math>f_1(x),f_2(x),\\ldots</math> are all badly approximable has Hausdorff dimension one. Schmidt also generalized Jarník's theorem to higher dimensions, a significant achievement because Jarník's argument is essentially one-dimensional, depending on the apparatus of continued fractions.\n\n== Uniform distribution ==\nAnother topic that has seen a thorough development is the theory of [[equidistributed sequence|uniform distribution mod 1]]. Take a sequence ''a''<sub>1</sub>, ''a''<sub>2</sub>, ... of real numbers and consider their ''fractional parts''. That is, more abstractly, look at the sequence in [[Quotient group|R/Z]], which is a circle. For any interval ''I'' on the circle we look at the proportion of the sequence's elements that lie in it, up to some integer ''N'', and compare it to the proportion of the circumference occupied by ''I''. ''Uniform distribution'' means that in the limit, as ''N'' grows, the proportion of hits on the interval tends to the 'expected' value. [[Hermann Weyl]] proved a basic result showing that this was equivalent to bounds for exponential sums formed from the sequence. This showed that Diophantine approximation results were closely related to the general problem of cancellation in exponential sums, which occurs throughout [[analytic number theory]] in the bounding of error terms.\n\nRelated to uniform distribution is the topic of [[irregularities of distribution]], which is of a [[combinatorics|combinatorial]] nature.\n\n== Unsolved problems ==\nThere are still simply-stated unsolved problems remaining in Diophantine approximation, for example the ''[[Littlewood conjecture]]'' and the ''[[Lonely runner conjecture]]''.\nIt is also unknown if there are algebraic numbers with unbounded coefficients in their continued fraction expansion.\n\n== Recent developments ==\nIn his plenary address at the [[International Mathematical Congress]] in Kyoto (1990), [[Grigory Margulis]] outlined a broad program rooted in [[ergodic theory]] that allows one to prove number-theoretic results using the dynamical and ergodic properties of actions of subgroups of [[semisimple Lie group]]s. The work of D.Kleinbock, G.Margulis, and their collaborators demonstrated the power of this novel approach to classical problems in Diophantine approximation. Among its notable successes are the proof of the decades-old [[Oppenheim conjecture]] by Margulis, with later extensions by Dani and Margulis and Eskin–Margulis–Mozes, and the proof of Baker and Sprindzhuk conjectures in the Diophantine approximations on manifolds by Kleinbock and Margulis. Various generalizations of the above results of [[Aleksandr Khinchin]] in metric Diophantine approximation have also been obtained within this framework.\n\n==See also==\n\n* [[Davenport–Schmidt theorem]]\n* [[Duffin–Schaeffer conjecture]]\n* [[Heilbronn set]]\n* [[Low-discrepancy sequence]]\n\n==Notes==\n{{Reflist|30em}}\n\n==References==\n{{refbegin|30em}}\n*{{cite journal |zbl=1148.11033 |last1=Beresnevich |first1=Victor |last2=Velani |first2=Sanju |title=A mass transference principle and the Duffin-Schaeffer conjecture for Hausdorff measures |journal=[[Annals of Mathematics]] |volume=164 |issue=3 |year=2006 |pages=971–992 |doi=10.4007/annals.2006.164.971|ref=harv|arxiv=math/0412141 }}\n*{{cite book\n | last1 = Bernik | first1 = V.\n | last2 = Beresnevich | first2 = V.\n | last3 = Götze | first3 = F.\n | last4 = Kukso | first4 = O.\n | editor1-last = Eichelsbacher | editor1-first = Peter\n | editor2-last = Elsner | editor2-first = Guido\n | editor3-last = Kösters | editor3-first = Holger\n | editor4-last = Löwe | editor4-first = Matthias\n | editor5-last = Merkl | editor5-first = Franz\n | editor6-last = Rolles | editor6-first = Silke\n | contribution = Distribution of algebraic numbers and metric theory of Diophantine approximation\n | doi = 10.1007/978-3-642-36068-8_2\n | location = Heidelberg\n | mr = 3079136\n | pages = 23–48\n | publisher = Springer\n | series = Springer Proceedings in Mathematics & Statistics\n | title = Limit Theorems in Probability, Statistics and Number Theory: In Honor of Friedrich Götze\n | volume = 42\n | year = 2013\n | ref = harv}}\n* {{cite book |last=Bugeaud |first=Yann |title=Distribution modulo one and Diophantine approximation |series=Cambridge Tracts in Mathematics |volume=193 |location=Cambridge |publisher=[[Cambridge University Press]] |year=2012 |isbn=978-0-521-11169-0 |zbl=1260.11001|ref=harv}}\n* {{cite book |first=J. W. S. |last=Cassels |authorlink=J. W. S. Cassels |title=An introduction to Diophantine approximation |series=Cambridge Tracts in Mathematics and Mathematical Physics |volume=45 |publisher=[[Cambridge University Press]] |year=1957 |ref=harv}}\n*{{cite journal |zbl=0025.11002 |last1=Duffin |first1=R. J. |last2=Schaeffer |first2=A. C. |title=Khintchine's problem in metric diophantine approximation |journal=[[Duke Mathematical Journal]] |volume=8 |issue=2 |pages=243–255 |year=1941 |issn=0012-7094 |doi=10.1215/s0012-7094-41-00818-9|ref=harv}}\n*{{cite journal | last1=Dyson | first1=Freeman J. | author1-link=Freeman Dyson | title=The approximation to algebraic numbers by rationals | doi= 10.1007/BF02404697 | mr=0023854 | zbl=0030.02101 | year=1947 | journal=[[Acta Mathematica]] | issn=0001-5962 | volume=79 | pages=225–240 | ref=harv}}\n*{{cite book |first=G. H. |last=Hardy |author1link=G. H. Hardy |first2=E. M. |last2=Wright |author2link=E. M. Wright |title=An Introduction to the Theory of Numbers |edition=5th |year=1979 |publisher=Oxford University Press |isbn=978-0-19-853170-8 |mr=568909 |ref=harv|title-link=An Introduction to the Theory of Numbers }}\n*{{cite journal |first=A. |last=Hurwitz |author-link=Adolf Hurwitz |title=Ueber die angenäherte Darstellung der Irrationalzahlen durch rationale Brüche |trans-title=On the approximate representation of irrational numbers by rational fractions |language=German |journal=Mathematische Annalen |volume=39 |year=1891 |issue=2 |pages=279–284 |mr=1510702 |doi=10.1007/BF01206656 |ref=harv}}\n* {{cite book |first=A. Ya. |last=Khinchin |authorlink=Aleksandr Khinchin |title=Continued Fractions |publisher=Dover |year=1997 |origyear=1964 |isbn=0-486-69630-8 |ref=harv}}\n*{{cite journal |last1=Kleinbock |first1=D. Y. |last2=Margulis |first2=G. A. |author2-link= Grigory Margulis |title=Flows on homogeneous spaces and Diophantine approximation on manifolds |journal=Ann. Math. |volume=148 |issue=1 |year=1998 |pages=339–360 |mr=1652916 |zbl=0922.11061 |doi=10.2307/120997 |jstor=120997|arxiv=math/9810036 }}\n* {{cite book |first=Serge |last=Lang |authorlink=Serge Lang |title=Introduction to Diophantine Approximations |edition=New expanded |publisher=[[Springer-Verlag]] |year=1995 |isbn=0-387-94456-7 |zbl=0826.11030 |ref=harv}}\n* {{cite book |first=G. A. |last=Margulis |authorlink=Grigory Margulis |chapter=Diophantine approximation, lattices and flows on homogeneous spaces |title=A panorama of number theory or the view from Baker's garden |editor1-last=Wüstholz |editor1-first=Gisbert |editor1-link=Gisbert Wüstholz |pages=280–310 |publisher=[[Cambridge University Press]] |location=Cambridge |year=2002 |mr=1975458 |zbl= |isbn=0-521-80799-9 }}\n* {{cite book |first=Oskar |last=Perron |authorlink=Oskar Perron |year=1913 |title=Die Lehre von den Kettenbrüchen |trans-title=The Theory of Continued Fractions |language=German |edition= |publisher=B. G. Teubner |location=Leipzig |url=https://books.google.com/books?id=Yjs4AAAAMAAJ |ref=harv}}\n* {{cite book |first=Oskar |last=Perron |authorlink=Oskar Perron |year=1929 |title=Die Lehre von den Kettenbrüchen |trans-title=The Theory of Continued Fractions |language=German |edition=2nd |location=Chelsea |url=http://catalog.hathitrust.org/Record/009514653 |ref=harv}}\n*{{cite journal | last1=Roth | first1=Klaus Friedrich | author1-link=Klaus Friedrich Roth |title=Rational approximations to algebraic numbers | doi=10.1112/S0025579300000644 | mr=0072182 | year=1955 | journal=[[Mathematika]] | issn=0025-5793 | volume=2 | pages=1–20, 168 | zbl=0064.28501|ref=harv}}\n* {{cite book |zbl=0421.10019 |last=Schmidt |first=Wolfgang M. |authorlink=Wolfgang M. Schmidt |edition=1996 |title=Diophantine approximation |series=Lecture Notes in Mathematics |volume=785 |location=Berlin-Heidelberg-New York |publisher=[[Springer-Verlag]] |year=1980 |isbn=3-540-09762-7 |ref=harv}}\n* {{cite book |last=Schmidt |first=Wolfgang M. |authorlink=Wolfgang M. Schmidt |title=Diophantine approximations and Diophantine equations |series=Lecture Notes in Mathematics |volume=1467 |publisher=[[Springer-Verlag]] |year=1996 |edition=2nd |isbn=3-540-54058-X |zbl=0754.11020 |ref=harv}}\n*{{cite journal | last1=Siegel | first1=Carl Ludwig | author1-link=Carl Ludwig Siegel | title=Approximation algebraischer Zahlen  | doi=10.1007/BF01211608 | year=1921 | journal=[[Mathematische Zeitschrift]] | issn=0025-5874 | volume=10 | issue=3 | pages=173–213 | ref=harv| url=https://zenodo.org/record/1538156 }}\n* {{cite book |last=Sprindzhuk |first=Vladimir G. |title=Metric theory of Diophantine approximations |others=Transl. from the Russian and ed. by Richard A. Silverman. With a foreword by Donald J. Newman |series=Scripta Series in Mathematics |publisher=John Wiley & Sons |year=1979 |isbn=0-470-26706-2 |mr=0548467 |zbl=0482.10047}}\n*{{cite journal | last1=Thue | first1=A. | author1-link=Axel Thue | title=Über Annäherungswerte algebraischer Zahlen | url=http://resolver.sub.uni-goettingen.de/purl?PPN243919689_0135 | year=1909 | journal=[[Journal für die reine und angewandte Mathematik]] | issn=0075-4102 | volume=1909 | issue=135 | pages=284–305 | doi=10.1515/crll.1909.135.284 | ref=harv}}\n{{refend}}\n\n== External links ==\n* [http://people.math.jussieu.fr/~miw/articles/pdf/HCMUNS10.pdf Diophantine Approximation: historical survey]. From ''Introduction to Diophantine methods'' course by [[Michel Waldschmidt]].\n* {{springer|title=Diophantine approximations|id=p/d032600}}\n\n{{Authority control}}\n\n{{DEFAULTSORT:Diophantine Approximation}}\n[[Category:Diophantine approximation|*]]"
    },
    {
      "title": "Auxiliary function",
      "url": "https://en.wikipedia.org/wiki/Auxiliary_function",
      "text": "In [[mathematics]], '''auxiliary functions''' are an important construction in [[transcendental number theory]].  They are [[Function (mathematics)|functions]] that appear in most proofs in this area of mathematics and that have specific, desirable properties, such as taking the value zero for many arguments, or having a zero of high [[Multiplicity (mathematics)|order]] at some point.<ref>Waldschmidt (2008).</ref>\n\n==Definition==\n\nAuxiliary functions are not a rigorously defined kind of function, rather they are functions which are either explicitly constructed or at least shown to exist and which provide a contradiction to some assumed hypothesis, or otherwise prove the result in question.  Creating a function during the course of a proof in order to prove the result is not a technique exclusive to transcendence theory, but the term \"auxiliary function\" usually refers to the functions created in this area.\n\n==Explicit functions==\n\n===Liouville's transcendence criterion===\n\nBecause of the naming convention mentioned above, auxiliary functions can be dated back to their source simply by looking at the earliest results in transcendence theory.  One of these first results was [[Joseph Liouville|Liouville's]] proof that [[transcendental numbers]] exist when he showed that the so called [[Liouville number]]s were transcendental.<ref>Liouville (1844).</ref>  He did this by discovering a transcendence criterion which these numbers satisfied.  To derive this criterion he started with a general [[algebraic number]] α and found some property that this number would necessarily satisfy. The auxiliary function he used in the course of proving this criterion was simply the [[Minimal polynomial (field theory)|minimal polynomial]] of α, which is the [[Irreducibility (mathematics)|irreducible]] polynomial ''f'' with integer coefficients such that ''f''(α)&nbsp;=&nbsp;0.  This function can be used to estimate how well the algebraic number α can be estimated by [[rational number]]s ''p''/''q''.  Specifically if α has degree ''d'' at least two then he showed that\n\n:<math>\\left|f\\left(\\frac{p}{q}\\right)\\right|\\geq\\frac{1}{q^d},</math>\n\nand also, using the [[mean value theorem]], that there is some constant depending on α, say ''c''(α), such that\n\n:<math>\\left|f\\left(\\frac{p}{q}\\right)\\right| \\leq c(\\alpha)\\left|\\alpha-\\frac{p}{q}\\right|.</math>\n\nCombining these results gives a property that the algebraic number must satisfy; therefore any number not satisfying this criterion must be transcendental.\n\nThe auxiliary function in Liouville's work is very simple, merely a polynomial that vanishes at a given algebraic number.  This kind of property is usually the one that auxiliary functions satisfy.  They either vanish or become very small at particular points, which is usually combined with the assumption that they do not vanish or can't be too small to derive a result.\n\n===Fourier's proof of the irrationality of ''e''===\n\nAnother simple, early occurrence is in [[Joseph Fourier|Fourier's]] proof of the irrationality of ''e'',<ref>Hermite (1873).</ref> though the notation used usually disguises this fact.  Fourier's proof used the power series of the [[exponential function]]:\n:<math>e^x=\\sum_{n=0}^{\\infty} \\frac{x^n}{n!}.</math>\nBy truncating this power series after, say, ''N''&nbsp;+&nbsp;1 terms we get a polynomial with rational coefficients of degree ''N'' which is in some sense \"close\" to the function ''e''<sup>''x''</sup>.  Specifically if we look at the auxiliary function defined by the remainder:\n:<math>R(x)=e^x-\\sum_{n=0}^{N} \\frac{x^n}{n!}</math>\nthen this function—an [[exponential polynomial]]—should take small values for ''x'' close to zero.  If ''e'' is a rational number then by letting ''x''&nbsp;=&nbsp;1 in the above formula we see that ''R''(1) is also a rational number.  However, Fourier proved that ''R''(1) could not be rational by eliminating every possible denominator.  Thus ''e'' cannot be rational.\n\n===Hermite's proof of the irrationality of ''e''<sup>''r''</sup>===\n\n[[Hermite]] extended the work of Fourier by approximating the function ''e''<sup>''x''</sup> not with a polynomial but with a [[rational function]], that is a quotient of two polynomials.  In particular he chose polynomials ''A''(''x'') and ''B''(''x'') such that the auxiliary function ''R'' defined by\n\n:<math>R(x)=B(x)e^x-A(x)</math>\n\ncould be made as small as he wanted around ''x''&nbsp;=&nbsp;0.  But if ''e''<sup>''r''</sup> were rational then ''R''(''r'') would have to be rational with a particular denominator, yet Hermite could make ''R''(''r'') too small to have such a denominator, hence a contradiction.\n\n===Hermite's proof of the transcendence of ''e''===\n\nTo prove that ''e'' was in fact transcendental, Hermite took his work one step further by approximating not just the function ''e''<sup>''x''</sup>, but also the functions ''e''<sup>''kx''</sup> for integers ''k''&nbsp;=&nbsp;1,...,''m'', where he assumed ''e'' was algebraic with degree ''m''.  By approximating ''e''<sup>''kx''</sup> by rational functions with integer coefficients and with the same denominator, say ''A''<sub>''k''</sub>(''x'')&nbsp;/&nbsp;''B''(''x''), he could define auxiliary functions ''R''<sub>''k''</sub>(''x'') by\n\n:<math>R_k(x)=B(x)e^{kx}-A_k(x).</math>\nFor his contradiction Hermite supposed that ''e'' satisfied the polynomial equation with integer coefficients ''a''<sub>0</sub>&nbsp;+&nbsp;''a''<sub>1</sub>''e''&nbsp;+&nbsp;...&nbsp;+&nbsp;''a''<sub>''m''</sub>''e''<sup>''m''</sup>&nbsp;=&nbsp;0.  Multiplying this expression through by ''B''(1) he noticed that it implied\n\n:<math>R=a_0+a_1 R_1(1) + \\cdots +a_m R_m(1)=a_1 A_1(1)+ \\cdots +a_m A_m(1).</math>\n\nThe right hand side is an integer and so, by estimating the auxiliary functions and proving that 0&nbsp;<&nbsp;|''R''|&nbsp;<&nbsp;1 he derived the necessary contradiction.\n\n==Auxiliary functions from the pigeonhole principle==\n{{main|Siegel's lemma}}\nThe auxiliary functions sketched above can all be explicitly calculated and worked with.  A breakthrough by [[Axel Thue]] and [[Carl Ludwig Siegel]] in the twentieth century was the realisation that these functions don't necessarily need to be explicitly known &ndash; it can be enough to know they exist and have certain properties.  Using the [[Pigeonhole Principle]] Thue, and later Siegel, managed to prove the existence of auxiliary functions which, for example, took the value zero at many different points, or took high order zeros at a smaller collection of points.  Moreover they proved it was possible to construct such functions without making the functions too large.<ref>Thue  (1977) and Siegel (1929).</ref>  Their auxiliary functions were not explicit functions, then, but by knowing that a certain function with certain properties existed, they used its properties to simplify the transcendence proofs of the nineteenth century and give several new results.<ref>Siegel (1932).</ref>\n\nThis method was picked up on and used by several other mathematicians, including [[Alexander Gelfond]] and [[Theodor Schneider]] who used it independently to prove the [[Gelfond–Schneider theorem]].<ref>Gel'fond (1934) and Schneider (1934).</ref>  [[Alan Baker (mathematician)|Alan Baker]] also used the method in the 1960s for his work on linear forms in logarithms and ultimately [[Baker's theorem]].<ref>Baker and Wüstholz (2007).</ref>  Another example of the use of this method from the 1960s is outlined below.\n\n===Auxiliary polynomial theorem===\nLet β equal the cube root of ''b/a'' in the equation ''ax''<sup>3</sup> + ''bx''<sup>3</sup> = ''c'' and assume ''m'' is an integer that satisfies ''m'' + 1 > 2''n''/3 ≥ ''m'' ≥ 3 where ''n'' is a positive integer.\n\nThen there exists\n\n: <math>F(X,Y) = P(X) + Y*Q(X)</math>\n\nsuch that\n\n: <math>\\sum_{i=0}^{m+n} u_i X^i = P(X),</math>\n\n: <math>\\sum_{i=0}^{m+n} v_i X^i = Q(X).</math>\n\nThe auxiliary polynomial theorem states\n\n: <math>\\max_{0 \\le i \\le m+n} {(|u_i|,|v_i|)}\\le 2b^{9(m+n)}.</math>\n\n===A theorem of Lang===\n{{main|Schneider–Lang theorem}}\nIn the 1960s [[Serge Lang]] proved a result using this non-explicit form of auxiliary functions.  The theorem implies both the [[Hermite–Lindemann theorem|Hermite–Lindemann]] and [[Gelfond–Schneider theorem]]s.<ref>Lang (1966).</ref>  The theorem deals with a [[number field]] ''K'' and [[meromorphic]] functions ''f''<sub>1</sub>,...,''f''<sub>''N''</sub> of [[Entire function|order]] at most ''ρ'', at least two of which are algebraically independent, and such that if we differentiate any of these functions then the result is a polynomial in all of the functions.  Under these hypotheses the theorem states that if there are ''m'' distinct [[complex number]]s ω<sub>1</sub>,...,ω<sub>''m''</sub> such that ''f''<sub>''i''</sub>&nbsp;(ω<sub>''j''&nbsp;</sub>) is in ''K'' for all combinations of ''i'' and ''j'', then ''m'' is bounded by\n:<math>m\\leq 20\\rho [K:\\mathbb{Q}].</math>\n\nTo prove the result Lang took two algebraically independent functions from ''f''<sub>1</sub>,...,''f''<sub>''N''</sub>, say ''f'' and ''g'', and then created an auxiliary function which was simply a polynomial ''F'' in ''f'' and ''g''.  This auxiliary function could not be explicitly stated since ''f'' and ''g'' are not explicitly known.  But using [[Siegel's lemma]] Lang showed how to make ''F'' in such a way that it vanished to a high order at the ''m'' complex numbers\nω<sub>1</sub>,...,ω<sub>''m''</sub>.  Because of this high order vanishing it can be shown that a high-order derivative of ''F'' takes a value of small size one of the ω<sub>''i''</sub>s, \"size\" here referring to an algebraic property of a number.  Using the [[maximum modulus principle]] Lang also found a separate way to estimate the absolute values of derivatives of ''F'', and using standard results comparing the size of a number and its absolute value he showed that these estimates were contradicted unless the claimed bound on ''m'' holds.\n\n==Interpolation determinants==\n\nAfter the myriad of successes gleaned from using existent but not explicit auxiliary functions, in the 1990s Michel Laurent introduced the idea of interpolation determinants.<ref>Laurent (1991).</ref>  These are alternants &ndash; determinants of matrices of the form\n:<math>\\mathcal{M}=\\left(\\varphi_i(\\zeta_j)\\right)_{1\\leq i,j\\leq N}</math>\nwhere φ<sub>''i''</sub> are a set of functions interpolated at a set of points ζ<sub>''j''</sub>.  Since a determinant is just a polynomial in the entries of a matrix, these auxiliary functions succumb to study by analytic means.  A problem with the method was the need to choose a basis before the matrix could be worked with.  A development by Jean-Benoît Bost removed this problem with the use of [[Arakelov theory]],<ref>Bost (1996).</ref> and research in this area is ongoing.  The example below gives an idea of the flavour of this approach.\n\n===A proof of the Hermite–Lindemann theorem===\n\nOne of the simpler applications of this method is a proof of the real version of the [[Lindemann–Weierstrass theorem|Hermite–Lindemann theorem]].  That is, if α is a non-zero, real algebraic number, then ''e''<sup>α</sup> is transcendental.  First we let ''k'' be some natural number and ''n'' be a large multiple of ''k''.  The interpolation determinant considered is the determinant '''Δ''' of the ''n''<sup>4</sup>&times;''n''<sup>4</sup> matrix\n:<math>\\left(\\{\\exp(j_2x)x^{j_1-1}\\}^{(i_1-1)}\\Big|_{x=(i_2-1)\\alpha}\\right).</math>\nThe rows of this matrix are indexed by 1&nbsp;≤&nbsp;''i''<sub>1</sub>&nbsp;≤&nbsp;''n''<sup>4</sup>/''k'' and 1&nbsp;≤&nbsp;''i''<sub>2</sub>&nbsp;≤&nbsp;''k'', while the columns are indexed by 1&nbsp;≤&nbsp;''j''<sub>1</sub>&nbsp;≤&nbsp;''n''<sup>3</sup> and 1&nbsp;≤&nbsp;''j''<sub>2</sub>&nbsp;≤&nbsp;''n''.  So the functions in our matrix are monomials in ''x'' and ''e''<sup>''x''</sup> and their derivatives, and we are interpolating at the ''k'' points 0,α,2α,...,(''k''&nbsp;&minus;&nbsp;1)α.  Assuming that ''e''<sup>α</sup> is algebraic we can form the number field '''Q'''(α,''e''<sup>α</sup>) of degree ''m'' over '''Q''', and then multiply '''Δ''' by a suitable [[denominator]] as well as all its images under the embeddings of the field '''Q'''(α,''e''<sup>α</sup>) into '''C'''.  For algebraic reasons this product is necessarily an integer, and using arguments relating to [[Wronskian]]s it can be shown that it is non-zero, so its absolute value is an integer Ω&nbsp;≥&nbsp;1.\n\nUsing a version of the [[mean value theorem]] for matrices it is possible to get an analytic bound on Ω as well, and in fact using [[Big O notation|big-O]] notation we have\n:<math>\\Omega=O\\left(\\exp\\left(\\left(\\frac{m+1}{k}-\\frac{3}{2}\\right)n^8\\log n\\right)\\right).</math>\nThe number ''m'' is fixed by the degree of the field '''Q'''(α,''e''<sup>α</sup>), but ''k'' is the number of points we are interpolating at, and so we can increase it at will.  And once ''k''&nbsp;>&nbsp;2(''m''&nbsp;+&nbsp;1)/3 we will have Ω&nbsp;→&nbsp;0, eventually contradicting the established condition Ω&nbsp;≥&nbsp;1.  Thus ''e''<sup>α</sup> cannot be algebraic after all.<ref>Adapted from Pila (1993).</ref>\n\n==Notes==\n{{Reflist}}\n\n==References==\n* {{cite web | last=Waldschmidt | first=Michel | title=An Introduction to Irrationality and Transcendence Methods | url=http://www.math.jussieu.fr/~miw/articles/pdf/AWSLecture1.pdf}}\n* {{cite journal | last=Liouville | first=Joseph | authorlink=Joseph Liouville | title=Sur des classes très étendues de quantités dont la valeur n'est ni algébrique, ni même réductible à des irrationnelles algébriques | journal=J. Math. Pures Appl. | volume=18 | pages=883–885, and 910–911 | year=1844}}\n* {{cite journal | last=Hermite | first=Charles | authorlink=Charles Hermite | title=Sur la fonction exponentielle | journal=C. R. Acad. Sci. Paris | volume=77 | year=1873}}\n* {{cite book | last=Thue | first=Axel | authorlink=Axel Thue | title=Selected Mathematical Papers | publisher=Universitetsforlaget | location=Oslo | year=1977}}\n* {{cite journal | last=Siegel | first=Carl Ludwig | authorlink=Carl Ludwig Siegel | title=Über einige Anwendungen diophantischer Approximationen | journal=Abhandlungen Akad. Berlin | volume=1 | page=70 | year=1929}}\n* {{cite journal | last=Siegel | first=Carl Ludwig | title=Über die Perioden elliptischer Funktionen | journal=Journal für die reine und angewandte Mathematik | volume=167 | pages=62–69 | year=1932 |doi=10.1515/crll.1932.167.62}}\n* {{cite journal | last=Gel'fond | first=A. O. | authorlink=Alexander Gelfond | title=Sur le septième Problème de D. Hilbert | journal=Izv. Akad. Nauk SSSR | volume=7 | pages=623–630 | year=1934}}\n* {{cite journal | last=Schneider | first=Theodor | authorlink=Theodor Schneider | title=Transzendenzuntersuchungen periodischer Funktionen. I. Transzendend von Potenzen | journal=J. reine angew. Math. | volume=172 | pages=65–69 | year=1934}}\n* {{Citation | last1=Baker | first1=Alan | authorlink=Alan Baker (mathematician) | last2=Wüstholz | first2=G. | title=Logarithmic forms and Diophantine geometry | periodical=New Mathematical Monographs | volume=9 | publisher=Cambridge University Press | page=198 | year=2007}}\n* {{cite book | last=Lang | first=Serge | authorlink=Serge Lang | title=Introduction to Transcendental Numbers | publisher=Addison–Wesley Publishing Company | year=1966}}\n* {{cite journal | last=Laurent | first=Michel | title=Sur quelques résultats récents de transcendance | journal=Astérisque | volume=198–200 | pages=209–230 | year=1991}}\n* {{cite journal | last=Bost | first=Jean-Benoît | title=Périodes et isogénies des variétés abéliennes sur les corps de nombres (d'après D. Masser et G. Wüstholz) | journal=Astérisque | volume=237 | page=795 | year=1996}}\n* {{cite journal |authorlink=Jonathan Pila | last=Pila | first=Jonathan | title=Geometric and arithmetic postulation of the exponential function | journal=J. Austral. Math. Soc. |series=A | volume=54 | pages=111–127 | year=1993 | doi=10.1017/s1446788700037022}}\n\n{{DEFAULTSORT:Auxiliary Function}}\n[[Category:Number theory]]\n[[Category:Diophantine approximation]]"
    },
    {
      "title": "Davenport–Schmidt theorem",
      "url": "https://en.wikipedia.org/wiki/Davenport%E2%80%93Schmidt_theorem",
      "text": "In [[mathematics]], specifically the area of [[Diophantine approximation]], the '''Davenport–Schmidt theorem''' tells us how well a certain kind of [[real number]] can be approximated by another kind.  Specifically it tells us that we can get a good approximation to irrational numbers that are not quadratic by using either [[quadratic irrational]]s or simply [[rational number]]s.  It is named after [[Harold Davenport]] and [[Wolfgang M. Schmidt]].\n\n==Statement==\nGiven a number α which is either rational or a quadratic irrational, we can find unique integers ''x'', ''y'', and ''z'' such that ''x'', ''y'', and ''z'' are not all zero, the first non-zero one among them is positive, they are relatively prime, and we have\n\n:<math>x\\alpha^2 +y\\alpha +z=0.</math>\n\nIf α is a quadratic irrational we can take ''x'', ''y'', and ''z'' to be the coefficients of its [[minimal polynomial (field theory)|minimal polynomial]].  If α is rational we will have ''x''&nbsp;=&nbsp;0.  With these integers uniquely determined for each such α we can define the ''height'' of α to be\n\n:<math>H(\\alpha)=\\max\\{|x|,|y|,|z|\\}.</math>\n\nThe theorem then says that for any real number ξ which is neither rational nor a quadratic irrational, we can find infinitely many real numbers α which ''are'' rational or quadratic irrationals and which satisfy\n\n:<math>|\\xi-\\alpha|<CH(\\alpha)^{-3}\\max(1,\\xi^2),</math>\n\nwhere ''C'' is any real number satisfying ''C''&nbsp;&gt;&nbsp;160/9.<ref>H. Davenport, Wolfgang M. Schmidt, \"''Approximation to real numbers by quadratic irrationals'',\" Acta Arithmetica '''13''', (1967).</ref>\n\nWhile the theorem is related to [[Thue–Siegel–Roth theorem|Roth's theorem]], its real use lies in the fact that it is [[Effective results in number theory|effective]], in the sense that the constant ''C'' can be worked out for any given ξ.\n\n==Notes==\n<references/>\n\n==References==\n* [[Wolfgang M. Schmidt]]. ''Diophantine approximation''. Lecture Notes in Mathematics 785. Springer. (1980 [1996 with minor corrections])\n* Wolfgang M. Schmidt.''Diophantine approximations and Diophantine equations'', Lecture Notes in Mathematics, Springer Verlag 2000\n\n==External links==\n*{{planetmath reference|id=4151|title=Davenport-Schmidt theorem}}\n\n{{DEFAULTSORT:Davenport-Schmidt theorem}}\n[[Category:Diophantine approximation]]\n[[Category:Theorems in number theory]]"
    },
    {
      "title": "Dirichlet's approximation theorem",
      "url": "https://en.wikipedia.org/wiki/Dirichlet%27s_approximation_theorem",
      "text": "In [[number theory]], '''Dirichlet's theorem on Diophantine approximation''', also called '''Dirichlet's approximation theorem''', states that for any [[real numbers]] <math> \\alpha </math> and <math> N </math>, with <math> 1 \\leq N </math>, there exists integers <math> p </math> and <math> q </math> such that <math> 1 \\leq q \\leq N </math> and\n\n:<math> \\left | q \\alpha -p \\right | \\leq \\frac{1}{[N]+1} < \\frac{1}{N}.  </math>\n\nHere <math> [N] </math> represents the [[integer part]] of <math> N </math>.\nThis is a fundamental result in [[Diophantine approximation]], showing that any real number has a sequence of good rational approximations: in fact an immediate consequence is that for a given irrational α, the inequality\n\n:<math> \\left | \\alpha -\\frac{p}{q} \\right | < \\frac{1}{q^2} </math>\n\nis satisfied by infinitely many integers ''p'' and ''q''. This corollary also shows that the [[Thue–Siegel–Roth theorem]], a result in the other direction, provides essentially the tightest possible bound, in the sense that the bound on rational approximation of [[algebraic number]]s cannot be improved by increasing the exponent beyond 2.\n\n==Simultaneous version==\n\nThe simultaneous version of the Dirichlet's approximation theorem states that given real numbers <math>\\alpha_1, \\ldots, \\alpha_d</math> and a natural number <math>N</math> then there are integers <math>p_1, \\ldots, p_d, q\\in\\Z,1\\le q\\leq N</math> such that <math>\\left|\\alpha_i-\\frac{p_i}q \\right| \\le \\frac1{qN^{1/d}}.</math>\n\n==Method of proof==\nThis theorem is a consequence of the [[pigeonhole principle]]. [[Peter Gustav Lejeune Dirichlet]] who proved the result used the same principle in other contexts (for example, the [[Pell equation]]) and by naming the principle (in German) popularized its use, though its status in textbook terms comes later.<ref>http://jeff560.tripod.com/p.html for a number of historical references.</ref> The method extends to simultaneous approximation.<ref>{{Springer|id=d/d032940|title=Dirichlet theorem}}</ref>\n\nAnother simple proof of the Dirichlet's approximation theorem is based on [[Minkowski's theorem|Minkowski's Theorem]] applied to the set <math>S = \\left\\{ (x,y) \\in \\R^2; -N-\\frac{1}{2} \\leq x \\leq N+\\frac{1}{2}, \\vert \\alpha x - y \\vert \\leq \\frac{1}{N} \\right\\} </math>. Since the volume of <math>S</math> is greater than <math>4</math>, [[Minkowski's theorem|Minkowski's Theorem]] establishes the existence of a non-trivial point with integral coordinates. This proof extends naturally to simultaneous approximations by considering the set: <math>S = \\left\\{ (x,y_1, \\dots, y_d) \\in \\R^{1+d}; -N-\\frac{1}{2} \\le x \\le N+\\frac{1}{2}, |\\alpha_i x - y_i| \\le \\frac{1}{N^{1/d}} \\right\\} </math>.\n\n==See also==\n*[[Dirichlet's theorem on arithmetic progressions]]\n*[[Hurwitz's theorem (number theory)]]\n*[[Heilbronn set]]\n\n==Notes==\n{{Reflist}}\n\n==References==\n* [[Wolfgang M. Schmidt]]. ''Diophantine approximation''. Lecture Notes in Mathematics 785. Springer. (1980 [1996 with minor corrections])\n* Wolfgang M. Schmidt.''Diophantine approximations and Diophantine equations'', Lecture Notes in Mathematics, Springer Verlag 2000\n\n==External links==\n*{{PlanetMath|urlname=DirichletsApproximationTheorem|title=Dirichlet's Approximation Theorem}}\n\n[[Category:Diophantine approximation]]\n[[Category:Theorems in number theory]]"
    },
    {
      "title": "Discrepancy of hypergraphs",
      "url": "https://en.wikipedia.org/wiki/Discrepancy_of_hypergraphs",
      "text": "'''Discrepancy of hypergraphs''' is an area of [[discrepancy theory]].\n\n== Hypergraph discrepancies in two colors ==\n\nIn the classical setting, we aim at partitioning the [[Vertex (graph theory)|vertices]] of a [[hypergraph]] <math>\\mathcal{H}=(V, \\mathcal{E})</math> into two classes in such a way that ideally each hyperedge contains the same number of vertices in both classes. A partition into two classes can be represented by a coloring <math>\\chi \\colon V \\rightarrow \\{-1, +1\\}</math>. We call -1 and +1 ''colors''. The color-classes <math>\\chi^{-1}(-1)</math> and <math>\\chi^{-1}(+1)</math> form the corresponding partition. For a hyperedge <math>E \\in \\mathcal{E}</math>, set\n:<math>\\chi(E) := \\sum_{v\\in E} \\chi(v).</math>\nThe ''discrepancy of <math>\\mathcal{H}</math> with respect to <math>\\chi</math>'' and the ''discrepancy of <math>\\mathcal{H}</math>'' are defined by\n:<math>disc(\\mathcal{H},\\chi) := \\max_{E \\in \\mathcal{E}} |\\chi(E)|,</math>\n:<math>disc(\\mathcal{H}) := \\min_{\\chi:V\\rightarrow\\{-1,+1\\}} disc(\\mathcal{H}, \\chi).</math>\nThese notions as well as the term 'discrepancy' seem to have appeared for the first time in a paper of [[József Beck|Beck]].<ref name=\"beck_roth_estimate\">J. Beck: \"Roth's estimate of the discrepancy of integer sequences is nearly sharp\", page 319-325. [[Combinatorica]], 1, 1981</ref> Earlier results on this problem include the famous lower bound on the discrepancy of arithmetic progressions by Roth<ref>K. F. Roth: \"Remark concerning integer sequences\", pages 257–260. Acta Arithmetica 9, 1964</ref> and upper bounds for this problem and other results by [[Paul Erdős|Erdős]] and Spencer<ref>J. Spencer: \"A remark on coloring integers\", pages 43–44. Canad. Math. Bull. 15, 1972.</ref><ref>P. Erdős and J. Spencer: \"Imbalances in k-colorations\", pages 379–385. Networks 1, 1972.</ref> and Sárközi (described on p.&nbsp;39).<ref>P. Erdős and J. Spencer: \"Probabilistic Methods in Combinatorics.\" Akadémia Kiadó, Budapest, 1974.</ref> At that time, discrepancy problems were called quasi-[[Ramsey theory|Ramsey]] problems.\n\nTo get some intuition for this concept, let's have a look at a few examples.\n\n* If all edges of <math>\\mathcal{H}</math> intersect trivially, i.e. <math>E_1 \\cap E_2 = \\emptyset</math> for any two distinct edges <math>E_1, E_2 \\in \\mathcal{E}</math>, then the discrepancy is zero, if all edges have even cardinality, and one, if there is an odd cardinality edge.\n* The other extreme is marked by the ''complete hypergraph'' <math>(V, 2^V)</math>. In this case the discrepancy is <math>\\lceil \\frac{1}{2} |V|\\rceil</math>. Any 2-coloring will have a color class of at least this size, and this set is also an edge. On the other hand, any coloring <math>\\chi</math> with color classes of size <math>\\lceil \\frac{1}{2} |V|\\rceil</math> and <math>\\lfloor \\frac{1}{2} |V|\\rfloor</math> proves that the discrepancy is not larger than <math>\\lceil \\frac{1}{2} |V|\\rceil</math>. It seems that the discrepancy reflects how chaotic the hyperedges of <math>\\mathcal{H}</math> intersect. Things are not that easy, however, as the following example shows.\n* Set <math>n=4k</math>, <math>k \\in \\mathcal{N}</math> and <math>\\mathcal{H}_n = ([n], \\{E \\subseteq [n] \\mid | E \\cap [2k]| = | E \\setminus [2k]|\\})</math>. Now <math>\\mathcal{H}_n</math> has many (more than <math>\\binom{n/2}{n/4}^2 = \\Theta(\\frac 1 n 2^n)</math>) complicatedly intersecting edges, but discrepancy zero.\n\nThe last example shows that we cannot expect to determine the discrepancy by looking at a single parameter like the number of hyperedges. Still, the size of the hypergraph yields first upper bounds.\n\n== Theorems ==\n\n* <math>disc(\\mathcal{H}) \\leq \\sqrt{2n \\ln (2m)}.</math>\nwith n the number of vertices and m the number of edges.\n\nThe proof is a simple application of the probabilistic method:\nLet <math>\\chi:V \\rightarrow \\{-1,1\\}</math> be a random coloring, i.e. we have\n:<math>\\Pr(\\chi(v) = -1) = \\Pr(\\chi(v) = 1) = \\frac{1}{2}</math>\nindependently for all <math>v \\in V</math>. Since <math>\\chi(E) = \\sum_{v \\in E} \\chi(v)</math> is a sum of independent -1, 1 random variables. So we have <math>\\Pr(|\\chi(E)|>\\lambda)<2 \\exp(-\\lambda^2/(2n))</math> for all <math>E \\subseteq V</math> and <math>\\lambda \\geq 0</math>. Put <math>\\lambda = \\sqrt{2n \\ln (2m)}</math> for convenience. Then\n:<math>\\Pr(disc(\\mathcal{H},\\chi)> \\lambda) \\leq \\sum_{E \\in \\mathcal{E}} \\Pr(|\\chi(E)| > \\lambda) < 1.</math>\nSince a random coloring with positive probability has discrepancy at most <math>\\lambda</math>, in particular, there are colorings that have discrepancy at most <math>\\lambda</math>. Hence <math>disc(\\mathcal{H}) \\leq \\lambda. \\  \\Box</math>\n\n* ''For any hypergraph <math>\\mathcal{H}</math> such that <math>m \\geq n</math> we have <math>disc(\\mathcal{H}) = O(\\sqrt{n}).</math>''\n\nTo prove this, a much more sophisticated approach using the entropy function was necessary.\nOf course this is particularly interesting for <math>m = O(n)</math>. In the case <math>m=n</math>, <math>disc(\\mathcal{H}) \\leq 6 \\sqrt{n}</math> can be shown for n large enough. Therefore, this result is usually known to as 'Six Standard Deviations Suffice'. It is considered to be one of the milestones of discrepancy theory.  The entropy method has seen numerous other applications, e.g. in the proof of the tight upper bound for the arithmetic progressions of [[Jiří Matoušek (mathematician)|Matoušek]] and Spencer<ref>J. Matoušek and J. Spencer: \"Discrepancy in arithmetic progressions\", pages 195–204. J. Amer. Math. Soc. 9, 1996.</ref> or the upper bound in terms of the primal shatter function due to Matoušek.<ref>J. Matoušek: \"Tight upper bound for the discrepancy of half-spaces\", pages 593–601. Discrepancy and Computational Geometry 13, 1995.</ref>\n\n* ''Assume that each vertex of <math>\\mathcal{H}</math> is contained in at most t edges. Then''\n:<math>disc(\\mathcal{H}) < 2t</math>\n\nThis result, the [[Beck–Fiala theorem]], is due to Beck and Fiala.<ref>J. Beck and T. Fiala: \"Integer making theorems\", pages 1–8. Discrete Applied Mathematics 3, 1981.</ref> They bound the discrepancy by the maximum degree of <math>\\mathcal{H}</math>.\nIt is a famous open problem whether this bound can be improved asymptotically (modified versions of the original proof give ''2t-1'' or even ''2t-3''). Beck and Fiala conjectured that <math>disc(\\mathcal{H}) = O(\\sqrt t)</math>, but little progress has been made so far in this direction. Bednarchak and Helm<ref>D. Bednarchak and M. Helm: \"A note on the Beck-Fiala theorem\", pages 147–149. Combinatorica 17, 1997.</ref> and Helm<ref>M. Helm: \"On the Beck-Fiala theorem\", page 207.  Discrete Mathematics 207, 1999.</ref> improved the Beck-Fiala bound in tiny steps to <math>disc(\\mathcal{H}) \\leq 2t - 3</math> (for a slightly restricted situation, i.e. <math> t \\geq 3 </math>). A corollary of Beck's paper<ref name=\"beck_roth_estimate\"/> – the first time the notion of discrepancy explicitly appeared – shows <math>disc(\\mathcal{H}) \\leq C \\sqrt{t \\log m} \\log n</math> for some constant C. The latest improvement in this direction is due to Banaszczyk:<ref>W. Banaszczyk: \"Balancing vectors and Gaussian measure of n-dimensional convex bodies\", pages 351–360. Random Structures and Algorithms 12, 1998.</ref> <math>disc(\\mathcal{H}) = O(\\sqrt{t \\log n})</math>.\n\n=== Classic theorems ===\n\n* Axis-parallel rectangles in the plane ([[Klaus Roth|Roth]], Schmidt)\n* Discrepancy of half-planes (Alexander, Matoušek)\n* Arithmetic progressions (Roth, Sárközy, [[Jozsef Beck|Beck]], Matoušek & [[Joel Spencer|Spencer]])\n* [[Beck–Fiala theorem]]\n* Six Standard Deviations Suffice (Spencer)\n\n== Major open problems ==\n\n* Axis-parallel rectangles in dimensions three and higher (Folklore)\n* Komlos conjecture\n\n== Applications ==\n\n* Numerical Integration: Monte Carlo methods in high dimensions.\n* Computational Geometry: [[Divide and conquer algorithms]].\n* Image Processing: Halftoning\n\n== Notes ==\n\n<references/>\n\n== References ==\n\n* {{Cite thesis |degree=[[Habilitation]]|title=Integral Approximation|url=http://www.mpi-inf.mpg.de/~doerr/papers/habil.pdf |last=Doerr|first=Benjamin|year=2005|publisher=[[University of Kiel]]|accessdate=April 28, 2010|oclc= 255383176}}\n* {{cite book|title=Irregularities of Distribution|last1=Beck|first1=József|author1-link=József Beck|last2=Chen|first2=William W. L.|publisher=Cambridge University Press|year=2009|ISBN=0-521-09300-7}}\n* {{cite book|title=Geometric Discrepancy: An Illustrated Guide|last= Matoušek|first= Jiří|author-link=Jiří Matoušek (mathematician)|publisher=Springer|year=1999|ISBN= 3-540-65528-X}}\n* {{cite book|title=The Discrepancy Method: Randomness and Complexity|last=Chazelle|first=Bernard|author-link=Bernard Chazelle|publisher=Cambridge University Press|year=2000|ISBN=0-521-77093-9}}\n\n{{DEFAULTSORT:Discrepancy Of Hypergraphs}}\n[[Category:Diophantine approximation]]\n[[Category:Unsolved problems in mathematics]]"
    },
    {
      "title": "Discrepancy theory",
      "url": "https://en.wikipedia.org/wiki/Discrepancy_theory",
      "text": "{{refimprove|date=January 2018}}\n\nIn mathematics, '''discrepancy theory''' describes the deviation of a situation from the state one would like it to be in. It is also called the '''theory of irregularities of distribution'''. This refers to the theme of ''classical'' discrepancy theory, namely distributing points in some space such that they are evenly distributed with respect to some (mostly geometrically defined) subsets. The discrepancy (irregularity) measures how far a given distribution deviates from an ideal one.\n\nDiscrepancy theory can be described as the study of inevitable irregularities of distributions, in [[Measure theory|measure-theoretic]] and [[Combinatorics|combinatorial]] settings. Just as [[Ramsey theory]] elucidates the impossibility of total disorder, discrepancy theory studies the deviations from total uniformity.\n\nA significant event in the history of discrepancy theory was the 1916 paper of [[H. Weyl|Weyl]] on the uniform distribution of sequences in the unit interval.{{citation needed|date=January 2018}}\n__NOTOC__\n== Theorems ==\nDiscrepancy theory is based on the following classic theorems:\n* The theorem of [[Tatyana Pavlovna Ehrenfest|van Aardenne–Ehrenfest]]\n* Axis-parallel rectangles in the plane ([[Klaus Roth|Roth]], Schmidt)\n* Discrepancy of half-planes (Alexander, [[Jiří Matoušek (mathematician)|Matoušek]])\n* Arithmetic progressions (Roth, Sarkozy, [[Jozsef Beck|Beck]], Matousek & [[Joel Spencer|Spencer]])\n* [[Beck–Fiala theorem]] <ref>{{cite  journal\n  | title = \"Integer-making\" theorems\n  | journal =  Discrete Applied Mathematics\n  | volume =  3\n  | issue =  1\n  | doi = 10.1016/0166-218x(81)90022-6    \n  | author =  József Beck and Tibor Fiala\n  | pages=1–8}}</ref>\n* Six Standard Deviations Suffice (Spencer)<ref>{{cite journal\n|title   = Six Standard Deviations Suffice\n|author  = Joel Spencer\n|authorlink = Joel Spencer\n|journal = Transactions of the American Mathematical Society\n|volume  = 289\n|issue   = 2\n|date=June 1985\n|pages   = 679–706\n|doi     = 10.2307/2000258\n|publisher   = Transactions of the American Mathematical Society, Vol. 289, No. 2\n|jstor   = 2000258\n}}</ref>\n\n== Major open problems ==\nThe unsolved problems relating to discrepancy theory include:\n* Axis-parallel rectangles in dimensions three and higher (folklore)\n* [[János Komlós (mathematician)|Komlós]] conjecture \n* [[Heilbronn triangle problem]] on the minimum area of a triangle determined by three points from an ''n''-point set\n\n== Applications ==\nApplications for discrepancy theory include:\n* Numerical integration: [[Monte Carlo method]]s in high dimensions.\n* Computational geometry: [[Divide-and-conquer algorithm]].\n* Image processing: [[Halftone|Halftoning]]\n\n== See also ==\n*[[Discrepancy of hypergraphs]]\n\n== References ==\n{{Reflist}}\n\n==Further reading==\n*{{cite book |title=Irregularities of Distribution |last=Beck |first=József |authorlink= |author2=Chen, William W. L. |year=1987 |publisher=Cambridge University Press |location=New York |isbn=0-521-30792-9 |pages= |url= }}\n*{{cite book |title=The Discrepancy Method: Randomness and Complexity |last=Chazelle |first=Bernard |authorlink=Bernard Chazelle |year=2000 |publisher=Cambridge University Press |location=New York |isbn=0-521-77093-9 |pages= |url= }}\n*{{cite book |title=Geometric Discrepancy: An Illustrated Guide |last=Matousek |first=Jiri |authorlink= |year=1999 |series=Algorithms and combinatorics |volume=18 |publisher=Springer |location=Berlin |isbn=3-540-65528-X |pages= |url= }}\n\n[[Category:Diophantine approximation]]\n[[Category:Unsolved problems in mathematics]]\n[[Category:Combinatorics]]\n[[Category:Measure theory]]"
    },
    {
      "title": "Duffin–Schaeffer conjecture",
      "url": "https://en.wikipedia.org/wiki/Duffin%E2%80%93Schaeffer_conjecture",
      "text": "The '''Duffin–Schaeffer conjecture''' is an important conjecture in [[metric number theory]] proposed by [[Richard Duffin|R. J. Duffin]] and [[Albert Charles Schaeffer|A. C. Schaeffer]] in 1941.<ref>{{cite journal | last1=Duffin | first1=R. J. | last2=Schaeffer | first2=A. C. | title=Khintchine's problem in metric diophantine approximation | jfm=67.0145.03 | zbl=0025.11002 | journal=Duke Math. J. | volume=8 | issue=2 | pages=243–255 | year=1941 | doi=10.1215/S0012-7094-41-00818-9 }}</ref> It states that if <math>f : \\mathbb{N} \\rightarrow \\mathbb{R}^+</math> is a real-valued function taking on positive values, then for [[almost all]] <math> \\alpha </math> (with respect to [[Lebesgue measure]]), the inequality\n\n: <math> \\left | \\alpha - \\frac{p}{q} \\right | < \\frac{f(q)}{q} </math>\n\nhas infinitely many solutions in [[co-prime]] integers <math>p,q</math> with <math>q > 0</math> if and only if the sum\n\n: <math> \\sum_{q=1}^\\infty f(q) \\frac{\\varphi(q)}{q} = \\infty. </math>\n\nHere <math> \\varphi(q)</math> is the [[Euler totient function]].\n\nThe full conjecture remains unsolved. However, a higher-dimensional analogue of this conjecture has been resolved.<ref name=Mon204/><ref>{{cite journal | first1=A.D. | last1=Pollington | first2=R.C. | last2=Vaughan | author2-link=Bob Vaughan | title=The ''k'' dimensional Duffin–Schaeffer conjecture | journal=Mathematika | volume=37 | number=2 | year=1990 | pages=190–200 | zbl=0715.11036 | issn=0025-5793 | doi=10.1112/s0025579300012900}}</ref><ref name=Har0269>Harman (2002) p.69</ref>\n\n==Progress==\nThe implication from the existence of the rational approximations to the divergence of the series follows from the [[Borel–Cantelli lemma]].<ref name=Har0268>Harman (2002) p.68</ref>  The converse implication is the crux of the conjecture.<ref name=Mon204/>\nThere have been many partial results of the Duffin–Schaeffer conjecture established to date. [[Paul Erdős]] established in 1970 that the conjecture holds if there exists a constant <math>c > 0 </math> such that for every integer <math> n </math> we have either <math>f(n) = c/n</math> or <math>f(n) = 0</math>.<ref name=Mon204>{{ cite book | last=Montgomery | first=Hugh L. | author-link=Hugh Montgomery (mathematician) | title=Ten lectures on the interface between analytic number theory and harmonic analysis | series=Regional Conference Series in Mathematics | volume=84 | location=Providence, RI | publisher=[[American Mathematical Society]] | year=1994 | isbn=978-0-8218-0737-8 | zbl=0814.11001 | page=204 }}</ref><ref name=Har27>Harman (1998) p.27</ref> This was strengthened by Jeffrey Vaaler in 1978 to the case <math>f(n) = O(n^{-1})</math>.<ref>{{cite web | url=http://www.math.osu.edu/files/duffin-schaeffer%20conjecture.pdf | title=Department of Mathematics}}</ref><ref name=Har28>Harman (1998) p.28</ref>  More recently, this was strengthened to the conjecture being true whenever there exists some <math>\\epsilon > 0 </math> such that the series <math>\\sum_{n=1}^\\infty \\left(\\frac{f(n)}{n}\\right)^{1 + \\epsilon} \\varphi(n) = \\infty </math>. This was done by Haynes, Pollington, and Velani.<ref>A. Haynes, A. Pollington, and S. Velani, ''The Duffin-Schaeffer Conjecture with extra divergence'', arXiv, (2009), https://arxiv.org/abs/0811.1234</ref>\n\nIn 2006, Beresnevich and Velani proved that a [[Hausdorff measure]] analogue of the Duffin–Schaeffer conjecture is equivalent to the original Duffin–Schaeffer conjecture, which is a priori weaker. This result is published in the ''[[Annals of Mathematics]]''.<ref>{{cite journal | last1=Beresnevich | first1=Victor | last2=Velani | first2=Sanju | title=A mass transference principle and the Duffin-Schaeffer conjecture for Hausdorff measures | journal=[[Annals of Mathematics]] |series=Second Series | volume=164 | number=3 | year=2006 | pages=971–992 | zbl=1148.11033 | issn=0003-486X | doi=10.4007/annals.2006.164.971| arxiv=math/0412141 }}</ref>\n\n==Notes==\n{{Reflist}}\n\n==References==\n* {{cite book | last=Harman | first=Glyn | author-link= | title=Metric number theory | series=London Mathematical Society Monographs. New Series | volume=18 | location=Oxford | publisher=[[Clarendon Press]] | year=1998 | isbn=978-0-19-850083-4 | zbl=1081.11057 }}\n* {{cite book | last=Harman | first=Glyn | authorlink=Glyn Harman | chapter=One hundred years of normal numbers | editor1-last=Bennett | editor1-first=M. A. | editor2-last=Berndt | editor2-first=B.C. | editor2-link=Bruce C. Berndt | editor3-last=Boston | editor3-first=N. | editor3-link=Nigel Boston | editor4-last=Diamond | editor4-first=H.G. | editor5-last=Hildebrand | editor5-first=A.J. | editor6-last=Philipp | editor6-first=W. | title=Surveys in number theory: Papers from the millennial conference on number theory | location=Natick, MA | publisher=A K Peters | pages=57–74 | year=2002 | isbn=978-1-56881-162-8 | zbl=1062.11052 }}\n\n{{DEFAULTSORT:Duffin-Schaeffer conjecture}}\n[[Category:Conjectures]]\n[[Category:Diophantine approximation]]"
    },
    {
      "title": "Equidistributed sequence",
      "url": "https://en.wikipedia.org/wiki/Equidistributed_sequence",
      "text": "In [[mathematics]], a sequence {''s''<sub>1</sub>, ''s''<sub>2</sub>, ''s''<sub>3</sub>, ...} of [[real number]]s is said to be '''equidistributed''', or '''uniformly distributed''', if the proportion of terms falling in a subinterval is proportional to the length of that interval.  Such sequences are studied in [[Diophantine approximation]] theory and have applications to [[Monte Carlo integration]].\n\n==Definition==\nA sequence {''s''<sub>1</sub>, ''s''<sub>2</sub>, ''s''<sub>3</sub>, ...} of [[real number]]s is said to be ''equidistributed'' on a non-degenerate [[Interval (mathematics)|interval]] [''a'',&nbsp;''b''] if for any subinterval [''c'',&nbsp;''d''] of [''a'',&nbsp;''b''] we have\n:<math>\\lim_{n\\to\\infty}{ \\left|\\{\\,s_1,\\dots,s_n \\,\\} \\cap [c,d] \\right| \\over n}={d-c \\over b-a} . </math>\n(Here, the notation |{''s''<sub>1</sub>,...,''s''<sub>''n''</sub>&nbsp;}∩[''c'',''d'']| denotes the number of elements, out of the first ''n'' elements of the sequence, that are between ''c'' and ''d''.)\n\nFor example, if a sequence is equidistributed in [0,&nbsp;2], since the interval [0.5,&nbsp;0.9] occupies 1/5 of the length of the interval [0,&nbsp;2], as ''n'' becomes large, the proportion of the first ''n'' members of the sequence which fall between 0.5 and 0.9 must approach 1/5.  Loosely speaking, one could say that each member of the sequence is equally likely to fall anywhere in its range.  However, this is not to say that {''s''<sub>''n''</sub>} is a sequence of random variables; rather, it is a determinate sequence of real numbers.\n\n===Discrepancy===\nWe define the '''discrepancy''' ''D''<sub>''N''</sub> for a sequence {''s''<sub>1</sub>, ''s''<sub>2</sub>, ''s''<sub>3</sub>, ...} with respect to the interval [''a'',&nbsp;''b''] as\n\n:<math> D_N = \\sup_{a\\le c\\le d\\le b} \\left\\vert \\frac{\\left|\\{\\,s_1,\\dots,s_N \\,\\} \\cap [c,d] \\right|}{N} - \\frac{d-c}{b-a} \\right\\vert . </math>\n\nA sequence is thus equidistributed if the discrepancy ''D''<sub>''N''</sub> tends to zero as ''N'' tends to infinity.\n\nEquidistribution is a rather weak criterion to express the fact that a sequence fills the segment leaving no gaps. For example, the drawings of a random variable uniform over a segment will be equidistributed in the segment, but there will be large gaps compared to a sequence which first enumerates multiples of ε in the segment, for some small ε, in an appropriately chosen way, and then continues to do this for smaller and smaller values of ε. For stronger criteria and for constructions of sequences that are more evenly distributed, see [[low-discrepancy sequence]].\n\n===Riemann integral criterion for equidistribution===\nRecall that if ''f'' is a function having a [[Riemann integral]] in the interval [''a'',&nbsp;''b''], then its integral is the limit of [[Riemann sum]]s taken by sampling the function ''f'' in a set of points chosen from a fine partition of the interval. Therefore, if some sequence is equidistributed in [''a'',&nbsp;''b''], it is expected that this sequence can be used to calculate the integral of a Riemann-integrable function. This leads to the following criterion<ref>Kuipers & Niederreiter (2006) pp. 2–3</ref> for an equidistributed sequence:\n\nSuppose {''s''<sub>1</sub>, ''s''<sub>2</sub>, ''s''<sub>3</sub>, ...} is a sequence contained in the interval [''a'',&nbsp;''b'']. Then the following conditions are equivalent:\n# The sequence is equidistributed on [''a'',&nbsp;''b''].\n# For every Riemann-integrable ([[Complex-valued function|complex-valued]]) function ''f''&nbsp;:&nbsp;[''a'',&nbsp;''b'']&nbsp;→&nbsp;ℂ, the following limit holds:\n\n: <math>\\lim_{N\\to\\infty}\\frac{1}{N}\\sum_{n=1}^{N} f\\left(s_n\\right) = \\frac{1}{b-a}\\int_a^b f(x)\\,dx</math>\n\n:{| class=\"toccolours collapsible collapsed\" width=\"90%\" style=\"text-align:left\"\n!Proof\n|-\n|First note that the definition of an equidistributed sequence is equivalent to the integral criterion whenever ''f'' is the [[indicator function]] of an interval: If ''f''&nbsp;=&nbsp;'''1'''<sub>[''c'',&nbsp;''d'']</sub>, then the left hand side is the proportion of points of the sequence falling in the interval [''c'',&nbsp;''d''], and the right hand side is exactly <math>\\textstyle\\frac{d-c}{b-a}.</math>\n\nThis means 2 → 1 (since indicator functions are Riemann-integrable), and 1 → 2 for ''f'' being an indicator function of an interval. It remains to assume that the integral criterion holds for indicator functions and prove that it holds for general Riemann-integrable functions as well.\n\nNote that both sides of the integral criterion equation are ''linear'' in ''f'', and therefore the criterion holds for [[linear combination]]s of interval indicators, that is, [[step function]]s.\n\nTo show it holds for ''f'' being a general Riemann-integrable function, first assume ''f'' is real-valued. Then by using [[Darboux integral|Darboux's definition]] of the integral, we have for every ε&nbsp;&gt;&nbsp;0 two step functions ''f''<sub>1</sub> and ''f''<sub>2</sub> such that ''f''<sub>1</sub>&nbsp;≤&nbsp;''f''&nbsp;≤&nbsp;''f''<sub>2</sub> and <math>\\textstyle\\int_a^b (f_2(x) - f_1(x))\\,dx \\le \\varepsilon(b-a).</math> Notice that:\n:<math>\\frac{1}{b-a}\\int_a^b f_1(x)\\,dx = \\lim_{N\\to\\infty} \\frac 1 N \\sum_{n=1}^N f_1(s_n) \\le \\liminf_{N\\to\\infty} \\frac 1 N \\sum_{n=1}^N f(s_n)</math>\n:<math>\\frac{1}{b-a}\\int_a^b f_2(x)\\,dx = \\lim_{N\\to\\infty} \\frac 1 N \\sum_{n=1}^N f_2(s_n) \\ge \\limsup_{N\\to\\infty} \\frac 1 N \\sum_{n=1}^N f(s_n)</math>\nBy subtracting, we see that the [[limit superior and limit inferior]] of <math>\\textstyle \\frac 1 N \\sum_{n=1}^N f(s_n)</math> differ by at most ε. Since ε is arbitrary, we have the existence of the limit, and by Darboux's definition of the integral, it is the correct limit.\n\nFinally, for complex-valued Riemann-integrable functions, the result follows again from linearity, and from the fact that every such function can be written as ''f''&nbsp;=&nbsp;''u''&nbsp;+&nbsp;''vi'', where ''u'', ''v'' are real-valued and Riemann-integrable.&nbsp;[[Q.E.D.|∎]]\n|}\nThis criterion leads to the idea of [[Monte-Carlo integration]], where integrals are computed by sampling the function over a sequence of random variables equidistributed in the interval.\n\nIt is not possible to generalize the integral criterion to a class of functions bigger than just the Riemann-integrable ones. For example, if the [[Lebesgue integral]] is considered and ''f'' is taken to be in [[Lp space|''L''<sup>1</sup>]], then this criterion fails. As a counterexample, take ''f'' to be the [[indicator function]] of some equidistributed sequence. Then in the criterion, the left hand side is always 1, whereas the right hand side is zero, because the sequence is countable, so ''f'' is zero [[almost everywhere]].\n\nIn fact, the '''de Bruijn–Post Theorem''' states the converse of the above criterion: If ''f'' is a function such that the criterion above holds for any equidistributed sequence in [''a'',&nbsp;''b''], then ''f'' is Riemann-integrable in [''a'',&nbsp;''b''].<ref>http://math.uga.edu/~pete/udnotes.pdf, Theorem 8</ref>\n\n==Equidistribution modulo 1==\nA sequence {''a''<sub>1</sub>, ''a''<sub>2</sub>, ''a''<sub>3</sub>, ...} of real numbers is said to be '''equidistributed modulo 1''' or '''uniformly distributed modulo 1''' if the sequence of the [[fractional part]]s of ''a''<sub>''n''</sub>, denoted by {''a''<sub>''n''</sub>} or by ''a''<sub>''n''</sub>&nbsp;−&nbsp;⌊''a''<sub>''n''</sub>⌋, is equidistributed in the interval [0,&nbsp;1].\n\n===Examples===\n[[File:Van der Corput sequence 999.svg|thumb|330px<!-- Use a multiple of 11 to reduce aliasing artifact -->|Illustration of the filling of the unit interval (''x''-axis) using the first ''n'' terms of the Van der Corput sequence, for ''n'' from 0 to 999 (''y''-axis). Gradation in colour is due to aliasing.]]\n* The [[equidistribution theorem]]: The sequence of all multiples of an irrational ''α'',\n::0, ''α'', 2''α'', 3''α'', 4''α'', ...\n:is equidistributed modulo 1.<ref name=KN8>Kuipers & Niederreiter (2006) p.&nbsp;8</ref>\n* More generally, if ''p'' is a polynomial with at least one coefficient other than the constant term irrational then the sequence ''p''(''n'') is uniformly distributed modulo 1.\n\nThis was proven by Weyl and is an application of van der Corput's difference theorem.<ref name=KN27>Kuipers & Niederreiter (2006) p.&nbsp;27</ref>\n\n* The sequence log(''n'') is ''not'' uniformly distributed modulo 1.<ref name=KN8/> This fact is related to the [[Benford's law]].\n* The sequence of all multiples of an irrational ''α'' by successive [[prime number]]s,\n::2''α'', 3''α'', 5''α'', 7''α'', 11''α'', ...\n:is equidistributed modulo 1.  This is a famous theorem of [[analytic number theory]], published by [[I. M. Vinogradov]] in 1948.<ref name=KN129>Kuipers & Niederreiter (2006) p.&nbsp;129</ref>\n* The [[van der Corput sequence]] is equidistributed.<ref name=KN127>Kuipers & Niederreiter (2006) p.&nbsp;127</ref>\n\n===Weyl's criterion===\n'''Weyl's criterion''' states that the sequence ''a''<sub>''n''</sub> is equidistributed modulo 1 if and only if for all non-zero integers ℓ,\n:<math>\\lim_{n\\to\\infty}\\frac{1}{n}\\sum_{j=1}^{n}e^{2\\pi i \\ell a_j}=0.</math>\nThe criterion is named after, and was first formulated by, [[Hermann Weyl]].<ref>{{cite journal|last=Weyl|first=H.|authorlink=Hermann Weyl|title=Über die Gleichverteilung von Zahlen mod. Eins |language=de |trans-title=On the distribution of numbers modulo one |journal=Math. Ann.|date=September 1916|volume=77|issue=3|pages=313–352 | doi=10.1007/BF01475864 |url=http://www.fuchs-braun.com/media/3ed54b58b68a224cffff80dffffffff1.pdf}}</ref> It allows equidistribution questions to be reduced to bounds on [[exponential sum]]s, a fundamental and general method.\n\n:{| class=\"toccolours collapsible collapsed\" width=\"90%\" style=\"text-align:left\"\n!Sketch of proof\n|-\n|If the sequence is equidistributed modulo 1, then we can apply the Riemann integral criterion (described above) on the function <math>\\textstyle f(x) = e^{2\\pi i \\ell x},</math> which has integral zero on the interval [0,&nbsp;1]. This gives Weyl's criterion immediately.\n\nConversely, suppose Weyl's criterion holds. Then the Riemann integral criterion holds for functions ''f'' as above, and by linearity of the criterion, it holds for ''f'' being any [[trigonometric polynomial]]. By the [[Stone–Weierstrass theorem]] and an approximation argument, this extends to any ''continuous'' function ''f''.\n\nFinally, let ''f'' be the [[indicator function]] of an interval. It is possible to bound ''f'' from above and below by two continuous functions on the interval, whose integrals differ by an arbitrary ε. By an argument similar to the proof of the Riemann integral criterion, it is possible to extend the result to any ''interval indicator'' function ''f'', thereby proving equidistribution modulo 1 of the given sequence.&nbsp;[[Q.E.D.|∎]]\n|}\n\n====Generalizations====\n* A quantitative form of Weyl's criterion is given by the [[Erdős–Turán inequality]].\n* Weyl's criterion extends naturally to higher [[dimension]]s, assuming the natural generalization of the definition of equidistribution modulo 1:\n\nThe sequence ''v''<sub>''n''</sub> of vectors in '''R'''<sup>''k''</sup> is equidistributed modulo 1 if and only if for any non-zero vector ℓ&nbsp;∈&nbsp;'''Z'''<sup>''k''</sup>,\n\n: <math>\\lim_{n\\to\\infty}\\frac{1}{n}\\sum_{j=0}^{n-1}e^{2\\pi i \\ell \\cdot v_j}=0.</math>\n\n====Example of usage====\nWeyl's criterion can be used to easily prove the [[equidistribution theorem]], stating that the sequence of multiples 0, ''α'', 2''α'', 3''α'', ... of some real number ''α'' is equidistributed modulo 1 if and only if ''α'' is irrational.<ref name=KN8/>\n\nSuppose ''α'' is irrational and denote our sequence by ''a''<sub>''j''</sub>&nbsp;=&nbsp;''jα'' (where ''j'' starts from 0, to simplify the formula later). Let ''ℓ''&nbsp;≠&nbsp;0 be an integer. Since ''α'' is irrational, ''ℓα'' can never be an integer, so <math display=inline>e^{2\\pi i \\ell \\alpha}</math> can never be&nbsp;1. Using the formula for the sum of a finite [[geometric series]],\n:<math>\\left|\\sum_{j=0}^{n-1}e^{2\\pi i \\ell j \\alpha}\\right| = \\left|\\sum_{j=0}^{n-1}\\left(e^{2\\pi i \\ell \\alpha}\\right)^j\\right| = \\left| \\frac{1 - e^{2\\pi i \\ell n \\alpha}} {1 - e^{2\\pi i \\ell \\alpha}}\\right| \\le \\frac 2 { \\left|1 - e^{2\\pi i \\ell \\alpha}\\right|},</math>\na finite bound that does not depend on ''n''. Therefore, after dividing by ''n'' and letting ''n'' tend to infinity, the left hand side tends to zero, and Weyl's criterion is satisfied.\n\nConversely, notice that if ''α'' is rational then this sequence is not equidistributed modulo 1, because there are only a finite number of options for the fractional part of ''a''<sub>''j''</sub>&nbsp;=&nbsp;''jα''.\n\n===van der Corput's difference theorem===\nA theorem of [[Johannes van der Corput]]<ref>{{Citation | last=van der Corput | first=J. | authorlink=Johannes van der Corput | title=Diophantische Ungleichungen. I. Zur Gleichverteilung Modulo Eins | publisher=Springer Netherlands | year=1931 | journal=[[Acta Mathematica]] | issn=0001-5962 | volume=56 | pages=373–456 | doi=10.1007/BF02545780 | zbl=0001.20102 | jfm=57.0230.05 }}</ref> states that if for each ''h'' the sequence ''s''<sub>''n''+''h''</sub>&nbsp;−&nbsp;''s''<sub>''n''</sub> is uniformly distributed modulo 1, then so is ''s''<sub>''n''</sub>.<ref>Kuipers & Niederreiter (2006) p.&nbsp;26</ref><ref name=Mon18>Montgomery (1994) p.18</ref><ref name=Mon2001>{{cite book | last=Montgomery | first=Hugh L. | authorlink=Hugh Montgomery (mathematician) | chapter=Harmonic analysis as found in analytic number theory | zbl=1001.11001 | editor1-last=Byrnes | editor1-first=James S. | title=Twentieth century harmonic analysis–a celebration. Proceedings of the NATO Advanced Study Institute, Il Ciocco, Italy, July 2–15, 2000 | location=Dordrecht | publisher=Kluwer Academic Publishers | series=NATO Sci. Ser. II, Math. Phys. Chem. | volume=33 | pages=271–293 | year=2001 | doi = 10.1007/978-94-010-0662-0_13 | isbn=978-0-7923-7169-4 | chapter-url=http://www.nato-us.org/analysis2000/papers/montgomery.pdf }}</ref>\n\nA '''van der Corput set''' is a set ''H'' of integers such that if for each ''h'' in ''H'' the sequence ''s''<sub>''n''+''h''</sub>&nbsp;−&nbsp;''s''<sub>''n''</sub> is uniformly distributed modulo 1, then so is s<sub>''n''</sub>.<ref name=Mon18/><ref name=Mon2001/>\n\n===Metric theorems===\nMetric theorems describe the behaviour of a parametrised sequence for [[almost all]] values of some parameter ''α'': that is, for values of ''α'' not lying in some exceptional set of [[Lebesgue measure]] zero.\n\n* For any sequence of distinct integers ''b''<sub>''n''</sub>, the sequence {{mset|''b''<sub>''n''</sub>''α''}} is equidistributed mod 1 for almost all values of ''α''.<ref>See {{citation | title=Über eine Anwendung der Mengenlehre auf ein aus der Theorie der säkularen Störungen herrührendes Problem | first=Felix | last=Bernstein | authorlink=Felix Bernstein (mathematician) | journal=[[Mathematische Annalen]] | volume=71 | number=3 | year=1911 | pages=417–439 | doi=10.1007/BF01456856 | url=https://zenodo.org/record/1701508 }}.</ref>\n* The sequence {{mset|''α''<sup>''n''</sup>}} is equidistributed mod 1 for almost all values of ''α'' > 1.<ref>{{citation | url=http://www.numdam.org/item?id=CM_1935__2__250_0 | title=Ein mengentheoretischer Satz über die Gleichverteilung modulo Eins | first=J. F. | last=Koksma | authorlink=Jurjen Ferdinand Koksma | journal=[[Compositio Mathematica]] | volume=2 | year=1935 | pages=250–258 | zbl=0012.01401  | jfm=61.0205.01 }}</ref>\nIt is not known whether the sequences {{mset|''e''<sup>''n''</sup>}} or {{mset|{{pi}}<sup>''n''</sup>}} are equidistributed mod&nbsp;1.  However it is known that the sequence {{mset|''α''<sup>''n''</sup>}} is ''not'' equidistributed mod&nbsp;1 if ''α'' is a [[PV number]].\n\n==Well-distributed sequence==\nA sequence {''s''<sub>1</sub>, ''s''<sub>2</sub>, ''s''<sub>3</sub>, ...} of [[real number]]s is said to be '''well-distributed''' on [''a'',&nbsp;''b''] if for any subinterval [''c'',&nbsp;''d''] of [''a'',&nbsp;''b''] we have\n:<math>\\lim_{n\\to\\infty}{ \\left|\\{\\,s_{k+1},\\dots,s_{k+n} \\,\\} \\cap [c,d] \\right| \\over n}={d-c \\over b-a} </math>\n''uniformly'' in ''k''.  Clearly every well-distributed sequence is uniformly distributed, but the converse does not hold.  The definition of well-distributed modulo 1 is analogous.\n\n==Sequences equidistributed with respect to an arbitrary measure==\nFor an arbitrary [[probability measure space]] <math>(X,\\mu)</math>, a sequence of points <math>x_n</math> is said to be equidistributed with respect to <math>\\mu</math> if the mean of [[Dirac delta function|point measures]] [[Convergence of measures#Weak convergence of measures|converges weakly]] to <math>\\mu</math>:<ref name=KN171>Kuipers & Niederreiter (2006) p.171</ref>\n:<math>\\frac{\\sum_{k=1}^n \\delta_{x_k}}{n}\\Rightarrow \\mu \\ . </math>\nIt is true, for example, that for any [[Borel measure|Borel]] [[probability measure]] on a [[separable space|separable]], [[metrizable]] space, there exists an equidistributed sequence (with respect to the measure).{{citation needed|date=February 2014}}\n\n==See also==\n*[[Equidistribution theorem]]\n*[[Low-discrepancy sequence]]\n*[[Erdős–Turán inequality]]\n\n==Notes==\n{{reflist}}\n\n==References==\n* {{cite book | first1=L. | last1=Kuipers | first2=H. | last2=Niederreiter | author2-link = Harald Niederreiter | title=Uniform Distribution of Sequences | publisher=Dover Publications | year=2006 | origyear=1974 | isbn=0-486-45019-8 }}\n* {{cite book | first1=L. | last1=Kuipers | first2=H. | last2=Niederreiter | author2-link = Harald Niederreiter | title=Uniform Distribution of Sequences | publisher=John Wiley & Sons Inc. | year=1974 | isbn=0-471-51045-9 | zbl=0281.10001 }}\n* {{cite book | last=Montgomery | first=Hugh L. | authorlink=Hugh Montgomery (mathematician) | title=Ten lectures on the interface between analytic number theory and harmonic analysis | series=Regional Conference Series in Mathematics | volume=84 | location=Providence, RI | publisher=[[American Mathematical Society]] | year=1994 | isbn=0-8218-0737-4 | zbl=0814.11001 }}\n\n==Further reading==\n* {{cite book | editor1-last=Granville | editor1-first=Andrew | editor2-last=Rudnick | editor2-first=Zeév | zbl=1121.11004 | title=Equidistribution in number theory, an introduction. Proceedings of the NATO Advanced Study Institute on equidistribution in number theory, Montréal, Canada, July 11–22, 2005 | location=Dordrecht | publisher=[[Springer-Verlag]] | isbn=978-1-4020-5403-7 | series=NATO Science Series II: Mathematics, Physics and Chemistry | volume=237 | year=2007 }}\n* {{cite book | zbl= 1277.11010 | last=Tao | first=Terence | authorlink=Terence Tao | title=Higher order Fourier analysis | series=[[Graduate Studies in Mathematics]] | volume=142 | location=Providence, RI | publisher=[[American Mathematical Society]] | year=2012 | isbn=978-0-8218-8986-2 | url=http://terrytao.wordpress.com/books/higher-order-fourier-analysis/ }}\n\n==External links==\n* {{MathWorld|title=Equidistributed Sequence|urlname=EquidistributedSequence}}\n* {{MathWorld|title=Weyl's Criterion|urlname=WeylsCriterion}}\n* {{PlanetMath|title=Weyl's Criterion|urlname=WeylsCriterion}}\n* [http://www.maths.manchester.ac.uk/~cwalkden/ergodic-theory/lecture06.pdf Lecture notes with proof of Weyl's Criterion]\n\n[[Category:Diophantine approximation]]\n[[Category:Dynamical systems]]\n[[Category:Ergodic theory]]"
    },
    {
      "title": "Equidistribution theorem",
      "url": "https://en.wikipedia.org/wiki/Equidistribution_theorem",
      "text": "[[File:equidistribution_theorem.svg|thumb|330px<!-- Use a multiple of 11 to reduce aliasing artifact -->|Illustration of filling the unit interval (horizontal axis) with the first ''n'' terms using the equidistribution theorem with four common irrational numbers, for ''n'' from 0 to 999 (vertical axis). The 113 distinct bands for {{pi}} are due to the closeness of its value to the rational number 355/113.<br />[[:File:equidistribution_theorem.svg|(click for detailed view)]] ]]\n\nIn [[mathematics]], the '''equidistribution theorem''' is the statement that the sequence\n\n:''a'', 2''a'', 3''a'', ... mod 1\n\nis [[Equidistributed sequence|uniformly distributed]] on the [[circle]] <math>\\mathbb{R}/\\mathbb{Z}</math>, when ''a'' is an [[irrational number]].  It is a special case of the [[ergodic theorem]] where one takes the normalized angle measure <math>\\mu=\\frac{d\\theta}{2\\pi}</math>.\n\n==History==\nWhile this theorem was proved in 1909 and 1910 separately by [[Hermann Weyl]], [[Wacław Sierpiński]] and [[Piers Bohl]], variants of this theorem continue to be studied to this day.\n\nIn 1916, Weyl proved that the sequence ''a'', 2<sup>2</sup>''a'', 3<sup>2</sup>''a'', ... mod 1 is uniformly distributed on the unit interval.  In 1935, [[Ivan Vinogradov]] proved that the sequence ''p''<sub>''n''</sub> ''a'' mod 1 is uniformly distributed, where ''p''<sub>''n''</sub> is the ''n''th [[prime number|prime]].  Vinogradov's proof was a byproduct of the [[odd Goldbach conjecture]], that every sufficiently large odd number is the sum of three primes.\n\n[[George Birkhoff]], in 1931, and [[Aleksandr Khinchin]], in 1933, proved that the generalization ''x''&nbsp;+&nbsp;''na'', for [[almost all]] ''x'',  is equidistributed on any [[Lebesgue measurable]] subset of the unit interval.  The corresponding generalizations for the Weyl and Vinogradov results were proven by [[Jean Bourgain]] in 1988.\n\nSpecifically, Khinchin showed that the identity\n\n:<math>\\lim_{n\\to\\infty} \\frac{1}{n} \\sum_{k=1}^n\nf( (x+ka) \\bmod 1 ) = \\int_0^1 f(y)\\,dy  </math>\n\nholds for almost all ''x'' and any Lebesgue integrable function&nbsp;ƒ.  In modern formulations, it is asked under what conditions the identity\n\n:<math>\\lim_{n\\to\\infty} \\frac{1}{n} \\sum_{k=1}^n\nf( (x+b_ka) \\bmod 1 ) = \\int_0^1 f(y)\\,dy  </math>\n\nmight hold, given some general [[sequence]] ''b''<sub>''k''</sub>.\n\nOne noteworthy result is that the sequence 2<sup>''k''</sup>''a''&nbsp;mod&nbsp;1 is uniformly distributed for almost all, but not all, irrational ''a''. Similarly, for the sequence ''b''<sub>''k''</sub>&nbsp;=&nbsp;2<sup>''k''</sup>, for every irrational ''a'', and almost all ''x'', there exists a function ƒ for which the sum diverges. In this sense, this sequence is considered to be a '''universally bad averaging sequence''', as opposed to ''b''<sub>''k''</sub>&nbsp;=&nbsp;''k'', which is termed a '''universally good averaging sequence''', because it does not have the latter shortcoming.\n\nA powerful general result is [[Weyl's criterion]], which shows that equidistribution is equivalent to having a non-trivial estimate for the [[exponential sum]]s formed with the sequence as exponents. For the case of  multiples of ''a'', Weyl's criterion reduces the problem to summing finite [[geometric series]].\n\n==See also==\n* [[Diophantine approximation]]\n* [[Low-discrepancy sequence]]\n*[[Dirichlet's approximation theorem]]\n*[[Three-gap theorem]]\n\n==References==\n===Historical references===\n* P. Bohl, (1909) ''Über ein in der Theorie der säkutaren Störungen vorkommendes Problem'', ''J. reine angew. Math.'' '''135''', pp. 189–283.\n* {{cite journal | last1 = Weyl | first1 = H. | year = 1910 | title = Über die Gibbs'sche Erscheinung und verwandte Konvergenzphänomene | url = | journal = [[Rendiconti del Circolo Matematico di Palermo]] | volume = 330 | issue = | pages = 377–407 | doi=10.1007/bf03014883}}\n* W. Sierpinski, (1910) ''Sur la valeur asymptotique d'une certaine somme'', ''Bull Intl. Acad. Polonaise des Sci. et des Lettres'' (Cracovie) '''series A''', pp. 9–11.\n* {{cite journal|last=Weyl|first=H.|title=Ueber die Gleichverteilung von Zahlen mod. Eins,|journal=Math. Ann.|year=1916|volume=77|pages=313–352 | doi = 10.1007/BF01475864 | issue = 3 |postscript = .}}\n* {{cite journal | first1= G. D. |last1=Birkhoff |url=http://www.pnas.org/cgi/reprint/17/12/656 |title = Proof of the ergodic theorem |year=1931  |volume = 17 |issue = 12 |pages= 656–660 | pmc = 1076138 |postscript = . |pmid=16577406 |journal=Proc. Natl. Acad. Sci. U.S.A. |doi=10.1073/pnas.17.12.656}}\n* {{cite journal | first1= A. |last1 = Ya. Khinchin |title = Zur Birkhoff's Lösung des Ergodensproblems |year= 1933 | journal = Math. Ann. | volume = 107| pages =  485–488 |postscript = . | doi = 10.1007/BF01448905}}\n\n===Modern references===\n* Joseph M. Rosenblatt and Máté Weirdl, ''Pointwise ergodic theorems via harmonic analysis'', (1993) appearing in ''Ergodic Theory and its Connections with Harmonic Analysis, Proceedings of the 1993 Alexandria Conference'', (1995) Karl E. Petersen and Ibrahim A. Salama, ''eds.'', Cambridge University Press, Cambridge, {{ISBN|0-521-45999-0}}. ''(An extensive survey of the ergodic properties of generalizations of the equidistribution theorem of [[shift map]]s on the [[unit interval]]. Focuses on methods developed by Bourgain.)''\n* [[Elias M. Stein]] and Rami Shakarchi, ''Fourier Analysis. An Introduction'', (2003) Princeton University Press, pp 105–113 ''(Proof of the Weyl's theorem based on Fourier Analysis)''\n\n[[Category:Ergodic theory]]\n[[Category:Diophantine approximation]]\n[[Category:Theorems in number theory]]"
    },
    {
      "title": "Faltings's product theorem",
      "url": "https://en.wikipedia.org/wiki/Faltings%27s_product_theorem",
      "text": "In [[arithmetic geometry]], the '''Faltings's product theorem''' gives sufficient conditions for a [[algebraic variety|subvariety]] of a product of [[projective space]]s to be a product of varieties in the projective spaces. It was introduced by {{harvs|txt|last=Faltings|year=1991|authorlink=Gerd Faltings}} in his proof of Lang's conjecture that subvarieties of an [[abelian variety]] containing no translates of non-trivial abelian subvarieties have only finitely many rational points.\n\n{{harvtxt|Evertse|1995}} and {{harvtxt|Ferretti|1996}} gave explicit versions of the Faltings's product theorem.\n\n==References==\n*{{Citation | last1=Evertse | first1=Jan-Hendrik | title=An explicit version of Faltings' product theorem and an improvement of Roth's lemma | url=http://matwbn.icm.edu.pl/ksiazki/aa/aa73/aa7332.pdf |mr=1364461 | year=1995 | journal=[[Acta Arithmetica]] | issn=0065-1036 | volume=73 | issue=3 | pages=215–248| doi=10.4064/aa-73-3-215-248 }}\n*{{Citation | last1=Faltings | first1=Gerd | title=Diophantine approximation on abelian varieties | doi=10.2307/2944319 |mr=1109353 | year=1991 | journal=[[Annals of Mathematics]] |series=Second Series | issn=0003-486X | volume=133 | issue=3 | pages=549–576| jstor=2944319 }}\n*{{Citation | last1=Ferretti | first1=Roberto | title=An effective version of Faltings' product theorem | doi=10.1515/form.1996.8.401 |mr=1393322 | year=1996 | journal=Forum Mathematicum | issn=0933-7741 | volume=8 | issue=4 | pages=401–427}}\n\n[[Category:Diophantine approximation]]\n[[Category:Theorems in number theory]]\n\n\n{{numtheory-stub}}"
    },
    {
      "title": "Harmonious set",
      "url": "https://en.wikipedia.org/wiki/Harmonious_set",
      "text": "In [[mathematics]], a '''harmonious set''' is a subset of a [[locally compact abelian group]] on which every weak character may be uniformly approximated by strong characters. Equivalently, a suitably defined dual set is relatively dense in the [[Pontryagin dual]] of the group. This notion was introduced by [[Yves Meyer]] in 1970 and later turned out to play an important role in the mathematical theory of [[quasicrystal]]s. Some related concepts are '''model sets''', '''[[Meyer set]]s''', and '''cut-and-project sets'''.\n\n== Definition ==\n\nLet ''&Lambda;'' be a subset of a locally compact abelian group ''G'' and ''&Lambda;''<sub>''d''</sub> be the subgroup of ''G'' generated by ''&Lambda;'', with [[discrete topology]]. A '''weak character''' is a restriction to ''&Lambda;'' of an algebraic homomorphism from ''&Lambda;''<sub>''d''</sub> into the [[circle group]]:\n\n: <math> \\chi: \\Lambda_d\\to\\mathbf{T}, \\quad \n\\chi\\in\\operatorname{Hom}(\\Lambda_d,\\mathbf{T}). </math>\n\nA '''strong character''' is a restriction to ''&Lambda;'' of a continuous homomorphism from ''G'' to '''T''', that is an element of the [[Pontryagin dual]] of ''G''.\n\nA set ''&Lambda;'' is '''harmonious''' if every weak character may be approximated by \nstrong characters uniformly on ''&Lambda;''. Thus for any ''&epsilon;'' > 0 and any weak character ''&chi;'', there exists a strong character ''&xi;'' such that\n\n: <math> \\sup_\\Lambda |\\chi(\\lambda)-\\xi(\\lambda)| \\leq \\epsilon, \\quad\n\\chi\\in\\operatorname{Hom}(\\Lambda_d,\\mathbf{T}), \\xi\\in\\hat{G}. </math>\n\nIf the locally compact abelian group ''G'' is [[separable topological space|separable]] and [[metrizable]] (its topology may be defined by a translation-invariant metric) then harmonious sets admit another, related, description. Given a subset ''&Lambda;'' of ''G'' and a positive ''&epsilon;'', let ''M''<sub>''&epsilon;''</sub> be the subset of the Pontryagin dual of ''G'' consisting of all characters that are almost trivial on ''&Lambda;'':\n\n: <math> \\sup_\\Lambda|\\chi(\\lambda)-1| \\leq \\epsilon, \\quad\n\\chi\\in\\hat{G}.</math>\n\nThen ''&Lambda;'' is '''harmonious''' if the sets ''M''<sub>''&epsilon;''</sub> are '''relatively dense''' in the sense of [[Besicovitch]]: for every ''&epsilon;'' > 0 there exists a compact subset ''K''<sub>''&epsilon;''</sub> of the Pontryagin dual such that\n\n: <math> M_\\epsilon + K_\\epsilon = \\hat{G}.</math>\n\n== Properties ==\n\n* A subset of a harmonious set is harmonious.\n* If ''&Lambda;'' is a harmonious set and ''F'' is a finite set then the set ''&Lambda;'' + ''F'' is also harmonious.\n\nThe next two properties show that the notion of a harmonious set is nontrivial only when the ambient group is neither compact nor discrete.\n\n* A finite set ''&Lambda;'' is always harmonious. If the group ''G'' is compact then, conversely, every harmonious set is finite.\n* If ''G'' is a [[discrete group]] then every set is harmonious.\n\n== Examples ==\n\nInteresting examples of multiplicatively closed harmonious sets of real numbers arise in the theory of [[diophantine approximation]].\n\n* Let ''G'' be the additive group of [[real number]]s, ''&theta;'' >1, and the set ''&Lambda;'' consist of all finite sums of different powers of ''&theta;''. Then ''&Lambda;'' is harmonious if and only if ''&theta;'' is a [[Pisot number]]. In particular, the sequence of powers of a Pisot number is harmonious.\n* Let '''K''' be a real [[algebraic number field]] of degree ''n'' over '''Q''' and the set ''&Lambda;'' consist of all Pisot or [[Salem number|Salem]] numbers of degree ''n'' in '''K'''. Then ''&Lambda;'' is contained in the open interval (1,&infin;), closed under multiplication, and harmonious. Conversely, any set of real numbers with these 3 properties consists of all Pisot or Salem numbers of degree ''n'' in some real algebraic number field '''K''' of degree ''n''.\n\n== See also ==\n\n* [[Almost periodic function]]\n\n== References ==\n\n* [[Yves Meyer]], ''Algebraic numbers and harmonic analysis'', North-Holland Mathematical Library, vol.2, North-Holland, 1972\n\n[[Category:Harmonic analysis]]\n[[Category:Diophantine approximation]]\n[[Category:Tessellation]]"
    },
    {
      "title": "Heilbronn set",
      "url": "https://en.wikipedia.org/wiki/Heilbronn_set",
      "text": "In mathematics, a '''Heilbronn set''' is an infinite set ''S'' of natural numbers for which every [[real number]] can be arbitrarily closely approximated by a fraction whose denominator is in&nbsp;''S''. For any given real number <math>\\theta</math> and natural number <math>h</math>, it is easy to find the integer <math>g</math> such that <math>g/h</math> is closest to <math>\\theta</math>. For example, for the real number <math>\\pi</math> and <math>h=100</math> we have <math>g=314</math>. If we call the closeness of <math>\\theta</math> to <math>g/h</math> the difference between <math>h\\theta</math> and <math>g</math>, the closeness is always less than 1/2 (in our example it is 0.15926...). A collection of numbers is a Heilbronn set if for any <math>\\theta</math> we can always find a sequence of values for <math>h</math> in the set where the closeness tends to zero.\n\nMore mathematically let <math>\\|\\alpha\\|</math> denote the distance from <math>\\alpha</math> to the nearest integer then <math>\\mathcal H</math> is a Heilbronn set if and only if for every real number <math>\\theta</math> and every <math>\\varepsilon>0</math> there exists <math>h\\in\\mathcal H</math> such that <math>\\|h\\theta\\|<\\varepsilon</math>.<ref>{{cite book | first=Hugh Lowell | last=Montgomery |authorlink =Hugh Lowell Montgomery | title=Ten lectures on the Interface Between Analytic Number Theory and Harmonic Analysis| volume=84 | series=CBMS Regional Conference Series in Mathematics  | year=1994 | publisher=American Mathematical Society | location=Providence Rhode Island | isbn=0-8218-0737-4 }}</ref>\n\n== Examples ==\nThe natural numbers are a Heilbronn set as [[Dirichlet's approximation theorem]] shows that there exists <math>q<[1/\\varepsilon]</math> with <math>\\|q\\theta\\|<\\varepsilon</math>.\n\nThe <math>k</math>th powers of integers are a Heilbronn set. This follows from a result of [[I. M. Vinogradov]] who showed that for every <math>N</math> and <math>k</math> there exists an exponent <math>\\eta_k>0</math> and <math>q<N</math> such that <math>\\|q^k\\theta\\|\\ll N^{-\\eta_k}</math>.<ref>{{cite journal |first=I. M. |last=Vinogradov |authorlink= I. M. Vinogradov | title= Analytischer Beweis des Satzes uber die Verteilung der Bruchteile eines ganzen Polynoms |year= 1927| volume=21| issue=6 | pages=567−578 | journal=Bull. Acad. Sci. USSR}}</ref> In the case <math>k=2</math> [[Hans Heilbronn]] was able to show that <math>\\eta_2</math> may be taken arbtrarily close to 1/2.<ref>{{cite journal |first=Hans|last=Heilbronn|authorlink = Hans Heilbronn |title= On the distribution of the sequence <math>n^2\\theta\\pmod 1</math> |year= 1948| volume=19 | pages=249−256 | journal=Quart. J. Math., Oxford Ser.|mr=0027294}}</ref> [[Alexandru Zaharescu]] has improved Heilbronn's result to show that <math>\\eta_2</math> may be taken arbitrarily close to 4/7.<ref>{{cite journal |first=Alexandru |last=Zaharescu|authorlink = Alexandru Zaharescu|title= Small values of <math>n^2\\alpha\\pmod 1</math> |year= 1995| volume=121| issue=2 | pages=379−388 | journal=Invent. Math.|mr=1346212}}</ref>\n\nAny [[Van der Corput set]] is also a Heilbronn set.\n\n== Example of a non-Heilbronn set ==\nThe powers of 10 are not a Heilbronn set. Take <math>\\varepsilon=0.001</math> then the statement that <math>\\|10^k\\theta\\|<\\varepsilon</math> for some <math>k</math> is equivalent to saying that the decimal expansion of <math>\\theta</math> has run of three zeros or three nines somewhere. This is not true for all real numbers.\n\n== References ==\n{{reflist}}\n\n[[Category:Analytic number theory]]\n[[Category:Diophantine approximation]]"
    },
    {
      "title": "Hurwitz's theorem (number theory)",
      "url": "https://en.wikipedia.org/wiki/Hurwitz%27s_theorem_%28number_theory%29",
      "text": "{{About|a theorem in number theory||Hurwitz's theorem (disambiguation){{!}}Hurwitz's theorem}}\nIn [[number theory]], '''Hurwitz's theorem''', named after [[Adolf Hurwitz]], gives a [[upper bound|bound]] on a [[Diophantine approximation]]. The theorem states that for every [[irrational number]] ''&xi;'' there are infinitely many [[coprime integers|relatively prime]] integers ''m'', ''n'' such that\n\n:<math>\\left |\\xi-\\frac{m}{n}\\right |<\\frac{1}{\\sqrt{5}\\, n^2}.</math>\n\nThe hypothesis that ''ξ'' is irrational cannot be omitted. Moreover the constant <math>\\scriptstyle \\sqrt{5}</math> is the best possible; if we replace <math>\\scriptstyle \\sqrt{5}</math> by any number <math>\\scriptstyle A > \\sqrt{5}</math> and we let <math>\\scriptstyle \\xi=(1+\\sqrt{5})/2</math> (the [[golden ratio]]) then there exist only ''finitely'' many relatively prime integers ''m'', ''n'' such that the formula above holds.\n\n== References ==\n* {{cite journal\n |first=A. |last= Hurwitz |authorlink=Adolf Hurwitz\n |title=Ueber die angenäherte Darstellung der Irrationalzahlen durch rationale Brüche (On the approximate representation of irrational numbers by rational fractions)\n |language=German\n |journal=[[Mathematische Annalen]]\n |volume=[http://gdz.sub.uni-goettingen.de/en/dms/loader/toc/?PID=PPN235181684_0039   39]\n|issue=2 |pages=279&ndash;284\n |year=1891\n |doi=10.1007/BF01206656\n |jfm=23.0222.02\n}}(note: a PDF version of the paper is available from the given weblink for the volume 39 of the journal, provided by [[GDZ|Göttinger Digitalisierungszentrum]])\n* {{cite book\n |author=[[G. H. Hardy]], Edward M. Wright, Roger Heath-Brown, Joseph Silverman, [[Andrew Wiles]]\n |title=An introduction to the Theory of Numbers\n |edition=6th\n |publisher=Oxford science publications\n |year=2008\n |isbn=0-19-921986-9\n |chapter=Theorem 193\n |page=209\n}}\n* {{Cite journal | last1=LeVeque | first1=William Judson |authorlink=William J. LeVeque| title=Topics in number theory | publisher=Addison-Wesley Publishing Co., Inc., Reading, Mass. | mr=0080682 | year=1956 | postscript=<!--None-->}}\n* {{cite book\n |author=[[Ivan Niven]]\n |title=Diophantine Approximations\n |publisher=Courier Corporation\n |year=2013\n |isbn=0486462676\n}}\n\n[[Category:Diophantine approximation]]\n[[Category:Theorems in number theory]]"
    },
    {
      "title": "Kronecker's theorem",
      "url": "https://en.wikipedia.org/wiki/Kronecker%27s_theorem",
      "text": "{{for| the theorem about the real analytic Eisenstein series|Kronecker limit formula}}\n{{for|the theorem about roots of polynomials|field extension}}\nIn [[mathematics]], '''Kronecker's theorem''' is a theorem about diophantine approximation,  introduced by {{harvs|txt|authorlink= Leopold Kronecker|first=Leopold|last= Kronecker|year=1884}}.\n\nKronecker's approximation theorem had been firstly proved by L. Kronecker in the end of the 19th century. It has been now revealed to relate to the idea of [[n-torus]] and [[Mahler measure]] since the later half of the 20th century. In terms of physical systems, it has the consequence that planets in circular orbits moving uniformly around a star will, over time, assume all alignments, unless there is an exact dependency between their orbital periods.\n\n== Statement ==\n'''Kronecker's theorem'''  is a result in [[diophantine approximation]]s applying to several [[real number]]s ''x<sub>i</sub>'', for 1 ≤ ''i'' ≤ ''n'', that generalises [[Dirichlet's approximation theorem]] to multiple variables.\n\nThe classical Kronecker approximation theorem is formulated as follows. \n\n:''Given real ''n''-[[tuple]]s <math>\\alpha_i=(\\alpha_{i 1},\\cdots,\\alpha_{i n})\\in\\mathbb{R}^n, i=1,\\cdots,m </math> and <math>\\beta=(\\beta_1,\\cdots,\\beta_n)\\in \\mathbb{R}^n</math> , the condition: ''\n::<math>\\forall \\epsilon > 0 \\, \\exists q_i, p_j \\in \\mathbb Z : \\biggl| \\sum^m_{i=1}q_i\\alpha_{ij}-p_j-\\beta_j\\biggr|<\\epsilon,  1\\le j\\le n</math> \n:''holds if and only if for any <math>r_1,\\dots,r_n\\in\\mathbb{Z},\\ i=1,\\dots,m</math> with''\n::<math>\\sum^n_{j=1}\\alpha_{ij}r_j\\in\\mathbb{Z}, \\ \\ i=1,\\dots,m\\ ,</math>\n:''the number <math>\\sum^n_{j=1}\\beta_jr_j</math> is also an integer.''\n\nIn plainer language the first condition states that the tuple <math>\\beta = (\\beta_1, \\ldots, \\beta_n)</math> can be approximated arbitrarily well by linear combinations of the <math>\\alpha_i</math>s and integer vectors.\n\n==Relation to tori==\n\nIn the case of ''N'' numbers, taken as a single ''N''-[[tuple]] and point ''P'' of the [[torus]]\n\n:''T'' = ''R<sup>N</sup>/Z<sup>N</sup>'',\n\nthe [[closure (mathematics)|closure]] of the subgroup <''P''> generated by ''P'' will be finite, or some torus ''T&prime;'' contained in ''T''. The original '''Kronecker's theorem''' ([[Leopold Kronecker]], 1884) stated that the [[necessary condition]] for\n\n:''T&prime;'' = ''T'',\n\nwhich is that the numbers ''x<sub>i</sub>'' together with 1 should be [[linearly independent]] over the [[rational number]]s, is also [[sufficient condition|sufficient]]. Here it is easy to see that if some [[linear combination]] of the ''x<sub>i</sub>'' and 1 with non-zero rational number coefficients is zero, then the coefficients may be taken as integers, and a [[character (mathematics)|character]] χ of the group ''T'' other than the [[trivial character]] takes the value 1 on ''P''. By [[Pontryagin duality]] we have ''T&prime;'' contained in the [[Kernel (group theory)|kernel]] of χ, and therefore not equal to ''T''.\n\nIn fact a thorough use of Pontryagin duality here shows that the whole Kronecker theorem describes the closure of <''P''> as the intersection of the kernels of the χ with\n\n:χ(''P'') = 1.\n\nThis gives an ([[antitone]]) [[Galois connection]] between [[Monogenic semigroup|monogenic]] closed subgroups of ''T'' (those with a single generator, in the topological sense), and sets of characters with kernel containing a given point. Not all closed subgroups occur as monogenic; for example a subgroup that has a \ntorus of dimension ≥ 1 as connected component of the identity element, and that is not connected, cannot be such a subgroup.\n\nThe theorem leaves open the question of how well (uniformly) the multiples ''mP'' of ''P'' fill up the closure.  In the one-dimensional case, the distribution is uniform by the [[equidistribution theorem]].\n\n==See also==\n* [[Weyl's criterion]]\n\n== References ==\n\n*{{citation|last=Kronecker|first= L.\n|title=Näherungsweise ganzzahlige Auflösung linearer Gleichungen\n|journal=Berl. Ber.|year= 1884|pages= 1179–1193, 1271–1299|url=https://archive.org/stream/n1werkehrsgaufvera03kronuoft#page/46 }}\n*{{eom|first=A.L.|last= Onishchik|id=k/k055910|title=Kronecker's theorem}}\n*<ref>{{cite web | url=http://mathworld.wolfram.com/KroneckersApproximationTheorem.html | title=Kronecker's Approximation Theorem | publisher=Wolfram Mathworld | accessdate=22 December 2015}}</ref>\n<references/>\n\n[[Category:Diophantine approximation]]\n[[Category:Topological groups]]"
    },
    {
      "title": "Lagrange number",
      "url": "https://en.wikipedia.org/wiki/Lagrange_number",
      "text": "In [[mathematics]], the '''Lagrange numbers''' are a sequence of numbers that appear in bounds relating to the approximation of [[irrational numbers]] by [[rational numbers]].  They are linked to [[Hurwitz's theorem (number theory)|Hurwitz's theorem]].\n\n==Definition==\n\nHurwitz improved [[Peter Gustav Lejeune Dirichlet]]'s criterion on irrationality to the statement that a real number α is irrational if and only if there are infinitely many rational numbers ''p''/''q'', written in lowest terms, such that\n:<math>\\left|\\alpha - \\frac{p}{q}\\right| < \\frac{1}{\\sqrt{5}q^2}.</math>\n\nThis was an improvement on Dirichlet's result which had 1/''q''<sup>2</sup> on the right hand side.  The above result is best possible since the [[golden ratio]] φ is irrational but if we replace {{radic|5}} by any larger number in the above expression then we will only be able to find finitely many rational numbers that satisfy the inequality for α&nbsp;=&nbsp;φ.\n\nHowever, Hurwitz also showed that if we omit the number φ, and numbers derived from it, then we ''can'' increase the number {{radic|5}}. In fact he showed we may replace it with 2{{radic|2}}.  Again this new bound is best possible in the new setting, but this time the number {{radic|2}} is the problem.  If we don't allow {{radic|2}} then we can increase the number on the right hand side of the inequality from 2{{radic|2}} to {{radic|221}}/5.  Repeating this process we get an infinite sequence of numbers {{radic|5}}, 2{{radic|2}}, {{radic|221}}/5, ... which converge to 3.<ref>Cassels (1957) p.14</ref> These numbers are called the '''Lagrange numbers''',<ref>Conway&Guy (1996) pp.187-189</ref> and are named after [[Joseph Louis Lagrange]].\n\n==Relation to Markov numbers==\n\nThe ''n''th Lagrange number ''L<sub>n</sub>'' is given by\n:<math>L_n=\\sqrt{9-\\frac{4}{{m_n}^2}}</math>\nwhere ''m<sub>n</sub>'' is the ''n''th [[Markov number]],<ref>Cassels (1957) p.41</ref> that is the ''n''th smallest integer ''m'' such that the equation\n:<math>m^2+x^2+y^2=3mxy\\,</math>\nhas a solution in positive integers ''x'' and ''y''.\n\n==References==\n{{reflist}}\n* {{cite book | first=J.W.S. | last=Cassels | authorlink=J. W. S. Cassels | title=An introduction to Diophantine approximation | series=Cambridge Tracts in Mathematics and Mathematical Physics | volume=45 | publisher=[[Cambridge University Press]] | year=1957 | zbl=0077.04801 }}\n* {{cite book | first1=J.H. | last1=Conway | author1-link=John Horton Conway | first2=R.K. | last2=Guy | author2-link=Richard K. Guy | title=The Book of Numbers | location=New York | publisher=[[Springer-Verlag]] | year=1996 | isbn=0-387-97993-X | zbl= }}\n\n==External links==\n*[http://mathworld.wolfram.com/LagrangeNumber.html Lagrange number].  From [[MathWorld]] at [[Wolfram Research]].\n*[http://www.math.jussieu.fr/~miw/articles/pdf/IntroductionDiophantineMethods.pdf Introduction to Diophantine methods irrationality and transcendence] - Online lecture notes by [[Michel Waldschmidt]], Lagrange Numbers on pp.&nbsp;24–26.\n\n[[Category:Diophantine approximation]]"
    },
    {
      "title": "Liouville number",
      "url": "https://en.wikipedia.org/wiki/Liouville_number",
      "text": "In [[number theory]], a '''Liouville number''' is a [[real number]] ''x'' with the property that, for every positive [[integer]] ''n'', there exist infinitely many pairs of integers (''p, q'') with ''q'' > 1 such that\n\n:<math>0<  \\left |x- \\frac{p}{q} \\right| < \\frac{1}{q^{n}}. </math>\n\nLiouville numbers are \"almost rational\", and can thus be approximated \"quite closely\" by [[sequence]]s of rational numbers. They are precisely the [[transcendental number|transcendental]] numbers that can be more closely approximated by rational numbers than any algebraic irrational number. In 1844, [[Joseph Liouville]] showed that all Liouville numbers are transcendental, thus establishing the existence of transcendental numbers for the first time.\n\n== The existence of Liouville numbers (Liouville's constant) ==\nHere we show that Liouville numbers exist by exhibiting a construction that produces such numbers.\n\nFor any integer ''b'' ≥ 2, and any sequence of integers (''a''<sub>1</sub>, ''a''<sub>2</sub>,&nbsp;&hellip;,&nbsp;), such that ''a''<sub>''k''</sub>&nbsp;∈&nbsp;{0,&nbsp;1,&nbsp;2,&nbsp;&hellip;,&nbsp;''b''&nbsp;−&nbsp;1} ∀''k''&nbsp;∈&nbsp;{1,&nbsp;2,&nbsp;3,&nbsp;&hellip;} and there are infinitely many k with ''a''<sub>''k''</sub> ≠ 0, define the number\n\n:<math>x = \\sum_{k=1}^\\infty \\frac{a_k}{b^{k!}}</math>\n\nIn the special case when ''b''&nbsp;=&nbsp;10, and ''a''<sub>''k''</sub>&nbsp;=&nbsp;1, ∀''k'', the resulting number ''x'' is called '''Liouville's constant:'''\n\n:''L'' = 0.'''11'''000'''1'''00000000000000000'''1'''0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000'''1'''...\n\nIt follows from the definition of ''x'' that its base-''b'' representation is\n\n:<math>x = \\left(0.a_{1}a_{2}000a_{3}00000000000000000a_{4}0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000a_{5}\\ldots a_{n}\\left[\\left(nn!-1\\right)\\text{zeros}\\right]a_{n+1}\\ldots\\right)_b\\;.</math>\n\nSince this base-''b'' representation is non-repeating it follows that ''x'' cannot be rational.  Therefore, for any rational number ''p''/''q'', we have |''x''&nbsp;&minus;&nbsp;''p''/''q''&nbsp;|&nbsp;>&nbsp;0.\n\nNow, for any integer ''n'' ≥ 1, define ''q''<sub>''n''</sub> and ''p''<sub>''n''</sub> as follows:\n\n:<math>q_n = b^{n!}\\,; \\quad p_n = q_n \\sum_{k=1}^n \\frac{a_k}{b^{k!}} = \\sum_{k=1}^n {a_k}{b^{n!-k!}} \\; .</math>\n\nThen\n\n:<math>\n\\begin{align}\n0 < \\left|x - \\frac{p_n}{q_n}\\right| & = \\left|x - \\frac{q_n\\sum_{k=1}^n \\frac{a_k}{b^{k!}}}{q_n}\\right| = \\left|x - \\sum_{k=1}^n \\frac{a_k}{b^{k!}}\\right| = \\left|\\sum_{k=1}^\\infty \\frac{a_k}{b^{k!}} - \\sum_{k=1}^n \\frac{a_k}{b^{k!}}\\right| = \\left|\\left(\\sum_{k=1}^n \\frac{a_k}{b^{k!}} + \\sum_{k=n+1}^\\infty \\frac{a_k}{b^{k!}}\\right) - \\sum_{k=1}^n \\frac{a_k}{b^{k!}}\\right| = \\sum_{k=n+1}^\\infty \\frac{a_k}{b^{k!}} \\\\[6pt]\n& \\le \\sum_{k=n+1}^\\infty \\frac{b-1}{b^{k!}} < \\sum_{k=(n+1)!}^\\infty \\frac{b-1}{b^k} = \\frac{b-1}{b^{(n+1)!}} + \\frac{b-1}{b^{(n+1)!+1}} + \\frac{b-1}{b^{(n+1)!+2}} + ... = \\frac{b-1}{b^{(n+1)!}b^{0}} + \\frac{b-1}{b^{(n+1)!}b^{1}} + \\frac{b-1}{b^{(n+1)!}b^{2}} + ... = \\frac{b-1}{b^{(n+1)!}} \\sum_{k=0}^\\infty \\frac{1}{b^k} \\\\[6pt]\n& = \\frac{b-1}{b^{(n+1)!}}\\cdot\\frac{b}{b-1} = \\frac{b}{b^{(n+1)!}} \\le \\frac{b^{n!}}{b^{(n+1)!}} = \\frac{1}{b^{(n+1)! - n!}} = \\frac{1}{b^{(n+1)n! - n!}} = \\frac{1}{b^{n(n!) + n! - n!}} = \\frac{1}{b^{(n!)n}} = \\frac{1}{{q_n}^n}\n\\end{align}\n</math>\n\nTherefore, we conclude that any such ''x'' is a Liouville number.\n\n=== Notes on the proof ===\n\n# The inequality <math>\\sum_{k=n+1}^\\infty \\frac{a_k}{b^{k!}} \\le \\sum_{k=n}^\\infty \\frac{b-1}{b^{k!}}</math>follows from the fact that \"il existe\" k, ''a''<sub>''k''</sub>&nbsp;∈&nbsp;{0,&nbsp;1,&nbsp;2,&nbsp;&hellip;, b−1}. Therefore, at most, ''a''<sub>''k''</sub>&nbsp;= b-1. The largest possible sum would occur if the sequence of integers, (''a''<sub>1</sub>, ''a''<sub>2</sub>,&nbsp;&hellip;), were (b-1, b-1, ...) where ''a''<sub>''k''</sub>&nbsp;= b-1, ∀ k. <math>\\sum_{k=n+1}^\\infty \\frac{a_k}{b^{k!}}</math>will thus be less than, or equal to, this largest possible sum.\n# The strong inequality <math>\n\\begin{align}\n\\sum_{k=n+1}^\\infty \\frac{b-1}{b^{k!}} < \\sum_{k=(n+1)!}^\\infty \\frac{b-1}{b^k}\n\\end{align}\n</math>follows from our motivation to eliminate the series by way of reducing it to a series for which we know a formula. In the proof so far, the purpose for introducing the inequality in 1. comes from intuition that <math>\\sum_{k=0}^\\infty \\frac{1}{b^{k}} = \\frac{b}{b-1}</math>(the [[geometric series]] formula); therefore, if we can find an inequality from <math>\\sum_{k=n+1}^\\infty \\frac{a_k}{b^{k!}}</math>that introduces a series with (b-1) in the numerator, and if we can work to further reduce the denominator term <math>b^{k!}</math>to <math>b^{k}</math>, as well as shifting the series indices from 0 to <math>\\infty</math>, then we will be able to eliminate both series and (b-1) terms, getting us closer to a fraction of the form <math>\\frac{1}{b^{(exponent)*n}}</math>, which is the end-goal of the proof. We further this motivation here by selecting now from the sum <math>\\sum_{k=n+1}^\\infty \\frac{b-1}{b^{k!}}</math>a partial sum. Observe that, for any term in <math>\\sum_{k=n+1}^\\infty \\frac{b-1}{b^{k!}}</math>, since  b ≥ 2, then <math>\\frac{b-1}{b^{k!}} < \\frac{b-1}{b^{k}}</math>, ∀ k (except for when n=1). Therefore, <math>\n\\begin{align}\n\\sum_{k=n+1}^\\infty \\frac{b-1}{b^{k!}} < \\sum_{k=n+1}^\\infty \\frac{b-1}{b^k}\n\\end{align}\n</math>(since, even if n=1, all subsequent terms are smaller). In order to manipulate the indices so that k starts at 0, we select a partial sum from within <math>\n\\sum_{k=n+1}^\\infty \\frac{b-1}{b^k}\n</math>(also less than the total value since it's a partial sum from a series whose terms are all positive). We will choose the partial sum formed by starting at k = (n+1)! which follows from our motivation to write a new series with k=0, namely by noticing that <math>b^{(n+1)!} = b^{(n+1)!}b^0</math>.\n#For the final inequality <math>\\frac{b}{b^{(n+1)!}} \\le \\frac{b^{n!}}{b^{(n+1)!}}</math>, we have chosen this particular inequality (true because ''b'' ≥ 2, where equality follows if and only if n=1) because we wish to manipulate <math>\\frac{b}{b^{(n+1)!}}</math>into something of the form <math>\\frac{1}{b^{(exponent)*n}}</math>. This particular inequality allows us to eliminate (n+1)! and the numerator, using the property that (n+1)! - n! = (n!)n, thus putting the denominator in ideal form for the substitution <math>q_n = b^{n!}</math>.\n\n== Irrationality ==\nHere we will show that the number ''x''&nbsp;=&nbsp;''c''/''d'', where ''c'' and ''d'' are integers and ''d''&nbsp;>&nbsp;0, cannot satisfy the inequalities that define a Liouville number. Since every [[rational number]] can be represented as such ''c''/''d'', we will have proven that '''no Liouville number can be rational'''.\n\nMore specifically, we show that for any positive integer ''n'' large enough that 2<sup>''n''&nbsp;−&nbsp;1</sup>&nbsp;>&nbsp;''d''&nbsp;>&nbsp;0 (that is, for any integer ''n''&nbsp;>&nbsp;1&nbsp;+&nbsp;log<sub>2</sub>(''d''&nbsp;)&nbsp;) no pair of integers (''p'',&nbsp;''q''&nbsp;) exists that simultaneously satisfies the two inequalities\n\n:<math>0 < \\left|x- \\frac{p}{q}\\right| < \\frac{1}{q^n}\\, .</math>\n\nFrom this the claimed conclusion follows.\n\nLet ''p'' and ''q'' be any integers with ''q''&nbsp;>&nbsp;1.  Then we have,\n\n:<math>\\left|x- \\frac{p}{q}\\right|= \\left| \\frac{c}{d} - \\frac{p}{q}  \\right| = \\frac{|cq - dp|}{dq}</math>\n\nIf |''cq''&nbsp;−&nbsp;''dp''&nbsp;|&nbsp;=&nbsp;0, we would have\n\n:<math>\\left|x- \\frac{p}{q}\\right|= \\frac{|cq - dp|}{dq} = 0\\, ,</math>\n\nmeaning that such pair of integers (''p'',&nbsp;''q''&nbsp;) would violate the ''first'' inequality in the definition of a Liouville number, irrespective of any choice of ''n''.\n\nIf, on the other hand, |''cq''&nbsp;−&nbsp;''dp''&nbsp;|&nbsp;>&nbsp;0, then, since ''cq''&nbsp;−&nbsp;''dp'' is an integer, we can assert the sharper inequality |''cq''&nbsp;−&nbsp;''dp''&nbsp;|&nbsp;≥&nbsp;1.  From this it follows that\n\n:<math>\\left|x- \\frac{p}{q}\\right|= \\frac{|cq - dp|}{dq} \\ge \\frac{1}{dq}</math>\n\nNow for any integer ''n''&nbsp;>&nbsp;1&nbsp;+&nbsp;log<sub>2</sub>(''d''&nbsp;), the last inequality above implies\n\n:<math>\\left|x- \\frac{p}{q}\\right| \\ge \\frac{1}{dq} > \\frac{1}{2^{n-1}q} \\ge \\frac{1}{q^n}\\,.</math>\n\nTherefore, in the case |''cq''&nbsp;−&nbsp;''dp''&nbsp;|&nbsp;>&nbsp;0 such pair of integers (''p'',&nbsp;''q''&nbsp;) would violate the ''second'' inequality in the definition of a Liouville number, for some positive integer ''n''.\n\nWe conclude that there is no pair of integers (''p'',&nbsp;''q''&nbsp;), with ''q''&nbsp;>1, that would qualify such an ''x''&nbsp;=&nbsp;''c''/''d'' as a Liouville number.\n\nHence a Liouville number, if it exists, cannot be rational.\n\n(The section on [[#The existence of Liouville numbers (Liouville's constant)|''Liouville's constant'']] proves that Liouville numbers exist by exhibiting the construction of one.  The proof given in this section implies that this number must be [[irrational number|irrational]].)\n\n==Uncountability==\nConsider, for example, the number\n\n:3.1400010000000000000000050000....\n3.14(3 zeros)1(17 zeros)5(95 zeros)9(599 zeros)2(4319 zeros)6...\n\nwhere the digits are zero except in positions ''n''! where the digit equals the ''n''th digit following the decimal point in the decimal expansion of&nbsp;{{pi}}.\n\nAs shown in the section on [[#The existence of Liouville numbers (Liouville's constant)|the existence of Liouville numbers]], this number, as well as any other non-terminating decimal with its non-zero digits similarly situated, satisfies the definition of a Liouville number. Since the set of all sequences of non-null digits has the [[cardinality of the continuum]], the same thing occurs with the set of all Liouville numbers.\n\nMoreover, the Liouville numbers form a [[Dense set|dense]] subset of the set of real numbers.\n\n==Liouville numbers and measure ==\nFrom the point of view of measure theory, the set of all Liouville numbers ''L'' is small. More precisely, its [[Lebesgue measure]] is zero. The proof given follows some ideas by John C. Oxtoby.<ref name=\"oxtoby\">{{Cite book | last = Oxtoby | first = John C. | year = 1980 | title = Measure and Category | series = Graduate Texts in Mathematics | volume = 2 | edition = Second | publisher = Springer-Verlag | isbn = 0-387-90508-1 | location = New York-Berlin | mr=0584443 | doi=10.1007/978-1-4684-9339-9}}</ref>{{Rp|8}}\n\nFor positive integers ''n'' > 2 and ''q'' ≥ 2 set:\n\n:<math>V_{n,q}=\\bigcup\\limits_{p=-\\infty}^\\infty \\left(\\frac{p}{q}-\\frac{1}{q^n},\\frac{p}{q}+\\frac{1}{q^n}\\right)</math>\n\nwe have\n\n:<math>L\\subseteq \\bigcup_{q=2}^\\infty V_{n,q}.</math>\n\nObserve that for each positive integer ''n'' ≥ 2 and ''m'' ≥ 1, we also have\n\n:<math>L\\cap (-m,m)\\subseteq \\bigcup\\limits_{q=2}^\\infty V_{n,q}\\cap(-m,m)\\subseteq \\bigcup\\limits_{q=2}^\\infty\\bigcup\\limits_{p=-mq}^{mq} \\left( \\frac{p}{q}-\\frac{1}{q^n},\\frac{p}{q}+\\frac{1}{q^n}\\right).</math>\n\nSince\n\n: <math> \\left|\\left(\\frac{p}{q}+\\frac{1}{q^n}\\right)-\\left(\\frac{p}{q}-\\frac{1}{q^n}\\right)\\right|=\\frac{2}{q^n}</math>\n\nand ''n'' > 2 we have\n\n: <math>\n\\begin{align}\nm(L\\cap (-m,\\, m)) & \\leq\\sum_{q=2}^\\infty\\sum_{p=-mq}^{mq}\\frac{2}{q^n} = \\sum_{q=2}^\\infty \\frac{2(2mq+1)}{q^n} \\\\[6pt]\n& \\leq (4m+1)\\sum_{q=2}^\\infty\\frac{1}{q^{n-1}} \\leq (4m+1) \\int^\\infty_1 \\frac{dq}{q^{n-1}}\\leq\\frac{4m+1}{n-2}.\n\\end{align}\n</math>\n\nNow\n\n:<math>\\lim_{n\\to\\infty}\\frac{4m+1}{n-2}=0</math>\n\nand it follows that for each positive integer ''m'', ''L'' ∩ (−''m'', ''m'') has Lebesgue measure zero. Consequently, so has ''L''.\n\nIn contrast, the Lebesgue measure of the set ''T'' of ''all'' real transcendental numbers is [[Infinity|infinite]] (since ''T'' is the complement of a null set).\n\nIn fact, the [[Hausdorff dimension]] of ''L'' is zero, which implies that the [[Hausdorff measure]] of ''L'' is zero for all dimension ''d'' > 0.<ref name=\"oxtoby\" /> Hausdorff dimension of ''L'' under other dimension functions has also been investigated.<ref name=\"olsen\">{{Cite journal  | first1=Lars Ole Rønnow | last1=Olsen | first2=Dave L. | last2=Renfro  | year=2006 | title = On the exact Hausdorff dimension of the set of Liouville numbers. II | journal = [[Manuscripta Mathematica]] | volume = 119 | issue = 2 | pages = 217–224  | doi = 10.1007/s00229-005-0604-z | mr=2215968}}</ref>\n\n==Structure of the set of Liouville numbers==\nFor each positive integer ''n'', set\n\n:<math>U_n  =\\bigcup\\limits_{q=2}^\\infty\\bigcup\\limits_{p=-\\infty}^\\infty  \\left\\{ x \\in \\mathbf R : 0<  \\left |x- \\frac{p}{q} \\right |< \\frac{1}{q^{n}}\\right\\} = \\bigcup\\limits_{q=2}^\\infty\\bigcup\\limits_{p=-\\infty}^\\infty \\left(\\frac{p}{q}-\\frac{1}{q^n},\\frac{p}{q}+\\frac{1}{q^n}\\right) \\setminus \\left\\{\\frac{p}{q}\\right\\}</math>\n\nThe set of all Liouville numbers can thus be written as\n\n:<math>L=\\bigcap\\limits_{n=1}^\\infty U_n = \\bigcap\\limits_{n\\in\\mathbb{Z}^+}\\bigcup\\limits_{q\\in\\mathbb{Z}_{\\geqslant 2}}\\bigcup\\limits_{p\\in\\mathbb{Z}} \\left(\\left(\\frac{p}{q}-\\frac{1}{q^n},\\frac{p}{q}+\\frac{1}{q^n}\\right) \\setminus \\left\\{\\frac{p}{q}\\right\\}\\right).</math>\n\nEach ''U<sub>n</sub>'' is an [[open set]]; as its closure contains all rationals (the <math>\\displaystyle p/q</math> from each punctured interval), it is also a [[dense set|dense]] subset of real line. Since it is the intersection of countably many such open dense sets, ''L'' is [[Meagre set|comeagre]], that is to say, it is a ''dense'' [[G-delta set|G<sub>δ</sub>]] set.\n\nAlong with the above remarks about measure, it shows that the set of Liouville numbers and its complement decompose the reals into two sets, one of which is meagre, and the other of Lebesgue measure zero.\n\n== Irrationality measure ==\nThe '''Liouville-Roth irrationality measure''' ('''irrationality exponent,''' '''approximation exponent,''' or '''Liouville–Roth constant''') of a real number ''x'' is a measure of how \"closely\" it can be approximated by rationals. Generalizing the definition of Liouville numbers, instead of allowing any ''n'' in the power of ''q'', we find the [[least upper bound]] of the set of ''real'' numbers ''μ'' such that\n\n:<math>0< \\left| x- \\frac{p}{q} \\right| < \\frac{1}{q^\\mu} </math>\n\nis satisfied by an infinite number of integer pairs (''p'', ''q'') with ''q'' > 0. This least upper bound is defined to be the irrationality measure of ''x''.<ref name=bugeaud>{{cite book | last=Bugeaud | first=Yann | title=Distribution modulo one and Diophantine approximation | series=Cambridge Tracts in Mathematics | volume=193 | location=Cambridge | publisher=[[Cambridge University Press]] | year=2012 | isbn=978-0-521-11169-0 | zbl=1260.11001 | mr=2953186 | doi=10.1017/CBO9781139017732}}</ref>{{rp|246}}  For any value ''μ'' less than this upper bound, the infinite set of all rationals ''p''/''q'' satisfying the above inequality yield an approximation of ''x''. Conversely, if ''μ'' is greater than the upper bound, then there are at most finitely many (''p'', ''q'') with ''q'' > 0 that satisfy the inequality; thus, the opposite inequality holds for all larger values of ''q''. In other words, given the irrationality measure ''μ'' of a real number ''x'', whenever a rational approximation ''x''&nbsp;≅&nbsp;''p''/''q'', ''p'',''q''&nbsp;∈&nbsp;'''N''' yields ''n''&nbsp;+&nbsp;1 exact decimal digits, we have\n\n:<math>\\frac{1}{10^n} \\ge \\left| x- \\frac{p}{q} \\right| \\ge \\frac{1}{q^{\\mu+\\varepsilon}} </math>\n\nfor any ε>0, except for at most a finite number of \"lucky\"  pairs (''p'', ''q'').\n\nFor a rational number ''α'' the irrationality measure is ''μ''(''α'')&nbsp;=&nbsp;1.<ref name=bugeaud/>{{rp|246}} The [[Thue–Siegel–Roth theorem]] states that if ''α'' is an [[algebraic number]], real but not rational, then ''μ''(''α'')&nbsp;=&nbsp;2.<ref name=bugeaud/>{{rp|248}}\n\nAlmost all numbers have an irrationality measure equal to 2.<ref name=bugeaud/>{{rp|246}}\n\nTranscendental numbers have irrationality measure 2 or greater. For example, the transcendental number ''[[e (mathematical constant)|e]]'' has ''μ''(''e'')&nbsp;=&nbsp;2.<ref name=bugeaud/>{{rp|185}} The irrationality measures of {{pi}}, log 2, and log 3 are at most 7.60630853, 3.57455391, and 5.125, respectively.<ref>{{cite journal | zbl=1140.11036 | last=Zudilin | first=Wadim | title=An essay on the irrationality measure of {{pi}} and other logarithms | language=ru | journal=Chebyshevskii Sbornik | volume=5 | number=2(10) | pages=49–65 | year=2004 | arxiv=math/0404523 | mr=2140069 | bibcode=2004math......4523Z }}</ref>\n\nIt has been proven that if the series <math>\\displaystyle\\sum^\\infty_{n=1}\\frac{\\csc^2 n}{n^3}</math> (where ''n'' is in radians) converges, then <math>\\pi</math>'s irrationality measure is most 2.5.<ref>Max A. Alekseyev, [https://arxiv.org/abs/1104.5100 On convergence of the Flint Hills series], arXiv:1104.5100, 2011.</ref><ref>{{MathWorld|FlintHillsSeries|Flint Hills Series}}</ref>\n\nThe Liouville numbers are precisely those numbers having infinite irrationality measure.<ref name=bugeaud/>{{rp|248}}\n\n===Irrationality base===\n\nThe ''irrationality base'' is a weaker measure of irrationality introduced by J.Sondow<ref>Sondow, Jonathan. (2004). Irrationality Measures, Irrationality Bases, and a Theorem of Jarnik. https://arxiv.org/abs/math/0406300</ref> and is regarded as an irrationality measure for Liouville numbers. It is defined as follows:\n\nLet <math>\\alpha </math> be an irrational number. If there exists a real number <math> \\beta \\geq 1 </math> with the property that for any <math> \\varepsilon >0 </math>, there is a positive integer <math> q(\\varepsilon)</math> such that\n\n: <math> \\left| \\alpha-\\frac{p}{q} \\right| > \\frac 1 {(\\beta+\\varepsilon)^q} \\text{ for all integers } p,q \\text{ with } q \\geq q(\\varepsilon) </math>,\n\nthen <math>\\beta</math> is called the irrationality base of <math>\\alpha</math> and is represented as <math>\\beta(\\alpha)</math>\n\nIf no such <math>\\beta</math> exists, then <math>\\alpha</math> is called a ''super Liouville number''.\n\n'''Example''': The series <math>\\varepsilon_{2e}=1+\\frac{1}{2^1}+\\frac{1}{4^{2^1}}+\\frac{1}{8^{4^{2^1}}}+\\frac{1}{16^{8^{4^{2^1}}}}+\\frac{1}{32^{16^{8^{4^{2^1}}}}}+\\ldots</math> is a ''super Liouville number''.\n\n== Liouville numbers and transcendence ==\nEstablishing that a given number is a Liouville number provides a useful tool for proving a given number is transcendental. However, not every transcendental number is a Liouville number. The terms in the [[continued fraction]] expansion of every Liouville number are unbounded; using a counting argument, one can then show that there must be uncountably many transcendental numbers which are not Liouville. Using the explicit continued fraction expansion of [[e (mathematical constant)|''e'']], one can show that ''e'' is an example of a transcendental number that is not Liouville. Mahler proved in 1953 that [[pi|{{pi}}]] is another such example.<ref>The irrationality measure of {{pi}} does not exceed 7.6304, according to {{MathWorld |title=Irrationality Measure |urlname=IrrationalityMeasure}}</ref>\n\nThe proof proceeds by first establishing a property of [[irrational number|irrational]] [[algebraic number]]s. This property essentially says that irrational algebraic numbers cannot be well approximated by rational numbers, where the condition for \"well approximated\" becomes more stringent for larger denominators. A Liouville number is irrational but does not have this property, so it can't be algebraic and must be transcendental. The following [[lemma (mathematics)|lemma]] is usually known as '''Liouville's theorem (on diophantine approximation)''', there being several results known as [[Liouville's theorem (disambiguation)|Liouville's theorem]]<!--intentional link to DAB page-->. \n\nBelow, we will show that '''no Liouville number can be algebraic.'''\n\n'''Lemma:''' If ''α'' is an irrational number which is the root of a [[polynomial]] ''f'' of degree ''n'' > 0 with integer coefficients, then there exists a real number ''A'' > 0 such that, for all integers ''p'', ''q'', with ''q'' > 0,\n\n: <math>  \\left| \\alpha - \\frac{p}{q}  \\right | > \\frac{A}{q^n} </math>\n\n'''Proof of Lemma:''' Let ''M'' be the maximum value of |''f'' ′(''x'')| (the [[absolute value]] of the [[derivative]] of ''f'') over the [[interval (mathematics)|interval]] [''α''&nbsp;−&nbsp;1, ''α''&nbsp;+&nbsp;1]. Let ''α''<sub>1</sub>, ''α''<sub>2</sub>, ..., ''α''<sub>''m''</sub> be the distinct roots of ''f'' which differ from ''α''. Select some value ''A'' > 0 satisfying\n\n: <math>A< \\min \\left(1, \\frac{1}{M}, \\left| \\alpha - \\alpha_1 \\right|, \\left| \\alpha - \\alpha_2 \\right|, \\ldots , \\left| \\alpha-\\alpha_m \\right| \\right)  </math>\n\nNow assume that there exist some integers ''p'', ''q'' contradicting the lemma. Then\n\n: <math>\\left| \\alpha - \\frac{p}{q}\\right| \\le \\frac{A}{q^n} \\le A< \\min\\left(1, \\frac{1}{M}, \\left| \\alpha - \\alpha_1 \\right|, \\left|\\alpha - \\alpha_2 \\right|, \\ldots , \\left| \\alpha-\\alpha_m \\right| \\right) </math>\n\nThen ''p''/''q'' is in the interval [''α'' − 1, ''α'' + 1]; and ''p''/''q'' is not in {''α''<sub>1</sub>, ''α''<sub>2</sub>, ..., ''α''<sub>''m''</sub>}, so ''p''/''q'' is not a root of ''f''; and there is no root of ''f'' between ''α'' and ''p''/''q''.\n\nBy the [[mean value theorem]], there exists an ''x''<sub>0</sub> between ''p''/''q'' and ''α'' such that\n\n: <math>f(\\alpha)-f(\\tfrac{p}{q}) = (\\alpha - \\frac{p}{q}) \\cdot f'(x_0)</math>\n\nSince ''α'' is a root of ''f'' but ''p''/''q'' is not, we see that |''f'' ′(''x''<sub>0</sub>)| > 0 and we can rearrange:\n\n: <math>\\left|\\alpha -\\frac{p}{q}\\right |= \\frac{\\left | f(\\alpha)- f(\\tfrac{p}{q})\\right |}{|f'(x_0)|} = \\left | \\frac{f(\\tfrac{p}{q})}{f'(x_0)} \\right |</math>\n\nNow, ''f'' is of the form <math>\\sum_{i=0}^n</math> ''c''<sub>''i''</sub> ''x''<sup>''i''</sup> where each ''c''<sub>''i''</sub> is an integer; so we can express |''f''(''p''/''q'')| as\n\n: <math>\\left|f \\left (\\frac{p}{q} \\right) \\right| = \\left| \\sum_{i=0}^n c_i p^i q^{-i} \\right| = \\frac{1}{q^n} \\left| \\sum_{i=0}^n c_i p^i q^{n-i} \\right | \\ge \\frac {1}{q^n} </math>\n\nthe last inequality holding because ''p''/''q'' is not a root of ''f'' and the ''c''<sub>''i''</sub> are integers.\n\nThus we have that |''f''(''p''/''q'')| ≥ 1/''q''<sup>''n''</sup>. Since |''f'' ′(''x''<sub>0</sub>)| ≤ ''M'' by the definition of ''M'', and 1/''M'' > ''A'' by the definition of ''A'', we have that\n\n: <math>\\left | \\alpha - \\frac{p}{q} \\right | = \\left|\\frac{f(\\tfrac{p}{q})}{f'(x_0)}\\right| \\ge \\frac{1}{Mq^n} > \\frac{A}{q^n} \\ge \\left| \\alpha - \\frac{p}{q} \\right|</math>\n\nwhich is a contradiction; therefore, no such ''p'', ''q'' exist; proving the lemma.\n\n'''Proof of assertion:''' As a consequence of this lemma, let ''x'' be a Liouville number; as noted in the article text, ''x'' is then irrational. If ''x'' is algebraic, then by the lemma, there exists some integer ''n'' and some positive real ''A'' such that for all ''p'', ''q''\n\n: <math>  \\left| x - \\frac{p}{q}  \\right|> \\frac{A}{q^{n}} </math>\n\nLet ''r'' be a positive integer such that 1/(2<sup>''r''</sup>) ≤ ''A''. If we let ''m'' = ''r'' + ''n'', and since ''x'' is a Liouville number, then there exist integers ''a'', ''b'' where ''b'' > 1 such that\n\n: <math>\\left|x-\\frac ab\\right|<\\frac1{b^m}=\\frac1{b^{r+n}}=\\frac1{b^rb^n} \\le \\frac1{2^r}\\frac1{b^n} \\le \\frac A{b^n} </math>\n\nwhich contradicts the lemma. Hence, if a Liouville number exists, it cannot be algebraic, and therefore must be transcendental.\n\n== See also ==\n* [[Brjuno number]]\n* [[Diophantine approximation]]\n\n== References ==\n{{reflist}}\n\n==External links==\n*[http://www.math.sc.edu/~filaseta/gradcourses/Math785/Math785Notes5.pdf The Beginning of Transcendental Numbers]\n{{Irrational number}}\n\n{{DEFAULTSORT:Liouville Number}}\n[[Category:Diophantine approximation]]\n[[Category:Transcendental numbers]]\n[[Category:Mathematical constants]]\n[[Category:Articles containing proofs]]\n[[Category:Real transcendental numbers]]\n[[Category:Irrational numbers]]"
    },
    {
      "title": "Littlewood conjecture",
      "url": "https://en.wikipedia.org/wiki/Littlewood_conjecture",
      "text": "In [[mathematics]], the '''Littlewood conjecture''' is an [[open problem]] ({{As of|2016|lc=on}}) in [[Diophantine approximation]], proposed by [[John Edensor Littlewood]] around 1930. It states that for any two [[real number]]s α and β,\n\n:<math>\\liminf_{n\\to\\infty} \\ n\\,\\Vert n\\alpha\\Vert \\,\\Vert n\\beta\\Vert = 0,</math>\n\nwhere <math>\\Vert \\,\\Vert</math> is here the distance to the nearest integer.\n\n==Formulation and explanation==\n\nThis means the following: take a point (α,β) in the plane, and then consider the sequence of points\n\n:(2α,2β), (3α,3β), ... .\n\nFor each of these, multiply the distance to the closest line with integer x-coordinate by the distance to the closest line with integer y-coordinate. This product will certainly be at most 1/4. The conjecture makes no statement about whether this sequence of values will [[Convergence (mathematics)|converge]]; it typically does not, in fact. The conjecture states something about the [[Limit superior and limit inferior|limit inferior]], and says that there is a subsequence for which the distances decay faster than the reciprocal, i.e.\n\n:o(1/''n'')\n\nin the [[Big O notation#Little-o notation|little-o notation]].\n\n==Connection to further conjectures==\n\nIt is known that this would follow from a result in the [[geometry of numbers]], about the minimum on a non-zero [[lattice (group)|lattice]] point of a product of three linear forms in three real variables: the implication was shown in 1955 by [[J. W. S. Cassels]] and [[Swinnerton-Dyer]].<ref>{{cite journal |author1=J.W.S. Cassels |author2=H.P.F. Swinnerton-Dyer | title=On the product of three homogeneous linear forms and the indefinite ternary quadratic forms | journal=[[Philosophical Transactions of the Royal Society A]] | volume=248 | issue=940 | date=1955-06-23 | pages=73–96 | doi=10.1098/rsta.1955.0010 | jstor=91633 | mr = 70653 |zbl=0065.27905| bibcode=1955RSPTA.248...73C }}</ref> This can be formulated another way, in group-theoretic terms. There is now another conjecture, expected to hold for ''n'' ≥ 3: it is stated in terms of ''G'' = ''SL<sub>n</sub>''(''R''), Γ = ''SL<sub>n</sub>''(''Z''), and the subgroup ''D'' of [[diagonal matrices]] in ''G''.\n\n'''''Conjecture''''': for any ''g'' in ''G''/Γ such that ''Dg'' is [[relatively compact]] (in ''G''/Γ), then ''Dg'' is closed.\n\nThis in turn is a special case of a general conjecture of [[Grigory Margulis|Margulis]] on [[Lie group]]s.\n\n==Partial results==\nBorel showed in 1909 that the exceptional set of real pairs (α,β) violating the statement of the conjecture is of [[Lebesgue measure]] zero.<ref name=AB444>Adamczewski & Bugeaud (2010) p.444</ref> [[Manfred Einsiedler]], [[Anatole Katok]] and [[Elon Lindenstrauss]] have shown<ref>{{cite journal |author1=M. Einsiedler |author2=A. Katok |author3=E. Lindenstrauss | title=Invariant measures and the set of exceptions to Littlewood's conjecture | journal=[[Annals of Mathematics]] | volume=164 | issue=2 | pages=513–560 | date=2006-09-01 | doi=10.4007/annals.2006.164.513 | arxiv=math.DS/0612721 | mr = 2247967 |zbl=1109.22004}}</ref> that it must have [[Hausdorff dimension]] zero;<ref name=AB445>Adamczewski & Bugeaud (2010) p.445</ref> and in fact is a union of countably many [[Compact space|compact set]]s of [[Minkowski–Bouligand dimension|box-counting dimension]] zero. The result was proved by using a measure classification theorem for diagonalizable actions of higher-rank groups, and an ''isolation theorem'' proved by Lindenstrauss and Barak Weiss.\n\nThese results imply that non-trivial pairs satisfying the conjecture exist: indeed, given a real number α such that <math>\\inf_{n \\ge 1} n \\cdot || n \\alpha || > 0 </math>, it is possible to construct an explicit β such that (α,β) satisfies the conjecture.<ref name=AB446>Adamczewski & Bugeaud (2010) p.446</ref>\n\n==See also==\n* [[Littlewood polynomial]]\n\n==References==\n{{reflist}}\n*{{cite book | last1=Adamczewski | first1=Boris | last2=Bugeaud | first2=Yann | chapter=8. Transcendence and diophantine approximation | editor1-last=Berthé | editor1-first=Valérie |editor1-link = Valérie Berthé | editor2-last=Rigo | editor2-first=Michael | title=Combinatorics, automata, and number theory | location=Cambridge | publisher=[[Cambridge University Press]] | series=Encyclopedia of Mathematics and its Applications | volume=135 | pages=410–451 | year=2010 | isbn=978-0-521-51597-9 | zbl=1271.11073}}\n\n==Further reading==\n* {{cite journal | author=Akshay Venkatesh | title=The work of Einsiedler, Katok, and Lindenstrauss on the Littlewood conjecture | journal=[[Bulletin of the American Mathematical Society|Bull. Amer. Math. Soc. (N.S.)]] | volume=45 | issue=1 | date=2007-10-29 | pages=117–134 | url=http://www.cims.nyu.edu/~venkatesh/research/eklexp.pdf | doi=10.1090/S0273-0979-07-01194-9 | accessdate=2011-03-27 | mr=2358379 | zbl = 1194.11075 }}\n\n{{DEFAULTSORT:Littlewood Conjecture}}\n[[Category:Diophantine approximation]]\n[[Category:Conjectures]]"
    },
    {
      "title": "Low-discrepancy sequence",
      "url": "https://en.wikipedia.org/wiki/Low-discrepancy_sequence",
      "text": "In [[mathematics]], a '''low-discrepancy sequence''' is a [[sequence]] with the property that for all values of ''N'', its subsequence ''x''<sub>1</sub>, ..., ''x''<sub>''N''</sub> has a low [[discrepancy of a sequence|discrepancy]].\n\nRoughly speaking, the discrepancy of a sequence is low if the proportion of points in the sequence falling into an arbitrary set ''B'' is close to proportional to the [[Measure (mathematics)|measure]] of ''B'', as would happen on average (but not for particular samples) in the case of an [[equidistributed sequence]]. Specific definitions of discrepancy differ regarding the choice of ''B'' ([[hyperspheres]], hypercubes, etc.) and how the discrepancy for every B is computed (usually normalized) and combined (usually by taking the worst value).\n\nLow-discrepancy sequences are also called '''quasi-random''' or '''sub-random''' sequences, due to their common use as a replacement of uniformly distributed [[random sequence|random numbers]].\nThe \"quasi\" modifier is used to denote more clearly that the values of a low-discrepancy sequence are neither random nor [[pseudorandom]], but such sequences share some properties of random variables and in certain applications such as the [[quasi-Monte Carlo method]] their lower discrepancy is an important advantage.\n\n==Applications==\n\n[[Image:Subrandom Kurtosis.gif|thumb|370px|right|Error in estimated kurtosis as a function of number of datapoints. 'Additive subrandom' gives the maximum error when ''c''&nbsp;=&nbsp;({{radic|5}}&nbsp;&minus;&nbsp;1)/2. 'Random' gives the average error over six runs of random numbers, where the average is taken to reduce the magnitude of the wild fluctuations]]\n\nSubrandom numbers have an advantage over pure random numbers in that they cover the domain of interest quickly and evenly. They have an advantage over purely deterministic methods in that deterministic methods only give high accuracy when the number of datapoints is pre-set whereas in using subrandom sequences the accuracy typically improves continually as more datapoints are added, with full reuse of the existing points. On the other hand, subrandom sets can have a significantly lower discrepancy for a given number of points than purely random sequences.\n\nTwo useful applications are in finding the [[characteristic function (probability theory)|characteristic function]] of a [[probability density function]], and in finding the [[derivative]] function of a deterministic function with a small amount of noise. Subrandom numbers allow higher-order [[moment (mathematics)|moments]] to be calculated to high accuracy very quickly.\n\nApplications that don't involve sorting would be in finding the [[mean]], [[standard deviation]], [[skewness]] and [[kurtosis]] of a statistical distribution, and in finding the [[integral]] and global [[maxima and minima]] of difficult deterministic functions. Subrandom numbers can also be used for providing starting points for deterministic algorithms that only work locally, such as [[Newton–Raphson iteration]].\n\nSubrandom numbers can also be combined with search algorithms. A binary tree [[Quicksort]]-style algorithm ought to work exceptionally well because subrandom numbers flatten the tree far better than random numbers, and the flatter the tree the faster the sorting. With a search algorithm, subrandom numbers can be used to find the [[mode (statistics)|mode]], [[median]], [[confidence intervals]] and [[cumulative distribution function|cumulative distribution]] of a statistical distribution, and all [[local minima]] and all solutions of deterministic functions.\n\n=== Low-discrepancy sequences in numerical integration ===\n\nAt least three methods of [[numerical integration]] can be phrased as follows.\nGiven a set {''x''<sub>1</sub>, ..., ''x''<sub>''N''</sub>} in the interval <nowiki>[0,1]</nowiki>, approximate the integral of a function ''f'' as the average of the function evaluated at those points:\n\n:<math> \\int_0^1 f(u)\\,du \\approx \\frac{1}{N}\\,\\sum_{i=1}^N f(x_i). </math>\n\nIf the points are chosen as ''x''<sub>''i''</sub> = ''i''/''N'', this is the ''rectangle rule''.\nIf the points are chosen to be randomly (or [[pseudorandom]]ly) distributed, this is the ''[[Monte Carlo method]]''.\nIf the points are chosen as elements of a low-discrepancy sequence, this is the ''quasi-Monte Carlo method''.\nA remarkable result, the '''Koksma–Hlawka inequality''' (stated below), shows that the error of such a method can be bounded by the product of two terms, one of which depends only on ''f'', and the other one is the discrepancy of the set {''x''<sub>1</sub>, ..., ''x''<sub>''N''</sub>}.\n\nIt is convenient to construct the set {''x''<sub>1</sub>, ..., ''x''<sub>''N''</sub>} in such a way that if a set with ''N''+1 elements is constructed, the previous ''N'' elements need not be recomputed.\nThe rectangle rule uses points set which have low discrepancy, but in general the elements must be recomputed if ''N'' is increased.\nElements need not be recomputed in the random Monte Carlo method if ''N'' is increased,\nbut the point sets do not have minimal discrepancy.\nBy using low-discrepancy sequences we aim for low discrepancy and no need for recomputations, but actually low-discrepancy sequences can only be incrementally good on discrepancy if we allow no recomputation.\n\n==Definition of discrepancy==\n\nThe ''discrepancy'' of a set P = {''x''<sub>1</sub>, ..., ''x''<sub>''N''</sub>} is defined, using [[Harald Niederreiter|Niederreiter's]] notation, as\n:<math> D_N(P) = \\sup_{B\\in J}\n  \\left|  \\frac{A(B;P)}{N} - \\lambda_s(B)  \\right|</math>\n\nwhere\nλ<sub>''s''</sub> is the ''s''-dimensional [[Lebesgue measure]],\n''A''(''B'';''P'') is the number of points in ''P'' that fall into ''B'',\nand ''J'' is the set of ''s''-dimensional intervals or boxes of the form\n\n:<math> \\prod_{i=1}^s [a_i, b_i) = \\{ \\mathbf{x} \\in \\mathbf{R}^s : a_i \\le x_i < b_i \\} \\, </math>\n\nwhere <math> 0 \\le a_i < b_i \\le 1 </math>.\n\nThe ''star-discrepancy'' ''D''<sup>*</sup><sub>''N''</sub>(''P'') is defined similarly, except that the supremum is taken over the set ''J''<sup>*</sup> of rectangular boxes of the form\n\n:<math> \\prod_{i=1}^s [0, u_i) </math>\n\nwhere ''u''<sub>''i''</sub> is in the half-open interval <nowiki>[0, 1)</nowiki>.\n\nThe two are related by\n\n:<math>D^*_N \\le D_N \\le 2^s D^*_N . \\,</math>\n\nNote: With these definitions, discrepancy represents the worst-case or maximum point density deviation of a uniform set. However, also other error measures are meaningful, leading to other definitions and variation measures. For instance, L2 discrepancy or modified centered L2 discrepancy are also used intensively to compare the quality of uniform point sets. Both are much easier to calculate for large N and s.\n\n==The Koksma–Hlawka inequality==\n\nLet Ī<sup>''s''</sup> be the ''s''-dimensional unit cube,\nĪ<sup>''s''</sup> = [0, 1] &times; ... &times; [0, 1].\nLet ''f'' have [[bounded variation]] ''V''(''f'') on Ī<sup>''s''</sup> in the sense of [[G. H. Hardy|Hardy]] and Krause.\nThen for any ''x''<sub>1</sub>, ..., ''x''<sub>''N''</sub>\nin ''I''<sup>''s''</sup> =\n<nowiki>[</nowiki>0, 1<nowiki>)</nowiki> &times; ... &times;\n<nowiki>[</nowiki>0, 1<nowiki>)</nowiki>,\n: <math> \\left| \\frac{1}{N} \\sum_{i=1}^N f(x_i)\n      - \\int_{\\bar I^s} f(u)\\,du \\right|\n     \\le V(f)\\, D_N^* (x_1,\\ldots,x_N).\n</math>\n\nThe [[Jurjen Ferdinand Koksma|Koksma]]–[[Edmund Hlawka|Hlawka]] inequality is sharp in the following sense: For any point set {''x''<sub>1</sub>,...,''x''<sub>N</sub>} in ''I''<sup>s</sup> and any <math>\\varepsilon>0</math>, there is a function ''f'' with bounded variation and ''V''(''f'')&nbsp;=&nbsp;1 such that\n\n:<math>\n\\left| \\frac{1}{N} \\sum_{i=1}^N f(x_i)\n      - \\int_{\\bar I^s} f(u)\\,du \\right|>D_{N}^{*}(x_1,\\ldots,x_N)-\\varepsilon.\n</math>\n\nTherefore, the quality of a numerical integration rule depends only on the discrepancy D<sup>*</sup><sub>N</sub>(''x''<sub>1</sub>,...,''x''<sub>N</sub>).\n\n==The formula of Hlawka–Zaremba==\n\nLet <math>D=\\{1,2,\\ldots,d\\}</math>. For <math>\\emptyset\\neq u\\subseteq D</math> we\nwrite\n:<math>\ndx_u:=\\prod_{j\\in u} dx_j\n</math>\nand denote by <math>(x_u,1)</math> the point obtained from ''x'' by replacing the\ncoordinates not in ''u'' by <math>1</math>. Then\n\n:<math>\n\\frac{1}{N} \\sum_{i=1}^N f(x_i)\n      - \\int_{\\bar I^s} f(u)\\,du=\n\\sum_{\\emptyset\\neq u\\subseteq D}(-1)^{|u|}\n\\int_{[0,1]^{|u|}} \\operatorname{disc}(x_u,1)\\frac{\\partial^{|u|}}{\\partial x_u}f(x_u,1) \\, dx_u,\n</math>\n\nwhere <math>\\operatorname{disc}(z)= \\frac{1}{N}\\sum_{i=1}^N \\prod_{j=1}^d 1_{[0,z_j)}(x_{i,j}) - \\prod_{j=1}^d z_i</math> is the discrepancy function.\n\n==The <math>L^2</math> version of the Koksma–Hlawka inequality==\n\nApplying the [[Cauchy–Schwarz inequality]] for integrals and sums to the Hlawka–Zaremba identity, we obtain an <math>L^2</math> version of the Koksma–Hlawka inequality:\n\n: <math>\n\\left|\\frac{1}{N} \\sum_{i=1}^N f(x_i)\n      - \\int_{\\bar I^s} f(u)\\,du\\right|\\le\n\\|f\\|_d \\operatorname{disc}_d (\\{t_i\\}),\n</math>\n\nwhere\n\n:<math>\n\\operatorname{disc}_d(\\{t_i\\})=\\left(\\sum_{\\emptyset\\neq u\\subseteq D}\n\\int_{[0,1]^{|u|}} \\operatorname{disc}(x_u,1)^2 \\, dx_u\\right)^{1/2}\n</math>\n\nand\n\n:<math>\n\\|f\\|_d = \\left(\\sum_{u\\subseteq D}\n\\int_{[0,1]^{|u|}}\n\\left|\\frac{\\partial^{|u|}}{\\partial x_u} f(x_u,1)\\right|^2 dx_u\\right)^{1/2}.\n</math>\nL2 discrepancy has a high practical importance because fast explicit calculations are possible for a given point set. This way it is easy to create point set optimizers using L2 discrepancy as criteria.\n\n==The Erd&#337;s–Turán–Koksma inequality==\n\nIt is computationally hard to find the exact value of the discrepancy of large point sets. The [[Paul Erd&#337;s|Erd&#337;s]]–[[Turán]]–[[Jurjen Ferdinand Koksma|Koksma]] inequality provides an upper bound.\n\nLet ''x''<sub>1</sub>,...,''x''<sub>N</sub> be points in ''I''<sup>s</sup> and ''H'' be an arbitrary positive integer. Then\n\n:<math>\nD_{N}^{*}(x_1,\\ldots,x_N)\\leq\n\\left(\\frac{3}{2}\\right)^s\n\\left(\n\\frac{2}{H+1}+\n\\sum_{0<\\|h\\|_{\\infty}\\leq H}\\frac{1}{r(h)}\n\\left|\n\\frac{1}{N}\n\\sum_{n=1}^{N} e^{2\\pi i\\langle h,x_n\\rangle}\n\\right|\n\\right)\n</math>\n\nwhere\n\n:<math>\nr(h)=\\prod_{i=1}^s\\max\\{1,|h_i|\\}\\quad\\mbox{for}\\quad h=(h_1,\\ldots,h_s)\\in\\Z^s.\n</math>\n\n==The main conjectures==\n\n'''Conjecture 1.''' There is a constant ''c''<sub>s</sub> depending only on the dimension ''s'', such that\n\n:<math>D_{N}^{*}(x_1,\\ldots,x_N)\\geq c_s\\frac{(\\ln N)^{s-1}}{N}</math>\n\nfor any finite point set  {''x''<sub>1</sub>,...,''x''<sub>''N''</sub>}.\n\n'''Conjecture 2.'''  There is a constant ''c''<sup>'</sup><sub>s</sub> depending only on ''s'', such that\n\n:<math>D_{N}^{*}(x_1,\\ldots,x_N)\\geq c'_s\\frac{(\\ln N)^{s}}{N}</math>\n\nfor infinite number of ''N'' for any infinite sequence  ''x''<sub>1</sub>,''x''<sub>2</sub>,''x''<sub>3</sub>,....\n\nThese conjectures are equivalent. They have been proved for ''s'' ≤ 2 by [[W. M. Schmidt]]. In higher dimensions, the corresponding problem is still open. The best-known lower bounds are due to [[Michael Lacey]] and collaborators.\n\n==Lower bounds==\n\nLet ''s''&nbsp;=&nbsp;1. Then\n\n:<math>\nD_N^*(x_1,\\ldots,x_N)\\geq\\frac{1}{2N}\n</math>\n\nfor any finite point set {''x''<sub>1</sub>,&nbsp;...,&nbsp;''x''<sub>''N''</sub>}.\n\nLet ''s''&nbsp;=&nbsp;2. W. M. Schmidt proved that for any finite point set {''x''<sub>1</sub>,&nbsp;...,&nbsp;''x''<sub>''N''</sub>},\n\n:<math>\nD_N^*(x_1,\\ldots,x_N)\\geq C\\frac{\\log N}{N}\n</math>\n\nwhere\n\n:<math>\nC=\\max_{a\\geq3}\\frac{1}{16}\\frac{a-2}{a\\log a}=0.023335\\dots\n</math>\n\nFor arbitrary dimensions ''s''&nbsp;>&nbsp;1, K.F. Roth proved that\n\n:<math>\nD_N^*(x_1,\\ldots,x_N)\\geq\\frac{1}{2^{4s}}\\frac{1}{((s-1)\\log2)^\\frac{s-1}{2}}\\frac{\\log^{\\frac{s-1}{2}}N}{N}\n</math>\n\nfor any finite point set {''x''<sub>1</sub>,&nbsp;...,&nbsp;''x''<sub>''N''</sub>}.\nThis bound is the best known for ''s''&nbsp;>&nbsp;3.\n\n==Construction of low-discrepancy sequences==\n\nBecause any distribution of random numbers can be mapped onto a uniform distribution, and subrandom numbers are mapped in the same way, this article only concerns generation of subrandom numbers on a multidimensional uniform distribution.\n\nThere are constructions of sequences known such that\n:<math>\nD_N^{*}(x_1,\\ldots,x_N)\\leq C\\frac{(\\ln N)^{s}}{N}.\n</math>\nwhere ''C'' is a certain constant, depending on the sequence. After Conjecture 2, these sequences are believed to have the best possible order of convergence. Examples below are the [[van der Corput sequence]], the [[Halton sequence]]s, and the [[Sobol sequence]]s. One general limitation is that construction methods can usually only guarantee the order of convergence. Practically, low discrepancy can be only achieved if N is large enough, and for large given s this minimum N can be very large. This means running a Monte-Carlo analysis with e.g. s=20 variables and N=1000 points from a low-discrepancy sequence generator may offer only a very minor accuracy improvement.\n\n===Random numbers===\n\nSequences of subrandom numbers can be generated from random numbers by imposing a negative correlation on those random numbers. One way to do this is to start with a set of random numbers <math>r_i</math> on <math>[0,0.5)</math> and construct subrandom numbers <math>s_i</math> which are uniform on <math>[0,1)</math> using:\n\n<math>s_i = r_i</math> for <math>i</math> odd and <math>s_i = 0.5 + r_i</math> for <math>i</math> even.\n\nA second way to do it with the starting random numbers is to construct a random walk with offset 0.5 as in:\n\n: <math>s_i = s_{i-1} + 0.5+ r_i \\pmod 1. \\, </math>\n\nThat is, take the previous subrandom number, add 0.5 and the random number, and take the result [[modular arithmetic|modulo]]&nbsp;1.\n\nFor more than one dimension, [[Latin squares]] of the appropriate dimension can be used to provide offsets to ensure that the whole domain is covered evenly.\n\n[[Image:Subrandom 2D.gif|thumb|270px|right|Coverage of the unit square. Left for additive subrandom numbers with ''c''&nbsp;=&nbsp;0.5545497...,&nbsp;0.308517... Right for random numbers. From top to bottom. 10, 100, 1000, 10000 points.]]\n\n===Additive recurrence===\n\nFor any irrational <math>\\alpha</math>, the sequence\n\n: <math>s_n = \\{s_0 + n\\alpha\\}</math>\n\nhas discrepancy tending to 0. (Note the sequence can be defined recursively by <math>s_{n+1} = (s_n + \\alpha)\\bmod\\, 1</math>.) A good value of <math>\\alpha</math> gives lower discrepancy than a sequence of independent uniform random numbers.\n\nThe discrepancy can be bounded by the [[approximation exponent]] of <math>\\alpha</math>. If the approximation exponent is <math>\\mu</math>, then for any <math>\\varepsilon>0</math>, the following bound holds:<ref name=\"kn05\">Kuipers and Niederreiter, 2005, p. 123</ref>\n\n: <math> D_N((s_n)) = O_\\varepsilon (N^{-1/(\\mu-1)+\\varepsilon}).</math>\n\nBy the [[Thue–Siegel–Roth theorem]], the approximation exponent of any irrational algebraic number is 2, giving a bound of <math>N^{-1+\\varepsilon}</math> above.\n\nThe value of <math>c</math> with lowest discrepancy is <ref>http://mollwollfumble.blogspot.com/, Subrandom numbers</ref>\n\n: <math>c = \\frac{\\sqrt{5}-1}{2} \\approx 0.618034.</math>\n\nAnother value that is nearly as good is:\n\n: <math>c = \\sqrt{2}-1 \\approx 0.414214. \\, </math>\n\nIn more than one dimension, separate subrandom numbers are needed for each dimension.  In higher dimensions, one set of values that can be used is the square roots of [[primes]] from two up, all taken modulo 1:\n\n: <math>c = \\sqrt{2}, \\sqrt{3}, \\sqrt{5}, \\sqrt{7}, \\sqrt{11}, \\ldots \\, </math>\n\nThe recurrence relation above is similar to the recurrence relation used by a [[Linear congruential generator]], a poor-quality pseudorandom number generator:<ref>Donald E. Knuth ''The Art of Computer Programming'' Vol. 2, Ch. 3</ref>\n\n: <math>r_i = \\bmod(a r_{i-1} + c, m)</math>\n\nFor the low discrepancy additive recurrence above, a and m are chosen to be 1.  Note, however, that this will not generate independent random numbers, so should not be used for purposes requiring independence.  The [[list of pseudorandom number generators]] lists methods for generating independent pseudorandom numbers.\nNote: In few dimensions, recursive recurrence leads to uniform sets of good quality, but for larger s (like s>8) other point set generators can offer much lower discrepancies.\n\n===van der Corput sequence===\n{{Main article|van der Corput sequence}}\n\nLet\n\n:<math>\nn=\\sum_{k=0}^{L-1}d_k(n)b^k\n</math>\n\nbe the ''b''-ary representation of the positive integer ''n'' ≥ 1, i.e. 0 ≤ ''d''<sub>''k''</sub>(''n'') < ''b''. Set\n\n:<math>\ng_b(n)=\\sum_{k=0}^{L-1}d_k(n)b^{-k-1}.\n</math>\n\nThen there is a constant ''C'' depending only on ''b'' such that (''g''<sub>''b''</sub>(''n''))<sub>''n'' ≥ 1</sub>satisfies\n\n:<math>\nD^*_N(g_b(1),\\dots,g_b(N))\\leq C\\frac{\\log N}{N},\n</math>\n\nwhere ''D''<sup>*</sup><sub>''N''</sub> is the  \n'''[[Low-discrepancy sequence#Definition of discrepancy|star discrepancy]]'''.\n\n===Halton sequence===\n[[Image:Halton sequence 2D.svg|thumb|right|First 256 points of the (2,3) Halton sequence]]\n{{Main article|Halton sequence}}\n\nThe Halton sequence is a natural generalization of the van der Corput sequence to higher dimensions. Let ''s'' be an arbitrary dimension and ''b''<sub>1</sub>, ..., ''b''<sub>''s''</sub> be arbitrary [[coprime]] integers greater than 1. Define\n\n:<math>\nx(n)=(g_{b_1}(n),\\dots,g_{b_s}(n)).\n</math>\n\nThen there is a constant ''C'' depending only on ''b''<sub>1</sub>, ..., ''b''<sub>''s''</sub>, such that sequence {''x''(''n'')}<sub>''n''≥1</sub> is a ''s''-dimensional sequence with\n\n:<math>\nD^*_N(x(1),\\dots,x(N))\\leq C'\\frac{(\\log N)^s}{N}.\n</math>\n\n===Hammersley set===\n\n[[File:Hammersley set 2D.svg|thumb|right|2D Hammersley set of size 256]]\nLet ''b''<sub>1</sub>,...,''b''<sub>''s''−1</sub> be [[coprime]] positive integers greater than 1. For given ''s'' and ''N'', the ''s''-dimensional [[John Hammersley|Hammersley]] set of size ''N'' is defined by<ref name=\"HammersleyHandscomb1964\">{{cite book|title=Monte Carlo Methods|last1=Hammersley|first1=J. M.|last2=Handscomb|first2=D. C.|year=1964|doi=10.1007/978-94-009-5819-7}}</ref>\n\n:<math>\nx(n)=(g_{b_1}(n),\\dots,g_{b_{s-1}}(n),\\frac{n}{N})\n</math>\n\nfor ''n'' = 1, ..., ''N''. Then\n\n:<math>\nD^*_N(x(1),\\dots,x(N))\\leq C\\frac{(\\log N)^{s-1}}{N}\n</math>\n\nwhere ''C'' is a constant depending only on ''b''<sub>1</sub>, ..., ''b''<sub>''s''&minus;1</sub>.\nNote: The formulas show that the Hammersley set is actually the Halton sequence, but we get one more dimension for free by adding a linear sweep. This is only possible if ''N'' is known upfront. A linear set is also the set with lowest possible one-dimensional discrepancy in general. Unfortunately, for higher dimensions, no such \"discrepancy record sets\" are known. For ''s''&nbsp;=&nbsp;2, most low-discrepancy point set generators deliver at least near-optimum discrepancies.\n\n===Sobol sequence===\n{{Main article|Sobol sequence}}\n\nThe Antonov–Saleev variant of the Sobol sequence generates numbers between zero and one directly as binary fractions of length <math>w</math>, from a set of <math>w</math> special binary fractions, <math>V_i, i = 1, 2, \\dots, w</math> called direction numbers. The bits of the [[Gray code]] of <math>i</math>, <math>G(i)</math>, are used to select direction numbers. To get the Sobol sequence value <math>s_i</math> take the [[exclusive or]] of the binary value of the Gray code of <math>i</math> with the appropriate direction number. The number of dimensions required affects the choice of <math>V_i</math>.\n\n=== Poisson disk sampling ===\n{{main article|Supersampling#Poisson_disc}}\n[[Supersampling#Poisson disc|Poisson disk sampling]] is popular in video games to rapidly place objects in a way that appears random-looking\nbut guarantees that every two points are separated by at least the specified minimum distance.<ref>\nHerman Tulleken.\n[http://devmag.org.za/2009/05/03/poisson-disk-sampling/ \"Poisson Disk Sampling\"].\n''Dev.Mag'' Issue 21, March 2008.\n</ref> This does not guarantee low discrepancy (as e. g. Sobol), but at least a significantly lower discrepancy than pure random sampling.\n\n==Graphical examples==\nThe points plotted below are the first 100, 1000, and 10000 elements in a sequence of the Sobol' type.\nFor comparison, 10000 elements of a sequence of pseudorandom points are also shown.\nThe low-discrepancy sequence was generated by [[ACM Transactions on Mathematical Software|TOMS]] algorithm 659.<ref>P. Bratley and B.L. Fox in ''ACM Transactions on Mathematical Software'', vol. 14, no. 1, pp 88—100</ref>\nAn implementation of the algorithm in [[Fortran]] is available from [[Netlib]].\n\n[[Image:Low discrepancy 100.png|frame|150px|left|The first 100 points in a low-discrepancy sequence of the [[Sobol sequence|Sobol]] type.]]\n[[Image:Low discrepancy 1000.png|frame|150px|right|The first 1000 points in the same sequence. These 1000 comprise the first 100, with 900 more points.]]\n[[Image:Low discrepancy 10000.png|frame|150px|left|The first 10000 points in the same sequence. These 10000 comprise the first 1000, with 9000 more points.]]\n[[Image:Random 10000.png|frame|150px|right|For comparison, here are the first 10000 points in a sequence of uniformly distributed pseudorandom numbers. Regions of higher and lower density are evident.]]\n\n{{Clear}}\n\n==See also==\n* [[Discrepancy theory]]\n* [[Quasi-Monte Carlo method]]\n* [[Markov chain Monte Carlo]]\n* [[Sparse grid]]\n\n==References==\n{{reflist}}\n* Josef Dick and Friedrich Pillichshammer, ''Digital Nets and Sequences. Discrepancy Theory and Quasi-Monte Carlo Integration'', Cambridge University Press, Cambridge, 2010, {{isbn|978-0-521-19159-3}}\n* {{Citation | last=Kuipers | first=L. | last2= Niederreiter |author2-link= Harald Niederreiter | first2=H. | title = Uniform distribution of sequences | publisher=[[Dover Publications]] | year=2005 | isbn=0-486-45019-8 }}\n* [[Harald Niederreiter]]. ''Random Number Generation and Quasi-Monte Carlo Methods.'' Society for Industrial and Applied Mathematics, 1992. {{isbn|0-89871-295-5}}\n* Michael Drmota and Robert F. Tichy, ''Sequences, discrepancies and applications'', Lecture Notes in Math., 1651, Springer, Berlin, 1997, {{isbn|3-540-62606-9}}\n* William H. Press, Brian P. Flannery, Saul A. Teukolsky, William T. Vetterling. ''[[Numerical Recipes|Numerical Recipes in C]]''. Cambridge, UK: Cambridge University Press, second edition 1992. {{isbn|0-521-43108-5}} ''(see Section 7.7 for a less technical discussion of low-discrepancy sequences)''\n\n==External links==\n* [http://calgo.acm.org/ Collected Algorithms of the ACM] ''(See algorithms 647, 659, and 738.)''\n* [https://www.gnu.org/software/gsl/doc/html/qrng.html Quasi-Random Sequences] from the [[GNU Scientific Library]]\n* [http://finmathblog.blogspot.com/2013/09/quasi-random-sampling-subject-to-linear.html Quasi-random sampling subject to constraints] at FinancialMathematics.Com\n* [http://kirillsprograms.com/top_3Sobol.php C++ generator of Sobol sequence]\n\n{{DEFAULTSORT:Low-Discrepancy Sequence}}\n[[Category:Numerical analysis]]\n[[Category:Quasirandomness]]\n[[Category:Random number generation]]\n[[Category:Diophantine approximation]]\n[[Category:Sequences and series]]"
    },
    {
      "title": "Mahler's 3/2 problem",
      "url": "https://en.wikipedia.org/wiki/Mahler%27s_3%2F2_problem",
      "text": "In mathematics, '''Mahler's 3/2 problem''' concerns the existence of \"{{math|''Z''}}-numbers\".\n\nA '''{{math|''Z''}}-number''' is a real number {{math|''x''}} such that the  [[fractional part]]s\n\n:<math>\\left\\lbrace x \\left(\\frac 3 2\\right)^ n \\right\\rbrace </math>\n\nare less than {{math|1/2}} for all positive integers {{math|''n''}}.  [[Kurt Mahler]] conjectured in 1968 that there are no {{math|''Z''}}-numbers.\n\nMore generally, for a real number {{math|''α''}}, define {{math|Ω(''α'')}} as\n\n:<math>\\Omega(\\alpha) = \\inf_\\theta\\left({ \\limsup_{n \\rightarrow \\infty} \\left\\lbrace{\\theta\\alpha^n}\\right\\rbrace - \\liminf_{n \\rightarrow \\infty} \\left\\lbrace{\\theta\\alpha^n}\\right\\rbrace }\\right). </math>\n\nMahler's conjecture would thus imply that {{math|Ω(3/2)}} exceeds {{math|1/2}}.  Flatto, [[Jeffrey C. Lagarias|Lagarias]], and Pollington showed<ref>{{cite journal | first1=Leopold | last1=Flatto | first2=Jeffrey C. | last2=Lagarias | author2-link=Jeffrey C. Lagarias | first3=Andrew D. | last3=Pollington | title=On the range of fractional parts of&nbsp;''&zeta;''&nbsp;{&nbsp;(''p''/''q'')<sup>''n''</sup>&nbsp;} | journal=[[Acta Arithmetica]] | volume=LXX | number=2 | year=1995 | pages=125–147 | zbl=0821.11038 | issn=0065-1036 }}</ref> that\n\n:<math>\\Omega\\left(\\frac p q\\right) > \\frac 1 p </math>\n\nfor rational&nbsp;{{math|''p''/''q''}}.\n\n==References==\n{{reflist}}\n* {{cite book | last1=Everest | first1=Graham | last2=van der Poorten | first2=Alf | author2-link=Alf van der Poorten | last3=Shparlinski | first3=Igor | last4=Ward | first4=Thomas | title=Recurrence sequences | series=Mathematical Surveys and Monographs | volume=104 | location=[[Providence, RI]] | publisher=[[American Mathematical Society]] | year=2003 | isbn=0-8218-3387-1 | zbl=1033.11006 }}\n\n{{DEFAULTSORT:Mahler's 3 2 problem}}\n[[Category:Analytic number theory]]\n[[Category:Conjectures]]\n[[Category:Diophantine approximation]]"
    },
    {
      "title": "Restricted partial quotients",
      "url": "https://en.wikipedia.org/wiki/Restricted_partial_quotients",
      "text": "In [[mathematics]], and more particularly in the analytic theory of [[continued fraction|regular continued fractions]], an infinite regular continued fraction ''x'' is said to be ''restricted'', or composed of '''restricted partial quotients''', if the sequence of denominators of its partial quotients is bounded; that is\n:<math>x = [a_0;a_1,a_2,\\dots] = a_0 + \\cfrac{1}{a_1 + \\cfrac{1}{a_2 + \\cfrac{1}{a_3 + \\cfrac{1}{a_4 + \\ddots}}}} = a_0 + \\underset{i=1}{\\overset{\\infty}{K}} \\frac{1}{a_i},\\,</math>\n\nand there is some positive integer ''M'' such that all the (integral) partial denominators ''a<sub>i</sub>'' are less than or equal to ''M''.<ref>{{cite book|last = Rockett|first = Andrew M.|author2=Szüsz, Peter|title = Continued Fractions|publisher = World Scientific|date = 1992|isbn = 981-02-1052-3}}</ref><ref>For a fuller explanation of the K notation used here, please see [[Generalized continued fraction#Notation|this article]].</ref>\n\n==Periodic continued fractions==\n\nA regular [[periodic continued fraction]] consists of a finite initial block of partial denominators followed by a repeating block; if\n\n:<math>\n\\zeta = [a_0;a_1,a_2,\\dots,a_k,\\overline{a_{k+1},a_{k+2},\\dots,a_{k+m}}],\\,\n</math>\n\nthen ζ is a [[quadratic irrational]] number, and its representation as a regular continued fraction is periodic. Clearly any regular periodic continued fraction consists of restricted partial quotients, since none of the partial denominators can be greater than the largest of ''a''<sub>0</sub> through ''a''<sub>''k''+''m''</sub>. Historically, mathematicians studied periodic continued fractions before considering the more general concept of restricted partial quotients.\n\n==Restricted CFs and the Cantor set==\nThe [[Cantor set]] is a set ''C'' of [[measure zero]] from which a complete [[interval (mathematics)|interval]] of real numbers can be constructed by simple addition &ndash; that is, any real number from the interval can be expressed as the sum of exactly two elements of the set ''C''. The usual proof of the existence of the Cantor set is based on the idea of punching a \"hole\" in the middle of an interval, then punching holes in the remaining sub-intervals, and repeating this process ''ad infinitum''.\n\nThe process of adding one more partial quotient to a finite continued fraction is in many ways analogous to this process of \"punching a hole\" in an interval of real numbers. The size of the \"hole\" is inversely proportional to the next partial denominator chosen &ndash; if the next partial denominator is 1, the gap between successive [[convergent (continued fraction)|convergent]]s is maximized.\nTo make the following theorems precise we will consider CF(''M''), the set of restricted continued fractions whose values lie in the open interval (0,&nbsp;1) and whose partial denominators are bounded by a positive integer ''M'' &ndash; that is,\n\n:<math>\n\\mathrm{CF}(M) = \\{[0;a_1,a_2,a_3,\\dots]: 1 \\leq a_i \\leq M \\}.\\,\n</math>\n\nBy making an argument parallel to the one used to construct the Cantor set two interesting results can be obtained.\n* If ''M'' ≥ 4, then any real number in an interval can be constructed as the sum of two elements from CF(''M''), where the  interval is given by\n\n::<math>\n(2\\times[0;\\overline{M,1}], 2\\times[0;\\overline{1,M}]) =\n\\left(\\frac{1}{M} \\left[\\sqrt{M^2 + 4M} - M \\right], \\sqrt{M^2 + 4M} - M \\right).\n</math>\n\n*A simple argument shows that <math>{\\scriptstyle[0;\\overline{1,M}]-[0;\\overline{M,1}]\\ge\\frac{1}{2}}</math> holds when ''M''&nbsp;≥&nbsp;4, and this in turn implies that if ''M''&nbsp;≥&nbsp;4, every real number can be represented in the form ''n''&nbsp;+&nbsp;CF<sub>1</sub>&nbsp;+&nbsp;CF<sub>2</sub>, where ''n'' is an integer, and CF<sub>1</sub> and CF<sub>2</sub> are elements of CF(''M'').<ref>{{cite journal|authorlink=Marshall Hall (mathematician)|last = Hall|first = Marshall|title = On the Sum and Product of Continued Fractions|journal = The Annals of Mathematics|volume = 48|issue = 4|date = October 1947|pages = 966–993|doi = 10.2307/1969389|jstor=1969389}}</ref>\n\n==Zaremba's conjecture==\n[[:de:Stanisław Krystyn Zaremba|Zaremba]] has conjectured the existence of an absolute constant ''A'', such that the rationals with partial quotients restricted by ''A'' contain at least one for every (positive integer) denominator. The choice ''A'' = 5 is compatible with the numerical evidence.<ref>{{cite book|author1=Cristian S. Calude|author2=Elena Calude|author3=M. J. Dinneen|title=Developments in Language Theory: 8th International Conference, DLT 2004, Auckland, New Zealand, December 13-17, Proceedings|url=https://books.google.com/books?id=z_-SzxRaZ4sC&pg=PA180|date=29 November 2004|publisher=Springer|isbn=978-3-540-24014-3|page=180}}</ref> Further conjectures reduce that value, in the case of all sufficiently large denominators.<ref>{{cite book|author1=Hee Oh|author2=Emmanuel Breuillard|title=Thin Groups and Superstrong Approximation|url=https://books.google.com/books?id=XsKfAgAAQBAJ&pg=PA15|date=17 February 2014|publisher=Cambridge University Press|isbn=978-1-107-03685-7|page=15}}</ref> [[Jean Bourgain]] and Alex Kontorovich have shown that ''A'' can be chosen so that the conclusion holds for a set of denominators of density 1.<ref>{{cite journal|last1=Bourgain|first1=Jean|author1link=Jean Bourgain|last2=Kontorovich|first2=Alex|title=On Zaremba's conjecture|journal=[[Annals of Mathematics]]|volume=180|year=2014|issue=1|pages=137-196|doi=10.4007/annals.2014.180.1.3|mr=3194813|arxiv=1107.3776}}</ref>\n\n==See also==\n*[[Markov spectrum]]\n\n==References==\n{{reflist}}\n\n[[Category:Continued fractions]]\n[[Category:Diophantine approximation]]"
    },
    {
      "title": "Schneider–Lang theorem",
      "url": "https://en.wikipedia.org/wiki/Schneider%E2%80%93Lang_theorem",
      "text": "In mathematics, the '''Schneider–Lang theorem''' is a refinement by {{harvtxt| Lang|1966}} of a theorem of {{harvtxt|Schneider|1949}} about the [[transcendental number|transcendence]] of values of [[meromorphic function]]s.  The theorem implies both the [[Hermite–Lindemann theorem|Hermite–Lindemann]] and [[Gelfond–Schneider theorem]]s, and implies the transcendence of some values of [[elliptic function]]s and [[elliptic modular function]]s.\n\n== Statement ==\n\nThe theorem deals with a [[number field]] ''K'' and [[meromorphic]] functions ''f''<sub>1</sub>,&nbsp;...,&nbsp;''f''<sub>''N''</sub>, at least two of which are algebraically independent of [[entire function|order]]s ρ<sub>1</sub> and ρ<sub>2</sub>, and such that if we differentiate any of these functions then the result is a polynomial in ''f''<sub>1</sub>,&nbsp;...,&nbsp;''f''<sub>''N''</sub> with coefficients in ''K''.  Under these hypotheses the theorem states that if there are ''m'' distinct [[complex number]]s ''ω''<sub>1</sub>,&nbsp;...,&nbsp;''ω''<sub>''m''</sub> such that ''f''<sub>''i''</sub>&nbsp;(ω<sub>''j''&nbsp;</sub>) is in ''K'' for all combinations of ''i'' and ''j'', then ''m'' is bounded by\n\n: <math>m\\leq (\\rho_1+\\rho_2) [K:\\mathbb{Q}]. \\, </math>\n\n== Examples ==\n\n* If the two functions are ''f''<sub>1</sub>&nbsp;=&nbsp;''z'' and ''f''<sub>2</sub>&nbsp;=&nbsp;''e''<sup>''z''</sup> then the theorem implies the [[Hermite–Lindemann theorem]] that ''e''<sup>α</sup> is transcendental for any nonzero algebraic α, otherwise α, 2α, 3α,... would be an infinite number of values at which both ''f''<sub>1</sub> and ''f''<sub>2</sub> are algebraic.\n* Similarly taking the two function to be ''f''<sub>1</sub>&nbsp;=&nbsp;''e''<sup>''z''</sup> and ''f''<sub>2</sub>&nbsp;=&nbsp;''e''<sup>β''z''</sup> for ''β'' irrational algebraic implies the [[Gelfond–Schneider theorem]] that ''α''<sup>''β''</sup> cannot be algebraic if α is algebraic and  not 0 or 1. Otherwise log&nbsp;''α'', 2&nbsp;log&nbsp;''α'', 3&nbsp;log&nbsp;''α'' would be an infinite number of values at which both ''f''<sub>1</sub> and ''f''<sub>2</sub> are algebraic.\n* Taking the three functions to be  ''z'', ℘(αz), ℘'(αz) shows that if ''g''<sub>2</sub> and ''g''<sub>3</sub> are algebraic then the [[Weierstrass P function]] ℘(α), which satisfies the differential equation\n: <math> [\\wp'(z)]^2 = 4[\\wp(z)]^3-g_2\\wp(z)-g_3, \\, </math>\n: is transcendental for any algebraic&nbsp;α.\n* Taking the functions to be ''z'' and ''e''<sup>''f''(''z'')</sup> for a polynomial ''f'' of degree ρ shows that the number of points where the functions are all algebraic can grow linearly with the order ρ = deg(''f'').\n\n== Proof ==\n\nTo prove the result Lang took two algebraically independent functions from ''f''<sub>1</sub>,&nbsp;...,&nbsp;''f''<sub>''N''</sub>, say ''f'' and ''g'', and then created an auxiliary function which was simply a polynomial ''F'' in ''f'' and ''g''.  This auxiliary function could not be explicitly stated since ''f'' and ''g'' are not explicitly known.  But using [[Siegel's lemma]] Lang showed how to make ''F'' in such a way that it vanished to a high order at the ''m'' complex numbers\nω<sub>1</sub>,...,ω<sub>m</sub>.  Because of this high order vanishing it can be shown that a high-order derivative of ''F'' takes a value of small size one of the ω<sub>''i''</sub>s, \"size\" here referring to an algebraic property of a number.  Using the [[maximum modulus principle]] Lang also found a separate way to estimate the absolute values of derivatives of ''F'', and using standard results comparing the size of a number and its absolute value he showed that these estimates were contradicted unless the claimed bound on ''m'' holds.\n\n== Bombieri's theorem ==\n{{harvtxt|Bombieri|Lang|1970}} and {{harvtxt|Bombieri|1970}} generalized the result to functions of several variables. Bombieri showed that if ''K'' is an algebraic number field and ''f''<sub>1</sub>,&nbsp;...,&nbsp;''f''<sub>''N''</sub> are meromorphic functions of ''d'' complex variables of order at most ρ generating a field ''K''( ''f''<sub>1</sub>,&nbsp;...,&nbsp;''f''<sub>''N''</sub>) of transcendence degree at least ''d''&nbsp;+&nbsp;1 that is closed under all partial derivatives, then the set of points where all the functions ''f''<sub>''n''</sub> have values in ''K'' is contained in an algebraic hypersurface in '''C'''<sup>''d''</sup> of degree at most ''d''(''d''&nbsp;+&nbsp;1)ρ[''K'':'''Q''']&nbsp;+&nbsp;''d''\n{{harvtxt|Waldschmidt|1979|loc=theorem 5.1.1}} gave a simpler proof of Bombieri's theorem, with a slightly stronger bound of ''d''(ρ<sub>1</sub>+...+ρ<sub>''d''+1</sub>)[''K'':'''Q'''] for the degree, where the ρ<sub>''j''</sub> are the orders of ''d''+1 algebraically independent functions.\nThe special case ''d''&nbsp;=&nbsp;1 gives the Schneider&ndash;Lang theorem, with a bound of (ρ<sub>1</sub>+ρ<sub>2</sub>)[''K'':'''Q''']  for the number of points.\n\nExample. If ''p'' is a polynomial with integer coefficients then the functions ''z''<sub>1</sub>,...,''z''<sub>''n''</sub>,e<sup>''p''(''z''<sub>1</sub>,...,''z''<sub>''n''</sub>)</sup> are all algebraic at a dense set of points of the hypersurface ''p''=0.\n\n== References ==\n\n* {{Citation | last1=Bombieri | first1=Enrico | author1-link=Enrico Bombieri | title=Algebraic values of meromorphic maps | doi=10.1007/BF01418775 | mr=0306201 | year=1970 | journal=[[Inventiones Mathematicae]] | issn=0020-9910 | volume=10 | pages=267–287 | issue=4}}, {{Citation | last1=Bombieri | first1=Enrico | author1-link=Enrico Bombieri | title=Addendum to my paper: \"Algebraic values of meromorphic maps\" (Invent. Math.  10 (1970), 267&ndash;287) | doi= 10.1007/BF01404610 | mr=0322203 | year=1970 | journal=[[Inventiones Mathematicae]] | issn=0020-9910 | volume=11 | pages=163–166 | issue=2}}\n* {{Citation | last1=Bombieri | first1=Enrico | author1-link=Enrico Bombieri | last2=Lang | first2=Serge | author2-link=Serge Lang | title=Analytic subgroups of group varieties | doi=10.1007/BF01389801 | mr=0296028 | year=1970 | journal=[[Inventiones Mathematicae]] | issn=0020-9910 | volume=11 | pages=1–14}}\n* S. Lang, \"''Introduction to Transcendental Numbers'',\" Addison&ndash;Wesley Publishing Company, (1966)\n* {{Citation | last1=Lelong | first1=Pierre | title=Séminaire Bourbaki, 23ème année (1970/1971) | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Lecture Notes in Math. | doi=10.1007/BFb0058695 | mr=0414500 | year=1971 | volume=244 | chapter=Valeurs algébriques d'une application méromorphe               (d'après E. Bombieri) Exp. No. 384  | pages=29–45 | isbn=978-3-540-05720-8}}\n* {{Citation | last1=Schneider | first1=Theodor | title=Ein Satz über ganzwertige Funktionen als Prinzip für Transzendenzbeweise | doi=10.1007/BF01329621 | mr=0031498 | year=1949 | journal=[[Mathematische Annalen]] | issn=0025-5831 | volume=121 | pages=131–140}}\n* {{Citation | last1=Waldschmidt | first1=Michel | title=Nombres transcendants et groupes algébriques | publisher=[[Société Mathématique de France]] | location=Paris | series=Astérisque | year=1979 | volume=69}}\n\n{{DEFAULTSORT:Schneider-Lang theorem}}\n[[Category:Diophantine approximation]]\n[[Category:Transcendental numbers]]"
    },
    {
      "title": "Siegel's lemma",
      "url": "https://en.wikipedia.org/wiki/Siegel%27s_lemma",
      "text": "In [[transcendental number theory]] and [[Diophantine approximation]], '''Siegel's lemma''' refers to bounds on the solutions of linear equations obtained by the construction of [[auxiliary function]]s. The existence of these polynomials was proven by [[Axel Thue]];<ref>\n{{cite journal|last = Thue|first = Axel|authorlink = Axel Thue|title = Über Annäherungswerte algebraischer Zahlen|journal = [[Crelle's Journal|J. Reine Angew. Math.]]|volume=135|year = 1909|pages = 284–305|ref = harv}}</ref> Thue's proof used [[Dirichlet's box principle]]. [[Carl Ludwig Siegel]] published his lemma in 1929.<ref>{{cite journal|last = Siegel|first = Carl Ludwig|authorlink = Carl Ludwig Siegel|title = Über einige Anwendungen diophantischer Approximationen|journal = Abh. Preuss. Akad. Wiss. Phys. Math. Kl.|year = 1929|pages = 41–69|ref = harv}}, reprinted in Gesammelte Abhandlungen, volume 1; the lemma is stated on page 213</ref>  It is a pure [[existence theorem]] for a [[system of linear equations]].\n\nSiegel's lemma has been refined in recent years to produce sharper bounds on the estimates given by the lemma.<ref>\n{{cite journal|last = Bombieri|first = E.|authorlink = Enrico Bombieri|author2=Mueller, J. |title = On effective measures of irrationality for <math>{\\scriptscriptstyle\\sqrt[r]{a/b}}</math> and related numbers|journal = Journal für die reine und angewandte Mathematik|volume = 342|year = 1983|pages = 173–196}}</ref>\n\n==Statement==\nSuppose we are given a system of ''M'' linear equations in ''N'' unknowns such that ''N'' > ''M'', say\n\n:<math>a_{11} X_1 + \\cdots+ a_{1N} X_N = 0</math>\n\n:<math>\\cdots</math>\n\n:<math>a_{M1} X_1 +\\cdots+ a_{MN} X_N = 0</math>\n\nwhere the coefficients are rational integers, not all 0, and bounded by ''B''. The system then has a solution\n\n:<math>(X_1, X_2, \\dots, X_N)</math>\n\nwith the ''X''s all rational integers, not all 0, and bounded by\n\n:<math>(NB)^{M/(N-M)}.</math><ref>{{harv|Hindry|Silverman|2000}} Lemma D.4.1, page 316.</ref>\n\n{{harvtxt|Bombieri|Vaaler|1983}} gave the following sharper bound for the ''X'''s:\n:<math>\\max|X_j|\\le \\left(D^{-1}\\sqrt{\\det(AA^T)}\\right)^{1/(N-M)}</math>\nwhere ''D'' is the greatest common divisor of the ''M'' by ''M'' minors of the matrix ''A'', and ''A''<sup>''T''</sup> is its transpose.\nTheir proof involved replacing the [[Dirichlet box principle]] by techniques from the [[geometry of numbers]].\n\n==See also==\n*[[Diophantine approximation]]\n\n==References==\n{{reflist|30em}}\n\n*{{Cite journal|last = Bombieri|first = E.|last2= Vaaler|first2= J.|title = On Siegel's lemma|journal = Inventiones Mathematicae|volume = 73|issue = 1|year=1983|pages = 11–32|url = http://www.springerlink.com/content/k55042224131lp42|doi = 10.1007/BF01393823|ref = harv|postscript = <!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->{{inconsistent citations}}}}\n*{{Cite book | last1=Hindry | first1=Marc | author1-link=Marc Hindry | last2=Silverman | first2=Joseph H. | author2-link=Joseph H. Silverman | title=Diophantine geometry | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Graduate Texts in Mathematics | isbn=978-0-387-98981-5 | mr=1745599 | year=2000 | volume=201 | ref=harv | postscript=<!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->{{inconsistent citations}}}}\n* [[Wolfgang M. Schmidt]]. ''Diophantine approximation''. Lecture Notes in Mathematics 785. Springer. (1980 [1996 with minor corrections]) (Pages 125-128 and 283-285)\n* Wolfgang M. Schmidt. \"Chapter I: Siegel's Lemma and Heights\" (pages 1–33). ''Diophantine approximations and Diophantine equations'', Lecture Notes in Mathematics, Springer Verlag 2000.\n\n[[Category:Lemmas]]\n[[Category:Diophantine approximation]]\n[[Category:Diophantine geometry]]"
    },
    {
      "title": "Subspace theorem",
      "url": "https://en.wikipedia.org/wiki/Subspace_theorem",
      "text": "In mathematics, the '''subspace theorem''' says that points of small [[Height function|height]] in [[projective space]] lie in a finite number of [[hyperplane]]s. It is a result obtained by {{harvs|txt|authorlink=Wolfgang M. Schmidt|first=Wolfgang M. |last=Schmidt|year= 1972}}.\n\n==Statement==\nThe subspace theorem states that if ''L''<sub>1</sub>,...,''L''<sub>''n''</sub> are [[linear independence|linearly independent]] [[linear]] [[algebraic form|forms]] in ''n'' variables with [[algebraic number|algebraic]] coefficients and if ε>0 is any given real number, then\nthe non-zero integer points ''x'' with\n:<math>|L_1(x)\\cdots L_n(x)|<|x|^{-\\epsilon}</math>\nlie in a finite number of [[linear subspace|proper subspaces]] of '''Q'''<sup>''n''</sup>.\n\nA quantitative form of the theorem, in which the number of subspaces containing all solutions, was also obtained by Schmidt, and the theorem was generalised by {{harvtxt|Schlickewei|1977}} to allow more general [[absolute value (algebra)|absolute values]] on [[number field]]s.\n\n==Applications==\nThe theorem may be used to obtain results on [[Diophantine equation]]s such as [[Siegel's theorem on integral points]] and solution of the [[S-unit equation]].<ref>Bombieri & Gubler (2006) pp.176–230</ref>}}\n\n===A corollary on Diophantine approximation===\nThe following corollary to the subspace theorem is often itself referred to as the ''subspace theorem''.\nIf ''a''<sub>1</sub>,...,''a''<sub>''n''</sub> are algebraic such that 1,''a''<sub>1</sub>,...,''a''<sub>''n''</sub> are linearly independent over '''Q''' and ε>0 is any given real number, then there are only finitely many rational ''n''-tuples (''x''<sub>1</sub>/y,...,''x''<sub>''n''</sub>/y) with\n:<math>|a_i-x_i/y|<y^{-(1+1/n+\\epsilon)},\\quad i=1,\\ldots,n.</math>\n\nThe specialization ''n'' = 1 gives the [[Thue–Siegel–Roth theorem]]. One may also note that the exponent 1+1/''n''+ε is best possible by [[Dirichlet's theorem on diophantine approximation]].\n\n==References==\n{{Reflist}}\n* {{cite book | first1=Enrico | last1=Bombieri | authorlink1=Enrico Bombieri | first2=Walter | last2=Gubler | title=Heights in Diophantine Geometry | series=New Mathematical Monographs | volume=4 | publisher=[[Cambridge University Press]] | location=Cambridge | year=2006 | isbn=978-0-521-71229-3 | zbl=1130.11034 | doi=10.2277/0521846153 | mr=2216774}}\n* {{cite journal | last=Schlickewei | first=Hans Peter | title=On norm form equations | journal=[[J. Number Theory]] | doi=10.1016/0022-314X(77)90072-5 | year=1977 | volume=9 | issue=3 | pages=370–380 | mr=0444562 | ref=harv}}\n* {{cite journal | last1=Schmidt | first1=Wolfgang M. | authorlink=Wolfgang M. Schmidt | title=Norm form equations | mr=0314761 | year=1972 | journal=[[Annals of Mathematics]] |series=Second Series | volume=96 | pages=526–551 | issue=3 | doi=10.2307/1970824 | ref=harv}}\n* {{cite book | last=Schmidt | first=Wolfgang M. | authorlink=Wolfgang M. Schmidt | title=Diophantine approximation | series=Lecture Notes in Mathematics | volume=785 | publisher=[[Springer-Verlag]] | year=1980 | edition=1996 with minor corrections | zbl=0421.10019  | mr=568710 | doi=10.1007/978-3-540-38645-2 | isbn=3-540-09762-7 | location=Berlin}}\n* {{cite book | last=Schmidt | first=Wolfgang M. | authorlink=Wolfgang M. Schmidt | title=Diophantine approximations and Diophantine equations | series=Lecture Notes in Mathematics | volume=1467 | publisher=[[Springer-Verlag]] | year=1991 | location=Berlin | isbn=3-540-54058-X | zbl=0754.11020 | mr=1176315 | doi=10.1007/BFb0098246}}\n\n[[Category:Diophantine approximation]]\n[[Category:Theorems in number theory]]"
    },
    {
      "title": "Three-gap theorem",
      "url": "https://en.wikipedia.org/wiki/Three-gap_theorem",
      "text": "In mathematics, the '''three-gap theorem''', '''three-distance theorem''', or '''Steinhaus conjecture''' states that if one places ''n'' points on a circle, at angles of ''&theta;'', 2''&theta;'', 3''&theta;'' ... from the starting point, then there will be at most three distinct distances between pairs of points in adjacent positions around the circle. When there are three distances, the largest of the three always equals the sum of the other two.{{r|automatic}} Unless ''&theta;'' is a rational multiple of {{pi}}, there will also be at least two distinct distances.\n\nThis result was conjectured by [[Hugo Steinhaus]], and proved in the 1950s by [[Vera T. Sós]], {{ill|János Surányi|hu|Surányi János (matematikus)}}, and [[Stanisław Świerczkowski]]. Its applications include the study of plant growth and musical tuning systems, and the theory of [[Sturmian word]]s.\n\n==Applications==\n[[File:Goldener Schnitt Blattstand.png|thumb|End-on view of a plant stem in which consecutive leaves are separated by the [[golden angle]] ]]\nIn [[phyllotaxis]] (the theory of plant growth), it has been observed that each successive leaf on the stems of many plants is turned from the previous leaf by the [[golden angle]], approximately 137.5°. It has been suggested that this angle maximizes the sun-collecting power of the plant's leaves.{{r|mnw}} If one looks end-on at a plant stem that has grown in this way, there will be at most three distinct angles between two leaves that are consecutive in the cyclic order given by this end-on view.{{r|vr87}} In the figure, the largest of these three angles occurs three times, between the leaves numbered 3 and 6, between leaves 4 and 7, and between leaves 5 and 8. The second-largest angle occurs five times, between leaves 6 and 1, 9 and 4, 7 and 2, 10 and 5, and 8 and 3. And the smallest angle occurs only twice, between leaves 1 and 9 and between leaves 2 and 10. (This phenomenon has nothing to do with the [[golden ratio]]; the same property, of having only three distinct gaps between consecutive points on a circle, happens for any other rotation angle, and not just for the golden angle.){{r|vr87}}\n\n[[File:PythagoreanTuningGeometric.png|thumb|upright=1.3|A geometric view of the tones of the [[Pythagorean tuning]] as points on a circle, showing the [[Pythagorean comma]] (the gap between the first and last points of the path) as the amount by which this tuning system fails to close up to a regular [[dodecagram]]. The edges between the points of the circle are the [[perfect fifth]]s from which this tuning system is constructed.]]\nIn [[music theory]], this theorem implies that if a [[tuning system]] is [[Generated collection|generated]] by some number of consecutive multiples of a given [[Interval (music)|interval]], reduced to a cyclic sequence by considering two tones to be equivalent when they differ by whole numbers of [[octaves]], then there are at most three different intervals between consecutive tones of the scale.{{r|carey|microtonal}} For instance, the [[Pythagorean tuning]] is constructed in this way from multiples of a [[perfect fifth]]. It has only two distinct intervals representing its [[semitone]]s,{{r|semitones}} but if it were extended by one more step then the sequence of intervals between its tones would include a third shorter interval, the [[Pythagorean comma]].{{r|pebble}}\n\nIn the theory of [[Sturmian word]]s, the theorem implies that the words of a given length ''n'' that appear within a given Sturmian word have at most three distinct frequencies. If there are three frequencies, then one of them must equal the sum of the other two.{{r|sturmian}}\n\n==History and proof==\nThe three-gap theorem was conjectured by [[Hugo Steinhaus]], and its first proofs were published in the late 1950s by [[Vera T. Sós]],{{r|sos}} {{ill|János Surányi|hu|Surányi János (matematikus)}},{{r|suranyi}} and [[Stanisław Świerczkowski]].{{r|sw}} Several later proofs have also been published.{{r|halton|slater67|vr88|mayero|ms}}\n\nThe following simple proof is due to Frank Liang. Define a gap (an arc of the circle between adjacent points of the given set) to be ''rigid'' if rotating that gap by an angle of ''&theta;'' does not produce another gap of the same length. Each rotation by ''&theta;'' increases the position of the gap endpoints in the placement ordering of the points, and such an increase cannot be repeated indefinitely, so every gap has the same length as a rigid gap. But the only ways for a gap to be rigid are for one of its two endpoints to be the last point in the placement sequence (so that the corresponding point is missing from the rotated gap) or for another point to land in its rotated copy. An endpoint can only be missing if the gap is one of the two gaps on either side of the last point in the placement ordering. And a point can only land within the rotated copy if it is the first point in the placement ordering. So there can be at most three rigid gaps, and at most three lengths of gaps. Additionally, when there are three, the rotated copy of a rigid gap that has the first point in it is partitioned by that point into two smaller gaps, so in this case the longest gap length is the sum of the other two.{{r|liang|shiu}}\n\nA closely related but earlier theorem, also called the three-gap theorem, is that if ''A'' is any arc of the circle, then the [[integer sequence]] of multiples of ''&theta;'' that land in ''A'' has at most three gaps between sequence values. Again, if there are three gaps then one is the sum of the other two.{{r|slater50|florek}}\n\n== See also ==\n\n* [[Equidistribution theorem]]\n\n==References==\n{{reflist|refs=\n\n<ref name=automatic>{{citation|title=Automatic Sequences: Theory, Applications, Generalizations|first1=Jean-Paul|last1=Allouche|first2=Jeffrey|last2=Shallit|author2-link=Jeffrey Shallit|publisher=Cambridge University Press|year=2003|isbn=9780521823326|contribution=2.6 The Three-Distance Theorem|url=https://books.google.com/books?id=2ZsSUStt96sC&pg=PA53|pages=53–55}}</ref>\n\n<ref name=carey>{{citation\n | last = Carey | first = Norman\n | year = 2007\n | doi = 10.1080/17459730701376743\n | issue = 2\n | journal = Journal of Mathematics and Music\n | pages = 79–98\n | title = Coherence and sameness in well-formed and pairwise well-formed scales\n | volume = 1}}</ref>\n\n<ref name=florek>{{citation|first=K.|last=Florek|title=Une remarque sur la répartition des nombres <math>n\\xi\\, (\\operatorname{mod} 1)</math>|journal=Colloq. Math.|volume=2|year=1951|pages=323–324}}</ref>\n\n<ref name=halton>{{citation\n | last = Halton | first = John H.\n | journal = Proc. Cambridge Philos. Soc.\n | mr = 0202668\n | pages = 665–670\n | title = The distribution of the sequence <math>\\{n\\xi\\}\\,(n=0,\\,1,\\,2,\\,\\ldots)</math>\n | volume = 61\n | issue = 3\n | year = 1965| doi = 10.1017/S0305004100039013\n }}</ref>\n\n<ref name=liang>{{citation\n | last = Liang | first = Frank M.\n | doi = 10.1016/0012-365X(79)90140-7\n | issue = 3\n | journal = [[Discrete Mathematics (journal)|Discrete Mathematics]]\n | mr = 548632\n | pages = 325–326\n | title = A short proof of the <math>3d</math> distance theorem\n | volume = 28\n | year = 1979}}</ref>\n\n<ref name=mayero>{{citation\n | last = Mayero | first = Micaela\n | contribution = The three gap theorem (Steinhaus conjecture)\n | doi = 10.1007/3-540-44557-9_10\n | pages = 162–173\n | publisher = Springer\n | series = Lecture Notes in Computer Science\n | title = Types for Proofs and Programs: International Workshop, TYPES'99, Lökeberg, Sweden, June 12–16, 1999, Selected Papers\n | volume = 1956\n | year = 2000| arxiv = cs/0609124\n | isbn = 978-3-540-41517-6\n }}</ref>\n\n<ref name=microtonal>{{citation|title=Microtonality and the Tuning Systems of Erv Wilson: Mapping the Harmonic Spectrum|series=Routledge Studies in Music Theory|first=Terumi|last=Narushima|publisher=Routledge|year=2017|isbn=9781317513421|pages=90–91|url=https://books.google.com/books?id=_jBEDwAAQBAJ&pg=PT90}}</ref>\n\n<ref name=mnw>{{citation|title=A Mathematical Nature Walk|first=John A.|last=Adam|authorlink=John A. Adam (mathematician)|publisher=Princeton University Press|year=2011|isbn=9781400832903|pages=35–41|url=https://books.google.com/books?id=5ou-E9UKS_AC&pg=PA35}}</ref>\n\n<ref name=ms>{{citation\n | last1 = Marklof | first1 = Jens\n | last2 = Strömbergsson | first2 = Andreas\n | doi = 10.4169/amer.math.monthly.124.8.741\n | issue = 8\n | journal = American Mathematical Monthly\n | mr = 3706822\n | pages = 741–745\n | title = The three gap theorem and the space of lattices\n | volume = 124\n | year = 2017| hdl = 1983/b5fd0feb-e42d-48e9-94d8-334b8dc24505\n }}</ref>\n\n<ref name=pebble>{{citation|title=A Smoother Pebble: Mathematical Explorations|first=Donald C.|last=Benson|publisher=Oxford University Press|year=2003|isbn=9780198032977|page=51|url=https://books.google.com/books?id=nXZJauwH2N0C&pg=PA51}}</ref>\n\n<ref name=semitones>{{citation|title=Music as Concept and Practice in the Late Middle Ages, Volume 3, Part 1|series=New Oxford history of music|editor1-first=Reinhard|editor1-last=Strohm|editor2-first=Bonnie J.|editor2-last=Blackburn|editor2-link=Bonnie J. Blackburn|publisher=Oxford University Press|year=2001|isbn=9780198162056|page=252|url=https://books.google.com/books?id=tR11H5BHT0EC&pg=PA252}}</ref>\n\n<ref name=shiu>{{citation\n | last = Shiu | first = Peter\n | doi = 10.1080/00029890.2018.1412210\n | issue = 3\n | journal = [[American Mathematical Monthly]]\n | mr = 3768035\n | pages = 264–266\n | title = A footnote to the three gaps theorem\n | volume = 125\n | year = 2018}}</ref>\n\n<ref name=slater50>{{citation\n | last = Slater | first = N. B.\n | journal = Proc. Cambridge Philos. Soc.\n | mr = 0041891\n | pages = 525–534\n | title = The distribution of the integers <math>N</math> for which <math>\\theta N<\\phi</math>\n | volume = 46\n | issue = 4\n | year = 1950| doi = 10.1017/S0305004100026086\n }}</ref>\n\n<ref name=slater67>{{citation\n | last = Slater | first = Noel B.\n | journal = Proc. Cambridge Philos. Soc.\n | mr = 0217019\n | pages = 1115–1123\n | title = Gaps and steps for the sequence <math>n\\theta \\bmod 1</math>\n | volume = 63\n | issue = 4\n | year = 1967| doi = 10.1017/S0305004100042195\n }}</ref>\n\n<ref name=sos>{{citation\n | last = Sós | first = V. T. | author-link = Vera T. Sós\n | journal = Ann. Univ. Sci. Budapest, Eötvös Sect. Math.\n | pages = 127–134\n | title = On the distribution mod 1 of the sequence <math>n\\alpha</math>\n | volume = 1\n | year = 1958}}</ref>\n\n<ref name=sturmian>{{citation | last = Lothaire | first = M. | authorlink = M. Lothaire | title =  Algebraic Combinatorics on Words | url = http://www-igm.univ-mlv.fr/~berstel/Lothaire/AlgCWContents.html | year = 2002 | publisher = [[Cambridge University Press]] | location = Cambridge | isbn = 978-0-521-81220-7 | chapter = Sturmian Words | zbl=1001.68093 | chapterurl = http://www-igm.univ-mlv.fr/%7Eberstel/Lothaire/ChapitresACW/C2.ps }}</ref>\n\n<ref name=suranyi>{{citation\n | last = Surányi | first = J.\n | journal = Ann. Univ. Sci. Budapest, Eötvös Sect. Math.\n | pages = 107–111\n | title = Über die Anordnung der Vielfachen einer reelen Zahl mod 1\n | volume = 1\n | year = 1958}}</ref>\n\n<ref name=sw>{{citation\n | last = Świerczkowski | first = S. | authorlink = Stanisław Świerczkowski\n | doi = 10.4064/fm-46-2-187-189\n | journal = Fundamenta Mathematicae\n | mr = 0104651\n | pages = 187–189\n | title = On successive settings of an arc on the circumference of a circle\n | volume = 46\n | issue = 2 | year = 1959}}</ref>\n\n<ref name=vr87>{{citation\n | last = van Ravenstein | first = Tony\n | year = 1987\n | doi = 10.1017/s0004972700026605\n | issue = 2\n | journal = Bulletin of the Australian Mathematical Society\n | page = 333\n | title = Number sequences and phyllotaxis\n | volume = 36}}</ref>\n\n<ref name=vr88>{{citation\n | last = van Ravenstein | first = Tony\n | issue = 3\n | journal = Journal of the Australian Mathematical Society | series = Series A\n | mr = 957201\n | pages = 360–370\n | title = The three-gap theorem (Steinhaus conjecture)\n | volume = 45\n | year = 1988| doi = 10.1017/S1446788700031062\n }}</ref>\n\n}}\n\n[[Category:Diophantine approximation]]\n[[Category:Theorems in number theory]]\n[[Category:Articles containing proofs]]\n[[Category:Mathematics of music]]"
    },
    {
      "title": "Roth's theorem",
      "url": "https://en.wikipedia.org/wiki/Roth%27s_theorem",
      "text": "{{for|Roth's theorem on arithmetic progressions|Szemerédi's theorem}}\n[[File:Joseph liouville.jpeg|thumb|Joseph Liouville]]\n[[File:Freeman Dyson.jpg|thumb|Freeman Dyson in 2005]]\n[[File:Axel Thue.jpg|thumb|Axel Thue]]\n[[File:Carl Ludwig Siegel.jpeg|thumb|Carl Siegel in 1975]]\n\nIn [[mathematics]], '''Roth's theorem''' is a fundamental result in [[diophantine approximation]] to [[algebraic number]]s. It is of a qualitative type, stating that a given algebraic number <math>\\alpha</math> may not have too many [[rational number]] approximations, that are 'very good'. Over half a century, the meaning of ''very good'' here was refined by a number of mathematicians, starting with [[Joseph Liouville]] in 1844 and continuing with work of {{harvs|txt|first=Axel|last= Thue|authorlink=Axel Thue|year=1909}},  {{harvs|txt|authorlink=Carl Ludwig Siegel|first=Carl Ludwig |last=Siegel|year=1921}}, {{harvs|txt|authorlink=Freeman Dyson|first=Freeman|last=Dyson|year=1947}}, and {{harvs|txt|authorlink=Klaus Roth|first=Klaus|last= Roth|year=1955}}.\n\n== Statement ==\n\nRoth's theorem states that every [[Irrational number|irrational]] algebraic number <math>\\alpha</math> has [[approximation exponent]] equal to 2, ''i.e.'', for given <math>\\varepsilon>0</math>, the inequality\n\n:<math>\\left|\\alpha - \\frac{p}{q}\\right| < \\frac{1}{q^{2 + \\varepsilon}}</math>\n\ncan have only finitely many solutions in [[coprime integers]] <math>p</math> and <math>q</math>, as was conjectured by Siegel. Therefore every irrational algebraic  α satisfies\n\n:<math>\\left|\\alpha - \\frac{p}{q}\\right| > \\frac{C(\\alpha,\\varepsilon)}{q^{2 + \\varepsilon}}</math>\n\nwith <math>C(\\alpha,\\varepsilon)</math> a positive number depending only on <math>\\varepsilon>0</math> and <math>\\alpha</math>.\n\n==Discussion==\n\nThe first result in this direction is [[Liouville's theorem (transcendence theory)|Liouville's theorem]] on approximation of algebraic numbers, which gives an approximation exponent of ''d'' for an algebraic number α of degree ''d''&nbsp;≥&nbsp;2.  This is already enough to demonstrate the existence of [[transcendental number]]s.  Thue realised that an exponent less than ''d'' would have applications to the solution of [[Diophantine equation]]s and in '''Thue's theorem''' from 1909 established an exponent <math>d/2 + 1 + \\varepsilon</math>.  Siegel's theorem improves this to an exponent about 2{{radic|''d''}}, and Dyson's theorem of 1947 has exponent about {{radic|2''d''}}.\n\nRoth's result with exponent 2 is in some sense the best possible, because this statement would fail on setting <math>\\varepsilon = 0</math>: by [[Dirichlet's theorem on diophantine approximation]] there are infinitely many solutions in this case.  However, there is a stronger conjecture of [[Serge Lang]]  that\n\n:<math>\\left|\\alpha - \\frac{p}{q}\\right| < \\frac{1}{q^2 \\log(q)^{1+\\varepsilon}}</math>\n\ncan have only finitely many solutions in integers ''p'' and ''q''.  If one lets α run over the whole of the set of real numbers, not just the algebraic reals, then both Roth's conclusion and Lang's  hold\nfor [[almost everywhere|almost all]] <math>\\alpha</math>. So both the theorem and the conjecture assert that a certain [[countable set]] misses a certain set of measure zero.<ref>It is also closely related to the [[Arithmetic of abelian varieties#Manin–Mumford conjecture|Manin–Mumford conjecture]].</ref>\n\nThe theorem is not currently [[Effective results in number theory|effective]]: that is, there is no bound known on the possible values of ''p'',''q'' given <math>\\alpha</math>. <ref name=HindrySilverman344>{{cite book | first1=Marc | last1=Hindry | first2=Joseph H. | last2=Silverman | authorlink2=Joseph H. Silverman | title=Diophantine Geometry: An Introduction | series=[[Graduate Texts in Mathematics]] | volume=201 | year=2000 | isbn=0-387-98981-1 | pages=344–345}}</ref>  {{harvtxt|Davenport|Roth|1955}} showed that Roth's techniques could be used to give an effective bound for the number of  ''p''/''q'' satisfying the inequality, using a \"gap\" principle.<ref name=HindrySilverman344/>   The fact that we do not actually know ''C''(ε) means that the project of solving the equation, or bounding the size of the solutions, is out of reach.\n\n== Proof technique ==\n\nThe proof technique involves constructing an [[auxiliary function|auxiliary]] multivariate polynomial in an arbitrarily large number of variables depending upon <math>\\varepsilon</math>, leading to a contradiction in the presence of too many good approximations. More specifically, one finds a certain number of rational approximations to the irrational algebraic number in question, and then applies the function over each of these simultaneously (i.e. each of these rational numbers serve as the input to a unique variable in the expression defining our function). By its nature, it was ineffective (see [[effective results in number theory]]); this is of particular interest since a major application of this type of result is to bound the number of solutions of some [[diophantine equation]]s.\n\n== Generalizations ==\n\nThere is a higher-dimensional version, [[subspace theorem|Schmidt's subspace theorem]], of the basic result. There are also numerous extensions, for example using the [[p-adic metric]],<ref>{{cite journal | first=D. | last=Ridout | title=The ''p''-adic generalization of the Thue–Siegel–Roth theorem | journal=[[Mathematika]] | volume=5 | pages=40–48 | year=1958 | zbl=0085.03501 | doi=10.1112/s0025579300001339}}</ref> based on the Roth method.\n\n[[William J. LeVeque]] generalized the result by showing that a similar bound holds when the approximating numbers are taken from a fixed [[algebraic number field]].  Define the ''[[height function|height]]'' ''H''(ξ) of an algebraic number ξ to be the maximum of the absolute values of the coefficients of its [[Minimal polynomial (field theory)|minimal polynomial]].  Fix κ>2.  For a given algebraic number α and algebraic number field ''K'', the equation\n\n:<math> | \\alpha - \\xi | < \\frac{1}{H(\\xi)^\\kappa} </math>\n\nhas only finitely many solutions in elements ξ of ''K''.<ref>{{cite book | last = LeVeque | first = William J. | authorlink = William J. LeVeque | title = Topics in Number Theory, Volumes I and II | publisher = Dover Publications | location = New York | year = 2002 | origyear = 1956 | isbn = 978-0-486-42539-9 | zbl=1009.11001 | pages=II:148–152}}</ref>\n\n==See also==\n*[[Davenport–Schmidt theorem]]\n*[[Granville–Langevin conjecture]]\n*[[Størmer's theorem]]\n\n==Notes==\n<references/>\n\n==References==\n*{{Citation | last1=Davenport | first1=H. | last2=Roth | first2=Klaus Friedrich | author1-link=Harold Davenport | author2-link=Klaus Roth | title=Rational approximations to algebraic numbers | doi=10.1112/S0025579300000814 | mr=0077577 | zbl=0066.29302 | year=1955 | journal=[[Mathematika]] | issn=0025-5793 | volume=2 | pages=160–167}}\n*{{Citation | last1=Dyson | first1=Freeman J. | author1-link=Freeman Dyson | title=The approximation to algebraic numbers by rationals | doi= 10.1007/BF02404697 | mr=0023854 | zbl=0030.02101 | year=1947 | journal=[[Acta Mathematica]] | issn=0001-5962 | volume=79 | pages=225–240}}\n*{{Citation | last1=Roth | first1=Klaus Friedrich | author1-link=Klaus Roth |title=Rational approximations to algebraic numbers | doi=10.1112/S0025579300000644 | mr=0072182 | year=1955 | journal=[[Mathematika]] | issn=0025-5793 | volume=2 | pages=1–20, 168 | zbl=0064.28501}}\n* {{cite journal |author=[[Wolfgang M. Schmidt]] |title=Diophantine approximation |series=[[Lecture Notes in Mathematics]] |volume=785 |publisher=Springer |origyear=1980|year=1996 |doi=10.1007/978-3-540-38645-2}}\n* {{cite journal |author=[[Wolfgang M. Schmidt]] |title=Diophantine approximations and Diophantine equations |series=[[Lecture Notes in Mathematics]] |publisher=Springer-Verlag |year=1991 |volume=1467 |doi=10.1007/BFb0098246}}\n*{{Citation | last1=Siegel | first1=Carl Ludwig | author1-link=Carl Ludwig Siegel | title=Approximation algebraischer Zahlen  | doi=10.1007/BF01211608 | year=1921 | journal=[[Mathematische Zeitschrift]] | issn=0025-5874 | volume=10 | issue=3 | pages=173–213 | mr=1544471}}\n*{{Citation | last1=Thue | first1=A. | author1-link=Axel Thue | title=Über Annäherungswerte algebraischer Zahlen | url=http://resolver.sub.uni-goettingen.de/purl?PPN243919689_0135 | year=1909 | journal=[[Journal für die reine und angewandte Mathematik]] | issn=0075-4102 | volume=135 | pages=284–305 | doi=10.1515/crll.1909.135.284}}\n\n==Further reading==\n* {{cite book | first=Alan | last=Baker | authorlink=Alan Baker (mathematician) | title=Transcendental Number Theory | publisher=[[Cambridge University Press]] | year=1975 | isbn=0-521-20461-5 | zbl=0297.10013 }}\n* {{cite book | first1=Alan | last1=Baker | authorlink1=Alan Baker (mathematician)| first2=Gisbert | last2= Wüstholz | authorlink2=Gisbert Wüstholz | title=Logarithmic Forms and Diophantine Geometry | series=New Mathematical Monographs | volume=9 | publisher=[[Cambridge University Press]] | year=2007 | isbn=978-0-521-88268-2 | zbl=1145.11004 }}\n* {{cite book | first1=Enrico | last1=Bombieri | authorlink1=Enrico Bombieri | first2=Walter | last2=Gubler | title=Heights in Diophantine Geometry | series=New Mathematical Monographs | volume=4 | publisher=[[Cambridge University Press]] | year=2006 | isbn=978-0-521-71229-3 | zbl=1130.11034 }}\n* {{cite book | first=Paul | last=Vojta | authorlink=Paul Vojta | title=Diophantine Approximations and Value Distribution Theory | series=Lecture Notes in Mathematics | volume=1239 | publisher=[[Springer-Verlag]] | year=1987 | isbn=3-540-17551-2 | zbl=0609.14011 }}\n\n{{DEFAULTSORT:Thue-Siegel-Roth Theorem}}\n[[Category:Diophantine approximation]]\n[[Category:Theorems in number theory]]"
    },
    {
      "title": "Van der Corput sequence",
      "url": "https://en.wikipedia.org/wiki/Van_der_Corput_sequence",
      "text": "[[File:Van_der_Corput_sequence_999.svg|thumb|330px<!-- Use a multiple of 11 to reduce aliasing artifact -->|Illustration of the filling of the unit interval (horizontal axis) using the first ''n'' terms of the decimal Van der Corput sequence, for ''n'' from 0 to 999 (vertical axis)]]\n\nA '''van der Corput sequence''' is an example of the simplest one-dimensional  [[low-discrepancy sequence]] over the [[unit interval]]; it was first described in 1935 by the [[Netherlands|Dutch]] mathematician [[Johannes van der Corput|J. G. van der Corput]].  It is constructed by reversing the [[base (exponentiation)|base-''n'' representation]] of the sequence of [[natural number]]s (1, 2, 3, …).\n\nThe ''b''-ary representation of the positive integer ''n'' (≥ 1) is\n:<math>\nn=\\sum_{k=0}^{L-1}d_k(n)b^k,\n</math>\n\nwhere ''b'' is the base in which the number ''n'' is represented, and 0 ≤ ''d''<sub>''k''</sub>(''n'') < ''b'', i.e. the ''k''-th digit in the ''b''-ary expansion of ''n''.\nThe ''n''-th number in the van der Corput sequence is\n:<math>\ng_b(n)=\\sum_{k=0}^{L-1}d_k(n)b^{-k-1}.\n</math>\n\n== Examples ==\nFor example, to get the [[decimal]] van der Corput sequence, we start by dividing the numbers 1 to 9 in tenths (''x''/10), then we change the denominator to 100 to begin dividing in hundredths (''x''/100). In terms of numerator, we begin with all two-digit numbers from 10 to 99, but in '''backwards''' order of digits. Consequently, we will get the numerators grouped by the end digit. Firstly, all two-digit numerators that end with 1, so the next numerators are 01, 11, 21, 31, 41, 51, 61, 71, 81, 91. Then the numerators ending with 2, so they are 02, 12, 22, 32, 42, 52, 62, 72, 82, 92. An after the numerators ending in 3: 03, 13, 23 and so on...\n\nThus, the sequence begins\n\n:<math>\\left\\{ \\tfrac{1}{10}, \\tfrac{2}{10}, \\tfrac{3}{10}, \\tfrac{4}{10}, \\tfrac{5}{10}, \\tfrac{6}{10}, \\tfrac{7}{10}, \\tfrac{8}{10}, \\tfrac{9}{10}, \\tfrac{1}{100}, \\tfrac{11}{100}, \\tfrac{21}{100}, \\tfrac{31}{100}, \\tfrac{41}{100}, \\tfrac{51}{100}, \\tfrac{61}{100}, \\tfrac{71}{100}, \\tfrac{81}{100}, \\tfrac{91}{100}, \\tfrac{2}{100}, \\tfrac{12}{100}, \\tfrac{22}{100}, \\tfrac{32}{100}, \\ldots \\right\\},</math>\n\nor in floating-point representation:\n\n:0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.01, 0.11, 0.21, 0.31, 0.41, 0.51, 0.61, 0.71, 0.81, 0.91, 0.02, 0.12, 0.22, 0.32, …,\n\nThe same can be done for the [[binary numeral system]], and the binary van der Corput sequence is\n\n:0.1<sub>2</sub>, 0.01<sub>2</sub>, 0.11<sub>2</sub>, 0.001<sub>2</sub>, 0.101<sub>2</sub>, 0.011<sub>2</sub>, 0.111<sub>2</sub>, 0.0001<sub>2</sub>, 0.1001<sub>2</sub>, 0.0101<sub>2</sub>, 0.1101<sub>2</sub>, 0.0011<sub>2</sub>, 0.1011<sub>2</sub>, 0.0111<sub>2</sub>, 0.1111<sub>2</sub>, …\n\nor, equivalently,\n\n:<math>\\tfrac{1}{2}, \\tfrac{1}{4}, \\tfrac{3}{4}, \\tfrac{1}{8}, \\tfrac{5}{8}, \\tfrac{3}{8}, \\tfrac{7}{8}, \\tfrac{1}{16}, \\tfrac{9}{16}, \\tfrac{5}{16}, \\tfrac{13}{16}, \\tfrac{3}{16}, \\tfrac{11}{16}, \\tfrac{7}{16}, \\tfrac{15}{16}, \\ldots.</math>\n\nThe elements of the van der Corput sequence (in any base) form a [[dense set]] in the unit interval; that is, for any real number in [0, 1], there exists a [[subsequence]] of the van der Corput sequence that [[limit of a sequence|converges]] to that number.  They are also [[equidistributed]] over the unit interval.\n==C implementation==\n<source lang=\"C\">\ndouble corput(int n, int base){\n        double q=0, bk=(double)1/base;\n        while(n>0) { q += (n % base)*bk; n /= base; bk /= base; }\n        return q;\n}\n</source>\n\n==See also==\n* [[Bit-reversal permutation]]\n* [[Constructions of low-discrepancy sequences]]\n* [[Halton sequence]], a natural generalization of the van der Corput sequence to higher dimensions\n\n==References==\n* {{citation | last=van der Corput | first=J.G. | authorlink=Johannes van der Corput | title=Verteilungsfunktionen (Erste Mitteilung) | language=German | zbl=0012.34705 | journal=Proceedings of the Koninklijke Akademie van Wetenschappen te Amsterdam | volume=38 | pages=813–821 | year=1935 | url=http://www.dwc.knaw.nl/DL/publications/PU00014607.pdf }}\n* {{citation | last=Kuipers | first=L. | last2= Niederreiter | first2=H. | author2-link = Harald Niederreiter | title = Uniform distribution of sequences | publisher=[[Dover Publications]] | year=2005 | origyear=1974 | isbn=0-486-45019-8 | page=129,158 | zbl=0281.10001 }}\n\n==External links==\n* [http://mathworld.wolfram.com/vanderCorputSequence.html Van der Corput sequence] at [[MathWorld]]\n\n[[Category:Quasirandomness]]\n[[Category:Diophantine approximation]]\n[[Category:Sequences and series]]"
    },
    {
      "title": "Halton sequence",
      "url": "https://en.wikipedia.org/wiki/Halton_sequence",
      "text": "{{multiple image\n   | direction = vertical\n   | width     = \n   | footer    = 256 points from the first 256 points of the 2,3 Halton sequence (top) compared with a pseudorandom number source (bottom).  The Halton sequence covers the space more evenly. (red=1,..,10, blue=11,..,100, green=101,..,256)\n   | image1    = Halton sequence 2D.svg\n   | alt1      = \n   | caption1  = \n   | image2    = Pseudorandom sequence 2D.svg\n   | alt2      = \n   | caption2  = \n  }}\nIn [[statistics]], '''Halton sequences'''<!-- Who's Halton? --> are  [[sequence]]s used to generate points in space for numerical methods such as [[Monte Carlo simulations]].  Although these sequences are [[Deterministic system (mathematics)|deterministic]], they are of [[Low-discrepancy sequence|low discrepancy]], that is, appear to be [[random]] for many purposes. They were first introduced in 1960 and are an example of a [[quasi-random number]] sequence.  They generalise the one-dimensional [[van der Corput sequence]]s.\n\n== Example of Halton sequence used to generate points in (0,&nbsp;1) &times; (0,&nbsp;1) in R<sup>2</sup> ==\n[[File:Halton_sequence_2_3.svg|thumb|250px|Illustration of the first 8 points of the 2,3 Halton sequence]]\nThe Halton sequence is constructed according to a deterministic method that uses [[coprime integers|coprime numbers]] as its bases. As a simple example, let's take one dimension of the Halton sequence to be based on 2 and the other  on 3.  To generate the sequence for 2, we start by dividing the interval (0,1) in half, then in fourths, eighths, etc., which generates\n\n: {{frac|2}}, {{frac|4}}, {{frac|3|4}}, {{frac|8}}, {{frac|5|8}}, {{frac|3|8}}, {{frac|7|8}}, {{frac|16}}, {{frac|9|16}},...\n\nEquivalently, the nth number of this sequence is the number n written in binary representation, inverted, and written after the decimal point. This is true for any base. As an example, to find the sixth element of the above sequence, we'd write 6 = 1*2{{sup|2}} + 1*2{{sup|1}} + 0*2{{sup|0}} = 110{{sub|2}}, which can be inverted and placed after the decimal point to give 0.011{{sub|2}} = 0*2{{sup|-1}} + 1*2{{sup|-2}} + 1*2{{sup|-3}} = {{frac|3|8}}. So the sequence above is the same as\n\n: 0.1{{sub|2}}, 0.01{{sub|2}}, 0.11{{sub|2}}, 0.001{{sub|2}}, 0.101{{sub|2}}, 0.011{{sub|2}}, 0.111{{sub|2}}, 0.0001{{sub|2}}, 0.1001{{sub|2}},...\n\nTo generate the sequence for 3, we divide the interval (0,1) in thirds, then ninths, twenty-sevenths, etc., which generates\n\n: {{frac|3}}, {{frac|2|3}}, {{frac|9}}, {{frac|4|9}}, {{frac|7|9}}, {{frac|2|9}}, {{frac|5|9}}, {{frac|8|9}}, {{frac|27}},...\n\nWhen we pair them up, we get a sequence of points in a unit square:\n\n: ({{frac|2}}, {{frac|3}}), ({{frac|4}}, {{frac|2|3}}), ({{frac|3|4}}, {{frac|9}}), ({{frac|8}}, {{frac|4|9}}), ({{frac|5|8}}, {{frac|7|9}}), ({{frac|3|8}}, {{frac|2|9}}), ({{frac|7|8}}, {{frac|5|9}}), ({{frac|16}}, {{frac|8|9}}), ({{frac|9|16}}, {{frac|27}}).\n\nEven though standard Halton sequences perform very well in low dimensions, correlation problems have been noted between sequences generated from higher primes.  For example, if we started with the primes 17 and 19, the first 16 pairs of points: ({{frac|17}}, {{frac|19}}), ({{frac|2|17}}, {{frac|2|19}}), ({{frac|3|17}}, {{frac|3|19}}) ... ({{frac|16|17}}, {{frac|16|19}}) would have perfect [[linear correlation]]. To avoid this, it is common to drop the first 20 entries, or some other predetermined quantity depending on the primes chosen. Several other methods have also been proposed. One of the most prominent solutions is the scrambled Halton sequence, which uses permutations of the coefficients used in the construction of the standard sequence. Another solution is the leaped Halton, which skips points in the standard sequence. Using, eg, only each 409th point (also other prime numbers not used in the Halton core sequence are possible), can achieve significant improvements.<ref name=\"Kc1997\">Kocis and Whiten, 1997</ref>\n\n== Implementation in pseudocode ==\n\n '''algorithm''' Halton-Sequence '''is'''\n     '''inputs''': index <math>i</math>\n             base <math>b</math>\n     '''output''': result <math>r</math>\n \n     <math>f \\larr 1</math>\n     <math>r \\larr 0</math>\n \n     '''while''' <math>i > 0</math> '''do'''\n         <math>f \\larr f/b</math>\n         <math>r \\larr r + f * (i \\operatorname{mod} b)</math>\n         <math>i \\larr \\lfloor i/b \\rfloor</math>\n \n     '''return''' <math>r</math>\n\n== See also ==\n*[[Constructions of low-discrepancy sequences]]\n\n==References==\n{{Reflist}}\n* {{citation | last=Kuipers | first=L. | last2= Niederreiter | first2=H. | author2-link = Harald Niederreiter | title = Uniform distribution of sequences | publisher=[[Dover Publications]] | year=2005 | isbn=0-486-45019-8 | page=129 }}\n* {{citation | last=Niederreiter | first=Harald | authorlink = Harald Niederreiter | title=Random number generation and quasi-Monte Carlo methods | publisher=[[Society for Industrial and Applied Mathematics|SIAM]] | year=1992 | isbn=0-89871-295-5 | page=29 }}.\n* {{citation | last=Halton | first=J. | title=Algorithm 247: Radical-inverse quasi-random point sequence | journal=Communications of the ACM | volume=7 | year=1964 | doi=10.1145/355588.365104 | page=701-701 }}.\n* {{citation | last=Kocis | first=Ladislav | last2=Whiten | first2=William | title=Computational Investigations of Low-Discrepancy Sequences | journal=ACM Transactions on Mathematical Software | volume=23 | year=1997 | doi=10.1145/264029.264064 | page=266-296 }}.\n\n[[Category:Quasirandomness]]\n[[Category:Sequences and series]]\n[[Category:Articles with example pseudocode]]"
    },
    {
      "title": "Quasi-Monte Carlo method",
      "url": "https://en.wikipedia.org/wiki/Quasi-Monte_Carlo_method",
      "text": "{{Use American English|date=January 2019}}{{Short desc|Numerical integration process}}{{multiple image\n   | direction = horizontal\n   | width     = \n   | footer    = 256 points from a pseudorandom number source, Halton sequence, and Sobol sequence (red=1,..,10, blue=11,..,100, green=101,..,256). Points from Sobol sequence are more evenly distributed.\n   | image1    = Pseudorandom sequence 2D.svg\n   | alt1      = \n   | caption1  = [Pseudorandom sequence]\n   | image2    = Sobol_sequence_2D.svg\n   | alt2      = \n   | caption2  = [Low-discrepancy sequence (Sobol sequence)]\n  }}\n\nIn [[numerical analysis]], the '''quasi-Monte Carlo method''' is a method for [[numerical integration]] and solving some other problems using [[low-discrepancy sequence]]s (also called quasi-random sequences or sub-random sequences). This is in contrast to the regular [[Monte Carlo method]] or [[Monte Carlo integration]], which are based on sequences of [[pseudorandom]] numbers.\n\nMonte Carlo and quasi-Monte Carlo methods are stated in a similar way.\nThe problem is to approximate the integral of a function ''f'' as the average of the function evaluated at a set of points ''x''<sub>1</sub>, ..., ''x''<sub>''N''</sub>:\n\n:<math> \\int_{[0,1]^s} f(u)\\,{\\rm d}u \\approx \\frac{1}{N}\\,\\sum_{i=1}^N f(x_i). </math>\n\nSince we are integrating over the ''s''-dimensional unit cube, each ''x''<sub>''i''</sub> is a vector of ''s'' elements. The difference between quasi-Monte Carlo and Monte Carlo is the way the x<sub>i</sub> are chosen. Quasi-Monte Carlo uses a low-discrepancy sequence such as the [[Halton sequence]], the [[Sobol sequence]], or the Faure sequence, whereas Monte Carlo uses a pseudorandom sequence. The advantage of using low-discrepancy sequences is a faster rate of convergence. Quasi-Monte Carlo has a rate of convergence close to O(1/''N''), whereas the rate for the Monte Carlo method is O(''N''<sup>−0.5</sup>).<ref name=\"asmunssen_glynn_book\">Søren Asmussen and Peter W. Glynn, ''Stochastic Simulation: Algorithms and Analysis'', Springer, 2007, 476 pages</ref>\n\nThe Quasi-Monte Carlo method recently became popular in the area of [[mathematical finance]] or [[computational finance]].<ref name=\"asmunssen_glynn_book\" /> In these areas, high-dimensional numerical integrals, where the integral should be evaluated within a threshold ε, occur frequently. Hence, the Monte Carlo method and the quasi-Monte Carlo method are beneficial in these situations.\n\n== Approximation error bounds of quasi-Monte Carlo ==\nThe approximation error of the quasi-Monte Carlo method is bounded by a term proportional to the discrepancy of the set ''x''<sub>1</sub>, ..., ''x''<sub>''N''</sub>. Specifically, the [[Low-discrepancy sequence#The Koksma.E2.80.93Hlawka inequality|Koksma–Hlawka inequality]] states that the error\n\n:<math> \\varepsilon = \\left| \\int_{[0,1]^s} f(u)\\,{\\rm d}u - \\frac{1}{N}\\,\\sum_{i=1}^N f(x_i) \\right| </math>\nis bounded by \n:<math> |\\varepsilon| \\leq V(f) D_N, </math>\n\nwhere ''V''(''f'') is the Hardy–Krause variation of the function ''f'' (see Morokoff and Caflisch (1995) <ref name=\"morokoffNcaflisch\" /> for the detailed definitions). ''D''<sub>''N''</sub> is the so-called star discrepancy of the set (''x''<sub>1</sub>,...,''x''<sub>''N''</sub>) and is defined as\n\n:<math> D_N = \\sup_{Q \\subset [0,1]^s} \\left| \\frac{\\text{number of points in } Q} N - \\operatorname{volume}(Q) \\right|, </math>\n\nwhere ''Q'' is a rectangular solid in [0,1]<sup>''s''</sup> with sides parallel to the coordinate axes.<ref name=\"morokoffNcaflisch\" /> The inequality <math> |\\varepsilon| \\leq V(f) D_N </math> can be used to show that the error of the approximation by the quasi-Monte Carlo method is <math> O\\left(\\frac{(\\log N)^s}{N}\\right) </math>, whereas the Monte Carlo method has a probabilistic error of <math> O\\left(\\frac 1 {\\sqrt N}\\right) </math>. Though we can only state the upper bound of the approximation error, the convergence rate of quasi-Monte Carlo method in practice is usually much faster than its theoretical bound.<ref name=\"asmunssen_glynn_book\" /> Hence, in general, the accuracy of the quasi-Monte Carlo method increases faster than that of the Monte Carlo method. However, this advantage is only guaranteed if ''N'' is large enough and if the variation is finite.\n\n== Monte Carlo and quasi-Monte Carlo for multidimensional integrations ==\n\nFor one-dimensional integration, quadrature methods such as the [[trapezoidal rule]], [[Simpson's rule]], or [[Newton–Cotes formulas]] are known to be efficient if the function is smooth. These approaches can be also used for multidimensional integrations by repeating the one-dimensional integrals over multiple dimensions. However, the number of function evaluations grows exponentially as&nbsp;''s'', the number of dimensions, increases. Hence, a method that can overcome this [[curse of dimensionality]] should be used for multidimensional integrations. The standard Monte Carlo method is frequently used when the quadrature methods are difficult or expensive to implement.<ref name=\"morokoffNcaflisch\">William J. Morokoff and [[Russel E. Caflisch]], ''Quasi-Monte Carlo integration'', J. Comput. Phys. '''122''' (1995), no. 2, 218–230. ''(At [[CiteSeer]]: [http://citeseer.ist.psu.edu/morokoff95quasimonte.html])''</ref> Monte Carlo and quasi-Monte Carlo methods are accurate and relatively fast when the dimension is high, up to 300 or higher.<ref>Rudolf Schürer, ''A comparison between (quasi-)Monte Carlo and cubature rule based methods for solving high-dimensional integration problems'',  Mathematics and Computers in Simulation, Volume 62, Issues 3–6, 3 March 2003, 509–517</ref>\n\nMorokoff and Caflisch <ref name=\"morokoffNcaflisch\" /> studied the performance of Monte Carlo and quasi-Monte Carlo methods for integration. In the paper, Halton, Sobol, and Faure sequences for quasi-Monte Carlo are compared with the standard Monte Carlo method using pseudorandom sequences. They found that the Halton sequence performs best for dimensions up to around 6; the Sobol sequence performs best for higher dimensions; and the Faure sequence, while outperformed by the other two, still performs better than a pseudorandom sequence.\n\nHowever, Morokoff and Caflisch <ref name=\"morokoffNcaflisch\" /> gave examples where the advantage of the quasi-Monte Carlo is less than expected theoretically. Still, in the examples studied by Morokoff and Caflisch, the quasi-Monte Carlo method did yield a more accurate result than the Monte Carlo method with the same number of points. Morokoff and Caflisch remark that the advantage of the quasi-Monte Carlo method is greater if the integrand is smooth, and the number of dimensions ''s'' of the integral is small.\n\n== Drawbacks of quasi-Monte Carlo ==\nLemieux mentioned the drawbacks of quasi-Monte Carlo:<ref name=\"lemieuxbook\">Christiane Lemieux, ''Monte Carlo and Quasi-Monte Carlo Sampling'', Springer, 2009, {{ISBN|978-1441926760}}</ref>\n* In order for <math> O\\left(\\frac{(\\log N)^s}{N}\\right) </math> to be smaller than <math> O\\left(\\frac{1}{\\sqrt{N}}\\right) </math>, <math>s</math> needs to be small and <math>N</math> needs to be large (e.g. <math> N>2^s </math>). For large ''s'' and practical ''N'' values, the discrepancy of a point set from a low-discrepancy generator might be not smaller than for a random set.\n* For many functions arising in practice, <math> V(f) = \\infty </math> (e.g. if Gaussian variables are used).\n* We only know an upper bound on the error (i.e., ''ε'' ≤ ''V''(''f'') ''D''<sub>''N''</sub>) and it is difficult to compute <math> D_N^* </math> and <math> V(f) </math>.\nIn order to overcome some of these difficulties, we can use a randomized quasi-Monte Carlo method.\n\n== Randomization of quasi-Monte Carlo ==\nSince the low discrepancy sequence are not random, but deterministic, quasi-Monte Carlo method can be seen as a deterministic algorithm or derandomized algorithm. In this case, we only have the bound (e.g., ''ε'' ≤ ''V''(''f'') ''D''<sub>''N''</sub>) for error, and the error is hard to estimate. In order to recover our ability to analyze and estimate the variance, we can randomize the method (see [[randomization]] for the general idea). The resulting method is called the randomized quasi-Monte Carlo method and can be also viewed as a variance reduction technique for the standard Monte Carlo method.<ref>Moshe Dror, Pierre L’Ecuyer and Ferenc Szidarovszky, ''Modeling Uncertainty: An Examination of Stochastic Theory, Methods, and Applications'', Springer 2002, pp. 419-474</ref>  Among several methods, the simplest transformation procedure is through random shifting. Let {''x''<sub>1</sub>,...,''x''<sub>''N''</sub>} be the point set from the low discrepancy sequence. We sample ''s''-dimensional random vector ''U'' and mix it with {''x''<sub>1</sub>, ..., ''x''<sub>''N''</sub>}. In detail, for each ''x''<sub>''j''</sub>, create\n\n: <math> y_j = x_j + U \\pmod 1</math>\n\nand use the sequence <math>(y_j)</math> instead of <math>(x_j)</math>. If we have ''R'' replications for Monte Carlo, sample s-dimensional random vector U for each replication. Randomization allows to give an estimate of the variance while still using quasi-random sequences. Compared to pure quasi Monte-Carlo, the number of samples of the quasi random sequence will be divided by ''R'' for an equivalent computational cost, which reduces the theoretical convergence rate. Compared to standard Monte-Carlo, the variance and the computation speed are slightly better from the experimental results in Tuffin (2008) <ref>Bruno Tuffin, ''Randomization of Quasi-Monte Carlo Methods for Error Estimation: Survey and Normal Approximation'', Monte Carlo Methods and Applications mcma. Volume 10, Issue 3-4, Pages 617–628, ISSN (Online) 1569-3961, ISSN (Print) 0929-9629, DOI: 10.1515/mcma.2004.10.3-4.617, May 2008</ref>\n\n== See also ==\n\n* [[Monte Carlo method]]\n* [[Monte Carlo methods in finance]]\n* [[Quasi-Monte Carlo methods in finance]]\n* [[Biology Monte Carlo method]]\n* [[Computational physics]]\n* [[Low-discrepancy sequences]]\n* [[Discrepancy theory]]\n* [[Markov chain Monte Carlo]]\n\n== References ==\n\n<references />\n* [[Russel E. Caflisch|R. E. Caflisch]], ''Monte Carlo and quasi-Monte Carlo methods'', Acta Numerica vol. 7, Cambridge University Press, 1998, pp.&nbsp;1–49.\n* Josef Dick and Friedrich Pillichshammer, ''Digital Nets and Sequences. Discrepancy Theory and Quasi-Monte Carlo Integration'', Cambridge University Press, Cambridge, 2010, {{ISBN|978-0-521-19159-3}}\n* Gunther Leobacher and Friedrich Pillichshammer, ''Introduction to quasi-Monte Carlo Integration and Applications'', Compact Textbooks in Mathematics, Birkhäuser, 2014, {{ISBN|978-3-319-03424-9}}\n* Michael Drmota and Robert F. Tichy, ''Sequences, discrepancies and applications'', Lecture Notes in Math., '''1651''', Springer, Berlin, 1997, {{ISBN|3-540-62606-9}}\n* William J. Morokoff and [[Russel E. Caflisch]], ''Quasi-random sequences and their discrepancies'', SIAM J. Sci. Comput. '''15''' (1994), no. 6, 1251–1279 ''(At [[CiteSeer]]:[http://citeseer.ist.psu.edu/morokoff94quasirandom.html])''\n* [[Harald Niederreiter]]. ''Random Number Generation and Quasi-Monte Carlo Methods.'' Society for Industrial and Applied Mathematics, 1992. {{ISBN|0-89871-295-5}}\n* [[Harald Niederreiter|Harald G. Niederreiter]], ''Quasi-Monte Carlo methods and pseudo-random numbers'', Bull. Amer. Math. Soc. '''84''' (1978), no. 6, 957–1041\n* Oto Strauch and Štefan Porubský, ''Distribution of Sequences: A Sampler'', Peter Lang Publishing House, Frankfurt am Main 2005, {{ISBN|3-631-54013-2}}\n\n== External links ==\n* [http://roth.cs.kuleuven.be/wiki/Main_Page The MCQMC Wiki page contains a lot of free online material on Monte Carlo and quasi-Monte Carlo methods]\n* [https://web.archive.org/web/20071205010919/http://www.puc-rio.br/marco.ind/quasi_mc.html A very intuitive and comprehensive introduction to Quasi-Monte Carlo methods]\n\n[[Category:Monte Carlo methods]]\n[[Category:Quasirandomness]]"
    },
    {
      "title": "Quasi-Monte Carlo methods in finance",
      "url": "https://en.wikipedia.org/wiki/Quasi-Monte_Carlo_methods_in_finance",
      "text": "High-dimensional integrals in hundreds or thousands of variables occur commonly in finance. These integrals have to be computed numerically to within a threshold <math>\\epsilon</math>. If the integral is of dimension <math>d</math> then in the worst case, where one has a guarantee of error at most <math>\\epsilon</math>, the computational complexity is typically of order <math>\\epsilon^{-d}</math>. That is, the problem suffers the [[curse of dimensionality]]. In 1977 P. Boyle, University of Waterloo, proposed using [[Monte Carlo Method|Monte Carlo (MC)]] to evaluate options.<ref name=\"B77\">Boyle, P. (1977), Options: a Monte Carlo approach, J. Financial Economics, 4, 323-338.</ref> Starting in early 1992, [[Joseph Traub|J. F. Traub]], Columbia University, and a graduate student at the time, S. Paskov, used [[Quasi-Monte Carlo method|quasi-Monte Carlo]] (QMC) to price a [[Collateralized mortgage obligation]] with parameters specified by Goldman Sachs. Even though it was believed by the world's leading experts that QMC should not be used for high-dimensional integration, Paskov and Traub found that QMC beat MC by one to three orders of magnitude and also enjoyed other desirable attributes. Their results were first published<ref name=\"PTJPM\">Paskov, S. H. and Traub, J. F. (1995), Faster evaluation of financial derivatives, J. Portfolio Management, 22(1), 113-120.</ref> in 1995. Today QMC is widely used in the financial sector to value financial derivatives; see [[Quasi-Monte Carlo methods in finance#Books|list of books below]].\n\nQMC is not a panacea for all high-dimensional integrals. A number of explanations have been proposed for why QMC is so good for financial derivatives. This continues to be a very fruitful research area.\n\n==Monte Carlo and quasi-Monte Carlo methods==\nIntegrals in hundreds or thousands of variables are common in [[computational finance]]. These have to be approximated numerically to within an error threshold <math>\\epsilon</math>. It is well known that if a worst case guarantee of error at most <math>\\epsilon</math> is required then the computational complexity of integration may be exponential in <math>d</math>, the dimension of the integrand; See <ref name=\"TW98\">Traub, J. F and Werschulz, A. G. (1998), Complexity and Information, Cambridge University Press, Cambridge, UK.</ref> Ch. 3 for details. To break this curse of dimensionality one can use the Monte Carlo (MC) method defined by\n\n:<math>\\varphi^{\\mathop{\\rm MC}}(f)=\\frac 1n \\sum_{i=1}^nf(x_i),</math>\n\nwhere the evaluation points <math>x_i</math> are randomly chosen. It is well known that the expected error of Monte Carlo is of order <math>n^{-1/2}</math>. Thus the cost of the algorithm that has error <math>\\epsilon</math> is of order <math>\\epsilon^{-2}</math> breaking the curse of dimensionality.\n\nOf course in computational practice pseudo-random points are used. Figure 1 shows the distribution of 500 pseudo-random points on the unit square.\n[[File:MonteCarlo500points.JPG|thumb|right|Figure 1. 500 pseudo-random points]]\nNote there are regions where there are no points and other regions where there are clusters of points. It would be desirable to sample the integrand at uniformly distributed points. A rectangular grid would be uniform but even if there were only 2 grid points in each Cartesian direction there would be <math>2^d</math> points. So the desideratum should be as few points as possible chosen as uniform as possible.\n\nIt turns out there is a well-developed part of number theory which deals exactly with this desideratum. Discrepancy is a measure of deviation from uniformity so what one wants are low discrepancy sequences (LDS).<ref name=\"N92\">[[Harald Niederreiter|Niederreiter, H.]] (1992), Random Number Generation and Quasi-Monte Carlo Methods, CBMS-NSF Regional Conference Series in Applied Mathematics, SIAM, Philadelphia.</ref> Numerous LDS have been created named after their inventors, e.g.\n*Halton\n*Hammersley\n*[[Sobol sequence|Sobol]]\n*Faure\n*[[Harald Niederreiter|Niederreiter]]\nFigure 2. gives the distribution of 500 LDS points. [[File:LowDiscrepancy500points.JPG|thumb|right|Figure 2. 500 low discrepancy points]]\n\nThe quasi-Monte Carlo (QMC) method is defined by\n\n:<math> \\varphi^{\\mathop{\\rm QMC}}(f)=\\frac 1n \\sum_{i=1}^nf(x_i),</math>\n\nwhere the <math>x_i</math> belong to an LDS. The standard terminology quasi-Monte Carlo is somewhat unfortunate since MC is a randomized method whereas QMC is purely deterministic.\n\nThe uniform distribution of LDS is desirable. But the worst case error of QMC is of order\n\n:<math>\\frac{(\\log n)^d}{n},</math>\n\nwhere <math>n</math> is the number of sample points. See <ref name=\"N92\" /> for the theory of LDS and references to the literature. The rate of convergence of LDS may be contrasted with the expected rate of convergence of MC which is <math>n^{-1/2}</math>. For <math>d</math> small the rate of convergence of QMC is faster than MC but for <math>d</math> large the factor <math>(\\log n)^d</math> is devastating. For example, if <math>d=360</math>, then even with <math>\\log n=2</math> the QMC error is proportional to <math>2^{360}</math>. Thus it was widely believed by the world's leading experts that QMC should not be used for high-dimensional integration. For example, in 1992 Bratley, Fox and Niederreiter<ref name=\"BFN92\">Bratley, P., Fox, B. L. and [[Harald Niederreiter|Niederreiter, H.]] (1992), Implementation and tests of low-discrepancy sequences, ACM Transactions on Modelling and Computer Simulation, Vol. 2, No. 3, 195-213.</ref> performed extensive testing on certain mathematical problems. They conclude \"in high-dimensional problems (say <math>d > 12</math>), QMC seems to offer no practical advantage over MC\". In 1993, Rensburg and Torrie<ref name=\"RT93\">van Rensburg, E. J. J. and Torrie, G. M. (1993), Estimation of multidimensional integrals: is Monte Carlo the best method? J. Phys. A: Math. Gen., 26(4), 943-953.</ref> compared QMC with MC for the numerical estimation of high-dimensional integrals which occur in computing virial coefficients for the hard-sphere fluid. They conclude QMC is more effective than MC only if <math>d<10</math>. As we shall see, tests on 360-dimensional integrals arising from a collateralized mortgage obligation (CMO) lead to very different conclusions.\n\nWoźniakowski's 1991 paper<ref name=\"W91\">Woźniakowski, H. (1991), Average case complexity of multivariate integration, Bull. Amer. Math. Soc. (New Ser.), 24(1), 185-194.</ref> showing the connection between average case complexity of integration and QMC led to new interest in QMC.\nWoźniakowski's result received considerable coverage in the scientific press<ref name=\"SIAMQMC\">[[Barry Arthur Cipra|Cipra, Barry Arthur]] (1991), Multivariate Integration: It ain't so tough (on average), SIAM NEWS, 28 March.</ref>\n.<ref name=\"SCIAMER\">Traub, J. F. and Woźniakowski, H. (1994), Breaking intractability, Scientific American, 270(1), January, 102-107.</ref>\nIn early 1992, I. T. Vanderhoof, New York University, became aware of Woźniakowski's result and gave Woźniakowski's colleague [[Joseph Traub|J. F. Traub]], Columbia University, a CMO with parameters set by Goldman Sachs. This CMO had 10 tranches each requiring the computation of a 360 dimensional integral. Traub asked a Ph.D. student, Spassimir Paskov, to compare QMC with MC for the CMO. In 1992 Paskov built a software system called FinDer and ran extensive tests. To the Columbia's research group's surprise and initial disbelief Paskov reported that QMC was always superior to MC in a number of ways. Details are given below. Preliminary results were presented by Paskov and Traub to a number of Wall Street firms in Fall 1993 and Spring 1994. The firms were initially skeptical of the claim that QMC was superior to MC for pricing financial derivatives. A January 1994 article in Scientific American by Traub and Woźniakowski<ref name=\"SCIAMER\" /> discussed the theoretical issues and reported that \"Preliminary results obtained by testing certain finance problems suggests the superiority of the deterministic methods in practice\".\nIn Fall 1994 Paskov wrote a Columbia University Computer Science Report which appeared in slightly modified form in 1997.<ref name=\"P97\">Paskov, S. H., New methodologies for valuing derivatives, 545-582, in Mathematics of Derivative Securities, S. Pliska and M. Dempster eds., Cambridge University Press, Cambridge.</ref>\n\nIn Fall 1995\nPaskov and Traub published a paper in the \"Journal of Portfolio Management\".<ref name=\"PTJPM\" /> They compared MC and two QMC methods. The two deterministic methods used Sobol and Halton points. Since better LDS were created later, no comparison will be made between Sobol and Halton sequences. The experiments drew the following conclusions regarding the performance of MC and QMC on the 10 tranche CMO:\n*QMC methods converge significantly faster than MC\n*MC is sensitive to the initial seed\n*The convergence of QMC is smoother than the convergence of MC. This makes automatic termination easier for QMC.\n\nTo summarize, QMC beats MC for the CMO on accuracy, confidence level, and speed.\n\nThis paper was followed by reports on tests by a number of researchers which also led to the conclusion the QMC is superior to MC for a variety of high-dimensional finance problems. This includes papers by Caflisch and Morokoff (1996),<ref name=\"CM96\">[[Russel E. Caflisch|Caflisch, R. E.]] and Morokoff, W. (1996), Quasi-Monte Carlo computation of a finance problem, 15-30, in Proceedings Workshop on Quasi-Monte Carlo Methods and their Applications, 11 December 1995, K.-T. Fang and F. Hickernell eds., Hong Kong Baptist University.</ref>\nJoy, Boyle, Tan (1996),<ref name=\"JBT96\">Joy, C., Boyle, P. P. and Tang, K. S. (1996), Quasi-Monte Carlo methods in numerical finance, Management Science, 42(6), 926-938.</ref>\nNinomiya and Tezuka (1996),<ref name=\"NT96\">Ninomiya, S. and Tezuka, S. (1996), Toward real-time pricing of complex financial derivatives, Appl. Math. Finance, 3, 1-20.</ref>\nPapageorgiou and Traub (1996),<ref name=\"PT96\">Papageorgiou, A. and Traub, J. F. (1996), Beating Monte Carlo, Risk, 9(6), 63-65.</ref>\nAckworth, Broadie and Glasserman (1997),<ref name=\"ABG97\">Ackworth, P., Broadie, M. and Glasserman, P. (1997), A comparison of some Monte Carlo techniques for option pricing, 1-18, in Monte Carlo and Quasi-Monte Carlo Methods '96, H. Hellekalek, P. Larcher and G. Zinterhof eds., Springer Verlag, New York.</ref> Kucherenko and co-authors \n<ref name=\"Kuch07\">Kucherenko S., Shah N. The Importance of being Global.Application of Global Sensitivity Analysis in Monte Carlo option Pricing Wilmott, 82-91, July 2007. http://www.broda.co.uk/gsa/wilmott_GSA_SK.pdf</ref>\n<ref name=\"Bianc15\">Bianchetti M., Kucherenko S., Scoleri S., Pricing and Risk Management with High-Dimensional Quasi Monte Carlo and Global Sensitivity Analysis, Wilmott, July, pp. 46-70, 2015, http://www.broda.co.uk/doc/PricingRiskManagement_Sobol.pdf</ref>\n\nFurther testing of the CMO<ref name=\"PT96\"/> was carried out by Anargyros Papageorgiou, who developed an improved version of the FinDer software system. The new results include the following:\n*'''Small number of sample points:''' For the hardest CMO tranche QMC using the generalized Faure LDS due to S. Tezuka<ref name=\"TEZ\">Tezuka, S., Uniform Random Numbers:Theory and Practice, Kluwer, Netherlands.</ref> achieves accuracy <math>10^{-2}</math> with just 170 points. MC requires 2700 points for the same accuracy. The significance of this is that due to future interest rates and prepayment rates being unknown, financial firms are content with accuracy of <math>10^{-2}</math>.\n*'''Large number of sample points:''' The advantage of QMC over MC is further amplified as the sample size and accuracy demands grow. In particular, QMC is 20 to 50 times faster than MC with moderate sample sizes, and can be up to 1000 times faster than MC<ref name=\"PT96\" /> when high accuracy is desired QMC.\nCurrently the highest reported dimension for which QMC outperforms MC is 65536.<ref name=\"BRODA\">BRODA Ltd. http://www.broda.co.uk</ref> \nThe software is the Sobol' Sequence generator SobolSeq65536 which generates Sobol' Sequences satisfying Property A for all dimensions and Property A' for the adjacent dimensions.\nSobolSeq generators outperform all other known generators both in speed and accuracy <ref name=\"Sobol12\">Sobol’ I., Asotsky D. , Kreinin A. , Kucherenko S.  (2012) Construction and Comparison of High-Dimensional Sobol’ Generators, Wilmott, Nov, 64-79</ref>\n\n==Theoretical explanations==\nThe results reported so far in this article are empirical. A number of possible theoretical explanations have been advanced. This has been a very research rich area leading to powerful new concepts but a definite answer has not been obtained.\n\nA possible explanation of why QMC is good for finance is the following. Consider a tranche of the CMO mentioned earlier. The integral gives expected future cash flows from a basket of 30-year mortgages at 360 monthly intervals. Because of the discounted value of money variables representing future times are increasingly less important. In a seminal paper I. Sloan and H. Woźniakowski<ref name=\"SW98\">Sloan, I. and Woźniakowski, H. (1998), When are quasi-Monte Carlo algorithms efficient for high dimensional integrals?, J. Complexity, 14(1), 1-33.</ref>\nintroduced the idea of weighted spaces. In these spaces the dependence on the successive variables can be moderated by weights. If the weights decrease sufficiently rapidly the curse of dimensionality is broken even with a worst case guarantee. This paper led to a great amount of work on the tractability of integration and other problems.<ref name=\"NW08\" >Novak, E. and Wo�zniakowski, H. (2008), Tractability of multivariate problems, European Mathematical Society, Zurich (forthcoming).</ref>  A problem is tractable when its complexity is of order <math>\\epsilon^{-p}</math> and <math>p</math> is independent of the dimension.\n\nOn the other hand, ''effective dimension'' was proposed by Caflisch, Morokoff and Owen<ref name=\"CMO97\">[[Russel E. Caflisch|Caflisch, R. E.]], Morokoff, W. and Owen, A. B. (1997), Valuation of mortgage backed\nsecurities using Brownian bridges to reduce effective dimension, Journal of\nComputational Finance, 1, 27-46.</ref> as an indicator\nof the difficulty of high-dimensional integration. The purpose was to explain\nthe remarkable success of quasi-Monte Carlo (QMC) in approximating the very-high-dimensional integrals in finance. They argued that\nthe integrands are of low effective dimension and that is why QMC is much\nfaster than Monte Carlo (MC).\nThe impact of the arguments of Caflisch et al.<ref name=\"CMO97\" /> was great.\nA number of papers deal with the relationship between the error of QMC and the effective dimension<ref name=\"Hic98\">Hickernell, F. J. (1998), Lattice rules: how well do they measure up?, in P. Hellekalek and G. Larcher (Eds.), Random and Quasi-Random Point Sets, Springer, 109-166.</ref>\n.<ref name=\"Kuch07\"/>\n<ref name=\"Bianc15\"/>\n<ref name=\"WS05\">Wang, X. and Sloan, I. H. (2005), Why are high-dimensional finance problems often of low effective dimension?, SIAM Journal on Scientific Computing, 27(1),\n159-183.</ref>\n\nIt is known that QMC fails for certain functions that have high effective dimension.<ref name=\"BFN92\" />\nHowever, low effective dimension is not a necessary condition for QMC to beat MC and for\nhigh-dimensional integration\nto be tractable.  In 2005, Tezuka<ref name=\"T03\" >Tezuka, S. (2005), On the necessity of low-effective dimension, Journal of Complexity,\n21, 710-721.</ref>  exhibited a class of functions of\n<math>d</math>  variables, all with maximum effective dimension equal to <math>d</math>. For these functions QMC is very fast since its convergence rate is of order <math>n^{-1}</math>, where <math>n</math> is the number of function evaluations.\n\n==Isotropic integrals==\nQMC can also be superior to MC and to other methods for isotropic problems, that is, problems where all variables are equally important. For example, Papageorgiou and Traub<ref name=\"PT97\">Papageorgiou, A. and Traub, J. F. (1997), Faster evaluation of multidimensional\nintegrals, Computers in Physics, 11(6), 574-578.</ref> reported test results on the model integration problems suggested by the physicist B. D. Keister<ref name=\"K96\">Keister, B. D. (1996), Multidimensional quadrature algorithms, Computers in Physics, 10(20), 119-122.</ref>\n\n:<math>\\left(\\frac 1{2\\pi}\\right)^{d/2} \\int_{\\mathbb R^d}\\cos(\\|x\\| )e^{-\\| x\\|^2}\\,dx,</math>\n\nwhere <math>\\|\\cdot\\|</math> denotes the Euclidean norm and <math>d=25</math>. Keister reports that using a standard numerical method some 220,000 points were needed to obtain a relative error on the order of <math>10^{-2}</math>. A QMC calculation using the generalized Faure low discrepancy sequence<ref name=\"TEZ\" /> (QMC-GF) used only 500 points to obtain the same relative error. The same integral was tested for a range of values of <math> d</math> up to <math> d=100</math>. Its error was\n\n:<math> c\\cdot n^{-1},</math>\n\n<math>c<110</math>, where <math> n</math> is the number of evaluations of <math>f</math>. This may be compared with the MC method whose error was proportional to <math>n^{-1/2}</math>.\n\nThese are empirical results. In a theoretical investigation Papageorgiou<ref name=\"P01\">Papageorgiou, A. (2001), Fast convergence of quasi-Monte Carlo for a class of isotropic integrals, Math. Comp., 70, 297-306.</ref> proved that the convergence rate of QMC for a class of <math>d</math>-dimensional isotropic integrals which includes the integral defined above is of the order\n\n:<math>\\sqrt{\\log n}/n.</math>\n\nThis is with a worst case guarantee compared to the expected convergence rate of <math>n^{-1/2}</math> of Monte Carlo and shows the superiority of QMC for this type of integral.\n\nIn another theoretical investigation Papageorgiou<ref name=\"P03\">Papageorgiou, A. (2003), Sufficient conditions for fast quasi-Monte Carlo convergence, J. Complexity, 19(3), 332-351.</ref> presented sufficient conditions for fast QMC convergence. The conditions apply to isotropic and non-isotropic problems and, in particular, to a number of problems in computational finance. He presented classes of functions where even in the worst case the convergence rate of QMC is of order\n\n::::<math>n^{-1+p(\\log n)^{-1/2}},</math>\n\nwhere <math>p\\ge 0</math> is a constant that depends on the class of functions.\n\nBut this is only a sufficient condition and leaves open the major question we pose in the next section.\n\n==Open questions==\n# Characterize for which high-dimensional integration problems QMC is superior to MC.\n# Characterize types of financial instruments for which QMC is superior to MC.\n\n==See also==\n* [[Monte Carlo methods in finance]]\n* [[Historical simulation (finance)]]\n\n==Resources==\n\n=== Books ===\n* {{cite book | title = Monte Carlo: methodologies and applications for pricing and risk management | author = [[Bruno Dupire]] | year = 1998 | publisher = Risk |isbn = 1-899332-91-X}}\n* {{cite book | title = Monte Carlo methods in financial engineering | author = Paul Glasserman | year = 2003 | publisher = [[Springer-Verlag]] | isbn = 0-387-00451-3 }}\n* {{cite book | title = Monte Carlo methods in finance | author = [[Peter Jaeckel]] | year = 2002 | publisher = John Wiley and Sons | isbn = 0-471-49741-X }}\n* {{cite book | title = Modern Computational Finance: AAD and Parallel Simulations | author = Antoine Savine | year = 2018 | publisher = John Wiley and Sons | isbn = 978-1119539452}}\n* {{cite book | title = Monte Carlo Simulation & Finance | author = Don L. McLeish| year = 2005 | publisher = |isbn = 0-471-67778-7}}\n* {{cite book | title = Monte Carlo Statistical Methods| author = Christian P. Robert, George Casella| year = 2004 | publisher = |isbn = 0-387-21239-6}}\n\n===Models===\n*[https://web.archive.org/web/20071205010919/http://www.puc-rio.br/marco.ind/quasi_mc.html Spreadsheets available for download], Prof. Marco Dias, [[Pontifícia Universidade Católica do Rio de Janeiro|PUC-Rio]]\n\n==References==\n{{Reflist|2}}\n{{Use dmy dates|date=September 2010}}\n\n{{DEFAULTSORT:Quasi-Monte Carlo Methods In Finance}}\n[[Category:Monte Carlo methods in finance]]\n[[Category:Quasirandomness]]"
    },
    {
      "title": "Sobol sequence",
      "url": "https://en.wikipedia.org/wiki/Sobol_sequence",
      "text": "{{multiple image\n   | direction = vertical\n   | width     = \n   | footer    = 256 points from the first 256 points for the 2,3 Sobol sequence (top) compared with a pseudorandom number source (bottom).  The Sobol sequence covers the space more evenly. (red=1,..,10, blue=11,..,100, green=101,..,256)\n   | image1    = Sobol sequence 2D.svg\n   | alt1      = \n   | caption1  = \n   | image2    = Pseudorandom sequence 2D.svg\n   | alt2      = \n   | caption2  = \n  }}\n'''Sobol sequences''' (also called LP<sub>τ</sub> sequences or (''t'',&nbsp;''s'') sequences in base&nbsp;2) are an example of quasi-random [[low-discrepancy sequence]]s. They were first introduced by the Russian mathematician [[Ilya M. Sobol]] (Илья Меерович Соболь) in 1967.<ref name=Sobol67>Sobol,I.M.\n\n(1967), \"Distribution of points in a cube and approximate evaluation of integrals\". ''Zh. Vych. Mat. Mat. Fiz.'' '''7''': 784–802 (in Russian); ''U.S.S.R Comput. Maths. Math. Phys.'' '''7''': 86–112 (in English).</ref>\n\nThese sequences use a base of two to form successively finer uniform partitions of the unit interval and then reorder the coordinates in each dimension.\n\n== Good distributions in the ''s''-dimensional unit hypercube ==\n\nLet ''I<sup>s</sup>'' = [0,1]<sup>''s''</sup> be the ''s''-dimensional unit hypercube, and ''f'' a real integrable function over ''I<sup>s</sup>''. The original motivation of Sobol was to construct a sequence ''x<sub>n</sub>'' in ''I<sup>s</sup>'' so that\n:<math> \\lim_{n\\to\\infty} \\frac{1}{n} \\sum_{i=1}^n f(x_i) = \\int_{I^s} f </math>\nand the convergence be as fast as possible.\n\nIt is more or less clear that for the sum to converge towards the integral, the points ''x<sub>n</sub>'' should fill ''I<sup>s</sup>'' minimizing the holes. Another good property would be that the projections of ''x<sub>n</sub>'' on a lower-dimensional face of ''I<sup>s</sup>'' leave very few holes as well. Hence the homogeneous filling of ''I<sup>s</sup>'' does not qualify because in lower dimensions many points will be at the same place, therefore useless for the integral estimation.\n\nThese good distributions are called (''t'',''m'',''s'')-nets and (''t'',''s'')-sequences in base ''b''. To introduce them, define first an elementary ''s''-interval in base ''b'' a subset of ''I<sup>s</sup>'' of the form\n:<math> \\prod_{j=1}^s \\left[ \\frac{a_j}{b^{d_j}}, \\frac{a_j+1}{b^{d_j}} \\right], </math>\nwhere ''a<sub>j</sub>'' and ''d<sub>j</sub>'' are non-negative integers, and <math> a_j < b^{d_j} </math> for all ''j'' in {1, ...,s}.\n\nGiven 2 integers <math>0\\leq t\\leq m</math>, a (''t'',''m'',''s'')-net in base ''b'' is a sequence ''x<sub>n</sub>'' of ''b<sup>m</sup>'' points of ''I<sup>s</sup>'' such that <math>\\operatorname{Card} P \\cap \\{x_1, ..., x_{b^m}\\} = b^t</math> for all elementary interval ''P'' in base ''b'' of hypervolume ''λ''(''P'') = ''b<sup>t−m</sup>''.\n\nGiven a non-negative integer ''t'', a (''t'',''s'')-sequence in base ''b'' is an infinite sequence of points ''x<sub>n</sub>'' such that for all integers <math>k \\geq 0, m \\geq t</math>, the sequence <math>\\{x_{kb^m}, ..., x_{(k+1)b^m-1}\\}</math> is a (''t'',''m'',''s'')-net in base ''b''.\n\nIn his article, Sobol described ''Π<sub>τ</sub>-meshes'' and ''LP<sub>τ</sub> sequences'', which are (''t'',''m'',''s'')-nets and (''t'',''s'')-sequences in base 2 respectively. The terms (''t'',''m'',''s'')-nets and (''t'',''s'')-sequences in base ''b'' (also called Niederreiter sequences) were coined in 1988 by [[Harald Niederreiter]].<ref name=Nied88>[[Harald Niederreiter|Niederreiter, H.]] (1988). \"Low-Discrepancy and Low-Dispersion Sequences\", ''Journal of Number Theory'' '''30''': 51–70.</ref> The term ''Sobol sequences'' was introduced in late English-speaking papers in comparison with [[Halton sequence|Halton]], Faure and other low-discrepancy sequences.\n\n== A fast algorithm for the construction of Sobol sequences ==\n\nA more efficient [[Gray code]] implementation was proposed by Antonov and Saleev.<ref name=AS79>Antonov, I.A. and Saleev, V.M. (1979) \"An economic method of computing LP<sub>τ</sub>-sequences\". ''Zh. Vych. Mat. Mat. Fiz.'' '''19''': 243–245 (in Russian); ''U.S.S.R. Comput. Maths. Math. Phys.'' '''19''': 252–256 (in English).</ref>\n\nAs for the generation of Sobol numbers, they are clearly aided by the use of Gray code <math>G(n)=n \\oplus \\lfloor n/2 \\rfloor</math> instead of ''n'' for constructing the ''n''-th point draw.\n\nSuppose we have already generated all the Sobol sequence draws up to ''n''&nbsp;&minus;&nbsp;1 and kept in memory the values ''x''<sub>''n''&minus;1,''j''</sub> for all the required dimensions. Since the Gray code ''G''(''n'') differs from that of the preceding one ''G''(''n''&nbsp;&minus;&nbsp;1) by just a single, say the ''k''-th, bit (which is a rightmost bit of ''n''&nbsp;&minus;&nbsp;1), all that needs to be done is a single [[XOR]] operation for each dimension in order to propagate all of the ''x''<sub>''n''&minus;1</sub> to ''x''<sub>''n''</sub>, i.e.\n\n:<math>\nx_{n,i} = x_{n-1,i} \\oplus v_{k,i}.\n</math>\n\n== Additional uniformity properties ==\n\nSobol introduced additional uniformity conditions known as property A and A’.<ref name=Sobol76>Sobol, I. M. (1976) \"Uniformly distributed sequences with an additional uniform property\". ''Zh. Vych. Mat. Mat. Fiz.'' '''16''': 1332–1337 (in Russian); ''U.S.S.R. Comput. Maths. Math. Phys.'' '''16''': 236–242 (in English).</ref>\n\n;Definition: A low-discrepancy sequence is said to satisfy '''Property A''' if for any binary segment (not an arbitrary subset) of the ''d''-dimensional sequence of length 2<sup>''d''</sup> there is exactly one draw in each 2<sup>''d''</sup> hypercubes that result from subdividing the unit hypercube along each of its length extensions into half.\n\n;Definition: A low-discrepancy sequence is said to satisfy '''Property A’''' if for any binary segment (not an arbitrary subset) of the ''d''-dimensional sequence of length 4<sup>''d''</sup> there is exactly one draw in each 4<sup>''d''</sup> hypercubes that result from subdividing the unit hypercube along each of its length extensions into four equal parts.\n\nThere are mathematical conditions that guarantee properties A and A'.\n\n;Theorem: The ''d''-dimensional Sobol sequence possesses Property A iff\n::<math>\n\\det(\\mathbf{V}_d) \\equiv 1 (\\mod 2),\n</math>\n:where '''V'''<sup>''d''</sup> is the ''d''&nbsp;×&nbsp;''d'' binary matrix defined by\n::<math>\n\\mathbf{V}_d := \\begin{pmatrix}\n{v_{1,1,1}}&{v_{2,1,1}}&{\\dots}&{v_{d,1,1}}\\\\ \n{v_{1,2,1}}&{v_{2,2,1}}&{\\dots}&{v_{d,2,1}}\\\\ \n{\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}\\\\ \n{v_{1,d,1}}&{v_{2,d,1}}&{\\dots}&{v_{d,d,1}}\n\\end{pmatrix},\n</math>\n:with ''v''<sub>''k'',''j'',''m''</sub> denoting the ''m''-th digit after the binary point of the direction number ''v''<sub>''k'',''j''</sub> = (0.''v''<sub>''k'',''j'',1</sub>''v''<sub>''k'',''j'',2</sub>...)<sub>2</sub>.\n\n;Theorem: The ''d''-dimensional Sobol sequence possesses Property A' iff\n::<math>\n\\det(\\mathbf{U}_d) \\equiv 1 \\mod 2,\n</math>\n:where '''U'''<sup>''d''</sup> is the 2''d''&nbsp;×&nbsp;2''d'' binary matrix defined by\n::<math>\n\\mathbf{U}_d := \\begin{pmatrix}\n{v_{1,1,1}}&{v_{1,1,2}}&{v_{2,1,1}}&{v_{2,1,2}}&{\\dots}&{v_{d,1,1}}&{v_{d,1,2}}\\\\ \n{v_{1,2,1}}&{v_{1,2,2}}&{v_{2,2,1}}&{v_{2,2,2}}&{\\dots}&{v_{d,2,1}}&{v_{d,2,2}}\\\\ \n{\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{\\ddots}&{\\vdots}&{\\vdots}\\\\ \n{v_{1,2d,1}}&{v_{1,2d,2}}&{v_{2,2d,1}}&{v_{2,2d,2}}&{\\dots}&{v_{d,2d,1}}&{v_{d,2d,2}}\n\\end{pmatrix},\n</math>\n:with ''v''<sub>''k'',''j'',''m''</sub> denoting the ''m''-th digit after the binary point of the direction number ''v''<sub>''k'',''j''</sub> = (0.''v''<sub>''k'',''j'',1</sub>''v''<sub>''k'',''j'',2</sub>...)<sub>2</sub>.\n\nTests for properties A and A’ are independent. Thus it is possible to construct the Sobol sequence that satisfies both properties A and A’ or only one of them.\n\n== The initialisation of Sobol numbers ==\n\nTo construct a Sobol sequence, a set of direction numbers ''v''<sub>''i'',''j''</sub> needs to be selected. There is some freedom in the selection of initial direction numbers.<ref group=\"note\">These numbers are usually called ''initialisation numbers''.</ref> Therefore, it is possible to receive different realisations of the Sobol sequence for selected dimensions. A bad selection of initial numbers can considerably reduce the efficiency of Sobol sequences when used for computation.\n\nArguably the easiest choice for the initialisation numbers is just to have the ''l''-th leftmost bit set, and all other bits to be zero, i.e. ''m''<sub>''k'',''j''</sub> = 1 for all ''k'' and ''j''. This initialisation is usually called ''unit initialisation''. However, such a sequence fails the test for Property A and A’ even for low dimensions and hence this initialisation is bad.\n\n== Implementation and availability of Sobol sequences ==\n\nGood initialisation numbers for different numbers of dimensions are provided by several authors.  For example, Sobol provides initialisation numbers for dimensions up to 51.<ref name=SobLev76>Sobol, I.M. and Levitan, Y.L. (1976). \"The production of points uniformly distributed in a multidimensional cube\" ''Tech. Rep. 40, Institute of Applied Mathematics, USSR Academy of Sciences'' (in Russian).</ref> The same set of initialisation numbers is used by Bratley and Fox.<ref name=BF88>Bratley, P. and Fox, B. L. (1988), \"Algorithm 659: Implementing Sobol’s quasirandom sequence generator\". ''ACM Trans. Math. Software'' '''14''': 88–100.</ref>\n\nInitialisation numbers for high dimensions are available on Joe and Kuo.<ref name=JK>{{cite web|url=http://web.maths.unsw.edu.au/~fkuo/sobol/ |title=Sobol sequence generator |publisher=[[University of New South Wales]] |date=2010-09-16 |accessdate=2013-12-20}}</ref> [[Peter Jaeckel|Peter Jäckel]] provides initialisation numbers up to dimension 32 in his book \"[[Monte Carlo methods in finance]]\".<ref name=Jackel>Jäckel, P. (2002) \"Monte Carlo methods in finance\". New York: [[John Wiley and Sons]]. ({{ISBN|0-471-49741-X}}.)</ref>\n\nOther implementations are available as C, Fortran 77, or Fortran 90 routines in the [[Numerical Recipes]] collection of software.<ref name=NumRec>Press, W.H., Teukolsky, S. A., Vetterling, W. T., and Flannery, B. P. (1992) \"Numerical Recipes in Fortran 77: The Art of Scientific Computing, 2nd ed.\" ''Cambridge University Press, Cambridge, U.K.''</ref>  A [[Free and open-source software|free/open-source]] implementation in up to 1111 dimensions, based on the Joe and Kuo initialisation numbers, is available in C<ref>[https://github.com/stevengj/nlopt/blob/master/src/util/sobolseq.c C implementation of the Sobol sequence] in the [http://ab-initio.mit.edu/nlopt NLopt library] (2007).</ref>, and up to 21201 dimensions in Python<ref>{{Cite web|url=https://pyscenarios.readthedocs.io|title=pyscenarios: Python Scenario Generator|last=Imperiale|first=G.|date=|website=|archive-url=|archive-date=|dead-url=|access-date=}}</ref> and [[Julia (programming language)|Julia]]<ref>[https://github.com/stevengj/Sobol.jl Sobol.jl] package: Julia implementation of the Sobol sequence.</ref>. A different free/open-source implementation in up to 1111 dimensions is available for [[C++]], [[Fortran 90]], [[Matlab]], and [[Python (programming language)|Python]].<ref>[http://people.sc.fsu.edu/~jburkardt/cpp_src/sobol/sobol.html The Sobol Quasirandom Sequence], code for C++/Fortran 90/Matlab/Python by J. Burkardt</ref>\n\nFinally, commercial Sobol sequence generators are available within, for example, the [[NAG Numerical Libraries|NAG Library]],<ref name=NAG>{{cite web|url=http://www.nag.co.uk/ |title=Numerical Algorithms Group |publisher=Nag.co.uk |date=2013-11-28 |accessdate=2013-12-20}}</ref>. A version is available from the British-Russian Offshore Development Agency (BRODA).<ref name=BRODA_info_article>{{cite journal |author=I. Sobol’, D. Asotsky, A. Kreinin, S. Kucherenko |title=Construction and Comparison of High-Dimensional Sobol’ Generators |url=http://www.broda.co.uk/doc/HD_SobolGenerator.pdf |format=PDF |journal=Wilmott Journal |year=2011 |volume=Nov | pages=64–79}}</ref><ref name=BRODA>{{cite web|url=http://www.broda.co.uk |title=Broda |publisher=Broda |date=2004-04-16 |accessdate=2013-12-20}}</ref> MATLAB also contains an implementation<ref>[https://www.mathworks.com/help/stats/sobolset.html sobolset reference page]. Retrieved 2017-07-24.</ref> as part of its Statistics Toolbox.\n\n== See also ==\n\n*[[Low-discrepancy sequence]]s\n*[[Quasi-Monte Carlo method]]\n\n== Notes ==\n<references group=\"note\" />\n\n== References ==\n<references />\n\n== External links ==\n* [http://www.acm.org/calgo/contents/ Collected Algorithms of the ACM] ''(See algorithms 647, 659, and 738.)''\n* [http://www.mathfinance.cn/tags/sobol/ Collection of Sobol sequences generator programming codes]\n* [https://github.com/kirill77/RNGSobol/ Freeware C++ generator of Sobol sequence]\n\n{{DEFAULTSORT:Sobol Sequence}}\n[[Category:Quasirandomness]]\n[[Category:Sequences and series]]"
    },
    {
      "title": "Numerical analysis",
      "url": "https://en.wikipedia.org/wiki/Numerical_analysis",
      "text": "{{merge from|Numerical method|date=January 2019}}\n{{more footnotes|date=November 2013}}\n{{Use dmy dates|date=July 2012}}\n[[Image:Ybc7289-bw.jpg|thumb|250px|right|Babylonian clay tablet [[YBC 7289]] (c. 1800–1600 BC) with annotations. The approximation of the [[square root of 2]] is four [[sexagesimal]] figures, which is about six [[decimal]] figures. 1 + 24/60 + 51/60<sup>2</sup> + 10/60<sup>3</sup> = 1.41421296...<ref>[http://it.stlawu.edu/%7Edmelvill/mesomath/tablets/YBC7289.html Photograph, illustration, and description of the ''root(2)'' tablet from the Yale Babylonian Collection]</ref>]]\n'''Numerical analysis''' is the study of [[algorithm]]s that use numerical [[approximation]] (as opposed to [[symbolic computation|symbolic manipulations]]) for the problems of [[mathematical analysis]] (as distinguished from [[discrete mathematics]]). Numerical analysis naturally finds application in all fields of engineering and the physical sciences, but in the 21st&nbsp;century also the life sciences, social sciences, medicine, business and even the arts have adopted elements of scientific computations. The growth in computing power has revolutionized the use of realistic mathematical models in science and engineering, and subtle numerical analysis is required to implement these detailed models of the world. For example, [[ordinary differential equation]]s appear in [[celestial mechanics]] (predicting the motions of planets, stars and galaxies); [[numerical linear algebra]] is important for data analysis; [[stochastic differential equation]]s and [[Markov chain]]s are essential in simulating living cells for medicine and biology.\n\nBefore the advent of modern computers, [[numerical method]]s often depended on hand [[interpolation]] formulas applied to data from large printed tables. Since the mid 20th century, computers calculate the required functions instead, but many of the same  formulas nevertheless continue to be used as part of the software algorithms.\n\nThe numerical point of view goes back to the earliest mathematical writings. A tablet from the [[Yale Babylonian Collection]] ([[YBC 7289]]), gives a [[sexagesimal]] numerical approximation of the [[square root of 2]], the length of the [[diagonal]] in a [[unit square]]. Computing the sides of a triangle in terms of [[square root]]s is a basic practical problem, for example in [[astronomy]], [[carpentry]], and [[construction]].<ref>The New Zealand Qualification authority specifically mentions this skill in document 13004 version 2, dated 17 October 2003 titled [http://www.nzqa.govt.nz/nqfdocs/units/pdf/13004.pdf CARPENTRY THEORY: Demonstrate knowledge of setting out a building]</ref>\n\nNumerical analysis continues this long tradition: rather than exact symbolic answers, which can only be applied to real-world measurements by translation into digits, it gives approximate solutions within specified error bounds.\n\n==General introduction==\nThe overall goal of the field of numerical analysis is the design and analysis of techniques to give approximate but accurate solutions to hard problems, the variety of which is suggested by the following:\n* Advanced numerical methods are essential in making [[numerical weather prediction]] feasible.\n* Computing the trajectory of a spacecraft requires the accurate numerical solution of a system of ordinary differential equations.\n* Car companies can improve the crash safety of their vehicles by using computer simulations of car crashes. Such simulations essentially consist of solving [[partial differential equation]]s numerically.\n* [[Hedge fund]]s (private investment funds) use tools from all fields of numerical analysis to attempt to calculate the value of [[stock]]s and [[Derivative (finance)|derivatives]] more precisely than other market participants.\n* Airlines use sophisticated optimization algorithms to decide ticket prices, airplane and crew assignments and fuel needs. Historically, such algorithms were developed within the overlapping field of [[operations research]].\n* Insurance companies use numerical programs for [[Actuary|actuarial]] analysis.\n\nThe rest of this section outlines several important themes of numerical analysis.\n\n===History===\nThe field of numerical analysis predates the invention of modern computers by many centuries. [[Linear interpolation]] was already in use more than 2000 years ago. Many great mathematicians of the past were preoccupied by numerical analysis, as is obvious from the names of important algorithms like [[Newton's method]], [[Lagrange polynomial|Lagrange interpolation polynomial]], [[Gaussian elimination]], or [[Euler's method]].\n\nTo facilitate computations by hand, large books were produced with formulas and tables of data such as interpolation points and function coefficients. Using these tables, often calculated out to 16 decimal places or more for some functions, one could look up values to plug into the formulas given and achieve very good numerical estimates of some functions. The canonical work in the field is the [[NIST]] publication edited by [[Abramowitz and Stegun]], a 1000-plus page book of a very large number of commonly used formulas and functions and their values at many points. The function values are no longer very useful when a computer is available, but the large listing of formulas can still be very handy.\n\nThe [[mechanical calculator]] was also developed as a tool for hand computation. These calculators evolved into electronic computers in the 1940s, and it was then found that these computers were also useful for administrative purposes. But the invention of the computer also influenced the field of numerical analysis, since now longer and more complicated calculations could be done.\n\n===Direct and iterative methods===\n\n{| class=\"wikitable\" style=\"float: right; width: 250px; margin-left: 1em;\"\n |-\n |\n'''Direct vs iterative methods'''\n\nConsider the problem of solving\n\n:3''x''<sup>3</sup> + 4 = 28\n\nfor the unknown quantity ''x''.\n\n{| style=\"margin:auto; text-align:right\"\n |+ Direct method\n |-\n | || 3''x''<sup>3</sup> + 4 = 28.\n |-\n | ''Subtract 4'' || 3''x''<sup>3</sup> = 24.\n |-\n | ''Divide by 3'' || ''x''<sup>3</sup> = &nbsp;8.\n |-\n | ''Take cube roots'' || ''x'' = &nbsp;2.\n |}\n\nFor the iterative method, apply the [[bisection method]] to ''f''(''x'') = 3''x''<sup>3</sup> &minus; 24. The initial values are ''a'' = 0, ''b'' = 3, ''f''(''a'') = &minus;24, ''f''(''b'') = 57.\n\n{| style=\"margin:auto;\" class=\"wikitable\"\n |+ Iterative method\n |-\n ! ''a'' !! ''b'' !! mid !! ''f''(mid)\n |-\n | 0 || 3 || 1.5 || &minus;13.875\n |-\n | 1.5 || 3 || 2.25 || 10.17...\n |-\n | 1.5 || 2.25 || 1.875 || &minus;4.22...\n |-\n | 1.875 || 2.25 || 2.0625 || 2.32...\n |}\n\nFrom this table it can be concluded that the solution is between 1.875 and 2.0625. The algorithm might return any number in that range with an error less than 0.2.\n\n==== Discretization and numerical integration ====\n[[Image:Schumacher (Ferrari) in practice at USGP 2005.jpg|right|125px]]\nIn a two-hour race, the speed of the car is measured at three instants and recorded in the following table.\n\n{| style=\"margin:auto;\" class=\"wikitable\"\n! Time\n| 0:20 || 1:00 || 1:40\n|-\n! km/h\n| 140 || 150 || 180\n|}\n\nA '''discretization''' would be to say that the speed of the car was constant from 0:00 to 0:40, then from 0:40 to 1:20 and finally from 1:20 to 2:00. For instance, the total distance traveled in the first 40 minutes is approximately {{math|size=100%|1=({{val|2|/|3|u=h}}&nbsp;&times;&nbsp;{{val|140|u=km/h}})&nbsp;=&nbsp;{{val|93.3|u=km}}}}. This would allow us to estimate the total distance traveled as {{val|93.3|u=km}} + {{val|100|u=km}} + {{val|120|u=km}} = {{val|313.3|u=km}}, which is an example of '''numerical integration''' (see below) using a [[Riemann sum]], because displacement is the [[integral]] of velocity.\n\nIll-conditioned problem: Take the function {{math|size=100%|1=''f''(''x'') = 1/(''x''&nbsp;&minus;&nbsp;1)}}. Note that ''f''(1.1) = 10 and ''f''(1.001) = 1000: a change in ''x'' of less than 0.1 turns into a change in ''f''(''x'') of nearly 1000. Evaluating ''f''(''x'') near ''x'' = 1 is an ill-conditioned problem.\n\nWell-conditioned problem: By contrast, evaluating the same function {{math|size=100%|1=''f''(''x'') = 1/(''x''&nbsp;&minus;&nbsp;1)}} near ''x'' = 10 is a well-conditioned problem. For instance, ''f''(10) = 1/9 ≈ 0.111 and ''f''(11) = 0.1: a modest change in ''x'' leads to a modest change in ''f''(''x'').\n|}\n\nDirect methods compute the solution to a problem in a finite number of steps. These methods would give the precise answer if they were performed in [[Arbitrary-precision arithmetic|infinite precision arithmetic]]. Examples include [[Gaussian elimination]], the [[QR decomposition|QR factorization]] method for solving [[system of linear equations|systems of linear equations]], and the [[simplex method]] of [[linear programming]]. In practice, [[Floating point|finite precision]] is used and the result is an approximation of the true solution (assuming [[Numerically stable|stability]]).\n\nIn contrast to direct methods, [[iterative method]]s are not expected to terminate in a finite number of steps. Starting from an initial guess, iterative methods form successive approximations that [[Limit of a sequence|converge]] to the exact solution only in the limit. A convergence test, often involving [[Residual (numerical analysis)|the residual]], is specified in order to decide when a sufficiently accurate solution has (hopefully) been found. Even using infinite precision arithmetic these methods would not reach the solution within a finite number of steps (in general). Examples include Newton's method, the [[bisection method]], and [[Jacobi iteration]]. In computational matrix algebra, iterative methods are generally needed for large problems.\n\nIterative methods are more common than direct methods in numerical analysis. Some methods are direct in principle but are usually used as though they were not, e.g. [[GMRES]] and the [[conjugate gradient method]]. For these methods the number of steps needed to obtain the exact solution is so large that an approximation is accepted in the same manner as for an iterative method.\n\n===Discretization===\nFurthermore, continuous problems must sometimes be replaced by a discrete problem whose solution is known to approximate that of the continuous problem; this process is called '[[discretization]]'. For example, the solution of a [[differential equation]] is a [[function (mathematics)|function]]. This function must be represented by a finite amount of data, for instance by its value at a finite number of points at its domain, even though this domain is a [[Continuum (set theory)|continuum]].\n\n==Generation and propagation of errors==\nThe study of errors forms an important part of numerical analysis. There are several ways in which error can be introduced in the solution of the problem.\n\n===Round-off===\n[[Round-off error]]s arise because it is impossible to represent all [[real number]]s exactly on a machine with finite memory (which is what all practical [[digital computer]]s are).\n\n===Truncation and discretization error===\n[[Truncation error]]s are committed when an iterative method is terminated or a mathematical procedure is approximated, and the approximate solution differs from the exact solution. Similarly, discretization induces a [[discretization error]] because the solution of the discrete problem does not coincide with the solution of the continuous problem. For instance, in the iteration in the sidebar to compute the solution of <math>3x^3+4=28</math>, after 10 or so iterations, it can be concluded that the root is roughly 1.99 (for example). Therefore there is a truncation error of 0.01.\n\nOnce an error is generated, it will generally propagate through the calculation. For instance, already noted is that the operation + on a calculator (or a computer) is inexact. It follows that a calculation of the type {{tmath|a+b+c+d+e}} is even more inexact.\n\nThe truncation error is created when a mathematical procedure is approximated. To integrate a function exactly it is required to find the sum of infinite trapezoids, but numerically only the sum of only finite trapezoids can be found, and hence the approximation of the mathematical procedure. Similarly, to differentiate a function, the differential element approaches zero but numerically only a finite value of the differential element can be chosen.\n\n===Numerical stability and well-posed problems===\n[[Numerical stability]] is a notion in numerical analysis. An algorithm is called 'numerically stable' if an error, whatever its cause, does not grow to be much larger during the calculation. This happens if the problem is '[[condition number|well-conditioned]]', meaning that the solution changes by only a small amount if the problem data are changed by a small amount. To the contrary, if a problem is 'ill-conditioned', then any small error in the data will grow to be a large error.\n\nBoth the original problem and the algorithm used to solve that problem can be 'well-conditioned' or 'ill-conditioned', and any combination is possible.\n\nSo an algorithm that solves a well-conditioned problem may be either numerically stable or numerically unstable. An art of numerical analysis is to find a stable algorithm for solving a well-posed mathematical problem. For instance, computing the square root of 2 (which is roughly 1.41421) is a well-posed problem. Many algorithms solve this problem by starting with an initial approximation ''x''<sub>0</sub> to <math>\\sqrt{2}</math>, for instance ''x''<sub>0</sub> = 1.4, and then computing improved guesses ''x''<sub>1</sub>, ''x''<sub>2</sub>, etc. One such method is the famous [[Babylonian method]], which is given by ''x''<sub>''k''+1</sub> = ''x<sub>k</sub>''/2 + 1/''x<sub>k</sub>''. Another method, called 'method X', is given by ''x''<sub>''k''+1</sub> = (''x''<sub>''k''</sub><sup>2</sup> − 2)<sup>2</sup> + ''x''<sub>''k''</sub>.<ref>This is a [[fixed point iteration]] for the equation <math>x=(x^2-2)^2+x=f(x)</math>, whose solutions include <math>\\sqrt{2}</math>. The iterates always move to the right since <math>f(x)\\geq x</math>. Hence <math>x_1=1.4<\\sqrt{2}</math> converges and <math>x_1=1.42>\\sqrt{2}</math> diverges.</ref> A few iterations of each scheme are calculated in table form below, with initial guesses ''x''<sub>0</sub> = 1.4 and ''x''<sub>0</sub> = 1.42.\n\n{| class=\"wikitable\"\n |-\n! Babylonian\n! Babylonian\n! Method X\n! Method X\n |-\n | ''x''<sub>0</sub> = 1.4\n | ''x''<sub>0</sub> = 1.42\n | ''x''<sub>0</sub> = 1.4\n | ''x''<sub>0</sub> = 1.42\n |-\n | ''x''<sub>1</sub> = 1.4142857...\n | ''x''<sub>1</sub> = 1.41422535...\n | ''x''<sub>1</sub> = 1.4016\n | ''x''<sub>1</sub> = 1.42026896\n |-\n | ''x''<sub>2</sub> = 1.414213564...\n | ''x''<sub>2</sub> = 1.41421356242...\n | ''x''<sub>2</sub> = 1.4028614...\n | ''x''<sub>2</sub> = 1.42056...\n |-\n |\n |\n | ...\n | ...\n |-\n |\n |\n | ''x''<sub>1000000</sub> = 1.41421...\n | ''x''<sub>27</sub> = 7280.2284...\n |}\n\nObserve that the Babylonian method converges quickly regardless of the initial guess, whereas Method X converges extremely slowly with initial guess ''x''<sub>0</sub> = 1.4 and diverges for initial guess ''x''<sub>0</sub> = 1.42. Hence, the Babylonian method is numerically stable, while Method X is numerically unstable.\n:'''Numerical stability''' is affected by the number of the significant digits the machine keeps on, if a machine is used that keeps only the four most significant decimal digits, a good example on loss of significance can be given by these two equivalent functions\n:<math> \nf(x)=x\\left(\\sqrt{x+1}-\\sqrt{x}\\right)\n\\text{ and } g(x)=\\frac{x}{\\sqrt{x+1}+\\sqrt{x}}.\n</math>\n:Comparing the results of\n:: <math> f(500)=500 \\left(\\sqrt{501}-\\sqrt{500} \\right)=500 \\left(22.38-22.36 \\right)=500(0.02)=10</math>\n:and\n:<math>\n\\begin{alignat}{3}g(500)&=\\frac{500}{\\sqrt{501}+\\sqrt{500}}\\\\\n      &=\\frac{500}{22.38+22.36}\\\\\n      &=\\frac{500}{44.74}=11.17\n\\end{alignat}\n</math>\n: by comparing the two results above, it is clear that [[loss of significance]] (caused here by 'catastrophic cancelation') has a huge effect on the results, even though both functions are equivalent, as shown below\n:: <math> \\begin{alignat}{4}\nf(x)&=x \\left(\\sqrt{x+1}-\\sqrt{x} \\right)\\\\\n    &=x \\left(\\sqrt{x+1}-\\sqrt{x} \\right)\\frac{\\sqrt{x+1}+\\sqrt{x}}{\\sqrt{x+1}+\\sqrt{x}}\\\\\n    &=x\\frac{(\\sqrt{x+1})^2-(\\sqrt{x})^2}{\\sqrt{x+1}+\\sqrt{x}}\\\\\n    &=x\\frac{x+1-x}{\\sqrt{x+1}+\\sqrt{x}} \\\\\n    &=x\\frac{1}{\\sqrt{x+1}+\\sqrt{x}} \\\\\n    &=\\frac {x}{\\sqrt{x+1}+\\sqrt{x}} \\\\\n    &=g(x)\n\\end{alignat}</math>\n\n:The desired value, computed using infinite precision, is 11.174755...\n*The example is a modification of one taken from Mathew; Numerical methods using Matlab, 3rd ed.\n\n==Areas of study==\nThe field of numerical analysis includes many sub-disciplines. Some of the major ones are:\n\n===Computing values of functions===\n{| class=\"wikitable\" style=\"float: right; width: 250px; clear: right; margin-left: 1em;\"\n |\nInterpolation: Observing that the temperature varies from 20 degrees Celsius at 1:00 to 14 degrees at 3:00, a linear interpolation of this data would conclude that it was 17 degrees at 2:00 and 18.5 degrees at 1:30pm.\n\nExtrapolation: If the [[gross domestic product]] of a country has been growing an average of 5% per year and was 100 billion last year, it might extrapolated that it will be 105 billion this year.\n\n[[Image:Linear-regression.svg|right|100px|A line through 20 points]]\n\nRegression: In linear regression, given ''n'' points, a line is computed that passes as close as possible to those ''n'' points.\n\n[[Image:LemonadeJuly2006.JPG|right|100px|How much for a glass of lemonade?]]\n\nOptimization: Say lemonade is sold at a [[lemonade stand]], at $1 197 glasses of lemonade can be sold per day, and that for each increase of $0.01, one glass of lemonade less wilbe sold per day. If $1.485 could be charged, profit would be maximized but due to the constraint of having to charge a whole cent amount, charging $1.48 or $1.49 per glass will both yield the maximum income of $220.52 per day.\n\n[[Image:Wind-particle.png|right|Wind direction in blue, true trajectory in black, Euler method in red.]]\n\nDifferential equation: If 100 fans are set up to blow air from one end of the room to the other and then a feather isdropped into the wind, what happens? The feather will follow the air currents, which may be very complex. One approximation is to measure the speed at which the air is blowing near the feather every second, and advance the simulated feather as if it were moving in a straight line at that same speed for one second, before measuring the wind speed again. This is called the [[Euler method]] for solving an ordinary differential equation.\n|}\n\nOne of the simplest problems is the evaluation of a function at a given point. The most straightforward approach, of just plugging in the number in the formula is sometimes not very efficient. For polynomials, a better approach is using the [[Horner scheme]], since it reduces the necessary number of multiplications and additions. Generally, it is important to estimate and control [[round-off error]]s arising from the use of [[floating point]] arithmetic.\n\n===Interpolation, extrapolation, and regression===\n[[Interpolation]] solves the following problem: given the value of some unknown function at a number of points, what value does that function have at some other point between the given points?\n\n[[Extrapolation]] is very similar to interpolation, except that now the value of the unknown function at a point which is outside the given points must be found.\n\n[[Regression analysis|Regression]] is also similar, but it takes into account that the data is imprecise. Given some points, and a measurement of the value of some function at these points (with an error), the unknown function can be found. The [[numerical methods for linear least squares|least squares]]-method is one way to achieve this.\n\n===Solving equations and systems of equations===\nAnother fundamental problem is computing the solution of some given equation. Two cases are commonly distinguished, depending on whether the equation is linear or not. For instance, the equation <math>2x+5=3</math> is linear while <math>2x^2+5=3</math> is not.\n\nMuch effort has been put in the development of methods for solving [[systems of linear equations]]. Standard direct methods, i.e., methods that use some [[matrix decomposition]] are [[Gaussian elimination]], [[LU decomposition]], [[Cholesky decomposition]] for [[symmetric matrix|symmetric]] (or [[hermitian matrix|hermitian]]) and [[positive-definite matrix]], and [[QR decomposition]] for non-square matrices. Iterative methods such as the [[Jacobi method]], [[Gauss–Seidel method]], [[successive over-relaxation]] and [[conjugate gradient method]] are usually preferred for large systems. General iterative methods can be developed using a [[matrix splitting]].\n\n[[Root-finding algorithm]]s are used to solve nonlinear equations (they are so named since a root of a function is an argument for which the function yields zero). If the function is [[derivative|differentiable]] and the derivative is known, then Newton's method is a popular choice. [[Linearization]] is another technique for solving nonlinear equations.\n\n===Solving eigenvalue or singular value problems===\nSeveral important problems can be phrased in terms of [[eigenvalue decomposition]]s or [[singular value decomposition]]s. For instance, the [[image compression|spectral image compression]] algorithm<ref>[http://online.redwoods.cc.ca.us/instruct/darnold/maw/single.htm The Singular Value Decomposition and Its Applications in Image Compression] {{webarchive |url=https://web.archive.org/web/20061004041704/http://online.redwoods.cc.ca.us/instruct/darnold/maw/single.htm |date=4 October 2006 }}</ref> is based on the singular value decomposition. The corresponding tool in statistics is called [[principal component analysis]].\n\n===Optimization===\n{{Main|Mathematical optimization}}\n\nOptimization problems ask for the point at which a given function is maximized (or minimized). Often, the point also has to satisfy some [[Constraint (mathematics)|constraint]]s.\n\nThe field of optimization is further split in several subfields, depending on the form of the objective function and the constraint. For instance, [[linear programming]] deals with the case that both the objective function and the constraints are linear. A famous method in linear programming is the simplex method.\n\nThe method of [[Lagrange multipliers]] can be used to reduce optimization problems with constraints to unconstrained optimization problems.\n\n===Evaluating integrals===\n{{Main|Numerical integration}}\n\nNumerical integration, in some instances also known as numerical [[quadrature (mathematics)|quadrature]], asks for the value of a definite [[integral]]. Popular methods use one of the [[Newton–Cotes formulas]] (like the midpoint rule or [[Simpson's rule]]) or [[Gaussian quadrature]]. These methods rely on a \"divide and conquer\" strategy, whereby an integral on a relatively large set is broken down into integrals on smaller sets. In higher dimensions, where these methods become prohibitively expensive in terms of computational effort, one may use [[Monte Carlo method|Monte Carlo]] or [[quasi-Monte Carlo method]]s (see [[Monte Carlo integration]]), or, in modestly large dimensions, the method of [[sparse grid]]s.\n\n===Differential equations===\n{{main|Numerical ordinary differential equations|Numerical partial differential equations}}\n\nNumerical analysis is also concerned with computing (in an approximate way) the solution of differential equations, both ordinary differential equations and partial differential equations.\n\nPartial differential equations are solved by first discretizing the equation, bringing it into a finite-dimensional subspace. This can be done by a [[finite element method]], a [[finite difference]] method, or (particularly in engineering) a [[finite volume method]]. The theoretical justification of these methods often involves theorems from [[functional analysis]]. This reduces the problem to the solution of an algebraic equation.\n\n==Software==\n{{main|List of numerical analysis software|Comparison of numerical analysis software}}\n\nSince the late twentieth century, most algorithms are implemented in a variety of programming languages. The [[Netlib]] repository contains various collections of software routines for numerical problems, mostly in [[Fortran]] and [[C (programming language)|C]]. Commercial products implementing many different numerical algorithms include the [[IMSL Numerical Libraries|IMSL]] and [[Numerical Algorithms Group|NAG]] libraries; a [[free software|free-software]] alternative is the [[GNU Scientific Library]].\n\nThere are several popular numerical computing applications such as [[MATLAB]], [[TK Solver]], [[S-PLUS]], and [[IDL (programming language)|IDL]] as well as free and open source alternatives such as [[FreeMat]], [[Scilab]], [[GNU Octave]] (similar to Matlab), and [[IT++]] (a C++ library). There are also programming languages such as [[R (programming language)|R]] (similar to S-PLUS) and [[Python (programming language)|Python]] with libraries such as [[NumPy]], [[SciPy]] and [[SymPy]]. Performance varies widely: while vector and matrix operations are usually fast, scalar loops may vary in speed by more than an order of magnitude.<ref>[http://www.sciviews.org/benchmark/ Speed comparison of various number crunching packages] {{webarchive |url=https://web.archive.org/web/20061005024002/http://www.sciviews.org/benchmark/ |date=5 October 2006 }}</ref><ref>[http://www.scientificweb.com/ncrunch/ncrunch5.pdf Comparison of mathematical programs for data analysis] Stefan Steinhaus, ScientificWeb.com</ref>\n\nMany [[computer algebra system]]s such as [[Mathematica]] also benefit from the availability of [[arbitrary-precision arithmetic]] which can provide more accurate results.\n\nAlso, any [[spreadsheet]] [[software]] can be used to solve simple problems relating to numerical analysis.\n\n==See also==\n*[[Analysis of algorithms]]\n*[[Computational science]]\n*[[Interval arithmetic]]\n*[[List of numerical analysis topics]]\n*[[Numerical differentiation]]\n*[[Numerical Recipes]]\n*[[Symbolic-numeric computation]]\n\n==Notes==\n<references/>\n\n==References==\n<!--\nThis template can be used for additional references.\n*{{cite book |last= |first= |authorlink= |coauthors= |title= |year= |publisher= |location= |id= }}\n-->\n*{{cite book|author=[[Gene H. Golub|Golub, Gene H.]] and [[Charles F. Van Loan]]|title=Matrix Computations|edition=third|publisher=Johns Hopkins University Press|ISBN=0-8018-5413-X|year=1986}}\n*{{cite book |first=Nicholas J.|last=Higham | authorlink=Nicholas Higham|title=Accuracy and Stability of Numerical Algorithms|publisher=Society for Industrial and Applied Mathematics|ISBN=0-89871-355-2|year=1996}}\n*{{cite book |last=Hildebrand |first=F. B. | authorlink=Francis B. Hildebrand | title=Introduction to Numerical Analysis | edition=2nd |year=1974 |publisher=McGraw-Hill |location= |isbn= 0-07-028761-9}}\n*{{cite book |last=Leader |first=Jeffery J. | authorlink=Jeffery J. Leader|title=Numerical Analysis and Scientific Computation |year=2004 |publisher=Addison Wesley |location= |isbn= 0-201-73499-0 }}\n*{{cite book|last= Wilkinson |first =J.H.| authorlink=James H. Wilkinson| title=The Algebraic Eigenvalue Problem (Clarendon Press)|year=1965}}\n*{{cite journal | author=Kahan, W. | authorlink= William Kahan| title= \"A survey of error-analysis,\" in Info. Processing 71 (Proc. IFIP Congress 71 in Ljubljana), vol. 2, pp. 1214–39, North-Holland Publishing, Amsterdam|year=1972}} (examples of the importance of accurate arithmetic).\n* [[Lloyd N. Trefethen|Trefethen, Lloyd N.]] (2006). [http://people.maths.ox.ac.uk/trefethen/NAessay.pdf \"Numerical analysis\"], 20 pages. In: Timothy Gowers and June Barrow-Green (editors), ''Princeton Companion of Mathematics'', Princeton University Press.\n\n== External links ==\n{{Sister project links| wikt=no | b=Numerical Methods | n=no | q=Numerical analysis | s=no | v=no | voy=no | species=no | d=no}}\n\n===Journals===\n*[http://www-gdz.sub.uni-goettingen.de/cgi-bin/digbib.cgi?PPN362160546 gdz.sub.uni-goettingen], ''Numerische Mathematik'', volumes 1-66, Springer, 1959-1994 (searchable; pages are images). {{en icon}} {{de icon}}\n*[http://www.springerlink.com/content/0029-599X springerlink.com], ''Numerische Mathematik'', volumes 1-112, Springer, 1959–2009\n*[http://siamdl.aip.org/dbt/dbt.jsp?KEY=SJNAAM SIAM Journal on Numerical Analysis], volumes 1-47, SIAM, 1964–2009\n\n===Online texts===\n*{{springer|title=Numerical analysis|id=p/n120130}}\n*[http://www.nr.com/oldverswitcher.html ''Numerical Recipes''], William H. Press (free, downloadable previous editions)\n*[https://web.archive.org/web/20120225082123/http://kr.cs.ait.ac.th/~radok/math/mat7/stepsa.htm ''First Steps in Numerical Analysis''] ([[Internet Archive|archived]]), R.J.Hosking, S.Joe, D.C.Joyce, and J.C.Turner\n*[http://www.phy.ornl.gov/csep/CSEP/TEXTOC.html ''CSEP'' (Computational Science Education Project)], [[U.S. Department of Energy]]\n\n===Online course material===\n*[http://www.damtp.cam.ac.uk/user/fdl/people/sd103/lectures/nummeth98/index.htm#L_1_Title_Page Numerical Methods], Stuart Dalziel [[University of Cambridge]]\n*[http://www.math.upenn.edu/~wilf/DeturckWilf.pdf Lectures on Numerical Analysis], Dennis Deturck and Herbert S. Wilf [[University of Pennsylvania]]\n*[http://johndfenton.com/Lectures/Numerical-Methods/Numerical-Methods.pdf Numerical methods], John D. Fenton [[University of Karlsruhe]]\n*[http://www-teaching.physics.ox.ac.uk/computing/NumericalMethods/NMfP.pdf Numerical Methods for Physicists], Anthony O’Hare [[Oxford University]]\n*[https://web.archive.org/web/20120225082123/http://kr.cs.ait.ac.th/~radok/math/mat7/stepsa.htm Lectures in Numerical Analysis] ([[Internet Archive|archived]]), R. Radok [[Mahidol University]]\n*[http://ocw.mit.edu/courses/mechanical-engineering/2-993j-introduction-to-numerical-analysis-for-engineering-13-002j-spring-2005/ Introduction to Numerical Analysis for Engineering], Henrik Schmidt [[Massachusetts Institute of Technology]]\n*[http://ece.uwaterloo.ca/~dwharder/NumericalAnalysis/ ''Numerical Analysis for Engineering''], D. W. Harder [[University of Waterloo]]\n\n{{Areas of mathematics}}\n{{Branches of physics}}\n{{Computer science}}\n\n{{Authority control}}\n\n{{DEFAULTSORT:Numerical Analysis}}\n[[Category:Numerical analysis| ]]\n[[Category:Mathematical physics]]\n[[Category:Computational science]]"
    },
    {
      "title": "List of numerical analysis topics",
      "url": "https://en.wikipedia.org/wiki/List_of_numerical_analysis_topics",
      "text": "This is a '''list of [[numerical analysis]] topics'''.\n\n==General==\n*[[Iterative method]]\n*[[Rate of convergence]] — the speed at which a convergent sequence approaches its limit\n**[[Order of accuracy]] — rate at which numerical solution of differential equation converges to exact solution\n*[[Series acceleration]] — methods to accelerate the speed of convergence of a series\n**[[Aitken's delta-squared process]] — most useful for linearly converging sequences\n**[[Minimum polynomial extrapolation]] — for vector sequences\n**[[Richardson extrapolation]]\n**[[Shanks transformation]] — similar to Aitken's delta-squared process, but applied to the partial sums\n**[[Van Wijngaarden transformation]] — for accelerating the convergence of an alternating series\n*[[Abramowitz and Stegun]] — book containing formulas and tables of many special functions\n**[[Digital Library of Mathematical Functions]] — successor of book by Abramowitz and Stegun\n*[[Curse of dimensionality]]\n*[[Local convergence]] and global convergence — whether you need a good initial guess to get convergence\n*[[Superconvergence]]\n*[[Discretization]]\n*[[Difference quotient]]\n*Complexity:\n**[[Computational complexity of mathematical operations]]\n**[[Smoothed analysis]] — measuring the expected performance of algorithms under slight random perturbations of worst-case inputs\n*[[Symbolic-numeric computation]] — combination of symbolic and numeric methods\n*Cultural and historical aspects:\n**[[History of numerical solution of differential equations using computers]]\n**[[Hundred-dollar, Hundred-digit Challenge problems]] — list of ten problems proposed by [[Nick Trefethen]] in 2002\n**[[International Workshops on Lattice QCD and Numerical Analysis]]\n**[[Timeline of numerical analysis after 1945]]\n*General classes of methods:\n**[[Collocation method]] — discretizes a continuous equation by requiring it only to hold at certain points\n**[[Level-set method]]\n***[[Level set (data structures)]] — data structures for representing level sets\n**[[Sinc numerical methods]] — methods based on the sinc function, sinc(''x'') = sin(''x'') / ''x''\n**[[ABS methods]]\n\n==Error==\n[[Error analysis (mathematics)]]\n*[[Approximation]]\n*[[Approximation error]]\n*[[Condition number]]\n*[[Discretization error]]\n*[[Floating point]] number\n**[[Guard digit]] — extra precision introduced during a computation to reduce round-off error\n**[[Truncation]] — rounding a floating-point number by discarding all digits after a certain digit\n**[[Round-off error]]\n***[[Numeric precision in Microsoft Excel]]\n**[[Arbitrary-precision arithmetic]]\n*[[Interval arithmetic]] — represent every number by two floating-point numbers guaranteed to have the unknown number between them\n**[[Interval contractor]] — maps interval to subinterval which still contains the unknown exact answer\n**[[Interval propagation]] — contracting interval domains without removing any value consistent with the constraints\n***See also: [[Interval boundary element method]], [[Interval finite element]]\n*[[Loss of significance]]\n*[[Numerical error]]\n*[[Numerical stability]]\n*Error propagation:\n**[[Propagation of uncertainty]]\n***[[List of uncertainty propagation software]]\n**[[Significance arithmetic]]\n** [[Residual (numerical analysis)]]\n*[[Relative change and difference]] — the relative difference between ''x'' and ''y'' is |''x'' − ''y''| / max(|''x''|, |''y''|)\n*[[Significant figures]]\n**[[False precision]] — giving more significant figures than appropriate\n*[[Truncation error]] — error committed by doing only a finite numbers of steps\n*[[Well-posed problem]]\n*[[Affine arithmetic]]\n\n==Elementary and special functions==\n* [[Unrestricted algorithm]]\n*Summation:\n**[[Kahan summation algorithm]]\n**[[Pairwise summation]] — slightly worse than Kahan summation but cheaper\n**[[Binary splitting]]\n*Multiplication:\n**[[Multiplication algorithm]] — general discussion, simple methods\n**[[Karatsuba algorithm]] — the first algorithm which is faster than straightforward multiplication\n**[[Toom–Cook multiplication]] — generalization of Karatsuba multiplication\n**[[Schönhage–Strassen algorithm]] — based on Fourier transform, asymptotically very fast\n**[[Fürer's algorithm]] — asymptotically slightly faster than Schönhage–Strassen\n*[[Division algorithm]] — for computing quotient and/or remainder of two numbers\n**[[Long division]]\n**[[Restoring division]]\n**[[Non-restoring division]]\n**[[SRT division]]\n**[[Newton–Raphson division]]: uses [[Newton's method]] to find the [[Multiplicative inverse|reciprocal]] of D, and multiply that reciprocal by N to find the final quotient Q.\n**[[Goldschmidt division]]\n*Exponentiation:\n**[[Exponentiation by squaring]]\n**[[Addition-chain exponentiation]]\n* [[Multiplicative inverse#Algorithms|Multiplicative inverse Algorithms]]: for computing a number's multiplicative inverse (reciprocal).\n**[[Newton's_method#Multiplicative_inverses_of_numbers_and_power_series|Newton's method]]\n*Polynomials:\n**[[Horner's method]]\n**[[Estrin's scheme]] — modification of the Horner scheme with more possibilities for parallelization\n**[[Clenshaw algorithm]]\n**[[De Casteljau's algorithm]]\n*Square roots and other roots:\n**[[Integer square root]]\n**[[Methods of computing square roots]]\n**[[Nth root algorithm|''n''th root algorithm]]\n**[[Shifting nth root algorithm|Shifting ''n''th root algorithm]] — similar to long division\n**[[hypot]] — the function (''x''<sup>2</sup> + ''y''<sup>2</sup>)<sup>1/2</sup>\n**[[Alpha max plus beta min algorithm]] — approximates hypot(x,y)\n**[[Fast inverse square root]] — calculates 1 / {{radic|''x''}} using details of the IEEE floating-point system\n*Elementary functions (exponential, logarithm, trigonometric functions):\n**[[Trigonometric tables]] — different methods for generating them\n**[[CORDIC]] — shift-and-add algorithm using a table of arc tangents\n**[[BKM algorithm]] — shift-and-add algorithm using a table of logarithms and complex numbers\n*Gamma function:\n**[[Lanczos approximation]]\n**[[Spouge's approximation]] — modification of Stirling's approximation; easier to apply than Lanczos\n*[[AGM method]] — computes arithmetic–geometric mean; related methods compute special functions\n*[[FEE method]] (Fast E-function Evaluation) — fast summation of series like the power series for e<sup>''x''</sup>\n*[[Gal's accurate tables]] — table of function values with unequal spacing to reduce round-off error\n*[[Spigot algorithm]] — algorithms that can compute individual digits of a real number\n*[[Approximations of π|Approximations of {{pi}}]]:\n**[[Liu Hui's π algorithm]] — first algorithm that can compute π to arbitrary precision\n**[[Leibniz formula for π]] — alternating series with very slow convergence\n**[[Wallis product]] — infinite product converging slowly to π/2\n**[[Viète's formula]] — more complicated infinite product which converges faster\n**[[Gauss–Legendre algorithm]] — iteration which converges quadratically to π, based on arithmetic–geometric mean\n**[[Borwein's algorithm]] — iteration which converges quartically to 1/π, and other algorithms\n**[[Chudnovsky algorithm]] — fast algorithm that calculates a hypergeometric series\n**[[Bailey–Borwein–Plouffe formula]] — can be used to compute individual hexadecimal digits of π\n**[[Bellard's formula]] — faster version of Bailey–Borwein–Plouffe formula\n**[[List of formulae involving π]]\n\n==Numerical linear algebra==\n[[Numerical linear algebra]] — study of numerical algorithms for linear algebra problems\n\n===Basic concepts===\n*Types of matrices appearing in numerical analysis:\n**[[Sparse matrix]]\n***[[Band matrix]]\n***[[Bidiagonal matrix]]\n***[[Tridiagonal matrix]]\n***[[Pentadiagonal matrix]]\n***[[Skyline matrix]]\n**[[Circulant matrix]]\n**[[Triangular matrix]]\n**[[Diagonally dominant matrix]]\n**[[Block matrix]] — matrix composed of smaller matrices\n**[[Stieltjes matrix]] — symmetric positive definite with non-positive off-diagonal entries\n**[[Hilbert matrix]] — example of a matrix which is extremely ill-conditioned (and thus difficult to handle)\n**[[Wilkinson matrix]] — example of a symmetric tridiagonal matrix with pairs of nearly, but not exactly, equal eigenvalues\n**[[Convergent matrix]] – square matrix whose successive powers approach the zero matrix\n*Algorithms for matrix multiplication:\n**[[Strassen algorithm]]\n**[[Coppersmith–Winograd algorithm]]\n**[[Cannon's algorithm]] — a distributed algorithm, especially suitable for processors laid out in a 2d grid\n**[[Freivalds' algorithm]] — a randomized algorithm for checking the result of a multiplication\n*[[Matrix decomposition]]s:\n**[[LU decomposition]] — lower triangular times upper triangular\n**[[QR decomposition]] — orthogonal matrix times triangular matrix\n***[[RRQR factorization]] — rank-revealing QR factorization, can be used to compute rank of a matrix \n**[[Polar decomposition]] — unitary matrix times positive-semidefinite Hermitian matrix\n**Decompositions by similarity:\n***[[Eigendecomposition of a matrix|Eigendecomposition]] — decomposition in terms of eigenvectors and eigenvalues\n***[[Jordan normal form]] — bidiagonal matrix of a certain form; generalizes the eigendecomposition\n****[[Weyr canonical form]] — permutation of Jordan normal form\n***[[Jordan–Chevalley decomposition]] — sum of commuting nilpotent matrix and diagonalizable matrix\n***[[Schur decomposition]] — similarity transform bringing the matrix to a triangular matrix\n**[[Singular value decomposition]] — unitary matrix times diagonal matrix times unitary matrix\n*[[Matrix splitting]] – expressing a given matrix as a sum or difference of matrices\n\n===Solving systems of linear equations===\n*[[Gaussian elimination]]\n**[[Row echelon form]] — matrix in which all entries below a nonzero entry are zero\n**[[Bareiss algorithm]] — variant which ensures that all entries remain integers if the initial matrix has integer entries\n**[[Tridiagonal matrix algorithm]] — simplified form of Gaussian elimination for tridiagonal matrices\n*[[LU decomposition]] — write a matrix as a product of an upper- and a lower-triangular matrix\n**[[Crout matrix decomposition]]\n**[[LU reduction]] — a special parallelized version of a LU decomposition algorithm\n*[[Block LU decomposition]]\n*[[Cholesky decomposition]] — for solving a system with a positive definite matrix\n**[[Minimum degree algorithm]]\n**[[Symbolic Cholesky decomposition]]\n*[[Iterative refinement]] — procedure to turn an inaccurate solution in a more accurate one\n*Direct methods for sparse matrices:\n**[[Frontal solver]] — used in finite element methods\n**[[Nested dissection]] — for symmetric matrices, based on graph partitioning\n*[[Levinson recursion]] — for Toeplitz matrices\n*[[SPIKE algorithm]] — hybrid parallel solver for narrow-banded matrices\n*[[Cyclic reduction]] — eliminate even or odd rows or columns, repeat\n*Iterative methods:\n**[[Jacobi method]]\n**[[Gauss–Seidel method]]\n***[[Successive over-relaxation]] (SOR) — a technique to accelerate the Gauss–Seidel method\n****[[Symmetric successive overrelaxation]] (SSOR) — variant of SOR for symmetric matrices\n***[[Backfitting algorithm]] — iterative procedure used to fit a generalized additive model, often equivalent to Gauss–Seidel\n**[[Modified Richardson iteration]]\n**[[Conjugate gradient method]] (CG) — assumes that the matrix is positive definite\n***[[Derivation of the conjugate gradient method]]\n***[[Nonlinear conjugate gradient method]] — generalization for nonlinear optimization problems\n**[[Biconjugate gradient method]] (BiCG)\n***[[Biconjugate gradient stabilized method]] (BiCGSTAB) — variant of BiCG with better convergence \n**[[Conjugate residual method]] — similar to CG but only assumed that the matrix is symmetric\n**[[Generalized minimal residual method]] (GMRES) — based on the Arnoldi iteration\n**[[Chebyshev iteration]] — avoids inner products but needs bounds on the spectrum\n**[[Stone method|Stone's method]] (SIP – Srongly Implicit Procedure) — uses an incomplete LU decomposition\n**[[Kaczmarz method]]\n**[[Preconditioner]]\n***[[Incomplete Cholesky factorization]] — sparse approximation to the Cholesky factorization\n***[[Incomplete LU factorization]] — sparse approximation to the LU factorization\n**[[Uzawa iteration]] — for saddle node problems\n*Underdetermined and overdetermined systems (systems that have no or more than one solution):\n**[[Kernel (matrix)#Numerical computation of null space|Numerical computation of null space]] — find all solutions of an underdetermined system\n**[[Moore–Penrose pseudoinverse]] — for finding solution with smallest 2-norm (for underdetermined systems) or smallest residual\n**[[Sparse approximation]] — for finding the sparsest solution (i.e., the solution with as many zeros as possible)\n\n===Eigenvalue algorithms===\n[[Eigenvalue algorithm]] — a numerical algorithm for locating the eigenvalues of a matrix\n*[[Power iteration]]\n*[[Inverse iteration]]\n*[[Rayleigh quotient iteration]]\n*[[Arnoldi iteration]] — based on Krylov subspaces\n*[[Lanczos algorithm]] — Arnoldi, specialized for positive-definite matrices\n**[[Block Lanczos algorithm]] — for when matrix is over a finite field\n*[[QR algorithm]]\n*[[Jacobi eigenvalue algorithm]] — select a small submatrix which can be diagonalized exactly, and repeat\n**[[Jacobi rotation]] — the building block, almost a Givens rotation\n**[[Jacobi method for complex Hermitian matrices]]\n*[[Divide-and-conquer eigenvalue algorithm]]\n*[[Folded spectrum method]]\n*[[LOBPCG]] — Locally Optimal Block Preconditioned Conjugate Gradient Method\n*[[Eigenvalue perturbation]] — stability of eigenvalues under perturbations of the matrix\n\n===Other concepts and algorithms===\n*[[Orthogonalization]] algorithms:\n**[[Gram–Schmidt process]]\n**[[Householder transformation]]\n***[[Householder operator]] — analogue of Householder transformation for general inner product spaces\n**[[Givens rotation]]\n*[[Krylov subspace]]\n*[[Block matrix pseudoinverse]]\n*[[Bidiagonalization]]\n*[[Cuthill–McKee algorithm]] — permutes rows/columns in sparse matrix to yield a narrow band matrix\n*[[In-place matrix transposition]] — computing the transpose of a matrix without using much additional storage\n*[[Pivot element]] — entry in a matrix on which the algorithm concentrates\n*[[Matrix-free methods]] — methods that only access the matrix by evaluating matrix-vector products\n\n==Interpolation and approximation==\n[[Interpolation]] — construct a function going through some given data points\n*[[Nearest-neighbor interpolation]] — takes the value of the nearest neighbor\n\n===Polynomial interpolation===\n[[Polynomial interpolation]] — interpolation by polynomials\n*[[Linear interpolation]]\n*[[Runge's phenomenon]]\n*[[Vandermonde matrix]]\n*[[Chebyshev polynomials]]\n*[[Chebyshev nodes]]\n*[[Lebesgue constant (interpolation)]]\n*Different forms for the interpolant:\n**[[Newton polynomial]]\n***[[Divided differences]]\n***[[Neville's algorithm]] — for evaluating the interpolant; based on the Newton form\n**[[Lagrange polynomial]]\n**[[Bernstein polynomial]] — especially useful for approximation\n**[[Brahmagupta's interpolation formula]] — seventh-century formula for quadratic interpolation\n*Extensions to multiple dimensions:\n**[[Bilinear interpolation]]\n**[[Trilinear interpolation]]\n**[[Bicubic interpolation]]\n**[[Tricubic interpolation]]\n**[[Padua points]] — set of points in '''R'''<sup>2</sup> with unique polynomial interpolant and minimal growth of Lebesgue constant\n*[[Hermite interpolation]]\n*[[Birkhoff interpolation]]\n*[[Abel–Goncharov interpolation]]\n\n===Spline interpolation===\n[[Spline interpolation]] — interpolation by piecewise polynomials\n*[[Spline (mathematics)]] — the piecewise polynomials used as interpolants\n*[[Perfect spline]] — polynomial spline of degree ''m'' whose ''m''th derivate is &plusmn;1\n*[[Cubic Hermite spline]]\n**[[Centripetal Catmull–Rom spline]] — special case of cubic Hermite splines without self-intersections or cusps\n*[[Monotone cubic interpolation]]\n*[[Hermite spline]]\n*[[Bézier curve]]\n**[[De Casteljau's algorithm]]\n**[[composite Bézier curve]]\n**Generalizations to more dimensions:\n***[[Bézier triangle]] — maps a triangle to '''R'''<sup>3</sup>\n***[[Bézier surface]] — maps a square to '''R'''<sup>3</sup>\n*[[B-spline]]\n**[[Box spline]] — multivariate generalization of B-splines\n**[[Truncated power function]]\n**[[De Boor's algorithm]] — generalizes De Casteljau's algorithm\n*[[Non-uniform rational B-spline]] (NURBS)\n**[[T-spline]] — can be thought of as a NURBS surface for which a row of control points is allowed to terminate\n*[[Kochanek–Bartels spline]]\n*[[Coons patch]] — type of manifold parametrization used to smoothly join other surfaces together\n*[[M-spline]] — a non-negative spline\n*[[I-spline]] — a monotone spline, defined in terms of M-splines\n*[[Smoothing spline]] — a spline fitted smoothly to noisy data\n*[[Blossom (functional)]] — a unique, affine, symmetric map associated to a polynomial or spline\n*See also: [[List of numerical computational geometry topics]]\n\n===Trigonometric interpolation===\n[[Trigonometric interpolation]] — interpolation by trigonometric polynomials\n*[[Discrete Fourier transform]] — can be viewed as trigonometric interpolation at equidistant points\n**[[Relations between Fourier transforms and Fourier series]]\n*[[Fast Fourier transform]] (FFT) — a fast method for computing the discrete Fourier transform\n**[[Bluestein's FFT algorithm]]\n**[[Bruun's FFT algorithm]]\n**[[Cooley–Tukey FFT algorithm]]\n**[[Split-radix FFT algorithm]] — variant of Cooley–Tukey that uses a blend of radices 2 and 4\n**[[Goertzel algorithm]]\n**[[Prime-factor FFT algorithm]]\n**[[Rader's FFT algorithm]]\n**[[Bit-reversal permutation]] — particular permutation of vectors with 2<sup>''m''</sup> entries used in many FFTs.\n**[[Butterfly diagram]]\n**[[Twiddle factor]] — the trigonometric constant coefficients that are multiplied by the data\n**[[Cyclotomic fast Fourier transform]] — for FFT over finite fields\n**Methods for computing discrete convolutions with finite impulse response filters using the FFT:\n***[[Overlap–add method]]\n***[[Overlap–save method]]\n*[[Sigma approximation]]\n*[[Dirichlet kernel]] — convolving any function with the Dirichlet kernel yields its trigonometric interpolant\n*[[Gibbs phenomenon]]\n\n===Other interpolants===\n*[[Simple rational approximation]]\n**[[Polynomial and rational function modeling]] — comparison of polynomial and rational interpolation\n*[[Wavelet]]\n**[[Continuous wavelet]]\n**[[Transfer matrix]]\n**See also: [[List of functional analysis topics]], [[List of wavelet-related transforms]]\n*[[Inverse distance weighting]]\n*[[Radial basis function]] (RBF) — a function of the form ƒ(''x'') = ''φ''(|''x''−''x''<sub>0</sub>|)\n**[[Polyharmonic spline]] — a commonly used radial basis function\n**[[Thin plate spline]] — a specific polyharmonic spline: ''r''<sup>2</sup> log ''r''\n**[[Hierarchical RBF]]\n*[[Subdivision surface]] — constructed by recursively subdividing a piecewise linear interpolant\n**[[Catmull–Clark subdivision surface]]\n**[[Doo–Sabin subdivision surface]]\n**[[Loop subdivision surface]]\n*[[Slerp]] (spherical linear interpolation) — interpolation between two points on a sphere \n**Generalized quaternion interpolation — generalizes slerp for interpolation between more than two quaternions\n*[[Irrational base discrete weighted transform]]\n*[[Nevanlinna–Pick interpolation]] — interpolation by analytic functions in the unit disc subject to a bound\n**[[Pick matrix]] — the Nevanlinna–Pick interpolation has a solution if this matrix is positive semi-definite\n*[[Multivariate interpolation]] — the function being interpolated depends on more than one variable\n**[[Barnes interpolation]] — method for two-dimensional functions using Gaussians common in meteorology\n**[[Coons surface]] — combination of linear interpolation and bilinear interpolation\n**[[Lanczos resampling]] — based on convolution with a sinc function\n**[[Natural neighbor interpolation]]\n**[[Nearest neighbor value interpolation]]\n**[[PDE surface]]\n**[[Transfinite interpolation]] — constructs function on planar domain given its values on the boundary\n**[[Trend surface analysis]] — based on low-order polynomials of spatial coordinates; uses scattered observations\n**Method based on polynomials are listed under ''Polynomial interpolation''\n\n===Approximation theory===\n[[Approximation theory]]\n*[[Orders of approximation]]\n*[[Lebesgue's lemma]]\n*[[Curve fitting]]\n**[[Vector field reconstruction]]\n*[[Modulus of continuity]] — measures smoothness of a function\n*[[Least squares (function approximation)]] — minimizes the error in the L<sup>2</sup>-norm\n*[[Minimax approximation algorithm]] — minimizes the maximum error over an interval (the L<sup>∞</sup>-norm)\n**[[Equioscillation theorem]] — characterizes the best approximation in the L<sup>∞</sup>-norm\n*[[Unisolvent point set]] — function from given function space is determined uniquely by values on such a set of points\n*[[Stone–Weierstrass theorem]] — continuous functions can be approximated uniformly by polynomials, or certain other function spaces\n*Approximation by polynomials:\n**[[Linear approximation]]\n**[[Bernstein polynomial]] — basis of polynomials useful for approximating a function\n**[[Bernstein's constant]] — error when approximating |''x''| by a polynomial\n**[[Remez algorithm]] — for constructing the best polynomial approximation in the L<sup>∞</sup>-norm\n**[[Bernstein's inequality (mathematical analysis)]] — bound on maximum of derivative of polynomial in unit disk\n**[[Mergelyan's theorem]] — generalization of Stone–Weierstrass theorem for polynomials\n**[[Müntz–Szász theorem]] — variant of Stone–Weierstrass theorem for polynomials if some coefficients have to be zero\n**[[Bramble–Hilbert lemma]] — upper bound on L<sup>p</sup> error of polynomial approximation in multiple dimensions\n**[[Discrete Chebyshev polynomials]] — polynomials orthogonal with respect to a discrete measure\n**[[Favard's theorem]] — polynomials satisfying suitable 3-term recurrence relations are orthogonal polynomials\n*Approximation by Fourier series / trigonometric polynomials:\n**[[Jackson's inequality]] — upper bound for best approximation by a trigonometric polynomial\n***[[Bernstein's theorem (approximation theory)]] — a converse to Jackson's inequality\n**[[Fejér's theorem]] — Cesàro means of partial sums of Fourier series converge uniformly for continuous periodic functions\n**[[Erdős–Turán inequality]] — bounds distance between probability and Lebesgue measure in terms of Fourier coefficients\n*Different approximations:\n**[[Moving least squares]]\n**[[Padé approximant]]\n***[[Padé table]] — table of Padé approximants\n**[[Hartogs–Rosenthal theorem]] — continuous functions can be approximated uniformly by rational functions on a set of Lebesgue measure zero\n**[[Szász–Mirakyan operator]] — approximation by e<sup>&minus;''n''</sup> ''x''<sup>''k''</sup> on a semi-infinite interval\n**[[Szász–Mirakjan–Kantorovich operator]]\n**[[Baskakov operator]] — generalize Bernstein polynomials, Szász–Mirakyan operators, and Lupas operators\n**[[Favard operator]] — approximation by sums of Gaussians\n*[[Surrogate model]] — application: replacing a function that is hard to evaluate by a simpler function\n*[[Constructive function theory]] — field that studies connection between degree of approximation and smoothness\n*[[Universal differential equation]] — differential–algebraic equation whose solutions can approximate any continuous function\n*[[Fekete problem]] — find ''N'' points on a sphere that minimize some kind of energy\n*[[Carleman's condition]] — condition guaranteeing that a measure is uniquely determined by its moments\n*[[Krein's condition]] — condition that exponential sums are dense in weighted L<sup>2</sup> space\n*[[Lethargy theorem]] — about distance of points in a metric space from members of a sequence of subspaces\n*[[Wirtinger's representation and projection theorem]]\n*Journals:\n**[[Constructive Approximation]]\n**[[Journal of Approximation Theory]]\n\n===Miscellaneous===\n*[[Extrapolation]]\n**[[Linear predictive analysis]] — linear extrapolation\n*[[Unisolvent functions]] — functions for which the interpolation problem has a unique solution\n*[[Regression analysis]]\n**[[Isotonic regression]]\n*[[Curve-fitting compaction]]\n*[[Interpolation (computer graphics)]]\n\n==Finding roots of nonlinear equations==\n:''See [[#Numerical linear algebra]] for linear equations''\n\n[[Root-finding algorithm]] — algorithms for solving the equation ''f''(''x'') = 0\n*General methods:\n**[[Bisection method]] — simple and robust; linear convergence\n***[[Lehmer–Schur algorithm]] — variant for complex functions\n**[[Fixed-point iteration]]\n**[[Newton's method]] — based on linear approximation around the current iterate; quadratic convergence\n***[[Kantorovich theorem]] — gives a region around solution such that Newton's method converges\n***[[Newton fractal]] — indicates which initial condition converges to which root under Newton iteration\n***[[Quasi-Newton method]] — uses an approximation of the Jacobian:\n****[[Broyden's method]] — uses a rank-one update for the Jacobian\n****[[Symmetric rank-one]] — a symmetric (but not necessarily positive definite) rank-one update of the Jacobian\n****[[Davidon–Fletcher–Powell formula]] — update of the Jacobian in which the matrix remains positive definite\n****[[Broyden–Fletcher–Goldfarb–Shanno algorithm]] — rank-two update of the Jacobian in which the matrix remains positive definite\n****[[Limited-memory BFGS]] method — truncated, matrix-free variant of BFGS method suitable for large problems\n***[[Steffensen's method]] — uses divided differences instead of the derivative\n**[[Secant method]] — based on linear interpolation at last two iterates\n**[[False position method]] — secant method with ideas from the bisection method\n**[[Muller's method]] — based on quadratic interpolation at last three iterates\n**[[Sidi's generalized secant method]] — higher-order variants of secant method\n**[[Inverse quadratic interpolation]] — similar to Muller's method, but interpolates the inverse\n**[[Brent's method]] — combines bisection method, secant method and inverse quadratic interpolation\n**[[Ridders' method]] — fits a linear function times an exponential to last two iterates and their midpoint\n**[[Halley's method]] — uses ''f'', ''f''<nowiki>'</nowiki> and ''f''<nowiki>''</nowiki>; achieves the cubic convergence\n**[[Householder's method]] — uses first ''d'' derivatives to achieve order ''d'' + 1; generalizes Newton's and Halley's method\n*Methods for polynomials:\n**[[Aberth method]]\n**[[Bairstow's method]]\n**[[Durand–Kerner method]]\n**[[Graeffe's method]]\n**[[Jenkins–Traub algorithm]] — fast, reliable, and widely used\n**[[Laguerre's method]]\n**[[Splitting circle method]]\n*Analysis:\n**[[Wilkinson's polynomial]]\n*[[Numerical continuation]] — tracking a root as one parameter in the equation changes\n**[[Piecewise linear continuation]]\n\n==Optimization==\n[[Mathematical optimization]] — algorithm for finding maxima or minima of a given function\n\n===Basic concepts===\n*[[Active set]]\n*[[Candidate solution]]\n*[[Constraint (mathematics)]]\n**[[Constrained optimization]] — studies optimization problems with constraints\n**[[Binary constraint]] — a constraint that involves exactly two variables\n*[[Corner solution]]\n*[[Feasible region]] — contains all solutions that satisfy the constraints but may not be optimal\n*[[Global optimum]] and [[Local optimum]]\n*[[Maxima and minima]]\n*[[Slack variable]]\n*[[Continuous optimization]]\n*[[Discrete optimization]]\n\n===Linear programming===\n[[Linear programming]] (also treats ''integer programming'') — objective function and constraints are linear\n* Algorithms for linear programming:\n**[[Simplex algorithm]]\n***[[Bland's rule]] — rule to avoid cycling in the simplex method\n***[[Klee–Minty cube]] — perturbed (hyper)cube; simplex method has exponential complexity on such a domain\n***[[Criss-cross algorithm]] — similar to the simplex algorithm\n***[[Big M method]] — variation of simplex algorithm for problems with both \"less than\" and \"greater than\" constraints\n**[[Interior point method]]\n***[[Ellipsoid method]]\n***[[Karmarkar's algorithm]]\n***[[Mehrotra predictor–corrector method]]\n**[[Column generation]]\n**[[k-approximation of k-hitting set]] — algorithm for specific LP problems (to find a weighted hitting set)\n*[[Linear complementarity problem]]\n*Decompositions:\n**[[Benders' decomposition]]\n**[[Dantzig–Wolfe decomposition]]\n**[[Theory of two-level planning]]\n**[[Variable splitting]]\n*[[Basic solution (linear programming)]] — solution at vertex of feasible region\n*[[Fourier–Motzkin elimination]]\n*[[Hilbert basis (linear programming)]] — set of integer vectors in a convex cone which generate all integer vectors in the cone\n*[[LP-type problem]]\n*[[Linear inequality]]\n*[[Vertex enumeration problem]] — list all vertices of the feasible set\n\n===Convex optimization===\n[[Convex optimization]]\n*[[Quadratic programming]]\n**[[Linear least squares (mathematics)]]\n**[[Total least squares]]\n**[[Frank–Wolfe algorithm]]\n**[[Sequential minimal optimization]] — breaks up large QP problems into a series of smallest possible QP problems\n**[[Bilinear program]]\n*[[Basis pursuit]] — minimize L<sub>1</sub>-norm of vector subject to linear constraints\n**[[Basis pursuit denoising]] (BPDN) — regularized version of basis pursuit\n***[[In-crowd algorithm]] — algorithm for solving basis pursuit denoising\n*[[Linear matrix inequality]]\n*[[Conic optimization]]\n**[[Semidefinite programming]]\n**[[Second-order cone programming]]\n**[[Sum-of-squares optimization]]\n**Quadratic programming (see above)\n*[[Bregman method]] — row-action method for strictly convex optimization problems\n*[[Proximal gradient method]] — use splitting of objective function in sum of possible non-differentiable pieces\n*[[Subgradient method]] — extension of steepest descent for problems with a non-differentiable objective function\n*[[Biconvex optimization]] — generalization where objective function and constraint set can be biconvex\n\n===Nonlinear programming===\n[[Nonlinear programming]] — the most general optimization problem in the usual framework\n*Special cases of nonlinear programming:\n**See ''Linear programming'' and ''Convex optimization'' above\n**[[Geometric programming]] — problems involving signomials or posynomials\n***[[Signomial]] — similar to polynomials, but exponents need not be integers\n***[[Posynomial]] — a signomial with positive coefficients\n**[[Quadratically constrained quadratic program]]\n**[[Linear-fractional programming]] — objective is ratio of linear functions, constraints are linear\n***[[Fractional programming]] — objective is ratio of nonlinear functions, constraints are linear\n**[[Nonlinear complementarity problem]] (NCP) — find ''x'' such that ''x'' &ge; 0, ''f''(''x'') &ge; 0 and ''x''<sup>T</sup> ''f''(''x'') = 0\n**[[Least squares]] — the objective function is a sum of squares\n***[[Non-linear least squares]]\n***[[Gauss–Newton algorithm]]\n****[[BHHH algorithm]] — variant of Gauss–Newton in econometrics\n****[[Generalized Gauss–Newton method]] — for constrained nonlinear least-squares problems\n***[[Levenberg–Marquardt algorithm]]\n***[[Iteratively reweighted least squares]] (IRLS) — solves a weigted least-squares problem at every iteration\n***[[Partial least squares]] — statistical techniques similar to principal components analysis\n****[[Non-linear iterative partial least squares]] (NIPLS)\n**[[Mathematical programming with equilibrium constraints]] — constraints include variational inequalities or complementarities\n**Univariate optimization:\n***[[Golden section search]]\n***[[Successive parabolic interpolation]] — based on quadratic interpolation through the last three iterates\n*General algorithms:\n**Concepts:\n***[[Descent direction]]\n***[[Guess value]] — the initial guess for a solution with which an algorithm starts\n***[[Line search]]\n****[[Backtracking line search]]\n****[[Wolfe conditions]]\n**[[Gradient method]] — method that uses the gradient as the search direction\n***[[Gradient descent]]\n****[[Stochastic gradient descent]]\n***[[Landweber iteration]] — mainly used for ill-posed problems\n**[[Successive linear programming]] (SLP) — replace problem by a linear programming problem, solve that, and repeat\n**[[Sequential quadratic programming]] (SQP) — replace problem by a quadratic programming problem, solve that, and repeat\n**[[Newton's method in optimization]]\n***See also under ''Newton algorithm'' in the [[#Finding roots of nonlinear equations|section ''Finding roots of nonlinear equations'']]\n**[[Nonlinear conjugate gradient method]]\n**Derivative-free methods\n***[[Coordinate descent]] — move in one of the coordinate directions\n****[[Adaptive coordinate descent]] — adapt coordinate directions to objective function\n****[[Random coordinate descent]] — randomized version\n***[[Nelder–Mead method]]\n***[[Pattern search (optimization)]]\n***[[Powell's method]] — based on conjugate gradient descent\n***[[Rosenbrock methods]] — derivative-free method, similar to Nelder–Mead but with guaranteed convergence\n**[[Augmented Lagrangian method]] — replaces constrained problems by unconstrained problems with a term added to the objective function\n**[[Ternary search]]\n**[[Tabu search]]\n**[[Guided Local Search]] — modification of search algorithms which builds up penalties during a search\n**Reactive search optimization (RSO) — the algorithm adapts its parameters automatically\n**[[MM algorithm]] — majorize-minimization, a wide framework of methods\n**[[Least absolute deviations]]\n***[[Expectation–maximization algorithm]]\n****[[Ordered subset expectation maximization]]\n**[[Nearest neighbor search]]\n**[[Space mapping]] — uses \"coarse\" (ideal or low-fidelity) and \"fine\" (practical or high-fidelity) models\n\n===Optimal control and infinite-dimensional optimization===\n[[Optimal control]]\n*[[Pontryagin's minimum principle]] — infinite-dimensional version of Lagrange multipliers\n**[[Costate equations]] — equation for the \"Lagrange multipliers\" in Pontryagin's minimum principle\n**[[Hamiltonian (control theory)]] — minimum principle says that this function should be minimized\n*Types of problems:\n**[[Linear-quadratic regulator]] — system dynamics is a linear differential equation, objective is quadratic\n**[[Linear-quadratic-Gaussian control]] (LQG) — system dynamics is a linear SDE with additive noise, objective is quadratic\n***[[Optimal projection equations]] — method for reducing dimension of LQG control problem\n*[[Algebraic Riccati equation]] — matrix equation occurring in many optimal control problems\n*[[Bang–bang control]] — control that switches abruptly between two states\n*[[Covector mapping principle]]\n*[[Differential dynamic programming]] — uses locally-quadratic models of the dynamics and cost functions\n*[[DNSS point]] — initial state for certain optimal control problems with multiple optimal solutions\n*[[Legendre–Clebsch condition]] — second-order condition for solution of optimal control problem\n*[[Pseudospectral optimal control]]\n**[[Bellman pseudospectral method]] — based on Bellman's principle of optimality\n**[[Chebyshev pseudospectral method]] — uses Chebyshev polynomials (of the first kind)\n**[[Flat pseudospectral method]] — combines Ross–Fahroo pseudospectral method with differential flatness\n**[[Gauss pseudospectral method]] — uses collocation at the Legendre–Gauss points\n**[[Legendre pseudospectral method]] — uses Legendre polynomials\n**[[Pseudospectral knotting method]] — generalization of pseudospectral methods in optimal control\n**[[Ross–Fahroo pseudospectral method]] — class of pseudospectral method including Chebyshev, Legendre and knotting\n*[[Ross–Fahroo lemma]] — condition to make discretization and duality operations commute\n*[[Ross' π lemma]] — there is fundamental time constant within which a control solution must be computed for controllability and stability\n*[[Sethi model]] — optimal control problem modelling advertising\n\n[[Infinite-dimensional optimization]]\n*[[Semi-infinite programming]] — infinite number of variables and finite number of constraints, or other way around\n*[[Shape optimization]], [[Topology optimization]] — optimization over a set of regions\n**[[Topological derivative]] — derivative with respect to changing in the shape\n*[[Generalized semi-infinite programming]] — finite number of variables, infinite number of constraints\n\n===Uncertainty and randomness===\n*Approaches to deal with uncertainty:\n**[[Markov decision process]]\n**[[Partially observable Markov decision process]]\n**[[Robust optimization]]\n***[[Wald's maximin model]]\n**[[Scenario optimization]] — constraints are uncertain\n**[[Stochastic approximation]]\n**[[Stochastic optimization]]\n**[[Stochastic programming]]\n**[[Stochastic gradient descent]]\n*[[Random optimization]] algorithms:\n**[[Random search]] — choose a point randomly in ball around current iterate\n**[[Simulated annealing]]\n***[[Adaptive simulated annealing]] — variant in which the algorithm parameters are adjusted during the computation.\n***[[Great Deluge algorithm]]\n***[[Mean field annealing]] — deterministic variant of simulated annealing\n**[[Bayesian optimization]] — treats objective function as a random function and places a prior over it\n**[[Evolutionary algorithm]]\n***[[Differential evolution]]\n***[[Evolutionary programming]]\n***[[Genetic algorithm]], [[Genetic programming]]\n****[[Genetic algorithms in economics]]\n***[[MCACEA]] (Multiple Coordinated Agents Coevolution Evolutionary Algorithm) — uses an evolutionary algorithm for every agent\n***[[Simultaneous perturbation stochastic approximation]] (SPSA)\n**[[Luus–Jaakola]]\n**[[Particle swarm optimization]]\n**[[Stochastic tunneling]]\n**[[Harmony search]] — mimicks the improvisation process of musicians\n**see also the section ''Monte Carlo method''\n\n===Theoretical aspects===\n*[[Convex analysis]] — function ''f'' such that ''f''(''tx'' + (1 − ''t'')''y'') ≥ ''tf''(''x'') + (1 − ''t'')''f''(''y'') for ''t'' ∈ [0,1]\n**[[Pseudoconvex function]] — function ''f'' such that ∇''f'' · (''y'' − ''x'') ≥ 0 implies ''f''(''y'') ≥ ''f''(''x'')\n**[[Quasiconvex function]] — function ''f'' such that ''f''(''tx'' + (1 − ''t'')''y'') ≤ max(''f''(''x''), ''f''(''y'')) for ''t'' ∈ [0,1]\n**[[Subderivative]]\n**[[Geodesic convexity]] — convexity for functions defined on a Riemannian manifold\n*[[Duality (optimization)]]\n**[[Weak duality]] — dual solution gives a bound on the primal solution\n**[[Strong duality]] — primal and dual solutions are equivalent\n**[[Shadow price]]\n**[[Dual cone and polar cone]]\n**[[Duality gap]] — difference between primal and dual solution\n**[[Fenchel's duality theorem]] — relates minimization problems with maximization problems of convex conjugates\n**[[Perturbation function]] — any function which relates to primal and dual problems\n**[[Slater's condition]] — sufficient condition for strong duality to hold in a convex optimization problem\n**[[Total dual integrality]] — concept of duality for integer linear programming\n**[[Wolfe duality]] — for when objective function and constraints are differentiable\n*[[Farkas' lemma]]\n*[[Karush–Kuhn–Tucker conditions]] (KKT) — sufficient conditions for a solution to be optimal\n**[[Fritz John conditions]] — variant of KKT conditions\n*[[Lagrange multiplier]]\n**[[Lagrange multipliers on Banach spaces]]\n*[[Semi-continuity]]\n*[[Complementarity theory]] — study of problems with constraints of the form &lang;''u'', ''v''&rang; = 0\n**[[Mixed complementarity problem]]\n***[[Mixed linear complementarity problem]]\n***[[Lemke's algorithm]] — method for solving (mixed) linear complementarity problems\n*[[Danskin's theorem]] — used in the analysis of minimax problems\n*[[Maximum theorem]] — the maximum and maximizer are continuous as function of parameters, under some conditions\n*[[No free lunch in search and optimization]]\n*[[Relaxation (approximation)]] — approximating a given problem by an easier problem by relaxing some constraints\n**[[Lagrangian relaxation]]\n**[[Linear programming relaxation]] — ignoring the integrality constraints in a linear programming problem\n*[[Self-concordant function]]\n*[[Reduced cost]] — cost for increasing a variable by a small amount\n*[[Hardness of approximation]] — computational complexity of getting an approximate solution\n\n===Applications===\n*In geometry:\n**[[Geometric median]] — the point minimizing the sum of distances to a given set of points\n**[[Chebyshev center]] — the centre of the smallest ball containing a given set of points\n*In statistics:\n**[[Iterated conditional modes]] — maximizing joint probability of Markov random field\n**[[Response surface methodology]] — used in the design of experiments\n*[[Automatic label placement]]\n*[[Compressed sensing]] — reconstruct a signal from knowledge that it is sparse or compressible\n*[[Cutting stock problem]]\n*[[Demand optimization]]\n*[[Destination dispatch]] — an optimization technique for dispatching elevators\n*[[Energy minimization]]\n*[[Entropy maximization]]\n*[[Highly optimized tolerance]]\n*[[Hyperparameter optimization]]\n*[[Inventory control problem]]\n**[[Newsvendor model]]\n**[[Extended newsvendor model]]\n**[[Assemble-to-order system]]\n*[[Linear programming decoding]]\n*[[Linear search problem]] — find a point on a line by moving along the line\n*[[Low-rank approximation]] — find best approximation, constraint is that rank of some matrix is smaller than a given number\n*[[Meta-optimization]] — optimization of the parameters in an optimization method\n*[[Multidisciplinary design optimization]]\n*[[Optimal computing budget allocation]] — maximize the overall simulation efficiency for finding an optimal decision\n*[[Paper bag problem]]\n*[[Process optimization]]\n*[[Recursive economics]] — individuals make a series of two-period optimization decisions over time.\n*[[Stigler diet]]\n*[[Space allocation problem]]\n*[[Stress majorization]]\n*[[Trajectory optimization]]\n*[[Transportation theory (mathematics)|Transportation theory]]\n*[[Wing-shape optimization]]\n\n===Miscellaneous===\n*[[Combinatorial optimization]]\n*[[Dynamic programming]]\n**[[Bellman equation]]\n**[[Hamilton–Jacobi–Bellman equation]] — continuous-time analogue of Bellman equation\n**[[Backward induction]] — solving dynamic programming problems by reasoning backwards in time\n**[[Optimal stopping]] — choosing the optimal time to take a particular action\n***[[Odds algorithm]]\n***[[Robbins' problem]]\n*[[Global optimization]]:\n**[[BRST algorithm]]\n**[[MCS algorithm]]\n*[[Multi-objective optimization]] — there are multiple conflicting objectives\n**[[Benson's algorithm]] — for linear [[vector optimization]] problems\n*[[Bilevel optimization]] — studies problems in which one problem is embedded in another\n*[[Optimal substructure]]\n*[[Dykstra's projection algorithm]] — finds a point in intersection of two convex sets\n*Algorithmic concepts:\n**[[Barrier function]]\n**[[Penalty method]]\n**[[Trust region]]\n*[[Test functions for optimization]]:\n**[[Rosenbrock function]] — two-dimensional function with a banana-shaped valley\n**[[Himmelblau's function]] — two-dimensional with four local minima, defined by <math>f(x, y) = (x^2+y-11)^2 + (x+y^2-7)^2</math>\n**[[Rastrigin function]] — two-dimensional function with many local minima\n**[[Shekel function]] — multimodal and multidimensional\n*[[Mathematical Optimization Society]]\n\n==Numerical quadrature (integration)==\n[[Numerical integration]] — the numerical evaluation of an integral\n*[[Rectangle method]] — first-order method, based on (piecewise) constant approximation\n*[[Trapezoidal rule]] — second-order method, based on (piecewise) linear approximation\n*[[Simpson's rule]] — fourth-order method, based on (piecewise) quadratic approximation\n**[[Adaptive Simpson's method]]\n*[[Boole's rule]] — sixth-order method, based on the values at five equidistant points\n*[[Newton–Cotes formulas]] — generalizes the above methods\n*[[Romberg's method]] — Richardson extrapolation applied to trapezium rule\n*[[Gaussian quadrature]] — highest possible degree with given number of points\n**[[Chebyshev–Gauss quadrature]] — extension of Gaussian quadrature for integrals with weight {{nowrap|(1 − ''x''<sup>2</sup>)<sup>±1/2</sup>}} on [−1, 1]\n**[[Gauss–Hermite quadrature]] — extension of Gaussian quadrature for integrals with weight exp(−''x''<sup>2</sup>) on [−∞, ∞]\n**[[Gauss–Jacobi quadrature]] — extension of Gaussian quadrature for integrals with weight (1 − ''x'')<sup>''α''</sup> (1 + ''x'')<sup>''β''</sup> on [−1, 1]\n**[[Gauss–Laguerre quadrature]] — extension of Gaussian quadrature for integrals with weight exp(−''x'') on [0, ∞]\n**[[Gauss–Kronrod quadrature formula]] — nested rule based on Gaussian quadrature\n**[[Gaussian quadrature|Gauss–Kronrod rules]]\n*[[Tanh-sinh quadrature]] — variant of Gaussian quadrature which works well with singularities at the end points\n*[[Clenshaw–Curtis quadrature]] — based on expanding the integrand in terms of Chebyshev polynomials\n*[[Adaptive quadrature]] — adapting the subintervals in which the integration interval is divided depending on the integrand\n*[[Monte Carlo integration]] — takes random samples of the integrand\n**''See also [[#Monte Carlo method]]''\n*[[Quantized state systems method]] (QSS) — based on the idea of state quantization\n*[[Lebedev quadrature]] — uses a grid on a sphere with octahedral symmetry\n*[[Sparse grid]]\n*[[Coopmans approximation]]\n*[[Numerical differentiation]] — for fractional-order integrals\n**[[Numerical smoothing and differentiation]]\n**[[Adjoint state method]] — approximates gradient of a function in an optimization problem\n*[[Euler–Maclaurin formula]]\n\n==Numerical methods for ordinary differential equations==\n[[Numerical methods for ordinary differential equations]] — the numerical solution of ordinary differential equations (ODEs)\n*[[Euler method]] — the most basic method for solving an ODE\n*[[Explicit and implicit methods]] — implicit methods need to solve an equation at every step\n*[[Backward Euler method]] — implicit variant of the Euler method\n*[[Trapezoidal rule (differential equations)|Trapezoidal rule]] — second-order implicit method\n*[[Runge–Kutta methods]] — one of the two main classes of methods for initial-value problems\n**[[Midpoint method]] — a second-order method with two stages\n**[[Heun's method]] — either a second-order method with two stages, or a third-order method with three stages\n**[[Bogacki–Shampine method]] — a third-order method with four stages (FSAL) and an embedded fourth-order method\n**[[Cash–Karp method]] — a fifth-order method with six stages and an embedded fourth-order method\n**[[Dormand–Prince method]] — a fifth-order method with seven stages (FSAL) and an embedded fourth-order method\n**[[Runge–Kutta–Fehlberg method]] — a fifth-order method with six stages and an embedded fourth-order method\n**[[Gauss–Legendre method]] — family of A-stable method with optimal order based on Gaussian quadrature\n**[[Butcher group]] — algebraic formalism involving rooted trees for analysing Runge–Kutta methods\n**[[List of Runge–Kutta methods]]\n*[[Linear multistep method]] — the other main class of methods for initial-value problems\n**[[Backward differentiation formula]] — implicit methods of order 2 to 6; especially suitable for stiff equations\n**[[Numerov's method]] — fourth-order method for equations of the form <math>y'' = f(t,y)</math>\n**[[Predictor–corrector method]] — uses one method to approximate solution and another one to increase accuracy\n*[[General linear methods]] — a class of methods encapsulating linear multistep and Runge-Kutta methods\n*[[Bulirsch–Stoer algorithm]] — combines the midpoint method with Richardson extrapolation to attain arbitrary order\n*[[Exponential integrator]] — based on splitting ODE in a linear part, which is solved exactly, and a nonlinear part\n*Methods designed for the solution of ODEs from classical physics:\n**[[Newmark-beta method]] — based on the extended mean-value theorem\n**[[Verlet integration]] — a popular second-order method\n**[[Leapfrog integration]] — another name for Verlet integration\n**[[Beeman's algorithm]] — a two-step method extending the Verlet method\n**[[Dynamic relaxation]]\n*[[Geometric integrator]] — a method that preserves some geometric structure of the equation\n**[[Symplectic integrator]] — a method for the solution of Hamilton's equations that preserves the symplectic structure\n***[[Variational integrator]] — symplectic integrators derived using the underlying variational principle\n***[[Semi-implicit Euler method]] — variant of Euler method which is symplectic when applied to separable Hamiltonians\n**[[Energy drift]] — phenomenon that energy, which should be conserved, drifts away due to numerical errors\n*Other methods for initial value problems (IVPs):\n**[[Bi-directional delay line]]\n**[[Partial element equivalent circuit]]\n*Methods for solving two-point boundary value problems (BVPs):\n**[[Shooting method]]\n**[[Direct multiple shooting method]] — divides interval in several subintervals and applies the shooting method on each subinterval\n*Methods for solving differential-algebraic equations (DAEs), i.e., ODEs with constraints:\n**[[Constraint algorithm]] — for solving Newton's equations with constraints\n**[[Pantelides algorithm]] — for reducing the index of a DEA\n*Methods for solving stochastic differential equations (SDEs):\n**[[Euler–Maruyama method]] — generalization of the Euler method for SDEs\n**[[Milstein method]] — a method with strong order one\n**[[Runge–Kutta method (SDE)]] — generalization of the family of Runge–Kutta methods for SDEs\n*Methods for solving integral equations:\n**[[Nyström method]] — replaces the integral with a quadrature rule\n*Analysis:\n**[[Truncation error (numerical integration)]] — local and global truncation errors, and their relationships\n***[[Lady Windermere's Fan (mathematics)]] — telescopic identity relating local and global truncation errors\n*[[Stiff equation]] — roughly, an ODE for which unstable methods need a very short step size, but stable methods do not\n**[[L-stability]] — method is A-stable and stability function vanishes at infinity\n**[[Dynamic errors of numerical methods of ODE discretization]] — logarithm of stability function\n*[[Adaptive stepsize]] — automatically changing the step size when that seems advantageous\n*[[Parareal]] -- a parallel-in-time integration algorithm\n\n==Numerical methods for partial differential equations==\n[[Numerical partial differential equations]] — the numerical solution of partial differential equations (PDEs)\n\n===Finite difference methods===\n[[Finite difference method]] — based on approximating differential operators with difference operators\n*[[Finite difference]] — the discrete analogue of a differential operator\n**[[Finite difference coefficient]] — table of coefficients of finite-difference approximations to derivatives\n**[[Discrete Laplace operator]] — finite-difference approximation of the Laplace operator\n***[[Eigenvalues and eigenvectors of the second derivative]] — includes eigenvalues of discrete Laplace operator\n***[[Kronecker sum of discrete Laplacians]] — used for Laplace operator in multiple dimensions\n**[[Discrete Poisson equation]] — discrete analogue of the Poisson equation using the discrete Laplace operator\n*[[Stencil (numerical analysis)]] — the geometric arrangements of grid points affected by a basic step of the algorithm\n**[[Compact stencil]] — stencil which only uses a few grid points, usually only the immediate and diagonal neighbours\n***[[Higher-order compact finite difference scheme]]\n**[[Non-compact stencil]] — any stencil that is not compact\n**[[Five-point stencil]] — two-dimensional stencil consisting of a point and its four immediate neighbours on a rectangular grid\n*Finite difference methods for heat equation and related PDEs:\n**[[FTCS scheme]] (forward-time central-space) — first-order explicit\n**[[Crank–Nicolson method]] — second-order implicit\n*Finite difference methods for hyperbolic PDEs like the wave equation:\n**[[Lax–Friedrichs method]] — first-order explicit\n**[[Lax–Wendroff method]] — second-order explicit\n**[[MacCormack method]] — second-order explicit\n**[[Upwind scheme]]\n***[[Upwind differencing scheme for convection]] — first-order scheme for convection–diffusion problems\n**[[Lax–Wendroff theorem]] — conservative scheme for hyperbolic system of conservation laws converges to the weak solution\n*[[Alternating direction implicit method]] (ADI) — update using the flow in ''x''-direction and then using flow in ''y''-direction\n*[[Nonstandard finite difference scheme]]\n*Specific applications:\n**[[Finite difference methods for option pricing]]\n**[[Finite-difference time-domain method]] — a finite-difference method for electrodynamics\n\n===Finite element methods, gradient discretisation methods===\n[[Finite element method]] — based on a discretization of the space of solutions\n[[Gradient discretisation method]] — based on both the discretization of the solution and of its gradient\n*[[Finite element method in structural mechanics]] — a physical approach to finite element methods\n*[[Galerkin method]] — a finite element method in which the residual is orthogonal to the finite element space\n**[[Discontinuous Galerkin method]] — a Galerkin method in which the approximate solution is not continuous\n*[[Rayleigh–Ritz method]] — a finite element method based on variational principles\n*[[Spectral element method]] — high-order finite element methods\n*[[hp-FEM]] — variant in which both the size and the order of the elements are automatically adapted\n*Examples of finite elements:\n**[[Bilinear quadrilateral element]] — also known as the Q4 element\n**[[Constant strain triangle element]] (CST) — also known as the T3 element\n**[[Quadratic quadrilateral element]] — also known as the Q8 element\n**[[Barsoum elements]]\n*[[Direct stiffness method]] — a particular implementation of the finite element method, often used in structural analysis\n*[[Trefftz method]]\n*[[Finite element updating]]\n*[[Extended finite element method]] — puts functions tailored to the problem in the approximation space\n*[[Functionally graded element]]s — elements for describing functionally graded materials\n*[[Superelement]] — particular grouping of finite elements, employed as a single element\n*[[Interval finite element]] method — combination of finite elements with interval arithmetic\n*[[Discrete exterior calculus]] — discrete form of the exterior calculus of differential geometry\n*[[Modal analysis using FEM]] — solution of eigenvalue problems to find natural vibrations\n*[[Céa's lemma]] — solution in the finite-element space is an almost best approximation in that space of the true solution\n*[[Patch test (finite elements)]] — simple test for the quality of a finite element\n*[[MAFELAP]] (MAthematics of Finite ELements and APplications) — international conference held at Brunel University \n*[[NAFEMS]] — not-for-profit organisation that sets and maintains standards in computer-aided engineering analysis\n*[[Multiphase topology optimisation]] — technique based on finite elements for determining optimal composition of a mixture\n*[[Interval finite element]]\n*[[Applied element method]] — for simulation of cracks and structural collapse\n*[[Wood–Armer method]] — structural analysis method based on finite elements used to design reinforcement for concrete slabs\n*[[Isogeometric analysis]] — integrates finite elements into conventional NURBS-based CAD design tools\n*[[Loubignac iteration]]\n*[[Stiffness matrix]] — finite-dimensional analogue of differential operator\n*Combination with meshfree methods:\n**[[Weakened weak form]] — form of a PDE that is weaker than the standard weak form\n**[[G space]] — functional space used in formulating the weakened weak form\n**[[Smoothed finite element method]]\n*[[List of finite element software packages]]\n\n===Other methods===\n*[[Spectral method]] — based on the Fourier transformation\n**[[Pseudo-spectral method]]\n*[[Method of lines]] — reduces the PDE to a large system of ordinary differential equations\n*[[Boundary element method]] (BEM) — based on transforming the PDE to an integral equation on the boundary of the domain\n**[[Interval boundary element method]] — a version using interval arithmetics\n*[[Analytic element method]] — similar to the boundary element method, but the integral equation is evaluated analytically\n*[[Finite volume method]] — based on dividing the domain in many small domains; popular in computational fluid dynamics\n**[[Godunov's scheme]] — first-order conservative scheme for fluid flow, based on piecewise constant approximation\n**[[MUSCL scheme]] — second-order variant of Godunov's scheme\n**[[AUSM]] — advection upstream splitting method\n**[[Flux limiter]] — limits spatial derivatives (fluxes) in order to avoid spurious oscillations\n**[[Riemann solver]] — a solver for Riemann problems (a conservation law with piecewise constant data)\n**[[Properties of discretization schemes]] — finite volume methods can be conservative, bounded, etc.\n*[[Discrete element method]] — a method in which the elements can move freely relative to each other\n**[[Extended discrete element method]] — adds properties such as strain to each particle\n**[[Movable cellular automaton]] — combination of cellular automata with discrete elements\n*[[Meshfree methods]] — does not use a mesh, but uses a particle view of the field\n**[[Discrete least squares meshless method]] — based on minimization of weighted summation of the squared residual\n**[[Diffuse element method]]\n**[[Finite pointset method]] — represent continuum by a point cloud\n**[[Moving Particle Semi-implicit Method]]\n**[[Method of fundamental solutions]] (MFS) — represents solution as linear combination of fundamental solutions\n**Variants of MFS with source points on the physical boundary:\n***[[Boundary knot method]] (BKM)\n***[[Boundary particle method]] (BPM)\n***[[Regularized meshless method]] (RMM)\n***[[Singular boundary method]] (SBM)\n*Methods designed for problems from electromagnetics:\n**[[Finite-difference time-domain method]] — a finite-difference method\n**[[Rigorous coupled-wave analysis]] — semi-analytical Fourier-space method based on Floquet's theorem \n**[[Transmission-line matrix method]] (TLM) — based on analogy between electromagnetic field and mesh of transmission lines\n**[[Uniform theory of diffraction]] — specifically designed for scattering problems\n*[[Particle-in-cell]] — used especially in fluid dynamics\n**[[Multiphase particle-in-cell method]] — considers solid particles as both numerical particles and fluid\n*[[High-resolution scheme]]\n*[[Shock capturing method]]\n*[[Vorticity confinement]] — for vortex-dominated flows in fluid dynamics, similar to shock capturing\n*[[Split-step method]]\n*[[Fast marching method]]\n*[[Orthogonal collocation]]\n*[[Lattice Boltzmann methods]] — for the solution of the Navier-Stokes equations\n*[[Roe solver]] — for the solution of the Euler equation\n*[[Relaxation (iterative method)]] — a method for solving elliptic PDEs by converting them to evolution equations\n*Broad classes of methods:\n**[[Mimesis (mathematics)|Mimetic methods]] — methods that respect in some sense the structure of the original problem\n**[[Multiphysics]] — models consisting of various submodels with different physics\n**[[Immersed boundary method]] — for simulating elastic structures immersed within fluids\n*[[Multisymplectic integrator]] — extension of symplectic integrators, which are for ODEs\n*[[Stretched grid method]] — for problems solution that can be related to an elastic grid behavior.\n\n===Techniques for improving these methods===\n*[[Multigrid method]] — uses a hierarchy of nested meshes to speed up the methods\n*[[Domain decomposition methods]] — divides the domain in a few subdomains and solves the PDE on these subdomains\n**[[Additive Schwarz method]]\n**[[Abstract additive Schwarz method]] — abstract version of additive Schwarz without reference to geometric information\n**[[Balancing domain decomposition method]] (BDD) — preconditioner for symmetric positive definite matrices\n**[[BDDC|Balancing domain decomposition by constraints]] (BDDC) — further development of BDD\n**[[FETI|Finite element tearing and interconnect]] (FETI)\n**[[FETI-DP]] — further development of FETI\n**[[Fictitious domain method]] — preconditioner constructed with a structured mesh on a fictitious domain of simple shape\n**[[Mortar methods]] — meshes on subdomain do not mesh\n**[[Neumann–Dirichlet method]] — combines Neumann problem on one subdomain with Dirichlet problem on other subdomain\n**[[Neumann–Neumann methods]] — domain decomposition methods that use Neumann problems on the subdomains\n**[[Poincaré–Steklov operator]] — maps tangential electric field onto the equivalent electric current\n**[[Schur complement method]] — early and basic method on subdomains that do not overlap\n**[[Schwarz alternating method]] — early and basic method on subdomains that overlap\n*[[Coarse space (numerical analysis)|Coarse space]] — variant of the problem which uses a discretization with fewer degrees of freedom\n*[[Adaptive mesh refinement]] — uses the computed solution to refine the mesh only where necessary\n*[[Fast multipole method]] — hierarchical method for evaluating particle-particle interactions\n*[[Perfectly matched layer]] — artificial absorbing layer for wave equations, used to implement absorbing boundary conditions\n\n===Grids and meshes===\n*[[Grid classification]] / [[Types of mesh]]:\n**[[Polygon mesh]] — consists of polygons in 2D or 3D\n**[[Triangle mesh]] — consists of triangles in 2D or 3D\n***[[Triangulation (geometry)]] — subdivision of given region in triangles, or higher-dimensional analogue\n***[[Nonobtuse mesh]] — mesh in which all angles are less than or equal to 90°\n***[[Point set triangulation]] — triangle mesh such that given set of point are all a vertex of a triangle\n***[[Polygon triangulation]] — triangle mesh inside a polygon\n***[[Delaunay triangulation]] — triangulation such that no vertex is inside the circumcentre of a triangle\n***[[Constrained Delaunay triangulation]] — generalization of the Delaunay triangulation that forces certain required segments into the triangulation\n***[[Pitteway triangulation]] — for any point, triangle containing it has nearest neighbour of the point as a vertex\n***[[Minimum-weight triangulation]] — triangulation of minimum total edge length\n***[[Kinetic triangulation]] — a triangulation that moves over time\n***[[Triangulated irregular network]]\n***[[Quasi-triangulation]] — subdivision into simplices, where vertices are not points but arbitrary sloped line segments\n**[[Volume mesh]] — consists of three-dimensional shapes\n**[[Regular grid]] — consists of congruent parallelograms, or higher-dimensional analogue\n**[[Unstructured grid]]\n**[[Geodesic grid]] — isotropic grid on a sphere\n*[[Mesh generation]]\n**[[Image-based meshing]] — automatic procedure of generating meshes from 3D image data\n**[[Marching cubes]] — extracts a polygon mesh from a scalar field\n**[[Parallel mesh generation]]\n**[[Ruppert's algorithm]] — creates quality Delauney triangularization from piecewise linear data\n*Subdivisions:\n*[[Apollonian network]] — undirected graph formed by recursively subdividing a triangle \n*[[Barycentric subdivision]] —  standard way of dividing arbitrary convex polygons into triangles, or the higher-dimensional analogue\n*Improving an existing mesh:\n**[[Chew's second algorithm]] — improves Delauney triangularization by refining poor-quality triangles\n**[[Laplacian smoothing]] — improves polynomial meshes by moving the vertices\n*[[Jump-and-Walk algorithm]] — for finding triangle in a mesh containing a given point\n*[[Spatial twist continuum]] — dual representation of a mesh consisting of hexahedra\n*[[Pseudotriangle]] — simply connected region between any three mutually tangent convex sets\n*[[Simplicial complex]] — all vertices, line segments, triangles, tetrahedra, &hellip;, making up a mesh\n\n===Analysis===\n*[[Lax equivalence theorem]] — a consistent method is convergent if and only if it is stable\n*[[Courant–Friedrichs–Lewy condition]] — stability condition for hyperbolic PDEs\n*[[Von Neumann stability analysis]] — all Fourier components of the error should be stable\n*[[Numerical diffusion]] — diffusion introduced by the numerical method, above to that which is naturally present\n**[[False diffusion]]\n*[[Numerical resistivity]] — the same, with resistivity instead of diffusion\n*[[Weak formulation]] — a functional-analytic reformulation of the PDE necessary for some methods\n*[[Total variation diminishing]] — property of schemes that do not introduce spurious oscillations\n*[[Godunov's theorem]] — linear monotone schemes can only be of first order\n*[[Motz's problem]] — benchmark problem for singularity problems\n\n==[[Monte Carlo method]]==\n*Variants of the Monte Carlo method:\n**[[Direct simulation Monte Carlo]]\n**[[Quasi-Monte Carlo method]]\n**[[Markov chain Monte Carlo]]\n***[[Metropolis–Hastings algorithm]]\n****[[Multiple-try Metropolis]] — modification which allows larger step sizes\n****[[Wang and Landau algorithm]] — extension of Metropolis Monte Carlo\n****[[Equation of State Calculations by Fast Computing Machines]] — 1953 article proposing the Metropolis Monte Carlo algorithm\n****[[Multicanonical ensemble]] — sampling technique that uses Metropolis–Hastings to compute integrals\n***[[Gibbs sampling]]\n***[[Coupling from the past]]\n***[[Reversible-jump Markov chain Monte Carlo]]\n**[[Dynamic Monte Carlo method]]\n***[[Kinetic Monte Carlo]]\n***[[Gillespie algorithm]]\n**[[Particle filter]]\n***[[Auxiliary particle filter]]\n**[[Reverse Monte Carlo]]\n**[[Demon algorithm]]\n*[[Pseudo-random number sampling]]\n**[[Inverse transform sampling]] — general and straightforward method but computationally expensive\n**[[Rejection sampling]] — sample from a simpler distribution but reject some of the samples\n***[[Ziggurat algorithm]] — uses a pre-computed table covering the probability distribution with rectangular segments\n**For sampling from a normal distribution:\n***[[Box–Muller transform]]\n***[[Marsaglia polar method]]\n**[[Convolution random number generator]] — generates a random variable as a sum of other random variables\n**[[Indexed search]]\n*[[Variance reduction]] techniques:\n**[[Antithetic variates]]\n**[[Control variates]]\n**[[Importance sampling]]\n**[[Stratified sampling]]\n**[[VEGAS algorithm]]\n*[[Low-discrepancy sequence]]\n**[[Constructions of low-discrepancy sequences]]\n*[[Event generator]]\n*[[Parallel tempering]]\n*[[Umbrella sampling]] — improves sampling in physical systems with significant energy barriers\n*[[Hybrid Monte Carlo]]\n*[[Ensemble Kalman filter]] — recursive filter suitable for problems with a large number of variables\n*[[Transition path sampling]]\n*[[Walk-on-spheres method]] — to generate exit-points of Brownian motion from bounded domains \n*Applications:\n**[[Ensemble forecasting]] — produce multiple numerical predictions from slightly initial conditions or parameters\n**[[Bond fluctuation model]] — for simulating the conformation and dynamics of polymer systems\n**[[Iterated filtering]]\n**[[Metropolis light transport]]\n**[[Monte Carlo localization]] — estimates the position and orientation of a robot\n**[[Monte Carlo methods for electron transport]]\n**[[Monte Carlo method for photon transport]]\n**[[Monte Carlo methods in finance]]\n***[[Monte Carlo methods for option pricing]]\n***[[Quasi-Monte Carlo methods in finance]]\n**[[Monte Carlo molecular modeling]]\n***[[Path integral molecular dynamics]] — incorporates Feynman path integrals\n**[[Quantum Monte Carlo]]\n***[[Diffusion Monte Carlo]] — uses a Green function to solve the Schrödinger equation\n***[[Gaussian quantum Monte Carlo]]\n***[[Path integral Monte Carlo]]\n***[[Reptation Monte Carlo]]\n***[[Variational Monte Carlo]]\n**Methods for simulating the Ising model:\n***[[Swendsen–Wang algorithm]] — entire sample is divided into equal-spin clusters\n***[[Wolff algorithm]] — improvement of the Swendsen–Wang algorithm\n***[[Metropolis–Hastings algorithm]]\n**[[Auxiliary field Monte Carlo]] — computes averages of operators in many-body quantum mechanical problems\n**[[Cross-entropy method]] — for multi-extremal optimization and importance sampling\n*Also see the [[list of statistics topics]]\n\n==Applications==\n*[[Computational physics]]\n**[[Computational electromagnetics]]\n**[[Computational fluid dynamics]] (CFD)\n***[[Numerical methods in fluid mechanics]]\n***[[Large eddy simulation]]\n***[[Smoothed-particle hydrodynamics]]\n***[[Aeroacoustic analogy]] — used in numerical aeroacoustics to reduce sound sources to simple emitter types\n***[[Stochastic Eulerian Lagrangian method]] — uses Eulerian description for fluids and Lagrangian for structures\n***[[Explicit algebraic stress model]]\n**[[Computational magnetohydrodynamics]] (CMHD) — studies electrically conducting fluids\n**[[Climate model]]\n**[[Numerical weather prediction]]\n***[[Geodesic grid]]\n**[[Celestial mechanics]]\n***[[Numerical model of the Solar System]]\n**[[Quantum jump method]] — used for simulating open quantum systems, operates on wave function\n**[[Dynamic design analysis method]] (DDAM) — for evaluating effect of underwater explosions on equipment\n*[[Computational chemistry]]\n**[[Cell lists]]\n**[[Coupled cluster]]\n**[[Density functional theory]]\n**[[DIIS]] — direct inversion in (or of) the iterative subspace\n*[[Computational sociology]]\n*[[Computational statistics]]\n\n==Software==\nFor a large list of software, see the [[list of numerical analysis software]].\n\n[[Category:Numerical analysis|*Topics]]\n[[Category:Mathematics-related lists|Numerical analysis topics]]\n[[Category:Wikipedia outlines|Numerical analysis]]\n[[Category:Lists of topics|Numerical analysis]]"
    },
    {
      "title": "Adaptive stepsize",
      "url": "https://en.wikipedia.org/wiki/Adaptive_stepsize",
      "text": "{{Multiple issues|\n{{context|date=June 2012}}\n{{refimprove|date=October 2012}}\n}}\n\nIn [[numerical analysis]], some methods for the [[numerical methods for ordinary differential equations|numerical solution of ordinary differential equations]] (including the special case of [[numerical integration]]) use an '''adaptive stepsize''' in order to control the errors of the method and to ensure [[numerical stability|stability properties]] such as [[A-stability]]. [[Romberg's method]] is an example of a numerical integration method which uses an adaptive stepsize.\n\n==Example==\nFor simplicity, the following example uses the simplest integration method, the [[Euler method]]; in practice, higher-order methods such as [[Runge–Kutta]] methods are preferred due to their superior convergence and stability properties.\n\nConsider the initial value problem\n:  <math> y'(t) = f(t,y(t)), \\qquad y(a)=y_a </math>\nwhere ''y'' and ''f'' may denote vectors (in which case this equation represents a system of coupled ODEs in several variables).\n\nWe are given the function ''f(t,y)'' and the initial conditions ''(a, y<sub>a</sub>)'', and we are interested in finding the solution at ''t=b''. Let ''y(b)'' denote the exact solution at ''b'', and let ''y<sub>b</sub>'' denote the solution that we compute. We write <math>y_b + \\epsilon = y(b)</math>, where <math>\\epsilon</math> is the error in the numerical solution.\n\nFor a sequence ''(t<sub>n</sub>)'' of values of ''t'', with ''t<sub>n</sub> = a + nh'', the Euler method gives approximations to the corresponding values of ''y(t<sub>n</sub>)'' as\n: <math>y_{n+1}^{(0)}=y_n+hf(t_n,y_n)</math>\nThe local truncation error of this approximation is defined by\n:  <math>\\tau_{n+1}^{(0)}=y(t_{n+1}) - y_{n+1}^{(0)}</math>\nand by [[Taylor's theorem]], it can be shown that (provided ''f'' is sufficiently smooth) the local truncation error is proportional to the square of the step size:\n:  <math>\\tau_{n+1}^{(0)}=ch^2</math>\nwhere ''c'' is some constant of proportionality.\n\nWe have marked this solution and its error with a <math>(0)</math>.\n\nThe value of ''c'' is not known to us. Let us now apply Euler's method again with a different step size to generate a second approximation to ''y(t<sub>n+1</sub>)''. We get a second solution, which we label with a <math>(1)</math>. \nTake the new step size to be one half of the original step size, and apply two steps of Euler's method. This second solution is presumably more accurate. Since we have to apply Euler's method twice, the local error is (in the worst case) twice the original error.\n:  <math>y_{n+\\frac{1}{2}}=y_n+\\frac{h}{2}f(t_n,y_n)</math>\n:  <math>y_{n+1}^{(1)}=y_{n+\\frac{1}{2}}+\\frac{h}{2}f(t_{n+\\frac{1}{2}},y_{n+\\frac{1}{2}})</math>\n:  <math>\\tau_{n+1}^{(1)}=c\\left(\\frac{h}{2}\\right)^2+c\\left(\\frac{h}{2}\\right)^2=2c\\left(\\frac{h}{2}\\right)^2=\\frac{1}{2}ch^2=\\frac{1}{2}\\tau_{n+1}^{(0)}</math>\n:  <math>y_{n+1}^{(1)} + \\tau_{n+1}^{(1)}=y(t+h)</math>\n\nHere, we assume error factor <math>c</math> is constant over the interval <math>[t, t+h]</math>. In reality its rate of change is proportional to <math>y^{(3)}(t)</math>. Subtracting solutions gives the error estimate:\n\n:  <math> y_{n+1}^{(1)}-y_{n+1}^{(0)} = \\tau_{n+1}^{(1)} </math>\n\nThis local error estimate is third order accurate.\n\nThe local error estimate can be used to decide how stepsize <math>h</math> should be modified to achieve the desired accuracy. For example, if a local tolerance of <math>tol</math> is allowed, we could let h evolve like:\n\n:  <math> h \\rightarrow 0.9 \\times h \\times \\min{\\Big(\\max{\\Big(\\frac{tol}{|\\tau_{n+1}^{(1)}|}, 0.3\\Big)},2\\Big)} </math>\n\nThe <math>0.9</math> is a safety factor to ensure success on the next try. The minimum and maximum are to prevent extreme changes from the previous stepsize. This should, in principle give an error of about <math>0.9 \\times tol</math> in the next try. If <math>|\\tau_{n+1}^{(1)}| < tol</math>, we consider the step successful, and the error estimate is used to improve the solution:\n\n:  <math> y_{n+1}^{(2)} = y_{n+1}^{(1)} + \\tau_{n+1}^{(1)} </math>\n\nThis solution is actually '''third order''' accurate in the local scope (second order in the global scope), but since there is no error estimate for '''it''', this doesn't help in reducing the number of steps. This technique is called [[Richardson extrapolation]].\n\nBeginning with an initial stepsize of <math>h=b-a</math>, this theory facilitates our controllable integration of the ODE from point <math>a</math> to <math>b</math>, using an optimal number of steps given a local error tolerance. A drawback is that the step size may become prohibitively small, especially when using the low-order [[Euler method]].\n\nSimilar methods can be developed for higher order methods, such as the 4th order Runge-Kutta method. Also, a global error tolerance can be achieved by scaling the local error to global scope.\n\n==Embedded error estimates==\n\nAdaptive stepsize methods that use a so-called 'embedded' error estimate include the [[Runge–Kutta–Fehlberg method|Runge–Kutta–Fehlberg]], [[Cash–Karp method|Cash–Karp]] and [[Dormand–Prince method|Dormand–Prince]] methods. These methods are considered to be more computationally efficient, but have lower accuracy in their error estimates.\n\n== References ==\n{{Reflist}}\n\n== Further reading ==\n\n*William H. Press, Saul A. Teukolsky, William T. Vetterling, Brian P. Flannery, ''Numerical Recipes in C'', Second Edition, CAMBRIDGE UNIVERSITY PRESS, 1992. {{ISBN|0-521-43108-5}}\n*Kendall E. Atkinson, ''Numerical Analysis'', Second Edition, John Wiley & Sons, 1989. {{ISBN|0-471-62489-6}}\n\n{{DEFAULTSORT:Adaptive Stepsize}}\n[[Category:Numerical differential equations]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Adjoint state method",
      "url": "https://en.wikipedia.org/wiki/Adjoint_state_method",
      "text": "{{primary sources|date=January 2010}}\nThe '''adjoint state method''' is a [[numerical method]] for efficiently computing the [[gradient]] of a [[Function (mathematics)|function]] or [[operator (mathematics)|operator]] in a [[numerical optimization problem]].  It has applications in [[geophysics]], [[seismic imaging]] and more recently in [[neural networks]]<ref> Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, David Duvenaud ''Neural Ordinary Differential Equations'' [https://arxiv.org/pdf/1806.07366.pdf Available online] </ref>. \n\nThe adjoint state space is chosen to simplify the physical interpretation of equation [[constraint (mathematics)|constraint]]s.<ref name='symes_tr94'>Alain Sei & William Symes. ''Gradient Calculation of the Traveltime Cost Function Without Ray-tracing.'' Expanded Abstracts, 65th Annual [[Society of Exploration Geophysicists]] (SEG) Meeting and Exposition, pages 1351–1354 ([http://www.caam.rice.edu/caam/trs/94/TR94-15.pdf Available online])</ref> It may take the form of a [[Hilbert space]].\n\nAdjoint state techniques allow the use of [[integration by parts]], resulting in a form which explicitly contains the physically interesting quantity.  An adjoint state equation is introduced, including a new unknown variable.\n\nThe adjoint method formulates the gradient of a function towards its parameters in a constraint optimization form. By using the dual form of this constraint optimization problem, it can be used to calculate the gradient very fast. A nice property is that the number of computations is independent of the number of parameters for which you want the gradient.\nThe adjoint method is derived from the [[dual problem]] [http://www.dgp.toronto.edu/people/stam/reality/Research/pdf/sig04.pdf] and is used e.g. in the [[Landweber iteration]] method [http://liu.diva-portal.org/smash/get/diva2:24091/FULLTEXT01.pdf].\n\nThe name '''adjoint state method''' refers to the [[Linear programming#Duality|dual]] form of the problem, where the [[adjoint matrix]] <math>A^*=\\overline A ^T</math> is used.\n\nWhen the initial problem consists of calculating the product <math>s^T x</math> and <math>x</math> must satisfy <math>Ax=b</math>, the dual problem can be realized as calculating the product <math>r^T b</math> (<math> = s^T x </math>), where <math>r</math> must satisfy <math>A^* r = s </math>. \nAnd \n<math> r </math> is called the adjoint state vector.\n\n== See also ==\n* [[Backpropagation]]\n*[[Adjoint equation]]\n\n== References ==\n<references/>\n\n== External links ==\n\n* A well written explanation by Errico: [https://dx.doi.org/10.1175/1520-0477(1997)078%3C2577:WIAAM%3E2.0.CO;2 What is an adjoint Model? ]\n* Another well written explanation with worked examples, written by Bradley [http://cs.stanford.edu/~ambrad/adjoint_tutorial.pdf]\n* More technical explanation: A [http://gji.oxfordjournals.org/content/167/2/495.full.pdf+html review] of the adjoint-state method for computing the gradient of a functional with geophysical applications\n* MIT course [https://web.archive.org/web/20150906095132/http://ocw.mit.edu/courses/mathematics/18-325-topics-in-applied-mathematics-waves-and-imaging-fall-2012/lecture-notes/MIT18_325F12_Chapter4.pdf]\n* Deriving the adjoint state method from different fields: [http://gji.oxfordjournals.org/content/167/2/495.full]\n* MIT notes [http://math.mit.edu/~stevenj/18.336/adjoint.pdf]\n\n[[Category:Numerical analysis]]\n{{Mathapplied-stub}}"
    },
    {
      "title": "Affine arithmetic",
      "url": "https://en.wikipedia.org/wiki/Affine_arithmetic",
      "text": "'''Affine arithmetic''' ('''AA''') is a model for [[self-validated computation|self-validated]] [[numerical analysis]].  In AA, the quantities of interest are represented as [[affine combination]]s ('''affine forms''') of certain primitive variables, which stand for sources of uncertainty in the data or approximations made during the computation.\n\nAffine arithmetic is meant to be an improvement on [[interval arithmetic]] (IA), and is similar to [[generalized interval arithmetic]], first-order [[Taylor arithmetic]], the [[center-slope model]], and [[ellipsoid calculus]] &mdash; in the sense that it is an automatic method to derive first-order guaranteed approximations to general formulas.\n\nAffine arithmetic is potentially useful in every numeric problem where one needs guaranteed enclosures to smooth functions, such as solving [[equation system|system]]s of non-linear equations, analyzing [[dynamical system]]s, [[integral|integrating]] functions [[differential equation]]s, etc.  Applications include [[ray tracing (graphics)|ray tracing]], [[2D computer graphics|plotting]] [[curve]]s, intersecting [[implicit surface|implicit]] and [[parametric surface]]s, [[error analysis (mathematics)]], [[process control]], worst-case analysis of [[electric circuit]]s, and more.\n\n==Definition==\nIn affine arithmetic, each input or computed quantity ''x'' is represented by a formula\n<math>x = x_0 + x_1 \\epsilon_1 + x_2 \\epsilon_2 + {}</math><math>\\cdots</math><math>{} + x_n \\epsilon_n</math>\nwhere <math>x_0, x_1, x_2,</math><math>\\dots,</math><math> x_n </math> are known floating-point numbers, and <math>\\epsilon_1, \\epsilon_2,\\epsilon_n</math> are symbolic variables whose values are only known to lie in the range [-1,+1].\n\nThus, for example, a quantity ''X'' which is known to lie in the range [3,7] can be represented by the affine form <math>x = 5 + 2 \\epsilon_k</math>, for some ''k''.  Conversely, the form <math>x = 10 + 2 \\epsilon_3 - 5 \\epsilon_8</math> implies that the corresponding quantity ''X'' lies in the range [3,17].\n\nThe sharing of a symbol <math>\\epsilon_j</math> among two affine forms <math>x</math>,  <math>y</math> implies that the corresponding quantities ''X'', ''Y'' are partially dependent, in the sense that their joint range is smaller than the [[Cartesian product]] of their separate ranges.  For example, if \n<math>x = 10 + 2 \\epsilon_3 - 6 \\epsilon_8</math> and \n<math>y = 20 + 3 \\epsilon_4 + 4 \\epsilon_8</math>, \nthen the individual ranges of ''X'' and ''Y'' are [2,18] and [13,27], but the joint range of the pair (''X'',''Y'') is the [[hexagon]] with corners (2,27), (6,27), (18,19), (18,13), (14,13), (2,21) &mdash; which is a proper subset of the [[rectangle]] [2,18]×[13,27].\n\n==Affine arithmetic operations==\nAffine forms can be combined with the standard arithmetic operations or elementary functions, to obtain guaranteed approximations to formulas.\n\n===Affine operations===\nFor example, given affine forms <math>x,y</math>  for ''X'' and ''Y'', one can obtain an affine form <math>z</math> for ''Z'' = ''X'' + ''Y'' simply by adding the forms &mdash; that is, setting <math>z_j</math> <math>\\gets</math> <math>x_j + y_j</math> for every ''j''.  Similarly, one can compute an affine form <math>z</math> for ''Z'' = <math>\\alpha</math>''X'', where <math>\\alpha</math> is a known constant, by setting <math>z_j</math> <math>\\gets</math> <math>\\alpha x_j</math> for every ''j''.  This generalizes to arbitrary affine operations like ''Z'' = <math>\\alpha</math>''X'' + <math>\\beta</math>''Y'' + <math>\\gamma</math>.\n\n===Non-affine operations===\nA non-affine operation <math>Z</math> <math>\\gets</math> <math>F(X,Y,</math><math>\\dots</math><math>)</math>, like multiplication <math>Z</math> <math>\\gets</math> <math>X Y</math> or <math>Z</math> <math>\\gets</math> <math>\\sin(X)</math>, cannot be performed exactly, since the result would not be an affine form of the <math>\\epsilon_i</math>.  In that case, one should take a suitable affine function ''G'' that approximates ''F'' to first order, in the ranges implied by <math>x</math> and <math>y</math>; and compute <math>z</math> <math>\\gets</math> <math>G(x,y,</math><math>\\dots</math><math>) + z_k\\epsilon_k</math>, where <math>z_k</math> is an upper bound for the absolute error <math>|F-G|</math> in that range, and  <math>\\epsilon_k</math> is a new symbolic variable not occurring in any previous form.\n\nThe form <math>z</math> then gives a guaranteed enclosure for the quantity ''Z''; moreover, the affine forms <math>x,y,</math><math>\\dots</math><math>,z</math> jointly provide a guaranteed enclosure for the point (''X'',''Y'',...,''Z''), which is often much smaller than the Cartesian product of the ranges of the individual forms.\n\n===Chaining operations===\nSystematic use of this method allows arbitrary computations on given quantities to be replaced by equivalent computations on their affine forms, while preserving first-order correlations between the input and output and guaranteeing the complete enclosure of the joint range. One simply replaces each arithmetic operation or elementary function call in the formula by a call to the corresponding AA library routine.\n\nFor smooth functions, the approximation errors made at each step are proportional to the square ''h''<sup>2</sup> of the width ''h'' of the input intervals. For this reason, affine arithmetic will often yield much tighter bounds than standard interval arithmetic (whose errors are proportional to ''h'').\n\n===Roundoff errors===\nIn order to provide guaranteed enclosure, affine arithmetic operations must account for the roundoff errors in the computation of the resulting coefficients <math>z_j</math>. This cannot be done by rounding each <math>z_j</math> in a specific direction, because any such rounding would falsify the dependencies between affine forms that share the symbol <math>\\epsilon_j</math>.  Instead, one must compute an upper bound <math>\\delta_j</math> to the roundoff error of each <math>z_j</math>, and add all those <math>\\delta_j</math> to the coefficient <math>z_k</math> of the new symbol <math>\\epsilon_k</math> (rounding up). Thus, because of roundoff errors, even  affine operations like ''Z'' = <math>\\alpha</math>''X'' and ''Z'' = ''X'' + ''Y'' will add the extra term <math>z_k\\epsilon_k</math>.\n\nThe handling of roundoff errors increases the code complexity and execution time  of AA operations.  In applications where those errors are known to be unimportant (because they are dominated by uncertainties in the input data and/or by the linearization errors), one may use a simplified AA library that does not implement roundoff error control.\n\n==Affine projection model==\nAffine arithmetic can be viewed in matrix form as follows.  Let <math>X_1,X_2,</math><math>\\dots,</math><math>X_m</math> be all input and computed quantities in use at some point during a computation.   The affine forms for those quantities can be represented by a single coefficient matrix ''A'' and a vector ''b'', where element <math>A_{i,j}</math> is the coefficient of symbol <math>\\epsilon_j</math> in the affine form of ''<math>X_i</math>''; and <math>b_i</math> is the independent term of that form.  Then the joint range of the quantities &mdash; that is, the range of the point <math>(X_1,X_2,</math><math>\\dots,</math><math>X_m)</math> &mdash; is the image of the hypercube <math>U^n = [-1,+1]^n</math> by the affine map from <math>U^n</math> to <math>R^m</math> defined by <math>\\epsilon</math> <math>\\to</math> <math>A \\epsilon + b</math>.\n\nThe range of this affine map is a [[zonotope]] bounding the joint range of the quantities <math>X_1,X_2,</math><math>\\dots,</math><math>X_m</math>.  Thus one could say that AA is a \"zonotope arithmetic\". Each step of AA usually entails adding one more row and one more column to the matrix ''A''.\n\n==Affine form simplification==\nSince each AA operation generally creates a new symbol <math>\\epsilon_k</math>, the number of terms in an affine form may be proportional to the number of operations used to compute it.  Thus, it is often necessary to apply \"symbol condensation\" steps, where two or more symbols <math>\\epsilon_k</math> are replaced by a smaller set of new symbols. Geometrically, this means replacing a complicated zonotope ''P'' by a simpler zonotope ''Q'' that encloses it.  This operation can be done without destroying the first-order approximation property of the final zonotope.\n\n== Implementation ==\n\n===Matrix implementation===\nAffine arithmetic can be implemented by a global array ''A'' and a global vector ''b'', as described above. This approach is reasonably adequate when the set of quantities to be computed is small and known in advance. In this approach, the programmer must maintain externally the correspondence between the row indices and the quantities of interest.  Global variables hold the number ''m'' of affine forms (rows) computed so far, and the number ''n'' of symbols (columns) used so far; these are automatically updated at each AA operation.\n\n===Vector implementation===\nAlternatively, each affine form can be implemented as a separate vector of coefficients.  This approach is more convenient for programming, especially when there are calls to library procedures that may use AA internally.  Each affine form can be given a mnemonic name; it can be allocated when needed, be passed to procedures, and reclaimed when no longer needed. The AA code then looks much closer to the original formula.  A global variable holds the number ''n'' of symbols used so far.\n\n===Sparse vector implementation===\nOn fairly long computations, the set of \"live\" quantities (that will be used in future computations) is much smaller than the set of all computed quantities; and ditto for the set of \"live\" symbols <math>\\epsilon_j</math>.  In this situation, the matrix and vector implementations are too wasteful of time and space.\n\nIn such situations, one should use a [[sparse array|sparse]] implementation. Namely, each affine form is stored as a list of pairs (j,<math>x_j</math>), containing only the terms with non-zero coefficient <math>x_j</math>.  For efficiency, the terms should be sorted in order of ''j''.  This representation makes the AA operations somewhat more complicated; however, the cost of each operation becomes proportional to the number of nonzero terms appearing in the operands, instead of the number of total symbols used so far.\n\nThis is the representation used by LibAffa.\n\n== References ==\n\n*L. H. de Figueiredo and J. Stolfi (2004) \"Affine arithmetic: concepts and applications.\" ''Numerical Algorithms'' '''37''' (1&ndash;4), 147&ndash;158.\n* J. L. D. Comba and J. Stolfi (1993), \"Affine arithmetic and its applications to computer graphics\". ''Proc. SIBGRAPI'93 &mdash; VI Simpósio Brasileiro de Computação Gráfica e Processamento de Imagens (Recife, BR)'',  9&ndash;18.<!--com-sto-93-aa-->\n* L. H. de Figueiredo and J. Stolfi (1996), \"Adaptive enumeration of implicit surfaces with affine arithmetic\". ''Computer Graphics Forum'', '''15'''  ''5'',  287&ndash;296.<!--fig-sto-96-imp-->\n* W. Heidrich (1997), \"A compilation of affine arithmetic versions of common math library functions\". Technical Report 1997-3, Universität Erlangen-Nürnberg.<!--hei-97-aa-libs-tr-->\n* M. Kashiwagi (1998), \"An all solution algorithm using affine arithmetic\". ''NOLTA'98 &mdash; 1998 International Symposium on Nonlinear Theory and its Applications (Crans-Montana, Switzerland)'',  14&ndash;17.<!--kas-98-affa-->\n* L. Egiziano, N. Femia, and G.  Spagnuolo (1998), \"New approaches to the true worst-case evaluation in circuit tolerance and sensitivity analysis &mdash; Part II: Calculation of the outer solution using affine arithmetic\". ''Proc. COMPEL'98 &mdash; 6th Workshop on Computer in Power Electronics (Villa Erba, Italy)'',  19&ndash;22.<!--egi-fem-spa-98-aacir-->\n* W. Heidrich, Ph. Slusallek, and H.-P. Seidel (1998), \"Sampling procedural shaders using affine arithmetic\". ''ACM Transactions on Graphics'', '''17'''  ''3'',  158&ndash;176.<!--hei-slu-sei-98-aash-->\n* F. Messine and A. Mahfoudi (1998), \"Use of affine arithmetic in interval optimization algorithms to solve multidimensional scaling problems\". ''Proc. SCAN'98 &mdash; IMACS/GAMM International Symposium on Scientific Computing, Computer Arithmetic and Validated Numerics (Budapest, Hungary)'',  22&ndash;25.<!--mes-mah-98-mopt-->\n* A. de Cusatis Jr., L. H. Figueiredo, and M. Gattass (1999), \"Interval methods for ray casting surfaces with affine arithmetic\". ''Proc. SIBGRAPI'99 &mdash; 12th Brazilian Symposium on Computer Graphics and Image Processing'',  65&ndash;71. <!--cus-fig-gat-99-rtaa-->\n* K. Bühler and W. Barth (2000), \"A new intersection algorithm for parametric surfaces based on linear interval estimations\". ''Proc. SCAN 2000 / Interval 2000 &mdash; 9th GAMM-IMACS International Symposium on Scientific Computing, Computer Arithmetic, and Validated Numerics'', ???&ndash;???. <!--bue-bar-00-inter-->\n* I. Voiculescu, J. Berchtold, A. Bowyer, R. R. Martin, and Q. Zhang (2000), \"Interval and affine arithmetic for surface location of power- and Bernstein-form polynomials\". ''Proc. Mathematics of Surfaces IX'',  410&ndash;423. Springer, {{isbn|1-85233-358-8}}.<!--voi-ber-bow-mar-zha-00-aaloc-->\n* Q. Zhang and R. R. Martin (2000), \"Polynomial evaluation using affine arithmetic for curve drawing\". ''Proc. of Eurographics UK 2000 Conference'',  49&ndash;56. {{isbn|0-9521097-9-4}}.<!--zha-mar-00-aa-polycurv-->\n* D. Michelucci (2000), \"Reliable computations for dynamic systems\". ''Proc. SCAN 2000 / Interval 2000 &mdash; 9th GAMM-IMACS International Symposium on Scientific Computing, Computer Arithmetic, and Validated Numerics'',  ???&ndash;???.<!--mic-00-dyna-->\n* N. Femia and G. Spagnuolo (2000), \"True worst-case circuit tolerance analysis using genetic algorithm and affine arithmetic &mdash; Part I\". ''IEEE Transactions on Circuits and Systems'', '''47'''  ''9'',  1285&ndash;1296.<!--fem-spa-00-aa-eletr-->\n* R. Martin, H. Shou, I. Voiculescu, and G. Wang (2001), \"A comparison of Bernstein hull and affine arithmetic methods for algebraic curve drawing\". ''Proc. Uncertainty in Geometric Computations'',  143&ndash;154. Kluwer Academic Publishers, {{isbn|0-7923-7309-X}}.<!--mar-sho-voi-wan-01-aacomp-->\n* A. Bowyer, R. Martin, H. Shou, and I. Voiculescu (2001), \"Affine intervals in a CSG geometric modeller\". ''Proc. Uncertainty in Geometric Computations'',  1&ndash;14. Kluwer Academic Publishers, {{isbn|0-7923-7309-X}}.<!--bow-mar-sho-voi-01-aamodel-->\n* T. Kikuchi and M. Kashiwagi (2001), \"Elimination of non-existence regions of the solution of nonlinear equations using affine arithmetic\". ''Proc. NOLTA'01 &mdash; 2001 International Symposium on Nonlinear Theory and its Applications''.<!--kik-kas-01-aa-ode-->\n* T. Miyata and M. Kashiwagi (2001), \"On range evaluation of polynomials of affine arithmetic\". ''Proc. NOLTA'01 - 2001 International Symposium on Nonlinear Theory and its Applications''.<!--miy-kas-01-aa-poly-->\n* Y. Kanazawa and S. Oishi (2002), \"A numerical method of proving the existence of solutions for nonlinear ODEs using affine arithmetic\". ''Proc. SCAN'02 &mdash; 10th GAMM-IMACS International Symposium on Scientific Computing, Computer Arithmetic, and Validated Numerics''.  <!--kan-ois-02-aa-ode-->\n* H. Shou, R. R.Martin, I. Voiculescu, A. Bowyer, and G. Wang (2002), \"Affine arithmetic in matrix form for polynomial evaluation and algebraic curve drawing\". ''Progress in Natural Science'', '''12'''  ''1'',  77&ndash;81.<!--sho-mar-voi-02-aamatr-->\n* A. Lemke, L. Hedrich, and E. Barke (2002), \"Analog circuit sizing based on formal methods using affine arithmetic\". ''Proc. ICCAD-2002 &mdash; International Conference on Computer Aided Design'',  486&ndash;489.<!--lem-hed-bar-02-aa-circ-->\n* F. Messine (2002), \"Extensions of affine arithmetic: Application to unconstrained global optimization\". ''Journal of Universal Computer Science'', '''8'''  ''11'',  992&ndash;1015.<!--mes-02-aa-jucs-->\n* K. Bühler (2002), \"Implicit linear interval estimations\". ''Proc. 18th Spring Conference on Computer Graphics (Budmerice, Slovakia)'',  123&ndash;132. ACM Press, {{isbn|1-58113-608-0}}.<!--bue-02-aa-estim-->\n* L. H. de Figueiredo, J. Stolfi, and L. Velho (2003), \"Approximating parametric curves with strip trees using affine arithmetic\". ''Computer Graphics Forum'', '''22'''  ''2'',  171&ndash;179.<!--fig-sto-vel-03-parcur-cgf-->\n* C. F. Fang, T. Chen, and R. Rutenbar (2003), \"Floating-point error analysis based on affine arithmetic\". ''Proc. 2003 International Conf. on Acoustic, Speech and Signal Processing''.<!--fan-che-rut-03-fperr-->\n* A. Paiva, L. H. de Figueiredo, and J. Stolfi (2006), \"Robust visualization of strange attractors using affine arithmetic\". ''Computers & Graphics'', '''30'''  ''6'',  1020&ndash; 1026.\n\n== External links ==\n*[http://www.ic.unicamp.br/~stolfi/EXPORT/projects/affine-arith/Welcome.html] Stolfi's page on AA.\n*[http://savannah.nongnu.org/projects/libaffa] LibAffa, an LGPL implementation of affine arithmetic.\n*[http://sourceforge.net/projects/asol/] ASOL, a branch-and-prune method to find all solutions to systems of nonlinear equations using affine arithmetic\n*[http://www.scg.inf.uni-due.de/fileadmin/Projekte/YalAA/index.html] YalAA, an object-oriented C++ based template library for affine arithmetic (AA).\n[[Category:Numerical analysis]]\n[[Category:Affine geometry]]"
    },
    {
      "title": "Aitken's delta-squared process",
      "url": "https://en.wikipedia.org/wiki/Aitken%27s_delta-squared_process",
      "text": "In [[numerical analysis]], '''Aitken's delta-squared process''' or '''Aitken Extrapolation''' is a [[series acceleration]] method, used for accelerating the [[rate of convergence]] of a sequence. It is named after [[Alexander Aitken]], who introduced this method in 1926.<ref>Alexander Aitken, \"On Bernoulli's numerical solution of algebraic equations\", ''Proceedings of the Royal Society of Edinburgh'' (1926) '''46''' pp. 289&ndash;305.</ref> Its early form was known to [[Seki Kōwa]] (end of 17th century) and was found for rectification of the circle, i.e. the calculation of π. It is most useful for accelerating the convergence of a sequence that is converging linearly.\n\n==Definition==\nGiven a sequence <math>X = {(x_n)}_{n\\in\\N}</math>, one associates with this sequence the new sequence\n:<math>A X={\\left(\\frac{x_n\\,x_{n+2}-x_{n+1}^2}{x_n+x_{n+2}-2\\,x_{n+1}}\\right)}_{n\\in\\Z^*},</math>\n\nwhich can, with improved [[numerical stability]], also be written as\n:<math> (A X)_n = x_n-\\frac{(\\Delta x_n)^2}{\\Delta^2 x_n},</math>\nor equivalently as\n:<math>(A X)_n = x_{n+2} - \\frac{(\\Delta x_{n+1})^2}{\\Delta^2 x_n} = x_{n+2} - \\frac{(x_{n+2}-x_{n+1})^2}{(x_{n+2}-x_{n+1})-(x_{n+1}-x_{n})}</math>\n\nwhere\n\n:<math>\\Delta x_{n}={(x_{n+1}-x_{n})},\\ \\Delta x_{n+1}={(x_{n+2}-x_{n+1})},</math>\n\nand\n\n:<math>\\Delta^2 x_n=x_n -2x_{n+1} + x_{n+2}=\\Delta x_{n+1}-\\Delta x_{n},\\ </math>\n\nfor <math>n = 0, 1, 2, 3, \\dots \\, </math>\n\nObviously, <math> A X </math> is ill-defined if <math> \\Delta^2 x </math> contains a zero element, or equivalently, if the sequence of [[first difference]]s has a repeating term.\n\nFrom a theoretical point of view, if that occurs only for a finite number of indices, one could easily agree to consider the sequence <math> A X </math> restricted to indices <math> n > n_0 </math>  with a sufficiently large <math> n_0 </math>. From a practical point of view, one does in general rather consider only the first few terms of the sequence, which usually provide the needed precision. Moreover, when numerically computing the sequence, one has to take care to stop the computation when [[rounding error]]s in the denominator become too large, where the Δ² operation may cancel too many [[significant digit]]s. (It would be better for numerical calculation to use <math> \\Delta x_{n+1} - \\Delta x_{n}\\ = (x_{n+2}-x_{n+1})-(x_{n+1}-x_{n})\\ </math> rather than <math>x_n - 2x_{n+1} + x_{n+2}\\ </math> .)\n\n==Properties==\nAitken's delta-squared process is a method of [[acceleration of convergence]], and a particular case of a nonlinear [[sequence transformation]].\n\n<math>x</math> will [[rate of convergence|converge linearly]] to <math>\\ell</math> if there exists a number μ ∈ (0, 1) such that\n:<math> \\lim_{n\\to \\infty} \\frac{|x_{n+1}-\\ell|}{|x_n-\\ell|} = \\mu.</math>\n\nAitken's method will accelerate the sequence <math>x_n</math> if <math>\\lim_{n \\to \\infty}\\frac{(A x)_n-\\ell}{x_n-\\ell}=0.</math>\n\n<math>A</math> is not a linear operator, but a constant term drops out, viz: <math>A[x-\\ell] = Ax - \\ell</math>, if  <math>\\ell</math> is a constant. This is clear from the expression of <math>Ax</math> in terms of the [[finite difference]] operator <math>\\Delta</math>.\n\nAlthough the new process does not in general converge quadratically, it can be shown that for a [[Fixed point (mathematics)|fixed point]] process, that is, for an [[iterated function]] sequence <math>x_{n+1}=f(x_n)</math> for some function <math>f</math>, converging to a fixed point, the convergence is quadratic. In this case, the technique is known as [[Steffensen's method]].\n\nEmpirically, the ''A''-operation eliminates the \"most important error term\". One can check this by considering a sequence of the form <math>x_n=\\ell+a^n+b^n</math>, where <math>0<b<a<1</math>:\nThe sequence <math>Ax</math> will then go to the limit like <math>b^n</math> goes to zero.\n\nOne can also show that if <math>x</math> goes to its limit <math>\\ell</math> at a rate strictly greater than 1, <math>Ax</math> does not have a better rate of convergence. (In practice, one rarely has e.g. quadratic convergence which would mean over 30 resp. 100 correct decimal places after 5 resp. 7 iterations (starting with 1 correct digit); usually no acceleration is needed in that case.)\n\nIn practice, <math>Ax</math> converges much faster to the limit than <math>x</math> does, as demonstrated by the example calculations below.\nUsually, it is much cheaper to calculate <math>Ax</math> (involving only calculation of differences, one multiplication and one division) than to calculate many more terms of the sequence <math>x</math>. Care must be taken, however, to avoid introducing errors due to insufficient precision when calculating the ''differences'' in the numerator and denominator of the expression.\n\n==Example calculations==\n\n'''Example 1''': The value of <math>\\sqrt{2} \\approx 1.4142136</math> can be approximated by assuming an initial value for <math>a_0</math> and iterating the following:\n::<math>a_{n+1} = \\frac{a_n + \\frac{2}{a_n}}{2}. </math>\nStarting with <math>a_0 = 1:</math>\n{| class=\"wikitable\"\n|-\n| ''n''\n| ''x'' = iterated value\n| ''Ax''\n|-\n| 0\n| 1\n| 1.4285714\n|-\n| 1\n| 1.5\n| 1.4141414\n|-\n| 2\n| 1.4166667\n| 1.4142136\n|-\n| 3\n| 1.4142157\n| --\n|-\n| 4\n| 1.4142136\n| --\n|}\n\nIt is worth noting here that Aitken's method does not save two iteration steps; computation of the first three ''Ax'' values required the first five ''x'' values.  Also, the second Ax value is decidedly inferior to the 4th x value, mostly due to the fact that Aitken's process assumes linear, rather than quadratic, convergence{{cn|date=April 2017}}.\n\n'''Example 2''': The value of <math>\\frac{\\pi}{4}</math> may be calculated as an infinite sum:\n\n:: <math>\\frac{\\pi}{4} = \\sum_{n=0}^\\infty \\frac{(-1)^n}{2n+1} \\approx 0.785398</math>\n\n{| class=\"wikitable\"\n|-\n| ''n''\n| term\n| ''x'' = partial sum\n| ''Ax''\n|-\n| 0\n| 1\n| 1\n| 0.79166667\n|-\n| 1\n| &minus;0.33333333\n| 0.66666667\n| 0.78333333\n|-\n| 2\n| 0.2\n| 0.86666667\n| 0.78630952\n|-\n| 3\n| &minus;0.14285714\n| 0.72380952\n| 0.78492063\n|-\n| 4\n| 0.11111111\n| 0.83492063\n| 0.78567821\n|-\n| 5\n| &minus;9.0909091&times;10<sup>&minus;2</sup>\n| 0.74401154\n| 0.78522034\n|-\n| 6\n| 7.6923077&times;10<sup>&minus;2</sup>\n| 0.82093462\n| 0.78551795\n|-\n| 7\n| -6.6666667&times;10<sup>&minus;2</sup>\n| 0.75426795\n| --\n|-\n| 8\n| 5.8823529&times;10<sup>&minus;2</sup>\n| 0.81309148\n| --\n|}\n\nIn this example, Aitken's method is applied to a sublinearly converging series, accelerating convergence considerably.  It is still sublinear, but much faster than the original convergence{{cn|date=April 2017}}: the first Ax value, whose computation required the first three x values, is closer to the limit than the eighth x value.\n\n==Example pseudocode for Aitken extrapolation==\n\nThe following is an example of using the Aitken extrapolation to help find the limit of the sequence <math>x_{n+1} = f(x_n)</math> when given <math>x_0</math>, which we assume to be the fixed point <math> \\alpha = f(\\alpha)</math>. For instance, we could have <math>x_{n+1} = \\frac{1}{2}(x_n + \\frac{2}{x_n})</math> with <math>x_0 = 1</math> which has the fixed point <math>\\sqrt{2}</math> so that <math>f(x) = \\frac{1}{2}(x + \\frac{2}{x})</math> (see [[Methods of computing square roots]]).\n\nThis pseudo code also computes the Aitken approximation to <math>f'(\\alpha)</math>. The Aitken extrapolates will be denoted by <code>aitkenX</code>. We must check if during the computation of the extrapolate the denominator becomes too small, which could happen if we already have a large amount of accuracy, since otherwise a large amount of error could be introduced. We denote this small number by <code>epsilon</code>.\n\n<source lang=\"matlab\">\n%These choices depend on the problem being solved\nx0 = 1                      %The initial value\nf(x) = (1/2)*(x + 2/x)      %The function that finds the next element in the sequence\ntolerance = 10^-10          %10 digit accuracy is desired\nepsilon = 10^-16            %Don't want to divide by a number smaller than this\n\nmaxIterations = 20          %Don't allow the iterations to continue indefinitely\nhaveWeFoundSolution = false %Were we able to find the solution to within the desired tolerance? not yet.\n\nfor i = 1 : maxIterations \n    x1 = f(x0)\n    x2 = f(x1)\n\n    if(x1 ~= x0)\n        lambda = absoluteValue((x2 - x1)/(x1 - x0))  %OPTIONAL: computes an approximation of |f'(fixedPoint)|, which is denoted by lambda\n    end\n\n    denominator = (x2 - x1) - (x1 - x0);\n\n    if(absoluteValue(denominator) < epsilon)          %Don't want to divide by too small of a number\n        print('WARNING: denominator is too small')\n        break;                                        %Leave the loop\n    end\n\n    aitkenX = x2 - ( (x2 - x1)^2 )/denominator\n    \n    if(absoluteValue(aitkenX - x2) < tolerance)       %If the result is within tolerance\n        print(\"The fixed point is \", aitkenX))        %Display the result of the Aitken extrapolation\n        haveWeFoundSolution = true\n        break;                                        %Done, so leave the loop\n    end\n\n    x0 = aitkenX                                      %Update x0 to start again                  \n    \nend\n\nif(haveWeFoundSolution == false)   %If we weren't able to find a solution to within the desired tolerance\n    print(\"Warning: Not able to find solution to within the desired tolerance of \", tolerance)\n    print(\"The last computed extrapolate was \", aitkenX)\nend\n</source>\n\n==See also==\n* [[Rate of convergence]]\n* [[Limit of a sequence]]\n* [[Fixed point iteration]]\n* [[Richardson extrapolation]]\n* [[Sequence transformation]]\n* [[Shanks transformation]]\n* [[Steffensen's method]]\n\n==Notes==\n<references/>\n\n==References==\n* William H. Press, ''et al.'', ''Numerical Recipes in C'', (1987) Cambridge University Press, {{ISBN|0-521-43108-5}} ''(See [http://apps.nrbook.com/c/index.html section 5.1])''\n* Abramowitz and Stegun, ''[[Abramowitz and Stegun|Handbook of Mathematical Functions]]'', section 3.9.7\n* Kendall E. Atkinson, ''An Introduction to Numerical Analysis'', (1989) John Wiley & Sons, Inc, {{ISBN|0-471-62489-6}}\n\n{{DEFAULTSORT:Aitken's Delta-Squared Process}}\n[[Category:Numerical analysis]]"
    },
    {
      "title": "The Algebraic Eigenvalue Problem",
      "url": "https://en.wikipedia.org/wiki/The_Algebraic_Eigenvalue_Problem",
      "text": "#REDIRECT [[James H. Wilkinson#AEP]]\n\n{{Redirect category shell|1=\n{{R to related topic}}\n{{R with possibilities}}\n}}\n<!-- \"Algebraic eigenvalue problem\" (with lower-case initial letters and without the definite article) may be an appropriate title for an article about a problem in mathematics.  The present title with capital letters is appropriate only if there is to be an article about the book of that title. -->\n\n{{DEFAULTSORT:Algebraic Eigenvalue Problem}}\n[[Category:1965 books]]\n[[Category:Mathematics books]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Applied element method",
      "url": "https://en.wikipedia.org/wiki/Applied_element_method",
      "text": "The '''applied element method''' ('''AEM''') is a numerical analysis used in predicting the [[Linear continuum|continuum]] and [[Discrete mathematics|discrete]] behavior of structures.  The modeling method in AEM adopts the concept of discrete cracking allowing it to automatically track [[Structural failure|structural collapse]] behavior passing through all stages of loading: elastic, [[Crack propagation|crack initiation and propagation]] in tension-weak materials, reinforcement [[Yield (engineering)|yield]], element separation, element contact and [[collision]], as well as collision with the ground and adjacent structures.\n\n==History==\nExploration of the approach employed in the applied element method began in 1995 at the [[University of Tokyo]] as part of Dr.Hatem Tagel-Din's research studies. The term \"applied element method\" itself, however, was first coined in 2000 in a paper called \"Applied element method for structural analysis: Theory and application for linear materials\".<ref name=AEMTheory>{{cite journal|last= |first= |authorlink= |last1=Meguro |first1=K. |last2=Tagel-Din |first2=H. |title=Applied element method for structural analysis: Theory and application for linear materials |journal=Structural engineering/earthquake engineering. |volume=17 |issue=1 |pages=21–35 |publisher=Japan Society of Civil Engineers(JSCE) |location=Japan |year=2000 |url=http://sciencelinks.jp/j-east/article/200014/000020001400A0511912.php |doi= |id=F0028A |accessdate=2009-08-10 |deadurl=yes |archiveurl=https://web.archive.org/web/20120229032846/http://sciencelinks.jp/j-east/article/200014/000020001400A0511912.php |archivedate=2012-02-29 }}</ref> Since then AEM has been the subject of research by a number of [[academic institution]]s and the driving factor in real-world applications.  Research has verified its accuracy for: elastic analysis;<ref name=\"AEMTheory\"/> crack initiation and propagation; estimation of [[Structural failure|failure loads]] at reinforced concrete structures;<ref>{{cite journal | last = | first = | authorlink = |last1=Tagel-Din|first1= H.|last2=Meguro|first2= K| title = Applied Element Method for Simulation of Nonlinear Materials: Theory and Application for RC Structures | journal = Structural engineering/earthquake engineering  | volume = 17 | issue = 2 | pages = 137–148 | publisher = Japan Society of Civil Engineers(JSCE) | location = Japan | year = 2000  | url = http://www.jsce.or.jp/publication/e/book/book_seee.html#vol17| doi =  | id =  | accessdate = 2009-08-10}}</ref> [[reinforced concrete]] structures under cyclic loading;<ref>{{cite journal | last = | first = | authorlink = |last1=Tagel-Din|first1= H.|last2=Meguro|first2= Kimiro| title = Applied Element Simulation of RC Structures under Cyclic Loading | journal = Journal of Structural Engineering  | volume = 127 | issue = 11 | pages = 137–148 |doi=10.1061/(ASCE)0733-9445(2001)127:11(1295)| publisher = ASCE | location = Japan | date = November 2001  | url = http://cedb.asce.org/cgi/WWWdisplay.cgi?0106179 | issn = 0733-9445 | id =  | accessdate = 2009-08-10}}</ref>  [[buckling]] and post-buckling behavior;<ref>{{cite journal|last= |first= |authorlink= |last1=Tagel-Din |first1=H. |last2=Meguro |first2=K |title=AEM Used for Large Displacement Structure Analysis |journal=Journal of Natural Disaster Science |volume=24 |issue=1 |pages=25–34 |publisher= |location=Japan |year=2002 |url=http://www.drs.dpri.kyoto-u.ac.jp/jsnds/download.cgi?jsdn_24_1-3.pdf |issn= |doi= |id= |accessdate=2009-08-10 }}{{dead link|date=July 2017 |bot=InternetArchiveBot |fix-attempted=yes }}</ref> nonlinear dynamic analysis of structures subjected to severe earthquakes;<ref>{{cite conference | last = | first = | authorlink = |first1=Hatem|last1=Tagel-Din|last2=Kimiro Meguro|first2= K| title = Analysis of a Small Scale RC Building Subjected to Shaking Table Tests using Applied Element Method | publisher = Proceedings of the 12th World Conference on Earthquake Engineering | pages = 25–34 | location = New Zealand | date = January 30 – February 4, 2000  | url =  | issn =  | doi =  | id =  | accessdate = }}</ref> fault-rupture propagation;<ref>{{cite conference | last = | first = | authorlink = |first1=Tagel-Din|last1=HATEM|last2=Kimiro MEGURO|first2= K| title = Dynamic Modeling of Dip-Slip Faults for Studying Ground Surface Deformation Using Applied Element Method | publisher = Proceedings of the 13th World Conference on Earthquake Engineering | pages =  | location = Vancouver, Canada | date = August 1–6, 2004 | url =  | issn =  | doi =  | id =  | accessdate = }}</ref> nonlinear behavior of brick structures;<ref>{{cite journal | last = | first = | authorlink = |first1=Paola|last1=Mayorka|last2=Kimiro Meguro|first2= K| title = Modeling Masonry Structures using the Applied Element Method | journal = Seisan Kenkyu | volume = 55 | issue = 6 | publisher = Institute of Industrial Science, The University of Tokyo | pages = 123–126 | location = Japan | date = October 2003 | url = http://www.jstage.jst.go.jp/article/seisankenkyu/55/6/581/_pdf | issn = 1881-2058 | doi =  | id =  | accessdate = 2009-08-10}}</ref> and the analysis of [[Glass-reinforced plastic|glass reinforced polymers]] (GFRP) walls under blast loads.<ref>{{Cite book | last = | first = | authorlink = |first1=Paola|last1=Mayorka|last2=Kimiro Meguro|first2= K| title = Blast Testing and Research Bridge at the Tenza Viaduct | publisher = University of Missouri-Rolla, TSWG Contract Number N4175-05-R-4828, Final Report of Task 1| location = Japan | year = 2005 | url =  | issn = | doi =  | id = <!-- | accessdate = 2009-08-10 -->}}</ref>\n\n==Technical discussion==\nIn AEM, the structure is divided virtually and modeled as an assemblage of relatively small elements.  The elements are then connected through a set of normal and shear springs located at contact points distributed along the element faces. Normal and shear springs are responsible for the transfer of [[Normal stress|normal]] and [[Shear stress|shear]] stresses from one element to the next.\n\n===Element generation and formulation===\nThe modeling of objects in AEM is very similar to modeling objects in [[Finite element method|FEM]]. Each object is divided into a series of elements connected and forming a mesh.  The main difference between AEM and FEM, however, is how the elements are joined together.  In AEM the elements are connected by a series of [[Nonlinear system|non-linear]] springs representing the material behavior.\n\nThere are three types of springs used in AEM:\n*'''Matrix Springs''':  Matrix springs connect two elements together representing the main [[material properties]] of the object.\n*'''Reinforcing Bar Springs''':  Reinforcement springs are used to implicitly represent additional reinforcement bars running through the object without adding additional elements to the analysis.\n*'''Contact Springs''':  Contact Springs are generated when two elements collide with each other or the ground.  When this occurs three springs are generated (Shear Y, Shear X and Normal).\n\n===Automatic element separation===\nWhen the average strain value at the element face reaches the separation strain, all springs at this face are removed and elements are no longer connected until a collision occurs, at which point they collide together as rigid bodies.\n\nSeparation strain represents the strain at which adjacent elements are totally separated at the connecting face. This parameter is not available in the elastic material model. For concrete, all springs between the adjacent faces including reinforcement bar springs are cut. If the elements meet again, they will behave as two different rigid bodies that have now contacted each other. For steel, the bars are cut if the stress point reaches [[Ultimate tensile stress|ultimate stress]] or if the concrete reaches the [[Deformation (mechanics)|separation strain]].\n\n===Automatic element contact/collision===\nContact or collision is detected without any user intervention. Elements are able to separate, contract and/or make contact with other elements. In AEM three contact methods include Corner-to-Face, Edge-to-Edge, and Corner-to-Ground.\n\n==Stiffness matrix==\nThe spring stiffness in a 2D model can be calculated from the following equations:\n\n: <math>K_n=\\frac{E\\cdot T\\cdot d}{a}</math>\n: <math>K_s=\\frac{G\\cdot T\\cdot d}{a}</math>\n\nWhere ''d'' is the distance between springs, ''T'' is the thickness of the element, ''a'' is the length of the representative area, ''E'' is the [[Young's modulus]], and ''G'' is the [[shear modulus]] of the material. The above equation's indicate that each spring represents the stiffness of an area (''T''·''d'') within the length of the studied material.\n\nTo model reinforcement bars embedded in concrete, a spring is placed inside the element at the location of the bar; the area (''T''·''d'') is replaced by the actual cross section area of the reinforcement bar. Similar to modeling embedded [[steel sections]], the area (''T''·''d'') may be replaced by the area of the steel section represented by the spring.\n\nAlthough the element motion moves as a [[rigid body]], its internal [[Deformation (engineering)|deformations]] are represented by the spring deformation around each element. This means the element shape does not change during analysis, but the behavior of assembly of elements is deformable.\nThe two elements are assumed to be connected by only one pair of normal and shear springs. To have a general stiffness matrix, the locations of element and contact springs are assumed in a general position. The stiffness matrix components corresponding to each [[Degrees of freedom (physics and chemistry)|degree of freedom]] are determined by assuming a unit [[Displacement (vector)|displacement]] in the studied direction and by determining forces at the [[centroid]] of each element. The 2D element stiffness matrix size is 6 × 6; the components of the upper left quarter of the [[stiffness matrix]] are shown below:\n\n: <math>\\begin{bmatrix}\n  \\sin^2 (\\theta+\\alpha)K_n & -K_n \\sin(\\theta+\\alpha)\\cos(\\theta+\\alpha) &  \\cos(\\theta+\\alpha)K_s L\\sin(\\alpha) \\\\\n  +\\cos^2(\\theta+\\alpha)K_s &  +K_s\\sin(\\theta+\\alpha)\\cos(\\theta+\\alpha) & -\\sin(\\theta+\\alpha)K_n L\\cos(\\alpha) \\\\\n  \\\\\n  -K_n\\sin(\\theta+\\alpha)\\cos(\\theta+\\alpha) & \\sin^2(\\theta+\\alpha)K_s &  \\cos(\\theta+\\alpha)K_n L\\cos(\\alpha) \\\\\n  +K_s\\sin(\\theta+\\alpha)\\cos(\\theta+\\alpha) & +\\cos^2(\\theta+\\alpha)K_n &  +\\sin(\\theta+\\alpha)K_s L\\sin(\\alpha) \\\\\n  \\\\\n  \\cos(\\theta+\\alpha)K_s L\\sin(\\alpha) &  \\cos(\\theta+\\alpha)K_n L\\cos(\\alpha) & L^2\\cos^2(\\alpha)K_n \\\\\n  -\\sin(\\theta+\\alpha)K_n L\\cos(\\alpha) &  +\\sin(\\theta+\\alpha)K_s L\\sin(\\alpha) & +L^2\\sin^2(\\alpha)K_s\n\\end{bmatrix}</math>\n\nThe stiffness matrix depends on the contact spring stiffness and the spring location. The stiffness matrix is for only one pair of contact springs. However, the global stiffness matrix is determined by summing up the stiffness matrices of individual pairs of springs around each element. Consequently, the developed stiffness matrix has total effects from all pairs of springs, according to the stress situation around the element. This technique can be used in both [[Structural load|load]] and displacement control cases. The 3D stiffness matrix may be deduced similarly.\n\n==Applications==\nThe applied element method is currently being used in the following applications:\n*Structural vulnerability assessment\n**[[Progressive collapse]]\n**Blast analysis\n**Impact analysis\n**[[Seismic analysis]]\n*[[Forensic engineering]]\n*Performance based design\n*Demolition analysis\n*Glass performance analysis\n*[[Visual Effects|Visual effects]]\n\n==See also==\n* [[Building implosion]]\n* [[Earthquake engineering]]\n* [[Extreme Loading for Structures]]\n* [[Failure analysis]]\n* [[Multidisciplinary design optimization]]\n* [[Physics engine]]\n* [[Progressive collapse]]\n* [[Shear modulus]]\n* [[Structural engineering]]\n* [[Young's modulus]]\n\n==References==\n{{reflist}}\n\n==Further reading==\n*[http://www.appliedelementmethod.com/ Applied Element Method]\n*[https://www.extremeloading.com/extreme-loading-technology/ Extreme Loading for Structures - Applied Element Method]\n\n{{DEFAULTSORT:Applied Element Method}}\n[[Category:Structural analysis]]\n[[Category:Structural engineering]]\n[[Category:Construction]]\n[[Category:Demolition]]\n[[Category:Building engineering]]\n[[Category:Glass engineering and science]]\n[[Category:Numerical analysis]]\n[[Category:Scientific simulation software]]"
    },
    {
      "title": "Approximation error",
      "url": "https://en.wikipedia.org/wiki/Approximation_error",
      "text": "{{broader|Approximation}}\n[[File:E^x with linear approximation.png|thumb|Graph of <math>f(x) = e^x</math> (blue) with its linear approximation <math>P_1(x) = 1 + x</math> (red) at a = 0. The approximation error is the gap between the curves, and it increases for x values further from 0.]]\nThe '''approximation error''' in some data is the discrepancy between an exact value and some approximation to it. An approximation error can occur because:\n#the [[measurement]] of the [[data]] is not precise due to the instruments. (e.g., the accurate reading of a piece of paper is 4.5&nbsp;cm but since the ruler does not use decimals, you round it to 5&nbsp;cm.) or\n#approximations are used instead of the real data (e.g., 3.14 instead of [[pi|π]]).\n\nIn the [[mathematics|mathematical]] field of [[numerical analysis]], the [[numerical stability]] of an [[algorithm]] indicates how the error is propagated by the algorithm.\n\n==Formal Definition==\nOne commonly distinguishes between the '''relative error''' and the '''absolute error'''.\n\nGiven some value ''v'' and its approximation ''v''<sub>approx</sub>, the '''absolute error''' is\n\n:<math>\\epsilon = |v-v_\\text{approx}|\\ ,</math>\n\nwhere the vertical bars denote the [[absolute value]]. \nIf <math>v \\ne 0,</math> the '''relative error''' is\n\n:<math> \\eta = \\frac{\\epsilon}{|v|}\n    = \\left| \\frac{v-v_\\text{approx}}{v} \\right|\n    = \\left| 1 - \\frac{v_\\text{approx}}{v} \\right|,\n</math>\n\nand the '''percent error''' is\n\n:<math>\\delta = 100\\%\\times\\eta = 100\\%\\times\\frac{\\epsilon}{|v|} = 100\\%\\times\\left| \\frac{v-v_\\text{approx}}{v} \\right|.</math>\n\nIn words, the absolute error is the [[Magnitude (mathematics)|magnitude]] of the difference between the exact value and the approximation. The relative error is the absolute error divided by the magnitude of the exact value. The percent error is the relative error expressed in terms of per 100.\n\n===Generalizations===\nThese definitions can be extended to the case when <math>v</math> and <math>v_{\\text{approx}}</math> are [[Euclidean vector|''n''-dimensional vectors]], by replacing the absolute value with an [[norm (mathematics)|''n''-norm]].<ref name=\"GOLUB_MAT_COMP2.2.3\">{{cite book|last=Golub|first=Gene|authorlink=Gene_H._Golub|author2=Charles F. Van Loan|title=Matrix Computations – Third Edition|publisher=The Johns Hopkins University Press|year=1996|location=Baltimore|pages=53|isbn=0-8018-5413-X}}\n</ref>\n\n==Examples==\nAs an example, if the exact value is 50 and the approximation is 49.9, then the absolute error is 0.1 and the relative error is 0.1/50 = 0.002 = 0.2%.  Another example would be if, in measuring a 6&nbsp;mL beaker, the value read was 5&nbsp;mL. The correct reading being 6&nbsp;mL, this means the percent error in that particular situation is, rounded, 16.7%.\n\n==Uses of relative error==\nThe relative error is often used to compare approximations of numbers of widely differing size; for example, approximating the number 1,000 with an absolute error of 3 is, in most applications, much worse than approximating the number 1,000,000 with an absolute error of 3; in the first case the relative error is 0.003 and in the second it is only&nbsp;0.000003.\n\nThere are two features of relative error that should be kept in mind.  Firstly, relative error is undefined when the true value is zero as it appears in the denominator (see below).  Secondly, relative error only makes sense when measured on a [[Level_of_measurement#Ratio_scale|ratio scale]], (i.e. a scale which has a true meaningful zero), otherwise it would be sensitive to the measurement units.  For example, when an absolute error in a [[temperature]] measurement given in [[Celsius scale]] is 1&nbsp;°C, and the true value is 2&nbsp;°C, the relative error is 0.5, and the percent error is 50%.  For this same case, when the temperature is given in [[Kelvin scale]], the same 1&nbsp;K absolute error with the same true value of 275.15&nbsp;K gives a relative error of 3.63{{e|-3}} and a percent error of only 0.363%.  Celsius temperature is measured on an [[Level_of_measurement#Interval_scale|interval scale]], whereas the Kelvin scale has a true zero and so is a ratio scale.\n\n==Instruments==\nIn most indicating instruments, the accuracy is guaranteed to a certain percentage of full-scale reading. The limits of these deviations from the specified values are known as limiting errors or guarantee errors.<ref>Helfrick, Albert D. (2005) ''Modern Electronic Instrumentation and Measurement Techniques''. p. 16. {{ISBN|81-297-0731-4}}</ref>\n\n==See also==\n*[[Accepted and experimental value]]\n*[[Relative difference]]\n*[[Uncertainty]]\n*[[Experimental uncertainty analysis]]\n*[[Propagation of uncertainty]]\n*[[Errors and residuals in statistics]]\n*[[Round-off error]]\n*[[Quantization error]]\n*[[Measurement uncertainty]]\n*[[Measurement error]]\n*[[Machine epsilon]]\n\n==References==\n\n<!--<nowiki>\nSee http://en.wikipedia.org/wiki/Wikipedia:Footnotes for an explanation of how to generate footnotes using the <ref> and </ref> tags, and the template below. \n</nowiki>-->\n{{reflist}}\n\n==External links==\n*{{MathWorld|PercentageError|Percentage error}}\n\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Artificial precision",
      "url": "https://en.wikipedia.org/wiki/Artificial_precision",
      "text": "{{Orphan|date=September 2013}}\n\nIn [[numerical mathematics]], '''artificial precision''' is a source of error that occurs when a numerical value or semantic is expressed with more [[Significant figures|precision]] than was initially provided from measurement or user input.\nFor example, a person enters their birthday as the date 1984-01-01 but it is stored in a database as 1984-01-01T00:00:00Z which introduces the artificial precision of the hour, minute, and second they were born, and may even affect the date, depending on the user's actual place of birth. This is also an example of [[false precision]], which is artificial precision specifically of numerical quantities or measures.\n\n== See also ==\n* [[false precision]]\n* [[accuracy and precision]]\n* [[significant figures]]\n\n== References ==\n* {{Cite book | last1 = Smith | first1 = N. J. J. | chapter = Worldly Vagueness and Semantic Indeterminacy | doi = 10.1093/acprof:oso/9780199233007.003.0007 | title = Vagueness and Degrees of Truth | pages = 277 | year = 2008 | isbn = 9780199233007 | pmid =  | pmc = }}\n\n[[Category:Computational statistics]]\n[[Category:Numerical analysis]]\n\n\n{{applied-math-stub}}\n{{statistics-stub}}"
    },
    {
      "title": "Basis function",
      "url": "https://en.wikipedia.org/wiki/Basis_function",
      "text": "{{more footnotes|date=March 2013}}\nIn [[mathematics]],  a '''basis function''' is an element of a particular [[Basis (linear algebra)|basis]] for a [[function space]]. Every continuous function in the function space can be represented as a [[linear combination]] of basis functions, just as every vector in a [[vector space]] can be represented as a linear combination of [[basis vectors]].\n\nIn [[numerical analysis]] and [[approximation theory]], basis functions are also called '''blending functions,''' because of their use in [[interpolation]]: In this application, a mixture of the basis functions provides an interpolating function (with the \"blend\" depending on the evaluation of the basis functions at the data points).\n\n==Examples==\n\n===Polynomial bases===\nThe base of a polynomial is the factored polynomial equation into a linear function.<ref>{{Cite journal|date=2007-08-01|title=Solutions of differential equations in a Bernstein polynomial basis|url=https://www.sciencedirect.com/science/article/pii/S0377042706003153|journal=Journal of Computational and Applied Mathematics|language=en|volume=205|issue=1|pages=272–280|doi=10.1016/j.cam.2006.05.002|issn=0377-0427}}</ref>\n\n===Fourier basis===\nSines and cosines form an ([[orthonormality|orthonormal]]) [[Schauder basis]] for [[square-integrable function]]s. As a particular example, the collection:\n:<math>\\{\\sqrt{2}\\sin(2\\pi n x) \\; | \\; n\\in\\mathbb{N} \\} \\cup \\{\\sqrt{2} \\cos(2\\pi n x) \\; | \\; n\\in\\mathbb{N} \\} \\cup\\{1\\}</math>\nforms a basis for [[Lp space|L<sup>2</sup>(0,1)]].\n\n==References==\n*{{cite book |last=Ito |first=Kiyoshi |authorlink= |coauthors= |others= |title=Encyclopedic Dictionary of Mathematics |edition=2nd |year=1993 |publisher=MIT Press |location= |isbn=0-262-59020-4 | page=1141}}\n\n==See also==\n{{col-begin}}\n{{col-1-of-3}}\n* [[Basis (linear algebra)]]  ([[Hamel basis]])\n* [[Schauder basis]] (in a [[Banach space]])\n* [[Dual basis]]\n* [[Biorthogonal system]] (Markushevich basis)\n{{col-2-of-3}}\n* [[Orthonormal basis]] in an [[inner-product space]]\n* [[Orthogonal polynomials]]\n* [[Fourier analysis]] and [[Fourier series]]\n* [[Harmonic analysis]]\n* [[Orthogonal wavelet]]\n* [[Biorthogonal wavelet]]\n{{col-3-of-3}}\n* [[Radial basis function]] <!-- shape functions in the [[Galerkin method]] and -->\n* [[Finite element analysis#Choosing a basis|Finite-elements (bases)]]\n* [[Functional analysis]]\n* [[Approximation theory]]\n* [[Numerical analysis]]\n\n{{col-end}}\n\n==References==\n<references />\n\n[[Category:Numerical analysis]]\n[[Category:Fourier analysis]]\n[[Category:Linear algebra]]\n[[Category:Numerical linear algebra]]\n[[Category:Types of functions]]"
    },
    {
      "title": "Bernstein polynomial",
      "url": "https://en.wikipedia.org/wiki/Bernstein_polynomial",
      "text": "{{for|the Bernstein polynomial in [[D-module]] theory|Bernstein&ndash;Sato polynomial}}\n{{more footnotes|date=June 2016}}[[Image:Bernstein Approximation.gif|thumb|right|Bernstein polynomials approximating a curve]]\nIn the [[mathematics|mathematical]] field of [[numerical analysis]], a '''Bernstein polynomial''', named after [[Sergei Natanovich Bernstein]], is a [[polynomial]] in the '''Bernstein form''', that is a [[linear combination]] of '''Bernstein basis polynomials'''.\n\nA [[numerical stability|numerically stable]] way to evaluate polynomials in Bernstein form is [[de Casteljau's algorithm]].\n\nPolynomials in Bernstein form were first used by Bernstein in a constructive proof for the [[Stone–Weierstrass theorem|Stone–Weierstrass approximation theorem]]. With the advent of computer graphics, Bernstein polynomials, restricted to the interval [0,&nbsp;1], became important in the form of [[Bézier curve]]s.\n\n==Definition==\n[[File:Bernstein Polynomials.svg|thumb|Bernstein basis polynomials for 4th degree curve blending]]\nThe ''n''&nbsp;+&nbsp;1 '''Bernstein basis polynomials''' of degree ''n'' are defined as\n\n: <math>b_{\\nu,n}(x) = \\binom{n}{\\nu} x^{\\nu} \\left( 1 - x \\right)^{n - \\nu}, \\quad \\nu = 0, \\ldots, n,</math>\n\nwhere <math>\\tbinom{n}{\\nu}</math> is a [[binomial coefficient]].  So, for example, <math>b_{2,5}(x) = \\tbinom{5}{2}x^2(1-x)^3 = 10x^2(1-x)^3.</math>\n\nThe Bernstein basis polynomials of degree ''n'' form a [[basis (linear algebra)|basis]] for the [[vector space]] Π<sub>''n''</sub> of polynomials of degree at most&nbsp;''n'' with real coefficients.  A linear combination of Bernstein basis polynomials\n\n:<math>B_n(x) = \\sum_{\\nu=0}^{n} \\beta_{\\nu} b_{\\nu,n}(x)</math>\n\nis called a '''Bernstein polynomial''' or '''polynomial in Bernstein form''' of degree&nbsp;''n''.<ref>G. G. Lorentz (1953) ''Bernstein Polynomials'', [[University of Toronto Press]]</ref>  The coefficients <math>\\beta_\\nu</math> are called '''Bernstein coefficients''' or '''Bézier coefficients'''.\n\n==Properties==\nThe Bernstein basis polynomials have the following properties:\n* <math>b_{\\nu, n}(x) = 0</math>, if <math>\\nu < 0</math> or <math>\\nu > n</math>.\n* <math>b_{\\nu, n}(x) \\ge 0</math> for <math>x \\in [0,\\ 1]</math>.\n* <math>b_{\\nu, n}\\left( 1 - x \\right) = b_{n - \\nu, n}(x)</math>.\n* <math>b_{\\nu, n}(0) = \\delta_{\\nu, 0}</math> and <math>b_{\\nu, n}(1) = \\delta_{\\nu, n}</math> where <math>\\delta</math> is the [[Kronecker delta]] function: <math>\\delta_{ij} = \\begin{cases}\n0 &\\text{if } i \\neq j,   \\\\\n1 &\\text{if } i=j.   \\end{cases}</math>\n* <math>b_{\\nu, n}(x)</math> has a root with multiplicity <math>\\nu</math> at point <math>x = 0</math> (note: if <math>\\nu = 0</math>, there is no root at 0).\n* <math>b_{\\nu, n}(x)</math> has a root with multiplicity <math>\\left( n - \\nu \\right)</math> at point <math>x = 1</math> (note: if <math>\\nu = n</math>, there is no root at 1).\n* The [[derivative]] can be written as a combination of two polynomials of lower degree:\n*: <math>b'_{\\nu, n}(x) = n \\left( b_{\\nu - 1, n - 1}(x) - b_{\\nu, n - 1}(x) \\right).</math>\n* The transformation of the Bernstein polynomial to monomials is\n*: <math>b_{\\nu,n}(x) = \\binom{n}{\\nu}\\sum_{k=0}^{n-\\nu} \\binom{n-\\nu}{k}(-1)^{n-\\nu-k} x^{\\nu+k} = \\sum_{\\ell=\\nu}^n \\binom{n}{\\ell}\\binom{\\ell}{\\nu}(-1)^{\\ell-\\nu}x^\\ell</math>\nand by the [[Binomial transform|inverse binomial transformation]] the reverse transformation is<ref>{{cite arxiv |eprint=1802.09518 |first=R. J. |last=Mathar |year=2018 |title=Orthogonal basis function over the unit circle with the minimax property |at=Appendix&nbsp;B}}</ref>\n: <math>x^k=\\sum_{i=0}^{n-k}\\binom{n-k}{i}\\frac{1}{\\binom{n}{i}}b_{n-i,n}(x)\n=\\frac{1}{\\binom{n}{k}} \\sum_{j=k}^n \\binom{j}{k}b_{j,n}(x)\\;</math>.\n* The indefinite [[integral]] is given by:\n*: <math>\\int b_{\\nu, n}(x)dx = \\frac{1}{n+1} \\sum_{j=\\nu+1}^{n+1} b_{j, n+1}(x) </math> \n* The definite integral is constant for a given <math>n</math>\n*: <math>\\int_{0}^{1}b_{\\nu, n}(x)dx = \\frac{1}{n+1}        ; \\forall \\nu = 0,1, \\dots, n</math>\n* If <math>n \\ne 0</math>, then <math>b_{\\nu, n}(x)</math> has a unique local maximum on the interval <math>[0,\\ 1]</math> at <math>x = \\frac{\\nu}{n}</math>. This maximum takes the value:\n*: <math>\\nu^\\nu n^{-n} \\left( n - \\nu \\right)^{n - \\nu} {n \\choose \\nu}.</math>\n* The Bernstein basis polynomials of degree <math>n</math> form a [[partition of unity]]:\n*: <math>\\sum_{\\nu = 0}^n b_{\\nu, n}(x) = \\sum_{\\mu = 0}^n {n \\choose \\mu} x^\\mu \\left( 1 - x \\right)^{n - \\mu} = \\left(x + \\left( 1 - x \\right) \\right)^n = 1.</math>\n* By taking the first derivative of <math>(x+y)^n</math> where <math>y = 1-x</math>, it can be shown that\n*: <math>\\sum_{\\nu=0}^{n}\\nu b_{\\nu, n}(x) = nx</math>\n* The second derivative of <math>(x+y)^n</math> where <math>y = 1-x</math> can be used to show\n*: <math>\\sum_{\\nu=1}^{n}\\nu(\\nu-1) b_{\\nu, n}(x) = n(n-1)x^2</math>\n* A Bernstein polynomial can always be written as a linear combination of polynomials of higher degree:\n*: <math>b_{\\nu, n - 1}(x) = \\frac{n - \\nu}{n} b_{\\nu, n}(x) + \\frac{\\nu + 1}{n} b_{\\nu + 1, n}(x).</math>\n\n==Approximating continuous functions==\nLet ''&fnof;'' be a [[continuous function]] on the interval [0,&nbsp;1]. Consider the Bernstein polynomial\n:<math>B_n(f)(x) = \\sum_{\\nu = 0}^n f\\left( \\frac{\\nu}{n} \\right) b_{\\nu,n}(x).</math>\n\nIt can be shown that\n:<math>\\lim_{n \\to \\infty}{ B_n(f) } = f </math>\n\n[[uniform convergence|uniformly]] on the interval&nbsp;[0,&nbsp;1].<ref name=Nat6>Natanson (1964) p.&nbsp;6</ref> The word [[uniform convergence|''uniformly'']] signifies that the polynomial converges on the entire interval [0,&nbsp;1] at the same rate (or better). This is a more stringent form of convergence than [[pointwise convergence]], which only requires that the limit is achieved at each value of ''x'' on [0,&nbsp;1], with (possibly) separate rates at each point. Specifically, [[uniform convergence]] assures that\n:<math>\\lim_{n \\to \\infty} \\sup \\left\\{\\, \\left| f(x) - B_n(f)(x) \\right| \\,:\\, 0 \\leq x \\leq 1 \\,\\right\\} = 0.</math>\n\nBernstein polynomials thus provide one way to prove the [[Stone&ndash;Weierstrass theorem#Weierstrass approximation theorem|Weierstrass approximation theorem]] that every real-valued continuous function on a real interval [''a'',&nbsp;''b''] can be uniformly approximated by polynomial functions over&nbsp;<math>\\mathbb R</math>.<ref name=Nat3>Natanson (1964) p.&nbsp;3</ref>\n\nA more general statement for a function with continuous ''k''<sup>th</sup> derivative is\n:<math>{\\left\\| B_n(f)^{(k)} \\right\\|}_\\infty \\le \\frac{ (n)_k }{ n^k } \\left\\| f^{(k)} \\right\\|_\\infty \\text{ and } \\left\\| f^{(k)}- B_n(f)^{(k)} \\right\\|_\\infty \\to 0</math>\n\nwhere additionally\n:<math>\\frac{ (n)_k }{ n^k } = \\left( 1 - \\frac{0}{n} \\right) \\left( 1 - \\frac{1}{n} \\right) \\cdots \\left( 1 - \\frac{k - 1}{n} \\right)</math>\n\nis an [[eigenvalue]] of ''B''<sub>''n''</sub>; the corresponding eigenfunction is a polynomial of degree&nbsp;''k''.\n\n===Proof===\nSuppose ''K'' is a [[random variable]] distributed as the number of successes in ''n'' independent [[Bernoulli trial]]s with probability ''x'' of success on each trial; in other words, ''K'' has a [[binomial distribution]] with parameters ''n'' and&nbsp;''x''. Then we have the [[expected value]] <math>\\operatorname{\\mathcal E}\\left[\\frac{K}{n}\\right] = x\\ </math> and\n:<math>p(K) = {n \\choose K} x^{K} \\left( 1 - x \\right)^{n - K} = b_{K,n}(x)</math>\n\nBy the [[law of large numbers|weak law of large numbers]] of [[probability theory]],\n:<math>\\lim_{n \\to \\infty}{ P\\left( \\left| \\frac{K}{n} - x \\right|>\\delta \\right) } = 0</math>\n\nfor every ''&delta;''&nbsp;>&nbsp;0. Moreover, this relation holds uniformly in ''x'', which can be seen from its proof via [[Chebyshev's inequality]], taking into account that the variance of {{frac|1|''n''}}&nbsp;''K'', equal to {{frac|1|''n''}}&nbsp;''x''(1&minus;''x''), is bounded from above by {{frac|1|(4''n'')}} irrespective of ''x''.\n\nBecause ''&fnof;'', being continuous on a closed bounded interval, must be [[uniform continuity|uniformly continuous]] on that interval, one infers a statement of the form\n:<math>\\lim_{n \\to \\infty}{ P\\left( \\left| f\\left( \\frac{K}{n} \\right) - f\\left( x \\right) \\right| > \\varepsilon \\right) } = 0</math>\n\nuniformly in ''x''. Taking into account that ''ƒ'' is bounded (on the given interval) one gets for the expectation\n: <math>\\lim_{n \\to \\infty}{ \\operatorname{\\mathcal E}\\left( \\left| f\\left( \\frac{K}{n} \\right) - f\\left( x \\right) \\right| \\right) } = 0</math>\nuniformly in ''x''. To this end one splits the sum for the expectation in two parts. On one part the difference does not exceed ''ε''; this part cannot contribute more than ''ε''.\nOn the other part the difference exceeds ''ε'', but does not exceed 2''M'', where ''M'' is an upper bound for |''&fnof;''(x)|; this part cannot contribute more than 2''M'' times the small probability that the difference exceeds ''ε''.\n\nFinally, one observes that the absolute value of the difference between expectations never exceeds the expectation of the absolute value of the difference, and \n:<math>\\operatorname{\\mathcal E}\\left[f\\left(\\frac{K}{n}\\right)\\right] = \\sum_{K=0}^n f\\left(\\frac{K}{n}\\right) p(K) = \\sum_{K=0}^n f\\left(\\frac{K}{n}\\right) b_{K,n}(x) = B_n(f)(x)</math>\n\nSee for instance Koralov & Sinai (2007).<ref>{{cite book |first1=L. |last1=Koralov |first2=Y. |last2=Sinai |title=Theory of probability and random processes |edition=2nd |publisher=Springer |year=2007 |page=29 |chapter=\"Probabilistic proof of the Weierstrass theorem\"}}</ref>\n\n==See also==\n*[[Polynomial interpolation]]\n*[[Newton polynomial|Newton form]]\n*[[Lagrange polynomial|Lagrange form]]\n*[[Binomial QMF]]\n\n==Notes==\n<references />\n\n==References==\n* {{cite journal | last1=Caglar | first1=Hakan | last2=Akansu | first2=Ali N. | title=A generalized parametric PR-QMF design technique based on Bernstein polynomial approximation | zbl=0825.93863 | journal=IEEE Transactions on Signal Processing | volume=41 | number=7 | pages=2314–2321 | date=July 1993 | doi=10.1109/78.224242}}\n* {{springer|title=Bernstein polynomials|id=B/b015730|last=Korovkin|first=P.P.}}\n* {{cite book | last=Natanson | first=I.P. | authorlink=Isidor Natanson | title=Constructive function theory. Volume I: Uniform approximation | translator=Alexis N. Obolensky | zbl=0133.31101 | mr=0196340 | location=New York | publisher=Frederick Ungar | year=1964 }}\n\n==External links==\n* {{cite journal|first1=Mark|last1=Kac| authorlink=Mark Kac|title=Une remarque sur les polynomes de M. S. Bernstein |journal = [[Studia Mathematica]] | year =1938| volume =7 | pages=49–51| doi=10.4064/sm-7-1-49-51}}\n* {{cite journal|first1=Richard Paul | last1=Kelisky | first2=Theodore Joseph | last2=Rivlin\n  |title=Iteratives of Bernstein Polynomials|year=1967 | journal=[[Pacific Journal of Mathematics]] | volume=21 | number=3 \n  | page =511|doi=10.2130/pjm.1967.21.511}}\n* {{cite book|first1=E. L. | last1=Stark|chapter = Bernstein Polynome, 1912-1955 | year=1981| doi=10.1007/978-3-0348-9-369-5_40|pages=443–461|editor-first=P.L. | editor-last=Butzer\n  |title=ISNM60|isbn=978-3-0348-9369-5}}\n* {{cite journal|first1=Sonia |last1=Petrone |authorlink= Sonia Petrone | title=Random Bernstein polynomials | journal=Scand. J. Stat. \n  | year=1999|volume=26|number=3|pages=373–393|doi=10.1111/1467-9469.00155}}\n* {{cite journal|first1=Halil |last1=Oruc |first2=Geoerge M. |last2=Phillips|title= A generalization of the Bernstein Polynomials\n  | year=1999 | journal=[[Edinburgh_Mathematical_Society#Journals|Proceedings of the Edinburgh Mathematical Society]]|volume=42 |pages=403–413 |doi=10.1017/S0013091500020332}}\n* {{cite web|first1=Kenneth I. |last1= Joy |year=2000 |url =http://www.idav.ucdavis.edu/education/CAGDNotes/Bernstein-Polynomials.pdf |title=Bernstein Polynomials}} from [[University of California, Davis]]. Note the error in the summation limits in the first formula on page 9.\n* {{cite journal|first1=M.|last1=Idrees Bhatti |first2=P. |last2=Bracken | title=Solutions of differential equations in a Bernstein Polynomial basis|\n   doi=10.1016/j.cam.2006.05.002|year=2007|journal=J. Comput. Appl. Math.|pages=272–280|volume=205}}\n* {{cite web|first1=Bill|last1=Casselman|authorlink=Bill Casselman (mathematician)|url=http://www.ams.org/featurecolumn/archive/bezier.html| title= From Bézier to Bernstein|year=2008}} Feature Column from [[American Mathematical Society]]\n* {{cite journal|first1=Mehmet | last1=Acikgoz |first2=Serkan |last2=Araci|title=On the generating function for Bernstein Polynomials | year=2010 | journal = AIP Conf. Proc. | doi=10.1063/1.3497855|volume=1281 |page=1141}}\n* {{cite journal|first1=E. H. |last1=Doha|first2=A. H.|last2=Bhrawy |first3=M. A. |last3=Saker\n  |title=Integrals of Bernstein polynomials: An application for the solution of high even-order differential equations\n  |doi=10.1016/j.aml.2010.11.013| year=2011 | journal=Appl. Math. Lett.|volume=24 | pages=559–565}}\n* {{cite journal|first1=Rida T.|last1=Farouki | title=The Bernstein polynomial basis: a centennial retrospective | year=2012|journal= Comp. Aid. Geom. Des.|volume=29|pages=379–419|doi=10.1016/j.cagd.2012.03.001}}\n* {{cite journal|first1=Xiaoyan|last1=Chen|first2=Jieqing|last2=Tan|first3=Zhi|last3=Liu|first4=Jin|last4=Xie| \n  title=Approximations of functions by a new family of generalized Bernstein operators|journal=J. Math. Ann. Applic. | year=2017 | volume=450 | pages=244–261 | doi=10.1016/j.jmaa.2016.12.075}}\n* {{mathworld|urlname=BernsteinPolynomial|title=Bernstein Polynomial}}\n* {{PlanetMath attribution|urlname=BernsteinPolynomial|title=properties of Bernstein polynomial}}\n\n{{DEFAULTSORT:Bernstein Polynomial}}\n[[Category:Numerical analysis]]\n[[Category:Polynomials]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "Bernstein's constant",
      "url": "https://en.wikipedia.org/wiki/Bernstein%27s_constant",
      "text": "{| class=\"infobox\" style =\"width: 370px;\"\n|[[Binary numeral system|Binary]]\n| 0.01000111101110010011000000110011…\n|-\n| [[Decimal]]\n| 0.280169499…\n|-\n| [[Hexadecimal]]\n| 0.47B930338AAD…\n|-\n| [[Continued fraction]]\n| <math>\\cfrac{1}{3 + \\cfrac{1}{1 + \\cfrac{1}{1 + \\cfrac{1}{3 + \\cfrac{1}{9+ \\ddots}}}}}</math>\n|}\n\n'''Bernstein's constant''', usually denoted by the Greek letter β ([[Beta (letter)|beta]]), is a [[mathematical constant]] named after [[Sergei Natanovich Bernstein]] and is equal to 0.2801694990... (sequence [http://oeis.org/A073001 A073001] in the OEIS).\n\n== Definition ==\nLet ''E''<sub>''n''</sub>(ƒ) be the error of the best [[uniform approximation]] to a [[real function]] ''ƒ''(''x'') on the interval [&minus;1,&nbsp;1] by real polynomials of no more than degree ''n''. In the case of ''ƒ''(''x'')&nbsp;=&nbsp;|''x''|, {{harvtxt|Bernstein|1914}} showed that the limit \n\n:<math>\\beta=\\lim_{n \\to \\infty}2nE_{2n}(f),\\,</math>\n\ncalled '''Bernstein's constant''', exists and is between 0.278 and 0.286. His [[conjecture]] that the limit is:\n\n:<math>\\frac {1}{2\\sqrt {\\pi}}=0.28209\\dots\\,.</math>\n\nwas disproven by {{harvtxt|Varga|Carpenter|1987}}, who calculated \n\n:<math>\\beta=0.280169499023\\dots\\,.</math>\n\n== References ==\n* {{citation|title=Sur la meilleure approximation de ''x'' par des polynomes de degrés donnés|last=Bernstein|first= S. N. |journal= Acta Math. |volume=37|pages= 1–57|year= 1914 |doi=10.1007/BF02401828}}\n* {{citation|last=Varga|first= Richard S.|last2= Carpenter|first2= Amos J. |title=A conjecture of S. Bernstein in approximation theory|journal= Math. USSR Sbornik |volume=57|pages= 547–560|year= 1987|mr=0842399|doi = 10.1070/SM1987v057n02ABEH003086|issue=2}}\n* {{MathWorld |urlname=BernsteinsConstant |title=Bernstein's Constant}}\n\n[[Category:Numerical analysis]]\n[[Category:Mathematical constants]]"
    },
    {
      "title": "Bi-directional delay line",
      "url": "https://en.wikipedia.org/wiki/Bi-directional_delay_line",
      "text": "In [[mathematics]], a '''bi-directional delay line''' is a [[numerical analysis]] technique used in [[computer simulation]] for solving [[ordinary differential equation]]s by converting them to [[hyperbolic equation]]s. In this way an explicit solution scheme is obtained with highly robust numerical properties. It was introduced by Auslander in 1968.\n\nIt originates from simulation of [[hydraulic pipeline]]s where [[wave propagation]] was studied. It was then found that it could be used as an efficient numerical technique for numerically insulating different parts of a simulation model in each times step. It is used in the [[HOPSAN]] simulation package (Krus ''et al.'' 1990).\n\nIt is also known as the ''[[Transmission-line matrix method|Transmission Line Modelling]]'' (TLM) from an independent development by Johns and O'Brian 1980. This is also extended to partial differential equations.\n\n==References==\n*D.M. Auslander, \"Distributed System Simulation with Bilateral Delay Line Models\", ''Journal of Basic Engineering, Trans. ASME p195-p200. June 1968.''\n*P. B. Johns and M.O'Brien. \"Use of the transmission line modelling (t.l.m) method to solve nonlinear lumped networks\", ''The Radio Electron and Engineer.''  1980.\n*P Krus, A Jansson, J-O Palmberg, K Weddfeldt. \"[https://www.researchgate.net/profile/Petter_Krus/publication/230688389_Distributed_Simulation_of_Hydromechanical_Systems/file/d912f5030038bb02b2.pdf?origin=publication_detail Distributed Simulation of Hydromechanical Systems]\". Presented at ''Third Bath International Fluid Power Workshop'', Bath, UK 1990.\n\n[[Category:Numerical differential equations]]\n[[Category:Numerical analysis]]\n\n{{mathanalysis-stub}}"
    },
    {
      "title": "Blossom (functional)",
      "url": "https://en.wikipedia.org/wiki/Blossom_%28functional%29",
      "text": "In [[numerical analysis]], a '''blossom''' is a [[functional (mathematics)|functional]] that can be applied to any [[polynomial]], but is mostly used for [[Bézier curve|Bézier]] and [[Spline (mathematics)|spline]] curves and surfaces.\n\nThe blossom of a polynomial ''ƒ'', often denoted <math>\\mathcal{B}[f],</math>  is completely characterised by the three properties:\n* It is a symmetric function of its arguments:\n:: <math>\\mathcal{B}[f](u_1,\\dots,u_d) = \\mathcal{B}[f]\\big(\\pi(u_1,\\dots,u_d)\\big),\\,</math>\n: (where ''&pi;'' is any [[permutation]] of its arguments).\n* It is affine in each of its arguments:\n:: <math>\\mathcal{B}[f](\\alpha u + \\beta v,\\dots) = \\alpha\\mathcal{B}[f](u,\\dots) + \\beta\\mathcal{B}[f](v,\\dots),\\text{ when }\\alpha + \\beta = 1.\\,</math>\n* It satisfies the diagonal property:\n:: <math>\\mathcal{B}[f](u,\\dots,u) = f(u).\\,</math>\n\n==References==\n*{{cite journal | author=Ramshaw, Lyle | title=Blossoming: A Connect-the-Dots Approach to Splines | publisher=Digital Systems Research Center | year=1987 | url=https://www.hpl.hp.com/techreports/Compaq-DEC/SRC-RR-19.html | accessdate=2019-04-19 | df= }}\n*{{cite journal | author=Ramshaw, Lyle | title=Blossoms are polar forms | publisher=Digital Systems Research Center | year=1989 | url=https://www.hpl.hp.com/techreports/Compaq-DEC/SRC-RR-34.html | accessdate=2019-04-19 | df= }}\n*{{cite book | author=Casteljau, Paul de Faget de | authorlink = Paul de Casteljau | chapter= POLynomials, POLar Forms, and InterPOLation | year = 1992 | editor= Larry L. Schumaker |editor2= Tom Lyche | title = Mathematical methods in computer aided geometric design II | publisher = Academic Press Professional, Inc. | isbn = 978-0-12-460510-7}}\n*{{cite book | author=Farin, Gerald | title = Curves and Surfaces for CAGD: A Practical Guide | year = 2001 | publisher = Morgan Kaufmann | edition = fifth | isbn = 1-55860-737-4 }}\n\n[[Category:Numerical analysis]]\n\n\n{{mathanalysis-stub}}"
    },
    {
      "title": "Boundary knot method",
      "url": "https://en.wikipedia.org/wiki/Boundary_knot_method",
      "text": "In numerical mathematics, the '''boundary knot method (BKM)''' is proposed as an alternative boundary-type meshfree distance function collocation scheme.\n\nRecent decades have witnessed a research boom on the meshfree numerical PDE techniques since the construction of a mesh in the standard [[finite element method]] and [[boundary element method]] is not trivial especially for moving boundary, and higher-dimensional problems.  The boundary knot method is different from the other methods based on the fundamental solutions, such as [[boundary element method]], [[method of fundamental solutions]] and [[singular boundary method]] in that the former does not require special techniques to cure the singularity. The BKM is truly meshfree, spectral convergent (numerical observations), symmetric (self-adjoint PDEs), integration-free, and easy to learn and implement. The method has successfully been tested to the Helmholtz, diffusion, convection-diffusion, and Possion equations with very irregular 2D and 3D domains.\n\n== Description ==\nThe BKM is basically a combination of the distance function, non-singular general solution, and dual reciprocity method (DRM). The distance function is employed in the BKM to approximate the inhomogeneous terms via the DRM, whereas the non-singular general solution of the partial differential equation leads to a boundary-only formulation for the homogeneous solution. Without the singular fundamental solution, the BKM removes the controversial artificial boundary in the method of fundamental solutions. Some preliminary numerical experiments show that the BKM can produce excellent results with relatively a small number of nodes for various linear and nonlinear problems.\n\n== Formulation ==\nConsider the following problems,\n\n: (1) <math>Lu=f\\left( x,y \\right),\\ \\ \\left( x,y \\right)\\in \\Omega </math>\n\n: (2) <math>u=g\\left( x,y \\right),\\ \\ \\left( x,y \\right)\\in \\partial \\Omega_D</math>\n\n: (3) <math>\\frac{\\partial u}{\\partial n}=h\\left( x,y \\right),\\ \\ h\\left( x,y \\right)\\in \\partial \\Omega_N</math>\nwhere <math>L</math> is the differential operator, <math>\\Omega </math> represents the computational domain, <math>\\partial \\Omega_D</math> and <math>\\partial \\Omega_N</math> denote the Dirichlet and Neumann boundaries respectively, satisfied <math>\\partial \\Omega_D \\cup \\partial \\Omega_N=\\partial \\Omega </math> and <math>\\partial \\Omega_D \\cap \\partial \\Omega_N=\\varnothing </math>.\nThe BKM employs the non-singular general solution of the operator <math>L</math> to approximate the numerical solution as follows,\n\n: (4) <math>u^* \\left( x,y \\right)=\\sum\\limits_{i=1}^N \\alpha_i\\phi \\left( r_i \\right)</math>\nwhere <math>r_i = \\left\\| \\left( x,y \\right)-\\left( x_i,y_i \\right) \\right\\|_2</math> denotes the Euclidean distance, <math>\\phi \\left( \\cdot  \\right)</math> is the general solution satisfied\n: (5)<math>L\\phi =0</math>\nBy employing the collocation technique to satisfy the boundary conditions (2) and (3),\n\n: (6)<math>\\begin{align}\n  & g\\left( x_k,y_k \\right)=\\sum\\limits_{i=1}^N \\alpha_i\\phi \\left( r_i \\right),\\qquad k=1,\\ldots,m_1 \\\\\n & h\\left( x_k,y_k \\right)=\\sum\\limits_{i=1}^N \\alpha_i \\frac{\\partial \\phi \\left( r_i \\right)}{\\partial n}, \\qquad k=m_1 + 1,\\ldots,m \\\\\n\\end{align}</math>\n\nwhere <math>\\left( x_k,y_k \\right)|_{k=1}^{m_1}</math> and <math>\\left( x_k,y_k \\right)|_{k=m_1 + 1}^m </math> denotes the collocation points located at Dirichlet boundary and Neumann boundary respectively. The unknown coefficients <math>\\alpha_i</math> can be uniquely determined by above Eq. (6). And then the BKM solution at any location of computational domain can be evaluated by the formulation (4).\n\n== History and recent developments ==\nIt has long been noted that [[boundary element method]] (BEM) is an alternative method to [[finite element method]] (FEM) and [[finite volume method]] (FVM) for infinite domain, thin-walled structures, and [[inverse problems]], thanks to its dimensional reducibility. The major bottlenecks of BEM, however, are computationally expensive to evaluate integration of singular fundamental solution and to generate surface mesh or re-mesh. The method of fundamental solutions (MFS)<ref>R. Mathon and R. L. Johnston, The approximate solution of elliptic boundary-value problems by fundamental solutions, ''SIAM Journal on Numerical Analysis'', 638–650, 1977.</ref> has in recent decade emerged to alleviate these drawbacks and getting increasing attentions. The MFS is integration-free, spectral convergence and meshfree.\n\nAs its name implies, the fundamental solution of the governing equations is used as the basis function in the MFS. To avoid singularity of the fundamental solution, the artificial boundary outside the physical domain is required and has been a major bottleneck for the wide use of the MFS, since such fictitious boundary may cause computational instability. The BKM is classified as one kind of boundary-type meshfree methods without using mesh and artificial boundary.\n\nThe BKM has since been widely tested. In,<ref>W. Chen and M. Tanaka, A meshfree, exponential convergence, integration-free, and boundary-only RBF technique, ''Computers and Mathematics with Applications'', 43, 379–391, 2002.</ref> the BKM is used to solve Laplace equation, Helmholtz Equation, and varying-parameter Helmholtz equations; in<ref>W. Chen, Symmetric boundary knot method, ''Engineering Analysis with Boundary Elements'', 26(6), 489–494, 2002.</ref> by analogy with Fasshauer’s Hermite RBF interpolation, a symmetric BKM scheme is proposed in the presence of mixed boundary conditions; in,<ref>W. Chen and Y.C. Hon, Numerical convergence of boundary knot method in the analysis of Helmholtz, modified Helmholtz, and convection-diffusion problems, ''Computer Methods in Applied Mechanics and Engineering'', 192, 1859–1875, 2003.</ref> numerical investigations are made on the convergence of BKM in the analysis of homogeneous Helmholtz, modified Helmholtz and convection-diffusion problems; in<ref>Y.C. Hon and W. Chen, Boundary knot method for 2D and 3D Helmholtz and convection-diffusion problems with complicated geometry, ''International Journal for Numerical Methods in Engineering'', 1931-1948, 56(13), 2003.</ref> the BKM is employed to deal with complicated geometry of two and three dimension Helmholtz and convection-diffusion problems; in<ref>X.P. Chen, W.X. He and B.T. Jin, Symmetric boundary knot method for membrane vibrations under mixed-type boundary conditions, ''International Journal of Nonlinear Science and Numerical Simulation'', 6, 421–424, 2005.</ref> membrane vibration under mixed-type boundary conditions is investigated by symmetric boundary knot method; in<ref>B.T. Jing and Z. Yao, Boundary knot method for some inverse problems associated with the Helmholtz equation, ''International Journal for Numerical Methods in Engineering'', 62, 1636–1651, 2005.</ref> the BKM is applied to some inverse Helmholtz problems; in<ref>W. Chen, L.J. Shen, Z.J. Shen, G.W. Yuan, Boundary knot method for Poisson equations, ''Engineering Analysis with Boundary Elements'', 29(8), 756–760, 2005.</ref> the BKM solves Poisson equations; in<ref>B.T. Jin, Y. Zheng, Boundary knot method for the Cauchy problem associated with the inhomogeneous Helmholtz equation, ''Engineering Analysis with Boundary Elements'', 29, 925–935, 2005.</ref> the BKM calculates Cauchy inverse inhomogeneous Helmholtz equations; in<ref>B.T. Jin and W. Chen, Boundary knot method based on geodesic distance for anisotropic problems, ''Journal of Computational Physics'', 215(2), 614–629, 2006.</ref> the BKM simulates the anisotropic problems via the geodesic distance; in<ref>F.Z. Wang, W. Chen, X.R. Jiang, Investigation of regularized techniques for boundary knot method. ''International Journal for Numerical Methods in Biomedical Engineering'', 26(12), 1868–1877, 2010</ref>\n<ref>F.Z. Wang, Leevan L, W. Chen, Effective condition number for boundary knot method. ''CMC: Computers, Materials, & Continua'', 12(1), 57–70, 2009</ref> relationships among condition number, effective condition number, and regularizations are investigated; in<ref>Z.J. Fu; W. Chen, Q.H Qin, Boundary knot method for heat conduction in nonlinear functionally graded material, ''Engineering Analysis with Boundary Elements'', 35(5), 729–734, 2011.</ref> heat conduction in nonlinear functionally graded material is examined by the BKM; in<ref>D. Mehdi and S. Rezvan, A boundary-only meshfree method for numerical solution of the Eikonal equation, ''Computational Mechanics'', 47, 283–294, 2011.</ref> the BKM is also used to solve nonlinear Eikonal equation.\n\n==See also==\n\n*[[Method of fundamental solutions]]\n*[[Regularized meshfree method]]\n*[[Boundary particle method]]\n*[[Singular boundary method]]\n\n==References==\n{{Reflist}}\n\n==Related website==\n* [https://web.archive.org/web/20121227005503/http://em.hhu.edu.cn/chenwen/html/BKM.htm Boundary knot method]\n* [https://web.archive.org/web/20121227010856/http://em.hhu.edu.cn/chenwen/html/matlabbkm.htm Examplary Matlab codes and geometric configurations]\n\n{{Numerical PDE}}\n\n[[Category:Numerical analysis]]\n[[Category:Numerical differential equations]]"
    },
    {
      "title": "Boundary particle method",
      "url": "https://en.wikipedia.org/wiki/Boundary_particle_method",
      "text": "{{expert-subject|1=Mathematics|date=April 2012}}\nIn [[applied mathematics]], the '''boundary particle method (BPM)''' is a boundary-only [[meshfree method|meshless (meshfree)]] [[collocation method|collocation technique]], in the sense that none of inner nodes are required in the numerical solution of nonhomogeneous [[partial differential equations]]. Numerical experiments show that the BPM has [[spectral convergence]]. Its interpolation matrix can be symmetric.\n\n== History and recent developments ==\nIn recent decades, the [[dual reciprocity method]] (DRM)<ref>Partridge PW, Brebbia CA, Wrobel LC, The dual reciprocity boundary element method. Computational Mechanics Publications, 1992</ref> and [[multiple reciprocity method]] (MRM)<ref>Nowak AJ, Neves AC, The multiple reciprocity boundary element method. Computational Mechanics Publication, 1994</ref> have been emerging as promising techniques to evaluate the particular solution of nonhomogeneous [[partial differential equations]] in conjunction with the boundary discretization techniques, such as [[boundary element method]] (BEM). For instance, the so-called DR-BEM and MR-BEM are popular BEM techniques in the numerical solution of nonhomogeneous problems.\n\nThe DRM has become a common method to evaluate the particular solution. However, the DRM requires inner nodes to guarantee the convergence and stability. The MRM has an advantage over the DRM in that it does not require using inner nodes for nonhomogeneous problems.{{cn|date=January 2014}} Compared with the DRM, the MRM is computationally more expensive in the construction of the interpolation matrices and has limited applicability to general nonhomogeneous problems due to its conventional use of high-order Laplacian operators in the annihilation process.\n\nThe recursive composite multiple reciprocity method (RC-MRM),<ref name=\"Chena\">Chen W, Meshfree boundary particle method applied to Helmholtz problems. Engineering Analysis with Boundary Elements 2002,26(7): 577–581</ref><ref name=\"Chenb\">Chen W, Fu ZJ, Jin BT, A truly boundary-only meshfree method for inhomogeneous problems based on recursive composite multiple reciprocity technique. Engineering Analysis with Boundary Elements 2010,34(3): 196–205</ref> was proposed to overcome the above-mentioned problems. The key idea of the RC-MRM is to employ high-order composite differential operators instead of high-order Laplacian operators to eliminate a number of nonhomogeneous terms in the governing equation. The RC-MRM uses the recursive structures of the MRM interpolation matrix to reduce computational costs.\n\nThe boundary particle method (BPM) is a boundary-only discretization of an inhomogeneous partial differential equation by combining the RC-MRM with strong-form meshless boundary collocation discretization schemes, such as the [[method of fundamental solution]] (MFS), [[boundary knot method]] (BKM), [[regularized meshless method]] (RMM), [[singular boundary method]] (SBM), and [[Trefftz method]] (TM). The BPM has been applied to problems such as nonhomogeneous [[Helmholtz]] and [[convection-diffusion equation]]. The BPM interpolation representation is of a [[wavelet]] series.\n\nFor the application of the BPM to [[Helmholtz]],<ref name=\"Chena\" /> [[Siméon Denis Poisson|Poisson]]<ref name=\"Chenb\" /> and [[plate bending]] problems,<ref>Fu ZJ, Chen W, Yang W, Winkler plate bending problems by a truly boundary-only boundary particle method. Computational Mechanics 2009,44(6): 757–563</ref> the high-order [[fundamental solution]] or general solution, harmonic function<ref>Hon YC, Wu ZM, A numerical computation for inverse boundary determination problem. Engineering Analysis with Boundary Elements 2000,24(7–8): 599–606</ref> or [[Trefftz]] function (T-complete functions)<ref>Chen W, Fu ZJ, Qin QH, Boundary particle method with high-order Trefftz functions. CMC: Computers, Materials & Continua 2010,13(3): 201–217</ref> are often used, for instance, those of [[Marcel Berger|Berger]], [[Peter Winkler|Winkler]], and vibrational thin plate equations.<ref>Chen W, Shen ZJ, Shen LJ, Yuan GW, General solutions and fundamental solutions of varied orders to the vibrational thin, the Berger, and the Winkler plates. Engineering Analysis with Boundary Elements 2005,29(7): 699–702</ref> The method has been applied to inverse Cauchy problem associated with [[Siméon Denis Poisson|Poisson]]<ref>Fu ZJ, Chen W, Zhang CZ, Boundary particle method for Cauchy inhomogeneous potential problems. Inverse Problems in Science and Engineering 2012,20(2): 189–207</ref> and nonhomogeneous [[Helmholtz]] equations.<ref>Chen W, Fu ZJ, Boundary particle method for inverse Cauchy problems of inhomogeneous Helmholtz equations. Journal of Marine Science and Technology–Taiwan 2009,17(3): 157–163</ref>\n\n==Further comments==\nThe BPM may encounter difficulty in the solution of problems having complex source functions, such as non-smooth, large-gradient functions, or a set of discrete measured data. The  solution of such problems involves:{{cn|date=January 2014}}\n\n(1) The complex functions or a set of discrete measured data can be interpolated by a sum of [[polynomial]] or [[trigonometric]] function series. Then, the RC-MRM can reduce the nonhomogeneous equation to a high-order homogeneous equation, and the BPM can be implemented to solve these problems with boundary-only discretization.\n\n(2) The [[domain decomposition]] may be used to in the BPM boundary-only solution of large-gradient source functions problems.\n\n==See also==\n* [[Meshfree method]]\n* [[Radial basis function]]\n* [[Boundary element method]]\n* [[Trefftz method]]\n* [[Method of fundamental solution]]\n* [[Boundary knot method]]\n* [[Singular boundary method]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* [https://web.archive.org/web/20160303222653/http://www.ccms.ac.cn/fuzj/Boundary%20Particle%20Method.htm Boundary Particle Method]\n\n==Free software and Matlab codes==\n* [https://web.archive.org/web/20160303225250/http://www.ccms.ac.cn/fuzj/download/BPM-Winkler%20Plate.rar Winker plate bending analysis]\n* [http://www.ccms.ac.cn/shijh/bpmtoolbox.zip BPM toolbox for inverse Cauchy problems]{{Dead link|date=November 2018 |bot=InternetArchiveBot |fix-attempted=yes }}\n\n{{Numerical PDE}}\n\n[[Category:Numerical analysis]]\n[[Category:Numerical differential equations]]"
    },
    {
      "title": "Chebyshev nodes",
      "url": "https://en.wikipedia.org/wiki/Chebyshev_nodes",
      "text": "[[File:Chebyshev-nodes-by-projection.svg|thumb|The Chebyshev nodes are equivalent to the ''x'' coordinates of ''n'' equally spaced points on a unit semicircle (here, ''n''=10).<ref>Lloyd N. Trefethen, ''Approximation Theory and Approximation Practice'' (SIAM, 2012).  Online: https://people.maths.ox.ac.uk/trefethen/ATAP/ </ref>]]\n\nIn [[numerical analysis]], '''Chebyshev nodes''' are specific [[real number|real]] [[algebraic number]]s, namely the roots of the [[Chebyshev polynomials of the first kind]]. They are often used as nodes in [[polynomial interpolation]] because the resulting interpolation polynomial minimizes the effect of [[Runge's phenomenon]].<ref>Fink, Kurtis D., and John H. Mathews. ''Numerical Methods using MATLAB''. Upper Saddle River, NJ: Prentice Hall, 1999. 3rd ed. pp. 236-238.</ref>\n\n==Definition==\n[[File:Chebyshev Zeros.svg|thumb|Zeros of the first 50 Chebyshev polynomials of the first kind]]\n\nFor a given positive integer ''n'' the '''Chebyshev nodes''' in the interval (&minus;1, 1) are\n\n:<math>x_k = \\cos\\left(\\frac{2k-1}{2n}\\pi\\right), \\quad k = 1, \\ldots, n.</math>\n\nThese are the roots of the [[Chebyshev polynomial of the first kind]] of degree ''n''. For nodes over an arbitrary interval [''a'', ''b''] an [[affine transformation]] can be used:\n\n:<math>x_k = \\frac{1}{2} (a + b) + \\frac{1}{2} (b - a) \\cos\\left(\\frac{2k-1}{2n}\\pi\\right), \\quad k = 1, \\ldots, n.</math>\n\n==Approximation==\n\nThe Chebyshev nodes are important in [[approximation theory]] because they form a particularly good set of nodes for [[polynomial interpolation]]. Given a function ƒ on the interval <math>[-1,+1]</math> and <math>n</math> points <math>x_1,  x_2, \\ldots , x_n,</math> in that interval, the interpolation polynomial is that unique polynomial <math>P_{n-1}</math> of degree at most <math>n-1</math> which has value <math>f(x_i)</math> at each point <math>x_i</math>.   The interpolation error at <math>x</math> is\n\n:<math>f(x) - P_{n-1}(x) = \\frac{f^{(n)}(\\xi)}{n!} \\prod_{i=1}^n (x-x_i) </math>\n\nfor some <math>\\xi</math> (depending on x) in [&minus;1,&nbsp;1].<ref>{{harvtxt|Stewart|1996}}, (20.3)</ref> So it is logical to try to minimize\n\n:<math>\\max_{x \\in [-1,1]} \\left| \\prod_{i=1}^n (x-x_i) \\right|. </math>\n\nThis product is a ''[[monic polynomial|monic]]'' polynomial of degree ''n''.  It may be shown that the maximum absolute value (maximum norm) of any such polynomial is bounded from below by 2<sup>1&minus;''n''</sup>.  This bound is attained by the scaled Chebyshev polynomials 2<sup>1&minus;''n''</sup> ''T''<sub>''n''</sub>, which are also monic. (Recall that |''T''<sub>''n''</sub>(''x'')|&nbsp;≤&nbsp;1 for ''x''&nbsp;∈&nbsp;[&minus;1,&nbsp;1].<ref>{{harvtxt|Stewart|1996}}, Lecture 20, &sect;14</ref>) Therefore, when the interpolation nodes ''x''<sub>''i''</sub> are the roots of ''T''<sub>''n''</sub>, the error satisfies\n:<math>\\left|f(x) - P_{n-1}(x)\\right| \\le \\frac{1}{2^{n - 1}n!} \\max_{\\xi \\in [-1,1]} \\left|f^{(n)} (\\xi)\\right|.</math>\nFor an arbitrary interval [''a'', ''b''] a change of variable shows that\n:<math>\\left|f(x) - P_{n-1}(x)\\right| \\le \\frac{1}{2^{n - 1}n!} \\left(\\frac{b-a}{2}\\right)^n \\max_{\\xi \\in [a,b]} \\left|f^{(n)} (\\xi)\\right|.</math>\n<!-- To do: write about Lebesgue constant and connection with Fourier series -->\n\n==Notes==\n<references/>\n\n==References==\n*{{Citation | last1=Stewart | first1=Gilbert W. | title=Afternotes on Numerical Analysis | publisher=[[Society for Industrial and Applied Mathematics|SIAM]] | isbn=978-0-89871-362-6 | year=1996}}.\n\n==Further reading==\n*Burden, Richard L.; Faires, J. Douglas: ''Numerical Analysis'', 8th ed., pages 503–512, {{ISBN|0-534-39200-8}}.\n\n{{Algebraic numbers}}\n\n[[Category:Numerical analysis]]\n[[Category:Algebraic numbers]]"
    },
    {
      "title": "Clenshaw algorithm",
      "url": "https://en.wikipedia.org/wiki/Clenshaw_algorithm",
      "text": "In [[numerical analysis]], the '''Clenshaw algorithm''', also called '''Clenshaw summation''', is a [[Recursion|recursive]] method to evaluate a linear combination of [[Chebyshev polynomials]].<ref name=\"Clenshaw55\">{{Cite journal | last1 = Clenshaw | first1 = C. W.| title = A note on the summation of Chebyshev series| url = http://www.ams.org/journals/mcom/1955-09-051/S0025-5718-1955-0071856-0/| doi = 10.1090/S0025-5718-1955-0071856-0| journal = Mathematical Tables and Other Aids to Computation| issn = 0025-5718| volume = 9| issue = 51| page = 118| date=July 1955 | pmid = | pmc = }}  Note that this paper is written in terms of the ''Shifted'' Chebyshev polynomials of the first kind <math>T^*_n(x) = T_n(2x-1)</math>.</ref><ref name=\"Tscherning82\"/>  It is a generalization of [[Horner's method]] for evaluating a linear combination of [[monomial]]s.\n\nIt generalizes to more than just Chebyshev polynomials; it applies to any class of functions that can be defined by a three-term [[recurrence relation]].<ref>{{Citation |last1=Press |first1=WH |last2=Teukolsky |first2=SA |last3=Vetterling |first3=WT |last4=Flannery |first4=BP |year=2007 |title=Numerical Recipes: The Art of Scientific Computing |edition=3rd |publisher=Cambridge University Press |publication-place=New York |isbn=978-0-521-88068-8 |chapter=Section 5.4.2. Clenshaw's Recurrence Formula |chapter-url=http://apps.nrbook.com/empanel/index.html?pg=222}}</ref>\n\n==Clenshaw algorithm==\n\nIn full generality, the Clenshaw algorithm computes the weighted sum of a finite series of functions <math>\\phi_k(x)</math>:\n:<math>S(x) = \\sum_{k=0}^n a_k \\phi_k(x)</math>\n\nwhere <math>\\phi_k,\\; k=0, 1, \\ldots</math> is a sequence of functions that satisfy the linear recurrence relation\n\n:<math>\\phi_{k+1}(x) = \\alpha_k(x)\\,\\phi_k(x) + \\beta_k(x)\\,\\phi_{k-1}(x),</math>\n\nwhere the coefficients <math>\\alpha_k(x)</math> and <math>\\beta_k(x)</math> are known in advance.\n\nThe algorithm is most useful when <math>\\phi_k(x)</math> are functions that are complicated to compute directly, but <math>\\alpha_k(x)</math> and <math>\\beta_k(x)</math> are particularly simple.  In the most common applications, <math>\\alpha(x)</math> does not depend on <math>k</math>, and <math>\\beta</math> is a constant that depends on neither <math>x</math> nor <math>k</math>.\n\nTo perform the summation for given series of coefficients <math>a_0, \\ldots, a_n</math>, compute the values <math>b_k(x)</math> by the \"reverse\" recurrence formula:\n\n:<math>\n  \\begin{align}\n  b_{n+1}(x) &= b_{n+2}(x) = 0, \\\\\n  b_k(x) &= a_k + \\alpha_k(x)\\,b_{k+1}(x) + \\beta_{k+1}(x)\\,b_{k+2}(x).\n  \\end{align}\n</math>\n\nNote that this computation makes no direct reference to the functions <math>\\phi_k(x)</math>.  After computing <math>b_2(x)</math> and <math>b_1(x)</math>, \nthe desired sum can be expressed in terms of them and the simplest functions <math>\\phi_0(x)</math> and <math>\\phi_1(x)</math>:\n\n:<math>S(x) = \\phi_0(x)\\,a_0 + \\phi_1(x)\\,b_1(x) + \\beta_1(x)\\,\\phi_0(x)\\,b_2(x).</math>\n\nSee Fox and Parker<ref name=\"FoxParker\">{{Citation |first1=Leslie |last1=Fox |first2=Ian B. |last2=Parker |title=Chebyshev Polynomials in Numerical Analysis |publisher=Oxford University Press |year=1968 |isbn=0-19-859614-6}}</ref> for more information and stability analyses.\n\n==Examples==\n===Horner as a special case of Clenshaw===\nA particularly simple case occurs when evaluating a polynomial of the form\n:<math>S(x) = \\sum_{k=0}^n a_k x^k</math>.\nThe functions are simply\n:<math>\n  \\begin{align}\n  \\phi_0(x) &= 1, \\\\\n  \\phi_k(x) &= x^k = x\\phi_{k-1}(x)\n  \\end{align}\n</math>\nand are produced by the recurrence coefficients <math>\\alpha(x) = x</math> and <math>\\beta = 0</math>.\n\nIn this case, the recurrence formula to compute the sum is\n:<math>b_k(x) = a_k + x b_{k+1}(x)</math>\nand, in this case, the sum is simply\n:<math>S(x) = a_0 + x b_1(x) = b_0(x)</math>,\nwhich is exactly the usual [[Horner's method]].\n\n===Special case for Chebyshev series===\nConsider a truncated [[Chebyshev series]]\n\n:<math>p_n(x) = a_0 + a_1T_1(x) + a_2T_2(x) + \\cdots + a_nT_n(x).</math>\n\nThe coefficients in the recursion relation for the [[Chebyshev polynomials]] are\n\n:<math>\\alpha(x) = 2x, \\quad \\beta = -1,</math>\nwith the initial conditions\n:<math>T_0(x) = 1, \\quad T_1(x) = x.</math>\n\nThus, the recurrence is\n:<math>b_k(x) = a_k + 2xb_{k+1}(x) - b_{k+2}(x)</math>\nand the final sum is\n:<math>p_n(x) = a_0 + xb_1(x) - b_2(x).</math>\n\nOne way to evaluate this is to continue the recurrence one more step, and compute\n:<math>b_0(x) = 2a_0 + 2xb_1(x) - b_2(x),</math>\n(note the doubled ''a''<sub>0</sub> coefficient) followed by\n:<math>p_n(x) = \\frac{1}{2}\\left[b_0(x) - b_2(x)\\right].</math>\n\n===Meridian arc length on the ellipsoid===\nClenshaw summation is extensively used in geodetic applications.<ref name=\"Tscherning82\">\n{{Citation\n| last1=Tscherning\n| first1=C. C.\n| last2=Poder\n| first2=K.\n| year=1982\n| title=Some Geodetic applications of Clenshaw Summation\n| journal=Bolletino di Geodesia e Scienze Affini\n| volume=41\n| number=4\n| pages=349–375\n| url=http://cct.gfy.ku.dk/publ_cct/cct80.pdf\n}}</ref>  A simple application is summing the trigonometric series to compute\nthe [[meridian arc]] distance on the surface of an ellipsoid.  These have the form\n\n:<math>m(\\theta) = C_0\\,\\theta + C_1\\sin \\theta + C_2\\sin 2\\theta + \\cdots + C_n\\sin n\\theta.</math>\n\nLeaving off the initial <math>C_0\\,\\theta</math> term, the remainder is a summation of the appropriate form. There is no leading term because <math>\\phi_0(\\theta) = \\sin 0\\theta = \\sin 0 = 0</math>.\n\nThe [[List of trigonometric identities#Chebyshev method|recurrence relation for <math>\\sin k\\theta</math>]] is\n:<math>\\sin (k+1)\\theta = 2 \\cos\\theta \\sin k\\theta - \\sin (k-1)\\theta</math>,\n\nmaking the coefficients in the recursion relation\n\n:<math>\\alpha_k(\\theta) = 2\\cos\\theta, \\quad \\beta_k = -1.</math>\n\nand the evaluation of the series is given by\n\n:<math>\n  \\begin{align}\n  b_{n+1}(\\theta) &= b_{n+2}(\\theta) = 0, \\\\\n  b_k(\\theta) &= C_k + 2\\cos \\theta \\,b_{k+1}(\\theta) - b_{k+2}(\\theta),\\quad\\mathrm{for\\ } n\\ge k \\ge 1.\n  \\end{align}\n</math>\nThe final step is made particularly simple because <math>\\phi_0(\\theta) = \\sin 0 = 0</math>, so the end of the recurrence is simply <math>b_1(\\theta)\\sin(\\theta)</math>; the <math>C_0\\,\\theta</math> term is added separately:\n\n:<math>m(\\theta) = C_0\\,\\theta + b_1(\\theta)\\sin \\theta.</math>\n\nNote that the algorithm requires only the evaluation of two trigonometric quantities <math>\\cos \\theta</math> and <math>\\sin \\theta</math>.\n\n===Difference in meridian arc lengths===\nSometimes it necessary to compute the difference of two meridian arcs in\na way that maintains high relative accuracy.  This is accomplished by\nusing trigonometric identities to write\n:<math>\n  m(\\theta_1)-m(\\theta_2) = C_0(\\theta_1-\\theta_2) + \\sum_{k=1}^n 2 C_k\n  \\sin\\bigl({\\textstyle\\frac12}k(\\theta_1-\\theta_2)\\bigr)\n  \\cos\\bigl({\\textstyle\\frac12}k(\\theta_1+\\theta_2)\\bigr).\n</math>\nClenshaw summation can be applied in this case<ref>\n{{Citation\n| last=Karney\n| first=C. F. F.\n| year=2014\n| title=Clenshaw evaluation of differenced sums\n| url=http://geographiclib.sourceforge.net/html/rhumb.html#dividedclenshaw\n}}</ref>\nprovided we simultaneously compute <math>m(\\theta_1)+m(\\theta_2)</math>\nand perform a matrix summation,\n:<math>\n  \\mathsf M(\\theta_1,\\theta_2) = \\begin{bmatrix}\n  (m(\\theta_1) + m(\\theta_2)) / 2\\\\\n  (m(\\theta_1) - m(\\theta_2)) / (\\theta_1 - \\theta_2)\n  \\end{bmatrix} =\n  C_0 \\begin{bmatrix}\\mu\\\\1\\end{bmatrix} +\n  \\sum_{k=1}^n C_k \\mathsf F_k(\\theta_1,\\theta_2),\n</math>\nwhere\n:<math>\n  \\begin{align}\n  \\delta &= {\\textstyle\\frac12}(\\theta_1-\\theta_2), \\\\\n  \\mu &= {\\textstyle\\frac12}(\\theta_1+\\theta_2), \\\\\n  \\mathsf F_k(\\theta_1,\\theta_2) &=\n    \\begin{bmatrix}\n       \\cos k \\delta \\sin k \\mu\\\\\n    \\displaystyle\\frac{\\sin k \\delta}\\delta \\cos k \\mu\n    \\end{bmatrix}.\n  \\end{align}\n</math>\nThe first element of <math>\\mathsf M(\\theta_1,\\theta_2)</math> is the average\nvalue of <math>m</math> and the second element is the average slope.\n<math>\\mathsf F_k(\\theta_1,\\theta_2)</math> satisfies the recurrence\nrelation\n:<math>\n  \\mathsf F_{k+1}(\\theta_1,\\theta_2) =\n  \\mathsf A(\\theta_1,\\theta_2) \\mathsf F_k(\\theta_1,\\theta_2) -\n  \\mathsf F_{k-1}(\\theta_1,\\theta_2),\n</math>\nwhere\n:<math>\n   \\mathsf A(\\theta_1,\\theta_2) = 2\\begin{bmatrix}\n      \\cos \\delta \\cos \\mu & -\\delta\\sin \\delta \\sin \\mu \\\\\n      - \\displaystyle\\frac{\\sin \\delta}\\delta \\sin \\mu &   \\cos \\delta \\cos \\mu\n  \\end{bmatrix}\n</math>\ntakes the place of <math>\\alpha</math> in the recurrence relation, and <math>\\beta=-1</math>.\nThe standard Clenshaw algorithm can now be applied to yield\n:<math>\n  \\begin{align}\n  \\mathsf B_{n+1} &= \\mathsf B_{n+2} = \\mathsf 0, \\\\\n  \\mathsf B_k &= C_k \\mathsf I + \\mathsf A \\mathsf B_{k+1} -\n  \\mathsf B_{k+2}, \\qquad\\mathrm{for\\ } n\\ge k \\ge 1,\\\\\n  \\mathsf M(\\theta_1,\\theta_2) &=\n    C_0 \\begin{bmatrix}\\mu\\\\1\\end{bmatrix} +\n    \\mathsf B_1 \\mathsf F_1(\\theta_1,\\theta_2),\n  \\end{align}\n</math>\nwhere <math>\\mathsf B_k</math> are 2×2 matrices.  Finally\nwe have\n:<math>\n  \\frac{m(\\theta_1) - m(\\theta_2)}{\\theta_1 - \\theta_2} =\n  \\mathsf M_2(\\theta_1, \\theta_2).\n</math>\nThis technique can be used in the [[limit (mathematics)|limit]] <math>\\theta_2 = \\theta_1 = \\mu</math>\nand <math>\\delta = 0\\,</math> to simultaneously compute  <math>m(\\mu)</math> and the [[derivative]]\n<math>dm(\\mu)/d\\mu</math>, provided that, in evaluating <math>\\mathsf F_1</math> and <math>\\mathsf A</math>,\nwe take <math>\\lim_{\\delta\\rightarrow0}(\\sin \\delta)/\\delta = 1</math>.\n\n==See also==\n*[[Horner scheme]] to evaluate polynomials in [[monomial form]]\n*[[De Casteljau's algorithm]] to evaluate polynomials in [[Bézier form]]\n\n==References==\n<references/>\n\n{{DEFAULTSORT:Clenshaw Algorithm}}\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Closest point method",
      "url": "https://en.wikipedia.org/wiki/Closest_point_method",
      "text": "The '''closest point method (CPM)''' is an embedding method for solving partial differential equations on surfaces. The closest point method uses standard numerical approaches such as finite differences, finite element or spectral methods in order to solve the embedding partial differential equation (PDE) which is equal to the original PDE on the surface. The solution is computed in a band surrounding the surface in order to be computationally efficient. In order to extend the data off the surface, the closest point method uses a closest point representation. This representation extends function values to be constant along directions normal to the surface.  \n\n==Definitions==\nClosest Point function: Given a surface <math>\\mathcal S, cp(\\mathbf x) </math> refers to a (possibly non-unique) point belonging to <math>\\mathcal S </math>, which is closest to <math> \\mathbf x </math> '''[SE]'''.\n\nClosest point extension: Let <math>\\mathcal S </math>, be a smooth surface in  <math> \\mathbb R^d </math>. The closest point extension of a function <math>u : \\mathcal S  \\rightarrow \\mathbb R </math>, to a neighborhood <math> \\Omega </math> of <math>\\mathcal S </math>, is the function <math> v: \\Omega \\rightarrow \\mathbb R </math>, defined by <math> v(\\mathbf x) = u(cp(\\mathbf x))</math>.\n\n==Closest point method==\nInitialization consists of these steps '''[EW]''':\n* If it is not already given, a closest point representation of the surface is constructed.\n* A computational domain is chosen. Typically this is a band around the surface. \n* Replace surface gradients by standard gradients in <math>\\mathbb R^3 </math>. \n* Solution is initialized by extending the initial surface data on to the computational domain using the closest point function.\nAfter initialization, alternate between the following two steps:\n* Using the closest point function, extend the solution off the surface to the computational domain. \n* Compute the solution to the embedding PDE on a Cartesian mesh in the computational domain for one time step.\n\n==Banding==\nThe surface PDE is extended into <math> \\mathbb R^3 </math> however it is only necessary to solve this new PDE near the surface. Hence, we solve the PDE in a band surrounding the surface for efficient computational purposes. \n<math> \\Omega_c {x : \\| x - cp(x) \\|_2 \\leq \\lambda} </math>\nwhere <math> \\lambda </math> is the bandwidth.\n\n==Example: Heat equation on a circle==\nUsing initial profile <math> u_S (\\theta , t) = \\sin (\\theta) </math> leads to the solution <math> u_S (\\theta, t) = \\exp (-t)\\sin (\\theta) </math> for the heat equation. Forward Euler time-stepping is used with relation <math> \\Delta t = 0.1 \\Delta x^2 </math> and degree-four interpolation polynomials for the interpolations. Second-order centered differences are used for the spatial discretization. The CPM results in the expected second order error in the solution <math> u </math>.\n\n==Applications==\nThe closest point method can be applied to various PDEs on surfaces. Reaction–diffusion problems on point clouds '''[RD]''', eigenvalue problems '''[EV]''', and level set equations '''[LS]''' are a few examples.\n\n==See also==\n*[[Level-set method]]\n*[[Image segmentation]]\n*[[Eigenvalues and eigenvectors]]\n\n==References==\n*'''[EM]''' Ruuth, S. J., & Merriman, B. (2008). A simple embedding method for solving partial differential equations on surfaces.Journal of Computational Physics,227(3), 1943–1961 [http://www.sciencedirect.com/science/article/pii/S002199910700441X here]\n*'''[RD]''' Macdonald, C. B., Merriman, B., & Ruuth, S. J. (2013). Simple computation of reaction-diffusion processes on point clouds. Proceedings of the National Academy of Sciences, 110(23), 9209–9214 [http://www.pnas.org/content/110/23/9209.short here]\n*'''[EV]''' Macdonald, C. B., Brandman, J., & Ruuth, S. J. (2011). Solving eigenvalue problems on curved surfaces using the Closest Point Method. Journal of Computational Physics, 230(22), 7944–7956. [http://www.sciencedirect.com/science/article/pii/S0021999111003858 here]\n*'''[LS]''' Macdonald, C. B., & Ruuth, S. J. (2008). Level set equations on surfaces via the Closest Point Method. Journal of Scientific Computing, 35(2–3), 219–240. [https://link.springer.com/article/10.1007%2Fs10915-008-9196-6 here]\n\n{{Numerical PDE}}\n\n[[Category:Numerical analysis]]\n[[Category:Computer graphics algorithms]]\n[[Category:Image processing]]"
    },
    {
      "title": "Computational statistics",
      "url": "https://en.wikipedia.org/wiki/Computational_statistics",
      "text": "{{More citations needed|article|date=May 2009}}\n{{for|the journal|Computational Statistics (journal)}}\n[[File:London School of Economics Statistics Machine Room 1964.jpg|thumb|right|Students working in the Statistics [[Computer lab|Machine Room]] of the [[London School of Economics]] in 1964.]]\n'''Computational statistics''', or '''statistical computing''', is the interface between [[statistics]] and [[computer science]]. It is the area of [[computational science]] (or scientific computing) specific to the mathematical science of [[statistics]]. This area is also developing rapidly, leading to calls that a broader concept of computing should be taught as part of general [[statistical education]].<ref>[[Deborah A. Nolan|Nolan, D.]] & Temple Lang, D. (2010). \"Computing in the Statistics Curricula\", ''[[The American Statistician]]'' '''64''' (2), pp.97-107.</ref>\n\nAs in [[Statistics|traditional statistics]] the goal is to transform [[raw data]] into [[knowledge]],<ref name=\":0\">[[Edward Wegman|Wegman, Edward]] J. “[https://www.jstor.org/stable/24536995 Computational Statistics: A New Agenda for Statistical Theory and Practice.]” ''[http://www.washacadsci.org/journal/ Journal of the Washington Academy of Sciences]'', vol. 78, no. 4, 1988, pp. 310–322. ''JSTOR''</ref> but the focus lies on [[computer]] intensive [[statistical methods]], such as cases with very large [[Sample size determination|sample size]] and non-homogeneous [[data set]]s.<ref name=\":0\" />\n\nThe terms 'computational statistics' and 'statistical computing' are often used interchangeably, although Carlo Lauro (a former president of the [[International Association for Statistical Computing]]) proposed making a distinction, defining 'statistical computing' as \"the application of computer science to statistics\",\nand 'computational statistics' as \"aiming at the design of algorithm for implementing\nstatistical methods on computers, including the ones unthinkable before the computer\nage (e.g. [[bootstrapping (statistics)|bootstrap]], [[Monte Carlo simulation|simulation]]), as well as to cope with analytically intractable problems\" [''[[sic]]''].<ref>{{Citation| first=Carlo|last=Lauro| title=Computational statistics or statistical computing, is that the question?| journal= Computational Statistics & Data Analysis| volume=23| issue=1| year=1996| pages=191–193| url=http://www.sciencedirect.com/science/article/B6V8V-3SWT44Y-F/2/5320a35df36fb38ffba03483c73dc861| doi=10.1016/0167-9473(96)88920-1}}</ref>\n\nThe term 'Computational statistics' may also be used to refer to computationally ''intensive'' statistical methods including [[resampling (statistics)|resampling]] methods, [[Markov chain Monte Carlo]] methods, [[local regression]], [[kernel density estimation]], [[artificial neural networks]] and [[generalized additive model]]s.\n\n==Computational statistics journals==\n*''[[Communications in Statistics|Communications in Statistics - Simulation and Computation]]''\n*''[[Computational Statistics]]''\n*''[[Computational Statistics & Data Analysis]]''\n*''[[Journal of Computational and Graphical Statistics]]''\n*''[[Journal of Statistical Computation and Simulation]]''\n*''[[Journal of Statistical Software]]''\n*''[[The R Journal]]''\n*''[[Statistics and Computing]]''\n*''[[Wiley Interdisciplinary Reviews Computational Statistics]]''\n\n==Associations==\n*[[International Association for Statistical Computing]]\n\n==See also==\n*[[Algorithms for statistical classification]]\n*[[Data science]]\n*[[Artificial intelligence#Statistical|Statistical methods in artificial intelligence]]\n*[[Free statistical software]]\n*[[List of algorithms#Statistics|List of statistical algorithms]]\n*[[List of statistical packages]]\n*[[Machine learning]]\n\n==References==\n<references/>\n\n==Further reading==\n\n===Articles===\n*{{Citation|last1=Albert |first1=J.H.| last2=Gentle|first2=J.E.|title = Special Section: Teaching Computational Statistics| journal = The American Statistician| volume = 58| year=2004| pages=1–1 |doi=10.1198/0003130042872|editor1-last=Albert|editor1-first=James H|editor2-last=Gentle|editor2-first=James E}}\n*{{Citation|last1= Wilkinson |first1=Leland|title = The Future of Statistical Computing (with discussion)| journal = Technometrics| volume = 50| issue=4| year=2008| pages=418–435 |doi=10.1198/004017008000000460}}\n\n===Books===\n*{{Citation|title= Computational Probability: Algorithms and Applications in the Mathematical Sciences |series=Springer International Series in Operations Research & Management Science |first1=John H.|last1=Drew |first2=Diane L. |last2=Evans| first3=Andrew G. |last3=Glen | first4=Lawrence M. |last4=Lemis |publisher=Springer| year= 2007|isbn= 0-387-74675-7}}\n*{{Citation|title=Elements of Computational Statistics| first=James E.| last=Gentle| year=2002| publisher=Springer| isbn=0-387-95489-9}}\n*{{Citation|title=Handbook of Computational Statistics: Concepts and Methods | editor1-first=James E. |editor1-last=Gentle| editor2-first=Wolfgang|editor2-last=Härdle |editor3-first=Yuichi |editor3-last=Mori| publisher=Springer| year=2004| isbn=3-540-40464-3}}\n*{{Citation|title=Computational Statistics |series=Wiley Series in Probability and Statistics|first1=Geof H. |last1=Givens |first2=Jennifer A. |last2=Hoeting|author2-link= Jennifer A. Hoeting |publisher=Wiley-Interscience| year= 2005|isbn=978-0-471-46124-1}}\n*{{Citation|title=Modeling with Data: Tools and Techniques for Statistical Computing|first=Ben| last=Klemens|year=2008|publisher=Princeton University Press|isbn=978-0-691-13314-0}}\n*{{Citation|title=Numerical Methods of Statistics| first=John| last=Monahan|year=2001|publisher=Cambridge University Press|isbn=978-0-521-79168-7}}\n*{{Citation|title=Mathematical Statistics with Mathematica |series=Springer Texts in Statistics|first1=Colin |last1=Rose |first2=Murray D. |last2=Smith|publisher=Springer| year= 2002|isbn=0-387-95234-9}}\n*{{Citation|title=Elements of Statistical Computing: Numerical Computation |first=Ronald Aaron |last=Thisted| publisher=CRC Press| year=1988| isbn=0-412-01371-1}}\n*{{Citation|title=Data Science: Scientific and Statistical Computing  |first=Reda. R. |last=Gharieb| publisher=Noor Publishing| year=2017| isbn=978-3-330-97256-8}}\n\n==External links==\n\n===Associations===\n*[http://www.iasc-isi.org/ International Association for Statistical Computing]\n*[http://stat-computing.org/ Statistical Computing section of the American Statistical Association]\n\n===Journals===\n*[http://www.elsevier.com/wps/find/journaldescription.cws_home/505539/description Computational Statistics & Data Analysis]\n*[http://www.amstat.org/publications/jcgs/ Journal of Computational & Graphical Statistics] \n*[https://www.springer.com/statistics/computational/journal/11222 Statistics and Computing]\n*[http://www.informaworld.com/smpp/title~db=all~content=t713597237 Communications in Statistics – Simulation and Computation]\n*[http://www.informaworld.com/smpp/title~content=t713650378 Journal of Statistical Computation and Simulation]\n\n[[Category:Numerical analysis]]\n[[Category:Computational statistics| ]]\n[[Category:Computational fields of study]]\n[[Category:Mathematics of computing]]"
    },
    {
      "title": "Computer-assisted proof",
      "url": "https://en.wikipedia.org/wiki/Computer-assisted_proof",
      "text": "A '''computer-assisted proof''' is a [[mathematical proof]] that has been at least partially generated by [[computer]].\n\nMost computer-aided proofs to date have been implementations of large [[Proof by exhaustion|proofs-by-exhaustion]] of a mathematical [[theorem]]. The idea is to use a computer program to perform lengthy computations, and to provide a proof that the result of these computations implies the given theorem.  In 1976, the [[four color theorem]] was the first major theorem to be verified using a [[computer program]].\n\nAttempts have also been made in the area of [[artificial intelligence]] research to create smaller, explicit, new proofs of mathematical theorems from the bottom up using [[machine reasoning]] techniques such as [[Heuristic (computer science)|heuristic]] search. Such [[automated theorem prover]]s have proved a number of new results and found new proofs for known theorems.{{Citation needed|date=August 2016}}  Additionally, interactive [[proof assistant]]s allow mathematicians to develop human-readable proofs which are nonetheless formally verified for correctness.  Since these proofs are generally [[Non-surveyable proof|human-surveyable]] (albeit with difficulty, as with the proof of the [[Robbins conjecture]]) they do not share the controversial implications of computer-aided proofs-by-exhaustion.\n\n== Methods ==\nOne method for using computers in mathematical proofs is by means of so-called [[validated numerics]] or rigorous numerics. This means computing numerically yet with mathematical rigour. One uses set-valued arithmetic and inclusion principle in order to ensure that the set-valued output of a numerical program encloses the solution of the original mathematical problem. This is done by controlling, enclosing and propagating round-off and truncation errors using for example [[interval arithmetic]]. More precisely, one reduces the computation to a sequence of elementary operations, say <math>(+,-,*,/)</math>. In a computer, the result of each elementary operation is rounded off by the computer precision. However, one can construct an interval provided by upper and lower bounds on the result of an elementary operation. Then one proceeds by replacing numbers with intervals and  performing  elementary operations between such intervals of representable numbers.\n\n==Philosophical objections==\n{{main|Non-surveyable proof}}\nComputer-assisted proofs are the subject of some controversy in the mathematical world, with [[Thomas Tymoczko]] first to articulate objections. Those who adhere to Tymoczko's arguments believe that lengthy computer-assisted proofs are not, in some sense, 'real' [[mathematical proof]]s because they involve so many logical steps that they are not practically [[Verification and validation|verifiable]] by human beings, and that mathematicians are effectively being asked to replace logical deduction from assumed axioms with trust in an empirical computational process, which is potentially affected by errors in the computer program, as well as defects in the runtime environment and hardware.<ref name=\"tymoczko\">{{Citation|last = Tymoczko|first = Thomas|author-link = Thomas Tymoczko|title = The Four-Color Problem and its Mathematical Significance|year = 1979|journal = [[The Journal of Philosophy]]|volume = 76|issue = 2|pages = 57–83|doi=10.2307/2025976}}.</ref>\n\nOther mathematicians believe that lengthy computer-assisted proofs should be regarded as ''calculations'', rather than ''proofs'': the proof algorithm itself should be proved valid, so that its use can then be regarded as a mere \"verification\".  Arguments that computer-assisted proofs are subject to errors in their source programs, compilers, and hardware can be resolved by providing a formal proof of correctness for the computer program (an approach which was successfully applied to the four-color theorem in 2005) as well as replicating the result using different programming languages, different compilers, and different computer hardware.\n\nAnother possible way of verifying computer-aided proofs is to generate their reasoning steps in a machine-readable form, and then use an [[automated theorem prover]] to demonstrate their correctness. This approach of using a computer program to prove another program correct does not appeal to computer proof skeptics, who see it as adding another layer of complexity without addressing the perceived need for human understanding.\n\nAnother argument against computer-aided proofs is that they lack [[mathematical elegance]]—that they provide no insights or new and useful concepts.  In fact, this is an argument that could be advanced against any lengthy proof by exhaustion.\n\nAn additional philosophical issue raised by computer-aided proofs is whether they make mathematics into a [[quasi-empiricism in mathematics|quasi-empirical science]], where the [[scientific method]] becomes more important than the application of pure reason in the area of abstract mathematical concepts. This directly relates to the argument within mathematics as to whether mathematics is based on ideas, or \"merely\" an [[exercise (mathematics)|exercise]] in formal symbol manipulation. It also raises the question whether, if according to the [[mathematical Platonism|Platonist]] view, all possible mathematical objects in some sense \"already exist\", whether computer-aided mathematics is an [[observational study|observational]] science like astronomy, rather than an experimental one like physics or chemistry. This controversy within mathematics is occurring at the same time as questions are being asked in the physics community about whether twenty-first century [[theoretical physics]] is becoming too mathematical, and leaving behind its experimental roots.\n\nThe emerging field of [[experimental mathematics]] is confronting this debate head-on by focusing on numerical experiments as its main tool for mathematical exploration.\n\n==Theorems for sale==\nIn 2010, academics at The [[University of Edinburgh]] offered people the chance to \"buy their own theorem\" created through a computer-assisted proof. This new theorem would be named after the purchaser.<ref>{{cite web|title=Herald Gazette article on buying your own theorem|url=http://www.heraldscotland.com/news/education/your-own-maths-theorem-for-15-1.1068654|work=Herald Gazette Scotland|date=November 2010|deadurl=yes|archiveurl=https://web.archive.org/web/20101121000707/http://www.heraldscotland.com/news/education/your-own-maths-theorem-for-15-1.1068654|archivedate=2010-11-21|df=}}</ref><ref>{{cite web|title=School of Informatics, Univ.of Edinburgh website|url=http://www.ed.ac.uk/informatics/news-events/recentnews/theorem|work=School of Informatics, Univ.of Edinburgh|date=April 2015}}</ref>\n\n==List of theorems proved with the help of computer programs==\nInclusion in this list does not imply that a formal computer-checked proof exists, but rather, that a computer program has been involved in some way. See the main articles for details.\n*[[Four color theorem]], 1976\n*[[Mitchell Feigenbaum]]'s universality conjecture in non-linear dynamics. Proven by O. E. Lanford using rigorous computer arithmetic, 1982\n*[[Connect Four]], 1988 – a solved game\n*Non-existence of a finite [[projective plane]] of order 10, 1989\n*[[Double bubble conjecture]], 1995<ref>Hass, J., Hutchings, M., & Schlafly, R. (1995). The double bubble conjecture. Electronic Research Announcements of the American Mathematical Society, 1(3), 98-102.</ref>\n\n*[[Robbins conjecture]], 1996\n*[[Kepler conjecture]], 1998 – the problem of optimal sphere packing in a box\n*[[Lorenz attractor]], 2002 – 14th of [[Smale's problems]] proved by [[Warwick Tucker]] using [[interval arithmetic]]\n*17-point case of the [[Happy Ending problem]], 2006\n*[[NP-hard]]ness of [[minimum-weight triangulation]], 2008\n*[[Optimal solutions for Rubik's Cube]] can be obtained in at most 20 face moves, 2010\n*Minimum number of clues for a solvable [[Sudoku|Sudoku puzzle]] is 17, 2012\n*In 2014 a special case of the [[Erdős discrepancy problem]] was solved using a [[SAT-solver]]. The full conjecture was later solved by [[Terence Tao]] without computer assistance.<ref>{{cite web|last1=Cesare|first1=Chris|title=Maths whizz solves a master’s riddle|url=http://www.nature.com/news/maths-whizz-solves-a-master-s-riddle-1.18441|website=Nature|pages=19–20|doi=10.1038/nature.2015.18441|date=1 October 2015}}</ref>\n*[[Boolean Pythagorean triples problem]] solved using 200 terabytes of data in May 2016.<ref>{{Cite journal|last = Lamb|first = Evelyn|date = 26 May 2016|title = Two-hundred-terabyte maths proof is largest ever|url = http://www.nature.com/news/two-hundred-terabyte-maths-proof-is-largest-ever-1.19990|journal = Nature|doi = 10.1038/nature.2016.19990|volume=534|pages=17–18|pmid=27251254}}</ref>\n\n== See also ==\n* [[Mathematical proof]]\n* [[Model checking]]\n* [[Proof checking]]\n* [[Symbolic computation]]\n* [[Automated reasoning]]\n* [[Formal verification]]\n* [[Seventeen or Bust]]\n* [[Metamath]]\n\n==References==\n{{Reflist}}\n\n==Further reading==\n* Lenat, D.B., (1976), [http://www.dtic.mil/get-tr-doc/pdf?AD=ADA155378 AM: An artificial intelligence approach to discovery in mathematics as heuristic search], Ph.D. Thesis, STAN-CS-76-570, and Heuristic Programming Project Report HPP-76-8, Stanford University, AI Lab., Stanford, CA.\n\n==External links==\n* Oscar E. Lanford; [http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.bams/1183548786 A computer-assisted proof of the Feigenbaum conjectures], \"Bull. Amer. Math. Soc.\", 1982\n* Edmund Furse; [https://web.archive.org/web/20120717094035/https://www.comp.glam.ac.uk/pages/staff/efurse/Abstracts/Why-did-AM-halt.html Why did AM run out of steam?]\n* [http://www.post-gazette.com/pg/07012/753384-28.stm Number proofs done by computer might err]\n* {{cite web|title=A Special Issue on Formal Proof|url=http://www.ams.org/notices/200811/|work=Notices of the American Mathematical Society|date=December 2008}}\n\n[[Category:Argument technology]]\n[[Category:Artificial intelligence]]\n[[Category:Formal methods]]\n[[Category:Philosophy of mathematics]]\n[[Category:Automated theorem proving]]\n[[Category:Computer-assisted proofs]]\n[[Category:Numerical analysis]]"
    },
    {
      "title": "Condition number",
      "url": "https://en.wikipedia.org/wiki/Condition_number",
      "text": "In the field of [[numerical analysis]], the '''condition number''' of a function measures how much the output value of the function can change for a small change in the input argument. This is used to measure how [[sensitivity analysis|sensitive]] a function is to changes or errors in the input, and how much error in the output results from an error in the input. Very frequently, one is solving the inverse problem – given <math>f(x) = y,</math> one is solving for ''x,'' and thus the condition number of the (local) inverse must be used. In [[linear regression]] the condition number of the [[moment matrix]] can be used as a diagnostic for [[multicollinearity]].<ref>{{cite book |first=David A. |last=Belsley |first2=Edwin |last2=Kuh |authorlink2=Edwin Kuh |first3=Roy E. |last3=Welsch |chapter=The Condition Number |title=Regression Diagnostics: Identifying Influential Data and Sources of Collinearity |location=New York |publisher=John Wiley & Sons |year=1980 |isbn=0-471-05856-4 |pages=100–104 | chapterurl =https://books.google.com/books?id=GECBEUJVNe0C&pg=PA100 }}</ref><ref>{{cite book |first=M. Hashem |last=Pesaran |authorlink=M. Hashem Pesaran |chapter =The Multicollinearity Problem |title=Time Series and Panel Data Econometrics |location=New York |publisher=Oxford University Press |year=2015 | isbn=978-0-19-875998-0 |pages=67–72 [p. 70] |chapterurl=https://books.google.com/books?id=7RokCwAAQBAJ&pg=PA70 }}</ref>\n\nThe condition number is an application of the derivative, and is formally defined as the value of the asymptotic worst-case relative change in output for a relative change in input. The \"function\" is the solution of a problem and the \"arguments\" are the data in the problem. The condition number is frequently applied to questions in linear algebra, in which case the derivative is straightforward but the error could be in many different directions, and is thus computed from the geometry of the matrix. More generally, condition numbers can be defined for non-linear functions in several variables.\n\nA problem with a low condition number is said to be '''well-conditioned''', while a problem with a high condition number is said to be '''ill-conditioned'''. The condition number is a property of the problem.  Paired with the problem are any number of algorithms that can be used to solve the problem, that is, to calculate the solution.  Some algorithms have a property called '''[[Numerical stability|backward stability]]'''. In general, a backward stable algorithm can be expected to accurately solve well-conditioned problems. Numerical analysis textbooks give formulas for the condition numbers of problems and identify known backward stable algorithms.\n\nAs a rule of thumb, if the condition number <math>\\kappa(A) = 10^k</math>, then you may lose up to <math>k</math> digits of accuracy on top of what would be lost to the numerical method due to loss of precision from arithmetic methods.<ref name=\"Numerical Mathematics and Computing, by Cheney and Kincaid\">{{cite book|url=https://books.google.com/books?id=ZUfVZELlrMEC&pg=PA321 |title= Numerical Mathematics and Computing |last1=Cheney |last2=Kincaid |isbn= 978-0-495-11475-8 |date=2007-08-03 }}</ref> However, the condition number does not give the exact value of the maximum inaccuracy that may occur in the algorithm. It generally just bounds it with an estimate (whose computed value depends on the choice of the norm to measure the inaccuracy).\n\n== General definition in the context of error analysis ==\n\nGiven a problem ''f'' and an algorithm <math>\\tilde{f}</math> with an input ''x'', the ''absolute'' error is <math>\\left\\|f(x) - \\tilde{f}(x)\\right\\|</math> and the ''relative'' error is <math>\\left\\|f(x) - \\tilde{f}(x)\\right\\| / \\left\\|f(x)\\right\\|</math>.\n\nIn this context, the ''absolute'' condition number of a problem ''f'' is\n\n: <math> \\lim_{\\varepsilon \\rightarrow 0} \\sup_{\\|\\delta x\\| \\leq \\varepsilon} \\frac{\\|\\delta f\\|}{\\|\\delta x\\|} </math>\n\nand the ''relative'' condition number is\n\n: <math> \\lim_{\\varepsilon \\rightarrow 0} \\sup_{\\|\\delta x \\| \\leq \\varepsilon} \\frac{\\|\\delta f(x)\\| / \\|f(x)\\|}{\\|\\delta x\\| / \\|x\\|} </math>\n\n== Matrices ==<!-- This section is linked from [[Invertible matrix]] -->\n\nFor example, the condition number associated with the [[linear equation]]\n''Ax''&nbsp;=&nbsp;''b'' gives a bound on how inaccurate the solution ''x'' will be after approximation. Note that this is before the effects of [[round-off error]] are taken into account; conditioning is a property of the matrix, not the [[algorithm]] or [[floating point]] accuracy of the computer used to solve the corresponding system. In particular, one should think of the condition number as being (very roughly) the rate at which the solution, ''x'', will change with respect to a change in ''b''.  Thus, if the condition number is large, even a small error in ''b'' may cause a large error in ''x''. On the other hand, if the condition number is small then the error in ''x'' will not be much bigger than the error in ''b''.\n\nThe condition number is defined more precisely to be the maximum ratio of the [[relative error]] in ''x'' to the relative error in ''b''.\n\nLet ''e'' be the error in ''b''. Assuming that ''A'' is a nonsingular matrix, the error in the solution ''A''<sup>−1</sup>''b'' is ''A''<sup>−1</sup>''e''. The ratio of the relative error in the solution to the relative error in ''b'' is\n\n: <math> \\frac{\\frac{ \\left\\| A^{-1} e \\right\\| } {\\left\\| A^{-1} b \\right\\|} }{\\frac{\\| e \\|}{\\| b\\|}} </math>\n\nThis is easily transformed to\n\n: <math>\\frac{\\left\\| A^{-1} e \\right\\|}{\\| e \\|} \\frac{\\| b \\|}{\\left\\| A^{-1} b \\right\\|}.</math>\n\nThe maximum value (for nonzero ''b'' and ''e'') is then seen to be the product of the two [[operator norm]]s as follows:\n\n:<math>\\begin{align}\n  \\max_{e,b \\neq 0} \\left\\{ \\frac{\\left\\| A^{-1}e \\right\\|}{\\| e \\|} \\frac{\\| b \\|}{\\left\\| A^{-1}b \\right\\|} \\right\\}\n    &= \\max_{e \\neq 0} \\left\\{\\frac{\\left\\| A^{-1}e\\right\\| }{\\| e\\|} \\right\\} \\, \\max_{b \\neq 0} \\left\\{ \\frac {\\| b \\|}{\\left\\| A^{-1}b \\right\\|} \\right\\} \\\\\n    &= \\max_{e \\neq 0} \\left\\{\\frac{\\left\\| A^{-1}e\\right\\|}{\\| e \\|}\\right\\} \\, \\max_{x \\neq 0} \\left \\{\\frac {\\| Ax \\| }{\\| x \\|} \\right\\} \\\\\n    &= \\left\\| A^{-1} \\right \\| \\, \\|A\\|\n\\end{align}</math>\n\nThe same definition is used for any consistent [[matrix norm|norm]], i.e. one that satisfies\n\n: <math> \\kappa(A) = \\left\\| A^{-1} \\right\\| \\, \\left\\| A \\right\\| \\ge \\left\\| A^{-1} A \\right\\| = 1 .</math>\n\nWhen the condition number is exactly one (which can only happen if ''A'' is a  scalar multiple of a [[Isometry#Linear isometry|linear isometry]]), then a solution algorithm can  find (in principle, meaning if the algorithm introduces no errors of its own) an approximation of the solution whose precision is no worse than that of the data.\n\nHowever, it does not mean that the algorithm will converge rapidly to this solution, just that it won't diverge arbitrarily because of inaccuracy on the source data (backward error), provided that the forward error introduced by the algorithm does not diverge as well because of accumulating intermediate rounding errors.{{clarify|date=October 2014}}\n\nThe condition number may also be infinite, but this implies that the problem is [[well-posed problem|ill-posed]] (does not possess a unique, well-defined solution for each choice of data; that is, the matrix is not invertible), and no algorithm can be expected to reliably find a solution.\n\nThe definition of the condition number depends on the choice of norm, as can be illustrated by two examples.\n\nIf <math>\\| \\cdot\\| </math> is the [[matrix norm|norm]] defined in the square-summable [[sequence space]] [[Lp space|ℓ<sup>2</sup>]] (which matches the usual distance in a standard Euclidean space and is usually noted as <math>\\| \\cdot \\|_2 </math>), then\n\n: <math> \\kappa(A) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)},</math>\n\nwhere <math> \\sigma_{\\max}(A)</math> and <math>\\sigma_{\\min}(A)</math> are maximal and minimal [[singular value]]s of <math>A</math> respectively. Hence\n\n* If <math>A</math> is [[normal matrix|normal]] then\n::<math> \\kappa(A) = \\frac{\\left|\\lambda_{\\max}(A)\\right|}{\\left|\\lambda_{\\min}(A)\\right|} ,</math>\n:where <math>\\lambda_{\\max}(A)</math> and <math>\\lambda_{\\min}(A) </math> are maximal and minimal (by moduli) [[eigenvalue]]s of <math>A</math> respectively.\n\n* If <math>A</math> is [[unitary matrix|unitary]] then <math> \\kappa(A) = 1 .</math>\n\nThe condition number with respect to ''L''<sup>2</sup> arises so often in numerical [[linear algebra]] that it is given a name, the '''condition number of a matrix'''.\n\nIf <math>\\| \\cdot \\| </math> is the [[matrix norm|norm]] defined in the [[sequence space]] [[Lp space|ℓ<sup>∞</sup>]] of all [[Bounded operator|bounded]] sequences (which matches the maximum of distances measured on projections into the base subspaces and is usually denoted by <math>\\| \\cdot \\|_\\infty </math>), and <math>A</math> is [[triangular matrix|lower triangular]] non-singular (i.e., <math>\\forall i, a_{ii} \\ne 0</math>) then\n\n: <math> \\kappa(A) \\geq \\frac{\\max_i(|a_{ii}|)}{\\min_i(|a_{ii}|)} .</math>\n\nThe condition number computed with this norm is generally larger than the condition number computed with square-summable sequences, but it can be evaluated more easily (and this is often the only practicably computable condition number, when the problem to solve involves a ''non-linear algebra''{{what?|date=October 2014}}, for example when approximating irrational and transcendental functions or numbers with numerical methods).\n\nIf the condition number is not too much larger than one, the matrix is well conditioned which means its inverse can be computed with good accuracy. If the condition number is very large, then the matrix is said to be ill-conditioned. Practically, such a matrix is almost singular, and the computation of its inverse, or solution of a linear system of equations is prone to large numerical errors. A matrix that is not invertible has condition number equal to infinity.\n\n== Nonlinear ==\nCondition numbers can also be defined for nonlinear functions, and can be computed using calculus. The condition number varies with the point; in some cases one can use the maximum (or supremum) condition number over the domain of the function or domain of the question as an overall condition number, while in other cases the condition number at a particular point is of more interest.\n\n=== One variable ===\n{{See also|Significance arithmetic#Transcendental functions}}\n\nThe condition number of a differentiable function ''f'' in one variable as a function is <math>\\left|xf'/f\\right|.</math> Evaluated at a point ''x'' this is:\n\n:<math>\\left|\\frac{xf'(x)}{f(x)}\\right|</math>\n\nMost elegantly, this can be understood as (the absolute value of) the ratio of the [[logarithmic derivative]] of ''f,'' which is <math>(\\log f)' = f'/f</math> and the logarithmic derivative of ''x,'' which is <math>(\\log x)' = x'/x = 1/x,</math> yielding a ratio of <math>xf'/f.</math> This is because the logarithmic derivative is the infinitesimal rate of relative change in a function: it is the derivative <math>f'</math> scaled by the value of ''f.'' Note that if a function has a zero at a point, its condition number at the point is infinite, as infinitesimal changes in the input can change the output from zero to positive or negative, yielding a ratio with zero in the denominator, hence infinite relative change.\n\nMore directly, given a small change <math>\\Delta x</math> in ''x,'' the relative change in ''x'' is <math>[(x + \\Delta x) - x]/x = (\\Delta x)/x,</math> while the relative change in <math>f(x)</math> is <math>[f(x + \\Delta x) - f(x)]/f(x).</math> Taking the ratio yields:\n\n:<math>\\frac{[f(x + \\Delta x) - f(x)]/f(x)}{(\\Delta x)/x}= \\frac{x}{f(x)}\\frac{f(x + \\Delta x) - f(x)}{(x + \\Delta x) - x}.</math>\n\nThe last term is the [[difference quotient]] (the slope of the secant line), and taking the limit yields the derivative.\n\nCondition numbers of common [[elementary function]]s are particularly important in computing [[significant figures]], and can be computed immediately from the derivative; see [[Significance arithmetic#Transcendental functions|significance arithmetic of transcendental functions]]. A few important ones are given below:\n{{aligned table\n| class=wikitable\n| cols=3\n| col1style=text-align:right; vertical-align:middle;\n| col2style=text-align:right; vertical-align:middle;\n| row1header=on\n| Name | Symbol | Condition number\n| Exponential function | <math>e^x</math> | <math>x</math>\n| Natural logarithm function | <math>\\ln(x)</math> | <math>\\frac{1}{\\ln(x)}</math>\n| Sine function | <math>\\sin(x)</math> | <math>x\\cot(x)</math>\n| Cosine function | <math>\\cos(x)</math> | <math>x\\tan(x)</math>\n| Tangent function | <math>\\tan(x)</math> | <math>x(\\tan(x)+\\cot(x))</math>\n| Inverse sine function | <math>\\arcsin(x)</math> | <math>\\frac{x}{\\sqrt{1-x^2}\\arcsin(x)}</math>\n| Inverse cosine function | <math>\\arccos(x)</math> | <math>\\frac{x}{\\sqrt{1-x^2}\\arccos(x)}</math>\n| Inverse tangent function | <math>\\arctan(x)</math> | <math>\\frac{x}{(1+x^2)\\arctan(x)}</math>\n}}\n\n=== Several variables ===\n\nCondition numbers can be defined for any function ''f'' mapping its data from some [[function domain|domain]] (e.g. an ''m''-tuple of real numbers ''x'') into some [[codomain]] [e.g. an ''n''-tuple of real numbers ''f''(''x'')], where both the domain and codomain are [[Banach space]]s.  They express how sensitive that function is to small changes (or small errors) in its arguments.  This is crucial in assessing the sensitivity and potential accuracy difficulties of numerous computational problems, for example [[polynomial]] [[root finding]] or computing [[eigenvalue]]s.\n\nThe condition number of ''f'' at a point ''x'' (specifically, its '''relative condition number'''<ref name=TrefethenBau />) is then defined to be the maximum ratio of the fractional change in ''f''(''x'') to any fractional change in ''x'', in the limit where the change δ''x'' in ''x'' becomes infinitesimally small:<ref name=TrefethenBau>{{cite book|isbn= 978-0-89871-361-9|first1=L. N.|last1= Trefethen |first2= D. |last2=Bau|title=Numerical Linear Algebra|publisher=SIAM|year=1997|url=https://books.google.com/books?id=JaPtxOytY7kC&printsec=frontcover&dq=978-0898713619#v=onepage&q&f=false}}</ref>\n\n:<math>\\lim_{\\varepsilon \\to 0^+ } \\sup_{ \\| \\delta x \\| \\leq \\varepsilon} \\left[  \\left. \\frac{ \\left\\| f(x + \\delta x) - f(x)\\right\\| }{\\| f(x) \\|} \\right/ \\frac{ \\| \\delta x \\| }{ \\| x \\| } \\right],</math>\n\nwhere <math>\\| \\cdot \\|</math> is a [[Norm (mathematics)|norm]] on the domain/codomain of ''f''(''x'').\n\nIf ''f'' is differentiable, this is equivalent to:<ref name=TrefethenBau />\n\n: <math>\\frac{\\| J (x)\\|}{ \\| f(x) \\| / \\| x \\|},</math>\n\nwhere {{tmath|J(x)}} denotes the [[Jacobian matrix]] of [[partial derivative]]s of ''f'' at ''x'' and <math>\\| J (x)\\|</math> is the [[induced norm]] on the matrix.\n\n== See also ==\n* [[Numerical methods for linear least squares]]\n* [[Ill-posed problem]]\n* [[Singular value]]\n\n== References ==\n<references />\n\n== External links ==\n* [https://web.archive.org/web/20070121001740/http://numericalmethods.eng.usf.edu/mws/gen/04sle/mws_gen_sle_spe_adequacy.pdf Condition Number of a Matrix] at ''Holistic Numerical Methods Institute''\n* {{planetmath reference|id=3480|title=Matrix condition number}}\n* [http://www.mathworks.in/help/techdoc/ref/cond.html MATLAB library function to determine condition number]\n* [https://www.encyclopediaofmath.org/index.php/Condition_number Condition number – Encyclopedia of Mathematics]\n\n[[Category:Numerical analysis]]\n[[Category:Matrices]]"
    },
    {
      "title": "Continuous wavelet",
      "url": "https://en.wikipedia.org/wiki/Continuous_wavelet",
      "text": "{{Unreferenced|date=December 2009}}\nIn [[numerical analysis]], '''continuous [[wavelet]]s''' are functions used by the [[continuous wavelet transform]]. These functions are defined as [[analytical expression]]s, as functions either of time or of frequency.\nMost of the continuous wavelets are used for both wavelet decomposition and composition transforms. That is they are the continuous counterpart of [[orthogonal wavelet]]s.\n\nThe following continuous wavelets have been invented for various applications:\n\n* [[Poisson wavelet]]\n* [[Morlet wavelet]]\n* [[Modified Morlet wavelet]]\n* [[Mexican hat wavelet]]\n* [[Complex Mexican hat wavelet]]\n* [[Shannon wavelet]]\n* [[Meyer wavelet]]\n* [[Difference of Gaussians]]\n* [[Hermitian wavelet]]\n* [[Hermitian hat wavelet]]\n* [[Beta wavelet]]\n* [[Causal wavelet]]\n* [[μ wavelet]]s\n* [[Cauchy wavelet]]\n* [[Addison wavelet]]\n\n==See also==\n*[[Wavelet]]\n\n{{DEFAULTSORT:Continuous Wavelet}}\n[[Category:Continuous wavelets|*]]\n[[Category:Numerical analysis]]\n[[Category:Functional analysis]]"
    }
  ]
}