{
  "pages": [
    {
      "title": "Odd–even sort",
      "url": "https://en.wikipedia.org/wiki/Odd%E2%80%93even_sort",
      "text": "{{For|the (log&nbsp;n)² / n(log&nbsp;n)² parallel algorithm|Batcher odd–even mergesort}}\n{{Infobox Algorithm\n|class=[[Sorting algorithm]]\n|image=[[File:Odd even sort animation.gif|Example of odd-even transposition sort sorting a list of random numbers.|Example of odd-even transposition sort sorting a list of random numbers.]]\n|caption=Example of odd-even transposition sort sorting a list of random numbers.\n|data=[[Array data structure|Array]]\n|time=<math>O(n^2)</math>\n|best-time=<math>O(n)</math>\n|space= <math>O(1)</math>\n|optimal=No\n}}\n\nIn computing, an '''odd–even sort''' or '''odd–even transposition sort''' (also known as '''brick sort'''<ref>{{cite web|last=Phillips|first=Malcolm|title=Array Sorting|url=http://homepages.ihug.co.nz/~aurora76/Malc/Sorting_Array.htm#Exchanging_Sort_Techniques|website=Homepages.ihug.co.nz|accessdate=3 August 2011|deadurl=yes|archiveurl=https://web.archive.org/web/20111028201105/http://homepages.ihug.co.nz/~aurora76/Malc/Sorting_Array.htm#Exchanging_Sort_Techniques|archivedate=28 October 2011|df=}}</ref>{{self-published source|date=July 2014}}) is a relatively simple [[sorting algorithm]], developed originally for use on parallel processors with local interconnections.  It is a [[comparison sort]] related to [[bubble sort]], with which it shares many characteristics.  It functions by comparing all odd/even indexed pairs of adjacent elements in the list and, if a pair is in the wrong order (the first is larger than the second) the elements are switched.  The next step repeats this for even/odd indexed pairs (of adjacent elements). Then it alternates between odd/even and even/odd steps until the list is sorted.\n\n==Sorting on processor arrays==\nOn parallel processors, with one value per processor and only local left–right neighbor connections, the processors all concurrently do a compare–exchange operation with their neighbors, alternating between odd–even and even–odd pairings.  This algorithm was originally presented, and shown to be efficient on such processors, by Habermann in 1972.<ref>N. Haberman (1972) \"Parallel Neighbor Sort (or the Glory of the Induction Principle),\" CMU Computer Science Report (available as Technical report AD-759 248, National Technical Information Service, US Department of Commerce, 5285 Port Royal Rd Sprigfield VA 22151).</ref>\n\nThe algorithm extends efficiently to the case of multiple items per processor.  In the Baudet–Stevenson odd–even merge-splitting algorithm, each processor sorts its own sublist at each step, using any efficient sort algorithm, and then performs a merge splitting, or transposition–merge, operation with its neighbor, with neighbor pairing alternating between odd–even and even–odd on each step.<ref>\n{{citation\n | journal = Advances in computers\n | editor1-first = Franz L.\n | editor1-last = Alt\n | editor2-first = Marshall C.\n | editor2-last = Yovits\n | title = Parallel Sorting Algorithms\n | first1 = S.\n | last1 = Lakshmivarahan\n | first2 = S. K.\n | last2 = Dhall \n | first3 = L. L.\n | last3 = Miller  \n | last-author-amp=yes \n | volume = 23\n | publisher = Academic Press\n | pages = 295–351\n | isbn = 978-0-12-012123-6\n | year = 1984\n | url = https://books.google.com/books?id=Mo2Q-TEwKGUC&pg=PA322\n }}</ref>\n\n==Batcher's odd–even mergesort==\n\nA related but more efficient sort algorithm is the [[Batcher odd–even mergesort]], using compare–exchange operations and perfect-shuffle operations.<ref>\n{{cite book\n | title = Algorithms in Java, Parts 1-4\n | edition = 3rd\n | first = Robert\n | last = Sedgewick\n | publisher = Addison-Wesley Professional\n | year = 2003\n | isbn = 978-0-201-36120-9\n | pages = 454–464\n | url = https://books.google.com/books?id=hyvdUQUmf2UC&pg=PA455\n }}</ref>\nBatcher's method is efficient on parallel processors with long-range connections.<ref>\n{{cite book\n | title = Encyclopedia of Computer Science and Technology: Supplement 14\n | author1-link = Allen Kent\n | first1 = Allen\n | last1 = Kent\n | first2 = James G.\n | last2 = Williams\n | publisher = CRC Press\n | year = 1993\n | isbn = 978-0-8247-2282-1\n | pages = 33–38\n | url = https://books.google.com/books?id=F9Y4oZ9qZnYC&pg=PA33\n }}</ref>\n\n== Algorithm ==\n\nThe single-processor algorithm, like [[bubblesort]], is simple but not very efficient. Here a [[zero-based]] index is assumed:\n\n<source lang=\"javascript\">\nfunction oddEvenSort(list) {\n  function swap( list, i, j ){\n    var temp = list[i];\n    list[i] = list[j];\n    list[j] = temp;\n  }\n\n  var sorted = false;\n  while(!sorted)\n  {\n    sorted = true;\n    for(var i = 1; i < list.length-1; i += 2)\n    {\n      if(list[i] > list[i+1])\n      {\n        swap(list, i, i+1);\n        sorted = false;\n      }\n    }\n\n    for(var i = 0; i < list.length-1; i += 2)\n    {\n      if(list[i] > list[i+1])\n      {\n        swap(list, i, i+1);\n        sorted = false;\n      }\n    }\n  }\n}\n</source>\n\n==Proof of correctness==\nClaim:  Let <math>a_1, ..., a_n</math> be a sequence of data ordered by <.  The odd-even sort algorithm correctly sorts this data in <math>n</math> passes.  (A pass here is defined to be a full sequence of odd-even, or even-odd comparisons.  The passes occur in order pass 1: odd-even, pass 2: even-odd, etc.)\n\nProof:\n\nThis proof is based loosely on one by Thomas Worsch.<ref>{{cite web|url=http://liinwww.ira.uka.de/~thw/vl-hiroshima/slides-4.pdf|format=PDF|title=Five Lectures on CA|website=Liinwww.ira.uka.de|accessdate=2017-07-30}}</ref>\n\nSince the sorting algorithm only involves comparison-swap operations and is oblivious (the order of comparison-swap operations does not depend on the data), by Knuth's 0-1 sorting principle,<ref>{{cite web|url=http://www.iti.fh-flensburg.de/lang/algorithmen/sortieren/networks/nulleinsen.htm|title=The 0-1-principle|first=Hans Werner|last=Lang|website=Iti.fh-flensburg.de|accessdate=30 July 2017}}</ref><ref>{{cite web|url=http://www.net.t-labs.tu-berlin.de/~stefan/netalg13-9-sort.pdf|format=PDF|title=Distributed Sorting|website=Net.t-labs.tu-berlin.de|accessdate=2017-07-30}}</ref> it suffices to check correctness when each <math>a_i</math> is either 0 or 1. Assume that there are <math>e</math> 1's.\n\nObserve that the rightmost 1 can be either in an even or odd position, so it might not be moved by the first odd-even pass. But after the first odd-even pass, the rightmost 1 will be in an even position. It follows that it will be moved to the right by all remaining passes. Since the rightmost one starts in position greater than or equal to <math>e</math>, it must be moved at most <math>n-e</math> steps. It follows that it takes at most <math>n-e+1</math> passes to move the rightmost 1 to its correct position.\n\nNow, consider the second rightmost 1. After two passes, the 1 to its right will have moved right by at least one step. It follows that, for all remaining passes, we can view the second rightmost 1 as the rightmost 1. The second rightmost 1 starts in position at least <math>e-1</math> at must be moved to position at most <math>n-1</math>, so it must be moved at most <math>(n-1) - (e-1) = n-e</math> steps. After at most 2 passes, the rightmost 1 will have already moved, so the entry to the right of the second rightmost 1 will be 0.  Hence, for all passes after the first two, the second rightmost 1 will move to the right. It thus takes at most <math>n-e+2</math> passes to move the second rightmost 1 to its correct position.\n\nContinuing in this manner, by induction it can be shown that the <math>i</math>-th rightmost 1 is moved to its correct position in at most <math>n-e+i</math> passes. Since <math>i \\leq e</math>, it follows that the <math>i</math>-th rightmost 1 is moved to its correct position in at most <math>n-e+e = n</math> passes. The list is thus correctly sorted in <math>n</math> passes. QED.\n\nWe remark that each pass takes <math>O(n)</math> steps, so this algorithm is <math>O(n^2)</math> complexity.\n\n==References==\n{{Reflist}}\n\n{{sorting}}\n\n{{DEFAULTSORT:Odd-even sort}}\n[[Category:Sorting algorithms]]\n[[Category:Comparison sorts]]\n[[Category:Stable sorts]]\n[[Category:Articles with example pseudocode]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "Oscillating merge sort",
      "url": "https://en.wikipedia.org/wiki/Oscillating_merge_sort",
      "text": "'''Oscillating merge sort''' or '''oscillating sort''' is a variation of [[merge sort]] used with tape drives that can read backwards.  Instead of doing a complete distribution as is done in a tape merge, the distribution of the input and the merging of runs are interspersed.  The oscillating merge sort does not waste rewind time or have tape drives sit idle as in the conventional tape merge.\n\nThe oscillating merge sort \"was designed for tapes that can be read backward and is more efficient generally than either the [[Polyphase merge sort|polyphase]] or [[Cascade merge sort|cascade]] merges.\"<ref>{{harvnb|Bradley|1982|p=190}}</ref>\n\n==References==\n{{Reflist}}\n\n*{{Citation |last=Bradley |first=James |year=1982 |title=File and Data Base Techniques |publisher=Holt, Rinehart and Winston |isbn=0-03-058673-9 |doi= }}\n\n==Further reading==\n*{{Citation |last=Flores |first=Ivan |year=1969 |title=Computer Sorting |publisher=Prentice-Hall |isbn=978-0-13165746-5 |doi= |ref=none}}\n*{{Citation |last=Knuth |first=D. E. |year=1975 |title=Sorting and Searching |series=[[The Art of Computer Programming]] |volume=3 |publisher=Addison Wesley |isbn= |doi= |ref=none}}\n*{{Citation |last=Lowden |first=B. G. T. |title=A note on the oscillating sort |journal=The Computer Journal |volume=20 |issue=1 |page=92 |date= |url=http://comjnl.oxfordjournals.org/content/20/1/92.full.pdf |doi= 10.1093/comjnl/20.1.92 |ref=none}}\n*{{Citation |last=Martin |first=W. A. |year=1971 |title=Sorting |journal=Computing Surveys |publisher=ACM |doi= |ref=none}}\n*{{Citation |last=Sobel |first=Sheldon |title=Oscillating Sort&ndash;A New Sort Merging Technique |journal=Journal of the ACM |publisher=ACM |location=New York, NY |volume=9 |issue=3 |pages=372&ndash;374 |date=July 1962 |doi=10.1145/321127.321133 |ref=none}}\n\n==External links==\n*Mihaldinecz, Maximilian (2016), \"[https://github.com/MaximilianMihaldinecz/oscillating-merge-sort A variation of Oscillating Merge Sort implemented in Matlab]\", GitHub\n\n{{sorting}}\n\n<!-- [[Category:Articles with example pseudocode]] -->\n[[Category:Sorting algorithms]]\n[[Category:Comparison sorts]]\n[[Category:Stable sorts]]"
    },
    {
      "title": "Pairwise sorting network",
      "url": "https://en.wikipedia.org/wiki/Pairwise_sorting_network",
      "text": "{{Infobox Algorithm\n|class=[[Sorting algorithm]]\n|image=[[File:Pairwise Sorting Network for 16 inputs.svg|Visualization of the Pairwise sorting network with 16 inputs|300px]]\n|caption=Visualization of the Pairwise sorting network with 16 inputs\n|data=[[Array data structure|Array]]\n|time=<math>(\\log n)(\\log n + 1)/2</math> parallel time\n|space=<math>n(\\log n)(\\log n - 1)/4 + n - 1</math> non-parallel time\n|optimal=No\n}}\nThe '''pairwise sorting network''' is a [[sorting network]] discovered and published by Ian Parberry in 1992 in ''[[Parallel Processing Letters]]''.<ref>{{Citation\n | last=Parberry\n | first=Ian\n | title=The Pairwise Sorting Network\n | journal=Parallel Processing Letters\n | volume=2\n | issue=2,3\n | pages=205–211\n | year=1992\n | url=http://larc.unt.edu/ian/pubs/pairwise.pdf\n}}</ref> The pairwise sorting network has the same cost (number of comparators) and delay as the [[Batcher odd-even mergesort|odd-even mergesort network]]. It requires <math>n(\\log n)(\\log n - 1)/4 + n - 1</math> comparators and has depth <math>(\\log n)(\\log n + 1)/2</math>.\n\n== References ==\n<References />\n\n== External links ==\n* [http://larc.unt.edu/ian/research/sortingnetworks/ Sorting Networks] – Web page by the author.\n\n\n{{sorting}}\n\n{{DEFAULTSORT:Pairwise Sorting Network}}\n[[Category:Sorting algorithms]]\n\n\n{{algorithm-stub}}"
    },
    {
      "title": "Pancake sorting",
      "url": "https://en.wikipedia.org/wiki/Pancake_sorting",
      "text": "{{Use mdy dates|date=October 2014}}\n[[File:Pancake sort operation.png|right|frame|Demonstration of the primary operation. The spatula is flipping over the top three pancakes, with the result seen below. In the burnt pancake problem, their top sides would now be burnt instead of their bottom sides.]]\n\n'''Pancake sorting''' is the colloquial term for the mathematical problem of sorting a disordered stack of pancakes in order of size when a [[spatula]] can be inserted at any point in the stack and used to flip all pancakes above it.  A ''pancake number'' is the minimum number of flips required for a given number of pancakes. In this form, the problem was first discussed by [[United States of America|American]] [[geometer]] [[Jacob E. Goodman]].<ref>{{cite news|author=Simon Singh| title=Flipping pancakes with mathematics|newspaper=[[The Guardian]]| date=November 14, 2013| url=https://www.theguardian.com/science/blog/2013/nov/14/flipping-pancakes-mathematics-jacob-goodman| accessdate=March 25, 2014}}</ref> It is a variation of the [[sorting algorithm|sorting]] problem in which the only allowed operation is to reverse the elements of some ''[[prefix (computer science)|prefix]]'' of the sequence. Unlike a traditional sorting algorithm, which attempts to sort with the fewest comparisons possible, the goal is to sort the sequence in as few reversals as possible. A variant of the problem is concerned with ''burnt'' pancakes, where each pancake has a burnt side and all pancakes must, in addition, end up with the burnt side on bottom.\n\n==The pancake problems==\n\n===The original pancake problem===\nThe minimum number of flips required to sort any stack of {{math|''n''}} pancakes has been shown to lie between {{math|{{sfrac|15|14}}''n''}} and {{math|{{sfrac|18|11}}''n''}} (approximately 1.07''n'' and 1.64''n'',) but the exact value is not known.<ref>{{Cite book|last1=Fertin|first1=G.|last2=Labarre|first2=A.|last3=Rusu|first3=I.|last4=Tannier|first4=E.|last5=Vialette|first5=S. |title=Combinatorics of Genome Rearrangements|publisher= The MIT Press|year= 2009|isbn=9780262062824}}</ref>\n\nThe most simple pancake sorting algorithm must have at most {{math|2''n''{{thinsp}}&minus;{{thinsp}}3}} flips. In this algorithm, a number of [[selection sort]], we bring the largest pancake not yet sorted to the top with one flip; take it down to its final position with one more flip; and repeat this process for the remaining pancakes.\n\nIn 1979, [[Bill Gates]] and [[Christos Papadimitriou]]<ref name=Gates1979/> gave an upper bound of {{math|{{sfrac|(5''n''+5)|3}}}}. This was improved, thirty years later, to  {{math|{{sfrac|18|11}}''n''}} by a team of researchers at the [[University of Texas at Dallas]], led by Founders Professor [[Hal Sudborough]].<ref name=\"Sudborough\">{{cite web|title=Team Bests Young Bill Gates With Improved Answer to So-Called Pancake Problem in Mathematics|publisher=University of Texas at Dallas News Center|date=September 17, 2008|url=http://www.utdallas.edu/news/2008/09/17-002.php?WT.mc_id=NewsEmails&WT.mc_ev=EmailOpen|accessdate=November 10, 2008|quote=A team of UT Dallas computer science students and their faculty adviser have improved upon a longstanding solution to a mathematical conundrum known as the pancake problem. The previous best solution, which stood for almost 30 years, was devised by Bill Gates and one of his Harvard instructors, Christos Papadimitriou, several years before Microsoft was established.}}</ref><ref>{{Cite journal|last=Chitturi|first=B.|last2=Fahle|first2=W.|last3=Meng|first3=Z.|last4=Morales|first4=L.|last5=Shields|first5=C. O.|last6=Sudborough|first6=I. H.|last7=Voit|first7=W.|date=2009-08-31|title=An (18/11)n upper bound for sorting by prefix reversals|url=http://www.sciencedirect.com/science/article/pii/S0304397508003575|journal=Theoretical Computer Science|series=Graphs, Games and Computation: Dedicated to Professor Burkhard Monien on the Occasion of his 65th Birthday|volume=410|issue=36|pages=3372–3390|doi=10.1016/j.tcs.2008.04.045}}</ref>\n\nIn 2011, Laurent Bulteau, Guillaume Fertin, and Irena Rusu<ref>{{Cite journal|journal=Journal of Computer and System Sciences|doi=10.1016/j.jcss.2015.02.003|title=Pancake Flipping Is Hard|last1=Bulteau|first1=Laurent|last2=Fertin|first2=Guillaume|last3=Rusu|first3=Irena|volume=81|number=8|pages=1556–1574|arxiv=1111.0434|year=2015}}</ref> proved that the problem of finding the shortest sequence of flips for a given stack of pancakes is [[NP-hard]], thereby answering a question that had been open for over three decades.\n\n===The burnt pancake problem===\nIn a variation called the '''burnt pancake problem''', the bottom of each pancake in the pile is burnt, and the sort must be completed with the burnt side of every pancake down. It is a ''signed permutation'', and if a pancake ''i'' is \"burnt side up\" a negative element ''i`'' is put in place of ''i'' in the permutation. In 2008, a group of undergraduates built a [[DNA computing|bacterial computer]] that can solve a simple example of the burnt pancake problem by programming ''[[Escherichia coli|E. coli]]'' to flip segments of DNA which are analogous to burnt pancakes. DNA has an orientation (5' and 3') and an order (promoter before coding). Even though the processing power expressed by DNA flips is low, the high number of bacteria in a culture provides a large parallel computing platform. The bacteria report when they have solved the problem by becoming antibiotic resistant.<ref>{{Cite journal|doi=10.1186/1754-1611-2-8|title=Engineering bacteria to solve the Burnt Pancake Problem|year=2008|last1=Haynes|first1=Karmella A|last2=Broderick|first2=Marian L|last3=Brown|first3=Adam D|last4=Butner|first4=Trevor L|last5=Dickson|first5=James O|last6=Harden|first6=W Lance|last7=Heard|first7=Lane H|last8=Jessen|first8=Eric L|last9=Malloy|first9=Kelly J|journal=Journal of Biological Engineering|volume=2|pages=8|pmid=18492232|pmc=2427008|last10=Ogden|first10=Brad J|last11=Rosemond|first11=Sabriya|last12=Simpson|first12=Samantha|last13=Zwack|first13=Erin|last14=Campbell|first14=A Malcolm|last15=Eckdahl|first15=Todd T|last16=Heyer|first16=Laurie J|last17=Poet|first17=Jeffrey L}}</ref>\n\n===The identical pancakes stack problem===\nThis is inspired from the way Indian bread ([[Roti]] or [[chapati]]) is cooked. Initially, all rotis are stacked in one column, and the cook uses a spatula to flip the rotis so that each side of each roti touches the base fire at some point to toast. Several variants are possible: the rotis can be considered as single-sided or two-sided, and it may be forbidden or not to toast the same side twice. This version of the problem was first explored by Arka Roychowdhury.<ref>{{cite web|title=Itunes: Flipping Pancakes|first=Arka|last=Roychowdhury|url=https://arkaroychowdhury1.wordpress.com/portfolio/flippingpancakes/|website=Crazy1S|date=2015-03-18}}</ref>\n\n==The pancake problem on strings==\nThe discussion above presumes that each pancake is unique, that is, the sequence on which the prefix reversals are performed is a ''[[permutation]]''. However, \"strings\" are sequences in which a symbol can repeat, and this repetition may reduce the number of prefix reversals required to sort. Chitturi and Sudborough (2010) and Hurkens et al. (2007) independently showed that the complexity of transforming a compatible string into another with the minimum number of prefix reversals is [[NP-complete]]. They also gave bounds for the same. Hurkens et al. gave an exact algorithm to sort binary and ternary strings. Chitturi<ref name=\":0\" />  (2011) proved that the complexity of transforming a compatible signed string into another with the minimum number of signed prefix reversals—the burnt pancake problem on strings—is NP-complete.\n\n==History==\nThe pancake sorting problem was first posed by [[Jacob E. Goodman]], writing under the pseudonym \"Harry Dweighter\" (\"harried waiter\").<ref>{{citation\n|first1=Harry|last1=Dweighter\n|title= Elementary Problem E2569\n|journal = Amer. Math. Monthly\n|volume= 82\n|issue=10\n|pages=1009–1010\n|year=1975\n|doi=10.2307/2318260|jstor=2318260\n}}\n</ref>\n\nAlthough seen more often as an educational device, pancake sorting also appears in applications in parallel processor networks, in which it can provide an effective routing algorithm between processors.<ref>{{Cite journal| last1 = Gargano | first1 = L.\n | last2 = Vaccaro | first2 = U.\n | last3 = Vozella | first3 = A.\n | doi = 10.1016/0020-0190(93)90043-9\n | issue = 6\n | journal = Information Processing Letters\n | mr = 1216942\n | pages = 315–320\n | title = Fault tolerant routing in the star and pancake interconnection networks\n | volume = 45\n | year = 1993| postscript = <!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->{{inconsistent citations}} | citeseerx = 10.1.1.35.9056\n }}.</ref><ref>{{Cite book| last1 = Kaneko | first1 = K.\n | last2 = Peng | first2 = S.\n | contribution = Disjoint paths routing in pancake graphs\n | doi = 10.1109/PDCAT.2006.56\n | pages = 254–259\n | title = Proceedings of Seventh International Conference on Parallel and Distributed Computing, Applications and Technologies, 2006 (PDCAT '06)\n | year = 2006| isbn = 978-0-7695-2736-9 | postscript = <!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->{{inconsistent citations}} }}.</ref>\n\nThe problem is notable as the topic of the only well-known mathematics paper by [[Microsoft]] founder [[Bill Gates]] (as William Gates), entitled \"Bounds for Sorting by Prefix Reversal\". Published in 1979, it describes an efficient algorithm for pancake sorting.<ref name=Gates1979>{{cite journal |url=http://www.cs.berkeley.edu/~christos/papers/Bounds%20For%20Sorting%20By%20Prefix%20Reversal.pdf |title=Bounds for Sorting by Prefix Reversal |journal=[[Discrete Mathematics (journal)|Discrete Mathematics]] |volume=27 |pages=47–57 |year=1979 |doi=10.1016/0012-365X(79)90068-2 |last=Gates |first=W. |last2=Papadimitriou |first2=C. |authorlink1 = Bill Gates |authorlink2 = Christos Papadimitriou}}</ref> In addition, the most notable paper published by ''[[Futurama]]'' co-creator [[David X. Cohen]] (as David S. Cohen) concerned the burnt pancake problem.<ref>{{Cite journal|last1=Cohen|first1=D.S. |last2= Blum|first2=M.|doi=10.1016/0166-218X(94)00009-3|title=On the problem of sorting burnt pancakes|year=1995|journal=Discrete Applied Mathematics|volume=61|issue=2|pages=105|authorlink1=David X. Cohen|authorlink2=Manuel Blum}}</ref> Their collaborators were [[Christos Papadimitriou]] (then at [[Harvard University|Harvard]], now at [[Columbia University|Columbia]]) and [[Manuel Blum]] (then at [[University of California, Berkeley|Berkeley]], now at [[Carnegie Mellon University]]), respectively.\n\nThe connected problems of signed sorting by reversals and sorting by reversals were also studied more recently. Whereas efficient exact algorithms have been found for the signed sorting by reversals,<ref>{{cite journal|last1=Kaplan|first1=H.|last2=Shamir|first2=R.|author3-link=Robert Tarjan|last3=Tarjan|first3=R.E.|title=Faster and Simpler Algorithm for Sorting Signed Permutations by Reversals|journal=''Proc.'' 8th ''ACM-SIAM SODA''|date=1997|pages=178–87}}</ref> the problem of sorting by reversals has been proven to be hard even to approximate to within certain constant factor,<ref>{{cite journal|last1=Berman|first1=P.|author2-link=Marek Karpinski|last2=Karpinski|first2=M.|url=http://theory.cs.uni-bonn.de/ftp/reports/cs-reports/1998/85193-CS.ps.gz|title=On Some Tighter Inapproximability Results.|journal=''Proc.'' 26th ''ICALP'' (1999) LNCS 1644|year=1999|pages=200–09}}</ref> and also proven to be approximable in polynomial time to within the approximation factor 1.375.<ref>{{cite journal|last=Berman|first=P.|author2-link=Marek Karpinski|last2=Karpinski|first2=M.|last3=Hannenhalli|first3=S.|url=http://theory.cs.uni-bonn.de/ftp/reports/cs-reports/2001/85228-CS.ps.gz|title=1.375-Approximation Algorithms for Sorting by Reversals.|journal=''Proc.'' 10th ''ESA'' (2002), ''LNCS'' 2461|pages=200–10|year=2002}}</ref>\n\n==Pancake graphs==\n[[File:Pancake graph g3.svg|thumb|The pancake graph P<sub>3</sub>]]\n[[File:Pancake graph g4.svg|thumb|The pancake graph P<sub>4</sub> can be constructed recursively from 4 copies of P<sub>3</sub> by assigning a different element from the set {1, 2, 3, 4} as a suffix to each copy.]]\n{{main|Pancake graph}}\nAn '''''n''-pancake graph''' is a graph whose vertices are the permutations of ''n'' symbols from 1 to ''n'' and its edges are given between permutations transitive by prefix reversals. It is a [[regular graph]] with n! vertices, its degree is n&minus;1. The pancake sorting problem and the problem to obtain the [[Distance (graph theory)|diameter]] of the pancake graph is equivalent.<ref name=\"pancake17\">{{Cite book|last=Asai|first=Shogo|last2=Kounoike|first2=Yuusuke|last3=Shinano|first3=Yuji|last4=Kaneko|first4=Keiichi|date=2006-09-01|title=Computing the Diameter of 17-Pancake Graph Using a PC Cluster.|journal=Euro-Par 2006 Parallel Processing: 12th International Euro-Par Conference|volume=4128|pages=1114–1124|doi=10.1007/11823285_117|series=Lecture Notes in Computer Science|isbn=978-3-540-37783-2}}</ref>\n\nThe pancake graph of dimension ''n'', P<sub>n</sub> can be constructed recursively from n copies of P<sub>n&minus;1</sub>, by assigning a different element from the set {1, 2, …, n} as a suffix to each copy.\n\nTheir [[Girth (graph theory)|girth]]:\n\n<math>g(P_n) = 6 \\text{, if } n>2</math>.\n\nThe &gamma;(P<sub>n</sub>) [[graph embedding|genus]] of P<sub>n</sub> is:<ref name=\"ongenus\">{{Cite journal|last=Nguyen|first=Quan|last2=Bettayeb|first2=Said|date=2009-11-05|title=On the Genus of Pancake Network.|url=http://ccis2k.org/iajit/PDF/vol.8,no.3/1247.pdf |journal=The International Arab Journal of Information Technology |volume=8|issue=3|pages=289–292}}</ref>\n\n<math>n!\\left( \\frac{n-4}{6} \\right)+1 \\le \\gamma(P_n) \\le n!\\left( \\frac{n-3}{4} \\right)-\\frac{n}{2}+1 </math>\n\nSince pancake graphs have many interesting properties such as symmetric and recursive structures, small degrees and diameters compared against the size of the graph, much attention is paid to them as a model of interconnection networks for parallel computers.<ref>{{Cite journal|last=Akl|first=S.G.|last2=Qiu|first2=K.|last3=Stojmenović|first3=I.|date=1993|title=Fundamental algorithms for the star and pancake interconnection networks with applications to computational geometry.|journal=Networks|volume=23|issue=4|pages=215–225|doi=10.1002/net.3230230403|citeseerx=10.1.1.363.4949}}</ref><ref>{{Cite journal|last=Bass|first=D.W.|last2=Sudborough|first2=I.H.|date=March 2003|title=Pancake problems with restricted prefix reversals and some corresponding Cayley networks.|journal=Journal of Parallel and Distributed Computing |volume=63|issue=3|pages=327–336|doi=10.1016/S0743-7315(03)00033-9|citeseerx=10.1.1.27.7009}}</ref><ref>{{Cite journal|last=Berthomé|first=P.|last2=Ferreira|first2=A.|last3=Perennes|first3=S.|date=December 1996|title=Optimal information dissemination in star and pancake networks.|journal=IEEE Transactions on Parallel and Distributed Systems|volume=7|issue=12|pages=1292–1300|doi=10.1109/71.553290|citeseerx=10.1.1.44.6681}}</ref> When we regard the pancake graphs as the model of the interconnection networks, the diameter of the graph is a measure that represents the delay of communication.<ref>{{cite book|last1=Kumar|first1=V.|last2=Grama|first2=A.|last3=Gupta|first3=A.|last4=Karypis|first4=G.|title=Introduction to Parallel Computing: Design and Analysis of Algorithms|publisher=Benjamin/Cummings|date=1994}}</ref><ref>{{cite book|last=Quinn|first=M.J.|title=Parallel Computing: Theory and Practice|edition=second|publisher=McGraw-Hill|date=1994}}</ref>\n\nThe pancake graphs are [[Cayley graph]]s (thus are [[Vertex-transitive graph|vertex-transitive]]) and are especially attractive for parallel processing. They have sublogarithmic degree and diameter, and are relatively [[Dense graph|sparse]] (compared to e.g. [[Hypercube graph|hypercubes]]).<ref name=\"ongenus\"/>\n\n==Related integer sequences==\n:{| class=\"wikitable\" align=\"center\" style=\"float: center; text-align: right;\"\n|+ Number of stacks of given height {{math|''n''}} that require unique flips {{math|''k''}}&nbsp; to get sorted\n! rowspan=2 | Height<br />{{math|''n''}} !! colspan=16 | {{math|''k''}}\n|-\n! width=\"65px\" |  0 !! width=\"65px\" |  1 !! width=\"65px\" |  2 !! width=\"65px\" |  3 !! width=\"65px\" |  4 !! width=\"65px\" |  5 !! width=\"65px\" |  6 !! width=\"65px\" | 7 \n! width=\"65px\" |  8 !! width=\"65px\" |  9 !! width=\"65px\" | 10 !! width=\"65px\" | 11 !! width=\"65px\" | 12 !! width=\"65px\" | 13 !! width=\"65px\" |  14 !! width=\"65px\"|15\n|-\n! 1\n| 1 || || || || || || || || || || || || || || ||\n|-\n! 2\n| 1 || 1 || || || || || || || || || || || || || ||\n|-\n! 3\n| 1 || 2 || 2 || 1 || || || || || || || || || || || ||\n|-\n! 4\n| 1 || 3 || 6 || 11 || 3 || || || || || || || || || || ||\n|-\n! 5\n| 1 || 4 || 12 || 35 || 48 || 20 || || || || || || || || || ||\n|-\n! 6\n| 1 || 5 || 20 || 79 || 199 || 281 || 133 || 2 || || || || || || || ||\n|-\n! 7\n| 1 || 6 || 30 || 149 || 543 || 1357 || 1903 || 1016 || 35 || || || || || || ||\n|-\n! 8\n| 1 || 7 || 42 || 251 || 1191 || 4281 || 10561 || 15011 || 8520 || 455 || || || || || ||\n|-\n! 9\n| 1 || 8 || 56 || 391 || 2278 || 10666 || 38015 || 93585 || 132697 || 79379 || 5804 || || || || ||\n|-\n! 10\n| 1 || 9 || 72 || 575 || 3963 || 22825 || 106461 || 377863 || 919365 || 1309756 || 814678 || 73232 || || || ||\n|-\n! 11\n| 1 || 10 || 90 || 809 || 6429 || 43891 || 252737 || 1174766 || 4126515 || 9981073 || 14250471 || 9123648 || 956354 || 6 || ||\n|-\n! 12\n| 1 || 11 || 110 || 1099 || 9883 || 77937 || 533397 || 3064788 || 14141929 || 49337252 || 118420043 || 169332213 || 111050066 || 13032704 || 167 ||\n|-\n! 13\n| 1 || 12 || 132 || 1451 || 14556 || 130096 || 1030505 || 7046318 || 40309555 || 184992275 || 639783475 || 1525125357 || 2183056566 || 1458653648 || 186874852 || 2001\n|}\n\nSequences from [[On-Line Encyclopedia of Integer Sequences|The Online Encyclopedia of Integer Sequences]] of [[Neil Sloane]]:\n* {{OEIS2C|A058986}} – maximum number of flips\n* {{OEIS2C|A067607}} – number of stacks requiring the maximum number of flips (above)\n* {{OEIS2C|A078941}} – maximum number of flips for a \"burnt\" stack\n* {{OEIS2C|A078942}} – the number of flips for a sorted \"burnt-side-on-top\" stack\n* {{OEIS2C|A092113}} – the above triangle, read by rows\n\n<ref name=\":0\">{{cite journal|title=A NOTE ON COMPLEXITY OF GENETIC MUTATIONS|journal=Discrete Mathematics, Algorithms and Applications|volume= 03| issue = 3|pages=269–286|doi=10.1142/S1793830911001206|year=2011|last1=Chitturi|first1=Bhadrachalam}}</ref>\n\n==References==\n{{Reflist}}\n\n==Further reading==\n{{refbegin}}\n* {{cite journal|last1=Chitturi|first1=B.|last2=Sudborough|first2=H.|title=Prefix Reversals on Strings|journal=Proceedings of the International Conference on Bioinformatics & Computational Biology|volume=2|date=2010|pages=591–598|url=https://www.researchgate.net/publication/221051667}}\n* {{cite journal|last1=Chitturi|first1=B.| title=A NOTE ON COMPLEXITY OF GENETIC MUTATIONS| journal=Discrete Math. Algorithm. Appl.|volume=3|issue=3|date=2011| pages= 269–287| doi=10.1142/S1793830911001206}}\n* {{cite journal|last1=Heydari|first1=M. H.|last2=Sudborough|first2=I. H.|title=On the Diameter of the Pancake Network|journal=Journal of Algorithms|date=1997|volume=25|issue=1|pages=67–94|doi=10.1006/jagm.1997.0874}}\n*{{cite journal|last1=Hurkens|first1=C.|last2=van Iersel|first2=L.|last3=Keijsper|first3=J.|last4=Kelk|first4=S.|last5=Stougie|first5=L.|last6=Tromp|first6=J.|title=Prefix Reversals on Binary and Ternary Strings|journal=SIAM Journal on Discrete Mathematics|date=2007|volume=21|issue=3|pages=592–611|doi=10.1137/060664252|arxiv=math/0602456}}\n* {{cite journal|last1=Roney-Dougal|first1=C. |last2=Vatter|first2=V.|url=http://plus.maths.org/issue54/features/colvatter/ |title=Of Pancakes, Mice and Men|journal=Plus Magazine |volume=54 |date=March 2010}}\n{{refend}}\n\n==External links==\n* [http://www.cut-the-knot.org/SimpleGames/Flipper.shtml Cut-the-Knot: Flipping pancakes puzzle], including a Java applet for the pancake problem and some discussion.\n* [http://www.math.uiuc.edu/~west/openp/pancake.html Douglas B. West's \"The Pancake Problems\"]\n* {{MathWorld |urlname=PancakeSorting |title=Pancake Sorting}}\n* [https://www.webcitation.org/66htQBNaj?url=http://www.bio.davidson.edu/people/kahaynes/FAMU_talk/Living_computer.swf Animation explaining the bacterial computer that can solve the burnt pancake problem.]\n* [https://web.archive.org/web/20140607001227/http://arkaroychowdhury1.wordpress.com/2014/06/02/tower1/ \"Tower1/Pancake Flip\" by Arka. A game based on pancake problem principle]\n{{sorting}}\n\n{{DEFAULTSORT:Pancake Sorting}}\n[[Category:Sorting algorithms]]"
    },
    {
      "title": "Partial sorting",
      "url": "https://en.wikipedia.org/wiki/Partial_sorting",
      "text": "In [[computer science]], '''partial sorting''' is a [[Relaxation (approximation)|relaxed]] variant of the [[Sorting algorithm|sorting]] problem. Total sorting is the problem of returning a list of items such that its elements all appear in order, while partial sorting is returning a list of the ''k'' smallest (or ''k'' largest) elements in order. The other elements (above the ''k'' smallest ones) may also be sorted, as in an in-place partial sort, or may be discarded, which is common in streaming partial sorts. A common practical example of partial sorting is computing the \"Top 100\" of some list.\n\nIn terms of indices, in a partially sorted list, for every index ''i'' from 1 to ''k,'' the ''i''-th element is in the same place as it would be in the fully sorted list: element ''i'' of the partially sorted list contains [[order statistic]] ''i'' of the input list.\n\n==Offline problem==\n===Heap-based solution===\n[[Heap (data structure)|Heaps]] admit a simple single-pass partial sort when {{mvar|k}} is fixed: insert the first {{mvar|k}} elements of the input into a max-heap. Then make one pass over the remaining elements, add each to the heap in turn, and remove the largest element. Each insertion operation takes {{math|''O''(log ''k'')}} time, resulting in {{math|''O''(''n'' log ''k'')}} time overall; this algorithm is practical for small values of {{mvar|k}} and in [[online algorithm|online]] settings.<ref name=\"aofa04slides\"/> Another options is to build a min-heap for all the values (the build takes {{math|''O''(n)}}) and take out the head of the heap K times, each remove operation takes {{math|''O''(log ''n'')}}. In that case the algorithm takes {{math|''O''(n+klog ''n'')}}.\n\n===Solution by partitioning selection===\nA further relaxation requiring only a list of the {{mvar|k}} smallest elements, but without requiring that these be ordered, makes the problem equivalent to [[Selection algorithm#Partition-based selection|partition-based selection]]; the original partial sorting problem can be solved by such a selection algorithm to obtain an array where the first {{mvar|k}} elements are the {{mvar|k}} smallest, and sorting these, at a total cost of {{math|''O''(''n'' + ''k'' log ''k'')}} operations. A popular choice to implement this algorithm scheme is to combine [[quickselect]] and [[quicksort]]; the result is sometimes called \"quickselsort\".<ref name=\"aofa04slides\"/>\n\n==={{anchor|Partial quicksort}} Specialised sorting algorithms===\nMore efficient than the aforementioned are specialized partial sorting algorithms based on [[mergesort]] and [[quicksort]]. In the quicksort variant, there is no need to recursively sort partitions which only contain elements that would fall after the {{mvar|k}}'th place in the final sorted array (starting from the \"left\" boundary). Thus, if the pivot falls in position {{mvar|k}} or later, we recurse only on the left partition:<ref>{{cite conference |last=Martínez |first=Conrado |title=Partial quicksort |conference=Proc. 6th ACM-SIAM Workshop on Algorithm Engineering and Experiments and 1st ACM-SIAM Workshop on Analytic Algorithmics and Combinatorics |year=2004 |url=http://www.lsi.upc.edu/~conrado/research/reports/ALCOMFT-TR-03-50.pdf}}</ref>\n\n  '''function''' partial_quicksort(A, i, j, k)\n      '''if''' i < j\n          p ← pivot(A, i, j)\n          p ← partition(A, i, j, p)\n          partial_quicksort(A, i, p-1, k)\n          '''if''' p < k-1\n              partial_quicksort(A, p+1, j, k)\n\nThe resulting algorithm is called partial quicksort and requires an ''expected'' time of only {{math|''O''(''n'' + ''k'' log ''k'')}}, and is quite efficient in practice, especially if a [[selection sort]] is used as a base case when {{mvar|k}} becomes small relative to {{mvar|n}}. However, the worst-case time complexity is still very bad, in the case of a bad pivot selection. Pivot selection along the lines of the worst-case linear time selection algorithm could be used to get better worst-case performance.\n\n==Incremental sorting==\nIncremental sorting is an \"online\" version of the partial sorting problem, where the input is given up front but {{mvar|k}} is unknown: given a {{mvar|k}}-sorted array, it should be possible to extend the partially sorted part so that the array becomes {{math|(''k''+1)}}-sorted.{{r|paredes}}\n\n[[Heap (data structure)|Heaps]] lead to an {{math|''O''(''n'' + ''k'' log ''n'')}} solution to online partial sorting: first \"heapify\", in linear time, the complete input array to produce a min-heap. Then extract the minimum of the heap {{mvar|k}} times.<ref name=\"aofa04slides\">{{cite conference |author=Conrado Martínez |year=2004 |title=On partial sorting |url=http://www.lsi.upc.edu/~conrado/research/talks/aofa04.pdf |conference=10th Seminar on the Analysis of Algorithms}}</ref>\n\nAn [[Asymptotic analysis|asymptotically]] faster incremental sort can be obtained by modifying quickselect. The version due to Paredes and Navarro maintains a [[stack (data structure)|stack]] of pivots across calls, so that incremental sorting can be accomplished by repeatedly requesting the smallest item of an array {{mvar|A}} from the following algorithm:<ref name=\"paredes\">{{Cite conference| doi = 10.1137/1.9781611972863.16| chapter = Optimal Incremental Sorting| title = Proc. Eighth Workshop on Algorithm Engineering and Experiments (ALENEX)| pages = 171–182| year = 2006| last1 = Paredes | first1 = Rodrigo| last2 = Navarro | first2 = Gonzalo| isbn = 978-1-61197-286-3| citeseerx = 10.1.1.218.4119}}</ref>\n\n<div style=\"margin-left: 35px; width: 600px\">\n{{framebox|blue}}\nAlgorithm {{math|IQS(''A'' : array, ''i'' : integer, ''S'' : stack)}} returns the {{mvar|i}}'th smallest element in {{mvar|A}}\n* If {{math|''i'' {{=}} top(''S'')}}:\n** Pop {{mvar|S}}\n** Return {{math|''A''[''i'']}}\n* Let {{math|pivot ← random [''i'', top(''S''))}}\n* Update {{math|pivot ← partition(''A''[''i'' : top(''S'')), ''A''[pivot])}}\n* Push {{math|pivot}} onto {{mvar|S}}\n* Return {{math|IQS(''A'', ''i'', ''S'')}}\n{{frame-footer}}\n</div>\n\nThe stack {{mvar|S}} is initialized to contain only the length {{mvar|n}} of {{mvar|A}}. {{mvar|k}}-sorting the array is done by calling {{math|IQS(''A'', ''i'', ''S'')}} for {{math|''i'' {{=}} 0, 1, 2, ...}}; this sequence of calls has [[average-case complexity]] {{math|''O''(''n'' + ''k'' log ''k'')}}. The worst-case time is quadratic, but this can be fixed by replacing the random pivot selection by the [[median of medians]] algorithm.{{r|paredes}}\n\n== Language/library support ==\n* The [[C++]] standard specifies a library function called <code>[http://en.cppreference.com/w/cpp/algorithm/partial_sort std::partial_sort]</code>.\n* The [[Python (programming language)|Python]] standard library includes functions <code>[https://docs.python.org/library/heapq.html#heapq.nlargest nlargest]</code> and <code>[https://docs.python.org/library/heapq.html#heapq.nsmallest nsmallest]</code> in its <code>heapq</code> module.\n* The [[Julia_(programming_language)|Julia]] standard library includes a <code>[https://docs.julialang.org/en/stable/stdlib/sort/#Sorting-Algorithms-1 PartialQuickSort]</code> implementation.\n\n== See also ==\n* [[Selection algorithm]]\n\n==References==\n{{reflist}}\n\n== External links ==\n* J.M. Chambers (1971). [http://dl.acm.org/citation.cfm?id=362602 Partial sorting]. [[Communications of the ACM|CACM]] '''14'''(5):357–358.\n\n[[Category:Sorting algorithms]]\n[[Category:Online sorts]]\n[[Category:Articles with example pseudocode]]"
    },
    {
      "title": "Pigeonhole sort",
      "url": "https://en.wikipedia.org/wiki/Pigeonhole_sort",
      "text": "{{refimprove|date=July 2017}}\n{{Infobox Algorithm\n|class=[[Sorting algorithm]]\n|image=\n|data=[[Array data structure|Array]]\n|time=<math>O(N+n)</math>, where ''N'' is the range of key values and ''n'' is the input size \n|space=<math>O(N+n)</math>\n|optimal=If and only if <math>N=O(n)</math>\n}}\n'''Pigeonhole sorting''' is a [[sorting algorithm]] that is suitable for sorting lists of elements where the number of elements (''n'') and the length of the range of possible key values (''N'') are approximately the same.<ref>[https://xlinux.nist.gov/dads/HTML/pigeonholeSort.html NIST's Dictionary of Algorithms and Data Structures: pigeonhole sort]</ref> It requires [[big O notation|O]](''n'' + ''N'') time.  It is similar to [[counting sort]], but differs in that it \"moves items twice: once to the bucket array and again to the final destination [whereas] counting sort builds an auxiliary array then uses the array to compute each item's final destination and move the item there.\"<ref>{{cite web|last1=Black|first1=Paul E.|title=Dictionary of Algorithms and Data Structures|url=https://xlinux.nist.gov/dads/HTML/pigeonholeSort.html|website=NIST|accessdate=6 November 2015}}</ref>\n\nThe pigeonhole algorithm works as follows:\n#  Given an array of values to be sorted, set up an auxiliary array of initially empty \"pigeonholes,\" one pigeonhole for each key through the [[Range_(computer_science)|range]] of the original array.\n#  Going over the original array, put each value into the pigeonhole corresponding to its key, such that each pigeonhole eventually contains a list of all values with that key.\n#  Iterate over the pigeonhole array in order, and put elements from non-empty pigeonholes back into the original array.\n\n==Example==\nSuppose one were sorting these value pairs by their first element:\n\n* (5, \"hello\")\n* (3, \"pie\")\n* (8, \"apple\")\n* (5, \"king\")\n\nFor each value between 3 and 8 we set up a pigeonhole, then move each element to its pigeonhole:\n\n* 3: (3, \"pie\")\n* 4:\n* 5: (5, \"hello\"), (5, \"king\")\n* 6:\n* 7:\n* 8: (8, \"apple\")\n\nThe pigeonhole array is then iterated over in order, and the elements are moved back to the original list.\n\nThe difference between pigeonhole sort and counting sort is that in counting sort, the auxiliary array does not contain lists of input elements, only counts:\n\n* 3: 1\n* 4: 0\n* 5: 2\n* 6: 0\n* 7: 0\n* 8: 1\n\nUsing this information, one could perform a series of exchanges on the input array that would put it in order, moving items only once.\n\nFor arrays where ''N'' is much larger than ''n'', [[bucket sort]] is a generalization that is more efficient in space and time.\n\n== Python implementation ==\n<source lang=\"python\">\ndef pigeonhole_sort(a):\n\tmi = min(a)\n\tsize = max(a) - mi + 1\n\tholes = [0] * size\n\tfor x in a:\n\t\tholes[x - mi] += 1\n\ti = 0\n\tfor count in xrange(size):\n\t\twhile holes[count] > 0:\n\t\t\tholes[count] -= 1\n\t\t\ta[i] = count + mi\n\t\t\ti += 1\n</source>\n\n== See also ==\n* [[Pigeonhole principle]]\n* [[Radix sort]]\n* [[Bucket queue]], a related priority queue data structure\n\n== References ==\n\n<references />\n\n{{Wikibooks|Algorithm implementation|Sorting/Pigeonhole sort|Pigeonhole sort}}\n{{sorting}}\n\n[[Category:Sorting algorithms]]\n[[Category:Stable sorts]]\n\n[[ru:Сортировка подсчётом#Алгоритм со списком]]"
    },
    {
      "title": "Polyphase merge sort",
      "url": "https://en.wikipedia.org/wiki/Polyphase_merge_sort",
      "text": "A polyphase merge sort is a variation of bottom up [[Merge sort]] that sorts a list using an initial uneven distribution of sub-lists (runs), primarily used for [[external sorting]], and is more efficient than an ordinary merge sort when there are less than 8 external working files (such as a tape drive or a file on a hard drive). A polyphase merge sort is not a [[Sorting algorithm#Stability|stable sort]].\n\n==Ordinary merge sort==\n\nA [[merge sort]] splits the records of a dataset into sorted runs of records and then repeatedly merges sorted runs into larger sorted runs until only one run, the sorted dataset, remains.\n\nAn ordinary merge sort using four working files organizes them as a pair of input files and a pair of output files. The dataset is distributed evenly between two of the working files, either as sorted runs or in the simplest case, single records, which can be considered to be sorted runs of size 1. Once all of the dataset is transferred to the two working files, those two working files become the input files for the first merge iteration. Each merge iteration merges runs from the two input working files, alternating the merged output between the two output files, again distributing the merged runs evenly between the two output files (until the final merge iteration). Once all of the runs from the two inputs files are merged and output, then the output files become the input files and vice versa for the next merge iteration. The number of runs decreases by a factor of 2 at each iteration, such as 64, 32, 16, 8, 4, 2, 1. For the final merge iteration, the two input files only have one sorted run (1/2 of the dataset) each, and the merged result is a single sorted run (the sorted dataset) on one of the output files. This is also described at [[Merge sort#Use with tape drives]] .\n\nIf there are only three working files, then an ordinary merge sort merges sorted runs from two working files onto a single working file, then distributes the runs evenly between the two output files. The merge iteration reduces run count by a factor of 2, the redistribute iteration doesn't reduce run count (the factor is 1). Each iteration could be considered to reduce the run count by an average factor of the square root of 2 ~= 1.41. If there are 5 working files, then the pattern alternates between a 3 way merge and a 2 way merge, for an average factor of square root of 6 ~= 2.45.\n\nIn general, for an even number N working files, each iteration of an ordinary merge sort reduces run count by a factor of N/2, or for an odd number O working files, each iteration reduces the run count by an average factor of square_root((O<sup>2</sup>-1)/4).\n\n==Polyphase merge==\n\nFor N less than 8 working files, a polyphase merge sort achieves a higher effective run count reduction factor by unevenly distributing sorted runs between N-1 working files (explained in next section). Each iteration merges runs from N-1 working files onto a single output working file. When the end of one of the N-1 working files is reached, then it becomes the new output file and what was the output file becomes one of the N-1 working input files, starting a new iteration of polyphase merge sort. Each iteration merges only a fraction of the dataset (about 1/2 to 3/4), except for the last iteration which merges all of the dataset into a single sorted run. The initial distribution is set up so that only one input working file is emptied at a time, except for the final merge iteration which merges N-1 single runs (of varying size, this is explained next) from the N-1 input working files to the single output file, resulting in a single sorted run, the sorted dataset.\n\nFor each polyphase iteration, the total number of runs follows a pattern similar to a reversed [[Fibonacci numbers of higher order]] sequence. With 4 files, and a dataset consisting of 57 runs, the total run count on each iteration would be 57, 31, 17, 9, 5, 3, 1.<ref name=\"Knuth1973\">[[Donald Knuth]], [[The Art of Computer Programming]], Volume 3, Addison Wesley, 1973, Algorithm 5.4.2D.</ref><ref>http://oopweb.com/Algorithms/Documents/Sman/Volume/ExternalSorting.html</ref> Note that except for the last iteration, the run count reduction factor is a bit less than 2, 57/31, 31/17, 17/9, 9/5, 5/3, 3/1, about 1.84 for a 4 file case, but each iteration except the last reduced the run count while processing about 65% of the dataset, so the run count reduction factor per dataset processed during the intermediate iterations is about 1.84 / 0.65 = 2.83. For a dataset consisting of 57 runs of 1 record each, then after the initial distribution, polyphase merge sort moves 232 records during the 6 iterations it takes to sort the dataset, for an overall reduction factor of 2.70 (this is explained in more detail later).\n\nAfter the first polyphase iteration, what was the output file now contains the results of merging N-1 original runs, but the remaining N-2 input working files still contain the remaining original runs, so the second merge iteration produces runs of size (N-1) + (N-2) = (2N - 3) original runs. The third iteration produces runs of size (4N - 7) original runs. With 4 files, the first iteration creates runs of size 3 original runs, the second iteration 5 original runs, the third iteration 9 original runs and so on, following the Fibonacci like pattern, 1, 3, 5, 9, 17, 31, 57, ... , so the increase in run size follows the same pattern as the decrease in run count in reverse. In the example case of 4 files and 57 runs of 1 record each, the last iteration merges 3 runs of size 31, 17, 9, resulting in a single sorted run of size 31+17+9 = 57 records, the sorted dataset. An example of the run counts and run sizes for 4 files, 31 records can be found in table 4.3 of.<ref>{{cite web |url=http://pluto.ksi.edu/~cyh/cis501/ch4.htm |title=Archived copy |accessdate=2016-01-22 |deadurl=yes |archiveurl=https://web.archive.org/web/20160128191110/http://pluto.ksi.edu/~cyh/cis501/ch4.htm |archivedate=2016-01-28 |df= }}</ref>\n\n==Perfect 3 file polyphase merge sort==\n\nIt is easiest to look at the polyphase merge starting from its ending conditions and working backwards. At the start of each iteration, there will be two input files and one output file. At the end of the iteration, one input file will have been completely consumed and will become the output file for the next iteration. The current output file will become an input file for the next iteration. The remaining files (just one in the 3 file case) have only been partially consumed and their remaining runs will be input for the next iteration.\n\nFile 1 just emptied and became the new output file. One run is left on each input tape, and merging those runs together will make the sorted file.\n\n<pre>\nFile 1 (out):                                           <1 run> *        (the sorted file)\nFile 2 (in ): ... | <1 run> *               -->     ... <1 run> | *          (consumed)\nFile 3 (in ):     | <1 run> *                           <1 run> | *          (consumed)\n\n...  possible runs that have already been read\n|    marks the read pointer of the file\n*    marks end of file\n</pre>\n\nStepping back to the previous iteration, we were reading from 1 and 2. One run is merged from 1 and 2 before file 1 goes empty.  Notice that file 2 is not completely consumed—it has one run left to match the final merge (above).\n\n<pre>\nFile 1 (in ): ... | <1 run> *                      ... <1 run> | *\nFile 2 (in ):     | <2 run> *           -->            <1 run> | <1 run> *\nFile 3 (out):                                          <1 run> *\n</pre>\n\nStepping back another iteration, 2 runs are merged from 1 and 3 before file 3 goes empty.\n\n<pre>\nFile 1 (in ):     | <3 run>                        ... <2 run> | <1 run> *\nFile 2 (out):                               -->        <2 run> *\nFile 3 (in ): ... | <2 run> *                          <2 run> | *\n</pre>\n\nStepping back another iteration, 3 runs are merged from 2 and 3 before file 2 goes empty.\n\n<pre>\nFile 1 (out):                                          <3 run> *\nFile 2 (in ): ... | <3 run> *               -->    ... <3 run> | *\nFile 3 (in ):     | <5 run> *                          <3 run> | <2 run> *\n</pre>\n\nStepping back another iteration, 5 runs are merged from 1 and 2 before file 1 goes empty.\n\n<pre>\nFile 1 (in ): ... | <5 run> *                      ... <5 run> | *\nFile 2 (in ):     | <8 run> *               -->        <5 run> | <3 run> *\nFile 3 (out):                                          <5 run> *\n</pre>\n\n==Distribution for polyphase merge sort==\n\nLooking at the perfect 3 file case, the number of runs for merged working backwards: 1, 1, 2, 3, 5, ... reveals a Fibonacci sequence. The sequence for more than 3 files is a bit more complicated; for 4 files, starting at the final state and working backwards, the run count pattern is {1,0,0,0}, {0,1,1,1}, {1,0,2,2}, {3,2,0,4}, {7,6,4,0}, {0,13,11,7}, {13,0,24,20}, ... .\n\nFor everything to work out optimally, the last merge phase should have exactly one run on each input file.  If any input file has more than one run, then another phase would be required. Consequently, the polyphase merge sort needs to be clever about the initial distribution of the input data's runs to the initial output files.  For example, an input file with 13 runs would write 5 runs to file 1 and 8 runs to file 2.\n\nIn practice, the input file will not have the exact number of runs needed for a perfect distribution. One way to deal with this is by padding the actual distribution with imaginary \"dummy runs\" to simulate an ideal run distribution.<ref>Knuth</ref> A dummy run behaves like a run with no records in it. Merging one or more dummy runs with one or more real runs just merges the real runs, and merging one or more dummy runs with no real runs results in a single dummy run. Another approach is to emulate dummy runs as needed during the merge operations<ref>https://www.fq.math.ca/Scanned/8-1/lynch.pdf</ref>. \n\n\"Optimal\" distribution algorithms require knowing the number of runs in advance. Otherwise, in the more common case where the number of runs is not known in advance, \"near optimal\" distribution algorithms are used. Some distribution algorithms include rearranging runs.<ref>http://i.stanford.edu/pub/cstr/reports/cs/tr/76/543/CS-TR-76-543.pdf</ref> If the number of runs is known in advance, only a partial distribution is needed before starting the merge phases. For example, consider the 3 file case, starting with n runs on File_1. Define fib(i) as the \"ith\" Fibonacci number, where fib(i) = fib(i-1) + fib(i-2). If n = fib(i), then move fib(i-2) runs to File_2, leaving fib(i-1) runs remaining on File_1, a perfect run distribution. If fib(i) < n < fib(i+1), move n-fib(i) runs to File_2 and fib(i+1)-n runs to File_3. The first merge iteration merges n-fib(i) runs from File_1 and File_2, appending the n-fib(i) merged runs to the fib(i+1)-n runs already moved to File_3. File_1 ends up with fib(i-2) runs remaining, File_2 is emptied, and File_3 ends up with fib(i-1) runs, again a perfect run distribution. For 4 or more files, the math is more complicated, but the concept is the same.\n\n==Comparison versus ordinary merge sort==\n\nAfter the initial distribution, an ordinary merge sort using 4 files will sort 16 single record runs in 4 iterations of the entire dataset, moving a total of 64 records in order to sort the dataset after the initial distribution. A polyphase merge sort using 4 files will sort 17 single record runs in 4 iterations, but since each iteration but the last iteration only moves a fraction of the dataset, it only moves a total of 48 records in order to sort the dataset after the initial distribution. In this case, ordinary merge sort factor is 2.0, while polyphase overall factor is ~2.73.\n\nTo explain how the reduction factor is related to sort performance, the reduction factor equations are:\n\n reduction_factor = exp(number_of_runs*log(number_of_runs)/run_move_count)\n run_move_count = number_of_runs * log(number_of_runs)/log(reduction_factor)\n run_move_count = number_of_runs * log_reduction_factor(number_of_runs)\n\nUsing the run move count equation for the above examples: \n* ordinary merge sort &rarr; {{tmath|1=16 \\times \\log_{2}(16) = 64}}, \n* polyphase merge sort &rarr; {{tmath|1=17 \\times \\log_{2.73}(17) = 48}}. \nHere is a table of effective reduction factors for polyphase and ordinary merge sort listed by number of files, based on actual sorts of a few million records. This table roughly corresponds to the reduction factor per dataset moved tables shown in fig 3 and fig 4 of [http://www.computer.org/csdl/proceedings/afips/1960/5057/00/50570143.pdf polyphase merge sort.pdf]\n\n<pre>\n# files\n|     average fraction of data per iteration\n|     |     polyphase reduction factor on ideal sized data\n|     |     |     ordinary reduction factor on ideal sized data\n|     |     |     |\n3     .73   1.94  1.41  (sqrt  2)\n4     .63   2.68  2.00\n5     .58   3.20  2.45  (sqrt  6)\n6     .56   3.56  3.00\n7     .55   3.80  3.46  (sqrt 12)\n8     .54   3.95  4.00\n9     .53   4.07  4.47  (sqrt 20)\n10    .53   4.15  5.00\n11    .53   4.22  5.48  (sqrt 30)\n12    .53   4.28  6.00\n32    .53   4.87 16.00\n</pre>\n\nIn general, polyphase merge sort is better than ordinary merge sort when there are less than 8 files, while ordinary merge sort starts to become better at around 8 or more files.<ref>http://bluehawk.monmouth.edu/rclayton/web-pages/s06-503/esort.html</ref><ref>http://www.mif.vu.lt/~algis/dsax/DsSort.pdf</ref>\n\n==References==\n\n{{Reflist}}\n\n==Further reading==\n*{{Citation |last=Bradley |first=James |year=1982 |title=File and Data Base Techniques |publisher=Holt, Rinehart and Winston |isbn=0-03-058673-9 |ref=none}}\n*{{Citation |last=Reynolds |first=Samuel W. |title=A generalized polyphase merge algorithm |journal=Communications of the ACM |volume=4 |issue=8 |date=August 1961 |pages=347–349 |publisher=ACM |location=New York, NY |doi=10.1145/366678.366689 |ref=none}}\n*{{Citation |last=Sedgewick |first=Robert |title=Algorithms |year=1983 |publisher=Addison-Wesley |isbn=0-201-06672-6 |pages=163–165 |ref=none}}\n\n==External links==\n\n{{sorting}}\n\n{{DEFAULTSORT:Polyphase merge sort}}\n[[Category:Sorting algorithms]]\n[[Category:Comparison sorts]]\n[[Category:Online sorts]]"
    },
    {
      "title": "Pre-topological order",
      "url": "https://en.wikipedia.org/wiki/Pre-topological_order",
      "text": "In the field of [[computer science]], a '''pre-topological order''' or '''pre-topological ordering''' of a [[directed graph]] is a linear ordering of its [[vertex (graph theory)|vertices]] such that if there is a directed path from vertex ''u'' to vertex ''v'' and ''v'' comes before ''u'' in the ordering, then there is also a directed path from vertex ''v'' to vertex ''u''.<ref name=\":0\">{{Cite book|url=https://books.google.com/books?id=mqGeSQ6dJycC|title=Combinatorial Optimization: Polyhedra and Efficiency|last=Schrijver|first=Alexander|date=2002-12-10|publisher=Springer Science & Business Media|year=|isbn=9783540443896|location=|pages=89|language=en}}</ref><ref>{{Cite web|url=http://algs4.cs.princeton.edu/42digraph/|title=Directed Graphs|last=Sedgewick|first=Robert|last2=Wayne|first2=Kevin|date=2016-09-26|website=Algorithms, 4th Edition|language=en|archive-url=|archive-date=|dead-url=|access-date=2017-09-06}}</ref>\n\nIf the graph is a [[directed acyclic graph]] (DAG), [[topological ordering]]s are pre-topological orderings and vice versa.<ref name=\":0\" /> In other cases, any pre-topological ordering gives a [[partial order]].\n\n== References ==\n{{Reflist}}\n{{sorting}}\n\n[[Category:Graph algorithms]]\n[[Category:Sorting algorithms]]\n[[Category:Directed graphs]]"
    },
    {
      "title": "Proxmap sort",
      "url": "https://en.wikipedia.org/wiki/Proxmap_sort",
      "text": "{{Infobox Algorithm\n|class=[[Sorting algorithm]]\n|image=[[File:Insertion Sorting during proxmap.PNG|none|315px|Insertion sorting into buckets during proxmap.]]\n|caption=Example of insertion sort sorting a list of random numbers.\n|data=[[Array data structure|Array]]\n|time=<math>O(n^2)</math>\n|best-time=<math>O(n)</math>\n|average-time=<math>O(n)</math>\n|space=<math>O(n)</math>\n}}\n[[File:Bucket sort 1.png|right|frame|Elements are distributed among bins]]\n[[File:Bucket sort 2.png|right|frame|Unlike bucket sorting which sorts after all the buckets are filled, the elements are [[insertion sort]]ed as they are inserted]]\n'''ProxmapSort''', or '''Proxmap sort''', is a [[sorting algorithm]] that works by partitioning an [[Array data structure|array]] of data items, or keys, into a number of \"subarrays\" (termed [[bucket (computing)|bucket]]s, in similar sorts). The name is short for computing a \"proximity map,\" which indicates for each key K the beginning of a subarray where K will reside in the final sorted order. Keys are placed into each subarray using [[insertion sort]]. If keys are \"well distributed\" among the subarrays, sorting occurs in linear time. The [[Analysis of algorithms|computational complexity]] estimates involve the number of subarrays and the proximity mapping function, the \"map key,\" used. It is a form of [[bucket sort|bucket]] and [[radix sort]].\n\nOnce a ProxmapSort is complete, '''ProxmapSearch''' can be used to find keys in the sorted array in <math>O(1)</math> time if the keys were well distributed during the sort.\n\nBoth algorithms were invented in the late 1980s by Prof. Thomas A. Standish at the [[Donald Bren School of Information and Computer Sciences|University of California, Irvine]].\n\n==Overview==\n\n===Basic strategy===\n\nIn general:\nGiven an array '''A''' with ''n'' keys:\n* map a key to a subarray of the destination array '''A2''', by applying the map key function to each array item\n* determine how many keys will map to the same subarray, using an array of '''\"hit counts,\" H'''\n* determine where each subarray will begin in the destination array so that each bucket is exactly the right size to hold all the keys that will map to it, using an array of '''\"proxmaps,\" P'''\n* for each key, compute the subarray it will map to, using an array of '''\"locations,\" L'''\n* for each key, look up its location, place it into that cell of '''A2'''; if it collides with a key already in that position, insertion sort the key into place, moving keys greater than this key to the right by one to make a space for this key. Since the subarray is big enough to hold all the keys mapped to it, such movement will never cause the keys to overflow into the following subarray.\n\nSimplied version:\nGiven an array '''A''' with ''n'' keys\n# '''Initialize''': Create and initialize 2 arrays of ''n'' size: '''hitCount''', '''proxMap''', and 2 arrays of '''A'''.length: '''location''', and '''A2'''.\n# '''Partition''': Using a carefully chosen '''mapKey''' function, divide the '''A2''' into subarrays using the keys in '''A'''\n# '''Disperse''': Read over '''A''', dropping each key into its bucket in '''A2'''; insertion sorting as needed.\n# '''Collect''': Visit the subarrays in order and put all the elements back into the original array, or simply use '''A2'''.\n\nNote: \"keys\" may also contain other data, for instance an array of Student objects that contain the key plus a student ID and name. This makes ProxMapSort suitable for organizing groups of objects, not just keys themselves.\n\n===Example===\n\nConsider a full array: '''A'''[''0'' to ''n-1''] with ''n'' keys. Let ''i'' be an index of A. Sort '''A''''s keys into array '''A2''' of equal size.\n\nThe map key function is defined as mapKey(key) = floor(K).\n\n{| class=\"wikitable\" style=\"text-align: center; width: 400px; border: 1px solid black;\"\n|+ Array table\n|-\n! scope=\"row\" | A1\n| 6.7 || 5.9 || 8.4 || 1.2 || 7.3 || 3.7 || 11.5 || 1.1 || 4.8 || 0.4 || 10.5 || 6.1 || 1.8\n|-\n! scope=\"row\" | H\n| 1 || 3 || 0 || 1 || 1 || 1 || 2 || 1 || 1 || 0 || 1 || 1\n|-\n! scope=\"row\" | P\n| 0 || 1 || -9 || 4 || 5 || 6 || 7 || 9 || 10 || -9 || 11 || 12\n|-\n! scope=\"row\" | L\n| 7 || 6 || 10 || 1 || 9 || 4 || 12 || 1 || 5 || 0 || 11 || 7 || 1\n|-\n! scope=\"row\" | A2\n| 0.4 || 1.1 || 1.2 || 1.8 || 3.7 || 4.8 || 5.9 || 6.1 || 6.7 || 7.3 || 8.4 || 10.5 || 11.5\n|}\n[[File:ProxMapSortDemo.gif|A demonstration of ProxMapSort, a bucket sort variant that uses intermediate parallel arrays to efficiently index and size its sublists.]]\n{{clear}}\n\n===Pseudocode===\n<source lang=\"java\">\n// compute hit counts\nfor i = 0 to 11 // where 11 is n\n{\n    H[i] = 0;\n}\nfor i = 0 to 12 // where 12 is A.length\n{\n    pos = MapKey(A[i]);\n    H[pos] = H[pos] + 1;\n}\n\nrunningTotal = 0; // compute prox map – location of start of each subarray\nfor i = 0 to 11\n    if H[i] = 0\n        P[i] = -9;\n    else\n        P[i] = runningTotal;\n        runningTotal = runningTotal + H[i];\n\nfor i = 0 to 12 // compute location – subarray – in A2 into which each item in A is to be placed\n    L[i] = P[MapKey(A[i])];\n\nfor I = 0 to 12; // sort items\n    A2[I] = <empty>;\nfor i = 0 to 12 // insert each item into subarray beginning at start, preserving order\n{\n    start = L[i]; // subarray for this item begins at this location\n    insertion made = false;\n    for j = start to (<the end of A2 is found, and insertion not made>)\n    {\n        if A2[j] == <empty> // if subarray empty, just put item in first position of subarray\n            A2[j] = A[i];\n            insertion made = true;\n        else if A[i] < A2[j] // key belongs at A2[j]\n            int end = j + 1; // find end of used part of subarray – where first <empty> is\n            while (A2[end] != <empty>)\n                end++;\n            for k = end -1 to j // move larger keys to the right 1 cell\n                A2[k+1] = A2[k];\n                A2[j] = A[i];\n            insertion made = true; // add in new key\n    }\n}\n</source>\n\nHere '''A''' is the array to be sorted and the mapKey functions determines the number of subarrays to use. For example, floor(K) will simply assign as many subarrays as there are integers from the data in '''A'''. Dividing the key by a constant reduces the number of subarrays; different functions can be used to translate the range of elements in '''A''' to subarrays, such as converting the letters A–Z to 0–25 or returning the first character (0–255) for sorting strings. Subarrays are sorted as the data comes in, not after all data has been placed into the subarray, as is typical in [[bucket sorting]].\n\n==Proxmap Searching==\nProxmapSearch uses the '''proxMap''' array generated by a previously done ProxmapSort to find keys in the sorted array '''A2''' in constant time.\n\n===Basic strategy===\n*Sort the keys using ProxmapSort, keeping  the '''MapKey''' function, and the '''P''' and '''A2''' arrays\n*To search for a key, go to P[MapKey(k)], the start of the subarray that contains the key, if that key is in the data set\n*Sequentially search the subarray; if the key  is found, return it (and associated information); if find a value greater than the key, the key is not in the data set\n*Computing P[MapKey(k)] takes <math>O(1)</math> time. If a map key that gives a good distribution of keys was used during the sort, each subarray is bounded above by a constant ''c'', so at most ''c'' comparisons are needed to find the key or know it is not present; therefore ProxmapSearch is <math>O(1)</math>. If the worst map key was used, all keys are in the same subarray, so ProxmapSearch, in this worst case, will require <math>O(n)</math> comparisons.\n\n===Pseudocode===\n\n '''function''' mapKey(key)\n   '''return''' floor(key)\n\n   proxMap ← previously generated proxmap array of size n\n   A2 ← previously sorted array of size n\n '''function''' proxmap-search(key)\n   '''for''' i = proxMap[mapKey(key)] '''to''' length(array)-1\n     '''if''' (sortedArray[i].key == key)\n       '''return''' sortedArray[i]\n\n==Analysis==\n\n===Performance===\nComputing H, P, and L all take <math>O(n)</math> time. Each is computed with one pass through an array, with constant time spent at each array location.\n*Worst case: MapKey places all items into one subarray, resulting in a standard insertion sort, and time of <math>O(n^2)</math>.\n*Best case: MapKey delivers the same small number of items to each subarray in an order where the best case of insertion sort occurs. Each insertion sort is <math>O(c)</math>, ''c'' the size of the subarrays; there are ''p'' subarrays thus '''p * c = n''', so the insertion phase take O(n); thus, ProxmapSort is <math>O(n)</math>.\n*Average case: Each subarray is at most size ''c'', a constant; insertion sort for each subarray is then O(c^2) at worst – a constant. (The actual time can be much better, since c items are not sorted until the last item is placed in the bucket). Total time is the number of buckets, '''(n/c)''', times <math>O(c^2)</math> = <math>O(n)</math>.\n\nHaving a good MapKey function is imperative for avoiding the worst case. We must know something about the distribution of the data to come up with a good key.\n\n===Optimizations===\n# Save time: Save the MapKey(i) values so they don't have to be recomputed (as they are in the code above)\n# Save space: The proxMaps can be stored in the hitCount array, as the hit counts are not needed once the proxmap is computed; the data can be sorted back into A, instead of using A2, if one takes care to note which A values have been sorted so far, and which not.\n\n===Comparison with other sorting algorithms===\nSince ProxmapSort is not a [[comparison sort]], the Ω(''n'' log ''n'') lower bound is inapplicable.{{Citation needed|date=May 2015}} Its speed can be attributed to it not being comparison-based and using arrays instead of dynamically allocated objects and pointers that must be followed, such as is done with when using a [[binary search tree]].\n\nProxmapSort allows for the use of ProxmapSearch. Despite the O(n) build time, ProxMapSearch makes up for it with its <math>O(1)</math> average access time, making it very appealing for large databases. If the data doesn't need to be updated often, the access time may make this function more favorable than other [[non-comparison sorting]] based sorts.\n\n===Generic bucket sort related to ProxmapSort===\nLike ProxmapSort, bucket sort generally operates on a list of ''n'' numeric inputs between zero and some maximum key or value ''M'' and divides the value range into ''n'' buckets each of size ''M''/''n''. If each bucket is sorted using [[insertion sort]], ProxmapSort and bucket sort can be shown to run in predicted linear time.<ref>[[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. ''[[Introduction to Algorithms]]'', Second Edition. MIT Press and McGraw-Hill, 2001. {{ISBN|0-262-03293-7}}. Section 8.4: Bucket sort, pp.174&ndash;177.</ref>{{OR|date=May 2015}} However, the performance of this sort degrades with clustering (or too few buckets with too many keys); if many values occur close together, they will all fall into a single bucket and performance will be severely diminished. This behavior also holds for ProxmapSort: if the buckets are too large, its performance will degrade severely.\n\n==References==\n<references />\n* Thomas A. Standish. ''Data Structures in Java.'' Addison Wesley Longman, 1998. {{ISBN|0-201-30564-X}}. Section 10.6, pp.&nbsp;394&ndash;405.\n* {{Cite journal | doi = 10.1145/1113847.1113874| title = Using {{math|''O''(''n'')}} Proxmap ''Sort'' and {{math|''O''(1)}} Proxmap ''Search'' to motivate CS2 students (Part I)| journal = ACM SIGCSE Bulletin| volume = 37| issue = 4 <!--| pages = 41-->| year = 2005| last1 = Standish | first1 = T. A. | last2 = Jacobson | first2 = N. }}\n* {{Cite journal | doi = 10.1145/1138403.1138427| title = Using {{math|''O''(''n'')}} Proxmap ''Sort'' and {{math|''O''(1)}} Proxmap ''Search'' to motivate CS2 students, Part II| journal = ACM SIGCSE Bulletin| volume = 38| issue = 2 <!--| pages = 29-->| year = 2006| last1 = Standish | first1 = T. A. | last2 = Jacobson | first2 = N. }}\n* Norman Jacobson [http://www.ics.uci.edu/~jacobson/ics23/ProxmapHandout.pdf \"A Synopsis of ProxmapSort & ProxmapSearch\"] from Department of Computer Science, [[Donald Bren School of Information and Computer Sciences]], [[University of California, Irvine|UC Irvine]].\n\n==External links==\n* http://www.cs.uah.edu/~rcoleman/CS221/Sorting/ProxMapSort.html\n* http://www.valdosta.edu/~sfares/cs330/cs3410.a.sorting.1998.fa.html\n* http://www.cs.uml.edu/~giam/91.102/Demos/ProxMapSort/ProxMapSort.c\n\n{{sorting}}\n\n{{DEFAULTSORT:ProxmapSort}}\n[[Category:Sorting algorithms]]\n[[Category:Stable sorts]]\n[[Category:Articles with example pseudocode]]"
    },
    {
      "title": "Qsort",
      "url": "https://en.wikipedia.org/wiki/Qsort",
      "text": "{{lowercase title}}\n{{other uses|Q sort (disambiguation)}}\n\n'''qsort''' is a [[C standard library]] function that implements a [[Polymorphism (computer science)|polymorphic]] [[sorting algorithm]] for arrays of arbitrary objects according to a user-provided comparison function. It is named after the \"quicker sort\" algorithm (a [[quicksort]] variant due to R. S. Scowen), which was originally used to implement it in the [[Unix]] C library, although the C standard does not require it to implement quicksort.<ref name=\"engineering\">{{cite journal |first1=Jon L. |last1=Bentley |first2=M. Douglas |last2=McIlroy |title=Engineering a sort function |journal=Software—Practice and Experience |volume=23 |issue=11 |pages=1249–1265 |year=1993 |url=http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.14.8162 |doi=10.1002/spe.4380231105|citeseerx=10.1.1.14.8162 }}</ref>\n\nImplementations of the qsort function achieve [[Polymorphism (computer science)|polymorphism]], the ability to sort different kinds of data, by taking a [[function pointer]] to a [[three-way comparison]] function, as well as a parameter that specifies the size of its individual input objects. The [[ISO C|C standard]] requires the comparison function to implement a [[total order]] on the items in the input array.<ref>ISO/IEC 9899:201x, Programming Languages—C (draft). §7.22.5. November 16, 2010.</ref>\n\nA qsort function was in place in [[Research Unix|Version 3 Unix]] of 1973, but was then an [[assembly language|assembler]] subroutine.<ref>{{cite web |title=qsort(III), from UNIX Programmer's Manual, Third Edition |website=Unix Archive |url=http://minnie.tuhs.org/cgi-bin/utree.pl?file=V3/man/man3/qsort.3}}</ref> A C version, with roughly the interface of the standard C version, was in-place in [[Version 6 Unix]].<ref>{{cite web |title=qsort(III), from UNIX Programmer's Manual, Sixth Edition |website=Unix Archive |url=http://minnie.tuhs.org/cgi-bin/utree.pl?file=V6/usr/man/man3/qsort.3}}</ref>\nIt was rewritten in 1983 at [[Berkeley Software Distribution|Berkeley]].<ref name=\"engineering\"/>\nThe function was standardized in [[ANSI C]] (1989).\n\n==Example==\nThe following piece of C code shows how to sort a list of integers using qsort.\n\n<source lang=\"c\">\n#include <stdlib.h>\n\n/* Comparison function. Receives two generic (void) pointers to the items under comparison. */\nint compare_ints(const void *p, const void *q) {\n    int x = *(const int *)p;\n    int y = *(const int *)q;\n\n    /* Avoid return x - y, which can cause undefined behaviour\n       because of signed integer overflow. */\n    if (x < y)\n        return -1;  // Return -1 if you want ascending, 1 if you want descending order. \n    else if (x > y)\n        return 1;   // Return 1 if you want ascending, -1 if you want descending order. \n\n    return 0;\n}\n\n/* Sort an array of n integers, pointed to by a. */\nvoid sort_ints(int *a, size_t n) {\n    qsort(a, n, sizeof *a, &compare_ints);\n}\n</source>\n\n==References==\n{{reflist}}\n\n{{software-eng-stub}}\n\n[[Category:C standard library]]\n[[Category:Sorting algorithms]]"
    },
    {
      "title": "Quantum sort",
      "url": "https://en.wikipedia.org/wiki/Quantum_sort",
      "text": "{{Use American English|date=January 2019}}{{Short description|Sorting algorithms for quantum computers\n}}\nA '''quantum sort''' is any [[sorting algorithm]] that runs on a [[quantum computer]]. Any comparison-based quantum sorting algorithm would take at least <math>\\Omega(n \\log n)</math> steps,<ref>{{cite conference\n |last1=Høyer |first1=P.\n |last2=Neerbek |first2=J.\n |last3=Shi |first3=Y.\n |title=Quantum complexities of ordered searching, sorting, and element distinctness\n |booktitle=28th International Colloquium on Automata, Languages, and Programming\n |pages=62–73\n |year=2001\n |arxiv=quant-ph/0102078\n |doi=10.1007/3-540-48224-5_29\n}}</ref> which is already achievable by classical algorithms. Thus, for this task, quantum computers are no better than classical ones. However, in space-bounded sorts, quantum algorithms outperform their classical counterparts.<ref>{{cite conference\n |last=Klauck\n |first=Hartmut\n |title=Quantum Time-Space Tradeoffs for Sorting\n |booktitle=Proceedings of the thirty-fifth annual ACM symposium on Theory of computing\n |year=2003\n |doi=10.1145/780542.780553\n|arxiv=quant-ph/0211174\n }}</ref>\n\n==References==\n<!-- this 'empty' section displays references defined elsewhere -->\n{{reflist}}\n\n{{quantum computing}}\n{{sorting}}\n\n[[Category:Sorting algorithms]]\n[[Category:Quantum information science]]\n[[Category:Quantum algorithms]]\n\n{{Quantum-stub}}{{comp-sci-theory-stub}}"
    },
    {
      "title": "Quicksort",
      "url": "https://en.wikipedia.org/wiki/Quicksort",
      "text": "{{Short description|A divide and conquer sorting algorithm}}\n{{Use dmy dates|date=January 2012}}\n{{Infobox algorithm\n| class        = [[Sorting algorithm]]\n| image        = Sorting quicksort anim.gif\n| caption      = Animated visualization of the quicksort algorithm. The horizontal lines are pivot values.\n| data         =\n| time         = O(''n''<sup>2</sup>)  <!-- If it wasn't rare, the average time would be closer to it. -->\n| average-time = O(''n'' log ''n'')\n| best-time    = O(''n'' log ''n'') (simple partition)<br />or O(''n'') (three-way partition and equal keys)\n| space        = O(''n'') auxiliary (naive)<br />O(log ''n'') auxiliary (Sedgewick 1978)<!-- see [[#Space complexity]] -->\n| optimal      = No <!-- optimal is defined as worst case -->\n| stability    = [[Sorting algorithm#Classification|Not Stable]]\n}}\n\n'''Quicksort''' (sometimes called '''partition-exchange sort''') is an [[Algorithm efficiency|efficient]] [[sorting algorithm]], serving as a systematic method for placing the elements of a [[random access]] [[Computer file|file]] or an [[Array data structure|array]] in order. Developed by British computer scientist [[Tony Hoare]] in 1959<ref>{{cite web |title=Sir Antony Hoare |publisher=Computer History Museum |accessdate=22 April 2015 |url=http://www.computerhistory.org/fellowawards/hall/bios/Antony,Hoare/ |deadurl=yes |archiveurl=https://web.archive.org/web/20150403184558/http://www.computerhistory.org/fellowawards/hall/bios/Antony%2CHoare/ |archivedate=3 April 2015 |df=dmy-all }}</ref> and published in 1961,<ref name=alg64>{{Cite journal |last1 = Hoare |first1 = C. A. R. |authorlink1 = Tony Hoare |title = Algorithm 64: Quicksort |doi = 10.1145/366622.366644 |journal = [[Communications of the ACM|Comm. ACM]] |volume = 4 |issue = 7 |pages = 321 |year = 1961 |pmid = |pmc = }}</ref> it is still a commonly used algorithm for sorting. When implemented well, it can be about two or three times faster than its main competitors, [[merge sort]] and [[heapsort]].<ref name=\"skiena\">{{cite book |first=Steven S. |last=Skiena |year=2008 |authorlink=Steven Skiena |title=The Algorithm Design Manual |url=https://books.google.com/books?id=7XUSn0IKQEgC |publisher=Springer |isbn=978-1-84800-069-8 |page=129}}</ref>{{Contradict-inline|section=Relation to other algorithms|date=July 2017}}\n\nQuicksort is a [[comparison sort]], meaning that it can sort items of any type for which a \"less-than\" relation (formally, a [[total order]]) is defined. In efficient implementations it is not a [[stable sort]], meaning that the relative order of equal sort items is not preserved. Quicksort can operate [[in-place algorithm|in-place]] on an array, requiring small additional amounts of [[Main memory|memory]] to perform the sorting. It is very similar to [[selection sort]], except that it does not always choose worst-case partition.\n\n[[Analysis of algorithms|Mathematical analysis]] of quicksort shows that, [[best, worst and average case|on average]], the algorithm takes [[Big O notation|O]](''n''&nbsp;log&nbsp;''n'') comparisons to sort ''n'' items. In the [[best, worst and average case|worst case]], it makes O(''n''<sup>2</sup>) comparisons, though this behavior is rare.\n\n== History ==\nThe quicksort algorithm was developed in 1959 by [[Tony Hoare]] while in the [[Soviet Union]], as a visiting student at [[Moscow State University]]. At that time, Hoare worked on a project on [[machine translation]] for the [[National Physical Laboratory, UK|National Physical Laboratory]]. As a part of the translation process, he needed to sort the words in Russian sentences prior to looking them up in a Russian-English dictionary that was already sorted in alphabetic order on [[magnetic tape data storage|magnetic tape]].<ref>{{Cite journal |last = Shustek |first = L. |title = Interview: An interview with C.A.R. Hoare |doi = 10.1145/1467247.1467261 |journal = [[Communications of the ACM|Comm. ACM]] |volume = 52 |issue = 3 |pages = 38–41 |year = 2009 |pmid = |pmc = }}</ref> After recognizing that his first idea, [[insertion sort]], would be slow, he quickly came up with a new idea that was Quicksort. He wrote a program in Mercury [[Autocode]] for the partition but could not write the program to account for the list of unsorted segments. On return to England, he was asked to write code for [[Shellsort]] as part of his new job. Hoare mentioned to his boss that he knew of a faster algorithm and his boss bet sixpence that he did not. His boss ultimately accepted that he had lost the bet. Later, Hoare learned about [[ALGOL]] and its ability to do recursion that enabled him to publish the code in ''[[Communications of the ACM|Communications of the Association for Computing Machinery]]'', the premier computer science journal of the time.<ref name=alg64 /><ref>{{Cite web |url = http://anothercasualcoder.blogspot.com/2015/03/my-quickshort-interview-with-sir-tony.html |title = My Quickshort interview with Sir Tony Hoare, the inventor of Quicksort |date = 2015-03-15 |accessdate = |website = |publisher = Marcelo M De Barros |last = |first = }}</ref>\n\nQuicksort gained widespread adoption, appearing, for example, in [[Unix]] as the default library sort subroutine. Hence, it lent its name to the [[C standard library]] subroutine <tt>[[qsort]]</tt><ref name=\"engineering\" /> and in the reference implementation of [[Java (programming language)|Java]].\n\n[[Robert Sedgewick (computer scientist)|Robert Sedgewick]]'s Ph.D. thesis in 1975 is considered a milestone in the study of Quicksort where he resolved many open problems related to the analysis of various pivot selection schemes including [[Samplesort]], adaptive partitioning by Van Emden<ref>{{Cite journal | title = Algorithms 402: Increasing the Efficiency of Quicksort | journal = Commun. ACM | date = 1970-11-01 | issn = 0001-0782 | pages = 693–694 | volume = 13 | issue = 11 | doi = 10.1145/362790.362803 | first = M. H. | last = Van Emden}}</ref> as well as derivation of expected number of comparisons and swaps.<ref name=\"engineering\" /> Bentley and McIlroy incorporated various improvements for use in programming libraries, including a technique to deal with equal elements and a pivot scheme known as ''pseudomedian of nine,'' where a sample of nine elements is divided into groups of three and then the median of the three medians from three groups is chosen.<ref name=\"engineering\" /> [[Jon Bentley (computer scientist)|Jon Bentley]] described another simpler and compact partitioning scheme in his book ''[[Jon Bentley (computer scientist)|Programming Pearls]]'' that he attributed to Nico Lomuto. Later Bentley wrote that he used Hoare's version for years but never really understood it but Lomuto's version was simple enough to prove correct.<ref>{{Cite book | title = Beautiful Code: Leading Programmers Explain How They Think | editor1-last = Oram | editor1-first = Andy | editor2-last = Wilson | editor2-first = Greg | publisher = O'Reilly Media | year = 2007 | isbn = 978-0-596-51004-6|location = | pages = 30 | chapter = The most beautiful code I never wrote | first = Jon | last = Bentley | authorlink = Jon Bentley (computer scientist)}}</ref> Bentley described Quicksort as the \"most beautiful code I had ever written\" in the same essay. Lomuto's partition scheme was also popularized by the textbook ''[[Introduction to Algorithms]]'' although it is inferior to Hoare's scheme because it does three times more swaps on average and degrades to {{math|''O''(''n''<sup>2</sup>)}} runtime when all elements are equal.<ref name=\":1\">{{Cite web | title = Quicksort Partitioning: Hoare vs. Lomuto | url = http://cs.stackexchange.com/a/11550/4201 | website = cs.stackexchange.com | accessdate = 2015-08-03}}</ref> {{self-published inline | date=August 2015}}\n\nIn 2009, Vladimir Yaroslavskiy proposed the new dual pivot Quicksort implementation.<ref name=\":0\">{{Cite web | url = http://iaroslavski.narod.ru/quicksort/DualPivotQuicksort.pdf | title = Dual-Pivot Quicksort | date = 2009|archive-url = https://web.archive.org/web/20151002230717/http://iaroslavski.narod.ru/quicksort/DualPivotQuicksort.pdf|archive-date = 2015-10-02 | dead-url = yes | accessdate = | website = | publisher = | last = Yaroslavskiy | first = Vladimir}}</ref> In the Java core library mailing lists, he initiated a discussion claiming his new algorithm to be superior to the runtime library's sorting method, which was at that time based on the widely used and carefully tuned variant of classic Quicksort by Bentley and McIlroy.<ref>{{Cite web | title = Replacement of Quicksort in java.util.Arrays with new Dual-Pivot Quick | url = http://permalink.gmane.org/gmane.comp.java.openjdk.core-libs.devel/2628 | website = permalink.gmane.org | accessdate = 2015-08-03}}</ref> Yaroslavskiy's Quicksort has been chosen as the new default sorting algorithm in Oracle's Java 7 runtime library<ref>{{Cite web | url = https://docs.oracle.com/javase/7/docs/api/java/util/Arrays.html#sort(int[]) | title = Java 7 Arrays API documentation | accessdate =2018-07-23 | publisher = Oracle}}</ref> after extensive empirical performance tests.<ref>{{Cite book | title = Engineering Java 7's Dual Pivot Quicksort Using MaLiJAn | publisher = Society for Industrial and Applied Mathematics | date = 2013-01-07 | isbn = 978-1-61197-253-5 | pages = 55–69|series = Proceedings | doi = 10.1137/1.9781611972931.5 | first = S. | last = Wild | first2 = M. | last2 = Nebel | first3 = R. | last3 = Reitzig | first4 = U. | last4 = Laube}}</ref>\n\n== Algorithm ==\n[[File:Quicksort-diagram.svg|right|200px|thumb|Full example of quicksort on a random set of numbers. The shaded element is the pivot. It is always chosen as the last element of the partition. However, always choosing the last element in the partition as the pivot in this way results in poor performance ({{math|''O''(''n''²)}}) on ''already sorted'' arrays, or arrays of identical elements. Since sub-arrays of sorted / identical elements crop up a lot towards the end of a sorting procedure on a large set, versions of the quicksort algorithm that choose the pivot as the middle element run much more quickly than the algorithm described in this diagram on large sets of numbers.]]\nQuicksort is a [[divide and conquer algorithm]]. Quicksort first divides a large array into two smaller sub-arrays: the low elements and the high elements. Quicksort can then recursively sort the sub-arrays. The steps are:\n# Pick an element, called a ''pivot'', from the array.\n# ''Partitioning'': reorder the array so that all elements with values less than the pivot come before the pivot, while all elements with values greater than the pivot come after it (equal values can go either way). After this partitioning, the pivot is in its final position. This is called the ''partition'' operation.\n# [[Recursion (computer science)|Recursively]] apply the above steps to the sub-array of elements with smaller values and separately to the sub-array of elements with greater values.\n\nThe base case of the recursion is arrays of size zero or one, which are in order by definition, so they never need to be sorted.\n\nThe pivot selection and partitioning steps can be done in several different ways; the choice of specific implementation schemes greatly affects the algorithm's performance.\n\n=== Lomuto partition scheme ===\nThis scheme is attributed to Nico Lomuto and popularized by Bentley in his book ''Programming Pearls''<ref name=\":3\" /> and Cormen ''et al.'' in their book ''[[Introduction to Algorithms]]''.<ref name=\":2\"/> This scheme chooses a pivot that is typically the last element in the array. The algorithm maintains index {{mono|i}} as it scans the array using another index {{mono|j}} such that the elements {{mono|lo}} through {{mono|i-1}} (inclusive) are less than the pivot, and the elements {{mono|i}} through {{mono|j}} (inclusive) are equal to or greater than the pivot. As this scheme is more compact and easy to understand, it is frequently used in introductory material, although it is less efficient than Hoare's original scheme.<ref>{{Cite web |url = https://kluedo.ub.uni-kl.de/frontdoor/index/index/docId/3463 |title = Java 7's Dual Pivot Quicksort |date = 2012 |accessdate = |website = |publisher = Technische Universität Kaiserslautern |last = Wild |first = Sebastian}}</ref> This scheme degrades to {{math|''O''(''n''<sup>2</sup>)}} when the array is already in order.<ref name=\":1\" /> There have been various variants proposed to boost performance including various ways to select pivot, deal with equal elements, use other sorting algorithms such as [[Insertion sort]] for small arrays and so on. In [[pseudocode]], a quicksort that sorts elements {{mono|lo}} through {{mono|hi}} (inclusive) of an array {{mvar|A}} can be expressed as:<ref name=\":2\">{{Introduction to Algorithms|3 |chapter=Quicksort |pages=170–190}}</ref>\n\n '''algorithm''' quicksort(A, lo, hi) '''is'''\n     '''if''' lo < hi '''then'''\n         p := partition(A, lo, hi)\n         quicksort(A, lo, p - 1)\n         quicksort(A, p + 1, hi)\n \n '''algorithm''' partition(A, lo, hi) '''is'''\n     pivot := A[hi]\n     i := lo\n     '''for''' j := lo '''to''' hi - 1 '''do'''\n         '''if''' A[j] < pivot '''then'''\n             swap A[i] with A[j]\n             i := i + 1\n     swap A[i] with A[hi]\n     '''return''' i\n\nSorting the entire array is accomplished by {{mono|quicksort(A, 0, length(A) - 1)}}.\n\n=== Hoare partition scheme ===\nThe original partition scheme described by C.A.R. Hoare uses two indices that start at the ends of the array being partitioned, then move toward each other, until they detect an inversion: a pair of elements, one greater than or equal to the pivot, one lesser or equal, that are in the wrong order relative to each other. The inverted elements are then swapped.<ref>{{Cite journal | title = Quicksort | url = http://comjnl.oxfordjournals.org/content/5/1/10 | journal = The Computer Journal | date = 1962-01-01 | issn = 0010-4620 | pages = 10–16 | volume = 5 | issue = 1 | doi = 10.1093/comjnl/5.1.10 | first = C. A. R. | last = Hoare | authorlink = Tony Hoare }}</ref> When the indices meet, the algorithm stops and returns the final index. Hoare's scheme is more efficient than Lomuto's partition scheme because it does three times fewer swaps on average, and it creates efficient partitions even when all values are equal.<ref name=\":1\" />{{self-published inline | date=August 2015}} Like Lomuto's partition scheme, Hoare's partitioning also would cause Quicksort to degrade to {{math|''O''(''n''<sup>2</sup>)}} for already sorted input, if the pivot was chosen as the first or the last element. With the middle element as the pivot, however, sorted data results with (almost) no swaps in equally sized partitions leading to best case behavior of Quicksort, i.e. {{math|''O''(''n'' log(''n''))}}. Like others, Hoare's partitioning doesn't produce a stable sort. Note that in this scheme, the pivot's final location is not necessarily at the index that was returned, and the next two segments that the main algorithm recurs on are {{mono|(lo..p)}} and {{mono|(p+1..hi)}} as opposed to {{mono|(lo..p-1)}} and {{mono|(p+1..hi)}} as in Lomuto's scheme. However, the partitioning algorithm guarantees {{nowrap|{{mono|lo ≤ p < hi}}}} which implies both resulting partitions are non-empty, hence there's no risk of infinite recursion. In [[pseudocode]],<ref name=\":2\"/>\n\n '''algorithm''' quicksort(A, lo, hi) '''is'''\n     '''if''' lo < hi '''then'''\n         p := partition(A, lo, hi)\n         quicksort(A, lo, p)\n         quicksort(A, p + 1, hi)\n \n '''algorithm''' partition(A, lo, hi) '''is'''\n     pivot := A[(hi + lo) / 2]\n     '''loop forever'''\n         '''while''' A[lo] < pivot         \n             lo := lo + 1\n \n         '''while''' A[hi] > pivot\n             hi := hi - 1\n  \n         '''if''' lo >= hi '''then'''\n             '''return''' hi\n \n         swap A[lo] with A[hi]\n \n         lo := lo + 1\n         hi := hi - 1\n\nThe entire array is sorted by {{mono|quicksort(A, 0, length(A)-1)}}.\n\n=== Implementation issues ===\n\n==== Choice of pivot ====\nIn the very early versions of quicksort, the leftmost element of the partition would often be chosen as the pivot element. Unfortunately, this causes worst-case behavior on already sorted arrays, which is a rather common use-case. The problem was easily solved by choosing either a random index for the pivot, choosing the middle index of the partition or (especially for longer partitions) choosing the [[median]] of the first, middle and last element of the partition for the pivot (as recommended by [[Robert Sedgewick (computer scientist)|Sedgewick]]).<ref name=sedgewickBook /> This \"median-of-three\" rule counters the case of sorted (or reverse-sorted) input, and gives a better estimate of the optimal pivot (the true median) than selecting any single element, when no information about the ordering of the input is known.\n\nMedian-of-three code snippet for Lomuto partition:\n mid := (lo + hi) / 2\n '''if''' A[mid] < A[lo]\n     swap A[lo] with A[mid]\n '''if''' A[hi] < A[lo]\n     swap A[lo] with A[hi]\n '''if''' A[mid] < A[hi]\n     swap A[mid] with A[hi]\n pivot := A[hi]\n\nSpecifically, the expected number of comparisons needed to sort {{mvar|n}} elements (see {{Section link||Analysis of randomized quicksort}}) with random pivot selection is {{math|1.386 ''n'' log ''n''}}. Median-of-three pivoting brings this down to {{math|[[Binomial coefficient|''C'']]<sub>''n'', 2</sub> ≈ 1.188 ''n'' log ''n''}}, at the expense of a three-percent increase in the expected number of swaps.{{r|engineering}} An even stronger pivoting rule, for larger arrays, is to pick the [[ninther]], a recursive median-of-three (Mo3), defined as{{r|engineering}}\n\n:{{math|ninther(''a'') {{=}} median(Mo3(first ⅓ of ''a''), Mo3(middle ⅓ of ''a''), Mo3(final ⅓ of ''a''))}}\n\nSelecting a pivot element is also complicated by the existence of [[integer overflow]]. If the boundary indices of the subarray being sorted are sufficiently large, the naïve expression for the middle index, {{math|(''lo'' + ''hi'')/2}}, will cause overflow and provide an invalid pivot index. This can be overcome by using, for example, {{math|''lo'' + (''hi''−''lo'')/2}} to index the middle element, at the cost of more complex arithmetic. Similar issues arise in some other methods of selecting the pivot element.\n\n==== Repeated elements ====\n<!-- XXX This section liberally makes statements about complexity. It might be wise to move the analysis section up. -->\nWith a partitioning algorithm such as the Lomuto partition scheme described above (even one that chooses good pivot values), quicksort exhibits poor performance for inputs that contain many repeated elements. The problem is clearly apparent when all the input elements are equal: at each recursion, the left partition is empty (no input values are less than the pivot), and the right partition has only decreased by one element (the pivot is removed). Consequently, the Lomuto partition scheme takes [[quadratic time]] to sort an array of equal values. However, with a partitioning algorithm such as the Hoare partition scheme, repeated elements generally results in better partitioning, and although needless swaps of elements equal to the pivot may occur, the running time generally decreases as the number of repeated elements increases (with memory cache reducing the swap overhead). In the case where all elements are equal, Hoare partition scheme needlessly swaps elements, but the partitioning itself is best case, as noted in the Hoare partition section above.\n\nTo solve the Lomuto partition scheme problem (sometimes called the [[Dutch national flag problem]]<ref name=\"engineering\" />), an alternative linear-time partition routine can be used that separates the values into three groups: values less than the pivot, values equal to the pivot, and values greater than the pivot. (Bentley and McIlroy call this a \"fat partition\" and note that it was already implemented in the {{mono|[[qsort]]}} of [[Version 7 Unix]].<ref name=\"engineering\">{{cite journal |first1=Jon L. |last1=Bentley |first2=M. Douglas |last2=McIlroy |title=Engineering a sort function  |journal=Software—Practice and Experience |volume=23 |issue=11 |pages=1249–1265 |year=1993 |url=http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.14.8162 |doi=10.1002/spe.4380231105|citeseerx=10.1.1.14.8162 }}</ref>) The values equal to the pivot are already sorted, so only the less-than and greater-than partitions need to be recursively sorted. In pseudocode, the quicksort algorithm becomes\n\n '''algorithm''' quicksort(A, lo, hi) '''is'''\n     '''if''' lo < hi '''then'''\n         p := pivot(A, lo, hi)\n         left, right := partition(A, p, lo, hi)  ''// note: multiple return values''\n         quicksort(A, lo, left - 1)\n         quicksort(A, right + 1, hi)\n\nThe <code>partition</code> algorithm returns indices to the first ('leftmost') and to the last ('rightmost') item of the middle partition. Every item of the partition is equal to <code>p</code> and is therefore sorted. Consequently, the items of the partition need not be included in the recursive calls to <code>quicksort</code>.\n\nThe best case for the algorithm now occurs when all elements are equal (or are chosen from a small set of {{math|''k'' ≪ ''n''}} elements). In the case of all equal elements, the modified quicksort will perform only two recursive calls on empty subarrays and thus finish in linear time (assuming the <code>partition</code> subroutine takes no longer than linear time).\n\n==== Optimizations ====\nTwo other important optimizations, also suggested by Sedgewick and widely used in practice, are:<ref name=\"glibc_qsort\">qsort.c in [[GNU libc]]: [http://www.cs.columbia.edu/~hgs/teaching/isp/hw/qsort.c], [http://repo.or.cz/w/glibc.git/blob/HEAD:/stdlib/qsort.c]</ref><ref>http://www.ugrad.cs.ubc.ca/~cs260/chnotes/ch6/Ch6CovCompiled.html{{dead link |date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}</ref>\n* To make sure at most {{math|''O''(log ''n'')}} space is used, [[wikt:recurse|recur]] first into the smaller side of the partition, then use a [[tail call]] to recur into the other, or update the parameters to no longer include the now sorted smaller side, and iterate to sort the larger side.\n* When the number of elements is below some threshold (perhaps ten elements), switch to a non-recursive sorting algorithm such as [[insertion sort]] that performs fewer swaps, comparisons or other operations on such small arrays. The ideal 'threshold' will vary based on the details of the specific implementation.\n* An older variant of the previous optimization: when the number of elements is less than the threshold {{mvar|k}}, simply stop; then after the whole array has been processed, perform insertion sort on it. Stopping the recursion early leaves the array {{mvar|k}}-sorted, meaning that each element is at most {{mvar|k}} positions away from its final sorted position. In this case, insertion sort takes {{math|''O''(''kn'')}} time to finish the sort, which is linear if {{mvar|k}} is a constant.{{r|sedgewickQsortPaper}}<ref name=\":3\">{{cite book |author=Jon Bentley | title=Programming Pearls | publisher=Addison-Wesley Professional |year=1999}}</ref>{{rp|117}} Compared to the \"many small sorts\" optimization, this version may execute fewer instructions, but it makes suboptimal use of the [[Cache memory|cache memories]] in modern computers.<ref name=LaMarca1999>{{cite journal | first1=Anthony | last1=LaMarca | first2=Richard E. |last2=Ladner |title=The Influence of Caches on the Performance of Sorting  | journal=Journal of Algorithms | volume=31 | issue=1 | pages=66–104 |doi=10.1006/jagm.1998.0985 |quote=Although saving small subarrays until the end makes sense from an instruction count perspective, it is exactly the wrong thing to do from a cache performance perspective.| year=1999 | citeseerx=10.1.1.27.1788 }}</ref>\n\n==== Parallelization ====\nQuicksort's divide-and-conquer formulation makes it amenable to [[parallel algorithm|parallelization]] using [[task parallelism]].\nThe partitioning step is accomplished through the use of a [[prefix sum|parallel prefix sum]] algorithm to compute an index for each array element in its section of the partitioned array.<ref>Umut A. Acar, Guy E Blelloch, Margaret Reid-Miller, and Kanat Tangwongsan, [http://www.cs.cmu.edu/afs/cs/academic/class/15210-s13/www/lectures/lecture19.pdf Quicksort and Sorting Lower Bounds], ''Parallel and Sequential Data Structures and Algorithms''. 2013.</ref><ref>{{Cite journal | title=Quicksort Partition via Prefix Scan | year=2012  | journal=Dr. Dobbs | last=Breshears | first=Clay | url=http://www.drdobbs.com/parallel/quicksort-partition-via-prefix-scan/240003109}}</ref> Given an array of size {{mvar|n}}, the partitioning step performs {{math|O(''n'')}} work in {{math|''O''(log ''n'')}} time and requires {{math|O(''n'')}} additional scratch space. After the array has been partitioned, the two partitions can be sorted recursively in parallel. Assuming an ideal choice of pivots, parallel quicksort sorts an array of size {{mvar|n}} in {{math|O(''n'' log ''n'')}} work in {{math|O(log² ''n'')}} time using {{math|O(''n'')}} additional space.\n\nQuicksort has some disadvantages when compared to alternative sorting algorithms, like [[merge sort]], which complicate its efficient parallelization. The depth of quicksort's divide-and-conquer tree directly impacts the algorithm's scalability, and this depth is highly dependent on the algorithm's choice of pivot. Additionally, it is difficult to parallelize the partitioning step efficiently in-place. The use of scratch space simplifies the partitioning step, but increases the algorithm's memory footprint and constant overheads.\n\nOther more sophisticated parallel sorting algorithms can achieve even better time bounds.<ref>{{cite book | first=Russ | last=Miller | first2=Laurence | last2=Boxer | title=Algorithms sequential & parallel: a unified approach | url=https://books.google.com/books?id=dZoZAQAAIAAJ | accessdate=27 November 2012 | year=2000 | publisher=Prentice Hall | isbn=978-0-13-086373-7}}</ref> For example, in 1991 David Powers described a parallelized quicksort (and a related [[radix sort]]) that can operate in {{math|''O''(log ''n'')}} time on a [[Parallel random-access machine#Read/write conflicts|CRCW]] (concurrent read and concurrent write) [[Parallel Random Access Machine|PRAM]] (parallel random-access machine)with {{mvar|n}} processors by performing partitioning implicitly.<ref>{{cite conference | first=David M. W. | last=Powers | title=Parallelized Quicksort and Radixsort with Optimal Speedup |conference=Proc. Int'l Conf. on Parallel Computing Technologies | year=1991 |citeseerx=10.1.1.57.9071}}</ref>\n\n== Formal analysis ==\n=== Worst-case analysis ===\nThe most unbalanced partition occurs when one of the sublists returned by the partitioning routine is of size {{math|''n'' − 1}}.<ref name=\"unbalanced\">The other one may either have {{math|1}} element or be empty (have {{math|0}} elements), depending on whether the pivot is included in one of subpartitions, as in the Hoare's partitioning routine, or is excluded from both of them, like in the Lomuto's routine.</ref> This may occur if the pivot happens to be the smallest or largest element in the list, or in some implementations (e.g., the Lomuto partition scheme as described above) when all the elements are equal.\n\nIf this happens repeatedly in every partition, then each recursive call processes a list of size one less than the previous list. Consequently, we can make {{math|''n'' − 1}} nested calls before we reach a list of size 1. This means that the [[Call stack|call tree]] is a linear chain of {{math|''n'' − 1}} nested calls. The {{mvar|i}}th call does {{math|''O''(''n'' − ''i'')}} work to do the partition, and <math>\\textstyle\\sum_{i=0}^n (n-i) = O(n^2)</math>, so in that case Quicksort takes {{math|''O''(''n''²)}} time.\n\n=== Best-case analysis ===\nIn the most balanced case, each time we perform a partition we divide the list into two nearly equal pieces. This means each recursive call processes a list of half the size. Consequently, we can make only {{math|log<sub>2</sub> ''n''}} nested calls before we reach a list of size 1. This means that the depth of the [[Call stack|call tree]] is {{math|log<sub>2</sub> ''n''}}. But no two calls at the same level of the call tree process the same part of the original list; thus, each level of calls needs only {{math|''O''(''n'')}} time all together (each call has some constant overhead, but since there are only {{math|''O''(''n'')}} calls at each level, this is subsumed in the {{math|''O''(''n'')}} factor). The result is that the algorithm uses only {{math|''O''(''n'' log ''n'')}} time.\n\n=== Average-case analysis ===\nTo sort an array of {{mvar|n}} distinct elements, quicksort takes {{math|''O''(''n'' log ''n'')}} time in expectation, averaged over all {{math|''n''!}} permutations of {{mvar|n}} elements with [[Uniform distribution (discrete)|equal probability]]. We list here three common proofs to this claim providing different insights into quicksort's workings.\n\n==== Using percentiles ====\nIf each pivot has rank somewhere in the middle 50 percent, that is, between the 25th [[percentile]] and the 75th percentile, then it splits the elements with at least 25% and at most 75% on each side. If we could consistently choose such pivots, we would only have to split the list at most <math>\\log_{4/3} n</math> times before reaching lists of size 1, yielding an {{math|''O''(''n'' log ''n'')}} algorithm.\n\nWhen the input is a random permutation, the pivot has a random rank, and so it is not guaranteed to be in the middle 50 percent. However, when we start from a random permutation, in each recursive call the pivot has a random rank in its list, and so it is in the middle 50 percent about half the time. That is good enough. Imagine that you flip a coin: heads means that the rank of the pivot is in the middle 50 percent, tail means that it isn't. Imagine that you are flipping a coin over and over until you get {{mvar|k}} heads. Although this could take a long time, on average only {{math|2''k''}} flips are required, and the chance that you won't get {{mvar|k}} heads after {{math|100''k''}} flips is highly improbable (this can be made rigorous using [[Chernoff bound]]s). By the same argument, Quicksort's recursion will terminate on average at a call depth of only <math>2 \\log_{4/3} n</math>. But if its average call depth is {{math|''O''(log ''n'')}}, and each level of the call tree processes at most {{mvar|n}} elements, the total amount of work done on average is the product, {{math|''O''(''n'' log ''n'')}}. Note that the algorithm does not have to verify that the pivot is in the middle half—if we hit it any constant fraction of the times, that is enough for the desired complexity.\n\n==== Using recurrences ====\nAn alternative approach is to set up a [[recurrence relation]] for the {{math|''T''(''n'')}} factor, the time needed to sort a list of size {{mvar|n}}. In the most unbalanced case, a single quicksort call involves {{math|''O''(''n'')}} work plus two recursive calls on lists of size {{math|0}} and {{math|''n''−1}}, so the recurrence relation is\n\n:<math>T(n) = O(n) + T(0) + T(n-1) = O(n) + T(n-1).</math>\n\nThis is the same relation as for [[insertion sort]] and [[selection sort]], and it solves to worst case {{math|''T''(''n'') {{=}} ''O''(''n''²)}}.\n\nIn the most balanced case, a single quicksort call involves {{math|''O''(''n'')}} work plus two recursive calls on lists of size {{math|''n''/2}}, so the recurrence relation is\n\n:<math>T(n) = O(n) + 2T\\left(\\frac{n}{2}\\right).</math>\n\nThe [[master theorem (analysis of algorithms)|master theorem for divide-and-conquer recurrences]] tells us that {{math|''T''(''n'') {{=}} ''O''(''n'' log ''n'')}}.\n\nThe outline of a formal proof of the {{math|''O''(''n'' log ''n'')}} expected time complexity follows. Assume that there are no duplicates as duplicates could be handled with linear time pre- and post-processing, or considered cases easier than the analyzed. When the input is a random permutation, the rank of the pivot is uniform random from 0 to {{math|''n'' − 1}}. Then the resulting parts of the partition have sizes {{mvar|i}} and {{math|''n'' − ''i'' − 1}}, and i is uniform random from 0 to {{math|''n'' − 1}}. So, averaging over all possible splits and noting that the number of comparisons for the partition is {{math|''n'' − 1}}, the average number of comparisons over all permutations of the input sequence can be estimated accurately by solving the recurrence relation:\n\n:<math>C(n) = n - 1 + \\frac{1}{n} \\sum_{i=0}^{n-1} (C(i)+C(n-i-1)) = n - 1 + \\frac{2}{n} \\sum_{i=0}^{n-1} C(i)</math>\n:<math>n C(n) = n (n - 1) + 2 \\sum_{i=0}^{n-1} C(i)</math>\n\n:<math>n C(n) - (n - 1) C(n - 1) = n (n - 1) - (n - 1) (n - 2) + 2 C(n - 1)</math>\n\n:<math>n C(n) = (n + 1) C(n - 1) + 2n - 2</math>\n\n:<math>\\begin{align}\n\\frac{C(n)}{n + 1} & = \\frac{C(n - 1)}{n} + \\frac{2}{n+1} - \\frac{2}{n(n+1)} \\le \\frac{C(n - 1)}{n} + \\frac{2}{n+1} \\\\\n& = \\frac{C(n-2)}{n-1} + \\frac{2}{n} - \\frac{2}{(n-1)n} + \\frac{2}{n+1}  \\le \\frac{C(n-2)}{n-1} + \\frac{2}{n} + \\frac{2}{n+1} \\\\\n& \\ \\ \\vdots \\\\\n& = \\frac{C(1)}{2} + \\sum_{i=2}^n \\frac{2}{i + 1}\n\\leq 2 \\sum_{i=1}^{n-1} \\frac{1}{i} \n\\approx 2 \\int_1^n \\frac{1}{x} \\mathrm{d} x = 2 \\ln n\n\\end{align}\n</math>\n\nSolving the recurrence gives {{math|''C''(''n'') {{=}} 2''n'' ln ''n'' ≈ 1.39''n'' log₂ ''n''}}.\n\nThis means that, on average, quicksort performs only about 39% worse than in its best case. In this sense, it is closer to the best case than the worst case. Also note that a [[comparison sort]] cannot use less than {{math|log₂(''n''!)}} comparisons on average to sort {{mvar|n}} items (as [[Comparison sort#Lower bound for the average number of comparisons|explained in the article Comparison sort]]) and in case of large {{mvar|n}}, [[Stirling's approximation]] yields {{math|log₂(''n''!) ≈ ''n''(log₂ ''n'' − log₂ ''e'')}}, so quicksort is not much worse than an ideal comparison sort. This fast average runtime is another reason for quicksort's practical dominance over other sorting algorithms.\n\n==== Using a binary search tree ====\nTo each execution of quicksort corresponds the following [[binary search tree]] (BST): the initial pivot is the root node; the pivot of the left half is the root of the left subtree, the pivot of the right half is the root of the right subtree, and so on. The number of comparisons of the execution of quicksort equals the number of comparisons during the construction of the BST by a sequence of insertions. So, the average number of comparisons for randomized quicksort equals the average cost of constructing a BST when the values inserted <math>(x_1,x_2,\\ldots,x_n)</math> form a random permutation.\n\nConsider a BST created by insertion of a sequence <math>(x_1,x_2,\\ldots,x_n)</math> of values forming a random permutation. Let {{mvar|C}} denote the cost of creation of the BST. We have <math>C=\\sum_i \\sum_{j<i} c_{i,j}</math>, where <math>c_{i,j}</math> is an binary random variable expressing whether during the insertion of <math>x_i</math> there was a comparison to <math>x_j</math>.\n\nBy [[Expected value#Linearity|linearity of expectation]], the expected value <math>\\operatorname{E}[C]</math> of {{mvar|C}} is <math>\\operatorname{E}[C]= \\sum_i \\sum_{j<i} \\Pr(c_{i,j})</math>.\n\nFix {{mvar|i}} and {{math|''j''<''i''}}. The values <math>{x_1,x_2,\\ldots,x_j}</math>, once sorted, define {{math|''j''+1}} intervals. The core structural observation is that <math>x_i</math> is compared to <math>x_j</math> in the algorithm if and only if <math>x_i</math> falls inside one of the two intervals adjacent to <math>x_j</math>.\n\nObserve that since <math>(x_1,x_2,\\ldots,x_n)</math> is a random permutation, <math>(x_1,x_2,\\ldots,x_j,x_i)</math> is also a random permutation, so the probability that <math>x_i</math> is adjacent to <math>x_j</math> is exactly <math>\\frac{2}{j+1}</math>.\n\nWe end with a short calculation:\n\n: <math>\\operatorname{E}[C] = \\sum_i \\sum_{j<i} \\frac{2}{j+1} = O\\left(\\sum_i \\log i\\right)=O(n \\log n).</math>\n\n=== Space complexity ===\nThe space used by quicksort depends on the version used.\n\nThe in-place version of quicksort has a space complexity of {{math|''O''(log ''n'')}}, even in the worst case, when it is carefully implemented using the following strategies:\n* in-place partitioning is used. This unstable partition requires {{math|''O''(1)}} space.\n* After partitioning, the partition with the fewest elements is (recursively) sorted first, requiring at most {{math|''O''(log ''n'')}} space. Then the other partition is sorted using [[tail recursion]] or iteration, which doesn't add to the call stack. This idea, as discussed above, was described by [[Robert Sedgewick (computer scientist)|R. Sedgewick]], and keeps the stack depth bounded by {{math|''O''(log ''n'')}}.<ref name=sedgewickBook>{{cite book | last=Sedgewick | first=Robert|authorlink=Robert Sedgewick (computer scientist) | title=Algorithms in C: Fundamentals, Data Structures, Sorting, Searching, Parts 1–4 | url=https://books.google.com/books?id=ylAETlep0CwC | accessdate=27 November 2012 | edition=3 | date=1 September 1998 | publisher=Pearson Education | isbn=978-81-317-1291-7}}</ref><ref name=sedgewickQsortPaper>{{Cite journal | last1 = Sedgewick | first1 = R. | authorlink1 = Robert Sedgewick (computer scientist) | title = Implementing Quicksort programs | doi = 10.1145/359619.359631 | journal = [[Communications of the ACM|Comm. ACM]] | volume = 21 | issue = 10 | pages = 847–857 | year = 1978 | pmid = | pmc = }}</ref>\n\nQuicksort with in-place and unstable partitioning uses only constant additional space before making any recursive call. Quicksort must store a constant amount of information for each nested recursive call. Since the best case makes at most {{math|''O''(log ''n'')}} nested recursive calls, it uses {{math|''O''(log ''n'')}} space. However, without Sedgewick's trick to limit the recursive calls, in the worst case quicksort could make {{math|''O''(''n'')}} nested recursive calls and need {{math|''O''(''n'')}} auxiliary space.\n\nFrom a bit complexity viewpoint, variables such as ''lo'' and ''hi'' do not use constant space; it takes {{math|''O''(log ''n'')}} bits to index into a list of {{mvar|n}} items. Because there are such variables in every stack frame, quicksort using Sedgewick's trick requires {{math|''O''((log ''n'')²)}} bits of space. This space requirement isn't too terrible, though, since if the list contained distinct elements, it would need at least {{math|''O''(''n'' log ''n'')}} bits of space.\n\nAnother, less common, not-in-place, version of quicksort uses {{math|''O''(''n'')}} space for working storage and can implement a stable sort. The working storage allows the input array to be easily partitioned in a stable manner and then copied back to the input array for successive recursive calls. Sedgewick's optimization is still appropriate.\n\n== Relation to other algorithms ==\nQuicksort is a space-optimized version of the [[binary tree sort]]. Instead of inserting items sequentially into an explicit tree, quicksort organizes them concurrently into a tree that is implied by the recursive calls. The algorithms make exactly the same comparisons, but in a different order. An often desirable property of a [[sorting algorithm]] is stability – that is the order of elements that compare equal is not changed, allowing controlling order of multikey tables (e.g. directory or folder listings) in a natural way. This property is hard to maintain for in situ (or in place) quicksort (that uses only constant additional space for pointers and buffers, and {{nowrap|''O''(log ''n'')}} additional space for the management of explicit or implicit recursion). For variant quicksorts involving extra memory due to representations using pointers (e.g. lists or trees) or files (effectively lists), it is trivial to maintain stability. The more complex, or disk-bound, data structures tend to increase time cost, in general making increasing use of virtual memory or disk.\n\nThe most direct competitor of quicksort is [[heapsort]]. Heapsort's running time is {{nowrap|''O''(''n'' log ''n'')}}, but heapsort's average running time is usually considered slower than in-place quicksort. This result is debatable; some publications indicate the opposite.<ref>{{cite web | first=Paul | last=Hsieh | title=Sorting revisited. | url=http://www.azillionmonkeys.com/qed/sort.html | year=2004 | accessdate=26 April 2010 | publisher = www.azillionmonkeys.com}}</ref><ref>{{cite web | first=David | last=MacKay | title=Heapsort, Quicksort, and Entropy | url=http://users.aims.ac.za/~mackay/sorting/sorting.html | date=1 December 2005 | accessdate=26 April 2010 | publisher=users.aims.ac.za/~mackay | archive-url=https://web.archive.org/web/20090401163041/http://users.aims.ac.za/~mackay/sorting/sorting.html | archive-date=1 April 2009 | dead-url=yes | df=dmy-all }}</ref> [[Introsort]] is a variant of quicksort that switches to heapsort when a bad case is detected to avoid quicksort's worst-case running time.\n\nQuicksort also competes with [[merge sort]], another {{nowrap|''O''(''n'' log ''n'')}} sorting algorithm. Mergesort is a [[stable sort]], unlike standard in-place quicksort and heapsort, and can be easily adapted to operate on [[linked list]]s and very large lists stored on slow-to-access media such as [[disk storage]] or [[network-attached storage]]. Although quicksort can be implemented as a stable sort using linked lists, it will often suffer from poor pivot choices without random access. The main disadvantage of mergesort is that, when operating on arrays, efficient implementations require ''O''(''n'') auxiliary space, whereas the variant of quicksort with in-place partitioning and tail recursion uses only {{nowrap|''O''(log ''n'')}} space. (Note that when operating on linked lists, mergesort only requires a small, constant amount of auxiliary storage.)\n\n[[Bucket sort]] with two buckets is very similar to quicksort; the pivot in this case is effectively the value in the middle of the value range, which does well on average for uniformly distributed inputs.\n\n=== Selection-based pivoting ===\nA [[selection algorithm]] chooses the ''k''th smallest of a list of numbers; this is an easier problem in general than sorting. One simple but effective selection algorithm works nearly in the same manner as quicksort, and is accordingly known as [[quickselect]]. The difference is that instead of making recursive calls on both sublists, it only makes a single tail-recursive call on the sublist that contains the desired element. This change lowers the average complexity to linear or ''O''(''n'') time, which is optimal for selection, but the sorting algorithm is still ''O''(''n''<sup>2</sup>).\n\nA variant of quickselect, the [[median of medians]] algorithm, chooses pivots more carefully, ensuring that the pivots are near the middle of the data (between the 30th and 70th percentiles), and thus has guaranteed linear time – ''O''(''n''). This same pivot strategy can be used to construct a variant of quicksort (median of medians quicksort) with {{nowrap|''O''(''n'' log ''n'')}} time. However, the overhead of choosing the pivot is significant, so this is generally not used in practice.\n\nMore abstractly, given an ''O''(''n'') selection algorithm, one can use it to find the ideal pivot (the median) at every step of quicksort and thus produce a sorting algorithm with {{nowrap|''O''(''n'' log ''n'')}} running time. Practical implementations this variant are considerably slower on average, but they are of theoretical interest because they show an optimal selection algorithm can yield an optimal sorting algorithm.\n\n=== Variants ===\n; Multi-pivot quicksort\n: Instead of partitioning into two subarrays using a single pivot, multi-pivot quicksort (also multiquicksort{{r|LaMarca1999}}) partitions its input into some {{mvar|s}} number of subarrays using {{math|''s'' − 1}} pivots. While the dual-pivot case ({{math|''s'' {{=}} 3}}) was considered by Sedgewick and others already in the mid-1970s, the resulting algorithms were not faster in practice than the \"classical\" quicksort.<ref>{{cite conference | title=Average case analysis of Java 7's dual pivot quicksort | first1=Sebastian | last1=Wild | first2=Markus E. | last2=Nebel | year=2012 |conference=European Symposium on Algorithms |arxiv=1310.7409| bibcode=2013arXiv1310.7409W }}</ref> A 1999 assessment of a multiquicksort with a variable number of pivots, tuned to make efficient use of processor caches, found it to increase the instruction count by some 20%, but simulation results suggested that it would be more efficient on very large inputs.{{r|LaMarca1999}} A version of dual-pivot quicksort developed by Yaroslavskiy in 2009<ref name=\":0\" /> turned out to be fast enough to warrant implementation in [[Java version history#Java SE 7 (July 28, 2011)|Java 7]], as the standard algorithm to sort arrays of [[primitive data type|primitives]] (sorting arrays of [[Object (computer science)|objects]] is done using [[Timsort]]).<ref>{{cite web | publisher=Oracle | title=Arrays | website=Java Platform SE 7 | url=http://docs.oracle.com/javase/7/docs/api/java/util/Arrays.html#sort%28byte%5B%5D%29 | accessdate=4 September 2014}}</ref> The performance benefit of this algorithm was subsequently found to be mostly related to cache performance, and experimental results indicate that the three-pivot variant may perform even better on modern machines.<ref>{{Cite conference | title=Multi-Pivot Quicksort: Theory and Experiments | first1=Shrinu | last1=Kushagra | first2=Alejandro | last2=López-Ortiz | first3=Aurick | last3=Qiao | first4=J. Ian | last4=Munro | doi=10.1137/1.9781611973198.6 | year=2014 |conference=Proc. Workshop on Algorithm Engineering and Experiments (ALENEX)}}</ref>\n\n; External quicksort\n: For magnetic tape files is the same as regular quicksort except the pivot is replaced by a buffer. First, the M/2 first and last elements are read into the buffer and sortedThen the next element from the beginning or end is read to balance writing. If the next element is less than the least of the buffer, write it to available space at the beginning. If greater than the greatest, write it to the end. Otherwise write the greatest or least of the buffer, and put the next element in the buffer. Keep the maximum lower and minimum upper keys written to avoid resorting middle elements that are in order. When done, write the buffer. Recursively sort the smaller partition, and loop to sort the remaining partition. This is a kind of three-way quicksort in which the middle partition (buffer) represents a sorted subarray of elements that are approximately equal to the pivot.\n\n; Three-way radix quicksort\n{{Main article|Multi-key quicksort}}\n: This algorithm is a combination of [[radix sort]] and quicksort. Pick an element from the array (the pivot) and consider the first character (key) of the string (multikey). Partition the remaining elements into three sets: those whose corresponding character is less than, equal to, and greater than the pivot's character. Recursively sort the \"less than\" and \"greater than\" partitions on the same character. Recursively sort the \"equal to\" partition by the next character (key). Given we sort using bytes or words of length W bits, the best case is O(KN) and the worst case O(2<sup>K</sup>N) or at least O(N<sup>2</sup>) as for standard quicksort, given for unique keys N<2<sup>K</sup>, and K is a hidden constant in all standard [[comparison sort]] algorithms including quicksort. This is a kind of three-way quicksort in which the middle partition represents a (trivially) sorted subarray of elements that are ''exactly'' equal to the pivot.\n\n; Quick radix sort\n: Also developed by Powers as an o(K) parallel [[parallel random-access machine|PRAM]] algorithm. This is again a combination of [[radix sort]] and quicksort but the quicksort left/right partition decision is made on successive bits of the key, and is thus O(KN) for N K-bit keys. Note that all [[comparison sort]] algorithms effectively assume an ideal K of O(logN) as if k is smaller we can sort in O(N) using a hash table or [[integer sorting]], and if K >> logN but elements are unique within O(logN) bits, the remaining bits will not be looked at by either quicksort or quick radix sort, and otherwise all comparison sorting algorithms will also have the same overhead of looking through O(K) relatively useless bits but quick radix sort will avoid the worst case O(N<sup>2</sup>) behaviours of standard quicksort and radix quicksort, and will be faster even in the best case of those comparison algorithms under these conditions of uniqueprefix(K) >> logN. See Powers<ref>David M. W. Powers, [http://david.wardpowers.info/Research/AI/papers/199501-ACAW-PUPC.pdf Parallel Unification: Practical Complexity], Australasian Computer Architecture Workshop, Flinders University, January 1995</ref> for further discussion of the hidden overheads in comparison, radix and parallel sorting.\n\n; Partial and incremental quicksort\n{{Main article|Partial sorting}}\n: Several variants of quicksort exist that separate the {{mvar|k}} smallest or largest elements from the rest of the input.\n\n=== Generalization ===\nRichard Cole and David C. Kandathil, in 2004, discovered a one-parameter family of sorting algorithms, called partition sorts, which on average (with all input orderings equally likely) perform at most <math>n\\log n + {O}(n)</math> comparisons (close to the information theoretic lower bound) and <math>{\\Theta}(n\\log n)</math> operations; at worst they perform <math>{\\Theta}(n\\log^2 n)</math> comparisons (and also operations); these are in-place, requiring only additional <math>{O}(\\log n)</math> space. Practical efficiency and smaller variance in performance were demonstrated against optimised quicksorts (of [[Robert Sedgewick (computer scientist)|Sedgewick]] and [[Jon Bentley (computer scientist)|Bentley]]-[[Douglas McIlroy|McIlroy]]).<ref>Richard Cole, David C. Kandathil: [http://www.cs.nyu.edu/cole/papers/part-sort.pdf \"The average case analysis of Partition sorts\"], European Symposium on Algorithms, 14–17 September 2004, Bergen, Norway. Published: ''Lecture Notes in Computer Science'' 3221, Springer Verlag, pp. 240–251.</ref>\n\n== See also ==\n{{Portal|Computer programming}}\n* {{annotated link|Introsort}}\n* {{annotated link|Flashsort}}\n\n== Notes ==\n{{Reflist|30em}}\n\n== References ==\n* {{Cite journal | last1 = Sedgewick | first1 = R. | authorlink1 = Robert Sedgewick (computer scientist) | title = Implementing Quicksort programs | doi = 10.1145/359619.359631 | journal = [[Communications of the ACM|Comm. ACM]] | volume = 21 | issue = 10 | pages = 847–857 | year = 1978 | pmid = | pmc = }}\n* {{Cite journal | last1 = Dean | first1 = B. C. | doi = 10.1016/j.dam.2005.07.005 | title = A simple expected running time analysis for randomized 'divide and conquer' algorithms | journal = Discrete Applied Mathematics | volume = 154 | pages = 1–5 | year = 2006 | pmid = | pmc = }}\n* {{Cite journal | last1 = Hoare | title = Algorithm 63: Partition | doi = 10.1145/366622.366642 | first1 = C. A. R. | authorlink1 = Tony Hoare | journal = [[Communications of the ACM|Comm. ACM]] | volume = 4 | issue = 7 | pages = 321 | year = 1961 }}\n* {{Cite journal | last1 = Hoare | title = Algorithm 65: Find | doi = 10.1145/366622.366647 | first1 = C. A. R. | authorlink1 = Tony Hoare | journal = [[Communications of the ACM|Comm. ACM]] | volume = 4 | issue = 7 | pages = 321–322 | year = 1961  }}\n* {{Cite journal | last1 = Hoare | first1 = C. A. R. | authorlink1 = Tony Hoare | doi = 10.1093/comjnl/5.1.10 | title = Quicksort | journal = [[The Computer Journal|Comput. J.]] | volume = 5 | issue = 1 | pages = 10–16 | year = 1962 | pmid = | pmc = }} (Reprinted in Hoare and Jones: [http://portal.acm.org/citation.cfm?id=SERIES11430.63445 ''Essays in computing science''], 1989.)\n* {{Cite journal | last = Musser | first = David R. | authorlink = David Musser | title = Introspective Sorting and Selection Algorithms | url = http://www.cs.rpi.edu/~musser/gp/introsort.ps | doi = 10.1002/(SICI)1097-024X(199708)27:8<983::AID-SPE117>3.0.CO;2-# | journal = Software: Practice and Experience | volume = 27 | issue = 8 | pages = 983–993 | year = 1997}}\n* [[Donald Knuth]]. ''The Art of Computer Programming'', Volume 3: ''Sorting and Searching'', Third Edition. Addison-Wesley, 1997. {{ISBN|0-201-89685-0}}. Pages 113–122 of section 5.2.2: Sorting by Exchanging.\n* [[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. ''[[Introduction to Algorithms]]'', Second Edition. [[MIT Press]] and [[McGraw-Hill]], 2001. {{ISBN|0-262-03293-7}}. Chapter 7: Quicksort, pp.&nbsp;145–164.\n* [[Faron Moller]]. [http://www.cs.swan.ac.uk/~csfm/Courses/CS_332/quicksort.pdf Analysis of Quicksort]. CS 332: Designing Algorithms. Department of Computer Science, [[Swansea University]].\n* {{Cite journal | last1 = Martínez | first1 = C. | last2 = Roura | first2 = S. | doi = 10.1137/S0097539700382108 | title = Optimal Sampling Strategies in Quicksort and Quickselect | journal = [[SIAM Journal on Computing|SIAM J. Comput.]] | volume = 31 | issue = 3 | pages = 683–705 | year = 2001 | pmid = | pmc = | citeseerx = 10.1.1.17.4954 }}\n* {{Cite journal | last1 = Bentley | first1 = J. L. | last2 = McIlroy | first2 = M. D. | doi = 10.1002/spe.4380231105 | title = Engineering a sort function | journal = Software: Practice and Experience | volume = 23 | issue = 11 | pages = 1249–1265 | year = 1993 | pmid = | pmc = | citeseerx = 10.1.1.14.8162 }}\n\n== External links ==\n{{Wikibooks|Algorithm implementation|Sorting/Quicksort|Quicksort}}\n* {{Cite web |url=http://www.sorting-algorithms.com/quick-sort |title=Animated Sorting Algorithms: Quick Sort |access-date=25 November 2008 |archive-url=https://web.archive.org/web/20150302145415/http://www.sorting-algorithms.com/quick-sort |archive-date=2 March 2015 |dead-url=bot: unknown |df=dmy-all }} – graphical demonstration\n* {{Cite web |url=http://www.sorting-algorithms.com/quick-sort-3-way |title=Animated Sorting Algorithms: Quick Sort (3-way partition) |access-date=25 November 2008 |archive-url=https://web.archive.org/web/20150306071949/http://www.sorting-algorithms.com/quick-sort-3-way |archive-date=6 March 2015 |dead-url=bot: unknown |df=dmy-all }}\n* [http://opendatastructures.org/versions/edition-0.1e/ods-java/11_1_Comparison_Based_Sorti.html#SECTION001412000000000000000 Open Data Structures – Section 11.1.2 – Quicksort]\n* [https://web.archive.org/web/20180629183103/http://www.tomgsmith.com/quicksort/content/illustration/ Interactive illustration of Quicksort], with code walkthrough\n\n{{Sorting}}\n\n[[Category:1961 in computer science]]\n[[Category:Articles with example pseudocode]]\n[[Category:Comparison sorts]]\n[[Category:Sorting algorithms]]"
    },
    {
      "title": "Radix sort",
      "url": "https://en.wikipedia.org/wiki/Radix_sort",
      "text": "{{Infobox Algorithm\n|class=[[Sorting algorithm]]\n|image=\n|data=[[Array data type|Array]]\n|time=<math>O(w\\cdot n)</math>, where w is the number of bits required to store each key.\n|space=<math>O(w+n)</math>\n|optimal=exactly correct\n}}\nIn [[computer science]], '''radix sort''' is a non-[[Comparison sort|comparative]] [[integer sorting|integer]] [[sorting algorithm]] that sorts data with integer keys by grouping keys by the individual digits which share the same [[Significant figures|significant]] position and value.  A [[positional notation]] is required, but because integers can represent strings of characters (e.g., names or dates) and specially formatted floating point numbers, [[radix]] sort is not limited to integers.  Radix sort dates back as far as 1887 to the work of [[Herman Hollerith]] on [[tabulating machines]].<ref>{{Cite patent|US|395781}} and {{Cite patent|UK|327}}</ref>\n\nMost digital computers internally represent all of their data as electronic representations of binary numbers, so processing the digits of integer representations by groups of binary digit representations is most convenient.  Radix sorts can be implemented to start at either the [[most significant digit]] (MSD) or [[least significant digit]] (LSD). For example, when sorting the number 1234 into a list, one could start with the 1 or the 4.\n\nLSD radix sorts typically use the following sorting order: short keys come before longer keys, and then keys of the same length are sorted [[lexicographically]].  This coincides with the normal order of integer representations, such as the sequence 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11.\n\nMSD radix sorts use lexicographic order, which is suitable for sorting strings, such as words, or fixed-length integer representations.  A sequence such as \"b, c, d, e, f, g, h, i, j, ba\" would be lexicographically sorted as \"b, ba, c, d, e, f, g, h, i, j\".  If lexicographic ordering is used to sort variable-length integer representations, then the representations of the numbers from 1 to 10 would be output as 1, 10, 2, 3, 4, 5, 6, 7, 8, 9, as if the shorter keys were left-justified and padded on the right with blank characters to make the shorter keys as long as the longest key for the purpose of determining sorted order.\n[[File:SEACComputer 038.jpg|thumb|An [[IBM card sorter]] performing a radix sort on a large set of [[punched cards]]. Cards are fed into a hopper below the operator's chin and are sorted into one of the machine's 13 output baskets, based on the data punched into one column on the cards. The crank near the input hopper is used to move the read head to the next column as the sort progresses. The rack in back holds cards from the previous sorting pass.]]\n\n==Efficiency==\n\nThe topic of the efficiency of radix sort compared to other sorting algorithms is somewhat tricky and subject to quite a lot of misunderstandings. Whether radix sort is equally efficient, less efficient or more efficient than the best [[Comparison sort|comparison-based algorithms]] depends on the details of the assumptions made. Radix sort complexity is {{math|''O''(''wn'')}} for {{mvar|n}} keys which are integers of [[word size]] {{mvar|w}}. Sometimes {{mvar|w}} is presented as a constant, which would make radix sort better (for sufficiently large {{mvar|n}}) than the best comparison-based sorting algorithms, which all perform {{math|''Θ''(''n'' log ''n'')}} comparisons to sort {{mvar|n}} keys. \n\nThe counter argument is that comparison-based algorithms are measured in number of comparisons, not actual time complexity.\nUnder some assumptions the comparisons will be constant time on average, under others they will not. Comparisons of randomly generated keys takes constant time on average, as keys differ on the very first [[bit]] in half the cases, and differ on the second bit in half of the remaining half, and so on, resulting in an average of two bits that need to be compared. In a sorting algorithm the first comparisons made satisfies the randomness condition, but as the sort progresses the keys compared are clearly not randomly chosen anymore. For example, consider a bottom-up merge sort. The first pass will compare pairs of random keys, but the last pass will compare keys that are very close in the sorting order.  This makes merge sort, on this class of inputs, take {{math|''Ω''(''n'' (log ''n'')<sup>2</sup>)}} time.  That assumes all memory accesses cost the same, which is not a physically reasonable assumption as we scale {{mvar|n}} to infinity, and not, in practice, how real computers work. This argument extends from the observation that computers are filled with different types of memory (cache, system memory, virtual memory etc.) in different and limited quantities.  Modern operating systems will position variables into these different systems automatically making memory access time differ widely as more and more memory is utilized.\n\n==Least significant digit radix sorts==\nA [[Least significant digit]] (LSD) Radix sort is a fast [[Stable sort|stable]] [[sorting algorithm]] which can be used to sort keys in integer representation order. Keys may be a [[string (computer science)|string]] of characters, or numerical digits in a given 'radix'.  The processing of the keys begins at the least significant digit (i.e., the rightmost digit), and proceeds to the [[most significant digit]] (i.e., the leftmost digit).  The sequence in which digits are processed by an LSD radix sort is the opposite of the sequence in which digits are processed by a most significant digit (MSD) radix sort.\n\nAn LSD radix sort operates in [[big O notation|O]](''nw'') time, where ''n'' is the number of keys, and ''w'' is the average key length.  This kind of performance for variable-length keys can be achieved by grouping all of the keys that have the same length together and separately performing an LSD radix sort on each group of keys for each length, from shortest to longest, in order to avoid processing the whole list of keys on every sorting pass.\n\nA radix sorting algorithm was originally used to sort [[punched card]]s in several passes.  A computer algorithm was invented for radix sort in 1954 at [[Massachusetts Institute of Technology|MIT]] by [[Harold H. Seward]]. In many large applications needing speed, the computer radix sort is an improvement on (slower) comparison sorts.\n\nLSD radix sorts have resurfaced as an alternative to high performance [[comparison sort|comparison-based sorting algorithms]] (like [[heapsort]] and [[merge sort]]) that require Ω(''n'' · log ''n'') comparisons, where ''n'' is the number of items to be sorted. [[Comparison sort]]s can do no better than Ω(''n'' · log ''n'') execution time but offer the flexibility of being able to sort with respect to more complicated orderings than a lexicographic one; however, this ability is of little importance in many practical applications.\n\n===Definition===\nEach key is first figuratively dropped into one level of buckets corresponding to the value of the rightmost digit.  Each bucket preserves the original order of the keys as the keys are dropped into the bucket.  There is a one-to-one correspondence between the buckets and the values that can be represented by a digit plus one bucket for the [[Empty string|empty digit]], which signifies that the string is exhausted and which is not required if all strings are of same length.  Then, the process repeats with the next neighbouring more significant digit until there are no more digits to process.  In other words:\n#Take the least significant digit (or group of bits, both being examples of [[radix|radices]]) of each key.\n#Group the keys based on that digit, but otherwise keep the original order of keys. (From the second step on this is required for the algorithm. If applied generally it makes the LSD radix sort a [[stable sort]].)\n#Collect the groups (buckets) in the order of the digits and repeat the grouping process with each more significant digit.\n<!-- Entire paragraph implementation-dependent according to Base -->\n\n===An example===\n\nOriginal, unsorted list:\n\n:170, 45, 75, 90, 02, 802, 2, 66\n\nInterpreting as nonnegative [[decimal]] numbers with leading zeros possibly suppressed\nand sorting least significant digit (1's place) gives:\n\n:17<u>0</u>, 9<u>0</u>, 0<u>2</u>, 80<u>2</u>, <u>2</u>, 4<u>5</u>, 7<u>5</u>, 6<u>6</u>\n\n:<small>Notice that we keep 802 before 2, because 802 occurred before 2 in the original list, and similarly for pair 170 & 90.</small>\n\nSorting by next digit (10's place) gives:\n\n:<u>0</u>2, 8<u>0</u>2, <u>''0''</u>2, <u>4</u>5, <u>6</u>6, 1<u>7</u>0, <u>7</u>5, <u>9</u>0\n\n:<small>Notice that a ''0''-digit is “generated” in front of the 2 for the missing digit, that 802 again comes before the second ''0''2 as 802 comes before 2 in the previous list.</small>\n\nSorting by most significant digit (100's place) gives:\n\n:<u>''0''</u>02, <u>''0''</u>02, <u>''0''</u>45, <u>''0''</u>66, <u>''0''</u>75, <u>''0''</u>90, <u>1</u>70, <u>8</u>02\n\nIt is important to realize that each of the above steps requires just a single pass over the data, since each item can be placed in its correct bucket without having to be compared with other items.\n\nSome radix sort implementations allocate space for buckets by first counting the number of keys that belong in each bucket before moving keys into those buckets.  The number of times that each digit occurs is stored in an [[Array data type|array]].\n\nConsider the previous list of keys viewed as fixed-length decimals of length 3:\n\n:170, 045, 075, 090, 002, 802, 002, 066\n\nThe first counting pass starts on the least significant digit of each key, producing an array of bucket sizes:\n\n:2 (bucket size for digits of 0: 17<u>0</u>, 09<u>0</u>)\n:3 (bucket size for digits of 2: 00<u>2</u>, 80<u>2</u>, 00<u>2</u>)\n:2 (bucket size for digits of 5: 04<u>5</u>, 07<u>5</u>)\n:1 (bucket size for digits of 6: 06<u>6</u>)\n\nA second counting pass on the next more significant digit of each key will produce an array of bucket sizes:\n\n:3 (bucket size for digits of 0: 0<u>0</u>2, 8<u>0</u>2, 0<u>0</u>2)\n:1 (bucket size for digits of 4: 0<u>4</u>5)\n:1 (bucket size for digits of 6: 0<u>6</u>6)\n:2 (bucket size for digits of 7: 1<u>7</u>0, 0<u>7</u>5)\n:1 (bucket size for digits of 9: 0<u>9</u>0)\n\nA third and final counting pass on the most significant digit of each key will produce an array of bucket sizes:\n\n:6 (bucket size for digits of 0: <u>0</u>02, <u>0</u>02, <u>0</u>45, <u>0</u>66, <u>0</u>75, <u>0</u>90)\n:1 (bucket size for digits of 1: <u>1</u>70)\n:1 (bucket size for digits of 8: <u>8</u>02)\n\nAt least one LSD radix sort implementation now counts the number of times that each digit occurs in each column for all columns in a single counting pass. (See the [[Radix sort#External links|external links]] section.) Other LSD radix sort implementations allocate space for buckets dynamically as the space is needed.\n\n===Iterative version using queues===\nA simple version of an LSD radix sort can be achieved using [[Queue (data structure)|queues]] as buckets. The following process is repeated for a number of times equal to the length of the longest key:\n#The integers are enqueued into an array of ten separate queues based on their digits from right to left.  Computers often represent integers internally as fixed-length binary digits.  Here, we will do something analogous with fixed-length decimal digits.  So, using the numbers from the previous example, the queues for the 1st pass would be:\n#:0: 17<u>0</u>, 09<u>0</u>\n#:1: none\n#:2: 00<u>2</u>, 80<u>2</u>, 00<u>2</u>\n#:3: none\n#:4: none\n#:5: 04<u>5</u>, 07<u>5</u>\n#:6: 06<u>6</u>\n#:7–9: none\n#The queues are dequeued back into an array of integers, in increasing order. Using the same numbers, the array will look like this after the first pass:\n#:170, 090, 002, 802, 002, 045, 075, 066\n#For the second pass:\n#:Queues:\n#::0: 0<u>0</u>2, 8<u>0</u>2, 0<u>0</u>2\n#::1–3: none\n#::4: 0<u>4</u>5\n#::5: none\n#::6: 0<u>6</u>6\n#::7: 1<u>7</u>0, 0<u>7</u>5\n#::8: none\n#::9: 0<u>9</u>0\n#:Array:\n#::002, 802, 002, 045, 066, 170, 075, 090<br />(note that at this point only 802 and 170 are out of order)\n#For the third pass:\n#:Queues:\n#::0: <u>0</u>02, <u>0</u>02, <u>0</u>45, <u>0</u>66, <u>0</u>75, <u>0</u>90\n#::1: <u>1</u>70\n#::2–7: none\n#::8: <u>8</u>02\n#::9: none\n#:Array:\n#::002, 002, 045, 066, 075, 090, 170, 802 (sorted)\nWhile this may not be the most efficient radix sort algorithm, it is relatively simple, and still quite efficient.\n\n==Most significant digit radix sorts==\nA [[most significant digit]] (MSD) radix sort can be used to sort keys in [[lexicographic order]].  Unlike a least significant digit (LSD) radix sort, a most significant digit radix sort [[Stable sort|does not necessarily preserve the original order of duplicate keys]].  An MSD radix sort starts processing the keys from the [[most significant digit]], leftmost digit, to the [[least significant digit]], rightmost digit.  This sequence is opposite that of [[least significant digit]] (LSD) radix sorts.  An MSD radix sort stops rearranging the position of a key when the processing reaches a unique prefix of the key.  Some MSD radix sorts use one level of buckets in which to group the keys.  See the [[counting sort]] and [[pigeonhole sort]] articles.  Other MSD radix sorts use multiple levels of buckets, which form a [[trie]] or a path in a trie.  A [[Bucket sort#Postman's sort|postman's sort / postal sort]] is a kind of MSD radix sort.\n\n===Recursion===\nA [[recursion|recursively]] subdividing MSD radix sort algorithm works as follows:\n#Take the most significant digit of each key.\n#Sort the list of elements based on that digit, grouping elements with the same digit into one [[bucket (computing)|bucket]].\n#Recursively sort each bucket, starting with the next digit to the right.\n#[[Concatenate]] the buckets together in order.\n\n===Recursive forward radix sort example===\nSort the list:\n<br>170, 045, 075, 090, 002, 024, 802, 066\n#Sorting by most significant digit (100s place) gives:<br> Zero hundreds bucket: <u>0</u>45, <u>0</u>75, <u>0</u>90, <u>0</u>02, <u>0</u>24, <u>0</u>66 <br> One hundreds bucket: <u>1</u>70 <br> Eight hundreds bucket: <u>8</u>02\n#Sorting by next digit (10s place) is only needed for those numbers in the zero hundreds bucket (no other buckets contain more than one item):<br> Zero tens bucket: 0<u>0</u>2 <br> Twenties bucket: 0<u>2</u>4 <br> Forties bucket: 0<u>4</u>5 <br> Sixties bucket: 0<u>6</u>6 <br> Seventies  bucket: 0<u>7</u>5 <br> Nineties bucket: 0<u>9</u>0\nSorting by least significant digit (1s place) is not needed, as there is no tens bucket with more than one number.  Therefore, the now sorted zero hundreds bucket is concatenated, joined in sequence, with the one hundreds bucket and eight hundreds bucket to give:<br>002, 024, 045, 066, 075, 090, 170, 802\n\nThis example used [[base (exponentiation)|base]] ten digits for the sake of readability, but of course binary digits or perhaps [[byte]]s might make more sense for a binary computer to process.\n\n===In-place MSD radix sort implementations===\nBinary MSD radix sort, also called binary quicksort, can be implemented in-place by splitting the input array into two bins - the 0s bin and the 1s bin.  The 0s bin is grown from the beginning of the array, whereas the 1s bin is grown from the end of the array.  The 0s bin boundary is placed before the first array element.  The 1s bin boundary is placed after the last array element.  The most significant bit of the first array element is examined.  If this bit is a 1, then the first element is swapped with the element in front of the 1s bin boundary (the last element of the array), and the 1s bin is grown by one element by decrementing the 1s boundary array index.  If this bit is a 0, then the first element remains at its current location, and the 0s bin is grown by one element. The next array element examined is the one in front of the 0s bin boundary (i.e. the first element that is not in the 0s bin or the 1s bin).  This process continues until the 0s bin and the 1s bin reach each other. The 0s bin and the 1s bin are then sorted recursively based on the next bit of each array element.  Recursive processing continues until the least significant bit has been used for sorting.<ref>R. Sedgewick, \"Algorithms in C++\", third edition, 1998, p. 424-427</ref><ref>[http://www.drdobbs.com/architecture-and-design/220300654 V. J. Duvanenko, \"In-Place Hybrid Binary-Radix Sort\", Dr. Dobb's Journal, 1 October 2009]</ref>  Handling signed integers requires treating the most significant bit with the opposite sense, followed by unsigned treatment of the rest of the bits.\n\nIn-place MSD binary-radix sort can be extended to larger radix and retain in-place capability.  [[Counting sort]] is used to determine the size of each bin and their starting index.  Swapping is used to place the current element into its bin, followed by expanding the bin boundary.  As the array elements are scanned the bins are skipped over and only elements between bins are processed, until the entire array has been processed and all elements end up in their respective bins.  The number of bins is the same as the radix used - e.g. 16 bins for 16-Radix.  Each pass is based on a single digit (e.g. 4-bits per digit in the case of 16-Radix), starting from the [[most significant digit]].  Each bin is then processed recursively using the next digit, until all digits have been used for sorting.<ref>[http://www.drdobbs.com/architecture-and-design/221600153 V. J. Duvanenko, \"In-Place Hybrid N-bit-Radix Sort\", Dr. Dobb's Journal, November 2009]</ref><ref>[http://www.drdobbs.com/high-performance-computing/229000734 V. J. Duvanenko, \"Parallel In-Place Radix Sort Simplified\", Dr. Dobb's Journal, January 2011]</ref>\n\nNeither in-place binary-radix sort nor n-bit-radix sort, discussed in paragraphs above, are [[Sorting algorithm|stable algorithms]].\n\n===Stable MSD radix sort implementations===\nMSD Radix Sort can be implemented as a stable algorithm, but requires the use of a memory buffer of the same size as the input array.  This extra memory allows the input buffer to be scanned from the first array element to last, and move the array elements to the destination bins in the same order.  Thus, equal elements will be placed in the memory buffer in the same order they were in the input array.  The MSD-based algorithm uses the extra memory buffer as the output on the first level of recursion, but swaps the input and output on the next level of recursion, to avoid the overhead of copying the output result back to the input buffer.  Each of the bins are recursively processed, as is done for the in-place MSD Radix Sort.  After the sort by the last digit has been completed, the output buffer is checked to see if it is the original input array, and if it's not, then a single copy is performed.  If the digit size is chosen such that the key size divided by the digit size is an even number, the copy at the end is avoided.<ref>[http://www.drdobbs.com/tools/222200161 V. J. Duvanenko, \"Stable Hybrid N-bit-Radix Sort\", Dr. Dobb's Journal, January 2010]</ref>\n\n===Hybrid approaches===\nRadix sort, such as two pass method where [[counting sort]] is used during the first pass of each level of recursion, has a large constant overhead.  Thus, when the bins get small, other sorting algorithms should be used, such as [[insertion sort]].  A good implementation of [[Insertion sort]] is fast for small arrays, stable, in-place, and can significantly speed up Radix Sort.\n\n===Application to parallel computing===\nNote that this recursive sorting algorithm has particular application to [[parallel computing]], as each of the bins can be sorted independently.  In this case, each bin is passed to the next available processor.  A single processor would be used at the start (the most significant digit). By the second or third digit, all available processors would likely be engaged.  Ideally, as each subdivision is fully sorted, fewer and fewer processors would be utilized.  In the worst case, all of the keys will be identical or nearly identical to each other, with the result that there will be little to no advantage to using parallel computing to sort the keys.\n\nIn the top level of recursion, opportunity for parallelism is in the [[counting sort]] portion of the algorithm.  Counting is highly parallel, amenable to the parallel_reduce pattern, and splits the work well across multiple cores until reaching memory bandwidth limit.  This portion of the algorithm has data-independent parallelism.  Processing each bin in subsequent recursion levels is data-dependent, however.  For example, if all keys were of the same value, then there would be only a single bin with any elements in it, and no parallelism would be available.  For random inputs all bins would be near equally populated and a large amount of parallelism opportunity would be available.<ref>[http://www.drdobbs.com/high-performance-computing/226600004 V. J. Duvanenko, \"Parallel In-Place N-bit-Radix Sort\", Dr. Dobb's Journal, August 2010]</ref>\n\nNote that there are faster sorting algorithms available, for example optimal complexity O(log(''n'')) are those of the Three Hungarians and Richard Cole<ref>A. Gibbons and [[Wojciech Rytter|W. Rytter]], ''Efficient Parallel Algorithms''. Cambridge University Press, 1988.</ref><ref>H. Casanova et al, ''Parallel Algorithms''. Chapman & Hall, 2008.</ref> and [[Ken Batcher|Batcher]]'s [[Bitonic sorter|bitonic merge sort]] has an algorithmic complexity of O(log<sup>2</sup>(''n'')), all of which have a lower algorithmic time complexity to radix sort on a CREW-[[Parallel random-access machine|PRAM]]. The fastest known PRAM sorts were described in 1991 by David Powers with a parallelized quicksort that can operate in O(log(n)) time on a CRCW-PRAM with ''n'' processors by performing partitioning implicitly, as well as a radixsort that operates using the same trick in O(''k''), where ''k'' is the maximum keylength.<ref>David M. W. Powers, [http://citeseer.ist.psu.edu/327487.html Parallelized Quicksort and Radixsort with Optimal Speedup], ''Proceedings of International Conference on Parallel Computing Technologies''. [[Novosibirsk]]. 1991.</ref> However, neither the PRAM architecture or a single sequential processor can actually be built in a way that will scale without the number of constant [[fan-out]] gate delays per cycle increasing as O(log(''n'')), so that in effect a pipelined version of Batcher's bitonic mergesort and the O(log(''n'')) PRAM sorts are all O(log<sup>2</sup>(''n'')) in terms of clock cycles, with Powers acknowledging that Batcher's would have lower constant in terms of gate delays than his Parallel [[quicksort]] and radix sort, or Cole's [[merge sort]], for a keylength-independent [[sorting network]] of O(nlog<sup>2</sup>(''n'')).<ref>David M. W. Powers, [http://david.wardpowers.info/Research/AI/papers/199501-ACAW-PUPC.pdf Parallel Unification: Practical Complexity], Australasian Computer Architecture Workshop, Flinders University, January 1995</ref>\n\n=== trie-based radix sort===\nAnother way to proceed with an MSD radix sort is to use more memory to create a [[trie]] to represent the keys and then traverse the trie to visit each key in order.  A [[depth-first search|depth-first traversal]] of a trie starting from the [[root node]] will visit each key in order.  A depth-first traversal of a trie, or any other kind of [[Directed acyclic graph|acyclic]] tree structure, is equivalent to traversing a maze via the [[Maze solving algorithm#Wall follower|right-hand rule]].\n\nA trie essentially represents a [[set (mathematics)|set]] of strings or numbers, and a radix sort which uses a trie structure is not necessarily stable, which means that the original order of duplicate keys is not necessarily preserved, because a set does not contain duplicate elements.  Additional information will have to be associated with each key to indicate the population count or original order of any duplicate keys in a trie-based radix sort if keeping track of that information is important for a particular application.  It may even be desirable to discard any duplicate strings as the trie creation proceeds if the goal is to find only unique strings in sorted order.  Some people sort a list of strings first and then make a separate pass through the sorted list to discard duplicate strings, which can be slower than using a trie to simultaneously sort and discard duplicate strings in one pass.\n\nOne of the advantages of maintaining the trie structure is that the trie makes it possible to determine quickly if a particular key is a member of the set of keys in a time that is proportional to the length of the key, ''k'', in O(''k'') time, that is ''independent'' of the total number of keys.  Determining set membership in a plain list, as opposed to determining set membership in a trie, requires [[binary search]], O(''k&thinsp;log(n)'') time; [[linear search]], O(''kn'') time; or some other method whose execution time is in some way <u>dependent</u> on the total number, ''n'', of all of the keys in the worst case.  It is sometimes possible to determine set membership in a plain list in O(''k'') time, in a time that is independent of the total number of keys, such as when the list is known to be in an [[arithmetic sequence]] or some other computable sequence.\n\nMaintaining the trie structure also makes it possible to insert new keys into the set incrementally or delete keys from the set incrementally while maintaining sorted order in O(''k'') time, in a time that is independent of the total number of keys.  In contrast, other radix sorting algorithms must, in the worst case, re-sort the entire list of keys each time that a new key is added or deleted from an existing list, requiring O(''kn'') time.\n\n====Snow White analogy====\n[[File:7dwarves.svg|center]]\nIf the nodes were rooms connected by hallways, then here is how Snow White might proceed to visit all of the dwarfs if the place were dark, keeping her right hand on a wall at all times:\n# She travels down hall B to find Bashful.\n# She continues moving forward with her right hand on the wall, which takes her around the room and back up hall B.\n# She moves down halls D, O, and C to find Doc.\n# Continuing to follow the wall with her right hand, she goes back up hall C, then down hall P, where she finds Dopey.\n# She continues back up halls P, O, D, and then goes down hall G to find Grumpy.\n# She goes back up hall G, with her right hand still on the wall, and goes down hall H to the room where Happy is.\n# She travels back up hall H and turns right down halls S and L, where she finds Sleepy.\n# She goes back up hall L, down hall N, where she finally finds Sneezy.\n# She travels back up halls N and S to her starting point and knows that she is done.\n\nThese series of steps serve to illustrate the path taken in the trie by Snow White via a [[depth-first search|depth-first traversal]] to visit the dwarfs by the ascending order of their names, Bashful, Doc, Dopey, Grumpy, Happy, Sleepy, and Sneezy.  The algorithm for performing some operation on the data associated with each node of a tree first, such as printing the data, and then moving deeper into the tree is called a [[pre-order traversal]], which is a kind of [[depth-first search|depth-first traversal]].  A pre-order traversal is used to process the contents of a trie in ascending order.  If Snow White wanted to visit the dwarfs by the descending order of their names, then she could walk backwards while following the wall with her right hand, or, alternatively, walk forward while following the wall with her left hand. The algorithm for moving deeper into a tree first until no further descent to unvisited nodes is possible and then performing some operation on the data associated with each node is called [[post-order traversal]], which is another kind of depth-first traversal.  A [[post-order traversal]] is used to process the contents of a trie in descending order.\n\nThe [[root node]] of the [[trie]] in the diagram essentially represents a null string, an empty string, which can be useful for keeping track of the number of blank lines in a list of words.  The null string can be associated with a circularly [[linked list]] with the null string initially as its only member, with the forward and backward pointers both initially pointing to the null string.  The circularly linked list can then be expanded as each new key is inserted into the [[trie]].  The circularly linked list is represented in the following diagram as thick, grey, horizontally linked lines:\n\n[[File:7dwarvesThreaded.svg|center]]\nIf a new key, other than the null string, is inserted into a [[leaf node]] of the [[trie]], then the computer can go to the last preceding node where there was a key or a bifurcation to perform a [[depth-first search]] to find the lexicographic successor or predecessor of the inserted key for the purpose of splicing the new key into the circularly [[linked list]].  The last preceding node where there was a key or a bifurcation, a fork in the path, is a [[parent node]] in the type of trie shown here, where only unique string prefixes are represented as paths in the trie.  If there is already a key associated with the parent node that would have been visited during a movement ''away'' from the root during a right-hand, forward-moving, depth-first traversal, then that immediately ends the depth-first search, as that key is the predecessor of the inserted key.  For example, if Bashful is inserted into the trie, then the predecessor is the null string in the parent node, which is the [[root node]] in this case.  In other words, if the key that is being inserted is on the leftmost branch of the parent node, then any string contained in the parent node is the lexicographic predecessor of the key that is being inserted, else the lexicographic predecessor of the key that is being inserted exists down the parent node's branch that is immediately to the left of the branch where the new key is being inserted.  For example, if Grumpy were the last key inserted into the trie, then the computer would have a choice of trying to find either the predecessor, Dopey, or the successor, Happy, with a [[depth-first search]] starting from the parent node of Grumpy.  With no additional information to indicate which path is longer, the computer might traverse the longer path, D, O, P.  If Dopey were the last key inserted into the trie, then the depth-first search starting from the parent node of Dopey would soon find the predecessor, \"Doc\", because that would be the only choice.\n\nIf a new key is inserted into an [[internal node]], then a depth-first search can be started from the [[internal node]] to find the lexicographic successor.  For example, if the literal string \"DO\" were inserted in the node at the end of the path D, O, then a depth-first search could be started from that internal node to find the successor, \"DOC\", for the purpose of splicing the new string into the circularly [[linked list]].\n\nForming the circularly linked list requires more memory but allows the keys to be visited more directly in either ascending or descending order via a linear traversal of the [[linked list]] rather than a [[depth-first search|depth-first traversal]] of the entire trie.  This concept of a circularly linked trie structure is similar to the concept of a [[threaded binary tree]].  This structure will be called a circularly threaded trie.\n\n[[File:Trie002.svg|center]]\n\nWhen a [[trie]] is used to sort numbers, the number representations must all be the same length unless you are willing to perform a [[breadth-first search|breadth-first traversal]].  When the number representations will be visited via [[depth-first search|depth-first traversal]], as in the above diagram, the number representations will always be on the [[leaf node]]s of the [[trie]].  Note how similar in concept this particular example of a trie is to the [[radix sort#Recursive forward radix sort example|recursive forward radix sort example]] which involves the use of buckets instead of a trie.  Performing a radix sort with the buckets is like creating a trie and then discarding the non-leaf nodes.\n\n==Implementation in Java==\n<source lang=\"java\">\n// Radix sort Java implementation \nimport java.io.*; \nimport java.util.*; \n  \nclass Radix { \n    // A utility function to get maximum value in arr[] \n    static int getMax(int arr[], int n) \n    { \n        int mx = arr[0]; \n        for (int i = 1; i < n; i++) \n            if (arr[i] > mx) \n                mx = arr[i]; \n        return mx; \n    } \n  \n    // A function to do counting sort of arr[] according to \n    // the digit represented by exp. (e.g. 300 is represented by 100)\n    static void countSort(int arr[], int n, int exp) \n    { \n        int output[] = new int[n]; // output array \n        int i; \n        int count[] = new int[10]; \n        Arrays.fill(count,0); \n  \n        // Store count of occurrences in count[] \n        for (i = 0; i < n; i++) \n            count[ (arr[i]/exp)%10 ]++; \n  \n        // Change count[i] so that count[i] now contains \n        // actual position of this digit in output[] \n        for (i = 1; i < 10; i++) \n            count[i] += count[i - 1]; \n  \n        // Build the output array \n        for (i = n - 1; i >= 0; i--) \n        { \n            output[count[ (arr[i]/exp)%10 ] - 1] = arr[i]; \n            count[ (arr[i]/exp)%10 ]--; \n        } \n  \n        // Copy the output array to arr[], so that arr[] now \n        // contains sorted numbers according to current digit \n        for (i = 0; i < n; i++) \n            arr[i] = output[i]; \n    } \n  \n    // The main function to that sorts arr[] of size n using \n    // Radix Sort \n    static void radixsort(int arr[], int n) \n    { \n        // Find the maximum number to know number of digits \n        int m = getMax(arr, n); \n  \n        // Do counting sort for every digit. Note that instead \n        // of passing digit number, exp is passed. exp is 10^i \n        // where i is current digit number \n        for (int exp = 1; m/exp > 0; exp *= 10) \n            countSort(arr, n, exp); \n    } \n  \n    // A utility function to print an array \n    static void print(int arr[], int n) \n    { \n        for (int i=0; i<n; i++) \n            System.out.print(arr[i]+\" \"); \n    } \n  \n  \n    /*Driver function to check for above function*/\n    public static void main (String[] args) \n    { \n        int arr[] = {170, 45, 75, 90, 802, 24, 2, 66}; \n        int n = arr.length; \n        radixsort(arr, n); \n        print(arr, n); \n    } \n}\n</source>\n\n==LSD Radix Sort Performance Optimization==\nIn the above LSD Radix Sort implementation counting is performed per digit, scanning over the entire input array per digit. For instance, when 8-bit digits are used with an array of 32-bit integers, this results in 4 digits per array element.\nIn this case, 4 passes over the array will be performed with 4 counts. These 4 passes can be combined into a single pass over the array, which counts not just based on the current digit, but the counts for all of the digits. The count array\nturns from a 1-D count array into a 2-D count array: a count array per digit. This optimization is possible for the LSD Radix Sort because the entire array is scanned/counted for each digit, and the permutation step (moving the array elements)\nwill not alter the count totals for the entire array.<ref>[https://duvanenko.tech.blog/2019/02/27/lsd-radix-sort-performance-improvements/ V. J. Duvanenko, \"Faster LSD Radix Sort\", February 2019]</ref><ref>[http://stereopsis.com/radix.html Michael Herf, \"Faster Floating Point Sorting and Multiple Histogramming\", December 2001]</ref> This performance optimization has been implemented in C# in an open source and free HPCsharp NuGet package/library. With this optimization, the LSD Radix Sort can be split into a counting phase and a permutation phase.\n\n==See also==\n* [[IBM 80 series Card Sorters]]\n\n==References==\n{{reflist|30em}}\n\n==External links==<!-- This section is linked from Radix sort -->\n{{wikibooks|Algorithm implementation|Sorting/Radix_sort|Radix sort}}\n*[https://duvanenko.tech.blog/2017/06/15/faster-sorting-in-javascript/ High Performance Implementation] of LSD Radix sort in [[JavaScript]]\n*[https://duvanenko.tech.blog/2018/05/23/faster-sorting-in-c/ High Performance Implementation] of LSD & MSD Radix sort in [[C#]] with source in [https://github.com/DragonSpit/HPCsharp/ GitHub]\n*[https://www.youtube.com/watch?v=6YyflHO9GdE Video tutorial of MSD Radix Sort] \n*[http://www.csse.monash.edu.au/~lloyd/tildeAlgDS/Sort/Radix/ Demonstration and comparison] of Radix sort with [[Bubble sort]], [[Merge sort]] and [[Quicksort]] implemented in [[JavaScript]]\n*[http://www.codercorner.com/RadixSortRevisited.htm Article] about Radix sorting [[IEEE floating-point standard|IEEE floating-point]] numbers with implementation.\n*:[http://www.stereopsis.com/radix.html Faster Floating Point Sorting and Multiple Histogramming] with implementation in C++\n*Pointers to [https://web.archive.org/web/20060829193644/http://web-cat.cs.vt.edu/AlgovizWiki/RadixSort radix sort visualizations]\n*[http://bitbucket.org/ais/usort/wiki/Home USort library] contains tuned implementations of radix sort for most numerical C types (C99)\n* [[Donald Knuth]]. ''The Art of Computer Programming'', Volume 3: ''Sorting and Searching'', Third Edition. Addison-Wesley, 1997. {{ISBN|0-201-89685-0}}. Section 5.2.5: Sorting by Distribution, pp.&nbsp;168–179.\n* [[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. ''[[Introduction to Algorithms]]'', Second Edition. MIT Press and McGraw-Hill, 2001. {{ISBN|0-262-03293-7}}. Section 8.3: Radix sort, pp.&nbsp;170–173.\n* [https://web.archive.org/web/20010714213118/http://www.chinet.com/~edlee/bradsort.c BRADSORT v1.50 source code]\n* [http://goanna.cs.rmit.edu.au/~jz/fulltext/acsc03sz.pdf Efficient Trie-Based Sorting of Large Sets of Strings], by Ranjan Sinha and  Justin Zobel.  This paper describes a method of creating tries of buckets which figuratively burst into sub-tries when the buckets hold more than a predetermined capacity of strings, hence the name, \"Burstsort\".\n* [http://opendatastructures.org/versions/edition-0.1e/ods-java/11_2_Counting_Sort_Radix_So.html Open Data Structures - Java Edition - Section 11.2 - Counting Sort and Radix Sort]\n* [http://opendatastructures.org/ods-cpp/11_2_Counting_Sort_Radix_So.html Open Data Structures - C++ Edition - Section 11.2 - Counting Sort and Radix Sort]\n\n{{Use dmy dates|date=January 2012}}\n{{sorting}}\n\n{{DEFAULTSORT:Radix Sort}}\n[[Category:Articles with example C code]]\n[[Category:Sorting algorithms]]\n[[Category:Stable sorts]]\n[[Category:String sorting algorithms]]"
    },
    {
      "title": "Samplesort",
      "url": "https://en.wikipedia.org/wiki/Samplesort",
      "text": "'''Samplesort''' is a [[sorting algorithm]] that is a [[divide and conquer algorithm]] often used in parallel processing systems.<ref name=\"sample_sort_tr.pdf\">{{cite web |url=http://www.cra.org/Activities/craw_archive/dmp/awards/2007/Berlin/jberlin/sample_sort_tr.pdf |title=Samplesort using the Standard Template Adaptive Parallel Library |publisher=Texas A&M University }}</ref> Conventional divide and conquer sorting algorithms partitions the array into sub-intervals or buckets. The buckets are then sorted individually and then concatenated together. However, if the array is non-uniformly distributed, the performance of these sorting algorithms can be significantly throttled. Samplesort addresses this issue by selecting a sample of size s from the n-element sequence, and determining the range of the buckets by sorting the sample and choosing m -1 elements from the result. These elements (called splitters) then divide the sample into m equal-sized buckets.<ref>{{cite web |url=http://parallelcomp.uw.hu/ch09lev1sec5.html |title=Bucket and Samplesort}}</ref> Samplesort is described in the 1970 paper, \"Samplesort: A Sampling Approach to Minimal Storage Tree Sorting\", by W. D. Frazer and A. C. McKellar.\n\n== Algorithm ==\nSamplesort is a generalization of [[quicksort]]. Where quicksort partitions its input into two parts at each step, based on a single value called the pivot, samplesort instead takes a larger [[Sample (statistics)|sample]] from its input and divides its data into buckets accordingly. Like quicksort, it then recursively sorts the buckets.\n\nTo devise a samplesort implementation, one needs to decide on the number of buckets {{mvar|p}}. When this is done, the actual algorithm operates in three phases:{{r|hill}}\n# Sample {{math|''p''−1}} elements from the input (the ''splitters''). Sort these; each pair of adjacent splitters then defines a ''bucket''.\n# Loop over the data, placing each element in the appropriate bucket. (This may mean: send it to a processor, in a [[multiprocessor]] system.)\n# Sort each of the buckets.\n\nThe full sorted output is the concatenation of the buckets.\n\nA common strategy is to set {{mvar|p}} equal to the number of processors available. The data is then distributed among the processors, which perform the sorting of buckets using some other, sequential, sorting algorithm.\n\n=== Pseudo Code ===\n\nThe following listing shows the above mentioned three step algorithm as pseudo code and shows how the algorithm works in principle.<ref name=\":0\">{{cite book|last1=Sanders|first1=Peter|last2=Winkel|first2=Sebastian|title=Super Scalar Sample Sort|journal=Algorithms - ESA 2004|volume=3221|date=2004-09-14|issue=Lecture Notes in Computer Science|pages=784–796|doi=10.1007/978-3-540-30140-0_69|series=Lecture Notes in Computer Science|isbn=978-3-540-23025-0|citeseerx=10.1.1.68.9881}}</ref> In the following, <math>A</math> is the unsorted data, <math>k</math> is the oversampling factor, discussed later, and <math>p</math> is the number of splitters.\n\n <math>\\text{sampleSort}(A[1..n], k, p)</math>\n   <math>\\text{if } n/k < \\text{ threshold then smallSort}(A)</math> // if average bucket size is below a threshold switch to e.g. quicksort\n   // Step 1 //\n   <math>\\text{select } S=[S_1, \\dots, S_{p(k-1)}]\\text{ randomly from }A</math> // select samples\n   <math>\\text{sort } S</math> // sort sample\n   <math>[s_0, s_1, \\dots, s_{p-1}, s_p] \\leftarrow [-\\infty, S_k, S_{2k}, \\dots, S_{(p-1)k}, \\infty]</math> // select splitters\n   // Step 2 //\n   <math>\\text{for each } a \\in A</math>\n     <math>\\text{find } j \\text{ such that } s_{j-1} < a \\leq s_j</math>\n     <math>\\text{place } a \\text{ in bucket } b_j</math>\n   // Step 3 and concatenation //\n   <math>\\text{return concatenate}(\\text{sampleSort}(b_1), \\dots, \\text{sampleSort}(b_k))</math>\n\nThe pseudo code is different from the original Frazer and McKellar algorithm.<ref>{{cite journal|last1=Frazer|first1=W. D.|last2=McKellar|first2=A. C.|title=Samplesort: A Sampling Approach to Minimal Storage Tree Sorting|journal=Journal of the ACM|date=1970-07-01|volume=17|issue=3|pages=496–507|doi=10.1145/321592.321600|url=http://dl.acm.org/citation.cfm?id=321592.321600|accessdate=2018-03-22}}</ref> In the pseudo code, samplesort is called recursively. Frazer and McKellar called samplesort just once and used [[quicksort]] in all following iterations.\n\n=== Complexity ===\nThe complexity, given in [[Big O notation]]:\n\nFind the splitters.\n:<math>O\\left(\\frac{n}{p} + \\log(p)\\right)</math>\n\nSend to buckets.\n:<math>O(p)</math> for reading all nodes\n:<math>O(\\log(p))</math> for broadcasting\n:<math>O\\left(\\frac{n}{p} \\log(p)\\right)</math> for binary search for all keys\n:<math>O\\left(\\frac{n}{p}\\right)</math> to send keys to bucket\n\nSort buckets.\n:<math>O(c/p)</math> where <math>c</math> is the complexity of the underlying sequential sorting method<ref name=\"sample_sort_tr.pdf\" />\n\nThe number of comparisons, performed by this algorithm, approaches the information theoretical optimum <math>\\log_2(n!)</math> for big input sequences. In experiments, conducted by Frazer and McKellar, the algorithm needed 15% less comparisons than quicksort.\n\n== Sampling the data ==\nThe data may be sampled through different methods. Some methods include:\n# Pick evenly spaced samples.\n# Pick randomly selected samples.\n\n=== Oversampling ===\nThe [[oversampling]] ratio determines how many times more data elements to pull as samples, before determining the splitters. The goal is to get a good representation of the distribution of the data. If the data values are widely distributed, in that there are not many duplicate values, then a small sampling ratio is sufficient. In other cases where there are many duplicates in the distribution, a larger oversampling ratio will be necessary. In the ideal case, after step 2, each bucket contains <math>n/p</math> elements. In this case, no bucket takes longer to sort than the others, because all buckets are of equal size.\n\nAfter pulling <math>k</math> times more samples than necessary, the samples are sorted. Thereafter, the splitters used as bucket boundaries are the samples at position <math>k, 2k, 3k, \\dots, (p-1)k</math> of the sample sequence (together with <math>-\\infty</math> and <math>\\infty</math> as left and right boundaries for the left most and right most buckets respectively). This provides a better heuristic for good splitters than just selecting <math>p</math> splitters randomly.\n\n=== Bucket Size Estimate ===\nWith the resulting sample size, the expected bucket size and especially the probability of a bucket exceeding a certain size can be estimated. The following will show that for an oversampling factor of <math>S \\in \\Theta(\\dfrac{\\log n}{\\epsilon^2})</math> the probability that no bucket has more than <math>(1 + \\epsilon) \\cdot \\dfrac{n}{p}</math> elements is larger than <math>1 - \\dfrac{1}{n}</math>.\n\nTo show this let <math>\\langle e_1, \\dots, e_n\\rangle</math> be the input as a sorted sequence. For a processor to get more than <math>(1 + \\epsilon) \\cdot n/p</math> elements, there has to exist a subsequence of the input of length <math>(1 + \\epsilon) \\cdot n/p</math>, of which a maximum of <math>S</math> samples are picked. These cases constitute the probability <math>P_{fail}</math>. This can be represented as the random variable:\n\n<math>X_i := \\begin{cases}\n 1, & \\text{if } s_i \\in \\langle e_j, \\dots, e_j + (1 + \\epsilon) \\cdot \\dfrac{n}{p}\\rangle\\\\\n 0, & \\text{otherwise}\n\\end{cases}, X := \\sum_{i=0}^{S \\cdot p - 1}X_i</math>\n\nFor the expected value of <math>X_i</math> holds:\n\n<math>E(X_i) = P(X_i = 1) = \\dfrac{1 + \\epsilon}{p}</math>\n\nThis will be used to estimate <math>P_{fail}</math>:\n\n<math>P(X < S) \\approx P(X < (1 - \\epsilon^2)S) = P(X < (1 - \\epsilon)E(X))</math>\n\nUsing the [[chernoff bound]] now, it can be shown:\n\n<math>P_{fail} = n \\cdot P(X < S) \\le n \\cdot exp\\left(\\dfrac{\\epsilon^2 \\cdot S}{2}\\right) \\le n \\cdot \\dfrac{1}{n^2} \\text{ for } S \\ge \\dfrac{4}{\\epsilon^2}\\ln n</math>\n\n== Many Identical Keys ==\nIn case of many identical keys, the algorithm goes through many recursion levels where sequences are sorted, because the whole sequence consists of identical keys. This can be counteracted by introducing equality buckets. Elements equal to a pivot are sorted into their respective equality bucket, which can be implemented with only one additional conditional branch. Equality buckets are not further sorted. This works, since keys occurring more than <math>n/k</math> times are likely to become pivots.\n\n== Uses in parallel systems ==\n[[File:Parallelersamplesort.svg|thumb|Example of parallel Samplesort on <math>p=3</math> processors and an oversampling factor of <math>k=3</math>.]]\nSamplesort is often used in parallel systems, including [[distributed computing|distributed systems]] such as [[bulk synchronous parallel]] machines.<ref>{{cite journal |first1=Alexandros V. |last1=Gerbessiotis |first2=Leslie G. |last2=Valiant |title=Direct Bulk-Synchronous Parallel Algorithms |year=1992 |journal=J. Parallel and Distributed Computing |volume=22 |pages=22–251 |citeseerx=10.1.1.51.9332}}</ref><ref name=\"hill\">{{cite journal |first1=Jonathan M. D. |last1=Hill |first2=Bill |last2=McColl |first3=Dan C. |last3=Stefanescu |first4=Mark W. |last4=Goudreau |first5=Kevin |last5=Lang |first6=Satish B. |last6=Rao |first7=Torsten |last7=Suel |first8=Thanasis |last8=Tsantilas |first9=Rob H. |last9=Bisseling |title=BSPlib: The BSP Programming Library |journal=Parallel Computing |volume=24 |issue=14 |year=1998 |pages=1947–1980 |citeseerx=10.1.1.48.1862}}</ref><ref>{{cite conference |first1=William L. |last1=Hightower |first2=Jan F. |last2=Prins |first3=John H. |last3=Reif |year=1992 |title=Implementations of randomized sorting on large parallel machines |conference=ACM Symp. on Parallel Algorithms and Architectures |url=https://users.cs.duke.edu/~reif/paper/proteus/randsort.pdf}}</ref> Due to the variable amount of splitters (in contrast to only one pivot in [[Quicksort]]), Samplesort is very well suited and intuitive for parallelization and scaling. Furthermore Samplesort is also more cache-efficient than implementations of e.g. quicksort.\n\nParallelization is implemented by splitting the sorting for each processor or node, where the number of buckets is equal to the number of processors <math>p</math>. Samplesort is efficient in parallel systems because each processor receives approximately the same bucket size <math>n/p</math>. Since the buckets are sorted concurrently, the processors will complete the sorting at approximately the same time, thus not having a processor wait for others.\n\nOn [[distributed systems]], the splitters are chosen by taking <math>k</math> elements on each processor, sorting the resulting <math>kp</math> elements with a distributed sorting algorithm, taking every <math>k</math>-th element and broadcasting the result to all processors. This costs <math>T_\\text{sort}(kp,p)</math> for sorting the <math>kp</math> elements on <math>p</math> processors, as well as  <math>T_\\text{allgather}(p,p)</math> for distributing the <math>p</math> chosen splitters to <math>p</math> processors.\n\nWith the resulting splitters, each processor places its own input data into local buckets. This takes <math>\\mathcal O(n/p\\log p)</math> with [[binary search]]. Thereafter, the local buckets are redistributed to the processors. Processor <math>i</math> gets the local buckets <math>b_i</math> of all other processors and sorts these locally. The distribution takes <math>T_\\text{all-to-all}(N, p)</math> time, where <math>N</math> is the size of the biggest bucket. The local sorting takes <math>T_\\text{localsort}(N)</math>.\n\nExperiments performed in the early 1990s on [[Connection Machine]] supercomputers showed samplesort to be particularly good at sorting large datasets on these machines, because its incurs little interprocessor communication overhead.<ref>{{cite conference |title=A Comparison of Sorting Algorithms for the Connection Machine CM-2 |first1=Guy E. |last1=Blelloch |authorlink1=Guy Blelloch |first2=Charles E. |last2=Leiserson |authorlink2=Charles E. Leiserson |first3=Bruce M. |last3=Maggs |first4=C. Gregory |last4=Plaxton |first5=Stephen J. |last5=Smith |first6=Marco |last6=Zagha |conference=ACM Symp. on Parallel Algorithms and Architectures |year=1991 |url=https://www.cs.cmu.edu/~scandal/papers/cm-sort-SPAA91.html |citeseerx=10.1.1.131.1835}}</ref> On latter-day [[GPGPU|GPUs]], the algorithm may be less effective than its alternatives.<ref>{{cite conference |first1=Nadathur |last1=Satish |first2=Mark |last2=Harris |first3=Michael |last3=Garland |title=Designing Efficient Sorting Algorithms for Manycore GPUs |conference=Proc. IEEE Int'l Parallel and Distributed Processing Symp. |citeseerx=10.1.1.190.9846}}</ref>{{Citation needed|reason=The data are outdated in this paper since both GPU and CPU versions are much more efficient nowadays.|date=January 2018}}\n\n== Efficient Implementation of Samplesort ==\n[[File:Animation.png|thumb|Animated example of Super Scalar Samplesort. In each step, numbers that are compared are marked blue and numbers that are otherwise read or written are marked red.]]\nAs described above, the samplesort algorithm splits the elements according to the selected splitters. An efficient implementation strategy is proposed in the paper \"Super Scalar Sample Sort\".<ref name=\":0\"/> The implementation proposed in the paper uses two arrays of size <math>n</math> (the original array containing the input data and a temporary one) for an efficient implementation. Hence, this version of the implementation is not an in-place algorithm.\n\nIn each recursion step, the data gets copied to the other array in a partitioned fashion. If the data is in the temporary array in the last recursion step, then the data is copied back to the original array.\n\n=== Determining Buckets ===\nIn a comparison based sorting algorithm the comparison operation is the most performance critical part. In Samplesort this corresponds to determining the bucket for each element. This needs <math>\\log k</math> time for each element.\n\nSuper Scalar Sample Sort uses a balanced search tree which is implicitly stored in an array <math>t</math>. The root is stored at 0, the left successor of <math>t_i</math> is stored at <math>t_{2i}</math> and the right successor is stored at <math>t_{2i+1}</math>. Given the search tree <math>t</math>, the algorithm calculates the bucket number <math>j</math> of element <math>a_i</math> as follows (assuming <math>a_i>t_j</math> evaluates to 1 if it is ''true'' and 0 otherwise):\n\n <math>j\\leftarrow 1</math>\n <math>\\text{repeat}\\log_2 p \\text{ times}</math>\n  <math>j\\leftarrow 2j+(a>t_j)</math>\n <math>j\\leftarrow j-p+1</math>\n\nSince the number of buckets <math>k</math> is known at compile time, this loop can be [[loop unrolling|unrolled]] by the compiler. The comparison operation is implemented with [[Predication (computer architecture)|predicated instructions]]. Thus, there occur no [[branch misprediction]]s, which would slow down the comparison operation significantly.\n\n=== Partitioning ===\nFor an efficient partitioning of the elements, the algorithm needs to know the sizes of the buckets in advance. To partition the elements of the sequence and put them into the array, we need to know the size of the buckets in advance. A naive algorithm could count the number of elements of each bucket. Then the elements could be inserted to the other array at the right place. Using this, one has to determine the bucket for each elements twice (one time for counting the number of elements in a bucket, and one time for inserting them).\n\nTo avoid this doubling of comparisons, Super Scalar Sample Sort uses an additional array <math>o</math> (called oracle) which assigns each index of the elements to a bucket. First, the algorithm determines the contents of <math>o</math> by determining the bucket for each element and the bucket sizes, and then placing the elements into the bucket determined by <math>o</math>. The array <math>o</math> also incurs cost in storage space, but as it only needs to store <math>n\\cdot \\log k</math> bits, these cost are small compared to the space of the input array.\n\n== In-Place Samplesort ==\nA key disadvantage of the efficient Samplesort implementation shown above is that it is not in-place and requires a second temporary array of the same size as the input sequence during sorting. Efficient implementations of e.g. quicksort are in-place and thus more space efficient. However, Samplesort can be implemented in-place as well.<ref>{{cite journal|last1=Axtmann|first1=Michael|last2=Witt|first2=Sascha|last3=Ferizovic|first3=Daniel|last4=Sanders|first4=Peter|title=In-Place Parallel Super Scalar Samplesort (IPSSSSo)|journal=25th Annual European Symposium on Algorithms (ESA 2017)|date=2017|volume=87|issue=Leibniz International Proceedings in Informatics (LIPIcs)|pages=9:1–9:14|doi=10.4230/LIPIcs.ESA.2017.9|url=http://drops.dagstuhl.de/opus/volltexte/2017/7854|accessdate=29 March 2018}}</ref>\n\nThe in-place algorithm is separated into four phases:\n# '''Sampling''' which is equivalent to the sampling in the above mentioned efficient implementation.\n# '''Local Classification''' on each processor, which groups the input into blocks such that all elements in each block belong to the same bucket, but buckets are not necessarily continuous in memory.\n# '''Block permutation''' brings the blocks into the globally correct order.\n# '''Cleanup''' moves some elements on the edges of the buckets.\n\nOne obvious disadvantage of this algorithm is that it reads and writes every element twice, once in the classification phase and once in the block permutation phase. However, the algorithm performs up to three times faster than other state of the art in-place competitors and up to 1.5 times faster than other state of the art sequential competitors. As sampling was already discussed above, the three later stages will be further detailed in the following.\n\n=== Local Classification ===\nIn a first step, the input array is split up into <math>p</math> stripes of blocks of equal size, one for each processor. Each processor additionally allocates <math>k</math> buffers that are of equal size to the blocks, one for each bucket. Thereafter, each processor scans its stripe and moves the elements into the buffer of the according bucket. If a buffer is full, the buffer is written into the processors stripe, beginning at the front. There is always at least one buffer size of empty memory, because for a buffer to be written (i.e. buffer is full), at least a whole buffer size of elements more than elements written back had to be scanned. Thus, every full block contains elements of the same bucket. While scanning, the size of each bucket is kept track of.\n\n=== Block Permutation ===\nFirstly, a prefix sum operation is performed that calculates the boundaries of the buckets. However, since only full blocks are moved in this phase, the boundaries are rounded up to a multiple of the block size and a single overflow buffer is allocated. Before starting the block permutation, some empty blocks might have to be moved to the end of its bucket. Thereafter, a write pointer <math>w_i</math> is set to the start of the bucket <math>b_i</math> subarray for each bucket and a read pointer <math>r_i</math> is set to the last non empty block in the bucket <math>b_i</math> subarray for each bucket.\n\nTo limit work contention, each processor is assigned a different primary bucket <math>b_{prim}</math> and two swap buffers that can each hold a block. In each step, if both swap buffers are empty, the processor decrements the read pointer <math>r_{prim}</math> of its primary bucket and reads the block at <math>r_{prim - 1}</math> and places it in one of its swap buffers. After determining the destination bucket <math>b_{dest}</math> of the block by classifying the first element of the block, it increases the write pointer <math>w_{dest}</math>, reads the block at <math>w_{dest - 1}</math> into the other swap buffer and writes the block into its destination bucket. If <math>w_{dest} > r_{dest}</math>, the swap buffers are empty again. Otherwise the block remaining in the swap buffers has to be inserted into its destination bucket.\n\nIf all blocks in the subarray of the primary bucket of a processor are in the correct bucket, the next bucket is chosen as the primary bucket. If a processor chose all buckets as primary bucket once, the processor is finished.\n\n=== Cleanup ===\nSince only whole blocks were moved in the block permutation phase, some elements might still be incorrectly placed around the bucket boundaries. Since there has to be enough space in the array for each element, those incorrectly placed elements can be moved to empty spaces from left to right, lastly considering the overflow buffer.\n\n== See also ==\n* [[Flashsort]]\n* [[Quicksort]]\n\n==References==\n{{Reflist}}\n\n== External links ==\nFrazer and McKellar's samplesort and derivatives:\n* [http://portal.acm.org/citation.cfm?id=321600 Frazer and McKellar's original paper]\n* http://www.springerlink.com/content/p70564506802n575/\n* http://www.springerlink.com/content/l211p1q526j84174/\nAdapted for use on parallel computers:\n* http://citeseer.ist.psu.edu/91922.html \n* http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.49.214\n\n{{sorting}}\n\n[[Category:Sorting algorithms]]\n[[Category:Distributed algorithms]]"
    },
    {
      "title": "Schwartzian transform",
      "url": "https://en.wikipedia.org/wiki/Schwartzian_transform",
      "text": "In [[computer programming]], the '''Schwartzian transform''' is a technique used to improve the efficiency of [[sorting]] a list of items. This [[programming idiom|idiom]]<ref>{{cite book |year=2002 |chapter=2.3 Sorting While Guaranteeing Sort Stability |editor1-last=Martelli |editor1-first=Alex |editor2-last=Ascher |editor2-first=David |title=Python Cookbook |publisher=O'Reilly & Associates |page=43 |isbn=0-596-00167-3 |quote=This idiom is also known as the 'Schwartzian transform', by analogy with a related Perl idiom. }}</ref> is appropriate for [[comparison sort|comparison-based sorting]] when the ordering is actually based on the ordering of a certain property (the ''key'') of the elements, where computing that property is an intensive operation that should be performed a minimal number of times. The Schwartzian transform is notable in that it does not use named temporary arrays.\n\nThe Schwartzian transform is a version of a [[Lisp programming language|Lisp]] idiom known as '''decorate-sort-undecorate''', which avoids recomputing the sort keys by temporarily associating them with the input items. This approach is similar to [[memoization]], which avoids repeating the calculation of the key corresponding to a specific input value. By comparison, this idiom assures that each input item's key is calculated exactly once, which may still result in repeating some calculations if the input data contains duplicate items.\n\nThe idiom is named after [[Randal L. Schwartz]], who first demonstrated it in [[Perl]] shortly after the release of Perl 5 in 1994. The term \"Schwartzian transform\" applied solely to Perl [[programming language|programming]] for a number of years, but it has later been adopted by some users of other [[programming language|languages]], such as [[Python (programming language)|Python]], to refer to similar idioms in those languages. However, the algorithm was already in use in other languages (under no specific name) before it was popularized among the Perl community in the form of that particular idiom by Schwartz. The term \"Schwartzian transform\" indicates a specific idiom, and ''not'' the algorithm in general.\n\nFor example, to sort the word list (\"aaaa\",\"a\",\"aa\") according to word length: first build the list ([\"aaaa\",4],[\"a\",1],[\"aa\",2]), then sort it according to the numeric values getting ([\"a\",1],[\"aa\",2],[\"aaaa\",4]), then strip off the numbers and you get (\"a\",\"aa\",\"aaaa\"). That was the algorithm in general, so it does not count as a transform. To make it a true Schwartzian transform, it would be done in Perl like this:\n<source lang=\"perl\">\n@sorted = map  { $_->[0] }\n          sort { $a->[1] <=> $b->[1] or $a->[0] cmp $b->[0] } # use numeric comparison, fall back to string sort on original\n          map  { [$_, length($_)] }    # calculate the length of the string\n               @unsorted;\n</source>\n\n==The Perl idiom==\nThe general form of the Schwartzian Transform is:\n\n<source lang=\"perl\">\n@sorted = map  { $_->[0] }\n          sort { $a->[1] cmp $b->[1] or $a->[0] cmp $b->[0] }\n          map  { [$_, foo($_)] }\n               @unsorted;\n</source>\n\nWhere <code>foo($_)</code> represents an expression that takes <code>$_</code> (each item of the list in turn) and produces the corresponding value that is to be compared in its stead.\n\nReading from right to left (or from the bottom to the top):\n* the original list <code>@unsorted</code> is fed into a <code>map</code> operation that wraps each item into a (reference to an anonymous 2-element) array consisting of itself and the calculated value that will determine its sort order (list of item becomes a list of [item, value]);\n* then the list of lists produced by <code>map</code> is fed into <code>sort</code>, which sorts it according to the values previously calculated (list of [item, value] ⇒ sorted list of [item, value]);\n* finally, another <code>map</code> operation unwraps the values (from the anonymous array) used for the sorting, producing the items of the original list in the sorted order (sorted list of [item, value] ⇒ sorted list of item).\n\nThe use of anonymous arrays ensures that memory will be reclaimed by the Perl garbage collector immediately after the sorting is done.\n\n==Efficiency analysis==\nWithout the Schwartzian transform, the sorting in the example above would be written in Perl like this:\n<source lang=\"perl\">\n@sorted = sort { foo($a) cmp foo($b) } @unsorted;\n</source>\n\nWhile it is shorter to code, the naive approach here could be much less efficient if the key function (called {{mono|foo}} in the example above) is expensive to compute. This is because the code inside the brackets is evaluated each time two elements need to be compared. An optimal [[comparison sort]] performs ''[[big o notation|O]](n log n)'' comparisons (where ''n'' is the length of the list), with 2 calls to {{mono|foo}} every comparison, resulting in ''O(n log n)'' calls to {{mono|foo}}. In comparison, using the Schwartzian transform, we only make 1 call to {{mono|foo}} per element, at the beginning {{mono|map}} stage, for a total of ''n'' calls to {{mono|foo}}.\n\nHowever, if the function {{mono|foo}} is relatively simple, then the extra overhead of the Schwartzian transform may be unwarranted.\n\n==Example==\n\nFor example, to sort a list of files by their [[mac times|modification times]], a naive approach might be as follows:\n\n  '''function''' naiveCompare(file a, file b) {\n      '''return''' modificationTime(a) < modificationTime(b)\n  }\n  \n  ''// Assume that sort(list, comparisonPredicate) sorts the given list using''\n  ''// the comparisonPredicate to compare two elements.''\n  sortedArray := sort(filesArray, naiveCompare)\n\nUnless the modification times are memoized for each file, this method requires re-computing them every time a file is compared in the sort. Using the Schwartzian transform, the modification time is calculated only once per file.\n\nA Schwartzian transform involves the functional idiom described above, which does not use temporary arrays.\n\nThe same algorithm can be written procedurally to better illustrate how it works, but this requires using temporary arrays, and is not a Schwartzian transform. The following example pseudo-code implements the algorithm in this way:\n\n  '''for each''' file '''in''' filesArray\n      insert array(file, modificationTime(file)) at end of transformedArray\n  \n  '''function''' simpleCompare(array a, array b) {\n      '''return''' a[2] < b[2]\n  }\n  \n  transformedArray := sort(transformedArray, simpleCompare)\n  \n  '''for each''' file '''in''' transformedArray\n      insert file[1] at end of sortedArray\n\n==History==\n\nThe first known online appearance of the Schwartzian Transform is a December 16, 1994 [http://groups.google.com/group/comp.unix.shell/browse_frm/thread/31da970cebb30c6d?hl=en posting by Randal Schwartz] to a thread in [[comp.unix.shell]], crossposted to [[comp.lang.perl]]. (The current version of the [http://history.perl.org/PerlTimeline.html Perl Timeline] is incorrect and refers to a later date in 1995.) The thread began with a question about how to sort a list of lines by their \"last\" word:\n\n  adjn:Joshua Ng\n  adktk:KaLap Timothy Kwong\n  admg:Mahalingam Gobieramanan\n  admln:Martha L. Nangalama\n\nSchwartz responded with:\n\n<source lang=\"perl\">\n#!/usr/bin/perl\nrequire 5; # new features, new bugs!\nprint\n    map { $_->[0] }\n    sort { $a->[1] cmp $b->[1] }\n    map { [$_, /(\\S+)$/] }\n    <>; \n</source>\n\nThis code produces the result:\n\n  admg:Mahalingam Gobieramanan\n  adktk:KaLap Timothy Kwong\n  admln:Martha L. Nangalama\n  adjn:Joshua Ng\n\nSchwartz noted in the post that he was \"Speak[ing] with a lisp in Perl,\" a reference to the idiom's [[Lisp (programming language)|Lisp]] origins.\n\nThe term \"Schwartzian Transform\" itself was coined by [[Tom Christiansen]] in a follow-up reply.  Later posts by Christiansen made it clear that he had not intended to ''name'' the construct, but merely to refer to it from the original post: his attempt to finally name it \"The Black Transform\" did not take hold (\"Black\" here being a pun on \"schwar[t]z\", which means black in German).\n\n==Comparison to other languages==\nSome other languages provide a convenient interface to the same optimization as the Schwartzian transform:\n* In [[Python (programming language)|Python]] 2.4 and above, both the {{mono|sorted()}} function and the in-place {{mono|list.sort()}} method take a {{mono|1=key=}} parameter that allows the user to provide a \"key function\" (like {{mono|foo}} in the examples above). In Python 3 and above, use of the key function is the only way to specify a custom sort order (the previously-supported comparator argument was removed). Before Python 2.4, developers would use the lisp-originated Decorate-Sort-Undecorate (DSU) idiom,<ref>{{cite web|title=How To/Sorting/Decorate Sort Undecorate|url=https://wiki.python.org/moin/HowTo/Sorting#The_Old_Way_Using_Decorate-Sort-Undecorate}}</ref> usually by wrapping the objects in a (sortkey, object) tuple.\n* In [[Ruby (programming language)|Ruby]] 1.8.6 and above, the {{mono|Enumerable}} abstract class (which includes {{mono|Array}}s) contains a {{mono|sort_by}}<ref name=\"Module: Enumerable\">{{cite web|title=Ruby-doc Core-API Classes|url=http://www.ruby-doc.org/core/classes/Enumerable.html#method-i-sort_by|accessdate=14 September 2011}}</ref> method which allows you to specify the \"key function\" (like {{mono|foo}} in the examples above) as a code block.\n* In [[D (programming language)|D]] 2 and above, the {{mono|schwartzSort}} function is available. It might require less temporary data and be faster than the Perl idiom or the decorate-sort-undecorate idiom present in Python and Lisp. This is because sorting is done in-place and only minimal extra data (one array of transformed elements) is created.\n* [[Racket (programming language)|Racket's]] core <code>sort</code> function accepts a <code>#:key</code> keyword argument with a function that extracts a key, and an additional <code>#:cache-keys?</code> requests that the resulting values are cached during sorting.  For example, a convenient way to shuffle a list is {{code|2=racket|(sort l < #:key (λ (_) (random)) #:cache-keys? #t)}}.\n* In [[PHP (programming language)|PHP]] 5.3 and above the transform can be implemented by use of {{mono|array_walk}}, e.g. to work around the limitations of the unstable sort algorithms in PHP.<source lang=\"PHP\">\nfunction spaceballs_sort(array& $a) {\n  array_walk($a, function(&$v, $k){ $v = array($v, $k); });   \n  asort($a);            \n  array_walk($a, function(&$v, $_) { $v = $v[0]; }); \n}</source>\n* In [[Elixir (programming language)|Elixir]], the {{mono|Enum.sort_by/2}} and {{mono|Enum.sort_by/3}} methods allow users to perform a Schwartzian transform for any module that implements the {{mono|Enumerable}} protocol.\n* In [[Perl 6]], one needs to supply a comparator lambda that only takes 1 argument to perform a Schwartzian transform under the hood: <source lang=\"Perl6\">@a.sort( { $^a.Str } )</source> would sort on the string representation using a Schwartzian transform, <source lang=\"Perl6\">@a.sort( { $^a.Str cmp $^b.Str } )</source> would do the same converting the elements to compare just before each comparison.\n\n==References==\n<references/>\n\n==External links==\n{{Wikibooks|Algorithm Implementation/Sorting|Schwartzian transform}}\n*[http://www.stonehenge.com/merlyn/UnixReview/col64.html Sorting with the Schwartzian Transform by Randal L. Schwartz]\n*[http://perl.plover.com/TPC/1998/Hardware-notes.html#Schwartzian_Transform Mark-Jason Dominus explains the Schwartzian Transform]\n* http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/52234\n* Python Software Foundation (2005). [https://www.python.org/doc/faq/programming/#i-want-to-do-a-complicated-sort-can-you-do-a-schwartzian-transform-in-python 1.5.2   I want to do a complicated sort: can you do a Schwartzian Transform in Python?]. Retrieved June 22, 2005.\n*[https://metacpan.org/module/Memoize Memoize Perl module - making expensive functions faster by caching their results.]\n\n[[Category:Sorting algorithms]]\n[[Category:Articles with example Perl code]]\n[[Category:Articles with example Racket code]]\n[[Category:Perl]]"
    },
    {
      "title": "Selection sort",
      "url": "https://en.wikipedia.org/wiki/Selection_sort",
      "text": "{{No footnotes|date=May 2019}}\n{{Infobox Algorithm\n|class=[[Sorting algorithm]]\n|caption=Selection sort animation\n|data=[[Array data structure|Array]]\n|time=О(''n''<sup>2</sup>) comparisons, О(''n'') swaps\n|best-time=О(''n''<sup>2</sup>) comparisons, О(''n'') swaps\n|average-time=О(''n''<sup>2</sup>) comparisons, О(''n'') swaps\n|space=O(1) auxiliary\n|optimal=No\n|stable=No\n}}\n\nIn [[computer science]], '''selection sort''' is a [[sorting algorithm]], specifically an [[in-place algorithm|in-place]] [[comparison sort]]. It has [[Big O notation|O]](''n''<sup>2</sup>) [[time complexity]], making it inefficient on large lists, and generally performs worse than the similar [[insertion sort]]. Selection sort is noted for its simplicity, and it has performance advantages over more complicated algorithms in certain situations, particularly where [[auxiliary memory]] is limited.\n\nThe algorithm divides the input list into two parts: the sublist of items already sorted, which is built up from left to right at the front (left) of the list, and the sublist of items remaining to be sorted that occupy the rest of the list. Initially, the sorted sublist is empty and the unsorted sublist is the entire input list. The algorithm proceeds by finding the smallest (or largest, depending on sorting order) element in the unsorted sublist, exchanging (swapping) it with the leftmost unsorted element (putting it in sorted order), and moving the sublist boundaries one element to the right. Uniqueness of selection sort when compared to other sorting techniques:The time efficiency of selection sort is quadratic, so there exists a number of sorting techniques which have better time complexity than Selection Sort. Even then, considering the number of swaps made, the number of swaps will be n-1 both in worst as well as best case. That is, time efficiency of selection sort with respect to swaps is linear. This property distinguishes selection sort positively from many other sorting algorithms.\n\n== Example ==\nHere is an example of this sort algorithm sorting five elements:\n\n{| class=\"wikitable\"\n! Sorted sublist\n! Unsorted sublist\n! Least element in unsorted list\n|-\n| ( )\n| {{ts|ar}} | (11, 25, 12, 22, 64)\n| 11\n|-\n|  (11)\n| {{ts|ar}} |(25, 12, 22, 64)\n| 12\n|-\n| (11, 12)\n| {{ts|ar}} |(25, 22, 64)\n| 22\n|-\n| (11, 12, 22)\n| {{ts|ar}} |(25, 64)\n| 25\n|-\n| (11, 12, 22, 25)\n| {{ts|ar}} |(64)\n| 64\n|-\n| (11, 12, 22, 25, 64)\n| {{ts|ar}} |( )\n|\n|}\n[[Image:Selection-Sort-Animation.gif|right|thumb|Selection sort animation. Red is current min. Yellow is sorted list. Blue is current item.]]\n(Nothing appears changed on these last two lines because the last two numbers were already in order)\n\nSelection sort can also be used on list structures that make add and remove efficient, such as a [[linked list]]. In this case it is more common to ''remove'' the minimum element from the remainder of the list, and then ''insert'' it at the end of the values sorted so far. For example:\n\n<pre style=\"overflow:auto; width:auto;\">\narr[] = 64 25 12 22 11\n\n// Find the minimum element in arr[0...4]\n// and place it at beginning\n11 25 12 22 64\n\n// Find the minimum element in arr[1...4]\n// and place it at beginning of arr[1...4]\n11 12 25 22 64\n\n// Find the minimum element in arr[2...4]\n// and place it at beginning of arr[2...4]\n11 12 22 25 64\n\n// Find the minimum element in arr[3...4]\n// and place it at beginning of arr[3...4]\n11 12 22 25 64 \n</pre>\n\n== Implementations ==\n{{unreferenced section|date=May 2019}}\nBelow is an implementation in [[C (programming language)|C]]. More implementations can be found on [[Talk:Selection sort#Implementations|the talk page of this Wikipedia article]].\n<source lang=\"c\" style=\"overflow:auto; width:auto;\" line=\"1\">\n/* a[0] to a[aLength-1] is the array to sort */\nint i,j;\nint aLength; // initialise to a's length\n\n/* advance the position through the entire array */\n/*   (could do i < aLength-1 because single element is also min element) */\nfor (i = 0; i < aLength-1; i++)\n{\n    /* find the min element in the unsorted a[i .. aLength-1] */\n\n    /* assume the min is the first element */\n    int jMin = i;\n    /* test against elements after i to find the smallest */\n    for (j = i+1; j < aLength; j++)\n    {\n        /* if this element is less, then it is the new minimum */\n        if (a[j] < a[jMin])\n        {\n            /* found new minimum; remember its index */\n            jMin = j;\n        }\n    }\n\n    if (jMin != i) \n    {\n        swap(a[i], a[jMin]);\n    }\n}\n</source>\n\n== Complexity ==\nSelection sort is not difficult to analyze compared to other sorting algorithms since none of the loops depend on the data in the array. Selecting the minimum requires scanning <math>n</math> elements (taking <math>n-1</math> comparisons) and then swapping it into the first position. Finding the next lowest element requires scanning the remaining <math>n-1</math> elements and so on. Therefore, the total number of comparisons is\n\n<math>(n-1)+(n-2)+...+1 =\n\\sum_{i=1}^{n-1}i</math>\n\nBy [[arithmetic progression]],\n\n<math>\\sum_{i=1}^{n-1}i=\n\\frac{(n-1)+1}{2}(n-1)=\n\\frac{1}{2}n(n-1)=\n\\frac{1}{2}(n^2-n)</math>\n\nwhich is of complexity <math>O(n^2)</math> in terms of number of comparisons. Each of these scans requires one swap for <math>n-1</math> elements (the final element is already in place).\n\n== Comparison to other sorting algorithms ==\n\nAmong quadratic sorting algorithms (sorting algorithms with a simple average-case of [[Big O notation#Family of Bachmann–Landau notations|Θ(''n''<sup>2</sup>)]]), selection sort almost always outperforms [[bubble sort]] and [[gnome sort]]. [[Insertion sort]] is very similar in that after the ''k''th iteration, the first ''k'' elements in the array are in sorted order. Insertion sort's advantage is that it only scans as many elements as it needs in order to place the ''k''&nbsp;+&nbsp;1st element, while selection sort must scan all remaining elements to find the ''k''&nbsp;+&nbsp;1st element.\n\nSimple calculation shows that insertion sort will therefore usually perform about half as many comparisons as selection sort, although it can perform just as many or far fewer depending on the order the array was in prior to sorting. It can be seen as an advantage for some [[real-time computing|real-time]] applications that selection sort will perform identically regardless of the order of the array, while insertion sort's running time can vary considerably. However, this is more often an advantage for insertion sort in that it runs much more efficiently if the array is already sorted or \"close to sorted.\"\n\nWhile selection sort is preferable to insertion sort in terms of number of writes (Θ(''n'') swaps versus Ο(''n''<sup>2</sup>) swaps), it almost always far exceeds (and never beats) the number of writes that [[cycle sort]] makes, as cycle sort is theoretically optimal in the number of writes. This can be important if writes are significantly more expensive than reads, such as with [[EEPROM]] or [[Flash memory|Flash]] memory, where every write lessens the lifespan of the memory.\n\nFinally, selection sort is greatly outperformed on larger arrays by Θ(''n''&nbsp;log&nbsp;''n'') [[divide and conquer algorithm|divide-and-conquer algorithms]] such as [[mergesort]]. However, insertion sort or selection sort are both typically faster for small arrays (i.e. fewer than 10–20 elements). A useful optimization in practice for the recursive algorithms is to switch to insertion sort or selection sort for \"small enough\" sublists.\n\n== Variants ==\n\n[[Heapsort]] greatly improves the basic algorithm by using an [[implicit data structure|implicit]] [[heap (data structure)|heap]] [[data structure]] to speed up finding and removing the lowest datum. If implemented correctly, the heap will allow finding the next lowest element in Θ(log&nbsp;''n'') time instead of Θ(''n'') for the inner loop in normal selection sort, reducing the total running time to Θ(''n''&nbsp;log&nbsp;''n'').\n\nA bidirectional variant of selection sort, called '''cocktail sort''', is an algorithm which finds both the minimum and maximum values in the list in every pass. This reduces the number of scans of the list by a factor of 2, eliminating some loop overhead but not actually decreasing the number of comparisons or swaps. Note, however, that [[cocktail sort]] more often refers to a bidirectional variant of bubble sort. Sometimes this is '''double selection sort'''.\n\nSelection sort can be implemented as a [[Sorting algorithm#Classification|stable sort]]. If, rather than swapping in step 2, the minimum value is inserted into the first position (that is, all intervening items moved down), the algorithm is stable. However, this modification either requires a data structure that supports efficient insertions or deletions, such as a linked list, or it leads to performing Θ(''n''<sup>2</sup>) writes.\n\nIn the '''bingo sort''' variant, items are ordered by repeatedly looking through the remaining items to find the greatest value and moving all items with that value to their final location.<ref>{{DADS|Bingo sort|bingosort}}</ref> Like [[counting sort]], this is an efficient variant if there are many duplicate values. Indeed, selection sort does one pass through the remaining items for each item moved. Bingo sort does one pass for each value (not item): after an initial pass to find the biggest value, the next passes can move every item with that value to its final location while finding the next value as in the following [[pseudocode]] (arrays are zero-based and the for-loop includes both the top and bottom limits, as in [[Pascal (programming language)|Pascal]]):\n\n<source lang=\"pascal\">\nbingo(array A)\n\n{ This procedure sorts in ascending order. }\nbegin\n    max := length(A)-1;\n\n    { The first iteration is written to look very similar to the subsequent ones, but\n      without swaps. }\n    nextValue := A[max];\n    for i := max - 1 downto 0 do\n        if A[i] > nextValue then\n            nextValue := A[i];\n    while (max > 0) and (A[max] = nextValue) do\n        max := max - 1;\n\n    while max > 0 do begin\n        value := nextValue;\n        nextValue := A[max];\n        for i := max - 1 downto 0 do\n             if A[i] = value then begin\n                 swap(A[i], A[max]);\n                 max := max - 1;\n             end else if A[i] > nextValue then\n                 nextValue := A[i];\n        while (max > 0) and (A[max] = nextValue) do\n            max := max - 1;\n    end;\nend;\n</source>\n\nThus, if on average there are more than two items with the same value, bingo sort can be expected to be faster because it executes the inner loop fewer times than selection sort.\n<!--\n\nIf you came here to write an implementation of selection sort, note that this page used to have implementations but they were moved to Wikibooks. Therefore, implementations should not be added here.\n\n-->\n\n== See also ==\n* [[Selection algorithm]]\n\n== References ==\n{{reflist}}\n{{refbegin}}\n* [[Donald Knuth]]. ''[[The Art of Computer Programming]]'', Volume 3: ''Sorting and Searching'', Third Edition. Addison–Wesley, 1997. {{ISBN|0-201-89685-0}}. Pages 138–141 of Section 5.2.3: Sorting by Selection.\n* Anany Levitin. ''Introduction to the Design & Analysis of Algorithms'', 2nd Edition. {{ISBN|0-321-35828-7}}. Section 3.1: Selection Sort, pp 98–100.\n* [[Robert Sedgewick (computer scientist)|Robert Sedgewick]]. ''Algorithms in C++, Parts 1–4: Fundamentals, Data Structure, Sorting, Searching: Fundamentals, Data Structures, Sorting, Searching Pts. 1–4'', Second Edition. Addison–Wesley Longman, 1998. {{ISBN|0-201-35088-2}}. Pages 273–274\n{{refend}}\n\n== External links ==\n{{wikibooks|Algorithm implementation|Sorting/Selection_sort|Selection sort}}\n* {{webarchive |url=https://web.archive.org/web/20150307110315/http://www.sorting-algorithms.com/selection-sort |date=7 March 2015 |title=Animated Sorting Algorithms: Selection Sort}} – graphical demonstration\n\n{{sorting}}\n\n[[Category:Sorting algorithms]]\n[[Category:Comparison sorts]]"
    },
    {
      "title": "Shellsort",
      "url": "https://en.wikipedia.org/wiki/Shellsort",
      "text": "{{Short description|Sorting algorithm which uses multiple comparison intervals}}\n{{Infobox Algorithm\n |class=[[Sorting algorithm]]\n |image=[[File:Sorting shellsort anim.gif|Step-by-step visualisation of Shellsort]]<br/><small>Shellsort with gaps 23, 10, 4, 1 in action.</small>\n |data=[[Array data structure|Array]]\n |time=O(n<sup>2</sup>) (worst known worst case gap sequence)<br />O(''n'' log<sup>2</sup>''n'') (best known worst case gap sequence)<ref name=\"Pratt\">{{Cite book\n  |last=Pratt\n  |first=Vaughan Ronald |authorlink=Vaughan Ronald Pratt\n  |year=1979\n  |publisher=Garland\n  |title=Shellsort and Sorting Networks (Outstanding Dissertations in the Computer Sciences)\n  |url=http://www.dtic.mil/get-tr-doc/pdf?AD=AD0740110\n  |isbn=978-0-8240-4406-0}}</ref>\n |best-time=O(''n'' log ''n'') (most gap sequences)<br>O(''n'' log<sup>2</sup>''n'') (best known worst case gap sequence)<ref>{{cite web|title=Shellsort & Comparisons|url=http://www.cs.wcupa.edu/rkline/ds/shell-comparison.html}}</ref>\n |average-time=depends on gap sequence\n |space=О(n) total, O(1) auxiliary\n |optimal=No\n}}\n[[File:Shell sorting algorithm color bars.svg|thumb|alt=Vectorgraphic diagram depicting the step-by-step process.|The step-by-step process of replacing pairs of items during the shell sorting algorithm.]]\n'''Shellsort''', also known as '''Shell sort''' or '''Shell's method''', is an in-place [[comparison sort]]. It can be seen as either a generalization of sorting by exchange ([[bubble sort]]) or sorting by insertion ([[insertion sort]]).<ref name=\"Knuth\" /> The method starts by sorting pairs of elements far apart from each other, then progressively reducing the gap between elements to be compared. Starting with far apart elements, it can move some out-of-place elements into position faster than a simple nearest neighbor exchange. [[Donald Shell]] published the first version of this sort in 1959.<ref name=\"Shell\">{{Cite journal\n  |url=http://penguin.ewu.edu/cscd300/Topic/AdvSorting/p30-shell.pdf\n  |last=Shell\n  |first=D. L.\n  |title=A High-Speed Sorting Procedure\n  |journal=Communications of the ACM\n  |volume=2\n  |issue=7\n  |year=1959\n  |pages=30–32\n  |doi=10.1145/368370.368387}}</ref><ref>Some older textbooks and references call this the \"Shell-Metzner\" sort after [[Marlene Metzner Norton]], but according to Metzner, \"I had nothing to do with the sort, and my name should never have been attached to it.\" See {{Cite web\n  |title=Shell sort\n  |publisher=National Institute of Standards and Technology\n  |url=https://xlinux.nist.gov/dads/HTML/shellsort.html\n  |accessdate=2007-07-17 }}</ref> The running time of Shellsort is heavily dependent on the gap sequence it uses. For many practical variants, determining their [[time complexity]] remains an [[open problem]].\n\n== Description ==\nShellsort is a generalization of [[insertion sort]] that allows the exchange of items that are far apart. The idea is to arrange the list of elements so that, starting anywhere, considering every ''h''th element gives a sorted list. Such a list is said to be ''h''-sorted. Equivalently, it can be thought of as ''h'' interleaved lists, each individually sorted.<ref name=\"Sedgewick\">\n {{Cite book\n  |last=Sedgewick\n  |first=Robert\n  |authorlink=Robert Sedgewick (computer scientist)\n  |title=Algorithms in C\n  |edition=3rd\n  |volume=1\n  |publisher=Addison-Wesley\n  |location=\n  |year=1998\n  |pages=273–281\n  |isbn=978-0-201-31452-6\n }}</ref> Beginning with large values of ''h'', this rearrangement allows elements to move long distances in the original list, reducing large amounts of disorder quickly, and leaving less work for smaller ''h''-sort steps to do.<ref name=\"KR\">\n {{Cite book\n  |last1=Kernighan\n  |first1=Brian W.\n  |authorlink1=Brian Kernighan\n  |last2=Ritchie\n  |first2=Dennis M.\n  |authorlink2=Dennis Ritchie\n  |title=The C Programming Language\n  |edition=2nd\n  |publisher=Prentice Hall\n  |year=1996\n  |pages=62\n  |isbn=978-7-302-02412-5\n }}</ref> If the list is then ''k-sorted'' for some smaller integer ''k'', then the list remains ''h''-sorted. Following this idea for a decreasing sequence of ''h'' values ending in 1 is guaranteed to leave a sorted list in the end.<ref name=\"Sedgewick\"/>\n\nAn example run of Shellsort with gaps 5, 3 and 1 is shown below.\n\n{|class=\"wikitable\" style=\"text-align:center\"\n!\n! {{mvar|a}}<sub>1</sub> || {{mvar|a}}<sub>2</sub> || {{mvar|a}}<sub>3</sub> || {{mvar|a}}<sub>4</sub>\n! {{mvar|a}}<sub>5</sub> || {{mvar|a}}<sub>6</sub> || {{mvar|a}}<sub>7</sub> || {{mvar|a}}<sub>8</sub>\n! {{mvar|a}}<sub>9</sub> || {{mvar|a}}<sub>10</sub> || {{mvar|a}}<sub>11</sub> || {{mvar|a}}<sub>12</sub>\n|-\n! Input data\n| 62 || 83 || 18 || 53 || 07 || 17 || 95 || 86 || 47 || 69 || 25 || 28\n|-\n! After 5-sorting\n|                   17 ||                   28 ||                   18 ||                   47 ||                   07\n|bgcolor=lightcyan| 25 ||bgcolor=lightcyan| 83 ||bgcolor=lightcyan| 86 ||bgcolor=lightcyan| 53 ||bgcolor=lightcyan| 69\n|                   62 ||                   95\n|-\n! After 3-sorting\n|                   17 ||                   07 ||                   18\n|bgcolor=lightcyan| 47 ||bgcolor=lightcyan| 28 ||bgcolor=lightcyan| 25\n|                   69 ||                   62 ||                   53\n|bgcolor=lightcyan| 83 ||bgcolor=lightcyan| 86 ||bgcolor=lightcyan| 95\n|-\n! After 1-sorting\n| 07 || 17 || 18 || 25 || 28 || 47 || 53 || 62 || 69 || 83 || 86 || 95\n|}\n\nThe first pass, 5-sorting, performs insertion sort on five  separate subarrays (''a''<sub>1</sub>, ''a''<sub>6</sub>, ''a''<sub>11</sub>), (''a''<sub>2</sub>, ''a''<sub>7</sub>, ''a''<sub>12</sub>), (''a''<sub>3</sub>, ''a''<sub>8</sub>), (''a''<sub>4</sub>, ''a''<sub>9</sub>), (''a''<sub>5</sub>, ''a''<sub>10</sub>). For instance, it changes the subarray (''a''<sub>1</sub>, ''a''<sub>6</sub>, ''a''<sub>11</sub>) from (62, 17, 25) to (17, 25, 62). The next pass, 3-sorting, performs insertion sort on the three subarrays (''a''<sub>1</sub>, ''a''<sub>4</sub>, ''a''<sub>7</sub>, ''a''<sub>10</sub>), (''a''<sub>2</sub>, ''a''<sub>5</sub>, ''a''<sub>8</sub>, ''a''<sub>11</sub>), (''a''<sub>3</sub>, ''a''<sub>6</sub>, ''a''<sub>9</sub>, ''a''<sub>12</sub>). The last pass, 1-sorting, is an ordinary insertion sort of the entire array (''a''<sub>1</sub>,..., ''a''<sub>12</sub>).\n\nAs the example illustrates, the subarrays that Shellsort operates on are initially short; later they are longer but almost ordered. In both cases insertion sort works efficiently.\n\nShellsort is not [[sorting algorithm#Stability|stable]]: it may change the relative order of elements with equal values. It is an [[Adaptive sort|adaptive sorting algorithm]] in that it executes faster when the input is partially sorted.\n\n== Pseudocode ==\nUsing Marcin Ciura's gap sequence, with an inner insertion sort.\n<!-- javascript syntax color works well for pseudocode -->\n<syntaxhighlight lang=\"c\">\n# Sort an array a[0...n-1].\ngaps = [701, 301, 132, 57, 23, 10, 4, 1]\n\n# Start with the largest gap and work down to a gap of 1\nforeach (gap in gaps)\n{\n    # Do a gapped insertion sort for this gap size.\n    # The first gap elements a[0..gap-1] are already in gapped order\n    # keep adding one more element until the entire array is gap sorted\n    for (i = gap; i < n; i += 1)\n    {\n        # add a[i] to the elements that have been gap sorted\n        # save a[i] in temp and make a hole at position i\n        temp = a[i]\n        # shift earlier gap-sorted elements up until the correct location for a[i] is found\n        for (j = i; j >= gap and a[j - gap] > temp; j -= gap)\n        {\n            a[j] = a[j - gap]\n        }\n        # put temp (the original a[i]) in its correct location\n        a[j] = temp\n    }\n}\n</syntaxhighlight>\n\n== Gap sequences ==\nThe question of deciding which gap sequence to use is difficult. Every gap sequence that contains 1 yields a correct sort (as this makes the final pass an ordinary insertion sort); however, the properties of thus obtained versions of Shellsort may be very different. Too few gaps slows down the passes, and too many gaps produces an overhead.\n\nThe table below compares most proposed gap sequences published so far. Some of them have decreasing elements that depend on the size of the sorted array (''N''). Others are increasing infinite sequences, whose elements less than ''N'' should be used in reverse order.\n\n:{| class=\"wikitable\"\n|- style=\"background-color: #efefef;\"\n! [[OEIS]]\n! General term (''k'' ≥ 1)\n! Concrete gaps\n! Worst-case<br>time complexity\n! Author and year of publication\n|----\n|\n| <math>\\left\\lfloor\\frac{N}{2^k}\\right\\rfloor</math>\n| <math>\\left\\lfloor\\frac{N}{2}\\right\\rfloor,\n        \\left\\lfloor\\frac{N}{4}\\right\\rfloor, \\ldots, 1\n  </math>\n| <math>\\Theta\\left(N^2\\right)</math> [e.g. when ''N'' = 2<sup>''p''</sup>]\n| [[Donald Shell|Shell]], 1959<ref name=\"Shell\"/>\n|----\n|\n| <math>2 \\left\\lfloor\\frac{N}{2^{k+1}}\\right\\rfloor + 1</math>\n| <math>2 \\left\\lfloor\\frac{N}{4}\\right\\rfloor + 1, \\ldots, 3, 1</math>\n| <math>\\Theta\\left(N^\\frac{3}{2}\\right)</math>\n| Frank & Lazarus, 1960<ref>{{Cite journal\n  |last=Frank\n  |first=R. M.\n  |last2=Lazarus\n  |first2=R. B.\n  |title=A High-Speed Sorting Procedure\n  |journal=Communications of the ACM\n  |volume=3\n  |issue=1\n  |year=1960\n  |pages=20–22\n  |doi=10.1145/366947.366957}}</ref>\n|----\n| {{OEIS link|A168604}}\n| <math>2^k - 1</math>\n| <math>1, 3, 7, 15, 31, 63, \\ldots</math>\n| <math>\\Theta\\left(N^\\frac{3}{2}\\right)</math>\n| [[Thomas N. Hibbard|Hibbard]], 1963<ref>{{Cite journal\n  |last=Hibbard\n  |first=Thomas N.\n  |title=An Empirical Study of Minimal Storage Sorting\n  |journal=Communications of the ACM\n  |volume=6\n  |issue=5\n  |year=1963\n  |pages=206–213\n  |doi=10.1145/366552.366557}}</ref>\n|----\n| {{OEIS link|A083318}}\n| <math>2^k + 1</math>, prefixed with 1\n| <math>1, 3, 5, 9, 17, 33, 65, \\ldots</math>\n| <math>\\Theta\\left(N^\\frac{3}{2}\\right)</math>\n| Papernov & Stasevich, 1965<ref>{{Cite journal\n  |url=http://www.mathnet.ru/links/83f0a81df1ec06f76d3683c6cab7d143/ppi751.pdf\n  |last=Papernov\n  |first=A. A.\n  |last2=Stasevich\n  |first2=G. V.\n  |title=A Method of Information Sorting in Computer Memories\n  |journal=Problems of Information Transmission\n  |volume=1\n  |issue=3\n  |year=1965\n  |pages=63–75}}</ref>\n|----\n| {{OEIS link|A003586}}\n| Successive numbers of the form <math>2^p 3^q</math> ([[3-smooth]] numbers)\n| <math>1, 2, 3, 4, 6, 8, 9, 12, \\ldots</math>\n| <math>\\Theta\\left(N \\log^2 N\\right)</math>\n| [[Vaughan Ronald Pratt|Pratt]], 1971<ref name=\"Pratt\"/>\n|----\n| {{OEIS link|A003462}}\n| <math>\\frac{3^k - 1}{2}</math>, not greater than <math>\\left\\lceil\\frac{N}{3}\\right\\rceil</math>\n| <math>1, 4, 13, 40, 121, \\ldots </math>\n| <math>\\Theta\\left(N^\\frac{3}{2}\\right)</math>\n| [[Vaughan Ronald Pratt|Pratt]], 1971<ref name=\"Pratt\"/>\n|----\n| {{OEIS link|A036569}}\n| <math>\\begin{align}\n           &\\prod\\limits_I a_q, \\hbox{where} \\\\\n  a_q = {} &\\min\\left\\{n \\in \\mathbb{N}\\colon n \\ge \\left(\\frac{5}{2}\\right)^{q+1}, \\forall p\\colon 0 \\le p < q \\Rightarrow \\gcd(a_p, n) = 1\\right\\} \\\\\n    I = {} &\\left\\{0 \\le q < r \\mid q \\neq \\frac{1}{2}\\left(r^2 + r\\right) - k \\right\\} \\\\\n    r = {} &\\left\\lfloor \\sqrt{2k + \\sqrt{2k}} \\right\\rfloor\n\\end{align}</math>\n| <math>1, 3, 7, 21, 48, 112, \\ldots</math>\n| <math>O\\left(N^{1 + \\sqrt{\\frac{8\\ln\\left(5/2\\right)}{\\ln(N)}}}\\right)</math>\n| Incerpi & [[Robert Sedgewick (computer scientist)|Sedgewick]], 1985,<ref>{{Cite journal\n  |last=Incerpi\n  |first=Janet\n  |last2=Sedgewick\n  |first2=Robert |author2-link=Robert Sedgewick (computer scientist)\n  |title=Improved Upper Bounds on Shellsort\n  |journal=Journal of Computer and System Sciences\n  |volume=31\n  |issue=2\n  |year=1985\n  |pages=210–224\n  |doi=10.1016/0022-0000(85)90042-x}}</ref> [[Donald Knuth|Knuth]]<ref name=\"Knuth\">{{Cite book\n  |last=Knuth\n  |first=Donald E. |author-link=Donald Knuth\n  |title=The Art of Computer Programming. Volume 3: Sorting and Searching\n  |edition=2nd\n  |publisher=Addison-Wesley\n  |location=Reading, Massachusetts\n  |year=1997\n  |pages=83–95\n  |chapter=Shell's method\n  |isbn=978-0-201-89685-5}}</ref>\n|----\n| {{OEIS link|A036562}}\n| <math>4^k + 3 \\cdot 2^{k-1} + 1</math>, prefixed with 1\n| <math>1, 8, 23, 77, 281, \\ldots</math>\n| <math>O\\left(N^\\frac{4}{3}\\right)</math>\n| Sedgewick, 1986<ref name=\"Sedgewick\"/>\n|----\n| {{OEIS link|A033622}}\n| <math>\\begin{cases}\n  9\\left(2^{k} - 2^\\frac{k}{2}\\right) + 1 & k\\text{ even}, \\\\\n  8 \\cdot 2^{k} - 6 \\cdot 2^{(k+1)/2} + 1         & k\\text{ odd}\n\\end{cases}</math>\n| <math>1, 5, 19, 41, 109, \\ldots</math>\n| <math>O\\left(N^\\frac{4}{3}\\right)</math>\n| Sedgewick, 1986<ref name=\"Sedgewick2\">\n  {{Cite journal\n   |last=Sedgewick\n   |first=Robert |authorlink=Robert Sedgewick (computer scientist)\n   |title=A New Upper Bound for Shellsort\n   |journal=Journal of Algorithms\n   |volume=7\n   |issue=2\n   |year=1986\n   |pages=159–173\n   |doi=10.1016/0196-6774(86)90001-5\n  }}</ref>\n|----\n| \n| <math>h_k = \\max\\left\\{\\left\\lfloor \\frac{5h_{k-1}}{11} \\right\\rfloor, 1\\right\\}, h_0 = N</math>\n| <math>\\left\\lfloor \\frac{5N}{11} \\right\\rfloor, \\left\\lfloor \\frac{5}{11}\\left\\lfloor \\frac{5N}{11} \\right\\rfloor\\right\\rfloor, \\ldots, 1</math>\n| {{unk}}\n| [[Gaston Gonnet|Gonnet]] & [[Ricardo Baeza-Yates|{{nowrap|Baeza-Yates}}]], 1991<ref name=\"Gonnet\">{{Cite book\n  |last=Gonnet\n  |first=Gaston H.\n  |last2=Baeza-Yates\n  |first2=Ricardo\n  |title=Handbook of Algorithms and Data Structures: In Pascal and C\n  |publisher=Addison-Wesley\n  |location=Reading, Massachusetts\n  |edition=2nd\n  |year=1991\n  |pages=161–163\n  |chapter=Shellsort\n  |isbn=978-0-201-41607-7}}</ref>\n|----\n| {{OEIS link|A108870}}\n| <math>\\left\\lceil \\frac{1}{5} \\left(9\\cdot \\left(\\frac{9}{4}\\right)^{k-1} - 4 \\right) \\right\\rceil</math> \n| <math>1, 4, 9, 20, 46, 103, \\ldots</math>\n| {{unk}}\n| Tokuda, 1992<ref>{{Cite book\n  |editor-last=van Leeuven\n  |editor-first=Jan\n  |chapter=An Improved Shellsort\n  |last=Tokuda\n  |first=Naoyuki\n  |title=Proceedings of the IFIP 12th World Computer Congress on Algorithms, Software, Architecture\n  |publisher=North-Holland Publishing Co.\n  |location=Amsterdam\n  |year=1992\n  |pages=449–457\n  |isbn=978-0-444-89747-3}}</ref>\n|----\n| {{OEIS link|A102549}}\n| Unknown (experimentally derived)\n| <math>1, 4, 10, 23, 57, 132, 301, 701</math><!--Please don't add 1750. It doesn't belong here.-->\n| {{unk}}\n| Ciura, 2001<ref name=\":0\">{{Cite book\n  |chapter-url=http://sun.aei.polsl.pl/~mciura/publikacje/shellsort.pdf\n  |title=Proceedings of the 13th International Symposium on Fundamentals of Computation Theory\n  |editor-last=Freiwalds\n  |editor-first=Rusins\n  |last=Ciura\n  |first=Marcin\n  |chapter=Best Increments for the Average Case of Shellsort\n  |publisher=Springer-Verlag\n  |location=London\n  |year=2001\n  |pages=106–117\n  |isbn=978-3-540-42487-1}}</ref>\n|}\nWhen the binary representation of ''N'' contains many consecutive zeroes, Shellsort using Shell's original gap sequence makes Θ(''N''<sup>2</sup>) comparisons in the worst case. For instance, this case occurs for ''N'' equal to a power of two when elements greater and smaller than the median occupy odd and even positions respectively, since they are compared only in the last pass.\n\nAlthough it has higher complexity than the ''O''(''N''&nbsp;log&nbsp;''N'') that is optimal for comparison sorts, Pratt's version lends itself to [[sorting network]]s and has the same asymptotic gate complexity as Batcher's [[bitonic sorter]].\n\nGonnet and Baeza-Yates observed that Shellsort makes the fewest comparisons on average when the ratios of successive gaps are roughly equal to 2.2.<ref name=\"Gonnet\"/> This is why their sequence with ratio 2.2 and Tokuda's sequence with ratio 2.25 prove efficient. However, it is not known why this is so. Sedgewick recommends to use gaps that have low [[greatest common divisor]]s or are pairwise [[coprime]].<ref>{{Cite book\n  |title=Algorithms in C++, Parts 1–4: Fundamentals, Data Structure, Sorting, Searching\n  |last=Sedgewick\n  |first=Robert |authorlink=Robert Sedgewick (computer scientist)\n  |chapter=Shellsort\n  |publisher=Addison-Wesley\n  |location=Reading, Massachusetts\n  |year=1998\n  |pages=285–292\n  |isbn=978-0-201-35088-3}}</ref>\n\nWith respect to the average number of comparisons, Ciura's sequence<ref name=\":0\" /> has the best known performance; gaps from 701 were not determined but the sequence can be further extended according to the recursive formula <math>h_k = \\lfloor 2.25 h_{k-1} \\rfloor</math>.\n\nTokuda's sequence, defined by the simple formula <math>h_k = \\lceil h'_k \\rceil</math>, where <math>h'_k = 2.25 h'_{k-1} + 1</math>, <math>h'_1 = 1</math>, can be recommended for practical applications.\n\n== Computational complexity ==\nThe following property holds: after ''h''<sub>2</sub>-sorting of any ''h''<sub>1</sub>-sorted array, the array remains ''h''<sub>1</sub>-sorted.<ref>{{Cite journal\n  |last=Gale\n  |first=David\n  |last2=Karp\n  |first2=Richard M.\n  |title=A Phenomenon in the Theory of Sorting\n  |journal=Journal of Computer and System Sciences\n  |volume=6\n  |issue=2\n  |year=1972\n  |pages=103–115\n  |doi=10.1016/S0022-0000(72)80016-3}}</ref> Every ''h''<sub>1</sub>-sorted and ''h''<sub>2</sub>-sorted array is also (''a''<sub>1</sub>''h''<sub>1</sub>+''a''<sub>2</sub>''h''<sub>2</sub>)-sorted, for any nonnegative integers ''a''<sub>1</sub> and ''a''<sub>2</sub>. The worst-case complexity of Shellsort is therefore connected with the [[coin problem|Frobenius problem]]: for given integers ''h''<sub>1</sub>,..., ''h''<sub>''n''</sub> with gcd = 1, the Frobenius number ''g''(''h''<sub>1</sub>,..., ''h''<sub>''n''</sub>) is the greatest integer that cannot be represented as ''a''<sub>1</sub>''h''<sub>1</sub>+ ... +''a''<sub>''n''</sub>''h''<sub>''n''</sub> with nonnegative integer ''a''<sub>1</sub>,..., ''a''<sub>''n''</sub>. Using known formulae for Frobenius numbers, we can determine the worst-case complexity of Shellsort for several classes of gap sequences.<ref>{{Cite journal\n  |last=Selmer\n  |first=Ernst S.\n  |title=On Shellsort and the Frobenius Problem\n  |journal=BIT Numerical Mathematics\n  |volume=29\n  |issue=1\n  |year=1989\n  |pages=37–40\n  |doi=10.1007/BF01932703}}</ref> Proven results are shown in the above table.\n\nWith respect to the average number of operations, none of the proven results concerns a practical gap sequence. For gaps that are powers of two, Espelid computed this average as <math>0.5349N\\sqrt{N}-0.4387N-0.097\\sqrt{N}+O(1)</math>.<ref>{{Cite journal\n  |last=Espelid\n  |first=Terje O.\n  |title=Analysis of a Shellsort Algorithm\n  |journal=BIT Numerical Mathematics\n  |volume=13\n  |issue=4\n  |year=1973\n  |pages=394–400\n  |doi=10.1007/BF01933401}}</ref> [[Donald Knuth|Knuth]] determined the average complexity of sorting an ''N''-element array with two gaps (''h'', 1) to be <math>\\frac{2N^2}{h} + \\sqrt{\\pi N^3 h}</math>.<ref name=\"Knuth\" /> It follows that a two-pass Shellsort with ''h'' = Θ(''N''<sup>1/3</sup>) makes on average ''O''(''N''<sup>5/3</sup>) comparisons/inversions/running time. [[Andrew Yao|Yao]] found the average complexity of a three-pass Shellsort.<ref>{{Cite journal\n  |last=Yao\n  |first=Andrew Chi-Chih\n  |title=An Analysis of (''h'', ''k'', 1)-Shellsort\n  |journal=Journal of Algorithms\n  |volume=1\n  |issue=1\n  |year=1980\n  |pages=14–50\n  |doi=10.1016/0196-6774(80)90003-6}}</ref> His result was refined by Janson and Knuth:<ref>{{Cite journal\n  |arxiv=cs/9608105\n  |id=[[CiteSeerX]]: {{url|1=citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.54.9911|2=10.1.1.54.9911}}\n  |last=Janson\n  |first=Svante |author1-link=Svante Janson\n  |last2=Knuth\n  |first2=Donald E. |author2-link=Donald Knuth\n  |title=Shellsort with Three Increments\n  |journal=Random Structures and Algorithms\n  |volume=10\n  |issue=1–2\n  |year=1997\n  |pages=125–142\n  |doi=10.1002/(SICI)1098-2418(199701/03)10:1/2<125::AID-RSA6>3.0.CO;2-X}}</ref> the average number of comparisons/inversions/running time made during a Shellsort with three gaps (''ch'', ''cg'', 1), where ''h'' and ''g'' are coprime, is <math>\\frac{N^2}{4ch} + O(N)</math> in the first pass, <math>\\frac{1}{8g}\\sqrt{\\frac{\\pi}{ch}}(h - 1)N^{3/2} + O(hN)</math> in the second pass and <math>\\psi(h, g)N + \\frac{1}{8}\\sqrt{\\frac{\\pi}{c}}(c - 1)N^{3/2} + O\\left((c - 1)gh^{1/2}N\\right) + O\\left(c^2g^3h^2\\right)</math> in the third pass. ''ψ''(''h'', ''g'') in the last formula is a complicated function asymptotically equal to <math>\\sqrt{\\frac{\\pi h}{128}}g + O\\left(g^{-1/2}h^{1/2}\\right) + O\\left(gh^{-1/2}\\right)</math>. In particular, when ''h'' = Θ(''N''<sup>7/15</sup>) and ''g'' = Θ(''N''<sup>1/5</sup>), the average time of sorting is ''O''(''N''<sup>23/15</sup>).\n\nBased on experiments, it is conjectured that Shellsort with [[Thomas N. Hibbard|Hibbard]]'s gap sequence runs in ''O''(''N''<sup>5/4</sup>) average time,<ref name=\"Knuth\" /> and that Gonnet and Baeza-Yates's sequence requires on average 0.41''N''ln''N''(ln&nbsp;ln''N''+1/6) element moves.<ref name=\"Gonnet\" /> Approximations of the average number of operations formerly put forward for other sequences fail when sorted arrays contain millions of elements.\n\nThe graph below shows the average number of element comparisons in various variants of Shellsort, divided by the theoretical lower bound, i.e. log<sub>2</sub>''N''!, where the sequence 1, 4, 10, 23, 57, 132, 301, 701 has been extended according to the formula <math>h_k = \\lfloor2.25 h_{k-1}\\rfloor</math>.\n\n[[File:Shell sort average number of comparisons (English).svg|center]]\n\nApplying the theory of [[Kolmogorov complexity]], Jiang, [[Ming Li|Li]], and [[Paul Vitányi|Vitányi]] proved the following lower bound for the order of the average number of operations/running time in a ''p''-pass Shellsort: Ω(''pN''<sup>1+1/''p''</sup>) when ''p''≤log<sub>2</sub>''N'' and Ω(''pN'') when ''p''>log<sub>2</sub>''N''.<ref>{{Cite journal\n  |id = [[CiteSeerX]]: {{url|1=citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.6.6508|2=10.1.1.6.6508}}\n  |last=Jiang\n  |first=Tao\n  |last2=Li\n  |first2=Ming\n  |last3=Vitányi\n  |first3=Paul\n  |title=A Lower Bound on the Average-Case Complexity of Shellsort\n  |journal=[[Journal of the ACM]]\n  |volume=47\n  |issue=5\n  |year=2000\n  |pages=905–911\n  |doi=10.1145/355483.355488|url=http://www.cwi.nl/~paulv/papers/shell2.ps\n  |arxiv=cs/9906008\n  }}</ref> Therefore, Shellsort has prospects of running in an average time that asymptotically grows like ''N''log''N'' only when using gap sequences whose number of gaps grows in proportion to the logarithm of the array size. It is, however, unknown whether Shellsort can reach this asymptotic order of average-case complexity, which is optimal for comparison sorts. The lower bound was improved by Vitányi<ref>{{cite journal\n  |doi=10.1002/rsa.20737\n  |last=Vitányi\n  |first=Paul\n  |authorlink=Paul Vitányi\n  |year=2018\n  |title=On the average-case complexity of Shellsort\n  |journal=Random Structures and Algorithms\n  |volume=52\n  |issue=2\n  |pages=354–363|arxiv=cs/9906008\n  }}</ref> for every number of passes <math>p</math> to  \n<math>\n\\Omega ( N\\sum_{k=1}^p h_{k-1}/h_k )\n</math>\nwhere <math>h_0=N</math>. This result implies for example the Jiang-Li-Vitányi lower bound for all <math>p</math>-pass increment sequences and improves that lower bound for particular increment sequences. In fact all bounds (lower and upper) currently known for the average case are precisely matched by this lower bound. For example, this gives the new result that\nthe [[Svante Janson|Janson]]-[[Donald Knuth|Knuth]] upper bound is matched by the resulting lower bound for the used increment sequence, showing that three pass Shellsort for this increment sequence uses <math>\\Theta(N^{23/15})</math> comparisons/inversions/running time.\nThe formula allows us to search for increment sequences that yield lower bounds which are unknown; for example an increment sequence for four passes which has a lower\nbound greater than <math>\\Omega(pn^{1+1/p})=\\Omega(n^{5/4})</math> for the \nincrement sequence\n<math>h_1=n^{11/16}, h_2=n^{7/16}, h_3=n^{3/16}, h_4=1</math>. The lower bound becomes\n<math>T= \\Omega(n\\cdot (n^{1-11/16}+n^{11/16-7/16}+n^{7/16-3/16}+n^{3/16})=\n\\Omega(n^{1+5/16})=\\Omega(n^{21/16}).</math>\n\n\n\nThe worst-case complexity of any version of Shellsort is of higher order: Plaxton, [[Bjorn Poonen|Poonen]], and [[Torsten Suel|Suel]] showed that it grows at least as rapidly as <math>\\Omega\\left(N ({{\\log N} \\over {\\log \\log N}})^2\\right)</math>.<ref>{{Cite book\n  |id=[[CiteSeerX]]: {{url|1=citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.43.1393|2=10.1.1.43.1393}}\n  |last=Plaxton\n  |first=C. Greg\n  |last2=Poonen\n  |first2=Bjorn\n  |last3=Suel\n  |first3=Torsten\n  |title=Improved Lower Bounds for Shellsort\n  |journal=Annual Symposium on Foundations of Computer Science\n  |volume=33\n  |year=1992\n  |pages=226–235\n  |doi=10.1109/SFCS.1992.267769\n|isbn=978-0-8186-2900-6\n  |citeseerx=10.1.1.460.2429\n  }}</ref>\n\n== Applications ==\nShellsort performs more operations and has higher [[CPU cache#Cache miss|cache miss ratio]] than [[quicksort]]. However, since it can be implemented using little code and does not use the [[call stack]], some implementations of the [[qsort]] function in the [[C standard library]] targeted at [[embedded systems]] use it instead of quicksort. Shellsort is, for example, used in the [[uClibc]] library.<ref>{{Cite web\n  | url=http://git.uclibc.org/uClibc/tree/libc/stdlib/stdlib.c#n700\n  | title=libc/stdlib/stdlib.c\n  | first=Manuel III |last=Novoa\n  | accessdate=2014-10-29}}</ref> For similar reasons, an implementation of Shellsort is present in the [[Linux kernel]].<ref>{{Cite web\n  | url=https://github.com/torvalds/linux/blob/72932611b4b05bbd89fafa369d564ac8e449809b/kernel/groups.c#L105\n  | title=kernel/groups.c\n  | accessdate=2012-05-05}}</ref>\n\nShellsort can also serve as a sub-algorithm of [[introsort|introspective sort]], to sort short subarrays and to prevent a slowdown when the recursion depth exceeds a given limit. This principle is employed, for instance, in the [[bzip2]] compressor.<ref>{{Cite web\n  |url=https://www.ncbi.nlm.nih.gov/IEB/ToolBox/CPP_DOC/lxr/source/src/util/compress/bzip2/blocksort.c#L519\n  |title=bzip2/blocksort.c\n  |author=Julian Seward\n  |accessdate=2011-03-30}}</ref>\n\n== See also ==\n* [[Comb sort]]\n\n== References ==\n{{reflist|30em}}\n\n== Bibliography ==\n* {{Cite book\n  |last=Knuth\n  |first=Donald E. |authorlink=Donald Knuth\n  |title=The Art of Computer Programming. Volume 3: Sorting and Searching\n  |edition=2nd\n  |publisher=Addison-Wesley\n  |location=Reading, Massachusetts\n  |year=1997\n  |pages=83–95\n  |chapter=Shell's method\n  |isbn=978-0-201-89685-5\n  |title-link=The Art of Computer Programming }}\n* [http://www.cs.princeton.edu/~rs/shell/ Analysis of Shellsort and Related Algorithms], Robert Sedgewick, Fourth European Symposium on Algorithms, Barcelona, September 1996.\n\n== External links ==\n{{wikibooks|Algorithm implementation|Sorting/Shell_sort|Shell sort}}\n* {{webarchive |url=https://web.archive.org/web/20150310043846/http://www.sorting-algorithms.com/shell-sort |date=10 March 2015 |title=Animated Sorting Algorithms: Shell Sort}} – graphical demonstration\n* [https://www.youtube.com/watch?v=CmPA7zE8mx0 Shellsort with gaps 5, 3, 1 as a Hungarian folk dance]\n\n{{sorting}}\n{{Use dmy dates|date=September 2010}}\n\n{{DEFAULTSORT:Shellsort}}\n[[Category:Sorting algorithms]]\n[[Category:Comparison sorts]]"
    },
    {
      "title": "Slowsort",
      "url": "https://en.wikipedia.org/wiki/Slowsort",
      "text": "{{for|bogosort that is also known as \"slowsort\"|Bogosort}}\n'''Slowsort''' is a [[sorting algorithm]]. It is of humorous nature and not useful. It's based on the principle of ''multiply and surrender'', a tongue-in-cheek joke of [[divide and conquer algorithm|divide and conquer]]. It was published in 1986 by Andrei Broder and Jorge Stolfi in their paper ''Pessimal Algorithms and Simplexity Analysis''<ref>{{cite web|url=http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.116.9158 |title=CiteSeerX — Pessimal Algorithms and Simplexity Analysis |website=Citeseerx.ist.psu.edu |date= |accessdate=2017-07-26}}</ref> (a parody of [[asymptotically optimal algorithm|optimal algorithms]] and [[computational complexity theory|complexity analysis]]).\n\n==Algorithm==\nSlowsort is a [[recursive algorithm]].\n\nAn [[in-place]] implementation in pseudo code:\n\n  '''procedure''' slowsort(A,i,j)                            // sorts Array A[i],...,A[j]\n    '''if''' i >= j '''then''' '''return'''\n    m := ⌊(i+j)/2⌋                            \n    slowsort(A,i,m)                                    // (1.1)\n    slowsort(A,m+1,j)                                  // (1.2)\n    '''if''' A[j] < A[m] '''then''' swap A[j] and A[m]             // (1.3)\n    slowsort(A,i,j-1)                                  // (2)\n\n* Sort the first half recursively (1.1)\n* Sort the second half recursively (1.2)\n* Find the maximum of the whole array by comparing the results of 1.1 and 1.2 and place it at the end of the list (1.3)\n* Recursively sort the entire list without the maximum in 1.3 (2).\n\nAn implementation in [[Haskell (programming language)|Haskell]] (purely functional) may look as follows.\n  slowsort :: Ord a => [a] -> [a]\n  slowsort xs\n    | length xs <= 1 = xs\n    | otherwise      = slowsort xsnew ++ [max llast rlast]  -- (2)\n      where  \n        l     = slowsort $ take m xs  -- (1.1)\n        r     = slowsort $ drop m xs  -- (1.2)\n        llast = last l\n        rlast = last r\n        xsnew = init l ++ min llast rlast : init r\n        m     = fst (divMod (length xs) 2)\n\n==Complexity==\nThe runtime <math> T(n) </math> for Slowsort is <math> T(n) = 2 T(n/2) + T(n-1) + 1 </math>.\nA lower [[asymptotic bound]] for <math> T(n) </math> in [[Landau notation]] is \n<math>\\Omega\\left(n^{ \\frac{\\log_2(n)}{(2+\\epsilon)}}\\right)</math> for any <math> \\epsilon > 0 </math>. Slowsort is therefore not in [[polynomial time]]. Even the best case is worse than [[Bubble sort]].\n\n==References==\n\n{{Reflist}}\n\n{{sorting}}\n\n[[Category:Sorting algorithms]]\n[[Category:Computer humor]]"
    },
    {
      "title": "Smoothsort",
      "url": "https://en.wikipedia.org/wiki/Smoothsort",
      "text": "{{Infobox Algorithm\n|class=[[Sorting algorithm]]\n|image=[[File:Smoothsort.gif||alt=An animation depicting smoothsort's operation, showing the heap being built and then disassembled,]]\n|caption=Smoothsort operating on an array which is mostly in order.  The bars across the top show the tree structure.\n|data=[[Array data structure|Array]]\n|time={{math|''O''(''n'' log ''n'')}}\n|best-time={{math|''O''(''n'')}}\n|average-time={{math|''O''(''n'' log ''n'')}}\n|space={{math|''O''(''n'')}} total, {{math|''O''(1)}} auxiliary\n|optimal=When the data is already sorted\n}}\n\nIn [[computer science]], '''smoothsort''' is a [[comparison sort|comparison-based]] [[sorting algorithm]]. A variant of [[heapsort]], it was invented and published by [[Edsger Dijkstra]] in 1981.<ref name=EWD-796a>{{Cite EWD|796a|16 Aug 1981|Smoothsort – an alternative to sorting in situ|quote=One can also raise the question why I have not chosen as available stretch lengths: ... 63 31 15 7 3 1 which seems attractive since each stretch can then be viewed as the postorder traversal of a balanced binary tree. In addition, the recurrence relation would be simpler.  But I know why I chose the Leonardo numbers:}}</ref> Like heapsort, smoothsort is an [[in-place algorithm]] with an upper bound of [[Big O notation|O]](''n'' log&nbsp;''n''),{{r|hertel}} but it is not a [[stable sort]].<ref>{{cite web |title=Fastest In-Place Stable Sort |first=Craig |last=Brown |date=21 Jan 2013 |url=http://www.codeproject.com/Articles/26048/Fastest-In-Place-Stable-Sort |publisher=[[Code Project]]}}</ref>{{self-published inline|date=January 2016}} The advantage of smoothsort is that it comes closer to O(''n'') time if the [[Adaptive sort|input is already sorted to some degree]], whereas heapsort averages O(''n'' log&nbsp;''n'') regardless of the initial sorted state.\n\n==Overview==\nLike [[heapsort]], smoothsort first transforms the input array into an [[implicit data structure|implicit]] heap data structure, then produces the sorted array by repeatedly extracting the largest remaining element, whose location is determined by the heap structure, and the restoring the structure on the remaining elements. Whereas heapsort uses an implicit tree structure on the array in which a parent node always come before its descendants, so that the initial element is the root of the tree, smoothsort uses an implicit tree structure where a parent node always comes after its descendants. This has some important consequences, such as the fact that not all initial portions of the array correspond to a complete tree; however, it can help to avoid unnecessary displacements as in heapsort, where every element has to pass through the initial position (the root) just before getting swapped to its final position. Indeed, the algorithm is organized so that at the point where an element gets extracted from the heap structure, it is already at the rightmost position among the remaining elements, which is its proper place in the sorted array, so no swap is needed to bring it there.\n\nThe tree structure defined on array positions is fixed and independent of the array contents. It can be extended to all natural numbers, and doing so there is no root, since every position gets to be a child of a parent further on; however some positions are leaves in an absolute sense. This contrasts with the tree structure used for heapsort, where the initial position is the global root, but there are no absolute leaves, since every position gets two children when sufficiently many positions are added. In the smoothsort structure, every position ''i'' is the root of a unique subtree, whose nodes form an interval that ends at ''i''. An initial interval of positions, and in particular the set of all positions in a given array, might be such an interval corresponding to a subtree, but in general decomposes as a union of a number of successive such subtree intervals, which Dijkstra calls \"stretches\". Any subtree rooted at a position whose parent lies beyond the initial interval considered gives a stretch in the decomposition of that interval, which decomposition is therefore unique. When a new position following the sequence of stretches is added, one of two things can be the case: either the position is a leaf and adds a stretch of length 1 to the decomposition, or it combines with the last two stretches, becoming the parent of their respective roots, thus replacing the two stretches by a new stretch containing their union plus the new (root) position.\n\nDifferent rules could be used to determine for each position which of the two possibilities applies. For instance, one could stipulate that the last two stretches are combined if and only if they have equal size, in which case all subtrees would be perfect binary trees. Dijkstra's formulation of smoothsort uses a different rule, in which sibling subtrees never have equal sizes (except when both are 1), and the sizes of all subtrees are among a set of values that he calls [[Leonardo numbers]] (they are closely related to [[Fibonacci numbers]]). The general outline of the smoothsort procedure can be defined independently of the rule used to define the implicit tree structure, though the details of its steps depend on that rule. Dijkstra points out<ref name=EWD-796a/> that it would have been possible to use perfect binary trees (of size 2<sup>''k''</sup>−1); this would lead to the same [[Asymptotic analysis|asymptotic efficiency]],{{r|hertel}} but a constant factor in efficiency would be lost due to the on average greater number of stretches that a range of positions breaks up into.\n\nThe rule Dijkstra uses is that the last two stretches are combined if and only if their sizes are ''successive'' Leonardo numbers L(i+1), L(i) (in decreasing order), which numbers are recursively defined as:\n* L(0) = L(1) = 1\n* L(k+2) = L(k+1) + L(k) + 1\nAs a consequence, the size of any subtree is a Leonardo number. The sequence of sizes stretches decomposing the first ''n'' positions, for any ''n'', can be found in a greedy manner: the first size is the largest Leonardo number not exceeding ''n'', and the remainder (if any) is decomposed recursively. The sizes of stretches are decreasing, strictly so except possibly for two final sizes 1, and avoiding successive Leonardo numbers except possibly for the final two sizes.\n\nIn the initial phase of sorting, an increasingly large initial part of the array is reorganized so that the subtree for each of its stretches is a max-heap: the entry at any non-leaf position is at least as large as the entries at the positions that are its children. In addition, the subsequence of root (rightmost) entries of the stretches is kept increasing; this ensures in particular that the final root holds a maximal entry among the interval considered so far. In the second phase one shrinks the interval of interest back to smaller and smaller initial parts of the array, maintaining the same relations as before as the decomposition of the part considered into stretches evolves. At the point where a position disappears from the part under consideration due to shrinking, its entry is maximal among those in the part; as this is true at each shrinking step, those entries are left to sit in their proper position in the sorted result.\n\nThe rearrangements necessary to ensure the required relations at all times, and thereby realize the eventual sorting of the array, are as follows. During the first \"growing\" phase, the heap relation must be enforced whenever two final stretches are combined with a new root to a single stretch; then (also in the case where a singleton stretch was added) the new root has to be compared to any previous roots of stretches to ensure that the subsequence remains increasing, basically by performing an insertion step on the subsequence by repeated swaps with predecessors; and finally, if the new root was moved backwards in this insertion, the max-heap property must be restored for the subtree where it ended (because the entry at the root position has decreased). During the \"shrinking\" phase, the only worry is keeping the subsequence of roots of stretches increasing as positions are added to and removed from the subsequence. This automatic whenever a stretch of length 1 is removed; however when a longer stretch breaks up and leaves two smaller stretches in its place, the values at their roots are unrelated to the entries at preceding roots, so preserving increase of the subsequence requires two insertion steps, each one (as before) followed by restoring the max-heap property in the stretch where the new entry ends up, unless it remained in its final position.\n\nTo this general plan, a few refinements are applied to arrive at Dijkstra's sorting procedure. Notably, in the growing phase, ensuring the max-heap property at the root of a newly formed stretch can be combined with ordering it relative to the root of the preceding stretch: if the latter is larger than the new root, then either it also dominates both children of the new root, in which case swapping the roots also repairs the max-heap property, or else swapping the new root with its larger child also repairs the relation with the previous stretch root (but the repair of the max-heap property still needs to propagate down the heap). All in all, at most one series of swaps needs to propagate through the structure during each growing step; by contrast, a shrinking step may as mentioned involve either no such series or two of them.\n\nImplementation of the implicit tree structures could be easily done by computing once and for all a list of Leonardo numbers and tables for parent and child relations (which do not depend at all on the values to be sorted). But in order to make this qualify as an in-place sorting algorithm, Dijkstra maintains in a slick way a fixed number of integer variables in such a way that at each point in the computation gives access to the relevant information, without requiring any tables. While doing so does not affect complexity, it does take up a large part of the code for the algorithm, and makes the latter more difficult to understand.\n\n==Operations==\n\nWhile the two phases of the sorting procedure are opposite to each other as far as the evolution of the sequence-of-heaps structure is concerned, the operations they need to perform to ensure the invariants of the structure have much in common. All these operations are variations of the \"sift\" operation in a binary max-heap, which restores the heap invariant when it is possibly violated only at the root node. Restoring the increase among the roots of the stretches is obtained by considering the root of each stretch except the first to have as an additional child (Dijkstra uses the term \"stepson\") the root of the stretch to its left.\n\n===Growing the heap region by incorporating an element to the right===\n\nWhen an additional element is considered for incorporation into the sequence of stretches (list of disjoint heap structures) it either forms a new stretch of its own added to the sequence, or it combines the two rightmost stretches by becoming parent of both their roots and forming a new stretch that replaces the two in the sequence. Which of the two happens depends only on the sizes of the stretches currently present (and ultimately only on the index of the element added); Dijkstra stipulated that stretches are combined if and only if their sizes are {{nowrap|L(k+1)}} and L(k) for some k, i.e., consecutive Leonardo numbers; the new stretch will have size L(k+2).\n\nAfter a new element ''x'' is incorporated into the region of heap structures, as the root of a final stretch that is either a new singleton or obtained by combining two previous stretches, the following procedure called \"trinkle\" will restore the required relations. The element ''x'' is repeatedly swapped with the root of the previous stretches, as long as this condition holds: there is a stretch immediately to the left of the stretch of ''x'', whose root is greater than ''x'', and ''also'' greater than both of the children of ''x'', if ''x'' has them. Note that at this time no ordering relation between ''x'' and its children has yet been established, and if the largest of its children exceeds ''x'', then it is that child that will become the root once the max-heap property is ensured. After thus moving ''x'' into the appropriate stretch, the heap property of the tree for that stretch is established by \"sifting down\" the new element to its correct position. This is the same operation as used in heapsort, adapted to the different implicit tree structure used here. It proceeds as follows: while ''x'' is not at a leaf position, and its greater child exceeds ''x'', swap ''x'' with that child and continue sift-down.\n\nBecause there are {{math|''O''(log ''n'')}} stretches, whose tree structures have depth {{math|''O''(log ''n'')}}, the complexity of trinkle is bounded by {{math|''O''(log ''n'')}}.\n\nThe trees used are systematically slightly unbalanced (any left subtree has depth one more than the corresponding right subtree, except when both are singletons); still, the sift-down operation remains somewhat simpler than one that can handle general binary heaps, since each node has either two children or none; there are fewer cases to distinguish.\n\n====Optimization====\n\nWith respect to the description above, Dijkstra's algorithm implements the following improvement.\n\nThe decision of how to handle the incorporation of a new element is postponed until the type of the next element is inspected: if either that next element is the parent of the current element (having it as right child), or is a leaf but the parent of the current element still is within the range of the array being sorted (when it comes along it will take the current element as left child), then instead of trinkle a simple sift within its subtree is done, avoiding comparison with previous roots. For the final element of the array trinkle is always applied. Thus during the beginning of the growing phase, one is most often just applying sift-down in a single stretch; only those \"stepson\" relations are considered (by trinkle) that will continue to hold until the end of the growing phase.\n\n===Shrinking the heap region by separating the rightmost element from it===\n\nDuring this phase, the form of the sequence of stretches goes through the changes of the growing phase in reverse. No work at all is needed when separating off a leaf node, but for a non-leaf node its two children become roots of new stretches, and need to be moved to their proper place in the sequence of roots of stretches. This can be obtained by applying trinkle first for the left child, and then for the right child.\n\n====Optimization====\nIn this description, one knows that at the position where trinkle starts the heap property is already valid. So I can simplify by, instead of doing the tests that trinkle needs to do, just compare and possibly swap the element with the root before it (if any), and afterwards apply trinkle at its new position if a swap was actually done. Since a swap may invalidate the heap property at the new position where the smaller root moved to, the simplification only applies to the first step.\n\n==Analysis==\nSmoothsort takes {{math|''O''(''n'')}} time to process a presorted array and {{math|''O''(''n'' log ''n'')}} in the worst case, and achieves nearly-linear performance on many nearly-sorted inputs.  However, it does not handle all nearly-sorted sequences optimally.  Using the count of inversions as a measure of un-sortedness (the number of pairs of indices {{mvar|i}} and {{mvar|j}} with {{math|''i'' < ''j''}} and {{math|''A''[''i''] > ''A''[''j'']}}; for randomly sorted input this is approximately {{math|''n''<sup>2</sup>/4}}), there are possible input sequences with {{math|''O''(''n'' log ''n'')}} inversions which cause it to take {{math|Ω(''n'' log ''n'')}} time, whereas other [[adaptive sort|adaptive sorting]] algorithms can solve these cases in {{math|''O''(''n'' log log ''n'')}} time.<ref name=\"hertel\">{{cite journal |last=Hertel |first=Stefan |title=Smoothsort's behavior on presorted sequences |journal=[[Information Processing Letters]] |volume=16 |issue=4 |date=13 May 1983 |pages=165–170 |url=http://scidok.sulb.uni-saarland.de/volltexte/2011/4062/pdf/fb14_1982_11.pdf |doi=10.1016/0020-0190(83)90116-3}}</ref>\n\nThe smoothsort algorithm needs to be able to hold in memory the sizes of all of the trees in the Leonardo heap.  Since they are sorted by order and all orders are distinct, this is usually done using a [[bit vector]] indicating which orders are present.  Moreover, since the largest order is at most {{math|''O''(log ''n'')}}, these bits can be encoded in {{math|''O''(1)}} machine words, assuming a [[transdichotomous machine model]].\n\nNote that {{math|''O''(1)}} machine words is not the same thing as ''one'' machine word.  A 32-bit vector would only suffice for sizes less than L(32) = 7049155.  A 64-bit vector will do for sizes less than L(64) = 34335360355129 ≈ 2<sup>45</sup>.  In general, it takes 1/log<sub>2</sub>(''[[Golden ratio|φ]]'') ≈ 1.44 bits of vector per bit of size.\n\n==References==\n{{Reflist}}\n\n==External links==\n* [http://www.enterag.ch/hartwig/order/smoothsort.pdf Commented transcription of EWD796a, 16-Aug-1981]\n\n{{Edsger Dijkstra}}\n{{sorting}}\n\n[[Category:Sorting algorithms]]\n[[Category:Comparison sorts]]\n[[Category:Heaps (data structures)]]\n[[Category:Articles with example Java code]]\n[[Category:Edsger W. Dijkstra]]"
    },
    {
      "title": "Sort (C++)",
      "url": "https://en.wikipedia.org/wiki/Sort_%28C%2B%2B%29",
      "text": "{{use dmy dates|date=January 2012}}\n{{lowercase|title=sort}}\n'''sort''' is a [[generic programming|generic]] function in the [[C++ Standard Library]] for doing [[comparison sort]]ing. The function originated in the [[Standard Template Library]] (STL).\n\nThe specific [[sorting algorithm]] is not mandated by the language standard and may vary across implementations, but the [[worst-case]] [[asymptotic analysis|asymptotic]] complexity of the function is specified: a call to {{mono|sort}} must perform [[linearithmic function|{{math|''O''(''N'' log ''N'')}}]] comparisons when applied to a range of {{mvar|N}} elements.<ref>{{cite web |title=Working Draft, Standard for Programming Language C++ |url=http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4296.pdf |page=911 |publisher=[[International Organization for Standardization|ISO]]}}</ref>\n\n==Usage==\nThe {{mono|sort}} function is included from the {{mono|[[algorithm (C++)|&lt;algorithm&gt;]]}} header of the C++ Standard Library, and carries three [[argument (computer science)|argument]]s: {{mono|RandomAccessIterator first, RandomAccessIterator last, Compare comp}}. Here, {{mono|RandomAccessIterator}} is a [[template (C++)|templated]] type that must be a [[iterator|random access iterator]], and {{mono|first}} and {{mono|last}} must define a sequence of values, i.e., {{mono|last}} must be reachable from {{mono|first}} by repeated application of the [[increment operator]] to {{mono|first}}. The third argument, also of a templated type, denotes a comparison predicate. This comparison predicate must define a [[strict weak ordering]] on the elements of the sequence to be sorted. The third argument is optional; if not given, the \"less-than\" ({{mono|<}}) operator is used, which may be [[operator overloading|overloaded]] in C++.\n\nThis code sample sorts a given array of integers (in ascending order) and prints it out. Pointers into the array serve as iterators.\n\n<source lang=\"cpp\">\n#include <algorithm>\n#include <iostream>\nusing namespace std;\nint main() {\n  int array[] = { 23, 5, -10, 0, 0, 321, 1, 2, 99, 30 };\n  size_t size = sizeof(array) / sizeof(array[0]); \n  sort(array, array + size);\n  for (size_t i = 0; i < size; ++i) {\n     cout << array[i] << ' ';\n  }\n  cout << endl;\n}\n</source>\n\nThe same functionality using a {{mono|vector}} container, using its {{mono|begin}} and {{mono|end}} methods to obtain iterators:\n\n<source lang=\"cpp\">\n#include <algorithm>\n#include <iostream>\n#include <vector>\nusing namespace std;\nint main() {\n  vector<int> vec { 23, 5, -10, 0, 0, 321, 1, 2, 99, 30 };\n  sort(vec.begin(), vec.end());\n  for (int i = 0; i < vec.size(); ++i) {\n     cout << vec[i] << ' ';\n  }\n  cout << endl;\n}\n</source>\n\n===Genericity===\n{{mono|sort}} is specified generically, so that it can work on any [[random-access]] [[Container (abstract data type)|container]] and any way of determining that an element {{mono|x}} of such a container should be placed before another element {{mono|y}}.\n\nAlthough generically specified, {{mono|sort}} is not easily applied to ''all'' sorting problems. A particular problem that has been the subject of some study is the following:\n\n* Let {{mono|A}} and {{mono|B}} be two arrays, where there exists some relation between the element {{mono|A[i]}} and the element {{mono|B[i]}} for all valid indices {{mono|i}}.\n* Sort {{mono|A}} while maintaining the relation with {{mono|B}}, i.e., apply the same [[permutation]] to {{mono|B}} that sorts {{mono|A}}.\n* Do the previous without copying the elements of {{mono|A}} and {{mono|B}} into a new array of [[ordered pair|pairs]], sorting, and moving the elements back into the original arrays (which would require {{math|''O''(''n'')}} temporary space).\n\nA solution to this problem was suggested by A. Williams in 2002, who implemented a custom iterator type for pairs of arrays and analyzed some of the difficulties in correctly implementing such an iterator type.<ref>{{cite web |first=Anthony |last=Williams |year=2002 |title=Pairing off iterators |publisher=Just Software Solutions |url=https://www.justsoftwaresolutions.co.uk/articles/pair_iterators.pdf}}</ref> Williams's solution was studied and refined by K. Åhlander.<ref>{{cite conference |first=Krister |last=Åhlander |title=Sorting Out the Relationships Between Pairs of Iterators, Values, and References |conference=Proc. Int'l Conf. Generative Programming: Concepts & Experiences |year=2005 |series=[[Lecture Notes in Computer Science|LNCS]] |volume=3676 |pages=342–356 |citeseerx=10.1.1.184.8947}}</ref>\n\n==Complexity and implementations==\nThe C++ standard requires that a call to {{mono|sort}} performs [[linearithmic function|{{math|''O''(''N'' log ''N'')}}]] comparisons when applied to a range of {{mvar|N}} elements.<ref>{{cite web |title=Working Draft, Standard for Programming Language C++ |url=http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4296.pdf |page=911 |publisher=[[International Organization for Standardization|ISO]]}}</ref>\nIn previous versions of C++, such as [[C++03]], only average complexity was required to be {{math|''O''(''N'' log ''N'')}}.<ref name=\"C++03 25.3.1.1/2\">[[International Organization for Standardization|ISO]]/[[International Electrotechnical Commission|IEC]] (2003). ''[[ISO/IEC 14882|ISO/IEC 14882:2003(E): Programming Languages - C++]] §25.3.1.1 sort [lib.sort]'' para. 2</ref> This was to allow the use of algorithms like (median-of-3) [[quicksort]], which are fast in the average case, indeed significantly faster than other algorithms like [[heap sort]] with optimal worst-case complexity, and where the worst-case quadratic complexity rarely occurs.<ref>\"[http://www.cs.rpi.edu/~musser/gp/algorithms.html Generic Algorithms]\", [[David Musser]]</ref> The introduction of [[hybrid algorithm]]s such as [[introsort]] allowed both fast average performance and optimal worst-case performance, and thus the complexity requirements were tightened in later standards.\n\nDifferent implementations use different algorithms. The [[GNU Compiler Collection|GNU Standard C++ library]], for example, uses a 3-part hybrid sorting algorithm: [[introsort]] is performed first (introsort itself being a hybrid of quicksort and heap sort), to a maximum depth given by 2&times;log<sub>2</sub> ''n'', where ''n'' is the number of elements, followed by an [[insertion sort]] on the result.<ref>[https://gcc.gnu.org/onlinedocs/libstdc++/libstdc++-html-USERS-4.4/a01027.html libstdc++ Documentation: Sorting Algorithm]</ref>\n\n==Other types of sorting==\n<tt>sort</tt> is not stable: equivalent elements that are ordered one way before sorting may be ordered differently after sorting. <tt>stable_sort</tt> ensures stability of result at expense of worse performance (in some cases), requiring only [[quasilinear time]] with exponent 2 – O(''n'' log<sup>2</sup> ''n'') – if additional memory is not available, but [[linearithmic time]] O(''n'' log ''n'') if additional memory is available.<ref>[http://en.cppreference.com/w/cpp/algorithm/stable_sort stable_sort]</ref> This allows the use of [[in-place merge sort]] for in-place stable sorting and regular merge sort for stable sorting with additional memory.\n\n[[Partial sorting]] is implemented by {{mono|partial_sort}}, which takes a range of {{mvar|n}} elements and an integer {{math|''m'' < ''n''}}, and reorders the range so that the smallest {{mvar|m}} elements are in the first {{mvar|m}} positions in sorted order (leaving the remaining {{math|''n'' − ''m''}} in the remaining positions, in some unspecified order). Depending on design this may be considerably faster than complete sort. Historically, this was commonly implemented using a [[Binary heap|heap]]-based algorithm that takes {{math|Θ(''n'' + ''m'' log ''n'')}} worst-case time. A better algorithm called [[quickselsort]] is used in the Copenhagen STL implementation, bringing the complexity down to {{math|Θ(''n'' + ''m'' log ''m'')}}.<ref>{{cite conference |last=Martínez |first=Conrado |title=Partial quicksort |conference=Proc. 6th ACM-SIAM Workshop on Algorithm Engineering and Experiments and 1st ACM-SIAM Workshop on Analytic Algorithmics and Combinatorics |year=2004 |url=http://www.lsi.upc.edu/~conrado/research/reports/ALCOMFT-TR-03-50.pdf}}</ref>\n\n[[Selection algorithm|Selection]] of the ''n''th element is implemented by <tt>nth_element</tt>, which actually implements an in-place partial sort: it correctly sorts the ''n''th element, and also ensures that this element partitions so elements before it are less than it, and elements after it are greater than it. There is the requirement that this takes linear time on average, but there is no worst-case requirement; these requirements are exactly met by [[quickselect]], for any choice of pivot strategy.\n\nSome containers, among them <tt>list</tt>, provide specialised version of <tt>sort</tt> as a member function. This is because linked lists don't have random access (and therefore can't use the regular <tt>sort</tt> function); and the specialised version also preserves the values list iterators point to.\n\n==Comparison to qsort==\nAside from {{mono|sort}}, the C++ standard library also includes the {{mono|[[qsort]]}} function from the [[C standard library]]. Compared to {{mono|qsort}}, the templated {{mono|sort}} is more type-safe since it does not require access to data items through unsafe {{mono|void}} pointers, as {{mono|qsort}} does. Also, {{mono|qsort}} accesses the comparison function using a function pointer, necessitating large numbers of repeated function calls, whereas in {{mono|sort}}, comparison functions may be [[inline expansion|inlined]] into the custom object code generated for a template instantiation. In practice, C++ code using {{mono|sort}} is often considerably faster at sorting simple data like integers than equivalent C code using {{mono|qsort}}.<ref>{{cite book\n  | last = Meyers\n  | first = Scott\n  | authorlink = Scott Meyers\n  | coauthors = \n  | title = Effective STL: 50 specific ways to improve your use of the standard template library\n  | publisher = Addison-Wesley\n  | date = 2001\n  | location = \n  | pages = 203\n  | url = \n  | doi = \n  | id = \n  | isbn = 0-201-74962-9}}</ref>\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://en.cppreference.com/w/cpp/algorithm/sort C++ reference for <code>std::sort</code>]\n* [http://www.cplusplus.com/reference/algorithm/sort/ Another C++ reference for <code>std::sort</code>]\n\n[[Category:C++ Standard Library]]\n[[Category:Sorting algorithms]]"
    },
    {
      "title": "Sort (Unix)",
      "url": "https://en.wikipedia.org/wiki/Sort_%28Unix%29",
      "text": "{{lowercase}}\n{{Infobox Software \n| name                   = sort\n| logo                   = \n| screenshot             = Sortunix.png\n| screenshot size        = \n| caption                = The {{code|sort}} command\n| developer              = [[AT&T Bell Laboratories]]\n| released               = {{Start date and age|1971|11|3}}\n| latest release version = \n| latest release date    = \n| operating system       = [[Unix]] and [[Unix-like]], [[MSX-DOS]]\n| genre                  = [[Command (computing)|Command]]\n| license                = [[coreutils]]: [[GNU General Public License#Version 3|GNU GPL v3]]\n| website                = \n}}\nIn [[computing]], '''sort''' is a standard [[command line]] program of [[Unix-like operating system]]s, that prints the lines of its input or concatenation of all [[computer file|files]] listed in its [[command-line argument|argument list]] in sorted order. Sorting is done based on one or more sort keys extracted from each line of input. By default, the entire input is taken as sort key. Blank space is the default field separator. The command supports a number of [[Command-line interface#Command-line option|command-line options]] that can vary by implementation. For instance the \"<code>-r</code>\" flag will reverse the sort order.\n\n==History==\nSort was part of [[Version 1 Unix]]. By [[Version 4 Unix|Version 4]] [[Ken Thompson]] had modified it to use [[Unix pipes|pipes]], but sort retained an option to name the output file because it was used to sort a file in place. In [[Version 5 Unix|Version 5]], Thompson invented \"-\" to represent [[standard input]].<ref name=\"reader\">{{cite techreport |first1=M. D. |last1=McIlroy |authorlink1=Doug McIlroy |year=1987 |url=http://www.cs.dartmouth.edu/~doug/reader.pdf |title=A Research Unix reader: annotated excerpts from the Programmer's Manual, 1971–1986 |series=CSTR |number=139 |institution=Bell Labs}}</ref>\n\nThe version of {{Mono|sort}} bundled in [[GNU]] [[coreutils]] was written by Mike Haertel and Paul Eggert.<ref>https://linux.die.net/man/1/sort</ref> This implementation employs the [[merge sort]] algorithm.\n\nA {{Mono|sort}} command is also part of [[ASCII Corporation|ASCII]]'s ''MSX-DOS2 Tools'' for [[MSX-DOS]] version 2.<ref>[https://ia902205.us.archive.org/5/items/MSXDOS2TOOLS/MSX-DOS2%20TOOLS.pdf MSX-DOS2 Tools User's Manual by ASCII Corporation]</ref>\n\n==Syntax==\n sort [OPTION]... [FILE]...\n\nWith no <code>FILE</code>, or when <code>FILE</code> is <code>-</code>, the command reads from [[standard input]].\n\n===Parameters===\n{| class=\"wikitable\" border=\"6\"\n| '''-b'''\n| Ignores leading blanks.\n|-\n| '''-d'''\n| Considers only blanks and alphanumeric characters.\n|-\n| '''-f'''\n| Fold lower case to upper case characters.\n|-\n| '''-g'''\n| Compares according to general numerical value.\n|-\n| '''-i'''\n| Considers only printable characters.\n|-\n| '''-M'''\n| Compares (unknown) < 'JAN' < ... < 'DEC'.\n|-\n| '''-h'''\n| Compare human readable numbers (e.g., 2K 1G).\n|-\n| '''-n'''\n| Compares according to string numerical value.\n|-\n| '''-R'''\n| Shuffles, but groups identical keys. See also: [[shuf]]\n|-\n| '''-r'''\n| Reverses the result of comparisons.\n|}\n\n==Examples==\n\n===Sort a file in alphabetical order===\n $ [[cat (Unix)|cat]] ''phonebook''\n Smith, Brett     555-4321\n Doe, John        555-1234\n Doe, Jane        555-3214\n Avery, Cory      555-4132\n Fogarty, Suzie   555-2314\n \n $ '''sort''' ''phonebook''\n Avery, Cory      555-4132\n Doe, Jane        555-3214\n Doe, John        555-1234\n Fogarty, Suzie   555-2314\n Smith, Brett     555-4321\n\n===Sort by number===\nThe <code>-n</code> option makes the program sort according to numerical value. The {{mono|[[du (Unix)|du]]}} command produces output that starts with a number, the file size, so its output can be piped to {{mono|sort}} to produce a list of files sorted by (ascending) file size:\n<source lang=\"console\">\n$ du /bin/* | sort -n\n4       /bin/domainname\n24      /bin/ls\n102     /bin/sh\n304     /bin/csh\n</source>\n\n===Columns or fields===\nUse the <code>-k</code> option to sort on a certain column. For example, use \"<code>-k 2</code>\" to sort on the second column. In old versions of sort, the <code>+1</code> option made the program sort on the second column of data (<code>+2</code> for the third, etc.). This usage is deprecated.\n $ [[cat (Unix)|cat]] ''zipcode''\n Adam  12345\n Bob   34567\n Joe   56789\n Sam   45678\n Wendy 23456\n    \n $ '''sort''' -k 2n ''zipcode''\n Adam  12345\n Wendy 23456\n Bob   34567\n Sam   45678\n Joe   56789\n\n===Sort on multiple fields===\nThe <code>-k m,n</code> option lets you sort on a key that is potentially composed of multiple fields (start at column <code>m</code>, end at column <code>n</code>):\n $ [[cat (Unix)|cat]] ''quota''\n fred 2000\n bob 1000\n an 1000\n chad 1000\n don 1500\n eric 500\n \n $ '''sort''' -k2n,2 -k1,1 ''quota''\n eric 500\n an 1000\n bob 1000\n chad 1000\n don 1500\n fred 2000\n\nHere the first sort is done using column 2. <code>-k2,2</code> specifies sorting on the key starting and ending with column 2.  If <code>-k2</code> is used instead, the sort key would begin at column 2 and extend to the end of the line, spanning all the fields in between. The <code>n</code> stands for 'numeric ordering'. <code>-k1,1</code> dictates breaking ties using the value in column 1, sorting alphabetically by default. Note that bob, and chad have the same quota and are sorted alphabetically in the final output.\n\n===Sorting a pipe delimited file===\n $ sort -k2,2,-k1,1 ''zipcode''\n Adam|12345\n Wendy|23456\n Sam|45678\n Joe|56789\n Bob|34567\n\n===Sorting a tab delimited file===\nSorting a file with [[tab separated values]] requires a [[tab character]] to be specified as the column delimiter. This illustration uses the shell's dollar-quote notation<ref>\n{{cite web|title=The GNU Bash Reference Manual, for Bash, Version 4.2: Section 3.1.2.4 ANSI-C Quoting|url=https://www.gnu.org/software/bash/manual/bashref.html#ANSI_002dC-Quoting|accessdate=1 February 2013|date=28 December 2010|publisher=Free Software Foundation, Inc.|quote=Words of the form $'string' are treated specially. The word expands to string, with backslash-escaped characters replaced as specified by the ANSI C standard.}}</ref><ref name=\"KornShell 93\">\n{{cite web|title=KornShell FAQ|url=http://www2.research.att.com/~astopen/download/ksh/faq.html|accessdate=3 March 2015|archiveurl=https://web.archive.org/web/20130527195150/http://www2.research.att.com/~gsf/download/ksh/faq.html|archivedate=2013-05-27|deadurl=no|first1=Glenn S.|last1=Fowler|first2=David G.|last2=Korn|authorlink2=David_Korn_(computer_scientist)|first3=Kiem-Phong|last3=Vo|quote=The $'...' string literal syntax was added to ksh93 to solve the problem of entering special characters in scripts. It uses ANSI-C rules to translate the string between the '...'.|df=}}</ref>\nto specify the tab as a [[Escape sequences in C|C escape sequence]].\n<source lang=\"console\">\n$ sort -k2,2 -t $'\\t' phonebook \nDoe, John\t555-1234\nFogarty, Suzie\t555-2314\nDoe, Jane\t555-3214\nAvery, Cory\t555-4132\nSmith, Brett\t555-4321\n</source>\n\n===Sort in reverse===\nThe <code>-r</code> option just reverses the order of the sort:\n $ '''sort''' -rk 2n ''zipcode''\n Joe   56789\n Sam   45678\n Bob   34567\n Wendy 23456\n Adam  12345\n\n===Sort in random===\nThe GNU implementation has a <tt>-R</tt>/<tt>--random-sort</tt> option based on hashing; this is not a full random shuffle because it will sort identical lines together. A true random sort is provided by the Unix utility [[shuf]].\n\n==See also==\n* [[Collation]]\n* [[List of Unix commands]]\n* <tt>[[uniq]]</tt>\n* <tt>[[shuf]]</tt>\n\n==References==\n{{Reflist}}\n\n==External links==\n*[http://man7.org/linux/man-pages/man1/sort.1.html Sort manpage] The program's [[manpage]]\n*[http://softpanorama.org/Tools/sort.shtml Softpanorama Unix sort page]\n\n{{Unix commands}}\n\n[[Category:Computing commands]]\n[[Category:Sorting algorithms]]\n[[Category:Unix text processing utilities]]\n[[Category:Unix SUS2008 utilities]]"
    },
    {
      "title": "Sorting",
      "url": "https://en.wikipedia.org/wiki/Sorting",
      "text": "{{about|the process||Sort (disambiguation)}}\n{{selfref|On Wikipedia, SORTKEY may refer to [[Wikipedia:SORTKEY]].}}\n[[Image:Metal movable type.jpg|right|300px|thumb|Sorting type]]\n'''Sorting''' is any process of arranging items systematically, and has two common, yet distinct meanings:\n# [[Collating order|ordering]]: arranging items in a sequence ordered by some criterion;\n# [[categorization|categorizing]]: grouping items with similar properties.\n\n=={{anchor|Sort/Merge}}Sorting information or data==\nIn {{visible anchor|[[computer science]]}}, arranging in an ordered sequence is called \"sorting\". Sorting is a common operation in many applications, and efficient [[algorithms]] to perform it have been developed.\n\nThe most common uses of sorted sequences are:\n* making [[search algorithm|lookup or search]] efficient;\n* making [[merge algorithm|merging of sequences]] efficient.\n* enable [[data processing|processing of data]] in a defined order.\n\nThe opposite of sorting, rearranging a sequence of items in a random or meaningless order, is called [[shuffling]].\n\nFor sorting, either a weak order, \"should not come after\", can be specified, or a [[strict weak order]], \"should come before\" (specifying one defines also the other, the two are the complement of the inverse of each other, see [[Binary relation#Operations on binary relations|operations on binary relations]]). For the sorting to be unique, these two are restricted to a [[total order]] and a strict total order, respectively.\n\nSorting [[n-tuple]]s (depending on context also called e.g. [[Object composition|record]]s consisting of fields) can be done based on one or more of its components. More generally objects can be sorted based on a property. Such a component or property is called a '''sort key'''.\n\nFor example, the items are books, the sort key is the title, subject or author, and the order is alphabetical.\n\nA new sort key can be created from two or more sort keys by [[lexicographical order]]. The first is then called the '''primary sort key''', the second the '''secondary sort key''', etc.\n\nFor example, addresses could be sorted using the city as primary sort key, and the street as secondary sort key.\n\nIf the sort key values are [[total order|totally ordered]], the sort key defines a [[strict weak ordering#Total preorders|weak order]] of the items: items with the same sort key are equivalent with respect to sorting. See also [[Sorting algorithm#Stability|stable sorting]]. If different items have different sort key values then this defines a unique order of the items.\n\n[[File:Bundesarchiv Bild 183-22350-0001, Berlin, Postamt O 17, Päckchenverteilung.jpg|thumb|Workers sorting parcels in a postal facility]]\nA standard order is often called ''ascending'' (corresponding to the fact that the standard order of numbers is ascending, i.e. A to Z, 0 to 9), the reverse order ''descending'' (Z to A, 9 to 0).  For dates and times, ''ascending'' means that earlier values precede later ones e.g. 1/1/2000 will sort ahead of 1/1/2001.\n\n===Common sorting algorithms===\n{{main|Sorting algorithm}}\n\n*'''Bubble/Shell sort''' : Exchange two adjacent elements if they are out of order. Repeat until array is sorted.\n*'''Insertion sort''' : Scan successive elements for an out-of-order item, then insert the item in the proper place. \n*'''Selection sort''' : Find the smallest (or biggest) element in the array, and put it in the proper place. Swap it with the value in the first position. Repeat until array is sorted.\n*'''Quick sort''' : Partition the array into two segments. In the first segment, all elements are less than or equal to the pivot value. In the second segment, all elements are greater than or equal to the pivot value. Finally, sort the two segments recursively.\n*'''Merge sort''' : Divide the list of elements in two parts, sort the two parts individually and then merge it.\n\n==Physical sorting processes==\n[[File:Potomac Yard - aerial 1980s.jpg|thumb|A railroad [[classification yard]], used for sorting [[freight car]]s]]\nVarious sorting tasks are essential in industrial processes. For example, during the extraction of [[gold]] from ore, a device called a [[shaker table]] uses [[gravity]], [[oscillation|vibration]], and flow to separate gold from lighter materials in the ore (sorting by size and weight). Sorting is also a naturally occurring process that results in the concentration of [[ore]] or [[Sorting (sediment)|sediment]]. Sorting results from the application of some criterion or differential stressor to a mass to separate it into its components based on some variable quality. Materials that are different, but only slightly so, such as the isotopes of uranium, are very difficult to separate.\n\n[[Optical sorting]] is an automated process of sorting solid products using cameras and/or lasers and has widespread use in the food industry.\n\n==See also==\n* [[Help:Sorting]] in Wikipedia tables. For sorting of categories, see [[Wikipedia:Categorization#Sort keys]] and for sorting of article sections, see [[WP:ORDER]]\n* [[Collation]]\n* [[IBM mainframe utility programs#IBM SORT|IBM mainframe sort/merge]]\n* [[Unicode collation algorithm]]\n\n==External links==\n{{wiktionary|sort|sorting}}\n* [https://web.archive.org/web/20061008105719/http://www.cs.ubc.ca/~harrison/Java/sorting-demo.html Demonstration of Sorting Algorithms] (includes bubble and quicksort)\n* [https://www.youtube.com/watch?v=vxENKlcs2Tw Animated video] explaining bubble sort and quick sort and compares their performance.\n{{commons category}}\n\n[[Category:Sorting algorithms]]\n[[Category:Data processing]]"
    },
    {
      "title": "Sorting network",
      "url": "https://en.wikipedia.org/wiki/Sorting_network",
      "text": "[[File:SimpleSortingNetwork2.svg|thumb|250px|A simple sorting network consisting of four wires and five connectors]]\n\nIn [[computer science]], '''comparator networks''' are abstract devices built up of a fixed number of \"wires\", carrying values, and comparator modules that connect pairs of wires, swapping the values on the wires if they are not in a desired order. Such networks are typically designed to perform [[sorting algorithm|sorting]] on fixed numbers of values, in which case they are called '''sorting networks'''.\n\nSorting networks differ from general [[comparison sort]]s in that they are not capable of handling arbitrarily large inputs, and in that their sequence of comparisons is set in advance, regardless of the outcome of previous comparisons. This independence of comparison sequences is useful for parallel execution and for implementation in [[computer hardware|hardware]]. Despite the simplicity of sorting nets, their theory is surprisingly deep and complex. Sorting networks were first studied circa 1954 by Armstrong, Nelson and O'Connor,<ref name=\"knuth\"/> who subsequently patented the idea.<ref>{{cite patent |country=US |number=3029413 |title=Sorting system with {{mvar|n}}-line sorting switch |pubdate=10 April 1962 |fdate= 21 February 1957 |inventor1-first=Daniel G. |inventor1-last=O'Connor |inventor2-first=Raymond J. |inventor2-last=Nelson}}</ref>\n\nSorting networks can be implemented either in [[computer hardware|hardware]] or in [[software]]. [[Donald Knuth]] describes how the comparators for binary integers can be implemented as simple, three-state electronic devices.<ref name=\"knuth\"/> [[Ken Batcher|Batcher]], in 1968, suggested using them to construct [[switch|switching networks]] for computer hardware, replacing both [[Bus (computing)|buses]] and the faster, but more expensive, [[crossbar switch]]es.<ref>{{cite conference |first=K. E. |last=Batcher |url=http://www.cs.kent.edu/~batcher/sort.ps |title=Sorting networks and their applications |conference=Proc. AFIPS Spring Joint Computer Conference |pages=307–314 |year=1968}}</ref> Since the 2000s, sorting nets (especially [[bitonic sorter|bitonic mergesort]]) are used by the [[General-purpose computing on graphics processing units|GPGPU]] community for constructing sorting algorithms to run on [[graphics processing unit]]s.<ref>{{Cite journal | doi = 10.1109/JPROC.2008.917757| title = GPU Computing| journal = Proceedings of the IEEE| volume = 96| issue = 5| pages = 879–899| year = 2008| last1 = Owens | first1 = J. D. | last2 = Houston | first2 = M.| last3 = Luebke | first3 = D.| last4 = Green | first4 = S.| last5 = Stone | first5 = J. E. | last6 = Phillips | first6 = J. C. }}</ref>\n\n==Introduction==\n[[File:Sorting-network-comparator-demonstration.svg|thumb|150px|Demonstration of a comparator in a sorting network.]]\nA sorting network consists of two types of items: comparators and wires. The wires are thought of as running from left to right, carrying values (one per wire) that traverse the network all at the same time. Each comparator connects two wires. When a pair of values, traveling through a pair of wires, encounter a comparator, the comparator swaps the values [[if and only if]] the top wire's value is greater than the bottom wire's value.\n\nIn a formula, if the top wire carries {{mvar|x}} and the bottom wire carries {{mvar|y}}, then after hitting a comparator the wires carry <math>x' = \\min(x, y)</math> and <math>y' = \\max(x, y)</math>, respectively, so the pair of values is sorted.<ref name=\"clrs\">{{Introduction to Algorithms|1}}</ref>{{rp|635}} A network of wires and comparators that will correctly sort all possible inputs into ascending order is called a sorting network.\n\nThe full operation of a simple sorting network is shown below. It is easy to see why this sorting network will correctly sort the inputs; note that the first four comparators will \"sink\" the largest value to the bottom and \"float\" the smallest value to the top. The final comparator simply sorts out the middle two wires.\n\n[[File:SimpleSortingNetworkFullOperation.svg|650px]]\n\n===Depth and efficiency===\nThe efficiency of a sorting network can be measured by its total size, meaning the number of comparators in the network, or by its ''depth'', defined (informally) as the largest number of comparators that any input value can encounter on its way through the network. Noting that sorting networks can perform certain comparisons [[Parallel computing|in parallel]] (represented in the graphical notation by comparators that lie on the same vertical line), and assuming all comparisons to take unit time, it can be seen that the depth of the network is equal to the number of time steps required to execute it.<ref name=\"clrs\"/>{{rp|636–637}}\n\n=== Insertion and Bubble networks ===\nWe can easily construct a network of any size recursively using the principles of insertion and selection. Assuming we have a sorting network of size ''n'', we can construct a network of size {{nowrap|''n'' + 1}} by \"inserting\" an additional number into the already sorted subnet (using the principle behind [[insertion sort]]). We can also accomplish the same thing by first \"selecting\" the lowest value from the inputs and then sort the remaining values recursively (using the principle behind [[bubble sort]]).[[File:Recursive-bubble-sorting-network.svg|200px|thumb|A sorting network constructed recursively that first sinks the largest value to the bottom and then sorts the remaining wires. Based on [[bubble sort]]]]\n{|\n| style=\"vertical-align: top;\" |\n| style=\"vertical-align: top;\" |[[File:Recursive-insertion-sorting-network.svg|200px|thumb|A sorting network constructed recursively that first sorts the first n wires, and then inserts the remaining value. Based on [[insertion sort]]]]\n|}The structure of these two sorting networks are very similar. A construction of the two different variants, which collapses together comparators that can be performed simultaneously shows that, in fact, they are identical.<ref name=\"knuth\"/> \n{|\n|style=\"vertical-align: top;\"| [[File:Six-wire-bubble-sorting-network.svg|200px|thumb|Bubble sorting network]]\n|style=\"vertical-align: top;\"| [[File:Six-wire-insertion-sorting-network.svg|200px|thumb|Insertion sorting network]]\n|style=\"vertical-align: top;\"| [[File:Six-wire-pyramid-sorting-network.svg|200px|thumb|When allowing for parallel comparators, bubble sort and insertion sort are identical]]\n|}\n\nThe insertion network (or equivalently, bubble network) has a depth of {{math|2''n'' - 3}}.<ref name=\"knuth\"/> This is better than the {{math|''O''(''n'' log ''n'')}} time needed by [[Random-access machine|random-access machines]], but it turns out that there are much more efficient sorting networks with a depth of just {{math|''O''(log<sup>2</sup> ''n'')}}, as described [[#Constructing sorting networks|below]].\n\n===Zero-one principle===\n\nWhile it is easy to prove the validity of some sorting networks (like the insertion/bubble sorter), it is not always so easy. There are {{math|''n''!}} permutations of numbers in an {{mvar|n}}-wire network, and to test all of them would take a significant amount of time, especially when {{mvar|n}} is large.  The number of test cases can be reduced significantly, to {{math|2<sup>''n''</sup>}}, using the so-called zero-one principle.  While still exponential, this is smaller than {{math|''n''!}} for all {{math|''n'' ≥ 4}}, and the difference grows rapidly with increasing {{mvar|n}}.\n\nThe zero-one principle states that, if a sorting network can correctly sort all {{math|2<sup>''n''</sup>}} sequences of zeros and ones, then it is also valid for arbitrary ordered inputs. This not only drastically cuts down on the number of tests needed to ascertain the validity of a network, it is of great use in creating many constructions of sorting networks as well.\n\nThe principle can be proven by first observing the following fact about comparators: when a [[monotonic function|monotonically increasing]] function {{mvar|f}} is applied to the inputs, i.e., {{mvar|x}} and {{mvar|y}} are replaced by {{math|''f''(''x'')}} and {{math|''f''(''y'')}}, then the comparator produces {{math|min(''f''(''x''), ''f''(''y'')) {{=}} ''f''(min(''x'', ''y''))}} and {{math|max(''f''(''x''), ''f''(''y'')) {{=}} ''f''(max(''x'', ''y''))}}. By [[Mathematical induction|induction]] on the depth of the network, this result can be extended to a [[Lemma (mathematics)|lemma]] stating that if the network transforms the sequence {{math|''a''<sub>1</sub>, ..., ''a''<sub>''n''</sub>}} into {{math|''b''<sub>1</sub>, ..., ''b''<sub>''n''</sub>}}, it will transform {{math|''f''(''a''<sub>1</sub>), ..., ''f''(''a''<sub>''n''</sub>)}} into {{math|''f''(''b''<sub>1</sub>), ..., ''f''(''b''<sub>''n''</sub>)}}. Suppose that some input {{math|''a''<sub>1</sub>, ..., ''a''<sub>''n''</sub>}} contains two items {{math|''a<sub>i</sub>'' < ''a<sub>j</sub>''}}, and the network incorrectly swaps these in the output. Then it will also incorrectly sort {{math|''f''(''a''<sub>1</sub>), ..., ''f''(''a''<sub>''n''</sub>)}} for the function\n\n:<math>\nf(x) = \\begin{cases}\n        1\\ &\\mbox{if } x > a_i \\\\\n        0\\ &\\mbox{otherwise.}\n       \\end{cases}\n</math>\n\nThis function is monotonic, so we have the zero-one principle as the [[Contraposition|contrapositive]].<ref name=\"clrs\"/>{{rp|640–641}}\n\n==Constructing sorting networks==\nVarious algorithms exist to construct simple, yet efficient sorting networks of depth {{math|''O''(log<sup>2</sup> ''n'')}} (hence size {{math|''O''(''n'' log<sup>2</sup> ''n'')}}) such as [[Batcher odd–even mergesort]], [[bitonic sort]], [[Shell sort]], and the [[Pairwise sorting network]]. These networks are often used in practice. It is also possible, in theory, to construct networks of logarithmic depth for arbitrary size, using a construction called the ''AKS network'', after its discoverers [[Miklós Ajtai|Ajtai]], [[János Komlós (mathematician)|Komlós]], and [[Endre Szemerédi|Szemerédi]].<ref>{{Cite conference | doi = 10.1145/800061.808726| title = An {{math|''O''(''n'' log ''n'')}} sorting network| work = Proceedings of the fifteenth annual ACM symposium on Theory of computing | conference = [[Symposium on Theory of Computing|STOC]] '83| pages = 1–9| year = 1983| last1 = Ajtai | first1 = M. |author-link1 = Miklós Ajtai| last2 = Komlós | first2 = J. |author-link2 = János Komlós (mathematician)| last3 = Szemerédi | first3 = E. |author-link3 = Endre Szemerédi| isbn = 0-89791-099-0}}</ref> While an important theoretical discovery, the AKS network has little or no practical application because of the linear constant hidden by the [[Big-O notation]], which is in the \"many, many thousands\".<ref name=\"clrs\"/>{{rp|653}} These are partly due to a construction of an [[expander graph]]. A simplified version of the AKS network was described by [[Michael S. Paterson|Paterson]], who notes that \"the constants obtained for the depth bound still prevent the construction being of practical value\".<ref>{{Cite journal | doi = 10.1007/BF01840378| title = Improved sorting networks with {{math|''O''(log ''N'')}} depth| journal = Algorithmica| volume = 5| issue = 1–4| pages = 75–92| year = 1990| last1 = Paterson | first1 = M. S.}}</ref> Another construction of sorting networks of size {{math|''O''(''n'' log ''n'')}} was discovered by [[Michael T. Goodrich|Goodrich]].<ref>{{cite book|last1=Goodrich|first1=Michael|authorlink1=Michael T. Goodrich|arxiv=1403.2777|title=Zig-zag Sort: A Simple Deterministic Data-Oblivious Sorting Algorithm Running in O(n log n) Time|date=March 2014|doi=10.1145/2591796.2591830|journal=Proceedings of the 46th Annual ACM Symposium on Theory of Computing - STOC '14|pages=684–693|isbn=9781450327107}}</ref> While their size has a much smaller constant factor than that of AKS networks, their depth is {{math|''O''(''n'' log ''n'')}}, which makes them inefficient for parallel implementation.\n\n===Optimal sorting networks===\nFor small, fixed numbers of inputs {{mvar|n}}, ''optimal'' sorting networks can be constructed, with either minimal depth (for maximally parallel execution) or minimal size (number of comparators). These networks can be used to increase the performance of larger sorting networks resulting from the [[divide and conquer algorithm|recursive]] constructions of, e.g., Batcher, by halting the recursion early and inserting optimal nets as base cases.<ref name=\"parberry91\">{{cite journal |first=Ian |last=Parberry |title=A Computer Assisted Optimal Depth Lower Bound for Nine-Input Sorting Networks |journal=Mathematical Systems Theory |volume=24 |pages=101–116 |year=1991 |url=http://larc.unt.edu/ian/pubs/9-input.pdf |doi=10.1007/bf02090393|citeseerx=10.1.1.712.219 }}</ref> The following table summarizes the known optimality results:\n\n{| class=\"wikitable\" style=\"text-align: center;\"\n|-\n! style=\"text-align: left;\" | {{mvar|n}}\n| style=\"width: 20px;\" | 1 || style=\"width: 20px;\" | 2 || style=\"width: 20px;\" | 3 || style=\"width: 20px;\" | 4 || style=\"width: 20px;\" | 5 || style=\"width: 20px;\" | 6 || style=\"width: 20px;\" | 7 || style=\"width: 20px;\" | 8 || style=\"width: 20px;\" | 9 || style=\"width: 20px;\" | 10 || style=\"width: 20px;\" | 11 || style=\"width: 20px;\" | 12 || style=\"width: 20px;\" | 13 || style=\"width: 20px;\" | 14 || style=\"width: 20px;\" | 15 || style=\"width: 20px;\" | 16\n|17\n|- \n! style=\"text-align: left;\" | Depth<ref name=\"codish_end_game\">{{cite conference |last1=Codish |first1=Michael |last2=Cruz-Filipe |first2=Luís |last3=Ehlers |first3=Thorsten |last4=Müller |first4=Mike |last5= Schneider-Kamp |first5=Peter |title=Sorting Networks: to the End and Back Again |arxiv=1507.01428 |year=2015|bibcode=2015arXiv150701428C }}</ref>\n| 0 || 1 || 3 || 3 || 5 || 5 || 6 || 6 || 7 || 7 || 8 || 8 || 9 || 9 || 9 || 9\n|10\n|- \n! style=\"text-align: left;\" | Size, upper bound<ref name=\"codish\"/>\n| 0 || 1 || 3 || 5 || 9 || 12 || 16 || 19 || 25 || 29 || 35 || 39 || 45 || 51 || 56 || 60\n|71\n|- \n! style=\"text-align: left;\" | Size, lower bound (if different)<ref name=\"codish\"/>\n|  ||  ||  ||  ||  ||  ||  ||  ||  ||  || 33 || 37 || 41 || 45 || 49 || 53\n|58\n|}\n\nThe first sixteen depth-optimal networks are listed in Knuth's ''[[The Art of Computer Programming|Art of Computer Programming]]'',<ref name=\"knuth\">{{cite book |first=D. E. |last=Knuth |authorlink=Donald Knuth |title=The Art of Computer Programming, Volume 3: Sorting and Searching |edition=Second |publisher=Addison–Wesley |year=1997 |isbn=978-0-201-89685-5 |pages=219–247|title-link=The Art of Computer Programming }} Section 5.3.4: Networks for Sorting.</ref> and have been since the 1973 edition; however, while the optimality of the first eight was established by [[Robert Floyd|Floyd]] and Knuth in the 1960s, this property wasn't proven for the final six until 2014<ref name=\"bundala_zavodny\">{{Cite book| doi = 10.1007/978-3-319-04921-2_19| title = Optimal Sorting Networks| journal = Language and Automata Theory and Applications| volume = 8370| pages = 236–247| series = Lecture Notes in Computer Science| year = 2014| last1 = Bundala | first1 = D. | last2 = Závodný | first2 = J. | isbn = 978-3-319-04920-5| arxiv = 1310.6271}}</ref> (the cases nine and ten having been decided in 1991<ref name=\"parberry91\"/>).\n\nFor one to ten inputs, minimal (i.e. size-optimal) sorting networks are known, and for higher values, lower bounds on their sizes {{math|''S''(''n'')}} can be derived inductively using a lemma due to Van Voorhis: {{math|''S''(''n'' + 1) ≥ ''S''(''n'') + ⌈log<sub>2</sub>(''n'')⌉}}. All ten optimal networks have been known since 1969, with the first eight again being known as optimal since the work of Floyd and Knuth, but optimality of the cases {{math|''n'' {{=}} 9}} and {{math|''n'' {{=}} 10}} took until 2014 to be resolved.<ref name=\"codish\">{{cite conference |last1=Codish |first1=Michael |last2=Cruz-Filipe |first2=Luís |last3=Frank |first3=Michael |last4= Schneider-Kamp |first4=Peter |title=Twenty-Five Comparators is Optimal when Sorting Nine Inputs (and Twenty-Nine for Ten) |conference=Proc. Int'l Conf. Tools with AI (ICTAI) |arxiv=1405.5754 |year=2014 |pages=186–193|bibcode=2014arXiv1405.5754C }}</ref>\n\nSome work in designing optimal sorting network has been done using [[genetic algorithm]]s: D. Knuth mentions that the best known sorting networks for {{math|''n'' {{=}} 9}} and {{math|''n'' {{=}} 11}} were found by Loren Schwiebert using this method in 2001.<ref name=\"knuth\"/>\n\n===Complexity of testing sorting networks===\n\nIt is unlikely that significant further improvements can be made for testing general sorting networks, unless [[P vs NP|P=NP]], because the problem of testing whether a candidate network is a sorting network is known to be [[co-NP]]-complete.<ref name=parberry>{{cite conference |last=Parberry|first=Ian|title=On the Computational Complexity of Optimal Sorting Network Verification|journal=Proc. PARLE '91: Parallel Architectures and Languages Europe, Volume I: Parallel Architectures and Algorithms, Eindhoven, The Netherlands |year=1991|pages=252–269}}</ref>\n\n==References==\n{{reflist|30em}}\n* O. Angel, A.E. Holroyd, D. Romik, B. Virag, ''[https://arxiv.org/abs/math/0609538 Random Sorting Networks]'', Adv. in Math., 215(2):839&ndash;868, 2007.\n\n==External links==\n*[http://www.iti.fh-flensburg.de/lang/algorithmen/sortieren/networks/sortieren.htm Sorting Networks]\n*[https://web.archive.org/web/20080116050552/http://www.cs.uky.edu/~lewis/essays/algorithms/sortnets/sort-net.html Sorting Networks]\n*[http://pages.ripco.net/~jgamble/nw.html List of Sorting Networks]\n*[http://www.cs.brandeis.edu/~hugues/sorting_networks.html Sorting networks and the END algorithm]\n* {{cite web |url=http://rjlipton.wordpress.com/2014/04/24/galactic-sortning-networks/ |first1=Richard J. |last1=Lipton |authorlink1=Richard J. Lipton |first2=Ken |last2=Regan |date=24 April 2014 |title=Galactic Sorting Networks |website=Gödel’s Lost Letter and P=NP}}\n*[http://optimizacion.cic.ipn.mx/sortingnetworks/ Sorting Networks validity]\n\n{{sorting}}\n\n[[Category:Computer engineering]]\n[[Category:Sorting algorithms]]"
    },
    {
      "title": "Spaghetti sort",
      "url": "https://en.wikipedia.org/wiki/Spaghetti_sort",
      "text": "{{confusing|date=July 2013}}\n\n'''Spaghetti sort''' is a [[linear-time]], [[analog computer|analog]] [[algorithm]] for [[Sorting algorithm|sorting]] a sequence of items, introduced by [[Alexander Dewdney]] in his ''[[Scientific American]]'' column.<ref>{{Citation\n| last = Dewdney\n| first = A. K.\n| author-link = Alexander Dewdney\n| title = On the spaghetti computer and other analog gadgets for problem solving\n| periodical = [[Scientific American]]\n| volume = 250\n| issue = 6\n| pages = 19–26\n\n| date = June 1984}}</ref><ref>{{Citation\n| last = Stauffer\n| first = Dietrich\n| authorlink = Dietrich Stauffer\n| title = Annual Reviews of Computational Physics VI\n| publisher = [[World Scientific]]\n| date = May 15, 1999\n| page = 260\n| isbn = 981-02-3563-1}}</ref><ref>{{Citation\n| last = Adamatzky\n| first = Andrew\n| authorlink = Andrew Adamatzky\n| title = From Utopian to Genuine Unconventional Computers\n| publisher = [[Luniver Press]]\n| date = July 1, 2006\n| page = 96\n| isbn = 0-9551170-9-7}}</ref> This algorithm sorts a sequence of items requiring O(''n'') stack space in a stable manner. It requires a parallel processor.\n\n==Algorithm==\nFor simplicity, assume we are sorting a list of [[natural number]]s. The sorting method is illustrated using uncooked rods of [[spaghetti]]:\n# For each number ''x'' in the list, obtain a rod of length ''x''. (One practical way of choosing the unit is to let the largest number ''m'' in the list correspond to one full rod of spaghetti. In this case, the full rod equals ''m'' spaghetti units. To get a rod of length ''x'', break a rod in two so that one piece is of length ''x'' units; discard the other piece.)\n#Once you have all your spaghetti rods, take them loosely in your fist and lower them to the table, so that they all stand upright, resting on the table surface. Now, for each rod, lower your other hand from above until it meets with a rod—this one is clearly the longest. Remove this rod and insert it into the front of the (initially empty) output list (or equivalently, place it in the last unused slot of the output array). Repeat until all rods have been removed.\n\n==Analysis==\n\nPreparing the ''n'' rods of spaghetti takes linear time. Lowering the rods on the table takes constant time, [[big O notation|O]](''1''). This is possible because the hand, the spaghetti rods and the table work as a fully [[parallel computing]] device. There are then ''n'' rods to remove so, assuming each contact-and-removal operation takes constant time, the worst-case time complexity of the algorithm is O(''n'').\n\n== References ==\n{{reflist}}\n\n==External links==\n* [http://www.csd.uwo.ca/faculty/akd/ A. K. Dewdney's homepage]\n* [http://www.dna.caltech.edu/~woods/download/dw20-UC06-sort.pdf Implementations of a model of physical sorting, Boole Centre for Research in Informatics]\n* [http://iffwww.iff.kfa-juelich.de/~ekoch/talks/qc.pdf Classical/Quantum Computing, IFF-Institute]\n\n{{sorting}}\n\n{{DEFAULTSORT:Spaghetti Sort}}\n[[Category:Sorting algorithms]]\n[[Category:Metaphors referring to spaghetti]]"
    },
    {
      "title": "Splaysort",
      "url": "https://en.wikipedia.org/wiki/Splaysort",
      "text": "In [[computer science]], '''splaysort''' is an [[adaptive sort|adaptive]] [[comparison sort]]ing [[algorithm]] based on the [[splay tree]] [[data structure]].<ref name=\"mep\">{{citation\n | last1 = Moffat | first1 = Alistair\n | last2 = Eddy | first2 = Gary\n | last3 = Petersson | first3 = Ola\n | date = July 1996\n | doi = 10.1002/(SICI)1097-024X(199607)26:7<781::AID-SPE35>3.3.CO;2-2\n | issue = 7\n | journal = Software Practice and Experience\n | pages = 781–797\n | title = Splaysort: Fast, Versatile, Practical\n | volume = 26}}</ref>\n\n==Algorithm==\nThe steps of the algorithm are:\n# Initialize an empty splay tree\n# For each data item in the input order, insert it into the splay tree\n# Traverse the splay tree in [[inorder]] to find the sorted order of the data\nThus, the algorithm may be seen as a form of [[insertion sort]] or [[tree sort]], using a splay tree to speed up each insertion.\n\n==Analysis==\nBased on the [[amortized analysis]] of splay trees, the worst case running time of splaysort, on an input with ''n'' data items, is ''O''(''n''&nbsp;log&nbsp;''n''), matching the time bounds for efficient non-adaptive algorithms such as [[quicksort]], [[heap sort]], and [[merge sort]].\n\nFor an input sequence in which most items are placed close to their predecessor in the sorted order, or are out of order with only a small number of other items, splaysort can be faster than ''O''(''n''&nbsp;log&nbsp;''n''), showing that it is an [[adaptive sort]]. To quantify this, let ''d''<sub>''x''</sub> be the number of positions in the input that separate ''x'' from its predecessor, and let ''i''<sub>''x''</sub> be the number of items that appear on one side of ''x'' in the input and on the other side of ''x'' in the output (the number of [[Inversion (discrete mathematics)|inversions]] that involve ''x''). Then it follows from the dynamic finger theorem for splay trees that the total time for splaysort is bounded by\n:<math>\\sum_x \\log d_x</math>\nand by\n:<math>\\sum_x \\log i_x</math>.<ref>{{citation\n | last = Cole | first = Richard\n | doi = 10.1137/S009753979732699X\n | issue = 1\n | journal = [[SIAM Journal on Computing]]\n | mr = 1762706\n | pages = 44–85\n | title = On the dynamic finger conjecture for splay trees. II. The proof\n | volume = 30\n | year = 2000| citeseerx = 10.1.1.36.2713\n }}.</ref>\n\nSplaysort can also be shown to be adaptive to the [[Entropy (information theory)|entropy]] of the input sequence.<ref>{{citation\n | last = Gagie | first = Travis\n | arxiv = cs/0506027\n | title = Sorting a low-entropy sequence\n | year = 2005| bibcode = 2005cs........6027G}}.</ref>\n\n==Experimental results==\nIn experiments by {{harvtxt|Moffat|Eddy|Petersson|1996}}, splaysort was slower than quicksort on tables of random numbers by a factor of 1.5 to 2, and slower than mergesort by smaller factors. For data consisting of larger records, again in a random order, the additional amount of data movement performed by quicksort significantly slowed it down compared to pointer-based algorithms, and the times for splaysort and mergesort were very close to each other. However, for nearly presorted input sequences (measured in terms of the number of contiguous monotone subsequences in the data, the number of inversions, the number of items that must be removed to make a sorted subsequence, or the number of non-contiguous monotone subsequences into which the input can be partitioned) splaysort became significantly more efficient than the other algorithms.<ref name=\"mep\"/>\n\n{{harvtxt|Elmasry|Hammad|2005}} compared splaysort to several other algorithms that are adaptive to the total number of inversions in the input, as well as to quicksort. They found that, on the inputs that had few enough inversions to make an adaptive algorithm faster than quicksort, splaysort was the fastest algorithm.<ref>{{citation\n | last1 = Elmasry | first1 = Amr\n | last2 = Hammad | first2 = Abdelrahman\n | contribution = An empirical study for inversions-sensitive sorting algorithms\n | doi = 10.1007/11427186_52\n | pages = 597–601\n | publisher = Springer\n | series = [[Lecture Notes in Computer Science]]\n | title = Experimental and Efficient Algorithms: 4th International Workshop, WEA 2005, Santorini Island, Greece, May 10-13, 2005, Proceedings\n | volume = 3503\n | year = 2005}}.</ref>\n\n==Variations==\n{{harvtxt|Saikkonen|Soisalon-Soininen|2012}} modify splaysort to be more strongly adaptive to the number of contiguous monotone subsequences in the input, and report on experiments showing that the resulting algorithm is faster on inputs that are nearly presorted according to this measure.<ref>{{citation\n | last1 = Saikkonen | first1 = Riku\n | last2 = Soisalon-Soininen | first2 = Eljas\n | contribution = A general method for improving insertion-based adaptive sorting\n | doi = 10.1007/978-3-642-35261-4_25\n | pages = 217–226\n | publisher = Springer\n | series = Lecture Notes in Computer Science\n | title = Algorithms and Computation: 23rd International Symposium, ISAAC 2012, Taipei, Taiwan, December 19-21, 2012, Proceedings\n | volume = 7676\n | year = 2012}}.</ref>\n\n==References==\n{{reflist}}\n\n{{Sorting}}\n\n[[Category:Sorting algorithms]]"
    },
    {
      "title": "Spreadsort",
      "url": "https://en.wikipedia.org/wiki/Spreadsort",
      "text": "{{multiple|\n{{COI|date=August 2017}}\n{{essay-like|date=August 2017}}\n{{primary sources|date=July 2017}}\n}}\n'''Spreadsort''' is a [[sorting algorithm]] invented by Steven J. Ross in 2002.<ref>Steven J. Ross. The Spreadsort High-performance General-case Sorting Algorithm. ''Parallel and Distributed Processing Techniques and Applications'', Volume 3, pp. 1100&ndash;1106. Las Vegas Nevada. 2002.</ref> It combines concepts from distribution-based sorts, such as [[radix sort]] and [[bucket sort]], with partitioning concepts from comparison sorts such as [[quicksort]] and [[mergesort]]. In experimental results it was shown to be highly efficient, often outperforming traditional algorithms such as quicksort, particularly on distributions exhibiting structure and string sorting. There is an open-source implementation with performance analysis and benchmarks<ref>{{cite web|title=Boost.Sort github repository|url=https://github.com/boostorg/sort/tree/master|website=boostorg/sort}}</ref>, and HTML documentation\n<ref>{{cite web|title=HTML Spreadsort Documentation|url=http://www.boost.org/doc/libs/1_65_0/libs/sort/doc/html/index.html|accessdate=30 August 2017}}</ref>.\n\nQuicksort identifies a ''pivot element'' in the list and then partitions the list into two sublists, those elements less than the pivot and those greater than the pivot. Spreadsort generalizes this idea by partitioning the list into ''n''/''c'' partitions at each step, where ''n'' is the total number of elements in the list and ''c'' is a small constant (in practice usually between 4 and 8 when comparisons are slow, or much larger in situations where they are fast). It uses distribution-based techniques to accomplish this, first locating the minimum and maximum value in the list, and then dividing the region between them into ''n''/''c'' equal-sized bins.\nWhere caching is an issue, it can help to have a maximum number of bins in each recursive division step, causing this division process to take multiple steps.  Though this causes more iterations, it reduces cache misses and can make the algorithm run faster overall.\n\nIn the case where the number of bins is at least the number of elements, spreadsort degenerates to bucket sort and the sort completes. Otherwise, each bin is sorted recursively. The algorithm uses heuristic tests to determine whether each bin would be more efficiently sorted by spreadsort or some other classical sort algorithm, then recursively sorts the bin.\n\nLike other distribution-based sorts, spreadsort has the weakness that the programmer is required to provide a means of converting each element into a numeric key, for the purpose of identifying which bin it falls in. Although it is possible to do this for arbitrary-length elements such as strings by considering each element to be followed by an infinite number of minimum values, and indeed for any datatype possessing a [[total order]], this can be more difficult to implement correctly than a simple comparison function, especially on complex structures. Poor implementation of this ''value'' function can result in clustering that harms the algorithm's relative performance.\n\n== Performance ==\nThe worst-case performance of spreadsort is O(''n'' log ''n'') for small data sets, as it uses [[introsort]] as a fallback. In the case of distributions where the size of the key in bits ''k'' times 2 is roughly the square of the log of the list size ''n'' or smaller (2''k'' < (log ''n'')<sup>2</sup>), it does better in the worst case, achieving O(''n'' {{sqrt|''k'' - log ''n''}}) worst-case time for the originally published version, and O(''n''·((''k''/''s'') + s)) for the cache aware version.  For many real sorting problems with over 1000 items, including string sorting, this asymptotic worst-case is better than O(''n'' log ''n'').\n\nExperiments were done comparing an optimized version of spreadsort to the highly optimized C++ <code>std::sort</code>, implemented with introsort. On lists of integers and floats spreadsort shows a roughly 2–7× runtime improvement for random data on various operating systems.[http://www.boostpro.com/vault/index.php?action=downloadfile&filename=algorithm_sorting.zip&directory=&]\n\nIn space performance, spreadsort is worse than most in-place algorithms: in its simplest form, it is not an in-place algorithm, using O(''n'') extra space; in experiments, about 20% more than quicksort using a c of 4–8.  With a cache-aware form (as included in Boost.Sort), less memory is used and there is an upper bound on memory usage of the maximum bin count times the maximum number of recursions, which ends up being a few kilobytes times the size of the key in bytes.  Although it uses asymptotically more space than the O(log ''n'') overhead of quicksort or the O(1) overhead of heapsort, it uses considerably less space than the basic form of mergesort, which uses auxiliary space equal to the space occupied by the list.\n\n== Implementation ==\n<source lang=cpp>\nunsigned \nRoughLog2(DATATYPE input) \n{\n\tunsigned char cResult = 0;\n\t// The && is necessary on some compilers to avoid infinite loops; it doesn't\n\t// significantly impair performance\n\tif(input >= 0)\n\t\twhile((input >> cResult) && (cResult < DATA_SIZE)) cResult++;\n\telse\n\t\twhile(((input >> cResult) < -1) && (cResult < DATA_SIZE)) cResult++;\n\treturn cResult;\n}\nSIZETYPE\nGetMaxCount(unsigned logRange, unsigned uCount)\n{\n\tunsigned logSize = RoughLog2Size(uCount);\n\tunsigned uRelativeWidth = (LOG_CONST * logRange)/((logSize > MAX_SPLITS) ? MAX_SPLITS : logSize);\n\t// Don't try to bitshift more than the size of an element\n\tif(DATA_SIZE <= uRelativeWidth)\n\t\tuRelativeWidth = DATA_SIZE - 1;\n\treturn 1 << ((uRelativeWidth < (LOG_MEAN_BIN_SIZE + LOG_MIN_SPLIT_COUNT)) ? \n\t\t(LOG_MEAN_BIN_SIZE + LOG_MIN_SPLIT_COUNT) :  uRelativeWidth);\n}\n\nvoid \nFindExtremes(DATATYPE *Array, SIZETYPE uCount, DATATYPE & piMax, DATATYPE & piMin)\n{\n\tSIZETYPE u;\n\tpiMin = piMax = Array[0];\n\tfor(u = 1; u < uCount; ++u){\n\t\tif(Array[u] > piMax)\n\t\t\tpiMax=Array[u];\n\t\telse if(Array[u] < piMin)\n\t\t\tpiMin= Array[u];\n\t}\n}\t\n\n//---------------------SpreadSort Source-----------------\n\nBin *\nSpreadSortCore(DATATYPE *Array, SIZETYPE uCount, SIZETYPE & uBinCount, DATATYPE &iMax, DATATYPE &iMin)\n{\n\t// This step is roughly 10% of runtime but it helps avoid worst-case\n\t// behavior and improves behavior with real data.  If you know the\n\t// maximum and minimum ahead of time, you can pass those values in\n\t// and skip this step for the first iteration\n\tFindExtremes((DATATYPE *) Array, uCount, iMax, iMin);\n\tif(iMax == iMin)\n\t\treturn NULL;\n\tDATATYPE divMin,divMax;\n\tSIZETYPE u;\n\tint LogDivisor;\n\tBin * BinArray;\n\tBin* CurrentBin;\n\tunsigned logRange;\n\tlogRange = RoughLog2Size((SIZETYPE)iMax-iMin);\n\tif((LogDivisor = logRange - RoughLog2Size(uCount) + LOG_MEAN_BIN_SIZE) < 0)\n\t\tLogDivisor = 0;\n\t// The below if statement is only necessary on systems with high memory\n\t// latency relative to processor speed (most modern processors)\n\tif((logRange - LogDivisor) > MAX_SPLITS)\n\t\tLogDivisor = logRange - MAX_SPLITS;\n\tdivMin = iMin >> LogDivisor;\n\tdivMax = iMax >> LogDivisor;\n\tuBinCount = divMax - divMin + 1;\n\t\n\t// Allocate the bins and determine their sizes\n\tBinArray = calloc(uBinCount, sizeof(Bin));\n\t// Memory allocation failure check and clean return with sorted results\n    if(!BinArray) {\n\t\tprintf(\"Using std::sort because of memory allocation failure\\n\");\n\t\tstd::sort(Array, Array + uCount);\n\t\treturn NULL;\n\t}\n\t\t\n\t// Calculating the size of each bin; this takes roughly 10% of runtime\n\tfor(u = 0; u < uCount; ++u)\n\t\tBinArray[(Array[u] >> LogDivisor) - divMin].uCount++;\n\t// Assign the bin positions\n\tBinArray[0].CurrentPosition = (DATATYPE *)Array;\n\tfor(u = 0; u < uBinCount - 1; u++) {\n\t\tBinArray[u + 1].CurrentPosition = BinArray[u].CurrentPosition + BinArray[u].uCount;\n\t\tBinArray[u].uCount = BinArray[u].CurrentPosition - Array;\n\t}\n\tBinArray[u].uCount = BinArray[u].CurrentPosition - Array;\n\t\n\t// Swap into place.  This dominates runtime, especially in the swap;\n\t// std::sort calls are the other main time-user.\n\tfor(u = 0; u < uCount; ++u) {\n\t\tfor(CurrentBin = BinArray + ((Array[u] >> LogDivisor) - divMin);  (CurrentBin->uCount > u); \n\t\t\tCurrentBin = BinArray + ((Array[u] >> LogDivisor) - divMin))\n\t\t\t\tSWAP(Array + u, CurrentBin->CurrentPosition++);\n\t\t// Now that we've found the item belonging in this position,\n\t\t// increment the bucket count\n\t\tif(CurrentBin->CurrentPosition == Array + u)\n\t\t\t++(CurrentBin->CurrentPosition);\n\t}\n\t\n\t// If we've bucketsorted, the array is sorted and we should skip recursion\n\tif(!LogDivisor) {\n\t\tfree(BinArray);\n\t\treturn NULL;\n\t}\n\treturn BinArray;\n}\n\nvoid\nSpreadSortBins(DATATYPE *Array, SIZETYPE uCount, SIZETYPE uBinCount, const DATATYPE &iMax\n\t\t\t\t, const DATATYPE &iMin, Bin * BinArray, SIZETYPE uMaxCount)\n{\n\tSIZETYPE u;\n\tfor(u = 0; u < uBinCount; u++){\n\t\tSIZETYPE count = (BinArray[u].CurrentPosition - Array) - BinArray[u].uCount;\n\t\t// Don't sort unless there are at least two items to compare\n\t\tif(count < 2)\n\t\t\tcontinue;\n\t\tif(count < uMaxCount)\n\t\t\tstd::sort(Array + BinArray[u].uCount, BinArray[u].CurrentPosition);\n\t\telse\n\t\t\tSpreadSortRec(Array + BinArray[u].uCount, count);\n\t}\n\tfree(BinArray);\n}\n\nvoid \nSpreadSortRec(DATATYPE *Array, SIZETYPE uCount)\n{\n\tif(uCount < 2)\n\t\treturn;\t\t\n\tDATATYPE iMax, iMin;\n\tSIZETYPE uBinCount;\n\tBin * BinArray = SpreadSortCore(Array, uCount, uBinCount, iMax, iMin);\n\tif(!BinArray)\n\t\treturn;\n\tSpreadSortBins(Array, uCount, uBinCount, iMax, iMin, BinArray,\n\t               GetMaxCount(RoughLog2Size((SIZETYPE)iMax-iMin), uCount));\n}\n</source>\n\n== Two Levels are as Good as Any ==\nAn interesting result for algorithms of this general type (splitting based on the radix, then comparison-based sorting) is that they are O(''n'') for any bounded and integrable [[probability density function]].<ref>{{cite journal |first=Markku |last=Tamminen |title=Two Levels are as Good as Any |journal=J. Algorithms |volume=6 |issue=1 |pages=138–144 |date=March 1985 |doi=10.1016/0196-6774(85)90024-0}}</ref>  This result can be obtained by forcing Spreadsort to always iterate one more time if the bin size after the first iteration is above some constant value.  If the key density function is known to be [[Riemann integrable#Integrability|Riemann integrable]] and bounded, this modification of Spreadsort can attain some performance improvement over the basic algorithm, and will have better worst-case performance.  If this restriction cannot usually be depended on, this change will add a little extra runtime overhead to the algorithm and gain little.  Other similar algorithms are [[Flashsort]] (which is simpler) and Adaptive Left Radix.<ref>{{cite techreport |first=Arne |last=Maus |title=ARL, a faster in-place, cache friendly sorting algorithm |year=2002 |url=http://www.nik.no/2002/Maus.pdf |citeseerx=10.1.1.399.8357}}</ref>  Adaptive Left Radix is apparently quite similar, the main difference being recursive behavior, with Spreadsort checking for worst-case situations and using std::sort to avoid performance problems where necessary, and Adaptive Left Radix recursing continuously until done or the data is small enough to use insertion sort.\n\n== References ==\n<references />\n\n{{sorting}}\n\n[[Category:Sorting algorithms]]"
    },
    {
      "title": "Stooge sort",
      "url": "https://en.wikipedia.org/wiki/Stooge_sort",
      "text": "{{Infobox Algorithm\n|image = [[File:Sorting stoogesort anim.gif]]\n|caption = Visualization of Stooge sort (only shows swaps).\n|class=[[Sorting algorithm]]\n|data=[[Array data structure|Array]]\n|time = {{nowrap|1=O(''n''<sup>log 3/log 1.5</sup>)}}\n|space = O(''n'')\n|optimal=No\n}}\n'''Stooge sort''' is a [[Recursion|recursive]] [[sorting algorithm]]. It is notable for its exceptional bad [[time complexity]] of {{nowrap|1=[[Big O notation|O]](''n''<sup>log 3 / log 1.5 </sup>) = O(''n''<sup>2.7095...</sup>)}}.\nThe running time of the algorithm is thus slower compared\nto reasonable sorting algorithms, and is slower than [[Bubble sort]], a canonical example of a fairly inefficient sort.\nIt is however more efficient than [[Slowsort]].\nThe name comes from [[The Three Stooges]].<ref>https://courses.cs.washington.edu/courses/cse373/13wi/lectures/02-22/18-sorting1-bogo-stooge-bubble.pdf</ref>\n\nThe algorithm is defined as follows:\n* If the value at the start is larger than the value at the end, swap them.\n* If there are 3 or more elements in the list, then:\n** Stooge sort the initial 2/3 of the list\n** Stooge sort the final 2/3 of the list\n** Stooge sort the initial 2/3 of the list again\n\nIt is important to get the integer sort size used in the recursive calls by rounding the 2/3 ''upwards'', e.g. rounding 2/3 of 5 should give 4 rather than 3, as otherwise the sort can fail on certain data. However, if the code is written to end on a base case of size 1, rather than terminating on either size 1 or size 2, rounding the 2/3 of 2 upwards gives an infinite number of calls.\n\n==Implementation==\n<source lang=\"Javascript\">\n function stoogesort(array L, i = 0, j = length(L)-1){\n     if L[i] > L[j] then\n         L[i] ↔ L[j]\n     if (j - i + 1) > 2 then\n         t = (j - i + 1) / 3\n         stoogesort(L, i  , j-t)\n         stoogesort(L, i+t, j  )\n         stoogesort(L, i  , j-t)\n     return L\n }\n</source>\n\n==References==\n<references />\n*{{cite web|url=https://xlinux.nist.gov/dads/HTML/stoogesort.html|title=stooge sort|last=Black|first=Paul E.|work=Dictionary of Algorithms and Data Structures|publisher=[[National Institute of Standards and Technology]]|accessdate=2011-06-18}}\n*{{Introduction to Algorithms|edition=2|chapter=Problem 7-3|pages=161–162}}\n\n==External links==\n*[http://cg.scs.carleton.ca/~morin/misc/sortalg/ Sorting Algorithms (including Stooge sort)]\n*[http://impomatic.blogspot.com/2008/01/stooge-sort.html Stooge sort – implementation and comparison]\n\n{{sorting}}\n\n{{Use dmy dates|date=October 2010}}\n\n{{DEFAULTSORT:Stooge Sort}}\n[[Category:Sorting algorithms]]\n[[Category:Comparison sorts]]\n[[Category:Articles with example pseudocode]]\n\n\n{{comp-sci-stub}}"
    },
    {
      "title": "Strand sort",
      "url": "https://en.wikipedia.org/wiki/Strand_sort",
      "text": "[[File:StrandSort.gif|thumb|220x220px|Strand Sort Animation]]\n'''Strand sort''' is a very simple [[Recursion (computer science)|recursive]] [[sorting algorithm]] that sorts items of a list into increasing order.\n\nIt has [[Big O notation|O]](n<sup>2</sup>) worst time complexity which occurs when the input list is reverse sorted.<ref name=\":0\">{{Cite book|url=https://www.worldcat.org/oclc/641462443|title=IT enabled practices and emerging management paradigms|date=2008|publisher=Prestige Institute of Management and Research|others=Gupta, I. C. (Ishwar Chandra), 1946-, Jaroliya, Deepak., Prestige Institute of Management and Research.|isbn=9788174466761|edition=1st|location=Indore|oclc=641462443}}</ref> It has a best case [[time complexity]] of O(n) which occurs when the input is a list that is already sorted.<ref name=\":1\">{{Cite web|url=https://xlinux.nist.gov/dads/HTML/strandSort.html|title=strand sort|website=xlinux.nist.gov|language=en-US|access-date=2018-11-06}}</ref> Strand sort is not [[In-place algorithm|in-place]] as it’s space complexity is O(n).<ref name=\":0\" />\n\nThe algorithm first moves the first element of a list into a sub-list.<ref name=\":0\" /> It then compares the last element in the sub-list to each subsequent element in the original list.<ref name=\":0\" /> Once there is an element in the original list that is greater than the last element in the sub-list, the element is removed from the original list and added to the sub-list.<ref name=\":0\" /> This process continues until the last element in the sub-list is compared to the remaining elements in the original list.<ref name=\":0\" /> The sub-list is then merged into a new list.<ref name=\":0\" /> Repeat this process and merge all sub-lists until all elements are sorted.<ref name=\":0\" /> This algorithm is called strand sort because there are strands of sorted elements within the unsorted elements that are removed one at a time.<ref name=\":0\" /> This algorithm is also used in [[J Sort]] for fewer than 40 elements.<ref>{{Cite book|url=https://www.worldcat.org/oclc/311311576|title=Data structures using C : 1000 problems and solutions|last=Sudipta.|first=Mukherjee|date=2008|publisher=Tata McGraw-Hill|isbn=9780070667655|location=New Delhi|oclc=311311576}}</ref>\n\n== Example ==\nBased off of the description of the algorithm provided in the book, ''IT Enabled Practices and Emerging Management Paradigms''.<ref name=\":0\" />\n\n'''Step 1:''' Start with a list of numbers: {5, 1, 4, 2, 0, 9, 6, 3, 8, 7 }\n\n'''Step 2:''' Next move the first element of the list into a new sub-list:  sub-list contains {5}\n\n'''Step 3:''' Then iterate through the original list and compare each number to 5 until there is a number greater than 5.\n\n* 1 < 5 so 1 is not added to the sub-list.\n* 4 < 5 so 4 is not added to the sub-list.\n* 2 < 5 so 2 is not added to the sub-list.\n* 0 < 5 so 0  is not added to the sub-list.\n* 9 > 5 so 9 is added to the sub-list and removed from the original list.\n\n'''Step 4:''' Now compare 9 with the remaining elements in the original list until there is a number greater than 9.  \n\n* 6 < 9 so 6 is not added to the sub-list.\n* 3 < 9 so 3 is not added to the sub-list.\n* 8 < 9 so 8 is not added to the sub-list.\n* 7 < 9 so 7 is not added to the sub-list.\n\n'''Step 5:''' Now there are no more elements to compare 9 to so merge the sub-list into a new list, called solution-list.\n\nAfter step 5, the original list contains {1, 4, 2, 0, 6, 3, 8, 7}\n\nThe sub-list is empty, and the solution list contains {5, 9}\n\n'''Step 6:''' Move the first element of the original list into sub-list: sub-list contains {1}\n\n'''Step 7:''' Iterate through the original list and compare each number to 1 until there is a number greater than 1.\n\n* 4 > 1 so 4 is added to the sub-list and 4 is removed from the original list.\n\n'''Step 8:''' Now compare 4 with the remaining elements in the original list until there is a number greater than 4.\n\n* 2 < 4 so 2 is not added to the sub-list.\n* 0 < 4 so 0 is not added to the sub-list.\n* 6 > 4 so 6 is added to the sub-list and is removed from the original list.\n\n'''Step 9:''' Now compare 6 with the remaining elements in the original list until there is a number greater than 6.  \n\n* 3 < 6 so 3 is not added to the sub-list.\n* 8 > 6 so 8 is added to the sub-list and is removed from the original list.\n\n'''Step 10:''' Now compare 8 with the remaining elements in the original list until there is a number greater than 8.\n\n* 7 < 8 so 7 is not added to the sub-list.\n\n'''Step 11:''' Since there are no more elements in the original list to compare {8} to, the sub-list is merged with the solution list. Now the original list contains {2, 0, 3, 7}, the sub-list is empty and the solution-list contains: {1, 4, 5, 6, 8, 9}.\n\n'''Step 12:'''  Move the first element of the original list into sub-list. Sub-list contains {2}\n\n'''Step 13:''' Iterate through the original list and compare each number to 2 until there is a number greater than 2.\n\n* 0 < 2 so 0 is not added to the sub-list.\n* 3 > 2 so 3 is added to the sub-list and is removed from the original list.\n\n'''Step 14:''' Now compare 3 with the remaining elements in the original list until there is a number greater than 3.\n\n* 7 > 3 so 7 is added to the sub-list and is removed from the original list.\n\n'''Step 15:''' Since there are no more elements in the original list to compare {7} to, the sub-list is merged with the solution list. The original list now contains {0}, the sub-list is empty, and solution list contains: {1, 2, 3, 4, 5, 6, 7, 8, 9}.\n\n'''Step 16:'''  Move the first element of the original list into sub-list. Sub-list contains {0}.\n\n'''Step 17:'''  Since the original list is now empty, the sub-list is merged with the solution list. The solution list now contains: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}. There are now no more elements in the original list, and all of the elements in the solution list have successfully been sorted into increasing numerical order.\n\n== Implementation ==\nSince Strand Sort requires many insertions and deletions, it is best to use a linked list when implementing the algorithm.<ref name=\":1\" /> Linked lists require constant time for both insertions and removals of elements using iterators. The time to traverse through the linked list is directly related to the input size of the list.<ref>{{Cite web|url=https://www.cs.cmu.edu/~adamchik/15-121/lectures/Linked%20Lists/linked%20lists.html|title=LinkedLists|website=www.cs.cmu.edu|access-date=2018-11-06}}</ref> The following implementation is done in Java 8 and is based off of the description of the algorithm from the book, ''IT Enabled Practices and Emerging Management Paradigms''.<ref name=\":0\" /><syntaxhighlight lang=\"java\" line=\"1\">\npackage strandSort;\n\nimport java.util.*;\n\npublic class strandSort {\n\tstatic LinkedList<Integer> solList = new LinkedList<Integer>();\n\tstatic int k = 0;\n\n\t/**\n\t * This is a recursive Strand Sort method. It takes in a linked list of\n\t * integers as its parameter. It first checks the base case to see if the\n\t * linked list is empty. Then proceeds to the Strand sort algorithm until\n\t * the linked list is empty.\n\t * \n\t * @param origList:\n\t *            a linked list of integers\n\t */\n\tpublic static void strandSortIterative(LinkedList<Integer> origList) {\n\n\t\t// Base Case\n\t\tif (origList.isEmpty()) {\n\t\t\treturn;\n\t\t}\n\n\t\telse {\n\t\t\t// Create the subList and add the first element of\n\t\t\t// The original linked list to the sublist.\n\t\t\t// Then remove the first element from the original list.\n\t\t\tLinkedList<Integer> subList = new LinkedList<Integer>();\n\t\t\tsubList.add(origList.getFirst());\n\t\t\torigList.removeFirst();\n\n\t\t\t// Iterate through the original list, checking if any elements are\n\t\t\t// Greater than the element in the sub list.\n\t\t\tint index = 0;\n\t\t\tfor (int j = 0; j < origList.size(); j++) {\n\t\t\t\tif (origList.get(j) > subList.get(index)) {\n\t\t\t\t\tsubList.add(origList.get(j));\n\t\t\t\t\torigList.remove(j);\n\t\t\t\t\tj = j - 1;\n\t\t\t\t\tindex = index + 1;\n\t\t\t\t}\n\t\t\t}\n\t\t\t// Merge sub-list into solution list.\n\t\t\t// There are two cases for this step/\n\t\t\t// Case 1: The first recursive call, add all of the elements to the\n\t\t\t// solution list in sequential order\n\t\t\tif (k == 0) {\n\t\t\t\tfor (int i = 0; i < subList.size(); i++) {\n\n\t\t\t\t\tsolList.add(subList.get(i));\n\t\t\t\t\tk = k + 1;\n\t\t\t\t}\n\n\t\t\t}\n\n\t\t\t// Case 2: After the first recursive call, \n\t\t\t// merge the sub-list with the solution list.\n\t\t\t// This works by comparing the greatest element in the sublist (which is always the last element)\n\t\t\t// with the first element in the solution list. \n\t\t\telse {\n\t\t\t\tint subEnd = subList.size() - 1;\n\t\t\t\tint solStart = 0;\n\t\t\t\twhile (!subList.isEmpty()) {\n\n\t\t\t\t\tif (subList.get(subEnd) > solList.get(solStart)) {\n\t\t\t\t\t\tsolStart++;\n\n\t\t\t\t\t} else {\n\t\t\t\t\t\tsolList.add(solStart, subList.get(subEnd));\n\t\t\t\t\t\tsubList.remove(subEnd);\n\t\t\t\t\t\tsubEnd--;\n\t\t\t\t\t\tsolStart = 0;\n\t\t\t\t\t}\n\n\t\t\t\t}\n\n\t\t\t}\n\n\t\t\tstrandSortIterative(origList);\n\t\t}\n\n\t}\n\n\tpublic static void main(String[] args) {\n\t\t// Create a new linked list of Integers\n\t\tLinkedList<Integer> origList = new LinkedList<Integer>();\n\n\t\t// Add the following integers to the linked list: {5, 1, 4, 2, 0, 9, 6, 3, 8, 7}\n\n\t\torigList.add(5);\n\t\torigList.add(1);\n\t\torigList.add(4);\n\t\torigList.add(2);\n\t\torigList.add(0);\n\t\torigList.add(9);\n\t\torigList.add(6);\n\t\torigList.add(3);\n\t\torigList.add(8);\n\t\torigList.add(7);\n\n\t\tstrandSortIterative(origList);\n\t\t// Print out the solution list\n\t\tfor (int i = 0; i < solList.size(); i++) {\n\t\t\tSystem.out.println(solList.get(i));\n\t\t}\n\n\t}\n\n}\n\n</syntaxhighlight>\n\n== References ==\n<!-- Inline citations added to your article will automatically display here. See https://en.wikipedia.org/wiki/WP:REFB for instructions on how to add citations. -->\n{{reflist}}\n\n[[Category:Sorting algorithms]]"
    },
    {
      "title": "Stupid sort",
      "url": "https://en.wikipedia.org/wiki/Stupid_sort",
      "text": "'''Stupid sort''' may refer to:\n\n* [[Bogosort]], based on the generate and test paradigm\n* [[Gnome sort]], similar to insertion sort\n\n{{disambiguation}}\n[[Category:Sorting algorithms]]"
    },
    {
      "title": "Timsort",
      "url": "https://en.wikipedia.org/wiki/Timsort",
      "text": "{{Use dmy dates|date=April 2016}}\n{{Infobox Algorithm\n| class      = [[Sorting algorithm]]\n| image      =\n| caption    = A visual representation of Timsort\n| data       = [[Array data structure|Array]]\n| time       = <math>O(n\\log n)</math><ref>{{cite web |title=[Python-Dev] Sorting |url=http://mail.python.org/pipermail/python-dev/2002-July/026837.html |last=Peters |first=Tim |work=Python Developers Mailinglist |accessdate=24 February 2011 |quote=[Timsort] also has good aspects: It's stable (items that compare equal retain their relative order, so, e.g., if you sort first on zip code, and a second time on name, people with the same name still appear in order of increasing zip code; this is important in apps that, e.g., refine the results of queries based on user input). ... It has no bad cases (O(N log N) is worst case; N−1 compares is best).}}</ref><ref>{{cite web |title=[DROPS]|url=http://drops.dagstuhl.de/opus/volltexte/2018/9467/ |accessdate=1 September 2018 |quote=TimSort is an intriguing sorting algorithm designed in 2002 for Python, whose worst-case complexity was announced, but not proved until our recent preprint.}}</ref>\n|best-time=<math>O(n)</math><ref name=\"Chandramouli\">{{Cite conference |last1=Chandramouli |first1=Badrish |last2=Goldstein |first2=Jonathan |title=Patience is a Virtue: Revisiting Merge and Sort on Modern Processors |conference=SIGMOD/PODS |year=2014}}</ref>\n|average-time=<math>O(n\\log n)</math>\n|space=<math>O(n)</math>\n|optimal=Yes\n}}\n'''Timsort''' is a [[hybrid algorithm|hybrid]] [[:Category:stable sorts|stable]] [[sorting algorithm]], derived from [[merge sort]] and [[insertion sort]], designed to perform well on many kinds of real-world data. It uses techniques from [[Peter McIlroy]]'s \"Optimistic Sorting and Information Theoretic Complexity\", in ''Proceedings of the Fourth Annual ACM-SIAM Symposium on Discrete Algorithms'', pp.&nbsp;467–474, January 1993. It was implemented by [[Tim Peters (software engineer)|Tim Peters]] in 2002 for use in the [[Python (programming language)|Python programming language]]. The algorithm finds subsequences of the data that are already ordered, and uses that knowledge to sort the remainder more efficiently. This is done by merging an identified subsequence, called a run, with existing runs until certain criteria are fulfilled. Timsort has been Python's standard sorting algorithm since version 2.3. It is also used to sort arrays of non-primitive type in [[Java 7|Java SE 7]],<ref>{{cite web\n | title = [#JDK-6804124] (coll) Replace &quot;modified mergesort&quot; in java.util.Arrays.sort with timsort\n | url = https://bugs.openjdk.java.net/browse/JDK-6804124\n | work = JDK Bug System\n | accessdate = 11 June 2014\n}}</ref> on the [[Android (operating system)|Android platform]],<ref>\n{{cite web\n | title = Class: java.util.TimSort<T>\n | url = https://android.googlesource.com/platform/libcore/+/gingerbread/luni/src/main/java/java/util/TimSort.java\n | archive-url = https://web.archive.org/web/20150716000631/https://android.googlesource.com/platform/libcore/+/gingerbread/luni/src/main/java/java/util/TimSort.java\n | dead-url = yes\n | archive-date = 16 July 2015\n | work = Android Gingerbread Documentation\n | accessdate = 24 February 2011\n}}</ref> in [[GNU Octave]],<ref>\n{{cite web\n | title = liboctave/util/oct-sort.cc\n | url = http://hg.savannah.gnu.org/hgweb/octave/file/0486a29d780f/liboctave/util/oct-sort.cc\n | work = Mercurial repository of Octave source code\n | accessdate = 18 February 2013\n | quote = Code stolen in large part from Python's, listobject.c, which itself had no license header. However, thanks to Tim Peters for the parts of the code I ripped-off.\n | at = Lines 23-25 of the initial comment block.\n}}</ref> and [[Google Chrome]].<ref>{{Cite web|url=https://v8.dev/blog/array-sort|title=Getting things sorted in V8 · V8|website=v8.dev|access-date=2018-12-21}}</ref>\n\n== Operation ==\nTimsort was designed to take advantage of ''runs'' of consecutive ordered elements that already exist in most real-world data, ''natural runs''. It iterates over the data collecting elements into runs, and simultaneously merging those runs together.  When there are runs, doing this decreases the total number of comparisons needed to fully sort the list.\n\n=== Runs ===\nTimsort iterates over the data looking for ''natural runs'' of at least two elements that are either non-descending (each element is greater than or equal to its predecessor) or strictly descending (each element is less than its predecessor). Since descending runs are later blindly reversed, excluding equal elements maintains the algorithm's stability; i.e., equal elements won't be reversed. Note that any two elements are guaranteed to be either descending or non-descending.\n\nA reference to each run is then pushed onto a stack.\n\n==== Minimum size (minrun) ====\n[[File:Selection of minrun by timsort.png|280px|thumb|Timsort algorithm searches for minimum-size ordered sequences, minruns, to perform its sort.]]\n\nTimsort is an [[adaptive sort]],<ref name=python_timsort>{{ cite web | website=Python source code |title=listsort.txt | url=http://hg.python.org/cpython/file/tip/Objects/listsort.txt | date=10 February 2017}}</ref> using [[insertion sort]] to combine runs smaller than the ''minimum run size'' (''minrun''), and [[merge sort]] otherwise.\n\n''Minrun'' is selected so most runs in a random array are, or become, ''minrun'' in length. It also results in a reasonable number of function calls in the implementation of the sort.<ref name=drmaciver>{{cite web |last=MacIver |first=David R. |title=Understanding timsort, Part 1: Adaptive Mergesort |url=http://www.drmaciver.com/2010/01/understanding-timsort-1adaptive-mergesort/ |date=11 January 2010 |accessdate=2015-12-05}}</ref>\n\nBecause merging is most efficient when the number of runs is equal to, or slightly less than, a power of two, and notably less efficient when the number of runs is slightly more than a power of two, Timsort chooses ''minrun'' to try to ensure the former condition.<ref name=python_timsort /><!-- Specifically, the \"computng minrun\" section starting at line 252-->\n\n''Minrun'' is chosen from the range 32 to 64 inclusive, such that the size of the data, divided by ''minrun'', is equal to, or slightly less than, a power of two.  The final algorithm takes the six most significant bits of the size of the array, adds one if any of the remaining bits are set, and uses that result as the ''minrun''.  This algorithm works for all arrays, including those smaller than 64; for arrays of size 63 or less, this sets ''minrun'' equal to the array size and Timsort reduces to an insertion sort.<ref name=python_timsort />\n\n=== Merging ===\n[[File:Representation of stack for merge memory in Timsort.svg|280px|thumb|The runs are inserted in a [[stack (data structure)|stack]]. If {{nowrap|{{abs|''Z''}} ≤ {{abs|''Y''}} + {{abs|''X''}}}}, then ''X'' and ''Y'' are merged and replaced on the stack. In this way, merging is continued until all runs satisfy {{nowrap|i. {{abs|''Z''}} > {{abs|''Y''}} + {{abs|''X''}}}} and {{nowrap|ii. {{abs|''Y''}} > {{abs|''X''}}}}.]]\n\nConcurrently with the search for runs, the runs are merged with [[mergesort]]. Except where Timsort tries to optimise for merging disjoint <!-- disjoint domain/range? --> runs in galloping mode, runs are repeatedly merged two at a time, with the only concerns being to maintain stability and merge balance.\n\nStability requires non-consecutive runs are not merged, as elements could be transferred across equal elements in the intervening run, violating stability. Further, it would be impossible to recover the order of the equal elements at a later point.\n\nIn pursuit of balanced merges, Timsort considers three runs on the top of the stack, ''X'', ''Y'', ''Z'', and maintains the invariants:\n{{ordered list\n | list_style_type=lower-roman\n | {{abs|''Z''}} > {{abs|''Y''}} + {{abs|''X''}}\n | {{abs|''Y''}} > {{abs|''X''}}<ref name=python_timsort />\n}}\n\nIf the invariants are violated, ''Y'' is merged with the smaller of ''X'' or ''Z'' and the invariants are checked again. Once the invariants hold, the next run is formed.<ref name=drmaciver />\n\nSomewhat inappreciably, the invariants maintain merges as being approximately balanced while maintaining a compromise between delaying merging for balance, and exploiting fresh occurrence of runs in [[CPU cache|cache memory]], and also making merge decisions relatively simple.\n\nOn reaching the end of the data, Timsort repeatedly merges the two runs on the top of the stack, until only one run of the entire data remains.\n\n==== Individual merges ====\n[[File:Merging procedure for timsort.svg|280px|thumb|To merge, Timsort copies the elements of the smaller array (X in this illustration) to temporary memory, then sorts and fills elements in final order into the combined space of X and Y.]]\n\nTimsort performs an almost in-place merge sort, as actual in-place merge sort implementations have a high overhead. First Timsort performs a [[binary search]] to find the location in the first run of the first element in the second run, and the location in the second run of the last element in the first run.  Elements before and after these locations are already in their correct place, and may be removed from further consideration.  This not only optimises element movements and running time, but also allows the use of less temporary memory.  Then the smaller of the remaining runs is copied into temporary memory, and elements are merged with the larger run, into the now free space.  If the first run is smaller, the merge starts at the beginning; if the second is smaller, the merge starts at the end.\n\nSay, for example, two runs ''A'' and ''B'' are to be merged, with ''A'' being the smaller run. In this case a binary search examines ''A'' to find the first element ''a''ʹ larger than the first element of ''B''. Note that ''A'' and ''B'' are already sorted individually. When ''a''ʹ is found, the algorithm can ignore elements before that position when merging. Similarly, the algorithm also looks for the first element ''b''ʹ in ''B'' greater than the last element of ''A''. The elements after ''b''ʹ can also be ignored when merging. This preliminary searching is not efficient for highly random data, but is efficient in other situations and is hence included.\n\n==== Galloping mode ====\n[[File:One-one merging timsort.svg|280px|thumb|Elements (pointed to by blue arrow) are compared and the smaller element is moved to its final position (pointed to by red arrow).]]\n\nAn individual merge keeps a count of consecutive elements selected from the same input set. The algorithm switches to galloping mode when this reaches the ''minimum galloping threshold'' (''min_gallop'') in an attempt to capitalise on sub-runs in the data. The success or failure of galloping is used to adjust ''min_gallop'', as an indication of whether the data does or does not contain sufficient sub-runs.<ref name=python_timsort />\n\nIn galloping mode, the algorithm searches for the first element of one array in the other. This is done by comparing that initial element with the (2<sup>''k''</sup> − 1)th element of the other array (first, third, seventh, and so on) so as to get a range of elements between which the initial element will lie. This shortens the range for binary searching, thus increasing efficiency. In cases where galloping is found to be less efficient than [[binary search algorithm|binary search]], galloping mode is exited.\n\n[[File:Copy galloping mode timsort(2).svg|280px|thumb|All red elements are smaller than blue (here, 21). Thus they can be moved in a chunk to the final array.]]\n\nGalloping is beneficial only when the initial element of one run is not one of the first seven elements of the other run. This implies an initial threshold of 7. To avoid the drawbacks of galloping mode, the merging functions adjust the threshold value. If the selected element is from the same array that returned an element previously, ''min_gallop'' is reduced by one. Otherwise, the value is incremented by one, thus discouraging a return to galloping mode. In the case of random data, the value of ''min_gallop'' becomes so large that galloping mode never recurs.\n\nWhen merging is done right-to-left, galloping starts from the right end of the data, that is, the last element. Galloping from the beginning also gives the required results, but makes more comparisons. Thus, the galloping algorithm uses a variable that gives the index at which galloping should begin. Timsort can enter galloping mode at any index and continue checking at the next index which is offset by 1, 3, 7, …, (2<sup>''k''</sup> − 1)… and so on from the current index. In the case of right-to-left merging, the offsets to the index will be −1, −3, −7, …<ref name=python_timsort />\n\nGalloping is not always efficient. In some cases galloping mode requires more comparisons than a simple [[linear search]]. While for the first few cases both modes may require the same number of comparisons, over time galloping mode requires 33% more comparisons than linear search to arrive at the same results.\n\n== Analysis ==\nIn the [[Best, worst and average case|worst case]], Timsort takes <math>O(n \\log n)</math> comparisons to sort an array of {{mvar|n}} elements. In the best case, which occurs when the input is already sorted, it runs in linear time, meaning that it is an [[adaptive sort]]ing algorithm.{{r|Chandramouli}}\n\n== Formal verification ==\nIn 2015, Dutch and German researchers in the EU FP7 ENVISAGE project found a bug in the standard implementation of Timsort.<ref name=broken>{{cite journal |title=OpenJDK's Java.utils.Collection.sort() Is Broken: The Good, the Bad and the Worst Case |journal=Computer Aided Verification |pages=273–289 |date=July 2015 |first1=Stijn |last1=de Gouw |first2=Jurriaan |last2=Rot |first3=Frank S. |last3=de Boer |first4=Richard |last4=Bubel |first5=Reiner |last5=Hähnle |doi=10.1007/978-3-319-21690-4_16 |url=http://envisage-project.eu/wp-content/uploads/2015/02/sorting.pdf}}</ref>\n\nSpecifically, the invariants on stacked run sizes ensure a tight upper bound on the maximum size of the required stack.  The implementation preallocated a stack sufficient to sort 2<sup>64</sup> bytes of input, and avoided further overflow checks.\n\nHowever, the guarantee requires the invariants to apply to ''every'' group of three consecutive runs, but the implementation only checked it for the top three.<ref name=broken/>  Using the [[KeY]] tool for [[formal verification]] of Java software, the researchers found that this check is not sufficient, and they were able to find run lengths (and inputs which generated those run lengths) which would result in the invariants being violated deeper in the stack after the top of the stack was merged.<ref>{{cite web |first=Stijn |last=de Gouw |date=24 February 2015 |url=http://envisage-project.eu/proving-android-java-and-python-sorting-algorithm-is-broken-and-how-to-fix-it/ |title=Proving that Android’s, Java’s and Python’s sorting algorithm is broken (and showing how to fix it) |accessdate=6 May 2017}}</ref>\n\nAs a consequence, for certain inputs the allocated size is not sufficient to hold all unmerged runs. In Java, this generates for those inputs an array-out-of-bound exception. The smallest input that triggers this exception in Java and Android v7 is of size {{val|67108864}}. (Older Android versions already triggered this exception for certain inputs of size {{val|65536}})\n\nThe Java implementation was corrected by increasing the size of the preallocated stack based on an updated worst-case analysis.  The article also showed by formal methods how to establish the intended invariant by checking that the ''four'' topmost runs in the stack satisfy the two rules above.  This approach was adopted by Python<ref>[http://bugs.python.org/issue23515 Python Issue Tracker – Issue 23515: Bad logic in timsort's merge_collapse]</ref> and Android.\n\n== References ==\n{{Reflist|40em}}\n\n== Further reading ==\n* {{cite web |first1=Nicolas |last1=Auger |first2=Cyril |last2=Nicaud |first3=Carine |last3=Pivoteau |title=Merge Strategies: from Merge Sort to TimSort |url=https://hal-upec-upem.archives-ouvertes.fr/hal-01212839 |year=2015 |work=hal-01212839}}\n\n== External links ==\n* [http://bugs.python.org/file4451/timsort.txt timsort.txt] – original explanation by Tim Peters [https://hg.python.org/cpython/file/tip/Objects/listsort.txt revised branch version]\n** [https://hg.python.org/cpython/file/7b5057b89a56/Objects/listobject.c#l1910 listobject.c:1910@7b5057b89a56] – Python's Tree implementation\n* [http://hg.python.org/cpython/file/default/Objects/listobject.c Python's listobject.c] – the [[C (programming language)|C]] implementation of Timsort used in [[CPython]]\n* [http://cr.openjdk.java.net/~martin/webrevs/openjdk7/timsort/raw_files/new/src/share/classes/java/util/TimSort.java OpenJDK's TimSort.java] – the Java implementation of Timsort\n* [http://hg.savannah.gnu.org/hgweb/octave/file/0486a29d780f/liboctave/util/oct-sort.cc GNU Octave's oct-sort.cc] – the [[C++]] implementation of Timsort used in [[GNU Octave]]\n* [http://stromberg.dnsalias.org/~strombrg/sort-comparison/ Sort Comparison] – a pure Python and Cython implementation of Timsort, among other sorts\n* [https://git.gnome.org/browse/libgee/tree/gee/timsort.vala Gee.TimSort] - Vala implementation of Timsort\n\n{{Sorting}}\n\n[[Category:Sorting algorithms]]\n[[Category:Comparison sorts]]\n[[Category:Stable sorts]]"
    },
    {
      "title": "Topological sorting",
      "url": "https://en.wikipedia.org/wiki/Topological_sorting",
      "text": "{{Redirect|Dependency resolution|other uses|Dependency (disambiguation)}}\nIn [[computer science]], a '''topological sort''' or '''topological ordering''' of a [[directed graph]] is a [[total order|linear ordering]] of its [[vertex (graph theory)|vertices]] such that for every directed edge ''uv'' from vertex ''u'' to vertex ''v'', ''u'' comes before ''v'' in the ordering. For instance, the vertices of the graph may represent tasks to be performed, and the edges may represent constraints that one task must be performed before another; in this application, a topological ordering is just a valid sequence for the tasks. A topological ordering is possible if and only if the graph has no [[directed cycle]]s, that is, if it is a [[directed acyclic graph]] (DAG). Any DAG has at least one topological ordering, and [[algorithm]]s are known for constructing a topological ordering of any DAG in [[linear time]].\n\n== Examples ==\nThe canonical application of topological sorting is in [[Job shop scheduling|scheduling]] a sequence of jobs or tasks based on their [[Dependency graph|dependencies]]. The jobs are represented by vertices, and there is an edge from ''x'' to ''y'' if job ''x'' must be completed before job ''y'' can be started (for example, when washing clothes, the washing machine must finish before we put the clothes in the dryer). Then, a topological sort gives an order in which to perform the jobs.  A closely related application of topological sorting algorithms was first studied in the early 1960s in the context of the [[Program Evaluation and Review Technique|PERT]] technique for scheduling in [[project management]] {{harv|Jarnagin|1960}}; in this application, the vertices of a graph represent the milestones of a project, and the edges represent tasks that must be performed between one milestone and another. Topological sorting forms the basis of linear-time algorithms for finding the [[Critical path method|critical path]] of the project, a sequence of milestones and tasks that controls the length of the overall project schedule.\n\nIn computer science, applications of this type arise in [[instruction scheduling]], ordering of formula cell evaluation when recomputing formula values in [[spreadsheet]]s, [[logic synthesis]], determining the order of compilation tasks to perform in [[makefile]]s, data [[serialization]], and resolving symbol dependencies in [[Linker (computing)|linkers]]. It is also used to decide in which order to load tables with foreign keys in databases.\n\n{|\n|[[Image:Directed acyclic graph 2.svg|180px|left]]\n| The graph shown to the left has many valid topological sorts, including:\n\n* 5, 7, 3, 11, 8, 2, 9, 10 (visual left-to-right, top-to-bottom)\n* 3, 5, 7, 8, 11, 2, 9, 10 (smallest-numbered available vertex first)\n* 5, 7, 3, 8, 11, 10, 9, 2 (fewest edges first)\n* 7, 5, 11, 3, 10, 8, 9, 2 (largest-numbered available vertex first)\n* 5, 7, 11, 2, 3, 8, 9, 10 (attempting top-to-bottom, left-to-right)\n* 3, 7, 8, 5, 11, 10, 2, 9 (arbitrary)\n|}\n\n== Algorithms ==\nThe usual algorithms for topological sorting have running time linear in the number of nodes plus the number of edges, asymptotically, <math>O(\\left|{V}\\right| + \\left|{E}\\right|).</math>\n\n===Kahn's algorithm===\nOne of these algorithms, first described by {{harvp|Kahn|1962}}, works by choosing vertices in the same order as the eventual topological sort. First, find a list of \"start nodes\" which have no incoming edges and insert them into a set S; at least one such node must exist in a non-empty acyclic graph. Then:\n\n L ← Empty list that will contain the sorted elements\n S ← Set of all nodes with no incoming edge\n '''while''' S is non-empty '''do'''\n     remove a node n from S\n     add n to ''tail'' of L\n     '''for each''' node m with an edge ''e'' from n to m '''do'''\n         remove edge e from the graph\n         '''if''' m has no other incoming edges '''then'''\n             insert m into S\n '''if''' graph has edges '''then'''\n     return error   ''(graph has at least one cycle)''\n '''else''' \n     return L   ''(a topologically sorted order)''\n\nIf the graph is a [[Directed acyclic graph|DAG]], a solution will be contained in the list L (the solution is not necessarily unique). Otherwise, the graph must have at least one cycle and therefore a topological sort is impossible.\n\nReflecting the non-uniqueness of the resulting sort, the structure S can be simply a set or a queue or a stack. Depending on the order that nodes n are removed from set S, a different solution is created. A variation of Kahn's algorithm that breaks ties [[lexicographic order|lexicographically]] forms a key component of the [[Coffman–Graham algorithm]] for parallel scheduling and [[layered graph drawing]].\n\n===Depth-first search===\nAn alternative algorithm for topological sorting is based on [[depth-first search]]. The algorithm loops through each node of the graph, in an arbitrary order, initiating a depth-first search that terminates when it hits any node that has already been visited since the beginning of the topological sort or the node has no outgoing edges (i.e. a leaf node):\n\n L ← Empty list that will contain the sorted nodes\n '''while''' exists nodes without a permanent mark '''do'''\n     select an unmarked node n\n     visit(n)\n \n '''function''' visit(node n)\n     '''if''' n has a permanent mark '''then''' return\n     '''if''' n has a temporary mark '''then''' stop   ''(not a DAG)''\n     mark n with a temporary mark\n     '''for each''' node m with an edge from n to m '''do'''\n         visit(m)\n     remove temporary mark from n\n     mark n with a permanent mark\n     add n to ''head'' of L\n\nEach node ''n'' gets ''prepended'' to the output list L only after considering all other nodes which depend on ''n'' (all descendants of ''n'' in the graph).  Specifically, when the algorithm adds node ''n'', we are guaranteed that all nodes which depend on ''n'' are already in the output list L: they were added to L either by the recursive call to visit() which ended before the call to visit ''n'', or by a call to visit() which started even before the call to visit ''n''.  Since each edge and node is visited once, the algorithm runs in linear time. This depth-first-search-based algorithm is the one described by {{harvp|Cormen|Leiserson|Rivest|Stein|2001}}; it seems to have been first described in print by {{harvp|Tarjan|1976}}.\n\n===Parallel algorithms===\nOn a [[parallel random-access machine]], a topological ordering can be constructed in ''O''(log<sup>2</sup> ''n'') time using a polynomial number of processors, putting the problem into the complexity class '''[[NC (complexity)|NC]]<sup>2</sup>''' {{harv|Cook|1985}}.\nOne method for doing this is to repeatedly square the [[adjacency matrix]] of the given graph, logarithmically many times, using [[min-plus matrix multiplication]] with maximization in place of minimization. The resulting matrix describes the [[Longest path problem|longest path]] distances in the graph. Sorting the vertices by the lengths of their longest incoming paths produces a topological ordering {{harv|Dekel|Nassimi|Sahni|1981}}.\n\n==Application to shortest path finding==\nThe topological ordering can also be used to quickly compute [[shortest path problem|shortest paths]] through a [[Weighted graph|weighted]] directed acyclic graph. Let {{mvar|V}} be the list of vertices in such a graph, in topological order. Then the following algorithm computes the shortest path from some source vertex {{mvar|s}} to all other vertices:<ref name=\"clrs\">{{Introduction to Algorithms|3|pages=655–657}}</ref>\n\n<div style=\"margin-left: 35px; width: 600px\">\n{{framebox|blue}}\n* Let {{mvar|d}} be an array of the same length as {{mvar|V}}; this will hold the shortest-path distances from {{mvar|s}}. Set {{math|''d''[''s''] {{=}} 0}}, all other {{math|''d''[''u''] {{=}} ∞}}.\n* Let {{mvar|p}} be an array of the same length as {{mvar|V}}, with all elements initialized to {{mono|nil}}. Each {{math|''p''[''u'']}} will hold the predecessor of {{math|''u''}} in the shortest path from {{mvar|s}} to {{mvar|u}}.\n* Loop over the vertices {{mvar|u}} as ordered in {{mvar|V}}, starting from {{mvar|s}}:\n** For each vertex {{mvar|v}} directly following {{mvar|u}} (i.e., there exists an edge from {{mvar|u}} to {{mvar|v}}):\n*** Let {{mvar|w}} be the weight of the edge from {{mvar|u}} to {{mvar|v}}.\n*** Relax the edge: if {{math|''d''[''v''] > ''d''[''u''] + ''w''}}, set\n**** {{math|''d''[''v''] ← ''d''[''u''] + ''w''}},\n**** {{math|''p''[''v''] ← ''u''}}.\n{{frame-footer}}\n</div>\n\nOn a graph of {{mvar|n}} vertices and {{mvar|m}} edges, this algorithm takes {{math|Θ(''n'' + ''m'')}}, i.e., [[Linear time|linear]], time.{{r|clrs}}\n\n==Uniqueness==\nIf a topological sort has the property that all pairs of consecutive vertices in the sorted order are connected by edges, then these edges form a directed [[Hamiltonian path]] in the [[Directed acyclic graph|DAG]]. If a Hamiltonian path exists, the topological sort order is unique; no other order respects the edges of the path. Conversely, if a topological sort does not form a Hamiltonian path, the DAG will have two or more valid topological orderings, for in this case it is always possible to form a second valid ordering by swapping two consecutive vertices that are not connected by an edge to each other. Therefore, it is possible to test in linear time whether a unique ordering exists, and whether a Hamiltonian path exists, despite the [[NP-hard]]ness of the Hamiltonian path problem for more general directed graphs {{harv|Vernet|Markenzon|1997}}.\n\n==Relation to partial orders==\nTopological orderings are also closely related to the concept of a [[linear extension]] of a [[partial order]] in mathematics. In high-level terms, there is an [[Adjunction (category theory)|adjunction]] between directed graphs and partial orders.<ref>{{cite book |last1=Spivak |first1=David I.|title=Category Theory for the Sciences |date=2014 |publisher=MIT Press}}</ref>\n\nA partially ordered set is just a set of objects together with a definition of the \"≤\" inequality relation, satisfying the axioms of reflexivity (''x''&nbsp;≤&nbsp;''x''), antisymmetry (if ''x''&nbsp;≤&nbsp;''y'' and ''y''&nbsp;≤&nbsp;''x'' then ''x''&nbsp;=&nbsp;''y'') and [[transitive relation|transitivity]] (if ''x''&nbsp;≤&nbsp;''y'' and ''y''&nbsp;≤&nbsp;''z'', then ''x''&nbsp;≤&nbsp;''z''). A [[total order]] is a partial order in which, for every two objects ''x'' and ''y'' in the set, either ''x''&nbsp;≤&nbsp;''y'' or ''y''&nbsp;≤&nbsp;''x''. Total orders are familiar in computer science as the comparison operators needed to perform [[comparison sort]]ing algorithms. For finite sets, total orders may be identified with linear sequences of objects, where the \"≤\" relation is true whenever the first object precedes the second object in the order; a comparison sorting algorithm may be used to convert a total order into a sequence in this way. A linear extension of a partial order is a total order that is compatible with it, in the sense that, if ''x''&nbsp;≤&nbsp;''y'' in the partial order, then  ''x''&nbsp;≤&nbsp;''y'' in the total order as well.\n\nOne can define a partial ordering from any DAG by letting the set of objects be the vertices of the DAG, and defining ''x''&nbsp;≤&nbsp;''y'' to be true, for any two vertices ''x'' and ''y'', whenever there exists a [[directed path]] from ''x'' to ''y''; that is,  whenever ''y'' is [[reachability|reachable]] from ''x''. With these definitions, a topological ordering of the DAG is the same thing as a linear extension of this partial order. Conversely, any partial ordering on a finite set may be defined as the reachability relation in a DAG. One way of doing this is to define a DAG that has a vertex for every object in the partially ordered set, and an edge ''xy'' for every pair of objects for which ''x''&nbsp;≤&nbsp;''y''. An alternative way of doing this is to use the [[transitive reduction]] of the partial ordering; in general, this produces DAGs with fewer edges, but the reachability relation in these DAGs is still the same partial order. By using these constructions, one can use topological ordering algorithms to find linear extensions of partial orders.\n\n==See also==\n* [[tsort]], a Unix program for topological sorting\n* [[Feedback arc set]], a set of edges whose removal allows the remaining subgraph to be topologically sorted\n* [[Tarjan's strongly connected components algorithm]], an algorithm that gives the topologically sorted list of strongly connected components in a graph\n* [[pre-topological order]]\n\n== References ==\n{{Reflist}}\n*{{citation\n | last = Cook | first = Stephen A. | authorlink = Stephen Cook\n | title = A Taxonomy of Problems with Fast Parallel Algorithms\n | journal = Information and Control | volume = 64 | issue = 1–3\n | year = 1985 | pages = 2–22\n | doi = 10.1016/S0019-9958(85)80041-3}}.\n*{{citation\n | last1 = Cormen | first1 = Thomas H. | author1-link = Thomas H. Cormen\n | last2 = Leiserson | first2 = Charles E. | author2-link = Charles E. Leiserson\n | last3 = Rivest | first3 = Ronald L. | author3-link = Ronald L. Rivest\n | last4 = Stein | first4 = Clifford | author4-link = Clifford Stein\n | contribution = Section 22.4: Topological sort\n | edition = 2nd\n | isbn = 0-262-03293-7\n | pages = 549–552\n | publisher = MIT Press and McGraw-Hill\n | title = [[Introduction to Algorithms]]\n | year = 2001}}.\n*{{citation\n | last1 = Dekel | first1 = Eliezer\n | last2 = Nassimi | first2 = David\n | last3 = Sahni | first3 = Sartaj\n | doi = 10.1137/0210049\n | issue = 4\n | journal = SIAM Journal on Computing\n | mr = 635424\n | pages = 657–675\n | title = Parallel matrix and graph algorithms\n | volume = 10\n | year = 1981}}.\n*{{citation\n | last = Jarnagin | first = M. P.\n | title = Automatic machine methods of testing PERT networks for consistency\n | year = 1960\n | series = Technical Memorandum No. K-24/60\n | publisher = U. S. Naval Weapons Laboratory\n | location = Dahlgren, Virginia}}.\n*{{citation\n | last = Kahn | first = Arthur B.\n | title = Topological sorting of large networks\n | journal = Communications of the ACM\n | volume = 5 | issue = 11 | year = 1962 | pages = 558–562\n | doi = 10.1145/368996.369025}}.\n*{{citation\n | last = Tarjan | first = Robert E. | authorlink = Robert Tarjan\n | title = Edge-disjoint spanning trees and depth-first search\n | journal = Acta Informatica | volume = 6 | issue = 2 | year = 1976 | pages = 171–185\n | doi = 10.1007/BF00268499}}.\n*{{citation\n | last1 = Vernet | first1 = Oswaldo\n | last2 = Markenzon | first2 = Lilian\n | contribution = Hamiltonian problems for reducible flowgraphs\n | title = Proc. 17th International Conference of the Chilean Computer Science Society (SCCC '97)\n | year = 1997\n | pages = 264–267\n | doi = 10.1109/SCCC.1997.637099}}.\n\n==Further reading==\n*[[D. E. Knuth]], [[The Art of Computer Programming]], Volume 1, section 2.2.3, which gives an algorithm for topological sorting of a partial ordering, and a brief history.\n\n== External links ==\n* [https://xlinux.nist.gov/dads/HTML/topologicalSort.html NIST Dictionary of Algorithms and Data Structures: topological sort]\n* {{MathWorld|urlname=TopologicalSort|title=TopologicalSort}}\n\n{{sorting}}\n\n[[Category:Graph algorithms]]\n[[Category:Sorting algorithms]]\n[[Category:Directed graphs]]\n[[Category:Articles with example pseudocode]]"
    },
    {
      "title": "Tournament sort",
      "url": "https://en.wikipedia.org/wiki/Tournament_sort",
      "text": "{{Infobox Algorithm\n|class=[[Sorting algorithm]]\n|image=\n|caption=\n|data=[[Array data structure|Array]]\n|time=O(''n'' log ''n'')\n|average-time=O(''n'' log ''n'')\n|space=\n|optimal=\n}}\n\n{{refimprove|date=July 2012}}\n'''Tournament sort''' is a [[sorting algorithm]].  It improves upon the naive [[selection sort]] by using a [[priority queue]] to find the next element in the sort.  In the naive selection sort, it takes O(''n'') operations to select the next element of ''n'' elements; in a tournament sort, it takes O(log&nbsp;''n'') operations (after building the initial tournament in O(''n'')).  Tournament sort is a variation of [[heapsort]].\n\n== Common application ==\nTournament replacement selection sorts are used to gather the initial runs for external sorting algorithms. Conceptually, an external file is read and its elements are pushed into the priority queue until the queue is full. Then the minimum element is pulled from the queue and written as part of the first run. The next input element is read and pushed into the queue, and the min is selected again and added to the run. There's a small trick that if the new element being pushed into the queue is less than the last element added to the run, then the element's sort value is increased so it will be part of the next run. On average, a run will be 100% longer than the capacity of the priority queue.<ref>[[Donald Knuth]], [[The Art of Computer Programming]], Sorting and Searching, Volume 3, 1973. The \"snowplow\" argument. p. 254</ref>\n\nTournament sorts may also be used in N-way merges.\n\n== The tournament ==\nThe name comes from its similarity to a [[single-elimination tournament]] where there are many players (or teams) that play in two-sided matches. Each match compares the players, and the winning player is promoted to play at match at the next level up. The hierarchy continues until the final match determines the ultimate winner. The tournament determines the best player, but the player who was beaten in the final match may not be the second best &ndash; he may be inferior to other players the winner bested.\n\n== Implementation ==\nThe following is an implementation of tournament sort in [[Haskell (programming language)|Haskell]], based on [[Scheme (programming language)|Scheme]] code by Stepanov and Kershenbaum.<ref name=Stepanov1986>Stepanov, Alexander and Aaron Kershenbaum. ''Using Tournament Trees to Sort'', Brooklyn: Center for Advanced Technology in Telecommunications, 1986.</ref>\n\n<source lang=\"haskell\">\nimport Data.Tree\n\n\n-- | Adapted from `TOURNAMENT-SORT!` in the Stepanov and Kershenbaum report.\ntournamentSort :: Ord t\n       => [t]  -- ^ Input: an unsorted list\n       -> [t]  -- ^ Result: sorted version of the input\ntournamentSort alist\n        = go (pure<$>alist) -- first, wrap each element as a single-tree forest\n where go [] = []\n       go trees = (rootLabel winner) : (go (subForest winner))\n        where winner = playTournament trees\n\n\n-- | Adapted from `TOURNAMENT!` in the Stepanov and Kershenbaum report\nplayTournament :: Ord t\n         => Forest t -- ^ Input forest\n         -> Tree t   -- ^ The last promoted tree in the input\nplayTournament [tree] = tree\nplayTournament trees = playTournament (playRound trees [])\n\n\n-- | Adapted from `TOURNAMENT-ROUND!` in the Stepanov and Kershenbaum report\nplayRound :: Ord t\n       => Forest t -- ^ A forest of trees that have not yet competed in round\n       -> Forest t -- ^ A forest of trees that have won in round\n       -> Forest t -- ^ Output: a forest containing promoted versions\n                   --   of the trees that won their games\nplayRound [] done = done\nplayRound [tree] done = tree:done\nplayRound (tree0:tree1:trees) done = playRound trees (winner:done)\n where winner = playGame tree0 tree1\n\n\n-- | Adapted from `TOURNAMENT-PLAY!` in the Stepanov and Kershenbaum report\nplayGame :: Ord t\n         => Tree t  -- ^ Input: ...\n         -> Tree t  -- ^ ... two trees\n         -> Tree t  -- ^ Result: `promote winner loser`, where `winner` is\n                    --   the tree with the *lesser* root of the two inputs\nplayGame tree1 tree2\n    | rootLabel tree1 <= rootLabel tree2  = promote tree1 tree2\n    | otherwise                           = promote tree2 tree1\n\n\n-- | Adapted from `GRAB!` in the Stepanov and Kershenbaum report\npromote :: Tree t  -- ^ The `winner`\n        -> Tree t  -- ^ The `loser`\n        -> Tree t  -- ^ Result: a tree whose root is the root of `winner`\n                   --   and whose children are:\n                   --   * `loser`,\n                   --   * all the children of `winner`\npromote winner loser = Node {\n    rootLabel = rootLabel winner,\n    subForest = loser : subForest winner}\n\n\nmain :: IO ()\nmain = print $ tournamentSort testList\n where testList = [-0.202669, 0.969870, 0.142410, -0.685051, 0.487489, -0.339971, 0.832568, 0.00510796, -0.822352, 0.350187, -0.477273, 0.695266]\n</source>\n\n== References ==\n<references/>\n\n* Kershenbaum et al 1988, [http://stepanovpapers.com/hop.pdf \"Higher Order Imperative Programming\"]\n\n{{sorting}}\n\n[[Category:Sorting algorithms]]"
    },
    {
      "title": "Tree sort",
      "url": "https://en.wikipedia.org/wiki/Tree_sort",
      "text": "{{unreferenced|date=September 2014}}\n{{Infobox Algorithm\n|class=[[Sorting algorithm]]\n|image=[[File:Binary tree sort(2).png|200px]]\n|data=[[Array data structure|Array]]\n|time={{math|''O''(''n''²)}} (unbalanced)\n{{math|''O''(''n'' log ''n'')}} (balanced)\n|best-time={{math|''O''(''n'' log ''n'')}} {{Citation needed|date=September 2014}}\n|average-time={{math|''O''(''n'' log ''n''}})\n|space={{math|Θ(''n'')}}\n|optimal=Yes, if balanced\n}}\n\nA '''tree sort''' is a [[sort algorithm]] that builds a [[binary search tree]] from the elements to be sorted, and then traverses the tree ([[Tree traversal|in-order]]) so that the elements come out in sorted order. Its typical use is sorting elements [[online algorithm|online]]: after each insertion, the set of elements seen so far is available in sorted order.\n\n== Efficiency ==\nAdding one item to a binary search tree is on average an {{math|''O''(log ''n'')}} process (in [[big O notation]]). Adding n items is an {{math|''O''(''n'' log ''n'')}} process, making tree sorting a 'fast sort' process. Adding an item to an unbalanced binary tree requires {{math|''O''(''n'')}} time in the worst-case: When the tree resembles a [[linked list]] ([[Binary Tree#Types of binary trees|degenerate tree]]). This results in a worst case of {{math|''O''(''n''²)}} time for this sorting algorithm.\nThis worst case occurs when the algorithm operates on an already sorted set, or one that is nearly sorted, reversed or nearly reversed. Expected {{math|''O''(''n'' log ''n'')}} time can however be achieved by shuffling the array, but this does not help for equal items.\n\nThe worst-case behaviour can be improved by using a [[self-balancing binary search tree]]. Using such a tree, the algorithm has an {{math|''O''(''n'' log ''n'')}} worst-case performance, thus being degree-optimal for a [[comparison sort]]. However, trees require memory to be allocated on the [[Heap (memory management)|heap]], which is a significant performance hit when compared to [[quicksort]] and [[heapsort]]. When using a [[splay tree]] as the binary search tree, the resulting algorithm (called [[splaysort]]) has the additional property that it is an [[adaptive sort]], meaning that its running time is faster than {{math|''O''(''n'' log ''n'')}} for inputs that are nearly sorted.\n\n== Example ==\nThe following tree sort algorithm in pseudocode accepts a [[total order|collection of comparable items]] and outputs the items in ascending order:\n\n {{Blue|STRUCTURE}} BinaryTree\n     BinaryTree:LeftSubTree\n     Object:Node\n     BinaryTree:RightSubTree\n \n {{Blue|PROCEDURE}} Insert(BinaryTree:searchTree, Object:item)\n     {{Blue|IF}} searchTree.Node {{Blue|IS}} NULL {{Blue|THEN}}\n         {{Blue|SET}} searchTree.Node {{Blue|TO}} item\n     {{Blue|ELSE}}\n         {{Blue|IF}} item {{Blue|IS LESS THAN}} searchTree.Node {{Blue|THEN}}\n             Insert(searchTree.LeftSubTree, item)\n         {{Blue|ELSE}}\n             Insert(searchTree.RightSubTree, item)\n \n {{Blue|PROCEDURE}} InOrder(BinaryTree:searchTree)\n     {{Blue|IF}} searchTree.Node {{Blue|IS}} NULL {{Blue|THEN}}\n         {{Blue|EXIT PROCEDURE}}\n     {{Blue|ELSE}}\n         InOrder(searchTree.LeftSubTree)\n         {{Blue|EMIT}} searchTree.Node\n         InOrder(searchTree.RightSubTree)\n \n {{Blue|PROCEDURE}} TreeSort(Collection:items)\n     BinaryTree:searchTree\n    \n     {{Blue|FOR EACH}} individualItem {{Blue|IN}} items\n         Insert(searchTree, individualItem)\n    \n     InOrder(searchTree)\n\n\nIn a simple [[functional programming]] form, the algorithm (in [[Haskell (programming language)|Haskell]]) would look something like this:\n<source lang=\"haskell\">\ndata Tree a = Leaf | Node (Tree a) a (Tree a)\n\ninsert :: Ord a => a -> Tree a -> Tree a\ninsert x Leaf = Node Leaf x Leaf\ninsert x (Node t y s)\n    | x <= y = Node (insert x t) y s\n    | x > y  = Node t y (insert x s)\n\nflatten :: Tree a -> [a]\nflatten Leaf = []\nflatten (Node t x s) = flatten t ++ [x] ++ flatten s\n\ntreesort :: Ord a => [a] -> [a]\ntreesort = flatten . foldr insert Leaf\n</source>\nIn the above implementation, both the insertion algorithm and the retrieval algorithm have {{math|''O''(''n''²)}} worst-case scenarios.\n\n== External links==\n{{wikibooks|Algorithm Implementation|Sorting/Binary Tree Sort|Binary Tree Sort}}\n\n* {{webarchive|url=http://web.archive.org/web/20161129234513/http://qmatica.com/DataStructures/Trees/BST.html |date=29 November 2016|title=Binary Tree Java Applet and Explanation}}\n* [http://www.martinbroadhurst.com/articles/sorting-a-linked-list-by-turning-it-into-a-binary-tree.html Tree Sort of a Linked List]\n* [http://www.martinbroadhurst.com/cpp-sorting.html#tree-sort Tree Sort in C++]\n\n{{sorting}}\n\n[[Category:Sorting algorithms]]\n[[Category:Online sorts]]"
    },
    {
      "title": "X + Y sorting",
      "url": "https://en.wikipedia.org/wiki/X_%2B_Y_sorting",
      "text": "{{unsolved|computer science|Is there an X + Y sorting algorithm faster than <math>O(n^2 \\log n)</math>?}}\nIn [[computer science]], '''X + Y sorting''' is the problem of [[Sorting algorithm|sorting]] [[ordered pair|pairs]] of numbers by their sum. Given two finite sets {{mvar|X}} and {{mvar|Y}}, the problem is to order all pairs {{math|(''x'', ''y'')}} in the [[Cartesian product]] {{math|''X'' × ''Y''}} by the key {{math|''x'' + ''y''}}. The problem is attributed to [[Elwyn Berlekamp]].{{r|harper}}{{r|topp}}\n\nThis problem can be solved using a straightforward [[comparison sort]] on the Cartesian product, taking [[time complexity|time]] {{math|''O''(''nm'' log(''nm''))}} for sets of sizes {{mvar|n}} and {{mvar|m}}. When it is assumed that {{math|''m'' {{=}} ''n''}}, the complexity is {{math|''O''(''n''<sup>2</sup> log ''n''<sup>2</sup>) {{=}} ''O''(''n''<sup>2</sup> log ''n'')}}, which is also the best known bound on the problem, but whether X + Y sorting can be done strictly faster than sorting {{math|''n''⋅''m''}} arbitrary numbers is an [[List of unsolved problems in computer science|open problem]].<ref name=\"skiena\"/><ref name=\"topp\">{{cite web |first1=Erik |last1=Demaine |authorlink1=Erik Demaine |first2=Jeff |last2=Erickson |first3=Joseph |last3=O'Rourke |title=Problem 41: Sorting X + Y (Pairwise Sums) |date=20 August 2006 |accessdate=23 September 2014 |website=The Open Problems Project |url=http://cs.smith.edu/~orourke/TOPP/P41.html}}</ref>\nThe number of required comparisons is certainly lower than for ordinary comparison sorting: [[Michael Fredman|Fredman]] showed, in 1976, that X + Y sorting can be done using only {{math|''O''(''n''<sup>2</sup>)}} comparisons, though he [[Non-constructive algorithm existence proofs|did not show an algorithm]].<ref name=\"fredman\">{{cite journal |first=Michael L. |last=Fredman |authorlink=Michael Fredman |title=How good is the information theory bound in sorting? |journal=Theoretical Computer Science |volume=1 |issue=4 |pages=355–361 |doi=10.1016/0304-3975(76)90078-5 |year=1976}}</ref> The first actual algorithm that achieves this number of comparisons and {{math|''O''(''n''<sup>2</sup> log ''n'')}} total complexity was only published sixteen years later.<ref>{{cite journal |first=Jean-Luc |last=Lambert |title=Sorting the sums ({{mvar|x<sub>i</sub>}} + {{mvar|y<sub>j</sub>}}) in {{math|''O''(''n''<sup>2</sup>)}} comparisons |journal=Theoretical Computer Science |volume=103 |issue=1 |pages=137–141 |year=1992 |doi=10.1016/0304-3975(92)90089-X}}</ref>\n\nOn a [[RAM machine]] with [[word size]] {{mvar|w}} and integer inputs {{math|0 ≤ {''x'', ''y''} < ''n'' {{=}} 2<sup>''w''</sup>}}, the problem can be solved in {{math|''O''(''n'' log ''n'')}} operations by means of the [[fast Fourier transform]].<ref name=\"harper\">{{cite journal |last1=Harper |first1=L. H. |last2=Payne |first2=T.H. |last3=Savage |first3=J. E. |last4=Straus |first4=E. |title=Sorting X + Y |journal=[[Communications of the ACM]] |volume=18 |issue=6 |year=1975 |pages=347–349 |doi=10.1145/360825.360869}}</ref>\n\nSkiena recounts a practical application in transit [[fare]] minimisation, an instance of the [[shortest path problem]]: given fares {{mvar|x}} and {{mvar|y}} for trips from departure A to some intermediate destination B and from B to final destination C, determine the least expensive combined trip from A to C.<ref name=\"skiena\">{{cite book |first=Steven |last=Skiena |authorlink=Steven Skiena |title=The Algorithm Design Manual |publisher=Springer |year=2008 |page=119 |doi=10.1007/978-1-84800-070-4_4}}</ref>\n\n==See also==\n* [[3SUM]]\n* [[Integer sorting]]\n\n==References==\n{{reflist|30em}}\n\n[[Category:Sorting algorithms]]\n[[Category:Unsolved problems in computer science]]"
    },
    {
      "title": "Introsort",
      "url": "https://en.wikipedia.org/wiki/Introsort",
      "text": "{{Short description|Hybrid sorting algorithm}}\n{{Infobox Algorithm\n|class=[[Sorting algorithm]]\n|image=\n|caption=\n|data=[[Array data structure|Array]]\n|time=O(''n'' log ''n'')\n|average-time=O(''n'' log ''n'')\n|space=\n|optimal=yes\n}}\n\n'''Introsort''' or '''introspective sort''' is a [[hybrid algorithm|hybrid]] [[sorting algorithm]] that provides both fast average performance and (asymptotically) optimal worst-case performance. It begins with [[quicksort]] and switches to [[heapsort]] when the recursion depth exceeds a level based on (the [[logarithm]] of) the number of elements being sorted. This combines the good parts of both algorithms, with practical performance comparable to quicksort on typical data sets and worst-case [[Big-O notation|O]](''n'' log ''n'') runtime due to the heap sort. Since both algorithms it uses are [[comparison sort]]s, it is also a comparison sort.\n\nIntrosort was invented by [[David Musser]] in {{harvtxt|Musser|1997}}, in which he also introduced [[introselect]], a hybrid [[selection algorithm]] based on [[quickselect]] (a variant of quicksort), which falls back to [[median of medians]] and thus provides worst-case linear complexity, which is optimal. Both algorithms were introduced with the purpose of providing [[generic algorithm]]s for the [[C++ Standard Library]] which had both fast average performance and optimal worst-case performance, thus allowing the performance requirements to be tightened.<ref>\"[http://www.cs.rpi.edu/~musser/gp/algorithms.html Generic Algorithms]\", [[David Musser]]</ref>\n\n==Pseudocode==\nIf a heapsort implementation and partitioning functions of the type discussed in the [[quicksort]] article are available, the introsort can be described succinctly as\n\n '''procedure''' sort(A : array):\n     '''let''' maxdepth = ⌊log(length(A))⌋ × 2\n     introsort(A, maxdepth)\n \n '''procedure''' introsort(A, maxdepth):\n     n ← length(A)\n     '''if''' n ≤ 1:\n         '''return'''  ''// base case''\n     '''else if''' maxdepth = 0:\n         heapsort(A)\n     '''else''':\n         p ← partition(A)  ''// assume this function does pivot selection, p is the final position of the pivot''\n         introsort(A[0:p-1], maxdepth - 1)\n         introsort(A[p+1:n], maxdepth - 1)\n\nThe factor 2 in the maximum depth is arbitrary; it can be tuned for practical performance. {{math|''A''[''i'':''j'']}} denotes the [[array slicing|array slice]] of items {{mvar|i}} to {{mvar|j}}.\n\n==Analysis==\nIn quicksort, one of the critical operations is choosing the pivot: the element around which the list is partitioned. The simplest pivot selection algorithm is to take the first or the last element of the list as the pivot, causing poor behavior for the case of sorted or nearly sorted input. [[Niklaus Wirth]]'s variant uses the middle element to prevent these occurrences, degenerating to O(''n''<sup>2</sup>) for contrived sequences. The median-of-3 pivot selection algorithm takes the median of the first, middle, and last elements of the list; however, even though this performs well on many real-world inputs, it is still possible to contrive a ''median-of-3 killer'' list that will cause dramatic slowdown of a quicksort based on this pivot selection technique.\n\nMusser reported that on a median-of-3 killer sequence of 100,000 elements, introsort's running time was 1/200 that of median-of-3 quicksort. Musser also considered the effect on [[CPU cache|caches]] of [[Robert Sedgewick (computer scientist)|Sedgewick]]'s delayed small sorting, where small ranges are sorted at the end in a single pass of [[insertion sort]]. He reported that it could double the number of cache misses, but that its performance with [[double-ended queue]]s was significantly better and should be retained for template libraries, in part because the gain in other cases from doing the sorts immediately was not great.\n\n==Implementations==\nIntrosort or some variant is used in a number of [[standard library]] sort functions, including some [[sort (C++)|C++ sort]] implementations.\n\nThe June 2000 [[Silicon Graphics|SGI]] C++ [[Standard Template Library]] [http://www.sgi.com/tech/stl/stl_algo.h stl_algo.h] implementation of [[unstable sort]] uses the Musser introsort approach with the recursion depth to switch to heapsort passed as a parameter, median-of-3 pivot selection and the Knuth final insertion sort pass for partitions smaller than 16.\n\nThe [[GNU Standard C++ library]] is similar: uses introsort with a maximum depth of 2&times;log<sub>2</sub> ''n'', followed by an [[insertion sort]] on partitions smaller than 16.<ref>[https://gcc.gnu.org/onlinedocs/libstdc++/libstdc++-html-USERS-4.4/a01027.html libstdc++ Documentation: Sorting Algorithms]</ref>\n\nThe [[Microsoft .NET Framework]] [[Base Class Library|Class Library]], starting from version 4.5 (2012), uses Introsort instead of simple [[QuickSort]].<ref>[http://msdn.microsoft.com/en-us/library/6tf1f0bc(v=vs.110).aspx Array.Sort Method (Array)]</ref>\n\n==References==\n{{reflist}}\n\n===General===\n{{refbegin}}\n* {{Cite journal | last = Musser | first = David R. | authorlink = David Musser | title = Introspective Sorting and Selection Algorithms | url = http://www.cs.rpi.edu/~musser/gp/introsort.ps| doi = 10.1002/(SICI)1097-024X(199708)27:8<983::AID-SPE117>3.0.CO;2-# | journal = Software: Practice and Experience | volume = 27 | issue = 8 | pages = 983–993 | year = 1997 | ref = harv }}\n* Niklaus Wirth. ''Algorithms and Data Structures''. Prentice-Hall, Inc., 1985. {{ISBN|0-13-022005-1}}.\n{{refend}}\n\n{{sorting}}\n\n[[Category:Comparison sorts]]\n[[Category:Articles with example pseudocode]]"
    },
    {
      "title": "Merge-insertion sort",
      "url": "https://en.wikipedia.org/wiki/Merge-insertion_sort",
      "text": "In [[computer science]], '''merge-insertion sort''' or the '''Ford–Johnson algorithm''' is a [[comparison sort]]ing algorithm published in 1959 by [[L. R. Ford Jr.]] and [[Selmer M. Johnson]].{{r|fj|c4cs|distrib|knuth}} It uses fewer comparisons in the [[worst case analysis|worst case]] than the best previously known algorithms, [[insertion sort|binary insertion sort]] and [[merge sort]],{{r|fj}} and for 20 years it was the sorting algorithm with the fewest known comparisons.{{r|nonopt}} Although not of practical significance, it remains of theoretical interest in connection with the problem of sorting with a minimum number of comparisons.{{r|distrib}} The same algorithm may have also been independently discovered by [[Stanisław Trybuła]] and Czen Ping.{{r|knuth}}\n\n==Algorithm==\nMerge-insertion sort performs the following steps, on an input <math>X</math> of <math>n</math> elements:<ref>The original description by {{harvtxt|Ford|Johnson|1959}} sorted the elements in descending order. The steps listed here reverse the output, following the description in {{harvtxt|Knuth|1998}}. The resulting algorithm makes the same comparisons but produces ascending order instead.</ref>\n#Group the elements of <math>X</math> into <math>\\lfloor n/2\\rfloor</math> pairs of elements, arbitrarily, leaving one element unpaired if there is an odd number of elements. \n#Perform <math>\\lfloor n/2\\rfloor</math> comparisons, one per pair, to determine the larger of the two elements in each pair.\n#Recursively sort the <math>\\lfloor n/2\\rfloor</math> larger elements from each pair, creating a sorted sequence <math>S</math> of <math>\\lfloor n/2\\rfloor</math> of the input elements, in ascending order.\n#Insert at the start of <math>S</math> the element that was paired with the first and smallest element of <math>S</math>.\n#Insert the remaining <math>\\lceil n/2\\rceil-1</math> elements of <math>X\\setminus S</math> into <math>S</math>, one at a time, with a specially chosen insertion ordering described below. Use [[binary search]] in subsequences of <math>S</math> (as described below) to determine the position at which each element should be inserted.\n\nThe algorithm is designed to take advantage of the fact that the binary searches used to insert elements into <math>S</math> are most efficient (from the point of view of worst case analysis) when the length of the subsequence that is searched is one less than a [[power of two]]. This is because, for those lengths, all outcomes of the search use the same number of comparisons as each other.{{r|fj}} To choose an insertion ordering that produces these lengths, consider the sorted sequence <math>S</math> after step 4 of the outline above (before inserting the remaining elements),\nand let <math>x_i</math> denote the <math>i</math>th element of this sorted sequence. Thus,\n:<math>S=(x_1,x_2,x_3,\\dots),</math>\nwhere each element <math>x_i</math> with <math>i\\ge 3</math> is paired with an element <math>y_i < x_i</math> that has not yet been inserted. (There are no elements <math>y_1</math> or <math>y_2</math> because <math>x_1</math> and <math>x_2</math> were paired with each other.) If <math>n</math> is odd, the remaining unpaired element should also be numbered as <math>y_i</math> with <math>i</math> larger than the indexes of the paired elements.\nThen, the final step of the outline above can be expanded into the following steps:{{r|fj|c4cs|distrib|knuth}}\n*Partition the uninserted elements <math>y_i</math> into groups with contiguous indexes. There are two elements <math>y_3</math> and <math>y_4</math> in the first group, and the sums of sizes of every two adjacent groups form a sequence of powers of two. Thus, the sizes of groups are: 2, 2, 6, 10, 22, 42, ...\n*Order the uninserted elements by their groups (smaller indexes to larger indexes), but within each group order them from larger indexes to smaller indexes. Thus, the ordering becomes\n::<math>y_4,y_3,y_6,y_5,y_{12},y_{11},y_{10},y_9,y_8,y_7,y_{22},y_{21}\\dots</math> \n*Use this ordering to insert the elements <math>y_i</math> into <math>S</math>. For each element <math>y_i</math>, use a binary search from the start of <math>S</math> up to but not including <math>x_i</math> to determine where to insert <math>y_i</math>.\n\n==Analysis==\nLet <math>C(n)</math> denote the number of comparisons that merge-insertion sort makes, in the worst case, when sorting <math>n</math> elements.\nThis number of comparisons can be broken down as the sum of three terms:\n*<math>\\lfloor n/2\\rfloor</math> comparisons among the pairs of items,\n*<math>C(\\lfloor n/2\\rfloor)</math> comparisons for the recursive call, and\n*some number of comparisons for the binary insertions used to insert the remaining elements.\nIn the third term, the worst-case number of comparisons for the elements in the first group is two, because each is inserted into a subsequence of <math>S</math> of length at most three. First, <math>y_4</math> is inserted into the three-element subsequence <math>(x_1,x_2,x_3)</math>. Then, <math>y_3</math> is inserted into some permutation of the three-element subsequence <math>(x_1,x_2,y_4)</math>, or in some cases into the two-element subsequence <math>(x_1,x_2)</math>. Similarly, the elements <math>y_6</math> and <math>y_5</math> of the second group are each inserted into a subsequence of length at most seven, using three comparisons. More generally, the worst-case number of comparisons for the elements in the <math>i</math>th group is <math>i+1</math>, because each is inserted into a subsequence of length at most <math>2^{i+1}-1</math>.{{r|fj|c4cs|distrib|knuth}} By summing the number of comparisons used for all the elements and solving the resulting [[recurrence relation]],\nthis analysis can be used to compute the values of <math>C(n)</math>, giving the formula<ref>{{harvtxt|Knuth|1998}} credits the summation formula to the 1960 Ph.D. thesis of A. Hadian. The approximation formula was already given by {{harvtxt|Ford|Johnson|1959}}.</ref>\n:<math>C(n)=\\sum_{i=1}^n \\left\\lceil \\log_2 \\frac{3i}{4} \\right\\rceil \\approx n\\log_2 n - 1.415n</math>\nor, in [[closed-form expression|closed form]],{{r|gn}}\n:<math>C(n)=n\\biggl\\lceil\\log_2\\frac{3n}{4}\\biggr\\rceil-\\biggl\\lfloor\\frac{2^{\\lfloor \\log_2 6n\\rfloor}}{3}\\biggr\\rfloor+\\biggl\\lfloor\\frac{\\log_2 6n}{2}\\biggr\\rfloor.</math>\nFor <math>n=1,2,\\dots</math> the numbers of comparisons are{{r|fj}}\n:0, 1, 3, 5, 7, 10, 13, 16, 19, 22, 26, 30, 34, ... {{OEIS|A001768}}\n\n==Relation to other comparison sorts==\nThe algorithm is called merge-insertion sort because the initial comparisons that it performs before its recursive call (pairing up arbitrary items and comparing each pair) are the same as the initial comparisons of [[merge sort]],\nwhile the comparisons that it performs after the recursive call (using binary search to insert elements one by one into a sorted list) follow the same principle as [[insertion sort]]. In this sense, it is a [[hybrid algorithm]] that combines both merge sort and insertion sort.<ref>{{harvtxt|Knuth|1998}}, p. 184: \"Since it involves some aspects of merging and some aspects of insertion, we call it ''merge insertion''.\"</ref>\n\nFor small inputs (up to <math>n=11</math>) its numbers of comparisons equal the [[lower bound]] on comparison sorting of <math>\\lceil\\log_2 n!\\rceil\\approx n\\log_2 n -  1.443n</math>. However, for larger inputs the number of comparisons made by the merge-insertion algorithm is bigger than this lower bound.\nMerge-insertion sort also performs fewer comparisons than the [[sorting number]]s, which count the comparisons made by binary insertion sort or merge sort in the worst case. The sorting numbers fluctuate between <math>n\\log_2 n - 0.915n</math> and <math>n\\log_2 n - n</math>, with the same leading term but a worse constant factor in the lower-order linear term.{{r|fj}}\n\nMerge-insertion sort is the sorting algorithm with the minimum possible comparisons for <math>n</math> items whenever <math>n\\le 15</math> or <math>20\\le n\\le 22</math>, and it has the fewest comparisons known for <math>n\\le 46</math>.{{r|pec|pec2}}\nFor 20 years, merge-insertion sort was the sorting algorithm with the fewest comparisons known for all input lengths.\nHowever, in 1979 Glenn Manacher published another sorting algorithm that used even fewer comparisons, for large enough inputs.{{r|distrib|nonopt}}\nIt remains unknown exactly how many comparisons are needed for sorting, for all <math>n</math>, but Manacher's algorithm\nand later record-breaking sorting algorithms have all used modifications of the merge-insertion sort ideas.{{r|distrib}}\n\n==References==\n{{reflist|refs=\n\n<ref name=c4cs>{{citation\n | last = Williamson | first = Stanley Gill\n | contribution = 2.31 Merge insertion (Ford–Johnson)\n | contribution-url = https://books.google.com/books?id=YMIoy5JwdHMC&pg=PA66\n | isbn = 9780486420769\n | pages = 66–68\n | publisher = Courier Corporation\n | series = Dover books on mathematics\n | title = Combinatorics for Computer Science\n | year = 2002}}</ref>\n\n<ref name=distrib>{{citation\n | last = Mahmoud | first = Hosam M.\n | contribution = 12.3.1 The Ford–Johnson algorithm\n | contribution-url = https://books.google.com/books?id=kM5v2YqMVuoC&pg=PA286\n | isbn = 9781118031131\n | pages = 286–288\n | publisher = John Wiley & Sons\n | series = Wiley Series in Discrete Mathematics and Optimization\n | title = Sorting: A Distribution Theory\n | volume = 54\n | year = 2011}}</ref>\n\n<ref name=fj>{{citation\n | last1 = Ford | first1 = Lester R. Jr. | author1-link = L. R. Ford Jr.\n | last2 = Johnson | first2 = Selmer M. | author2-link = Selmer M. Johnson\n | doi = 10.2307/2308750\n | journal = [[American Mathematical Monthly]]\n | mr = 0103159\n | pages = 387–389\n | title = A tournament problem\n | volume = 66\n | year = 1959}}</ref>\n\n<ref name=gn>{{citation\n | last1 = Guy | first1 = Richard K. | author1-link = Richard K. Guy\n | last2 = Nowakowski | first2 = Richard J.\n | date = December 1995\n | doi = 10.2307/2975272\n | issue = 10\n | journal = [[American Mathematical Monthly]]\n | pages = 921–926\n | title = ''Monthly'' Unsolved Problems, 1969-1995\n | volume = 102}}</ref>\n\n<ref name=knuth>{{citation\n | last = Knuth | first = Donald E. | author-link = Donald Knuth\n | contribution = Merge insertion\n | edition = 2nd\n | pages = 184–186\n | title = [[The Art of Computer Programming]], Vol. 3: Sorting and Searching\n | year = 1998}}</ref>\n\n<ref name=nonopt>{{citation\n | last = Manacher | first = Glenn K.\n | date = July 1979\n | doi = 10.1145/322139.322145\n | issue = 3\n | journal = [[Journal of the ACM]]\n | pages = 441–456\n | title = The Ford-Johnson Sorting Algorithm Is Not Optimal\n | volume = 26}}</ref>\n\n<ref name=pec>{{citation\n | last = Peczarski | first = Marcin\n | doi = 10.1007/s00453-004-1100-7\n | issue = 2\n | journal = Algorithmica\n | mr = 2072769\n | pages = 133–145\n | title = New results in minimum-comparison sorting\n | volume = 40\n | year = 2004}}</ref>\n\n<ref name=pec2>{{citation\n | last = Peczarski | first = Marcin\n | doi = 10.1016/j.ipl.2006.09.001\n | issue = 3\n | journal = Information Processing Letters\n | mr = 2287331\n | pages = 126–128\n | title = The Ford-Johnson algorithm still unbeaten for less than 47 elements\n | volume = 101\n | year = 2007}}</ref>\n\n}}\n\n{{Sorting}}\n[[Category:Comparison sorts]]"
    },
    {
      "title": "Multi-key quicksort",
      "url": "https://en.wikipedia.org/wiki/Multi-key_quicksort",
      "text": "'''Multi-key quicksort''', also known as '''three-way radix quicksort''',<ref>{{DADS|multikey Quicksort|multikeyQuicksort}}</ref> is an [[algorithm]] for [[sorting algorithm|sorting]] [[String (computer science)|strings]]. This hybrid of [[quicksort]] and [[radix sort]] was originally suggested by P.&nbsp;Shackleton, as reported in one of [[Tony Hoare|C.A.R.&nbsp;Hoare]]'s seminal papers on [[quicksort]];<ref>{{Cite journal | last1 = Hoare | first1 = C. A. R. | authorlink1 = Tony Hoare| doi = 10.1093/comjnl/5.1.10 | title = Quicksort | journal = [[The Computer Journal|Comput. J.]] | volume = 5 | issue = 1 | pages = 10–16 | year = 1962 | pmid =  | pmc = }}</ref>{{rp|14}} its modern incarnation was developed by [[Jon Bentley (computer scientist)|Jon Bentley]] and [[Robert Sedgewick (computer scientist)|Robert Sedgewick]] in the mid-1990s.<ref name=\"soda\">{{cite conference |first1=Jon |last1=Bentley |first2=Robert |last2=Sedgewick |title=Fast algorithms for sorting and searching strings |year=1997 |conference=Proc. Annual ACM-SIAM Symp. on Discrete Algorithms (SODA) |url=http://www.cs.princeton.edu/~rs/strings/paper.pdf |isbn=0-89871-390-0}}</ref> The algorithm is designed to exploit the property that in many problems, strings tend to have shared [[Substring#Prefix|prefixes]].\n\nOne of the algorithm's uses is the construction of [[suffix array]]s, for which it was one of the fastest algorithms as of 2004.<ref>{{Cite journal| doi = 10.1007/s00453-004-1094-1| title = Engineering a Lightweight Suffix Array Construction Algorithm| journal = Algorithmica| volume = 40| pages = 33–50| year = 2004| last1 = Manzini | first1 = Giovanni| last2 = Ferragina | first2 = Paolo| citeseerx = 10.1.1.385.5959}}</ref>\n\n==Description==\nThe three-way radix quicksort algorithm sorts an array of {{mvar|N}} ([[Pointer (computer programming)|pointers]] to) strings in [[Lexicographical order|lexicographic order]]. It is assumed that all strings are of equal length {{mvar|K}}; if the strings are of varying length, they must be padded with extra elements that are less-than any element in the strings.{{Efn|One way to do so without altering the in-memory representation of the strings is to index them using a function that returns −1 or some other small value when the index is out of range.}} The pseudocode for the algorithm is then{{Efn|Arrays and strings are zero-indexed. An [[Array slicing|array slice]] {{mono|a[i:j)}} yields the subarray of {{mono|a}} from {{mono|i}} to {{mono|j}} (exclusive) and is assumed to be a non-copying, constant-time operation.}}\n\n '''algorithm''' sort(a : array of string, d : integer) '''is'''\n     '''if''' length(a) ≤ 1 '''or''' d ≥ K '''then'''\n         '''return'''\n     p := pivot(a, d)\n     i, j := partition(a, d, p)   ''(Note a simultaneous assignment of two variables.)''\n     sort(a[0:i), d)\n     sort(a[i:j), d+1)\n     sort(a[j:length(a)), d)\n\nThe {{mono|pivot}} function must return a single character. Bentley and Sedgewick suggest either picking the [[median]] of {{mono|a[0][d], ..., a[length(a)−1][d]}} or some random character in that range.{{r|soda}} The partition function is a variant of the one used in ordinary [[Quicksort#Repeated elements|three-way quicksort]]: it rearranges {{mono|a}} so that all of {{mono|a[0], ..., a[i−1]}} have an element at position {{mono|d}} that is less than {{mono|p}}, {{mono|a[i], ..., a[j−1]}} have {{mvar|p}} at position {{mvar|d}}, and strings from {{mvar|j}} onward have a {{mono|d}}'th element larger than {{mvar|p}}. (The original partitioning function suggested by Bentley and Sedgewick may be slow in the case of repeated elements; a [[Dutch national flag problem|Dutch national flag partitioning]] can be used to alleviate this.<ref>{{Cite journal | doi = 10.1016/j.ipl.2009.01.007| title = Improving multikey Quicksort for sorting strings with many equal elements| journal = Information Processing Letters| volume = 109| issue = 9| pages = 454–459| year = 2009| last1 = Kim | first1 = Eunsang| last2 = Park | first2 = Kunsoo}}</ref>)\n\nPractical implementations of multi-key quicksort can benefit from the same optimizations typically applied to quicksort: median-of-three pivoting, switching to [[insertion sort]] for small arrays, etc.<ref>{{cite journal |first1=Jon |last1=Bentley |first2=Robert |last2=Sedgewick |title=Sorting Strings with Three-Way Radix Quicksort |year=1998 |journal=[[Dr. Dobb's Journal]] |url=http://www.drdobbs.com/database/sorting-strings-with-three-way-radix-qui/184410724}}</ref>\n\n==See also==\n{{Portal|Computer science}}\n\n* [[American flag sort]]{{snd}} another radix sort variant that is fast for string sorting\n* [[Ternary search tree]]{{snd}} three-way radix quicksort is [[isomorphic]] to this data structure in the same way that quicksort is isomorphic to binary search trees{{r|soda}}\n\n==Notes==\n{{notelist}}\n\n==References==\n{{reflist|30em}}\n\n[[Category:Articles with example pseudocode]]\n[[Category:Comparison sorts]]\n[[Category:String sorting algorithms]]"
    },
    {
      "title": "Patience sorting",
      "url": "https://en.wikipedia.org/wiki/Patience_sorting",
      "text": "{{Infobox Algorithm\n|class=[[Sorting algorithm]]\n|image=\n|data=[[Array data structure|Array]]\n|time={{math|''O''(''n'' log ''n'')}}\n|best-time={{math|''O''(''n'')}}; occurs when the input is [[adaptive sort|pre-sorted]]<ref name=\"Chandramouli\">{{Cite conference |last1=Chandramouli |first1=Badrish |last2=Goldstein |first2=Jonathan |title=Patience is a Virtue: Revisiting Merge and Sort on Modern Processors |conference=SIGMOD/PODS |year=2014 |url=http://research.microsoft.com/pubs/209622/patsort-sigmod14.pdf}}</ref>\n|space=\n|optimal=?\n}}\nIn [[computer science]], '''patience sorting''' is a [[sorting algorithm]] inspired by, and named after, the card game [[Patience (game)|patience]]. A variant of the algorithm efficiently computes the length of a [[longest increasing subsequence]] in a given [[Array data structure|array]].\n\n==Overview==\nThe algorithm's name derives from a simplified variant of the patience card game. This game begins with a shuffled deck of cards. These cards are dealt one by one into a sequence of piles on the table, according to the following rules.<ref name=\"Burstein\">{{cite journal |journal=Séminaire Lotharingien de Combinatoire |volume=54A |year=2006 |title=Combinatorics of patience sorting piles |first1=Alexander |last1=Burstein |first2=Isaiah |last2=Lankham |url=http://www.emis.ams.org/journals/SLC/wpapers/s54Aburlank.pdf}}</ref>\n\n# Initially, there are no piles. The first card dealt forms a new pile consisting of the single card.\n# Each subsequent card is placed on the leftmost existing pile whose top card has a value greater than or equal the new card's value, or to the right of all of the existing piles, thus forming a new pile.\n# When there are no more cards remaining to deal, the game ends.\n\nThis card game is turned into a two-phase sorting algorithm, as follows. Given an array of {{mvar|n}} elements from some [[Total order|totally ordered]] domain, consider this array as a collection of cards and simulate the patience sorting game. When the game is over, recover the sorted sequence by repeatedly picking off the minimum visible card; in other words, perform a [[K-way merge|{{mvar|k}}-way merge]] of the {{mvar|p}} piles, each of which is internally sorted.\n\n===Analysis===\nThe first phase of patience sort, the card game simulation, can be implemented to take {{math|''O''(''n'' log ''n'')}} comparisons in the worst case for an {{mvar|n}}-element input array: there will be at most {{mvar|n}} piles, and by construction, the top cards of the piles form an increasing sequence from left to right, so the desired pile can be found by [[binary search]].{{r|Chandramouli}} The second phase, the merging of piles, can be done in {{math|''O''(''n'' log ''n'')}} time as well using a [[priority queue]].{{r|Chandramouli}}\n\nWhen the input data contain natural \"runs\", i.e., non-decreasing subarrays, then performance can be strictly better. In fact, when the input array is already sorted, all values form a single pile and both phases run in {{math|''O''(''n'')}} time. The [[average-case]] complexity is still {{math|''O''(''n'' log ''n'')}}: any uniformly random sequence of values will produce an [[Expected value|expected number]] of {{math|''O''({{sqrt|''n''}})}} piles,{{r|Bespamyatnikh}} which take {{math|''O''(''n'' log {{sqrt|''n''}}) {{=}} ''O''(''n'' log ''n'')}} time to produce and merge.{{r|Chandramouli}}\n\nAn evaluation of the practical performance of patience sort is given by Chandramouli and Goldstein, who show that a naïve version is about ten to twenty times slower than a state-of-the-art [[quicksort]] on their benchmark problem. They attribute this to the relatively small amount of research put into patience sort, and develop several optimizations that bring its performance to within a factor two of that of quicksort.{{r|Chandramouli}}\n\nIf values of cards are in the range <math>1, \\ldots, n</math>, there is an efficient implementation with <math>O(n \\log \\log n)</math> [[worst-case]] running time for putting the cards into piles, relying on a [[Van Emde Boas tree]].<ref name=Bespamyatnikh>{{cite journal |first1=Sergei |last1=Bespamyatnikh |first2=Michael |last2=Segal |title=Enumerating Longest Increasing Subsequences and Patience Sorting |journal=[[Information Processing Letters]] |volume=76 |issue=1–2 |year=2000 |pages=7–11 |doi=10.1016/s0020-0190(00)00124-1|citeseerx=10.1.1.40.5912 }}</ref>\n\n==Relations to other problems==\nPatience sorting is closely related to a card game called Floyd's game. This game is very similar to the game sketched earlier:{{r|Burstein}}\n\n# The first card dealt forms a new pile consisting of the single card.\n# Each subsequent card is placed on ''some'' existing pile whose top card has a value no less than the new card's value, or to the right of all of the existing piles, thus forming a new pile.\n# When there are no more cards remaining to deal, the game ends.\n\nThe object of the game is to finish with as few piles as possible. The difference with the patience sorting algorithm is that there is no requirement to place a new card on the ''leftmost'' pile where it is allowed. Patience sorting constitutes a [[greedy algorithm|greedy strategy]] for playing this game.\n\nAldous and Diaconis suggest defining 9 or fewer piles as a winning outcome for {{math|''n'' {{=}} 52}}, which happens with approximately 5% probability.<ref name=\"Aldous\">{{cite journal |first1=David |last1=Aldous |authorlink1=David Aldous |first2=Persi |last2=Diaconis |authorlink2=Persi Diaconis |url=http://www-stat.stanford.edu/~cgates/PERSI/year.html#99 |title=Longest increasing subsequences: from patience sorting to the Baik-Deift-Johansson theorem |journal=Bulletin (New Series) of the AMS |volume=36 |issue=4 |pages=413–432 |doi=10.1090/s0273-0979-99-00796-x|year=1999 }}</ref>\n\n===Algorithm for finding a longest increasing subsequence===\nFirst, execute the sorting algorithm as described above. The number of piles is the length of a longest subsequence. Whenever a card is placed on top of a pile, put a [[back-pointer]] to the top card in the previous pile (that, by assumption, has a lower value than the new card has). In the end, follow the back-pointers from the top card in the last pile to recover a decreasing subsequence of the longest length; its reverse is an answer to the longest increasing subsequence algorithm.\n\nS.&nbsp;Bespamyatnikh and M.&nbsp;Segal<ref name=Bespamyatnikh/> give a description of an efficient implementation of the algorithm, incurring no additional [[asymptotic]] cost over the sorting one (as the back-pointers storage, creation and traversal require linear time and space). They further show how to report ''all'' the longest increasing subsequences from the same resulting [[data structure]]s.\n\n==History==\nPatience sorting was named by C. L. Mallows, who attributed its invention to A.S.C. Ross in the early 1960s.{{r|Chandramouli}}\nAccording to Aldous and Diaconis,<ref name=Aldous/> patience sorting was first recognized as an algorithm to compute the longest increasing subsequence length by Hammersley,<ref>{{cite conference |first=John |last=Hammersley |authorlink=John Hammersley |title=A few seedlings of research |conference=Proc. Sixth Berkeley Symp. Math. Statist. and Probability |volume=1 |pages=345–394 |publisher=University of California Press |year=1972}}</ref> and by A.S.C. Ross and independently [[Robert W. Floyd]] as a sorting algorithm. Initial analysis was done by Mallows.<ref>{{cite journal |first=C. L. |last=Mallows |title=Patience sorting |journal=Bull. Inst. Math. Appl. |volume=9 |pages=216–224 |year=1973}}</ref> Floyd's game was developed by Floyd in correspondence with [[Donald Knuth]].{{r|Burstein}}\n\n==Use==\nThe patience sorting algorithm can be applied to [[process control]]. Within a series of measurements, the existence of a long increasing subsequence can be used as a trend marker. A 2002 article in SQL Server magazine includes a SQL implementation, in this context, of the patience sorting algorithm for the length of the longest increasing subsequence.<ref>{{cite journal|url=http://sqlmag.com/t-sql/statistical-process-control|last=Kass|first=Steve|title=Statistical Process Control|journal=SQL Server Pro|date=April 30, 2002|accessdate=23 April 2014}}</ref>\n\n==References==\n{{Reflist|30em}}\n\n{{wikibooks|Algorithm implementation|Sorting/Patience sort|Patience sorting}}\n\n{{sorting}}\n\n[[Category:Comparison sorts]]\n[[Category:Solitaire card games]]"
    },
    {
      "title": "Selection algorithm",
      "url": "https://en.wikipedia.org/wiki/Selection_algorithm",
      "text": "{{for|simulated natural selection in genetic algorithms|Selection (genetic algorithm)}}\n{{more footnotes|date=July 2017}}\nIn [[computer science]], a '''selection algorithm''' is an [[algorithm]] for finding the ''k''th smallest number in a [[List (abstract data type)|list]] or [[Array data structure|array]]; such a number is called the ''k''th ''[[order statistic]]''. This includes the cases of finding the [[minimum]], [[maximum]], and [[median]] elements. There are O(''n'')-time (worst-case linear time) selection algorithms, and sublinear performance is possible for structured data; in the extreme, O(1) for an array of sorted data. Selection is a subproblem of more complex problems like the [[nearest neighbor problem|nearest neighbor]] and [[shortest path]] problems. Many selection algorithms are derived by generalizing a [[sorting algorithm]], and conversely some sorting algorithms can be derived as repeated application of selection.\n\nThe simplest case of a selection algorithm is finding the minimum (or maximum) element by iterating through the list, keeping track of the running minimum – the minimum so far – (or maximum) and can be seen as related to the [[selection sort]]. Conversely, the hardest case of a selection algorithm is finding the median. In fact, a specialized median-selection algorithm can be used to build a general selection algorithm, as in [[median of medians]]. The best-known selection algorithm is [[quickselect]], which is related to [[quicksort]]; like quicksort, it has (asymptotically) optimal average performance, but poor worst-case performance, though it can be modified to give optimal worst-case performance as well.\n\n== Selection by sorting ==\nBy sorting the list or array then selecting the desired element, selection can be [[Reduction (complexity)|reduced]] to [[sorting algorithm|sorting]]. This method is inefficient for selecting a single element, but is efficient when many selections need to be made from an array, in which case only one initial, expensive sort is needed, followed by many cheap selection operations – O(1) for an array, though selection is O(''n'') in a linked list, even if sorted, due to lack of [[random access]]. In general, sorting requires O(''n'' log ''n'') time, where ''n'' is the length of the list, although a lower bound is possible with non-comparative sorting algorithms like [[radix sort]] and [[counting sort]].\n\nRather than sorting the whole list or array, one can instead use [[partial sorting]] to select the ''k'' smallest or ''k'' largest elements. The ''k''th smallest (resp., ''k''th largest element) is then the largest (resp., smallest element) of the partially sorted list – this then takes O(1) to access in an array and O(''k'') to access in a list.\n\n=== Unordered partial sorting ===\nIf partial sorting is relaxed so that the ''k'' smallest elements are returned, but not in order, the factor of O(''k'' log ''k'') can be eliminated. An additional maximum selection (taking O(''k'') time) is required, but since <math>k \\leq n</math>, this still yields asymptotic complexity of O(''n''). In fact, partition-based selection algorithms yield both the ''k''th smallest element itself and the ''k'' smallest elements (with other elements not in order). This can be done in O(''n'') time – average complexity of [[quickselect]], and worst-case complexity of refined partition-based selection algorithms.\n\nConversely, given a selection algorithm, one can easily get an unordered partial sort (''k'' smallest elements, not in order) in O(''n'') time by iterating through the list and recording all elements less than the ''k''th element. If this results in fewer than ''k''&nbsp;&minus;&nbsp;1 elements, any remaining elements equal the ''k''th element. Care must be taken, due to the possibility of equality of elements: one must not include all elements less than ''or equal to'' the ''k''th element, as elements greater than the ''k''th element may also be equal to it.\n\nThus unordered partial sorting (lowest ''k'' elements, but not ordered) and selection of the ''k''th element are very similar problems. Not only do they have the same asymptotic complexity, O(''n''), but a solution to either one can be converted into a solution to the other by a straightforward algorithm (finding a max of ''k'' elements, or filtering elements of a list below a cutoff of the value of the ''k''th element).\n\n=== Partial selection sort ===\nA simple example of selection by partial sorting is to use the partial [[selection sort]].\n\nThe obvious linear time algorithm to find the minimum (resp. maximum) – iterating over the list and keeping track of the minimum (resp. maximum) element so far – can be seen as a partial selection sort that selects the 1 smallest element. However, many other partial sorts also reduce to this algorithm for the case ''k''&nbsp;=&nbsp;1, such as a partial heap sort.\n\nMore generally, a partial selection sort yields a simple selection algorithm which takes O(''kn'') time. This is asymptotically inefficient, but can be sufficiently efficient if ''k'' is small, and is easy to implement. Concretely, we simply find the minimum value and move it to the beginning, repeating on the remaining list until we have accumulated ''k'' elements, and then return the ''k''th element. Here is partial selection sort-based algorithm:\n\n  '''function''' select(list[1..n], k)\n      '''for''' i '''from''' 1 '''to''' k\n          minIndex = i\n          minValue = list[i]\n          '''for''' j '''from''' i+1 '''to''' n\n              '''if''' list[j] < minValue\n                  minIndex = j\n                  minValue = list[j]\n                  swap list[i] and list[minIndex]\n      '''return''' list[k]\n\n== Partition-based selection ==\n{{further|Quickselect}}\n\nLinear performance can be achieved by a partition-based selection algorithm, most basically [[quickselect]]. Quickselect is a variant of [[quicksort]] – in both one chooses a pivot and then partitions the data by it, but while Quicksort recurses on both sides of the partition, Quickselect only recurses on one side, namely the side on which the desired ''k''th element is. As with Quicksort, this has optimal average performance, in this case linear, but poor worst-case performance, in this case quadratic. This occurs for instance by taking the first element as the pivot and searching for the maximum element, if the data is already sorted. In practice this can be avoided by choosing a random element as pivot, which yields [[almost certain]] linear performance. Alternatively, a more careful deterministic pivot strategy can be used, such as [[median of medians]]. These are combined in the hybrid [[introselect]] algorithm (analogous to [[introsort]]), which starts with Quickselect but falls back to median of medians if progress is slow, resulting in both fast average performance and optimal worst-case performance of O(''n'').\n\nThe partition-based algorithms are generally done in place, which thus results in partially sorting the data. They can be done out of place, not changing the original data, at the cost of O(''n'') additional space.\n\n=== Median selection as pivot strategy ===\n{{further|Median of medians}}\nA median-selection algorithm can be used to yield a general selection algorithm or sorting algorithm, by applying it as the pivot strategy in Quickselect or Quicksort; if the median-selection algorithm is asymptotically optimal (linear-time), the resulting selection or sorting algorithm is as well. In fact, an exact median is not necessary – an approximate median is sufficient. In the [[median of medians]] selection algorithm, the pivot strategy computes an approximate median and uses this as pivot, recursing on a smaller set to compute this pivot. In practice the overhead of pivot computation is significant, so these algorithms are generally not used, but this technique is of theoretical interest in relating selection and sorting algorithms.\n\nIn detail, given a median-selection algorithm, one can use it as a pivot strategy in Quickselect, obtaining a selection algorithm. If the median-selection algorithm is optimal, meaning O(''n''), then the resulting general selection algorithm is also optimal, again meaning linear. This is because Quickselect is a [[divide and conquer algorithm]], and using the median at each pivot means that at each step the search set decreases by half in size, so the overall complexity is a [[geometric series]] times the complexity of each step, and thus simply a constant times the complexity of a single step, in fact <math>2 = 1/(1-(1/2))</math> times (summing the series).\n\nSimilarly, given a median-selection algorithm or general selection algorithm applied to find the median, one can use it as a pivot strategy in Quicksort, obtaining a sorting algorithm. If the selection algorithm is optimal, meaning O(''n''), then the resulting sorting algorithm is optimal, meaning O(''n'' log ''n''). The median is the best pivot for sorting, as it evenly divides the data, and thus guarantees optimal sorting, assuming the selection algorithm is optimal. A sorting analog to median of medians exists, using the pivot strategy (approximate median) in Quicksort, and similarly yields an optimal Quicksort.\n\n== Incremental sorting by selection ==\nConverse to selection by sorting, one can incrementally sort by repeated selection. Abstractly, selection only yields a single element, the ''k''th element. However, practical selection algorithms frequently involve partial sorting, or can be modified to do so. Selecting by partial sorting naturally does so, sorting the elements up to ''k'', and selecting by partitioning also sorts some elements: the pivots are sorted to the correct positions, with the ''k''th element being the final pivot, and the elements between the pivots have values between the pivot values. The difference between partition-based selection and partition-based sorting, as in quickselect versus quicksort, is that in selection one recurses on only one side of each pivot, sorting only the pivots (an average of log(''n'') pivots are used), rather than recursing on both sides of the pivot.\n\nThis can be used to speed up subsequent selections on the same data; in the extreme, a fully sorted array allows O(1) selection. Further, compared with first doing a full sort, incrementally sorting by repeated selection [[amortized analysis|amortizes]] the sorting cost over multiple selections.\n\nFor partially sorted data (up to ''k''), so long as the partially sorted data and the index ''k'' up to which the data is sorted are recorded, subsequent selections of ''j'' less than or equal to ''k'' can simply select the ''j''th element, as it is already sorted, while selections of ''j'' greater than ''k'' only need to sort the elements above the ''k''th position.\n\nFor partitioned data, if the list of pivots is stored (for example, in a sorted list of the indices), then subsequent selections only need to select in the interval between two pivots (the nearest pivots below and above). The biggest gain is from the top-level pivots, which eliminate costly large partitions: a single pivot near the middle of the data cuts the time for future selections in half. The pivot list will grow over subsequent selections, as the data becomes more sorted, and can even be passed to a partition-based sort as the basis of a full sort.\n\n== Using data structures to select in sublinear time ==\nGiven an unorganized list of data, linear time (Ω(''n'')) is required to find the minimum element, because we have to examine every element (otherwise, we might miss it). If we organize the list, for example by keeping it sorted at all times, then selecting the ''k''th largest element is trivial, but then insertion requires linear time, as do other operations such as combining two lists.\n\nThe strategy to find an order statistic in [[sublinear time]] is to store the data in an organized fashion using suitable data structures that facilitate the selection. Two such data structures are tree-based structures and frequency tables.\n\nWhen only the minimum (or maximum) is needed, a good approach is to use a [[Heap (data structure)|heap]], which is able to find the minimum (or maximum) element in constant time, while all other operations, including insertion, are O(log ''n'') or better. More generally, a [[self-balancing binary search tree]] can easily be augmented to make it possible to both insert an element and find the ''k''th largest element in O(log ''n'') time; this is called an ''[[order statistic tree]].'' We simply store in each node a count of how many descendants it has, and use this to determine which path to follow. The information can be updated efficiently since adding a node only affects the counts of its O(log ''n'') ancestors, and tree rotations only affect the counts of the nodes involved in the rotation.\n\nAnother simple strategy is based on some of the same concepts as the [[hash table]]. When we know the range of values beforehand, we can divide that range into ''h'' subintervals and assign these to ''h'' buckets. When we insert an element, we add it to the bucket corresponding to the interval it falls in. To find the minimum or maximum element, we scan from the beginning or end for the first nonempty bucket and find the minimum or maximum element in that bucket. In general, to find the ''k''th element, we maintain a count of the number of elements in each bucket, then scan the buckets from left to right adding up counts until we find the bucket containing the desired element, then use the expected linear-time algorithm to find the correct element in that bucket.\n\nIf we choose ''h'' of size roughly sqrt(''n''), and the input is close to uniformly distributed, this scheme can perform selections in expected O(sqrt(''n'')) time. Unfortunately, this strategy is also sensitive to clustering of elements in a narrow interval, which may result in buckets with large numbers of elements (clustering can be eliminated through a good hash function, but finding the element with the ''k''th largest hash value isn't very useful). Additionally, like hash tables this structure requires table resizings to maintain efficiency as elements are added and ''n'' becomes much larger than ''h''<sup>2</sup>. A useful case of this is finding an order statistic or extremum in a finite range of data. Using above table with bucket interval 1 and maintaining counts in each bucket is much superior to other methods. Such hash tables are like [[frequency tables]] used to classify the data in [[descriptive statistics]].\n\n== Lower bounds ==\nIn ''[[The Art of Computer Programming]]'', [[Donald E. Knuth]] discussed a number of lower bounds for the number of comparisons required to locate the ''t'' smallest entries of an unorganized list of ''n'' items (using only comparisons). There is a trivial lower bound of ''n'' &minus; 1 for the minimum or maximum entry. To see this, consider a tournament where each game represents one comparison. Since every player except the winner of the tournament must lose a game before we know the winner, we have a lower bound of ''n'' &minus; 1 comparisons.\n\nThe story becomes more complex for other indexes. We define <math>W_{t}(n)</math> as the minimum number of comparisons required to find the ''t'' smallest values. Knuth references a paper published by S. S. Kislitsyn, which shows an upper bound on this value:\n\n:<math>W_{t}(n) \\leq n - t + \\sum_{n+1-t < j \\leq n} \\lceil{\\log_2\\, j}\\rceil \\quad \\text{for}\\, n \\geq t</math>\n\nThis bound is achievable for ''t''=2 but better, more complex bounds are known for larger ''t''.{{citation needed|date=April 2018}}\n\n== Space complexity ==\nThe required space complexity of selection is O(1) additional storage, in addition to storing the array in which selection is being performed. Such space complexity can be achieved while preserving optimal O(n) time complexity.<ref>Lai T.W., Wood D. (1988) Implicit selection. In: Karlsson R., Lingas A. (eds) SWAT 88. SWAT 1988. Lecture Notes in Computer Science, vol 318. Springer, Berlin, Heidelberg</ref>\n{{Expand section|date=January 2019}}\n\n==Online selection algorithm==\n[[online algorithm|Online]] selection may refer narrowly to computing the ''k''th smallest element of a stream, in which case partial sorting algorithms (with ''k'' + O(1)) space for the ''k'' smallest elements so far) can be used, but partition-based algorithms cannot be.\n\nAlternatively, selection itself may be required to be [[online algorithm|online]], that is, an element can only be selected from a sequential input at the instance of observation and each selection, respectively refusal, is irrevocable. The problem is to select, under these constraints, a specific element of the input sequence (as for example the largest or the smallest value)\twith largest probability. This problem can be tackled by the [[Odds algorithm]], which yields the optimal under an independence condition; it is also optimal itself as an algorithm with the number of computations being linear in the length of input.\n\nThe simplest example is the [[secretary problem]] of choosing the maximum with high probability, in which case optimal strategy (on random data) is to track the running maximum of the first ''n''/''e'' elements and reject them, and then select the first element that is higher than this maximum.\n\n== Related problems ==\nOne may generalize the selection problem to apply to ranges within a list, yielding the problem of [[Range Queries|range queries]]. The question of [[Range Queries#Median|range median queries]] (computing the medians of multiple ranges) has been analyzed.\n\n== Language support ==\nVery few languages have built-in support for general selection, although many provide facilities for finding the smallest or largest element of a list. A notable exception is [[C++]], which provides a templated <code>nth_element</code> method with a guarantee of expected linear time, and also partitions the data, requiring that the ''n''th element be sorted into its correct place, elements before the ''n''th element are less than it, and elements after the ''n''th element are greater than it. It is implied but not required that it is based on Hoare's algorithm (or some variant) by its requirement of expected linear time and partitioning of data.<ref>Section 25.3.2 of ISO/IEC 14882:2003(E) and 14882:1998(E)</ref><ref>[http://www.sgi.com/tech/stl/nth_element.html nth_element], SGI STL</ref>\n\nFor [[Perl]], the module [https://metacpan.org/module/Sort::Key::Top Sort::Key::Top], available from [[CPAN]], provides a set of functions to select the top n elements from a list using several orderings and custom key extraction procedures. Furthermore, the [https://metacpan.org/module/Statistics::CaseResampling Statistics::CaseResampling] module provides a function to calculate quantiles using quickselect.\n\n[[Python (programming language)|Python]]'s standard library (since 2.4) includes <code>[https://docs.python.org/library/heapq.html heapq].nsmallest()</code> and <code>nlargest()</code>, returning sorted lists, in O(''n'' log ''k'') time.<ref>https://stackoverflow.com/a/23038826</ref>\n\nBecause [[sorting algorithm#Language support|language support for sorting]] is more ubiquitous, the simplistic approach of sorting followed by indexing is preferred in many environments despite its disadvantage in speed. Indeed, for [[Lazy evaluation|lazy languages]], this simplistic approach can even achieve the best complexity possible for the ''k'' smallest/greatest sorted (with maximum/minimum as a special case) if the sort is lazy enough{{Citation needed|date=April 2014}}.\n\n== See also ==\n* [[Ordinal optimization]]\n\n== References ==\n{{reflist}}\n{{refbegin}}\n* {{Cite journal | last1 = Blum | first1 = M. | authorlink1 = Manuel Blum| last2 = Floyd | first2 = R. W. | authorlink2 = Robert Floyd| last3 = Pratt | first3 = V. R. | authorlink3 = Vaughan Pratt| last4 = Rivest | first4 = R. L. | authorlink4 = Ron Rivest| last5 = Tarjan | first5 = R. E. | authorlink5 = Robert Tarjan | title = Time bounds for selection | doi = 10.1016/S0022-0000(73)80033-9 | journal = Journal of Computer and System Sciences | volume = 7 | issue = 4  | pages = 448–461 | date =August 1973 | url = http://people.csail.mit.edu/rivest/pubs/BFPRT73.pdf| ref = harv }}\n* {{Cite journal | last1 = Floyd | first1 = R. W. | authorlink1 = Robert W. Floyd | last2 = Rivest | first2 = R. L. | authorlink2 = Ron Rivest | doi = 10.1145/360680.360691 | title = Expected time bounds for selection | journal = Communications of the ACM | volume = 18 | issue = 3 | pages = 165–172 | date=March 1975 }}\n* {{Cite journal | last1 = Kiwiel | first1 = K. C. | doi = 10.1016/j.tcs.2005.06.032 | title = On Floyd and Rivest's SELECT algorithm | journal = Theoretical Computer Science | volume = 347 | pages = 214–238 | year = 2005 | pmid =  | pmc = }}\n* [[Donald Knuth]]. ''[[The Art of Computer Programming]]'', Volume 3: ''Sorting and Searching'', Third Edition. Addison-Wesley, 1997. {{ISBN|0-201-89685-0}}. Section 5.3.3: Minimum-Comparison Selection, pp.&nbsp;207&ndash;219.\n* [[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. ''[[Introduction to Algorithms]]'', Second Edition. MIT Press and McGraw-Hill, 2001. {{ISBN|0-262-03293-7}}. Chapter 9: Medians and Order Statistics, pp.&nbsp;183&ndash;196. Section 14.1: Dynamic order statistics, pp.&nbsp;302&ndash;308.\n* {{DADS|Select|select}}\n{{refend}}\n\n==External links==\n* \"[http://www.ics.uci.edu/~eppstein/161/960125.html Lecture notes for January 25, 1996: Selection and order statistics]\", ''ICS 161: Design and Analysis of Algorithms,'' David Eppstein\n\n{{DEFAULTSORT:Selection Algorithm}}\n[[Category:Selection algorithms| ]]"
    },
    {
      "title": "BFPRT",
      "url": "https://en.wikipedia.org/wiki/BFPRT",
      "text": "#REDIRECT [[Median of medians]]\n[[Category:Selection algorithms]]\n\n[[it:BFPRT]]\n[[ru:BFPRT-Алгоритм]]"
    },
    {
      "title": "Floyd–Rivest algorithm",
      "url": "https://en.wikipedia.org/wiki/Floyd%E2%80%93Rivest_algorithm",
      "text": "{{Infobox Algorithm\n|name=Floyd–Rivest\n|class=[[Selection algorithm]]\n|image=\n|data=[[Array data structure|Array]]\n|average-time={{math|''n'' + min(''k'', ''n'' − ''k'') + ''O''(''n''<sup>1/2</sup>)}}\n|time=\n|space=\n}}\n\nIn [[computer science]], the '''Floyd-Rivest algorithm''' is a [[selection algorithm]] developed by [[Robert W. Floyd]] and [[Ronald L. Rivest]] that has an optimal expected number of comparisons within [[lower-order terms]]. It is functionally equivalent to [[quickselect]], but runs faster in practice on average.<ref>{{Cite journal | last1 = Floyd  | first1 = Robert W. | authorlink1 = Robert W. Floyd | last2 = Rivest  | first2 = Ronald L. | authorlink2 = Ronald L. Rivest | year = 1975 | title = Algorithm 489: The Algorithm SELECT—for Finding the ith Smallest of n elements | journal = [[Communications of the ACM|Comm. ACM]] | volume = 18 | issue = 3 | pages = 173 | doi = 10.1145/360680.360694 | citeseerx = 10.1.1.309.7108 | url = http://people.csail.mit.edu/rivest/pubs/FR75b.pdf}}</ref> It has an expected running time of {{math|''O''(''n'')}} and an expected number of comparisons of {{math|''n'' + min(''k'', ''n'' − ''k'') + ''O''(''n''<sup>1/2</sup>)}}.\n\nThe algorithm was originally presented in a Stanford University technical report containing two papers, where it was referred to as '''SELECT''' and paired with PICK, or [[median of medians]].<ref>{{cite techreport |title=Two papers on the selection problem: Time Bounds for Selection and Expected Time Bounds for Selection |url=http://i.stanford.edu/pub/cstr/reports/cs/tr/73/349/CS-TR-73-349.pdf |id=CS-TR-73-349 |series=[http://infolab.stanford.edu/TR/ Stanford Computer Science Technical Reports and Technical Notes] |date=April 1973}}</ref> It was subsequently published in [[Communications of the ACM]], Volume 18: Issue 3.\n\n==Algorithm==\nThe Floyd-Rivest algorithm is a [[divide and conquer algorithm]], sharing many similarities with [[quickselect]]. It uses [[Sampling (statistics)|sampling]] to help partition the list into three sets. It then recursively selects the ''k''th smallest element from the appropriate set.\n\nThe general steps are:\n\n# Select a small random sample ''S'' from the list ''L''.\n# From ''S'', recursively select two elements, ''u'' and ''v'', such that ''u'' < ''v''. These two elements will be the '''pivots''' for the partition and are expected to contain the ''k''th smallest element of the entire list between them (in a sorted list).\n# Using ''u'' and ''v'', partition ''S'' into three sets: ''A'', ''B'', and ''C''. ''A'' will contain the elements with values less than ''u'', ''B'' will contain the elements with values between ''u'' and ''v'', and ''C'' will contain the elements with values greater than ''v''.\n# Partition the remaining elements in ''L'' (that is, the elements in ''L'' - ''S'') by comparing them to ''u'' or ''v'' and placing them into the appropriate set. If ''k'' is smaller than half the number of the elements in ''L'' rounded up, then the remaining elements should be compared to ''v'' first and then only to ''u'' if they are smaller than ''v''. Otherwise, the remaining elements should be compared to ''u'' first and only to ''v'' if they are greater than ''u''.\n# Based on the value of ''k'', apply the algorithm recursively to the appropriate set to select the ''k''th smallest element in ''L''.\n\n===Pseudocode version===\nThe following [[pseudocode]] sorts the elements between <code>left</code> and <code>right</code> in ascending order, such that for some value ''k'', where <code>left</code> &le; ''k'' &le; <code>right</code>, the ''k''th element in the list will contain the (''k'' - <code>left</code> + 1)th smallest value:\n\n  // ''left is the left index for the interval''\n  // ''right is the right index for the interval''\n  // ''k is the desired index value, where array[k] is the (k+1)th smallest element when left = 0''\n  '''function''' select(array, left, right, k)\n      '''while''' right > left\n          // ''use select recursively to sample a smaller set of size s''\n          // ''the arbitrary constants 600 and 0.5 are used in the original''\n          // ''version to minimize execution time''\n          '''if''' right - left > 600\n              n := right - left + 1\n              i := k - left + 1\n              z := '''ln'''(n)\n              s := 0.5 * '''exp'''(2 * z/3)\n              sd := 0.5 * '''sqrt'''(z * s * (n - s)/n) * '''sign'''(i - n/2)\n              newLeft = '''max'''(left, k - i * s/n + sd)\n              newRight = '''min'''(right, k + (n - i) * s/n + sd)\n              '''select'''(array, newLeft, newRight, k)\n          // ''partition the elements between left and right around t''\n          t := array[k] \n          i := left\n          j := right\n          '''swap''' array[left] and array[k]\n          if array[right] > t \n              '''swap''' array[right] and array[left]\n          '''while''' i < j\n              '''swap''' array[i] and array[j]\n              i := i + 1\n              j := j - 1\n              '''while''' array[i] < t\n                  i := i + 1\n              '''while''' array[j] > t\n                  j := j - 1\n          '''if''' array[left] = t\n              '''swap''' array[left] and array[j]\n          '''else'''\n              j := j + 1\n              '''swap''' array[j] and array[right]\n          // ''adjust left and right towards the boundaries of the subset''\n          // ''containing the (k - left + 1)th smallest element''\n          '''if''' j &le; k\n              left := j + 1\n          '''if''' k &le; j \n              right := j - 1\n\n==See also==\n* [[Quickselect]]\n* [[Introselect]]\n* [[Median of medians]]\n\n==References==\n{{reflist}}\n{{refbegin}}\n* {{Cite journal | last1 = Floyd | first1 = Robert W. | authorlink1 = Robert W. Floyd | last2 = Rivest | first2 = Ron L. | authorlink2 = Ron Rivest | doi = 10.1145/360680.360691 | title = Expected time bounds for selection | journal = Communications of the ACM | volume = 18 | issue = 3 | pages = 165–172 | date = March 1975 | url = http://people.csail.mit.edu/rivest/pubs/FR75a.pdf}}\n* {{Cite journal | last1 = Kiwiel | first1 = Krzysztof C. | doi = 10.1016/j.tcs.2005.06.032 | title = On Floyd and Rivest's SELECT algorithm | journal = Theoretical Computer Science | volume = 347 | issue = 1–2 | pages = 214–238 | date = 30 November 2005 | url = https://core.ac.uk/download/pdf/82672439.pdf}}\n* {{Cite journal | title = A probabilistic analysis of the Floyd-Rivest expected time selection algorithm | journal = International Journal of Computer Mathematics | volume = 82 | issue = 5 | date = May 2005 | pages = 509–519 |first1 = Alexandros V. | last1 = Gerbessiotis | first2 = Constantinos J. | last2 = Siniolakis | first3 = Aghia | last3 = Paraskevi | citeseerx = 10.1.1.7.8672}}\n{{refend}}\n\n{{DEFAULTSORT:Floyd-Rivest algorithm}}\n[[Category:Selection algorithms]]"
    },
    {
      "title": "Introselect",
      "url": "https://en.wikipedia.org/wiki/Introselect",
      "text": "{{Infobox Algorithm\n|name=Introselect\n|class='''Selection algorithm'''\n|image=\n|data=[[Array data structure|Array]]\n|best-time=O(''n'')\n|time=O(''n logn'')\n|space=\n|optimal=Yes\n}}\n\nIn [[computer science]], '''introselect''' (short for \"introspective selection\") is a [[selection algorithm]] that is a [[hybrid algorithm|hybrid]] of [[quickselect]] and [[median of medians]] which has fast average performance and optimal worst-case performance. Introselect is related to the [[introsort]] [[sorting algorithm]]: these are analogous refinements of the basic quickselect and [[quicksort]] algorithms, in that they both start with the quick algorithm, which has good average performance and low overhead, but fall back to an optimal worst-case algorithm (with higher overhead) if the quick algorithm does not progress rapidly enough. Both algorithms were introduced by [[David Musser]] in {{harv|Musser|1997}}, with the purpose of providing [[generic algorithm]]s for the [[C++ Standard Library]] that have both fast average performance and optimal worst-case performance, thus allowing the performance requirements to be tightened.<ref>\"[http://www.cs.rpi.edu/~musser/gp/algorithms.html Generic Algorithms]\", [[David Musser]]</ref> However, in most C++ Standard Library implementations that use introselect, another \"introselect\" algorithm is used, which combines quickselect and heapselect, and has a worst-case running time of ''O''(''n'' log ''n'')<ref>https://gcc.gnu.org/bugzilla/show_bug.cgi?id=35968</ref>.\n\n== Algorithms ==\nIntrosort achieves practical performance comparable to quicksort while preserving ''O''(''n'' log ''n'') worst-case behavior by creating a hybrid of quicksort and [[heapsort]]. Introsort starts with quicksort, so it achieves performance similar to quicksort if quicksort works, and falls back to heapsort (which has optimal worst-case performance) if quicksort does not progress quickly enough. Similarly, introselect combines quickselect with median of medians to achieve worst-case linear selection with performance similar to quickselect.\n\nIntroselect works by optimistically starting out with quickselect and only switching to a worst-case linear-time selection algorithm (the Blum-Floyd-Pratt-Rivest-Tarjan [[median of medians]] algorithm) if it recurses too many times without making sufficient progress. The switching strategy is the main technical content of the algorithm. Simply limiting the recursion to constant depth is not good enough, since this would make the algorithm switch on all sufficiently large lists. Musser discusses a couple of simple approaches:\n* Keep track of the list of sizes of the subpartitions processed so far. If at any point ''k'' recursive calls have been made without halving the list size, for some small positive ''k'', switch to the worst-case linear algorithm.\n* Sum the size of all partitions generated so far. If this exceeds the list size times some small positive constant ''k'', switch to the worst-case linear algorithm. This sum is easy to track in a single scalar variable.\nBoth approaches limit the recursion depth to ''k'' ⌈log ''n''⌉ = ''O''(log ''n'') and the total running time to ''O''(''n)''.\n\nThe paper suggested that more research on introselect was forthcoming, but the author retired in 2007 without having published any such further research.\n\n==See also==\n* [[Quickselect]]\n* [[Floyd–Rivest algorithm]]\n* [[Median of medians]]\n\n==References==\n{{reflist}}\n{{refbegin}}\n* {{Cite journal | last = Musser | first = David R. | authorlink = David Musser | title = Introspective Sorting and Selection Algorithms | url = http://www.cs.rpi.edu/~musser/gp/introsort.ps| doi = 10.1002/(SICI)1097-024X(199708)27:8<983::AID-SPE117>3.0.CO;2-# | journal = Software: Practice and Experience | volume = 27 | issue = 8 | pages = 983–993 | year = 1997 | ref = harv }}\n{{refend}}\n\n[[Category:Selection algorithms]]"
    },
    {
      "title": "Median of medians",
      "url": "https://en.wikipedia.org/wiki/Median_of_medians",
      "text": "{{Infobox Algorithm\n|name=Median of Medians\n|class=[[Selection algorithm]]\n|image=\n|data=[[Array data structure|Array]]\n|best-time= <math>O(n)</math>\n|time=<math>O(n)</math>\n|space=<math>O(\\log n)</math> auxiliary\n|optimal=Yes\n}}\n\nIn [[computer science]], the '''[[median]] of medians''' is an approximate (median) [[selection algorithm]], frequently used to supply a good pivot for an exact selection algorithm, mainly the [[quickselect]], that selects the ''k''th largest element of an initially unsorted array. Median of medians finds an approximate median in linear time only, which is limited but an additional overhead for quickselect. When this approximate median is used as an improved pivot, the worst-case complexity of quickselect reduces significantly from quadratic to ''linear'', which is also the asymptotically optimal worst-case complexity of any selection algorithm. In other words, the median of medians is an approximate median-selection algorithm that helps building an asymptotically optimal, exact general selection algorithm (especially in the sense of worst-case complexity), by producing good pivot elements.\n\nMedian of medians can also be used as a pivot strategy in [[quicksort]], yielding an optimal algorithm, with worst-case complexity O(''n''&nbsp;log&nbsp;''n''). Although this approach optimizes the asymptotic worst-case complexity quite well, it is typically outperformed in practice by instead choosing random pivots for its average O(''n'') complexity for selection and average O(''n''&nbsp;log&nbsp;''n'') complexity for sorting, without any overhead of computing the pivot.\n\nSimilarly, Median of medians is used in the hybrid [[introselect]] algorithm as a fallback for pivot selection at each iteration until kth largest is found. This again ensures a worst-case linear performance, in addition to average-case linear performance: introselect starts with quickselect (with random pivot, default), to obtain good average performance, and then falls back to modified quickselect with pivot obtained from median of medians if the progress is too slow. Even though asymptotically similar, such a hybrid algorithm will have a lower complexity than a straightforward introselect up to a constant factor (both in average-case and worst-case), at any finite length.\n\nThe algorithm was published in {{harvtxt|Blum|Floyd|Pratt|Rivest|1973}}, and thus is sometimes called '''BFPRT''' after the last names of the authors. In the original paper the algorithm was referred to as '''PICK''', referring to quickselect as \"FIND\".\n\n==Outline==\nQuickselect is linear-time on average, but it can require quadratic time with poor pivot choices. This is because quickselect is a [[divide and conquer algorithm|divide and conquer]] algorithm, with each step taking O(''n'') time in the size of the remaining search set. If the search set decreases exponentially quickly in size (by a fixed proportion), this yields a [[geometric series]] times the O(''n'') factor of a single step, and thus linear overall time. However, if the search set decreases slowly in size, such as linearly (by a fixed number of elements, in the worst case only reducing by one element each time), then a linear sum of linear steps yields quadratic overall time (formally, [[triangular number]]s grow quadratically). For example, the worst case occurs when pivoting on the smallest element at each step, such as applying quickselect for the maximum element to already sorted data and taking the first element as pivot each time.\n\nIf one instead consistently chooses \"good\" pivots, this is avoided and one always gets linear performance even in the worst case. A \"good\" pivot is one for which we can establish that a constant proportion of elements fall both below and above it, as then the search set decreases at least by a constant proportion at each step, hence exponentially quickly, and the overall time remains linear. The median is a good pivot – the best for sorting, and the best overall choice for selection – decreasing the search set by half at each step. Thus if one can compute the median in linear time, this only adds linear time to each step, and thus the overall complexity of the algorithm remains linear.\n\nThe median-of-medians algorithm computes an approximate median, namely a point that is guaranteed to be between the 30th and 70th [[percentile]]s (in the middle 4 [[decile]]s). Thus the search set decreases by at least 30%. The problem is reduced to 70% of the original size, which is a fixed proportion smaller. Applying the same algorithm on the now smaller set recursively until only one or two elements remain results in a cost of <math>\\frac{n}{1-0.7}\\approx3.33 {n}</math>\n\n==Algorithm==\n\nAs stated before, median-of-medians is used as a pivot selection strategy in the [[quickselect]] algorithm, which in [[pseudocode]] looks as follows. Be careful to handle <code>left</code>, <code>right</code> and <code>n</code> when implementing. It's better to use the same index for <code>left</code>, <code>right</code> and <code>n</code> to avoid handle index converting.\n\n '''function''' select(list, left, right, n)\n     '''loop'''\n         '''if''' left = right\n              '''return''' left\n         pivotIndex := pivot(list, left, right)\n         pivotIndex := partition(list, left, right, pivotIndex, n)\n         '''if''' n = pivotIndex\n             '''return''' n\n         '''else if''' n < pivotIndex\n             right := pivotIndex - 1\n         '''else'''\n             left := pivotIndex + 1\n\nThere is a subroutine called {{mono|partition}} that can, in linear time, group a list (ranging from indices <code>left</code> to <code>right</code>) into three parts, those less than a certain element, those equal to it, and those greater than the element ([[Dutch national flag problem|a three-way partition]]). The grouping into three parts ensures that the median-of-medians maintains linear execution time in a case of many or all coincident elements. Here is pseudocode that performs a partition about the element <code>list[pivotIndex]</code>:\n\n  '''function''' partition(list, left, right, pivotIndex, n)\n      pivotValue := list[pivotIndex]\n      swap list[pivotIndex] and list[right]  ''// Move pivot to end''\n      storeIndex := left\n      ''//Move all elements smaller than the pivot to the left of the pivot''\n      '''for''' i '''from''' left '''to''' right-1\n          '''if''' list[i] < pivotValue\n              swap list[storeIndex] and list[i]\n              increment storeIndex\n      ''//Move all elements equal to the pivot right after''\n      ''//the smaller elements''\n      storeIndexEq = storeIndex\n      '''for''' i '''from''' storeIndex '''to''' right-1\n          '''if''' list[i] == pivotValue\n              swap list[storeIndexEq] and list[i]\n              increment storeIndexEq\n      swap list[right] and list[storeIndexEq]  ''// Move pivot to its final place''\n      ''//Return location of pivot considering the desired location n''\n      '''if''' n < storeIndex\n          '''return''' storeIndex  ''// n is in the group of smaller elements''\n      '''if''' n <= storeIndexEq\n          '''return''' n  ''// n is in the group equal to pivot''\n      '''return''' storeIndexEq ''// n is in the group of larger elements''\n\nSubroutine {{mono|pivot}} is the actual median-of-medians algorithm. It divides its input (a list of length {{mvar|n}}) into groups of at most five elements, computes the median of each of those groups using some subroutine, then [[Recursion (computer science)|recursively]] computes the true median of the {{math|{{sfrac|''n''|5}}}} medians found in the previous step:<ref name=\"clrs\">{{Introduction to Algorithms|3|page=220}}</ref>\n\n   '''function''' pivot(list, left, right)\n      ''// for 5 or less elements just get median''\n      '''if''' right - left < 5:\n          return partition5(list, left, right)\n      ''// otherwise move the medians of five-element subgroups to the first n/5 positions''\n      '''for''' i '''from''' left '''to''' right '''in steps of''' 5\n          ''// get the median position of the i'th five-element subgroup''\n          subRight := i + 4\n          if subRight > right:\n              subRight := right\n          median5 := partition5(list, i, subRight)\n          swap list[median5] and list[left + floor((i - left)/5)]\n \n      ''// compute the median of the n/5 medians-of-five''\n      mid := (right - left) / 10 + left + 1\n      '''return''' select(list, left, left + floor((right - left) / 5), mid )\n\nThe {{mono|partition5}} subroutine selects the median of a group of at most five elements; an easy way to implement this is [[insertion sort]], as shown below.{{r|clrs}} It can also be implemented as a [[decision tree]]. Note that {{mono|pivot}} calls {{mono|select}}; this is an instance of [[mutual recursion]].\n\n  '''function''' partition5( list, left, right)\n      i := left + 1\n      '''while''' i <= right\n          j := i\n          '''while''' j > left and list[j-1] > list[j]\n              swap list[j-1] and list[j]\n              j := j - 1\n          i :=  i + 1\n             \n      '''return''' floor((left + right) / 2)\n\n== Properties of pivot ==\nOf the ''n''/5 groups, half the number of groups(½×''n''/5=''n''/10) have their median less than the pivot(Median of Medians). Also, another half the number of groups(again, ½×''n''/5=''n''/10) have their median greater than the pivot. In each of the ''n''/10 groups with median less than the pivot, there are two elements that are smaller than their respective medians, which are smaller than the pivot. Thus, each of the ''n''/10 groups have at least 3 elements that are smaller than the pivot. Similarly, in each of the ''n''/10 groups with median greater than the pivot, there are two elements that are greater than their respective medians, which are greater than the pivot. Thus, each of the ''n''/10 groups have at least 3 elements that are greater than the pivot. Hence, the pivot is less than 3(''n''/10) elements and greater than another 3(''n''/10) elements. Thus the chosen median splits the elements somewhere between 30%/70% and 70%/30%, which assures worst-case linear behavior of the algorithm. To visualize:\n\n{|class=\"wikitable\" border=1\n|+ One iteration on a randomized set of 100 elements from 0 to 99\n!\n| bgcolor=gray|12||||bgcolor=gray|15||||bgcolor=gray|11||||bgcolor=gray|2||||bgcolor=gray|9||||bgcolor=gray|5||||bgcolor=gray|0||||bgcolor=gray|7||||bgcolor=gray|3||||bgcolor=gray|21||||bgcolor=gray|44||||bgcolor=gray|40||||bgcolor=gray|1||||bgcolor=gray|18||||bgcolor=gray|20||||bgcolor=gray|32||||bgcolor=gray|19||||bgcolor=gray|35||||bgcolor=gray|37||||bgcolor=gray|39\n|-\n!\n| bgcolor=gray|13||||bgcolor=gray|16||||bgcolor=gray|14||||bgcolor=gray|8||||bgcolor=gray|10||||bgcolor=gray|26||||bgcolor=gray|6||||bgcolor=gray|33||||bgcolor=gray|4||||bgcolor=gray|27||||bgcolor=white|49||||bgcolor=gray|46||||bgcolor=white|52||||bgcolor=gray|25||||bgcolor=white|51||||bgcolor=gray|34||||bgcolor=gray|43||||bgcolor=white|56||||bgcolor=white|72||||bgcolor=white|79\n|-\n! Medians\n| bgcolor=gray|17||||bgcolor=gray|23||||bgcolor=gray|24||||bgcolor=gray|28||||bgcolor=gray|29||||bgcolor=gray|30||||bgcolor=gray|31||||bgcolor=gray|36||||bgcolor=gray|42||||bgcolor=red|47||||bgcolor=white|50||||bgcolor=white|55||||bgcolor=white|58||||bgcolor=white|60||||bgcolor=white|63||||bgcolor=white|65||||bgcolor=white|66||||bgcolor=white|67||||bgcolor=white|81||||bgcolor=white|83\n|-\n!\n| bgcolor=gray|22||||bgcolor=gray|45||||bgcolor=gray|38||||bgcolor=white|53||||bgcolor=white|61||||bgcolor=gray|41||||bgcolor=white|62||||bgcolor=white|82||||bgcolor=white|54||||bgcolor=white|48||||bgcolor=white|59||||bgcolor=white|57||||bgcolor=white|71||||bgcolor=white|78||||bgcolor=white|64||||bgcolor=white|80||||bgcolor=white|70||||bgcolor=white|76||||bgcolor=white|85||||bgcolor=white|87\n|-\n!\n| bgcolor=white|96||||bgcolor=white|95||||bgcolor=white|94||||bgcolor=white|86||||bgcolor=white|89||||bgcolor=white|69||||bgcolor=white|68||||bgcolor=white|97||||bgcolor=white|73||||bgcolor=white|92||||bgcolor=white|74||||bgcolor=white|88||||bgcolor=white|99||||bgcolor=white|84||||bgcolor=white|75||||bgcolor=white|90||||bgcolor=white|77||||bgcolor=white|93||||bgcolor=white|98||||bgcolor=white|91\n|-\n|}\n\n(red = \"(one of the two possible) median of medians\", gray = \"number < red\", white = \"number > red\")\n\n5-tuples are shown here sorted by median, for clarity.  Sorting the tuples is not necessary because we only need the median for use as pivot element.\n\nNote that all elements above/left of the red (30% of the 100 elements) are less, and all elements below/right of the red (another 30% of the 100 elements) are greater.\n\n== Proof of O(''n'') running time ==\n\nThe median-calculating recursive call does not exceed worst-case linear behavior because the list of medians has size ''n/5'', while the other recursive call recurses on at most 70% of the list. Let ''T(n)'' be the time it takes to run a median-of-medians Quickselect algorithm on an array of size ''n''. Then we know this time is:\n:<math>T(n) \\leq T(n/5) + T(n \\cdot 7/10) + c \\cdot n.</math>\n\nwhere\n* the ''T(n/5)'' part is for finding the ''true'' median of the ''n/5'' medians, by running an (independent) Quickselect on them (since finding the median is just a special case of selecting a ''k''-largest element)\n* the O(''n'') term ''c·n'' is for the partitioning work to create the two sides, one of which our Quickselect will recurse (we visited each element a constant number of times, in order to form them into n/5 groups and take each median in O(1) time).\n* the ''T(n·7/10)'' part is for the actual Quickselect recursion (for the worst case, in which the ''k''-th element is in the bigger partition that can be of size n·7/10 maximally)\n\nFrom this, using induction one can easily show that\n:<math>T(n) \\leq 10 \\cdot c \\cdot n \\in O(n).</math>\n\n== Analysis ==\nThe key step is reducing the problem to selecting in two lists whose total length is shorter than the original list, plus a linear factor for the reduction step. This allows a simple induction to show that the overall running time is linear.\n\nThe specific choice of groups of five elements is explained as follows. Firstly, computing median of an odd list is faster and simpler; while one could use an even list, this requires taking the average of the two middle elements, which is slower than simply selecting the single exact middle element. Secondly, five is the smallest odd number such that median of medians works. With groups of only three elements, the resulting list of medians to search in is length ''n''/3, and reduces the list to recurse into length <math>\\frac{2}{3}n</math>, since it is greater than 1/2 × 2/3 = 1/3 of the elements and less than 1/2 × 2/3 = 1/3 of the elements. Thus this still leaves ''n'' elements to search in, not reducing the problem sufficiently. The individual lists are shorter, however, and one can bound the resulting complexity to <math>O(n \\log n)</math> by the [[Akra–Bazzi method]], but it does not prove linearity.\n\nConversely, one may instead group by ''g'' = seven, nine, or more elements, and this does work. This reduces the size of the list of medians to ''n''/''g,'' and the size of the list to recurse into asymptotes at 3''n''/4 (75%), as the quadrants in the above table approximate 25%, as the size of the overlapping lines decreases proportionally. This reduces the scaling factor from 10 asymptotically to 4, but accordingly raises the ''c'' term for the partitioning work. Finding the median of a larger group takes longer, but is a constant factor (depending only on ''g''), and thus does not change the overall performance as ''n'' grows. In fact, considering the number of comparisons in the worst case, the constant factor is <math>\\frac{2g(g-1)}{g-3}</math>.\n\nIf one instead groups the other way, say dividing the ''n'' element list into 5 lists, computing the median of each, and then computing the median of these – i.e., grouping by a constant fraction, not a constant number – one does not as clearly reduce the problem, since it requires computing 5 medians, each in a list of ''n''/5 elements, and then recursing on a list of length at most 7''n''/10. As with grouping by 3, the individual lists are shorter, but the overall length is no shorter – in fact longer – and thus one can only prove superlinear bounds. Grouping into a square of <math>\\sqrt{n}</math> lists of length <math>\\sqrt{n}</math> is similarly complicated.\n\n== References ==\n{{reflist}}\n{{refbegin}}\n* {{Cite journal | last1 = Blum | first1 = M. | authorlink1 = Manuel Blum| last2 = Floyd | first2 = R. W. | authorlink2 = Robert Floyd| last3 = Pratt | first3 = V. R. | authorlink3 = Vaughan Pratt| last4 = Rivest | first4 = R. L. | authorlink4 = Ron Rivest| last5 = Tarjan | first5 = R. E. | authorlink5 = Robert Tarjan | title = Time bounds for selection | doi = 10.1016/S0022-0000(73)80033-9 | journal = Journal of Computer and System Sciences | volume = 7 | issue = 4  | pages = 448–461 | date =August 1973 | url = http://people.csail.mit.edu/rivest/pubs/BFPRT73.pdf| ref = harv }}\n{{refend}}\n\n== External links ==\n* \"[http://www.ics.uci.edu/~eppstein/161/960130.html Lecture notes for January 30, 1996: Deterministic selection]\", ''ICS 161: Design and Analysis of Algorithms,'' David Eppstein\n\n[[Category:Selection algorithms]]"
    },
    {
      "title": "Order statistic tree",
      "url": "https://en.wikipedia.org/wiki/Order_statistic_tree",
      "text": "In [[computer science]], an '''order statistic tree''' is a variant of the [[binary search tree]] (or more generally, a [[B-tree]]<ref>{{cite web |url=http://www.chiark.greenend.org.uk/~sgtatham/algorithms/cbtree.html |title=Counted B-Trees |date=11 December 2004 |accessdate=18 January 2014}}</ref>) that supports two additional operations beyond insertion, lookup and deletion:\n\n* [[Selection algorithm|Select(''i'')]] — find the ''i'''th smallest element stored in the tree\n* Rank(''x'') – find the rank of element ''x'' in the tree, i.e. its index in the sorted list of elements of the tree\n\nBoth operations can be performed in {{math|''O''(log ''n'')}} [[Best, worst and average case|worst case]] time when a [[self-balancing binary search tree|self-balancing tree]] is used as the base data structure.\n\n==Augmented search tree implementation==\nTo turn a regular search tree into an order statistic tree, the nodes of the tree need to store one additional value, which is the size of the subtree rooted at that node (i.e., the number of nodes below it). All operations that modify the tree must adjust this information to preserve the [[Loop invariant|invariant]] that\n\n size[x] = size[left[x]] + size[right[x]] + 1\n\nwhere <code>size[nil] = 0</code> by definition. Select can then be implemented as<ref>{{Introduction to Algorithms|2}}</ref>{{rp|342}}\n\n '''function''' Select(t, i)\n     // Returns the i'th element (zero-indexed) of the elements in t\n     l ← size[left[t]]\n     '''if''' i = l\n         return key[t]\n     '''else if''' i < l\n         return Select(left[t], i)\n     '''else'''\n         return Select(right[t], i - (l + 1))\n\nRank can be implemented as<ref>{{Introduction to Algorithms|3}}</ref>{{rp|342}}\n\n '''function''' Rank(T, x)\n     // Returns the position of x (one-indexed) in the linear sorted list of elements of the tree T\n     r ← size[left[x]] + 1\n     y ← x\n     '''while''' y ≠ T.root\n          '''if''' y = right[y.p]\n               r ← r + size[left[y.p]] + 1\n          y ← y.p\n     '''return''' r\n\nOrder-statistic trees can be further amended with bookkeeping information to maintain balance (e.g., tree height can be added to get an order statistic [[AVL tree]], or a color bit to get a [[red-black tree|red-black]] order statistic tree). Alternatively, the size field can be used in conjunction with a [[Weight-balanced tree|weight-balancing]] scheme at no additional storage cost.<ref>{{Cite conference| doi = 10.1007/3-540-48224-5_39| title = A new method for balancing binary search trees| conference = [[ICALP]]| volume = 2076| pages = 469–480| series = Lecture Notes in Computer Science| year = 2001| last1 = Roura | first1 = Salvador| isbn = 978-3-540-42287-7}}</ref>\n\n==References==\n{{reflist|30em}}\n\n==External links==\n* [http://www.cs.yale.edu/homes/aspnes/pinewiki/OrderStatisticsTree.html Order statistic tree] on PineWiki, Yale University.\n* The [[Python (programming language)|Python]] package [http://stutzbachenterprises.com/blist/ blist] uses order statistic B-trees to implement [[List (abstract data type)|lists]] with fast insertion at arbitrary positions.\n\n{{CS-Trees}}\n\n[[Category:Search trees]]\n[[Category:Selection algorithms]]"
    },
    {
      "title": "Quickselect",
      "url": "https://en.wikipedia.org/wiki/Quickselect",
      "text": "{{refimprove|date=August 2013}}\n\n{{Infobox Algorithm\n|name=Quickselect\n|class=[[Selection algorithm]]\n|image=[[File:Selecting quickselect frames.gif|Animated visualization of the quickselect algorithm. Selecting the 22st smallest value.]]\n|caption=Animated visualization of the quickselect algorithm. Selecting the 22nd smallest value.\n|data=[[Array data structure|Array]]\n|best-time=О(''n'')\n|average-time=O(''n'')\n|time=О(''n''<sup>2</sup>)\n|space=\n|optimal=Yes\n}}\n\nIn [[computer science]], '''quickselect''' is a [[selection algorithm]] to find the ''k''th smallest element in an unordered list. It is related to the [[quicksort]] sorting algorithm. Like quicksort, it was developed by [[Tony Hoare]], and thus is also known as '''Hoare's selection algorithm'''.<ref>{{Cite journal | last1 = Hoare | title = Algorithm 65: Find | doi = 10.1145/366622.366647 | first1 = C. A. R. | authorlink1 = Tony Hoare | journal = [[Communications of the ACM|Comm. ACM]] | volume = 4 | issue = 7 | pages = 321–322 | year = 1961 | pmid =  | pmc = }}</ref> Like quicksort, it is efficient in practice and has good average-case performance, but has poor worst-case performance. Quickselect and its variants are the selection algorithms most often used in efficient real-world implementations. \n\nQuickselect uses the same overall approach as quicksort, choosing one element as a pivot and partitioning the data in two based on the pivot, accordingly as less than or greater than the pivot. However, instead of recursing into both sides, as in quicksort, quickselect only recurses into one side – the side with the element it is searching for. This reduces the average complexity from {{math|O(''n'' log ''n'')}} to {{math|O(''n'')}}, with a worst case of  {{math|O(''n''<sup>2</sup>)}}.\n\nAs with quicksort, quickselect is generally implemented as an [[in-place algorithm]], and beyond selecting the {{mvar|k}}'th element, it also partially sorts the data. See [[selection algorithm]] for further discussion of the connection with sorting.\n\n==Algorithm==\nIn quicksort, there is a subprocedure called partition that can, in linear time, group a list (ranging from indices <code>left</code> to <code>right</code>) into two parts: those less than a certain element, and those greater than or equal to the element. Here is pseudocode that performs a partition about the element <code>list[pivotIndex]</code>:\n\n  '''function''' partition(list, left, right, pivotIndex)\n      pivotValue := list[pivotIndex]\n      swap list[pivotIndex] and list[right]  ''// Move pivot to end''\n      storeIndex := left\n      '''for''' i '''from''' left '''to''' right-1\n          '''if''' list[i] < pivotValue\n              swap list[storeIndex] and list[i]\n              increment storeIndex\n      swap list[right] and list[storeIndex]  ''// Move pivot to its final place''\n      '''return''' storeIndex\n\nThis is known as the [[Quicksort#Lomuto partition scheme|Lomuto partition scheme]], which is simpler but less efficient than [[Quicksort#Hoare partition scheme|Hoare's original partition scheme]].\n\nIn quicksort, we recursively sort both branches, leading to best-case O(''n'' log ''n'') time. However, when doing selection, we already know which partition our desired element lies in, since the pivot is in its final sorted position, with all those preceding it in an unsorted order and all those following it in an unsorted order. Therefore, a single recursive call locates the desired element in the correct partition, and we build upon this for quickselect:\n\n   ''// Returns the k-th smallest element of list within left..right inclusive''\n   ''// (i.e. left <= k <= right).''\n   ''// The search space within the array is changing for each round - but the list''\n   ''// is still the same size. Thus, k does not need to be updated with each round.''\n   '''function''' select(list, left, right, k)\n      '''if''' left = right        ''// If the list contains only one element,''\n          '''return''' list[left]  ''// return that element''\n      pivotIndex  := ...     ''// select a pivotIndex between left and right,''\n                             ''// e.g.,'' left + floor(rand() % (right - left + 1))\n      pivotIndex  := partition(list, left, right, pivotIndex)\n      ''// The pivot is in its final sorted position''\n      '''if''' k = pivotIndex\n          '''return''' list[k]\n      '''else if''' k < pivotIndex\n          '''return''' select(list, left, pivotIndex - 1, k)\n      '''else'''\n          '''return''' select(list, pivotIndex + 1, right, k)\n\n----Note the resemblance to quicksort: just as the minimum-based selection algorithm is a partial selection sort, this is a partial quicksort, generating and partitioning only O(log ''n'') of its O(''n'') partitions. This simple procedure has expected linear performance, and, like quicksort, has quite good performance in practice. It is also an [[in-place algorithm]], requiring only constant memory overhead if [[tail call]] optimization is available, or if eliminating the [[tail recursion]] with a loop:\n\n  '''function''' select(list, left, right, k)\n      '''loop'''\n          '''if''' left = right\n              '''return''' list[left]\n          pivotIndex := ...     ''// select pivotIndex between left and right''\n          pivotIndex := partition(list, left, right, pivotIndex)\n          '''if''' k = pivotIndex\n              '''return''' list[k]\n          '''else if''' k < pivotIndex\n              right := pivotIndex - 1\n          '''else'''\n              left := pivotIndex + 1\n\n==Time complexity==\nLike quicksort, the quickselect has good average performance, but is sensitive to the pivot that is chosen. If good pivots are chosen, meaning ones that consistently decrease the search set by a given fraction, then the search set decreases in size exponentially and by induction (or summing the [[geometric series]]) one sees that performance is linear, as each step is linear and the overall time is a constant times this (depending on how quickly the search set reduces). However, if bad pivots are consistently chosen, such as decreasing by only a single element each time, then worst-case performance is quadratic: O(''n''<sup>2</sup>). This occurs for example in searching for the maximum element of a set, using the first element as the pivot, and having sorted data.\n\n==Variants==\nThe easiest solution is to choose a random pivot, which yields [[almost certain]] linear time. Deterministically, one can use median-of-3 pivot strategy (as in the quicksort), which yields linear performance on partially sorted data, as is common in the real world. However, contrived sequences can still cause worst-case complexity; [[David Musser]] describes a \"median-of-3 killer\" sequence that allows an attack against that strategy, which was one motivation for his [[introselect]] algorithm.\n\nOne can assure linear performance even in the worst case by using a more sophisticated pivot strategy; this is done in the [[median of medians]] algorithm. However, the overhead of computing the pivot is high, and thus this is generally not used in practice. One can combine basic quickselect with median of medians as fallback to get both fast average case performance and linear worst-case performance; this is done in [[introselect]].\n\nFiner computations of the average time complexity yield a worst case of <math>n(2+2\\log 2+o(1)) \\leq 3.4n + o(n)</math> for random pivots (in the case of the median; other ''k'' are faster).<ref>[https://11011110.github.io/blog/2007/10/09/blum-style-analysis-of.html Blum-style analysis of Quickselect], [[David Eppstein]], October 9, 2007.</ref> The constant can be improved to 3/2 by a more complicated pivot strategy, yielding the [[Floyd–Rivest algorithm]], which has average complexity of <math>1.5 n + O(n^{1/2})</math> for median, with other ''k'' being faster.\n\n==See also==\n* [[Floyd–Rivest algorithm]]\n* [[Introselect]]\n* [[Median of medians]]\n\n==References==\n{{reflist}}\n\n[[Category:Selection algorithms]]"
    },
    {
      "title": "American flag sort",
      "url": "https://en.wikipedia.org/wiki/American_flag_sort",
      "text": "{{Refimprove|date=July 2017}}\nAn '''American flag sort''' is an efficient, [[in-place]] variant of [[radix sort]] that distributes items into [[Bucket (computing)|buckets]].  Non-comparative sorting algorithms such as radix sort and American flag sort are typically used to sort large objects such as strings, for which comparison is not a unit-time operation.<ref name=\"mcilroy\">\n{{cite journal\n| first1 = Peter M.\n| last1 = McIlroy\n| first2 = Keith\n| last2 = Bostic\n| first3 = M. Douglas\n| last3 = McIlroy\n| title = Engineering radix sort\n| journal = Computing Systems\n| volume = 6\n| issue = 1\n| pages = 5-27\n| year = 1993\n| url = http://static.usenix.org/publications/compsystems/1993/win_mcilroy.pdf\n}}\n</ref>\nAmerican flag sort iterates through the bits of the objects, considering several bits of each object at a time.  For each set of bits, American flag sort makes two passes through the array of objects: first to count the number of objects that will fall in each bin, and second to place each object in its bucket.  This works especially well when sorting a byte at a time, using 256 buckets. With some optimizations, it is twice as fast as [[quicksort]] for large sets of [[string (computer science)|strings]].<ref name=\"mcilroy\" />\n\nThe name [[Flag of the United States|American flag]] sort comes by [[analogy]] with the [[Dutch national flag problem]] in the last step: efficiently [[partition of a set|partition]] the array into many \"stripes\".\n\n== Algorithm ==\nSorting algorithms in general sort a list of objects according to some ordering scheme. In contrast to [[Comparison sort|comparison-based sorting algorithms]], such as [[quicksort]], American flag sort can only sort integers (or objects that can be interpreted as integers). In-place sorting algorithms, including American flag sort, run without allocating a significant amount of memory beyond that used by the original array. This is a significant advantage, both in memory savings and in time saved copying the array.\n\nAmerican flag sort works by successively dividing a list of objects into buckets based on the first digit of their base-N representation (the base used is referred to as the ''radix'').  When N is 2, each object can be swapped into the correct bucket by using the [[Dutch national flag problem|Dutch national flag algorithm]]. When N is larger, however, objects cannot be immediately swapped into place, because it is unknown where each bucket should begin and end. American flag sort gets around this problem by making two passes through the array.  The first pass counts the number of objects that belong in each of the N buckets. The beginning and end of each bucket in the original array is then computed as the sum of sizes of preceding buckets. The second pass swaps each object into place.\n\nAmerican flag sort is most efficient with a radix that is a power of 2, because bit-shifting operations can be used instead of expensive [[exponentiation]]s to compute the value of each digit. When sorting strings using 8- or 7-bit encodings such as [[ASCII]], it is typical to use a radix of 256 or 128, which amounts to sorting character-by-character.<ref name=\"mcilroy\" />\n\n=== Performance considerations ===\nIt is worth noting that for pure English alphabet text, the counts histogram is always sparse. Depending on the hardware, it may be worth clearing the counts in correspondence with completing a bucket (as in the original paper.) Or it may be worth maintaining a max and min active bucket, or a more complex data structure suitable for sparse arrays. It is also important to use a more basic sorting method for very small data sets, except in pathological cases where keys may share very long prefixes.\n\nMost critically, this algorithm follows a random permutation, and is thus particularly cache-unfriendly for large datasets. It is a suitable algorithm in conjunction with a [[K-Way Merge Algorithms|k-way merge]].<ref>{{cite web|url=http://stackoverflow.com/questions/463105/in-place-radix-sort/475108#475108 |title=algorithm - In-Place Radix Sort |publisher=Stack Overflow |date= |accessdate=2017-07-19}}</ref> (The original paper was written before cached memory was in common use.)\n\n=== Pseudocode ===\n<pre>\namerican_flag_sort(Array, Radix)\n  for each digit D:\n    # first pass: compute counts\n    Counts <- zeros(Radix)\n    for object X in Array:\n      Counts[digit D of object X in base Radix] += 1\n    # compute bucket offsets\n    Offsets <- [ sum(Counts[0..i]) for i in 1..Radix]\n    # swap objects into place\n    for object X in Array:\n      swap X to the bucket starting at Offsets[digit D of X in base Radix]\n    for each Bucket:\n      american_flag_sort(Bucket, Radix)\n</pre>\n\n=== Sample implementation in Python===\nThis example written in the Python programming language will perform American flag sort for any radix of 2 or greater. Simplicity of exposition is chosen over clever programming, and so the log function is used instead of bit shifting techniques.\n<source lang=python>\ndef get_radix_val(x, digit, radix):\n    return int(floor(x / radix**digit)) % radix\n\ndef compute_offsets(a_list, start, end, digit, radix):\n    counts = [0 for _ in range(radix)]\n    for i in range(start, end):\n        val = get_radix_val(a_list[i], digit, radix)\n        counts[val] += 1\n    offsets = [0 for _ in range(radix)]\n    sum = 0\n    for i in range(radix):\n        offsets[i] = sum\n        sum += counts[i]\n    return offsets\n\ndef swap(a_list, offsets, start, end, digit, radix):\n    i = start\n    next_free = copy(offsets)\n    cur_block = 0\n    while cur_block < radix-1:\n        if i >= start + offsets[cur_block+1]:\n            cur_block += 1\n            continue\n        radix_val = get_radix_val(a_list[i], digit, radix)\n        if radix_val == cur_block:\n            i += 1\n            continue\n        swap_to = next_free[radix_val]\n        a_list[i], a_list[swap_to] = a_list[swap_to], a_list[i]\n        next_free[radix_val] += 1\n\ndef american_flag_sort_helper(a_list, start, end, digit, radix):\n    offsets = compute_offsets(a_list, start, end, digit, radix)\n    swap(a_list, offsets, start, end, digit, radix)\n    if digit == 0:\n        return\n    for i in range(len(offsets)-1):\n        american_flag_sort_helper(a_list, offsets[i], offsets[i+1], digit-1, radix)\n\ndef american_flag_sort(a_list, radix):\n    for x in a_list:\n        assert(type(x) == int)\n    max_val = max(a_list)\n    max_digit = int(floor(log(max_val, radix)))\n    american_flag_sort_helper(a_list, 0, len(a_list), max_digit, radix)\n</source>\n\n==See also==\n*[[Bucket sort]]\n*[[Multi-key quicksort]]\n*[[Radixsort]]\n*[[Dutch national flag problem]]\n\n== References ==\n{{reflist}}\n\n===General===\n* {{DADS|American flag sort|americanFlagSort}}\n\n== External links ==\n\n{{sorting}}\n\n[[Category:String sorting algorithms]]\n[[Category:Articles with example pseudocode]]\n[[Category:Articles with example Python code]]"
    },
    {
      "title": "Burstsort",
      "url": "https://en.wikipedia.org/wiki/Burstsort",
      "text": "{{more footnotes|date=July 2017}}\n{{Infobox algorithm\n|class=[[Sorting algorithm]]\n|image=\n|data=[[Trie]]\n|time={{math|''O''(''wn'')}}\n|space={{math|''O''(''wn'')}}\n|optimal=?\n}}\n\n'''Burstsort''' and its variants are cache-efficient algorithms for sorting [[String (computer science)|strings]] and are faster than [[radix sort]] for large [[data set]]s of common strings, first published in 2003.<ref name=\"ref1\">{{Cite journal | last1 = Sinha | first1 = R. | last2 = Zobel | first2 = J. | doi = 10.1145/1005813.1041517 | title = Cache-conscious sorting of large sets of strings with dynamic tries | journal = Journal of Experimental Algorithmics| volume = 9 | url = https://people.eng.unimelb.edu.au/jzobel/fulltext/acmjea04.pdf| pages = 1.5 | year = 2005 | pmid =  | pmc = | citeseerx = 10.1.1.599.861 }}</ref>\n\nBurstsort algorithms use a [[trie]] to store prefixes of strings, with [[Dynamic array|growable arrays]] of pointers as end nodes containing sorted, unique, suffixes (referred to as ''buckets''). Some variants copy the string tails into the buckets. As the buckets grow beyond a predetermined threshold, the buckets are \"burst\" into tries, giving the sort its name. A more recent variant uses a bucket index with smaller sub-buckets to reduce memory usage. Most implementations delegate to multikey quicksort, an extension of three-way radix quicksort, to sort the contents of the buckets. By dividing the input into buckets with common prefixes, the sorting can be done in a cache-efficient manner.\n\nBurstsort was introduced as a sort that is similar to [[Radix sort#Most significant digit radix sorts|MSD radix sort]],<ref name=\"ref1\" /> but is faster due to being aware of caching and related radixes being stored closer to each other due to specifics of trie structure. It exploits specifics of strings that are usually encountered in real world. And although asymptotically it is the same as radix sort, with time complexity of {{math|''O''(''wn'')}} (''w'' – word length and ''n'' – number of strings to be sorted), but due to better memory distribution it tends to be twice as fast on big data sets of strings.\n\n== References ==\n\n{{Reflist}}\n* A burstsort derivative (C-burstsort), faster than burstsort: {{cite journal |url=http://www.cs.mu.oz.au/~rsinha/papers/SinhaRingZobel-2006.pdf |title=Cache-Efficient String Sorting Using Copying |first1=Ranjan |last1=Sinha |first2=Justin |last2=Zobel |first3=David |last3=Ring |journal=Journal of Experimental Algorithmics |volume=11 |date=January 2006 |issue=1.2 |pages=1.2 |doi=10.1145/1187436.1187439 |citeseerx=10.1.1.85.3498 |access-date=2007-05-31 |archive-url=https://web.archive.org/web/20071001002932/http://www.cs.mu.oz.au/~rsinha/papers/SinhaRingZobel-2006.pdf |archive-date=2007-10-01 |dead-url=yes }}\n* The data type used in burstsort: {{cite journal |url=http://goanna.cs.rmit.edu.au/~jz/fulltext/acmtois02.pdf |title=Burst Tries: A Fast, Efficient Data Structure for String Keys |first1=Steffen |last1=Heinz |first2=Justin |last2=Zobel |first3=Hugh E. |last3=Williams |journal=ACM Transactions on Information Systems |volume=20 |issue=2 |pages=192&ndash;223 |date=April 2002 |citeseerx=10.1.1.18.3499 |doi=10.1145/506309.506312 |access-date=2007-09-25 |archive-url=https://web.archive.org/web/20131205060509/http://goanna.cs.rmit.edu.au/~jz/fulltext/acmtois02.pdf |archive-date=2013-12-05 |dead-url=yes }}\n* {{cite book |chapter-url=http://goanna.cs.rmit.edu.au/~jz/fulltext/acsc03sz.pdf |chapter=Efficient Trie-Based Sorting of Large Sets of Strings |first1=Ranjan |last1=Sinha |first2=Justin |last2=Zobel |title=Proceedings of the 26th Australasian Computer Science Conference |pages=11&ndash;18 |year=2003 |volume=16 |isbn=978-0-909-92594-9 |citeseerx=10.1.1.12.2757}}\n* {{cite journal |title=Engineering Burstsort: Towards Fast In-Place String Sorting |first1=Ranjan |last1=Sinha |first2=Anthony |last2=Wirth |journal=ACM Journal of Experimental Algorithmics|volume=15 |issue=2.5 |pages=1&ndash;24 |date=March 2010 |doi=10.1145/1671973.1671978 |url=https://app.cs.amherst.edu/~ccmcgeoch/cs34/papers/a2_5-sinha.pdf|doi-broken-date=2019-02-15 }}\n\n==External links==\n* A burstsort implementation in Java: [https://github.com/nlfiedler/burstsort4j burstsort4j]\n* [[Judy array]]s are a type of copy burstsort: [https://code.google.com/p/judyarray C implementation]\n\n{{sorting}}\n\n[[Category:String sorting algorithms]]"
    },
    {
      "title": "Axiom of regularity",
      "url": "https://en.wikipedia.org/wiki/Axiom_of_regularity",
      "text": "In mathematics, the '''axiom of regularity''' (also known as the '''axiom of foundation''') is an axiom of [[Zermelo–Fraenkel set theory]] that states that every non-empty [[Set (mathematics)|set]] ''A'' contains an element that is [[Disjoint sets|disjoint]] from ''A''. In [[first-order logic]], the axiom reads:\n: <math>\\forall x\\,(x \\neq \\varnothing \\rightarrow \\exists y \\in x\\,(y \\cap x = \\varnothing))</math>.\nThe axiom implies that no set is an element of itself, and that there is no infinite [[sequence]] (''a<sub>n</sub>'') such that ''a<sub>i+1</sub>'' is an element of ''a<sub>i</sub>'' for all ''i''. With the [[axiom of dependent choice]] (which is a weakened form of the [[axiom of choice]]), this result can be reversed: if there are no such infinite sequences, then the axiom of regularity is true. Hence, the axiom of regularity is equivalent, given the axiom of dependent choice, to the alternative axiom that there are no downward infinite membership chains.\n\nThe axiom of regularity was introduced by {{harvtxt|von Neumann|1925}}; it was adopted in a formulation closer to the one found in contemporary textbooks by {{harvtxt|Zermelo|1930}}. Virtually all results in the branches of mathematics based on set theory hold even in the absence of regularity; see chapter 3 of {{harvtxt|Kunen|1980}}.  However, regularity makes some properties of [[Ordinal number|ordinals]] easier to prove; and it not only allows induction to be done on [[well-ordering|well-ordered sets]] but also on proper classes that are [[well-founded relation|well-founded relational structures]] such as the [[lexicographical ordering]] on <math>\\{ (n, \\alpha) \\vert n \\in \\omega \\land \\alpha \\text{ is an ordinal } \\} \\,.</math>\n\nGiven the other axioms of Zermelo–Fraenkel set theory, the axiom of regularity is equivalent to the [[epsilon-induction|axiom of induction]]. The axiom of induction tends to be used in place of the axiom of regularity in [[intuitionism|intuitionistic]] theories (ones that do not accept the [[law of the excluded middle]]), where the two axioms are not equivalent.\n\nIn addition to omitting the axiom of regularity, [[Non-well-founded set theory|non-standard set theories]] have indeed postulated the existence of sets that are elements of themselves.\n\n==Elementary implications of regularity==\n\n===No set is an element of itself===\nLet ''A'' be a set, and apply the axiom of regularity to {''A''}, which is a set by the [[axiom of pairing]]. We see that there must be an element of {''A''} which is disjoint from {''A''}. Since the only element of {''A''} is ''A'', it must be that ''A'' is disjoint from {''A''}. So, since ''A'' ∈ {''A''}, we cannot have ''A'' ∈ ''A'' (by the definition of [[Disjoint sets|disjoint]]).\n\n===No infinite descending sequence of sets exists===\nSuppose, to the contrary, that there is a [[function (mathematics)|function]], ''f'', on the [[natural number]]s with ''f''(''n''+1) an element of ''f''(''n'') for each ''n''. Define ''S'' = {''f''(''n''): ''n'' a natural number}, the range of ''f'', which can be seen to be a set from the [[axiom schema of replacement]]. Applying the axiom of regularity to ''S'', let ''B'' be an element of ''S'' which is disjoint from ''S''. By the definition of ''S'', ''B'' must be ''f''(''k'') for some natural number ''k''. However, we are given that ''f''(''k'') contains ''f''(''k''+1) which is also an element of ''S''. So ''f''(''k''+1) is in the intersection of ''f''(''k'') and ''S''. This contradicts the fact that they are disjoint sets. Since our supposition led to a contradiction, there must not be any such function, ''f''.\n\nThe nonexistence of a set containing itself can be seen as a special case where the sequence is infinite and constant.\n\nNotice that this argument only applies to functions ''f'' that can be represented as sets as opposed to undefinable classes. The [[hereditarily finite set]]s, V<sub>ω</sub>, satisfy the axiom of regularity (and all other axioms of [[ZFC]] except the [[axiom of infinity]]). So if one forms a non-trivial [[ultraproduct|ultrapower]] of V<sub>ω</sub>, then it will also satisfy the axiom of regularity. The resulting [[model (logic)|model]] <!--WHAT model?--> will contain elements, called non-standard natural numbers, that satisfy the definition of natural numbers in that model but are not really natural numbers. They are fake natural numbers which are \"larger\" than any actual natural number. This model will contain infinite descending sequences of elements. For example, suppose ''n'' is a non-standard natural number, then <math>(n-1) \\in n</math> and <math>(n-2) \\in (n-1)</math>, and so on. For any actual natural number ''k'', <math>(n-k-1) \\in (n-k)</math>. This is an unending descending sequence of elements. But this sequence is not definable in the model and thus not a set.  So no contradiction to regularity can be proved.\n\n===Simpler set-theoretic definition of the ordered pair===\nThe axiom of regularity enables defining the ordered pair (''a'',''b'') as <nowiki>{</nowiki>''a'',<nowiki>{</nowiki>''a'',''b''<nowiki>}}</nowiki>. See [[ordered pair]] for specifics. This definition eliminates one pair of braces from the canonical [[Kuratowski]] definition (''a'',''b'') = <nowiki>{{</nowiki>''a''<nowiki>}</nowiki>,<nowiki>{</nowiki>''a'',''b''<nowiki>}}</nowiki>.\n\n=== Every set has an ordinal rank ===\nThis was actually the original form of the axiom in von Neumann's axiomatization.\n\nSuppose ''x'' is any set. Let ''t'' be the [[transitive closure (set)|transitive closure]] of {''x''}. Let ''u'' be the subset of ''t'' consisting of unranked sets. If ''u'' is empty, then ''x'' is ranked and we are done. Otherwise, apply the axiom of regularity to ''u'' to get an element ''w'' of ''u'' which is disjoint from ''u''. Since ''w'' is in ''u'', ''w'' is unranked. ''w'' is a subset of ''t'' by the definition of transitive closure. Since ''w'' is disjoint from ''u'', every element of ''w'' is ranked. Applying the axioms of replacement and union to combine the ranks of the elements of ''w'', we get an ordinal rank for ''w'', to wit <math>\\textstyle \\operatorname{rank} (w) = \\cup \\{ \\operatorname{rank} (z) + 1 \\vert z \\in w \\}</math>. This contradicts the conclusion that ''w'' is unranked. So the assumption that ''u'' was non-empty must be false and ''x'' must have rank.\n\n=== For every two sets, only one can be an element of the other ===\nLet ''X'' and ''Y'' be sets. Then apply the axiom of regularity to the set {''X'',''Y''}. We see there must be an element of {''X'',''Y''} which is also disjoint from it. It must be either ''X'' or ''Y''. By the definition of disjoint then, we must have either ''Y'' is not an element of ''X'' or vice versa.\n\n==The axiom of dependent choice and no infinite descending sequence of sets implies regularity==\nLet the non-empty set ''S'' be a counter-example to the axiom of regularity; that is, every element of ''S'' has a non-empty intersection with ''S''. We define a binary relation ''R'' on ''S'' by <math>aRb :\\Leftrightarrow b \\in S \\cap a</math>, which is entire by assumption. Thus, by the axiom of dependent choice, there is some sequence (''a<sub>n</sub>'') in ''S'' satisfying ''a<sub>n</sub>Ra<sub>n+1</sub>'' for all ''n'' in '''N'''. As this is an infinite descending chain, we arrive at a contradiction and so, no such ''S'' exists.\n\n== Regularity and the rest of ZF(C) axioms ==\nRegularity was shown to be relatively consistent with the rest of ZF by {{harvtxt|Skolem|1923}} and {{harvtxt|von Neumann|1929}}, meaning that if ZF without regularity is consistent, then ZF (with regularity) is also consistent. For his proof in modern notation see {{harvtxt|Vaught|2001|loc=§10.1}} for instance.\n\nThe axiom of regularity was also shown to be [[Independence (mathematical logic)|independent]] from the other axioms of ZF(C), assuming they are consistent. The result was announced by [[Paul Bernays]] in 1941, although he did not publish a proof until 1954. The proof involves (and led to the study of) [[Rieger-Bernays permutation]] models (or method), which were used for other proofs of independence for non-well-founded systems ({{harvnb|Rathjen|2004|p=193}} and {{harvnb|Forster|2003|pp=210–212}}).\n\n== Regularity and Russell's paradox ==\n[[Naive set theory]] (the axiom schema of [[unrestricted comprehension]] and the [[axiom of extensionality]]) is inconsistent due to [[Russell's paradox]]. In early formalizations of sets, mathematicians and logicians have avoided that contradiction by replacing the axiom schema of comprehension with the much weaker [[axiom schema of separation]]. However, this step alone takes one to theories of sets which are considered too weak. So some of the power of comprehension was added back via the other existence axioms of ZF set theory (pairing, union, powerset, replacement, and infinity) which may be regarded as special cases of comprehension. So far, these axioms do not seem to lead to any contradiction. Subsequently, the axiom of choice and the axiom of regularity were added to exclude models with some undesirable properties. These two axioms are known to be relatively consistent.\n\nIn the presence of the axiom schema of separation, Russell's paradox becomes a proof that there is no [[universal set|set of all sets]]. The axiom of regularity together with the axiom of pairing also prohibit such a universal set.  However, Russell's paradox yields a proof that there is no \"set of all sets\" using the axiom schema of separation alone, without any additional axioms.  In particular, ZF without the axiom of regularity already prohibits such a universal set.\n\nIf a theory is extended by adding an axiom or axioms, then any (possibly undesirable) consequences of the original theory remain consequences of the extended theory. In particular, if ZF without regularity is extended by adding regularity to get ZF, then any contradiction (such as Russell's paradox) which followed from the original theory would still follow in the extended theory.\n\nThe existence of [[Quine atom]]s (sets that satisfy the formula equation ''x''&nbsp;=&nbsp;{''x''}, i.e. have themselves as their only elements) is consistent with the theory obtained by removing the axiom of regularity from ZFC. Various [[non-well-founded set theory|non-wellfounded set theories]] allow \"safe\" circular sets, such as Quine atoms, without becoming inconsistent by means of Russell's paradox.{{harv|Rieger|2011|pp=175,178}}\n\n== Regularity, the cumulative hierarchy, and types ==\nIn ZF it can be proven that the class <math> \\bigcup_{\\alpha} V_\\alpha \\! </math>, called the [[von Neumann universe]], is equal to the class of all sets. This statement is even equivalent to the axiom of regularity (if we work in ZF with this axiom omitted). From any model which does not satisfy axiom of regularity, a model which satisfies it can be constructed by taking only sets in <math> \\bigcup_{\\alpha} V_\\alpha \\! </math>.\n\n{{harvs|txt|last=Enderton|first=Herbert|year=1977|loc=p. 206|author-link=Herbert Enderton}} wrote that \"The idea of rank is a descendant of Russell's concept of ''type''\". Comparing ZF with [[type theory]], [[Alasdair Urquhart]] wrote that \"Zermelo's system has the notational advantage of not containing any explicitly typed variables, although in fact it can be seen as having an implicit type structure built into it, at least if the axiom of regularity is included. The details of this implicit typing are spelled out in [[#{{harvid|Zermelo|1930}}|[Zermelo 1930]]], and again in a well-known article of [[George Boolos]] [[#{{harvid|Boolos|1971}}|[Boolos 1971]]].\" {{harvtxt|Urquhart|2003|p=305}}\n\n{{harvs|txt|last=Scott|first=Dana|year=1974|authorlink=Dana Scott}} went further and claimed that:\n\n{{quote|The truth is that there is only one satisfactory way of avoiding the paradoxes: namely, the use of some form of the ''theory of types''. That was at the basis of both Russell's and Zermelo's intuitions. Indeed the best way to regard Zermelo's theory is as a simplification and extension of Russell's. (We mean Russell's ''simple'' theory of types, of course.) The simplification was to make the types ''cumulative''. Thus mixing of types is easier and annoying repetitions are avoided. Once the later types are allowed to accumulate the earlier ones, we can then easily imagine ''extending'' the types into the transfinite&mdash;just how far we want to go must necessarily be left open. Now Russell made his types ''explicit'' in his notation and Zermelo left them ''implicit''. [emphasis in original]}}\n\nIn the same paper, Scott shows that an axiomatic system based on the inherent properties of the cumulative hierarchy turns out to be equivalent to ZF, including regularity.{{harv|Lévy|2002|p=73}}\n\n== History ==\n\nThe concept of well-foundedness and [[Von Neumann universe|rank]] of a set were both introduced by [[Dmitry Mirimanoff]] ([[#{{harvid|Mirimanoff|1917}}|1917]]) cf. {{harvtxt|Lévy|2002|p=68}} and {{harvtxt|Hallett|1986|loc=§4.4, esp. p. 186, 188}}. Mirimanoff called a set ''x'' \"regular\" (French: \"ordinaire\") if every descending chain ''x'' ∋ ''x<sub>1</sub>'' ∋ ''x<sub>2</sub>''  ∋ ... is finite. Mirimanoff however did not consider his notion of regularity (and well-foundedness) as an axiom to be observed by all sets {{harv|Halbeisen|2012|pp=62–63}}; in later papers Mirimanoff also explored what are now called [[non-well-founded set theory|non-well-founded sets]] (\"extraordinaire\" in Mirimanoff's terminology) {{harv|Sangiorgi|2011|pp=17–19, 26}}.\n\n{{harvtxt|Skolem|1923}} and {{harvtxt|von Neumann|1925}} pointed out that  non-well-founded sets are superfluous (on p.&nbsp;404 in [[#{{harvid|van Heijenoort|1967}}|van Heijenoort's translation]]) and in the same publication von Neumann gives an axiom (p.&nbsp;412 in translation) which excludes some, but not all, non-well-founded sets {{harv|Rieger|2011|p=179}}. In a subsequent publication, {{harvtxt|von Neumann|1928}} gave the following axiom (rendered in modern notation by A. Rieger):\n\n: <math>\\forall x\\,(x \\neq \\emptyset \\rightarrow \\exists y \\in x\\,(y \\cap x = \\emptyset))</math>.\n\n== Regularity in the presence of urelements ==\n\n[[Urelements]] are objects that are not sets, but which can be elements of sets. In ZF set theory, there are no urelements, but in some other set theories such as [[Urelement#Urelements in set theory|ZFA]], there are. In these theories, the axiom of regularity must be modified. The statement \"<math>x \\not = \\emptyset</math>\" needs to be replaced with a statement that <math>x</math> is not empty and is not an urelement. One suitable replacement is <math>(\\exists y)[y \\in x]</math>, which states that ''x'' is [[inhabited set|inhabited]].\n\n==See also==\n*[[Non-well-founded set theory]]\n*[[Scott's trick]]\n\n== References ==\n\n* {{citation |first=Paul Isaac|last= Bernays|authorlink=Paul Bernays| title= A system of axiomatic set theory. Part II |journal= The Journal of Symbolic Logic| volume= 6 |issue= 1| year = 1941 | pages = 1–17 | doi=10.2307/2267281 | jstor=2267281}}\n* {{citation | first= Paul Isaac|last = Bernays|authorlink=Paul Bernays| title= A system of axiomatic set theory. Part VII |journal = The Journal of Symbolic Logic| volume = 19 |issue = 2| year = 1954 | pages = 81–96 | doi=10.2307/2268864 | jstor=2268864}}\n*{{citation|last=Boolos|first= George |authorlink=George Boolos | year= 1971 | title = The iterative conception of set | journal = Journal of Philosophy | volume = 68 |issue= 8 |pages= 215–231 | doi=10.2307/2025204 | jstor=2025204}} reprinted in {{citation|last=Boolos|first= George |year=1998|title=Logic, Logic and Logic|pages=13–29|publisher=Harvard University Press}}\n*{{citation | last= Enderton | first = Herbert B. | title = Elements of Set Theory | publisher = Academic Press | year=1977}}\n*{{citation|title = Logic, induction and sets| last = Forster | first = T. | publisher = Cambridge University Press | year = 2003}}\n*{{citation| first= Lorenz J. |last = Halbeisen | title=Combinatorial Set Theory: With a Gentle Introduction to Forcing|year=2012|publisher=Springer}}\n* {{citation|first=Michael|last=Hallett|title=Cantorian set theory and limitation of size|publisher=Oxford University Press|year=1996|origyear=first published 1984|isbn=978-0-19-853283-5}}\n*{{Citation | last=Jech | first= Thomas |authorlink=Thomas Jech | year= 2003 |title = Set Theory: The Third Millennium Edition, Revised and Expanded  |publisher=Springer| isbn = 978-3-540-44085-7}}\n*{{Citation | last=Kunen | first=Kenneth |authorlink=Kenneth Kunen | year = 1980|title = Set Theory: An Introduction to Independence Proofs| publisher=Elsevier| isbn=978-0-444-86839-8}}\n*{{cite book| last = Lévy | first =Azriel | authorlink=Azriel Lévy | isbn = 978-0-486-42079-0 | year =2002 |origyear= first published in 1979 | title=Basic set theory| publisher=Dover Publications|location=Mineola, New York}}\n*{{Citation | last1=Mirimanoff | first1=D. | title=Les antinomies de Russell et de Burali-Forti et le probleme fondamental de la theorie des ensembles | year=1917 | journal=L'Enseignement Mathématique | volume=19|pages=37–52}}\n*{{citation|editor1-first=Godehard |editor1-last=Link|title=One Hundred Years of Russell ́s Paradox: Mathematics, Logic, Philosophy|year=2004|publisher=Walter de Gruyter|isbn=978-3-11-019968-0| chapter =Predicativity, Circularity, and Anti-Foundation | first = M. | last= Rathjen| chapter-url=http://www1.maths.leeds.ac.uk/~rathjen/russelle.pdf}}\n* {{Citation\n| last1 = Rieger | first1 = Adam\n| chapter = Paradox, ZF, and the Axiom of Foundation \n| doi = 10.1007/978-94-007-0214-1_9 \n| title = Logic, Mathematics, Philosophy, Vintage Enthusiasms. Essays in Honour of John L. Bell.\n| editors = David DeVidi, Michael Hallett, Peter Clark\n| series = The Western Ontario Series in Philosophy of Science\n| volume = 75\n| pages = 171–187 \n| year = 2011 \n| isbn = 978-94-007-0213-4 \n| pmid =  \n| pmc = \n| chapter-url = http://eprints.gla.ac.uk/3810/1/JLB.pdf| citeseerx = 10.1.1.100.9052\n}}\n* {{citation | first = L. |last = Riegger | url = http://dml.cz/bitstream/handle/10338.dmlcz/100254/CzechMathJ_07-1957-3_1.pdf | title = A contribution to Gödel's axiomatic set theory | journal = Czechoslovak Mathematical Journal | volume = 7 | year = 1957 | pages = 323–357}}\n*{{citation| first = Davide | last = Sangiorgi | year = 2011 | chapter = Origins of bisimulation and coinduction | editor1-first = Davide | editor1-last = Sangiorgi | editor2-first = Jan | editor2-last = Rutten | title = Advanced Topics in Bisimulation and Coinduction | publisher = Cambridge University Press}}\n* {{citation | last = Scott | first = Dana Stewart | authorlink=Dana Scott | year = 1974 | chapter = Axiomatizing set theory | title =  Axiomatic set theory. Proceedings of Symposia in Pure Mathematics Volume 13, Part II | pages = 207–214}}\n* {{cite book|ref=harv| last=Skolem| first=Thoralf|authorlink=Thoralf Skolem | year=1923|title=Axiomatized set theory}} Reprinted in ''From Frege to Gödel'', van Heijenoort, 1967, in English translation by Stefan Bauer-Mengelberg, pp.&nbsp;291&ndash;301.\n*{{citation|last = Urquhart|first = Alasdair | chapter = The Theory of Types | editor-last = Griffin| editor-first =Nicholas | title =The Cambridge Companion to Bertrand Russell | publisher = Cambridge University Press | year=2003}}\n* {{citation|first=Robert L. |last = Vaught|title=Set Theory: An Introduction| year=2001| publisher=Springer| isbn=978-0-8176-4256-3| edition=2nd}}\n*{{citation|last=von Neumann|first = John| authorlink=John von Neumann|year=1925|title=Eine axiomatiserung der Mengenlehre|journal=Journal für die Reine und Angewandte Mathematik|volume=154|pages=219–240}}; translation in {{citation|last=van Heijenoort | first =Jean | year =1967 | title = From Frege to Gödel: A Source Book in Mathematical Logic, 1879–1931 | pages = 393–413 }}\n*{{citation|last = von Neumann|first = John| authorlink=John von Neumann |year= 1928|title= Über die Definition durch transfinite Induktion und verwandte Fragen der allgemeinen Mengenlehre| journal= Mathematische Annalen|volume = 99 |pages=373–391|doi=10.1007/BF01459102}}\n*{{citation| last = von Neumann |first = John| authorlink=John von Neumann| year = 1929 | title= Uber eine Widerspruchfreiheitsfrage in der axiomatischen Mengenlehre| journal = Journal für die Reine und Angewandte Mathematik |volume = 160 |issue = 160|pages = 227–241 | doi=10.1515/crll.1929.160.227}}\n*{{citation |last=Zermelo|first= Ernst |authorlink=Ernst Zermelo | year= 1930 | title = Über Grenzzahlen und Mengenbereiche. Neue Untersuchungen über die Grundlagen der Mengenlehre. | journal = Fundamenta Mathematicae | volume = 16 |pages= 29–47|url=http://matwbn.icm.edu.pl/ksiazki/fm/fm16/fm1615.pdf|doi= 10.4064/fm-16-1-29-47 }}; translation in {{citation | editor-last= Ewald |editor-first= W.B. | year = 1996| title = From Kant to Hilbert: A Source Book in the Foundations of Mathematics Vol. 2 | publisher= Clarendon Press |pages = 1219–33}}\n\n==External links==\n*{{PlanetMath|urlname=axiomoffoundation|title=Axiom of foundation}}\n*[https://ncatlab.org/nlab/show/inhabited+set Inhabited set] and [https://ncatlab.org/nlab/show/axiom+of+foundation the axiom of foundation] on nLab\n\n{{Set theory}}\n\n{{DEFAULTSORT:Axiom Of Regularity}}\n[[Category:Axioms of set theory]]\n[[Category:Wellfoundedness]]"
    },
    {
      "title": "Dickson's lemma",
      "url": "https://en.wikipedia.org/wiki/Dickson%27s_lemma",
      "text": "In [[mathematics]], '''Dickson's lemma''' states that every set of <math>n</math>-tuples of [[natural number]]s has finitely many [[minimal element]]s. This simple fact from [[combinatorics]] has become attributed to the American algebraist [[Leonard Dickson|L. E. Dickson]], who used it to prove a result in [[number theory]] about [[perfect number]]s.<ref name=\"dickson\"/> However, the lemma was certainly known earlier, for example to [[Paul Gordan]] in his research on [[invariant theory]].<ref name=\"buchberger\">{{citation\n | last1 = Buchberger | first1 = Bruno | author1-link = Bruno Buchberger\n | last2 = Winkler | first2 = Franz\n | isbn = 9780521632980\n | page = 83\n | publisher = Cambridge University Press\n | series = London Mathematical Society Lecture Note Series\n | title = Gröbner Bases and Applications\n | url = https://books.google.com/books?id=tfa7dpQf1OIC&pg=PA83\n | volume = 251\n | year = 1998}}.</ref>\n\n==Example==\n[[File:Dickson hyperbola9.svg|thumb|240px|Infinitely many minimal pairs of real numbers ''x'',''y'' (the black hyperbola) but only five minimal pairs of positive integers (red) have ''xy''&nbsp;≥&nbsp;9.]]\nLet <math>K</math> be a fixed number, and let <math>S = \\{(x,y)\\mid xy\\ge K\\}</math> be the set of pairs of numbers whose product is at least <math>K</math>. When defined over the positive [[real number]]s, <math>S</math> has infinitely many minimal elements of the form <math>(x,K/x)</math>, one for each positive number <math>x</math>; this set of points forms one of the branches of a [[hyperbola]]. The pairs on this hyperbola are minimal, because it is not possible for a different pair that belongs to <math>S</math> to be less than or equal to <math>(x,K/x)</math> in both of its coordinates. However, Dickson's lemma concerns only tuples of natural numbers, and over the natural numbers there are only finitely many minimal pairs. Every minimal pair <math>(x,y)</math> of natural numbers has <math>x\\le K</math> and <math>y\\le K</math>, for if ''x'' were greater than ''K'' then (''x''&nbsp;&minus;1,''y'') would also belong to ''S'', contradicting the minimality of (''x'',''y''), and symmetrically if ''y'' were greater than ''K'' then (''x'',''y''&nbsp;&minus;1) would also belong to&nbsp;''S''. Therefore, over the natural numbers, <math>S</math> has at most <math>K^2</math> minimal elements, a finite number.<ref group=note>With more care, it is possible to show that one of <math>x</math> and <math>y</math> is at most <math>\\sqrt K</math>, and that there is at most one minimal pair for each choice of one of the coordinates, from which it follows that there are at most <math>2\\sqrt K</math> minimal elements.</ref>\n\n==Formal statement==\nLet <math>\\mathbb{N}</math> be the set of non-negative integers ([[natural numbers]]), let ''n'' be any fixed constant, and let <math>\\mathbb{N}^n</math> be the set of <math>n</math>-tuples of natural numbers. These tuples may be given a [[pointwise]] [[partial order]], the [[product order]], in which <math>(a_1,a_2,\\dots,a_n)\\le (b_1,b_2,\\dots b_n)</math> if and only if, for every <math>i</math>, <math>a_i\\le b_i</math>.\nThe set of tuples that are greater than or equal to some particular tuple <math>(a_1,a_2,\\dots,a_n)</math> forms a positive [[orthant]] with its apex at the given tuple.\n\nWith this notation, Dickson's lemma may be stated in several equivalent forms:\n*In every subset <math>S\\neq\\emptyset</math> of <math>\\mathbb{N}^n</math>, there is at least one but no more than a finite number of elements that are [[minimal element]]s of <math>S</math> for the pointwise partial order.<ref name=\"kruskal\">{{cite journal | authorlink = Joseph Kruskal | last=Kruskal |first= Joseph B. | title=The theory of well-quasi-ordering: A frequently discovered concept | journal=[[Journal of Combinatorial Theory]] | series = Series A | year=1972 | volume=13 | page=298 | doi=10.1016/0097-3165(72)90063-5 | issue = 3}}</ref>\n*For every infinite sequence <math>(x_i)_{i\\in\\mathbb{N}}</math> of <math>n</math>-tuples of natural numbers, there exist two indices <math>i<j</math> such that <math>x_i \\leq x_j</math> holds with respect to the pointwise order.<ref name=\"ffss\">{{citation\n | last1 = Figueira | first1 = Diego\n | last2 = Figueira | first2 = Santiago\n | last3 = Schmitz | first3 = Sylvain\n | last4 = Schnoebelen | first4 = Philippe\n | arxiv = 1007.2989\n | contribution = Ackermannian and primitive-recursive bounds with Dickson's lemma\n | doi = 10.1109/LICS.2011.39\n | mr = 2858898\n | page = 269\n | publisher = IEEE Computer Soc., Los Alamitos, CA\n | title = 26th Annual IEEE Symposium on Logic in Computer Science (LICS 2011)\n | year = 2011}}.</ref>\n*The partially ordered set <math>(\\mathbb{N}^n,\\le)</math> does not contain infinite [[antichain|antichains]] nor [[descending chain condition|infinite (strictly) descending sequences]] of <math>n</math>-tuples.<ref name=\"ffss\" />\n*The partially ordered set <math>(\\mathbb{N}^n,\\le)</math> is a [[well-quasi-ordering|well partial order]].<ref>{{citation\n | last = Onn | first = Shmuel\n | editor1-last = Floudas | editor1-first = Christodoulos A.\n | editor2-last = Pardalos | editor2-first = Panos M.\n | arxiv = math/0703575\n | contribution = Convex Discrete Optimization\n | edition = 2nd\n | isbn = 9780387747583\n | pages = 513–550\n | publisher = Springer\n | title = Encyclopedia of Optimization, Vol. 1\n | year = 2008| bibcode = 2007math......3575O}}.</ref>\n*Every  subset <math>S</math> of <math>\\mathbb{N}^n</math> may be covered by a finite set of positive orthants, whose apexes all belong to <math>S</math>.\n\n==Generalizations and applications==\nDickson used his lemma to prove that, for any given number <math>n</math>, there can exist only a finite number of [[odd number|odd]] [[perfect number]]s that have at most <math>n</math> [[prime factor]]s.<ref name=\"dickson\">{{citation\n | last = Dickson | first = L. E. | author-link = Leonard Dickson\n | doi = 10.2307/2370405\n | issue = 4\n | journal = American Journal of Mathematics\n | jstor = 2370405\n | pages = 413–422\n | title = Finiteness of the odd perfect and primitive abundant numbers with ''n'' distinct prime factors\n | volume = 35\n | year = 1913}}.</ref> However, it remains open whether there exist any odd perfect numbers at all.\n\nThe [[divisibility]] relation among the [[smooth number|''P''-smooth numbers]], natural numbers whose prime factors all belong to the [[finite set]] ''P'', gives these numbers the structure of a partially ordered set isomorphic to <math>(\\mathbb{N}^{|P|},\\le)</math>. Thus, for any set ''S'' of ''P''-smooth numbers, there is a finite subset of ''S'' such that every element of ''S'' is divisible by one of the numbers in this subset. This fact has been used, for instance, to show that there exists an [[algorithm]] for classifying the winning and losing moves from the initial position in the game of [[Sylver coinage]], even though the algorithm itself remains unknown.<ref>{{citation\n | last1 = Berlekamp | first1 = Elwyn R. | author1-link = Elwyn Berlekamp\n | last2 = Conway | first2 = John H.\n | last3 = Guy | first3 = Richard K.\n | contribution = 18 The Emperor and his Money\n | pages = 609–640\n | publisher = Academic Press\n | title = [[Winning Ways for your Mathematical Plays]], Vol. 3\n | year = 2003}}. See especially \"Are outcomes computable\", p. 630.</ref>\n\nThe tuples <math>(a_1,a_2,\\dots,a_n)</math> in  <math>\\mathbb{N}^n</math> correspond one-for-one with the [[monomial]]s <math>x_1^{a_1}x_2^{a_2}\\dots x_n^{a_n}</math> over a set of <math>n</math> variables <math>x_1,x_2,\\dots x_n</math>. Under this correspondence, Dickson's lemma may be seen as a special case of [[Hilbert's basis theorem]] stating that every [[Polynomial ring|polynomial]] [[ideal (ring theory)|ideal]] has a finite basis, for the ideals generated by monomials. Indeed, [[Paul Gordan]] used this restatement of Dickson's lemma in 1899 as part of a proof of Hilbert's basis theorem.<ref name=\"buchberger\"/>\n\n==See also==\n*[[Gordan's lemma]]\n\n==Notes==\n{{reflist|group=note}}\n\n==References==\n{{reflist}}\n\n[[Category:Combinatorics]]\n[[Category:Lemmas]]\n[[Category:Wellfoundedness]]"
    },
    {
      "title": "Epsilon-induction",
      "url": "https://en.wikipedia.org/wiki/Epsilon-induction",
      "text": "In [[mathematics]], '''<math>\\in</math>-induction''' ('''epsilon-induction''') is a variant of [[transfinite induction]] that can be used in [[axiomatic set theory|set theory]] to prove that all [[Set (mathematics)|sets]] satisfy a given property ''P''[''x''].  If the truth of the property for ''x'' follows from its truth [[for all]] elements of ''x'', for every set ''x'', then the property is true of all sets.  In symbols:\n\n: ''<math>\\forall x \\Big(\\forall y (y \\in x \\rightarrow P[y]) \\rightarrow P[x]\\Big) \\rightarrow \\forall x \\, P[x]</math>''\n\nThis principle, sometimes called the '''axiom of induction''' (in set theory), is equivalent to the [[axiom of regularity]] given the other [[Zermelo–Fraenkel set theory|ZF]] axioms.  <math>\\in</math>-induction is a special case of [[well-founded relation#Induction and recursion|well-founded induction]]. The Axiom of Foundation (regularity) implies epsilon-induction.\n\nThe name is most often pronounced \"epsilon-induction\", because the set membership symbol <math>\\in</math> historically developed from the Greek letter <math>\\epsilon </math>.\n\n==See also==\n* [[Mathematical induction]]\n* [[Transfinite induction]]\n* [[Well-founded relation#Induction and recursion|Well-founded induction]]\n\n[[Category:Mathematical induction]]\n[[Category:Wellfoundedness]]\n\n{{settheory-stub}}"
    },
    {
      "title": "Kőnig's lemma",
      "url": "https://en.wikipedia.org/wiki/K%C5%91nig%27s_lemma",
      "text": "{{Other uses|König's theorem (disambiguation)}}\n\n[[File:Denes König - Über eine Schlussweise aus dem Endlichen ins Unendliche.png|thumb|Kőnig's 1927 publication]]\n'''Kőnig's lemma''' or '''Kőnig's infinity lemma''' is a [[theorem]] in [[graph theory]] due to the Hungarian mathematician [[Dénes Kőnig]] who published it in 1927.<ref>{{harvtxt|Kőnig|1927}} as explained in {{harvtxt|Franchella|1997}}</ref> It gives a sufficient condition for an infinite graph to have an infinitely long path. The computability aspects of this theorem have been thoroughly investigated by researchers in [[mathematical logic]], especially in [[recursion theory|computability theory]].  This theorem also has important roles in [[constructive mathematics]] and [[proof theory]].\n\n==Statement of the lemma==\nLet ''G'' be a [[connected graph|connected]], [[Glossary of graph theory terms#finite|locally finite]], [[Glossary of graph theory terms#finite|infinite graph]] (this means: the graph has infinitely many vertices, each vertex is adjacent to only finitely many other vertices, and any two vertices can be connected by a path). Then ''G'' contains a [[Ray (graph theory)|ray]]: a [[path (graph theory)|simple path]] (a path with no repeated vertices) that starts at one vertex and continues from it through infinitely many vertices.\n\nA common special case of this is that every infinite [[tree (graph theory)|tree]] contains either a vertex of infinite [[degree (graph theory)|degree]] or an infinite simple path.\n\n===Proof===\nStart with any vertex ''v''<sub>1</sub>. Every one of the infinitely many vertices of ''G'' can be reached from ''v''<sub>1</sub> with a simple path,  and each such path must start with one of the finitely many vertices adjacent to ''v''<sub>1</sub>. There must be one of those adjacent vertices through which infinitely many vertices can be reached without going through ''v''<sub>1</sub>. If there were not, then the entire graph would be the union of finitely many finite sets, and thus finite, contradicting the assumption that the graph is infinite. We may thus pick one of these vertices and call it ''v''<sub>2</sub>.\n\nNow infinitely many vertices of ''G'' can be reached from ''v''<sub>2</sub> with a simple path which does not include the vertex ''v''<sub>1</sub>. Each such path must start with one of the finitely many vertices adjacent to ''v''<sub>2</sub>. So an argument similar to the one above shows that there must be one of those adjacent vertices through which infinitely many vertices can be reached; pick one and call it ''v''<sub>3</sub>.\n\nContinuing in this fashion, an infinite simple path can be constructed using [[mathematical induction]] and a weak version of the [[axiom of dependent choice]]. At each step, the induction hypothesis states that there are infinitely many nodes reachable by a simple path from a particular node ''v''<sub>i</sub> that does not go through one of a finite set of vertices. The induction argument is that one of the vertices adjacent to ''v''<sub>i</sub> satisfies the induction hypothesis, even when ''v''<sub>i</sub> is added to the finite set.\n\nThe result of this induction argument is that for all ''n'' it is possible to choose a vertex ''v''<sub>''n''</sub> as the construction describes. The set of vertices chosen in the construction is then a chain in the graph, because each one was chosen to be adjacent to the previous one, and the construction guarantees that the same vertex is never chosen twice.\n\n==Computability aspects==\nThe computability aspects of Kőnig's lemma have been thoroughly investigated.  The form of Kőnig's lemma most convenient for this purpose is the one which states that any infinite finitely branching subtree of <math>\\omega^{<\\omega}</math> has an infinite path.   Here <math>\\omega</math> denotes the set of natural numbers (thought of as an [[ordinal number]]) and <math>\\omega^{<\\omega}</math> the tree whose nodes are all finite sequences of natural numbers, where the parent of a node is obtained by removing the last element from a sequence.  Each finite sequence can be identified with a partial function from <math>\\omega</math> to itself, and each infinite path can be identified with a total function.  This allows for an analysis using the techniques of computability theory.\n\nA subtree of <math>\\omega^{<\\omega}</math> in which each sequence has only finitely many immediate extensions (that is, the tree has finite degree when viewed as a graph) is called '''finitely branching'''.  Not every infinite subtree of <math>\\omega^{<\\omega}</math> has an infinite path, but Kőnig's lemma shows that any finitely branching subtree must have such a path.\n\nFor any subtree ''T'' of <math>\\omega^{<\\omega}</math> the notation Ext(''T'') denotes the set of nodes of ''T'' through which there is an infinite path.  Even when ''T'' is computable the set Ext(''T'') may not be computable.  Every subtree ''T'' of\n<math>\\omega^{<\\omega}</math> that has a path has a path computable from Ext(''T'').\n\nIt is known that there are non-finitely branching computable subtrees of <math>\\omega^{<\\omega}</math> that have no [[arithmetical hierarchy|arithmetical]] path, and indeed no [[analytical hierarchy|hyperarithmetical]] path.<ref>{{harvtxt|Rogers|1967}}, p.&nbsp;418ff.</ref>  However, every computable subtree of <math>\\omega^{<\\omega}</math> with a path must have a path computable from [[Kleene's O]], the canonical <math>\\Pi^1_1</math> complete set.  This is because the set Ext(''T'') is always <math>\\Sigma^1_1</math> (see [[analytical hierarchy]]) when ''T'' is computable.\n\nA finer analysis has been conducted for computably bounded trees.  A subtree of <math>\\omega^{<\\omega}</math> is called '''computably bounded''' or '''recursively bounded''' if there is a computable function ''f'' from <math>\\omega</math> to <math>\\omega</math> such that for every sequence in the tree and every ''n'', the ''n''th element of the sequence is at most ''f''(''n'').  Thus ''f'' gives a bound for how “wide” the tree is.  The following [[basis theorem (computability)|basis theorem]]s apply to infinite, computably bounded, computable subtrees of <math>\\omega^{< \\omega}</math>.\n* Any such tree has a path computable from <math>0'</math>, the canonical Turing complete set that can decide the [[halting problem]].\n* Any such tree has a path that is [[Low (computability)|low]].  This is known as the [[low basis theorem]].\n* Any such tree has a path that is ''hyperimmune free''.  This means that any function computable from the path is dominated by a computable function.\n* For any noncomputable subset ''X'' of <math>\\omega</math> the tree has a path that does not compute&nbsp;''X''.\n\nA weak form of Kőnig's lemma which states that every infinite binary tree has an infinite branch is used to define the subsystem WKL<sub>0</sub> of [[second-order arithmetic]].  This subsystem has an important role in [[reverse mathematics]]. Here a binary tree is one in which every term of every sequence in the tree is 0 or 1, which is to say the tree is computably bounded via the constant function 2.  The full form of Kőnig's lemma is not provable in WKL<sub>0</sub>, but is equivalent to the stronger subsystem ACA<sub>0</sub>.\n\n==Relationship to constructive mathematics and compactness==\nThe proof given above is not generally considered to be [[mathematical constructivism|constructive]], because at each step it uses a [[Reductio ad absurdum|proof by contradiction]] to establish that there exists an adjacent vertex from which infinitely many other vertices can be reached, and because of the reliance on a weak form of the [[axiom of choice]]. Facts about the computational aspects of the lemma suggest that no proof can be given that would be considered constructive by the main schools of [[constructive mathematics]].\n\nThe fan theorem of {{harvs|first=L. E. J.|last=Brouwer|authorlink=Luitzen Egbertus Jan Brouwer|year=1927|txt}} is, from a classical point of view, the [[contrapositive]] of a form of Kőnig's lemma.  A subset ''S'' of <math>\\{0,1\\}^{<\\omega}</math> is called a ''bar'' if any function from <math>\\omega</math> to the set <math>\\{0,1\\}</math> has some initial segment in ''S''.  A bar is ''detachable'' if every sequence is either in the bar or not in the bar (this assumption is required because the theorem is ordinarily considered in situations where the law of the excluded middle is not assumed).   A bar is ''uniform'' if there is some number ''N'' so that any function from <math>\\omega</math> to <math>\\{0,1\\}</math> has an initial segment in the bar of length no more than <math>N</math>.  Brouwer's fan theorem says that any detachable bar is uniform.\n\nThis can be proven in a classical setting by considering the bar as an open covering of the [[Compact space|compact]] topological space <math>\\{0,1\\}^\\omega</math>.  Each sequence in the bar represents a basic open set of this space, and these basic open sets cover the space by assumption.  By compactness, this cover has a finite subcover.  The ''N'' of the fan theorem can be taken to be the length of the longest sequence whose basic open set is in the finite subcover. This topological proof can be used in classical mathematics to show that the following form of Kőnig's lemma holds:  for any natural number ''k'', any infinite subtree of the tree <math>\\{0,\\ldots,k\\}^{<\\omega}</math> has an infinite path.\n\n==Relationship with the axiom of choice==\nKőnig's lemma may be considered to be a choice principle; the first proof above illustrates the relationship between the lemma and the [[axiom of dependent choice]].  At each step of the induction, a vertex with a particular property must be selected.  Although it is proved that at least one appropriate vertex exists, if there is more than one suitable vertex there may be no canonical choice.  In fact, the full strength of the axiom of dependent choice is not needed; as described below, the [[axiom of countable choice]] suffices.\n\nIf the graph is countable, the vertices are well-ordered and one can canonically choose the smallest suitable vertex. In this case, Kőnig's lemma is provable in second-order arithmetic with [[arithmetical comprehension]], and, a fortiori, in [[Zermelo–Fraenkel set theory|ZF set theory]] (without choice).\n\nKőnig's lemma is essentially the restriction of the axiom of dependent choice to entire relations ''R'' such that for each ''x'' there are only finitely many ''z'' such that ''xRz''.  Although the axiom of choice is, in general, stronger than the principle of dependent choice, this restriction of dependent choice is equivalent to a restriction of the axiom of choice.\nIn particular, when the branching at each node is done on a finite subset of an arbitrary set not assumed to be countable, the form of Kőnig's lemma that says \"Every infinite finitely branching tree has an infinite path\" is equivalent to the principle that every countable set of finite sets has a choice function, that is to say, the axiom of countable choice for finite sets.<ref>{{harvtxt|Truss|1976}}, p.&nbsp;273; compare {{harvtxt|Lévy|1979}}, Exercise IX.2.18.</ref><ref>{{cite book | last1 = Howard | first1 = Paul | authorlink1 = Paul Howard (mathematician) | last2 = Rubin | first2 = Jean | authorlink2 = Jean E. Rubin | title = Consequences of the Axiom of Choice | year = 1998 | publisher = American Mathematical Society | place = Providence, RI | series = Mathematical Surveys and Monographs | volume = 59}}</ref> This form of the axiom of choice (and hence of Kőnig's lemma) is not provable in ZF set theory.\n\n== Generalization ==\nIn the [[category of sets]], the [[inverse limit]] of any inverse system of non-empty finite sets is non-empty. This may be seen as a generalization of Kőnig's lemma and can be proved with [[Tychonoff's theorem]], viewing the finite sets as compact discrete spaces, and then using the [[finite intersection property]] characterization of compactness.\n\n==See also==\n* [[Aronszajn tree]], for the possible existence of counterexamples when generalizing the lemma to higher cardinalities.\n* [[PA degree]]\n\n==Notes==\n{{reflist}}\n\n==References==\n*{{citation\n | last = Brouwer | first = L. E. J. | author-link = Luitzen Egbertus Jan Brouwer\n | title = On the Domains of Definition of Functions\n | year = 1927}}. published in {{citation\n | editor-last = van Heijenoort | editor-first = Jean\n | title = From Frege to Gödel\n | year = 1967}}.\n*{{citation\n | last = Cenzer | first = Douglas\n | contribution = <math>\\Pi^0_1</math> classes in computability theory\n | doi = 10.1016/S0049-237X(99)80018-4\n | isbn = 0-444-89882-4\n | mr = 1720779\n | pages = 37–85\n | publisher = Elsevier\n | title = Handbook of Computability Theory\n | year = 1999}}.\n*{{citation\n | last = Kőnig | first = D. | authorlink = Dénes Kőnig\n | issue = 8\n | journal = Fundamenta Mathematicae\n | language = French\n | pages = 114–134\n | title = Sur les correspondances multivoques des ensembles\n | url = http://matwbn.icm.edu.pl/ksiazki/fm/fm8/fm815.pdf\n | year = 1926}}.\n*{{citation\n | last = Kőnig | first = D. | authorlink = Dénes Kőnig\n | issue = 3(2-3)\n | journal = Acta Sci. Math. (Szeged)\n | language = German\n | pages = 121–130\n | title = Über eine Schlussweise aus dem Endlichen ins Unendliche\n | url = http://acta.fyx.hu/acta/showCustomerArticle.action?id=5131&dataObjectType=article&returnAction=showCustomerVolume&sessionDataSetId=2b29ea26fa2c9ba&style=\n | year = 1927}}.\n*{{citation\n | last = Franchella | first = Miriam | authorlink = Miriam Franchella\n | issue = 51(1)3:2-3\n | journal = Archive for History of Exact Sciences\n | pages = 3–27\n | title = On the origins of Dénes König's infinity lemma\n | year = 1997\n | doi=10.1007/BF00376449}}.\n*{{citation\n | last1 = Howard | first1 = Paul | authorlink1 = Paul Howard (mathematician)\n | last2 = Rubin | first2 = Jean | authorlink2 = Jean E. Rubin\n | title = Consequences of the Axiom of Choice\n | year = 1998\n | publisher = American Mathematical Society\n | place = Providence, RI\n | series = Mathematical Surveys and Monographs\n | volume = 59\n}}.\n*{{citation\n | last = Kőnig | first = D. | author-link = Dénes Kőnig\n | language = German\n | location = Leipzig\n | publisher = Akad. Verlag\n | title = Theorie der Endlichen und Unendlichen Graphen: Kombinatorische Topologie der Streckenkomplexe\n | year = 1936}}.\n*{{citation\n | last = Lévy | first = Azriel | author-link = Azriel Lévy\n | isbn = 3-540-08417-7\n | mr = 0533962\n | publisher = Springer\n | title = Basic Set Theory\n | year = 1979}}. Reprint Dover 2002, {{isbn|0-486-42079-5}}.\n*{{citation\n | last = Rogers | first = Hartley, Jr. | author-link = Hartley Rogers, Jr.\n | mr = 0224462\n | publisher = McGraw-Hill\n | title = Theory of Recursive Functions and Effective Computability\n | year = 1967}}.\n*{{citation\n | last = Simpson | first = Stephen G.\n | isbn = 3-540-64882-8\n | mr = 1723993\n | publisher = Springer\n | series = Perspectives in Mathematical Logic\n | title = Subsystems of Second Order Arithmetic\n | year = 1999}}.\n*{{citation\n | last = Soare | first = Robert I. | authorlink = Robert I. Soare\n | isbn = 3-540-15299-7\n | mr = 0882921\n | publisher = Springer\n | series = Perspectives in Mathematical Logic\n | title = Recursively Enumerable Sets and Degrees: A study of computable functions and computably generated sets\n | year = 1987}}.\n*{{citation\n | last = Truss | first = J.\n | editor1-last = Marek | editor1-first = V. Wiktor\n | editor2-last = Srebrny | editor2-first = Marian\n | editor3-last = Zarach | editor3-first = Andrzej\n | contribution = Some cases of König's lemma\n | doi = 10.1007/BFb0096907\n | mr = 0429557\n | pages = 273–284\n | publisher = Springer\n | series = Lecture Notes in Mathematics\n | title = Set theory and hierarchy theory: a memorial tribute to Andrzej Mostowski\n | volume = 537\n | year = 1976}}.\n\n==External links==\n* [http://plato.stanford.edu/entries/mathematics-constructive/ Stanford Encyclopedia of Philosophy: Constructive Mathematics]\n* The [[Mizar system|Mizar project]] has completely formalized and automatically checked the proof of a version of König's lemma in the file [http://mizar.org/JFM/Vol3/trees_2.html TREES_2].\n\n{{DEFAULTSORT:Koenigs Lemma}}\n[[Category:Lemmas]]\n[[Category:Articles containing proofs]]\n[[Category:Computability theory]]\n[[Category:Wellfoundedness]]\n[[Category:Axiom of choice]]\n[[Category:Theorems in graph theory]]\n[[Category:Infinite graphs]]\n[[Category:Constructivism (mathematics)]]"
    },
    {
      "title": "Mostowski collapse lemma",
      "url": "https://en.wikipedia.org/wiki/Mostowski_collapse_lemma",
      "text": "In [[mathematical logic]], the '''Mostowski collapse lemma''', also known as the '''Shepherdson&ndash;Mostowski collapse''', is a theorem of [[set theory]] introduced by {{harvs|txt|authorlink=Andrzej Mostowski|first=Andrzej|last= Mostowski|year=1949|loc=theorem 3}} and {{harvs|txt|authorlink=John C. Shepherdson|first=John|last= Shepherdson|year=1953}}.\n\n==Statement==\nSuppose that ''R'' is a binary relation on a class ''X'' such that\n*''R'' is [[binary relation#Relations over a set|set-like]]: ''R''<sup>−1</sup>[''x''] = {''y'' : ''y'' ''R'' ''x''} is a set for every ''x'',\n*''R'' is [[well-founded relation|well-founded]]: every nonempty subset ''S'' of ''X'' contains an ''R''-minimal element (i.e. an element ''x'' ∈ ''S'' such that ''R''<sup>−1</sup>[''x''] ∩ ''S'' is empty),\n*''R'' is [[axiom of extensionality|extensional]]: ''R''<sup>−1</sup>[''x''] ≠ ''R''<sup>−1</sup>[''y''] for every distinct elements ''x'' and ''y'' of ''X''\nThe Mostowski collapse lemma states that for any such ''R'' there exists a unique [[transitive set|transitive]] class (possibly [[proper class|proper]]) whose structure under the membership relation is isomorphic to (''X'', ''R''), and the isomorphism is unique. The isomorphism maps each element ''x'' of ''X'' to the set of images of elements ''y'' of ''X'' such that ''y R x'' (Jech 2003:69).\n\n==Generalizations==\nEvery well-founded set-like relation can be embedded into a well-founded set-like extensional relation. This implies the following variant of the Mostowski collapse lemma: every well-founded set-like relation is isomorphic to set-membership on a (non-unique, and not necessarily transitive) class.\n\nA mapping ''F'' such that ''F''(''x'') = {''F''(''y'') : ''y R x''} for all ''x'' in ''X'' can be defined for any well-founded set-like relation ''R'' on ''X'' by [[well-founded relation|well-founded recursion]]. It provides a [[homomorphism#Homomorphisms of relational structures|homomorphism]] of ''R'' onto a (non-unique, in general) transitive class. The homomorphism ''F'' is an isomorphism if and only if ''R'' is extensional.\n\nThe well-foundedness assumption of the Mostowski lemma can be alleviated or dropped in [[non-well-founded set theory|non-well-founded set theories]]. In Boffa's set theory, every set-like extensional relation is isomorphic to set-membership on a (non-unique) transitive class. In set theory with [[Aczel's anti-foundation axiom]], every set-like relation is [[bisimulation|bisimilar]] to set-membership on a unique transitive class, hence every bisimulation-minimal set-like relation is isomorphic to a unique transitive class.\n\n==Application==\nEvery set [[model theory|model]] of [[Zermelo–Fraenkel set theory|ZF]] is set-like and extensional. If the model is well-founded, then by the Mostowski collapse lemma it is isomorphic to a [[transitive model]] of ZF and such a transitive model is unique.\n\nSaying that the membership relation of some model of ZF is well-founded is stronger than saying that the [[axiom of regularity]] is true in the model. There exists a model ''M'' (assuming the consistency of ZF) whose domain has a subset ''A'' with no ''R''-minimal element, but this set ''A'' is not a \"set in the model\" (''A'' is not in the domain of the model, even though all of its members are). More precisely, for no such set ''A'' there exists ''x'' in ''M'' such that ''A'' = ''R''<sup>−1</sup>[''x'']. So ''M'' satisfies the axiom of regularity (it is \"internally\" well-founded) but it is not well-founded and the collapse lemma does not apply to it.\n\n== References ==\n\n* {{Citation | last1=Jech | first1=Thomas | author1-link=Thomas Jech | title=Set Theory | publisher=[[Springer-Verlag]] | location=Berlin, New York | edition=third millennium | series=Springer Monographs in Mathematics | isbn=978-3-540-44085-7 | year=2003}}\n*{{citation|title=An undecidable arithmetical statement\n|first=Andrzej|last= Mostowski | authorlink = Andrzej Mostowski\n|url=http://matwbn.icm.edu.pl/ksiazki/fm/fm36/fm36120.pdf \n|publisher= Institute of Mathematics Polish Academy of Sciences\n|journal= [[Fundamenta Mathematicae]]\n|year= 1949\n|volume =36\n|issue =1\n|pages= 143–164\n}}\n*{{citation|title=Inner models for set theory, Part III\n|first=John|last= Shepherdson | authorlink = John C. Shepherdson \n|publisher= Association for Symbolic Logic\n|journal= [[Journal of Symbolic Logic]]\n|year= 1953\n|volume =18\n|pages= 145–167\n}}\n\n[[Category:Wellfoundedness]]\n[[Category:Lemmas]]"
    },
    {
      "title": "Newman's lemma",
      "url": "https://en.wikipedia.org/wiki/Newman%27s_lemma",
      "text": "In [[mathematics]], in the theory of [[rewriting]] systems, '''Newman's [[Lemma (mathematics)|lemma]]''', also commonly called the '''diamond lemma''', states that a [[Abstract rewriting system#Termination and convergence|terminating]] (or strongly normalizing) [[abstract rewriting system]] (ARS), that is, one in which there are no infinite reduction sequences, is [[confluence (term rewriting)|confluent]] if it is [[Confluence (abstract rewriting)#Local confluence|locally confluent]]. In fact a terminating ARS is confluent [[precisely when]] it is locally confluent.<ref>[[Franz Baader]], [[Tobias Nipkow]], (1998) ''[https://books.google.com/books?id=N7BvXVUCQk8C&printsec=frontcover#v=onepage&q=%22Newman's%20lemma%22&f=false Term Rewriting and All That]'', Cambridge University Press {{isbn|0-521-77920-0}}</ref>\n\nEquivalently, for every [[binary relation]] with no decreasing infinite chains and satisfying a weak version of the diamond property, there is a unique [[minimal element]] in every [[connected component (graph theory)|connected component]] of the relation considered as a [[Graph (discrete mathematics)|graph]].\n\nToday, this is seen as a purely combinatorial result based on [[well-foundedness]] due to a proof of [[Gérard Huet]] in 1980.<ref>[[Gérard Huet]], \"Confluent Reductions: Abstract Properties and Applications to Term Rewriting Systems\", ''Journal of the ACM'' ([[JACM]]), October 1980, Volume '''27''', Issue 4, pp. 797 - 821.</ref> Newman's original proof was considerably more complicated.<ref>Harrison, p. 260, Paterson(1990), p. 354.</ref>\n\n==Diamond lemma==\nIn general, Newman's lemma can be seen as a [[combinatorics|combinatorial]] result about binary relations → on a set ''A'' (written backwards, so that ''a'' → ''b'' means that ''b'' is below ''a'') with the following two properties:\n\n* → is a [[well-founded relation]]: every non-empty subset ''X'' of ''A'' has a minimal element (an element ''a'' of ''X'' such that ''a'' → ''b'' for no ''b'' in ''X''). Equivalently, there is no infinite chain {{math|''a''<sub>0</sub> → ''a''<sub>1</sub> → ''a''<sub>2</sub> → ''a''<sub>3</sub> → ...}}. In the terminology of rewriting systems, → is terminating.\n* Every covering is bounded below.  That is, if an element ''a'' in ''A'' covers elements ''b'' and ''c'' in ''A'' in the sense that {{math|''a'' → ''b''}} and {{math|''a'' →  ''c''}}, then there is an element ''d'' in ''A'' such that {{math|''b'' {{overset|&lowast;|→}} ''d''}} and {{math|''c'' {{overset|&lowast;|→}} ''d''}}, where {{overset|&lowast;|→}} denotes the [[reflexive closure|reflexive]] [[transitive closure]] of →.  In the terminology of rewriting systems, → is locally confluent.\n\nIf the above two conditions hold, then the lemma states that → is confluent: whenever {{math|''a'' {{overset|&lowast;|→}} ''b''}} and {{math|''a'' {{overset|&lowast;|→}} ''c''}}, there is an element ''d'' such that {{math|''b'' {{overset|&lowast;|→}} ''d''}} and {{math|''c'' {{overset|&lowast;|→}} ''d''}}. In view of the termination of →, this implies that every connected component of → as a graph contains a unique minimal element ''a'', moreover {{math|''b'' {{overset|&lowast;|→}} ''a''}} for every element ''b'' of the component.<ref>Paul M. Cohn, (1980) ''Universal Algebra'', D. Reidel Publishing, {{isbn|90-277-1254-9}} (''See pp. 25-26'')</ref>\n\n==Notes==\n{{reflist}}\n\n==References==\n* [[M. H. A. Newman]]. ''On theories with a combinatorial definition of \"equivalence\"''. Annals of Mathematics, 43, Number 2, pages 223–243, 1942.\n* {{cite book |last1=Paterson |first1=Michael S.  |authorlink1= |last2= |first2= |authorlink2= |title=Automata, languages, and programming: 17th international colloquium |edition= |series=Lecture Notes in Computer Science |volume=443 |year=1990 |publisher=Springer |location= Warwick University, England|isbn=978-3-540-52826-5 |id= |url=https://books.google.com/books?id=u1-bLklS-9AC&printsec=frontcover#v=onepage&q=%22Newman's%20lemma%22&f=false}}\n\n=== Textbooks ===\n* ''Term Rewriting Systems'', Terese, Cambridge Tracts in Theoretical Computer Science, 2003. [https://web.archive.org/web/20081224172353/http://www.cs.vu.nl/~terese/ (book weblink)]\n* ''Term Rewriting and All That'', Franz Baader and Tobias Nipkow, Cambridge University Press, 1998 [https://web.archive.org/web/20090601161732/http://www.forsoft.de/~nipkow/TRaAT/ (book weblink)]\n* John Harrison, ''Handbook of Practical Logic and Automated Reasoning'', Cambridge University Press, 2009, {{isbn|978-0-521-89957-4}}, chapter 4 \"Equality\".\n\n==External links==\n* {{planetmath|urlname=diamondlemma|title=Diamond lemma}}\n* [http://www.phil.uu.nl/~oostrom/publication/pdf/newmansproof.pdf PDF on original proof], {{webarchive|url=https://web.archive.org/web/20170706084416/http://www.phil.uu.nl/~oostrom/publication/pdf/newmansproof.pdf |date=July 6, 2017 |6 July 2017}}\n\n[[Category:Wellfoundedness]]\n[[Category:Lemmas]]\n[[Category:Rewriting systems]]"
    },
    {
      "title": "Noetherian topological space",
      "url": "https://en.wikipedia.org/wiki/Noetherian_topological_space",
      "text": "In [[mathematics]], a '''Noetherian topological space''', named for [[Emmy Noether]], is a [[topological space]] in which closed subsets satisfy the [[descending chain condition]].  Equivalently, we could say that the open subsets satisfy the [[ascending chain condition]], since they are the complements of the closed subsets.  It can also be shown to be equivalent that every open subset of such a space is [[compact space|compact]], and in fact the seemingly stronger statement that ''every'' subset is compact.\n\n== Definition ==\nA [[topological space]] <math>X</math> is called '''Noetherian''' if it satisfies the [[descending chain condition]] for [[closed subset]]s: for any [[sequence]]  \n\n:<math> Y_1 \\supseteq Y_2 \\supseteq \\cdots </math>\n\nof closed subsets <math>Y_i</math> of <math>X</math>, there is an [[integer]] <math>m</math> such that  <math>Y_m=Y_{m+1}=\\cdots.</math> \n\n== Relation to compactness ==\nThe Noetherian condition can be seen as a strong [[compact space|compactness]] condition:\n*Every Noetherian topological space is compact.\n*A topological space <math>X</math> is Noetherian if and only if every [[Subspace topology|subspace]] of <math>X</math> is compact. (i.e. <math>X</math> is hereditarily compact).\n\n== From algebraic geometry ==\n\nMany examples of Noetherian topological spaces come from [[algebraic geometry]], where for the [[Zariski topology]] an [[irreducible set]] has the intuitive property that any closed proper subset has smaller dimension. Since dimension can only 'jump down' a finite number of times, and [[algebraic set]]s are made up of finite unions of irreducible sets, descending chains of Zariski closed sets must eventually be constant.\n\nA more algebraic way to see this is that the associated [[ideal (ring theory)|ideals]] defining algebraic sets must satisfy the [[ascending chain condition]]. That follows because the rings of algebraic geometry, in the classical sense, are [[Noetherian ring]]s. This class of examples therefore also explains the name. \n\nIf ''R'' is a commutative Noetherian ring, then  Spec(''R''), the [[prime spectrum]] of ''R'', is a Noetherian topological space. More generally, a [[Noetherian scheme]] is a Noetherian topological space. The converse does not hold, since Spec(''R'') of a one-dimensional valuation domain ''R'' consists of exactly two points and therefore is Noetherian, but there are examples of such rings which are not Noetherian.\n\n== Example ==\nThe space  <math>\\mathbb{A}^n_k</math> (affine <math>n</math>-space over a [[Field (mathematics)|field]] <math>k</math>) under the [[Zariski topology]] is an example of a Noetherian topological space. By properties of the [[right ideal|ideal]] of a subset of  <math>\\mathbb{A}^n_k</math>, we know that if\n\n:<math>Y_1 \\supseteq Y_2 \\supseteq Y_3 \\supseteq \\cdots</math>\n\nis a descending chain of Zariski-closed subsets, then\n\n:<math>I(Y_1) \\subseteq I(Y_2) \\subseteq I(Y_3) \\subseteq \\cdots</math>\n\nis an ascending chain of ideals of  <math>k[x_1,\\ldots,x_n].</math>  Since <math>k[x_1,\\ldots,x_n]</math> is a Noetherian ring, there exists an integer <math>m</math> such that  \n\n:<math>I(Y_m)=I(Y_{m+1})=I(Y_{m + 2})=\\cdots.</math>\n\nSince <math>V(I(Y))</math> is the closure of ''Y'' for all ''Y'', <math>V(I(Y_i))=Y_i</math> for all <math>i.</math>  Hence\n\n:<math>Y_m=Y_{m+1}=Y_{m + 2}=\\cdots</math> as required.\n\n==References==\n*{{Hartshorne AG}}\n\n{{PlanetMath attribution|id=3465|title=Noetherian topological space}}\n\n[[Category:Algebraic geometry]]\n[[Category:Properties of topological spaces]]\n[[Category:Scheme theory]]\n[[Category:Wellfoundedness]]"
    },
    {
      "title": "Non-well-founded set theory",
      "url": "https://en.wikipedia.org/wiki/Non-well-founded_set_theory",
      "text": "'''Non-well-founded set theories''' are variants of [[axiomatic set theory]] that allow sets to contain themselves and otherwise violate the rule of [[well-foundedness]]. In non-well-founded set theories, the [[axiom of regularity|foundation axiom]] of [[ZFC]] is replaced by axioms implying its negation.\n\nThe study of non-well-founded sets was initiated by [[Dmitry Mirimanoff]] in a series of papers between 1917 and 1920, in which he formulated the distinction between well-founded and non-well-founded sets; he did not regard well-foundedness as an [[axiom]]. Although a number of axiomatic systems of non-well-founded sets were proposed afterwards, they did not find much in the way of applications until [[Peter Aczel]]’s [[hyperset theory]] in 1988.{{sfnp|Pakkan|Akman|1994|loc=[http://tinf2.vub.ac.be/~dvermeir/mirrors/www.cs.bilkent.edu.tr/%257Eakman/jour-papers/air/node8.html section link]}}{{sfnp|Rathjen|2004|p=}}{{sfnp|Sangiorgi|2011|pp=17–19, 26}}\nThe theory of non-well-founded sets has been applied in the [[logic]]al [[model (abstract)|modelling]] of non-terminating [[Computing|computational]] processes in computer science ([[process algebra]] and [[final semantics]]), [[linguistics]] and [[natural language]] [[semantics]] ([[situation theory]]), philosophy (work on the [[Liar Paradox]]), and in a different setting, [[non-standard analysis]].{{sfnp|Ballard|Hrbáček|1992|p=}}\n\n== Details ==\nIn 1917, Dmitry Mirimanoff introduced{{sfnp|Levy|2002|p=68}}{{sfnp|Hallett|1986|p=[https://books.google.com/books?id=TM3AKPYdQVgC&pg=PA186 186]}}{{sfnp|Aczel|1988|p=105}}{{sfnp|Mirimanoff|1917|p=}} the concept of [[well-founded set|well-foundedness]] of a set:\n\n:A set, x<sub>0</sub>, is well-founded if it has no infinite descending membership sequence <math> \\cdots \\in x_2 \\in x_1 \\in x_0. </math>\n\nIn ZFC, there is no infinite descending ∈-sequence by the [[axiom of regularity]]. In fact, the axiom of regularity is often called the ''foundation axiom'' since it can be proved within ZFC<sup>−</sup> (that is, ZFC without the axiom of regularity) that well-foundedness implies regularity. In variants of ZFC without the [[axiom of regularity]], the possibility of non-well-founded sets with set-like ∈-chains arises. For example, a set ''A'' such that ''A'' ∈ ''A'' is non-well-founded.\n\nAlthough Mirimanoff also introduced a notion of isomorphism between possibly non-well-founded sets, he considered neither an axiom of foundation nor of anti-foundation.{{sfnp|Aczel|1988|p=105}} In 1926, [[Paul Finsler]] introduced the first axiom that allowed non-well-founded sets. After Zermelo adopted Foundation into his own system in 1930 (from previous work of [[John von Neumann|von Neumann]] 1925–1929) interest in non-well-founded sets waned for decades.{{sfnp|Aczel|1988|p=107}} An early non-well-founded set theory was [[Willard Van Orman Quine]]’s [[New Foundations]], although it is not merely ZF with a replacement for Foundation.\n\nSeveral proofs of the independence of Foundation from the rest of ZF were published in 1950s particularly by [[Paul Bernays]] (1954), following an announcement of the result in earlier paper of his from 1941, and by [[Ernst Specker]] who gave a different proof in his [[Habilitationsschrift]] of 1951, proof which was published in 1957. Then in 1957 [[Rieger's theorem]] was published, which gave a general method for such proof to be carried out, rekindling some interest in non-well-founded axiomatic systems.{{sfnp|Aczel|1988|pp=107–8}} The next axiom proposal came in a 1960 congress talk of [[Dana Scott]] (never published as a paper), proposing an alternative axiom now called [[Anti-Foundation Axiom|SAFA]].{{sfnp|Aczel|1988|pp=108–9}} Another axiom proposed in the late 1960s was [[Maurice Boffa]]'s axiom of [[superuniversality]], described by Aczel as the highpoint of research of its decade.{{sfnp|Aczel|1988|p=110}} Boffa's idea was to make foundation fail as badly as it can (or rather, as extensionality permits): Boffa's axiom implies that every [[extensionality|extensional]] [[binary relation|set-like]] relation is isomorphic to the elementhood predicate on a transitive class.\n\nA more recent approach to non-well-founded set theory, pioneered by M. Forti and F. Honsell in the 1980s, borrows from computer science the concept of a [[bisimulation]]. Bisimilar sets are considered indistinguishable and thus equal, which leads to a strengthening of the [[axiom of extensionality]]. In this context, axioms contradicting the axiom of regularity are known as '''anti-foundation axioms''', and a set that is not necessarily well-founded is called a '''hyperset'''.\n\nFour mutually [[Independence (mathematical logic)|independent]] anti-foundation axioms are well-known, sometimes abbreviated by the first letter in the following list:\n# '''A'''FA (\"Anti-Foundation Axiom\") – due to M. Forti and F. Honsell (this is also known as [[Aczel's anti-foundation axiom]]);\n# '''S'''AFA (\"Scott’s AFA\") – due to [[Dana Scott]],\n# '''F'''AFA (\"Finsler’s AFA\") – due to [[Paul Finsler]],\n# '''B'''AFA (\"Boffa’s AFA\") – due to [[Maurice Boffa]].\nThey essentially correspond to four different notions of equality for non-well-founded sets. The first of these, AFA, is based on [[accessible pointed graph]]s (apg) and states that two hypersets are equal if and only if they can be pictured by the same apg.  Within this framework, it can be shown that the so-called [[Quine atom]], formally defined by Q={Q}, exists and is unique.\n\nEach of the axioms given above extends the universe of the previous, so that: [[Von Neumann universe|V]] ⊆ A ⊆ S ⊆ F ⊆ B. In the Boffa universe, the distinct Quine atoms form a proper class.{{sfnp|Nitta|Okada|Tsouvaras|2003}}\n\nIt is worth emphasizing that hyperset theory is an extension of classical set theory rather than a replacement: the well-founded sets within a hyperset domain conform to classical set theory.\n\n== Applications ==\n{{expand section|date=November 2012}}\nAczel’s hypersets were extensively used by [[Jon Barwise]] and [[John Etchemendy]] in their 1987 book ''The Liar'', on the [[liar's paradox]]; The book is also good introduction to the topic of non-well-founded sets.\n\nBoffa’s superuniversality axiom has found application as a basis for axiomatic [[nonstandard analysis]].{{sfnp|Kanovei|Reeken|2004|p=303}}\n\n== See also ==\n* [[Alternative set theory]]\n* [[Universal set]]\n* [[Turtles all the way down]]\n\n== Notes ==\n{{reflist|30em}}\n\n== References ==\n* {{citation |last=Aczel |first=Peter |title=Non-well-founded sets |series= CSLI Lecture Notes |volume=14 |publisher= Stanford University, Center for the Study of Language and Information |place=Stanford, CA |year=1988 |pages=xx+137 | isbn=0-937073-22-9 |url=http://www.irafs.org/courses/materials/aczel_set_theory.pdf|postscript=. |mr=0940014}}\n* {{citation |first1=David |last1=Ballard |first2=Karel |last2=Hrbáček |title=Standard foundations for nonstandard analysis |journal=Journal of Symbolic Logic |volume=57 |year=1992 |pages=741–748 |postscript=. |jstor=2275304 |issue=2|doi=10.2307/2275304}}\n* {{citation |last1=Barwise |first1=Jon |last2=Etchemendy |first2=John |year=1987 |title=The Liar: An Essay on Truth and Circularity |publisher=Oxford University Press |url=https://books.google.com/books?id=L3M8DwAAQBAJ |isbn=9780195059441}}\n* {{citation |first1=Jon |last1=Barwise |first2=Lawrence S.  |last2=Moss |title=Vicious circles. On the mathematics of non-wellfounded phenomena |series=CSLI Lecture Notes |volume=60 |publisher=CSLI Publications |year=1996 |isbn=1-57586-009-0 }}\n* {{citation |last=Boffa. |first=M. |title=Les enesembles extraordinaires |journal=Bulletin de la Societe Mathematique de Belgique |volume=XX |pages=3–15 |year=1968 |zbl=0179.01602}}\n* {{citation |last=Boffa |first=M. |title=Forcing et négation de l'axiome de Fondement |journal=Memoire Acad. Sci. Belg. |volume=XL |issue=7 |year=1972 |zbl=0286.02068}}\n* {{citation |last=Devlin |first=Keith |author1link=Keith Devlin |title=The Joy of Sets: Fundamentals of Contemporary Set Theory |year=1993 |publisher=Springer |isbn=978-0-387-94094-6|edition=2nd |chapter=§7. Non-Well-Founded Set Theory }}\n* {{citation |last=Finsler |first=P. |title=Über die Grundlagen der Mengenlehre. I: Die Mengen und ihre Axiome |journal=Math. Zeitschrift |volume=25 |year=1926 |pages=683–713 |jfm=52.0192.01|doi=10.1007/BF01283862 }}; translation in {{cite book |last1=Finsler |first1=Paul |last2=Booth |first2=David |title=Finsler Set Theory: Platonism and Circularity : Translation of Paul Finsler's Papers on Set Theory with Introductory Comments |year=1996 |publisher=Springer |isbn=978-3-7643-5400-8}}\n* {{citation |first=Michael |last=Hallett |title=Cantorian set theory and limitation of size |publisher=Oxford University Press |year=1986 |postscript=. |url=https://books.google.com/books?id=TM3AKPYdQVgC |isbn=9780198532835}}\n* {{citation| last1=Kanovei |first1=Vladimir |author1link=Vladimir Kanovei |last2=Reeken |first2=Michael |title=Nonstandard Analysis, Axiomatically|year=2004  |publisher=Springer |isbn=978-3-540-22243-9}}\n* {{citation |first=Azriel |last=Levy |title=Basic set theory |publisher=Dover Publications |origyear=2002 |postscript=. |url=https://books.google.com/books?id=zbGjAQAAQBAJ |year=2012 |isbn=9780486150734}}\n* {{citation |last1=Mirimanoff |first1=D. |title=Les antinomies de Russell et de Burali-Forti et le probleme fondamental de la theorie des ensembles |year=1917 |journal=L'Enseignement Mathématique |volume=19 |pages=37–52 |postscript=. |jfm=46.0306.01}}\n* {{citation |last1=Nitta |last2=Okada |last3=Tzouvaras|title=Classification of non-well-founded sets and an application|url=http://users.auth.gr/~tzouvara/Texfiles.htm/non-well.pdf|year=2003}}\n* {{citation |last=Pakkan |first=M. J. |last2=Akman |first2=V. |author2-link=Varol Akman |doi=10.1007/BF00849061 |title=Issues in commonsense set theory |journal=Artificial Intelligence Review |volume=8 |issue=4 |pages=279–308 |year=1994–1995}}\n* {{citation|editor1-first=Godehard |editor1-last=Link|title=One Hundred Years of Russell ́s Paradox: Mathematics, Logic, Philosophy|year=2004|publisher=Walter de Gruyter|isbn=978-3-11-019968-0 |chapter=Predicativity, Circularity, and Anti-Foundation |last=Rathjen |first=M. | chapter-url=http://www1.maths.leeds.ac.uk/~rathjen/russelle.pdf}}\n* {{citation |first=Davide |last=Sangiorgi | year=2011 |chapter=Origins of bisimulation and coinduction | editor1-first = Davide | editor1-last = Sangiorgi | editor2-first = Jan |editor2-last=Rutten |title=Advanced Topics in Bisimulation and Coinduction |publisher=Cambridge University Press| isbn=978-1-107-00497-9}}\n* {{citation |last=Scott |first=Dana |title=A different kind of model for set theory |work=Unpublished paper, talk given at the 1960 Stanford Congress of Logic, Methodology and Philosophy of Science |year=1960}}\n\n== Further reading ==\n* {{cite web |last=Moss |first=Lawrence S. |url=http://plato.stanford.edu/entries/nonwellfounded-set-theory/ | title=Non-wellfounded Set Theory |work=Stanford Encyclopedia of Philosophy }}\n\n== External links ==\n* [[Metamath]] page on the [http://us.metamath.org/mpegif/axreg.html axiom of Regularity.] Scroll to the bottom to see how few Metamath theorems invoke this axiom.\n\n[[Category:Systems of set theory]]\n[[Category:Wellfoundedness]]\n[[Category:Self-reference]]"
    },
    {
      "title": "Robertson–Seymour theorem",
      "url": "https://en.wikipedia.org/wiki/Robertson%E2%80%93Seymour_theorem",
      "text": "In [[graph theory]], the '''Robertson–Seymour theorem''' (also called the '''graph minor theorem'''<ref>{{harvtxt|Bienstock|Langston|1995}}.</ref>) states that the [[undirected graph]]s, [[partial order|partially ordered]] by the [[minor (graph theory)|graph minor]] relationship, form a [[well-quasi-ordering]].<ref>{{harvtxt|Robertson|Seymour|2004}}.</ref> Equivalently, every family of graphs that is closed under minors can be defined by a finite set of [[forbidden minor]]s, in the same way that [[Wagner's theorem]] characterizes the [[planar graph]]s as being the graphs that do not have the [[complete graph]] ''K''<sub>5</sub> or the [[complete bipartite graph]] ''K''<sub>3,3</sub> as minors.\n\nThe Robertson–Seymour theorem is named after mathematicians [[Neil Robertson (mathematician)|Neil Robertson]] and [[Paul Seymour (mathematician)|Paul D. Seymour]], who proved it in a series of twenty papers spanning over 500 pages from 1983 to 2004.<ref>{{harvs|last1=Robertson|last2=Seymour|year=1983|year2=2004|txt}}; {{harvtxt|Diestel|2005|p=333}}.</ref> Before its proof, the statement of the theorem was known as '''Wagner's conjecture''' after the German mathematician [[Klaus Wagner (mathematician)|Klaus Wagner]], although Wagner said he never conjectured it.<ref>{{harvtxt|Diestel|2005|p=355}}.</ref>\n\nA weaker result for [[tree (graph theory)|trees]] is implied by [[Kruskal's tree theorem]], which was conjectured in 1937 by Andrew Vázsonyi and proved in 1960 independently by [[Joseph Kruskal]] and S. Tarkowski.<ref>{{harvtxt|Diestel|2005|pp=335–336}}; {{harvtxt|Lovász|2005}}, Section 3.3, pp. 78–79.</ref>\n\n==Statement==\nA [[minor (graph theory)|minor]] of an [[undirected graph]] ''G'' is any graph that may be obtained from ''G'' by a sequence of zero or more contractions of edges of ''G'' and deletions of edges and vertices of ''G''. The minor relationship forms a [[partial order]] on the set of all distinct finite undirected graphs, as it obeys the three axioms of partial orders: it is [[Reflexive relation|reflexive]] (every graph is a minor of itself), [[Transitive relation|transitive]] (a minor of a minor of ''G'' is itself a minor of ''G''), and [[Antisymmetric relation|antisymmetric]] (if two graphs ''G'' and ''H'' are minors of each other, then they must be [[graph isomorphism|isomorphic]]). However, if graphs that are isomorphic may nonetheless be considered as distinct objects, then the minor ordering on graphs forms a [[preorder]], a relation that is reflexive and transitive but not necessarily antisymmetric.<ref>E.g., see {{harvtxt|Bienstock|Langston|1995}}, Section 2, \"well-quasi-orders\".</ref>\n\nA preorder is said to form a [[well-quasi-ordering]] if it contains neither an [[infinite descending chain]] nor an infinite [[antichain]].<ref>{{harvtxt|Diestel|2005|p=334}}.</ref> For instance, the usual ordering on the non-negative integers is a well-quasi-ordering, but the same ordering on the set of all integers is not, because it contains the infinite descending chain 0,&nbsp;&minus;1,&nbsp;&minus;2,&nbsp;&minus;3...\n\nThe Robertson–Seymour theorem states that finite undirected graphs and graph minors form a well-quasi-ordering. The graph minor relationship does not contain any infinite descending chain, because each contraction or deletion reduces the number of edges and vertices of the graph (a non-negative integer).<ref name=\"l05-78\"/> The nontrivial part of the theorem is that there are no infinite antichains, infinite sets of graphs that are all unrelated to each other by the minor ordering. If ''S'' is a set of graphs, and ''M'' is a subset of ''S'' containing one representative graph for each equivalence class of [[minimal element]]s (graphs that belong to ''S'' but for which no proper minor belongs to ''S''), then ''M'' forms an antichain; therefore, an equivalent way of stating the theorem is that, in any infinite set ''S'' of graphs, there must be only a finite number of non-isomorphic minimal elements.\n\nAnother equivalent form of the theorem is that, in any infinite set ''S'' of graphs, there must be a pair of graphs one of which is a minor of the other.<ref name=\"l05-78\">{{harvtxt|Lovász|2005|p=78}}.</ref> The statement that every infinite set has finitely many minimal elements implies this form of the theorem, for if there are only finitely many minimal elements, then each of the remaining graphs must belong to a pair of this type with one of the minimal elements. And in the other direction, this form of the theorem implies the statement that there can be no infinite antichains, because an infinite antichain is a set that does not contain any pair related by the minor relation.\n\n==Forbidden minor characterizations==\nA family ''F'' of graphs is said to be [[Closure (mathematics)|closed]] under the operation of taking minors if every minor of a graph in ''F'' also belongs to ''F''. If ''F'' is a minor-closed family, then let ''S'' be the set of graphs that are not in ''F'' (the [[complement (set theory)|complement]] of ''F''). According to the Robertson–Seymour theorem, there exists a finite set ''H'' of minimal elements in ''S''. These minimal elements form a [[forbidden graph characterization]] of ''F'': the graphs in ''F'' are exactly the graphs that do not have any graph in ''H'' as a minor.<ref>{{harvtxt|Bienstock|Langston|1995}}, Corollary 2.1.1; {{harvtxt|Lovász|2005}}, Theorem 4, p.&nbsp;78.</ref> The members of ''H'' are called the '''excluded minors''' (or '''forbidden minors''', or '''minor-minimal obstructions''') for the family ''F''.\n\nFor example, the [[planar graph]]s are closed under taking minors: contracting an edge in a planar graph, or removing edges or vertices from the graph, cannot destroy its planarity. Therefore, the planar graphs have a forbidden minor characterization, which in this case is given by [[Wagner's theorem]]: the set ''H'' of minor-minimal nonplanar graphs contains exactly two graphs, the [[complete graph]] ''K''<sub>5</sub> and the [[complete bipartite graph]] ''K''<sub>3,3</sub>, and the planar graphs are exactly the graphs that do not have a minor in the set {''K''<sub>5</sub>,&nbsp;''K''<sub>3,3</sub>}.\n\nThe existence of forbidden minor characterizations for all minor-closed graph families is an equivalent way of stating the Robertson–Seymour theorem. For, suppose that every minor-closed family ''F'' has a finite set ''H'' of minimal forbidden minors, and let ''S'' be any infinite set of graphs. Define ''F'' from ''S'' as the family of graphs that do not have a minor in ''S''. Then ''F'' is minor-closed and has a finite set ''H'' of minimal forbidden minors. Let ''C'' be the complement of ''F''. ''S'' is a subset of ''C'' since ''S'' and ''F'' are disjoint, and ''H'' are the minimal graphs in ''C''. Consider a graph ''G'' in ''H''. ''G'' cannot have a proper minor in ''S'' since ''G'' is minimal in ''C''. At the same time, ''G'' must have a minor in ''S'', since otherwise ''G'' would be an element in ''F''. Therefore, ''G'' is an element in ''S'', i.e., ''H'' is a subset of ''S'', and all other graphs in ''S'' have a minor among the graphs in ''H'', so ''H'' is the finite set of minimal elements of ''S''.\n\nFor the other implication, assume that every set of graphs has a finite subset of minimal graphs and let a minor-closed set ''F'' be given. We want to find a set ''H'' of graphs such that a graph is in ''F'' if and only if it does not have a minor in ''H''. Let ''E'' be the graphs which are not minors of any graph in ''F'', and let ''H'' be the finite set of minimal graphs in ''E''. Now, let an arbitrary graph ''G'' be given. Assume first that ''G'' is in ''F''. ''G'' cannot have a minor in ''H'' since ''G'' is in ''F'' and ''H'' is a subset of ''E''. Now assume that ''G'' is not in ''F''. Then ''G'' is not a minor of any graph in ''F'', since ''F'' is minor-closed. Therefore, ''G'' is in ''E'', so ''G'' has a minor in ''H''.\n\n==Examples of minor-closed families==\n{{main|Forbidden graph characterization}}\nThe following sets of finite graphs are minor-closed, and therefore (by the Robertson–Seymour theorem) have forbidden minor characterizations:\n*[[forest (graph theory)|forests]], linear forests ([[disjoint union]]s of [[path graph]]s), [[pseudoforest]]s, and [[cactus graph]]s;\n*[[planar graph]]s, [[outerplanar graph]]s, [[apex graph]]s (formed by adding a single vertex to a planar graph), [[toroidal graph]]s, and the graphs that can be [[graph embedding|embedded]] on any fixed two-dimensional [[manifold]];<ref name=\"l05-76-77\">{{harvtxt|Lovász|2005|pp=76–77}}.</ref>\n*graphs that are [[linkless embedding|linklessly embeddable]] in Euclidean 3-space, and graphs that are [[knot (mathematics)|knotlessly]] embeddable in Euclidean 3-space;<ref name=\"l05-76-77\"/>\n*graphs with a [[feedback vertex set]] of size bounded by some fixed constant; graphs with [[Colin de Verdière graph invariant]] bounded by some fixed constant; graphs with [[treewidth]], [[pathwidth]], or [[branchwidth]] bounded by some fixed constant.\n\n==Obstruction sets==\n[[File:Petersen family.svg|thumb|The [[Petersen family]], the obstruction set for linkless embedding.]]\nSome examples of finite obstruction sets were already known for specific classes of graphs before the Robertson–Seymour theorem was proved. For example, the obstruction for the set of all forests is the [[Graph (discrete mathematics)|loop]] graph (or, if one restricts to [[Graph (discrete mathematics)|simple graph]]s, the cycle with three vertices). This means that a graph is a forest if and only if none of its minors is the loop (or, the cycle with three vertices, respectively).  The sole obstruction for the set of paths is the tree with four vertices, one of which has degree 3. In these cases, the obstruction set contains a single element, but in general this is not the case. [[Wagner's theorem]] states that a graph is planar if and only if it has neither ''K''<sub>5</sub> nor ''K''<sub>3,3</sub> as a minor. In other words, the set {''K''<sub>5</sub>,&nbsp;''K''<sub>3,3</sub>} is an obstruction set for the set of all planar graphs, and in fact the unique minimal obstruction set.  A similar theorem states that ''K''<sub>4</sub> and ''K''<sub>2,3</sub> are the forbidden minors for the set of outerplanar graphs.\n\nAlthough the Robertson–Seymour theorem extends these results to arbitrary minor-closed graph families, it is not a complete substitute for these results, because it does not provide an explicit description of the obstruction set for any family. For example, it tells us that the set of [[toroidal graph]]s has a finite obstruction set, but it does not provide any such set. The complete set of forbidden minors for toroidal graphs remains unknown, but contains at least 16000 graphs.<ref>{{harvtxt|Chambers|2002}}.</ref>\n\n==Polynomial time recognition==\nThe Robertson–Seymour theorem has an important consequence in computational complexity, due to the proof by Robertson and Seymour that, for each fixed graph ''G'', there is a [[polynomial time]] algorithm for testing whether larger graphs have ''G'' as a minor. The running time of this algorithm can be expressed as a [[cubic polynomial]] in the size of the larger graph (although there is a constant factor in this polynomial that depends superpolynomially on the size of ''G''), which has been improved to quadratic time by Kawarabayashi, Kobayashi, and Reed.<ref>{{harvtxt|Kawarabayashi|Kobayashi|Reed|2012}}</ref> As a result, for every minor-closed family ''F'', there is polynomial time algorithm for testing whether a graph belongs to ''F'': simply check, for each of the forbidden minors for ''F'', whether the given graph contains that forbidden minor.<ref>{{harvtxt|Robertson|Seymour|1995}}; {{harvtxt|Bienstock|Langston|1995}}, Theorem 2.1.4 and Corollary 2.1.5; {{harvtxt|Lovász|2005}}, Theorem 11, p. 83.</ref>\n\nHowever, this method requires a specific finite obstruction set to work, and the theorem does not provide one. The theorem proves that such a finite obstruction set exists, and therefore the problem is polynomial because of the above algorithm. However, the algorithm can be used in practice only if such a finite obstruction set is provided. As a result, the theorem proves that the problem can be solved in polynomial time, but does not provide a concrete polynomial-time algorithm for solving it. Such proofs of polynomiality are [[non-constructive]]: they prove polynomiality of problems without providing an explicit polynomial-time algorithm.<ref>{{harvtxt|Fellows|Langston|1988}}; {{harvtxt|Bienstock|Langston|1995}}, Section 6.</ref> In many specific cases, checking whether a graph is in a given minor-closed family can be done more efficiently: for example, checking whether a graph is planar can be done in linear time.\n\n==Fixed-parameter tractability==\nFor [[graph invariant]]s with the property that, for each ''k'', the graphs with invariant at most ''k'' are minor-closed, the same method applies. For instance, by this result, treewidth, branchwidth, and pathwidth, vertex cover, and the minimum genus of an embedding are all amenable to this approach, and for any fixed ''k'' there is a polynomial time algorithm for testing whether these invariants are at most ''k'', in which the exponent in the running time of the algorithm does not depend on ''k''. A problem with this property, that it can be solved in polynomial time for any fixed ''k'' with an exponent that does not depend on ''k'', is known as [[fixed-parameter tractable]].\n\nHowever, this method does not directly provide a single fixed-parameter-tractable algorithm for computing the parameter value for a given graph with unknown ''k'', because of the difficulty of determining the set of forbidden minors. Additionally, the large constant factors involved in these results make them highly impractical. Therefore, the development of explicit fixed-parameter algorithms for these problems, with improved dependence on ''k'', has continued to be an important line of research.\n\n==Finite form of the graph minor theorem==\n{{harvtxt|Friedman|Robertson|Seymour|1987}} showed that the following theorem exhibits the [[Independence (mathematical logic)|independence]] phenomenon by being ''unprovable'' in various formal systems that are much stronger than [[Peano arithmetic]], yet being ''provable'' in systems much weaker than [[ZFC]]:\n \n:'''Theorem''': For every positive integer ''n'', there is an integer ''m'' so large that if ''G''<sub>1</sub>, ..., ''G''<sub>''m''</sub> is a sequence of finite undirected graphs, \n:where each ''G''<sub>''i''</sub> has size at most ''n''+''i'', then ''G''<sub>''j''</sub> ≤ ''G''<sub>''k''</sub> for some ''j'' < ''k''.\n\n(Here, the ''size'' of a graph is the total number of its vertices and edges, and ≤ denotes the minor ordering.)\n\n==See also==\n*[[Graph structure theorem]]\n\n==Notes==\n{{reflist|2}}\n\n==References==\n*{{citation\n | last1 = Bienstock | first1 = Daniel\n | last2 = Langston | first2 = Michael A. | author2-link = Michael Langston\n | contribution = Algorithmic implications of the graph minor theorem\n | doi = 10.1016/S0927-0507(05)80125-2\n | pages = 481–502\n | series = Handbooks in Operations Research and Management Science\n | title = Network Models\n | url = http://www.cs.utk.edu/~langston/courses/cs594-fall2003/BL.pdf\n | volume = 7\n | year = 1995}}.\n*{{citation\n | last = Chambers | first = J.\n | publisher = Department of Computer Science, University of Victoria\n | series = M.Sc. thesis\n | title = Hunting for torus obstructions\n | year = 2002}}.\n*{{citation\n | last = Diestel | first = Reinhard\n | contribution = Minors, Trees, and WQO\n | edition = Electronic Edition 2005\n | pages = 326–367\n | publisher = Springer\n | title = Graph Theory\n | url = http://www.math.uni-hamburg.de/home/diestel/books/graph.theory/preview/Ch12.pdf\n | year = 2005}}.\n*{{citation\n | last1 = Fellows | first1 = Michael R. | author1-link = Michael Fellows\n | last2 = Langston | first2 = Michael A. | author2-link = Michael Langston\n | doi = 10.1145/44483.44491\n | issue = 3\n | journal = [[Journal of the ACM]]\n | pages = 727–739\n | title = Nonconstructive tools for proving polynomial-time decidability\n | volume = 35\n | year = 1988}}.\n*{{citation\n | last1 = Friedman | first1 = Harvey | author1-link = Harvey Friedman\n | last2 = Robertson | first2 = Neil | author2-link = Neil Robertson (mathematician)\n | last3 = Seymour | first3 = Paul | author3-link = Paul Seymour (mathematician)\n | contribution = The metamathematics of the graph minor theorem\n | editor-last = Simpson | editor-first = S.\n | pages = 229–261\n | publisher = [[American Mathematical Society]]\n | series = Contemporary Mathematics\n | title = Logic and Combinatorics\n | volume = 65\n | year = 1987}}.\n*{{citation\n | last1 = Kawarabayashi |first1 = Ken-ichi | author1-link = Ken-ichi Kawarabayashi\n | last2 = Kobayashi | first2 = Yusuke\n | last3 = Reed | first3 = Bruce | author3-link = Bruce Reed (mathematician)\n | title = The disjoint paths problem in quadratic time\n | journal = Journal of Combinatorial Theory, Series B\n | volume = 102\n | issue  = 2\n | year = 2012\n | pages = 424–435\n | doi = 10.1016/j.jctb.2011.07.004\n | url = http://research.nii.ac.jp/~k_keniti/quaddp1.pdf}}.\n*{{citation\n | last = Lovász | first = László | author-link = László Lovász\n | doi = 10.1090/S0273-0979-05-01088-8\n | issue = 1\n | journal = Bulletin of the American Mathematical Society |series=New Series\n | pages = 75–86\n | title = Graph Minor Theory\n | volume = 43\n | year = 2005}}.\n*{{citation\n | last1 = Robertson | first1 = Neil | author1-link = Neil Robertson (mathematician)\n | last2 = Seymour | first2 = Paul | author2-link = Paul Seymour (mathematician)\n | doi = 10.1016/0095-8956(83)90079-5\n | issue = 1\n | journal = Journal of Combinatorial Theory, Series B\n | pages = 39–61\n | title = Graph Minors. I. Excluding a forest\n | volume = 35\n | year = 1983}}.\n*{{citation\n | last1 = Robertson | first1 = Neil | author1-link = Neil Robertson (mathematician)\n | last2 = Seymour | first2 = Paul | author2-link = Paul Seymour (mathematician)\n | doi = 10.1006/jctb.1995.1006\n | issue = 1\n | journal = Journal of Combinatorial Theory, Series B\n | pages = 65–110\n | title = Graph Minors. XIII. The disjoint paths problem\n | volume = 63\n | year = 1995}}.\n*{{citation\n | last1 = Robertson | first1 = Neil | author1-link = Neil Robertson (mathematician)\n | last2 = Seymour | first2 = Paul | author2-link = Paul Seymour (mathematician)\n | doi = 10.1016/j.jctb.2004.08.001\n | issue = 2\n | journal = Journal of Combinatorial Theory, Series B\n | pages = 325–357\n | title = Graph Minors. XX. Wagner's conjecture\n | volume = 92\n | year = 2004}}.\n\n==External links==\n*{{mathworld|urlname=Robertson-SeymourTheorem|title=Robertson-Seymour Theorem}}\n\n{{DEFAULTSORT:Robertson-Seymour theorem}}\n[[Category:Graph minor theory]]\n[[Category:Wellfoundedness]]\n[[Category:Theorems in graph theory]]"
    },
    {
      "title": "Scott–Potter set theory",
      "url": "https://en.wikipedia.org/wiki/Scott%E2%80%93Potter_set_theory",
      "text": "An approach to the [[foundations of mathematics]] that is of relatively recent origin, '''Scott–Potter set theory''' is a collection of nested [[axiomatic set theory|axiomatic set theories]] set out by the [[philosopher]] Michael Potter, building on earlier work by the [[mathematician]] [[Dana Scott]] and the philosopher [[George Boolos]].\n\nPotter (1990, 2004) clarified and simplified the approach of Scott (1974), and showed how the resulting [[axiomatic set theory]] can do what is expected of such theory, namely grounding the [[cardinal number|cardinal]] and [[ordinal number]]s, [[Peano axioms|Peano arithmetic]] and the other usual [[number system]]s, and the theory of [[Relation (mathematics)|relations]].\n\n==ZU etc.==\n\n===Preliminaries===\nThis section and the next follow Part I of Potter (2004) closely. The background logic is [[first-order logic]] with [[Identity (mathematics)|identity]]. The [[ontology]] includes [[urelement]]s as well as [[Set (mathematics)|sets]], which makes it clear that there can be sets of entities defined by first-order theories not based on sets. The urelements are not essential in that other mathematical structures can be defined as sets, and it is permissible for the set of urelements to be empty.\n\nSome terminology peculiar to Potter's set theory:\n* ι is a [[definite description]] operator and binds a variable. (In Potter's notation the iota symbol is inverted.)\n* The predicate U holds for all urelements (non-collections).\n* ιxΦ(x) exists [[iff]] ([[Uniqueness quantification|∃!x]])Φ(x). (Potter uses Φ and other upper-case Greek letters to represent formulas.)\n* {x : Φ(x)} is an abbreviation for  ιy(not U(y) and ([[forall|∀]]x)(x ∈ y ⇔ Φ(x))). \n* ''a'' is a '''collection''' if {''x'' : ''x''∈''a''} exists. (All sets are collections, but not all collections are sets.)\n* The '''accumulation''' of ''a'', acc(''a''), is the set {''x'' : ''x'' is an urelement or [[Existential quantification|∃]]''b''∈''a'' (''x''∈''b'' or ''x''⊂''b'')}.\n* If ∀''v''∈''V''(''v'' = acc(''V''∩''v'')) then ''V'' is a '''history'''.\n* A '''level''' is the accumulation of a history.\n* An '''initial level''' has no other levels as members. \n* A '''limit level''' is a level that is neither the initial level nor the level above any other level. \n* A '''set''' is a subcollection of some level.\n* The '''birthday''' of set ''a'', denoted ''V''(''a''), is the lowest level ''V'' such that ''a''⊂''V''.\n\n===Axioms===\nThe following three axioms define the theory '''ZU'''.\n\n'''Creation''': ∀''V''∃''V' ''(''V''∈''V' '').\n\n''Remark'': There is no highest level, hence there are infinitely many levels. This axiom establishes the [[ontology]] of levels.\n\n'''Separation''': An [[axiom schema]]. For any first-order formula Φ(''x'') with (bound) variables ranging over the level ''V'', the collection {''x''∈''V'' : Φ(''x'')} is also a set.  (See [[Axiom schema of separation]].)\n\n''Remark'': Given the levels established by ''Creation'', this schema establishes the existence of sets and how to form them. It tells us that  a level is a set, and all subsets, definable via [[first-order logic]], of levels are also sets. This schema can be seen as an extension of the background logic.\n\n'''Infinity''': There exists at least one limit level.  (See [[Axiom of infinity]].)\n\n''Remark'': Among the sets ''Separation'' allows, at least one is [[Infinity|infinite]]. This axiom is primarily [[mathematical]], as there is no need for the [[actual infinite]] in other human contexts, the human sensory order being necessarily [[Wikt:finite|finite]]. For mathematical purposes, the axiom \"There exists an [[inductive set (axiom of infinity)|inductive set]]\" would suffice.\n\n===Further existence premises===\nThe following statements, while in the nature of axioms, are not axioms of '''ZU'''. Instead, they assert the existence of sets satisfying a stated condition. As such, they are \"existence premises,\" meaning the following. Let '''X''' denote any statement below. Any theorem whose proof requires '''X''' is then formulated conditionally as \"If '''X''' holds, then...\" Potter defines several systems using existence premises, including the following two:\n* '''ZfU''' =<sub>'''df'''</sub>  '''ZU''' + ''Ordinals'';\n* '''ZFU''' =<sub>'''df'''</sub>   ''Separation'' + ''Reflection''.\n\n'''Ordinals''': For each (infinite) ordinal α, there exists a corresponding level ''V''<sub>α</sub>.\n\n''Remark'': In words, \"There exists a level corresponding to each infinite ordinal.\" ''Ordinals'' makes possible the conventional [[ordinal number|Von Neumann definition of ordinal numbers]]. \n\nLet τ(''x'') be a [[first-order logic|first-order term]].\n\n'''[[axiom schema of replacement|Replacement]]''':  An [[axiom schema]]. For any collection ''a'', ∀''x''∈''a''[τ(''x'') is a set] → {τ(''x'') : ''x''∈''a''} is a set.\n\n''Remark'': If the term τ(''x'') is a [[function (mathematics)|function]] (call it ''f''(''x'')), and if the [[domain (mathematics)|domain]] of ''f'' is a set, then the [[range (mathematics)|range]] of ''f'' is also a set.\n\n'''Reflection''': Let Φ denote a [[first-order logic|first-order formula]] in which any number of [[free variable]]s are present. Let Φ<sup>(''V'')</sup> denote Φ with these free variables all quantified, with the quantified variables restricted to the level ''V''.\n\nThen ∃''V''[Φ→Φ<sup>(''V'')</sup>] is an axiom.\n\n''Remark'': This schema asserts the existence of a \"partial\" universe, namely the level ''V'', in which all properties Φ holding when the quantified variables range over all levels, also hold when these variables range over ''V'' only. ''Reflection'' turns ''Creation'', ''Infinity'', ''Ordinals'', and ''Replacement'' into theorems (Potter 2004: §13.3).\n\nLet ''A'' and ''a'' denote sequences of non[[empty set]]s, each indexed by ''n''.\n\n'''[[axiom of countable choice|Countable Choice]]''': Given any sequence ''A'', there exists a sequence ''a'' such that:\n:∀''n''∈ω[''a''<sub>n</sub>∈''A''<sub>n</sub>].\n\n''Remark''. ''Countable Choice'' enables proving that any set must be one of finite or infinite.\n\nLet ''B'' and ''C'' denote sets, and let ''n'' index the members of ''B'', each denoted ''B''<sub>''n''</sub>.\n\n'''[[axiom of choice|Choice]]''': Let the members of ''B'' be disjoint nonempty sets. Then:\n:∃''C''∀''n''[''C''∩''B''<sub>''n''</sub> is a [[singleton (mathematics)|singleton]]].\n\n==Discussion==\nThe [[Von Neumann universe]] implements the \"iterative conception of set\" by stratifying the universe of sets into a series of \"levels,\" with the sets at a given level being the members of the sets making up the next higher level. Hence the levels form a nested and [[well-ordered]] sequence, and would form a [[hierarchy (mathematics)|hierarchy]] if set membership were [[transitive relation|transitive]]. The resulting iterative conception steers clear, in a well-motivated way, of the well-known [[paradox]]es of [[Russell's paradox|Russell]], [[Burali-Forti paradox|Burali-Forti]], and [[Cantor's paradox|Cantor]]. These paradoxes all result from the unrestricted use of the [[axiom of comprehension|principle of comprehension]] that [[naive set theory]] allows. Collections such as \"the class of all sets\" or \"the class of all ordinals\" include sets from all levels of the hierarchy. Given the iterative conception, such collections cannot form sets at any given level of the hierarchy and thus cannot be sets at all. The iterative conception has gradually become more accepted over time, despite an imperfect understanding of its historical origins.\n\nBoolos's (1989) axiomatic treatment of the iterative conception is his set theory  ''S'', a two sorted [[first order logic|first order theory]] involving sets and levels.\n\n===Scott's theory===\nScott (1974) did not mention the \"iterative conception of set,\" instead proposing his theory as a natural outgrowth of the [[type theory|simple theory of types]]. Nevertheless, Scott's theory can be seen as an axiomatization of the iterative conception and the associated iterative hierarchy.\n\nScott began with an axiom he declined to name: the [[atomic formula]] ''x''∈''y'' implies that ''y'' is a set. In symbols:\n:∀''x'',''y''∃''a''[''x''∈''y''→''y''=''a''].\nHis axiom of ''[[axiom of extensionality|Extensionality]]'' and [[axiom schema]] of ''Comprehension'' ([[axiom of separation|Separation]]) are strictly analogous to their [[Zermelo–Fraenkel set theory|ZF]] counterparts and so do not mention levels. He then invoked two axioms that do mention levels:\n* ''Accumulation''. A given level \"accumulates\" all members and subsets of all earlier levels. See the above definition of ''accumulation''.\n* ''Restriction''. All collections belong to some level.\n''Restriction'' also implies the existence of at least one level and assures that all sets are well-founded.\n\nScott's final axiom, the ''Reflection'' [[axiom schema|schema]], is identical to the above existence premise bearing the same name, and likewise does duty for ZF's ''[[axiom of infinity|Infinity]]'' and ''[[axiom of replacement|Replacement]]''. Scott's system has the same strength as ZF.\n\n===Potter's theory===\nPotter (1990, 2004) introduced the idiosyncratic terminology described earlier in this entry, and discarded or replaced all of Scott's axioms except ''Reflection''; the result is '''ZU'''. '''ZU''', like ZF, cannot be finitely axiomatized. '''ZU''' differs from [[ZFC]] in that it:\n* Includes no [[axiom of extensionality]] because the usual extensionality principle follows from the definition of collection and an easy lemma.\n* Admits [[axiom of foundation|nonwellfounded]] collections. However Potter (2004) never invokes such collections, and all sets (collections which are contained in a level) are wellfounded. No theorem in Potter would be overturned if an axiom stating that all collections are sets were added to '''ZU'''.\n*Includes no equivalents of [[axiom of choice|Choice]] or the axiom schema of [[axiom of replacement|Replacement]].\n\nHence '''ZU''' is closer to the [[Zermelo set theory]] of 1908, namely ZFC minus Choice, [[axiom of replacement|Replacement]], and Foundation. It is stronger than this theory, however, since cardinals and [[ordinal number|ordinals]] can be defined, despite the absence of Choice, using [[Scott's trick]] and the existence of levels, and no such definition is possible in Zermelo set theory. Thus in ZU, an equivalence class of:\n*[[Equinumerous]] sets from a common level is a [[cardinal number]];\n* [[Isomorphic]] [[well-ordering]]s, also from a common level, is an ordinal number.\n\nSimilarly the [[natural number]]s are not defined as a particular set within the iterative hierarchy, but as [[model theory|models]] of a \"pure\" Dedekind algebra. \"Dedekind algebra\" is Potter's name for a set closed under a unary [[injective]] operation, [[successor function|successor]], whose [[domain (mathematics)|domain]] contains a unique element, zero, absent from its [[range (mathematics)|range]]. Because the theory of Dedekind algebras is [[categorical theory|categorical]] (all models are [[isomorphic]]), any such algebra can proxy for the natural numbers.\n\nAlthough Potter (2004) devotes an entire appendix to [[proper class]]es, the strength and merits of Scott–Potter set theory relative to the well-known rivals to ZFC that admit proper classes, namely [[Von Neumann–Bernays–Gödel set theory|NBG]] and [[Morse–Kelley set theory]], have yet to be explored.\n\nScott–Potter set theory resembles [[New Foundations|NFU]] in that the latter is a recently (Jensen 1967) devised [[axiomatic set theory]] admitting both [[urelement]]s and sets that are not [[axiom of foundation|well-founded]]. But the urelements of NFU, unlike those of ZU, play an essential role; they and the resulting restrictions on [[axiom of extensionality|Extensionality]] make possible a proof of NFU's [[consistency]] relative to [[Peano arithmetic]]. But nothing is known about the strength of NFU relative to ''Creation''+''Separation'', NFU+''Infinity'' relative to ZU, and of NFU+''Infinity''+''Countable Choice'' relative to ZU + ''Countable Choice''.\n\nUnlike nearly all writing on set theory in recent decades, Potter (2004) mentions [[mereology|mereological fusions]]. His ''collections'' are also synonymous with the \"virtual sets\" of [[Willard Quine]] and [[Richard Milton Martin]]: entities arising from the free use of the [[set builder notation|principle of comprehension]] that can never be admitted to the [[universe of discourse]].\n\n==See also==\n*[[Foundation of mathematics]]\n*[[Hierarchy (mathematics)]]\n*[[List of set theory topics]]\n*[[Philosophy of mathematics]]\n*[[S (Boolos 1989)]]\n*[[Von Neumann universe]]\n*[[Zermelo set theory]]\n*[[ZFC]]\n\n==References==\n*[[George Boolos]], 1971, \"The iterative conception of set,\" ''Journal of Philosophy 68'': 215–31. Reprinted in Boolos 1999. ''Logic, Logic, and Logic''. Harvard Univ. Press: 13-29.\n*--------, 1989, \"Iteration Again,\" ''Philosophical Topics 42'': 5-21. Reprinted in Boolos 1999. ''Logic, Logic, and Logic''. Harvard Univ. Press: 88-104.\n*Potter, Michael, 1990. ''Sets: An Introduction''. Oxford Univ. Press.\n*------, 2004. ''Set Theory and its Philosophy''. Oxford Univ. Press.\n*[[Dana Scott]], 1974, \"Axiomatizing set theory\" in Jech, Thomas, J., ed., ''Axiomatic Set Theory II'', Proceedings of Symposia in Pure Mathematics 13. American Mathematical Society: 207–14.\n\n==External links==\nReviews of Potter (2004):\n* Bays, Timothy, 2005, \"[http://ndpr.nd.edu/review.cfm?id=2141 Review,]\" ''Notre Dame Philosophical Reviews''.\n*Uzquiano, Gabriel, 2005, \"[http://philmat.oxfordjournals.org/cgi/content/full/13/3/308 Review,]\" ''Philosophia Mathematica 13'': 308-46.\n\n{{DEFAULTSORT:Scott-Potter set theory}}\n[[Category:Systems of set theory]]\n[[Category:Urelements]]\n[[Category:Wellfoundedness]]"
    },
    {
      "title": "Structural induction",
      "url": "https://en.wikipedia.org/wiki/Structural_induction",
      "text": "'''Structural induction''' is a [[proof method]] that is used in [[mathematical logic]] (e.g., in the proof of [[Ultraproduct#Łoś' theorem|Łoś' theorem]]), [[computer science]], [[graph theory]], and some other mathematical fields.  It is a generalization of [[mathematical induction|mathematical induction over natural numbers]] and can be further generalized to arbitrary [[Noetherian induction]].  '''Structural recursion''' is a [[recursion]] method bearing the same relationship to structural induction as ordinary recursion bears to ordinary [[mathematical induction]].\n\nStructural induction is used to prove that some proposition ''P''(''x'') holds [[for all]] ''x'' of some sort of [[recursive definition|recursively defined]] structure, such as\n[[First-order logic#Formulas|formulas]], [[List (computer science)|lists]], or [[Tree (graph theory)|trees]].  A [[well-founded]] [[partial order]] is defined on the structures (\"subformula\" for formulas, \"sublist\" for lists, and \"subtree\" for trees).  The structural induction proof is a proof that the proposition holds for all the [[minimal element|minimal]] structures and that if it holds for the immediate substructures of a certain structure ''S'', then it must hold for ''S'' also. (Formally speaking, this then satisfies the premises of an axiom of [[well-founded induction]], which asserts that these two conditions are sufficient for the proposition to hold for all ''x''.)\n\nA structurally recursive function uses the same idea to define a recursive function: \"base cases\" handle each minimal structure and a rule for recursion.  Structural recursion is usually proved correct by structural induction; in particularly easy cases, the inductive step is often left out.  The ''length'' and ++ functions in the example below are structurally recursive.\n\nFor example, if the structures are lists, one usually introduces the partial order \"<\", in which ''L'' < ''M'' whenever list ''L'' is the tail of list ''M''.  Under this ordering, the empty list [] is the unique minimal element.  A structural induction proof of some proposition {{math|''P''(''l'')}} then consists of two parts:  A proof that ''P''([]) is true and a proof that if ''P''(''L'') is true for some list ''L'', and if ''L'' is the tail of list ''M'', then ''P''(''M'') must also be true.\n\nEventually, there may exist more than one base case and/or more than one inductive case, depending on how the function or structure was constructed. In those cases, a structural induction proof of some proposition {{math|''P''(''l'')}} then consists of: \n{{ordered list|list_style_type=upper-alpha\n|a proof that ''P''(''BC'') is true for each base case ''BC'',\n|a proof that if {{math|''P''(''I'')}} is true for some instance {{mvar|I}}, and ''M'' can be obtained from {{mvar|I}} by applying any one recursive rule once, then ''P''(''M'') must also be true.}}\n\n==Examples==\n\n[[File:Waldburg Ahnentafel.jpg|thumb|Ancient ancestor tree, showing 31 persons in 5 generations]]\nAn [[family tree|ancestor tree]] is a commonly known data structure, showing the parents, grandparents, etc. of a person as far as known (see picture for an example). It is recursively defined:\n* in the simplest case, an ancestor tree shows just one person (if nothing is known about their parents);\n* alternatively, an ancestor tree shows one person and, connected by branches, the two ancestor subtrees of their parents (using for brevity of proof the simplifying assumption that if one of them is known, both are).\n\nAs an example, the property \"An ancestor tree extending over ''g'' generations shows at most {{math|2<sup>''g''</sup>&nbsp;−&nbsp;1}} persons\" can be proven by structural induction as follows:\n* In the simplest case, the tree shows just one person and hence one generation; the property is true for such a tree, since {{math|1 ≤ 2<sup>1</sup>&nbsp;−&nbsp;1}}.\n* Alternatively, the tree shows one person and their parents' trees. Since each of the latter is a substructure of the whole tree, it can be assumed to satisfy the property to be proven (a.k.a. the ''induction hypothesis''). That is, {{math|''p'' ≤ 2<sup>''g''</sup>&nbsp;−&nbsp;1}} and {{math|''q'' ≤ 2<sup>''h''</sup>&nbsp;−&nbsp;1}} can be assumed, where ''g'' and ''h'' denotes the number of generations the father's and the mother's subtree extends over, respectively, and ''p'' and ''q'' denote the numbers of persons they show.\n** In case {{math|''g''&nbsp;≤&nbsp;''h''}}, the whole tree extends over {{math|1&nbsp;+&nbsp;''h''}} generations and shows {{math|1=''p''&nbsp;+&nbsp;''q''&nbsp;+&nbsp;1}} persons, and {{math|1=''p''&nbsp;+&nbsp;''q''&nbsp;+&nbsp;1 ≤ (2<sup>''g''</sup>&nbsp;−&nbsp;1) + (2<sup>''h''</sup>&nbsp;−&nbsp;1)&nbsp;+&nbsp;1 ≤&nbsp;2<sup>''h''</sup>&nbsp;+&nbsp;2<sup>''h''</sup>&nbsp;−&nbsp;1 = 2<sup>1+''h''</sup>&nbsp;−&nbsp;1}}, i.e. the whole tree satisfies the property.\n** In case {{math|1=''h''&nbsp;≤&nbsp;''g''}}, the whole tree extends over {{math|1=1&nbsp;+&nbsp;''g''}} generations and shows {{math|''p''&nbsp;+&nbsp;''q''&nbsp;+&nbsp;1 ≤&nbsp;2<sup>1&nbsp;+&nbsp;''g''</sup>&nbsp;−&nbsp;1}} persons by similar reasoning, i.e. the whole tree satisfies the property in this case also.\nHence, by structural induction, each ancestor tree satisfies the property.\n\nAs another, more formal example, consider the following property of lists:\n\n     length (L ++ M) = length L + length M          [EQ]\n\nHere ++ denotes the list concatenation operation, and L and M are lists.\n\nIn order to prove this, we need definitions for length and for the concatenation operation. Let (h:t) denote a list whose head (first element) is ''h'' and whose tail (list of remaining elements) is ''t'', and let [] denote the empty list. The definitions for length and the concatenation operation are:\n\n     length []     = 0                  [LEN1]\n     length (h:t)  = 1 + length t       [LEN2]\n\n     []    ++ list = list               [APP1]\n     (h:t) ++ list = h : (t ++ list)    [APP2]\n\nOur proposition {{math|1=''P''(''l'')}} is that  EQ is true for all lists ''M'' when ''L'' is {{mvar|l}}.  We want to show that {{math|1=''P''(''l'')}} is true for all lists {{mvar|l}}.    We will prove this by structural induction on lists.\n\nFirst we will prove that ''P''([]) is true; that is, EQ is true for all lists ''M'' when ''L'' happens to be the empty list []. Consider EQ:\n\n       length (L ++ M)  = length ([] ++ M)\n                        = length M                     (by APP1)\n                        = 0 + length M\n                        = length [] + length M         (by LEN1)\n                        = length L  + length M\n\nSo this part of the theorem is proved; EQ is true for all ''M'', when ''L'' is [], because the left-hand side and the right-hand side  are equal.\n\nNext, consider any nonempty list {{mvar|I}}. Since {{mvar|I}} is nonempty, it has a head item, x, and a tail list, xs, so we can express it as (x:xs).  The induction hypothesis is that EQ is true for all values of ''M'' when ''L'' is ''xs'':\n\n     length (xs ++ M) = length xs + length M    (hypothesis)\n\nWe would like to show that if this is the case, then EQ is also true for all values of ''M'' when {{math|1=''L'' = ''I''}} = (x:xs). We proceed as before:\n\n     length L  + length M      = length (x:xs) + length M\n                               = 1 + length xs + length M     (by LEN2)\n                               = 1 + length (xs ++ M)         (by hypothesis)\n                               = length (x: (xs ++ M))        (by LEN2)\n                               = length ((x:xs) ++ M)         (by APP2)\n                               = length (L ++ M)\n\nThus, from structural induction, we obtain that P(L) is true for all lists L.\n\n==Well-ordering==\n\nJust as standard [[mathematical induction]] is equivalent to the [[well-ordering principle]], structural induction is also equivalent to a well-ordering principle.  If the set of all structures of a certain kind admits a well-founded partial order, then every nonempty subset must have a minimal element.  (This is the definition of \"[[well-founded]]\".)  The significance of the lemma in this context is that it allows us to deduce that if there are any counterexamples to the theorem we want to prove, then there must be a minimal counterexample.  If we can show the existence of the minimal counterexample implies an even smaller counterexample, we have a contradiction (since the minimal counterexample isn't minimal) and so the set of counterexamples must be empty.\n\nAs an example of this type of argument, consider the set of all [[binary tree]]s.  We will show that the number of leaves in a full binary tree is one more than the number of interior nodes.  Suppose there is a counterexample; then there must exist one with the minimal possible number of interior nodes.  This counterexample, ''C'', has ''n'' interior nodes and {{mvar|l}} leaves, where {{math|1=''n''&nbsp;+&nbsp;1 ≠&nbsp;''l''}}.   Moreover, ''C'' must be nontrivial, because the trivial tree has {{math|1=''n''&nbsp;=&nbsp;0}} and {{math|1=''l''&nbsp;=&nbsp;1}} and is therefore not a counterexample. ''C'' therefore has at least one leaf whose parent node is an interior node.  Delete this leaf and its parent from the tree, promoting the leaf's sibling node to the position formerly occupied by its parent.  This  reduces both ''n'' and {{mvar|l}} by 1, so the new tree also has {{math|1=''n''&nbsp;+&nbsp;1 ≠&nbsp;''l''}} and is therefore a smaller counterexample.  But by hypothesis, ''C'' was already the smallest counterexample; therefore, the supposition that there were any counterexamples to begin with must have been false.  The  partial ordering implied by 'smaller' here is the one that says that ''S'' < ''T'' whenever ''S'' has fewer nodes than ''T''.\n\n==See also==\n*[[Coinduction]]\n*[[Initial algebra]]\n*[[Loop invariant]], analog for loops\n\n==References==\n* {{cite book | last = Hopcroft | first = John E.|author2=Rajeev Motwani |author3=Jeffrey D. Ullman | title = Introduction to Automata Theory, Languages, and Computation| edition = 2nd  | publisher = Addison-Wesley | location =  Reading Mass | year = 2001 | isbn = 978-0-201-44124-6}}\nEarly publications about structural induction include:\n* {{cite journal | last = Burstall | first = R. M. | title = Proving Properties of Programs by Structural Induction | journal = The Computer Journal | volume = 12 | number = 1 | pages = 41–48 | year = 1969 | doi=10.1093/comjnl/12.1.41}}\n* {{citation | last = Aubin | first = Raymond | title = Mechanizing Structural Induction | publisher = University of Edinburgh |  series = EDI-INF-PHD | volume = 76-002 | year = 1976 | hdl = 1842/6649 }}\n* {{cite conference | last1 = Huet | first1 = G. | last2 = Hullot | first2 = J. M. | title = Proofs by Induction in Equational Theories with Constructors | booktitle = 21st Ann. Symp. on Foundations of Computer Science | publisher = IEEE | pages = 96–107 | year = 1980 | url = https://hal.inria.fr/inria-00076533/file/RR-0028.pdf }}\n* [[Rózsa Péter]], ''Über die Verallgemeinerung der Theorie der rekursiven Funktionen für abstrakte Mengen geeigneter Struktur als Definitionsbereiche'',  Symposium International, Varsovie septembre (1959) <small>(''On the generalization of the theory of recursive functions for abstract quantities with suitable structures as domains'')</small>.\n\n[[Category:Logic in computer science]]\n[[Category:Mathematical logic]]\n[[Category:Mathematical proofs]]\n[[Category:Wellfoundedness]]\n[[Category:Mathematical induction]]\n[[Category:Graph theory]]"
    },
    {
      "title": "Universal set",
      "url": "https://en.wikipedia.org/wiki/Universal_set",
      "text": "{{other uses}}\nIn [[set theory]], a '''universal set''' is a set which contains all objects, including itself.<ref>Forster 1995 p. 1.</ref> In set theory as usually formulated, the conception of a universal set leads to [[Russell's paradox]] and is consequently not allowed. However, some non-standard variants of set theory include a universal set.\n\n== Notation ==\nThere is no standard notation for the universal set of a given set theory. Common symbols include '''V''', '''U''' and '''[[xi (letter)|ξ]]'''.{{cn|reason=Give an example reference for each naming|date=June 2019}}\n\n==Reasons for nonexistence==\n[[Zermelo–Fraenkel set theory]] and related set theories, which are based on the idea of the [[cumulative hierarchy]], do not allow for the existence of a universal set. It is directly contradicted by the [[axiom of regularity]], and its existence would cause paradoxes which would make the theory inconsistent.\n\n===Russell's paradox===\n{{Main|Russell's paradox}}\nRussell's paradox prevents the existence of a universal set in [[Zermelo–Fraenkel set theory]] and other set theories that include [[Zermelo]]'s [[axiom of comprehension]].\nThis axiom states that, for any formula <math>\\varphi(x)</math> and any set {{mvar|A}}, there exists a set \n:<math>\\{x \\in A \\mid \\varphi(x)\\}</math>\nthat contains exactly those elements {{mvar|x}} of {{mvar|A}} that satisfy <math>\\varphi</math>.  If a universal set&nbsp;{{mvar|V}} existed and the axiom of comprehension could be applied to it, then\nthere would also exist a set <math>\\{x \\in V\\mid x\\not\\in x\\}</math>, the set of all sets that do not contain themselves. However, as [[Bertrand Russell]] observed, this set is paradoxical. If it contains itself, then it should not contain itself, and vice versa. For this reason, it cannot exist.\n\n===Cantor's theorem===\n{{Main|Cantor's theorem}}\nA second difficulty with the idea of a universal set concerns the [[power set]] of the set of all sets. Because this power set is a set of sets, it would necessarily be a subset of the set of all sets, provided that both exist.  However, this conflicts with Cantor's theorem that the power set of any set (whether infinite or not) always has strictly higher [[cardinality]] than the set itself.\n\n==Theories of universality==\nThe difficulties associated with a universal set can be avoided either by using a variant of set theory in which the axiom of comprehension is restricted in some way, or by using a universal object that is not considered to be a set.\n\n===Restricted comprehension===\nThere are set theories known to be [[consistent]] (if the usual set theory is consistent) in which the universal set {{mvar|V}} does exist (and <math>V \\in V</math> is true). In these theories, Zermelo's [[axiom of comprehension]] does not hold in general, and the axiom of comprehension of [[naive set theory]] is restricted in a different way. A set theory containing a universal set is necessarily a [[non-well-founded set theory]].\nThe most widely studied set theory with a universal set is [[Willard Van Orman Quine]]'s [[New Foundations]]. [[Alonzo Church]] and {{ill|Arnold Oberschelp|de}} also published work on such set theories.  Church speculated that his theory might be extended in a manner consistent with Quine's,<ref>Church 1974 p. 308. See also Forster 1995 p. 136 or 2001 p. 17.</ref>\n<ref>{{cite journal |author=Flash Sheridan |date=2016 |title=A Variant of Church’s Set Theory with a Universal Set in which the Singleton Function is a Set |url=http://www.logic-center.be/Publications/Bibliotheque/SheridanVariantChurch.pdf |journal=Logique et Analyse |volume=59 |issue=233 |at=§0.2 |doi=10.2143/LEA.233.0.3149532|lay-url=http://www-logic.stanford.edu/seminar/1314/Sheridan_Fixing_Freges_Set_Theory.pdf}}</ref> but this is not possible for Oberschelp's, since in it the singleton function is provably a set,<ref>Oberschelp 1973 p. 40.</ref> which leads immediately to paradox in New Foundations.<ref>Holmes 1998 p. 110.</ref> In 1998, {{clarify span|advances|reason=DBriefly describe them.|date=June 2019}} in this area have been made by [[Randall Holmes]].<ref>{{cite book | isbn=2-87209-488-1 | author=Melvin Randall Holmes | editor= | title=Elementary Set Theory with a Universal Set | location=Louvain-La-Neuve | publisher=Bruylant-Academia | series=Cahiers du Centre de logique | volume=10 | edition=1st | year=1998 }} [http://math.boisestate.edu/~holmes/holmes/head.pdf Draft] of the 2nd edition.</ref>\n\nAnother example is [[positive set theory]], where the axiom of comprehension is restricted to hold only for the [[positive formula]]s (formulas that do not contain negations). Such set theories are motivated by notions of closure in topology.\n\n===Universal objects that are not sets===\n{{main|Universe (mathematics)}}\nThe idea of a universal set seems intuitively desirable in the [[Zermelo–Fraenkel set theory]], particularly because most versions of this theory do allow the use of quantifiers over all sets (see [[universal quantifier]]).  One way of allowing an object that behaves similarly to a universal set, without creating paradoxes, is to describe {{mvar|V}} and similar large collections as [[Class (set theory)|proper classes]] rather than as sets. One difference between a universal set and a [[universal class (set theory)|universal class]] is that the universal class does not contain itself, because [[proper class]]es cannot be elements of other classes.{{citation needed|date=May 2014}} Russell's paradox does not apply in these theories because the axiom of comprehension operates on sets, not on classes.\n\nThe [[category of sets]] can also be considered to be a universal object that is, again, not itself a set. It has all sets as elements, and also includes arrows for all functions from one set to another. \nAgain, it does not contain itself, because it is not itself a set.\n\n== See also ==\n\n* [[Universe (mathematics)]]\n* [[Grothendieck universe]]\n* [[Domain of discourse]]\n\n== Notes ==\n{{Reflist}}\n\n== References ==\n* [[Alonzo Church]] (1974). [https://books.google.com/books?id=6GFNxtPAK8UC&hl=en&output=reader&pg=GBS.PA297 “Set Theory with a Universal Set,”] [http://www.ams.org/books/pspum/025/pspum025-endmatter.pdf ''Proceedings of the Tarski Symposium. Proceedings of Symposia in Pure Mathematics XXV,''] ed. L. Henkin, American Mathematical Society, pp.&nbsp;297–308.\n* {{cite book | author=[[T. E. Forster]] | title= Set Theory with a Universal Set: Exploring an Untyped Universe (Oxford Logic Guides 31) | publisher=Oxford University Press | year=1995 | isbn=0-19-851477-8}}\n* [http://www.dpmms.cam.ac.uk/~tf/ T. E. Forster] (2001). [http://www.dpmms.cam.ac.uk/~tf/church2001.ps “Church’s Set Theory with a Universal Set.”]\n* [http://math.boisestate.edu/~holmes/holmes/setbiblio.html Bibliography: Set Theory with a Universal Set], originated by T. E. Forster and maintained by Randall Holmes at Boise State University.\n* [[:de:Arnold Oberschelp|Arnold Oberschelp]] (1973). “Set Theory over Classes,” ''Dissertationes Mathematicae'' 106.\n* [[Willard Van Orman Quine]] (1937) “New Foundations for Mathematical Logic,” ''American Mathematical Monthly'' 44, pp.&nbsp;70–80.\n\n== External links ==\n*{{MathWorld |title=Universal Set |id=UniversalSet }}\n\n{{Set theory}}\n\n{{DEFAULTSORT:Universal Set}}\n[[Category:Basic concepts in set theory]]\n[[Category:Set families]]\n[[Category:Paradoxes of naive set theory]]\n[[Category:Systems of set theory]]\n[[Category:Wellfoundedness]]\n[[Category:Self-reference]]"
    },
    {
      "title": "Well-ordering principle",
      "url": "https://en.wikipedia.org/wiki/Well-ordering_principle",
      "text": "{{refimprove|date=July 2008}}\n{{distinguish|Well-ordering theorem}}\nIn [[mathematics]], the '''well-ordering principle''' states that every non-empty set of positive integers contains a [[least element]].<ref>{{cite book |title=Introduction to Analytic Number Theory |last=Apostol |first=Tom |authorlink=Tom M. Apostol |year=1976 |publisher=Springer-Verlag |location=New York |isbn=0-387-90163-9 |pages=13 }}</ref> In other words, the set of positive integers is [[well-order]]ed by its \"natural\" or \"magnitude\" order in which x precedes y if and only if y is either x or the sum of x and some positive integer (other orderings include the ordering 2, 4, 6, ..., 1, 3, 5, ...).\n\nThe phrase \"well-ordering principle\" is sometimes taken to be synonymous with the \"[[well-ordering theorem]]\". On other occasions it is understood to be the proposition that the set of [[integers]] {…, −2, −1, 0, 1, 2, 3, …} contains a [[well-order|well-ordered]] subset, called the [[natural numbers]], in which every nonempty subset contains a least element.\n\nDepending on the framework in which the natural numbers are introduced, this (second order) property of the set of natural numbers is either an [[axiom]] or a provable theorem. For example:\n* In [[Peano arithmetic]], [[second-order arithmetic]] and related systems, and indeed in most (not necessarily formal) mathematical treatments of the well-ordering principle, the principle is derived from the principle of [[mathematical induction]], which is itself taken as basic. \n* Considering the natural numbers as a subset of the real numbers, and assuming that we know already that the real numbers are complete (again, either as an axiom or a theorem about the real number system), i.e., every bounded (from below) set has an infimum, then also every set ''A'' of natural numbers has an infimum, say ''a''*. We can now find an integer ''n''* such that ''a''* lies in the half-open interval (''n''*−1,&nbsp;''n''*], and can then show that we must have ''a''*&nbsp;=&nbsp;''n''*, and ''n''* in ''A''. \n* In [[axiomatic set theory]], the natural numbers are defined as the smallest [[Inductive set (axiom of infinity)|inductive set]] (i.e., set containing 0 and closed under the successor operation). One can (even without invoking the [[axiom of regularity|regularity axiom]]) show that the set of all natural numbers ''n'' such that \"{0,&nbsp;…,&nbsp;''n''} is well-ordered\" is inductive, and must therefore contain all natural numbers; from this property one can conclude that the set of all natural numbers is also well-ordered. \n\nIn the second sense, this phrase is used when that proposition is relied on for the purpose of justifying proofs that take the following form: to prove that every natural number belongs to a specified set S, assume the contrary, which implies that the set of counterexamples is non-empty and thus contains a smallest counterexample.  Then show that for any counterexample there is a still smaller counterexample, producing a contradiction.  This mode of argument is the [[contrapositive]] of proof by [[complete induction]]. It is known light-heartedly as the \"[[minimal criminal]]\" method and is similar in its nature to [[Fermat|Fermat's]] method of \"[[infinite descent]]\".\n\n[[Garrett Birkhoff]] and [[Saunders Mac Lane]] wrote in ''A Survey of Modern Algebra'' that this property, like the [[least upper bound axiom]] for real numbers, is non-algebraic; i.e., it cannot be deduced from the algebraic properties of the integers (which form an ordered [[integral domain]]).\n\n==References==\n{{reflist}} \n\n[[Category:Wellfoundedness]]\n[[Category:Mathematical principles]]\n\n[[cs:Princip dobrého uspořádání]]"
    },
    {
      "title": "Well-structured transition system",
      "url": "https://en.wikipedia.org/wiki/Well-structured_transition_system",
      "text": "In computer science, specifically in the field of [[formal verification]], '''well-structured transition systems (WSTSs)''' are a general class of infinite state systems for which many verification problems are [[decidable language|decidable]], owing to the existence of a kind of [[well-quasi-ordering|order]] between the states of the system which is compatible with the transitions of the system. WSTS decidability results can be applied to [[Petri nets]], lossy channel systems, and more.\n\n== Formal definition ==\nRecall that a [[well-quasi-ordering]] <math>\\leq</math> on a set <math>X</math> is a [[quasi-ordering]] (i.e., a [[preorder]] or [[reflexive relation|reflexive]], [[transitive relation|transitive]] [[binary relation]]) such that any infinite sequence of elements <math>x_0, x_1, x_2, \\ldots</math>, from <math>X</math> contains an increasing pair <math>x_i \\leq x_j</math> with <math>i < j</math>. The set <math>X</math> is said to be '''well-quasi-ordered''', or shortly '''wqo'''.\n\nFor our purposes, a ''[[transition system]]'' is a structure <math>\\mathcal S = \\langle S, \\rightarrow, \\cdots \\rangle</math>, where <math>S</math> is any set (its elements are called ''states''), and <math>\\rightarrow \\subseteq S \\times S</math> (its elements are called ''transitions''). In general a transition system may have additional structure like initial states, labels on transitions, accepting states, etc. (indicated by the dots), but they do not concern us here.\n\nA '''well-structured transition system''' consists of a transition system <math>\\langle S, \\to, \\leq \\rangle</math>, such that\n* <math>\\leq \\subseteq S \\times S</math> is a well-quasi-ordering on the set of states.\n* <math>\\leq</math> is upward compatible with <math>\\to</math>: that is, for all transitions <math>s_1 \\to s_2</math> (by this we mean <math>(s_1, s_2) \\in \\to</math>) and for all <math>t_1</math> such that <math>s_1 \\leq t_1</math>, there exists <math>t_2</math> such that <math>t_1 \\xrightarrow{*} t_2</math> (that is, <math>t_2</math> can be reached from <math>t_1</math> by a sequence of zero or more transitions) and <math>s_2 \\leq t_2</math>.\n\n[[File:UpwardCompatibilityDiagramForWSTS.svg|thumb|120px|The upward compatibility requirement]]\n\n=== Well-structured systems ===\n\nA '''well-structured system'''<ref name=\"ACJT\">Parosh Aziz Abdulla, Kārlis Čerāns, Bengt Jonsson, Yih-Kuen Tsay: ''Algorithmic Analysis of Programs with Well Quasi-ordered Domains'' (2000), Information and Computation, Vol. 160 issues 1-2, pp. 109--127</ref> is a transition system <math>(S,\\to)</math> with state set <math>S = Q \\times D</math> made up from a finite ''control state'' set <math>Q</math>, a ''data values'' set <math>D</math>, furnished with a [[Decidability (logic)|''decidable'']] pre-order <math>\\leq \\subseteq D \\times D</math> which is extended to states by <math>(q,d)\\le(q',d') \\Leftrightarrow q=q' \\wedge d\\le d'</math>, which is well-structured as defined above (<math>\\to</math> is monotonic, i.e. upward compatible, with respect to <math>\\le</math>) and in addition has a [[Computability|''computable'']] set of minima for the set of predecessors of any [[Ideal (order theory)|upward closed]] subset of <math>S</math>.\n\nWell-structured systems adapt the theory of well-structured transition systems for modelling certain classes of systems encountered in [[computer science]] and provide the basis for decision procedures to analyse such systems, hence the supplementary requirements: the definition of a WSTS itself says nothing about the computability of the relations <math>\\le</math>, <math>\\to</math>.\n\n== Uses in Computer Science ==\n\n<!-- If <math>\\le</math> is computable, then one can construct a  -->\n\n=== Well-structured Systems ===\n\nCoverability can be decided for any well-structured system, and so can reachability of a given control state, by the ''backward algorithm'' of Abdulla et al.<ref name=\"ACJT\"/> or for specific subclasses of well-structured systems (subject to strict monotonicity,<ref name=\"Schnoebelen\">Alain Finkel and Philippe Schnoebelen, [http://www.lsv.ens-cachan.fr/Publis/PAPERS/PDF/FinSch-TCS99.pdf Well-Structured Transition Systems Everywhere!], Theoretical Computer Science 256(1–2), pages 63–92, 2001.</ref> e.g. in the case of unbounded [[Petri nets]]) by a forward analysis based on a Karp-Miller [[coverability]] graph.\n\n==== Backward Algorithm ====\n\nThe backward algorithm allows the following question to be answered: given a well-structured system and a state <math>s</math>, is there any transition path that leads from a given start state <math>s_0</math> to a state <math>s' \\ge s</math> (such a state is said to ''cover'' <math>s</math>)?\n\nAn intuitive explanation for this question is: if <math>s</math> represents an error state, then any state ''containing'' it should also be regarded as an error state. If a well-quasi-order can be found that models this \"containment\" of states and which also fulfills the requirement of monotonicity with respect to the transition relation, then this question can be answered.\n\nInstead of one minimal error state <math>s</math>, one typically considers an upward closed set <math>S_e</math> of error states.\n\nThe algorithm is based on the facts that in a well-quasi-order <math>(A,\\le)</math>, any upward closed set has a finite set of minima, and any sequence <math>S_1 \\subseteq S_2 \\subseteq ...</math> of upward-closed subsets of <math>A</math> converges after finitely many steps (1).\n\nThe algorithm needs to store an upward-closed set <math>S_s</math> of states in memory, which it can do because an upward-closed set is representable as a finite set of minima. It starts from the upward closure of the set of error states <math>S_e</math> and computes at each iteration the (by monotonicity also upward-closed) set of immediate predecessors and adding it to the set <math>S_s</math>. This iteration terminates after a finite number of steps, due to the property (1) of well-quasi-orders. If <math>s_0</math> is in the set finally obtained, then the output is \"yes\" (a state of <math>S_e</math> can be reached), otherwise it is \"no\" (it is not possible to reach such a state).\n\n== References ==\n{{Reflist}}\n\n[[Category:Wellfoundedness]]\n[[Category:Automata (computation)]]"
    },
    {
      "title": "Enumeration",
      "url": "https://en.wikipedia.org/wiki/Enumeration",
      "text": "{{For|enumeration types in programming languages|enumerated type}}\n{{For|enumeration algorithms|enumeration algorithm}}\nAn '''enumeration''' is a complete, ordered listing of all the items in a collection. The term is commonly used in [[mathematics]] and [[computer science]] to refer to a listing of all of the [[element (mathematics)|element]]s of a [[Set (mathematics)|set]]. The precise requirements for an enumeration (for example, whether the set must be [[finite set|finite]], or whether the list is allowed to contain repetitions) depend on the discipline of study and the context of a given problem.\n\nSome sets can be enumerated by means of a '''natural ordering''' (such as 1, 2, 3, 4, ... for the set of [[positive integer]]s), but in other cases it may be necessary to impose a (perhaps arbitrary) ordering. In some contexts, such as [[enumerative combinatorics]], the term ''enumeration'' is used more in the sense of ''[[counting]]'' – with emphasis on determination of the number of elements that a set contains, rather than the production of an explicit listing of those elements.\n\n== Combinatorics ==\n{{main article|Enumerative combinatorics}}\n\nIn combinatorics, enumeration means [[counting]], i.e., determining the exact number of elements of finite sets, usually grouped into infinite families, such as the family of sets each consisting of all [[permutation]]s of some finite set. There are flourishing subareas in many branches of mathematics concerned with enumerating in this sense objects of special kinds. For instance, in ''[[partition (number theory)|partition]] enumeration'' and ''[[graph enumeration]]'' the objective is to count partitions or graphs that meet certain conditions.\n\n== Set theory ==\n\nIn [[set theory]], the notion of enumeration has a broader sense, and does not require the set being enumerated to be finite.\n\n===Listing===\n\nWhen an enumeration is used in an [[sequence|ordered list]] context, we impose some sort of ordering structure requirement on the [[Index set (recursion theory)|index set]]. While we can make the requirements on the ordering quite lax in order to allow for great generality, the most natural and common prerequisite is that the index set be [[well-ordered]]. According to this characterization, an ordered enumeration is defined to be a surjection (an onto relationship) with a well-ordered domain. This definition is natural in the sense that a given well-ordering on the index set provides a unique way to list the next element given a partial enumeration.\n\n===Countable vs. uncountable===\n\nThe most common use of enumeration in set theory occurs in the context where infinite sets are separated into those that are countable and those that are not. In this case, an enumeration is merely an enumeration with domain &omega;, the ordinal of the [[natural number]]s. This definition can also be stated as follows:\n\n* As a [[surjective]] mapping from <math>\\mathbb{N}</math> (the [[natural number]]s) to ''S'' (i.e., every element of ''S'' is the image of at least one natural number). This definition is especially suitable to questions of [[computability]] and elementary [[set theory]].\n\nWe may also define it differently when working with finite sets. In this case an enumeration may be defined as follows:\n\n* As a [[bijective]] mapping from ''S'' to an initial segment of the natural numbers. This definition is especially suitable to combinatorial questions and finite sets; then the initial segment is {1,2,...,''n''} for some ''n'' which is the [[cardinality]] of ''S''.\n\nIn the first definition it varies whether the mapping is also required to be [[injective]] (i.e., every element of ''S'' is the image of ''exactly one'' natural number), and/or allowed to be [[partial function|partial]] (i.e., the mapping is defined only for some natural numbers). In some applications (especially those concerned with computability of the set ''S''), these differences are of little importance, because one is concerned only with the mere existence of some enumeration, and an enumeration according to a liberal definition will generally imply that enumerations satisfying stricter requirements also exist.\n\nEnumeration of [[finite set]]s obviously requires that either non-injectivity or partiality is accepted, and in contexts where finite sets may appear one or both of these are inevitably present.\n\n==== Examples ====\n\n* The [[Natural number|natural numbers]] are enumerable by the function f(x) = x. In this case <math>f: \\mathbb{N} \\to \\mathbb{N}</math> is simply the [[identity function]].\n* <Math>\\mathbb{Z}</math>, the set of [[integers]] is enumerable by\n\n:: <Math>f(x):= \\begin{cases} -(x+1)/2, & \\mbox{if } x \\mbox{ is odd} \\\\ x/2, & \\mbox{if } x \\mbox{ is even}. \\end{cases} </math>\n\n<math>f: \\mathbb{N} \\to \\mathbb{Z}</math> is a bijection since every natural number corresponds to exactly one integer. The following table gives the first few values of this enumeration:\n\n{| cellpadding=\"8\"\n! ''x''\n| 0\n| 1\n| 2\n| 3\n| 4\n| 5\n| 6\n| 7\n| 8\n|-\n! ''&fnof;''(''x'')\n| 0\n| −1\n| 1\n| −2\n| 2\n| −3\n| 3\n| −4\n| 4\n|}\n\n* All (non empty) finite sets are enumerable. Let ''S'' be a finite set with ''n > 0'' elements and let ''K'' = {1,2,...,''n''}. Select any element ''s'' in ''S'' and assign ''&fnof;''(''n'') = ''s''. Now set ''S<nowiki>'</nowiki>'' = ''S''&nbsp;&minus;&nbsp;{''s''} (where &minus; denotes [[set difference]]). Select any element ''s' ''&nbsp;&isin;&nbsp;''S' '' and assign ''&fnof;''(''n''&nbsp;&minus;&nbsp;1) = ''s' ''. Continue this process until all elements of the set have been assigned a natural number. Then <math>f: K \\to S</math> is an enumeration of ''S''.\n* The [[real number]]s have no countable enumeration as proved by [[Cantor's diagonal argument]] and [[Cantor's first uncountability proof]].\n\n==== Properties ====\n\n* There exists an enumeration for a set (in this sense) if and only if the set is [[countable]].\n* If a set is enumerable it will have an [[uncountable]] infinity of different enumerations, except in the degenerate cases of the empty set or (depending on the precise definition) sets with one element. However, if one requires enumerations to be injective ''and'' allows only a limited form of partiality such that if ''&fnof;''(''n'') is defined then ''&fnof;''(''m'') must be defined for all ''m''&nbsp;<&nbsp;''n'', then a finite set of ''N'' elements has exactly ''N''! enumerations.\n* An enumeration ''e'' of a set ''S'' with domain <math>\\mathbb{N}</math> induces a [[well-order]] ≤ on that set defined by ''s'' ≤ ''t'' if and only if <math>\\min</math>&nbsp;''e''<sup>&minus;1</sup>(''s'')&nbsp;&le;&nbsp;<math>\\min</math>&nbsp;''e''<sup>&minus;1</sup>(''t''). Although the order may have little to do with the underlying set, it is useful when some order of the set is necessary.\n\n=== Ordinals ===\n\nIn [[set theory]], there is a more general notion of an enumeration than the characterization requiring the domain of the listing function to be an [[initial segment]] of the Natural numbers where the domain of the enumerating function can assume any [[Ordinal number|ordinal]]. Under this definition, an enumeration of a set ''S'' is any [[surjection]] from an ordinal &alpha; onto ''S''. The more restrictive version of enumeration mentioned before is the special case where &alpha; is a finite ordinal or the first limit ordinal &omega;. This more generalized version extends the aforementioned definition to encompass [[Transfinite induction|transfinite]] listings.\n\nUnder this definition, the [[first uncountable ordinal|first uncountable ordinal <math>\\omega_1</math>]] can be enumerated by the identity function on <math>\\omega_1</math> so that these two notions do '''not''' coincide. More generally, it is a theorem of ZF that any [[well-ordered]] set can be enumerated under this characterization so that it coincides up to relabeling with the generalized listing enumeration. If one also assumes the [[Axiom of Choice]], then all sets can be enumerated so that it coincides up to relabeling with the most general form of enumerations.\n\nSince [[set theorist]]s work with infinite sets of arbitrarily large [[cardinality|cardinalities]], the default definition among this group of mathematicians of an enumeration of a set tends to be any arbitrary &alpha;-sequence exactly listing all of its elements. Indeed, in Jech's book, which is a common reference for set theorists, an enumeration is defined to be exactly this. Therefore, in order to avoid ambiguity, one may use the term finitely enumerable or [[denumerable]] to denote one of the corresponding types of distinguished countable enumerations.\n\n=== Comparison of cardinalities ===\n\nFormally, the most inclusive definition of an enumeration of a set ''S'' is any [[surjection]] from an arbitrary [[index set]] ''I'' onto ''S''. In this broad context, every set ''S'' can be trivially enumerated by the [[identity function]] from ''S'' onto itself. If one does ''not'' assume the [[axiom of choice]] or one of its variants, ''S'' need not have any [[well-ordering]]. Even if one does assume the axiom of choice, ''S'' need not have any natural well-ordering.\n\nThis general definition therefore lends itself to a counting notion where we are interested in \"how many\" rather than \"in what order.\" In practice, this broad meaning of enumeration is often used to compare the relative sizes or [[cardinality|cardinalities]] of different sets. If one works in [[Zermelo–Fraenkel set theory]] without the axiom of choice, one may want to impose the additional restriction that an enumeration must also be [[injective]] (without repetition) since in this theory, the existence of a surjection from ''I'' onto ''S'' need not imply the existence of an [[Injection (mathematics)|injection]] from ''S'' into ''I''.\n\n== Computability and complexity theory ==\n\nIn [[computability theory]] one often considers countable enumerations with the added requirement that the mapping from <math>\\mathbb{N}</math> (set of all natural numbers) to the enumerated set must be [[computable function|computable]]. The set being enumerated is then called [[recursively enumerable]] (or computably enumerable in more contemporary language), referring to the use of [[recursion theory]] in formalizations of what it means for the map to be computable.\n\nIn this sense, a subset of the natural numbers is [[computably enumerable]] if it is the range of a computable function. In this context, enumerable may be used to mean computably enumerable. However, these definitions characterize distinct classes since there are uncountably many subsets of the natural numbers that can be enumerated by an arbitrary function with domain &omega; and only countably many computable functions. A specific example of a set with an enumeration but not a computable enumeration is the complement of the [[halting problem|halting set]].\n\nFurthermore, this characterization illustrates a place where the ordering of the listing is important. There exists a computable enumeration of the halting set, but '''not''' one that lists the elements in an increasing ordering. If there were one, then the halting set would be [[Decidable language|decidable]], which is provably false. In general, being recursively enumerable is a weaker condition than being a [[decidable set]].\n\nThe notion of enumeration has also been studied from the point of view of [[computational complexity theory]] for various tasks in the context of [[enumeration algorithm]]s.\n\n==See also==\n* [[Ordinal number]]\n* [[Enumerative definition]]\n* [[Sequence]]\n\n==References==\n* {{cite book|author=Jech, Thomas|title=Set theory, third millennium edition (revised and expanded)|publisher=Springer|year=2002|isbn=3-540-44085-2|authorlink=Thomas Jech}}\n\n==External links==\n* {{Wiktionary-inline}}\n\n{{Mathematical logic}}\n\n{{Authority control}}\n\n[[Category:Enumerative combinatorics]]\n[[Category:Mathematical logic]]\n[[Category:Ordering]]"
    },
    {
      "title": "Integrative level",
      "url": "https://en.wikipedia.org/wiki/Integrative_level",
      "text": "An '''integrative level''', or '''level of organization''', is a set of phenomena [[emergence|emerging]] from pre-existing phenomena of a lower level. The levels concept is an intellectual framework for structuring reality. It arranges all material entities and all processes in the universe into a hierarchy based on how complex the entity's organization is. When arranged this way, each entity is three things at the same time: It is made up of parts from the previous level below. It is a whole in its own right. And it is a part of the whole that is on the next level above. Typical examples include [[life]] emerging from non-living substances, and [[consciousness]] emerging from [[nervous system]]s. \n\n==Levels==\nThe main levels usually acknowledged are those of [[matter]], [[life]], [[mind]], and [[society]]. These are called ''strata'' in [[Nicolai Hartmann]]'s ontology. They can be further analyzed into more specific ''layers'', such as those of particles, atoms, molecules, and rocks forming the material stratum, or those of cells, organisms, populations, and ecosystems forming the life stratum.\n\nThe sequence of levels is often described as one of increasing [[complexity]], although it is not clear whether this is always true: for example, [[parasitism]] emerges on pre-existing organisms, although parasites are often simpler than their originating forms.\n\n==Philosophies==\n\nIdeas connected to integrative levels can be found in the works of both [[materialism|materialist]] philosophers,  and [[Idealism|anti-materialist]] ones.\n\n== See also ==\n\n* [[Big History]]\n* [[Emergence]]\n* [[Level of analysis]]\n* [[Structuralism (biology)]]\n* [[T. C. Schneirla]]\n* [[Vitalism]]\n* [[Nicolai Hartmann]]\n\n== References ==\n{{Reflist}}\n* [[Samuel Alexander|Alexander, Samuel]], ''Space, time and deity'', London, 1920\n* Blitz, D., ''Emergent evolution: qualitative novelty and the levels of reality'', Kluwer, 1992\n* Conger, G.P., \"The doctrine of levels\", Journal of philosophy, 22: 1925, 12, p.&nbsp;309-321\n* Feibleman, James K., \"Theory of integrative levels\", British journal for the philosophy of science, 5: 1954, 17, p.&nbsp;59-66\n* [[Douglas John Foskett|Foskett, Douglas John]], \"The theory of integrative levels and its relevance to the design of information systems\", Aslib proceedings, 30: 1978, 6, p.&nbsp;202-208\n* Greenberg, Gary and Tobach, Ethel, \"Behavioral Evolution and Integrative Levels\", Lawrence Earlbaum Associates, Inc. Publisher, 1984\n* Korzybski, Alfred, ''Science and Sanity: An Introduction to Non-Aristotelian Systems and General Semantics'', 1933\n* Hartmann, N., ''Die Aufbau der realen Welt: Grundriss der allgemeinen Kategorienlehre'', De Gruyter, 1940\n* Hartmann, N., ''New ways of ontology'', Greenwood Press, 1952\n* Morgan, C.L., ''Emergent evolution'', Williams and Norgate, London 1923\n* [[Joseph Needham|Needham, Joseph]], \"Integrative levels: a revaluation of the idea of progress\", in ''[https://archive.org/details/timerefreshingri00need Time: the refreshing river: essays and addresses, 1932-1942]'', Allen and Unwin, London 1943, p.&nbsp;233-272\n* Novikoff, A.B., \"The concept of integrative levels and biology\", Science, 101: 1945, p.&nbsp;209-215\n* Pettersson, M., ''Complexity and evolution'', Cambridge University Press, 1996\n* Poli, R., \"Levels\", Axiomathes, 9: 1998, 1-2. p.&nbsp;197-211\n* Poli, R., \"The basic problem of the theory of levels of reality\", Axiomathes, 12: 2001, 3-4, p.&nbsp;261-283\n\n[[Category:Ordering]]\n[[Category:Emergence]]"
    },
    {
      "title": "Marie Kondo",
      "url": "https://en.wikipedia.org/wiki/Marie_Kondo",
      "text": "{{short description|Japanese author and consultant}}\n{{Use dmy dates|date=January 2019}}\n{{Infobox person\n| name          = Marie Kondo<br>{{nihongo|近藤 麻理恵}}\n| image         = Marie Kondō, 2016 (cropped).jpg\n| caption       = Marie Kondo in 2016\n| birth_date    = {{Birth date and age|df=yes|9 October 1984}}\n| birth_place   = [[Tokyo]], [[Japan]]\n| death_date    = \n| death_place   = \n| residence     = [[Los Angeles]], [[California]], U.S.\n| nationality   = Japanese\n| alma_mater    = [[Tokyo Woman's Christian University]]\n| spouse        = {{marriage|Takumi Kawahara|2012}}\n| children      = 2\n| occupation    = {{hlist|[[Professional organizing|Organizing]] [[consultant]]|[[author]]}} \n| years_active  = 1997–present\n| known_for     = KonMari method and organizational books\n| notable_works = ''The Life-Changing Magic of Tidying Up''\n| website = {{URL|konmari.com}}\n}}\n\n{{nihongo|'''Marie Kondo'''|近藤 麻理恵|Kondō Marie|born 9 October 1984<ref name=Stern>{{cite magazine|url=http://www.instyle.com/news/marie-kondo-decluttering-queen-trivia|date=23 January 2016|author=Stern, Claire|title=Who Is Marie Kondo? 7 Things You Might Not Know About the Japanese Decluttering Guru|magazine=InStyle|publisher=Time Inc.|accessdate=12 August 2016}}</ref>}}, otherwise known as {{nihongo||こんまり|''Konmari''<ref name=こんまり>{{cite magazine|url=https://www.newsweekjapan.jp/stories/woman/2019/01/post-133.php|date=21 January 2019|author=寺町幸枝|title=「こんまり」流が全米に拡散――ネット番組のヒットで日本発の片付け術が急激に広まっている|magazine=Newsweek Japan|publisher=Newsweek Japan|accessdate=11 March 2019}}</ref>}}, is a Japanese [[Professional organizer|organising consultant]] and author.<ref name=\"Socks\">{{cite news|url=https://www.nytimes.com/2014/10/23/garden/home-organization-advice-from-marie-kondo.html?_r=0|title=Kissing Your Socks Goodbye: Home Organization Advice from Marie Kondo|newspaper=[[The New York Times]]|accessdate=26 October 2014}}</ref>\n\nKondo has written four books on organising, which have collectively sold millions of copies and have been translated from Japanese into several languages including [[Korean language|Korean]], [[Written Chinese|Chinese]], [[Spanish language|Spanish]], [[Indonesian language|Indonesian]],<ref>{{cite web|title=Gramedia.com – Marie Kondo, \"The Life Changing Magic Of Tidying Up\" (Indonesian version)|date=2018|publisher=Gramedia Indonesia – Online books|accessdate=31 March 2018|url=http://www.gramedia.com/products/the-life-changing-magic-of-tidying-up/}}</ref> [[French language|French]], [[German language|German]], [[Swedish language|Swedish]], [[Portuguese language|Portuguese]], [[Catalan language|Catalan]] and [[English language|English]].<ref name=\"Socks\"/> In particular, her book ''The Life-Changing Magic of Tidying Up'' (2011) has been published in more than 30 countries.<ref name=\"wsj._Mari\">{{Cite web|title=Marie Kondo and the Cult of Tidying Up|last=Maloney|first=Jennifer|last2=Fujikawa|first2=Megumi|work=Wall Street Journal|date=26 February 2015|accessdate=1 March 2015|url=https://www.wsj.com/articles/marie-kondo-and-the-tidying-up-trend-1424970535|via={{URL|konmari.com}}}}</ref> It was a best-seller in Japan and in Europe, and was published in the United States in 2014.<ref name=\"Socks\"/>\n\nIn the United States and United Kingdom the profile of Kondo and her methods were greatly raised by the success of [[Netflix]] series ''[[Tidying Up with Marie Kondo]]'', released in 2019.<ref>{{cite web|title=Marie Kondo’s Tidying Up Won Netflix. Next? Cleaning Consultants - Bloomberg|url=https://www.bloomberg.com/news/articles/2019-01-29/marie-kondo-conquered-netflix-and-is-now-training-cleaning-consultants}}</ref><ref>{{cite web|title=John Lewis is selling 47% more of this thanks to the Marie Kondo effect|url=https://www.idealhome.co.uk/news/john-lewis-sales-marie-kondo-effect-219192}}</ref>\n\nShe was listed as one of ''[[Time (magazine)|Time]]''{{'s}} \"100 most influential people\" in 2015.<ref>{{cite web|title=Marie Kondo|author=Jamie Lee Curtis|date=2015|publisher=Time Magazine|accessdate=16 May 2017|url=http://time.com/3822899/marie-kondo-2015-time-100/}}</ref>\n\n==Background==\n[[File:Sportsfile (Web Summit) (22790692681).jpg|thumb|Kondo in 2015]]\n\nKondo says that she has been interested in organizing since childhood.<ref name=\"wsj._Mari\"/> In junior school, Kondo ran into the classroom to [[Orderliness|tidy up]] bookshelves while her classmates were playing in physical education class<ref>{{cite web|url=https://thefinancetwins.com/konmari-method-marie-kondo-save-money/|title=How To Use The KonMari Method|last=Maldonado |first=Camilo |website=The Finance Twins |date=22 Jan 2019 |accessdate=30 Apr 2019 |language=en}}</ref>. Whenever there were nominations for class roles, she did not seek to be the class representative or the pet feeder. Instead, she yearned to be the bookshelf manager to continue to tidy up books. She said she experienced a breakthrough in organizing one day, \"I was obsessed with what I could throw away. One day, I had a kind of [[nervous breakdown]] and fainted. I was unconscious for two hours. When I came to, I heard a mysterious voice, like some god of tidying telling me to look at my things more closely. And I realized my mistake: I was only looking for things to throw out. What I should be doing is finding the things I want to keep. Identifying the things that make you happy: that is the work of tidying.\"<ref name=\"oz\"/>\n\nShe spent five years as an [[Miko|attendant maiden]] at a [[Shinto]] shrine.<ref name=\"oz\"/> She founded her organising consulting business when she was 19 and a sociology student at [[Tokyo Woman's Christian University]].<ref>{{cite web|title=6 surprising things about Marie Kondo and her life-changing method|author=Maguire, Katy|date=7 July 2016|publisher=Well+Good LLC|accessdate=13 August 2016|url=https://www.wellandgood.com/good-advice/marie-kondo-surprising-things-about-her/}}</ref> In her senior year, she wrote her capstone thesis titled \"Tidying up as seen from the perspective of gender\".<ref>Aihara, Hitoshi (13 May 2015). [https://www.nikkansports.com/entertainment/news/1475696.html \"こんまりキレイ術の心は感謝　31カ国200万部超\"]. Nikkan Sports. Retrieved 11 February 2019.\n</ref>\n\n==KonMari method==\nKondo's method of organising is known as the KonMari method, and consists of gathering together all of one's belongings, one category at a time, and then keeping only those things that \"spark joy\" (ときめく ''tokimeku,'' the word in Japanese, means \"flutter, throb, palpitate\"),<ref name=\"kanj_Japa\">{{Cite web|title=Japanese-English translation: tokimeku: Dictionary|work=kanjijapanese.com|accessdate=1 March 2015|url=http://www.kanjijapanese.com/en/dictionary-japanese-english/tokimeku}}</ref> and choosing a place for everything from then on.<ref name=\"globe\"/><ref>{{cite web|url=http://www.japantimes.co.jp/culture/2014/10/11/books/konmaris-phenomenal-book-can-help-put-house-order/|title=How KonMari's phenomenal book can help put your house in order|publisher=Japantimes.co.jp|accessdate=26 October 2014}}</ref>\n\nKondo says that her method is partly inspired by the [[Shinto]] religion.<ref>{{Cite news|url=https://www.telegraph.co.uk/news/worldnews/asia/japan/12102664/Japans-decluttering-guru-says-she-is-on-a-mission-to-organise-the-world.html|title=Japan’s decluttering guru says she is on a mission to 'organise the world'|last=Demetriou|first=Danielle|date=16 January 2016|access-date=14 January 2019|language=en-GB|issn=0307-1235}}</ref> Cleaning and organising things properly can be a spiritual practice in Shintoism, which is concerned with the energy or divine spirit of things (''[[kami]]'') and the right way to live (''[[kannagara]]''). \"Treasuring what you have; treating the objects you own as not disposable, but valuable, no matter their actual monetary worth; and creating displays so you can value each individual object are all essentially Shinto ways of living.\"<ref>{{cite web |title=What White, Western Audiences Don’t Understand About Marie Kondo’s ‘Tidying Up’ |url=https://www.huffingtonpost.com/entry/marie-kondo-white-western-audineces_us_5c47859be4b025aa26bde77c?ncid=fcbklnkushpmg00000063&utm_campaign=hp_fb_pages&ir=Entertainment&utm_medium=facebook&utm_source=main_fb&fbclid=IwAR2ciRyNXffvoi8vHG5BxdhFz_PtM-eEAzeIfoHzR9xheN8bmuEabpce9JE |last=Dilloway |first=Margaret |website=Huffington Post |date=22 Jan 2019 |accessdate=27 Jan 2019 |language=en }}</ref>\n\n==Media appearances==\nA two-part TV dramatisation was filmed in 2013 based on Kondo and her work, titled {{lang|ja|人生がときめく片づけの魔法}} ({{transl|ja|Jinsei ga Tokimeku Katazuke no Mahō}}).<ref name=\"kataduke\">{{cite web|url=http://www.ntv.co.jp/kataduke/|title=人生がときめく片づけの魔法|publisher=ntv.co.jp|accessdate=7 March 2015}}</ref> She has lectured and made television appearances.<ref name=\"Socks\"/><ref name=\"oz\">{{cite web|url=http://www.theaustralian.com.au/news/world/marie-kondo-is-the-maiden-of-mess/story-fnb64oi6-1226888044451|title=Marie Kondo is the maiden of mess|publisher=Theaustralian.com.au|accessdate=26 October 2014}}</ref> She released a series of videos teaching \"the best way to fold for perfect appearance\".<ref name=\"globe\">{{cite web|url=https://www.theglobeandmail.com/life/relationships/japans-queen-of-clean-promotes-benefits-of-a-tidy-home/article18192133/|title=Japan’s 'queen of clean' promotes benefits of a tidy home|work=The Globe and Mail|accessdate=26 October 2014}}</ref>\n\nOn 1 January 2019, [[Netflix]] released a series called ''[[Tidying Up with Marie Kondo]]''.<ref>{{cite web|url=https://www.forbes.com/sites/camilomaldonado/2019/02/05/marie-kondo-financial-goals/|title=5 Ways Marie Kondo Can Declutter Your Home And Help You Reach Your Financial Goals|publisher=Forbes.com|accessdate=5 February 2019}}</ref> In the series, Kondo visits various American family homes full of clutter and guides the families in tidying up their houses through her KonMari method.\n\nOn 4 February 2019, Kondo appeared on ''[[The Late Show with Stephen Colbert]]'' on [[CBS]].<ref>{{cite episode|title=Taraji P. Henson, Matt Walsh, Marie Kondo|series=The Late Show with Stephen Colbert|first=Stephen|last=Colbert|network=CBS|date=4 February 2019}}</ref>\n\n==Personal life==\nKondo married Takumi Kawahara in 2012.<ref name=\"wsj._Mari\"/><ref name=\"Fujikawa2017\"/> At the time they met, Kawahara was working in sales support and marketing at a corporation in [[Osaka]]. Once Kondo's career took off, he left his job to become her manager and, eventually, CEO of Konmari Media, LLC.<ref>https://www.eonline.com/news/1007829/how-an-obsession-with-organizing-built-an-empire-inside-marie-kondo-s-controversially-tidy-world</ref> The couple has two children.<ref name=\"Fujikawa2017\">{{Cite web\n| title = Should You Kondo Your Kids?\n| last = Fujikawa | first = Megumi\n| work = Wall Street Journal\n| date = 9 August 2017\n| accessdate = 9 August 2017\n| url = https://www.wsj.com/articles/should-you-kondo-your-kids-1502283600\n| quote = ...2-year-old Satsuki  Younger sister Miko, 10 months, Ms. Kondo’s husband, Takumi Kawahara, 33, ...\n}}</ref><ref>Tonya C. Snyder. [https://www.washingtonpost.com/news/parenting/wp/2016/01/14/the-real-reason-marie-kondos-life-changing-magic-doesnt-work-for-parents/ The real reasons Marie Kondo’s life-changing magic doesn't work for parents]. ''[[The Washington Post]]'', 14 January 2016.</ref>\n\nAfter getting married she lived in Tokyo, and later moved to [[San Francisco]]. As of 2019, she and her family live in [[Los Angeles, California]].<ref>{{cite web|url=https://inews.co.uk/inews-lifestyle/marie-kondo-tidying-netflix-show/ |title=As Marie Kondo gets her own Netflix show, can she help me tidy up? |publisher=iNews |date=31 December 2018 |accessdate=12 January 2019 }}</ref>\n\n==Publications==\n*''Jinsei ga Tokimeku Katazuke no Mahō (人生がときめく片づけの魔法).''\n**Tokyo: Sunmark Shuppan, 2011; {{ISBN|978-4-7631-3120-1}} {{jp icon}}\n**German translation. 2013; {{ISBN|978-3-499-62481-0}}.\n**English translation. ''The Life-Changing Magic of Tidying Up: The Japanese Art of Decluttering and Organizing.'' New York: Ten Speed Press, 2014; {{ISBN|978-1607747307}}.\n*''Jinsei ga Tokimeku Katazuke no Mahō 2 (人生がときめく片づけの魔法2).'' Tokyo: Sunmark Shuppan, 2012; {{ISBN|978-4-7631-3241-3}}.\n*''Mainichi ga Tokimeku Katazuke no Mahō (毎日がときめく片付けの魔法),'' Tokyo: Sunmark Shuppan, 2014; {{ISBN|978-4-7631-3352-6}}.\n*''Irasuto de Tokimeku Katazuke no Mahō = The Illustrated Guide to the Life-Changing Magic of Tidying Up (イラストでときめく片付けの魔法）.'' Tokyo: Sunmark Shuppan, 2015; {{ISBN|978-4-7631-3427-1}}.\n*''Manga de Yomu Jinsei ga Tokimeku Katazuke no Mahō''. Tokyo: Sunmark Publishing, 2017;\n**English translation. ''The Life-Changing Magic of Tidying Up: a magical story.'' New York: Ten Speed Press, 2017; {{ISBN|978-0-399-58053-6}}.\n\n==References==\n{{Reflist}}\n\n==External links==\n{{Wiktionary|kondo|tokimeku}}\n{{Commonscat}}\n* {{Official website|konmari.com/}}\n* {{imdb name|5700825}}\n* [https://www.youtube.com/watch?v=w1-HMMX_NR8&spfreload=10 Marie Kondo: \"The Life Changing Magic of Tidying Up\" | Talks at Google]\n\n{{Authority control}}\n\n{{DEFAULTSORT:Kondo, Marie}}\n[[Category:21st-century Japanese writers]]\n[[Category:Cleaning]]\n[[Category:Japanese emigrants to the United States]]\n[[Category:Japanese women writers]]\n[[Category:Japanese Shintoists]]\n[[Category:Living people]]\n[[Category:Ordering]]\n[[Category:American television personalities]]\n[[Category:Japanese television personalities]]\n[[Category:Tokyo Woman's Christian University alumni]]\n[[Category:Writers from Tokyo]]\n[[Category:1984 births]]"
    },
    {
      "title": "Mise en place",
      "url": "https://en.wikipedia.org/wiki/Mise_en_place",
      "text": "{{for|the restaurant|Mise en Place (restaurant)}}\n{{Italic title}}\n[[File:Mise en place for hot station.jpg|thumb|250px|''Mise en place'' in a professional kitchen]]\n'''''Mise en place''''' ({{IPA-fr|mi zɑ̃ ˈplas}}) is a French [[culinary]] phrase which means \"putting in place\" or \"everything in its place.\" It refers to the set up required before cooking, and is often used in professional kitchens to refer to organizing and arranging the ingredients (e.g., cuts of [[meat]], [[relish]]es, sauces, [[Par-cooking|par-cooked]] items, spices, freshly chopped vegetables, and other components) that a cook will require for the menu items that are expected to be prepared during a shift.<ref>Montagné, Prosper. ''Larousse Gastronomique'', ed: Jennifer Harvey Lang. New York: Crown, 1988. Second English edition.</ref>\n\nThe practice can be applied in home kitchens.<ref>{{cite web |url=http://www.reluctantgourmet.com/mise-en-place/ |title=The Reluctant Gourmet, \"Mise en place\" |accessdate=10 February 2013}}</ref><ref>{{cite book |last=Ruhlman |first=Michael |date=14 September 2018 |title=Ruhlman’s Twenty |url=http://ruhlman.com/my-books/ |publisher=Chronicle Books |isbn=978-0811876438 |access-date=27 January 2015 |page=13 |quote=There's no reason it won't work for you in your kitchen at home.}}</ref>\n\nThe writer and chef Dan Charnas uses the concept of mise en place as a \"philosophy\" and \"system\" for what chefs believe and do, even going so far to call it an \"ethical code\". In the kitchen, the phrase is used as a noun (i.e., the setup of the array of ingredients), a verb (i.e., the process of preparing) and a state of mind. All of these uses, however, refer to someone who knows to be well-prepared. In this view, the term's broader meanings can be applied to classrooms, hospitals, and elsewhere.<ref>{{Cite book|url=https://books.google.com/books?id=l8tUCwAAQBAJ|title=Work Clean: The life-changing power of mise-en-place to organize your life, work, and mind|last=Charnas|first=Dan|date=2016-05-03|publisher=Rodale|isbn=9781623365936|language=en}}</ref>\n\n==Use outside cooking==\nThe term has also been used outside of cooking: [[psychologist]]s Weisberg et al., used the phrase to refer to \"how one's stance towards a given environment places constraints on what one feels able to do within that environment, and how these assessments and predispositions impact the process of preparing to act.\" They used the term in a study of how a school became safer after security measures — like metal detectors and bars on the windows — were removed, leading to the unexpected outcome.<ref>{{Cite journal|last=Weisberg|first=Deena Skolnick |display-authors=etal |date=June 2014|title=Mise en place: setting the stage for thought and action|url=http://kathyhirshpasek.com/wp-content/uploads/2015/08/Weisberg-Hirsh-Pasek-et-al.TICS1313.pdf|journal=Trends in Cognitive Sciences|doi=10.1016/j.tics.2014.02.012|pmid=|access-date=May 31, 2016}}</ref>\n\n==See also==\n*[[List of food preparation utensils]]\n*[[Service à la russe]]\n*[[Knolling]]\n\n==References==\n{{reflist}}\n\n==External links==\n{{wiktionary}}\n* [http://www.hertzmann.com/articles/2003/mise/ À la carte webpage discussing ''mise en place'']\n* [https://www.npr.org/blogs/thesalt/2014/08/11/338850091/for-a-more-ordered-life-organize-like-a-chef NPR Morning Edition story about ''mise en place'']\n* [https://effectiviology.com/mise-en-place-efficiency-kitchen-life/ “Mise en Place”: Become More Efficient in the Kitchen and in Life]: an article on implementing the concept of mise en place.\n\n[[Category:French words and phrases]]\n[[Category:Cooking]]\n[[Category:Ordering]]\n[[Category:Time management]]"
    },
    {
      "title": "Organizing (management)",
      "url": "https://en.wikipedia.org/wiki/Organizing_%28management%29",
      "text": "{{multiple issues|\n{{More citations needed|date=March 2013}}\n{{Tone|date=December 2007}}\n}}\n\n<!--\nTo wikipedians:\nPlease READ the Talk Page before doing any major modification.\n-->\n\n'''Organizing''' is the establishment of effective authority relationships among selected work, persons and work places in order for the group to work together efficiently. Or it is a process of dividing work into sections and departments.\n\n==History==\nThe [[information organization|organizing of information]] could be seen since humans began to [[write]]. Prior to that, history was passed down only through song and word.{{citation needed|date=April 2016}} As can be seen with religion, books and spoken word, science (through journals and studies) organizing not only is history, but also supports the communication of history. Recording ideas in a written text, as opposed to verbally communicating with someone, and more specifically cataloging ideas and thoughts, is also an attempt to organize information.\n\n[[Science book]]s are notable by their organization of a specific subject.{{citation needed|date=October 2013}} [[Encyclopedia]]s usually organize subjects into a single place, for faster [[index (publishing)|indexing]] and seeking of meanings.\n\n== Characteristics ==\nThe following are the important characteristics of organization.\n\n*'''Specialization and division of work.''' The entire philosophy of organization is centered on the concepts of [[specialization of work|specialization]] and [[division of work]]. The division of work is assigning responsibility for each organizational component to a specific individual or group thereof. It becomes specialization when the responsibility for a specific task lies with a designated expert in that field. The efforts of the operatives are coordinated to allow the process at hand to function correctly. Certain operatives occupy positions of [[management]] at various points in the process to ensure coordination.\n*'''Orientation towards goals.''' Every organization has its own purposes and objectives. Organizing is the function employed to achieve the overall goals of the organization. Organization harmonizes the individual goals of the employees with overall objectives of the firm.\n*'''Composition of individuals and groups.''' Individuals form a group and the groups form an organization. Thus, organization is the composition of individual and groups. Individuals are grouped into departments and their work is coordinated and directed towards organizational goals.\n*'''Continuity.''' An organization is a group of people with a defined relationship in which they work together to achieve the goals of that organization. This relationship does not come to end after completing each task. Organization is a never ending process.\n*'\"FLEXIBILITY.'\" The organizing process should be flexible so that any change can be incorporated easily.  It ensures the ability to adapt and adjust the activities in response to the change taking place in the external environment. The programs, policies and strategies can be changed as and when required if the provision for flexibility is made in the organizing process.\n\n== Purpose ==\n\n*'''Helps to achieve organizational goal.''' Organization is employed to achieve the overall objectives of business firms. Organization focuses attention of individuals objectives towards overall objectives.\n*'''Optimum use of resources.''' To make optimum use of resources such as men, material, money, machine and method, it is necessary to design an organization properly. Work should be divided and right people should be given right jobs to reduce the wastage of resources in an organization.\n*'''To perform managerial function.''' Planning, Organizing, Staffing, Directing and Controlling cannot be implemented without proper organization.\n*'''Facilitates growth and diversification.''' A good organization structure is essential for expanding business activity. Organization structure determines the input resources needed for expansion of a business activity similarly organization is essential for product diversification such as establishing a new product line. it also stimulates creativity in managers by organizing.\n*'''Humane treatment of employees.''' Organization has to operate for the betterment of employees and must not encourage monotony of work due to higher degree of specialization. Now, organization has adapted the modern concept of systems approach based on human relations and it discards the traditional productivity and specialization approach.\n\n==Applications==\n'''Organizing''', in [[company|companies]] point of view, is the management function that usually follows after planning. And it involves the assignment of tasks, the grouping of tasks into departments and the assignment of authority with adequate responsibility and allocation of resources across the organization to achieve common goals. Organizing involves the establishment of an intentional structures of roles through determination and enumeration of the activities required to achieve the goals of an enterprise and each part of it,the grouping of these activities, the assignments of such groups of activities to managers,the delegation of authority to carry them out,and provision for coordination of authority and informal relationships,horizontally and vertically,in the organisation structure.\n\n===Structure===\n\nThe framework in which the organization defines how tasks are divided, resources are deployed, and departments are coordinated.\n\n#A set of formal tasks assigned to individuals and departments.\n#Formal reporting relationships, including lines of authority, decision responsibility, number of hierarchical levels and span of managers control.\n#The design of systems to ensure effective coordination of employees across departments.\n\n=== Work specialization===\n\nWork specialization (also called [[division of labor]] or job specialization) is the degree to which organizational tasks are sub-divided into individual jobs. It may increase the efficiency of workers, but with too much specialization, employees may feel isolated and bored. Many organizations enlarge jobs or rotate assigned tasks to provide greater challenges.\n\n===Chain of command===\n[[Command hierarchy|Chain of command]] is the vertical lines of a command structure that is used for the purposes of overall responsibility and accountability in the achieving of stated goals and objectives through the use of orders one direction and reports of compliance in the other direction.  Chain of command differs from horizontal lines in an organization which are basically the communication and coordinating lines of the organization.\nChain of command(also referred to as 'scalar principle') states that a clear , unbroken chain of command should link every employee with someone at a higher level, all the way to the top of the organisation.\n\n===Authority, responsibility, and accountability===\n\n*'''Authority''' is a manager's formal and legitimate right to make decisions, issue orders, and allocate resources to achieve organizationally desired outcomes.\n*'''Responsibility''' means an employee's duty to perform assigned task or activities.\n*'''Accountability''' means that those with authority and responsibility must report and justify task outcomes to those above them in the chain of command.\n\n===Delegation===\n\nDelegation is the transfer of authority and/or responsibility to others, often lower in position. Delegation can improve flexibility to meet customers’ needs and adapt to competitive environments.\nPossible reasons for delegation:\n1. Efficiency - many people can complete a task faster than one/few\n2. Specialization - delegating simple tasks allows more important/complex tasks to be completed by the most qualified\n3. Training - delegating a task to a trainee so that they may learn from experience\n\n===Types of authority (and responsibility)===\n\n'''Line authority''' managers have the formal power to direct and control immediate subordinates. The superior issues orders and is responsible for the result and the subordinate obeys and is responsible only for executing the order according to instructions.\n\n'''Functional authority''' is where managers have formal power over a specific subset of activities. For instance, the Production Manager may have the line authority to decide whether and when a new machine is needed but the Controller demands that a Capital Expenditure Proposal is submitted first, showing that the investment will have a yield of at least x%; or, a legal department may have functional authority to interfere in any activity that could have legal consequences. This authority would not be functional but it would rather be staff authority if such interference is \"advice\" rather than \"order\".\n\n'''Staff authority''' is granted to staff specialists in their areas of expertise. It is not a real authority in the sense that a staff manager does not order or instruct but simply advises,  recommends, and counsels in the staff specialists' area of expertise and is responsible only for the quality of the advice (to be in line with the respective professional standards etc.) It is a communication relationship with [[management]].  It has an influence that derives indirectly from line authority at a higher level.\n\n''' Line and Staff Authority''' is the combination of Line organization and Staff organization.  Such organization follows both the principles of scalar chain of command and there is a provision for specialized activities  to be performed by staff officers who act in an advisory capacity\n\n===Span of management===\nCategories:\n*Direct single relationship.\n*Direct group relationships.\n*Cross relationship.\nFactors influencing larger span of management.\n#Work performed by subordinates is stable and routine.\n#Subordinates perform similar work tasks.\n#Subordinates are concentrated in a single location.\n#Subordinates are highly trained and need little direction in performing tasks.\n#Rules and procedures defining task activities are available.\n#Support systems and personnel are available for the managers.\n#Little time is required in non-supervisory activities such as coordination with other departments or planning.\n#Managers' personal preferences and styles favor a large span.\n\n===Tall versus flat structure===\n\n*'''Tall''' - A management structure characterized by an overall narrow span of management and a relatively large number of hierarchical levels. Tight control. [[Hierarchical organization|Reduced communication overhead]].\n*'''Flat''' - A management structure characterized by a wide span of control and relatively few hierarchical levels. Loose control. Facilitates delegation.\n\n===Centralization, decentralization, and formalization===\n\n*'''Centralization''' - The location of decision-making authority near top organizational levels.\n*'''Decentralization''' - The location of decision-making authority near lower organizational levels.\n*'''Formalization''' - The written documentation used to direct and control employees.\n\n===Departmentalization===\n\n[[Departmentalization]] is the basis on which individuals are grouped into departments and departments into total organizations. Approach options include:\n#'''Functional''' - by common skills and work task\n#'''Divisional''' - common product, program or geographical location\n#'''Matrix'''     - combination of Functional and Divisional\n#'''Team'''       - to accomplish specific tasks\n#'''Network'''    - departments are independent providing functions for a central core breaker\n\n===Importance of organizing===\n\n*Organizations are often troubled by how to organize, particularly when a new strategy is developed\n*Changing market conditions or new technology requires change\n*Organizations seek efficiencies through improvements in organizing\n\n==See also==\n* [[Order theory]]\n* [[Sorting]]\n* [[Community organizing]]\n* [[Union organizer]]\n* [[Professional organizer]]\n* [[The organization of the artist]]\n\n==References==\n{{wiktionary|organize}}\n{{Reflist}}\n\n[[Category:Ordering]]\n[[Category:Management]]\nCoase, Ronald (1937). \"The Nature of the Firm\" Economica, 4(16), pp. 386–405.\nHandy, Charles (1990). Inside Organizations: 21 Ideas for Managers. London: BBC Books. {{ISBN|978-0-563-20830-3}}.\nHandy, Charles (2005). Understanding Organizations (4th ed.). London: Penguin Books. {{ISBN|978-0-14-015603-4}}.\nHewlett, Roderic. (2006). The Cognitive leader. Rowman & Littlefield Pub Inc."
    },
    {
      "title": "Professional organizing",
      "url": "https://en.wikipedia.org/wiki/Professional_organizing",
      "text": "{{more citations needed|date=September 2014}}\n{{tone|date=February 2019}}\n[[Image:basement before.jpg|thumb|Basement, before tidying]]\n[[Image:Basement-after.jpg|thumb|Basement, after tidying]]\n'''Professional Organizing''' emerged as an industry in 1984 within Los Angeles.<ref name=\":0\" /> It intends to assist individuals and businesses to improve their organizing systems and process, for the purpose of improving quality of life, increasing personal productivity and achieving greater efficiency.\n\nThe [http://www.napo.net/ National Association of Productivity and Organizing Professionals]<ref>{{Cite web|url=http://www.napo.net/|title=National Association of Productivity and Organizing Professionals (NAPO)|website=www.napo.net|access-date=2018-01-22}}</ref> (NAPO) is recognised as the largest professional association within this industry, with approximately 3,500 members.<ref>{{cite web|title=About NAPO|url=http://www.napo.net/who|accessdate=19 December 2014}}</ref>\n\nThis industry has been popularised by a number of television programs produced on the subject, beginning with ''[[Life Laundry]]'' in 2002. This was followed by other programs, such as ''[[Clean Sweep]]'', [[Neat (TV series)|''Neat'']], ''[[Mission: Organization]]'', ''[https://www.tlc.com/tv-shows/hoarding-buried-alive/ Hoarding: Buried Alive,]'' ''[http://www.aetv.com/shows/hoarders Hoarders]'', [[imdbtitle:0381740|''Clean House'',]] and [[imdbtitle:0476624|NeatTV]]. Professional organizing has also been the basis of magazines like [https://www.realsimple.com/ ''Real Simple''.]\n\n== Definitions ==\nNAPO defines Professional Organizer and Productivity Consultant as follows:<ref name=\":0\">{{Cite web|url=http://www.napo.net/?page=about_aboutnapo|title=About NAPO – National Association of Productivity and Organizing Professionals (NAPO)|website=www.napo.net|access-date=2018-01-22}}</ref>\n* A '''Professional Organizer''' supports evaluation, decision-making, and action around objects, space, and data; helping clients achieve desired outcomes regarding function, order, and clarity. \n* A '''Productivity Consultant''' supports evaluation, decision-making, and action around time, energy, and resources; helping clients achieve desired outcomes regarding goals, effectiveness, and priorities.\n\n== Certifications and Credentials ==\nCertified Professional Organizers (CPOs) have proven industry proficiency by demonstrating they possess the body of knowledge and experience essential to professional organizing and productivity consulting. The CPO® credential identifies professional organizers who’ve documented a specific number of paid hours that include transferring organizing skills to the client, and passed the Board of Certification for Professional Organizers ([http://www.napo.net/?page=certification_cpo BCPO®]) examination. The credential provides the organizing and productivity industry a way to elevate its professional standards.<ref>{{Cite web|url=http://www.napo.net/?page=certification_cpo|title=Board of Certified Professional Organizers|last=|first=|date=|website=|archive-url=|archive-date=|dead-url=|access-date=}}</ref>\n\nThe National Association of Productivity and Organizing Professionals' education arm is known as [https://www.pathlms.com/napo NAPO University]. Currently NAPO University offers two [http://www.napo.net/?page=education_speccert Specialist Certificates]: Residential Organizing and Workplace Productivity.  A third Specialist Certificate in Life Transitions will be launched in early 2018. NAPO University also offers a [http://www.napo.net/page/education_bustrack Business Resources Track] to support Professional Organizers and Productivity Consultants in developing and growing their businesses.\n\n==Principles==\nProfessional Organizers achieve the goal of creating and maintaining organizational systems by teaching others the basic principals of organization.  Writer [http://www.juliemorgenstern.com/ Julie Morgenstern] suggests communicating these principals by using the acronym \"SPACE\", interpreted as: S=Sort, P=Purge, A=Assign a Home, C=Containerize and E=Equalize.<ref name=\"Morgenstern2004\">{{cite book|author= Julie Morgenstern|title= Organizing from the Inside Out, second edition: The Foolproof System For Organizing Your Home, Your Office and Your Life|url= https://books.google.com/books?id=TG2II7hRVfkC&pg=PA22|date= 1 September 2004|publisher= Henry Holt and Company|isbn= 978-1-4299-5536-2|pages= 22}}</ref> The last step (\"E\") consists in monitoring how the new system that has been created is working, adjusting it if needed, and maintaining it.  This principle is applicable to every type of organization.\n\nAs one of their main jobs, Professional Organizers help [[customer|clients]] reduce excessive clutter (paper, books, clothing, shoes, office supplies, home decor items, etc.) in the home or in the office.<ref>{{cite news|title= Personal Finance: It's time to tame paper tiger |author= Buck, Claudia |url= http://www.sacbee.com/2013/02/03/5159308/its-time-to-tame-paper-tiger.html |newspaper= The Sacramento Bee |date= February 3, 2013 |accessdate= June 3, 2013 |deadurl= yes |archiveurl= https://web.archive.org/web/20130513235632/http://www.sacbee.com/2013/02/03/5159308/its-time-to-tame-paper-tiger.html |archivedate= 13 May 2013 }}</ref> Professional Organizers endeavor to help individuals and business owners take control of their surroundings, their time, their paper, and their systems for life. Professional Organizers help redirect paradigms into more useful cross-applications that ensure properly co-sustainable futures for their clients' spaces and processes.\n\nProfessional Organizers offer a wide variety of services, from designing a functional [[closet]], to organizing a cross-country move.  For homeowners, a Professional Organizer might plan and reorganize the space of a room, improve paper management, or [[coaching|coach]] in [[time-management]], or [[goal-setting]].  In a [[business]] setting, Professional Organizers work closely with their clients to increase [[productivity]] by stream-lining paper-filing, electronic organization, and employee time-management.<ref>\n{{cite web\n|title=Our Profession\n|url= http://www.napo.net/our_profession/ |accessdate= 14 April 2013\n}}\n</ref>\n\n==See also==\n{{wikibookshas|[[Wikibooks:Housecleaning|Housecleaning]] as well as books on these subjects|\n* [[Wikibooks:Housecleaning/Indoor Litter|Housecleaning/Indoor Litter]]\n* [[Wikibooks:Do-It-Yourself/Home staging|Do-It-Yourself/Home staging]]\n}}\n\n{{div col|colwidth=30em}}\n*[[Adjustable shelving]]\n*[[Bookcase]]\n*[[Cabinetry]]\n*[[Filing cabinet]]\n*[[Kinesthetic sympathy]]\n*[[Mobile shelving]]\n*[[Personal organizer]]\n*[[Shelf (storage)]]\n*[[Small office/home office]]\n*[[Study (room)]]\n{{div col end}}\n<!-- please keep entries in alphabetical order -->\n\n'''Home:'''\n*[[Closet]]\n*[[Kitchen cabinet]]\n*[[Pantry]]\n*[[Wardrobe]]\n\n==References==\n<references />\n\n[[Category:Cabinets (furniture)]]\n[[Category:Clothing containers]]\n[[Category:Office equipment]]\n[[Category:Ordering]]"
    },
    {
      "title": "Stroke order",
      "url": "https://en.wikipedia.org/wiki/Stroke_order",
      "text": "{{Use mdy dates|date = February 2019}}\n{{Short description|Order in which CJKV characters should be written traditionally}}\n{{Use American English|date = February 2019}}[[File:筆-red.png|thumb|Stroke order for character 筆 shown by shade going from black to red]]\n[[File:順-red.png|thumb|Stroke order for each component (川 and 頁) of the character 順 shown by shade going from black to red]]\n{{Table Hanzi}}\n{{Infobox Chinese\n|s=笔顺\n|t=筆順\n|p=bǐshùn\n|y=bāt seuhn\n|j= bat<sup>1</sup> seon<sup>6</sup>\n| kanji = 筆順\n| revhep = hitsujun\n| kanji2 = 書き順\n| revhep2 = kaki-jun\n| hangul            = 필순\n| hanja             =筆順\n| rr                = pilsun\n| mr   = p'ilsun\n| hangul2            = 획순\n| hanja2            =劃順\n| rr2                = hoeksun\n| mr2   = hoeksun\n }}\n\n'''Stroke order''' refers to the order in which the [[Stroke (CJK character)|strokes]] of a [[Chinese character]] (or [[Chinese family of scripts#Adaptations for other languages|Chinese derivative character]]) are written. A stroke is a movement of a writing instrument on a writing surface. Chinese characters are used in various forms in [[Chinese language|Chinese]], [[Japanese language|Japanese]], [[Korean language|Korean]] and formerly [[Vietnamese language|Vietnamese]]. They are known as ''[[Hanzi]]'' in ([[Mandarin Chinese|Mandarin]]) Chinese, ''[[kanji]]'' in Japanese, ''[[Hanja]]'' in Korean and ''[[:vi:Chữ_Hán|Chữ Hán]]'' in Vietnamese. Stroke order is also attested in other [[logogram | logographic scripts]], e.g. [[cuneiform]].<ref>{{cite journal|last1=Bramanti|first1=Armando|title=Rethinking the Writing Space: Anatomy of Some Early Dynastic Signs|journal=Current Research in Cuneiform Palaeography. Proceedings of the Workshop organised at the 60ᵗʰ Rencontre Assyriologique Internationale, Warsaw 2014, pp. 31-47|date=2015|url=https://www.academia.edu/21411909/2015._Rethinking_the_Writing_Space_Anatomy_of_Some_Early_Dynastic_Signs}}</ref>\n\n==Basic principles==\n[[Chinese character]]s are basically [[logogram]]s constructed with strokes. Over the millennia a set of generally agreed rules have been developed by custom. Minor variations exist between countries, but the basic principles remain the same, namely that writing characters should be economical, with the fewest hand movements to write the most strokes possible. This promotes writing speed, accuracy, and readability. This idea is particularly important since as learners progress, characters often get more complex. Since stroke order also aids learning and memorization, students are often taught about it from a very early age in schools and encouraged to follow them. \n\nThe [[Eight Principles of Yong]] (永字八法 [[Pinyin]]: ''yǒngzì bā fǎ''; Japanese: ''eiji happō''; Korean: 영자팔법, ''yeongjapalbeop'', ''yŏngjap'albŏp'') uses the single character {{wiktmul|永}}, meaning \"eternity\", to teach eight of the most basic strokes in [[Regular script|Regular Script]].\n\n== Stroke order per style ==\n{{multiple image\n | footer    = The character {{Linktext|馬}} (\"[[horse]]\") in different [[Chinese script styles|script styles]]\n | align     = center\n | image1    = 馬-oracle.svg\n | width1    = 60\n | caption1  = [[Oracle bone script|Jiǎgǔwén]]\n | image2    = 馬-bronze.svg\n | width2    = 60\n | caption2  = [[Chinese bronze inscriptions|Jīnwén]]\n | image3    = 馬-bigseal.svg\n | width3    = 60\n | caption3  = [[Large Seal Script|Dàzhuàn]]\n | image4    = 馬-seal.svg\n | width4    = 60\n | caption4  = [[Small Seal Script|Xiǎozhuàn]]\n | image5    = 馬-clerical.svg\n | width5    = 60\n | caption5  = [[Clerical script|Lìshū]]\n | image6    = 馬-xingshu.svg\n | width6    = 60\n | caption6  = [[Semi-cursive script|Xíngshū]]\n | image7    = 馬-caoshu.svg\n | width7    = 60\n | caption7  = [[Cursive script (East Asia)|Cǎoshū]]\n | image8    = 馬-kaishu.svg\n | width8    = 60\n | caption8  = [[Regular script|Kǎishū]] ([[Traditional Chinese characters|trad.]])\n | image9    = 马-kaishu.svg\n | width9    = 60\n | caption9  = [[Regular script|Kǎishū]] ([[Simplified Chinese character|simp.]])\n}}\n\n===Ancient China===\nIn [[ancient China]], the [[Oracle bone script|Jiǎgǔwén characters]] carved on [[ox]] [[scapula]] and [[tortoise]] [[plastrons]] showed no indication of stroke order. The characters show huge variations from piece to piece, sometimes even within one piece. During the divination ceremony, after the cracks were made, the characters were written with a brush on the shell or bone (to be carved in a workshop later). Although the brush-written stroke order is not discernible after carving, there exists some evidence that it was not entirely idiosyncratic: a few of the characters, often marginal administrative notations recording the provenance of the shells or bones, were not later recarved, and the stroke order of these characters tends to resemble traditional and modern stroke order.<ref name=\"Keightley 1978\">{{harvnb|Keightley|1978}}</ref> For those characters (the vast majority) which were later engraved into the hard surface using a knife, perhaps by a separate individual, there is evidence (from incompletely engraved pieces) that in at least some cases all the strokes running one way were carved, then the piece was turned, and strokes running another way were then carved.<ref name=\"Keightley 1978\" />\n\n===Imperial China===\nIn early [[Imperial era of Chinese history|Imperial China]], the common script was the [[Xiaozhuan|Xiaozhuan style]]. About 220 BC, the emperor [[Qin Shi Huang]], the first to conquer all of China, imposed [[Li Si]]'s character uniformisation, a set of 3300 standardized ''Xiǎozhuàn'' characters.<ref>{{harvnb|Fazzioli|1987|p=13|quote=\"And so the first Chinese dictionary was born, the ''S&#x101;n Ch&#x101;ng'', containing {{formatnum:3300}} characters\"}}</ref> Its graphs on old steles — some dating from 200 BC — reveal indications of the stroke order of the time. However, stroke order could still not yet be ascertained from the steles, and no paper from that time is extant.\n\nThe true starting point of stroke order is the [[Clerical script|Lìshū style]] (clerical script) which is more regularized, and in some ways similar to modern text. In theory, by looking at the Lìshū style steles' graphs and the placement of each stroke, one can see hierarchical priority between the strokes, which indicates the stroke order used by the calligrapher or stele sculptors.{{Citation needed|date=June 2007}}\n\n[[Regular script|Kǎishū style]] (regular script) — still in use today —  is more regularized, allowing one to more easily guess the stroke order used to write on the steles. The stroke order 1000 years ago was similar to that toward the end of Imperial China.{{Citation needed|date=June 2007}} For example, the stroke order of <big>广</big> is clear in the [[Kangxi dictionary]] of 1716; but in a modern book, the official stroke order (the same) will not appear clearly. The Kangxi and current shapes have tiny differences, while current stroke order is still the same, according to the old style.<ref>{{harvnb|Kangxi|1716|p=41}} See by example the radicals <big>卩</big>, <big>厂</big> or <big>广</big>. The 2007 common shape for those characters don't allow clearly to \"guess\" the stroke order, but old versions, visible on the Kangxi Zidian p.41 clearly allow us to guess the stroke order.</ref> However, the stroke orders implied by the Kangxi dictionary are not necessarily similar to nowadays' norm.\n\n===Cursive styles and hand-written styles===\nCursive styles such as ''[[Semi-cursive script|Xíngshū]]'' (semi-cursive or running script) and ''[[Grass script|Cǎoshū]]'' (cursive or grass script) show stroke order more clearly than Regular Script, as each move made by the writing tool is visible.\n\n== Stroke order per polity ==\n{| class=\"wikitable\" style=\"float: right;\"\n|-\n| colspan=\"4\" style=\"text-align: center;\" | '''Different stroke orders of the character [[wikt:必|必]].'''\n|-\n| style=\"text-align: center;\" | [[File:必-order.gif|100px|center]]<br>Traditional|| style=\"text-align: center;\" | [[File:必-torder.gif|100px|center]]<br>ROC & Hong Kong || style=\"text-align: center;\" | [[File:必-jorder.gif|100px|center]]<br>Japan || style=\"text-align: center;\" | [[File:必-aorder.gif|100px|center]]<br>PRC\n|}\nThe modern governments of mainland China, Hong Kong,<ref>{{cite web|url=http://www.edbchinese.hk/lexlist_ch/|title=香港小學學習字詞表|author=|date=|website=www.edbchinese.hk|accessdate=19 April 2018}}</ref> Taiwan,<ref>{{cite web|url=http://stroke-order.learningweb.moe.edu.tw/character.do|title=常用國字標準字體筆順學習網|author=|date=|website=stroke-order.learningweb.moe.edu.tw|accessdate=19 April 2018}}</ref> and Japan<ref>{{cite web|url=https://kakijun.jp/|title=漢字の正しい書き順(筆順)|first=|last=kakijun|date=|website=漢字の正しい書き順(筆順)|accessdate=19 April 2018}}</ref> have [[standardization|standardized]] official stroke orders to be taught in schools. These stroke order standards are prescribed in conjunction to each government's standard character sets. The various official stroke orders agree on the vast majority of characters, but each have their differences. No governmental standard matches traditional stroke orders completely. The differences between the governmental standards and traditional stroke orders arise from accommodation for schoolchildren who may be overwhelmed if the rules about stroke orders are too detailed, or if there are too many exceptions.{{Citation needed|date=February 2012}} The differences listed below are not exhaustive.\n* '''Traditional stroke order''': Widely used in [[Imperial China]], currently used in the [[Chinese cultural sphere]] secondary to each region's governmental standards. Practiced mainly by informed scholars of calligraphy. Also called \"calligraphic\" stroke order. These stroke orders are established by study of handwritten documents from pre-Republic China, especially those of notable calligraphers. These stroke orders are most conservative regarding etymology, character construction, character evolution, and tradition. Many characters have more than one stroke correct [[Variant Chinese character|form]]. Stroke orders may vary depending on the [[Chinese script styles|script style]]. Unlike the other standards, this is not a governmental standard.\n* '''Japanese stroke order''': Prescribed mostly in modern [[Japan]]. The standard character set of the [[Ministry of Education, Culture, Sports, Science and Technology|MEXT]] is the [[Jōyō kanji]], which contains many characters [[shinjitai|reformed]] in 1946. The MEXT lets editors freely prescribe a character's stroke order, which all should \"follow commonsensical orders which are widely accepted in the society\"{{Citation needed|date=November 2010}}. This standard diverges from the traditional stroke order in that the two sides of the grass radical ([[wikt:艹|艹]]) are joined, and written with three strokes. Also, this standard is influenced by [[semi-cursive script]], leading to some vertical strokes to precede intersecting horizontal strokes if the vertical stroke does not pass through the lowest horizontal stroke, as in 隹 and 生. 必 is written with the top dot first, while the traditional stroke order writes the 丿 first.\n* '''Taiwan stroke order''' ({{harvnb|Li|al.|1995}}): Prescribed mostly in modern [[Taiwan]]. The standard character set of the [[Ministry of Education (Republic of China)|ROC Ministry of Education]] is the [[Standard Form of National Characters]]. This standard diverges from the traditional stroke order in that the upper-right dot of the 戈 component is written second to last. The vertical stroke in [[wikt:忄|忄]] is written second. 成 starts with the horizontal. Also, the [[wikt:𠂇|𠂇]] component, as seen in 左 and 右, is written with the horizontal stroke first in all instances, while the traditional stroke order differentiates the stroke order of [[wikt:𠂇|𠂇]] according to etymology and character structure.\n* '''Mainland China stroke order''': Prescribed mostly in modern [[Mainland China]]. In 1956, the government of the PRC introduced many newly created characters and substitutions, called [[Simplified Chinese character]]s, which form part of the [[Ministry of Education of the People's Republic of China|PRC Ministry of Education]]'s standard character set, the [[Xiàndài Hànyǔ Chángyòng Zìbiǎo]]. This in turn reformed the stroke order of many characters. Besides these characters, this standard diverges from the traditional stroke order in characters with the [[wikt:艹|艹]] radical, merging both sides like the Japanese standard. Also, the horizontal stroke of the [[wikt:𠂇|𠂇]] component is written first in all instances. 乃 ends with 丿. 成 starts with the horizontal. In 1997, the [[Ministry of Education of the People's Republic of China|PRC Ministry of Education]] published the official stroke order standard for commonly used characters.\n* '''Hong Kong stroke order''': Prescribed mostly in modern [[Hong Kong]]. The standard character set of the Hong Kong [[Education Bureau]] is the [[List of Forms of Frequently Used Characters]]. In this standard, [[wikt:艹|艹]] is written vertical-horizontal-vertical-horizontal, instead of the traditional vertical-horizontal-horizontal-vertical. 成 starts with the horizontal.\n\n== Alternative stroke orders ==\nBesides general errors and regional differences in stroke order, it is common in the PRC to apply alternative stroke orders which resemble PRC stroke orders to Traditional Chinese characters, although the PRC generally uses Simplified characters.{{Citation needed|date=March 2017}} In the below example, the traditional character 門 (simplified: 门) is shown with both the traditional stroke order (left, starting with the left vertical stroke), as in imperial China, Taiwan, Japan, and Hong Kong, and with the Simplified stroke order (right, with the left vertical stroke fourth).\n<gallery>\nFile:門-order.gif|Traditional 門, traditional stroke order.\nFile:門-aorder.gif|Traditional 門, PRC stroke order.<ref>[http://guide.wenlininstitute.org/wenlin4.3/Stroking_Characters#How_Standard_is_the_Standard_Stroke_Order.3F Wenlin Institute: How Standard is the Standard Stroke Order?]</ref>\nFile:门-jorder.gif|Simplified 门, traditional stroke order, comes from [[Cursive script (East Asia)|Cursive script]].\nFile:门-order.gif|Simplified 门, PRC stroke order.\n</gallery>\n\n== General guidelines ==\n'''''Note:''' There are exceptions within and among different standards. The following are only guidelines.''\n\n'''1. Write from top to bottom, and left to right.''' [[File:三-order.gif|40px]]\n\nAs a general rule, strokes are written from top to bottom and left to right. For example, among the first characters usually learned is the number one, which is written with a single horizontal line: 一. This character has one stroke which is written from left to right.\n\nThe character for \"two\" has two strokes: 二. In this case, both are written from left to right, but the top stroke is written first. The character for \"three\" has three strokes: 三. Each stroke is written from left to right, starting with the uppermost stroke.\n[[File:人-red.png|thumb|200px|right|The Chinese character meaning \"person\" ([[File:人-order.gif|20px|人 animation]], [[Mandarin Chinese]]: ''rén'', [[Cantonese|Cantonese Chinese]]: ''yàhn'', [[Korean language|Korean]]: ''in'', [[Japanese language|Japanese]]: ''hito'', ''nin; jin'').  The character has two strokes, the first shown here in dark, and the second in red.  The black area represents the starting position of the writing instrument.]]\nThis rule also applies to the order of components. For example, 校 can be divided into two. The entire left side (木) is written before the right side (交). There are some exceptions to this rule, mainly occurring when the right side of a character has a lower enclosure (see below).\n\nWhen there are upper and lower components, the upper components are written first, then the lower components, as in 品 and 星.\n\n'''2. Horizontal before vertical''' [[File:十-order.gif|40px]]\n\nWhen horizontal and vertical strokes cross, horizontal strokes are usually written before vertical strokes: the character for \"ten,\" 十, has two strokes. The horizontal stroke 一 is written first, followed by the vertical stroke <span style=\"color:red; font-size:100%;\">→</span> 十.\n\nIn the Japanese standard, a vertical stroke may precede many intersecting horizontal strokes if the vertical stroke does not pass through the lowest horizontal stroke.\n\n'''3. Character-spanning strokes last''' [[File:聿-order.gif|40px]]\n\nVertical strokes that pass through many other strokes are written after the strokes through which they pass, as in 聿 and 弗.\n\nHorizontal strokes that pass through many other strokes are written last, as in 毋 and 舟.\n\n'''4. Diagonals right-to-left before diagonals left-to-right''' [[File:文-order.gif|40px]]\n\nRight-to-left diagonals (丿) are written before left-to-right diagonals (乀): 文.\n\nNote that this is for symmetric diagonals; for asymmetric diagonals, as in 戈, the left-to-right may precede the right-to-left, based on other rules.\n\n'''5. Center before outside in vertically symmetrical characters''' [[File:水-order.gif|40px]]\n\nIn vertically symmetrical characters, the center components are written before components on the left or right. Components on the left are written before components on the right, as in 兜 and 承.\n\n'''6. Enclosures before contents''' [[File:回-order.gif|40px]] [[File:国-order.gif|40px]]\n\nOutside enclosing components are written before inside components; bottom strokes in the enclosure are written last if present, as in 日 and 口. (A common mnemonic is \"Put people inside first, then close the door.\") Enclosures may also have no bottom stroke, as in 同 and 月.\n\n'''7. Left vertical before enclosing''' [[File:口-order.gif|40px]]\n\nLeft vertical strokes are written before enclosing strokes. In the following two examples, the leftmost vertical stroke (|) is written first, followed by the uppermost and rightmost lines (┐) (which are written as one stroke): 日 and 口.\n\n'''8. Bottom enclosures last''' [[File:道-order.gif|40px]]\n\nBottom enclosing components are usually written last: 道, 建, 凶.\n\n'''9. Dots and minor strokes last''' [[File:玉-order.gif|40px]]\n\nMinor strokes are usually written last, as the small \"dot\" in the following: 玉, 求, 朮.\n\n== Representations ==\nThere are various ways to describe the stroke order of a character. Children learn the stroke order in courses, as part of writing learning. Various graphical representations are possible, most notably successive images of the character with one more stroke added (or changing color) each time, numbering strokes, color-coding, fanning,<ref>{{cite web|url=http://www.lri.fr/~dragice/strokefanning/ |title=Stroke Fanning |website=Lri.fr |date= |accessdate=2017-01-16}}</ref> and more recently animations. Stroke order is often described in person by writing characters on paper or in the air.\n\n==See also==\n{{Commons category|CJK stroke order}}\n* [[Chinese character]]\n* [[Chinese characters description languages]]\n* [[CJK strokes]]\n* [[Horizontal and vertical writing in East Asian scripts]]\n* [[Radical (Chinese character)]]\n\n==Notes==\n{{Reflist}}\n\n==References==\n;Traditional stroke order\n* {{Citation|title=歐楷解析 |author=田蘊章 |year=2004 |publisher=天津人民美術出版社|location=天津|isbn=7-5305-2587-5}}\n* {{Citation|author=田其湜|title=六体书法大字典|publisher=湖南人民出版社|isbn=7-5438-3668-8}}\n\n;ROC stroke order\n* {{Citation|last1=Li|first1=Xian (李鍌)|last2=al. |year=1995 |title=常用國字標準字體筆順手冊 (ROC-Taiwan standard shapes and stroke orders of commonly used characters) |publisher=Taiwan Ministry of Education |url=http://language.moe.gov.tw/001/Upload/files/SITE_CONTENT/M0001/BISHUEN/c8.htm |isbn=957-00-7082-X}} (Authoritative)\n\n;PRC stroke order\n* {{Citation|title=现代汉语通用字笔顺规范 (PRC-China modern Chinese commonly used characters standard stroke orders)|author=国家语言文字工作委员会标准化工作委员会 |year=1997 |page= 453|publisher=语文出版社 (Language and Literature Press) |location=Beijing |isbn=7-80126-201-8 |url=http://www.moe.gov.cn/s78/A19/yxs_left/moe_810/s230/201001/t20100115_75615.html |accessdate=2018-03-12}} 现代汉语通用字笔顺规范 (PRC-China modern Chinese commonly used characters standard stroke orders) (Authoritative)\n\n;Japanese\n* {{Citation|last1=|first1=|title= 筆順指導の手びき (Hitsujun shidō no tebiki)|year=1958}} (Authoritative from 1958 to 1977).\n* Hadamitzky, Wolfgang & Mark Spahn.  ''A Handbook of the Japanese Writing System''.  Charles E. Tuttle Co.  {{ISBN|0-8048-2077-5}}.\n* Henshall, Kenneth G. ''A Guide to Remembering Japanese Characters''. Charles E. Tuttle Co.  {{ISBN|0-8048-2038-4}}.\n* O'Neill, P.G.  ''Essential Kanji: 2,000 Basic Japanese Characters Systematically Arranged for Learning and Reference''.  Weatherhill.  {{ISBN|0-8348-0222-8}}.\n* {{citation|last=Pye|first=Michael|title=The Study of Kanji: A Handbook of Japanese Characters|publisher=Hokuseido Press|isbn=0-89346-232-2}}\n** Includes a translation of the Japanese Ministry of Education rules on Kanji stroke order.\n\n;Hong Kong\n* {{Citation|author=|year=|title=香港小學學習字詞表 (Hong Kong Chinese Lexical Lists for Primary Learning)|publisher=Chinese Language Education Section, Curriculum Development Institute, Education Bureau, The Government of the Hong Kong Special Administrative Region}}. Online version available at http://www.edbchinese.hk/lexlist_ch/. (Print version is official curriculum supporting material.)\n\n;Archaic characters\n* {{Citation|last1=Keightley|first1=David N.|year=1978|title=Sources of Shang History: The Oracle-Bone Inscriptions of Bronze Age China|publisher=University of California Press|place=Berkeley|isbn=0-520-02969-0}}\n\n;Other issues\n* {{citation|last=Fazzioli |first= Edoardo |others= calligraphy by Rebecca Hon Ko |title= Chinese calligraphy : from pictograph to ideogram : the history of 214 essential Chinese/Japanese characters |year= 1987 |publisher= [[Abbeville Publishing Group (Abbeville Press, Inc.)|Abbeville Press]] |location= New York |isbn= 0-89659-774-1 }}\n* {{citation|author=Kangxi|title=康熙字典 Kangxi Zidian|year=1716|url=http://www.kangxizidian.com}}\n\n==External links==\n<!-- {{Cleanup-spam|section|date=|date=May 2012}} -->\n{{Commons|Category:Order.gif stroke order images|stroke order animations in GIF format}}\n;PRC\n*[http://www.csulb.edu/~txie/azi/page1.htm Animated stroke order], from the California State University, Long Beach\n*[http://www.learnchineseok.com/2011/05/write-chinese-character-stroke-order.html LearnchineseOK.com] Web resources, instructions and animations for Chinese character stroke order\n*[https://www.chineseconverter.com/en/convert/chinese-stroke-order-tool Animated stroke order for Chinese characters] Stroke Order tool with option to see many characters' stroke order at once.\n\n;ROC\n*[http://stroke-order.learningweb.moe.edu.tw Learning Program for Stroke Order of Frequently Used Chinese Characters (常用國字標準字體筆順學習網)] with animated stroke order, by the Ministry of Education, R.O.C. (Taiwan).\n\n;Hong Kong\n*[http://www.edbchinese.hk/lexlist_en/index.htm 中英對照香港學校中文學習基礎字詞] - Lexical Items with English Explanations for Fundamental Chinese Learning in Hong Kong Schools, by the Hong Kong [[Education Bureau]]\n*[http://www.cchar.com/education/hong-kong-student/chinese-for-hong-kong-student.php 香港標準字形及筆順] - stroke orders following the Hong Kong Education Bureau's List of Commonly Used Characters\n\n;Japanese\n*[http://www.infomongolia.com/lesson-category/67 Learning Japanese Kanji], a free application for the 1st Grade Kanji. \n*[http://infohost.nmt.edu/~armiller/japanese/strokeorder.htm Kanji Stroke Order], from the Engineering Department of New Mexico Tech, Socorro.\n*[http://kanjialive.com Kanji alive], a free web application for learning Japanese [[kanji]] with stroke order animations.\n*[http://www.kanjicafe.com/license.htm SODER Project], 1,513 Japanese [[kanji]] stroke order diagrams and animations, freely downloadable under license.\n*[http://kakijun.jp/ Kakijun] Kanji stroke order animations.{{ja icon}}\n*[http://www.nihilist.org.uk Kanji stroke order font], Japanese [[kanji]] stroke order diagrams presented as a TrueType font.\n\n;Korean\n*[http://hanja.naver.com/ 한자사전(漢字辭典) with stroke order diagrams] {{ko icon}}\n\n{{Portal bar|Language}}\n\n{{DEFAULTSORT:Stroke Order}}\n\n\n[[Category:Chinese characters]]\n[[Category:East Asian calligraphy]]\n[[Category:Japanese writing system]]\n[[Category:Korean writing system]]\n[[Category:Ordering]]\n[[Category:Orthography]]"
    },
    {
      "title": "Collation",
      "url": "https://en.wikipedia.org/wiki/Collation",
      "text": "{{about|collation in library, information, and computer science}}\n{{refimprove|date=March 2019}}\n'''Collation''' is the assembly of written information into a standard order. Many systems of collation are based on [[number|numerical order]] or [[alphabetical order]], or extensions and combinations thereof. Collation is a fundamental element of most office [[library classification|filing systems]], [[library catalog]]s, and [[reference book]]s.\n\nCollation differs from ''[[document classification|classification]]'' in that classification is concerned with arranging information into logical categories, while collation is concerned with the ordering of items of information, usually based on the form of their [[identifier]]s. Formally speaking, a collation method typically defines a [[total order]] on a set of possible identifiers, called [[sort key]]s, which consequently produces a [[total preorder]] on the set of items of information (items with the same identifier are not placed in any defined order).\n\nA collation algorithm such as the [[Unicode collation algorithm]] defines an order through the process of comparing two given [[character string]]s and deciding which should come before the other. When an order has been defined in this way, a ''[[sorting algorithm]]'' can be used to put a list of any number of items into that order.\n\nThe main advantage of collation is that it makes it fast and easy for a user to find an element in the list, or to confirm that it is absent from the list. In automatic systems this can be done using a [[binary search algorithm]] or [[interpolation search]]; manual searching may be performed using a roughly similar procedure, though this will often be done unconsciously. Other advantages are that one can easily find the first or last elements on the list (most likely to be useful in the case of numerically sorted data), or elements in a given range (useful again in the case of numerical data, and also with alphabetically ordered data when one may be sure of only the first few letters of the sought item or items).\n\n==Numerical and chronological order==\nStrings representing [[number]]s may be sorted based on the values of the numbers that they represent. For example, \"−4\", \"2.5\", \"10\", \"89\", \"30,000\". Note that pure application of this method may provide only a partial ordering on the strings, since different strings can represent the same number (as with \"2\" and \"2.0\" or, when [[scientific notation]] is used, \"2e3\" and \"2000\").\n\nA similar approach may be taken with strings representing [[calendar date|dates]] or other items that can be ordered chronologically or in some other natural fashion.\n\n==Alphabetical order==\n{{Main article|Alphabetical order}}\n[[Alphabetical order]] is the basis for many systems of collation where items of information are identified by strings consisting principally of [[letter (alphabet)|letters]] from an [[alphabet]]. The ordering of the strings relies on the existence of a standard ordering for the letters of the alphabet in question. (The system is not limited to alphabets in the strict technical sense; languages that use a [[syllabary]] or [[abugida]], for example [[Cherokee language|Cherokee]], can use the same ordering principle provided there is a set ordering for the symbols used.)\n\nTo decide which of two strings comes first in alphabetical order, initially their first letters are compared. The string whose first letter appears earlier in the alphabet comes first in alphabetical order. If the first letters are the same, then the second letters are compared, and so on, until the order is decided. (If one string runs out of letters to compare, then it is deemed to come first; for example, \"cart\" comes before \"carthorse\".) The result of arranging a set of strings in alphabetical order is that words with the same first letter are grouped together, and within such a group words with the same first two letters are grouped together, and so on.\n\n[[Capital letter]]s are typically treated as equivalent to their corresponding lowercase letters. (For alternative treatments in computerized systems, see [[#Automated collation|Automated collation]], below.)\n\nCertain limitations, complications, and special conventions may apply when alphabetical order is used:\n*When strings contain [[space (character)|spaces]] or other word dividers, the decision must be taken whether to ignore these dividers or to treat them as symbols preceding all other letters of the alphabet. For example, if the first approach is taken then \"car park\" will come after \"carbon\" and \"carp\" (as it would if it were written \"carpark\"), whereas in the second approach \"car park\" will come before those two words. The first rule is used in many (but not all) [[dictionary|dictionaries]], the second in [[telephone directory|telephone directories]] (so that Wilson, Jim K appears with other people named Wilson, Jim and not after Wilson, Jimbo).\n*Abbreviations may be treated as if they were spelt out in full. For example, names containing \"St.\" (short for the English word ''[[Saint]]'') are often ordered as if they were written out as \"Saint\". There is also a traditional convention in English that surnames beginning ''Mc'' and ''M''' are listed as if those prefixes were written ''Mac''.\n*Strings that represent personal names will often be listed by alphabetical order of surname, even if the [[given name]] comes first. For example, Juan Hernandes and Brian O'Leary should be sorted as \"Hernandes, Juan\" and \"O'Leary, Brian\" even if they are not written this way.\n*Very common initial words, such as ''The'' in English, are often ignored for sorting purposes. So ''[[The Shining (novel)|The Shining]]'' would be sorted as just \"Shining\" or \"Shining, The\".\n*When some of the strings contain [[numerical digit|numerals]] (or other non-letter characters), various approaches are possible. Sometimes such characters are treated as if they came before or after all the letters of the alphabet. Another method is for numbers to be sorted alphabetically as they would be spelled: for example ''[[1776 (film)|1776]]'' would be sorted as if spelled out \"seventeen seventy-six\", and ''[[24 heures du Mans]]'' as if spelled \"vingt-quatre...\" (French for \"twenty-four\"). When numerals or other symbols are used as special graphical forms of letters, as in ''1337'' for [[leet]] or ''Se7en'' for the movie title ''[[Seven (1995 film)|Seven]]'', they may be sorted as if they were those letters.\n*Languages have different conventions for treating [[modified letter]]s and certain letter combinations. For example, in [[Spanish language|Spanish]] the letter ''ñ'' is treated as a basic letter following ''n'', and the [[digraph (orthography)|digraphs]] ''ch'' and ''ll'' were formerly (until 1994) treated as basic letters following ''c'' and ''l'', although they are now alphabetized as two-letter combinations. A list of such conventions for various languages can be found at {{slink|Alphabetical order|Language-specific conventions}}.\n\nIn several languages the rules have changed over time, and so older dictionaries may use a different order than modern ones. Furthermore, collation may depend on use. For example, German [[Dictionary|dictionaries]] and [[telephone directory|telephone directories]] use different approaches.\n\n==Radical-and-stroke sorting==\n:''See also [[Chinese characters#Indexing|Indexing of Chinese characters]]''\nAnother form of collation is '''radical-and-stroke sorting''', used for non-alphabetic writing systems such as the [[hanzi]] of [[Chinese language|Chinese]] and the [[kanji]] of [[Japanese language|Japanese]], whose thousands of symbols defy ordering by convention. In this system, common components of characters are identified; these are called [[radical (Chinese character)|radicals]] in Chinese and logographic systems derived from Chinese. Characters are then grouped by their primary radical, then ordered by number of pen strokes within radicals. When there is no obvious radical or more than one radical, convention governs which is used for collation. For example, the Chinese character 妈 (meaning \"mother\") is sorted as a six-stroke character under the three-stroke primary radical 女.\n\nThe radical-and-stroke system is cumbersome compared to an alphabetical system in which there are a few characters, all unambiguous. The choice of which components of a logograph comprise separate radicals and which radical is primary is not clear-cut. As a result, logographic languages often supplement radical-and-stroke ordering with alphabetic sorting of a phonetic conversion of the logographs. For example, the kanji word ''[[Tokyo|Tōkyō]]'' (東京) can be sorted as if it were spelled out in the Japanese characters of the [[hiragana]] syllabary as \"to-u-ki-<sub>yo</sub>-u\" (とうきょう), using the conventional sorting order for these characters.{{citation needed|date=October 2012}}\n\nIn addition, in Greater China, [[surname stroke order]]ing is a convention in some official documents where people's names are listed without hierarchy.\n\nThe radical-and-stroke system, or some similar pattern-matching and stroke-counting method, was traditionally the only practical method for constructing dictionaries that someone could use to look up a logograph whose pronunciation was unknown. With the advent of computers, dictionary programs are now available that allow one to handwrite a character using a mouse or stylus.{{citation needed|date=October 2012}}\n\n==Automated collation==\nWhen information is stored in digital systems, collation may become an automated process. It is then necessary to implement an appropriate collation [[algorithm]] that allows the information to be sorted in a satisfactory manner for the application in question. Often the aim will be to achieve an alphabetical or numerical ordering that follows the standard criteria as described in the preceding sections. However, not all of these criteria are easy to automate.<ref name=\"Walters\">[https://books.google.com/books?id=5Pd_iFM4eLsC&pg=PA278&dq=%22collation+algorithms%22&hl=pl&sa=X&ei=2k-yT_GVEIrP4QSSx428CQ&redir_esc=y#v=onepage&q=%22collation%20algorithms%22&f=false ''M Programming: A Comprehensive Guide''], Richard F. Walters, Digital Press, 1997</ref>\n\nThe simplest kind of automated collation is based on the numerical codes of the symbols in a [[character set]], such as [[ASCII]] coding (or any of its [[superset]]s such as [[Unicode]]), with the symbols being ordered in increasing numerical order of their codes, and this ordering being extended to strings in accordance with the basic principles of alphabetical ordering (mathematically speaking, [[lexicographical order]]ing). So a computer program might treat the characters ''a'', ''b'', ''C'', ''d'', and ''$'' as being ordered ''$'', ''C'', ''a'', ''b'', ''d'' (the corresponding ASCII codes are ''$'' = 36, ''a'' = 97, ''b'' = 98, ''C'' = 67, and ''d'' = 100). Therefore, strings beginning with ''C'', ''M'', or ''Z'' would be sorted before strings with lower-case ''a'', ''b'', etc. This is sometimes called ''[[ASCIIbetical order]]''. This deviates from the standard alphabetical order, particularly due to the ordering of capital letters before all lower-case ones (and possibly the treatment of spaces and other non-letter characters). It is therefore often applied with certain alterations, the most obvious being case conversion (often to uppercase, for historical reasons<ref group=\"note\">Historically, computers only handled text in uppercase (this dates back to [[telegraph]] conventions).</ref>) before comparison of ASCII values.\n\nIn many collation algorithms, the comparison is based not on the numerical codes of the characters, but with reference to the '''collating sequence''' – a sequence in which the characters are assumed to come for the purpose of collation – as well as other ordering rules appropriate to the given application. This can serve to apply the correct conventions used for alphabetical ordering in the language in question, dealing properly with differently cased letters, [[modified letter]]s, [[digraph (orthography)|digraphs]], particular abbreviations, and so on, as mentioned above under [[#Alphabetical order|Alphabetical order]], and in detail in the [[Alphabetical order]] article. Such algorithms are potentially quite complex, possibly requiring several passes through the text.<ref name=\"Walters\"/>\n\nProblems are nonetheless still common when the algorithm has to encompass more than one language. For example, in [[German (language)|German]] dictionaries the word ''ökonomisch'' comes between ''offenbar'' and ''olfaktorisch'', while [[Turkish (language)|Turkish]] dictionaries treat ''o'' and ''ö'' as different letters, placing ''oyun'' before ''öbür''.\n\nA standard algorithm for collating any collection of strings composed of any standard [[Unicode]] symbols is the [[Unicode Collation Algorithm]]. This can be adapted to use the appropriate collation sequence for a given language by tailoring its default collation table. Several such tailorings are collected in [[Common Locale Data Repository]].\n\n===Sort keys===\n{{redirect|Sort key|sort keys in Wikipedia|WP:SORTKEY}}\nIn some applications, the strings by which items are collated may differ from the identifiers that are displayed. For example, ''The Shining'' might be sorted as ''Shining, The'' (see [[#Alphabetical order|Alphabetical order]] above), but it may still be desired to display it as ''The Shining''. In this case two sets of strings can be stored, one for display purposes, and another for collation purposes. Strings used for collation in this way are called ''sort keys''.\n\n===Issues with numbers===\nSometimes, it is desired to order text with embedded numbers using proper numerical order. For example, \"Figure 7b\" goes before \"Figure 11a\", even though '7' comes after '1' in [[Unicode]]. This can be extended to [[Roman numeral]]s. This behavior is not particularly difficult to produce as long as only integers are to be sorted, although it can slow down sorting significantly. For example, [[Microsoft Windows]] does this when sorting [[file name]]s.\n\nSorting decimals properly is a bit more difficult, because different locales use different symbols for a [[decimal separator|decimal point]], and sometimes the same character used as a [[Decimal mark|decimal point]] is also used as a separator, for example \"Section 3.2.5\". There is no universal answer for how to sort such strings; any rules are application dependent.\n\nAscending order of numbers differs from alphabetical order, e.g. 11 comes alphabetically before 2. This can be fixed with [[leading zero]]s: 02 comes alphabetically before 11. See e.g. [[ISO 8601]].\n\nAlso −13 comes alphabetically after −12 although it is less. With negative numbers, to make ascending order correspond with alphabetical sorting, more drastic measures are needed such as adding a constant to all numbers to make them all positive.\n\n==Labeling of ordered items==\nIn some contexts, numbers and letters are used not so much as a basis for establishing an ordering, but as a means of labeling items that are already ordered. For example, pages, sections, chapters, and the like, as well as the items of lists, are frequently \"numbered\" in this way. Labeling series that may be used include ordinary [[Arabic numerals]] (1, 2, 3, ...), [[Roman numerals]] (I, II, III, ... or i, ii, iii, ...), or letters (A, B, C, ... or a, b, c, ...). (An alternative method for indicating list items, without numbering them, is to use a [[bulleted list]].)\n\nWhen letters of an alphabet are used for this purpose of [[enumeration]], there are certain language-specific conventions as to which letters are used. For example, the [[Russian alphabet|Russian]] letters [[Ъ]] and [[Ь]] (which in writing are only used for modifying the preceding [[consonant]]), and usually also [[Ы]], [[Й]], and [[Ё]], are usually omitted. Also in many languages that use extended [[Latin script]], the [[modified letter]]s are often not used in enumeration.\n\n==See also==\n* [[Alphabetical order]]\n* [[ASCII#Order|Asciibetical order]]\n* [[Sorting]]\n* [[Taxonomic sequence]]\n* [[Mac and Mc together]]\n* [[Unicode equivalence]]\n* [[Natural sort order]]\n\n==Notes==\n{{Reflist|group=\"note\"}}\n\n==References==\n{{Reflist}}\n\n==External links==\n{{Wiktionary|collation|alphabetization}}\n*[https://www.unicode.org/unicode/reports/tr10/ Unicode Collation Algorithm]: Unicode Technical Standard #10\n*[http://spanish.about.com/library/weekly/aa092099.htm#letters Collation in Spanish]\n*[https://www.un.org/Overview/unmember.html Collation of the names of the member states of the United Nations]\n*[http://www.w3.org/TR/css3-lists Typographical collation for many languages], as proposed in the List module of [[Cascading Style Sheet]]s.\n*[http://www.collation-charts.org/ Collation Charts]: Charts demonstrating language-specific sorting orders in various operating systems and DBMS\n*[http://demo.icu-project.org/icu-bin/locexp?_=en_US&x=col ICU Locale Explorer]: An online demonstration of sorting in different languages that uses the [[Unicode Collation Algorithm]] with [[International Components for Unicode]]\n\n[[Category:Collation| ]]"
    },
    {
      "title": "Abjad numerals",
      "url": "https://en.wikipedia.org/wiki/Abjad_numerals",
      "text": "{{more citations needed|date=March 2019}}\n{{original research|date=March 2019}}\n{{Numeral systems}}\n{{Arabic alphabet}}\nThe '''Abjad numerals''', also called '''Hisab al-Jummal''' ({{lang-ar|حِسَاب الْجُمَّل}}, {{transl|ar|ALA-LC|ḥisāb al-jummal}}), are a decimal [[numeral system]] in which the 28 letters of the [[Arabic alphabet]] are assigned numerical values. They have been used in the [[Arabic]]-speaking world since before the eighth century when [[Arabic numerals]] were adopted. In modern Arabic, the word ''{{transl|ar|ALA|ʾabjadīyah}}'' ({{lang|ar|أبجدية}}) means '[[alphabet|<span lang=\"en\" dir=\"ltr\">alphabet</span>]]' in general.\n\nIn the Abjad system, the first letter of the Arabic alphabet, [[aleph#Arabic|ʾalif]], is used to represent 1; the second letter, [[Bet (letter)#Arabic bāʾ|bāʾ]], is used to represent 2, etc. Individual letters also represent 10s and 100s: [[Yodh#Arabic yāʼ|yāʾ]] for 10, [[Kaph#Arabic kāf|kāf]] for 20, [[Qoph#Arabic qāf|qāf]] for 100, etc.\n\nThe word ''[[abjad|ʾabjad]]'' ({{lang|ar|أبجد}}) itself derives from the first four letters (A-B-J-D) of the Semitic alphabet, including the [[Phoenician alphabet]], [[Aramaic alphabet]], [[Hebrew alphabet]] and other scripts for [[Semitic languages]]. These older alphabets contained only 22 letters, stopping at [[taw]], numerically equivalent to 400. The Arabic Abjad system continues at this point with letters not found in other alphabets: [[ṯāʾ|thāʾ]]= 500, etc.\n\n==Abjad order==\nThe Abjad order of the [[Arabic alphabet]] has two slightly different variants. The Abjad order is not a simple historical continuation of the earlier north Semitic alphabetic order, since it has a position corresponding to the Aramaic letter ''[[samekh]] / semkat'' {{lang|he|ס}}, yet no letter of the Arabic alphabet historically derives from that letter. Loss of ''{{transl|he|ALA|samekh}}'' was compensated for by the split of [[shin (letter)|''{{transl|he|ALA|shin}}'']] {{lang|he|ש}} into two independent Arabic letters, {{lang|ar|ش}} (''{{transl|ar|ALA|shīn}}'') and {{lang|ar|ﺱ}} (''{{transl|ar|ALA|sīn}}''), which moved up to take the place of ''{{transl|he|ALA|samekh}}''.\n\nThe most common Abjad sequence, read from right to left, is:\n{| class=\"wikitable\" style=\"line-height:180%;font-size:larger\"\n|-\n| {{lang|ar|غ}} || {{lang|ar|ظ}} || {{lang|ar|ض}} || {{lang|ar|ذ}} || {{lang|ar|خ}} || {{lang|ar|ث}} || {{lang|ar|ت}} || {{lang|ar|ش}} || {{lang|ar|ر}} || {{lang|ar|ق}} || {{lang|ar|ص}} || {{lang|ar|ف}} || {{lang|ar|ع}} || {{lang|ar|س}} || {{lang|ar|ن}} || {{lang|ar|م}} || {{lang|ar|ل}} || {{lang|ar|ك}} || {{lang|ar|ي}} || {{lang|ar|ط}} || {{lang|ar|ح}} || {{lang|ar|ز}} || {{lang|ar|و}} || {{lang|ar|ه}} || {{lang|ar|د}} || {{lang|ar|ج}} || {{lang|ar|ب}} || {{lang|ar|أ}}\n|-\n| {{transl|ar|gh}} || {{transl|ar|ẓ}} || {{transl|ar|ḍ}} || {{transl|ar|dh}} || {{transl|ar|kh}} || {{transl|ar|th}} || {{transl|ar|t}} || {{transl|ar|sh}} || {{transl|ar|r}} || {{transl|ar|q̈}} || {{transl|ar|ṣ}} || {{transl|ar|f}} || {{transl|ar|ʿ}} || {{transl|ar|s}} || {{transl|ar|n}} || {{transl|ar|m}} || {{transl|ar|l}} || {{transl|ar|k}} || {{transl|ar|y}} || {{transl|ar|ṭ}} || {{transl|ar|ḥ}} || {{transl|ar|z}} || {{transl|ar|w}} || {{transl|ar|h}} || {{transl|ar|d}} || {{transl|ar|j}} || {{transl|ar|b}} || {{transl|ar|ʾ}}\n|}\n\nThis is commonly vocalized as follows:\n:*{{transl|ar|ALA|ʾabjad hawwaz ḥuṭṭī kalaman ṣaʿfaṣ qarashat thakhadh ḍaẓagh}}.\nAnother vocalization is:\n:*{{transl|ar|ʾabujadin hawazin ḥuṭiya kalman ṣaʿfaṣ qurishat thakhudh ḍaẓugh}}\n\nAnother Abjad sequence (probably older, now mainly confined to the Maghreb), is:<ref name=magb>{{ar icon}} [http://alyaseer.net/vb/showthread.php?t=8807 Alyaseer.net {{lang|ar|ترتيب المداخل والبطاقات في القوائم والفهارس الموضوعية}} Ordering entries and cards in subject indexes] Discussion thread ''(Accessed 2009-Oct-06)''</ref>\n\n{| class=\"wikitable\" style=\"line-height:180%;font-size:larger\"\n|-\n| {{lang|ar|ش}} || {{lang|ar|غ}} || {{lang|ar|ظ}} || {{lang|ar|ذ}} || {{lang|ar|خ}} || {{lang|ar|ث}} || {{lang|ar|ت}} || {{lang|ar|س}} || {{lang|ar|ر}} || {{lang|ar|ق}} || {{lang|ar|ض}} || {{lang|ar|ف}} || {{lang|ar|ع}} || {{lang|ar|ص}} || {{lang|ar|ن}} || {{lang|ar|م}} || {{lang|ar|ل}} || {{lang|ar|ك}} || {{lang|ar|ي}} || {{lang|ar|ط}} || {{lang|ar|ح}} || {{lang|ar|ز}} || {{lang|ar|و}} || {{lang|ar|ه}} || {{lang|ar|د}} || {{lang|ar|ج}} || {{lang|ar|ب}} || {{lang|ar|أ}}\n|-\n| {{transl|ar|sh}} || {{transl|ar|gh}} || {{transl|ar|ẓ}} || {{transl|ar|dh}} || {{transl|ar|kh}} || {{transl|ar|th}} || {{transl|ar|t}} || {{transl|ar|s}} || {{transl|ar|r}} || {{transl|ar|q̈}} || {{transl|ar|ḍ}} || {{transl|ar|f}} || <big>{{transl|ar|ʿ}}</big> || {{transl|ar|ṣ}} || {{transl|ar|n}} || {{transl|ar|m}} || {{transl|ar|l}} || {{transl|ar|k}} || {{transl|ar|y}} || {{transl|ar|ṭ}} || {{transl|ar|ḥ}} || {{transl|ar|z}} || {{transl|ar|w}} || {{transl|ar|h}} || {{transl|ar|d}} || {{transl|ar|j}} || {{transl|ar|b}} || {{transl|ar|ʾ}}\n|}\n\nwhich can be vocalized as:\n:*{{transl|ar|ALA|ʾabujadin hawazin ḥuṭiya kalman ṣaʿfaḍ qurisat thakhudh ẓaghush}}\n\nAnother vocalization is:\n:*{{transl|ar|ʾabajd hawazin ḥuṭīyin kalamnin ṣaʿfaḍin qurisat thakhudh ẓughshin}}\n\nModern dictionaries and other reference books do not use the Abjad order to sort alphabetically; instead, the newer ''{{transl|ar|ALA|hijāʾī}}'' ({{lang|ar|هجائي}}) order, which partially groups letters together by similarity of shape, is used:\n\n{| class=\"wikitable\" style=\"line-height:180%;font-size:larger\"\n|-\n| {{lang|ar|ي}} || {{lang|ar|و}} || {{lang|ar|ه}} || {{lang|ar|ن}} || {{lang|ar|م}} || {{lang|ar|ل}} || {{lang|ar|ك}} || {{lang|ar|ق}} || {{lang|ar|ف}} || {{lang|ar|غ}} || {{lang|ar|ع}} || {{lang|ar|ظ}} || {{lang|ar|ط}} || {{lang|ar|ض}} || {{lang|ar|ص}} || {{lang|ar|ش}} || {{lang|ar|س}} || {{lang|ar|ز}} || {{lang|ar|ر}} || {{lang|ar|ذ}} || {{lang|ar|د}} || {{lang|ar|خ}} || {{lang|ar|ح}} || {{lang|ar|ج}} || {{lang|ar|ث}} || {{lang|ar|ت}} || {{lang|ar|ب}} || {{lang|ar|أ}}\n|-\n| {{transl|ar|y}} || {{transl|ar|w}} || {{transl|ar|h}} || {{transl|ar|n}} || {{transl|ar|m}} || {{transl|ar|l}} || {{transl|ar|k}} || {{transl|ar|q̈}} || {{transl|ar|f}} || {{transl|ar|gh}} || <big>{{transl|ar|ʿ}}</big> || {{transl|ar| ẓ}} || {{transl|ar|ṭ}} || {{transl|ar|ḍ}} || {{transl|ar|ṣ}} || {{transl|ar|sh}} || {{transl|ar|s}} || {{transl|ar|z}} || {{transl|ar|r}} || {{transl|ar|dh}} || {{transl|ar|d}} || {{transl|ar|kh}} || {{transl|ar|ḥ}} || {{transl|ar|j}} || {{transl|ar|th}} || {{transl|ar|t}} || {{transl|ar|b}} || {{transl|ar|ʾ}}\n|}\n\nAnother kind of ''{{transl|ar|ALA|alfabaʾī}}'' order used to be widely used in the [[Maghreb]] until recently, when it was replaced by the [[Mashriq]]i order:<ref name=magb/>\n\n{| class=\"wikitable\" style=\"line-height:180%;font-size:larger\"\n| {{lang|ar|ي}} || {{lang|ar|و}} || {{lang|ar|ه}} || {{lang|ar|ش}} || {{lang|ar|س}} || {{lang|ar|ق}} || {{lang|ar|ف}} || {{lang|ar|غ}} || {{lang|ar|ع}} || {{lang|ar|ض}} || {{lang|ar|ص}} || {{lang|ar|ن}} || {{lang|ar|م}} || {{lang|ar|ل}} || {{lang|ar|ك}} || {{lang|ar|ظ}} || {{lang|ar|ط}} || {{lang|ar|ز}} || {{lang|ar|ر}} || {{lang|ar|ذ}} || {{lang|ar|د}} || {{lang|ar|خ}} || {{lang|ar|ح}} || {{lang|ar|ج}} || {{lang|ar|ث}} || {{lang|ar|ت}} || {{lang|ar|ب}} || {{lang|ar|أ}}\n|-\n| {{transl|ar|y}} || {{transl|ar|w}} || {{transl|ar|h}} || {{transl|ar|sh}} || {{transl|ar|s}} || {{transl|ar|q̈}} || {{transl|ar|f}} || {{transl|ar|gh}} || <big>{{transl|ar|ʿ}}</big> || {{transl|ar|ḍ}} || {{transl|ar|ṣ}} || {{transl|ar|n}} || {{transl|ar|m}} || {{transl|ar|l}} || {{transl|ar|k}} || {{transl|ar|ẓ}} || {{transl|ar|ṭ}} || {{transl|ar|z}} || {{transl|ar|r}} || {{transl|ar|dh}} || {{transl|ar|d}} || {{transl|ar|kh}} || {{transl|ar|ḥ}} || {{transl|ar|j}} || {{transl|ar|th}} || {{transl|ar|t}} || {{transl|ar|b}} || {{transl|ar|ʾ}}\n|}\n\n[[Persian alphabet|Persian]] dictionaries use a slightly different [[Persian alphabet|order]], in which و comes before ه instead of after it.\n\n==Uses of the Abjad system==\nBefore the introduction of the [[Hindu–Arabic numeral system]], the abjad numbers were used for all mathematical purposes. In modern Arabic, they are primarily used for numbering [[Outline (list)|outlines]],  items in lists, and points of information. In English, points of information are sometimes referred to as \"A\", \"B\", and \"C\" (or perhaps use Roman numerals: I, II, III, IV), and in Arabic, they are \"{{lang|ar|أ}}\", then \"{{lang|ar|ب}}\", then \"{{lang|ar|ج}}\", not the first three letters of the modern ''{{transl|ar|ALA|hijāʼī}}'' order.\n\nThe abjad numbers are also used to assign numerical values to Arabic words for purposes of [[isopsephy|numerology]].  The common Islamic phrase {{lang|ar|بسم الله الرحمن الرحيم}} ''{{transl|ar|ALA|bismillāh al-Raḥmān al-Raḥīm}}'' ('In the name of Allah, the most merciful, the most compassionate'&nbsp;– see [[Basmala]]) has a numeric value of 786 (from a letter-by-letter cumulative value of 2+60+40+1+30+30+5+1+30+200+8+40+50+1+30+200+8+10+40). The name [[Allah|Allāh]] {{lang|ar|الله}} by itself has the value 66 (1+30+30+5).\n\n==Letter values==<!--This section is linked from [[Mawza exile]] ([[MOS:HEAD]])-->\n{|\n|\n{| class=\"wikitable\" style=\"text-align:center;\"\n|-\n! Value || Letter || Name || Trans-<br />literation\n|-\n| 1 || <span style=\"font-size:140%;line-height:130%\">{{lang|ar|[[أ]]}}</span> || {{transl|ar|ALA|ʾalif}} || {{transl|ar|ALA|[[ʾ]]}} / {{transl|ar|ALA|[[ā]]}}</span>\n|-\n|  2 || <span style=\"font-size:140%;line-height:130%\">{{lang|ar|[[ب]]}}</span> || {{transl|ar|ALA|bāʾ}} || {{transl|ar|ALA|[[b]]}}\n|-\n| 3 || <span style=\"font-size:140%;line-height:130%\">{{lang|ar|[[ج]]}}</span> || {{transl|ar|ALA|jīm}} || {{transl|ar|ALA|[[j]]}}\n|-\n| 4 || <span style=\"font-size:140%;line-height:130%\">{{lang|ar|[[د]]}}</span> || {{transl|ar|ALA|dāl}} || {{transl|ar|ALA|[[d]]}}\n|-\n| 5 || <span style=\"font-size:140%;line-height:130%\">{{lang|ar|[[ه]]}}</span> || {{transl|ar|ALA|hāʾ}} || {{transl|ar|ALA|[[h]]}}\n|-\n| 6 || <span style=\"font-size:140%;line-height:130%\">{{lang|ar|[[و]]}}</span> || {{transl|ar|ALA|wāw}} || {{transl|ar|ALA|[[w]] / [[ū]]}}\n|-\n| 7 || <span style=\"font-size:140%;line-height:130%\">{{lang|ar|[[ز]]}}</span> || {{transl|ar|ALA|zāy}}/{{transl|ar|ALA|zayn}} || {{transl|ar|ALA|[[z]]}}\n|-\n| 8 || <span style=\"font-size:140%;line-height:130%\">{{lang|ar|[[ح]]}}</span> || {{transl|ar|ALA|ḥāʾ}} || {{transl|ar|ALA|[[Heth|ḥ]]}}\n|-\n| 9 || <span style=\"font-size:140%;line-height:130%\">{{lang|ar|[[ط]]}}</span> || {{transl|ar|ALA|ṭāʾ}} || {{transl|ar|ALA|[[ṭ]]}}\n|-\n| &nbsp; || <span style=\"font-size:140%;line-height:130%\">{{lang|ar|&nbsp;}}</span> || &nbsp; || &nbsp;\n|}\n|\n{| class=\"wikitable\" style=\"text-align:center;\"\n|-\n! Value || Letter || Name || Trans- <br>literation\n|-\n| 10 || <span style=\"font-size:140%;line-height:130%\">{{lang|ar|[[ي]]}}</span> <!--in such context the letter is always undotted-->|| {{transl|ar|ALA|yāʾ}} || {{transl|ar|ALA|[[y]] / [[ī]]}}\n|-\n| 20 || <span style=\"font-size:140%;line-height:130%\">{{lang|ar|[[ك]]}}</span> || {{transl|ar|ALA|kāf}} || {{transl|ar|ALA|[[k]]}}\n|-\n| 30 || <span style=\"font-size:140%;line-height:130%\">{{lang|ar|[[ل]]}}</span> || {{transl|ar|ALA|lām}} || {{transl|ar|ALA|[[l]]}}\n|-\n| 40 || <span style=\"font-size:140%;line-height:130%\">{{lang|ar|[[م]]}}</span> || {{transl|ar|ALA|mīm}} || {{transl|ar|ALA|[[m]]}}\n|-\n| 50 || <span style=\"font-size:140%;line-height:130%\">{{lang|ar|[[ن]]}}</span> || {{transl|ar|ALA|nūn}} || {{transl|ar|ALA|[[n]]}}\n|-\n| 60 || <span style=\"font-size:140%;line-height:130%\">{{lang|ar|[[س]]}}</span> || {{transl|ar|ALA|sīn}} || {{transl|ar|ALA|[[s]]}}\n|-\n| 70 || <span style=\"font-size:140%;line-height:130%\">{{lang|ar|[[ع]]}}</span> || {{transl|ar|ALA|ʿayn}} || <span style=\"font-size:140%\">{{transl|ar|ALA|[[ʿ]]}}</span>\n|-\n| 80 || <span style=\"font-size:140%;line-height:130%\">{{lang|ar|[[ف]]}}</span> || {{transl|ar|ALA|fāʾ}} || {{transl|ar|ALA|[[f]]}}\n|-\n| 90 || <span style=\"font-size:140%;line-height:130%\">{{lang|ar|[[ص]]}}</span> || {{transl|ar|ALA|ṣād}} || {{transl|ar|ALA|[[ṣ]]}}\n|-\n| &nbsp; || <span style=\"font-size:140%;line-height:130%\">{{lang|ar|&nbsp;}}</span> || &nbsp; || &nbsp;\n|}\n|\n{| class=\"wikitable\" style=\"text-align:center;\"\n|-\n! Value || Letter || Name || Trans-<br />literation\n|-\n| 100 || <span style=\"font-size:140%;line-height:130%\">{{lang|ar|[[ق]]}}</span> || {{transl|ar|ALA|q̈āf}} || {{transl|ar|ALA|[[q̈]]}}\n|-\n| 200 || <span style=\"font-size:140%;line-height:130%\">{{lang|ar|[[ر]]}}</span> || {{transl|ar|ALA|rāʾ}} || {{transl|ar|ALA|[[r]]}}\n|-\n| 300 || <span style=\"font-size:140%;line-height:130%\">{{lang|ar|[[ش]]}}</span> || {{transl|ar|ALA|shīn}} || {{transl|ar|ALA|[[Shin (letter)|sh]]}}\n|-\n| 400 || <span style=\"font-size:140%;line-height:130%\">{{lang|ar|[[ت]]}}</span> || {{transl|ar|ALA|tāʾ}} || {{transl|ar|ALA|[[t]]}}\n|-\n| 500 || <span style=\"font-size:140%;line-height:130%\">{{lang|ar|[[ث]]}}</span> || {{transl|ar|ALA|thāʾ}} || {{transl|ar|ALA|[[Ṯāʾ|th]]}}\n|-\n| 600 || <span style=\"font-size:140%;line-height:130%\">{{lang|ar|[[خ]]}}</span> || {{transl|ar|ALA|khāʾ}} || {{transl|ar|ALA|[[Ḫāʾ|kh]]}}</span>\n|-\n| 700 || <span style=\"font-size:140%;line-height:130%\">{{lang|ar|[[ذ]]}}</span> || {{transl|ar|ALA|dhāl}} || {{transl|ar|ALA|[[Ḏāl|dh]]}}\n|-\n| 800 || <span style=\"font-size:140%;line-height:130%\">{{lang|ar|[[ض]]}}</span> || {{transl|ar|ALA|ḍād}} || {{transl|ar|ALA|[[ḍ]]}}\n|-\n| 900 || <span style=\"font-size:140%;line-height:130%\">{{lang|ar|[[ظ]]}}</span> || {{transl|ar|ALA|ẓāʾ}} || {{transl|ar|ALA|[[ẓ]]}}\n|-\n| 1000 || <span style=\"font-size:140%;line-height:130%\">{{lang|ar|[[غ]]}}</span> || {{transl|ar|ALA|ghayn}} || {{transl|ar|ALA|[[ġ|gh]]}}\n|}\n|}\n\nA few of the numerical values are different in the alternative Abjad order. For four [[Persian alphabet|Persian letters]] these values are used:\n\n{| class=\"wikitable\" style=\"text-align:center;\"\n|-\n! Value || Letter || Name || Trans-<br />literation\n|-\n| 2 || <span style=\"font-size:140%;line-height:130%\">{{lang|fa|[[پ]]}}</span> || {{transl|fa|pe}} || {{transl|fa|p}}\n|-\n| 3 || <span style=\"font-size:140%;line-height:130%\">{{lang|fa|[[چ]]}}</span> || {{transl|fa|che}} || {{transl|fa|ch}} or {{transl|fa|č}}\n|-\n| 7 || <span style=\"font-size:140%;line-height:130%\">{{lang|fa|[[ژ]]}}</span> || {{transl|fa|zhe}}|| {{transl|fa|zh}} or {{transl|fa|ž}}\n|-\n| 20 || <span style=\"font-size:140%;line-height:130%\">{{lang|fa|[[گ]]}}</span> || {{transl|fa|gâf}} || {{transl|fa|g}}\n|}\n\n==Similar systems==\n\nThe Abjad numerals are equivalent to the earlier [[Hebrew numerals]] up to 400. The Hebrew numeral system is known as [[Gematria]] and is used in [[Kabbalah|Kabbalistic]] texts and numerology. Like the Abjad order, it is used in modern times for numbering outlines and points of information, including the first six days of the week. The [[Greek numerals]] differ in a number of ways from the Abjad ones (for instance in the [[Greek alphabet]] there is no equivalent for {{lang|ar|ص}}, ''{{transl|ar|ALA|ṣād}}''). The [[Greek language]] system of letters-as-numbers is called [[isopsephy]]. In modern times the old 27-letter alphabet of this system also continues to be used for numbering lists.\n\n==See also==\n*[[Western Arabic numerals]]\n*[[Eastern Arabic numerals]]\n*[[Arabic alphabet]]\n*[[Kufic]]\n*[[Abjad]]\n*[[Hurufism]]\n*[[ʿilm al-Ḥurouf|ʿIlm al-Ḥurūf]] (science of letters)\n*[[Gematria]]\n*[[Isopsephy]]\n*[[Katapayadi system]]\n\n==References==\n{{Reflist}}\n\n==External links==\n*[http://bahai-library.com/lewis_Abjad_numerological_system Overview of the ''abjad ''numerological system]\n*[http://www.nurmuhammad.com/IlmHuroof/IlmHuroofArticles/welcometothescienceofhuroof.htm Sufi numerology site]\n*[http://alavibohra.org/abjad%20arabic%20calculator/arabic%20numeric%20value.php Numerical Value of an Arabic Text as per \"Abjad\" Calculation - www.alavibohra.org]\n*[https://web.archive.org/web/20190103022542/http://abjadcalculator.ir/ Online Abjad Calculator in Arabic , Hebrew , Persian - With Reverse Abjad Numeral to Text]\n\n{{Arabic language|state=collapsed}}\n{{list of writing systems}}\n\n[[Category:Numerals]]\n[[Category:Collation]]"
    },
    {
      "title": "Alphabetical order",
      "url": "https://en.wikipedia.org/wiki/Alphabetical_order",
      "text": "{{short description|System for ordering words, names and phrases}}\n{{Hatnote|\"Alphabetization\" redirects here. For the creation of an alphabetic [[writing system]], which in instances of Latin script is called romanization, see [[Romanization]].}}\n{{more citations needed|date=June 2017}}\n{{Use dmy dates|date=June 2017}}\n\n'''Alphabetical order''' is a system whereby [[character string|string]]s of [[character (symbol)|character]]s are placed in order based on the position of the characters in the conventional ordering of an [[alphabet]]. It is one of the methods of [[collation]]. In mathematics, a [[lexicographical order]] is the generalization of the alphabetical order to other data types, such as [[sequence (mathematics)|sequences]] of digits or numbers.\n\nWhen applied to strings or [[sequence (mathematics)|sequences]] that, beside alphabetical characters, may contain also digits, numbers or more elaborate types of elements, the alphabetical order is generally called a [[lexicographical order]].\n\nTo determine which of two strings of characters comes when arranging in alphabetical order, their first [[letter (alphabet)|letters]] are compared. If they differ, then the string whose first letter comes earlier in the alphabet comes before the other string. If the first letters are the same, then the second letters are compared, and so on. If a position is reached where one string has no more letters to compare while the other does, then the first (shorter) string is deemed to come first in alphabetical order.\n\n[[Capital letter]]s (upper case) are generally considered to be identical to their corresponding lower case letters for the purposes of alphabetical ordering, though conventions may be adopted to handle situations where two strings differ ''only'' in capitalization. Various conventions also exist for the handling of strings containing [[space (symbol)|space]]s, modified letters (such as those with [[diacritic]]s), and non-letter characters such as marks of [[punctuation]].\n\nThe result of placing a set of words or strings in alphabetical order is that all the strings beginning with the same letter are grouped together; and within that grouping all words beginning with the same two-letter sequence are grouped together; and so on. The system thus tends to maximize the number of common initial letters between adjacent words.\n\n==History==\nAlphabetical order was first used in the 1st millennium BC by Northwest Semitic scribes using the [[Abjad]] system.<ref>Reinhard G. Lehmann: \"27-30-22-26. How Many Letters Needs an Alphabet? The Case of Semitic\", in: The idea of writing: Writing across borders / edited by Alex de Voogt and Joachim Friedrich Quack, Leiden: Brill 2012, p. 11-52</ref> The first effective use of alphabetical order as a cataloging device among scholars may have been in ancient Alexandria.<ref>Daly, Lloyd. ''Contributions to the History of Alphabetization in Antiquity and the Middle Ages'' Brussels, 1967. p. 25</ref>\nIn the 1st century BC, Roman writer [[Varro]] compiled alphabetic lists of authors and titles.<ref name=\"O'Hara\">{{cite journal |title=Messapus, Cycnus, and the Alphabetical Order of Vergil's Catalogue of Italian Heroes |last=O'Hara |first=James |year=1989 |jstor=1088539 |volume=43 |pages=35–38}}</ref>\nIn the 2nd century CE, [[Sextus Pompeius Festus]] wrote an encyclopedic [[epitome]] of the works of [[Verrius Flaccus]], ''[[De verborum significatu]]'', with entries in alphabetic order.<ref name=remacle>{{cite book |url=http://remacle.org/bloodwolf/erudits/Festus/m.htm |title=LIVRE XI – texte latin – traduction + commentaires}}</ref>\nIn the 3rd century CE, [[Harpocration]] wrote a [[Homer]]ic lexicon alphabetized by all letters.<ref name=gibson>{{cite book |title=Interpreting a classic: Demosthenes and his ancient commentators |last=Gibson |first=Craig |year=2002}}</ref>\nIn the 10th century, the author of the ''[[Suda]]'' used alphabetic order with phonetic variations.\nIn the 14th century, the author of the ''[[Fons memorabilium universi]]'' used a classification, but used alphabetical order within some of the books.<ref name=Yeo>{{cite book |title=Encyclopaedic visions: scientific dictionaries and enlightenment culture |last=Yeo |first=Richard |year=2001 |isbn=0521651913 |publisher=Cambridge University Press |url=https://books.google.com/books?id=8bJYm1N8bqgC&pg=PA22&lpg=PA22}}</ref>\n\nIn 1604 [[Robert Cawdrey]] had to explain in ''[[Table Alphabeticall]]'', the first monolingual English dictionary, \"Nowe if the word, which thou art desirous to finde, begin with (a) then looke in the beginning of this Table, but if with (v) looke towards the end.\"<ref name=Cawdrey>{{cite book |url=http://www.library.utoronto.ca/utel/ret/cawdrey/cawdrey0.html#reader |title=Robert Cawdrey's – A Table Alphabetical OBERT (1604)}}</ref>\nAlthough as late as 1803 [[Samuel Taylor Coleridge]] condemned encyclopedias with \"an arrangement determined by the accident of initial letters\",<ref name=Coleridge>{{cite book |title=Coleridge's Letters, No.507 |url=http://inamidst.com/coleridge/letters/letter507}}</ref> many lists are today based on this principle.\n\n==Ordering in the Latin script==\n\n===Basic order and example===\nThe standard order of the modern [[ISO basic Latin alphabet]] is:\n:'''A-B-C-D-E-F-G-H-I-J-K-L-M-N-O-P-Q-R-S-T-U-V-W-X-Y-Z'''\n\nAn example of straightforward alphabetical ordering follows:\n*'''''As; Aster; Astrolabe; Astronomy; Astrophysics; At; Ataman; Attack; Baa'''''\nAnother example:\n*'''''Barnacle; Be; Been; Benefit; Bent'''''\n \nThe above words are ordered alphabetically. ''As'' comes before ''Aster'' because they begin with the same two letters and ''As'' has no more letters after that whereas ''Aster'' does. The next three words come after ''Aster'' because their fourth letter (the first one that differs) is ''r'', which comes after ''e'' (the fourth letter of ''Aster'') in the alphabet. Those words themselves are ordered based on their sixth letters (''l'', ''n'' and ''p'' respectively). Then comes ''At'', which differs from the preceding words in the second letter (''t'' comes after ''s''). ''Ataman'' comes after ''At'' for the same reason that ''Aster'' came after ''As''. ''Attack'' follows ''Ataman'' based on comparison of their third letters, and ''Baa'' comes after all of the others because it has a different first letter.\n\n===Treatment of multiword strings===\nWhen some of the strings being ordered consist of more than one word, i.e., they contain [[space (character)|spaces]] or other separators such as [[hyphen]]s, then two basic approaches may be taken. In the first approach, all strings are ordered initially according to their first  word, as in the sequence:\n*''Oak; Oak Hill; Oak Ridge; Oakley Park; Oakley River''\n*:where all strings beginning with the separate word ''Oak'' precede all those beginning ''Oakley'', because ''Oak'' precedes ''Oakley'' in alphabetical order.\n\nIn the second approach, strings are alphabetized as if they had no spaces, giving the sequence:\n*''Oak; Oak Hill; Oakley Park; Oakley River; Oak Ridge''\n*:where ''Oak Ridge'' now comes after the ''Oakley'' strings, as it would if it were written \"Oakridge\".\n\nThe second approach is the one usually taken in dictionaries, and it is thus often called ''[[dictionary order (disambiguation)|dictionary order]]'' by [[publishing|publishers]]. The first approach has often been used in [[index (publishing)|book indexes]], although each publisher traditionally set its own standards for which approach to use therein; there was no ISO standard for book indexes ([[ISO 999]]) before 1975.\n\n===Special cases===\n{{Unreferenced section|date=June 2017}}\n\n====Modified letters====\nIn French, modified letters (such as those with [[diacritic]]s) are treated the same as the base letter for alphabetical ordering purposes. For example, ''rôle'' comes between ''rock'' and ''rose'', as if it were written ''role''. However languages that use such letters systematically generally have their own ordering rules. See [[#Language-specific conventions|Language-specific conventions]] below.\n\n====Ordering by surname====\nIn most cultures where [[family name]]s are written after [[given name]]s, it is still desired to sort lists of names (as in telephone directories) by family name first.  In this case, names need to be reordered to be sorted properly.  For example, Juan Hernandes and Brian O'Leary should be sorted as \"Hernandes, Juan\" and \"O'Leary, Brian\" even if they are not written this way.  Capturing this rule in a computer collation algorithm is difficult, and simple attempts will necessarily fail.  For example, unless the algorithm has at its disposal an extensive list of family names, there is no way to decide if \"Gillian Lucille van der Waal\" is \"van der Waal, Gillian Lucille\", \"Waal, Gillian Lucille van der\", or even \"Lucille van der Waal, Gillian\".\n\nOrdering by surname is frequently encountered in academic contexts. Within a single multi-author paper, ordering the authors alphabetically by surname, rather than by other methods such as reverse seniority or subjective degree of contribution to the paper, is seen as a way of \"acknowledg[ing] similar contributions\" or \"avoid[ing] disharmony in collaborating groups\".<ref>{{cite journal|first=Teja|last=Tscharntke|first2=Michael E|last2=Hochberg|first3=Tatyana A|last3=Rand|first4=Vincent H|last4=Resh|first5=Jochen|last5=Krauss|title=Author Sequence and Credit for Contributions in Multiauthored Publications|journal=PLoS Biol.|date=January 2007|volume=5|issue=1|pmid=17227141|doi=10.1371/journal.pbio.0050018|pmc=1769438}}</ref> The practice in certain fields of ordering [[citation]]s in bibliographies by the surnames of their authors has been found to create bias in favour of authors with surnames which appear earlier in the alphabet, while this effect does not appear in fields in which bibliographies are ordered chronologically.<ref>{{cite journal|url=https://decisionslab.unl.edu/pubs/stevens_duque_2018_SM.pdf|first=Jeffrey R.|last=Stevens|first2=Juan F.|last2=Duque|title=Order Matters: Alphabetizing In-Text Citations Biases Citation Rates|journal=Psychonomic Bulletin & Review|year=2018|doi=10.3758/s13423-018-1532-8|lay-url=https://www.insidehighered.com/news/2018/10/22/study-takes-aim-psychologys-practice-ordering-reference-lists-alphabetically|lay-source=[[Inside Higher Ed]]|lay-date=22 October 2018}}</ref>\n\n====''The'' and other common words====\nSometimes if a phrase begins with a very common word (such as \"the\" or \"a\"), that word is ignored or moved to the end of the phrase, but this is not always the case. The book title \"[[The Shining (novel)|The Shining]]\" might be treated as \"Shining\", or \"Shining, The\" and therefore would be ordered before the book title \"[[Summer of Sam]]\", although it may also be treated as simply \"The Shining\" and therefore would be ordered after \"Summer of Sam\". Similarly, the book title \"[[A Wrinkle in Time]]\" might be treated as \"Wrinkle in Time\", \"Wrinkle in Time, A\", or simply \"A Wrinkle in Time\", depending on whom you ask. All three alphabetization methods are fairly easy to create by algorithm, but many programs rely instead on simple [[lexicographic order]]ing.\n\n====''Mac'' prefixes====\n{{main|Mac and Mc together}}\nThe prefixes ''M''' and ''Mc'' in Irish and Scottish surnames are abbreviations for ''Mac'', and are sometimes alphabetized as if the spelling is ''Mac'' in full. Thus ''McKinley'' might be listed before ''Mackintosh'' (as it would be if it had been spelled out as \"MacKinley\"). Since the advent of computer-sorted lists, this type of alphabetization is less frequently encountered, though it is still used in British telephone directories.\n\n====Ligatures====\n[[Typographic ligature|Ligatures]] (two or more letters merged into one symbol) which are not considered distinct letters, such as [[Æ]] and [[Œ]] in English, are typically collated as if the letters were separate- \"æther\" and \"aether\" would be ordered the same relative to all other words. This is true even when the ligature is not purely stylistic, such as in [[loanword]]s and brand names.\n\nSpecial rules may need to be adopted to sort strings which vary only by whether two letters are ligaturized.\n\n===Treatment of numerals===\n{{Unreferenced section|date=June 2017}}\n{{main|Lexicographical order}}\nWhen some of the strings contain [[Numerical digit|numeral]]s (or other non-letter characters), various approaches are possible. Sometimes such characters are treated as if they came before or after all the letters of the alphabet. Another method is for numbers to be sorted alphabetically as they would be spelled: for example ''[[1776 (film)|1776]]'' would be sorted as if spelled out \"seventeen seventy-six\", and ''[[24 heures du Mans]]'' as if spelled \"vingt-quatre...\" (French for \"twenty-four\"). When numerals or other symbols are used as special graphical forms of letters, as ''1337'' for [[leet]] or the movie ''[[Seven (1995 film)|Seven]]'' (which was stylised as ''Se7en''), they may be sorted as if they were those letters. [[Natural sort order]] orders strings alphabetically, except that multi-digit numbers are treated as a single character and ordered by the value of the number encoded by the digits.\n\n===Language-specific conventions===\n{{more citations needed|section|date=June 2017}}\nLanguages which use an [[extended Latin alphabet]] generally have their own conventions for treatment of the extra letters. Also in some languages certain [[digraph (orthography)|digraph]]s are treated as single letters for collation purposes. For example, the 29-letter alphabet of [[Spanish language|Spanish]] treats ''ñ'' as a basic letter following ''n'', and formerly treated the digraphs ''ch'' and ''ll'' as basic letters following ''c'' and ''l'', respectively. ''Ch'' and ''ll'' are still considered letters, but are now alphabetized as two-letter combinations. (The new alphabetization rule was issued by the [[Royal Spanish Academy]] in 1994.) On the other hand, the digraph ''rr'' follows ''rqu'' as expected, and did so even before the 1994 alphabetization rule.\n\nIn a few cases, such as [[Kiowa alphabet|Kiowa]], the alphabet has been completely reordered.\n\nAlphabetization rules applied in various languages are listed below.\n* In [[Azerbaijani language|Azerbaijani]], there are eight additional letters to the standard Latin alphabet. Five of them are vowels: i, ı, ö, ü, [[ə]] and three are consonants: ç, ş, ğ. The alphabet is the same as the [[Turkish alphabet]], with the same sounds written with the same letters, except for three additional letters: q, x and ə for sounds that do not exist in Turkish. Although all the \"Turkish letters\" are collated in their \"normal\" alphabetical order like in Turkish, the three extra letters are collated arbitrarily after letters whose sounds approach theirs. So, q is collated just after k, x (pronounced like a German ''ch'') is collated just after h and ə (pronounced roughly like an English short ''a'') is collated just after e.\n* In [[Breton language|Breton]], there is no \"c\", \"q\", \"x\" but there are the digraphs \"ch\" and \"c'h\", which are collated between \"b\" and \"d\". For example: «&nbsp;buzhugenn, chug, c'hoar, daeraouenn&nbsp;» (earthworm, juice, sister, teardrop).\n* In [[Bosnian language|Bosnian]], [[Croatian language|Croatian]] and [[Serbian language|Serbian]] and other related South Slavic languages, the five accented characters and three conjoined characters are sorted after the originals: ..., C, Č, Ć, D, DŽ, Đ, E, ..., L, LJ, M, N, NJ, O, ..., S, Š, T, ..., Z, Ž.\n* In [[Czech language|Czech]] and [[Slovak language|Slovak]], accented vowels have secondary collating weight – compared to other letters, they are treated as their unaccented forms (A-Á, E-É-Ě, I-Í, O-Ó-Ô, U-Ú-Ů, Y-Ý), but then they are sorted after the unaccented letters (for example, the correct lexicographic order is baa, baá, báa, bab, báb, bac, bác, bač, báč). Accented consonants (the ones with [[caron]]) have primary collating weight and are collocated immediately after their unaccented counterparts, with exception of Ď, Ň and Ť, which have again secondary weight. [[Ch (digraph)|CH]] is considered to be a separate letter and goes between [[H]] and [[I]]. In Slovak, [[Dz (digraph)|DZ]] and [[DŽ]] are also considered separate letters and are positioned between [[Ď]] and [[E]] (A-Á-Ä-B-C-Č-D-Ď-DZ-DŽ-E-É...).\n* In the [[Danish and Norwegian alphabet]]s, the same extra vowels as in Swedish (see below) are also present but in a different order and with different [[glyph]]s (..., X, Y, Z, [[Æ]], [[Ø]], [[Å]]). Also, \"Aa\" collates as an equivalent to \"Å\". The Danish alphabet has traditionally seen \"W\" as a variant of \"V\", but today \"W\" is considered a separate letter.\n* In [[Dutch language|Dutch]] the combination IJ (representing [[IJ (letter)|Ĳ]]) was formerly to be collated as Y (or sometimes, as a separate letter Y < IJ < Z), but is currently mostly collated as 2 letters (II < IJ < IK). Exceptions are phone directories; IJ is always collated as Y here because in many Dutch family names Y is used where modern spelling would require IJ. Note that a word starting with ij that is written with a capital I is also written with a capital J, for example, the town [[IJmuiden]], the river [[IJssel]] and the country IJsland ([[Iceland]]).\n* In [[Esperanto]], consonants with [[circumflex]] accents ('''[[c-circumflex|ĉ]]''', '''[[g-circumflex|ĝ]]''', '''[[h-circumflex|ĥ]]''', '''[[j-circumflex|ĵ]]''', '''[[s-circumflex|ŝ]]'''), as well as '''[[u-breve|ŭ]]''' (u with [[breve]]), are counted as separate letters and collated separately (c, ĉ, d, e, f, g, ĝ, h, ĥ, i, j, ĵ ... s, ŝ, t, u, ŭ, v, z).\n* In [[Estonian language|Estonian]] [[õ]], [[ä]], [[ö]] and [[ü]] are considered separate letters and collate after [[w]]. Letters [[š]], [[z]] and [[ž]] appear in loanwords and foreign proper names only and follow the letter [[s]] in the [[Estonian alphabet]], which otherwise does not differ from the basic Latin alphabet.\n* The [[Faroese alphabet]] also has some of the Danish, Norwegian, and Swedish extra letters, namely [[Æ]] and [[Ø]]. Furthermore, the [[Faroese alphabet]] uses the Icelandic eth, which follows the [[D]]. Five of the six vowels [[A]], [[I]], [[O]], [[U]] and [[Y]] can get accents and are after that considered separate letters. The consonants [[C]], [[Q]], [[X]], [[W]] and [[Z]] are not found. Therefore, the first five letters are [[A]], [[Á]], [[B]], [[D]] and [[Ð]], and the last five are [[V]], [[Y]], [[Ý]], [[Æ]], [[Ø]]\n* In [[Filipino language|Filipino (Tagalog)]] and other Philippine languages, the letter Ng is treated as a separate letter.  It is pronounced as in sing, ping-pong, etc. By itself, it is pronounced '''nang''', but in general [[Filipino orthography]], it is spelled as if it were two separate letters (n and g).  Also, letter derivatives (such as [[Ñ]]) immediately follow the base letter.  [[Filipino language|Filipino]] also is written with diacritics, but their use is very rare (except the [[tilde]]).  (Philippine orthography also includes spelling.)\n* The [[Finnish alphabet]] and collating rules are the same as those of Swedish.\n* For [[French language|French]], the ''last'' accent in a given word determines the order.<ref name=unicode10>{{cite web| title=Unicode Technical Standard #10: Unicode collation algorithm| publisher=Unicode, Inc. (unicode.org) |date=20 March 2008| url= https://www.unicode.org/unicode/reports/tr10/| accessdate=27 August 2008}}</ref>  For example, in French, the following four words would be sorted this way: cote < côte < coté < côté.\n* In [[German alphabet|German]] letters with umlaut ([[Ä]], [[Ö]], [[Ü]]) are treated generally just like their non-umlauted versions; [[ß]] is always sorted as ss. This makes the alphabetic order Arg, Ärgerlich, Arm, Assistent, Aßlar, Assoziation. For phone directories and similar lists of names, the umlauts are to be collated like the letter combinations \"ae\", \"oe\", \"ue\" because a number of German surnames appear both with umlaut and in the non-umlauted form with \"e\" (Müller/Mueller). This makes the alphabetic order Udet, Übelacker, Uell, Ülle, Ueve, Üxküll, Uffenbach.\n* The [[Hungarian language|Hungarian]] vowels have accents, umlauts, and double accents, while consonants are written with single, double (digraphs) or triple (trigraph) characters.  In collating, accented vowels are equivalent with their non-accented counterparts and double and triple characters follow their single originals. Hungarian alphabetic order is: '''A=Á''', B, C, '''Cs''', D, '''Dz''', '''Dzs''', '''E=É''', F, G, '''Gy''', H, '''I=Í''', J, K, L, '''Ly''', M, N, '''Ny''', '''O=Ó''', '''Ö=Ő''', P, Q, R, S, '''Sz''', T, '''Ty''', '''U=Ú''', '''Ü=Ű''', V, W, X, Y, Z, '''Zs'''. (Before 1984, ''dz'' and ''dzs'' were not considered single letters for collation, but two letters each, d+z and d+zs instead.) It means that e.g. ''nádcukor'' should precede ''nádcsomó'' (even though ''s'' normally precedes ''u''), since ''c'' precedes ''cs'' in the collation. Difference in vowel length should only be taken into consideration if the two words are otherwise identical (e.g. ''egér, éger''). Spaces and hyphens within phrases are ignored in collation. ''Ch'' also occurs as a digraph in certain words but it is not considered as a grapheme on its own right in terms of collation.\n*:A particular feature of Hungarian collation is that contracted forms of double di- and trigraphs (such as ''ggy'' from ''gy&nbsp;+&nbsp;gy'' or ''ddzs'' from ''dzs&nbsp;+&nbsp;dzs'') should be collated as if they were written in full (independently of the fact of the contraction and the elements of the di- or trigraphs). For example, ''kaszinó'' should precede ''kassza'' (even though the 4th character ''z'' would normally come after ''s'' in the alphabet), because the fourth \"character\" ([[grapheme]]) of the word ''kassza'' is considered a second ''sz'' (decomposing ''ssz'' into ''sz&nbsp;+&nbsp;sz''), which does follow ''i'' (in ''kaszinó'').<!-- source: 14. c) of the Rules of Hungarian Orthography, cf. [[Hungarian orthography]] -->\n* In [[Icelandic language|Icelandic]], [[Þ]] is added, and D is followed by [[Ð]]. Each vowel (A, E, I, O, U, Y) is followed by its correspondent with [[Acute accent|acute]]: Á, É, Í, Ó, Ú, Ý. There is no Z, so the alphabet ends: ... X, Y, Ý, [[Þ]], [[Æ]], Ö.\n** Both letters were also used by [[Anglo-Saxons|Anglo-Saxon]] scribes who also used the Runic letter [[Wynn]] to represent /w/.\n** [[thorn (letter)|Þ]] (called thorn; lowercase þ) is also a Runic letter.\n** [[Eth (letter)|Ð]] (called eth; lowercase ð) is the letter [[D]] with an added stroke.\n* [[Kiowa language|Kiowa]] is ordered on phonetic principles, like the [[Brahmic scripts]], rather than on the historical Latin order.  Vowels come first, then stop consonants ordered from the front to the back of the mouth, and from negative to positive [[voice-onset time]], then the affricates, fricatives, liquids, and nasals:\n:: A, AU, E, I, O, U, B, F, P, V, D, J, T, TH, G, C, K, Q, CH, X, S, Z, L, Y, W, H, M, N\n* In [[Lithuanian language|Lithuanian]], specifically Lithuanian letters go after their Latin originals. Another change is that [[Y]] comes just before [[J]]: ... G, H, I, Į, Y, J, K...\n* In [[Polish language|Polish]], specifically Polish letters derived from the Latin alphabet are collated after their originals: A, Ą, B, C, Ć, D, E, Ę, ..., L, Ł, M, N, Ń, O, Ó, P, ..., S, Ś, T, ..., Z, Ź, Ż. The digraphs for collation purposes are treated as if they were two separate letters.\n* In [[Portuguese alphabet|Portuguese]], the collating order is just like in English: A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z. Digraphs and letters with diacritics are not included in the alphabet.\n* In [[Romanian language|Romanian]], special characters derived from the Latin alphabet are collated after their originals: A, Ă, Â, ..., I, Î, ..., S, Ș, T, Ț, ..., Z.\n* [[Spanish alphabet|Spanish]] treated (until 1994) \"CH\" and \"LL\" as single letters, giving an ordering of ''{{wiktspa|cinco}}, {{wiktspa|credo}}, {{wiktspa|chispa}}'' and ''{{wiktspa|lomo}}, {{wiktspa|luz}}, {{wiktspa|llama}}.'' This is not true any more since in 1994 the [[Real Academia Española|RAE]] adopted the more conventional usage, and now LL is collated between LK and  LM, and CH between CG and CI. The six characters with diacritics Á, É, Í, Ó, Ú, Ü are treated as the original letters A, E, I, O, U, for example: ''{{wiktspa|radio}}, {{wiktspa|ráfaga}}, {{wiktspa|rana}}, {{wiktspa|rápido}}, {{wiktspa|rastrillo}}.'' The only Spanish-specific collating question is [[Ñ]] ({{wiktspa|eñe}}) as a different letter collated after N.\n* In the [[Swedish alphabet]], there are three extra [[vowel]]s placed at its end (..., X, Y, Z, [[Å]], [[Ä]], [[Ö]]), similar to the Danish and Norwegian alphabet, but with different glyphs and a different collating order. The letter \"W\" has been treated as a variant of \"V\", but in the 13th edition of ''[[Svenska Akademiens Ordlista|Svenska Akademiens ordlista]]'' (2006) \"W\" was considered a separate letter.\n* In the [[Turkish alphabet]] there are 6 additional letters: ç, ğ, ı, ö, ş, and ü (but no q, w, and x). They are collated with ç after c, ğ after g, ı ''before'' i, ö after o, ş after s, and ü after u. Originally, when the alphabet was introduced in 1928, ı was collated after i, but the order was changed later so that letters having shapes containing dots, cedilles or other adorning marks always follow the letters with corresponding bare shapes. Note that in Turkish orthography the letter I is the majuscule of dotless ı, whereas İ is the majuscule of dotted i.\n* In many [[Turkic languages]] (such as [[Azeri language|Azeri]] or the [[Yañalif|Jaꞑalif]] orthography for [[Tatar language|Tatar]]), there used to be the letter [[Gha]] (Ƣƣ), which came between [[G]] and [[H]]. It is now in disuse.\n* In [[Vietnamese language|Vietnamese]],  there are 7 additional letters: [[ă]], [[â]], [[đ]], [[ê]], [[ô]], [[ơ]], [[ư]] while [[f]], [[j]], [[w]], [[z]] are absent, even though they are still in some use (like Internet address, foreign loan language). \"f\" is replaced by the combination \"ph\". The same as for \"w\" is \"qu\".\n* In [[Volapük]] [[ä]], [[ö]] and [[ü]] are counted as separate letters and collated separately (a, ä, b ... o, ö, p ... u, ü, v) while [[q]] and [[w]] are absent.<ref>{{Cite web|last=Midgley |first=Ralph |title=Volapük to English dictionary |url=http://volap%C3%BCk.com/VoEnDictionary-20100830.pdf |archive-url=https://web.archive.org/web/20120901034151/http://xn--volapk-7ya.com/VoEnDictionary-20100830.pdf |archive-date=1 September 2012 |dead-url=no |df=dmy }}</ref>\n* In [[Welsh language|Welsh]] the digraphs CH, DD, FF, NG, LL, PH, RH, and TH are treated as single letters, and each is listed after the first character of the pair (except for NG which is listed after G), producing the order A, B, C, CH, D, DD, E, F, FF, G, NG, H, and so on. It can sometimes happen, however, that word compounding results in the juxtaposition of two letters which do ''not'' form a digraph. An example is the word LLONGYFARCH (composed from LLON + GYFARCH). This results in such an ordering as, for example, LAWR, LWCUS, LLONG, LLOM, LLONGYFARCH (NG is a digraph in LLONG, but not in LLONGYFARCH). The letter combination R+H (as distinct from the digraph RH) may similarly arise by juxtaposition in compounds, although this tends not to produce any pairs in which misidentification could affect the ordering. For the other potentially confusing letter combinations that may occur – namely, D+D and L+L – a hyphen is used in the spelling (e.g. AD-DAL, CHWIL-LYS).\n\n==Automation==\n[[Collation algorithm]]s (in combination with [[sorting algorithm]]s) are used in computer programming to place strings in alphabetical order. A standard example is the [[Unicode Collation Algorithm]], which can be used to put strings containing any [[Unicode]] symbols into (an extension of) alphabetical order.<ref name=unicode10/> It can be made to conform to most of the language-specific conventions described above by tailoring its default collation table. Several such tailorings are collected in [[Common Locale Data Repository]].\n\nFor more details, see {{slink|Collation|Automated collation}}.\n\n==Similar orderings==\n{{Unreferenced section|date=June 2017}}\nThe principle behind alphabetical ordering can still be applied in languages that do not strictly speaking use an [[alphabet]] – for example, they may be written using a [[syllabary]] or [[abugida]] – provided the symbols used have an established ordering.\n\nFor [[logograph]]ic writing systems, such as Chinese [[hanzi]] or Japanese [[kanji]], the method of [[radical-and-stroke sorting]] is frequently used as a way of defining an ordering on the symbols. Japanese sometimes uses pronunciation order, most commonly with the [[Gojūon]] order but sometimes with the older [[Iroha]] ordering.\n\nIn mathematics, [[lexicographical order]] is a means of ordering sequences in a manner analogous to that used to produce alphabetical order.<ref name=\"BaaderNipkow1999\">{{cite book|author1=Franz Baader|author2=Tobias Nipkow|title=Term Rewriting and All That|year=1999|publisher=Cambridge University Press|isbn=978-0-521-77920-3|pages=18–19}}</ref>\n\nSome computer applications use a version of alphabetical order that can be achieved using a very simple [[algorithm]], based purely on the [[ASCII]] or [[Unicode]] codes for characters. This may have non-standard effects such as placing all capital letters before lower-case ones. See [[ASCIIbetical order]].\n\nA [[rhyming dictionary]] is based on sorting words in alphabetical order starting from the last to the first letter of the word.\n\n==See also==\n*[[Collation]]\n*[[Help:Alphabetical order]]\n*[[Sorting]]\n\n==References==\n{{Reflist|30em}}\n\n==Further reading==\n* Chauvin, Yvonne. ''Pratique du classement alphabétique''. 4e éd. Paris: Bordas, 1977. {{ISBN|2-04-010155-1}}\n\n==External links==\n* [http://alphabetizer.flap.tv/ Alphabetize any list in Alphabetical Order with The Alphabetizer]\n* [http://www.text-filter.com/Sort-Text-List-Online-Alphabetize-Arrange-Lines-Word-Text-Alphabetically-Numerically-Natural-Abc-Order-Arranger-Alphabetical-Sorter.htm Online Sort Lists in Alphabetical Order with The Alphabetizer]\n\n[[Category:Alphabets]]\n[[Category:Collation]]\n\n[[ta:அகரவரிசை]]"
    },
    {
      "title": "European ordering rules",
      "url": "https://en.wikipedia.org/wiki/European_ordering_rules",
      "text": "The '''European ordering rules''' (EOR / EN 13710), define an ordering for strings written in languages that are written with the [[Latin alphabet|Latin]], [[Greek alphabet|Greek]] and [[Cyrillic script|Cyrillic]] [[alphabet]]s. The standard covers languages used by the [[European Union]], the [[European Free Trade Association]], and parts of the [[former Soviet Union]]. It is a tailoring of the ''Common Tailorable Template'' of [[ISO 14651|ISO/IEC 14651]].<ref name=\"EOR\">{{cite web|url=http://anubis.dkuug.dk/CEN/TC304/EOR/eorhome.html |title=ENV 13710 – a \"European Pre-Standard\": European ordering rules |work= |accessdate=2009-08-30 |deadurl=yes |archiveurl=https://web.archive.org/web/20110514101431/http://anubis.dkuug.dk/CEN/TC304/EOR/eorhome.html |archivedate=May 14, 2011 }}\n</ref> EOR can in turn be tailored for different (European) languages. But in inter-European contexts, EOR can be used without further tailoring.\n\n==Method==\n\nJust as for [[ISO 14651|ISO/IEC 14651]], upon which EOR is based, EOR has 4 levels of weights.\n\n'''Level 1''' sorts the letters. The following [[Latin script|Latin]] letters are concerned by this level, in order:\n\n:a b c d e f g h i j k l m n o p q r s t u v w x y z þ\n\nThe [[Greek alphabet]] has the following order:\n\n:α β γ δ ε Ϝ Ϛ ζ η θ ι κ λ μ ν ξ ο π Ϟ ρ σ τ υ φ χ ψ ω Ϡ\n\n[[Cyrillic script]] has the following order:\n\n:а ӑ ӓ ә ӛ ӕ б в г ғ ҕ д ђ ҙ е ӗ є ж ӝ җ з ӟ ѕ ӡ и ӥ і ї й ј к қ ӄ ҡ ҟ ҝ л љ м н ң ӊ ҥ њ о ӧ ө ӫ п ҧ р с ҫ т ҭ ћ у ў ӱ ӳ ү ұ ф х ҳ һ ц ҵ ч ӵ ҷ ӌ ҹ ҽ ҿ џ ш щ ъ ы ӹ ь э ю я ҩ Ӏ\n\nThe order for the three alphabets is:\n# Latin alphabet\n# Greek alphabet\n# Cyrillic alphabet\n\nThe [[Georgian alphabet|Georgian]] and [[Armenian alphabet]]s have not been included in ENV 13710. However, they are covered in CR 14400:2001 \"European ordering rules – Ordering for Latin, Greek, Cyrillic, Georgian and Armenian scripts\". Note also that all scripts encoded in ISO/IEC 10646 and Unicode are covered by [[ISO 14651|ISO/IEC 14651]] (and its datafile CTT) as well as [[Unicode Collation Algorithm]] (UCA and the associated DUCET), both of which are available at no charge.\n\n'''Level 2''' is where different additions, such as [[diacritic]]s and variations, to the letters are ordered. Letters with diacritical marks (like {{angbr|å}}, {{angbr|ä}}, {{angbr|ö}}, and {{angbr|ø}}) are ordered as variants of the base letter. {{angbr|æ}}, {{angbr|œ}}, {{angbr|ĳ}} and {{angbr|ŋ}} are ordered as modifications of {{angbr|ae}}, {{angbr|oe}}, {{angbr|ij}} and {{angbr|n}} respectively, similarly for similar cases.\n\nLevel 2 defines the following order of diacritics and other modifications:\n# [[Acute accent]] (á)\n# [[Grave accent]] (à)\n# [[Breve]] (ă)\n# [[Circumflex]] (â)\n# [[hacek|Hacek (háček)]] (š)\n# [[ring (diacritic)|Ring]] (å)\n# [[trema (diacritic)|Trema]] (ä)\n# [[Double acute accent]] (ő)\n# [[Tilde]] (ã)\n# [[dot (diacritic)|Dot]] (ż)\n# [[Cedilla]] (ş)\n# [[Ogonek]] (ą)\n# [[Macron (diacritic)|Macron]] (ā)\n# With stroke through (ø)\n# Modified letter(s) (æ)\n\n'''Level 3''' makes the distinction between Capital and small letters, as in \"Polish\" and \"polish\".\n\n'''Level 4''' concerns [[punctuation]] and [[whitespace character]]s. This level makes the distinction between \"MacDonald\" and \"Mac Donald\", \"its\" and \"it's\".\n\nAn optional, and usually omitted, fifth level can distinguish typographical differences, including whether the text is ''italic'', normal or '''bold'''.\n\n==See also==\n*[[Collation]]\n*[[ISO 14651|ISO/IEC 14651]]\n*[[Unicode collation algorithm]] (UCA)\n*[[Common Locale Data Repository]] (CLDR)\n\n==References==\n<references/>\n;Notes\n{{refbegin}}\n* Hansson, Roger; Lindgren, Carl Göran; Ljung, Heléne; Lundén, Thomas. ''Språk och skrift i Europa''. SNS Förlag. (2004) {{ISBN|91-7150-936-4}}\n* Küster, Marc Wilhelm: ''Geordnetes Weltbild. Die Tradition des alphabetischen Sortierens von der Keilschrift bis zur EDV. Eine Kulturgeschichte.'' Niemeyer (2006) {{ISBN|3-484-10899-1}}. Written by the editor of ENV 13710, it discusses in chapter 17.4 the genesis and the contents of the EOR. Cf. also [https://web.archive.org/web/20070717044335/http://www.budabe.de/grammatologie], in particular also [https://web.archive.org/web/20110718204711/http://www.budabe.de/grammatologie/geordnetes_weltbild_toc.pdf]\n{{refend}}\n\n==External links==\n* [https://web.archive.org/web/20110514101431/http://anubis.dkuug.dk/CEN/TC304/EOR/eorhome.html European Ordering Rules], ENV 13710 – a \"European Pre-Standard\" \n\n[[Category:Library science]]\n[[Category:Collation]]"
    },
    {
      "title": "Gojūon",
      "url": "https://en.wikipedia.org/wiki/Goj%C5%ABon",
      "text": "{{short description|Japanese ordering of kana}}\n{{Italic title|reason=[[:Category:Japanese words and phrases]]}}\n{| align=right style=\"margin-left:1em;\"\n|-\n|\n{| class=\"wikitable\" border=\"0\" cellpadding=\"2\" align=right style=\"vertical-align:top;padding:2px;\"\n|+ ''Gojūon'' ordering<br>(hiragana)\n|-\n! !! ''a'' !!| ''i'' !!| ''u'' !!| ''e'' !!| ''o''\n|- align=center\n!title=\"no lead consonant\"|∅\n|<big>[[あ]]</big>||<big>[[い]]</big>||<big>[[う]]</big>||<big>[[え]]</big>||<big>[[お]]</big>\n|- align=center\n!''K''\n|<big>[[か]]</big>||<big>[[き]]</big>||<big>[[く]]</big>||<big>[[け]]</big>||<big>[[こ]]</big>\n|- align=center\n!''S''\n|<big>[[さ]]</big>||<big>[[し]]</big>||<big>[[す]]</big>||<big>[[せ]]</big>||<big>[[そ]]</big>\n|- align=center\n!''T''\n|<big>[[た]]</big>||<big>[[ち]]</big>||<big>[[つ]]</big>||<big>[[て]]</big>||<big>[[と]]</big>\n|- align=center\n!''N''\n|<big>[[な]]</big>||<big>[[に]]</big>||<big>[[ぬ]]</big>||<big>[[ね]]</big>||<big>[[の]]</big>\n|- align=center\n!''H''\n|<big>[[は]]</big>||<big>[[ひ]]</big>||<big>[[ふ]]</big>||<big>[[へ]]</big>||<big>[[ほ]]</big>\n|- align=center\n!''M''\n|<big>[[ま]]</big>||<big>[[み]]</big>||<big>[[む]]</big>||<big>[[め]]</big>||<big>[[も]]</big>\n|- align=center\n!''Y''\n|<big>[[や]]</big>||bgcolor=\"#E0E0D8\"| ||<big>[[ゆ]]</big>||bgcolor=\"#E0E0D8\"| ||<big>[[よ]]</big>\n|- align=center\n!''R''\n|<big>[[ら]]</big>|||<big>[[り]]</big>||<big>[[る]]</big>||<big>[[れ]]</big>||<big>[[ろ]]</big>\n|- align=center\n!''W''\n|<big>[[わ]]</big>||bgcolor=\"#E0E0D8\"|<big>[[ゐ]]</big>||bgcolor=\"#E0E0D8\"| ||bgcolor=\"#E0E0D8\"|<big>[[ゑ]]</big>||<big>[[を]]</big>\n|-\n! colspan=\"6\"| Additional kana\n|-\n|<big>[[ん]]</big> || colspan=5 | \n|}\n|-  style=\"text-align: right;\"\n| {{Color box|#E0E0D8;|border=darkgray}} ''unused/obsolete''\n|}\n{{Japanese writing}}\n\nIn the [[Japanese language]], the {{Nihongo||{{linktext|五十音}}|'''gojūon'''|{{IPA-ja|ɡoʑɯːoɴ}}, {{abbr|lit.|literally}} \"fifty sounds\"}} is a traditional system ordering [[kana]] by their component phonemes, roughly analogous to [[alphabetical order]]. The \"fifty\" (''gojū'') in its name refers to the 5×10 grid in which the characters are displayed. Each kana, which may be a [[hiragana]] or [[katakana]] character, corresponds to one sound in Japanese. As depicted at the right using hiragana characters, the sequence begins with あ&nbsp;(''a''), い&nbsp;(''i''), う&nbsp;(''u''), え&nbsp;(''e''), お&nbsp;(''o''), then continues with か&nbsp;(''ka''), き&nbsp;(''ki''), く&nbsp;(''ku''), け&nbsp;(''ke''), こ&nbsp;(''ko''), and so on and so forth for a total of ten rows of five columns.\n\nAlthough nominally containing 50 characters, the grid is not completely filled, and, further, there is an extra character added outside the grid at the end: with 5 gaps and 1 extra character, the current number of distinct kana in a syllabic chart in modern Japanese is therefore 46. Some of these gaps have always existed as gaps in sound: there was no ''yi'' or ''wu'' in [[Old Japanese]], and ''ye'' disappeared in [[Early Middle Japanese]], predating the kana; the kana for ''i'', ''u'' and ''e'' double up for those phantom values. Also, with the spelling reforms after [[World War II]], the kana for ''wi'' and ''we'' were replaced with ''i'' and ''e'', the sounds they had developed into. The kana for syllabic ''n'' (hiragana [[ん]]) is not part of the grid, as it was introduced long after ''gojūon'' ordering was devised. (Previously ''mu'' (hiragana [[む]]) was used for this sound.)\n\nThe ''gojūon'' contains all the basic kana, but it does not include: \n*versions of kana with a ''[[dakuten]]'' such as が (''ga'') or だ (''da''), or kana with ''[[handakuten]]'' such as ぱ (''pa'') or ぷ (''pu''),\n*smaller kana, such as the ''[[sokuon]]'' (っ) or ''[[yōon]]'' (ゃ,ゅ,ょ).\n\nThe ''gojūon'' order is the prevalent system for [[collation|collating]] Japanese in Japan. For example, dictionaries are ordered using this method.\nOther systems used are the ''[[iroha]]'' ordering, and, for kanji, the [[Radical (Chinese character)|radical]] ordering.\n\n==History==\n{{Expand Japanese|date=June 2016}}<!-- 51全てが異なる字・音: 江戸後期から明治 in particular -->\n\nThe ''gojūon'' arrangement is thought to have been influenced by both the [[Siddham script]] used for writing [[Sanskrit]] and the Chinese ''[[fanqie]]'' system.<ref name=\"Auroux2000\">{{cite book|author=Sylvain Auroux|title=Geschichte Der Sprachwissenschaften: Ein Internationales Handbuch Zur Entwicklung Der Sprachforschung Von Den Anfängen Bis Zur Gegenwart|url=https://books.google.com/books?id=ygDHVYyEXOMC&pg=PA78|year=2000|publisher=Walter de Gruyter|isbn=978-3-11-011103-3|page=78|language=English}}</ref><ref name=\"KoernerAsher2014\">{{cite book|author1=E.F.K. Koerner|author2=R.E. Asher|title=Concise History of the Language Sciences: From the Sumerians to the Cognitivists|url=https://books.google.com/books?id=VCqLBQAAQBAJ&pg=PA46|date=28 June 2014|publisher=Elsevier|isbn=978-1-4832-9754-5|page=46}}</ref>\n\nThe monk [[Kūkai]] introduced the [[Siddhaṃ script]] to Japan in 806 on his return from China. Belonging to the [[Brahmic family of scripts|Brahmic script]], the Sanskrit ordering of letters was used for it. Buddhist monks who invented [[katakana]] chose to use the word order of Sanskrit and Siddham, since important Buddhist writings were written with those alphabets.<ref>[http://www.omniglot.com/writing/japanese_katakana.htm Japanese katakana] (Omniglot.com)</ref>\n\nIn an unusual set of events, although it uses Sanskrit organization (grid, with order of consonants and vowels), it also uses the Chinese order of writing (in columns, right-to-left).\n\n[[File:Brahmika.svg|thumb|[[Brāhmī script]], showing vowel ordering]]\nThe order of consonants and vowels, and the grid layout, originates in Sanskrit ''[[shiksha]]'' (''śikṣā'', Hindu phonetics and phonology), and [[Brāhmī script]], as reflected throughout the [[Brahmic family of scripts]].<ref>Daniels & Bright, ''The World's Writing Systems''</ref><ref name=\"miller\" /><ref>[http://www.sljfaq.org/afaq/kana-order-origin.html 1.1.5. What is the origin of the gojuuon kana ordering?], [http://www.sljfaq.org/afaq/afaq.html sci.lang.japan FAQ]</ref>\n\nThe Sanskrit was written left-to-right, with vowels changing in rows, not columns; writing the grid vertically follows [[vertical writing in East Asian scripts|Chinese writing convention]].\n\n===Discrepancies===\nThere are three ways in which the grid does not exactly accord with Sanskrit ordering of Modern Japanese; that is because the grid is based on [[Old Japanese]], and some sounds have changed in the interim.\n\n====''s''/{{lang|ja|さ}}====\nWhat is now ''s''/{{lang|ja|さ}} was previously pronounced {{IPA|[ts]}}, hence its location corresponding to Sanskrit {{IPA|/t͡ʃ/}}; in Sanskrit {{IPA|/s/}} appears towards the end of the list.<ref name=\"miller\">Miller, Roy Andrew ''The Japanese Language,'' {{ISBN|4-8053-0460-X}}, p. 128:\n\"The Indic order of listing phonemes as found in the arrangement of this so-called 'siddhāṃ' script, as well as in all the Indic writing systems, arranges the consonants in the following order: k, kh, g, gh, ñ, c, ch, j, jh, ṭ, ṭh, ḍ, ḍh, ṇ, t, th, d, dh, p, ph, b, bh, m, y, r, l, v, ś, ṣ, s, and ḥ ... Here the juxtaposition of modern 'h', Old Japanese 'f', with Indic 'p' is interesting and significant; the only other point which needs particular comment is the location of modern Japanese 's' following 'k'. This is easily understood since modern Japanese 's' goes back to the Old Japanese affricate phoneme {{IPA|/ts/}} which had an allophone {{IPA|[ts]}} before Old Japanese {{IPA|/a, u, o, ö/}} and an allophone {{IPA|[s]}} before {{IPA|/i, e/}}.\"</ref>\n\n====''h''/{{lang|ja|は}}====\nKana starting with ''h'' (e.g. {{lang|ja|は}}), ''b'' (e.g. {{lang|ja|ば}}) and ''p'' (e.g. {{lang|ja|ぱ}}) are placed where ''p/b'' are in Sanskrit (in Sanskrit, ''h'' is at the end) and the diacritics do not follow the usual pattern: ''p/b'' (as in Sanskrit) is the usual unvoiced/voiced pattern, and {{IPA|[h]}} has different articulation. This is because {{IPA|/h/}} was previously {{IPA|[p]}}, and pronouncing {{IPA|/h/}} as {{IPA|[h]}} is recent.\n\n(More detail at [[Old Japanese#Consonants|Old Japanese: Consonants]]; in brief: prior to Old Japanese, modern {{IPA|/h/}} was presumably {{IPA|[p]}}, as in [[Ryukyuan languages]]. [[Proto-Japanese]] is believed to have split into Old Japanese and the Ryukyuan languages in the [[Yamato period]] (250–710). In Old Japanese (from 9th century) and on to the 17th century, {{IPA|/h/}} was pronounced {{IPA|[ɸ]}}. The earliest evidence was from 842, by the monk [[Ennin]], writing in the ''[[Zaitōki]]'' that Sanskrit {{IPA|/p/}} is more labial than Japanese. The Portuguese later transcribed the は-row as ''fa/fi/fu/fe/fo''.)\n\n====''n''/{{lang|ja|ん}}====\nSyllable-final ''n'' ({{lang|ja|[[ん]]}}) was not present in Old Japanese (it developed following Chinese borrowings), does not fit with other characters due to having no vowel, and thus is attached at the end of the grid, as in Sanskrit treatment of miscellaneous characters.\n\n=== Examples ===\n[[File:Keypad on Japanese phone 708SC.jpg|right|thumb|Japanese mobile phone keypad, showing ''gojūon'' column labels]]\n\nThe earliest example of a ''gojūon''-style layout dates from a manuscript known as {{Nihongo|''Kujakukyō Ongi''|孔雀経音義}} dated {{circa|1004}}–1028.<ref>Mabuchi (1993: 169-174)</ref> In contrast, the earliest example of the alternative [[iroha]] ordering is from the 1079 text {{Nihongo|''Konkōmyō Saishōōkyō Ongi''|金光明最勝王経音義}}.<ref>Kubota (2007: 26)</ref>\n\n''Gojūon'' ordering was first used for a dictionary in the 1484 {{Nihongo|''[[Onkochishinsho]]''|温故知新書}}; following this use, ''gojūon'' and ''[[iroha]]'' were both used for a time, but today ''gojūon'' is more prevalent.\n\nToday the ''gojūon'' system forms the basis of [[Japanese input methods#Mobile phones|input methods for Japanese mobile phones]] – each key corresponds to a column in the ''gojūon'', while the number of presses determines the row. For example, the '2' button corresponds to the ''ka''-column (''ka'', ''ki'', ''ku'', ''ke'', ''ko''), and the button is pressed repeatedly to get the intended kana.\n\n==Table==\n\nThis table uses the [[tategaki and yokogaki|vertical system of Japanese writing]], and should be read from the top down, starting from the rightmost column, then to the left. In each entry, the top entry is the hiragana, the second entry is the corresponding katakana, the third entry is the [[Hepburn romanization]] of the kana, and the fourth entry is the pronunciation written in the [[International Phonetic Alphabet]] (IPA). Please see [[Japanese phonology]] for more details on the individual sounds.\n\n{| class=\"wikitable\"\n! &nbsp;/<small>N</small>/ || /w/|| /r/|| /y/|| /m/|| /h/|| /n/|| /t/|| /s/|| /k/|| Ø\n|-\n|rowspan=5 valign=\"top\"|{{lang|ja|[[ん]]}}<br>{{lang|ja|ン}}<br>''n''<br>{{IPA|[ɴ]}}<br>''etc.''\n|{{lang|ja|[[わ]]}}<br>{{lang|ja|ワ}}<br>''wa''<br>{{IPA|[w͍a]}}\n|{{lang|ja|[[ら]]}}<br>{{lang|ja|ラ}}<br>''ra''<br>{{IPA|[ɽa]}}\n|{{lang|ja|[[や]]}}<br>{{lang|ja|ヤ}}<br>''ya''<br>{{IPA|[ja]}}\n|{{lang|ja|[[ま]]}}<br>{{lang|ja|マ}}<br>''ma''<br>{{IPA|[ma]}}\n|{{lang|ja|[[は]]}}<br>{{lang|ja|ハ}}<br>''ha''<br>{{IPA|[ha]}}\n|{{lang|ja|[[な]]}}<br>{{lang|ja|ナ}}<br>''na''<br>{{IPA|[na]}}\n|{{lang|ja|[[た]]}}<br>{{lang|ja|タ}}<br>''ta''<br>{{IPA|[ta]}}\n|{{lang|ja|[[さ]]}}<br>{{lang|ja|サ}}<br>''sa''<br>{{IPA|[sa]}}\n|{{lang|ja|[[か]]}}<br>{{lang|ja|カ}}<br>''ka''<br>{{IPA|[ka]}}\n|{{lang|ja|[[あ]]}}<br>{{lang|ja|ア}}<br>''a''<br>{{IPA|[a]}}\n! /a/\n|-\n|{{lang|ja|[[ゐ]]{{ref|1a|1}}<br>ヰ}}<br>''wi''<br>{{IPA|[i]}}\n|{{lang|ja|[[り]]}}<br>{{lang|ja|リ}}<br>''ri''<br>{{IPA|[ɽi]}}\n|&nbsp;\t\t\t     \n|{{lang|ja|[[み]]}}<br>{{lang|ja|ミ}}<br>''mi''<br>{{IPA|[mi]}}\n|{{lang|ja|[[ひ]]}}<br>{{lang|ja|ヒ}}<br>''hi''<br>{{IPA|[çi]}}\n|{{lang|ja|[[に]]}}<br>{{lang|ja|ニ}}<br>''ni''<br>{{IPA|[ni]}}\n|{{lang|ja|[[ち]]}}<br>{{lang|ja|チ}}<br>''chi''<br>{{IPA|[tɕi]}}\n|{{lang|ja|[[し]]}}<br>{{lang|ja|シ}}<br>''shi''<br>{{IPA|[ɕi]}}\n|{{lang|ja|[[き]]}}<br>{{lang|ja|キ}}<br>''ki''<br>{{IPA|[ki]}}\n|{{lang|ja|[[い]]}}<br>{{lang|ja|イ}}<br>''i''<br>{{IPA|[i]}}\n! /i/\n|-\n| &nbsp;               \n|{{lang|ja|[[る]]}}<br>{{lang|ja|ル}}<br>''ru''<br>{{IPA|[ɽu͍]}}\n|{{lang|ja|[[ゆ]]}}<br>{{lang|ja|ユ}}<br>''yu''<br>{{IPA|[ju͍]}}\n|{{lang|ja|[[む]]}}<br>{{lang|ja|ム}}<br>''mu''<br>{{IPA|[mu͍]}}\n|{{lang|ja|[[ふ]]}}<br>{{lang|ja|フ}}<br>''fu''<br>{{IPA|[ɸu͍]}}\n|{{lang|ja|[[ぬ]]}}<br>{{lang|ja|ヌ}}<br>''nu''<br>{{IPA|[nu͍]}}\n|{{lang|ja|[[つ]]}}<br>{{lang|ja|ツ}}<br>''tsu''<br>{{IPA|[tsu͍]}}\n|{{lang|ja|[[す]]}}<br>{{lang|ja|ス}}<br>''su''<br>{{IPA|[su͍]}}\n|{{lang|ja|[[く]]}}<br>{{lang|ja|ク}}<br>''ku''<br>{{IPA|[ku͍]}}\n|{{lang|ja|[[う]]}}<br>{{lang|ja|ウ}}<br>''u''<br>{{IPA|[u͍]}}\n! /u/\n|-\n|{{lang|ja|[[ゑ]]{{ref|1b|1}}<br>ヱ}}<br>''we''<br>{{IPA|[e]}}\n|{{lang|ja|[[れ]]}}<br>{{lang|ja|レ}}<br>''re''<br>{{IPA|[ɽe]}}\n|&nbsp;\t\t\t     \n|{{lang|ja|[[め]]}}<br>{{lang|ja|メ}}<br>''me''<br>{{IPA|[me]}}\n|{{lang|ja|[[へ]]}}<br>{{lang|ja|ヘ}}<br>''he''<br>{{IPA|[he]}}\n|{{lang|ja|[[ね]]}}<br>{{lang|ja|ネ}}<br>''ne''<br>{{IPA|[ne]}}\n|{{lang|ja|[[て]]}}<br>{{lang|ja|テ}}<br>''te''<br>{{IPA|[te]}}\n|{{lang|ja|[[せ]]}}<br>{{lang|ja|セ}}<br>''se''<br>{{IPA|[se]}}\n|{{lang|ja|[[け]]}}<br>{{lang|ja|ケ}}<br>''ke''<br>{{IPA|[ke]}}\n|{{lang|ja|[[え]]}}<br>{{lang|ja|エ}}<br>''e''<br>{{IPA|[e]}}\n! /e/\n|-\n|{{lang|ja|[[を]]}}<br>{{lang|ja|ヲ}}<br>''wo''<br>{{IPA|[o]}}\n|{{lang|ja|[[ろ]]}}<br>{{lang|ja|ロ}}<br>''ro''<br>{{IPA|[ɽo]}}\n|{{lang|ja|[[よ]]}}<br>{{lang|ja|ヨ}}<br>''yo''<br>{{IPA|[jo]}}\n|{{lang|ja|[[も]]}}<br>{{lang|ja|モ}}<br>''mo''<br>{{IPA|[mo]}}\n|{{lang|ja|[[ほ]]}}<br>{{lang|ja|ホ}}<br>''ho''<br>{{IPA|[ho]}}\n|{{lang|ja|[[の]]}}<br>{{lang|ja|ノ}}<br>''no''<br>{{IPA|[no]}}\n|{{lang|ja|[[と]]}}<br>{{lang|ja|ト}}<br>''to''<br>{{IPA|[to]}}\n|{{lang|ja|[[そ]]}}<br>{{lang|ja|ソ}}<br>''so''<br>{{IPA|[so]}}\n|{{lang|ja|[[こ]]}}<br>{{lang|ja|コ}}<br>''ko''<br>{{IPA|[ko]}}\n|{{lang|ja|[[お]]}}<br>{{lang|ja|オ}}<br>''o''<br>{{IPA|[o]}}\n! /o/\n|}\n{{refbegin}}\n#^{{note label|1||a}}{{note label|1||b}} These kana are no longer in common use. They, and the three empty cells, are normally replaced with the plain vowel kana {{lang|ja|いうえ}} ({{lang|ja|イウエ}}) in the charts that Japanese use, but that has not been done here to avoid confusion.\n{{refend}}\n\nThe rows are referred to as {{nihongo3||段|dan}}, and the columns as {{nihongo3||行|gyō}}. They are named for their first entry, thus the rows are {{lang|ja|あ段 い段 う段 え段 お段}} while the columns are (in left-to-right order) {{lang|ja|わ行 ら行 や行 ま行 は行 な行 た行 さ行 か行 あ行}}. These are sometimes written in katakana, such as {{lang|ja|ア行}}, and conspicuously used when referring to [[Japanese verb conjugation]] – for example, the verb {{nihongo3|\"read\"|読む|yomu}} is of {{nihongo3|\"''ma''-column [[godan verb|5-class]] conjugation\"|マ行[[:ja:五段活用|五段活用]]|ma-gyō go-dan katsuyō}} type.\n\n==Ordering of variant kana==\n{{Unreferenced section|date=June 2016}}\nIn the ordering based on the ''gojūon'', smaller versions of kana are treated in the same way as full-size versions:\n* The ''[[sokuon]]'', the small kana ''tsu'', is ordered at the same position as the large ''tsu''. When the words are otherwise identical, it goes ''after'' them. For example,\n*: まつ,　まったく,　まつば,　まとう　(''matsu'', ''mattaku'', ''matsuba'', ''matou'')\n* ''[[Yōon]]'' sounds are ordered in the same positions as the full-sized sounds. When the words are otherwise identical, they collate ''after'' them. For example,\n*: きや,　きゃ,　きやく,　きゃく,　きゆ　(''kiya'', ''kya'', ''kiyaku'', ''kyaku'', ''kiyu'').\n\nVoiced versions (those with a ''[[dakuten]]'') are classified under their unvoiced versions; If the words are otherwise identical, the voiced version is placed ''after'' the unvoiced; ''[[handakuten]]'' are placed after dakuten. For example,\n: すす,　すず,　すすき,　すすぎ,　すずき,　すすむ,　すずむ　(''susu'', ''suzu'', ''susuki'', ''susugi'', ''suzuki'', ''susumu'', ''suzumu'').\nand\n: は,　ば,　ぱ　(''ha'', ''ba'', ''pa'')\n\n==Mnemonics==\n{{Unreferenced section|date=June 2016}}\nIn order to remember the ''gojūon'', various [[mnemonic]]s have been devised. For example,\n:'''''A'''h, '''K'''ana '''S'''ymbols: '''T'''ake '''N'''ote '''H'''ow '''M'''any '''Y'''ou '''R'''ead '''W'''ell.''\nand\n:'''''A'''h, '''K'''ana. '''S'''urely '''T'''ake '''N'''ote '''H'''ow '''M'''any '''Y'''ou '''R'''ead '''W'''ell.''\nand\n:'''''K'''ana '''S'''igns, '''T'''hink '''N'''ow '''H'''ow '''M'''uch '''Y'''ou '''R'''eally '''W'''ant'' (to learn them).\nand also\n:'''''A''' '''K'''ind '''S'''amurai '''T'''old '''N'''aomi '''H'''ow '''M'''y '''Y'''ak '''R'''an '''W'''ild.\n\nThe first letters in these phrases give the ordering of the non-voiced initial sounds.\n\nFor vowel ordering, the vowel sounds in the following English phrase may be used as a mnemonic:\n:'''''A'''h, w'''e''' s'''oo'''n g'''e'''t '''o'''ld.''\nThe vowel sounds in the English words approximate the Japanese vowels: a, i, u, e, o.\n\nOne can also use\n:H'''''AI'''''L [[UNESCO|'''''U'''''N'''''E'''''SC'''''O''''']]\nto remember the order of the vowels.\n\n==References==\n{{reflist}}\n\n==Bibliography==\n* {{NKBJ}}\n*{{cite book\n  | editor-last1 = Hanzawa\n  | editor-first1 = Kan'ichi\n  | editor-last2 = Abe\n  | editor-first2 = Kiyosuke\n  | editor-last3 = Ono\n  | editor-first3 = Masahiro\n  |display-editors = 3 | editor-last4 = Kaneko\n  | editor-first4 = Hiroshi\n  | title = Case Study: Nihongo no Rekishi\n  | publisher = Ofū\n  | language = Japanese\n  | year = 2002\n  | isbn =  978-4-273-03267-8\n  | chapter = Gojūonzu\n  | pages = 6–11\n}}\n*{{cite book\n  | last = Mabuchi\n  | first = Kazuo\n  | title = Gojūonzu no Hanashi\n  | publisher = Taishūkan Shoten\n  | language = Japanese\n  | year = 1993\n  | isbn =  4-469-22093-0\n}}\n*\"The Japanese language\", Roy Andrew Miller, {{ISBN|0-226-52718-2}}, describes the origin of gojūon in Sanskrit.\n*''Gendai Kokugo Reikai Jiten'', {{ISBN|4-09-501042-8}}, used to obtain examples of dictionary ordering.\n\n==External links==\n*[http://www.sljfaq.org/html/goj%C5%ABon.html sci.lang.japan FAQ on the origin of kana order] contains the relevant quote from the above reference.\n*[http://brng.jp/benri50on.pdf Kana Table (PDF)]\n\n{{Japanese language}}\n\n{{DEFAULTSORT:Gojuon}}\n[[Category:Japanese writing system]]\n[[Category:Collation]]"
    },
    {
      "title": "Icelandic orthography",
      "url": "https://en.wikipedia.org/wiki/Icelandic_orthography",
      "text": "'''Icelandic orthography''' is the way in which [[Icelandic language|Icelandic]] words are spelled and how their spelling corresponds with their pronunciation.\n\n==Alphabet==\n{|style=\"float:right\" cellspacing=0 border=0\n|-valign=\"top\"\n|[[Image:Latin letter Ð.svg|125px|thumb|right|[[Eth]]]]\n|[[Image:Latin letter Þþ.svg|125px|thumb|right|[[Thorn (letter)|Thorn]]]]\n|-\n|colspan=2|[[Image:Icelandic handwriting.JPG|300px|thumb|A handwriting extract; the Icelandic letters [[Eth|ð]] & [[Thorn (letter)|þ]] are visible]]\n|}\n\nThe Icelandic [[alphabet]] is a [[Latin-script alphabet]] including some letters duplicated with [[acute accent]]s; in addition, it includes the letter [[eth]] ({{lang|is|Ðð}}), [[transliterated]] as ''d'', and the [[runic]] letter [[Thorn (letter)|thorn]] ({{lang|is|Þþ}}), transliterated as ''th'' (see picture); {{lang|is|[[Ææ]]}} and {{lang|is|[[Öö]]}} are considered letters in their own right and not a [[Typographic ligature|ligature]] or [[diacritic]]al version of their respective letters. Icelanders call the ten extra letters (not in the [[English alphabet]]), especially thorn and eth, {{wikt-lang|is|séríslenskur}} (\"specifically Icelandic\" or \"uniquely Icelandic\"), although they are not. Eth is also used in [[Faroese language|Faroese]], and while thorn is no longer used in any other living language, it was used in many historical languages, including [[Old English]]. Icelandic words never start with {{lang|is|ð}}, which means the capital version {{lang|is|Ð}} is mainly just used when words are spelled using [[all caps|all capitals]].\n\nSometimes the [[glyph]]s are simplified when handwritten, for example {{lang|is|[[Ash (letter)|æ]]}} (considered a separate letter, originally a [[Typographical ligature|ligature]]) may be written as {{lang|is|[[ae (digraph)|ae]]}}, which can make it easier to write [[cursive]]ly.\n\nThe alphabet consists of the following 32 letters.\n{{Listen\n|filename=Is-Icelandic alphabet.oga\n|title=Icelandic alphabet\n|description=An Icelandic speaker reciting the alphabet in Icelandic\n|format=Ogg\n}}\n\n<center>\n{| class=\"wikitable\" style=\"width: 50em; text-align: center; border-collapse:collapse;\"\n|-\n! style=\"background: #efefef; font-weight: normal;\" colspan=\"32\" | '''[[Capital letters|Majuscule forms]]''' (also called '''uppercase''' or '''capital letters''')\n|- lang=\"is\"\n| [[A]] || [[Á]] || [[B]] || [[D]] || [[Eth|Ð]] || [[E]] || [[É]] || [[F]] || [[G]] || [[H]] || [[I]] || [[Í]] || [[J]] || [[K]] || [[L]] || [[M]] || [[N]] || [[O]] || [[Ó]] || [[P]] || [[R]] || [[S]] || [[T]] || [[U]] || [[Ú]] || [[V]] || [[X]] || [[Y]] || [[Ý]] || [[Þ]] || [[Æ]] || [[Ö]]\n|-\n! style=\"background: #efefef; font-weight: normal;\" colspan=\"32\" | '''[[Lower case|Minuscule forms]]''' (also called '''lowercase''' or '''small letters''')\n|- lang=\"is\"\n| a || á || b || d || ð || e || é || f || g || h || i || í || j || k || l || m || n || o || ó || p || r || s || t || u || ú || v || x || y || ý || þ || æ || ö\n|}\n</center>\n\n{|class=\"wikitable sortable\"\n!Letter\n!Name\n![[International Phonetic Alphabet|IPA]]\n!Frequency<ref>{{cite web|url=http://practicalcryptography.com/cryptanalysis/letter-frequencies-various-languages/icelandic-letter-frequencies/ |title=Icelandic Letter Frequencies |publisher=Practical cryptography |accessdate=4 April 2013}}</ref>\n|-\n|Aa || a || {{IPA-is|aː|}}||10.11%\n|-\n|Áá || á || {{IPA-is|auː|}} ||1.8%\n|-\n|Bb || bé || {{IPA-is|pjɛː|}}||1.04%\n|-\n|Dd || dé || {{IPA-is|tjɛː|}}||1.58%\n|-\n|Ðð || eð || {{IPA-is|ɛːθ|}}||4.39%\n|-\n|Ee || e || {{IPA-is|ɛː|}}||6.42%\n|-\n|Éé || é || {{IPA-is|jɛː|}}||0.65%\n|-\n|Ff || eff || {{IPA-is|ɛfː|}}||3.01%\n|-\n|Gg || ge || {{IPA-is|cɛː|}}||4.24%\n|-\n|Hh || há || {{IPA-is|hauː|}}||1.87%\n|-\n|Ii || i || {{IPA-is|ɪː|}}||7.58%\n|-\n|Íí || í || {{IPA-is|iː|}}||1.57%\n|-\n|Jj || joð || {{IPA-is|jɔːθ|}}||1.14%\n|-\n|Kk || ká || {{IPA-is|kʰauː|}}||3.31%\n|-\n|Ll || ell || {{IPA-is|ɛtːl̥|}}||4.53%\n|-\n|Mm || emm || {{IPA-is|ɛmː|}}||4.04%\n|-\n|Nn || enn || {{IPA-is|ɛnː|}}||7.71%\n|-\n|Oo || o || {{IPA-is|ɔː|}}||2.17%\n|-\n|Óó || ó || {{IPA-is|ou|}}||0.99%\n|-\n|Pp || pé || {{IPA-is|pʰjɛː|}}||0.79%\n|-\n|Rr || err || {{IPA-is|ɛr̥ː|}}||8.58%\n|-\n|Ss || ess || {{IPA-is|ɛsː|}}||5.63%\n|-\n|Tt || té || {{IPA-is|tʰjɛː|}}||4.95%\n|-\n|Uu || u || {{IPA-is|ʏː|}}||4.56%\n|-\n|Úú || ú || {{IPA-is|uː|}}||0.61%\n|-\n|Vv || vaff || {{IPA-is|vafː|}}||2.44%\n|-\n|Xx || ex || {{IPA-is|ɛxs|}}||0.05%\n|-\n|Yy || ufsilon y || {{IPA-is|ˈʏfsɪlɔn ɪː|}}||0.9%\n|-\n|Ýý || ufsilon ý || {{IPA-is|ˈʏfsɪlɔn iː|}}||0.23%\n|-\n|Þþ || þorn || {{IPA-is|θɔrtn̥|}}||1.45%\n|-\n|Ææ || æ || {{IPA-is|ai|}}||0.87%\n|-\n|Öö || ö || {{IPA-is|œː|}}||0.78%\n|-\n|}\n\n;Deleted letter\n\n{|class=\"wikitable\"\n! Letter\n! Name\n! [[International Phonetic Alphabet|IPA]]\n|-\n|Zz || seta || {{IPA-is|ˈsɛːta|}}\n|-\n|}\n\nThe letters ''a'', ''á'', ''e'', ''é'', ''i'', ''í'', ''o'', ''ó'', ''u'', ''ú'', ''y'', ''ý'', ''æ'' and ''ö'' are considered [[vowel]]s, and the remainder are [[consonant]]s.\n\nThe letters '''C''' (''sé'', {{IPA-is|sjɛː|}}), '''Q''' (''kú'', {{IPA-is|kʰuː|}}) and '''W''' (''tvöfalt vaff'', {{IPA-is|ˈtʰvœːfal̥t ˌvafː|}}) are only used in Icelandic in words of foreign origin and some proper names that are also of foreign origin. Otherwise, ''c, qu,'' and ''w'' are replaced by ''k/s/ts, hv,'' and ''v'' respectively. (In fact, ''hv'' etymologically corresponds to [[Latin language|Latin]] ''qu'' and English ''wh'' in words inherited from [[Proto-Indo-European language|Proto-Indo-European]]: Icelandic ''hvað'', Latin ''quod'', English ''what''.)\n\nThe letter '''Z''' (''seta'', {{IPA-is|ˈsɛːta|}}) was used until 1973, when it was abolished, as it was only an etymological detail. It originally represented an [[affricate]] {{IPAblink|t͡s}}, which arose from the combinations ''t''+''s'', ''d''+''s'', ''ð''+''s''; however, in modern Icelandic it came to be pronounced {{IPA-is|s|}}, and as it was a rare letter anyway it was decided in 1973 to replace all instances of ''z'' with ''s''.<ref>[http://www.visindavefur.is/svar.php?id=192 Hvers vegna var bókstafurinn ''z'' svona mikið notaður á Íslandi en því svo hætt?] {{is icon}}</ref> However, one of the most important newspapers in [[Iceland]], ''[[Morgunblaðið]]'', still uses it sometimes (although very rarely), and a secondary school, [[The Commercial College of Iceland|Verzlunarskóli Íslands]] has it in its name. It is also found in some proper names, and loanwords such as ''pizza''. Older people, who were educated before the abolition of the ''z'' sometimes also use it.\n\nWhile the letters '''C''', '''Q''', '''W''', and '''Z''' are found on the [[Icelandic keyboard]], they are rarely used in Icelandic; they are used in some proper names of Icelanders, mainly family names (family names are the [[Icelandic name|exception]] in Iceland). Many believe these letters should be included in the alphabet, as its purpose is a tool to collate.<!--\n\nFimm stafir í íslenska stafrófinu tákna tvö hljóð: x é á ó æ.\n\nStafirnir C, Q, W, og Z eru ekki notaðir almennt í íslensku, en koma fyrir í sumum nöfnum sem Íslendingar bera, aðallega ættarnöfnum, og finnast á íslensku lyklaborði. Margir telja að þeir ættu að vera með í íslenska stafrófinu, enda er stafrófið fyrst og fremst tæki til þess að raða orðum og/eða nöfnum í \"rétta\" röð. Séu þessir stafir ekki á ákveðnum stað í stafrófinu getur enginn sagt hvar raða skal nöfnum eins og Carl eða Walter, sem bæði eru vel þekkt hérlendis.--> The alphabet as taught in schools up to about 1980 has these 36 letters (and computers still order this way): a, á, b, c, d, ð, e, é, f, g, h, i, í, j, k, l, m, n, o, ó, p, q, r, s, t, u, ú, v, w, x, y, ý, z, þ, æ, ö.\n\n===History===\nThe modern Icelandic alphabet has developed from a standard established in the 19th century, by the Danish linguist [[Rasmus Rask]] primarily. It is ultimately based heavily on an orthographic standard created in the early 12th century by a document referred to as ''[[First Grammatical Treatise|The First Grammatical Treatise]]'', author unknown. The standard was intended for the common [[North Germanic languages|North Germanic language]], [[Old Norse]]. It did not have much influence, however, at the time.\n\nThe most defining characteristics of the alphabet were established in the old treatise:\n\n* Use of the [[acute accent]] (originally to signify [[vowel length]]).\n* Use of ''þ'', also used in the [[Old English language|Old English]] alphabet as the letter [[Þ|thorn]].\n\nThe later Rasmus Rask standard was basically a re-enactment of the old treatise, with some changes to fit concurrent [[North Germanic languages|North Germanic]] conventions, such as the exclusive use of ''k'' rather than ''c''. Various old features, like ''ð'', had actually not seen much use in the later centuries, so Rask's standard constituted a major change in practice.\n\nLater 20th century changes are most notably the adoption of ''é'', which had previously been written as ''je'' (reflecting the modern pronunciation), and the replacement of ''z'' with ''s'' in 1973.<ref>{{cite web|url=https://notendur.hi.is/~eirikur/av/stafsetn.htm |title=Stafsetning og greinarmerkjasetning |quote=2. og 3. grein fjalla um bókstafinn z, brottnám hans úr íslensku, og ýmsar afleiðingar þess. z var numin brott úr íslensku ritmáli með auglýsingu menntamálaráðuneytisins í september 1973 (ekki 1974, eins og oft er haldið fram). |author=Eiríkur Rögnvaldsson |language=is |accessdate=9 May 2014}} {{is icon}}</ref>\n\n==Function of symbols==\nThis section lists [[Icelandic language|Icelandic]] letters and letter combinations, and how to pronounce them using a narrow [[International Phonetic Alphabet]] transcription.<ref name=\"thrainsson\">{{cite book|author=Höskuldur Þráinsson|chapter=Icelandic |title=The Germanic Languages |date=2002 |editor1-last=König |editor1-first=Ekkehard |editor2-last=van der Auwera |editor2-first=Johan |pages=142–152 |series=Routledge Language Family Descriptions}}</ref><ref name=\"Einarsson 1949\">{{cite book|author=Stefán Einarsson|title=Icelandic: Grammar, Texts, Glossary|year=1949|publisher=The Johns Hopkins Press|location=Baltimore|pages=1–25}}</ref>\n\nIcelandic vowels may be either long or short, but this distinction is only relevant in stressed syllables: unstressed vowels are neutral in quantitative aspect. The vowel length is determined by the consonants that follow the vowel: if there is only one consonant (i.e., a {{IPA-is|VC|}} syllable), the vowel is long; if there are more than one ({{IPA-is|VCC|}}), including geminates and pre-aspirated stops, the vowel is short. There are, however, some exceptions to this rule:\n# A vowel is long when the first consonant following it is {{IPA-is|p t k s|}} and the second {{IPA-is|v j r|}}, e.g. ''esja'', ''vepja'', ''akrar'', ''vökvar'', ''tvisvar''.\n# A vowel is also long in monosyllabic substantives with a genitive ''-s'' whose stem ends in a single {{IPA-is|p t k|}} following a vowel (e.g. ''ráps'', ''skaks''), except if the final {{IPA-is|p t k|}} is assimilated into the {{IPA-is|s|}}, e.g. ''báts''.\n# The first word of a compound term preserves its long vowel if its following consonant is one of the group {{IPA-is|p t k s|}}, e.g. ''matmál''.\n# The non-compound verbs ''vitkast'' and ''litka'' have long vowels.\n\n\n{| class=\"wikitable\" style=\"text-align: center;\"\n|+ Vowels\n|-\n! rowspan=\"2\" | Grapheme\n! colspan=\"3\" | Sound ([[Help:IPA|IPA]])\n! rowspan=\"2\" | Examples\n|-\n! Long\n! Short\n! Before<br/>''ng'' or ''nk''\n|-\n! a\n| {{IPA-is|aː|}}\n| {{IPA-is|a|}}\n| {{IPA-is|au|}}\n| style=\"text-align: left;\" | ''taska'' {{IPA-is|ˈtʰaska||Is-taska.oga}} \"handbag\"<br/>''kaka'' {{IPA-is|ˈkʰaːka||Is-kaka.oga}} \"cake\"<br/>''svangur'' {{IPA-is|ˈsvauŋkʏr̥|}} \"hungry\"\n|-\n! á\n| {{IPA-is|auː|}}\n| colspan=\"2\" | {{IPA-is|au|}}\n| style=\"text-align: left;\" | ''fár'' {{IPA-is|fauːr̥||Is-fár.oga}} \"disaster\"\n|-\n! au\n| {{IPA-is|øiː|}}\n| colspan=\"2\" | {{IPA-is|øi|}}\n| style=\"text-align: left;\" | ''þau'' {{IPA-is|θøiː||Is-Þau.oga}} \"they\"\n|-\n! e\n| {{IPA-is|ɛː|}}\n| {{IPA-is|ɛ|}}\n| {{IPA-is|ei|}}\n| style=\"text-align: left;\" | ''skera'' {{IPA-is|ˈscɛːra|}} \"to cut\"<br/>''drekka'' {{IPA-is|ˈtrɛʰka||Is-drekka.oga}} \"to drink\"<br/>''drengur'' {{IPA-is|ˈtreiŋkʏr̥|}} \"boy\"\n|-\n! é\n| {{IPA-is|jɛː|}}\n| colspan=\"2\" | {{IPA-is|jɛ|}}\n| style=\"text-align: left;\" | ''ég'' {{IPA-is|jɛːx||Is-ég.oga}} \"I\"\n|-\n! ei, ey\n| {{IPA-is|eiː|}}\n| colspan=\"2\" | {{IPA-is|ei|}}\n| style=\"text-align: left;\" | ''skeið'' {{IPA-is|sceiːθ||Is-skeið.oga}} \"spoon\"<br/>''hey'' {{IPA-is|heiː|}}  \"hay\"\n|-\n! i, y\n| {{IPA-is|ɪː|}}\n| {{IPA-is|ɪ|}}\n| {{IPA-is|i|}}\n| style=\"text-align: left;\" | ''sin'' {{IPA-is|sɪːn||Is-sin.oga}} \"sinew\"<br/>''syngja'' {{IPA-is|ˈsiɲca|}} \"to sing\"\n|-\n! í, ý\n| {{IPA-is|iː|}}\n| colspan=\"2\" | {{IPA-is|i|}}\n| style=\"text-align: left;\" | ''íslenska'' {{IPA-is|ˈistlɛnska||Is-Íslenska.oga}} \"Icelandic\"\n|-\n! o\n| {{IPA-is|ɔː|}}\n| colspan=\"2\" | {{IPA-is|ɔ|}}\n| style=\"text-align: left;\" | ''lofa'' {{IPA-is|ˈlɔːva||lofa.ogg}} \"to promise\"<br/>''dolla'' {{IPA-is|ˈtɔtla|}} \"pot\"\n|-\n! ó\n| {{IPA-is|ouː|}}\n| colspan=\"2\" | {{IPA-is|ou|}}\n| style=\"text-align: left;\" | ''rós'' {{IPA-is|rouːs||Is-rós.oga}} \"rose\"\n|-\n! u\n| {{IPA-is|ʏː|}}\n| {{IPA-is|ʏ|}}\n| {{IPA-is|u|}}\n| style=\"text-align: left;\" | ''hundur'' {{IPA-is|ˈhʏntʏr̥||Is-hundur.oga}} \"dog\"<br/>''munkur'' {{IPA-is|ˈmuŋ̊kʏr̥|}} \"monk\"\n|-\n! ú\n| {{IPA-is|uː|}}\n| colspan=\"2\" | {{IPA-is|u|}}\n| style=\"text-align: left;\" | ''þú'' {{IPA-is|θuː||Is-Þú.oga}} \"you\"\n|-\n! æ\n| {{IPA-is|aiː|}}\n| colspan=\"2\" | {{IPA-is|ai|}}\n| style=\"text-align: left;\" | ''læsa'' {{IPA-is|ˈlaiːsa||Is-Læsa.ogg}} \"lock\"\n|-\n! ö\n| {{IPA-is|œː|}}\n| {{IPA-is|œ|}}\n| {{IPA-is|øi|}}\n| style=\"text-align: left;\" | ''ör'' {{IPA-is|œːr||Is-ör.oga}} \"scar\"<br/>''hnöttur'' {{IPA-is|ˈn̥œʰtʏr̥|}} \"globe\"<br />''öngull'' {{IPA-is|ˈøiŋkʏtl̥|}} \"hook\"\n|}\n\n{| class=\"wikitable\"\n|+ Consonants\n|-\n! Grapheme\n! Phonetic realization ([[Help:IPA|IPA]])\n! Examples\n|-\n! rowspan=\"2\" | b\n| ''in most cases:''\n:{{IPA-is|p|}} unaspirated [[voiceless bilabial stop]]\n| ''bær'' {{IPA-is|paiːr̥||Is-Bær.ogg}} \"town\"\n|-\n| ''between '''m''' and '''d''', '''t''', '''s''', or '''g''':''\n:{{IPA|∅}}\n| ''kembt'' {{IPA-is|cʰɛm̥t|}} \"combed [past participle]\"\n|-\n! rowspan=\"2\" | d\n| ''in most cases:''\n:{{IPA-is|t|}} unaspirated [[voiceless dental stop]]\n| ''dalur'' {{IPA-is|ˈtaːlʏr̥||Is-dalur.oga}} \"valley\"\n|-\n| ''between '''l''' or '''n''' and '''g''', '''n''', '''l''', '''k''', or '''s''':''\n:{{IPA|∅}}\n| ''lands'' {{IPA-is|lans|}} \"land's [genitive]\"\n|-\n! rowspan=\"3\" | ð\n| ''between vowels, between a vowel and a voiced consonant, or at end of word:''\n:{{IPA-is|ð̠|}} [[Voiced alveolar fricative#voiced alveolar non-sibilant fricative|voiced alveolar non-sibilant fricative]]\n| ''eða'' {{IPA-is|ˈɛːða||Is-eða.oga}} \"or\"<br />''bað'' {{IPA-is|paːθ||Is-bað.ogg}} \"bath\"\n|-\n| ''before a [[voiceless consonant]] and before a pause:''\n:{{IPA-is|θ̠|}} [[voiceless alveolar fricative#voiceless alveolar non-sibilant fricative|voiceless alveolar non-sibilant fricative]]\n| ''maðkur'' {{IPA-is|ˈmaθkʏr̥||Is-maðkur.oga}} \"worm\"\n|-\n| ''between '''r''' and '''n''', and between '''g''' and '''s''':''\n:{{IPA|∅}}\n| ''harðna'' {{IPA-is|ˈhartna|}} \"to harden\"<br />''bragðs'' {{IPA-is|praxs|}} \"trick's [genitive], flavour's [genitive]\"\n|-\n! rowspan=\"4\" | f\n| ''at the beginning of a word or before a voiceless consonant, and when doubled:''\n: {{IPA-is|f|}}\n| ''fundur'' {{IPA-is|ˈfʏntʏr̥|}} \"meeting\"<br />''haft'' {{IPA-is|haft|}} \"had [past participle]\"\n|-\n| ''between vowels, between a vowel and a voiced consonant, or at the end of a word:''\n: {{IPA-is|v|}}\n| ''lofa'' {{IPA-is|ˈlɔːva||lofa.ogg}} \"to promise\"<br />''horfa'' {{IPA-is|ˈhɔrva|}} \"look\"\n|-\n| ''between '''ó''' and a vowel:''\n: {{IPA|∅}}\n| ''prófa'' {{IPA-is|ˈpʰr̥ou.a||is-prófa.ogg}} \"test\"<br />''gulrófa'' {{IPA-is|ˈkʏlˌrou.a||is-gulrófa.ogg}} \"rutabaga\"\n|-\n| ''before '''l''' or '''n''':''\n: {{IPA-is|p|}}\n| ''Keflavík'' {{IPA-is|ˈcʰɛplaˌviːk||is-keflavík.ogg}} \"[[Keflavík]]\"\n|-\n! fnd\n| {{IPA-is|mt|}}\n| ''hefnd'' {{IPA-is|hɛmt||is-hefnd.ogg}} \"revenge\"\n|-\n! fnt\n| {{IPA-is|m̥t|}} (voiceless)\n| ''nefnt'' {{IPA-is|nɛm̥t||IsNefnt.ogg}} \"named\"\n|-\n! rowspan=\"6\" | g\n| ''beginning of word, before a consonant or '''a''', '''á''', '''é''', '''o''', '''ó''', '''u''', '''ú''' or '''ö'''; or between vowel and '''l''' or '''n''':''\n: {{IPA-is|k|}} unaspirated [[voiceless velar stop]]\n| ''glápa'' {{IPA-is|ˈklauːpa||Is-glápa.oga}} \"to stare\"<br />''logn'' {{IPA-is|lɔkn||Is-logn.ogg}} \"windstill\"\n|-\n| ''beginning of word, before '''e''', '''i''', '''í''', '''j''', '''y''', '''ý''', '''æ''', '''ei''' or '''ey''':''\n: {{IPA-is|c|}} unaspirated [[voiceless palatal stop]]\n| ''geta'' {{IPA-is|ˈcɛːta||Is-geta.oga}} \"to be able\"\n|-\n| ''between a vowel and '''a''', '''u''', '''ð''', '''l''' or '''r'''; or at end of word:''\n: {{IPA-is|ɣ|}} [[voiced velar fricative]]\n| ''fluga'' {{IPA-is|ˈfl̥ʏːɣa||Is-fluga.oga}} \"fly\"<br />''lag'' {{IPA-is|laːx||Is-lag.oga}} \"layer\"\n|-\n| ''before '''t''' or '''s''' or before a pause:''\n: {{IPA-is|x|}} [[voiceless velar fricative]]\n| ''dragt'' {{IPA-is|traxt|}} \"suit\"\n|-\n| ''between a vowel and '''j''' or '''i''':''\n: {{IPA-is|j|}} [[palatal approximant]]\n| ''segja'' {{IPA-is|ˈsɛjːa|}} \"to say\"\n|-\n| ''between '''á''', '''ó''', '''ú''', and '''a''' or '''u''':''\n: {{IPA|∅}}\n| ''fljúga'' {{IPA-is|ˈfl̥juː.a|}} \"to fly\"\n|-\n! gj\n| {{IPA-is|c|}} unaspirated [[voiceless palatal stop]]\n| ''gjalda'' {{IPA-is|ˈcalta|}} \"to pay\"\n|-\n! h\n| {{IPA-is|h|}} [[voiceless glottal fricative]]\n| ''hár'' {{IPA-is|hauːr̥|}} \"hair\"\n|-\n! hj\n| {{IPA-is|ç|}} [[voiceless palatal fricative]]\n| ''hjá'' {{IPA-is|çauː|}} \"next to\"\n|-\n! hl\n| {{IPA-is|l̥|}} voiceless [[alveolar lateral approximant]]\n| ''hlýr'' {{IPA-is|l̥iːr̥|}} \"warm\"\n|-\n! hn\n| {{IPA-is|n̥|}} voiceless [[alveolar lateral approximant]]\n| ''hné'' {{IPA-is|n̥jɛː|}} \"knee\"\n|-\n! hr\n| {{IPA-is|r̥|}} [[voiceless alveolar trill]]\n| ''hratt'' {{IPA-is|r̥aʰt|}} \"fast\"\n|-\n! hv\n| {{IPA-is|kʰv|}} ({{IPA-is|xv|}} among some older speakers in southern Iceland)\n| ''hvað'' {{IPA-is|kʰvaːθ||Is-hvað.oga}} \"what\"\n|-\n! j\n| {{IPA-is|j|}}\n| ''já'' {{IPA-is|jauː|}} \"yes\"\n|-\n! rowspan=\"6\" | k\n| ''beginning of word, before a consonant or '''a''', '''á''', '''é''', '''o''', '''ó''', '''u''', '''ú''' or '''ö''':''\n: {{IPA-is|kʰ|}}\n| ''kaka'' {{IPA-is|ˈkʰaːka||Is-kaka.oga}} \"cake\"\n|-\n| ''beginning of word, before '''e''', '''i''', '''í''', '''y''', '''ý''', '''æ''', '''ei''' or '''ey''':''\n: {{IPA-is|cʰ|}} aspirated [[voiceless palatal stop]]\n| ''keyra'' {{IPA-is|ˈcʰeiːra|}} \"to drive\"<br />''kynskiptingur'' {{IPA-is|ˈcʰɪːnscɪftiŋkʏr̥||is-kynskiptingur.ogg}} \"transsexual\"\n|-\n| ''other contexts, before '''a''', '''á''', '''é''', '''o''', '''ó''', '''u''', '''ú''' or '''ö''':''\n: {{IPA-is|k|}}\n| ''skarfur'' {{IPA-is|ˈskarvʏr̥|}} \"cormorant\"<br />''haka'' {{IPA-is|ˈhaːka|}} \"chin\"\n|-\n| ''other contexts, before '''e''', '''i''', '''í''', '''y''', '''ý''', '''æ''', '''ei''' or '''ey''':''\n: {{IPA-is|c|}} unaspirated [[voiceless palatal stop]]\n| ''skip'' {{IPA-is|ˈscɪːp|}} \"boat\"<br />''hroki'' {{IPA-is|ˈr̥ɔːcɪ|}} \"arrogance\"\n|-\n| ''before '''n''', '''l''' or '''m''':''\n: {{IPA-is|ʰk|}}\n| ''vakna'' {{IPA-is|vaʰkna|}} \"wake up\", ''miklir'' {{IPA-is|mɪʰklɪr̥|}} \"great (pl.)\"\n|-\n| ''before '''t''':''\n: {{IPA-is|x|}} [[voiceless velar fricative]]\n| ''október'' {{IPA-is|ˈɔxtouːpɛr̥|}} \"October\"\n|-\n! rowspan=\"2\" | kj\n| ''beginning of word:''\n: {{IPA-is|cʰ|}} aspirated [[voiceless palatal stop]]\n| ''kjöt'' {{IPA-is|cʰœːt|}} \"meat\"\n|-\n| ''other contexts:''\n: {{IPA-is|c|}} unaspirated [[voiceless palatal stop]]\n| ''þykja'' {{IPA-is|ˈθɪːca|}} \"to regard\"\n|-\n! kk\n| {{IPA-is|ʰk|}}, {{IPA-is|ʰc|}}\n| ''þakka'' {{IPA-is|ˈθaʰka||is-þakka.ogg}} \"to thank\"<br />''ekki'' {{IPA-is|ˈɛʰcɪ|}} \"not\"\n|-\n! rowspan=\"2\" | l\n| ''in most cases:''\n: {{IPA-is|l|}}\n| ''lás'' {{IPA-is|lauːs||Is-Lás.ogg}} \"lock\"\n|-\n| ''at end of word, or next to a voiceless consonant:''\n: {{IPA-is|l̥|}} voiceless [[alveolar lateral approximant]]\n| ''sól'' {{IPA-is|souːl̥||Is-sól.oga}} \"sun\"<br />''stúlka'' {{IPA-is|ˈstul̥ka|}} \"girl\"\n|-\n! rowspan=\"2\" | ll\n| ''in most cases:''\n: {{IPA-is|tl|}}\n| ''bolli'' {{IPA-is|ˈpɔtlɪ||Is-bolli.oga}} \"cup\"<br />''milli'' {{IPA-is|ˈmɪtlɪ||Is-milli.oga}} \"between\"\n|-\n| ''in loan words and pet names:''\n: {{IPA-is|lː|}}\n| ''bolla'' {{IPA-is|ˈpɔlːa||Is-bolla.oga}} \"bun, bread roll\"<br />''mylla'' {{IPA-is|ˈmɪlːa||Is-mylla.oga}} \"mill\"\n|-\n! rowspan=\"2\" | m\n| ''in most cases:''\n: {{IPA-is|m|}}\n| ''mamma'' {{IPA-is|ˈmamːa|}} \"mum\"\n|-\n| ''after and before voiceless consonants:''\n: {{IPA-is|m̥|}}\n| ''lampi'' {{IPA-is|ˈlam̥pɪ|}} \"lamp\"\n|-\n! rowspan=\"2\" | n\n| ''in most cases:''\n: {{IPA-is|n|}}\n| ''nafn'' {{IPA-is|napn̥|}} \"name\"\n|-\n| ''after and before voiceless consonants:''\n: {{IPA-is|n̥|}}\n| ''planta'' {{IPA-is|ˈpʰlan̥ta|}} \"plant\"<br />''hnífur'' {{IPA-is|ˈn̥iːvʏr|}} \"knife\"\n|-\n! rowspan=\"2\" | ng\n| ''in most cases:''\n: {{IPA-is|ŋk|}}, {{IPA-is|ɲc|}}\n| ''vængur'' {{IPA-is|ˈvaiŋkʏr̥|}} \"wing\"<br />''engi'' {{IPA-is|ˈeiɲcɪ|}} \"meadow\"\n|-\n| ''before '''d''', '''l''' or '''s''':''\n: {{IPA-is|ŋ|}}\n| ''kringla'' {{IPA-is|ˈkʰriŋla|}} \"disc\"<br />''gangs'' {{IPA-is|ˈkauŋs|}} \"movement's [genitive]\"\n|-\n! nk\n| {{IPA-is|ŋ̊k|}}, {{IPA-is|ɲ̊c|}}\n| ''hönk'' {{IPA-is|ˈhøiŋ̊k|}} \"coil, loop\"<br />''banki'' {{IPA-is|ˈpauɲ̊cɪ|}} \"bank\"\n|-\n! rowspan=\"2\" | nn\n| ''after accented vowels or diphthongs:''\n: {{IPA-is|tn̥|}}\n| ''steinn'' {{IPA-is|steitn̥|}} \"rock\"<br />''fínn'' {{IPA-is|fitn̥|}} \"fine\"\n|-\n| ''all other contexts:''\n: {{IPA-is|nː|}}\n| ''finna'' {{IPA-is|ˈfɪnːa|}} \"to find\"\n|-\n! rowspan=\"4\" | p\n| ''beginning of word:''\n: {{IPA-is|pʰ|}} aspirated [[voiceless bilabial stop]]\n| ''par'' {{IPA-is|pʰaːr̥||Is-par.oga}} \"pair\"\n|-\n| ''other contexts:''\n: {{IPA-is|p|}} unaspirated [[voiceless bilabial stop]]\n| ''spara'' {{IPA-is|ˈspaːra||Is-spara.oga}} \"to save\"<br />''kápa'' {{IPA-is|ˈkʰauːpa|}} \"coat\"\n|-\n| ''before '''s''', '''k''' or '''t''':''\n: {{IPA-is|f|}} [[voiceless labiodental fricative]]\n| ''September'' {{IPA-is|ˈsɛftɛmpɛr̥|}} \"September\"<br />''skips'' {{IPA-is|scɪfs|}} \"ship's [genitive]\"\n|-\n| ''before '''n''', '''l''' or '''m''':''\n: {{IPA-is|ʰp|}}\n| ''vopn'' {{IPA-is|vɔʰpn̥|}} \"weapon(s)\", ''epli'' {{IPA-is|ɛʰplɪ|}} \"apple(s)\"\n|-\n! pp\n| {{IPA-is|ʰp|}}\n| ''stoppa'' {{IPA-is|ˈstɔʰpa||is-stoppa.ogg}} \"to stop\"\n|-\n! rowspan=\"2\" | r\n| ''at the beginning of words and between vowels:''\n: {{IPA-is|r|}} (voiced [[alveolar trill]] or [[alveolar tap|tap]])\n| ''rigna'' {{IPA-is|ˈrɪkna|}} \"to rain\"<br />''læra'' {{IPA-is|ˈlaiːra|}} \"to learn\"\n|-\n| ''before and after voiceless consonants and before a pause:''\n: {{IPA-is|r̥|}} (voiceless alveolar trill or [[voiceless alveolar tap|tap]])\n| ''svartur'' {{IPA-is|ˈsvar̥tʏr̥|}} \"black\"\n|-\n! rl\n| {{IPA-is|rtl̥|}}\n| ''karlmaður'' {{IPA-is|ˈkʰartl̥ˌmaːðʏr̥|}} \"male human\"\n|-\n! rowspan=\"2\" | rn\n| ''in most cases:''\n: {{IPA-is|rtn̥|}}\n| ''þorn'' {{IPA-is|θɔrtn̥|}} \"the name of the letter Þ\"\n|-\n| ''before '''d''':''\n: {{IPA-is|rn|}}\n| ''vernd'' {{IPA-is|vɛrnt|}} \"protection\" \n|-\n! s\n| {{IPA-is|s|}}\n| ''sósa'' {{IPA-is|ˈsouːsa|}} \"sauce\"\n|-\n! sl\n| {{IPA-is|stl̥|}}\n| ''rusl'' {{IPA-is|rʏstl̥|}} \"garbage\"\n|-\n! sn\n| {{IPA-is|stn̥|}}\n| ''býsna'' {{IPA-is|ˈpistn̥a|}} \"extremes\"\n|-\n! rowspan=\"3\" | t\n| ''beginning of word:''\n: {{IPA-is|tʰ|}} aspirated [[voiceless dental stop]]\n| ''taka'' {{IPA-is|ˈtʰaːka||Is-taka.oga}} \"take\"\n|-\n| ''before '''n''', '''l''' or '''m''':''\n: {{IPA-is|ʰt|}}\n| ''vatn'' {{IPA-is|vaʰtn̥|}} \"water\", ''Atli'' {{IPA-is|aʰtlɪ|}} \"man's name\", ''rytmi'' {{IPA-is|rɪʰtmɪ|}} \"rhythm\"\n|-\n| ''other contexts:''\n: {{IPA-is|t|}} unaspirated [[voiceless dental stop]]\n| ''stela'' {{IPA-is|ˈstɛːla||Is-stela.oga}} \"to steal\"<br />''skutur'' {{IPA-is|ˈskʏːtʏr̥|}} \"stern\"\n|-\n! tt\n| {{IPA-is|ʰt|}}\n| ''detta'' {{IPA-is|ˈtɛʰta|}} \"to fall\"\n|-\n! v\n| {{IPA-is|v|}}\n| ''vera'' {{IPA-is|ˈvɛːra|}} \"to be\"\n|-\n! x\n| {{IPA-is|xs|}} (or {{IPA-is|ks|}})\n| ''lax'' {{IPA-is|laxs|}} \"salmon\"\n|-\n! z\n| {{IPA-is|s|}}\n| ''beztur'' {{IPA-is|ˈpɛstʏr̥|}} \"the best\" (former orthography)<br />''Zakarías'' {{IPA-is|ˈsaːkʰariːas|}} \"Zachary\"\n|-\n! þ\n| {{IPA-is|θ̠|}} [[voiceless alveolar fricative#voiceless alveolar non-sibilant fricative|voiceless alveolar non-sibilant fricative]]\n| ''þú'' {{IPA-is|θuː|}} \"you\"<br />''Aþena'' {{IPA-is|ˈaːθɛna||is-aþena.ogg}} \"Athens\"\n|}\n\n==Code pages==\nBesides the alphabet being part of [[Unicode]], that's much used in Iceland, [[ISO 8859-1]] has historically been the most used code page.   [[ISO 8859-15]] also supports Icelandic and [[Windows-1252]] that extends it, which may also have a lot of use.\n\n==See also==\n* [[Icelandic Encyclopedia A-Ö]]\n* [[N-rule]]\n\n==References==\n{{Reflist}}\n\n==External links==\n*[http://www.hi.is/~eirikur/ptg_ice.pdf Guidelines for phonetic transcription]\n*[http://raskraska.ru/azbuka/icelandic_abc/icelandic_letters.html Printable icelandic alphabet. Letters for printing on the printer]\n\n{{Icelandic language}}\n{{Language orthographies}}\n\n{{DEFAULTSORT:Icelandic Orthography}}\n[[Category:Icelandic orthography]]\n[[Category:Latin-script orthographies]]\n[[Category:Collation]]"
    },
    {
      "title": "Iroha",
      "url": "https://en.wikipedia.org/wiki/Iroha",
      "text": "{{short description|Early Middle Japanese pangram poem, composed before 1079 in the Heian period}}\n{{other uses}}\n{{Italic title|reason=[[:Category:Japanese words and phrases]]}}\nThe {{Nihongo|'''''Iroha'''''|いろは}} is a [[Japanese language|Japanese]] poem, probably written in the [[Heian period|Heian era]] (794–1179).  Originally the poem was attributed to the founder of the [[Shingon]] Esoteric sect of Buddhism in Japan, [[Kūkai]], but more modern research has found the date of composition to be later in the [[Heian Period]].<ref>{{harvp|Abe|1999|pp=392, 398}}</ref> The first record of its existence dates from 1079. It is famous because it is a perfect [[pangram]], containing each character of the [[Kana|Japanese syllabary]] exactly once. Because of this, it is also used as an [[collation|ordering]] for the syllabary, in the same way as the ''A'', ''B'', ''C'', ''D''... sequence of the Latin alphabet.\n\n==Text==\nThe first appearance of the Iroha, in {{nihongo|''Konkōmyōsaishōōkyō Ongi''|金光明最勝王経音義||'Readings of [[Golden Light Sutra]]'}} was in seven lines: six with seven [[morae]] each, and one with five. It was also written in [[man'yōgana]].\n<blockquote lang=\"ja\" style=\"line-height:1.8em;\"><poem>\n以呂波耳本部止\n千利奴流乎和加\n餘多連曽津祢那\n良牟有為能於久\n耶万計不己衣天\n阿佐伎喩女美之\n恵比毛勢須\n</poem></blockquote>\n\nStructurally, however, the poem follows the standard 7–5 pattern of Japanese poetry (with one hypometric line), and in modern times it is generally written that way, in contexts where line breaks are used. The text of the poem in [[hiragana]] (with archaic {{lang|ja|[[Wi (kana)|ゐ]]}} and {{lang|ja|[[We (kana)|ゑ]]}} but without voiced consonant marks) is:\n\n{| class=\"wikitable\" border=\"1\"\n! colspan=2 | Archaic !! colspan=2 | Modern !! colspan=1 | Ordering (see [[#Usage|usage]]) !! Translation\n|-\n! [[hiragana]] !! transliteration !! [[kanji]] and [[hiragana]] !! pronunciation !! numbers !!\n|-\n| {{lang|ja|いろはにほへと}} || ''Iro ha nihoheto'' || {{lang|ja|色は匂へど}} || ''Iro wa nioedo'' || 1–7 ||Even the blossoming flowers [Colors are fragrant, but they]\n|-\n| {{lang|ja|ちりぬるを}} || ''Chirinuru wo'' || {{lang|ja|散りぬるを}} || ''Chirinuru o'' || 8–12 ||Will eventually scatter\n|-\n| {{lang|ja|わかよたれそ}} || ''Wa ka yo tare so'' || {{lang|ja|我が世誰ぞ}} || ''Wa ga yo dare zo'' <!-- agreed to keep 'ga' separate from 'wa' after consulting similar usage of 'wa' alone independent of 'ga', in Man'yôshû, the 8th-century collection of poetry --> || 13–18 ||Who in our world\n|-\n| {{lang|ja|つねならむ}} || ''Tsune naramu'' || {{lang|ja|常ならん}} || ''Tsune naran'' || 19–23 || Is unchanging?\n|-\n| {{lang|ja|うゐのおくやま}} || ''Uwi no okuyama'' || {{lang|ja|有為の奥山}} || ''Ui no okuyama'' || 24–30 || The deep mountains of karma—\n|-\n| {{lang|ja|けふこえて}} || ''Kefu koete'' || {{lang|ja|今日越えて}} || ''Kyō koete'' || 31–35 || We cross them today\n|-\n| {{lang|ja|あさきゆめみし}} || ''Asaki yume mishi'' || {{lang|ja|浅き夢見じ}} || ''Asaki yume miji'' || 36–42 || And we shall not have superficial dreams\n|-\n| {{lang|ja|ゑひもせす}} || ''Wehi mo sesu'' || {{lang|ja|酔ひもせず}} || ''Yoi mo sezu'' || 43–47 || Nor be deluded.\n|}\n\nNote that archaic hiragana uses {{lang|ja|ゐ}} and {{lang|ja|ゑ}}, which are now only used in proper names and certain [[Okinawan language |Okinawan]] [[Okinawan scripts|orthographies]]. Modern writing uses voiced consonant marks (with [[dakuten]]). This is used as an indicator of [[sound change]]s in the spoken Japanese language in the Heian era.\n\nAn English translation by Professor [[Ryuichi Abe]]<ref name= \"Abe_398\" /> reads as:\n\n{{quotation|<poem>\nAlthough its scent still lingers on\n  the form of a flower has scattered away\nFor whom will the glory\n  of this world remain unchanged?\nArriving today at the yonder side\n  of the deep mountains of evanescent existence\nWe shall never allow ourselves to drift away\n  intoxicated, in the world of shallow dreams.\n</poem>}}\n\nKomatsu Hideo has revealed that the last syllable of each line of the Man'yō-gana original ({{lang|ja|止加那久天之須}}), when put together, reveals a hidden sentence, ''toka [=toga] nakute shisu'' ({{lang |ja|咎無くて死す}}), which means \"die without wrong-doing\". It is thought that this might be eulogy in praise of [[Kūkai]], further supporting the notion that the Iroha was written after Kūkai's death.<ref name= \"Abe_398\">{{harvp |Abe|1999|p=398}}</ref>\n\n==Usage==\nThe iroha contains every kana only once, with the exception of {{lang|ja|ん}} (-''n''), which was not distinguished from {{lang|ja|む}} ''mu'' in writing until the early 20th century (see [[Japanese_script_reform#Pre-World_War_II_reforms|Japanese script reform]]).  For this reason, the poem was frequently used as an ordering of the kana until the [[Meiji era]] reforms in the 19th century. Around 1890, with the publication of the ''Wakun no Shiori'' ({{lang|ja|和訓栞}}) and ''Genkai'' ({{lang|ja|言海}}) dictionaries, the ''[[gojūon]]'' ({{lang|ja|五十音}}, literally \"fifty sounds\") ordering system, which is based on Sanskrit, became more common. It begins with ''a'', ''i'', ''u'', ''e'', ''o'' then ''ka'', ''ki'', ''ku''... and so on for each kana used in Japanese.  Although the earliest known copy of the ''gojūon'' predated the iroha, ''gojūon'' was considered too scholarly and had not been widely used.\n\nEven after widespread use of ''gojūon'' in education and dictionaries, the ''iroha'' sequence was commonly used as a system of showing order, just like ''a'', ''b'', ''c''... in English.\n\nFor example, [[Imperial Japanese Navy submarines]] during the [[World War II|Second World War]] had official designations beginning with ''I'' (displacement 1,000 tonnes or more), ''Ro'' (500 to 999 tonnes), and ''Ha'' (less than 500 tonnes).  Also, Japanese tanks had official designations partly using ''iroha'', such as ''[[Type 97 Chi-Ha#Japanese tank designations|Chi-ha]]'' (''ha'' meaning the third model). Other examples include subsection ordering in documents, seat numbering in theaters, and showing [[Go (board game)|go]] moves in diagrams ([[kifu]]).\n\n===Current uses===\nThe ''iroha'' sequence is still used today in many areas with long traditions.\n\nMost notably, [[:ja:法令の基本形式#号|Japanese laws and regulations]] officially use ''iroha'' for lower-level subsection ordering purposes, for example {{lang|ja|第四十九条第二項第一号ロ}} (Article 49, Section 2, Subsection 1-''ro''). In official translation to English, ''i'', ''ro'', ''ha''... are replaced by ''a'', ''b'', ''c''... as in ''49(2)(i)(b)''.\n\nIn music, the [[Musical note|note]]s of an [[octave]] are named ''i ro ha ni ho he to'', written in [[katakana]].\n{| class=\"wikitable\" style=\"margin: auto;\"\n|+ '''Musical notes'''\n|-\n! English || A || B || C || D || E || F || G\n|-\n! Japanese\n|{{lang|ja|イ}} (i)\n|{{lang|ja|ロ}} (ro)\n|{{lang|ja|ハ}} (ha)\n|{{lang|ja|ニ}} (ni)\n|{{lang|ja|ホ}} (ho)\n|{{lang|ja|ヘ}} (he)\n|{{lang|ja|ト}} (to)\n|}\n\n''Iroha'' is also used in numbering the classes of the conventional train cars of [[Japanese National Railways]] (now known as [[Japan Railways Group|JR]]). ''I'' is first class (no longer used), ''Ro'' is second class (now \"Green car\") and ''Ha'' is third class (standard carriages).\n\nSome Japanese expressions need knowledge of ''iroha'' to understand. The word ''iroha'' ({{lang|ja|イロハ}}, often in katakana) itself can mean \"the basics\" in Japanese, comparable to the term \"the ABCs\" in English. Similarly,  ''iroha no i'' ({{lang|ja|イロハのイ}}) means \"the most basic element of all\". ''I no ichiban'' ({{lang|ja|いの一番}}, \"number one of ''i''\") means \"the very first\".\n\n''[[karuta#Iroha Karuta|Iroha karuta]]'', a traditional card game, is still sold as an educational toy.\n\nIrohazaka ({{lang|ja|[[:ja:いろは坂|いろは坂]]}}), a one-way switchback mountain road at [[Nikkō, Tochigi]], is named for the poem because it has 48 corners. The route was popular with Buddhist pilgrims on their way to [[Lake Chūzenji]], which is at the top of the forested hill that this road climbs. While the narrow road has been modernized over the years, care has been taken to keep the number of curves constant.\n\n== Origin ==\n\nAuthorship is traditionally ascribed to the Heian era Japanese [[Buddhism|Buddhist]] priest and scholar [[Kūkai]] ({{lang|ja|空海}}) (774–835).  However, this is unlikely as it is believed that in his time there were separate ''e'' sounds in the ''a'' and ''ya'' columns of the kana table. The {{lang|ja|え}} (''e'') above would have been pronounced ''ye'', making the pangram incomplete.<ref>{{harvp|Abe|1999|p=392}}</ref>\n\nIt is said{{by whom|date=September 2012}} that the ''iroha'' is a transformation of these verses in the ''[[Nirvana Sutra]]'':\n\n<blockquote lang=\"ja\"><poem>\n諸行無常\n是生滅法\n生滅滅已\n寂滅為楽\n</poem></blockquote>\n\nwhich translates into\n\n{{quote|<poem>\nAll acts are [[impermanence|impermanent]]\nThat's the law of creation and destruction.\nWhen all creation and destruction are extinguished\nThat ultimate stillness ([[nirvana]]) is true bliss.\n</poem>}}\n\nThe above in Japanese is read\n\n<blockquote lang=\"ja-Latn\"><poem>\n''Shogyō mujō''\n''Zeshō meppō''\n''Shōmetsu metsui''\n''Jakumetsu iraku''\n</poem></blockquote>\n\n== See also ==\n\n* ''[[Ametsuchi No Uta]]'' (an earlier pangram)\n* [[Japanese literature]]\n\n=== Other languages ===\n* [[Alphabet song]]\n* ''[[Shiva Sutra]]'', Sanskrit poem with similar function\n* [[Javanese script#Collation|Hanacaraka]], the traditional arrangement of the letters of the Javanese alphabet\n* [[Thousand Character Classic]], Chinese poem with similar function, especially used in Korea\n\n==Notes==\n{{reflist}}\n\n===References===\n{{refbegin}}\n*{{cite book | last = Abe | first = Ryuichi | title = The Weaving of Mantra: Kûkai and the Construction of Esoteric Buddhist Discourse | publisher = Columbia University Press | year = 1999 | isbn = 0-231-11286-6 |ref=harv}}\n{{refend}}\n\n{{Japanese poetry}}\n\n[[Category:Japanese poems]]\n[[Category:Japanese writing system]]\n[[Category:Collation]]\n[[Category:Articles containing Japanese poems]]\n[[Category:Constrained writing]]\n[[Category:Pangrams]]"
    },
    {
      "title": "ISO 14651",
      "url": "https://en.wikipedia.org/wiki/ISO_14651",
      "text": "'''[http://standards.iso.org/ittf/PubliclyAvailableStandards/c068309_ISO_IEC_14651_2016.zip ISO/IEC 14651:2016]''', ''Information technology -- International string ordering and comparison -- Method for comparing character strings and description of the common template tailorable ordering'', is an [[International Organization for Standardization|ISO]] Standard specifying an [[algorithm]] that can be used when comparing two [[String (computer science)|strings]]. This comparison can be used when collating a set of strings. The standard also specifies a datafile specifying the comparison order, the ''Common Tailorable Template'', CTT. The comparison order is supposed to be tailored for different languages (hence the CTT is regarded as a ''template'' and not a default, though the empty tailoring, not changing any weighting, is appropriate in many cases), since different languages have incompatible ordering requirements. One such tailoring is [[European ordering rules]] (EOR), which in turn is supposed to be tailored for different European languages.\n\nThe ''Common Tailorable Template'' (''CTT'') datafile of this ISO Standard is aligned with the ''Default Unicode Collation Entity Table'' (DUCET) datafile of the '''[[Unicode Collation Algorithm]]''' (UCA) specified in ''Unicode Technical Standard #10''.\n\nThis is the fourth edition of the standard and was published on 2016-02-15, corrected on 2016-05-01 and covers up to and including Unicode 8.0. One additional amendment '''[http://standards.iso.org/ittf/PubliclyAvailableStandards/c070338_ISO_IEC_14651_2016_Amd_1_2017.zip Amd.1:2017]''' was published in September 2017 and covers up to and including Unicode 9.0.\n\n==See also==\n*[[Collation]]\n*[[European ordering rules]]\n*[[ISO/IEC JTC 1/SC 2]]\n*[[Unicode]]\n\n==External links and references==\n*[https://www.iso.org/standard/68309.html ISO site, \"ISO/IEC 14651:2016\"]. ISO/IEC 14651:2016 and Amd.1:2017 are [http://standards.iso.org/ittf/PubliclyAvailableStandards/index.html freely available from the ISO website] \n*[http://unicode.org/faq/collation.html#13 \"What are the differences between the UCA and ISO 14651?\"]\n\n{{Unicode navigation}}\n{{ISO standards}}\n\n[[Category:String collation algorithms]]\n[[Category:ISO standards|#14651]]\n[[Category:Unicode algorithms]]\n[[Category:Collation]]\n\n\n{{compu-stub}}\n{{standard-stub}}"
    },
    {
      "title": "Kangxi radical",
      "url": "https://en.wikipedia.org/wiki/Kangxi_radical",
      "text": "{{Use dmy dates|date=April 2012}}\n[[File:List of the 214 Kangxi Radicals - old style.svg|right|thumb|400px|List of Kangxi radicals in a font imitating the original character shapes of the [[Kangxi Dictionary]].]]\nThe 214 '''Kangxi radicals''' ({{zh|c=康熙部首|p=Kāngxī bùshǒu}}) form a system of [[radical (Chinese character)|radicals]] ({{lang|zh|{{linktext|部首}}}}) of [[Chinese characters]]. \nThe radicals are numbered in [[stroke (CJKV character)|stroke]] count order. They are the de facto standard used as the basis for most modern [[Chinese dictionaries]], such that reference to \"[[radical 61]]\", for example, without additional context, refers to the 61st radical of the ''Kangxi Dictionary'',  [[:wikt:心|心]]; [[pinyin|''xīn'']] \"heart\".\n\nOriginally introduced in the 1615 ''[[Zihui]]'', they are named in relation to the ''[[Kangxi Dictionary]]'' of 1716 (''[[Kangxi Emperor|Kāngxī]]'' {{lang|zh|{{linktext|康|熙}}}} being the [[Chinese era name|era name]] for 1662–1723). The system of 214 Kangxi radicals is based on the older system of [[List of Shuowen Jiezi radicals|540 radicals]] used in the Han-era ''[[Shuowen Jiezi]]''.\n\n==Statistics==\n{{Unreferenced section|date=October 2016}}\n[[File:Radicals frequency table.png|thumb|300px|Distribution of the number of entries per radical in the ''Kangxi Dictionary'']]\nThe Kangxi dictionary lists a total of 47,035 characters divided among the 214 radicals, for an average of 220 characters per radical, but distribution is unequal, the [[median]] number of characters per radical being 64, with a maximum number of 1,902 characters (for [[radical 140]]  {{lang|zh-Hant|[[:wikt:艸|艸]]}}) and a minimum number of five ([[radical 138]]  {{lang|zh-Hant|[[:wikt:艮|艮]]}}). The radicals have between one and seventeen strokes, the median number of strokes being 5 while the average number of strokes is slightly below 5.7.<!--average 1219/214=5.696-->\n\nThe ten radicals with the largest number of characters account for 10,665 characters (or 23% of the dictionary). The same ten radicals account for 7,141 out of 20,992 characters (34%) in the Unicode [[CJK Unified Ideographs]] block as introduced in 1992, as follows:\n\n{|class=\"wikitable sortable\"\n|-\n!class=\"unsortable\"|Radical \n!Kangxi Dictionary \n! CJK Unified Ideographs\n|-\n|[[Radical 140]] {{lang|zh-Hant|[[:wikt:艸|艸]]}} \"grass\"|| data-sort-value=\"1902\"| 1,902 characters || 981 characters<br/>(U+8278&ndash;864C)\n|-\n|[[Radical 85]] {{lang|zh-Hant|[[:wikt:水|水]]}} \"water\"|| data-sort-value=\"1595\"|1,595 characters || data-sort-value=\"1079\"|1,079 characters<br/>(U+6C34&ndash;706A)\n|-\n|[[Radical 75]] {{lang|zh-Hant|[[:wikt:木|木]]}} \"tree\"||  data-sort-value=\"1369\"| 1,369 characters || data-sort-value=\"1016\"|1,016 characters<br/>(U+6728&ndash;6B1F)\n|-\n|[[Radical 64]] {{lang|zh-Hant|[[:wikt:手|手]]}} \"hand\"|| data-sort-value=\"1203\"| 1,203 characters || 740 characters<br/>(U+624B&ndash;652E)\n|-\n|[[Radical 30]] {{lang|zh-Hant|[[:wikt:口|口]]}} \"mouth\"|| data-sort-value=\"1146\"| 1,146 characters || 756 characters<br/>(U+53E3&ndash;56D6)\n|-\n|[[Radical 61]] {{lang|zh-Hant|[[:wikt:心|心]]}} \"heart\"|| data-sort-value=\"1115\"| 1,115 characters || 581 characters<br/>(U+5FC3&ndash;6207)\n|-\n|[[Radical 142]] {{lang|zh-Hant|[[:wikt:虫|虫]]}} \"insect\"|| data-sort-value=\"1067\"| 1,067 characters || 469 characters<br/>(U+866B&ndash;883F)\n|-\n|[[Radical 118]] {{lang|zh-Hant|[[:wikt:竹|竹]]}} \"bamboo\"||  953 characters || 378 characters<br/>(U+7AF9&ndash;7C72)\n|-\n|[[Radical 149]] {{lang|zh-Hant|[[:wikt:言|言]]}} \"speech\"||  861 characters || 567 characters<br/>(U+8A00&ndash;8C36)\n|-\n|[[Radical 120]] {{lang|zh-Hant|[[:wikt:糸|糸]]}} \"silk\"|| 823 characters || 574 characters<br/>(U+7CF8&ndash;7F35)\n|-\n|}\n\n==Modern dictionaries==\nModern Chinese dictionaries continue to use the Kangxi radical-stroke order, both in  traditional ''zìdiǎn'' ({{linktext|字典}}, lit. \"character/logograph dictionary\") for written [[Chinese character]]s and modern ''cídiǎn'' ({{linktext|詞典}} \"word/phrase dictionary\") for spoken expressions. The  214 Kangxi radicals act as a de facto standard, which may not be duplicated exactly in every Chinese dictionary, but which few dictionary compilers can afford to completely ignore.  They also serve as the basis for many computer encoding systems, including [[Unihan]]. The number of radicals may be reduced in modern practical dictionaries, as some of the more obscure Kangxi radicals do not form any characters that remain in frequent use. Thus, the  ''Oxford Concise English–Chinese Dictionary'' ({{ISBN|0-19-596457-8}}), for example, has 188 radicals. The ''[[Xinhua Zidian]]'',  a pocket-sized character dictionary  containing about 13,000 characters, uses 189 radicals. A few dictionaries also introduce new radicals, treating groups of radicals that are used together in many different characters as a kind of radical. For example, ''[[Hanyu Da Cidian]]'', the most inclusive available [[Chinese dictionary]] (published in 1993) has 23,000 head character entries organised by a novel system of 200 radicals.\n\n==Table of radicals==\n{| class=\"wikitable sortable collapsible\"\n!No.\n!class=\"unsortable\"|Radical (variants)\n!Stroke count\n! class=\"unsortable\" |Meaning\n![[Pīnyīn]]\n![[Sino-Vietnamese vocabulary|Sino-<br>Vietnamese]]<br>(Hán-Việt)\n![[Hiragana]]-[[Romanization of Japanese|Romaji]]\n![[Hangeul]]-[[Romanization of Korean|Romaja]]\n!Frequency  \n!class=\"unsortable\"|[[Simplified Chinese characters|Simplified]]\n!class=\"unsortable\"|Examples\n|----\n|style=\"text-align:right\"|[[Radical 1|1]]\n|'''<big>{{lang|zh-Hant|一}}</big>'''\n|1\n|one\n|yī\n|nhất\n|{{lang|ja|いち}} / ichi\n|{{lang|ko|한일}} / hanil\n|42\n|\n|{{lang|zh-Hant|王、丁、七、三}}\n|----\n|style=\"text-align:right\"|[[Radical 2|2]]\n|'''<big>{{lang|zh-Hant|丨}}</big>'''\n|1\n|line\n|gǔn\n|cổn\n|{{lang|ja|ぼう}} / bō\n|{{lang|ko|뚫을곤}} / ddulheulgon\n|21\n|\n|{{lang|zh-Hant|十、中、串、丰}}\n|----\n|style=\"text-align:right\"|[[Radical 3|3]]\n|'''<big>{{lang|zh-Hant|丶}}</big>'''\n|1\n|dot\n|zhǔ\n|chủ\n|{{lang|ja|てん}} / ten\n|{{lang|ko|점주}} / jeomju\n|10\n|\n|{{lang|zh-Hant|丸、凡、丹、户}}\n|----\n|style=\"text-align:right\"|[[Radical 4|4]]\n|'''<big>{{lang|zh-Hant|丿}}<br/>({{lang|zh|乚、乛}})</big>'''\n|1\n|slash\n|piě\n|phiệt\n|{{lang|ja|の}} / no\n|{{lang|ko|삐침별}} / bbichimbyeol\n|33\n|\n|{{lang|zh-Hant|乂、乃、久、八}}\n|----\n|style=\"text-align:right\"|[[Radical 5|5]]\n|'''<big>{{lang|zh-Hant|乙}}<br/>({{lang|zh|乀}})</big>'''\n|1\n|second\n|yǐ\n|ất\n|{{lang|ja|おつ}} / otsu\n|{{lang|ko|새을}} / saeeul\n|42\n|\n|{{lang|zh-Hant|九、乞、也}}\n|----\n|style=\"text-align:right\"|[[Radical 6|6]]\n|'''<big>{{lang|zh-Hant|亅}}</big>'''\n|1\n|hook\n|jué\n|quyết\n|{{lang|ja|はねぼう}} / hanebō\n|{{lang|ko|갈고리궐}} / galgorigweol\n|19\n|\n|{{lang|zh-Hant|了、矛、事}}\n|----\n|style=\"text-align:right\"|[[Radical 7|7]]\n|'''<big>{{lang|zh-Hant|二}}</big>'''\n|2\n|two\n|èr\n|nhị\n|{{lang|ja|ふた}} / futa\n|{{lang|ko|두이}} / dui\n|29\n|\n|{{lang|zh-Hant|貳、于、云、些}}\n|----\n|style=\"text-align:right\"|[[Radical 8|8]]\n|'''<big>{{lang|zh-Hant|亠}}</big>'''\n|2\n|lid\n|tóu\n|đầu\n|{{lang|ja|なべぶた}} / nabebuta\n|{{lang|ko|돼지해머리}} / dwaejihaemeori\n|38\n|\n|{{lang|zh-Hant|方、亡、亢、交}}\n|----\n|style=\"text-align:right\"|[[Radical 9|9]]\n|'''<big>{{lang|zh-Hant|人}}<br/>({{lang|zh|亻}})</big>'''\n|2\n|man\n|rén\n|nhân (đứng, nón)\n|{{lang|ja|ひと}} / hito\n|{{lang|ko|사람인 (변)}} / saramin (byeon)\n|794\n|\n|{{lang|zh-Hant|你、什、仁、仇}}\n|----\n|style=\"text-align:right\"|[[Radical 10|10]]\n|'''<big>{{lang|zh-Hant|儿}}</big>'''\n|2\n|son, legs\n|ér\n|nhi/nhân (đi)\n|{{lang|ja|にんにょう}} / ninnyō\n|{{lang|ko|어진사람인발}} / eojinsaraminbal\n|52\n||(pr. {{lang|zh|兒}})\n|{{lang|zh-Hant|兒、兀、允、元}}\n|----\n|style=\"text-align:right\"|[[Radical 11|11]]\n|'''<big>{{lang|zh-Hant|入}}</big>'''\n|2\n|enter\n|rù\n|nhập\n|{{lang|ja|いる}} / iru\n|{{lang|ko|들입}} / deurip\n|28\n|\n|{{lang|zh-Hant|內、全、兩、汆}}\n|----\n|style=\"text-align:right\"|[[Radical 12|12]]\n|'''<big>{{lang|zh-Hant|八}}<br/>({{lang|zh|丷}})</big>'''\n|2\n|eight\n|bā\n|bát\n|{{lang|ja|はちがしら}} / hachigashira\n|{{lang|ko|여덟팔}} / yeodeolbpal\n|44\n|\n|{{lang|zh-Hant|公、六、兮、穴}}\n|----\n|style=\"text-align:right\"|[[Radical 13|13]]\n|'''<big>{{lang|zh-Hant|冂}}</big>'''\n|2\n|wide\n|jiōng\n|quynh<br>khuynh\n|{{lang|ja|まきがまえ}} / makigamae\n|{{lang|ko|멀경몸}} / meolgyeongmom\n|50\n|\n|{{lang|zh-Hant|冇、冊、冉、肉}}\n|----\n|style=\"text-align:right\"|[[Radical 14|14]]\n|'''<big>{{lang|zh-Hant|冖}}</big>'''\n|2\n|cloth cover\n|mì\n|mịch\n|{{lang|ja|わかんむり}} / wakammuri\n|{{lang|ko|민갓머리}} / mingatmeori\n|30\n|\n|{{lang|zh-Hant|冗、冠、冥、運}}\n|----\n|style=\"text-align:right\"|[[Radical 15|15]]\n|'''<big>{{lang|zh-Hant|冫}}</big>'''\n|2\n|ice\n|bīng\n|băng\n|{{lang|ja|にすい}} / nisui\n|{{lang|ko|이수변}} / isubyeon\n|115\n|\n|{{lang|zh-Hant|冬、冰、冶、凉}}\n|----\n|style=\"text-align:right\"|[[Radical 16|16]]\n|'''<big>{{lang|zh-Hant|几}}</big>'''\n|2\n|table\n|jī\n|kỷ\n|{{lang|ja|つくえ}} / tsukue\n|{{lang|ko|안석궤}} / anseokgwe\n|38\n|\n|{{lang|zh-Hant|風、殳、凡、凱}}\n|----\n|style=\"text-align:right\"|[[Radical 17|17]]\n|'''<big>{{lang|zh-Hant|凵}}</big>'''\n|2\n|receptacle\n|kǎn\n|khảm\n|{{lang|ja|うけばこ}} / ukebako\n|{{lang|ko|위튼입구몸}} / witeunipgumom\n|23\n|\n|{{lang|zh-Hant|凶、凸、凹、齒}}\n|----\n|style=\"text-align:right\"|[[Radical 18|18]]\n|'''<big>{{lang|zh-Hant|刀}}<br/>({{lang|zh|刂、⺈}})</big>'''\n|2\n|knife\n|dāo\n|đao\n|{{lang|ja|かたな}} / katana\n|{{lang|ko|(선) 칼도 (방)}} / (seon) kaldo (bang)\n|377\n|\n|{{lang|zh-Hant|刁、解、分、劍}}\n|----\n|style=\"text-align:right\"|[[Radical 19|19]]\n|'''<big>{{lang|zh-Hant|力}}</big>'''\n|2\n|power\n|lì\n|lực\n|{{lang|ja|ちから}} / chikara\n|{{lang|ko|힘력}} / himryeok\n|163\n|\n|{{lang|zh-Hant|加、功、劣、男}}\n|----\n|style=\"text-align:right\"|[[Radical 20|20]]\n|'''<big>{{lang|zh-Hant|勹}}</big>'''\n|2\n|wrap\n|bāo\n|bao\n|{{lang|ja|つつみがまえ}} / tsutsumigamae\n|{{lang|ko|쌀포몸}} / ssalpomom\n|64\n|\n|{{lang|zh-Hant|包、勺、勻、勾}}\n|----\n|style=\"text-align:right\"|[[Radical 21|21]]\n|'''<big>{{lang|zh-Hant|匕}}</big>'''\n|2\n|spoon\n|bǐ\n|chủy\n|{{lang|ja|さじのひ}} / sajinohi\n|{{lang|ko|비수비}} / bisubi\n|19\n|\n|{{lang|zh-Hant|化、北、匙、比}}\n|----\n|style=\"text-align:right\"|[[Radical 22|22]]\n|'''<big>{{lang|zh-Hant|匚}}</big>'''\n|2\n|box\n|fāng\n|phương\n|{{lang|ja|はこがまえ}} / hakogamae\n|{{lang|ko|튼입구몸}} / teunipgumom\n|64\n|\n|{{lang|zh-Hant|匿、區、巨、匠}}\n|----\n|style=\"text-align:right\"|[[Radical 23|23]]\n|'''<big>{{lang|zh-Hant|匸}}</big>'''\n|2\n|hiding enclosure\n|xǐ/xì\n|hệ\n|{{lang|ja|かくしがまえ}} / kakushigamae\n|{{lang|ko|감출혜몸}} / gamchulhyemom\n|17\n|\n|{{lang|zh-Hant|亡、匹、匼、匽}}\n|----\n|style=\"text-align:right\"|[[Radical 24|24]]\n|'''<big>{{lang|zh-Hant|十}}</big>'''\n|2\n|ten\n|shí\n|thập\n|{{lang|ja|じゅう}} / jū\n|{{lang|ko|열십}} / yeolsip\n|55\n|\n|{{lang|zh-Hant|千、木、卅、升}}\n|----\n|style=\"text-align:right\"|[[Radical 25|25]]\n|'''<big>{{lang|zh-Hant|卜}}</big>'''\n|2\n|divination\n|bǔ\n|bốc\n|{{lang|ja|ぼくのと}} / bokunoto\n|{{lang|ko|점복}} / jeombok\n|45\n|\n|{{lang|zh-Hant|下、上、卡、占}}\n|----\n|style=\"text-align:right\"|[[Radical 26|26]]\n|'''<big>{{lang|zh-Hant|卩}}<br/>({{lang|zh|㔾}})</big>'''\n|2\n|seal (device)\n|jié\n|tiết\n|{{lang|ja|ふしづくり}} / fushizukuri\n|{{lang|ko|병부절}} / byeongbujeol\n|40\n|\n|{{lang|zh-Hant|即、卬、卷、夗}}\n|----\n|style=\"text-align:right\"|[[Radical 27|27]]\n|'''<big>{{lang|zh-Hant|厂}}</big>'''\n|2\n|cliff\n|hǎn<ref>http://stroke-order.learningweb.moe.edu.tw/advExplain2.do?big5=ADEC</ref>\n|hán\n|{{lang|ja|がんだれ}} / gandare\n|{{lang|ko|민엄호}} / mineomho\n|129\n|(pr. {{lang|zh|廠}})\n|{{lang|zh-Hant|厄、原、厚、厰}}\n|----\n|style=\"text-align:right\"|[[Radical 28|28]]\n|'''<big>{{lang|zh-Hant|厶}}</big>'''\n|2\n|private\n|sī\n|khư\n|{{lang|ja|む}} / mu\n|{{lang|ko|마늘모}} / maneulmo\n|40\n|\n|{{lang|zh-Hant|台、公、去、參、}}\n|----\n|style=\"text-align:right\"|[[Radical 29|29]]\n|'''<big>{{lang|zh-Hant|又}}</big>'''\n|2\n|again\n|yòu\n|hựu\n|{{lang|ja|また}} / mata\n|{{lang|ko|또우}} / ddou\n|91\n|\n|{{lang|zh-Hant|叉、及、友、取}}\n|----\n|style=\"text-align:right\"|[[Radical 30|30]]\n|'''<big>{{lang|zh-Hant|口}}</big>'''\n|3\n|mouth\n|kǒu\n|khẩu\n|{{lang|ja|くち}} / kuchi\n|{{lang|ko|입구}} / ipgu\n|1,146\n|\n|{{lang|zh-Hant|哎、古、只、品}}\n|----\n|style=\"text-align:right\"|[[Radical 31|31]]\n|'''<big>{{lang|zh-Hant|囗}}</big>'''\n|3\n|enclosure\n|wéi\n|vi\n|{{lang|ja|くにがまえ}} / kunigamae\n|{{lang|ko|큰입구 (몸)}} / keunipgu (mom)\n|118\n|\n|{{lang|zh-Hant|囚、四、國、圍}}\n|----\n|style=\"text-align:right\"|[[Radical 32|32]]\n|'''<big>{{lang|zh-Hant|土}}</big>'''\n|3\n|earth\n|tǔ\n|thổ\n|{{lang|ja|つち}} / tsuchi\n|{{lang|ko|흙토}} / heulkto\n|580\n|\n|{{lang|zh-Hant|地、圣、圭、堯}}\n|----\n|style=\"text-align:right\"|[[Radical 33|33]]\n|'''<big>{{lang|zh-Hant|士}}</big>'''\n|3\n|scholar\n|shì\n|sĩ\n|{{lang|ja|さむらい}} / samurai\n|{{lang|ko|선비사}} / seonbisa\n|24\n|\n|{{lang|zh-Hant|壬、喜、時、壽}}\n|----\n|style=\"text-align:right\"|[[Radical 34|34]]\n|'''<big>{{lang|zh-Hant|夂}}</big>'''\n|3\n|go\n|zhǐ\n|truy/trĩ\n|{{lang|ja|ふゆがしら}} / fuyugashira\n|{{lang|ko|뒤져올치}} / dwijyeoolchi\n|11\n|\n|{{lang|zh-Hant|各、冬、夆、愛}}\n|----\n|style=\"text-align:right\"|[[Radical 35|35]]\n|'''<big>{{lang|zh-Hant|夊}}</big>'''\n|3\n|go slowly\n|suī\n|truy/tuy\n|{{lang|ja|すいにょう}} / suinyō\n|{{lang|ko|천천히걸을쇠발}} / cheoncheonhigeoreulsoebal\n|23\n|\n|{{lang|zh-Hant|夌、复、夏}}\n|----\n|style=\"text-align:right\"|[[Radical 36|36]]\n|'''<big>{{lang|zh-Hant|夕}}</big>'''\n|3\n|evening\n|xī\n|tịch\n|{{lang|ja|ゆうべ}} / yūbe\n|{{lang|ko|저녁석}} / jeonyeokseok\n|34\n|\n|{{lang|zh-Hant|外、夗、多、岁}}\n|----\n|style=\"text-align:right\"|[[Radical 37|37]]\n|'''<big>{{lang|zh-Hant|大}}</big>'''\n|3\n|big\n|dà\n|đại\n|{{lang|ja|だい}} / dai\n|{{lang|ko|큰대}} / keundae\n|132\n|\n|{{lang|zh-Hant|太、天、夭、夯}}\n|----\n|style=\"text-align:right\"|[[Radical 38|38]]\n|'''<big>{{lang|zh-Hant|女}}</big>'''\n|3\n|woman\n|nǚ\n|nữ\n|{{lang|ja|おんな}} / onna\n|{{lang|ko|계집녀}} / gyejipnyeo\n|681\n|\n|{{lang|zh-Hant|妳、婆、奶、如}}\n|----\n|style=\"text-align:right\"|[[Radical 39|39]]\n|'''<big>{{lang|zh-Hant|子}}</big>'''\n|3\n|child\n|zǐ\n|tử\n|{{lang|ja|こ}} / ko\n|{{lang|ko|아들자}} / adeulja\n|83\n|\n|{{lang|zh-Hant|孩、學、孔、仔}}\n|----\n|style=\"text-align:right\"|[[Radical 40|40]]\n|'''<big>{{lang|zh-Hant|宀}}</big>'''\n|3\n|roof\n|mián\n|miên\n|{{lang|ja|うかんむり}} / ukammuri\n|{{lang|ko|갓머리}} / gatmeori\n|246\n|\n|{{lang|zh-Hant|家、安、寧、它}}\n|----\n|style=\"text-align:right\"|[[Radical 41|41]]\n|'''<big>{{lang|zh-Hant|寸}}</big>'''\n|3\n|inch\n|cùn\n|thốn\n|{{lang|ja|すん}} / sun\n|{{lang|ko|마디촌}} / madichon\n|40\n|\n|{{lang|zh-Hant|村、寺、封、射}}\n|----\n|style=\"text-align:right\"|[[Radical 42|42]]\n|'''<big>{{lang|zh-Hant|小}}<br/>({{lang|zh|⺌、⺍}})</big>'''\n|3\n|small\n|xiǎo\n|tiểu\n|{{lang|ja|しょう}} / shō\n|{{lang|ko|작을소}} / jageulso\n|41\n|\n|{{lang|zh-Hant|少、尖、當、栄}}\n|----\n|style=\"text-align:right\"|[[Radical 43|43]]\n|'''<big>{{lang|zh-Hant|尢}}<br/>({{lang|zh|尣}})</big>'''\n|3\n|lame\n|wāng\n|uông\n|{{lang|ja|まげあし}} / mageashi\n|{{lang|ko|절름발이왕}} / jeolleumbariwang\n|66\n|\n|{{lang|zh-Hant|尤、尷、尬、尩}}\n|----\n|style=\"text-align:right\"|[[Radical 44|44]]\n|'''<big>{{lang|zh-Hant|尸}}</big>'''\n|3\n|corpse\n|shī\n|thi\n|{{lang|ja|しかばね}} / shikabane\n|{{lang|ko|주검시엄}} / jugeomsieom\n|148\n|(pr. {{lang|zh|屍}})\n|{{lang|zh-Hant|屍、尺、尼、尻}}\n|----\n|style=\"text-align:right\"|[[Radical 45|45]]\n|'''<big>{{lang|zh-Hant|屮}}</big>'''\n|3\n|sprout\n|chè\n|triệt\n|{{lang|ja|てつ}} / tetsu\n|{{lang|ko|왼손좌}} / oensonjwa\n|38\n|\n|{{lang|zh-Hant|艸、屯、屰、頓}}\n|----\n|style=\"text-align:right\"|[[Radical 46|46]]\n|'''<big>{{lang|zh-Hant|山}}</big>'''\n|3\n|mountain\n|shān\n|sơn\n|{{lang|ja|やま}} / yama\n|{{lang|ko|뫼산}} / moesan\n|636\n|\n|{{lang|zh-Hant|嵗、密、峰、幽}}\n|----\n|style=\"text-align:right\"|[[Radical 47|47]]\n|'''<big>{{lang|zh-Hant|巛}}<br/>({{lang|zh|川}})</big>'''\n|3\n|river\n|chuān\n|xuyên\n|{{lang|ja|まがりがわ}} / magarigawa\n|{{lang|ko|개미허리 (내천)}} / gaemiheori (naecheon)\n|26\n|\n|{{lang|zh-Hant|川、州、順、災}}\n|----\n|style=\"text-align:right\"|[[Radical 48|48]]\n|'''<big>{{lang|zh-Hant|工}}</big>'''\n|3\n|work\n|gōng\n|công\n|{{lang|ja|たくみ}} / takumi\n|{{lang|ko|장인공}} / jangingong\n|17\n|\n|{{lang|zh-Hant|左、巧、功、式}}\n|----\n|style=\"text-align:right\"|[[Radical 49|49]]\n|'''<big>{{lang|zh-Hant|己}}</big>'''\n|3\n|oneself\n|jǐ\n|kỷ\n|{{lang|ja|おのれ}} / onore\n|{{lang|ko|몸기}} / momgi\n|20\n|\n|{{lang|zh-Hant|記、改、忌、龍}}\n|----\n|style=\"text-align:right\"|[[Radical 50|50]]\n|'''<big>{{lang|zh-Hant|巾}}</big>'''\n|3\n|turban\n|jīn\n|cân\n|{{lang|ja|はば}} / haba\n|{{lang|ko|수건건}} / sugeongeon\n|295\n|\n|{{lang|zh-Hant|市、帥、刷、砸}}\n|----\n|style=\"text-align:right\"|[[Radical 51|51]]\n|'''<big>{{lang|zh-Hant|干}}</big>'''\n|3\n|dry\n|gān\n|can\n|{{lang|ja|ほす}} / hosu\n|{{lang|ko|방패간}} / bangpaegan\n|9\n||(pr. {{lang|zh|乾、幹}})\n|{{lang|zh-Hant|旱、开、平、年}}\n|----\n|style=\"text-align:right\"|[[Radical 52|52]]\n|'''<big>{{lang|zh-Hant|幺}}<br/>({{lang|zh|么}})</big>'''\n|3\n|short thread\n|yāo\n|yêu\n|{{lang|ja|いとがしら}} / itogashira\n|{{lang|ko|작을요}} / jageulyo\n|50\n|\n|{{lang|zh-Hant|幻、麼、幽、幾}}\n|----\n|style=\"text-align:right\"|[[Radical 53|53]]\n|'''<big>{{lang|zh-Hant|广}}</big>'''\n|3\n|dotted cliff\n|guǎng\n|nghiễm\n|{{lang|ja|まだれ}} / madare\n|{{lang|ko|엄호}} / eomho\n|15\n||(pr. {{lang|zh|廣}})\n|{{lang|zh-Hant|庀、庂、庄}}\n|----\n|style=\"text-align:right\"|[[Radical 54|54]]\n|'''<big>{{lang|zh-Hant|廴}}</big>'''\n|3\n|long stride\n|yǐn\n|dẫn\n|{{lang|ja|えんにょう}} / ennyō\n|{{lang|ko|민책받침}} / minchaekbatchim\n|9\n|\n|{{lang|zh-Hant|廵、廷、延}}\n|----\n|style=\"text-align:right\"|[[Radical 55|55]]\n|'''<big>{{lang|zh-Hant|廾}}</big>'''\n|3\n|arch\n|gǒng\n|củng\n|{{lang|ja|にじゅうあし}} / nijūashi\n|{{lang|ko|스물입발}} / sumeuripbal\n|50\n|\n|{{lang|zh-Hant|廿、弁、弄}}\n|----\n|style=\"text-align:right\"|[[Radical 56|56]]\n|'''<big>{{lang|zh-Hant|弋}}</big>'''\n|3\n|shoot\n|yì\n|dặc\n|{{lang|ja|しきがまえ}} / shikigamae\n|{{lang|ko|주살익}} / jusarik\n|15\n|\n|{{lang|zh-Hant|弌、弍、弎}}\n|----\n|style=\"text-align:right\"|[[Radical 57|57]]\n|'''<big>{{lang|zh-Hant|弓}}</big>'''\n|3\n|bow\n|gōng\n|cung\n|{{lang|ja|ゆみ}} / yumi\n|{{lang|ko|활궁}} / hwalgung\n|165\n|\n|{{lang|zh-Hant|弔、引、弗}}\n|----\n|style=\"text-align:right\"|[[Radical 58|58]]\n|'''<big>{{lang|zh-Hant|彐}}<br/>({{lang|zh|彑}})</big>'''\n|3\n|snout\n|jì\n|ký/kệ\n|{{lang|ja|けいがしら}} / keigashira\n|{{lang|ko|튼가로왈}} / teungarowal\n|25\n|\n|{{lang|zh-Hant|彔、彖、彗}}\n|----\n|style=\"text-align:right\"|[[Radical 59|59]]\n|'''<big>{{lang|zh-Hant|彡}}</big>'''\n|3\n|bristle\n|shān\n|sam\n|{{lang|ja|さんづくり}} / sandzukuri\n|{{lang|ko|터럭삼}} / teoreoksam\n|62\n|\n|{{lang|zh-Hant|形、彤、彥}}\n|----\n|style=\"text-align:right\"|[[Radical 60|60]]\n|'''<big>{{lang|zh-Hant|彳}}</big>'''\n|3\n|step\n|chì\n|sách\n|{{lang|ja|ぎょうにんべん}} / gyōnimben\n|{{lang|ko|두인변}} / duinbyeon\n|215\n|\n|{{lang|zh-Hant|㣔、彴、彷}}\n|----\n|style=\"text-align:right\"|[[Radical 61|61]]\n|'''<big>{{lang|zh-Hant|心}}<br/>({{lang|zh|忄、⺗}})</big>'''\n|4\n|heart\n|xīn\n|tâm\n|{{lang|ja|りっしんべん}} / risshimben\n|{{lang|ko|마음심 (심방변 / 마음심밑)}} / maeumsim (simbangbyeon / maeumsimmit)\n|1,115\n|\n|{{lang|zh-Hant|必、忉、忌}}\n|----\n|style=\"text-align:right\"|[[Radical 62|62]]\n|'''<big>{{lang|zh-Hant|戈}}</big>'''\n|4\n|halberd\n|gē\n|qua\n|{{lang|ja|かのほこ}} / kanohoko\n|{{lang|ko|창과}} / changgwa\n|116\n|\n|{{lang|zh-Hant|戊、戉、戌}}\n|----\n|style=\"text-align:right\"|[[Radical 63|63]]\n|'''<big>{{lang|zh-Hant|戶}}<br/>({{lang|zh|户、戸}})</big>'''\n|4\n|door\n|hù\n|hộ\n|{{lang|ja|と}} / to\n|{{lang|ko|지게호}} / jigeho\n|44\n|{{lang|zh-Hans|户}}\n|{{lang|zh-Hant|戹、戼、戽}}\n|----\n|style=\"text-align:right\"|[[Radical 64|64]]\n|'''<big>{{lang|zh-Hant|手}}<br/>({{lang|zh|扌、龵}})</big>'''\n|4\n|hand\n|shǒu\n|thủ\n|{{lang|ja|て}} / te\n|{{lang|ko|손수 (재방변)}} / sonsu (jaebangbyeon)\n|1,203\n|\n|{{lang|zh-Hant|才、扎、扐}}\n|----\n|style=\"text-align:right\"|[[Radical 65|65]]\n|'''<big>{{lang|zh-Hant|支}}</big>'''\n|4\n|branch\n|zhī\n|chi\n|{{lang|ja|しにょう}} / shinyō\n|{{lang|ko|지탱할지}} / jitaenghalji\n|26\n|\n|{{lang|zh-Hant|攰、攱、攲}}\n|----\n|style=\"text-align:right\"|[[Radical 66|66]]\n|'''<big>{{lang|zh-Hant|攴}}<br/>({{linktext|lang=zh|攵}})</big>'''\n|4\n|rap, tap\n|pū\n|phộc\n|{{lang|ja|ぼくづくり}} / bokuzukuri\n|{{lang|ko|칠복 (등글월문)}} / chilbok (deunggeulweolmun)\n|296\n|\n|{{lang|zh-Hant|收、攷、攸}}\n|----\n|style=\"text-align:right\"|[[Radical 67|67]]\n|'''<big>{{lang|zh-Hant|文}}</big>'''\n|4\n|script\n|wén\n|văn\n|{{lang|ja|ぶん}} / bun\n|{{lang|ko|글월문}} / geulweolmun\n|26\n|\n|{{lang|zh-Hant|斉、斌、斐}}\n|----\n|style=\"text-align:right\"|[[Radical 68|68]]\n|'''<big>{{lang|zh-Hant|斗}}</big>'''\n|4\n|dipper\n|dǒu\n|đẩu\n|{{lang|ja|とます}} / tomasu\n|{{lang|ko|말두}} / maldu\n|32\n|\n|{{lang|zh-Hant|料、斚、斛}}\n|----\n|style=\"text-align:right\"|[[Radical 69|69]]\n|'''<big>{{lang|zh-Hant|斤}}</big>'''\n|4\n|axe\n|jīn\n|cân\n|{{lang|ja|おの}} / ono\n|{{lang|ko|날근}} / nalgeun\n|55\n|\n|{{lang|zh-Hant|斥、斧、斨}}\n|----\n|style=\"text-align:right\"|[[Radical 70|70]]\n|'''<big>{{lang|zh-Hant|方}}</big>'''\n|4\n|square\n|fāng\n|phương\n|{{lang|ja|ほう}} / hō\n|{{lang|ko|모방}} / mobang\n|92\n|\n|{{lang|zh-Hant|㫃、於、㫄}}\n|----\n|style=\"text-align:right\"|[[Radical 71|71]]\n|'''<big>{{lang|zh-Hant|无}}<br/>({{lang|zh|旡}})</big>'''\n|4\n|not\n|wú\n|vô\n|{{lang|ja|なし}} / nashi\n|{{lang|ko|이미기방}} / imigibang\n|12\n|\n|{{lang|zh-Hant|既、旣、旤}}\n|----\n|style=\"text-align:right\"|[[Radical 72|72]]\n|'''<big>{{lang|zh-Hant|日}}</big>'''\n|4\n|sun\n|rì\n|nhật\n|{{lang|ja|にち}} / nichi\n|{{lang|ko|날일}} / naril\n|453\n|\n|{{lang|zh-Hant|旦、旨、早}}\n|----\n|style=\"text-align:right\"|[[Radical 73|73]]\n|'''<big>{{lang|zh-Hant|曰}}</big>'''\n|4\n|say\n|yuē\n|viết\n|{{lang|ja|いわく}} / iwaku\n|{{lang|ko|가로왈}} / garowal\n|37\n|\n|{{lang|zh-Hant|曲、曳、更}}\n|----\n|style=\"text-align:right\"|[[Radical 74|74]]\n|'''<big>{{lang|zh-Hant|月}}</big>'''\n|4\n|moon\n|yuè\n|nguyệt\n|{{lang|ja|つき}} / tsuki\n|{{lang|ko|달월}} / dalweol\n|69\n|\n|{{lang|zh-Hant|有、朋、服}}\n|----\n|style=\"text-align:right\"|[[Radical 75|75]]\n|'''<big>{{lang|zh-Hant|木}}</big>'''\n|4\n|tree\n|mù\n|mộc\n|{{lang|ja|き}} / ki\n|{{lang|ko|나무목}} / namumok\n|1,369\n|\n|{{lang|zh-Hant|未、末、本}}\n|----\n|style=\"text-align:right\"|[[Radical 76|76]]\n|'''<big>{{lang|zh-Hant|欠}}</big>'''\n|4\n|lack\n|qiàn\n|khiếm\n|{{lang|ja|あくび}} / akubi\n|{{lang|ko|하품흠}} / hapumheum\n|235\n|\n|{{lang|zh-Hant|次、欣、欥}}\n|----\n|style=\"text-align:right\"|[[Radical 77|77]]\n|'''<big>{{lang|zh-Hant|止}}</big>'''\n|4\n|stop\n|zhǐ\n|chỉ\n|{{lang|ja|とめる}} / tomeru\n|{{lang|ko|그칠지}} / geulchilji\n|99\n|\n|{{lang|zh-Hant|正、此、步}}\n|----\n|style=\"text-align:right\"|[[Radical 78|78]]\n|'''<big>{{lang|zh-Hant|歹}}<br/>({{lang|zh|歺}})</big>'''\n|4\n|death\n|dǎi\n|ngạt/đãi\n|{{lang|ja|がつ}} / gatsu\n|{{lang|ko|죽을사변}} / jukeulsabyeon\n|231\n|\n|{{lang|zh-Hant|歺、死、歿}}\n|----\n|style=\"text-align:right\"|[[Radical 79|79]]\n|'''<big>{{lang|zh-Hant|殳}}</big>'''\n|4\n|weapon\n|shū\n|thù\n|{{lang|ja|ほこつくり}} / hokotsukuri\n|{{lang|ko|갖은등글월문}} / gajeundeunggeulweolmun\n|93\n|\n|{{lang|zh-Hant|段、殷、殺}}\n|----\n|style=\"text-align:right\"|[[Radical 80|80]]\n|'''<big>{{lang|zh-Hant|毋}}<br/>({{lang|zh|母}})</big>'''\n|4\n|do not\n|wú\n|vô\n|{{lang|ja|なかれ}} / nakare\n|{{lang|ko|말무}} / malmu\n|16\n|\n|{{lang|zh-Hant|母、每、毐}}\n|----\n|style=\"text-align:right\"|[[Radical 81|81]]\n|'''<big>{{lang|zh-Hant|比}}</big>'''\n|4\n|compare\n|bǐ\n|tỷ\n|{{lang|ja|くらべる}} / kuraberu\n|{{lang|ko|견줄비}} / gyeonjulbi\n|21\n|\n|{{lang|zh-Hant|毖、毗、毘}}\n|----\n|style=\"text-align:right\"|[[Radical 82|82]]\n|'''<big>{{lang|zh-Hant|毛}}</big>'''\n|4\n|fur\n|máo\n|mao\n|{{lang|ja|け}} / ke\n|{{lang|ko|터럭모}} / teoreokmo\n|211\n|\n|{{lang|zh-Hant|毡、毣、毧}}\n|----\n|style=\"text-align:right\"|[[Radical 83|83]]\n|'''<big>{{lang|zh-Hant|氏}}</big>'''\n|4\n|clan\n|shì\n|thị\n|{{lang|ja|うじ}} / uji\n|{{lang|ko|각시씨}} / gaksissi\n|10\n|\n|{{lang|zh-Hant|氐、民、氓}}\n|----\n|style=\"text-align:right\"|[[Radical 84|84]]\n|'''<big>{{lang|zh-Hant|气}}</big>'''\n|4\n|steam\n|qì\n|khí\n|{{lang|ja|きがまえ}} / kigamae\n|{{lang|ko|기운기엄}} / giungieom\n|17\n|(pr. {{lang|zh|氣}})\n|{{lang|zh-Hant|氕、氘、氚}}\n|----\n|style=\"text-align:right\"|[[Radical 85|85]]\n|'''<big>{{lang|zh-Hant|水}}<br/>({{lang|zh|氵、氺}})</big>'''\n|4\n|water \n|shuǐ\n|thủy\n|{{lang|ja|みず}} / mizu\n|{{lang|ko|(아래) 물수 (삼수변)}} / (arae) mulsu (samsubyeon)\n|1,595\n|\n|{{lang|zh-Hant|永、氷、氾}}\n|----\n|style=\"text-align:right\"|[[Radical 86|86]]\n|'''<big>{{lang|zh-Hant|火}}<br/>({{lang|zh|灬}})</big>'''\n|4\n|fire\n|huǒ\n|hỏa\n|{{lang|ja|ひ}} / hi\n|{{lang|ko|불화 (연화발)}} / bulhwa (yeonhwabal)\n|639\n|\n|{{lang|zh-Hant|灰、灯、灶}}\n|----\n|style=\"text-align:right\"|[[Radical 87|87]]\n|'''<big>{{lang|zh-Hant|爪}}<br/>({{lang|zh|爫}})</big>'''\n|4\n|claw\n|zhǎo\n|trảo\n|{{lang|ja|つめ}} / tsume\n|{{lang|ko|손톱조}} / sontopjo\n|36\n|\n|{{lang|zh-Hant|爬、爭、爰}}\n|----\n|style=\"text-align:right\"|[[Radical 88|88]]\n|'''<big>{{lang|zh-Hant|父}}</big>'''\n|4\n|father\n|fù\n|phụ\n|{{lang|ja|ちち}} / chichi\n|{{lang|ko|아버지}} / abeoji\n|10\n|\n|{{lang|zh-Hant|爸、爹、爺}}\n|----\n|style=\"text-align:right\"|[[Radical 89|89]]\n|'''<big>{{lang|zh-Hant|爻}}</big>'''\n|4\n|Trigrams\n|yáo\n|hào\n|{{lang|ja|こう}} / kō\n|{{lang|ko|점괘효}} / jeomgwaehyo\n|16\n|\n|{{lang|zh-Hant|爼、爽、爾}}\n|----\n|style=\"text-align:right\"|[[Radical 90|90]]\n|'''<big>{{lang|zh-Hant|爿}}<br/>({{lang|zh|丬}})</big>'''\n|4\n|split wood\n|qiáng\n|tường  \n|{{lang|ja|しょうへん}} / shōhen\n|{{lang|ko|장수장변}} / jangsujangbyeon\n|48\n|\n|{{lang|zh-Hant|牀、牁、牂}}\n|----\n|style=\"text-align:right\"|[[Radical 91|91]]\n|'''<big>{{lang|zh-Hant|片}}</big>'''\n|4\n|slice\n|piàn\n|phiến\n|{{lang|ja|かた}} / kata\n|{{lang|ko|조각편}} / jogakpyeon\n|77\n|\n|{{lang|zh-Hant|版、牉、牌}}\n|----\n|style=\"text-align:right\"|[[Radical 92|92]]\n|'''<big>{{lang|zh-Hant|牙}}</big>'''\n|4\n|fang\n|yá\n|nha\n|{{lang|ja|きば}} / kiba\n|{{lang|ko|어금니아}} / eogeumnia\n|9\n|\n|{{lang|zh-Hant|㸦、㸧、牚}}\n|----\n|style=\"text-align:right\"|[[Radical 93|93]]\n|'''<big>{{lang|zh-Hant|牛}}<br/>({{lang|zh|牜、⺧}})</big>'''\n|4\n|cow\n|niú\n|ngưu\n|{{lang|ja|うし}} / ushi\n|{{lang|ko|소우}} / sou\n|233\n|\n|{{lang|zh-Hant|牝、牟、牠}}\n|----\n|style=\"text-align:right\"|[[Radical 94|94]]\n|'''<big>{{lang|zh-Hant|犬}}<br/>({{lang|zh|犭}})</big>'''\n|4\n|dog\n|quǎn\n|khuyển\n|{{lang|ja|いぬ}} / inu\n|{{lang|ko|개견 (개사슴록변)}} / gaegyeon (gaesaseumnokbyeon)\n|444\n|\n|{{lang|zh-Hant|犮、犯、犰}}\n|----\n|style=\"text-align:right\"|[[Radical 95|95]]\n|'''<big>{{lang|zh-Hant|玄}}</big>'''\n|5\n|profound\n|xuán\n|huyền\n|{{lang|ja|げん}} / gen\n|{{lang|ko|검을현}} / geomeulhyeon\n|6\n|\n|{{lang|zh-Hant|玅、率、玈}}\n|----\n|style=\"text-align:right\"|[[Radical 96|96]]\n|'''<big>{{lang|zh-Hant|玉}}<br/>({{lang|zh|王、玊}})</big>'''\n|5\n|jade\n|yù\n|ngọc\n|{{lang|ja|たま}} / tama\n|{{lang|ko|구슬옥변}} / guseulokbyeon\n|473\n|\n|{{lang|zh-Hant|玻、瑪、璧}}\n|----\n|style=\"text-align:right\"|[[Radical 97|97]]\n|'''<big>{{lang|zh-Hant|瓜}}</big>'''\n|5\n|melon\n|guā\n|qua\n|{{lang|ja|うり}} / uri\n|{{lang|ko|오이과}} / oigwa\n|55\n|\n|{{lang|zh-Hant|瓝、瓞、瓟}}\n|----\n|style=\"text-align:right\"|[[Radical 98|98]]\n|'''<big>{{lang|zh-Hant|瓦}}</big>'''\n|5\n|tile\n|wǎ\n|ngõa\n|{{lang|ja|かわら}} / kawara\n|{{lang|ko|기와와}} / giwawa\n|174\n|\n|{{lang|zh-Hant|㼚、瓮、瓫}}\n|----\n|style=\"text-align:right\"|[[Radical 99|99]]\n|'''<big>{{lang|zh-Hant|甘}}</big>'''\n|5\n|sweet\n|gān\n|cam\n|{{lang|ja|あまい}} / amai\n|{{lang|ko|달감}} / dalgam\n|22\n|\n|{{lang|zh-Hant|甚、甜、甛}}\n|----\n|style=\"text-align:right\"|[[Radical 100|100]]\n|'''<big>{{lang|zh-Hant|生}}</big>'''\n|5\n|life\n|shēng\n|sinh\n|{{lang|ja|うまれる}} / umareru\n|{{lang|ko|날생}} / nalshaeng\n|22\n|\n|{{lang|zh-Hant|甡、產、甥}}\n|----\n|style=\"text-align:right\"|[[Radical 101|101]]\n|'''<big>{{lang|zh-Hant|用}}</big>'''\n|5\n|use\n|yòng (shuǎi)\n|dụng\n|{{lang|ja|もちいる}} / mochiiru\n|{{lang|ko|쓸용}} / sseulyong\n|10\n|\n|{{lang|zh-Hant|甩、甫、甬}}\n|----\n|style=\"text-align:right\"|[[Radical 102|102]]\n|'''<big>{{lang|zh-Hant|田}}</big>'''\n|5\n|field\n|tián\n|điền\n|{{lang|ja|た}} / ta\n|{{lang|ko|밭전}} / batjeon\n|192\n|\n|{{lang|zh-Hant|由、甲、申}}\n|----\n|style=\"text-align:right\"|[[Radical 103|103]]\n|'''<big>{{lang|zh-Hant|疋}}<br/>({{lang|zh|⺪}})</big>'''\n|5\n|bolt of cloth\n|pǐ\n|thất/sơ\n|{{lang|ja|ひき}} / hiki\n|{{lang|ko|짝필}} / jjakpil\n|15\n|\n|{{lang|zh-Hant|疌、疏、疎}}\n|----\n|style=\"text-align:right\"|[[Radical 104|104]]\n|'''<big>{{lang|zh-Hant|疒}}</big>'''\n|5\n|sickness\n|nè\n|nạch\n|{{lang|ja|やまいだれ}} / yamaidare\n|{{lang|ko|병질엄}} / byeongjileom\n|526\n|\n|{{lang|zh-Hant|疔、疚、疝}}\n|----\n|style=\"text-align:right\"|[[Radical 105|105]]\n|'''<big>{{lang|zh-Hant|癶}}</big>'''\n|5\n|footsteps\n|bō\n|bát\n|{{lang|ja|はつがしら}} / hatsugashira\n|{{lang|ko|필발머리}} / pilbalmeori\n|15\n|\n|{{lang|zh-Hant|癸、癹、発}}\n|----\n|style=\"text-align:right\"|[[Radical 106|106]]\n|'''<big>{{lang|zh-Hant|白}}</big>'''\n|5\n|white\n|bái\n|bạch\n|{{lang|ja|しろ}} / shiro\n|{{lang|ko|흰백}} / heuinbaek \n|109\n|\n|{{lang|zh-Hant|百、皀、皁}}\n|----\n|style=\"text-align:right\"|[[Radical 107|107]]\n|'''<big>{{lang|zh-Hant|皮}}</big>'''\n|5\n|skin\n|pí\n|bì\n|{{lang|ja|けがわ}} / kegawa\n|{{lang|ko|가죽피}} / gajukpi\n|94\n|\n|{{lang|zh-Hant|皯、皰、皴}}\n|----\n|style=\"text-align:right\"|[[Radical 108|108]]\n|'''<big>{{lang|zh-Hant|皿}}</big>'''\n|5\n|dish\n|mǐn\n|mãnh\n|{{lang|ja|さら}} / sara\n|{{lang|ko|그릇명}} / geureutmyeong\n|129\n|\n|{{lang|zh-Hant|盂、盃、盅}}\n|----\n|style=\"text-align:right\"|[[Radical 109|109]]\n|'''<big>{{lang|zh-Hant|目}}<br/>({{linktext|lang=zh|⺫}})</big>'''\n|5\n|eye\n|mù\n|mục\n|{{lang|ja|め}} / me\n|{{lang|ko|눈목}} / nunmok\n|647\n|\n|{{lang|zh-Hant|盯、盱、盲}}\n|----\n|style=\"text-align:right\"|[[Radical 110|110]]\n|'''<big>{{lang|zh-Hant|矛}}</big>'''\n|5\n|spear\n|máo\n|mâu\n|{{lang|ja|むのほこ}} / munohoko\n|{{lang|ko|창모}} / changmo\n|65\n|\n|{{lang|zh-Hant|矜、矞、矟}}\n|----\n|style=\"text-align:right\"|[[Radical 111|111]]\n|'''<big>{{lang|zh-Hant|矢}}</big>'''\n|5\n|arrow\n|shǐ\n|thỉ\n|{{lang|ja|や}} / ya\n|{{lang|ko|화살시}} / hwasalsi\n|64\n|\n|{{lang|zh-Hant|矣、知、矧}}\n|----\n|style=\"text-align:right\"|[[Radical 112|112]]\n|'''<big>{{lang|zh-Hant|石}}</big>'''\n|5\n|stone\n|shí\n|thạch\n|{{lang|ja|いし}} / ishi\n|{{lang|ko|돌석}} / dolseok\n|499\n|\n|{{lang|zh-Hant|矸、矻、矼}}\n|----\n|style=\"text-align:right\"|[[Radical 113|113]]\n|'''<big>{{lang|zh-Hant|示}}<br/>({{lang|zh|礻}})</big>'''\n|5\n|spirit\n|shì\n|thị/kỳ\n|{{lang|ja|しめす}} / shimesu\n|{{lang|ko|보일시 (변)}} / boilsi (byeon)\n|213\n|\n|{{lang|zh-Hant|礼、礽、社}}\n|----\n|style=\"text-align:right\"|[[Radical 114|114]]\n|'''<big>{{lang|zh-Hant|禸}}</big>'''\n|5\n|track\n|róu\n|nhựu\n|{{lang|ja|ぐうのあし}} / gūnoashi\n|{{lang|ko|짐승발자국유}} / jimseungbaljagugyu\n|12\n|\n|{{lang|zh-Hant|禹、禺、离}}\n|----\n|style=\"text-align:right\"|[[Radical 115|115]]\n|'''<big>{{lang|zh-Hant|禾}}</big>'''\n|5\n|grain\n|hé\n|hòa\n|{{lang|ja|のぎ}} / nogi\n|{{lang|ko|벼화}} / byeohwa\n|431\n|\n|{{lang|zh-Hant|禿、秀、私}}\n|----\n|style=\"text-align:right\"|[[Radical 116|116]]\n|'''<big>{{lang|zh-Hant|穴}}</big>'''\n|5\n|cave\n|xué\n|huyệt\n|{{lang|ja|あな}} / ana\n|{{lang|ko|구멍혈}} / gumeonghyeol\n|298\n|\n|{{lang|zh-Hant|究、穸、空}}\n|----\n|style=\"text-align:right\"|[[Radical 117|117]]\n|'''<big>{{lang|zh-Hant|立}}</big>'''\n|5\n|stand\n|lì\n|lập\n|{{lang|ja|たつ}} / tatsu\n|{{lang|ko|설립}} / seollip\n|101\n|\n|{{lang|zh-Hant|竑、竒、竘}}\n|----\n|style=\"text-align:right\"|[[Radical 118|118]]\n|'''<big>{{lang|zh-Hant|竹}}<br/>({{lang|zh|⺮}})</big>'''\n|6\n|bamboo\n|zhú\n|trúc\n|{{lang|ja|たけ}} / take\n|{{lang|ko|대죽}} / daejuk\n|953\n|\n|{{lang|zh-Hant|竺、竽、竿}}\n|----\n|style=\"text-align:right\"|[[Radical 119|119]]\n|'''<big>{{lang|zh-Hant|米}}</big>'''\n|6\n|rice\n|mǐ\n|mễ\n|{{lang|ja|こめ}} / kome\n|{{lang|ko|쌀미}} / ssalmi\n|318\n|\n|{{lang|zh-Hant|籸、籹、籽}}\n|----\n|style=\"text-align:right\"|[[Radical 120|120]]\n|'''<big>{{lang|zh-Hant|糸}}<br/>({{lang|zh|糹}})</big>'''\n|6\n|silk  \n|mì\n|mịch/ty\n|{{lang|ja|いと}} / ito\n|{{lang|ko|실사}} / silsa\n|823\n|{{lang|zh-Hans|纟}}\n|{{lang|zh-Hant|系、糾、紀}}\n|----\n|style=\"text-align:right\"|[[Radical 121|121]]\n|'''<big>{{lang|zh-Hant|缶}}</big>'''\n|6\n|jar\n|fǒu\n|phẫu\n|{{lang|ja|ほとぎ}} / hotogi\n|{{lang|ko|장군부}} / janggunbu\n|77\n|\n|{{lang|zh-Hant|缸、缺、缽}}\n|----\n|style=\"text-align:right\"|[[Radical 122|122]]\n|'''<big>{{lang|zh-Hant|网}}<br/>({{lang|zh|{{linktext|⺲}}、罓、⺳}})</big>'''\n|6\n|net\n|wǎng\n|võng\n|{{lang|ja|あみがしら}} / amigashira\n|{{lang|ko|그물망}} / geumulmang\n|163\n|\n|{{lang|zh-Hant|罔、罕、罘}}\n|----\n|style=\"text-align:right\"|[[Radical 123|123]]\n|'''<big>{{lang|zh-Hant|羊}}<br/>({{lang|zh|⺶、⺷}})</big>'''\n|6\n|sheep\n|yáng\n|dương\n|{{lang|ja|ひつじ}} / hitsuji\n|{{lang|ko|양양}} / yangyang\n|156\n|\n|{{lang|zh-Hant|羋、羌、美}}\n|----\n|style=\"text-align:right\"|[[Radical 124|124]]\n|'''<big>{{lang|zh-Hant|羽}}</big>'''\n|6\n|feather\n|yǔ\n|vũ\n|{{lang|ja|はね}} / hane\n|{{lang|ko|깃우}} / gisu\n|220\n|\n|{{lang|zh-Hant|羿、翀、翁}}\n|----\n|style=\"text-align:right\"|[[Radical 125|125]]\n|'''<big>{{lang|zh-Hant|老}}<br/>({{lang|zh|耂}})</big>'''\n|6\n|old\n|lǎo\n|lão\n|{{lang|ja|おい}} / oi\n|{{lang|ko|늙을로}} / neulgeullo\n|22\n|\n|{{lang|zh-Hant|考、者、耆}}\n|----\n|style=\"text-align:right\"|[[Radical 126|126]]\n|'''<big>{{lang|zh-Hant|而}}</big>'''\n|6\n|and\n|ér\n|nhi\n|{{lang|ja|しかして}} / shikashite\n|{{lang|ko|말이을이}} / malieuri\n|22\n|\n|{{lang|zh-Hant|耍、耎、耏}}\n|----\n|style=\"text-align:right\"|[[Radical 127|127]]\n|'''<big>{{lang|zh-Hant|耒}}</big>'''\n|6\n|plow\n|lěi\n|lỗi\n|{{lang|ja|らいすき}} / raisuki\n|{{lang|ko|가래뢰}} / garaeroe\n|84\n|\n|{{lang|zh-Hant|耔、耕、耖}}\n|----\n|style=\"text-align:right\"|[[Radical 128|128]]\n|'''<big>{{lang|zh-Hant|耳}}</big>'''\n|6\n|ear\n|ěr\n|nhĩ\n|{{lang|ja|みみ}} / mimi\n|{{lang|ko|귀이}} / gwii\n|172\n|\n|{{lang|zh-Hant|耴、耵、耷}}\n|----\n|style=\"text-align:right\"|[[Radical 129|129]]\n|'''<big>{{lang|zh-Hant|聿}}<br/>({{lang|zh|⺺、⺻}})</big>'''\n|6\n|brush\n|yù\n|duật\n|{{lang|ja|ふでづくり}} / fudezukuri\n|{{lang|ko|붓율}} / busyul\n|19\n|\n|{{lang|zh-Hant|肄、肆、肅}}\n|----\n|style=\"text-align:right\"|[[Radical 130|130]]\n|'''<big>{{lang|zh-Hant|肉}}<br/>({{lang|zh|⺼}})</big>'''\n|6\n|meat\n|ròu\n|nhục\n|{{lang|ja|にく}} / niku\n|{{lang|ko|고기육 (육달월)}} / gogiyuk (yukdalwol)\n|674\n|{{lang|zh-Hans|月}}\n|{{lang|zh-Hant|肊、肋、然}}\n|----\n|style=\"text-align:right\"|[[Radical 131|131]]\n|'''<big>{{lang|zh-Hant|臣}}</big>'''\n|6\n|minister\n|chén\n|thần\n|{{lang|ja|しん}} / shin\n|{{lang|ko|신하신}} / sinhasin\n|16\n|\n|{{lang|zh-Hant|臤、臥、臧}}\n|----\n|style=\"text-align:right\"|[[Radical 132|132]]\n|'''<big>{{lang|zh-Hant|自}}</big>'''\n|6\n|self\n|zì\n|tự\n|{{lang|ja|みずから}} / mizukara\n|{{lang|ko|스스로자}} / seuseuroja\n|34\n|\n|{{lang|zh-Hant|臬、臭、臯}}\n|----\n|style=\"text-align:right\"|[[Radical 133|133]]\n|'''<big>{{lang|zh-Hant|至}}</big>'''\n|6\n|arrive\n|zhì\n|chí\n|{{lang|ja|いたる}} / itaru\n|{{lang|ko|이를지}} / ireulji\n|24\n|\n|{{lang|zh-Hant|致、臷、臺}}\n|----\n|style=\"text-align:right\"|[[Radical 134|134]]\n|'''<big>{{lang|zh-Hant|臼}}</big>'''\n|6\n|mortar\n|jiù\n|cữu\n|{{lang|ja|うす}} / usu\n|{{lang|ko|절구구 (변)}} / jeolgugu (byeon)\n|71\n|\n|{{lang|zh-Hant|臾、臿、舁}}\n|----\n|style=\"text-align:right\"|[[Radical 135|135]]\n|'''<big>{{lang|zh-Hant|舌}}</big>'''\n|6\n|tongue\n|shé\n|thiệt\n|{{lang|ja|した}} / shita\n|{{lang|ko|혀설}} / hyeoseol\n|31\n|\n|{{lang|zh-Hant|舍、舐、舑}}\n|----\n|style=\"text-align:right\"|[[Radical 136|136]]\n|'''<big>{{lang|zh-Hant|舛}}</big>'''\n|6\n|oppose\n|chuǎn\n|suyễn\n|{{lang|ja|ます}} / masu\n|{{lang|ko|어그러질천}} / eogeureojilcheon\n|10\n|\n|{{lang|zh-Hant|舜、舝、舞}}\n|----\n|style=\"text-align:right\"|[[Radical 137|137]]\n|'''<big>{{lang|zh-Hant|舟}}</big>'''\n|6\n|boat\n|zhōu\n|chu\n|{{lang|ja|ふね}} / fune\n|{{lang|ko|배주}} / baeju\n|197\n|\n|{{lang|zh-Hant|舠、舡、舢}}\n|----\n|style=\"text-align:right\"|[[Radical 138|138]]\n|'''<big>{{lang|zh-Hant|艮}}</big>'''\n|6\n|stopping\n|gèn\n|cấn\n|{{lang|ja|こん}} / kon\n|{{lang|ko|괘이름간}} / gwaeireumgan\n|5\n|\n|{{lang|zh-Hant|良、艱}}\n|----\n|style=\"text-align:right\"|[[Radical 139|139]]\n|'''<big>{{lang|zh-Hant|色}}</big>'''\n|6\n|color\n|sè\n|sắc\n|{{lang|ja|いろ}} / iro\n|{{lang|ko|빛색}} / bitsaek\n|21\n|\n|{{lang|zh-Hant|艳、艴、艵}}\n|----\n|style=\"text-align:right\"|[[Radical 140|140]]\n|'''<big>{{lang|zh-Hant|艸}}<br/>({{lang|zh-Hant|⺿}})</big>'''\n|6\n|grass\n|cǎo\n|thảo\n|{{lang|ja|くさ}} / kusa\n|{{lang|ko|풀초 (초두머리)}} / pulcho (chodumeori)\n|1,902\n|{{lang|zh-Hans|⺾}}\n|{{lang|zh-Hant|艽、艾、芃}}\n|----\n|style=\"text-align:right\"|[[Radical 141|141]]\n|'''<big>{{lang|zh-Hant|虍}}</big>'''\n|6\n|tiger\n|hū\n|hô\n|{{lang|ja|とらかんむり}} / torakammuri\n|{{lang|ko|범호엄}} / beomhoeom\n|114\n|\n|{{lang|zh-Hant|虎、虐、虒}}\n|----\n|style=\"text-align:right\"|[[Radical 142|142]]\n|'''<big>{{lang|zh-Hant|虫}}</big>'''\n|6\n|insect\n|chóng\n|trùng\n|{{lang|ja|むし}} / mushi\n|{{lang|ko|벌레훼}} / beollehwe\n|1,067\n|(pr. {{lang|zh|蟲}})\n|{{lang|zh-Hant|虬、虯、虱}}\n|----\n|style=\"text-align:right\"|[[Radical 143|143]]\n|'''<big>{{lang|zh-Hant|血}}</big>'''\n|6\n|blood\n|xuè\n|huyết\n|{{lang|ja|ち}} / chi\n|{{lang|ko|피혈}} / pihyeol\n|60\n|\n|{{lang|zh-Hant|衁、衂、衃}}\n|----\n|style=\"text-align:right\"|[[Radical 144|144]]\n|'''<big>{{lang|zh-Hant|行}}</big>'''\n|6\n|walk enclosure\n|xíng\n|hành\n|{{lang|ja|ぎょう}} / gyō\n|{{lang|ko|다닐행}} / danilhaeng\n|53\n|\n|{{lang|zh-Hant|衍、衎、衒}}\n|----\n|style=\"text-align:right\"|[[Radical 145|145]]\n|'''<big>{{lang|zh-Hant|衣}}<br/>({{lang|zh|⻂}})</big>'''\n|6\n|clothes\n|yī\n|y\n|{{lang|ja|ころも}} / koromo\n|{{lang|ko|옷의 (변)}} /oseui (byeon)\n|607\n|\n|{{lang|zh-Hant|初、表、衫}}\n|----\n|style=\"text-align:right\"|[[Radical 146|146]]\n|'''<big>{{lang|zh-Hant|襾}}<br/>({{lang|zh|西、覀}})</big>'''\n|6\n|cover\n|yà\n|che\n|'''{{lang|ja|ア}}''' / a\n|{{lang|ko|덮개}} / ko\n|29\n|\n|{{lang|zh-Hant|西、要、覂}}\n|----\n|style=\"text-align:right\"|[[Radical 147|147]]\n|'''<big>{{lang|zh-Hant|見}}</big>'''\n|7\n|see\n|jiàn\n|kiến\n|{{lang|ja|みる}} / miru\n|{{lang|ko|볼견}} / bolgyeon\n|161\n|{{lang|zh-Hans|见}}\n|{{lang|zh-Hant|規、覓、視}}\n|----\n|style=\"text-align:right\"|[[Radical 148|148]]\n|'''<big>{{lang|zh-Hant|角}}<br/>({{lang|zh|⻇}})</big>'''\n|7\n|horn\n|jiǎo\n|giác\n|{{lang|ja|つの}} / tsuno\n|{{lang|ko|뿔각}} / bbulgak\n|158\n|{{lang|zh-Hans|⻆}}\n|{{lang|zh-Hant|觓、觔、觕}}\n|----\n|style=\"text-align:right\"|[[Radical 149|149]]\n|'''<big>{{lang|zh-Hant|言}}<br/>({{lang|zh|訁}})</big>'''\n|7\n|speech\n|yán\n|ngôn\n|{{lang|ja|ことば}} / kotoba\n|{{lang|ko|말씀언}} / malsseumeon\n|861\n|{{lang|zh-Hans|讠}}\n|{{lang|zh-Hant|訂、訃、計}}\n|----\n|style=\"text-align:right\"|[[Radical 150|150]]\n|'''<big>{{lang|zh-Hant|谷}}</big>'''\n|7\n|valley\n|gǔ\n|cốc\n|{{lang|ja|たに}} / tani\n|{{lang|ko|골곡}} / golgok\n|54\n|\n|{{lang|zh-Hant|谹、谽、谿}}\n|----\n|style=\"text-align:right\"|[[Radical 151|151]]\n|'''<big>{{lang|zh-Hant|豆}}</big>'''\n|7\n|bean\n|dòu\n|đậu\n|{{lang|ja|まめ}} / mame\n|{{lang|ko|콩두}} / kongdu\n|68\n|\n|{{lang|zh-Hant|豇、豈、豉}}\n|----\n|style=\"text-align:right\"|[[Radical 152|152]]\n|'''<big>{{lang|zh-Hant|豕}}</big>'''\n|7\n|pig\n|shǐ\n|thỉ\n|{{lang|ja|いのこ}} / inoko\n|{{lang|ko|돼지시}} / dwaejisi\n|148\n|\n|{{lang|zh-Hant|豗、豚、豜}}\n|----\n|style=\"text-align:right\"|[[Radical 153|153]]\n|'''<big>{{lang|zh-Hant|豸}}</big>'''\n|7\n|badger\n|zhì\n|trãi\n|{{lang|ja|むじな}} / mujina\n|{{lang|ko|갖은돼지시변}} / gajeundwaejisibyeon\n|140\n|\n|{{lang|zh-Hant|豺、豻、豹}}\n|----\n|style=\"text-align:right\"|[[Radical 154|154]]\n|'''<big>{{lang|zh-Hant|貝}}</big>'''\n|7\n|shell\n|bèi\n|bối\n|{{lang|ja|かい}} / kai\n|{{lang|ko|조개패}} / jogaepae\n|277\n|{{lang|zh-Hans|贝}}\n|{{lang|zh-Hant|貞、負、財}}\n|----\n|style=\"text-align:right\"|[[Radical 155|155]]\n|'''<big>{{lang|zh-Hant|赤}}</big>'''\n|7\n|red\n|chì\n|xích\n|{{lang|ja|あか}} / aka\n|{{lang|ko|붉을적}} / bulgeuljeok\n|31\n|\n|{{lang|zh-Hant|赦、赧、赨}}\n|----\n|style=\"text-align:right\"|[[Radical 156|156]]\n|'''<big>{{lang|zh-Hant|走}}</big>'''\n|7\n|run\n|zǒu\n|tẩu\n|{{lang|ja|はしる}} / hashiru\n|{{lang|ko|달릴주}} / dallilju\n|285\n|\n|{{lang|zh-Hant|赳、赴、赶}}\n|----\n|style=\"text-align:right\"|[[Radical 157|157]]\n|'''<big>{{lang|zh-Hant|足}}<br/>({{lang|zh|⻊}})</big>'''\n|7\n|foot\n|zú\n|túc\n|{{lang|ja|あし}} / ashi\n|{{lang|ko|발족}} / baljok\n|580\n|\n|{{lang|zh-Hant|趴、趵、趷}}\n|----\n|style=\"text-align:right\"|[[Radical 158|158]]\n|'''<big>{{lang|zh-Hant|身}}</big>'''\n|7\n|body\n|shēn\n|thân\n|{{lang|ja|み}} / mi\n|{{lang|ko|몸신}} / momsin\n|97\n|\n|{{lang|zh-Hant|躬、躭、躰}}\n|----\n|style=\"text-align:right\"|[[Radical 159|159]]\n|'''<big>{{lang|zh-Hant|車}}</big>'''\n|7\n|cart\n|chē\n|xa\n|{{lang|ja|くるま}} / kuruma\n|{{lang|ko|수레거}} / suregeo\n|361\n|{{lang|zh-Hans|车}}\n|{{lang|zh-Hant|軋、軌、軍}}\n|----\n|style=\"text-align:right\"|[[Radical 160|160]]\n|'''<big>{{lang|zh-Hant|辛}}</big>'''\n|7\n|bitter\n|xīn\n|tân\n|{{lang|ja|からい}} / karai\n|{{lang|ko|매울신}} / maeulsin\n|36\n|\n|{{lang|zh-Hant|辜、辝、辟}}\n|----\n|style=\"text-align:right\"|[[Radical 161|161]]\n|'''<big>{{lang|zh-Hant|辰}}</big>'''\n|7\n|morning\n|chén\n|thần/thìn\n|{{lang|ja|しんのたつ}} / shinnotatsu\n|{{lang|ko|별진}} / byeoljin\n|15\n|\n|{{lang|zh-Hant|辱、農、辴}}\n|----\n|style=\"text-align:right\"|[[Radical 162|162]]\n|'''<big>{{lang|zh-Hant|辵}}<br/>({{lang|zh-Hans|⻌}}、{{lang|ko|⻍}}、{{lang|zh-Hant|⻎}}}})</big>'''\n|7\n|walk\n|chuò\n|sước\n|{{lang|ja|しんにょう}} / shinnyō\n|{{lang|ko|갖은책받침 (책받침)}} / gajeunchaekbatchim (chaekbatchim)\n|381\n|\n|{{lang|zh-Hant|边、巡、迂}}\n|----\n|style=\"text-align:right\"|[[Radical 163|163]]\n|'''<big>{{lang|zh-Hant|邑}}<br/>({{linktext|lang=zh|⻏}})</big>'''\n|7\n|city\n|yì\n|ấp\n|{{lang|ja|むら}} / mura\n|{{lang|ko|고을읍 (우부방)}} / goeureup (ububang)\n|350\n|\n|{{lang|zh-Hant|邕、邗、邘}}\n|----\n|style=\"text-align:right\"|[[Radical 164|164]]\n|'''<big>{{lang|zh-Hant|酉}}</big>'''\n|7\n|wine\n|yǒu\n|dậu\n|{{lang|ja|ひよみのとり}} / hyominotori\n|{{lang|ko|닭유}} / dalgyu\n|290\n|\n|{{lang|zh-Hant|酊、酋、酌}}\n|----\n|style=\"text-align:right\"|[[Radical 165|165]]\n|'''<big>{{lang|zh-Hant|釆}}</big>'''\n|7\n|distinguish\n|biàn\n|biện\n|{{lang|ja|のごめ}} / nogome\n|{{lang|ko|분별할변}} / bunbyeolhalbyeon\n|14\n|\n|{{lang|zh-Hant|采、釉、釋}}\n|----\n|style=\"text-align:right\"|[[Radical 166|166]]\n|'''<big>{{lang|zh-Hant|里}}</big>'''\n|7\n|village\n|lǐ\n|lý\n|{{lang|ja|さと}} / sato\n|{{lang|ko|마을리}} / maeulli\n|14\n|\n|{{lang|zh-Hant|重、野、量}}\n|----\n|style=\"text-align:right\"|[[Radical 167|167]]\n|'''<big>{{lang|zh-Hant|金}}<br/>({{lang|zh|釒}})</big>'''\n|8\n|gold\n|jīn\n|kim\n|{{lang|ja|かね}} / kane\n|{{lang|ko|쇠금}} / soegeum\n|806\n|{{lang|zh-Hans|钅}}\n|{{lang|zh-Hant|釓、釔、釕}}\n|----\n|style=\"text-align:right\"|[[Radical 168|168]]\n|'''<big>{{lang|zh-Hant|長}}<br/>({{lang|zh|镸}})</big>'''\n|8\n|long\n|cháng\n|trường\n|{{lang|ja|ながい}} / nagai\n|{{lang|ko|길장 (변)}} / giljang (byeon)\n|55\n|{{lang|zh-Hans|长}}\n|{{lang|zh-Hant|镺、镻、镼}}\n|----\n|style=\"text-align:right\"|[[Radical 169|169]]\n|'''<big>{{lang|zh-Hant|門}}</big>'''\n|8\n|gate\n|mén\n|môn\n|{{lang|ja|もん}} / mon\n|{{lang|ko|문문}} / munmun\n|246\n|{{lang|zh-Hans|门}}\n|{{lang|zh-Hant|閂、閃、閆}}\n|----\n|style=\"text-align:right\"|[[Radical 170|170]]\n|'''<big>{{lang|zh-Hant|阜}}<br/>({{linktext|lang=zh|⻖}})</big>'''\n|8\n|mound\n|fù\n|phụ\n|{{lang|ja|おか}} / oka\n|{{lang|ko|언덕부 (좌부변)}} / eondeokbu (jwabubyeon)\n|348\n|\n|{{lang|zh-Hant|阞、阡、阢}}\n|----\n|style=\"text-align:right\"|[[Radical 171|171]]\n|'''<big>{{lang|zh-Hant|隶}}</big>'''\n|8\n|slave\n|lì\n|lệ\n|{{lang|ja|れいづくり}} / reizukuri\n|{{lang|ko|미칠이}} / michiri\n|12\n|\n|{{lang|zh-Hant|𨽻、隷、隸}}\n|----\n|style=\"text-align:right\"|[[Radical 172|172]]\n|'''<big>{{lang|zh-Hant|隹}}</big>'''\n|8\n|short-tailed bird\n|zhuī\n|chuy\n|{{lang|ja|ふるとり}} / furutori\n|{{lang|ko|새추}} / saechu\n|233\n|\n|{{lang|zh-Hant|隻、隼、隽}}\n|----\n|style=\"text-align:right\"|[[Radical 173|173]]\n|'''<big>{{lang|zh-Hant|雨}}</big>'''\n|8\n|rain\n|yǔ\n|vũ\n|{{lang|ja|あめ}} / ame\n|{{lang|ko|비우}} / biu\n|298\n|\n|{{lang|zh-Hant|雩、雪、雯}}\n|----\n|style=\"text-align:right\"|[[Radical 174|174]]\n|'''<big>{{lang|zh-Hant|青}}<br/>({{lang|zh|靑}})</big>'''\n|8\n|blue\n|qīng\n|thanh\n|{{lang|ja|あお}} / ao\n|{{lang|ko|푸를청}} / pureulcheong\n|17\n|\n|{{lang|zh-Hant|靖、静、靚}}\n|----\n|style=\"text-align:right\"|[[Radical 175|175]]\n|'''<big>{{lang|zh-Hant|非}}</big>'''\n|8\n|wrong\n|fēi\n|phi\n|{{lang|ja|あらず}} / arazu\n|{{lang|ko|아닐비}} / anilbi\n|25\n|\n|{{lang|zh-Hant|靟、靠、靡}}\n|----\n|style=\"text-align:right\"|[[Radical 176|176]]\n|'''<big>{{lang|zh-Hant|面}}<br/>({{lang|zh|靣}})</big>'''\n|9\n|face\n|miàn\n|diện\n|{{lang|ja|めん}} / men\n|{{lang|ko|낯면}} / natmyeon\n|66\n|\n|{{lang|zh-Hant|靤、靦、靧}}\n|----\n|style=\"text-align:right\"|[[Radical 177|177]]\n|'''<big>{{lang|zh-Hant|革}}</big>'''\n|9\n|leather\n|gé\n|cách\n|{{lang|ja|かくのかわ}} / kakunokawa\n|{{lang|ko|가죽혁}} / gajukhyeok\n|305\n|\n|{{lang|zh-Hant|靭、靮、靳}}\n|----\n|style=\"text-align:right\"|[[Radical 178|178]]\n|'''<big>{{lang|zh-Hant|韋}}</big>'''\n|9\n|tanned leather\n|wéi\n|vi\n|{{lang|ja|なめしがわ}} / nameshigawa\n|{{lang|ko|가죽위}} / gajugwi\n|100\n|{{lang|zh-Hans|韦}}\n|{{lang|zh-Hant|韌、韍、韎}}\n|----\n|style=\"text-align:right\"|[[Radical 179|179]]\n|'''<big>{{lang|zh-Hant|韭}}</big>'''\n|9\n|leek\n|jiǔ\n|cửu\n|{{lang|ja|にら}} / nira\n|{{lang|ko|부추구}} / buchugu\n|20\n|\n|{{lang|zh-Hant|韰、韲、䪢}}\n|----\n|style=\"text-align:right\"|[[Radical 180|180]]\n|'''<big>{{lang|zh-Hant|音}}</big>'''\n|9\n|sound\n|yīn\n|âm\n|{{lang|ja|おと}} / oto\n|{{lang|ko|소리음}} / sorieum\n|43\n|\n|{{lang|zh-Hant|竟、章、韵}}\n|----\n|style=\"text-align:right\"|[[Radical 181|181]]\n|'''<big>{{lang|zh-Hant|頁}}</big>'''\n|9\n|leaf\n|yè\n|hiệt\n|{{lang|ja|おおがい}} / ōgai\n|{{lang|ko|머리혈}} / meorihyeol\n|372\n|{{lang|zh-Hans|页}}\n|{{lang|zh-Hant|頂、頃、頄}}\n|----\n|style=\"text-align:right\"|[[Radical 182|182]]\n|'''<big>{{lang|zh-Hant|風}}</big>'''\n|9\n|wind\n|fēng\n|phong\n|{{lang|ja|かぜ}} / kaze\n|{{lang|ko|바람풍}} / barampung\n|182\n|{{lang|zh-Hans|风}}\n|{{lang|zh-Hant|颩、颭、颮}}\n|----\n|style=\"text-align:right\"|[[Radical 183|183]]\n|'''<big>{{lang|zh-Hant|飛}}</big>'''\n|9\n|fly\n|fēi\n|phi\n|{{lang|ja|とぶ}} / tobu\n|{{lang|ko|날비}} / nalbi\n|92\n|{{lang|zh-Hans|飞}}\n|{{lang|zh-Hant|䬡、飜、飝}}\n|----\n|style=\"text-align:right\"|[[Radical 184|184]]\n|'''<big>{{lang|zh-Hant|食}}<br/>({{lang|zh|飠}})</big>'''\n|9\n|eat\n|shí\n|thực\n|{{lang|ja|しょく}} / shoku\n|{{lang|ko|밥식 (변)}} / bapsik (byeon)\n|403\n|{{lang|zh-Hans|饣}}\n|{{lang|zh-Hant|飡、飢、飣}}\n|----\n|style=\"text-align:right\"|[[Radical 185|185]]\n|'''<big>{{lang|zh-Hant|首}}</big>'''\n|9\n|head\n|shǒu\n|thủ\n|{{lang|ja|くび}} / kubi\n|{{lang|ko|머리수}} / meorisu\n|20\n|\n|{{lang|zh-Hant|馗、䭫、馘}}\n|----\n|style=\"text-align:right\"|[[Radical 186|186]]\n|'''<big>{{lang|zh-Hant|香}}</big>'''\n|9\n|fragrant\n|xiāng\n|hương\n|{{lang|ja|においこう}} / nioikō\n|{{lang|ko|향기향}} / hyanggihyang\n|37\n|\n|{{lang|zh-Hant|馝、馞、馡}}\n|----\n|style=\"text-align:right\"|[[Radical 187|187]]\n|'''<big>{{lang|zh-Hant|馬}}</big>'''\n|10\n|horse\n|mǎ\n|mã\n|{{lang|ja|うま}} / uma\n|{{lang|ko|말마}} / malma\n|472\n|{{lang|zh-Hans|马}}\n|{{lang|zh-Hant|馭、馮、馯}}\n|----\n|style=\"text-align:right\"|[[Radical 188|188]]\n|'''<big>{{lang|zh-Hant|骨}}</big>'''\n|10\n|bone\n|gǔ\n|cốt\n|{{lang|ja|ほね}} / hone\n|{{lang|ko|뼈골}} / ppyeogol\n|185\n|{{lang|zh-Hans|⻣}}\n|{{lang|zh-Hant|骫、骭、骯}}\n|----\n|style=\"text-align:right\"|[[Radical 189|189]]\n|'''<big>{{lang|zh-Hant|高}}<br/>({{lang|zh|髙}})</big>'''\n|10\n|tall\n|gāo\n|cao\n|{{lang|ja|たかい}} / takai\n|{{lang|ko|높을고}} / nopeulgo\n|34\n|\n|{{lang|zh-Hant|髚、髛、𩫛}}\n|----\n|style=\"text-align:right\"|[[Radical 190|190]]\n|'''<big>{{lang|zh-Hant|髟}}</big>'''\n|10\n|hair\n|biāo\n|bưu/tiêu\n|{{lang|ja|かみがしら}} / kamigashira\n|{{lang|ko|터럭발}} / teoreokbal\n|243\n|\n|{{lang|zh-Hant|髠、髡、髢}}\n|----\n|style=\"text-align:right\"|[[Radical 191|191]]\n|'''<big>{{lang|zh-Hant|鬥}}</big>'''\n|10\n|fight\n|dòu\n|đấu\n|{{lang|ja|とうがまえ}} / tōgamae\n|{{lang|ko|싸울투}} / ssaultu\n|23\n|{{lang|zh-Hans|门}}\n|{{lang|zh-Hant|鬦、鬧、鬨}}\n|----\n|style=\"text-align:right\"|[[Radical 192|192]]\n|'''<big>{{lang|zh-Hant|鬯}}</big>'''\n|10\n|sacrificial wine\n|chàng\n|sưởng\n|{{lang|ja|ちょう}} / chō\n|{{lang|ko|울창주창}} / ulchangjuchang\n|8\n|\n|{{lang|zh-Hant|鬰、鬱}}\n|----\n|style=\"text-align:right\"|[[Radical 193|193]]\n|'''<big>{{lang|zh-Hant|鬲}}</big>'''\n|10\n|cauldron\n|lì\n|cách\n|{{lang|ja|かなえ}} / kanae\n|{{lang|ko|다리굽은솥력}} / darigubeunsotryeok\n|73\n|\n|{{lang|zh-Hant|鬳、鬴、鬵}}\n|----\n|style=\"text-align:right\"|[[Radical 194|194]]\n|'''<big>{{lang|zh-Hant|鬼}}</big>'''\n|10\n|ghost\n|guǐ\n|quỷ\n|{{lang|ja|おに}} / oni\n|{{lang|ko|귀신귀}} / gwisingwi\n|141\n|\n|{{lang|zh-Hant|鬾、魁、魂}}\n|----\n|style=\"text-align:right\"|[[Radical 195|195]]\n|'''<big>{{lang|zh-Hant|魚}}</big>'''\n|11\n|fish\n|yú\n|ngư\n|{{lang|ja|うお}} / uo\n|{{lang|ko|물고기어}} / mulgogieo\n|571\n|{{lang|zh-Hans|鱼}}\n|{{lang|zh-Hant|魟、魠、魦}}\n|----\n|style=\"text-align:right\"|[[Radical 196|196]]\n|'''<big>{{lang|zh-Hant|鳥}}</big>'''\n|11\n|bird\n|niǎo\n|điểu\n|{{lang|ja|とり}} / tori\n|{{lang|ko|새조}} / saejo\n|750\n|{{lang|zh-Hans|鸟}}\n|{{lang|zh-Hant|鳦、鳧、鳩}}\n|----\n|style=\"text-align:right\"|[[Radical 197|197]]\n|'''<big>{{lang|zh-Hant|鹵}}</big>'''\n|11\n|salt\n|lǔ\n|lỗ\n|{{lang|ja|ろ}} / ro\n|{{lang|ko|짠땅로}} / jjanddangro\n|44\n|{{lang|zh-Hans|卤}}\n|{{lang|zh-Hant|鹹、鹺、鹼}}\n|----\n|style=\"text-align:right\"|[[Radical 198|198]]\n|'''<big>{{lang|zh-Hant|鹿}}</big>'''\n|11\n|deer\n|lù\n|lộc\n|{{lang|ja|しか}} / shika\n|{{lang|ko|사슴록}} / saseumrok\n|104\n|\n|{{lang|zh-Hant|麀、麁、麂}}\n|----\n|style=\"text-align:right\"|[[Radical 199|199]]\n|'''<big>{{lang|zh-Hant|麥}}</big>'''\n|11\n|wheat\n|mài\n|mạch\n|{{lang|ja|むぎ}} / mugi\n|{{lang|ko|보리맥}} / borimaek\n|131\n|{{lang|zh-Hans|麦}}\n|{{lang|zh-Hant|麨、麩、麪}}\n|----\n|style=\"text-align:right\"|[[Radical 200|200]]\n|'''<big>{{lang|zh-Hant|麻}}</big>'''\n|11\n|hemp\n|má\n|ma\n|{{lang|ja|あさ}} / asa\n|{{lang|ko|삼마}} / samma\n|34\n|\n|{{lang|zh-Hant|麼、麾、黁}}\n|----\n|style=\"text-align:right\"|[[Radical 201|201]]\n|'''<big>{{lang|zh-Hant|黃}}</big>'''\n|12\n|yellow\n|huáng\n|hoàng\n|{{lang|ja|きいろ}} / kiiro\n|{{lang|ko|누를황}} / nureulhwang\n|42\n|{{lang|zh-Hans|黄}}\n|{{lang|zh-Hant|黈、䵍、黌}}\n|----\n|style=\"text-align:right\"|[[Radical 202|202]]\n|'''<big>{{lang|zh-Hant|黍}}</big>'''\n|12\n|millet\n|shǔ\n|thử\n|{{lang|ja|きび}} / kibi\n|{{lang|ko|기장서}} / gijangseo\n|46\n|\n|{{lang|zh-Hant|黎、黏、黐}}\n|----\n|style=\"text-align:right\"|[[Radical 203|203]]\n|'''<big>{{lang|zh-Hant|黑}}</big>'''\n|12\n|black\n|hēi\n|hắc\n|{{lang|ja|くろ}} / kuro\n|{{lang|ko|검을흑}} / geomeulheuk\n|172\n|\n|{{lang|zh-Hant|墨、黓、黔}}\n|----\n|style=\"text-align:right\"|[[Radical 204|204]]\n|'''<big>{{lang|zh-Hant|黹}}</big>'''\n|12\n|embroidery\n|zhǐ\n|chỉ\n|{{lang|ja|ふつ}} / futsu\n|{{lang|ko|바느질할치}} / baneujilhalchi\n|8\n|\n|{{lang|zh-Hant|黺、黻、黼}}\n|----\n|style=\"text-align:right\"|[[Radical 205|205]]\n|'''<big>{{lang|zh-Hant|黽}}</big>'''\n|13\n|frog\n|mǐn\n|mãnh\n|{{lang|ja|べん}} / ben\n|{{lang|ko|맹꽁이}} / maengkkongi\n|40\n|{{lang|zh-Hans|黾}}\n|{{lang|zh-Hant|黿、鼀、鼁}}\n|----\n|style=\"text-align:right\"|[[Radical 206|206]]\n|'''<big>{{lang|zh-Hant|鼎}}</big>'''\n|13\n|tripod\n|dǐng\n|đỉnh\n|{{lang|ja|かなえ}} / kanae\n|{{lang|ko|솥정}} / sotjeong\n|14\n|\n|{{lang|zh-Hant|鼏、鼐、鼒}}\n|----\n|style=\"text-align:right\"|[[Radical 207|207]]\n|'''<big>{{lang|zh-Hant|鼓}}</big>'''\n|13\n|drum\n|gǔ\n|cổ\n|{{lang|ja|つづみ}} / tsudzumi\n|{{lang|ko|북고}} / bukgo\n|46\n|\n|{{lang|zh-Hant|鼕、鼖、鼗}}\n|----\n|style=\"text-align:right\"|[[Radical 208|208]]\n|'''<big>{{lang|zh-Hant|鼠}}</big>'''\n|13\n|rat\n|shǔ\n|thử\n|{{lang|ja|ねずみ}} / nezumi\n|{{lang|ko|쥐서}} / jwiseo\n|92\n|\n|{{lang|zh-Hant|鼢、鼥、鼩}}\n|----\n|style=\"text-align:right\"|[[Radical 209|209]]\n|'''<big>{{lang|zh-Hant|鼻}}</big>'''\n|14\n|nose\n|bí\n|tị\n|{{lang|ja|はな}} / hana\n|{{lang|ko|코비}} / kobi\n|49\n|\n|{{lang|zh-Hant|鼽、鼾、齁}}\n|----\n|style=\"text-align:right\"|[[Radical 210|210]]\n|'''<big>{{lang|zh-Hant|齊}}<br/>({{lang|zh|斉}})</big>'''\n|14\n|even\n|qí\n|tề\n|{{lang|ja|せい}} / sei\n|{{lang|ko|가지런할제}} / gajireonhalje\n|18\n|{{lang|zh-Hans|齐}}\n|{{lang|zh-Hant|齋、齌、齍}}\n|----\n|style=\"text-align:right\"|[[Radical 211|211]]\n|'''<big>{{lang|zh-Hant|齒}}</big>'''\n|15\n|tooth\n|chǐ\n|xỉ/sỉ\n|{{lang|ja|は}} / ha\n|{{lang|ko|이치}} / ichi\n|162\n|{{lang|zh-Hans|齿}}\n|{{lang|zh-Hant|齔、齕、齖}}\n|----\n|style=\"text-align:right\"|[[Radical 212|212]]\n|'''<big>{{lang|zh-Hant|龍}}</big>'''\n|16\n|dragon\n|lóng\n|long\n|{{lang|ja|りゅう}} / ryū\n|{{lang|ko|용룡}} / yongryong\n|14\n|{{lang|zh-Hans|龙}}\n|{{lang|zh-Hant|龏、龑、龔}}\n|----\n|style=\"text-align:right\"|[[Radical 213|213]]\n|'''<big>{{lang|zh-Hant|龜}}</big>'''\n|16\n|turtle\n|guī\n|quy\n|{{lang|ja|かめ}} / kame\n|{{lang|ko|거북귀}} / geobukgwi\n|24\n|{{lang|zh-Hans|龟}}\n|{{lang|zh-Hant|䶰、龝、龞}}\n|----\n|style=\"text-align:right\"|[[Radical 214|214]]\n|'''<big>{{lang|zh-Hant|龠}}</big>'''\n|17\n|flute\n|yuè\n|dược\n|{{lang|ja|やく}} / yaku\n|{{lang|ko|피리약}} / piriyak\n|19\n|\n|{{lang|zh-Hant|龡、龢、龤}}\n|}\n\n==Unicode==\n{{further|Han unification}}\nThe [[Unicode]] standard encoded 20,992 characters in version 1.0.1 (1992) in the [[CJK Unified Ideographs]] block (U+4E00&ndash;9FFF). This standard followed the Kangxi order of radicals ([[radical 1]] at U+4E00, [[radical 214]] at U+9FA0) but did not encode all characters found in the Kangxi dictionary. Individual characters were listed based on their Kangxi radical and number of additional strokes, e.g.  U+5382 厂, the unaugmented [[radical 27]] meaning \"cliff\" is listed under \"27.0\", while [[:wikt:&#x5383;|U+5383]] to [[:wikt:&#x5386;|U+5386]] are listed under \"27.2\" as they all consist of radical 27 plus two additional strokes. More characters were added in later versions, adding \"CJK Unified Ideographs Extensions\" [[CJK Unified Ideographs Extension A|A]], [[CJK Unified Ideographs Extension B|B]], [[CJK Unified Ideographs Extension C|C]], [[CJK Unified Ideographs Extension D|D]], [[CJK Unified Ideographs Extension E|E]] and [[CJK Unified Ideographs Extension F|F]] as of Unicode 12.1 (2019) with further additions planned for Unicode 13.0. Within each \"Extension\", characters are also ordered by Kangxi radical and additional strokes. The Unicode Consortium maintains the \"[[Unihan Database]]\", with a  [https://www.unicode.org/cgi-bin/UnihanRadicalIndex.pl Radical-Stroke-Index]. The Unicode [[Common Locale Data Repository]] provides no  official [[Unicode collation|collation]] (sort order) rule for Unicode CJK characters (short of sorting characters by code point);<ref>Ken Whistler, Markus Scherer, [https://www.unicode.org/reports/tr10/#Implicit_Weights Unicode Collation Algorithm, Unicode Technical Standard #10, version 7.0.0] (2014).</ref> such collation rules as there are language-specific (such as [[JIS X 0208]] for Japanese kanji) and do not include any of the CJK Unified Ideographs Extension characters.\n\n===Kangxi Radicals block===\n{{Infobox Unicode block\n|blockname  = Kangxi Radicals\n|rangestart = 2F00\n|rangeend   = 2FDF\n|script1    = [[Chinese characters|Han]]\n|symbols    = CJK [[Radical (Chinese character)|Radical]]\n|3_0        = 214\n|sources    = [[CNS 11643]]-1992\n|note = <ref>{{cite web|url=https://www.unicode.org|title=Unicode character database|work=The Unicode Standard|accessdate=2016-07-09}}</ref><ref>{{cite web|url=https://www.unicode.org/versions/enumeratedversions.html|title=Enumerated Versions of The Unicode Standard|work=The Unicode Standard|accessdate=2016-07-09}}</ref>\n}}\nIn Unicode version 3.0 (1999), a separate Kangxi Radicals block was introduced which encodes the 214 radicals in sequence, at U+2F00&ndash;2FD5. These are specific code points intended to represent the radical ''qua'' radical, as opposed to the character consisting of the unaugmented radical; thus, U+2F00  represents  [[radical 1]]  while U+4E00 represents the character ''yī'' meaning \"one\". In addition, the  [[CJK Radicals Supplement]] block (2E80–2EFF) was introduced, encoding alternative (often positional) forms taken by Kangxi radicals as they appear within specific characters. For example, ⺁ \"CJK RADICAL CLIFF\" (U+2E81) is a variant of ⼚ [[radical 27]] (U+2F1A), itself identical in shape to the character consisting of unaugmented radical 27,   厂 \"cliff\" (U+5382).\n\n{{Unicode chart Kangxi Radicals}}\n\n===History===\nThe following Unicode-related documents record the purpose and process of defining specific characters in the Kangxi Radicals block:\n\n{| class=\"wikitable collapsible\"\n|-\n! [[Unicode#Versions|Version]] !! {{nobr|Final code points<ref group=lower-alpha name=final/>}} !! Count !! [[International Committee for Information Technology Standards|L2]]&nbsp;ID !! [[ISO/IEC JTC 1/SC 2|WG2]]&nbsp;ID !! [[Ideographic Rapporteur Group|IRG]]&nbsp;ID !! Document\n|-\n| rowspan=\"22\" | 3.0 || rowspan=\"22\" | U+2F00..2FD5 || rowspan=\"22\" | 214 || {{nobr|L2/97-017}} || N1182 || N202 || {{Citation|title=Proposal to add 210 KangXi Radicals and 3 HANGZHOU Numbers in BMP for compatibility|date=1995-03-23}}\n|-\n| || [http://std.dkuug.dk/jtc1/sc2/wg2/docs/n1203.txt N1203] || || {{Citation|title=Unconfirmed minutes of SC2/WG2 Meeting 27, Geneva|date=1995-05-03|first1=V. S.|last1=Umamaheswaran|first2=Mike|last2=Ksar|section=6.1.11}}\n|-\n| || {{nobr|[http://std.dkuug.dk/jtc1/sc2/wg2/docs/n1303d.html N1303 (html],}} [http://std.dkuug.dk/jtc1/sc2/wg2/docs/n1303.doc doc]) || || {{Citation|title=Minutes of Meeting 29, Tokyo|date=1996-01-26|first1=V. S.|last1=Umamaheswaran|first2=Mike|last2=Ksar}}\n|-\n| {{nobr|L2/97-030}} || {{nobr|[http://std.dkuug.dk/jtc1/sc2/wg2/docs/n1503.pdf N1503 (pdf],}} [http://std.dkuug.dk/jtc1/sc2/wg2/docs/n1503.doc doc]) || || {{Citation|title=Unconfirmed Minutes of WG 2 Meeting #32, Singapore; 1997-01-20--24|date=1997-04-01|first1=V. S.|last1=Umamaheswaran|first2=Mike|last2=Ksar|section=8.10}}\n|-\n| || || [http://appsrv.cse.cuhk.edu.hk/~irg/tmp/N449.doc N449] || {{Citation|title=Naming of Kangxi Radicals|date=1997-04-14|first1=John|last1=Jenkins|first2=Xiaoming|last2=Wang}}\n|-\n| {{nobr|L2/97-154}} || N1609 || N479 || {{Citation|title=Kangxi Radicals|date=1997-06-27|first=Zhoucai|last=Zhang}}\n|-\n| {{nobr|L2/97-284}} || N1629 || N486 || {{Citation|title=Kangxi Radicals and Hangzhou Numerals|date=1997-07-07|first=Zhoucai|last=Zhang}}\n|-\n| {{nobr|L2/98-112}} || N1629R || || {{Citation|title=Kangxi Radicals, Hangzhou Numerals|date=1998-03-19|first=Zhoucai|last=Zhang}}\n|-\n| {{nobr|[https://www.unicode.org/L2/L1998/02n3213.pdf L2/98-332]}} || [http://std.dkuug.dk/jtc1/sc2/wg2/docs/n1923_02n3213_text_pdam15_kangxi.pdf N1923] || || {{Citation|title=Combined PDAM registration and consideration ballot on WD for  ISO/IEC 10646-1/Amd. 15,  AMENDMENT 15: Kang Xi radicals and CJK radicals supplement|date=1998-10-28}}\n|-\n| {{nobr|[https://www.unicode.org/L2/L1999/n1903.rtf L2/99-010]}} || {{nobr|[https://www.unicode.org/wg2/docs/n1903.pdf N1903 (pdf],}} [https://www.unicode.org/wg2/docs/n1903.htm html], [https://www.unicode.org/wg2/docs/n1903w97.doc doc]) || || {{Citation|title=Minutes of WG 2 meeting 35, London, U.K.; 1998-09-21--25|date=1998-12-30|first=V. S.|last=Umamaheswaran|section=10.4}}\n|-\n| || || [http://appsrv.cse.cuhk.edu.hk/~irg/tmp/N616_RadName.doc N616] || {{Citation|title=KangXi Radical Names|date=1999-01-04|first=John|last=Jenkins}}\n|-\n| {{nobr|[https://www.unicode.org/L2/L1999/02n32471.pdf L2/99-073.1]}} || [https://www.unicode.org/wg2/docs/n1969.pdf N1969] || || {{Citation|title=Irish Comments on SC 2 N 3213|date=1999-01-19}}\n|-\n| {{nobr|[https://www.unicode.org/L2/L1999/02n3247.htm L2/99-073]}} || {{nobr|[https://www.unicode.org/wg2/docs/n1968.htm N1968 (html],}} [https://www.unicode.org/wg2/docs/n1968_02n3247_pdam15_ballot_responses.doc doc]) || || {{Citation|title=Summary of Voting on SC 2 N 3213, PDAM  ballot on WD for 10646-1/Amd. 15: Kang Xi radicals and CJK radicals supplement|date=1999-02-08}}\n|-\n| {{nobr|[https://www.unicode.org/L2/L1999/02n3311.pdf L2/99-119]}} || || || {{Citation|title=Text for FPDAM ballot of ISO/IEC 10646, Amd. 15 - Kang Xi radicals and CJK radicals supplement|date=1999-04-07}}\n|-\n| {{nobr|[https://www.unicode.org/L2/L1999/N2003.pdf L2/99-232]}} || [https://www.unicode.org/wg2/docs/n2003.pdf N2003] || || {{Citation|title=Minutes of WG 2 meeting 36, Fukuoka, Japan, 1999-03-09--15|date=1999-08-03|first=V. S.|last=Umamaheswaran}}\n|-\n| {{nobr|[https://www.unicode.org/L2/L1999/02n3346.pdf L2/99-252]}} || [https://www.unicode.org/wg2/docs/n2065.pdf N2065] || || {{Citation|title=Summary of Voting on SC 2 N 3311, ISO 10646-1/FPDAM 15 - Kang Xi radicals and CJK radicals supplement|date=1999-08-19}}\n|-\n| {{nobr|[https://www.unicode.org/L2/L1999/99300-02n3361.pdf L2/99-300]}} || [http://std.dkuug.dk/jtc1/sc2/wg2/docs/n2122.pdf N2122] || || {{Citation|title=Revised Text for FDAM ballot of ISO/IEC 10646-1/FDAM 15, AMENDMENT 15: Kang Xi radicals and CJK radicals supplement|date=1999-09-21|first=Bruce|last=Paterson}}\n|-\n| {{nobr|[https://www.unicode.org/L2/L2000/00010-n2103.pdf L2/00-010]}} || [https://www.unicode.org/wg2/docs/n2103.pdf N2103] || || {{Citation|title=Minutes of WG 2 meeting 37, Copenhagen, Denmark: 1999-09-13--16|date=2000-01-05|first=V. S.|last=Umamaheswaran}}\n|-\n| {{nobr|[https://www.unicode.org/L2/L2000/00044-02n3405.pdf L2/00-044]}} || || || {{Citation|title=Summary of FDAM voting: ISO 10646 Amendment 15: Kang Xi radicals and CJK radicals supplement|date=2000-01-31}}\n|-\n| {{nobr|[https://www.unicode.org/L2/L2002/02436-n2534-irg955.pdf L2/02-436]}} || [https://www.unicode.org/wg2/docs/n2534.pdf N2534] || N955 || {{Citation|title=IRG Radical Classification|date=2002-11-21}}\n|-\n| {{nobr|[https://www.unicode.org/L2/L2003/03362-n2659.pdf L2/03-362]}} || [https://www.unicode.org/wg2/docs/n2659.pdf N2659] || [https://drive.google.com/uc?id=1-vUx81XlYpo-vGNiyRv-Wz00CRQ2BHH3 N981] || {{Citation|title=Defect Report on Kangxi Radical Forms|date=2003-10-15|first=Richard|last=Cook}}\n|-\n| {{nobr|[https://www.unicode.org/L2/L2004/04423-kangxi-defects-revisited.pdf L2/04-423]}} || [https://www.unicode.org/wg2/docs/n2882.pdf N2882] || || {{Citation|title=Defect Report on Kangxi Radical Forms, revisited|date=2004-11-18|first=Richard|last=Cook}}\n|- class=\"sortbottom\"\n| colspan=\"7\" | {{reflist|group=lower-alpha|refs=<ref name=final>Proposed code points and characters names may differ from final code points and names</ref>}}\n|}\n\n==See also==\n{{commons category|The 214 Kangxi radicals in the dictionary’s own style (in SVG format)|Kangxi radicals}}\n{{commons category|Chinese radicals}}\n* [[List of Shuowen Jiezi radicals]]\n* [[List of radicals in Unicode]]\n** [[#Unicode|Unicode chart – Kangxi Radicals]] (above)\n** [[CJK Radicals Supplement|Unicode chart – CJK Radicals Supplement]]\n* [[Table of Indexing Chinese Character Components]] (Xinhua Zidian) – 189 radicals\n* [[List of kanji radicals by stroke count|List of Japanese radicals]]\n* [[Radical (Chinese characters)|Section headers of a Chinese dictionary]]\n* [[CJK Unified Ideographs]]\n\n==References==\n{{Reflist}}\n* An Analysis of the Two Chinese Radical Systems, Journal of the Chinese Language Teachers Association, 13, 2, 95–109, May 78\n\n==External links==\n{{wiktionary|Index:Chinese radical}}\n* [http://www.archchinese.com/arch_chinese_radicals.html Simplified Chinese characters with English definitions, grouped by radicals]\n* [https://www.unicode.org/cgi-bin/UnihanGrid.pl?codepoint=2F00&useutf8=false Table of the 214 radicals in the unicode project]\n* [http://sensiblechinese.com/chinese-radicals-chart/ List of radicals in home-printable A4 layout]\n* [http://kanjialive.com/214-traditional-kanji-radicals/ List of 214 Japanese radicals and exceptions to Kangxi], searchable and grouped by stroke number\n* [http://tangorin.com/KanjiRadicals Tangorin], search Japanese kanji using the 214 Kangxi radicals (link broken: [https://web.archive.org/web/20071221031700/http://tangorin.com/KanjiRadicals use the Web Archive.org [[Wayback Machine]] copy instead])\n* [http://ctext.org/dictionary.pl?if=en Chinese characters by radical]\n*[http://kanjidict.com/demo/radicals.html List of Radicals], meaning and naming with Japanese.\n*[http://www.chineseetymology.org Chinese etymology] search radicals and receive the meaning as well as illustrations of radicals in history\n\n{{Kangxi Radicals}}\n{{Unicode CJK Unified Ideographs}}\n\n[[Category:Kangxi radicals| ]]\n[[Category:Chinese characters]]\n[[Category:Chinese dictionaries]]\n[[Category:Collation]]\n[[Category:Unicode blocks]]\n[[Category:Kangxi Emperor]]"
    },
    {
      "title": "List of Latin-script alphabets",
      "url": "https://en.wikipedia.org/wiki/List_of_Latin-script_alphabets",
      "text": "The tables below summarize and compare the letter inventory of some of the [[Latin-script alphabet]]s. In this article, the scope of the word \"alphabet\" is broadened to include letters with tone marks, and other diacritics used to represent a wide range of orthographic traditions, without regard to whether or how they are sequenced in their alphabet or the table.\n{{SpecialChars}}\n\n==Letters contained in the ISO basic Latin alphabet==\n{{Main|ISO basic Latin alphabet}}\n\n===Alphabets that contain all ISO basic Latin letters===\nThe [[International Phonetic Alphabet]] (IPA) includes all 26 letters in their lowercase forms, although ''g'' is always single-storey (''[[ɡ]]'') in the IPA and never double-storey ([[File:Looptail g.svg|8px]]).\n\nAmong alphabets for natural languages the [[Afrikaans]],<sup style=\"font-size:80%\" id=\"r-af\">[[#n-af|[54]]]</sup> [[Aromanian alphabet|Aromanian]], [[Basque alphabet|Basque]],<sup style=\"font-size:80%\" id=\"r-eu\">[[#n-eu|[4]]]</sup> [[Breton alphabet|Breton]], [[Common Brittonic|Celtic British]], [[Catalan alphabet|Catalan]],<sup style=\"font-size:80%\" id=\"r-ca\">[[#n-ca|[6]]]</sup> [[Cornish language|Cornish]], [[Czech alphabet|Czech]],<sup style=\"font-size:80%\" id=\"r-cs\">[[#n-cs|[8]]]</sup> [[Danish alphabet|Danish]],<sup style=\"font-size:80%\" id=\"r-da\">[[#n-da|[9]]]</sup> [[Dutch alphabet|Dutch]],<sup style=\"font-size:80%\" id=\"r-nl\">[[#n-nl|[10]]]</sup> [[Emilian-Romagnol language|Emilian-Romagnol]], [[English alphabet|English]],<sup style=\"font-size:80%\" id=\"r-en\">[[#n-en|[36]]]</sup> [[Estonian alphabet|Estonian]], [[Extremaduran language|Extremaduran]], [[Fala language|Fala]], [[Filipino alphabet|Filipino]],<sup style=\"font-size:80%\" id=\"r-tl\">[[#n-tl|[11]]]</sup> [[Finnish alphabet|Finnish]], [[French alphabet|French]],<sup style=\"font-size:80%\" id=\"r-fr\">[[#n-fr|[12]]]</sup> [[Galician alphabet|Galician]],<sup style=\"font-size:80%\" id=\"r-gl\">[[#n-gl|[33]]]</sup> [[German alphabet|German]],<sup style=\"font-size:80%\" id=\"r-de\">[[#n-de|[13]]]</sup> [[Greenlandic alphabet|Greenlandic]], [[Hungarian alphabet|Hungarian]],<sup style=\"font-size:80%\" id=\"r-hu\">[[#n-hu|[15]]]</sup>  [[Indonesian alphabet|Indonesian]], [[Javanese Latin alphabet|Javanese]], [[Karakalpak alphabet|Karakalpak]],<sup style=\"font-size:80%\" id=\"r-kaa\">[[#n-kaa|[23]]]</sup> [[Kurdish alphabet|Kurdish]], [[Latin alphabet|Modern Latin]], [[Luxembourgish language|Luxembourgish]], [[Malay alphabet|Malay]], [[Mirandese language|Mirandese]], [[Norwegian alphabet|Norwegian]],<sup style=\"font-size:80%\" id=\"r-da\">[[#n-da|[9]]]</sup> [[Oromo language|Oromo]]<sup style=\"font-size:80%\" id=\"r-om\">[[#n-om|[65]]]</sup> [[Papiamento orthography|Papiamento]]<sup style=\"font-size:80%\" id=\"r-pap\">[[#n-pap|[63]]]</sup> [[Portuguese alphabet|Portuguese]], [[Quechua alphabet|Quechua]], [[Rhaeto-Romance languages|Rhaeto-Romance]], [[Romanian alphabet|Romanian]], [[Slovak alphabet|Slovak]],<sup style=\"font-size:80%\" id=\"r-sk\">[[#n-sk|[24]]]</sup> [[Spanish alphabet|Spanish]],<sup style=\"font-size:80%\" id=\"r-es\">[[#n-es|[25]]]</sup> [[Sundanese language|Sundanese]], [[Swedish alphabet|Swedish]], [[Tswana language|Tswana]],<sup style=\"font-size:80%\" id=\"r-tn\">[[#n-tn|[52]]]</sup> [[Tunisian Arabic#Elyssa Alphabet|Tunisian Arabic]],<sup style=\"font-size:80%\" id=\"r-aeb\">[[#n-aeb|[58]]]</sup> [[Uyghur language|Uyghur]], [[Venda language#Writing system|Venda]],<sup style=\"font-size:80%\" id=\"r-ve\">[[#n-ve|[51]]]</sup> [[Võro alphabet|Võro]], [[Walloon alphabet|Walloon]],<sup style=\"font-size:80%\" id=\"r-wa\">[[#n-wa|[27]]]</sup> [[West Frisian alphabet|West Frisian]], [[Xhosa alphabet|Xhosa]], [[Zulu alphabet|Zulu]] alphabets include all 26 letters, ''at least'' in their largest version.\n\nAmong alphabets for constructed languages the [[Ido alphabet|Ido]], [[Interglossa]], [[Interlingua alphabet|Interlingua]], [[Occidental alphabet|Occidental]] alphabets include all 26 letters.\n\n===Alphabets that do not contain all ISO basic Latin letters===\nThis list is based on official definitions of each alphabet. Still, missing letters might occur in non-integrated loan words and place names.\n{| cellspacing=\"0\" cellpadding=\"1\" class=\"wikitable sortable collapsible\" style=\"border:1px solid #aaa;border-collapse:collapse;line-height:1.4;text-align:center;table-layout:fixed\"\n|+ Reduced usage of the letters of the [[ISO basic Latin alphabet]]<ref>As defined in [[ISO/IEC 646]] based on [[ASCII]], which was based on the 26 letters of the [[English alphabet]] and previous [[telecommunications]] standards, and used in later ISO standards, see [[Latin characters in Unicode]].</ref> (A–Z) in various alphabets:\n|-\n!style=\"width:10em\"|Alphabet\n!style=\"width:1.5em\"|[[A]]\n!style=\"width:1.5em\"|[[B]]\n!style=\"width:1.5em\"|[[C]]\n!style=\"width:1.5em\"|[[D]]\n!style=\"width:1.5em\"|[[E]]\n!style=\"width:1.5em\"|[[F]]\n!style=\"width:1.5em\"|[[G]]\n!style=\"width:1.5em\"|[[H]]\n!style=\"width:1.5em\"|[[I]]\n!style=\"width:1.5em\"|[[J]]\n!style=\"width:1.5em\"|[[K]]\n!style=\"width:1.5em\"|[[L]]\n!style=\"width:1.5em\"|[[M]]\n!style=\"width:1.5em\"|[[N]]\n!style=\"width:1.5em\"|[[O]]\n!style=\"width:1.5em\"|[[P]]\n!style=\"width:1.5em\"|[[Q]]\n!style=\"width:1.5em\"|[[R]]\n!style=\"width:1.5em\"|[[S]]\n!style=\"width:1.5em\"|[[T]]\n!style=\"width:1.5em\"|[[U]]\n!style=\"width:1.5em\"|[[V]]\n!style=\"width:1.5em\"|[[W]]\n!style=\"width:1.5em\"|[[X]]\n!style=\"width:1.5em\"|[[Y]]\n!style=\"width:1.5em\"|[[Z]]\n!class=\"nocollapse\"| #\n|-\n![[Classical Latin alphabet|Classical Latin]]<sup style=\"font-size:80%\" id=\"r-la\">[[#n-la|[2]]]</sup>\n|A||B||C||D||E||F||G||H||I|| ||K||L||M||N||O||P||Q||R||S||T|| ||V|| ||X||Y||Z||23\n|-\n![[Albanian alphabet|Albanian]]<sup style=\"font-size:80%\" id=\"r-sq\">[[#n-sq|[3]]]</sup>\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P||Q||R||S||T||U||V|| ||X||Y||Z||25\n|-\n![[Old English Latin alphabet|Anglo-Saxon]]\n|A||B||C||D||E||F||G||H||I|| ||K||L||M||N||O||P||Q||R||S||T||U|| || ||X||Y||Z||23\n|-\n![[Arbëresh alphabet|Arbëresh]]\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P||Q||R||S||T||U||V|| ||X|| ||Z||24\n|-\n![[Asturian alphabet|Asturian]]\n|A||B||C||D||E||F||G||H||I|| || ||L||M||N||O||P||Q||R||S||T||U||V|| ||X||Y||Z||23\n|-\n![[Azerbaijani alphabet|Azeri]]<sup style=\"font-size:80%\" id=\"r-az\">[[#n-az|[53]]]</sup>\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P||Q||R||S||T||U||V|| ||X||Y||Z||25\n|-\n![[Bambara language|Bambara]]<sup style=\"font-size:80%\" id=\"r-bm\">[[#n-bm|[39]]]</sup>\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U|| ||W|| ||Y||Z||23\n|-\n![[Belarusian Latin alphabet|Belarusian]]<sup style=\"font-size:80%\" id=\"r-be\">[[#n-be|[5]]]</sup>\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U||V|| || ||Y||Z||23\n|-\n![[Berber Latin alphabet|Berber]]\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N|| || ||Q||R||S||T||U|| ||W||X||Y||Z||23\n|-\n![[Bislama language|Bislama]]<sup style=\"font-size:80%\" id=\"r-bi:1\">[[#n-bi|[45]]]</sup>\n|A||B|| ||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U||V||W|| ||Y|| ||22\n|-\n![[Chamorro alphabet|Chamorro]]<sup style=\"font-size:80%\" id=\"r-ch\">[[#n-ch|[43]]]</sup>\n|A||B||C||D||E||F||G||H||I|| ||K||L||M||N||O||P|| ||R||S||T||U|| || || ||Y|| ||20\n|-\n![[Chewa language|Chewa]]\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U||V||W|| ||Y||Z||25\n|-\n![[Corsican alphabet|Corsican]]<sup style=\"font-size:80%\" id=\"r-co\">[[#n-co|[31]]]</sup>\n|A||B||C||D||E||F||G||H||I||J|| ||L||M||N||O||P||Q||R||S||T||U||V|| || || ||Z||22\n|-\n![[Crimean Tatar alphabet|Crimean Tatar]]\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P||Q||R||S||T||U||V|| || ||Y||Z||24\n|-\n![[Croatian alphabet|Croatian]] <sup style=\"font-size:80%\" id=\"r-hr\">[[#n-hr|[7]]]</sup>\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U||V|| || || ||Z||22\n|-\n![[Cypriot Arabic#Writing System|Cypriot Arabic]]<sup style=\"font-size:80%\" id=\"r-acy\">[[#n-acy|[59]]]</sup>\n|A||B||C||D||E||F||G|| ||I||J||K||L||M||N||O||P|| ||R||S||T||U||V||W||X||Y||Z||24\n|-\n![[Carrier language|Dakelh]]<sup style=\"font-size:80%\" id=\"r-crx\">[[#n-crx|[61]]]</sup>\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O|| || ||R||S||T||U|| ||W|| ||Y||Z||22\n|-\n![[Dakota language|Dakota]]\n|A||B||C||D||E||F||G||H||I||J||K|| ||M||N||O||P|| || ||S||T||U|| ||W|| ||Y||Z||20\n|-\n![[Dalecarlian alphabet|Dalecarlian]]\n|A||B|| ||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U||V||W|| ||Y|| ||22\n|-\n![[Dinka alphabet|Dinka]]<sup style=\"font-size:80%\" id=\"r-din\">[[#n-din|[40]]]</sup>\n|A||B||C||D||E|| ||G||H||I||J||K||L||M||N||O||P|| ||R|| ||T||U|| ||W|| ||Y|| ||20\n|-\n![[Esperanto alphabet|Esperanto]]\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U||V|| || || ||Z||22\n|-\n![[Extremaduran language|Extremaduran]]\n|A||B||C||D||E||F||G||H||I||J|| ||L||M||N||O||P||Q||R||S||T||U||V|| ||X||Y||Z||24\n|-\n![[Fala language|Traditional Fala]]\n|A||B||C||D||E||F||G||H||I||J|| ||L||M||N||O||P||Q||R||S||T||U||V|| ||X|| ||Z||23\n|-\n![[Faroese alphabet|Faroese]]\n|A||B|| ||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U||V|| || ||Y|| ||21\n|-\n![[Friulian alphabet|Friulian]]\n|A||B||C||D||E||F||G||H||I||J|| ||L||M||N||O||P||Q||R||S||T||U||V|| || || ||Z||22\n|-\n![[Fula alphabets|Fula]]<sup style=\"font-size:80%\" id=\"r-ff\">[[#n-ff|[41]]]</sup>\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U|| ||W||X||Y|| ||23\n|-\n![[Gagauz language#Latin alphabet|Gagauz]]\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U||V|| || ||Y||Z||23\n|-\n![[Galician alphabet|Traditional Galician]]<sup style=\"font-size:80%\" id=\"r-gl\">[[#n-gl|[33]]]</sup>\n|A||B||C||D||E||F||G||H||I|| || ||L||M||N||O||P||Q||R||S||T||U||V|| ||X|| ||Z||22\n|-\n![[Gilbertese alphabet|Gilbertese]]\n|A||B|| || ||E|| || || ||I|| ||K|| ||M||N||O|| || ||R|| ||T||U|| ||W|| || || ||12\n|-\n![[Glosa language|Glosa]]\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P||Q||R||S||T||U||V||W||X|| ||Z||25\n|-\n![[Greenlandic alphabet|Traditional Greenlandic]]\n|A|| || || ||E||F||G||H||I||J||K||L||M||N||O||P||Q||R||S||T||U||V|| || || || ||19\n|-\n![[Guaraní alphabet|Guaraní]]<sup style=\"font-size:80%\" id=\"r-gn\">[[#n-gn|[14]]]</sup>\n|A||B||C||D||E|| ||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U||V|| || ||Y|| ||21\n|-\n![[Gwich'in language|Gwich'in]]\n|A||B||C||D||E||F||G||H||I||J||K|| ||M||N||O|| || ||R||S||T||U||V||W|| ||Y||Z||22\n|-\n![[Haitian Creole|Haitian]]\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U||V||W||X||Y||Z||25\n|-\n![[Hän language|Hän]]\n|A||B||C||D||E|| ||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U|| ||W|| ||Y||Z||22\n|-\n![[Hausa alphabet|Hausa]]<sup style=\"font-size:80%\" id=\"r-ha\">[[#n-ha|[30]]]</sup>\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O|| || ||R||S||T||U|| ||W|| ||Y||Z||22\n|-\n![[Hawaiian alphabet|Hawaiian]]\n|A|| || || ||E|| || ||H||I|| ||K||L||M||N||O||P|| || || || ||U|| ||W|| || || ||12\n|-\n![[Icelandic orthography|Icelandic]]\n|A||B|| ||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U||V|| ||X||Y|| ||22\n|-\n![[Igbo language#Writing System|Igbo]]<sup style=\"font-size:80%\" id=\"r-ig\">[[#n-ig|[42]]]</sup>\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U||V||W|| ||Y||Z||24\n|-\n![[Inari Sami language|Inari Sami]]\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U||V|| || ||Y||Z||23\n|-\n![[Irish orthography|Irish]]<sup style=\"font-size:80%\" id=\"r-ga\">[[#n-ga|[16]]]</sup>\n|A||B||C||D||E||F||G||H||I|| || ||L||M||N||O||P|| ||R||S||T||U||V|| || || ||Z||20\n|-\n![[Italian alphabet|Italian]]<sup style=\"font-size:80%\" id=\"r-it\">[[#n-it|[17]]]</sup>\n|A||B||C||D||E||F||G||H||I|| || ||L||M||N||O||P||Q||R||S||T||U||V|| || || ||Z||21\n|-\n![[Kashubian alphabet|Kashubian]]\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U|| ||W|| ||Y||Z||23\n|-\n![[Kazakh alphabets|Kazakh]]<sup style=\"font-size:80%\" id=\"r-kk\">[[#n-kk|[38]]]</sup>\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P||Q||R||S||T||U||V|| || ||Y||Z||24\n|-\n![[Khasi language|Khasi]]\n|A||B|| ||D||E|| ||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U|| ||W|| ||Y|| ||20\n|-\n![[Latvian alphabet|Latvian]]<sup style=\"font-size:80%\" id=\"r-lv\">[[#n-lv|[18]]]</sup>\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U||V|| || ||Y||Z||23\n|-\n![[Lithuanian alphabet|Lithuanian]]<sup style=\"font-size:80%\" id=\"r-lt\">[[#n-lt|[19]]]</sup>\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U||V|| || ||Y||Z||23\n|-\n![[Livonian language#Alphabet|Livonian]]<sup style=\"font-size:80%\" id=\"r-liv\">[[#n-liv|[46]]]</sup>\n|A||B|| ||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U||V|| || ||Y||Z||23\n|-\n![[Lojban language|Lojban]]\n|A||B||C||D||E||F||G|| ||I||J||K||L||M||N||O||P|| ||R||S||T||U||V|| ||X||Y||Z||23\n|-\n![[Lule Sami language|Lule Sami]]<sup style=\"font-size:80%\" id=\"r-ls\">[[#n-ls|[60]]]</sup>\n|A||B|| ||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U||V|| || || || ||20\n|-\n![[Malagasy alphabet|Malagasy]] \n|A||B|| ||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T|| ||V|| || ||Y||Z||21\n|-\n![[Maltese alphabet|Maltese]]<sup style=\"font-size:80%\" id=\"r-mt\">[[#n-mt|[20]]]</sup>\n|A||B|| ||D||E||F||G||H||I||J||K||L||M||N||O||P||Q||R||S||T||U||V||W||X|| ||Z||24\n|-\n![[Manx Gaelic]]\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P||Q||R||S||T||U||V||W|| ||Y|| ||24\n|-\n![[Māori alphabet|Māori]]<sup style=\"font-size:80%\" id=\"r-mi\">[[#n-mi|[34]]]</sup>\n|A|| || || ||E|| ||G||H||I|| ||K|| ||M||N||O||P|| ||R|| ||T||U|| ||W|| || || ||14\n|-\n![[Marshallese language|Marshallese]]<sup style=\"font-size:80%\" id=\"r-mh\">[[#n-mh|[47]]]</sup>\n|A||B|| ||D||E|| || || ||I||J||K||L||M||N||O||P|| ||R|| ||T||U|| ||W|| ||Y|| ||17\n|-\n![[Massachusett writing systems|Massachusett]]<sup style=\"font-size:80%\" id=\"r-mss\">[[#n-mss|[62]]]</sup>\n|A|| ||C|| ||E|| || ||H|| || ||K|| ||M||N|| ||P||Q|| ||S||T||U|| ||W|| ||Y|| ||14\n|-\n![[Mirandese language|Traditional Mirandese]]\n|A||B||C||D||E||F||G||H||I||J|| ||L||M||N||O||P||Q||R||S||T||U|| || ||X||Y||Z||23\n|-\n![[Mohawk alphabet|Mohawk]]<sup style=\"font-size:80%\" id=\"r-moh\">[[#n-moh|[35]]]</sup>\n|A|| || || ||E|| || ||H||I|| ||K|| || ||N||O|| || ||R||S||T|| || ||W|| ||Y|| ||12\n|-\t\n![[Na'vi language#Phonology and orthography|Na'vi]]<sup style=\"font-size:80%\" id=\"r-mis\">[[#n-mis|[57]]]<ref>[http://www.omniglot.com/conscripts/nav.htm]</ref></sup>\n|A|| || || ||E||F||G||H||I|| ||K||L||M||N||O||P|| ||R||S||T||U||V||W||X||Y||Z||21\n|-\n![[Navajo language|Navajo]]\n|A||B||C||D||E|| ||G||H||I||J||K||L||M||N||O|| || || ||S||T|| || ||W||X||Y||Z||20\n|-\n![[Northern Sami]]\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U||V|| || || ||Z||22\n|-\n![[Nuxalk language|Nuxalk]]\n|A|| ||C|| || || || ||H||I|| ||K||L||M||N|| ||P||Q|| ||S||T||U|| ||W||X||Y|| ||16\n|-\n![[Occitan alphabet|Occitan]]\n|A||B||C||D||E||F||G||H||I||J|| ||L||M||N||O||P||Q||R||S||T||U||V|| ||X|| ||Z||23\n|-\n![[Pan-Nigerian alphabet|Pan-Nigerian]]\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U||V||W|| ||Y||Z||24\n|-\n![[Piedmontese language|Piedmontese]]\n|A||B||C||D||E||F||G||H||I||J|| ||L||M||N||O||P||Q||R||S||T||U||V|| || || ||Z||22\n|-\n![[Pinyin]]<sup style=\"font-size:80%\" id=\"r-pinyin\">[[#n-pinyin|[32]]]</sup>\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P||Q||R||S||T||U|| ||W||X||Y||Z||25\n|-\n![[Polish alphabet|Polish]]<sup style=\"font-size:80%\" id=\"r-pl\">[[#n-pl|[22]]]</sup>\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U|| ||W|| ||Y||Z||23\n|-\n![[Romani writing systems|Romani]]<sup style=\"font-size:80%\" id=\"r-rom\">[[#n-rom|[29]]]</sup>\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U||V|| ||X|| ||Z||23\n|-\n![[Rotokas alphabet|Rotokas]]\n|A|| || || ||E|| ||G|| ||I|| ||K|| || || ||O||P|| ||R||S||T||U||V|| || || || ||12\n|-\n![[Samoan language#Alphabet|Samoan]]\n|A|| || || ||E||F||G|| ||I|| || ||L||M||N||O||P|| || ||S||T||U||V|| || || || ||14\n|-\n![[Sardinian alphabet|Sardinian]]\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P||Q||R||S||T||U||V|| ||X||Y||Z||25\n|-\n![[Scottish Gaelic alphabet|Scots Gaelic]]\n|A||B||C||D||E||F||G||H||I|| || ||L||M||N||O||P|| ||R||S||T||U|| || || || || ||18\n|-\n![[Serbian alphabet|Serbian]]<sup style=\"font-size:80%\" id=\"r-sr\">[[#n-hr|[7]]]</sup>\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U||V|| || || ||Z||22\n|-\n![[Shona alphabet|Shona]]\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U||V||W|| ||Y||Z||24\n|-\n![[Sicilian alphabet|Sicilian]]\n|A||B||C||D||E||F||G||H||I||J|| ||L||M||N||O||P||Q||R||S||T||U||V|| || || ||Z||22\n|-\n![[Skolt Sami language|Skolt Sami]]\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U||V|| || || ||Z||22\n|-\n![[Slovenian alphabet|Slovenian]]\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U||V|| || || ||Z||22\n|-\n![[Somali alphabet|Somali]]\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O|| ||Q||R||S||T||U|| ||W||X||Y|| ||23\n|-\n![[Sorbian alphabet|Sorbian]]<sup style=\"font-size:80%\" id=\"r-wen\">[[#n-wen|[64]]]</sup>\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U|| ||W|| ||Y||Z||23\n|-\n![[Southern Sami language|Southern Sami]]\n|A||B|| ||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U||V|| || ||Y|| ||21\n|-\n![[Swahili alphabet|Swahili]]\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U||V||W|| ||Y||Z||24\n|-\n![[Abakada|Tagalog]]<sup style=\"font-size:80%\" id=\"r-tl\">[[#n-tl|[11]]]</sup>\n|A||B|| ||D||E|| ||G||H||I|| ||K||L||M||N||O||P|| ||R||S||T||U|| ||W|| ||Y|| ||19\n|-\n![[Tahitian language#Phonology|Tahitian]]\n|A|| || || ||E||F|| ||H||I|| || || ||M||N||O||P|| ||R|| ||T||U||V|| || || || ||13\n|-\n![[Tetum alphabet|Tetum]]\n|A||B|| ||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U||V||W||X|| ||Z||23\n|-\n![[Tongan language#Tongan alphabet|Tongan]]\n|A|| || || ||E||F||G||H||I|| ||K||L||M||N||O||P|| || ||S||T||U||V|| || || || ||16\n|-\n![[Turkish alphabet|Turkish]]\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U||V|| || ||Y||Z||23\n|-\n![[Turkmen alphabet|Turkmen]]<sup style=\"font-size:80%\" id=\"r-tk\">[[#n-tk|[55]]]</sup>\n|A||B|| ||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U|| ||W|| ||Y||Z||22\n|-\n![[Ulithian language|Ulithian]]<sup style=\"font-size:80%\" id=\"r-uli\">[[#n-uli|[49]]]</sup>\n|A||B||C||D||E||F||G||H||I|| ||K||L||M||N||O||P|| ||R||S||T||U|| ||W|| ||Y|| ||21\n|-\n![[Ume Sami language|Ume Sami]]\n|A||B|| ||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U||V|| || ||Y|| ||21\n|-\n![[Uzbek alphabet|Uzbek]]<sup style=\"font-size:80%\" id=\"r-uz\">[[#n-uz|[25]]]</sup>\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P||Q||R||S||T||U||V|| ||X||Y||Z||25\n|-\n![[Vietnamese alphabet|Vietnamese]]<sup style=\"font-size:80%\" id=\"r-vi\">[[#n-vi|[26]]]</sup>\n|A||B||C||D||E|| ||G||H||I|| ||K||L||M||N||O||P||Q||R||S||T||U||V|| ||X||Y|| ||22\n|-\n![[Volapük alphabet|Volapük]]\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U||V||W||X||Y||Z||25\n|-\n![[Welsh alphabet|Welsh]]<sup style=\"font-size:80%\" id=\"r-cy\">[[#n-cy|[28]]]</sup>\n|A||B||C||D||E||F||G||H||I||J|| ||L||M||N||O||P|| ||R||S||T||U|| ||W|| ||Y|| ||21\n|-\n![[Wolof alphabet|Wolof]]\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P||Q||R||S||T||U|| ||W||X||Y|| ||24\n|-\n![[Yapese language|Yapese]]<sup style=\"font-size:80%\" id=\"r-yap\">[[#n-yap|[50]]]</sup>\n|A||B||C||D||E||F||G||H||I||J||K||L||M||N||O||P||Q||R||S||T||U|| ||W|| ||Y|| ||23\n|-\n![[Yoruba language#Writing system|Yoruba]]<sup style=\"font-size:80%\" id=\"r-yo\">[[#n-yo|[44]]]</sup>\n|A||B|| ||D||E||F||G||H||I||J||K||L||M||N||O||P|| ||R||S||T||U|| ||W|| ||Y|| ||21\n|-\n![[Zuni language#Writing system|Zuni]]<sup style=\"font-size:80%\" id=\"r-zun\">[[#n-zun|[66]]]</sup>\n|A||B||C||D||E|| || ||H||I|| ||K||L||M||N||O||P|| || ||S||T||U|| ||W|| ||Y|| ||18\n|-class=\"sortbottom\"\n! count\n|100||89||74||88||99||83||92||94||99||77||84||92||98||99||97||92||32||92||94||99\n||96||66||46||31||70||65\n|}\n\nNote: The I is used in two distinct versions in Turkic languages, [[Dotted and dotless I|dotless (I ı) and dotted (İ i)]]. They are considered different letters, and case conversion must take care to preserve the distinction. Note that [[Irish alphabet|Irish]] traditionally does not write the dot, or [[tittle]], over the small letter ''i'', but the language makes no distinction here if a dot is displayed, so no specific encoding and special case conversion rule is needed like for Turkic alphabets.\n\n====Statistics====\nThe chart above lists a variety of alphabets that do not officially contain all 26 letters of the ISO basic Latin alphabet. In this list, one letter is used by all of them: A. For each of the 26 basic ISO Latin alphabet letters, the number of alphabets in the list above using it is as follows:\n{| cellspacing=\"0\" cellpadding=\"1\" class=\"wikitable\" style=\"border:1px solid\"\n! Letter\n|A ||E ||I ||N ||T ||M ||O ||U ||H ||S ||G ||L ||P ||R ||B ||D ||K ||F ||J ||C ||Y ||V ||Z ||W ||Q ||X\n|-\n! Alphabets\n|100 ||99 ||99 ||99 ||99 ||98 ||97 ||96 ||94 ||94 ||92 ||92 ||92 ||92 ||89 ||88 ||84 ||83 ||77 ||74 ||70 ||66 ||65 ||46 ||32 ||31\n|}\n\n==Letters not contained in the ISO basic Latin alphabet==\n{{See also|List of Latin-script letters#Extensions|label 1=List of Latin-script letters: Extensions}}\nSome languages have extended the Latin alphabet with [[ligature (typography)|ligatures]], [[diacritic|modified letters]], or [[digraph (orthography)|digraphs]]. These symbols are listed below. The characters in the following tables may not all render, depending on which [[Operating system advocacy|operating system]] and [[user agent|browser]] version are used, and the presence or absence of [[Unicode control characters|Unicode]] [[List of type designers|fonts]].\n\n===Additional letters by type===\n\n====Independent letters and ligatures====\n{| cellspacing=\"0\" cellpadding=\"1\" class=\"wikitable\" style=\"border:1px solid #aaa;border-collapse:collapse;line-height:1.4;text-align:center;table-layout:fixed\"\n|-\n!style=\"width:10em\" rowspan=\"2\"|Additional base letters\n!style=\"width:1.5em\"|[[Æ]]\n!style=\"width:1.5em\"|[[Ɑ]]\n!style=\"width:1.5em\"|[[Ꞵ]]\n!style=\"width:1.5em\"|[[Eth (letter)|Ð]]\n!style=\"width:1.5em\"|[[Ǝ]]\n!style=\"width:1.5em\"|[[Ə]]\n!style=\"width:1.5em\"|[[Ɛ]]\n!style=\"width:1.5em\"|[[Ɣ]]\n!style=\"width:1.5em\"|I\n!style=\"width:1.5em\"|[[Ɩ]]\n!style=\"width:1.5em\"|[[Eng (letter)|Ŋ]]\n!style=\"width:1.5em\"|[[Œ]]\n!style=\"width:1.5em\"|[[Ɔ]]\n!style=\"width:1.5em\"|[[Latin omega| Ꞷ]]\n!style=\"width:1.5em\"|[[Ʊ]]\n!style=\"width:1.5em\"|K’\n!style=\"width:1.5em\"|[[Capital ß|ẞ]] \n!style=\"width:1.5em\"|[[Esh (letter)|Ʃ]]\n!style=\"width:1.5em\"|[[Thorn (letter)|Þ]]\n!style=\"width:1.5em\"|[[Ʋ]]\n!style=\"width:1.5em\"|[[Wynn|Ƿ]]\n!style=\"width:1.5em\"|[[Yogh|Ȝ]]\n!style=\"width:1.5em\"|[[Ezh|Ʒ]]\n!style=\"width:1.5em\"|[[ʔ]]\n|-\n!æ!!ɑ!!ꞵ!!ð!!ǝ!!ə!!ɛ!!ɣ!![[ı]]!!ɩ!!ŋ!!œ!!ɔ!!ꞷ!!ʊ!![[ĸ]]!![[ß]]!!ʃ!!þ!!ʋ!!ƿ!!ȝ!!ʒ!!ʔ\n|-\n![[Old English Latin alphabet|Anglo-Saxon]]\n|Æ|| || ||Ð|| || || || || || || ||Œ|| || || || ||     ||     ||Þ|| ||Ƿ||Ȝ|| ||\n|-\n![[Azeri alphabet|Azeri]]<sup style=\"font-size:80%\" id=\"r-az\">[[#n-az|[53]]]</sup>\n| || || || || ||Ə|| || || || || || || || || || ||     ||     || || || || || ||\n|-\n![[Bambara language|Bambara]]<sup style=\"font-size:80%\" id=\"r-bm\">[[#n-bm|[39]]]</sup>\n| || || || || || ||Ɛ|| || || ||Ŋ|| ||Ɔ|| || ||     ||     || || || || || || ||\n|-\n![[Berber Latin alphabet|Berber]]\n| || || || || || ||Ɛ||Ɣ\n|  || || || || || || || ||     ||     || || || || || ||\n|-\n![[Crimean Tatar language#Latin alphabet|Crimean Tatar]]\n| || || || || || || || ||ı|| || || || || || || ||     ||     || || || || || ||\n|-\n![[Dalecarlian alphabet|Dalecarlian]]\n| || || ||Ð|| || || || || || || || || || || || ||     ||     || || || || || ||\n|-\n![[Danish alphabet|Danish]]<sup style=\"font-size:80%\" id=\"r-da:0\">[[#n-da|[9]]]</sup><br /><!--\n-->[[Norwegian alphabet|Norwegian]]<sup style=\"font-size:80%\" id=\"r-no:0\">[[#n-da|[9]]]</sup><br /><!--\n-->[[Southern Sami language|Southern&nbsp;Sami&nbsp;(Norway)]]\n|Æ|| || || || || || || || || || || || || || || ||     ||     || || || || || ||\n|-\n![[Dinka language|Dinka]]\n| || || || || || ||Ɛ||Ɣ|| || ||Ŋ|| ||Ɔ|| || ||     ||     || || || || || || ||\n|-\n![[Faroese alphabet|Faroese]]\n|Æ|| || ||Ð|| || || || || || || || || || || || ||     ||     || || || || || ||\n|-\n![[Kalaallisut language|Greenlandic]]\n|Æ|| || || || || || || || || || || || || || ||ĸ||     ||     || || || || || ||\n|-\n![[German alphabet|German]]<sup style=\"font-size:80%\" id=\"r-de:0\">[[#n-de|[13]]]</sup><br /><!--\n-->[[Luxembourgish language|Luxembourgish]]\n| || || ||[[Pubi]]\n|  || || || || || || || || || || || ||ß    ||     || || || || || ||\n|-\n![[Icelandic orthography|Icelandic]]<br /><!--\n-->[[Norn language|Norn]]\n|Æ|| || ||Ð|| || || || || || || || || || || || ||     ||     ||Þ|| || || || ||\n|-\n![[British language (Celtic)|Celtic British]]<br /><!--\n-->[[English alphabet|English]]<sup style=\"font-size:80%\" id=\"r-en:0\">[[#n-en|[36]]]</sup><br /><!--\n-->[[French alphabet|French]]<sup style=\"font-size:80%\" id=\"r-fr:0\">[[#n-fr|[12]]]</sup><br /><!--\n-->[[Latin alphabet|Latin]]<sup style=\"font-size:80%\" id=\"r-la:0\">[[#n-la|[2]]]</sup>\n|Æ|| || || || || || || || || || ||Œ|| || || || ||     ||     || || || || || ||\n|-\n![[Inari Sami language|Inari Sami]]<br /><!--\n-->[[Northern Sami]]<br /><!--\n-->[[Lule Sami language|Lule Sami]]<sup style=\"font-size:80%\" id=\"r-ls\">[[#n-ls|[60]]]</sup><br /><!--\n--><!--[[Ume Sami language|Ume Sami]]<br />\n-->[[Fula alphabets|Fula]]<sup style=\"font-size:80%\" id=\"r-ff\">[[#n-ff|[41]]]</sup>\n| || || || || || || || || || ||Ŋ|| || || || || ||     ||     || || || || || ||\n|-\n![[Skolt Sami language|Skolt Sami]]\n| || || || || || || || || || ||Ŋ|| || || || || ||     ||     || || || || ||Ʒ||\n|-\n![[Pan-Nigerian alphabet|Pan-Nigerian]]\n| || || || ||Ǝ|| || || || || || || || || || || ||     ||     || || || || || ||\n|-\n![[Turkish alphabet|Turkish]]\n| || || || || || || || ||ı|| || || || || || || ||     ||     || || || || || ||\n|-\n![[General Alphabet of Cameroon Languages|Alphabet of Cameroon]]\n|Æ||Ɑ|| || || ||Ə||Ɛ|| || || ||Ŋ||Œ||Ɔ|| || || ||     ||     || || || || || ||\n|-\n![[National Languages Alphabet of Benin|Alphabet of Benin]]\n| || || || ||Ǝ|| ||Ɛ||Ɣ|| || ||Ŋ|| ||Ɔ|| ||Ʊ|| ||     ||     || ||Ʋ|| || || ||\n|-\n![[National Alphabet of Burkina Faso|Alphabet of Burkina Faso]]\n| || || || ||Ǝ|| ||Ɛ|| || ||Ɩ||Ŋ|| ||Ɔ|| || || ||     ||     || ||Ʋ|| || || ||\n|-\n![[National Alphabet of Chad|Alphabet of Chad]]\n| || || || || ||Ə||Ɛ|| || || ||Ŋ|| ||Ɔ|| || || ||     ||     || || || || || ||\n|-\n![[Alphabet of Côte d'Ivoire]]\n| || || || || || ||Ɛ|| || ||Ɩ||Ŋ|| ||Ɔ|| ||Ʊ|| ||     ||     || || || || || ||Ɂ\n|-\n![[Scientific Alphabet of Gabon]]\n| || ||Ꞵ||Ð||Ǝ|| ||Ɛ||Ɣ|| || ||Ŋ|| ||Ɔ|| || || ||     ||Ʃ    || || || || ||Ʒ||Ɂ\n|-\n![[National Languages Alphabet and Orthography of Mali|Alphabet of Mali]]\n| || || || ||Ǝ|| ||Ɛ||Ɣ|| || ||Ŋ|| ||Ɔ|| || || ||     ||     || || || || || ||Ɂ\n|-\n![[Alphabet of Niger]]\n| || || || ||Ǝ|| || ||Ɣ|| || ||Ŋ|| || || || || ||     ||     || || || || || ||\n|-\n![[Alphabet of Zaïre]]\n| || || || || || ||Ɛ|| || || || || ||Ɔ|| || || ||     ||     || || || || || ||\n|-\n![[African reference alphabet]]\n| ||Ɑ|| || ||Ǝ|| ||Ɛ||Ɣ|| ||Ɩ||Ŋ|| ||Ɔ||Ꞷ|| || ||     ||Ʃ    || ||Ʋ|| || ||Ʒ||Ɂ\n|}\n\n====Letter–diacritic combinations: connected or overlaid====\n{| cellspacing=\"0\" cellpadding=\"1\" class=\"wikitable\" style=\"border:1px solid #aaa;border-collapse:collapse;line-height:1.4;text-align:center;table-layout:fixed\"\n|-\n!style=\"width:10em\" rowspan=\"2\"|Modified letters\n!style=\"width:1.5em\"|[[Ą]]\n!style=\"width:1.5em\"|[[A̧]]\n!style=\"width:1.5em\"|[[Ą̊]]\n!style=\"width:1.5em\"|[[Ɓ]]\n!style=\"width:1.5em\"|[[Ç]]\n!style=\"width:1.5em\"|[[Đ]]\n!style=\"width:1.5em\"|[[Ɗ]]\n!style=\"width:1.5em\"|[[Ɖ]]\n!style=\"width:1.5em\"|[[Ę]]\n!style=\"width:1.5em\"|[[Ȩ]]\n!style=\"width:1.5em\"|[[Ə̧]]\n!style=\"width:1.5em\"|[[Ɛ̧]]\n!style=\"width:1.5em\"|[[Ƒ]]\n!style=\"width:1.5em\"|[[Ǥ]]\n!style=\"width:1.5em\"|[[Ɠ]]\n!style=\"width:1.5em\"|[[Ħ]]\n!style=\"width:1.5em\"|[[Ɦ]]\n!style=\"width:1.5em\"|[[Į]]\n!style=\"width:1.5em\"|[[I̧]]\n!style=\"width:1.5em\"|[[Ɨ]]\n!style=\"width:1.5em\"|[[Ɨ̧]]\n!style=\"width:1.5em\"|[[Ƙ]]\n!style=\"width:1.5em\"|[[Ł]]\n!style=\"width:1.5em\"|[[M̧]]\n!style=\"width:1.5em\"|[[Ɲ]]\n!style=\"width:1.5em\"|[[Ǫ]]\n!style=\"width:1.5em\"|[[O̧]]\n!style=\"width:1.5em\"|[[Ø]]\n!style=\"width:1.5em\"|[[Ơ]]\n!style=\"width:1.5em\"|[[Ɔ̧]]\n!style=\"width:1.5em\"|[[Ɍ]]\n!style=\"width:1.5em\"|[[Ş]]\n!style=\"width:1.5em\"|[[Ţ]]\n!style=\"width:1.5em\"|[[Ŧ]]\n!style=\"width:1.5em\"|[[Ų]]\n!style=\"width:1.5em\"|[[U̧]]\n!style=\"width:1.5em\"|[[Ư]]\n!style=\"width:1.5em\"|[[Ʉ]]\n!style=\"width:1.5em\"|[[Y̨]]\n!style=\"width:1.5em\"|[[Ƴ]]\n|-\n!ą!!a̧!!ą̊!!ɓ!!ç!!đ!!ɗ!!ɖ!!ę!!ȩ!!ə̧!!ɛ̧!!ƒ!!ǥ!!ɠ!!ħ!!ɦ!!į!!i̧!!ɨ!!ɨ̧!!ƙ!!ł!!m̧!!ɲ!!ǫ!!o̧!!ø!!ơ!!ɔ̧!!ɍ!!ş!!ţ!!ŧ!!ų!!u̧||ư!!ʉ!!y̨!!ƴ\n|-\n![[Zuni language#Writing system|Zuni]]<sup style=\"font-size:80%\" id=\"r-zun\">[[#n-zun|[66]]]</sup>\n| || || || || || || || || || || || || || || || || || || || || || ||Ł|| || || || || || || || || || || || || || || || ||\n|-\n![[Hän language|Hän]]<br /><!--\n-->[[Navajo language|Navajo]]\n|Ą|| || || || || || || ||Ę|| || || || || || || || ||Į|| || || || ||Ł|| || ||Ǫ|| || || || || || || || || || || || || ||\n|-\n![[Lithuanian alphabet|Lithuanian]]<sup style=\"font-size:80%\" id=\"r-lt:0\">[[#n-lt|[19]]]</sup>\n|Ą|| || || || || || || ||Ę|| || || || || || || || ||Į|| || || || || || || || || || || || || || || || ||Ų|| || || || ||\n|-\n![[Gwich'in language|Gwich'in]]\n|Ą|| || || || || || || ||Ę|| || || || || || || || ||Į|| || || || ||Ł|| || ||Ǫ|| || || || || || || || ||Ų|| || || || ||\n|-\n![[Dalecarlian alphabet|Dalecarlian]]\n|Ą|| || || || || || || ||Ę|| || || || || || || || ||Į|| || || || || || || || || || || || || || || || ||Ų|| || || ||Y̨||\n|-\n![[Kashubian alphabet|Kashubian]]<br /><!--\n-->[[Polish alphabet|Polish]]<sup style=\"font-size:80%\" id=\"r-pl:0\">[[#n-pl|[22]]]</sup>\n|Ą|| || || || || || || ||Ę|| || || || || || || || || || || || || ||Ł|| || || || || || || || || || || || || || || || ||\n|-\n![[Pan-Nigerian alphabet|Pan-Nigerian]]\n| || || ||Ɓ|| || ||Ɗ|| || || || || || || || || || || || || || ||Ƙ|| || || || || || || || || || || || || || || || || ||\n|-\n![[Bambara language|Bambara]]<sup style=\"font-size:80%\" id=\"r-bm\">[[#n-bm|[39]]]</sup><br /><!--\n-->[[Dinka alphabet|Dinka]]<sup style=\"font-size:80%\" id=\"r-din\">[[#n-din|[40]]]</sup>\n| || || || || || || || || || || || || || || || || || || || || || || || ||Ɲ|| || || || || || || || || || || || || || ||\n|-\n![[Marshallese language|Marshallese]]<sup style=\"font-size:80%\" id=\"r-mh:0\">[[#n-mh|[47]]]</sup>\n| || || || || || || || || || || || || || || || || || || || || || || ||M̧|| || ||O̧|| || || || || || || || || || || || ||\n|-\n![[Fula alphabets|Fula]]<sup style=\"font-size:80%\" id=\"r-ff\">[[#n-ff|[41]]]</sup>\n| || || ||Ɓ|| || ||Ɗ|| || || || || || || ||Ɠ|| || || || || || || || || ||Ɲ|| || || || || || || || || || || || || || ||Ƴ\n|-\n![[Hausa language|Hausa]]<sup style=\"font-size:80%\" id=\"r-ha:0\">[[#n-ha|[30]]]</sup>\n| || || ||Ɓ|| || ||Ɗ||  || || || || || || || || || || || || || ||Ƙ|| || || || || || || || || || || || || || || || || ||Ƴ\n|-\n![[General Alphabet of Cameroon Languages|Alphabet of Cameroon]]\n| ||A̧|| ||Ɓ|| || ||Ɗ|| || ||Ȩ||Ə̧||Ɛ̧|| || || || || || ||I̧||Ɨ||Ɨ̧|| || || || || ||O̧||Ø|| ||Ɔ̧|| || || || || ||U̧|| || || ||Ƴ\n|-\n![[National Languages Alphabet of Benin|Alphabet of Benin]]\n| || || || || || || ||Ɖ|| || || || ||Ƒ|| || || || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[National Alphabet of Burkina Faso|Alphabet of Burkina Faso]]\n| || || ||Ɓ||Ç|| ||Ɗ|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ƴ\n|-\n![[National Alphabet of Chad|Alphabet of Chad]]\n| || || ||Ɓ|| || ||Ɗ|| || || || || || || || || ||Ɦ|| || ||Ɨ|| || || || || || || || || || || || || || || || || || || ||Ƴ\n|-\n![[Scientific Alphabet of Gabon]]\n| || || || || || || ||Ɖ|| || || || || || || || || || || || || || || || || || || || || || ||Ɍ|| || || || || || || || ||\n|-\n![[National Languages Alphabet and Orthography of Mali|Alphabet of Mali]]\n| || || ||Ɓ|| || ||Ɗ|| || || || || || || || || || || || || || || || || ||Ɲ|| || || || || || || || || || || || || || ||Ƴ\n|-\n![[Alphabet of Niger]]\n| || || ||Ɓ|| || ||Ɗ|| || || || || || || || || || || || || || ||Ƙ|| || ||Ɲ|| || || || || ||Ɍ|| || || || || || || || ||Ƴ\n|-\n![[Albanian alphabet|Albanian]]<sup style=\"font-size:80%\" id=\"r-sq:0\">[[#n-sq|[3]]]</sup><br /><!--\n-->[[Arbëresh language|Arbëresh]]<br /><!--\n-->[[Basque alphabet|Basque]],<sup style=\"font-size:80%\" id=\"r-eu\">[[#n-eu|[4]]]</sup> <!--\n-->[[Catalan alphabet|Catalan]]<sup style=\"font-size:80%\" id=\"r-ca:0\">[[#n-ca|[6]]]</sup><br /><!--\n-->[[English alphabet|English]]<sup style=\"font-size:80%\" id=\"r-en:0\">[[#n-en|[36]]]</sup><br /><!--\n-->[[Extremaduran language|Extremaduran]]<br /><!--\n-->[[Fala language|Fala]]<br /><!--\n-->[[French alphabet|French]]<sup style=\"font-size:80%\" id=\"r-fr:0\">[[#n-fr|[12]]]</sup><br /><!--\n-->[[Friulian language|Friulian]]<br /><!--\n-->[[German alphabet|German]]<sup style=\"font-size:80%\" id=\"r-de:2\">[[#n-de|[13]]]</sup><br /><!--\n-->[[Manx Gaelic]]<br /><!--\n-->[[Mirandese language|Mirandese]]<br /><!--\n-->[[Norwegian alphabet|Norwegian]]<sup style=\"font-size:80%\" id=\"r-da\">[[#n-da|[9]]]</sup><br /><!--\n-->[[Occitan alphabet|Occitan]]<br /><!--\n-->[[Portuguese alphabet|Portuguese]]<sup style=\"font-size:80%\" id=\"r-pt:0\">[[#n-pt|[23]]]</sup><br /><!--\n-->[[Spanish alphabet|Spanish]]<sup style=\"font-size:80%\" id=\"r-es:1\">[[#n-es|[25]]]</sup><br /><!--\n-->[[Walloon language|Walloon]]<sup style=\"font-size:80%\" id=\"r-wa:0\">[[#n-wa|[27]]]</sup>\n| || || || ||Ç|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Azeri alphabet|Azeri]]<sup style=\"font-size:79%\" id=\"r-az:0\">[[#n-az|[53]]]</sup><br /><!--\n-->[[Crimean Tatar language|Crimean Tatar]]<br /><!--\n-->[[Kurdish alphabet|Kurdish]]<br /><!--\n-->[[Turkish alphabet|Turkish]]<br /><!--\n-->[[Turkmen alphabet|Turkmen]]<sup style=\"font-size:80%\" id=\"r-tk:0\">[[#n-tk|[55]]]</sup>\n| || || || ||Ç|| || || || || || || || || || || || || || || || || || || || || || || || || || ||Ş|| || || || || || || ||\n|-\n![[Gagauz language#Latin alphabet|Gagauz]]\n| || || || ||Ç|| || || || || || || || || || || || || || || || || || || || || || || || || || ||Ş||Ţ|| || || || || || ||\n|-\n![[Croatian alphabet|Croatian]]<sup style=\"font-size:80%\" id=\"r-hr:0\">[[#n-hr|[7]]]</sup><br /><!--\n-->[[Inari Sami language|Inari Sami]]<br /><!--\n-->[[Slovenian alphabet|Slovenian]]\n| || || || || ||Đ|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Northern Sami]]<br /><!--\n-->[[Ume Sami language|Ume Sami]]\n| || || || || ||Đ|| || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ŧ|| || || || || ||\n|-\n![[Skolt Sami language|Skolt Sami]]\n| || || || || ||Đ|| || || || || || || ||Ǥ|| || || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Vietnamese alphabet|Vietnamese]]<sup style=\"font-size:80%\" id=\"r-vi:0\">[[#n-vi|[26]]]</sup>\n| || || || || ||Đ|| || || || || || || || || || || || || || || || || || || || || || ||Ơ|| || || || || || || ||Ư|| || ||\n|-\n![[Maltese alphabet|Maltese]]<sup style=\"font-size:80%\" id=\"r-mt:0\">[[#n-mt|[20]]]</sup>\n| || || || || || || || || || || || || || || ||Ħ|| || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Tunisian Arabic#Elyssa Alphabet|Tunisian Arabic]]<sup style=\"font-size:80%\" id=\"r-aeb:0\">[[#n-aeb|[58]]</sup>\n| || || || || ||Đ|| || || || || || || || || ||Ħ|| || || || || || || || || || || || || || ||Ɍ|| || ||Ŧ|| || || || || ||\n|-\n![[Cypriot Arabic#Writing System|Cypriot Arabic]]<sup style=\"font-size:80%\" id=\"r-acy:0\">[[#n-acy|[59]]]</sup>\n| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ş|| || || || || || || ||\n|-\n![[Belarusian Latin alphabet|Belarusian]]\n<sup style=\"font-size:80%\" id=\"r-be:0\">[[#n-be|[5]]]</sup><br /><!--\n-->[[Sorbian alphabet|Sorbian]]<sup style=\"font-size:80%\" id=\"r-wen:1\">[[#n-wen|[64]]]</sup>\n| || || || || || || || || || || || || || || || || || || || || || ||Ł|| || || || || || || || || || || || || || || || ||\n|-\n![[Danish alphabet|Danish]]<sup style=\"font-size:80%\" id=\"r-da:0\">[[#n-da|[9]]]</sup><br /><!--\n-->[[Faroese alphabet|Faroese]]<br /><!--\n-->[[Kalaallisut language|Greenlandic]]<br /><!--\n-->[[Norn language|Norn]]<br /><!--\n-->[[Norwegian alphabet|Norwegian]]<sup style=\"font-size:80%\" id=\"r-no:0\">[[#n-da|[9]]]</sup><br /><!--\n-->[[Southern Sami language|Southern&nbsp;Sami&nbsp;(Norway)]]\n| || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ø|| || || || || || || || || || || ||\n|-\n![[Romanian alphabet|Romanian]]<sup style=\"font-size:80%\" id=\"r-no:0\">[[#n-ro|[10]]]</sup>\n| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ş||Ţ|| || || || || || ||\n|}\n\n===Other letters in collation order===\nNote: The tables below are a work in progress. Eventually, table cells with light blue shading will indicate letter forms that do not constitute distinct letters in their associated alphabets. Please help with this task if you have the required linguistic knowledge and technical editing skill.\n\nFor the order in which the characters are sorted in each alphabet, see [[collating sequence]].\n\n====Letters derived from A–H====\n{| cellspacing=\"0\" cellpadding=\"1\" class=\"wikitable\" style=\"border:1px solid #aaa;border-collapse:collapse;line-height:1.4;text-align:center;table-layout:fixed\"\n|+Letter-diacritic combinations (detached) in various Latin alphabets (A–H)\n|-\n!style=\"width:10em\" rowspan=\"2\"|Alphabet\n!style=\"width:1.5em\"|[[Á]]\n!style=\"width:1.5em\"|[[À]]\n!style=\"width:1.5em\"|[[Ȧ]]\n!style=\"width:1.5em\"|[[Â]]\n!style=\"width:1.5em\"|[[Ä]]\n!style=\"width:1.5em\"|[[Ǟ]]\n!style=\"width:1.5em\"|[[Ǎ]]\n!style=\"width:1.5em\"|[[Ă]]\n!style=\"width:1.5em\"|[[Ā]]\n!style=\"width:1.5em\"|[[Ã]]\n!style=\"width:1.5em\"|[[Å]]\n!style=\"width:1.5em\"|[[Ǻ]]\n!style=\"width:1.5em\"|[[Ǽ]]\n!style=\"width:1.5em\"|[[Ǣ]]\n!style=\"width:1.5em\"|[[Ć]]\n!style=\"width:1.5em\"|[[Ċ]]\n!style=\"width:1.5em\"|[[Ĉ]]\n!style=\"width:1.5em\"|[[Č]]\n!style=\"width:1.5em\"|[[Ď]]\n!style=\"width:1.5em\"|[[Ḍ]]\n!style=\"width:1.5em\"|[[Ḑ]]\n!style=\"width:1.5em\"|[[Ḓ]]\n!style=\"width:1.5em\"|[[É]]\n!style=\"width:1.5em\"|[[È]]\n!style=\"width:1.5em\"|[[Ė]]\n!style=\"width:1.5em\"|[[Ê]]\n!style=\"width:1.5em\"|[[Ë]]\n!style=\"width:1.5em\"|[[Ě]]\n!style=\"width:1.5em\"|[[Ĕ]]\n!style=\"width:1.5em\"|[[Ē]]\n!style=\"width:1.5em\"|[[Ẽ]]\n!style=\"width:1.5em\"|[[E̊]]\n!style=\"width:1.5em\"|[[Ẹ]]\n!style=\"width:1.5em\"|[[Ǵ]]\n!style=\"width:1.5em\"|[[Ġ]]\n!style=\"width:1.5em\"|[[Ĝ]]\n!style=\"width:1.5em\"|[[Ǧ]]\n!style=\"width:1.5em\"|[[Ğ]]\n!style=\"width:1.5em\"|[[G̃]]\n!style=\"width:1.5em\"|[[Ģ]]\n!style=\"width:1.5em\"|[[Ĥ]]\n!style=\"width:1.5em\"|[[Dot (diacritic)|Ḥ]]\n|-\n!á!!à!!ȧ!!â!!ä!!ǟ!!ǎ!!ă!!ā!!ã!!å!!ǻ!!ǽ!!ǣ!!ć!!ċ!!ĉ!!č!!ď!!ḍ!!ḑ!!ḓ!!é!!è!!ė!!ê!!ë!!ě!!ĕ!!ē!!ẽ!!e̊!!ẹ!!ǵ!!ġ!!ĝ!!ǧ!!ğ!!g̃!!ģ!!ĥ!!ḥ\n|-\n![[Latin alphabet|Latin]]<sup style=\"font-size:80%\" id=\"r-la:1\">[[#n-la|[2]]]</sup>\n| || || || || || || ||Ă||Ā|| || || || || || || || || || || || || || || || || ||Ë|| ||Ĕ||Ē|| || || || || || || || || || || ||\n|-\n![[Afrikaans#Alphabet|Afrikaans]]<sup style=\"font-size:80%\" id=\"r-af:1\">[[#n-af|[54]]]</sup>\n|Á|| || || || || || || || || || || || || || || || || || || || || ||É||È|| ||Ê||Ë|| || || || || || || || || || || || || || ||\n|-\n![[Albanian alphabet|Albanian]]<sup style=\"font-size:80%\" id=\"r-sq:1\">[[#n-sq|[3]]]</sup>\n| || || ||Â|| || || || || || || || || || || || || || || || || || || || || ||Ê||Ë|| || || || || || || || || || || || || || ||\n|-\n![[Alemannic German|Alemannic]]\n|Á||À|| ||Â||Ä|| || || || || ||Å|| || || || || || || || || || || ||É||È|| ||Ê|| || || || || || || || || || || || || || || ||\n|-\n![[Old English language#Alphabet|Anglo-Saxon]]\n| || || || || || || || ||Ā|| || || || ||Ǣ|| || || || || || || || || || || || || || || ||Ē|| || || || || || || || || || || ||\n|-\n![[Arbëresh language|Arbëresh]]\n|Á|| || || || || || || || || || || || || || || || || || || || || ||É|| || || ||Ë|| || || || || || || || || || || || || || ||\n|-\n![[Aromanian language|Aromanian]]\n| || || || || || || || || ||Ã|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Asturian language|Asturian]]\n|Á|| || || || || || || || || || || || || || || || || || || || || ||É|| || || || || || || || || || || || || || || || || || ||Ḥ\n|-\n![[Austro-Bavarian German|Austro-Bavarian]]\n|Á||À|| ||Â||Ä|| || || || || ||Å|| || || || || || || || || || || ||É||È|| ||Ê|| || || || || || || || || || || || || || || ||\n|-\n![[Azeri alphabet|Azeri]]<sup style=\"font-size:80%\" id=\"r-az:1\">[[#n-az|[53]]]</sup>\n| || || || ||Ä|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ğ|| || || ||\n|-\n![[Belarusian Latin alphabet|Belarusian]]<sup style=\"font-size:80%\" id=\"r-be:1\">[[#n-be|[5]]]</sup>\n| || || || || || || || || || || || || || ||Ć|| || ||Č|| || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Berber Latin alphabet|Berber]]\n| || || || ||  || || || || || || || || || || || || ||Č|| ||Ḍ|| || || || || || || || || || || || || || || ||  ||Ǧ|| || || || ||Ḥ\n|-\n![[Bislama language|Bislama]]<sup style=\"font-size:80%\" id=\"r-bi:1\">[[#n-bi|[45]]]</sup>\n| || || || || || || || || || || || || || || || || || || || || || ||É|| || || || || || || || || || || || || || || || || || ||\n|-\n![[Breton language|Breton]]\n| || || ||Â|| || || || || || || || || || || || || || || || || || ||É|| || ||Ê|| || || || || || || || || || || || || || || ||\n|-\n![[Catalan alphabet|Catalan]]<sup style=\"font-size:80%\" id=\"r-ca:1\">[[#n-ca|[6]]]</sup>\n| ||À|| || || || || || || || || || || || || || || || || || || || ||É||È|| || || || || || || || || || || || || || || || || ||\n|-\n![[British language (Celtic)|Celtic British]]\n| || || || || || || ||Ă||Ā|| || || || || || || || || || || || || || || || || || || ||Ĕ||Ē|| || || || || || || || || || || ||\n|-\n![[Chamorro alphabet|Chamorro]]<sup style=\"font-size:80%\" id=\"r-ch\">[[#n-ch|[43]]]</sup>\n| || || || || || || || || || ||Å|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Corsican language|Corsican]]<sup style=\"font-size:80%\" id=\"r-co:1\">[[#n-co|[31]]]</sup>\n| ||À|| || || || || || || || || || || || || || || || || || || || || ||È|| || || || || || || || || || || || || || || || || ||\n|-\n![[Crimean Tatar language|Crimean Tatar]]\n| || || ||Â|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ğ|| || || ||\n|-\n![[Croatian alphabet|Croatian]]<sup style=\"font-size:80%\" id=\"r-hr:1\">[[#n-hr|[7]]]</sup>\n| || || || || || || || || || || || || || ||Ć|| || ||Č|| || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Cypriot Arabic#Writing System|Cypriot Arabic]]<sup style=\"font-size:80%\" id=\"r-acy:1\">[[#n-acy|[59]]]</sup>\n| || || || || || || || || || || || || || || ||Ċ|| || || || || || || || || || || || || || || || || || ||Ġ|| || || || || || ||\n|-\n![[Czech alphabet|Czech]]<sup style=\"font-size:80%\" id=\"r-cs:1\">[[#n-cs|[8]]]</sup>\n|Á|| || || || || || || || || || || || || || || || ||Č||Ď|| || || ||É|| || || || ||Ě|| || || || || || || || || || || || || ||\n|-\n![[Dalecarlian alphabet|Dalecarlian]]\n| || || || ||Ä|| || || || || ||Å|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Danish alphabet|Danish]]<sup style=\"font-size:80%\" id=\"r-da:1\">[[#n-da|[9]]]</sup>\n|Á|| || || || || || || || || ||Å||Ǻ||Ǽ|| || || || || || || || || ||É|| || || || || || || || || || || || || || || || || || ||\n|-\n![[Dutch alphabet|Dutch]]<sup style=\"font-size:80%\" id=\"r-nl:1\">[[#n-nl|[10]]]</sup>\n|Á||À|| ||Â||Ä|| || || || || || || || || || || || || || || || || ||É||È|| ||Ê||Ë|| || || || || || || || || || || || || || ||\n|-\n![[Emilian-Romagnol language|Emilian-Romagnol]]\n| ||À|| ||Â||Ä|| || || || || ||Å|| || || || || || || || || || || ||É||È|| ||Ê||Ë|| || ||Ē|| || || || || || || || || || || ||\n|-\n![[English alphabet|English]]<sup style=\"font-size:80%\" id=\"r-en:1\">[[#n-en|[36]]]</sup>\n| ||À|| ||Â||Ä|| || || || || ||Å|| || || || || || || || || || || ||É||È|| ||Ê||Ë|| || || || || || || || || || || || || || ||\n|-\n![[Esperanto alphabet|Esperanto]]\n| || || || || || || || || || || || || || || || ||Ĉ|| || || || || || || || || || || || || || || || || || ||Ĝ|| || || || ||Ĥ||\n|-\n![[Estonian alphabet|Estonian]]\n| || || || ||Ä|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Extremaduran language|Extremaduran]]\n|Á|| || || || || || || || || || || || || || || || || || || || || ||É|| || || || || || || || || || || || || || || || || || ||\n|-\n![[Fala language|Fala]]\n|Á|| || || || || || || || ||Ã|| || || || || || || || || || || || ||É|| || || || || || || ||Ẽ|| || || || || || || || || || ||\n|-\n![[Faroese alphabet|Faroese]]\n|Á|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Filipino alphabet|Filipino]]<sup style=\"font-size:80%\" id=\"r-tl:1\">[[#n-tl|[11]]]</sup>\n|Á||À|| ||Â|| || || || || || || || || || || || || || || || || || ||É||È|| ||Ê|| || || || || || || || || || || || ||G̃|| || ||\n|-\n![[Finnish alphabet|Finnish]]\n| || || || ||Ä|| || || || || ||Å|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[French alphabet|French]]<sup style=\"font-size:80%\" id=\"r-fr:1\">[[#n-fr|[12]]]</sup>\n| ||À|| ||Â|| || || || || || || || || || || || || || || || || || ||É||È|| ||Ê||Ë|| || || || || || || || || || || || || || ||\n|-\n![[Friulian language|Friulian]]\n| ||À|| ||Â|| || || || || || || || || || || || || || || || || || || ||È|| ||Ê|| || || || || || || || || || || || || || || ||\n|-\n![[Gagauz language#Latin alphabet|Gagauz]]\n| || || || ||Ä|| || || || || || || || || || || || || || || || || || || || ||Ê|| || || || || || || || || || || || || || || ||\n|-\n![[Galician language|Galician]]<sup style=\"font-size:80%\" id=\"r-gl\">[[#n-gl|[33]]]</sup>\n|Á|| || || || || || || || || || || || || || || || || || || || || ||É|| || || || || || || || || || || || || || || || || || ||\n|-\n![[German alphabet|German]]<sup style=\"font-size:80%\" id=\"r-de:1\">[[#n-de|[13]]]</sup>\n|Á||À|| ||Â||Ä|| || || || || || || || || || || || || || || || || ||É||È|| ||Ê|| || || || || || || || || || || || || || || ||\n|-\n![[Greenlandic language|Greenlandic]]\n|Á|| || ||Â|| || || || || ||Ã||Å|| || || || || || || || || || || ||É|| || ||Ê|| || || || || || || || || || || || || || || ||\n|-\n![[Guaraní alphabet|Guaraní]]<sup style=\"font-size:80%\" id=\"r-gn:1\">[[#n-gn|[14]]]</sup>\n|Á|| || || || || || || || ||Ã|| || || || || || || || || || || || ||É|| || || || || || || ||Ẽ|| || || || || || || ||G̃|| || ||\n|-\n![[Gwich'in language|Gwich'in]]\n| ||À|| || || || || || || || || || || || || || || || || || || || || ||È|| || || || || || || || || || || || || || || || || ||\n|-\n![[Haitian Creole|Haitian]]\n| ||À|| || || || || || || || || || || || || || || || || || || || || ||È|| || || || || || || || || || || || || || || || || ||\n|-\n![[Hän language|Hän]]\n| ||À|| ||Â||Ä|| ||Ǎ|| || || || || || || || || || || || || || || || ||È|| ||Ê||Ë||Ě|| || || || || || || || || || || || || ||\n|-\n![[Hawaiian alphabet|Hawaiian]]\n| || || || || || || || ||Ā|| || || || || || || || || || || || || || || || || || || || ||Ē|| || || || || || || || || || || ||\n|-\n![[Hungarian alphabet|Hungarian]]<sup style=\"font-size:80%\" id=\"r-hu:1\">[[#n-hu|[15]]]</sup>\n|Á|| || || || || || || || || || || || || || || || || || || || || ||É|| || || || || || || || || || || || || || || || || || ||\n|-\n![[Icelandic orthography|Icelandic]]\n|Á|| || || || || || || || || || || || || || || || || || || || || ||É|| || || || || || || || || || || || || || || || || || ||\n|-\n![[Igbo language#Writing system|Igbo]]\n|Á||À|| || || || || || || || || || || || || || || || || || || || ||É||È|| || || || || || || || || || || || || || || || || ||\n|-\n![[Inari Sami language|Inari Sami]]\n|Á|| || ||Â||Ä|| || || || || ||Å|| || || || || || ||Č|| || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Irish orthography|Irish]]<sup style=\"font-size:80%\" id=\"r-ga:1\">[[#n-ga|[16]]]</sup>\n|Á|| || || || || || || || || || || || || || || || || || || || || ||É|| || || || || || || || || || || || || || || || || || ||\n|-\n![[Italian alphabet|Italian]]<sup style=\"font-size:80%\" id=\"r-it:1\">[[#n-it|[17]]]</sup>\n|Á||À|| || || || || || || || || || || || || || || || || || || || ||É||È|| || || || || || || || || || || || || || || || || ||\n|-\n![[Javanese Latin alphabet|Javanese]]\n| || || || || || || || || || || || || || || || || || || || || || ||É||È|| || || || || || || || || || || || || || || || || ||\n|-\n![[Kashubian alphabet|Kashubian]]\n| || || || || || || || || ||Ã|| || || || ||Ć|| || || || || || || ||É|| || || ||Ë|| || || || || || || || || || || || || || ||\n|-\n![[Kazakh alphabets|Kazakh]]<sup style=\"font-size:80%\" id=\"r-kz:0\">[[#n-kz|[38]]]</sup>\n|Á|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ǵ|| || || || || || || ||\n|-\n![[Kurdish alphabet|Kurdish]]\n| || || || || || || || || || || || || || || || || || || || || || || || || ||Ê|| || || || || || || || || || || || || || || ||\n|-\n![[Kurdish alphabet|Kurdish (IS)]]\n| || || || || || || || || || || || || || || || || || || || || || ||É|| || || || || || || || || || || || || || || || || || ||\n|-\n![[Latvian alphabet|Latvian]]<sup style=\"font-size:80%\" id=\"r-lv:1\">[[#n-lv|[18]]]</sup>\n| || || || || || || || ||Ā|| || || || || || || || ||Č|| || || || || || || || || || || ||Ē|| || || || || || || || || ||Ģ|| ||\n|-\n![[Lithuanian alphabet|Lithuanian]]<sup style=\"font-size:80%\" id=\"r-lt:1\">[[#n-lt|[19]]]</sup>\n| || || || || || || || || || || || || || || || || ||Č|| || || || || || ||Ė|| || || || || || || || || || || || || || || || ||\n|-\n![[Livonian language#Alphabet|Livonian]]<sup style=\"font-size:80%\" id=\"r-liv:1\">[[#n-liv:1|[46]]]</sup>\n| || || || ||Ä||Ǟ|| || ||Ā|| || || || || || || || || || || ||Ḑ|| || || || || || || || ||Ē|| || || || || || || || || || || ||\n|-\n![[Lule Sami language|Lule Sami]]<sup style=\"font-size:80%\" id=\"r-ls\">[[#n-ls|[60]]]</sup>\n|Á|| || || ||Ä|| || || || || ||Å|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Luxembourgish alphabet|Luxembourgish]]\n| || || ||Â||Ä|| || || || || || || || || || || || || || || || || ||É||È|| ||Ê||Ë|| || || || || || || || || || || || || || ||\n|-\n![[Malagasy language|Malagasy]]\n|Á||À|| ||Â|| || || || || || || || || || || || || || || || || || || ||È|| ||Ê|| || || || || || || || || || || || || || || ||\n|-\n![[Maltese alphabet|Maltese]]<sup style=\"font-size:80%\" id=\"r-mt:1\">[[#n-mt|[20]]]</sup>\n| ||À|| || || || || || || || || || || || || ||Ċ|| || || || || || || ||È|| || || || || || || || || || ||Ġ|| || || || || || ||\n|-\n![[Māori language|Māori]]\n| || || || || || || || ||Ā|| || || || || || || || || || || || || || || || || || || || ||Ē|| || || || || || || || || || || ||\n|-\n![[Marshallese language|Marshallese]]<sup style=\"font-size:80%\" id=\"r-mh:1\">[[#n-mh|[47]]]</sup>\n| || || || || || || || ||Ā|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Massachusett writing systems|Massachusett]]<sup style=\"font-size:80%\" id=\"r-mss:1\">[[#n-mss|[62]]]</sup>\n| || || ||Â|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Mirandese language|Mirandese]]\n|Á|| || || || || || || || || || || || || || || || || || || || || ||É|| || ||Ê|| || || || || || || || || || || || || || || ||\n|-\n![[Mohawk language|Mohawk]]\n|Á||À|| || || || || || || || || || || || || || || || || || || || ||É||È|| || || || || || || || || || || || || || || || || ||\n|-\t\n![[Na'vi language#Phonology and orthography|Na'vi]]<sup style=\"font-size:80%\" id=\"r-mis:1\">[[#n-mis|[57]]]</sup>\n| || || || ||Ä|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Navajo language|Navajo]]\n|Á|| || || || || || || || || || || || || || || || || || || || || ||É|| || || || || || || || || || || || || || || || || || ||\n|-\n![[Norn language|Norn]]\n|Á|| || || || || || || || || ||Å|| || || || || || || || || || || ||É|| || || || || || || || || || || || || || || || || || ||\n|-\n![[Northern Sami]]\n|Á|| || || || || || || || || || || || || || || || ||Č|| || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Norwegian alphabet|Norwegian]]<sup style=\"font-size:80%\" id=\"r-no:1\">[[#n-da|[9]]]</sup>\n|Á||À|| ||Â||Ä|| || || || || ||Å|| || || || || || || || || || || ||É||È|| ||Ê||Ë|| || || || || || || || || || || || || || ||\n|-\n![[Occitan alphabet|Occitan]]\n|Á||À|| || || || || || || || || || || || || || || || || || || || ||É||È|| || || || || || || || || || || || || || || || || ||\n|-\n![[Pan-Nigerian alphabet|Pan-Nigerian]]\n| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ẹ|| || || || || || || || ||\n|-\n![[Papiamento orthography|Papiamento]]<sup style=\"font-size:80%\" id=\"r-pap:1\">[[#n-pap|[63]]]</sup>\n|Á|| || || || || || || || || || || || || || || || || || || || || ||É||È|| || || || || || || || || || || || || || || || || ||\n|-\n![[Piedmontese language|Piedmontese]]<sup style=\"font-size:80%\" id=\"r-pms:1\">[[#n-pms|[37]]]</sup>\n| ||À|| || || || || || || || || || || || || || || || || || || || ||É||È|| || ||Ë|| || || || || || || || || || || || || || ||\n|-\n![[Pinyin]]<sup style=\"font-size:80%\" id=\"r-pinyin:1\">[[#n-pinyin|[32]]]</sup>\n|Á||À|| || || || ||Ǎ|| ||Ā|| || || || || || || || || || || || || ||É||È|| || || ||Ě|| ||Ē|| || || || || || || || || || || ||\n|-\n![[Polish alphabet|Polish]]<sup style=\"font-size:80%\" id=\"r-pl:1\">[[#n-pl|[22]]]</sup>\n| || || || || || || || || || || || || || ||Ć|| || || || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Portuguese alphabet|Portuguese]]<sup style=\"font-size:80%\" id=\"r-pt:1\">[[#n-pt|[23]]]</sup>\n|Á||À|| ||Â|| || || || || ||Ã|| || || || || || || || || || || || ||É||È|| ||Ê|| || || || || || || || || || || || || || || ||\n|-\n![[Rhaeto-Romance languages|Rhaeto-Romance]]\n| ||À|| || || || || || || || || || || || || || || || || || || || ||É||È|| || || || || || || || || || || || || || || || || ||\n|-\n![[Romani writing systems|Romani]]<sup style=\"font-size:80%\" id=\"r-rom:1\">[[#n-rom|[29]]]</sup>\n| || || || || || || || || || || || || || || || || ||Č|| || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Romanian alphabet|Romanian]]\n| || || ||Â|| || || ||Ă|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Samoan language#Alphabet|Samoan]]\n|Á|| || || || || || || ||Ā|| || || || || || || || || || || || || ||É|| || || || || || ||Ē|| || || || || || || || || || || ||\n|-\n![[Sardinian language|Sardinian]]\n|Á||À|| || || || || || || || || || || || || || || || || || || || ||É||È|| || || || || || || || || || || || || || || || || ||\n|-\n![[Scottish Gaelic alphabet|Scots Gaelic]]\n|Á||À|| || || || || || || || || || || || || || || || || || || || ||É||È|| || || || || || || || || || || || || || || || || ||\n|-\n![[Serbian alphabet|Serbian]]<sup style=\"font-size:80%\" id=\"r-sr:1\">[[#n-hr|[7]]]</sup>\n| || || || || || || || || || || || || || ||Ć|| || ||Č|| || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Sicilian language|Sicilian]]\n| ||À|| ||Â|| || || || || || || || || || || || || || || || || || || ||È|| ||Ê|| || || || || || || || || || || || || || || ||\n|-\n![[Skolt Sami language|Skolt Sami]]\n| || || ||Â||Ä|| || || || || ||Å|| || || || || || ||Č|| || || || || || || || || || || || || || || || || || ||Ǧ|| || || || ||\n|-\n![[Slovak alphabet|Slovak]]<sup style=\"font-size:80%\" id=\"r-sk:1\">[[#n-sk|[24]]]</sup>\n|Á|| || || ||Ä|| || || || || || || || || || || || ||Č||Ď|| || || ||É|| || || || ||Ě|| || || || || || || || || || || || || ||\n|-\n![[Slovenian alphabet|Slovenian]]\n|Á||À|| || ||Ä|| || || || || || || || || ||Ć|| || ||Č|| || || || ||É||È|| ||Ê|| || || || || || || || || || || || || || || ||\n|-\n![[Sorbian alphabet|Sorbian]]<sup style=\"font-size:80%\" id=\"r-wen:2\">[[#n-wen|[64]]]</sup>\n| || || || || || || || || || || || || || ||Ć|| || ||Č|| || || || || || || || || ||Ě|| || || || || || || || || || || || || ||\n|-\n![[Southern Sami language|Southern Sami (Norway)]]\n| || || || || || || || || || ||Å|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Southern Sami language|Southern Sami (Sweden)]]\n| || || || ||Ä|| || || || || ||Å|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Spanish alphabet|Spanish]]<sup style=\"font-size:80%\" id=\"r-es:1\">[[#n-es|[25]]]</sup>\n|Á|| || || || || || || || || || || || || || || || || || || || || ||É|| || || || || || || || || || || || || || || || || || ||\n|-\n![[Sundanese language|Sundanese]]\n| || || || || || || || || || || || || || || || || || || || || || ||É|| || || || || || || || || || || || || || || || || || ||\n|-\n![[Swedish alphabet|Swedish]]<sup style=\"font-size:80%\" id=\"r-sv\">[[#n-sv|[21]]]</sup>\n|Á||À|| || ||Ä|| || || || || ||Å|| || || || || || || || || || || ||É||È|| || ||Ë|| || || || || || || || || || || || || || ||\n|-\n![[Tahitian language#Phonology|Tahitian]]\n| || || || || || || || ||Ā|| || || || || || || || || || || || || || || || || || || || ||Ē|| || || || || || || || || || || ||\n|-\n![[Tetum alphabet|Tetum]]\n|Á|| || || || || || || || || || || || || || || || || || || || || ||É|| || || || || || || || || || || || || || || || || || ||\n|-\n![[Tongan language#Tongan alphabet|Tongan]]\n|Á|| || || || || || || ||Ā|| || || || || || || || || || || || || ||É|| || || || || || ||Ē|| || || || || || || || || || || ||\n|-\n![[Tswana language|Tswana]]<sup style=\"font-size:80%\" id=\"r-tn:1\">[[#n-tn|[52]]]</sup>\n| || || || || || || || || || || || || || || || || || || || || || || || || ||Ê|| || || || || || || || || || || || || || || ||\n|-\n![[Tunisian Arabic#Elyssa Alphabet|Tunisian Arabic]]<sup style=\"font-size:80%\" id=\"r-aeb:1\">[[#n-aeb|[58]]</sup>\n| ||À|| || || || || || || || || || || || || ||Ċ|| || || || || || || ||È|| || || || || || || || || || ||Ġ|| || || || || || ||\n|-\n![[Turkish alphabet|Turkish]]\n| || || ||Â|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ğ|| || || ||\n|-\n![[Turkmen alphabet|Turkmen]]<sup style=\"font-size:80%\" id=\"r-tk:1\">[[#n-tk|[55]]]</sup>\n| || || || ||Ä|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Ulithian language|Ulithian]]<sup style=\"font-size:80%\" id=\"r-uli\">[[#n-uli:1|[49]]]</sup>\n| || ||Ȧ|| || || || || || || || || || || || || || || || || || || || || ||Ė|| || || || || || || || || || || || || || || || ||\n|-\n![[Ume Sami language|Ume Sami]]\n|Á|| || || ||Ä|| || || || || ||Å|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Uyghur language|Uyghur]]\n| || || || || || || || || || || || || || || || || || || || || || || || || || ||Ë|| || || || || || || || || || || || || || ||\n|-\n![[Venda language#Writing system|Venda]]<sup style=\"font-size:80%\" id=\"r-ve:1\">[[#n-ve|[51]]]</sup>\n|Á|| || || || || || || || || || || || || || || || || || || || ||Ḓ||É|| || || || || || || || || || || || || || || || || || ||\n|-\n![[Vietnamese alphabet|Vietnamese]]<sup style=\"font-size:80%\" id=\"r-vi:1\">[[#n-vi|[26]]]</sup>\n|Á||À|| ||Â|| || || ||Ă|| ||Ã|| || || || || || || || || || || || ||É||È|| ||Ê|| || || || ||Ẽ|| ||Ẹ|| || || || || || || || ||\n|-\n![[Volapük]]\n| || || || ||Ä|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Võro language|Võro]]\n| || || || ||Ä|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Walloon language|Walloon]]<sup style=\"font-size:80%\" id=\"r-wa:1\">[[#n-wa|[27]]]</sup>\n| ||À|| ||Â|| || || || || || ||Å|| || || || || || || || || || || ||É||È|| ||Ê||Ë|| || || || ||E̊|| || || || || || || || || ||\n|-\n![[Welsh alphabet|Welsh]]<sup style=\"font-size:80%\" id=\"r-cy:1\">[[#n-cy|[28]]]</sup>\n|Á||À|| ||Â||Ä|| || || || || || || || || || || || || || || || || ||É||È|| ||Ê||Ë|| || || || || || || || || || || || || || ||\n|-\n![[West Frisian alphabet|West Frisian]]\n| || || ||Â||Ä|| || || || || || || || || || || || || || || || || ||É|| || ||Ê||Ë|| || || || || || || || || || || || || || ||\n|-\n![[Wolof language|Wolof]]\n| ||À|| || || || || || || || || || || || || || || || || || || || ||É|| || || ||Ë|| || || || || || || || || || || || || || ||\n|-\n![[Xhosa language|Xhosa]]\n|Á||À|| ||Â||Ä|| || || || || || || || || || || || || || || || || ||É||È|| ||Ê||Ë|| || || || || || || || || || || || || || ||\n|-\n![[Yapese language|Yapese]]<sup style=\"font-size:80%\" id=\"r-yap:1\">[[#n-yap|[50]]]</sup>\n| || || || ||Ä|| || || || || || || || || || || || || || || || || || || || || ||Ë|| || || || || || || || || || || || || || ||\n|-\n![[Yoruba Language#Writing system|Yoruba]]<sup style=\"font-size:80%\" id=\"r-yo:1\">[[#n-yo|[56]]]</sup>\n|Á||À|| ||Â|| || ||Ǎ|| || ||Ã|| || || || || || || || || || || || ||É||È|| ||Ê|| ||Ě|| || ||Ẽ|| ||Ẹ|| || || || || || || || ||\n|}\n\n====Letters derived from I–O====\n{| cellspacing=\"0\" cellpadding=\"1\" class=\"wikitable\" style=\"border:1px solid #aaa;border-collapse:collapse;line-height:1.4;text-align:center;table-layout:fixed\"\n|+Letter-diacritic combinations (detached) in various Latin alphabets (I–O)\n|-\n!style=\"width:10em\" rowspan=\"2\"|Alphabet\n!style=\"width:1.5em\"|[[Í]]\n!style=\"width:1.5em\"|[[Ì]]\n!style=\"width:1.5em\"|[[Dotted and dotless I|İ]]\n!style=\"width:1.5em\"|[[Î]]\n!style=\"width:1.5em\"|[[Ï]]\n!style=\"width:1.5em\"|[[Ǐ]]\n!style=\"width:1.5em\"|[[Ĭ]]\n!style=\"width:1.5em\"|[[Ī]]\n!style=\"width:1.5em\"|[[Ĩ]]\n!style=\"width:1.5em\"|[[Ị]]\n!style=\"width:1.5em\"|[[Ĵ]]\n!style=\"width:1.5em\"|[[Ķ]]\n!style=\"width:1.5em\"|[[Ǩ]]\n!style=\"width:1.5em\"|[[Ĺ]]\n!style=\"width:1.5em\"|[[Ļ]]\n!style=\"width:1.5em\"|[[Ľ]]\n!style=\"width:1.5em\"|[[Ŀ]]\n!style=\"width:1.5em\"|[[Ḽ]]\n!style=\"width:1.5em\"|[[M̂]]\n!style=\"width:1.5em\"|[[M̄]]\n!style=\"width:1.5em\"|ʼN\n!style=\"width:1.5em\"|[[Ń]]\n!style=\"width:1.5em\"|[[N̂]]\n!style=\"width:1.5em\"|[[Ṅ]]\n!style=\"width:1.5em\"|[[N̈]]\n!style=\"width:1.5em\"|[[Ň]]\n!style=\"width:1.5em\"|[[N̄]]\n!style=\"width:1.5em\"|[[Ñ]]\n!style=\"width:1.5em\"|[[Ņ]]\n!style=\"width:1.5em\"|[[Ṋ]]\n!style=\"width:1.5em\"|[[Ó]]\n!style=\"width:1.5em\"|[[Ò]]\n!style=\"width:1.5em\"|[[Ȯ]]\n!style=\"width:1.5em\"|[[Ȱ]]\n!style=\"width:1.5em\"|[[Ô]]\n!style=\"width:1.5em\"|[[Ö]]\n!style=\"width:1.5em\"|[[Ȫ]]\n!style=\"width:1.5em\"|[[Ǒ]]\n!style=\"width:1.5em\"|[[Ŏ]]\n!style=\"width:1.5em\"|[[Ō]]\n!style=\"width:1.5em\"|[[Õ]]\n!style=\"width:1.5em\"|[[Ȭ]]\n!style=\"width:1.5em\"|[[Ő]]\n!style=\"width:1.5em\"|[[Ọ]]\n!style=\"width:1.5em\"|[[Ǿ]]\n!style=\"width:1.5em\"|[[Ơ]]\n|-\n!í!!ì!!i!!î!!ï!!ǐ!!ĭ!!ī!!ĩ!!ị!!ĵ!!ķ!!ǩ!!ĺ!!ļ!!ľ!!ŀ!!ḽ!!m̂!!m̄!![[ŉ]]!!ń!!n̂!!ṅ!!{{IPA|n̈}}!!ň!!n̄!!ñ!!ņ!!ṋ!!ó!!ò!!ô!!ȯ!!ȱ!!ö!!ȫ!!ǒ!!ŏ!!ō!!õ!!ȭ!!ő!!ọ!!ǿ!!ơ\n|-\n![[Latin alphabet|Latin]]<sup style=\"font-size:80%\" id=\"r-la:2\">[[#n-la|[2]]]</sup>\n| || || || || || ||Ĭ||Ī|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ŏ||Ō|| || || || || ||\n|-\n![[Afrikaans#Alphabet|Afrikaans]]<sup style=\"font-size:80%\" id=\"r-af:2\">[[#n-af|[54]]]</sup>\n|Í|| || ||Î||Ï|| || || || || || || || || || || || || || || ||ŉ|| || || || || || || || || ||Ó|| || || ||Ô|| || || || || || || || || || ||\n|-\n![[Albanian alphabet|Albanian]]<sup style=\"font-size:80%\" id=\"r-sq:2\">[[#n-sq|[3]]]</sup>\n| || || ||Î|| || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ô|| || || || || || || || || || || || ||\n|-\n![[Alemannic German|Alemannic]]\n|Í||Ì|| ||Î|| || || || || || || || || || || || || || || || || || || || || || || || || || ||Ó||Ò|| || ||Ô||Ö|| || || || || || || || || ||\n|-\n![[Old English language#Alphabet|Anglo-Saxon]]\n| || || || || || || ||Ī|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ō|| || || || || ||\n|-\n![[Arbëresh language|Arbëresh]]\n|Í|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ó||Ò|| || ||Ô|| || || || || || || || || || ||\n|-\n![[Asturian language|Asturian]]\n|Í|| || || || || || || || || || || || || ||Ḷ|| || || || || || || || || || || || ||Ñ|| || ||Ó|| || || || || || || || || || || || || || ||\n|-\n![[Austro-Bavarian German|Austro-Bavarian]]\n|Í||Ì|| ||Î|| || || || || || || || || || || || || || || || || || || || || || || || || || ||Ó||Ò|| || ||Ô||Ö|| || || || || || || || || ||\n|-\n![[Azeri alphabet|Azeri]]<sup style=\"font-size:80%\" id=\"r-az:2\">[[#n-az|[53]]]</sup>\n| || ||İ|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ö|| || || || || || || || || ||\n|-\n![[Basque language|Basque]]<sup style=\"font-size:80%\" id=\"r-eu:2\">[[#n-eu|[4]]]</sup>\n| || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ñ|| || || || || || || || || || || || || || || || || ||\n|-\n![[Belarusian Latin alphabet|Belarusian]]<sup style=\"font-size:80%\" id=\"r-be:2\">[[#n-be|[5]]]</sup>\n| || || || || || || || || || || || || || || || || || || || || ||Ń|| || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Berber Latin alphabet|Berber]]\n| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Bislama language|Bislama]]<sup style=\"font-size:80%\" id=\"r-bi:2\">[[#n-bi|[45]]]</sup>\n| || || || ||Ï|| || || || || || || || || || || || || || ||M̄|| || || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Breton language|Breton]]\n| || || ||Î|| || || || || || || || || || || || || || || || || || || || || || || ||Ñ|| || || || || || ||Ô|| || || || || || || || || || ||\n|-\n![[Catalan alphabet|Catalan]]<sup style=\"font-size:80%\" id=\"r-ca:2\">[[#n-ca|[6]]]</sup>\n|Í|| || || ||Ï|| || || || || || || || || || || ||Ŀ|| || || || || || || || || || ||Ñ|| || ||Ó||Ò|| || || || || || || || || || || || || ||\n|-\n![[British language (Celtic)|Celtic British]]\n| || || || || || ||Ĭ||Ī|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ŏ||Ō|| || || || || ||\n|-\n![[Chamorro alphabet|Chamorro]]<sup style=\"font-size:80%\" id=\"r-ch\">[[#n-ch|[43]]]</sup>\n| || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ñ|| || || || || || || || || || || || || || || || || ||\n|-\n![[Corsican language|Corsican]]<sup style=\"font-size:80%\" id=\"r-co:2\">[[#n-co|[31]]]</sup>\n| ||Ì|| || ||Ï|| || || || || || || || || || || || || || || || || || || || || || || || || || ||Ò|| || || || || || || || || || || || || ||\n|-\n![[Crimean Tatar language|Crimean Tatar]]\n| || ||İ|| || || || || || || || || || || || || || || || || || || || || || || || ||Ñ|| || || || || || || ||Ö|| || || || || || || || || ||\n|-\n![[Czech alphabet|Czech]]<sup style=\"font-size:80%\" id=\"r-cs:2\">[[#n-cs|[8]]]</sup>\n|Í|| || || || || || || || || || || || || || || || || || || || || || || || ||Ň|| || || || ||Ó|| || || || |||| || || || || || || || || ||\n|-\n![[Dalecarlian alphabet|Dalecarlian]]\n| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ö|| || || || || || || || || ||\n|-\n![[Danish alphabet|Danish]]<sup style=\"font-size:80%\" id=\"r-da:1\">[[#n-da|[9]]]</sup>\n|Í|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ó|| || || || || || || || || || || || || ||Ǿ||\n|-\n![[Dutch alphabet|Dutch]]<sup style=\"font-size:80%\" id=\"r-nl:2\">[[#n-nl|[10]]]</sup>\n|Í||Ì|| ||Î||Ï|| || || || || || || || || || || || || || || || || || || || || || || || || ||Ó||Ò|| || ||Ô||Ö|| || || || || || || || || ||\n|-\n![[Emilian-Romagnol language|Emilian-Romagnol]]\n| ||Ì|| ||Î|| || || || || || || || || || || || || || || || || || || ||Ṅ|| || || || || || ||Ó||Ò|| || ||Ô||Ö|| || || ||Ō|| || || || || ||\n|-\n![[English alphabet|English]]<sup style=\"font-size:80%\" id=\"r-en:2\">[[#n-en|[36]]]</sup>\n| || || ||Î||Ï|| || || || || || || || || || || || || || || || || || || || || || || || || ||Ó|| || || ||Ô||Ö|| || || || || || || || || ||\n|-\n![[Esperanto alphabet|Esperanto]]\n| || || || || || || || || || ||Ĵ|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Estonian alphabet|Estonian]]\n| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ö|| || || || ||Õ|| || || || ||\n|-\n![[Extremaduran language|Extremaduran]]\n|Í|| || || || || || || || || || || || || || || || || || || || || || || || || || ||Ñ|| || ||Ó|| || || || || || || || || || || || || || ||\n|-\n![[Fala language|Fala]]\n|Í|| || || || || || || ||Ĩ|| || || || || || || || || || || || || || || || || || || || || ||Ó|| || || || || || || || || ||Õ|| || || || ||\n|-\n![[Faroese alphabet|Faroese]]\n|Í|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ó|| || || || || || || || || || || || || || ||\n|-\n![[Filipino alphabet|Filipino]]<sup style=\"font-size:80%\" id=\"r-tl:2\">[[#n-tl|[11]]]</sup>\n|Í||Ì|| ||Î|| || || || || || || || || || || || || || || || || || || || || || || ||Ñ|| || ||Ó||Ò|| || || || || || || || || || || || || ||\n|-\n![[Finnish alphabet|Finnish]]\n| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ö|| || || || || || || || || ||\n|-\n![[French alphabet|French]]<sup style=\"font-size:80%\" id=\"r-fr:2\">[[#n-fr|[12]]]</sup>\n| || || ||Î||Ï|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ô|| || || || || || || || || || ||\n|-\n![[Friulian language|Friulian]]\n| ||Ì|| ||Î|| || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ò|| || ||Ô|| || || || || || || || || || ||\n|-\n![[Fula alphabets|Fula]]<sup style=\"font-size:80%\" id=\"r-ff\">[[#n-ff|[41]]]</sup>\n| || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ñ|| || || || || || || || || || || || || || || || || ||\n|-\n![[Gagauz language#Latin alphabet|Gagauz]]\n| || ||İ|| || || || || || || || || || || || || || || || || || || || || || || || ||Ñ|| || || || || || || ||Ö|| || || || || || || || || ||\n|-\n![[Galician language|Galician]]<sup style=\"font-size:80%\" id=\"r-gl\">[[#n-gl|[33]]]</sup>\n|Í|| || || ||Ï|| || || || || || || || || || || || || || || || || || || || || || ||Ñ|| || ||Ó|| || || || || || || || || || || || || || ||\n|-\n![[German alphabet|German]]<sup style=\"font-size:80%\" id=\"r-de:2\">[[#n-de|[13]]]</sup>\n| || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ñ|| || || || || || || ||Ö|| || || || || || || || || ||\n|-\n![[Kalaallisut language|Greenlandic]]\n|Í|| || ||Î|| || || || ||Ĩ|| || || || || || || || || || || || || || || || || || || || || || || || || ||Ô|| || || || || || || || || || ||\n|-\n![[Guaraní alphabet|Guaraní]]<sup style=\"font-size:80%\" id=\"r-gn:2\">[[#n-gn|[14]]]</sup>\n|Í|| || || || || || || ||Ĩ|| || || || || || || || || || || || || || || || || || ||Ñ|| || ||Ó|| || || || || || || || || ||Õ|| || || || ||\n|-\n![[Gwich'in language|Gwich'in]]\n| ||Ì|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ò|| || || || || || || || || || || || || ||\n|-\n![[Haitian Creole|Haitian]]\n| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ò|| || || || || || || || || || || || || ||\n|-\n![[Hän language|Hän]]\n| ||Ì|| ||Î|| ||Ǐ|| || || || || || || || || || || || || || || || || || || || || || || || || ||Ò|| || ||Ô|| || ||Ǒ|| || || || || || || ||\n|-\n![[Hawaiian alphabet|Hawaiian]]\n| || || || || || || ||Ī|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ō|| || || || || ||\n|-\n![[Hungarian alphabet|Hungarian]]<sup style=\"font-size:80%\" id=\"r-hu:2\">[[#n-hu|[15]]]</sup>\n|Í|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ó|| || || || ||Ö|| || || || || || ||Ő|| || ||\n|-\n![[Icelandic orthography|Icelandic]]\n|Í|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ó|| || || || ||Ö|| || || || || || || || || ||\n|-\n![[Igbo language#Writing system|Igbo]]\n|Í||Ì|| || || || || || || ||Ị|| || || || || || || || || || || || || ||Ṅ|| || || || || || ||Ó||Ò|| || || || || || || || || || || ||Ọ|| ||\n|-\n![[Inari Sami language|Inari Sami]]\n| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ö|| || || || || || || || || ||\n|-\n![[Irish orthography|Irish]]<sup style=\"font-size:80%\" id=\"r-ga:2\">[[#n-ga|[16]]]</sup>\n|Í|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ó|| || || || || || || || || || || || || || ||\n|-\n![[Italian alphabet|Italian]]<sup style=\"font-size:80%\" id=\"r-it:2\">[[#n-it|[17]]]</sup>\n|Í||Ì|| ||Î||Ï|| || || || || || || || || || || || || || || || || || || || || || || || || ||Ó||Ò|| || || || || || || || || || || || || ||\n|-\n![[Kashubian alphabet|Kashubian]]\n| || || || || || || || || || || || || || || || || || || || || ||Ń|| || || || || || || || ||Ó||Ò|| || ||Ô|| || || || || || || || || || ||\n|-\n![[Kazakh alphabets|Kazakh]]<sup style=\"font-size:80%\" id=\"r-kz:0\">[[#n-kz|[38]]]</sup>\n| || ||ı|| || || || || || || || || || || || || || || || || || ||Ń|| || || || || || || || ||Ó|| || || || || || || || || || | || || || ||\n|-\n![[Khasi language|Khasi]]<sup style=\"font-size:80%\" id=\"r-kz:0\">[[#n-kz|[38]]]</sup>\n| || || || ||Ï|| || || || || || || || || || || || || || || || || || || || || || ||Ñ|| || || || || || || || || || || || || | || || || ||\n|-\n![[Kurdish alphabet|Kurdish]]\n| || || ||Î|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Kurdish alphabet|Kurdish (IS)]]\n|Í|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Latvian alphabet|Latvian]]<sup style=\"font-size:80%\" id=\"r-lv:2\">[[#n-lv|[18]]]</sup>\n| || || || || || || ||Ī|| || || ||Ķ|| || ||Ļ|| || || || || || || || || || || || || ||Ņ|| || || || || || || || || || ||Ō|| || || || || ||\n|-\n![[Livonian language#Alphabet|Livonian]]<sup style=\"font-size:80%\" id=\"r-liv:2\">[[#n-liv:2|[46]]]</sup>\n| || || || || || || ||Ī|| || || || || || ||Ļ|| || || || || || || || || || || || || ||Ņ|| || || ||Ȯ||Ȱ|| ||Ö||Ȫ|| || ||Ō||Õ||Ȭ|| || || ||\n|-\n![[Luxembourgish alphabet|Luxembourgish]]\n| || || ||Î|| || || || || || || || || || || || || || ||M̂|| || || ||N̂|| || || || || || || || || || || ||Ô||Ö|| || || || || || || || || ||\n|-\n![[Malagasy language|Malagasy]]\n| ||Ì|| || || || || || || || || || || || || || || || || || || || || || ||N̈|| || ||Ñ|| || || ||Ò|| || ||Ô|| || || || || || || || || || ||\n|-\n![[Maltese alphabet|Maltese]]<sup style=\"font-size:80%\" id=\"r-mt:2\">[[#n-mt|[20]]]</sup>\n| ||Ì|| ||Î|| || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ò|| || || || || || || || || || || || || ||\n|-\n![[Māori language|Māori]]\n| || || || || || || ||Ī|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ō|| || || || || ||\n|-\n![[Marshallese language|Marshallese]]<sup style=\"font-size:80%\" id=\"r-mh:2\">[[#n-mh|[47]]]</sup>\n| || || || || || || || || || || || || || ||Ļ|| || || || || || || || || || || ||N̄|| ||Ņ|| || || || || || || || || || ||Ō|| || || || || ||\n|-\n![[Massachusett writing systems|Massachusett]]<sup style=\"font-size:80%\" id=\"r-mss:2\">[[#n-mss|[62]]]</sup>\n| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ô|| || || || || || || || || || ||\n|-\n![[Mirandese language|Mirandese]]\n|Í|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ó|| || || ||Ô|| || || || || || || || || || ||\n|-\n![[Mohawk language|Mohawk]]\n|Í||Ì|| || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ó||Ò|| || || || || || || || || || || || || ||\n|-\t\n![[Na'vi language#Phonology and orthography|Na'vi]]<sup style=\"font-size:80%\" id=\"r-mis:2\">[[#n-mis|[57]]]</sup>\n| ||Ì|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Navajo language|Navajo]]\n|Í|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ó|| || || || || || || || || || || || || || ||\n|-\n![[Norn language|Norn]]\n|Í|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ó|| || || || || || || || || || || || || || ||\n|-\n![[Norwegian alphabet|Norwegian]]<sup style=\"font-size:80%\" id=\"r-no:2\">[[#n-da|[9]]]</sup>\n|Í||Ì|| ||Î|| || || || || || || || || || || || || || || || || || || || || || || || || || ||Ó||Ò|| || ||Ô||Ö|| || || || || || || || || ||\n|-\n![[Occitan alphabet|Occitan]]\n|Í|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ó||Ò|| || || || || || || || || || || || || ||\n|-\n![[Pan-Nigerian alphabet|Pan-Nigerian]]\n| || || || || || || || || ||Ị|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ọ|| ||\n|-\n![[Papiamento orthography|Papiamento]]<sup style=\"font-size:80%\" id=\"r-pap:2\">[[#n-pap|[63]]]</sup>\n|Í|| || || || || || || || || || || || || || || || || || || || || || || || || || ||Ñ|| || ||Ó||Ò|| || || || || || || || || || || || || ||\n|-\n![[Piedmontese language|Piedmontese]]<sup style=\"font-size:80%\" id=\"r-pms:1\">[[#n-pms|[37]]]</sup>\n| ||Ì|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ò|| || || || || || || || || || || || || ||\n|-\n![[Pinyin]]<sup style=\"font-size:80%\" id=\"r-pinyin:2\">[[#n-pinyin|[32]]]</sup>\n|Í||Ì|| || || ||Ǐ|| ||Ī|| || || || || || || || || || || || || || || || || || || || || || ||Ó||Ò|| || || || || ||Ǒ|| ||Ō|| || || || || ||\n|-\n![[Polish alphabet|Polish]]<sup style=\"font-size:80%\" id=\"r-pl:2\">[[#n-pl|[22]]]</sup>\n| || || || || || || || || || || || || || || || || || || || || ||Ń|| || || || || || || || ||Ó|| || || || || || || || || || || || || || ||\n|-\n![[Portuguese alphabet|Portuguese]]<sup style=\"font-size:80%\" id=\"r-pt:2\">[[#n-pt|[23]]]</sup>\n|Í|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ó||Ò|| || ||Ô|| || || || || ||Õ|| || || || ||\n|-\n![[Quechua alphabet|Quechua]]\n| || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ñ|| || || || || || || || || || || || || || || || || ||\n|-\n![[Rhaeto-Romance languages|Rhaeto-Romance]]\n| ||Ì|| ||Î|| || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ò|| || || || || || || || || || || || || ||\n|-\n![[Romanian alphabet|Romanian]]\n| || || ||Î|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Samoan language#Alphabet|Samoan]]\n|Í|| || || || || || ||Ī|| || || || || || || || || || || || || || || || || || || || || || ||Ó|| || || || || || || || ||Ō|| || || || || ||\n|-\n![[Sardinian language|Sardinian]]\n|Í||Ì|| || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ó||Ò|| || || || || || || || || || || || || ||\n|-\n![[Scottish Gaelic alphabet|Scots Gaelic]]\n| ||Ì|| || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ó||Ò|| || || || || || || || || || || || || ||\n|-\n![[Sicilian language|Sicilian]]\n| ||Ì|| ||Î|| || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ò|| || ||Ô|| || || || || || || || || || ||\n|-\n![[Skolt Sami language|Skolt Sami]]\n| || || || || || || || || || || || ||Ǩ|| || || || || || || || || || || || || || || || || || || || || || ||Ö|| || || || ||Õ|| || || || ||\n|-\n![[Slovak alphabet|Slovak]]<sup style=\"font-size:80%\" id=\"r-sk:2\">[[#n-sk|[24]]]</sup>\n|Í|| || || || || || || || || || || || ||Ĺ|| ||Ľ|| || || || || || || || || ||Ň|| || || || ||Ó|| || || ||Ô||Ö|| || || || || || || || || ||\n|-\n![[Slovenian alphabet|Slovenian]]\n|Í||Ì|| || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ó||Ò|| || ||Ô||Ö|| || || || || || || || || ||\n|-\n![[Sorbian alphabet|Sorbian]]<sup style=\"font-size:80%\" id=\"r-wen:3\">[[#n-wen|[64]]]</sup>\n| || || || || || || || || || || || || || || || || || || || || ||Ń|| || || || || || || || ||Ó|| || || || || || || || || || || || || || ||\n|-\n![[Southern Sami language|Southern Sami (Norway)]]\n| || || || ||Ï|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Southern Sami language|Southern Sami (Sweden)]]\n| || || || ||Ï|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ö|| || || || || || || || || ||\n|-\n![[Spanish alphabet|Spanish]]<sup style=\"font-size:80%\" id=\"r-es:2\">[[#n-es|[25]]]</sup>\n|Í|| || || ||Ï|| || || || || || || || || || || || || || || || || || || || || || ||Ñ|| || ||Ó|| || || || || || || || || || || || || || ||\n|-\n![[Swedish alphabet|Swedish]]<sup style=\"font-size:80%\" id=\"r-sv\">[[#n-sv|[21]]]</sup>\n| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ö|| || || || || || || || || ||\n|-\n![[Tahitian language#Phonology|Tahitian]]\n| || || || || || || ||Ī|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ō|| || || || || ||\n|-\n![[Tetum alphabet|Tetum]]\n|Í|| || || || || || || || || || || || || || || || || || || || || || || || || || ||Ñ|| || ||Ó|| || || || || || || || || || || || || || ||\n|-\n![[Tongan language#Tongan alphabet|Tongan]]\n|Í|| || || || || || ||Ī|| || || || || || || || || || || || || || || || || || || || || || ||Ó|| || || || || || || || ||Ō|| || || || || ||\n|-\n![[Tswana language|Tswana]]<sup style=\"font-size:80%\" id=\"r-tn:2\">[[#n-tn|[52]]]</sup>\n| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ô|| || || || || || || || || || ||\n|-\n![[Tunisian Arabic#Elyssa Alphabet|Tunisian Arabic]]<sup style=\"font-size:80%\" id=\"r-aeb:2\">[[#n-aeb|[58]]</sup>\n| ||Ì|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ò|| || || || || || || || || || || || || ||\n|-\n![[Turkish alphabet|Turkish]]\n| || ||İ||Î|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ö|| || || || || || || || || ||\n|-\n![[Turkmen alphabet|Turkmen]]<sup style=\"font-size:80%\" id=\"r-tk:2\">[[#n-tk|[55]]]</sup>\n| || || || || || || || || || || || || || || || || || || || || || || || || || ||Ň|| || || || || || || || ||Ö|| || || || || || || || || ||\n|-\n![[Ulithian language|Ulithian]]<sup style=\"font-size:80%\" id=\"r-uli\">[[#n-uli:2|[49]]]</sup>\n| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ȯ|| || || || || || || || || || || || ||\n|-\n![[Ume Sami language|Ume Sami]]\n| || || || ||Ï|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ö|| || || || || || || || || ||\n|-\n![[Uyghur language|Uyghur]]\n| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ö|| || || || || || || || || ||\n|-\n![[Venda language#Writing system|Venda]]<sup style=\"font-size:80%\" id=\"r-ve:2\">[[#n-ve|[51]]]</sup>\n|Í|| || || || || || || || || || || || || || || || ||Ḽ|| || || || || ||Ṅ|| || || || || ||Ṋ||Ó|| || || || || || || || || || || || || || ||\n|-\n![[Vietnamese alphabet|Vietnamese]]<sup style=\"font-size:80%\" id=\"r-vi:2\">[[#n-vi|[26]]]</sup>\n|Í||Ì|| || || || || || ||Ĩ||Ị|| || || || || || || || || || || || || || || || || || || || ||Ó||Ò|| || ||Ô|| || || || || ||Õ|| || ||Ọ|| ||Ơ\n|-\n![[Volapük]]\n| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ö|| || || || || || || || || ||\n|-\n![[Võro language|Võro]]\n| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ö|| || || || ||Õ|| || || || || \n|-\n![[Walloon language|Walloon]]<sup style=\"font-size:80%\" id=\"r-wa:2\">[[#n-wa|[27]]]</sup>\n| ||Ì|| ||Î|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ô||Ö|| || || || || || || || || ||\n|-\n![[Welsh alphabet|Welsh]]<sup style=\"font-size:80%\" id=\"r-cy:2\">[[#n-cy|[28]]]</sup><!--for Ñ see talk page-->\n|Í||Ì|| ||Î||Ï|| || || || || || || || || || || || || || || || || || || || || || || || || ||Ó||Ò|| || ||Ô||Ö|| || || || || || || || || ||\n|-\n![[West Frisian alphabet|West Frisian]]\n| || || || ||Ï|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ô||Ö|| || || || || || || || || ||\n|-\n![[Wolof language|Wolof]]\n| || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ñ|| || ||Ó|| || || || || || || || || || || || || || ||\n|-\n![[Xhosa language|Xhosa]]\n|Í||Ì|| ||Î||Ï|| || || || || || || || || || || || || || || || || || || || || || || || || ||Ó||Ò|| || ||Ô||Ö|| || || || || || || || || ||\n|-\n![[Yapese language|Yapese]]<sup style=\"font-size:80%\" id=\"r-yap:2\">[[#n-yap|[50]]]</sup>\n| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ö|| || || || || || || || || ||\n|-\n![[Yoruba Language#Writing system|Yoruba]]<sup style=\"font-size:80%\" id=\"r-yo:2\">[[#n-yo|[56]]]</sup>\n|Í||Ì|| ||Î|| ||Ǐ|| || ||Ĩ|| || || || || || || || || || ||M̄|| ||Ń|| || || || ||N̄|| || || ||Ó||Ò|| || ||Ô|| || ||Ǒ|| || ||Õ|| || ||Ọ|| ||\n|}\n\n====Letters derived from R–Z====\n{| cellspacing=\"0\" cellpadding=\"1\" class=\"wikitable\" style=\"border:1px solid #aaa;border-collapse:collapse;line-height:1.4;text-align:center;table-layout:fixed\"\n|+Letter-diacritic combinations (detached) in various Latin alphabets (P–Z)\n|-\n!style=\"width:10em\" rowspan=\"2\"|Alphabet\n!style=\"width:1.5em\"|[[P̄]]\n!style=\"width:1.5em\"|[[Ŕ]]\n!style=\"width:1.5em\"|[[Ř]]\n!style=\"width:1.5em\"|[[Ŗ]]\n!style=\"width:1.5em\"|[[Ś]]\n!style=\"width:1.5em\"|[[Ŝ]]\n!style=\"width:1.5em\"|[[Ṡ]]\n!style=\"width:1.5em\"|[[Š]]\n!style=\"width:1.5em\"|[[Ș]]\n!style=\"width:1.5em\"|[[Ṣ]]\n!style=\"width:1.5em\"|[[Ť]]\n!style=\"width:1.5em\"|[[Ț]]\n!style=\"width:1.5em\"|[[Ṭ]]\n!style=\"width:1.5em\"|[[Ṱ]]\n!style=\"width:1.5em\"|[[Ú]]\n!style=\"width:1.5em\"|[[Ù]]\n!style=\"width:1.5em\"|[[Û]]\n!style=\"width:1.5em\"|[[Ü]]\n!style=\"width:1.5em\"|[[Ǔ]]\n!style=\"width:1.5em\"|[[Ŭ]]\n!style=\"width:1.5em\"|[[Ū]]\n!style=\"width:1.5em\"|[[Ũ]]\n!style=\"width:1.5em\"|[[Ű]]\n!style=\"width:1.5em\"|[[Ů]]\n!style=\"width:1.5em\"|[[Ụ]]\n!style=\"width:1.5em\"|[[Ẃ]]\n!style=\"width:1.5em\"|[[Ẁ]]\n!style=\"width:1.5em\"|[[Ŵ]]\n!style=\"width:1.5em\"|[[Ẅ]]\n!style=\"width:1.5em\"|[[Ý]]\n!style=\"width:1.5em\"|[[Ỳ]]\n!style=\"width:1.5em\"|[[Ŷ]]\n!style=\"width:1.5em\"|[[Ÿ]]\n!style=\"width:1.5em\"|[[Ȳ]]\n!style=\"width:1.5em\"|[[Ỹ]]\n!style=\"width:1.5em\"|[[Ź]]\n!style=\"width:1.5em\"|[[Ż]]\n!style=\"width:1.5em\"|[[Ž]]\n!style=\"width:1.5em\"|[[Ẓ]]\n!style=\"width:1.5em\"|[[Ǯ]]\n|-\n!p̄!!ŕ!!ř!!ŗ!!ś!!ŝ!!ṡ!!š!!ş!!ṣ!!ť!!ț!!ṭ!!ṱ!!ú!!ù!!û!!ü!!ǔ!!ŭ!!ū!!ũ!!ű!!ů!!ụ!!ẃ!!ẁ!!ŵ!!ẅ!!ý!!ỳ!!ŷ!!ÿ!!ȳ!!ỹ!!ź!!ż!!ž!!ẓ!!ǯ\n|-\n![[Latin alphabet|Latin]]<sup style=\"font-size:80%\" id=\"r-la:3\">[[#n-la|[2]]]</sup>\n| || || || || || || || || || || || || || || || || || || ||Ŭ||Ū|| || || || || || || || || || || || || || || || || || ||\n|-\n![[Afrikaans#Alphabet|Afrikaans]]<sup style=\"font-size:80%\" id=\"r-af:3\">[[#n-af|[54]]]</sup>\n| || || || || || || || || || || || || || ||Ú|| ||Û|| || || || || || || || || || || || ||Ý|| || || || || || || || || ||\n|-\n![[Albanian alphabet|Albanian]]<sup style=\"font-size:80%\" id=\"r-sq:3\">[[#n-sq|[3]]]</sup>\n| || || || || || || || || || || || || || || || ||Û|| || || || || || || || || || || || || || ||Ŷ|| || || || || || || ||\n|-\n![[Alemannic German|Alemannic]]\n| || || || || || || || || || || || || || ||Ú||Ù||Û||Ü|| || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Old English language#Alphabet|Anglo-Saxon]]\n| || || || || || || || || || || || || || || || || || || || ||Ū|| || || || || || || || || || || || ||Ȳ|| || || || || ||\n|-\n![[Arbëresh language|Arbëresh]]\n| || || || || || || || || || || || || || ||Ú||Ù||Û|| || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Asturian language|Asturian]]\n| || || || || || || || || || || || || || ||Ú|| || ||Ü|| || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Austro-Bavarian German|Austro-Bavarian]]\n| || || || || || || || || || || || || || ||Ú||Ù||Û||Ü|| || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Azeri alphabet|Azeri]]<sup style=\"font-size:80%\" id=\"r-az:3\">[[#n-az|[53]]]</sup>\n| || || || || || || || || || || || || || || || || ||Ü|| || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Basque language|Basque]]<sup style=\"font-size:80%\" id=\"r-eu:3\">[[#n-eu|[4]]]</sup>\n| || || || || || || || || || || || || || || || || ||Ü|| || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Belarusian Latin alphabet|Belarusian]]<sup style=\"font-size:80%\" id=\"r-be:3\">[[#n-be|[5]]]</sup>\n| || || || ||Ś|| || ||Š|| || || || || || || || || || || ||Ŭ|| || || || || || || || || || || || || || || ||Ź|| ||Ž|| ||\n|-\n![[Berber Latin alphabet|Berber]]\n| || ||Ř|| || || || || || ||Ṣ|| || ||Ṭ|| || || || || || || || || || || || || || || || || || || || || || || || || ||Ẓ||\n|-\n![[Bislama language|Bislama]]<sup style=\"font-size:80%\" id=\"r-bi:3\">[[#n-ca|[45]]]</sup>\n|P̄|| || || || || || || || || || || || || || || || ||Ü|| || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Breton language|Breton]]\n| || || || || || || || || || || || || || || ||Ù||Û||Ü|| || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Catalan alphabet|Catalan]]<sup style=\"font-size:80%\" id=\"r-ca:3\">[[#n-ca|[6]]]</sup>\n| || || || || || || || || || || || || || ||Ú|| || ||Ü|| || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[British language (Celtic)|Celtic British]]\n| || || || || || || || || || || || || || || || || || || ||Ŭ||Ū|| || || || || || || || || || || || || || || || || || ||\n|-\n![[Chewa language|Chewa]]\n| || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ŵ|| || || || || || || || || || || ||\n|-\n![[Corsican language|Corsican]]<sup style=\"font-size:80%\" id=\"r-co:3\">[[#n-co|[31]]]</sup>\n| || || || || || || || || || || || || || || ||Ù|| || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Crimean Tatar language|Crimean Tatar]]\n| || || || || || || || || || || || || || || || || ||Ü|| || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Croatian alphabet|Croatian]]<sup style=\"font-size:80%\" id=\"r-hr:3\">[[#n-hr|[7]]]</sup>\n| || || || || || || ||Š|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ž|| ||\n|-\n![[Czech alphabet|Czech]]<sup style=\"font-size:80%\" id=\"r-cs:3\">[[#n-cs|[8]]]</sup>\n| || ||Ř|| || || || ||Š|| || ||Ť|| || || ||Ú|| || ||Ü|| || || || || ||Ů|| || || || || ||Ý|| || || || || || || ||Ž|| ||\n|-\n![[Danish alphabet|Danish]]<sup style=\"font-size:80%\" id=\"r-da:1\">[[#n-da|[9]]]</sup>\n| || || || || || || || || || || || || || ||Ú|| || || || || || || || || || || || || || ||Ý|| || || || || || || || || ||\n|-\n![[Dutch alphabet|Dutch]]<sup style=\"font-size:80%\" id=\"r-nl:3\">[[#n-nl|[10]]]</sup>\n| || || || || || || || || || || || || || ||Ú||Ù||Û||Ü|| || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Emilian-Romagnol language|Emilian-Romagnol]]\n| || || || || || ||Ṡ|| || || || || || || || ||Ù||Û||Ü|| || || || || || || || || || || || || || || || || || ||Ż|| || ||\n|-\n![[English alphabet|English]]<sup style=\"font-size:80%\" id=\"r-en:3\">[[#n-en|[36]]]</sup>\n| || || || || || || || || || || || || || || || ||Û||Ü|| || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Esperanto alphabet|Esperanto]]\n| || || || || ||Ŝ|| || || || || || || || || || || || || ||Ŭ|| || || || || || || || || || || || || || || || || || || ||\n|-\n![[Estonian alphabet|Estonian]]\n| || || || || || || ||Š|| || || || || || || || || ||Ü|| || || || || || || || || || || || || || || || || || || ||Ž|| ||\n|-\n![[Extremaduran language|Extremaduran]]\n| || || || || || || || || || || || || || ||Ú|| || ||Ü|| || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Fala language|Fala]]\n| || || || || || || || || || || || || || ||Ú|| || ||Ü|| || || ||Ũ|| || || || || || || || || || || || || || || || || ||\n|-\n![[Faroese alphabet|Faroese]]\n| || || || || || || || || || || || || || ||Ú|| || || || || || || || || || || || || || ||Ý|| || || || || || || || || ||\n|-\n![[Filipino alphabet|Filipino]]<sup style=\"font-size:80%\" id=\"r-tl:3\">[[#n-tl|[11]]]</sup>\n| || || || || || || || || || || || || || ||Ú||Ù||Û|| || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Finnish alphabet|Finnish]]\n| || || || || || || ||Š|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ž|| ||\n|-\n![[French alphabet|French]]<sup style=\"font-size:80%\" id=\"r-fr:3\">[[#n-fr|[12]]]</sup>\n| || || || || || || || || || || || || || || ||Ù||Û||Ü|| || || || || || || || || || || || || || ||Ÿ|| || || || || || ||\n|-\n![[Friulian language|Friulian]]\n| || || || || || || || || || || || || || || ||Ù||Û|| || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Gagauz language#Latin alphabet|Gagauz]]\n| || || || || || || || || || || || || || || || || ||Ü|| || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Galician language|Galician]]<sup style=\"font-size:80%\" id=\"r-gl\">[[#n-gl|[33]]]</sup>\n| || || || || || || || || || || || || || ||Ú|| || ||Ü|| || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[German alphabet|German]]<sup style=\"font-size:80%\" id=\"r-de:3\">[[#n-de|[13]]]</sup>\n| || || || || || || || || || || || || || || || || ||Ü|| || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Greenlandic language|Greenlandic]]\n| || || || || || || || || || || || || || ||Ú|| ||Û|| || || || ||Ũ|| || || || || || || || || || || || || || || || || ||\n|-\n![[Guaraní alphabet|Guaraní]]<sup style=\"font-size:80%\" id=\"r-gn:3\">[[#n-gn|[14]]]</sup>\n| || || || || || || || || || || || || || ||Ú|| || || || || || ||Ũ|| || || || || || || ||Ý|| || || || ||Ỹ|| || || || ||\n|-\n![[Gwich'in language|Gwich'in]]\n| || || || || || || || || || || || || || || ||Ù|| || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Hän language|Hän]]\n| || || || || || || || || || || || || || || ||Ù||Û|| ||Ǔ|| || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Hawaiian alphabet|Hawaiian]]\n| || || || || || || || || || || || || || || || || || || || ||Ū|| || || || || || || || || || || || || || || || || || ||\n|-\n![[Hungarian alphabet|Hungarian]]<sup style=\"font-size:80%\" id=\"r-hu:3\">[[#n-hu|[15]]]</sup>\n| || || || || || || || || || || || || || ||Ú|| || ||Ü|| || || || ||Ű|| || || || || || || || || || || || || || || || ||\n|-\n![[Icelandic orthography|Icelandic]]\n| || || || || || || || || || || || || || ||Ú|| || || || || || || || || || || || || || ||Ý|| || || || || || || || || ||\n|-\n![[Igbo language#Writing system|Igbo]]\n| || || || || || || || || || || || || || ||Ú||Ù|| || || || || || || || ||Ụ|| || || || || || || || || || || || || || ||\n|-\n![[Inari Sami language|Inari Sami]]\n| || || || || || || ||Š|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ž|| ||\n|-\n![[Irish orthography|Irish]]<sup style=\"font-size:80%\" id=\"r-ga:3\">[[#n-ga|[16]]]</sup>\n| || || || || || || || || || || || || || ||Ú|| || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Italian alphabet|Italian]]<sup style=\"font-size:80%\" id=\"r-it:3\">[[#n-it|[17]]]</sup>\n| || || || || || || || || || || || || || ||Ú||Ù|| || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Kashubian alphabet|Kashubian]]\n| || || || ||Ś|| || || || || || || || || || ||Ù|| || || || || || || || || || || || || || || || || || || ||Ź||Ż|| || ||\n|-\n![[Kazakh alphabets|Kazakh]]<sup style=\"font-size:80%\" id=\"r-kz:0\">[[#n-kz|[38]]]</sup>\n| || || || || || || || || || || || || || ||Ú|| || || || || || || || || || || || || || ||Ý|| || || || || || || || || ||\n|-\n![[Kurdish alphabet|Kurdish (IS)]]\n| || || || || || || || || || || || || || ||Ú||Ù|| || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Kurdish alphabet|Kurdish]]\n| || || || || || || || || || || || || || || || ||Û|| || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Latvian alphabet|Latvian]]<sup style=\"font-size:80%\" id=\"r-lv:3\">[[#n-lv|[18]]]</sup>\n| || || ||Ŗ|| || || ||Š|| || || || || || || || || || || || ||Ū|| || || || || || || || || || || || || || || || ||Ž|| ||\n|-\n![[Lithuanian alphabet|Lithuanian]]<sup style=\"font-size:80%\" id=\"r-lt:3\">[[#n-lt|[19]]]</sup>\n| || || || || || || ||Š|| || || || || || || || || || || || ||Ū|| || || || || || || || || || || || || || || || ||Ž|| ||\n|-\n![[Livonian language#Alphabet|Livonian]]<sup style=\"font-size:80%\" id=\"r-liv:3\">[[#n-liv:3|[46]]]</sup>\n| || || ||Ŗ|| || || ||Š|| || || ||Ț|| || || || || || || || ||Ū|| || || || || || || || || || || || ||Ȳ|| || || ||Ž|| ||\n|-\n![[Luxembourgish alphabet|Luxembourgish]]\n| || || || || || || || || || || || || || || || ||Û||Ü|| || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Malagasy language|Malagasy]]\n| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ỳ|| || || || || || || || ||\n|-\n![[Maltese alphabet|Maltese]]<sup style=\"font-size:80%\" id=\"r-mt:3\">[[#n-mt|[20]]]</sup>\n| || || || || || || || || || || || || || || ||Ù|| || || || || || || || || || || || || || || || || || || || ||Ż|| || ||\n|-\n![[Māori language|Māori]]\n| || || || || || || || || || || || || || || || || || || || ||Ū|| || || || || || || || || || || || || || || || || || ||\n|-\n![[Marshallese language|Marshallese]]<sup style=\"font-size:80%\" id=\"r-mh:3\">[[#n-mh|[47]]]</sup>\n| || || || || || || || || || || || || || || || || || || || ||Ū|| || || || || || || || || || || || || || || || || || ||\n|-\n![[Mirandese language|Mirandese]]\n| || || || || || || || || || || || || || ||Ú|| || || || || || ||Ũ|| || || || || || || || || || || || || || || || || ||\n|-\n![[Norn language|Norn]]\n| || || || || || || || || || || || || || ||Ú|| || || || || || || || || || || || || || ||Ý|| || || || || || || || || ||\n|-\n![[Northern Sami]]\n| || || || || || || ||Š|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ž|| ||\n|-\n![[Norwegian alphabet|Norwegian]]<sup style=\"font-size:80%\" id=\"r-no:3\">[[#n-da|[9]]]</sup>\n| || || || || || || || || || || || || || ||Ú||Ù||Û||Ü|| || || || || || || || || || || ||Ý||Ỳ||Ŷ|| || || || || || || ||\n|-\n![[Occitan alphabet|Occitan]]\n| || || || || || || || || || || || || || ||Ú|| || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Pan-Nigerian alphabet|Pan-Nigerian]]\n| || || || || || || || || ||Ṣ|| || || || || || || || || || || || || || ||Ụ|| || || || || || || || || || || || || || ||\n|-\n![[Papiamento orthography|Papiamento]]<sup style=\"font-size:80%\" id=\"r-pap:3\">[[#n-pap|[63]]]</sup>\n| || || || || || || || || || || || || || ||Ú||Ù|| ||Ü|| || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Piedmontese language|Piedmontese]]<sup style=\"font-size:80%\" id=\"r-pms:1\">[[#n-pms|[37]]]</sup>\n| || || || || || || || || || || || || || || ||Ù|| || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Pinyin]]<sup style=\"font-size:80%\" id=\"r-pinyin:3\">[[#n-pinyin|[32]]]</sup>\n| || || || || || || || || || || || || || ||Ú||Ù|| ||Ü||Ǔ|| ||Ū|| || || || || || || || || || || || || || || || || || ||\n|-\n![[Polish alphabet|Polish]]<sup style=\"font-size:80%\" id=\"r-pl:3\">[[#n-pl|[22]]]</sup>\n| || || || ||Ś|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ź||Ż|| || ||\n|-\n![[Portuguese alphabet|Portuguese]]<sup style=\"font-size:80%\" id=\"r-pt:3\">[[#n-pt|[23]]]</sup>\n| || || || || || || || || || || || || || ||Ú|| || ||Ü|| || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Rhaeto-Romance languages|Rhaeto-Romance]]\n| || || || || || || || || || || || || || || ||Ù|| || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Romani writing systems|Romani]]<sup style=\"font-size:80%\" id=\"r-rom:3\">[[#n-rom|[29]]]</sup>\n| || || || || || || ||Š|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ž|| ||\n|-\n![[Romanian alphabet|Romanian]]\n| || || || || || || || ||Ș|| || ||Ț|| || || || || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Samoan language#Alphabet|Samoan]]\n| || || || || || || || || || || || || || ||Ú|| || || || || ||Ū|| || || || || || || || || || || || || || || || || || ||\n|-\n![[Sardinian language|Sardinian]]\n| || || || || || || || || || || || || || ||Ú||Ù|| || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Scottish Gaelic alphabet|Scots Gaelic]]\n| || || || || || || || || || || || || || || ||Ù|| || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Serbian alphabet|Serbian]]<sup style=\"font-size:80%\" id=\"r-sr:3\">[[#n-hr|[7]]]</sup>\n| || || || || || || ||Š|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ž|| ||\n|-\n![[Sicilian language|Sicilian]]\n| || || || || || || || || || || || || || || ||Ù||Û|| || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Skolt Sami language|Skolt Sami]]\n| || || || || || || ||Š|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ž|| ||Ǯ\n|-\n![[Slovak alphabet|Slovak]]<sup style=\"font-size:80%\" id=\"r-sk:3\">[[#n-sk|[24]]]</sup>\n| ||Ŕ||Ř|| || || || ||Š|| || ||Ť|| || || ||Ú|| || ||Ü|| || || || || || || || || || || ||Ý|| || || || || || || ||Ž|| ||\n|-\n![[Slovenian alphabet|Slovenian]]\n| || || || || || || ||Š|| || || || || || ||Ú||Ù|| ||Ü|| || || || || || || || || || || || || || || || || || || ||Ž|| ||\n|-\n![[Sorbian alphabet|Sorbian]]<sup style=\"font-size:80%\" id=\"r-wen:4\">[[#n-wen|[64]]]</sup>\n| ||Ŕ||Ř|| ||Ś|| || ||Š|| || || || || || || || || || || || || || || || || || || || || || || || || || || ||Ź|| ||Ž|| ||\n|-\n![[Spanish alphabet|Spanish]]<sup style=\"font-size:80%\" id=\"r-es:3\">[[#n-es|[25]]]</sup>\n| || || || || || || || || || || || || || ||Ú|| || ||Ü|| || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Swedish alphabet|Swedish]]\n| || || || || || || || || || || || || || || || || ||Ü|| || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Tahitian language#Phonology|Tahitian]]\n| || || || || || || || || || || || || || || || || || || || ||Ū|| || || || || || || || || || || || || || || || || || ||\n|-\n![[Tetum alphabet|Tetum]]\n| || || || || || || || || || || || || || ||Ú|| || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Tongan language#Tongan alphabet|Tongan]]\n| || || || || || || || || || || || || || ||Ú|| || || || || ||Ū|| || || || || || || || || || || || || || || || || || ||\n|-\n![[Tswana language|Tswana]]<sup style=\"font-size:80%\" id=\"r-tn:3\">[[#n-tn|[52]]]</sup>\n| || || || || || || ||Š|| || || || || || || || || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Tunisian Arabic#Elyssa Alphabet|Tunisian Arabic]]<sup style=\"font-size:80%\" id=\"r-aeb:3\">[[#n-aeb|[58]]</sup>\n| || || || || || ||Ṡ|| || || || || || || || ||Ù|| || || || || || || || || || || || || || || || || || || || ||Ż|| || ||\n|-\n![[Turkish alphabet|Turkish]]\n| || || || || || || || || || || || || || || || ||Û||Ü|| || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Turkmen alphabet|Turkmen]]<sup style=\"font-size:80%\" id=\"r-tk:3\">[[#n-tk|[55]]]</sup>\n| || || || || || || || || || || || || || || || || ||Ü|| || || || || || || || || || || ||Ý|| || || || || || || ||Ž|| ||\n|-\n![[Ume Sami language|Ume Sami]]\n| || || || || || || || || || || || || || || || || ||Ü|| || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Uyghur language|Uyghur]]\n| || || || || || || || || || || || || || || || || ||Ü|| || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Venda language#Writing system|Venda]]<sup style=\"font-size:80%\" id=\"r-ve:3\">[[#n-ve|[51]]]</sup>\n| || || || || || || || || || || || || ||Ṱ||Ú|| || || || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Vietnamese alphabet|Vietnamese]]<sup style=\"font-size:80%\" id=\"r-vi:3\">[[#n-vi|[26]]]</sup>\n| || || || || || || || || || || || || || ||Ú||Ù|| || || || || ||Ũ|| || ||Ụ|| || || || ||Ý||Ỳ|| || || ||Ỹ|| || || || ||\n|-\n![[Volapük]]\n| || || || || || || || || || || || || || || || || ||Ü|| || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Võro language|Võro]]\n| || || || || || || ||Š|| || || || || || || || || ||Ü|| || || || || || || || || || || || || || || || || || || ||Ž|| ||\n|-\n![[Walloon language|Walloon]]<sup style=\"font-size:80%\" id=\"r-wa:3\">[[#n-wa|[27]]]</sup>\n| || || || || || || || || || || || || || || ||Ù||Û|| || || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Welsh alphabet|Welsh]]<sup style=\"font-size:80%\" id=\"r-cy:3\">[[#n-cy|[28]]]</sup>\n| || || || || || || || || || || || || || ||Ú||Ù||Û||Ü|| || || || || || || ||Ẃ||Ẁ||Ŵ||Ẅ||Ý||Ỳ||Ŷ||Ÿ|| || || || || || ||\n|-\n![[West Frisian alphabet|West Frisian]]\n| || || || || || || || || || || || || || ||Ú|| ||Û||Ü|| || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Xhosa language|Xhosa]]\n| || || || || || || || || || || || || || ||Ú||Ù||Û||Ü|| || || || || || || || || || || || || || || || || || || || || ||\n|-\n![[Yoruba Language#Writing system|Yoruba]]<sup style=\"font-size:80%\" id=\"r-yo:3\">[[#n-yo|[56]]]</sup>\n| || || || || || || || || ||Ṣ|| || || || ||Ú||Ù||Û|| ||Ǔ|| || ||Ũ|| || || || || || || || || || || || || || || || || ||\n|}\n\n===Notes===\n# <span id=\"n-la\">[[#r-la|↑]][[#r-la:1|↑]][[#r-la:2|↑]][[#r-la:3|↑]]</span> In classical '''Latin''', the digraphs ''[[ch (digraph)|CH]]'', ''[[ph (digraph)|PH]]'', ''[[rh (digraph)|RH]]'', ''[[th (digraph)|TH]]'' were used in loanwords from [[Greek language|Greek]], but they were not included in the alphabet. The ligatures ''[[Æ]]'', ''[[Œ]]'' and ''[[W]]'', as well as lowercase letters, were added to the alphabet only in [[Middle Ages]]. The letters ''J'' and ''U'' were used as [[typography|typographical variants]] of ''I'' and ''V'', respectively, roughly until the [[Age of Enlightenment|Enlightenment]].\n# <span id=\"n-af\">[[#r-af|↑]][[#r-af:1|↑]][[#r-af:2|↑]][[#r-af:3|↑]]</span> In '''Afrikaans''', ''C'' and ''Q'' are only (and ''X'' and ''Z'' almost only) used in loanwords.\n# <span id=\"n-sq\">[[#r-sq|↑]][[#r-sq:1|↑]][[#r-sq:2|↑]][[#r-sq:3|↑]]</span> '''Albanian''' officially has the digraphs ''dh, gj, ll, nj, rr, sh, th, xh, zh'', which is sufficient to represent the [[Tosk Albanian|Tosk]] dialect.  The [[Gheg Albanian|Gheg]] dialect supplements the official alphabet with 6 nasal vowels, namely â, ê, î, ô, û, ŷ.\n# <span id=\"n-aat\">[[#r-aat|↑]][[#r-aat:1|↑]][[#r-aat:2|↑]][[#r-aat:3|↑]]</span> '''Arbëresh''' apparently requires the digraphs ''dh, gj, hj, ll, nj, rr, sh, th, xh, zh''.  Arbëresh has the distinctive hj, which is considered as a letter in its own right.\n# <span id=\"n-az\">[[#r-az|↑]][[#r-az:1|↑]][[#r-az:2|↑]][[#r-az:3|↑]]</span> A with diaeresis is used only as a replacement character for schwa if the latter cannot be used (it was replaced by the schwa one year later because it is the most common letter). These cases should be avoided!\n# <span id=\"n-eu\">[[#r-eu|↑]][[#r-eu|↑]][[#r-eu:2|↑]][[#r-eu:3|↑]]</span> '''Basque''' has several digraphs: ''dd, ll, rr, ts, tt, tx, tz''.  The ü, which is pronounced as /ø/, is required for various words in its Zuberoan dialect. ''C, Ç, Q, V, W, Y'' are used in foreign words, but are officially considered part of the alphabet (except Ç, which is considered a variant of C).\n# <span id=\"n-bm\">[[#r-bm|↑]]</span> '''Bambara''' also has the digraphs: ''kh''(only present in loanwords), ''sh'' (also written as ''ʃ''; only present in some dialects). Historically, ''è'' was used instead of ''ɛ'', ''ny'' was used instead of ''ɲ'', and ''ò'' was used instead of ''ɔ'' in Mali.\n# <span id=\"n-be\">[[#r-be|↑]][[#r-be:1|↑]][[#r-be:2|↑]][[#r-be:3|↑]]</span> '''Belarusian''' also has several digraphs: ''ch, dz, dź, dž''.\n# <span id=\"n-bi\">[[#r-bi|↑]][[#r-bi:1|↑]][[#r-bi:2|↑]][[#r-bi:3|↑]]</span> '''Bislama''' also has the digraph ''ng''.\n# <span id=\"n-br\">[[#r-br|↑]][[#r-br:1|↑]][[#r-br:2|↑]][[#r-br:3|↑]]</span> '''Breton''' also has the digraphs ''ch, c'h, zh''. ''C, Q, X'' are used in foreign words or digraphs only.\n# <span id=\"n-ca\">[[#r-ca:1|↑]][[#r-ca:2|↑]][[#r-ca:3|↑]]</span> '''Catalan''' also has a large number of digraphs: ''dj, gu, gü, ig, ix, ll, l·l, ny, qu, qü, rr, ss, tg, tj, ts, tx, tz''. The letters ''K'', ''Ñ'', ''Q'' ''W'', and ''Y'' are only used in loanwords or the digraphs mentioned.\n# <span id=\"n-ch\">[[#r-ch|↑]][[#r-ch:1|↑]][[#r-ch:2|↑]]</span> '''Chamorro''' also has the digraphs ''ch, ng''. ''C'' used only in digraphs.\n# <span id=\"n-co\">[[#r-co|↑]][[#r-co:1|↑]][[#r-co:2|↑]][[#r-co:3|↑]]</span> '''Corsican''' has the trigraphs: ''chj, ghj''.\n# <span id=\"n-hr\">[[#r-hr|↑]][[#r-hr:1|↑]][[#r-hr|↑]][[#r-hr:3|↑]][[#r-sr|↑]][[#r-sr:1|↑]][[#r-sr|↑]][[#r-sr:3|↑]]</span> '''Croatian''' also has the digraphs: ''dž'', ''lj'', ''nj''. It can also be written with four tone markers above on top of the vowels. Note that Croatian Latin is the same as Serbian Latin and [[Bosnian language|Bosnian]] and they both map 1:1 to [[Serbian Cyrillic]], where the three digraphs map to Cyrillic letters ''џ'', ''љ'' and ''њ'', respectively. Rarely and non-standardly, digraph ''dj'' is used instead of ''đ'' (like it was previously) (Cyrillic ''ђ''). [[Montenegrin alphabet|Montenegrin]] uses ''Ś'' and ''Ź'' in addition to the letters used in Croatian.\n# <span id=\"n-acy\">[[#r-acy|↑]][[#r-acy:1|↑]] </span> '''Cypriot Arabic''' also has the letters ''Θ'' and ''Δ''.\n# <span id=\"n-cs\">[[#r-cs|↑]][[#r-cs:1|↑]][[#r-cs:2|↑]][[#r-cs:3|↑]]</span> '''Czech''' also has the digraph ''ch'', which is considered a separate letter and is sorted between ''h'' and ''i''. While'' á, ď, é, ě, í, ň, ó, ť, ú, ů, ''and ''ý'' are considered separate letters, in collation they are treated merely as letters with diacritics. However, ''č, ř, š, ''and ž'' ''are actually sorted as separate letters. ''Q, W, X'' occur only in loanwords.\n# <span id=\"n-crx\">[[#r-crx|↑]] '''Dakelh''' also contains the letter ''&apos;'', which represents the glottal stop. The letters ''F'' and ''R'' are only used in loanwords.</span>\n# <span id=\"n-da\">[[#r-da:1|↑]][[#r-da:2|↑]][[#r-da|↑]][[#r-no:1|↑]][[#r-no:2|↑]][[#r-no:3|↑]]</span> The '''Norwegian''' alphabet is currently identical with the '''Danish''' alphabet. ''C'' is part of both alphabets and is not used in native Danish or Norwegian words (except some proper names), but occures quite frequently in well-established loanwords in Danish. Norwegian and Danish use é in some words such as ''én'', although é is considered a diacritic mark, while å, æ and ø are letters. ''Q, W, X, Z'' are not used except for names and some foreign words.\n# <span id=\"n-din\">[[#r-din|↑]]</span> '''Dinka''' also has the digraphs: ''dh'', ''nh'', ''ny'', ''th''. ''H'' is only present in these digraphs. Dinka also used the letters Ää, Ëë, Ïï, Öö, Ɛ̈ɛ̈, Ɔ̈ɔ̈ (the last two which do not exist as precomposed characters in Unicode)\n# <span id=\"n-nl\">[[#r-nl:1|↑]][[#r-nl:2|↑]][[#r-nl:3|↑]]</span> The status of ''[[IJ (letter)|ij]]'' as a letter, ligature or digraph in '''Dutch''' is disputed. ''C'' (outside the digraph ''ch'') ''Q'', ''X'', and ''Y'' occur mostly in foreign words. Letters with grave and letters with circumflex occur only in loanwords.\n# <span id=\"n-en\">[[#r-en:1|↑]][[#r-en:2|↑]][[#r-en:3|↑]]</span> '''English''' generally now uses extended Latin letters only in loan words, such as fiancé, fiancée, and résumé.  Rare publication guides may still use the dieresis on words, such as \"coöperate\", rather than the now-more-common \"co-operate\" (UK) or \"cooperate\" (US).  For a fuller discussion, see articles branching from [[Lists of English words of international origin]], which was used to determine the diacritics needed for more unambiguous English.  However, an ''é'' or ''è'' is sometimes used in poetry to show that a normally silent vowel is to be pronounced, as in \"blessèd\".\n# <span id=\"n-tl\">[[#r-tl|↑]][[#r-tl:1|↑]][[#r-tl:2|↑]][[#r-tl:3|↑]]</span> '''Filipino''' also known as '''Tagalog''' also uses the digraph ''ng'', even originally with a large tilde that spanned both n and g (as in n͠g) when a vowel follows the digraph. (The use of the tilde over the two letters is now rare). Only ñ is required for everyday use (only in loanwords). The accented vowels are used in dictionaries to indicate pronunciation, and g with tilde is only present in older works.\n# <span id=\"n-fr\">[[#r-fr:1|↑]][[#r-fr:2|↑]][[#r-fr:3|↑]]</span> Uppercase diacritics in '''French''' are often (incorrectly) thought of as being optional, but the official rules of French orthography designate accents on uppercase letters as obligatory in most cases. Many pairs or triplets are read as digraphs or trigraphs depending on context, but are not treated as such lexicographically: consonants ''ph, (ng), th, gu/gü, qu, ce, ch/(sh/sch), rh''; vocal vowels ''(ee), ai/ay, ei/ey, eu, au/eau, ou''; nasal vowels ''ain/aim, in/im/ein, un/um/eun, an/am, en/em, om/on''; the half-consonant ''-(i)ll-''; half-consonant and vowel pairs ''oi, oin/ouin, ien, ion''. When rules that govern the French orthography are not observed, they are read as separate letters, or using an approximating phonology of a foreign language for loan words, and there are many exceptions. In addition, most final consonants are mute (including those consonants that are part of feminine, plural, and conjugation endings). Y with diaeresis and U with diaeresis are only used in certain geographical names and proper names plus their derivatives, or, in the case of U with diaeresis, newly proposed reforms. Eg. capharnaüm `shambles' is derived from the proper name Capharnaüm. ''Æ'' occurs only in Latin or Greek loanwords.\n# <span id=\"n-ff\">[[#r-ff|↑]]</span> '''Fula''' has ''X'' as part of the alphabet in all countries except Guinea, Guinea-Bissau, Liberia, and Sierra Leone (used only in loanwords in these countries). ''Ɠ'', which is used only in loanwords (but still part of the alphabet), is used in Guinea only. Fula also uses the digraphs ''mb'' (In Guinea spelled ''nb''), ''nd'', ''ng'', and ''nj''. ''aa'', ''ee'', ''ii'', ''oo'', and ''uu'' are part of the alphabet in all countries except Guinea, Guinea-Bissau, Liberia, and Sierra Leone. ''Ƴ'' is used in all countries except for Nigeria, where it is written ''&apos;y''. ''Ŋ'' is used in all countries except for Nigeria. ''Ɲ'' is used in Guinea, Mali, and Burkina Faso, ''Ñ'' is used in Senegal, Gambia, Mauritania, Guinea-Bissau, Liberia, and Sierra Leone, and the digraph ''ny'' is used in Niger, Cameroon, Chad, Central African Republic, and Nigeria. The apostrophe is a letter (representing the glottal stop) in Guinea-Bissau, Liberia, and Sierra Leone. ''Q'', ''V'', and ''Z'' are only used in loanwords, and are not part of the alphabet.\n# <span id=\"n-gl\">[[#r-gl|↑]][[#r-gl|↑]][[#r-gl|↑]][[#r-gl|↑]]</span> '''Galician'''. The standard of 1982 set also the digraphs ''gu'', ''qu'' (both always before ''e'' and ''i''), ''ch, ll, nh'' and ''rr''. In addition, the standard of 2003 added the grapheme ''ao'' as an alternative writing of ''ó''. Although not marked (or forgotten) in the list of digraphs, they are used to represent the same sound, so the sequence ''ao'' should be considered as a digraph. Note also that ''nh'' represents a [[velar]] nasal (not a [[palatal]] as in [[Portuguese language|Portuguese]]) and is restricted only to three feminine words, being either [[demonstrative]] or [[pronoun]]: ''unha'' ('a' and 'one'), ''algunha'' ('some') and ''ningunha'' ('not one'). The Galician ''[[reintegracionismo]]'' movement uses it as in Portuguese.\n# <span id=\"n-de\">[[#r-de|↑]][[#r-de:1|↑]][[#r-de:2|↑]][[#r-de:3|↑]]</span> '''German''' also retains most original letters in French loan words. Swiss German does not use ''ß'' any more. The [[long s]] ''(ſ)'' was in use until the mid-20th century. ''Sch'' is usually not treated like a true trigraph, neither are ''ch'', ''ck'', ''st'', ''sp'', ''th'', (''ph'', ''rh'') and ''qu'' digraphs. ''Q'' only appears in the sequence ''qu'' and in loanwords, while ''x'' and ''y'' are found almost only in loan words. The capital ''ß'' (''ẞ'') is almost never used, ''ß'' is replaced with ''SS'' when writing all-caps. The accented letters (other than the letters ''ä'', ''ö'', ''ü'', and ''ß'') are used only in loanwords.\n# <span id=\"n-gn\">[[#r-gn|↑]][[#r-gn:1|↑]][[#r-gn:2|↑]][[#r-gn:3|↑]]</span> '''Guaraní''' also uses digraphs ''ch, mb, nd, ng, nt, rr'' and the glottal stop ''&apos;''. ''B'', ''C'', and ''D'' are only used in these digraphs.\n# <span id=\"n-gwi\">[[#r-gwi|↑]] '''Gwich'in''' also contains the letter ''&apos;'', which represents the glottal stop. Gwich'in also uses the letters ''Ą̀'', ''Ę̀'', ''Į̀'', ''Ǫ̀'', and ''Ų̀'', which are not available as precomposed characters in Unicode. Gwich'in also uses the digraphs: ''aa, ąą, àà, ą̀ą̀, ch, [ch'], ddh, dh, dl, dr, dz, ee, ęę, èè, ę̀ę̀, gh, ghw, gw, ii, įį, ìì, į̀į̀, kh, kw, [k'], nd, nh, nj, oo, ǫǫ, òò, ǫ̀ǫ̀, rh, sh, shr, th, tl, [tl'], tr, [tr'], ts, [ts'], tth, [tth'], [t'], uu, ųų, ùù, ų̀ų̀, zh, zhr''. The letter ''C'' is only used the digraphs above. The letters ''B'', ''F'', and ''M'' are only used in loanwords.</span>\n# <span id=\"n-ha\">[[#r-ha|↑]][[#r-ha:1|↑]][[#r-ha:2|↑]][[#r-ha:3|↑]]</span> '''Hausa''' has the digraphs: ''sh, ts''. Vowel length and tone are usually not marked. Textbooks usually use macron or doubled vowel to mark the length, grave to mark the low tone and circumflex to mark the falling tone. Therefore, in some systems, it is possible that macron is used in combination with grave or circumflex over a, e, i, o or u.\n# <span id=\"n-hu\">[[#r-hu|↑]][[#r-hu:1|↑]][[#r-hu:2|↑]][[#r-hu:3|↑]]</span> '''Hungarian''' also has the digraphs: ''cs, dz, gy, ly, ny, sz, ty, zs''; and the trigraph: ''dzs''. Letters ''á, é, í, ó, ő, ú, ''and ''ű'' are considered separate letters, but are collated as variants of a, e, i, o, ö, u, and ü.\n# <span id=\"n-ga\">[[#r-ga|↑]][[#r-ga:1|↑]][[#r-ga:2|↑]][[#r-ga:3|↑]]</span> '''Irish''' formerly used the dot diacritic in ''ḃ, ċ, ḋ, ḟ, ġ, ṁ, ṗ, ṡ, ṫ''. These have been replaced by the digraphs: ''bh, ch, dh, fh, gh, mh, ph, sh, th'' except for in formal instances. ''V'' only occurs in onomotopoeia, such as ''vácarnach'', ''vác'', or ''vrác'', or in rare alternative spellings ''víog'' and ''vís'' (usually spelled ''bíog'' and ''bís''), or in loanwords. ''Z'' only occurs in the West Muskerry dialect in the digraph ''zs'' (a rare eclipsis of s, spelled s in other dialects and the language proper) or in loanwords.\n# <span id=\"n-ig\">[[#r-ig|↑]]</span> '''Igbo''' writes Ṅṅ alternatively as N̄n̄. Igbo has the digraphs: ''ch, gb, gh, gw, kp, kw, nw, ny, sh''. ''C'' is only used in the digraph before. Also, vowels take a grave accent, an acute accent, or no accent, depending on tone.\n# <span id=\"n-it\">[[#r-it|↑]][[#r-it:1|↑]][[#r-it:2|↑]][[#r-it:3|↑]]</span> '''Italian''' also has the digraphs: ''ch, gh, gn, gl, sc''. J, K, W, X, Y are used in foreign words. X is also used for native words derived from Latin and Greek; J is also used for just a few native words, mainly names of persons (as in Jacopo) or of places (as in [[Jesolo]] and [[Jesi]]), in which is always pronounced as letter I. While it does not occur in ordinary running texts, geographical names on maps are often written only with acute accents. The circumflex is used on an -i ending that was anciently written -ii (or -ji, -ij, -j, etc.) to distinguish homograph plurals and verb forms: e.g. e.g. principî form principi, genî from geni.\n# <span id=\"n-kaa\">[[#r-kaa|↑]]</span> '''Karakalpak''' also has the digraphs: ''ch, sh''. ''A', G', I', N', O', U&apos;'' are considered as letters. ''C, F, V'' are used in foreign words.\n# <span id=\"n-kk\">[[#r-kk|↑]]</span> '''Kazakh''' also has the digraphs: ''ch, ıa, ıo, ıý, sh, ts''. ''C'' is only used in the digraph ''ch''. ''F, H, V'' and the digraphs ''ch'', ''ıo'', and ''ts'' are used in foreign words.\n# <span id=\"n-lv\">[[#r-lv|↑]][[#r-lv:1|↑]][[#r-lv:2|↑]][[#r-lv:3|↑]]</span> '''Latvian''' also has the digraphs: ''dz, dž, ie.'' ''Dz'' and ''dž'' are occasionally considered separate letters of the alphabet in more archaic examples, which have been published as recently as the 1950s; however, modern alphabets and teachings discourage this due to an ongoing effort to set decisive rules for Latvian and eliminate barbaric words accumulated during the Soviet occupation. The digraph \"ie\" is never considered a separate letter. ''Ō'', ''Ŗ'', and the digraphs ''CH'' (only used in loanwords) and ''UO'' are no longer part of the alphabet, but are still used in certain dialects and newspapers that use the old orthography. ''Y'' is used only in certain dialects and not in the standard language. ''F'' and ''H'' are only used in loanwords.\n# <span id=\"n-ls\">[[#r-ls|↑]][[#r-ls:1|↑]][[#r-ls:2|↑]][[#r-ls:3|↑]]</span> A nearby language, '''Pite Sami''', uses '''Lule Sami''' orthography but also includes the letter '''Đđ''', which is not in Lule Sami.\n# <span id=\"n-lt\">[[#r-lt|↑]][[#r-lt:1|↑]][[#r-lt:2|↑]][[#r-lt:3|↑]]</span> '''Lithuanian''' also has the digraphs: ''ch, dz, dž, ie, uo''. However, these are not considered separate letters of the alphabet. ''F'', ''H'', and the digraph ''CH'' are only used in loanwords. Demanding publications such as dictionaries, maps, schoolbooks etc need additional diacritical marks to differentiate homographs. Using grave accent on A, E, I, O, U, acute accent on all vowels, and tilde accent on all vowels and on L, M, N and R. Small E and I (also with ogonek) must retain the dot when additional accent mark is added to the character; the use of ì and í (note the missing dot) is considered unacceptable.\n# <span id=\"n-liv\">[[#r-liv|↑]][[#r-liv:1|↑]][[#r-liv:2|↑]][[#r-liv:3|↑]]</span> In '''Livonian''', the letters ''Ö, Ȫ, Y, Ȳ'' were used by the older generation, but the younger generation merged these sounds; Around the late 1990's, these letters were removed from the alphabet.\n# <span id=\"n-mt\">[[#r-mh|↑]][[#r-mh:1|↑]][[#r-mh:2|↑]][[#r-mh:3|↑]]</span> '''Marshallese''' often uses the old orthography (because people did not approve of the new orthography), which writes ''ļ'' as l, ''m̧'' as m, ''ņ'' as n, ''p'' as b, ''o̧'' as o at the ends of words or in the word ''yokwe'' (also spelled iakwe under the old orthography; under the new orthography, spelled io̧kwe), but a at other places, and ''d'' as dr before vowels, or r after vowels. The old orthography writes ''ā'' as e in some words, but ā in others; it also writes ''ū'' as i between consonants. The old orthography writes geminates and long vowels as two letters instead. Allophones of {{IPA|/ɘ/}}, written as only e o ō in the new orthography, are also written as i u and very rarely, ū. The letter ''Y'' only occurs in the words ''yokwe'' or the phrase ''yokwe yuk'' (also spelled iakwe iuk in the old orthography or io̧kwe eok in the new orthography).\n# <span id=\"n-mt\">[[#r-mt|↑]][[#r-mt:1|↑]][[#r-mt:2|↑]][[#r-mt:3|↑]]</span> '''Maltese''' also has the digraphs: ''ie, għ''.\n# <span id=\"n-mi\">[[#r-mi|↑]]</span> '''Māori''' uses ''g'' only in ''ng'' digraph. ''Wh'' is also a digraph.\n# <span id=\"n-moh\">[[#r-moh|↑]]</span> Some '''Mohawk''' speakers use orthographic ''i'' in place of the consonant ''y''. The glottal stop is indicated with an apostrophe ''’'' and long vowels are written with a colon '':''.\n# <span id=\"n-mis\">[[#r-mis|↑]]</span> '''[Na'vi]''' uses the letter ''ʼ'' and the digraphs ''aw'', ''ay'', ''ew'', ''ey'', ''kx'', ''ll'', ''ng'' (sometimes written as ''G''), ''px'', ''rr'', ''ts'' (sometimes written as ''C''), ''tx''. ''G'' (in standard orthography) and ''X'' are used only in digraphs.\n# <span id=\"n-mss\">[[#r-mss|↑]][[#r-mss:1|↑]][[#r-mss:2|↑]] '''Massachusett''' also uses the digraphs ''ch, ee, sh, ty'' and the letter ''8'' (which was previously written ''oo''). ''C'' is only used in the digraph ''ch''.</span>\n# <span id=\"n-om\">[[#r-om|↑]] '''Oromo''' uses the following digraphs: ''ch, dh, ny, ph, sh''. ''P'' is only used in the digraph ''ph'' and loanwords. ''V'' and ''Z'' are only used in loanwords.</span>\n# <span id=\"n-pap\">[[#r-pap|↑]][[#r-pap:1|↑]][[#r-pap:2|↑]][[#r-pap:3|↑]] '''Papiamento''' also has the digraphs: ''ch, dj, sh, zj''. ''Q'' and ''X'' are only used in loanwords and proper names. ''J'' is only used in digraphs, loanwords, and proper names. Papiamentu in Bonaire and Curaçao is different from Papiamento in Aruba in the following ways: Papiamento in Aruba uses a more etymological spelling, so Papiamento uses ''C'' in native words outside of the digraph ''ch'', but Papiamentu in Bonaire and Curaçao does not. Papiamentu in Bonaire and Curaçao uses ''È'', ''Ò'', ''Ù'', and ''Ü'' for various sounds and ''Á'', ''É'', ''Í'', ''Ó'', and ''Ú'' for stress, but Papiamento in Aruba does not use these letters.</span>\n# <span id=\"n-pms\">[[#r-pms:1|↑]]</span> '''Piedmontese''' also uses the letter ''n-'' to indicate a velar nasal N-sound (pronounced as the gerundive termination in going), which usually precedes a vowel, as in lun-a [moon].\n# <span id=\"n-pinyin\">[[#r-pinyin|↑]][[#r-pinyin:1|↑]][[#r-pinyin:2|↑]][[#r-pinyin:3|↑]]</span> '''Pinyin''' has four tone markers that can go on top of any of the six vowels ''(a, e, i, o, u, ü)''; e.g.: macron ''(ā, ē, ī, ō, ū, ǖ)'', acute accent ''(á, é, í, ó, ú, ǘ)'', caron ''(ǎ, ě, ǐ, ǒ, ǔ, ǚ)'', grave accent ''(à, è, ì, ò, ù, ǜ)''. It also uses the digraphs: ''ch, sh, zh''.\n# <span id=\"n-pl\">[[#r-pl|↑]][[#r-pl:1|↑]][[#r-pl:2|↑]][[#r-pl:3|↑]]</span> '''Polish''' also has the digraphs: ''ch, cz, dz, dż, dź, sz, rz''.\n# <span id=\"n-pt\">[[#r-pt|↑]][[#r-pt:1|↑]][[#r-pt:2|↑]][[#r-pt:3|↑]]</span> '''Portuguese''' also uses the digraphs ''ch, lh, nh, rr, ss''. The [[Umlaut (diacritic)|trema]] on ''ü'' was used in [[Brazilian Portuguese]] before 2009. The grave accent was used on e, i, o, and u, until 1973. The letters ''è''and ''ò'' are used in geographical names outside Europe and not part of the language proper. The now abandoned practice was to indicate underlying stress in words ending in -mente -- sòmente, ùltimamente etc. Neither the digraphs nor accented letters are considered part of the alphabet.\n# <span id=\"n-ro\">[[#r-ro|↑]]</span> '''Romanian''' normally uses a [[comma (diacritic)|comma]] diacritic below the letters ''s'' and ''t'' (''ș, ț''), but it is frequently replaced with an attached [[cedilla]] below these letters (''ş, ţ'') due to past lack of standardization. ''K'', ''Q'', ''W'', ''X'', and ''Y'' occur only in loanwords.\n# <span id=\"n-rom\">[[#r-rom|↑]][[#r-rom:1|↑]][[#r-rom|↑]][[#r-rom:3|↑]]</span> '''Romani''' has the digraphs: ''čh, dž, kh, ph, th''.\n# <span id=\"n-sk\">[[#r-sk:1|↑]][[#r-sk:2|↑]][[#r-sk:3|↑]]</span> '''Slovak''' also has the digraphs ''dz, dž,'' and'' ch,'' which are considered separate letters. While'' á, ä, ď, é, í, ĺ, ň, ó, ô, ŕ, ť, ú, ''and ''ý'' are considered separate letters, in collation they are treated merely as letters with diacritics. However, ''č, ľ, š, ''and ''ž'', as well as the digraphs,'' ''are actually sorted as separate letters. ''Q, W, X, Ě, Ö, Ř, Ü'' occur only in loanwords.\n# <span id=\"n-wen\">[[#r-wen|↑]][[#r-wen:1|↑]][[#r-wen:2|↑]][[#r-wen:3|↑]][[#r-wen:4|↑]]</span> ''Sorbian'' also uses the digraphs: ''ch'', ''dź''. ''Ř'' is only used in Upper Sorbian, and ''Ŕ'', ''Ś'', and ''Ź'' (outside the digraph ''dź'') are only used in Lower Sorbian.\n# <span id=\"n-es\">[[#r-es:1|↑]][[#r-es:2|↑]][[#r-es:3|↑]]</span> '''Spanish''' uses several digraphs to represent single sounds: ''ch'', ''gu'' (preceding ''e'' or ''i''), ''ll'', ''qu'', ''rr''; of these, the digraphs ''[[ch (digraph)|ch]]'' and ''[[ll]]'' were traditionally considered individual letters with their own name (''che'', ''elle'') and place in the alphabet (after ''c'' and ''l'', respectively), but in order to facilitate international compatibility the [[Real Academia Española|Royal Spanish Academy]] decided to cease this practice in 1994 and all digraphs are now collated as combinations of two separate characters. While cedilla is etymologically Spanish diminutive of ceda (z) and Sancho Pança is the original form in Cervantes books, C with cedilla ''ç'' is now completely displaced by z in contemporary language. In poetry, the diaeresis may be used to break a diphthong into separate vowels. Regarding that usage, Ortografía de la lengua española states that \"diaeresis is usually placed over the closed vowel [i.e. 'i' or 'u'] and, when both are closed, generally over the first\". In this context, the use of ï is rare, but part of the normative orthography.\n# <span id=\"n-sv\">[[#r-sv|↑]]</span> '''Swedish''' uses é in well integrated loan words like ''[[idea|idé]]'' and ''[[army|armé]]'', although é is considered a modified e, while å, ä, ö are letters. á and à are rarely used words. W and z are used in some integrated words like [[World Wide Web|webb]] and [[wikt:zone|zon]]. Q, ü, è, and ë are used for names only, but exist in Swedish names. For foreign names ó, ç, ñ and more are sometimes used, but usually not. Swedish has many digraphs and some trigraphs. ''ch'', ''dj'', ''lj'', ''rl'', ''rn'', ''rs'', ''sj'', ''sk'', ''si'', ''ti'', ''sch'', ''skj'', ''stj'' and others are usually pronounced as one sound.\n# <span id=\"n-aeb\">[[#r-aeb:1|↑]][[#r-aeb:2|↑]][[#r-aeb:3|↑]]</span> '''Tunisian Arabic''' also uses the digraph: ''għ''. Usually Tunisian Arabic is written in the Arabic script; this Latin alphabet, which is based on Maltese, was not created until 2015.\n# <span id=\"n-tn\">[[#r-tn|↑]][[#r-tn:1|↑]][[#r-tn:2|↑]][[#r-tn:3|↑]]</span> '''Tswana''' also has the digraphs: ''kg, kh, ng, ph, th, tl, tlh, ts, tsh, tš, tšh''. The letters ''C'', ''Q'', and ''X'' only appear in onomotopeia and loanwords. The letters ''V'' and ''Z'' only appear in loanwords.\n# <span id=\"n-tk\">[[#r-tk|↑]][[#r-tk:1|↑]][[#r-tk:2|↑]][[#r-tk:3|↑]]</span> '''Turkmen''' had a slightly different alphabet in 1993–1995 (which used some unusual letters) ''Ýý'' was written as ''¥ÿ'', ''Ňň'' was written as ''Ññ'', and ''Şş'' was written as ''$¢'', and ''Žž'' was written ''£⌠'' (so that all  characters were available in [[Code page 437]]). In the new alphabet, all characters are available in [[ISO/IEC 8859-2]].\n# <span id=\"n-uli\">[[#r-uli|↑]][[#r-uli:1|↑]][[#r-uli:2|↑]]</span> '''Ulithian''' also has the digraphs: ''ch'', ''[l']'', ''mw'', ''ng''. ''C'' used only in digraphs.\n# <span id=\"n-uz\">[[#r-uz|↑]]</span> '''Uzbek''' also has the digraphs: ''ch, ng, sh'' considered as letters. ''C'' used only in digraphs. ''G&#39;, O&#39;'' and apostrophe (') are considered as letters. These letters have preferred typographical variants: Gʻ, Oʻ and ʼ respectively.\n# <span id=\"n-ve\">[[#r-ve|↑]][[#r-ve:1|↑]][[#r-ve:2|↑]][[#r-ve:3|↑]]</span> '''Venda''' also has the digraphs and trigraphs: ''bv, bw, dz, dzh, dzw, fh, hw, kh, khw, ng, ny, nz, ṅw, ph, pf, pfh, sh, sw, th, ts, tsh, tsw, ty, ṱh, vh, zh, zw''. ''C, J, Q'' are used in foreign words.\n# <span id=\"n-vi\">[[#r-vi|↑]][[#r-vi:1|↑]][[#r-vi:2|↑]][[#r-vi:3|↑]]</span> '''Vietnamese''' has seven additional base letters: ''ă â đ ê ô ơ ư''. It uses five tone markers that can go on top (or below) any of the 12 vowels ''(a, ă, â, e, ê, i, o, ô, ơ, u, ư, y)''; e.g.: grave accent ''(à, ằ, ầ, è, ề, ì, ò, ồ, ờ, ù, ừ, ỳ)'', hook above ''(ả, ẳ, ẩ, ẻ, ể, ỉ, ỏ, ổ, ở, ủ, ử, ỷ)'', tilde ''(ã, ẵ, ẫ, ẽ, ễ, ĩ, õ, ỗ, ỡ, ũ, ữ, ỹ)'', acute accent ''(á, ắ, ấ, é, ế, í, ó, ố, ớ, ú, ứ, ý)'', and dot below ''(ạ, ặ, ậ, ẹ, ệ, ị, ọ, ộ, ợ, ụ, ự, ỵ)''. It also uses several digraphs and trigraphs{{spaced ndash}} ''ch, gh, gi, kh, ng, ngh, nh, ph, th, tr''{{spaced ndash}} but they are no longer considered letters.\n# <span id=\"n-wa\">[[#r-wa|↑]][[#r-wa:1|↑]][[#r-wa:2|↑]][[#r-wa:3|↑]]</span> '''Walloon''' has the digraphs and trigraphs: ''ae, ch, dj, ea, jh, oe, oen, oi, sch, sh, tch, xh''. The letter ''X'' outside the digraph ''xh'' is in some orthographies, but not the default two. The letter ''Q'' is in some orthographies (including one default orthography), but not in the other default orthography. Also in some orthographies are ''À'', ''Ì'', ''Ù'', ''Ë'', ''Ö'', and even ''E̊'' (which is not available as a precomposed character in Unicode)\n# <span id=\"n-cy\">[[#r-cy|↑]][[#r-cy:1|↑]][[#r-cy:2|↑]][[#r-cy:3|↑]]</span> '''Welsh''' has the digraphs ''ch'', ''dd'', ''ff'', ''ng'', ''ll'', ''ph'', ''rh'', ''th''. Each of these digraphs is collated as a separate letter, and ''ng'' comes immediately after ''g'' in the alphabet. It also frequently uses [[circumflex]]es, and occasionally uses [[Diaeresis (diacritic)|diaereses]], [[acute accent]]s and [[grave accent]]s, on its seven vowels (''a'', ''e'', ''i'', ''o'', ''u'', ''w'', ''y''), but accented characters are not regarded as separate letters of the alphabet.\n# <span id=\"n-xh\">[[#r-xh|↑]][[#r-xh:1|↑]][[#r-xh:2|↑]][[#r-xh:3|↑]]</span> '''Xhosa''' has a large number of digraphs, trigraphs, and even one tetragraph are used to represent various phonemes: ''bh, ch, dl, dy, dz, gc, gq, gr, gx, hh, hl, kh, kr, lh, mb, mf, mh, nc, ndl, ndz, ng, ng', ngc, ngh, ngq, ngx, nh, nkc, nkq, nkx, nq, nx, ntl, ny, nyh, ph, qh, rh, sh, th, ths, thsh, ts, tsh, ty, tyh, wh, xh, yh, zh''. It also occasionally uses [[acute accent]]s, [[grave accent]]s, [[circumflex]]es, and [[Diaeresis (diacritic)|diaereses]] on its five vowels ''(a, e, i, o, u)'', but accented characters are not regarded as separate letters of the alphabet.\n# <span id=\"n-ya\">[[#r-ya|↑]][[#r-ya:1|↑]][[#r-ya:2|↑]]</span> '''Yapese''' has the digraphs and trigraphs: ''aa, ae, ch, ea, ee, ii, [k'], [l'], [m'], [n'], ng, [ng'], oe, oo, [p'], [t'], th, [th'], uu, [w'], [y']''. ''Q'', representing the glottal stop, is not always used. Often an apostrophe is used to represent the glottal stop instead. ''C'' is used only in digraphs. ''H'' is used only in digraphs and loanwords. ''J'' is used only in loanwords.\n# <span id=\"n-yo\">'''Yoruba'''</span> uses the digraph ''gb''. Also, vowels take a grave accent, an acute accent, or no accent, depending on tone. Although the \"dot below\" diacritic is widely used, purists prefer a short vertical underbar (Unicode COMBINING VERTICAL LINE BELOW U+0329) - this resembles the IPA notation for a syllabic consonant, attached to the base of the letter (E, O or S). The seven Yoruba vowels (A, E, E underbar, I, O, O underbar, U) can be uttered in three different tones: high (acute accent); middle (no accent) and low (grave accent). The letters M and N, when written without diacritics, indicate nasalisation of the preceding vowel. M and N also occur as syllabics - in these circumstances, they take acute or grave tonal diacritics, like the vowels. Middle tone is marked with a macron to differentiate it from the unmarked nasalising consonants. A tilde was used in older orthography (still occasionally used) to indicate a double vowel. This is tonally ambiguous, and has now been replaced by showing the paired vowels, each marked with the appropriate tones.\nHowever, where a double vowel has the tonal sequence high-low or low-high, it may optionally be replaced by a single vowel with a circumflex (high-low) or caron (low-high), eg. á + à = â; à + á = ǎ.\n# <span id=\"n-zun\">[[#r-zun|↑]][[#r-zun:1|↑]] '''Zuni''' contains the glottal stop ''&apos;'' and the digraph: ''ch''; ''C'' is only used in that digraph. The other digraphs ''kw'', ''sh'', and ''ts'' are not part of the alphabet.</span>\n\n==Miscellanea==\n*[[Africa Alphabet]]\n*[[African reference alphabet]]\n*[[Beghilos]]\n*[[Dinka alphabet]]\n*[[Gaj's Latin alphabet]], is the only script of the [[Croatian language|Croatian]] and [[Bosnian language|Bosnian]] [[standard language]]s in current use, and one of the two scripts of the [[Serbian language|Serbian]] standard language.\n*[[Hawaiian alphabet]]\n*[[Initial Teaching Alphabet]]\n*[[International Phonetic Alphabet]]\n*[[Łatynka]] for Ukrainian\n*[[Leet]] (1337 alphabet)\n*[[Romanization]] schemes\n*[[Romany alphabet]] for most Romany languages\n*[[Sami languages|Sámi Latin alphabet]]\n*[[Standard Alphabet by Lepsius]]\n*[[Tatar alphabet]], similar to Turkish alphabet and [[Yañalif|Jaꞑalif]] as a part of [[Uniform Turkic alphabet]]\n*[[Uralic Phonetic Alphabet]]\n\n==See also==\n*[[Diacritic]]\n*[[Latin-script alphabet]]\n*[[Latin-script multigraph]]\n*[[Latin characters in Unicode]]\n*[[List of Latin letters]]\n*[[List of precomposed Latin characters in Unicode]]\n*[[Romanization]]\n*[[Typographical ligature]]\n*[[Writing systems of Africa#Latin|Writing systems of Africa]]\n;Categories\n*{{c|Uncommon Latin letters}}\n*{{c|Specific letter-diacritic combinations}}\n*{{c|Latin-script ligatures}}\n*{{c|Phonetic transcription symbols}}\n\n==Footnotes==\n{{Reflist}}\n\n==External links==\n*[[Michael Everson]]'s [http://www.evertype.com/alphabets Alphabets of Europe]\n*[http://euro.typo.cz Typo.cz Information on Central European typography and typesetting]\n*[http://www.eki.ee/letter/ Letter database of the Institute of Estonian Language]\n*[http://unicode.org/repos/cldr-tmp/trunk/diff/by_type/misc.exemplarCharacters.html Unicode language coverage tables]\n*[http://diacritics.typo.cz Diacritics Project – All you need to design a font with correct accents]\n\n{{Latin script}}\n\n{{DEFAULTSORT:Latin alphabets}}\n[[Category:Collation]]\n[[Category:Latin alphabets| ]]\n[[Category:Writing-related lists]]"
    },
    {
      "title": "Natural sort order",
      "url": "https://en.wikipedia.org/wiki/Natural_sort_order",
      "text": "'''Natural sort order''' is an [[collation|ordering of strings]] in [[alphabetical order]], except that multi-digit numbers are ordered as a single character. Natural sort order has been promoted as being more human-friendly (\"natural\") than the machine-oriented pure alphabetical order.<ref>{{cite web|url=http://blog.codinghorror.com/sorting-for-humans-natural-sort-order/|title=Sorting for Humans : Natural Sort Order|website=blog.codinghorror.com}}</ref>\n\nFor example, in alphabetical sorting \"z11\" would be sorted before \"z2\" because \"1\" is sorted as smaller than \"2\", while in natural sorting \"z2\" is sorted before \"z11\" because \"2\" is sorted as smaller than \"11\".\n\nAlphabetical sorting:\n# z11\n# z2\nNatural sorting:\n# z2\n# z11\n\nFunctionality to sort by natural sort order is built into many programming languages and libraries.<ref>{{cite web|url=http://php.net/manual/en/function.natsort.php|title=PHP: natsort - Manual|website=php.net}}</ref><ref>{{cite web|url=http://search.cpan.org/~bingos/Sort-Naturally-1.03/lib/Sort/Naturally.pm|title=Sort::Naturally - search.cpan.org|website=search.cpan.org}}</ref><ref>{{cite web|url=https://github.com/SethMMorton/natsort|title=natsort: Simple yet flexible natural sorting in Python.|first=Seth M.|last=Morton|publisher=|via=PyPI}}</ref><ref>{{cite web|url=http://www.mathworks.com/matlabcentral/fileexchange/34464-customizable-natural-order-sort|title=Customizable Natural-Order Sort - File Exchange - MATLAB Central|publisher=}}</ref>\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://www.text-filter.com/Sort-Text-List-Online-Alphabetize-Arrange-Lines-Word-Text-Alphabetically-Numerically-Natural-Abc-Order-Arranger-Alphabetical-Sorter.htm Alphabetizer] Natural Sort Order.\n[[Category:Collation]]"
    },
    {
      "title": "Radical (Chinese characters)",
      "url": "https://en.wikipedia.org/wiki/Radical_%28Chinese_characters%29",
      "text": "{{Redirect|Bushu|the former Japanese province|Musashi Province}}\n{{Use dmy dates|date=July 2013}}\n[[File:媽部首.svg|right|thumb|upright=1.2|In the [[traditional Chinese character]] 媽 ''mā'' \"mother\", the left part is the radical 女 ''nǚ'' \"female\". In this case the radical is the semantic component of a [[phono-semantic]] compound, while the right part, 馬 ''mǎ'' \"horse\", is the phonetic component.]]\n{{Table Hanzi}}\n\nA Chinese '''radical''' ({{zh|c=部首|p=bùshǒu|l=section header}}) is a graphical component of a [[Chinese character]] under which the character is traditionally listed in a Chinese dictionary.  This component is often a [[determinative|semantic indicator]] (that is, an indicator of the meaning of the character), though in some cases the original semantic connection has become obscure, owing to changes in character meaning over time.  In other cases, the radical may be a [[Chinese character classification#Rebus_.28phonetic_loan.29_characters|phonetic component]] or even an artificially extracted portion of the character.\n\nThe English term \"radical\" is based on an analogy between the structure of characters and [[inflection]] of words in European languages.{{efn|As [[Léon Wieger]] explains:\n{{quote|The [[inflection|inflected]] words of European languages are decomposed into ''radical'' and ''termination''. The radical gives the meaning; the termination indicates case, time, mood. The first sinologists applied those grammatical terms belonging to inflected languages, to the Chinese language which is not an inflected one.{{sfn|Wieger|1927|p=14}}}}}} Radicals are also sometimes called \"classifiers\", but this name is more commonly applied to [[classifier (linguistics)|grammatical classifiers]] (measure words).{{sfn|Wilkinson|2013|p=34}}\n\n== History ==\n{{see also|List of Shuowen Jiezi radicals|List of Kangxi radicals}}\nIn the earliest Chinese dictionaries, such as the ''[[Erya]]'' (3rd century BC), characters were grouped together in broad semantic categories.\nBecause the vast majority of characters are [[phono-semantic]] compounds, combining a semantic component with a phonetic component, each semantic component tended to recur within a particular section of the dictionary. In the 2nd century AD, the [[Han dynasty]] scholar [[Xu Shen]] organized his etymological dictionary ''[[Shuowen Jiezi]]'' by selecting 540 recurring graphic elements he called ''bù'' (部 , \"categories\").{{sfn|Wilkinson|2013|p=74}} Most were common semantic components, but they also included shared graphic elements such as a dot or horizontal stroke. Some were even artificially extracted groups of strokes, termed \"glyphs\" by Serruys (1984, p.&nbsp;657), which never had an independent existence other than being listed in ''Shuowen''. Each character was listed under only one element, which is then referred to as the radical for that character. For example, characters containing 女 ''nǚ'' \"female\" or 木 ''mù'' \"tree, wood\" are often grouped together in the sections for those radicals.\n\nMei Yingzuo's 1615 dictionary ''[[Zihui]]'' made two further innovations. He reduced the list of radicals to 214, and arranged characters under each radical in increasing order of the number of additional [[stroke (CJKV character)|stroke]]s – the \"radical-and-stroke-count\" method still used in the vast majority of present-day Chinese dictionaries.  These innovations were also adopted by the more famous ''[[Kangxi Dictionary]]'' of 1716. Thus the standard 214 radicals introduced in the ''Zihui'' are usually known as the [[Kangxi radical]]s. These were first called ''bùshǒu'' (部首, literally \"section header\") in the ''Kangxi Dictionary''.{{sfn|Wilkinson|2013|p=74}} Although there is some variation in such lists – depending primarily on what secondary radicals are also indexed – these canonical 214 radicals of the ''Kangxi Dictionary'' still serve as the basis for most modern [[Chinese dictionaries]]. Some of the graphically similar radicals are combined in many dictionaries, such as 月 ''yuè'' \"moon\" and the 月 form (⺼) of 肉 ''ròu'', \"meat, flesh\".\n\n==Shape and position within characters==\n\nRadicals may appear in any position in a character.  For example, the radical 女 appears on the left side in the characters 姐, 媽, 她, 好 and 姓, but it appears at the bottom in 妾.  However, there are two radicals that have the shape [[阝]], but are indexed as different radicals depending on where they appear in the character. When used as the abbreviated radical form of 邑 ''yì'' \"city\" (as in 都 ''dū'' \"metropolis\", also read ''dōu'' \"all\") this component appears on the right side of a character, but when used as the abbreviated radical form of 阜 ''fù'' \"mound, hill\" (as in 陸 ''lù'' \"land\") it appears on the left. However, there are regularities in the positioning of radicals within most characters, depending on their function: semantic components tend to appear on the top or on the left side of the character; similarly, phonetic components tend to appear on the right side of the character or at its bottom.{{sfn|Chan|2013}} These are only loose rules, though, and have exceptions. Sometimes, the radical may be placed outside, as in 園  = 囗 \"enclosure\" + 袁, or 街 =  行 \"go, movement\" + 圭. More complicated combinations also exist, such as 勝 = 力 \"strength\" + 朕, where the radical is in the lower-right quadrant.\n\nMany character components (including those used as radicals) are distorted or changed in form in order to fit into a block with other components.  They may be narrowed, shortened, or may have different shapes entirely. Changes in shape, rather than simple distortion, may result in a reduction in the number of strokes used to write a component. In some cases, these combining forms may have several variants.  The actual shape of the component when it is used in a character can depend on its placement with respect to the other elements in the character.\n\nSome of the most important variant combining forms (besides 邑 → 阝 and 阜 → 阝as discussed above) are:\n* 刀 \"knife\"  → 刂 when placed to the right of other elements:\n** examples: 分, 召 ~ 刖\n** counter-example: 切\n* 人 \"man\" → 亻 on the left:\n** 囚, 仄, 坐 ~ 他\n** counter-example: 从\n* 心 \"heart\" → 忄 on the left:\n** 杺, 您, 恭* ~ 快\n:<small>(*) 心 occasionally becomes ⺗ when written at the bottom of a character.</small>\n* 手 \"hand\" → 扌 on the left:\n** 杽, 拏, 掱 ~ 扡\n** counter-examples: 掰, 拜\n* 水 \"water\" → 氵 on the left:\n** 汆, 呇, 沊 ~ 池\n** counter-example: 沝\n* 火 \"fire\" → 灬 at the bottom:\n** 伙, 秋, 灱 ~ 黑\n** counter-example: 災\n* 犬 \"dog\" → 犭 on the left:\n** 伏, 状 ~ 狙\n** counter-example: 㹜\n\n===Semantic components===\n{{see also|Determinative}}\nOver 80% of Chinese characters are [[phono-semantic]] compounds,{{sfn|Liu|2010}} with a semantic component giving a broad category of meaning and a phonetic component suggesting the sound. Usually, the radical is also the semantic component, but that is not always the case.{{sfn|Woon|1987|p=148}}\n\nThus, although some authors use the term \"radical\" for semantic components (義符 ''yìfú''),{{efn|Wieger uses the terms \"keys of the dictionary\" and \"the 214 keys of K'ang-hsi\" for 部首 ''bùshǒu'', reserving the term \"radical\" for any element bearing meaning.{{sfn|Wieger|1927|p=14}}}}{{sfn|Ramsey|1987|pp=136–137}} others distinguish the latter as \"determinatives\"{{sfn|Boltz|1994|pp=67–68}} or \"significs\"{{sfn|Norman|1988|p=62}} or by some other term.{{efn|Woon gives an extensive list of the various translations of 義符 ''yìfú'': semantic element, radical, determinative, signific, signifying part, significant, significant part, semantic part, meaning element, meaning part, sense-indicator, radical-determinative, lexical morpheme symbol, ideographic element, and logographic part. Among them, \"radical\" and \"ideographic\" have both been strenuously objected to as misleading.{{sfn|Woon|1987|p=291}}}}{{efn|Professor Woon Wee Lee (1987) also explains:\n\n{{quote|It is important to note that the concepts of semantic element and \"section heading\" (部首 bùshǒu) are different, and should be clearly distinguished. The semantic element is parallel to the phonetic element in terms of the phonetic compound, while the section heading is a terminology of Chinese lexicography, which is a generic heading for the characters arranged in each section of a dictionary according to the system established by Xu Shen. It is the \"head\" of a section, assigned for convenience only. Thus, a section heading is usually the element common to all characters belonging to the same section. (Cf. L. Wang, 1962:1.151). The semantic elements of phonetic compounds were usually also used as section headings. However, characters in the same section are not necessarily all phonetic compounds. ...In some sections, such as 品 pin3 \"the masses\" (S. Xu 1963:48) and 爪 zhua3 \"a hand\" (S. Xu 1963:63), no phonetic compound is incorporated. In other words, the section heading was not commonly used as a semantic element...To sum up, the selection of a section heading is to some extent arbitrary.{{sfn|Woon|1987|pp=147–148}}}}}}{{efn|When an etymon (original \"root\" form of a graph, such as 采 ''cǎi'' \"to pick\", in 採 ''cǎi'' \"to pick\") is analyzed alongside the remaining element(s), it cannot be said to be playing only a phonetic role. For instance, operating under the two misconceptions that a) all characters have exactly one semantic and one phonetic part, and b) each part can only play one role, many would mistakenly dissect 採 as comprising 扌 ''shǒu'' \"hand\" semantic and 采 ''cǎi'' phonetic. However, being the original graph, it must necessarily impart its original semantic meaning (showing as it does a hand ''picking'' from a tree) as well as its sound. In the case of 陷 ''xiàn'' \"pit trap; fall into\", for instance, [[Duan Yucai]] notes in his annotation of ''[[Shuowen Jiezi]]'' (v.14, p.732) that the Dà Xú 大徐 edition acknowledges that 臽 plays the dual roles of phonetic and semantic in 陷, stating \"从阝, 从臽 , 臽 亦聲\".}}\n\nThere are numerous instances of characters listed under radicals which are merely artificial extractions of portions of those characters, and some of these portions are not even actual graphs with an independent existence (e.g., 亅 ''jué'' or ''juě''  in 了 ''liǎo''), as explained by Serruys (1984), who therefore prefers the term \"glyph\" extraction rather than graphic extraction (p.&nbsp;657). This is even truer of modern dictionaries, which reduce the number of radicals to less than half the number in ''Shuowen'', at which point it becomes impossible to have enough radicals to cover semantic elements in every character. In the ''Far Eastern Chinese English Dictionary'' for instance, 一 is a mere artificial extraction of a stroke from most of its sub-entries such as 丁 ''dīng'' and 且 ''qǐe''; the same is true of 乙 ''yǐ'' in 九 ''jiǔ''; 亅 ''jué'' or ''juě''  in 了 ''liǎo'', ''le''; 二 ''èr'' in 亞 ''yà'' and ''yǎ'';  田 ''tián'' in 禺 ''yù''; 豕 ''shǐ'' in 象 ''xiàng'' \"elephant\", and so on.\n\n===Phonetic components===\nThere are also instances of radicals which play a phonetic and not a semantic role in characters, such as the following:\n{| class=\"wikitable sortable\"\n! Phonetic part !! pinyin !! meaning !! Character !! pinyin !! meaning\n|-\n| rowspan=\"2\" | 臼\n| rowspan=\"2\" | ''jiù''\n| rowspan=\"2\" | \"a mortar\"\n| 舅 || ''jiù'' || \"maternal uncle\" (''Shuowen'' lists this under its semantic component 男 ''nán'', \"male\", but the 200-odd radicals used in modern dictionaries do not include all the semantic components that are used)\n|-\n| 舊 || ''jiù'' || \"owl; old\" (listed in the Far East on p.&nbsp;1141 under the header 臼)\n|-\n| 虎 || ''hǔ'' || \"tiger\" || 虖 || ''hū'' || \"shout\"\n|-\n| 鬼 || ''guǐ'' || (originally \"helmet\"{{sfn|Wu|1990|p=350}}), now \"ghost\" || 魁 || ''kúi'' || \"leader\"\n|-\n| 鹿 || ''lù'' || \"deer\" || 麓 || ''lù'' || foothills\n|-\n| 麻 || ''má'' || \"hemp\" || 麼 || ''ma'', ''mó'' || \"tiny\"\n|-\n| 黃 || ''huáng'' || \"yellow\" || 黌 || ''hóng'' || \"a school\"\n|-\n| 羽 || ''yǔ'' || \"feather\" || 翌 || ''yì'' || \"next\"{{sfn|Qiu|2000|p=7}}\n|-\n| 齊 || ''qí'' || || 齎 || ''jī'' || \"to present\"\n|-\n| rowspan=\"3\" | 青\n| rowspan=\"3\" | ''qīng''\n| rowspan=\"3\" |\n| 靖 || ''jìng'' || \"peaceful\"\n|-\n| 靚 || ''jìng'' || \"to ornament; quiet\"\n|-\n| 靜 || ''jìng'' || \"quiet\"\n|}\n\nIn some cases, arbitrarily chosen radicals coincidentally play a semantic role in the characters listed under them.{{sfn|Woon|1987|p=148}} In general, phonetic components do not determine the exact pronunciation of a character, but only give a clue to a its pronunciation. While some characters take the exact pronunciation of their phonetic component, others take only the initial or final sounds.{{sfn|Williams}} In fact, some characters' pronunciations may not correspond to the pronunciations of their phonetic parts at all, which is sometimes the case with characters after having undergone simplification.\n\n{| class=\"wikitable sortable\"\n|+ style=\"font-variant:small-caps\" | 8 phono-semantic compounds with phonetic part 也 (yě)<ref>{{cite web|title=也|url=http://zhongwen.com/d/164/x93.htm|website=中文.com|accessdate=17 February 2015}}</ref> \n! Character !! Semantic part !! Phonetic part !! pinyin !! meaning\n|-\n| 池 || 水（氵）water || 也 || chí || pool\n|-\n| 驰 / 馳 || 马 / 馬 horse || 也 || chí || gallop\n|-\n| 弛 || 弓 bow (bend)|| 也 || chí || relaxation\n|-\n| 施 || 方 square || 也 || shī || application\n|-\n| 地 || 土 earth || 也 || dì (de) || ground\n|-\n| 他 || 人 （亻）person || 也 || tā || he\n|-\n| 她 || 女 female || 也 || tā || she\n|-\n| 拖 || 手 （扌）hand || 也 || tuō || drag\n|}\nThe 8 characters above all take 也 for their phonetic part, however, as it is readily apparent, none of them take the pronunciation of 也, which is yě.  This disparity was caused by complex phonetic shifts and phonetic component swapping.\n\n===Character simplification===\n{{see also|List of Xinhua Zidian radicals}}\nThe [[Simplified Chinese character|character simplification]] adopted in the People's Republic of China and elsewhere has modified a number of components, including those used as radicals.  This has created a number of new radical forms. For instance, [[食]] ''shí'' is written 飠 when it forms a part of other [[Traditional Chinese character|traditional characters]], but is written 饣 in simplified characters. The difference between the traditional and simplified version of the same character can therefore lie solely in the visual appearance of the radical. One example is the character for ''yín'' \"silver\"; the traditional character is 銀, whilst in the simplified 银 only the radical is altered. Another example is the character for ''yǔ'' \"language\"; the traditional character is 語, whilst in the simplified 语 only the radical is altered. The same characters (or characters with a common ancestor) are used not only in China, but in Japan as well. However, simplification of the older, more complex characters has been done in different ways in these two countries. Chinese simplification of the characters tends to be more liberal, done in an effort to increase literacy by greatly simplifying characters. Conversely, Japanese simplification of the characters has been more conservative, and inherently has created modern characters which more strongly resemble their traditional counterparts.{{sfn|Imafuku}}\n\n==Dictionary lookup==\nMany dictionaries support using radical classification to index and lookup characters, although many present-day dictionaries supplement it with other methods as well. For example, modern dictionaries in PRC usually use the Pinyin transcription of a character to perform character lookup. Following the \"section-header-and-stroke-count\" method of [[Mei Yingzuo]], characters are listed by their radical and then ordered by the number of strokes needed to write them.\n\nThe steps involved in looking up a character are:\n#Identify the radical under which the character is most likely to have been indexed. If one does not know, then the component on the left side or top is often a good first guess.\n#Find the section of the dictionary associated with that radical.\n#Count the number of strokes in the remaining portion of the character.\n#Find the pages listing characters under that radical that have that number of additional strokes.\n#Find the appropriate entry or experiment with different choices for steps 1 and 3.\n\nFor example, consider the character 信 xìn, meaning \"truth\", \"faith\", \"sincerity\", and \"trust\".  Its radical is 亻 rén \"human\" (a compressed form of 人) and there are seven additional strokes in the remaining portion (言 ''yán'', \"speech\").  To look up this character in a dictionary, one finds the radical for \"human\" in the part of dictionary that indexes radicals. The various radicals will be organized by the number of strokes they themselves contain. 人 and its compressed version 亻 contain only two strokes, so it will be near the beginning of the list. Locating it, one can see the page for the index on that radical, and one then normally passes through the lists of characters with one additional stroke, two additional strokes, etc. until one reaches the entries with seven additional strokes. If the chosen radical matches the radical used by the dictionary compiler (which can be difficult to guarantee for more complicated characters), and if both the user and the dictionary compiler count strokes the same way (also often a problem with characters that the user is unfamiliar with), the entry will be in that list, and will appear next to an entry number or a page number where the full dictionary entry for that character can be found.\n\nAs a rule of thumb, components at the left or top of the character, or elements which surround the rest of the character, are the ones most likely to be used as radical.  For example, 信 is typically indexed under the left-side component 人 instead of the right-side 言; and 套 is typically indexed under the top 大 instead of the bottom 長.  There are, however, idiosyncratic differences between dictionaries, and except for simple cases, the same character cannot be assumed to be indexed the same way in two different dictionaries.\n\nIn order to further ease dictionary lookup, dictionaries sometimes list radicals both under the number of strokes used to write their canonical form and under the number of strokes used to write their variant forms.  For example, 心 can be listed as a four-stroke radical but might also be listed as a three-stroke radical because it is usually written as 忄 when it forms a part of another character.  This means that the dictionary user need not know that the two are etymologically identical.\n\nIt is sometimes possible to find a single character indexed under multiple radicals.  For example, many dictionaries list 義 under either 羊 or 戈 (the radical of its lower part 我).  Furthermore, with digital dictionaries, it is now possible to search for characters by cross-reference. Using this \"multi-component method\"<ref>which can be tried out at  [http://nihongo.monash.edu/cgi-bin/wwwjdic?1R Jim Breen's WWWJDIC Server] {{webarchive|url=https://web.archive.org/web/20131016101418/http://nihongo.monash.edu/cgi-bin/wwwjdic?1R |date=16 October 2013 }}, also [http://www.edrdg.org/cgi-bin/wwwjdic/wwwjdic?1C here]</ref> a relatively new development enabled by computing technology, the user can select ''all'' of a character's components from a table and the computer will present a list of matching characters. This eliminates the guesswork of choosing the correct radical and calculating the correct stroke count, and cuts down searching time significantly.  One can query for characters containing both 羊 and 戈, and get back only five characters (羢, 義, 儀, 羬 and  羲) to search through. The Academia Sinica’s 漢字構形資料庫 Chinese character structure database<ref>{{cite web|url=http://www.sinica.edu.tw/~cdp/cdphanzi/|title=中央研究院網站|author=|date=|website=www.sinica.edu.tw|accessdate=4 April 2018}}</ref> also works this way, returning only seven characters in this instance.  Harbaugh's Chinese Characters dictionary<ref>Harbaugh, Rick (1998). Chinese Characters: a Genealogy and Dictionary 中文字譜 – 漢英字元字典, Zhongwen.com publ., {{ISBN|0-9660750-0-5}}</ref> similarly allows searches based on any component. Some modern computer dictionaries allow the user to draw characters with a mouse, stylus or finger, ideally tolerating a degree of imperfection, thus eliminating the problem of radical identification altogether.{{efn|See, for example, http://www.nciku.com/.}}\n\n===Variations in the number of radicals===\nThough radicals are widely accepted as a method to categorize Chinese characters and to locate a certain character in a dictionary, there is no universal agreement about either the exact number of radicals, or the set of radicals. This is because radicals are merely arbitrarily chosen categories for lexicographical purposes.\n\nThe [[List of Kangxi radicals|214 Kangxi radicals]] act as a de facto standard, which may not be duplicated exactly in every Chinese dictionary, but which few dictionary compilers can afford to completely ignore.  They serve as the basis for many computer encoding systems. Specifically, the [[Unicode]] standard's radical-stroke charts are based on the Kangxi radicals or radicals.\n\nThe count of commonly used radicals in modern abridged dictionaries is often less than 214. The ''Oxford Concise English–Chinese Dictionary'' ({{ISBN|0-19-596457-8}}), for example, has 188.  A few dictionaries also introduce new radicals based on the principles first used by [[Xu Shen]], treating groups of radicals that are used together in many different characters as a kind of radical.\n\nIn modern practice, radicals are primarily used as [[Lexicography|lexicographic]] tools and as learning aids when writing characters.  They have become increasingly disconnected from [[semantics|meaning]], [[etymology]] and [[phonetics]].\n\n===Limitations===\nSome of the radicals used in Chinese dictionaries, even in the era of Kangxi, were not genuinely distinctive graphic elements.  They served only to index certain unique characters that do not have more obvious possible radicals.  The radical 鬯 (''chàng'' \"sacrificial wine\") is used to index only one character: 鬱 (''yù'', \"luxuriant\", \"dense\", or \"moody\"). Modern dictionaries tend to eliminate these kinds of radicals when it is possible to find some more widely used alternative graphic element under which a character can be categorized. In addition, in some modern dictionaries, characters may even be indexed under more than one radical in order to make it easier to find them.\n\n==Unicode==\n{{see also|List of radicals in Unicode}}\n{{Unicode chart Kangxi Radicals}}\n{{Unicode chart CJK Radicals Supplement}}\n\n== See also ==\n* [[Chinese language]]\n* [[Japanese language]]\n* [[Korean language]]\n* [[Hanzi]]/[[Kanji]]/[[Hanja]]\n* [[Chinese character description languages]]\n* [[List of kanji radicals by stroke count]]\n* [[List of kanji radicals by frequency]]\n\n==Notes==\n{{notelist}}\n\n==References==\n{{reflist|colwidth=25em}}\n\n===Works cited===\n* {{citation\n  | title = The origin and early development of the Chinese writing system\n  | first = William | last = Boltz\n  | publisher = American Oriental Society | year = 1994\n  | isbn = 978-0-940490-78-9\n  | postscript = .\n  }} (revised 2003)\n* {{citation\n  | title = Chinese | first = Jerry | last = Norman | authorlink = Jerry Norman (sinologist)\n  | location = Cambridge | publisher = Cambridge University Press | year = 1988\n  | isbn = 978-0-521-29653-3\n  | postscript = .\n  }}\n* {{citation\n  | last = Qiu | first = Xigui | authorlink = Qiu Xigui\n  | others = trans. by Gilbert L. Mattos and [[Jerry Norman (sinologist)|Jerry Norman]]\n  | title = Chinese writing\n  | location = Berkeley | publisher = Society for the Study of Early China and The Institute of East Asian Studies, University of California | year = 2000 | isbn = 978-1-55729-071-7\n  | postscript = .\n  }} (English translation of ''Wénzìxué Gàiyào'' 文字學概要, Shangwu, 1988.)\n* {{citation\n  | first = S. Robert | last = Ramsey | title = The Languages of China\n  | publisher = Princeton University Press | year = 1987 | isbn = 978-0-691-01468-5\n  | postscript = .\n  }}\n* {{citation\n  | title = Chinese Characters: Their Origin, Etymology, History, Classification and Signification. A Thorough Study from Chinese Documents\n  | first = Léon | last = Wieger\n  | year = 1927\n  | postscript = .\n  }} Translated from the French original ca. 1915 by L. Davrout, S.J., orig. Catholic Mission Press; reprinted in US – Dover; Taiwan – Lucky Book Co. Dover paperback {{ISBN|0-486-21321-8}}.\n* {{citation\n  | first = Endymion | last = Wilkinson | title = Chinese History: A New Manual\n  | location = Cambridge, MA | publisher = Harvard University Asia Center\n  | series = Harvard-Yenching Institute Monograph Series | year = 2013 | isbn = 978-0-674-06715-8\n  | postscript = .\n  }}\n* {{citation\n  | title = Chinese Writing: Its Origin and Evolution (''漢字的原始和演變'')\n  | last = Woon | first = Wee Lee 雲惟利\n  | url = https://books.google.com/books?id=Hv8PAAAAYAAJ\n  | publisher = Univ. of East Asia | location = Macau | year = 1987\n  | postscript = .\n  }}\n* {{citation\n  | title = The Origin and Dissemination of Chinese Characters (''中國文字只起源與繁衍'')\n  | first = Teresa L. | last = Wu\n  | publisher = Caves Books | location = Taipei | year = 1990\n  | isbn = 978-957-606-002-1\n  | postscript = .\n  }}\n* {{Cite thesis \n  |last=Chan \n  |first=Yi-Chin \n  |title=Learning to Read Chinese: The Relative Roles of Phonological Awareness and Morphological Awareness \n  | type=Ph.D. \n  | url=https://kuscholarworks.ku.edu/bitstream/handle/1808/12968/Chan_ku_0099D_13162_DATA_1.pdf?sequence=1\n  }}\n* {{cite journal\n  |last1=Liu\n  |first1=Phil\n  |title=Holistic versus analytic processing: Evidence for a different approach to processing of Chinese at the word and character levels in Chinese children\n  |journal=Journal of Experimental Child Psychology\n  |date=December 2010\n  |volume=107\n  |issue=4\n  |pages=466–478\n  |url=http://www.sciencedirect.com/science/article/pii/S002209651000127X\n  |doi=10.1016/j.jecp.2010.06.006\n  |pmid=20673579\n }}\n\n* {{Cite thesis \n  |last=Williams \n  |first=C. H. \n  |title=Semantic vs. phonetic decoding strategies in non-native readers of Chinese \n  | type=Ph.D. \n  | url=http://arizona.openrepository.com/arizona/bitstream/10150/195163/1/azu_etd_10830_sip1_m.pdf\n  }}\n\n* {{Cite thesis \n  |last=Imafuku \n  |first=K. \n  |title=Contrasting approaches to Chinese character reform: A comparative analysis of the simplification of Chinese characters in japan and china \n  | type=Ph.D. \n  | url=http://kuscholarworks.ku.edu/handle/1808/12968\n  }}\n\n== Further reading ==\n* Luó Zhènyù (羅振玉) 1958. 增訂殷墟書契考釋 (revised and enlarged edition on the interpretation of oracle bone inscriptions). Taipei: Yiwen Publishing (cited in Wu 1990).\n* Serruys, Paul L-M. (1984) \"On the System of the Pu Shou 部首 in the Shuo-wen chieh-tzu 說文解字\", in 中央研究院歷史語言研究所集刊  Zhōngyāng Yánjiūyuàn Lìshǐ Yǔyán Yánjiūsuǒ Jíkān, v. 55:4, pp.&nbsp;651–754.\n* [[Xu Shen]] ''Shuōwén Jǐezì'' (說文解字), is most often accessed in annotated versions, the most famous of which is [[Duan Yucai]] (1815). 說文解字注 ''Shuōwén Jǐezì Zhù'' (commentary on the ''Shuōwén Jíezì''), compiled 1776–1807, and still reproduced in facsimile by various publishers. The reproduction by 天工書局 Tiāngōng Books (1998) in Taibei is useful because the seal characters are highlighted in red ink.\n\n==External links==\n{{Wiktionary|Index:Chinese radical}}\n{{commons category|CJKV radicals and their variants in regular script style (in SVG format)|radicals and their variants in regular script}}\n{{commons category|The 214 Kangxi radicals in the dictionary’s own style (in SVG format)|the 214 Kangxi radicals}}\n{{commons category|The 540 Shuowen radicals|the 540 Shuowen radicals}}\n* [http://www.archchinese.com/arch_chinese_radicals.html Chinese Character Radicals] List of Chinese Character Radicals\n* [http://users.numericable.be/lucvileyn/radicals.pdf 汉语大词典部首表] a list of radicals in the [[Hanyu Da Cidian]]\n* [https://www.zhonga.org/radicals List of Chinese Character Radicals] A comprehensive list of Chinese character radicals with translations\n\n{{Unicode CJK Unified Ideographs}}\n\n{{DEFAULTSORT:Radical (Chinese Character)}}\n\n[[Category:Chinese characters]]\n[[Category:Collation]]\n[[Category:Hanja]]\n[[Category:Kanji]]"
    },
    {
      "title": "Shiva Sutras",
      "url": "https://en.wikipedia.org/wiki/Shiva_Sutras",
      "text": "{{hatnote|For the [[Kashmir Shaivism|Kashmir Shaivistic]] text, see [[Shiva Sutras of Vasugupta]]}}\n{{italic title}}\n{{refimprove|date=September 2016}}\nThe '''''Shiva Sutras''''' ([[International Alphabet of Sanskrit Transliteration|IAST]]: {{IAST|Śivasūtrāṇi}}) or ''{{IAST|Māheśvara Sūtrāṇi}}'' are fourteen verses that organize the [[phoneme]]s of [[Sanskrit]] as referred to in the {{IAST|''Aṣṭādhyāyī''}} of {{IAST|[[Pāṇini]]}}, the foundational text of [[Sanskrit grammar]].\n\nWithin the tradition they are known as the ''{{IAST|Akṣarasamāmnāya}}'', \"recitation of phonemes,\" but they are popularly known as the ''Shiva Sutras'' because they are said to have been revealed to Pāṇini by [[Shiva]]. They were either composed by Pāṇini to accompany his ''{{IAST|Aṣṭādhyāyī}}'' or predate him. The latter is less plausible, but the practice of encoding complex rules in short, mnemonic verses is typical of the [[sutra]] style.\n\n==Text==\n\n{| class=\"wikitable\" style=\"margin:auto;\"\n! [[IAST]]\n!देवनागरी (Devanāgari)\n!తెలుగు (Telugu)\n!ಕನ್ನಡ (Kannada)\n|- valign=top\n|\n<poem>1. a i u Ṇ\n2. ṛ ḷ K \n3. e o Ṅ\n4. ai au C\n5. ha ya va ra Ṭ\n6. la Ṇ\n7. ña ma ṅa ṇa na M\n8. jha bha Ñ\n9. gha ḍha dha Ṣ\n10. ja ba ga ḍa da Ś\n11. kha pha cha ṭha tha ca ṭa ta V\n12. ka pa Y\n13. śa ṣa sa R\n14. ha L</poem>\n|\n<poem>१. अ इ उ ण्।\n२. ऋ ऌ क्।\n३. ए ओ ङ्।\n४. ऐ औ च्।\n५. ह य व र ट्।\n६. ल ण्।\n७. ञ म ङ ण न म्।\n८. झ भ ञ्।\n९. घ ढ ध ष्।\n१०. ज ब ग ड द श्।\n११. ख फ छ ठ थ च ट त व्।\n१२. क प य्।\n१३. श ष स र्।\n१४. ह ल्।</poem>\n|\n<poem>1. అ ఇ ఉ ణ్\n2. ఋ ఌ క్ \n3. ఏ ఓ ఙ్\n4. ఐ ఔ చ్\n5. హ య వ ర ట్\n6. ల ణ్\n7. ఞ మ ఙ ణ న ం\n8. ఝ భ ఞ్\n9. ఘ ఢ ధ ష్\n10. జ బ గ డ ద శ్\n11. ఖ ఫ ఛ ఠ థ చ ట త వ్\n12. క ప య్\n13. శ ష స ర్\n14. హ ల్</poem>\n|\n<poem>1. అ ఇ ಉ ಣ್\n2. ಋ ಲ್ರು ಕ್\n3. ಏ ಓ ಙ್\n4. ಐ ಔ ಚ್ \n5. ಹ ಯ ವ ರ ಟ್\n6. ಲ ಣ್\n7. ಞ ಮ ಙ ಣ ನ ಮ್ \n8. ಝ ಭ ಞ\n9. ಘ ಢ ಧ ಷ್ \n10. ಜ ಬ ಗ ಡ ದ ಶ್ \n11. ಖ ಫ ಛ ಠ ಥ ಚ ಟ ತ ವ್\n12. ಕ ಪ ಯ್\n13. ಶ ಷ ಸ ರ್ \n14. ಹ ಲ್ \n</poem>\n|}\n\nEach of the fourteen verses consists of a group of basic Sanskrit phonemes (i.e. either open syllables consisting either of initial vowels or consonants followed by the ''basic'' vowel \"a\") followed by a single 'dummy letter', or ''anubandha'', conventionally rendered by capital letters in Roman [[transliteration]] and named '{{IAST|IT}}' by Pāṇini.\n\nThis allows Pāṇini to refer to groups of phonemes with ''{{IAST|pratyāhāras}}'', which consist of a phoneme-letter and an ''anubandha'' (and often the vowel ''a'' to aid pronunciation) and signify all of the intervening phonemes. ''Pratyāhāras'' are thus single syllables, but they can be [[Declension|declined]] (see Aṣṭādhyāyī 6.1.77 below). Hence the ''pratyāhāra'' ''aL'' refers to all phonemes (because it consists of the first phoneme of the first verse (''a'') and the last ''anubandha'' of the last verse (''L''); ''aC'' refers to vowels (i.e., all of the phonemes before the ''anubandha'' ''C'': i.e. ''a i u ṛ ḷ e o ai au''); ''haL'' to consonants, and so on.\n\nNote that some ''pratyāhāras'' are ambiguous. The ''anubandha'' ''Ṇ'' occurs twice in the list, which means that you can assign two different meanings to ''pratyāhāra'' ''aṆ'' (including or excluding ''ṛ'', etc.); in fact, both of these meanings are used in the ''Aṣṭādhyāyī''. On the other hand, the ''pratyāhāra'' ''haL'' is always used in the meaning \"all consonants\"---Pāṇini never uses ''pratyāhāras'' to refer to sets consisting of a single phoneme.\n\nFrom these 14 verses, a total of 281 ''pratyāhāras'' can be formed: 14*3 + 13*2 + 12*2 + 11*2 + 10*4 + 9*1 + 8*5 + 7*2 + 6*3 * 5*5 + 4*8 + 3*2 + 2*3 +1*1, minus 14 (as Pāṇini does not use single element ''pratyāhāras'') minus 10 (as there are 10 duplicate sets due to ''h'' appearing twice); the second multiplier in each term represents the number of phonemes in each. But [[Pāṇini]] uses only 41 (with a 42nd introduced by later grammarians, ''raṆ''=''r l'') ''pratyāhāras'' in the ''Aṣṭādhyāyī''.\n\nThe Shiva Sutras put phonemes with a similar [[manner of articulation]] together (so [[sibilants]] in 13 ''śa ṣa sa R,'' [[Nasal stop|nasals]] in 7 ''ñ m ṅ ṇ n M'').  Economy ([[Sanskrit]]: {{IAST|lāghava}}) is a major principle of their organization, and it is debated whether Pāṇini deliberately encoded [[Phonology|phonological]] patterns in them (as they were treated in traditional phonetic texts called [[Shiksha|Prātiśakyas]]) or simply grouped together phonemes which he needed to refer to in the ''Aṣṭādhyāyī'' and which only ''secondarily'' reflect phonological patterns (as argued by [http://www.stanford.edu/~kiparsky/Papers/siva-t.pdf Paul Kiparsky] and [http://user.phil-fak.uni-duesseldorf.de/~petersen Wiebke Petersen], for example).  Pāṇini does not use the Shiva Sutras to refer to homorganic stops ([[stop consonants]] produced at the same [[place of articulation]]), but rather the ''anubandha'' ''U'': to refer to the [[palatals]] ''c ch j jh'' he uses ''cU''.\n\nAs an example, consider ''Aṣṭādhyāyī'' 6.1.77: इकः यण् अचि ''{{IAST|iKaḥ yaṆ aCi}}'':\n\n* ''iK'' means ''i u ṛ ḷ'', \n* ''iKaḥ'' is ''iK'' in the [[genitive]] case, so it means ' in place of ''i u ṛ ḷ''; \n* ''yaṆ'' means the [[semivowels]] ''y v r l'' and is in the nominative, so ''iKaḥ yaṆ'' means: ''y v r l'' replace ''i u ṛ ḷ''.\n* ''aC'' means all vowels, as noted above\n* ''aCi'' is in the [[locative]] case, so it means ''before any vowel''.\n[[File:Pratyahara Sutras.jpg|thumb|Pratyāhāra Sūtras in [[Grantha Script]]]]\nHence this rule replaces a vowel with its corresponding semivowel when followed by any vowel, and that is why ''{{IAST|dadhi}}'' together with ''{{IAST|atra}}'' makes ''{{IAST|dadhyatra}}''. To apply this rule correctly we must be aware of some of the other rules of the grammar, such as:\n*1.1.49 षष्ठी स्थानेयोगा ''{{IAST|ṣaṣṭhī sthāneyogā}}'' which says that the genitive case in a sutra signifies \"in the place of\"\n*1.1.50 स्थानेऽन्तरतमः ''{{IAST|sthāne 'ntaratamaḥ}}'' which says that in a substitution, the element in the substitute series that most closely resembles the letter to be substituted should be used (e.g. ''y'' for ''i'', ''r'' for ''ṛ'' etc.)\n*1.1.71 आदिरन्त्येन सहेता ''{{IAST|ādir antyena sahetā}}'' which says that a sequence with an element at the beginning (e.g. ''i'') and an ''IT'' letter (e.g. ''K'') at the end stands for the intervening letters (i.e. ''i u ṛ ḷ'', because the Shiva sutras read ''{{IAST|i u ṛ ḷ K}}'').\n\nAlso, rules can be debarred by other rules:\n\n* 6.1.101 अकः सवर्णे दीर्घः ''{{IAST|aKaḥ savarṇe dīrghaḥ}}'' teaches that vowels (from the ''aK pratyāhāra'') of the same quality come together to make a long vowel, so for instance ''{{IAST|dadhi}}'' and ''{{IAST|indraḥ}}'' make ''{{IAST|dadhīndraḥ}}'', not ''{{IAST|*dadhyindraḥ}}''. This ''{{IAST|aKaḥ savarṇe dīrghaḥ}}'' rule takes precedence over the general ''{{IAST|iKaḥ yaṆ aCi}}'' rule mentioned above, because this rule is more specific.\n\n==See also==\n*[[Sanskrit]]\n*[[Aṣṭādhyāyī]]\n*[[Shiksha]]\n\n;Other languages\n*[[Alphabet song]]\n*[[Iroha]], Japanese poem with similar function\n*[[Thousand Character Classic]], Chinese poem with similar function, esp. used in Korea\n\n==External links==\n*[http://www.stanford.edu/~kiparsky/Papers/siva-t.pdf] Paper by [[Paul Kiparsky]] on 'Economy and the Construction of the Śiva sūtras'.\n*[http://kornai.com/Papers/mol2.pdf] Paper by [[Andras Kornai]] relating the Śiva sūtras to contemporary [[Feature Geometry]].\n*[http://user.phil-fak.uni-duesseldorf.de/~petersen/paper/petersen_jolli_proof.pdf] Paper by Wiebke Petersen  on 'A Mathematical Analysis of Pāṇini’s  Śiva sūtras.'\n*[https://www.scribd.com/doc/81266812/Who-Inspired-Panini-Reconstructing-the-Hindu-and-Buddhist-Counter-Claims-Madhav-M-Deshpande] Paper by Madhav Deshpande on 'Who Inspired Pāṇini? Reconstructing the Hindu and Buddhist Counter-Claims.'\n\n[[Category:Vyakarana]]\n[[Category:Collation]]\n[[Category:Shaiva texts]]\n[[Category:Sanskrit texts]]\n[[Category:Hindu texts]]"
    },
    {
      "title": "Surname stroke order",
      "url": "https://en.wikipedia.org/wiki/Surname_stroke_order",
      "text": "{{About|ordering by stroke count|the order in which the strokes are written|Stroke order}}\n\nThe '''surname stroke order''' ({{zh|s=姓氏笔划排序}}) is a system for the [[collation]] of [[Chinese surname]]s. It arose as an impartial method of categorization of the order in which names appear in official documentation or in ceremonial procedure without any line of hierarchy. In official setting, the number of [[stroke (CJK character)|stroke]]s in a person's surname determines where a name should be placed and the list order. Surnames \"[[Ding (surname)|Ding]]\" and \"[[Wang (surname)|Wang]]\" (written simply in the Chinese language with two and four strokes, respectively, \"丁\", \"王\") for example, are simple surnames that usually appear on the front of lists, while surnames such as \"[[Dai (surname)|Dai]]\" and \"[[Wei (surname)|Wei]]\" (\"戴\", \"魏\", both written with 17 strokes) often appear on the bottom of lists.<ref>{{cite web |title = 按姓氏笔画排序规则 |url = http://wenku.baidu.com/view/b616c10bf78a6529647d53eb.html |website = Baidu Wenku}}</ref>\n\nIf the first character is the same, then the names are ordered by the stroke on the second character. In some naming lists, names with two characters appear before names with three characters. In other naming lists, the second character takes precedence regardless of how many characters there are in total in the name. If there are equal number of strokes in a surname, they are then ordered by the order of the way strokes are formed, starting with the first stroke, with a horizontal stroke ordering first, then a vertical stroke, then a downwards right-to-left stroke, then a point, then a hook ({{zh|c=橫竖撇捺折}}).\n\nAlthough this ordering method is widely used, its most prominent use is in the ordering of important official bodies, including the members of the [[Central Committee of the Communist Party of China]], the members of the [[National People's Congress]], and the members of the [[Chinese People's Political Consultative Conference]].\n\nThe ordering method is comparable to an [[alphabetical order]] of names, but in Chinese this is impractical as [[written Chinese]] lacks an alphabetical structure, and the general populace does not rely on the [[pinyin]] romanization, from which some order schemes have developed.\n\nThe [[2008 Summer Olympics Parade of Nations|parade of nations at the 2008 Beijing Olympics]] used a similar ordering method.\n\n== See also ==\n* [[Chinese name]]\n* [[Radical-and-stroke sorting]]\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [https://web.archive.org/web/20101113210057/http://news.xinhuanet.com/ziliao/2007-10/21/content_6917474.htm An example of surname stroke order in the Communist Party's Central Committee membership list]\n\n{{DEFAULTSORT:Surname Stroke Order}}\n[[Category:Collation]]\n[[Category:Chinese characters]]\n[[Category:Chinese-language surnames|*]]\n\n{{writingsystem-stub}}\n{{china-stub}}"
    },
    {
      "title": "Table of Indexing Chinese Character Components",
      "url": "https://en.wikipedia.org/wiki/Table_of_Indexing_Chinese_Character_Components",
      "text": "'''The Table of Indexing Chinese Character Components''' ({{Zh|s=汉字部首表|t=漢字部首表|p=hànzì bùshǒu biǎo|l=Chinese character radicals table}}) is a [[lexicography|lexicographic]] tool used to order the [[Chinese characters]] in [[Mainland China]].  The specification is also known as '''GF 0011-2009'''.\n\nIt was distributed by the [[Ministry of Education of the People's Republic of China]] and the [[State Language Work Committee]] ({{Zh|s=国家语言文字工作委员会|labels=no}}) in 2009.  There are 201 dictionary radicals and 100 supplementary components.<ref>{{cite web |url=http://www.china-language.gov.cn/9/2009_9_23/1_9_4344_0_1253687722552.html |title=Archived copy |accessdate=2016-01-07 |deadurl=yes |archiveurl=https://web.archive.org/web/20150923202814/http://www.china-language.gov.cn/9/2009_9_23/1_9_4344_0_1253687722552.html |archivedate=2015-09-23 |df= }}</ref>  It has been adapted in the newer versions of ''[[Xinhua Zidian]]'' and ''[[Xiandai Hanyu Cidian]]''.\n\n== List of radicals ==\n1 stroke: {{Zh|s=一 丨 丿 丶 乛|labels=no}}\n\n2 strokes: {{Zh|s=十 厂 匚 卜 冂 八 人 勹 儿 匕 几 亠 冫 冖 凵 卩 刀 力 又 厶 廴|labels=no}}\n\n3 strokes: {{Zh|s=干 工 土 艹 寸 廾 大 尢 弋 小 口 囗 山 巾 彳 彡 夕 夂 丬 广 门 宀 辶 彐 尸 己 弓 子 屮 女 飞 马 么 巛|labels=no}}\n\n4 strokes: {{Zh|s=王 无 韦 木 支 犬 歹 车 牙 戈 比 瓦 止 攴 日 贝 水 见 牛 手 气 毛 长 片 斤 爪 父 月 氏 欠 风 殳 文 方 火 斗 户 心 毋|labels=no}}\n\n5 strokes: {{Zh|s=示 甘 石 龙 业 目 田 罒 皿 生 矢 禾 白 瓜 鸟 疒 立 穴 疋 皮 癶 矛|labels=no}}\n\n6 strokes: {{Zh|s=耒 老 耳 臣 覀 而 页 至 虍 虫 肉 缶 舌 竹 臼 自 血 舟 色 齐 衣 羊 米 聿 艮 羽 糸|labels=no}}\n\n7 strokes: {{Zh|s=麦 走 赤 豆 酉 辰 豕 卤 里 足 邑 身 釆 谷 豸 龟 角 言 辛|labels=no}}\n\n8 strokes: {{Zh|s=青 龺 雨 非 齿 黾 隹 阜 金 鱼 隶|labels=no}}\n\n9 strokes: {{Zh|s=革 面 韭 骨 香 鬼 食 音 首|labels=no}}\n\n10 strokes: {{Zh|s=髟 鬲 鬥 高|labels=no}}\n\n11 strokes: {{Zh|s=黄 麻 鹿|labels=no}}\n\n12 strokes: {{Zh|s=鼎 黑 黍|labels=no}}\n\n13 strokes: {{Zh|s=鼓 鼠|labels=no}}\n\n14 strokes: {{Zh|s=鼻|labels=no}}\n\n17 strokes: {{Zh|s=龠|labels=no}}\n\n==See also==\n{{commons category|Chinese radicals}}\n* [[Radical (Chinese character)]]\n* [[List of Shuowen Jiezi radicals|List of ''Shuowen Jiezi'' radicals]]\n* [[List of unicode radicals|List of Unicode radicals]]\n** [[Kangxi Radicals#Unicode|Unicode chart - Kangxi Radicals]]\n** [[CJK Radicals Supplement|Unicode Chart - CJK Radicals Supplement]]\n* [[List of Kangxi radicals]] - 214 radicals\n* [[Table of Japanese kanji radicals]]\n** [[Simplified table of Japanese kanji radicals]]\n\n==References==\n{{reflist}}\n\n== External links ==\n{{wiktionary|Index:Chinese radical}}\n\n[[Category:Chinese language]]\n[[Category:Chinese dictionaries]]\n[[Category:Collation]]"
    },
    {
      "title": "Unicode collation algorithm",
      "url": "https://en.wikipedia.org/wiki/Unicode_collation_algorithm",
      "text": "{{no footnotes|date=September 2016}}\nThe '''Unicode collation algorithm''' ('''UCA''') is an algorithm defined in Unicode Technical Report #10, which defines a customizable method to compare two [[String (computer science)|strings]]. These comparisons can then be used to [[collate]] or sort text in any [[writing system]] and [[language]] that can be represented with [[Unicode]].\n\nUnicode Technical Report #10 also specifies the ''Default Unicode Collation Element Table'' (DUCET). This datafile specifies the default collation ordering. The DUCET is customizable for different languages. Some such customisations can be found in [[Common Locale Data Repository]] (CLDR).\n\nAn important open source implementation of UCA is included with the [[International Components for Unicode]], ICU. ICU also supports tailoring and the collation tailorings from CLDR are included in ICU. You can see the effects of tailoring and a large number of language specific tailorings in the on-line '''ICU Locale Explorer'''.\n\n==See also==\n*[[Collation]]\n*[[ISO 14651|ISO/IEC 14651]]\n*[[European ordering rules]] (EOR)\n*[[Common Locale Data Repository]] (CLDR)\n\n==External links and references==\n*[https://www.unicode.org/unicode/reports/tr10/ Unicode Collation Algorithm]: Unicode Technical Standard #10\n*[http://developer.mimer.com/sql-unicode-collation-charts/ Mimer SQL Unicode Collation Charts]\n*[http://www.collation-charts.org/mysql60/by-charset.html#utf8 MySQL UCA-based Unicode Collation Charts]\n\n==Tools==\n*[http://demo.icu-project.org/icu-bin/locexp?_=en_US&x=col ICU Locale Explorer] An online demonstration of the Unicode Collation Algorithm using [[International Components for Unicode]]\n*[http://billposer.org/Software/msort.html msort] A sort program that provides an unusual level of flexibility in defining collations and extracting keys.\n*[http://code.google.com/p/openrtl OpenRTL] A library of functions using Unicode collation based on the Unicode collation algorithm. Also supports the customized Unicode collations for the locales defined by CLDR.\n\n{{Unicode navigation}}\n\n[[Category:String collation algorithms]]\n[[Category:Unicode algorithms|Collation]]\n[[Category:Collation]]\n\n{{algorithm-stub}}\n{{standard-stub}}"
    },
    {
      "title": "Alphabet",
      "url": "https://en.wikipedia.org/wiki/Alphabet",
      "text": "{{short description|A standard set of letters that represent phonemes of a spoken language}}\n{{pp-semi-indef}}\n{{about|sets of letters used in written languages|other uses|Alphabet (disambiguation)|and|Alphabetical (disambiguation)}}\n{{pp-move-indef}}\n{{Use dmy dates|date=June 2013}}\n[[File:Orbis eruditi literatura à charactere Samaritico deducta 1689.jpg|thumb|[[Edward Bernard]]'s \"Orbis eruditi\", comparing all known alphabets as of 1689]]\n{{Writing systems sidebar}}\n\nAn '''alphabet''' is a standard set of [[letter (alphabet)|letters]] (basic written [[symbols]] or [[graphemes]]) that represent the [[phoneme]]s (basic significant sounds) of any [[spoken language]] it is used to write. This is in contrast to other types of [[writing system]]s, such as [[syllabary|syllabaries]] (in which each character represents a [[syllable]]) and [[Logogram|logographic systems]] (in which each character represents a word, [[morpheme]], or semantic unit).\n\nThe first fully phonemic script, the [[Proto-Canaanite]] script, later known as the [[Phoenician alphabet]], is considered to be the first alphabet, and is the ancestor of most modern alphabets, including [[Arabic alphabet|Arabic]], [[Greek alphabet|Greek]], [[Latin alphabet|Latin]], [[Cyrillic alphabet|Cyrillic]], [[Hebrew alphabet|Hebrew]], and possibly [[Brahmic scripts|Brahmic]].<ref name=\"Coulmas 140\"/><ref name=\"Daniels 9296\">{{harvnb|Daniels|Bright|1996|pp=92–96}}</ref> [[Peter T. Daniels]], however, distinguishes an [[abugida]] or alphasyllabary, a set of graphemes that represent consonantal base letters which [[diacritic]]s modify to represent vowels (as in [[Devanagari]] and other South Asian scripts), an [[abjad]], in which letters predominantly or exclusively represent consonants (as in the original Phoenician, [[Hebrew alphabet|Hebrew]] or [[Arabic script|Arabic]]), and an \"alphabet\", a set of graphemes that represent both [[vowel]]s and [[consonant]]s. In this narrow sense of the word the first \"true\" alphabet was the [[Greek alphabet]],<ref name=\"Blackwell\">{{cite book|last=Coulmas|first=Florian|title=The Blackwell Encyclopedia of Writing Systems|year=1996|publisher=[[Blackwell Publishing]] |location=Oxford|isbn=978-0-631-21481-6}}</ref><ref>{{harvnb|Millard|1986|p=396}}</ref> which was developed on the basis of the earlier [[Phoenician alphabet]]. \n\nOf the dozens of alphabets in use today, the most popular is the [[Latin alphabet]]<ref>{{harvnb|Haarmann|2004|p=96}}</ref>, which was derived from the [[Greek alphabet|Greek]], and which many languages modify by adding letters formed using diacritical marks.  While most alphabets have letters composed of lines ([[linear writing]]), there are also [[non-linear writing|exceptions]] such as the alphabets used in [[Braille]]. The [[Khmer alphabet]] (for [[Cambodian language|Cambodian]]) is the longest, with 74 letters.<ref>{{cite web |url=http://www.angmohdan.com/language-largest-alphabet/ |title=What Language Has the Largest Alphabet? |quote=Languages like Chinese, technically, do not use an alphabet but have an ideographic writing system. There are thousands of symbols (pictographs) in Chinese representing different words, syllables and concepts. [..] The language with the most letters is Khmer (Cambodian), with 74 (including some without any current use). According to Guinness Book of World Records, 1995, the Khmer alphabet is the largest alphabet in the world. It consists of 33 consonants, 23 vowels and 12 independent vowels.|date=2014-12-26 }}</ref>\n\nAlphabets are usually associated with a standard ordering of letters. This makes them useful for purposes of [[collation]], specifically by allowing words to be sorted in [[alphabetical order]]. It also means that their letters can be used as an alternative method of \"numbering\" ordered items, in such contexts as [[numbered list]]s and number placements.\n\n==Etymology==\nThe English word ''alphabet'' came into [[Middle English]] from the [[Late Latin]] word ''alphabetum'', which in turn originated in the [[Greek language|Greek]] ἀλφάβητος (''alphabētos'').  The Greek word was made from the first two letters, ''[[alpha (letter)|alpha]]''(α) and ''[[beta (letter)|beta]]''(β).<ref>{{cite web|url=http://www.merriam-webster.com/dictionary/alphabet|title=alphabet|publisher=[[Merriam-Webster.com]]}}</ref>   The names for the Greek letters came from the first two letters of the [[Phoenician alphabet]]; ''[[aleph]]'', which also meant ''ox'', and ''[[bet (letter)|bet]]'', which also meant ''house''.\n\nSometimes, like in the [[alphabet song]] in English, the term \"ABCs\" is used instead of the word \"alphabet\" (''Now I know my ABCs''...).  \"Knowing one's ABCs\", in general, can be used as a metaphor for knowing the basics about anything.\n\n==History==\n{{Main article|History of the alphabet}}\n[[File:A Specimen by William Caslon.jpg|thumb|300px|''A Specimen'' of [[typeset]] [[font]]s and [[language]]s, by [[William Caslon]], letter founder; from the 1728 ''[[Cyclopaedia, or an Universal Dictionary of Arts and Sciences|Cyclopaedia]]'']]\n\n===Ancient Northeast African and Middle Eastern scripts===\nThe history of the alphabet started in [[ancient Egypt]]. Egyptian writing had a set of some [[Egyptian uniliteral signs|24 hieroglyphs]] that are called uniliterals,<ref>{{cite web |url=http://www.bbc.co.uk/dna/h2g2/A2451890 |title=The Development of the Western Alphabet |accessdate=2008-08-04 |author =Lynn, Bernadette |date=2004-04-08 |work=h2g2 |publisher=BBC}}</ref> to represent syllables that begin with a single [[consonant]] of their language, plus a vowel (or no vowel) to be supplied by the native speaker. These glyphs were used as pronunciation guides for [[logogram]]s, to write grammatical inflections, and, later, to transcribe loan words and foreign names.<ref name=Daniels>{{harvnb|Daniels|Bright|1996|pp=74–75}}</ref>\n\n[[File:Ba`alat.png|thumb|left|A specimen of [[Proto-Sinaitic script]], one of the earliest (if not the very first) phonemic scripts]]\nIn the [[Middle Bronze Age]], an apparently \"alphabetic\" system known as the [[Proto-Sinaitic script]] appears in Egyptian turquoise mines in the [[Sinai peninsula]] dated to circa the 15th century BC, apparently left by Canaanite workers. In 1999, John and Deborah Darnell discovered an even earlier version of this first alphabet at Wadi el-Hol dated to circa 1800 BC and showing evidence of having been adapted from specific forms of Egyptian hieroglyphs that could be dated to circa 2000 BC, strongly suggesting that the first alphabet had been developed about that time.<ref>{{cite journal |first1=J. C. |last1=Darnell |first2=F. W. |last2=Dobbs-Allsopp |authorlink2=F. W. Dobbs-Allsopp |first3=Marilyn J. |last3=Lundberg |first4=P. Kyle |last4=McCarter |authorlink4=P. Kyle McCarter |first5=Bruce |last5=Zuckerman |first6=Colleen |last6=Manassa |title=Two Early Alphabetic Inscriptions from the Wadi el-Ḥôl: New Evidence for the Origin of the Alphabet from the Western Desert of Egypt |journal=The Annual of the American Schools of Oriental Research |volume=59 |year=2005 |pages=63, 65, 67–71, 73–113, 115–124 |ref=harv |jstor=3768583}}</ref> Based on letter appearances and names, it is believed to be based on Egyptian hieroglyphs.<ref name=\"Coulmas 140\">{{harvnb|Coulmas|1989|pp=140–141}}</ref> This script had no characters representing vowels, although originally it probably was a syllabary, but unneeded symbols were discarded. An alphabetic [[cuneiform]] script with 30 signs including three that indicate the following vowel was invented in [[Ugarit]] before the 15th century BC. This script was not used after the destruction of Ugarit.<ref>''Ugaritic Writing'' [http://www.mazzaroth.com/ChapterThree/UgariticWriting.htm online]</ref>\n\nThe Proto-Sinaitic script eventually developed into the [[Phoenician alphabet]], which is conventionally called \"Proto-Canaanite\" before ca. 1050 BC.<ref name=\"Daniels 9296\"/> The oldest text in Phoenician script is an inscription on the sarcophagus of King [[Ahiram]]. This script is the parent script of all western alphabets. By the tenth century, two other forms can be distinguished, namely [[Canaanite language|Canaanite]] and [[Aramaic alphabet|Aramaic]]. The Aramaic gave rise to the [[Hebrew alphabet|Hebrew]] script.<ref>{{harvnb|Coulmas|1989|p=142}}</ref>  The [[South Arabian alphabet]], a sister script to the Phoenician alphabet, is the script from which the [[Ge'ez alphabet]] (an [[abugida]]) is descended. Vowelless alphabets are called [[abjad]]s, currently exemplified in scripts including [[Arabic alphabet|Arabic]], [[Hebrew alphabet|Hebrew]], and [[Syriac alphabet|Syriac]]. The omission of vowels was not always a satisfactory solution and some \"weak\" consonants are sometimes used to indicate the vowel quality of a syllable ([[Mater lectionis|matres lectionis]]). These letters have a dual function since they are also used as pure consonants.<ref>{{harvnb|Coulmas|1989|p=147}}</ref>\n\nThe Proto-Sinaitic or Proto-Canaanite script and the [[Ugaritic script]] were the first scripts with a limited number of signs, in contrast to the other widely used writing systems at the time, [[Cuneiform]], [[Egyptian hieroglyphs]], and [[Linear B]]. The Phoenician script was probably the first phonemic script<ref name=\"Coulmas 140\"/><ref name=\"Daniels 9296\"/> and it contained only about two dozen distinct letters, making it a script simple enough for common traders to learn. Another advantage of Phoenician was that it could be used to write down many different languages, since it recorded words phonemically.\n[[File:Acta Eruditorum - I alfabeti, 1743 – BEIC 13408919.jpg|thumb|Illustration from [[Acta Eruditorum]], 1741]]\nThe script was spread by the Phoenicians across the Mediterranean.<ref name=\"Daniels 9296\" /> In Greece, the script was modified to add vowels, giving rise to the ancestor of all alphabets in the West. It was the first alphabet in which vowels have independent letter forms separate from those of consonants. The Greeks chose letters representing sounds that did not exist in Greek to represent vowels. Vowels are significant in the Greek language, and the syllabical [[Linear B]] script that was used by the [[Mycenaean Greece|Mycenaean]] Greeks from the 16th century BC had 87 symbols, including 5 vowels. In its early years, there were many variants of the Greek alphabet, a situation that caused many different alphabets to evolve from it.\n\n===European alphabets===\n[[File:ZographensisColour.jpg|thumb|[[Codex Zographensis]] in the [[Glagolitic alphabet]] from Medieval [[Bulgaria]]]]\nThe [[Greek alphabet]], in its [[Euboean alphabet|Euboean form]], was carried over by Greek colonists to the Italian peninsula, where it gave rise to a variety of alphabets used to write the [[Italic languages]]. One of these became the [[Latin alphabet]], which was spread across Europe as the Romans expanded their empire. Even after the fall of the Roman state, the alphabet survived in intellectual and religious works. It eventually became used for the descendant languages of Latin (the [[Romance languages]]) and then for most of the other languages of Europe.\n\nSome adaptations of the Latin alphabet are augmented with [[ligature (typography)|ligatures]], such as [[æ]] in [[Danish language|Danish]] and [[Icelandic language|Icelandic]] and [[Ou (letter)|Ȣ]] in [[Algonquian languages|Algonquian]]; by borrowings from other alphabets, such as the [[thorn (letter)|thorn]] þ in [[Old English language|Old English]] and [[Icelandic language|Icelandic]], which came from the [[Runic alphabet|Futhark]] runes; and by modifying existing letters, such as the [[Eth (letter)|eth]] ð of Old English and Icelandic, which is a modified ''d''. Other alphabets only use a subset of the Latin alphabet, such as Hawaiian, and [[Italian language|Italian]], which uses the letters ''j, k, x, y'' and ''w'' only in foreign words.\n\nAnother notable script is [[Elder Futhark]], which is believed to have evolved out of one of the [[Old Italic alphabet]]s. Elder Futhark gave rise to a variety of alphabets known collectively as the [[Runic alphabet]]s. The Runic alphabets were used for Germanic languages from AD 100 to the late Middle Ages. Its usage is mostly restricted to engravings on stone and jewelry, although inscriptions have also been found on bone and wood. These alphabets have since been replaced with the Latin alphabet, except for decorative usage for which the runes remained in use until the 20th century.\n\nThe [[Old Hungarian script]] is a contemporary writing system of the Hungarians. It was in use during the entire history of Hungary, albeit not as an official writing system. From the 19th century it once again became more and more popular.\n\nThe [[Glagolitic alphabet]] was the initial script of the liturgical language [[Old Church Slavonic]] and became, together with the Greek uncial script, the basis of the [[Cyrillic script]]. Cyrillic is one of the most widely used modern alphabetic scripts, and is notable for its use in Slavic languages and also for other languages within the former [[Soviet Union]]. [[Cyrillic alphabets]] include the [[Serbian Cyrillic alphabet|Serbian]], [[Macedonian alphabet|Macedonian]], [[Bulgarian alphabet|Bulgarian]],  [[Russian alphabet|Russian]], [[Belarusian alphabet|Belarusian]] and [[Ukrainian alphabet|Ukrainian]]. The Glagolitic alphabet is believed to have been created by [[Saints Cyril and Methodius]], while the Cyrillic alphabet was invented by [[Clement of Ohrid]], who was their disciple. They feature many letters that appear to have been borrowed from or influenced by the [[Greek alphabet]] and the [[Hebrew alphabet]].\n\nThe longest European alphabet is the Latin-derived [[Slovak alphabet]] which has 46 letters.\n\n===Asian alphabets===\nBeyond the logographic [[Written Chinese|Chinese writing]], many phonetic scripts are in existence in Asia. The [[Arabic alphabet]], [[Hebrew alphabet]], [[Syriac alphabet]], and other [[abjad]]s of the Middle East are developments of the [[Aramaic alphabet]].\n\nMost alphabetic scripts of India and Eastern Asia are descended from the [[Brahmi script]], which is often believed to be a descendant of Aramaic.\n\n[[File:Zhuyin on cell phone detail.jpeg|thumb|left|250px|[[Bopomofo|Zhuyin]] on a cell phone]]\n\nIn [[Korea]], the [[Hangul]] alphabet was created by [[Sejong the Great]].<ref>\"上親制諺文二十八字…是謂訓民正音(His majesty created 28 characters himself... It is [[Hunminjeongeum]] (original name for [[Hangul]]))\", 《세종실록 (The Annals of the Choson Dynasty : Sejong)》 25년 12월.</ref> Hangul is a unique alphabet: it is a [[featural alphabet]], where many of the letters are designed from a sound's place of articulation (P to look like the widened mouth, L to look like the tongue pulled in, etc.); its design was planned by the government of the day; and it places individual letters in syllable clusters with equal dimensions, in the same way as [[Chinese characters]], to allow for mixed-script writing<ref>{{cite web|url=https://kuiwon.wordpress.com/2013/10/16/on-hangul-supremacy-exclusivity-mixed-script-predates-the-japanese-colonial-period/|title=On Hangul Supremacy & Exclusivity—Mixed Script Predates the Japanese Colonial Period|date=October 16, 2013|author=Kuiwon|website=kuiwon.wordpress.com}}</ref> (one syllable always takes up one type-space no matter how many letters get stacked into building that one sound-block).\n\n[[Zhuyin]] (sometimes called ''Bopomofo'') is a [[semi-syllabary]] used to phonetically transcribe [[Standard Chinese|Mandarin Chinese]] in the [[Taiwan|Republic of China]]. After the later establishment of the [[China|People's Republic of China]] and its adoption of [[Pinyin|Hanyu Pinyin]], the use of Zhuyin today is limited, but it is still widely used in [[Taiwan]] where the Republic of China still governs. Zhuyin developed out of a form of Chinese shorthand based on Chinese characters in the early 1900s and has elements of both an alphabet and a syllabary. Like an alphabet the phonemes of [[syllable onset|syllable initials]] are represented by individual symbols, but like a syllabary the phonemes of the [[syllable rime|syllable finals]] are not; rather, each possible final (excluding the [[Syllable medial|medial glide]]) is represented by its own symbol. For example, ''luan'' is represented as ㄌㄨㄢ (''l-u-an''), where the last symbol ㄢ represents the entire final ''-an''. While Zhuyin is not used as a mainstream writing system, it is still often used in ways similar to a [[romanization]] system—that is, for aiding in pronunciation and as an input method for Chinese characters on computers and cellphones.\n\nEuropean alphabets, especially Latin and Cyrillic, have been adapted for many languages of Asia. Arabic is also widely used, sometimes as an abjad (as with [[Urdu alphabet|Urdu]] and [[Persian alphabet|Persian]]) and sometimes as a complete alphabet (as with [[Kurdish alphabet|Kurdish]] and [[Uyghur alphabet|Uyghur]]).\n\n==Types==\n[[File:Writing systems worldwide.png|460px|thumb|\n{{aligned table|cols=4|style=font-size:90%;\n|titlestyle = font-weight:bold; font-size:105%; padding-bottom:5px\n|title = Predominant national and selected regional or minority scripts\n|row1header=y|row1style=background:lavender;font-weight:normal;text-align:center;border-right:1px #fefefe solid;\n|row2style=white-space:nowrap|fullwidth=y\n| Alphabetic | {{longitem|[[Logogram|[L]ogographic]]<br />and [[Syllabary|[S]yllabic]]}} | [[Abjad]] | [[Abugida]]\n| {{legend|#aaa|[[Latin script|Latin]]}} {{legend|#008080|[[Cyrillic script|Cyrillic]]}} {{legend|blue|[[Greek alphabet|Greek]]}} {{legend|#1E90FF|[[Armenian alphabet|Armenian]]}} {{legend|#00FFFF|[[Georgian scripts|Georgian]]}} {{legend|#FF00FF|[[Hangul]]}}\n| {{legend|#8B0000|[[Hanzi]] {{smaller|[L]}}}} {{legend|#FF0000|[[Kana]] {{smaller|[S]}}{{\\}}[[Kanji]] {{smaller|[L]}}{{nbsp|2}}}}\n| {{legend|green|[[Arabic script|Arabic]]}} {{legend|#00ff7f|[[Hebrew alphabet|Hebrew]]}}\n| {{legend| #FFA500|[[Brahmic scripts|North Indic]]}} {{legend|#D2691E|[[Brahmic scripts|South Indic]]}} {{legend|#8B4513\n|[[Ge'ez script|Ethiopic]]}} {{legend|#808000|[[Thaana]]}} {{legend|#FFFF80|[[Canadian Aboriginal syllabics|Canadian syllabic]]}}\n}}]]\n{{alphabet}}\n\nThe term \"alphabet\" is used by [[Linguistics|linguists]] and [[paleographer]]s in both a wide and a narrow sense. In the wider sense, an alphabet is a script that is ''segmental'' at the [[phoneme]] level—that is, it has separate glyphs for individual sounds and not for larger units such as syllables or words. In the narrower sense, some scholars distinguish \"true\" alphabets from two other types of segmental script, [[abjad]]s and [[abugida]]s. These three differ from each other in the way they treat vowels: abjads have letters for consonants and leave most vowels unexpressed; abugidas are also consonant-based, but indicate vowels with [[diacritic]]s to or a systematic graphic modification of the consonants. In alphabets in the narrow sense, on the other hand, consonants and vowels are written as independent letters.<ref>For critics of the abjad-abugida-alphabet distinction, see Reinhard G. Lehmann: \"27-30-22-26. How Many Letters Needs an Alphabet? The Case of Semitic\", in: The idea of writing: Writing across borders / edited by Alex de Voogt and Joachim Friedrich Quack, Leiden: Brill 2012, p. 11-52, esp p. 22-27</ref> The earliest known alphabet in the wider sense is the [[Middle Bronze Age alphabets|Wadi el-Hol script]], believed to be an abjad, which through its successor [[Phoenician alphabet|Phoenician]] is the ancestor of modern alphabets, including [[Arabic alphabet|Arabic]], [[Greek alphabet|Greek]], [[Latin alphabet|Latin]] (via the [[Old Italic alphabet]]), [[Cyrillic]] (via the Greek alphabet) and [[Hebrew alphabet|Hebrew]] (via [[Aramaic alphabet|Aramaic]]).\n\nExamples of present-day abjads are the [[Arabic script|Arabic]] and [[Hebrew script]]s; true alphabets include [[Latin script|Latin]], Cyrillic, and Korean [[hangul]]; and abugidas are used to write [[tigrinya language|Tigrinya]], [[Amharic language|Amharic]], [[Hindi]], and [[Thai language|Thai]]. The [[Canadian Aboriginal syllabics]] are also an abugida rather than a syllabary as their name would imply, since each glyph stands for a consonant that is modified by rotation to represent the following vowel. (In a true syllabary, each consonant-vowel combination would be represented by a separate glyph.)\n\nAll three types may be augmented with syllabic glyphs. [[Ugaritic script|Ugaritic]], for example, is basically an abjad, but has syllabic letters for {{IPA|/ʔa, ʔi, ʔu/}}. (These are the only time vowels are indicated.) Cyrillic is basically a true alphabet, but has syllabic letters for {{IPA|/ja, je, ju/}} (я, е, ю); [[Coptic alphabet|Coptic]] has a letter for {{IPA|/ti/}}. [[Devanagari]] is typically an abugida augmented with dedicated letters for initial vowels, though some traditions use अ as a [[zero consonant]] as the graphic base for such vowels.\n\nThe boundaries between the three types of segmental scripts are not always clear-cut. For example, [[Sorani]] [[Kurdish language|Kurdish]] is written in the [[Arabic script]], which is normally an abjad. However, in Kurdish, writing the vowels is mandatory, and full letters are used, so the script is a true alphabet. Other languages may use a Semitic abjad with mandatory vowel diacritics, effectively making them abugidas. On the other hand, the [[Phagspa script]] of the [[Mongol Empire]] was based closely on the [[Tibetan script|Tibetan abugida]], but all vowel marks were written after the preceding consonant rather than as diacritic marks. Although short ''a'' was not written, as in the Indic abugidas, one could argue that the linear arrangement made this a true alphabet. Conversely, the vowel marks of the [[Ge'ez alphabet|Tigrinya abugida]] and the [[Ge'ez alphabet|Amharic abugida]] (ironically, the original source of the term \"abugida\") have been so completely assimilated into their consonants that the modifications are no longer systematic and have to be learned as a syllabary rather than as a segmental script. Even more extreme, the Pahlavi abjad eventually became [[logogram|logographic]]. (See below.)\n[[File:Ethiopic genesis.jpg|thumbnail|left|[[Ge'ez Script]] of [[Ethiopia]] and [[Eritrea]]]]\nThus the primary [[Categorisation|classification]] of alphabets reflects how they treat vowels. For [[Tone (linguistics)|tonal languages]], further classification can be based on their treatment of tone, though names do not yet exist to distinguish the various types. Some alphabets disregard tone entirely, especially when it does not carry a heavy functional load, as in [[Somali language|Somali]] and many other languages of Africa and the Americas. Such scripts are to tone what abjads are to vowels. Most commonly, tones are indicated with diacritics, the way vowels are treated in abugidas. This is the case for [[Vietnamese alphabet|Vietnamese]] (a true alphabet) and [[Thai alphabet|Thai]] (an abugida). In Thai, tone is determined primarily by the choice of consonant, with diacritics for disambiguation. In the [[Pollard script]], an abugida, vowels are indicated by diacritics, but the placement of the diacritic relative to the consonant is modified to indicate the tone. More rarely, a script may have separate letters for tones, as is the case for [[Hmong alphabet|Hmong]] and [[Zhuang alphabet|Zhuang]]. For most of these scripts, regardless of whether letters or diacritics are used, the most common tone is not marked, just as the most common vowel is not marked in Indic abugidas; in [[Zhuyin]] not only is one of the tones unmarked, but there is a diacritic to indicate lack of tone, like the [[virama]] of Indic.\n\nThe number of letters in an alphabet can be quite small. The Book [[Pahlavi scripts|Pahlavi]] script, an abjad, had only twelve letters at one point, and may have had even fewer later on. Today the [[Rotokas alphabet]] has only twelve letters. (The [[Hawaiian alphabet]] is sometimes claimed to be as small, but it actually consists of 18 letters, including the [[ʻOkina|ʻokina]] and five long vowels.  However, [[Hawaiian Braille]] has only 13 letters.) While Rotokas has a small alphabet because it has few phonemes to represent (just eleven), Book Pahlavi was small because many letters had been ''conflated''—that is, the graphic distinctions had been lost over time, and diacritics were not developed to compensate for this as they were in [[Arabic alphabet|Arabic]], another script that lost many of its distinct letter shapes. For example, a comma-shaped letter represented ''g'', ''d'', ''y'', ''k'', or ''j''. However, such apparent simplifications can perversely make a script more complicated. In later Pahlavi [[papyrus|papyri]], up to half of the remaining graphic distinctions of these twelve letters were lost, and the script could no longer be read as a sequence of letters at all, but instead each word had to be learned as a whole—that is, they had become [[logogram]]s as in Egyptian [[Demotic Egyptian|Demotic]].\n[[File:Venn diagram showing Greek, Latin and Cyrillic letters.svg|left|thumb|263x263px|Circles containing the [[Greek alphabet|Greek]] (left), [[Cyrillic alphabet|Cyrillic]] (bottom) and [[Latin alphabet|Latin]] (right) alphabets, which share many of the same [[Letter (alphabet)|letters]], although they have different pronunciations]]\nThe largest segmental script is probably an abugida, [[Devanagari]]. When written in Devanagari, Vedic [[Sanskrit]] has an alphabet of 53 letters, including the ''visarga'' mark for final aspiration and special letters for ''kš'' and ''jñ,'' though one of the letters is theoretical and not actually used. The Hindi alphabet must represent both Sanskrit and modern vocabulary, and so has been expanded to 58 with the ''khutma'' letters (letters with a dot added) to represent sounds from Persian and English. Thai has a total of 59 symbols, consisting of 44 consonants, 13 vowels and 2 syllabics, not including 4 diacritics for tone marks and one for vowel length.\n\nThe largest known abjad is [[Sindhi language|Sindhi]], with 51 letters. The largest alphabets in the narrow sense include [[Kabardian language|Kabardian]] and [[Abkhaz language|Abkhaz]] (for [[Cyrillic]]), with 58 and 56 letters, respectively, and [[Slovak language|Slovak]] (for the [[Latin script]]), with 46. However, these scripts either count [[digraph (orthography)|di- and tri-graphs]] as separate letters, as Spanish did with ''ch'' and ''ll'' until recently, or uses [[diacritic]]s like Slovak ''č''.\n\nThe [[Georgian alphabet]] ({{lang-ka|ანბანი}} ''{{transl|ka|Anbani}}'') is an alphabetic writing system. With 33 letters, it is the largest true alphabet where each letter is graphically independent.{{citation needed|reason=It is unclear why having 33 (or 34? this one conflicts with the next sentence) graphically independent letters makes the Georgian alphabet the largest true one? For example, the Armenian alphabet has 39 graphically independent letters. See 'Armenian alphabet' and 'Georgian scripts' pages for more information.|date=August 2016}} The original Georgian alphabet had 38 letters but 5 letters were removed in 19th century by [[Ilia Chavchavadze]]. The Georgian alphabet is much closer to Greek than the other Caucasian alphabets. The letter order parallels the Greek, with the consonants without a Greek equivalent organized at the end of the alphabet. The origins of the alphabet are still unknown. Some Armenian and Western scholars believe it was created by Mesrop Mashtots (Armenian: Մեսրոպ Մաշտոց Mesrop Maštoc') also known as Mesrob the Vartabed, who was an early medieval Armenian linguist, theologian, statesman and hymnologist, best known for inventing the Armenian alphabet c. 405 AD;<ref name=\"Donald Rayfield\">{{cite book |url=https://books.google.com/books?id=VstdAgAAQBAJ&lpg=PP1&vq=unlikely%20pre-Christian&pg=PT18#v=onepage&q=pre-Christian&f=false |first=Donald |last=Rayfield |authorlink=Donald Rayfield |title=The Literature of Georgia: A History |series=Caucasus World |year=2013 |publisher=Routledge |isbn=978-0-7007-1163-5 |page=19 |quote=The Georgian alphabet seems unlikely to have a pre-Christian origin, for the major archaeological monument of the first century first century AD, the bilingual Armazi gravestone commemorating Serafita, daughter of the Georgian viceroy of Mtskheta, is inscribed in Greek and Aramaic only. It has been believed, and not only in Armenia, that all the Caucasian alphabets—Armenian, Georgian and Caucaso-Albanian—were invented in the fourth century by the Armenian scholar Mesrop Mashtots.... The Georgian chronicles ''The Life of Kartli'' (ქართლის ცხოვრება) assert that a Georgian script was invented two centuries before Christ, an assertion unsupported by archaeology. There is a possibility that the Georgians, like many minor nations of the area, wrote in a foreign language—Persian, Aramaic, or Greek—and translated back as they read.}}</ref><ref>[[Glen Warren Bowersock]], [[Peter Robert Lamont Brown]], [[Oleg Grabar]]. ''Late Antiquity: A Guide to the Postclassical World''. Harvard University Press, 1999. {{ISBN|0-674-51173-5}}. p. 289. [[James R. Russell]]. Alphabets. \"Mastoc' was a charismatic visionary who accomplished his task at a time when Armenia stood in danger of losing both its national identity, through partition, and its newly acquired Christian faith, through Sassanian pressure and reversion to paganism. By preaching in Armenian, he was able to undermine and co-opt the discourse founded in native tradition, and to create a counterweight against both Byzantine and Syriac cultural hegemony in the church. Mastoc' also created the Georgian and Caucasian-Albanian alphabets, based on the Armenian model.\"</ref> other Georgian<ref name=\"Javakhishvili\">Georgian: ივ. ჯავახიშვილი, ქართული პალეოგრაფია, გვ. 205–208, 240–245</ref> and Western<ref name=\"Lig1\">{{cite journal |url=https://www.academia.edu/1355678 |title=The Creation of the Caucasian Alphabets as Phenomenon of Cultural History |first= Werner |last= Seibt}}</ref> scholars are against this theory.\n\nSyllabaries typically contain 50 to 400 glyphs, and the glyphs of logographic systems typically number from the many hundreds into the thousands. Thus a simple count of the number of distinct symbols is an important clue to the nature of an unknown script.\n\nThe [[Armenian alphabet]] ({{lang-hy|Հայոց գրեր}} ''{{transl|hy|Hayots grer}}'' or {{lang|hy|Հայոց այբուբեն}} ''{{transl|hy|Hayots aybuben}}'') is a graphically unique alphabetical writing system that has been used to write the Armenian language. It was created in year 405 A.D. originally contained 36 letters. Two more letters, օ (o) and ֆ (f), were added in the Middle Ages. During the 1920s orthography reform, a new letter և (capital ԵՎ) was added, which was a ligature before ե+ւ, while the letter Ւ ւ was discarded and reintroduced as part of a new letter ՈՒ ու (which was a digraph before).\n\n[[File:Ishkhani inscription.jpg|thumb|Old Georgian alphabet inscription on Monastery gate]]\n\nThe Armenian script's directionality is horizontal left-to-right, like the Latin and Greek alphabets.<ref>{{cite web|url=http://www.omniglot.com/writing/armenian.htm|title=Armenian alphabet|first=Simon|last=Ager|year=2010|website=Omniglot|accessdate=2010-01-02|archiveurl= https://web.archive.org/web/20100102062010/http://omniglot.com/writing/armenian.htm| archivedate= 2 January 2010 | deadurl= no}}</ref> It also uses [[bicameral script]] like those. The Armenian word for \"alphabet\" is {{lang|hy|այբուբեն}} ''{{transl|hy|aybuben}}'' ({{IPA-hy|ɑjbubɛn}}), named after the first two letters of the Armenian alphabet Ա այբ ayb and Բ բեն ben. \n\n==Alphabetical order==\n{{main article|Alphabetical order}}\nAlphabets often come to be associated with a standard ordering of their letters, which can then be used for purposes of [[collation]]—namely for the listing of words and other items in what is called ''[[alphabetical order]]''.\n\nThe basic ordering of the [[Latin alphabet]] (A\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nP\nQ\nR\nS\nT\nU\nV\nW\nX\nY\nZ), which is derived from the Northwest Semitic \"Abgad\" order,<ref>Reinhard G. Lehmann: \"27-30-22-26. How Many Letters Needs an Alphabet? The Case of Semitic\", in: The idea of writing: Writing across borders / edited by Alex de Voogt and Joachim Friedrich Quack, Leiden: Brill 2012, p. 11-52</ref> is well established, although languages using this alphabet have different conventions for their treatment of modified letters (such as the [[French language|French]] ''é'', ''à'', and ''ô'') and of certain combinations of letters ([[Multigraph (orthography)|multigraphs]]). In French, these are not considered to be additional letters for the purposes of collation. However, in [[Icelandic language|Icelandic]], the accented letters such as ''á'', ''í'', and ''ö'' are considered distinct letters representing different vowel sounds from the sounds represented by their unaccented counterparts. In Spanish, ''ñ'' is considered a separate letter, but accented vowels such as ''á'' and ''é'' are not. The ''ll'' and ''ch'' were also considered single letters, but in 1994 the [[Real Academia Española]] changed the collating order so that ''ll'' is between ''lk'' and ''lm'' in the dictionary and ''ch'' is between ''cg'' and ''ci'', and in 2010 the tenth congress of the [[Association of Spanish Language Academies]] changed it so they were no longer letters at all.<ref>Real Academia Española. \"Spanish Pronto!: Spanish Alphabet.\" Spanish Pronto! 22 April 2007. January 2009 [http://www.spanishpronto.com/spanishpronto/spanishalphabet.html#english Spanish Pronto: Spanish ↔ English Medical Translators.] {{webarchive|url=https://web.archive.org/web/20070906105503/https://www.spanishpronto.com/spanishpronto/spanishalphabet.html |date=6 September 2007 }}</ref><ref>\"La 'i griega' se llamará 'ye'\". Cuba Debate. 2010-11-05. Retrieved 12 December 2010. [http://www.cubadebate.cu/noticias/2010/11/05/la-i-griega-se-llamara-ye-y-la-ch-y-la-ll-desaparecen-por-decreto-de-la-academia-espanola/ Cubadebate.cu]</ref>\n\nIn German, words starting with ''sch-'' (which spells the German phoneme {{IPAslink|ʃ}}) are inserted between words with initial ''sca-'' and ''sci-'' (all incidentally loanwords) instead of appearing after initial ''sz'', as though it were a single letter—in contrast to several languages such as [[Albanian alphabet|Albanian]], in which ''dh-'', ''ë-'', ''gj-'', ''ll-'', ''rr-'', ''th-'', ''xh-'' and ''zh-'' (all representing phonemes and considered separate single letters) would follow the letters ''d'', ''e'', ''g'', ''l'', ''n'', ''r'', ''t'', ''x'' and ''z'' respectively, as well as Hungarian and Welsh. Further, German words with [[umlaut (diacritic)|umlaut]] are collated ignoring the umlaut—contrary to [[Turkish alphabet|Turkish]] that adopted the [[grapheme]]s '''ö''' and '''ü''', and where a word like ''tüfek'', would come after ''tuz'', in the dictionary. An exception is the German telephone directory where umlauts are sorted like ''ä'' = ''ae'' since names such as ''Jäger'' also appear with the spelling ''Jaeger'', and are not distinguished in the spoken language.\n\nThe [[Danish orthography|Danish]] and [[Norwegian orthography|Norwegian]] alphabets end with ''æ''—''ø''—''å'', whereas the Swedish and [[Finnish orthography|Finnish]] ones conventionally put ''å''—''ä''—''ö'' at the end.\n\nIt is unknown whether the earliest alphabets had a defined sequence. Some alphabets today, such as the [[Hanuno'o script]], are learned one letter at a time, in no particular order, and are not used for [[collation]] where a definite order is required. However, a dozen [[Ugaritic alphabet|Ugaritic]] tablets from the fourteenth century BC preserve the alphabet in two sequences. One, the ''ABCDE'' order later used in Phoenician, has continued with minor changes in [[Hebrew alphabet|Hebrew]], [[Greek alphabet|Greek]], [[Armenian alphabet|Armenian]], [[Gothic alphabet|Gothic]], [[Cyrillic]], and [[Latin alphabet|Latin]]; the other, ''HMĦLQ,'' was used in southern Arabia and is preserved today in [[Ge'ez alphabet|Ethiopic]].<ref>{{harvnb|Millard|1986|p=395}}</ref> Both orders have therefore been stable for at least 3000 years.\n\n[[Runic alphabet|Runic]] used an unrelated [[Elder Futhark|Futhark]] sequence, which was later [[Younger Futhark|simplified]]. [[Arabic alphabet|Arabic]] uses its own sequence, although Arabic retains the traditional [[abjadi order]] for numbering.\n\nThe [[Brahmic family]] of alphabets used in India use a unique order based on [[phonology]]: The letters are arranged according to how and where they are produced in the mouth. This organization is used in Southeast Asia, Tibet, Korean [[hangul]], and even Japanese [[kana]], which is not an alphabet.\n\n==Names of letters==\nThe Phoenician letter names, in which each letter was associated with a word that begins with that sound ([[acrophony]]), continue to be used to varying degrees in [[Samaritan alphabet|Samaritan]], [[Aramaic alphabet|Aramaic]], [[Syriac alphabet|Syriac]], [[Hebrew alphabet|Hebrew]], [[Greek alphabet|Greek]] and [[Arabic alphabet|Arabic]].\n\nThe names were abandoned in [[Latin alphabet|Latin]], which instead referred to the letters by adding a vowel (usually e) before or after the consonant; the two exceptions were [[Y]] and [[Z]], which were borrowed from the Greek alphabet rather than Etruscan, and were known as ''Y Graeca'' \"Greek Y\" (pronounced ''I Graeca'' \"Greek I\") and ''zeta'' (from Greek)—this discrepancy was inherited by many European languages, as in the term ''zed'' for Z in all forms of English other than American English. Over time names sometimes shifted or were added, as in ''double U'' for [[W]] (\"double V\" in French), the English name for Y, and American ''zee'' for Z. Comparing names in English and French gives a clear reflection of the [[Great Vowel Shift]]: A, B, C and D are pronounced {{IPA|/eɪ, biː, siː, diː/}} in today's English, but in contemporary French they are {{IPA|/a, be, se, de/}}. The French names (from which the English names are derived) preserve the qualities of the English vowels from before the Great Vowel Shift. By contrast, the names of F, L, M, N and S ({{IPA|/ɛf, ɛl, ɛm, ɛn, ɛs/}}) remain the same in both languages, because \"short\" vowels were largely unaffected by the Shift.\n\nIn Cyrillic originally the letters were given names based on Slavic words; this was later abandoned as well in favor of a system similar to that used in Latin.\n\nLetters of [[Armenian alphabet]] also have distinct letter names.\n\n==Orthography and pronunciation==\n{{main article|Phonemic orthography}}\nWhen an alphabet is adopted or developed to represent a given language, an [[orthography]] generally comes into being, providing rules for the [[spelling]] of words in that language. In accordance with the principle on which alphabets are based, these rules will generally map letters of the alphabet to the [[phoneme]]s (significant sounds) of the spoken language. In a perfectly [[phonemic orthography]] there would be a consistent one-to-one correspondence between the letters and the phonemes, so that a writer could predict the spelling of a word given its pronunciation, and a speaker would always know the pronunciation of a word given its spelling, and vice versa. However this ideal is not usually achieved in practice; some languages (such as [[Spanish language|Spanish]] and [[Finnish language|Finnish]]) come close to it, while others (such as English) deviate from it to a much larger degree.\n\nThe pronunciation of a language often evolves independently of its writing system, and writing systems have been borrowed for languages they were not designed for, so the degree to which letters of an alphabet correspond to phonemes of a language varies greatly from one language to another and even within a single language.\n\nLanguages may fail to achieve a one-to-one correspondence between letters and sounds in any of several ways:\n* A language may represent a given phoneme by a combination of letters rather than just a single letter. Two-letter combinations are called [[digraph (orthography)|digraphs]] and three-letter groups are called [[trigraph (orthography)|trigraphs]]. [[German language|German]] uses the [[tetragraph]]s (four letters) \"tsch\" for the phoneme {{IPA-de|tʃ|}} and (in a few borrowed words) \"dsch\" for {{IPA|[dʒ]}}. [[Kabardian language|Kabardian]] also uses a tetragraph for one of its phonemes, namely \"кхъу\". Two letters representing one sound occur in several instances in Hungarian as well (where, for instance, ''cs'' stands for [tʃ], ''sz'' for [s], ''zs'' for [ʒ], ''dzs'' for [dʒ]).\n* A language may represent the same phoneme with two or more different letters or combinations of letters. An example is [[modern Greek]] which may write the phoneme {{IPA-el|i|}} in six different ways: {{angbr|ι}}, {{angbr|η}}, {{angbr|υ}}, {{angbr|ει}}, {{angbr|οι}}, and {{angbr|υι}} (though the last is rare).\n* A language may spell some words with unpronounced letters that exist for historical or other reasons. For example, the spelling of the Thai word for \"beer\" [เบียร์] retains a letter for the final consonant \"r\" present in the English word it was borrowed from, but silences it.\n* Pronunciation of individual words may change according to the presence of surrounding words in a sentence ([[sandhi]]).\n* Different dialects of a language may use different phonemes for the same word.\n* A language may use different sets of symbols or different rules for distinct sets of vocabulary items, such as the Japanese [[hiragana]] and [[katakana]] syllabaries, or the various rules in English for spelling words from Latin and Greek, or the original [[Germanic languages|Germanic]] vocabulary.\n\nNational languages sometimes elect to address the problem of dialects by simply associating the alphabet with the national standard. Some national languages like [[Finnish language|Finnish]], [[Armenian language|Armenian]], [[Turkish language|Turkish]], [[Russian language|Russian]], [[Serbo-Croatian language|Serbo-Croatian]] ([[Serbian language|Serbian]], [[Croatian language|Croatian]] and [[Bosnian language|Bosnian]]) and [[Bulgarian language|Bulgarian]] have a very regular spelling system with a nearly one-to-one correspondence between letters and phonemes. Strictly speaking, these national languages lack a word corresponding to the verb \"to spell\" (meaning to split a word into its letters), the closest match being a verb meaning to split a word into its syllables. Similarly, the [[Italian language|Italian]] verb corresponding to 'spell (out)', ''compitare'', is unknown to many Italians because spelling is usually trivial, as Italian spelling is highly phonemic. In standard [[Spanish language|Spanish]], one can tell the pronunciation of a word from its spelling, but not vice versa, as certain phonemes can be represented in more than one way, but a given letter is consistently pronounced. [[French language|French]], with its [[silent letter]]s and its heavy use of [[nasal vowel]]s and [[elision]], may seem to lack much correspondence between spelling and pronunciation, but its rules on pronunciation, though complex, are actually consistent and predictable with a fair degree of accuracy.\n\nAt the other extreme are languages such as English, where the pronunciations of many words simply have to be memorized as they do not correspond to the spelling in a consistent way. For English, this is partly because the [[Great Vowel Shift]] occurred after the orthography was established, and because English has acquired a large number of loanwords at different times, retaining their original spelling at varying levels. Even English has general, albeit complex, rules that predict pronunciation from spelling, and these rules are successful most of the time; rules to predict spelling from the pronunciation have a higher failure rate.\n\nSometimes, countries have the written language undergo a [[spelling reform]] to realign the writing with the contemporary spoken language. These can range from simple spelling changes and word forms to switching the entire writing system itself, as when [[Turkey]] switched from the Arabic alphabet to a Latin-based [[Turkish alphabet]].\n\nThe standard system of symbols used by [[linguist]]s to represent sounds in any language, independently of orthography, is called the [[International Phonetic Alphabet]].\n\n== See also ==\n{{div col|colwidth=22em}}\n* {{Portal inline|size=tiny|Alphabets}}\n* ''[[A Is For Aardvark]]''\n* [[Abecedarium]]\n* [[Acrophony]]\n* [[Akshara]]\n* [[Alphabet book]]\n* [[Alphabet effect]]\n* [[Alphabet song]]\n* [[Alphabetical order]]\n* ''[[Butterfly Alphabet]]''\n* [[Character encoding]]\n* [[Constructed script]]\n* [[Cyrillic]]\n* [[English alphabet]]\n* [[Hangul]]\n* [[NATO phonetic alphabet|ICAO (NATO) spelling alphabet]]\n* [[Lipogram]]\n* [[List of alphabets]]\n* [[Pangram]]\n* [[Thai script#Alphabet listing|Thai script]]\n* [[Thoth]]\n* [[Transliteration]]\n* [[Unicode]]\n{{div col end}}\n\n== References ==\n{{Reflist|30em}}\n\n== Bibliography ==\n* {{cite book |last =Coulmas |first=Florian |title=The Writing Systems of the World |publisher=Blackwell Publishers Ltd. |year=1989 |url=https://books.google.com/?id=VOywmavmZ3UC|isbn=978-0-631-18028-9|ref=harv}}\n* {{cite book |last1=Daniels |first1=Peter T. |last2=Bright |first2=William |title=The World's Writing Systems |publisher=[[Oxford University Press]] |year=1996 |isbn=978-0-19-507993-7 |ref=harv}} Overview of modern and some ancient writing systems.\n* {{cite book |author =Driver, G. R. |authorlink=Godfrey Rolles Driver |title=Semitic Writing (Schweich Lectures on Biblical Archaeology S.) 3Rev Ed |publisher=Oxford University Press |year=1976 |isbn=978-0-19-725917-7 |title-link=Schweich Lectures on Biblical Archaeology }}\n* {{Cite book\n | last = Haarmann\n | first = Harald\n | title = Geschichte der Schrift\n |trans-title=History of Writing\n | language=German\n | publisher = C. H. Beck\n | location = München\n | edition = 2nd\n | year = 2004\n | isbn = 978-3-406-47998-4\n | ref = harv\n | postscript = <!--None-->\n}}\n* {{cite book |author =Hoffman, Joel M. |title=In the Beginning: A Short History of the Hebrew Language |url=https://books.google.com/books?id=momIk7nVNdkC |publisher=NYU Press |year=2004 |isbn=978-0-8147-3654-8 }} Chapter 3 traces and summarizes the invention of alphabetic writing.\n* {{cite book |author =Logan, Robert K. |title=The Alphabet Effect: A Media Ecology Understanding of the Making of Western Civilization |publisher=Hampton Press |year=2004 |isbn=978-1-57273-523-1 }}\n* {{cite journal |last1=McLuhan |first1=Marshall |last2=Logan |first2=Robert K. |year=1977 |title=Alphabet, Mother of Invention |journal=ETC: A Review of General Semantics |volume=34 |issue=4 |pages=373–383 |jstor=42575278}}\n* {{Cite journal\n | doi = 10.1080/00438243.1986.9979978\n | jstor = 124703\n | last = Millard\n | first = A. R.\n | year = 1986\n | title = The Infancy of the Alphabet\n | journal = World Archaeology\n | volume = 17\n | issue = 3\n | pages = 390–398\n | ref = harv\n | postscript = <!--None-->\n}}\n* {{cite book |author1=Ouaknin, Marc-Alain |author2=Bacon, Josephine |title=Mysteries of the Alphabet: The Origins of Writing |publisher=Abbeville Press |year=1999 |isbn=978-0-7892-0521-6 }}\n* {{cite book |author =Powell, Barry |title=Homer and the Origin of the Greek Alphabet |publisher=[[Cambridge University Press]] |year=1991 |isbn=978-0-521-58907-9 }}\n* {{cite book |last=Powell |first=Barry B. |year=2009 |title=Writing: Theory and History of the Technology of Civilization|location=Oxford |publisher=Blackwell |isbn=978-1-4051-6256-2}}\n* {{cite book |author =Sacks, David |title=Letter Perfect: The Marvelous History of Our Alphabet from A to Z |publisher=Broadway Books |url=http://www.uca.edu.ar/esp/sec-ffilosofia/esp/docs-institutos/s-cehao/boletin/damqatum3_eng2007.pdf |format=PDF |year=2004 |isbn=978-0-7679-1173-3}}\n* {{cite book|author =Saggs, H. W. F. |title=Civilization Before Greece and Rome|publisher=[[Yale University Press]] |year=1991 |isbn=978-0-300-05031-8}} Chapter 4 traces the invention of writing\n\n== External links ==\n{{Wiktionary|alphabet}}\n{{commons category|Alphabets}}\n* [http://ilovetypography.com/2010/08/07/where-does-the-alphabet-come-from/ The Origins of abc]\n* [http://www.uca.edu.ar/esp/sec-ffilosofia/esp/docs-institutos/s-cehao/boletin/damqatum3_eng2007.pdf \"Language, Writing and Alphabet: An Interview with Christophe Rico\"],  ''Damqātum 3'' (2007)\n* [[Michael Everson]]'s [http://www.evertype.com/alphabets/index.html Alphabets of Europe]\n* [http://www.wam.umd.edu/~rfradkin/alphapage.html Evolution of alphabets], animation by Prof. Robert Fradkin at the [[University of Maryland]]\n* [https://web.archive.org/web/20100429051634/http://www.bib-arch.org/bar/article.asp?PubID=BSBA&Volume=36&Issue=2&ArticleID=6 How the Alphabet Was Born from Hieroglyphs]—Biblical Archaeology Review\n* [https://onedrive.live.com/view.aspx?cid=E39B50D7D9EA3235&resid=E39B50D7D9EA3235!126&app=WordPdf An Early Hellenic Alphabet]\n* [https://web.archive.org/web/20160128234751/http://www.jaars.org/museum/alphabet/index.htm Museum of the Alphabet]\n* [http://www.bbc.co.uk/programmes/p0054950 The Alphabet], BBC Radio 4 discussion with Eleanor Robson, Alan Millard and Rosalind Thomas (''In Our Time'', Dec. 18, 2003)\n\n{{Writing systems |expanded=Alphabets}}\n{{list of writing systems}}\n{{Authority control}}\n\n[[Category:Alphabets| ]]\n[[Category:Orthography]]"
    },
    {
      "title": "History of the alphabet",
      "url": "https://en.wikipedia.org/wiki/History_of_the_alphabet",
      "text": "{{Alphabet}}\nThe history of [[alphabet]]ic writing goes back to the consonantal writing system used for [[Semitic languages]] in the [[Levant]] in the 2nd millennium BCE. Most or nearly all alphabetic scripts used throughout the world today ultimately go back to this Semitic proto-alphabet.<ref>{{cite book|last=Sampson| first=Geoffrey| title=Writing systems: A linguistic introduction|publisher=Stanford University Press| url=https://books.google.com/?id=tVcdNRvwoDkC&pg=PA9&dq=Sampson,+Geoffrey+%281985%29.+Writing+systems:+a+linguistic+introduction#v=onepage&q=Sampson%2C%20Geoffrey%20%281985%29.%20Writing%20systems%3A%20a%20linguistic%20introduction&f=false |year=1985| isbn=0-8047-1254-9| page=77}}</ref> Its first origins can be traced back to a [[Proto-Sinaitic]] script developed in [[Ancient Egypt]] to represent the language of Semitic-speaking workers in Egypt. This script was partly influenced by the older Egyptian [[hieratic]], a cursive script related to [[Egyptian hieroglyphs]].<ref name=\"Himelfarb, Elizabeth J 2000\">Himelfarb, Elizabeth J. \"[http://archive.archaeology.org/0001/newsbriefs/egypt.html First Alphabet Found in Egypt]\", ''Archaeology'' 53, Issue 1 (Jan./Feb. 2000): 21.</ref><ref>{{cite journal|last=Goldwasser|first=Orly|authorlink=Orly Goldwasser|title=How the Alphabet Was Born from Hieroglyphs|journal=Biblical Archaeology Review|volume=36|issue=1|pages=|publisher=Biblical Archaeology Society|location=Washington, DC|date=Mar–Apr 2010|url=https://members.bib-arch.org/biblical-archaeology-review/36/2/6|issn=0098-9444|accessdate=6 Nov 2011}}</ref>\n\nMainly through [[Phoenician alphabet|Phoenician]] and [[Aramaic script|Aramaic]], two closely related members of the Semitic family of scripts that were in use during the early first millennium BCE, the Semitic alphabet became the ancestor of multiple writing systems across the Middle East, Europe, northern Africa and South Asia.\n\nSome modern authors distinguish between consonantal scripts of the Semitic type, called \"[[abjad]]s\", and \"true alphabets\" in the narrow sense,<ref name=\"Blackwell\">{{cite book|last=Coulmas|first=Florian|title=The Blackwell Encyclopedia of Writing Systems|year=1996|publisher=Blackwell Publishers Ltd.|location=Oxford|isbn=0-631-21481-X}}</ref><ref>{{cite book|last1=Daniels|first1=Peter T|last2=Bright|first2=William|year=1996|title=The World's Writing Systems|publisher=Oxford University Press}}</ref> the distinguishing criterion being that true alphabets consistently assign letters to both consonants and vowels on an equal basis, while in an abjad each symbol usually stands for a consonant. In this sense, the first true alphabet was the [[Greek alphabet]], which was adapted from the Phoenician. [[Latin alphabet|Latin]], the most widely used alphabet today,<ref>{{harvnb|Haarmann|2004|p=96}}</ref> in turn derives from Greek (by way of [[Cumae]] and the [[Etruscan civilization|Etruscans]]).\n\n== Predecessors ==\nTwo scripts are well attested from before the end of the fourth millennium BCE: [[cuneiform script|Mesopotamian cuneiform]] and [[Egyptian hieroglyphs]].\nHieroglyphs were employed in three ways in Ancient Egyptian texts: as logograms (ideograms) that represent a word denoting an object pictorially depicted by the hieroglyph; more commonly as phonograms writing a sound or sequence of sounds; and as determinatives (which provide clues to meaning without directly writing sounds).<ref>{{cite web|url=http://csmt.uchicago.edu/glossary2004/hieroglyphics.htm|title=hieroglyphics}}</ref> Since vowels were mostly unwritten, the [[Egyptian hieroglyph#Uniliteral signs|hieroglyphs which indicated a single consonant]] could have been used as a consonantal alphabet (or \"abjad\").  This was not done when writing the Egyptian language, but seems to have been a significant influence{{citation needed|date=April 2014}} on the creation of the first alphabet (used to write a Semitic language). All subsequent alphabets around the world have either descended from this first Semitic alphabet, or have been inspired by one of its descendants (i.e. \"[[stimulus diffusion]]\"), with the possible exception of the [[Meroitic alphabet]], a 3rd-century BCE adaptation of hieroglyphs in [[Nubia]] to the south of Egypt.  The [[Rongorongo]] script of [[Easter Island]] may also be an independently invented alphabet, but too little is known of it to be certain.\n\n== Consonantal alphabets ==\n\n=== Semitic alphabet ===\nThe [[Proto-Sinaitic script]] of Egypt has yet to be fully deciphered. However, it may be alphabetic and probably records the [[Canaanite language]]. The oldest examples are found as [[Graffito (archaeology)|graffiti]] in the Wadi el Hol and date to perhaps 1850 BCE.<ref>J. C. Darnell, F. W. Dobbs-Allsopp, Marilyn J. Lundberg, P. Kyle McCarter, and Bruce Zuckermanet, \"Two early alphabetic inscriptions from the Wadi el-Hol: new evidence for the origin of the alphabet from the western desert of Egypt.\" The Annual of the American Schools of Oriental Research, 59 (2005). {{JSTOR|3768583}}.</ref> The table below shows hypothetical prototypes of the [[Phoenician alphabet]] in Egyptian hieroglyphs. Several correspondences have been proposed with Proto-Sinaitic letters.\n\n{| class=wikitable\n|-\n! Possible Egyptian prototype\n|<hiero>F1</hiero>\n|<hiero>O1</hiero>\n|<hiero>T14</hiero>\n|<hiero>O31</hiero>\n|<hiero>A28</hiero>\n|<hiero>T3</hiero>\n|\n|<hiero>O6</hiero>\n|<hiero>F35</hiero>\n|<hiero>D42</hiero>\n|<hiero>D46</hiero>\n|-\n! [[Phoenician alphabet|Phoenician]]\n|[[Image:PhoenicianA-01.svg|20px]]\n|[[Image:PhoenicianB-01.svg|20px]]\n|[[Image:PhoenicianG-01.svg|20px]]\n|[[Image:PhoenicianD-01.svg|20px]]\n|[[Image:PhoenicianE-01.svg|20px]]\n|[[Image:PhoenicianW-01.svg|20px]]\n|[[Image:PhoenicianZ-01.svg|20px]]\n|[[Image:PhoenicianH-01.svg|20px]]\n|[[Image:PhoenicianTet-01.png|20px]]\n|[[Image:PhoenicianI-01.svg|20px]]\n|[[Image:PhoenicianK-01.svg|20px]]\n|-\n! Possible<br/>acrophony\n|[[ʾalp]] ''ox''\n|[[beth (letter)|bet]] ''house''\n|[[gaml]] ''[[Boomerang#Hunting|thrown hunting club]]''\n|[[daleth|digg]] ''fish'', ''door''\n|[[he (letter)|haw, hillul]] ''jubilation''\n|[[waw (letter)|waw]] ''hook''\n|[[zayin|zen, ziqq]] ''handcuff''\n|[[ḥet]] ''courtyard''/''fence''\n|[[ṭēt]] ''wheel''\n|[[yodh|yad]] ''arm''\n|[[Kaph|kap]] ''hand''\n|}\n<br>\n{| class=wikitable\n|-\n! Possible Egyptian prototype\n|<hiero>S39</hiero>\n|<hiero>N35</hiero>\n|<hiero>I10</hiero>\n|<hiero>R11</hiero>\n|<hiero>D4</hiero>\n|\n|\n|<hiero>V24</hiero>\n|<hiero>D1</hiero>\n|<hiero>F18</hiero>\n|\n|-\n! [[Phoenician alphabet|Phoenician]]\n|[[Image:PhoenicianL-01.svg|20px]]\n|[[File:PhoenicianM-02.svg|20px]]\n|[[Image:PhoenicianN-01.svg|20px]]\n|[[Image:PhoenicianX-01.svg|20px]]\n|[[Image:PhoenicianO-01.svg|20px]]\n|[[Image:PhoenicianP-01.svg|20px]]\n|[[Image:PhoenicianTsade-01.svg|20px]]\n|[[Image:PhoenicianQ-01.svg|20px]]\n|[[Image:PhoenicianR-01.svg|20px]]\n|[[Image:PhoenicianS-01.svg|20px]]\n|[[Image:Proto-semiticT-01.svg|20px]]\n|-\n! Possible<br>acrophony\n|[[Lamedh|lamd]] ''goad''\n|[[mem]] ''water''\n|[[Nun (letter)|nun]] ''large fish''/''snake''\n|[[samek]] ''fish''\n|[[ʿen]] ''eye''\n|[[piʾt]] ''bend''\n|[[ṣad]] ''plant''\n|[[qup]] ''monkey''/''cord of wool''\n|[[raʾs]] ''head''\n|[[šimš|šananuma]] ''bow''\n|[[taw (letter)|taw]] ''signature''\n|}\nThis Semitic script adapted Egyptian hieroglyphs to write consonantal values based on the first sound of the Semitic name for the object depicted by the hieroglyph (the \"acrophonic principle\").<ref>Hooker, J. T., C. B. F. Walker, W. V. Davies, John Chadwick, John F. Healey, B. F. Cook, and Larissa Bonfante, (1990). ''Reading the Past: Ancient Writing from Cuneiform to the Alphabet''. Berkeley: University of California Press. pages 211–213.</ref> So, for example, the hieroglyph ''[[Per (hieroglyph)|per]]'' (\"house\" in Egyptian) was used to write the sound [b] in Semitic, because [b] was the first sound in the Semitic word for \"house\", ''bayt''.<ref>McCarter, P. Kyle. \"The Early Diffusion of the Alphabet.\" ''The Biblical Archaeologist'' 37, No. 3 (Sep., 1974): 54–68. page 57. {{doi|10.2307/3210965}}. {{JSTOR|3210965}}.</ref> The script was used only sporadically, and retained its pictographic nature, for half a millennium, until adopted for governmental use in Canaan{{Citation needed|date=July 2015}}. The first Canaanite states to make extensive use of the alphabet were the [[Phoenicia]]n [[city-state]]s and so later stages of the Canaanite script are called [[Phoenician alphabet|Phoenician]]. The Phoenician cities were maritime states at the center of a vast trade network and soon the Phoenician alphabet spread throughout the Mediterranean.  Two variants of the Phoenician alphabet had major impacts on the history of writing: the [[Aramaic alphabet]] and the [[Greek alphabet]].<ref>{{cite web |url=http://www.bbc.co.uk/dna/h2g2/A2451890 |title=The Development of the Western Alphabet |website=h2g2.com |date=April 8, 2004}}</ref>\n\n=== Descendants of the Aramaic abjad ===\n[[File:Phönizisch-5Sprachen.svg|thumb|left|Chart showing details of four alphabets' descent from Phoenician [[abjad]], from left to right [[Latin alphabet|Latin]], [[Greek alphabet|Greek]], [[Phoenician alphabet|original Phoenician]], [[Hebrew alphabet|Hebrew]], [[Arabic alphabet|Arabic]].]]\n\n[[File:Arabic alphabet world distribution.png|thumb|400px|World distribution of the Arabic alphabet. The dark green areas shows the countries where this alphabet is the sole main script. The light green shows the countries where the alphabet co-exists with other scripts.]]\n\nThe Phoenician and Aramaic alphabets, like their Egyptian prototype, represented only consonants, a system called an ''[[abjad]]''. The Aramaic alphabet, which evolved from the Phoenician in the 7th century BCE, to become the official script of the [[Greater Iran|Persian Empire]], appears to be the ancestor of nearly all the modern alphabets of Asia:\n*The modern [[Hebrew alphabet]] started out as a local variant of Imperial Aramaic. (The original Hebrew alphabet has been retained by the [[Samaritan alphabet|Samaritans]].)<ref>Hooker, J. T., C. B. F. Walker, W. V. Davies, John Chadwick, John F. Healey, B. F. Cook, and Larissa Bonfante, (1990). ''Reading the Past: Ancient Writing from Cuneiform to the Alphabet'', Berkeley: University of California Press. page 222.</ref><ref>Robinson, Andrew, (1995). ''The Story of Writing: Alphabets, Hieroglyphs & Pictograms'', New York: Thames & Hudson Ltd. page 172.</ref>\n*The [[Arabic alphabet]] descended from Aramaic via the [[Nabataean alphabet]] of what is now southern [[Jordan]].\n*The [[Syriac alphabet]] used after the third century CE evolved, through the [[Pahlavi scripts]] and [[Sogdian alphabet]], into the alphabets of [[North Asia]] such as the [[Old Turkic alphabet]] (probably), the [[Old Uyghur alphabet]], the [[Mongolian writing systems]], and the [[Manchu alphabet]].\n*The [[Georgian scripts]] are of uncertain provenance, but appear to be part of the Persian-Aramaic (or perhaps the Greek) family.\n\n== Alphabets with vowels ==\n\n=== Greek alphabet ===\n;Adoption\n\n{{Main|History of the Greek alphabet}}\n[[Image:NAMA Alphabet grec.jpg|thumb|250px|Greek alphabet on an ancient  <!--early 6th c. BCE?-->[[black figure]] vessel. There is a ''[[digamma]]'' but no ''ksi'' or ''omega''. The letter ''phi'' is missing a stroke and looks like the ''omicron'' Ο, but on the underside of the bowl it is a full Φ.]]\n[[File:Cippo perugino, con iscrizione in lingua etrusca su un atto giuridico tra le famiglie dei velthina e degli afuna, 02.jpg|thumbnail|[[Etruscan civilization|Etruscan]] writing, the beginning of the writing with the [[Latin alphabet]].]]\n\nBy at least the 8th century BCE the Greeks borrowed the Phoenician alphabet and adapted it to their own language,<ref name=\"McCarter, P. Kyle 1974 page 62\">McCarter, P. Kyle. \"The Early Diffusion of the Alphabet\", ''The Biblical Archaeologist'' 37, No. 3 (Sep., 1974): 54–68. page 62. {{doi|10.2307/3210965}}. {{JSTOR|3210965}}.</ref> creating in the process the first \"true\" alphabet, in which vowels were accorded equal status with consonants. According to Greek legends transmitted by [[Herodotus]], the alphabet was brought from Phoenicia to Greece by [[Cadmus|Cadmos]]. The letters of the Greek alphabet are the same as those of the Phoenician alphabet, and both alphabets are arranged in the same order.<ref name=\"McCarter, P. Kyle 1974 page 62\"/> However, whereas separate letters for vowels would have actually hindered the legibility of Egyptian, Phoenician, or Hebrew, their absence was problematic for Greek, where [[vowel]]s played a much more important role.<ref>\"there are languages for which an alphabet is ''not'' an ideal writing system. The Semitic abjads really do fit the structure of Hebrew, Aramaic, and Arabic very well, [more] than an alphabet would [...], since the spelling ensures that each root looks the same through its plethora of inflections and derivations.\" Peter Daniels, ''The World's Writing Systems,'' p. 27.</ref> The Greeks used for vowels some of the Phoenician letters representing consonants which weren't used in Greek speech. All of the names of the letters of the Phoenician alphabet started with consonants, and these consonants were what the letters represented, something called the [[acrophony|acrophonic principle]].\n\nHowever, several Phoenician consonants were absent in Greek, and thus several letter names came to be pronounced with initial vowels. Since the start of the name of a letter was expected to be the sound of the letter (the acrophonic principle), in Greek these letters came to be used for vowels. For example, the Greeks had no glottal stop or voiced pharyngeal sounds, so the Phoenician letters ''’alep'' and ''`ayin'' became Greek ''[[alpha (letter)|alpha]]'' and ''o'' (later renamed ''[[omicron (letter)|o micron]]''), and stood for the vowels {{IPA|/a/}} and {{IPA|/o/}} rather than the consonants {{IPA|/ʔ/}} and {{IPA|/ʕ/}}. As this fortunate development only provided for five or six (depending on dialect) of the twelve Greek vowels, the Greeks eventually created [[digraph (orthography)|digraphs]] and other modifications, such as ''ei'', ''ou'', and <u>''o''</u> (which became [[omega (letter)|omega]]), or in some cases simply ignored the deficiency, as in long ''a, i, u''.<ref>Robinson, Andrew, (1995). ''The Story of Writing: Alphabets, Hieroglyphs & Pictograms'', New York: Thames & Hudson Ltd. page 170.</ref>\n\nSeveral varieties of the Greek alphabet developed. One, known as [[Cumae alphabet|Western Greek or Chalcidian]], was used west of [[Athens]] and in [[southern Italy]]. The other variation, known as [[History of the Greek alphabet|Eastern Greek]], was used in Asia Minor (also called Asian Greece i.e. present-day aegean [[Turkey]]). The Athenians (c. 400 BCE) adopted that latter variation and eventually the rest of the Greek-speaking world followed. After first writing right to left, the Greeks eventually chose to write from left to right, unlike the Phoenicians who wrote from right to left.  Many Greek letters are similar to Phoenician, except the letter direction is reversed or changed, which can be the result of historical changes from right-to-left writing to [[boustrophedon]] to left-to-right writing.\n\n;Descendants\n[[Image:Cyrillic alphabet world distribution.svg|thumb|right|350px|World distribution of the Cyrillic alphabet. The dark green areas shows the countries where this alphabet is the sole main script. The light green shows the countries where the alphabet co-exists with other scripts.]]\nGreek is in turn the source of all the modern scripts of Europe. The alphabet of the early western Greek dialects, where the letter [[eta (letter)|eta]] remained an {{IPA|/h/}}, gave rise to the [[Old Italic alphabet]] which in turn developed into the Old [[Roman alphabet]]. In the eastern Greek dialects, which did not have an /h/, eta stood for a vowel, and remains a vowel in modern Greek and all other alphabets derived from the eastern variants: [[Glagolitic alphabet|Glagolitic]], [[Cyrillic script|Cyrillic]], [[Armenian alphabet|Armenian]], [[Gothic alphabet|Gothic]] (which used both Greek and Roman letters), and perhaps [[Georgian alphabet|Georgian]].<ref>Robinson, Andrew. The Story of Writing: Alphabets, Hieroglyphs & Pictograms. New York: Thames & Hudson Ltd., 1995.</ref>\n\nAlthough this description presents the evolution of scripts in a linear fashion, this is a simplification. For example, the [[Manchu alphabet]], descended from the [[abjad]]s of West Asia, was also influenced by Korean [[hangul]]{{cn|date=November 2015}}, which was either independent (the traditional view) or derived from the [[abugida]]s of South Asia. Georgian apparently derives from the Aramaic family, but was strongly influenced in its conception by Greek. A modified version of the Greek alphabet, using an additional half dozen [[Demotic (Egyptian)|demotic]] hieroglyphs, was used to write [[Coptic alphabet|Coptic]] Egyptian. Then there is [[Canadian Aboriginal syllabics|Cree syllabics]] (an [[abugida]]), which is a fusion of [[Devanagari]] and [[Pitman shorthand]] developed by the missionary [[James Evans (linguist)|James Evans]].<ref>Andrew Dalby (2004:139) ''Dictionary of Languages''</ref>\n\n=== Latin alphabet ===\n{{Main|History of the Latin alphabet}}\n[[Image:Latin alphabet world distribution.svg|thumb|right|350px|World distribution of the Latin alphabet. The dark green areas show the countries where this alphabet is the sole main script. The light green shows the countries where the alphabet co-exists with other scripts.]]\n\nA tribe known as the [[Latins (Italic tribe)|Latins]], who became the Romans, also lived in the Italian peninsula like the Western Greeks. From the [[Etruscan civilization|Etruscans]], a tribe living in the first millennium BCE in central [[Italy]], and the Western Greeks, the Latins adopted writing in about the seventh century. In adopting writing from these two groups, the Latins dropped four characters from the Western Greek alphabet. They also adapted the [[Etruscan language|Etruscan letter]] [[F]], pronounced 'w,' giving it the 'f' sound, and the Etruscan S, which had three zigzag lines, was curved to make the modern [[S]]. To represent the [[G]] sound in Greek and the [[K]] sound in Etruscan, the [[Gamma]] was used. These changes produced the modern alphabet without the letters [[G]], [[J]], [[U]], [[W]], [[Y]], and [[Z]], as well as some other differences.\n\n[[C]], [[K]], and [[Q]] in the Roman alphabet could all be used to write both the {{IPA|/k/}} and {{IPA|/ɡ/}} sounds; the Romans soon modified the letter C to make G, inserted it in seventh place, where [[Z]] had been, to maintain the [[gematria]] (the numerical sequence of the alphabet). Over the few centuries after [[Alexander the Great]] conquered the Eastern Mediterranean and other areas in the third century BCE, the Romans began to borrow Greek words, so they had to adapt their alphabet again in order to write these words. From the Eastern Greek alphabet, they borrowed [[Y]] and [[Z]], which were added to the end of the alphabet because the only time they were used was to write Greek words.\n\nThe [[Anglo-Saxons]] began using Roman letters to write [[Old English language|Old English]] as they converted to Christianity, following [[Augustine of Canterbury]]'s mission to Britain in the sixth century. Because the [[Runic alphabet|Runic]] ''wen'', which was first used to represent the sound 'w' and looked like a p that is narrow and triangular, was easy to confuse with an actual p, the 'w' sound began to be written using a double u. Because the u at the time looked like a v, the double u looked like two v's, [[W]] was placed in the alphabet by [[V]]. [[U]] developed when people began to use the rounded [[U]] when they meant the vowel u and the pointed [[V]] when the meant the consonant [[V]]. [[J]] began as a variation of [[I]], in which a long tail was added to the final [[I]] when there were several in a row. People began to use the [[J]] for the consonant and the [[I]] for the vowel by the fifteenth century, and it was fully accepted in the mid-seventeenth century.\n\n[[File:Evolution of minuscule.svg|thumb|none|600px|Simplified relationship between various scripts leading to the development of modern lower case of standard Latin alphabet and that of the modern variants, [[Fraktur (script)|Fraktur]] (used in Germany until recently) and [[Insular script|Insular]]/[[Gaelic type|Gaelic]] (Ireland). Several scripts coexisted such as [[half-uncial]] and [[uncial]], which derive from [[Roman cursive]] and [[Greek uncial]], and [[Visigothic script|Visigothic]], [[Merovingian script|Merovingian (Luxeuil variant here)]] and [[Beneventan script|Beneventan]]. The [[Carolingian script]] was the basis for [[blackletter]] and [[humanist sans-serif|humanist]]. What is commonly called \"gothic writing\" is technically called blackletter (here [[Textualis|Textualis quadrata]]) and is completely unrelated to Visigothic script.<br />The letter j is i with a [[:wikt:flourish|flourish]]; u and v were the same letter in early scripts and were used depending on their position in insular half-uncial and caroline minuscule and later scripts; w is a ligature of vv; in insular the [[rune]] [[wynn]] is used as a w (three other runes in use were the [[Thorn (letter)|thorn]] (þ), ʻféʼ (ᚠ) as an abbreviation for cattle/goods and maðr (ᛘ) for man).<br />The letters y and z were very rarely used; þ was written identically to y, so y was dotted to avoid confusion; the dot was adopted for i only after late-Caroline (protgothic); in Benevetan script the [[scribal abbreviation|macron abbreviation]] featured a dot above.<br />Lost variants such as [[r rotunda]], ligatures and [[scribal abbreviation|scribal abbreviation marks]] are omitted; [[long s]] (ſ) is shown when no terminal s (surviving variant) is present.<br />[[Humanist sans-serif|Humanist script]] was the basis for Venetian [[Typeface|types]] which have changed little to this day, such as [[Times Roman|Times New Roman]] ([[Serif|a serifed typeface]])]]\n\n== Letter names and order ==\nThe order of the letters of the alphabet is attested from the fourteenth century BCE in the town of [[Ugarit]] on [[Syria]]'s northern coast.<ref>Robinson, Andrew, (1995). ''The Story of Writing: Alphabets, Hieroglyphs & Pictograms'', New York: Thames & Hudson Ltd. page 162.</ref> Tablets found there bear over one thousand cuneiform signs, but these signs are not Babylonian and there are only thirty distinct characters. About twelve of the tablets have the signs set out in alphabetic order. There are two orders found, one of which is nearly identical to the order used for [[Hebrew alphabet|Hebrew]], [[Greek alphabet|Greek]] and [[Latin alphabet|Latin]], and a second order very similar to that used for [[Ge'ez alphabet|Ethiopian]].<ref>Millard, A. R. \"The Infancy of the Alphabet\", ''World Archaeology'' 17, No. 3, Early Writing Systems (Feb., 1986): 390–398. page 395. {{doi|10.1080/00438243.1986.9979978}}. {{JSTOR|124703}}.</ref>\n\nIt is not known how many letters the [[Proto-Sinaitic alphabet]] had nor what their alphabetic order was. Among its descendants, the [[Ugaritic alphabet]] had 27 consonants, the [[South Arabian alphabet]]s had 29, and the [[Phoenician alphabet]] 22. These scripts were arranged in two orders, an ''ABGDE'' order in Phoenician and an ''HMĦLQ'' order in the south; Ugaritic preserved both orders. Both sequences proved remarkably stable among the descendants of these scripts.\n\nThe letter names proved stable among the many descendants of Phoenician, including [[Samaritan alphabet|Samaritan]], [[Aramaic alphabet|Aramaic]], [[Syriac alphabet|Syriac]], [[Arabic alphabet|Arabic]], [[Hebrew alphabet|Hebrew]], and [[Greek alphabet]]. However, they were largely abandoned in [[Tifinagh]], [[Latin alphabet|Latin]] and [[Cyrillic script|Cyrillic]]. The letter sequence continued more or less intact into Latin, [[Armenian alphabet|Armenian]], [[Gothic alphabet|Gothic]], and [[Cyrillic script|Cyrillic]], but was abandoned in [[Brāhmī script|Brahmi]], [[Runic alphabet|Runic]], and Arabic, although a traditional ''[[abjadi order]]'' remains or was re-introduced as an alternative in the latter.\n\nThe table is a schematic of the Phoenician alphabet and its descendants.\n\n{| class=\"wikitable\" style=\"border-collapse:collapse; text-align:center;\"\n|-\n! nr.\n! Reconstruction\n! [[International Phonetic Alphabet|IPA]]\n! value\n! [[Ugaritic alphabet|Ugaritic]]\n! [[Phoenician alphabet|Phoenician]]\n! [[Hebrew alphabet|Hebrew]]\n! [[Arabic alphabet|Arabic]]\n! [[Greek alphabet|Greek]]\n! [[Latin alphabet|Latin]]\n! [[Cyrillic script|Cyrillic]]\n! [[Runic alphabet|Runic]]\n|-\n| 1\n| {{transl|sem|[[ʼalp|ʾalpu]]}} \"ox\"\n| {{IPA|/ʔ/}}\n| 1\n| 𐎀\n| [[File:phoenician aleph.svg|15px|Aleph]] {{transl|sem|ʾālep}}\n| {{script|Hebr|א}} {{transl|sem|ʾālef}}\n| {{script|Arab|[[aleph|ﺍ]]}} {{transl|sem|ʾalif}}\n| [[Α]] alpha\n| [[A]]\n| [[А]] azŭ\n| [[Ansuz rune|ᚨ]] *ansuz\n|-\n| 2\n| {{transl|sem|[[beth (letter)|baytu]]}} \"house\"\n| {{IPA|/b/}}\n| 2\n| 𐎁\n| [[File:phoenician beth.svg|15px|Beth]] {{transl|sem|bēt}}\n| {{script|Hebr|ב}}  {{transl|sem|bēṯ}}\n| {{script|Arab|ﺏ}} {{transl|sem|bāʾ}}\n| [[Beta (letter)|Β]] bēta\n| [[B]]\n| [[В]] vĕdĕ, [[Б]] buky\n| [[ᛒ]] *berkanan\n|-\n|3\n| {{transl|sem|[[gaml]]u}}  \"throwstick\"\n| {{IPA|/ɡ/}}\n| 3\n| 𐎂\n| [[File:phoenician gimel.svg|15px|Gimel]] {{transl|sem|gīmel}}\n| {{script|Hebr|ג}} {{transl|sem|gīmel}}\n| {{script|Arab|ﺝ}} {{transl|sem|jīm}}\n| [[Γ]] gamma\n| [[C]], [[G]]\n| [[Г]] glagoli\n| [[ᚲ]] *kaunan\n|-\n| 4\n| {{transl|sem|[[dalet|daltu]]}} \"door\" / {{transl|sem|diggu}} \"fish\"\n| {{IPA|/d/}}, {{IPA|/ð/}}\n| 4\n| 𐎄\n| [[File:phoenician daleth.svg|15px|Daleth]] {{transl|sem|dālet}}\n| {{script|Hebr|ד}} {{transl|sem|dāleṯ}}\n| {{script|Arab|ﺩ}} {{transl|sem|dāl}}, {{script|Arab|ذ}} {{transl|sem|ḏāl}}\n| [[Δ]] delta\n| [[D]]\n| [[Д]] dobro\n|\n|-\n| 5\n| {{transl|sem|[[he (letter)|haw]]}} \"window\" / {{transl|sem|hallu}} \"[[hallel|jubilation]]\"\n| {{IPA|/h/}}\n| 5\n| 𐎅\n| [[File:phoenician he.svg|15px|He]] {{transl|sem|hē}}\n| {{script|Hebr|ה}} {{transl|sem|hē}}\n| {{script|Arab|ﻫ}} {{transl|sem|hāʾ}}\n| [[Ε]] epsilon\n| [[E]]\n| [[Е]] ye, [[Є]] estĭ\n|\n|-\n| 6\n| {{transl|sem|[[wāw]]u}} \"hook\"\n| {{IPA|/β/}} or {{IPA|/w/}}\n| 6\n| 𐎆\n| [[File:phoenician waw.svg|15px|Waw]] {{transl|sem|wāw}}\n| {{script|Hebr|ו}} {{transl|sem|vāv}}\n| {{script|Arab|و}} {{transl|sem|wāw}}\n| {{lang|grc|[[Ϝ]]}} digamma, [[Υ]] upsilon\n| [[F]], [[U]], [[V]], [[W]], [[Y]]\n| [[Ѹ]] / [[Ꙋ]] ukŭ → [[У]]\n| [[ᚢ]] *ûruz / *ûran\n|-\n| 7\n| {{transl|sem|[[zayin|zaynu]]}} \"weapon\" /  {{transl|sem|ziqqu}} \"manacle\"\n| {{IPA|/z/}}\n| 7\n| 𐎇\n|[[File:phoenician zayin.svg|15px|Zayin]] {{transl|sem|zayin}}\n| {{script|Hebr|ז}} {{transl|sem|zayin}}\n| {{script|Arab|ز}} {{transl|sem|zayn or zāy}}\n| [[Ζ]] zēta\n| [[Z]]\n| [[Ꙁ]] / [[З]] zemlya\n|\n|-\n| 8\n| {{transl|sem|[[ḥet|ḥaytu]]}} \"thread\" / \"fence\"?\n| {{IPA|/ħ/}}, {{IPA|/x/}}\n| 8\n| 𐎈\n| [[File:phoenician heth.svg|15px|Heth]] {{transl|sem|ḥēt}}\n| {{script|Hebr|ח}} {{transl|sem|ḥēṯ}}\n| {{script|Arab|ح}} {{transl|sem|ḥāʾ}}, {{script|Arab|خ}} {{transl|sem|ḫāʾ}}\n| [[Η]] ēta\n| [[H]]\n| [[И]] iže\n| [[ᚺ]] *haglaz\n|-\n| 9\n| {{transl|sem|[[ṭēt|ṭaytu]]}} \"wheel\"\n| {{IPA|/tˤ/}}, {{IPA|/θˤ/}}\n| 9\n| 𐎉\n| [[File:phoenician teth.svg|15px|Teth]] {{transl|sem|ṭēt}}\n| {{script|Hebr|ט}} {{transl|sem|ṭēṯ}}\n| {{script|Arab|ط}} {{transl|sem|ṭāʾ}}, {{script|Arab|ظ}} {{transl|sem|ẓāʾ}}\n| [[Θ]] thēta\n|\n| [[Ѳ]] fita\n|\n|-\n| 10\n| {{transl|sem|[[yad]]u}} \"arm\"\n| {{IPA|/j/}}\n| 10\n| 𐎊\n|[[File:phoenician yodh.svg|15px|Yodh]] {{transl|sem|yōd}}\n| {{script|Hebr|י}} {{transl|sem|yōḏ}}\n| {{script|Arab|ي}} {{transl|sem|yāʾ}}\n| [[Ι]] iota\n| [[I]], [[J]]\n| [[І]] ižei\n| [[ᛁ]] *isaz\n|-\n| 11\n| {{transl|sem|[[kaph|kapu]]}} \"hand\"\n| {{IPA|/k/}}\n| 20\n| 𐎋\n| [[File:phoenician kaph.svg|15px|Kaph]] {{transl|sem|kap}}\n| {{script|Hebr|כ ך}} {{transl|sem|kāf}}\n| {{script|Arab|ك}} {{transl|sem|kāf}}\n| [[Κ]] kappa\n| [[K]]\n| [[К]] kako\n|\n|-\n| 12\n| {{transl|sem|[[Lamedh|lamdu]]}} \"goad\"\n| {{IPA|/l/}}\n| 30\n| 𐎍\n| [[File:phoenician lamedh.svg|15px|Lamedh]] {{transl|sem|lāmed}}\n| {{script|Hebr|ל}} {{transl|sem|lāmeḏ}}\n| {{script|Arab|ل}} {{transl|sem|lām}}\n| [[Λ]] lambda\n| [[L]]\n| [[Л]] lyudiye\n| [[ᛚ]] *laguz / *laukaz\n|-\n| 13\n| {{transl|sem|[[mem|mayim]]}} \"waters\"\n| {{IPA|/m/}}\n| 40\n| 𐎎\n|[[File:phoenician mem.svg|15px|Mem]] {{transl|sem|mēm}}\n| {{script|Hebr|מ ם}} {{transl|sem|mēm}}\n| {{script|Arab|م}} {{transl|sem|mīm}}\n| [[Μ]] mu\n| [[M]]\n| [[М]] myslite\n|\n|-\n| 14\n| {{transl|sem|[[naḥš]]u}} \"snake\" / {{transl|sem|nunu}} \"fish\"\n| {{IPA|/n/}}\n| 50\n| 𐎐\n| [[File:phoenician nun.svg|15px|Nun]] {{transl|sem|nun}}\n| {{script|Hebr|נ ן}} {{transl|sem|nun}}\n| {{script|Arab|ن}} {{transl|sem|nūn}}\n| [[Ν]] nu\n| [[N]]\n| [[Н]] našĭ\n|\n|-\n| 15\n| {{transl|sem|[[samek|samku]]}} \"support\" / \"fish\" ?\n| {{IPA|/s/}}\n| 60\n|𐎒\n| [[File:phoenician samekh.svg|15px|Samek]] {{transl|sem|sāmek}}\n| {{script|Hebr|ס}} {{transl|sem|sāmeḵ}}\n|\n| [[Ξ]] ksi, ([[Χ]] ksi)\n| ([[X]])\n| [[Ѯ]] ksi, ([[Х]] xĕrŭ)\n|\n|-\n| 16\n| {{transl|sem|[[ʻen|ʿaynu]]}} \"eye\"\n| {{IPA|/ʕ/}}, {{IPA|/ɣ/}}\n| 70\n|𐎓\n| [[File:phoenician ayin.svg|15px|Ayin]] {{transl|sem|ʿayin}}\n| {{script|Hebr|ע}} {{transl|sem|ʿayin}}\n| {{script|Arab|ع}} {{transl|sem|ʿayn}}, {{script|Arab|غ}} {{transl|sem|ġayn}}\n| [[Ο]] omikron\n| [[O]]\n| [[О]] onŭ\n|\n|-\n| 17\n| {{transl|sem|[[pe (letter)|pu]]}} \"mouth\" /  {{transl|sem|piʾtu}} \"corner\"\n| {{IPA|/p/}}\n| 80\n| 𐎔\n|[[File:phoenician pe.svg|15px|Pe]] {{transl|sem|pē}}\n| {{script|Hebr|פ ף}} {{transl|sem|pē}}\n| {{script|Arab|ف}} {{transl|sem|fāʾ}}\n| [[Π]] pi\n| [[P]]\n| [[П]] pokoi\n|\n|-\n| 18\n| {{transl|sem|[[ṣad]]u}}  \"plant\"\n| {{IPA|/sˤ/}}, {{IPA|/ɬˤ/}}\n| 90\n|𐎕\n|[[File:phoenician sade.svg|15px|Sade]] ṣādē\n| {{script|Hebr|צ ץ}} {{transl|sem|ṣāḏi}}\n| {{script|Arab|ص}} {{transl|sem|ṣād}}, {{script|Arab|ض}} {{transl|sem|ḍād}}\n| {{lang|grc|[[Ϻ]] san, ([[Ϡ]] sampi)}}\n|\n| [[Ц]] tsi, [[Ч]] črvĭ\n|\n|-\n| 19\n| {{transl|sem|[[qup]]u}} \"Copper\"?\n| {{IPA|/kˤ/}} or {{IPA|/q/}}\n| 100\n|𐎖\n|[[File:phoenician qoph.svg|15px|Qoph]] {{transl|sem|qōp}}\n| {{script|Hebr|ק}} {{transl|sem|qōf}}\n| {{script|Arab|ق}} {{transl|sem|qāf}}\n| {{lang|grc|[[Ϙ]]}} koppa\n| [[Q]]\n| [[Ҁ]] koppa\n|\n|-\n| 20\n| {{transl|sem|[[raʼs|raʾsu]]}} \"head\"\n| {{IPA|/r/}} or {{IPA|/ɾ/}}\n| 200\n|𐎗\n| [[File:phoenician res.svg|15px|Res]] {{transl|sem|rēš}}\n| {{script|Hebr|ר}} {{transl|sem|rēš}}\n| {{script|Arab|ر}} {{transl|sem|rāʾ}}\n| [[Ρ]] rho\n| [[R]]\n| [[Р]] rĭtsi\n| [[ᚱ]] *raidô\n|-\n| 21\n| {{transl|sem|[[šin]]nu}} \"tooth\" / {{transl|sem|šimš}} \"[[shamash|sun]]\"\n| {{IPA|/ʃ/}}, {{IPA|/ɬ/}}\n| 300\n|𐎌\n| [[File:phoenician sin.svg|15px|Sin]] {{transl|sem|šin}}\n| {{script|Hebr|ש}} {{transl|sem|šin}}/{{transl|sem|śin}}\n| {{script|Arab|س}} {{transl|sem|sīn}}, {{script|Arab|ش}} {{transl|sem|šīn}}\n| [[Sigma|Σ]] sigma, [[ϛ]] stigma\n| [[S]]\n| [[С]] slovo, [[Ш]] ša, [[Щ]] šta, [[Ꙃ]] / [[Ѕ]] dzĕlo\n| [[ᛊ]] *sowilô\n|-\n| 22\n| {{transl|sem|[[taw (letter)|tawu]]}}  \"mark\"\n| {{IPA|/t/}}, {{IPA|/θ/}}\n| 400\n| 𐎚\n| [[File:phoenician taw.svg|15px|Taw]] {{transl|sem|tāw}}\n| {{script|Hebr|ת}} {{transl|sem|tāv}}\n| {{script|Arab|ت}} {{transl|sem|tāʾ}}, {{script|Arab|ث}} {{transl|sem|ṯāʾ}}\n| [[Τ]] tau\n| [[T]]\n| [[Т]] tvrdo\n| [[ᛏ]] *tîwaz\n|-\n|}\n\nThese 22 consonants account for the phonology of [[Northwest Semitic]]. Of the 29 consonant phonemes commonly reconstructed for [[Proto-Semitic]], seven are missing: the interdental fricatives {{transl|sem|ḏ, ṯ, ṱ}}, the voiceless lateral fricatives {{transl|sem|ś, ṣ́}}, the voiced uvular fricative {{transl|sem|ġ}}, and the distinction between uvular and pharyngeal voiceless fricatives {{transl|sem|ḫ, ḥ}}, in Canaanite merged in {{transl|sem|[[ḥet]]}}.  The six variant letters added in the [[Arabic alphabet]] include these (except for {{transl|sem|ś}}, which survives as a separate phoneme in [[Ge'ez alphabet|Ge'ez]] [[Śawt|ሠ]]):\n{{transl|sem|ḏ}} → [[ḏāl]];\n{{transl|sem|ṯ}} → [[ṯāʾ]];\n{{transl|sem|ṱ}} → [[ḍād]];\n{{transl|sem|ġ}} → [[ġayn]];\n{{transl|sem|ṣ́}} → [[ẓāʾ]];\n{{transl|sem|ḫ}} → [[ḫāʾ]]\n\n== Graphically independent alphabets ==\nOne modern national alphabet that has not been graphically traced back to the Canaanite alphabet is the [[Thaana|Maldivian]] script, which is unique in that, although it is clearly modeled after [[Arabic]] and perhaps other existing alphabets, it derives its letter forms from numerals. Another is the Korean [[Hangul]], which was created independently in 1443. The [[Osmanya alphabet]] was devised for [[Somali language|Somali]] in the 1920s by [[Osman Yusuf Kenadid]], and the forms of its consonants appear to be complete innovations.\n\nAmong alphabets that are not used as national scripts today, a few are clearly independent in their letter forms. The [[Zhuyin]] phonetic alphabet and Japanese [[Kana]] both derive from [[Chinese character]]s. The [[Santali alphabet]] of eastern India appears to be based on traditional symbols such as \"danger\" and \"meeting place\", as well as pictographs invented by its creator. (The names of the Santali letters are related to the sound they represent through the acrophonic principle, as in the original alphabet, but it is the ''final'' consonant or vowel of the name that the letter represents:  ''le'' \"swelling\" represents ''e'', while ''en'' \"thresh grain\" represents ''n''.)\n\nIn early medieval Ireland, [[Ogham]] consisted of tally marks, and the monumental inscriptions of the [[Old Persian]] Empire were written in an essentially alphabetic cuneiform script whose letter forms seem to have been created for the occasion.\n\n== Alphabets in other media ==\nChanges to a new writing medium sometimes caused a break in graphical form, or make the relationship difficult to trace. It is not immediately obvious that the cuneiform [[Ugaritic alphabet]] derives from a prototypical Semitic abjad, for example, although this appears to be the case. And while [[manual alphabet]]s are a direct continuation of the local written alphabet (both the [[Two-handed manual alphabet|British two-handed]] and the [[French Sign Language|French]]/[[American Sign Language alphabet|American one-handed]] alphabets retain the forms of the Latin alphabet, as the [[Indian Sign Language|Indian manual alphabet]] does [[Devanagari]], and the [[Korean manual alphabet|Korean]] does Hangul), [[Braille]], [[Flag semaphore|semaphore]], [[International maritime signal flags|maritime signal flags]], and the [[Morse code]]s are essentially arbitrary geometric forms. The shapes of the English Braille and semaphore letters, for example, are derived from the [[alphabetic order]] of the Latin alphabet, but not from the graphic forms of the letters themselves. Most modern forms of [[shorthand]] are also unrelated to the alphabet, generally transcribing sounds instead of letters.\n\n== See also ==\n* [[History of writing]]\n* [[History of the Arabic alphabet]]\n* [[History of the Latin alphabet]]\n* [[History of the Hebrew alphabet]]\n* [[History of the Greek alphabet]]\n* [[Runic alphabet]]\n* [[List of inventors of writing systems]]\n* [[List of languages by first written accounts]]\n\n== References ==\n{{reflist|2}}\n\n== Further reading ==\n* Brian E. Colless, \"The Origin of the Alphabet\", ''Antiguo Oriente'' 12 (2014) 71–104.\n* Peter T. Daniels, William Bright (eds.), 1996.  ''The World's Writing Systems'', {{ISBN|0-19-507993-0}}.\n* [[David Diringer]], ''History of the Alphabet'', 1977, {{ISBN|0-905418-12-3}}.\n* Stephen R. Fischer, ''A History of Writing'', 2005 Reaktion Books CN 136481\n* {{cite book\n|last = Haarmann\n|first = Harald\n|title = Geschichte der Schrift\n|trans-title=History of Writing\n|language=German\n|publisher = C. H. Beck\n|location = München\n|edition = 2nd\n|year = 2004\n|isbn = 3-406-47998-7\n|ref = harv\n}}\n* Joel M. Hoffman, ''In the Beginning: A Short History of the Hebrew Language'', 2004, {{ISBN|0-8147-3654-8}}.\n* Robert K. Logan, ''The Alphabet Effect: The Impact of the Phonetic Alphabet on the Development of Western Civilization'', New York: William Morrow and Company, Inc., 1986.\n* {{cite journal\n|last = Millard\n|first = A. R.\n|year = 1986\n|title = The Infancy of the Alphabet\n|journal = World Archaeology\n|volume = 17\n|issue = 3\n|pages = 390–398\n|doi=10.1080/00438243.1986.9979978\n|jstor=124703 \n}}\n* Joseph Naveh, ''Early History of the Alphabet: an Introduction to West Semitic Epigraphy and Palaeography'' (Magnes Press&nbsp;– Hebrew University, Jerusalem, 1982)\n* Barry B. Powell, ''Homer and Origin of the Greek Alphabet,'' Cambridge: Cambridge University Press, 1991.\n* B.L. Ullman, \"The Origin and Development of the Alphabet,\" ''American Journal of Archaeology'' 31, No. 3 (Jul., 1927): 311–328.\n\n== External links ==\n* [http://www.wam.umd.edu/~rfradkin/alphapage.html  Animated examples of how the English alphabet evolved] by [[Robert Fradkin]], University of Maryland\n* [http://www.bbc.co.uk/dna/h2g2/A216073 The Greek alphabet] on [[h2g2]]\n* [http://www.bbc.co.uk/dna/h2g2/A2451890 The Development of the Western Alphabet] on [[h2g2]]\n* [http://www.greek-language.com/alphabet/ Site by Micheal W. Palmer about the Greek alphabet]\n* [http://www.bbc.co.uk/radio4/history/inourtime/inourtime_20031218.shtml \"The Alphabet&nbsp;– its creation and development\"] on [[BBC Radio 4]]'s [[In Our Time (BBC Radio 4)|''In Our Time'']] featuring Eleanor Robson, Alan Millard, Rosalind Thomas\n* [http://www.publishersrow.com/JDL/?bookid=351 Book Jacket] Early History of the Alphabet-Free cover-to-cover limited-time access through Judaic Digital Library\n{{writing systems}}\n{{Northwest Semitic abjad}}\n\n{{DEFAULTSORT:History Of The Alphabet}}\n[[Category:Alphabets|*]]\n[[Category:History of writing|Alphabetic]]"
    },
    {
      "title": "Acrophony",
      "url": "https://en.wikipedia.org/wiki/Acrophony",
      "text": "{{improverefs|date=January 2016}}\n'''Acrophony''' ({{IPAc-en|ə|ˈ|k|r|ɒ|f|ə|n|i|}}; Greek: ἄκρος ''akros'' uppermost + φωνή ''phone'' sound) is the naming of [[grapheme|letter]]s of an [[alphabet]]ic writing system so that a letter's name begins with the letter itself. For example, Greek letter names are acrophonic: the names of the letters α, β, γ, δ, are spelled with the respective letters: {{lang|el|αλφα}} (''alpha''), {{lang|el|βήτα}} (''beta''), {{lang|el|γάμμα}} (''gamma''), {{lang|el|δέλτα}} (''delta'').\n\nThe paradigm for acrophonic alphabets is the [[Proto-Sinaitic script]] and the succeeding [[Phoenician alphabet]], in which the letter A, representing the sound {{IPAblink|ʔ}}, is thought to have derived from an [[Egyptian hieroglyph]] representing an [[ox]], and is called \"ox\", ''ʾalp'', which starts with the [[glottal stop]] sound the letter represents. The [[Latin alphabet]] is descended from the Phoenician, and the stylized head of an ox can still be seen if the letter A is turned upside-down: ∀. The second letter of the Phoenician alphabet is ''bet'' (which means \"house\" and looks a bit like a shelter) representing the sound {{IPA|[b]}}, and from ''ālep-bēt'' we have the word \"alphabet\"{{snd}}another case where the beginning of a thing gives the name to the whole, which was in fact common practice in the ancient Near East.{{Citation needed|date=October 2017}}\n\nThe [[Glagolitic]] and [[early Cyrillic alphabet]]s, although not consisting of ideograms, also have letters named acrophonically.  The letters representing /a, b, v, g, d, e/ are named ''Az'', ''Buky'', ''Vedi'', ''Glagol'', ''Dobro'', ''Est''.  Naming the letters in order, one recites a poem, a [[mnemonic]] which helps students and scholars learn the alphabet: ''Az buky vedi, glagol’ dobro est’'' means \"I know letters, [the] word is good\" in [[Old Church Slavonic]].\n\nIn [[Irish orthography|Irish]] and [[Ogham]], letters were formerly named after [[tree]]s, for example A was ''ailm'' ([[white fir]]), B was ''beith'' ([[birch]]) and C was ''coll'' ([[hazel]]). The [[rune]] alphabets used by the Germanic peoples were also named acrophonically; for example, the first three letters, which represented the sounds /f, u, þ/, were named ''fé, ur, þurs'' in Norse (wealth, slag/rain, giant) and ''feoh, ur, þorn'' in Old English (wealth, ox, thorn). Both sets of names probably stemmed from Proto-Germanic ''*[[fehu]], *[[ur (rune)|uruz]], *[[thurisaz]]''.\n\nThe [[Thai alphabet]] is learned acrophonically, each letter being represented pictorially in school-books (chicken, egg, ox, snake, bell, etc.).\n\n[[Rudyard Kipling]] gives a fictional description of the process in one of his ''[[Just So Stories]]'', \"How the Alphabet was Made.\"<ref>Just So Stories, Rudyard Kipling</ref>\n\nModern [[radiotelephony]] and aviation uses [[spelling alphabet]]s (the best-known of which is the [[NATO Phonetic Alphabet]], which begins with ''Alfa'', ''Bravo'', ''Charlie'', ''Delta''...) in which the letters of the English alphabet are arbitrarily assigned words and names in an acrophonic manner to avoid misunderstanding.\n\nMost notes of the [[solfege]] scale{{snd}}namely ''re'', ''mi'', ''fa'', ''sol'', and ''la''{{snd}}derive their names from the first [[syllable]] of the lines of ''[[Ut queant laxis]]'', a [[Latin]] hymn.\n\n==References==\n\n{{Reflist}}\n\n[[Category:Onomastics]]\n[[Category:Alphabets]]"
    },
    {
      "title": "Alphabet of the Magi",
      "url": "https://en.wikipedia.org/wiki/Alphabet_of_the_Magi",
      "text": "{{notability|date=June 2017}}\n'''Alphabet of the Magi''' is the modern name of a variant of the [[Hebrew alphabet]] used for inscriptions in [[talisman]]s in 17th-century occultism.\n\nIt is based on a variant of the [[Semitic abjad|Semitic alphabet]] given by  [[Theseus Ambrosius]] (1469–1540) in his ''Introductio in chaldaicam linguam'' (1539, pp. 202f.)\nAmbrosius here simply gives variant glyphs of the [[Hebrew alphabet]], labelled ''Aleph, Beth, Gimel, Daleth, He, Vau, Zain, Hhet, Teth, Iod, Caph, Lamed, Mem, Nun, Samech, Ain, Phe, Zadai, Coph, Res, Sin, Thau''. The alphabet is different from the other variants of the Semitic abjad given by Ambrosius in that he mentions that these letters are said to have been invented by [[Gamaliel]] and transmitted in the a book called ''Liber ignis'' associated{{huh|date=June 2017}} with the angel ''Raphiel''.\n\n[[Claude Duret]] (1570?–1611) included it in his ''Thresor'' (1613, [https://books.google.ch/books?id=YEScMxcpM-8C&pg=PA125&lpg=PA125  p. 117]) under the name \"the characters of the angel [[Raphael]]\",  citing Ambrosius.  \n\n[[Edmund Fry]] included it in his ''Pantographia'' (pp. 28–29), stating:\n<blockquote>\"Theseus Ambrosius asserts that this character was brought from Heaven by the Angel Raphael by who it was communicated to Adam who used it in composing Psalms after his expulsion from the terrestrial paradise. Some authors pretend that Moses and the prophets used this letter and that they were forbidden to divulge it to mortal man.\"</blockquote>\n\nThat alphabet is described in the  [[Paracelsianism|pseudo-Paracelsian]] ''[[Archidoxis magica]]'', translated into English by R. Turner (1656).{{page needed|date=June 2017}}\n\nS.L. MacGregor Mathers included it in his 1888 edition of the ''[[Key of Solomon]]'' (plate XV) under the name \"Alphabet of the Magi.\"\n\n==See also==\n*[[Key of Solomon]]\n\n==References==\n{{reflist}}\n{{refimprove|date=June 2017}}\n* [https://books.google.com/books?id=PQddAAAAcAAJ&printsec=frontcover&source=gbs_ge_summary_r&cad=0#v=onepage&q&f=false Theseus Ambrosius, Introductio in chaldaicam linguam, 1539]\n* [http://gallica.bnf.fr/ark:/12148/bpt6k4386k/f155.item.r=.zoom Claude Duret, Thresor de l'histoire des langues de cest univers, 1613]\n* [https://books.google.com/books?id=tWIoAAAAYAAJ&printsec=frontcover&source=gbs_ge_summary_r&cad=0#v=onepage&q=chaldean%202&f=false Edmund Fry, Pantographia, 1799]\n*R. Turner, [http://hdl.loc.gov/loc.rbc/general.31040.1 Paracelsus, Of the Supreme Mysteries of Nature] (1656)\n\n==External links==\n* [http://www.omniglot.com/writing/magi.htm Omniglot.com]\n\n[[Category:Alphabets]]\n[[Category:Magic (paranormal)]]\n[[Category:Amulets]]\n[[Category:Hermeticism]]"
    },
    {
      "title": "Alphabets of Asia Minor",
      "url": "https://en.wikipedia.org/wiki/Alphabets_of_Asia_Minor",
      "text": "{{alphabet}}\nVarious [[alphabetic writing systems]] were in use in [[Iron Age]] [[Anatolia]] to record [[Anatolian languages]] and [[Phrygian language|Phrygian]]. Several of these languages had previously been written with [[logogram|logographic]] and [[syllabary|syllabic]] scripts.\n\nThe alphabets of Asia Minor proper share characteristics that distinguish them from the earliest attested forms of the Greek alphabet. Many letters in these alphabets resemble Greek letters but have unrelated readings, most extensively in the case of Carian. The [[Phrygian alphabet|Phrygian]] and [[Lemnos stele|Lemnian]] alphabets by contrast were early adaptations of regional variants of the [[Greek alphabet]]; the earliest Phrygian inscriptions are contemporary with early Greek inscriptions, but contain Greek innovations such as the letters [[Φ]] and [[Ψ]] which did not exist in the earliest forms of the Greek alphabet.\n\nThe Anatolian alphabets fell out of use around the 4th century BCE with the onset of the [[Hellenistic period]].\n\n==Alphabets==\n*The '''[[Lydian script]]''', an alphabet used to record the [[Lydian language]] from ca. the 5th to 4th centuries BCE; a related script is the \"Para-Lydian\" alphabet known from a single inscription in [[Sardis]]. \n* The '''Para-Lydian script''', known from a single inscription found in [[Sardis Synagogue]],<ref>{{cite web|url=http://www.harvardartmuseums.org/art/288671|title=From the Harvard Art Museums' collections Cast of an Inscribed Marble Stele from the Sardis Synagogue|website=Harvardartmuseums.org|accessdate=13 September 2018}}</ref> language unknown, undeciphered but closely resembles the Lydian script, hence the name.\n*The '''[[Carian script]]''', recording the [[Carian language]], known from inscriptions in [[Caria]], [[Egypt]] and [[Athens]]. Only partially understood, there were 45 letters. Many of these resemble the Greek alphabet in form, but have different values.\n*The '''[[Lycian script]]''', an alphabet recording the [[Lycian language]] from the 5th to 4th centuries BCE.\n*The '''[[Sidetic script]]''', an alphabet of 25 letters, only a few of which are clearly derived from Greek, known from coin legends in what might be a [[Sidetic language]]. Essentially undeciphered.\n*The '''[[Pisidian script]]''', an alphabet used to write the [[Pisidian language]]. It is attested in about 30 inscriptions around the region of [[Pisidia]].\n*The '''[[Phrygian script]]''', an alphabet of 21 letters used for the [[Phrygian language]] (22 for the [[Mysian language]]) which is very similar to [[Archaic Greek alphabets|early Greek epichoric alphabets]], except for the presence of a special character for '''j'''.\n\n==See also==\n{{Portal|Ancient Near East}}\n*[[History of the alphabet]]\n*[[Greek alphabet]]\n*[[Old Italic alphabets]]\n*[[Luwian hieroglyphs]]\n\n==References==\n{{Reflist}}\n\n==Further reading==\n*{{cite book |title=The Alphabet: A Key to the History of Mankind |last=Diringer |first=David |authorlink= David Diringer|year=1948 |publisher=Hutchinson |location=Cambridge |isbn= |pages= |url= }}\n*{{cite book |title=Geschichte der Schrift. Unter besonderer Berücksichtigung ihrer geistigen Entwicklung |last=Friedrich |first=Johannes |authorlink= |year=1966 |publisher=Winter |location=Heidelberg |isbn= |pages= |url= }}\n\n==External links==\n*[https://web.archive.org/web/20060712164315/http://indoeuro.bizland.com/project/script/asiam.html Alphabets of Asia Minor] (indoeuro.bizland.com)\n**[https://web.archive.org/web/20061102135723/http://indoeuro.bizland.com/tree/anat/carian.html Carian inscription]\n*[http://titus.uni-frankfurt.de/didact/idg/anat/lydbeisp.htm Lydian-Aramaean bilingue] (titus.uni-frankfurt.de)\n*[http://holylandphotos.org/browse.asp?s=1,3,7,183,190&img=TWTQXN09 Lycian inscribed pillar] (holylandphotos.org)\n\n{{DEFAULTSORT:Alphabets Of Asia Minor}}\n[[Category:Ancient Greek language]]\n[[Category:Anatolian languages]]\n[[Category:Alphabets|Asia Minor]]\n[[Category:Iron Age Anatolia]]"
    },
    {
      "title": "ASL-phabet",
      "url": "https://en.wikipedia.org/wiki/ASL-phabet",
      "text": "{{Infobox Writing system\n|type = alphabet\n|name = ASL-phabet\n|languages = [[ASL]]\n|time = \n|fami1 = [[Stokoe notation]]\n|sample = \n|imagesize =\n|note = none\n|}}\n'''ASL-phabet''', or the '''ASL Alphabet''', is a [[writing system]] developed by [[Samuel James Supalla|Samuel Supalla]] for [[American Sign Language]] (ASL).  It is based on a system called SignFont,<ref name=\"McIntreNewkirkHutchinsPoizner\">{{Citation\n| last  = McIntire\n| first = Marina\n| last2 = Newkirk\n| first2 = Don\n| last3 = Hutchins\n| first3 = Sandra\n| last4 = Poizner\n| first4 = Howard\n| lastauthoramp = yes\n| title = Hands and Faces: A Preliminary Inventory for Written ASL\n| journal = Sign Language Studies\n| volume  = 56\n| issue   = Fall 1987\n| year    = 1987\n}}</ref><ref name=\"SignFontHandbook\">{{Citation\n| last  = Newkirk\n| first = Don\n| title = Signfont Handbook, October 1987: Architect: Final Version\n| publisher = Salk Institute for Biological Studies and Emerson & Stern Associates\n| place = San Diego\n| year    = 1987\n}}\n</ref> which Supalla modified and streamlined for use in an educational setting with Deaf children.<ref name=\"SupallaBlackburn\">{{Citation\n |last1=Supalla \n |first1=Sam \n |first2=Laura \n |last2=Blackburn \n |lastauthoramp=yes \n |year=2003 \n |title=Learning How to Read and Bypassing Sound \n |journal=Odyssey \n |volume=5:1 \n |issue=Fall 2003 \n |url=http://www.gallaudet.edu/clerc_center/information_and_resources/products_and_publications/odyssey/vol_5_issue_1.html \n |deadurl=yes \n |archiveurl=https://web.archive.org/web/20121111075634/http://www.gallaudet.edu/Clerc_Center/Information_and_Resources/Products_and_Publications/Odyssey/Vol_5_Issue_1.html \n |archivedate=2012-11-11 \n}}</ref>\n\nLike SignFont and [[Stokoe notation]], ASL-phabet is a [[phonemic]] script, but it has been simplified to the point where there is some ambiguity, that is, one symbol can represent more than one phonemic element (handshape, location or movement).  For example, whereas SignFont has 25 letters encoding types of movement, and Stokoe notation has 24, ASL-phabet has just 5.  This can result in [[homograph]]s (more than one sign spelled the same way).\n\nAltogether, ASL-phabet has 22 letters for handshape, 5 for location, and 5 for movement. They are written in that order, left-to-right, with the possibility for several letters of each type, such as two handshape letters for a two-handed sign. Like Stokoe notation (but unlike SignFont), the ASL-phabet does not provide symbols for facial expressions, [[mouthing]], and other aspects of sign language structure, which may make it hard to use for extended text. However, it is sufficient to look up ASL words in an ASL–English dictionary.<ref>http://www.asl-phabet.com/</ref>  Hulst & Channon (2010)<ref name=\"HulstChannon\">\n{{Citation\n| last = van der Hulst\n| first = Harry\n| last2 = Channon\n| first2 = Rachel\n| lastauthoramp = yes\n| year = 2010\n| chapter = Notation systems\n| editor-last = Brentari\n| editor-first = Dianne\n| title = Sign Languages\n| publisher = Cambridge University Press\n| place = Cambridge UK\n| pages = 151–172\n}}</ref> note, \"This system, much more than [[SignWriting]], acknowledges the fact (rightly, we believe) that a written representation of a word does not need to be a recipe to produce it, but only to be sufficiently unique to act as a trigger to activate the relevant words in the reader's mind.\"\n\n[[File:Brief Comparison of ASL Writing Systems.jpg|Brief Comparison of ASL Writing Systems]]\n\n==References==\n<references />\n\n==External links==\n*[http://www.asl-phabet.com/ ASL-phabet homepage], with an ASLphabet-to-ASL video input system and an ASL-to-English dictionary for children\n\n{{sign language navigation}}\n\n[[Category:American Sign Language]]\n[[Category:Sign language notation]]\n[[Category:Alphabets]]"
    },
    {
      "title": "Avestan alphabet",
      "url": "https://en.wikipedia.org/wiki/Avestan_alphabet",
      "text": "{{Infobox writing system\n |name=Avestan\n |type=[[Alphabet]]\n |languages=[[Avestan language]], [[Middle Persian]]\n |time=400&ndash;1000 CE\n |fam1=[[Phoenician alphabet]]\n |fam2=[[Aramaic alphabet]]\n |fam3=[[Pahlavi script]]\n |sample=Bodleian_J2_fol_175_Y_28_1.jpg|Yasna 28.1 (Bodleian MS J2)\n |imagesize=200px\n |unicode=[https://www.unicode.org/charts/PDF/U10B00.pdf U+10B00&ndash;U+10B3F]\n |iso15924=Avst\n}}\n{{Contains special characters\n| special    = uncommon [[Unicode]] characters\n| fix        = Help:Multilingual support#Avestan\n| image      = Replacement character.svg\n| link       = Specials (Unicode block)#Replacement character\n| alt        = <?>\n| compact    = yes\n}}\n\nThe '''Avestan alphabet''' is a writing system developed during Iran's [[Sassanid Empire|Sassanid era]] (226&ndash;651&nbsp;CE) to render the [[Avestan language]].\n\nAs a side effect of its development, the script was also used for [[Pazend]], a method of writing [[Middle Persian]] that was used primarily for the ''Zend'' commentaries on the texts of the [[Avesta]]. In the texts of [[Zoroastrianism|Zoroastrian]] tradition, the alphabet is referred to as ''din dabireh'' or ''din dabiri'', Middle Persian for \"the religion's script\".\n\n== History ==\n{{alphabet}}\n\nThe development of the Avestan alphabet was initiated by the need to represent <!-- \"with extreme precision\" so Kellens --> recited [[Avestan language]] texts correctly. The various text collections that today constitute the canon of [[Avesta|Zoroastrian scripture]] are the result of a collation that occurred in the 4th century, probably during the reign of [[Shapur II]] (309&ndash;379). It is likely that the Avestan alphabet was an ''[[ad hoc]]''<ref name=\"Kellens_1989_36\">{{harvnb|Kellens|1989|p=36}}.</ref> innovation related to this – \"Sassanid archetype\" – collation.\n\nThe enterprise, \"which is indicative of a Mazdean revival and of the establishment of a strict orthodoxy closely connected with the political power, was probably caused by the desire to compete more effectively with Buddhists, Christians, and Manicheans, whose faith was based on a revealed book\".<ref name=\"Kellens_1989_36\" /> In contrast, the Zoroastrian priesthood had for centuries been accustomed to memorizing scripture — following by rote the words of a teacher-priest until they had memorized the words, cadence, inflection and intonation of the prayers. This they passed on to their pupils in turn, so preserving for many generations the correct way to recite scripture. This was necessary because the priesthood considered (and continue to consider) precise and correct enunciation and cadence a prerequisite of effective prayer. Further, the recitation of the liturgy was (and is) accompanied by ritual activity that leaves no room to attend to a written text.\n\nThe ability to correctly render Avestan did, however, have a direct benefit: By the common era the Avestan language words had almost ceased to be understood, which led to the preparation of the ''Zend'' texts (from Avestan ''zainti'' \"understanding\"), that is commentaries on and translations of the canon. The development of the Avestan alphabet allowed these commentaries to interleave quotation of scripture with explanation thereof. The direct effect of these texts was a \"standardized\" interpretation of scripture that survives to the present day. For scholarship these texts are enormously interesting since they occasionally preserve passages that have otherwise been lost.\n\nThe 9th&ndash;12th century texts of Zoroastrian tradition suggest that there was once a much larger collection of ''written'' [[Zoroastrian literature]], but these texts — if they ever existed — have since been lost, and it is hence not known what script was used to render them. The question of the ''existence'' of a pre-Sassanid \"Arsacid archetype\" occupied Avestan scholars for much of the 19th century, and, \"[w]hatever may be the truth about the Arsacid [[Avesta]], the linguistic evidence shows that even if it did exist, it can not have had any practical influence, since no linguistic form in the Vulgate can be explained with certainty as resulting from wrong transcription and the number of doubtful cases is minimal; in fact it is being steadily reduced. Though the existence of an Arsacid archetype is not impossible, it has proved to contribute nothing to Avestan philology.\"<ref name=\"Kellens_1989_36\" />\n\n== Genealogy and script ==\n\nThe [[Pahlavi script]], upon which the Avestan alphabet is based, was in common use for representing various [[Middle Iranian languages]], but was not adequate for representing a religious language that demanded precision since Pahlavi was a simplified [[abjad]] syllabary with at most 22<!-- at the time under consideration --> symbols, most of which were ambiguous (i.e. could represent more than one sound).\n\nIn contrast, Avestan was a full alphabet, with explicit characters for vowels, and allowed for phonetic disambiguation of [[allophone]]s. The alphabet included many characters (''a'', ''i'', ''k'', ''t'', ''p'', ''b'', ''m'', ''n'', ''r'', ''s'', ''z'', ''š'', ''x<sup>v</sup>'') from cursive Pahlavi, while some (''ā'', ''γ'') are characters that only exist in the Psalter Pahlavi variant (in cursive Pahlavi ''γ'' and ''k'' have the same symbol).<ref name=\"Hoffmann_1989_49\">{{harvnb|Hoffmann|1989|p=49}}.</ref> Some of the vowels, such as ''ə'' appear to derive from Greek [[Minuscule cursive|minuscule]]s.<ref name=\"Hoffmann_1989_49\" /> Avestan ''o'' is a special form of Pahlavi <span style=\"font-family:monospace;\">''l''</span> that exists only in Aramaic signs. Some letters (e.g. ''ŋ́'', ''ṇ'', ''ẏ'', ''v''), are free inventions.<ref name=\"Hoffmann_1989_50\">{{harvnb|Hoffmann|1989|p=50}}.</ref>\n\nAvestan script, like Pahlavi script and Aramaic script also, is written from right to left. In Avestan script, letters are not connected, and ligatures are \"rare and clearly of secondary origin\".<ref name=\"Hoffmann_1989_49\" />\n\n== Letters ==\n[[Image:Das Buch der Schrift (Faulmann) 106.jpg|thumb|200px|Avestan chart by Carl Faulmann]]\n[[Image:Encyclopedie volume 2-183.png|thumb|200px|Avestan chart on p138 in l'[[Encyclopédie, ou Dictionnaire raisonné des sciences, des arts et des métiers|Encyclopédie]]]]\n[[Image:Encyclopedie volume 2-184.png|thumb|200px|Avestan chart on p134 in l'Encyclopédie]]\n\nIn total, the Avestan alphabet has 37 consonants and 16 vowels. There are two main transcription schemes for Avestan, the newer style used by [[Karl Hoffmann (German historian)|Karl Hoffmann]] and the older style used by [[Christian Bartholomae]].\n\n{| class=\"wikitable\" style=\"text-align: center;\"\n|+ Avestan alphabet\n|-\n! rowspan=\"2\" | Letter\n! colspan=\"2\" | Transcription<ref>{{cite journal |first=Jost|last=Gippert|authorlink=Jost Gippert|date=2012|title=The Encoding of Avestan – Problems and Solutions|url=http://www.jlcl.org/2012_Heft2/H2012-2.pdf|journal=JLCL: Journal for Language Technology and Computational Linguistics|volume=27|issue=2|access-date=2017-08-25}}</ref>\n! scope=\"col\" rowspan=\"2\" |  [[International Phonetic Alphabet|IPA]]\n! rowspan=\"2\" |  [[Unicode]]\n|-\n! Hoff.\n! Bar.\n|-\n| {{huge|{{script|Avst|𐬀}}}}\n| a\n| a\n| {{IPA|/a/}}\n| style=\"text-align: left;\" | U+10B00: AVESTAN LETTER A\n|-\n| {{huge|{{script|Avst|𐬁}}}}\n| ā\n| ā\n| {{IPA|/aː/}}\n| style=\"text-align: left;\" | U+10B01: AVESTAN LETTER AA\n|-\n| {{huge|{{script|Avst|𐬂}}}}\n| å\n| &mdash;\n| {{IPA|/ɒ/}}\n| style=\"text-align: left;\" | U+10B02: AVESTAN LETTER AO\n|-\n| {{huge|{{script|Avst|𐬃}}}}\n| ā̊\n| å\n| {{IPA|/ɒː/}}\n| style=\"text-align: left;\" | U+10B03: AVESTAN LETTER AAO\n|-\n| {{huge|{{script|Avst|𐬄}}}}\n| ą\n| ą\n| {{IPA|/ã/}}\n| style=\"text-align: left;\" | U+10B04: AVESTAN LETTER AN\n|-\n| {{huge|{{script|Avst|𐬅}}}}\n| ą̇\n| &mdash;\n| {{IPA|/ã/}}\n| style=\"text-align: left;\" | U+10B05: AVESTAN LETTER AAN\n|-\n| {{huge|{{script|Avst|𐬆}}}}\n| ə\n| ə\n| {{IPA|/ə/}}\n| style=\"text-align: left;\" | U+10B06: AVESTAN LETTER AE\n|-\n| {{huge|{{script|Avst|𐬇}}}}\n| ə̄\n| ə̄\n| {{IPA|/əː/}}\n| style=\"text-align: left;\" | U+10B07: AVESTAN LETTER AEE\n|-\n| {{huge|{{script|Avst|𐬈}}}}\n| e\n| e\n| {{IPA|/e/}}\n| style=\"text-align: left;\" | U+10B08: AVESTAN LETTER E\n|-\n| {{huge|{{script|Avst|𐬉}}}}\n| ē\n| ē\n| {{IPA|/eː/}}\n| style=\"text-align: left;\" | U+10B09: AVESTAN LETTER EE\n|-\n| {{huge|{{script|Avst|𐬊}}}}\n| o\n| o\n| {{IPA|/ɔ/}}\n| style=\"text-align: left;\" | U+10B0A: AVESTAN LETTER O\n|-\n| {{huge|{{script|Avst|𐬋}}}}\n| ō\n| ō\n| {{IPA|/oː/}}\n| style=\"text-align: left;\" | U+10B0B: AVESTAN LETTER OO\n|-\n| {{huge|{{script|Avst|𐬌}}}}\n| i\n| i\n| {{IPA|/ɪ/}}\n| style=\"text-align: left;\" | U+10B0C: AVESTAN LETTER I\n|-\n| {{huge|{{script|Avst|𐬍}}}}\n| ī\n| ī\n| {{IPA|/iː/}}\n| style=\"text-align: left;\" | U+10B0D: AVESTAN LETTER II\n|-\n| {{huge|{{script|Avst|𐬎}}}}\n| u\n| u\n| {{IPA|/ʊ/}}\n| style=\"text-align: left;\" | U+10B0E: AVESTAN LETTER U\n|-\n| {{huge|{{script|Avst|𐬏}}}}\n| ū\n| ū\n| {{IPA|/uː/}}\n| style=\"text-align: left;\" | U+10B0F: AVESTAN LETTER UU\n|-\n| {{huge|{{script|Avst|𐬐}}}}\n| k\n| k\n| {{IPA|/k/}}\n| style=\"text-align: left;\" | U+10B10: AVESTAN LETTER KE\n|-\n| {{huge|{{script|Avst|𐬑}}}}\n| x\n| x\n| {{IPA|/x/}}\n| style=\"text-align: left;\" | U+10B11: AVESTAN LETTER XE\n|-\n| {{huge|{{script|Avst|𐬒}}}}\n| x́\n| ḣ\n| {{IPA|/xʲ/}}, {{IPA|/ç/}}\n| style=\"text-align: left;\" | U+10B12: AVESTAN LETTER XYE\n|-\n| {{huge|{{script|Avst|𐬓}}}}\n| xᵛ\n| xᵛ\n| {{IPA|/xʷ/}}\n| style=\"text-align: left;\" | U+10B13: AVESTAN LETTER XVE\n|-\n| {{huge|{{script|Avst|𐬔}}}}\n| g\n| g\n| {{IPA|/ɡ/}}\n| style=\"text-align: left;\" | U+10B14: AVESTAN LETTER GE\n|-\n| {{huge|{{script|Avst|𐬕}}}}\n| ġ\n| &mdash;\n| {{IPA|/ɡʲ/}}, {{IPA|/ɟ/}}\n| style=\"text-align: left;\" | U+10B15: AVESTAN LETTER GGE\n|-\n| {{huge|{{script|Avst|𐬖}}}}\n| γ\n| γ\n| {{IPA|/j/}}\n| style=\"text-align: left;\" | U+10B16: AVESTAN LETTER GHE\n|-\n| {{huge|{{script|Avst|𐬗}}}}\n| c\n| č\n| {{IPA|/t͡ʃ/}}\n| style=\"text-align: left;\" | U+10B17: AVESTAN LETTER CE\n|-\n| {{huge|{{script|Avst|𐬘}}}}\n| j\n| ǰ\n| {{IPA|/d͡ʒ/}}\n| style=\"text-align: left;\" | U+10B18: AVESTAN LETTER JE\n|-\n| {{huge|{{script|Avst|𐬙}}}}\n| t\n| t\n| {{IPA|/t/}}\n| style=\"text-align: left;\" | U+10B19: AVESTAN LETTER TE\n|-\n| {{huge|{{script|Avst|𐬚}}}}\n| ϑ\n| ϑ\n| {{IPA|/θ/}}\n| style=\"text-align: left;\" | U+10B1A: AVESTAN LETTER THE\n|-\n| {{huge|{{script|Avst|𐬛}}}}\n| d\n| d\n| {{IPA|/d/}}\n| style=\"text-align: left;\" | U+10B1B: AVESTAN LETTER DE\n|-\n| {{huge|{{script|Avst|𐬜}}}}\n| δ\n| δ\n| {{IPA|/ð/}}\n| style=\"text-align: left;\" | U+10B1C: AVESTAN LETTER DHE\n|-\n| {{huge|{{script|Avst|𐬝}}}}\n| t̰\n| t̰\n| {{IPA|/t̚/}}<ref name=\"WWS\">{{cite encyclopedia | first=Pods Octor | last=Skjærvø | author-link=Prods Oktor Skjaervo | title=<!-- Section 48: -->Aramaic Scripts for Iranian Languages | encyclopedia=The World's Writing Systems | year=1996 | editor1-last=Daniels | editor1-first=Peter T. | editor1-link=Peter T. Daniels | editor2-last=Bright | editor2-first=William | publisher=Oxford University Press | isbn=978-0195079937 | pages=527-528 | editor2-link=William Bright}}</ref>\n| style=\"text-align: left;\" | U+10B1D: AVESTAN LETTER TTE\n|-\n| {{huge|{{script|Avst|𐬞}}}}\n| p\n| p\n| {{IPA|/p/}}\n| style=\"text-align: left;\" | U+10B1E: AVESTAN LETTER PE\n|-\n| {{huge|{{script|Avst|𐬟}}}}\n| f\n| f\n| {{IPA|/f/}}\n| style=\"text-align: left;\" | U+10B1F: AVESTAN LETTER FE\n|-\n| {{huge|{{script|Avst|𐬠}}}}\n| b\n| b\n| {{IPA|/b/}}\n| style=\"text-align: left;\" | U+10B20: AVESTAN LETTER BE\n|-\n| {{huge|{{script|Avst|𐬡}}}}\n| β\n| w\n| {{IPA|/β/}}\n| style=\"text-align: left;\" | U+10B21: AVESTAN LETTER BHE\n|-\n| {{huge|{{script|Avst|𐬢}}}}\n| ŋ\n| ŋ\n| {{IPA|/ŋ/}}\n| style=\"text-align: left;\" | U+10B22: AVESTAN LETTER NGE\n|-\n| {{huge|{{script|Avst|𐬣}}}}\n| ŋ́\n| ŋ́\n| {{IPA|/ŋʲ/}}\n| style=\"text-align: left;\" | U+10B23: AVESTAN LETTER NGYE\n|-\n| {{huge|{{script|Avst|𐬤}}}}\n| ŋᵛ\n| &mdash;\n| {{IPA|/ŋʷ/}}\n| style=\"text-align: left;\" | U+10B24: AVESTAN LETTER NGVE\n|-\n| {{huge|{{script|Avst|𐬥}}}}\n| n\n| n\n| {{IPA|/n/}}\n| style=\"text-align: left;\" | U+10B25: AVESTAN LETTER NE\n|-\n| {{huge|{{script|Avst|𐬦}}}}\n| ń\n| &mdash;\n| {{IPA|/ɲ/}}\n| style=\"text-align: left;\" | U+10B26: AVESTAN LETTER NYE\n|-\n| {{huge|{{script|Avst|𐬧}}}}\n| ṇ\n| n, m\n| {{IPA|/ŋ/}}<br>{{Verify source|date=August 2017}}\n| style=\"text-align: left;\" | U+10B27: AVESTAN LETTER NNE\n|-\n| {{huge|{{script|Avst|𐬨}}}}\n| m\n| m\n| {{IPA|/m/}}\n| style=\"text-align: left;\" | U+10B28: AVESTAN LETTER ME\n|-\n| {{huge|{{script|Avst|𐬩}}}}\n| m̨\n| &mdash;\n| {{IPA|/m̥/}}, {{IPA|/mʰ/}}<br>{{Verify source|date=August 2017}}\n| style=\"text-align: left;\" | U+10B29: AVESTAN LETTER HME\n|-\n| {{huge|{{script|Avst|𐬪}}}}\n| ẏ\n| rowspan=\"3\" | y\n| {{IPA|/j/}}\n| style=\"text-align: left;\" | U+10B2A: AVESTAN LETTER YYE\n|-\n| {{huge|{{script|Avst|𐬫}}}}\n| y\n| {{IPA|/j/}}\n| style=\"text-align: left;\" | U+10B2B: AVESTAN LETTER YE\n|-\n| {{huge|{{script|Avst|𐬌𐬌}}}}\n| ii\n| {{IPA|/ii̯/}}<ref name=\"WWS\"/>\n| style=\"text-align: left;\" | U+10B0C: AVESTAN LETTER I ''(doubled)''\n|-\n| {{huge|{{script|Avst|𐬬}}}}\n| v\n| rowspan=\"2\" | v\n| {{IPA|/v/}}<br>{{Verify source|date=August 2017}}\n| style=\"text-align: left;\" | U+10B2C: AVESTAN LETTER VE\n|-\n| {{huge|{{script|Avst|𐬎𐬎}}}}\n| uu\n| {{IPA|/uu̯/}}<ref name=\"WWS\"/>\n| style=\"text-align: left;\" | U+10B0E: AVESTAN LETTER U ''(doubled)''\n|-\n| {{huge|{{script|Avst|𐬭}}}}\n| r\n| r\n| {{IPA|/r/}}\n| style=\"text-align: left;\" | U+10B2D: AVESTAN LETTER RE\n|-\n| {{huge|{{script|Avst|𐬯}}}}\n| s\n| s\n| {{IPA|/s/}}\n| style=\"text-align: left;\" | U+10B2F: AVESTAN LETTER SE\n|-\n| {{huge|{{script|Avst|𐬰}}}}\n| z\n| z\n| {{IPA|/z/}}\n| style=\"text-align: left;\" | U+10B30: AVESTAN LETTER ZE\n|-\n| {{huge|{{script|Avst|𐬱}}}}\n| š\n| š\n| {{IPA|/ʃ/}}\n| style=\"text-align: left;\" | U+10B31: AVESTAN LETTER SHE\n|-\n| {{huge|{{script|Avst|𐬲}}}}\n| ž\n| ž\n| {{IPA|/ʒ/}}\n| style=\"text-align: left;\" | U+10B32: AVESTAN LETTER ZHE\n|-\n| {{huge|{{script|Avst|𐬳}}}}\n| š́\n| rowspan=\"2\" | š\n| {{IPA|/ɕ/}}\n| style=\"text-align: left;\" | U+10B33: AVESTAN LETTER SHYE\n|-\n| {{huge|{{script|Avst|𐬴}}}}\n| ṣ̌\n| {{IPA|/ʂ/}}<br>{{Verify source|date=August 2017}}\n| style=\"text-align: left;\" | U+10B34: AVESTAN LETTER SSHE\n|-\n| {{huge|{{script|Avst|𐬵}}}}\n| h\n| h\n| {{IPA|/h/}}\n| style=\"text-align: left;\" | U+10B35: AVESTAN LETTER HE\n|-\n! rowspan=\"2\" | Letter\n! Hoff.\n! Bar.\n! rowspan=\"2\" |  IPA\n! rowspan=\"2\" |  Unicode\n|-\n! colspan=\"2\" | Transcription\n|}\n\nLater, when writing [[Middle Persian]] in the script (i.e. [[Pazend]]), another consonant {{large|{{script|Avst|𐬮}}}} was added to represent the /l/ phoneme that didn't exist in the Avestan language.\n\n==Ligatures==\n[[Image:Avesta-ligatury.svg|thumb|200px|List of Avestan ligatures according to Skjærvø (2003)]]\nFour ligatures are commonly used in Avestan manuscripts:<ref name=\"TUS\"/>\n* {{large|{{script|Avst|𐬱}}}} (š) + {{large|{{script|Avst|𐬀}}}} (a) = {{large|{{script|Avst|𐬱𐬀}}}} (ša)\n* {{large|{{script|Avst|𐬱}}}} (š) + {{large|{{script|Avst|𐬗}}}} (c) = {{large|{{script|Avst|𐬱𐬗}}}} (šc)\n* {{large|{{script|Avst|𐬱}}}} (š) + {{large|{{script|Avst|𐬙}}}} (t) = {{large|{{script|Avst|𐬱𐬙}}}} (št)\n* {{large|{{script|Avst|𐬀}}}} (a) + {{large|{{script|Avst|𐬵}}}} (h) = {{large|{{script|Avst|𐬀𐬵}}}} (ah)\n\nU+200C ZERO WIDTH NON-JOINER can be used to prevent ligatures if desired.\nFor example, compare {{large|{{script|Avst|𐬱𐬀}}}} (U+10B31 10B00) with {{large|{{script|Avst|𐬱‌𐬀}}}} (U+10B31 200C 10B00).\n\nFossey<ref name=\"Fossey_1948_49\">{{harvnb|Fossey|1948|p=49}}.</ref> lists 16 ligatures, but most are formed by the interaction of swash tails.\n\n==Digits==\nDigits and numbers can be seen on the Faulmann chart above.\n\n==Punctuation==\n\nWords and the end of the first part of a compound are separated by a dot (in a variety of vertical positions). Beyond that, punctuation is weak or non-existent in the manuscripts, and in the 1880s [[Karl Friedrich Geldner]] had to devise one for standardized transcription. In his system, which he developed based on what he could find, a triangle of three dots serves as a colon, a semicolon, an end of sentence or end of section; which is determined by the size of the dots and whether there is one dot above and two below, or two above and one below. Two above and one below signify — in ascending order of \"dot\" size — colon, semicolon, end of sentence or end of section.\n\n{| class=\"wikitable\"\n|+Avestan punctuation<ref name=\"TUS\">{{cite web|url=https://www.unicode.org/versions/Unicode12.0.0/ch10.pdf#G29021|title=The Unicode Standard, Chapter 10.7: Avestan|publisher=Unicode Consortium|date=March 2019}}</ref>\n|-\n! Mark\n! Function\n! Unicode\n|-\n| style=\"text-align: center;\" | {{large|⸱}}\n| rowspan=\"3\" | word separator\n| U+2E31: WORD SEPARATOR MIDDLE DOT\n|-\n| style=\"text-align: center;\" | {{large|·}}\n| U+00B7: MIDDLE DOT\n|-\n| style=\"text-align: center;\" | {{large|.}}\n| U+002E: FULL STOP\n|-\n| style=\"text-align: center;\" | {{large|{{script|Avst|𐬹}}}}\n| abbreviation or repetition\n| U+10B39: AVESTAN ABBREVIATION MARK\n|-\n| style=\"text-align: center;\" | {{large|{{script|Avst|𐬺}}}}\n| colon\n| U+10B3A: TINY TWO DOTS OVER ONE DOT PUNCTUATION\n|-\n| style=\"text-align: center;\" | {{large|{{script|Avst|𐬻}}}}\n| semicolon\n| U+10B3B: SMALL TWO DOTS OVER ONE DOT PUNCTUATION\n|-\n| style=\"text-align: center;\" | {{large|{{script|Avst|𐬼}}}}\n| end of sentence\n| U+10B3C: LARGE TWO DOTS OVER ONE DOT PUNCTUATION\n|-\n| style=\"text-align: center;\" | {{large|{{script|Avst|𐬽}}}}\n| alternative mark for end of sentence<br>(found in Avestan texts but not used by Geldner)\n| U+10B3D: LARGE ONE DOT OVER TWO DOTS PUNCTUATION\n|-\n| style=\"text-align: center;\" | {{large|{{script|Avst|𐬾}}}}\n| end of section<br>(may be doubled for extra finality)\n| U+10B3E: LARGE TWO RINGS OVER ONE RING PUNCTUATION\n|-\n| style=\"text-align: center;\" | {{large|{{script|Avst|𐬿}}}}\n| alternative mark for end of section<br>(found in Avestan texts but not used by Geldner)\n| U+10B3F: LARGE ONE RING OVER TWO RINGS PUNCTUATION\n|}\n\n== Unicode ==\n\n{{Main|Avestan (Unicode block)}}\n\nThe Avestan alphabet was added to the [[Unicode]] Standard in October, 2009 with the release of version 5.2.\n\nThe characters are encoded at U+10B00&mdash;10B35 for letters (''ii'' and ''uu'' are not represented as single characters, but as sequences of characters<ref name=\"ucs\">{{harvnb|Everson|Pournader|2007|p=4}}</ref>) and U+10B38&mdash;10B3F for punctuation.\n\n{{Unicode chart Avestan}}\n\n== References ==\n{{reflist}}\n\n== External links ==\nThe above likely display in Noto Sans Avestan font, which has four automated ligatures. There is also,\n*Ernst Tremel's free ''Ahura Mazda'' Unicode font,[https://fontlibrary.org/en/font/ahuramazda-updated/] based on the type used by Geldner 1896, with the addition of ligatures in the PUA.\n\n== Bibliography ==\n{{commons category|Avestan script}}\n{{refbegin}}\n* {{citation|last=Dhalla|first=Maneckji Nusservanji|title=History of Zoroastrianism|publisher=OUP|location=New York|year=1938}}.\n* {{citation|last=Everson|first=Michael|last2=Pournader|first2=Roozbeh|url=http://std.dkuug.dk/jtc1/sc2/wg2/docs/n3197.pdf|title=Revised proposal to encode the Avestan script in the SMP of the UCS|year=<!-- 22 March -->2007|accessdate=2007-06-10|format=PDF}}.\n* {{citation|last=Fossey|first=Charles|year=1948|chapter=Notices sur les caractères étrangers anciens et modernes rédigées par une groupe de savants|title=Nouvelle édition mise à jour à l'occasion du 21<sup>e</sup> Congrès des Orientalistes|location=Paris|publisher=Imprimerie Nationale de France}}.\n* {{citation|last=Hoffmann|first=Karl|chapter=Avestan language|title=Encyclopaedia Iranica|volume=3|year=1989|location=London|publisher=Routledge & Kegan Paul| pages=47&ndash;52}}.\n* {{citation|last=Hoffmann|first=Karl|last2=Forssman|first2=Bernhard|title=Avestische Laut- und Flexionslehre|language=German|publisher=Innsbrucker Beiträge zur Sprachwissenschaft|location=Innsbruck|year=1996 |isbn=3-85124-652-7}}.\n* {{citation|last=Kellens|first=Jean|chapter=Avesta|title=Encyclopaedia Iranica|volume=3|year=1989|location=London|publisher=Routledge & Kegan Paul|pages= 35&ndash;44}}.\n<!-- unused - what is this doing here?\n* {{citation|last=Rashed Mohassel |first=Mohammad Taghi |title=The Avesta: Praise to Truth and Purity |language=Persian |publisher=Cultural Research Bureau |location=Tehran |year=1382 [[Iranian calendar|AP]] |ISBN=964-379-008-8}}.\n-->\n{{refend}}\n\n{{list of writing systems}}\n\n{{portal bar|Writing}}\n\n[[Category:Alphabets]]\n[[Category:Avesta]]\n[[Category:Avestan language|Alphabet]]\n[[Category:Scripts with ISO 15924 four-letter codes]]\n[[Category:Scripts encoded in Unicode 5.2]]\n[[Category:Persian scripts]]\n[[Category:Obsolete writing systems]]"
    },
    {
      "title": "Bhujimol",
      "url": "https://en.wikipedia.org/wiki/Bhujimol",
      "text": "{{refimprove|date=April 2015}}\n[[Image:Devimahatmya Sanskrit MS Nepal 11c.jpg|thumb|400px|Bhujimol script, palm-leaf MS of the [[Devimahatmya]], [[Bihar]] or [[Nepal]], 11th century.]]\n\n'''Bhujimol''' (or ''Bhujinmol'', [[Devanagari]]: {{script|deva|भुजिमोल}} or {{script|deva|भुजिंमोल}}) is the most ancient form of [[Nepal script]]. It is also one of the most common varieties of the Nepal alphabet.<ref>Lienhard, Siegfried (1992). ''Songs of Nepal: An Anthology of Nevar Folksongs and Hymns.'' New Delhi: Motilal Banarsidas. {{ISBN|81-208-0963-7}}. Page 2.</ref>\n\nBhujimol has been used to write [[Nepal Bhasa]] and [[Sanskrit]].\n\n==Etymology==\n[[Image:Nepal Scripts.jpg|thumb|350px|Bhujimol compared to other historical scripts of Nepal.]]\nThe term Bhujinmol means \"fly-headed\", from the Nepal Bhasa words \"bhujin\", meaning \"housefly\", and \"mol\", meaning \"head\". The \"head\" is the horizontal line that is put above each letter, and Bhujimol refers to its rounded shape.{{cn|date=May 2017}}\n\n==Recent findings==\n[[Image:Bhujimol and Devanagari.jpg|thumb|200px|Chart of Bhujimol script vowel letters, with Devanagari and Latin correspondences.]]\n\nIn 2003, a brick was discovered in [[Chabahil]], in the course of reconstruction of the Chabahil Stupa or Dhando [[Chaitya]], bearing inscriptions in both [[Brāhmī script|Brahmi]] and Bhujimol: The upper face is inscribed with ''Cha Ru Wa Ti'' in Brahmi, and with  ''Cha Ru Wa Ti Dhande / He Tu Pra Bha'' in Bhujimol script. There are [[Swastika]] marks at the two ends of the upper face with a [[Chakra]] mark in between. The brick measures 35.5&nbsp;cm x 23&nbsp;cm x 7&nbsp;cm and weighs 8.6&nbsp;kg. The brick may date to as early as the 3rd century BC. {{cn|date=May 2017}}\n\nThe previously earliest known inscription in the Kathmandu Valley dates from the 6th century and is installed at [[Changu Narayan]]. The inscription is interpreted to refer to Charumati, a daughter of emperor [[Ashoka]].\n\n== References ==\n{{Reflist}}\n\n{{Newar}}\n{{list of writing systems}}\n\n[[Category:Alphabets]]\n[[Category:Nepalese culture]]\n[[Category:Newar]]\n[[Category:Scripts not encoded in Unicode]]\n\n\n{{Writingsystem-stub}}"
    },
    {
      "title": "Caucasian Albanian script",
      "url": "https://en.wikipedia.org/wiki/Caucasian_Albanian_script",
      "text": "{{Infobox writing system\n |name      = Caucasian Albanian\n |sample    = Alban-script.jpg\n |caption   = Matenadaran [[MS No. 7117]], fol. 142r\n |imagesize = 175px\n |type      = \n |languages = \n |time      = \n |fam1=\n |fam2=\n |fam3=\n |sisters=\n |children=\n |unicode  = [https://www.unicode.org/charts/PDF/U10530.pdf U+10530–U+1056F]<br />\n[https://www.unicode.org/L2/L2011/11296r-n4131r-caucasian-albanian.pdf Final&nbsp;Accepted&nbsp;Script&nbsp;Proposal]\n |iso15924 = Aghb\n |note     = none\n }}\nThe '''Caucasian Albanian script''' was an [[alphabet]]ic [[writing system]] used by the [[Caucasian Albania]]ns, one of the ancient and indigenous [[Northeast Caucasian languages|Northeast Caucasian peoples]] whose territory comprised parts of present-day [[Azerbaijan]] and [[Republic of Dagestan|Daghestan]]. It was one of only two indigenous scripts ever developed for speakers of indigenous [[Languages of the Caucasus|Caucasian languages]] (i.e. Caucasian languages that are not a part of larger groupings like the Turkic and Indo-European language families) to represent any of their languages, the other being the [[Georgian script]].<ref>{{cite journal|last=Catford|first=J.C.|title=Mountain of Tongues:The Languages of the Caucasus|journal=Annual Review of Anthropology|year=1977|volume=6|pages=283–314 [296]|doi=10.1146/annurev.an.06.100177.001435}}</ref> (The [[Armenian language]], the third language of Caucasus with its own script, is an independent branch of the [[Indo-European language family]].)\n\n==History==\n[[Image:Mesrop Mashtots by Francesco Majotto.jpg|thumb|200px|[[Mesrop Mashtots]] by [[Francesco Maggiotto]] (1750-1805). Mesrop Mashtots, an Armenian medieval evangelizer and enlightener, invented the Caucasian Albanian script in the 5th century, shortly after creating the [[Armenian script]].<ref>Peter R. Ackroyd. The Cambridge history of the Bible. — Cambridge University Press, 1963. — vol. 2. — p. 368:\"''The third Caucasian people, the Albanians, also received an alphabet from Mesrop, to supply scripture for their Christian church. This church did not survive beyond the conquests of Islam, and all but few traces of the script have been lost...''\"</ref>]]\nAccording to [[Movses Kaghankatvatsi]], the Caucasian Albanian script was created by [[Mesrop Mashtots]],<ref>{{cite journal|last=Gippert|first=Jost|author2=Wolfgang Schulze|title=Some Remarks on the Caucasian Albanian Palimpsests|journal=Iran and the Caucasus|year=2007|volume=11|issue=2|pages=201–212 [210]|doi=10.1163/157338407X265441}}'' \"Rather, we have to assume that Old Udi corresponds to the language of the ancient Gargars (cf. Movsēs Kałankatuac‘i who tells us that Mesrop Maštoc‘ (362-440) created with the help [of the bishop Ananian and the translator Benjamin] an alphabet for the guttural, harsh, barbarous, and rough language of the Gargarac‘ik‘).\"''</ref><ref>К. В. Тревер. Очерки по истории и культуре Кавказской Албании. М—Л., 1959:''\"Как известно, в V в. Месроп Маштоц, создавая албанский алфавит, в основу его положил гаргарское наречие албанского языка («создал письмена гаргарского языка, богатого горловыми звуками»). Это последнее обстоятельство позволяет высказать предположение, что именно гаргары являлись наиболее культурным и ведущим албанским племенем.\"''</ref><ref>Peter R. Ackroyd. The Cambridge history of the Bible. — Cambridge University Press, 1963. — vol. 2. — p. 368:\"''The third Caucasian people, the Albanians, also received an alphabet from Mesrop, to supply scripture for their Christian church. This church did not survive beyond the conquests of Islam, and all but few traces of the script have been lost, and there are no remains of the version known.''\"</ref> the Armenian [[monk]], [[theology|theologian]] and [[translation|translator]] who is also credited with creating the Armenian script.<ref name=\"Lenore A. Grenoble\">''Lenore A. Grenoble''. Language policy in the Soviet Union. Springer, 2003. {{ISBN|1-4020-1298-5}}. P. 116. \"''The creation of the Georgian alphabet is '''generally attributed''' to Mesrop, who is also credited with the creation of the Armenian alphabet.''\"</ref><ref name=\"Donald Rayfield\">''[[Donald Rayfield]] \"''The Literature of Georgia: A History (Caucasus World). RoutledgeCurzon. {{ISBN|0-7007-1163-5}}. P. 19. \"''The Georgian alphabet seems unlikely to have a pre-Christian origin, for the major archaeological monument of the first century 4IX the bilingual Armazi gravestone commemorating Serafua, daughter of the Georgian viceroy of Mtskheta, is inscribed in Greek and Aramaic only. '''It has been believed''', and not only in Armenia, that all the Caucasian alphabets — Armenian, Georgian and Caucaso-Albanian — were invented in the fourth century by the Armenian scholar Mesrop Mashtots.<...> The Georgian chronicles The Life of Kanli - assert that a Georgian script was invented two centuries before Christ, an assertion unsupported by archaeology. There is a possibility that the Georgians, like many minor nations of the area, wrote in a foreign language — Persian, Aramaic, or Greek — and translated back as they read.''\"</ref>\n\n[[Koriun]], a pupil of Mesrop Mashtots, in his book ''The Life of Mashtots'', wrote about the circumstances of its creation:\n<blockquote>Then there came and visited them an elderly man, an Albanian named Benjamin.  And he, Mesrop Mashtots, inquired and examined the barbaric diction of the Albanian language, and then through his usual God-given keenness of mind invented an alphabet, which he, through the grace of Christ, successfully organized and put in order.<ref>Koriun, [http://www.vehi.net/istoriya/armenia/korun/english/03.html ''The life of Mashtots''], Ch. 16.</ref></blockquote>\n\nThe alphabet was in use from its creation in the early 5th century through the 12th century, and was used not only formally by the [[Church of Caucasian Albania]], but also for secular purposes.<ref>{{cite journal|last=Schulze|first=Wolfgang|title=Towards a History of Udi|journal=International Journal of Diachronic Linguistics|year=2005|pages=1–27 [12]|url=http://udilang.narod.ru/papers/Schulze_History-of-Udi.pdf|accessdate=4 July 2012}} \"In addition, a small number of inscriptions on candleholders, roofing tiles and on a pedestal found since 1947 in Central and Northern Azerbaijan illustrate that the Aluan alphabet had in fact been in practical use.\"</ref>\n\n==Rediscovery==\n[[File:Caucasian albanian stone azerbaijan mingechaur2.jpg|thumb|150px|left|A capital from a 5th-century church with an inscription using Caucasian Albanian lettering, found at [[Mingachevir]] in 1949]]\nAlthough mentioned in early sources, no examples of it were known to exist until its rediscovery in 1937 by a Georgian scholar, Professor [[Ilia Abuladze]],<ref>Ilia Abuladze. \"About the discovery of the alphabet of the Caucasian Aghbanians\". In the ''Bulletin of the Institute of Language, History and Material Culture (ENIMK)'', Vol. 4, Ch. I, Tbilisi, 1938.</ref> in [[Matenadaran]] [[MS No. 7117]], a manual from the 15th century. This manual presents different alphabets for comparison: [[Greek alphabet|Greek]], [[Latin alphabet|Latin]], [[Syriac alphabet|Syriac]], [[Georgian alphabet|Georgian]], [[Coptic alphabet|Coptic]], and Caucasian Albanian among them.\n\nBetween 1947 and 1952, archaeological excavations at [[Mingachevir]] under the guidance of S. Kaziev found a number of artifacts with Caucasian Albanian writing — a stone altar post with an inscription around its border that consisted of 70 letters, and another 6 artifacts with brief texts (containing from 5 to 50 letters), including candlesticks, a tile fragment, and a vessel fragment.<ref>Philip L. Kohl, Mara Kozelsky, Nachman Ben-Yehuda. Selective Remembrances: Archaeology in the Construction, Commemoration, and Consecration of National Pasts. University of Chicago Press, 2007. {{ISBN|0-226-45058-9}}, {{ISBN|978-0-226-45058-2}}</ref>\n\nThe first literary work in the Caucasian Albanian alphabet was discovered on a [[palimpsest]] in [[Saint Catherine's Monastery]] on [[Mount Sinai]] in 2003 by Dr. [[Zaza Aleksidze]]; it is a fragmentary [[lectionary]] dating to the late 4th or early 5th century AD, containing verses from [[Second Epistle to the Corinthians|2 Corinthians 11]], with a Georgian [[Patericon]] written over it.<ref>Zaza Alexidze; Discovery and Decipherment of Caucasian Albanian Writing  {{cite web|url=http://www.science.org.ge/2007-vol1/161-166.pdf |title=Archived copy |accessdate=2011-01-18 |deadurl=yes |archiveurl=https://web.archive.org/web/20110721030511/http://www.science.org.ge/2007-vol1/161-166.pdf |archivedate=2011-07-21 |df= }}</ref><ref>{{cite web | url= http://azer.com/aiweb/categories/magazine/ai113_folder/113_articles/113_zaza_aleksidze_ashes.html | title=Caucasian Albanian Alphabet: Ancient Script Discovered in the Ashes | first1=Zaza | last1=Aleksidze | first2=Betty | last2=Blair | publisher=Azerbaijan International | date=2003 }}</ref> [[Jost Gippert]], professor of Comparative Linguistics at the [[University of Frankfurt am Main]], and other have published this palimpsest that contains also liturgical readings taken from the [[Gospel of John]].<ref>Gippert, Jost / Schulze, Wolfgang / Aleksidze, Zaza / Mahé, Jean-Pierre: ''The Caucasian Albanian Palimpsests of Mount Sinai,'' 2 vols., XXIV + 530 pp.; Turnhout: Brepols 2009</ref>\n\n==Legacy==\nThe [[Udi language]], spoken by some 8,000 people, mostly in [[Azerbaijan]] but also in [[Georgia (country)|Georgia]] and [[Armenia]],<ref>Wolfgang Schulze, \"The Udi Language\", {{cite web |url=http://www.lrz-muenchen.de/~wschulze/udinhalt.htm |title=Archived copy |accessdate=2010-02-24 |deadurl=yes |archiveurl=https://web.archive.org/web/20090826145045/http://www.lrz-muenchen.de/~wschulze/udinhalt.htm |archivedate=2009-08-26 |df= }}</ref> is considered to be the last direct continuator of the Caucasian Albanian language.<ref>''The Arab geographers refer to the Arranian language as still spoken in the neighbourhood of Barda'a (Persian: Peroz-Abadh, Armenian Partav), but now only the two villages inhabited by the Udi are considered as the direct continuators of the Albanian linguistic tradition''. V. Minorsky. Caucasica IV. Bulletin of the School of Oriental and African Studies, University of London, Vol. 15, No. 3. (1953), pp. 504-529.</ref><ref>\"[http://www.azer.com/aiweb/categories/magazine/ai113_folder/113_articles/113_zaza_significance.html Caucasian Albanian Script. The Significance of Decipherment]\" (2003) by Dr. Zaza Alexidze.</ref>\n\n== Characters ==\nThe script consists of 52 characters, all of which can also represent numerals from 1-700,000 when a combining mark is added above, below, or both above and below them, described as similar to Coptic. 49 of the characters are found in the Sinai palimpsests.<ref>{{cite web | url=https://www.unicode.org/L2/L2011/11296r-n4131r-caucasian-albanian.pdf | title=N4131R: Proposal for encoding the Caucasian Albanian script in the SMP of the UCS | first1=Michael | last1=Everson | first2=Jost | last2=Gippert | publisher=Working Group Document, ISO/IEC JTC1/SC2/WG2 | date=2011-10-28 }}</ref> Several punctuation marks are also present, including a middle dot, a separating colon, an apostrophe, paragraph marks, and citation marks.\n\n==Unicode==\n{{Main|Caucasian Albanian (Unicode block)}}\nThe Caucasian Albanian alphabet was added to the [[Unicode]] Standard in June, 2014 with the release of version 7.0.\n\nThe Unicode block for Caucasian Albanian is U+10530&ndash;1056F:\n\n{{Unicode chart Caucasian Albanian}}\n\n==References==\n<references/>\n\n==External links==\n* Armazi project:\n** [http://armazi.uni-frankfurt.de/sinai/albschr.htm#start Jost Gippert: ''The \"Albanian\" Alphabet as preserved in Armenian Tradition''] – has images of glyphs\n** [http://armazi.uni-frankfurt.de/sinai/alban2.htm#start Zaza Aleksidze: ''A Breakthrough in the Script of Caucasian Albany'']\n\n{{list of writing systems}}\n{{ISO 15924/footer}}\n\n[[Category:Alphabets]]\n[[Category:Obsolete writing systems]]\n[[Category:Udi language]]\n[[Category:Scripts encoded in Unicode 7.0]]"
    },
    {
      "title": "Chakma script",
      "url": "https://en.wikipedia.org/wiki/Chakma_script",
      "text": "{{refimprove|date=November 2017}}\n{{Infobox writing system\n|name=Chakma\n|type=[[Abugida]]\n|languages=[[Chakma language]]\n|sample=Chakma Letter-Biplob Rahman.jpg\n|imagesize=250px\n|fam1=[[Proto-Sinaitic alphabet]]\n|fam2=[[Phoenician alphabet]]\n|fam3=[[Aramaic alphabet]]\n|fam4=[[Brahmi script]]\n|unicode=[https://www.unicode.org/charts/PDF/U11100.pdf U+11100&ndash;U+1114F]\n|iso15924 = Cakm\n }}\n\nThe '''Chakma Script''' (''Ajhā pāṭh''), also called '''Ojhapath''', '''Ojhopath''', '''Aaojhapath''', is an [[abugida]] used for the [[Chakma language]].\n\n==Origin==\n\nThe Chakma alphabet is probably descended from [[Brahmi]] through [[Pallava alphabet|Pallava]]. Proto Chakma developed around the 6th century CE. Old Chakma developed in the 8th century CE. Classical Literary Chakma was used in the 11th to 15th centuries and the current Standard Chakma was developed and revived in the 20th century.{{citation needed|date=July 2015}}\n\nClassical Chakma was probably a sister script of [[Tai Tham]] and [[Tai Lue]] scripts of Northern [[Thailand]] also from the 8th century CE.{{citation needed|date=July 2015}}\n\n==Structure==\nChakma is of the Brahmic type: the consonant letters contain an inherent vowel. Unusually for Brahmic scripts, the inherent vowel in Chakma is a long 'ā' ([[Open central unrounded vowel|aː]]) as opposed to short 'a' ([[mid-central vowel|ə]]) which is standard in most other languages of India such as [[Hindi]], [[Marathi language|Marathi]] or [[Tamil language|Tamil]]. Consonant clusters are written with conjunct characters, and a visible vowel killer shows the deletion of the inherent vowel when there is no conjunct.\n\n===Vowels===\nFour independent vowels exist: {{Script/Chakma|𑄃}} a, {{Script/Chakma|𑄄}} i, {{Script/Chakma|𑄅}} u, and {{Script/Chakma|𑄆}} e. Other vowels in initial position are formed by adding the vowel sign to {{Script/Chakma|𑄃}} a, as in {{Script/Chakma|𑄃𑄩}} ī, {{Script/Chakma|𑄃𑄫}} ū, {{Script/Chakma|𑄃𑄭}} ai, {{Script/Chakma|𑄃𑄰}} oi. Some modern writers are generalizing this spelling in {{Script/Chakma|𑄃𑄨}} i, {{Script/Chakma|𑄃𑄪}} u, and {{Script/Chakma|𑄃𑄬}} e.\n\nChakma vowel signs with the letter {{Script/Chakma|𑄇}} ''ka'' are given below:\n\n{{Script/Chakma|𑄇}}  Ka  = {{Script/Chakma|𑄇}} Ka\n\n{{Script/Chakma|𑄇𑄧}}  Ka  = {{Script/Chakma|𑄇}} Kā + {{Script/Chakma|𑄧}} - a (11127)\n\n{{Script/Chakma|𑄇𑄨}}  Ki  = {{Script/Chakma|𑄇}} Kā + {{Script/Chakma|𑄨}} - i (11128)\n\n{{Script/Chakma|𑄇𑄩}}  Kī  = {{Script/Chakma|𑄇}} Kā + {{Script/Chakma|𑄩}} - ī (11129)\n\n{{Script/Chakma|𑄇𑄪}}  Ku  = {{Script/Chakma|𑄇}} Kā + {{Script/Chakma|𑄪}} - u (1112A)\n\n{{Script/Chakma|𑄇𑄫}}  Kū  = {{Script/Chakma|𑄇}} Kā + {{Script/Chakma|𑄫}} - ū (1112B)\n\n{{Script/Chakma|𑄇𑄬}}  Ke  = {{Script/Chakma|𑄇}} Kā + {{Script/Chakma|𑄬}} - e (1112C)\n\n{{Script/Chakma|𑄇𑄭}}  Kāi = {{Script/Chakma|𑄇}} Kā + {{Script/Chakma|𑄭}} - āi (1112D)\n\n{{Script/Chakma|𑄇𑄮}}  Ko  = {{Script/Chakma|𑄇}} Kā + {{Script/Chakma|𑄮}} - o (1112E)\n\n{{Script/Chakma|𑄇𑄯}}  Kau = {{Script/Chakma|𑄇}} Kā + {{Script/Chakma|𑄯}} - au (1112F)\n\n{{Script/Chakma|𑄇𑄰}} Koi = {{Script/Chakma|𑄇}} Kā + {{Script/Chakma|𑄰}} - oi (11130)\n\n{{Script/Chakma|𑄇𑄀}}  Kaṃ = {{Script/Chakma|𑄇}} Kā + {{Script/Chakma|𑄀}} - ṃ (11100)\n\n{{Script/Chakma|𑄇𑄁}}  Kaṃ = {{Script/Chakma|𑄇}} Kā + {{Script/Chakma|𑄁}} - ṃ (11101)\n\n{{Script/Chakma|𑄇𑄂}}  Kaḥ = {{Script/Chakma|𑄇}} Kā + {{Script/Chakma|𑄂}} - ḥ (11102)\n\n{{Script/Chakma|𑄇𑄴}}  K   = {{Script/Chakma|𑄇}} Kā + {{Script/Chakma|𑄴}} - MAAYYAA (11134)\n\nOne of the interesting features of Chakma writing is that candrabindu (cānaphudā) can be used together with anusvara (ekaphudā) and visarga (dviphudā):\n\n{{Script/Chakma|𑄃𑄂𑄀}}  Aḥṃ = {{Script/Chakma|𑄃}} ā +  {{Script/Chakma|𑄂 h +  𑄀ṃ}}\n\n{{Script/Chakma|𑄃𑄁𑄀}}  Aṃṃ = {{Script/Chakma|𑄃}} ā +  {{Script/Chakma|𑄁 ṃ +  𑄀ṃ}}\n\n{{Script/Chakma|𑄅𑄁𑄀}}  Uṃṃ = {{Script/Chakma|𑄅}} u +  {{Script/Chakma|𑄁 ṃ +  𑄀ṃ}}\n\n{{Script/Chakma|𑄟𑄪𑄀}}  Muṃ = {{Script/Chakma|𑄟}} mā + {{Script/Chakma|𑄪 u +  𑄀ṃ}}\n\n===Consonants with killed Vowels and Conjunct Consonants===\nLike other Brahmic scripts, Chakma makes use of the ''maayyaa'' (killer) to invoke conjoined consonants. In the past, practice was much more common than it is today. Like the Myanmar script, Chakma is encoded with two vowel-killing characters in order to conform to modern user expectations. As shown above, most letters have their vowels killed with the use of the explicit ''maayyaa'':\n\n𑄇𑄴 k = 𑄇 kā +  𑄴 MAAYYAA\n\nIn 2001 an orthographic reform was recommended in the book ''Cāṅmā pattham pāt'' which would limit the standard repertoire of conjuncts to those composed with the five letters 𑄠 yā, 𑄢 rā, 𑄣 lā, 𑄤 wā, and 𑄚 nā. The four here are the most widely accepted repertoire of conjuncts.\n\nya: X + 𑄳 VIRAMA + 𑄠 yā\n\n{{Script/Chakma| 𑄇𑄳𑄠 𑄈𑄳𑄠 𑄉𑄳𑄠 𑄊𑄳𑄠 𑄋𑄳𑄠 - 𑄌𑄳𑄠 𑄍𑄳𑄠 𑄎𑄳𑄠 𑄏𑄳𑄠 𑄐𑄳𑄠 - 𑄑𑄳𑄠 𑄒𑄳𑄠 𑄓𑄳𑄠 𑄔𑄳𑄠 𑄕𑄳𑄠}}\n\n{{Script/Chakma|𑄖𑄳𑄠 𑄗𑄳𑄠 𑄘𑄳𑄠 𑄙𑄳𑄠 𑄚𑄳𑄠 - 𑄛𑄳𑄠 𑄜𑄳𑄠 𑄝𑄳𑄠 𑄞𑄳𑄠 𑄟𑄳𑄠 - 𑄦𑄳𑄠 𑄠𑄳𑄠 𑄡𑄳𑄠 𑄢𑄳𑄠 𑄤𑄳𑄠 𑄥𑄳𑄠}}\n\nra: X + {{Script/Chakma|𑄳 VIRAMA}} + {{Script/Chakma|𑄢}} rā\n\n{{Script/Chakma|𑄇𑄳𑄢 𑄈𑄳𑄢 𑄉𑄳𑄢 𑄊𑄳𑄢 𑄋𑄳𑄢 - 𑄌𑄳𑄢 𑄍𑄳𑄢 𑄎𑄳𑄢 𑄏𑄳𑄢 𑄐𑄳𑄢 - 𑄑𑄳𑄢 𑄒𑄳𑄢 𑄓𑄳𑄢 𑄔𑄳𑄢 𑄕𑄳𑄢}}\n\n{{Script/Chakma|𑄖𑄳𑄢 𑄗𑄳𑄢 𑄘𑄳𑄢 𑄙𑄳𑄢 𑄚𑄳𑄢 - 𑄛𑄳𑄢 𑄜𑄳𑄢 𑄝𑄳𑄢 𑄞𑄳𑄢 𑄟𑄳𑄢 - 𑄦𑄳𑄢 𑄠𑄳𑄢 𑄡𑄳𑄢 𑄢𑄳𑄢 𑄤𑄳𑄢 𑄥𑄳𑄢}}\n\nla: X + {{Script/Chakma|𑄳}} VIRAMA + {{Script/Chakma|𑄣}} lā\n\n{{Script/Chakma|𑄇𑄳𑄣 𑄈𑄳𑄣 𑄉𑄳𑄣 𑄊𑄳𑄣 𑄋𑄳𑄣 - 𑄌𑄳𑄣 𑄍𑄳𑄣 𑄎𑄳𑄣 𑄏𑄳𑄣 𑄐𑄳𑄣 -  𑄑𑄳𑄣 𑄒𑄳𑄣 𑄓𑄳𑄣 𑄔𑄳𑄣 𑄕𑄳𑄣}}\n\n{{Script/Chakma|𑄖𑄳𑄣 𑄗𑄳𑄣 𑄘𑄳𑄣 𑄙𑄳𑄣 𑄚𑄳𑄣 - 𑄛𑄳𑄣 𑄜𑄳𑄣 𑄝𑄳𑄣 𑄞𑄳𑄣 𑄟𑄳𑄣 - 𑄦𑄳𑄣 𑄠𑄳𑄣 𑄡𑄳𑄣 𑄢𑄳𑄣 𑄥𑄳𑄣}}\n\nwa: X + {{Script/Chakma|𑄳}} VIRAMA + {{Script/Chakma|𑄤}} wā\n\n{{Script/Chakma|𑄇𑄳𑄤 𑄈𑄳𑄤 𑄉𑄳𑄤 𑄊𑄳𑄤 𑄋𑄳𑄤 - 𑄌𑄳𑄤 𑄍𑄳𑄤 𑄎𑄳𑄤 𑄏𑄳𑄤 𑄐𑄳𑄤 - 𑄑𑄳𑄤 𑄒𑄳𑄤 𑄓𑄳𑄤 𑄔𑄳𑄤 𑄕𑄳𑄤}}\n\n{{Script/Chakma|𑄖𑄳𑄤 𑄗𑄳𑄤 𑄘𑄳𑄤 𑄙𑄳𑄤 𑄚𑄳𑄤 - 𑄛𑄳𑄤 𑄜𑄳𑄤 𑄝𑄳𑄤 𑄞𑄳𑄤 𑄟𑄳𑄤 - 𑄦𑄳𑄤 𑄠𑄳𑄤 𑄡𑄳𑄤 𑄢𑄳𑄤 𑄥𑄳𑄤}}\n\nNo separate conjunct forms of subjoined full-form -yā or -rā appear to exist. The fifth of these conjuncts, the -na conjunct, is exemplary of the orthographic shift which has taken place in the Chakma language.\n\nna: X + {{Script/Chakma|𑄳}} VIRAMA + {{Script/Chakma|𑄚}} nā\n\n{{Script/Chakma|𑄇𑄳𑄚 𑄈𑄳𑄚 𑄉𑄳𑄚 𑄊𑄳𑄚 𑄋𑄳𑄚 - 𑄌𑄳𑄚 𑄍𑄳𑄚 𑄎𑄳𑄚 𑄏𑄳𑄚 𑄐𑄳𑄚 - 𑄑𑄳𑄚 𑄒𑄳𑄚 𑄓𑄳𑄚 𑄔𑄳𑄚 𑄕𑄳𑄚}}\n\n{{Script/Chakma|𑄖𑄳𑄚 𑄗𑄳𑄚 𑄘𑄳𑄚 𑄙𑄳𑄚 𑄚𑄳𑄚 - 𑄛𑄳𑄚 𑄜𑄳𑄚 𑄝𑄳𑄚 𑄞𑄳𑄚 𑄟𑄳𑄚 - 𑄦𑄳𑄚 𑄠𑄳𑄚 𑄡𑄳𑄚 𑄢𑄳𑄚 𑄥𑄳𑄚}}\n\nWhile some writers would indeed write kakna (in ligating style) as 𑄇𑄇𑄳𑄚 or (in subjoining style) as 𑄇𑄇𑄳𑄚, most now would probably expect it to be written as 𑄇𑄇𑄴𑄚. The ligating style of glyphs is now considered old-fashioned. Thus, taking the letter 𑄟 mā as the second element, while the glyph shapes 𑄇𑄳𑄟  kmā, 𑄖𑄳𑄟 tmā, 𑄚𑄳𑄟 nmā, 𑄝𑄳𑄝 bbā, 𑄟𑄳𑄟 mmā, 𑄣𑄳𑄣 llā, 𑄥𑄳𑄟 smā, and 𑄦𑄳𑄟 hmā are attested, most users now prefer the glyph shapes 𑄇𑄳𑄟 kmā, 𑄖𑄳𑄟 tmā, 𑄚𑄳𑄟 nmā, 𑄝𑄳𑄝 bbā, 𑄟𑄳𑄟 mmā, 𑄣𑄳𑄣 llā, 𑄥𑄳𑄟 smā, and 𑄦𑄳𑄟 hmā. Again, this distinction is stylistic and not orthographic.\n\nThe 2004 book ''Phadagaṅ'' shows examples of the five conjuncts above together alongside conjuncts formed with 𑄝 bā, 𑄟 mā, and 𑄦 hā. These are all formed by simple subjoining.\n\nba: X + 𑄳 VIRAMA + 𑄝 nā\n\n{{Script/Chakma|𑄇𑄳𑄝 𑄈𑄳𑄝 𑄉𑄳𑄝 𑄊𑄳𑄝 𑄋𑄳𑄝 - 𑄌𑄳𑄝 𑄍𑄳𑄝 𑄎𑄳𑄝 𑄏𑄳𑄝 𑄐𑄳𑄌 -  𑄑𑄳𑄝 𑄒𑄳𑄝 𑄓𑄳𑄝 𑄔𑄳𑄚 𑄕𑄳𑄝}}\n\n{{Script/Chakma|𑄖𑄳𑄝 𑄗𑄳𑄝 𑄘𑄳𑄝 𑄙𑄳𑄝 𑄚𑄳𑄚 - 𑄛𑄳𑄝 𑄜𑄳𑄝 𑄝𑄳𑄝 𑄞𑄳𑄝 𑄟𑄳𑄝 - 𑄠𑄳𑄝 𑄡𑄳𑄝 𑄢𑄳𑄝 𑄣𑄳𑄝 𑄤𑄳𑄝 𑄥𑄳𑄝}}\n\nma: X + {{Script/Chakma|𑄳}} VIRAMA + {{Script/Chakma|𑄟}} nā\n\n{{Script/Chakma|𑄇𑄳𑄟 𑄈𑄳𑄟 𑄉𑄳𑄟 𑄊𑄳𑄟 𑄋𑄳𑄟 -  𑄌𑄳𑄟 𑄍𑄳𑄟 𑄎𑄳𑄟 𑄏𑄳𑄟 𑄐𑄳𑄟 - 𑄑𑄳𑄟 𑄒𑄳𑄟 𑄓𑄳𑄟 𑄔𑄳𑄟 𑄕𑄳𑄟}}\n\n{{Script/Chakma|𑄖𑄳𑄟 𑄗𑄳𑄟 𑄘𑄳𑄟 𑄙𑄳𑄟 𑄚𑄳𑄟 - 𑄛𑄳𑄟 𑄜𑄳𑄟 𑄝𑄳𑄟 𑄞𑄳𑄟 𑄟𑄳𑄟 - 𑄠𑄳𑄟 𑄡𑄳𑄟 𑄢𑄳𑄟 𑄣𑄳𑄟 𑄤𑄳𑄟 𑄥𑄳𑄟}}\n\nha: X + {{Script/Chakma|𑄳}} VIRAMA + {{Script/Chakma|𑄦}} nā\n\n{{Script/Chakma|𑄇𑄳𑄦 𑄈𑄳𑄦 𑄉𑄳𑄦 𑄊𑄳𑄦 𑄋𑄳𑄦 - 𑄌𑄳𑄦 𑄍𑄳𑄦 𑄎𑄳𑄦 𑄏𑄳𑄦 𑄐𑄳𑄦 - 𑄑𑄳𑄦 𑄒𑄳𑄦 𑄓𑄳𑄦 𑄔𑄳𑄦 𑄕𑄳𑄦}}\n\n{{Script/Chakma|𑄖𑄳𑄦 𑄗𑄳𑄦 𑄘𑄳𑄦 𑄙𑄳𑄦 𑄚𑄳𑄦 - 𑄛𑄳𑄦 𑄜𑄳𑄦 𑄝𑄳𑄦 𑄞𑄳𑄦 𑄟𑄳𑄦 - 𑄠𑄳𑄦 𑄡𑄳𑄦 𑄢𑄳𑄦 𑄣𑄳𑄦 𑄤𑄳𑄦 𑄥𑄳𑄦}}\n\nIn the 1982 book ''Cāṅmār āg pudhi'' a much wider range of conjunct pairs is shown, some of them with fairly complicated glyphs:\n\n{{Script/Chakma|𑄇𑄳𑄇}}  Kkā   = {{Script/Chakma|𑄇}} Kā + {{Script/Chakma|𑄳}} VIRAMA + 𑄇 Kā\n\n{{Script/Chakma|𑄇𑄳𑄑}}  Ktā   = {{Script/Chakma|𑄇}} Kā + {{Script/Chakma|𑄳}} VIRAMA + 𑄑 Tā\n\n{{Script/Chakma|𑄇𑄳𑄖}}  Ktā   = {{Script/Chakma|𑄇}} Kā + {{Script/Chakma|𑄳}} VIRAMA + 𑄖 Tā\n\n{{Script/Chakma|𑄇𑄳𑄟}}  Kmā   = {{Script/Chakma|𑄇}} Kā + {{Script/Chakma|𑄳}} VIRAMA + 𑄟 Mā\n\n{{Script/Chakma|𑄇𑄳𑄌}}  Kcā   = {{Script/Chakma|𑄇}} Kā + {{Script/Chakma|𑄳}} VIRAMA + 𑄌 Cā\n\n{{Script/Chakma|𑄋𑄳𑄇}}  ńkā   = {{Script/Chakma|𑄋}} ńā + {{Script/Chakma|𑄳}} VIRAMA + 𑄇 Kā\n\n{{Script/Chakma|𑄋𑄳𑄉}}  ńkā   = {{Script/Chakma|𑄋}} ńā + {{Script/Chakma|𑄳}} VIRAMA + 𑄉 Gā\n\n{{Script/Chakma|𑄌𑄳𑄌}}  ccā   = {{Script/Chakma|𑄌}} cā + 𑄳 VIRAMA + 𑄌 Cā\n\n{{Script/Chakma|𑄌𑄳𑄍}}  Cchā  = {{Script/Chakma|𑄌}} Cā + 𑄳 VIRAMA + 𑄍 Chā\n\n{{Script/Chakma|𑄐𑄳𑄌}}  ñcā   = {{Script/Chakma|𑄐}} ñā + 𑄳 VIRAMA + 𑄌 Cā\n\n{{Script/Chakma|𑄐𑄳𑄎}}  ñjā   = {{Script/Chakma|𑄐}} ñā + 𑄳 VIRAMA + 𑄎 Jā\n\n{{Script/Chakma|𑄐𑄳𑄏}}  Ñjhā  = {{Script/Chakma|𑄐}} ñā + 𑄳 VIRAMA + 𑄏 Jhā\n\n{{Script/Chakma|𑄑𑄳𑄑}}  Ttā   = {{Script/Chakma|𑄑}} Tā + 𑄳 VIRAMA + 𑄑 Tā\n\n{{Script/Chakma|𑄖𑄳𑄖}}  Ttā   = {{Script/Chakma|𑄖}} Tā + 𑄳 VIRAMA + 𑄖 Tā\n\n{{Script/Chakma|𑄖𑄳𑄟}}  Tmā   = {{Script/Chakma|𑄖}} Tā + 𑄳 VIRAMA + 𑄟 mā\n\n{{Script/Chakma|𑄖𑄳𑄗}}  Tthā  = {{Script/Chakma|𑄖}} Tā + 𑄳 VIRAMA + 𑄗 Thā\n\n{{Script/Chakma|𑄘𑄳𑄘}}  Ddā   = {{Script/Chakma|𑄘}} Dā + 𑄳 VIRAMA + 𑄘 Dā\n\n{{Script/Chakma|𑄘𑄳𑄙}}  Ddhā  = 𑄘 Dā + 𑄳 VIRAMA + 𑄙 Dhā\n\n{{Script/Chakma|𑄚𑄳𑄖}}  ntā  = 𑄚 nā + 𑄳 VIRAMA + 𑄖 tā\n\n{{Script/Chakma|𑄚𑄳𑄗}}  nthā  = 𑄚 nā + 𑄳 VIRAMA + 𑄗 thā\n\n{{Script/Chakma|𑄚𑄳𑄟}}  nmā  = 𑄚 nā + 𑄳 VIRAMA + 𑄟 mā\n\n{{Script/Chakma|𑄛𑄳𑄛}}  ppā  = 𑄛 pā + 𑄳 VIRAMA + 𑄛 pā\n\n{{Script/Chakma|𑄝𑄳𑄝}}  bbā  = 𑄝 bā + 𑄳 VIRAMA + 𑄝 bā\n\n{{Script/Chakma|𑄟𑄳𑄟}}  mmā  = 𑄟 mā + 𑄳 VIRAMA + 𑄟 mā\n\n{{Script/Chakma|𑄎𑄳𑄎}}  jjā  = 𑄎 jā + 𑄳 VIRAMA + 𑄎 jā \n\n{{Script/Chakma|𑄣𑄳𑄇}}  lkā  = 𑄣 lā + 𑄳 VIRAMA + 𑄇 kā\n\n{{Script/Chakma|𑄣𑄳𑄉}}  lgā  = 𑄣 lā + 𑄳 VIRAMA + 𑄉 gā\n\n{{Script/Chakma|𑄣𑄳𑄣}}  llā  = 𑄣 lā + 𑄳 VIRAMA + 𑄣 lā\n\n{{Script/Chakma|𑄣𑄳𑄑}}  ltā  = 𑄣 lā + 𑄳 VIRAMA + 𑄑 tā\n\n{{Script/Chakma|𑄣𑄳𑄛}}  lpā  = 𑄣 lā + 𑄳 VIRAMA + 𑄛 pā\n\n{{Script/Chakma|𑄣𑄳𑄍}}  lchā  = 𑄣 lā + 𑄳 VIRAMA + 𑄍 chā\n\n{{Script/Chakma|𑄥𑄳𑄑}}  stā  = 𑄥 sā + 𑄳 VIRAMA + 𑄑 tā\n\n{{Script/Chakma|𑄥𑄳𑄇}}  skā  = 𑄥 sā + 𑄳 VIRAMA + 𑄇 kā\n\n{{Script/Chakma|𑄥𑄳𑄛}}  spā  = 𑄥 sā + 𑄳 VIRAMA + 𑄛 pā\n\n{{Script/Chakma|𑄥𑄳𑄟}}  smā  = 𑄥 sā + 𑄳 VIRAMA + 𑄟 mā\n\n{{Script/Chakma|𑄦𑄳𑄟}}  hmā  = 𑄦 hā + 𑄳 VIRAMA + 𑄟 hmā\n\n==Letter, punctuation and digit names==\nChakma letters have a descriptive name followed by a traditional Brahmic consonant. These are given in annotations to the character names. Alongside a single and double danda punctuation, Chakma has a unique question mark, and a section sign, ''Phulacihna''. There is some variation in the glyphs for the ''Phulacihna'',some looking like flowers or leaves. A set of digits exists although Bengali digits are also used.\n\n==Unicode==\n{{Main|Chakma (Unicode block)}}\nChakma script was added to the [[Unicode]] Standard in January, 2012 with the release of version 6.1.\n\nThe Unicode block for Chakma script is U+11100&ndash;U+1114F. Grey areas indicate non-assigned code points:\n\n{{Unicode chart Chakma}}\n\n== References ==\n<references/>\n* [https://languagetools-153419.appspot.com/ccp/ Prototype Chakma Keyboard]\n\n==Further reading==\n* {{cite web|publisher= The Unicode Consortium | last1=Everson | first1=Michael| last2=Hosken| first2=Martin | title=Proposal for encoding the Chakma script in the UCS | url=http://std.dkuug.dk/JTC1/SC2/WG2/docs/n3645.pdf | date=August 13, 2009 }}\n* {{cite news |script-title='রিবেং ইউনি'তে লেখা হবে চাকমা ভাষা |trans-title=RibengUni will be written in the Chakma language |url=http://www.kalerkantho.com/print_edition/index.php?view=details&archiev=yes&arch_date=06-10-2012&type=gold&data=Food&pub_no=1024&cat_id=1&menu_id=14&news_type_id=1&index=6#.UsIKQvTuI2Y |publisher=[[Kaler Kantho]] |date=2012-06-18 |archive-url=https://web.archive.org/web/20140101080358/http://www.kalerkantho.com/print_edition/index.php?view=details&archiev=yes&arch_date=06-10-2012&type=gold&data=Food&pub_no=1024&cat_id=1&menu_id=14&news_type_id=1&index=6 |archive-date=2014-01-01 |accessdate=2013-12-31}}\n\n==External links==\n*[http://uni.hilledu.com RibengUni (First & Only Chakma Unicode Font)]\n*[http://hilledu.com Chakma Script]\n*[http://hillbd.com Chakma Bangla Blog]\n*[https://languagetools-153419.appspot.com/ccp/ Chakma Prototype Keyboard]\n*[https://languagetools-153419.appspot.com/ccp/convertUI/ Chakma Unicode Converter]\n*[https://languagetools-153419.appspot.com/ccp/downloads/ Available Chakma Unicode Font]\n*[https://web.archive.org/web/20160122103909/https://github.com/zenideas/chakma-keylayout-osx Chakma Keyboard Layout for Mac OSX]*[https://web.archive.org/web/20111209003042/http://dictionary.chakma.info/ Chakma Open Dictionary]\n*{{Cite web\n| title = Chakma alphabet, pronunciation and language\n| work = Omniglot\n| accessdate = 2012-09-02\n| url = http://www.omniglot.com/writing/chakma.htm\n}}\n*{{Cite web\n| title = Tribal Languages - Banglapedia\n| work = Banglapedia\n| accessdate = 2018-09-01\n| url = http://en.banglapedia.org/index.php?title=Tribal_Languages\n}}\n\n{{list of writing systems}}\n\n[[Category:Alphabets]]\n[[Category:Brahmic scripts]]\n[[Category:Scripts encoded in Unicode 6.1]]"
    },
    {
      "title": "Cirth",
      "url": "https://en.wikipedia.org/wiki/Cirth",
      "text": "{{Infobox writing system\n| name = Cirth\n| sample = Cirth word.png\n| caption = The word \"Cirth\" written using the Cirth in the Angerthas Daeron mode\n| image_size = 180px\n| type = [[Alphabet]]\n| languages = [[Khuzdul]], [[Sindarin]], [[Quenya]], [[Westron]], [[English language|English]]\n| creator = [[J.R.R. Tolkien]]\n| iso15924 = Cirt\n}}\n[[Image:Ashton_Park_rock_carving.jpg|thumb|270px|Rock carving in Cirth in the [[Sydney Harbour National Park]], dating back to the 1980s at least]]\nThe '''Cirth''' ({{IPA-sjn|ˈkirθ}}, meaning \"[[rune]]s\"; sing. '''certh''' {{IPA-sjn|ˈkɛrθ|}})&nbsp;is a semi‑[[artificial script]], based on real‑life [[runic alphabet]]s, invented by [[J. R. R. Tolkien]] for the [[constructed language]]s he devised and used in his works. ''Cirth'' is written with a capital letter when referring to the writing system; the runes themselves can be called ''cirth''.\n\nIn the fictional history of [[Middle-earth]], the original '''''Certhas''''' was created by the [[Sindar|Grey Elves]] for their language, [[Sindarin]]. Its extension and\nelaboration was known as the '''''Angerthas Daeron''''', as it was attributed to the Sindar [[Daeron]], although it was most probably expanded by the [[Noldor]] in order to represent the sounds of other languages like [[Quenya]].\n\nAlthough the Cirth was later largely replaced by the [[Tengwar]], it was adopted by [[Dwarf (Middle-earth)|Dwarves]] to write down both their [[Khuzdul]] language ('''''Angerthas Moria''''') and the languages of [[Man (Middle-earth)|Men]] ('''''Angerthas Erebor'''''). The Cirth was also adapted, in its oldest and simplest form, by various races including [[Man (Middle-earth)|Men]] and even [[Orc (Middle-earth)|Orcs]].\n\n==External history==\n===Concept and creation===\nMany letters have shapes also found in the historical [[runic alphabets]], but their sound values are only similar in a few of the vowels. Rather, the system of assignment of sound values is much more systematic in the Cirth than in the historical runes (e.g., voiced variants of a voiceless sound are expressed by an additional stroke). A similar system has been proposed for a few historical runes but is in any case much more obscure.\n\nThe division between the older Cirth of Daeron and their adaptation by Dwarves and Men has been interpreted as a parallel drawn by Tolkien to the development of the Fuþorc to the [[Younger Fuþark]].<ref>{{cite book |title=Mittelerde: Tolkien und die germanische Mythologie |trans-title=Middle-earth: Tolkien and Germanic Mythology |first=Rudolf |last=Simek |language=German |authorlink=Rudolf Simek |pages=155–156 |publisher=C. H. Beck |year=2005 |isbn=3-406-52837-6}}</ref> The original Elvish Cirth «as supposed products of a superior culture» are focused on logical arrangement and a close connection between form and value whereas the adaptations by mortal races introduced irregularities. Similar to the Germanic tribes who had no written literature and used only simple runes before their conversion to Christianity, the Sindarin Elves of Beleriand with their Cirth were introduced to the more elaborate Tengwar of Fëanor when the Noldorin Elves returned to Middle-earth from the lands of the divine [[Valar]].<ref>{{cite conference  |booktitle=Semiotics Around the World: Synthesis in Diversity. Proceedings of the Fifth Congress of the International Association for Semiotic Studies, Berkeley, 1994 |editor-first=Irmengard |editor-last=Rauch |editor-link=Irmengard Rauch |editor2-first=Gerald F. |editor2-last=Carr |title=The semiotics of the writing systems of Tolkien's Middle-earth |first=Arden R. |last=Smith |authorlink=Arden R. Smith |pages=1239–1242 |volume=1 |publisher= [[Walter de Gruyter]] |year=1997 |isbn=3-11-012223-5}}</ref>\n\n==Internal history and Description==\n===First ''Certhas''===\nIn the Appendix E of ''[[The Return of the King]]'', Tolkien writes that the [[Sindar]] of [[Beleriand]] first developed an [[alphabet]] for their language sometimes between the invention of the [[Tengwar]] by [[Fëanor]] and their introduction to [[Middle-earth]] by the exiled [[Noldor]].\n\nThis alphabet was devised to represent only the sounds of their [[Sindarin]] language and its letters were entirely used for inscribing names or brief memorials on wood, stone or metal, hence their angular forms and straight lines.<ref name=\"Tolkien 1955\">{{cite book |title=[[The Return of the King]] – Being the Third Part of [[The Lord of the Rings]]; Appendix E |first=J.R.R. |last=Tolkien |authorlink=J.R.R. Tolkien |publisher=London: George Allen & Unwin. |year=1955}}</ref> In [[Sindarin]] these letters were named ''cirth'' (sing. ''certh''), from the Elvish root ''*kir&#8209;'' meaning \"to cleave, to cut\".<ref>{{cite web |url= https://eldamo.org/content/words/word-285722173.html|title= Sindarin Words: ''certh''|author=<!--Not stated-->|website=eldamo.org|access-date=2019-03-31}}</ref> An [[abecedarium]] of cirth, consisting of the runes listed in due order, was commonly known as '''''Certhas''''' ({{IPA-sjn|ˈkɛrθɑs|}}, meaning \"rune-rows\" in Sindarin and loosely translated as \"runic alphabet\"<ref>{{cite web |url= https://eldamo.org/content/words/word-1641782601.html|title= Sindarin Words: ''certhas''|author=<!--Not stated-->|website=eldamo.org|access-date=2019-03-31}}</ref>).\n\nThe cirth used for [[voiceless consonant|voiceless]] [[stop consonant]]s were constructed systematically by the combination of a \"stem\" and a \"branch\". The attachment of the branch was usually made on the right side. The reverse was not infrequent, but had no phonetic significance<ref name=\"Tolkien 1955\"/> (this means that [[Image:Certh 10.svg|16px]] would just be an alternative form of [[Image:Certh 8.svg|16px]]).<br/>\nOther consonants were formed following two basic principles:\n# adding a stroke to a branch added [[Voiced consonant|voice]] (e.g., [[Image:Certh 1.svg|16px]] {{IPA|/p/}} → [[Image:Certh 2.svg|16px]] {{IPA|/b/}});\n# placing the branch on both sides of the stem added [[Nasal stop|voice and nasality]] (e.g., [[Image:Certh 18.svg|16px]] {{IPA|/k/}} → [[Image:Certh 22.svg|16px]] {{IPA|/ŋ/}}).\nThe cirth constructed in this way can therefore be grouped into series. Each series corresponds to a [[place of articulation]]. This earliest system had three series:\n* [[labial consonant]]s, based on [[Image:Certh 1.svg|16px]];\n*[[dental consonant]]s, based on [[Image:Certh 8.svg|16px]];\n* [[velar consonant]]s, based on [[Image:Certh 18.svg|16px]].\n\nThere are also additional cirth that do not have regular shapes. These include [[liquid consonant]]s {{angbr|r}} and {{angbr|l}}, the [[voiceless glottal transition]] {{angbr|h}}, the [[voiceless alveolar fricative]] {{angbr|s}}, and [[vowel]]s.\n\nThe original display of Cirth should have been this:<ref name=\"Tolkien 1955\"/>\n<center>\n{| class=\"wikitable\"\n|- align=\"center\"\n! '''''Certh'''''|| '''Sindarin<br/>grapheme'''\n!style=\"border-right: 3px solid #a2a9b1;\"| '''[[International Phonetic Alphabet|IPA]]'''\n! '''''Certh'''''|| '''Sindarin<br/>grapheme''' \n!style=\"border-right: 3px solid #a2a9b1;\"| '''[[International Phonetic Alphabet|IPA]]'''\n! '''''Certh'''''|| '''Sindarin<br/>grapheme''' \n!style=\"border-right: 3px solid #a2a9b1;\"| '''[[International Phonetic Alphabet|IPA]]'''\n! '''''Certh'''''|| '''Sindarin<br/>grapheme''' \n!style=\"border-right: 3px solid #a2a9b1;\"| '''[[International Phonetic Alphabet|IPA]]'''\n! '''''Certh'''''|| '''Sindarin<br/>grapheme''' || '''[[International Phonetic Alphabet|IPA]]'''\n|- align=\"center\"\n| [[Image:Certh 1.svg|20px]] || {{angbr|p}} \n|style=\"border-right: 3px solid #a2a9b1;\"| {{IPA|/p/}}\n| [[Image:Certh 8.svg|20px]] || {{angbr|t}} \n|style=\"border-right: 3px solid #a2a9b1;\"| {{IPA|/t/}}\n| [[Image:Certh 18.svg|20px]] || {{angbr|c}} \n|style=\"border-right: 3px solid #a2a9b1;\"| {{IPA|/k/}}\n| [[Image:Certh 29.svg|20px]] || {{angbr|r}} \n|style=\"border-right: 3px solid #a2a9b1;\"| {{IPA|/r/}}\n| [[Image:Certh 13.svg|20px]] || {{angbr|h}}&nbsp;<span style=\"font-size:65%\">or</span> {{angbr|s}}{{ref|ECh|[D]}} || {{IPA|/h/}}&nbsp;<span style=\"font-size:65%\">or</span> {{IPA|/s/}}\n|- align=\"center\"\n| [[Image:Certh 2.svg|20px]] || {{angbr|b}} \n|style=\"border-right: 3px solid #a2a9b1;\"| {{IPA|/b/}}\n| [[Image:Certh 9.svg|20px]] || {{angbr|d}} \n|style=\"border-right: 3px solid #a2a9b1;\"| {{IPA|/d/}}\n| [[Image:Certh 19.svg|20px]] || {{angbr|g}} \n|style=\"border-right: 3px solid #a2a9b1;\"| {{IPA|/ɡ/}}\n| [[Image:Certh 31.svg|20px]] || {{angbr|l}} \n|style=\"border-right: 3px solid #a2a9b1;\"| {{IPA|/l/}}\n| [[Image:Certh 35.svg|20px]] || {{angbr|s}}&nbsp;<span style=\"font-size:65%\">or</span> {{angbr|h}}{{ref|ECh|[D]}} || {{IPA|/s/}}&nbsp;<span style=\"font-size:65%\">or</span> {{IPA|/h/}}\n|- align=\"center\"\n| [[Image:Certh 5.svg|20px]] || {{angbr|m}}{{ref|ECm|[A]}} \n|style=\"border-right: 3px solid #a2a9b1;\"| {{IPA|/m/}}\n| [[Image:Certh 12.svg|20px]] || {{angbr|n}} \n|style=\"border-right: 3px solid #a2a9b1;\"| {{IPA|/n/}}\n| [[Image:Certh 22.svg|20px]] || {{angbr|n}}, {{angbr|&#8209;ng}}{{ref|ECng|[C]}} \n|style=\"border-right: 3px solid #a2a9b1;\"| {{IPA|/ŋ/}}\n|colspan=\"3\" style=\"border-right: 3px solid #a2a9b1;\"|\n| [[Image:Certh 36.svg|20px]] || {{ref|ECE|[E]}} ||\n|- align=\"center\"\n| [[Image:Certh 6.svg|20px]] || {{ref|ECB|[B]}} \n|style=\"border-right: 3px solid #a2a9b1;\"|\n|colspan=\"3\" style=\"border-right: 3px solid #a2a9b1;\"|\n|colspan=\"3\" style=\"border-right: 3px solid #a2a9b1;\"|\n|colspan=\"3\" style=\"border-right: 3px solid #a2a9b1;\"|\n|colspan=\"3\"|\n|- align=\"center\"\n|colspan=\"15\"|\n|-align=\"center\"\n| [[Image:Certh 39.svg|20px]] || {{angbr|i}}{{ref|ECi|[F]}} \n|style=\"border-right: 3px solid #a2a9b1;\"| {{IPA|/i/}}, {{IPA|/j/}}\n| [[Image:Certh 42.svg|20px]] || {{angbr|u}}{{ref|ECu|[G]}}\n|style=\"border-right: 3px solid #a2a9b1;\"| {{IPA|/u/, /w/}}<sup>?</sup>\n| [[Image:Certh 46.svg|20px]] || {{angbr|e}} \n|style=\"border-right: 3px solid #a2a9b1;\"| {{IPA|/ɛ/}}\n| <big>�</big> || {{angbr|a}}{{ref|ECa|[H]}}\n|style=\"border-right: 3px solid #a2a9b1;\"| {{IPA|/ɑ/}}\n|| [[Image:Certh 50.svg|20px]] || {{angbr|o}} || {{IPA|/ɔ/}}\n|}\n</center>\n\nThe known ancient cirth do not cover all the sounds of Sindarin: there is no certh for {{angbr|rh}}, {{angbr|lh}}, {{angbr|mh}}, {{angbr|y}} or {{angbr|œ}}. Perhaps this system had been devised for the Old Sindarin tongue, as many of the above-mentioned sounds did not exist in that language.<br/>\nHowever, still frequent sounds {{angbr|w}} and {{angbr|a}} are missing, too. This indicates that some ancient, unknown cirth could have existed, but did not make it to the later systems; a fuller table therefore cannot be reconstructed.<br/>\nLong vowels were evidently indicated by doubling.\n\n{|\n|- style=\"vertical-align: top;\"\n| align=\"right\" | A. || {{note|ECm||The original value of the certh [[Image:Certh 5.svg|16px]] was not given by Tolkien, but he mentions that it took the value {{angbr|hw}} after [[Image:Certh 6.svg|16px]] was adopted for {{angbr|m}}. As he did not indicate the former certh for {{angbr|m}}, we can infer that it was this one, judging by both its labial shape and the typical symmetry of nasals.}}\n|- style=\"vertical-align: top;\"\n| align=\"right\" | B. || {{note|ECB||The original value of the certh [[Image:Certh 6.svg|16px]] cannot be guessed but, judging from its shape, it was probably a labial consonant.}}\n|- style=\"vertical-align: top;\"\n| align=\"right\" | C. || {{note|ECng||The certh [[Image:Certh 22.svg|16px]] was used to represent {{angbr|n}} followed by {{angbr|c}} or {{angbr|g}}. In these positions, {{angbr|n}} is not pronounced {{IPA|/n/}} but [[Assimilation (phonology)|assimilates]] to {{IPA|/ŋ/}} instead: [[Image:Certh 22.svg|16px]][[Image:Certh 18.svg|16px]] {{IPA|/ŋk/}} and [[Image:Certh 22.svg|16px]][[Image:Certh 19.svg|16px]] {{IPA|/ŋɡ/}}.<br/>The certh [[Image:Certh 22.svg|16px]] was also used for the grapheme {{angbr|&#8209;ng}} at the end of a word. In fact, although the grapheme {{angbr|ng}} usually represents the [[consonant cluster]] {{IPA|/ŋɡ/}} in Sindarin, it represents a simple [[velar nasal]] {{IPA|/ŋ/}} when final (the {{angbr|g}} is silent, like in the English word {{angbr|sing}} {{IPAc-en|ˈ|s|i|N}}).}}\n|- style=\"vertical-align: top;\"\n| align=\"right\" | D. || {{note|ECh||The sound attributed to the certh [[Image:Certh 13.svg|16px]] and the certh [[Image:Certh 35.svg|16px]] was interchangeable.<ref name=\"Tolkien 1955\"/>}}\n|- style=\"vertical-align: top;\"\n| align=\"right\" | E. || {{note|ECE||The certh [[Image:Certh 36.svg|16px]] will later have the value {{angbr|ss}} in Elvish languages. It could have had another unknown value before.}}\n|- style=\"vertical-align: top;\"\n| align=\"right\" | F. || {{note|ECi||In Sindarin, {{angbr|i}} represents {{IPA|/j/}} when initial before vowels, {{IPA|/i/}} everywhere else.}}\n|- style=\"vertical-align: top;\"\n| align=\"right\" | G. || {{note|ECu||Perhaps the certh [[Image:Certh 42.svg|16px]] was used for both {{IPA|/u/}} and {{IPA|/w/}}, like [[Latin spelling and pronunciation|Latin {{angbr|{{sqc|v}}}}]] (e.g., {{angbr|{{sqc|vvlnvs}}}}, [[Classical Latin|Classical]] pronunciation: {{IPA-la|ˈwuːɫ.nʊs|}}) and similarly to the certh [[Image:Certh 39.svg|16px]], used for both {{IPA|/i/}} and {{IPA|/j/}}.}}\n|- style=\"vertical-align: top;\"\n| align=\"right\" | H. || {{note|ECa||The earliest certh for {{angbr|a}} cannot be guessed: it was likely one of some other cirth that did not survive in later systems.}}\n|}\n\n===''Angerthas Daeron''===\nBefore the end of the [[First Age]] the ''Certhas'' was rearranged and further developed, partly under the influence of the [[Tengwar]]. This reorganisation of the Cirth was commonly attributed to the [[Elf (Middle-earth)|Elf]] [[Daeron]], [[minstrel]] and loremaster of king [[Thingol]] of [[Doriath]]. Thus, the new system became known as the '''''Angerthas Daeron'''''<ref name=\"Tolkien 1955\"/> (where \"angerthas\" {{IPA-sjn|ɑŋˈɡɛrθɑs|}} is a compound of the Sindarin words \"an(d)\" {{IPA-sjn|ɑn(d)|}} and \"certhas\" {{IPA-sjn|ˈkɛrθɑs|}}, meaning \"long rune-rows\"<ref>{{cite web |url= https://eldamo.org/content/words/word-698537603.html|title= Sindarin Words: ''angerthas''|author=<!--Not stated-->|website=eldamo.org|access-date=2019-05-11}}</ref>).\n\nUnlike the previous system, the flipped form of a certh had now a phonemic significance: it signalled the [[lenition]] of the original rune. These new cirth were needed in order to represent [[fricative consonant|fricatives]] that were developed at one point in Sindarin (e.g., [[Image:Certh 8.svg|16px]] {{IPA|/t/}} → [[Image:Certh 10.svg|16px]] {{IPA|/θ/}}).\n\nSome new runes were introduced in the ''Angerthas'' with the purpose to represent:\n* the frequent sounds {{IPA|/ɑ/}} and {{IPA|/w/}};\n* [[long vowel]]s, that evidently used to be written by doubling the certh of the corresponding short vowel (e.g., [[Image:Certh 50.svg|16px]][[Image:Certh 50.svg|16px]] → [[Image:Certh 51.svg|16px]] {{IPA|/oː/}});\n* two [[front vowel]]s, probably originated as ligatures of the corresponding [[back vowel]] with {{angbr|i}}: [[Image:Certh 42.svg|16px]][[Image:Certh 39.svg|16px]] → [[Image:Certh 45a.svg|16px]] {{IPA|/y/}}, and [[Image:Certh 50.svg|16px]][[Image:Certh 39.svg|16px]] → [[Image:Certh 52.svg|16px]] {{IPA|/œ/}};\n* two common [[consonant cluster]]s: {{IPA|/ŋɡ/}} and {{IPA|/nd/}}.\n\nHowever, the principal additions to the former ''Certhas'' were two entirely new series of regularly-formed cirth:\n* [[palatal consonant]]s, based on [[Image:Certh 13.svg|16px]];\n* [[labialized velar consonant]]s, based on [[Image:Certh 23.svg|16px]].\nSince these new series represent sounds which do not occur in Sindarin but are present in [[Quenya]], they were most probably invented by the Exiled [[Noldor]]<ref name=\"Tolkien 1955\"/> that spoke Quenya as a language of knowledge. By loan-translation, the Cirth became known in Quenya as ''Certar'' {{IPA-qya|ˈkɛrtar|}}, while a single certh was called ''certa'' {{IPA-qya|ˈkɛrta|}}.\n\nAccording to Tolkien, the ''Angerthas Daeron'' was used primarily for carved inscriptions, as for most other forms of written communication the Tengwar were used after their introduction in Middle-earth. Apparently, the Elves abandoned the Cirth altogether, with the exception of the Noldor dwelling in [[Eregion]], where the ''Angerthas'' was maintained and became also known as '''''Angerthas Eregion'''''.\n\n<center>\n{| cellpadding=\"4\" cellspacing=\"0\"  class=\"wikitable\"\n|- align=\"center\"\n!'''''Certh'''''||'''Translit.'''\n!style=\"border-right: 3px solid #a2a9b1;\"|'''[[International Phonetic Alphabet|IPA]]'''\n!'''''Certh'''''||'''Translit.'''\n!style=\"border-right: 3px solid #a2a9b1;\"|'''[[International Phonetic Alphabet|IPA]]'''\n!'''''Certh'''''||'''Translit.'''\n!style=\"border-right: 3px solid #a2a9b1;\"|'''[[International Phonetic Alphabet|IPA]]'''\n!'''''Certh'''''||'''Translit.'''||'''[[International Phonetic Alphabet|IPA]]'''\n|- align=\"center\"\n|[[Image:Certh 1.svg|20px]]||p\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/p/}}\n|[[Image:Certh 16.svg|20px]]||zh\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/ʝ/}}&nbsp;<span style=\"font-size:65%\">or</span> {{IPA|/ʒ/}}\n|[[Image:Certh 31.svg|20px]]||l\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/l/}}\n|[[Image:Certh 46.svg|20px]]||e||{{IPA|/ɛ/}}\n|- align=\"center\"\n|[[Image:Certh 2.svg|20px]]||b\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/b/}}\n|[[Image:Certh 17.svg|20px]]||nj\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/ɲ/}}&nbsp;<span style=\"font-size:65%\">or</span> {{IPA|/n̠ʲd͡ʒ/}}\n|[[Image:Certh 32.svg|20px]]||lh\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/ɬ/}}\n|[[Image:Certh 47.svg|20px]]||ê||{{IPA|/eː/}}\n|- align=\"center\"\n|[[Image:Certh 3.svg|20px]]||f\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/f/}}\n|[[Image:Certh 18.svg|20px]]||k\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/k/}}\n|[[Image:Certh 33.svg|20px]]||ng\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/ŋɡ/}}\n|[[Image:Certh 48.svg|20px]]||a||{{IPA|/a/}}\n|- align=\"center\"\n|[[Image:Certh 4.svg|20px]]||v\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/v/}}\n|[[Image:Certh 19.svg|20px]]||g\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/ɡ/}}\n|[[Image:Certh 34.svg|20px]]||rowspan=\"2\"|s\n|rowspan=\"2\" style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/s/}}\n|[[Image:Certh 49.svg|20px]]||â||{{IPA|/aː/}}\n|- align=\"center\"\n|[[Image:Certh 5.svg|20px]]||hw\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/ʍ/}}\n|[[Image:Certh 20.svg|20px]]||kh\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/x/}}\n|[[Image:Certh 35.svg|20px]]\n|[[Image:Certh 50.svg|20px]]||o||{{IPA|/ɔ/}}\n|- align=\"center\"\n|[[Image:Certh 6.svg|20px]]||m\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/m/}}\n|[[Image:Certh 21.svg|20px]]||gh\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/ɣ/}} \n|[[Image:Certh 36.svg|20px]]||ss—z{{ref|ADB|[B]}}\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/ss/}}—{{IPA|/z/}}\n|[[Image:Certh 51.svg|20px]]&nbsp;<span style=\"font-size:65%\">or</span>&nbsp;[[Image:Certh 51a.svg|20px]]||ô||{{IPA|/oː/}}\n|- align=\"center\"\n|[[Image:Certh 7.svg|20px]]||mh{{ref|ADmh|[D]}}\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/ṽ/}}\n|[[Image:Certh 22.svg|20px]]||ŋ\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/ŋ/}}\n|colspan=\"3\" style=\"border-right: 3px solid #a2a9b1;\"|\n|[[Image:Certh 52.svg|20px]]&nbsp;<span style=\"font-size:65%\">or</span>&nbsp;[[Image:Certh 52a.svg|20px]]||ö||{{IPA|/œ/}}\n|- align=\"center\"\n|[[Image:Certh 8.svg|20px]]||t\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/t/}}\n|[[Image:Certh 23.svg|20px]]||kw\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/kʷ/}}\n|[[Image:Certh 38.svg|20px]]&nbsp;<span style=\"font-size:65%\">or</span>&nbsp;[[Image:Certh 38a.svg|20px]]||nd\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/nd/}}\n|colspan=\"3\"|\n|- align=\"center\"\n|[[Image:Certh 9.svg|20px]]||d\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/d/}}\n|[[Image:Certh 24.svg|20px]]||gw\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/ɡʷ/}}\n|[[Image:Certh 39.svg|20px]]||i,&nbsp;y\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/i/}},&nbsp;{{IPA|/j/}}\n|[[Image:Certh 54.svg|20px]]||h{{ref|ADC|[C]}}||{{IPA|/h/}}\n|- align=\"center\"\n|[[Image:Certh 10.svg|20px]]||th\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/θ/}}\n|[[Image:Certh 25.svg|20px]]||khw\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/xʷ/}}\n|colspan=\"3\" style=\"border-right: 3px solid #a2a9b1;\"|\n|colspan=\"3\"|\n|- align=\"center\"\n|[[Image:Certh 11.svg|20px]]||dh\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/ð/}}\n|[[Image:Certh 26.svg|20px]]||ghw\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/ɣʷ/}}\n|colspan=\"3\" style=\"border-right: 3px solid #a2a9b1;\"|\n|colspan=\"3\"|\n|- align=\"center\"\n|[[Image:Certh 12.svg|20px]]||n\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/n/}}\n|[[Image:Certh 27.svg|20px]]||ngw\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/ŋɡʷ/}}\n|[[Image:Certh 42.svg|20px]]||u\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/u/}}\n|colspan=\"3\"|\n|- align=\"center\"\n|[[Image:Certh 13.svg|20px]]||ch{{ref|ADA|[A]}}\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/c/}}&nbsp;<span style=\"font-size:65%\">or</span> {{IPA|/t͡ʃ/}}\n|[[Image:Certh 28.svg|20px]]||nw\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/ŋʷ/}}→{{IPA|/nʷ/}}\n|[[Image:Certh 43.svg|20px]]||û\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/uː/}}\n|colspan=\"3\"|\n|- align=\"center\"\n|[[Image:Certh 14.svg|20px]]||j\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/ɟ/}}&nbsp;<span style=\"font-size:65%\">or</span> {{IPA|/d͡ʒ/}}\n|[[Image:Certh 29.svg|20px]]||r\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/r/}}\n|[[Image:Certh 44.svg|20px]]||w\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/w/}}\n|colspan=\"3\"|\n|- align=\"center\"\n|[[Image:Certh 15.svg|20px]]||sh\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/ç/}}&nbsp;<span style=\"font-size:65%\">or</span> {{IPA|/ʃ/}}\n|[[Image:Certh 30.svg|20px]]||rh\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/r̥/}}\n|[[Image:Certh 45.svg|20px]]&nbsp;<span style=\"font-size:65%\">or</span> [[Image:Certh 45a.svg|20px]]||ü\n|style=\"border-right: 3px solid #a2a9b1;\"|{{IPA|/y/}}\n|colspan=\"3\"|\n|}\n</center>\n\nWhereas the regular Sindarin spelling is used in this article to transliterate the primitive ''Certhas'', the ''Angerthas'' follows the peculiar transliteration introduced by Tolkien in the Appendix E. For example, the Sindarin spelling for {{IPA|/y/}} is {{angbr|y}}, but the same sound is spelled {{angbr|ü}} when transliterating the ''Angerthas'' (where {{angbr|y}} represents the sound {{IPA|/j/}}). The motivation behind this peculiar transliteration is that this alphabet was meant to cover a much larger set of sounds than the previous one.\n\nIn this article, the cirth of the [[Image:Certh 13.svg|16px]]‑series present two IPA transcriptions each. The reason is that the [[palatal consonant]]s of Noldorin Quenya are realised as [[palato-alveolar consonant]]s in Vanyarin Quenya (or [[Vanyar#Vanyarin Quenya|Quendya]]). For example, the Quenya world {{angbr|tyelpë}} is pronounced {{IPA-qya|ˈcȷ̊ɛlpɛ|}}<ref>{{cite journal|last1= Tolkien|first1= J.R.R.|date=2015-06-12|title=Quenya consonants|url= http://www.eldalamberon.com/parma22.html|journal=Parma Eldalamberon|issue=22|pages= 66|quote={{angbr|ty}} is pronounced as a 'front explosive' [c], as e.g. Hungarian ''ty'', but it is followed by an appreciable partly unvoiced y-offglide.}}</ref> in the Noldorin dialect, but {{IPA-qya|ˈt͡ʃɛlpɛ|}}<ref>{{cite web|url=https://realelvish.net/pronunciation/quenya/|title=Quenya pronunciation|website=RealElvish.net|access-date=2019-05-07}}</ref> in Vanyarin.<br/>\nAlthough in the fictional history of Middle-earth this series of consonants was introduced by the Noldor, it is deemed necessary to show the Vanyarin pronunciation as well, given that the very transliteration used by Tolkien is more similar to the Vanyarin [[phonotactics]] than the Noldorin.\n\n{|\n|- style=\"vertical-align: top;\"\n| align=\"right\" | A. || {{note|ADA||The certh [[Image:Certh 13.svg|16px]] (that had been used for {{IPA|/h/}} in the original ''Certhas'') was chosen as the basis for the new series of palatal consonants. This means that it was given the value {{angbr|ch}}, i.e. the [[voiceless palatal stop]] {{IPA|/c/}} (or the [[voiceless palato-alveolar affricate]] {{IPA|/t͡ʃ/}}, in the Vanyarin dialect).<br/>N.B., this sound is completely unrelated to Sindarin {{angbr|ch}}, which is pronounced {{IPA|/x/}}, and is transliterated {{angbr|kh}} in the ''Angerthas''.}}\n|- style=\"vertical-align: top;\"\n| align=\"right\" | B. || {{note|ADB||Tolkien gives the sound {{IPA|/z/}} to this certh (probably in non-Elvish languages), but points out that it was used as {{IPA|/ss/}} in Quenya and in Sindarin.<ref name=\"Tolkien 1955\"/>}}\n|- style=\"vertical-align: top;\"\n| align=\"right\" | C. || {{note|ADC||This certh was made anew for the sound {{IPA|/h/}}. It is similar in shape both to the former {{IPA|/h/}}&#8209;certh [[Image:Certh 13.svg|16px]] (here used for {{IPA|/tʃ/}}), and to the tengwa ''hyarmen'' [[Image:Tengwa hyarmen.svg|16px]].}}\n|- style=\"vertical-align: top;\"\n| align=\"right\" | D. ||{{note|ADmh||In archaic Sindarin a certh for {{angbr|mh}} (representing the sound [[Nasalization#Nasalized consonants|{{IPA|/ṽ/}}]]) was needed, and the most appropriate solution was to flip the certh for {{angbr|m}} to indicate its lenition.<ref name=\"Tolkien 1955\"/> But, being the certh [[Image:Certh 5.svg|16px]] horizontally symmetric, it could not be flipped. Therefore, the value {{angbr|m}} was given to [[Image:Certh 6.svg|16px]] (which until then had a different, unknown value), {{angbr|mh}} was given to [[Image:Certh 7.svg|16px]], and the certh [[Image:Certh 5.svg|16px]] assumed the value {{angbr|hw}}.<ref name=\"Tolkien 1955\"/> The sound {{IPA|/ṽ/}} merged with {{IPA|/v/}} in later Sindarin.}}\n|}\n\n===Angerthas Moria===\nAccording to [[Tolkien's legendarium|Tolkien's ''legendarium'']], the [[Dwarf (Middle-earth)|Dwarves]] first came to know the runes of the Noldor at the beginning of the [[Second Age]]. The Dwarves «introduced a number of unsystematic changes in value, as well as certain new cirth».<ref name=\"Tolkien 1955\"/> They modified the previous system to suit the specific needs of their language, [[Khuzdul]]. The Dwarves spread their revised alphabet to [[Moria (Middle-earth)|Moria]], where it came to be known as '''''Angerthas Moria''''', and developed both carved and pen-written forms of these runes.\n\nMany cirth here represent sounds not occurring in Khuzdul (at least in published words of Khuzdul: of course, our corpus is very limited to judge the necessity or not, of these sounds). Here they are marked with a black star (<sup>★</sup>).\n\n<center>\n{| cellpadding=\"4\" cellspacing=\"0\"  class=\"wikitable\"\n|- align=\"center\"\n!'''''Certh'''''||'''Translit.'''||'''[[International Phonetic Alphabet|IPA]]'''\n!'''''Certh'''''||'''Translit.'''||'''[[International Phonetic Alphabet|IPA]]'''\n!'''''Certh'''''||'''Translit.'''||'''[[International Phonetic Alphabet|IPA]]'''\n!'''''Certh'''''||'''Translit.'''||'''[[International Phonetic Alphabet|IPA]]'''\n|- align=\"center\"\n|[[Image:Certh 1.svg|20px]]||p||{{IPA|/p/}}<sup>★</sup>\n|colspan=\"3\"|\n|[[Image:Certh 31.svg|20px]]||l||{{IPA|/l/}}\n|[[Image:Certh 46.svg|20px]]||e||{{IPA|/e/}}\n|- align=\"center\"\n|[[Image:Certh 2.svg|20px]]||b||{{IPA|/b/}}\n|[[Image:Certh 17.svg|20px]]||z||{{IPA|/z/}}\n|[[Image:Certh 32.svg|20px]]||lh||{{IPA|/ɬ/}}<sup>★</sup>\n|[[Image:Certh 47.svg|20px]]||ê||{{IPA|/eː/}}\n|- align=\"center\"\n|[[Image:Certh 3.svg|20px]]||f||{{IPA|/f/}}\n|[[Image:Certh 18.svg|20px]]||k||{{IPA|/k/}}\n|[[Image:Certh 33.svg|20px]]||nd||{{IPA|/nd/}}\n|[[Image:Certh 48.svg|20px]]||a||{{IPA|/a/}}\n|- align=\"center\"\n|[[Image:Certh 4.svg|20px]]||v||{{IPA|/v/}}<sup>★</sup>\n|[[Image:Certh 19.svg|20px]]||g||{{IPA|/ɡ/}}\n|[[Image:Certh 34.svg|20px]]||h{{ref|AMA|[A]}}||{{IPA|/h/}}\n|[[Image:Certh 49.svg|20px]]||â||{{IPA|/aː/}}\n|- align=\"center\"\n|[[Image:Certh 5.svg|20px]]||hw||{{IPA|/ʍ/}}<sup>★</sup>\n|[[Image:Certh 20.svg|20px]]||kh||{{IPA|/x/}}<sup>★</sup>\n|[[Image:Certh 35.svg|20px]]||''{{okina}}''||{{IPA|/ʔ/}}\n|[[Image:Certh 50.svg|20px]]||o||{{IPA|/o/}}\n|- align=\"center\"\n|[[Image:Certh 6.svg|20px]]||m||{{IPA|/m/}}\n|[[Image:Certh 21.svg|20px]]||gh||{{IPA|/ɣ/}}<sup>★</sup>\n|[[Image:Certh 36.svg|20px]]||ŋ||{{IPA|/ŋ/}}<sup>★</sup>\n|[[Image:Certh 51.svg|20px]]&nbsp;<span style=\"font-size:65%\">or</span>&nbsp;[[Image:Certh 51a.svg|20px]]||ô||{{IPA|/oː/}}\n|- align=\"center\"\n|[[Image:Certh 7.svg|20px]]||mb||{{IPA|/mb/}}\n|[[Image:Certh 22.svg|20px]]||n||{{IPA|/n/}}\n|[[Image:Certh 37.svg|20px]]||ng||{{IPA|/ŋɡ/}}\n|[[Image:Certh 52.svg|20px]]&nbsp;<span style=\"font-size:65%\">or</span>&nbsp;[[Image:Certh 52a.svg|20px]]||ö||{{IPA|/œ/}}<sup>★</sup>\n|- align=\"center\"\n|[[Image:Certh 8.svg|20px]]||t||{{IPA|/t/}}\n|[[Image:Certh 23.svg|20px]]||kw||{{IPA|/kʷ/}}<sup>★</sup>\n|[[Image:Certh 38.svg|20px]]&nbsp;<span style=\"font-size:65%\">or</span>&nbsp;[[Image:Certh 38a.svg|20px]]||nj||{{IPA|/ndʒ/}}<sup>★</sup>\n|[[Image:Certh 53.svg|20px]]||n||{{IPA|/n/}}\n|- align=\"center\"\n|[[Image:Certh 9.svg|20px]]||d||{{IPA|/d/}}\n|[[Image:Certh 24.svg|20px]]||gw||{{IPA|/ɡʷ/}}<sup>★</sup>\n|[[Image:Certh 39.svg|20px]]||i||{{IPA|/i/}}\n|[[Image:Certh 54.svg|20px]]||s||{{IPA|/s/}}\n|- align=\"center\"\n|[[Image:Certh 10.svg|20px]]||th||{{IPA|/θ/}}<sup>★</sup>\n|[[Image:Certh 25.svg|20px]]||khw||{{IPA|/xʷ/}}<sup>★</sup>\n|[[Image:Certh 40.svg|20px]]||y||{{IPA|/j/}}\n|[[Image:Certh 55.svg|20px]]&nbsp;<span style=\"font-size:65%\">or</span>&nbsp;[[Image:Certh 55a.svg|20px]]||{{ref|AMB|[B]}}||{{IPA|/ə/}}\n|- align=\"center\"\n|[[Image:Certh 11.svg|20px]]||dh||{{IPA|/ð/}}<sup>★</sup>\n|[[Image:Certh 26.svg|20px]]||ghw||{{IPA|/ɣʷ/}}<sup>★</sup>\n|[[Image:Certh 41.svg|20px]]||hy||{{IPA|/j̊/}} or {{IPA|/ç/}}<sup>★</sup>\n|[[Image:Certh 56.svg|20px]]&nbsp;<span style=\"font-size:65%\">or</span>&nbsp;[[Image:Certh 56a.svg|20px]]||{{ref|AMB|[B]}}||{{IPA|/ʌ/}}\n|- align=\"center\"\n|[[Image:Certh 12.svg|20px]]||r||{{IPA|/ʀ/}} or {{IPA|/ʁ/}}\n|[[Image:Certh 27.svg|20px]]||ngw||{{IPA|/ŋɡʷ/}}<sup>★</sup>\n|[[Image:Certh 42.svg|20px]]||u||{{IPA|/u/}}\n|colspan=\"3\"|\n|- align=\"center\"\n|[[Image:Certh 13.svg|20px]]||ch||{{IPA|/tʃ/}}<sup>★</sup>\n|[[Image:Certh 28.svg|20px]]||nw||{{IPA|/nʷ/}}<sup>★</sup>\n|[[Image:Certh 43.svg|20px]]||û||{{IPA|/uː/}}\n|colspan=\"3\"|\n|- align=\"center\"\n|colspan=\"3\"|\n|[[Image:Certh 29.svg|20px]]||j||{{IPA|/dʒ/}}<sup>★</sup>\n|[[Image:Certh 44.svg|20px]]||w||{{IPA|/w/}}<sup>★</sup>\n|[[Image:Certh 59.svg|20px]]||+h{{ref|AMC|[C]}}||{{IPA|/◌ʰ/}}\n|- align=\"center\"\n|[[Image:Certh 15.svg|20px]]||sh||{{IPA|/ʃ/}}\n|[[Image:Certh 30.svg|20px]]||zh||{{IPA|/ʒ/}}<sup>★</sup>\n|[[Image:Certh 45.svg|20px]]&nbsp;<span style=\"font-size:65%\">or</span>&nbsp;[[Image:Certh 45a.svg|20px]]||ü||{{IPA|/y/}}<sup>★</sup>\n|[[Image:Certh 60.svg|20px]]||&{{ref|AM&|[D]}}||\n|}\n</center>\n\n====Notes on Angerthas Moria====\n{|\n|- style=\"vertical-align: top;\"\n| align=\"right\" | A. || {{note|AMA|}}The Khuzdul language has two [[glottal consonant]]s: {{IPA|/h/}} and {{IPA|/ʔ/}}, the latter being «the [[glottal stop|glottal]] beginning of a word with an initial vowel».<ref name=\"Tolkien 1955\"/> Thus, in need of a reversible certh to represent these sounds, [[Image:Certh 54.svg|16px]] and [[Image:Certh 34.svg|16px]] were switched, giving the former the value {{IPA|/s/}} and using the latter for {{IPA|/h/}}, and its reversed counterpart [[Image:Certh 35.svg|16px]] for {{IPA|/ʔ/}}.\n|- style=\"vertical-align: top;\"\n| align=\"right\" | B. || {{note|AMB|}}These cirth were a halved form of [[Image:Certh 46.svg|16px]], used for vowels like those in the word {{angbr|b<u>u</u>tt<u>e</u>r}} {{IPAc-en|ˈ|b|ʌ|t|ə}}. Thus, [[Image:Certh 55.svg|16px]] represented a {{IPA|/ə/}} sound in unstressed syllables, while [[Image:Certh 56.svg|16px]] represented {{IPA|/ʌ/}}, a somehow similar sound, in stressed syllables. When weak they were reduced to a stroke without a stem ([[Image:Certh 55a.svg|16px]], [[Image:Certh 56a.svg|16px]]).<ref name=\"Tolkien 1955\"/>\n|- style=\"vertical-align: top;\"\n| align=\"right\" | C. || {{note|AMC|}}This sign denotes [[Aspirated consonant|aspiration]] in voiceless stops, occurring frequently in Khuzdul.<ref name=\"Tolkien 1955\"/>\n|- style=\"vertical-align: top;\"\n| align=\"right\" | D. || {{note|AM&|}}This certh is a [[scribal abbreviation]] used to represent a [[Conjunction (grammar)#Coordinating conjunctions|conjunction]], and is basically identical to the [[ampersand]] {{angbr|&}} used in [[Latin script]]. Although in Khuzdul it stands for ⟨ra⟩, it can assume different values in other languages (e.g., in English it would have the value {{angbr|and}}).\n|}\n[[File:Balin zg2.PNG|thumb|right|300px|Runes in the upper inscription of [[Balin (Middle-earth)|Balin]]'s tomb use ''Angerthas Moria'', reading left-to-right:<br/><center>Balin<br/>[[List_of_Middle-earth_Dwarves#Fundin|Fu[nd]in]]&#8203;ul<br/>Uzbad&#8203;[[Moria (Middle-earth)|Kʰazaddûm]]<nowiki/>u</center>]]\nIn ''Angerthas Moria'' the cirth [[Image:Certh 14.svg|16px]] {{IPA|/dʒ/}} and [[Image:Certh 16.svg|16px]] {{IPA|/ʒ/}} were dropped. Thus [[Image:Certh 29.svg|16px]] and [[Image:Certh 30.svg|16px]] were adopted for {{IPA|/dʒ/}} and {{IPA|/ʒ/}}, although they were used for {{IPA|/r/}} and {{IPA|/r̥/}} in Elvish languages. Subsequently, this script used the certh [[Image:Certh 12.svg|16px]] for {{IPA|/ʀ/ (or /ʁ/)}}, which had the sound {{IPA|/n/}} in the Elvish systems. Therefore, the certh [[Image:Certh 22.svg|16px]] (which was previously used for the sound {{IPA|/ŋ/}}, useless in Khuzdul) was adopted for the sound {{IPA|/n/}}. A totally new introduction was the certh [[Image:Certh 53.svg|16px]], used as an alternative, simplified and, maybe, weaker form of [[Image:Certh 22.svg|16px]]. Because of the visual relation of these two cirth, the certh [[Image:Certh 17.svg|16px]] was given the sound {{IPA|/z/}} to relate better with [[Image:Certh 54.svg|16px]] that, in this script, had the sound {{IPA|/s/}}.<ref name=\"Tolkien 1955\"/>\n\n===Angerthas Erebor===\nAt the beginning of the [[Third Age]] the Dwarves were driven out of Moria, and some migrated to [[Lonely Mountain|Erebor]]. As the Dwarves of Erebor would trade with the Men of the nearby towns of [[Dale (Middle-earth)|Dale]] and [[Esgaroth|Lake-town]], they needed a script to write in [[Westron]] (the ''[[lingua franca]]'' of Middle-earth, usually rendered in English by Tolkien in his works). The ''Angerthas Moria'' was adapted accordingly: some new cirth were added, while some were restored to their Elvish usage, thus creating the '''''Angerthas Erebor'''''.<ref name=\"Tolkien 1955\"/>\n\nWhile the ''Angerthas Moria'' was still used to write down Khuzdul, this new script was primarily used for Mannish languages. It is also the script used in the [[List of Middle-earth objects#Book of Mazarbul|Book of Mazarbul]].\n\n<center>\n{| cellpadding=\"4\" cellspacing=\"0\"  class=\"wikitable\"\n|- align=\"center\"\n!'''''Certh'''''||'''Translit.'''||'''[[International Phonetic Alphabet|IPA]]'''\n!'''''Certh'''''||'''Translit.'''||'''[[International Phonetic Alphabet|IPA]]'''\n!'''''Certh'''''||'''Translit.'''||'''[[International Phonetic Alphabet|IPA]]'''\n!'''''Certh'''''||'''Translit.'''||'''[[International Phonetic Alphabet|IPA]]'''\n|- align=\"center\"\n|[[Image:Certh 1.svg|20px]]||p||{{IPA|/p/}}\n|[[Image:Certh 16.svg|20px]]||zh||{{IPA|/ʒ/}}\n|[[Image:Certh 31.svg|20px]]||l||{{IPA|/l/}}\n|[[Image:Certh 46.svg|20px]]||e||{{IPA|/e/}}\n|- align=\"center\"\n|[[Image:Certh 2.svg|20px]]||b||{{IPA|/b/}}\n|[[Image:Certh 17.svg|20px]]||ks||{{IPA|/ks/}}\n|colspan=\"3\"|\n|colspan=\"3\"|\n|- align=\"center\"\n|[[Image:Certh 3.svg|20px]]||f||{{IPA|/f/}}\n|[[Image:Certh 18.svg|20px]]||k||{{IPA|/k/}}\n|[[Image:Certh 33.svg|20px]]||nd||{{IPA|/nd/}}\n|[[Image:Certh 48.svg|20px]]||a||{{IPA|/a/}}\n|- align=\"center\"\n|[[Image:Certh 4.svg|20px]]||v||{{IPA|/v/}}\n|[[Image:Certh 19.svg|20px]]||g||{{IPA|/ɡ/}}\n|[[Image:Certh 34.svg|20px]]||rowspan=\"2\"|s||rowspan=\"2\"|{{IPA|/s/}}\n|colspan=\"3\"|\n|- align=\"center\"\n|[[Image:Certh 5.svg|20px]]||hw||{{IPA|/ʍ/}}\n|[[Image:Certh 20.svg|20px]]||kh||{{IPA|/x/}}\n|[[Image:Certh 35.svg|20px]]\n|[[Image:Certh 50.svg|20px]]||o||{{IPA|/o/}}\n|- align=\"center\"\n|[[Image:Certh 6.svg|20px]]||m||{{IPA|/m/}}\n|[[Image:Certh 21.svg|20px]]||gh||{{IPA|/ɣ/}}\n|[[Image:Certh 36.svg|20px]]||ŋ||{{IPA|/ŋ/}}\n|colspan=\"3\"|\n|- align=\"center\"\n|[[Image:Certh 7.svg|20px]]||mb||{{IPA|/mb/}}\n|[[Image:Certh 22.svg|20px]]||n||{{IPA|/n/}}\n|[[Image:Certh 37.svg|20px]]||ng||{{IPA|/ŋɡ/}}\n|[[Image:Certh 52.svg|20px]]&nbsp;<span style=\"font-size:65%\">or</span>&nbsp;[[Image:Certh 52a.svg|20px]]||ö||{{IPA|/œ/}}\n|- align=\"center\"\n|[[Image:Certh 8.svg|20px]]||t||{{IPA|/t/}}\n|[[Image:Certh 23.svg|20px]]||kw||{{IPA|/kʷ/}}\n|colspan=\"3\"|\n|[[Image:Certh 53.svg|20px]]||n||{{IPA|/n/}}\n|- align=\"center\"\n|[[Image:Certh 9.svg|20px]]||d||{{IPA|/d/}}\n|[[Image:Certh 24.svg|20px]]||gw||{{IPA|/ɡʷ/}}\n|[[Image:Certh 39.svg|20px]]||i||{{IPA|/i/}}\n|[[Image:Certh 54.svg|20px]]||h||{{IPA|/h/}}\n|- align=\"center\"\n|[[Image:Certh 10.svg|20px]]||th||{{IPA|/θ/}}\n|[[Image:Certh 25.svg|20px]]||khw||{{IPA|/xʷ/}}\n|[[Image:Certh 40.svg|20px]]||y||{{IPA|/j/}}\n|[[Image:Certh 55.svg|20px]]&nbsp;<span style=\"font-size:65%\">or</span>&nbsp;[[Image:Certh 55a.svg|20px]]||||{{IPA|/ə/}}\n|- align=\"center\"\n|[[Image:Certh 11.svg|20px]]||dh||{{IPA|/ð/}}\n|[[Image:Certh 26.svg|20px]]||ghw||{{IPA|/ɣʷ/}}\n|[[Image:Certh 41.svg|20px]]||hy||{{IPA|/j̊/}} or {{IPA|/ç/}}\n|[[Image:Certh 56.svg|20px]]&nbsp;<span style=\"font-size:65%\">or</span>&nbsp;[[Image:Certh 56a.svg|20px]]||||{{IPA|/ʌ/}}\n|- align=\"center\"\n|[[Image:Certh 12.svg|20px]]||r||{{IPA|/r/}}\n|[[Image:Certh 27.svg|20px]]||ngw||{{IPA|/ŋɡʷ/}}\n|[[Image:Certh 42.svg|20px]]||u||{{IPA|/u/}}\n|[[Image:Certh 57.svg|20px]]||ps||{{IPA|/ps/}}\n|- align=\"center\"\n|[[Image:Certh 13.svg|20px]]||ch||{{IPA|/tʃ/}}\n|[[Image:Certh 28.svg|20px]]||nw||{{IPA|/nʷ/}}\n|[[Image:Certh 43.svg|20px]]||z||{{IPA|/z/}}\n|[[Image:Certh 58.svg|20px]]||ts||{{IPA|/ts/}}\n|- align=\"center\"\n|[[Image:Certh 14.svg|20px]]||j||{{IPA|/dʒ/}}\n|[[Image:Certh 29.svg|20px]]||g||{{IPA|/ɡ/}}\n|[[Image:Certh 44.svg|20px]]||w||{{IPA|/w/}}\n|[[Image:Certh 59.svg|20px]]||+h||{{IPA|/◌ʰ/}}\n|- align=\"center\"\n|[[Image:Certh 15.svg|20px]]||sh||{{IPA|/ʃ/}}\n|[[Image:Certh 30.svg|20px]]||gh||{{IPA|/ɣ/}}\n|[[Image:Certh 45.svg|20px]]&nbsp;<span style=\"font-size:65%\">or</span>&nbsp;[[Image:Certh 45a.svg|20px]]||ü||{{IPA|/y/}}\n|[[Image:Certh 60.svg|20px]]||&||\n|}\n</center>\n\nAngerthas Erebor also features [[combining diacritic]]s:\n*a [[circumflex]] [[Image:Certh Circumflex.png|16px]] used to denote [[gemination|long consonants]];\n*a [[macron below]] [[Image:Certh Macron below.png|16px]] to indicate a [[long vowel]] sound;\n*an [[dot (diacritic)#Underdot|underdot]] [[Image:Certh Underdot.png|16px]] to mark cirth used as [[numerical digits|numerals]]. As a matter of fact, in the [[List of Middle-earth objects#Book of Mazarbul|Book of Mazarbul]] some cirth are used as numerals: [[Image:Certh 39.svg|16px]] for 1, [[Image:Certh 50.svg|16px]] for 2, [[Image:Certh 52.svg|16px]] for 3, [[Image:Gondolin rune b.svg|16px]] for 4, [[Image:Certh 22.svg|16px]] for 5.\n\n[[File:Balin sp2.PNG|thumb|right|400px|The bottom inscription of [[Balin (Middle-earth)|Balin]]'s tomb is written in English using the ''Angerthas Erebor''. It reads left-to-right: «Balin sʌn ov Fu[nd]in lord ov Moria»]]\nThe ''Angerthas Erebor'' is used twice in ''[[The Lord of the Rings]]'' to write in English:\n# in the upper inscription of the title page, where it reads «''[dh]ə&#183;&#8203;lord&#183;&#8203;ov&#183;&#8203;[dh]ə&#183;&#8203;riŋs&#183;&#8203;translatᵊd&#183;&#8203;from&#183;&#8203;[dh]ə&#183;&#8203;[[Red Book of Westmarch|red&#183;&#8203;b[oo]k]]''&#8230;» (the sentence follows in the bottom inscription, written in Tengwar: «&#8230;''of Westmarch by John Ronald Reuel Tolkien. Herein is set forth/ the history of the War of the Ring and the Return of the King as seen by the Hobbits.''»);\n# in the bottom inscription of [[Balin (Middle-earth)|Balin]]'s tomb—being the translation of the upper inscription, which is written in Khuzdul using ''Angerthas Moria''.\n\nThe Book of Mazarbul shows some additional cirth used in ''Angerthas Erebor'': one for a double {{angbr|l}} [[Typographic ligature|ligature]], one for the [[Article (grammar)#Definite article|definite article]], and six for the representation of the same number of English [[diphthong]]s:\n\n<center>\n{| cellpadding=\"4\" cellspacing=\"0\"  class=\"wikitable\"\n|- align=\"center\"\n!'''''Certh'''''||'''English spelling'''\n|- align=\"center\"\n|[[Image:Certh LL.svg|20px]]{{ref|AE|∗}}||{{angbr|ll}}\n|- align=\"center\"\n|[[Image:Certh Article.svg|20px]]{{ref|AE|∗}}||{{angbr|the}}{{ref|the|[A]}}\n|- align=\"center\"\n|[[Image:Certh AI.svg|20px]]{{ref|AE|∗}}||{{angbr|ai}}, {{angbr|ay}}\n|- align=\"center\"\n|[[Image:Certh AU.svg|20px]]{{ref|AE|∗}}||{{angbr|au}}, {{angbr|aw}}\n|- align=\"center\"\n|[[Image:Certh EA.svg|20px]]{{ref|AE|∗}}||{{angbr|ea}}\n|- align=\"center\"\n|[[Image:Certh 47.svg|20px]]||{{angbr|ee}}\n|- align=\"center\"\n|[[Image:Certh 38a.svg|20px]]||{{angbr|eu}}, {{angbr|ew}}\n|- align=\"center\"\n|[[Image:Certh OA.svg|20px]]{{ref|AE|∗}}||{{angbr|oa}}\n|- align=\"center\"\n|[[Image:Certh 51.svg|20px]]||{{angbr|oo}}\n|- align=\"center\"\n|[[Image:Certh 38.svg|20px]]||{{angbr|ou}}, {{angbr|ow}}\n|}\n</center>\n\n====Notes on Angerthas Erebor====\n{|\n|- style=\"vertical-align: top;\"\n| align=\"right\" | A. || {{note|the|}}This certh is a [[scribal abbreviation]] used to represent the definite article. Although in English it stands for {{angbr|the}}, it can assume different values according to the used language.\n|- style=\"vertical-align: top;\"\n| align=\"right\" | ∗. || {{note|AE|}}The cirth marked with an asterisk are unique of ''Angerthas Erebor''.\n|}\n\n==Other runic systems of Middle-earth==\nThe Cirth is not the only runic writing system devised by Tolkien for Middle-earth. In fact, he invented a great number of runic alphabets, of which only a few others have been published. Most of these runic scripts were published in the \"Appendix on Runes\" in ''[[The History of Middle-earth]]'', vol. VII, ''[[The Treason of Isengard]]'', edited by [[Christopher Tolkien]].\n\n===Runes from ''The Hobbit''===\n<!-- [[File:Hobbit cover.JPG|thumb|right|150px|Runes around the edges of the cover of ''The Hobbit'' are a transliteration of English, giving information on the book.]] -->\n\nAccording to Tolkien, those used in ''[[The Hobbit]]'' are a form of «[[Fuþorc|our ancient runes]]» deployed in the book to transliterate the actual Dwarvish runes.<ref>{{cite book |title=[[The Hobbit]]|first=J.R.R. |last=Tolkien |authorlink=J.R.R. Tolkien |publisher=London: George Allen & Unwin. |year=1937}}</ref> They can be interpreted as an attempt made by Tolkien to adapt the [[Fuþorc]] (i.e., the [[Old English]] runic alphabet) to the [[Modern English]] language.\n\nThese runes are basically the same found in Fuþorc, but their sound may change according to their position, just as the [[Latin script]] letters do: the writing mode adopted by Tolkien for these runes is mainly orthographic.\n\nThis system has one rune for each letter, regardless of pronunciation. For example, the rune [[Image:Certh 13.svg|16px]] {{angbr|c}} can sound either {{IPAc-en|k}} (in the word {{angbr|<u>c</u>at}}) or {{IPAc-en|s}} (in the word {{angbr|<u>c</u>ellar}}) or even {{IPAc-en|ʃ}} (in the word {{angbr|o<u>c</u>ean}}) and {{IPAc-en|tʃ}} (in the digraph [[Image:Certh 13.svg|16px]][[Image:Certh 47.svg|16px]] {{angbr|ch}}).\n\nA few sounds are instead written with the same rune, regardless of the way it is spelled with the Latin script. For example, the sound {{IPAc-en|ɔː}} is always written with the rune [[Image:Certh 24.svg|16px]] either if in English it is written {{angbr|o}} as in {{angbr|n<u>o</u>rth}}, {{angbr|a}} as in {{angbr|f<u>a</u>ll}}, or {{angbr|oo}} as in {{angbr|d<u>oo</u>r}}. The letters that are subject to this phonemic spelling are {{angbr|a}} and {{angbr|o}}.\n\nIn addition, there are also some runes which stand for particular English digraphs and diphthongs.\n\nHere the runes used in ''The Hobbit'' are represented along with their corresponding English grapheme and Fuþorc counterpart:\n\n<center>\n{| cellpadding=\"4\" cellspacing=\"0\"  class=\"wikitable\"\n|- align=\"center\"\n!'''Rune'''||'''Fuþorc'''||'''English grapheme'''\n!'''Rune'''||'''Fuþorc'''||'''English grapheme'''\n|- align=\"center\"\n|[[Image:Tolkien's Futhorc A.svg|20px]]||{{script|Runr|ᚪ}} ||rowspan=2|''phonemic''{{ref|ph|[table below]}}\n|[[Image:Certh 28.svg|20px]]||{{script|Runr|ᛈ}}||{{angbr|p}}\n|- align=\"center\"\n|[[Image:Certh 9.svg|20px]]||{{script|Runr|ᚫ}}\n|[[Image:Certh 2.svg|20px]]||{{script|Runr|ᚱ}}||{{angbr|r}}\n|- align=\"center\"\n|[[Image:Certh 6.svg|20px]]||{{script|Runr|ᛒ}}||{{angbr|b}}\n|[[Image:Certh 40.svg|20px]]||{{script|Runr|ᛋ}}||{{angbr|s}}\n|- align=\"center\"\n|[[Image:Certh 13.svg|20px]]||{{script|Runr|ᚳ}}||{{angbr|c}}\n|[[Image:Certh 12.svg|20px]]||{{script|Runr|ᛏ}}||{{angbr|t}}\n|- align=\"center\"\n|[[Image:Certh 38.svg|20px]]||{{script|Runr|ᛞ}}||{{angbr|d}}\n|[[Image:Certh 48.svg|20px]]||{{script|Runr|ᚢ}}||{{angbr|u}}, {{angbr|v}}\n|- align=\"center\"\n|[[Image:Tolkien's Futhorc E.svg|20px]]||{{script|Runr|ᛖ}}||{{angbr|e}}\n|[[Image:Certh 1.svg|20px]]||{{script|Runr|ᚹ}}||{{angbr|w}}\n|- align=\"center\"\n|[[Image:Tolkien's Futhorc F.svg|20px]]||{{script|Runr|ᚠ}}||{{angbr|f}}\n|[[Image:Certh 22.svg|20px]]||{{script|Runr|ᛉ}}||{{angbr|x}}\n|- align=\"center\"\n|[[Image:Certh 36.svg|20px]]||{{script|Runr|ᚷ}}||{{angbr|g}}\n|[[Image:Certh AU.svg|20px]]||{{script|Runr|ᚣ}}||{{angbr|y}}\n|- align=\"center\"\n|[[Image:Certh 47.svg|20px]]||{{script|Runr|ᚻ}}||{{angbr|h}}\n|[[Image:Certh 17.svg|20px]]||{{script|Runr|ᛣ}}||{{angbr|z}}\n|- align=\"center\"\n|[[Image:Certh 39.svg|20px]]||{{script|Runr|ᛁ}}||{{angbr|i}}, {{angbr|j}}\n|[[Image:Certh 57.svg|20px]]||{{script|Runr|ᚦ}}||{{angbr|th}}\n|- align=\"center\"\n|[[Image:Tolkien's Futhorc K.svg|20px]]{{ref|H|∗}}||||{{angbr|k}}\n|[[Image:Certh 27.svg|20px]]||{{script|Runr|ᛠ}}||{{angbr|ea}}\n|- align=\"center\"\n|[[Image:Certh 8.svg|20px]]||{{script|Runr|ᛚ}}||{{angbr|l}}\n|[[Image:Tolkien's Futhorc ST.svg|20px]]||{{script|Runr|ᛥ}}||{{angbr|st}}\n|- align=\"center\"\n|[[Image:Tolkien's Futhorc M.svg|20px]]||{{script|Runr|ᛗ}}||{{angbr|m}}\n|[[Image:Certh 42.svg|20px]]||{{script|Runr|ᛟ}}||{{angbr|ee}}\n|- align=\"center\"\n|[[Image:Certh 32.svg|20px]]||{{script|Runr|ᚾ}}||{{angbr|n}}\n|[[Image:Certh 43.svg|20px]]||{{script|Runr|ᛝ}}||{{angbr|ng}}\n|- align=\"center\"\n|[[Image:Certh 24.svg|20px]]||{{script|Runr|ᚩ}}||''phonemic''{{ref|ph|[table below]}}\n|[[Image:Tolkien's Futhorc EO.svg|20px]]||{{script|Runr|ᛇ}}||{{angbr|eo}}\n|}\n</center>\n\nTwo other runes, not attested in ''The Hobbit'', were added by Tolkien in order to represent additional English graphemes:\n\n<center>\n{| class=\"wikitable\"\n|-\n! '''Rune'''\n| <center>[[Image:Certh 5.svg|20px]]{{ref|H|∗}}</center> || <center>[[Image:Certh 41.svg|20px]]{{ref|H|∗}}</center>\n|-\n! '''English grapheme'''\n| <center>{{angbr|oo}}</center> || <center>{{angbr|sh}}</center>\n|}\n</center>\n\n====Notes====\n*{{note|ph|}}This table summarises the transcription of {{angbr|a}} and {{angbr|o}} in runes:<ref>{{cite web |url= http://www.forodrim.org/daeron/runes-eng.pdf |title= Tolkien English Runes|last= Lindberg|first= Per|date= 2016-11-27|website= forodrim.org|access-date= 2019-03-27}}</ref>\n<center>\n{| class=\"wikitable\"\n|- align=\"center\"\n! '''English grapheme'''\n! '''Sound in [[International Phonetic Alphabet|IPA]]'''\n!'''Rune'''\n|- align=\"center\"\n| rowspan=\"3\" | {{angbr|a}}\n| {{IPAc-en|æ}}\n| [[Image:Certh 9.svg|20px]]\n|- align=\"center\"\n| every other sound\n| [[Image:Tolkien's Futhorc A.svg|20px]]\n|- align=\"center\"\n| {{IPAc-en|ɔː}}\n| rowspan=3 | [[Image:Certh 24.svg|20px]]\n|- align=\"center\"\n| {{angbr|o}}\n| every sound\n|- align=\"center\"\n| rowspan=\"2\" | {{angbr|oo}}\n| {{IPAc-en|ɔː}}\n|- align=\"center\"\n| every other sound\n| [[Image:Certh 5.svg|21px]]\n|}\n</center>\n*Tolkien always wrote the [[Pronunciation of English ⟨wh⟩|English digraph {{angbr|wh}}]] (representing the sound {{IPA-endia|ʍ|}}, or {{IPAc-en|hw}}, like in {{angbr|<u>wh</u>ine}}) as [[Image:Certh 47.svg|16px]][[Image:Certh 1.svg|16px]] {{angbr|hw}}.\n*There is no rune to transliterate {{angbr|q}}: the digraph {{angbr|qu}} (representing the sound {{IPA-endia|kʷw|}}, like in {{angbr|<u>qu</u>een}}) is always written in runes as [[Image:Certh 13.svg|16px]][[Image:Certh 1.svg|16px]] {{angbr|cw}}.\n*∗ {{note|H|}} The three runes marked with an asterisk are not attested in real-life Fuþorc. They are completely new characters invented by Tolkien, and have also been introduced in Unicode 7.0 ([[#Unicode|see below]]).\n\n===Gondolinic Runes===\nNot all the runes mentioned in ''The Hobbit'' are Dwarf-runes. The swords found in the [[Troll (Middle-earth)|Troll]]s' cave (which were from the ancient kingdom of [[Gondolin]]) bore runes that [[Gandalf]] allegedly could not read. In fact, the swords [[List of Middle-earth weapons and armour#Glamdring|Glamdring]] and [[List of Middle-earth weapons and armour#Orcrist|Orcrist]], forged in Gondolin,  bore a type of letters known as '''''Gondolinic runes'''''. They seem to have been obsoleted and forgotten by the [[Third Age]], and this is supported by the fact that Tolkien writes that only [[Elrond]] could still read the inscriptions of the swords.<br>\nTolkien devised this runic alphabet in a very early stage of his shaping of Middle-earth. Nevertheless, they are known to us from a slip of paper written by J.R.R. Tolkien, a photocopy of which [[Christopher Tolkien]] sent to Paul Nolan Hyde in February 1992. Hyde then published it, together with an extensive analysis, in the 1992 Summer issue of [[Mythlore]], no. 69.<ref>{{cite journal |last1= Hyde|first1= Paul Nolan|date= July 1992|title= Gondolinic Runes|url= http://www.mythsoc.org/mythlore/mythlore-69.htm|journal= Mythlore, no. 69|volume= 18|issue= 3|pages= }}</ref><br />\nThe system provides sounds not found in any of the known Elven languages of the First Age, but perhaps it was designed for a variety of languages. However, the consonants seem to be, more or less, the same found in [[Welsh phonology]], a theory supported by the fact that Tolkien was heavily influenced by [[Welsh language|Welsh]] when creating Elven languages.<ref>{{cite news |author= <!--Staff writer(s); no by-line.-->|date= 2011-05-21|title= Study explores JRR Tolkien's Welsh influences|url= https://www.bbc.com/news/uk-wales-south-east-wales-13472344|work= BBC|access-date= 2019-03-27}}</ref>\n\n<center>\n{|class=\"wikitable\"\n|- align=\"center\"\n|+ Consonants\n!rowspan=2|\n!colspan=3|<small>[[Labial consonant|Labial]]</small>\n!colspan=9|<small>[[Dental consonant|Dentals]]</small>\n!colspan=3|<small>[[Palatal consonant|Palatal]]</small>\n!colspan=3|<small>[[Dorsal consonant|Dorsal]]</small>\n!colspan=3|<small>[[Glottal consonant|Glottal]]</small>\n|- align=\"center\"\n!colspan=2|<small>'''Rune'''</small>||<small>'''[[International Phonetic Alphabet|IPA]]'''</small>\n!colspan=2|<small>'''Rune'''</small>||<small>'''[[International Phonetic Alphabet|IPA]]'''</small>\n!colspan=2|<small>'''Rune'''</small>||<small>'''[[International Phonetic Alphabet|IPA]]'''</small>\n!colspan=2|<small>'''Rune'''</small>||<small>'''[[International Phonetic Alphabet|IPA]]'''</small>\n!colspan=2|<small>'''Rune'''</small>||<small>'''[[International Phonetic Alphabet|IPA]]'''</small>\n!colspan=2|<small>'''Rune'''</small>||<small>'''[[International Phonetic Alphabet|IPA]]'''</small>\n!colspan=2|<small>'''Rune'''</small>||<small>'''[[International Phonetic Alphabet|IPA]]'''</small>\n|- align=\"center\"\n! rowspan=2|<small>[[Stop consonant|Plosive]]</small>\n|[[Image:Gondolin rune p.svg|20px]]||p||{{IPA|/p/}}\n|colspan=3|\n|[[Image:Certh 8.svg|20px]]||t||{{IPA|/t/}}\n|colspan=3|\n|colspan=3|\n|[[Image:Certh 57.svg|20px]]||k (c)||{{IPA|/k/}}\n|colspan=3|\n|- align=\"center\"\n|[[Image:Gondolin rune b.svg|20px]][[Image:Gondolin rune b2.svg|20px]]||b||{{IPA|/b/}}\n|colspan=3|\n|[[Image:Certh 12.svg|20px]]||d||{{IPA|/d/}}\n|colspan=3|\n|colspan=3|\n|[[Image:Certh 6.svg|20px]]||g||{{IPA|/ɡ/}}\n|colspan=3|\n|- align=\"center\"\n! rowspan=2|<small>[[fricative consonant|Fricative]]</small>\n|[[Image:Gondolin rune f.svg|20px]]||f||{{IPA|/f/}}\n|[[Image:Certh 9.svg|20px]]||þ||{{IPA|/θ/}}\n|[[Image:Certh 35.svg|20px]]||s||{{IPA|/s/}}\n|[[Image:Gondolin rune sh.svg|20px]]||š||{{IPA|/ʃ/}}\n|colspan=3|\n|[[Image:Certh 40.svg|20px]]||χ||{{IPA|/x/}}\n|[[Image:Certh 59.svg|20px]]||h||{{IPA|/h/}}\n|- align=\"center\"\n|[[Image:Certh 27.svg|20px]][[Image:Certh 22.svg|20px]]||v||{{IPA|/v/}}\n|[[Image:Certh 19.svg|20px]]||ð||{{IPA|/ð/}}\n|[[Image:Gondolin rune z.svg|20px]]||z||{{IPA|/z/}}\n|[[Image:Gondolin rune zh.svg|20px]]||ž||{{IPA|/ʒ/}}\n|colspan=3|\n|colspan=3|\n|colspan=3|\n|- align=\"center\"\n! rowspan=2|<small>[[affricate consonant|Affricate]]</small>\n|colspan=3|\n|colspan=3|\n|colspan=3|\n|[[Image:Certh 60.svg|20px]]||tš (ch)||{{IPA|/t͡ʃ/}}\n|colspan=3|\n|colspan=3|\n|colspan=3|\n|- align=\"center\"\n|colspan=3|\n|colspan=3|\n|colspan=3|\n|[[Image:Gondolin rune j.svg|20px]]||dž (j)||{{IPA|/d͡ʒ/}}\n|colspan=3|\n|colspan=3|\n|colspan=3|\n|- align=\"center\"\n! rowspan=2|<small>[[nasal consonant|Nasal]]</small>\n|[[Image:Certh 43.svg|20px]]||m||{{IPA|/m/}}\n|colspan=3|\n|[[Image:Certh 54.svg|20px]]||n||{{IPA|/n/}}\n|colspan=3|\n|colspan=3|\n|[[Image:Certh 2.svg|20px]]||ŋ||{{IPA|/ŋ/}}\n|colspan=3|\n|- align=\"center\"\n|([[Image:Gondolin rune mh.svg|20px]]||mh)||{{IPA|/m̥/}}\n|colspan=3|\n|[[Image:Certh 28.svg|20px]]||χ̃||{{IPA|/n̥/}}?\n|colspan=3|\n|colspan=3|\n|([[Image:Gondolin rune ŋh.svg|20px]][[Image:Gondolin rune ŋh2.svg|20px]]||ŋh)||{{IPA|/ŋ̊/}}\n|colspan=3|\n|- align=\"center\"\n! rowspan=2|<small>[[Trill consonant|Trill]]</small>\n|colspan=3|\n|colspan=3|\n|[[Image:Certh 29.svg|20px]]||r||{{IPA|/r/}}\n|colspan=3|\n|colspan=3|\n|colspan=3|\n|colspan=3|\n|- align=\"center\"\n|colspan=3|\n|colspan=3|\n|[[Image:Certh 33.svg|20px]]||rh||{{IPA|/r̥/}}\n|colspan=3|\n|colspan=3|\n|colspan=3|\n|colspan=3|\n|- align=\"center\"\n! rowspan=2|<small>[[Lateral consonant|Lateral]]</small>\n|colspan=3|\n|colspan=3|\n|[[Image:Certh 36.svg|20px]]||l||{{IPA|/l/}}\n|colspan=3|\n|colspan=3|\n|colspan=3|\n|colspan=3|\n|- align=\"center\"\n|colspan=3|\n|colspan=3|\n|[[Image:Gondolin rune lh.svg|20px]][[Image:Gondolin rune lh2.svg|20px]][[Image:Gondolin rune lh3.svg|20px]]||lh||{{IPA|/ɬ/}}\n|colspan=3|\n|colspan=3|\n|colspan=3|\n|colspan=3|\n|- align=\"center\"\n! rowspan=2|<small>[[Approximant consonant|Approximant]]</small>\n|colspan=3|\n|colspan=3|\n|colspan=3|\n|colspan=3|\n|[[Image:Certh 13.svg|20px]][[Image:Tolkien's Futhorc K.svg|20px]][[Image:Gondolin rune y3 (consonant).svg|20px]]||j (i̯)||{{IPA|/j/}}\n|[[Image:Certh 42.svg|20px]]||w (u̯)||{{IPA|/w/}}\n|colspan=3|\n|- align=\"center\"\n|colspan=3|\n|colspan=3|\n|colspan=3|\n|colspan=3|\n|colspan=3|\n|[[Image:Certh 37.svg|20px]]||ƕ||{{IPA|/ʍ/}}\n|colspan=3|\n|}\n</center>\n\n<center>\n{|class=\"wikitable\"\n|+ Vowels\n|- align=\"center\"\n!<small>'''Rune'''</small>||<small>'''Translit.'''</small>||<small>'''[[International Phonetic Alphabet|IPA]]'''</small>\n!<small>'''Rune'''</small>||<small>'''Translit.'''</small>||<small>'''[[International Phonetic Alphabet|IPA]]'''</small>\n!<small>'''Rune'''</small>||<small>'''Translit.'''</small>||<small>'''[[International Phonetic Alphabet|IPA]]'''</small>\n!<small>'''Rune'''</small>||<small>'''Translit.'''</small>||<small>'''[[International Phonetic Alphabet|IPA]]'''</small>\n!<small>'''Rune'''</small>||<small>'''Translit.'''</small>||<small>'''[[International Phonetic Alphabet|IPA]]'''</small>\n|- align=\"center\"\n|[[Image:Gondolin rune a.svg|20px]]||a||{{IPA|/a/}}\n|[[Image:Gondolin rune e.svg|20px]]||e||{{IPA|/ɛ/}}\n|[[Image:Certh 39.svg|20px]]||i||{{IPA|/i/}}\n|[[Image:Certh 48.svg|20px]]||o||{{IPA|/ɔ/}}\n|[[Image:Gondolin rune u.svg|20px]]||u||{{IPA|/u/}}\n|- align=\"center\"\n|[[Image:Certh 38.svg|20px]]||ā||{{IPA|/aː/}}\n|[[Image:Gondolin rune e-.svg|20px]][[Image:Certh 38a.svg|20px]]||ē||{{IPA|/eː/}}\n|[[Image:Certh 31.svg|20px]]||ī||{{IPA|/iː/}}\n|[[Image:Certh 49.svg|20px]] [[Image:Gondolin rune o-2.svg|20px]]||ō||{{IPA|/oː/}}\n|[[Image:Gondolin rune u-.svg|20px]][[Image:Gondolin rune u-2.svg|20px]]||ū||{{IPA|/uː/}}\n|- align=\"center\"\n|[[Image:Gondolin rune æ.svg|20px]][[Image:Gondolin rune æ2.svg|20px]]||æ||{{IPA|/æ/}}\n|colspan=\"3\"|\n|colspan=\"3\"|\n|[[Image:Certh AU.svg|20px]]||œ||{{IPA|/œ/}}\n|[[Image:Gondolin rune y.svg|20px]][[Image:Certh 44.svg|20px]]||y||{{IPA|/y/}}\n|- align=\"center\"\n|[[Image:Gondolin rune æ-.svg|20px]][[Image:Gondolin rune æ-2.svg|20px]]||ǣ||{{IPA|/æː/}}\n|colspan=\"3\"|\n|colspan=\"3\"|\n|[[Image:Gondolin rune œ-.svg|20px]]||œ̄||{{IPA|/œː/}}\n|[[Image:Gondolin rune y-.svg|20px]][[Image:Certh 5.svg|20px]][[Image:Gondolin rune y-3.svg|20px]][[Image:Gondolin rune y-4.svg|20px]]||ȳ||{{IPA|/yː/}}\n|}\n</center>\n\n==Encoding schemes==\n===Unicode===\nEquivalents for some (but not all) cirth can be found in the [[Runic (Unicode block)|Runic block]] of Unicode.\n\nThree runic letters invented by Tolkien were added to the block in June 2014, with the release of Unicode 7.0:\n<div style=\"font-family: Quivira, Everson Mono;\">\n*{{unichar|16F1|RUNIC LETTER K}}\n*{{unichar|16F2|RUNIC LETTER SH}}\n*{{unichar|16F3|RUNIC LETTER OO}}\n</div>\n\nA formal [[Unicode]] proposal to encode Cirth as a separate script was made in September 1997 by [[Michael Everson]].<ref>{{cite web|url= http://std.dkuug.dk/JTC1/SC2/WG2/docs/n1642/n1642.htm|title=N1642: Proposal to encode Cirth in Plane 1 of ISO/IEC 10646-2 | first = Michael | last = Everson | publisher=Working Group Document, ISO/IEC JTC1/SC2/WG2 and UTC|date=1997-09-18|accessdate=2015-08-08}}</ref>\nNo action was taken by the Unicode Technical Committee (UTC) but Cirth appears in the Roadmap to the SMP.<ref name=\"roadmapsmp\">{{cite web|url=https://www.unicode.org/roadmaps/smp/ |title=Roadmap to the SMP |publisher=Unicode.org |date=2015-06-03 |accessdate=2015-08-08}}</ref>\n\n===ConScript Unicode Registry===\n{{Infobox Unicode block\n|rangestart = E080\n|rangeend = E0FF\n|assigned = 109\n|script1 = Artificial Scripts\n|alphabets = Cirth\n|sources = [[ConScript Unicode Registry|CSUR]]\n|note = Part of the Private-Use Area, font conflicts possible\n}}\n\nUnicode [[Private Use Areas|Private Use Area]] layouts for Cirth are defined at the '''[[ConScript Unicode Registry]]''' (CSUR)<ref>{{cite web|url=http://www.evertype.com/standards/csur/ |title=ConScript Unicode Registry |publisher=Evertype.com | accessdate=2015-08-08}}</ref> and the '''Under-ConScript Unicode Registry''' (UCSUR).<ref>{{cite web|url=http://www.kreativekorp.com/ucsur/ |title=Under-ConScript Unicode Registry | accessdate=2015-08-08}}</ref>\n\nTwo different layouts are defined by the CSUR/UCSUR:\n* 1997-11-03 proposal<ref>{{cite web|url=http://www.evertype.com/standards/csur/cirth.html |title=Cirth: U+E080&ndash;U+E0FF|publisher=ConScript Unicode Registry|date=1997-11-03|accessdate=2015-08-08}}</ref> implemented by fonts like [[GNU Unifont]]<ref>{{cite web|url=http://unifoundry.com/unifont.html|title=GNU Unifont|publisher=Unifoundry.com |accessdate=2015-07-24}}</ref> and Code2000.\n* 2000-04-22 discussion paper<ref>{{cite web|url=http://www.evertype.com/standards/iso10646/pdf/cirth.pdf |title=X.X Cirth 1xx00–1xx7F|first=Michael|last=Everson|date=2000-04-22|accessdate=2015-08-08}}</ref><ref>{{cite web|url=http://www.kreativekorp.com/ucsur/charts/PDF/UE080.pdf|title=Cirth, Range: E080–E0FF|date=2008-04-14|publisher=Under-ConScript Unicode Registry|accessdate=2015-08-08}}</ref> implemented by fonts like Constructium and Fairfax.\n\nWithout proper rendering support, you may see [[Specials (Unicode block)#Replacement character|question marks, boxes, or other symbols]] below instead of Cirth.\n\n{{CSUR chart Cirth}}\n\n==References==\n{{Reflist}}\n\n{{Languages of Middle-earth}}\n{{Middle-earth}}\n{{Constructed languages}}\n\n[[Category:Middle-earth writing systems]]\n[[Category:Alphabets]]\n[[Category:Runology]]\n[[Category:Scripts not encoded in Unicode]]"
    },
    {
      "title": "Clear Script",
      "url": "https://en.wikipedia.org/wiki/Clear_Script",
      "text": "{{Infobox Writing system\n|name=Clear Script\n|altname=Oirat alphabet\n|languages=[[Oirat language|Oirat]]<br/>[[Sanskrit]]<br/>[[Tibetic languages|Tibetic]]\n|type=[[Alphabet]]\n |fam1=[[Proto-Sinaitic script]]\n |fam2=[[Phoenician alphabet]]\n |fam3=[[Aramaic alphabet]]\n |fam4=[[Syriac alphabet]]\n |fam5=[[Sogdian alphabet]]\n |fam6=[[Mongolian script]]\n|creator=[[Zaya Pandita]]\n|sisters=[[Manchu alphabet]]<br/>[[Vagindra script]]\n|children=\n|time=ca. 1648 &ndash; today\n|unicode=[https://www.unicode.org/charts/PDF/U1800.pdf U+1800 – U+18AF]\n|iso15924=Mong\n|sample=Smp kalmyk.gif\n|imagesize=250px\n}}\n[[File:Prijutnoe1.jpg|thumb|right|250px|A border sign in Clear Script (Priyutnensky District, Kalmykia)]]\n'''Clear Script''' ({{lang-xal|{{MongolUnicode|ᡐᡆᡑᡆ<br />ᡋᡅᡔᡅᡎ}}, Тод бичг}}, {{IPA-all|tot bit͡ʃ(ə)k|}}, ''tod biçg''; {{lang-mn|Тод бичиг, {{MongolUnicode|ᠲᠣᠳᠣ<br />ᠪᠢᠴᠢᠭ}}}} ''tod bichig'', {{IPA-mn|tɔt bit͡ʃək|}}, {{lang-bxr|Тодо бэшэг}}, ''Todo besheg'' ({{IPA-all|tɔdɔ bɛʃək|}}), or just '''todo''') is an alphabet created in 1648 by the [[Oirats|Oirat]] [[bhikkhu|Buddhist monk]] [[Zaya Pandita]] for the [[Oirat language]].<ref>N. Yakhantova, [http://www.nlc.gov.cn/newhxjy/wjls/wjqcsy/wjd17q/201011/P020101123700814793877.pdf The Mongolian and Oirat Translations of the Sutra of Golden Light], 2006</ref><ref name= \"Kara\">Kara, György. ''Books of the Mongolian Nomads''. Bloomington: Indiana University, 2005.</ref><ref name=\"Daniels\">Eds. Daniels, Peter T. and William Bright. ''The World's Writing Systems''. New York: Oxford University Press, 1996</ref> It was developed on the basis of the [[Mongolian script]] with the goal of distinguishing all sounds in the spoken language, and to make it easier to transcribe [[Sanskrit]] and the [[Tibetic languages]].\n\n==History==\nClear Script is a Mongolian script, whose obvious closest forebear is vertical Mongolian. This Mongolian script was derived from the [[Old Uyghur alphabet]], which itself was descended from the [[Aramaic alphabet]].<ref name=\"Gnana\">Gnanadesikan, Amalia. ''The Writing Revolution''. West Sussex: Wiley-Blackwell, 2009.</ref> Aramaic is an [[abjad]], an alphabet that has no symbols for vowels, and Clear Script is the first in this line of descendants to develop a full system of symbols for all the vowel sounds.<ref name= \"Gnana\"/>\n\n===Formation===\nAs mentioned above, Clear Script was developed as a better way to write Mongolian, specifically of the Western Mongolian groups of the Oirats and Kalmyks.<ref name=\"Daniels\" /> The practicality of Clear Script lies in the fact that it was supremely created in order to dissolve any ambiguities that might appear when one attempts to write down a language. Not only were vowels assigned symbols, but all existing symbols were clarified. All of the 'old' symbols, those that did not change from the previously used script, were assigned a fixed meaning, based mostly on their Uyghur ancestors.<ref name=\"Kara\" />  New symbols and [[diacritic]]s were added to show vowels and vowel lengths, as well as distinguish between voiced and unvoiced consonants.<ref name=\"Daniels\" /> There were even some marks enabling distinctions such as between ''ši'' and ''si'' which are unimportant for words written in the Oirat language but are useful for the transcription of foreign words and names.<ref name=\"Kara\" />\n\n==Usage==\nClear Script was used by Oirat and neighboring Mongols, mostly in the late 17th and early 18th centuries.<ref name=\"Kara\" /> It was widely used by its creator and others to translate Buddhist works so that they might better spread the Buddhist religion throughout western Mongolia. Though the script was useful for translating works from other languages, especially Tibetan, it was also used more informally, as evidenced by some letters from the late 1690s.<ref name=\"Kara\" />\n\nThe script was used by [[Kalmyks]] in [[Russia]] until 1924, when it was replaced by the [[Cyrillic script]]. In [[Xinjiang]], Oirats still use it, although today Mongolian education takes place in [[Chakhar Mongolian]] all across China.\n\n== Writing in Clear Script==\nThis script is a vertical script, as was its 'vertical Mongolian' parent script. Letters and diacritics are written along a central axis. Portions of letters to the right of the axis generally slant up, and portions to the left of the axis generally slant down. The only signs that do not follow these rules are the horizontal signs for ''S'' ''Š '' and part of ''Ö''.<ref name=\"Kara\"/>  Words are delineated by a space, as well as different letter forms. Though most letters only come in one shape, there are some letters that look different depending on where in the word they occur, whether they are initial, medial, or final.<ref name=\"Daniels\" />\n\nThere is an alphabetic order in Clear Script, as in other related scripts, but the order for Clear Script is not the same as its Mongolian parents nor its Aramaic ancestors.<ref name=\"Kara\"/>\n\n== Tables ==  \n{| class=\"wikitable\"\n|+ Vowels\n! Tod bichig\n! Cyrillic\n! Latin\n!\n|-\n|{{MongolUnicode|size=30px|[[ᠠ]]|v}}|| А|| A|| Same as Hudam bichig\n|-\n|{{MongolUnicode|size=30px|[[ᡄ]]|v}}|| Э || E\n|-\n|{{MongolUnicode|size=30px|[[ᡅ]]|v}}||И|| I \n|-\n|{{MongolUnicode|size=30px|[[ᡆ]]|v}}|| О || O \n|-\n|{{MongolUnicode|size=30px|[[ᡇ]]|v}}||У  || U \n|-\n|{{MongolUnicode|size=30px|[[ᡈ]]|v}}|| Ө || Ö\n|-\n|{{MongolUnicode|size=30px|[[ᡉ]]|v}}||Ү  || Ü \n|-\n|}\n\n{| class=\"wikitable\"\n|+ Native consonants\n! Tod bichig\n! Cyrillic\n! Latin\n!\n|-\n|{{MongolUnicode|size=30px|[[ᡋ]]|v}}|| Б || B \n|-\n|{{MongolUnicode|size=30px|[[ᡌ]]|v}}|| П || P \n|-\n|{{MongolUnicode|size=30px|[[ᡏ]]|v}}|| М || M \n|-\n|{{MongolUnicode|size=30px|[[ᠯ]]|v}}|| Л || L || Same as Hudam bichig\n|-\n|{{MongolUnicode|size=30px|[[ᠰ]]|v}}||С  || S || Same as Hudam bichig\n|-\n|{{MongolUnicode|size=30px|[[ᠱ]]|v}}||Ш  || Sh || Same as Hudam bichig\n|-\n|{{MongolUnicode|size=30px|[[ᠨ]]|v}}||Н  || N || Same as Hudam bichig\n|-\n|{{MongolUnicode|size=30px|[[ᡍ]]|v}}|| Х || X \n|-\n|{{MongolUnicode|size=30px|[[ᡎ]]|v}}||Һ  || Q \n|-\n|{{MongolUnicode|size=30px|[[ᡐ]]|v}}|| Т|| T \n|-\n|{{MongolUnicode|size=30px|[[ᡑ]]|v}}|| Д || D \n|-\n|{{MongolUnicode|size=30px|[[ᡔ]]|v}}||Ц  || C \n|-\n|{{MongolUnicode|size=30px|[[ᡒ]]|v}}||Ч  || Ch\n|-\n|{{MongolUnicode|size=30px|[[ᡓ]]|v}}|| З || Z \n|-\n|{{MongolUnicode|size=30px|[[ᡕ]]|v}}|| Й || Y \n|-\n|{{MongolUnicode|size=30px|[[ᠷ]]|v}}|| Р|| R || Same as Hudam bichig\n|-\n|{{MongolUnicode|size=30px|[[ᡖ]]|v}}|| В || W \n|-\n|}\n\n{| class=\"wikitable\"\n|+ Letters used in foreign words\n! Tod bichig\n! Cyrillic\n! Latin\n!\n|-\n|{{MongolUnicode|size=30px|[[ᡙ]]|v}}|| Г || G \n|-\n|{{MongolUnicode|size=30px|[[ᡘ]]|v}}||К  || K \n|-\n|{{MongolUnicode|size=30px|[[ᡗ]]|v}}||Қ  || Kh \n|-\n|{{MongolUnicode|size=30px|[[ᡚ]]|v}}|| Җ || J \n|-\n|{{MongolUnicode|size=30px|[[ᡛ]]|v}}||  ||  \n|-\n|{{MongolUnicode|size=30px|[[ᡜ]]|v}}|| || \n|-\n|{{MongolUnicode|size=30px|[[ᢘ]]|v}}|| ||  \n|-\n|{{MongolUnicode|size=30px|[[ᢙ]]|v}}|| || \n|-\n|{{MongolUnicode|size=30px|[[ᡊ]]|v}}||Ң  || Ng \n|-\n|}\n\n== See also ==\n* [[Mongolian writing systems]]\n* [[Mongolian script]]\n* [[Soyombo alphabet]]\n\n{{commons category|Todo bichig}}\n\n==References==\n{{Reflist}}\n\n== External links ==\n* [http://www.omniglot.com/writing/kalmyk.htm Oirat Clear Script at Omniglot]\n* [http://www.dusal.net/downloads/vertNote.rar Traditional Mongolian Notepad (for Windows XP/Vista)]\n\n{{list of writing systems}}\n\n[[Category:Mongolian writing systems]]\n[[Category:Alphabets]]\n[[Category:Kalmyk language]]"
    },
    {
      "title": "Coelbren y Beirdd",
      "url": "https://en.wikipedia.org/wiki/Coelbren_y_Beirdd",
      "text": "[[File:Dewi Wyn o Eifion (4702805).jpg||thumb|right|Painting of [[David Owen (Dewi Wyn o Eifion)|Dewi Wyn o Eifion]] (1784–1841) with the title written in Coelbren y Beirdd]]\nThe '''Coelbren y Beirdd''' (English: \"Bards' alphabet\") is a [[runic]] [[alphabet]] system created in the late eighteenth century by the [[Literary forgery|literary forger]] Edward Williams, best known as [[Iolo Morganwg]].<ref>{{cite web |url=http://www.museumwales.ac.uk/en/888/ |title=Archived copy of \"Coelbren y Beirdd – The Bardic Alphabet\"|accessdate=2011-02-26 |deadurl=yes |archiveurl=https://web.archive.org/web/20101117215438/http://www.museumwales.ac.uk/en/888/ |archivedate=17 November 2010 |df=dmy-all }} </ref>\n\nThe alphabet system consisted of twenty letters and twenty other representations of elongated vowels that resembled [[Ancient Greece|Ancient Greek]] and could be carved on four-sided pieces of wood and fitted into a frame he called a \"peithynen\". Williams presented wooden [[druidic alphabet]]s to friends and notables, and succeeded in persuading many of its authenticity.<ref name=\"Williams2010\">{{cite book|author=Jane Williams|title=A History of Wales: Derived from Authentic Sources|url=https://books.google.com/books?id=DIXiB0AX-rAC&pg=PA6|accessdate=24 October 2012|date=18 November 2010|publisher=Cambridge University Press|isbn=978-1-108-02085-5|pages=6–}}</ref>\n\nA Welsh [[Bard]]ic and [[Druid]]ic essay, written by his son [[Taliesin Williams]] and published as a [[pamphlet]] in 1840, defended the authenticity of the alphabet and won the Abergavenny Eisteddfod in 1838.<ref>Williams, Taliesin., (ab Iolo), Coelbren Y Beirdd; a Welsh Essay on the Bardic Alphabet, W. Rees, Llandovery, 1840.</ref><ref name=\"Williams1852\">{{cite book|author=Rob Williams|title=A biographical dictionary of eminent Welshmen., from the earliest times to the present|url=https://books.google.com/books?id=7NM5AAAAcAAJ&pg=PA536|accessdate=24 October 2012|year=1852|publisher=W. Rees|pages=536–}}</ref>\n\nTaliesin Williams's book was written about other Coelbrennau'r Beirdd, which is the name of a Welsh language manuscript in the [[Iolo Manuscripts]] and two manuscripts in [[Barddas]], one with the subtitle \"yn dorredig a chyllell\". Iolo Morganwg suggested they were originally the work of [[bard]]s from [[Glamorgan]] who had their manuscripts copied into collections stored at [[Plas y Fan]], [[Neath Abbey]], [[Margam Abbey]] and [[Raglan Library]], and compiled by [[Meurig Dafydd]] and [[Lewys Morgannwg]], amongst others, in the 1700s. These were suggested to have again been transcribed by [[Edward Dafydd]], [[John Bradford]] and [[Llywelyn Siôn]]. Moganwg suggested that he had collected some of Siôn and Bradford's manuscripts, while the majority, including all of Lewys Morgannwg's sources, were lost. This claim to authenticity has been questioned by numerous scholars such as [[Glyn Cothi Lewis]].<ref name=\"Association1846\">{{cite book|author=Cambrian Archaeological Association|title=Archaeologia cambrensis|url=https://books.google.com/books?id=Q7g1AAAAMAAJ&pg=PA472|accessdate=8 November 2012|year=1846|publisher=W. Pickering|pages=472–}}</ref><ref name=\"Cothi)1837\">{{cite book|author=Lewis (Glyn Cothi)|title=Gwaith Lewis Glyn Cothi: The Poetical Works of Lewis Glyn Cothi, a Celebrated Bard, who Flourished in the Reigns of Henry VI, Edward IV, Richard III, and Henry VII|url=https://books.google.com/books?id=FksAAAAAcAAJ&pg=PA260|accessdate=8 November 2012|year=1837|publisher=Hughes|pages=260–}}</ref><ref name=\"MorganwgJones1848\">{{cite book|author1=Iolo Morganwg|author2=Owen Jones|author3=Society for the Publication of Ancient Welsh Manuscripts, Abergavenny|title=Iolo manuscripts: A selection of ancient Welsh manuscripts, in prose and verse, from the collection made by the late Edward Williams, Iolo Morganwg, for the purpose of forming a continuation of the Myfyrian archaiology; and subsequently proposed as materials for a new history of Wales|url=https://books.google.com/books?id=6jREAAAAIAAJ&pg=PR10|accessdate=24 October 2012|year=1848|publisher=W. Rees; sold by Longman and co., London|pages=10–}}</ref><ref name=\"Löffler2007\">{{cite book|author=Marion Löffler|title=The literary and historical legacy of Iolo Morganwg, 1826–1926|url=https://books.google.com/books?id=TIBnAAAAMAAJ|accessdate=24 October 2012|year=2007|publisher=University of Wales Press|isbn=978-0-7083-2113-3}}</ref>\n\nTable of letters in ''Celtic Researches'' (1804) by [[Edward Davies (Celtic)|Edward Davies]] (1756–1831):\n[[File:Celtic Researches (1804) page 272 plate 1.jpg|400px|center]]\n\n==See also==\n*[[Ogham]]\n\n==References==\n{{Reflist|30em}}\n\n==External links==\n{{Commons Category}}\n* [http://www.museumwales.ac.uk/en/888/ Coelbren y Beirdd at the University of Wales]\n* [http://www.sacred-texts.com/neu/celt/bim1/bim1017.htm Coelbren y Beirdd in Barddas on Sacred Texts]\n* [https://archive.is/20130421025549/http://www.peoplescollectionwales.co.uk/Story/253-our-own-pageantry-and-peacockry-the-gorsedd-o/1789 The People's Collection Wales, Coelbren y Beirdd – The Bardic Alphabet]\n\n{{Authority control}}\n[[Category:Alphabets]]\n[[Category:Artificial scripts]]\n[[Category:Constructed languages]]\n[[Category:Obsolete writing systems]]\n[[Category:Scripts not encoded in Unicode]]"
    },
    {
      "title": "Coptic alphabet",
      "url": "https://en.wikipedia.org/wiki/Coptic_alphabet",
      "text": "{{refimprove|date=February 2015}}\n{{Infobox Writing system\n |name=Coptic alphabet\n |type=[[Alphabet]]\n |languages=[[Coptic language]]\n |time=c. 200 BC to present (in Coptic liturgy)\n |fam1= [[Egyptian hieroglyphs]]\n |fam2=[[Proto-Sinaitic]]\n |fam3=[[Phoenician alphabet]]\n |fam4=[[Greek script]] augmented by [[Demotic (Egyptian)|Demotic]]\n |children=[[Old Nubian language#Writing|Old Nubian]]\n |unicode = {{ublist |class=nowrap |[https://www.unicode.org/charts/PDF/U2C80.pdf U+2C80–U+2CFF] {{smaller|Coptic}} |[https://www.unicode.org/charts/PDF/U0370.pdf U+0370–U+03FF] {{smaller|Greek and Coptic}} |[https://www.unicode.org/charts/PDF/U102E0.pdf U+102E0–U+102FF] {{smaller|Coptic Epact Numbers}}}}\n |iso15924=Copt\n }}\n{{contains Coptic text}}\n\nThe '''Coptic alphabet''' is the script used for writing the [[Coptic language]]. The repertoire of [[glyph]]s is based on the [[Greek alphabet]] augmented by letters borrowed from the Egyptian [[Demotic (Egyptian)|Demotic]] and is the first alphabetic script used for the [[Egyptian language]]. There are several Coptic alphabets, as the Coptic writing system may vary greatly among the various dialects and subdialects of the [[Coptic language]].\n\n==History==\n[[File:CopticLetters.svg|thumb|left|Coptic letters in a florid [[Bohairic]] script]]\n{{Copts}}\n{{alphabet}}\nThe Coptic alphabet has a long history, going back to the [[Ptolemaic Egypt|Hellenistic]] period, of using the Greek alphabet to [[Transcription (linguistics)|transcribe]] Demotic texts, with the aim of recording the correct pronunciation of Demotic. During the first two centuries of the [[Common Era]], an entire series of magical texts were written in what scholars term ''Old Coptic'', Egyptian language texts written in the [[Greek alphabet]]. A number of letters, however, were derived from Demotic, and many of these (though not all) are used in \"true\" Coptic writing. With the spread of [[Christianity]] in [https://www.laits.utexas.edu/cairo/history/ancient/ancient.html Egypt], by the late 3rd century, knowledge of [[Egyptian hieroglyph|hieroglyphic]] writing was lost, as well as Demotic slightly later, making way for a writing system more closely associated with the [[Christian church]]. By the 4th century, the Coptic alphabet was \"standardised\", particularly for the Sahidic dialect. (There are a number of differences between the alphabets as used in the various dialects in Coptic.) Coptic is not generally used today except by the members of the [[Coptic Orthodox Church of Alexandria]] to write their [[religion|religious]] texts. All the [[Gnosticism|Gnostic]] codices found in [[Nag Hammadi]] used the Coptic alphabet.\n\nThe Old Nubian alphabet&mdash;used to write  [[Old Nubian language|Old Nubian]], a [[Nilo-Saharan language]] &mdash;is written mainly in an [[uncial]] Greek alphabet, which borrows Coptic and [[Meroitic script|Meroitic]] letters of Demotic origin into its inventory.\n\n== Form ==\nThe Coptic alphabet was the first Egyptian writing system to indicate [[vowels]], making Coptic documents invaluable for the interpretation of earlier Egyptian texts. Some Egyptian syllables had [[sonorant]]s but no vowels; in Sahidic, these were written in Coptic with a line above the entire syllable. Various scribal schools made limited use of diacritics: some used an apostrophe as a [[word divider]] and to mark [[clitic]]s, a function of [[determinative]]s in [[logogram|logographic]] Egyptian; others used [[Diaeresis (diacritic)|diereses]] over {{coptic|ⲓ}} and {{coptic|ⲩ}} to show that these started a new syllable, others a [[circumflex]] over any vowel for the same purpose.<ref name=D&B>Ritner, Robert Kriech. 1996. \"The Coptic Alphabet\". In ''The World's Writing Systems'', edited by Peter T. Daniels and William Bright. Oxford and New York: Oxford University Press. 1994:287–290.</ref>\n\nThe Coptic alphabet's glyphs are largely based on the Greek alphabet, another help in interpreting older Egyptian texts,<ref>Campbell, George L. \"Coptic.\" Compendium of the World's Writing Systems. 2nd ed. Vol. 1. Biddles LTD, 1991. 415.</ref> with 24 letters of Greek origin; 6 or 7 more were retained from [[Demotic (Egyptian)|Demotic]], depending on the dialect (6 in Sahidic, another each in Bohairic and Akhmimic).<ref name=D&B/> In addition to the alphabetic letters, the letter ϯ stood for the syllable {{IPA|/te/}} or {{IPA|/de/}}.\n\nAs the Coptic alphabet is simply a [[typeface]] of the Greek alphabet,<ref>{{Cite web|url=http://www.ancientscripts.com/coptic.html|title=Coptic|last=|first=|date=|website=Ancient Scripts|archive-url=|archive-date=|dead-url=|accessdate=2 December 2017}}</ref> with a few added letters, it can be used to write Greek without any transliteration schemes. Latin equivalents would include the [[Icelandic orthography|Icelandic alphabet]] (which likewise has added letters), or the [[Fraktur]] alphabet (which has distinctive forms). While initially unified with the Greek alphabet by [[Unicode]], a proposal was later accepted to separate it, with the proposal noting that Coptic is never written using modern Greek letter-forms (unlike German, which may be written with Fraktur or Roman [[Antiqua (typeface class)|Antiqua]] letter-forms), and that the Coptic letter-forms have closer mutual legibility with the Greek-based letters incorporated into the separately encoded [[Cyrillic alphabet]] than with the forms used in modern Greek.<ref>{{cite web|title=L2/02-205 N2444: Coptic supplementation in the BMP|date=2002-05-08|first1=Michael|last1=Everson|first2=Kamal|last2=Mansour|url=https://www.unicode.org/L2/L2002/02205-n2444-coptic.pdf}}</ref><ref>For example: The composer's name \"Dmitri Dmitriyevich Shostakovich\" is Дмитрий Дмитриевич Шостакович in Cyrillic, and Ⲇⲙⲏⲧⲣⲓⲓ Ⲇⲙⲏⲧⲣⲓⲉⲃⲓϭ Ϣⲟⲥⲧⲁⲕⲟⲃⲓϭ in Coptic.</ref>\n\n== Alphabet table ==\n\n{| style=\"text-align: center; margin: 10pt;\" class=\"wikitable\"\n! Image {{abbr|maj.|majuscule}}\n! Image {{abbr|min.|minuscule}}\n! Unicode {{abbr|maj.|majuscule}}\n! Unicode {{abbr|min.|minuscule}}\n! Numeric value\n! Name<ref>Peust (1999.59-60)</ref>\n! Greek equivalent\n! Transliteration\n! Sahidic<br>pronunciation<ref>Peust (1999)</ref>\n! Bohairic<br>pronunciation<ref>Peust (1999)</ref>\n! Late Coptic<br>pronunciation<ref>Before the [[Greco-Bohairic]] reforms of the mid 19th century.</ref>\n!Greco-Bohairic {{Abbr|pronunciation|hover over IPA symbol for when pronunciation is used, if no message appears when hovering: pronounced such always}}<ref>{{Cite web|url=https://www.suscopts.org/deacons/coptic/FT-Coptic%20Language-Lectures.pdf|title=The Coptic Language|last=|first=|date=|website=Coptic Orthodox Diocese of the Southern United States|archive-url=|archive-date=|dead-url=|access-date=}}</ref>\n|-\n! [[File:CopteAmaj.png]]\n! [[File:CopteAmin.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ⲁ</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ⲁ</span>}}\n| 1\n| ''alpha''|| Α, α \n| a \n| [a]\n| [a]\n| [a]\n|{{IPA|[ɐ]}}\n|-\n! [[File:CopteBmaj.png]]\n! [[File:CopteBmin.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ⲃ</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ⲃ</span>}}\n| 2\n| ''wêta, wida''|| Β, β || w, b\n| {{IPA|[β]}}\n| {{IPA|[β]}}\n| {{IPA|[w]}}<br>(final [b])\n|{{IPA|[b}}, {{Abbr|v]|before a vowel (except in a name)}}\n|-\n! [[File:CopteCmaj.png]]\n! [[File:CopteCmin.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ⲅ</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ⲅ</span>}}\n| 3\n| ''gamma''|| Γ, γ \n| g \n|{{IPA|[k]}}<br>(marked Greek words)\n|—\n|—\n|{{IPA|[ɣ}}, {{Abbr|g|before [e̞] or [i]}}, {{Abbr|ŋ]|before [g] or [k]}}\n|-\n! [[File:CopteDmaj.png]]\n! [[File:CopteDmin.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ⲇ</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ⲇ</span>}}\n| 4\n| ''dalda''|| Δ, δ || d \n|{{IPA|[t]}}<br>(marked Greek words)\n|—\n|—\n|{{IPA|[ð}}, {{Abbr|d]|in a name}}\n|-\n! [[File:CopteEmaj.png]]\n! [[File:CopteEmin.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ⲉ</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ⲉ</span>}}\n| 5\n| ''aia''|| Ε, ε || ə\n|{{IPA|[ɛ, ə]}}<br>(''ei'' = {{IPA|[i, j]}})\n|{{IPA|[ɛ, ə]}}<br>(''ei'' = {{IPA|[əj]}})\n|{{IPA|[a]}}\n|{{IPA|[e̞]}}\n|-\n! [[File:Copte6.png]]\n! [[File:Copte6.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ⲋ</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ⲋ</span>}}\n| 6\n| (''soou'' '6') || [[ϛ]]<br>Ϛ, ϛ[[Stigma (letter)|*]]<br>({{GrGl|Digamma cursive 07}}, {{GrGl|Digamma cursive 04}})|| s͡t<ref group=note>The upper line of ''s'' connected with ''t'' to distinguishes it from the standalone \"s\" and \"t\"</ref>\n|—\n|—\n|—\n|—\n|-\n! [[File:CopteZmaj.png]]\n! [[File:CopteZmin.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ⲍ</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ⲍ</span>}}\n| 7\n| ''zêta, zita''|| Ζ, ζ || z \n|{{IPA|[s]}}<br>(marked Greek words)\n|—\n|—\n|{{IPA|[z]}}\n|-\n! [[File:CopteYmaj.png]]\n! [[File:CopteYmin.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ⲏ</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ⲏ</span>}}\n| 8\n| ''ê(i)ta, hada''|| Η, η || aa, ê\n|{{IPA|[e]}}\n|{{IPA|[e]}}\n|{{IPA|[i, a]}}\n|{{IPA|[iː]}}\n|-\n! [[File:CopteTHmaj.png]]\n! [[File:CopteTHmin.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ⲑ</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ⲑ</span>}}\n| 9\n| tida|| Θ, θ || t\n| {{IPA|[t.h]}}\n| {{IPA|[tʰ]}}\n| {{IPA|[t]}}\n|{{IPA|[θ]}}\n|-\n! [[File:CopteImaj.png]]\n! [[File:CopteImin.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ⲓ</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ⲓ</span>}}\n| 10\n| ''iôta, iauda''|| Ι, ι || i \n|{{IPA|[i, j]}}\n|{{IPA|[i, j]}}\n|{{IPA|[i, j]}}\n|{{IPA|[i,}}{{Abbr|j|before vowels}},{{Abbr|ɪ]|after vowels to form diphthongs}}\n|-\n! [[File:CopteKmaj.png]]\n! [[File:CopteKmin.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ⲕ</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ⲕ</span>}}\n| 20\n| ''kab(b)a''|| Κ, κ || k \n|{{IPA|[k]}}\n|{{IPA|[k]}}\n|{{IPA|[k]}}\n|{{IPA|[k]}}\n|-\n! [[File:CopteLmaj.png]]\n! [[File:CopteLmin.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ⲗ</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ⲗ</span>}}\n| 30\n| ''lauda, laula''|| Λ, λ || l \n|{{IPA|[l]}}\n|{{IPA|[l]}}\n|{{IPA|[l]}}\n|{{IPA|[l]}}\n|-\n! [[Image:CopteMmaj.png]]\n! [[Image:CopteMmin.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ⲙ</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ⲙ</span>}}\n| 40\n| ''mê, me, mi''|| Μ, μ || m \n|{{IPA|[m]}}\n|{{IPA|[m]}}\n|{{IPA|[m]}}\n|{{IPA|[m]}}\n|-\n! [[Image:CopteNmaj.png]]\n! [[Image:CopteNmin.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ⲛ</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ⲛ</span>}}\n| 50\n| ''ne, ni, nnê''|| Ν, ν || n \n|{{IPA|[n]}}\n|{{IPA|[n]}}\n|{{IPA|[n]}}\n|{{IPA|[n]}}\n|-\n! [[Image:CopteKSmaj.png]]\n! [[Image:CopteKSmin.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ⲝ</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ⲝ</span>}}\n| 60\n| ''ksi''|| Ξ, ξ || ks\n|—\n|—\n|—\n|{{IPA|[ks, }} {{Abbr|e̞ks]|usually following a consonant, or sometimes when starting a word}}\n|-\n! [[Image:CopteOmaj.png]]\n! [[Image:CopteOmin.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ⲟ</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ⲟ</span>}}\n| 70\n| ''ou, o''|| Ο, ο || o \n|{{IPA|[ɔ]}}\n|{{IPA|[ɔ]}}\n|{{IPA|[u]}}\n|{{IPA|[o̞}}, {{Abbr|u|diphthong \"ⲟⲩ\"}}]\n|-\n! [[Image:CoptePmaj.png]]\n! [[Image:CoptePmin.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ⲡ</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ⲡ</span>}}\n| 80\n|''bi''|| Π, π || b\n|{{IPA|[p]}}\n|{{IPA|[p]}}\n|{{IPA|[b]}}\n|{{IPA|[p]}}\n|-\n! [[Image:CopteFmaj.png]]\n! [[Image:CopteFmin.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ϥ</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ϥ</span>}}\n| 90\n| ''fai, fêei, fei''|| [[ϙ]]<br>(numerical value) || f\n| {{IPA|[f]}}\n| {{IPA|[f]}}\n| {{IPA|[f]}}\n|{{IPA|[f]}}\n|-\n! [[Image:CopteRmaj.png]]\n! [[Image:CopteRmin.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ⲣ</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ⲣ</span>}}\n| 100\n| ''ro, hro, rô''|| Ρ, ρ || r \n|{{IPA|[r]}}\n|{{IPA|[r]}}\n|{{IPA|[r]}}\n|{{IPA|[ɾ]}}\n|-\n! [[Image:CopteCCmaj.png]]\n! [[Image:CopteCCmin.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ⲥ</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ⲥ</span>}}\n| 200\n| ''sima, summa, sêma''|| Σ, σ, ς || s \n|{{IPA|[s]}}\n|{{IPA|[s]}}\n|{{IPA|[s]}}\n|{{IPA|[s]}}\n|-\n! [[Image:CopteTmaj.png]]\n! [[Image:CopteTmin.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ⲧ</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ⲧ</span>}}\n| 300\n|''dau''|| Τ, τ || d, t\n|{{IPA|[t]}}\n|{{IPA|[t]}}\n|{{IPA|[d]}}<br>(final {{IPA|[t]}})\n|{{IPA|[t]}}\n|-\n! [[Image:CopteUmaj.png]]\n! [[Image:CopteUmin.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ⲩ</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ⲩ</span>}}\n| 400\n| ''he, ue''|| Υ, υ || u \n| colspan=2| {{IPA|[w]}} (''ou'' = {{IPA|[u, w]}})\n|—\n|{{IPA|[i}}, {{Abbr|w|between \"ⲟ\" and another vowel except \"ⲱ\"}}, {{Abbr|v|after [ɑ] (ⲁ) or [e̞] (ⲉ)}}, {{Abbr|u|diphthong \"ⲟⲩ\"}}]\n|-\n! [[Image:CopteVmaj.png]]\n! [[Image:CopteVmin.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ⲫ</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ⲫ</span>}}\n| 500\n| ''phi''|| Φ, φ || f, b\n|{{IPA|[p.h]}}\n|{{IPA|[pʰ]}}\n|{{IPA|[b~f]}}\n|{{IPA|[f]}}\n|-\n! [[Image:CopteXmaj.png]]\n! [[Image:CopteXmin.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ⲭ</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ⲭ</span>}}\n| 600\n| ''khi''|| Χ, χ || kh \n|{{IPA|[k.h]}}\n|{{IPA|[kʰ]}}\n|{{IPA|[k]}}\n|{{Abbr|[k,|if the word is Coptic in origin}} {{Abbr|x|if the word is Greek in origin}}, {{Abbr|ç]|if the word is Greek in origin but before [e̞] or [i]}}\n|-\n! [[Image:CoptePSmaj.png]]\n! [[Image:CoptePSmin.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ⲯ</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ⲯ</span>}}\n| 700\n| ''psi''|| Ψ, ψ || ps\n|—\n|—\n|—\n|{{IPA|[ps, }} {{Abbr|e̞ps]|usually following a consonant}}\n|-\n! [[Image:CopteWmaj.png]]\n! [[Image:CopteWmin.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ⲱ</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ⲱ</span>}}\n| 800\n| ''ô, au''|| Ω, ω || ô\n|{{IPA|[o]}}\n|{{IPA|[o]}}\n|{{IPA|[u]}}\n|{{IPA|[o̞ː]}}\n|-\n! [[File:Copte r barre.png]]\n! [[File:Copte r barre.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ⳁ</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ⳁ</span>}}\n| 900\n|''sampi''\n|[[Sampi|Ϡ,ϡ]]<br>(numerical value)\n|—\n|—\n|—\n|—\n|—\n|-\n! [[Image:CopteSmaj.png]]\n! [[Image:CopteSmin.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ϣ</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ϣ</span>}}\n|—\n| ''šai, šei''|| (none) || š\n|{{IPA|[ʃ]}}\n|{{IPA|[ʃ]}}\n|{{IPA|[ʃ]}}\n|{{IPA|[ʃ]}}\n|-\n! [[Image:CopteKHmaj.png]]\n! [[Image:CopteKHmin.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ϧ (Ⳉ)</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ϧ (ⳉ)</span>}}<br><ref group=note>Akhmimic dialect uses the letter {{coptic|Ⳉ}} {{coptic|ⳉ}} for {{IPA|/x/}}. No name is recorded.</ref>\n|—\n| ''xai, xei''|| (none) || x \n| NA\n|{{IPA|[x]}}\n|{{IPA|[x]}}\n|{{IPA|[x]}}\n|-\n! [[Image:CopteHmaj.png]]\n! [[Image:CopteHmin.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ϩ</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ϩ</span>}}\n|—\n| ''hori, hôrei''|| (none) || h \n|{{IPA|[h]}}\n|{{IPA|[h]}}\n|{{IPA|[h]}}\n|{{IPA|[h]}}\n|-\n! [[Image:CopteJmaj.png]]\n! [[Image:CopteJmin.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ϫ</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ϫ</span>}}\n|—\n| ''janjia''|| (none) || j\n| {{IPA|[tʲ]}}\n| {{IPA|[c]}}\n| {{IPA|[ɟ]}}\n|{{IPA|[g,}} {{Abbr|dʒ]|before [e̞] or [i]}}\n|-\n! [[File:CopteTSHmaj.png]]\n! [[File:CopteTSHmin.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ϭ</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ϭ</span>}}\n|—\n| ''cima''||(none)\n| c\n|{{IPA|[kʲ]}}\n|{{IPA|[cʰ]}}\n|{{IPA|[ʃ]}}\n|{{IPA|[tʃ}}, {{Abbr|e̞tʃ]|usually following a consonant}}\n|-\n! [[File:CopteTImaj.png]]\n! [[File:CopteTImin.png]]\n! {{coptic|1=<span style=\"font-size:190%;\">Ϯ</span>}}\n! {{coptic|1=<span style=\"font-size:190%;\">ϯ</span>}}\n|—\n|''di, †ei''|| (none) || di\n|{{IPA|[di]}}\n|{{IPA|[də]}}?<br>{{cn|date=July 2018}}\n|—\n|{{IPA|[di]}}\n|}\n\n<div style=\"font-size:111%\">{{Reflist|group=note}}</div>\n\n=== Letters derived from Demotic ===\nIn Old Coptic, there were a large number of [[Demotic (Egyptian)|Demotic]] Egyptian characters, including some logograms. They were soon reduced to half a dozen, for sounds not covered by the Greek alphabet. The following letters remained:\n\n{| border=0\n|--\n! Hieroglyph\n! &nbsp;\n! Demotic\n! &nbsp;\n! Coptic\n! &nbsp;\n! {{abbr|Translit.|Transliteration}}\n|-\n| <hiero>SA</hiero> || → || [[File:demotique sh.png]]\n| → || {{coptic|1=<span style=\"font-size:190%;\">Ϣ</span>}} || || š \n|-\n| <hiero>f</hiero> || → || [[File:demotique f.png]]\n| → || {{coptic|1=<span style=\"font-size:190%;\">Ϥ</span>}} || || f\n|-\n| <hiero>M12</hiero> || → || [[File:demotique kh.png]]\n| → || {{coptic|1=<span style=\"font-size:190%;\">Ϧ</span>}} || || x\n|-\n| <hiero>F18:Y1</hiero> || → || [[File:demotique h.png]]\n| → || {{coptic|1=<span style=\"font-size:190%;\">Ϩ</span>}} || || h\n|-\n| <hiero>U29</hiero> || → || [[File:demotique j.png]]\n| → || {{coptic|1=<span style=\"font-size:190%;\">Ϫ</span>}} || || j\n|-\n| <hiero>k</hiero> || → || [[File:demotique tsh.png]]\n| → || {{coptic|1=<span style=\"font-size:190%;\">Ϭ</span>}} || || c\n|-\n| <hiero>D37:t</hiero> || → || [[File:demotique ti.png]]\n| → || {{coptic|1=<span style=\"font-size:190%;\">Ϯ</span>}} || || di\n|}\n\n== Unicode ==\n{{Main | Greek and Coptic | Coptic (Unicode block) | Coptic Epact Numbers| l1 = Greek and Coptic (Unicode block) | l3 = Coptic Epact Numbers (Unicode block)}}\n\nIn [[Unicode]], most Coptic letters formerly shared codepoints with similar [[Greek alphabet|Greek]] letters, but a disunification was accepted for version 4.1, which appeared in 2005. The new Coptic block is U+2C80 to U+2CFF. Most [[fonts]] contained in mainstream [[operating systems]] use a distinctive Byzantine style for this block. The Greek block includes seven Coptic letters (U+03E2&ndash;U+03EF highlighted below) derived from Demotic, and these need to be included in any complete implementation of Coptic.\n\n{{Unicode chart Greek and Coptic|coptic=}}\n{{Unicode chart Coptic}}\n{{Unicode chart Coptic Epact Numbers}}\n\n== Diacritics and punctuation ==\n\nThese are also included in the Unicode specification.\n\n=== Punctuation ===\n* Normal English punctuation (comma, period, question mark, semicolon, colon, hyphen) uses the regular Unicode codepoints for punctuation\n* Dicolon: standard colon U+003A\n* Middle dot: U+00B7\n* En dash: U+2013\n* Em dash: U+2014\n* Slanted double hyphen: U+2E17\n\n=== Combining diacritics ===\n\nThese are codepoints applied after that of the character they modify.\n\n* Combining overstroke: U+0305 (= supralinear stroke)\n* Combining character-joining overstroke (from middle of one character to middle of the next): U+035E\n* Combining dot under a letter: U+0323\n* Combining dot over a letter: U+0307\n* Combining overstroke and dot below: U+0305,U+0323\n* Combining acute accent: U+0301\n* Combining grave accent: U+0300\n* Combining circumflex accent (caret shaped): U+0302\n* Combining circumflex (curved shape) or inverted breve above: U+0311\n* Combining circumflex as wide inverted breve above joining two letters: U+0361\n* Combining diaeresis: U+0308\n\n===Macrons and overlines===\n\nCoptic uses {{unichar|0304|COMBINING MACRON|cwith=&#x25cc;|use=script|use2=Copt}} to indicate [[syllabic consonant]]s, for example {{coptic|1=<span style=\"font-size:150%;\">ⲛ̄</span>}}.<ref>{{cite web|url=https://www.unicode.org/L2/L2004/04130-n2744-coptic.pdf|title=Revision of the Coptic block under ballot for the BMP of the UCS|date=2004-04-20|publisher=ISO/IEC JTC1/SC2/WG2}}</ref><ref name=\"N3222R\"/>\n\nCoptic abbreviations use {{unichar|0305|COMBINING OVERLINE|cwith=&#x25cc;|use=script|use2=Copt}} to draw a continuous line across the remaining letters of an abbreviated word.<ref name=\"N3222R\">{{cite web|url=https://www.unicode.org/L2/L2007/07085r-n3222r-coptic-adds.pdf|title=N3222R: Proposal to add additional characters for Coptic and Latin to the UCS|first1=Michael|last1=Everson|authorlink1=Michael Everson|first2=Stephen|last2=Emmel|first3=Antti|last3=Marjanen|first4=Ismo|last4=Dunderberg|first5=John|last5=Baines|first6=Susana|last6=Pedro|first7=António|last7=Emiliano|publisher=ISO/IEC JTC1/SC2/WG2|date=2007-05-12}}</ref><ref name=\"G24556\">{{cite web|url=https://www.unicode.org/versions/Unicode10.0.0/ch07.pdf#G24556|date=July 2017|title=Section 7.3: Coptic, Supralineation|work=The Unicode Standard|publisher=The Unicode Consortium}}</ref>  It extends from the left edge of the first letter to the right edge of the last letter.  For example, {{coptic|1=<span style=\"font-size:150%;\">ⲡ̅ⲛ̅ⲁ̅</span>}}.\n\nA different kind of overline uses {{unichar|FE24|COMBINING MACRON LEFT HALF|cwith=&#x25cc;|use=script|use2=Copt}}, {{unichar|FE26|COMBINING CONJOINING MACRON|cwith=&#x25cc;|use=script|use2=Copt}}, and {{unichar|FE25|COMBINING MACRON RIGHT HALF|cwith=&#x25cc;|use=script|use2=Copt}} to distinguish the spelling of certain common words or to highlight proper names of divinities and heroes.<ref name=\"N3222R\"/><ref name=\"G24556\"/>\nFor this the line begins in the middle of the first letter and continues to the middle of the last letter.  A few examples: {{coptic|1=<span style=\"font-size:150%;\">ⲣ︤ⲙ︥</span>}}, {{coptic|1=<span style=\"font-size:150%;\">ϥ︤ⲛ︦ⲧ︥</span>}}, {{coptic|1=<span style=\"font-size:150%;\">ⲡ︤ϩ︦ⲣ︦ⲃ︥</span>}}.\n\nCoptic numerals are indicated with letters of the alphabet such as {{coptic|1=<span style=\"font-size:150%;\">ⲁ</span>}} for 1.<ref>{{cite web | url=https://www.unicode.org/versions/Unicode9.0.0/ch07.pdf#G20251 | date=July 2016 | title=Section 7.3: Coptic, Numerical Use of Letters | work=The Unicode Standard | publisher=The Unicode Consortium}}</ref>\nSometimes numerical use is indicated with a continuous line above using {{unichar|0305|COMBINING OVERLINE|cwith=&#x25cc;|use=script|use2=Copt}} as in {{coptic|1=<span style=\"font-size:150%;\">ⲁ͵ⲱ̅ⲡ̅ⲏ̅</span>}} for 1,888 (where \"{{coptic|1=<span style=\"font-size:150%;\">ⲁ͵</span>}}\" is 1,000 and \"{{coptic|1=<span style=\"font-size:150%;\">ⲱ̅ⲡ̅ⲏ̅</span>}}\" is 888).  Multiples  of  1,000 can be indicated by a continuous double line above using {{unichar|033F|COMBINING DOUBLE OVERLINE|cwith=&#x25cc;|use=script|use2=Copt}} as in {{coptic|1=<span style=\"font-size:150%;\">ⲁ̿</span>}} for 1,000.\n\n==See also==\n* [[Coptic pronunciation reform]]\n* [[Institute of Coptic Studies]]\n\n==References==\n{{Reflist}}\n*Quaegebeur, Jan. 1982. \"De la préhistoire de l'écriture copte.\" ''Orientalia lovaniensia analecta'' 13:125&ndash;136.\n*Kasser, Rodolphe. 1991. \"Alphabet in Coptic, Greek\". In ''[[The Coptic Encyclopedia]]'', edited by [[Aziz S. Atiya]]. New York: Macmillan Publishing Company, Volume 8. 30&ndash;32.\n*Kasser, Rodolphe. 1991. \"Alphabets, Coptic\". In ''The Coptic Encyclopedia'', edited by Aziz S. Atiya. New York: Macmillan Publishing Company, Volume 8. 32&ndash;41.\n*Kasser, Rodolphe. 1991. \"Alphabets, Old Coptic\". In ''The Coptic Encyclopedia'', edited by Aziz S. Atiya. New York: Macmillan Publishing Company, Volume 8. 41&ndash;45.\n* Wolfgang Kosack: ''Koptisches Handlexikon des Bohairischen.'' Koptisch - Deutsch - Arabisch. Verlag Christoph Brunner, Basel 2013, {{ISBN|978-3-9524018-9-7}}.\n\n== External links ==\n{{Commons category|Coptic script}}\n*[[Michael Everson]]'s [http://std.dkuug.dk/JTC1/SC2/WG2/docs/n2636.pdf Revised proposal to add the Coptic alphabet to the BMP of the UCS]\n*[https://www.unicode.org/L2/L2011/11062r-coptic-epact.pdf Final Proposal to Encode Coptic Epact Numbers in ISO/IEC 1064]\n*[http://copticsounds.wordpress.com/ Copticsounds – a resource for the study of Coptic phonology]\n*[http://yearsandmiles.com/articles/coptic-alphabet.htm Phonological overview of the Coptic alphabet in comparison to classical and modern Greek.]\n*[http://ucbclassics.dreamhosters.com/djm/coptic.html Coptic Unicode input]\n*[[Michael Everson]]'s [http://www.evertype.com/fonts/coptic/ ''Antinoou'': A standard font for Coptic] supported by the [https://web.archive.org/web/20110201011732/http://rmcisadu.let.uniroma1.it/~iacs/ International Association for Coptic Studies].\n*[http://www.ifao.egnet.net/  Ifao N Copte] – A professional Coptic font for researchers, students and publishers has been developed by the French institute of oriental archeology (IFAO). Unicode, Mac and Windows compatible, this free font is available through downloading from the IFAO website ([http://www.ifao.egnet.net/publications/outils/polices/ direct link]).\n*[http://www.typographies.fr/ Coptic fonts] ; Coptic fonts made by Laurent Bourcellier & Jonathan Perez, type designers\n*[http://sites.google.com/site/pisakho/Home {{coptic|ⲡⲓⲥⲁϧⲟ}}]: Coptic [http://sites.google.com/site/pisakho/fonts font support] – how to install, use and manipulate Coptic ASCII and Unicode fonts\n*[http://st-takla.org/Download-Software-Free/Coptic_Downloads_Ta7mil___Christian_&_Coptic_Free-Fonts_01.html Download Free Coptic Fonts]\n*[http://www.omniglot.com/writing/coptic.htm The Coptic Alphabet] (omniglot.com)\n*[https://www.gnu.org/software/freefont/ GNU FreeFont] Coptic range in serif face\n\n{{list of writing systems}}\n\n[[Category:Coptic language]]\n[[Category:Scripts with ISO 15924 four-letter codes|Copt]]\n[[Category:Greek alphabet]]\n[[Category:Writing systems derived from the Phoenician]]\n[[Category:Alphabets]]\n[[Category:Writing systems of Africa]]\n[[Category:Ancient Egyptian language]]\n[[Category:Coptic Orthodox Church]]\n[[Category:Scripts encoded in Unicode 4.1]]"
    },
    {
      "title": "Cyrillic script",
      "url": "https://en.wikipedia.org/wiki/Cyrillic_script",
      "text": "{{redirect2|Cyrillic|Cyrillic alphabet|national variants of the Cyrillic\n script|Cyrillic alphabets|other uses|Cyrillic (disambiguation)}}\n{{Use dmy dates|date=July 2013}}\n{{Infobox writing system\n|name = Cyrillic\n|type = [[Alphabet]]\n|time = [[early Cyrillic alphabet|Earliest variants]] exist c.893<ref name=\"Auty\">Auty, R. ''Handbook of Old Church Slavonic, Part II: Texts and Glossary.'' 1977.</ref>-c.940\n|languages ={{plainlist|\n*{{flag|Kosovo}} ([[Serbian language]])\n*{{flag|Karachay-Cherkessia}} ([[Karachay-Balkar language]], [[Kabardian language]] & [[Nogai language]])\n*{{flag|Crimea}} ([[Cyrillic]])\n*{{flag|Mari El}} ([[Meadow Mari language]] & [[Hill Mari language]])\n*{{flag|Chelyabinsk Oblast}} ([[Bashkir language]] & [[Tatar language]])\n*{{flag|Belarus}}\n*{{flag|Tatarstan}} ([[Tatar language]])\n*{{flag|Bosnia and Herzegovina}} (also [[Latin script|Latin]])\n*{{flag|Kabardino-Balkaria}} ([[Kabardian language]] & [[Karachay-Balkar language]])\n*{{flag|Abkhazia}}\n*{{flag|Sakha Republic}} ([[Yakut language]])\n*{{flag|Karakalpakstan}} ([[Karakalpak language]])\n*{{flag|Chukotka Autonomous Okrug}} ([[Chukchi language]])\n*{{flag|Evenk Autonomous Okrug}} ([[Evenki language]])\n*{{flag|North Ossetia-Alania}} ([[Ossetian language]])\n*{{flag|Agin-Buryat Autonomous Okrug}} ([[Buryat language]])\n*{{flag|Bulgaria}} (from c.893)\n*{{flag|Buryatia}} ([[Buryat language]])\n*{{flag|Kazakhstan}} (until 2025)<ref name=\"KazSwitch\"/>\n*{{flag|Khakassia}} ([[Khakas language]])\n*{{flag|Nenets Autonomous Okrug}} ([[Nenets language]])\n*{{flag|Kyrgyzstan}}\n*{{flag|Udmurtia}} ([[Udmurt language]])\n*{{flag|East Turkestan}} ([[Uyghur Cyrillic alphabet]])\n*{{flag|Komi Republic}} ([[Komi language]])\n*{{flag|Mongolia}} (also [[Mongolian script]])\n*{{flag|Mordovia}} ([[Moksha language]] & [[Erzya language]])\n*{{flag|South Ossetia}} ([[Ossetian language]])\n*{{flag|Montenegro}} (also [[Latin script|Latin]])\n*{{flag|Bashkortostan}} ([[Bashkir language]])\n*{{flag|North Macedonia}}\n*{{flag|Tuva}} ([[Tuvan language]])\n*{{flag|Carpathian Ruthenia}} ([[Rusyn language]])\n*{{flag|Russia}}\n*{{flag|Ingushetia}} ([[Ingush language]])\n*{{flag|Adygea}} ([[Adyghe language]])\n*{{flag|Chechnya}} ([[Chechen language]])\n*{{flag|Altai Republic}} ([[Altai language]])\n*{{flag|Kalmykia}} ([[Kalmyk language]])\n*{{flag|Serbia}} (also [[Latin script|Latin]])\n*{{flag|Tajikistan}}\n*{{flag|Chuvashia}} ([[Chuvash language]])\n*{{flag|Transnistria}} (de jure part of [[Republic of Moldova|Moldova]])\n*{{flag|Ukraine}}}}\n(see [[Languages using Cyrillic]])\n|states =\n|names = {{lang-be|кірыліца}}, {{lang-bg|кирилица}} {{IPA-bg|ˈkirilit͡sɐ|}}, {{lang-mk|кирилица}} {{IPA|[kiˈrilit͡sa]}}, {{lang-ru|кириллица}} {{IPA-ru|kʲɪˈrʲilʲɪtsə|}}, {{lang-sr|ћирилица}}, {{lang-uk|кирилиця}}\n|fam1 = [[Egyptian hieroglyphs]]<ref>[http://news.bbc.co.uk/2/hi/middle_east/521235.stm Oldest alphabet found in Egypt]. BBC. 1999-11-15. Retrieved 2015-01-14.</ref>\n|fam2 = [[Proto-Sinaitic]]\n|fam3=[[Phoenician alphabet|Phoenician]]\n|fam4=[[Greek script|Greek]]\n|sisters = {{plainlist|\n*[[Latin alphabet]]\n*[[Coptic alphabet]]\n*[[Armenian alphabet]]\n}}\n|unicode = {{ublist |class=nowrap |[https://www.unicode.org/charts/PDF/U0400.pdf U+0400–U+04FF] {{smaller|Cyrillic}} |[https://www.unicode.org/charts/PDF/U0500.pdf U+0500–U+052F] {{smaller|Cyrillic Supplement}} |[https://www.unicode.org/charts/PDF/U2DE0.pdf U+2DE0–U+2DFF] {{smaller|Cyrillic Extended-A}} |[https://www.unicode.org/charts/PDF/UA640.pdf U+A640–U+A69F] {{smaller|Cyrillic Extended-B}} |[https://www.unicode.org/charts/PDF/U1C80.pdf U+1C80–U+1C8F] {{smaller|Cyrillic Extended-C}}}}\n|iso15924 = Cyrl\n|iso15924 note = <br><code>Cyrs</code> (Old Church Slavonic variant)\n|sample = Romanian Cyrillic - Lord's Prayer text.svg\n}}\n{{Contains Cyrillic text}}\n\nThe '''Cyrillic script''' ({{IPAc-en|s|ᵻ|ˈ|r|ɪ|l|ɪ|k}}) is a [[writing system]] used for various alphabets across [[Eurasia]] and is used as the national script in various [[Slavs|Slavic]]-, [[Turkic peoples|Turkic]]- and [[Persian language|Persian]]-speaking countries in [[Eastern Europe]], the [[Caucasus]], [[Central Asia]], and [[North Asia]]. It is based on the [[Early Cyrillic alphabet]] developed during the 9th century AD at the [[Preslav Literary School]] in the [[First Bulgarian Empire]].<ref>{{cite book | first=Francis | last=Dvornik |title=The Slavs: Their Early History and Civilization | quote = The Psalter and the Book of Prophets were adapted or \"modernized\" with special regard to their use in Bulgarian churches, and it was in this school that the [[Glagolitic script]] was replaced by the so-called Cyrillic writing, which was more akin to the Greek uncial, simplified matters considerably and is still used by the Orthodox Slavs. | year=1956 |place=Boston | publisher=American Academy of Arts and Sciences |page=179}}</ref><ref>{{cite book|url=https://books.google.com/?id=YIAYMNOOe0YC&pg=PR1&dq=Curta,+Florin,+Southeastern+Europe+in+the+Middle+Ages,+500-1250+(Cambridge+Medieval+Textbooks),+Cambridge+University+Press#v=onepage&q=Cyrillic%20preslav&f=false |title=Southeastern Europe in the Middle Ages, 500–1250|series=Cambridge Medieval Textbooks|author= Florin Curta|publisher=Cambridge University Press|year=2006|isbn=978-0-521-81539-0|pages= 221–222}}</ref><ref>{{cite book|chapter-url=https://books.google.com/?id=J-H9BTVHKRMC&pg=PR3-IA34&lpg=PR3-IA34&dq=The+Orthodox+Church+in+the+Byzantine+Empire+Cyrillic+preslav+eastern#v=onepage&q=%20preslav%20eastern&f=false|chapter= The Orthodox Church in the Byzantine Empire|title=Oxford History of the Christian Church|author= J. M. Hussey, Andrew Louth|publisher= Oxford University Press|year= 2010|isbn=978-0-19-161488-0|pages= 100}}</ref> It is the basis of [[alphabet]]s used in various languages, especially those of [[Orthodox Slavs|Orthodox Slavic]] origin, and non-Slavic languages influenced by Russian. {{As of|2011}}, around 250 million people in Eurasia use it as the official alphabet for their national languages, with [[Russia]] accounting for about half of them.<ref>[[List of countries by population]]</ref> With the [[accession of Bulgaria to the European Union]] on 1 January 2007, Cyrillic became the third official script of the [[European Union]], following [[Latin script|Latin]] and [[Greek alphabet|Greek]].<ref>{{cite web|author1=Leonard Orban|title=Cyrillic, the third official alphabet of the EU, was created by a truly multilingual European|url=http://europa.eu/rapid/press-release_SPEECH-07-330_en.pdf|website=europe.eu|accessdate=3 August 2014|date=24 May 2007}}</ref>\n\nCyrillic is derived from the [[Greek alphabet|Greek]] [[uncial script]], augmented by letters from the older [[Glagolitic alphabet]], including some [[Typographic ligature|ligature]]s. These additional letters were used for [[Old Church Slavonic]] sounds not found in Greek. The script is named in honor of the two [[Byzantine]]  brothers,<ref>''Columbia Encyclopedia'', Sixth Edition. 2001–05, s.v. \"Cyril and Methodius, Saints\"; ''Encyclopædia Britannica'', Encyclopædia Britannica Incorporated, Warren E. Preece – 1972, p. 846, s.v., \"Cyril and Methodius, Saints\" and \"Eastern Orthodoxy, Missions ancient and modern\"; ''Encyclopedia of World Cultures'', David H. Levinson, 1991, p. 239, s.v., \"Social Science\"; Eric M. Meyers, ''The Oxford Encyclopedia of Archaeology in the Near East'', p. 151, 1997; Lunt, ''Slavic Review'', June 1964, p. 216; Roman Jakobson, ''Crucial problems of Cyrillo-Methodian Studies''; Leonid Ivan Strakhovsky, ''A Handbook of Slavic Studies'', p. 98; V. Bogdanovich, ''History of the ancient Serbian literature'', Belgrade, 1980, p. 119</ref> [[Saints Cyril and Methodius]], who created the Glagolitic alphabet earlier on. Modern scholars believe that Cyrillic was developed and formalized by early disciples of Cyril and Methodius.\n\nIn the early 18th century, the Cyrillic script used in Russia was heavily reformed by [[Peter the Great]], who had recently returned from his [[Grand Embassy of Peter the Great|Grand Embassy]] in [[western Europe]]. The new letterforms became closer to those of the Latin alphabet; several archaic letters were removed and several letters were personally designed by Peter the Great (such as Я, which was inspired by the Latin R). West European typography culture was also adopted.<ref name=\"Civil Type\">{{cite web|title=Civil Type and Kis Cyrillic|url=http://typejournal.ru/en/articles/Civil-Type|website=typejournal.ru|accessdate=22 March 2016}}</ref>\n\n==Letters==\nCyrillic script spread throughout the East Slavic and some South Slavic territories, being adopted for writing local languages, such as [[Old East Slavic]]. Its adaptation to local languages produced a number of Cyrillic alphabets, discussed hereafter.\n\n{| cellpadding=4 style=\"font-size:larger; text-align:center;\" class=\"Unicode\" summary=\"Letters of the early Cyrillic alphabet\"\n|+ style=\"font-size:smaller;\" | The [[early Cyrillic alphabet]]<ref>А. Н. Стеценко. ''Хрестоматия по Старославянскому Языку'', 1984.</ref><ref>Cubberley, Paul. ''The Slavic Alphabets'', 1996.</ref>\n|-\n| {{script|Cyrs|{{script|Cyrs|[[A (Cyrillic)|А]]}}}} || {{script|Cyrs|[[Be (Cyrillic)|Б]]}} || {{script|Cyrs|[[Ve (Cyrillic)|В]]}} || {{script|Cyrs|[[Ge (Cyrillic)|Г]]}} || {{script|Cyrs|[[De (Cyrillic)|Д]]}} || {{script|Cyrs|[[Ye (Cyrillic)|Е]]}} || {{script|Cyrs|[[Zhe (Cyrillic)|Ж]]}} || {{script|Cyrs|[[Dze (Cyrillic)|Ѕ]]}}<ref>Variant form Ꙃ</ref> || {{script|Cyrs|[[Zemlya (Cyrillic)|Ꙁ]]}} || {{script|Cyrs|[[I (Cyrillic)|И]]}} || {{script|Cyrs|[[Dotted I (Cyrillic)|І]]}} || {{script|Cyrs|[[Ka (Cyrillic)|К]]}} || {{script|Cyrs|[[El (Cyrillic)|Л]]}} || {{script|Cyrs|[[Em (Cyrillic)|М]]}} || {{script|Cyrs|[[En (Cyrillic)|Н]]}} || {{script|Cyrs|[[O (Cyrillic)|О]]}} || {{script|Cyrs|[[Pe (Cyrillic)|П]]}} || {{script|Cyrs|[[Er (Cyrillic)|Р]]}} || {{script|Cyrs|[[Es (Cyrillic)|С]]}} || {{script|Cyrs|[[Te (Cyrillic)|Т]]}} || {{script|Cyrs|[[Uk (Cyrillic)|ОУ]]}}<ref>Variant form Ꙋ</ref> || {{script|Cyrs|[[Ef (Cyrillic)|Ф]]}}\n|-\n| {{script|Cyrs|[[Kha (Cyrillic)|Х]]}} || {{script|Cyrs|[[Omega (Cyrillic)|Ѡ]]}} || {{script|Cyrs|[[Tse (Cyrillic)|Ц]]}} || {{script|Cyrs|[[Che (Cyrillic)|Ч]]}} || {{script|Cyrs|[[Sha (Cyrillic)|Ш]]}} || {{script|Cyrs|[[Shcha|Щ]]}} || {{script|Cyrs|[[Yer|Ъ]]}} || {{script|Cyrs|[[Yery|ЪІ]]}}<ref>Variant form ЪИ</ref> || {{script|Cyrs|[[soft sign|Ь]]}} || {{script|Cyrs|[[yat|Ѣ]]}} || {{script|Cyrs|[[Iotated A (Cyrillic)|Ꙗ]]}} || {{script|Cyrs|[[Iotated E (Cyrillic)|Ѥ]]}} || {{script|Cyrs|[[Yu (Cyrillic)|Ю]]}} || {{script|Cyrs|[[yus|Ѫ]]}} || {{script|Cyrs|[[yus|Ѭ]]}} || {{script|Cyrs|[[yus|Ѧ]]}} || {{script|Cyrs|[[yus|Ѩ]]}} || {{script|Cyrs|[[Ksi (Cyrillic)|Ѯ]]}} || {{script|Cyrs|[[Psi (Cyrillic)|Ѱ]]}} || {{script|Cyrs|[[Fita|Ѳ]]}} || {{script|Cyrs|[[Izhitsa|Ѵ]]}} || {{script|Cyrs|[[Koppa (Cyrillic)|Ҁ]]}}<ref name=Lunt>Lunt, Horace G. ''Old Church Slavonic Grammar, Seventh Edition'', 2001.</ref>\n|}\n\nCapital and lowercase letters were not distinguished in old manuscripts.\n\n[[File:Meletius Smotrisky Cyrillic Alphabet.PNG|thumb|A page from the ''Church Slavonic Grammar'' of [[Meletius Smotrytsky]] (1619)]]\n\nYeri ({{script|Cyrs|Ы}}) was originally a [[ligature (typography)|ligature]] of Yer and I ({{script|Cyrs|Ъ}} + {{script|Cyrs|І}} = {{script|Cyrs|Ы}}). [[Iotation]] was indicated by ligatures formed with the letter І: {{script|Cyrs|[[Iotated A (Cyrillic)|Ꙗ]]}} (not an ancestor of modern Ya, Я, which is derived from {{script|Cyrs|Ѧ}}), {{script|Cyrs|Ѥ}}, {{script|Cyrs|Ю}} (ligature of {{script|Cyrs|І}} and {{script|Cyrs|ОУ}}), {{script|Cyrs|Ѩ}}, {{script|Cyrs|Ѭ}}. Sometimes different letters were used interchangeably, for example {{script|Cyrs|И}} = {{script|Cyrs|І}} = {{script|Cyrs|Ї}}, as were typographical variants like {{script|Cyrs|О}} = {{script|Cyrs|Ѻ}}.  There were also commonly used ligatures like {{script|Cyrs|ѠТ}} = {{script|Cyrs|Ѿ}}.\n\nThe letters also had numeric values, based not on Cyrillic alphabetical order, but inherited from the letters' [[Greek numerals|Greek ancestors]].\n\n{| cellpadding=4 style=\"text-align:center;\" class=\"Unicode\" summary=\"Letters of the Early Cyrillic alphabet\"\n|+ [[Cyrillic numerals]]\n|-\n| 1 || 2 || 3 || 4 || 5 || 6 || 7 || 8 || 9\n|-\n| {{script|Cyrs|А}} || {{script|Cyrs|В}} || {{script|Cyrs|Г}} || {{script|Cyrs|Д}} || {{script|Cyrs|Є}} || {{script|Cyrs|Ѕ}} || {{script|Cyrs|З}} || {{script|Cyrs|И}} || {{script|Cyrs|Ѳ}}\n|-\n|colspan=\"9\"|\n|-\n| 10|| 20|| 30|| 40|| 50|| 60|| 70|| 80|| 90\n|-\n| {{script|Cyrs|І}} || {{script|Cyrs|К}} || {{script|Cyrs|Л}} || {{script|Cyrs|М}} || {{script|Cyrs|Н}} || {{script|Cyrs|Ѯ}} || {{script|Cyrs|Ѻ}} || {{script|Cyrs|П}} || {{script|Cyrs|Ч}} ({{script|Cyrs|[[Koppa (Cyrillic)|Ҁ]]}})\n|-\n|colspan=\"9\"|\n|-\n|100||200||300||400||500||600||700||800||900\n|-\n| {{script|Cyrs|Р}} || {{script|Cyrs|С}} || {{script|Cyrs|Т}} || {{script|Cyrs|Ѵ}} || {{script|Cyrs|Ф}} || {{script|Cyrs|Х}} || {{script|Cyrs|Ѱ}} || {{script|Cyrs|Ѿ}} || {{script|Cyrs|Ц}}\n|}\n\nThe early Cyrillic alphabet is difficult to represent on computers. Many of the letterforms differed from those of modern Cyrillic, varied a great deal in [[manuscript]]s, and changed over time. Few fonts include [[glyph]]s sufficient to reproduce the alphabet. In accordance with [[Unicode]] policy, the standard does not include letterform variations or [[Ligature (typography)|ligatures]] found in manuscript sources unless they can be shown to conform to the Unicode definition of a character.\n\nThe Unicode 5.1 standard, released on 4 April 2008, greatly improves computer support for the early Cyrillic and the modern [[Church Slavonic]] language. In Microsoft Windows, the [[Segoe UI]] user interface font is notable for having complete support for the archaic Cyrillic letters since Windows 8.{{citation needed|date=February 2018}}\n\n{| border=\"0\" cellpadding=\"5\" cellspacing=\"0\" class=\"Unicode\" style=\"vertical-align:top; border-collapse:collapse; border:1px solid #999; text-align:center; clear:both;\"\n|-\n! colspan=12 style=\"background-color:#fbec5d; font-family:inherit; font-weight:normal;\" | '''[[Slavic languages|Slavic]] Cyrillic letters'''\n|-  style=\"vertical-align:top; background:#f8f8f8;\"\n| style=\"width:7%;\"| [[A (Cyrillic)|<big>А</big><br><small>A</small>]]\n| style=\"width:7%;\"| [[Be (Cyrillic)|<big>Б</big><br><small>Be</small>]]\n| style=\"width:7%;\"| [[Ve (Cyrillic)|<big>В</big><br><small>Ve</small>]]\n| style=\"width:7%;\"| [[Ge (Cyrillic)|<big>Г</big><br><small>Ge</small>]]\n| style=\"width:7%;\"| [[Ghe with upturn|<big>Ґ</big><br><small>Ghe upturn</small>]]\n| style=\"width:7%;\"| [[De (Cyrillic)|<big>Д</big><br><small>De</small>]]\n| style=\"width:7%;\"| [[Dje|<big>Ђ</big><br><small>Dje</small>]]\n| style=\"width:7%;\"| [[Gje|<big>Ѓ</big><br><small>Gje</small>]]\n| style=\"width:7%;\"| [[Ye (Cyrillic)|<big>Е</big><br><small>Ye</small>]]\n| style=\"width:7%;\"| [[Yo (Cyrillic)|<big>Ё</big><br><small>Yo</small>]]\n| style=\"width:7%;\"| [[Ukrainian Ye|<big>Є</big><br><small>Ukrainian Ye</small>]]\n| style=\"width:7%;\"| [[Zhe (Cyrillic)|<big>Ж</big><br><small>Zhe</small>]]\n|- valign=top\n| [[Ze (Cyrillic)|<big>З</big><br><small>Ze</small>]]\n| [[З́|<big>З́</big><br><small>Zje</small>]]\n| [[Dze|<big>Ѕ</big><br><small>Dze</small>]]\n| [[I (Cyrillic)|<big>И</big><br><small>I</small>]]\n| [[Dotted I (Cyrillic)|<big>І</big><br><small>Dotted I</small>]]\n| [[Yi (Cyrillic)|<big>Ї</big><br><small>Yi</small>]]\n| [[Short I|<big>Й</big><br><small>Short I</small>]]\n| [[Je (Cyrillic)|<big>Ј</big><br><small>Je</small>]]\n| [[Ka (Cyrillic)|<big>К</big><br><small>Ka</small>]]\n| [[El (Cyrillic)|<big>Л</big><br><small>El</small>]]\n| [[Lje|<big>Љ</big><br><small>Lje</small>]]\n| [[Em (Cyrillic)|<big>М</big><br><small>Em</small>]]\n|-  style=\"vertical-align:top; background:#f8f8f8;\"\n| [[En (Cyrillic)|<big>Н</big><br><small>En</small>]]\n| [[Nje|<big>Њ</big><br><small>Nje</small>]]\n| [[O (Cyrillic)|<big>О</big><br><small>O</small>]]\n| [[Pe (Cyrillic)|<big>П</big><br><small>Pe</small>]]\n| [[Er (Cyrillic)|<big>Р</big><br><small>Er</small>]]\n| [[Es (Cyrillic)|<big>С</big><br><small>Es</small>]]\n| [[С́|<big>С́</big><br><small>Sje</small>]]\n| [[Te (Cyrillic)|<big>Т</big><br><small>Te</small>]]\n| [[Tshe|<big>Ћ</big><br><small>Tshe</small>]]\n| [[Kje|<big>Ќ</big><br><small>Kje</small>]]\n| [[U (Cyrillic)|<big>У</big><br><small>U</small>]]\n| [[Short U (Cyrillic)|<big>Ў</big><br><small>Short U</small>]]\n|- valign=top\n| [[Ef (Cyrillic)|<big>Ф</big><br><small>Ef</small>]]\n| [[Kha (Cyrillic)|<big>Х</big><br><small>Kha</small>]]\n| [[Tse (Cyrillic)|<big>Ц</big><br><small>Tse</small>]]\n| [[Che (Cyrillic)|<big>Ч</big><br><small>Che</small>]]\n| [[Dzhe|<big>Џ</big><br><small>Dzhe</small>]]\n| [[Sha (Cyrillic)|<big>Ш</big><br><small>Sha</small>]]\n| [[Shcha|<big>Щ</big><br><small>Shcha</small>]]\n| [[Yer|<big>Ъ</big><br><small>Hard&nbsp;sign (Yer)</small>]]\n| [[Yery|<big>Ы</big><br><small>Yery</small>]]\n| [[Soft sign|<big>Ь</big><br><small>Soft&nbsp;sign (Yeri)</small>]]\n| [[E (Cyrillic)|<big>Э</big><br><small>E</small>]]\n| [[Yu (Cyrillic)|<big>Ю</big><br><small>Yu</small>]]\n|-  style=\"vertical-align:top; background:#f8f8f8;\"\n| [[Ya (Cyrillic)|<big>Я</big><br><small>Ya</small>]]\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|\n|- valign=top\n| colspan=12 style=\"background-color:#b0bf1a; font-family:inherit; font-weight:normal;\" | '''Examples of non-Slavic Cyrillic letters (see [[List of Cyrillic letters]] for more)'''\n|-  style=\"vertical-align:top; background:#f8f8f8;\"\n| [[A with breve (Cyrillic)|<big>Ӑ</big><br><small>A with<br>breve</small>]]\n| [[Schwa (Cyrillic)|<big>Ә</big><br><small>Schwa</small>]]\n| [[Ae (Cyrillic)|<big>Ӕ</big><br><small>Ae</small>]]\n| [[Ghayn (Cyrillic)|<big>Ғ</big><br><small>Ghayn</small>]]\n| [[Ge with middle hook|<big>Ҕ</big><br><small>Ge with<br>middle hook</small>]]\n| [[Ge with stroke and hook|<big>Ӻ</big><br><small>Ghayn with<br>hook</small>]]\n| [[Ge with descender|<big>Ӷ</big><br><small>Ge with<br>descender</small>]]\n| [[Zhe with breve|<big>Ӂ</big><br><small>Zhe with<br>breve</small>]]\n| [[Zhe with diaeresis|<big>Ӝ</big><br><small>Zhe with<br>diaeresis</small>]]\n| [[Abkhazian Dze|<big>Ӡ</big><br><small>Abkhazian<br>Dze</small>]]\n| [[Bashkir Qa|<big>Ҡ</big><br><small>Bashkir Qa</small>]]\n| [[Ka with stroke|<big>Ҟ</big><br><small>Ka with<br>stroke</small>]]\n|- valign=top\n| [[En with tail|<big>Ӊ</big><br><small>En with<br>tail</small>]]\n| [[En with descender|<big>Ң</big><br><small>En with<br>descender</small>]]\n| [[En with hook|<big>Ӈ</big><br><small>En with<br>hook</small>]]\n| [[En-ghe|<big>Ҥ</big><br><small>En-ghe</small>]]\n| [[Oe (Cyrillic)|<big>Ө</big><br><small>Oe</small>]]\n| [[O-hook|<big>Ҩ</big><br><small>O-hook</small>]]\n| [[Er with tick|<big>Ҏ</big><br><small>Er with<br>tick</small>]]\n| [[The (Cyrillic)|<big>Ҫ</big><br><small>The</small>]]\n| [[U with tilde (Cyrillic)|<big>У̃</big><br><small>U with<br>tilde</small>]]\n| [[U with macron (Cyrillic)|<big>Ӯ</big><br><small>U with<br>macron</small>]]\n| [[U with diaeresis (Cyrillic)|<big>Ӱ</big><br><small>U with<br>diaeresis</small>]]\n| [[U with double acute (Cyrillic)|<big>Ӳ</big><br><small>U with<br>double acute</small>]]\n|- valign=top\n| [[Ue (Cyrillic)|<big>Ү</big><br><small>Ue</small>]]\n| [[Kha with descender|<big>Ҳ</big><br><small>Kha with<br>descender</small>]]\n| [[Kha with hook|<big>Ӽ</big><br><small>Kha with<br>hook</small>]]\n| [[Kha with stroke|<big>Ӿ</big><br><small>Kha with<br>stroke</small>]]\n| [[Shha|<big>Һ</big><br><small>Shha (He)</small>]]\n| [[Te Tse (Cyrillic)|<big>Ҵ</big><br><small>Te Tse</small>]]\n| [[Che with descender|<big>Ҷ</big><br><small>Che with<br>descender</small>]]\n| [[Khakassian Che|<big>Ӌ</big><br><small>Khakassian<br>Che</small>]]\n| [[Che with vertical stroke|<big>Ҹ</big><br><small>Che with<br>vertical stroke</small>]]\n| [[Abkhazian Che|<big>Ҽ</big><br><small>Abkhazian<br>Che</small>]]\n| [[Semisoft sign|<big>Ҍ</big><br><small>Semisoft<br>sign</small>]]\n| [[Palochka|<big>Ӏ</big><br><small>Palochka</small>]]\n|- valign=top\n| colspan=12 style=\"background-color:#87ceeb;font-family:inherit; font-weight:normal;\" | '''Cyrillic letters used in the past'''\n|-  style=\"vertical-align:top; background:#f8f8f8;\"\n| [[A iotified|<big>Ꙗ</big><br><small>A&nbsp;iotified</small>]]\n| [[E iotified|<big>Ѥ</big><br><small>E&nbsp;iotified</small>]]\n| [[Yus|<big>Ѧ</big><br><small>Yus&nbsp;small</small>]]\n| [[Yus|<big>Ѫ</big><br><small>Yus&nbsp;big</small>]]\n| [[Yus|<big>Ѩ</big><br><small>Yus&nbsp;small iotified</small>]]\n| [[Yus|<big>Ѭ</big><br><small>Yus&nbsp;big iotified</small>]]\n| [[Ksi (Cyrillic)|<big>Ѯ</big><br><small>Ksi</small>]]\n| [[Psi (Cyrillic)|<big>Ѱ</big><br><small>Psi</small>]]\n| [[Yn (Cyrillic)|<big>Ꙟ</big><br><small>Yn</small>]]\n| [[Fita|<big>Ѳ</big><br><small>Fita</small>]]\n| [[Izhitsa|<big>Ѵ</big><br><small>Izhitsa</small>]]\n| [[Izhitsa okovy|<big>Ѷ</big><br><small>Izhitsa okovy</small>]]\n|- valign=top\n| [[Koppa (Cyrillic)|<big>Ҁ</big><br><small>Koppa</small>]]\n| [[Uk (Cyrillic)|<big>ОУ</big><br><small>Uk</small>]]\n| [[Omega (Cyrillic)|<big>Ѡ</big><br><small>Omega</small>]]\n| [[Ot (Cyrillic)|<big>Ѿ</big><br><small>Ot</small>]]\n| [[Yat|<big>Ѣ</big><br><small>Yat</small>]]\n|\n|\n|\n|\n|\n|\n|\n|}\n\n==Letterforms and typography==\nThe development of Cyrillic [[typography]] passed directly from the [[medieval]] stage to the late [[Baroque]], without a [[Renaissance]] phase as in [[Western Europe]]. Late Medieval Cyrillic letters (still found on many [[icon]] inscriptions today) show a marked tendency to be very tall and narrow, with strokes often shared between adjacent letters.\n\n[[Peter the Great]], Czar of Russia, mandated the use of [[Civil script|westernized letter forms]] ([[:ru:Гражданский шрифт|ru]]) in the early 18th century. Over time, these were largely adopted in the other languages that use the script. Thus, unlike the majority of modern Greek fonts that retained their own set of design principles for lower-case letters (such as the placement of [[serif]]s, the shapes of stroke ends, and stroke-thickness rules, although Greek capital letters do use Latin design principles), modern Cyrillic fonts are much the same as modern Latin fonts of the same font family. The development of some Cyrillic computer typefaces from Latin ones has also contributed to the visual Latinization of Cyrillic type.\n\n[[File:Cyrillic upright-cursive-n.png|frame|right|Letters Ge, De, I, I kratkoye, Me, Te, Tse, Be and Ve in upright (printed) and cursive (handwritten) variants. (Top is set in Georgia font, bottom in Odessa Script.)]]\nCyrillic [[capital letters|uppercase]] and [[lower case|lowercase]] letter forms are not as differentiated as in Latin typography. Upright Cyrillic lowercase letters are essentially [[small caps|small capitals]] (with exceptions: Cyrillic {{angle bracket|а}}, {{angle bracket|е}}, {{angle bracket|і}}, {{angle bracket|ј}}, {{angle bracket|р}}, and {{angle bracket|у}} adopted Western lowercase shapes, lowercase {{angle bracket|ф}} is typically designed under the influence of Latin {{angle bracket|p}}, lowercase {{angle bracket|б}}, {{angle bracket|ђ}} and {{angle bracket|ћ}} are traditional handwritten forms), although a good-quality Cyrillic typeface will still include separate small-caps glyphs.<ref>Bringhurst (2002) writes \"in Cyrillic, the difference between normal lower case and small caps is more subtle than it is in the Latin or Greek alphabets,{{nbsp}}...\" (p 32) and \"in most Cyrillic faces, the lower case is close in color and shape to Latin small caps\" (p 107).</ref>\n\nCyrillic fonts, as well as Latin ones, have [[roman type|roman]] and [[italic type|italic]] types (practically all popular modern fonts include parallel sets of Latin and Cyrillic letters, where many glyphs, uppercase as well as lowercase, are simply shared by both). However, the native font terminology in most Slavic languages (for example, in Russian) does not use the words \"roman\" and \"italic\" in this sense.<ref>Name ''{{lang|ru-Latn|ital'yanskiy shrift}}'' (Italian font) in Russian refers to a particular font family [http://citforum.univ.kiev.ua/open_source/fonts/theory/thumbs/ris320.jpg JPG] {{webarchive|url=https://web.archive.org/web/20070926182512/http://citforum.univ.kiev.ua/open_source/fonts/theory/thumbs/ris320.jpg |date=26 September 2007 }}, whereas ''{{lang|ru-Latn|rimskiy shrift}}'' (roman font) is just a synonym for Latin font, Latin alphabet.</ref> Instead, the nomenclature follows German naming patterns:\n\n[[File:Cyrillic cursive.svg|thumb|upright=1.35|Cyrillic letters in cursive]]\n*Roman type is called ''{{lang|ru-Latn|pryamoy shrift}}'' (\"upright type\")—compare with ''{{lang|de|Normalschrift}}'' (\"regular type\") in German\n*Italic type is called ''{{lang|ru-Latn|kursiv}}'' (\"cursive\") or ''{{lang|ru-Latn|kursivniy shrift}}'' (\"cursive type\")—from the German word ''{{lang|de|Kursive}}'', meaning italic typefaces and not cursive writing\n*[[Cursive]] handwriting is ''{{lang|ru-Latn|rukopisniy shrift}}'' (\"handwritten type\") in Russian—in German: ''{{lang|de|[[:de:Kurrentschrift|Kurrentschrift]]}}'' or ''{{lang|de|Laufschrift}}'', both meaning literally 'running type'\n\nAs in Latin typography, a [[sans-serif]] face may have a mechanically sloped oblique type (''{{lang|ru-Latn|naklonniy shrift}}''—\"sloped\", or \"slanted type\") instead of italic.\n\nSimilarly to Latin fonts, italic and cursive types of many Cyrillic letters (typically lowercase; uppercase only for handwritten or stylish types) are very different from their upright roman types. In certain cases, the correspondence between uppercase and lowercase glyphs does not coincide in Latin and Cyrillic fonts: for example, italic Cyrillic <span style=\"font-family: times, 'Times New Roman', serif; font-size: larger\">{{angle bracket|''т''}}</span> is the lowercase counterpart of {{angle bracket|''Т''}} not of {{angle bracket|''М''}}.\n\nA boldfaced type is called ''{{lang|ru-Latn|poluzhirniy shrift}}'' (\"semi-bold type\"), because there existed fully boldfaced shapes that have been out of use since the beginning of the 20th century. A bold italic combination (bold slanted) does not exist for all font families.\n\nIn Standard Serbian, as well as in Macedonian,<ref>{{cite book |title=Pravopis na makedonskiot jazik |date=2017 |publisher=Institut za makedonski jazik Krste Misirkov |location=Skopje |isbn=978-608-220-042-2 |page=3 |url=http://www.pravopis.mk/sites/default/files/Pravopis-2017.PDF |ref=MakedonskiPravopis}}</ref> some italic and cursive letters are allowed to be different to resemble more to the handwritten letters. The regular (upright) shapes are generally standardized among languages and there are no officially recognized variations.<ref>{{cite book |last1=Peshikan |first1=Mitar |last2=Jerković |first2=Jovan |last3=Pižurica |first3=Mato |title=Pravopis srpskoga jezika |date=1994 |publisher=Matica Srpska |location=Beograd |isbn=978-86-363-0296-5 |page=42 |ref=PravopisSrpskog}}</ref>\n\nThe following table shows the differences between the upright and italic Cyrillic letters of the [[Russian alphabet]]. Italic forms significantly different from their upright analogues, or especially confusing to users of a Latin alphabet, are highlighted.\n\n{| border=0 cellpadding=4 cellspacing=1 style=\"padding:0 .5em .2em; border:1px solid #999; margin:1em 0;\"\n|+ align=bottom style=\"text-align:left; font-size:smaller; \" | Also available as a [[commons:Image:Cyrillic-italics-nonitalics.png|graphical image]].\n|- style=\"font-family:FreeSerif,Georgia,'Times New Roman','Nimbus Roman No9 L','Century Schoolbook L','Trebuchet MS','URW Bookman L','URW Chancery L','URW Palladio L',Teams,serif; font-size:large; text-align:center; \"\n| а || б || в || г || д || е || ё || ж || з || и || й || к || л || м || н || о || п || р || с || т || у || ф || х || ц || ч || ш || щ || ъ || ы || ь || э || ю || я\n|- style=\"font-family:FreeSerif,Georgia,'Times New Roman','Nimbus Roman No9 L','Century Schoolbook L','Trebuchet MS','URW Bookman L','URW Chancery L','URW Palladio L',Teams,serif; font-size:large; text-align:center; \"\n||''а'' || ''б'' ||bgcolor=\"#BBBBFF\"| ''в'' ||bgcolor=\"#BBBBFF\"| ''г'' ||bgcolor=\"#BBBBFF\"| ''д'' || ''е'' || ''ё'' || ''ж'' || ''з'' ||bgcolor=\"#BBBBFF\"| ''и'' ||bgcolor=\"#BBBBFF\"| ''й'' || ''к'' ||bgcolor=\"#BBBBFF\"| ''л'' || ''м'' || ''н'' || ''о'' ||bgcolor=\"#BBBBFF\"| ''п'' || ''р'' || ''с'' ||bgcolor=\"#BBBBFF\"| ''т'' || ''у'' || ''ф'' || ''х'' ||bgcolor=\"#BBBBFF\"| ''ц'' || ''ч'' ||bgcolor=\"#BBBBFF\"| ''ш'' ||bgcolor=\"#BBBBFF\"| ''щ'' || ''ъ'' || ''ы'' || ''ь'' || ''э'' || ''ю'' || ''я''\n|}\n\nNote: in some fonts or styles, lowercase italic Cyrillic {{angle bracket|д}} ({{angle bracket|''д''}}) may look like Latin {{angle bracket|''g''}} and lowercase italic Cyrillic {{angle bracket|т}} ({{angle bracket|''т''}}) may look exactly like a capital italic {{angle bracket|T}} ({{angle bracket|''T''}}), only smaller.\n\n[[File:Cyrillic alphabet world distribution.svg|thumb|upright=2|Distribution of the Cyrillic script worldwide:<br>\n{{legend|#0b280b|Cyrillic is the sole official script.}}\n{{legend|#217821|Cyrillic is co-official with another alphabet. In the cases of Moldova and Georgia, this is in breakaway regions not recognized by the central government.}}\n{{legend|#87de87|Cyrillic is not official, but is in common use as a legacy script.}}{{legend|#999999|Cyrillic is not widely used}}]]\n\n==Cyrillic alphabets==\n{{Main|Cyrillic alphabets}}\n\nAmong others, Cyrillic is the standard script for writing the following languages:\n*'''Slavic languages''': [[Belarusian language|Belarusian]], [[Bulgarian language|Bulgarian]], [[Macedonian language|Macedonian]], [[Russian language|Russian]], [[Rusyn language|Rusyn]], [[Serbo-Croatian]] (for [[Serbian language|Standard Serbian]], [[Bosnian language|Bosnian]], and [[Montenegrin language|Montenegrin]]), [[Ukrainian language|Ukrainian]]\n*'''Non-Slavic languages''': [[Abkhaz language|Abkhaz]], [[Aleut language|Aleut]] (now mostly in church texts), [[Bashkir language|Bashkir]], [[Chuvash language|Chuvash]], [[Erzya language|Erzya]], [[Kazakh language|Kazakh]] (to be replaced by Latin script by 2025<ref name=\"KazSwitch\">{{Cite news|url=https://www.theguardian.com/world/2017/oct/26/kazakhstan-switch-official-alphabet-cyrillic-latin|title=Alphabet soup as Kazakh leader orders switch from Cyrillic to Latin letters|last=Reuters|date=2017-10-26|work=The Guardian|access-date=2017-10-30|language=en-GB|issn=0261-3077}}</ref>), [[Kildin Sami language|Kildin Sami]], [[Komi language|Komi]], [[Kyrgyz language|Kyrgyz]], [[Dungan language|Dungan]], [[Mari language|Mari]], [[Moksha language|Moksha]], [[Mongolian language|Mongolian]], [[Ossetic language|Ossetic]], [[Romani orthography|Romani]] (some dialects), [[Sakha language|Sakha/Yakut]], [[Tajik language|Tajik]], [[Tatar language|Tatar]], [[Tlingit alphabet#Cyrillic alphabets|Tlingit]] (now only in church texts), [[Tuvan language|Tuvan]], [[Udmurt language|Udmurt]], [[Siberian Yupik language|Yuit]] (Siberian Yupik), and [[Yupik languages#Writing systems|Yupik]] (in [[Alaska]]).\n\nThe Cyrillic script has also been used for languages of Alaska,<ref>[http://www.asna.ca/alaska/ \"Orthodox Language Texts\"], Retrieved 2011-06-20</ref> [[Slavic Europe]] (except for [[Western Slavs|Western Slavic]] and some [[Southern Slavs|Southern Slavic]]), the [[Caucasus]], [[Siberia]], and the [[Russian Far East]].\n\nThe first alphabet derived from Cyrillic was [[Abur]], used for the [[Komi language]]. Other Cyrillic alphabets include the [[Molodtsov alphabet]] for the Komi language and various alphabets for [[Caucasian languages]].\n\n==Name==\n[[File:Cyrillic monument.jpg|thumb|165px|Cyrillic Script Monument in [[Antarctica]]]]\nSince the script was conceived and popularised by the followers of [[Cyril and Methodius]], rather than by Cyril and Methodius themselves, its name denotes homage rather than authorship. The name \"Cyrillic\" often confuses people who are not familiar with the script's history, because it does not identify a country of origin (in contrast to the \"Greek alphabet\"). Among the general public, it is often called \"the Russian alphabet,\" because Russian is the most popular and influential alphabet based on the script. Some Bulgarian intellectuals, notably [[Stefan Tsanev]], have expressed concern over this, and have suggested that the Cyrillic script be called the \"Bulgarian alphabet\" instead, for the sake of historical accuracy.<ref>Tsanev, Stefan. ''Български хроники, том 4 (Bulgarian Chronicles, Volume 4)'', Sofia, 2009, p. 165</ref>\n\nIn Bulgarian, Macedonian, Russian, and Serbian, the Cyrillic alphabet is also known as ''azbuka'', derived from the old names of the first two letters of most Cyrillic alphabets (just as the term ''alphabet'' came from the first two Greek letters ''alpha'' and ''beta'').\n\n==History==\n{{Main|Early Cyrillic alphabet}}\n[[File:Azbuka 1574 by Ivan Fyodorov.png|thumb|left|upright=1.1|A page from Азбука (Читанка) (ABC (Reader)), the first Ruthenian language textbook, printed by [[Ivan Fyodorov (printer)|Ivan Fyodorov]] in 1574. This page features the Cyrillic alphabet.]]\n{{Alphabet}}\n\nThe Cyrillic script was created in the [[First Bulgarian Empire]].<ref name=Cubberley1996>Paul Cubberley (1996) \"The Slavic Alphabets\". In Daniels and Bright, eds. ''The World's Writing Systems.'' Oxford University Press. {{ISBN|0-19-507993-0}}.</ref> Its first variant, the [[Early Cyrillic alphabet]], was created at the [[Preslav Literary School]]. It is derived from the [[Greek alphabet|Greek uncial script]] letters, augmented by [[Typographic ligature|ligature]]s and consonants from the older [[Glagolitic alphabet]] for sounds not found in Greek. Tradition holds that Cyrillic and Glagolitic were formalized either by [[Saints Cyril and Methodius]] who brought Christianity to the southern Slavs, or by their disciples.<ref name=\"Columbia Encyclopedia 1972, p.846\">''Columbia Encyclopedia'', Sixth Edition. 2001–05, s.v. \"Cyril and Methodius, Saints\"; ''Encyclopædia Britannica'', Encyclopædia Britannica Incorporated, Warren E. Preece – 1972, p.846, s.v., \"Cyril and Methodius, Saints\" and \"Eastern Orthodoxy, Missions ancient and modern\"; ''Encyclopedia of World Cultures'', David H. Levinson, 1991, p.239, s.v., \"Social Science\"; Eric M. Meyers, ''The Oxford Encyclopedia of Archaeology in the Near East'', p.151, 1997; Lunt, ''Slavic Review'', June, 1964, p. 216; Roman Jakobson, ''Crucial problems of Cyrillo-Methodian Studies''; Leonid Ivan Strakhovsky, ''A Handbook of Slavic Studies'', p.98; V. Bogdanovich, ''History of the ancient Serbian literature'', Belgrade, 1980, p.119</ref><ref name=\"ReferenceB\">The Columbia Encyclopaedia, Sixth Edition. 2001–05, O.Ed. Saints Cyril and Methodius \"Cyril and Methodius, Saints) 869 and 884, respectively, \"Greek missionaries, brothers, called Apostles to the Slavs and fathers of Slavonic literature.\"</ref><ref name=BritGlago>Encyclopædia Britannica, ''Major alphabets of the world, Cyrillic and Glagolitic alphabets'', 2008, O.Ed. \"The two early Slavic alphabets, the Cyrillic and the Glagolitic, were invented by St. Cyril, or Constantine (c. 827–869), and St. Methodii (c. 825–884). These men from Thessaloniki who became apostles to the southern Slavs, whom they converted to Christianity.\"</ref><ref>{{Cite book | last1 = Kazhdan | first1 = Alexander P. | title = The Oxford dictionary of Byzantium | year = 1991 | publisher = Oxford University Press | location = New York | isbn = 978-0-19-504652-6 | page = 507|quote=Constantine (Cyril) and his brother Methodius were the sons of the droungarios Leo and Maria, who may have been a Slav.}}</ref> Paul Cubberley posits that although Cyril may have codified and expanded Glagolitic, it was his students in the [[First Bulgarian Empire]] under Tsar [[Simeon the Great]] that developed Cyrillic from the Greek letters in the 890s as a more suitable script for church books.<ref name=Cubberley1996/> Later Cyrillic spread among other Slavic peoples, as well as among non-Slavic [[Vlachs]].\n\nCyrillic and [[Glagolitic]] were used for the [[Church Slavonic language]], especially the [[Old Church Slavonic]] variant. Hence expressions such as \"И is the tenth Cyrillic letter\" typically refer to the order of the Church Slavonic alphabet; not every Cyrillic alphabet uses every letter available in the script.\n\nThe Cyrillic script came to dominate Glagolitic in the 12th century. The literature produced in the Old Bulgarian language soon spread north and became the [[lingua franca]] of the Balkans and Eastern Europe, where it came to also be known as [[Old Church Slavonic]].<ref name=\"ReferenceA\">\"On the relationship of old Church Slavonic to the written language of early Rus'\" Horace G. Lunt; Russian Linguistics, Volume 11, Numbers 2–3 / January, 1987</ref><ref>{{cite book\n|last=Schenker\n|first=Alexander\n|title=The Dawn of Slavic\n|publisher=Yale University Press\n|year=1995\n|location=\n|pages=185–186, 189–190\n|url=\n|doi=\n|id=\n|isbn=}}</ref><ref>{{cite book\n|last=Lunt\n|first=Horace\n|title=Old Church Slavonic Grammar\n|publisher=Mouton de Gruyter\n|pages=3–4\n|isbn=}}</ref><ref>{{cite book\n|last=Wien\n|first=Lysaght\n|title=Old Church Slavonic (Old Bulgarian)-Middle Greek-Modern English dictionary\n|publisher=Verlag Bruder Hollinek\n|year=1983}}</ref><ref name=fortson>Benjamin W. Fortson. ''Indo-European Language and Culture: An Introduction'', p. 374</ref> The alphabet used for the modern [[Church Slavonic language]] in [[Eastern Orthodox Church|Eastern Orthodox]] and [[Eastern Catholic]] rites still resembles early Cyrillic. However, over the course of the following millennium, Cyrillic adapted to changes in spoken language, developed regional variations to suit the features of national languages, and was subjected to academic reform and political decrees. A notable example of such linguistic reform can be attributed to [[Vuk Karadžić|Vuk Stefanović Karadžić]] who updated the [[Serbian Cyrillic alphabet]] by removing certain graphemes no longer represented in the vernacular, and introducing graphemes specific to Serbian (i.e. Љ Њ Ђ Ћ Џ Ј), distancing it from Church Slavonic alphabet in use prior to the reform. Today, [[Languages using Cyrillic|many languages]] in the [[Languages of the Balkans|Balkans]], Eastern Europe, and [[Eurasiatic languages|northern Eurasia]] are written in Cyrillic alphabets.\n\n==Relationship to other writing systems==\n===Latin script===\nA number of languages written in a Cyrillic alphabet have also been written in a [[Latin alphabet]], such as [[Azerbaijani language|Azerbaijani]], [[Uzbek language|Uzbek]], [[Serbian language|Serbian]] and [[Romanian Language|Romanian]] (in the [[Republic of Moldova]] until 1989, in [[Romania]] throughout the 19th century). After the disintegration of the Soviet Union in 1991, some of the former republics officially shifted from Cyrillic to Latin. The transition is complete in most of Moldova (except the breakaway region of [[Transnistria]], where [[Moldovan Cyrillic alphabet|Moldovan Cyrillic]] is official), [[Turkmenistan]], and [[Azerbaijan]]. [[Uzbekistan]] still uses both systems, and [[Kazakhstan]] has officially begun a transition from Cyrillic to Latin (scheduled to be complete by 2025). The [[Russia]]n government has mandated that Cyrillic must be used for all public communications in all [[federal subjects of Russia]], to promote closer ties across the federation.{{citation needed|date=January 2011}} This act was controversial for speakers of many Slavic languages; for others, such as [[Chechen language|Chechen]] and [[Ingush language|Ingush]] speakers, the law had political ramifications. For example, the separatist Chechen government mandated a Latin script which is still used by many Chechens. Those in the diaspora especially refuse to use the Chechen Cyrillic alphabet, which they associate with Russian imperialism.\n\n[[File:Scripts of European national languages.png|thumb|upright=1.2|Map of European countries by script of national language.{{Fix|link=WP:CSB|text=Eurocentric bias?|title=This map should represent all Cyrillic-alphabet regions including Central Asia and Mongolia, or the entire globe.}}\n{{legend-table|lang=en\n|title=Alphabets in Europe\n|#008000|Greek\n|#008080|Greek & Latin\n|#000080|Latin\n|#800080|Latin and Cyrillic\n|#FF0000|Cyrillic\n|#FF6600|Georgian\n|#FFCC00|Armenian\n}}]]\n\nStandard [[Serbian language|Serbian]] uses [[Serbian language#Writing system|both the Cyrillic and Latin scripts]]. Cyrillic is nominally the official script of Serbia's administration according to the Serbian constitution;<ref>[http://www.ustavni.sud.rs/page/view/en-GB/235-100028/constitution Serbian constitution]</ref> however, the law does not regulate scripts in standard language, or standard language itself by any means. In practice the scripts are equal, with Latin being used more often in a less official capacity.<ref>{{Cite journal | url=http://www.csmonitor.com/World/Europe/2008/0529/p20s01-woeu.html | title=Serbian signs of the times are not in Cyrillic| journal=Christian Science Monitor| date=2008-05-29}}</ref>\n\nThe [[Zhuang alphabet]], used between the 1950s and 1980s in portions of the People's Republic of China, used a mixture of Latin, phonetic, numeral-based, and Cyrillic letters. The non-Latin letters, including Cyrillic, were removed from the alphabet in 1982 and replaced with Latin letters that closely resembled the letters they replaced.\n\n===Romanization===\n<!-- NON VALID LINK {{main|Romanization of Cyrillic}} -->\n\nThere are various systems for [[Romanization]] of Cyrillic text, including [[transliteration]] to convey Cyrillic spelling in [[Latin]] letters, and [[Transcription (linguistics)|transcription]] to convey [[pronunciation]].\n\nStandard Cyrillic-to-Latin transliteration systems include:\n*[[Scientific transliteration of Cyrillic|Scientific transliteration]], used in linguistics, is based on the [[Gaj's Latin alphabet|Bosnian and Croatian Latin alphabet]].\n*The Working Group on Romanization Systems<ref>[http://www.eki.ee/wgrs/ ''UNGEGN Working Group on Romanization Systems'']</ref> of the [[United Nations]] recommends different systems for specific languages. These are the most commonly used around the world.\n*[[ISO 9]]:1995, from the International Organization for Standardization.\n*American Library Association and Library of Congress Romanization tables for Slavic alphabets ([[ALA-LC Romanization]]), used in North American libraries.\n*[[BGN/PCGN Romanization]] (1947), United States Board on Geographic Names & Permanent Committee on Geographical Names for British Official Use).\n*[[GOST 16876-71|GOST 16876]], a now defunct Soviet transliteration standard. Replaced by GOST 7.79, which is [[ISO 9]] equivalent.\n*Various [[informal romanizations of Cyrillic]], which adapt the Cyrillic script to Latin and sometimes Greek glyphs for compatibility with small character sets.\n\nSee also [[Romanization of Belarusian]], [[Romanization of Bulgarian|Bulgarian]], [[romanization of Kyrgyz|Kyrgyz]], [[romanization of Russian|Russian]], [[romanization of Macedonian|Macedonian]] and [[romanization of Ukrainian|Ukrainian]].\n\n===Cyrillization===\nRepresenting other writing systems with Cyrillic letters is called [[Cyrillization]].\n\n==Computer encoding==\n===Unicode===\n{{Main|Cyrillic script in Unicode}}\n\nAs of Unicode version 12.0, Cyrillic letters, including national and historical alphabets, are encoded across several [[Unicode block|blocks]]:\n*[[Cyrillic (Unicode block)|Cyrillic]]: [https://www.unicode.org/charts/PDF/U0400.pdf U+0400–U+04FF]\n*[[Cyrillic Supplement]]: [https://www.unicode.org/charts/PDF/U0500.pdf U+0500–U+052F]\n*[[Cyrillic Extended-A]]: [https://www.unicode.org/charts/PDF/U2DE0.pdf U+2DE0–U+2DFF]\n*[[Cyrillic Extended-B]]: [https://www.unicode.org/charts/PDF/UA640.pdf U+A640–U+A69F]\n*[[Cyrillic Extended-C]]: [https://www.unicode.org/charts/PDF/U1C80.pdf U+1C80–U+1C8F]\n*[[Phonetic Extensions]]: [https://www.unicode.org/charts/PDF/U1D00.pdf U+1D2B, U+1D78]\n*[[Combining Half Marks]]: [https://www.unicode.org/charts/PDF/UFE20.pdf U+FE2E–U+FE2F]\n\nThe characters in the range U+0400 to U+045F are basically the characters from [[ISO 8859-5]] moved upward by 864 positions. The characters in the range U+0460 to U+0489 are historic letters, not used now. The characters in the range U+048A to U+052F are additional letters for various languages that are written with Cyrillic script.\n\nUnicode as a general rule does not include accented Cyrillic letters. A few exceptions include:\n*combinations that are considered as separate letters of respective alphabets, like [[Й]], [[Ў]], [[Ё]], [[Ї]], [[Ѓ]], [[Ќ]] (as well as many letters of non-Slavic alphabets);\n*two most frequent combinations orthographically required to distinguish [[homonym]]s in Bulgarian and Macedonian: [[Ѐ]], [[Ѝ]];\n*a few Old and New Church Slavonic combinations: [[Ѷ]], [[Ѿ]], [[Ѽ]].\n\nTo indicate stressed or long vowels, [[combining diacritical mark]]s can be used after the respective letter (for example, {{unichar|0301|combining acute accent|cwith=◌}}: ы́ э́ ю́ я́ etc.).\n\nSome languages, including [[Church Slavonic language|Church Slavonic]], are still not fully supported.{{Citation needed|date=June 2015}}\n\nUnicode 5.1, released on 4 April 2008, introduces major changes to the Cyrillic blocks. Revisions to the existing Cyrillic blocks, and the addition of Cyrillic Extended A (2DE0 ... 2DFF) and Cyrillic Extended B (A640 ... A69F), significantly improve support for the [[early Cyrillic alphabet]], [[Abkhaz language|Abkhaz]], [[Aleut language|Aleut]], [[Chuvash language|Chuvash]], [[Kurdish language|Kurdish]], and [[Moksha language|Moksha]].<ref>{{cite web| url= http://std.dkuug.dk/jtc1/sc2/wg2/docs/n3194.pdf |title = IOS Universal Multiple-Octet Coded Character Set|format=PDF | accessdate=2012-06-13}}</ref>\n\n===Other===\nPunctuation for Cyrillic text is similar to that used in European Latin-alphabet languages.\n\nOther [[character encoding]] systems for Cyrillic:\n*[[CP866]] – 8-bit Cyrillic character encoding established by [[Microsoft]] for use in [[MS-DOS]] also known as GOST-alternative. Cyrillic characters go in their native order, with a \"window\" for pseudographic characters.\n*[[ISO/IEC 8859-5]] – 8-bit Cyrillic character encoding established by [[International Organization for Standardization]]\n*[[KOI8-R]] – 8-bit native Russian character encoding. Invented in the USSR for use on Soviet clones of American IBM and DEC computers. The Cyrillic characters go in the order of their Latin counterparts, which allowed the text to remain readable after transmission via a 7-bit line that removed the [[most significant bit]] from each byte—the result became a very rough, but readable, Latin transliteration of Cyrillic. Standard encoding of early 1990s for [[Unix]] systems and the first Russian Internet encoding.\n*[[KOI8-U]] – KOI8-R with addition of Ukrainian letters.\n*[[MIK Code page|MIK]] – 8-bit native Bulgarian character encoding for use in [[Microsoft]] [[DOS]].\n*[[Windows-1251]] – 8-bit Cyrillic character encoding established by Microsoft for use in [[Microsoft Windows]]. The simplest 8-bit Cyrillic encoding—32 capital chars in native order at 0xc0–0xdf, 32 usual chars at 0xe0–0xff, with rarely used \"YO\" characters somewhere else. No pseudographics. Former standard encoding in some [[GNU]]/[[Linux]] distributions for Belarusian and Bulgarian, but currently displaced by [[UTF-8]].\n*GOST-main.\n*[[GB 2312]] – Principally simplified Chinese encodings, but there are also the basic 33 Russian Cyrillic letters (in upper- and lower-case).\n*[[JIS encoding|JIS]] and [[Shift JIS]] – Principally Japanese encodings, but there are also the basic 33 Russian Cyrillic letters (in upper- and lower-case).\n\n===Keyboard layouts===\n{{see also|Keyboard layout#Keyboard layouts for non-Latin alphabetic scripts|label 1=Keyboard layouts for non-Latin alphabetic scripts}}\n\nEach language has its own standard [[keyboard layout]], adopted from [[typewriter]]s. With the flexibility of computer input methods, there are also transliterating or phonetic/homophonic keyboard layouts made for typists who are more familiar with other layouts, like the common English [[QWERTY keyboard]]. When practical Cyrillic keyboard layouts or fonts are unavailable, computer users sometimes use transliteration or look-alike [[volapuk encoding|\"volapuk\" encoding]] to type in languages that are normally written with the Cyrillic alphabet.\n\n==See also==\n{{Portal|Writing|Languages}}\n*[[Languages using Cyrillic]]\n*[[List of Cyrillic letters]]\n*[[Cyrillic digraphs]]\n*[[List of Cyrillic digraphs]]\n*[[Cyrillic (Unicode block)]]\n*[[Cyrillic Alphabet Day]]\n*[[Faux Cyrillic]], real or fake Cyrillic letters used to give Latin-alphabet text a Soviet or Russian feel\n*[[Russian cursive]]\n*[[Russian manual alphabet]]\n*[[Yugoslav manual alphabet]]\n*[[Russian Braille]]\n*[[Yugoslav Braille]]\n*[[Vladislav the Grammarian]]\n*Internet [[top-level domains]] in Cyrillic: [[List of Internet top-level domains#Cyrillic script|gTLDs]], [[.мон]], [[.бг]], [[.қаз]], [[.рф]], [[.срб]], [[.укр]]\n\n==Notes==\n{{Reflist}}\n\n==References==\n*Ivan G. Iliev. Short History of the Cyrillic Alphabet. Plovdiv. 2012. [http://ivanilievlogosmaster.blogspot.com/2012/07/scriptura-mundi-cyrillic-alphabet.html Short History of the Cyrillic Alphabet]\n*[[Robert Bringhurst|Bringhurst, Robert]] (2002). ''[[The Elements of Typographic Style]]'' (version 2.5), pp.&nbsp;262–264. Vancouver, Hartley & Marks. {{ISBN|0-88179-133-4}}.\n*Nezirović, M. (1992). ''Jevrejsko-španjolska književnost''. Sarajevo: Svjetlost. [cited in Šmid, 2002]\n*Šmid, Katja (2002). \"{{cite web|url=http://hispanismo.cervantes.es/documentos/smidX.pdf |title=Los problemas del estudio de la lengua sefardí |deadurl=yes |archiveurl=https://web.archive.org/web/20080407074136/http://hispanismo.cervantes.es/documentos/smidX.pdf |archivedate=7 April 2008 |df= }}&nbsp;{{small|(603 [[Kibibyte|KiB]])}}\", in ''Verba Hispanica'', vol X. Liubliana: Facultad de Filosofía y Letras de la Universidad de Liubliana. {{ISSN|0353-9660}}.\n*'The Lives of St. Tsurho and St. Strahota', Bohemia, 1495, Vatican Library\n*Philipp Ammon: [http://sjani.ge/sjani-17/ფილიპპ%20ამონი.pdf ''Tractatus slavonicus''.] in: ''Sjani (Thoughts) Georgian Scientific Journal of Literary Theory and Comparative Literature'', N 17, 2016, pp.&nbsp;248–256\n\n==External links==\n{{Commons}}\n{{wiktionary|Appendix:Cyrillic script}}\n*[http://czyborra.com/charsets/cyrillic.html The Cyrillic Charset Soup] overview and history of Cyrillic charsets.\n*[http://transliteration.eki.ee/ Transliteration of Non-Roman Scripts], a collection of writing systems and transliteration tables\n*[http://www.omniglot.com/writing/cyrillic.htm History and development of the Cyrillic alphabet]\n*[http://localfonts.eu/cyrillic-alphabets-of-slavic-languages/ Cyrillic Alphabets of Slavic Languages] review of Cyrillic charsets in Slavic Languages.\n*[http://andregarzia.on-rev.com/richmond/LANGTOOLS.html data entry in Old Cyrillic / Стара Кирилица]\n*[http://blog-en.namepedia.org/2015/04/cyrillic-and-its-long-journey-east/ Cyrillic and its Long Journey East - NamepediA Blog], article about the Cyrillic script\n*{{cite podcast | url=  https://soundcloud.com/chssedinburgh/vladimir-alpatov-latin-alphabet-for-the-russian-language | author= [[Vladimir Mikhaylovich Alpatov|Vladimir M. Alpatov]]  | publisher= The University of Edinburgh | title= Latin Alphabet for the Russian Language | website= Soundcloud | date= 24 January 2013 |access-date= 28 January 2016 }}\n\n{{Slavic languages}}\n{{List of writing systems}}\n{{Cyrillization}}\n{{ISO 15924/footer}}\n{{Authority control}}\n\n{{DEFAULTSORT:Cyrillic alphabet}}\n[[Category:Cyrillic script| ]]\n[[Category:Alphabets]]\n[[Category:Scripts encoded in Unicode 1.0]]\n[[Category:Bulgarian inventions]]\n[[Category:Eastern Europe]]\n[[Category:North Asia]]\n[[Category:Central Asia]]"
    },
    {
      "title": "Defective script",
      "url": "https://en.wikipedia.org/wiki/Defective_script",
      "text": "A '''defective script''' is a [[writing system]] that does not represent all the [[phoneme|phonemic]] distinctions of a language.<ref name=\"Sampson\">{{cite book|last=Sampson|first=Geoffrey|year=1985|title=Writing Systems|publisher=Stanford University Press|isbn=978-0-8047-1756-4}}</ref>{{rp|36-38}}<ref>{{cite book|last=Coulmas|first=Florian|year=1996|title=The Blackwell Encyclopedia of Writing Systems|publisher=Blackwell|isbn=978-0-631-21481-6}}</ref>{{rp|118}} For example, [[Italian language|Italian]] has seven [[vowel]]s, but the [[Italian alphabet]] has only five vowel [[letter (alphabet)|letter]]s to represent them; in general, the difference between [[Vowel#height|close]] {{IPA|/e, o/}} and [[Vowel#height|open]] {{IPA|/ɛ, ɔ/}} is simply ignored, though stress marks, if used, may distinguish them. Among the [[consonant]]s, both {{IPA|/s/}} and {{IPA|/z/}} are written {{angbr|{{lang|it|s}}}}, and both {{IPA|/ts/}} and {{IPA|/dz/}} are written {{angbr|{{lang|it|z}}}}, though not many words are distinguished by the latter. [[Stress (linguistics)|Stress]] and [[hiatus (linguistics)|hiatus]] are not reliably distinguished.<ref>{{cite book|last=Danesi|first=Marcel|year=1996|title=Italian the Easy way|url=https://books.google.com/books?id=_RuiM7-I7ScC|isbn=9780812091465}}</ref>\n\nSuch shortcomings are not uncommon. The [[Greek alphabet]] was defective during its early history. [[Ancient Greek language|Ancient Greek]] had distinctive [[vowel length]]: five short vowels, {{IPA|/i e a o u/}}, and seven long vowels, {{IPA|/iː eː ɛː aː ɔː oː uː/}}. When the [[Phoenician alphabet]] was adapted to Greek, the names of five letters were pronounced by the Greeks with initial consonants made silent, and were then used [[acrophony|acrophonically]] to represent vowels. These were ''[[alpha (letter)|alpha]]'', ''e'' (later called ''[[epsilon|e psilon]]''), ''[[iota (letter)|iota]]'', ''o'' (later called ''[[omicron (letter)|o micron]]''), and ''u'' (later called ''[[upsilon (letter)|u psilon]]'') &ndash; <{{lang|grc|α, ε, ι, ο, υ}}> &ndash; five letters for twelve vowel sounds. Later the [h] dropped  from the Eastern Greek dialects, and the letter ''heta'' (now pronounced ''[[eta (letter)|eta]]'') became available; it was used for {{IPA|/ɛː/}}. About the same time the Greeks created an additional letter, ''[[omega (letter)|o mega]]'', probably by writing ''o micron'' with an underline, that was used for {{IPA|/ɔː/}}. [[Digraph (orthography)|Digraph]]s ''ei'' and ''ou'' were adopted for {{IPA|/eː/}} and {{IPA|/oː/.}} Thus Greek entered its classical era with seven letters and two digraphs &ndash; <{{lang|grc|α, ε, ι, ο, υ, η, ω, ει, ου}}> &ndash; for twelve vowel sounds. Long {{IPA|/iː aː uː/}} were never distinguished from short {{IPA|/i a u/}}, even though the distinction was meaningful. Although the Greek alphabet was a good match to the consonants of the language, it was defective when it came to some vowels.<ref>{{cite book|author=Pierre Swiggers|editor=P.T. Daniels & W. Bright|title=The World's Writing Systems|year=1996|publisher=Oxford University Press|isbn=978-0-19-507993-7|chapter=Transmission of the Phoenician Script to the West}}</ref><ref>{{cite book|author=Leslie Threatte|editor=P.T. Daniels & W. Bright|title=The World's Writing Systems|year=1996|publisher=Oxford University Press|isbn=978-0-19-507993-7|chapter=The Greek Alphabet}}</ref>\n\nOther ancient scripts were also defective. Egyptian [[hieroglyphs]] had no vowel representation at all, while the [[cuneiform script]] frequently failed to distinguish among a consonant triad like /t/, /d/ and /t'/ (emphatic /t/), or between the vowels /e/ and /i/.  \n\nLanguages with a long literary history have a tendency to freeze spelling at an early stage, leaving subsequent pronunciation shifts unrecorded.  Such is the case with English, French, Greek, Hebrew, Persian, and Thai, among others.  By contrast, some writing systems have been periodically respelled in accordance with changed pronunciation, such as Dutch, Portuguese, Spanish, Irish Gaelic, and Japanese hiragana.    \n\nA broadly defective script is the [[Arabic abjad]].<ref name=\"DB\">{{cite book|author1=Peter T. Daniels|author2=William Bright|title=The World's Writing Systems|year=1996|publisher=Oxford University Press|isbn=978-0-19-507993-7}}</ref>{{rp|561-3}} The modern script does not normally write short vowels, but for the first few centuries of the [[Islam]]ic era, long vowels were not written and many consonant letters were ambiguous as well. The Arabic script derives from the Aramaic, and not only did the [[Aramaic language]] have fewer [[phoneme]]s than Arabic, but several originally distinct Aramaic letters had conflated (become indistinguishable in shape), so that in the early Arabic writings 28 consonant phonemes were represented by only 18 letters—and in the middle of words, only 15 were distinct. For example, medial {{angbr|{{lang|ar|ـٮـ}}}} represented {{IPA|/b, t, θ, n, j/}}, and {{angbr|{{lang|ar|ح}}}} represented {{IPA|/d͡ʒ, ħ, x/}}. A system of [[diacritic]] marks, or ''pointing,'' was later developed to resolve the ambiguities, and over the centuries became nearly universal. However, even today unpointed texts of a style called ''{{transl|ar|DIN|mašq}}'' are found, where these consonants are not distinguished.<ref>{{cite book|author1=Richard Bell|author2=William Montgomery Watt|title=Bell's Introduction to the Qur'ān|year=1970|publisher=University Press|location=Edinburgh|isbn=978-0-85224-171-4}}</ref>\n\nWithout short vowels or [[Gemination|geminate]] consonants being written, modern Arabic {{lang|ar|نظر}} ''{{transl|ar|DIN|nẓr}}'' could represent {{lang|ar|نَظَرَ}} {{IPA|/naðˤara/}} 'he saw', {{lang|ar|نَظَّرَ}} {{IPA|/naðˤːara/}} 'he compared', {{lang|ar|نُظِرَ}} {{IPA|/nuðˤira/}} 'he was seen', {{lang|ar|نُظِّرَ}} {{IPA|/nuðˤːira/}} 'he was compared', {{lang|ar|نَظَر}} {{IPA|/naðˤar/}} 'a glance', or {{lang|ar|نِظْر}} {{IPA|/niðˤr/}} 'similar'. However, in practice there is little ambiguity, as the vowels are more easily predictable in Arabic than they are in a language like English. Moreover, the defective nature of the script has its benefits: the stable shape of the root words, despite grammatical [[inflection]], results in quicker word recognition and therefore faster reading speeds; and the lack of short vowels, the sounds which vary the most between [[Varieties of Arabic|Arabic dialects]], makes texts more widely accessible to a diverse audience.<ref>{{cite book|author=Thomas Bauer|editor=P.T. Daniels & W. Bright|title=The World's Writing Systems|year=1996|publisher=Oxford University Press|isbn=978-0-19-507993-7|chapter=Arabic Writing}}</ref>\n\nHowever, in ''{{transl|ar|DIN|mašq}}'' and those styles of ''[[kufic]]'' writing which lack consonant pointing, the ambiguities are more serious, for here different roots are written the same. {{lang|ar|ٮطر}} could represent the root ''{{transl|ar|DIN|nẓr}}'' 'see' as above, but also ''{{transl|ar|DIN|nṭr}}'' 'protect', ''{{transl|ar|DIN|bṭr}}'' 'pride', ''{{transl|ar|DIN|bẓr}}'' 'clitoris' or 'with flint', as well as several inflections and derivations of each of these root words.\n\nThe Arabic alphabet has been adopted by many Muslim peoples to write their languages.  In them, new consonant letters have been devised for sounds lacking in Arabic (e.g. {{IPA|/p/}}, {{IPA|/g/}}, {{IPA|/tʃ/}}, and {{IPA|/ʒ/}} in [[Persian language|Persian]];<ref name=\"DB\" />{{rp|747}} all the aspirate and retroflex stops in [[Sindhi language|Sindhi]]<ref name=\"DB\" />{{rp|757}}).  But rarely have the full set of vowels been represented in those new alphabets: [[Ottoman Turkish language|Ottoman Turkish]] had eight vowels, but used only three letters to notate them.<ref name=\"DB\" />{{rp|758}}  However, some adaptions of the Arabic alphabet do unambiguously mark all vowels: those for [[Arebica|Bosnian]], [[Kashmiri language|Kashmiri]],<ref name=\"DB\" />{{rp|753}} [[Kurdish language|Kurdish]], [[Kyrgyz alphabets|Kyrgyz]], and [[Uyghur Arabic alphabet|Uyghur]].<ref name=\"DB\" />{{rp|748}}\n\nWhen a defective script is written with diacritics or other conventions to indicate all phonemic distinctions, the result is called ''plene'' writing.<ref>{{cite book|author=Werner Weinberg|title=The History of Hebrew Plene Spelling|year=1985|publisher=Hebrew Union College Press|isbn=978-0-87820-205-8}}</ref>\n\nDefectiveness is a [[cline (linguistics)|cline]]: the Semitic ''[[abjad]]s'' do not indicate all vowels, but there are also alphabets which mark vowels but not [[tone (linguistics)|tone]] (e.g. many [[Writing systems of Africa|African languages]]), or vowel quality but not vowel length (e.g. [[Latin spelling and pronunciation|Latin]]). Even if English orthography were regularized, the English alphabet would still be incapable of unambiguously conveying [[intonation (linguistics)|intonation]], though since this is not expected of scripts, it is not normally counted as defectiveness.<ref name=\"Sampson\" />\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Defective script}}\n[[Category:Orthography]]\n[[Category:Alphabets]]"
    }
  ]
}