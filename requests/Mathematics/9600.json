{
  "pages": [
    {
      "title": "Stieltjes matrix",
      "url": "https://en.wikipedia.org/wiki/Stieltjes_matrix",
      "text": "In [[mathematics]], particularly [[Matrix (mathematics)|matrix theory]], a '''Stieltjes matrix''', named after [[Thomas Joannes Stieltjes]], is a [[real numbers|real]]  [[symmetric matrix|symmetric]] [[positive definite matrix]] with [[nonpositive]] [[off-diagonal]] entries.  A Stieltjes matrix is necessarily an [[M-matrix]].  Every ''n×n'' Stieltjes matrix is invertible to a nonsingular symmetric [[nonnegative matrix|nonnegative]] matrix, though the converse of this statement is not true in general for&nbsp;''n''&nbsp;>&nbsp;2.\n\nFrom the above definition, a Stieltjes matrix is a symmetric invertible [[Z-matrix_(mathematics)|Z-matrix]] whose eigenvalues have positive real parts. As it is a Z-matrix, its off-diagonal entries are less than or equal to zero.\n\n== See also ==\n* [[Hurwitz matrix]]\n* [[Metzler matrix]]\n\n==References==\n* {{cite book | title=Iterative Solution of Large Linear Systems | author=David M. Young | publisher=[[Dover Publications]] | date=2003 | isbn=0-486-42548-7 | page=42 }}\n* {{cite book | title=Iterative Methods for Solving Linear Systems |author=Anne Greenbaum | publisher=[[Society for Industrial and Applied Mathematics|SIAM]] | date=1987 | isbn=0-89871-396-X | page=162 }}\n\n{{Numerical linear algebra}}\n\n[[Category:Matrices]]\n[[Category:Numerical linear algebra]]\n\n\n{{Linear-algebra-stub}}"
    },
    {
      "title": "Stone method",
      "url": "https://en.wikipedia.org/wiki/Stone_method",
      "text": "{{Multiple issues|\n{{primary sources|date=October 2015}}\n{{no footnotes|date=September 2013}}\n}}\n\nIn [[numerical analysis]], '''Stone's method''', also known as the '''strongly implicit procedure''' or '''SIP''', is an [[algorithm]] for solving a [[sparse matrix|sparse]] [[linear system of equations]]. The method uses an [[incomplete LU factorization|incomplete LU decomposition]], which approximates the exact [[LU decomposition]], to get an [[iterative method|iterative]] solution of the problem. The method is named after Herbert L. Stone, who proposed it in 1968.\n\nThe LU decomposition is an excellent general-purpose linear equation solver. The biggest disadvantage is that it fails to take advantage of coefficient matrix to be a sparse matrix. The LU decomposition of a sparse matrix is usually not sparse, thus, for a large system of equations, LU decomposition may require a prohibitive amount of [[Random access memory|memory]] and number of [[Instruction (computer science)|arithmetical operations]].\n\nIn the [[preconditioned]] [[iterative method]]s, if the preconditioner matrix ''M'' is a good approximation of coefficient matrix ''A'' then the convergence is faster. This brings one to idea of using approximate factorization ''LU'' of ''A'' as the iteration matrix ''M''.\n\nA version of incomplete lower-upper decomposition method was proposed by Stone in 1968. This method is designed for equation system arising from discretisation of [[partial differential equation]]s and was firstly used for a [[Pentadiagonal matrix|pentadiagonal]] system of equations obtained while solving an [[Elliptic operator|elliptic]] partial differential equation in a [[two-dimensional]] space by a [[finite difference]] method. The LU approximate decomposition was looked{{Clarify|reason=vague|date=March 2016}} in the same pentadiagonal form as the original matrix (three diagonals for ''L'' and three diagonals for ''U'') as the best match of the seven possible equations for the five unknowns for each row of the matrix.\n\n==Algorithm==\n         For the linear system {{math|'''A'''x {{=}} b}}\n         calculate incomplete {{math|'''LU'''}} factorization of matrix {{math|'''A'''}}\n            {{math|'''A'''x {{=}} ('''M'''-'''N''')x {{=}} ('''LU'''-'''N''')x {{=}} b}}\n            {{math|'''M'''x<sup>(k+1)</sup> {{=}} '''N'''x<sup>(k)</sup>+b , with {{!}}{{!}}'''M'''{{!}}{{!}} >> {{!}}{{!}}'''N'''{{!}}{{!}}}}\n            {{math|'''M'''x<sup>(k+1)</sup> {{=}} '''LU'''x<sup>(k+1)</sup> {{=}} c<sup>(k)</sup>}}\n            {{math|'''LU'''x<sup>(k)</sup> {{=}} '''L'''('''U'''x<sup>(k+1)</sup>) {{=}} '''L'''y<sup>(k)</sup> {{=}} c<sup>(k)</sup>}}\n         set a guess\n            {{math|k {{=}} 0, x<sup>(k)</sup>\n            r<sup>(k)</sup>{{=}}b - '''A'''x<sup>(k)</sup>}}\n         while ( {{math|{{!}}{{!}}r<sup>(k)</sup>{{!}}{{!}}<sub>2</sub> ≥ ε}} ) do\n            evaluate new right hand side\n               {{math|c<sup>(k)</sup> {{=}} '''N'''x<sup>(k)</sup> + b}}\n            solve {{math|'''L'''y<sup>(k)</sup> {{=}} c<sup>(k)</sup>}} by forward substitution\n               {{math|y<sup>(k)</sup> {{=}} '''L'''<sup>−1</sup>c<sup>(k)</sup>}}\n            solve {{math|'''U'''x<sup>(k+1)</sup> {{=}} y<sup>(k)</sup>}} by back substitution\n               {{math|x<sup>(k+1)</sup> {{=}} '''U'''<sup>−1</sup>y<sup>(k)</sup>}}\n         end while\n\n==Footnotes==\n{{Reflist}}\n\n== References ==\n* {{cite journal | author= Stone, H. L. | title= Iterative Solution of Implicit Approximations of Multidimensional Partial Differential Equations | journal=SIAM Journal on Numerical Analysis | year= 1968 | volume= 5| pages= 530–538 | doi= 10.1137/0705044 | issue= 3}} - '''the original article'''\n* {{cite book|author=[[Joel H. Ferziger|Ferziger, J.H.]] and Peric, M.|year=2001|title=Computational Methods for Fluid Dynamics|isbn=3-540-42074-6|publisher=Springer-Verlag, Berlin}}\n* {{cite book|author=Acosta, J.M.|year=2001|title=Numerical Algorithms for Three Dimensional Computational Fluid Dynamic Problems. PhD Thesis|publisher=Polytechnic University of Catalonia}}\n* {{CFDWiki|name=Stone's_method}}\n\n{{Numerical linear algebra}}\n\n[[Category:Numerical linear algebra]]\n[[Category:Articles with example pseudocode]]"
    },
    {
      "title": "Symmetric successive over-relaxation",
      "url": "https://en.wikipedia.org/wiki/Symmetric_successive_over-relaxation",
      "text": "In applied mathematics, '''symmetric successive over-relaxation (SSOR)''',<ref>[http://www.cfd-online.com/Wiki/Iterative_methods Iterative methods] at CFD-Online wiki</ref> is a [[preconditioner]].\n\nIf the original matrix can be [[Matrix splitting|split]] into diagonal, lower and upper triangular as <math>A=D+L+L^T</math> then the SSOR preconditioner matrix is defined as\n\n: <math>M=(D+L) D^{-1} (D+L)^T</math>\n\nIt can also be parametrised by <math>\\omega</math> as follows.<ref>[http://www.netlib.org/linalg/html_templates/node58.html SSOR preconditioning] at [[Netlib]]</ref>\n\n:<math>M(\\omega)={\\omega\\over{2-\\omega}} \\left ( {1\\over\\omega} D + L \\right ) D^{-1} \\left ( {1\\over\\omega} D + L\\right)^T</math>\n\n== See also==\n*[[Successive over-relaxation]]\n\n== References ==\n<references/>\n\n[[Category:Numerical linear algebra]]\n\n\n{{mathapplied-stub}}"
    },
    {
      "title": "System of linear equations",
      "url": "https://en.wikipedia.org/wiki/System_of_linear_equations",
      "text": "{{more footnotes|date=October 2015}}\n[[File:Secretsharing 3-point.svg|thumb|right|A linear system in three variables determines a collection of [[plane (mathematics)|planes]]. The intersection point is the solution.]]\nIn [[mathematics]], a '''system of linear equations''' (or '''linear system''') is a collection of two or more [[linear equation]]s involving the same set of [[variable (math)|variable]]s. For example,\n:<math>\\begin{alignat}{7}\n3x &&\\; + \\;&& 2y             &&\\; - \\;&& z  &&\\; = \\;&& 1 & \\\\\n2x &&\\; - \\;&& 2y             &&\\; + \\;&& 4z &&\\; = \\;&& -2 & \\\\\n-x &&\\; + \\;&& \\tfrac{1}{2} y &&\\; - \\;&& z  &&\\; = \\;&& 0 &\n\\end{alignat}</math>\nis a system of three equations in the three variables {{math|''x'', ''y'', ''z''}}. A '''solution''' to a linear system is an assignment of values to the variables such that all the equations are simultaneously satisfied. A [[Equation solving|solution]] to the system above is given by\n:<math>\\begin{alignat}{2}\nx &\\,=\\,& 1 \\\\\ny &\\,=\\,& -2 \\\\\nz &\\,=\\,& -2\n\\end{alignat}</math>\nsince it makes all three equations valid. The word \"system\" indicates that the equations are to be considered collectively, rather than individually.\n\nIn mathematics, the theory of linear systems is the basis and a fundamental part of [[linear algebra]], a subject which is used in most parts of modern mathematics. Computational [[algorithm]]s for finding the solutions are an important part of [[numerical linear algebra]], and play a prominent role in [[engineering]], [[physics]], [[chemistry]], [[computer science]], and [[economics]]. A [[Nonlinear system|system of non-linear equations]] can often be [[approximation|approximated]] by a linear system (see [[linearization]]), a helpful technique when making a [[mathematical model]] or [[computer simulation]] of a relatively [[complex system]].\n\nVery often, the coefficients of the equations are [[real number|real]] or [[complex number]]s and the solutions are searched in the same set of numbers, but the theory and the algorithms apply for coefficients and solutions in any [[field (mathematics)|field]]. For solutions in an [[integral domain]] like the [[ring (mathematics)|ring]] of the [[integer]]s, or in other [[algebraic structure]]s, other theories have been developed, see [[Linear equation over a ring]]. [[Integer linear programming]] is a collection of methods for finding the \"best\" integer solution (when there are many). [[Gröbner basis]] theory provides algorithms when coefficients and unknowns are [[polynomial]]s. Also [[tropical geometry]] is an example of linear algebra in a more exotic structure.\n\n==Elementary example==\nThe simplest kind of linear system involves two equations and two variables:\n:<math>\\begin{alignat}{5}\n2x &&\\; + \\;&& 3y &&\\; = \\;&& 6 & \\\\\n4x &&\\; + \\;&& 9y &&\\; = \\;&& 15&.\n\\end{alignat}</math>\nOne method for solving such a system is as follows. First, solve the top equation for <math>x</math> in terms of <math>y</math>:\n:<math>x = 3 - \\frac{3}{2}y.</math>\nNow [[substitution (algebra)|substitute]] this expression for ''x'' into the bottom equation:\n:<math>4\\left( 3 - \\frac{3}{2}y \\right) + 9y = 15.</math>\nThis results in a single equation involving only the variable <math>y</math>. Solving gives <math>y = 1</math>, and substituting this back into the equation for <math>x</math> yields <math>x = 3/2</math>. This method generalizes to systems with additional variables (see \"elimination of variables\" below, or the article on [[elementary algebra]].)\n\n==General form==\nA general system of ''m'' linear equations with ''n'' unknowns can be written as\n:<math>\\begin{align}\na_{11} x_1 + a_{12} x_2  + \\cdots + a_{1n} x_n  &= b_1 \\\\\na_{21} x_1 + a_{22} x_2  + \\cdots + a_{2n} x_n  &= b_2 \\\\\n& \\ \\ \\vdots\\\\\na_{m1} x_1 + a_{m2} x_2  + \\cdots + a_{mn} x_n  &= b_ m,\n\\end{align}</math>\nwhere <math>x_1, x_2,\\ldots,x_n</math> are the unknowns, <math>a_{11},a_{12},\\ldots,a_{mn}</math> are the coefficients of the system, and <math>b_1,b_2,\\ldots,b_m</math> are the constant terms.\n\nOften the coefficients and unknowns are [[real number|real]] or [[complex number]]s, but [[integer]]s and [[rational number]]s are also seen, as are polynomials and elements of an abstract [[algebraic structure]].\n\n===Vector equation===\nOne extremely helpful view is that each unknown is a weight for a [[column vector]] in a [[linear combination]].\n:<math>\n x_1 \\begin{bmatrix}a_{11}\\\\a_{21}\\\\ \\vdots \\\\a_{m1}\\end{bmatrix} +\n x_2 \\begin{bmatrix}a_{12}\\\\a_{22}\\\\ \\vdots \\\\a_{m2}\\end{bmatrix} +\n \\cdots +\n x_n \\begin{bmatrix}a_{1n}\\\\a_{2n}\\\\ \\vdots \\\\a_{mn}\\end{bmatrix}\n =\n \\begin{bmatrix}b_1\\\\b_2\\\\ \\vdots \\\\b_m\\end{bmatrix}\n</math>\nThis allows all the language and theory of ''[[vector space]]s'' (or more generally, ''[[module (mathematics)|module]]s'') to be brought to bear. For example, the collection of all possible linear combinations of the vectors on the left-hand side is called their ''[[span (linear algebra)|span]]'', and the equations have a solution just when the right-hand vector is within that span. If every vector within that span has exactly one expression as a linear combination of the given left-hand vectors, then any solution is unique. In any event, the span has a ''[[basis (linear algebra)|basis]]'' of [[linearly independent]] vectors that do guarantee exactly one expression; and the number of vectors in that basis (its ''[[dimension (linear algebra)|dimension]]'') cannot be larger than ''m'' or ''n'', but it can be smaller. This is important because if we have ''m'' independent vectors a solution is guaranteed regardless of the right-hand side, and otherwise not guaranteed.\n\n===Matrix equation===\nThe vector equation is equivalent to a [[matrix (mathematics)|matrix]] equation of the form\n:<math>A\\mathbf{x}=\\mathbf{b}</math>\nwhere ''A'' is an ''m''×''n'' matrix, '''x''' is a [[column vector]] with ''n'' entries, and '''b''' is a column vector with ''m'' entries.\n: <math>\nA=\n\\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix},\\quad\n\\mathbf{x}=\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix},\\quad\n\\mathbf{b}=\n\\begin{bmatrix}\nb_1 \\\\\nb_2 \\\\\n\\vdots \\\\\nb_m\n\\end{bmatrix}\n</math>\nThe number of vectors in a basis for the span is now expressed as the ''[[rank (linear algebra)|rank]]'' of the matrix.\n\n==Solution set==\n[[Image:Intersecting Lines.svg|thumb|right|The solution set for the equations {{nowrap|''x'' &minus; ''y'' {{=}} &minus;1}} and {{nowrap|3''x'' + ''y'' {{=}} 9}} is the single point (2,&nbsp;3).]]\nA '''solution''' of a linear system is an assignment of values to the variables {{nowrap|''x''<sub>1</sub>, ''x''<sub>2</sub>, ..., ''x<sub>n</sub>''}} such that each of the equations is satisfied. The [[Set (mathematics)|set]] of all possible solutions is called the '''[[solution set]]'''.\n\nA linear system may behave in any one of three possible ways:\n# The system has ''infinitely many solutions''.\n# The system has a single ''unique solution''.\n# The system has ''no solution''.\n\n===Geometric interpretation===\nFor a system involving two variables (''x'' and ''y''), each linear equation determines a [[line (mathematics)|line]] on the ''xy''-[[Cartesian coordinate system|plane]]. Because a solution to a linear system must satisfy all of the equations, the solution set is the [[intersection (set theory)|intersection]] of these lines, and is hence either a line, a single point, or the [[empty set]].\n\nFor three variables, each linear equation determines a [[plane (mathematics)|plane]] in [[three-dimensional space]], and the solution set is the intersection of these planes. Thus the solution set may be a plane, a line, a single point, or the empty set. For example, as three parallel planes do not have a common point, the solution set of their equations is empty; the solution set of the equations of three planes intersecting at a point is single point; if three planes pass through two points, their equations have at least two common solutions; in fact the solution set is infinite and consists in all the line passing through these points.<ref name=\"Linear Matrices\">{{cite book |isbn= 978-0-486-66328-9|url= https://www.amazon.com/Matrices-Linear-Transformations-Edition-Mathematics/dp/0486663280/ref=sr_1_1?ie=UTF8&qid=1400776703&sr=8-1&keywords=cullen+matrices |title=Matrices and Linear Transformations |author=Charles G. Cullen |location= MA |publisher=Dover| year=1990 |page=3}}</ref>\n\nFor ''n'' variables, each linear equation determines a [[hyperplane]] in [[n-dimensional space|''n''-dimensional space]]. The solution set is the intersection of these hyperplanes, and is a [[flat (geometry)|flat]], which may have any dimension lower than ''n''.\n\n===General behavior===\n[[File:Intersecting Planes 2.svg|thumb|right|The solution set for two equations in three variables is, in general, a line.]]\nIn general, the behavior of a linear system is determined by the relationship between the number of equations and the number of unknowns. Here, \"in general\" means that a different behavior may occur for specific values of the coefficients of the equations.\n* In general, a system with fewer equations than unknowns has infinitely many solutions, but it may have no solution. Such a system is known as an [[underdetermined system]].\n* In general, a system with the same number of equations and unknowns has a single unique solution.\n* In general, a system with more equations than unknowns has no solution. Such a system is also known as an [[overdetermined system]].\nIn the first case, the [[dimension]] of the solution set is, in general, equal to {{nowrap|''n'' &minus; ''m''}}, where ''n'' is the number of variables and ''m'' is the number of equations.\n\nThe following pictures illustrate this trichotomy in the case of two variables:\n:{| border=0 cellpadding=5\n|-\n| width=\"150\" align=\"center\" | [[Image:One Line.svg|120px]]\n| width=\"150\" align=\"center\" | [[Image:Two Lines.svg|120px]]\n| width=\"150\" align=\"center\" | [[Image:Three Lines.svg|120px]]\n|-\n|align=\"center\"| One equation\n|align=\"center\"| Two equations\n|align=\"center\"| Three equations\n|}\nThe first system has infinitely many solutions, namely all of the points on the blue line. The second system has a single unique solution, namely the intersection of the two lines. The third system has no solutions, since the three lines share no common point.\n\nIt must be kept in mind that the pictures above show only the most common case (the general case). It is possible for a system of two equations and two unknowns to have no solution (if the two lines are parallel), or for a system of three equations and two unknowns to be solvable (if the three lines intersect at a single point). \n\nA system of linear equations behave differently from the general case if the equations are ''[[linear independence|linearly dependent]]'', or if it is ''[[#Consistency|inconsistent]]'' and has no more equations than unknowns.\n\n==Properties==\n\n===Independence===\nThe equations of a linear system are '''independent''' if none of the equations can be derived algebraically from the others. When the equations are independent, each equation contains new information about the variables, and removing any of the equations increases the size of the solution set. For linear equations, logical independence is the same as [[linear independence]].\n\n[[Image:Three Intersecting Lines.svg|thumb|right|The equations {{nowrap|''x'' &minus; 2''y'' {{=}} &minus;1}}, {{nowrap|3''x'' + 5''y'' {{=}} 8}}, and {{nowrap|4''x'' + 3''y'' {{=}} 7}} are linearly dependent.]]\nFor example, the equations\n:<math>3x+2y=6\\;\\;\\;\\;\\text{and}\\;\\;\\;\\;6x+4y=12</math>\nare not independent — they are the same equation when scaled by a factor of two, and they would produce identical graphs. This is an example of equivalence in a system of linear equations.\n\nFor a more complicated example, the equations\n:<math>\\begin{alignat}{5}\n x &&\\; - \\;&& 2y &&\\; = \\;&& -1 & \\\\\n 3x &&\\; + \\;&& 5y &&\\; = \\;&& 8 & \\\\\n 4x &&\\; + \\;&& 3y &&\\; = \\;&& 7 &\n\\end{alignat}</math>\nare not independent, because the third equation is the sum of the other two. Indeed, any one of these equations can be derived from the other two, and any one of the equations can be removed without affecting the solution set. The graphs of these equations are three lines that intersect at a single point.\n\n===Consistency===\n{{See also|Consistent and inconsistent equations}}\n[[Image:Parallel Lines.svg|thumb|right|The equations {{nowrap|3''x'' + 2''y'' {{=}} 6}} and {{nowrap|3''x'' + 2''y'' {{=}} 12}} are inconsistent.]]\nA linear system is '''inconsistent''' if it has no solution, and otherwise it is said to be '''consistent'''. When the system is inconsistent, it is possible to derive a [[contradiction]] from the equations, that may always be rewritten as the statement {{nowrap|0 {{=}} 1}}.\n\nFor example, the equations\n:<math>3x+2y=6\\;\\;\\;\\;\\text{and}\\;\\;\\;\\;3x+2y=12</math>\nare inconsistent. In fact, by subtracting the first equation from the second one and multiplying both sides of the result by 1/6, we get {{nowrap|0 {{=}} 1}}. The graphs of these equations on the ''xy''-plane are a pair of [[parallel (geometry)|parallel]] lines.\n\nIt is possible for three linear equations to be inconsistent, even though any two of them are consistent together. For example, the equations\n:<math>\\begin{alignat}{7}\n x &&\\; + \\;&& y &&\\; = \\;&& 1 & \\\\\n2x &&\\; + \\;&& y &&\\; = \\;&& 1 & \\\\\n3x &&\\; + \\;&& 2y &&\\; = \\;&& 3 &\n\\end{alignat}</math>\nare inconsistent. Adding the first two equations together gives {{nowrap|3''x'' + 2''y'' {{=}} 2}}, which can be subtracted from the third equation to yield {{nowrap|0 {{=}} 1}}. Note that any two of these equations have a common solution. The same phenomenon can occur for any number of equations.\n\nIn general, inconsistencies occur if the left-hand sides of the equations in a system are linearly dependent, and the constant terms do not satisfy the dependence relation. A system of equations whose left-hand sides are linearly independent is always consistent.\n\nPutting it another way, according to the [[Rouché–Capelli theorem]], any system of equations (overdetermined or otherwise) is inconsistent if the [[rank (linear algebra)|rank]] of the [[augmented matrix]] is greater than the rank of the [[coefficient matrix]]. If, on the other hand, the ranks of these two matrices are equal, the system must have at least one solution. The solution is unique if and only if the rank equals the number of variables. Otherwise the general solution has ''k'' free parameters where ''k'' is the difference between the number of variables and the rank; hence in such a case there are an infinitude of solutions. The rank of a system of equations (i.e. the rank of the augmented matrix) can never be higher than [the number of variables] + 1, which means that a system with any number of equations can always be reduced to a system that has a number of [[independent equation]]s that is at most equal to [the number of variables] + 1.\n\n===Equivalence===\nTwo linear systems using the same set of variables are '''equivalent''' if each of the equations in the second system can be derived algebraically from the equations in the first system, and vice versa. Two systems are equivalent if either both are inconsistent or each equation of each of them is a linear combination of the equations of the other one. It follows that two linear systems are equivalent if and only if they have the same solution set.\n\n==Solving a linear system==\nThere are several [[algorithm]]s for [[equation solving|solving]] a system of linear equations.\n\n===Describing the solution===\nWhen the solution set is finite, it is reduced to a single element. In this case, the unique solution is described by a sequence of equations whose left-hand sides are the names of the unknowns and right-hand sides are the corresponding values, for example <math>(x=3, \\;y=-2,\\; z=6)</math>. When an order on the unknowns has been fixed, for example the [[alphabetical order]] the solution may be described as a [[vector space|vector]] of values, like <math>(3, \\,-2,\\, 6)</math> for the previous example.\n\nTo describe a set with an infinite number of solutions, typically some of the variables are designated as '''free''' (or '''independent''', or as '''parameters'''), meaning that they are allowed to take any value, while the remaining variables are '''dependent''' on the values of the free variables.\n\nFor example, consider the following system:\n:<math>\\begin{alignat}{7}\n x &&\\; + \\;&& 3y &&\\; - \\;&& 2z &&\\; = \\;&& 5 & \\\\\n3x &&\\; + \\;&& 5y &&\\; + \\;&& 6z &&\\; = \\;&& 7 &\n\\end{alignat}</math>\nThe solution set to this system can be described by the following equations:\n:<math>x=-7z-1\\;\\;\\;\\;\\text{and}\\;\\;\\;\\;y=3z+2\\text{.}</math>\nHere ''z'' is the free variable, while ''x'' and ''y'' are dependent on ''z''. Any point in the solution set can be obtained by first choosing a value for ''z'', and then computing the corresponding values for ''x'' and ''y''.\n\nEach free variable gives the solution space one [[Degrees of freedom (statistics)|degree of freedom]], the number of which is equal to the [[dimension]] of the solution set. For example, the solution set for the above equation is a line, since a point in the solution set can be chosen by specifying the value of the parameter ''z''. An infinite solution of higher order may describe a plane, or higher-dimensional set.\n\nDifferent choices for the free variables may lead to different descriptions of the same solution set. For example, the solution to the above equations can alternatively be described as follows:\n:<math>y=-\\frac{3}{7}x + \\frac{11}{7}\\;\\;\\;\\;\\text{and}\\;\\;\\;\\;z=-\\frac{1}{7}x-\\frac{1}{7}\\text{.}</math>\nHere ''x'' is the free variable, and ''y'' and ''z'' are dependent.\n\n===Elimination of variables===\nThe simplest method for solving a system of linear equations is to repeatedly eliminate variables. This method can be described as follows:\n# In the first equation, solve for one of the variables in terms of the others.\n# Substitute this expression into the remaining equations. This yields a system of equations with one fewer equation and one fewer unknown.\n# Repeat until the system is reduced to a single linear equation.\n# Solve this equation, and then back-substitute until the entire solution is found.\n\nFor example, consider the following system:\n:<math>\\begin{alignat}{7}\n x &&\\; + \\;&& 3y &&\\; - \\;&& 2z &&\\; = \\;&& 5 & \\\\\n3x &&\\; + \\;&& 5y &&\\; + \\;&& 6z &&\\; = \\;&& 7 & \\\\\n2x &&\\; + \\;&& 4y &&\\; + \\;&& 3z &&\\; = \\;&& 8 &\n\\end{alignat}</math>\nSolving the first equation for ''x'' gives {{nowrap|''x'' {{=}} 5 + 2''z'' &minus; 3''y''}}, and plugging this into the second and third equation yields\n:<math>\\begin{alignat}{5}\n-4y &&\\; + \\;&& 12z &&\\; = \\;&& -8 & \\\\\n-2y &&\\; + \\;&& 7z &&\\; = \\;&& -2 &\n\\end{alignat}</math>\nSolving the first of these equations for ''y'' yields {{nowrap|''y'' {{=}} 2 + 3''z''}}, and plugging this into the second equation yields {{nowrap|''z'' {{=}} 2}}. We now have:\n:<math>\\begin{alignat}{7}\n x &&\\; = \\;&& 5 &&\\; + \\;&& 2z &&\\; - \\;&& 3y & \\\\\n y &&\\; = \\;&& 2 &&\\; + \\;&& 3z && && & \\\\\n z &&\\; = \\;&& 2 && && && && &\n\\end{alignat}</math>\nSubstituting {{nowrap|''z'' {{=}} 2}} into the second equation gives {{nowrap|''y'' {{=}} 8}}, and substituting {{nowrap|''z'' {{=}} 2}} and {{nowrap|''y'' {{=}} 8}} into the first equation yields {{nowrap|''x'' {{=}} &minus;15}}. Therefore, the solution set is the single point {{nowrap|(''x'', ''y'', ''z'') {{=}} (&minus;15, 8, 2)}}.\n\n===Row reduction===\n{{main|Gaussian elimination}}\nIn '''row reduction''' (also known as '''Gaussian elimination'''), the linear system is represented as an [[augmented matrix]]:\n:<math>\\left[\\begin{array}{rrr|r}\n1 & 3 & -2 & 5 \\\\\n3 & 5 & 6 & 7 \\\\\n2 & 4 & 3 & 8\n\\end{array}\\right]\\text{.}\n</math>\nThis matrix is then modified using [[elementary row operations]] until it reaches [[reduced row echelon form]]. There are three types of elementary row operations:\n:'''Type 1''': Swap the positions of two rows.\n:'''Type 2''': Multiply a row by a nonzero [[scalar (mathematics)|scalar]].\n:'''Type 3''': Add to one row a scalar multiple of another.\nBecause these operations are reversible, the augmented matrix produced always represents a linear system that is equivalent to the original.\n\nThere are several specific algorithms to row-reduce an augmented matrix, the simplest of which are [[Gaussian elimination]] and [[Gauss-Jordan elimination]]. The following computation shows Gauss-Jordan elimination applied to the matrix above:\n:<math>\\begin{align}\\left[\\begin{array}{rrr|r}\n1 & 3 & -2 & 5 \\\\\n3 & 5 & 6 & 7 \\\\\n2 & 4 & 3 & 8\n\\end{array}\\right]&\\sim\n\\left[\\begin{array}{rrr|r}\n1 & 3 & -2 & 5 \\\\\n0 & -4 & 12 & -8 \\\\\n2 & 4 & 3 & 8\n\\end{array}\\right]\\sim\n\\left[\\begin{array}{rrr|r}\n1 & 3 & -2 & 5 \\\\\n0 & -4 & 12 & -8 \\\\\n0 & -2 & 7 & -2\n\\end{array}\\right]\\sim\n\\left[\\begin{array}{rrr|r}\n1 & 3 & -2 & 5 \\\\\n0 & 1 & -3 & 2 \\\\\n0 & -2 & 7 & -2\n\\end{array}\\right]\n\\\\\n&\\sim\n\\left[\\begin{array}{rrr|r}\n1 & 3 & -2 & 5 \\\\\n0 & 1 & -3 & 2 \\\\\n0 & 0 & 1 & 2\n\\end{array}\\right]\\sim\n\\left[\\begin{array}{rrr|r}\n1 & 3 & -2 & 5 \\\\\n0 & 1 & 0 & 8 \\\\\n0 & 0 & 1 & 2\n\\end{array}\\right]\\sim\n\\left[\\begin{array}{rrr|r}\n1 & 3 & 0 & 9 \\\\\n0 & 1 & 0 & 8 \\\\\n0 & 0 & 1 & 2\n\\end{array}\\right]\\sim\n\\left[\\begin{array}{rrr|r}\n1 & 0 & 0 & -15 \\\\\n0 & 1 & 0 & 8 \\\\\n0 & 0 & 1 & 2\n\\end{array}\\right].\\end{align}</math>\nThe last matrix is in reduced row echelon form, and represents the system {{nowrap|''x'' {{=}} &minus;15}}, {{nowrap|''y'' {{=}} 8}}, {{nowrap|''z'' {{=}} 2}}. A comparison with the example in the previous section on the algebraic elimination of variables shows that these two methods are in fact the same; the difference lies in how the computations are written down.\n\n===Cramer's rule===\n{{main|Cramer's rule}}\n'''Cramer's rule''' is an explicit formula for the solution of a system of linear equations, with each variable given by a quotient of two [[determinant]]s. For example, the solution to the system\n:<math>\\begin{alignat}{7}\n x &\\; + &\\; 3y &\\; - &\\; 2z &\\; = &\\; 5 \\\\\n3x &\\; + &\\; 5y &\\; + &\\; 6z &\\; = &\\; 7 \\\\\n2x &\\; + &\\; 4y &\\; + &\\; 3z &\\; = &\\; 8 \n\\end{alignat}</math>\nis given by\n:<math>\nx=\\frac\n{\\,\\left| \\begin{matrix}5&3&-2\\\\7&5&6\\\\8&4&3\\end{matrix} \\right|\\,}\n{\\,\\left| \\begin{matrix}1&3&-2\\\\3&5&6\\\\2&4&3\\end{matrix} \\right|\\,}\n,\\;\\;\\;\\;y=\\frac\n{\\,\\left| \\begin{matrix}1&5&-2\\\\3&7&6\\\\2&8&3\\end{matrix} \\right|\\,}\n{\\,\\left| \\begin{matrix}1&3&-2\\\\3&5&6\\\\2&4&3\\end{matrix} \\right|\\,}\n,\\;\\;\\;\\;z=\\frac\n{\\,\\left| \\begin{matrix}1&3&5\\\\3&5&7\\\\2&4&8\\end{matrix} \\right|\\,}\n{\\,\\left| \\begin{matrix}1&3&-2\\\\3&5&6\\\\2&4&3\\end{matrix} \\right|\\,}.\n</math>\nFor each variable, the denominator is the determinant of the matrix of coefficients, while the numerator is the determinant of a matrix in which one column has been replaced by the vector of constant terms.\n\nThough Cramer's rule is important theoretically, it has little practical value for large matrices, since the computation of large determinants is somewhat cumbersome. (Indeed, large determinants are most easily computed using row reduction.)\nFurther, Cramer's rule has very poor numerical properties, making it unsuitable for solving even small systems reliably, unless the operations are performed in rational arithmetic with unbounded precision.{{Citation needed|date=March 2017}}\n\n===Matrix solution===\n\nIf the equation system is expressed in the matrix form <math>A\\mathbf{x}=\\mathbf{b}</math>, the entire solution set can also be expressed in matrix form. If the matrix ''A'' is square (has ''m'' rows and ''n''=''m'' columns) and has full rank (all ''m'' rows are independent), then the system has a unique solution given by\n\n:<math>\\mathbf{x}=A^{-1}\\mathbf{b}</math>\n\nwhere <math>A^{-1}</math> is the [[matrix inverse|inverse]] of ''A''. More generally, regardless of whether ''m''=''n'' or not and regardless of the rank of ''A'', all solutions (if any exist) are given using the [[Moore-Penrose pseudoinverse]] of ''A'', denoted <math>A^+</math>, as follows:\n\n:<math>\\mathbf{x}=A^+ \\mathbf{b} + (I-A^+ A)\\mathbf{w}</math>\n\nwhere <math>\\mathbf{w}</math> is a vector of free parameters that ranges over all possible ''n''×1 vectors. A necessary and sufficient condition for any solution(s) to exist is that the potential solution obtained using <math>\\mathbf{w}=\\mathbf{0}</math> satisfy <math>A\\mathbf{x}=\\mathbf{b}</math> &mdash; that is, that <math>AA^+ \\mathbf{b}=\\mathbf{b}.</math> If this condition does not hold, the equation system is inconsistent and has no solution. If the condition holds, the system is consistent and at least one solution exists. For example, in the above-mentioned case in which ''A'' is square and of full rank, <math>A^+</math> simply equals <math>A^{-1}</math> and the general solution equation simplifies to <math>\\mathbf{x}=A^{-1}\\mathbf{b} + (I - A^{-1}A)\\mathbf{w} = A^{-1}\\mathbf{b} + (I-I)\\mathbf{w} = A^{-1}\\mathbf{b}</math> as previously stated, where <math>\\mathbf{w}</math> has completely dropped out of the solution, leaving only a single solution.  In other cases, though, <math>\\mathbf{w}</math> remains and hence an infinitude of potential values of the free parameter vector <math>\\mathbf{w}</math> give an infinitude of solutions of the equation.\n\n===Other methods===\nWhile systems of three or four equations can be readily solved by hand (see [[Cracovian]]), computers are often used for larger systems. The standard algorithm for solving a system of linear equations is based on Gaussian elimination with some modifications. Firstly, it is essential to avoid division by small numbers, which may lead to inaccurate results. This can be done by reordering the equations if necessary, a process known as [[Pivot element|''pivoting'']]. Secondly, the algorithm does not exactly do Gaussian elimination, but it computes the [[LU decomposition]] of the matrix ''A''. This is mostly an organizational tool, but it is much quicker if one has to solve several systems with the same matrix ''A'' but different vectors '''b'''.\n\nIf the matrix ''A'' has some special structure, this can be exploited to obtain faster or more accurate algorithms. For instance, systems with a [[symmetric matrix|symmetric]] [[positive-definite matrix|positive definite]] matrix can be solved twice as fast with the [[Cholesky decomposition]]. [[Levinson recursion]] is a fast method for [[Toeplitz matrix|Toeplitz matrices]]. Special methods exist also for matrices with many zero elements (so-called [[sparse matrix|sparse matrices]]), which appear often in applications.\n\nA completely different approach is often taken for very large systems, which would otherwise take too much time or memory. The idea is to start with an initial approximation to the solution (which does not have to be accurate at all), and to change this approximation in several steps to bring it closer to the true solution. Once the approximation is sufficiently accurate, this is taken to be the solution to the system. This leads to the class of [[iterative method]]s.\n\nThere is also a [[quantum algorithm for linear systems of equations]].<ref name=\"Quantum algorithm for solving linear systems of equations by Harrow et al.\">[https://arxiv.org/abs/0811.3171 Quantum algorithm for solving linear systems of equations, by Harrow et al.].</ref>\n\n==Homogeneous systems==\n{{see also|Homogeneous differential equation}}\nA system of linear equations is '''homogeneous''' if all of the constant terms are zero:\n:<math>\\begin{alignat}{7}\na_{11} x_1 &&\\; + \\;&& a_{12} x_2 &&\\; + \\cdots + \\;&& a_{1n} x_n &&\\; = \\;&&& 0 \\\\\na_{21} x_1 &&\\; + \\;&& a_{22} x_2 &&\\; + \\cdots + \\;&& a_{2n} x_n &&\\; = \\;&&& 0 \\\\\n\\vdots\\;\\;\\; &&     && \\vdots\\;\\;\\; &&              && \\vdots\\;\\;\\; &&     &&& \\,\\vdots \\\\\na_{m1} x_1 &&\\; + \\;&& a_{m2} x_2 &&\\; + \\cdots + \\;&& a_{mn} x_n &&\\; = \\;&&& 0. \\\\\n\\end{alignat}</math>\nA homogeneous system is equivalent to a matrix equation of the form\n:<math>A\\textbf{x}=\\textbf{0}</math>\nwhere ''A'' is an {{nowrap|''m'' &times; ''n''}} matrix, '''x''' is a column vector with ''n'' entries, and '''0''' is the [[zero vector]] with ''m'' entries.\n\n===Homogeneous solution set===\nEvery homogeneous system has at least one solution, known as the '''zero solution''' (or '''trivial solution'''), which is obtained by assigning the value of zero to each of the variables. If the system has a [[non-singular matrix]] ('''det'''('''A''') ≠ 0) then it is also the only solution.  If the system has a singular matrix then there is a solution set with an infinite number of solutions.  This solution set has the following additional properties:\n# If '''u''' and '''v''' are two [[vector (mathematics)|vectors]] representing solutions to a homogeneous system, then the vector sum {{nowrap|'''u''' + '''v'''}} is also a solution to the system.\n# If '''u''' is a vector representing a solution to a homogeneous system, and ''r'' is any [[scalar (mathematics)|scalar]], then ''r'''''u''' is also a solution to the system.\nThese are exactly the properties required for the solution set to be a [[Euclidean subspace|linear subspace]] of '''R'''<sup>''n''</sup>. In particular, the solution set to a homogeneous system is the same as the [[Kernel (matrix)|null space]] of the corresponding matrix ''A''.\nNumerical solutions to a homogeneous system can be found with a [[Singular value decomposition#Solving homogeneous linear equations|singular value decomposition]].\n\n===Relation to nonhomogeneous systems===\nThere is a close relationship between the solutions to a linear system and the solutions to the corresponding homogeneous system:\n:<math>A\\textbf{x}=\\textbf{b}\\qquad \\text{and}\\qquad A\\textbf{x}=\\textbf{0}\\text{.}</math>\nSpecifically, if '''p''' is any specific solution to the linear system {{nowrap|''A'''''x''' {{=}} '''b'''}}, then the entire solution set can be described as\n:<math>\\left\\{ \\textbf{p}+\\textbf{v} : \\textbf{v}\\text{ is any solution to }A\\textbf{x}=\\textbf{0} \\right\\}.</math>\nGeometrically, this says that the solution set for {{nowrap|''A'''''x''' {{=}} '''b'''}} is a [[translation (geometry)|translation]] of the solution set for {{nowrap|''A'''''x''' {{=}} '''0'''}}. Specifically, the [[flat (geometry)|flat]] for the first system can be obtained by translating the [[Euclidean subspace|linear subspace]] for the homogeneous system by the vector '''p'''.\n\nThis reasoning only applies if the system {{nowrap|''A'''''x''' {{=}} '''b'''}} has at least one solution. This occurs if and only if the vector '''b''' lies in the [[image (mathematics)|image]] of the [[linear transformation]] ''A''.\n\n==See also==\n* [[Arrangement of hyperplanes]]\n* [[Iterative refinement]]\n* [[Coates graph]]\n* [[LAPACK]] (the free standard package to solve linear equations numerically; available in [[Fortran]], [[C (programming_language)|C]], [[C++]])\n* [[Linear equation over a ring]]\n* [[Linear least squares (mathematics)|Linear least squares]]\n* [[Matrix decomposition]]\n* [[Matrix splitting]]\n* [[NAG Numerical Library]] (NAG Library versions of LAPACK solvers) \n* [[Simultaneous equations]]\n* [[Moore–Penrose pseudoinverse]]\n\n==References==\n{{reflist}}\n\n==Further reading==\n* {{Citation\n | last = Axler\n | first = Sheldon Jay\n | year = 1997\n | title = Linear Algebra Done Right\n | publisher = Springer-Verlag\n | edition = 2nd\n | isbn = 0-387-98259-0\n}}\n* {{Citation\n | last = Lay\n | first = David C.\n | date = August 22, 2005\n | title = Linear Algebra and Its Applications\n | publisher = Addison Wesley\n | edition = 3rd\n | isbn = 978-0-321-28713-7\n}}\n* {{Citation\n |last        = Meyer\n |first       = Carl D.\n |date        = February 15, 2001\n |title       = Matrix Analysis and Applied Linear Algebra\n |publisher   = Society for Industrial and Applied Mathematics (SIAM)\n |isbn        = 978-0-89871-454-8\n |url         = http://www.matrixanalysis.com/DownloadChapters.html\n |deadurl     = yes\n |archiveurl  = https://web.archive.org/web/20010301161440/http://matrixanalysis.com/DownloadChapters.html\n |archivedate = March 1, 2001\n |df          = \n}}\n* {{Citation\n | last = Poole\n | first = David\n | year = 2006\n | title = Linear Algebra: A Modern Introduction\n | publisher = Brooks/Cole\n | edition = 2nd\n | isbn = 0-534-99845-3\n}}\n* {{Citation\n | last = Anton\n | first = Howard\n | year = 2005\n | title = Elementary Linear Algebra (Applications Version)\n | publisher = Wiley International\n | edition = 9th\n}}\n* {{Citation\n | last = Leon\n | first = Steven J.\n | year = 2006\n | title = Linear Algebra With Applications\n | publisher = Pearson Prentice Hall\n | edition = 7th\n}}\n* {{Citation\n | last = Strang\n | first = Gilbert \n | authorlink = Gilbert Strang \n | year = 2005\n | title = Linear Algebra and Its Applications\n | publisher = \n | edition = \n}}\n\n==External links==\n*{{Commonscat-inline}}\n\n{{linear algebra}}\n\n[[Category:Equations]]\n[[Category:Linear algebra]]\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "Triangular matrix",
      "url": "https://en.wikipedia.org/wiki/Triangular_matrix",
      "text": "{{distinguish|text=a [[triangular array]], a related concept}}\n{{for|the rings|triangular matrix ring}}\n\n[[File:Cyclic group Z4; Cayley table; powers of Gray code permutation (small).svg|thumb|[[Logical matrix|Binary]] lower unitriangular [[Toeplitz matrix|Toeplitz]] matrices, multiplied using [[Finite field|'''F'''<sub>2</sub>]] operations. They form the [[Cayley table]] of [[cyclic group|Z<sub>4</sub>]] and correspond to [[v:Gray code permutation powers#4 bit|powers of the 4-bit Gray code permutation]].]]\n\nIn the [[mathematics|mathematical]] discipline of [[linear algebra]], a '''triangular matrix''' is a special kind of [[square matrix|square]] [[matrix (mathematics)|matrix]]. A square matrix is called '''{{visible anchor|lower triangular}}''' if all the entries ''above'' the [[main diagonal]] are zero. Similarly, a square matrix is called '''{{visible anchor|upper triangular}}''' if all the entries ''below'' the [[main diagonal]] are zero. A triangular matrix is one that is either lower triangular or upper triangular. A matrix that is both upper and lower triangular is called a [[diagonal matrix]].\n\nBecause matrix equations with triangular matrices are easier to solve, they are very important in [[numerical analysis]]. By the [[LU decomposition]] algorithm, an [[invertible matrix]] may be written as the product of a lower triangular matrix ''L'' and an upper triangular matrix ''U'' [[if and only if]] all its leading principal [[minor (linear algebra)|minors]] are non-zero.\n\n== Description ==\nA matrix of the form \n:<math>L = \\begin{bmatrix}\n  \\ell_{1,1} &            &        &              &          0 \\\\\n  \\ell_{2,1} & \\ell_{2,2} &        &              &            \\\\\n  \\ell_{3,1} & \\ell_{3,2} & \\ddots &              &            \\\\\n      \\vdots &     \\vdots & \\ddots &       \\ddots &            \\\\\n  \\ell_{n,1} & \\ell_{n,2} & \\ldots & \\ell_{n,n-1} & \\ell_{n,n}\n\\end{bmatrix}</math>\n\nis called a '''lower triangular matrix''' or '''left triangular matrix''', and analogously a matrix of the form\n:<math>U = \\begin{bmatrix}\n  u_{1,1} & u_{1,2} & u_{1,3} & \\ldots &   u_{1,n} \\\\\n          & u_{2,2} & u_{2,3} & \\ldots &   u_{2,n} \\\\\n          &         &  \\ddots & \\ddots &    \\vdots \\\\\n          &         &         & \\ddots & u_{n-1,n} \\\\\n        0 &         &         &        &   u_{n,n}\n\\end{bmatrix}</math>\n\nis called an '''upper triangular matrix''' or '''right triangular matrix'''. A lower or left triangular matrix is commonly denoted with the variable ''L'', and an upper or right triangular matrix is commonly denoted with the variable ''U'' or ''R''.\n\nA matrix that is both upper and lower triangular is [[diagonal matrix|diagonal]]. Matrices that are [[similar (linear algebra)|similar]] to triangular matrices are called '''triangularisable'''.\n\nUpper triangularity is preserved by many operations: \n* The sum of two upper triangular matrices is upper triangular.\n* The product of two upper triangular matrices is upper triangular.\n* The inverse of an upper triangular matrix, where extant, is upper triangular.\n* The product of an upper triangular matrix and a scalar is upper triangular. \n\nTogether these facts mean that the upper triangular matrices form a [[subalgebra]] of the [[associative algebra]] of square matrices for a given size. Additionally, this also shows that the upper triangular matrices can be viewed as a Lie subalgebra of the [[Lie algebra]] of square matrices of a fixed size, where the [[Lie bracket]] [''a'', ''b''] given by the [[commutator#Ring theory|commutator]] {{nowrap|''ab − ba''}}. The Lie algebra of all upper triangular matrices is a [[solvable Lie algebra]]. It is often referred to as a [[Borel subalgebra]] of the Lie algebra of all square matrices.\n\nAll these results hold if ''upper triangular'' is replaced by ''lower triangular'' throughout; in particular the lower triangular matrices also form a Lie algebra. However, operations mixing upper and lower triangular matrices do not in general produce triangular matrices. For instance, the sum of an upper and a lower triangular matrix can be any matrix; the product of a lower triangular with an upper triangular matrix is not necessarily triangular either.\n\n{{see also|Affine group}}\n===Examples===\n\nThis matrix\n:<math>\\begin{bmatrix}\n  1 & 4 & 1 \\\\\n  0 & 6 & 4 \\\\\n  0 & 0 & 1 \\\\\n\\end{bmatrix}</math>\n\nis upper triangular and this matrix\n:<math>\\begin{bmatrix}\n  1 & 0 & 0 \\\\\n  2 & 8 & 0 \\\\\n  4 & 9 & 7 \\\\\n\\end{bmatrix}</math>\n\nis lower triangular.\n\n==Special forms==\n=== Unitriangular matrix ===\n\nIf the entries on the [[main diagonal]] of a (upper or lower) triangular matrix are all 1, the matrix is called (upper or lower) '''unitriangular'''. All unitriangular matrices are [[unipotent]]. Other names used for these matrices are '''unit''' (upper or lower) '''triangular''' (of which \"unitriangular\" might be a contraction), or very rarely '''normed''' (upper or lower) '''triangular'''. However a ''unit'' triangular matrix is not the same as '''the''' ''[[identity matrix|unit matrix]]'', and a ''normed'' triangular matrix has nothing to do with the notion of [[matrix norm]]. The [[identity matrix]] is the only matrix which is both upper and lower unitriangular.\n\nThe set of unitriangular matrices forms a [[Lie group]].\n\n=== Strictly triangular matrix ===\n\nIf all of the entries on the main diagonal of a (upper or lower) triangular matrix are 0, the matrix is called '''strictly''' (upper or lower) '''triangular'''. All strictly triangular matrices are [[nilpotent matrix|nilpotent]], and the set of strictly upper (or lower) triangular matrices forms a [[nilpotent Lie algebra]], denoted <math>\\mathfrak{n}.</math> This algebra is the [[derived Lie algebra]] of <math>\\mathfrak{b}</math>, the Lie algebra of all upper triangular matrices; in symbols, <math>\\mathfrak{n} = [\\mathfrak{b},\\mathfrak{b}].</math> In addition, <math>\\mathfrak{n}</math> is the Lie algebra of the Lie group of unitriangular matrices.\n\nIn fact, by [[Engel's theorem]], any finite-dimensional nilpotent Lie algebra is conjugate to a subalgebra of the strictly upper triangular matrices, that is to say, a finite-dimensional nilpotent Lie algebra is simultaneously strictly upper triangularizable.\n\n=== Atomic triangular matrix ===\n{{see also|Frobenius matrix}}\nAn '''atomic''' (upper or lower) '''triangular matrix''' is a special form of unitriangular matrix, where all of the [[off-diagonal element]]s are zero, except for the entries in a single column. Such a matrix is also called a '''Frobenius matrix''', a '''Gauss matrix''', or a '''Gauss transformation matrix'''. So an atomic lower triangular matrix is of the form\n:<math> \\mathbf{L}_{i} =\n\\begin{bmatrix}\n     1 &        &        &           &        &         &     & 0 \\\\\n     0 & \\ddots &        &           &        &         &     &   \\\\\n     0 & \\ddots &      1 &           &        &         &     &   \\\\\n     0 & \\ddots &      0 &         1 &        &         &     &   \\\\\n       &        &      0 & \\ell_{i+1,i} &      1 &         &     &   \\\\\n\\vdots &        &      0 & \\ell_{i+2,i} &      0 &  \\ddots &     &   \\\\\n       &        & \\vdots &    \\vdots & \\vdots &  \\ddots &   1 &   \\\\\n     0 &  \\dots &      0 &   \\ell_{n,i} &      0 &   \\dots &   0 & 1 \\\\\n\\end{bmatrix}.\n</math>\nThe inverse of an '''atomic''' triangular matrix is again atomic triangular. Indeed, we have\n:<math> \\mathbf{L}_{i}^{-1} =\n\\begin{bmatrix}\n     1 &        &        &            &        &         &     & 0 \\\\\n     0 & \\ddots &        &            &        &         &     &   \\\\\n     0 & \\ddots &      1 &            &        &         &     &   \\\\\n     0 & \\ddots &      0 &          1 &        &         &     &   \\\\\n       &        &      0 & -\\ell_{i+1,i} &      1 &         &     &   \\\\\n\\vdots &        &      0 & -\\ell_{i+2,i} &      0 &  \\ddots &     &   \\\\\n       &        & \\vdots &     \\vdots & \\vdots &  \\ddots &   1 &   \\\\\n     0 &  \\dots &      0 &   -\\ell_{n,i} &      0 &   \\dots &   0 & 1 \\\\\n\\end{bmatrix},\n</math>\ni.e., the off-diagonal entries are replaced in the inverse matrix by their additive inverses.\n\n==== Examples ====\n\nThe matrix\n:<math>\n\\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 4 & 1 & 0 \\\\\n0 & 2 & 0 & 1 \\\\\n\\end{bmatrix}\n</math>\nis atomic lower triangular. Its inverse is\n:<math>\n\\begin{bmatrix}\n1 &  0 & 0 & 0 \\\\\n0 &  1 & 0 & 0 \\\\\n0 & -4 & 1 & 0 \\\\\n0 & -2 & 0 & 1 \\\\\n\\end{bmatrix}.\n</math>\n\n==Special properties==\n\nA matrix which is simultaneously triangular and [[normal matrix|normal]] is also diagonal. This can be seen by looking at the diagonal entries of ''A''<sup>*</sup>''A'' and ''AA''<sup>*</sup>, where ''A'' is a normal, triangular matrix. \n\nThe [[transpose]] of an upper triangular matrix is a lower triangular matrix and vice versa.\n\nThe [[determinant]] of a triangular matrix equals the product of the diagonal entries. Since for any triangular matrix&nbsp;''A'' the matrix <math>\\lambda I-A</math>, whose determinant is the [[characteristic polynomial]] of ''A'', is also triangular, the diagonal entries of ''A'' in fact give the [[multiset]] of  [[eigenvalue]]s of ''A'' (an eigenvalue with multiplicity ''m'' occurs exactly ''m'' times as diagonal entry).<ref name=\"axler\">{{Harv|Axler|1996|loc=pp. 86&ndash;87, 169}}</ref>\n\n==Triangularisability==\nA matrix that is [[similar matrix|similar]] to a triangular matrix is referred to as '''triangularizable'''. Abstractly, this is equivalent to stabilizing a [[flag (linear algebra)|flag]]: upper triangular matrices are precisely those that preserve the [[standard flag]], which is given by the standard ordered basis <math>(e_1,\\ldots,e_n)</math> and the resulting flag <math>0 < \\left\\langle e_1\\right\\rangle < \\left\\langle e_1,e_2\\right\\rangle < \\cdots < \\left\\langle e_1,\\ldots,e_n \\right\\rangle = K^n.</math> All flags are conjugate (as the general linear group acts transitively on bases), so any matrix that stabilises a flag is similar to one that stabilises the standard flag.\n\nAny complex square matrix is triangularizable.<ref name=\"axler\"/> In fact, a matrix ''A'' over a [[field (mathematics)|field]] containing all of the eigenvalues of ''A'' (for example, any matrix over an [[algebraically closed field]]) is similar to a triangular matrix. This can be proven by using induction on the fact that ''A'' has an eigenvector, by taking the quotient space by the eigenvector and inducting to show that ''A'' stabilises a flag, and is thus triangularizable with respect to a basis for that flag.\n\nA more precise statement is given by the [[Jordan normal form]] theorem, which states that in this situation, ''A'' is similar to an upper triangular matrix of a very particular form. The simpler triangularization result is often sufficient however, and in any case used in proving the Jordan normal form theorem.<ref name=\"axler\"/><ref name=\"herstein\">{{Harv|Herstein|1975|loc=pp. 285&ndash;290}}</ref>\n\nIn the case of complex matrices, it is possible to say more about triangularization, namely, that any square matrix ''A'' has a [[Schur decomposition]]. This means that ''A'' is unitarily equivalent (i.e. similar, using a [[unitary matrix]] as change of basis) to an upper triangular matrix; this follows by taking an Hermitian basis for the flag.\n\n===Simultaneous triangularisability===\n{{see also|Simultaneously diagonalizable}}\nA set of matrices <math>A_1, \\ldots, A_k</math> are said to be '''{{visible anchor|simultaneously triangularisable}}''' if there is a basis under which they are all upper triangular; equivalently, if they are upper triangularizable by a single similarity matrix ''P.'' Such a set of matrices is more easily understood by considering the algebra of matrices it generates, namely all polynomials in the <math>A_i,</math> denoted <math>K[A_1,\\ldots,A_k].</math> Simultaneous triangularizability means that this algebra is conjugate into the Lie subalgebra of upper triangular matrices, and is equivalent to this algebra being a Lie subalgebra of a [[Borel subalgebra]].\n\nThe basic result is that (over an algebraically closed field), the [[commuting matrices]] <math>A,B</math> or more generally <math>A_1,\\ldots,A_k</math> are simultaneously triangularizable. This can be proven by first showing that commuting matrices have a common eigenvector, and then inducting on dimension as before. This was proven by Frobenius, starting in 1878 for a commuting pair, as discussed at [[commuting matrices]]. As for a single matrix, over the complex numbers these can be triangularized by unitary matrices.\n\nThe fact that commuting matrices have a common eigenvector can be interpreted as a result of [[Hilbert's Nullstellensatz]]: commuting matrices form a commutative algebra <math>K[A_1,\\ldots,A_k]</math> over <math>K[x_1,\\ldots,x_k]</math> which can be interpreted as a variety in ''k''-dimensional affine space, and the existence of a (common) eigenvalue (and hence a common eigenvector) corresponds to this variety having a point (being non-empty), which is the content of the (weak) Nullstellensatz. In algebraic terms, these operators correspond to an [[algebra representation]] of the polynomial algebra in ''k'' variables.\n\nThis is generalized by [[Lie's theorem]], which shows that any representation of a [[solvable Lie algebra]] is simultaneously upper triangularizable, the case of commuting matrices being the [[abelian Lie algebra]] case, abelian being a fortiori solvable.\n\nMore generally and precisely, a set of matrices <math>A_1,\\ldots,A_k</math> is simultaneously triangularisable if and only if the matrix <math>p(A_1,\\ldots,A_k)[A_i,A_j]</math> is [[nilpotent]] for all polynomials ''p'' in ''k'' ''non''-commuting variables, where <math>[A_i,A_j]</math> is the [[commutator]]; note that for commuting <math>A_i</math> the commutator vanishes so this holds. This was proven in {{Harv|Drazin|Dungey|Gruenberg|1951}}; a brief proof is given in {{Harv|Prasolov|1994|loc=[https://books.google.com/books?id=fuONq1od6nsC&pg=PA178 pp. 178–179]}}. One direction is clear: if the matrices are simultaneously triangularisable, then <math>[A_i, A_j]</math> is ''strictly'' upper triangularizable (hence nilpotent), which is preserved by multiplication by any <math>A_k</math> or combination thereof – it will still have 0s on the diagonal in the triangularizing basis.\n\n==Generalizations==\nBecause the product of two upper triangular matrices is again upper triangular, the set of upper triangular matrices forms an [[associative algebra|algebra]]. Algebras of upper triangular matrices have a natural generalization in [[functional analysis]] which yields [[nest algebra]]s on [[Hilbert space]]s.\n\nA non-square (or sometimes any) matrix with zeros above (below) the diagonal is called a lower (upper) trapezoidal matrix. The non-zero entries form the shape of a [[trapezoid]].\n\n===Borel subgroups and Borel subalgebras===\n{{main|Borel subgroup|Borel subalgebra}}\nThe set of invertible triangular matrices of a given kind (upper or lower) forms a [[group (mathematics)|group]], indeed a [[Lie group]], which is a subgroup of the [[general linear group]] of all invertible matrices. Note that a triangular matrix is invertible precisely\nwhen its diagonal entries are invertible (non-zero).\n\nOver the real numbers, this group is disconnected, having <math>2^n</math> components accordingly as each diagonal entry is positive or negative. The identity component is invertible triangular matrices with positive entries on the diagonal, and the group of all invertible triangular matrices is a [[semidirect product]] of this group and diagonal entries with <math>\\pm 1</math> on the diagonal, corresponding to the components.\n\nThe [[Lie algebra]] of the Lie group of invertible upper triangular matrices is the set of all upper triangular matrices, not necessarily invertible, and is a [[solvable Lie algebra]]. These are, respectively, the standard [[Borel subgroup]] ''B'' of the Lie group GL<sub>n</sub> and the standard [[Borel subalgebra]] <math>\\mathfrak{b}</math> of the Lie algebra gl<sub>n</sub>.\n\nThe upper triangular matrices are precisely those that stabilize the [[Flag (linear algebra)|standard flag]]. The invertible ones among them form a subgroup of the general linear group, whose conjugate subgroups are those defined as the stabilizer of some (other) complete flag. These subgroups are [[Borel subgroup]]s. The group of invertible lower triangular matrices is such a subgroup, since it is the stabilizer of the standard flag associated to the standard basis in reverse order.\n\nThe stabilizer of a partial flag obtained by forgetting some parts of the standard flag can be described as a set of block upper triangular matrices (but its elements are ''not'' all triangular matrices). The conjugates of such a group are the subgroups defined as the stabilizer of some partial flag. These subgroups are called [[parabolic subgroup]]s.\n\n=== Examples ===\nThe group of 2 by 2 upper unitriangular matrices is [[isomorphic]] to the [[Abelian group|additive group]] of the field of scalars; in the case of complex numbers it corresponds to a group formed of parabolic [[Möbius transformation]]s; the 3 by 3 upper unitriangular matrices form the [[Heisenberg group]].\n\n==Forward and back substitution==\n<!-- Section is linked from several redirects (Back substitution etc.) – please update if you change the section title -->\n\nA matrix equation in the form <math>\\mathbf{L}\\mathbf{x} = \\mathbf{b}</math> or <math>\\mathbf{U}\\mathbf{x} = \\mathbf{b}</math> is very easy to solve by an iterative process called '''forward substitution''' for lower triangular matrices and analogously '''back substitution''' for upper triangular matrices.\nThe process is so called because for lower triangular matrices, one first computes <math>x_1</math>, then substitutes that ''forward'' into the ''next'' equation to solve for <math>x_2</math>, and repeats through to <math>x_n</math>. In an upper triangular matrix, one works ''backwards,'' first computing <math>x_n</math>, then substituting that ''back'' into the ''previous'' equation to solve for <math>x_{n-1}</math>, and repeating through <math>x_1</math>.\n\nNotice that this does not require inverting the matrix.\n\n===Forward substitution===\nThe matrix equation '''L'''''x'' = ''b'' can be written as a system of linear equations\n\n:<math>\\begin{matrix}\n  \\ell_{1,1} x_1 &   &                &   &        &   &                & = &    b_1 \\\\\n  \\ell_{2,1} x_1 & + & \\ell_{2,2} x_2 &   &        &   &                & = &    b_2 \\\\\n          \\vdots &   &         \\vdots &   & \\ddots &   &                &   & \\vdots \\\\\n  \\ell_{m,1} x_1 & + & \\ell_{m,2} x_2 & + & \\dotsb & + & \\ell_{m,m} x_m & = &    b_m \\\\\n\\end{matrix}</math>\n\nObserve that the first equation (<math>\\ell_{1,1} x_1 = b_1</math>) only involves <math>x_1</math>, and thus one can solve for <math>x_1</math> directly. The second equation only involves <math>x_1</math> and <math>x_2</math>, and thus can be solved once one substitutes in the already solved value for <math>x_1</math>. Continuing in this way, the <math>k</math>-th equation only involves <math>x_1,\\dots,x_k</math>, and one can solve for <math>x_k</math> using the previously solved values for <math>x_1,\\dots,x_{k-1}</math>.\n\nThe resulting formulas are:\n:<math>\\begin{align}\n  x_1 &= \\frac{b_1}{\\ell_{1,1}}, \\\\\n  x_2 &= \\frac{b_2 - \\ell_{2,1} x_1}{\\ell_{2,2}}, \\\\\n      &\\ \\ \\vdots \\\\\n  x_m &= \\frac{b_m - \\sum_{i=1}^{m-1} \\ell_{m,i}x_i}{\\ell_{m,m}}.\n\\end{align}</math>\n\nA matrix equation with an upper triangular matrix '''U''' can be solved in an analogous way, only working backwards.\n\n===Applications===\nForward substitution is used in financial [[Bootstrapping (finance)|bootstrapping]] to construct a [[yield curve]].\n\n== See also ==\n* [[Gaussian elimination]]\n* [[QR decomposition]]\n* [[Cholesky decomposition]]\n* [[Hessenberg matrix]]\n* [[Tridiagonal matrix]]\n* [[Invariant subspace]]\n\n== Notes ==\n{{reflist|group=note}}\n\n== Glossary ==\n\n{{term|unit upper triangular matrix}}\n{{defn|a [[unitriangular matrix|unitriangular]] [[upper triangular matrix]] }}\n\n{{term|unit lower triangular matrix}}\n{{defn|a [[unitriangular matrix|unitriangular]] [[lower triangular matrix]] }}\n\n== References ==\n{{reflist}}\n{{refbegin}} \n* {{Citation | first = Sheldon | last = Axler | title = Linear Algebra Done Right | publisher = Springer-Verlag | year = 1996 | isbn=0-387-98258-2}}\n* {{Citation | first1 = M. P. | last1 = Drazin | first2 = J. W. | last2 = Dungey | first3 = K. W. | last3 = Gruenberg | title = Some theorems on commutative matrices | journal = J. London Math. Soc. | volume = 26 | pages = 221–228 | year = 1951 | url = http://jlms.oxfordjournals.org/cgi/pdf_extract/s1-26/3/221 |doi=10.1112/jlms/s1-26.3.221 | issue = 3}}\n* {{Citation | first = I. N. | last = Herstein | title=Topics in Algebra | edition=2nd | publisher=John Wiley and Sons | year = 1975 | isbn = 0-471-01090-1}}\n* {{Citation | title = Problems and theorems in linear algebra | first = Viktor | last = Prasolov | year = 1994 | url = https://books.google.com/books?id=fuONq1od6nsC&lpg=PP1&dq=victor%20prasolov%20Problems%20and%20theorems%20in%20linear%20algebra&pg=PP1#v=onepage&q&f=false | isbn = 9780821802366 }}\n{{refend}}\n\n{{Numerical linear algebra}}\n\n{{DEFAULTSORT:Triangular Matrix}}\n[[Category:Numerical linear algebra]]\n[[Category:Matrices]]"
    },
    {
      "title": "Tridiagonal matrix algorithm",
      "url": "https://en.wikipedia.org/wiki/Tridiagonal_matrix_algorithm",
      "text": "In [[numerical linear algebra]], the '''tridiagonal matrix algorithm''', also known as the '''Thomas algorithm''' (named after [[Llewellyn Thomas]]), is a simplified form of [[Gaussian elimination]] that can be used to solve [[Tridiagonal matrix|tridiagonal systems of equations]].  A tridiagonal system for ''n'' unknowns may be written as\n\n:<math>\na_i x_{i - 1}  + b_i x_i  + c_i x_{i + 1}  = d_i , \\,\\!</math>\nwhere <math> a_1  = 0\\, </math> and <math> c_n = 0\\, </math>.\n\n:<math>\n\\begin{bmatrix}\n   {b_1} & {c_1} & {   } & {   } & { 0 } \\\\\n   {a_2} & {b_2} & {c_2} & {   } & {   } \\\\\n   {   } & {a_3} & {b_3} & \\ddots & {   } \\\\\n   {   } & {   } & \\ddots & \\ddots & {c_{n-1}}\\\\\n   { 0 } & {   } & {   } & {a_n} & {b_n}\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n   {x_1 }  \\\\\n   {x_2 }  \\\\\n   {x_3 }  \\\\\n   \\vdots   \\\\\n   {x_n }  \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n   {d_1 }  \\\\\n   {d_2 }  \\\\\n   {d_3 }  \\\\\n   \\vdots   \\\\\n   {d_n }  \\\\\n\\end{bmatrix}\n.\n</math>\n\nFor such systems, the solution can be obtained in <math>O(n)</math> operations instead of <math>O(n^3)</math> required by [[Gaussian elimination]]. A first sweep eliminates the <math>a_i</math>'s, and then an (abbreviated) backward substitution produces the solution. Examples of such matrices commonly arise from the discretization of 1D [[Poisson equation]] and natural cubic [[spline interpolation]].\n\nThomas' algorithm is not [[numerical stability|stable]] in general, but is so in several special cases, such as when the matrix is [[diagonally dominant]] (either by rows or columns) or [[symmetric positive definite]];<ref name=\"Niyogi2006\">{{cite book|author=Pradip Niyogi|title=Introduction to Computational Fluid Dynamics|year=2006|publisher=Pearson Education India|isbn=978-81-7758-764-7|page=76}}</ref><ref name=\"Datta2010\"/> for a more precise characterization of stability of Thomas' algorithm, see Higham Theorem 9.12.<ref name=\"Higham2002\">{{cite book|author=Nicholas J. Higham|title=Accuracy and Stability of Numerical Algorithms: Second Edition|year=2002|publisher=SIAM|isbn=978-0-89871-802-7|page=175}}</ref> If stability is required in the general case, [[Gaussian elimination]] with [[partial pivoting]] (GEPP) is recommended instead.<ref name=\"Datta2010\">{{cite book|author=Biswa Nath Datta|title=Numerical Linear Algebra and Applications, Second Edition|year=2010|publisher=SIAM|isbn=978-0-89871-765-5|page=162}}</ref>\n\n==Method==\nThe forward sweep consists of modifying the coefficients as follows, denoting the new coefficients with primes:\n\n:<math>c'_i =\n\\begin{cases}\n\\begin{array}{lcl}\n  \\cfrac{c_i}{b_i}                  & ; & i = 1 \\\\\n  \\cfrac{c_i}{b_i - a_i c'_{i - 1}} & ; & i = 2, 3, \\dots, n-1 \\\\\n\\end{array}\n\\end{cases}\n\\,</math>\n\nand\n\n:<math>d'_i =\n\\begin{cases}\n\\begin{array}{lcl}\n  \\cfrac{d_i}{b_i}                  & ; & i = 1 \\\\\n  \\cfrac{d_i - a_i d'_{i - 1}}{b_i - a_i c'_{i - 1}} & ; & i = 2, 3, \\dots, n. \\\\\n\\end{array}\n\\end{cases}\n\\,</math>\n\nThe solution is then obtained by back substitution:\n\n:<math>x_n = d'_n\\,</math>\n\n:<math>x_i = d'_i - c'_i x_{i + 1} \\qquad ; \\ i = n - 1, n - 2, \\ldots, 1.</math>\n\nThe method above preserves the original coefficient vectors. If this is not required, then a much simpler form of the algorithm is\n\n:<math>\n\\begin{array}{lcl}\nw_i=\\cfrac{a_i}{b_{i-1}}\\\\\nb_i:=b_i-w_i c_{i-1}\\\\\nd_i:=d_i-w_i d_{i-1}\n\\end{array} ,   i=2,3, \\dots, n\n</math>\n\nfollowed by the back substitution\n\n:<math>\n\\begin{array}{lcl}\nx_n=\\cfrac{d_n}{b_n}\\\\\nx_i=\\cfrac{d_i-c_i x_{i+1}}{b_i} ,   i=n-1,n-2, \\dots, 1\n\\end{array}\n</math>\n\nThe implementation in a VBA subroutine without preserving the coefficient vectors is shown below\n\n\n Sub TriDiagonal_Matrix_Algorithm(N%, A#(), B#(), C#(), D#(), X#())\n  Dim i%, W#\n  For i = 2 To N\n   W = A(i) / B(i - 1)\n   B(i) = B(i) - W * C(i - 1)\n   D(i) = D(i) - W * D(i - 1)\n  Next i\n  X(N) = D(N) / B(N)\n  For i = N - 1 To 1 Step -1\n   X(i) = (D(i) - C(i) * X(i + 1)) / B(i)\n  Next i\n End Sub\n\n==Derivation==\nThe derivation of the tridiagonal matrix algorithm is a special case of [[Gaussian elimination]].\n\nSuppose that the unknowns are <math>x_1,\\ldots, x_n</math>, and that the equations to be solved are:\n\n:<math>\\begin{align}\nb_1 x_1 + c_1 x_2 & = d_1;& i & = 1 \\\\\na_i x_{i - 1} + b_i x_i  + c_i x_{i + 1} & = d_i;& i & = 2, \\ldots, n - 1 \\\\\na_n x_{n - 1} + b_n x_n & = d_n;& i & = n.\n\\end{align}\n</math>\n\nConsider modifying the second (<math>i = 2</math>) equation with the first equation as follows:\n\n:<math>\n(\\mbox{equation 2}) \\cdot b_1 - (\\mbox{equation 1}) \\cdot a_2\n</math>\n\nwhich would give:\n\n:<math>\n(a_2 x_1 + b_2 x_2  + c_2 x_3) b_1 - (b_1 x_1  + c_1 x_2) a_2 = d_2 b_1 - d_1 a_2\n\\,</math>\n\n:<math>\n(b_2 b_1 - c_1 a_2) x_2  + c_2 b_1 x_3 = d_2 b_1 - d_1 a_2\n\\,</math>\n\nwhere the second equation immediately above is a simplified version of the equation immediately preceding it.  The effect is that <math>x_1</math> has been eliminated from the second equation. Using a similar tactic with the '''modified''' second equation on the third equation yields:\n\n:<math>\n(a_3 x_2 + b_3 x_3 + c_3 x_4) (b_2 b_1 - c_1 a_2) -\n((b_2 b_1 - c_1 a_2) x_2 + c_2 b_1 x_3) a_3\n= d_3 (b_2 b_1 - c_1 a_2) - (d_2 b_1 - d_1 a_2) a_3\n\\,</math>\n\n:<math>\n(b_3 (b_2 b_1 - c_1 a_2) - c_2 b_1 a_3 )x_3 + c_3 (b_2 b_1 - c_1 a_2) x_4\n= d_3 (b_2 b_1 - c_1 a_2) - (d_2 b_1 - d_1 a_2) a_3.\n\\,</math>\n\nThis time <math>x_2</math> was eliminated. If this procedure is repeated until the <math>n^{th}</math> row; the (modified) <math>n^{th}</math> equation will involve only one unknown, <math>x_n</math>. This may be solved for and then used to solve the <math>(n - 1)^{th}</math> equation, and so on until all of the unknowns are solved for.\n\nClearly, the coefficients on the modified equations get more and more complicated if stated explicitly. By examining the procedure, the modified coefficients (notated with tildes) may instead be defined recursively:\n\n:<math>\\tilde a_i = 0\\,</math>\n\n:<math>\\tilde b_1 = b_1\\,</math>\n:<math>\\tilde b_i = b_i \\tilde b_{i - 1} - \\tilde c_{i - 1} a_i\\,</math>\n\n:<math>\\tilde c_1 = c_1\\,</math>\n:<math>\\tilde c_i = c_i \\tilde b_{i - 1}\\,</math>\n\n:<math>\\tilde d_1 = d_1\\,</math>\n:<math>\\tilde d_i = d_i \\tilde b_{i - 1} - \\tilde d_{i - 1} a_i.\\,</math>\n\nTo further hasten the solution process, <math>\\tilde b_i</math> may be divided out (if there's no division by zero risk), the newer modified coefficients, each notated with a prime, will be:\n\n:<math>a'_i = 0\\,</math>\n\n:<math>b'_i = 1\\,</math>\n\n:<math>c'_1 = \\frac{c_1}{b_1}\\,</math>\n:<math>c'_i = \\frac{c_i}{b_i - c'_{i - 1} a_i}\\,</math>\n\n:<math>d'_1 = \\frac{d_1}{b_1}\\,</math>\n:<math>d'_i = \\frac{d_i - d'_{i - 1} a_i}{b_i - c'_{i - 1} a_i}.\\,</math>\n\nThis gives the following system with the same unknowns and coefficients defined in terms of the original ones above:\n\n:<math>\\begin{array}{lcl}\nx_i + c'_i x_{i + 1} = d'_i \\qquad &;& \\ i = 1, \\ldots, n - 1 \\\\\nx_n = d'_n \\qquad &;& \\ i = n. \\\\\n\\end{array}\n\\,</math>\n\nThe last equation involves only one unknown. Solving it in turn reduces the next last equation to one unknown, so that this backward substitution can be used to find all of the unknowns:\n\n:<math>x_n = d'_n\\,</math>\n\n:<math>x_i = d'_i - c'_i x_{i + 1} \\qquad ; \\ i = n - 1, n - 2, \\ldots, 1.</math>\n\n==Variants==\nIn some situations, particularly those involving [[periodic boundary conditions]], a slightly perturbed form of the tridiagonal system may need to be solved:\n\n:<math>\n\\begin{align}\na_1 x_{n}  + b_1 x_1  + c_1 x_2  & = d_1, \\\\\na_i x_{i - 1}  + b_i x_i  + c_i x_{i + 1}  & = d_i,\\quad\\quad i = 2,\\ldots,n-1 \\\\\na_n x_{n-1}  + b_n x_n  + c_n x_1  & = d_n.\n\\end{align}\n</math>\n\nIn this case, we can make use of the [[Sherman-Morrison formula]] to avoid the additional operations of Gaussian elimination and still use the Thomas algorithm. The method requires solving a modified non-cyclic version of the system for both the input and a sparse corrective vector, and then combining the solutions. This can be done efficiently if both solutions are computed at once, as the forward portion of the pure tridiagonal matrix algorithm can be shared.\n\nIn other situations, the system of equations may be '''block tridiagonal''' (see [[block matrix]]), with smaller submatrices arranged as the individual elements in the above matrix system (e.g., the 2D [[Poisson equation discretized into block tridiagonal|Poisson problem]]). Simplified forms of Gaussian elimination have been developed for these situations.<ref name=\"Quarteroni2007\">{{cite  book|last1=Quarteroni|first1=Alfio|last2=Sacco|first2=Riccardo|last3=Saleri|first3=Fausto | year=2007|title=Numerical Mathematics|publisher= Springer, New York|isbn= 978-3-540-34658-6|chapter=Section 3.8}}</ref>\n\nThe textbook ''Numerical Mathematics'' by Quarteroni, Sacco and Saleri, lists a modified version of the algorithm which avoids some of the divisions (using instead multiplications), which is beneficial on some computer architectures.\n\nParallel tridiagonal solvers have been published for many vector and parallel architectures, including GPUs<ref name=\"ChangHwu14\">{{cite conference|last1=Chang|first1=L.-W.|last2=Hwu|first2=W,-M.|title=A guide for implementing tridiagonal solvers on GPUs|book-title=Numerical Computations with GPUs|publisher=Springer|isbn=978-3-319-06548-9|editor=V. Kidratenko|year=2014}}</ref>\n<ref name=\"Venetis\">{{cite article|last1=Venetis|first1=I.E.|last2=Kouris|first2=A.|last3=Sobczyk|first3=A.|last4=Gallopoulos|first4=E.|last5=Sameh|first5=A.|title=A direct tridiagonal solver based on Givens rotations for GPU architectures|journal=Parallel Computing|volume=49|pages=101-116|year=2015}}</ref>\n\nFor an extensive treatment of parallel tridiagonal and block tridiagonal solvers see <ref name=\"GPS.16\">{{cite  book|last1=Gallopoulos|first1=E.|last2=Philippe|first2=B.|last3=Sameh|first3=A.H. | year=2016|title=Parallelism in Matrix Computations|publisher= Springer|isbn= 978-94-017-7188-7|chapter=Chapter 5}}</ref>\n\n==References==\n{{Wikibooks|Algorithm Implementation|Linear Algebra/Tridiagonal matrix algorithm|Tridiagonal matrix algorithm}}\n{{reflist}}\n\n*{{cite book|author1=Conte, S.D. |author2=deBoor, C. |lastauthoramp=yes |year=1972|title=Elementary Numerical Analysis|publisher= McGraw-Hill, New York|isbn= 0070124469}}\n*{{CFDWiki|name=Tridiagonal_matrix_algorithm_-_TDMA_(Thomas_algorithm)}}\n*{{Cite book|last1=Press|first1=WH|last2=Teukolsky|first2=SA|last3=Vetterling|first3=WT|last4=Flannery|first4=BP|year=2007|title=Numerical Recipes: The Art of Scientific Computing|edition=3rd|publisher=Cambridge University Press| publication-place=New York|isbn=978-0-521-88068-8|chapter=Section 2.4|chapter-url=http://apps.nrbook.com/empanel/index.html?pg=56|postscript=<!-- Bot inserted parameter. Either remove it; or change its value to \".\" for the cite to end in a \".\", as necessary. -->{{inconsistent citations}}}}\n\n{{Numerical linear algebra}}\n\n{{DEFAULTSORT:Tridiagonal Matrix Algorithm}}\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "Vandermonde matrix",
      "url": "https://en.wikipedia.org/wiki/Vandermonde_matrix",
      "text": "In [[linear algebra]], a '''Vandermonde matrix''', named after [[Alexandre-Théophile Vandermonde]], is a [[matrix (math)|matrix]] with the terms of a [[geometric progression]] in each row, i.e., an ''m''&nbsp;&times;&nbsp;''n'' matrix\n:<math>V=\\begin{bmatrix}\n1 & \\alpha_1 & \\alpha_1^2 & \\dots & \\alpha_1^{n-1}\\\\\n1 & \\alpha_2 & \\alpha_2^2 & \\dots & \\alpha_2^{n-1}\\\\\n1 & \\alpha_3 & \\alpha_3^2 & \\dots & \\alpha_3^{n-1}\\\\\n\\vdots & \\vdots & \\vdots & \\ddots &\\vdots \\\\\n1 & \\alpha_m & \\alpha_m^2 & \\dots & \\alpha_m^{n-1}\n\\end{bmatrix},</math>\nor\n:<math>V_{i,j} = \\alpha_i^{j-1} \\, </math>\nfor all indices ''i'' and ''j''.<ref>Roger A. Horn and Charles R. Johnson (1991), ''Topics in matrix analysis'', Cambridge University Press. ''See Section 6.1''.</ref> (Some authors use the [[transpose]] of the above matrix.)\n\nThe [[determinant]] of a square Vandermonde matrix (where ''m''&nbsp;=&nbsp;''n'') can be expressed as\n\n:<math>\\det(V) = \\prod_{1 \\le i < j \\le n} (\\alpha_j - \\alpha_i). </math>\n\nThis is called the '''Vandermonde determinant''' or '''[[Vandermonde polynomial]].''' If all the numbers <math>\\alpha_i</math> are distinct, then it is non-zero.\n\nThe Vandermonde determinant was sometimes called the [[discriminant]], although, presently, the discriminant is the square of the Vandermonde determinant. The Vandermonde determinant is an [[alternating form]] in the <math>\\alpha_i</math>, meaning that exchanging two <math>\\alpha_i</math> changes the sign, while permuting the <math>\\alpha_i</math> by an [[even permutation]] does not change the value of the determinant. It thus depends on the choice of an order on the <math>\\alpha_i</math>, while its square, the discriminant, does not depend on any order, and this implies, by [[Galois theory]], that the discriminant is a [[polynomial function]] of the coefficients of the polynomial that has the <math>\\alpha_i</math> as roots.\n\n==Proof==\nThe main property of a square Vandermonde matrix\n:<math>V=\\begin{bmatrix}\n1 & x_1 & x_1^2 & \\dots & x_1^{n-1}\\\\\n1 & x_2 & x_2^2 & \\dots & x_2^{n-1}\\\\\n1 & x_3 & x_3^2 & \\dots & x_3^{n-1}\\\\\n\\vdots & \\vdots & \\vdots & \\ddots &\\vdots \\\\\n1 & x_n & x_n^2 & \\dots & x_n^{n-1}\n\\end{bmatrix}</math>\nis that its determinant has the simple form\n:<math>\\det(V)=\\prod_{1\\le i<j\\le n} (x_j-x_i).</math>\n\nThis may be proved either by using properties of polynomials or [[elementary matrix|elementary row and column operations]]. The former is simpler but it is non-constructive and uses [[unique factorization domain|unique factorization property]] of [[multivariate polynomial]]s. The latter is constructive and more elementary, at the price of being more complicated. A third proof, based on Gaussian elimination, is sketched. It is still more complicated, if written in details, but provides the U-part of the [[LU decomposition]] of Vandermonde matrices.\n\n===Using polynomial properties===\n\nBy [[Leibniz formula (determinant)|Leibniz formula]], {{math|det(''V'')}} is a polynomial in the <math>x_i,</math> with integer coefficients. All entries of the {{mvar|i}}th column have the [[total degree]] {{math|''i'' – 1}}. Thus, again by Leibniz formula, all terms of the determinant have the total degree \n:<math>0 + 1 + 2 + \\cdots + (n-1) = \\frac{n(n-1)}2;</math>\n(that is the determinant is a [[homogeneous polynomial]] of this degree).\n\nIf, for {{math|''i'' ≠ ''j''}}, one substitutes <math>x_i</math> for <math>x_j</math>, one gets a matrix with two equal columns, which has thus a zero determinant. Thus, by the [[factor theorem]], <math>x_j-x_i</math> is a divisor of {{math|det(''V'')}}. By [[unique factorization domain|unique factorization property]] of [[multivariate polynomial]]s, the product of all <math>x_j-x_i</math> divides {{math|det(''V'')}}, that is \n:<math>\\det(V)=Q\\prod_{1\\le i<j\\le n} (x_j-x_i),</math>\nwhere {{mvar|Q}} is a polynomial. As the product of all <math>x_j-x_i</math> and {{math|det(''V'')}} have the same degree <math>n(n -1)/2,</math> the polynomial {{mvar|Q}} is, in fact, a constant. This constant is one, because the product of the diagonal entries of {{mvar|V}} is \n<math>x_2x_3^2\\cdots x_n^{n-1},</math>\nwhich is also the [[monomial]] that is obtained by taking the first term of all factors in <math>\\textstyle \\prod_{1\\le i<j\\le n} (x_j-x_i).</math> This proves that \n:<math>\\det(V)=\\prod_{1\\le i<j\\le n} (x_j-x_i).</math>\n\n===By row and column operations===\n\nThis second proof is based on the fact that, if one adds to a row (or a column) of a matrix the product by a scalar of another row  (or column), the determinant remains unchanged.\n\nIf one subtracts the first row of {{mvar|V}} to all the other rows, the determinant is not changed, and the new matrix has the form\n:<math>\\begin{bmatrix}\n1&\\mathbf L\\\\\\mathbf 0& A\n\\end{bmatrix},</math>\nwhere <math>\\mathbf L</math> is a row matrix, <math>\\mathbf 0</math> is a column of zeros, and {{mvar|A}} is a square matrix, such that \n:<math>\\det(A) = \\det(V).</math>\nThe entry of the {{math|(''i'' – 1)}}th row and the {{math|(''j'' – 1)}}th column of {{mvar|A}} (that is the {{mvar|i}}th row and the {{mvar|j}}th column of the whole matrix) is \n:<math>x_i^{j-1} -x_1^{j-1}=(x_i-x_1)\\sum_{k=0}^{j-2} x_i^kx_1^{j-2-k}.</math>\nDividing out <math>x_i-x_1</math> from the {{math|(''i'' – 1)}}th row of {{mvar|A}}, for {{math|1=''i'' = 2, ..., ''n''}}, one gets a matrix {{mvar|B}} such that\n:<math>\\det(V)=\\det(A)=\\det(B)\\prod_{i=2}^n (x_i-x_1).</math>\nThe coefficient of the  {{math|(''i'' – 1)}}th row and the {{math|(''j'' – 1)}}th column of {{mvar|B}} is\n:<math>b_{i,j}=\\sum_{k=0}^{j-2} x_i^kx_1^{j-2-k}=x_i^{j-2} + x_1b_{i,j-1},</math>\nfor {{math|1=''i'' = 2, ..., n}}, and setting <math>b_{i,1}=1.</math>\n\nThus, subtracting, for {{mvar|j}} running from {{mvar|n}} down to 2, the {{math|(''j'' – 2)}}th column of {{mvar|B}} multiplied by <math>x_1</math> from the {{math|(''j'' – 1)}}th column, one gets a {{math|(''n'' – 1) × (''n'' – 1)}} Vandermonde matrix in <math>x_2,\\ldots, x_n,</math> which has the same determinant as {{mvar|B}}. Iterating this process on this smaller Vandermonde matrix, one gets eventually the desired expression of {{math|det(''V'')}} as the product of the <math>x_j-x_i.</math>\n\n=== By Gaussian elimination, U-part of LU decomposition===\nThe determinant of Vandermonde matrices may also be computed using [[Gaussian elimination]]. This provides an explicit form of the upper triangular matrix of the [[LU decomposition]] of the matrix. For this computation one uses only the [[elementary row operations]] consisting of adding to a row a scalar multiple of a preceding row. This means than one multiplies the matrix by a lower triangular matrix with a diagonal consisting only of 1. In particular, the determinant is not changed by these transformations.\n\nApplying Gaussian elimination to a square Vandermonde matrix, one gets eventually an upper triangular matrix\n:<math>W=\\begin{bmatrix}\n w_{1,1}& w_{1,2} & w_{1,3} & \\dots & w_{1,n}\\\\\n0 & w_{2,2} & w_{2,3} & \\dots & w_{2,n}\\\\\n0 & 0 & w_{3,3} & \\dots & w_{3,n}\\\\\n\\vdots & \\vdots & \\vdots & \\ddots &\\vdots \\\\\n0 & 0 & 0 & \\dots & w_{n,n}\n\\end{bmatrix},</math>\nwhich has the same determinant as {{mvar|V}}.\n\nA [[proof by induction]] on the steps of Gaussian elimination allows showing that, for {{math|1 ≤ ''i'' ≤ ''j'' ≤ ''n''}}, one has \n:<math>w_{i,j}=M_{j-i}(\\mathbf x_{i-1}, x_i)\\prod_{k=1}^{i-1}(x_i-x_k),</math>\nwhere <math>\\mathbf x_{i-1}</math> is an abbreviation for <math>x_1,\\ldots, x_{i-1}</math>, and <math>M_{d}(\\mathbf x_{i-1}, x_i)</math> is the sum of all [[monomial]]s of degree {{math|''d''}} in <math>\\mathbf x_{i-1}, x_i.</math> In particular, the first rows of {{mvar|V}} and {{mvar|W}} are equal, and the factor <math>M_{j-i}(\\mathbf x_{i-1}, x_i)</math> equals 1 for the entries of the diagonal (since 1 is the only monomial of degree 0).\n\nA key ingredient of this proof is that, for {{math|''k'' > ''i''}}\n:<math>M_{d}(\\mathbf x_{i-1}, x_h)-M_{d}(\\mathbf x_{i-1}, x_i)=(x_h-x_i)M_{d-1}(\\mathbf x_{i-1}, x_i,x_h)</math>\n\nFor the recursion, one has to describe the matrix at each step of the Gaussian elimination. Let <math>a_{i,j}</math> be the entry of the {{mvar|i}}th row and {{mvar|j}}th column of this variable matrix. Before the {{mvar|i}}th step, the entries that belong either to the {{math|''i''}} first rows or the {{math|''i'' – 1}} first columns have the values that they will have at the end of Gaussian elimination, and, for {{math|''i'' ≤ ''j'' ≤ ''n''}} and {{math|''i'' ≤ ''h'' ≤ ''n''}}, one has \n:<math>a_{h,j}=M_{j-i}(\\mathbf x_{i-1}, x_h)\\prod_{k=1}^{i-1}(x_h-x_k).</math>\nThis is true before the first step, and one has to prove that this remains true during Gaussian elimination.\nThe {{mvar|i}}th step does not change the {{mvar|i}} first rows nor the {{math|''i'' – 1}} first columns. It changes <math>a_{h,i}</math> to zero for {{math|1=''i'' < ''h'' ≤ ''n''}}. For {{math|''i'' < ''j'' ≤ ''n''}} and {{math|''i'' < ''h'' ≤ ''n''}}, it changes <math>a_{h,j}</math> into <math>a_{h,j}-a_{i,j}a_{h,i}/a_{i,i}.</math> That is, the new <math>a_{h,j}</math> is\n:<math>\\begin{align}\na_{h,j}&=M_{j-i}(\\mathbf x_{i-1}, x_h)\\prod_{k=1}^{i-1}(x_h-x_k)-\n\\frac{M_{0}(\\mathbf x_{i-1}, x_h)\\prod_{k=1}^{i-1}(x_h-x_k)}\n{M_{0}(\\mathbf x_{i-1}, x_i)\\prod_{k=1}^{i-1}(x_i-x_k)}\nM_{j-i}(\\mathbf x_{i-1}, x_i)\\prod_{k=1}^{i-1}(x_i-x_k)\\\\\n&=\\left(M_{j-i}(\\mathbf x_{i-1}, x_h)-M_{j-i}(\\mathbf x_{i-1}, x_i)\\right)\\prod_{k=1}^{i-1}(x_h-x_k)\\\\\n&=M_{j-(i+1)}(\\mathbf x_{i-1}, x_i,x_h)\\prod_{k=1}^{i}(x_h-x_k).\n\\end{align}</math>\nThis shows that the structure of the <math>a_{h,j}</math> is kept during Gaussian elimination, and thus the form of {{mvar|W}}.\n\nIt follows from the structure of {{mvar|W}} that <math>\\det(V)=\\det(W)</math> is the product of the diagonal entries of {{mvar|W}}, which proves again the formula for the determinant of a Vandermonde matrix.\n\n===Resulting properties===\n\nA {{math|''m'' × ''n''}} rectangular Vandermonde matrix such that {{math|''m'' ≤ ''n''}} has maximum [[rank of a matrix|rank]] {{math|''m''}} [[if and only if]] all {{math|''x''<sub>''i''</sub>}} are distinct. \n\nA {{math|''m'' × ''n''}} rectangular Vandermonde matrix such that {{math|''m'' ≥ ''n''}} has maximum [[rank of a matrix|rank]] {{math|''n''}} if and only if there are {{mvar|n}} of the {{math|''x''<sub>''i''</sub>}} that are distinct. \n\nA square Vandermonde matrix is [[invertible matrix|invertible]] if and only if the {{math|''x''<sub>''i''</sub>}} are distinct. An explicit formula for the inverse is known.<ref>{{Cite book\n| title = Inverse of the Vandermonde matrix with applications\n| last = Turner\n| first = L. Richard\n| url =https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19660023042_1966023042.pdf\n}}</ref><ref>{{Cite journal\n| volume = 65\n| issue = 2\n| pages = 95–100\n| last = Macon\n| first = N.\n|author2=A. Spitzbart\n | title = Inverses of Vandermonde Matrices\n| journal = The American Mathematical Monthly\n| date = February 1958\n| jstor = 2308881\n| doi = 10.2307/2308881\n}}</ref><ref>[https://proofwiki.org/wiki/Inverse_of_Vandermonde_Matrix Inverse of Vandermonde Matrix (ProofWiki)]</ref>\n\n==Applications==\nThe Vandermonde matrix ''evaluates'' a polynomial at a set of points; formally, it is the matrix of the [[linear map]] that maps the vector of coefficients of a polynomial to the vector of the values of the polynomial at the values appearing in the Vandermonde matrix. The non-vanishing of the Vandermonde determinant for distinct points <math>\\alpha_i</math> shows that, for distinct points, the map from coefficients to values at those points is a one-to-one correspondence, and thus that the polynomial interpolation problem is solvable with a unique solution; this result is called the ''[[unisolvence theorem]].''\n\nThis may be useful in [[polynomial interpolation]], since inverting the Vandermonde matrix allows expressing the coefficients of the polynomial in terms of the <math>\\alpha_i</math> and the values of the polynomial at the <math>\\alpha_i.</math> However, the interpolation polynomial is generally easier to compute with the [[Lagrange polynomials|Lagrange interpolation formula]],<ref>{{Cite book |last1=Press|first1=WH|last2=Teukolsky|first2=SA|last3=Vetterling|first3=WT|last4=Flannery|first4=BP|year=2007|title=Numerical Recipes: The Art of Scientific Computing|edition=3rd|publisher=Cambridge University Press| location=New York|isbn=978-0-521-88068-8|chapter=Section 2.8.1. Vandermonde Matrices|chapter-url=http://apps.nrbook.com/empanel/index.html?pg=94 |postscript={{inconsistent citations}}}}</ref> which may be used for deriving a formula for the inverse of a Vandermonde matrix. \n\nThe Vandermonde determinant is used in the [[representation theory of the symmetric group]].<ref>{{Fulton-Harris}} ''Lecture 4 reviews the representation theory of symmetric groups, including the role of the Vandermonde determinant''.</ref>\n\nWhen the values <math>\\alpha_k</math> belong to a [[finite field]], then the Vandermonde determinant is also called a \n[[Moore matrix|Moore determinant]] and has specific properties that are used, for example, in the theory of [[BCH code]] and [[Reed–Solomon error correction]] codes.\n\nThe [[discrete Fourier transform]] is defined by a specific Vandermonde matrix, the [[DFT matrix]], where the numbers α<sub>''i''</sub> are chosen to be [[roots of unity]].\n\nThe [[Laughlin wavefunction]] with filling factor one (appearing in the [[Quantum Hall effect]]), by the formula for the Vandermond determinant, can be seen to be a [[Slater determinant]]. This is not true anymore for filling factors different from one, i.e., in the [[fractional Quantum Hall effect]].\n\n==Confluent Vandermonde matrices==\nAs described before, a Vandermonde matrix describes the linear algebra [[polynomial interpolation|interpolation problem]] of finding the coefficients of a polynomial <math>p(x)</math> of degree <math>n-1</math> based on the values <math> p(\\alpha_1),...,p(\\alpha_n)</math>, where <math>\\alpha_1,...,\\alpha_n</math> are ''distinct'' points. If <math>\\alpha_i</math> are not distinct, then this problem does not have a unique solution (which is reflected by the fact that the corresponding Vandermonde matrix is singular). However, if we give the values of the derivatives at the repeated points, then the problem can have a unique solution. For example, the problem\n\n:<math> \\begin{cases} p(0)=a \\\\ p'(0)=b \\\\ p(1)=c \\end{cases} </math>\n\nwhere <math>p</math> is a polynomial of degree <math>\\leq 2</math>, has a unique solution for all <math>a,b,c</math>. In general, suppose that <math>\\alpha_1,\\alpha_2,...,\\alpha_n</math> are (not necessarily distinct) numbers, and suppose for ease of notation that equal values come in continuous sequences in the list. That is\n\n:<math> \\alpha_1 = \\cdots = \\alpha_{m_1},\\alpha_{m_1+1} = \\cdots = \\alpha_{m_2}, \\ldots ,\\alpha_{m_{k-1}+1} = \\cdots = \\alpha_{m_k} </math>\n\nwhere <math>m_k = n,</math> <math>m_1 < m_2 < \\cdots < m_k,</math> and <math>\\alpha_{m_1}, \\ldots ,\\alpha_{m_k}</math> are distinct. Then the corresponding interpolation problem is\n\n:<math> \\begin{cases} p(\\alpha_1) = \\beta_1, & p'(\\alpha_1)=\\beta_2, & \\ldots, & p^{(m_1-1)}(\\alpha_1)=\\beta_{m_1} \\\\ p(\\alpha_{m_1+1}) = \\beta_{m_1+1}, & p'(\\alpha_{m_1+1})=\\beta_{m_1+2}, & \\ldots, & p^{(m_2-m_1-1)}(\\alpha_{m_2})=\\beta_{m_2} \\\\ \\qquad \\vdots \\\\ p(\\alpha_{m_{k-1}+1}) = \\beta_{m_{k-1}+1}, & p'(\\alpha_{m_{k-1}+2})=\\beta_{m_{k-1}+2}, & \\ldots, & p^{(m_k-m_{k-1}-1)}(\\alpha_{m_k})=\\beta_{m_k} \\end{cases} </math>\n\nAnd the corresponding matrix for this problem is called a '''confluent Vandermonde matrices'''. In our case (which is the general case, up to permuting the rows of the matrix) the formula for it is given as follows: if <math>1 \\leq i,j \\leq n</math>, then <math>m_\\ell < i \\leq m_{\\ell + 1} </math> for some (unique) <math>0 \\leq \\ell \\leq k-1</math> (we consider <math>m_0=0</math>). Then, we have\n\n:<math> V_{i,j} = \\begin{cases} 0, & \\text{if } j < i - m_\\ell; \\\\[6pt] \\dfrac{(j-1)!}{(j-(i-m_\\ell))!} \\alpha_i^{j-(i-m_\\ell)}, & \\text{if } j \\geq i - m_\\ell. \\end{cases} </math>\n\nThis generalization of the Vandermonde matrix makes it [[Invertible matrix|non-singular]] (such that there exists a unique solution to the system of equations) while retaining most properties of the Vandermonde matrix. Its rows are derivatives (of some order) of the original Vandermonde rows.\n\nAnother way to receive this formula is to let some of the <math>\\alpha_i</math>'s go arbitrarily close to each other. For example, if <math>\\alpha_1=\\alpha_2</math>, then letting <math>\\alpha_2\\to\\alpha_1</math> in the original Vandermonde matrix, the difference between the first and second rows yields the corresponding row in the confluent Vandermonde matrix. This allows us to link the generalized interpolation problem (given value and derivatives on a point) to the original case where all points are distinct: Being given <math>p(\\alpha),p'(\\alpha)</math> is similar to being given <math>p(\\alpha),p(\\alpha+\\varepsilon)</math> where <math>\\varepsilon</math> is very small.\n\n==See also==\n* [[Schur polynomial]] – a generalization\n* [[Alternant matrix]]\n* [[Lagrange polynomial]]\n* [[Wronskian]]\n* [[List of matrices]]\n* [[Moore determinant over a finite field]]\n\n==References==\n{{reflist}}\n\n==Further reading==\n*{{Citation |last=Ycart |first=Bernard |title=A case of mathematical eponymy: the Vandermonde determinant |journal=Revue d'Histoire des Mathématiques |volume=13 |year=2013 |arxiv=1204.4716|bibcode=2012arXiv1204.4716Y }}.\n\n==External links==\n{{ProofWiki|id=Vandermonde_Determinant|title=Vandermonde Determinant}}\n\n{{Numerical linear algebra}}\n\n[[Category:Matrices]]\n[[Category:Determinants]]\n[[Category:Numerical linear algebra]]"
    },
    {
      "title": "Wilkinson matrix",
      "url": "https://en.wikipedia.org/wiki/Wilkinson_matrix",
      "text": "In [[linear algebra]], '''Wilkinson matrices''' are [[Symmetric matrix|symmetric]], [[Tridiagonal matrix|tridiagonal]], order-''N'' [[matrix (math)|matrices]] with pairs of nearly, but not exactly, equal [[eigenvalue]]s.<ref>{{cite book | title = The Algebraic Eigenvalue Problem | author = Wilkinson | edition = | publisher = [[Oxford University Press]] | year = 1965 | isbn = 0-19-853418-3}}</ref> It is named after the British mathematician [[James H. Wilkinson]]. For ''N''&nbsp;=&nbsp;7, the Wilkinson matrix is given by\n\n:<math>\\begin{bmatrix}\n 3 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n 1 & 2 & 1 & 0 & 0 & 0 & 0 \\\\\n 0 & 1 & 1 & 1 & 0 & 0 & 0 \\\\\n 0 & 0 & 1 & 0 & 1 & 0 & 0 \\\\\n 0 & 0 & 0 & 1 & 1 & 1 & 0 \\\\\n 0 & 0 & 0 & 0 & 1 & 2 & 1 \\\\\n 0 & 0 & 0 & 0 & 0 & 1 & 3 \\\\\n\\end{bmatrix}.</math>\n\nWilkinson matrices have applications in many fields, including [[scientific computing]], [[numerical linear algebra]], and [[signal processing]].\n\n==References==\n{{reflist}}\n\n{{Numerical linear algebra}}\n\n[[Category:Matrices]]\n[[Category:Numerical linear algebra]]\n\n\n{{Linear-algebra-stub}}"
    },
    {
      "title": "Block LU decomposition",
      "url": "https://en.wikipedia.org/wiki/Block_LU_decomposition",
      "text": "{{Unreferenced|date=December 2009}}\nIn [[linear algebra]], a '''Block LU decomposition''' is a [[matrix decomposition]] of a [[block matrix]] into a lower block triangular matrix ''L'' and an upper block triangular matrix ''U''. This decomposition is used in [[numerical analysis]] to reduce the complexity of the block matrix formula.\n\n==Block LDU decomposition==\n:<math>\n\\begin{bmatrix}\n A & B \\\\\n C & D \n\\end{bmatrix}\n=\n\\begin{bmatrix}\nI & 0 \\\\\nC A^{-1} & I\n\\end{bmatrix}\n\\begin{bmatrix}\nA & 0 \\\\\n0 & D-C A^{-1} B\n\\end{bmatrix}\n\\begin{bmatrix}\nI & A^{-1} B \\\\\n0 & I\n\\end{bmatrix}\n</math>\n\n==Block Cholesky decomposition==\nConsider a [[block matrix]]:\n:<math>\n\\begin{pmatrix}\n A & B \\\\\n C & D \n\\end{pmatrix}\n=\n\\begin{pmatrix}\nI \\\\\nC A^{-1}\n\\end{pmatrix}\n\\,A\\,\n\\begin{pmatrix}\nI & A^{-1}B\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n0 & 0 \\\\\n0 & D-C A^{-1} B\n\\end{pmatrix},\n</math>\nwhere the matrix <math>\\begin{matrix}A\\end{matrix}</math> is assumed to be non-singular,\n<math>\\begin{matrix}I\\end{matrix}</math> is an identity matrix with proper dimension, and <math>\\begin{matrix}0\\end{matrix}</math> is a matrix whose elements are all zero.\n\nWe can also rewrite the above equation using the half matrices:\n:<math>\n\\begin{pmatrix}\n A & B \\\\\n C & D \n\\end{pmatrix}\n=\n\\begin{pmatrix}\nA^{\\frac{1}{2}} \\\\\nC A^{-\\frac{1}{2}}\n\\end{pmatrix}\n\\begin{pmatrix}\nA^{\\frac{1}{2}} & A^{-\\frac{1}{2}}B\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n0 & 0 \\\\\n0 & Q^{\\frac{1}{2}}\n\\end{pmatrix}\n\\begin{pmatrix}\n0 & 0 \\\\\n0 & Q^{\\frac{1}{2}}\n\\end{pmatrix}\n,</math>\nwhere the [[Schur complement]] of <math>\\begin{matrix}A\\end{matrix}</math>\nin the block matrix is defined by\n:<math>\n\\begin{matrix}\nQ = D - C A^{-1} B\n\\end{matrix}\n</math> \nand the half matrices can be calculated by means of [[Cholesky decomposition]] or [[LDL decomposition]].\nThe half matrices satisfy that\n:<math>\n\\begin{matrix}\nA^{\\frac{1}{2}}\\,A^{\\frac{1}{2}}=A;\n\\end{matrix}\n\\qquad\n\\begin{matrix}\nA^{\\frac{1}{2}}\\,A^{-\\frac{1}{2}}=I;\n\\end{matrix}\n\\qquad\n\\begin{matrix}\nA^{-\\frac{1}{2}}\\,A^{\\frac{1}{2}}=I;\n\\end{matrix}\n\\qquad\n\\begin{matrix}\nQ^{\\frac{1}{2}}\\,Q^{\\frac{1}{2}}=Q.\n\\end{matrix}</math>\n\nThus, we have\n:<math>\n\\begin{pmatrix}\n A & B \\\\\n C & D \n\\end{pmatrix}\n=\nLU,\n</math>\nwhere\n:<math>\nLU =\n\\begin{pmatrix}\nA^{\\frac{1}{2}}    & 0 \\\\\nC A^{-\\frac{1}{2}} & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nA^{\\frac{1}{2}} & A^{-\\frac{1}{2}}B \\\\\n0               & 0\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n0 & 0 \\\\\n0 & Q^{\\frac{1}{2}}\n\\end{pmatrix}\n\\begin{pmatrix}\n0 & 0 \\\\\n0 & Q^{\\frac{1}{2}}\n\\end{pmatrix}.\n</math>\n\nThe matrix <math>\\begin{matrix}LU\\end{matrix}</math> can be decomposed in an algebraic manner into\n::<math>L = \n\\begin{pmatrix}\nA^{\\frac{1}{2}}    & 0 \\\\\nC A^{-\\frac{1}{2}} & Q^{\\frac{1}{2}}\n\\end{pmatrix}\n\\mathrm{~~and~~}\nU =\n\\begin{pmatrix}\nA^{\\frac{1}{2}} & A^{-\\frac{1}{2}}B \\\\\n0               & Q^{\\frac{1}{2}}\n\\end{pmatrix}.\n</math>\n\n==See also==\n*[[Matrix decomposition]]\n*[[Schur complement]]\n\n{{DEFAULTSORT:Block Lu Decomposition}}\n[[Category:Matrix decompositions]]\n\n\n{{linear-algebra-stub}}"
    },
    {
      "title": "Crout matrix decomposition",
      "url": "https://en.wikipedia.org/wiki/Crout_matrix_decomposition",
      "text": "In [[linear algebra]], the '''Crout matrix decomposition''' is an [[LU decomposition]] which decomposes a [[matrix (mathematics)|matrix]] into a [[lower triangular matrix]] (L), an [[upper triangular matrix]] (U) and, although not always needed, a [[permutation matrix]] (P). It was developed by [[Prescott Durand Crout]]. <ref name=Press2007>{{cite book|last=Press|first=William H.|title=Numerical Recipes 3rd Edition: The Art of Scientific Computing|year=2007|publisher=Cambridge University Press|isbn=9780521880688|pages=50–52}}</ref>\n\nThe Crout matrix decomposition [[algorithm]] differs slightly from the [[Doolittle decomposition|Doolittle method]]. Doolittle's method returns a unit lower triangular matrix and an upper triangular matrix, while the Crout method returns a lower triangular matrix and a unit upper triangular matrix.\n\nSo, if a matrix decomposition of a matrix A is such that:\n\n:A = LDU\n\nbeing L a unit lower triangular matrix, D a diagonal matrix and U a unit upper triangular matrix, then Doolittle's method produces\n\n:A = L(DU)\n\nand Crout's method produces\n\n:A = (LD)U.\n\n== Implementations ==\nC implementation:\n\n<source lang=\"c\">\nvoid crout(double const **A, double **L, double **U, int n) {\n\tint i, j, k;\n\tdouble sum = 0;\n\n\tfor (i = 0; i < n; i++) {\n\t\tU[i][i] = 1;\n\t}\n\n\tfor (j = 0; j < n; j++) {\n\t\tfor (i = j; i < n; i++) {\n\t\t\tsum = 0;\n\t\t\tfor (k = 0; k < j; k++) {\n\t\t\t\tsum = sum + L[i][k] * U[k][j];\t\n\t\t\t}\n\t\t\tL[i][j] = A[i][j] - sum;\n\t\t}\n\n\t\tfor (i = j; i < n; i++) {\n\t\t\tsum = 0;\n\t\t\tfor(k = 0; k < j; k++) {\n\t\t\t\tsum = sum + L[j][k] * U[k][i];\n\t\t\t}\n\t\t\tif (L[j][j] == 0) {\n\t\t\t\tprintf(\"det(L) close to 0!\\n Can't divide by 0...\\n\");\n\t\t\t\texit(EXIT_FAILURE);\n\t\t\t}\n\t\t\tU[j][i] = (A[j][i] - sum) / L[j][j];\n\t\t}\n\t}\n}\n</source>\n\nOctave/Matlab implementation:\n<source lang=\"matlab\">\n   function [L, U] = LUdecompCrout(A)\n        \n        [R, C] = size(A);\n        for i = 1:R\n            L(i, 1) = A(i, 1);\n            U(i, i) = 1;\n        end\n        for j = 2:R\n            U(1, j) = A(1, j) / L(1, 1);\n        end\n        for i = 2:R\n            for j = 2:i\n                L(i, j) = A(i, j) - L(i, 1:j - 1) * U(1:j - 1, j);\n            end\n            \n            for j = i + 1:R\n                U(i, j) = (A(i, j) - L(i, 1:i - 1) * U(1:i - 1, j)) / L(i, i);\n            end\n        end\n   end\n</source>\n\n==References==\n{{reflist}}\n*[https://web.archive.org/web/20131003135132/http://tutorialfrenzy.com/matlab-crout-lu-factorization-linear-equations-systems.php  Implementation using functions ] In Matlab\n\n{{DEFAULTSORT:Crout Matrix Decomposition}}\n[[Category:Matrix decompositions]]"
    },
    {
      "title": "Dynamic mode decomposition",
      "url": "https://en.wikipedia.org/wiki/Dynamic_mode_decomposition",
      "text": "{{Multiple issues|\n{{original research|date=March 2012}}\n{{refimprove|date=March 2012}}\n}}\n\n'''Dynamic mode decomposition''' ('''DMD''') is a [[dimensionality reduction]] algorithm developed by Peter Schmid in 2008.\nGiven a time series of data, DMD computes a set of modes each of which is associated with a fixed oscillation frequency and decay/growth rate.  For linear systems in particular, these modes and frequencies are analogous to the [[normal mode|normal modes]] of the system, but more generally, they are approximations of the modes and eigenvalues of the [[composition operator]] (also called the Koopman operator).  Due to the intrinsic temporal behaviors associated with each mode, DMD differs from dimensionality reduction methods such as [[principal component analysis]], which computes orthogonal modes that lack predetermined temporal behaviors.  Because its modes are not orthogonal, DMD-based representations can be less parsimonious than those generated by PCA.  However, they can also be more physically meaningful because each mode is associated with a damped (or driven) sinusoidal behavior in time.\n\n==Overview==\nDynamic mode decomposition was first introduced by Schmid as a numerical procedure for extracting dynamical features from flow data.<ref name=schmid2010>P.J. Schmid. \"Dynamic mode decomposition of numerical and experimental data.\" Journal of Fluid Mechanics 656.1 (2010): 5–28.</ref>\n\nThe data takes the form of a snapshot sequence \n: <math> V_1^N = \\{v_1, v_2, \\dots, v_N\\}, </math>\nwhere <math> v_i\\in \\mathbb{R}^M</math> is the <math>i</math>-th snapshot of the flow field, and <math>V_1^N\\in\\mathbb{R}^{M\\times N}</math> is a data matrix whose columns are the individual snapshots.  The subscript and superscript denote the index of the snapshot in the first and last columns respectively.  These snapshots are assumed to be related via a linear mapping that defines a [[linear dynamical system]]\n: <math> v_{i+1} = A v_i, </math> \nthat remains approximately the same over the duration of the sampling period. Written in matrix form, this implies that \n:<math> V_{2}^N = A V_{1}^{N-1} + re_{N-1}^T,</math>\nwhere <math>r</math> is the vector of residuals that accounts for behaviors that cannot be described completely by <math>A</math>, <math>e_{N-1}=\\{0,0,\\ldots,1\\}\\in\\mathbb{R}^{N-1}</math>, <math>V_1^{N-1}=\\{v_1, v_2, \\dots, v_{N-1}\\}</math>, and <math>V_2^{N}=\\{v_2, v_3, \\dots, v_{N}\\}</math>.  Regardless of the approach, the output of DMD is the eigenvalues and eigenvectors of <math>A</math>, which are referred to as the ''DMD eigenvalues'' and ''DMD modes'' respectively.\n\n== Algorithm ==\n\nThere are two methods for obtaining these eigenvalues and modes. The first is [[Arnoldi iteration|Arnoldi-like]], which is useful for theoretical analysis due to its connection with [[Krylov subspace|Krylov methods]].  The second is a [[singular value decomposition]] (SVD) based approach that is more robust to noise in the data and to numerical errors.\n\n=== The Arnoldi approach ===\n\nIn fluids applications, the size of a snapshot, <math>M</math>, is assumed to be much larger than the number of snapshots <math>N</math>, so there are many equally valid choices of <math>A</math>.  The original DMD algorithm picks <math>A</math> so that each of the snapshots in <math>V_2^N</math> can be written as the linear combination of the snapshots in <math>V_1^{N-1}</math>.\nBecause most of the snapshots appear in both data sets, this representation is error free for all snapshots except <math>v_N</math>, which is written as \n:<math> v_N = a_1 v_1 + a_2 v_2 + \\dots + a_{N-1}v_{N-1} + r = V_1^{N-1}a + r,</math>\nwhere <math>a={a_1, a_2, \\dots, a_{N-1}}</math> is a set of coefficients DMD must identify and <math>r</math> is the residual.\nIn total, \n: <math> V_{2}^N = A V_1^{N-1} = V_1^{N-1} S + re_{N-1}^T, </math>\nwhere <math>S</math> is the [[companion matrix]]\n:<math>S=\\begin{pmatrix}\n0 & 0 & \\dots & 0 & a_1 \\\\\n1 & 0 & \\dots & 0 & a_2 \\\\\n0 & 1 & \\dots & 0 & a_3 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & \\dots & 1 & a_{N-1}\n\\end{pmatrix}.</math>\nThe vector <math>a</math> can be computed by solving a least squares problem, which minimizes the overall residual.  In particular if we take the QR decomposition of <math>V_1^{N-1} = QR</math>, then <math>a = R^{-1}Q^Tv_N</math>.\n\nIn this form, DMD is a type of [[Arnoldi iteration|Arnoldi method]], and therefore the eigenvalues of <math>S</math> are approximations of the eigenvalues of <math>A</math>.  Furthermore, if <math>y</math> is an eigenvector of <math>S</math>, then <math>V_1^{N-1}y</math> is an approximate eigenvector of <math>A</math>. The reason an [[Eigendecomposition of a matrix|eigendecomposition]] is performed on <math>S</math> rather than <math>A</math> is because <math>S</math> is much smaller than <math>A</math>, so the computational cost of DMD is determined by the number of snapshots rather than the size of a snapshot.\n\n=== The SVD-based approach ===\n\nInstead of computing the companion matrix <math> S </math>, the SVD-based approach yields the matrix <math>\\tilde S</math> that is related to <math>A</math> via a similarity transform.  To do this, assume we have the SVD of <math>V_1^{N-1} = U\\Sigma W^T</math>.  Then \n:<math> V_{2}^N = A V_1^{N-1} + re_{N-1}^T = AU\\Sigma W^T + re_{N-1}^T.</math>\nEquivalent to the assumption made by the Arnoldi-based approach, we choose <math>A</math> such that the snapshots in <math>V_2^N</math> can be written as the linear superposition of the columns in <math>U</math>, which is equivalent to requiring that they can be written as the superposition of [[Principal component analysis|POD modes]].  With this restriction, minimizing the residual requires that it is orthogonal to the POD basis (i.e., <math>U^Tr = 0</math>).  Then multiplying both sides of the equation above by <math>U^T</math> yields <math> U^TV_2^N = U^T A U\\Sigma W^T </math>, which can be manipulated to obtain\n:<math> U^T A U = U^TV_2^N W \\Sigma^{-1} \\equiv \\tilde S. </math>\nBecause <math>A</math> and <math>\\tilde S</math> are related via similarity transform, the eigenvalues of <math>S</math> are the eigenvalues of <math>A</math>, and if <math>y</math> is an eigenvector of <math>\\tilde S</math>, then <math>Uy</math> is an eigenvector of <math>A</math>.\n\nIn summary, the SVD-based approach is as follows:\n# Split the time series of data in  <math>V_1^N</math> into the two matrices <math>V_1^{N-1}</math> and <math>V_2^N</math>. \n# Compute the SVD of <math>V_1^{N-1} = U\\Sigma W^T</math>.\n# Form the matrix <math>\\tilde S =  U^TV_2^N W \\Sigma^{-1}</math>, and compute its eigenvalues <math>\\lambda_i</math> and eigenvectors <math>y_i</math>.\n# The <math>i</math>-th DMD eigenvalues is <math>\\lambda_i</math> and <math>i</math>-th DMD mode is the <math>Uy_i</math>.\n\nThe advantage of the SVD-based approach over the Arnoldi-like approach is that noise in the data and numerical truncation issues can be compensated for by truncating the SVD of <math>V_1^{N-1}</math>.  As noted in  <ref name=schmid2010 /> accurately computing more than the first couple modes and eigenvalues can be difficult on experimental data sets without this truncation step.\n\n== Theoretical and algorithmic advancements ==\n\nSince its inception in 2010, a considerable amount of work has focused on understanding and improving DMD.  One of the first analyses of DMD by Rowley et al.<ref>C.W. Rowley, I Mezic, S. Bagheri, P. Schlatter, and D.S. Henningson, \"Spectral analysis of nonlinear flows.\" Journal of Fluid Mechanics 641 (2009): 85-113</ref> established the connection between DMD and the Koopman operator, and helped to explain the output of DMD when applied to nonlinear systems.  Since then, a number of modifications have been developed that either strengthen this connection further or enhance the robustness and applicability of the approach.\n\n*'''Optimized DMD''': Optimized DMD is a modification of the original DMD algorithm designed to compensate for two limitations of that approach: (i) the difficulty of DMD mode selection, and (ii) the sensitivity of DMD to noise or other errors in the last snapshot of the time series.<ref>K.K. Chen, J.H. Tu, and C.W. Rowley, \"Variants of dynamic mode decomposition: boundary condition, Koopman, and Fourier analyses.\" Journal of Nonlinear Science 22 (2012): 887-915.</ref>  Optimized DMD recasts the DMD procedure as an optimization problem where the identified linear operator has a fixed rank.  Furthermore, unlike DMD which perfectly reproduces all of the snapshots except for the last, Optimized DMD allows the reconstruction errors to be distributed throughout the data set, which appears to make the approach more robust in practice.\n*'''Optimal Mode Decomposition''': Optimal Mode Decomposition (OMD) recasts the DMD procedure as an optimization problem and allows the user to directly impose the rank of the identified system.<ref>A. Wynn, D. S. Pearson, B. Ganapathisubramani and P. J. Goulart, \"Optimal mode decomposition for unsteady flows.\" Journal of Fluid Mechanics 733 (2013): 473-503</ref>  Provided this rank is chosen properly, OMD can produce linear models with smaller residual errors and more accurate eigenvalues on both synthetic and experimental data sets.\n*'''Exact DMD''': The Exact DMD algorithm generalizes the original DMD algorithm in two ways.  First, in the original DMD algorithm the data must be a time series of snapshots, but Exact DMD accepts a data set of snapshot pairs.<ref name=tu2014>{{Cite journal|url = http://www.aimsciences.org/journals/displayArticlesnew.jsp?paperID=10631|title = On Dynamic Mode Decomposition: Theory and Applications|last = Tu, Rowley, Luchtenburg, Brunton, and Kutz|first = |date = December 2014|journal = American Institute of Mathematical Sciences|doi = 10.3934/jcd.2014.1.391|pmid = |access-date = |arxiv = 1312.0041}}</ref>  The snapshots in the pair must be separated by a fixed <math>\\Delta t</math>, but do not need to be drawn from a single time series.  In particular, Exact DMD can allow data from multiple experiments to be aggregated into a single data set.  Second, the original DMD algorithm effectively pre-processes the data by projecting onto a set of POD modes.  The Exact DMD algorithm removes this pre-processing step, and can produce DMD modes that cannot be written as the superposition of POD modes.\n*'''Sparsity Promoting DMD''': Sparsity promoting DMD is a post processing procedure for DMD mode and eigenvalue selection.<ref>M.R. Jovanovic, P.J. Schmid, and J.W. Nichols, \"Sparsity-promoting dynamic mode decomposition.\"  Physics of Fluids 26 (2014)</ref>  Sparsity promoting DMD uses an [[Lasso (statistics)|<math>\\ell_1</math> penalty]] to identify a smaller set of important DMD modes, and is an alternative approach to the DMD mode selection problem that can be solved efficiently using [[Augmented Lagrangian method|convex optimization techniques]].\n*'''Multi-Resolution DMD''': Multi-Resolution DMD (mrDMD) is a combination of the techniques used in [[multiresolution analysis]] with Exact DMD designed to robust extracting DMD modes and eigenvalues from data sets containing multiple timescales.<ref>J.N. Kutz, X. Fu, and S.L. Brunton, \"Multi-resolution dynamic mode decomposition.\" arXiv preprint arXiv:1506.00564 (2015).</ref>  The mrDMD approach was applied to global surface temperature data, and identifies a DMD mode that appears during  El Nino years.\n*'''Extended DMD''': Extended DMD is a modification of Exact DMD that strengthens the connection between DMD and the Koopman operator.<ref>M.O. Williams , I.G. Kevrekidis, C.W. Rowley, \"A Data–Driven Approximation of the Koopman Operator: Extending Dynamic Mode Decomposition.\"  Journal of Nonlinear Science 25 (2015): 1307-1346.</ref>  As the name implies, Extended DMD is an extension of DMD that uses a richer set of observable functions to produce more accurate approximations of the Koopman operator.  It also demonstrated the DMD and related methods produce approximations of the Koopman eigenfunctions in addition to the more commonly used eigenvalues and modes.\n*'''DMD with Control''': Dynamic mode decomposition with control (DMDc) <ref>J.L. Proctor, S.L. Brunton, and J.N. Kutz, \"Dynamic mode decomposition with control.\" arXiv preprint arXiv:1409.6358 (2014).</ref> is a modification of the DMD procedure designed for data obtained from input output systems. One unique feature of DMDc is the ability to disambiguate the effects of system actuation from the open loop dynamics, which is useful when data are obtained in the presence of actuation.\n*'''Total Least Squares DMD''': Total Least Squares DMD is a recent modification of Exact DMD meant to address issues of robustness to measurement noise in the data.  In,<ref>M.S. Hemati, C.W. Rowley, E.A. Deem, and L.N. Cattafesta, \"De-Biasing the Dynamic Mode Decomposition for Applied Koopman Spectral Analysis of Noisy Datasets.\" arXiv preprint arXiv:1502.03854 (2015).</ref> the authors interpret the Exact DMD as a regression problem that is solved using [[ordinary least squares]] (OLS), which assumes that the regressors are noise free.  This assumption creates a bias in the DMD eigenvalues when it is applied to experimental data sets where all of the observations are noisy.  Total least squares DMD replaces the OLS problem with a [[Total least squares|total least squares problem]], which eliminates this bias.\n\nIn addition to the algorithms listed here, similar application-specific techniques have been developed.  For example, like DMD, [[Prony's method]] represents a signal as the superposition of damped sinusoids.  In climate science, linear inverse modeling is also strongly connected with DMD.<ref>{{Cite journal|url=http://journals.ametsoc.org/doi/abs/10.1175/1520-0442(1993)006%3C1067:PONSST%3E2.0.CO%3B2| title=Prediction of Niño 3 Sea Surface Temperatures Using Linear Inverse Modeling|last=Penland, Magorian|first=Cecile, Theresa|date=1993|journal=J. Climate|volume=6}}</ref>  For a more comprehensive list, see Tu et al.<ref name=tu2014 />\n\n== Examples ==\n{{Section OR|date=March 2012}}\n\n=== Trailing edge of a profile ===\n[[File:Joukowsky Karman Vortex Street s.png|thumb|300px|Fig 1 Trailing edge Vortices (Entropy)]]\n\nThe wake of an obstacle in the flow may develop a  [[Kármán vortex street]]. The Fig.1 shows the shedding of a vortex behind the trailing edge of a profile. The DMD-analysis was applied to 90 sequential Entropy fields [[:media:Joukowsky Karman Vortex Street video.gif|(animated gif (1.9MB) )]]<nowiki/>and yield an approximated eigenvalue-spectrum as depicted below. The analysis was applied to the numerical results, without referring to the governing equations. The profile is seen in white. The white arcs are the processor boundaries since the computation was performed on a parallel computer using different computational blocks.\n[[File:Joukowsky Karman Vortex Street Spectrum.png|center|300px|Fig.2 DMD-spectrum]]\nRoughly a third of the spectrum was highly damped (large, negative <math>\\lambda_r</math>) and is not shown.  The dominant shedding mode is shown in the following pictures. The image to the left is the real part, the image to the right, the imaginary part of the eigenvector.\n\n{| style=\"margin:0 auto;\"\n|-\n| [[File:Joukowsky Karman Vortex Street EV real.png|400px]]\n| [[File:Joukowsky Karman Vortex Street EV imag.png|400px]]\n|}\n\nAgain, the entropy-eigenvector is shown in this picture. The acoustic contents of the same mode is seen in the bottom half of the next plot. The top half corresponds to the entropy mode as above.\n[[File:Joukowsky Karman Vortex Street ps EV real.png|center|600px]]\n\n=== Synthetic example of a traveling pattern ===\nThe DMD analysis assumes a pattern of the form\n<math>\nq(x_1,x_2,x_3, \\ldots)=e^ {c x_1 }\\hat q(x_2,x_3,\\ldots)\n</math>\nwhere <math> x_1 </math> is any of the independent variables of the problem, but has to be selected in advance.\nTake for example the pattern\n:<math>\nq(x,y,t)=e^{-i \\omega t} \\hat q (x,t) e^{-(y/b)^2} \\Re \\left\\{  e^{i (k x - \\omega t)}    \\right\\} + \\text{random noise}\n</math>\nWith  the time as the preselected exponential factor.\n\nA sample is given in the following figure with <math> \\omega = 2\\pi /0.1 </math>, <math> b=0.02 </math> and <math> k = 2\\pi/ b </math>. The left picture shows the pattern without, the right with noise added. The amplitude of the random noise is the same as that of the pattern.\n\n[[File:Q periodic.png|400px]][[File:Q periodic noise.png|400px]]\n\nA DMD analysis is performed with 21 synthetically generated fields using a time interval  <math> \\Delta t =1/90\\text{ s}</math>, limiting the analysis to <math> f =45\\text{ Hz}</math>.\n\n[[File:Synth Spec.png|center|600px]]\nThe spectrum is symmetric and shows three almost undamped modes (small negative real part), whereas the other modes are heavily damped.\nTheir numerical values are <math> \\omega_1=-0.201, \\omega_{2/3}=-0.223 \\pm i 62.768</math> respectively. The real one corresponds to the mean of the field, whereas <math> \\omega_{2/3}</math> corresponds to the imposed pattern with <math> f = 10\\text{ Hz} </math>. Yielding a relative error of&nbsp;−1/1000. Increasing the noise to 10 times the signal value yields about the same error. The real and imaginary part of one of the latter two eigenmodes is depicted in the following figure.\n[[File:Detected Eigenmode.png|center|600px]]\n\n== See also ==\nSeveral other decompositions of experimental data exist. If the governing equations are available, an eigenvalue decomposition might be feasible.\n* [[Eigenvalue decomposition]]\n* [[Empirical mode decomposition]]\n* [[Global mode]]\n* [[Normal mode]]\n* [[Proper orthogonal decomposition]]\n* [[Singular-value decomposition]]\n\n== References ==\n<references />\n\n\n* Schmid, P. J. & Sesterhenn, J. L. 2008 Dynamic mode decomposition of numerical and experimental data. In Bull. Amer. Phys. Soc., 61st APS meeting, p.&nbsp;208. San Antonio.\n* Hasselmann, K., 1988. POPs and PIPs. The reduction of complex dynamical systems using principal oscillation and interaction patterns. J. Geophys. Res., 93(D9): 10975&ndash;10988.\n\n[[Category:Experimental physics]]\n[[Category:Time series]]\n[[Category:Matrix decompositions]]"
    },
    {
      "title": "Eigendecomposition of a matrix",
      "url": "https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix",
      "text": "In [[linear algebra]], '''eigendecomposition''' or sometimes '''[[spectral decomposition]]''' is the [[factorization]] of a [[matrix (math)|matrix]] into a [[canonical form]], whereby the matrix is represented in terms of its [[eigenvalues and eigenvectors]]. Only [[diagonalizable matrix|diagonalizable matrices]] can be factorized in this way.\n\n== Fundamental theory of matrix eigenvectors and eigenvalues ==\n\n{{Main|Eigenvalue, eigenvector and eigenspace}}\n\nA (non-zero) vector {{math|'''v'''}} of dimension {{mvar|N}} is an '''eigenvector''' of a square {{math|''N'' × ''N''}} matrix {{math|'''A'''}} if it satisfies the linear equation\n:<math>\\mathbf{A} \\mathbf{v} = \\lambda \\mathbf{v}</math>\n\nwhere {{mvar|[[λ]]}} is a scalar, termed the '''eigenvalue''' corresponding to {{math|'''v'''}}.  That is, the eigenvectors are the vectors that the linear transformation {{math|'''A'''}} merely elongates or shrinks, and the amount that they elongate/shrink by is the eigenvalue.  The above equation is called the '''eigenvalue equation''' or the '''eigenvalue problem'''.\n\nThis yields an equation for the eigenvalues\n:<math> p\\left(\\lambda\\right) = \\det\\left(\\mathbf{A} - \\lambda \\mathbf{I}\\right)= 0. </math>\n\nWe call {{math|''p''(''λ'')}} the '''[[characteristic polynomial]]''', and the equation, called the '''characteristic equation''', is an {{mvar|N}}th order polynomial equation in the unknown {{mvar|λ}}. This equation will have {{mvar|N<sub>λ</sub>}} distinct solutions, where {{math|1 ≤ ''N<sub>λ</sub>'' ≤ ''N''}}.  The set of solutions, that is, the eigenvalues, is called the '''[[spectrum of a matrix|spectrum]]''' of {{math|'''A'''}}.<ref>{{harvtxt|Golub|Van Loan|1996|p=310}}</ref><ref>{{harvtxt|Kreyszig|1972|p=273}}</ref><ref>{{harvtxt|Nering|1970|p=270}}</ref>\n\nWe can [[Factorization|factor]] {{mvar|p}} as\n:<math>p\\left(\\lambda\\right) = \\left(\\lambda - \\lambda_1\\right)^{n_1}\\left(\\lambda - \\lambda_2\\right)^{n_2} \\cdots \\left(\\lambda-\\lambda_k\\right)^{n_k} = 0. </math>\n\nThe integer {{mvar|n<sub>i</sub>}} is termed the '''[[algebraic multiplicity]]''' of eigenvalue {{mvar|λ<sub>i</sub>}}. If the field of scalars is [[Algebraically closed field|algebraically closed]], the algebraic multiplicities sum to {{mvar|N}}:\n:<math>\\sum\\limits_{i=1}^{N_\\lambda}{n_i} = N.</math>\n\nFor each eigenvalue {{mvar|λ<sub>i</sub>}}, we have a specific eigenvalue equation\n:<math>\\left(\\mathbf{A} - \\lambda_i \\mathbf{I}\\right)\\mathbf{v} = 0. </math>\n\nThere will be {{math|1 ≤ ''m''<sub>''i''</sub> ≤ ''n''<sub>''i''</sub>}} [[linearly independent]] solutions to each eigenvalue equation.  The linear combinations of the {{math|''m''<sub>''i''</sub>}} solutions are the eigenvectors associated with the eigenvalue {{math|''λ''<sub>''i''</sub>}}.  The integer {{math|''m''<sub>''i''</sub>}} is termed the '''[[geometric multiplicity]]''' of {{math|''λ''<sub>''i''</sub>}}.  It is important to keep in mind that the algebraic multiplicity {{math|''n''<sub>''i''</sub>}} and geometric multiplicity {{math|''m''<sub>''i''</sub>}} may or may not be equal, but we always have {{math|''m''<sub>''i''</sub> ≤ ''n''<sub>''i''</sub>}}.  The simplest case is of course when {{math|1=''m''<sub>''i''</sub> = ''n''<sub>''i''</sub> = 1}}.  The total number of linearly independent eigenvectors, {{math|''N''<sub>'''v'''</sub>}}, can be calculated by summing the geometric multiplicities\n:<math>\\sum\\limits_{i=1}^{N_{\\lambda}}{m_i} = N_{\\mathbf{v}}.</math>\n\nThe eigenvectors can be indexed by eigenvalues, using a double index, with {{math|'''v'''<sub>''ij''</sub>}} being the {{mvar|j}}th eigenvector for the {{mvar|i}}th eigenvalue.  The eigenvectors can also be indexed using the simpler notation of a single index {{math|'''v'''<sub>''k''</sub>}}, with {{math|1=''k'' = 1, 2, ..., ''N''<sub>'''v'''</sub>}}.\n\n== Eigendecomposition of a matrix ==\n\nLet {{math|'''A'''}} be a square {{math|''n'' × ''n''}} matrix with {{mvar|n}} [[linearly independent]] eigenvectors {{mvar|q<sub>i</sub>}} (where {{math|1=''i'' = 1, ..., ''n''}}).  Then {{math|'''A'''}} can be [[matrix decomposition|factorized]] as\n:<math>\\mathbf{A}=\\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^{-1}  </math>\nwhere {{math|'''Q'''}} is the square {{math|''n'' × ''n''}} matrix whose {{mvar|i}}th column is the eigenvector {{mvar|q<sub>i</sub>}} of {{math|'''A'''}}, and {{math|'''Λ'''}} is the [[diagonal matrix]] whose diagonal elements are the corresponding eigenvalues, {{math|1=''Λ<sub>ii</sub>'' = ''λ<sub>i</sub>''}}. Note that only [[diagonalizable matrix|diagonalizable matrices]] can be factorized in this way. For example, the [[defective matrix]] <math>\\left[ \\begin{smallmatrix} 1 & 1 \\\\ 0 & 1 \\end{smallmatrix} \\right]</math> cannot be diagonalized.\n\nThe {{mvar|n}} eigenvectors {{mvar|q<sub>i</sub>}} are usually normalized, but they need not be.  A non-normalized set of {{mvar|n}} eigenvectors, {{mvar|v<sub>i</sub>}} can also be used as the columns of {{math|'''Q'''}}.  That can be understood by noting that the magnitude of the eigenvectors in {{math|'''Q'''}} gets canceled in the decomposition by the presence of {{math|'''Q'''<sup>−1</sup>}}.\n\nThe decomposition can be derived from the fundamental property of eigenvectors:\n:<math>\\begin{align}\n\\mathbf{A} \\mathbf{v} &= \\lambda \\mathbf{v} \\\\\n\\mathbf{A} \\mathbf{Q} &= \\mathbf{Q} \\mathbf{\\Lambda} \\\\\n\\mathbf{A} &= \\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^{-1} .\n\\end{align}</math>\n\n=== Example ===\n\nThe 2 × 2 real matrix {{math|'''A'''}}\n:<math>\\mathbf{A} = \\begin{bmatrix} 1 & 0 \\\\ 1 & 3 \\\\ \\end{bmatrix}</math>\nmay be decomposed into a diagonal matrix through multiplication of a non-singular matrix {{math|'''B'''}}\n:<math>\\mathbf{B} = \\begin{bmatrix}\n    a & b \\\\\n    c & d\n  \\end{bmatrix} \\in \\mathbb{R}^{2\\times2}.\n</math>\n\nThen\n: <math>\\begin{bmatrix}\n    a & b \\\\\n    c & d \n  \\end{bmatrix}^{-1}\\begin{bmatrix}\n    1 & 0 \\\\\n    1 & 3\n  \\end{bmatrix}\\begin{bmatrix}\n    a & b \\\\\n    c & d\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    x & 0 \\\\\n    0 & y\n  \\end{bmatrix},\n</math>\nfor some real diagonal matrix <math>\\left[ \\begin{smallmatrix} x & 0 \\\\ 0 & y \\end{smallmatrix} \\right]</math>.\n\nMultiplying both sides of the equation on the left by {{math|'''B'''}}:\n\n: <math>\\begin{bmatrix} 1 & 0 \\\\ 1 & 3 \\end{bmatrix} \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\begin{bmatrix} x & 0 \\\\ 0 & y \\end{bmatrix}.</math>\n\nThe above equation can be decomposed into two [[simultaneous equation]]s:\n\n: <math> \\begin{cases} \\begin{bmatrix} 1 & 0\\\\ 1 & 3 \\end{bmatrix} \\begin{bmatrix} a \\\\ c \\end{bmatrix} = \\begin{bmatrix} ax \\\\ cx \\end{bmatrix} \\\\ \\begin{bmatrix} 1 & 0\\\\ 1 & 3 \\end{bmatrix} \\begin{bmatrix} b \\\\ d \\end{bmatrix} = \\begin{bmatrix} by \\\\ dy \\end{bmatrix} \\end{cases} .</math>\n\nFactoring out the [[eigenvalues]] {{mvar|x}} and {{mvar|y}}:\n: <math> \\begin{cases} \\begin{bmatrix} 1 & 0\\\\ 1 & 3 \\end{bmatrix} \\begin{bmatrix} a \\\\ c \\end{bmatrix} = x\\begin{bmatrix} a \\\\ c \\end{bmatrix} \\\\ \\begin{bmatrix} 1 & 0\\\\ 1 & 3 \\end{bmatrix} \\begin{bmatrix} b \\\\ d \\end{bmatrix} = y\\begin{bmatrix} b \\\\ d \\end{bmatrix} \\end{cases} </math>\n\nLetting\n:<math>\\overrightarrow{a} = \\begin{bmatrix} a \\\\ c \\end{bmatrix}, \\quad \\overrightarrow{b} = \\begin{bmatrix} b \\\\ d \\end{bmatrix},</math>\nthis gives us two vector equations:\n\n: <math> \\begin{cases} A \\overrightarrow{a} = x \\overrightarrow{a} \\\\ A \\overrightarrow{b} = y \\overrightarrow{b} \\end{cases}</math>\n\nAnd can be represented by a single vector equation involving two solutions as eigenvalues:\n: <math>\\mathbf{A} \\mathbf{u} = \\lambda \\mathbf{u}</math>\n\nwhere {{mvar|λ}} represents the two eigenvalues {{mvar|x}} and {{mvar|y}}, and {{math|'''u'''}} represents the vectors {{math|{{vec|''a''}}}} and {{math|{{vec|''b''}}}}.\n\nShifting {{math|''λ'''''u'''}} to the left hand side and factoring {{math|'''u'''}} out\n: <math>(\\mathbf{A} - \\lambda \\mathbf{I}) \\mathbf{u} = \\mathbf{0}</math>\n\nSince {{math|'''B'''}} is non-singular, it is essential that {{math|'''u'''}} is non-zero. Therefore,\n: <math>\\det(\\mathbf{A} - \\lambda \\mathbf{I}) = 0</math>\n\nThus\n: <math>(1- \\lambda)(3 - \\lambda) = 0</math>\n\ngiving us the solutions of the eigenvalues for the matrix {{math|'''A'''}} as {{math|1=''λ'' = 1}} or {{math|1=''λ'' = 3}}, and the resulting diagonal matrix from the eigendecomposition of {{math|'''A'''}} is thus <math>\\left[ \\begin{smallmatrix} 1 & 0 \\\\ 0 & 3 \\end{smallmatrix} \\right]</math>.\n\nPutting the solutions back into the above simultaneous equations\n: <math> \\begin{cases} \\begin{bmatrix} 1 & 0 \\\\ 1 & 3 \\end{bmatrix} \\begin{bmatrix} a \\\\ c \\end{bmatrix} = 1\\begin{bmatrix} a \\\\ c \\end{bmatrix} \\\\ \\begin{bmatrix} 1 & 0\\\\ 1 & 3 \\end{bmatrix} \\begin{bmatrix} b \\\\ d \\end{bmatrix} = 3\\begin{bmatrix} b \\\\ d \\end{bmatrix} \\end{cases} </math>\n\nSolving the equations, we have\n:<math>a = -2c \\quad\\text{and} \\quad b = 0, \\qquad [c,d] \\in \\mathbb{R}.</math>\n\nThus the matrix {{math|'''B'''}} required for the eigendecomposition of {{math|'''A'''}} is\n:<math>\\mathbf{B} = \\begin{bmatrix} -2c & 0 \\\\ c & d \\end{bmatrix},\\qquad [c, d]\\in \\mathbb{R}, </math>\nthat is:\n: <math>\\begin{bmatrix}\n-2c & 0 \\\\ c & d \\end{bmatrix}^{-1} \\begin{bmatrix} 1 & 0 \\\\ 1 & 3 \\end{bmatrix} \\begin{bmatrix} -2c & 0 \\\\ c & d \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 3 \\end{bmatrix},\\qquad [c, d]\\in \\mathbb{R}</math>\n\n=== Matrix inverse via eigendecomposition ===\n\n{{Main|Inverse matrix}}\n\nIf a matrix {{math|'''A'''}} can be eigendecomposed and if none of its eigenvalues are zero, then {{math|'''A'''}} is [[nonsingular]] and its inverse is given by\n:<math>\\mathbf{A}^{-1} = \\mathbf{Q}\\mathbf{\\Lambda}^{-1}\\mathbf{Q}^{-1}</math>\n\nFurthermore, because {{math|'''Λ'''}} is a [[diagonal matrix]], its inverse is easy to calculate:\n:<math>\\left[\\Lambda^{-1}\\right]_{ii} = \\frac{1}{\\lambda_i}</math>\n\n==== Practical implications<ref name=inverse>{{cite journal|last1=Hayde|first1= A. F. |last2=Twede|first2=D. R. |title=Observations on relationship between eigenvalues, instrument noise and detection performance|bibcode=2002SPIE.4816..355H|volume=4816|year=2002|pages=355|journal=Imaging Spectrometry VIII.|doi=10.1117/12.453777|series=Proceedings of SPIE|editor1-last=Shen|editor1-first=Sylvia S.}}</ref> ====\n\nWhen eigendecomposition is used on a matrix of measured, real [[data]], the [[inverse function|inverse]] may be less valid when all eigenvalues are used unmodified in the form above. This is because as eigenvalues become relatively small, their contribution to the inversion is large.  Those near zero or at the \"noise\" of the measurement system will have undue influence and could hamper solutions (detection) using the inverse.\n\nTwo mitigations have been proposed: truncating small or zero eigenvalues, and extending the lowest reliable eigenvalue to those below it.\n\nThe first mitigation method is similar to a sparse sample of the original matrix, removing components that are not considered valuable.  However, if the solution or detection process is near the noise level, truncating may remove components that influence the desired solution.\n\nThe second mitigation extends the eigenvalue so that lower values have much less influence over inversion, but do still contribute, such that solutions near the noise will still be found.\n\nThe reliable eigenvalue can be found by assuming that eigenvalues of extremely similar and low value are a good representation of measurement noise (which is assumed low for most systems).\n\nIf the eigenvalues are rank-sorted by value, then the reliable eigenvalue can be found by minimization of the [[Laplace operator|Laplacian]] of the sorted eigenvalues:<ref name=inverse2>{{cite journal| last1=Twede|first1= D. R. |last2=Hayden|first2= A. F. |title=Refinement and generalization of the extension method of covariance matrix inversion by regularization|bibcode=2004SPIE.5159..299T| volume=5159| year=2004| pages=299| journal=Imaging Spectrometry IX.| doi=10.1117/12.506993| series=Proceedings of SPIE| editor1-last=Shen| editor1-first=Sylvia S| editor2-last=Lewis| editor2-first=Paul E}}</ref>\n\n:<math>\\min\\left|\\nabla^2 \\lambda_\\mathrm{s}\\right|</math>\n\nwhere the eigenvalues are subscripted with an {{math|s}} to denote being sorted.  The position of the minimization is the lowest reliable eigenvalue.  In measurement systems, the square root of this reliable eigenvalue is the average noise over the components of the system.\n\n== Functional calculus ==\n\nThe eigendecomposition allows for much easier computation of [[power series]] of matrices. If {{math|''f''&thinsp;(''x'')}} is given by\n:<math>f(x) = a_0 + a_1 x + a_2 x^2 + \\cdots</math>\n\nthen we know that\n:<math>f\\left(\\mathbf{A}\\right) = \\mathbf{Q}f\\left(\\mathbf{\\Lambda}\\right)\\mathbf{Q}^{-1}</math>\n\nBecause {{math|'''Λ'''}} is a [[diagonal matrix]], functions of {{math|'''Λ'''}} are very easy to calculate:\n:<math>\\left[f\\left(\\mathbf{\\Lambda}\\right)\\right]_{ii} = f\\left(\\lambda_i\\right)</math>\n\nThe off-diagonal elements of {{math|''f''&thinsp;('''Λ''')}} are zero; that is, {{math|''f''&thinsp;('''Λ''')}} is also a diagonal matrix. Therefore, calculating {{math|''f''&thinsp;('''A''')}} reduces to just calculating the function on each of the eigenvalues.\n\nA similar technique works more generally with the [[holomorphic functional calculus]], using\n:<math>\\mathbf{A}^{-1} = \\mathbf{Q}\\mathbf{\\Lambda}^{-1}\\mathbf{Q}^{-1}</math>\n\nfrom [[#Matrix inverse via eigendecomposition|above]]. Once again, we find that\n:<math>\\left[f\\left(\\mathbf{\\Lambda}\\right)\\right]_{ii} = f\\left(\\lambda_i\\right)</math>\n\n=== Examples ===\n:<math>\\begin{align}\n  \\mathbf{A}^2 &= \\left(\\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^{-1}\\right)\\left(\\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^{-1}\\right)\n                = \\mathbf{Q}\\mathbf{\\Lambda}\\left(\\mathbf{Q}^{-1}\\mathbf{Q}\\right)\\mathbf{\\Lambda}\\mathbf{Q}^{-1}\n                = \\mathbf{Q}\\mathbf{\\Lambda}^2\\mathbf{Q}^{-1} \\\\\n  \\mathbf{A}^n &= \\mathbf{Q}\\mathbf{\\Lambda}^n\\mathbf{Q}^{-1}\n\\end{align}</math>\n\n== Decomposition for special matrices ==\n{{Expand section|date=June 2008}}\n\n=== Normal matrices ===\nA complex [[normal matrix]] ({{math|1='''A'''*'''A''' = '''AA'''*}}) has an orthogonal eigenvector basis, so a normal matrix can be decomposed as\n\n:<math>\\mathbf{A} = \\mathbf{U}\\mathbf{\\Lambda}\\mathbf{U}^*</math>\n\nwhere {{math|'''U'''}} is a [[unitary matrix]] and {{math|1='''Λ''' = diag(''λ''<sub>1</sub>, ..., ''λ<sub>n</sub>'')}} is a [[diagonal matrix]].{{Citation needed|date=November 2018}} If {{math|'''A'''}} is further restricted to be a [[Hermitian matrix]] ({{math|1='''A''' = '''A'''*}}), then {{math|'''Λ'''}} has only real valued entries. If {{math|'''A'''}} instead is further restricted to a unitary matrix ({{math|1='''A'''*'''A''' = '''AA'''* = '''I'''}}), {{math|'''Λ'''}} takes all its values on the complex unit circle, that is, {{math|1={{abs|''λ<sub>i</sub>''}} = 1}}.\n\n=== Real symmetric matrices ===\n\nAs a special case, for every {{math|''N'' × ''N''}} real [[symmetric matrix]], the eigenvalues are real and the eigenvectors can be chosen such that they are orthogonal to each other. Thus a real symmetric matrix {{math|'''A'''}} can be decomposed as\n:<math>\\mathbf{A} = \\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^\\mathsf{T}</math>\n\nwhere {{math|'''Q'''}} is an [[orthogonal matrix]] whose columns are the eigenvectors of {{math|'''A'''}}, and {{math|'''Λ'''}} is a diagonal matrix whose entries are the eigenvalues of {{math|'''A'''}}.\n\n== Useful facts ==\n\n=== Useful facts regarding eigenvalues ===\n{{bulleted list\n| The product of the eigenvalues is equal to the [[determinant]] of {{math|'''A'''}}\n: <math>\\det\\left(\\mathbf{A}\\right) = \\prod\\limits_{i=1}^{N_{\\lambda}}{\\lambda_i^{n_i}} </math>\n\nNote that each eigenvalue is raised to the power {{math|''n<sub>i</sub>''}}, the [[algebraic multiplicity]].\n| The sum of the eigenvalues is equal to the [[Trace (linear algebra)|trace]] of {{math|'''A'''}}\n: <math>\\operatorname{tr}\\left(\\mathbf{A}\\right) = \\sum\\limits_{i=1}^{N_{\\lambda}}{{n_i}\\lambda_i} </math>\nNote that each eigenvalue is multiplied by {{math|''n<sub>i</sub>''}}, the [[algebraic multiplicity]].\n| If the eigenvalues of {{math|'''A'''}} are {{math|''λ''<sub>''i''</sub>}}, and {{math|'''A'''}} is invertible, then the eigenvalues of {{math|'''A'''<sup>−1</sup>}} are simply {{math|''λ''{{su|b=''i''|p=−1}}}}.\n| If the eigenvalues of {{math|'''A'''}} are {{math|''λ''<sub>''i''</sub>}}, then the eigenvalues of {{math|''f''&thinsp;('''A''')}} are simply {{math|''f''&thinsp;(''λ''<sub>''i''</sub>)}}, for any [[holomorphic function]] {{mvar|f}}.\n}}\n\n=== Useful facts regarding eigenvectors ===\n* If {{math|'''A'''}} is [[Hermitian matrix|Hermitian]] and full-rank, the basis of eigenvectors may be chosen to be mutually [[orthogonal]]. The eigenvalues are real.\n* The eigenvectors of {{math|'''A'''<sup>−1</sup>}} are the same as the eigenvectors of {{math|'''A'''}}.\n* Eigenvectors are defined up to a phase, i.e. if {{math|1='''Av''' = ''λ'''''v'''}} then {{math|''e''<sup>''iθ''</sup>'''v'''}} is also an eigenvector,  and specifically so is {{math|−'''v'''}} (where {{math|1=''θ'' = π}}.\n* In the case of degenerate eigenvalues (an eigenvalue appearing more than once), the eigenvectors have an additional freedom of rotation, that is to say any linear (orthonormal) combination of eigenvectors sharing an eigenvalue (in the degenerate subspace), are themselves eigenvectors (in the subspace).\n\n=== Useful facts regarding eigendecomposition ===\n\n* {{math|'''A'''}} can be eigendecomposed if and only if\n*: <math>N_\\mathbf{v} = N\\,</math>\n* If {{math|''p''(''λ'')}} has no repeated roots, i.e. {{math|1=''N<sub>λ</sub>'' = ''N''}}, then {{math|'''A'''}} can be eigendecomposed.\n* The statement \"{{math|'''A'''}} can be eigendecomposed\" does ''not'' imply that {{math|'''A'''}} has an inverse.\n* The statement \"{{math|'''A'''}} has an inverse\" does ''not'' imply that {{math|'''A'''}} can be eigendecomposed. A counterexample is <math>\\left[ \\begin{smallmatrix} 1 & 1 \\\\ 0 & 1 \\end{smallmatrix} \\right]</math>, which is an invertable [[defective matrix]].\n\n=== Useful facts regarding matrix inverse ===\n\n* {{math|'''A'''}} can be inverted [[if and only if]]\n*: <math>\\lambda_i \\ne 0 \\quad \\forall \\,i</math>\n* If {{math|''λ<sub>i</sub>'' ≠ 0}} ''and'' {{math|1=''N''<sub>'''v'''</sub> = ''N''}}, the inverse is given by\n*: <math>\\mathbf{A}^{-1} = \\mathbf{Q}\\mathbf{\\Lambda}^{-1}\\mathbf{Q}^{-1}</math>\n\n== Numerical computations ==\n{{details|eigenvalue algorithm}}\n\n=== Numerical computation of eigenvalues ===\n\nSuppose that we want to compute the eigenvalues of a given matrix. If the matrix is small, we can compute them symbolically using the [[characteristic polynomial]]. However, this is often impossible for larger matrices, in which case we must use a [[numerical analysis|numerical method]].\n\nIn practice, eigenvalues of large matrices are not computed using the characteristic polynomial. Computing the polynomial becomes expensive in itself, and exact (symbolic) roots of a high-degree polynomial can be difficult to compute and express: the [[Abel–Ruffini theorem]] implies that the roots of high-degree (5 or above) polynomials cannot in general be expressed simply using  {{mvar|n}}th roots. Therefore, general algorithms to find eigenvectors and eigenvalues are [[iterative method|iterative]].\n\nIterative numerical algorithms for approximating roots of polynomials exist, such as [[Newton's method]], but in general it is impractical to compute the characteristic polynomial and then apply these methods.  One reason is that small [[round-off error]]s in the coefficients of the characteristic polynomial can lead to large errors in the eigenvalues and eigenvectors: the roots are an extremely [[ill-conditioned]] function of the coefficients.<ref name=Trefethen>{{Cite book|author1-link=Lloyd N. Trefethen|author1-first=Lloyd N.|author1-last=Trefethen|author2-first=David|author2-last=Bau|title=Numerical Linear Algebra|isbn=978-0-89871-361-9|publisher=SIAM|year= 1997}}</ref>\n\nA simple and accurate iterative method is the [[power method]]: a [[random]] vector {{math|'''v'''}} is chosen and a sequence of [[unit vector]]s is computed as\n: <math>\\frac{\\mathbf{A}\\mathbf{v}}{\\left\\|\\mathbf{A}\\mathbf{v}\\right\\|}, \\frac{\\mathbf{A}^2\\mathbf{v}}{\\left\\|\\mathbf{A}^2\\mathbf{v}\\right\\|}, \\frac{\\mathbf{A}^3\\mathbf{v}}{\\left\\|\\mathbf{A}^3\\mathbf{v}\\right\\|}, \\ldots</math>\n\nThis [[sequence]] will [[almost always]] converge to an eigenvector corresponding to the eigenvalue of greatest magnitude, provided that {{math|'''v'''}} has a nonzero component of this eigenvector in the eigenvector basis (and also provided that there is only one eigenvalue of greatest magnitude). This simple algorithm is useful in some practical applications; for example, [[Google]] uses it to calculate the [[PageRank|page rank]] of documents in their search engine.<ref>[[Ilse Ipsen|Ipsen, Ilse]], and Rebecca M. Wills, ''[http://www4.ncsu.edu/~ipsen/ps/slides_imacs.pdf Analysis and Computation of Google's PageRank]'', 7th IMACS International Symposium on Iterative Methods in Scientific Computing, Fields Institute, Toronto, Canada, 5–8 May 2005.</ref>  Also, the power method is the starting point for many more sophisticated algorithms.  For instance, by keeping not just the last vector in the sequence, but instead looking at the [[linear span|span]] of ''all'' the vectors in the sequence, one can get a better (faster converging) approximation for the eigenvector, and this idea is the basis of [[Arnoldi iteration]].<ref name=Trefethen />  Alternatively, the important [[QR algorithm]] is also based on a subtle transformation of a power method.<ref name=Trefethen />\n\n=== Numerical computation of eigenvectors ===\n\nOnce the eigenvalues are computed, the eigenvectors could be calculated by solving the equation\n:<math>\\left(\\mathbf{A} - \\lambda_i \\mathbf{I}\\right)\\mathbf{v}_{i,j} = 0 </math>\n\nusing [[Gaussian elimination]] or [[System of linear equations#Solving a linear system|any other method]] for solving [[System of linear equations|matrix equations]].\n\nHowever, in practical large-scale eigenvalue methods, the eigenvectors are usually computed in other ways, as a byproduct of the eigenvalue computation.  In [[power iteration]], for example, the eigenvector is actually computed before the eigenvalue (which is typically computed by the [[Rayleigh quotient]] of the eigenvector).<ref name=Trefethen />  In the QR algorithm for a [[Hermitian matrix]] (or any [[normal matrix]]), the orthonormal eigenvectors are obtained as a product of the {{math|'''Q'''}} matrices from the steps in the algorithm.<ref name=Trefethen />  (For more general matrices, the QR algorithm yields the [[Schur decomposition]] first, from which the eigenvectors can be obtained by a [[backsubstitution]] procedure.<ref>{{Cite book|url=https://books.google.com/books?id=YVpyyi1M7vUC |publisher=Springer|chapter= section 5.8.2|title=Numerical Mathematics|pages=15|first1=Alfio |last1=Quarteroni |first2=Riccardo |last2=Sacco |first3=Fausto |last3=Saleri |isbn=978-0-387-98959-4|year=2000}}</ref>)  For Hermitian matrices, the [[Divide-and-conquer eigenvalue algorithm]] is more efficient than the QR algorithm if both eigenvectors and eigenvalues are desired.<ref name=Trefethen />\n\n== Additional topics ==\n\n=== Generalized eigenspaces ===\nRecall that the ''geometric'' multiplicity of an eigenvalue can be described as the dimension of the associated eigenspace, the nullspace of {{math|''λ'''''I''' − '''A'''}}. The algebraic multiplicity can also be thought of as a dimension: it is the dimension of the associated '''[[generalized eigenspace]]''' (1st sense), which is the nullspace of the matrix {{math|(''λ'''''I''' − '''A''')<sup>''k''</sup>}} for ''any sufficiently large {{mvar|k}}''. That is, it is the space of ''[[generalized eigenvector]]s'' (first sense), where a generalized eigenvector is any vector which ''eventually'' becomes 0 if {{math|''λ'''''I''' − '''A'''}} is applied to it enough times successively. Any eigenvector is a generalized eigenvector, and so each eigenspace is contained in the associated generalized eigenspace. This provides an easy proof that the geometric multiplicity is always less than or equal to the algebraic multiplicity.\n\nThis usage should not be confused with the ''generalized eigenvalue problem'' described below.\n\n=== Conjugate eigenvector ===\nA '''conjugate eigenvector''' or '''coneigenvector''' is a vector sent after transformation to a scalar multiple of its conjugate, where the scalar is called the '''conjugate eigenvalue''' or '''coneigenvalue''' of the linear transformation. The coneigenvectors and coneigenvalues represent essentially the same information and meaning as the regular eigenvectors and eigenvalues, but arise when an alternative coordinate system is used. The corresponding equation is\n: <math>\\mathbf{A}\\mathbf{v} = \\lambda \\mathbf{v}^*.</math>\n\nFor example, in coherent electromagnetic scattering theory, the linear transformation {{math|'''A'''}} represents the action performed by the scattering object, and the eigenvectors represent polarization states of the electromagnetic wave. In [[optics]], the coordinate system is defined from the wave's viewpoint, known as the [[Forward Scattering Alignment]] (FSA), and gives rise to a regular eigenvalue equation, whereas in [[radar]], the coordinate system is defined from the radar's viewpoint, known as the [[Back Scattering Alignment]] (BSA), and gives rise to a coneigenvalue equation.\n\n=== Generalized eigenvalue problem ===\nA '''generalized eigenvalue problem''' (second sense) is the problem of finding a vector {{math|'''v'''}} that obeys\n: <math> \\mathbf{A}\\mathbf{v} = \\lambda \\mathbf{B} \\mathbf{v}</math>\n\nwhere {{math|'''A'''}} and {{math|'''B'''}} are matrices. If {{math|'''v'''}} obeys this equation, with some {{mvar|λ}}, then we call {{math|'''v'''}} the ''generalized eigenvector'' of {{math|'''A'''}} and {{math|'''B'''}} (in the second sense), and {{mvar|λ}} is called the ''generalized eigenvalue'' of {{math|'''A'''}} and {{math|'''B'''}} (in the second sense) which corresponds to the generalized eigenvector {{math|'''v'''}}. The possible values of {{mvar|λ}} must obey the following equation\n:<math>\\det(\\mathbf{A} - \\lambda \\mathbf{B})=0. </math>\n\nIn the case we can find {{math|''n'' ∈ '''ℕ'''}} linearly independent vectors {{math|{'''v'''<sub>1</sub>, ..., '''v'''<sub>''n''</sub><nowiki>}</nowiki>}} so that for every {{math|''i'' ∈ {1, ..., ''n''<nowiki>}</nowiki>}}, {{math|1='''Av'''<sub>''i''</sub> = ''λ<sub>i</sub>'''''Bv'''<sub>''i''</sub>}}, where {{math|''λ<sub>i</sub>'' ∈ '''F'''}} then we define the matrices {{math|'''P'''}} and {{math|'''D'''}} such that\n:<math>P = \\begin{pmatrix}\n    | & & | \\\\\n    \\mathbf{v}_1 & \\cdots & \\mathbf{v}_n \\\\\n    | &  & | \n  \\end{pmatrix} \\equiv\n  \\begin{pmatrix}\n    (\\mathbf{v}_1)_1 & \\cdots & (\\mathbf{v}_n)_1 \\\\\n    \\vdots &  & \\vdots   \\\\\n    (\\mathbf{v}_1)_n & \\cdots & (\\mathbf{v}_n)_n \n  \\end{pmatrix}\n</math>\n:<math>(D)_{ij} = \\begin{cases}\n  \\lambda_i,  & \\text{if }i = j\\\\\n  0,          & \\text{otherwise}\n\\end{cases}</math>\n\nThen the following equality holds\n:<math>\\mathbf{A} = \\mathbf{B}\\mathbf{P}\\mathbf{D}\\mathbf{P}^{-1}</math>\n\nAnd the proof is\n:<math>\n  \\mathbf{A}\\mathbf{P}= \\mathbf{A} \\begin{pmatrix}\n    | & & | \\\\\n    \\mathbf{v}_1 & \\cdots & \\mathbf{v}_n   \\\\\n    | &  & | \n  \\end{pmatrix} = \\begin{pmatrix}\n    | & & | \\\\\n    A\\mathbf{v}_1 & \\cdots & A\\mathbf{v}_n   \\\\\n    | &  & | \n  \\end{pmatrix} = \\begin{pmatrix}\n    | & & | \\\\\n    \\lambda_1B\\mathbf{v}_1 & \\cdots & \\lambda_nB\\mathbf{v}_n   \\\\\n    | &  & | \n  \\end{pmatrix} = \\begin{pmatrix}\n    | & & | \\\\\n    B\\mathbf{v}_1 & \\cdots & B\\mathbf{v}_n   \\\\\n    | &  & | \n  \\end{pmatrix}\n  \\mathbf{D} =\n  \\mathbf{B}\\mathbf{P}\\mathbf{D}\n</math>\n\nAnd since {{math|'''P'''}} is invertible, we multiply the equation from the right by its inverse, finishing the proof.\n\nThe set of matrices of the form {{math|'''A''' − ''λ'''''B'''}}, where {{mvar|λ}} is a complex number, is called a ''pencil''; the term ''[[matrix pencil]]'' can also refer to the pair {{math|('''A''', '''B''')}} of matrices.<ref name=Bai-GHEP>{{cite book|editor1-first=Z. |editor1-last=Bai |editor2-link=James Demmel|editor2-first=J. |editor2-last=Demmel |editor3-first=J. |editor3-last=Dongarra |editor4-first=A. |editor4-last=Ruhe |editor5-first=H. |editor5-last=Van Der Vorst |title=Templates for the Solution of Algebraic Eigenvalue Problems: A Practical Guide|publisher=SIAM|location=Philadelphia|year= 2000|url=http://www.cs.utk.edu/~dongarra/etemplates/node156.html| chapter=Generalized Hermitian Eigenvalue Problems|isbn= 978-0-89871-471-5}}</ref>\n\nIf {{math|'''B'''}} is invertible, then the original problem can be written in the form\n: <math>\\mathbf{B}^{-1}\\mathbf{A}\\mathbf{v} = \\lambda \\mathbf{v}</math>\n\nwhich is a standard eigenvalue problem. However, in most situations it is preferable not to perform the inversion, but rather to solve the generalized eigenvalue problem as stated originally.  This is especially important if {{math|'''A'''}} and {{math|'''B'''}} are [[Hermitian matrices]], since in this case {{math|'''B'''<sup>−1</sup>'''A'''}} is not generally Hermitian and important properties of the solution are no longer apparent.\n\nIf {{math|'''A'''}} and {{math|'''B'''}} are Hermitian and {{math|'''B'''}} is a [[positive-definite matrix]], the eigenvalues {{math|''λ''}} are real and eigenvectors {{math|'''v'''<sub>1</sub>}} and {{math|'''v'''<sub>2</sub>}} with distinct eigenvalues are {{math|'''B'''}}-orthogonal ({{math|1='''v'''<sub>1</sub>*'''Bv'''<sub>2</sub> = '''0'''}}).<ref>{{cite book|last=Parlett|first=Beresford N.|title=The symmetric eigenvalue problem|date=1998|publisher=Society for Industrial and Applied Mathematics|location=Philadelphia|isbn=978-0-89871-402-9|page=345|url=http://epubs.siam.org/doi/book/10.1137/1.9781611971163|edition=Reprint.|doi=10.1137/1.9781611971163}}</ref>   Also, in this case it is guaranteed that there exists a [[basis (linear algebra)|basis]] of generalized eigenvectors (it is not a [[defective matrix|defective]] problem).<ref name=Bai-GHEP />  This case is sometimes called a ''Hermitian definite pencil'' or ''definite pencil''.<ref name=Bai-GHEP />\n\n== See also ==\n*[[Matrix decomposition]]\n*[[List of matrices]]\n*[[Eigenvalue, eigenvector and eigenspace]]\n*[[Spectral theorem]]\n*[[Singular value decomposition]]\n*[[Householder transformation]]\n*[[Frobenius covariant]]\n*[[Sylvester's formula]]\n*[[Eigenvalue perturbation]]\n\n== Notes ==\n{{Reflist}}\n\n== References ==\n\n* {{cite book|last= Franklin|first= Joel N. |year=1968|title=Matrix Theory|publisher= Dover Publications|isbn =978-0-486-41179-8}}\n* {{ citation | first1 = Gene H. | last1 = Golub | first2 = Charles F. | last2 = Van Loan | year = 1996 | isbn = 978-0-8018-5414-9 | title = Matrix Computations | edition = 3rd | publisher = [[Johns Hopkins University Press]] | location = Baltimore }}\n* {{cite book|last1=Horn|first1= Roger A. |last2= Johnson|first2=Charles R.|year=1985|title=Matrix Analysis|publisher=Cambridge University Press|isbn= 978-0-521-38632-6}}\n* {{cite book|last1=Horn|first1= Roger A. |last2= Johnson|first2=Charles R.|year=1991|title=Topics in Matrix Analysis|publisher= Cambridge University Press| isbn =978-0-521-46713-1}}\n* {{ citation | first1 = Erwin | last1 = Kreyszig | year = 1972 | isbn = 978-0-471-50728-4 | title = Advanced Engineering Mathematics | edition = 3rd | publisher = [[John Wiley & Sons|Wiley]] | location = New York }}\n* {{ citation | first1 = Evar D. | last1 = Nering | year = 1970 | title = Linear Algebra and Matrix Theory | edition = 2nd | publisher = [[John Wiley & Sons|Wiley]] | location = New York | lccn = 76091646 }}\n* {{cite book|last=Strang |first=G. |year=1998|title=Introduction to Linear Algebra|edition=3rd |publisher= Wellesley-Cambridge Press|isbn =978-0-9614088-5-5}}\n\n== External links ==\n* [http://people.revoledu.com/kardi/tutorial/LinearAlgebra/SpectralDecomposition.html Interactive program & tutorial of Spectral Decomposition].\n\n[[Category:Matrix theory]]\n[[Category:Matrix decompositions]]"
    },
    {
      "title": "Jordan normal form",
      "url": "https://en.wikipedia.org/wiki/Jordan_normal_form",
      "text": "{{short description|Form of a matrix indicating its eigenvalues and their algebraic multiplicities}}\n[[File:Jordan blocks.svg|right|thumb|250px|An example of a matrix in Jordan normal form. The grey blocks are called Jordan blocks. Note that the <math>\\lambda_i</math> in different blocks can be equal.]]\nIn [[linear algebra]], a '''Jordan normal form''', also known as a '''Jordan canonical form'''<ref>\nShilov defines the term ''Jordan canonical form'' and in a footnote says that ''Jordan normal form'' is synonymous.\nThese terms are sometimes shortened to ''Jordan form''. (Shilov)\nThe term ''Classical canonical form'' is also sometimes used in the sense of this article. (James & James, 1976)\n</ref>\nor '''JCF''',<ref name=\"Holt 2009 9\">{{harvtxt|Holt|Rumynin|2009|p=9}}</ref>\nis an [[upper triangular matrix]] of a particular form called a [[Jordan matrix]] representing a [[linear operator]] on a [[finite-dimensional]] [[vector space]] with respect to some [[Basis (linear algebra)|basis]]. Such a matrix has each non-zero off-diagonal entry equal to&nbsp;1, immediately above the main diagonal (on the [[superdiagonal]]), and with identical diagonal entries to the left and below them.\n\nLet ''V'' be a vector space over a [[field (mathematics)|field]] ''K''. Then a basis with respect to which the matrix has the required form exists [[if and only if]] all [[eigenvalue]]s of the matrix lie in ''K'', or equivalently if the [[characteristic polynomial]] of the operator splits into linear factors over ''K''. This condition is always satisfied if ''K'' is [[algebraically closed]] (for instance, if it is the field of [[complex number]]s). The diagonal entries of the normal form are the eigenvalues (of the operator), and the number of times each eigenvalue occurs is called the [[algebraic multiplicity]] of the eigenvalue.<ref name=\"Beauregard 1973 310–316\">{{harvtxt|Beauregard|Fraleigh|1973|pp=310–316}}</ref><ref name=\"Golub 1996 354\">{{harvtxt|Golub|Van Loan|1996|p=355}}</ref><ref name=\"Nering 1970 118–127\">{{harvtxt|Nering|1970|pp=118–127}}</ref>\n\nIf the operator is originally given by a [[square matrix]] ''M'', then its Jordan normal form is also called the Jordan normal form of ''M''. Any square matrix has a Jordan normal form if the field of coefficients is extended to one containing all the eigenvalues of the matrix. In spite of its name, the normal form for a given ''M'' is not entirely unique, as it is a [[block diagonal matrix]] formed of [[Jordan block]]s, the order of which is not fixed; it is conventional to group blocks for the same eigenvalue together, but no ordering is imposed among the eigenvalues, nor among the blocks for a given eigenvalue, although the latter could for instance be ordered by weakly decreasing size.<ref name=\"Beauregard 1973 310–316\"/><ref name=\"Golub 1996 354\"/><ref name=\"Nering 1970 118–127\"/>\n\nThe [[Jordan–Chevalley decomposition]] is particularly simple with respect to a basis for which the operator takes its Jordan normal form. The diagonal form for [[diagonalizable]] matrices, for instance [[normal matrix|normal matrices]], is a special case of the Jordan normal form.<ref>{{harvtxt|Beauregard|Fraleigh|1973|pp=270–274}}</ref><ref>{{harvtxt|Golub|Van Loan|1996|p=353}}</ref><ref>{{harvtxt|Nering|1970|pp=113–118}}</ref>\n\nThe Jordan normal form is named after [[Camille Jordan]], who first stated the Jordan decomposition theorem in 1870.<ref name=\"Brechenmacher-thesis\">Brechenmacher, [https://tel.archives-ouvertes.fr/tel-00142786 \"Histoire du théorème de Jordan de la décomposition matricielle (1870-1930). Formes de représentation et méthodes de décomposition\"], Thesis, 2007</ref>\n\n== Overview ==\n=== Notation ===\nSome textbooks have the ones on the [[subdiagonal]], i.e., immediately below the main diagonal instead of on the superdiagonal.  The eigenvalues are still on the main diagonal.<ref>{{harvtxt|Cullen|1966|p=114}}</ref><ref>{{harvtxt|Franklin|1968|p=122}}</ref>\n\n=== Motivation ===\nAn ''n'' &times; ''n'' matrix ''A'' is [[diagonalizable matrix|diagonalizable]] if and only if the sum of the dimensions of the eigenspaces is ''n''. Or, equivalently, if and only if ''A'' has ''n'' [[linearly independent]] [[eigenvectors]]. Not all matrices are diagonalizable; matrices that are not diagonalizable are called [[defective matrix|defective]] matrices. Consider the following matrix:\n\n<math>A=\n\\left[\\!\\!\\!\\begin{array}{*{20}{r}}\n  5 &  4 &  2 &  1 \\\\[2pt]\n  0 &  1 & -1 & -1 \\\\[2pt]\n -1 & -1 &  3 &  0 \\\\[2pt]\n  1 &  1 & -1 &  2\n\\end{array}\\!\\!\\right].</math>\n\nIncluding multiplicity, the eigenvalues of ''A'' are λ = 1, 2, 4, 4. The [[Hamel dimension|dimension]] of the eigenspace corresponding to the eigenvalue 4 is 1 (and not 2), so ''A'' is not diagonalizable. However, there is an invertible matrix ''P'' such that ''J'' = ''PAP''<sup>&minus;1</sup>, where\n\n:<math>J = \\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\[2pt]\n0 & 2 & 0 & 0 \\\\[2pt]\n0 & 0 & 4 & 1 \\\\[2pt]\n0 & 0 & 0 & 4 \\end{bmatrix}.</math>\n\nThe matrix J is almost diagonal. This is the Jordan normal form of ''A''. The section [[#Example|''Example'']] below fills in the details of the computation.\n\n== Complex matrices ==\n\nIn general, a square complex matrix ''A'' is [[similar (linear algebra)|similar]] to a [[block diagonal matrix]]\n\n:<math>J = \\begin{bmatrix}\nJ_1 & \\;     & \\; \\\\\n\\;  & \\ddots & \\; \\\\ \n\\;  & \\;     & J_p\\end{bmatrix}</math>\n\nwhere each block ''J''<sub>i</sub> is a square matrix of the form\n\n:<math>J_i = \n\\begin{bmatrix}\n\\lambda_i & 1            & \\;     & \\;  \\\\\n\\;        & \\lambda_i    & \\ddots & \\;  \\\\\n\\;        & \\;           & \\ddots & 1   \\\\\n\\;        & \\;           & \\;     & \\lambda_i       \n\\end{bmatrix}.</math>\n \nSo there exists an invertible matrix ''P'' such that ''P<sup>−1</sup>AP'' = ''J'' is such that the only non-zero entries of ''J'' are on the diagonal and the superdiagonal. ''J'' is called the '''Jordan normal form''' of ''A''. Each ''J''<sub>''i''</sub> is called a [[Jordan block]] of ''A''. In a given Jordan block, every entry on the superdiagonal is 1.\n\nAssuming this result, we can deduce the following properties:\n\n* Counting multiplicity, the eigenvalues of ''J'', therefore ''A'', are the diagonal entries.\n* Given an eigenvalue λ<sub>''i''</sub>, its '''[[geometric multiplicity]]''' is the dimension of Ker(''A'' &minus; λ<sub>''i'' </sub>'''[[identity matrix|I]]'''), and it is the number of Jordan blocks corresponding to λ<sub>''i''</sub>.<ref name=\"HJp321\">{{harvtxt|Horn|Johnson|1985|loc=§3.2.1}}</ref>\n* The sum of the sizes of all Jordan blocks corresponding to an eigenvalue λ<sub>''i''</sub> is its '''[[algebraic multiplicity]]'''.<ref name=\"HJp321\" />\n* ''A'' is diagonalizable if and only if, for every eigenvalue λ of ''A'', its geometric and algebraic multiplicities coincide.\n* The Jordan block corresponding to λ is of the form λ '''I''' + ''N'', where ''N'' is a [[nilpotent matrix]] defined as ''N''<sub>''ij''</sub> = δ<sub>''i'',''j''&minus;1</sub> (where δ is the [[Kronecker delta]]). The nilpotency of ''N'' can be exploited when calculating ''f''(''A'') where ''f'' is a complex analytic function. For example, in principle the Jordan form could give a closed-form expression for the exponential exp(''A'').\n* The number of Jordan blocks corresponding to λ of size at least ''j'' is dim Ker(''A - λI)<sup>j</sup> -'' dim Ker''(A - λI)<sup>j-1</sup>''. Thus, the number of Jordan blocks of size exactly ''j'' is\n:<math>2 \\dim \\ker (A - \\lambda_i I)^j - \\dim \\ker (A - \\lambda_i I)^{j+1} - \\dim \\ker (A - \\lambda_i I)^{j-1}</math>\n* Given an eigenvalue λ<sub>''i''</sub>, its multiplicity in the minimal polynomial is the size of its largest Jordan block.\n\n=== Example ===\nConsider the matrix ''A'' from the example in the previous section. The Jordan normal form is obtained by some similarity transformation ''P''<sup>&minus;1</sup>''AP'' = ''J'', i.e.,\n\n:<math>\\; AP = PJ.</math>\n\nLet ''P'' have column vectors ''p''<sub>''i''</sub>, ''i'' = 1, ..., 4, then\n\n: <math>A \\begin{bmatrix} p_1 & p_2 & p_3 & p_4 \\end{bmatrix} = \\begin{bmatrix} p_1 & p_2 & p_3 & p_4 \\end{bmatrix}\n\\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 2 & 0 & 0 \\\\ \n0 & 0 & 4 & 1 \\\\\n0 & 0 & 0 & 4 \\end{bmatrix} = \\begin{bmatrix} p_1 & 2p_2 & 4p_3 & p_3+4p_4 \\end{bmatrix}.</math>\n\nWe see that\n:<math>\\; (A - 1 I) p_1 = 0 </math>\n:<math>\\; (A - 2 I) p_2 = 0 </math>\n:<math>\\; (A - 4 I) p_3 = 0 </math>\n:<math>\\; (A - 4 I) p_4 = p_3. </math>\n\nFor ''i'' = 1,2,3 we have <math>p_i \\in \\operatorname{Ker}(A-\\lambda_{i} I)</math>, i.e., ''p''<sub>i</sub> is an eigenvector of ''A'' corresponding to the eigenvalue λ<sub>i</sub>. For ''i''=4, multiplying both sides by <math>(A-4I)</math> gives\n:<math>\\; (A-4I)^2 p_4 = (A-4I) p_3. </math>\nBut <math>(A-4I)p_3 = 0</math>, so\n:<math>\\; (A-4I)^2 p_4 = 0. </math>\nThus, <math>p_4 \\in \\operatorname{Ker}(A-4 I)^2.</math>\n\nVectors such as <math>p_4</math> are called [[generalized eigenvector]]s of ''A''.\n=== Example: Obtaining the normal form ===\n\nThis example shows how to calculate the Jordan normal form of a given matrix. As the next section explains, it is important to do the computation exactly instead of rounding the results.\n\nConsider the matrix\n:<math>A =\n\\begin{bmatrix}\n 5 &  4 &  2 &  1 \\\\\n 0 &  1 & -1 & -1 \\\\\n-1 & -1 &  3 &  0 \\\\ \n 1 &  1 & -1 &  2\n\\end{bmatrix}</math>\nwhich is mentioned in the beginning of the article.\n\nThe [[characteristic polynomial]] of ''A'' is\n:<math> \\begin{align} \\chi(\\lambda) & = \\det(\\lambda I - A) \\\\ &  = \\lambda^4 - 11 \\lambda^3 + 42 \\lambda^2 - 64 \\lambda + 32  \\\\ & = (\\lambda-1)(\\lambda-2)(\\lambda-4)^2. \\, \\end{align} </math>\nThis shows that the eigenvalues are 1, 2, 4 and 4, according to algebraic multiplicity. The eigenspace corresponding to the eigenvalue 1 can be found by solving the equation ''Av'' = ''λ v''. It is spanned by the column vector ''v'' = (&minus;1, 1, 0, 0)<sup>T</sup>. Similarly, the eigenspace corresponding to the eigenvalue 2 is spanned by ''w'' = (1, &minus;1, 0, 1)<sup>T</sup>. Finally, the eigenspace corresponding to the eigenvalue 4 is also one-dimensional (even though this is a double eigenvalue) and is spanned by ''x'' = (1, 0, &minus;1, 1)<sup>T</sup>. So, the [[geometric multiplicity]] (i.e., the dimension of the eigenspace of the given eigenvalue) of each of the three eigenvalues is one. Therefore, the two eigenvalues equal to 4 correspond to a single Jordan block, and the Jordan normal form of the matrix ''A'' is the [[Matrix addition#Direct sum|direct sum]]\n:<math> J = J_1(1) \\oplus J_1(2) \\oplus J_2(4) = \n\\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 2 & 0 & 0 \\\\ 0 & 0 & 4 & 1 \\\\ 0 & 0 & 0 & 4 \\end{bmatrix}. </math>\nThere are three chains. Two have length one: {''v''} and {''w''}, corresponding to the eigenvalues 1 and 2, respectively. There is one chain of length two corresponding to the eigenvalue 4. To find this chain, calculate\n: <math>\\ker{(A-4I)}^2 = \\operatorname{span} \\, \\left\\{ \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ 0 \\\\ -1 \\\\ 1 \\end{bmatrix} \\right\\}</math>\n \nwhere ''I'' is the <!-- <math>4\\times 4</math> -->4 x 4 identity matrix. Pick a vector in the above span that is not in the kernel of ''A''&nbsp;&minus;&nbsp;4''I'', e.g., ''y'' = (1,0,0,0)<sup>T</sup>. Now, (''A''&nbsp;&minus;&nbsp;4''I'')''y'' = ''x'' and (''A''&nbsp;&minus;&nbsp;4''I'')''x'' = 0, so {''y'', ''x''} is a chain of length two corresponding to the eigenvalue 4.\n\nThe transition matrix ''P'' such that ''P''<sup>&minus;1</sup>''AP'' = ''J'' is formed by putting these vectors next to each other as follows\n:<math> P = \\Big[ \\,v\\, \\Big| \\,w\\, \\Big| \\,x\\, \\Big| \\,y\\, \\Big] = \n\\begin{bmatrix}\n-1 &  1 &  1 &  1 \\\\\n 1 & -1 &  0 &  0 \\\\ \n 0 &  0 & -1 &  0 \\\\\n 0 &  1 &  1 &  0\n\\end{bmatrix}. </math>\nA computation shows that the equation ''P''<sup>&minus;1</sup>''AP'' = ''J'' indeed holds.\n\n:<math>P^{-1}AP=J=\\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 2 & 0 & 0 \\\\\n0 & 0 & 4 & 1 \\\\\n0 & 0 & 0 & 4 \\end{bmatrix}.</math>\n\nIf we had interchanged the order in which the chain vectors appeared, that is, changing the order of ''v'', ''w'' and {''x'', ''y''} together, the Jordan blocks would be interchanged. However, the Jordan forms are equivalent Jordan forms.\n\n== Generalized eigenvectors ==\n{{main|Generalized eigenvector}}\nGiven an eigenvalue λ, its corresponding Jordan block gives rise to a '''[[Jordan chain]]'''. The '''generator''', or '''lead vector''', say ''p<sub>r</sub>'', of the chain is a generalized eigenvector such that (''A'' &minus; λ '''I''')<sup>''r''</sup>''p''<sub>''r''</sub> = 0, where ''r'' is the size of the Jordan block. The vector ''p''<sub>1</sub> =  (''A'' &minus; λ '''I''')<sup>''r''&minus;1</sup>''p''<sub>''r''</sub> is an eigenvector corresponding to λ. In general, ''p''<sub>''i''</sub> is a preimage of ''p''<sub>''i''&minus;1</sub> under ''A'' &minus; λ '''I'''. So the lead vector generates the chain via multiplication by (''A'' &minus; λ '''I''').<ref>{{harvtxt|Bronson|1970|pp=189,194}}</ref><ref name=\"Holt 2009 9\" />\n\nTherefore, the statement that every square matrix ''A'' can be put in Jordan normal form is equivalent to the claim that there exists a basis consisting only of eigenvectors and generalized eigenvectors of ''A''.\n\n=== A proof ===\nWe give a [[proof by induction]] that any complex-valued matrix A may be put in Jordan normal form.{{Citation needed|date=October 2017}} The 1 &times; 1 case is trivial. Let ''A'' be an ''n'' &times; ''n'' matrix. Take any [[eigenvalue]] λ of ''A''. The [[range (mathematics)|range]] of ''A'' &minus; λ '''I''', denoted by Ran(''A'' &minus; λ '''I'''), is an [[invariant subspace]] of ''A''. Also, since λ is an eigenvalue of ''A'', the dimension of Ran(''A'' &minus; λ '''I'''), ''r'', is strictly less than ''n''. Let ''A' '' denote the restriction of ''A'' to Ran(''A'' &minus; λ '''I'''), by inductive hypothesis, there exists a [[basis (linear algebra)|basis]] {''p''<sub>1</sub>, ..., ''p''<sub>''r''</sub>} such that ''A' '', expressed with respect to this basis, is in Jordan normal form.\n\nNext consider the [[kernel (linear algebra)|kernel]], that is, the [[linear subspace|subspace]] Ker(''A'' &minus; λ '''I'''). If\n\n:<math>\\mathrm{Ran}(A - \\lambda I) \\cap \\mathrm{Ker}(A - \\lambda I) = \\{0\\},</math>\n\nthe desired result follows immediately from the [[rank–nullity theorem]]. This would be the case, for example, if ''A'' was [[Hermitian]].\n\nOtherwise, if\n\n:<math>Q = \\mathrm{Ran}(A - \\lambda I) \\cap \\mathrm{Ker}(A - \\lambda I) \\neq \\{0\\},</math>\n\nlet the dimension of ''Q'' be ''s'' ≤ ''r''. Each vector in ''Q'' is an eigenvector of ''A' '' corresponding to eigenvalue ''λ''. So the Jordan form of ''A' '' must contain ''s'' Jordan chains corresponding to ''s'' linearly independent eigenvectors. So the basis {''p''<sub>1</sub>, ..., ''p''<sub>''r''</sub>} must contain ''s'' vectors, say {''p''<sub>''r''&minus;''s''+1</sub>, ..., ''p''<sub>''r''</sub>}, that are lead vectors in these Jordan chains from the Jordan normal form of ''A'''. We can \"extend the chains\" by taking the preimages of these lead vectors. (This is the key step of argument; in general, generalized eigenvectors need not lie in Ran(''A'' &minus; λ '''I''').) Let ''q''<sub>''i''</sub> be such that\n\n:<math>\\; (A - \\lambda I) q_i = p_i \\mbox{ for } i = r-s+1, \\ldots, r.</math>\n\nClearly no non-trivial linear combination of the ''q''<sub>''i''</sub> can lie in Ker(''A'' &minus; λ I). Furthermore, no non-trivial linear combination of the ''q''<sub>''i''</sub> can be in Ran(''A'' &minus; λ '''I'''), for that would contradict the assumption that each ''p<sub>i</sub>'' is a lead vector in a Jordan chain. The set {''q''<sub>''i''</sub>}, being preimages of the linearly independent set {''p''<sub>''i''</sub>} under ''A'' &minus; λ '''I''', is also linearly independent.\n\nFinally, we can pick any linearly independent set {''z''<sub>''1''</sub>, ..., ''z''<sub>''t''</sub>} that spans\n\n:<math>\\; \\mathrm{Ker}(A - \\lambda I) / Q.</math>\n\nBy construction, the union of the three sets {''p''<sub>1</sub>, ..., ''p''<sub>''r''</sub>}, {''q''<sub>''r''&minus;''s'' +1</sub>, ..., ''q''<sub>''r''</sub>}, and  {''z''<sub>1</sub>, ..., ''z''<sub>''t''</sub>} is linearly independent. Each vector in the union is either an eigenvector or a generalized eigenvector of ''A''. Finally, by the rank–nullity theorem, the cardinality of the union is ''n''. In other words, we have found a basis that consists of eigenvectors and generalized eigenvectors of ''A'', and this shows ''A'' can be put in Jordan normal form.\n\n=== Uniqueness ===\n\nIt can be shown that the Jordan normal form of a given matrix ''A'' is unique up to the order of the Jordan blocks.\n\nKnowing the algebraic and geometric multiplicities of the eigenvalues is not sufficient to determine the Jordan normal form of ''A''. Assuming the algebraic multiplicity ''m''(λ) of an eigenvalue λ is known, the structure of the Jordan form can be ascertained by analyzing the ranks of the powers (''A'' &minus; λ I)<sup>''m''(λ)</sup>. To see this, suppose an ''n'' &times; ''n'' matrix ''A'' has only one eigenvalue λ. So ''m''(λ) = ''n''. The smallest integer ''k''<sub>1</sub> such that\n\n:<math>(A - \\lambda I)^{k_1} = 0</math>\n\nis the size of the largest Jordan block in the Jordan form of ''A''. (This number ''k''<sub>1</sub> is also called the '''index''' of λ. See discussion in a following section.) The rank of\n\n:<math>(A - \\lambda I)^{k_1 - 1}</math>\n\nis the number of Jordan blocks of size ''k''<sub>1</sub>. Similarly, the rank of\n\n:<math>(A - \\lambda I)^{k_1 - 2}</math>\n\nis twice the number of Jordan blocks of size ''k''<sub>1</sub> plus the number of Jordan blocks of size ''k''<sub>1</sub>&minus;1. The general case is similar.\n\nThis can be used to show the uniqueness of the Jordan form. Let ''J''<sub>1</sub> and ''J''<sub>2</sub> be two Jordan normal forms of ''A''. Then ''J''<sub>1</sub> and ''J''<sub>2</sub> are similar and have the same spectrum, including algebraic multiplicities of the eigenvalues. The procedure outlined in the previous paragraph can be used to determine the structure of these matrices. Since the rank of a matrix is preserved by similarity transformation, there is a bijection between the Jordan blocks of ''J''<sub>1</sub> and ''J''<sub>2</sub>. This proves the uniqueness part of the statement.\n\n== Real matrices ==\nIf ''A'' is a real matrix, its Jordan form can still be non-real. Instead of representing it with complex eigenvalues and 1's on the superdiagonal, as discussed above, there exists a real invertible matrix ''P'' such that ''P<sup>−1</sup>AP'' = ''J'' is a real [[block diagonal matrix]] with each block being a real Jordan block.<ref>{{harvtxt|Horn|Johnson|1985|loc=Theorem 3.4.5}}</ref> A real Jordan block is either identical to a complex Jordan block (if the corresponding eigenvalue <math>\\lambda_i</math> is real), or is a block matrix itself, consisting of 2&times;2 blocks (for non-real eigenvalue <math>\\lambda_i = a_i+ib_i</math> with given algebraic multiplicity) of the form\n\n:<math>C_i = \n\\begin{bmatrix}\na_i  & -b_i \\\\\nb_i & a_i \\\\ \n\\end{bmatrix}</math>\n\nand describe multiplication by <math>\\lambda_i</math> in the complex plane. The superdiagonal blocks are 2&times;2 identity matrices and hence in this representation the matrix dimensions are larger than the complex Jordan form. The full real Jordan block is given by\n\n:<math>J_i = \n\\begin{bmatrix}\nC_i    & I       & \\;     & \\;    \\\\\n\\;     & C_i     & \\ddots & \\;    \\\\     \n\\;     & \\;      & \\ddots & I     \\\\\n\\;     & \\;      & \\;     & C_i   \\\\\n\\end{bmatrix}.</math>\n\nThis real Jordan form is a consequence of the complex Jordan form. For a real matrix the nonreal eigenvectors and generalized eigenvectors can always be chosen to form [[complex conjugate]] pairs. Taking the real and imaginary part (linear combination of the vector and its conjugate), the matrix has this form with respect to the new basis.\n\n== Matrices with entries in a field ==\n\nJordan reduction can be extended to any square matrix ''M'' whose entries lie in a [[field (mathematics)|field]] ''K''.  The result states that any ''M'' can be written as a sum ''D'' + ''N'' where ''D'' is [[semisimple operator|semisimple]], ''N'' is [[nilpotent matrix|nilpotent]], and ''DN'' = ''ND''. This is called the [[Jordan–Chevalley decomposition]]. Whenever ''K'' contains the eigenvalues of ''M'', in particular when ''K'' is [[algebraically closed]], the normal form can be expressed explicitly as the [[direct sum]] of Jordan blocks. \n \nSimilar to the case when ''K'' is the complex numbers, knowing the dimensions of the kernels of (''M'' &minus; λ''I'')<sup>''k''</sup> for 1 ≤ ''k'' ≤ ''m'', where ''m'' is the [[algebraic multiplicity]] of the eigenvalue λ, allows one to determine the Jordan form of ''M''. We may view the underlying vector space ''V'' as a ''K''[''x'']-[[module (mathematics)|module]] by regarding the action of ''x'' on ''V'' as application of ''M'' and extending by ''K''-linearity. Then the polynomials (''x''&nbsp;&minus;&nbsp;λ)<sup>''k''</sup> are the elementary divisors of ''M'', and the Jordan normal form is concerned with representing ''M'' in terms of blocks associated to the elementary divisors.\n\nThe proof of the Jordan normal form is usually carried out as an application to the [[ring (mathematics)|ring]] ''K''[''x''] of the [[structure theorem for finitely generated modules over a principal ideal domain]], of which it is a corollary.\n\n== Consequences ==\n\nOne can see that the Jordan normal form is essentially a classification result for square matrices, and as such several important results from linear algebra can be viewed as its consequences.\n\n=== Spectral mapping theorem ===\n\nUsing the Jordan normal form, direct calculation gives a spectral mapping theorem for the [[functional calculus|polynomial functional calculus]]: Let ''A'' be an ''n'' &times; ''n'' matrix with eigenvalues λ<sub>1</sub>, ..., λ<sub>''n''</sub>, then for any polynomial ''p'', ''p''(''A'') has eigenvalues ''p''(λ<sub>1</sub>), ..., ''p''(λ<sub>''n''</sub>).\n\n=== Cayley–Hamilton theorem ===\n\nThe [[Cayley–Hamilton theorem]] asserts that every matrix ''A'' satisfies its characteristic equation: if {{math|''p''}} is the [[characteristic polynomial]] of {{math|''A''}}, then {{math|''p''(''A'') {{=}} 0}}. This can be shown via direct calculation in the Jordan form, since any Jordan block for {{math|''&lambda;''}} is annihilated by {{math|(''X'' − &lambda;)<sup>''m''</sup>}} where {{math|''m''}} is the multiplicity of the root {{math|''&lambda;''}} of {{math|''p''}}, the sum of the sizes of the Jordan blocks for {{math|''&lambda;''}}, and therefore no less than the size of the block in question. The Jordan form can be assumed to exist over a field extending the base field of the matrix, for instance over the [[splitting field]] of {{math|''p''}}; this field extension does not change the matrix {{math|''p''(''A'')}} in any way.\n\n=== Minimal polynomial ===\n\nThe [[Minimal polynomial (linear algebra)|minimal polynomial]] P of a square matrix ''A'' is the unique [[monic polynomial]] of least degree, ''m'', such that ''P''(''A'') = 0. Alternatively, the set of polynomials that annihilate a given ''A'' form an ideal ''I'' in ''C''[''x''], the [[principal ideal domain]] of polynomials with complex coefficients. The monic element that generates ''I'' is precisely ''P''.\n\nLet λ<sub>1</sub>, ..., λ<sub>''q''</sub> be the distinct eigenvalues of ''A'', and ''s''<sub>''i''</sub> be the size of the largest Jordan block corresponding to λ<sub>''i''</sub>. It is clear from the Jordan normal form that the minimal polynomial of ''A'' has degree {{math|''&Sigma;''}}''s''<sub>''i''</sub>.\n\nWhile the Jordan normal form determines the minimal polynomial, the converse is not true. This leads to the notion of '''elementary divisors'''. The elementary divisors of a square matrix ''A'' are the characteristic polynomials of its Jordan blocks. The factors of the minimal polynomial ''m'' are the elementary divisors of the largest degree corresponding to distinct eigenvalues.\n\nThe degree of an elementary divisor is the size of the corresponding Jordan block, therefore the dimension of the corresponding invariant subspace. If all elementary divisors are linear, ''A'' is diagonalizable.\n\n=== Invariant subspace decompositions ===\n\nThe Jordan form of a ''n'' &times; ''n'' matrix ''A'' is block diagonal, and therefore gives a decomposition of the ''n'' dimensional Euclidean space into [[invariant subspace]]s of ''A''. Every Jordan block ''J''<sub>''i''</sub> corresponds to an invariant subspace ''X''<sub>''i''</sub>. Symbolically, we put\n\n:<math>\\mathbb{C}^n = \\bigoplus_{i = 1}^k X_i</math>\n\nwhere each ''X''<sub>''i''</sub> is the span of the corresponding Jordan chain, and ''k'' is the number of Jordan chains.\n\nOne can also obtain a slightly different decomposition via the Jordan form. Given an eigenvalue λ<sub>''i''</sub>, the size of its largest corresponding Jordan block ''s''<sub>i</sub> is called the '''index''' of  λ<sub>''i''</sub> and denoted by ν(λ<sub>''i''</sub>). (Therefore, the degree of the minimal polynomial is the sum of all indices.) Define a subspace ''Y''<sub>''i''</sub> by\n\n:<math>\\; Y_i = \\operatorname{Ker} (\\lambda_i I - A)^{\\nu(\\lambda_i)}.</math>\n\nThis gives the decomposition\n\n:<math>\\mathbb{C}^n = \\bigoplus_{i = 1}^l Y_i</math>\n\nwhere ''l'' is the number of distinct eigenvalues of ''A''. Intuitively, we glob together the Jordan block invariant subspaces corresponding to the same eigenvalue. In the extreme case where ''A'' is a multiple of the identity matrix we have ''k'' = ''n'' and ''l'' = 1.\n\nThe projection onto ''Y<sub>i</sub>'' and along all the other ''Y<sub>j</sub>'' ( ''j'' ≠ ''i'' ) is called '''the spectral projection of ''A'' at λ<sub>''i''</sub>''' and is usually denoted by '''''P''(λ<sub>''i''</sub> ; ''A'')'''. Spectral projections are mutually orthogonal in the sense that ''P''(λ<sub>''i''</sub> ; ''A'') ''P''(λ<sub>''j''</sub> ; ''A'') = 0 if ''i'' ≠ ''j''. Also they commute with ''A'' and their sum is the identity matrix. Replacing every λ<sub>''i''</sub> in the Jordan matrix ''J'' by one and zeroising all other entries gives ''P''(λ<sub>''i''</sub> ; ''J''), moreover if ''U J U''<sup>−1</sup> is the similarity transformation such that ''A'' = ''U J U''<sup>−1</sup> then ''P''(λ<sub>''i''</sub> ; ''A'') = ''U P''(λ<sub>''i''</sub> ; ''J'') ''U''<sup>−1</sup>. They are not confined to finite dimensions. See below for their application to compact operators, and in [[holomorphic functional calculus]] for a more general discussion.\n\nComparing the two decompositions, notice that, in general, ''l'' ≤ ''k''. When ''A'' is normal, the subspaces ''X''<sub>''i''</sub>'s in the first decomposition are one-dimensional and mutually orthogonal. This is the [[spectral theorem]] for normal operators. The second decomposition generalizes more easily for general compact operators on Banach spaces.\n\nIt might be of interest here to note some properties of the index, ν(''λ''). More generally, for a complex number λ, its index can be defined as the least non-negative integer ν(λ) such that\n\n:<math>\\mathrm{Ker}(\\lambda - A)^{\\nu(\\lambda)} = \\operatorname{Ker} (\\lambda - A)^m, \\; \\forall m \\geq \\nu(\\lambda) .</math>\n\nSo ν(λ) &gt; 0 if and only if λ is an eigenvalue of ''A''. In the finite-dimensional case, ν(λ) ≤ the algebraic multiplicity of λ.\n\n===Plane (flat) normal form===\n\nThe Jordan form is used to find a normal form of matrices up to conjugacy such that normal matrices make up\nan algebraic variety of a low fixed degree in the ambient matrix space. \n\nSets of representatives of matrix conjugacy\nclasses for Jordan normal form or rational canonical forms in general do not constitute linear or \naffine subspaces in the ambient matrix spaces.\n\n[[Vladimir Arnold]] posed a problem in <ref>{{Cite book | author = Vladimir I. Arnold (Ed.) |date=2004 | \ntitle = Arnold's problems| doi = 10.1007/b138219 | isbn = 978-3-540-20748-1 |page=127 |publisher = Springer-Verlag Berlin Heidelberg}}</ref> - \nfind a canonical form of matrices over a field for which the set of representatives of \nmatrix conjugacy classes is a union of affine linear subspaces (flats). In other words, map the set of matrix conjugacy  classes injectively back \ninto the intitial set of matrices so that the image of this embedding - the set of all normal matrices, \nhas the lowest possible degree – it is a union of shifted linear subspaces.\n\nIt was solved for \nalgebraically closed fields by Peteris Daugulis \n<ref name=\"originalpaper\">{{cite journal | author = Peteris Daugulis.\n |date=2012 | title = A parametrization of matrix conjugacy orbit sets as unions of affine planes| \npage = 709–721 | journal = Linear Algebra and its Applications | volume = 436 | issue = 3 |  \ndoi = 10.1016/j.laa.2011.07.032 |arxiv = 1110.0907 }}</ref>. \nThe construction of a uniquely defined '''plane normal form''' of \na matrix starts by considering its Jordan normal form.\n\n== Matrix functions ==\n{{Main|Matrix function}}\nIteration of the Jordan chain motivates various extensions to more abstract settings. For finite matrices, once gets matrix functions; this can be extended to compact operators and the holomorphic functional calculus, as described further below.\n\nThe Jordan normal form is the most convenient for computation of the matrix functions (though it may be not the best choice for computer computations). Let ''f(z)'' be an analytical function of a complex argument. Applying the function on a ''n×n'' Jordan block ''J'' with eigenvalue ''λ'' results in an upper triangular matrix:\n\n:<math>\nf(J)\n=\\begin{bmatrix}\n f(\\lambda) & f'(\\lambda) & \\tfrac{f''(\\lambda)}{2} & ...   &  \\tfrac{f^{(n-1)}(\\lambda)}{(n-1)!}\\\\\n 0  & f(\\lambda) & f'(\\lambda) & ...   & \\tfrac{f^{(n-2)}(\\lambda)}{(n-2)!} \\\\\n \\vdots  & \\vdots  & \\ddots & \\ddots   & \\vdots \\\\ \n 0  & 0  & 0  & f(\\lambda) & f'(\\lambda) \\\\\n 0  & 0  & 0  & 0   & f(\\lambda)\n\\end{bmatrix},</math>\n\nso that the elements of the ''k''-th superdiagonal of the resulting matrix are <math>\\tfrac{f^{(k)}(\\lambda)}{k!}</math>. For a matrix of general Jordan normal form\nthe above expression shall be applied to each Jordan block.\n\nThe following example shows the application to the power function ''f(z)=z<sup>n</sup>'':\n:<math>\n\\begin{bmatrix}\n \\lambda_1 & 1 & 0 & 0 & 0 \\\\\n 0 & \\lambda_1 & 1 & 0 & 0 \\\\\n 0 & 0 & \\lambda_1 & 0 & 0 \\\\ \n 0 & 0 & 0 & \\lambda_2 & 1 \\\\\n 0 & 0 & 0 & 0 & \\lambda_2\n\\end{bmatrix}^n\n=\\begin{bmatrix}\n \\lambda_1^n & \\tbinom{n}{1}\\lambda_1^{n-1} & \\tbinom{n}{2}\\lambda_1^{n-2} & 0   & 0 \\\\\n 0  & \\lambda_1^n & \\tbinom{n}{1}\\lambda_1^{n-1} & 0   & 0 \\\\\n 0  & 0  & \\lambda_1^n & 0   & 0 \\\\ \n 0  & 0  & 0  & \\lambda_2^n & \\tbinom{n}{1}\\lambda_2^{n-1} \\\\\n 0  & 0  & 0  & 0   & \\lambda_2^n\n\\end{bmatrix},</math>\nwhere the binomial coefficients are defined as <math>\\tbinom{n}{k}=\\prod_{i=1}^k \\tfrac{n+1-i}{i}</math>. For integer positive ''n'' it reduces to standard definition\nof the coefficients. For negative ''n'' the identity <math>\\tbinom{-n}{k}=\\left(-1\\right)^k\\tbinom{n+k-1}{k}</math> may be of use.\n\n== Compact operators ==\nA result analogous to the Jordan normal form holds for [[compact operator]]s on a [[Banach space]]. One restricts to compact operators because every point ''x'' in the spectrum of a compact operator ''T'', the only exception being when ''x'' is the limit point of the spectrum, is an eigenvalue. This is not true for bounded operators in general. To give some idea of this generalization, we first reformulate the Jordan decomposition in the language of functional analysis.\n\n=== Holomorphic functional calculus ===\n{{ Details|holomorphic functional calculus}}\nLet ''X'' be a Banach space, ''L''(''X'') be the bounded operators on ''X'', and σ(''T'') denote the [[spectrum (functional analysis)|spectrum]] of ''T'' ∈ ''L''(''X''). The [[holomorphic functional calculus]] is defined as follows:\n\nFix a bounded operator ''T''. Consider the family Hol(''T'') of complex functions that is [[Holomorphic function|holomorphic]] on some open set ''G'' containing σ(''T''). Let Γ = {γ<sub>''i''</sub>} be a finite collection of [[Jordan curve]]s such that σ(''T'') lies in the ''inside'' of Γ, we define ''f''(''T'') by\n\n: <math>f(T) = \\frac{1}{2 \\pi i} \\int_{\\Gamma} f(z)(z - T)^{-1} dz.</math>\n\nThe open set ''G'' could vary with ''f'' and need not be connected. The integral is defined as the limit of the Riemann sums, as in the scalar case. Although the integral makes sense for continuous ''f'', we restrict to holomorphic functions to apply the machinery from classical function theory (e.g., the Cauchy integral formula). The assumption that σ(''T'') lie in the inside of Γ ensures ''f''(''T'') is well defined; it does not depend on the choice of Γ. The functional calculus is the mapping Φ from Hol(''T'') to ''L''(''X'') given by\n\n: <math>\\; \\Phi(f) = f(T).</math>\n\nWe will require the following properties of this functional calculus:\n# Φ extends the polynomial functional calculus.\n# The ''spectral mapping theorem'' holds: σ(''f''(''T'')) = ''f''(σ(''T'')).\n# Φ is an algebra homomorphism.\n\n=== The finite-dimensional case ===\n\nIn the finite-dimensional case, σ(''T'') = {λ<sub>''i''</sub>} is a finite discrete set in the complex plane. Let ''e''<sub>''i''</sub> be the function that is 1 in some open neighborhood of λ<sub>''i''</sub> and 0 elsewhere. By property 3 of the functional calculus, the operator\n\n:<math>\\; e_i(T)</math>\n\nis a projection. Moreoever, let ν<sub>''i''</sub> be the index of λ<sub>''i''</sub> and\n\n:<math>f(z)= (z - \\lambda_i)^{\\nu_i}.</math>\n\nThe spectral mapping theorem tells us\n\n:<math> f(T) e_i (T) = (T - \\lambda_i)^{\\nu_i} e_i (T)</math>\n\nhas spectrum {0}. By property 1, ''f''(''T'') can be directly computed in the Jordan form, and by inspection, we see that the operator ''f''(''T'')''e<sub>i</sub>''(''T'') is the zero matrix.\n\nBy property 3, ''f''(''T'') ''e''<sub>''i''</sub>(''T'') = ''e''<sub>''i''</sub>(''T'') ''f''(''T''). So ''e''<sub>''i''</sub>(''T'') is precisely the projection onto\nthe subspace\n\n:<math>\\mathrm{Ran} \\; e_i (T) = \\mathrm{Ker}(T - \\lambda_i)^{\\nu_i}.</math>\n\nThe relation\n\n:<math>\\; \\sum_i e_i = 1</math>\n\nimplies\n\n:<math>\\mathbb{C}^n = \\bigoplus_i \\; \\mathrm{Ran}\\; e_i (T) = \\bigoplus_i \\; \\mathrm{Ker}(T - \\lambda_i)^{\\nu_i}</math>\n\nwhere the index ''i'' runs through the distinct eigenvalues of ''T''. This is exactly the invariant subspace decomposition\n\n:<math>\\mathbb{C}^n = \\bigoplus_i Y_i</math>\n\ngiven in a previous section. Each ''e<sub>i</sub>''(''T'') is the projection onto the subspace spanned by the Jordan chains corresponding to λ<sub>''i''</sub> and along the subspaces spanned by the Jordan chains corresponding to λ<sub>''j''</sub> for ''j'' ≠ ''i''. In other words, ''e<sub>i</sub>''(''T'') = ''P''(λ<sub>''i''</sub>;''T''). This explicit identification of the operators ''e<sub>i</sub>''(''T'') in turn gives an explicit form of holomorphic functional calculus for matrices:\n\n:For all ''f'' ∈ Hol(''T''),\n\n:<math>f(T) = \\sum_{\\lambda_i \\in \\sigma(T)} \\sum_{k = 0}^{\\nu_i -1} \\frac{f^{(k)}}{k!} (T - \\lambda_i)^k e_i (T).</math>\n\nNotice that the expression of ''f''(''T'') is a finite sum because, on each neighborhood of λ<sub>''i''</sub>, we have chosen the Taylor series expansion of ''f'' centered at λ<sub>''i''</sub>.\n\n=== Poles of an operator ===\n\nLet ''T'' be a bounded operator λ be an isolated point of σ(''T''). (As stated above, when ''T'' is compact, every point in its spectrum is an isolated point, except possibly the limit point 0.)\n\nThe point λ is called a '''pole''' of operator ''T'' with order ν if the [[Resolvent formalism|resolvent]] function ''R''<sub>''T''</sub> defined by\n\n:<math>\\; R_T(\\lambda) = (\\lambda - T)^{-1}</math>\n\nhas a [[pole (complex analysis)|pole]] of order ν at λ.\n\nWe will show that, in the finite-dimensional case, the order of an eigenvalue coincides with its index. The result also holds for compact operators.\n\nConsider the annular region ''A'' centered at the eigenvalue λ with sufficiently small radius ε such that the intersection of the open disc ''B''<sub>ε</sub>(λ) and σ(''T'') is {λ}. The resolvent function ''R''<sub>''T''</sub> is holomorphic on ''A''.\nExtending a result from classical function theory, ''R''<sub>''T''</sub> has a [[Laurent series]] representation on ''A'':\n\n:<math>R_T(z) = \\sum _{- \\infty} ^{\\infty} a_m (\\lambda - z)^m</math>\n\nwhere\n\n:<math>a_{-m} = - \\frac{1}{2 \\pi i} \\int_C (\\lambda - z) ^{m-1} (z - T)^{-1} d z</math> and ''C'' is a small circle centered at λ.\n\nBy the previous discussion on the functional calculus,\n\n:<math>\\; a_{-m} = -(\\lambda - T)^{m-1} e_{\\lambda} (T)</math> where <math>\\; e_{\\lambda}</math> is 1 on <math>\\; B_{\\epsilon}(\\lambda)</math> and 0 elsewhere.\n\nBut we have shown that the smallest positive integer ''m'' such that\n\n:<math>a_{-m} \\neq 0</math> and <math>a_{-l} = 0 \\; \\; \\forall \\; l \\geq m</math>\n\nis precisely the index of λ, ν(λ). In other words, the function ''R''<sub>''T''</sub> has a pole of order ν(λ) at λ.\n\n== Numerical analysis ==\n\nIf the matrix ''A'' has multiple eigenvalues, or is close to a matrix with multiple eigenvalues, then its Jordan normal form is very sensitive to perturbations. Consider for instance the matrix\n:<math> A = \\begin{bmatrix} 1 & 1 \\\\ \\varepsilon & 1 \\end{bmatrix}. </math>\nIf ε = 0, then the Jordan normal form is simply\n:<math> \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}. </math>\nHowever, for ε ≠ 0, the Jordan normal form is\n:<math> \\begin{bmatrix} 1+\\sqrt\\varepsilon & 0 \\\\ 0 & 1-\\sqrt\\varepsilon \\end{bmatrix}. </math>\nThis [[condition number|ill conditioning]] makes it very hard to develop a robust numerical algorithm for the Jordan normal form, as the result depends critically on whether two eigenvalues are deemed to be equal. For this reason, the Jordan normal form is usually avoided in [[numerical analysis]]; the stable [[Schur decomposition]]<ref>See Golub & Van Loan (2014), §7.6.5; or Golub & Wilkinson (1976) for details.</ref> or [[pseudospectrum|pseudospectra]]<ref>See Golub & Van Loan (2014), §7.9</ref> are better alternatives.\n\n== See also ==\n* [[Canonical basis]]\n* [[Canonical form]]\n* [[Frobenius normal form]]\n* [[Jordan matrix]]\n* [[Jordan–Chevalley decomposition]]\n* [[Matrix decomposition]]\n* [[Modal matrix]]\n* [[Weyr canonical form]]\n\n== Notes ==\n<references/>\n\n==References==\n{{refbegin}}\n* {{ citation | first1 = Raymond A. | last1 = Beauregard | first2 = John B. | last2 = Fraleigh | year = 1973 | isbn = 0-395-14017-X | title = A First Course In Linear Algebra: with Optional Introduction to Groups, Rings, and Fields | publisher = [[Houghton Mifflin Co.]] | location = Boston }}\n* {{ citation | first1 = Richard | last1 = Bronson | year = 1970 | lccn = 70097490 | title = Matrix Methods:  An Introduction | publisher = [[Academic Press]] | location = New York }}\n* {{ citation | first1 = Charles G. | last1 = Cullen | title = Matrices and Linear Transformations | location = Reading | publisher = [[Addison-Wesley]] | year = 1966 | lccn = 66021267 }}\n* {{ citation | first1 = N. | last1 = Dunford | first2 = J. T. | last2 = Schwartz | title = Linear Operators, Part I: General Theory | publisher = [[Interscience]] | year = 1958 }}\n* {{ citation | first1 = Daniel T. | last1 = Finkbeiner II | title = Introduction to Matrices and Linear Transformations | edition = 3rd | publisher = [[W. H. Freeman and Company]] | year = 1978 }}\n* {{ citation | first1 = Joel N. | last1 = Franklin | title = Matrix Theory | location = Englewood Cliffs | publisher = [[Prentice-Hall]] | year = 1968 | lccn = 68016345 }}\n* {{ citation | first1 = Gene H. | last1 = Golub | first2 = Charles F. | last2 = Van Loan | year = 1996 | isbn = 0-8018-5414-8 | title = Matrix Computations | edition = 3rd | publisher = [[Johns Hopkins University Press]] | location = Baltimore }}\n* {{ cite journal | first1 = Gene H. | last1 = Golub | first2 = J. H. | last2 = Wilkinson | title = Ill-conditioned eigensystems and the computation of the Jordan normal form | journal = [[SIAM Review]] | volume = 18 | issue = 4 | pages = 578–619 | year = 1976 }}\n* {{ citation | last1 = Holt | first1 = Derek | last2 = Rumynin | first2 = Dmitriy | title = Algebra I – Advanced Linear Algebra (MA251) Lecture Notes | year = 2009 | url = http://homepages.warwick.ac.uk/~masdf/alg1/lec_notes_revised_at_the_end.pdf }}\n* {{ citation | last1=Horn | first1=Roger A. | last2=Johnson | first2=Charles R. | title=Matrix Analysis | publisher=[[Cambridge University Press]] | isbn=978-0-521-38632-6 | year=1985}}\n* {{ citation | first1 = Glenn | last1 = James | first2 = Robert C. | last2 = James | title = Mathematics Dictionary | edition = 2nd | publisher = [[Van Nostrand Reinhold]] | year = 1976 }}\n* {{ citation | first1 = Saunders | last1 = MacLane | first2 = Garrett | last2 = Birkhoff | title = Algebra | publisher = [[Macmillan Publishers]] | year = 1967 }}\n* {{ citation | first1 = Anthony N. | last1 = Michel | first2 = Charles J. | last2 = Herget | title = Applied Algebra and Functional Analysis | publisher = [[Dover Publications]] | year = 1993 }}\n* {{ citation | first1 = Evar D. | last1 = Nering | year = 1970 | title = Linear Algebra and Matrix Theory | edition = 2nd | publisher = [[John Wiley & Sons|Wiley]] | location = New York | lccn = 76091646 }}\n* {{ citation | first1 = I. R. | last1 = Shafarevich | first2 = A. O. | last2 = Remizov | title = Linear Algebra and Geometry | publisher = [[Springer Science+Business Media|Springer]] | year = 2012 | ISBN = 978-3-642-30993-9 }}\n* {{ citation | first1 = Georgi E. | last1 = Shilov | title = Linear Algebra | publisher = [[Dover Publications]] | year = 1977 }}\n* [http://mathworld.wolfram.com/JordanCanonicalForm.html ''Jordan Canonical Form'' article at mathworld.wolfram.com]\n\n{{refend}}\n\n[[Category:Linear algebra]]\n[[Category:Matrix theory]]\n[[Category:Matrix normal forms]]\n[[Category:Matrix decompositions]]"
    },
    {
      "title": "Lifting scheme",
      "url": "https://en.wikipedia.org/wiki/Lifting_scheme",
      "text": "[[Image:Second_generation_wavelet_transform.svg|right|300px|thumb|Lifting sequence consisting of two steps]]\nThe '''lifting scheme''' is a technique for both designing [[wavelet]]s and performing the [[discrete wavelet transform]] (DWT).\nIn an implementation, it is often worthwhile to merge these steps and design the wavelet filters ''while'' performing the wavelet transform.\nThis is then called the [[second-generation wavelet transform]].\nThe technique was introduced by [[Wim Sweldens]].<ref name=\"Sweldens1997\">{{cite journal|last=Sweldens|first=Wim|author-link=Wim Sweldens|year=1997|title=The Lifting Scheme: A Construction of Second Generation Wavelets|journal=Journal on Mathematical Analysis|volume=29|number=2|pages=511–546|doi=10.1137/S0036141095289051|url=https://cm-bell-labs.github.io/who/wim/papers/lift2.pdf}}</ref>\n\nThe lifting scheme factorizes any discrete wavelet transform with finite filters into a series of elementary convolution operators, so-called lifting steps, which reduces the number of arithmetic operations by nearly a factor two. Treatment of signal boundaries is also simplified.<ref name=\"Mallat2009\">{{cite book|last=Mallat|first=Stéphane|author-link=Stéphane Mallat|year=2009|isbn=978-0-12-374370-1|title=A Wavelet Tour of Signal Processing|publisher=Academic Press|url=https://wavelet-tour.github.io/}}</ref>\n\nThe discrete wavelet transform applies several filters separately to the same signal.\nIn contrast to that, for the lifting scheme, the signal is divided like a zipper.\nThen a series of [[multiply-accumulate|convolution-accumulate]] operations across the divided signals is applied.\n\n==Basics==\n\nThe simplest version of a forward wavelet transform expressed in the lifting scheme is shown in the figure above. <math>P</math> means predict step, which will be considered in isolation. The predict step calculates the wavelet function in the wavelet transform. This is a high-pass filter. The update step calculates the scaling function, which results in a smoother version of the data.\n\nAs mentioned above, the lifting scheme is an alternative technique for performing the DWT using biorthogonal wavelets. In order to perform the DWT using the lifting scheme, the corresponding lifting and scaling steps must be derived from the biorthogonal wavelets. The analysis filters (<math>g, h</math>) of the particular wavelet are first written in polyphase matrix\n: <math>P(z) = \\begin{bmatrix}\n h_\\text{even}(z) & g_\\text{even}(z) \\\\\n h_\\text{odd}(z)  & g_\\text{odd}(z)\n\\end{bmatrix},</math>\nwhere <math>\\det P(z) = z^{-m}</math>.\n\nThe polyphase matrix is a 2 × 2 matrix containing the analysis low-pass and high-pass filters, each split up into their even and odd polynomial coefficients and normalized. From here the matrix is factored into a series of 2 × 2 upper- and lower-triangular matrices, each with diagonal entries equal to 1. The upper-triangular matrices contain the coefficients for the predict steps, and the lower-triangular matrices contain the coefficients for the update steps. A matrix consisting of all zeros with the exception of the diagonal values may be extracted to derive the scaling-step coefficients. The polyphase matrix is factored into the form\n: <math>P(z) = \\begin{bmatrix}\n 1 & a(1 + z^{-1}) \\\\\n 0 & 1\n\\end{bmatrix} \\begin{bmatrix}\n 1        & 0 \\\\\n b(1 + z) & 1\n\\end{bmatrix},</math>\nwhere <math>a</math> is the coefficient for the predict step, and <math>b</math> is the coefficient for the update step.\n\nAn example of a more complicated extraction having multiple predict and update steps, as well as scaling steps, is shown below; <math>a</math> is the coefficient for the first predict step, <math>b</math> is the coefficient for the first update step, <math>c</math> is the coefficient for the second predict step, <math>d</math> is the coefficient for the second update step, <math>k_1</math> is the odd-sample scaling coefficient, and <math>k_2</math> is the even-sample scaling coefficient:\n: <math>P(z) = \\begin{bmatrix}\n 1 & a(1 + z^{-1}) \\\\\n 0 & 1\n\\end{bmatrix} \\begin{bmatrix}\n 1        & 0 \\\\\n b(1 + z) & 1\n\\end{bmatrix} \\begin{bmatrix}\n 1 & c(1 + z^{-1}) \\\\\n 0 & 1\n\\end{bmatrix} \\begin{bmatrix}\n 1        & 0 \\\\\n d(1 + z) & 1\n\\end{bmatrix} \\begin{bmatrix}\n k_1 & 0 \\\\\n 0   & k_2\n\\end{bmatrix}.</math>\n\nAccording to matrix theory, any matrix having polynomial entries and a determinant of 1 can be factored as described above. Therefore every wavelet transform with finite filters can be decomposed into a series of lifting and scaling steps. Daubechies and Sweldens discuss lifting-step extraction in further detail.<ref name=\"Daubechies1998\">{{cite journal|last1=Daubechies|first1=Ingrid|author-link1=Ingrid Daubechies|last2=Sweldens|first2=Wim|author-link2=Wim Sweldens|year=1998|title=Factoring Wavelet Transforms into Lifting Steps|journal=Journal of Fourier Analysis and Applications|volume=4|issue=3|pages=247–269|doi=10.1007/BF02476026|url=https://9p.io/who/wim/papers/factor/factor.pdf}}</ref>\n\n== CDF 9/7 filter ==\nTo perform the CDF 9/7 transform, a total of four lifting steps are required: two predict and two update steps.\nThe lifting factorization leads to the following sequence of filtering steps.<ref name=\"Daubechies1998\" />\n: <math>d_l = d_l + a (s_l + s_{l+1}),</math>\n: <math>s_l = s_l + b (d_l + d_{l-1}),</math>\n: <math>d_l = d_l + c (s_l + s_{l+1}),</math>\n: <math>s_l = s_l + d (d_l + d_{l-1}),</math>\n: <math>d_l = k_1 d_l,</math>\n: <math>s_l = k_2 s_l.</math>\n\n==Properties==\n\n=== Perfect reconstruction ===\nEvery transform by the lifting scheme can be inverted.\nEvery perfect-reconstruction filter bank can be decomposed into lifting steps by the [[Euclidean algorithm]].\nThat is, \"lifting-decomposable filter bank\" and \"perfect-reconstruction filter bank\" denotes the same.\nEvery two perfect-reconstruction filter banks can be transformed into each other by a sequence of lifting steps.\nFor a better understanding, if <math>P</math> and <math>Q</math> are [[Polyphase matrix|polyphase matrices]] with the same determinant, then the lifting sequence from <math>P</math> to <math>Q</math> is the same as the one from the lazy polyphase matrix <math>I</math> to <math>P^{-1}\\cdot Q</math>.\n\n=== Speedup ===\nSpeedup is by a factor of two. This is only possible because lifting is restricted to perfect-reconstruction filter banks. That is, lifting somehow squeezes out redundancies caused by perfect reconstruction.\n\nThe transformation can be performed immediately in the memory of the input data (in place, in situ) with only constant memory overhead.\n\n=== Non-linearities ===\nThe convolution operations can be replaced by any other operation. For perfect reconstruction only the invertibility of the addition operation is relevant. This way rounding errors in convolution can be tolerated and bit-exact reconstruction is possible. However, the numeric stability may be reduced by the non-linearities. This must be respected if the transformed signal is processed like in [[lossy compression]]. Although every reconstructable filter bank can be expressed in terms of lifting steps, a general description of the lifting steps is not obvious from a description of a wavelet family. However, for instance, for simple cases of the [[Cohen–Daubechies–Feauveau wavelet]], there is an explicit formula for their lifting steps.\n\n=== Increasing vanishing moments, stability, and regularity ===\nA lifting modifies biorthogonal filters in order to increase the number of vanishing moments of the resulting biorthogonal wavelets, and hopefully their stability and regularity. Increasing the number of vanishing moments decreases the amplitude of wavelet coefficients in regions where the signal is regular, which produces a more sparse representation. However, increasing the number of vanishing moments with a lifting also increases the wavelet support, which is an adverse effect that increases the number of large coefficients produced by isolated singularities. Each lifting step maintains the filter biorthogonality but provides no control on the Riesz bounds and thus on the stability of the resulting wavelet biorthogonal basis. When a basis is orthogonal then the dual basis is equal to the original basis. Having a dual basis that is similar to the original basis is, therefore, an indication of stability. As a result, stability is generally improved when dual wavelets have as much vanishing moments as original wavelets and a support of similar size. This is why a lifting procedure also increases the number of vanishing moments of dual wavelets. It can also improve the regularity of the dual wavelet. A lifting design is computed by adjusting the number of vanishing moments. The stability and regularity of the resulting biorthogonal wavelets are measured a posteriori, hoping for the best. This is the main weakness of this wavelet design procedure.\n\n==Generalized Lifting==\n{{main|Generalized Lifting}}\nThe [[Generalized lifting|generalized lifting scheme]] is a derivative of the lifting scheme, in which the addition and subtraction operations are absorbed into the update and prediction steps, respectively. These steps can be any (invertible) mapping, leading to a more general lifting scheme.\n\n==Applications==\n\n* Wavelet transforms that map integers to integers\n* Fourier transform with bit-exact reconstruction<ref name=\"Oraintara2002\">{{cite journal|first1=Soontorn|last1=Oraintara|first2=Ying-Jui|last2=Chen|first3=Truong Q.|last3=Nguyen|url=http://www-ee.uta.edu/msp/pub/Journaintfft.pdf|title=Integer Fast Fourier Transform|journal=Transactions on Signal Processing|volume=50|number=3|pages=607–618|year=2002|doi=10.1109/78.984749}}</ref>\n* Construction of wavelets with a required number of smoothness factors and vanishing moments\n* Construction of wavelets matched to a given pattern<ref name=\"Thielemann2004\">{{cite journal|first=Henning|last=Thielemann|title=Optimally matched wavelets|journal=Proceedings in Applied Mathematics and Mechanics|volume=4|pages=586–587|year=2004|doi=10.1002/pamm.200410274}}</ref>\n* Implementation of the [[discrete wavelet transform]] in [[JPEG 2000]]\n* Data-driven transforms, e.g., edge-avoiding wavelets<ref name=\"Fattal2009\">{{cite journal|first=Raanan|last=Fattal|url=http://www.cs.huji.ac.il/~raananf/projects/eaw/|title=Edge-Avoiding Wavelets and their Applications|journal=Transactions on Graphics|volume=28|pages=1|number=3|year=2009|doi=10.1145/1531326.1531328|citeseerx=10.1.1.205.8462}}</ref>\n* [[Non-separable wavelet|Wavelet transforms on non-separable lattices]], e.g., red-black wavelets on the quincunx lattice<ref name=\"Uytterhoeven1998\">{{cite conference|first1=Geert|last1=Uytterhoeven|first2=Adhemar|last2=Bultheel|author2-link= Adhemar Bultheel |url=http://nalag.cs.kuleuven.be/papers/ade/redblack/|title=The Red-Black Wavelet Transform|conference=Signal Processing Symposium (IEEE Benelux)|pages=191–194|year=1998}}</ref>\n\n==See also==\n* The [[Feistel scheme]] in cryptology uses much the same idea of dividing data and alternating function application with addition. Both in the Feistel scheme and the lifting scheme this is used for symmetric en- and decoding.\n\n== References ==\n\n<references />\n\n==External links==\n* [http://wavelets.org/schemes-lifting.php Lifting Scheme] – brief description of the factoring algorithm\n* [http://www.mathnet.or.kr/Video/etc/dongseo/1002_Yoo.ppt Introduction to The Lifting Scheme]\n\n[[Category:Digital signal processing]]\n[[Category:Matrix decompositions]]\n[[Category:Wavelets]]"
    },
    {
      "title": "Matrix decomposition",
      "url": "https://en.wikipedia.org/wiki/Matrix_decomposition",
      "text": "{{Short description|Representation of a matrix as a product}}\nIn the [[mathematics|mathematical]] discipline of [[linear algebra]], a '''matrix decomposition''' or '''matrix factorization''' is a [[factorization]] of a [[Matrix (mathematics)|matrix]] into a product of matrices. There are many different matrix decompositions; each finds use among a particular class of problems.\n\n== Example ==\nIn [[numerical analysis]], different decompositions are used to implement efficient matrix [[algorithm]]s.\n\nFor instance, when solving a [[system of linear equations]] <math>Ax=b</math>, the matrix ''A'' can be decomposed via the [[LU decomposition]]. The LU decomposition factorizes a matrix into a [[lower triangular matrix]] ''L'' and an [[upper triangular matrix]] ''U''. The systems <math>L(Ux)=b</math> and <math>Ux=L^{-1}b</math> require fewer additions and multiplications to solve, compared with the original system <math>Ax=b</math>, though one might require significantly more digits in inexact arithmetic such as [[floating point]].\n\nSimilarly, the [[QR decomposition]] expresses ''A'' as ''QR'' with ''Q'' an [[orthogonal matrix]] and ''R'' an upper triangular matrix.  The system ''Q''(''Rx'') = ''b'' is solved by ''Rx'' = ''Q''<sup>T</sup>''b'' = ''c'', and the system ''Rx'' = ''c'' is solved by '[[Triangular matrix#Forward and back substitution|back substitution]]'.  The number of additions and multiplications required is about twice that of using the LU solver, but no more digits are required in inexact arithmetic because the QR decomposition is [[numerically stable]].\n\n== Decompositions related to solving systems of linear equations ==\n\n=== LU decomposition ===\n{{main|LU decomposition}}\n*Applicable to: [[square matrix]] ''A''\n*Decomposition: <math>A=LU</math>, where ''L'' is [[triangular matrix|lower triangular]] and ''U'' is [[triangular matrix|upper triangular]]\n*Related: the [[LDU decomposition|''LDU'' decomposition]] is <math>A=LDU</math>, where ''L'' is [[triangular matrix|lower triangular]] with ones on the diagonal, ''U'' is [[triangular matrix|upper triangular]] with ones on the diagonal, and ''D'' is a [[diagonal matrix]].\n*Related: the [[LUP decomposition|''LUP'' decomposition]] is <math>A=LUP</math>, where ''L'' is [[triangular matrix|lower triangular]], ''U'' is [[triangular matrix|upper triangular]], and ''P'' is a [[permutation matrix]].\n*Existence: An LUP decomposition exists for any square matrix ''A''. When ''P'' is an [[identity matrix]], the LUP decomposition reduces to the LU decomposition. If the LU decomposition exists, then the LDU decomposition exists.<ref>{{harvnb|Simon|Blume|1994}} Chapter 7.</ref>\n*Comments: The LUP and LU decompositions are useful in solving an ''n''-by-''n'' system of linear equations <math>Ax=b</math>. These decompositions summarize the process of [[Gaussian elimination]] in matrix form. Matrix ''P'' represents any row interchanges carried out in the process of Gaussian elimination. If Gaussian elimination produces the [[row echelon form]] without requiring any row interchanges, then ''P''&nbsp;=&nbsp;''I'', so an LU decomposition exists.\n\n=== LU reduction ===\n{{main|LU reduction}}\n\n=== Block LU decomposition ===\n{{main|Block LU decomposition}}\n\n=== Rank factorization ===\n{{main|Rank factorization}}\n*Applicable to: ''m''-by-''n'' matrix ''A'' of rank ''r''\n*Decomposition: <math>A=CF</math> where ''C'' is an ''m''-by-''r'' full column rank matrix and ''F'' is an ''r''-by-''n'' full row rank matrix\n*Comment: The rank factorization can be used to [[Moore–Penrose pseudoinverse#Rank decomposition|compute the Moore–Penrose pseudoinverse]] of ''A'',<ref>{{cite journal|last1=Piziak|first1=R.|last2=Odell|first2=P. L.|title=Full Rank Factorization of Matrices|journal=Mathematics Magazine|date=1 June 1999|volume=72|issue=3|pages=193|doi=10.2307/2690882|jstor=2690882}}</ref> which one can apply to [[Moore–Penrose pseudoinverse#Obtaining all solutions of a linear system|obtain all solutions of the linear system]] <math>Ax=b</math>.\n\n=== Cholesky decomposition ===\n{{main|Cholesky decomposition}}\n*Applicable to: [[square matrix|square]], [[symmetric matrix|hermitian]], [[positive-definite matrix|positive definite]] matrix ''A''\n*Decomposition: <math>A=U^*U</math>, where ''U'' is upper triangular with real positive diagonal entries\n*Comment: if the matrix '''A''' is Hermitian and positive semi-definite, then it has a decomposition of the form <math>A=U^*U</math> if the diagonal entries of <math>U</math> are allowed to be zero\n*Uniqueness: for positive definite matrices Cholesky decomposition is unique. However, it is not unique in the positive semi-definite case.\n*Comment: if A is real and symmetric, <math>U</math> has all real elements\n*Comment: An alternative is the [[LDL decomposition]], which can avoid extracting square roots.\n\n=== QR decomposition ===\n{{main|QR decomposition}}\n*Applicable to: ''m''-by-''n'' matrix ''A'' with linearly independent columns\n*Decomposition: <math>A=QR</math> where ''Q'' is a [[unitary matrix]] of size ''m''-by-''m'', and ''R'' is an [[triangular matrix|upper triangular]] matrix of size ''m''-by-''n''\n*Uniqueness: In general it is not unique, but if <math>A</math> is of full [[Matrix rank|rank]], then there exists a single <math>R</math> that has all positive diagonal elements. If <math>A</math> is square, also <math>Q</math> is unique.\n*Comment: The QR decomposition provides an alternative way of solving the system of equations <math>Ax=b</math> without [[matrix inverse|inverting]] the matrix ''A''. The fact that ''Q'' is [[orthogonal matrix|orthogonal]]  means that <math>Q^TQ=I</math>, so that <math>Ax=b</math> is equivalent to <math>Rx=Q^Tb</math>, which is easier to solve since ''R'' is [[triangular matrix|triangular]].\n\n=== RRQR factorization ===\n{{main|RRQR factorization}}\n\n=== Interpolative decomposition ===\n\n{{main|Interpolative decomposition}}\n\n== Decompositions based on eigenvalues and related concepts ==\n\n=== Eigendecomposition ===\n{{main|Eigendecomposition (matrix)}}\n*Also called ''[[Spectral decomposition (Matrix)|spectral decomposition]]''.\n*Applicable to: [[square matrix]] ''A'' with linearly independent eigenvectors (not necessarily distinct eigenvalues).\n*Decomposition: <math>A=VDV^{-1}</math>, where ''D'' is a [[diagonal matrix]] formed from the [[eigenvalue]]s of ''A'', and the columns of ''V'' are the corresponding [[eigenvector]]s of ''A''.\n*Existence: An ''n''-by-''n'' matrix ''A'' always has ''n'' (complex) eigenvalues, which can be ordered (in more than one way) to form an ''n''-by-''n'' diagonal matrix ''D'' and a corresponding matrix of nonzero columns ''V'' that satisfies the [[Eigenvalue, eigenvector and eigenspace#Definitions: the eigenvalue equation|eigenvalue equation]] <math>AV=VD</math>.  <math>V</math> is invertible if and only if the ''n'' eigenvectors are [[Linear independence|linearly independent]] (i.e., each eigenvalue has [[geometric multiplicity]] equal to its [[algebraic multiplicity]]). A sufficient (but not necessary) condition for this to happen is that all the eigenvalues are different (in this case geometric and algebraic multiplicity are equal to 1)\n*Comment: One can always normalize the eigenvectors to have length one (see the definition of the eigenvalue equation)\n*Comment: Every [[normal matrix]] ''A'' (i.e., matrix for which <math>AA^*=A^*A</math>, where <math>A^*</math> is a [[conjugate transpose]]) can be eigendecomposed. For a [[normal matrix]] ''A'' (and only for a normal matrix), the eigenvectors can also be made orthonormal (<math>VV^*=I</math>) and the eigendecomposition reads as <math>A=VDV^*</math>. In particular all [[Unitary matrix|unitary]], [[Hermitian matrix|Hermitian]], or [[Skew-Hermitian matrix|skew-Hermitian]] (in the real-valued case, all [[Orthogonal matrix|orthogonal]], [[Symmetric matrix|symmetric]], or [[Skew-symmetric matrix|skew-symmetric]], respectively) matrices are normal and therefore possess this property.\n*Comment: For any real [[Symmetric matrix|symmetric]] matrix ''A'', the eigendecomposition always exists and can be written as <math>A=VDV^T</math>, where both ''D'' and ''V'' are real-valued.\n*Comment: The eigendecomposition is useful for understanding the solution of a system of linear ordinary differential equations or linear difference equations. For example, the difference equation <math>x_{t+1}=Ax_t</math> starting from the initial condition <math>x_0=c</math> is solved by <math>x_t = A^tc</math>, which is equivalent to <math>x_t = VD^tV^{-1}c</math>, where ''V'' and ''D'' are the matrices formed from the eigenvectors and eigenvalues of ''A''. Since ''D'' is diagonal, raising it to power <math>D^t</math>, just involves raising each element on the diagonal to the power ''t''. This is much easier to do and understand than raising ''A'' to power ''t'', since ''A'' is usually not diagonal.\n\n=== Jordan decomposition ===\nThe [[Jordan normal form]] and the [[Jordan–Chevalley decomposition]]\n*Applicable to: [[square matrix]] ''A''\n*Comment: the Jordan normal form generalizes the eigendecomposition to cases where there are repeated eigenvalues and cannot be diagonalized, the Jordan–Chevalley decomposition does this without choosing a basis.\n\n=== Schur decomposition ===\n{{main|Schur decomposition}}\n*Applicable to: [[square matrix]] ''A''\n*Decomposition (complex version): <math>A=UTU^*</math>, where ''U'' is a [[unitary matrix]], <math>U^*</math> is the [[conjugate transpose]] of ''U'', and ''T'' is an [[upper triangular]] matrix called the complex [[Schur form]] which has the [[eigenvalue]]s of ''A'' along its diagonal.\n*Comment: if A is a [[normal matrix]], then T is diagonal and the Schur decomposition coincides with the spectral decomposition.\n\n=== Real Schur decomposition ===\n*Applicable to: [[square matrix]] ''A''\n*Decomposition: This is a version of Schur decomposition where <math>V</math> and <math>S</math> only contain real numbers. One can always write <math>A=VSV^T</math> where ''V'' is a real [[orthogonal matrix]], <math>V^T</math> is the [[matrix transpose|transpose]] of ''V'', and ''S'' is a [[block matrix|block upper triangular]] matrix called the real [[Schur form]]. The blocks on the diagonal of ''S'' are of size 1×1 (in which case they represent real eigenvalues) or 2×2 (in which case they are derived from [[complex conjugate]] eigenvalue pairs).\n\n=== QZ decomposition ===\n{{main|QZ decomposition}}\n*Also called: ''generalized Schur decomposition''\n*Applicable to: [[square matrix|square matrices]] ''A'' and ''B''\n*Comment: there are two versions of this decomposition: complex and real.\n*Decomposition (complex version): <math>A=QSZ^*</math> and <math>B=QTZ^*</math> where ''Q'' and ''Z'' are [[unitary matrix|unitary matrices]], the * superscript represents [[conjugate transpose]], and ''S'' and ''T'' are [[upper triangular]] matrices.\n*Comment: in the complex QZ decomposition, the ratios of the diagonal elements of ''S'' to the corresponding diagonal elements of ''T'', <math>\\lambda_i = S_{ii}/T_{ii}</math>, are the generalized [[eigenvalue]]s that solve the [[Eigendecomposition of a matrix#Additional topics|generalized eigenvalue problem]] <math>Av=\\lambda Bv</math> (where <math>\\lambda</math> is an unknown scalar and ''v'' is an unknown nonzero vector).\n*Decomposition (real version): <math>A=QSZ^T</math> and <math>B=QTZ^T</math> where ''A'', ''B'', ''Q'', ''Z'', ''S'', and ''T'' are matrices containing real numbers only. In this case ''Q'' and ''Z'' are [[orthogonal matrix|orthogonal matrices]], the ''T'' superscript represents [[matrix transpose|transposition]], and ''S'' and ''T'' are [[block matrix|block upper triangular]] matrices. The blocks on the diagonal of ''S'' and ''T'' are of size 1×1 or 2×2.\n\n=== Takagi's factorization ===\n*Applicable to: square, complex, symmetric matrix ''A''.\n*Decomposition: <math>A=VDV^T</math>, where ''D'' is a real nonnegative [[diagonal matrix]], and ''V'' is [[unitary matrix|unitary]]. <math>V^T</math> denotes the [[matrix transpose]] of ''V''.\n*Comment: The diagonal elements of ''D'' are the nonnegative square roots of the eigenvalues of <math>AA^*</math>.\n*Comment: ''V'' may be complex even if ''A'' is real.\n*Comment: This is not a special case of the eigendecomposition (see above), which uses <math>V^{-1}</math> instead of <math>V^T</math>.\n\n=== Singular value decomposition ===\n{{main|Singular value decomposition}}\n*Applicable to: ''m''-by-''n'' matrix ''A''.\n*Decomposition: <math>A=UDV^*</math>, where ''D'' is a nonnegative [[diagonal matrix]], and  ''U'' and ''V'' satisfy <math>U^*U = I, V^*V = I</math>. Here <math>V^*</math> is the [[conjugate transpose]] of ''V'' (or simply the [[matrix transpose|transpose]], if ''V'' contains real numbers only), and ''I'' denotes the identity matrix (of some dimension). \n*Comment: The diagonal elements of ''D'' are called the [[singular value]]s of ''A''.\n*Comment: Like the eigendecomposition above, the singular value decomposition involves finding basis directions along which matrix multiplication is equivalent to scalar multiplication, but it has greater generality since the matrix under consideration need not be square.\n*Uniqueness: the singular values of <math>A</math> are always uniquely determined. <math>U</math> and <math>V</math> need not to be unique in general.\n\n=== Scale-invariant decompositions ===\n\nRefers to variants of existing matrix decompositions, such as the SVD, that are invariant with respect to diagonal scaling.\n\n*Applicable to: ''m''-by-''n'' matrix ''A''.\n*Unit-Scale-Invariant Singular-Value Decomposition: <math>A=DUSV^*E</math>, where ''S'' is a unique nonnegative [[diagonal matrix]] of scale-invariant singular values,  ''U'' and ''V'' are [[unitary matrix|unitary matrices]], <math>V^*</math> is the [[conjugate transpose]] of ''V'', and positive diagonal matrices ''D'' and ''E''.\n*Comment: Is analogous to the SVD except that the diagonal elements of ''S'' are invariant with respect to left and/or right multiplication of ''A'' by arbitrary nonsingular diagonal matrices, as opposed to the standard SVD for which the singular values are invariant with respect to left and/or right multiplication of ''A'' by arbitrary unitary matrices.\n*Comment: Is an alternative to the standard SVD when invariance is required with respect to diagonal rather than unitary transformations of ''A''.\n*Uniqueness: The scale-invariant singular values of <math>A</math> (given by the diagonal elements of ''S'') are always uniquely determined. Diagonal matrices ''D'' and ''E'', and unitary ''U'' and ''V'', are not necessarily unique in general.\n*Comment: ''U'' and ''V'' matrices are not the same as those from the SVD.\n\nAnalogous scale-invariant decompositions can be derived from other matrix decompositions, e.g., to obtain scale-invariant eigenvalues.<ref>{{citation|last=Uhlmann |first=J.K. |title=A Generalized Matrix Inverse that is Consistent with Respect to Diagonal Transformations |series=SIAM Journal on Matrix Analysis |year=2018 |volume=239:2 |pages=781–800}}</ref><ref>{{citation|last=Uhlmann |first=J.K. |title=A Rank-Preserving Generalized Matrix Inverse for Consistency with Respect to Similarity |series=IEEE Control Systems Letters |issn=2475-1456 |year=2018}}</ref>\n\n== Other decompositions ==\n\n=== Polar decomposition ===\n{{main|Polar decomposition}}\n*Applicable to: any square complex matrix ''A''.\n*Decomposition: <math>A=UP</math> (right polar decomposition) or <math>A=P'U</math> (left polar decomposition), where ''U'' is a [[unitary matrix]] and ''P'' and ''P''' are [[positive semidefinite matrix|positive semidefinite]] [[Hermitian matrices]].\n*Uniqueness: <math>P</math> is always unique and equal to <math>\\sqrt{A^*A}</math> (which is always hermitian and positive semidefinite). If <math>A</math> is invertible, then <math>U</math> is unique.\n*Comment: Since any Hermitian matrix admits a spectral decomposition with a unitary matrix, <math>P</math> can be written as <math>P=VDV^*</math>. Since <math>P</math> is positive semidefinite, all elements in <math>D</math> are non-negative. Since the product of two unitary matrices is unitary, taking <math>W=UV</math>one can write <math>A=U(VDV^*)=WDV^* </math> which is the singular value decomposition. Hence, the existence of the polar decomposition is equivalent to the existence of the singular value decomposition.\n\n=== Algebraic polar decomposition ===\n*Applicable to: square, complex, non-singular matrix ''A''.<ref>{{harvnb|Choudhury|Horn|1987|pp=219–225}}</ref>\n*Decomposition: <math>A=QS</math>, where ''Q'' is a complex orthogonal matrix and ''S'' is complex symmetric matrix.\n*Uniqueness: If <math>A^{T}A</math> has no negative real eigenvalues, then the decomposition is unique.<ref name=\":0\">{{Cite journal|last=Bhatia|first=Rajendra|date=2013-11-15|title=The bipolar decomposition|url=http://www.sciencedirect.com/science/article/pii/S0024379513005612|journal=Linear Algebra and its Applications|volume=439|issue=10|pages=3031–3037|doi=10.1016/j.laa.2013.09.006}}</ref>\n*Comment: The existence of this decomposition is equivalent to <math>AA^{T}</math> being similar to <math>A^{T}A</math>.<ref>{{harvnb|Horn|merino|1995|pp=43–92}}</ref>\n*Comment: A variant of this decomposition is <math>A=RC</math>, where ''R'' is a real matrix and ''C'' is a [[circular matrix]].<ref name=\":0\" />\n\n=== Mostow's decomposition ===\n{{main|Mostow decomposition}}\n* Applicable to: square, complex, non-singular matrix ''A''.<ref>{{citation|last=Mostow|first= G. D.|title= Some new decomposition theorems for semi-simple groups|series= Mem. Amer. Math. Soc. |year=1955|volume=14|pages= 31–54|url=https://archive.org/details/liealgebrasandli029541mbp|publisher= American Mathematical Society}}</ref><ref>{{Cite book|title=Matrix Information Geometry|last=Nielsen|first=Frank|last2=Bhatia|first2=Rajendra|publisher=Springer|year=2012|isbn=9783642302329|location=|pages=224|language=en|doi=10.1007/978-3-642-30232-9|arxiv = 1007.4402}}</ref>\n* Decomposition: <math>A=Ue^{iM}e^{S}</math>, where ''U'' is unitary, ''M'' is real anti-symmetric and ''S'' is real symmetric.\n* Comment: The matrix ''A'' can also be decomposed as <math>A=U_2e^{S_2}e^{iM_2}</math>, where ''U<sub>2</sub>'' is unitary, ''M<sub>2</sub>'' is real anti-symmetric and ''S<sub>2</sub>'' is real symmetric.<ref name=\":0\" />\n\n=== Sinkhorn normal form ===\n{{main|Sinkhorn's theorem}}\n*Applicable to: square real matrix ''A'' with strictly positive elements.\n*Decomposition: <math>A=D_{1}SD_{2}</math>, where ''S'' is [[Doubly stochastic matrix|doubly stochastic]] and ''D''<sub>1</sub> and ''D''<sub>2</sub> are real diagonal matrices with strictly positive elements.\n\n=== Sectoral decomposition ===\n*Applicable to: square, complex matrix ''A'' with [[numerical range]] contained in the sector <math>S_\\alpha = \\left\\{r e^{i \\theta} \\in \\mathbb{C} \\mid r> 0, |\\theta| \\le \\alpha < \\frac{\\pi}{2}\\right\\}</math>.\n*Decomposition: <math>A = CZC^*</math>, where ''C'' is an invertible complex matrix and <math>Z = \\operatorname{diag}\\left(e^{i\\theta_1},\\ldots,e^{i\\theta_n}\\right)</math> with all <math>\\left|\\theta_j\\right| \\le \\alpha </math>.<ref name=Zhang2014>{{cite journal|last1=Zhang|first1=Fuzhen|title=A matrix decomposition and its applications|journal=Linear and Multilinear Algebra|volume=63|issue=10|date=30 June 2014|pages=2033–2042|doi=10.1080/03081087.2014.933219}}</ref><ref>{{cite journal|last1=Drury|first1=S.W.|title=Fischer determinantal inequalities and Highamʼs Conjecture|journal=Linear Algebra and its Applications|date=November 2013|volume=439|issue=10|pages=3129–3133|doi=10.1016/j.laa.2013.08.031}}</ref>\n\n=== Williamson's normal form ===\n* Applicable to: square, [[Positive-definite matrix|positive-definite]] real matrix ''A'' with order 2''n''-by-2''n''.\n* Decomposition: <math>A=S^{T}\\textrm{diag}(D,D)S</math>, where <math>S \\in \\text{Sp}(2n)</math> is a [[symplectic matrix]] and ''D'' is a nonnegative ''n''-by-''n'' diagonal matrix.<ref>{{Cite journal|last=Idel|first=Martin|last2=Soto Gaona|first2=Sebastián|last3=Wolf|first3=Michael M.|date=2017-07-15|title=Perturbation bounds for Williamson's symplectic normal form|url=http://www.sciencedirect.com/science/article/pii/S0024379517301751|journal=Linear Algebra and its Applications|volume=525|pages=45–58|doi=10.1016/j.laa.2017.03.013|arxiv=1609.01338}}</ref>\n\n== Generalizations ==\n{{Expand section|1=examples and additional citations|date=December 2014}}\nThere exist analogues of the SVD, QR, LU and Cholesky factorizations for '''quasimatrices''' and '''cmatrices''' or '''continuous matrices'''.<ref>{{harvnb|Townsend|Trefethen|2015}}</ref> A ‘quasimatrix’ is, like a matrix, a rectangular scheme whose elements are indexed, but one discrete index is replaced by a continuous index. Likewise, a ‘cmatrix’, is continuous in both indices. As an example of a cmatrix, one can think of the kernel of an [[integral operator]].\n\nThese factorizations are based on early work by {{harvtxt|Fredholm|1903}}, {{harvtxt|Hilbert|1904}} and {{harvtxt|Schmidt|1907}}. For an account, and a translation to English of the seminal papers, see {{harvtxt|Stewart|2011}}.\n\n==See also==\n* [[Matrix splitting]]\n* [[Non-negative matrix factorization]]\n* [[Principal component analysis]]\n\n==Notes==\n{{reflist}}\n\n== References ==\n*{{cite journal|last1=Choudhury|first1=Dipa|last2=Horn|first2=Roger A.|title=A Complex Orthogonal-Symmetric Analog of the Polar Decomposition|journal=SIAM Journal on Algebraic and Discrete Methods|date=April 1987|volume=8|issue=2|pages=219–225|doi=10.1137/0608019}}\n*{{citation|first=I.|last=Fredholm|title=Sur une classe d'´equations fonctionnelles|journal=Acta Mathematica|volume=27|pages=365–390|year=1903|language=French|authorlink=Ivar Fredholm|doi=10.1007/bf02421317}}\n*{{citation|first=D.|last=Hilbert|title=Grundzüge einer allgemeinen Theorie der linearen Integralgleichungen|journal=Nachr. Königl. Ges. Gött|volume=1904|pages=49–91|year=1904|language=German|authorlink=David Hilbert}}\n*{{cite journal|last1=Horn|first1=Roger A.|last2=Merino|first2=Dennis I.|title=Contragredient equivalence: A canonical form and some applications|journal=Linear Algebra and its Applications|date=January 1995|volume=214|pages=43–92|doi=10.1016/0024-3795(93)00056-6}}\n*{{Citation|last1=Meyer|first1=C. D.|title=Matrix Analysis and Applied Linear Algebra|url=http://www.matrixanalysis.com/|publisher=[[Society for Industrial and Applied Mathematics|SIAM]]|isbn=978-0-89871-454-8|year=2000}}\n*{{citation|first=E.|last=Schmidt|title=Zur Theorie der linearen und nichtlinearen Integralgleichungen. I Teil. Entwicklung willkürlichen Funktionen nach System vorgeschriebener|journal=Mathematische Annalen|volume=63|issue=4|pages=433–476|year=1907|language=German|authorlink=Erhard Schmidt|doi=10.1007/bf01449770}}\n*{{Cite book|last=Simon|first=C.|last2=Blume|first2=L.|year=1994|title=Mathematics for Economists|publisher= Norton|isbn=978-0-393-95733-4|ref=harv}}\n*{{citation|last=Stewart|first=G. W.|year=2011|title=Fredholm, Hilbert, Schmidt: three fundamental papers on integral equations|url=http://www.cs.umd.edu/~stewart/FHS.pdf|access-date=2015-01-06}}\n*{{citation|last=Townsend|first=A.|last2=Trefethen|first2=L. N.|year=2015|title=Continuous analogues of matrix factorizations|journal=[[Proceedings of the Royal Society|Proc. R. Soc. A]]|volume=471|issue=2173|pages=20140585|doi=10.1098/rspa.2014.0585|pmid=25568618|pmc=4277194|bibcode=2014RSPSA.47140585T}}\n\n==External links==\n*[http://www.bluebit.gr/matrix-calculator/ Online Matrix Calculator]\n*[http://www.wolframalpha.com/input/?i=matrix+decomposition&rawformassumption={%22C%22,+%22matrix+decomposition%22}+-%3E+{%22Calculator%22}&rawformassumption={%22MC%22,%22%22}-%3E{%22Formula%22} Wolfram Alpha Matrix Decomposition Computation » LU and QR Decomposition]\n*[http://eom.springer.de/M/m120140.htm  Springer Encyclopaedia of Mathematics » Matrix factorization]\n* [https://web.archive.org/web/20110314171151/http://www.graphlab.ml.cmu.edu/pmf.html GraphLab] [[GraphLab]] collaborative filtering library, large scale parallel implementation of matrix decomposition methods (in C++) for multicore.\n\n{{linear algebra}}\n\n[[Category:Matrix theory]]\n[[Category:Matrix decompositions]]"
    },
    {
      "title": "Polynomial matrix spectral factorization",
      "url": "https://en.wikipedia.org/wiki/Polynomial_matrix_spectral_factorization",
      "text": "{{Orphan|date=October 2018}}\n\n[[Polynomial matrix|Polynomial matrices]] are widely studied in the fields of [[systems theory]] and [[control theory]] and have seen other uses relating to [[stable polynomial]]s. In stability theory, Spectral Factorization has been used to find determinental matrix representations for bivariate stable polynomials and real zero polynomials.<ref>{{Cite journal|last=Anatolii Grinshpan, Dmitry S. Kaliuzhnyi-Verbovetskyir, Victor Vinnikov, Hugo J. Woerdeman|title=Stable and real-zero polynomials in two variables|journal=Multidimensional Systems and Signal Processing|volume=27|pages=1–26|doi=10.1007/s11045-014-0286-3|year=2016|citeseerx=10.1.1.767.8178}}</ref> A key tool used to study these is a matrix factorization known as either the Polynomial Matrix Spectral Factorization or the Matrix Fejer–Riesz Theorem.\n\nGiven a univariate [[Positive polynomial|positive polynomial <math>p(t)</math>]], a polynomial which takes on non-negative values for any real input [[Positive polynomial|<math>t</math>]], the Fejer–Riesz Theorem yields the polynomial spectral factorization [[Positive polynomial|<math>p(t) = q(t) q(\\bar{t})^*</math>]]. Results of this form are generically referred to as [[Positivstellensatz]]. Considering positive definiteness as the matrix analogue of positivity, Polynomial Matrix Spectral Factorization provides a similar factorization for polynomial matrices which have positive definite range. This decomposition also relates to the [[Cholesky decomposition]] for scalar matrices <math>A =LL^* </math>. This result was originally proven by Wiener<ref>{{Cite journal|last=N. Wiener and P. Masani|title=The prediction theory of multivariate stochastic processe|url=|journal=Acta Math.|volume=98|pages=111–150|via=|doi=10.1007/BF02404472|year=1957}}</ref> in a more general context which was concerned with integrable matrix-valued functions that also had integrable log determinant. Because applications are often concerned with the polynomial restriction, simpler proofs and individual analysis exist focusing on this case.<ref>{{Cite journal|last=Tim N.T. Goodman Charles A. Micchelli Giuseppe Rodriguez Sebastiano Seatzu|title=Spectral factorization of Laurent polynomials|journal=Advances in Computational Mathematics|volume=7|issue=4|pages=429–454|doi=10.1023/A:1018915407202|year=1997}}</ref> Weaker positivstellensatz conditions have been studied, specifically considering when the polynomial matrix has positive definite image on semi-algebraic subsets of the reals.<ref>{{Cite journal|last=Aljaž Zalar|title=Matrix Fejér–Riesz theorem with gaps|url=http://www.sciencedirect.com/science/article/pii/S0022404915003345|journal=Journal of Pure and Applied Algebra|volume=220|issue=7|pages=2533–2548|via=|doi=10.1016/j.jpaa.2015.11.018|year=2016|arxiv=1503.06034}}</ref> Many publications recently have focused on streamlining proofs for these related results.<ref>{{Cite journal|last=Zalar|first=Aljaž|date=2016-07-01|title=Matrix Fejér–Riesz theorem with gaps|url=http://www.sciencedirect.com/science/article/pii/S0022404915003345|journal=Journal of Pure and Applied Algebra|volume=220|issue=7|pages=2533–2548|doi=10.1016/j.jpaa.2015.11.018|arxiv=1503.06034}}</ref><ref>{{Cite journal|url=https://www.researchgate.net/publication/225398913|title=A Simple Proof of the Matrix-Valued Fejer–Riesz Theorem|journal=Journal of Fourier Analysis and Applications|volume=15|pages=124–127|last=Lasha Ephremidze|language=|archive-url=|archive-date=|dead-url=|access-date=2017-05-23|doi=10.1007/s00041-008-9051-z|year=2009|citeseerx=10.1.1.247.3400}}</ref> This article roughly follows the recent proof method of Lasha Ephremidze<ref>{{Cite journal|last=Ephremidze|first=Lasha|title=An Elementary Proof of the Polynomial Matrix Spectral Factorization Theorem|url=|journal=Proceedings of the Royal Society of Edinburgh Section A: Mathematics|volume=144|issue=4|pages=747–751|via=|doi=10.1017/S0308210512001552|citeseerx=10.1.1.755.9575|year=2014}}</ref> which relies only on elementary [[linear algebra]] and [[complex analysis]].\n\nSpectral Factorization is used extensively in [[linear–quadratic–Gaussian control]]. Because of this application there have been many algorithms to calculate spectral factors.<ref>{{Cite journal|last=Thomas Kailath, A. H. Sayed|title=A survey of spectral factorization methods|journal=Numerical Linear Algebra Techniques for Control and Signal Processing|volume= 8| issue = 6–7|pages=467–496|doi=10.1002/nla.250|year=2001}}</ref> Some modern algorithms focus on the more general setting originally studied by Wiener.<ref>{{Cite journal|last=Gigla Janashia, Edem Lagvilava, Lasha Ephremidze|title=A New Method of Matrix Spectral Factorization|url=http://ieeexplore.ieee.org/abstract/document/5730565/|journal=IEEE Transactions on Information|volume=57|issue=4|pages=2318–2326|via=|doi=10.1109/TIT.2011.2112233|year=2011|arxiv=0909.5361}}</ref> In the <math>1 \\times 1</math> case the problem is known as polynomial spectral factorization, or Fejer-Riesz Theorem, and has many classical algorithms. Some modern algorithms have used [[Toeplitz matrix]] advances to speed up factor calculations.<ref>{{Cite journal|last=D.A. Bini, G. Fiorentino, L. Gemignani, B. Meini|title=Effective Fast Algorithms for Polynomial Spectral Factorization|journal=Numerical Algorithms|volume=34|issue=2–4|pages=217–227|doi=10.1023/B:NUMA.0000005364.00003.ea|year=2003}}</ref>\n\n== Statement ==\nLet <math>P(t) = \\begin{bmatrix}p_{11}(t) &\\ldots &p_{1n}(t) \\\\ \\vdots & \\ddots & \\vdots\\\\ p_{n1}(t) & \\cdots& p_{nn}(t)\\\\  \\end{bmatrix}</math>be a polynomial matrix where each entry <math>p_{ij}(t)</math> is a complex coefficient polynomial of degree at most <math>N</math>. Suppose that for almost all <math>t \\in \\mathbb{R}</math> we have <math>P(t)</math> is a positive definite hermitian matrix. Then there exists a polynomial matrix <math>Q(t)</math> such that <math>P(t) = Q(t) Q(t)^*</math> for all <math>t \\in \\mathbb{R}</math>. We can furthermore find <math>Q(t)</math> which is nonsingular on the lower half plane.\n\n=== Extension to complex inputs ===\nNote that if <math>Q(t) = \\begin{bmatrix}q_{11}(t) &\\ldots &q_{1n}(t) \\\\ \\vdots & \\ddots & \\vdots\\\\ q_{n1}(t) & \\cdots& q_{nn}(t)\\\\  \\end{bmatrix}</math>then <math>Q(\\bar{t})^* = \\begin{bmatrix}q_{11}(\\bar{t})^* &\\ldots &q_{n1}(\\bar{t})^* \\\\ \\vdots & \\ddots & \\vdots\\\\ q_{1n}(\\bar{t})^* & \\cdots& q_{nn}(\\bar{t})^*\\\\  \\end{bmatrix}</math>. When <math>q_{ij}(t)</math> is a complex coefficient polynomial or complex coefficient rational function we have <math>q_{ij}(\\bar{t})^*</math>is also a polynomial or rational function respectively. For <math>t \\in \\mathbb{R}</math> we have<math display=\"block\">P(t) = Q(t) Q(t)^* = Q(t)Q(\\bar{t})^*</math>Since the entries of <math>Q(t) Q(\\bar{t})^*</math> and <math>P(t)</math> are complex polynomials which agree on the real line, they are in fact the same polynomials. We can conclude they in fact agree for all complex inputs.\n\n== Rational spectral factorization ==\nLet <math>p(t)</math> be a rational polynomial function where <math>p(t) > 0</math> for almost all <math>t \\in \\mathbb{R}</math>. Then there exists rational <math>q(t)</math> with [[Positive polynomial|<math>p(t) = q(t) q(\\bar{t})^*</math>]] where <math>q(t)</math> has no poles or zeroes in the lower half plane. This decomposition is unique up to multiplication by complex scalars of norm <math>1</math>. This is related to the statement of the Polynomial Matrix Spectral Factorization theorem restricted to the <math>1 \\times 1</math> case.\n\nTo prove existence write <math>p(x) = c \\frac{\\prod_i (x- \\alpha_i)}{\\prod_i (x-\\beta_i)}</math>where <math>\\alpha_i \\neq \\beta_j</math>. Letting <math>x \\to \\infty</math>we can conclude that <math>c</math> is real and positive. Dividing out by <math>\\sqrt{c} </math> we reduce to the monic scenario. The numerator and denominator have distinct sets of roots, so all real roots which show up in either must have even multiplicity (to prevent a sign change locally). We can divide out these real roots to reduce to the case where <math>p(t)</math> has only complex roots and poles. By hypothesis we have <math>p(x) = \\frac{\\prod_i (x- \\alpha_i)}{\\prod_i (x-\\beta_i)} = \\frac{\\prod_i (x- \\bar{\\alpha_i})}{\\prod_i (x-\\bar{\\beta_i})} = \\bar{p(\\bar{x})}</math>. Since all of the <math>\\alpha_i, \\beta_j</math>are complex (and hence not fixed points of conjugation) they both come in conjugate pairs. For each conjugate pair, pick the zero or pole in the upper half plane and accumulate these to obtain <math>q(t)</math>. The uniqueness result follows in a standard fashion.\n\n== Cholesky decomposition ==\nThe inspiration for this result is a factorization which characterizes positive definite matrices.\n\n=== Decomposition for scalar matrices ===\nGiven any positive definite scalar matrix <math>A</math>, we are guaranteed by [[Cholesky decomposition]] <math>A = LL^*</math> where <math>L</math> is a lower triangular matrix. If we don't restrict to lower triangular matrices we can consider all factorizations of the form <math>A = V V^*</math>. It is not hard to check that all factorizations are achieved by looking at the orbit of <math>L</math> under right multiplication by a unitary matrix, <math>V = L U</math>.\n\nTo obtain the lower triangular decomposition we induct by splitting off the first row and first column:<math display=\"block\">\\begin{bmatrix} a_{11} & \\mathbf{a}_{12}^* \\\\ \\mathbf{a}_{12} & A_{22} \\\\ \\end{bmatrix} = \\begin{bmatrix} l_{11} & 0 \\\\ \\mathbf{l}_{21} & L_{22} \\end{bmatrix}  \\begin{bmatrix} l_{11}^* & \\mathbf{l}_{21}^* \\\\ 0 & L_{22}^* \\end{bmatrix}  = \\begin{bmatrix} l_{11} l_{11}^* & l_{11} \\mathbf{l}_{21}^* \\\\ l_{11}^* \\mathbf{l}_{21} & \\mathbf{l}_{21} \\mathbf{l}_{21}^*+  L_{22} L_{22}^* \\end{bmatrix}</math>Solving these in terms of <math>a_{ij}</math> we get<math display=\"block\">l_{11} = \\sqrt{a_{11}} </math><math display=\"block\">\\mathbf{l}_{21} = \\frac{1}{\\sqrt{a_{11}}}\\mathbf{a}_{12} </math><math display=\"block\">L_{22} L_{22}^* = A_{22} - \\frac{1}{a_{11}} \\mathbf{a}_{12} \\mathbf{a}^*_{12} </math>\n\nSince <math>A</math> is positive definite we have <math>a_{11}</math>is a positive real number, so it has a square root. The last condition from induction since the right hand side is the [[Schur complement]] of <math>A</math>, which is itself positive definite.\n\n=== Decomposition for rational matrices ===\nNow consider <math>P(t) = \\begin{bmatrix}p_{11}(t) &\\ldots &p_{1n}(t) \\\\ \\vdots & \\ddots & \\vdots\\\\ p_{n1}(t) & \\cdots& p_{nn}(t)\\\\  \\end{bmatrix}</math>where the <math>p_{ij}(t)</math>are complex [[rational function]]s and <math>P(t)</math> is positive definite hermitian for almost all real <math>t</math>. Then by the symmetric [[Gaussian elimination]] we performed above, all we need to show is there exists a rational <math>q_{11}(t)</math> such that <math>p_{11}(t) = q_{11}(t) q_{11}(t)^*</math>for real <math>t</math>, which follows from our rational spectral factorization. Once we have that then we can solve for <math>l_{11}(t), \\mathbf{l}_{21}(t)</math>. Since the [[Schur complement]] is positive definite for the real <math>t</math> away from the poles and the Schur complement is a rational polynomial matrix we can induct to find <math>L_{22}</math>.\n\nIt is not hard to check that we in fact get <math>P(t) = L(t) L(t)^*</math>where <math>L(t)</math> is a rational polynomial matrix with no poles in the lower half plane.\n\n== Extension to polynomial decompositions ==\nTo prove the existence of polynomial matrix spectral factorization, we begin with the rational polynomial matrix Cholesky Decomposition and modify it to remove lower half plane singularities. Namely given  <math>P(t) = \\begin{bmatrix}p_{11}(t) &\\ldots &p_{1n}(t) \\\\ \\vdots & \\ddots & \\vdots\\\\ p_{n1}(t) & \\cdots& p_{nn}(t)\\\\  \\end{bmatrix}</math>with each entry <math>p_{ij}(t)</math> a complex coefficient polynomial we have rational polynomial matrix <math>L(t)</math> with <math>P(t) = L(t) L(t)^*</math>for real <math>t</math>, where <math>L(t)</math> has no lower half plane poles. Given a rational polynomial matrix <math>U(t)</math> which is unitary valued for real <math>t</math>, there exists another decomposition,<math>P(t) = L(t) L(t)^* = (L(t) U(t)) (L(t) U(t))^*</math>.\n\n=== Removing lower half-plane singularities ===\nIf <math>\\det(L(a)) = 0</math> then there exists a scalar unitary matrix <math>U</math>such that <math>U e_1 = \\frac{a}{|a|}</math>. This implies <math>L(t) U </math>has first column vanish at <math>a</math>. To remove the singularity at <math>a</math> we multiply by<math display=\"block\">U(t) = \\operatorname{diag}(1, \\ldots, \\frac{z - \\bar{a}}{z - a}, \\ldots, 1)</math><math>L(t) U U(t)</math>has determinant with one less zero (by multiplicity) at a, without introducing any poles in the lower half plane of any of the entries.\n\n=== Extend analyticity to all of C ===\nAfter modifications, the decomposition <math>P(t)=Q(t) Q(t)^*</math>satisfies <math>Q(t)</math> is analytic and invertible on the lower half plane. To extend analyticity to the upper half plane we need this key observation: Given a rational matrix <math>Q(t)</math> who is analytic in the lower half plane and nonsingular in the lower half plane, we have <math>Q(t)^{-1}</math> is analytic and nonsingular in the lower half plane. The analyticity follows from the [[adjugate matrix]] formula (since both the entries of <math>Q(t)</math> and  <math>\\det(Q(t))^{-1}</math>are analytic on the lower half plane). The nonsingularity follows from <math>\\det(Q(t)^{-1}) = \\det(Q(t))^{-1}</math>which can only have zeroes at places where <math>\\det(Q(t))</math> had poles. The determinant of a rational polynomial matrix can only have poles where its entries have poles, so <math>\\det(Q(t))</math> has no poles in the lower half plane.\n\nFrom our observation in Extension to Complex Inputs, we have <math>P(t)=Q(t)Q(\\bar{t})^*</math>for all complex numbers. This implies <math>Q(\\bar{t})=(P(t)Q(t)^{-1})^*</math>. Since <math>Q(t)^{-1}</math> is analytic on the lower half plane, <math>Q(t)</math> is analytic on the upper half plane. Finally if <math>Q(t)</math> has a pole on the real line then <math>Q(\\bar{t})^*</math>has the same pole on the real line which contradicts the fact <math>P(t)</math> has no poles on the real line (it is analytic everywhere by hypothesis).\n\nThe above shows that if <math>Q(t)</math> is analytic and invertible on the lower half plane indeed <math>Q(t)</math> is analytic everywhere and hence a polynomial matrix.\n\n== Uniqueness ==\nGiven two polynomial matrix decompositions which are invertible on the lower half plane<math>P(t)=Q(t) Q(\\bar{t})^*=R(t)R(\\bar{t})^*</math> we rearrange to <math>(R(t)^{-1} Q(t))(R(\\bar{t})^{-1} Q(\\bar{t}))^*=I</math>. Since <math>R(t)</math> is analytic on the lower half plane and nonsingular, <math>R(t)^{-1}Q(t) </math>is a rational polynomial matrix which is analytic and invertible on the lower half plane. Then by the same argument as above we have <math>R(t)^{-1}Q(t) </math> is in fact a polynomial matrix which is unitary for all real <math>t</math>. This means that if <math>\\mathbf{q}_i(t)</math> is the <math>i</math>th row of <math>Q(t)</math>then <math>\\mathbf{q}_i(t) \\mathbf{q}_i(\\bar{t})^* = 1</math>. For real <math>t</math> this is a sum of non-negative polynomials which sums to a constant, implying each of the summands are in fact constant polynomials. Then <math>Q(t) = R(t) U</math>where <math>U</math> is a scalar unitary matrix.\n\n== Example ==\nConsider <math>A(t) = \\begin{bmatrix} t^2 + 1 & 2 t \\\\ 2 t & t^2 + 1 \\\\ \\end{bmatrix}</math>. Then through symmetric Gaussian elimination we get the rational decomposition <math>A(t) = \\begin{bmatrix} t - i & 0  \\\\ \\frac{2t}{t + i} & \\frac{t^2 - 1}{t + i} \\\\ \\end{bmatrix}  \\begin{bmatrix} t - i & 0  \\\\ \\frac{2t}{t + i} & \\frac{t^2 - 1}{t + i} \\\\ \\end{bmatrix}^* </math>. This decomposition has no poles in the upper half plane. However the determinant is <math>\\frac{(t-1) (t-i) (t+1)}{t+i}</math>, so we need to modify our decomposition to get rid of the singularity at <math>-i</math>. First we multiply by a scalar unitary matrix to make a column vanish at <math>i</math>. Consider <math>U =\\frac{1}{\\sqrt{2}} \\begin{bmatrix}1 & 1 \\\\ i & -i \\\\ \\end{bmatrix}</math>. Then we have a new candidate for our decomposition<math> \\begin{bmatrix} t - i & 0  \\\\ \\frac{2t}{t + i} & \\frac{t^2 - 1}{t + i} \\\\ \\end{bmatrix} U = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} t-i & t - i \\\\ i \\frac{(t-i)^2}{t+i} & -i(t+i) \\\\ \\end{bmatrix}</math>. Now the first column vanishes at\n\n<math>i</math>, so we multiply through (on the right) by <math>U(t)  =\\frac{1}{\\sqrt{2}} \\begin{bmatrix}\\frac{t+i}{t-i} & 0 \\\\ 0 & 1\\end{bmatrix}</math> to obtain <math> Q(t) = \\begin{bmatrix} t - i & 0  \\\\ \\frac{2t}{t + i} & \\frac{t^2 - 1}{t + i} \\\\ \\end{bmatrix} U U(t) = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} t + i & t - i \\\\ i(t-i) & -i(t+i) \\\\ \\end{bmatrix}.</math> Notice <math>\\det(Q(t)) = i(1-t^2)</math>. This is our desired decomposition <math>A(t) = Q(t) Q(t)^*</math> with no singularities in the lower half plane.\n\n== References ==\n{{reflist}}\n\n[[Category:Matrix decompositions]]\n[[Category:Polynomials]]"
    },
    {
      "title": "Principal component analysis",
      "url": "https://en.wikipedia.org/wiki/Principal_component_analysis",
      "text": "{{machine learning bar}}\n[[File:GaussianScatterPCA.svg|thumb|right|PCA of a [[multivariate Gaussian distribution]] centered at (1,3) with a standard deviation of 3 in roughly the (0.866, 0.5) direction and of 1 in the orthogonal direction. The vectors shown are the [[Eigenvalues and eigenvectors|eigenvectors]] of the [[covariance matrix]] scaled by the square root of the corresponding eigenvalue, and shifted so their tails are at the mean.]]\n\n'''Principal component analysis''' ('''PCA''') is a statistical procedure that uses an [[orthogonal transformation]] to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of [[Correlation and dependence|linearly uncorrelated]] variables called '''principal components'''. This transformation is defined in such a way that the first principal component has the largest possible [[variance]] (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is [[orthogonal]] to the preceding components. The resulting vectors (each being a [[linear combination]] of the variables and containing ''n'' observations) are an uncorrelated [[orthogonal basis set]]. PCA is sensitive to the relative scaling of the original variables.\n\nPCA was invented in 1901 by [[Karl Pearson]],<ref>{{Cite journal| author = Pearson, K. | authorlink=Karl Pearson |year = 1901 | title = On Lines and Planes of Closest Fit to Systems of Points in Space | journal = Philosophical Magazine | volume = 2 | issue = 11 | pages = 559–572 | doi=10.1080/14786440109462720}}</ref> as an analogue of the [[principal axis theorem]] in mechanics; it was later independently developed and named by [[Harold Hotelling]] in the 1930s.<ref>Hotelling, H. (1933). Analysis of a complex of statistical variables into principal components. ''[[Journal of Educational Psychology]]'', '''24''', 417–441, and 498–520.<br> {{cite journal | last1 = Hotelling | first1 = H | year = 1936 | title = Relations between two sets of variates | url = | journal = [[Biometrika]] | volume = 28 | issue = 3/4| pages = 321–377 | doi=10.2307/2333955| jstor = 2333955 }}</ref> Depending on the field of application, it is also named the discrete [[Karhunen–Loève theorem|Karhunen–Loève]] transform (KLT) in [[signal processing]], the [[Harold Hotelling|Hotelling]] transform in multivariate quality control, proper orthogonal decomposition (POD) in mechanical engineering, [[singular value decomposition]] (SVD) of '''X''' (Golub and Van Loan, 1983), [[Eigendecomposition|eigenvalue decomposition]] (EVD) of '''X'''<sup>T</sup>'''X''' in linear algebra, [[factor analysis]] (for a discussion of the differences between PCA and factor analysis see Ch.&nbsp;7 of Jolliffe's ''Principal Component Analysis''),<ref name=\"Principal Component Analysis\">Jolliffe I.T. ''Principal Component Analysis'', Series: Springer Series in Statistics, 2nd ed., Springer, NY, 2002, XXIX, 487 p. 28 illus. {{isbn|978-0-387-95442-4}}</ref> [[Eckart–Young theorem]] (Harman, 1960), or  [[empirical orthogonal functions]] (EOF) in meteorological science, [[empirical eigenfunction decomposition]] (Sirovich, 1987), [[empirical component analysis]] (Lorenz, 1956), [[quasiharmonic modes]] (Brooks et al., 1988), [[Spectral theorem|spectral decomposition]] in noise and vibration, and [[Mode shape|empirical modal analysis]] in structural dynamics.\n\nPCA is mostly used as a tool in [[exploratory data analysis]] and for making [[predictive modeling|predictive models]]. It is often used to visualize genetic distance and relatedness between populations. PCA can be done by [[Eigendecomposition of a matrix|eigenvalue decomposition]] of a data [[covariance]] (or [[correlation]]) matrix or [[singular value decomposition]] of a [[Data matrix (multivariate statistics)|data matrix]], usually after a normalization step of the initial data. The normalization of each attribute consists of ''mean centering'' – subtracting each data value from its variable's measured mean so that its empirical mean (average) is zero – and, possibly, normalizing each variable's variance to make it equal to 1; see [[Z-score]]s.<ref>{{Cite journal|author1=Abdi. H.  |author2=Williams, L.J.  |lastauthoramp=yes | authorlink=AbdiWilliams | year = 2010 | title = Principal component analysis | journal = Wiley Interdisciplinary Reviews: Computational Statistics | volume = 2 | issue=4 | pages = 433–459 | doi = 10.1002/wics.101 |arxiv=1108.4372  }}</ref> The results of a PCA are usually discussed in terms of ''component scores'', sometimes called ''factor scores'' (the transformed variable values corresponding to a particular data point), and ''loadings'' (the weight by which each standardized original variable should be multiplied to get the component score).<ref>Shaw P.J.A. (2003) ''Multivariate statistics for the Environmental Sciences'', Hodder-Arnold. {{isbn|0-340-80763-6}}. {{Page needed|date=June 2011}}</ref> If component scores are standardized to unit variance, loadings must contain the data variance in them (and that is the magnitude of eigenvalues). If component scores are not standardized (therefore they contain the data variance) then loadings must be unit-scaled, (\"normalized\") and these weights are called eigenvectors; they are the cosines of orthogonal rotation of variables into principal components or back.\n\nPCA is the simplest of the true [[Eigenvectors|eigenvector]]-based multivariate analyses. Often, its operation can be thought of as revealing the internal structure of the data in a way that best explains the variance in the data. If a [[multivariate dataset]] is visualised as a set of coordinates in a high-[[Dimension (metadata)|dimensional]] data space (1 axis per variable), PCA can supply the user with a lower-dimensional picture, a projection of this object when viewed from its most informative viewpoint{{citation needed|date=May 2018}}. This is done by using only the first few principal components so that the dimensionality of the transformed data is reduced.\n\nPCA is closely related to [[factor analysis]].  Factor analysis typically incorporates more domain specific assumptions about the underlying structure and solves eigenvectors of a slightly different matrix.\n\nPCA is also related to [[Canonical correlation|canonical correlation analysis (CCA)]]. CCA defines coordinate systems that optimally describe the cross-covariance between two datasets while PCA defines a new [[orthogonal coordinate system]] that optimally describes variance in a single dataset.<ref>{{Cite journal|author1=Barnett, T. P. |author2=R. Preisendorfer.  |lastauthoramp=yes | title = Origins and levels of monthly and seasonal forecast skill for United States surface air temperatures determined by canonical correlation analysis | journal = Monthly Weather Review |volume=115 |issue=9  |pages=1825  |year= 1987 |doi=10.1175/1520-0493(1987)115<1825:oaloma>2.0.co;2}}</ref><ref>{{Cite journal| author = Hsu, Daniel, Sham M. Kakade, and Tong Zhang| title =A spectral algorithm for learning hidden markov models. | journal =  | arxiv = 0811.4413 |year= 2008| bibcode =2008arXiv0811.4413H}}</ref>\n\n== Intuition ==\nPCA can be thought of as fitting a ''p''-dimensional [[ellipsoid]] to the data, where each axis of the ellipsoid represents a principal component. If some axis of the ellipsoid is small, then the variance along that axis is also small, and by omitting that axis and its corresponding principal component from our representation of the dataset, we lose only a commensurately small amount of information.\n\nTo find the axes of the ellipsoid, we must first subtract the mean of each variable from the dataset to center the data around the origin. Then, we compute the [[covariance matrix]] of the data and calculate the eigenvalues and corresponding eigenvectors of this covariance matrix. Then we must normalize each of the orthogonal eigenvectors to become unit vectors. Once this is done, each of the mutually orthogonal, unit eigenvectors can be interpreted as an axis of the ellipsoid fitted to the data. This choice of basis will transform our covariance matrix into a diagonalised form with the diagonal elements representing the variance of each axis. The proportion of the variance that each eigenvector represents can be calculated by dividing the eigenvalue corresponding to that eigenvector by the sum of all eigenvalues.\n\nThis procedure is sensitive to the scaling of the data, and there is no consensus as to how to best scale the data to obtain optimal results.\n\n== Details ==\n\nPCA is mathematically defined as an [[orthogonal transformation|orthogonal]] [[linear transformation]] that transforms the data to a new [[coordinate system]] such that the greatest variance by some projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.<ref name=\"Principal Component Analysis\" />\n\nConsider a data [[Matrix (mathematics)|matrix]], '''X''', with column-wise zero [[empirical mean]] (the sample mean of each column has been shifted to zero), where each of the ''n'' rows represents a different repetition of the experiment, and each of the ''p'' columns gives a particular kind of feature (say, the results from a particular sensor).\n\nMathematically, the transformation is defined by a set of ''p''-dimensional vectors of weights or coefficients <math>\\mathbf{w}_{(k)} = (w_1, \\dots, w_p)_{(k)} </math> that map each row vector <math>\\mathbf{x}_{(i)}</math> of '''X''' to a new vector of principal component ''scores'' <math>\\mathbf{t}_{(i)} = (t_1, \\dots, t_l)_{(i)}</math>, given by\n\n:<math>{t_{k}}_{(i)} = \\mathbf{x}_{(i)} \\cdot \\mathbf{w}_{(k)} \\qquad \\mathrm{for} \\qquad i = 1,\\dots,n \\qquad k = 1,\\dots,l </math>\nin such a way that the individual variables <math>t_1, \\dots, t_l</math> of '''t''' considered over the data set successively inherit the maximum possible variance from '''x''', with each coefficient vector '''w''' constrained to be a [[unit vector]] (where <math>l</math> is usually selected to be less than <math>p</math> to reduce dimensionality).\n\n=== First component ===\nIn order to maximize variance, the first weight vector '''w'''<sub>(1)</sub> thus has to satisfy\n:<math>\\mathbf{w}_{(1)}\n = \\underset{\\Vert \\mathbf{w} \\Vert = 1}{\\operatorname{\\arg\\,max}}\\,\\left\\{ \\sum_i \\left(t_1\\right)^2_{(i)} \\right\\}\n = \\underset{\\Vert \\mathbf{w} \\Vert = 1}{\\operatorname{\\arg\\,max}}\\,\\left\\{ \\sum_i \\left(\\mathbf{x}_{(i)} \\cdot \\mathbf{w} \\right)^2 \\right\\}</math>\n\nEquivalently, writing this in matrix form gives\n:<math>\\mathbf{w}_{(1)}\n = \\underset{\\Vert \\mathbf{w} \\Vert = 1}{\\operatorname{\\arg\\,max}}\\, \\{ \\Vert \\mathbf{Xw} \\Vert^2 \\}\n = \\underset{\\Vert \\mathbf{w} \\Vert = 1}{\\operatorname{\\arg\\,max}}\\, \\left\\{ \\mathbf{w}^T \\mathbf{X^T} \\mathbf{X w} \\right\\}</math>\n\nSince '''w'''<sub>(1)</sub> has been defined to be a unit vector, it equivalently also satisfies\n:<math>\\mathbf{w}_{(1)} = {\\operatorname{\\arg\\,max}}\\, \\left\\{ \\frac{\\mathbf{w}^T\\mathbf{X^T} \\mathbf{X w}}{\\mathbf{w}^T \\mathbf{w}} \\right\\}</math>\n\nThe quantity to be maximised can be recognised as a [[Rayleigh quotient]]. A standard result for a [[positive semidefinite matrix]] such as  '''X'''<sup>T</sup>'''X''' is that the quotient's maximum possible value is the largest [[eigenvalue]] of the matrix, which occurs when '''''w''''' is the corresponding [[eigenvector]].\n\nWith '''w'''<sub>(1)</sub> found, the first principal component of a data vector '''x'''<sub>(''i'')</sub> can then be given as a score ''t''<sub>1(''i'')</sub> = '''x'''<sub>(''i'')</sub> ⋅ '''w'''<sub>(1)</sub> in the transformed co-ordinates, or as the corresponding vector in the original variables, {'''x'''<sub>(''i'')</sub> ⋅ '''w'''<sub>(1)</sub>} '''w'''<sub>(1)</sub>.\n\n=== Further components ===\n\nThe ''k''th component can be found by subtracting the first ''k''&nbsp;−&nbsp;1 principal components from '''X''':\n\n:<math>\\mathbf{\\hat{X}}_{k} = \\mathbf{X} - \\sum_{s = 1}^{k - 1} \\mathbf{X} \\mathbf{w}_{(s)} \\mathbf{w}_{(s)}^{\\rm T} </math>\n\nand then finding the weight vector which extracts the maximum variance from this new data matrix\n:<math>\\mathbf{w}_{(k)} = \\underset{\\Vert \\mathbf{w} \\Vert = 1}{\\operatorname{arg\\,max}} \\left\\{ \\Vert \\mathbf{\\hat{X}}_{k} \\mathbf{w} \\Vert^2 \\right\\} = {\\operatorname{\\arg\\,max}}\\, \\left\\{ \\tfrac{\\mathbf{w}^T\\mathbf{\\hat{X}}_{k}^T \\mathbf{\\hat{X}}_{k} \\mathbf{w}}{\\mathbf{w}^T \\mathbf{w}} \\right\\}</math>\n\nIt turns out that this gives the remaining eigenvectors of '''X'''<sup>T</sup>'''X''', with the maximum values for the quantity in brackets given by their corresponding eigenvalues. Thus the weight vectors are eigenvectors of '''X'''<sup>T</sup>'''X'''.\n\nThe ''k''th principal component of a data vector '''x'''<sub>(''i'')</sub> can therefore be given as a score ''t''<sub>''k''(''i'')</sub> = '''x'''<sub>(''i'')</sub> ⋅ '''w'''<sub>(''k'')</sub> in the transformed co-ordinates, or as the corresponding vector in the space of the original variables, {'''x'''<sub>(''i'')</sub> ⋅ '''w'''<sub>(''k'')</sub>} '''w'''<sub>(''k'')</sub>, where '''w'''<sub>(''k'')</sub> is the ''k''th eigenvector of '''X'''<sup>T</sup>'''X'''.\n\nThe full principal components decomposition of '''X''' can therefore be given as\n:<math>\\mathbf{T} = \\mathbf{X} \\mathbf{W}</math>\nwhere '''W''' is a ''p''-by-''p'' matrix of weights whose columns are the eigenvectors of '''X'''<sup>T</sup>'''X'''.  The transpose of '''W''' is sometimes called the [[whitening transformation|whitening or sphering transformation]]. Columns of '''W''' multiplied by the square root of corresponding eigenvalues, i.e. eigenvectors scaled up by the variances, are called ''loadings'' in PCA or in Factor analysis.\n\n=== Covariances ===\n\n'''X'''<sup>T</sup>'''X''' itself can be recognised as proportional to the empirical sample [[covariance matrix]] of the dataset '''X'''.\n\nThe sample covariance ''Q'' between two of the different principal components over the dataset is given by:\n\n:<math>\\begin{align}\nQ(\\mathrm{PC}_{(j)}, \\mathrm{PC}_{(k)}) & \\propto (\\mathbf{X}\\mathbf{w}_{(j)})^T (\\mathbf{X}\\mathbf{w}_{(k)}) \\\\\n& = \\mathbf{w}_{(j)}^T \\mathbf{X}^T \\mathbf{X} \\mathbf{w}_{(k)} \\\\\n& = \\mathbf{w}_{(j)}^T \\lambda_{(k)} \\mathbf{w}_{(k)} \\\\\n& = \\lambda_{(k)} \\mathbf{w}_{(j)}^T \\mathbf{w}_{(k)}\n\\end{align}</math>\n\nwhere the eigenvalue property of '''w'''<sub>(''k'')</sub> has been used to move from line 2 to line 3.  However eigenvectors '''w'''<sub>(''j'')</sub> and '''w'''<sub>(''k'')</sub> corresponding to eigenvalues of a symmetric matrix are orthogonal (if the eigenvalues are different), or can be orthogonalised (if the vectors happen to share an equal repeated value).  The product in the final line is therefore zero; there is no sample covariance between different principal components over the dataset.\n\nAnother way to characterise the principal components transformation is therefore as the transformation to coordinates which diagonalise the empirical sample covariance matrix.\n\nIn matrix form, the empirical covariance matrix for the original variables can be written\n:<math>\\mathbf{Q} \\propto \\mathbf{X}^T \\mathbf{X} = \\mathbf{W} \\mathbf{\\Lambda} \\mathbf{W}^T</math>\n\nThe empirical covariance matrix between the principal components becomes\n:<math>\\mathbf{W}^T \\mathbf{Q} \\mathbf{W} \\propto \\mathbf{W}^T \\mathbf{W} \\, \\mathbf{\\Lambda} \\, \\mathbf{W}^T \\mathbf{W}\n=  \\mathbf{\\Lambda} </math>\n\nwhere '''Λ''' is the diagonal matrix of eigenvalues ''λ''<sub>(''k'')</sub> of '''X'''<sup>T</sup>'''X'''. Note that λ<sub>(k)</sub> is equal to the sum of the squares over the dataset associated with each component ''k'', i.e. ''λ''<sub>(''k'')</sub> = Σ<sub>''i''</sub> ''t''<sub>''k''</sub><sup>2</sup><sub>(''i'')</sub> = Σ<sub>''i''</sub> ('''x'''<sub>(''i'')</sub> ⋅ '''w'''<sub>(''k'')</sub>)<sup>2</sup>.\n\n=== Dimensionality reduction ===\nThe transformation '''T''' = '''X''' '''W''' maps a data vector '''x'''<sub>(''i'')</sub> from an original space of ''p'' variables to a new space of ''p'' variables which are uncorrelated over the dataset.  However, not all the principal components need to be kept.  Keeping only the first ''L'' principal components, produced by using only the first ''L'' eigenvectors, gives the truncated transformation\n\n:<math>\\mathbf{T}_L = \\mathbf{X} \\mathbf{W}_L</math>\n\nwhere the matrix '''T'''<sub>L</sub> now has ''n'' rows but only ''L'' columns. In other words, PCA learns a linear transformation <math> t = W^T x, x \\in R^p, t \\in R^L,</math> where the columns of {{math|''p'' × ''L''}} matrix ''W'' form an orthogonal basis for the ''L'' features (the components of representation ''t'') that are decorrelated.<ref>{{Cite journal| author = Bengio, Y.|year = 2013| title = Representation Learning: A Review and New Perspectives | journal = IEEE Transactions on Pattern Analysis and Machine Intelligence | volume = 35 | issue = 8 | pages = 1798–1828| doi=10.1109/TPAMI.2013.50|display-authors=etal|arxiv=1206.5538}}</ref> By construction, of all the transformed data matrices with only ''L'' columns, this score matrix maximises the variance in the original data that has been preserved, while minimising the total squared reconstruction error <math>\\|\\mathbf{T}\\mathbf{W}^T - \\mathbf{T}_L\\mathbf{W}^T_L\\|_2^2</math> or <math>\\|\\mathbf{X} - \\mathbf{X}_L\\|_2^2</math>.\n\n[[File:PCA of Haplogroup J using 37 STRs.png|thumb|right|A principal components analysis scatterplot of [[Y-STR]] [[haplotype]]s calculated from repeat-count values for 37 Y-chromosomal STR markers from 354 individuals.<br />  PCA has successfully found linear combinations of the different markers, that separate out different clusters corresponding to different lines of individuals' Y-chromosomal genetic descent.]]\nSuch [[dimensionality reduction]] can be a very useful step for visualising and processing high-dimensional datasets, while still retaining as much of the variance in the dataset as possible.  For example, selecting ''L''&nbsp;=&nbsp;2 and keeping only the first two principal components finds the two-dimensional plane through the high-dimensional dataset in which the data is most spread out, so if the data contains [[Cluster analysis|clusters]] these too may be most spread out, and therefore most visible to be plotted out in a two-dimensional diagram; whereas if two directions through the data (or two of the original variables) are chosen at random, the clusters may be much less spread apart from each other, and may in fact be much more likely to substantially overlay each other, making them indistinguishable.\n\nSimilarly, in [[regression analysis]], the larger the number of [[explanatory variable]]s allowed, the greater is the chance of [[overfitting]] the model, producing conclusions that fail to generalise to other datasets.  One approach, especially when there are strong correlations between different possible explanatory variables, is to reduce them to a few principal components and then run the regression against them, a method called [[principal component regression]].\n\nDimensionality reduction may also be appropriate when the variables in a dataset are noisy.  If each column of the dataset contains independent identically distributed Gaussian noise, then the columns of '''T''' will also contain similarly identically distributed Gaussian noise (such a distribution is invariant under the effects of the matrix '''W''', which can be thought of as a high-dimensional rotation of the co-ordinate axes).  However, with more of the total variance concentrated in the first few principal components compared to the same noise variance, the proportionate effect of the noise is less—the first few components achieve a higher [[signal-to-noise ratio]].  PCA thus can have the effect of concentrating much of the signal into the first few principal components, which can usefully be captured by dimensionality reduction; while the later principal components may be dominated by noise, and so disposed of without great loss.\n\n=== Singular value decomposition ===\nThe principal components transformation can also be associated with another matrix factorization, the [[singular value decomposition]] (SVD) of '''X''',\n:<math>\\mathbf{X} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{W}^T</math>\nHere '''Σ''' is an ''n''-by-''p'' [[Diagonal matrix|rectangular diagonal matrix]] of positive numbers ''σ''<sub>(''k'')</sub>, called the singular values of '''X'''; '''U''' is an ''n''-by-''n'' matrix, the columns of which are orthogonal unit vectors of length ''n'' called the left singular vectors of '''X'''; and '''W''' is a ''p''-by-''p'' whose columns are orthogonal unit vectors of length ''p'' and called the right singular vectors of '''X'''.\n\nIn terms of this factorization, the matrix '''X'''<sup>T</sup>'''X''' can be written\n:<math>\\begin{align}\n\\mathbf{X}^T\\mathbf{X} & = \\mathbf{W}\\mathbf{\\Sigma}^T \\mathbf{U}^T \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{W}^T \\\\\n                       & = \\mathbf{W}\\mathbf{\\Sigma}^T \\mathbf{\\Sigma} \\mathbf{W}^T \\\\\n                       & = \\mathbf{W}\\mathbf{\\hat{\\Sigma}}^2 \\mathbf{W}^T\n\n\\end{align}</math>\n\nwhere ''' <math> \\mathbf{\\hat{\\Sigma}} </math>''' is the square diagonal matrix with the singular values of ''' X ''' and the excess zeros chopped off that satisfies ''' <math> \\mathbf{\\hat{\\Sigma}^2}=\\mathbf{\\Sigma}^T \\mathbf{\\Sigma} </math>'''. Comparison with the eigenvector factorization of '''X'''<sup>T</sup>'''X''' establishes that the right singular vectors '''W''' of '''X''' are equivalent to the eigenvectors of '''X'''<sup>T</sup>'''X''', while the singular values ''σ''<sub>(''k'')</sub> of ''' <math> \\mathbf{{X}}</math>''' are equal to the square-root of the eigenvalues ''λ''<sub>(''k'')</sub> of '''X'''<sup>T</sup>'''X'''.\n\nUsing the singular value decomposition the score matrix '''T''' can be written\n:<math>\\begin{align}\n\\mathbf{T} & = \\mathbf{X} \\mathbf{W} \\\\\n           & = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{W}^T \\mathbf{W} \\\\\n           & = \\mathbf{U}\\mathbf{\\Sigma}\n\\end{align}</math>\nso each column of '''T''' is given by one of the left singular vectors of '''X''' multiplied by the corresponding singular value. This form is also the [[polar decomposition]] of '''T'''.\n\nEfficient algorithms exist to calculate the SVD of '''X''' without having to form the matrix '''X'''<sup>T</sup>'''X''', so computing the SVD is now the standard way to calculate a principal components analysis from a data matrix{{citation needed|reason=A reference to SVD algorithms that outperform eigendecomposition of the covariance matrix is needed, particularly in cases where n is greater than p|date=August 2014}}, unless only a handful of components are required.\n\nAs with the eigen-decomposition, a truncated {{math|''n'' × ''L''}} score matrix '''T'''<sub>L</sub> can be obtained by considering only the first L largest singular values and their singular vectors:\n:<math>\\mathbf{T}_L = \\mathbf{U}_L\\mathbf{\\Sigma}_L = \\mathbf{X} \\mathbf{W}_L </math>\nThe truncation of a matrix '''M''' or '''T''' using a truncated singular value decomposition in this way produces a truncated matrix that is the nearest possible matrix of [[Rank (linear algebra)|rank]] ''L'' to the original matrix, in the sense of the difference between the two having the smallest possible [[Frobenius norm]], a result known as the Eckart–Young theorem [1936].\n\n== Further considerations ==\n\nGiven a set of points in [[Euclidean space]], the first principal component corresponds to a line that passes through the multidimensional mean and minimizes the sum of squares of the distances of the points from the line. The second principal component corresponds to the same concept after all correlation with the first principal component has been subtracted from the points. The singular values (in '''Σ''') are the square roots of the [[eigenvalue]]s of the matrix '''X'''<sup>T</sup>'''X'''. Each eigenvalue is proportional to the portion of the \"variance\" (more correctly of the sum of the squared distances of the points from their multidimensional mean) that is associated with each eigenvector. The sum of all the eigenvalues is equal to the sum of the squared distances of the points from their multidimensional mean. PCA essentially rotates the set of points around their mean in order to align with the principal components. This moves as much of the variance as possible (using an orthogonal transformation) into the first few dimensions. The values in the remaining dimensions, therefore, tend to be small and may be dropped with minimal loss of information (see [[Principle Component Analysis#PCA and information theory|below]]). PCA is often used in this manner for [[dimensionality reduction]]. PCA has the distinction of being the optimal orthogonal transformation for keeping the subspace that has largest \"variance\" (as defined above). This advantage, however, comes at the price of greater computational requirements if compared, for example, and when applicable, to the [[discrete cosine transform]], and in particular to the DCT-II which is simply known as the \"DCT\". [[Nonlinear dimensionality reduction]] techniques tend to be more computationally demanding than PCA.\n\nPCA is sensitive to the scaling of the variables. If we have just two variables and they have the same [[sample variance]] and are positively correlated, then the PCA will entail a rotation by 45° and the \"weights\" (they are the cosines of rotation) for the two variables with respect to the principal component will be equal. But if we multiply all values of the first variable by 100, then the first principal component will be almost the same as that variable, with a small contribution from the other variable, whereas the second component will be almost aligned with the second original variable. This means that whenever the different variables have different units (like temperature and mass), PCA is a somewhat arbitrary method of analysis. (Different results would be obtained if one used Fahrenheit rather than Celsius for example.) Note that Pearson's original paper was entitled \"On Lines and Planes of Closest Fit to Systems of Points in Space\" – \"in space\" implies physical Euclidean space where such concerns do not arise. One way of making the PCA less arbitrary is to use variables scaled so as to have unit variance, by standardizing the data and hence use the autocorrelation matrix instead of the autocovariance matrix as a basis for PCA. However, this compresses (or expands) the fluctuations in all dimensions of the signal space to unit variance.\n\nMean subtraction (a.k.a. \"mean centering\") is necessary for performing classical PCA to ensure that the first principal component describes the direction of maximum variance. If mean subtraction is not performed, the first principal component might instead correspond more or less to the mean of the data. A mean of zero is needed for finding a basis that minimizes the [[Minimum mean square error|mean square error]] of the approximation of the data.<ref>A. A. Miranda, Y. A. Le Borgne, and G. Bontempi. [http://www.ulb.ac.be/di/map/yleborgn/pub/NPL_PCA_07.pdf New Routes from Minimal Approximation Error to Principal Components], Volume 27, Number 3 / June, 2008, Neural Processing Letters, Springer</ref>\n\nMean-centering is unnecessary if performing a principal components analysis on a correlation matrix, as the data are already centered after calculating correlations. Correlations are derived from the cross-product of two standard scores (Z-scores) or statistical moments (hence the name: ''Pearson Product-Moment Correlation''). Also see the article by Kromrey & Foster-Johnson (1998) on ''\"Mean-centering in Moderated Regression: Much Ado About Nothing\".''\n\nAn [[autoencoder]] [[Artificial neural network|neural network]] with a linear hidden layer is similar to PCA. Upon convergence, the weight vectors of the ''K'' neurons in the hidden layer will form a basis for the space spanned by the first ''K'' principal components. Unlike PCA, this technique will not necessarily produce [[orthogonal]] vectors, yet the principal components can easily be recovered from them using singular value decomposition.<ref>{{cite arxiv |last1= Plaut |first1=E |title= From Principal Subspaces to Principal Components with Linear Autoencoders |eprint=1804.10253 |date=2018|class=stat.ML }}</ref>\n\nPCA is a popular primary technique in [[pattern recognition]]. It is not, however, optimized for class separability.<ref>{{Cite book| author=Fukunaga, Keinosuke | title = Introduction to Statistical Pattern Recognition |publisher=Elsevier | year = 1990 | url=https://books.google.com/books?visbn=0-12-269851-7| isbn=978-0-12-269851-4}}</ref> However, it has been used to quantify the distance between two or more classes by calculating center of mass for each class in principal component space and reporting Euclidean distance between center of mass of two or more classes.<ref>{{cite journal|last1=Alizadeh|first1=Elaheh|last2=Lyons|first2=Samanthe M|last3=Castle|first3=Jordan M|last4=Prasad|first4=Ashok|title=Measuring systematic changes in invasive cancer cell shape using Zernike moments|journal=Integrative Biology|date=2016|volume=8|issue=11|pages=1183–1193|doi=10.1039/C6IB00100A|pmid=27735002|url=http://pubs.rsc.org/en/Content/ArticleLanding/2016/IB/C6IB00100A#!divAbstract}}</ref> The [[linear discriminant analysis]] is an alternative which is optimized for class separability.\n\n== Table of symbols and abbreviations ==\n\n{| class=\"wikitable\"\n|-\n! Symbol\n! Meaning\n! Dimensions\n! Indices\n|-\n| <math>\\mathbf{X} = \\{ X_{ij} \\}</math>\n| data matrix, consisting of the set of all data vectors, one vector per row\n| <math> n \\times p</math>\n| <math> i = 1 \\ldots n </math> <br /> <math> j = 1 \\ldots p </math>\n|-\n| <math>n \\,</math>\n| the number of row vectors in the data set\n| <math>1 \\times 1</math>\n| ''scalar''\n|-\n| <math>p \\,</math>\n| the number of elements in each row vector (dimension)\n| <math>1 \\times 1</math>\n| ''scalar''\n|-\n| <math>L \\,</math>\n| the number of dimensions in the dimensionally reduced subspace, <math> 1 \\le L \\le p </math>\n| <math>1 \\times 1</math>\n| ''scalar''\n|-\n| <math>\\mathbf{u} = \\{ u_j \\}</math>\n| vector of empirical [[mean]]s, one mean for each column ''j'' of the data matrix\n| <math> p \\times 1</math>\n| <math> j = 1 \\ldots p </math>\n|-\n| <math>\\mathbf{s} = \\{ s_j \\}</math>\n| vector of empirical [[standard deviation]]s, one standard deviation for each column ''j'' of the data matrix\n| <math> p \\times 1</math>\n| <math> j = 1 \\ldots p </math>\n|-\n| <math>\\mathbf{h} = \\{ h_i \\}</math>\n| vector of all 1's\n| <math> 1 \\times n</math>\n| <math> i = 1 \\ldots n </math>\n|-\n| <math>\\mathbf{B} = \\{ B_{ij} \\}</math>\n| [[Standard deviation|deviations]] from the mean of each column ''j'' of the data matrix\n| <math> n \\times p</math>\n| <math> i = 1 \\ldots n </math> <br /> <math> j = 1 \\ldots p </math>\n|-\n| <math>\\mathbf{Z} = \\{ Z_{ij} \\} </math>\n| [[z-score]]s, computed using the mean and standard deviation for each row ''m'' of the data matrix\n| <math> n \\times p</math>\n| <math> i = 1 \\ldots n </math> <br /> <math> j = 1 \\ldots p </math>\n|-\n| <math>\\mathbf{C} = \\{ C_{jj'} \\} </math>\n| [[covariance matrix]]\n| <math> p \\times p </math>\n| <math> j = 1 \\ldots p </math> <br /><math> j' = 1 \\ldots p </math>\n|-\n| <math>\\mathbf{R} = \\{ R_{jj'} \\} </math>\n| [[correlation matrix]]\n| <math> p \\times p </math>\n| <math> j = 1 \\ldots p </math> <br /><math> j' = 1 \\ldots p </math>\n|-\n| <math> \\mathbf{V} = \\{ V_{jj'} \\} </math>\n| matrix consisting of the set of all [[eigenvectors]] of '''C''', one eigenvector per column\n| <math> p \\times p </math>\n| <math> j = 1 \\ldots p </math> <br /><math> j' = 1 \\ldots p </math>\n|-\n| <math>\\mathbf{D} = \\{ D_{jj'} \\} </math>\n| [[diagonal matrix]] consisting of the set of all [[eigenvalues]] of '''C''' along its [[principal diagonal]], and 0 for all other elements\n| <math> p \\times p </math>\n| <math> j = 1 \\ldots p </math> <br /><math> j' = 1 \\ldots p </math>\n|-\n| <math>\\mathbf{W} = \\{ W_{jl} \\} </math>\n| matrix of basis vectors, one vector per column, where each basis vector is one of the eigenvectors of '''C''', and where the vectors in '''W''' are a sub-set of those in '''V'''\n| <math> p \\times L</math>\n| <math> j = 1 \\ldots p </math> <br /><math> l = 1 \\ldots L</math>\n|-\n| <math>\\mathbf{T} = \\{ T_{il} \\} </math>\n| matrix consisting of ''n'' row vectors, where each vector is the projection of the corresponding data vector from matrix '''X''' onto the basis vectors contained in the columns of matrix '''W'''.\n| <math> n \\times L</math>\n| <math> i = 1 \\ldots n </math> <br /><math> l = 1 \\ldots L</math>\n|}\n\n== Properties and limitations of PCA ==\n\n=== Properties ===\n\nSome properties of PCA include:<ref>Jolliffe, I. T. (2002). ''Principal Component Analysis,'' second edition Springer-Verlag. {{isbn|978-0-387-95442-4}}.</ref>\n\n:<big>'''''Property 1'':'''</big> For any integer ''q, 1 ≤ q ≤ p,'' consider the orthogonal [[linear transformation]]\n::<math>y =\\mathbf{B'}x</math>\n:where <math>y</math> is a ''q-element'' vector and <math>\\mathbf{B'}</math> is a ''(q × p)'' matrix, and let <math>\\mathbf{{\\Sigma}}_y = \\mathbf{B'}\\mathbf{\\Sigma}\\mathbf{B}</math> be the [[variance]]-[[covariance]] matrix for <math>y</math>. Then the trace of <math>\\mathbf{\\Sigma}_y</math>, denoted <math>\\text{tr} (\\mathbf{\\Sigma}_y)</math>, is maximized by taking <math>\\mathbf{B} = \\mathbf{A}_q</math>, where <math>\\mathbf{A}_q</math> consists of the first ''q'' columns of <math>\\mathbf{A}</math> <math>(\\mathbf{B'}</math> is the transposition of <math>\\mathbf{B})</math>.\n\n:<big>'''''Property 2'':'''</big> Consider again the [[orthonormal transformation]]\n::<math>y = \\mathbf{B'}x</math>\n:with <math>x, \\mathbf{B}, \\mathbf{A}</math> and <math>\\mathbf{\\Sigma}_y</math> defined as before. Then <math>\\text{tr}(\\mathbf{\\Sigma}_y)</math> is minimized by taking <math>\\mathbf{B} = \\mathbf{A}_q^*,</math> where <math>\\mathbf{A}_q^*</math> consists of the last ''q'' columns of <math>\\mathbf{A}</math>.\n\nThe statistical implication of this property is that the last few PCs are not simply unstructured left-overs after removing the important PCs. Because these last PCs have variances as small as possible they are useful in their own right. They can help to detect unsuspected near-constant linear relationships between the elements of {{mvar|x}}, and they may also be useful in [[regression analysis|regression]], in selecting a subset of variables from {{mvar|x}}, and in outlier detection.\n\n:<big>'''''Property 3'':'''</big> (Spectral Decomposition of {{math|'''Σ'''}})\n::<math>\\mathbf{{\\Sigma}} = \\lambda_{1}\\alpha_{1}\\alpha_{1}' + \\cdots + \\lambda_{p}\\alpha_{p}\\alpha_{p}'</math>\n\nBefore we look at its usage, we first look at [[diagonal]] elements,\n:<math>\\text{Var}(x_j) = \\sum_{k=1}^P \\lambda_{k}\\alpha_{kj}^2</math>\nThen, perhaps the main statistical implication of the result is that not only can we decompose the combined variances of all the elements of {{mvar|x}} into decreasing contributions due to each PC, but we can also decompose the whole [[covariance matrix]] into contributions <math>\\lambda_{k}\\alpha_{k}\\alpha_{k}'</math> from each PC. Although not strictly decreasing, the elements of <math>\\lambda_{k}\\alpha_{k}\\alpha_{k}'</math> will tend to become smaller as <math>k</math> increases, as <math>\\lambda_{k}\\alpha_{k}\\alpha_{k}'</math> is nonincreasing for increasing <math>k</math>, whereas the elements of <math>\\alpha_{k}</math> tend to stay about the same size because of the normalization constraints: <math>\\alpha_{k}'\\alpha_{k}=1, k=1,\\cdots, p</math>.\n\n=== Limitations ===\n\nAs noted above, the results of PCA depend on the scaling of the variables. This can be cured by scaling each feature by its standard deviation, so that one ends up with dimensionless features with unital variance <ref name=Leznik>Leznik, M; Tofallis, C. 2005 [https://uhra.herts.ac.uk/bitstream/handle/2299/715/S56.pdf Estimating Invariant Principal Components Using Diagonal Regression.]</ref>\n\nThe applicability of PCA as described above is limited by certain (tacit) assumptions <ref>Jonathon Shlens, [https://arxiv.org/abs/1404.1100 A Tutorial on Principal Component Analysis.]</ref> made in its derivation. In particular, PCA can capture linear correlations between the features but fails when this assumption is violated (see Figure 6a in the reference). In some cases, coordinate transformations can restore the linearity assumption and PCA can then be applied (see [[Kernel principal component analysis|kernel PCA]]).\n\nAnother limitation is the mean-removal process before constructing the covariance matrix for PCA. In fields such as astronomy, all the signals are non-negative, and the mean-removal process will force the mean of some astrophysical exposures to be zero, which consequently creates unphysical negative fluxes,<ref name=\"soummer12\"/> and forward modeling has to be performed to recover the true magnitude of the signals.<ref name=\"pueyo16\">{{Cite journal|arxiv= 1604.06097 |last1= Pueyo|first1= Laurent |title= Detection and Characterization of Exoplanets using Projections on Karhunen Loeve Eigenimages: Forward Modeling |journal= The Astrophysical Journal |volume= 824|issue= 2|pages= 117|year= 2016|doi= 10.3847/0004-637X/824/2/117|bibcode = 2016ApJ...824..117P}}</ref> As an alternative method, [[non-negative matrix factorization]] focusing only on the non-negative elements in the matrices, which is well-suited for astrophysical observations.<ref name=\"blantonRoweis07\"/><ref name=\"zhu16\"/><ref name=\"ren18\"/> See more at [[Principal component analysis#Non-negative matrix factorization|Relation between PCA and Non-negative Matrix Factorization]].\n\n=== PCA and information theory ===\nDimensionality reduction loses information, in general. PCA-based dimensionality reduction tends to minimize that information loss, under certain signal and noise models.\n\nUnder the assumption that\n\n:<math>\\mathbf{x}=\\mathbf{s}+\\mathbf{n}</math>\n\ni.e., that the data vector <math>\\mathbf{x}</math> is the sum of the desired information-bearing signal <math>\\mathbf{s}</math> and a noise signal <math>\\mathbf{n}</math> one can show that PCA can be optimal for dimensionality reduction, from an information-theoretic point-of-view.\n\nIn particular, Linsker showed that if <math>\\mathbf{s}</math> is Gaussian and <math>\\mathbf{n}</math> is Gaussian noise with a covariance matrix proportional to the identity matrix, the PCA maximizes the [[mutual information]] <math>I(\\mathbf{y};\\mathbf{s})</math> between the desired information <math>\\mathbf{s}</math> and the dimensionality-reduced output <math>\\mathbf{y}=\\mathbf{W}_L^T\\mathbf{x}</math>.<ref>{{cite journal|last=Linsker|first=Ralph|title=Self-organization in a perceptual network|journal=IEEE Computer|date=March 1988|volume=21|issue=3|pages=105–117|doi=10.1109/2.36}}</ref>\n\nIf the noise is still Gaussian and has a covariance matrix proportional to the identity matrix (i.e., the components of the vector <math>\\mathbf{n}</math> are [[iid]]), but the information-bearing signal <math>\\mathbf{s}</math> is non-Gaussian (which is a common scenario), PCA at least minimizes an upper bound on the ''information loss'', which is defined as<ref>{{cite book|last=Deco & Obradovic|title=An Information-Theoretic Approach to Neural Computing|year=1996|publisher=Springer|location=New York, NY}}</ref><ref>{{cite journal|last=Plumbley|first=Mark|title=Information theory and unsupervised neural networks|year=1991}}Tech Note</ref>\n\n:<math>I(\\mathbf{x};\\mathbf{s})-I(\\mathbf{y};\\mathbf{s}).</math>\n\nThe optimality of PCA is also preserved if the noise <math>\\mathbf{n}</math> is iid and at least more Gaussian (in terms of the [[Kullback–Leibler divergence]]) than the information-bearing signal <math>\\mathbf{s}</math>.<ref>{{cite journal|last=Geiger|first=Bernhard|author2=Kubin, Gernot|title=Signal Enhancement as Minimization of Relevant Information Loss|journal=Proc. ITG Conf. On Systems, Communication and Coding|date=January 2013|arxiv=1205.6935|bibcode=2012arXiv1205.6935G}}</ref> In general, even if the above signal model holds, PCA loses its information-theoretic optimality as soon as the noise <math>\\mathbf{n}</math> becomes dependent.\n\n== Computing PCA using the covariance method ==\n\nThe following is a detailed description of PCA using the covariance method (see also [http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf here]) as opposed to the correlation method.<ref>{{cite web|title=Engineering Statistics Handbook Section 6.5.5.2|url=http://www.itl.nist.gov/div898/handbook/pmc/section5/pmc552.htm|accessdate=19 January 2015}}</ref>\n\nThe goal is to transform a given data set '''X''' of dimension ''p'' to an alternative data set '''Y''' of smaller dimension ''L''. Equivalently, we are seeking to find the matrix '''Y''', where '''Y''' is the  [[Karhunen–Loève theorem|Karhunen–Loève]] transform (KLT) of matrix '''X''':\n\n:<math> \\mathbf{Y} = \\mathbb{KLT} \\{ \\mathbf{X} \\} </math>\n\n=== Organize the data set ===\n\n'''Suppose''' you have data comprising a set of observations of ''p'' variables, and you want to reduce the data so that each observation can be described with only ''L'' variables, ''L'' < ''p''.  Suppose further, that the data are arranged as a set of ''n'' data vectors <math>\\mathbf{x}_1 \\ldots \\mathbf{x}_n</math> with each <math>\\mathbf{x}_i </math> representing a single grouped observation of the ''p'' variables.\n* Write <math>\\mathbf{x}_1 \\ldots \\mathbf{x}_n</math> as row vectors, each of which has ''p'' columns.\n* Place the row vectors into a single matrix '''X''' of dimensions ''n'' × ''p''.\n\n=== Calculate the empirical mean ===\n* Find the empirical mean along each column ''j'' = 1,&nbsp;...,&nbsp;''p''.\n* Place the calculated mean values into an empirical mean vector '''u''' of dimensions ''p'' × 1.\n::<math>u_j = {1 \\over n} \\sum_{i=1}^n X_{ij} </math>\n\n=== Calculate the deviations from the mean ===\nMean subtraction is an integral part of the solution towards finding a principal component basis that minimizes the mean square error of approximating the data.<ref>A.A. Miranda, Y.-A. Le Borgne, and G. Bontempi. [http://www.ulb.ac.be/di/map/yleborgn/pub/NPL_PCA_07.pdf New Routes from Minimal Approximation Error to Principal Components], Volume 27, Number 3 / June, 2008, Neural Processing Letters, Springer</ref> Hence we proceed by centering the data as follows:\n* Subtract the empirical mean vector  <math> \\mathbf{u}^{T} </math> from each row of the data matrix '''X'''.\n* Store mean-subtracted data in the ''n'' × ''p'' matrix '''B'''.\n\n::<math>\\mathbf{B} = \\mathbf{X} - \\mathbf{h}\\mathbf{u}^{T} </math>\n:: where '''h''' is an {{math|''n'' × 1}} column vector of all&nbsp;1s:\n\n:::<math>h_i = 1 \\, \\qquad \\qquad \\text{for } i = 1, \\ldots, n </math>\n\n=== Find the covariance matrix ===\n* Find the ''p'' × ''p'' empirical [[covariance matrix]] '''C''' from matrix '''B''':\n::<math>\\mathbf{C} = { 1 \\over {n-1} } \\mathbf{B}^{*} \\mathbf{B}</math>\n:where <math> *</math> is the [[conjugate transpose]] operator. Note that if '''B''' consists entirely of real numbers, which is the case in many applications, the \"conjugate transpose\" is the same as the regular [[transpose]].\n* The reasoning behind using {{math|''n'' − 1}} instead of ''n'' to calculate the covariance is [[Bessel's correction]]\n\n=== Find the eigenvectors and eigenvalues of the covariance matrix ===\n* Compute the matrix '''V''' of [[eigenvector]]s which [[diagonalizable matrix|diagonalizes]] the covariance matrix '''C''':\n\n::<math>\\mathbf{V}^{-1} \\mathbf{C} \\mathbf{V} = \\mathbf{D} </math>\n\n: where '''D''' is the [[diagonal matrix]] of [[eigenvalue]]s of '''C'''. This step will typically involve the use of a computer-based algorithm for computing eigenvectors and eigenvalues. These algorithms are readily available as sub-components of most [[matrix algebra]] systems, such as [[SAS (software)|SAS]],<ref>{{Cite web | url=http://support.sas.com/documentation/cdl/en/statug/63962/HTML/default/viewer.htm#statug_princomp_sect001.htm | title=SAS/STAT(R) 9.3 User's Guide}}</ref> [[R (programming language)|R]], [[MATLAB]],<ref>[http://www.mathworks.com/access/helpdesk/help/techdoc/ref/eig.html#998306 eig function] Matlab documentation</ref><ref>[http://www.mathworks.com/matlabcentral/fileexchange/24634 MATLAB PCA-based Face recognition software]</ref> [[Mathematica]],<ref>[http://reference.wolfram.com/mathematica/ref/Eigenvalues.html Eigenvalues function] Mathematica documentation</ref> [[SciPy]], [[IDL (programming language)|IDL]] ([[Interactive Data Language]]), or [[GNU Octave]] as well as [[OpenCV]].\n* Matrix '''D''' will take the form of an ''p'' × ''p'' diagonal matrix, where\n::<math>D_{kl} = \\lambda_k \\qquad \\text{for } k = l</math>\n:is the ''j''th eigenvalue of the covariance matrix '''C''', and\n::<math>D_{kl} = 0 \\qquad \\text{for } k \\ne l.</math>\n* Matrix '''V''', also of dimension ''p'' × ''p'', contains ''p'' column vectors, each of length ''p'', which represent the ''p'' eigenvectors of the covariance matrix '''C'''.\n* The eigenvalues and eigenvectors are ordered and paired. The ''j''th eigenvalue corresponds to the ''j''th eigenvector.\n* Matrix '''V''' denotes the matrix of ''right'' eigenvectors (as opposed to ''left'' eigenvectors). In general, the matrix of right eigenvectors need ''not'' be the (conjugate) transpose of the matrix of left eigenvectors.\n\n=== Rearrange the eigenvectors and eigenvalues ===\n* Sort the columns of the eigenvector matrix '''V''' and eigenvalue matrix '''D''' in order of ''decreasing'' eigenvalue.\n* Make sure to maintain the correct pairings between the columns in each matrix.\n\n=== Compute the cumulative energy content for each eigenvector ===\n* The eigenvalues represent the distribution of the source data's energy{{Clarify|date=March 2011}} among each of the eigenvectors, where the eigenvectors form a [[basis (linear algebra)|basis]] for the data. The cumulative energy content ''g'' for the ''j''th eigenvector is the sum of the energy content across all of the eigenvalues from 1 through ''j'':\n\n::<math>g_j = \\sum_{k=1}^j D_{kk} \\qquad \\mathrm{for} \\qquad j = 1,\\dots,p </math>{{Citation needed|date=March 2011}}\n\n=== Select a subset of the eigenvectors as basis vectors ===\n* Save the first ''L'' columns of '''V''' as the ''p'' × ''L'' matrix '''W''':\n\n::<math> W_{kl} = V_{kl} \\qquad \\mathrm{for} \\qquad k = 1,\\dots,p \\qquad l = 1,\\dots,L </math>\n\n: where\n\n::<math>1 \\leq L \\leq p.</math>\n* Use the vector '''g''' as a guide in choosing an appropriate value for ''L''. The goal is to choose a value of ''L'' as small as possible while achieving a reasonably high value of ''g'' on a percentage basis. For example, you may want to choose ''L'' so that the cumulative energy ''g'' is above a certain threshold, like 90 percent. In this case, choose the smallest value of ''L'' such that\n\n::<math> \\frac{g_L}{g_p} \\ge 0.9\\, </math>\n\n=== Project the z-scores of the data onto the new basis ===\n* The projected vectors are the columns of the matrix\n::<math> \\mathbf{T} =  \\mathbf{Z} \\cdot \\mathbf{W} = \\mathbb{KLT} \\{ \\mathbf{X} \\}.</math>\n* The rows of matrix '''T''' represent the  [[Karhunen–Loève theorem|Kosambi-Karhunen–Loève]] transforms (KLT) of the data vectors in the rows of matrix&nbsp;'''X'''.\n\n== Derivation of PCA using the covariance method ==\n\nLet '''X''' be a ''d''-dimensional random vector expressed as column vector. Without loss of generality, assume '''X''' has zero mean.\n\nWe want to find <math>(\\ast)\\,</math> a {{math|''d'' × ''d''}} [[orthonormal basis|orthonormal transformation matrix]] '''P''' so that '''PX''' has a diagonal covariance matrix (''i.e.'' '''PX''' is a random vector with all its distinct components pairwise uncorrelated).\n\nA quick computation assuming <math>P</math> were unitary yields:\n\n:<math>\\begin{align}\n\\operatorname{cov}(PX) &= \\mathbb{E}[PX~(PX)^{*}]\\\\\n &= \\mathbb{E}[PX~X^{*}P^{*}]\\\\\n &= P~\\mathbb{E}[XX^{*}]P^{*}\\\\\n &= P~\\operatorname{cov}(X)P^{-1}\\\\\n\\end{align}</math>\n\nHence <math>(\\ast)\\,</math> holds if and only if <math>\\operatorname{cov}(X)</math> were diagonalisable by <math>P</math>.\n\nThis is very constructive, as cov('''X''') is guaranteed to be a non-negative definite matrix and thus is guaranteed to be diagonalisable by some unitary matrix.\n\n== Covariance-free computation  ==\n\nIn practical implementations, especially with [[high dimensional data]] (large {{mvar|p}}), the naive covariance method is rarely used because it is not efficient due to high computational and memory costs of explicitly determining the covariance matrix. The covariance-free approach avoids the {{math|''np''<sup>2</sup>}} operations of explicitly calculating and storing the covariance matrix {{math|'''X<sup>T</sup>X'''}}, instead utilizing one of [[matrix-free methods]], e.g., based on the function evaluating the product {{math|'''X<sup>T</sup>(X r)'''}} at the cost of {{math|''2np''}} operations.\n\n=== Iterative computation ===\n\nOne way to compute the first principal component efficiently<ref name=\"roweis\">Roweis, Sam. \"EM Algorithms for PCA and SPCA.\" Advances in Neural Information Processing Systems. Ed. Michael I. Jordan, Michael J. Kearns, and Sara A. Solla The MIT Press, 1998.</ref> is shown in the following pseudo-code, for a data matrix {{math|'''X'''}} with zero mean, without ever computing its covariance matrix.\n\n {{math|'''r'''}} = a random vector of length {{mvar|p}}\n {{nowrap|<math>\\mathbf{r} = \\frac{\\mathbf{r}}{|\\mathbf{r}|}</math>}}\n do {{mvar|c}} times:\n       {{math|1='''s''' = 0}} (a vector of length {{mvar|p}})\n       {{nowrap|for each row <math>\\mathbf{x} \\in \\mathbf{X}</math>}}\n             {{nowrap|<math>\\mathbf{s} = \\mathbf{s} + (\\mathbf{x} \\cdot \\mathbf{r})\\mathbf{x}</math>}}\n       {{nowrap|<math>eigenvalue = \\mathbf{r}^T\\mathbf{s}</math>}}\n       {{nowrap|<math>error = |eigenvalue\\cdot\\mathbf{r}-\\mathbf{s}|</math>}}\n       {{nowrap|<math>\\mathbf{r} = \\frac{\\mathbf{s}}{|\\mathbf{s}|}</math>}}\n       {{nowrap|exit if <math>error < tolerance</math>}}\n return {{nowrap|<math>eigenvalue, \\mathbf{r}</math>}}\n\nThis [[power iteration]] algorithm simply calculates the vector {{math|'''X<sup>T</sup>(X r)'''}}, normalizes, and places the result back in {{math|'''r'''}}. The eigenvalue is approximated by {{math|'''r<sup>T</sup> (X<sup>T</sup>X) r'''}}, which is the [[Rayleigh quotient]] on the unit vector {{math|'''r'''}} for the covariance matrix {{math|'''X<sup>T</sup>X '''}}. If the largest singular value is well separated from the next largest one, the vector {{math|'''r'''}} gets close to the first principal component of {{math|'''X'''}} within the number of iterations {{mvar|c}}, which is small relative to {{mvar|p}}, at the total cost {{math|''2cnp''}}. The [[power iteration]] convergence can be accelerated without noticeably sacrificing the small cost per iteration using more advanced [[matrix-free methods]], such as the [[Lanczos algorithm]] or the Locally Optimal Block Preconditioned Conjugate Gradient ([[LOBPCG]]) method.\n\nSubsequent principal components can be computed one-by-one via deflation or simultaneously as a block. In the former approach, imprecisions in already computed approximate principal components additively affect the accuracy of the subsequently computed principal components, thus increasing the error with every new computation. The latter approach in the block power method replaces single-vectors {{math|'''r'''}} and {{math|'''s'''}} with block-vectors, matrices {{math|'''R'''}} and {{math|'''S'''}}. Every column of {{math|'''R'''}} approximates one of the leading principal components, while all columns are iterated simultaneously. The main calculation is evaluation of the product {{math|'''X<sup>T</sup>(X R)'''}}. Implemented, e.g., in [[LOBPCG]], efficient blocking eliminates the accumulation of the errors, allows using high-level [[BLAS]] matrix-matrix product functions, and typically leads to faster convergence, compared to the single-vector one-by-one technique.\n\n=== The NIPALS method ===\n''Non-linear iterative partial least squares (NIPALS)'' is a variant the classical [[power iteration]] with matrix deflation by subtraction implemented for computing the first few components in a principal component or [[partial least squares]] analysis. For very-high-dimensional datasets, such as those generated in the *omics sciences (e.g., [[genomics]], [[metabolomics]]) it is usually only necessary to compute the first few PCs. The [[non-linear iterative partial least squares]] (NIPALS) algorithm updates iterative approximations to the leading scores and loadings '''t<sub>1</sub>''' and '''r<sub>1</sub>'''<sup>T</sup> by the [[power iteration]] multiplying on every iteration by '''X''' on the left and on the right, i.e. calculation of the covariance matrix is avoided, just as in the matrix-free implementation of the power iterations to {{math|'''X<sup>T</sup>X'''}}, based on the function evaluating the product {{math|'''X<sup>T</sup>(X r)'''}} = {{math|'''((X r)<sup>T</sup>X)<sup>T</sup>'''}}.\n\nThe matrix deflation by subtraction is performed by subtracting the outer product, '''t<sub>1</sub>r<sub>1</sub>'''<sup>T</sup> from '''X''' leaving the deflated residual matrix used to calculate the subsequent leading PCs.<ref>{{Cite journal\n  | last = Geladi\n  | first = Paul\n  | author-link =\n  | last2 = Kowalski\n  | first2 = Bruce\n  | author2-link =\n  | title = Partial Least Squares Regression:A Tutorial\n  | journal = Analytica Chimica Acta\n  | volume = 185\n  | issue =\n  | pages = 1–17\n  | year = 1986\n  | url =\n  | doi = 10.1016/0003-2670(86)80028-9\n  | id = }}</ref> \nFor large data matrices, or matrices that have a high degree of column collinearity, NIPALS suffers from loss of orthogonality of PCs due to machine precision [[round-off errors]] accumulated in each iteration and matrix deflation by subtraction.<ref>{{cite book |last=Kramer |first=R. |year=1998 |title=Chemometric Techniques for Quantitative Analysis |publisher=CRC Press |location=New York |isbn= }}</ref> A [[Gram–Schmidt]] re-orthogonalization algorithm is applied to both the scores and the loadings at each iteration step to eliminate this loss of orthogonality.<ref>{{cite journal |first=M. |last=Andrecut |title=Parallel GPU Implementation of Iterative PCA Algorithms |journal=Journal of Computational Biology |volume=16 |issue=11 |year=2009 |pages=1593–1599 |doi=10.1089/cmb.2008.0221 |pmid=19772385 |arxiv=0811.1081 }}</ref> NIPALS reliance on single-vector multiplications cannot take advantage of high-level [[BLAS]] and results in slow convergence for clustered leading singular values—both these deficiencies are resolved in more sophisticated matrix-free block solvers, such as the Locally Optimal Block Preconditioned Conjugate Gradient ([[LOBPCG]]) method.\n\n=== Online/sequential estimation ===\nIn an \"online\" or \"streaming\" situation with data arriving piece by piece rather than being stored in a single batch, it is useful to make an estimate of the PCA projection that can be updated sequentially. This can be done efficiently, but requires different algorithms.<ref>{{Cite journal\n  | last = Warmuth\n  | first = M. K.\n  | last2 = Kuzmin\n  | first2 = D.\n  | title = Randomized online PCA algorithms with regret bounds that are logarithmic in the dimension\n  | journal = Journal of Machine Learning Research\n  | volume = 9\n  | issue =\n  | pages = 2287–2320\n  | year = 2008\n  | url =\n  | doi =\n  | id = }}</ref>\n\n== PCA and qualitative variables ==\nIn PCA, it is common that we want to introduce qualitative variables as supplementary elements. For example, many quantitative variables have been measured on plants. For these plants, some qualitative variables are available as, for example, the species to which the plant belongs. These data were subjected to PCA for quantitative variables. When analyzing the results, it is natural to connect the principal components to the qualitative variable ''species''.\nFor this, the following results are produced.\n* Identification, on the factorial planes, of the different species e.g. using different colors.\n* Representation, on the factorial planes, of the centers of gravity of plants belonging to the same species.\n* For each center of gravity and each axis,  p-value to judge the significance of the difference between the center of gravity and origin.\n\nThese results are what is called ''introducing a qualitative variable as supplementary element''. This procedure is detailed in and Husson, Lê & Pagès 2009 and Pagès 2013.\nFew software offer this option in an \"automatic\" way. This is the case of [http://www.coheris.com/produits/analytics/logiciel-data-mining/ SPAD] that historically, following the work of [[Ludovic Lebart]], was the first to propose this option, and the R package [http://factominer.free.fr/ FactoMineR].\n\n== Applications ==\n\n=== Quantitative finance ===\n{{See also|Portfolio optimization}}\nIn [[quantitative finance]], principal component analysis can be directly applied to the [[risk management]] of [[interest rate derivative]] portfolios.<ref name=PHIRS>[http://www.tradinginterestrates.com The Pricing and Hedging of Interest Rate Derivatives: A Practical Guide to Swaps], J H M Darbyshire, 2016, {{isbn|978-0995455511}}</ref> Trading multiple [[swap (finance)|swap instruments]] which are usually a function of 30-500 other market quotable swap instruments is sought to be reduced to usually 3 or 4 principal components, representing the path of interest rates on a macro basis. Converting risks to be represented as those to factor loadings (or multipliers) provides assessments and understanding beyond that available to simply collectively viewing risks to individual 30-500 buckets.\n\nPCA has also been applied to [[Stock|share portfolios]] in a similar fashion,<ref>Giorgia Pasini (2017); [https://ijpam.eu/contents/2017-115-1/12/12.pdf Principal Component Analysis for Stock Portfolio Management]. ''International Journal of Pure and Applied Mathematics''. Volume 115 No. 1 2017, 153-167</ref> both to [[Risk return ratio|portfolio risk]] and to [[Risk–return spectrum|risk return]].  One application is to reduce portfolio risk, where [[asset allocation|allocation strategies]] are applied to the \"principal portfolios\" instead of the underlying stocks.<ref>Libin Yang. [https://ir.canterbury.ac.nz/bitstream/handle/10092/10293/thesis.pdf?sequence=1 ''An Application of Principal Component Analysis to Stock Portfolio Management'']. Department of Economics and Finance, [[University of Canterbury]], January 2015.</ref>  A second is to enhance portfolio return, using the principal components to [[Stock selection criterion|select stocks]] with upside potential.<ref>CA Hargreaves, Chandrika Kadirvel Mani (2015). [files.aiscience.org/journal/article/pdf/70210034.pdf The Selection of Winning Stocks Using Principal Component Analysis]. '''American Journal of Marketing Research'''. Vol. 1, No. 3, 2015, pp. 183-188</ref>\n\n=== Neuroscience ===\nA variant of principal components analysis is used in [[neuroscience]] to identify the specific properties of a stimulus that increase a [[neuron]]'s probability of generating an [[action potential]].<ref name=\"brenner00\">Brenner, N., Bialek, W., & de Ruyter van Steveninck, R.R. (2000).</ref> This technique is known as [[Spike-triggered covariance|spike-triggered covariance analysis]]. In a typical application an experimenter presents a [[white noise]] process as a stimulus (usually either as a sensory input to a test subject, or as a [[Electric current|current]] injected directly into the neuron) and records a train of action potentials, or spikes, produced by the neuron as a result. Presumably, certain features of the stimulus make the neuron more likely to spike. In order to extract these features, the experimenter calculates the [[covariance matrix]] of the ''spike-triggered ensemble'', the set of all stimuli (defined and discretized over a finite time window, typically on the order of 100 ms) that immediately preceded a spike. The [[Eigenvectors and eigenvalues|eigenvectors]] of the difference between the spike-triggered covariance matrix and the covariance matrix of the ''prior stimulus ensemble'' (the set of all stimuli, defined over the same length time window) then indicate the directions in the [[Vector space|space]] of stimuli along which the variance of the spike-triggered ensemble differed the most from that of the prior stimulus ensemble. Specifically, the eigenvectors with the largest positive eigenvalues correspond to the directions along which the variance of the spike-triggered ensemble showed the largest positive change compared to the variance of the prior. Since these were the directions in which varying the stimulus led to a spike, they are often good approximations of the sought after relevant stimulus features.\n\nIn neuroscience, PCA is also used to discern the identity of a neuron from the shape of its action potential. [[Spike sorting]] is an important procedure because [[Electrophysiology#Extracellular recording|extracellular]] recording techniques often pick up signals from more than one neuron. In spike sorting, one first uses PCA to reduce the dimensionality of the space of action potential waveforms, and then performs [[Cluster analysis|clustering analysis]] to associate specific action potentials with individual neurons.\n\nPCA as a dimension reduction technique is particularly suited to detect coordinated activities of large neuronal ensembles. It has been used in determining collective variables, i.e. [[order parameters]], during [[phase transitions]] in the brain.<ref>{{cite journal|last1=Jirsa|first1=Victor|last2=Friedrich|first2=R|last3=Haken|first3=Herman|last4=Kelso|first4=Scott|title=A theoretical model of phase transitions in the human brain|journal=Biological Cybernetics|date=1994|volume=71|issue=1|pages=27–35|doi=10.1007/bf00198909|pmid=8054384}}</ref>\n\n== Relation with other methods ==\n\n=== Correspondence analysis ===\n\n[[Correspondence analysis]] (CA)\nwas developed by [[Jean-Paul Benzécri]]<ref>{{Cite book\n | author = Benzécri, J.-P.\n | publisher=Dunod |location= Paris, France\n | year = 1973\n | title = L'Analyse des Données. Volume II. L'Analyse des Correspondances\n }}</ref>\nand is conceptually similar to PCA, but scales the data (which should be non-negative) so that rows and columns are treated equivalently. It is traditionally applied to [[contingency tables]].\nCA decomposes the [[chi-squared statistic]] associated to this table into orthogonal factors.<ref>{{Cite book\n | author = Greenacre, Michael\n | publisher=Academic Press |location= London\n | year = 1983\n | title = Theory and Applications of Correspondence Analysis\n | isbn = 978-0-12-299050-2\n }}</ref>\nBecause CA is a descriptive technique, it can be applied to tables for which the chi-squared statistic is appropriate or not.\nSeveral variants of CA are available including [[detrended correspondence analysis]] and [[canonical correspondence analysis]]. One special extension is [[multiple correspondence analysis]], which may be seen as the counterpart of principal component analysis for categorical data.<ref>{{Cite book\n |author1=Le Roux |author2=Brigitte and Henry Rouanet | publisher=Kluwer|location= Dordrecht\n | year = 2004\n | title = Geometric Data Analysis, From Correspondence Analysis to Structured Data Analysis\n | isbn =\n }}</ref>\n\n=== Factor analysis ===\nPrincipal component analysis creates variables that are linear combinations of the original variables. The new variables have the property that the variables are all orthogonal. The PCA transformation can be helpful as a pre-processing step before clustering. PCA is a variance-focused approach seeking to reproduce the total variable variance, in which components reflect both common and unique variance of the variable. PCA is generally preferred for purposes of data reduction (i.e., translating variable space into optimal factor space) but not when the goal is to detect the latent construct or factors.\n\n[[Factor analysis]] is similar to principal component analysis, in that factor analysis also involves linear combinations of variables. Different from PCA, factor analysis is a correlation-focused approach seeking to reproduce the inter-correlations among variables, in which the factors \"represent the common variance of variables, excluding unique variance\".<ref>Timothy A. Brown. Confirmatory Factor Analysis for Applied Research Methodology in the social sciences. Guilford Press, 2006</ref> In terms of the correlation matrix, this corresponds with focusing on explaining the off-diagonal terms (i.e. shared co-variance), while PCA focuses on explaining the terms that sit on the diagonal. However, as a side result, when trying to reproduce the on-diagonal terms, PCA also tends to fit relatively well the off-diagonal correlations.<ref>I.T. Jolliffe. Principal Component Analysis, Second Edition. Chapter 7. 2002</ref> Results given by PCA and factor analysis are very similar in most situations, but this is not always the case, and there are some problems where the results are significantly different. Factor analysis is generally used when the research purpose is detecting data structure (i.e., latent constructs or factors) or causal modeling.\n\n=== ''K''-means clustering ===\nIt was asserted in <ref>{{cite journal|author=H. Zha |author2=C. Ding |author3=M. Gu |author4=X. He |author5=H.D. Simon|title=Spectral Relaxation for K-means Clustering|journal=Neural Information Processing Systems Vol.14 (NIPS 2001)|pages=1057–1064|date=Dec 2001|url=http://ranger.uta.edu/~chqding/papers/Zha-Kmeans.pdf}}</ref><ref>{{cite journal|author=Chris Ding |author2=Xiaofeng He|title=K-means Clustering via Principal Component Analysis|journal=Proc. Of Int'l Conf. Machine Learning (ICML 2004)|pages=225–232|date=July 2004|url=http://ranger.uta.edu/~chqding/papers/KmeansPCA1.pdf}}</ref> that the relaxed solution of [[k-means clustering|{{math|<var>k</var>}}-means clustering]], specified by the cluster indicators, is given by the principal components, and the PCA subspace spanned by the principal directions is identical to the cluster centroid subspace. However, that PCA is a useful relaxation of k-means clustering was not a new result (see, for example,<ref>{{cite journal | title = Clustering large graphs via the singular value decomposition | journal = Machine Learning | year = 2004 | first = P. | last = Drineas |author2=A. Frieze |author3=R. Kannan |author4=S. Vempala |author5=V. Vinay  | volume = 56 | issue = 1–3 | pages = 9–33| id = | url = http://www.cc.gatech.edu/~vempala/papers/dfkvv.pdf | accessdate = 2012-08-02 | doi=10.1023/b:mach.0000033113.59016.96}}</ref>), and it is straightforward to uncover counterexamples to the statement that the cluster centroid subspace is spanned by the principal directions.<ref>{{cite journal | title = Dimensionality reduction for k-means clustering and low rank approximation (Appendix B) | journal =  | year = 2014 | first = M. | last = Cohen |author2=S. Elder |author3=C. Musco |author4=C. Musco |author5=M. Persu | arxiv = 1410.6801|bibcode=2014arXiv1410.6801C }}</ref>\n\n=== Non-negative matrix factorization ===\n[[File:Fractional_Residual_Variances_comparison,_PCA_and_NMF.pdf|thumb|500px|Fractional residual variance (FRV) plots for PCA and NMF;<ref name=\"ren18\"/> for PCA, the theoretical values are the contribution from the residual eigenvalues. In comparison, the FRV curves for PCA reaches a flat plateau where no signal are captured effectively; while the NMF FRV curves are declining continuously, indicating a better ability to capture signal. The FRV curves for NMF also converges to higher levels than PCA, indicating the less-overfitting property of NMF.]] [[Non-negative matrix factorization]] (NMF) is a dimension reduction method where only non-negative elements in the matrices are used, which is therefore a promising method in astronomy,<ref name=\"blantonRoweis07\">{{Cite journal|arxiv=astro-ph/0606170|last1= Blanton|first1= Michael R.|title= K-corrections and filter transformations in the ultraviolet, optical, and near infrared |journal= The Astronomical Journal|volume= 133|issue= 2|pages= 734–754|last2=  Roweis|first2= Sam |year= 2007|doi= 10.1086/510127|bibcode = 2007AJ....133..734B }}</ref><ref name=\"zhu16\"/><ref name=\"ren18\"/> in the sense that astrophysical signals are non-negative. The PCA components are orthogonal to each other, while the NMF components are all non-negative and therefore constructs a non-orthogonal basis.\n\nIn PCA, the contribution of each component is ranked based on the magnitude of its corresponding eigenvalue, which is equivalent to the fractional residual variance (FRV) in analyzing empirical data.<ref name = \"soummer12\">{{Cite journal|arxiv=1207.4197|last1= Soummer|first1= Rémi |title=  Detection and Characterization of Exoplanets and Disks Using Projections on Karhunen-Loève Eigenimages|journal= The Astrophysical Journal Letters |volume= 755|issue= 2|pages= L28|last2=  Pueyo|first2= Laurent|last3= Larkin | first3 = James|year= 2012|doi= 10.1088/2041-8205/755/2/L28|bibcode = 2012ApJ...755L..28S }}</ref> For NMF, its components are ranked based only on the empirical FRV curves.<ref name = \"ren18\">{{Cite journal|arxiv=1712.10317|last1= Ren|first1= Bin |title=  Non-negative Matrix Factorization: Robust Extraction of Extended Structures|journal= The Astrophysical Journal|volume= 852|issue= 2|pages= 104|last2=  Pueyo|first2= Laurent|last3= Zhu | first3 = Guangtun B.|last4=  Duchêne|first4= Gaspard |year= 2018|doi= 10.3847/1538-4357/aaa1f2|bibcode = 2018ApJ...852..104R }}</ref> The residual fractional eigenvalue plots, i.e., <math>1-\\sum_{i=1}^k \\lambda_i/\\sum_{k=1}^n \\lambda_k</math> as a function of component number <math>k</math> given a total of <math>n</math> components, for PCA has a flat plateau, where no data is captured to remove the quasi-static noise, then the curves dropped quickly as an indication of over-fitting and captures random noise.<ref name=\"soummer12\"/> The FRV curves for NMF is decreasing continuously <ref name=\"ren18\"/> when the NMF components are constructed [[Non-negative matrix factorization#Sequential NMF|sequentially]],<ref name=\"zhu16\">{{Cite arXiv|last=Zhu|first=Guangtun B.|date=2016-12-19|title=Nonnegative Matrix Factorization (NMF) with Heteroscedastic Uncertainties and Missing data |eprint=1612.06037|class=astro-ph.IM}}</ref> indicating the continuous capturing of quasi-static noise; then converge to higher levels than PCA,<ref name=\"ren18\"/> indicating the less over-fitting property of NMF.\n\n== Generalizations ==\n\n=== Sparse PCA ===\n{{main|Sparse PCA}}\nA particular disadvantage of PCA is that the principal components are usually linear combinations of all input variables. [[Sparse PCA]] overcomes this disadvantage by finding linear combinations that contain just a few input variables. It extends the classic method of principal component analysis (PCA) for the reduction of dimensionality of data by adding sparsity constraint on the input variables. \nSeveral approaches have been proposed, including\n* a regression framework,<ref>\n{{cite journal\n |author1=Hui Zou\n |author2=Trevor Hastie\n |author3=Robert Tibshirani\n |year=2006\n |title=Sparse principal component analysis\n |journal=[[Journal of Computational and Graphical Statistics]]\n |volume=15 |issue=2 |pages=262–286\n |url=http://www-stat.stanford.edu/~hastie/Papers/spc_jcgs.pdf\n |doi=10.1198/106186006x113430\n|citeseerx=10.1.1.62.580\n }}</ref>\n* a convex relaxation/semidefinite programming framework,<ref name=\"SDP\">\n{{cite journal\n |author1=Alexandre d’Aspremont\n |author2=Laurent El Ghaoui\n |author3=Michael I. Jordan\n |author4=Gert R. G. Lanckriet\n |year=2007\n |title=A Direct Formulation for Sparse PCA Using Semidefinite Programming\n |journal=[[SIAM Review]]\n |volume=49 |issue=3 |pages=434–448\n |url=http://www.cmap.polytechnique.fr/~aspremon/PDF/sparsesvd.pdf\n |doi=10.1137/050645506\n|arxiv=cs/0406021\n }}</ref>\n* a generalized power method framework<ref>\n{{cite journal\n |author1=Michel Journee\n |author2=Yurii Nesterov\n |author3=Peter Richtarik\n |author4=Rodolphe Sepulchre\n |year=2010\n |title=Generalized Power Method for Sparse Principal Component Analysis\n |journal=[[Journal of Machine Learning Research]]\n |volume=11 |pages=517–553\n |url=http://jmlr.csail.mit.edu/papers/volume11/journee10a/journee10a.pdf\n |arxiv=0811.4724\n |id=CORE Discussion Paper 2008/70\n|bibcode=2008arXiv0811.4724J\n }}</ref>\n* an alternating maximization framework<ref>\n{{cite arXiv\n |author1=Peter Richtarik\n |author2=Martin Takac\n |author3=S. Damla Ahipasaoglu \n |year=2012\n |title=Alternating Maximization: Unifying Framework for 8 Sparse PCA Formulations and Efficient Parallel Codes\n |eprint=1212.4137\n |class=stat.ML\n}}</ref>\n* forward-backward greedy search and exact methods using branch-and-bound techniques,<ref>\n{{cite conference\n |author1=Baback Moghaddam\n |author2=Yair Weiss\n |author3=Shai Avidan \n |year=2005 \n |chapter=Spectral Bounds for Sparse PCA: Exact and Greedy Algorithms \n |title=Advances in Neural Information Processing Systems\n |volume=18\n |publisher=MIT Press\n |url=http://books.nips.cc/papers/files/nips18/NIPS2005_0643.pdf\n}}</ref>\n* Bayesian formulation framework.<ref>\n{{cite journal\n |author1=Yue Guan\n |author2=Jennifer Dy \n |year=2009 \n |title=Sparse Probabilistic Principal Component Analysis \n |journal=[[Journal of Machine Learning Research Workshop and Conference Proceedings]]\n |volume=5 |page=185\n |url=http://jmlr.csail.mit.edu/proceedings/papers/v5/guan09a/guan09a.pdf\n}}</ref>\nThe methodological and theoretical developments of Sparse PCA as well as its applications in scientific studies are recently reviewed in a survey paper.<ref>\n{{cite journal\n |author1=Hui Zou\n |author2=Lingzhou Xue\n |year=2018\n |title=A Selective Overview of Sparse Principal Component Analysis \n |journal=[[Proceedings of the IEEE]]\n |volume=106 |issue=8\n |pages=1311–1320\n |url=https://ieeexplore.ieee.org/abstract/document/8412518\n|doi=10.1109/JPROC.2018.2846588\n }}</ref>\n\n=== Nonlinear PCA ===\n[[File:Elmap breastcancer wiki.png|thumb|300px| Linear PCA versus nonlinear Principal Manifolds<ref>[[Alexander Nikolaevich Gorban|A. N. Gorban]], A. Y. Zinovyev, [https://arxiv.org/abs/0809.0490 Principal Graphs and Manifolds], In: Handbook of Research on Machine Learning Applications and Trends: Algorithms, Methods and Techniques, Olivas E.S. et al Eds. Information Science Reference, IGI Global: Hershey, PA, USA, 2009. 28–59.</ref> for [[Scientific visualization|visualization]] of [[breast cancer]] [[microarray]] data: a) Configuration of nodes and 2D Principal Surface in the 3D PCA linear manifold. The dataset is curved and cannot be mapped adequately on a 2D principal plane; b) The distribution in the internal 2D non-linear principal surface coordinates (ELMap2D) together with an estimation of the density of points; c) The same as b), but for the linear 2D PCA manifold (PCA2D). The \"basal\" breast cancer subtype is visualized more adequately with ELMap2D and some features of the distribution become better resolved in comparison to PCA2D. Principal manifolds are produced by the [[elastic map]]s algorithm. Data are available for public competition.<ref>{{cite journal |last=Wang |first=Y. |last2=Klijn |first2=J. G. |last3=Zhang |first3=Y. |last4=Sieuwerts |first4=A. M. |last5=Look |first5=M. P. |last6=Yang |first6=F. |last7=Talantov |first7=D. |last8=Timmermans |first8=M. |last9=Meijer-van Gelder |first9=M. E. |last10=Yu |first10=J. |title=Gene expression profiles to predict distant metastasis of lymph-node-negative primary breast cancer |journal=[[The Lancet]] |volume=365 |issue=9460 |pages=671–679 |year=2005 |doi=10.1016/S0140-6736(05)17947-1 |pmid=15721472 |display-authors=etal}} [http://www.ihes.fr/~zinovyev/princmanif2006/ Data online]</ref> Software is available for free non-commercial use.<ref>{{cite web |first=A. |last=Zinovyev |url=http://bioinfo-out.curie.fr/projects/vidaexpert/ |title=ViDaExpert – Multidimensional Data Visualization Tool |work=[[Curie Institute (Paris)|Institut Curie]] |location=Paris }} (free for non-commercial use)</ref>]]\n\nMost of the modern methods for [[nonlinear dimensionality reduction]] find their theoretical and algorithmic roots in PCA or K-means. Pearson's original idea was to take a straight line (or plane) which will be \"the best fit\" to a set of data points. '''Principal [[curve]]s and [[manifold]]s'''<ref>A.N. Gorban, B. Kegl, D.C. Wunsch, A. Zinovyev  (Eds.),  [https://www.researchgate.net/publication/271642170_Principal_Manifolds_for_Data_Visualisation_and_Dimension_Reduction_LNCSE_58 Principal Manifolds for Data Visualisation and Dimension Reduction,]\nLNCSE 58, Springer, Berlin – Heidelberg – New York, 2007. {{isbn|978-3-540-73749-0}}</ref> give the natural geometric framework for PCA generalization and extend the geometric interpretation of PCA by explicitly constructing an embedded manifold for data [[approximation]], and by encoding using standard geometric [[Projection (mathematics)|projection]] onto the manifold, as it is illustrated by Fig.\nSee also the [[elastic map]] algorithm and [[principal geodesic analysis]]. Another popular generalization is [[kernel PCA]], which corresponds to PCA performed in a reproducing kernel Hilbert space associated with a positive definite kernel.\n\nIn [[multilinear subspace learning]],<ref>{{cite journal\n |first=Haiping |last=Lu\n |first2=K.N. |last2=Plataniotis\n |first3=A.N. |last3=Venetsanopoulos\n |url=http://www.dsp.utoronto.ca/~haiping/Publication/SurveyMSL_PR2011.pdf\n |title=A Survey of Multilinear Subspace Learning for Tensor Data\n |journal=Pattern Recognition\n |volume=44 |number=7 |pages=1540–1551 |year=2011\n |doi=10.1016/j.patcog.2011.01.004\n}}</ref> PCA is generalized to [[multilinear principal component analysis|multilinear PCA]] (MPCA) that extracts features directly from tensor representations. MPCA is solved by performing PCA in each mode of the tensor iteratively. MPCA has been applied to face recognition, gait recognition, etc. MPCA is further extended to uncorrelated MPCA, non-negative MPCA and robust MPCA.\n\n''N''-way principal component analysis may be performed with models such as [[Tucker decomposition]],  [[PARAFAC]], multiple factor analysis, co-inertia analysis, STATIS, and DISTATIS.\n\n=== Robust PCA ===\nWhile PCA finds the mathematically optimal method (as in minimizing the squared error), it is still sensitive to [[outlier]]s in the data that produce large errors, something that the method tries to avoid in the first place. It is therefore common practice to remove outliers before computing PCA. However, in some contexts, outliers can be difficult to identify. For example, in [[data mining]] algorithms like [[correlation clustering]], the assignment of points to clusters and outliers is not known beforehand. \nA recently proposed generalization of PCA<ref>{{Cite book | doi = 10.1007/978-3-540-69497-7_27 | title = A General Framework for Increasing the Robustness of PCA-Based Correlation Clustering Algorithms | isbn = 978-3-540-69476-2 | series = Lecture Notes in Computer Science | journal = Scientific and Statistical Database Management| year = 2008 | last1 = Kriegel | first1 = H. P. | last2 = Kröger | first2 = P. | last3 = Schubert | first3 = E. | last4 = Zimek | first4 = A. | volume = 5069 | pages = 418–435 | citeseerx = 10.1.1.144.4864 }}</ref> based on a weighted PCA increases robustness by assigning different weights to data objects based on their estimated relevancy.\nOutlier-resistant versions of PCA have also been proposed on L1-norm formulations ([[L1-norm principal component analysis|L1-PCA]]).<ref name=\"mark2014\">{{cite journal|last1=Markopoulos|first1=Panos P.|last2=Karystinos|first2=George N.|last3=Pados|first3=Dimitris A.|date=October 2014|title=Optimal Algorithms for L1-subspace Signal Processing|journal=IEEE Transactions on Signal Processing|volume=62|issue=19|pages=5046–5058|arxiv=1405.6785|bibcode=2014ITSP...62.5046M|doi=10.1109/TSP.2014.2338077}}</ref>\n\n[[Robust principal component analysis]] (RPCA) via decomposition in low-rank and sparse matrices is a modification of PCA that works well with respect to grossly corrupted observations.<ref name=RPCA>{{cite journal|last=Emmanuel J. Candes|author2=Xiaodong Li |author3=Yi Ma |author4=John Wright |title=Robust Principal Component Analysis?|journal=Journal of the ACM|volume=58|issue=3|pages=11 |doi=10.1145/1970392.1970395|arxiv=0912.3599|year=2011 }}</ref><ref name=RPCA-BOUWMANS>{{cite journal|last=T. Bouwmans|author2= E. Zahzah|title=Robust PCA via Principal Component Pursuit: A Review for a Comparative Evaluation in Video Surveillance|journal=Special Issue on Background Models Challenge, Computer Vision and Image Understanding|volume= 122|pages= 22–34|year=2014|doi= 10.1016/j.cviu.2013.11.009}}</ref><ref name=RPCA-BOUWMANS-COSREV>{{Cite journal|last=T. Bouwmans|author2= A. Sobral|author3= S. Javed|author4= S. Jung|author5= E. Zahzah|title=Decomposition into Low-rank plus Additive Matrices for Background/Foreground Separation: A Review for a Comparative Evaluation with a Large-Scale Dataset |journal= Computer Science Review|volume= 23|pages= 1–71|arxiv=1511.01245|year=2015|doi= 10.1016/j.cosrev.2016.11.001}}</ref>\n\n== Similar techniques ==\n\n=== Independent component analysis ===\n[[Independent component analysis]] (ICA) is directed to similar problems as principal component analysis, but finds additively separable components rather than successive approximations.\n\n=== Network component analysis ===\nGiven a matrix <math>E</math>, it tries to decompose it into two matrices such that <math>E=AP\n</math>. A key difference from techniques such as PCA and ICA is that some of the entries of <math>A</math> are constrained to be 0. Here <math>P</math> is termed the regulatory layer. While in general such a decomposition can have multiple solutions, they prove that if the following conditions are satisfied :\n# <math>A</math> has full column rank\n# Each column of <math>A</math> must have at least <math>L-1</math>  zeroes where <math>L</math> is the number of columns of <math>A</math> (or alternatively the number of rows of <math>P</math>). The justification for this criterion is that if a node is removed from the regulatory layer along with all the output nodes connected to it, the result must still be characterized by a connectivity matrix with full column rank.\n# <math>P</math> must have full row rank.\nthen the decomposition is unique up to multiplication by a scalar.<ref>{{Cite journal|url = http://www.pnas.org/content/100/26/15522.full.pdf|title = Network component analysis: Reconstruction of regulatory signals in biological systems|date = |accessdate = February 10, 2015}}</ref>\n\n== Software/source code ==\n* [[ALGLIB]] - a C++ and C# library that implements PCA and truncated PCA\n* [[Analytica (software)|Analytica]] – The built-in EigenDecomp function computes principal components.\n* [[ELKI]] – includes PCA for projection, including robust variants of PCA, as well as PCA-based [[Cluster analysis|clustering algorithms]].\n* [[Gretl]] – principal component analysis can be performed either via the <code>pca</code> command or via the <code>princomp()</code> function.\n* [[Julia language|Julia]] – Supports PCA with the <code>pca</code> function in the MultivariateStats package\n* [[KNIME]] – A java based nodal arrenging software for Analysis, in this the nodes called PCA, PCA compute, PCA Apply, PCA inverse make it easily.\n* [[Mathematica]] – Implements principal component analysis with the PrincipalComponents command using both covariance and correlation methods.\n* [[MATLAB]] Statistics Toolbox – The functions <code>princomp</code> and <code>pca</code> (R2012b) give the principal components, while the function <code>pcares</code> gives the residuals and reconstructed matrix for a low-rank PCA approximation.\n* [[Matplotlib]] – [[Python (programming language)|Python]] library have a PCA package in the .mlab module.\n* [[mlpack]] – Provides an implementation of principal component analysis in [[C++]].\n* [[NAG Numerical Library|NAG Library]] – Principal components analysis is implemented via the <code>g03aa</code> routine (available in both the Fortran versions of the Library).\n* [[NMath]] – Proprietary numerical library containing PCA for the [[.NET Framework]].\n* [[GNU Octave]] – Free software computational environment mostly compatible with MATLAB, the function <code>princomp</code> gives the principal component.\n* [[OpenCV]]\n* [[Oracle Database]] 12c – Implemented via <code>DBMS_DATA_MINING.SVDS_SCORING_MODE</code> by specifying setting value <code>SVDS_SCORING_PCA</code>\n* [[Orange (software)]] – Integrates PCA in its visual programming environment. PCA displays a scree plot (degree of explained variance) where user can interactively select the number of principal components.\n* [[Origin (data analysis software)|Origin]] – Contains PCA in its Pro version.\n* [[Qlucore]] – Commercial software for analyzing multivariate data with instant response using PCA.\n* [[R (programming language)|R]] – [[free software|Free]] statistical package, the functions <code>princomp</code> and <code>prcomp</code> can be used for principal component analysis; <code>prcomp</code> uses [[singular value decomposition]] which generally gives better numerical accuracy. Some packages that implement PCA in R, include, but are not limited to: <code>ade4</code>, <code>vegan</code>, <code>ExPosition</code>, <code>dimRed</code>, and <code>FactoMineR</code>.\n* [[SAS (software)|SAS]] - Proprietary software; for example, see <ref>{{cite web|title=Principal Components Analysis|url=https://stats.idre.ucla.edu/sas/output/principal-components-analysis/|website=Institute for Digital Research and Education|publisher=UCLA|accessdate=29 May 2018}}</ref>\n* [[Scikit-learn]] – Python library for machine learning which contains PCA, Probabilistic PCA, Kernel PCA, Sparse PCA and other techniques in the decomposition module.\n* [[Weka (machine learning)|Weka]] – Java library for machine learning which contains modules for computing principal components.\n\n== See also ==\n{{Div col|colwidth=22em}}\n* [[Correspondence analysis]] (for contingency tables)\n* [[Multiple correspondence analysis]] (for qualitative variables)\n* [[Factor analysis of mixed data]] (for quantitative '''and''' qualitative variables)\n* [[Canonical correlation]]\n* [[CUR matrix approximation]] (can replace of low-rank SVD approximation)\n* [[Detrended correspondence analysis]]\n* [[Dynamic mode decomposition]]\n* [[Eigenface]]\n* [[v:Exploratory factor analysis|Exploratory factor analysis]] (Wikiversity)\n* [[Factorial code]]\n* [[Functional principal component analysis]]\n* [[Geometric data analysis]]\n* [[Independent component analysis]]\n* [[Kernel PCA]]\n* [[L1-norm principal component analysis]]\n* [[Low-rank approximation]]\n* [[Matrix decomposition]]\n* [[Non-negative matrix factorization]]\n* [[Nonlinear dimensionality reduction]]\n* [[Oja's rule]]\n* [[Point distribution model]] (PCA applied to morphometry and computer vision)\n* [[b:Statistics/Multivariate Data Analysis/Principal Component Analysis|Principal component analysis]] (Wikibooks)\n* [[Principal component regression]]\n* [[Singular spectrum analysis]]\n* [[Singular value decomposition]]\n* [[Sparse PCA]]\n* [[Transform coding]]\n* [[Weighted least squares]]\n{{div col end}}\n\n== References ==\n\n{{Reflist|30em}}\n\n== Further reading ==\n* Jackson, J.E. (1991).  ''A User's Guide to Principal Components'' (Wiley).\n* {{Cite book\n  | last = Jolliffe\n  | first = I. T.\n  | title = Principal Component Analysis\n  | publisher = Springer-Verlag\n  | year = 1986\n  | location =\n  | pages = 487\n  | url = https://www.springer.com/west/home/new+%26+forthcoming+titles+%28default%29?SGWID=4-40356-22-2285433-0\n  | doi = 10.1007/b98835\n  | id =\n  | isbn = 978-0-387-95442-4 | series = Springer Series in Statistics\n  | citeseerx = 10.1.1.149.8828\n  }}\n* Jolliffe, I.T. (2002).  ''Principal Component Analysis,'' second edition (Springer).\n* Husson François, Lê Sébastien & Pagès Jérôme (2009). ''Exploratory Multivariate Analysis by Example Using R''. Chapman & Hall/CRC The R Series, London. 224p. {{isbn|978-2-7535-0938-2}}\n* Pagès Jérôme (2014). ''Multiple Factor Analysis by Example Using R''. Chapman & Hall/CRC The R Series London 272 p\n\n== External links ==\n{{Commons category}}\n* {{YouTube|UUxIXU_Ob6E|University of Copenhagen video by Rasmus Bro}}\n* {{YouTube|id=ey2PE5xi9-A#t=2385|title=Stanford University video by Andrew Ng}}\n* [https://arxiv.org/abs/1404.1100 A Tutorial on Principal Component Analysis]\n* {{YouTube|BfTMmoDFXyE|A layman's introduction to principal component analysis}} (a video of less than 100 seconds.)\n* {{YouTube|_UVHneBUBW0|StatQuest: Principal Component Analysis (PCA) clearly explained}}\n* See also the list of [[#Software/source code|Software implementations]]\n\n{{Statistics|analysis|state=collapsed}}\n\n[[Category:Matrix decompositions]]\n[[Category:Dimension reduction]]"
    },
    {
      "title": "Rank factorization",
      "url": "https://en.wikipedia.org/wiki/Rank_factorization",
      "text": "Given an <math display=\"inline\">m \\times n</math> [[matrix (mathematics)|matrix]] <math display=\"inline\">A</math> of [[rank (linear algebra)|rank]] <math display=\"inline\">r</math>, a '''rank decomposition''' or '''rank factorization''' of <math display=\"inline\">A</math> is a product <math display=\"inline\">A = CF</math>, where <math display=\"inline\">C</math> is an <math display=\"inline\">m \\times r</math> matrix and <math display=\"inline\">F</math> is an <math display=\"inline\">r \\times n</math> matrix.\n\n== Existence ==\n'''Every finite-dimensional matrix has a rank decomposition:''' Let <math display=\"inline\">A</math> be an <math display=\"inline\">m\\times n</math> matrix whose [[column rank]] is <math display=\"inline\">r</math>. Therefore, there are <math display=\"inline\">r</math> [[linearly independent]] columns in <math display=\"inline\">A</math>; equivalently, the [[dimension]] of the [[column space]] of <math display=\"inline\">A</math> is <math display=\"inline\">r</math>. Let <math display=\"inline\">c_1,c_2,\\ldots,c_r</math> be any [[basis (linear algebra)|basis]] for the column space of <math display=\"inline\">A</math> and place them as column vectors to form the <math display=\"inline\">m\\times r</math> matrix <math display=\"inline\">C = [c_1:c_2:\\ldots:c_r]</math>. Therefore, every column vector of <math display=\"inline\">A</math> is a [[linear combination]] of the columns of <math display=\"inline\">C</math>. To be precise, if <math display=\"inline\">A = [a_1:a_2:\\ldots:a_n]</math> is an <math display=\"inline\">m\\times n</math> matrix with <math display=\"inline\">a_j</math> as the <math display=\"inline\">j</math>-th column, then\n:<math>a_j = f_{1j}c_1 + f_{2j}c_2 + \\cdots + f_{rj}c_r,</math>   \n\nwhere <math display=\"inline\">f_{ij}</math>'s are the scalar coefficients of <math display=\"inline\">a_j</math> in terms of the basis <math display=\"inline\">c_1,c_2,\\ldots,c_r</math>. This implies that <math display=\"inline\">A = CF</math>, where <math display=\"inline\">f_{ij}</math> is the <math display=\"inline\">(i,j)</math>-th element of <math display=\"inline\">F</math>. \n\n== Non-uniqueness ==\nIf <math display=\"inline\">A = C_1 F_1</math> is rank factorization, taking <math display=\"inline\">C_2 = C_1 R</math> and \n<math display=\"inline\">F_2 = R^{-1} F_1</math> gives another rank factorization for any invertible matrix <math display=\"inline\">R</math> of compatible dimensions. \n\nConversely, if <math display=\"inline\">A = F_{1}G_{1} = F_{2}G_{2} </math> are two rank factorizations of <math display=\"inline\">A</math>, then there exists an invertible matrix <math display=\"inline\">R</math> such that <math display=\"inline\">F_1 = F_2 R</math> and <math display=\"inline\">G_1 = R^{-1} G_{2}</math>.<ref>{{cite journal|last1=Piziak|first1=R.|last2=Odell|first2=P. L.|title=Full Rank Factorization of Matrices|journal=Mathematics Magazine|date=1 June 1999|volume=72|issue=3|pages=193|doi=10.2307/2690882}}</ref>\n\n== Construction ==\n=== Rank factorization from row echelon forms === \nIn practice, we can construct one specific rank factorization as follows: we can compute <math display=\"inline\">B</math>, the [[row echelon form|reduced row echelon form]] of <math display=\"inline\">A</math>. Then <math display=\"inline\">C</math> is obtained by removing from <math display=\"inline\">A</math> all non-[[gaussian elimination|pivot columns]], and <math display=\"inline\">F</math> by eliminating all zero rows of <math display=\"inline\">B</math>.\n\n==== Example ====\n\nConsider the matrix\n:<math>\n  A = \\begin{bmatrix} 1 & 3 & 1 & 4 \\\\ 2 & 7 & 3 & 9 \\\\ 1 & 5 & 3 & 1 \\\\ 1 & 2 & 0 & 8 \\end{bmatrix}\n \\sim \\begin{bmatrix} 1 & 0 & -2 & 0 \\\\ 0 & 1 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix} \n    = B\\text{.}\n</math>\n\n<math display=\"inline\">B</math> is in reduced echelon form.\n\nThen <math display=\"inline\">C</math> is obtained by removing the third column of <math display=\"inline\">A</math>, the only one which is not a pivot column, and <math display=\"inline\">F</math> by getting rid of the last row of zeroes, so\n:<math>\n  C = \\begin{bmatrix} 1 & 3 & 4 \\\\ 2 & 7 & 9 \\\\ 1 & 5 & 1 \\\\ 1 & 2 & 8 \\end{bmatrix}\\text{,}\\qquad\n  F = \\begin{bmatrix} 1 & 0 & -2 & 0 \\\\ 0 & 1 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix}\\text{.}\n</math>\n\nIt is straightforward to check that\n:<math>\n  A = \\begin{bmatrix} 1 & 3 & 1 & 4 \\\\ 2 & 7 & 3 & 9 \\\\ 1 & 5 & 3 & 1 \\\\ 1 & 2 & 0 & 8 \\end{bmatrix}\n    = \\begin{bmatrix} 1 & 3 & 4 \\\\ 2 & 7 & 9 \\\\ 1 & 5 & 1 \\\\ 1 & 2 & 8 \\end{bmatrix}\n      \\begin{bmatrix} 1 & 0 & -2 & 0 \\\\ 0 & 1 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix}\n    = CF\\text{.}\n</math>\n\n==== Proof ====\nLet <math display=\"inline\">P</math> be an <math display=\"inline\">n\\times n</math> permutation matrix such that <math display=\"inline\">AP = (C, D)</math> in [[block matrix|block partitioned]] form, where the columns of <math display=\"inline\">C</math> are the <math display=\"inline\">r</math> pivot columns of <math display=\"inline\">A</math>. Every column of <math display=\"inline\">D</math> is a linear combination of the columns of <math display=\"inline\">C</math>, so there is a matrix <math display=\"inline\">G</math> such that <math display=\"inline\">D = CG</math>, where the columns of <math display=\"inline\">G</math> contain the coefficients of each of those linear combinations. So <math display=\"inline\">AP = (C, CG) = C(I_r, G)</math>, <math display=\"inline\">I_r</math> being the <math display=\"inline\">r\\times r</math> identity matrix. We will show now that <math display=\"inline\">(I_r, G) = FP</math>.\n\nTransforming <math display=\"inline\">AP</math> into its reduced row echelon form amounts to left-multiplying by a matrix <math display=\"inline\">E</math> which is a product of [[elementary matrix|elementary matrices]], so <math display=\"inline\">EAP = BP = EC(I_r, G)</math>, where <math display=\"inline\">EC = \\begin{pmatrix} I_r \\\\ 0 \\end{pmatrix}</math>. We then can write <math display=\"inline\">BP = \\begin{pmatrix} I_r & G \\\\ 0 & 0 \\end{pmatrix}</math>, which allows us to identify <math display=\"inline\">(I_r, G) = FP</math>, i.e. the nonzero <math display=\"inline\">r</math> rows of the reduced echelon form, with the same permutation on the columns as we did for <math display=\"inline\">A</math>. We thus have <math display=\"inline\">AP = CFP</math>, and since <math display=\"inline\">P</math> is invertible this implies <math display=\"inline\">A = CF</math>, and the proof is complete.\n\n=== Singular value decomposition ===\nOne can also construct a full rank factorization of <math display=\"inline\">A</math> by using its [[singular value decomposition]]\n\n:<math>\n  A = U \\Sigma V^*\n    = \\begin{bmatrix} U_1 & U_2 \\end{bmatrix} \\begin{bmatrix} \\Sigma_r & 0 \\\\ 0 & 0 \\end{bmatrix} \\begin{bmatrix} V_1^* \\\\ V_2^* \\end{bmatrix}\n    = U_1 \\left(\\Sigma_r V_1^*\\right) .\n</math>\n\nSince <math display=\"inline\">U_1</math> is a full column rank matrix and <math display=\"inline\">\\Sigma_r V_1^*</math> is a full row rank matrix, we can take <math display=\"inline\">C = U_1</math> and <math display=\"inline\">F = \\Sigma_r V_1^*</math>.\n\n== Consequences ==\n=== rank(A) = rank(A<sup>T</sup>) ===\nAn immediate consequence of rank factorization is that the rank of <math display=\"inline\">A</math> is equal to the rank of its transpose <math display=\"inline\">A^\\textsf{T}</math>. Since the columns of <math display=\"inline\">A</math> are the rows of <math display=\"inline\">A^\\textsf{T}</math>, the [[column rank]] of <math display=\"inline\">A</math> equals its [[row rank]]. \n\n'''Proof:''' To see why this is true, let us first define rank to mean column rank. Since <math display=\"inline\">A = CF</math>, it follows that <math display=\"inline\">A^\\textsf{T} = F^\\textsf{T}C^\\textsf{T}</math>. From the definition of [[matrix multiplication]], this means that each column of <math display=\"inline\">A^\\textsf{T}</math> is a [[linear combination]] of the columns of <math display=\"inline\">F^\\textsf{T}</math>. Therefore, the column space of <math display=\"inline\">A^\\textsf{T}</math> is contained within the column space of <math display=\"inline\">F^\\textsf{T}</math> and, hence, rank<math display=\"inline\">\\left(A^\\textsf{T}\\right)</math> ≤ rank<math display=\"inline\">\\left(F^\\textsf{T}\\right)</math>.\n\nNow, <math display=\"inline\">F^\\textsf{T}</math> is <math display=\"inline\">n \\times r</math>, so there are <math display=\"inline\">r</math> columns in <math display=\"inline\">F^\\textsf{T}</math> and, hence, rank<math display=\"inline\">\\left(A^\\textsf{T}\\right)</math> ≤ <math display=\"inline\">r</math> = rank<math display=\"inline\">\\left(A\\right)</math>. This proves that rank<math display=\"inline\">\\left(A^\\textsf{T}\\right)</math> ≤ rank<math display=\"inline\">\\left(A\\right)</math>.\n\nNow apply the result to <math display=\"inline\">A^\\textsf{T}</math> to obtain the reverse inequality: since <math display=\"inline\">\\left(A^\\textsf{T}\\right)^\\textsf{T}</math> = <math display=\"inline\">A</math>, we can write rank<math display=\"inline\">\\left(A\\right)</math> = rank<math display=\"inline\">\\left(\\left(A^\\textsf{T}\\right)^\\textsf{T}\\right)</math> ≤ rank<math display=\"inline\">\\left(A^\\textsf{T}\\right)</math>. This proves rank<math display=\"inline\">\\left(A\\right)</math> ≤ rank<math display=\"inline\">\\left(A^\\textsf{T}\\right)</math>.\n\nWe have, therefore, proved rank<math display=\"inline\">\\left(A^\\textsf{T}\\right)</math> ≤ rank<math display=\"inline\">\\left(A\\right)</math> and rank<math display=\"inline\">\\left(A\\right)</math> ≤ rank<math display=\"inline\">\\left(A^\\textsf{T}\\right)</math>, so rank<math display=\"inline\">\\left(A\\right)</math> = rank<math display=\"inline\">\\left(A^\\textsf{T}\\right)</math>. (Also see the first proof of column rank = row rank under [[rank (linear algebra)|rank]]).\n\n==Notes==\n{{reflist}}\n\n==References==\n{{refbegin}}\n* {{Citation | last = Banerjee | first = Sudipto | last2 = Roy | first2 = Anindya | date = 2014 | title = Linear Algebra and Matrix Analysis for Statistics | series = Texts in Statistical Science | publisher = Chapman and Hall/CRC | edition =  1st | isbn =  978-1420095388}}\n* {{Citation | last = Lay | first = David C. | date = 2005 | title = Linear Algebra and its Applications | publisher = Addison Wesley | edition = 3rd | isbn = 978-0-201-70970-4}}\n* {{Citation | last = Golub | first = Gene H. | last2 = Van Loan | first2 = Charles F. | date = 1996 | title = Matrix Computations | series = Johns Hopkins Studies in Mathematical Sciences | publisher = The Johns Hopkins University Press | edition = 3rd | isbn = 978-0-8018-5414-9}}\n* {{Citation | last = Stewart | first = Gilbert W. | date = 1998 | title = Matrix Algorithms. I. Basic Decompositions  | publisher = SIAM | isbn = 978-0-89871-414-2}}\n* {{cite journal|last1=Piziak|first1=R.|last2=Odell|first2=P. L.|title=Full Rank Factorization of Matrices|journal=Mathematics Magazine|date=1 June 1999|volume=72|issue=3|pages=193|doi=10.2307/2690882}}\n{{refend}}\n\n[[Category:Matrix decompositions]]\n[[Category:Linear algebra]]"
    },
    {
      "title": "Robust principal component analysis",
      "url": "https://en.wikipedia.org/wiki/Robust_principal_component_analysis",
      "text": "{{Underlinked|date=April 2014}}\n\n'''Robust Principal Component Analysis (RPCA)''' is a modification of the widely used statistical procedure of [[principal component analysis]] (PCA) which works well with respect to ''grossly'' corrupted observations. A number of different approaches exist for Robust PCA, including an idealized version of Robust PCA, which aims to recover a low-rank matrix L<sub>0</sub> from highly corrupted measurements M = L<sub>0</sub> +S<sub>0</sub>.<ref name=RPCA/> This decomposition in low-rank and sparse matrices can be achieved by techniques such as Principal Component Pursuit method (PCP),<ref name=RPCA /> Stable PCP,<ref name=RPCA-WRIGHT>{{cite journal|author1=J. Wright|author2=Y. Peng|author3=Y. Ma|author4=A. Ganesh|author5=S. Rao|title=Robust Principal Component Analysis: Exact Recovery of Corrupted Low-Rank Matrices by Convex Optimization|journal=Neural Information Processing Systems, NIPS 2009|year=2009}}</ref> Quantized PCP,<ref name=RPCA-BECKER>{{cite journal|last= S. Becker|author2=E. Candes, M. Grant|title=TFOCS: Flexible First-order Methods for Rank Minimization|journal=Low-rank Matrix Optimization Symposium, SIAM Conference on Optimization|year=2011}}</ref> Block based PCP,<ref name=RPCA-TANG>{{cite journal|last= G. Tang|author2= A. Nehorai|title=Robust principal component analysis based on low-rank and block-sparse matrix decomposition|journal=Annual Conference on Information Sciences and Systems, CISS 2011|year=2011}}</ref> and Local PCP.<ref name=RPCA-WOHLBERG>{{cite journal|last= B. Wohlberg|author2= R. Chartrand|author3=J. Theiler|title=Local Principal Component Pursuit for Nonlinear Datasets|journal=International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2012|year=2012}}</ref> Then, optimization methods are used such as the [[Augmented Lagrangian method|Augmented Lagrange Multiplier]] Method (ALM<ref name=RPCA-LIN>{{cite journal|last=Z. Lin|author2= M. Chen|author3=L. Wu|author4=Y. Ma|title=The Augmented Lagrange Multiplier Method for Exact Recovery of Corrupted Low-Rank Matrices|journal= Journal of Structural Biology|volume= 181|issue= 2|pages= 116–27|arxiv= 1009.5055|doi= 10.1016/j.jsb.2012.10.010|pmid= 23110852|year= 2013|pmc=3565063}}</ref>), [[Alternating direction method of multipliers|Alternating Direction Method]] (ADM<ref name=RPCA-YUAN>{{cite journal|last=X. Yuan|author2= J. Yang|title=Sparse and Low-Rank Matrix Decomposition via Alternating Direction Methods|journal=Optimization Online|year=2009}}</ref>), Fast Alternating Minimization (FAM<ref name=RPCA-RODRIGUEZ>{{cite journal|last=P. Rodríguez|author2= B. Wohlberg|title=Fast Principal Component Pursuit Via Alternating Minimization|journal=IEEE International Conference on Image Processing, ICIP 2013|year=2013}}</ref>) \nor Iteratively Reweighted Least Squares (IRLS <ref name=RPCA-GUYON-1>{{cite journal|last=C. Guyon|author2= T. Bouwmans|author3=E. Zahzah|title=Foreground Detection via Robust Low Rank Matrix Decomposition including Spatio-Temporal Constraint|journal=International Workshop on Background Model Challenges, ACCV 2012|year=2012}}</ref><ref name=RPCA-GUYON-2>{{cite journal|last=C. Guyon|author2= T. Bouwmans|author3=E. Zahzah|title=Foreground Detection via Robust Low Rank Matrix Factorization including Spatial Constraint with Iterative Reweighted Regression|journal=International Conference on Pattern Recognition, ICPR 2012|year=2012}}</ref><ref name=RPCA-GUYON-3>{{cite journal|last=C. Guyon|author2= T. Bouwmans|author3=E. Zahzah|title=Moving Object Detection via Robust Low Rank Matrix Decomposition with IRLS scheme|journal=International Symposium on Visual Computing, ISVC 2012|year=2012}}</ref>).\n\n==Algorithms==\n===Non-convex method===\nThe state-of-the-art guaranteed algorithm for the robust PCA problem (with the input matrix being <math>M=L+S</math>) is an alternating minimization type algorithm.<ref name=\"netrapalli2014non\">{{cite journal|first1=Netrapalli|last1=P. |first2=Niranjan|last2=U.|first3=Sanghavi|last3=S.|first4=Anandkumar|last4=A.|first5=Jain|last5=P.|title=Non-convex robust PCA|journal=Advances in Neural Information Processing Systems|volume=1410|pages=1107–1115|year=2014|bibcode=2014arXiv1410.7660N|arxiv=1410.7660}}</ref> The computational complexity is <math>O\\left(m n r^2 \\log\\frac{1}{\\epsilon} \\right)</math> where the input is the superposition of a low-rank (of rank <math>r</math>) and a sparse matrix of dimension <math>m \\times n</math> and <math>\\epsilon</math> is the desired accuracy of the recovered solution, i.e., <math> \\| \\widehat{L}-L \\|_F \\leq \\epsilon</math> where <math>L</math> is the true low-rank component and <math>\\widehat{L}</math> is the estimated or recovered low-rank component. Intuitively, this algorithm performs projections of the residual on to the set of low-rank matrices (via the [[Singular value decomposition|SVD]] operation) and sparse matrices (via entry-wise hard thresholding) in an alternating manner - that is, low-rank projection of the difference the input matrix and the sparse matrix obtained at a given iteration followed by sparse projection of the difference of the input matrix and the low-rank matrix obtained in the previous step, and iterating the two steps until convergence.\n\n===Convex relaxation===\nThis method consists of relaxing the rank constraint <math>rank(L)</math> in the optimization problem to the [[nuclear norm]] <math>\\|L\\|_*</math> and the sparsity constraint <math>\\|S\\|_0</math> to <math>\\ell_1</math>-norm <math>\\|S\\|_1</math>. The resulting program can be solved using methods such as the method of Augmented Lagrange Multipliers.\n\n== Applications ==\nRPCA has many real life important applications particularly when the data under study can naturally be modeled as a low-rank plus a sparse contribution. Following examples are inspired by contemporary challenges in computer science, and depending on the applications, either the low-rank component or the sparse component could be the object of interest:\n\n=== Video surveillance ===\nGiven a sequence of surveillance video frames, it is often required to identify the activities that stand out from the background. If we stack the video frames as columns of a matrix M, then the low-rank component L<sub>0</sub> naturally corresponds to the stationary background and the sparse component S<sub>0</sub> captures the moving objects in the foreground.<ref name=RPCA>{{cite journal|last=Emmanuel J. Candes|author2=Xiaodong Li |author3=Yi Ma |author4=John Wright |title=Robust Principal Component Analysis?}}</ref><ref name=RPCA-BOUWMANS>{{cite journal|last=T. Bouwmans|author2= E. Zahzah|title=Robust PCA via Principal Component Pursuit: A Review for a Comparative Evaluation in Video Surveillance|journal=Special Issue on Background Models Challenge, Computer Vision and Image Understanding|year=2014}}</ref>\n\n=== Face recognition ===\nImages of a convex, [[Lambertian reflectance|Lambertian surface]] under varying illuminations span a low-dimensional subspace.<ref>{{cite journal|last=R. Basri|author2=D. Jacobs |title=Lambertian reflectance and linear subspaces}}</ref> This is one of the reasons for effectiveness of low-dimensional models for imagery data. In particular, it is easy to approximate images of a human's face by a low-dimensional subspace. To be able to correctly retrieve this subspace is crucial in many applications such as face recognition and alignment. It turns out that RPCA can be applied successfully to this problem to exactly recover the face.<ref name=RPCA/>\n\n== Surveys ==\n*Robust PCA <ref name=RPCA-BOUWMANS />\n*Dynamic RPCA <ref name=RPCA-VASWANI>{{cite journal|last=N. Vaswani|author2= T. Bouwmans|author3=S. Javed|author4=P. Narayanamurthy|title=Robust PCA and Robust Subspace Tracking|journal=Preprint|volume= 35|issue= 4|pages= 32–55|arxiv= 1711.09492| year=2017|bibcode=2017arXiv171109492V|doi= 10.1109/MSP.2018.2826566}}</ref>\n*Decomposition into Low-rank plus Additive Matrices <ref name=RPCA-BOUWMANS-COSREV>{{Cite journal|last=T. Bouwmans|author2= A. Sobral|author3= S. Javed|author4= S. Jung|author5= E. Zahzahg|title=Decomposition into Low-rank plus Additive Matrices for Background/Foreground Separation: A Review for a Comparative Evaluation with a Large-Scale Dataset |journal= Computer Science Review|volume= 23|pages= 1–71|arxiv=1511.01245|year=2015|doi= 10.1016/j.cosrev.2016.11.001}}</ref>\n*Low-rank models<ref name=LR-LIN>{{cite journal|last=Z. Lin|title=A Review on Low-Rank Models in Data Analysis|journal=Big Data and Information Analytics|volume=1|issue=2|pages=139–161|year=2016|doi=10.3934/bdia.2016001}}</ref>\n\n== Books, journals and workshops ==\n=== Books ===\n*T. Bouwmans, N. Aybat, and E. Zahzah. ''Handbook on Robust Low-Rank and Sparse Matrix Decomposition: Applications in Image and Video Processing'', CRC Press, Taylor and Francis Group, May 2016. (more information: http://www.crcpress.com/product/isbn/9781498724623)\n*Z. Lin, H. Zhang, \"Low-Rank Models in Visual Analysis: Theories, Algorithms, and Applications\", Academic Press, Elsevier, June 2017. (more information: https://www.elsevier.com/books/low-rank-models-in-visual-analysis/lin/978-0-12-812731-5)\n\n=== Journals ===\n*N. Vaswani, Y. Chi, T. Bouwmans, Special Issue on “[http://proceedingsoftheieee.ieee.org/upcoming-issues/pca/ Rethinking PCA for Modern Datasets: Theory, Algorithms, and Applications]”, Proceedings of the IEEE, 2018.\n*T. Bouwmans, N. Vaswani, P. Rodriguez, R. Vidal, Z. Lin, Special Issue on “[https://signalprocessingsociety.org/blog/ieee-jstsp-special-issue-data-science-robust-subspace-learning-and-tracking-theory-algorithms Robust Subspace Learning and Tracking: Theory, Algorithms, and Applications]”, IEEE Journal of Selected Topics in Signal Processing, December 2018.\n\n=== Workshops ===\n*RSL-CV 2015: Workshop on Robust Subspace Learning and Computer Vision in conjunction with ICCV 2015 (For more information: http://rsl-cv2015.univ-lr.fr/workshop/)\n*RSL-CV 2017: Workshop on Robust Subspace Learning and Computer Vision in conjunction with ICCV 2017 (For more information: http://rsl-cv.univ-lr.fr/2017/)\n\n=== Sessions ===\n* Special Session on \"Online Algorithms for Static and Dynamic Robust PCA and Compressive Sensing\" in conjunction with SSP 2018. (More information: https://ssp2018.org/)\n\n== Resources and libraries ==\n=== Websites ===\n*[http://sites.google.com/site/backgroundsubtraction/recent-background-modeling/background-modeling-via-rpca Background Subtraction Website]\n*[https://sites.google.com/site/robustdlam/ DLAM Website]\n*[http://perception.csl.illinois.edu/matrix-rank/sample_code.html Documentation from the University of Illinois]\n\n=== Libraries ===\nLRS Library (A. Sobral, L3i, Univ. La Rochelle, France), The LRSLibrary provides a collection of low-rank and sparse decomposition algorithms in MATLAB. The library was designed for motion segmentation in videos, but it can be also used or adapted for other computer vision. Currently the LRSLibrary contains a total of 72 matrix-based and tensor-based algorithms. The LRSLibrary was tested successfully in MATLAB R2013b both x86 and x64 versions.\n\n==References==\n{{Reflist}}\n\n==External links==\n*[https://github.com/andrewssobral/lrslibrary#lrslibrary LRSLibrary]\n\n[[Category:Matrix decompositions]]\n[[Category:Dimension reduction]]\n[[Category:Robust statistics]]"
    },
    {
      "title": "Schur decomposition",
      "url": "https://en.wikipedia.org/wiki/Schur_decomposition",
      "text": "In the [[mathematics|mathematical]] discipline of [[linear algebra]], '''the Schur decomposition''' or '''Schur triangulation''', named after [[Issai Schur]], is a [[matrix decomposition]]. It allows to write an arbitrary matrix as [[Matrix equivalence|unitarily equivalent]] to an [[Upper-triangular matrix|upper triangular matrix]] whose diagonal elements are the eigenvalues of the original matrix.\n\n== Statement ==\nThe Schur decomposition reads as follows: if ''A'' is a ''n'' &times; ''n'' [[square matrix]] with [[complex numbers|complex]] entries, then ''A'' can be expressed as<ref name=horn1985>{{cite book|author1=Horn, R.A.  |author2=Johnson, C.R. |lastauthoramp=yes |year=1985|title=Matrix Analysis|publisher=Cambridge University Press|isbn=0-521-38632-2}}(Section 2.3 and further at [{{Google books|plainurl=y|id=PlYQN0ypTwEC|page=79|text=Schur}} p. 79])</ref><ref name=Golub1996>{{cite book|author1=Golub, G.H.  |author2=Van Loan, C.F. |lastauthoramp=yes |year=1996|title=Matrix Computations|edition=3rd|publisher=Johns Hopkins University Press|isbn=0-8018-5414-8}}(Section 7.7 at [{{Google books|plainurl=y|id=mlOa7wPX6OYC|page=313|text=Schur Decomposition}} p. 313])</ref><ref>{{cite book |first=James R. |last=Schott |title=Matrix Analysis for Statistics |location=New York |publisher=John Wiley & Sons |year=2016 |edition=3rd |isbn=978-1-119-09247-6 |pages=175–178 |url=https://books.google.com/books?id=e-JFDAAAQBAJ&pg=PA177 }}</ref>\n\n:<math> A = Q U Q^{-1}</math>\n\nwhere ''Q'' is a [[unitary matrix]] (so that its inverse ''Q''<sup>−1</sup> is also the [[conjugate transpose]] ''Q''* of ''Q''), and ''U'' is an [[upper triangular matrix]], which is called a '''Schur form''' of ''A''. Since ''U'' is [[similar (linear algebra)|similar]] to ''A'', it has the same [[Spectrum of a matrix|spectrum]], and since it is triangular, its [[eigenvalue]]s are the diagonal entries of ''U''.\n\nThe Schur decomposition implies that there exists a nested sequence of ''A''-invariant subspaces {0} = ''V''<sub>0</sub> ⊂ ''V''<sub>1</sub> ⊂ ... ⊂ ''V<sub>n</sub>'' = '''C'''<sup>''n''</sup>, and that there exists an ordered [[orthonormal basis]] (for the standard [[Hermitian form]] of '''C'''<sup>''n''</sup>) such that the first ''i'' basis vectors span ''V''<sub>''i''</sub> for each ''i'' occurring in the nested sequence. Phrased somewhat differently, the first part says that a [[linear operator]] ''J'' on a complex finite-dimensional vector space [[Orbit-stabilizer theorem#Orbits and stabilizers|stabilizes]] a complete [[Flag (linear algebra)|flag]] (''V''<sub>1</sub>,...,''V<sub>n</sub>'').\n\n== Proof ==\nA constructive proof for the Schur decomposition is as follows: every operator ''A'' on a complex finite-dimensional vector space has an eigenvalue ''&lambda;'', corresponding to some eigenspace ''V<sub>&lambda;</sub>''. Let ''V<sub>&lambda;</sub>''<sup>⊥</sup> be its orthogonal complement. It is clear that, with respect to this orthogonal decomposition, ''A'' has matrix representation (one can pick here any orthonormal bases ''Z<sub>1</sub>'' and ''Z<sub>2</sub>'' spanning ''V<sub>&lambda;</sub>'' and ''V<sub>&lambda;</sub>''<sup>⊥</sup> respectively)\n\n:<math>\\begin{bmatrix} Z_1 & Z_2 \\end{bmatrix}^{*} A \\begin{bmatrix}Z_1 & Z_2\\end{bmatrix} = \\begin{bmatrix} \\lambda \\, I_{\\lambda} & A_{12} \\\\ 0 & A_{22} \\end{bmatrix}: \n\\begin{matrix}\nV_{\\lambda} \\\\\n\\oplus \\\\\nV_{\\lambda}^{\\perp}\n\\end{matrix}\n\\rightarrow\n\\begin{matrix}\nV_{\\lambda} \\\\\n\\oplus \\\\\nV_{\\lambda}^{\\perp}\n\\end{matrix}\n</math>\n\nwhere ''I<sub>&lambda;</sub>'' is the identity operator on ''V<sub>&lambda;</sub>''. The above matrix would be upper-triangular except for the ''A''<sub>22</sub> block. But exactly the same procedure can be applied to the sub-matrix ''A''<sub>22</sub>, viewed as an operator on ''V<sub>&lambda;</sub>''<sup>⊥</sup>, and its submatrices. Continue this way n times. Thus the space '''C'''<sup>''n''</sup> will be exhausted and the procedure has yielded the desired result.\n\nThe above argument can be slightly restated as follows: let ''&lambda;'' be an eigenvalue of ''A'', corresponding to some eigenspace ''V<sub>&lambda;</sub>''. ''A'' induces an operator ''T'' on the [[quotient space (linear algebra)|quotient space]] '''C'''<sup>''n''</sup> modulo ''V<sub>&lambda;</sub>''. This operator is precisely the ''A''<sub>22</sub> submatrix from above. As before, ''T'' would have an eigenspace, say ''W<sub>&mu;</sub>'' ⊂ '''C'''<sup>''n''</sup> modulo ''V<sub>&lambda;</sub>''. Notice the preimage of ''W<sub>&mu;</sub>'' under the quotient map is an [[invariant subspace]] of ''A'' that contains ''V<sub>&lambda;</sub>''. Continue this way until the resulting quotient space has dimension 0. Then the successive preimages of the eigenspaces found at each step form a flag that ''A'' stabilizes.\n\n== Notes ==\nAlthough every square matrix has a Schur decomposition, in general this decomposition is not unique. For example, the eigenspace ''V<sub>&lambda;</sub>'' can have dimension > 1, in which case any orthonormal basis for ''V<sub>&lambda;</sub>'' would lead to the desired result.\n\nWrite the triangular matrix ''U'' as ''U'' = ''D'' + ''N'', where ''D'' is diagonal and ''N'' is strictly upper triangular (and thus a [[nilpotent matrix]]). The diagonal matrix ''D'' contains the eigenvalues of ''A'' in arbitrary order (hence its Frobenius norm, squared, is the sum of the squared moduli of the eigenvalues of ''A'', while \nthe Frobenius norm of ''A'', squared, is the sum of the squared [[singular value]]s of ''A''). The nilpotent part ''N'' is generally not unique either, but its [[Matrix norm#Frobenius norm|Frobenius norm]] is uniquely determined by ''A'' (just because the Frobenius norm of A is equal to the Frobenius norm of  ''U'' = ''D'' + ''N'').\n\nIt is clear that if ''A'' is a [[normal matrix]], then ''U'' from its Schur decomposition must be a [[diagonal matrix]] and the column vectors of ''Q'' are the [[eigenvector]]s of ''A''. Therefore, the Schur decomposition extends the [[spectral decomposition]]. In particular, if ''A'' is [[Positive-definite matrix|positive definite]], the Schur decomposition of ''A'', its spectral decomposition, and its [[singular value decomposition]] coincide.\n\nA [[commutative operation|commuting]] family {''A<sub>i</sub>''} of matrices can be simultaneously triangularized, i.e. there exists a unitary matrix ''Q'' such that, for every ''A<sub>i</sub>'' in the given family, ''Q A<sub>i</sub> Q*'' is upper triangular. This can be readily deduced from the above proof. Take element ''A'' from {''A<sub>i</sub>''} and again consider an eigenspace ''V<sub>A</sub>''. Then ''V<sub>A</sub>'' is invariant under all matrices in {''A<sub>i</sub>''}. Therefore, all matrices in {''A<sub>i</sub>''} must share one common eigenvector in ''V<sub>A</sub>''. Induction then proves the claim. As a corollary, we have that every commuting family of normal matrices can be simultaneously [[Diagonalizable matrix|diagonalized]].\n\nIn the infinite dimensional setting, not every [[bounded operator]] on a [[Banach space]] has an invariant subspace. However, the upper-triangularization of an arbitrary square matrix does generalize to [[compact operator]]s. Every [[compact operator]] on a complex Banach  space has a [[Flag (linear algebra)#Subspace nest|nest]] of closed invariant subspaces.\n\n== Computation ==\nThe Schur decomposition of a given matrix is numerically computed by the [[QR algorithm]] or its variants. In other words, the roots of the [[characteristic polynomial]] corresponding to the matrix are not necessarily computed ahead in order to obtain its Schur decomposition. Conversely, the [[QR algorithm]] can be used to compute the roots of any given [[characteristic polynomial]] by finding the Schur decomposition of its [[companion matrix]]. Similarly, the [[QR algorithm]] is used to compute the eigenvalues of any given matrix, which are the diagonal entries of the upper triangular matrix of the Schur decomposition.\nSee the Nonsymmetric Eigenproblems section in [[LAPACK]] Users' Guide.<ref>{{cite book|last1=Anderson|first1=E|last2=Bai|first2=Z|last3=Bischof|first3=C|last4=Blackford|first4=S|last5=Demmel|first5=J|last6=Dongarra|first6=J|last7=Du Croz|first7=J|last8=Greenbaum|first8=A|last9=Hammarling|first9=S|last10=McKenny|first10=A|last11=Sorensen|first11=D|title=LAPACK Users guide|date=1995|publisher=Society for Industrial and Applied Mathematics|location=Philadelphia, PA|isbn=0-89871-447-8|url=http://www.netlib.org/lapack/lug/}}</ref>\n\n== Applications ==\n[[Lie theory]] applications include:\n* Every invertible operator is contained in a [[Borel group]].\n* Every operator fixes a point of the [[flag manifold]].\n\n== Generalized Schur decomposition ==\nGiven square matrices ''A'' and ''B'', the '''generalized Schur decomposition''' factorizes both matrices as <math>A=QSZ^*</math> and <math>B=QTZ^*</math>, where ''Q'' and ''Z'' are [[unitary matrix|unitary]], and ''S'' and ''T'' are [[upper triangular]].  The generalized Schur decomposition is also sometimes called the '''QZ decomposition'''.<ref name=Golub1996/>{{rp|375}}\n\nThe generalized [[eigenvalue]]s <math>\\lambda</math> that solve the [[Eigendecomposition of a matrix#Additional topics|generalized eigenvalue problem]] <math>Ax=\\lambda Bx</math> (where ''x'' is an unknown nonzero vector) can be calculated as the ratio of the diagonal elements of ''S'' to those of ''T''. That is, using subscripts to denote matrix elements, the ''i''th generalized eigenvalue <math>\\lambda_i</math> satisfies <math>\\lambda_i=S_{ii}/T_{ii}</math>.\n\n== References ==\n<references />\n\n[[Category:Matrix theory]]\n[[Category:Articles containing proofs]]\n[[Category:Matrix decompositions]]"
    },
    {
      "title": "Symbolic Cholesky decomposition",
      "url": "https://en.wikipedia.org/wiki/Symbolic_Cholesky_decomposition",
      "text": "{{Unreferenced|date=December 2009}}\nIn the [[mathematics|mathematical]] subfield of [[numerical analysis]] the '''symbolic Cholesky decomposition''' is an [[algorithm]] used to determine the non-zero pattern for the <math>L</math> factors of a [[symmetric matrix|symmetric]] [[sparse matrix]] when applying the [[Cholesky decomposition]] or variants.\n\n==Algorithm==\nLet\n<math>A=(a_{ij}) \\in \\mathbb{K}^{n \\times n}</math>\nbe a sparse symmetric positive definite matrix with elements from a field <span style=\"vertical-align:18%;\"><math>\\mathbb{K}</math></span>, which we wish to factorize as <math>A = LL^T\\,</math>.\n\nIn order to implement an efficient sparse factorization it has been found to be necessary to determine the non zero structure of the factors before doing any numerical work. To write the algorithm down we use the following notation:\n\n*  Let <math>\\mathcal{A}_i</math> and <math>\\mathcal{L}_j</math> be sets representing the non-zero patterns of columns {{mvar|i}} and {{mvar|j}} (below the diagonal only, and including diagonal elements) of matrices {{mvar|A}} and {{mvar|L}} respectively.\n* Take <math>\\min\\mathcal{L}_j</math> to mean the smallest element of <math>\\mathcal{L}_j</math>.\n* Use a parent function <math>\\pi(i)\\,\\!</math> to define the elimination tree within the matrix.\n\nThe following algorithm gives an efficient \nsymbolic factorization of {{mvar|A}} : \n\n: <math>\n\\begin{align}\n& \\pi(i):=0~\\mbox{for all}~i\\\\\n& \\mbox{For}~i:=1~\\mbox{to}~n\\\\\n& \\qquad \\mathcal{L}_i := \\mathcal{A}_i\\\\\n& \\qquad \\mbox{For all}~j~\\mbox{such that}~\\pi(j) = i\\\\\n& \\qquad \\qquad \\mathcal{L}_i := (\\mathcal{L}_i \\cup \\mathcal{L}_j)\\setminus\\{j\\}\\\\\n& \\qquad \\pi(i) := \\min(\\mathcal{L}_i\\setminus\\{i\\})\n\\end{align}\n</math>\n\n{{DEFAULTSORT:Symbolic Cholesky Decomposition}}\n[[Category:Articles with example pseudocode]]\n[[Category:Matrix decompositions]]"
    },
    {
      "title": "Weyr canonical form",
      "url": "https://en.wikipedia.org/wiki/Weyr_canonical_form",
      "text": "[[Image:WeyrMatrixExample.jpg|right|thumb|300px|The image shows an example of a general Weyr matrix consisting of two blocks each of which is a basic Weyr matrix. The basic Weyr matrix in the top-left corner has the structure (4,2,1) and the other one has the structure (2,2,1,1).]]\n\nIn [[mathematics]], in [[linear algebra]], a '''Weyr canonical form''' (or, '''Weyr form''' or '''Weyr matrix''') is a [[square matrix]] satisfying certain conditions. A square matrix is said to be ''in'' the Weyr [[canonical form]] if the matrix satisfies the conditions defining the Weyr canonical form. The Weyr form was discovered by the [[Czech Republic|Czech]] [[mathematician]] [[Eduard Weyr]] in 1885.<ref>{{cite journal|last=Eduard Weyr|title=Répartition des matrices en espèces et formation de toutes les espèces|journal=[[Comptes Rendus]], Paris|year=1885|volume=100|pages=966–969|url=http://dml.cz/bitstream/handle/10338.dmlcz/400545/DejinyMat_02-1995-1_15.pdf|accessdate=10 December 2013}}</ref><ref>{{cite journal|last=Eduard Weyr|title=Zur Theorie der bilinearen Formen|journal=Monatsh. Math. Physik|year=1890|volume=1|pages=163–236|url=http://www.literature.at/viewer.alo?objid=12450&viewmode=fullscreen&scale=3.33&rotate=&page=166}}</ref><ref name=Weyr>{{cite book|author1=Kevin C. Meara |author2=John Clark |author3=Charles I. Vinsonhaler |title=Advanced Topics in Linear Algebra: Weaving Matrix Problems through the Weyr Form|year=2011|publisher=Oxford University Press}}</ref>  The Weyr form did not become popular among mathematicians and it was overshadowed by the closely related, but distinct, canonical form known by the name [[Jordan normal form|Jordan canonical form]].<ref name=\"Weyr\"/> The Weyr form has been rediscovered several times since Weyr’s original discovery in 1885.<ref name=\"Weyr44\">{{cite book|author1=Kevin C. Meara |author2=John Clark |author3=Charles I. Vinsonhaler |title=Advanced Topics in Linear Algebra: Weaving Matrix Problems through the Weyr Form|year=2011|publisher=Oxford University Press|pages=44, 81–82}}</ref>  This form has been variously called as ''modified Jordan form,'' ''reordered Jordan form,'' ''second Jordan form,'' and ''H-form''.<ref name=\"Weyr44\"/> The current terminology is credited to Shapiro who introduced it in a paper published in the [[American Mathematical Monthly]] in 1999.<ref name=\"Weyr44\"/><ref>{{cite journal|last=Shapiro, H.|title=The Weyr characteristic|journal=The American Mathematical Monthly|year=1999|volume=106|pages=919–929|doi=10.2307/2589746}}</ref>\n\nRecently several applications have been found for the Weyr matrix. Of particular interest is an application of the Weyr matrix in the study of [[phylogenetics|phylogenetic invariant]]s in [[biomathematics]].\n\n==Definitions==\n\n===Basic Weyr matrix===\n\n===Definition===\n\nA basic Weyr matrix with [[eigenvalue]] <math>\\lambda</math>  is an <math>n\\times n</math> matrix <math>W</math> of the following form: There is a [[Partition (number theory)|partition]] \n: <math>n_1 + n_2+ \\cdots +n_r=n</math> of  <math>n</math> with <math>n_1\\ge n_2\\ge \\cdots \\ge  n_r\\ge 1</math> \nsuch that, when <math>W</math> is viewed as an <math> r \\times r</math>  [[block matrix|blocked matrix]] <math>(W_{ij})</math>, where the <math> (i, j)</math> block <math> W_{ij}</math>  is an <math>n_i \\times n_j</math> matrix, the following three features are present:\n# The main [[diagonal]] blocks <math> W_{ii}</math> are the <math>n_i\\times  n_i </math> [[scalar matrix|scalar matrices]] <math>\\lambda I </math> for <math>i = 1, \\ldots  , r</math>.\n# The first [[superdiagonal]] blocks <math>W_{i,i+1} </math> are full [[column rank]] <math>n_i \\times n_{i+1}</math> matrices in [[reduced row-echelon form]] (that is, an [[identity matrix]] followed by zero rows) for <math> i=1, \\ldots, r-1 </math>.\n# All other blocks of ''W'' are zero (that is, <math> W_{ij} = 0 </math> when <math>j \\ne  i, i + 1</math>).\n\nIn this case, we say that <math>W</math> has Weyr structure <math>(n_1, n_2, \\ldots  , n_r)</math>.\n\n===Example===\n\nThe following is an example of a basic Weyr matrix.\n\n<center>\n<math>W = </math>\n[[File:BasicWeyrMatrix.jpg|A Basic Weyr matrix with structure (4,2,2,1)]]\n<math> =\n\\begin{bmatrix}\nW_{11} & W_{12} &  &    \\\\\n       & W_{22} & W_{23} &    \\\\\n       &        & W_{33} & W_{34}   \\\\\n       &        &        & W_{44}  \\\\\n\\end{bmatrix}\n</math>\n</center>\n\nIn this matrix, <math> n=9</math> and <math> n_1=4, n_2=2, n_3=2, n_4=1</math>. So <math> W</math> has the Weyr structure <math>(4,2,2,1)</math>. Also,\n\n<center>\n<math>\nW_{11} =\n\\begin{bmatrix}\n\\lambda &      0 &       0 &       0 \\\\\n   0     &\\lambda &       0 &       0 \\\\\n   0     &    0    & \\lambda &       0 \\\\\n   0     &    0    &     0    & \\lambda \\\\\n\\end{bmatrix} = \\lambda I_4, \\quad\nW_{22} =\n\\begin{bmatrix}\n\\lambda &      0 \\\\\n    0    &\\lambda & \\\\\n \\end{bmatrix} = \\lambda I_2, \\quad\nW_{33} =\n\\begin{bmatrix}\n\\lambda &      0 \\\\\n    0    &\\lambda & \\\\\n \\end{bmatrix} =\\lambda I_2, \\quad\nW_{44} =\n\\begin{bmatrix}\n\\lambda \\\\\n \\end{bmatrix} = \\lambda I_1\n</math>\n</center>\n\nand\n\n<center>\n<math>\nW_{12}=\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\\\\\n0 & 0\\\\\n0 & 0\\\\\n\\end{bmatrix}, \\quad\nW_{23}=\n\\begin{bmatrix}\n1 & 0 \\\\\n0& 1\\\\\n\\end{bmatrix},\\quad\nW_{34} =\n\\begin{bmatrix}\n1 \\\\\n0 \\\\\n\\end{bmatrix}.\n</math>\n</center>\n\n===General Weyr matrix===\n\n===Definition===\n\nLet <math> W </math>  be a square matrix and let <math>\\lambda_1, \\ldots, \\lambda_k  </math> be the distinct eigenvalues of <math>W  </math>. We say that <math> W </math> is in Weyr form (or is a Weyr matrix) if <math> W </math>  has the following form:\n\n<center>\n<math>\nW =\n\\begin{bmatrix}\nW_1 &     &        &    \\\\\n    & W_2 &        &    \\\\\n    &     & \\ddots &    \\\\\n    &     &        & W_k \\\\\n\\end{bmatrix}\n</math>\n</center>\n\nwhere   <math> W_i </math>  is a basic Weyr matrix with eigenvalue <math> \\lambda_i </math> for <math> i = 1, \\ldots , k</math>.\n\n===Example===\n\nThe following image shows an example of a general Weyr matrix consisting of three basic Weyr matrix blocks. The basic Weyr matrix in the top-left corner has the structure (4,2,1) with eigenvalue 4, the middle block has structure (2,2,1,1) with eigenvalue -3 and the one in the lower-right corner has the structure (3, 2) with eigenvalue 0.\n\n<center>\n[[Image:WeyrMatrixExample02.jpg]]\n</center>\n\n==Relation between Weyr and Jordan forms==\n\nThe Weyr canonical form <math>W=P^{-1} J P</math> is related to the Jordan form <math>J</math> by a simple permutation <math>P</math> for each Weyr basic block as follows: The first index of each Weyr subblock forms the largest Jordan chain.  After crossing out these rows and columns, the first index of each new subblock forms the second largest Jordan chain, and so forth.<ref name=\"sergeichuk\">Sergeichuk, [https://arxiv.org/abs/0709.2485v1 \"Canonical matrices for linear matrix problems\"], Arxiv:0709.2485 [math.RT], 2007</ref>\n\n==The Weyr form is canonical==\n\nThat the Weyr form is a canonical form of a matrix is a consequence of the following result:<ref name=\"Weyr\"/> ''Each square matrix <math>A</math> over an algebraically closed field is similar to a Weyr matrix <math>W</math> which is unique up to permutation of its basic blocks. The matrix <math>W</math> is called the Weyr (canonical) form of <math>A</math>.''\n\n==Computation of the Weyr canonical form==\n\n===Reduction to the nilpotent case===\n\nLet <math>A</math> be a square matrix of order <math>n</math> over an [[algebraically closed field]] and let the distinct eigenvalues of <math>A</math> be <math>\\lambda_1, \\lambda_2, \\ldots, \\lambda_k</math>. The [[Jordan–Chevalley decomposition]] theorem states that <math>A</math> is [[matrix similarity|similar]] to a block diagonal matrix of the form\n\n<math>\nA=\n\\begin{bmatrix}\n\\lambda_1I + N_1&   &   &    \\\\\n    & \\lambda_2I + N_2 &  &  \\\\\n    &      & \\ddots & \\\\\n    &      &        & \\lambda_kI + N_k \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\lambda_1I &   &   &    \\\\\n    & \\lambda_2I  &  &  \\\\\n    &      & \\ddots & \\\\\n    &      &        & \\lambda_kI  \\\\\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n N_1&   &   &    \\\\\n    &  N_2 &  &  \\\\\n    &      & \\ddots & \\\\\n    &      &        &  N_k \\\\\n\\end{bmatrix}\n=\nD+N\n</math>\n\nwhere <math>D</math> is a [[diagonal matrix]], <math>N</math> is a [[nilpotent matrix]], and <math>[D,N]=0</math>, justifying the reduction of <math>N</math> into subblocks <math>N_i</math>. So the problem of reducing <math>A</math> to the Weyr form reduces to the problem of reducing the nilpotent matrices <math>N_i</math> to the Weyr form.  This is leads to the generalized [[eigenspace]] decomposition theorem.\n\n===Reduction of a nilpotent matrix to the Weyr form===\n\nGiven a nilpotent square matrix <math>A</math> of order <math> n</math> over an algebraically closed field <math> F</math>, the following algorithm produces an invertible matrix <math> C </math> and a  Weyr matrix <math> W</math> such that <math>W=C^{-1}AC</math>.\n\n'''Step 1'''\n\nLet <math>A_1=A</math>\n\n'''Step 2'''\n\n# Compute a [[Basis (linear algebra)|basis]] for the [[null space]] of  <math>A_1</math>. \n# Extend the basis for the null space of  <math>A_1</math> to a basis for the  <math>n</math>-dimensional vector space  <math>F^n</math>.\n# Form the matrix  <math>P_1</math> consisting of these basis vectors.\n# Compute <math> P_1^{-1}A_1P_1=\\begin{bmatrix}0 & B_2 \\\\ 0 & A_2 \\end{bmatrix}</math>. <math>A_2</math> is a square matrix of size  <math>n</math> &minus; nullity  <math>(A_1)</math>.\n\n'''Step 3'''\n\nIf <math>A_2</math> is nonzero, repeat Step 2 on <math>A_2</math>.\n\n# Compute a basis for the null space of  <math>A_2</math>.\n# Extend the basis for the null space of  <math>A_2</math> to a basis for the  vector space  having dimension <math>n</math> &minus; nullity  <math>(A_1)</math>.\n# Form the matrix  <math>P_2</math> consisting of these basis vectors.\n# Compute <math> P_2^{-1}A_2P_2=\\begin{bmatrix}0 & B_3 \\\\ 0 & A_3 \\end{bmatrix}</math>. <math>A_2</math> is a square matrix of size  <math>n</math> &minus; nullity  <math>(A_1)</math> &minus; nullity<math>(A_2)</math>.\n\n'''Step 4'''\n\nContinue the processes of Steps 1 and 2 to obtain increasingly smaller square matrices <math>A_1, A_2, A_3, \\ldots</math>  and associated [[invertible matrix|invertible matrices]] <math>P_1, P_2, P_3, \\ldots</math> until the first zero matrix <math>A_r</math> is obtained.\n\n'''Step 5'''\n\nThe Weyr structure of <math>A</math> is <math>(n_1,n_2, \\ldots, n_r)</math> where <math>n_i </math> = nullity<math>(A_i)</math>.\n\n'''Step 6'''\n\n# Compute the matrix <math> P = P_1 \\begin{bmatrix} I & 0 \\\\ 0 & P_2 \\end{bmatrix}\\begin{bmatrix} I & 0 \\\\ 0 & P_3 \\end{bmatrix}\\cdots \\begin{bmatrix} I & 0 \\\\ 0 & P_r \\end{bmatrix}</math> (here the <math>I</math>'s are appropriately sized identity matrices).\n# Compute <math>X=P^{-1}AP</math>. <math>X</math> is a matrix of the following form:\n\n:: <math> X = \\begin{bmatrix}0 & X_{12} & X_{13} & \\cdots & X_{1,r-1} &X_{1r}\\\\  & 0 & X_{23} & \\cdots & X_{2,r-1} & X_{2r}\\\\  &  &  & \\ddots & \\\\ & & & \\cdots & 0& X_{r-1,r} \\\\ & & & & & 0 \\end{bmatrix}</math>.\n\n'''Step 7'''\n\nUse elementary row operations to find an invertible matrix <math> Y_{r-1}</math> of appropriate size such that the product <math>Y_{r-1}X_{r,r-1}</math> is a matrix of the form <math>I_{r,r-1}= \\begin{bmatrix} I \\\\ O \\end{bmatrix}</math>.\n\n'''Step 8'''\n\nSet <math>Q_1= </math> diag <math>(I,I, \\ldots, Y_{r-1}^{-1}, I)</math> and compute <math> Q_1^{-1}XQ_1</math>. In this matrix, the <math>(r,r-1)</math>-block is <math>I_{r,r-1}</math>.\n\n'''Step 9'''\n\nFind a matrix <math>R_1</math> formed as a product of [[elementary matrix|elementary matrices]] such that <math> R_1^{-1} Q_1^{-1}XQ_1R_1</math> is  a matrix in which all the blocks above the block <math>I_{r,r-1}</math> contain only <math>0</math>'s.\n\n'''Step 10'''\n\nRepeat Steps 8 and 9 on column <math> r-1</math> converting <math>(r-1, r-2)</math>-block to <math>I_{r-1,r-2}</math> via [[conjugation (group theory)|conjugation]] by some invertible matrix <math>Q_2</math>. Use this  block to clear out the blocks above, via conjugation by a product <math>R_2</math>  of elementary matrices.\n\n'''Step 11'''\n\nRepeat these processes on <math>r-2,r-3,\\ldots , 3, 2</math> columns, using conjugations by <math> Q_3, R_3,\\ldots , Q_{r-2}, R_{r-2}, Q_{r-1} </math>. The resulting matrix <math>W</math> is now in Weyr form.\n\n'''Step 12'''\n\nLet    <math> C = P_1 \\text{diag} (I, P_2) \\cdots \\text{diag}(I, P_{r-1})Q_1R_1Q_2\\cdots  R_{r-2}Q_{r-1}</math>.     Then <math> W = C^{-1}AC</math>.\n\n==Applications of the Weyr form==\nSome well-known applications of the Weyr form are listed below:<ref name=\"Weyr\"/>\n\n# The Weyr form can be used to simplify the proof of Gerstenhaber’s Theorem which asserts that the subalgebra generated by two commuting <math>n \\times n</math> matrices has dimension at most <math>n</math>. \n# A set of finite matrices is said to be approximately simultaneously diagonalizable if they can be perturbed to simultaneously diagonalizable matrices.  The Weyr form is used to prove approximate simultaneous diagonalizability of various classes of matrices.  The approximate simultaneous diagonalizability property has applications  in the study of phylogenetic invariants in [[biomathematics]]. \n# The Weyr form can be used to simplify the proofs of the irreducibility of the variety of all ''k''-tuples of commuting complex matrices.\n\n==References==\n{{reflist}}\n\n[[Category:Linear algebra]]\n[[Category:Matrix theory]]\n[[Category:Matrix normal forms]]\n[[Category:Matrix decompositions]]"
    },
    {
      "title": "Blind signal separation",
      "url": "https://en.wikipedia.org/wiki/Blind_signal_separation",
      "text": "#REDIRECT [[Signal separation]] {{R from merge}}\n\n[[Category:Statistical mechanics]]\n[[Category:Signal processing]]\n[[Category:Singular value decomposition]]"
    },
    {
      "title": "Generalized singular value decomposition",
      "url": "https://en.wikipedia.org/wiki/Generalized_singular_value_decomposition",
      "text": "{{Use dmy dates|date=August 2012}}\nIn [[linear algebra]], the '''generalized singular value decomposition''' ('''GSVD''') is the name of two different techniques based on the [[singular value decomposition]]. The two versions differ because one version decomposes two (or more) matrices (much like [[Principal component analysis#Higher order|higher order PCA]]) and the other version uses a set of constraints imposed on the left and right singular vectors.\n\n==Higher order version==\nThe '''generalized singular value decomposition''' ('''GSVD''') is a [[matrix decomposition]] more general than the [[singular value decomposition]]. It is used to study the [[condition number|conditioning]] and [[regularization (mathematics)|regularization]] of linear systems with respect to quadratic [[semi-norm]]s.\n\nLet <math>\\mathbb{F} = \\mathbb{R}</math>, or <math>\\mathbb{F} = \\mathbb{C}</math>.\nGiven matrices <math>A \\in \\mathbb{F}^{m \\times n}</math> and <math>B \\in \\mathbb{F}^{p \\times n}</math>, their GSVD is given by\n:<math>A=U\\Sigma_1 [ X, 0] Q^*</math>\nand\n:<math>B=V\\Sigma_2 [ X, 0] Q^*</math>\n\nwhere <math>U \\in \\mathbb{F}^{m \\times m},V \\in \\mathbb{F}^{p \\times p}</math>, and <math>Q \\in \\mathbb{F}^{n \\times n}</math> are [[unitary matrix|unitary matrices]], and <math>X \\in \\mathbb{F}^{r \\times r} </math> is non-singular, where <math>r = rank([A^*,B^*])</math>. Also,\n<math>\\Sigma_1 \\in \\mathbb{F}^{m \\times r}</math> is non-negative diagonal, and <math>\\Sigma_2 \\in \\mathbb{F}^{p \\times r}</math> is non-negative block-diagonal, with diagonal blocks; <math>\\Sigma_2 </math> is not always diagonal. It holds that <math>\\Sigma_1^T \\Sigma_1 = \\lceil\\alpha_1^2, \\dots, \\alpha_r^2\\rfloor</math> and <math>\\Sigma_2^T \\Sigma_2 = \\lceil\\beta_1^2, \\dots, \\beta_r^2\\rfloor</math>, and that <math>\\Sigma_1^T \\Sigma_1 + \\Sigma_2^T \\Sigma_2 = I_r</math>. This implies <math> 0 \\le \\alpha_i,\\beta_i\\le 1</math>.\nThe ratios <math>\\sigma_i=\\alpha_i/\\beta_i</math> are called the ''generalized singular values'' of <math>A</math> and <math>B</math>. If <math>B</math> is square and invertible, then the generalized singular values ''are'' the singular values, and <math>U</math> and <math>V</math> are the matrices of singular vectors, of the matrix <math> AB^{-1}</math>. Further, if <math>B = I</math>, then the GSVD reduces to the singular value decomposition, explaining the name.\n\n==Weighted version==\nThe weighted version of the '''generalized singular value decomposition''' ('''GSVD''') is a constrained [[matrix decomposition]] with constraints imposed on the left and right singular vectors of the [[singular value decomposition]].<ref>Jolliffe I.T. [https://www.springer.com/west/home/new+%26+forthcoming+titles+%28default%29?SGWID=4-40356-22-2285433-0 Principal Component Analysis], Series: [https://www.springer.com/west/home/statistics/statistical+theory+and+methods?SGWID=4-10129-69-173621571-0 Springer Series in Statistics], 2nd ed., Springer, NY, 2002, XXIX, 487 p. 28 illus. {{ISBN|978-0-387-95442-4}}\n</ref><ref>{{Cite book\n | author = Greenacre, Michael\n | publisher=Academic Press |location= London\n | year = 1983\n | title = Theory and Applications of Correspondence Analysis\n | isbn = 978-0-12-299050-2\n }}</ref><ref>{{Cite journal|author1=Abdi. H.  |author2=Williams, L.J. |year = 2010 | title = Principal component analysis. | journal = Wiley Interdisciplinary Reviews: Computational Statistics | volume = 2 |issue=4 | pages = 433–459 | doi=10.1002/wics.101}}</ref> This form of the ''GSVD'' is an extension of the ''SVD'' as such. Given the ''SVD'' of an ''m×n'' real or complex matrix ''M''\n:<math>M = U\\Sigma V^* \\,</math>\nwhere\n:<math>U^* W_u U = V^* W_v V = I.</math>\nWhere ''I'' is the [[Identity Matrix]] and where <math>U</math> and <math>V</math> are orthonormal given their constraints (<math>W_u</math> and <math>W_v</math>). Additionally, <math>W_u</math> and <math>W_v</math> are positive definite matrices (often diagonal matrices of weights). This form of the ''GSVD'' is the core of certain techniques, such as generalized principal component analysis and [[Correspondence analysis]].\n\nThe weighted form of the ''GSVD'' is called as such because, with the correct selection of weights, it ''generalizes'' many techniques (such as [[multidimensional scaling]] and [[linear discriminant analysis]])<ref>Abdi, H. (2007). Singular Value Decomposition (SVD) and Generalized Singular Value Decomposition (GSVD). In N.J. Salkind (Ed.): Encyclopedia of Measurement and Statistics. Thousand Oaks (CA): Sage. pp. 907-912.</ref>\n\n== Applications ==\nThe GSVD has been successfully applied to signal processing and big data, e.g., in genomic signal processing.<ref>{{Cite journal\n |author1=O. Alter |author2=P. O. Brown |author3=D. Botstein | title = Generalized Singular Value Decomposition for Comparative Analysis of Genome-Scale Expression Datasets of Two Different Organisms\n | journal = PNAS\n | volume = 100\n | issue = 6\n | pages = 3351–3356\n | date = March 2003\n | doi = 10.1073/pnas.0530258100\n |pmid=12631705 |pmc=152296}}</ref><ref>{{Cite journal\n |author1=C. H. Lee* |author2=B. O. Alpert* |author3=P. Sankaranarayanan |author4=O. Alter | title = GSVD Comparison of Patient-Matched Normal and Tumor aCGH Profiles Reveals Global Copy-Number Alterations Predicting Glioblastoma Multiforme Survival\n | journal = PLOS ONE\n | volume = 7\n | issue = 1\n | pages = e30098\n | date = January 2012\n | doi =  10.1371/journal.pone.0030098\n | id = [http://www.alterlab.org/research/highlights/pone.0030098_Highlight.pdf Highlight] | pmid=22291905 | pmc=3264559\n}}</ref><ref>{{Cite journal\n |author1=K. A. Aiello |author2=O. Alter | title = Platform-Independent Genome-Wide Pattern of DNA Copy-Number Alterations Predicting Astrocytoma Survival and Response to Treatment Revealed by the GSVD Formulated as a Comparative Spectral Decomposition\n | journal = PLOS ONE\n | volume = 11\n | issue = 10\n | pages = e0164546\n | date = October 2016\n | doi =  10.1371/journal.pone.0164546\n | pmid=27798635\n | pmc=5087864\n}}</ref>\nThese applications also inspired a higher-order GSVD (HO GSVD)<ref>{{Cite journal\n |author1=S. P. Ponnapalli |author2=M. A. Saunders |author3=C. F. Van Loan |author4=O. Alter | title = A Higher-Order Generalized Singular Value Decomposition for Comparison of Global mRNA Expression from Multiple Organisms\n | journal = PLOS ONE\n | volume = 6\n | issue = 12\n | pages = e28072\n | date = December 2011\n | doi = 10.1371/journal.pone.0028072\n | id = [http://www.alterlab.org/research/highlights/pone.0028072_Highlight.pdf Highlight] | pmid=22216090 | pmc=3245232\n}}</ref>\nand a tensor GSVD.<ref>{{Cite journal\n |author1=P. Sankaranarayanan* |author2=T. E. Schomay* |author3=K. A. Aiello |author4=O. Alter | title = Tensor GSVD of Patient- and Platform-Matched Tumor and Normal DNA Copy-Number Profiles Uncovers Chromosome Arm-Wide Patterns of Tumor-Exclusive Platform-Consistent Alterations Encoding for Cell Transformation and Predicting Ovarian Cancer Survival\n | journal = PLOS ONE\n | volume = 10\n | issue = 4\n | pages = e0121396\n | date = April 2015\n | doi = 10.1371/journal.pone.0121396\n | id = [http://www.eurekalert.org/pub_releases/2015-04/uouh-nmi040915.php AAAS EurekAlert! Press Release] and [https://www.nae.edu/Projects/20730/wtop/134897.aspx NAE Podcast Feature]\n | pmid=25875127 | pmc=4398562\n}}</ref>\n\n==See also==\n\n* {{cite journal | last1 = Paige | first1 = C. C. | last2 = Saunders | first2 = M. A. | year = 1981 | title = Towards a Generalized Singular Value Decomposition | url = | journal = SIAM J. Numer. Anal. | volume = 18 | issue = 3| pages = 398–405| doi = 10.1137/0718026 }}\n* [[Gene Golub]], and [[Charles Van Loan]], Matrix Computations, Third Edition, Johns Hopkins University Press, Baltimore, 1996, {{ISBN|0-8018-5414-8}}\n* Hansen, Per Christian, Rank-Deficient and Discrete Ill-Posed Problems: Numerical Aspects of Linear Inversion, SIAM Monographs on Mathematical Modeling and Computation 4.  {{ISBN|0-89871-403-6}}\n* [[LAPACK]] manual [http://www.netlib.org/lapack/lug/node36.html]\n\n==References==\n{{Reflist}}\n\n[[Category:Linear algebra]]\n[[Category:Singular value decomposition]]"
    },
    {
      "title": "Normal mode",
      "url": "https://en.wikipedia.org/wiki/Normal_mode",
      "text": "{{No footnotes|date=December 2010}}\nA '''normal mode''' of an [[oscillation|oscillating system]] is a pattern of motion in which all parts of the system move [[sinusoidal]]ly with the same frequency and with a fixed phase relation.  The free motion described by the normal modes takes place at the fixed frequencies.  These fixed frequencies of the normal modes of a system are known as its [[natural frequency|natural frequencies]] or [[Resonance|resonant frequencies]].  A physical object, such as a building, bridge, or molecule, has a set of normal modes and their natural frequencies that depend on its structure, materials and boundary conditions. When relating to music, normal modes of vibrating instruments (strings, air pipes, drums, etc.) are called \"harmonics\" or \"overtones\".\n\nThe most general motion of a system is a [[Superposition principle|superposition]] of its normal modes. The modes are normal in the sense that they can move independently, that is to say that an excitation of one mode will never cause motion of a different mode. In mathematical terms, normal modes are [[Orthogonality|orthogonal]] to each other.\n\n[[File:Drum vibration mode12.gif|right|thumb|248px|Vibration of a single normal mode of a circular disc with a pinned boundary condition along the entire outer edge. [[:commons:Category:Drum vibration animations|See other modes]].]]\n[[File:A cup of black coffee vibrating in normal modes.jpeg|right|thumb| A flash photo of a cup of black coffee vibrating in normal modes]]\n[[File:Spherical harmonic in water drop.ogv|thumb|Excitation of normal modes in a drop of water during the [[Leidenfrost effect]] ]]\n\n== General definitions ==\n\n=== Mode ===\nIn physics and engineering, for a [[dynamical system]] according to [[Wave|wave theory]], a '''mode''' is a [[standing wave]] state of excitation, in which all the components of the system will be affected sinusoidally under a specified fixed frequency.\n\nBecause no real system can perfectly fit under the standing wave framework, the ''mode'' concept is taken as a general characterization of specific states of oscillation, thus treating the dynamic system in a ''linear'' fashion, in which linear [[superposition principle|superposition]] of states can be performed.\n\nAs classical examples, there are:\n* In a mechanical dynamical system, a vibrating rope is the most clear example of a mode, in which the rope is the medium, the stress on the rope is the excitation, and the displacement of the rope with respect to its static state is the modal variable.\n* In an acoustic dynamical system, a single sound pitch is a mode, in which the air is the medium, the sound pressure in the air is the excitation, and the displacement of the air molecules is the modal variable.\n* In a structural dynamical system, a high tall building oscillating under its most flexural axis is a mode, in which all the material of the building -under the proper numerical simplifications- is the medium, the seismic/wind/environmental solicitations are the excitations and the displacements are the modal variable.\n* In an electrical dynamical system, a resonant cavity made of thin metal walls, enclosing a hollow space, for a particle accelerator is a pure standing wave system, and thus an example of a mode, in which the hollow space of the cavity is the medium, the RF source (a Klystron or another RF source) is the excitation and the electromagnetic field is the modal variable.\n* When relating to [[music]], normal modes of vibrating instruments (strings, air pipes, drums, etc.) are called \"[[harmonics]]\" or \"[[overtones]]\".\n* The concept of normal modes also finds application in [[optics]], [[quantum mechanics]], and [[molecular dynamics]].\n\nMost dynamical system can be excited under several modes. Each mode is characterized by one or several frequencies, according to the modal variable field. For example, a vibrating rope in the 2D space is defined by a single-frequency (1D axial displacement), but a vibrating rope in the 3D space is defined by two frequencies (2D axial displacement).\n\nFor a given amplitude on the modal variable, each mode will store a specific amount of energy, because of the sinusoidal excitation.\n\nFrom all the modes of a dynamical system, the ''normal'' or ''dominant'' mode of a system, will be the mode storing the minimum amount of energy, for a given amplitude of the modal variable. Or equivalently, for a given stored amount of energy, will be the mode imposing the maximum amplitude of the modal variable.\n\n===Mode numbers===\nA mode of vibration is characterized by a modal frequency and a mode shape. It is numbered according to the number of half waves in the vibration.  For example, if a vibrating beam with both ends pinned displayed a mode shape of half of a sine wave (one peak on the vibrating beam) it would be vibrating in mode 1. If it had a full sine wave (one peak and one trough) it would be vibrating in mode 2.\n\nIn a system with two or more dimensions, such as the pictured disk, each dimension is given a mode number.  Using [[polar coordinate system|polar coordinates]], we have a radial coordinate and an angular coordinate. If one measured from the center outward along the radial coordinate one would encounter a full wave, so the mode number in the radial direction is 2. The other direction is trickier, because only half of the disk is considered due to the antisymmetric (also called [[Symmetry in mathematics#Skew-symmetry|skew-symmetry]]) nature of a disk's vibration in the angular direction. Thus, measuring 180° along the angular direction you would encounter a half wave, so the mode number in the angular direction is 1. So the mode number of the system is 2–1 or 1–2, depending on which coordinate is considered the \"first\" and which is considered the \"second\" coordinate (so it is important to always indicate which mode number matches with each coordinate direction).\n\nIn linear systems each mode is entirely independent of all other modes.  In general all modes have different frequencies (with lower modes having lower frequencies) and different mode shapes.\n\n===Nodes===\n[[File:Mode Shape of a Round Plate with Node Lines.jpg|right|thumb|220px|A mode shape of a drum membrane, with nodal lines shown in pale green]]\nIn a one-dimensional system at a given mode the vibration will have nodes, or places where the displacement is always zero.  These nodes correspond to points in the mode shape where the mode shape is zero.  Since the vibration of a system is given by the mode shape multiplied by a time function, the displacement of the node points remain zero at all times.\n\nWhen expanded to a two dimensional system, these nodes become lines where the displacement is always zero.  If you watch the animation above you will see two circles (one about halfway between the edge and center, and the other on the edge itself) and a straight line bisecting the disk, where the displacement is close to zero.  In an idealized system these lines equal zero exactly, as shown to the right.\n\n==In mechanical systems==\n\n=== Coupled oscillators ===\nConsider two equal bodies (not affected by gravity), each of [[mass]] ''m'', attached to three springs, each with [[spring constant]] ''k''. They are attached in the following manner, forming a system that is physically symmetric:\n\n:[[File:Coupled Harmonic Oscillator.svg|300px]]\n\nwhere the edge points are fixed and cannot move. We'll use ''x''<sub>1</sub>(''t'') to denote the horizontal [[displacement (distance)|displacement]] of the left mass, and ''x''<sub>2</sub>(''t'') to denote the displacement of the right mass.\n\nIf one denotes acceleration (the second [[derivative]] of ''x''(''t'') with respect to time) as <math>\\scriptstyle \\ddot x</math>, the [[equations of motion]] are:\n\n:<math>\\begin{align}\n  m \\ddot x_1 &= - k x_1 + k (x_2 - x_1) = - 2 k x_1 + k x_2 \\\\\n  m \\ddot x_2 &= - k x_2 + k (x_1 - x_2) = - 2 k x_2 + k x_1\n\\end{align}</math>\n\nSince we expect oscillatory motion of a normal mode (where ω is the same for both masses), we try:\n\n:<math>\\begin{align}\n  x_1(t) &= A_1 e^{i \\omega t} \\\\\n  x_2(t) &= A_2 e^{i \\omega t}\n\\end{align}</math>\n\nSubstituting these into the equations of motion gives us:\n\n:<math>\\begin{align}\n  -\\omega^2 m A_1 e^{i \\omega t} &= - 2 k A_1 e^{i \\omega t} + k A_2 e^{i \\omega t} \\\\\n  -\\omega^2 m A_2 e^{i \\omega t} &= k A_1 e^{i \\omega t} - 2 k A_2 e^{i \\omega t}\n\\end{align}</math>\n\nSince the exponential factor is common to all terms, we omit it and simplify:\n\n:<math>\\begin{align}\n  (\\omega^2 m - 2 k) A_1 + k A_2 &= 0 \\\\\n  k A_1 + (\\omega^2 m - 2 k) A_2 &= 0\n\\end{align}</math>\n\nAnd in [[matrix (mathematics)|matrix]] representation:\n\n:<math>\\begin{bmatrix}\n  \\omega^2 m - 2 k & k \\\\\n                 k & \\omega^2 m - 2 k\n  \\end{bmatrix} \\begin{pmatrix}\n    A_1 \\\\\n    A_2\n  \\end{pmatrix} = 0\n</math>\n\nIf the matrix on the left is invertible, the unique solution is the trivial solution (''A''<sub>1</sub>,&nbsp;''A''<sub>2</sub>) = (''x''<sub>1</sub>,&nbsp;''x''<sub>2</sub>) = (0,0). The non trivial solutions are to be found for those values of ω whereby the matrix on the left is [[singular matrix|singular]] i.e. is not invertible. It follows that the [[determinant]] of the matrix must be equal to 0, so:\n\n:<math> (\\omega^2 m - 2 k)^2 - k^2 = 0 </math>\n\nSolving for <math>\\omega</math>, we have two positive solutions:\n\n:<math>\\begin{align}\n  \\omega_1 &= \\sqrt{\\frac{k}{m}} \\\\\n  \\omega_2 &= \\sqrt{\\frac{3 k}{m}}\n\\end{align}</math>\n\nIf we substitute ω<sub>1</sub> into the matrix and solve for (''A''<sub>1</sub>,&nbsp;''A''<sub>2</sub>), we get (1,&nbsp;1).  If we substitute ω<sub>2</sub>, we get (1,&nbsp;−1).  (These vectors are [[eigenvector]]s, and the frequencies are [[eigenvalue]]s.)\n\nThe first normal mode is:\n:<math>\\vec \\eta_1 = \\begin{pmatrix}\n    x^1_1(t) \\\\\n    x^1_2(t)\n  \\end{pmatrix} = c_1 \\begin{pmatrix}\n    1 \\\\\n    1\n  \\end{pmatrix} \\cos{(\\omega_1 t + \\varphi_1)}\n</math>\n\nWhich corresponds to both masses moving in the same direction at the same time. This mode is called antisymmetric. \n\nThe second normal mode is:\n\n:<math>\\vec \\eta_2 = \\begin{pmatrix}\n    x^2_1(t) \\\\\n    x^2_2(t)\n  \\end{pmatrix} = c_2 \\begin{pmatrix}\n     1 \\\\\n    -1\n  \\end{pmatrix} \\cos{(\\omega_2 t + \\varphi_2)}\n</math>\n\nThis corresponds to the masses moving in the opposite directions, while the center of mass remains stationary. This mode is called symmetric.\n\nThe general solution is a [[Superposition principle|superposition]] of the '''normal modes''' where ''c''<sub>1</sub>, ''c''<sub>2</sub>, φ<sub>1</sub>, and φ<sub>2</sub>, are determined by the [[initial condition]]s of the problem.\n\nThe process demonstrated here can be generalized and formulated using the formalism of [[Lagrangian mechanics]] or [[Hamiltonian mechanics]].\n\n=== Standing waves ===\nA [[standing wave]] is a continuous form of normal mode. In a standing wave, all the space elements (i.e. (''x'',&nbsp;''y'',&nbsp;''z'') coordinates) are oscillating in the same [[frequency]] and in [[phase (waves)|phase]] (reaching the [[mechanical equilibrium|equilibrium]] point together), but each has a different amplitude.\n\n[[File:Standing-wave05.png]]\n\nThe general form of a standing wave is:\n\n:<math>\n\\Psi(t) = f(x,y,z) (A\\cos(\\omega t) + B\\sin(\\omega t))\n</math>\n\nwhere ''ƒ''(''x'',&nbsp;''y'',&nbsp;''z'') represents the dependence of amplitude on location and the cosine\\sine are the oscillations in time.\n\nPhysically, standing waves are formed by the [[Interference (wave propagation)|interference]] (superposition) of waves and their reflections (although one may also say the opposite; that a moving wave is a [[superposition principle|superposition]] of standing waves). The geometric shape of the medium determines what would be the interference pattern, thus determines the ''ƒ''(''x'', ''y'',&nbsp;''z'') form of the standing wave. This space-dependence is called a '''normal mode'''.\n\nUsually, for problems with continuous dependence on (''x'',&nbsp;''y'',&nbsp;''z'') there is no single or finite number of normal modes, but there are infinitely many normal modes. If the problem is bounded (i.e. it is defined on a finite section of space) there are [[countably many]] normal modes (usually numbered ''n'' = 1,&nbsp;2,&nbsp;3,&nbsp;...). If the problem is not bounded, there is a continuous spectrum of normal modes.\n\n=== Elastic solids ===\n{{Main|Einstein solid|Debye model}}\n\nIn any solid at any temperature, the primary particles (e.g. atoms or molecules) are not stationary, but rather vibrate about mean positions. In insulators the capacity of the solid to store thermal energy is due almost entirely to these vibrations. Many physical properties of the solid (e.g. modulus of elasticity) can be predicted given knowledge of the frequencies with which the particles vibrate. The simplest assumption (by Einstein) is that all the particles oscillate about their mean positions with the same natural frequency ''ν''. This is equivalent to the assumption that all atoms vibrate independently with a frequency ''ν''. Einstein also assumed that the allowed energy states of these oscillations are harmonics, or integral multiples of ''hν''. The spectrum of waveforms can be described mathematically using a Fourier series of sinusoidal density fluctuations (or thermal [[phonons]]).\n\n[[File:Harmonic partials on strings.svg|thumb|250px|The [[Fundamental frequency|fundamental]] and the first six [[overtone]]s of a vibrating string. The mathematics of [[wave propagation]] in crystalline solids consists of treating the [[harmonics]] as an ideal [[Fourier series]] of [[Sine wave|sinusoidal]] density fluctuations (or atomic displacement waves).]]\n\nDebye subsequently recognized that each oscillator is intimately coupled to its neighboring oscillators at all times. Thus, by replacing Einstein's identical uncoupled oscillators with the same number of coupled oscillators, Debye correlated the elastic vibrations of a one-dimensional solid with the number of mathematically special modes of vibration of a stretched string (see figure). The pure tone of lowest pitch or frequency is referred to as the fundamental and the multiples of that frequency are called its harmonic overtones. He assigned to one of the oscillators the frequency of the fundamental vibration of the whole block of solid. He assigned to the remaining oscillators the frequencies of the harmonics of that fundamental, with the highest of all these frequencies being limited by the motion of the smallest primary unit.\n\nThe normal modes of vibration of a crystal are in general superpositions of many overtones, each with an appropriate amplitude and phase. Longer wavelength (low frequency) [[phonons]] are exactly those acoustical vibrations which are considered in the theory of sound. Both longitudinal and transverse waves can be propagated through a solid, while, in general, only longitudinal waves are supported by fluids.\n\nIn the [[longitudinal mode]], the displacement of particles from their positions of equilibrium coincides with the propagation direction of the wave. Mechanical longitudinal waves have been also referred to as ''compression waves''. For [[transverse mode]]s, individual particles move perpendicular to the propagation of the wave.\n\nAccording to quantum theory, the mean energy of a normal vibrational mode of a crystalline solid with characteristic frequency ''ν'' is:\n\n: <math>E(v) = \\frac{1}{2}hv + \\frac{hv}{e^{hv/kT} - 1}</math>\n\nThe term (1/2)''hν'' represents the \"zero-point energy\", or the energy which an oscillator will have at absolute zero. ''E''(''ν'') tends to the classic value ''kT'' at high temperatures\n\n: <math>E(v) = kT\\left[1 + \\frac{1}{12}\\left(\\frac{hv}{kT}\\right)^2 + O\\left(\\frac{hv}{kT}\\right)^4 + \\cdots\\right]</math>\n\nBy knowing the thermodynamic formula,\n\n: <math>\\left( \\frac{\\partial S}{\\partial E}\\right)_{N,V} = \\frac{1}{T}</math>\n\nthe entropy per normal mode is:\n\n: <math>\\begin{align}\n  S\\left(v\\right) &= \\int_0^T\\frac{d}{dT}E\\left(v\\right)\\frac{dT}{T} \\\\[10pt]\n                  &= \\frac{E\\left(v\\right)}{T} - k\\log\\left(1 - e^{-\\frac{hv}{kT}}\\right)\n\\end{align}</math>\n\nThe free energy is:\n\n: <math>F(v) = E - TS=kT\\log \\left(1-e^{-\\frac{hv}{kT}}\\right)</math>\n\nwhich, for ''kT''&nbsp;>> ''hν'', tends to:\n\n:<math>F(v) = kT\\log \\left(\\frac{hv}{kT}\\right)</math>\n\nIn order to calculate the internal energy and the specific heat, we must know the number of normal vibrational modes a frequency between the values ''ν'' and ''ν''&nbsp;+ ''dν''. Allow this number to be ''f''(''ν'')d''ν''. Since the total number of normal modes is 3''N'', the function ''f''(''ν'') is given by:\n\n: <math>\\int f(v)\\,dv = 3N</math>\n\nThe integration is performed over all frequencies of the crystal. Then the internal energy ''U'' will be given by:\n\n: <math>U = \\int f(v)E(v)\\,dv</math>\n\n== In quantum mechanics ==\nIn [[quantum mechanics]], a state <math>\\ | \\psi \\rang</math> of a system is described by a [[wavefunction]] <math>\\ \\psi (x, t) </math> which solves the [[Schrödinger equation]]. The square of the absolute value of  <math>\\  \\psi </math>, i.e.\n\n:<math>\n\\ P(x,t) = |\\psi (x,t)|^2\n</math>\n\nis the [[probability density function|probability density]] to measure the particle in [[distance|place]] ''x'' at [[time]]&nbsp;''t''.\n\nUsually, when involving some sort of [[potential]], the wavefunction is decomposed into a [[Quantum superposition|superposition]] of energy [[eigenstate]]s, each oscillating with frequency of <math> \\omega = E_n / \\hbar </math>. Thus, one may write\n\n:<math>\n|\\psi (t) \\rang = \\sum_n |n\\rang  \\left\\langle n | \\psi ( t=0) \\right\\rangle   e^{-iE_nt/\\hbar}\n</math>\n\nThe eigenstates have a physical meaning further than an [[orthonormal basis]]. When the energy of the system is [[measurement in quantum mechanics|measured]], the wavefunction collapses into one of its eigenstates and so the particle wavefunction is described by the pure eigenstate corresponding to the measured [[energy]].\n\n==In seismology==\nNormal modes are generated in the earth from long wavelength [[seismic waves]] from large earthquakes interfering to form standing waves.\n\nFor an elastic, isotropic, homogeneous sphere, spheroidal, toroidal and radial (or breathing) modes arise. Spheroidal modes only involve P and SV waves (like [[Rayleigh waves]]) and depend on overtone number n and angular order l but have degeneracy of azimuthal order m. Increasing l concentrates fundamental branch closer to surface and at large l this tends to Rayleigh waves. Toroidal modes only involve SH waves (like [[Love waves]]) and do not exist in fluid outer core. Radial modes are just a subset of spheroidal modes with l=0. The degeneracy doesn’t exist on Earth as it is broken by rotation, ellipticity and 3D heterogeneous velocity and density structure.\n\nWe either assume that each mode can be isolated, the self-coupling approximation, or that many modes close in frequency [[resonant]], the cross-coupling approximation.  Self-coupling will change just the phase velocity and not the number of waves around a great circle resulting in a stretching or shrinking of standing wave pattern. Cross-coupling can be caused by rotation of Earth leading to mixing of fundamental spheroidal and toroidal modes, or by aspherical mantle structure or Earth’s ellipticity.\n\n== See also ==\n* [[Antiresonance]]\n* [[Critical speed]]\n* [[Harmonic oscillator]]\n* [[Harmonic series (music)]]\n* [[Infrared spectroscopy]]\n* [[Leaky mode]]\n* [[Mechanical resonance]]\n* [[Modal analysis]]\n* [[Mode (electromagnetism)]]\n* [[Quasinormal mode]]\n* [[Sturm–Liouville theory]]\n* [[Torsional vibration]]\n* [[Vibrations of a circular membrane]]\n<!--\n== References ==\n{{Reflist}}-->\n\n== Sources ==\n* {{cite book|last1=Blevins|first1=Robert D.|title=Formulas for natural frequency and mode shape|date=2001|publisher=Krieger Pub.|location=Malabar, Florida|isbn=978-1575241845|edition=Reprint}}\n* {{cite book|editor-last1=Tzou|editor-first1=H.S.|editor-last2=Bergman|editor-first2=L.A.|title=Dynamics and Control of Distributed Systems.|date=2008|publisher=[[Cambridge University Press]]|location=Cambridge [England]|isbn=978-0521033749}}\n* {{cite book|last1=Shearer|first1=Peter M.|title=Introduction to seismology|date=2009|publisher=Cambridge University Press|location=Cambridge|isbn=9780521882101|pages=231–237|edition=2nd}}\n\n== External links ==\n* [http://www.people.fas.harvard.edu/~djmorin/waves/normalmodes.pdf Harvard lecture notes on normal modes]\n\n{{DEFAULTSORT:Normal Mode}}\n[[Category:Ordinary differential equations]]\n[[Category:Classical mechanics]]\n[[Category:Quantum mechanics]]\n[[Category:Spectroscopy]]\n[[Category:Singular value decomposition]]\n[[Category:Articles containing video clips]]"
    },
    {
      "title": "Orthogonal Procrustes problem",
      "url": "https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem",
      "text": "The '''orthogonal Procrustes problem''' <ref>{{Citation | last1=Gower | first1=J.C | last2=Dijksterhuis | first2=G.B. | year=2004 | title=Procrustes Problems | publisher = Oxford University Press}}</ref> is a [[matrix approximation]] problem in [[linear algebra]].  In its classical form, one is given two [[Matrix (mathematics)|matrices]] <math>A</math> and <math>B</math> and asked to find an [[orthogonal matrix]] <math>R</math> which most closely maps <math>A</math> to <math>B</math>. <ref>{{Citation | last1=Hurley | first1=J.R. | last2=Cattell | first2=R.B. | year=1962 | title=Producing direct rotation to test a hypothesized factor structure | journal = Behavioral Science | volume = 7 | pages=258&ndash;262 | doi=10.1002/bs.3830070216 | issue=2}}</ref> Specifically,\n\n:<math>R = \\arg\\min_\\Omega\\|\\Omega A-B\\|_F \\quad\\mathrm{subject\\ to}\\quad \\Omega^T\n\\Omega=I,</math>\n\nwhere <math>\\|\\cdot\\|_F</math> denotes the [[Frobenius norm]].  This is a special case of [[Wahba's problem]] (with identical weights; instead of considering two matrices, in Wahba's problem the columns of the matrices are considered as individual vectors).\n\nThe name [[Procrustes]] refers to a bandit from Greek mythology who made his victims fit his bed by either stretching their limbs or cutting them off.\n\n== Solution ==\nThis problem was originally solved by [[Peter Schönemann]] in a 1964 thesis, and shortly after appeared in the journal Psychometrika. <ref>{{Citation | last=Schönemann | first=P.H. | authorlink = Peter Schönemann | year=1966 | title=A generalized solution of the orthogonal Procrustes problem | journal=Psychometrika | volume=31 | pages=1–10 | url=http://web.stanford.edu/class/cs273/refs/procrustes.pdf | doi=10.1007/BF02289451 | postscript=.}}</ref> A proof appeared in 1998. <ref>{{Citation | last = Zhang | first = Z. |title = A Flexible New Technique for Camera Calibration | series = Microsoft Research Technical Report | volume=71 | url=http://research.microsoft.com/en-us/um/people/zhang/Papers/TR98-71.pdf  | year = 1998}}</ref>\n\nThis problem is equivalent to finding the nearest orthogonal matrix to a given matrix <math>M=BA^{T}</math>.  To find this orthogonal matrix <math>R</math>, one uses the [[singular value decomposition]]\n:<math>M=U\\Sigma V^T\\,\\!</math>\nto write\n:<math>R=UV^T.\\,\\!</math>\n\n== Proof ==\n\nOne proof depends on basic properties of the [[Frobenius inner product|matrix inner product]] that induces the [[Frobenius norm]]:\n:<math>\n\\begin{align}\nR &= \\arg\\min_\\Omega ||\\Omega A-B\\|_F^2 \\\\\n&= \\arg\\min_\\Omega  \\langle \\Omega A-B, \\Omega A-B \\rangle  \\\\\n&= \\arg\\min_\\Omega  \\|A\\|_F^2 + \\|B\\|_F^2 - 2 \\langle \\Omega A , B \\rangle  \\\\\n&= \\arg\\max_\\Omega  \\langle \\Omega , B A^T \\rangle  \\\\\n&= \\arg\\max_\\Omega  \\langle \\Omega, U\\Sigma V^T \\rangle  \\\\\n&= \\arg\\max_\\Omega  \\langle U^T \\Omega V , \\Sigma \\rangle  \\\\\n&= \\arg\\max_\\Omega  \\langle S , \\Sigma \\rangle  \\quad \\text{where } S = U^T \\Omega V \\\\\n\\end{align}\n</math>\n:This quantity <math>S</math> is an orthonormal matrix (as it is a product of orthonormal matrices) and thus the expression is maximised when <math>S</math> equals the identity matrix <math>I</math>. Thus\n:<math>\n\\begin{align}\nI &= U^T R V  \\\\\nR &= U V^T \\\\\n\\end{align}\n</math>\n\n== Generalized/constrained Procrustes problems ==\nThere are a number of related problems to the classical orthogonal Procrustes problem.  One might generalize it by seeking the closest matrix in which the columns are [[orthogonal]], but not necessarily [[orthonormal]]. <ref>{{Citation| last=Everson| first=R| year=1997| title=Orthogonal, but not Orthonormal, Procrustes Problems| url=http://empslocal.ex.ac.uk/people/staff/reverson/uploads/Site/procrustes.pdf}}</ref>  \n\nAlternately, one might constrain it by only allowing [[rotation matrix|rotation matrices]] (i.e. orthogonal matrices with [[determinant]] 1, also known as [[Orthogonal matrix|special orthogonal matrices]]).  In this case, one can write (using the above decomposition <math>M=U\\Sigma V^T</math>)\n\n:<math>R=U\\Sigma'V^T,\\,\\!</math>\n\nwhere <math>\\Sigma'\\,\\!</math> is a modified <math>\\Sigma\\,\\!</math>, with the smallest singular value replaced by <math>\\operatorname{sign}(\\det(UV^T))</math> (+1 or -1), and the other singular values replaced by 1, so that the determinant of R is guaranteed to be positive. <ref>{{Citation|last1=Eggert|first1=DW| last2=Lorusso|first2=A| last3=Fisher|first3=RB| title=Estimating 3-D rigid body transformations: a comparison of four major algorithms| journal=Machine Vision and Applications| volume=9| issue=5| pages=272&ndash;290| year=1997|doi=10.1007/s001380050048}}</ref> For more information, see the [[Kabsch algorithm]]. \n\n== See also ==\n* [[Procrustes analysis]]\n* [[Procrustes transformation]]\n* [[Wahba's problem]]\n\n== References ==\n<references/>\n\n[[Category:Linear algebra]]\n[[Category:Matrix theory]]\n[[Category:Singular value decomposition]]"
    },
    {
      "title": "Schmidt decomposition",
      "url": "https://en.wikipedia.org/wiki/Schmidt_decomposition",
      "text": "{{Use American English|date = February 2019}}\n{{Short description|Process in linear algebra}}\n{{Use mdy dates|date = February 2019}}\n\nIn [[linear algebra]], the '''Schmidt decomposition''' (named after its originator [[Erhard Schmidt]]) refers to a particular way of expressing a [[Coordinate vector|vector]] in the [[tensor product]] of two [[inner product space]]s. It has numerous applications in [[quantum information theory]], for example in [[quantum entanglement|entanglement]] characterization and in [[purification of quantum state|state purification]], and [[Plasticity (physics)|plasticity]].\n\n==Theorem==\nLet <math>H_1</math> and <math>H_2</math> be [[Hilbert spaces]] of [[dimension]]s ''n'' and ''m'' respectively. Assume <math>n \\geq m</math>. For any vector <math>w</math> in the tensor product <math>H_1 \\otimes H_2</math>, there exist orthonormal sets <math>\\{ u_1, \\ldots, u_m \\} \\subset H_1</math> and <math>\\{ v_1, \\ldots, v_m \\} \\subset H_2</math> such that <math>w= \\sum_{i =1} ^m \\alpha _i u_i \\otimes v_i</math>, where the scalars <math>\\alpha_i</math> are real, non-negative, and, as a [[Multiset|(multi-)set]], uniquely determined by <math>w</math>.\n\n===Proof===\nThe Schmidt decomposition is essentially a restatement of the [[singular value decomposition]] in a different context. Fix orthonormal bases <math>\\{ e_1, \\ldots, e_n \\} \\subset H_1</math> and <math>\\{ f_1, \\ldots, f_m \\} \\subset H_2</math>. We can identify an elementary tensor <math>e_i \\otimes f_j</math> with the matrix <math>e_i f_j ^T</math>, where <math>f_j ^T</math> is the [[transpose]] of <math>f_j</math>. A general element of the tensor product\n\n:<math>w = \\sum _{1 \\leq i \\leq n, 1 \\leq j \\leq m} \\beta _{ij} e_i \\otimes f_j</math>\n\ncan then be viewed as the ''n'' × ''m'' matrix\n\n:<math>\\; M_w = (\\beta_{ij}) .</math>\n\nBy the [[singular value decomposition]], there exist an ''n'' × ''n'' unitary ''U'', ''m'' × ''m'' unitary ''V'', and a [[Positive-semidefinite matrix|positive semidefinite]] diagonal ''m'' × ''m'' matrix Σ such that\n\n:<math>M_w = U \\begin{bmatrix} \\Sigma \\\\ 0 \\end{bmatrix} V^\\star .</math>\n\nWrite <math>U =\\begin{bmatrix} U_1 & U_2 \\end{bmatrix}</math> where <math>U_1</math> is ''n'' × ''m'' and we have\n\n:<math>\\; M_w = U_1 \\Sigma V^\\star .</math>\n\nLet <math>\\{ u_1, \\ldots, u_m \\}</math> be the first ''m'' column vectors of <math>U_1</math>, <math>\\{ v_1, \\ldots, v_m \\}</math> the column vectors of ''V'', and <math>\\alpha_1, \\ldots, \\alpha_m</math> the diagonal elements of Σ.  The previous expression is then\n\n:<math>M_w = \\sum _{k=1} ^m \\alpha_k u_k v_k ^\\star ,</math>\n\nThen\n\n:<math>w = \\sum _{k=1} ^m \\alpha_k u_k \\otimes v_k ,</math>\n\nwhich proves the claim.\n\n==Some observations==\nSome properties of the Schmidt decomposition are of physical interest.\n\n===Spectrum of reduced states===\nConsider a vector ''w''  of the tensor product\n:<math>H_1 \\otimes H_2</math>\n\nin the form of Schmidt decomposition\n\n:<math>w = \\sum_{i =1} ^m \\alpha _i u_i \\otimes v_i.</math>\n\nForm the rank 1 matrix ''ρ'' = ''w w*''. Then the [[partial trace]] of ''ρ'', with respect to either system ''A'' or ''B'', is a diagonal matrix whose non-zero diagonal elements are |''α<sub>i</sub>'' |<sup>2</sup>. In other words, the Schmidt decomposition shows that the reduced state of ''ρ'' on either subsystem have the same spectrum.\n\n===Schmidt rank and entanglement===\nThe strictly positive values ''<math>\\alpha_i</math>'' in the Schmidt decomposition of ''w'' are its '''Schmidt coefficients'''.  The number of Schmidt coefficients of <math>w</math>, counted with multiplicity, is called its '''Schmidt rank''', or '''Schmidt number'''.\n\nIf ''w'' can be expressed as a product\n:<math>u \\otimes v</math>\nthen ''w'' is called a [[separable state]]. Otherwise, ''w'' is said to be an [[quantum entanglement|entangled state]]. From the Schmidt decomposition, we can see that ''w'' is entangled if and only if ''w'' has Schmidt rank strictly greater than 1.  Therefore, two subsystems that partition a pure state are entangled if and only if their reduced states are mixed states.\n\n===Von Neumann entropy===\n\nA consequence of the above comments is that, for bipartite pure states, the [[von Neumann entropy]] of the reduced states is a well-defined measure of [[quantum entanglement|entanglement]].  For the von Neumann entropy of both reduced states of ''ρ'' is <math>-\\sum_i |\\alpha_i|^2 \\log(|\\alpha_i|^2)</math>, and this is zero if and only if ''ρ'' is a product state (not entangled).\n\n==Crystal plasticity==\nIn the field of plasticity, crystalline solids such as metals deform plastically primarily along crystal planes. Each plane, defined by its normal vector ν can \"slip\" in one of several directions, defined by a vector μ. Together a slip plane and direction form a slip system which is described by the Schmidt tensor <math>P=\\mu\\otimes \\nu</math>. The velocity gradient is a linear combination of these across all slip systems where the scaling factor is the rate of slip along the system.\n\n==See also==\n* [[Singular value decomposition]]\n* [[Purification of quantum state]]\n\n==Further reading==\n* {{cite book |first=Anirban |last=Pathak |title=Elements of Quantum Computation and Quantum Communication |location=London |publisher=Taylor & Francis |year=2013 |isbn=978-1-4665-1791-2 |url={{Google books |plainurl=yes |id=cEPSBQAAQBAJ |page=92}} |pages=92–98 }} \n\n{{DEFAULTSORT:Schmidt Decomposition}}\n[[Category:Linear algebra]]\n[[Category:Singular value decomposition]]\n[[Category:Quantum information theory]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "Singular value",
      "url": "https://en.wikipedia.org/wiki/Singular_value",
      "text": "In [[mathematics]], in particular [[functional analysis]], the '''singular values''', or '''''s''-numbers''' of a [[compact operator]] {{nowrap|''T''&thinsp;: ''X'' → ''Y''}} acting between [[Hilbert space]]s ''X'' and ''Y'', are the square roots of non-negative [[eigenvalue]]s of the self-adjoint operator {{nowrap|''T{{sup|*}}T''&thinsp;}} (where ''T{{sup|*}}'' denotes the [[adjoint operator|adjoint]] of ''T'').\n\nThe singular values are non-negative [[real number]]s, usually listed in decreasing order (''s''<sub>1</sub>(''T''), ''s''<sub>2</sub>(''T''), …).  The largest singular value ''s''<sub>1</sub>(''T'') is equal to the [[operator norm]] of ''T'' (see [[Min-max theorem#Min-max principle for singular values|Min-max theorem]]).\n\n[[File:Singular value decomposition.gif|thumb|right|280px|Visualisation of a [[singular value decomposition]] (SVD) of a 2-dimensional, real [[:en:Shear mapping|shearing matrix]] ''M''. First, we see the [[unit disc]] in blue together with the two [[standard basis|canonical unit vectors]]. We then see the action of ''M'', which distorts the disc to an [[ellipse]]. The SVD decomposes ''M'' into three simple transformations: a [[rotation matrix|rotation]] ''V''{{sup|*}}, a [[scaling (geometry)|scaling]] ''&Sigma;'' along the rotated coordinate axes and a second rotation ''U''. ''&Sigma;'' is a [[diagonal matrix]] containing in its diagonal the singular values of ''M'', which represent the lengths &sigma;<sub>1</sub> and &sigma;<sub>2</sub> of the [[ellipse#Elements of an ellipse|semi-axes]] of the ellipse.]]\n\nIn the case that ''T'' acts on euclidean space '''R'''<sup>''n''</sup>, there is a simple geometric interpretation for the singular values: Consider the image by ''T'' of the [[N-sphere|unit sphere]]; this is an [[ellipsoid]], and the lengths of its semi-axes are the singular values of ''T'' (the figure provides an example in '''R'''<sup>''2''</sup>).\n\nThe singular values are the absolute values of the [[eigenvalues]] of a [[normal matrix]] ''A'', because the [[spectral theorem]] can be applied to obtain unitary diagonalization of ''A'' as {{nowrap|''A'' {{=}} ''U''Λ''U''{{sup|*}}}}. Therefore, <math>\\sqrt{A^*A} = \\sqrt{U \\Lambda^2 U^*} = U\\ |\\Lambda|\\ U^*</math>.\n\nMost [[normed linear space|norms]] on Hilbert space operators studied are defined using ''s''-numbers.  For example, the [[Ky Fan]]-''k''-norm is the sum of first ''k'' singular values, the trace norm is the sum of all singular values, and the [[Schatten norm]] is the ''p''th root of the sum of the ''p''th powers of the singular values.  Note that each norm is defined only on a special class of operators, hence ''s''-numbers are useful in classifying different operators.\n\nIn the finite-dimensional case, a [[matrix (mathematics)|matrix]] can always be decomposed in the form  ''U''&Sigma;''V''{{sup|*}}, where ''U'' and ''V''{{sup|*}} are [[unitary matrix|unitary matrices]] and ''&Sigma;'' is a [[diagonal matrix]] with the singular values lying on the diagonal.  This is the [[singular value decomposition]].\n\n== Basic properties ==\n\nFor <math>A \\in \\mathbb{C}^{m \\times n}, </math> and <math>i = 1,2, \\ldots, \\min \\{m,n\\}</math>.\n\n[[Min-max theorem#Min-max principle for singular values|Min-max theorem for singular values]]. Here <math>U: \\dim(U) = i</math> is a subspace of <math>\\mathbb{C}^n</math> of dimension <math>i</math>.\n\n:<math>\\begin{align}\n  \\sigma_i(A) &= \\min_{\\dim(U)=n-i+1} \\max_{\\underset{\\| x \\|_2 = 1}{x \\in U}} \\left\\| Ax \\right\\|_2. \\\\\n  \\sigma_i(A) &= \\max_{\\dim(U)=i} \\min_{\\underset{\\| x \\|_2 = 1}{x \\in U}} \\left\\| Ax \\right\\|_2.\n\\end{align}</math>\n\nMatrix transpose and conjugate do not alter singular values.\n\n:<math>\\sigma_i(A) = \\sigma_i\\left(A^\\textsf{T}\\right) = \\sigma_i\\left(A^*\\right) = \\sigma_i\\left(\\bar{A}\\right).</math>\n\nFor any unitary <math>U \\in \\mathbb{C}^{m \\times m}, V \\in \\mathbb{C}^{n \\times n}.</math>\n\n:<math>\\sigma_i(A) = \\sigma_i(UAV).</math>\n\nRelation to eigenvalues:\n\n:<math>\\sigma_i^2(A) = \\lambda_i\\left(AA^*\\right) = \\lambda_i\\left(A^*A\\right).</math>\n\n== Inequalities about singular values ==\nSee also <ref>R.A. Horn and C.R. Johnson. Topics in Matrix Analysis. Cambridge University Press, Cambridge, 1991. Chap. 3</ref>.\n\n===Singular values of sub-matrices===\n\nFor <math>A \\in \\mathbb{C}^{m \\times n}.</math>\n{{ordered list\n| list-style-type=lower-alpha\n| Let <math>B</math> denote <math>A</math> with one of its rows ''or'' columns deleted. Then\n: <math>\\sigma_{i+1}(A) \\leq \\sigma_i (B) \\leq \\sigma_i(A)</math>\n| Let <math>B</math> denote <math>A</math> with one of its rows ''and'' columns deleted. Then\n: <math>\\sigma_{i+2}(A) \\leq \\sigma_i (B) \\leq \\sigma_i(A)</math>\n| Let <math>B</math> denote an <math>(m-k)\\times(n-l)</math> submatrix of <math>A</math>. Then\n: <math>\\sigma_{i+k+l}(A) \\leq \\sigma_i (B) \\leq \\sigma_i(A)</math>\n}}\n\n===Singular values of <math>A + B</math>===\n\nFor <math>A, B \\in \\mathbb{C}^{m \\times n}</math>\n{{ordered list\n| list-style-type=lower-alpha\n| <math>\\sum_{i=1}^{k} \\sigma_i(A + B) \\leq \\sum_{i=1}^{k} \\sigma_i(A) + \\sigma_i(B), \\quad k=\\min \\{m,n\\}</math>\n| <math>\\sigma_{i+j-1}(A + B) \\leq \\sigma_i(A) + \\sigma_j(B). \\quad i,j\\in\\mathbb{N},\\ i + j - 1 \\leq \\min \\{m,n\\}</math>\n}}\n\n===Singular values of <math>AB</math>===\n\nFor <math>A, B \\in \\mathbb{C}^{n \\times n}</math>\n\n{{ordered list\n| list-style-type=lower-alpha\n| <math>\\begin{align}\n  \\prod_{i=n}^{i=n-k+1} \\sigma_i(A) \\sigma_i(B) &\\leq \\prod_{i=n}^{i=n-k+1} \\sigma_i(AB) \\\\\n                     \\prod_{i=1}^k \\sigma_i(AB) &\\leq \\prod_{i=1}^k \\sigma_i(A) \\sigma_i(B), \\\\\n                    \\sum_{i=1}^k \\sigma_i^p(AB) &\\leq \\sum_{i=1}^k \\sigma_i^p(A) \\sigma_i^p(B),\n\\end{align}</math>\n| <math>\\sigma_n(A) \\sigma_i(B) \\leq \\sigma_i (AB) \\leq \\sigma_1(A) \\sigma_i(B) \\quad i = 1, 2, \\ldots, n. </math>\n}}\n\nFor <math>A, B \\in \\mathbb{C}^{m \\times n}</math>\n<ref>X. Zhan. Matrix Inequalities. Springer-Verlag, Berlin, Heidelberg, 2002. p.28</ref>\n:<math>2 \\sigma_i(A B^*) \\leq \\sigma_i \\left(A^* A + B^* B\\right), \\quad i = 1, 2, \\ldots, n. </math>\n\n===Singular values and eigenvalues===\n\nFor <math>A \\in \\mathbb{C}^{n \\times n}</math>.\n{{ordered list\n| list-style-type=lower-alpha\n| See<ref>R. Bhatia. Matrix Analysis. Springer-Verlag, New York, 1997. Prop. III.5.1</ref>\n:<math>\\lambda_i\\left(A + A^*\\right) \\leq 2 \\sigma_i(A), \\quad i = 1, 2, \\ldots, n.</math>\n| Assume <math>\\left|\\lambda_1(A)\\right| \\geq \\cdots \\geq \\left|\\lambda_n(A)\\right|</math>. Then for <math>k = 1, 2, \\ldots, n</math>:\n{{ordered list\n | [[Weyl's inequality#Weyl's inequality in matrix theory|Weyl's theorem]]\n:<math> \\prod_{i=1}^k \\left|\\lambda_i(A)\\right| \\leq \\prod_{i=1}^{k} \\sigma_i(A).</math>\n | For <math>p>0</math>.\n:<math> \\sum_{i=1}^k \\left|\\lambda_i^p(A)\\right| \\leq \\sum_{i=1}^{k} \\sigma_i^p(A).</math>\n }}\n}}\n\n== History ==\nThis concept was introduced by [[Erhard Schmidt]] in 1907.  Schmidt called singular values \"eigenvalues\" at that time.  The name \"singular value\" was first quoted by Smithies in 1937.  In 1957, Allahverdiev proved the following characterization of the ''n''th ''s''-number {{ref|1}}: \n: <math>s_n(T) = \\inf\\big\\{\\, \\|T-L\\| : L\\text{ is an operator of finite rank }<n \\,\\big\\}.</math>\n\nThis formulation made it possible to extend the notion of ''s''-numbers to operators in [[Banach space]].\n\n== See also ==\n*[[Condition number]]\n*[[Min-max theorem#Cauchy interlacing theorem|Cauchy interlacing theorem]] or [[Poincaré separation theorem]]\n*[[Schur–Horn theorem]]\n*[[Singular value decomposition]]\n\n==References==\n{{Reflist}}\n# {{note|1}}[[Israel Gohberg|I. C. Gohberg]] and [[Mark Krein|M. G. Krein]]. Introduction to the Theory of Linear Non-selfadjoint Operators. American Mathematical Society, Providence, R.I.,1969. Translated from the Russian by A. Feinstein. Translations of Mathematical Monographs, Vol. 18.\n\n[[Category:Operator theory]]\n[[Category:Singular value decomposition]]"
    },
    {
      "title": "Two-dimensional singular-value decomposition",
      "url": "https://en.wikipedia.org/wiki/Two-dimensional_singular-value_decomposition",
      "text": "{{Use dmy dates|date=August 2012}}\n'''Two-dimensional singular-value decomposition''' ('''2DSVD''') computes the [[low rank approximation|low-rank approximation]] of a set of matrices such as [[two dimensional|2D]] images or weather maps in a manner almost identical to SVD ([[singular-value decomposition]]) which computes the low-rank approximation of a single matrix (or a set of [[one dimensional|1D]] vectors).\n\n==SVD==\n\nLet matrix <math> X=(x_1,\\ldots,x_n) </math> contains the set of 1D vectors which have been centered. In PCA/SVD, we construct \ncovariance matrix <math> F </math>\nand Gram matrix <math> G </math>\n:  <math> F=X X^T </math> , <math> G=X^T X, </math> \nand compute their eigenvectors <math> U = (u_1, \\ldots, u_n) </math> and \n<math> V=(v_1,\\ldots, v_n) </math>. \nSince <math> VV^T=I, UU^T=I </math>, we have\n: <math> X = UU^T X VV^T = U (U^T XV) V^T = U \\Sigma V^T. </math>\nIf we retain only  <math> K </math> principal eigenvectors in <math> U , V</math>, \nthis gives low-rank approximation of <math> X </math>.\n\n==2DSVD==\n\nHere we deal with a set of 2D matrices <math> (X_1,\\ldots,X_n) </math>.\nSuppose they are centered  <math> \\sum_i X_i =0 </math>.\nWe construct row–row and column–column covariance matrices\n\n: <math> F=\\sum_i X_i X_i^T </math> ,  <math> G=\\sum_i X_i^T X_i </math> \n\nin exactly the same manner as in SVD, and compute their eigenvectors <math> U </math> and <math> V</math>.\nWe approximate <math> X_i </math>  as\n\n: <math> X_i = U U^T X_i  V V^T = U (U^T X_i V) V^T = U M_i V^T </math>\n\nin identical fashion as in SVD.\nThis gives a near optimal low-rank approximation of <math> (X_1,\\ldots,X_n) </math>\nwith the objective function\n\n: <math> J=  \\sum_i | X_i - L M_i R^T| ^2 </math>\n\nError bounds similar to [[Eckard–Young theorem]] also exist.\n\n2DSVD is mostly used in [[image compression]] and representation.\n\n==References==\n* Chris Ding and Jieping Ye. \"Two-dimensional Singular Value Decomposition (2DSVD) for 2D Maps and Images\". Proc. SIAM Int'l Conf. Data Mining (SDM'05), pp. 32–43, April 2005. http://ranger.uta.edu/~chqding/papers/2dsvdSDM05.pdf\n* Jieping Ye. \"Generalized Low Rank Approximations of Matrices\". Machine Learning Journal. Vol. 61, pp. 167—191, 2005.\n\n[[Category:Singular value decomposition]]"
    },
    {
      "title": "Matrix multiplication algorithm",
      "url": "https://en.wikipedia.org/wiki/Matrix_multiplication_algorithm",
      "text": "{{short description|Algorithm to multiply matrices}}\n{{unsolved|computer science|What is the fastest algorithm for matrix multiplication?}}\nBecause [[matrix multiplication]] is such a central operation in many [[numerical algorithm]]s, much work has been invested in making '''matrix multiplication algorithms''' efficient. Applications of matrix multiplication in computational problems are found in many fields including [[scientific computing]] and [[pattern recognition]] and in seemingly unrelated problems such as counting the paths through a [[Graph (graph theory)|graph]].<ref name=\"skiena\"/> Many different algorithms have been designed for multiplying matrices on different types of hardware, including [[parallel computing|parallel]] and [[distributed computing|distributed]] systems, where the computational work is spread over multiple processors (perhaps over a network).\n\nDirectly applying the mathematical definition of matrix multiplication gives an algorithm that [[Analysis of algorithms|takes time]] on the order of {{math|''n''<sup>3</sup>}} to multiply two {{math|''n'' × ''n''}} matrices ({{math|Θ(''n''<sup>3</sup>)}} in [[big O notation]]). Better asymptotic bounds on the time required to multiply matrices have been known since the work of Strassen in the 1960s, but it is still unknown what the optimal time is (i.e., what the [[Computational complexity theory|complexity of the problem]] is).\n\n==Iterative algorithm==\nThe [[Matrix multiplication#General definition of the matrix product|definition of matrix multiplication]] is that if {{math|''C'' {{=}} ''AB''}} for an {{math|''n'' × ''m''}} matrix {{mvar|A}} and an {{math|''m'' × ''p''}} matrix {{mvar|B}}, then {{mvar|C}} is an {{math|''n'' × ''p''}} matrix with entries\n\n:<math>c_{ij} = \\sum_{k=1}^m a_{ik} b_{kj}</math>.\n\nFrom this, a simple algorithm can be constructed which loops over the indices {{mvar|i}} from 1 through {{mvar|n}} and {{mvar|j}} from 1 through {{mvar|p}}, computing the above using a nested loop:\n\n<div style=\"margin-left: 35px; width: 600px\">\n{{framebox|blue}}\n* Input: matrices {{mvar|A}} and {{mvar|B}}\n* Let {{mvar|C}} be a new matrix of the appropriate size\n* For {{mvar|i}} from 1 to {{mvar|n}}:\n** For {{mvar|j}} from 1 to {{mvar|p}}:\n*** Let {{math|sum {{=}} 0}}\n*** For {{mvar|k}} from 1 to {{mvar|m}}:\n**** Set {{math|sum ← sum + ''A<sub>ik</sub>'' × ''B<sub>kj</sub>''}}\n*** Set {{math|''C<sub>ij</sub>'' ← sum}}\n* Return {{mvar|C}}\n{{frame-footer}}\n</div>\n\nThis algorithm takes [[time complexity|time]] {{math|Θ(''nmp'')}} (in [[asymptotic notation]]).<ref name=\"skiena\"/> A common simplification for the purpose of [[analysis of algorithms|algorithms analysis]] is to assume that the inputs are all square matrices of size {{math|''n'' × ''n''}}, in which case the running time is {{math|Θ(''n''<sup>3</sup>)}}, i.e., cubic.<ref name=\"clrs\">{{Introduction to Algorithms|3|pages=75–79}}</ref>\n\n===Cache behavior===\n[[File:Row_and_column_major_order.svg|thumb|upright|Illustration of row- and column-major order]]\nThe three loops in iterative matrix multiplication can be arbitrarily swapped with each other without an effect on correctness or asymptotic running time. However, the order can have a considerable impact on practical performance due to the [[locality of reference|memory access patterns]] and [[CPU cache|cache]] use of the algorithm;<ref name=\"skiena\">{{cite book |first=Steven |last=Skiena |authorlink=Steven Skiena |title=The Algorithm Design Manual |publisher=Springer |year=2008 |pages=45–46, 401–403 |doi=10.1007/978-1-84800-070-4_4|chapter=Sorting and Searching |isbn=978-1-84800-069-8 }}</ref>\nwhich order is best also depends on whether the matrices are stored in [[row-major order]], column-major order, or a mix of both.\n\nIn particular, in the idealized case of a [[CPU cache#Associativity|fully associative cache]] consisting of {{mvar|M}} cache lines of {{mvar|b}} bytes each, the above algorithm is sub-optimal for {{mvar|A}} and {{mvar|B}} stored in row-major order. When {{math|''n'' > {{sfrac|''M''|''b''}}}}, every iteration of the inner loop (a simultaneous sweep through a row of {{mvar|A}} and a column of {{mvar|B}}) incurs a cache miss when accessing an element of {{mvar|B}}. This means that the algorithm incurs {{math|Θ(''n''<sup>3</sup>)}} cache misses in the worst case. {{As of|2010}}, the speed of memories compared to that of processors is such that the cache misses, rather than the actual calculations, dominate the running time for sizable matrices.<ref name=\"ocw\">{{cite web |first1=Saman |last1=Amarasinghe |first2=Charles |last2=Leiserson |title=6.172 Performance Engineering of Software Systems, Lecture 8 |year=2010 |publisher=Massachusetts Institute of Technology |website=MIT OpenCourseWare |url=http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-172-performance-engineering-of-software-systems-fall-2010/video-lectures/lecture-8-cache-efficient-algorithms/ |accessdate=27 January 2015}}</ref>\n\nThe optimal variant of the iterative algorithm for {{mvar|A}} and {{mvar|B}} in row-major layout is a ''[[loop tiling|tiled]]'' version, where the matrix is implicitly divided into square tiles of size {{math|{{radic|''M''}}}} by {{math|{{radic|''M''}}}}:<ref name=\"ocw\"/><ref>{{cite conference |first1=Monica S. |last1=Lam |first2=Edward E. |last2=Rothberg |first3=Michael E. |last3=Wolf |title=The Cache Performance and Optimizations of Blocked Algorithms |conference=Int'l Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS) |year=1991}}</ref>\n\n<div style=\"margin-left: 35px; width: 600px\">\n{{framebox|blue}}\n* Input: matrices {{mvar|A}} and {{mvar|B}}\n* Let {{mvar|C}} be a new matrix of the appropriate size\n* Pick a tile size {{math|''T'' {{=}} Θ({{radic|''M''}})}}\n* For {{mvar|I}} from 1 to {{mvar|n}} in steps of {{mvar|T}}:\n** For {{mvar|J}} from 1 to {{mvar|p}} in steps of {{mvar|T}}:\n*** For {{mvar|K}} from 1 to {{mvar|m}} in steps of {{mvar|T}}:\n**** Multiply {{math|''A''<sub>''I'':''I''+''T'', ''K'':''K''+''T''</sub>}} and {{math|''B''<sub>''K'':''K''+''T'', ''J'':''J''+''T''</sub>}} into {{math|''C''<sub>''I'':''I''+''T'', ''J'':''J''+''T''</sub>}}, that is:\n**** For {{mvar|i}} from {{mvar|I}} to {{math|min(''I'' + ''T'', ''n'')}}:\n***** For {{mvar|j}} from {{mvar|J}} to {{math|min(''J'' + ''T'', ''p'')}}:\n****** Let {{math|sum {{=}} 0}}\n****** For {{mvar|k}} from {{mvar|K}} to {{math|min(''K'' + ''T'', ''m'')}}:\n******* Set {{math|sum ← sum + ''A<sub>ik</sub>'' × ''B<sub>kj</sub>''}}\n****** Set {{math|''C<sub>ij</sub>'' ← ''C<sub>ij</sub>'' + sum}}\n* Return {{mvar|C}}\n{{frame-footer}}\n</div>\n\nIn the idealized cache model, this algorithm incurs only {{math|Θ({{sfrac|''n''<sup>3</sup>|''b'' {{radic|''M''}}}})}} cache misses; the divisor {{math|''b'' {{radic|''M''}}}} amounts to several orders of magnitude on modern machines, so that the actual calculations dominate the running time, rather than the cache misses.<ref name=\"ocw\"/>\n\n==Divide and conquer algorithm==\nAn alternative to the iterative algorithm is the [[divide and conquer algorithm]] for matrix multiplication. This relies on the [[block matrix|block partitioning]]\n\n:<math>C = \\begin{pmatrix}\nC_{11} & C_{12} \\\\\nC_{21} & C_{22} \\\\\n\\end{pmatrix},\\,\nA = \\begin{pmatrix}\nA_{11} & A_{12} \\\\\nA_{21} & A_{22} \\\\\n\\end{pmatrix},\\,\nB = \\begin{pmatrix}\nB_{11} & B_{12} \\\\\nB_{21} & B_{22} \\\\\n\\end{pmatrix}</math>.\n\nwhich works for all square matrices whose dimensions are powers of two, i.e., the shapes are {{math|2<sup>''n''</sup> × 2<sup>''n''</sup>}} for some {{mvar|n}}. The matrix product is now\n\n:<math>\\begin{pmatrix}\nC_{11} & C_{12} \\\\\nC_{21} & C_{22} \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\nA_{11} & A_{12} \\\\\nA_{21} & A_{22} \\\\\n\\end{pmatrix} \\begin{pmatrix}\nB_{11} & B_{12} \\\\\nB_{21} & B_{22} \\\\\n\\end{pmatrix} = \\begin{pmatrix}\nA_{11} B_{11} + A_{12} B_{21} & A_{11} B_{12} + A_{12} B_{22}\\\\\nA_{21} B_{11} + A_{22} B_{21} & A_{21} B_{12} + A_{22} B_{22}\\\\\n\\end{pmatrix}\n</math>\n\nwhich consists of eight multiplications of pairs of submatrices, followed by an addition step. The divide and conquer algorithm computes the smaller multiplications [[recursion|recursively]], using the [[scalar multiplication]] {{math|''c''<sub>11</sub> {{=}} ''a''<sub>11</sub>''b''<sub>11</sub>}} as its base case.\n\nThe complexity of this algorithm as a function of {{mvar|n}} is given by the recurrence<ref name=\"clrs\"/>\n\n:<math>T(1) = \\Theta(1)</math>;\n:<math>T(n) = 8T(n/2) + \\Theta(n^2)</math>,\n\naccounting for the eight recursive calls on matrices of size {{math|''n''/2}} and {{math|Θ(''n''<sup>2</sup>)}} to sum the four pairs of resulting matrices element-wise. Application of the [[master theorem (analysis of algorithms)|master theorem for divide-and-conquer recurrences]] shows this recursion to have the solution {{math|Θ(''n''<sup>3</sup>)}}, the same as the iterative algorithm.<ref name=\"clrs\"/>\n\n===Non-square matrices===\nA variant of this algorithm that works for matrices of arbitrary shapes and is faster in practice<ref name=\"ocw\"/> splits matrices in two instead of four submatrices, as follows.<ref name=\"prokop\">{{cite thesis |type=Master's |first=Harald |last=Prokop |authorlink=Harald Prokop |title=Cache-Oblivious Algorithms |publisher=MIT |year=1999 |url=http://supertech.csail.mit.edu/papers/Prokop99.pdf}}</ref>\nSplitting a matrix now means dividing it into two parts of equal size, or as close to equal sizes as possible in the case of odd dimensions.\n\n<div style=\"margin-left: 35px; width: 600px\">\n{{framebox|blue}}\n* Inputs: matrices {{mvar|A}} of size {{math|''n'' × ''m''}}, {{mvar|B}} of size {{math|''m'' × ''p''}}.\n* Base case: if {{math|max(''n'', ''m'', ''p'')}} is below some threshold, use an [[loop unrolling|unrolled]] version of the iterative algorithm.\n* Recursive cases:\n\n:* If {{math|max(''n'', ''m'', ''p'') {{=}} ''n''}}, split {{mvar|A}} horizontally:\n\n::<math>C = \\begin{pmatrix} A_1 \\\\ A_2 \\end{pmatrix} {B}\n          = \\begin{pmatrix} A_1 B \\\\ A_2 B \\end{pmatrix}</math>\n\n:* Else, if {{math|max(''n'', ''m'', ''p'') {{=}} ''p''}}, split {{mvar|B}} vertically:\n\n::<math>C = A \\begin{pmatrix} B_1 & B_2 \\end{pmatrix}\n          = \\begin{pmatrix} A B_1 & A B_2 \\end{pmatrix}  </math>\n\n:* Otherwise, {{math|max(''n'', ''m'', ''p'') {{=}} ''m''}}. Split {{mvar|A}} vertically and {{mvar|B}} horizontally:\n\n::<math>C = \\begin{pmatrix} A_1 & A_2 \\end{pmatrix} \\begin{pmatrix} B_1 \\\\ B_2 \\end{pmatrix}\n          = A_1 B_1 + A_2 B_2</math>\n{{frame-footer}}\n</div>\n\n===Cache behavior===\nThe cache miss rate of recursive matrix multiplication is the same as that of a [[Loop tiling|tiled]] iterative version, but unlike that algorithm, the recursive algorithm is [[Cache-oblivious matrix multiplication|cache-oblivious]]:<ref name=\"prokop\"/> there is no tuning parameter required to get optimal cache performance, and it behaves well in a [[multiprogramming]] environment where cache sizes are effectively dynamic due to other processes taking up cache space.<ref name=\"ocw\"/>\n(The simple iterative algorithm is cache-oblivious as well, but much slower in practice if the matrix layout is not adapted to the algorithm.)\n\nThe number of cache misses incurred by this algorithm, on a machine with {{mvar|M}} lines of ideal cache, each of size {{mvar|b}} bytes, is bounded by{{r|prokop}}{{rp|13}}\n\n:<math>\\Theta \\left(m + n + p + \\frac{mn + np + mp}{b} + \\frac{mnp}{b\\sqrt{M}} \\right)</math>\n\n==Sub-cubic algorithms==\n[[File:Bound on matrix multiplication omega over time.svg|thumb|400px|right|The lowest {{mvar|ω}} such that matrix multiplication is known to be in {{math|''O''(''n<sup>ω</sup>'')}}, plotted against time.]]\n\nAlgorithms exist that provide better running times than the straightforward ones. The first to be discovered was [[Strassen algorithm|Strassen's algorithm]], devised by [[Volker Strassen]] in 1969 and often referred to as \"fast matrix multiplication\". It is based on a way of multiplying two {{math|2 × 2}}-matrices which requires only 7 multiplications (instead of the usual 8), at the expense of several additional addition and subtraction operations. Applying this recursively gives an algorithm with a multiplicative cost of <math>O( n^{\\log_{2}7}) \\approx O(n^{2.807})</math>. Strassen's algorithm is more complex, and the [[numerical stability]] is reduced compared to the naïve algorithm,<ref>{{Citation | last1=Miller | first1=Webb | title=Computational complexity and numerical stability | citeseerx = 10.1.1.148.9947 | year=1975 | journal=SIAM News | volume=4 | pages=97–107}}</ref>\nbut it is faster in cases where {{math|''n'' > 100}} or so<ref name=\"skiena\"/> and appears in several libraries, such as [[Basic Linear Algebra Subprograms|BLAS]].<ref>{{cite book |last1=Press |first1=William H. |last2=Flannery |first2=Brian P. |last3=Teukolsky |first3=Saul A. |author3-link=Saul Teukolsky |last4=Vetterling |first4=William T. |title=Numerical Recipes: The Art of Scientific Computing |publisher=[[Cambridge University Press]] |edition=3rd |isbn=978-0-521-88068-8 |year=2007 |page=108|title-link=Numerical Recipes }}</ref> It is very useful for large matrices over exact domains such as finite fields, where numerical stability is not an issue.\n\nThe current {{math|''O''(''n''<sup>''k''</sup>)}} algorithm with the lowest known exponent {{mvar|k}} is a generalization of the [[Coppersmith–Winograd algorithm]] that has an asymptotic complexity of {{math|''O''(''n''<sup>2.3728639</sup>)}}, by François Le Gall.<ref>{{Citation | last1=Le Gall | first1=François | contribution=Powers of tensors and fast matrix multiplication | year = 2014 | arxiv=1401.7714 | title = Proceedings of the 39th International Symposium on Symbolic and Algebraic Computation (ISSAC 2014)| bibcode=2014arXiv1401.7714L }}. The original algorithm was presented by [[Don Coppersmith]] and [[Shmuel Winograd]] in 1990, has an asymptotic complexity of {{math|''O''(''n''<sup>2.376</sup>)}}. It was improved in 2013 to {{math|''O''(''n''<sup>2.3729</sup>)}} by [[Virginia Vassilevska Williams]], giving a time only slightly worse than Le Gall's improvement: {{cite web |url=http://www.cs.stanford.edu/~virgi/matrixmult-f.pdf |title=Multiplying matrices faster than Coppersmith-Winograd |first=Virginia Vassilevska |last=Williams|authorlink= Virginia Vassilevska Williams}}</ref>  The Le Gall algorithm, and the Coppersmith–Winograd algorithm on which it is based, are similar to Strassen's algorithm: a way is devised for multiplying two {{math|''k'' × ''k''}}-matrices with fewer than {{math|''k''<sup>3</sup>}} multiplications, and this technique is applied recursively. However, the constant coefficient hidden by the [[Big O notation]] is so large that these algorithms are only worthwhile for matrices that are too large to handle on present-day computers.<ref>{{citation\n | last = Iliopoulos | first = Costas S.\n | doi = 10.1137/0218045\n | issue = 4\n | journal = SIAM Journal on Computing\n | mr = 1004789\n | quote = The Coppersmith–Winograd algorithm is not practical, due to the very large hidden constant in the upper bound on the number of multiplications required.\n | pages = 658–669\n | title = Worst-case complexity bounds on algorithms for computing the canonical structure of finite abelian groups and the Hermite and Smith normal forms of an integer matrix\n | url = http://www.williamstein.org/home/wstein/www/home/pernet/Papers/Hermite/Iliopoulos88.pdf\n | volume = 18\n | year = 1989| citeseerx = 10.1.1.531.9309\n }}</ref><ref name=\"robinson\">{{Cite journal | last1=Robinson | first1=Sara | title=Toward an Optimal Algorithm for Matrix Multiplication | url=http://www.siam.org/pdf/news/174.pdf | year=2005 | journal=SIAM News | volume=38 | issue=9}}</ref>\n\nSince any algorithm for multiplying two {{math|''n'' × ''n''}}-matrices has to process all {{math|2''n''<sup>2</sup>}} entries, there is an asymptotic lower bound of {{math|Ω(''n''<sup>2</sup>)}} operations. Raz proved a lower bound of {{math|Ω(''n''<sup>2</sup> log(''n''))}} for bounded coefficient arithmetic circuits over the real or complex numbers.<ref>[[Ran Raz]]. On the complexity of matrix product. In Proceedings of the thirty-fourth annual ACM symposium on Theory of computing. ACM Press, 2002. {{doi|10.1145/509907.509932}}.</ref>\n\nCohn ''et al.'' put methods such as the Strassen and Coppersmith–Winograd algorithms in an entirely different [[group theory|group-theoretic]] context, by utilising triples of subsets of finite groups which satisfy a disjointness property called the [[Triple product property|triple product property (TPP)]]. They show that if families of [[wreath product]]s of [[Abelian group]]s with symmetric groups realise families of subset triples with a simultaneous version of the TPP, then there are matrix multiplication algorithms with essentially quadratic complexity.<ref>Henry Cohn, [[Robert Kleinberg]], [[Balázs Szegedy]], and Chris Umans. Group-theoretic Algorithms for Matrix Multiplication. {{arxiv|math.GR/0511460}}. ''Proceedings of the 46th Annual Symposium on Foundations of Computer Science'', 23–25 October 2005, Pittsburgh, PA, IEEE Computer Society, pp.&nbsp;379–388.</ref><ref>Henry Cohn, Chris Umans. A Group-theoretic Approach to Fast Matrix Multiplication. {{arxiv|math.GR/0307321}}. ''Proceedings of the 44th Annual IEEE Symposium on Foundations of Computer Science'', 11–14 October 2003, Cambridge, MA, IEEE Computer Society, pp.&nbsp;438–449.</ref> Most researchers believe that this is indeed the case.<ref name=\"robinson\"/> However, Alon, Shpilka and [[Chris Umans]] have recently shown that some of these conjectures implying fast matrix multiplication are incompatible with another plausible conjecture, the [[sunflower conjecture]].<ref>[[Noga Alon|Alon]], Shpilka, Umans, [http://eccc.hpi-web.de/report/2011/067/ On Sunflowers and Matrix Multiplication]</ref>\n\n[[Freivalds' algorithm]] is a simple Monte Carlo algorithm that, given matrices {{mvar|A}}, {{mvar|B}} and {{mvar|C}}, verifies in {{math|Θ(''n''<sup>2</sup>)}} time if {{math|''AB'' {{=}} ''C''}}.\n\n==Parallel and distributed algorithms==\n\n===Shared-memory parallelism===\nThe [[#Divide and conquer algorithm|divide and conquer algorithm]] sketched earlier can be [[parallel algorithm|parallelized]] in two ways for [[shared-memory multiprocessor]]s. These are based on the fact that the eight recursive matrix multiplications in\n\n:<math>\\begin{pmatrix}\nA_{11} B_{11} + A_{12} B_{21} & A_{11} B_{12} + A_{12} B_{22}\\\\\nA_{21} B_{11} + A_{22} B_{21} & A_{21} B_{12} + A_{22} B_{22}\\\\\n\\end{pmatrix}\n</math>\n\ncan be performed independently of each other, as can the four summations (although the algorithm needs to \"join\" the multiplications before doing the summations). Exploiting the full parallelism of the problem, one obtains an algorithm that can be expressed in [[fork–join model|fork–join style]] [[pseudocode]]:<ref name=\"cilk\"/>\n\n<div style=\"margin-left: 35px; width: 600px\">\n{{framebox|blue}}\n'''Procedure''' {{math|multiply(''C'', ''A'', ''B'')}}:\n\n* Base case: if {{math|''n'' {{=}} 1}}, set {{math|''c''<sub>11</sub> ← ''a''<sub>11</sub> × ''b''<sub>11</sub>}} (or multiply a small block matrix).\n* Otherwise, allocate space for a new matrix {{mvar|T}} of shape {{math|''n'' × ''n''}}, then:\n** Partition {{mvar|A}} into {{math|''A''<sub>11</sub>}}, {{math|''A''<sub>12</sub>}}, {{math|''A''<sub>21</sub>}}, {{math|''A''<sub>22</sub>}}.\n** Partition {{mvar|B}} into {{math|''B''<sub>11</sub>}}, {{math|''B''<sub>12</sub>}}, {{math|''B''<sub>21</sub>}}, {{math|''B''<sub>22</sub>}}.\n** Partition {{mvar|C}} into {{math|''C''<sub>11</sub>}}, {{math|''C''<sub>12</sub>}}, {{math|''C''<sub>21</sub>}}, {{math|''C''<sub>22</sub>}}.\n** Partition {{mvar|T}} into {{math|''T''<sub>11</sub>}}, {{math|''T''<sub>12</sub>}}, {{math|''T''<sub>21</sub>}}, {{math|''T''<sub>22</sub>}}.\n** Parallel execution:\n*** ''Fork'' {{math|multiply(''C''<sub>11</sub>, ''A''<sub>11</sub>, ''B''<sub>11</sub>)}}.\n*** ''Fork'' {{math|multiply(''C''<sub>12</sub>, ''A''<sub>11</sub>, ''B''<sub>12</sub>)}}.\n*** ''Fork'' {{math|multiply(''C''<sub>21</sub>, ''A''<sub>21</sub>, ''B''<sub>11</sub>)}}.\n*** ''Fork'' {{math|multiply(''C''<sub>22</sub>, ''A''<sub>21</sub>, ''B''<sub>12</sub>)}}.\n*** ''Fork'' {{math|multiply(''T''<sub>11</sub>, ''A''<sub>12</sub>, ''B''<sub>21</sub>)}}.\n*** ''Fork'' {{math|multiply(''T''<sub>12</sub>, ''A''<sub>12</sub>, ''B''<sub>22</sub>)}}.\n*** ''Fork'' {{math|multiply(''T''<sub>21</sub>, ''A''<sub>22</sub>, ''B''<sub>21</sub>)}}.\n*** ''Fork'' {{math|multiply(''T''<sub>22</sub>, ''A''<sub>22</sub>, ''B''<sub>22</sub>)}}.\n** ''Join'' (wait for parallel forks to complete).\n** {{math|add(''C'', ''T'')}}.\n** Deallocate {{mvar|T}}.\n\n'''Procedure''' {{math|add(''C'', ''T'')}} adds {{mvar|T}} into {{mvar|C}}, element-wise:\n\n* Base case: if {{math|''n'' {{=}} 1}}, set {{math|''c''<sub>11</sub> ← ''c''<sub>11</sub> + ''t''<sub>11</sub>}} (or do a short loop, perhaps unrolled).\n* Otherwise:\n** Partition {{mvar|C}} into {{math|''C''<sub>11</sub>}}, {{math|''C''<sub>12</sub>}}, {{math|''C''<sub>21</sub>}}, {{math|''C''<sub>22</sub>}}.\n** Partition {{mvar|T}} into {{math|''T''<sub>11</sub>}}, {{math|''T''<sub>12</sub>}}, {{math|''T''<sub>21</sub>}}, {{math|''T''<sub>22</sub>}}.\n** In parallel:\n*** ''Fork'' {{math|add(''C''<sub>11</sub>, ''T''<sub>11</sub>)}}.\n*** ''Fork'' {{math|add(''C''<sub>12</sub>, ''T''<sub>12</sub>)}}.\n*** ''Fork'' {{math|add(''C''<sub>21</sub>, ''T''<sub>21</sub>)}}.\n*** ''Fork'' {{math|add(''C''<sub>22</sub>, ''T''<sub>22</sub>)}}.\n** ''Join''.\n{{frame-footer}}\n</div>\n\nHere, ''fork'' is a keyword that signal a computation may be run in parallel with the rest of the function call, while ''join'' waits for all previously \"forked\" computations to complete. {{math|partition}} achieves its goal by pointer manipulation only.\n\nThis algorithm has a [[critical path length]] of {{math|Θ(log<sup>2</sup> ''n'')}} steps, meaning it takes that much time on an ideal machine with an infinite number of processors; therefore, it has a maximum possible [[speedup]] of {{math|Θ(''n''<sup>3</sup>/log<sup>2</sup> ''n'')}} on any real computer. The algorithm isn't practical due to the communication cost inherent in moving data to and from the temporary matrix {{mvar|T}}, but a more practical variant achieves {{math|Θ(''n''<sup>2</sup>)}} speedup, without using a temporary matrix.<ref name=\"cilk\">{{cite thesis |type=Ph.D. |last=Randall |first=Keith H. |title=Cilk: Efficient Multithreaded Computing |publisher=Massachusetts Institute of Technology |year=1998 |pages=54–57 |url=http://supertech.csail.mit.edu/papers/randall-phdthesis.pdf}}</ref>\n\n[[File:Block matrix multiplication.svg|thumb|Block matrix multiplication. In the 2D algorithm, each processor is responsible for one submatrix of {{mvar|C}}. In the 3D algorithm, every pair of submatrices from {{mvar|A}} and {{mvar|B}} that is multiplied is assigned to one processor.]]\n\n===Communication-avoiding and distributed algorithms===\nOn modern architectures with hierarchical memory, the cost of loading and storing input matrix elements tends to dominate the cost of arithmetic. On a single machine this is the amount of data transferred between RAM and cache, while on a distributed memory multi-node machine it is the amount transferred between nodes; in either case it is called the ''communication bandwidth''. The naïve algorithm using three nested loops uses {{math|Ω(''n''<sup>3</sup>)}} communication bandwidth.\n\n[[Cannon's algorithm]], also known as the ''2D algorithm'', is a [[Communication-Avoiding Algorithms|communication-avoiding algorithm]] that partitions each input matrix into a block matrix whose elements are submatrices of size {{math|{{sqrt|''M''/3}}}} by {{math|{{sqrt|''M''/3}}}}, where {{math|''M''}} is the size of fast memory.<ref>Lynn Elliot Cannon, ''[http://portal.acm.org/citation.cfm?coll=GUIDE&dl=GUIDE&id=905686 A cellular computer to implement the Kalman Filter Algorithm]'', Technical report, Ph.D. Thesis, Montana State University, 14 July 1969.</ref> The naïve algorithm is then used over the block matrices, computing products of submatrices entirely in fast memory. This reduces communication bandwidth to {{math|''O''(''n''<sup>3</sup>/{{sqrt|''M''}})}}, which is asymptotically optimal (for algorithms performing {{math|Ω(''n''<sup>3</sup>)}} computation).<ref>{{cite journal|last=Hong|first=J. W.|first2=H. T. |last2=Kung|title=I/O complexity: The red-blue pebble game|journal=STOC '81: Proceedings of the Thirteenth Annual ACM Symposium on Theory of Computing|year=1981|pages=326–333|url=https://apps.dtic.mil/dtic/tr/fulltext/u2/a104739.pdf}}</ref><ref name=irony>{{cite journal|last=Irony|first=Dror|first2=Sivan |last2=Toledo |first3=Alexander |last3=Tiskin |title=Communication lower bounds for distributed-memory matrix multiplication|journal=J. Parallel Distrib. Comput.|date=September 2004|volume=64|issue=9|pages=1017–1026|doi=10.1016/j.jpdc.2004.03.021|citeseerx=10.1.1.20.7034}}</ref>\n\nIn a distributed setting with {{mvar|p}} processors arranged in a {{math|{{sqrt|''p''}}}} by {{math|{{sqrt|''p''}}}} 2D mesh, one submatrix of the result can be assigned to each processor, and the product can be computed with each processor transmitting {{math|''O''(''n''<sup>2</sup>/{{sqrt|''p''}})}} words, which is asymptotically optimal assuming that each node stores the minimum {{math|''O''(''n''<sup>2</sup>/''p'')}} elements.<ref name=irony/> This can be improved by the ''3D algorithm,'' which arranges the processors in a 3D cube mesh, assigning every product of two input submatrices to a single processor. The result submatrices are then generated by performing a reduction over each row.<ref name=\"Agarwal\">{{cite journal|last=Agarwal|first=R.C.|first2=S. M. |last2=Balle |first3=F. G. |last3=Gustavson |first4=M. |last4=Joshi |first5=P. |last5=Palkar |title=A three-dimensional approach to parallel matrix multiplication|journal=IBM J. Res. Dev.|date=September 1995|volume=39|issue=5|pages=575–582|doi=10.1147/rd.395.0575|citeseerx=10.1.1.44.3404}}</ref> This algorithm transmits {{math|''O''(''n''<sup>2</sup>/''p''<sup>2/3</sup>)}} words per processor, which is asymptotically optimal.<ref name=irony/> However, this requires replicating each input matrix element {{math|''p''<sup>1/3</sup>}} times, and so requires a factor of {{math|''p''<sup>1/3</sup>}} more memory than is needed to store the inputs. This algorithm can be combined with Strassen to further reduce runtime.<ref name=\"Agarwal\"/> \"2.5D\" algorithms provide a continuous tradeoff between memory usage and communication bandwidth.<ref>{{cite journal|last=Solomonik|first=Edgar|first2=James |last2=Demmel|title=Communication-optimal parallel 2.5D matrix multiplication and LU factorization algorithms|journal=Proceedings of the 17th International Conference on Parallel Processing|year=2011|volume=Part II|pages=90–109}}</ref> On modern distributed computing environments such as [[MapReduce]], specialized multiplication algorithms have been developed.<ref>{{cite journal|last1=Bosagh Zadeh|first1=Reza|last2=Carlsson|first2=Gunnar|title=Dimension Independent Matrix Square Using MapReduce|url=http://stanford.edu/~rezab/papers/dimsum.pdf|accessdate=12 July 2014}}</ref>\n\n===Algorithms for meshes===\nThere are a variety of algorithms for multiplication on [[Mesh networking|meshes]]. For multiplication of two ''n''×''n'' on a standard two-dimensional mesh using the 2D [[Cannon's algorithm]], one can complete the multiplication in 3''n''-2 steps although this is reduced to half this number for repeated computations.<ref>Bae, S.E., T.-W. Shinn, T. Takaoka (2014) [https://www.sciencedirect.com/science/article/pii/S1877050914003858/pdf?md5=4d641ad288db14733fb2f82ee445c258&pid=1-s2.0-S1877050914003858-main.pdf&_valck=1 A faster parallel algorithm for matrix multiplication on a mesh array]. Procedia Computer Science 29: 2230-2240</ref> The standard array is inefficient because the data from the two matrices does not arrive simultaneously and it must  be padded with zeroes.\n\nThe result is even faster on a two-layered cross-wired mesh, where only 2''n''-1 steps are needed.<ref>Kak, S. (1988) [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.88.8527&rep=rep1&type=pdf A two-layered mesh array for matrix multiplication]. Parallel Computing 6: 383-385</ref> The performance improves further for repeated computations leading to 100% efficiency.<ref>Kak, S. (2014) Efficiency of matrix multiplication on the cross-wired mesh array. https://arxiv.org/abs/1411.3273</ref> The cross-wired mesh array may be seen as a special case of a non-planar (i.e. multilayered) processing structure.<ref>Kak, S. (1988) [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.90.4753&rep=rep1&type=pdf Multilayered array computing]. Information Sciences 45: 347-365</ref>\n[[File:Matrix multiplication on a cross-wire two-dimensional array.png|thumb|240px|Matrix multiplication completed in 2n-1 steps for two n×n matrices on a cross-wired  mesh.]]\n\n==See also==\n* [[Computational complexity of mathematical operations]]\n* [[CYK algorithm#Valiant's algorithm|CYK algorithm, §Valiant's algorithm]]\n* [[Matrix chain multiplication]]\n* [[Sparse matrix-vector multiplication]]\n\n==References==\n{{reflist|30em}}\n\n==Further reading==\n* {{Cite journal| doi = 10.1016/j.parco.2008.10.002| title = A class of parallel tiled linear algebra algorithms for multicore architectures| journal = Parallel Computing| volume = 35| pages = 38–53| year = 2009| last1 = Buttari | first1 = Alfredo| last2 = Langou | first2 = Julien| last3 = Kurzak | first3 = Jakub| last4 = Dongarra | first4 = Jack |authorlink4=Jack Dongarra| arxiv = 0709.1272}}\n* {{Cite journal | doi = 10.1145/1356052.1356053| title = Anatomy of high-performance matrix multiplication| journal = ACM Transactions on Mathematical Software| volume = 34| issue = 3| pages =1–25| year = 2008| last1 = Goto | first1 = Kazushige| last2 = van de Geijn | first2 = Robert A.| citeseerx = 10.1.1.140.3583}}\n* {{Cite journal | doi = 10.1145/2764454 | title = BLIS: A Framework for Rapidly Instantiating BLAS Functionality| journal = ACM Transactions on Mathematical Software| volume = 41| issue = 3| pages =1–33| year = 2015| last1 = Van Zee  | first1 = Field G.| last2 = van de Geijn | first2 = Robert A.}}\n* [https://github.com/flame/how-to-optimize-gemm How To Optimize GEMM]\n\n{{Numerical linear algebra}}\n\n[[Category:Matrix multiplication algorithms| ]]"
    },
    {
      "title": "Cannon's algorithm",
      "url": "https://en.wikipedia.org/wiki/Cannon%27s_algorithm",
      "text": "In [[computer science]], '''Cannon's algorithm''' is a [[distributed algorithm|distributed]] [[algorithm for matrix multiplication]] for two-dimensional [[Mesh networking|meshes]] first described in 1969 by [[Lynn Elliot Cannon]].<ref>Lynn Elliot Cannon, ''[http://portal.acm.org/citation.cfm?coll=GUIDE&dl=GUIDE&id=905686 A cellular computer to implement the Kalman Filter Algorithm]'', Technical report, Ph.D. Thesis, Montana State University, 14 July 1969.</ref><ref name=\"stanfordpaper\">[http://dbpubs.stanford.edu:8090/pub/1994-25 Gupta, H.; Sadayappan, P.: Communication Efficient Matrix-Multiplication on Hypercubes], dbpubs.stanford.edu</ref>\n\nIt is especially suitable for computers laid out in an ''N'' × ''N'' mesh.<ref name=\"ornl.gov\">[http://www.phy.ornl.gov/csep/la/node6.html 4.2 Matrix Multiplication on a Distributed Memory Machine], www.phy.ornl.gov</ref>  While Cannon's algorithm works well in homogeneous 2D grids, extending it to heterogeneous 2D grids has been shown to be difficult.<ref name=\"lyonfr\">Jean-François Pineau, [https://tel.archives-ouvertes.fr/tel-00530131/document Communication-aware scheduling on heterogeneous master-worker platforms], PhD thesis, October 2010.</ref>\n\nThe main advantage of the algorithm is that its storage requirements remain constant and are independent of the number of processors.<ref name=\"stanfordpaper\"/>\n\nThe Scalable Universal Matrix Multiplication Algorithm (SUMMA)<ref>Robert A. van de Geijn and Jerrell Watts, [http://onlinelibrary.wiley.com/doi/10.1002/(SICI)1096-9128(199704)9:4%3C255::AID-CPE250%3E3.0.CO;2-2/abstract SUMMA: scalable universal matrix multiplication algorithm], Concurrency: Practice and Experience. Volume 9, Issue 4, pages 255–274, April 1997.</ref>\nis a more practical algorithm that requires less workspace and overcomes the need for a square 2D grid.  It is used by the [[ScaLAPACK]], [[PLAPACK]], and [http://code.google.com/p/elemental/ Elemental] libraries.\n\n==Algorithm overview==\n\nWhen multiplying two n×n matrices A and B, we need ''n''×n processing nodes p arranged in a 2d grid. Initially p<sub>i,j</sub> is responsible for a<sub>i,j</sub> and b<sub>i,j</sub>.\n // PE(i , j)\n k := (i + j) mod N;\n <nowiki>a := a[i][k];</nowiki>\n <nowiki>b := b[k][j];</nowiki>\n c[i][j] := 0;\n for(l := 0; l < N − 1; l++){\n <nowiki>    c[i][j] := c[i][j] + a * b;</nowiki>\n         concurrently{\n             send a to PE(i, (j + N − 1) mod N);\n             send b to PE((i + N − 1) mod N, j);\n         } with {\n             receive a' from PE(i, (j + 1) mod N);\n             receive b' from PE((i + 1) mod N, j );\n         }\n     a := a';\n     b := b';\n }\nWe need to select k in every iteration for every Processor Element (PE) so that processors don't access the same data for computing <math>a_{ik} * b_{kj}</math>.\n\nTherefore processors in the same row / column must begin summation with different indexes. If for example ''PE(0,0)''  calculates <math>a_{00} * b_{00}</math> in the first step, ''PE(0,1) chooses'' <math>a_{01} * b_{11}</math> first. The selection of ''k := (i + j) mod n'' for ''PE(i,j)'' satisfies this constraint for the first step.\n\nIn the first step we distribute the input matrices between the processors based on the previous rule.\n\nIn the next iterations we choose a new ''k' := (k + 1) mod n'' for every processor. This way every processor will continue accessing different values of the matrices. The needed data is then always at the neighbour processors. ''A PE(i,j)'' needs then the '''''<math>a</math>''''' from ''PE(i,(j + 1) mod n)'' and the <math>b</math> from ''PE((i + 1) mod n,j)'' for the next step. This means that <math>a</math> has to be passed cyclically to the left and also <math>b</math> cyclically upwards. The results of the multiplications are summed up as usual. After n steps, each processor has calculated all <math>a_{ik} * b_{kj}</math> once and its sum is thus the searched <math>c_{ij}</math>.\n\nAfter the initial distribution of each processor, only the data for the next step has to be stored. These are the intermediate result of the previous sum, a <math>a_{ik}</math> and a <math> b_{kj}</math>. This means that all three matrices only need to be stored in memory once evenly distributed across the processors.\n\n=== Generalisation ===\nIn practice we have much fewer processors than the matrix elements. We can replace the matrix elements with submatrices, so that every processor processes more values. The scalar multiplication and addition become sequential matrix multiplication and addition. The width and height of the submatrices will be <math>N=n/\\sqrt {p}</math>.\n\nThe runtime of the algorithm is  <math>T\\mathcal{(n, p)} = T_{coll} (n/N, p) + N*T_{seq}(n/N) + 2(N - 1)(T_{start} + T_{byte}(n/ N)^2)\n</math> , where <math>T_{coll}</math> is the time of the initial distribution of the matrices in the first step, <math>T_{seq}</math> is the calculation of the intermediate results and <math>T_{start}</math> and <math>T_{byte}</math> stands for the time it takes to establish a connection and transmission of byte respectively.\n\nA disadvantage of the algorithm is that there are many connection setups, with small message sizes. It would be better to be able to transmit more data in each message.\n\n== See also ==\n* [[Systolic array]]\n\n== References ==\n<references/>\n\n== External links ==\n* [http://www.cs.berkeley.edu/~demmel/cs267/lecture11/lecture11.html Lecture at Berkeley]\n* [https://web.archive.org/web/20070703035731/http://www.cs.mu.oz.au/498/notes/node30.html mu.oz.au] \n\n{{Numerical linear algebra}}\n\n{{DEFAULTSORT:Cannon's Algorithm}}\n[[Category:Distributed algorithms]]\n[[Category:Matrix multiplication algorithms]]\n[[Category:Mesh networking]]\n\n\n{{mathapplied-stub}}"
    },
    {
      "title": "Freivalds' algorithm",
      "url": "https://en.wikipedia.org/wiki/Freivalds%27_algorithm",
      "text": "'''Freivalds' algorithm''' (named after [[Rūsiņš Mārtiņš Freivalds]]) is a probabilistic [[randomized algorithm]] used to verify [[matrix multiplication]]. Given three ''n''&nbsp;&times;&nbsp;''n'' [[matrix (mathematics)|matrices]] <math>A</math>, <math>B</math>, and <math>C</math>, a general problem is to verify whether <math>A \\times B = C</math>. A naïve [[algorithm]] would compute the product <math>A \\times B</math> explicitly and compare term by term whether this product equals <math>C</math>. However, the best known matrix multiplication algorithm runs in <math>O(n^{2.3729})</math> time.<ref name=\"williams\">{{cite web |url=http://www.cs.berkeley.edu/~virgi/matrixmult-f.pdf |title=Breaking the Coppersmith-Winograd barrier\n|author=Virginia Vassilevska Williams|authorlink= Virginia Vassilevska Williams}}</ref> Freivalds' algorithm utilizes [[randomization]] in order to reduce this time bound to <math>O(n^2)</math>\n<ref>\n{{cite journal\n|doi=10.1145/234313.234327\n|last=Raghavan\n|first=Prabhakar\n|title=Randomized algorithms\n|journal=ACM Computing Surveys\n|year=1997\n|volume=28\n|pages=33\n|publisher=\n|accessdate=2008-12-16\n|url=http://portal.acm.org/citation.cfm?id=234327}}</ref>\n[[with high probability]]. In <math>O(kn^2)</math> time the algorithm can verify a matrix product with probability of failure less than <math>2^{-k}</math>.\n\n==The algorithm==\n\n===Input===\nThree ''n''&nbsp;&times;&nbsp;''n'' [[matrix (mathematics)|matrices]] <math>A</math>, <math>B</math>, and <math>C</math>.\n\n===Output===\nYes, if <math>A \\times B = C</math>; No, otherwise.\n\n===Procedure===\n\n# Generate an ''n''&nbsp;&times;&nbsp;1 [[random]] 0/1 [[vector (geometric)|vector]] <math>\\vec{r}</math>.\n# Compute <math>\\vec{P} = A \\times (B \\vec{r}) - C\\vec{r}</math>.\n# Output \"Yes\" if <math>\\vec{P} = (0,0,\\ldots,0)^T</math>; \"No,\" otherwise.\n\n===Error===\nIf <math>A \\times B = C</math>, then the algorithm always returns \"Yes\". If <math>A \\times B \\neq C</math>, then the probability that the algorithm returns \"Yes\" is less than or equal to one half. This is called [[one-sided error]].\n\nBy iterating the algorithm ''k'' times and returning \"Yes\" only if all iterations yield \"Yes\", a runtime of <math>O(kn^2)</math> and error probability of <math>\\leq 1/2^k</math> is achieved.\n\n==Example==\nSuppose one wished to determine whether:\n:<math>\nAB =\n\\begin{bmatrix}\n 2 & 3 \\\\\n 3 & 4\n\\end{bmatrix}\n\\begin{bmatrix}\n 1 & 0 \\\\\n 1 & 2\n\\end{bmatrix}\n\\stackrel{?}{=}\n\\begin{bmatrix}\n 6 & 5 \\\\\n 8 & 7\n\\end{bmatrix}\n= C.\n</math>\nA random two-element vector with entries equal to 0 or 1 is selected &mdash; say <math>\\vec{r} = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}</math> &mdash; and used to compute:\n:<math>\n\\begin{align}\nA \\times (B \\vec{r}) - C\\vec{r} & =\n\\begin{bmatrix}\n 2 & 3 \\\\\n 3 & 4\n\\end{bmatrix}\n\\left(\n\\begin{bmatrix}\n 1 & 0 \\\\\n 1 & 2\n\\end{bmatrix}\n\\begin{bmatrix}1 \\\\ 1\\end{bmatrix}\n\\right)\n-\n\\begin{bmatrix}\n 6 & 5 \\\\\n 8 & 7\n\\end{bmatrix}\n\\begin{bmatrix}1 \\\\ 1\\end{bmatrix} \\\\\n& =\n\\begin{bmatrix}\n 2 & 3 \\\\\n 3 & 4\n\\end{bmatrix}\n\\begin{bmatrix}1 \\\\ 3\\end{bmatrix}\n-\n\\begin{bmatrix}11 \\\\ 15\\end{bmatrix} \\\\\n& =\n\\begin{bmatrix}11 \\\\ 15\\end{bmatrix}\n-\n\\begin{bmatrix}11 \\\\ 15\\end{bmatrix} \\\\\n& =\n\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}.\n\\end{align}\n</math>\nThis yields the zero vector, suggesting the possibility that AB = C. However, if in a second trial the vector <math>\\vec{r} = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix}</math> is selected, the result becomes:\n:<math>\nA \\times (B \\vec{r}) - C\\vec{r} =\n\\begin{bmatrix}\n 2 & 3 \\\\\n 3 & 4\n\\end{bmatrix}\n\\left(\n\\begin{bmatrix}\n 1 & 0 \\\\\n 1 & 2\n\\end{bmatrix}\n\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}\n\\right)\n-\n\\begin{bmatrix}\n 6 & 5 \\\\\n 8 & 7\n\\end{bmatrix}\n\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}\n=\n\\begin{bmatrix}-1 \\\\ -1\\end{bmatrix}.\n</math>\nThe result is nonzero, proving that in fact AB &ne; C.\n\nThere are four two-element 0/1 vectors, and half of them give the zero vector in this case (<math>\\vec{r} = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}</math> and <math>\\vec{r} = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}</math>), so the chance of randomly selecting these in two trials (and falsely concluding that AB=C) is 1/2<sup>2</sup> or 1/4. In the general case, the proportion of ''r'' yielding the zero vector may be less than 1/2, and a larger number of trials (such as 20) would be used, rendering the probability of error very small.\n\n==Error analysis==\nLet ''p'' equal the [[probability]] of error. We claim that if ''A''&nbsp;&times;&nbsp;''B'' = ''C'', then ''p'' = 0, and if ''A''&nbsp;&times;&nbsp;''B'' ≠ ''C'', then ''p'' ≤ 1/2.\n\n===Case ''A''&nbsp;&times;&nbsp;''B'' = ''C''===\n:<math>\n\\begin{align}\n\\vec{P} &= A \\times (B \\vec{r}) - C \\vec{r}\\\\\n&= (A \\times B)\\vec{r} - C\\vec{r}\\\\\n&= (A \\times B - C)\\vec{r}\\\\\n&= \\vec{0}\n\\end{align}\n</math>\n\nThis is regardless of the value of <math>\\vec{r}</math>, since it uses only that <math>A \\times B - C = 0</math>. Hence the probability for error in this case is:\n\n:<math>\\Pr[\\vec{P} \\neq 0] = 0</math>\n\n===Case ''A''&nbsp;&times;&nbsp;''B'' ≠ ''C''===\nLet <math>D</math> such that\n\n:<math>\\vec{P} = D \\times \\vec{r} = (p_1, p_2, \\dots, p_n)^T</math>\n\nWhere\n\n:<math>D = A \\times B - C = (d_{ij})</math>.\n\nSince <math>A \\times B \\neq C</math>, we have that some element of <math>D</math> is nonzero. Suppose that the element <math>d_{ij} \\neq 0</math>. By the definition of [[matrix multiplication]], we have:\n\n:<math>p_i = \\sum_{k = 1}^n d_{ik}r_k = d_{i1}r_1 + \\cdots + d_{ij}r_j + \\cdots + d_{in}r_n = d_{ij}r_j + y</math>.\n\nFor some constant <math>y</math>.\nUsing [[Bayes' Theorem]], we can partition over <math>y</math>:\n\n{{NumBlk|:|<math>\\Pr[p_i = 0] = \\Pr[p_i = 0 | y = 0]\\cdot \\Pr[y = 0]\\, +\\, \\Pr[p_i = 0 | y \\neq 0] \\cdot \\Pr[y \\neq 0]</math>|{{EquationRef|1}}}}\n\nWe use that:\n\n:<math>\\Pr[p_i = 0 | y = 0] = \\Pr[r_j = 0] = \\frac{1}{2}</math>\n:<math>\\Pr[p_i = 0 | y \\neq 0] = \\Pr[r_j = 1 \\land d_{ij}=-y] \\leq \\Pr[r_j = 1] = \\frac{1}{2}</math>\n\nPlugging these in the equation ({{EquationNote|1}}), we get:\n\n:<math>\n\\begin{align}\n\\Pr[p_i = 0] &\\leq \\frac{1}{2}\\cdot \\Pr[y = 0] + \\frac{1}{2}\\cdot \\Pr[y \\neq 0]\\\\\n &= \\frac{1}{2}\\cdot \\Pr[y = 0] + \\frac{1}{2}\\cdot (1 - \\Pr[y = 0])\\\\\n &= \\frac{1}{2}\n\\end{align}\n</math>\n\nTherefore, \n:<math>\\Pr[\\vec{P} = 0] = \\Pr[p_1 = 0 \\land \\dots \\land p_i = 0 \\land \\dots \\land p_n = 0] \\leq \\Pr[p_i = 0] \\leq \\frac{1}{2}.</math>\nThis completes the proof.\n\n==Ramifications==\nSimple algorithmic analysis shows that the running time of this [[algorithm]] is [[Big O notation|O]](''n''<sup>2</sup>), beating the classical [[deterministic algorithm|deterministic algorithm's]] bound of [[Big O notation|O]](''n''<sup>3</sup>). The error analysis also shows that if we run our [[algorithm]] ''k'' times, we can achieve an [[error bound]] of less than <math>\\frac{1}{2^k}</math>, an exponentially small quantity. The algorithm is also fast in practice due to wide availability of fast implementations for matrix-vector products. Therefore, utilization of [[randomized algorithms]] can speed up a very slow [[deterministic algorithm]]. In fact, the best known deterministic matrix multiplication algorithm known at the current time is a variant of the [[Coppersmith–Winograd algorithm]] with an asymptotic running time of [[Big O notation|O]](''n''<sup>2.3729</sup>).<ref name=\"williams\"/>\n\nFreivalds' algorithm frequently arises in introductions to [[probabilistic algorithm]]s due to its simplicity and how it illustrates the superiority of probabilistic algorithms in practice for some problems.\n\n==See also==\n* [[Schwartz–Zippel lemma]]\n\n==References==\n{{reflist}}\n* Freivalds, R. (1977), “Probabilistic Machines Can Use Less Running Time”, IFIP Congress 1977, pp.&nbsp;839–842.\n\n{{Numerical linear algebra}}\n\n{{DEFAULTSORT:Freivald's Algorithm}}\n[[Category:Articles containing proofs]]\n[[Category:Matrix theory]]\n[[Category:Randomized algorithms]]\n[[Category:Matrix multiplication algorithms]]"
    },
    {
      "title": "Strassen algorithm",
      "url": "https://en.wikipedia.org/wiki/Strassen_algorithm",
      "text": "{{distinguish|text=the [[Schönhage–Strassen algorithm]] for multiplication of polynomials}}\n\nIn [[linear algebra]], the '''Strassen algorithm''', named after [[Volker Strassen]], is an [[Matrix multiplication algorithm|algorithm for matrix multiplication]]. It is faster than the standard matrix multiplication algorithm and is useful in practice for large matrices, but would be slower than [[Coppersmith–Winograd algorithm|the fastest known algorithms]] for extremely large matrices.\n\nStrassen's algorithm works for any [[ring (mathematics)|ring]], such as plus/multiply, but not all [[semirings]], such as [[Min-plus matrix multiplication|min-plus]] or [[boolean algebra]], where the naive algorithm still works, and so called [[combinatorial matrix multiplication]].\n\n== History ==\n[[Volker Strassen]] first published this algorithm in 1969 and proved that the {{math|''n''<sup>3</sup>}} general matrix multiplication algorithm wasn't optimal. The '''Strassen algorithm''' is only slightly better than that, but its publication resulted in much more research about matrix multiplication that led to faster approaches, such as the [[Coppersmith-Winograd algorithm]].\n\n== Algorithm ==\n[[Image:Strassen algorithm.svg|thumb|800px|centre|The left column represents 2x2 [[matrix multiplication]]. Naïve matrix multiplication requires one multiplication for each \"1\" of the left column. Each of the other columns represents a single one of the 7 multiplications in the algorithm, and the sum of the columns gives the full matrix multiplication on the left. <!-- Feel free to rewrite this description so it actually makes sense. -->]]\n\nLet ''A'', ''B'' be two [[square matrix|square matrices]] over a [[ring (mathematics)|ring]] ''R''. We want to calculate the matrix product ''C'' as\n\n:<math>\\mathbf{C} = \\mathbf{A} \\mathbf{B} \\qquad \\mathbf{A},\\mathbf{B},\\mathbf{C} \\in R^{2^n \\times 2^n}</math>\n\nIf the matrices ''A'', ''B'' are not of type 2<sup>''n''</sup> × 2<sup>''n''</sup> we fill the missing rows and columns with zeros.\n\nWe partition ''A'', ''B'' and ''C'' into equally sized [[block matrix|block matrices]]\n:<math> \n\\mathbf{A} =\n\\begin{bmatrix}\n\\mathbf{A}_{1,1} & \\mathbf{A}_{1,2} \\\\\n\\mathbf{A}_{2,1} & \\mathbf{A}_{2,2}\n\\end{bmatrix}\n\\mbox { , }\n\\mathbf{B} =\n\\begin{bmatrix}\n\\mathbf{B}_{1,1} & \\mathbf{B}_{1,2} \\\\\n\\mathbf{B}_{2,1} & \\mathbf{B}_{2,2}\n\\end{bmatrix}\n\\mbox { , }\n\\mathbf{C} =\n\\begin{bmatrix}\n\\mathbf{C}_{1,1} & \\mathbf{C}_{1,2} \\\\\n\\mathbf{C}_{2,1} & \\mathbf{C}_{2,2}\n\\end{bmatrix}\n</math>\n\nwith\n\n:<math>\\mathbf{A}_{i,j}, \\mathbf{B}_{i,j}, \\mathbf{C}_{i,j} \\in R^{2^{n-1} \\times 2^{n-1}}</math>.\n\nThe naive algorithm would be:\n\n:<math>\\mathbf{C}_{1,1} = \\mathbf{A}_{1,1} \\mathbf{B}_{1,1} + \\mathbf{A}_{1,2} \\mathbf{B}_{2,1} </math>\n:<math>\\mathbf{C}_{1,2} = \\mathbf{A}_{1,1} \\mathbf{B}_{1,2} + \\mathbf{A}_{1,2} \\mathbf{B}_{2,2} </math>\n:<math>\\mathbf{C}_{2,1} = \\mathbf{A}_{2,1} \\mathbf{B}_{1,1} + \\mathbf{A}_{2,2} \\mathbf{B}_{2,1} </math>\n:<math>\\mathbf{C}_{2,2} = \\mathbf{A}_{2,1} \\mathbf{B}_{1,2} + \\mathbf{A}_{2,2} \\mathbf{B}_{2,2} </math>\n\nWith this construction we have not reduced the number of multiplications. We still need 8 multiplications to calculate the ''C<sub>i,j</sub>'' matrices, the same number of multiplications we need when using standard matrix multiplication.\n\nThe Strassen algorithm defines instead new matrices:\n\n:<math>\\mathbf{M}_{1} := (\\mathbf{A}_{1,1} + \\mathbf{A}_{2,2}) (\\mathbf{B}_{1,1} + \\mathbf{B}_{2,2})</math>\n:<math>\\mathbf{M}_{2} := (\\mathbf{A}_{2,1} + \\mathbf{A}_{2,2}) \\mathbf{B}_{1,1}</math>\n:<math>\\mathbf{M}_{3} := \\mathbf{A}_{1,1} (\\mathbf{B}_{1,2} - \\mathbf{B}_{2,2})</math>\n:<math>\\mathbf{M}_{4} := \\mathbf{A}_{2,2} (\\mathbf{B}_{2,1} - \\mathbf{B}_{1,1})</math>\n:<math>\\mathbf{M}_{5} := (\\mathbf{A}_{1,1} + \\mathbf{A}_{1,2}) \\mathbf{B}_{2,2}</math>\n:<math>\\mathbf{M}_{6} := (\\mathbf{A}_{2,1} - \\mathbf{A}_{1,1}) (\\mathbf{B}_{1,1} + \\mathbf{B}_{1,2})</math>\n:<math>\\mathbf{M}_{7} := (\\mathbf{A}_{1,2} - \\mathbf{A}_{2,2}) (\\mathbf{B}_{2,1} + \\mathbf{B}_{2,2})</math>\n\nonly using 7 multiplications (one for each ''M''<sub>k</sub>) instead of 8. We may now express the ''C''<sub>i,j</sub> in terms of ''M''<sub>k</sub>:\n\n:<math>\\mathbf{C}_{1,1} = \\mathbf{M}_{1} + \\mathbf{M}_{4} - \\mathbf{M}_{5} + \\mathbf{M}_{7}</math>\n:<math>\\mathbf{C}_{1,2} = \\mathbf{M}_{3} + \\mathbf{M}_{5}</math>\n:<math>\\mathbf{C}_{2,1} = \\mathbf{M}_{2} + \\mathbf{M}_{4}</math>\n:<math>\\mathbf{C}_{2,2} = \\mathbf{M}_{1} - \\mathbf{M}_{2} + \\mathbf{M}_{3} + \\mathbf{M}_{6}</math>\n\nWe iterate this division process ''n'' times (recursively) until the [[submatrices]] degenerate into numbers (elements of the ring ''R''). The resulting product will be padded with zeroes just like ''A'' and ''B'', and should be stripped of the corresponding rows and columns.\n\nPractical implementations of Strassen's algorithm switch to standard methods of matrix multiplication for small enough submatrices, for which those algorithms are more efficient. The particular crossover point for which Strassen's algorithm is more efficient depends on the specific implementation and hardware. Earlier authors had estimated that Strassen's algorithm is faster for matrices with widths from 32 to 128 for optimized implementations.<ref>{{Citation | last1=Skiena | first1=Steven S. | title=The Algorithm Design Manual | publisher=[[Springer-Verlag]] | location=Berlin, New York | isbn=978-0-387-94860-7 | year=1998 | chapter=§8.2.3 Matrix multiplication}}.</ref> However, it has been observed that this crossover point has been increasing in recent years, and a 2010 study found that even a single step of Strassen's algorithm is often not beneficial on current architectures, compared to a highly optimized traditional multiplication, until matrix sizes exceed 1000 or more, and even for matrix sizes of several thousand the benefit is typically marginal at best (around 10% or less).<ref name=\"dalberto\"/>  A more recent study (2016) observed benefits for matrices as small as 512 and a benefit around 20%.<ref name=\"huang et al.\"/>\n\n== Asymptotic complexity ==\n\nThe standard matrix multiplication takes approximately {{math|2''N''<sup>3</sup>}} (where \n{{math|''N'' {{=}} 2<sup>''n''</sup>)}} arithmetic operations (additions and multiplications); the asymptotic complexity is {{math|Θ(''N''<sup>3</sup>)}}.\n\nThe number of additions and multiplications required in the Strassen algorithm can be calculated as follows: let {{math|''f''(''n'')}} be the number of operations for a {{math|2<sup>''n''</sup> × 2<sup>''n''</sup>}} matrix. Then by recursive application of the Strassen algorithm, we see that {{math|''f''(''n'') {{=}} 7''f''(''n''−1) + ''ℓ''4<sup>''n''</sup>}}, for some constant {{mvar|ℓ}} that depends on the number of additions performed at each application of the algorithm. Hence {{math|''f''(''n'') {{=}} (7 + o(1))<sup>''n''</sup>}}, i.e., the asymptotic complexity for multiplying matrices of size {{math|''N'' {{=}} 2<sup>''n''</sup>}} using the Strassen algorithm is\n\n:<math>O([7+o(1)]^n) = O(N^{\\log_{2}7+o(1)}) \\approx O(N^{2.8074})</math>.\n\nThe reduction in the number of arithmetic operations however comes at the price of a somewhat reduced [[numerical stability]],<ref>{{cite journal|last=Webb|first=Miller|title=Computational complexity and numerical stability|journal=SIAM J. Comput.|year=1975|pages=97–107}}</ref> and the algorithm also requires significantly more memory compared to the naive algorithm. Both initial matrices must have their dimensions expanded to the next power of 2, which results in storing up to four times as many elements, and the seven auxiliary matrices each contain a quarter of the elements in the expanded ones.\n\nThe \"naive\" way of doing the matrix multiplication would require 8 instead of 7 multiplications of sub-blocks. This would then give rise to the complexity one expects from the standard approach:\n:<math>O(8^n) = O(N^{\\log_{2}8}) = O(N^3)</math>.\n\n\n=== Rank or bilinear complexity ===\nThe bilinear complexity or '''rank''' of a [[bilinear map]] is an important concept in the asymptotic complexity of matrix multiplication.  The rank of a bilinear map <math>\\phi:\\mathbf A \\times \\mathbf B \\rightarrow \\mathbf C</math> over a field '''F''' is defined as (somewhat of an [[abuse of notation]])\n:<math>R(\\phi/\\mathbf F) = \\min \\left\\{r\\left|\\exists f_i\\in \\mathbf A^*,g_i\\in\\mathbf B^*,w_i\\in\\mathbf C , \\forall \\mathbf a\\in\\mathbf A, \\mathbf b\\in\\mathbf B, \\phi(\\mathbf a,\\mathbf b) = \\sum_{i=1}^r f_i(\\mathbf a)g_i(\\mathbf b)w_i \\right.\\right\\}</math>\nIn other words, the rank of a bilinear map is the length of its shortest bilinear computation.<ref>Burgisser, Clausen, and Shokrollahi.  ''Algebraic Complexity Theory.'' Springer-Verlag 1997.</ref>  The existence of Strassen's algorithm shows that the rank of 2&times;2 matrix multiplication is no more than seven.  To see this, let us express this algorithm (alongside the standard algorithm) as such a bilinear computation.  In the case of matrices, the [[dual space]]s '''A'''* and '''B'''* consist of maps into the field '''F''' induced by a scalar '''[[Dot product|double-dot product]]''', (i.e. in this case the sum of all the entries of a [[Hadamard product (matrices)|Hadamard product]].)\n{| class = \"wikitable\"\n| || colspan=\"3\" | Standard algorithm || ||colspan=\"3\" | Strassen algorithm\n|-\n|| ''i'' || ''f<sub>i</sub>''('''a''') || ''g<sub>i</sub>''('''b''') || ''w<sub>i</sub>'' || || ''f<sub>i</sub>''('''a''') || ''g<sub>i</sub>''('''b''') || ''w<sub>i</sub>''\n|-\n|| 1\n||<math>\\begin{bmatrix}1&0\\\\0&0\\end{bmatrix}:\\mathbf a</math>\n||<math>\\begin{bmatrix}1&0\\\\0&0\\end{bmatrix}:\\mathbf b</math>\n||<math>\\begin{bmatrix}1&0\\\\0&0\\end{bmatrix}</math>\n||\n||<math>\\begin{bmatrix}1&0\\\\0&1\\end{bmatrix}:\\mathbf a</math>\n||<math>\\begin{bmatrix}1&0\\\\0&1\\end{bmatrix}:\\mathbf b</math>\n||<math>\\begin{bmatrix}1&0\\\\0&1\\end{bmatrix}</math>\n|-\n|| 2\n||<math>\\begin{bmatrix}0&1\\\\0&0\\end{bmatrix}:\\mathbf a</math>\n||<math>\\begin{bmatrix}0&0\\\\1&0\\end{bmatrix}:\\mathbf b</math>\n||<math>\\begin{bmatrix}1&0\\\\0&0\\end{bmatrix}</math>\n||\n||<math>\\begin{bmatrix}0&0\\\\1&1\\end{bmatrix}:\\mathbf a</math>\n||<math>\\begin{bmatrix}1&0\\\\0&0\\end{bmatrix}:\\mathbf b</math>\n||<math>\\begin{bmatrix}0&0\\\\1&-1\\end{bmatrix}</math>\n|-\n|| 3\n||<math>\\begin{bmatrix}1&0\\\\0&0\\end{bmatrix}:\\mathbf a</math>\n||<math>\\begin{bmatrix}0&1\\\\0&0\\end{bmatrix}:\\mathbf b</math>\n||<math>\\begin{bmatrix}0&1\\\\0&0\\end{bmatrix}</math>\n||\n||<math>\\begin{bmatrix}1&0\\\\0&0\\end{bmatrix}:\\mathbf a</math>\n||<math>\\begin{bmatrix}0&1\\\\0&-1\\end{bmatrix}:\\mathbf b</math>\n||<math>\\begin{bmatrix}0&1\\\\0&1\\end{bmatrix}</math>\n|-\n|| 4\n||<math>\\begin{bmatrix}0&1\\\\0&0\\end{bmatrix}:\\mathbf a</math>\n||<math>\\begin{bmatrix}0&0\\\\0&1\\end{bmatrix}:\\mathbf b</math>\n||<math>\\begin{bmatrix}0&1\\\\0&0\\end{bmatrix}</math>\n||\n||<math>\\begin{bmatrix}0&0\\\\0&1\\end{bmatrix}:\\mathbf a</math>\n||<math>\\begin{bmatrix}-1&0\\\\1&0\\end{bmatrix}:\\mathbf b</math>\n||<math>\\begin{bmatrix}1&0\\\\1&0\\end{bmatrix}</math>\n|-\n|| 5\n||<math>\\begin{bmatrix}0&0\\\\1&0\\end{bmatrix}:\\mathbf a</math>\n||<math>\\begin{bmatrix}1&0\\\\0&0\\end{bmatrix}:\\mathbf b</math>\n||<math>\\begin{bmatrix}0&0\\\\1&0\\end{bmatrix}</math>\n||\n||<math>\\begin{bmatrix}1&1\\\\0&0\\end{bmatrix}:\\mathbf a</math>\n||<math>\\begin{bmatrix}0&0\\\\0&1\\end{bmatrix}:\\mathbf b</math>\n||<math>\\begin{bmatrix}-1&1\\\\0&0\\end{bmatrix}</math>\n|-\n|| 6\n||<math>\\begin{bmatrix}0&0\\\\0&1\\end{bmatrix}:\\mathbf a</math>\n||<math>\\begin{bmatrix}0&0\\\\1&0\\end{bmatrix}:\\mathbf b</math>\n||<math>\\begin{bmatrix}0&0\\\\1&0\\end{bmatrix}</math>\n||\n||<math>\\begin{bmatrix}-1&0\\\\1&0\\end{bmatrix}:\\mathbf a</math>\n||<math>\\begin{bmatrix}1&1\\\\0&0\\end{bmatrix}:\\mathbf b</math>\n||<math>\\begin{bmatrix}0&0\\\\0&1\\end{bmatrix}</math>\n|-\n|| 7\n||<math>\\begin{bmatrix}0&0\\\\1&0\\end{bmatrix}:\\mathbf a</math>\n||<math>\\begin{bmatrix}0&1\\\\0&0\\end{bmatrix}:\\mathbf b</math>\n||<math>\\begin{bmatrix}0&0\\\\0&1\\end{bmatrix}</math>\n||\n||<math>\\begin{bmatrix}0&1\\\\0&-1\\end{bmatrix}:\\mathbf a</math>\n||<math>\\begin{bmatrix}0&0\\\\1&1\\end{bmatrix}:\\mathbf b</math>\n||<math>\\begin{bmatrix}1&0\\\\0&0\\end{bmatrix}</math>\n|-\n|| 8\n||<math>\\begin{bmatrix}0&0\\\\0&1\\end{bmatrix}:\\mathbf a</math>\n||<math>\\begin{bmatrix}0&0\\\\0&1\\end{bmatrix}:\\mathbf b</math>\n||<math>\\begin{bmatrix}0&0\\\\0&1\\end{bmatrix}</math>\n||\n|colspan=\"3\"|\n|-\n||\n|colspan=\"3\"|<math>\\mathbf a\\mathbf b = \\sum_{i=1}^8 f_i(\\mathbf a)g_i(\\mathbf b)w_i</math>\n||\n|colspan=\"3\"|<math>\\mathbf a\\mathbf b = \\sum_{i=1}^7 f_i(\\mathbf a)g_i(\\mathbf b)w_i</math>\n|}\nIt can be shown that the total number of elementary multiplications ''L'' required for matrix multiplication is tightly asymptotically bound to the rank ''R'', i.e. <math>L = \\Theta(R)</math>, or more specifically, since the constants are known, <math>\\frac 1 2 R\\le L\\le R.</math>  One useful property of the rank is that it is submultiplicative for [[tensor product]]s, and this enables one to show that 2<sup>''n''</sup>&times;2<sup>''n''</sup>&times;2<sup>''n''</sup> matrix multiplication can be accomplished with no more than 7<sup>''n''</sup> elementary multiplications for any ''n''.  (This ''n''-fold tensor product of the 2&times;2&times;2 matrix multiplication map with itself&mdash;an ''n''th tensor power&mdash;is realized by the recursive step in the algorithm shown.)\n\n===Cache behavior===\nStrassen's algorithm is [[Cache-oblivious algorithm|cache oblivious]]. Analysis of its [[CPU cache|cache]] behavior algorithm has shown it to incur\n\n:<math>\\Theta \\left(1 + \\frac{n^2}{b} + \\frac{n^{\\log_2 7}}{b\\sqrt{M}} \\right)</math>\n\ncache misses during its execution, assuming an idealized cache of {{mvar|M}} lines, each of {{mvar|b}} bytes.<ref name=\"prokop\">{{cite conference |first1=M. |last1=Frigo |first2=C. E. |last2=Leiserson |authorlink2=Charles Leiserson |first3=H. |last3=Prokop |authorlink3=Harald Prokop |first4=S. |last4=Ramachandran |title=Cache-oblivious algorithms |conference=Proc. IEEE Symp. on Foundations of Computer Science (FOCS) |pages=285–297 |year=1999 |url=http://supertech.csail.mit.edu/papers/FrigoLePr99.pdf}}</ref>{{rp|13}}\n\n== Implementation considerations ==\n{{refimprove section|date=January 2015}}\nThe description above states that the matrices are square, and the size is a power of two, and that padding should be used if needed. This restriction allows the matrices to be split in half, recursively, until limit of scalar multiplication is reached. The restriction simplifies the explanation, and analysis of complexity, but is not actually necessary;<ref>{{cite journal |last=Higham |first=Nicholas J. |title=Exploiting fast matrix multiplication within the level 3 BLAS |journal=ACM Transactions on Mathematical Software |volume=16 |issue=4 |year=1990 |pages=352–368 |url=http://www.maths.manchester.ac.uk/~higham/papers/high90s.pdf |doi=10.1145/98267.98290}}</ref>\nand in fact, padding the matrix as described will increase the computation time and can easily eliminate the fairly narrow time savings obtained by using the method in the first place.\n\nA good implementation will observe the following:\n\n* It is not necessary or desirable to use the Strassen algorithm down to the limit of scalars. Compared to conventional matrix multiplication, the algorithm adds a considerable  <math>O(n^{2})</math> workload in addition/subtractions; so below a certain size, it will be better to use conventional multiplication. Thus, for instance, if you start with matrices that are 1600x1600, there is no need to pad to 2048x2048, since you could subdivide down to 25x25 and then use conventional multiplication at that level.\n* The method can indeed be applied to square matrices of any dimension.<ref name=\"dalberto\">{{cite conference |first1=Paolo |last1=D'Alberto |first2=Alexandru |last2=Nicolau |title=Using Recursion to Boost ATLAS's Performance |year=2005 |conference=Sixth Int'l Symp. on High Performance Computing |url=https://www.ics.uci.edu/~paolo/Reference/paoloA.ishp-vi.pdf}}</ref> If the dimension is even, they are split in half as described. If the dimension is odd, zero padding by one row and one column is applied first.  Such padding can be applied on-the-fly and lazily, and the extra rows and columns discarded as the result is formed. For instance, suppose the matrices are 199x199. They can be split so that the upper-left portion is 100x100 and the lower-right is 99x99. Wherever the operations require it, dimensions of 99 are zero padded to 100 first. Note, for instance, that the product <math>M_2</math> is only used in the lower row of the output, so is only required to be 99 rows high; and thus the left factor <math>(A_{2,1} + A_{2,2})</math> used to generate it need only be 99 rows high; accordingly, there is no need to pad that sum to 100 rows; it is only necessary to pad <math>A_{2,2}</math> to 100 columns to match <math>A_{2,1}</math>.\n\nFurthermore, there is no need for the matrices to be square. Non-square matrices can be split in half using the same methods, yielding smaller non-square matrices. If the matrices are sufficiently non-square it will be worthwhile reducing the initial operation to more square products, using simple methods which are essentially  <math>O(n^{2})</math>,  for instance:\n\n* A product of size [2''N'' x ''N''] * [''N'' x 10''N''] can be done as 20 separate [''N'' x ''N''] * [''N'' x ''N''] operations, arranged to form the result;\n* A product of size [''N'' x 10''N''] * [10''N'' x ''N''] can be done as 10 separate [''N'' x ''N''] * [''N'' x ''N'']  operations, summed to form the result.\n\nThese techniques will make the implementation more complicated, compared to simply padding to a power-of-two square; however, it is a reasonable assumption that anyone undertaking an implementation of Strassen, rather than conventional, multiplication, will place a higher priority on computational efficiency than on simplicity of the implementation.\n\nIn practice, Strassen's algorithm can be implemented to attain better performance than conventional multiplication even for small matrices, for matrices that are not at all square, and without requiring workspace beyond buffers that are already needed for a high-performance conventional multiplication.<ref name=\"huang et al.\">{{cite conference |first1=Jianyu |last1=Huang |first2=Tyler |last2=Smith|first3=Greg| last3=Henry |first4=Robert|last4=van de Geijn |title=Strassen's Algorithm Reloaded |year=2016 |conference=International Conference for High Performance Computing, Networking, Storage and Analysis (SC'16) |url=http://dl.acm.org/citation.cfm?id=3014983&CFID=929265487&CFTOKEN=50939174}}</ref>\n\n== See also ==\n* [[Computational complexity of mathematical operations]]\n* [[Gauss–Jordan elimination]]\n* [[Coppersmith–Winograd algorithm]]\n* [[Z-order (curve)|Z-order matrix representation]]\n* [[Karatsuba algorithm]], for multiplying ''n''-digit integers in <math>O(n^{\\log_2 3})</math> instead of in <math>O(n^2)</math> time\n* [[Multiplication algorithm#Complex multiplication algorithm|Gauss's complex multiplication algorithm]] multiplies two complex numbers using 3 real multiplications instead of 4\n\n== References ==\n<references/>\n* Strassen, Volker, ''Gaussian Elimination is not Optimal'', Numer. Math. 13, p.&nbsp;354-356, 1969\n* [[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. ''[[Introduction to Algorithms]]'', Second Edition. MIT Press and McGraw-Hill, 2001. {{isbn|0-262-03293-7}}. Chapter 28: Section 28.2: Strassen's algorithm for matrix multiplication, pp.&nbsp;735&ndash;741.\n\n==External links==\n*{{MathWorld|urlname=StrassenFormulas|title=Strassen's Formulas}} (also includes formulas for fast [[matrix inversion]])\n*Tyler J. Earnest, ''[https://web.archive.org/web/20100612150812/http://www.mc2.umbc.edu/docs/earnest.pdf Strassen's Algorithm on the Cell Broadband Engine]''\n* ''[http://gitorious.org/intelws2010/matrix-multiplication/blobs/master/src/matmul.c Simple Strassen's algorithms implementation in C] (easy for education purposes)''\n\n{{Numerical linear algebra}}\n\n[[Category:Matrix multiplication algorithms]]"
    },
    {
      "title": "Sparse matrix",
      "url": "https://en.wikipedia.org/wiki/Sparse_matrix",
      "text": "{| class=wikitable align=right width=240px style=\"margin: 3px 0 5px 14px;\"\n| <center>'''Example of sparse matrix'''</center>\n<center><math>\\left(\\begin{smallmatrix}\n11 & 22 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 33 & 44 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 55 & 66 & 77 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 88 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 99 \\\\\n\\end{smallmatrix}\\right)</math></center>\n|-\n| <center>{{midsize|The above sparse matrix contains only 9 nonzero elements, with 26 zero elements. Its sparsity is 74%, and its density is 26%.}}</center>\n|}\n[[Image:Finite element sparse matrix.png|right|thumb|A sparse matrix obtained when solving a [[finite element method|finite element problem]] in two dimensions. The non-zero elements are shown in black.]]\n\nIn [[numerical analysis]] and [[scientific computing]], a '''sparse matrix''' or '''sparse array''' is a [[matrix (mathematics)|matrix]] in which most of the elements are zero. By contrast, if most of the elements are nonzero, then the matrix is considered '''dense'''. The number of zero-valued elements divided by the total number of elements (e.g., m × n for an m × n matrix) is called the '''sparsity''' of the matrix (which is equal to 1 minus the '''density''' of the matrix). Using those definitions, a matrix will be sparse when its sparsity is greater than 0.5.\n\nConceptually, sparsity corresponds to systems that are [[loosely coupled]]. Consider a line of balls connected by springs from one to the next: this is a sparse system as only adjacent balls are coupled. By contrast, if the same line of balls had springs connecting each ball to all other balls, the system would correspond to a dense matrix. The concept of sparsity is useful in [[combinatorics]] and application areas such as [[network theory]], which have a low density of significant data or connections.\n\nLarge sparse matrices often appear in [[scientific]] or [[engineering]] applications when solving [[partial differential equation]]s.\n\nWhen storing and manipulating sparse matrices on a [[computer]], it is beneficial and often necessary to use specialized [[algorithm]]s and [[data structure]]s that take advantage of the sparse structure of the matrix. Operations using standard dense-matrix structures and algorithms are slow and inefficient when applied to large sparse matrices as processing and [[Computer memory|memory]] are wasted on the zeroes. Sparse data is by nature more easily [[data compression|compressed]] and thus requires significantly less [[computer data storage|storage]]. Some very large sparse matrices are infeasible to manipulate using standard dense-matrix algorithms.\n\n=={{anchor|storage}} Storing a sparse matrix==\n\nA matrix is typically stored as a two-dimensional array. Each entry in the array represents an element {{math|''a''<sub>''i'',''j''</sub>}} of the matrix and is accessed by the two [[Array data structure|indices]] {{math|''i''}} and {{math|''j''}}. Conventionally, {{math|''i''}} is the row index, numbered from top to bottom, and {{math|''j''}} is the column index, numbered from left to right. For an {{math|''m'' × ''n''}} matrix, the amount of memory required to store the matrix in this format is proportional to {{math|''m'' × ''n''}} (disregarding the fact that the dimensions of the matrix also need to be stored).\n\nIn the case of a sparse matrix, substantial memory requirement reductions can be realized by storing only the non-zero entries. Depending on the number and distribution of the non-zero entries, different data structures can be used and yield huge savings in memory when compared to the basic approach. The trade-off is that accessing the individual elements becomes more complex and additional structures are needed to be able to recover the original matrix unambiguously.\n\nFormats can be divided into two groups:\n\n* Those that support efficient modification, such as DOK (Dictionary of keys), LIL (List of lists), or COO (Coordinate list). These are typically used to construct the matrices.\n* Those that support efficient access and matrix operations, such as CSR (Compressed Sparse Row) or CSC (Compressed Sparse Column).\n\n===Dictionary of keys (DOK)===\n\nDOK consists of a [[associative array|dictionary]] that maps {{math|(row, column)}}-[[ordered pair|pairs]] to the value of the elements. Elements that are missing from the dictionary are taken to be zero. The format is good for incrementally constructing a sparse matrix in random order, but poor for iterating over non-zero values in lexicographical order. One typically constructs a matrix in this format and then converts to another more efficient format for processing.<ref>See [http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.dok_matrix.html <code>scipy.sparse.dok_matrix</code>]</ref>\n\n===List of lists (LIL)===\n\nLIL stores one list per row, with each entry containing the column index and the value. Typically, these entries are kept sorted by column index for faster lookup. This is another format good for incremental matrix construction.<ref>See [http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html <code>scipy.sparse.lil_matrix</code>]</ref>\n\n.\n\n===Compressed sparse row (CSR, CRS or Yale format)===\n\nThe ''compressed sparse row'' (CSR) or ''compressed row storage'' (CRS) or Yale format represents a matrix {{math|'''M'''}} by three (one-dimensional) arrays, that respectively contain nonzero values, the extents of rows, and column indices. It is similar to COO, but compresses the row indices, hence the name. This format allows fast row access and matrix-vector multiplications ({{math|'''M'''''x''}}). The CSR format has been in use since at least the mid-1960s, with the first complete description appearing in 1967.<ref>{{cite conference |first1=Aydın |last1=Buluç |first2=Jeremy T. |last2=Fineman |first3=Matteo |last3=Frigo |first4=John R. |last4=Gilbert |first5=Charles E. |last5=Leiserson |authorlink5=Charles E. Leiserson |title=Parallel sparse matrix-vector and matrix-transpose-vector multiplication using compressed sparse blocks |year=2009 |conference=ACM Symp. on Parallelism in Algorithms and Architectures |url=http://gauss.cs.ucsb.edu/~aydin/csb2009.pdf |citeseerx=10.1.1.211.5256}}</ref>\n\nThe CSR format stores a sparse {{math|''m'' × ''n''}} matrix {{math|'''M'''}} in row form using three (one-dimensional) arrays {{math|(A, IA, JA)}}. Let {{math|NNZ}} denote the number of nonzero entries in {{math|'''M'''}}. (Note that [[Zero-based numbering|zero-based indices]] shall be used here.)\n[[File:Row_and_column_major_order.svg|thumb|upright|Illustration of row-major order compared to column-major order]]\n* The array {{math|A}} is of length {{math|NNZ}} and holds all the nonzero entries of {{math|'''M'''}} in left-to-right top-to-bottom (\"row-major\") order.\n* The array {{math|IA}} is of length {{math|''m'' + 1}}. It is defined by this recursive definition:\n**{{math|IA[0] {{=}} 0}}\n** {{math|IA[''i''] {{=}} IA[''i'' − 1] +}} (number of nonzero elements on the {{math|''i''-1}}-th row in the original matrix)\n** Thus, the first {{math|''m''}} elements of {{math|IA}} store the index into {{math|A}} of the first nonzero element in each row of {{math|'''M'''}}, and the last element {{math|IA[''m+1'']}} stores {{math|NNZ}}, the number of elements in {{math|A}}, which can be also thought of as the index in {{math|A}} of first element of a phantom row just beyond the end of the matrix {{math|'''M'''}}. The values of the {{math|''i''}}-th row of the original matrix is read from the elements {{math|A[IA[''i''<nowiki>]]</nowiki>}} to {{math|A[IA[''i'' + 1] − 1]}} (inclusive on both ends), i.e. from the start of one row to the last index just before the start of the next.<ref>[http://netlib.org/linalg/html_templates/node91.html netlib.org]</ref>\n* The third array, {{math|JA}}, contains the column index in {{math|'''M'''}} of each element of {{math|A}} and hence is of length {{math|NNZ}} as well.\n\nFor example, the matrix\n:: <math>\\begin{pmatrix}\n0 & 0 & 0 & 0 \\\\\n5 & 8 & 0 & 0 \\\\\n0 & 0 & 3 & 0 \\\\\n0 & 6 & 0 & 0 \\\\\n\\end{pmatrix}</math>\n\nis a {{math|4 × 4}} matrix with 4 nonzero elements, hence\n\n    A  = [ 5 8 3 6 ]\n    IA = [ 0 0 2 3 4 ]\n    JA = [ 0 1 2 1 ]\n\nSo, in array {{math|JA}}, the element \"{{math|5}}\" from {{math|A}} has column index {{math|0}}, \"{{math|8}}\" and \"{{math|6}}\" have index {{math|1}}, and element \"{{math|3}}\" has index {{math|2}}.\n\nIn this case the CSR representation contains 13 entries, compared to 16 in the original matrix. The CSR format saves on memory only when {{math|NNZ < (''m'' (''n'' − 1) − 1) / 2}}.\nAnother example, the matrix\n\n:<math>\\begin{pmatrix}\n10 & 20 &  0 &  0 &  0 &  0 \\\\\n 0 & 30 &  0 & 40 &  0 &  0 \\\\\n 0 &  0 & 50 & 60 & 70 &  0 \\\\\n 0 &  0 &  0 &  0 &  0 & 80 \\\\\n\\end{pmatrix}</math>\n\nis a {{math|4 × 6}} matrix (24 entries) with 8 nonzero elements, so\n\n     A = [ 10 20 30 40 50 60 70 80 ]\n    IA = [  0  2  4  7  8 ]\n    JA = [  0  1  1  3  2  3  4  5 ]   \n\nThe whole is stored as 21 entries.\n\n* {{math|IA}} splits the array {{math|A}} into rows: <code>(10, 20) (30, 40) (50, 60, 70) (80)</code>;\n* {{math|JA}} aligns values in columns: <code>(10, 20, ...) (0, 30, 0, 40, ...)(0, 0, 50, 60, 70, 0) (0, 0, 0, 0, 0, 80)</code>.\n\nNote that in this format, the first value of {{math|IA}} is always zero and the last is always {{math|NNZ}}, so they are in some sense redundant (although in programming languages where the array length needs to be explicitly stored, {{math|NNZ}} would not be redundant). Nonetheless, this does avoid the need to handle an exceptional case when computing the length of each row, as it guarantees the formula {{math|IA[''i'' + 1] − IA[''i'']}} works for any row {{math|''i''}}. Moreover, the memory cost of this redundant storage is likely insignificant for a sufficiently large matrix.\n\nThe (old and new) Yale sparse matrix formats are instances of the CSR scheme. The old Yale format works exactly as described above, with three arrays; the new format combines {{math|IA}} and {{math|JA}} into a single array and handles the diagonal of the matrix separately.<ref>{{citation |title=Sparse Matrix Multiplication Package (SMMP) |first1=Randolph E. |last1=Bank |first2=Craig C. |last2=Douglas |journal=Advances in Computational Mathematics |volume=1 |year=1993 |url=http://www.mgnet.org/~douglas/Preprints/pub0034.pdf}}</ref>\n\nFor [[Logical matrix | logical]] [[Adjacency matrix | adjacency matrices]], the data array can be omitted, as the existence of an entry in the row array is sufficient to model a binary adjacency relation.\n\nIt is likely known as the Yale format because it was proposed in the 1974 Yale Sparse Matrix Package report from Department of Computer Science at Yale University.<ref>{{cite web |url=https://apps.dtic.mil/dtic/tr/fulltext/u2/a047724.pdf |title=Yale Sparse Matrix Package |last=Eisenstat  |first=S. C. |last2=Gursky |first2=M. C. |last3=Schultz |first3=M. H. |last4=Sherman |first4=A. H. |date=April 1974 |language=English |access-date=6 April 2019 }}</ref>\n\n===Compressed sparse column (CSC or CCS)===\n\n[http://netlib.org/linalg/html_templates/node92.html#SECTION00931200000000000000 CSC] is similar to CSR except that values are read first by column, a row index is stored for each value, and column pointers are stored. For example, CSC is {{math|(val, row_ind, col_ptr)}}, where {{math|val}} is an array of the (top-to-bottom, then left-to-right) non-zero values of the matrix; {{math|row_ind}} is the row indices corresponding to the values; and, {{math|col_ptr}} is the list of {{math|val}} indexes where each column starts. The name is based on the fact that column index information is compressed relative to the COO format. One typically uses another format (LIL, DOK, COO) for construction. This format is efficient for arithmetic operations, column slicing, and matrix-vector products.  See [http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html scipy.sparse.csc_matrix].\nThis is the traditional format for specifying a sparse matrix in MATLAB (via the [https://www.mathworks.com/help/matlab/ref/sparse.html <code>sparse</code>] function).\n\n==Special structure==\n\n===Banded===\n{{main article|Band matrix}}\nAn important special type of sparse matrices is [[band matrix]], defined as follows. The [[lower bandwidth of a matrix]] {{math|'''A'''}} is the smallest number {{math|''p''}} such that the entry {{math|''a''<sub>''i'',''j''</sub>}} vanishes whenever {{math|''i'' > ''j'' + ''p''}}. Similarly, the [[Band matrix#upper bandwidth|upper bandwidth]] is the smallest number {{math|''p''}} such that {{math|''a''<sub>''i'',''j''</sub> {{=}} 0}} whenever {{math|''i'' < ''j'' − ''p''}} {{harv|Golub|Van Loan|1996|loc=§1.2.1}}. For example, a [[tridiagonal matrix]] has lower bandwidth {{math|1}} and upper bandwidth {{math|1}}. As another example, the following sparse matrix has lower and upper bandwidth both equal to 3(???? .According to the definition both must be 2). Notice that zeros are represented with dots for clarity.\n::<math>\n\\left(\n\\begin{smallmatrix}\n  X   &   X   &   X   & \\cdot & \\cdot & \\cdot & \\cdot & \\\\\n  X   &   X   & \\cdot &   X   &   X   & \\cdot & \\cdot & \\\\\n  X   & \\cdot &   X   & \\cdot &   X   & \\cdot & \\cdot & \\\\\n\\cdot &   X   & \\cdot &   X   & \\cdot &   X   & \\cdot & \\\\\n\\cdot &   X   &   X   & \\cdot &   X   &   X   &   X   & \\\\\n\\cdot & \\cdot & \\cdot &   X   &   X   &   X   & \\cdot & \\\\      \n\\cdot & \\cdot & \\cdot & \\cdot &   X   & \\cdot &   X   & \\\\              \n\\end{smallmatrix}\n\\right)\n</math>\n\nMatrices with reasonably small upper and lower bandwidth are known as band matrices and often lend themselves to simpler algorithms than general sparse matrices; or one can sometimes apply dense matrix algorithms and gain efficiency simply by looping over a reduced number of indices.\n\nBy rearranging the rows and columns of a matrix {{math|'''A'''}} it may be possible to obtain a matrix {{math|'''A'''′}} with a lower bandwidth. A number of algorithms are designed for [[Graph bandwidth|bandwidth minimization]].\n\n===Diagonal===\nA very efficient structure for an extreme case of band matrices, the [[diagonal matrix]], is to store just the entries in the [[main diagonal]] as a [[one-dimensional array]], so a diagonal {{math|''n'' × ''n''}} matrix requires only {{math|''n''}} entries.\n\n===Symmetric===\nA symmetric sparse matrix arises as the [[adjacency matrix]] of an [[undirected graph]]; it can be stored efficiently as an [[adjacency list]].\n\n==Reducing fill-in==\n\nThe ''fill-in'' of a matrix are those entries that change from an initial zero to a non-zero value during the execution of an algorithm. To reduce the memory requirements and the number of arithmetic operations used during an algorithm, it is useful to minimize the fill-in by switching rows and columns in the matrix. The [[symbolic Cholesky decomposition]] can be used to calculate the worst possible fill-in before doing the actual [[Cholesky decomposition]].\n\nThere are other methods than the [[Cholesky decomposition]] in use. Orthogonalization methods (such as QR factorization) are common, for example, when solving problems by least squares methods. While the theoretical fill-in is still the same, in practical terms the \"false non-zeros\" can be different for different methods. And symbolic versions of those algorithms can be used in the same manner as the symbolic Cholesky to compute worst case fill-in.\n\n==Solving sparse matrix equations==\n\nBoth [[Iterative method|iterative]] and direct methods exist for sparse matrix solving.\n\nIterative methods, such as [[conjugate gradient]] method and [[GMRES]] utilize fast computations of matrix-vector products <math>Ax_i</math>, where matrix <math>A</math> is sparse. The use of [[preconditioner]]s can significantly accelerate convergence of such iterative methods.\n\n==Software==\n\nSeveral software libraries support sparse matrices, and provide solvers for sparse matrix equations. The following are open-source:\n* [http://faculty.cse.tamu.edu/davis/suitesparse.html SuiteSparse], a suite of sparse matrix algorithms\n* [[ALGLIB]] is a C++ and C# library with sparse linear algebra support\n* [[Portable, Extensible Toolkit for Scientific Computation|PETSc]], a huge C library, contains many different matrix solvers.\n* [[Eigen (C++ library)|Eigen3]] is a C++ library that contains several sparse matrix solvers. However, none of them are [[Parallel computing|parallelized]].\n* [[MUMPS (software)|MUMPS]] ('''MU'''ltifrontal '''M'''assively '''P'''arallel sparse direct '''S'''olver), written in Fortran90, is a [[frontal solver]]\n* [http://pastix.gforge.inria.fr/ PaStix]\n* [http://crd-legacy.lbl.gov/~xiaoye/SuperLU/ SuperLU]\n* [[Armadillo (C++ library)|Armadillo]] provides a user-friendly C++ wrapper for [http://crd-legacy.lbl.gov/~xiaoye/SuperLU/ SuperLU]\n*[[SciPy]] provides support for several sparse matrix formats, linear algebra, and solvers\n\n==History==\n\nThe term ''sparse matrix'' was possibly coined by [[Harry Markowitz]] who triggered some pioneering work but then left the field.<ref>pp. 9,10 in [http://purl.umn.edu/107467 Oral history interview with Harry M. Markowitz]</ref>\n\n==See also==\n{{columns-list|colwidth=22em|\n* [[Matrix representation]]\n* [[Pareto principle]]\n* [[Ragged matrix]]\n* [[Skyline matrix]]\n* [[Sparse graph code]]\n* [[Sparse file]]\n* [[Harwell-Boeing file format]]\n* [[Matrix Market exchange formats]]\n}}\n\n==Notes==\n{{Reflist}}\n\n==References==\n* {{Cite book | first1=Gene H. | last1=Golub | author1-link=Gene H. Golub | first2=Charles F. | last2=Van Loan | author2-link=Charles F. Van Loan | year=1996 | title=Matrix Computations | edition=3rd | publisher=Johns Hopkins | place=Baltimore | isbn=978-0-8018-5414-9 | ref=harv}}\n* {{Cite book | last1=Stoer | first1=Josef | last2=Bulirsch | first2=Roland | title=Introduction to Numerical Analysis | publisher=[[Springer-Verlag]] | location=Berlin, New York | edition=3rd | isbn=978-0-387-95452-3 | year=2002}}\n* {{Cite book | last=Tewarson| first=Reginald P.|title=Sparse Matrices (Part of the Mathematics in Science & Engineering series)|publisher= Academic Press Inc.|date=May 1973}} (This book, by a professor at the State University of New York at Stony Book, was the first book exclusively dedicated to Sparse Matrices.  Graduate courses using this as a textbook were offered at that University in the early 1980s).\n* {{Cite web |title=Sparse Matrix Multiplication Package|first1= Randolph E.|last1= Bank|first2= Craig C.|last2= Douglas |url=http://www.mgnet.org/~douglas/Preprints/pub0034.pdf}}\n* {{Cite book |last=Pissanetzky|first= Sergio|year= 1984|title=Sparse Matrix Technology|publisher= Academic Press}}\n*{{cite journal|doi=10.1007/BF02521587|title=Reducing the profile of sparse symmetric matrices|year=1976|last1=Snay|first1=Richard A.|journal=Bulletin Géodésique|volume=50|issue=4|pages=341}} Also NOAA Technical Memorandum NOS NGS-4, National Geodetic Survey, Rockville, MD.\n\n==Further reading==\n* {{cite journal | title = A comparison of several bandwidth and profile reduction algorithms | journal = ACM Transactions on Mathematical Software | year = 1976 | volume = 2 | issue = 4 | pages = 322–330 | url = http://portal.acm.org/citation.cfm?id=355707 | doi = 10.1145/355705.355707 | last1 = Gibbs | first1 = Norman E. | last2 = Poole | first2 = William G. | last3 = Stockmeyer | first3 = Paul K.  }}\n* {{cite journal | title = Sparse matrices in MATLAB: Design and Implementation | journal = SIAM Journal on Matrix Analysis and Applications | year = 1992 | volume = 13 | issue = 1 | pages = 333–356 | url = http://citeseer.ist.psu.edu/gilbert91sparse.html | doi = 10.1137/0613024 | last1 = Gilbert | first1 = John R. | last2 = Moler | first2 = Cleve | last3 = Schreiber | first3 = Robert | citeseerx = 10.1.1.470.1054 }}\n* [http://www.cise.ufl.edu/research/sparse Sparse Matrix Algorithms Research] at the University of Florida, containing the UF sparse matrix collection.\n* [http://www.small-project.eu SMALL project] A EU-funded project on sparse models, algorithms and dictionary learning for large-scale data.\n\n{{Data structures}}\n{{Numerical linear algebra}}\n[[Category:Sparse matrices| ]]"
    },
    {
      "title": "Anti-diagonal matrix",
      "url": "https://en.wikipedia.org/wiki/Anti-diagonal_matrix",
      "text": "In [[mathematics]], an '''anti-diagonal matrix''' is a [[square matrix]] where all the entries are zero except those on the diagonal going from the lower left corner to the upper right corner (↗)<!--this description redundant and misleading; there is no ordering as the words and arrow imply-->, known as the anti-diagonal.\n\n==Formal definition==\nAn ''n''-by-''n'' matrix ''A'' is an anti-diagonal matrix if the (''i'', ''j'') element is zero <math>\\forall i,j  \\in \\left\\{1, \\ldots, n\\right\\} (i+j \\ne n+1).</math>\n\n==Example==\nAn example of an anti-diagonal matrix is\n:<math> \\begin{bmatrix} 0 & 0 & 0 & 0 & 1 \\\\ 0 & 0 & 0 & 2 & 0 \\\\ 0 & 0 & 5 & 0 & 0 \\\\ 0 & 7 & 0 & 0 & 0 \\\\ -1 & 0 & 0 & 0 & 0 \\end{bmatrix}. </math>\n\n==Properties==\nAll anti-diagonal matrices are also [[persymmetric matrix|persymmetric]].\n\nThe product of two anti-diagonal matrices is a [[diagonal matrix]]. Furthermore, the product of an anti-diagonal matrix with a diagonal matrix is anti-diagonal, as is the product of a diagonal matrix with an anti-diagonal matrix.\n\nAn anti-diagonal matrix is [[invertible]] if and only if the entries on the diagonal from the lower left corner to the upper right corner are nonzero. The inverse of any invertible anti-diagonal matrix is also anti-diagonal, as can be seen from the paragraph above. The [[determinant]] of an anti-diagonal matrix has [[absolute value]] given by the [[product (mathematics)|product]] of the entries on the diagonal from the lower left corner to the upper right corner. However, the sign of this determinant will vary because the one nonzero signed elementary product from an anti-diagonal matrix will have a different sign depending on whether the [[permutation]] related to it is odd or even:\n\n{| class=\"wikitable\"\n! Matrix size\n! Permutation for<br>nonzero elementary product of<br>anti-diagonal matrix\n! Even or odd\n! Sign of elementary product\n|-\n| 2 × 2\n| {2, 1}\n| Odd\n| −\n|-\n| 3 × 3\n| {3, 2, 1}\n| Odd\n| −\n|-\n| 4 × 4\n| {4, 3, 2, 1}\n| Even\n| +\n|-\n| 5 × 5\n| {5, 4, 3, 2, 1}\n| Even\n| +\n|-\n| 6 × 6\n| {6, 5, 4, 3, 2, 1}\n| Odd\n| −\n|-\n|}\n\nMore precisely, the sign of the elementary product needed to calculate the determinant of an anti-diagonal matrix is related to whether the corresponding [[triangular number]] is even or odd. This is because the number of inversions in the permutation for the only nonzero signed elementary product of any ''n''&nbsp;×&nbsp;''n'' anti-diagonal matrix is always equal to the ''n''th such number.\n\n== See also ==\n* [[Main diagonal]]\n* [[Exchange matrix]], an anti-diagonal matrix with 1s along the counter-diagonal.\n\n==External links==\n* {{planetmath reference|id=6964|title=Anti-diagonal matrix}}\n\n[[Category:Sparse matrices]]\n[[Category:Matrices]]"
    },
    {
      "title": "Band matrix",
      "url": "https://en.wikipedia.org/wiki/Band_matrix",
      "text": "In [[mathematics]], particularly [[Matrix (mathematics)|matrix theory]], a '''band matrix''' is a [[sparse matrix]] whose non-zero entries are confined to a diagonal ''band'', comprising the [[main diagonal]] and zero or more diagonals on either side.\n\n==Band matrix==<!-- \n[[Bandwidth (sparse matrix)]], [[matrix bandwidth]], [[bandwidth (matrix)]], [[bandwidth (matrix theory)]]\nredirect here -->\n\n===Bandwidth===\n\nFormally, consider an ''n''&times;''n'' matrix ''A''=(''a''<sub>''i,j'' </sub>). If all matrix elements are zero outside a diagonally bordered band whose range is determined by constants ''k''<sub>1</sub> and ''k''<sub>2</sub>: \n\n:<math>a_{i,j}=0 \\quad\\mbox{if}\\quad j<i-k_1 \\quad\\mbox{ or }\\quad j>i+k_2; \\quad k_1, k_2 \\ge 0.\\,</math>\n\nthen the quantities ''k''<sub>1</sub> and ''k''<sub>2</sub> are called the '''{{visible anchor|lower bandwidth}}''' and '''{{visible anchor|upper bandwidth}}''', respectively.{{sfn|Golub|Van Loan|1996|loc=§1.2.1}} The '''{{visible anchor|bandwidth}}''' of the matrix is the maximum of ''k''<sub>1</sub> and ''k''<sub>2</sub>; in other words, it is the number ''k'' such that <math> a_{i,j}=0 </math> if <math> |i-j| > k </math>.{{sfn|Atkinson|1989|p=527}}\n\n===Definition===\n\nA matrix is called a '''band matrix''' or '''banded matrix''' if its bandwidth is reasonably small.{{clarify|date=January 2017}}\n\n==Examples==\n\n*A band matrix with ''k''<sub>1</sub> = ''k''<sub>2</sub> = 0 is a [[diagonal matrix]]\n*A band matrix with ''k''<sub>1</sub> = ''k''<sub>2</sub> = 1 is a [[tridiagonal matrix]]\n*For ''k''<sub>1</sub> = ''k''<sub>2</sub> = 2 one has a [[pentadiagonal matrix]] and so on. \n*[[triangular matrix|Triangular matrices]]\n**For ''k''<sub>1</sub> = 0, ''k''<sub>2</sub> = ''n''&minus;1, one obtains the definition of an upper [[triangular matrix]]\n**similarly, for ''k''<sub>1</sub> = ''n''&minus;1, ''k''<sub>2</sub> = 0 one obtains a lower triangular matrix.\n* Upper and lower [[Hessenberg matrix|Hessenberg matrices]]\n* [[Toeplitz matrices]] when bandwidth is limited.\n* [[block-diagonal matrix|Block diagonal matrices]]\n* [[shift matrix|Shift matrices]] and [[shear matrix|shear matrices]]\n* Matrices in [[Jordan normal form]]\n* A [[skyline matrix]], also called \"variable band matrix\"{{snd}}a generalization of band matrix\n* The inverses of [[Lehmer matrix|Lehmer matrices]] are constant tridiagonal matrices, and are thus band matrices.\n\n==Applications==\nIn [[numerical analysis]], matrices from [[finite element]] or [[finite difference]] problems are often banded. Such matrices can be viewed as descriptions of the coupling between the problem variables; the banded property corresponds to the fact that variables are not coupled over arbitrarily large distances. Such matrices can be further divided{{snd}}for instance, banded matrices exist where every element in the band is nonzero. These often arise when discretising one-dimensional problems.{{Fact|date=February 2007}}\n\nProblems in higher dimensions also lead to banded matrices, in which case the band itself also tends to be sparse. For instance, a partial differential equation on a square domain (using central differences) will yield a matrix with a bandwidth equal to the [[square root]] of the matrix dimension, but inside the band only 5 diagonals are nonzero. Unfortunately, applying [[Gaussian elimination]] (or equivalently an [[LU decomposition]]) to such a matrix results in the band being filled in by many non-zero elements.\n\n==Band storage==\nBand matrices are usually stored by storing the diagonals in the band; the rest is implicitly zero.\n\nFor example, a [[tridiagonal matrix]] has bandwidth 1. The 6-by-6 matrix\n:<math>\n\\begin{bmatrix}\n B_{11} & B_{12} & 0      & \\cdots & \\cdots & 0 \\\\\n B_{21} & B_{22} & B_{23} & \\ddots & \\ddots & \\vdots \\\\\n  0     & B_{32} & B_{33} & B_{34} & \\ddots & \\vdots \\\\\n \\vdots & \\ddots & B_{43} & B_{44} & B_{45} & 0 \\\\\n \\vdots & \\ddots & \\ddots & B_{54} & B_{55} & B_{56} \\\\\n 0      & \\cdots & \\cdots & 0      & B_{65} & B_{66}\n\\end{bmatrix}\n</math>\nis stored as the 6-by-3 matrix\n:<math>\n\\begin{bmatrix}\n 0 & B_{11} & B_{12}\\\\\n B_{21} & B_{22} & B_{23} \\\\\n B_{32} & B_{33} & B_{34} \\\\\n B_{43} & B_{44} & B_{45} \\\\\n B_{54} & B_{55} & B_{56} \\\\\n B_{65} & B_{66} & 0\n\\end{bmatrix}.\n</math>\n\nA further saving is possible when the matrix is symmetric. For example, consider a symmetric 6-by-6 matrix with an upper bandwidth of 2:\n\n:<math>\n\\begin{bmatrix}\n A_{11} & A_{12} & A_{13} &   0  & \\cdots & 0 \\\\\n      & A_{22} & A_{23} & A_{24} & \\ddots & \\vdots \\\\\n      &        & A_{33} & A_{34} & A_{35} & 0 \\\\\n      &        &        & A_{44} & A_{45} & A_{46} \\\\\n      & sym    &        &        & A_{55} & A_{56} \\\\\n      &        &        &        &        & A_{66}\n\\end{bmatrix}.\n</math>\n\nThis matrix is stored as the 6-by-3 matrix:\n\n:<math>\n\\begin{bmatrix}\n A_{11} & A_{12} & A_{13} \\\\\n A_{22} & A_{23} & A_{24} \\\\\n A_{33} & A_{34} & A_{35} \\\\\n A_{44} & A_{45} & A_{46} \\\\\n A_{55} & A_{56} & 0 \\\\\n A_{66} & 0 & 0\n\\end{bmatrix}.\n</math>\n\n==Band form of sparse matrices==\nFrom a computational point of view, working with band matrices is always preferential to working with similarly dimensioned [[square matrices]]. A band matrix can be likened in complexity to a rectangular matrix whose row dimension is equal to the bandwidth of the band matrix. Thus the work involved in performing operations such as multiplication falls significantly, often leading to huge savings in terms of calculation time and [[calculation complexity|complexity]].\n\nAs sparse matrices lend themselves to more efficient computation than dense matrices, as well as in more efficient utilization of computer storage,  there has been much research focused on finding ways to minimise the bandwidth (or directly minimise the fill-in) by applying permutations to the matrix, or other such equivalence or similarity transformations.{{sfn|Davis|2006|loc=§7.7}}\n\nThe [[Cuthill–McKee algorithm]] can be used to reduce the bandwidth of a sparse [[symmetric matrix]].  There are, however, matrices for which the [[reverse Cuthill–McKee algorithm]] performs better. There are many other methods in use.\n\nThe problem of finding a representation of a matrix with minimal bandwidth by means of permutations of rows and columns is [[NP-hard]].{{sfn|Feige|2000}}\n\n==See also==\n* [[Graph bandwidth]]\n\n==Notes==\n{{reflist}}\n\n==References==\n* {{Citation | last = Atkinson | first = Kendall E. | title = An Introduction to Numerical Analysis | year = 1989 | publisher = John Wiley & Sons | isbn = 0-471-62489-6 }}.\n* {{Citation | last = Davis | first = Timothy A. | title = Direct Methods for Sparse Linear Systems | year = 2006 | publisher = Society for Industrial and Applied Mathematics | isbn = 978-0-898716-13-9 }}.\n* {{Citation | last = Feige | first = Uriel | contribution = Coping with the NP-Hardness of the Graph Bandwidth Problem | title = Algorithm Theory - SWAT 2000 | series = Lecture Notes in Computer Science | volume = 1851 | year = 2000 | pp = 129–145 | doi = 10.1007/3-540-44985-X_2 }}.\n* {{Citation | first1=Gene H. | last1=Golub | author1-link=Gene H. Golub | first2=Charles F. | last2=Van Loan | author2-link=Charles F. Van Loan | year=1996 | title=Matrix Computations | edition=3rd | publisher=Johns Hopkins | place=Baltimore | isbn=978-0-8018-5414-9 }}.\n* {{Citation|last1=Press|first1=WH|last2=Teukolsky|first2=SA|last3=Vetterling|first3=WT|last4=Flannery|first4=BP|year=2007|title=Numerical Recipes: The Art of Scientific Computing|edition=3rd|publisher=Cambridge University Press| publication-place=New York|isbn=978-0-521-88068-8|chapter=Section 2.4|chapter-url=http://apps.nrbook.com/empanel/index.html?pg=56}}.\n\n==External links==\n* [http://www.netlib.org/lapack/lug/node124.html Information pertaining to LAPACK and band matrices]\n* [http://www.netlib.org/linalg/html_templates/node89.html#SECTION00930000000000000000 A tutorial on banded matrices and other sparse matrix formats]\n\n\n[[Category:Sparse matrices]]"
    },
    {
      "title": "Bidiagonal matrix",
      "url": "https://en.wikipedia.org/wiki/Bidiagonal_matrix",
      "text": "In [[mathematics]], a '''bidiagonal matrix''' is a [[banded matrix]] with non-zero entries along the main diagonal and ''either'' the diagonal above or the diagonal below. This means there are exactly two non zero diagonals in the matrix.\n\nWhen the diagonal above the main diagonal has the non-zero entries the matrix is '''upper bidiagonal'''.  When the diagonal below the main diagonal has the non-zero entries the matrix is '''lower bidiagonal'''.\n\nFor example, the following matrix is '''upper bidiagonal''':\n:<math>\\begin{pmatrix}\n1 & 4 & 0 & 0 \\\\\n0 & 4 & 1 & 0 \\\\\n0 & 0 & 3 & 4 \\\\\n0 & 0 & 0 & 3 \\\\\n\\end{pmatrix}</math>\n\nand the following matrix is '''lower bidiagonal''':\n\n:<math>\\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n2 & 4 & 0 & 0 \\\\\n0 & 3 & 3 & 0 \\\\\n0 & 0 & 4 & 3 \\\\\n\\end{pmatrix}.</math>\n\n==Usage==\nOne variant of the [[QR algorithm]] starts with reducing a general matrix into a bidiagonal one,<ref>Bochkanov Sergey Anatolyevich. ALGLIB User Guide - General Matrix operations - Singular value decomposition . ALGLIB Project. 2010-12-11. URL:http://www.alglib.net/matrixops/general/svd.php. Accessed: 2010-12-11. (Archived by WebCite at https://www.webcitation.org/5utO4iSnR)</ref>\nand the [[Singular value decomposition]] uses this method as well.\n\n===Bidiagonalization===\n{{main|Bidiagonalization}}\n\n{{expand Section|date=January 2017}}\n\n==See also==\n* [[List of matrices]]\n* [[LAPACK]]\n* [[Hessenberg form]] The Hessenberg form is similar, but has more non zero diagonal lines than 2.\n\n==References==\n* Stewart, G. W. (2001) ''Matrix Algorithms, Volume II: Eigensystems''. Society for Industrial and Applied Mathematics. {{ISBN|0-89871-503-2}}.\n{{Reflist}}\n\n==External links==\n* [http://www.cs.utexas.edu/users/flame/pubs/flawn53.pdf High performance algorithms] for reduction to condensed (Hessenberg, tridiagonal, bidiagonal) form\n\n[[Category:Linear algebra]]\n[[Category:Sparse matrices]]\n\n{{Linear-algebra-stub}}\n{{compu-prog-stub}}"
    },
    {
      "title": "Block matrix",
      "url": "https://en.wikipedia.org/wiki/Block_matrix",
      "text": "{{Refimprove|date=December 2009}}\nIn [[mathematics]], a '''block matrix''' or a '''partitioned matrix''' is a [[matrix (mathematics)|matrix]] that is ''interpreted'' as having been broken into sections called '''blocks''' or '''submatrices'''.<ref>{{cite book |last=Eves |first=Howard |authorlink=Howard Eves |title=Elementary Matrix Theory |year=1980 |publisher=Dover |location=New York |isbn=0-486-63946-0 |page=37 |url=https://books.google.com/books?id=ayVxeUNbZRAC&lpg=PA40&dq=block%20multiplication&pg=PA37#v=onepage&q&f=false |edition=reprint |accessdate=24 April 2013 |quote=We shall find that it is sometimes convenient to subdivide a matrix into rectangular blocks of elements. This leads us to consider so-called ''partitioned'', or ''block'', ''matrices''.}}</ref> Intuitively, a matrix interpreted as a block matrix can be visualized as the original matrix with a collection of horizontal and vertical lines, which break it up, or [[Partition of a set|partition]] it, into a collection of smaller matrices.<ref>{{cite book |last=Anton |first=Howard |title=Elementary Linear Algebra |year=1994 |publisher=John Wiley |location=New York |isbn=0-471-58742-7 |page=30 |edition=7th |quote=A matrix can be subdivided or '''''partitioned''''' into smaller matrices by inserting horizontal and vertical rules between selected rows and columns.}}</ref> Any matrix may be interpreted as a block matrix in one or more ways, with each interpretation defined by how its rows and columns are partitioned.\n\nThis notion can be made more precise for an <math>n</math> by <math>m</math> matrix <math>M</math> by partitioning <math>n</math> into a collection <math>rowgroups</math>, and then partitioning <math>m</math> into a collection <math>colgroups</math>. The original matrix is then considered as the \"total\" of these groups, in the sense that the <math>(i, j)</math> entry of the original matrix corresponds in a [[Bijection|1-to-1]] way with some <math>(s, t)</math> [[offset (computer science)|offset]] entry of some <math>(x,y)</math>, where <math>x \\in \\mathit{rowgroups}</math> and <math>y \\in \\mathit{colgroups}</math>.\n\nBlock matrix algebra arises in general from [[biproduct]]s in categories of matrices.<ref>{{cite journal | last1 = Macedo | first1 = H.D. | last2 = Oliveira | first2 = J.N. | year = 2013 | title = Typing linear algebra: A biproduct-oriented approach | doi = 10.1016/j.scico.2012.07.012 | journal = Science of Computer Programming | volume = 78 | issue = 11| pages = 2160–2191 | arxiv = 1312.4818 }}</ref>\n\n==Example==\n[[File:BlockMatrix168square.png|thumb|A 168×168 element block matrix with 12×12, 12×24, 24x12, and 24×24 sub-Matrices. Non-zero elements are in blue, zero elements are grayed.]]\n\nThe matrix\n\n:<math>\\mathbf{P} = \\begin{bmatrix}\n  1 & 1 & 2 & 2 \\\\\n  1 & 1 & 2 & 2 \\\\\n  3 & 3 & 4 & 4 \\\\\n  3 & 3 & 4 & 4\n\\end{bmatrix}</math>\n\ncan be partitioned into four 2×2 blocks\n\n:<math>\n  \\mathbf{P}_{11} = \\begin{bmatrix}\n    1 & 1 \\\\\n    1 & 1\n  \\end{bmatrix},\\quad\n  \\mathbf{P}_{12} = \\begin{bmatrix}\n    2 & 2\\\\\n    2 & 2\n  \\end{bmatrix},\\quad\n  \\mathbf{P}_{21} = \\begin{bmatrix}\n    3 & 3 \\\\\n    3 & 3\n  \\end{bmatrix},\\quad\n  \\mathbf{P}_{22} = \\begin{bmatrix}\n    4 & 4 \\\\\n    4 & 4\n  \\end{bmatrix}.\n</math>\n\nThe partitioned matrix can then be written as\n\n:<math>\\mathbf{P} = \\begin{bmatrix}\n  \\mathbf{P}_{11} & \\mathbf{P}_{12} \\\\\n  \\mathbf{P}_{21} & \\mathbf{P}_{22}\n\\end{bmatrix}.</math>\n\n==Block matrix multiplication==\nIt is possible to use a block partitioned matrix product that involves only algebra on submatrices of the factors. The partitioning of the factors is not arbitrary, however, and requires \"conformable partitions\"<ref>{{cite book |last=Eves |first=Howard |authorlink=Howard Eves |title=Elementary Matrix Theory |year=1980 |publisher=Dover |location=New York |isbn=0-486-63946-0 |page=37 |url=https://books.google.com/books?id=ayVxeUNbZRAC&lpg=PA40&dq=block%20multiplication&pg=PA39#v=onepage&q&f=false |edition=reprint |accessdate=24 April 2013 |quote=A partitioning as in Theorem 1.9.4 is called a ''conformable partition'' of ''A'' and ''B''.}}</ref> between two matrices <math>A</math> and <math>B</math> such that all submatrix products that will be used are defined.<ref>{{cite book |last=Anton |first=Howard |title=Elementary Linear Algebra |year=1994 |publisher=John Wiley |location=New York |isbn=0-471-58742-7 |page=36 |edition=7th |quote=...provided the sizes of the submatrices of A and B are such that the indicated operations can be performed.}}</ref> Given an <math>(m \\times p)</math> matrix <math>\\mathbf{A}</math> with <math>q</math> row partitions and <math>s</math> column partitions\n\n:<math>\\mathbf{A} = \\begin{bmatrix}\n  \\mathbf{A}_{11} & \\mathbf{A}_{12} & \\cdots & \\mathbf{A}_{1s} \\\\\n  \\mathbf{A}_{21} & \\mathbf{A}_{22} & \\cdots & \\mathbf{A}_{2s} \\\\\n  \\vdots          & \\vdots          & \\ddots & \\vdots          \\\\\n  \\mathbf{A}_{q1} & \\mathbf{A}_{q2} & \\cdots & \\mathbf{A}_{qs}\n\\end{bmatrix}</math>\n\nand a <math>(p \\times n)</math> matrix <math>\\mathbf{B}</math> with <math>s</math> row partitions and <math>r</math> column partitions\n\n:<math>\\mathbf{B} = \\begin{bmatrix}\n  \\mathbf{B}_{11} & \\mathbf{B}_{12} & \\cdots &\\mathbf{B}_{1r} \\\\\n  \\mathbf{B}_{21} & \\mathbf{B}_{22} & \\cdots &\\mathbf{B}_{2r} \\\\\n  \\vdots          & \\vdots          & \\ddots &\\vdots          \\\\\n  \\mathbf{B}_{s1} & \\mathbf{B}_{s2} & \\cdots &\\mathbf{B}_{sr}\n\\end{bmatrix},</math>\n\nthat are compatible with the partitions of <math>A</math>, the matrix product\n\n:<math>\n  \\mathbf{C}=\\mathbf{A}\\mathbf{B}\n</math>\n\ncan be formed blockwise, yielding <math>\\mathbf{C}</math> as an <math>(m \\times n)</math> matrix with <math>q</math> row partitions and <math>r</math> column partitions. The matrices in the resulting matrix <math>\\mathbf{C}</math> are calculated by multiplying:\n\n:<math>\n  \\mathbf{C}_{q r} = \\sum^s_{i=1}\\mathbf{A}_{q i}\\mathbf{B}_{i r}. \n</math>\n\nOr, using the [[Einstein notation]] that implicitly sums over repeated indices:\n\n:<math>\n  \\mathbf{C}_{q r} = \\mathbf{A}_{q i}\\mathbf{B}_{i r}. \n</math>\n\n==Block matrix inversion{{anchor|Inversion}}==\n{{see also|Helmert–Wolf blocking}}\n\nIf a matrix is partitioned into four blocks, it can be [[invertible matrix#Blockwise inversion|inverted blockwise]] as follows:\n\n:<math>\\begin{bmatrix}\n    \\mathbf{A} & \\mathbf{B} \\\\\n    \\mathbf{C} & \\mathbf{D}\n  \\end{bmatrix}^{-1} = \\begin{bmatrix}\n     \\mathbf{A}^{-1} + \\mathbf{A}^{-1}\\mathbf{B}\\left(\\mathbf{D} - \\mathbf{CA}^{-1}\\mathbf{B}\\right)^{-1}\\mathbf{CA}^{-1} &\n      -\\mathbf{A}^{-1}\\mathbf{B}\\left(\\mathbf{D} - \\mathbf{CA}^{-1}\\mathbf{B}\\right)^{-1} \\\\\n    -\\left(\\mathbf{D} - \\mathbf{CA}^{-1}\\mathbf{B}\\right)^{-1}\\mathbf{CA}^{-1} &\n       \\left(\\mathbf{D} - \\mathbf{CA}^{-1}\\mathbf{B}\\right)^{-1}\n  \\end{bmatrix},\n</math>\n\nwhere '''A''', '''B''', '''C''' and '''D''' have arbitrary size. ('''A''' and '''D''' must be square, so that they can be inverted. Furthermore, '''A''' and {{nowrap|'''D''' − '''CA'''<sup>−1</sup>'''B'''}} must be nonsingular.<ref>\n{{cite book\n  | last = Bernstein\n  | first = Dennis\n  | title = Matrix Mathematics\n  | publisher = Princeton University Press\n  | year = 2005\n  | pages = 44\n  | isbn = 0-691-11802-7\n}}</ref>)\n\nEquivalently,\n\n:<math>\\begin{bmatrix}\n    \\mathbf{A} & \\mathbf{B} \\\\\n    \\mathbf{C} & \\mathbf{D}\n  \\end{bmatrix}^{-1} = \\begin{bmatrix}\n     \\left(\\mathbf{A} - \\mathbf{BD}^{-1}\\mathbf{C}\\right)^{-1} &\n      -\\left(\\mathbf{A}-\\mathbf{BD}^{-1}\\mathbf{C}\\right)^{-1}\\mathbf{BD}^{-1} \\\\\n    -\\mathbf{D}^{-1}\\mathbf{C}\\left(\\mathbf{A} - \\mathbf{BD}^{-1}\\mathbf{C}\\right)^{-1} &\n       \\quad \\mathbf{D}^{-1} + \\mathbf{D}^{-1}\\mathbf{C}\\left(\\mathbf{A} - \\mathbf{BD}^{-1}\\mathbf{C}\\right)^{-1}\\mathbf{BD}^{-1}\n  \\end{bmatrix}.\n</math>\n\n==Block diagonal matrices {{anchor|Block diagonal matrix}} ==\nA '''block diagonal matrix''' is a block matrix that is a [[square matrix]] such that the main-diagonal blocks are square matrices and all off-diagonal blocks are zero matrices.  A block diagonal matrix '''A''' has the form\n\n:<math>\\mathbf{A} = \\begin{bmatrix} \n  \\mathbf{A}_1 & 0             & \\cdots & 0            \\\\\n  0            & \\mathbf{A}_2  & \\cdots & 0            \\\\\n  \\vdots       & \\vdots        & \\ddots & \\vdots       \\\\\n  0            & 0             & \\cdots & \\mathbf{A}_n\n\\end{bmatrix}</math>\n\nwhere '''A'''<sub>''k''</sub> is a square matrix; in other words, matrix '''A''' is the [[direct sum of matrices|direct sum]] of '''A'''<sub>1</sub>, …, '''A'''<sub>''n''</sub>. It can also be indicated as '''A'''<sub>1</sub>&nbsp;&oplus;&nbsp;'''A'''<sub>2</sub>&nbsp;&oplus;&nbsp;…&nbsp;&oplus;&nbsp;'''A'''<sub>n</sub> &nbsp;or&nbsp; diag('''A'''<sub>1</sub>, '''A'''<sub>2</sub>, …, '''A'''<sub>n</sub>) &nbsp;(the latter being the same formalism used for a [[diagonal matrix]]). Any square matrix can trivially be considered a block diagonal matrix with only one block.\n\nFor the [[determinant]] and [[trace (linear algebra)|trace]], the following properties hold\n:<math>\\begin{align}\n               \\det\\mathbf{A} &= \\det\\mathbf{A}_1 \\times \\cdots \\times \\det\\mathbf{A}_n, \\\\\n  \\operatorname{tr}\\mathbf{A} &= \\operatorname{tr} \\mathbf{A}_1 + \\cdots + \\operatorname{tr} \\mathbf{A}_n.\\end{align}</math>\n\nThe inverse of a block diagonal matrix is another block diagonal matrix, composed of the inverse of each block, as follows:\n:<math>\\begin{pmatrix}\n    \\mathbf{A}_{1} & 0              & \\cdots & 0 \\\\\n    0              & \\mathbf{A}_{2} & \\cdots & 0 \\\\\n    \\vdots         & \\vdots         & \\ddots & \\vdots \\\\\n    0              & 0              & \\cdots & \\mathbf{A}_{n} \n  \\end{pmatrix}^{-1} = \\begin{pmatrix}\n    \\mathbf{A}_{1}^{-1} & 0                   & \\cdots & 0 \\\\\n    0                   & \\mathbf{A}_{2}^{-1} & \\cdots & 0 \\\\\n    \\vdots              & \\vdots              & \\ddots & \\vdots \\\\\n    0                   & 0                   & \\cdots & \\mathbf{A}_{n}^{-1} \n  \\end{pmatrix}.\n</math>\n\nThe eigenvalues and eigenvectors of <math>A</math> are simply those of <math>A_{1}</math> and <math>A_{2}</math> and … and <math>A_{n}</math> (combined).\n\n==Block tridiagonal matrices==\nA '''block tridiagonal matrix''' is another special block matrix, which is just like the block diagonal matrix a [[square matrix]], having square matrices (blocks) in the lower diagonal, [[main diagonal]] and upper diagonal, with all other blocks being zero matrices. It is essentially a [[tridiagonal matrix]] but has submatrices in places of scalars. A block tridiagonal matrix '''A''' has the form\n\n:<math>\\mathbf{A} = \\begin{bmatrix}\n  \\mathbf{B}_{1} & \\mathbf{C}_{1} &                &                &           \\cdots &                  &                0 \\\\\n  \\mathbf{A}_{2} & \\mathbf{B}_{2} & \\mathbf{C}_{2} &                &                  &                  &                  \\\\\n                 &         \\ddots &         \\ddots &         \\ddots &                  &                  &           \\vdots \\\\\n                 &                & \\mathbf{A}_{k} & \\mathbf{B}_{k} &   \\mathbf{C}_{k} &                  &                  \\\\\n          \\vdots &                &                &         \\ddots &           \\ddots &           \\ddots &                  \\\\\n                 &                &                &                & \\mathbf{A}_{n-1} & \\mathbf{B}_{n-1} & \\mathbf{C}_{n-1} \\\\\n               0 &                &         \\cdots &                &                  &   \\mathbf{A}_{n} &   \\mathbf{B}_{n}\n\\end{bmatrix}</math>\n\nwhere '''A'''<sub>''k''</sub>, '''B'''<sub>''k''</sub> and '''C'''<sub>''k''</sub> are square sub-matrices of the lower, main and upper diagonal respectively.\n\nBlock tridiagonal matrices are often encountered in numerical solutions of engineering problems (e.g., [[computational fluid dynamics]]). Optimized numerical methods for [[LU factorization]] are available and hence efficient solution algorithms for equation systems with a block tridiagonal matrix as coefficient matrix. The [[Thomas algorithm]], used for efficient solution of equation systems involving a [[tridiagonal matrix]] can also be applied using matrix operations to block tridiagonal matrices (see also [[Block LU decomposition]]).\n\n==Block Toeplitz matrices==\nA '''block Toeplitz matrix''' is another special block matrix, which contains blocks that are repeated down the diagonals of the matrix, as a [[Toeplitz matrix]] has elements repeated down the diagonal. The individual block matrix elements, Aij, must also be a Toeplitz matrix.\n\nA block Toeplitz matrix '''A''' has the form\n\n:<math>\\mathbf{A} = \\begin{bmatrix}\n    \\mathbf{A}_{(1,1)} &   \\mathbf{A}_{(1,2)} &                    &                    &             \\cdots & \\mathbf{A}_{(1,n-1)} &   \\mathbf{A}_{(1,n)} \\\\\n    \\mathbf{A}_{(2,1)} &   \\mathbf{A}_{(1,1)} & \\mathbf{A}_{(1,2)} &                    &                    &                      & \\mathbf{A}_{(1,n-1)} \\\\\n                       &               \\ddots &             \\ddots &             \\ddots &                    &                      &               \\vdots \\\\\n                       &                      & \\mathbf{A}_{(2,1)} & \\mathbf{A}_{(1,1)} & \\mathbf{A}_{(1,2)} &                      &                      \\\\\n                \\vdots &                      &                    &             \\ddots &             \\ddots &               \\ddots &                      \\\\\n  \\mathbf{A}_{(n-1,1)} &                      &                    &                    & \\mathbf{A}_{(2,1)} &   \\mathbf{A}_{(1,1)} &   \\mathbf{A}_{(1,2)} \\\\\n    \\mathbf{A}_{(n,1)} & \\mathbf{A}_{(n-1,1)} &             \\cdots &                    &                    &   \\mathbf{A}_{(2,1)} &   \\mathbf{A}_{(1,1)}\n\\end{bmatrix}.</math>\n\n==Direct sum==\nFor any arbitrary matrices '''A''' (of size ''m''&nbsp;×&nbsp;''n'') and '''B''' (of size ''p''&nbsp;×&nbsp;''q''), we have the '''direct sum''' of '''A''' and '''B''', denoted by '''A'''&nbsp;<math>\\oplus</math>&nbsp;'''B''' and defined as \n \n:<math>\n  \\mathbf{A} \\oplus \\mathbf{B} =\n  \\begin{bmatrix}\n    a_{11} & \\cdots & a_{1n} &      0 & \\cdots &      0 \\\\\n    \\vdots & \\cdots & \\vdots & \\vdots & \\cdots & \\vdots \\\\\n    a_{m1} & \\cdots & a_{mn} &      0 & \\cdots &      0 \\\\\n         0 & \\cdots &      0 & b_{11} & \\cdots & b_{1q} \\\\\n    \\vdots & \\cdots & \\vdots & \\vdots & \\cdots & \\vdots \\\\\n         0 & \\cdots &      0 & b_{p1} & \\cdots & b_{pq} \n  \\end{bmatrix}.\n</math>\n\nFor instance,\n\n:<math>\n  \\begin{bmatrix}\n    1 & 3 & 2 \\\\\n    2 & 3 & 1\n  \\end{bmatrix} \\oplus\n  \\begin{bmatrix}\n    1 & 6 \\\\\n    0 & 1\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    1 & 3 & 2 & 0 & 0 \\\\\n    2 & 3 & 1 & 0 & 0 \\\\\n    0 & 0 & 0 & 1 & 6 \\\\\n    0 & 0 & 0 & 0 & 1\n  \\end{bmatrix}.\n</math>\n\nThis operation generalizes naturally to arbitrary dimensioned arrays (provided that '''A''' and '''B''' have the same number of dimensions).\n\nNote that any element in the [[direct sum of vector spaces|direct sum]] of two [[vector space]]s of matrices could be represented as a direct sum of two matrices.\n\n==Direct product==\n{{main article|Kronecker product}}\n\n==Application==\nIn [[linear algebra]] terms, the use of a block matrix corresponds to having a [[linear mapping]] thought of in terms of corresponding 'bunches' of [[basis vector]]s. That again matches the idea of having distinguished direct sum decompositions of the [[domain (mathematics)|domain]] and [[range (mathematics)|range]]. It is always particularly significant if a block is the [[zero matrix]]; that carries the information that a summand maps into a sub-sum.\n\nGiven the interpretation ''via'' linear mappings and direct sums, there is a special type of block matrix that occurs for square matrices (the case ''m'' = ''n''). For those we can assume an interpretation as an [[endomorphism]] of an ''n''-dimensional space ''V''; the block structure in which the bunching of rows and columns is the same is of importance because it corresponds to having a single direct sum decomposition on ''V'' (rather than two). In that case, for example, the [[diagonal]] blocks in the obvious sense are all square. This type of structure is required to describe the [[Jordan normal form]].\n\nThis technique is used to cut down calculations of matrices, column-row expansions, and many [[computer science]] applications, including [[VLSI]] chip design. An example is the [[Strassen algorithm]] for fast [[matrix multiplication]], as well as the [[Hamming(7,4)]] encoding for error detection and recovery in data transmissions.\n\n==Notes==\n{{Reflist}}\n\n==References==\n*{{Cite web |last=Strang |first=Gilbert |authorlink=Gilbert Strang |url=http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/lecture-3-multiplication-and-inverse-matrices |title=Lecture 3: Multiplication and inverse matrices |publisher=MIT Open Course ware |at=18:30–21:10 |date=1999}}\n\n{{Linear algebra}}\n\n{{DEFAULTSORT:Block Matrix}}\n[[Category:Matrices]]\n[[Category:Sparse matrices]]"
    },
    {
      "title": "Cuthill–McKee algorithm",
      "url": "https://en.wikipedia.org/wiki/Cuthill%E2%80%93McKee_algorithm",
      "text": "[[File:can 73 cm svg.svg|thumb|Cuthill-McKee ordering of a matrix]]\n\n[[File:can 73 rcm svg.svg|thumb|RCM ordering of the same matrix]]\n\nIn [[numerical linear algebra]], the '''Cuthill–McKee algorithm''' ('''CM'''), named for Elizabeth Cuthill  and James<ref name=\"mckee\">[http://calhoun.nps.edu/bitstream/handle/10945/30131/recommendationsf00fran.pdf''Recommendations for ship hull surface representation''], page 6</ref> McKee,<ref name=\"cm\">E. Cuthill and J. McKee. [http://portal.acm.org/citation.cfm?id=805928''Reducing the bandwidth of sparse symmetric matrices''] In Proc. 24th Nat. Conf. [[Association for Computing Machinery|ACM]], pages 157–172, 1969.</ref> is an [[algorithm]] to permute a [[sparse matrix]] that has a [[symmetric matrix|symmetric]] sparsity pattern into a   [[band matrix]] form with a small [[bandwidth (matrix theory)|bandwidth]]. The '''reverse Cuthill–McKee algorithm'''  ('''RCM''')  due to Alan George  is the same algorithm but with the resulting index numbers reversed.<ref>http://ciprian-zavoianu.blogspot.ch/2009/01/project-bandwidth-reduction.html</ref> In practice this generally results in less [[Sparse matrix#Reducing fill-in|fill-in]]  than the CM ordering when Gaussian elimination is applied.<ref name=\"gl\">J. A.  George and J. W-H. Liu, Computer Solution of Large Sparse Positive Definite Systems, Prentice-Hall, 1981</ref>\n\nThe Cuthill McKee algorithm is a variant of the standard [[breadth-first search]]\nalgorithm used in graph algorithms. It starts with a peripheral node and then\ngenerates [[Level structure|levels]] <math>R_i</math> for <math>i=1, 2,..</math> until all nodes\nare exhausted. The set <math> R_{i+1} </math> is created from set <math> R_i</math>\nby listing all vertices adjacent to all nodes in <math> R_i </math>. These \nnodes are listed in increasing degree. This last detail is the only difference\nwith the breadth-first search algorithm.\n\n==Algorithm==\n\nGiven a symmetric <math>n\\times n</math> matrix we visualize the matrix as the [[adjacency matrix]] of a [[Graph (discrete mathematics)|graph]]. The Cuthill–McKee algorithm is then a relabeling of the [[vertex (graph theory)|vertices]] of the graph to reduce the bandwidth of the adjacency matrix.\n\nThe algorithm produces an ordered [[n-tuple|''n''-tuple]] <math>R</math> of vertices which is the new order of the vertices.\n\nFirst we choose a [[peripheral vertex]] (the vertex with the lowest [[Degree (graph theory)|degree]]) <math>x</math> and set <math>R := ( \\{ x \\})</math>.\n\nThen for <math>i = 1,2,\\dots</math> we iterate the following steps while <math>|R| < n</math>\n\n*Construct the adjacency set <math>A_i</math> of <math>R_i</math> (with <math>R_i</math> the ''i''-th component of <math>R</math>) and exclude the vertices we already have in <math>R</math>\n:<math>A_i := \\operatorname{Adj}(R_i) \\setminus R</math>\n*Sort <math>A_i</math> with ascending vertex order ([[Degree (graph theory)|vertex degree]]).\n*Append <math>A_i</math> to the Result set <math>R</math>.\n\nIn other words, number the vertices according to a particular [[breadth-first search|breadth-first traversal]] where neighboring vertices are visited in order from lowest to highest vertex order.\n\n==See also==\n*[[Graph bandwidth]]\n*[[Sparse matrix]]\n\n==References==\n<references />\n* [http://www.boost.org/doc/libs/1_37_0/libs/graph/doc/cuthill_mckee_ordering.html Cuthill–McKee documentation] for the [[Boost C++ Libraries]].\n* [http://ciprian-zavoianu.blogspot.com/2009/01/project-bandwidth-reduction.html A detailed description of the Cuthill–McKee algorithm].\n* [http://www.mathworks.com/help/matlab/ref/symrcm.html symrcm] MATLAB's implementation of RCM.\n* [http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csgraph.reverse_cuthill_mckee.html reverse_cuthill_mckee] RCM routine from [[SciPy]] written in [[Cython]].\n\n{{DEFAULTSORT:Cuthill-McKee algorithm}}\n[[Category:Matrix theory]]\n[[Category:Graph algorithms]]\n[[Category:Sparse matrices]]"
    },
    {
      "title": "Diagonal matrix",
      "url": "https://en.wikipedia.org/wiki/Diagonal_matrix",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|Matrix whose only nonzero elements are on its main diagonal}}\nIn [[linear algebra]], a '''diagonal matrix''' is a [[matrix (mathematics)|matrix]] in which the entries outside the [[main diagonal]] are all zero. The term usually refers to [[square matrices]]. An example of a 2-by-2 diagonal matrix is <math>\\begin{bmatrix}\n3 & 0 \\\\\n0 & 2 \\end{bmatrix}</math>; the following matrix is a 3-by-3 diagonal matrix:<math>\n  \\begin{bmatrix}\n    6 &       0 & 0       \\\\\n          0 & 7 & 0       \\\\\n          0 &       0 & 19\n  \\end{bmatrix}</math>. An [[identity matrix]] of any size, or any multiple of it, will be a diagonal matrix.\n\n==Background==\n\nAs stated above, the off-diagonal entries are zero.  That is, the matrix {{nowrap|1=''D'' = (''d''<sub>''i'',''j''</sub>)}} with ''n'' columns and ''n'' rows is diagonal if\n\n:<math>\\forall i,j \\in \\{1, 2, \\ldots, n\\}, i \\ne j \\implies d_{i,j} = 0</math>.\n\nHowever, the main diagonal entries are unrestricted.\n\n==Rectangular diagonal matrices==\n\nThe term ''diagonal matrix'' may sometimes refer to a '''{{visible anchor|rectangular diagonal matrix}}''', which is an ''m''-by-''n'' matrix with all the entries not of the form ''d''<sub>''i'',''i''</sub> being zero. For example:\n:<math>\\begin{bmatrix}\n1 & 0 & 0\\\\\n0 & 4 & 0\\\\\n0 & 0 & -3\\\\\n0 & 0 & 0\\\\\n\\end{bmatrix}</math> or <math>\\begin{bmatrix}\n1 & 0 & 0 & 0 & 0\\\\\n0 & 4 & 0& 0 & 0\\\\\n0 & 0 & -3& 0 & 0\\end{bmatrix}</math>\n\n==Symmetric diagonal matrices==\n\nThe following matrix is a symmetric diagonal matrix:\n:<math>\\begin{bmatrix}\n1 & 0 & 0\\\\\n0 & 4 & 0\\\\\n0 & 0 & -2\\end{bmatrix}</math>\n\nIf the entries are [[real numbers]] or [[complex numbers]], then it is a [[normal matrix]] as well.\n\nIn the remainder of this article we will consider only square matrices.\n\n=== Scalar matrix ===\n<!-- Linked from [[Scalar matrix]] and [[Scalar transformation]] -->\nA square diagonal matrix with all its main diagonal entries equal is a '''scalar matrix''', that is, a scalar multiple ''&lambda;I'' of the [[identity matrix]] ''I''. Its effect on a [[vector (mathematics and physics)|vector]] is [[scalar multiplication]] by ''&lambda;''. For example, a 3&times;3 scalar matrix has the form:\n:<math>\n  \\begin{bmatrix}\n    \\lambda &       0 & 0       \\\\\n          0 & \\lambda & 0       \\\\\n          0 &       0 & \\lambda\n  \\end{bmatrix} \\equiv \\lambda \\boldsymbol{I}_3\n</math>\n\nThe scalar matrices are the [[center of an algebra|center]] of the algebra of matrices: that is, they are precisely the matrices that [[commute (mathematics)|commute]] with all other square matrices of the same size. All other diagonal matrices which are not scalar only commute with  other diagonal matrices  and not with any matrix unlike scalar matrices.<ref>{{cite web |url=https://math.stackexchange.com/questions/1697991/do-diagonal-matrices-always-commute |title=do-diagonal-matrices-always-commute |author=<!--Not stated--> |date=<!--Not stated--> |publisher=stackexchange |access-date=August 4, 2018 }}</ref> Intuitively, this stems from the fact that scalar matrices are Identity matrices multiplied with scalars.\n\nFor an abstract vector space ''V'' (rather than the concrete vector space <math>K^n</math>), or more generally a [[module (ring theory)|module]] ''M'' over a [[ring (algebra)|ring]] ''R'', with the [[endomorphism algebra]] End(''M'') (algebra of linear operators on ''M'') replacing the algebra of matrices, the analog of scalar matrices are '''scalar transformations'''. Formally, scalar multiplication is a linear map, inducing a map <math>R \\to \\operatorname{End}(M),</math> (send a scalar ''&lambda;'' to the corresponding scalar transformation, multiplication by ''&lambda;'') exhibiting End(''M'') as a ''R''-[[Algebra (ring theory)|algebra]]. For vector spaces, or more generally [[free module]]s <math>M \\cong R^n</math>, for which the endomorphism algebra is isomorphic to a matrix algebra, the scalar transforms are exactly the [[center of a ring|center]] of the endomorphism algebra, and similarly invertible transforms are the center of the [[general linear group]] GL(''V''), where they are denoted by Z(''V''), follow the usual notation for the center.\n\n== Matrix operations ==\nThe operations of matrix addition and [[matrix multiplication]] are especially simple for symmetric diagonal matrices. Write {{nowrap|diag(''a''<sub>1</sub>, ..., ''a''<sub>''n''</sub>)}} for a diagonal matrix whose diagonal entries starting in the upper left corner are ''a''<sub>1</sub>, ..., ''a''<sub>''n''</sub>. Then, for addition, we have\n\n:{{nowrap|diag(''a''<sub>1</sub>, ..., ''a''<sub>''n''</sub>)}} + {{nowrap|diag(''b''<sub>1</sub>, ..., ''b''<sub>''n''</sub>)}} = {{nowrap|diag(''a''<sub>1</sub> + ''b''<sub>1</sub>, ..., ''a''<sub>''n''</sub> + ''b''<sub>''n''</sub>)}}\n\nand for [[matrix multiplication]],\n\n:{{nowrap|diag(''a''<sub>1</sub>, ..., ''a''<sub>''n''</sub>)}} &middot; {{nowrap|diag(''b''<sub>1</sub>, ..., ''b''<sub>''n''</sub>)}} = {{nowrap|diag(''a''<sub>1</sub>''b''<sub>1</sub>, ..., ''a''<sub>''n''</sub>''b''<sub>''n''</sub>)}}.\n\nThe diagonal matrix {{nowrap|diag(''a''<sub>1</sub>, ..., ''a''<sub>''n''</sub>)}} is [[invertible matrix|invertible]] [[if and only if]] the entries ''a''<sub>1</sub>, ..., ''a''<sub>''n''</sub> are all non-zero. In this case, we have\n\n:{{nowrap|diag(''a''<sub>1</sub>, ..., ''a''<sub>''n''</sub>)<sup>−1</sup>}} = {{nowrap|diag(''a''<sub>1</sub><sup>−1</sup>, ..., ''a''<sub>''n''</sub><sup>−1</sup>)}}.\n\nIn particular, the diagonal matrices form a [[subring]] of the ring of all ''n''-by-''n'' matrices.\n\nMultiplying an ''n''-by-''n'' matrix ''A'' from the ''left'' with {{nowrap|diag(''a''<sub>1</sub>, ..., ''a''<sub>''n''</sub>)}} amounts to multiplying the ''i''th ''row'' of ''A'' by ''a''<sub>''i''</sub> for all ''i''; multiplying the matrix ''A'' from the ''right'' with {{nowrap|diag(''a''<sub>1</sub>, ..., ''a''<sub>''n''</sub>)}} amounts to multiplying the ''i''th ''column'' of ''A'' by ''a''<sub>''i''</sub> for all ''i''.\n\n== Operator matrix in eigenbasis ==\n\n{{Main|Transformation_matrix#Finding the matrix of a transformation|Eigenvalues and eigenvectors|l1=Finding the matrix of a transformation}}\n\nAs explained in [[transformation matrix#Finding the matrix of a transformation|determining coefficients of operator matrix]], there is a special basis, ''e''<sub>1</sub>, ..., ''e''<sub>''n''</sub>, for which the matrix <math>A</math> takes the diagonal form. Hence, in the defining equation <math>A \\vec e_j = \\sum a_{i,j} \\vec e_i</math>, all coefficients <math>a_{i,j} </math> with ''i'' ≠ ''j'' are zero, leaving only one term per sum. The surviving diagonal elements, <math>a_{i,i}</math>, are known as '''eigenvalues''' and designated with <math>\\lambda_i</math> in the equation, which reduces to <math>A \\vec e_i = \\lambda_i \\vec e_i</math>. The resulting equation is known as '''eigenvalue equation'''<ref>{{cite book |last=Nearing |first=James |year=2010 |title=Mathematical Tools for Physics |url=http://www.physics.miami.edu/nearing/mathmethods |chapter=Chapter 7.9: Eigenvalues and Eigenvectors |chapterurl= http://www.physics.miami.edu/~nearing/mathmethods/operators.pdf |accessdate=January 1, 2012|isbn=048648212X}}</ref> and used to derive the [[characteristic polynomial]] and, further, [[eigenvalues and eigenvectors]].\n\nIn other words, the [[eigenvalue]]s of {{nowrap|diag(''λ''<sub>1</sub>, ..., ''λ''<sub>''n''</sub>)}} are ''λ''<sub>1</sub>, ..., ''λ''<sub>''n''</sub> with associated [[eigenvectors]] of ''e''<sub>1</sub>, ..., ''e''<sub>''n''</sub>.\n\n== Properties ==\n\nThe [[determinant]] of {{nowrap|diag(''a''<sub>1</sub>, ..., ''a''<sub>''n''</sub>)}} is the product ''a''<sub>1</sub>...''a''<sub>''n''</sub>.\n\nThe [[adjugate]] of a diagonal matrix is again diagonal.\n\nA square matrix is diagonal if and only if it is triangular and [[Normal matrix|normal]].\n\nAny square diagonal matrix is also a [[symmetric matrix]].\n\nA symmetric diagonal matrix can be defined as a matrix that is both [[triangular matrix|upper-]] and [[triangular matrix|lower-triangular]]. The [[identity matrix]] ''I''<sub>''n''</sub> and any square [[zero matrix]] are diagonal. A one-dimensional matrix is always diagonal.\n\n== Applications ==\nDiagonal matrices occur in many areas of linear algebra. Because of the simple description of the matrix operation and eigenvalues/eigenvectors given above, it is typically desirable to represent a given matrix or [[linear operator|linear map]] by a diagonal matrix.\n\nIn fact, a given ''n''-by-''n'' matrix ''A'' is [[similar matrix|similar]] to a diagonal matrix (meaning that there is a matrix ''X'' such that ''X''<sup>−1</sup>''AX'' is diagonal) if and only if it has ''n'' [[linearly independent]] eigenvectors. Such matrices are said to be [[diagonalizable matrix|diagonalizable]].\n\nOver the [[field (mathematics)|field]] of [[real number|real]] or [[complex number|complex]] numbers, more is true. The [[spectral theorem]] says that every [[normal matrix]] is [[matrix similarity|unitarily similar]] to a diagonal matrix (if ''AA''<sup>∗</sup> = ''A''<sup>∗</sup>''A'' then there exists a [[unitary matrix]] ''U'' such that ''UAU''<sup>∗</sup> is diagonal). Furthermore, the [[singular value decomposition]] implies that for any matrix ''A'', there exist unitary matrices ''U'' and ''V'' such that ''UAV''<sup>∗</sup> is diagonal with positive entries.\n\n== Operator theory ==\nIn [[operator theory]], particularly the study of [[PDEs]], operators are particularly easy to understand and PDEs easy to solve if the operator is diagonal with respect to the basis with which one is working; this corresponds to a [[separable partial differential equation]]. Therefore, a key technique to understanding operators is a change of coordinates–in the language of operators, an [[integral transform]]–which changes the basis to an [[eigenbasis]] of [[eigenfunction]]s: which makes the equation separable. An important example of this is the [[Fourier transform]], which diagonalizes constant coefficient differentiation operators (or more generally translation invariant operators), such as the Laplacian operator, say, in the [[heat equation]].\n\nEspecially easy are [[multiplication operator]]s, which are defined as multiplication by (the values of) a fixed function–the values of the function at each point correspond to the diagonal entries of a matrix.\n\n== See also ==\n{{colbegin}}\n* [[Anti-diagonal matrix]]\n* [[Banded matrix]]\n* [[Bidiagonal matrix]]\n* [[Diagonally dominant matrix]]\n* [[Jordan normal form]]\n* [[Multiplication operator]]\n* [[Tridiagonal matrix]]\n* [[Toeplitz matrix]]\n* [[Toral Lie algebra]]\n* [[Circulant matrix]]\n{{colend}}\n\n== Notes ==\n<references />\n\n== References ==\n* Roger A. Horn and Charles R. Johnson, ''Matrix Analysis'', Cambridge University Press, 1985. {{ISBN|0-521-30586-1}} (hardback), {{ISBN|0-521-38632-2}} (paperback).\n[[Category:Matrix normal forms]]\n[[Category:Sparse matrices]]"
    },
    {
      "title": "Generalized permutation matrix",
      "url": "https://en.wikipedia.org/wiki/Generalized_permutation_matrix",
      "text": "In [[mathematics]], a '''generalized permutation matrix''' (or '''monomial matrix''') is a [[matrix (mathematics)|matrix]] with the same nonzero pattern as a [[permutation matrix]], i.e. there is exactly one nonzero entry in each row and each column. Unlike a permutation matrix, where the nonzero entry must be 1, in a generalized permutation matrix the nonzero entry can be any nonzero value. An example of a generalized permutation matrix is\n\n:<math>\\begin{bmatrix}\n0 &  0 & 3 & 0\\\\\n0 & -2 & 0 & 0\\\\\n1 &  0 & 0 & 0\\\\\n0 &  0 & 0 & 1\\end{bmatrix}.</math>\n\n== Structure ==\nAn [[invertible matrix]] ''A'' is a generalized permutation matrix [[if and only if]] it can be written as a product of an [[invertible matrix|invertible]] [[diagonal matrix]] ''D'' and an (implicitly [[invertible matrix|invertible]]) [[permutation matrix]] ''P'': i.e.,\n\n:<math> A=DP.</math>\n\n===Group structure===\nThe set of ''n''&times;''n'' generalized permutation matrices with entries in a [[field (mathematics)|field]] ''F'' forms a [[subgroup]] of the [[general linear group]] GL(''n'',''F''), in which the group of nonsingular diagonal matrices Δ(''n'', ''F'') forms a [[normal subgroup]]. Indeed, the generalized permutation matrices are the [[normalizer]] of the diagonal matrices, meaning that the generalized permutation matrices are the ''largest'' subgroup of GL in which diagonal matrices are normal.\n\nThe abstract group of generalized permutation matrices is the [[wreath product]] of ''F''<sup>&times;</sup> and ''S''<sub>''n''</sub>. Concretely, this means that it is the [[semidirect product]] of Δ(''n'', ''F'') by the [[symmetric group]] ''S''<sub>''n''</sub>:\n:&Delta;(''n'', ''F'') &#x22C9; ''S''<sub>''n''</sub>,\nwhere ''S''<sub>''n''</sub> acts by permuting coordinates and the diagonal matrices Δ(''n'', ''F'') are isomorphic to the ''n''-fold product (''F''<sup>&times;</sup>)<sup>''n''</sup>.\n\nTo be precise, the generalized permutation matrices are a (faithful) [[linear representation]] of this abstract wreath product: a realization of the abstract group as a subgroup of matrices.\n\n===Subgroups===\n* The subgroup where all entries are 1 is exactly the [[permutation matrices]], which is isomorphic to the symmetric group.\n* The subgroup where all entries are ±1 is the [[signed permutation matrices]], which is the [[hyperoctahedral group]].\n* The subgroup where the entries are ''m''th [[roots of unity]] <math>\\mu_m</math> is isomorphic to a [[generalized symmetric group]].\n* The subgroup of diagonal matrices is abelian, normal, and a maximal abelian subgroup. The quotient group is the symmetric group, and this construction is in fact the [[Weyl group]] of the general linear group: the diagonal matrices are a [[maximal torus]] in the general linear group (and are their own centralizer), the generalized permutation matrices are the normalizer of this torus, and the quotient, <math>N(T)/Z(T) = N(T)/T \\cong S_n</math> is the Weyl group.\n\n== Properties ==\n* If a nonsingular matrix and its inverse are both [[nonnegative matrices]] (i.e. matrices with nonnegative entries), then the matrix is a permutation matrix.\n\n== Generalizations ==\nOne can generalize further by allowing the entries to lie in a ring, rather than in a field. In that case if the non-zero entries are required to be [[unit (ring theory)|units]] in the ring (invertible), one again obtains a group. On the other hand, if the non-zero entries are only required to be non-zero, but not necessarily invertible, this set of matrices forms a [[semigroup]] instead.\n\nOne may also schematically allow the non-zero entries to lie in a group ''G,'' with the understanding that matrix multiplication will only involve multiplying a single pair of group elements, not \"adding\" group elements. This is an [[abuse of notation]], since element of matrices being multiplied must allow multiplication and addition, but is suggestive notion for the (formally correct) abstract group <math>G \\wr S_n</math> (the wreath product of the group ''G'' by the symmetric group).\n\n==Signed permutation group==\n{{further|Hyperoctahedral group}}\nA '''signed permutation matrix''' is a generalized permutation matrix whose nonzero entries are ±1, and are the integer generalized permutation matrices with integer inverse.\n\n===Properties===\n* It is the [[Coxeter group]] <math>B_n</math>, and has order <math>2^nn!</math>.\n* It is the symmetry group of the [[hypercube]] and (dually) of the [[cross-polytope]].\n* Its index 2 subgroup of matrices with determinant 1 is the Coxeter group <math>D_n</math> and is the symmetry group of the [[demihypercube]].\n* It is a subgroup of the [[orthogonal group]].\n\n==Applications==\n\n===Monomial representations===\n{{main|Monomial representation}}\nMonomial matrices occur in [[representation theory]] in the context of [[monomial representation]]s. A monomial representation of a group ''G'' is a linear representation ''&rho;'' : ''G'' → GL(''n'', ''F'') of ''G'' (here ''F'' is the defining field of the representation) such that the image ''&rho;''(''G'')  is a subgroup of the group of monomial matrices.\n\n==References==\n* {{cite book | last=Joyner | first=David | title=Adventures in group theory. Rubik's cube, Merlin's machine, and other mathematical toys | edition=2nd updated and revised | location=Baltimore, MD | publisher=Johns Hopkins University Press | year=2008 | isbn=978-0-8018-9012-3 | zbl=1221.00013 }}\n\n[[Category:Matrices]]\n[[Category:Permutations]]\n[[Category:Sparse matrices]]"
    },
    {
      "title": "Heptadiagonal matrix",
      "url": "https://en.wikipedia.org/wiki/Heptadiagonal_matrix",
      "text": "{{unreferenced|date=June 2017}}\n\nIn [[linear algebra]], a '''heptadiagonal matrix''' is a [[matrix (mathematics)|matrix]] that is nearly [[diagonal matrix|diagonal]]; to be exact, it is a matrix in which the only nonzero entries are on the main diagonal, and the first three diagonals above and below it. So it is of the form   \n\n\n:<math>\n\\begin{bmatrix}\n B_{11} & B_{12} & B_{13}& B_{14} & 0      & \\cdots & 0 & 0 \\\\\n  B_{21} & B_{22} & B_{23} &B_{24}& B_{25}  & 0    & \\ddots & 0 \\\\\n   B_{31} & B_{32} & B_{33} & B_{34} & B_{35} & B_{36}    & \\ddots & \\vdots \\\\\n B_{41}  & B_{42} & B_{43} & B_{44} & B_{45} & B_{46}& B_{47}  & 0 \\\\\n0   & B_{52}& B_{53} & B_{54} & B_{55} & B_{56}& B_{57}& B_{58}  \\\\\n\\vdots  & \\ddots   & B_{63}& B_{64} & B_{65} & B_{66} & B_{67}& B_{68} \\\\\n0     & \\cdots  & 0  & B_{74}& B_{75} & B_{76} & B_{77} & B_{78} \\\\\n0    & 0   & \\cdots  &  0 & B_{85} & B_{86} & B_{87} & B_{88} \n\\end{bmatrix}\n</math>\n\nIt follows that a heptadiagonal matrix has at most <math>7n-12</math> nonzero entries, where ''n'' is the size of the matrix. Hence, heptadiagonal matrices are [[sparse matrix|sparse]]. This makes them useful in [[numerical analysis]].\n\n==See also==\n* [[Tridiagonal matrix]]\n* [[Pentadiagonal matrix]]\n\n[[Category:Sparse matrices]]"
    },
    {
      "title": "Identity matrix",
      "url": "https://en.wikipedia.org/wiki/Identity_matrix",
      "text": "{{confuse|matrix of ones|unitary matrix}}\n\nIn [[linear algebra]], the '''identity matrix''', or sometimes ambiguously called a '''unit matrix''', of size ''n'' is the ''n'' × ''n'' [[square matrix]] with ones on the [[main diagonal]] and zeros elsewhere. It is denoted by ''I''<sub>''n''</sub>, or simply by ''I'' if the size is immaterial or can be trivially determined by the context. (In some fields, such as [[quantum mechanics]], the identity matrix is denoted by a boldface one, '''1'''; otherwise it is identical to ''I''.) Less frequently, some mathematics books use ''U'' or ''E'' to represent the identity matrix, meaning \"unit matrix\"<ref>{{cite book |title=Matrix Methods for Engineering |series=Prentice-Hall International Series in Applied Mathematics |first=Louis Albert |last=Pipes |publisher=Prentice-Hall |year=1963 |page=91 |url=https://books.google.com/books?id=rJNRAAAAMAAJ&pg=PA91 }}</ref> and the German word \"Einheitsmatrix\",<ref>[http://mathworld.wolfram.com/IdentityMatrix.html \"Identity Matrix\"] on [[MathWorld]];</ref> respectively.\n\n:<math>\nI_1 = \\begin{bmatrix}\n1 \\end{bmatrix}\n,\\ \nI_2 = \\begin{bmatrix}\n1 & 0 \\\\\n0 & 1 \\end{bmatrix}\n,\\ \nI_3 = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\end{bmatrix}\n,\\ \\cdots ,\\ \nI_n = \\begin{bmatrix}\n1 & 0 & 0 & \\cdots & 0 \\\\\n0 & 1 & 0 & \\cdots & 0 \\\\\n0 & 0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 1 \\end{bmatrix}\n</math>\n\nWhen ''A'' is ''m''×''n'', it is a property of [[matrix multiplication]] that\n:<math>I_mA = AI_n = A. \\,</math>\nIn particular, the identity matrix serves as the unit of the [[ring (mathematics)|ring]] of all ''n''×''n'' matrices, and as the [[identity element]] of the [[general linear group]] GL(''n'') consisting of all [[invertible matrix|invertible]] ''n''×''n'' matrices. (The identity matrix itself is invertible, being [[Involutory matrix|its own inverse]].)\n\nWhere ''n''×''n'' matrices are used to represent [[linear transformation]]s from an ''n''-dimensional vector space to itself, ''I<sub>n</sub>'' represents the [[identity function]], regardless of the [[Basis (linear algebra)|basis]].\n\nThe ''i''th column of an identity matrix is the [[unit vector]] ''e<sub>i</sub>''.  It follows that the [[determinant]] of the identity matrix is&nbsp;1 and the [[trace (linear algebra)|trace]] is&nbsp;''n''.\n\nUsing the notation that is sometimes used to concisely describe [[diagonal matrix|diagonal matrices]], we can write:\n:<math> I_n = \\mathrm{diag}(1,1,...,1). \\,</math>\n\nIt can also be written using the [[Kronecker delta]] notation:\n:<math>(I_n)_{ij} = \\delta_{ij}. \\,</math>\n\nThe identity matrix also has the property that, when it is the product of two square matrices, the matrices can be said to be the inverse of one another.\n\nThe identity matrix is the only [[idempotent matrix]] with non-zero determinant. That is, it is the only matrix such that (a) when multiplied by itself the result is itself, and (b) all of its rows, and all of its columns, are [[linear independence|linearly independent]].\n\nThe [[Square root of a matrix|principal square root]] of an identity matrix is itself, and this is its only [[Positive-definite matrix|positive definite]] square root. However, every identity matrix with at least two rows and columns has an infinitude of symmetric square roots.<ref>Mitchell, Douglas W. \"Using Pythagorean triples to generate square roots of ''I''<sub>2</sub>\". [[The Mathematical Gazette]] 87, November 2003, 499-500.</ref>\n\n==See also==\n*[[Logical matrix|Binary matrix]]\n*[[Zero matrix]]\n*[[Unitary matrix]]\n*[[Matrix of ones]]\n*[[Square root of a 2 by 2 matrix#Identity matrix|Square root of a 2 by 2 identity matrix]]\n*[[Zero-One matrix]]\n\n==Notes==\n<references />\n\n==External links==\n*{{planetmath reference|title=Identity matrix|id=1223}}\n\n[[Category:Matrices]]\n[[Category:1 (number)]]\n[[Category:Sparse matrices]]"
    },
    {
      "title": "Pentadiagonal matrix",
      "url": "https://en.wikipedia.org/wiki/Pentadiagonal_matrix",
      "text": "In [[linear algebra]], a '''pentadiagonal matrix''' is a [[matrix (mathematics)|matrix]] that is nearly [[diagonal matrix|diagonal]]; to be exact, it is a matrix in which the only nonzero entries are on the main diagonal, and the first two diagonals above and below it. So it is of the form   \n\n:<math> \\begin{pmatrix} \n   c_1 & d_1 & e_1 & 0 & \\cdots & \\cdots & 0 \\\\ \n   b_1 & c_2 & d_2 & e_2 & \\ddots & & \\vdots \\\\\n   a_1 & b_2 & \\ddots & \\ddots & \\ddots & \\ddots & \\vdots \\\\\n   0 & a_2 & \\ddots & \\ddots & \\ddots & e_{n-3} & 0 \\\\\n   \\vdots & \\ddots & \\ddots & \\ddots & \\ddots & d_{n-2} & e_{n-2} \\\\\n   \\vdots & & \\ddots & a_{n-3} & b_{n-2} & c_{n-1} & d_{n-1} \\\\\n   0 & \\cdots & \\cdots & 0 & a_{n-2} & b_{n-1} & c_n \n\\end{pmatrix}. </math>\n\nIt follows that a pentadiagonal matrix has at most <math>5n-6</math> nonzero entries, where ''n'' is the size of the matrix. Hence, pentadiagonal matrices are [[sparse matrix|sparse]]. This makes them useful in [[numerical analysis]].\n\n==See also==\n* [[Tridiagonal matrix]]\n* [[Heptadiagonal matrix]]\n\n{{PlanetMath attribution|urlname=pentadiagonalmatrix|title=Pentadiagonal matrix}}\n\n[[Category:Sparse matrices]]"
    },
    {
      "title": "Permutation matrix",
      "url": "https://en.wikipedia.org/wiki/Permutation_matrix",
      "text": "{{Use American English|date = January 2019}}\n{{Short description|Matrices representing permutation of vector elements; with exactly one 1 per row and column}}<!--\n{| class=\"wikitable\" align=\"right\" style=\"width: 420px;\"\n!colspan=\"2\"| <math>\\pi  = \\begin{pmatrix} 0 & 1 & 2 & 3 & 4 \\\\ 1 & 3 & 0 & 2 & 4 \\end{pmatrix} = (0132)(4)</math>\n|-\n|colspan=\"2\"| [[File:5-el perm; active; only P as arrow diagram.svg|210px|center]]\n|-\n! Left [[Group action|action]] <small>([[Polish notation|prefix]])</small><br><math>M\\bigl(\\pi(i), i\\bigr) = 1</math>\n! Right action <small>([[Reverse Polish notation|postfix]])</small><br><math>M\\bigl(i, \\pi(i)\\bigr) = 1</math>\n|-\n| [[File:5-el perm; active; only P as matrix; prefix; map.svg|210px]]\n| [[File:5-el perm; active; only P as matrix; postfix; map.svg|210px]]\n|-\n| [[File:5-el perm; active; only P as matrix; prefix; cycle.svg|210px]]\n| [[File:5-el perm; active; only P as matrix; postfix; cycle.svg|210px]]\n|-\n|colspan=\"3\" style=\"font-size: 90%;\"|\nThere are two ways to assign a matrix to a permutation, corresponding to the two ways of multiplying permutations.\n\nThere are two ways to draw arrows in the chosen matrix, one similar to two-line and the other to cycle notation.\n|}\n-->\nIn [[mathematics]], particularly in [[Matrix (mathematics)|matrix theory]], a '''permutation matrix''' is a square [[binary matrix]] that has exactly one entry of 1 in each row and each column and 0s elsewhere. Each such matrix, say {{mvar|P}}, represents a  [[permutation]] of {{mvar|m}} elements and, when used to multiply another matrix, say {{mvar|A}}, results in  permuting the rows (when pre-multiplying, to form {{mvar|PA}}) or columns (when post-multiplying, to form {{mvar|AP}}) of the matrix {{mvar|A}}.\n\n== Definition ==\n\nGiven a permutation {{pi}} of ''m'' elements,\n:<math>\\pi : \\lbrace 1, \\ldots, m \\rbrace \\to \\lbrace 1, \\ldots, m \\rbrace</math>\nrepresented in two-line form by \n:<math>\\begin{pmatrix} 1 & 2 & \\cdots & m \\\\ \\pi(1) & \\pi(2) & \\cdots & \\pi(m) \\end{pmatrix},</math>\nthere are two natural ways to associate the permutation with a permutation matrix; namely, starting with the ''m'' × ''m'' [[identity matrix]], {{math|I<sub>''m''</sub>}}, either permute the columns or permute the rows, according to {{pi}}. Both methods of defining permutation matrices appear in the literature and the properties expressed in one representation can be easily converted to the other representation. This article will primarily deal with just one of these representations and the other will only be mentioned when there is a difference to be aware of.\n\nThe ''m &times; m'' permutation matrix ''P''<sub>{{pi}}</sub> = (''p''<sub>''ij''</sub>) obtained by permuting the columns of the identity matrix {{math|I<sub>''m''</sub>}}, that is, for each ''i'', ''p''<sub>''ij''</sub> = 1 if ''j'' = {{pi}}(''i'') and 0 otherwise, will be referred to as the '''column representation''' in this article.<ref>Terminology is not standard. Most authors choose one representation to be consistent with other notation they have introduced, so there is generally no need to supply a name.</ref> Since the entries in row ''i'' are all 0 except that a 1 appears in column {{pi}}(''i''), we may write\n:<math>P_\\pi = \\begin{bmatrix} \\mathbf e_{\\pi(1)} \\\\ \\mathbf e_{\\pi(2)} \\\\ \\vdots \\\\ \\mathbf e_{\\pi(m)} \\end{bmatrix},</math>\nwhere <math>\\mathbf e_j</math>, a '''standard basis vector''', denotes a row vector of length ''m'' with 1 in the ''j''th position and 0 in every other position.<ref name=Bru2>Brualdi (2006) p.2</ref>\n\nFor example, the permutation matrix ''P''<sub>{{pi}}</sub> corresponding to the permutation :<math>\\pi=\\begin{pmatrix} 1 & 2 & 3 & 4 & 5 \\\\ 1 & 4 & 2 & 5 & 3 \\end{pmatrix},</math> is\n:<math>P_\\pi \n= \n\\begin{bmatrix}\n\\mathbf{e}_{\\pi(1)} \\\\\n\\mathbf{e}_{\\pi(2)} \\\\\n\\mathbf{e}_{\\pi(3)} \\\\\n\\mathbf{e}_{\\pi(4)} \\\\\n\\mathbf{e}_{\\pi(5)} \n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf{e}_{1} \\\\\n\\mathbf{e}_{4} \\\\\n\\mathbf{e}_{2} \\\\\n\\mathbf{e}_{5} \\\\\n\\mathbf{e}_{3} \n\\end{bmatrix}\n=\n\\begin{bmatrix} \n1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0 & 0 \n\\end{bmatrix}.\n</math>\n \nObserve that the ''j''th column of the {{math|I<sub>5</sub>}} identity matrix now appears as the {{pi}}(''j'')th column of ''P''<sub>{{pi}}</sub>.\n \nThe other representation,  obtained by permuting the rows of the identity matrix {{math|I<sub>''m''</sub>}}, that is, for each ''j'', ''p''<sub>''ij''</sub> = 1 if ''i'' = {{pi}}(''j'') and 0 otherwise, will be referred to as the '''row representation'''.\n\n== Properties ==\n\nThe column representation of a permutation matrix is used throughout this section, except when otherwise indicated.\n\nMultiplying <math>P_{\\pi}</math> times a [[column vector]] '''g''' will permute the rows of the vector:\n:<math>P_\\pi \\mathbf{g} \n= \n\\begin{bmatrix}\n\\mathbf{e}_{\\pi(1)} \\\\\n\\mathbf{e}_{\\pi(2)} \\\\\n\\vdots \\\\\n\\mathbf{e}_{\\pi(n)}\n\\end{bmatrix}\n\n\\begin{bmatrix}\ng_1 \\\\\ng_2 \\\\\n\\vdots \\\\\ng_n\n\\end{bmatrix}\n=\n\\begin{bmatrix}\ng_{\\pi(1)} \\\\\ng_{\\pi(2)} \\\\\n\\vdots \\\\\ng_{\\pi(n)}\n\\end{bmatrix}.\n</math>\n\nRepeated use of this result shows that if {{mvar|M}} is an appropriately sized matrix, the product, <math>P_{\\pi} M</math> is just a permutation of the rows of {{mvar|M}}. However, observing that \n:<math>P_{\\pi} \\mathbf{e}_k^{\\mathsf T} = \\mathbf{e}_{\\pi^{-1} (k)}^{\\mathsf T}</math>\nfor each {{mvar|k}} shows that the permutation of the rows is given by {{pi}}<sup>−1</sup>. (<math>M^{\\mathsf T}</math> is the [[Transpose matrix|transpose]] of matrix {{mvar|M}}.)\n\nAs permutation matrices are [[orthogonal matrix|orthogonal matrices]] (i.e., <math>P_{\\pi}P_{\\pi}^{\\mathsf T} = I</math>), the inverse matrix exists and can be written as\n:<math>P_{\\pi}^{-1} = P_{\\pi^{-1}} = P_{\\pi}^{\\mathsf T}.</math>\n\nMultiplying a [[row vector]] '''h''' times <math>P_{\\pi}</math> will permute the columns of the vector:\n:<math>\\mathbf{h}P_\\pi\n= \n\\begin{bmatrix} h_1 \\; h_2 \\; \\dots \\; h_n \\end{bmatrix}\n\n\\begin{bmatrix}\n\\mathbf{e}_{\\pi(1)} \\\\\n\\mathbf{e}_{\\pi(2)} \\\\\n\\vdots \\\\\n\\mathbf{e}_{\\pi(n)}\n\\end{bmatrix}\n=\n\\begin{bmatrix} h_{\\pi^{-1}(1)} \\; h_{\\pi^{-1}(2)} \\; \\dots \\; h_{\\pi^{-1}(n)} \\end{bmatrix}\n</math>\n\nAgain, repeated application of this result shows that post-multiplying a matrix {{mvar|M}} by the permutation matrix {{math|''P''<sub>{{pi}}</sub>}}, that is, {{math|''M P''<sub>{{pi}}</sub>}}, results in permuting the columns of {{mvar|M}}. Notice also that\n:<math>\\mathbf{e}_k P_{\\pi} = \\mathbf{e}_{\\pi (k)}.</math>\n \nGiven two permutations {{pi}} and {{math|&sigma;}} of {{math|''m''}} elements, the corresponding permutation matrices {{math|''P''<sub>{{pi}}</sub>}} and {{math|''P''<sub>&sigma;</sub>}} acting on column vectors are composed with\n:<math>P_{\\sigma} P_{\\pi}\\, \\mathbf{g}  = P_{\\pi\\,\\circ\\,\\sigma}\\, \\mathbf{g}. </math>\nThe same matrices acting on row vectors (that is, post-multiplication) compose according to the same rule\n:<math> \\mathbf{h} P_{\\sigma} P_{\\pi}  = \\mathbf{h} P_{\\pi\\,\\circ\\,\\sigma}. </math>\nTo be clear, the above formulas use the [[prefix notation]] for permutation composition, that is,\n:<math>\\pi\\,\\circ\\,\\sigma (k) = \\pi \\left(\\sigma (k) \\right).</math>\n\nLet <math>Q_{\\pi}</math> be the permutation matrix corresponding to {{pi}} in its row representation. The properties of this representation can be determined from those of the column representation since <math>Q_{\\pi} = P_{\\pi}^{\\mathsf T} = P_{{\\pi}^{-1}}.</math> In particular, \n:<math>Q_{\\pi} \\mathbf{e}_k^{\\mathsf T} = P_{{\\pi}^{-1}} \\mathbf{e}_k^{\\mathsf T} = \\mathbf{e}_{(\\pi^{-1})^{-1} (k)}^{\\mathsf T} = \\mathbf{e}_{\\pi (k)}^{\\mathsf T}.</math>\nFrom this it follows that \n:<math>Q_{\\sigma} Q_{\\pi}\\, \\mathbf{g} = Q_{\\sigma\\,\\circ\\,\\pi}\\, \\mathbf{g}.</math>\nSimilarly,\n:<math>\\mathbf{h}\\, Q_{\\sigma} Q_{\\pi} = \\mathbf{h}\\, Q_{\\sigma\\,\\circ\\,\\pi}.</math>\n\n== Matrix group ==\nIf (1) denotes the identity permutation, then {{math|''P''<sub>(1)</sub>}} is the [[identity matrix]].\n\nLet {{math|''S<sub>n</sub>''}} denote the [[symmetric group]], or [[permutation group|group of permutations]], on {1,2,...,{{math|''n''}}}.  Since there are {{math|''n''}}{{math|}}! permutations, there are {{math|''n''}}! permutation matrices.  By the formulas above, the {{math|''n'' × ''n''}} permutation matrices form a [[Group (mathematics)|group]] under matrix multiplication with the identity matrix as the [[identity element]].\n\nThe map {{math|''S''<sub>''n''</sub> &rarr; ''A'' &sub; GL(''n'', '''Z'''<sub>2</sub>)}} is a [[faithful representation]].  Thus, {{math|1={{!}}''A''{{!}} = ''n''!}}.\n\n==Doubly stochastic matrices==\n\nA permutation matrix is itself a [[doubly stochastic matrix]], but it also plays a special role in the theory of these matrices. The [[Birkhoff–von Neumann theorem]] says that every doubly stochastic real matrix is a [[convex combination]] of permutation matrices of the same order and the permutation matrices are precisely the [[extreme point]]s of the set of doubly stochastic matrices. That is, the [[Birkhoff polytope]], the set of doubly stochastic matrices, is the [[convex hull]] of the set of permutation matrices.<ref name=Bru19>Brualdi (2006) p.19</ref>\n\n==Linear algebraic properties==\n\nThe [[Trace (linear algebra)|trace]] of a permutation matrix is the number of fixed points of the permutation. If the permutation has fixed points, so it can be written in cycle form as {{math|1=&pi; = (''a''<sub>1</sub>)(''a''<sub>2</sub>)...(''a''<sub>''k''</sub>)&sigma;}} where {{mvar|&sigma;}} has no fixed points, then {{math|'''''e'''''<sub>''a''<sub>1</sub></sub>,'''''e'''''<sub>''a''<sub>2</sub></sub>,...,'''''e'''''<sub>''a''<sub>''k''</sub></sub>}} are [[eigenvector]]s of the permutation matrix.\n\nTo calculate the eigenvalues of a permutation matrix <math>P_{\\sigma}</math>, write <math>\\sigma</math> as a product of [[cyclic permutation|cycles]], say, <math>\\sigma= C_{1}C_{2} \\cdots C_{t}</math>. Let the corresponding lengths of these cycles be <math>l_{1},l_{2}...l_{t}</math>, and let <math>R_{i}  (1 \\le i \\le t)</math> be the set of complex solutions of <math>x^{l_{i}}=1</math>. The union of all <math>R_{i}</math>s is the set of eigenvalues of the corresponding permutation matrix. The [[geometric multiplicity]] of each eigenvalue equals the number of <math>R_{i}</math>s that contain it.<ref name=J_Najnudel2010_4>J Najnudel, A Nikeghbali 2010 p.4</ref>\n\nFrom [[group theory]] we know that any permutation may be written as a product of [[transposition (mathematics)|transposition]]s. Therefore, any permutation matrix {{math|''P''}} factors as a product of row-interchanging [[elementary matrix|elementary matrices]], each having determinant &minus;1. Thus the determinant of a permutation matrix {{math|''P''}} is just the [[signature of a permutation|signature]] of the corresponding permutation.\n\n== Examples ==\n\n===Permutation of rows and columns===\nWhen a permutation matrix ''P'' is multiplied from the left with a matrix ''M'' to make ''PM'' it will permute the rows of ''M'' (here the elements of a column vector),<br>\nwhen ''P'' is multiplied from the right with ''M'' to make ''MP'' it will permute the columns of ''M'' (here the elements of a row vector):\n{|  style=\"text-align: center; width: 100%;\"\n|style=\"width:50%\"|[[File:Permutation matrix; P * column.svg|thumb|center|180px|''P'' * (1,2,3,4)<sup>T</sup> = (4,1,3,2)<sup>T</sup>]] \n|style=\"width:50%\"|[[File:Permutation matrix; row * P.svg|thumb|center|257px|(1,2,3,4) * ''P'' = (2,4,3,1)]]\n|}\n\nPermutations of rows and columns are for example reflections (see below) and cyclic permutations (see [[Circulant matrix#Properties|cyclic permutation matrix]]).\n\n{| class=\"collapsible collapsed\" style=\"width: 100%; border: 1px solid #aaaaaa;\"\n! bgcolor=\"#ccccff\"|reflections\n|-\n|\n{|  style=\"text-align: center; width: 100%;\"\n|style=\"width:50%\"|[[File:Permutation matrix; row * P^T.svg|thumb|center|257px|(1,2,3,4) * ''P''<sup>T</sup> = (4,1,3,2)]] \n|style=\"width:50%\"|[[File:Permutation matrix; P^T * column.svg|thumb|center|180px|''P''<sup>T</sup> * (1,2,3,4)<sup>T</sup> = (2,4,3,1)<sup>T</sup>]]\n|}\n\nThese arrangements of matrices are reflections of those directly above.<br>\nThis follows from the rule <math>\\left( \\mathbf{A B} \\right) ^\\mathsf{T} = \\mathbf{B}^\\mathsf{T} \\mathbf{A}^\\mathsf{T} \\,</math> &nbsp;&nbsp;&nbsp;&nbsp; (Compare: [[Transpose#Properties|Transpose]])\n|}\n\n===Permutation of rows===\nThe permutation matrix ''P''<sub>&pi;</sub> corresponding to the permutation :<math>\\pi=\\begin{pmatrix} 1 & 2 & 3 & 4 & 5 \\\\ 1 & 4 & 2 & 5 & 3 \\end{pmatrix},</math> is\n:<math>P_\\pi \n= \n\\begin{bmatrix}\n\\mathbf{e}_{\\pi(1)} \\\\\n\\mathbf{e}_{\\pi(2)} \\\\\n\\mathbf{e}_{\\pi(3)} \\\\\n\\mathbf{e}_{\\pi(4)} \\\\\n\\mathbf{e}_{\\pi(5)} \n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf{e}_{1} \\\\\n\\mathbf{e}_{4} \\\\\n\\mathbf{e}_{2} \\\\\n\\mathbf{e}_{5} \\\\\n\\mathbf{e}_{3} \n\\end{bmatrix}\n=\n\\begin{bmatrix} \n1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0 & 0 \n\\end{bmatrix}.\n</math>\n\nGiven a vector '''g''',\n:<math>P_\\pi \\mathbf{g}\n=\n\\begin{bmatrix}\n\\mathbf{e}_{\\pi(1)} \\\\\n\\mathbf{e}_{\\pi(2)} \\\\\n\\mathbf{e}_{\\pi(3)} \\\\\n\\mathbf{e}_{\\pi(4)} \\\\\n\\mathbf{e}_{\\pi(5)} \n\\end{bmatrix}\n\n\\begin{bmatrix}\ng_1 \\\\\ng_2 \\\\\ng_3 \\\\\ng_4 \\\\\ng_5\n\\end{bmatrix}\n=\n\\begin{bmatrix}\ng_1 \\\\\ng_4 \\\\\ng_2 \\\\\ng_5 \\\\\ng_3\n\\end{bmatrix}.\n</math>\n\n== Explanation ==\nA permutation matrix will always be in the form \n:<math>\\begin{bmatrix}\n\\mathbf{e}_{a_1} \\\\\n\\mathbf{e}_{a_2} \\\\\n\\vdots \\\\\n\\mathbf{e}_{a_j} \\\\\n\\end{bmatrix}</math>\nwhere '''e'''<sub>''a''<sub>''i''</sub></sub> represents the ''i''th basis vector (as a row) for '''R'''<sup>''j''</sup>, and where\n:<math>\\begin{bmatrix} \n1   & 2   & \\ldots & j \\\\\na_1 & a_2 & \\ldots & a_j\\end{bmatrix}</math>\nis the [[permutation]] form of the permutation matrix.\n\nNow, in performing matrix multiplication, one essentially forms the [[dot product]] of each row of the first matrix with each column of the second. In this instance, we will be forming the dot product of each row of this matrix with the vector of elements we want to permute. That is, for example, '''v'''= (''g''<sub>0</sub>,...,''g''<sub>5</sub>)<sup>T</sup>, \n:'''e'''<sub>''a''<sub>''i''</sub></sub>&middot;'''v'''=''g''<sub>''a''<sub>''i''</sub></sub>\n\nSo, the product of the permutation matrix with the vector '''v''' above, \nwill be a vector in the form (''g''<sub>''a''<sub>1</sub></sub>, ''g''<sub>''a''<sub>2</sub></sub>, ..., ''g''<sub>''a''<sub>''j''</sub></sub>), and that this then is a permutation of '''v''' since we have said that the permutation form is  \n:<math>\\begin{pmatrix} \n1   & 2   & \\ldots & j \\\\\na_1 & a_2 & \\ldots & a_j\\end{pmatrix}.</math>\nSo, permutation matrices do indeed permute the order of elements in vectors multiplied with them.\n\n== See also ==\n* [[Alternating sign matrix]]\n* [[Generalized permutation matrix]]\n\n==References==\n{{reflist}}\n* {{cite book | last=Brualdi | first=Richard A. | title=Combinatorial matrix classes | series=Encyclopedia of Mathematics and Its Applications | volume=108 | location=Cambridge | publisher=[[Cambridge University Press]] | year=2006 | isbn=0-521-86565-4 | zbl=1106.05001 }}\n*{{Citation\n| last1=Joseph\n| first1=Najnudel\n| last2=Ashkan\n| first2=Nikeghbali\n| title=The Distribution of Eigenvalues of Randomized Permutation Matrices\n| url=https://arxiv.org/pdf/1005.0402.pdf\n| year=2010 }}\n\n[[Category:Matrices]]\n[[Category:Permutations]]\n[[Category:Sparse matrices]]"
    },
    {
      "title": "Reverse Cuthill-McKee algorithm",
      "url": "https://en.wikipedia.org/wiki/Reverse_Cuthill-McKee_algorithm",
      "text": "#REDIRECT [[Cuthill–McKee algorithm]]\n\n[[Category:Sparse matrices]]"
    },
    {
      "title": "Shear matrix",
      "url": "https://en.wikipedia.org/wiki/Shear_matrix",
      "text": "{{Refimprove|date=December 2013}}\nIn [[mathematics]], a '''shear matrix''' or '''transvection''' is an [[elementary matrix]] that represents the [[Elementary row operations#Row-addition transformations|addition]] of a multiple of one row or column to another. Such a matrix may be derived by taking the [[identity matrix]] and replacing one of the zero elements with a non-zero value.\n\nA typical shear matrix is shown below:\n\n:<math>S=\n\\begin{pmatrix}\n1 & 0 & 0 & \\lambda & 0 \\\\\n0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 1\n\\end{pmatrix}.\n</math>\n\nThe name ''shear'' reflects the fact that the matrix represents a [[shear mapping|shear transformation]]. Geometrically, such a transformation takes pairs of points in a linear space, that are purely axially separated along the axis whose row in the matrix contains the shear element, and effectively replaces those pairs by pairs whose separation is no longer purely axial but has two vector components. Thus, the shear axis is always an [[eigenvector]] of ''S''.\n\nA shear parallel to the ''x'' axis results in <math>x' = x + \\lambda y</math> and <math>y' = y</math>. In matrix form:\n\n::<math>\n\\begin{pmatrix} x' \\\\ y' \\end{pmatrix} = \n\\begin{pmatrix}\n1 & \\lambda \\\\\n0 & 1\n\\end{pmatrix}\n\\begin{pmatrix} x \\\\ y \\end{pmatrix}.\n</math>\n\nSimilarly, a shear parallel to the ''y'' axis has <math>x' = x</math> and <math>y' = y + \\lambda x</math>. In matrix form:\n\n::<math>\n\\begin{pmatrix}x' \\\\ y' \\end{pmatrix} =\n\\begin{pmatrix}\n1 & 0 \\\\\n\\lambda & 1\n\\end{pmatrix}\n\\begin{pmatrix} x \\\\ y \\end{pmatrix}.\n</math>\n\nClearly the determinant will always be 1, as no matter where the shear element is placed, it will be a member of a skew-diagonal that also contains zero elements (as all skew-diagonals have length at least two) hence its product will remain zero and won't contribute to the determinant. Thus every shear matrix has an inverse, and the inverse is simply a shear matrix with the shear element negated, representing a shear transformation in the opposite direction. In fact, this is part of an easily derived more general result: if ''S'' is a shear matrix with shear element <math>\\lambda</math>, then ''S<sup>n</sup>'' is a shear matrix whose shear element is simply ''n''<math>\\lambda</math>. Hence, raising a shear matrix to a power ''n'' multiplies its [[shear mapping#Definition|shear factor]] by ''n''.\n\n==Properties==\nIf ''S'' is an ''n×n'' shear matrix, then:\n* ''S'' has rank ''n'' and therefore is invertible\n* ''1'' is the only [[eigenvalue]] of ''S'', so det ''S'' = 1 and trace ''S'' = ''n''\n* the [[eigenspace]] of ''S'' has ''n-1'' dimensions.\n* ''S'' is asymmetric\n* ''S'' may be made into a [[block matrix]] by at most 1 column interchange and 1 row interchange operation\n* the [[area (geometry)|area]], [[volume (geometry)|volume]], or any higher order interior capacity of a [[polytope]] is invariant under the shear transformation of the polytope's vertices.\n\n==Applications==\n* Shear matrices are often used in [[computer graphics]].<ref>{{harvtxt|Foley|van Dam|Feiner|Hughes|1991|pp=207–208,216–217}}</ref>\n\n==See also==\n*[[Transformation matrix]]\n\n==Notes==\n<references/>\n\n==References==\n* {{ citation | first1 = James D. | last1 = Foley | first2 = Andries | last2 = van Dam | first3 = Steven K. | last3 = Feiner | first4 = John F. | last4 = Hughes | year = 1991 | isbn = 0-201-12110-7 | title =  Computer Graphics: Principles and Practice | edition = 2nd | publisher = [[Addison-Wesley]] | location = Reading }}\n\n{{Computer graphics}}\n{{Matrix classes}}\n\n{{DEFAULTSORT:Shear Matrix}}\n[[Category:Matrices]]\n[[Category:Linear algebra]]\n[[Category:Sparse matrices]]"
    },
    {
      "title": "Shift matrix",
      "url": "https://en.wikipedia.org/wiki/Shift_matrix",
      "text": "In [[mathematics]], a '''shift matrix''' is a [[binary matrix]] with ones only on the [[superdiagonal]] or [[subdiagonal]], and zeroes elsewhere. A shift matrix ''U'' with ones on the superdiagonal is an '''upper shift matrix'''.\nThe alternative subdiagonal matrix ''L'' is unsurprisingly known as a '''lower shift matrix'''. The (''i'',''j''):th component of ''U'' and ''L'' are\n:<math> U_{ij} = \\delta_{i+1,j}, \\quad L_{ij} = \\delta_{i,j+1},</math>\nwhere <math>\\delta_{ij}</math> is the [[Kronecker delta]] symbol.\n\nFor example, the ''5&times;5'' shift matrices are\n::<math>\nU_5=\\begin{pmatrix}\n0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0 & 0\n\\end{pmatrix} \\quad\nL_5=\\begin{pmatrix}\n0 & 0 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0\n\\end{pmatrix}.\n</math>\n\nClearly, the [[matrix transpose|transpose]] of a lower shift matrix is an upper shift matrix and vice versa.\n\nAs a linear transformation, a lower shift matrix shifts the components of a column vector one position down, with a zero appearing in the first position.  An upper shift matrix shifts the components of a column vector one position up, with a zero appearing in the last position.<ref>{{harvtxt|Beauregard|Fraleigh|1973|p=312}}</ref>\n\nPremultiplying a matrix ''A'' by a lower shift matrix results in the elements of ''A'' being shifted downward by one position, with zeroes appearing in the top row. Postmultiplication by a lower shift matrix results in a shift left.\nSimilar operations involving an upper shift matrix result in the opposite shift.\n\nClearly all shift matrices are [[nilpotent]]; an ''n'' by ''n'' shift matrix ''S'' becomes the [[null matrix]] when raised to the power of its dimension ''n''.\n\n==Properties==\nLet ''L'' and ''U'' be the ''n'' by ''n'' lower and upper shift matrices, respectively. The following properties hold for both ''U'' and ''L''.\nLet us therefore only list the properties for ''U'':\n* [[determinant|det]](''U'') = 0\n* [[Trace (linear algebra)|trace]](''U'') = 0\n* [[Rank (linear algebra)|rank]](''U'') = ''n''&minus;1\n* The [[characteristic polynomial]]s of ''U'' is\n:<math>p_U(\\lambda) = (-1)^n\\lambda^n.</math>\n* ''U''<sup>''n''</sup> = 0. This follows from the previous property by the [[Cayley–Hamilton theorem]].\n* The [[Permanent (mathematics)|permanent]] of ''U'' is ''0''.\n\nThe following properties show how ''U'' and ''L'' are related:\n* ''L''<sup>T</sup> = ''U''; ''U''<sup>T</sup> = L\n*The [[null space]]s of ''U'' and ''L'' are\n:<math> N(U) = \\operatorname{span}\\{ (1,0,\\ldots, 0)^T \\}, </math>\n:<math> N(L) = \\operatorname{span}\\{ (0,\\ldots, 0, 1)^T \\}.</math>\n* The [[spectral theorem|spectrum]] of ''U'' and ''L'' is <math>\\{0\\}</math>. The [[algebraic multiplicity]] of ''0'' is ''n'', and its [[geometric multiplicity]] is ''1''. From the expressions for the null spaces, it follows that (up to a scaling) the only eigenvector for ''U'' is <math>(1,0,\\ldots, 0)^T</math>, and the only eigenvector for ''L'' is <math>(0,\\ldots, 0,1)^T</math>.\n* For ''LU'' and ''UL'' we have\n:<math>UL = I - \\operatorname{diag}(0,\\ldots, 0,1),</math>\n:<math>LU = I - \\operatorname{diag}(1,0,\\ldots, 0).</math>\n:These matrices are both idempotent, symmetric, and have the same rank as ''U'' and ''L''\n\n* ''L''<sup>''n-a''</sup>''U''<sup>''n-a''</sup> + ''L''<sup>''a''</sup>''U''<sup>''a''</sup> = ''U''<sup>''n-a''</sup>''L''<sup>''n-a''</sup> + ''U''<sup>''a''</sup>''L''<sup>''a''</sup> = ''I'' (the [[identity matrix]]), for any integer ''a'' between 0 and ''n'' inclusive.\n\nIf ''N'' is any [[nilpotent matrix]], then ''N'' is [[matrix similarity|similar]] to a [[block diagonal matrix]] of the form\n:<math> \\begin{pmatrix} \n   S_1 & 0 & \\ldots & 0 \\\\ \n   0 & S_2 & \\ldots & 0 \\\\\n   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n   0 & 0 & \\ldots & S_r \n\\end{pmatrix} </math>\nwhere each of the blocks ''S''<sub>1</sub>,&nbsp;''S''<sub>2</sub>,&nbsp;...,&nbsp;''S''<sub>''r''</sub> is a shift matrix (possibly of different sizes).<ref>{{harvtxt|Beauregard|Fraleigh|1973|pp=312,313}}</ref><ref>{{harvtxt|Herstein|1964|p=250}}</ref>\n\n==Examples==\n\n::<math>\nS=\\begin{pmatrix}\n0 & 0 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0\n\\end{pmatrix}; \\quad A=\\begin{pmatrix}\n1 & 1 & 1 & 1 & 1 \\\\\n1 & 2 & 2 & 2 & 1 \\\\\n1 & 2 & 3 & 2 & 1 \\\\\n1 & 2 & 2 & 2 & 1 \\\\\n1 & 1 & 1 & 1 & 1\n\\end{pmatrix}.</math>\n\nThen <math>\nSA=\\begin{pmatrix}\n0 & 0 & 0 & 0 & 0 \\\\\n1 & 1 & 1 & 1 & 1 \\\\\n1 & 2 & 2 & 2 & 1 \\\\\n1 & 2 & 3 & 2 & 1 \\\\\n1 & 2 & 2 & 2 & 1\n\\end{pmatrix}; \\quad AS=\\begin{pmatrix}\n1 & 1 & 1 & 1 & 0 \\\\\n2 & 2 & 2 & 1 & 0 \\\\\n2 & 3 & 2 & 1 & 0 \\\\\n2 & 2 & 2 & 1 & 0 \\\\\n1 & 1 & 1 & 1 & 0\n\\end{pmatrix}.</math>\n\nClearly there are many possible [[permutations]]. For example, <math>S^{T}AS</math> is equal to the matrix ''A'' shifted up and left along the main diagonal.\n\n:::::<math>\nS^{T}AS=\\begin{pmatrix}\n2 & 2 & 2 & 1 & 0 \\\\\n2 & 3 & 2 & 1 & 0 \\\\\n2 & 2 & 2 & 1 & 0 \\\\\n1 & 1 & 1 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0\n\\end{pmatrix}.</math>\n\n==See also==\n* [[Nilpotent matrix]]\n\n==Notes==\n<references/>\n\n==References==\n* {{ citation | first1 = Raymond A. | last1 = Beauregard | first2 = John B. | last2 = Fraleigh | year = 1973 | isbn = 0-395-14017-X | title = A First Course In Linear Algebra: with Optional Introduction to Groups, Rings, and Fields | publisher = [[Houghton Mifflin Co.]] | location = Boston }}\n* {{ citation | first1 = I. N. | last1 = Herstein | year = 1964 | isbn = 978-1114541016 | title = Topics In Algebra | publisher = [[Blaisdell Publishing Company]] | location = Waltham }}\n\n==External links==\n*[http://www.ee.imperial.ac.uk/hp/staff/dmb/matrix/special.html#Shift_Matrix Shift Matrix - entry in the Matrix Reference Manual]\n\n[[Category:Matrices]]\n[[Category:Sparse matrices]]"
    },
    {
      "title": "Single-entry matrix",
      "url": "https://en.wikipedia.org/wiki/Single-entry_matrix",
      "text": "In [[mathematics]] a '''single-entry matrix''' is a [[matrix (mathematics)|matrix]] where a single element is one and the rest of the elements are zero,<ref>{{Cite web\n |author1=Kaare Brandt Petersen  |author2=Michael Syskind Pedersen\n |lastauthoramp=yes | title = The Matrix Cookbook\n | date = 2008-02-16\n | url = http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf\n}}</ref><ref>{{Cite journal\n | author = Shohei Shimizu, Patrick O. Hoyer, Aapo Hyvärinen & Antti Kerminen\n | title = A Linear Non-Gaussian Acyclic Model for Causal Discovery\n | journal = [[Journal of Machine Learning Research]]\n | volume = 7\n | year = 2006\n | pages = 2003&ndash;2030\n | url = http://www.cs.helsinki.fi/group/neuroinf/lingam/JMLR06.pdf\n}}</ref> e.g.,\n:<math>\\mathbf{J}^{23} = \\left[\\begin{matrix}\n0 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{matrix}\\right].</math>\n\nIt is a specific type of a [[sparse matrix]]. The single-entry matrix can be regarded a row-selector when it is multiplied on the left side of the matrix, e.g.:\n\n: <math>\n \\mathbf{J}^{23}\\mathbf{A} = \\left[ \\begin{matrix} 0 & 0& 0 \\\\ a_{31} & a_{32} & a_{33} \\\\ 0 & 0 & 0 \\end{matrix}\\right].\n</math>\n\nAlternatively, a column-selector when multiplied on the right side:\n\n: <math>\n \\mathbf{A}\\mathbf{J}^{23} = \\left[ \\begin{matrix} 0 & 0 & a_{12} \\\\ 0 & 0 & a_{22} \\\\ 0 & 0 & a_{32}  \\end{matrix}\\right].\n</math>\n\nThe name, single-entry matrix, is not common, but seen in a few works.<ref>Examples:\n\n* {{cite web\n | title = Distributed Gain Matrix Optimization in Non-Regenerative MIMO Relay Networks\n | url = http://www.nari.ee.ethz.ch/wireless/pubs/files/asilomar09r.pdf\n}}\n* {{Cite web\n | author = Marcel Blattner\n | title = B-Rank: A top N Recommendation Algorithm\n | url = https://arxiv.org/pdf/0908.2741.pdf\n}}</ref>\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Single-Entry Matrix}}\n[[Category:Sparse matrices]]\n\n\n{{Linear-algebra-stub}}"
    },
    {
      "title": "Skyline matrix",
      "url": "https://en.wikipedia.org/wiki/Skyline_matrix",
      "text": "In [[scientific computing]], '''skyline matrix storage''', or SKS, or a '''variable band matrix storage''', or '''envelope storage scheme'''<ref>{{Citation|title=Fundamentals of matrix computations|last=Watkins|first=David S.|publisher=John Wiley & Sons, Inc.|place=New York|year=2002|edition=Second|page=60|isbn=0-471-21394-2 |url=https://books.google.com/books?id=xi5omWiQ-3kC&lpg=PA60&ots=KiXRfu0zfe&dq=%22envelope%20storage%22%20matrix&pg=PA60#v=onepage&q=%22envelope%20storage%22%20matrix&f=false}}</ref> is a form of a [[sparse matrix]] storage format matrix that reduces the storage requirement of a matrix more than [[band matrix|banded storage]]. In banded storage, all entries within a fixed distance from the diagonal (called half-bandwidth) are stored. In column-oriented skyline storage, only the entries from the first nonzero entry to the last nonzero entry in each column are stored. There is also row oriented skyline storage, and, for symmetric matrices, only one triangle is usually stored.<ref>{{Citation|title=Templates for the solution of linear systems|last1=Barrett|first1=Richard|last2=Berry|last3=Chan|last4=Demmel|last5=Donato|last6=Dongarra|last7=Eijkout|last8=Pozo|last9=Romine|last10=Van der Vorst|chapter=Skyline Storage (SKS)|publisher=SIAM|year=1994|isbn=0-89871-328-5| chapterurl=http://www.netlib.org/linalg/html_templates/node96.html|url=http://www.netlib.org/linalg/html_templates/Templates.html}}</ref>\n \nSkyline storage has become very popular in the [[finite element]] codes for [[structural mechanics]], because the skyline is preserved by [[Cholesky decomposition]] (a method of solving systems of [[linear equations]] with a symmetric, [[positive-definite matrix]]; all [[Sparse_matrix#Reducing_fill-in|fill-in]] falls within the skyline), and systems of equations from finite elements have a relatively small skyline. In addition, the effort of coding skyline Cholesky<ref name=\"George\">{{Citation|title=Computer solution of large sparse positive definite systems|last1=George|first1=Alan|last2=Liu|first2=Joseph W. H.|publisher=Prentice-Hall Inc.|year=1981|isbn=0-13-165274-5}}. The book also contains the description and source code of simple sparse matrix routines, still useful even if long superseded.</ref> is about same as for Cholesky for banded matrices (available for [[banded matrix|banded matrices]], e.g. in [[LAPACK]]; for a prototype skyline code, see <ref name=\"George\"/>). \n\nBefore storing a matrix in skyline format, the rows and columns are typically renumbered to reduce the size of the skyline (the number of nonzero entries stored) and to decrease the number of operations in the skyline Cholesky algorithm. The same heuristic renumbering algorithm that reduce the bandwidth are also used to reduce the skyline. The basic and one of the earliest algorithms to do that is [[reverse Cuthill–McKee algorithm]]. \n\nHowever, skyline storage is not as popular for very large systems (many millions of equations) because skyline Cholesky is not so easily adapted for [[massively parallel computing]], and general sparse methods,<ref>{{Citation|last1=Duff|first1=Iain S.|last2=Erisman|first2=Albert M.|last3=Reid|first3=John K.|title=Direct methods for sparse matrices|publisher=Oxford University Press|year=1986|isbn=0-19-853408-6}}</ref> which store only the nonzero entries of the matrix, become more efficient for very large problems due to much less fill-in. \n\n== See also ==\n\n*[[Sparse matrix]]\n*[[Band matrix]]\n*[[Frontal solver]]\n\n== References ==\n{{reflist}}\n\n\n[[Category:Sparse matrices]]"
    },
    {
      "title": "Sparse matrix-vector multiplication",
      "url": "https://en.wikipedia.org/wiki/Sparse_matrix-vector_multiplication",
      "text": "'''Sparse matrix-vector multiplication (SpMV)''' of the form <math>y=Ax</math> is a widely used computational kernel existing in many scientific applications.\nThe input matrix <math>A</math> is [[sparse matrix|sparse]]. The input vector <math>x</math> and the output vector <math>y</math> are dense.\nIn case of repeated <math>y=Ax</math> operation involving the same input matrix <math>A</math> but possibly changing numerical values of its elements, <math>A</math> can be preprocessed to reduce both the parallel and sequential run time of the SpMV kernel.<ref>{{cite web | url=http://epubs.siam.org/doi/abs/10.1137/100813956 | title=Hypergraph Partitioning Based Models and Methods for Exploiting Cache Locality in Sparse Matrix-Vector Multiplication | accessdate=13 April 2014}}</ref>\n\n==References==\n{{Reflist}}\n\n[[Category:Sparse matrices]]"
    },
    {
      "title": "Tridiagonal matrix",
      "url": "https://en.wikipedia.org/wiki/Tridiagonal_matrix",
      "text": "In [[linear algebra]], a ''tridiagonal matrix'' is a [[band matrix]] that has nonzero elements on the [[main diagonal]], the first diagonal below this, and the first diagonal above the main diagonal only.\n\nFor example, the following matrix is tridiagonal:\n:<math>\\begin{pmatrix}\n1 & 4 & 0 & 0 \\\\\n3 & 4 & 1 & 0 \\\\\n0 & 2 & 3 & 4 \\\\\n0 & 0 & 1 & 3 \\\\\n\\end{pmatrix}.</math>\n\nThe [[determinant]] of a tridiagonal matrix is given by the '''[[Continuant (mathematics)|continuant]]''' of its elements.<ref>{{cite book | author=Thomas Muir | authorlink=Thomas Muir (mathematician) | title=A treatise on the theory of determinants | year=1960 | publisher=[[Dover Publications]] | pages=516–525 }}</ref>\n\nAn [[orthogonal transformation]] of a symmetric (or Hermitian) matrix to tridiagonal form can be done with the [[Lanczos algorithm]].\n\n==Properties==\nA tridiagonal matrix is a matrix that is both upper and lower [[Hessenberg matrix]].<ref>{{cite book|first1=Roger A.| last1=Horn | first2=Charles R. | last2=Johnson | title = Matrix Analysis | publisher= Cambridge University Press |year= 1985 |isbn = 0521386322 | page=28}}</ref> In particular, a tridiagonal matrix is a [[direct sum]] of ''p'' 1-by-1 and ''q'' 2-by-2 matrices such that ''p'' + ''q''/2 = ''n'' — the dimension of the tridiagonal. Although a general tridiagonal matrix is not necessarily [[Symmetric matrix|symmetric]] or [[Hermitian matrix|Hermitian]], many of those that arise when solving linear algebra problems have one of these properties. Furthermore, if a real tridiagonal matrix ''A'' satisfies ''a''<sub>''k'',''k''+1</sub> ''a''<sub>''k''+1,''k''</sub> > 0 for all ''k'', so that the signs of its entries are symmetric, then it is [[similar (linear algebra)|similar]] to a Hermitian matrix, by a diagonal change of basis matrix. Hence, its [[eigenvalue]]s are real. If we replace the strict inequality by ''a''<sub>''k'',''k''+1</sub> ''a''<sub>''k''+1,''k''</sub> &ge; 0, then by continuity, the eigenvalues are still guaranteed to be real, but the matrix need no longer be similar to a Hermitian matrix.<ref>Horn & Johnson, page 174</ref>\n\nThe [[Set (mathematics)|set]] of all ''n &times; n'' tridiagonal matrices forms a ''3n-2''\n[[dimensional]] [[vector space]].\n\nMany linear algebra [[algorithm]]s require significantly less [[Computational complexity theory|computational effort]] when applied to diagonal matrices, and this improvement often carries over to tridiagonal matrices as well.\n\n===Determinant===\n{{Main|continuant (mathematics)}}\nThe [[determinant]] of a tridiagonal matrix ''A'' of order ''n'' can be computed from a three-term [[recurrence relation]].<ref>{{Cite journal | last1 = El-Mikkawy | first1 = M. E. A. | title = On the inverse of a general tridiagonal matrix | doi = 10.1016/S0096-3003(03)00298-4 | journal = Applied Mathematics and Computation | volume = 150 | issue = 3 | pages = 669–679 | year = 2004 | pmid = | pmc = }}</ref> Write ''f''<sub>1</sub>&nbsp;=&nbsp;|''a''<sub>1</sub>|&nbsp;=&nbsp;''a''<sub>1</sub> (i.e., ''f''<sub>1</sub> is the determinant of the 1 by 1 matrix consisting only of ''a''<sub>1</sub>), and let\n\n:<math>f_n = \\begin{vmatrix}\na_1 & b_1 \\\\\nc_1 & a_2 & b_2 \\\\\n& c_2 & \\ddots & \\ddots \\\\\n& & \\ddots & \\ddots & b_{n-1} \\\\\n& & & c_{n-1} & a_n\n\\end{vmatrix}.</math>\n\nThe sequence (''f''<sub>''i''</sub>) is called the [[continuant (mathematics)|continuant]] and satisfies the recurrence relation\n\n:<math>f_n = a_n f_{n-1} - c_{n-1}b_{n-1}f_{n-2}</math>\n\nwith initial values ''f''<sub>0</sub>&nbsp;=&nbsp;1 and ''f''<sub>−1</sub>&nbsp;=&nbsp;0. The cost of computing the determinant of a tridiagonal matrix using this formula is linear in ''n'', while the cost is cubic for a general matrix.\n\n===Inversion===\n\nThe [[inverse matrix|inverse]] of a non-singular tridiagonal matrix ''T''\n\n:<math>T = \\begin{pmatrix}\na_1 & b_1 \\\\\nc_1 & a_2 & b_2 \\\\\n& c_2 & \\ddots & \\ddots \\\\\n& & \\ddots & \\ddots & b_{n-1} \\\\\n& & & c_{n-1} & a_n\n\\end{pmatrix}</math>\n\nis given by\n\n:<math>(T^{-1})_{ij} = \\begin{cases}\n(-1)^{i+j}b_i \\cdots b_{j-1} \\theta_{i-1} \\phi_{j+1}/\\theta_n & \\text{ if } i < j\\\\\n\\theta_{i-1} \\phi_{j+1}/\\theta_n & \\text{ if } i = j\\\\\n(-1)^{i+j}c_j \\cdots c_{i-1} \\theta_{j-1} \\phi_{i+1}/\\theta_n & \\text{ if } i > j\\\\\n\\end{cases}</math>\n\nwhere the ''θ<sub>i</sub>'' satisfy the recurrence relation\n\n:<math>\\theta_i = a_i \\theta_{i-1} - b_{i-1}c_{i-1}\\theta_{i-2} \\qquad i=2,3,\\ldots,n</math>\n\nwith initial conditions ''θ''<sub>0</sub>&nbsp;=&nbsp;1, ''θ''<sub>1</sub>&nbsp;=&nbsp;''a''<sub>1</sub> and the ''ϕ''<sub>''i''</sub> satisfy\n\n:<math>\\phi_i = a_i \\phi_{i+1} - b_i c_i \\phi_{i+2} \\qquad i=n-1,\\ldots,1</math>\n\nwith initial conditions ''ϕ''<sub>''n''+1</sub>&nbsp;=&nbsp;1 and ''ϕ''<sub>''n''</sub>&nbsp;=&nbsp;''a<sub>n</sub>''.<ref>{{Cite journal | last1 = Da Fonseca | first1 = C. M. | doi = 10.1016/j.cam.2005.08.047 | title = On the eigenvalues of some tridiagonal matrices | journal = Journal of Computational and Applied Mathematics | volume = 200 | pages = 283–286 | year = 2007 | pmid = | pmc = }}</ref><ref>{{Cite journal | last1 = Usmani | first1 = R. A. | doi = 10.1016/0024-3795(94)90414-6 | title = Inversion of a tridiagonal jacobi matrix | journal = Linear Algebra and its Applications | volume = 212-213 | pages = 413–414 | year = 1994 | pmid = | pmc = }}</ref>\n\nClosed form solutions can be computed for special cases such as [[symmetric matrix|symmetric matrices]] with all off-diagonal elements equal<ref>{{Cite journal | last1 = Hu | first1 = G. Y. | last2 = O'Connell | first2 = R. F. | doi = 10.1088/0305-4470/29/7/020 | title = Analytical inversion of symmetric tridiagonal matrices | journal = Journal of Physics A: Mathematical and General | volume = 29 | issue = 7 | pages = 1511 | year = 1996 | pmid = | pmc = }}</ref> or [[Toeplitz matrices]]<ref>{{Cite journal | last1 = Huang | first1 = Y. | last2 = McColl | first2 = W. F. | doi = 10.1088/0305-4470/30/22/026 | title = Analytical inversion of general tridiagonal matrices | journal = Journal of Physics A: Mathematical and General | volume = 30 | issue = 22 | pages = 7919 | year = 1997 | pmid = | pmc = }}</ref> and for the general case as well.<ref>{{Cite journal | last1 = Mallik | first1 = R. K. | doi = 10.1016/S0024-3795(00)00262-7 | title = The inverse of a tridiagonal matrix | journal = Linear Algebra and its Applications | volume = 325 | pages = 109–139 | year = 2001 | pmid = | pmc = }}</ref><ref>{{Cite journal | last1 = Kılıç | first1 = E. | doi = 10.1016/j.amc.2007.07.046 | title = Explicit formula for the inverse of a tridiagonal matrix by backward continued fractions | journal = Applied Mathematics and Computation | volume = 197 | pages = 345–357 | year = 2008 | pmid = | pmc = }}</ref>\n\nIn general, the inverse of a tridiagonal matrix is a [[semiseparable matrix]] and vice versa.<ref name=\"VandebrilBarel2008\">{{cite book|author1=Raf Vandebril|author2=Marc Van Barel|author3=Nicola Mastronardi|title=Matrix Computations and Semiseparable Matrices. Volume I: Linear Systems|year=2008|publisher=JHU Press|isbn=978-0-8018-8714-7|at=Theorem 1.38, p. 41}}</ref>\n\n===Solution of linear system===\n{{Main|tridiagonal matrix algorithm}}\nA system of equations ''Ax''&nbsp;=&nbsp;''b'' for&nbsp;<math>b\\in \\R^n</math> can be solved by an efficient form of Gaussian elimination when ''A'' is tridiagonal called [[tridiagonal matrix algorithm]], requiring ''O''(''n'') operations.<ref>{{cite book|first1=Gene H.|last1=Golub|authorlink1=Gene H. Golub |first2=Charles F. |last2=Van Loan|authorlink2=Charles F. Van Loan| title =Matrix Computations| edition=3rd|publisher=The Johns Hopkins University Press|year=1996|isbn=0-8018-5414-8}}</ref>\n\n===Eigenvalues===\n\nWhen a tridiagonal matrix is also [[Toeplitz matrix|Toeplitz]], there is a simple closed-form solution for its eigenvalues, namely:<ref>{{Cite journal | doi = 10.1002/nla.1811| title = Tridiagonal Toeplitz matrices: Properties and novel applications| journal = Numerical Linear Algebra with Applications| volume = 20| issue = 2| pages = 302| year = 2013| last1 = Noschese | first1 = S. | last2 = Pasquini | first2 = L. | last3 = Reichel | first3 = L. }}</ref><ref>This can also be written as <math> a - 2 \\sqrt{bc} \\cos(k \\pi / {(n+1)}) </math> because <math> \\cos(x) = -\\cos(\\pi-x) </math>, as is done in: {{Cite journal | last1 = Kulkarni | first1 = D. | last2 = Schmidt | first2 = D. | last3 = Tsui | first3 = S. K. | title = Eigenvalues of tridiagonal pseudo-Toeplitz matrices | doi = 10.1016/S0024-3795(99)00114-7 | journal = Linear Algebra and its Applications | volume = 297 | pages = 63 | year = 1999 | pmid = | pmc = }}</ref>\n\n:<math> a + 2 \\sqrt{bc} \\cos \\left (\\frac{k\\pi}{n+1} \\right ), \\qquad k=1, \\ldots, n. </math>\n\nA real [[symmetric matrix|symmetric]] tridiagonal matrix has real eigenvalues, and all the eigenvalues are [[Eigenvalues and eigenvectors#Algebraic multiplicity|distinct (simple)]] if all off-diagonal elements are nonzero.<ref>{{Cite book | last1 = Parlett | first1 = B.N. | title = The Symmetric Eigenvalue Problem | year = 1980 | publisher = Prentice Hall, Inc. }}</ref> Numerous methods exist for the numerical computation of the eigenvalues of a real symmetric tridiagonal matrix to arbitrary finite precision, typically requiring <math>O(n^2)</math> operations for a matrix of size <math>n\\times n</math>, although fast algorithms exist which (without parallel computation) require only <math>O(n\\log n)</math>.<ref>{{Cite journal |last1 = Coakley |first1= E.S. |last2=Rokhlin | first2=V. | title =A fast divide-and-conquer algorithm for computing the spectra of real symmetric tridiagonal matrices | doi = 10.1016/j.acha.2012.06.003 |journal = Applied and Computational Harmonic Analysis |volume = 34 |issue = 3 |pages = 379–414 |year =2012 }}</ref>\n\n==Computer programming==\nA transformation that reduces a general matrix to Hessenberg form will reduce a Hermitian matrix to ''tridiagonal'' form.  So, many [[eigenvalue algorithm]]s, when applied to a Hermitian matrix, reduce the input Hermitian matrix to tridiagonal form as a first step.\n\nA ''tridiagonal matrix'' can also be stored more efficiently than a general matrix by using a special [[matrix representation|storage scheme]]. For instance, the [[LAPACK]] [[Fortran]] package stores an unsymmetric tridiagonal matrix of order ''n'' in three one-dimensional arrays, one of length ''n'' containing the diagonal elements, and two of length ''n'' &minus; 1 containing the [[subdiagonal]] and [[superdiagonal]] elements.\n\n==See also==\n* [[Pentadiagonal matrix]]\n\n==Notes==\n<references/>\n\n==External links==\n* [http://www.netlib.org/lapack/lug/node125.html Tridiagonal and Bidiagonal Matrices] in the LAPACK manual.\n* {{cite journal |author=Moawwad El-Mikkawy, Abdelrahman Karawia |title=Inversion of general tridiagonal matrices |journal=Applied Mathematics Letters |volume=19 |issue=8 |year=2006 |pages=712–720 |doi=10.1016/j.aml.2005.11.012 |url=http://app2.mans.edu.eg/eulc/Libraries/itemsAttach/IssueArticle/1037592/papermik.pdf |deadurl=yes |archiveurl=https://web.archive.org/web/20110720131242/http://app2.mans.edu.eg/eulc/Libraries/itemsAttach/IssueArticle/1037592/papermik.pdf |archivedate=2011-07-20 |df= }}\n* [http://www.cs.utexas.edu/users/flame/pubs/flawn53.pdf High performance algorithms] for reduction to condensed (Hessenberg, tridiagonal, bidiagonal) form\n* [https://web.archive.org/web/20140310122757/http://michal.is/projects/tridiagonal-system-solver-sor-c/ Tridiagonal linear system solver] in C++\n\n[[Category:Sparse matrices]]"
    },
    {
      "title": "Zero matrix",
      "url": "https://en.wikipedia.org/wiki/Zero_matrix",
      "text": "In [[mathematics]], particularly [[linear algebra]], a '''zero matrix''' or '''null matrix''' is a [[matrix (mathematics)|matrix]] all of whose entries are [[0 (number)|zero]].<ref>{{citation|title=Linear Algebra|series=[[Undergraduate Texts in Mathematics]]|first=Serge|last=Lang|authorlink=Serge Lang|publisher=Springer|year=1987|isbn=9780387964126|page=25|url=https://books.google.com/books?id=0DUXym7QWfYC&pg=PA25|quotation=We have a zero matrix in which ''a<sub>ij</sub>''&nbsp;=&nbsp;0 for all ''i'',&nbsp;''j''. ... We shall write it&nbsp;''O''.}}</ref> Some examples of zero matrices are\n\n:<math>\n0_{1,1} = \\begin{bmatrix}\n0 \\end{bmatrix}\n,\\ \n0_{2,2} = \\begin{bmatrix}\n0 & 0 \\\\\n0 & 0 \\end{bmatrix}\n,\\ \n0_{2,3} = \\begin{bmatrix}\n0 & 0 & 0 \\\\\n0 & 0 & 0 \\end{bmatrix}\n.\\ \n</math>\n\n==Properties==\n\nThe set of ''m''&times;''n'' matrices with entries in a [[ring (mathematics)|ring]] K forms a ring <math>K_{m,n} \\,</math>.  The zero matrix <math>0_{K_{m,n}} \\, </math> in <math>K_{m,n} \\, </math> is the matrix with all entries equal to <math>0_K \\, </math>, where <math>0_K \\, </math> is the [[additive identity]] in K.  \n\n:<math>\n0_{K_{m,n}} = \\begin{bmatrix}\n0_K & 0_K & \\cdots & 0_K \\\\\n0_K & 0_K & \\cdots & 0_K \\\\\n\\vdots & \\vdots & \\ddots  & \\vdots \\\\\n0_K & 0_K & \\cdots & 0_K \\end{bmatrix}_{m \\times n}\n</math>\n\nThe zero matrix is the additive identity in <math>K_{m,n} \\, </math>.<ref>{{citation|title=Modern Algebra|first=Seth|last=Warner|publisher=Courier Dover Publications|year=1990|isbn=9780486663418|page=291|url=https://books.google.com/books?id=dT2KAAAAQBAJ&pg=PA291|quotation=The neutral element for addition is called the zero matrix, for all of its entries are zero.}}</ref> That is, for all <math>A \\in K_{m,n} \\, </math> it satisfies\n\n:<math>0_{K_{m,n}}+A = A + 0_{K_{m,n}} = A.</math>\n\nThere is exactly one zero matrix of any given size ''m''&times;''n'' having entries in a given ring, so when the context is clear one often refers to ''the'' zero matrix.  In general the zero element of a ring is unique and typically denoted as 0 without any [[subscript]] indicating the parent ring.  Hence the examples above represent zero matrices over any ring.\n\nThe zero matrix represents the [[linear transformation]] sending all [[vector (geometric)|vector]]s to the [[zero vector]].<ref>{{citation|title=Linear Algebra: An Introduction|first1=Richard|last1=Bronson|first2=Gabriel B.|last2=Costa|publisher=Academic Press|year=2007|isbn=9780120887842|page=377|url=https://books.google.com/books?id=ZErjtA3mIvkC&pg=PA377|quotation=The zero matrix represents the zero transformation ''0'', having the property ''0''(''v'')&nbsp;=&nbsp;'''0''' for every vector ''v''&nbsp;∈&nbsp;''V''.}}</ref>\n\nThe zero matrix is [[idempotent matrix|idempotent]], meaning that when it is multiplied by itself the result is itself.\n\nThe zero matrix is the only matrix whose [[rank (linear algebra)|rank]] is 0.\n\n==Occurrences==\n\nThe '''mortal matrix problem''' is the problem of determining, given a finite set of ''n'' × ''n'' matrices with integer entries, whether they can be multiplied in some order, possibly with repetition, to yield the zero matrix. This is known to be [[undecidable problem|undecidable]] for a set of six or more 3 × 3 matrices, or a set of two 15 × 15 matrices.<ref>{{cite arXiv |eprint=1404.0644|last1=Cassaigne|first1=Julien|title=Tighter Undecidability Bounds for Matrix Mortality, Zero-in-the-Corner Problems, and More|last2=Halava|first2=Vesa|last3=Harju|first3=Tero|last4=Nicolas|first4=Francois|class=cs.DM|year=2014}}</ref>\n\nIn [[ordinary least squares]] regression, if there is a perfect fit to the data the [[annihilator matrix]] is the zero matrix.\n\n==See also==\n*[[Identity matrix]], the multiplicative identity for matrices\n*[[Matrix of ones]], a matrix where all elements are one\n*[[Single-entry matrix]], a matrix where all but one element is zero\n\n==References==\n{{reflist}}\n\n==External links==\n*{{MathWorld|title=Zero Matrix|urlname=ZeroMatrix}}\n\n[[Category:Matrices]]\n[[Category:0 (number)]]\n[[Category:Sparse matrices]]"
    },
    {
      "title": "Comparison of numerical-analysis software",
      "url": "https://en.wikipedia.org/wiki/Comparison_of_numerical-analysis_software",
      "text": "{{Use mdy dates|date=October 2014}}\n{{Expand list|date=February 2011}}\n\nThe following tables provide a '''comparison of [[numerical analysis|numerical-analysis]] software'''.\n\n== Applications ==\n\n=== General ===\n\n{| class=\"wikitable sortable\" style=\"font-size: smaller; text-align: center; width: auto;\"\n|-\n! style=\"width: 12em\"|\n! Creator\n! Development started\n! First public release\n! data-sort-type=\"number\"|Latest stable version\n! Stable release date\n! data-sort-type=\"currency\"|Cost ([[United States dollar|USD]])\n! License\n! Notes\n|-\n! [[ADMB]]\n| David Fournier, ADMB Project\n| 1989\n| 1990\n| 12.0\n| {{dts|21 December 2017}}\n| {{free}}\n| [[BSD Licenses#3-clause|BSD 3-clause (aka new) License]]\n| Automatic differentiation makes it well suited to complex minimization problems\n|-\n! [[Analytica (software)|Analytica]]\n| Lumina Decision Systems\n| 1982 (Demos)\n| \n| 4.6\n| {{dts|May 2015}}\n| {{depends|Free (Analytica Free 101), $995 (Professional),  $2795 (Enterprise)}}\n| Proprietary\n| A numerical modeling environment with a [[Declarative language|declarative]] and [[visual programming language]] based on [[influence diagrams]].\n|-\n! [[Ch (computer programming)|Ch]]\n| SoftIntegration\n|\n| data-sort-value=\"2001-10-01\"|{{dts|1 October 2001}}\n| 7.5.1\n| {{dts|2 December 2015}}\n| {{depends|$399 (commercial) / $199 (academic) / Free (student)}}\n| Proprietary\n| C/C++ based numerical computing and graphical plotting<ref>[http://www.softintegration.com/docs/ch/numeric/ Ch Scientific Numerical Computing]</ref>\n|-\n! [[DADiSP]] \n| DSP Development\n| 1984\n| 1987\n| 6.7 B02\n| {{dts|17 January 2017}}\n| {{nonfree|$1995 (commercial) / $129 (academic) / Free (student)}}\n| Proprietary\n| Numeric computations for science and engineering featuring a [[spreadsheet]] like interface.\n|-\n! [[Dyalog APL]]\n| Dyalog Ltd.\n| 1981\n| 1983\n| 17.0\n| {{dts|23 July 2018}}\n| {{depends|£850/year or 2% royalty (free for non-commercial use)}}\n| Proprietary\n| A modern dialect of [[APL (programming language)|APL]], enhanced with features for functional and object-oriented programming.\n|-\n! [[Euler (software)|Euler Math Toolbox]]\n| René Grothmann\n| 1987\n| 1988\n| 2019-04-17\n| {{dts|16 April 2019}}\n| {{free}}\n| [[GNU General Public License|GPL]]\n| Also a computer algebra system through interface with [[Maxima (software)|Maxima]]\n|-\n! [[Fityk]]\n| Marcin Wojdyr\n|\n| 2002\n| 1.3.1\n| {{dts|19 December 2016}}\n| {{free|$115 (1.x binaries) / Free (source code and 0.x binaries)}}\n| [[GNU General Public License|GPL]]\n| interactive graphics, scriptable, specialized in curve fitting and peak fitting, 2D only\n|-\n![[FlexPro]]\n|Weisang GmbH\n|n/a\n|1991\n|2017\n|2017\n| {{depends|Not free (commercial) / Free (academic)}}\n|[[Proprietary software|Proprietary]]\n|Dynamic, interactive 2D/3D diagrams, programmable, VBA, high performances, multicore compatible, large data sets.\n|-\n! [[FreeMat]]\n| Samit Basu\n|\n| 2004\n| 4.2\n| {{dts|30 June 2013}}\n| {{free}}\n| [[GNU General Public License|GPL]]\n| Codeless interface to external [[C (programming language)|C]], [[C++]], and [[Fortran]] code.  Mostly compatible with MATLAB.\n|-\n! [[GAUSS (software)|GAUSS]]\n| Aptech Systems\n|\n| 1984\n| 17\n| {{dts|22 December 2016}}\n| {{nonfree|Not free}}\n| Proprietary\n| \n|-\n! [[GNU Data Language]]\n| Marc Schellens\n|\n| 2004\n| 0.9.7\n| {{dts|21 January 2017}}\n| {{free}}\n| [[GNU General Public License|GPL]]\n| Aimed as a drop-in replacement for IDL/PV-WAVE\n|-\n! [[IBM SPSS Statistics]]\n| [[Norman H. Nie|Normal H. Nie]], Dale H. Bent, and C. Hadlai Hull\n| \n| 1968\n| 23.0\n| {{dts|3 March 2015}}\n| {{nonfree|Not free}}\n| [[Proprietary software|Proprietary]]\n| Primarily for statistics\n|-\n! [[MCSim|GNU MCSim]]\n| Frederic Y. Bois & Don Maszle\n| 1991\n| 1993\n| 6.0.0\n| {{dts|24 February 2018}}\n| {{free}}\n| [[GNU General Public License|GPL]]\n| General simulation and Monte Carlo sampling software\n|-\n! [[GNU Octave]]\n| John W. Eaton\n| 1988\n| 1993\n| 5.1.0\n| {{dts|1 March 2019}}\n| {{free}}\n| [[GNU General Public License|GPL]]\n| General numerical computing package with lots of extension modules. Syntax mostly compatible with MATLAB\n|-\n! [[IGOR Pro]]\n| WaveMetrics\n| 1986\n| 1988\n| 8.00\n| {{dts|May 22, 2018}}\n| {{nonfree|$995 (commercial) $225 upgrade / $499 (academic) $175 upgrade / $85 (student)}}\n| Proprietary\n| interactive graphics, programmable, 2D/3D, used for science and engineering, large data sets.\n|-\n! [[J (programming language)|J]]\n| Jsoftware\n| 1989\n| 1990\n| J806\n| {{dts|12 November 2017}}\n| {{free}}\n| [[GNU General Public License|GPL]]\n| online access to: J Application Library (JAL)\n|-\n! [[Julia (programming language)|Julia]]\n| [[Jeff Bezanson]], Stefan Karpinski, Viral B. Shah, and other<ref>[https://github.com/JuliaLang/julia/contributors Contributors to JuliaLang/julia - GitHub]</ref> contributors\n| 2009\n| 2012\n| 1.1.1\n| {{dts|16 May 2019}}\n| {{free}}\n| [[MIT License]]\n| A fast<ref>[https://julialang.org/ \"Julia in a Nutshell\"], from the official Julia homepage.  Accessed 2019-01-25.</ref><ref>Sai K. Popuri and Matthias K. Gobbert. [https://userpages.umbc.edu/~gobbert/papers/PopuriHPCF2017.pdf ''A Comparative Evaluation of Matlab, Octave, R, and Julia on Maya'']. Technical Report HPCF-2017-03, UMBC High Performance Computing Facility, University of Maryland, U.S.A., 2017. Accessed 2019-01-25.</ref><ref>Jules Kouatchou; [https://modelingguru.nasa.gov/docs/DOC-2676 ''Basic Comparison of Python, Julia, Matlab, IDL and Java (2018 Edition)'']\nVersion 74. [[NASA]] Modeling Guru, Technical Report DOC-2676. Created on: 05-Feb-2018. Last Modified:  14-Sep-2018Accessed 2019-01-25.</ref>, high-level numerical computing language.  \n<ref group=\"Note\">Julia allows direct calls of C functions (no wrappers needed). Designed for cloud parallel computing with LLVM JIT as a backend. Lightweight \"green\" threading (coroutines). Efficient support for Unicode. Shell-like capabilities for managing other processes. Lisp-like macros and other metaprogramming facilities.</ref>\n|-\n! [[LabVIEW]]\n| [[National Instruments]]\n| 1985\n| 1986\n| 2016\n| {{dts|August 2016}}\n| {{nonfree|$1249 (commercial) / $79.95 (student)}}\n| Proprietary\n|  [[Graphical programming|Graphical]], and textual through formula nodes, mathscript and .m file scripts<ref>{{cite web|url=http://www.ni.com/white-paper/7006/en/|title=Working with .m File Scripts in NI LabVIEW for Text Based Signal Processing, Analysis, and Math|last=[[National Instruments]]|accessdate=3 April 2017}}</ref>\n|-\n! [[Maple (software)|Maple]]\n| [[Waterloo Maple Inc.|Maplesoft]]\n| 1980\n| 1982\n| {{Latest stable software release/Maple}}\n| {{dts|14 March 2019}}\n| {{nonfree|$2390 (commercial) / $239 (personal) / $99 (student)}}\n| Proprietary\n| Mainly a [[computer algebra system]]\n|-\n! [[Mathcad]]\n| [[Parametric Technology Corporation]]\n| 1985\n| 1986\n| 15.0 ; Prime 4.0<ref>{{Cite news|url=https://www.ptc.com/en/products/mathcad/new-release|title=PTC Mathcad Prime 4.0 {{!}} PTC|access-date=2018-08-12}}</ref>\n| {{dts|2 March 2015}}\n| {{nonfree|$1195 (commercial) / $99 (student)}}\n| Proprietary\n| \n|-\n! [[Mathematica]]\n| [[Wolfram Research]]\n| 1986\n| 1988\n| {{Latest stable software release/Mathematica}}\n| {{dts|8 March 2018}} \n| {{depends| Free ([[Raspberry Pi]]<ref>[https://www.theverge.com/2013/11/21/5130394/raspberry-pi-includes-mathematica-wolfram-language-free Raspberry Pi now includes Mathematica and Wolfram Language for free]</ref>), Free [https://www.open.wolframcloud.com/ Cloud Access], $2495 (commercial) / $145 (student) / $295 (personal)<ref>[http://www.macworld.com/article/138664/2009/02/mathematica.html Mathematica Home Edition Released] Macworld, February 2009</ref> }}\n| Proprietary\n| Also computer algebra system\n|-\n! [[MATLAB]]\n| [[MathWorks]]\n| data-sort-value=\"1978\"|late 1970s\n| 1984\n| data-sort-value=\"9.5\"|9.5 (R2018b)\n| {{dts|12 September 2018}}\n| {{nonfree|$2150 (standard) / $500 (education) / $149 (home) / $49 (student)}}\n| Proprietary\n| Numerical computation and simulation with extended 2D/3D visualization. Emphasis on vectorised processing.  \n|-\n! [[Maxima (software)|Maxima]]\n| [[MIT Project MAC]] and [[Bill Schelter]] et al.\n| 1967\n| 1982\n| 5.41.0\n| {{dts|3 October 2017}}\n| {{free}}\n| [[GNU General Public License|GPL]]\n| Mainly a [[computer algebra system]]\n|-\n! [[MLAB]]\n| Civilized Software, Inc.\n| 1970 (in SAIL), 1985 (in C)\n| 1972 (on DEC-10), 1988 (on PCs), 1993 (on MACs)\n| 2015\n| {{dts|2015}}\n| {{nonfree|$2250 (standard) / $50 (student)}}\n| Proprietary\n| Numerical and Statistical computation for Scientific Applications, e.g. Chemical Kinetics. [[Ordinary differential equation|ODE]] Solving and Curve-Fitting. Symbolic Differentiation, Survival Analysis, Cluster Analysis, 2D/3D Graphics. \n|-\n! [[Origin (data analysis software)|Origin]]\n| OriginLab\n|\n| 1991\n| 2019b\n| April 24, 2019\n| {{nonfree|$1095 (std.)/$1800 (Pro)<br />$550 (std., academic)<br />$850 (Pro, academic)<br /> $69/yr. (Pro, student)}}\n| Proprietary\n| Integrated data analysis graphing software for science and engineering.  Flexible multi-layer graphing framework. 2D, 3D and statistical graph types. Built-in digitizing tool. Analysis with auto recalculation and report generation. Built-in scripting and programming languages.\n|-\n! [[Perl Data Language]]\n| Karl Glazebrook\n| data-sort-value=\"1996\"|1996\n| data-sort-value=\"1997\"|c. 1997\n| 2019\n| {{dts|5 May 2018}}\n| {{free}}\n| [[Artistic License]]\n| Used for astrophysics, solar physics, oceanography, biophysics, and simulation. 2D plotting via PGPLOT, PLPlot bindings; 3D via GL.\n|-\n! [[PSPP]]\n| Ben Pfaff\n| 1990s\n| 1990s\n| 1.2.0\n| {{dts|6 November 2018}}\n| {{free}}\n| [[GNU General Public License|GPL]] v.3 or later\n| [[FOSS]] statistics program, intended as an alternative to IBM SPSS Statistics. \n<ref group=\"Note\">Capabilities of PSPP include analysis of sampled data, frequencies, cross-tabs comparison of means (t-tests and one-way ANOVA); linear regression, logistic regression, reliability (Cronbach's Alpha, not failure or Weibull), and re-ordering data, non-parametric tests, factor analysis, cluster analysis, principal components analysis, chi-square analysis and more.</ref>\n|-\n! [[R (programming language)|R]]\n| R Foundation\n| 1997\n| 1997\n| 3.5.2\n| {{dts|20 December 2018}}\n| {{free}}\n| [[GNU General Public License|GPL]]\n| Primarily for statistics, but there are many interfaces to open-source numerical software\n|-\n! [[SageMath]]\n| [[William A. Stein|William Stein]]\n| \n| 2005\n| {{Latest stable software release/SageMath}}\n| {{dts|23 March 2019}}\n| {{free}}\n| [[GNU General Public License|GPL]]\n| Programmable, includes computer algebra, 2D+3D plotting. Interfaces to many open-source and proprietary software. Web based interface [[HTTP]] or [[HTTPS]]\n|-\n! [[SAS (software)|SAS]]\n| [[Anthony James Barr|Anthony Barr]] and [[James Goodnight]]\n| 1966\n| 1972\n| 9.4\n| {{dts|10 July 2014}}\n| {{nonfree|Not free}}\n| [[Proprietary software|Proprietary]]\n| Mainly for statistics\n|-\n! [[SequenceL]]\n| [[Texas Multicore Technologies]]\n| 1989\n| 2012\n| 2.4\n| {{dts|10 February 2016}}\n| {{depends| Free (Community Edition), $2495 (Professional Edition)}}\n| Proprietary\n| Functional programming language and tools. \n<ref group=\"Note\">SequenceL delivers high performance on multicore hardware with ease of programming, and code clarity/readability. Designed to work with other languages, including C, C++, C#, Java, Fortran, Python, etc. Can be compiled to multithreaded C++ (and optionally OpenCL) code with no explicit indications from the programmer of how or what to parallelize. A platform-specific runtime manages the threads safely.</ref>\n|-\n! [[S-Lang (programming library)|S-Lang]]\n| John E. Davis\n|\n| 1992\n| 2.3.0\n| {{dts|18 September 2014}}\n| {{free}}\n| [[GNU General Public License|GPL]], [[Artistic License]] (1.x only)\n| Available as a standalone (slsh) and embedded interpreter ([[JED (text editor)|jed]], [[slrn]], ...)\n|-\n! [[Scilab]]\n| [[Scilab Enterprises]] Was:[[Inria]]\n| 1990\n| 1994\n| 6.0.2\n| {{dts|14 February 2019}}\n| {{free}}\n| [[GPL]]\n| Programmable, direct support of 2D+3D plotting. Interfaces to many other software packages. Interfacing to external modules written in C, Java, Python or other languages. Language syntax similar to MATLAB. Used for numerical computing in engineering and physics.\n|-\n! [[Sysquake]]\n| [[Calerga]]\n|\n| 1998\n| 5.0\n| {{dts|2013}}\n| {{depends|free / $2500 (Pro, commercial) / $1000 (Pro, academic)}}\n| Proprietary\n| interactive graphics\n|-\n! [[TK Solver]]\n| [[Universal Technical Systems, Inc.]]\n| data-sort-value=\"1978\"|late 1970s\n| 1982\n| 5.0.141\n| {{dts|2011}}\n| {{nonfree|$399 commercial / $49 (student)}}\n| Proprietary\n| Numerical computation and rule-based application development\n|-\n! [[VisSim]]\n| Visual Solutions\n|\n| 1989\n| 10.1\n| {{dts|January 2011}}\n| {{depends|$495-$2800 (commercial)<br>free view-only version<br>$50-$250/free v3.0 (academic)}}\n| Proprietary\n| [[Visual programming language|Visual Language]] for simulation and [[Model Based Design]]. Used in business, science and engineering. Performs complex scalar or matrix based ODE solving with parametric optimization. Has 2D and 3D plotting, 3D animation, and state transition built in.\n|-\n! [[Yorick (programming language)|Yorick]]\n| n/a\n| n/a\n| n/a\n| 9\n| {{dts|January 2015}}\n| {{free}}\n| [[GNU General Public License|GPL]]\n| Programmable, callable 2D+3D plotting. Language syntax similar to C. Interfacing to other software packages via C calls.\n|-\n|}\n\n=== Operating system support ===\nThe [[operating system]]s the software can run on natively (without [[emulator|emulation]]).\n\n{| class=\"wikitable sortable\" style=\"text-align: center; width: auto;\"\n|-\n! style=\"width: 12em\" |\n! [[Microsoft Windows|Windows]]\n! [[macOS]]\n! [[Linux]]\n! [[Berkeley Software Distribution|BSD]]\n! [[Unix]]\n! [[DOS]]\n! [[Android (operating system)|Android]]\n! [[Software as a service|SaaS]]\n|-\n! [[ADMB]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n|-\n! [[Analytica (software)|Analytica]]\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n|-\n! [[Ch (computer programming)|Ch]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n|-\n! [[Dyalog APL]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{yes}}\n| {{no}}<ref name=\"once\">Once was supported</ref>\n| {{no}}\n| {{no}}\n|-\n! [[DADiSP]]\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n|-\n! [[Euler (software)|Euler Math Toolbox]]\n| {{yes}}\n| {{no}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n|-\n![[FlexPro]]\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n|-\n! [[FreeMat]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n|-\n! [[GAUSS (software)|GAUSS]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n|-\n! [[GNU Data Language]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n|-\n! [[MCSim|GNU MCSim]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n|-\n! [[GNU Octave]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n|-\n! [[IGOR Pro]]\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n|-\n! [[Julia (programming language)|Julia]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n|-\n! [[LabVIEW]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n|-\n! [[Maple (software)|Maple]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n|-\n! [[Mathematica]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{yes}}\n|-\n! [[MATLAB]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n|-\n! [[MLAB]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{yes}}\n| {{no}}\n| {{no}}\n|-\n! [[Origin (data analysis software)|Origin]]\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n|-\n! [[Perl Data Language]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n|-\n! [[R (programming language)|R]]\n| {{yes}}\n| {{yes}}\n| {{Yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n|-\n! [[SageMath]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{yes}}\n|-\n! [[SAS (software)|SAS]]\n| {{yes}}\n| {{no}}\n| {{yes}}\n| {{no}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n|-\n! [[SequenceL]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n|-\n! [[S-Lang (programming library)|S-Lang]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n|-\n! [[Scilab]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n|-\n! [[Sysquake]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n|-\n! [[TK Solver]]\n| {{Yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n|-\n! [[The Unscrambler]]\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n|-\n! [[VisSim]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n|-\n|}\n\n=== Language features ===\nColors indicate features available as\n{|\n|{{yes|}}basic system capabilities\n|-\n|{{Depends|}}official or officially supported extensions and libraries\n|-\n|{{No|}}[[third-party software component]]s or not supported\n|}\n\n{| class=\"wikitable sortable\" style=\"text-align: center; width: auto;\"\n|-\n! style=\"width: 12em\" |\n! Standalone [[executable]]s creation support\n! [[Symbolic computation]] support\n! [[Object-oriented programming|OOP]] support\n! [[Graphical user interface|GUI]] creation support\n! [[Array data type#Multi-dimensional arrays|Multi-dimensional arrays]] as [[primitive data type]]\n! Centralized extension library website\n! Can call code in other languages\n! Can be called from other languages\n|-\n! [[Analytica (software)|Analytica]]\n| {{no}}\n| {{no}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n|{{yes|}}[[Component Object Model|COM]], [[Microsoft Excel|Excel]]\n|{{yes|}}[[Microsoft Excel|Excel]], [[Component Object Model|COM]], [[.NET Framework|.NET]]\n|-\n! [[Dyalog APL]]\n| {{yes}}<ref>{{cite web|url= http://docs.dyalog.com/13.2/Dyalog%20APL%20User%20Guide.pdf|title=Dyalog APL Users Guide}}</ref>\n| {{yes}}\n| {{yes}}<ref>{{cite web|url= http://archive.vector.org.uk/trad/v221/oops221.htm|title=An Introduction to Object Oriented Programming for APL programmers}}</ref>\n| {{yes|}}[[Win32]], [[.NET Framework|.NET]], [[Windows Presentation Foundation|WPF]], [[JavaScript|HTML/JS]]\n| {{yes}}\n| {{yes}}\n| {{yes|}}[[Component Object Model|COM]], [[.NET Framework|.NET]], [[Web service|WebServices]], [[Library (computing)|Shared Libraries]], [[Dynamic-link library|DLL]]s, [[NAG Numerical Library|NAG]], [[R (programming language)|R]], [[JavaScript]]<ref name=\"Dyalog APL Interface Guide\">{{cite web|url= http://docs.dyalog.com/13.2/Dyalog%20APL%20Interface%20Guide.pdf|title=Dyalog APL Interface Guide}}</ref>\n| {{yes|}}[[Component Object Model|COM]], [[.NET Framework|.NET]], [[Web service|WebServices]]<ref name=\"Dyalog APL Interface Guide\"/>\n|-\n! [[GNU Data Language]]\n| {{no}}\n| {{no}}\n| {{yes}}\n| {{Depends|}} GUI Widgets since v.0.9 but still incomplete\n| {{yes}}\n| {{no}}\n|{{yes|}}[[C (programming language)|C]], [[Python (programming language)|Python]]\n|{{yes|}}[[Python (programming language)|Python]]\n|-\n! [[MCSim|GNU MCSim]]\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{Depends|}} XMCSim with TCL/TK and wish\n| {{no}}\n| {{no}}\n|{{yes|}}[[C (programming language)|C]]\n|{{yes|}}[[R (programming language)|R]]\n|-\n! [[GNU Octave]]\n| {{Yes}} with mkoctfile\n| {{Depends|}} only with [[GiNaC]] extension\n| {{yes}}<ref>{{cite web|url=https://www.gnu.org/software/octave/doc/interpreter/Object-Oriented-Programming.html|title=GNU Octave: Object Oriented Programming|accessdate=18 May 2011}}</ref>\n| {{no|''GUI Octave'' code could be used<ref>{{cite web|url=http://sites.google.com/site/guioctave/home|title=GUI Octave|last=Varandas|first=Joaquim|accessdate=18 May 2011}}</ref>, although this \"is no longer available\"<ref>{{cite web|url=http://wiki.octave.org/FAQ#Why_did_you_create_yet_another_GUI_instead_of_making_one_that_already_exists_better.3F|title=\"Octave FAQ\"|accessdate=2019-01-25}}</ref>}}\n| {{Yes}}\n| {{Yes}}<ref>{{cite web|url=http://octave.sourceforge.net/index.html|title=Octave-Forge|accessdate=18 May 2011}}</ref>\n| {{Depends|}}[[C++]], [[Fortran]],<ref>{{cite web|url=http://wiki.octave.org/wiki.pl?OctaveFortran|title=Octave Wiki: OctaveFortran|accessdate=18 May 2011}}</ref> [[Perl]],<ref name=\"OctavePerl\"/> [[Tcl]]<ref>{{cite web|url=http://wiki.octave.org/wiki.pl?OctaveTcl|title=Octave Wiki: OctaveTcl|accessdate=18 May 2011}}</ref>\n| {{Depends|}}[[Java (programming language)|Java]],<ref>{{cite web|url=http://wiki.octave.org/Main_Page |title=Octave Wiki: OctaveJava|accessdate=18 May 2011}}</ref> [[Perl]],<ref name=\"OctavePerl\">{{cite web|url=http://wiki.octave.org/wiki.pl?OctavePerl|title=Octave Wiki: OctavePerl|accessdate=18 May 2011}}</ref> [[C++]]<ref>{{cite web|url=http://wiki.octave.org/wiki.pl?CategoryExternal|title=Octave Wiki: CategoryExternal|accessdate=18 May 2011}}</ref>\n|-\n! [[J (programming language)|J]]\n| {{no}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n|{{yes|}}[[JavaScript]]\n|{{yes|}}[[JavaScript]]\n|-\n! [[Julia (programming language)|Julia]]\n| {{no}}\n| {{no}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n|\n{|\n| {{yes|}}[[C (programming language)|C]], [[Fortran]]\n|-\n| {{no|}}[[Python (programming language)|Python]], [[C++]], [[Java (programming language)|Java]], [[MATLAB]]\n|}\n| {{yes|}}[[C (programming language)|C]]/[[C++]]\n|-\n! [[LabVIEW]]\n|\n{|\n|{{Yes|Professional System version}}\n|-\n|{{Depends|With add-on}}<ref>{{cite web|url=http://sine.ni.com/nips/cds/view/p/lang/en/nid/212940|title=LabVIEW Application Builder|last=[[National Instruments]]|accessdate=3 April 2017}}</ref>\n|}\n| {{No}}\n|\n{|\n|{{Yes}}<ref>{{cite web|url=http://zone.ni.com/reference/en-XX/help/371361N-01/lvconcepts/front_oolv|title=LabVIEW Object-Oriented Programming|last=[[National Instruments]]|accessdate=3 April 2017}}</ref>\n|-\n|{{Depends|GOOP}}<ref>{{cite web|url=http://sine.ni.com/nips/cds/view/p/lang/en/nid/209038|title=NI GOOP Development Suite|last=[[National Instruments]]|accessdate=3 April 2017}}</ref>\n|-\n|{{No|G#}}<ref>{{cite web|url=http://sine.ni.com/nips/cds/view/p/lang/en/nid/209103|title=G# Framework|last=[[National Instruments]]|accessdate=3 April 2017}}</ref>\n|}\n| {{Yes}}\n| {{Yes}}\n| {{Yes}}<ref>{{cite web|url=http://www.ni.com/labview-tools-network/|title=LabVIEW Tools Network|last=[[National Instruments]]|accessdate=3 April 2017}}</ref>\n|\n{|\n|{{Yes|}}[[C (programming language)|C]]/[[C++]],<ref>{{cite web|url=https://forums.ni.com/t5/Developer-Center-Resources/Calling-External-Code-From-LabVIEW/ta-p/3522282|title=Calling External Code From LabVIEW|last=[[National Instruments]]|accessdate=3 April 2017}}</ref> [[Dynamic-link library|DLLs]], [[Python (programming language)|Python]],<ref>{{cite web|url=http://www.ni.com/tutorial/8493/en/|title=Call Perl and Python Scripts from LabVIEW|last=[[National Instruments]]|accessdate=3 April 2017}}</ref> [[Perl]], [[Tcl]],<ref>{{cite web|url=http://www.ni.com/white-paper/8910/en/|title=Introduction to Scripting in Perl, Python and Tcl|last=[[National Instruments]]|accessdate=3 April 2017}}</ref> [[MATLAB]], [[Dynamic-link library|ActiveX]], [[C Sharp (programming language)|C#]]/[[.NET]]\n|-\n|{{No|With add-ons. I.e.}}<ref>{{cite web|url=http://www.luaforlabview.com/|title=Lua for LabVIEW|accessdate=3 April 2017}}</ref>\n|}\n|{{Yes}}<ref>{{cite web|url=http://www.ni.com/tutorial/5719/en/|title=Calling LabVIEW VIs from Other Programming Languages|last=[[National Instruments]]|accessdate=3 April 2017}}</ref>\n|-\n! [[Maple (software)|Maple]]\n| {{no}}\n| {{yes}}\n| {{yes}}<ref>{{cite web|url=http://www.maplesoft.com/applications/view.aspx?SID=4669|title=Object-Oriented Programming, Polymorphism, and More in Maple 9.5|last=[[Maplesoft]]|accessdate=18 May 2011}}</ref>\n| {{yes}}\n| {{yes}}\n| {{yes}}<ref>{{cite web|url=http://www.maplesoft.com/applications/index.aspx/|title=Maple Application Center}}</ref>\n|{{yes|}}[[C (programming language)|C]], [[C Sharp (programming language)|C#]], [[Fortran]]\n|{{Depends|}} [[MATLAB]],<ref>{{cite web|url=http://www.maplesoft.com/products/maple/features/feature_detail.aspx?fid=6721|title=MAPLE: MATLAB® Connectivity|accessdate=18 May 2011}}</ref> [[Microsoft Excel|Excel]],<ref>{{cite web|url=http://www.maplesoft.com/support/help/Maple/view.aspx?path=Excel|title=Maple and Excel}}</ref> [[VisualBasic]], [[Java (programming language)|Java]], and [[C (programming language)|C]]<ref>{{cite web|url=http://www.maplesoft.com/applications/view.aspx?SID=4666|title=OpenMaple API for VisualBasic and Java|last=[[Maplesoft]]|accessdate=18 May 2011}}</ref>\n|-\n! [[Mathematica]]\n|{{yes}}<ref>{{cite web|url=http://reference.wolfram.com/mathematica/CCodeGenerator/tutorial/Overview.html|title=C Code Generation User Guide|last=[[Wolfram Research]]|accessdate=19 May 2011}}</ref>\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}<ref>[http://library.wolfram.com/ library.wolfram.com]</ref>\n|{{yes|}}[[Java (programming language)|Java]], [[.NET Framework|.NET]], [[C++]], [[Fortran]], [[CUDA]], [[OpenCL]], [[R Statistics|R]], [[Python (programming language)|Python]], [[SQL]], [[SPARQL]], [[NodeJS]]\n|\n{|\n|{{yes|}}[[Java (programming language)|Java]], [[.NET Framework|.NET]], [[C++]], [[Python (programming language)|Python]]\n|-\n|{{depends|}}[[Microsoft Excel|Excel]],<ref>{{cite web|url=http://www.wolfram.com/products/applications/excel_link/|title=Mathematica Link for Excel 3.2|last=[[Wolfram Research]]|accessdate=18 May 2011}}</ref> [[LabVIEW]]<ref>{{cite web|url=http://www.wolfram.com/products/applications/labview/|title=Mathematica Link for LabVIEW 2.1|accessdate=18 May 2011}}</ref>\n|-\n|{{no|}}[[Haskell (programming language)|Haskell]],<ref>[http://hackage.haskell.org/package/mathlink Haskell packages]</ref> [[AppleScript]],<ref>[http://www.unisoftwareplus.com/products/mathlinkosax/ Unisoftware plus]</ref> [[Racket (programming language)|Racket]],<ref>[http://www.cs.utah.edu/~czhu/SchemeLink/mrmma.html MrMathematica website]</ref> [[Visual Basic]],<ref>[http://library.wolfram.com/infocenter/TechNotes/4710/ Mathematica for ActivX]</ref> [[Clojure]]<ref>{{cite web |url= http://clojuratica.weebly.com/ |title=Clojuratica |work=clojuratica.weebly.com |year=2013 |accessdate=14 June 2013}}</ref> [[MATLAB]]<ref name=\"mathworks.com\">{{cite web|url=http://www.mathworks.com/matlabcentral/fileexchange/6044-mathematica-symbolic-toolbox-for-matlab-version-2-0|title=Mathematica Symbolic Toolbox for MATLAB--Version 2.0|accessdate=18 May 2011}}</ref>\n|}\n|-\n! [[MATLAB]]\n| {{Depends|with extension}}<ref>{{cite web|url=http://www.mathworks.com/products/compiler/|title=MATLAB Compiler|last=[[Mathworks]]|accessdate=18 May 2011}}</ref>\n| {{Depends|}}with extension library<ref>{{cite web|url=http://www.mathworks.com/products/symbolic/|title=Symbolic Math Toolbox |last=[[Mathworks]]|accessdate=18 May 2011}}</ref>\n| {{yes}}<ref>{{cite web|url=http://www.mathworks.com/discovery/object-oriented-programming.html|title=Object-Oriented Programming in MATLAB|last=[[Mathworks]]|accessdate=18 May 2011}}</ref>\n| {{yes}}\n| {{yes}}\n| {{yes}}<ref>{{cite web|url=http://www.mathworks.com/matlabcentral/fileexchange|title=MATLAB File Exchange|accessdate=18 May 2011}}</ref>\n|\n{|\n|{{yes|}} Through MEX<ref>{{cite web|url=http://www.mathworks.com/support/tech-notes/1600/1605.html|title=MEX-files Guide|last=[[Mathworks]]|accessdate=18 May 2011}}</ref> files: [[C (programming language)|C]], [[C++]], [[Fortran]]. Also compiled [[Java (programming language)|Java]] and [[ActiveX]] components.\n|-\n|{{no|}}[[Mathematica]]<ref name=\"mathworks.com\"/>\n|}\n\n| {{Depends|}}[[.NET Framework|.NET]],<ref>{{cite web|url=http://www.mathworks.com/products/netbuilder/|title=MATLAB Builder NE for Microsoft .NET Framework |last=[[Mathworks]]|accessdate=18 May 2011}}</ref> [[Java (programming language)|Java]],<ref>{{cite web|url=http://www.mathworks.com/products/javabuilder/|title=MATLAB Builder JA for Java language |last=[[Mathworks]]|accessdate=18 May 2011}}</ref> [[Microsoft Excel|Excel]]<ref>{{cite web|url=http://www.mathworks.com/products/matlabxl/|title=MATLAB Builder EX for Microsoft Excel |accessdate=18 May 2011}}</ref>\n|-\n! [[MLAB]]\n| {{no}}\n| {{yes}}\n| {{no}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}} \n|-\n! [[Perl Data Language|PDL]]\n|{{yes}}<ref>{{cite web|url=http://www.perlmonks.org/?node_id=215299|accessdate=24 January 2013|title=Perlmonks}}</ref>\n|{{no}}\n|{{yes}}<ref>{{cite web|url=http://perl-begin.org/topics/object-oriented/|title=O'Reilly tutorial|accessdate=24 January 2013}}</ref>\n|{{yes}}<ref>{{cite web|url=http://www.perl.com/pub/1999/10/perltk/|title=PerlTK tutorial|accessdate=24 January 2013}}</ref>\n|{{yes}}\n|{{yes}}<ref>{{cite web|url=http://www.cpan.org/|title=CPAN|accessdate=24 January 2013}}</ref>\n|{{yes|}}C via perlXS; C, Python, F77, etc. via Inline\n|{{yes|}}Perl, C<ref>{{cite web|url=http://www.perlmonks.org/?node_id=830663|title=Calling Perl from C|accessdate=24 January 2013}}</ref>\n|-\n! [[R (programming language)|R]]\n| {{No}}\n| {{Yes| Yes and extended via Ryacas package}}\n| {{yes}}<ref>{{cite book|last=R Development Core Team|title=R Language Definition|date=13 April 2011|chapter=Object-oriented programming|isbn=3-900051-13-5 |url=https://cran.r-project.org/doc/manuals/R-lang.html#Object_002doriented-programming|accessdate=18 May 2011}}</ref>\n| {{depends|}} Via the tcltk library\n| {{yes}}\n| {{yes}}<ref>{{cite web|url=https://cran.r-project.org/web/packages/|title=CRAN: Contributed Packages|accessdate=18 May 2011}}</ref>\n|\n{|\n|{{Yes|}} [[C (programming language)|C]], [[C++]], [[Fortran]]<ref>{{cite book|last=Hornik|first=Kurt |title=The R FAQ|year=2011|isbn=3-900051-08-9|url=https://cran.r-project.org/doc/FAQ/R-FAQ.html#R-Basics}}</ref>\n|-\n|{{Depends|}}[[MATLAB]]<ref>{{cite web|url=https://cran.r-project.org/web/packages/R.matlab/index.html|title=CRAN: R.matlab package|last=Bengtsson|first=Henrik|author2=Jason Riedy|accessdate=18 May 2011}}</ref>\n|-\n|{{No|}}[[Python (programming language)|Python]]<ref>{{cite web|url=http://rjython.r-forge.r-project.org/|title=rJython R package|last=Grothendieck|first=G.|author2=Carlos J. Gil Bellosta|accessdate=18 May 2011}}</ref>\n|}\n|\n{|\n|{{Depends|}}[[Microsoft Excel|Excel]]<ref>{{cite web|url=https://cran.r-project.org/web/packages/RExcelInstaller/index.html|title=CRAN: RExcelInstaller package|last=Neuwirth|first=Erich|accessdate=18 May 2011|deadurl=yes|archiveurl=https://web.archive.org/web/20110525215422/http://cran.r-project.org/web/packages/RExcelInstaller/index.html|archivedate=May 25, 2011|df=mdy-all}}</ref>\n|-\n|{{No|}}[[Python (programming language)|Python]]<ref>{{cite web|url=http://rpy.sourceforge.net/index.html|title=A simple and efficient access to R from Python|accessdate=18 May 2011}}</ref>\n|-\n|{{Depends|}}[[SAS (software)|SAS]]<ref>{{cite web|url=http://support.sas.com/rnd/app/studio/Rinterface2.html|title=R Interface Now Available in SAS/IML Studio|accessdate=10 October 2016}}</ref>\n|}\n|-\n! [[SageMath]]\n| {{no}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{yes}}\n| {{yes}}<ref>{{cite web|url=http://www.sagemath.org/download-packages.html|title=Additional Packages|accessdate=5 June 2013}}</ref>\n| {{yes|}} Many languages<ref>{{cite web|title=Interpreter Interfaces|url=http://www.sagemath.org/doc/reference/interfaces/index.html|accessdate=6 June 2013}}</ref><ref>{{cite web|title=C/C++ Library Interfaces|url=http://www.sagemath.org/doc/reference/libs/index.html|accessdate=6 June 2013}}</ref><ref>{{cite web|title=Using Compiled Code Interactively|url=http://www.sagemath.org/doc/numerical_sage/using_compiled_code_iteractively.html|accessdate=6 June 2013}}</ref>\n| {{yes}} (any language that can call Python)\n|-\n! [[SAS (software)|SAS]]\n| {{no}}\n| {{no}}\n| {{Depends|}}DS2 and SCL\n| {{yes}}\n| {{no}}\n| {{no}}\n| \n{|\n| {{yes|}}[[C (programming language)|C]], [[Java (programming language)|Java]]\n|-\n| {{Depends|}}[[R (programming language)|R]] (requires IML)\n|-\n|}\n| {{no}}\n|-\n! [[TK Solver]]\n| {{Depends|with extension}}\n| {{no}}\n| {{yes}}\n| {{Depends|with extension}}\n| {{yes}}\n| {{yes}}\n| {{Depends|}}[[.NET Framework|.NET]] [[Microsoft Excel|Excel]]\n| {{Depends|}}Unknown\n|}\n\n== Libraries ==\n\n=== General ===\n{| class=\"wikitable sortable\" style=\"font-size: smaller; text-align: center; width: auto;\"\n|-\n! style=\"width: 12em\"|\n! Creator\n! Language\n! First public release\n! Latest stable version\n! Cost ([[United States dollar|USD]])\n! License\n! Notes\n|-\n! [[ALGLIB]]\n| Sergey Bochkanov\n| C++, C#, FreePascal\n| 2006\n| 3.12.0 / August 2017\n| Dual licensed\n| GPL/commercial\n| General purpose numerical analysis library. Cross-platform (Windows, *nix).\n|-\n! [[Armadillo (C++ library)|Armadillo]]\n| [[NICTA]]\n| C++\n| 2009\n| 3.900 / 2013\n| {{free}}\n| [[Mozilla Public License|MPL]]\n| C++ template library for linear algebra; includes various decompositions and factorisations; syntax ([[Application Programming Interface|API]]) is similar to [[MATLAB]].\n|-\n! [[GNU Scientific Library]]\n| GNU Project\n| C\n| 1996\n| 2.5 / 14 June 2018\n| {{free}}\n| [[GNU General Public License|GPL]]\n| General purpose numerical analysis library. Targets GNU/Linux, can be built on almost any *nix OS with Ansi C compiler.\n|-\n! [[ILNumerics]]\n| H. Kutschbach\n| C#, PowerShell\n| 2007\n| 1.3.14 / August 2008\n| {{nonfree}}\n| Proprietary\n| aims .Net/mono, 2D/3D plottings (beta)\n|-\n! [[IMSL Numerical Libraries]]\n| [[Rogue Wave Software]]\n| C, Java, C#, Fortran, Python\n| 1970\n| many components\n| {{nonfree|Not free}}\n| Proprietary\n| General purpose numerical analysis library.\n|-\n! [[Math.NET Numerics]]\n| C. Rüegg, M. Cuda, et al.\n| C#, F#, C, PowerShell\n| 2009\n| 4.7.0 / November 2018\n| {{free}}\n| [[MIT License|MIT/X11]]\n| General purpose numerical analysis and statistics library for the [[.NET Framework]] and [[Mono (software)|Mono]], with optional support for native providers.\n|-\n! [[NAG Numerical Library]]\n| [[Numerical Algorithms Group|The Numerical Algorithms Group]]\n| C, Fortran\n| 1971\n| many components\n| {{nonfree|Not free}}\n| Proprietary\n| General purpose numerical analysis library.\n|-\n! [[NMath]]\n| [[CenterSpace Software]]\n| C#\n| 2003\n| 6.2 / March 2016\n| {{nonfree|$995}}\n| Proprietary\n| Math and statistical libraries for the [[.NET Framework]]\n|-\n! [[SciPy]]\n| scipy.org community\n| Python\n| 2001\n| 1.2.1 / February 2019\n| {{free}}\n| [[BSD licenses|BSD]]\n| Adds numerical programming capabilities to the [[Python (programming language)|Python programming language]]. Related to [[NumPy]], and therefore connected to the previous Numeric and Numarray packages for Python\n|}\n\n=== Operating-system support ===\nThe [[operating system]]s the software can run on natively (without [[emulator|emulation]]).\n\n{| class=\"wikitable sortable\" style=\"text-align: center; width: auto;\"\n|-\n! style=\"width: 12em\" |\n! [[Microsoft Windows|Windows]]\n! [[macOS]]\n! [[Linux]]\n! [[Berkeley Software Distribution|BSD]]\n! [[Unix]]\n! [[DOS]]\n! [[Android (operating system)|Android]]\n|-\n! [[ALGLIB]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{yes}}\n|-\n! [[GNU Scientific Library]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{yes}}\n|-\n! [[ILNumerics]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n|-\n! [[IMSL Numerical Libraries]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{yes}}\n| {{no}}\n| {{no}}\n|-\n! [[Math.NET Numerics]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{no}}\n|-\n! [[NAG Numerical Library]]\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n|-\n! [[NMath]]\n| {{yes}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n| {{no}}\n|-\n! [[SciPy]] (Python packages)\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{yes}}\n| {{no}}\n|}\n\n== See also ==\n*[[Comparison of computer algebra systems]]\n*[[Comparison of deep-learning software]]\n*[[Comparison of statistical packages]]\n*[[List of numerical-analysis software]]\n\n== Footnotes ==\n{{reflist|group=Note}}\n\n\n== References ==\n{{reflist|30em}}\n\n\n{{Numerical analysis software}}\n\n[[Category:Comparisons of mathematical software|Numerical analysis software]]\n[[Category:Numerical software| ]]"
    },
    {
      "title": "Comparison of optimization software",
      "url": "https://en.wikipedia.org/wiki/Comparison_of_optimization_software",
      "text": "{{lead too short|date=October 2010}}\nGiven a system transforming a set of inputs to output values, described by a [[Function (mathematics)|mathematical function]] ''f'', [[Mathematical optimization|optimization]] refers to the generation and selection of a best solution from some set of available alternatives,<ref>\"[http://glossary.computing.society.informs.org/ver2/mpgwiki/index.php?title=Extra:Mathematical_programming The Nature of Mathematical Programming],\" ''Mathematical Programming Glossary'', INFORMS Computing Society.</ref> by systematically choosing input values from within an allowed set, computing the value of the function, and recording the best value found during the process. Many real-world and theoretical problems may be modeled in this general framework. For example, the inputs can be design parameters of a motor, the output can be the power consumption, or the inputs can be business choices and the output can be the obtained profit, or the inputs can describe the configuration of a physical system and the output can be its energy.\n\nAn [[optimization problem]] can be represented in the following way\n:''Given:'' a [[function (mathematics)|function]] ''f'' : ''A'' <math>\\to</math> '''R''' from some [[Set (mathematics)|set]] ''A'' to the [[real number]]s\n:''Search for:'' an element ''x''<sub>0</sub> in ''A'' such that ''f''(''x''<sub>0</sub>) ≤ ''f''(''x'') for all ''x'' in ''A'' (\"minimization\").\n\nTypically, ''A'' is some [[subset]] of the [[Euclidean space]] '''R'''<sup>''n''</sup>, often specified by a set of ''[[constraint (mathematics)|constraint]]s'', equalities or inequalities that the members of ''A'' have to satisfy. Maximization can be reduced to minimization by multiplying the function by minus one.\n\nThe use of optimization software requires that the function ''f'' is defined in a suitable programming language and linked to the optimization software. The optimization software will deliver input values in ''A'', the software module realizing ''f'' will deliver the computed value ''f''(''x''). In this manner, a clear separation of concerns is obtained: different optimization software modules can be easily tested on the same function ''f'', or a given optimization software can be used for different functions ''f''.\n\nThe following tables provide a comparison of notable optimization software libraries, either specialized or general purpose libraries with significant optimization coverage.\n\n{| class=\"wikitable sortable\" style=\"font-size: smaller; text-align: center; width: auto;\"\n|-\n! style=\"width: 12em\"| Name\n! Language\n! Latest stable version\n! Academic/noncommercial<br>use is free\n! Can be used in<br>proprietary apps\n! License\n! Notes\n|-\n! [[ALGLIB]]\n| C++, C#, FreePascal, VBA\n| 3.8.0 / August 2013\n| {{yes}}\n| {{yes}}\n| Dual (Commercial, GPL)\n| General purpose library, includes optimization package.\n|-\n! [[AMPL]]\n| C\n| October 2013\n| {{yes}}\n| {{yes}}\n| Dual (Commercial, academic)\n| A popular algebraic modeling language for linear, mixed-integer and nonlinear optimization. Student and AMPL for courses versions are available for free.\n|-\n! [[APMonitor]]\n| Fortran, C++, Python, Matlab, Julia\n| 0.6.2 / March 2016\n| {{yes}}\n| {{yes}}\n| Dual (Commercial, academic)\n| A differential and algebraic modeling language for mixed-integer and nonlinear optimization. Freely available interfaces for Matlab, Python, and Julia.\n|-\n! [[Artelys Knitro]]\n| C, C++, C#, Python, Java, Julia, Matlab, R\n| 11.1 / November 2018\n| {{no}}\n| {{yes}}\n| Commercial, Academic, Trial\n| General purpose library, specialized in nonlinear optimization. Handles mixed-integer problems (MINLP) and mathematical programs with equilibrium constraints ([[Mathematical programming with equilibrium constraints|MPEC]]). Specialized algorithms for nonlinear least squares problems.\n|-\n![[FICO Xpress]]\n|Mosel, BCL, C, C++, Java, R Python, Matlab, .Net, VB6\n|8.5 / Aug 2018\n|{{Yes}}\n|{{Yes}}\n|Commercial, academic, community, trial\n|Suite of Optimization Technologies and Solutions. Includes: Solver technologies including (LP (Simplex & Barrier), MIP, MIQP, MIQCQP, MISOCP, MINLP QP, QCQP, SOCP, NLP (SLP & Interior Point); An algebraic modelling and procedural programming language; an Integrated Development Environment; Supports for a range of execution services; Support for packaging of optimization models and services as software solutions\n|-\n! [[Gekko (optimization software)|GEKKO]]\n| Python\n| 0.2rc4 / April 2019\n| {{yes}}\n| {{yes}}\n| Dual (Commercial, academic)\n| GEKKO is a Python package for machine learning and optimization of mixed-integer and differential algebraic equations. It is coupled with large-scale solvers for linear, quadratic, nonlinear, and mixed integer programming (LP, QP, NLP, MILP, MINLP). Modes of operation include parameter regression, data reconciliation, real-time optimization, dynamic simulation, and nonlinear predictive control.\n|-\n![[GNU Linear Programming Kit]]\n| C\n| 4.52 / July 2013\n| {{yes}}\n| {{no}}\n| GPL\n| Free library for linear programming (LP) and mixed integer programming (MIP).\n|-\n! [[GNU Scientific Library]]\n| C\n| 1.16 / July 2013 \n| {{yes}}\n| {{no}}\n| GPL\n| Free library provided by GNU project.\n|-\n! [[Gurobi]]\n| C, C++, C#, Java, .Net, Matlab, Python, R\n| 8.1 / Sep 2018\n| {{Yes}}\n| {{Yes}}\n| Commercial, academic, trial\n| Optimization library. Handles mixed-integer linear problems, convex quadratic constraints and objective, multi-objective optimization and SOS constraints\n|-\n! [[IMSL Numerical Libraries]]\n| C, Java, C#, Fortran, Python\n| many components\n| {{no}}\n| {{yes}}\n| Proprietary\n| \n|-\n! [[LIONsolver]]\n| C++, Java \n| 2.0.198 / October 2011\n| {{yes}}\n| {{yes}}\n| Proprietary\n| Support for interactive and learning optimization,\naccording to RSO principles\n.<ref>{{cite book\n|title=Reactive Search and Intelligent Optimization\n|last=Battiti\n|first=Roberto\n|authorlink=\n|author2=Mauro Brunato |author3=Franco Mascia\n |year=2008\n|publisher=[[Springer Verlag]]\n|location=\n|isbn=978-0-387-09623-0\n}}\n</ref>\n|-\n! [[Math Kernel Library]] (MKL)\n| C++, Fortran\n| 11.1 / October 2013\n| {{no}}\n| {{yes}}\n| Proprietary\n| Numerical library from Intel. MKL is specialized on linear algebra,<br>but contains some optimization-related functionality.\n|-\n|-\n![[MIDACO]]\n|C++, C#, Python, Matlab, Octave, Fortran, R, Java, Excel, VBA, Julia\n|6.0 / Mar 2018\n|{{Yes}}\n|{{Yes}}\n|Dual (Commercial, academic) \n|Lightweight software tool for single- and multi-objective optimization. Supporting MINLP and parallelization.\n|-\n|-\n! [[NAG Numerical Libraries]]\n| C, Fortran\n| Mark 26 / October 2017\n| {{no}}\n| {{yes}}\n| Proprietary\n| \n|-\n! [[NMath]]\n| C#\n| 5.3 / May 2013\n| {{no}}\n| {{yes}}\n| Proprietary\n| C# numerical library built on top of MKL.\n|-\n! [[OptaPlanner]]\n| Java\n| 7.9.0.Final / July 2018\n| {{yes}}\n| {{yes}}\n| ASL\n| Lightweight optimization solver in Java\n|-\n! [[SciPy]]\n| Python\n| 0.13.1 / November 2013\n| {{yes}}\n| {{yes}}\n| BSD\n| General purpose numerical and scientific computing library for Python.\n|-\n|}\n\n== See also ==\n\n* [[List of optimization software]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* [http://www.orms-today.org/surveys/LP/LP-survey.html OR/MS Today: 2013 Linear Programming Software Survey]\n* [http://www.orms-today.org/surveys/nlp/nlp.html OR/MS Today: 1998 Nonlinear Programming Software Survey]\n\n{{Mathematical optimization software}}\n\n[[Category:Numerical software| ]]"
    },
    {
      "title": "List of numerical libraries",
      "url": "https://en.wikipedia.org/wiki/List_of_numerical_libraries",
      "text": "This is a list of [[WP:N|notable]] '''numerical libraries''', which are [[Library (computing)|libraries]] used in [[software development]] for performing [[numerical analysis|numerical]] calculations.  It is not a complete listing but is instead a list of numerical libraries with articles on Wikipedia, with few exceptions.\n\nThe choice of a typical library depends on a diverse range of requirements such as: desired features (for e.g.: large dimensional linear algebra, parallel computation, partial differential equations), commercial/opensource nature, readability of API, portability or platform/compiler dependence (for e.g.: Linux, Windows, Visual C++, GCC), performance in speed, ease-of-use, continued support from developers, standard compliance, specialized optimization in code for specific application scenarios or even the size of the code-base to be installed.\n\nAs we find comprehensive surveys rarely available, there is almost always (at least initially) a difficult choice among a number of possible libraries.\nOften it tends to be at the discretion of the user based on his own taste and comforts, only due to the lack of proper information.\n\n== Multi-language ==\n* [[ALGLIB]] is an open source numerical analysis library which may be used from C++, C#, FreePascal, Delphi, VBA.\n* [[ArrayFire]] is a high performance open source software library for parallel computing with an easy-to-use API.\n* [[IMSL Numerical Libraries]] are libraries of numerical analysis functionality implemented in standard programming languages like C, Java, C# .NET, Fortran, and Python.\n* The [[NAG Numerical Library|NAG Library]] is a collection of mathematical and statistical routines for multiple programming languages (C, C++, Fortran, Visual Basic, Java, Python and C#) and packages (MATLAB, Excel, R, LabVIEW).\n* [[GNU Octave]] is an open source high level programming language and library, including a command line interface and GUI, analogous to commercial alternatives such as Maple, MATLAB, Mathematica, etc. APIs, functions and libraries can be called from many platforms, including high level engineering programs, where functions are, in many cases, seamlessly interpreted and integrated in similar fashion to MATLAB. It also can be used with batch orientation.\n* [[librsb]] is an open source library for high performance sparse matrix computations providing multi-threaded primitives to build iterative solvers (implements also the Sparse [[BLAS]] standard). It can be used from C, C++, Fortran, and a dedicated [[GNU Octave]] package.\n\n==[[C (programming language)|C]]==\n* [[BLOPEX]] (Block Locally Optimal Preconditioned Eigenvalue Xolvers) is an [[open source|open-source]] [[Library (computing)|library]] for the [[scalability|scalable]] ([[parallel computing|parallel]]) solution of eigenvalue problems.\n* [[FFTW]] (Fastest Fourier Transform in the West) is a software library for computing Fourier and related transforms.\n* [[GNU Scientific Library]], a popular, free numerical analysis library implemented in [[C (programming language)|C]].\n* [[GNU Multi-Precision Library]] is a library for doing [[arbitrary-precision arithmetic]].\n* [[hypre]] (High Performance Preconditioners) is an [[open source|open-source]] [[Library (computing)|library]] of [[computer code|routines]] for [[scalability|scalable]] ([[parallel computing|parallel]]) solution of linear systems and preconditioning.\n* [[LabWindows/CVI]] is an ANSI C [[Integrated Development Environment|IDE]] that includes built-in libraries for analysis of raw measurement data, signal generation, windowing, filter functions, signal processing, linear algebra, array and complex operations, curve fitting and statistics.\n* [[Lis (linear algebra library)|Lis]] is a scalable parallel library for solving systems of linear equations and eigenvalue problems using iterative methods.\n* [[Intel MKL]], Intel Math Kernel Library (in C), a library of optimized math routines for science, engineering, and financial applications, written in C/C++ and Fortran. Core math functions include BLAS, LAPACK, ScaLAPACK, sparse solvers, fast Fourier transforms, and vector math.\n* [[Portable, Extensible Toolkit for Scientific Computation]] ('''PETSc'''), is a suite of [[data structures]] and [[computer code|routines]] for the [[scalability|scalable]] ([[parallel computing|parallel]]) solution of [[scientific computing|scientific applications]] modeled by [[partial differential equations]].\n* [[SLEPc]] '''Scalable Library for Eigenvalue Problem Computations''' is a [[PETSc]]-based [[open source|open-source]] [[Library (computing)|library]] for the [[scalability|scalable]] ([[parallel computing|parallel]]) solution of eigenvalue problems.\n\n==[[Boost (C++ libraries)|C++]]==\n* [[Adept (C++ library)|Adept]] is a combined [[automatic differentiation]] and [[Array programming|array]] library.\n* [[Advanced Simulation Library]] is free and open source hardware accelerated multiphysics simulation software with an [[OpenCL]]-based internal computational engine.\n* [[ALGLIB]] is an open source / commercial numerical analysis library with C++ version\n* [[Armadillo (C++ library)|Armadillo]] is a C++ linear algebra library (matrix and vector maths), aiming towards a good balance between speed and ease of use. It employs template classes, and has optional links to BLAS and LAPACK. The syntax ([[Application Programming Interface|API]]) is similar to [[MATLAB]].\n* Blaze  is an open-source, high-performance C++ math library for dense and sparse arithmetic. \n* [[Blitz++]] is a high-performance vector mathematics library written in C++.\n* [[Boost (C++ libraries)|Boost.uBLAS]] C++ libraries for numerical computation\n* [[deal.II]] is a library supporting all the finite element solution of partial differential equations.\n* [[Dlib]] is a modern C++ library with easy to use linear algebra and optimization tools which benefit from optimized BLAS and LAPACK libraries.\n* [[Eigen (C++ library)|Eigen]] is a vector mathematics library with performance comparable with Intel's [[Math Kernel Library]]\n* [[Hermes Project]]: C++/Python library for rapid prototyping of space- and space-time adaptive hp-FEM solvers.\n* [[IML++]] is a C++ library for solving linear systems of equations, capable of dealing with dense, sparse, and distributed matrices.\n* [[IT++]] is a C++ library for linear algebra (matrices and vectors), signal processing and communications. Functionality similar to MATLAB and Octave.\n* [[LAPACK++]], a C++ wrapper library for [[LAPACK]] and [[BLAS]]\n* [[Intel MKL]], Intel Math Kernel Library (in C and C++), a library of optimized math routines for science, engineering, and financial applications, written in C/C++ and Fortran. Core math functions include BLAS, LAPACK, ScaLAPACK, sparse solvers, fast Fourier transforms, and vector math.\n* [[mlpack]] is an open-source library for machine learning, exploiting C++ language features to provide maximum performance and flexibility while providing a simple and consistent API \n* [[Matrix Template Library|MTL4]] is a generic [[C++]] template library providing sparse and dense BLAS functionality. MTL4 establishes an intuitive interface (similar to [[MATLAB]]) and broad applicability thanks to [[Generic programming]].\n* The [[NAG Numerical Library|NAG Library]] has C++ API\n* [[Number Theory Library|NTL]] is a C++ library for number theory.\n* [[Trilinos]] is an effort to develop algorithms and enabling technologies for the solution of large-scale, complex multi-physics engineering and scientific problems. It is a collection of ''packages''.\n\n== [[Delphi (programming language)|Delphi]]  ==\n* Analytics & Physics: commercial Delphi libraries for symbolic and numerical calculations (including symbolic derivatives) and working with physical values and units of measurement.\n* [[ALGLIB]] - an open source numerical analysis library.\n\n== .NET Framework languages [[C Sharp (programming language)|C#]], [[F Sharp (programming language)|F#]], [[Visual Basic .NET|VB.NET]] and [[Windows PowerShell|PowerShell]] ==\n* [[Accord.NET]] is a collection of libraries for scientific computing, including numerical linear algebra, optimization, statistics, artificial neural networks, machine learning, signal processing and computer vision. LGPLv3, partly GPLv3.\n* [[AForge.NET]] is a computer vision and artificial intelligence library. It implements a number of genetic, fuzzy logic and machine learning algorithms with several architectures of artificial neural networks with corresponding training algorithms. LGPLv3 and partly GPLv3.\n* [[ALGLIB]] is an open source numerical analysis library with C# version. Dual licensed: GPLv2+, commercial license.\n* [[ILNumerics.Net]] Commercial high performance, typesafe numerical array classes and functions for general math, FFT and linear algebra, aims .NET/mono, 32&64 bit, script-like syntax in C#, 2D & 3D plot controls, efficient memory management.\n* [[IMSL Numerical Libraries]] have C# version (commercially licensed).\n* [[Math.NET Numerics]] aims to provide methods and algorithms for numerical computations in science, engineering and every day use. Covered topics include special functions, linear algebra, probability models, random numbers, interpolation, integral transforms and more. Free software under MIT/X11 license.\n* [[Measurement Studio]] is a commercial integrated suite UI controls and class libraries for use in developing test and measurement applications. The analysis class libraries provide various digital signal processing, signal filtering, signal generation, peak detection, and other general mathematical functionality.\n* [[ML.NET]] is a [[free software]] [[machine learning]] [[Library (computing)|library]] for the [[C Sharp (programming language)|C#]] programming language.<ref name=\"visu_Open\">{{Cite web | title = Open Source, Cross-Platform ML.NET Simplifies Machine Learning -- Visual Studio Magazine | author = David Ramel | work = Visual Studio Magazine | date = 2018-05-08 | accessdate = 2018-05-10 | url = https://visualstudiomagazine.com/articles/2018/05/08/ml-net-framework.aspx | quote = }}</ref><ref name=\"onms_Micr\">{{Cite web | title = Microsoft debuts ML.NET cross-platform machine learning framework | author = Kareem Anderson | work = On MSFT | date = 2017-05-09 | accessdate = 2018-05-10 | url = https://www.onmsft.com/news/microsoft-debuts-ml-net-cross-platform-machine-learning-framework | quote = }}</ref>\n* The [[NAG Numerical Library|NAG Library]] has C# API. Commercially licensed.\n* [[NMath]] by [[CenterSpace Software]]: Commercial numerical component libraries for the .NET platform, including signal processing (FFT) classes, a linear algebra (LAPACK & BLAS) framework, and a statistics package.\n\n==[[Fortran]]==\n* [[BLAS]] (Basic Linear Algebra Subprograms) is a de facto [[application programming interface]] standard for publishing libraries to perform basic [[linear algebra]] operations such as [[Vector space|vector]] and [[matrix multiplication]].\n* [[CERN Program Library|CERNLIB]] is a collection of [[FORTRAN 77]] libraries and modules.\n* [[EISPACK]] is a [[software library]] for [[numerical computation]] of [[eigenvalues]] and [[eigenvectors]] of matrices, written in [[FORTRAN]]. It contains subroutines for calculating the eigenvalues of nine classes of [[matrix (Mathematics)|matrices]]:  complex general, complex [[Hermitian]], real general, real symmetric, real symmetric [[band matrix|banded]], real symmetric tridiagonal, special real tridiagonal, generalized real, and generalized real symmetric matices.\n* [[IMSL Numerical Libraries]] are cross-platform libraries containing a comprehensive set of mathematical and statistical functions that can be embedded in a users application.\n* [[Harwell Subroutine Library]] is a collection of [[Fortran 77]] and 95 codes that address core problems in numerical analysis.\n* [[LAPACK]], the '''Linear Algebra PACKage''', is a [[software library]] for [[numerical computation|numerical computing]] originally written in [[Fortran|FORTRAN 77]] and now written in [[Fortran|Fortran 90]].\n* [[LINPACK]] is a software [[library (computer science)|library]] for performing numerical [[linear algebra]] on digital computers.  It was written in [[Fortran]] by [[Jack Dongarra]], Jim Bunch, [[Cleve Moler]], and Pete Stewart, and was intended for use on [[supercomputer]]s in the 1970s and early 1980s.  It has been largely superseded by [[LAPACK]], which will run more efficiently on modern architectures.\n* [[Lis (linear algebra library)|Lis]] is a scalable parallel library for solving systems of linear equations and eigenvalue problems using iterative methods.\n* [[MINPACK]]  is a library of [[FORTRAN]] subroutines for the solving of systems of nonlinear equations, or the [[least squares]] minimization of the residual of a set of [[System of linear equations|linear]] or nonlinear equations.\n* The [[NAG Numerical Library|NAG Fortran Library]] is a collection of mathematical and statistical routines for Fortran.\n* [[Naval Observatory Vector Astrometry Subroutines|NOVAS]] is a software library for astrometry-related numerical computations.  Both [[Fortran]] and [[C (programming language)|C]] versions are available.\n* [[Netlib]] is a repository of scientific computing software which contains a large number of separate programs and libraries including [[BLAS]], [[EISPACK]], [[LAPACK]] and others.\n* [[Physics Analysis Workstation|PAW]] is a free [[data analysis]] package developed at [[CERN]].\n* [[Portable, Extensible Toolkit for Scientific Computation]] ('''PETSc'''), is a suite of [[data structures]] and [[computer code|routines]] for the [[scalability|scalable]] ([[parallel computing|parallel]]) solution of [[scientific computing|scientific applications]] modeled by [[partial differential equations]].\n* [[QUADPACK]] is a [[FORTRAN 77]] library for [[numerical integration]] of one-dimensional functions\n* [[SLATEC]] is a [[FORTRAN 77]] library of over 1400 general purpose mathematical and statistical routines.\n* [[SOFA (Astronomy)|SOFA]] is a collection of subroutines that implement official [[International Astronomical Union|IAU]] [[algorithms]] for [[astronomy|astronomical]] computations.  Both [[Fortran]] and [[C (programming language)|C]] versions are available.\n* [[ARPACK]] is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems.\n\n==[[Java programming language|Java]]==\n\n* [[ND4J (software)|ND4J]] is an [[open source|open-source]] library that supports n-dimensional (ND) arrays, similar to [[NumPy]]. It runs on distributed [[Graphics processing unit|GPU]]s or CPUs cross-platform, and leverages Spark for parallel computation. It supports the [[deep learning]] library [[Deeplearning4j]].\n* [[Apache Commons]], is an [[open source|open-source]] for creating reusable Java components.  It has numerical packages for linear algebra and non-linear optimization.\n* [[Colt (libraries)|Colt]] provides a set of Open Source Libraries for High Performance Scientific and Technical Computing. \n* [[Efficient Java Matrix Library|Efficient Java Matrix Library (EJML)]] is an open-source linear algebra library for manipulating dense matrices. \n* [[JAMA (numerical linear algebra library)|JAMA]], a numerical [[linear algebra]] toolkit for the [[Java programming language]]. No active development has taken place since 2005, but it still one of the more popular linear algebra packages in Java.\n* [[Jblas: Linear Algebra for Java]], a linear algebra library which is an easy to use wrapper around [[BLAS]] and [[LAPACK]].\n* [[Parallel Colt]] is an open source library for scientific computing.  A parallel extension of [[Colt (libraries)|Colt]].\n* [[Matrix Toolkit Java]] is a linear algebra library based on [[BLAS]] and [[LAPACK]].\n* [[ojAlgo]] is an open source Java library for mathematics, linear algebra and optimisation.\n* [[exp4j]] is a small [[Java (programming language)|Java]] library for evaluation of mathematical expressions.\n\n==[[Scala programming language|Scala]]==\n* [[ND4S]] is a scientific computing library for Scala that includes support for n-dimensional arrays on the JVM.\n\n==[[Perl programming language|Perl]]==\n*[[Perl Data Language]] gives standard Perl the ability to compactly store and speedily manipulate the large ''N''-dimensional data arrays.\n\n==[[Python programming language|Python]]==\n* [[NumPy]], a BSD-licensed library that adds support for the manipulation of large, multi-dimensional arrays and matrices; it also includes a large collection of high-level mathematical functions. NumPy serves as the backbone for a number of other numerical libraries, notably [[SciPy]]. De facto standard for matrix/tensor operations in Python.\n* [[SageMath]] is a large mathematical software application which integrates the work of nearly 100 [[free software]] projects and supports linear algebra, combinatorics, numerical mathematics, calculus, and more.\n* [[SciPy]], a large BSD-licensed library of scientific tools. De facto standard for scientific computations in Python.\n* [[ScientificPython]], a library with a different set of scientific tools\n* [[SymPy]] , a library based on New BSD license for symbolic computation. Features of Sympy range from basic symbolic arithmetic to calculus, algebra, discrete mathematics and quantum physics.\n\n==Others==\n* [[XNUMBERS]] &mdash; Multi Precision Floating Point Computing and Numerical Methods for [[Microsoft Excel]].\n* INTLAB &mdash; [[Interval arithmetic]] library for [[MATLAB]].\n\n==See also==\n* [[Comparison of computer algebra systems]]\n* [[Comparison of numerical analysis software]]\n* [[List of graphing software]]\n* [[List of numerical analysis software]]\n* [[List of optimization software]]\n* [[List of statistical packages]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* [http://mathforum.org/library/topics/num_analysis/ The Math Forum - Math Libraries], an extensive list of mathematical libraries with short descriptions\n\n{{DEFAULTSORT:List Of Numerical Libraries}}\n[[Category:Lists of software|Numerical analysis software]]\n[[Category:Mathematics-related lists|Numerical analysis software]]\n[[Category:Numerical libraries| ]]\n[[Category:Numerical software|*Software]]\n\n[[ja:数値解析ソフトウェア]]"
    },
    {
      "title": "List of numerical-analysis software",
      "url": "https://en.wikipedia.org/wiki/List_of_numerical-analysis_software",
      "text": "Listed here are notable end-user computer applications intended for use with [[numerical analysis|numerical]] or [[data analysis]]:\n\n<!--Entries in these lists should have a Wikipedia article based on independent sources. See also WP:GNG and WP:WTAF.-->\n== Numerical-software packages ==\n* [[Analytica (software)|Analytica]] is a widely used proprietary tool for building and analyzing numerical models. It is a [[Declarative programming|declarative]] and [[visual programming language]] based on [[influence diagrams]].\n* [[FlexPro]] is a program for data analysis and presentation of measurement data. It provides a rich Excel-like user interface and its built-in vector programming language [[FPScript]] has a syntax similar to MATLAB.\n* [[FreeMat]], an [[Open-source software|open-source]] MATLAB-like environment with a [[GNU General Public License|GPL license]].\n* [[GNU Octave]] is a high-level language, primarily intended for numerical computations. It provides a convenient command-line interface for solving linear and nonlinear problems numerically, and for performing other numerical experiments using a language that is mostly compatible with MATLAB. The 4.0 and newer releases of Octave include a GUI. A number of independently developed [[Linux]] programs ([[Cantor (Software)|Cantor]], [[KAlgebra]]) also offer GUI front-ends to Octave. An active community provides technical support to users.\n* [[Jacket (software)|Jacket]], A proprietary GPU Toolbox for MATLAB, enabling some MATLAB computations to be offloaded to the GPU for acceleration and data visualization purposes.\n* [[jLab]], a research platform for building an open-source MATLAB-like environment in pure Java and Groovy. Currently supports interpreted j-Scripts (MATLAB-like) and compiled GroovySci (extension to Groovy) scripts that provides direct interfacing to Java code and scripting access to many popular Java scientific libraries (e.g. [[Weka (machine learning)|Weka]] and [[JSci]]) and application Wizards.\n* [[Julia (programming language)]] is a high-level dynamic language with a surface similarity to MATLAB.\n* [[LabVIEW]] offers both textual and [[graphical programming|graphical-programming]] approaches to numerical analysis. Its text-based programming language MathScript uses .m-file-script syntax providing some compatibility with [[MATLAB]] and its clones.\n* [[LAPACK]] provides Fortran 90 routines for solving systems of simultaneous linear equations, least-squares solutions of linear systems of equations, eigenvalue problems, and singular value problems and the associated matrix factorizations (LU, Cholesky, QR, SVD, Schur, and generalized Schur).\n* [[MATLAB]] is a widely used proprietary software for performing numerical calculations. It comes with its own programming language, in which numerical algorithms can be implemented.\n* [[MCSim|GNU MCSim]] a simulation and numerical integration package, with fast Monte Carlo and Markov chain Monte Carlo capabilities.\n* [[ML.NET]] is a [[free software|free-software]] [[machine learning|machine-learning]] [[Library (computing)|library]] for the [[C Sharp (programming language)|C#]] programming language.<ref name=\"visu_Open\">{{Cite web | title = Open Source, Cross-Platform ML.NET Simplifies Machine Learning -- Visual Studio Magazine | author = David Ramel | work = Visual Studio Magazine | date = 2018-05-08 | accessdate = 2018-05-10 | url = https://visualstudiomagazine.com/articles/2018/05/08/ml-net-framework.aspx | quote = }}</ref><ref name=\"onms_Micr\">{{Cite web | title = Microsoft debuts ML.NET cross-platform machine learning framework | author = Kareem Anderson | work = On MSFT | date = 2017-05-09 | accessdate = 2018-05-10 | url = https://www.onmsft.com/news/microsoft-debuts-ml-net-cross-platform-machine-learning-framework | quote = }}</ref>\n* [[NAG Numerical Libraries|NAG Library]] is an extensive software library of highly optimized numerical-analysis routines for various programming environments.\n* [[O-Matrix]]\n* [[pandas (software)|pandas]] is an open-source BSD-licensed library providing data structures and data analysis tools for the Python programming language.\n* [[Perl Data Language]] provides large multidimensional arrays for the Perl programming language, and utilities for image processing and graphical plotting.\n* [[Plotly]] – Plotting library, [[Python (programming language)|Python]] command line, and graphical interface for analyzing data and creating browser-based graphs. Available for [[R (software)|R]], [[Python (programming language)|Python]], [[MATLAB]], [[Julia (programming language)|Julia]], and [[Perl]].\n* [[Rlab]] is another [[free software|free-software]] computer program which bears a strong resemblance to MATLAB. Rlab development ceased for several years but it was revived as [[RlabPlus]].\n* [[ScaLAPACK]] is a library of high-performance linear algebra routines for parallel distributed-memory machines that features functionality similar to LAPACK (solvers for dense and banded linear systems, least-squares problems, eigenvalue problems, and singular-value problem).\n* [[Scilab]] is advanced numerical analysis package similar to MATLAB or Octave. Comes with a complete GUI and Xcos which is alternative to Simulink. ([[free software]], [[GNU General Public License|GPL]]-compatible [[CeCILL]] license)\n* [[Sysquake]] is a computing environment with [[interactive]] graphics for mathematics, physics and engineering. Like other applications from [[Calerga]], it is based on a MATLAB-compatible language.\n* [[TK Solver]] is a mathematical modeling and problem-solving software system based on a declarative, rule-based language, commercialized by Universal Technical Systems, Inc.\n* [[Torch (machine learning)|Torch]] is a deep-learning library with support for manipulation, statistical analysis and presentation of Tensors.\n* [[XLfit]], A plugin to Excel which provides curve-fitting and statistical analysis.\n\n==General-purpose computer algebra systems==\n\n{{Main article|List of computer algebra systems}}\n\n* [[Macsyma]], a general-purpose computer algebra system, which has a free GPL-licensed version called [[Maxima (software)|Maxima]].\n* [[Maple (software)|Maple]], a general-purpose commercial mathematics software package.\n* [[Mathcad]] offers a [[WYSIWYG]] interface and the ability to generate publication-quality mathematical equations.\n* [[Mathematica]] offers numerical evaluation, optimization and visualization of a very wide range of numerical functions. It also includes a programming language and computer algebra capabilities.\n* [[PARI/GP]] is a widely used computer algebra system designed for fast computations in number theory (factorizations, algebraic number theory, elliptic curves...), but also contains a large number of other useful functions to compute with mathematical entities such as matrices, polynomials, power series, algebraic numbers etc., and a lot of transcendental functions. PARI is also available as a C library to allow for faster computations.\n* [[SageMath]] is an open-source math software, with a unified Python interface which is available as a text interface or a graphical web-based one. Includes interfaces for open-source and proprietary general purpose CAS, and other numerical analysis programs, like PARI/GP, GAP, gnuplot, Magma, and Maple.\n* [[Speakeasy (computational environment)|Speakeasy]] is an interactive numerical environment also featuring an interpreted programming language. Born in the mid '60s for matrix manipulation and still in continuous evolution, it pioneered the most common paradigms of this kind of tools, featuring dynamic typing of the structured data objects, dynamic allocation and garbage collection, operators overloading, dynamic linking of compiled or interpreted additional modules contributed by the community of the users and so on.\n* [[Trilinos]] is a collection of [[Open-source software|open-source]] [[object-oriented]] libraries for use in scientific and engineering applications. Trilinos is based on scalable, parallel linear-algebra algorithms.\n\n==Interface-oriented==\n* [[Baudline]] is a time-frequency browser for numerical signals analysis and scientific visualization.\n* [[COMSOL Multiphysics]] is a finite-element analysis, solver and simulation software / FEA Software package for various physics and engineering applications, especially coupled phenomena, or multiphysics. \n* [[Dataplot]] is provided by [[NIST]].\n* [[DADiSP]] is a commercial program focused on [[Digital signal processing|DSP]] that combines the numerical capability of MATLAB with a [[spreadsheet]]-like interface.\n* [[EJS]] is an [[Open-source software|open-source]] software tool, written in Java, for generating simulations.\n* [[Euler (software)|Euler Mathematical Toolbox]] is a powerful numerical laboratory with a programming language that can handle real, complex and interval numbers, vectors and matrices. It can produce 2D/3D plots.\n* [[FEATool Multiphysics]] is a [[Matlab]] GUI toolbox for finite element FEM and PDE multiphysics simulations.\n* [[FEniCS Project]] is a collection of project for automated solutions to [[partial differential equations|PDEs]].\n* [[Hermes Project|Hermes]] is a C++ library of advanced adaptive finite element algorithms to solve [[partial differential equations|PDEs]] and multiphysics coupled problems.\n* [[Fityk]] is a curve fitting and data-analysis program. Primarily used for peak fitting and analyzing peak data.\n* [[FlexPro]] is a commercial program for interactive and automated analysis and presentation of mainly measurement data. It supports many binary instrument data formats and has its own vectorized programming language.\n* [[IGOR Pro]], a software package with emphasis on time series, image analysis, and curve fitting. It comes with its own programming language and can be used interactively.\n* [[LabPlot]] is a data analysis and visualization application built on the KDE Platform.\n*[[MFEM]] is a free, lightweight, scalable C++ library for [[Finite element method|finite element methods]] .\n*[[Origin (data analysis software)|Origin]], a software package that is widely used for making scientific graphs. It comes with its own C/C++ compiler that conforms quite closely to ANSI standard.\n* [[Physics Analysis Workstation|PAW]] is a free [[data analysis]] package developed at [[CERN]].\n* [[SPSS]], an application for statistical analysis.\n* [[QtiPlot]] is a data analysis and scientific visualisation program, similar to Origin.\n* [[ROOT]] is a free object-oriented multi-purpose [[data analysis|data-analysis]] package, developed at [[CERN]].\n* [[Salome (software)|Salome]] is a free software that provides a generic platform for pre- and post-processing for numerical simulation.\n* [[Shogun (toolbox)|Shogun]], an [[Open-source software|open-source]] large-scale [[machine learning|machine-learning]] toolbox that provides several SVM implementations (like libSVM, SVMlight) under a common framework and interfaces to Octave, MATLAB, Python, R\n* [[Waffles (machine learning)|Waffles]] is a free-software collection of command-line tools designed for scripting machine-learning operations in automated experiments and processes.\n* [[Weka (machine learning)|Weka]] is a suite of [[machine learning]] software written at the [[University of Waikato]].\n\n==Language-oriented==\n* [[Advanced Continuous Simulation Language|acslX]] is a software application for modeling and evaluating the performance of continuous systems described by time-dependent, nonlinear differential equations.\n* [[ADMB]] is a software suite for non-linear statistical modeling based on C++ which uses automatic differentiation.\n* [[AMPL]] is a mathematical modeling language for describing and solving high complexity problems for large-scale optimization.\n* [[Ch (computer programming)|Ch]], a commercial C/C++-based interpreted language with computational array for scientific numerical computation and visualization.<ref>[http://www.softintegration.com/docs/ch/numeric/ Ch Scientific Numerical Computing]</ref>\n* [[APMonitor]]: APMonitor is a mathematical modeling language for describing and solving representations of physical systems in the form of differential and algebraic equations.\n* [[Armadillo (C++ library)|Armadillo]] is  C++ template library for linear algebra; includes various decompositions, factorisations, and statistics functions; its syntax ([[Application Programming Interface|API]]) is similar to MATLAB.\n* [[Clojure]] with numeric libraries Neanderthal, ClojureCUDA, and ClojureCL to call optimized matrix and linear algebra functions on CPU and GPU.\n* [[Julia (programming language)|Julia]] is designed for cloud parallel scientific computing in mind on [[LLVM]]-based [[Just-in-time compilation|JIT]] as a backend. Lightweight “green” threading (coroutines). Direct calls of C functions from code (no wrappers or special APIs needed), support for Unicode. Powerful shell-like capabilities for managing other processes. Lisp-like macros and other metaprogramming facilities.\n* [[Environment for DeveLoping KDD-Applications Supported by Index-Structures|ELKI]] a [[software framework]] for development of [[data mining]] algorithms in [[Java (programming language)|Java]].\n* [[GAUSS]], a matrix programming language for mathematics and statistics.\n* [[GNU Data Language]], a free compiler designed as a drop-in replacement for IDL.\n* [[IDL (programming language)|IDL]], a commercial interpreted language based on FORTRAN with some vectorization. Widely used in the [[solar physics]], [[fusion power|fusion]], [[atmospheric science]]s and [[medicine|medical]] communities. The [[GNU Data Language]] is a free alternative.\n* [[ILNumerics.Net]], a [[C Sharp (programming language)|C#]] math library that brings numeric computing functions for science, engineering and financial analysis to the [[.NET Framework]].  \n* [[Kinetic PreProcessor|KPP]] generates [[Fortran#Fortran 90|Fortran 90]], [[Fortran#FORTRAN 77|FORTRAN 77]], [[C (programming language)|C]], or [[Matlab]] code for the integration of [[Numerical ordinary differential equations|ordinary differential equations (ODEs)]] resulting from chemical reaction mechanisms.\n* [[Madagascar (software)|Madagascar]], an open-source software package for multidimensional data analysis and reproducible computational experiments. \n* [[mlpack]] is an open-source library for machine learning, providing a simple and consistent API, while exploiting C++ language features to provide maximum performance and flexibility\n* [[NCAR Command Language]] is an interpreted language designed specifically for scientific [[data analysis]] and visualization.\n* [[O-Matrix]] - a matrix programming language for mathematics, engineering, science, and financial analysis.\n* [[OptimJ]] is a mathematical Java-based modeling language for describing and solving high-complexity problems for large-scale optimization.\n* [[Perl Data Language]], also known as PDL, an array extension to [[Perl]] ver.5, used for data manipulation, statistics, numerical simulation and visualization.\n* [[Python (programming language)|Python]] with well-known scientific computing packages: [[NumPy]], [[SymPy]] and [[SciPy]].\n* [[R (programming language)|R]] is a widely used system with a focus on data manipulation and statistics which implements the [[S (programming language)|S]] language. Many add-on packages are available ([[free software]], [[GNU]] [[GNU General Public License|GPL license]]).\n* [[SAS (software)|SAS]], a system of software products for statistics. It includes SAS/IML<ref>[https://www.sas.com/en_us/software/iml.html SAS/IML]</ref>, a matrix programming language.\n* [[VisSim]] is a visual [[block diagram|block-diagram]] language for simulation of nonlinear dynamic systems and model-based embedded development. Its fast ODE engine supports real-time simulation of complex large-scale models. The highly efficient fixed-point code generator allows targeting of low-cost fixed-point embedded processors.\n* [[Wolfram Language]] which is used within many Wolfram technologies such as [[Mathematica]] and the Wolfram Cloud\n* [[World Programming System|World Programming System (WPS)]], supports mixing [[Python (programming language)|Python]], [[R (programming language)|R]] and [[SAS language|SAS]] languages in a single-user program for statistical analysis and data manipulation\n* [[Yorick (programming language)|Yorick]] is an interpreted programming language designed for numerics, graph plotting and simulation.\n\n==Historically significant==\n* [[Expensive Desk Calculator]] written for the [[TX-0]] and [[PDP-1]] in the late 1950s or early 1960s.\n* [[S (programming language)|S]] is an (array-based) programming language with strong numerical support. R is an implementation of the S language.\n\n==See also==\n*[[Comparison of deep-learning software]]\n*[[Comparison of numerical-analysis software]]\n*[[List of graphing software]]\n*[[List of numerical libraries]]\n*[[List of statistical packages]]\n*[[Lists of software]]\n*[[Mathematical software]]\n*[[Web-based simulation]]\n\n==References==\n{{Reflist}}\n\n{{Numerical analysis software}}\n\n{{DEFAULTSORT:Numerical Analysis Software}}\n[[Category:Lists of software]]\n[[Category:Mathematics-related lists]]\n[[Category:Numerical software|*Software]]"
    },
    {
      "title": "ARPACK",
      "url": "https://en.wikipedia.org/wiki/ARPACK",
      "text": "{{Infobox software\n| name                       = ARPACK\n| logo                       =\n| screenshot                 =\n| caption                    =\n| collapsible                =\n| author                     =\n| developer                  =\n| released                   =\n| latest release version     =\n| latest release date        =\n| latest preview version     =\n| latest preview date        =\n| programming language       = [[FORTRAN 77]]\n| operating system           =\n| platform                   =\n| size                       =\n| language                   =\n| genre                      = [[Software library]]\n| license                    = [[BSD-new]]\n| website                    = {{URL|http://www.caam.rice.edu/software/ARPACK/}}\n}}\n'''ARPACK''', the '''ARnoldi PACKage''', is a [[numerical computation|numerical]]\n[[software library]] written in [[Fortran|FORTRAN 77]] for solving large scale [[eigenvalue]] problems<ref>{{cite book\n  | last1 = Lehoucq\n  | first1 = R. B.\n  | last2 = Sorensen\n  | first2 = D. C.\n  | last3 = Yang\n  | first3 = C.\n  | title = ARPACK Users Guide: Solution of Large-Scale Eigenvalue Problems with Implicitly Restarted Arnoldi Methods\n  | publisher = SIAM\n  | location = Philadelphia\n  | date = 1998\n  | url = https://books.google.com/books?id=iMUea23N_CQC\n  | isbn = 978-0-89871-407-4}}\n</ref>\nin the [[matrix-free methods|matrix-free]] fashion.\n\nThe package is designed to compute a few eigenvalues and corresponding\neigenvectors of large [[Sparse matrix|sparse]] or structured [[Matrix (mathematics)|matrices]], using the\n[[Arnoldi iteration#Implicitly restarted Arnoldi method .28IRAM.29|Implicitly Restarted Arnoldi Method]] (IRAM)\nor, in the case of symmetric matrices, the corresponding variant of the\n[[Lanczos algorithm#Variations|Lanczos algorithm]].\nIt is used by many popular numerical computing environments such as [[SciPy]],<ref>\n{{cite web\n |url        = http://docs.scipy.org/doc/scipy/reference/tutorial/arpack.html\n |title      = Sparse Eigenvalue Problems with ARPACK\n |website  = Scipy.org\n |accessdate = 8 Dec 2013\n}}</ref> [[Mathematica]],<ref>{{cite web\n |url        = https://reference.wolfram.com/language/tutorial/SomeNotesOnInternalImplementation.html\n |title      = Some Notes on Internal Implementation\n |website  = wolfram.com\n |accessdate = 14 Oct 2016\n}}</ref> [[GNU Octave]]<ref>\n{{cite web\n |url        = https://www.gnu.org/software/octave/doc/interpreter/External-Packages.html\n |title      = External packages - GNU Octave\n |website  = gnu.org\n |accessdate = 8 Dec 2013\n}}</ref>\nand [[MATLAB]] to provide this functionality.\n\n== Reverse Communication Interface ==\nA powerful [[matrix-free methods|matrix-free]] feature of ARPACK is its ability to use any matrix storage format. This is possible because it doesn't operate on the matrices directly, but instead when a matrix operation is required it returns control to the calling program with a flag indicating what operation is required. The calling program must then perform the operation and call the ARPACK routine again to continue. The operations are typically matrix-vector products, and solving linear systems. \n\n== Fork ==\nDue to stalled upstream development, ARPAСK has been forked into ARPACK-NG,<ref>[https://github.com/opencollab/arpack-ng ARPACK-NG]</ref> as a form of a collaborative effort of the various groups that rely on ARPACK.\n\n==See also==\n{{Portal|Free and open-source software}}\n* [[LOBPCG]], a different [[matrix-free methods|matrix-free method]] for symmetric or Hermitian matrices only, implemented in C in [[BLOPEX]], ported to [[hypre]] and [[SLEPc]], in [[C++]] in Anasazi ([[Trilinos]]), in [[Python (programming language)|Python]] in [[scipy]], in [[Julia language]], [[MATLAB]], [[GNU Octave]], as well as in [[Matrix Algebra on GPU and Multicore Architectures]] (MAGMA) and [[NVIDIA]] [[CUDA]].\n* [[LAPACK]], software library based on [[matrix transformations]] for [[dense matrix|dense matrices]].\n\n==References==\n{{Reflist}}\n\n==External links==\n* {{Official website|http://www.caam.rice.edu/software/ARPACK/}} at Rice University\n* [https://github.com/opencollab/arpack-ng arpack-ng fork], joint effort between [[Scilab]], [[GNU Octave]], [[Debian]], and others.\n\n{{DEFAULTSORT:Arpack}}\n[[Category:Fortran libraries]]\n[[Category:Free software programmed in Fortran]]\n[[Category:Numerical software]]\n[[Category:Software using the BSD license]]"
    },
    {
      "title": "Baudline",
      "url": "https://en.wikipedia.org/wiki/Baudline",
      "text": "{{Infobox Software\n| name = baudline\n| logo =\n| screenshot = Baudline spectro1.png\n| caption = Baudline Signal Analyzer\n| developer = [http://sigblips.com SigBlips DSP Engineering]\n| released = {{Start date and age|2000|09}}\n| programming language = C<ref name=\"sourcecode\">{{cite web |url=http://www.baudline.com/source_code.html |title=Source code |website=Baudline |publisher=SigBlips DSP Engineering |accessdate=22 May 2010 }}</ref>\n| operating system = [[FreeBSD]],<ref>{{cite web |url=http://www.freshports.org/audio/baudline |title=FreshPorts -- audio/baudline |publisher=DVL Software Limited |accessdate=22 May 2010 | archiveurl= https://web.archive.org/web/20100518192716/http://www.freshports.org/audio/baudline| archivedate= 18 May 2010 <!--DASHBot-->| deadurl= no}}</ref> [[Linux]],<ref>{{cite web |url=http://www.linux.org/apps/AppId_8213.html |title=Linux Online - Application: baudline |publisher=Linux Online, Inc. |accessdate=22 May 2010 |deadurl=yes |archiveurl=https://web.archive.org/web/20101207132741/http://www.linux.org/apps/AppId_8213.html |archivedate=7 December 2010 |df= }}</ref> [[macOS|Mac OS X]], [[Solaris (operating system)|Solaris]]\n| genre = [[List of numerical analysis software|numerical analysis]]\n|license=Binary: proprietary;<ref name=\"download\">{{cite web|url=http://www.baudline.com/download.html |title=Download |website=Baudline |publisher=SigBlips DSP Engineering |access-date=12 May 2015}}</ref> Source code: proprietary<ref name=\"sourcecode\" />\n| website = {{URL|www.baudline.com}}\n}}\nThe '''baudline''' [[Time-frequency representation|time-frequency]] browser is a [[signal analysis]] tool designed for [[scientific visualization]]. It runs on several [[Unix-like]] operating systems under the [[X11|X Window System]]. Baudline is useful for [[Real-time computing|real-time]] spectral monitoring, collected signals analysis, generating test signals, making [[distortion]] measurements, and playing back audio files.\n__NOTOC__\n==Applications==\n* [[Acoustic cryptanalysis]] <ref>{{cite web |url=http://people.csail.mit.edu/tromer/acoustic/ |title=Acoustic cryptanalysis:On nosy people and noisy machines |publisher=Adi Shamir, Eran Tromer |accessdate=22 May 2010 }}</ref><ref>{{cite web |url=http://seclab.uiuc.edu/pubs/LeMayT06.pdf |title=Acoustic Surveillance of Physically Unmodified PCs |publisher=Michael LeMay, Jack Tan |accessdate=22 May 2010 |deadurl=yes |archiveurl=https://web.archive.org/web/20100615210240/http://seclab.uiuc.edu/pubs/LeMayT06.pdf |archivedate=15 June 2010 |df= }}</ref><ref>{{cite web |url=http://security1.win.tue.nl/~bskoric/seminar/papers1/MuijnckHughes_acoustic.pdf |title=2IF03 – Seminar IST Overview of Acoustic Side Channel Attacks |publisher=Jan de Muijnck-Hughes |accessdate=17 February 2010 }} {{Dead link|date=October 2010|bot=H3llBot}}</ref><ref>{{cite web |url=https://www.usenix.org/events/sec09/tech/full_papers/sec09_attacks.pdf |title=Compromising Electromagnetic Emanations of Wired and Wireless Keyboards |publisher=Martin Vuagnoux, Sylvain Pasini |accessdate=22 May 2010 }}</ref><ref>{{cite web |url=http://www.medienwissenschaft.hu-berlin.de/de/medienwissenschaft/medientheorien/downloads/hausarbeiten/em-sniffer.pdf/at_download/file |title=EM-Sniffing |publisher=Oswald Berthold |accessdate=25 Jul 2016 |archive-url=https://web.archive.org/web/20160809151342/http://www.medienwissenschaft.hu-berlin.de/de/medienwissenschaft/medientheorien/downloads/hausarbeiten/em-sniffer.pdf/at_download/file |archive-date=9 August 2016 |dead-url=yes |df=dmy-all }}</ref>\n* [[Audio codec]] [[lossy compression]] analysis <ref>{{cite web |url=http://baudline.com/solutions/codec/index.html |title=Audio Compression Codecs |publisher=E. Olson |accessdate=22 May 2010 }}</ref><ref>{{cite web |url=http://sysdoc.doors.ch/CREATIVE/oggVorbis.pdf |title=Ogg Vorbis and MP3 Audio Stream charecterization |publisher=Ayman Ammoura Franco Carlacci |accessdate=22 May 2010 |deadurl=yes |archiveurl=https://web.archive.org/web/20110706224354/http://sysdoc.doors.ch/CREATIVE/oggVorbis.pdf |archivedate=6 July 2011 |df= }}</ref>\n* [[Audio signal processing]]\n* [[Bioacoustics]] research\n* [[Data acquisition]] (DAQ)\n* [[Infrasound]] monitoring\n* [[Musical acoustics]] <ref>{{cite web |url=https://perswww.kuleuven.be/~u0023287/reports/zangstemanalyse2.pdf |title=Analyse van de zangstem : geluiden in beeld |publisher=Koen Eneman, Tom Francart |accessdate=22 May 2010 }}</ref><ref>{{cite web |citeseerx=10.1.1.67.3546 |title=Improving the Aesthetic Quality of Realtime Motion Data Sonification |publisher=Christoph Henkelmann |accessdate=22 May 2010 }}</ref><ref>{{cite web |url=http://eprints.ucm.es/9016/1/TC2005-38.pdf |title=Analizador de un espectro musical |publisher=Andrés Jiménez Sánchez, Amaia de Miguel Morate, Javier Villellas Bernal |accessdate=22 May 2010 }}</ref>\n* [[Seismology|Seismic data processing]]\n* [[SETI]] <ref>{{cite web |url=http://baudline.blogspot.com/2010/04/setiquest-kepler-exo4-1420-mhz.html |title=setiQuest Kepler-Exo4 1420 MHz |publisher=baudline |accessdate=23 May 2010 | archiveurl= https://web.archive.org/web/20100518053827/http://baudline.blogspot.com/2010/04/setiquest-kepler-exo4-1420-mhz.html| archivedate= 18 May 2010 <!--DASHBot-->| deadurl= no}}</ref><ref>{{cite web |url=http://exoplanetology.blogspot.com/2010/05/kepler-4b-and-seti.html |title=SETI and Kepler-4b |publisher=exoplanetology.com |accessdate=23 May 2010 }}</ref><ref>{{cite web |url=http://www.portaltotheuniverse.org/blogs/posts/view/56588/ |title=Exoplanetology Kepler 4b and SETI |publisher=portaltotheuniverse.org |accessdate=23 May 2010 | archiveurl= https://web.archive.org/web/20100513021255/http://www.portaltotheuniverse.org/blogs/posts/view/56588/| archivedate= 13 May 2010 <!--DASHBot-->| deadurl= no}}</ref>\n* [[Signal analysis]]\n* [[Software Defined Radio]] <ref>{{cite web |url=http://www.domenech.org/homebrew-sdr/receiver-2.htm |title=My First Software Defined Radio (SDR) Receiver - 2nd Section Downconverter |publisher=Juan Domenech Fernandez |accessdate=22 May 2010  }}</ref><ref>{{cite web |url=https://berlin.ccc.de/wiki/GNU_Radio |title=GNU Radio ist ein freies Toolkit fuer Software Defined Radio |publisher=Chaos Computer Club Berlin |accessdate=22 May 2010 |archive-url=https://web.archive.org/web/20110718172001/https://berlin.ccc.de/wiki/GNU_Radio |archive-date=18 July 2011 |dead-url=yes }}</ref>\n* [[Spectrum analyzer|Spectral analysis]]\n* [[Very low frequency]] (VLF) reception<ref>{{cite web |url=http://www.dalcanton.it/tito/esperimenti/lowfreq |title=Segnali radio a bassissima frequenza (VLF) |publisher=Tito Dal Canton |accessdate=22 May 2010 }}</ref>\n* [[WWV (radio station)#Standard frequency signals|WWV frequency measurement]] <ref>{{cite web |url=http://www.febo.com/time-freq/FMT/technique/index.html |title=Techniques for Measuring Frequency Off-the-Air |publisher=John Ackermann |accessdate=22 May 2010 }}</ref><ref>{{cite web |url=http://www.febo.com/pages/baudline/ |title=Using Baudline for Frequency Measurement |publisher=John Ackermann |accessdate=22 May 2010 }}</ref>\n\n==Features==\n* Spectrogram, [[frequency spectrum|Spectrum]], Waveform, and Histogram displays\n* [[Fast Fourier transform|Fourier]], [[Cross-correlation|Correlation]], and [[Raster graphics|Raster]] transforms\n* [[Signal-to-noise ratio|SNR]], [[Total harmonic distortion|THD]], [[SINAD]], ENOB, SFDR distortion measurements\n* Channel [[equalization (audio)|equalization]]\n* [[Function generator]]\n* [[Downsampling|Digital down converter]]\n* Audio playing with real-time DSP effects like [[Sample rate conversion|speed control]], [[Pitch shifter (audio processor)|pitch scaling]], [[Tuner (radio)|frequency shifting]], matrix [[surround sound|surround]] panning, filtering, and digital gain boost\n* Audio recording of multiple channels\n* [[JACK Audio Connection Kit]] sound server support<ref>{{cite web |url=http://ardour.org/node/361#comment-1160 |title=FFT visualizing : Ardour |publisher=Paul Davis |accessdate=22 May 2010 }}</ref>\n* Import [[AIFF]], [[Au file format|AU]], [[WAV]], [[FLAC]], [[MP3]], [[Ogg]] [[Vorbis]], [[Audio Video Interleave|AVI]], MOV, and other [[file format]]s\n\n==License==\nBaudline binaries are available for free download.<ref name=\"download\" /> The binaries may be used for any purpose, though no form of redistribution is permitted.<ref>{{cite web|url=http://www.baudline.com/faq.html |title=Baudline FAQ |website=Baudline |publisher=SigBlips DSP Engineering}}</ref> Source code is available for purchase under a number of licenses.<ref name=\"sourcecode\" /> While the Baudline website states that the source code is dual-licensed, under both a proprietary license and a version of the [[GNU General Public License]], no source code released under this dual-license is made available for download on the software's website nor has been shown to exist elsewhere.<ref>[https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=784771 \"ITP: baudline -- time-frequency browser designed for spectral visualisation\"] {{webarchive |url=https://web.archive.org/web/20160115171620/https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=784771 |date=January 15, 2016 }} (#784771). ''Debian Bug Tracking System''. [[Debian|The Debian Project]]. Retrieved 12th May 2015.</ref>\n\n==See also==\n* [[Linux audio software]]\n* [[List of information graphics software]]\n* [[List of numerical analysis software]]\n* [[Digital signal processing]]\n\n==References==\n{{reflist|30em}}\n\n==External links==\n* {{Official website|www.baudline.com}}\n* [https://groups.google.com/group/baudline User discussion group] at Google Groups\n* [http://www.sigblips.com/ SigBlips DSP Engineering]\n\n{{Media player (application software)}}\n\n[[Category:Time–frequency analysis]]\n[[Category:Linux media players]]\n[[Category:Acoustics software]]\n[[Category:Numerical software]]\n[[Category:Unix software]]\n[[Category:Audio software with JACK support]]\n[[Category:Science software for Linux]]\n[[Category:Science software for MacOS]]\n[[Category:Audio software for Linux]]"
    },
    {
      "title": "CERN Program Library",
      "url": "https://en.wikipedia.org/wiki/CERN_Program_Library",
      "text": "\n{{Infobox software\n| name = CERN Program Library\n| developer = [[CERN]]\n| latest_release_version = {{Start date and age|2005}}\n| latest_release_date = {{Start date and age|2005|05|09}}\n| operating_system = [[Cross-platform]]\n| genre = [[List of numerical analysis software|Technical computing]]\n| license = [[GNU General Public License]], except for [[GEANT (program)|GEANT]]\n| website = {{URL|cern.ch/cernlib}}\n}}\n{{Portal|Free and open-source software}}\nThe '''CERN Program Library''' or '''CERNLIB''' was a set of [[FORTRAN 77]] libraries and modules, developed at the European Organization for Nuclear Research [[CERN]]. Its content ranged from more specialized [[data analysis]] of [[high energy physics]] to general purpose [[numerical analysis]]. Lower-level parts of the CERN Program Library were most prominently used by the data analysis software [[Physics Analysis Workstation]] (PAW) and the detector simulation framework [[GEANT-3|GEANT]], both of which were also part of the CERN Program Library.\n\nThe major fields covered by the libraries contained therein were:\n* [[Elementary particle]] data\n* Graphics and plotting\n* Histograming\n* [[I/O]] and structured data storage\n* Numerical analysis\n* [[Statistics]] and data analysis\n* Detector simulation and [[Hadron]]ic [[event generator|event generation]]\n\nCERN Program Library used the year as its version, with not explicitly denoted minor revisions within a year. Besides legacy software dependency, for newer applications written in [[C++]], CERNLIB is now superseded by [[ROOT]].\n\n==Status==\nDevelopment and support for CERNLIB was discontinued in 2003. Libraries are still available \"as is\" \"for ever\" from [http://cernlib.web.cern.ch/cernlib/version.html the CERNLIB web site] but with no new code, no user support and no port to [[IA-64]]. \n\n==External links==\n{{no footnotes|date=May 2016}}\n*{{Official website|cern.ch/cernlib}}, CERN Program Library\n\n{{DEFAULTSORT:Cern Program Library}}\n[[Category:Fortran libraries]]\n[[Category:Free mathematics software]]\n[[Category:Free physics software]]\n[[Category:Free software programmed in Fortran]]\n[[Category:Numerical software]]\n[[Category:CERN software]]"
    },
    {
      "title": "DnAnalytics",
      "url": "https://en.wikipedia.org/wiki/DnAnalytics",
      "text": "{{ infobox software\n| name                   = dnAnalytics\n| latest_release_version = 2009.4\n| latest_release_date    = {{Start date and age|2009|04|29}}\n| programming language   = [[C Sharp (programming language)|C#]], [[F Sharp (programming language)|F#]],  [[Common Language Runtime|.NET CLR]]\n| operating system       = [[Cross-platform]]\n| discontinued           = yes\n| genre                  = [[List of numerical analysis software|Numerical library]]\n| license                = [[BSD licenses|BSD]]/[[Microsoft Public License]]\n| website                = {{URL|https://dnanalytics.codeplex.com}}\n}}\n'''dnAnalytics''' is an [[open-source software|open-source]] numerical library for [[Microsoft .NET|.NET]] written in [[C Sharp (programming language)|C#]] and [[F Sharp (programming language)|F#]]. It features functionality similar to [[Basic Linear Algebra Subprograms|BLAS]] and [[LAPACK]].\n\n== Features ==\nThe software library provides facilities for:\n* Linear algebra classes with support for sparse matrices and vectors (with a F# friendly interface).\n* Dense and sparse solvers.\n* Probability distributions.\n* Random number generation (including [[Mersenne twister|Mersenne Twister MT19937]]).\n* [[QR decomposition|QR]], [[LU decomposition|LU]], [[Singular value decomposition|SVD]], and [[Cholesky decomposition]] classes.\n* Matrix IO classes that read and write matrices from/to Matlab, Matrix Market, and delimited files.\n* Complex and “special” math routines.\n* Descriptive Statistics, Histogram, and [[Pearson product-moment correlation coefficient|Pearson Correlation Coefficient]].\n* Overload mathematical operators to simplify complex expressions.\n* Visual Studio visual debuggers for matrices and vectors\n* Runs under Microsoft Windows and platforms that support Mono.\n* Optional support for Intel Math Kernel Library (Microsoft Windows and Linux)\n\n== See also ==\n* [[Math.NET Numerics]]\n* [[List of numerical libraries]]\n* [[list of numerical analysis software]]\n\n== External links ==\n* [https://dnanalytics.codeplex.com dnAnalytics] homepage\n\n[[Category:Numerical software]]\n[[Category:Software using the BSD license]]"
    },
    {
      "title": "Efficient Java Matrix Library",
      "url": "https://en.wikipedia.org/wiki/Efficient_Java_Matrix_Library",
      "text": "{{Infobox software\n| author                 = Peter Abeles\n| operating system       = [[Cross-platform]]\n| latest_release_version = 0.37.1\n| latest_release_date    = {{Start date and age|2018|12|25}}\n| genre                  = Library\n| license                = [[Apache_License]]\n| website                = {{url|http://ejml.org/}}\n}}\n\n'''Efficient Java Matrix Library''' (EJML) is a linear algebra library for manipulating real/complex/dense/sparse matrices. Its design goals are; 1) to be as computationally and memory efficient as possible for both small and large matrices, and 2) to be accessible to both novices and experts. These goals are accomplished by dynamically selecting the best algorithms to use at runtime, clean API, and multiple interfaces. EJML is free, written in 100% Java and has been released under an Apache v2.0 license.\n\nEJML has three distinct ways to interact with it: 1) procedural, 2) SimpleMatrix, and 3) Equations. Procedure provides all capabilities of EJML and almost complete control over memory creation, speed, and specific algorithms. SimpleMatrix provides a simplified subset of the core capabilities in an easy to use flow styled object-oriented API, inspired by Jama. Equations is a symbolic interface, similar in spirit to Matlab and other CAS, that provides a compact way of writing equations. <ref name=\"ProjectPage\"/>\n\n== Capabilities ==\n\nEJML provides the following capabilities for dense matrices.\n\n* Basic Operators (addition, multiplication, ... )\n* Matrix Manipulation (extract, insert, combine, ... )\n* Linear Solvers (linear, least squares, incremental, ... )\n* Decompositions (LU, QR, Cholesky, SVD, Eigenvalue, ...)\n* Matrix Features (rank, symmetric, definitiveness, ... )\n* Random Matrices (covariance, orthogonal, symmetric, ... )\n* Different Internal Formats (row-major, block)\n* Unit Testing\n\n== Usage Example (Equations) ==\n\nComputing the Kalman gain:\n\n<source lang=\"java\">\neq.process(\"K = P*H'*inv( H*P*H' + R )\");\n</source>\n\n== Usage Example (SimpleMatrix) ==\n\nExample of Singular Value Decomposition (SVD):\n<source lang=\"java\">\nSimpleSVD s = matA.svd();\nSimpleMatrix U=s.getU();\nSimpleMatrix W=s.getW();\nSimpleMatrix V=s.getV();\n</source>\n\nExample of matrix multiplication:\n<source lang=\"java\">\nSimpleMatrix result = matA.mult(matB);\n</source>\n\n== Usage Example (DenseMatrix64F) ==\n\nExample of Singular Value Decomposition (SVD):\n<source lang=\"java\">\nSingularValueDecomposition_F64<DenseMatrix64F> svd = \n    DecompositionFactory_DDRM.svd(true,true,true);\n\nif( !DecompositionFactory.decomposeSafe(svd,matA) )\n    throw new DetectedException(\"Decomposition failed\");\n\nDenseMatrix64F U = svd.getU(null,false);\nDenseMatrix64F S = svd.getW(null);\nDenseMatrix64F V = svd.getV(null,false);\n</source>\n\nExample of matrix multiplication:\n<source lang=\"java\">\nCommonOps_DDRM.mult(matA,matB,result);\n</source>\n\n==See also==\n* [[List of numerical libraries]]\n\n== References ==\n{{reflist|refs=\n<ref name=ProjectPage>{{cite web |url=http://ejml.org/ |title=EJML Project Page |work=EJML|publisher=Peter Abeles|accessdate=Jan 21, 2019}}</ref>\n}}\n\n== External links ==\n* [http://ejml.org/ Efficient Java Matrix Library (EJML) homepage]\n\n[[Category:Numerical software]]"
    },
    {
      "title": "Extreme Loading for Structures",
      "url": "https://en.wikipedia.org/wiki/Extreme_Loading_for_Structures",
      "text": "{{Advert|date=September 2009}}\n\n{{Infobox Software\n|name = Extreme Loading for Structures\n|screenshot = \n|caption = \n|developer = [[Applied Science International]]\n|latest_release_version = ELS Version 3.0\n|latest_release_date = {{Start date and age|2009|09|03}}\n|operating_system = [[Microsoft Windows|Windows]]\n|genre = [[Simulation]] software\n|license = \n|website = {{URL|www.extremeloading.com}}\n}}\n\n'''Extreme Loading for Structures''' ('''ELS''') is commercial structural-analysis software based on the [[applied element method]] (AEM) for the automatic tracking and propagation of cracks, separation of elements, element collision, and collapse of structures under extreme loads.<ref name=\"AEMTheory\">{{cite journal | last =  | first =  | authorlink =  | authors = Meguro, K. and Tagel-Din, H. | title = Applied element method for structural analysis: Theory and application for linear materials | journal = Structural engineering/earthquake engineering | volume = 17 | issue = 1 | pages = 21–35 | publisher = Japan Society of Civil Engineers | location = Japan | year = 2000 | url = http://sciencelinks.jp/j-east/article/200014/000020001400A0511912.php | doi =  | id = F0028A | accessdate = 2009-08-10 | deadurl = yes | archiveurl = https://web.archive.org/web/20120229032846/http://sciencelinks.jp/j-east/article/200014/000020001400A0511912.php | archivedate = 2012-02-29 | df =  }}</ref> AEM combines features of [[Finite element method]] and [[Discrete element method]] simulation with its own solver capabilities for the generation of PC-based structural analysis.\n\n==History==\n\n===2003===\n* Research and development related to the software begins with the formation of Applied Science International. The first release of ELS appears in the form of 2D analysis with structures modeled, loading scenarios applied, and results viewed.\n\n===2008===\n* Version 2.0 allows users to perform 3D analysis, though modeling is largely limited to 2D and restricted 3D functionality.\n* The United States Department of Homeland Security assigns ELS Designation Status for Anti-terrorism under the SAFETY Act.<ref name=\"Dhs\">{{cite web | author = DHS | title = SAFETY Act Designation Status Listing  | publisher = Department of Homeland Security  | url = https://www.safetyact.gov/DHS/SActHome.nsf/Designations?ReadForm | accessdate = 2009-08-10}}</ref>\n\n===2009===\n* ELS version 3.0 is released with complete 3D functionality.\n\n==ELS Modeler==\nIn the ELS modeler the user has the ability to model using a variety of default [[List of structural elements|structural components]] such as columns, slabs, walls, windows, [[steel sections]], reinforcement and 8-node objects. Pre-stressing, cables, pre-cracking and holes can also be added to models to reflect the required attributes of structures. To aid the user in modeling, attachments such as [[DXF|*.dxf]], [[STL (file format)|*.stl]], [[DGN|*.dgn]], and [[JPEG|*.jpeg]] can be easily imported. Models can also be imported from several FEM based software programs such as [[Abaqus]], [[ANSYS]], Etabs, Gambit, Ideas, [[LS-DYNA]], [[Nastran]], Patran, [[SAP2000]], and STAAD. ELS is [[Building Information Modeling]] ([[BIM]]) compatible with a plug-in for [[Revit|Autodesk Revit Structure]], allowing users to import previously created structural components.<ref name=ELSOverview>{{cite web | author = ASI | title = Extreme Loading for Structures Software - Overview  | publisher = Applied Science International  | url = http://www.extremeloading.com/Overview.aspx | accessdate = 2009-08-12}}</ref>\n\n==ELS Solver==\nThe solver performs multi-stage static and dynamic analysis in 2D and 3D.  Static loading types include concentrated loads, [[Displacement (vector)|displacements]], [[hydrostatic pressure]]s, uniform pressures, moving loads, element removals, and line loads. Available dynamic loading types include concentrated loads, displacements, [[Earthquake|seismic loads]], uniform pressures, element removals, and moving loads. ELS comes with a predefined material library with both linear and non-linear material models that include [[steel]], [[concrete]], [[reinforced concrete]], [[glass]], [[aluminum]], [[Elasticity (physics)|elastic]], and [[Tension (physics)|tension]].{{clarify|date=November 2011}}<!-- confusing, first there is a list of materials and then \"elastic\" (sic) and \"tension\" which are concepts, not materials --><ref name=\"ELSOverview\"/> \n\nThe '''AEM solver''' automatically calculates:\n*Crack generation\n*Plastic hinge formation\n*Element separation\n*Buckling, post buckling and large displacements\n*Contact and collisions\n\n==ELS Output Viewer==\nOnce the AEM solver begins, output can be immediately observed in the ELS viewer.  A variety of animated diagrams, charts and files can be created for visual representation of the analysis.  The ELS viewer can create the following exportable content:\n*Animated diagrams:\n**Internal force\n**Envelope\n*Animated stress-strain contours\n*Load displacement curves\n*Animated charts available:\n**Load\n**Displacement\n**Stress\n**Strain\n**Time\n**Velocity\n**Acceleration\n*Eigen modes\n*Export to *.avi and *.bmp\n*Through additional plug-ins ELS can export animation data to [[Autodesk 3ds Max]] and [[Autodesk Maya|Maya]] for use in [[visual effects]] for television and film.<ref name=\"ELSOverview\"/>\n\n==Applications==\nELS is currently being utilized for structural engineering analysis functions such as structural vulnerability assessment & risk mitigation, [[Progressive collapse|progressive collapse analysis]], blast analysis, demolition prediction analysis, impact analysis, [[seismic analysis]], forensic engineering, performance based design, glass performance analysis product development and visual effects.<ref name=\"ELSOverview\"/>\n\n==See also==\n*[[Failure analysis]]\n*[[Physics engine]]\n*[[Structural engineering]]\n*[[Earthquake simulation]]\n*[[Applied Element Method]]\n\n==Academic institutions==\nMore than 20 [[universities]] and [[academic institutions]] are currently involved in research and development projects resulting in the creation of publications on topics related to the Applied Element Method and Extreme Loading for Structures.<ref name=\"Website\">{{cite web | author = AEM Website | title = Applied Element Method - Publications Library  | publisher = AppliedElementMethod.com | url = http://www.appliedelementmethod.com/Publications.aspx |accessdate= 2009-08-12}}</ref>  Academic institutions working with ELS include:\n\n{| class=\"wikitable\" align=\"center\" width=\"950\"\n! colspan=\"4\" | Academic Institutions working with ELS<ref name=\"Asi\">{{cite web | author = ASI | title = Extreme Loading for Structures - Universities  | publisher = Applied Science International, LLC | url = http://www.extremeloading.com/Clients.aspx |accessdate= 2009-08-12}}</ref>\n|-align=\"center\"\n|[[Auburn University]] \n|[[Carleton University]] \n|[[Concordia University (Quebec)|Concordia University]]\n|[[Dresden University of Technology]]\n|-align=\"center\"\n|[[École de technologie supérieure]]\n|[[Helwan University]]\n|[[Technical Military Academy of Bucharest]]\n|[[Milwaukee School of Engineering]]\n|-align=\"center\"\n|[[National Society for Earthquake Technology]]\n|[[National University of Singapore]]\n|[[Northeastern University]]\n|[[North Carolina State University]]\n|-align=\"center\"\n|[[Pennsylvania State University]]\n|[[Portuguese Military Academy]]\n|[[Purdue University]]\n|[[Royal Military Academy (Belgium)]]\n|-align=\"center\"\n|[[Rutgers University]]\n|[[Technical University of Cluj-Napoca]]\n|[[Tokyo Institute of Technology]]\n|[[United States Military Academy]]\n|-align=\"center\"\n|[[University of Bristol]]\n|[[University of Connecticut]]\n|[[University of California]]\n|[[University of Miami]]\n|-align=\"center\"\n|[[University of Missouri]]\n|[[University of Missouri–Kansas City]]\n|[[University of Oxford]]\n|[[University of Texas at Austin]]\n|-align=\"center\"\n|[[University of Wollongong]]\n|}\n\n==References==\n{{reflist}}\n\n== External links ==\n* [https://www.appliedelementmethod.com/ Applied Element Method]\n* [https://www.extremeloading.com/ Extreme Loading for Structures]\n* [https://www.appliedscienceint.com/ Applied Science International, LLC]\n\n{{DEFAULTSORT:Extreme Loading For Structures}}\n[[Category:Scientific simulation software]]\n[[Category:Numerical software]]\n[[Category:Computer-aided engineering software]]"
    },
    {
      "title": "Fast Library for Number Theory",
      "url": "https://en.wikipedia.org/wiki/Fast_Library_for_Number_Theory",
      "text": "{{Redirect|FLINT||Flint (disambiguation)}}\n{{Infobox Software\n| name = FLINT\n| developer = William Hart and David Harvey\n| latest_release_version = 2.5.2\n| latest_release_date = {{Start date and age|2015|08|07}}\n| programming language = [[C (programming language)|C]]\n| operating_system = [[POSIX]] systems\n| genre = [[Mathematical software]]\n| license = [[GNU General Public License]]\n| website = {{url|flintlib.org}}\n}}\n\nThe '''Fast Library for Number Theory (FLINT)''' is a [[C (programming language)|C]] [[library (computing)|library]] for [[number theory]] applications. The two major areas of functionality currently implemented in FLINT are [[polynomial arithmetic]] over the [[integers]] and a [[quadratic sieve]]. The library is designed to be compiled with the [[GNU Multi-Precision Library|GNU Multi-Precision Library (GMP)]] and is released under the [[GNU General Public License]]. It is developed by [[William Hart (mathematician)|William Hart]] of the [[University of Kaiserslautern]] (formerly [[University of Warwick]]) and [[David Harvey (mathematician)|David Harvey]] of [[University of New South Wales]] (formerly [[Harvard University]]) to address the speed limitations of the [[PARI/GP|PARI]] and [[Number Theory Library|NTL]] libraries.<ref>Page 10 of the [http://wstein.org/grants/sage-06/project_description.pdf Project Description] for [http://wstein.org/grants/sage-06/ UW 2006 COMPMATH Proposal] by William Stein</ref>\n\n== Design Philosophy ==\n* Asymptotically Fast Algorithms\n* Implementations Fast as or Faster than Alternatives\n* Written in Pure C\n* Reliance on GMP\n* Extensively Tested\n* Extensively Profiled\n* Support for Parallel Computation\n\n== Functionality ==\n* Polynomial Arithmetic over the Integers\n* Quadratic Sieve\n\n== References ==\n{{reflist}}\n;Notes\n{{refbegin}}\n* [http://flintlib.org/flint-1.0.9.pdf FLINT 1.0.9: Fast Library for Number Theory] by William Hart and David Harvey\n* [http://alita.msri.org:8080/13276/13276-13276-QuickTime.mov Video]{{dead link|date=September 2017 |bot=InternetArchiveBot |fix-attempted=yes }} of the talk Parallel Computation in Number Theory (January 30, 2007) by William Hart\n* [https://web.archive.org/web/20110720111749/http://sage.math.washington.edu/sd4video/david_harvey.mov Video] of the talk [http://wiki.sagemath.org/days4schedule?action=AttachFile&do=get&target=flint-talk.pdf FLINT and Fast Polynomial Arithmetic] (June 13, 2007) By David Harvey\n* [http://video.google.com/videoplay?docid=-5481874090998683168 Video] of the talk A short talk on short division (October 1, 2007) by William Hart\n* [http://sage.math.washington.edu/home/was/sagedays6/medium/M2U00220.mp4 Video]{{dead link|date=September 2017 |bot=InternetArchiveBot |fix-attempted=yes }} of the talk Algebraic Number Theory with FLINT (November 11, 2007) by William Hart\n<!--* Pages 2 and 6–9 of William Hart's grant proposal [http://sage.math.washington.edu/home/wbhart/fullproposal.pdf Algorithms for Algebraic Number Theory] -->\n{{refend}}\n\n[[Category:Computational number theory]]\n[[Category:Free software programmed in C]]\n[[Category:Integer factorization algorithms]]\n[[Category:Numerical software]]"
    },
    {
      "title": "FEKO",
      "url": "https://en.wikipedia.org/wiki/FEKO",
      "text": "{{Infobox software\n| name                   = \n| title                  = FEKO\n| logo                   = <!-- Image name is enough -->\n| logo caption           = \n| logo_size              = \n| logo_alt               = \n| screenshot             = <!-- Image name is enough -->\n| caption                = \n| screenshot_size        = \n| screenshot_alt         = \n| collapsible            = \n| author                 = \n| developer              = [[Altair Engineering]]\n| released               = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| discontinued           = \n| latest release version = 2018<ref>{{cite web|title=FEKO New Features |url=http://www.altairhyperworks.com/product/FEKO/New-Features |date=2018-03-04 |accessdate=2018-03-14}}</ref>\n| latest release date    = {{Release date and age|2018|03}}<ref>{{cite web |title=Altair HyperWorks® 2017 Released; Comprehensive Simulation Platform for Innovation™ |url=http://www.altair.com/newsdetail.aspx?news_id=11343 |date=2017-03-07|accessdate=2017-04-05}}</ref>\n| latest preview version = \n| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| programming language   = \n| operating system       = [[Microsoft Windows]] and [[Linux]]\n| platform               = \n| size                   = \n| language               = \n| language count         = <!-- DO NOT include this parameter unless you know what it does -->\n| language footnote      = \n| genre                  = EM simulation software\n| license                = [[Proprietary software|Proprietary]] [[commercial software]]\n| website                = {{URL|http://www.altairhyperworks.com/FEKO|Altair website}}<br/> \n{{URL|http://www.feko.info|Feko website}}\n}}\n'''FEKO''' is a [[computational electromagnetics]] software product developed by [[Altair Engineering]].  The name is derived from the German acronym \"'''FE'''ldberechnung für '''K'''örper mit beliebiger '''O'''berfläche\", which can be translated as \"field calculations involving bodies of arbitrary shape\".<ref>{{cite web |url=http://training.altairuniversity.com/analysis/feko-electromagnetic/|title=FEKO Suite 7.0 User Manual|website=Training.altairuniversity.com |publisher=EM Software & Systems-S.A. (Pty) Ltd |access-date=2015-04-02}}</ref> It is a general purpose 3D [[Comparison of EM simulation software|electromagnetic (EM) simulator]].\n\nFEKO originated in 1991 from research activities of Dr [[Ulrich Jakobus]] at the [[University of Stuttgart]], Germany.<ref>{{cite web|url=https://www.feko.info/about-us/News/ulrich-jakobus-named-ieee-fellow |title=Ulrich Jakobus named IEEE Fellow — |website=Feko.info |date= |accessdate=2016-09-28}}</ref> Cooperation between Dr Jakobus and EM Software & Systems (EMSS)<ref name=\"emss-history\">{{cite web|url=http://www.emss.co.za/history.php |title=EM Software & Systems |website=Emss.co.za |date= |accessdate=2016-09-28}}</ref> resulted in the commercialisation of FEKO in 1997.<ref>{{cite web|url=http://www.feko.info/about-us/quarterly/feko-quarterly-june-2014 |title=Archived copy |accessdate=2015-04-02 |deadurl=yes |archiveurl=https://web.archive.org/web/20140709024950/http://feko.info/about-us/quarterly/feko-quarterly-june-2014 |archivedate=2014-07-09 |df= }}</ref> In June 2014, Altair Engineering acquired 100% of EMSS-S.A. and its international distributor offices in the United States, Germany and China, leading to the addition of FEKO to the Altair Hyperworks suite of engineering simulation software.<ref name=\"emss-history\" /><ref>{{cite web|url=http://www.altair.com/newsdetail.aspx?news_id=10918 |title=Altair to Acquire EM Software & Systems – S.A. (EMSS) |website=Altair.com |date=2013-12-10 |accessdate=2016-09-28}}</ref><ref>{{cite web|url=http://www.altair.com/newsdetail.aspx?news_id=11023 |title=Altair and EM Software & Systems – S.A. (EMSS) Close Deal |website=Altair.com |date=2014-06-24 |accessdate=2016-09-28}}</ref><ref>{{cite web|url=http://www.prnewswire.com/news-releases/altair-and-em-software--systems---sa-emss-close-deal-264349111.html |title=Altair and EM Software & Systems - S.A. (EMSS) Close Deal |website=Prnewswire.com |date= |accessdate=2016-09-28}}</ref>\n\nThe software is based on the [[Computational electromagnetics#Method of moments (MoM) or boundary element method (BEM)|Method of Moments (MoM)]] integral formulation of [[Maxwell's equations]]<ref>{{cite book|last=Jakobus|first=Ulrich|title=Erweiterte Momentenmethode zur Behandlung kompliziert aufgebauter und elektrisch großer elektromagnetischer Streuprobleme|year=1995|publisher=VDI Verlag|location=Düsseldorf}}</ref> \n<ref>{{Citation\n  | last1 = Jakobus | first1 = U.\n  | last2 = Van Tonder | first2 = J.J.\n  | contribution = Fast Multipole Solution of Metallic and Dielectric Scattering Problems in FEKO\n  | title = 21st Annual Review of Progress in Applied Computational Electromagnetics\n  | publisher = Applied Computational Electromagnetics Society (ACES)\n  | year = 2005 }}</ref>\n<ref>{{Citation\n  | last1 = Jakobus | first1 = U.\n  | last2 = Van Tonder | first2 = J.J.\n  | contribution = Fast Multipole Acceleration of a MoM Code for the Solution of Composed Metallic/Dielectric Scattering Problems\n  | title = Advances in Radio Science 3\n  | publisher = Copernicus GmbH\n  | pages = 189–194\n  | year = 2005 }}</ref>\n<ref>\n{{Citation\n  | last1 = Jakobus | first1 = U.\n  | last2 = Van Tonder | first2 = J.J.\n  | contribution = Challenges Regarding the Commercial Implementation of the Parallel MLFMM in FEKO\n  | title = IEEE Antennas and Propagation Society International Symposium 2006\n  | publisher = IEEE\n  | pages = 95–98\n  | year = 2006 }}</ref> and pioneered the commercial implementation of various hybrid methods such as:\n\n* [[Computational electromagnetics#Finite Element Method (FEM)|Finite Element Method (FEM)]] / MoM where a FEM region is bounded with an integral equation based boundary condition to ensure full coupling between the FEM and MoM solution areas of the problem.\n* MoM / [[Computational electromagnetics#Physical optics (PO)|Physical Optics]] (PO) where computationally expensive MoM current elements are used to excite computationally inexpensive PO elements, inducing currents on the PO elements.  Special features in the FEKO implementation of the MoM/PO hybrid include the analysis of dielectric or magnetically coated metallic surfaces.<ref>{{Citation\n  | last1 = Jakobus | first1 = U.\n  | last2 = Theron | first2 = I.P.\n  | contribution = Analysis of Coated Metallic Surfaces with Physical Optics for the Solution of High-Frequency EMC Problems\n  | title = 15th International Zurich Symposium on Electromagnetic Compatibility\n  | pages = 257–261\n  | year = 2003 }}\n</ref>\n\n* MoM / [[Geometrical optics|Geometrical Optics]] (GO) where rays are launched from radiating MoM elements.<ref>{{Citation\n  | last1 = Jakobus | first1 = U.\n  | last2 = Bingle| first2 = M.\n  | last3 = Schoeman| first3 = M.\n  | last4 = van Tonder| first4 = J.J.\n  | last5 = Illenseer| first5 = F.\n  | contribution = Tailoring FEKO for Microwave Problems\n  | title = IEEE Microwave Magazine\n  | pages = 76–85\n  |date=December 2008}}\n</ref>\n\n* MoM / [[Computational electromagnetics#Uniform theory of diffraction (UTD)|Uniform Theory of Diffraction]] (UTD) where computationally expensive MoM current elements are used to excite canonical UTD shapes (plates, cylinders) with ray-based principles of which the computational cost is independent of wavelength.\n\nA [[Computational electromagnetics#Finite-difference time-domain (FDTD)|Finite Difference Time Domain (FDTD)]] solver was added in May 2014 with the release of FEKO Suite 7.0.<ref>{{cite web|url=http://www.feko.info/about-us/News/release-of-feko-suite-7.0-feature-overview |title=Release of FEKO Suite 7.0: Feature Overview |website=Feko.info |date= |accessdate=2016-09-28}}</ref>\n\n==References==\n{{Reflist|30em}}\n\n==External links==\n*{{cite web|url=http://www.feko.info |title=FEKO - EM Simulation Software |website=Feko.info |date= |accessdate=2016-09-28}}\n*{{cite web|url=http://www.altair.com |title=Using Simulation Technology to Synthesize and Optimize Designs, Processes and Decisions |website=Altair.com |date=2016-08-04 |accessdate=2016-09-28}}\n*{{cite web|url=http://www.altairhyperworks.com |title=HyperWorks: Open Architecture CAE solution |website=Altairhyperworks.com |date=2016-09-22 |accessdate=2016-09-28}}\n*{{cite web|url=http://www.antenna-theory.com/tutorial/feko/feko.php |title=Antenna Simulation Software - FEKO |website=Antenna-theory.com |date= |accessdate=2016-09-28}}\n\n{{DEFAULTSORT:Feko}}\n[[Category:Numerical software]]\n[[Category:Software that uses Qt]]\n[[Category:Electromagnetic simulation software]]"
    },
    {
      "title": "FlexPro",
      "url": "https://en.wikipedia.org/wiki/FlexPro",
      "text": "{{Infobox Software\n| name = FlexPro\n|developer = Weisang GmbH\n|operating_system = [[Microsoft Windows]]\n|genre = [[Numerical analysis]], [[Data Analysis]], [[:Category:Plotting software|plotting]]\n|latest_release_version = 2017\n|license = [[Proprietary software|Proprietary]]\n|website = {{URL|http://www.weisang.com/}}\n|released={{Start date and age|1991|df=yes}}|status=Active|language=English, German, Japanese, Chinese, French|logo=[[Image:FlexPro_software_logo.png|250px|FlexPro logo.]]}}\n\n'''FlexPro''' is a software package for analysis and presentation of scientific and technical data, produced by Weisang GmbH. It runs on [[Microsoft Windows]] and is available in English, German, Japanese, Chinese and French. FlexPro has its roots in the test and measurement domain and supports different binary file formats of data acquisition instruments and software. In particular, FlexPro can analyze large amounts of data with high sampling rates.\n\n== Features ==\nFlexPro is a software application for analyzing and presenting data. All data, analyses and presentations are stored in an object database.<ref name=\":0\">{{Cite journal|last=Beneke|first=Thomas W.|last2=Schwippert|first2=Wolfgang W.|date=October 2009|title=Software: FlexPro 8 – analysiert alles|url=http://www.pro-physik.de/details/physikjournalIssue/1089667/Issue_10_2009.html|journal=Physik Journal |volume=8 |issue=10|page=49|pages=|via=}}</ref><ref name=\":1\">{{Cite journal|last=|first=|date=December 2006|title=Review FlexPro 7|url=https://www.scientific-computing.com/review/flexpro-7|journal=Scientific Computing World|volume=|pages=|via=}}</ref><ref name=\":2\">{{Cite journal|last=|first=|date=July 2011|title=Review FlexPro 9|url=https://www.scientific-computing.com/review/flexpro-9|journal=Scientific Computing World|volume=|pages=|via=}}</ref><ref>{{Cite journal|last=Beneke|first=Thomas W.|last2=Schwippert|first2=Wolfgang W.|date=March 2006|title=FlexPro 7.0 – Datenanalyse auf höchstem Niveau|url=http://www.pro-physik.de/details/physikjournalIssue/1089745/Issue_3_2006.html|journal=Physik Journal |volume=5 |issue=3|pages=64|via=}}</ref> The structure of the database is similar to a file system on a hard drive. It is possible to build up a hierarchy of folders in FlexPro to organize the analysis. An entire FlexPro database can be stored in one file with size limited only by the hard drive space - not limited by the computer's RAM. The FlexPro user interface is based on Microsoft Office-like Ribbon technology.<ref name=\":3\">{{Cite journal|last=|first=|date=May 2017|title=Big Data für die Messtechnik|url=|journal=PC & Industrie – Zeitschrift für Mess-, Steuer- und Regeltechnik|volume=|pages=95|via=}}</ref>\n\nFlexPro provides wizards to create different 2D and 3D graphs as well as tables for data presentation. Typical graphs featured are line, symbol, scatter, bar, contour, waterfall, surface and polar plots. FlexPro also supports the media object (video format).<ref name=\":2\" /> FlexPro lets you create multi-page reports directly in the FlexPro project database.\n\nFlexPro has a built-in programming language, '''FPScript''', which is optimized for data analysis and supports direct operations on non-scalar objects such as vectors and matrices as well as composed data structures like signals, signal series or surfaces. All operations can be executed either graphically (through menus or dialog boxes) or programmatically. Programmatic access is provided through an Automation Object Model and the built-in Microsoft Visual Basic for Applications Development Environment (VBA).<ref name=\":1\" />\n\nData can be analyzed either graphically using cursors in 2D or 3D graphs or mathematically using analysis objects or FPScript formulas. The underlying algorithm of an analysis object can be parameterized through a property sheet. Raw data, analysis objects and presentation objects like graphs, tables and documents form a dynamic network<ref name=\":0\" /> which can be updated after new data has been imported. FlexPro supports time and frequency domain signal analysis,<ref name=\":1\" /> spectral analysis,<ref name=\":1\" /> order tracking, linear and non-linear curve fitting, descriptive and inductive statistics, event isolation,<ref name=\":2\" /> acoustics, FIR and IIR filtering as well as counting procedures (e.g. Rainflow-counting).\n[[File:FlexPro Project Database.png|left|thumb|250x250px|FlexPro 2017 Project Database]]\nFlexPro carries out all calculations not only with numbers, but also with physical quantities composed of a value and unit.<ref name=\":2\" /> In addition to SI units, FlexPro also handles popular non-SI units such as Gaussian units and US units.<ref name=\":2\" />\n\nFlexPro can export publication-quality graphs and reports to a number of file formats, including [[HTML]], [[JPEG]], [[Portable Network Graphics|PNG]], and [[Windows Metafile|WMF]].\n\nFlexPro’s Data Explorer indexes data archives on the hard drive or server.<ref name=\":3\" /> Configurable queries can be used to search for characteristic values or other data attributes and to find the data records to be evaluated.<ref name=\":3\" /> It is also possible to set up user-defined calculations during the indexing process which can be used for further data analysis.\n\nFlexPro supports various data files for import (standard file formats and binary file formats of data acquisition instruments and software): e.g. text and ASCII data (.csv and .txt), Excel workbooks, media files, wave files, ODBC data source, Matlab (.mat), National Instruments (.tdm, .tdms), ASAM ODS and ASAM COMMON MDF4, IMC Famos, NASA-CDF, Dewetron, DEWESoft, Graphtec, Hioki, IMC, Nicolet/Gould, OROS, SEFRAM, Viper, TEAC, Sony, Tektronik, Powermeter, Catman, Caesar, Imtec, Stemmer, Yokogawa, SPSS, LabView, Diadem, TurboLab, Systat, TableCurve.<ref name=\":0\" />\n\n== See also ==\n*[[List of information graphics software]]\n*[[List of numerical analysis software]]\n*[[Comparison of numerical analysis software]]\n\n==References==\n{{Reflist}}\n\n== External links ==\n* [https://www.weisang.com/en/ Official website]\n\n\n[[Category:Plotting software]]\n[[Category:Science software for Windows]]\n[[Category:Data visualization software]]\n[[Category:Numerical software]]\n[[Category:Regression and curve fitting software]]"
    },
    {
      "title": "Fractint",
      "url": "https://en.wikipedia.org/wiki/Fractint",
      "text": "{{ref improve|date=August 2016}}\n{{Infobox software\n| name                   = Fractint\n| logo                   = \n| screenshot             = Fractint_mandel.png\n| caption                = The [[Mandelbrot set]] rendered in Fractint\n| developer              = Stone Soup Group\n| released               = {{Start date and age|1988|09}}\n| latest release version = 20.04p14\n| latest release date    = {{Start date and age|2015|08|22}}\n| operating system       = [[DOS]], [[Linux]]\n| language               = English\n| genre                  = Fractal generating software\n| license                = [[Freeware]]\n| website                = {{URL|fractint.org}}\n}}\n\n'''Fractint''' is a [[freeware]] [[computer program]] that can render and display many kinds of [[fractal]]s. The program originated on the [[MS-DOS]] platform, but has since been ported to [[Linux]].  Additionally Tim Gilman has ported Fractint to the [[Macintosh]]. During the early 1990s the program was the definitive fractal generating program for personal computers.<ref>{{cite web |url=https://www.newscientist.com/article/mg13117834.200-review-fractint-brought-to-book-.html |title=Review: Fractint brought to book |author=Ray Girvan |date=24 August 1991 |work=Newscientist |publisher=Reed Business Information |accessdate=25 March 2013 }}</ref>\n\n==Name==\nIts name comes from the words ''fractal'' and ''[[integer]]'', since the first versions of it computed fractals by using only integer arithmetic (also known as [[fixed-point arithmetic]]), which led to much faster rendering on [[x86]] computers without math [[coprocessor]]s. Since then, [[floating-point arithmetic]] and \"[[Arbitrary-precision arithmetic|arbitrary-precision]]\" modes have been added, the latter of which emulates an arbitrarily large [[significand|mantissa]] in [[random access memory|RAM]]. The arbitrary-precision mode is slow even on modern computers.\n\n==Features==\nFractInt can draw most kinds of fractals that have appeared in the literature. It also has a few \"fractal types\" that are not strictly speaking fractals, but may be more accurately described as [[display hack]]s. These include [[Cellular automaton|cellular automata]].\n\n==History==\n[[File:Mandelbrot palette colour editing with Fractint.png|thumb|right|A Mandelbrot fractal with FractInt's colour palette editor (version 20.0 in DOSBOX 0.72 on Vista).]]\n[[File:Fract027.jpg|thumb|right|One portion of the Mandelbrot set at extreme magnification, showing how the set [[self-similarity|contains near copies of itself]].]]\nFractInt originally appeared in 1988 as '''FRACT386''', a computer program for rendering fractals very quickly on the [[Intel 80386]] processor using integer arithmetic.  Most 386 processors of the era did not come with [[floating point unit]]s (387), so the integer approach was much faster.\n\nThe early versions of FRACT386 were written by [[Bert Tyler]], who based it on a [[Mandelbrot set|Mandelbrot]] generator for a TI-based processor that used integer math and decided to try programming something similar for his 386 machine.<ref name=\"FractalCreations\">Tyler, Bert and Wegner, Timothy, ''Fractal Creations, 2nd edition'', Waite Group Press, 1993, {{ISBN|1-878739-34-4}}, p. 461</ref>\n\nIn February 1989, the program was renamed FractInt. In July 1990, it was ported to the Atari ST with the math routines rewritten in M68K assembler by Howard Chu.\n\nIt was written and maintained by the \"Stone Soup Group\" who took their name from the fable of the [[stone soup]]. Along with [[Emacs]] and [[NetHack]], it is one of the oldest still-maintained free programs.\n\n==See also==\n{{Portal|Computer graphics}}\n* [[Fractal art]]\n* [[Fractal-generating software]]\n* [[Sterling (program)|Sterling]]\n* [[XaoS]]\n{{Commons category|Fractals created with Fractint}}\n==References==\n{{Reflist}}\n\n==Further reading==\n* Michael Frame, [[Benoît B. Mandelbrot]], ''Fractals, Graphics, and Mathematics Education'', Volume 58 of [[Mathematical Association of America]] Notes, Cambridge University Press, 2002, {{ISBN|0-88385-169-5}}, pp.&nbsp;57–59 (and used throughout the book)\n\n<!--======================== {{No more links}} ============================\n    | PLEASE BE CAUTIOUS IN ADDING MORE LINKS TO THIS ARTICLE. Wikipedia  |\n    | is not a collection of links nor should it be used for advertising. |\n    |                                                                     |\n    |           Excessive or inappropriate links WILL BE DELETED.         |\n    | See [[Wikipedia:External links]] & [[Wikipedia:Spam]] for details.  |\n    |                                                                     |\n    | If there are already plentiful links, please propose additions or   |\n    | replacements on this article's discussion page, or submit your link |\n    | to the relevant category at the Open Directory Project (dmoz.org)   |\n    | and link back to that category using the {{dmoz}} template.         |\n    ======================= {{No more links}} =============================-->\n\n==External links==\n{{wikibooks|Fractals/fractint}}\n*[https://web.archive.org/web/20090323085051/http://www.nahee.com/spanky/www/fractint/fractint.html Mirror of now-gone spanky.triumf.ca Fractint site]\n*[https://web.archive.org/web/20161001153925/http://fractint.org/ Fractint Development Team WWW pages] via the [[Wayback Machine]]\n\n{{Fractal software}}\n{{Fractals}}\n{{Mathematics and art}}\n{{Computer science}}\n\n[[Category:1988 software]]\n[[Category:Fractal software]]\n[[Category:Numerical software]]\n[[Category:Cellular automaton software]]\n[[Category:DOS software]]\n[[Category:Proprietary freeware for Linux]]"
    },
    {
      "title": "GNU Scientific Library",
      "url": "https://en.wikipedia.org/wiki/GNU_Scientific_Library",
      "text": "{{Infobox software\n| name                   = GNU Scientific Library\n| title                  = \n| logo                   = \n| logo caption           = \n| screenshot             = \n| caption                = \n| collapsible            = \n| author                 = [[Mark Galassi]], James Theiler, Brian Gough, Gerard Jungman and many others\n| developer              = [[GNU Project]]\n| released               = {{Start date and age|1996|05}}\n| discontinued           = \n| latest release version = GSL-2.5\n| latest release date    = {{Start date and age|2018|06|14|df=yes/no}}\n| latest preview version = \n| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| programming language   = [[C (programming language)|C]]\n| operating system       = \n| platform               = \n| size                   = \n| language               = \n| status                 = \n| genre                  = [[List of numerical libraries|Numerical library]]\n| license                = [[GNU General Public License]]\n| alexa                  = \n| website                = {{URL|https://www.gnu.org/software/gsl/}}\n}}\n\nThe '''GNU Scientific Library''' (or '''GSL''') is a [[software library]] for numerical computations in [[applied mathematics]] and [[science]]. The GSL is written in [[C (programming language)|C]]; wrappers are available for other programming languages. The GSL is part of the [[GNU Project]]<ref>http://directory.fsf.org/GNU/</ref> and is distributed under the [[GNU General Public License]].\n\n== Project History ==\n\nThe GSL project was initiated in 1996 by physicists Mark Galassi and James Theiler of [[Los Alamos National Laboratory]].<ref name=\"GSL homepage as of oct 2012\">GSL homepage as of oct 2012.</ref> They aimed at writing a modern replacement for widely used but somewhat outdated Fortran libraries such as [[Netlib]].<ref>GSL design document https://www.gnu.org/software/gsl/design/gsl-design.html#SEC1 as of oct 2012.</ref>  They carried out the overall design and wrote early modules; with that ready they recruited other scientists to contribute.\n\nThe \"overall development of the library and the design and implementation of the major modules\" was carried out by [[Brian Gough]] and Gerard Jungman.<ref name=\"GSL homepage as of oct 2012\"/> Other major contributors were [[Jim Davies (cognitive scientist)|Jim Davies]], Reid Priedhorsky, M. Booth, and F. Rossi.<ref name=\"GSL homepage as of oct 2012\"/>\n\nVersion 1.0 was released in 2001. In the following, the library expanded only slowly; as the documentation stated, the maintainers were more interested in stability than in additional functionality. Major version 1 ended with release 1.16 of July 2013; this was the only public activity in the three years 2012-2014.\n\nVigorous development resumed with publication of version 2.0 in October 2015. The latest version 2.5 was released on 14 June 2018.\n\n== Example ==\nThe following example program calculates the value of the [[Bessel function]] for 5:<ref>https://www.gnu.org/software/gsl/manual/html_node/Using-the-library.html</ref>\n\n<source lang=\"c\">\n#include <stdio.h>\n#include <gsl/gsl_sf_bessel.h>\n\nint main(void)\n{\n  double x = 5.0;\n  double y = gsl_sf_bessel_J0(x);\n  printf(\"J0(%g) = %.18e\\n\", x, y);\n  return 0;\n}\n</source>\nThe example program has to be linked to the GSL library\nupon compilation:\n<source lang=\"bash\">\ngcc $(gsl-config --cflags) example.c $(gsl-config --libs)\n</source>\n\nThe output is shown below, and should be correct to [[double-precision]] accuracy:\n<pre>J0(5) = -1.775967713143382920e-01\n</pre>\n\n== Features ==\nThe software library provides facilities for:\n*[[elementary function|Basic mathematical functions]]\n*[[Complex number]]s\n*[[Polynomial]]s\n*[[B-Spline]]s\n*[[Special functions]]\n*[[Vector space|Vector]]s and [[Matrix (mathematics)|matrices]]\n*[[Permutation]]s\n*[[Combination]]s\n*[[Multiset]]s\n*[[Sorting]]\n*[[Basic Linear Algebra Subprograms|BLAS]]\n*[[Linear algebra]]\n*[[Eigensystem]]s\n*[[Fast Fourier transform]]s\n*[[Numerical integration]] (based on [[QUADPACK]])\n*[[Random number generator|Random number generation]]\n*[[Quasi-random sequences]]\n*[[probability distribution|Random number distributions]]\n*[[Statistics]]\n*[[Histogram]]s\n*[[N-tuple]]s\n*[[Monte Carlo integration]]\n*[[Simulated annealing]]\n*[[Ordinary differential equation]]s\n*[[Interpolation]]\n*[[Numerical differentiation]]\n*[[Chebyshev approximation]]s\n*[[Series acceleration]]\n*Discrete [[Hankel transform]]\n*[[Root-finding algorithm|Root-finding]] in one and multiple dimensions\n*[[Optimization (mathematics)|Minimization]] in one and multiple dimensions\n*[[least squares|Least-squares fitting]]\n*[[Levenberg-Marquardt algorithm|Nonlinear least-squares fitting]]\n*[[Physical constants]]\n*[[IEEE floating-point arithmetic]]\n*[[Discrete wavelet transform]]\n\n===Programming language bindings===\n\nSince the GSL is written in C, it is straightforward to provide wrappers for other programming languages. Such wrappers currently exist for\n* [[AMPL]]<ref>[https://ampl.github.io/amplgsl/ AMPL bindings for the GNU Scientific Library]</ref>\n* [[C++]]<ref name=\"gslmm\">[http://cholm.home.cern.ch/cholm/misc/#gslmm C++ wrappers for GSL]</ref>\n* [[Fortran]]<ref>[http://www.lrz.de/services/software/mathematik/gsl/fortran/ FGSL - A Fortran interface to the GNU Scientific Library]</ref>\n* [[Haskell (programming language)|Haskell]]<ref name=\"hmatrix-special\">[https://hackage.haskell.org/package/hmatrix-special The hmatrix-special package, an interface to GSL special functions.]</ref><ref name=\"hmatrix-gsl\">[https://hackage.haskell.org/package/hmatrix-gsl The hmatrix-gsl package, a purely functional interface to selected numerical computations, internally implemented using GSL.]</ref>\n* [[Java (programming language)|Java]]<ref>[https://github.com/bytedeco/javacpp-presets/tree/master/gsl JavaCPP Presets for GSL]</ref>\n* [[Common Lisp (programming language)|Lisp]]<ref>[http://common-lisp.net/project/gsll/ GSLL, GNU Scientific Library for Lisp]</ref>\n* [[Ocaml]]<ref>[https://mmottl.github.io/gsl-ocaml/ gsl-ocaml: OCaml bindings to the GSL]</ref>\n* [[Octave (programming language)|Octave]]\n* [[Perl Data Language]]\n* [[Python (programming language)|Python]]<ref>[http://pygsl.sourceforge.net/#pygsl Python interface for GNU Scientific Library]</ref>\n* [[R (programming language)|R]]<ref>[https://cran.r-project.org/web/packages/RcppGSL/index.html RcppGSL: 'Rcpp' Integration for 'GNU GSL' Vectors and Matrices]</ref><ref>[https://cran.r-project.org/web/packages/gsl/index.html gsl: wrapper for the Gnu Scientific Library]</ref>\n* [[Ruby (programming language)|Ruby]]<ref>[https://rubygems.org/gems/rb-gsl rb-gsl: Ruby/GSL is a Ruby interface to the GNU Scientific Library, for numerical computing with Ruby]</ref>\n\n=== C++ support ===\nThe GSL can be used in [[C++]] classes, but not using pointers to member functions, because the type of ''pointer to member function'' is different from ''pointer to function''.<ref>[http://www.parashift.com/c++-faq-lite/pointers-to-members.html#faq-33.1 pointer to member function] {{webarchive|url=https://web.archive.org/web/20041013202445/http://www.parashift.com/c++-faq-lite/pointers-to-members.html |date=2004-10-13 }}</ref> Instead, pointers to static functions have to be used. Another common work around is using a [[Function object|functor]]. C++ wrappers for GSL are available,<ref name=\"gslmm\"/> although many are not regularly maintained.\n\n== See also ==\n{{Portal|Free and open-source software}}\n* [[List of numerical analysis software]]\n* [[List of numerical libraries]]\n* [[Netlib]]\n* ''[[Numerical Recipes]]''\n\n==References==\n{{Reflist}}\n\n==External links==\n* {{Official website|https://www.gnu.org/software/gsl/}}\n* The [https://cran.r-project.org/web/packages/gsl/index.html '''gsl''' package] for [[R (programming language)]],  an R wrapper for the special functions and quasi random number generators.\n\n{{GNU}}\n\n[[Category:C libraries]]\n[[Category:Free computer libraries]]\n[[Category:Free software programmed in C]]\n[[Category:GNU Project software|Scientific Library]]\n[[Category:Mathematical libraries]]\n[[Category:Numerical libraries]]\n[[Category:Numerical software]]"
    },
    {
      "title": "GoldSim",
      "url": "https://en.wikipedia.org/wiki/GoldSim",
      "text": "{{Infobox Software\n| name                   = GoldSim\n| logo                   = GoldSim Technology Group Logo.JPG\n| caption                = GoldSim Logo\n| collapsible            = \n| author                 = \n| developer              = GoldSim Technology Group LLC\n| released               = \n| latest release version = 12.1.3\n| latest release date    = {{Start date and age|2019|03|21}}\n| programming language   = C++\n| operating system       = Windows\n| platform               = \n| size                   = \n| language               = \n| genre                  = [[Simulation software]]\n| license                = [[Proprietary software|Proprietary]]\n| website                = {{url|www.goldsim.com}}\n}}\n\n'''GoldSim''' is dynamic, [[probabilistic]] [[simulation software]] developed by GoldSim Technology Group. \nThis general-purpose simulator is a hybrid of several simulation approaches, combining an extension of [[system dynamics]] with some aspects of [[discrete event simulation]], and embedding the dynamic simulation engine within a [[Monte Carlo simulation]] framework.\n\nWhile it is a general-purpose simulator, GoldSim has been most extensively used for environmental and engineering [[Risk analysis (engineering)|risk analysis]], with applications in the areas of [[water resource]] management \n<ref name=\"water1\">\nAlfred Kalyanapu, Jason Lillywhite, Brantley Thames and Ebrahim Ahmadisharaf (2014),[https://www.researchgate.net/publication/261725545_Probabilistic_analysis_to_evaluate_the_effects_of_dam_breach_methodologies_on_downstream_flood_hazard Probabilistic Analysis To Evaluate The Effects Of Dam Breach Methodologies On Downstream Flood Hazard], ''Proceedings of the World Environmental & Water Resources Congress 2014'', Portland, Oregon.\n</ref>\n<ref name=\"water2\">\nErfan Goharian and Steven J. Burian (2014), [https://www.conftool.pro/hic2014/index.php/HIC2014-1447.pdf?page=downloadPaper&filename=HIC2014-1447.pdf&form_id=1447 Integrated Urban Water Resources Modeling In A Semi-Arid Mountainous Region Using A Cyberinfrastructure Framework], ''Proceedings of the 11th International Conference on Hydroinformatics, HIC 2014'', New York, New York. \n</ref>\n<ref name=\"water3\">\nEset Alemu, Richard Palmer, Austin Polebitski and Bruce Meaker (2011), [http://ascelibrary.org/doi/abs/10.1061/%28ASCE%29WR.1943-5452.0000088 Decision Support System for Optimizing Reservoir Operations Using Ensemble Streamflow Predictions], ''Journal of Water Resources Planning and Management'', 137(1), 72–82.\n</ref> \n<ref name=\"water4\">\nMichel Raymond (2014), [http://1b44afbef1bb776d712c-3dcd56c3560bcd90dd22a85e7e925b21.r1.cf1.rackcdn.com/Seqwater_Raymond.pdf Wivenhoe Somerset Dam Optimisation Study – Simulating Dam Operations for Numerous Floods], ''Proceedings of Australian National Committee on Large Dams (ANCOLD) Annual Conference 2014'', Canberra, Australia.\n</ref>\n,<ref name=\"water5\">\nLuke Toombes and Rob Ayre (2014),  [http://1b44afbef1bb776d712c-3dcd56c3560bcd90dd22a85e7e925b21.r1.cf1.rackcdn.com/Seqwater_Toombes_Ayre.pdf Holistic Dam Operations Assessment for Southeast Queensland], ''Proceedings of Australian National Committee on Large Dams (ANCOLD) Annual Conference 2014'', Canberra, Australia.\n</ref>\n[[mining]]\n<ref name=\"mining1\">\nBrent Usher, Roald Strand, Chris Strachotta and Jim Jackson (2010),  [http://www.imwa.info/docs/imwa_2010/IMWA2010_Usher_486.pdf Linking Fundamental Geochemistry And Empirical Observations For Water Quality Predictions Using GoldSim], Brazil, ''Proceedings of IMWA 2010, \"Mine Water and Innovative Thinking\"'', Wolkersdorfer. Ch. and Freund, A., p 313-316, Sydney, Nova Scotia, Canada.\n</ref>\n<ref name=\"mining2\">\nTed Eary, Jody Eshleman, Ryan Jakubowski and Andrew Watson (2008), [http://www.infomine.com/publications/docs/Eary2008.pdf Applying Numerical Hydrochemical Models as Decision Support Tools for Mine Closure Planning], presented at ''Tailings and Mine Waste ’08'', October 19–22, 2008, Vail, Colorado.\n</ref>\n,<ref name=\"mining3\">\nLisa Wade (2014), [http://search.proquest.com/docview/1545872190 A Probabilistic Water Balance], Dissertation for Montana Tech of The University of Montana, Copyright ProQuest, UMI Dissertations Publishing 2014.\n</ref>\n[[radioactive waste]] management\n<ref name=\"rad-waste1\">\nDavid Ewing Duncan (2003), [https://www.wired.com/wired/archive/11.04/yucca.html Do or Die at Yucca Mountain], ''Wired Magazine'', Issue 11.04, April 2003.\n</ref>\n<ref name=\"rad-waste2\">\nPatrick D. Mattie, Robert G. Knowlton & Bill W. Arnold. (2007). [http://infoserve.sandia.gov/sand_doc/2007/071354.pdf A User’s Guide to the GoldSim/BLT-MS Integrated Software Package: A Low-Level Radioactive Waste Disposal Performance Assessment Model]. ''Sandia Report (SAND2007-1354)''\n</ref>\n<ref name=\"rad-waste3\">\nD. Vopálka, D. Lukin and A. Vokál (2006), [http://www.springerlink.com/content/t7832q251l647281/ Modelling of processes occurring in deep geological repository — development of new modules in the GoldSim environment], ''Czechoslovak Journal of Physics'', Volume 56, Supplement 4 / December, 2006.\n</ref>\n<ref name=\"rad-waste4\">\nChris Markley et al. (2011), [http://pbadupws.nrc.gov/docs/ML1130/ML113060286.pdf SOAR: A Model For Scoping Of Options And Analyzing Risk Version 1.0 User Guide], Prepared for U.S. Nuclear Regulatory Commission Contract No. NRC–02–07–006, August 2011.\n</ref>\n,<ref name=\"rad-waste5\">\nJose Luis Cormenzana (2013), [http://www.posiva.fi/files/3178/WR_2013-25.pdf Probabilistic Sensitivity Analysis for the “Initial Defect in the Canister” Reference Model], Workreport 2013-25, Posiva Oy, Eurajoki, Finland.\n</ref>\ngeological [[Carbon capture and storage|carbon sequestration]]\n,<ref name=\"CO2-1\">\nPhilip H. Stauffer, Hari S. Viswanathan, Rajesh J. Pawar and George D. Guthrie (2009), [http://pubs.acs.org/doi/full/10.1021/es800403w A System Model for Geologic Sequestration of Carbon Dioxide], ''Environ. Sci. Technol.'', 2009, 43 (3), pp 565–570.\n</ref>\n[[aerospace]] mission risk analysis\n<ref name=\"nasa1\">\nDonovan L. Mathias, Susie Go, Ken Gee, and Scott Lawrence (2008), [https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20080046926_2008046464.pdf Simulation Assisted Risk Assessment Applied to Launch Vehicle Conceptual Design], ''NASA Center for AeroSpace Information (CASI).''\n</ref>\nand energy.<ref name=\"energy1\">\nSteven P. Miller, Jennifer E. Granata and Joshua S. Stein (2012),  [http://energy.sandia.gov/wp/wp-content/gallery/uploads/SAND2012-10342_final.pdf The Comparison of Three Photovoltaic System Designs Using the Photovoltaic Reliability and Performance Model (PV-RPM)], Sandia Report SAND2012-10342, Sandia National Laboratories, Albuquerque, New Mexico.\n</ref>\n\n==History==\n\nIn 1990, [[Golder Associates]], an international engineering consulting firm, was asked by the [[United States Department of Energy]] (DOE) to develop probabilistic simulation software that could be used to help with decision support and management within the Office of Civilian Radioactive Waste Management. The results of this effort were two [[MS-DOS|DOS]]-based programs (RIP and STRIP), which were used to support radioactive waste management projects within the DOE.\n\nIn 1996, in an effort funded by Golder Associates, the US DOE, the Japan Nuclear Cycle Development Institute (currently the [[Japan Atomic Energy Agency]]) and the [[:es:ENRESA|Spanish National Radioactive Waste Company]] (ENRESA), the capabilities of RIP and STRIP were incorporated into a general purpose [[Microsoft Windows|Windows]]-based simulator called GoldSim. Subsequent funding was also provided by [[NASA]].\n\nInitially only offered to the original funding organizations, GoldSim was released to the public in 2002.   In 2004, GoldSim Technology Group LLC was spun off from Golder Associates and is now a wholly independent company.<ref>[http://www.wwdmag.com/Golder-Associates-Launches-Independent-Software-Company-Based-on-GoldSim-Software-NewsPiece6502 Golder Associates Launches Independent Software Company Based on GoldSim Software] (2004), ''Water & Wastes DIGEST''</ref>\n\nNotable applications include providing the simulation framework for: 1) the [[Yucca Mountain nuclear waste repository|Yucca Mountain]] Repository Performance Assessment model developed by [[Sandia National Laboratories]];<ref name=\"rad-waste1\" /> 2) a comprehensive system-level computational model for performance assessment of [[Carbon capture and storage|geological sequestration of CO<sub>2</sub>]] developed by [[Los Alamos National Laboratory]];<ref name=\"CO2-1\" /> 3) a flood operations model to help better understand and fine tune operations of a large dam used for water supply and flood control in Queensland, Australia;<ref name=\"water4\" /><ref name=\"water5\" /> and 4) models for simulating risks associated with future manned space missions in NASA’s [[Constellation program]] developed by [[NASA Ames Research Center]].<ref name=\"nasa1\" />\n\n==Modeling Environment==\n\nGoldSim provides a visual and hierarchical modeling environment, which allows users to construct models by adding “elements” (model objects) that represent data, equations, processes or events, and linking them together into graphical representations that resemble [[influence diagrams]]. Influence arrows are automatically drawn as elements are referenced by other elements. Complex systems can be translated into hierarchical GoldSim models by creating layer of “containers” (or sub-models). Visual representations and hierarchical structures help users to build very large, complex models that can still be explained to interested stakeholders (e.g., government regulators, elected officials, and the public).\n\nThough it is primarily a continuous simulator, GoldSim has a number of features typically associated with [[Discrete event simulation|discrete simulators]].  By combining these two simulation methods, systems that are best represented using both continuous and discrete dynamics can often be more accurately simulated.  Examples include tracking the quantity of water in a reservoir that is subject to both continuous inflows and outflows, as well as sudden storm events; and tracking the quantity of fuel in a space vehicle as it is subjected to random perturbations (e.g., component failures, extreme environmental conditions).\n\nBecause the software was originally developed for complex environmental applications, in which many inputs are [[Uncertainty|uncertain]] and/or [[stochastic]], in addition to being a dynamic simulator, GoldSim is a [[Monte Carlo Method|Monte Carlo]] simulator, such that inputs can be defined as distributions and the entire system simulated a large number of times to provide probabilistic outputs.<ref>[http://www.goldsim.com/Web/Introduction/Probabilistic/ Probabilistic Simulation].  ''GoldSim website''.</ref>  As such, the software incorporates a number of computational features to facilitate probabilistic simulation of complex systems, including tools for generating and correlating stochastic [[time series]], advanced sampling capabilities (including [[latin hypercube sampling]], nested Monte Carlo analysis, and [[importance sampling]]), and support for [[distributed processing]].\n\n==See also==\n*[[Computer Simulation]]\n*[[Monte Carlo Simulation]]\n*[[List of computer simulation software]]\n\n==References==\n{{reflist|30em}}\n\n==External links==\n* {{official website|www.goldsim.com}}\n\n{{DEFAULTSORT:Goldsim}}\n[[Category:Simulation software]]\n[[Category:Risk management software]]\n[[Category:Scientific simulation software]]\n[[Category:Mathematical software]]\n[[Category:Environmental science software]]\n[[Category:Numerical software]]\n[[Category:Simulation programming languages]]\n[[Category:Probabilistic software]]\n[[Category:Science software for Windows]]"
    },
    {
      "title": "Graph-tool",
      "url": "https://en.wikipedia.org/wiki/Graph-tool",
      "text": "{{Infobox software\n| name                   = graph-tool\n| title                  = \n| logo                   = Graph-tool-logo.png\n| screenshot             = \n| caption                = \n| collapsible            = \n| author                 = \n| developer              = Tiago de Paula Peixoto\n| released               = <!-- {{Start date|YYYY|MM|DD|df=yes/no}} -->\n| discontinued           = \n| latest release version = 2.23\n| latest release date    = {{Start date and age|2017|09|25|df=yes/no}}\n| latest preview version = \n| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| programming language   = [[Python (programming language)|Python]], [[C++]]\n| operating system       = [[OS X]], [[Linux]]\n| platform               = \n| size                   = \n| language               = \n| status                 = \n| genre                  = [[Software library]]\n| license                = [[GPL]]\n| website                = {{URL|graph-tool.skewed.de}}\n}}\n'''graph-tool''' is a [[Python (programming language)|Python]] module for manipulation and statistical analysis of [[Graph (discrete mathematics)|graphs]] (AKA  [[Network theory|networks]]). The core data structures and algorithms of graph-tool are implemented in [[C++]], making extensive use of [[metaprogramming]], based heavily on the [[Boost Graph Library]]. This type of approach can confer a level of performance which is comparable (both in memory usage and computation time) to that of a pure [[C++]] library, which can be several orders of magnitude better than pure Python.<ref>[http://graph-tool.skewed.de/performance Graph-tool performance comparison], Graph-tool</ref>\n\nFurthermore, many algorithms are implemented in parallel using [[OpenMP]], which provides increased performance on [[Multi-core processor|multi-core]] architectures.\n\n== Features ==\n* Creation and manipulation of [[Directed graph|directed]] or [[Graph (discrete mathematics)|undirected graphs]].\n* Association of arbitrary information to the vertices, edges or even the graph itself, by means of property maps.\n* Filter vertices and/or edges \"on the fly\", such that they appear to have been removed.\n* Support for [[DOT language|dot]], [[Graph Modelling Language]] and [[GraphML]] formats.\n* Convenient and powerful [[graph drawing]] based on [[cairo (graphics)|cairo]] or [[Graphviz]].\n* Support for typical statistical measurements: degree/property histogram, combined degree/property histogram, vertex-vertex correlations, [[assortativity]], average vertex-vertex [[shortest path]], etc. \n* Support for several graph-theoretical algorithms: such as [[graph isomorphism]], [[Subgraph isomorphism problem|subgraph isomorphism]], [[minimum spanning tree]], [[Connected component (graph theory)|connected components]], [[Dominator (graph theory)|dominator]] tree, [[maximum flow]], etc.\n* Support for several [[centrality]] measures.\n* Support for [[clustering coefficient]]s, as well as [[network motif]] statistics and [[community structure]] detection.\n* Generation of [[random graphs]], with arbitrary degree distribution and correlations.\n* Support for well-established network models: [[preferential attachment|Price]], [[Barabási–Albert model|Barabási-Albert]], Geometric Networks, Multidimensional [[lattice graph]], etc.\n\n==Suitability==\n\nGraph-tool can be used to work with very large graphs in a variety of contexts, including simulation of [[epithelium|cellular tissue]],<ref>Bruno Monier et al, \"Apico-basal forces exerted by apoptotic cells drive epithelium folding\", Nature, 2015 [http://www.nature.com/nature/journal/vaop/ncurrent/full/nature14152.html]</ref> [[data mining]],<ref>Ma, Shuai, et al. \"Distributed graph pattern matching.\" Proceedings of the 21st international conference on World Wide Web. ACM, 2012. [http://dl.acm.org/citation.cfm?id=2187963]</ref><ref>Ma, Shuai, et al. \"Capturing topology in graph pattern matching.\" Proceedings of the VLDB Endowment 5.4 (2011): 310-321. [http://dl.acm.org/citation.cfm?id=2095690]</ref> analysis of [[Social network theory|social networks]],<ref>Janssen, E., M. A. T. T. Hurshman, and N. A. U. Z. E. R. Kalyaniwalla. \"Model selection for social networks using graphlets.\" Internet Mathematics (2012). [http://www.mathstat.dal.ca/~janssen/papers/Graphlets.pdf]</ref><ref>Asadi, Hirad Cyrus. Design and implementation of a middleware for data analysis of social networks. Diss. M Sc thesis report, KTH School of Computer Science and Communication, Stockholm, Sweden, 2007. [http://www.student.nada.kth.se/~hias02/xjobb-en.pdf]</ref> analysis of [[Peer-to-Peer|P2P]] systems,<ref>Teresniak, Sven, et al. \"Information-Retrieval in einem P2P-Netz mit Small-World-Eigenschaften Simulation und Evaluation des SemPIR-Modells.\"[http://asv.informatik.uni-leipzig.de/thesis/thesis_document/25/masterarbeit.pdf]</ref> large-scale modeling of [[Agent-based model|agent-based systems]],<ref>Hamacher, Kay, and Stefan Katzenbeisser. \"Public security: simulations need to replace conventional wisdom.\" Proceedings of the 2011 workshop on New security paradigms workshop. ACM, 2011.  [http://dl.acm.org/citation.cfm?id=2073288]</ref>\nstudy of academic [[Genealogy tree]]s,<ref>Miyahara, Edson Kiyohiro, Jesus P. Mena-Chalco, and Roberto M. Cesar-Jr. \"Genealogia Acadêmica Lattes.\" [http://www.linux.ime.usp.br/~edsonkm/mac499/download/monografia.pdf]</ref> theoretical assessment and modeling of network [[Clustering coefficient|clustering]],<ref>Abdo, Alexandre H., and A. P. S. de Moura. \"Clustering as a measure of the local topology of networks.\" arXiv preprint physics/0605235 (2006). [https://arxiv.org/abs/physics/0605235]</ref> large-scale [[call graph]] analysis,<ref>Narayan, Ganesh, K. Gopinath, and V. Sridhar. \"Structure and interpretation of computer programs.\" Theoretical Aspects of Software Engineering, 2008. TASE'08. 2nd IFIP/IEEE International Symposium on. IEEE, 2008. [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4549888]</ref><ref>Campos, José Creissac, et al. \"GUIsurfer: A Reverse Engineering Framework for User Interface Software.\" [http://cdn.intechopen.com/pdfs/30511/InTech-Guisurfer_a_reverse_engineering_framework_for_user_interface_software.pdf]</ref> and analysis of the brain's [[Connectome]].<ref>Gerhard, Stephan, et al. \"The connectome viewer toolkit: an open source framework to manage, analyze, and visualize connectomes.\" Frontiers in neuroinformatics 5 (2011). [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3112315/]</ref>\n\n==References==\n{{Reflist|30em}}\n\n==External links==\n* {{official website|graph-tool.skewed.de}}\n\n{{Portal|Free and open-source software}}\n\n{{DEFAULTSORT:graph-tool}}\n[[Category:Numerical software]]\n[[Category:Free mathematics software]]\n[[Category:Free software programmed in Python]]\n[[Category:Free software programmed in C++]]\n[[Category:Graph drawing software]]"
    },
    {
      "title": "Gretl",
      "url": "https://en.wikipedia.org/wiki/Gretl",
      "text": "{{Lowercase|title=gretl}}\n{{Infobox software\n| name                   = gretl\n| logo                   = Gretl logo.png\n| logo caption           = \n| screenshot             = Gretl screenshot.png\n| caption                = [[Screenshot]] of gretl\n| developer              = [http://sourceforge.net/project/memberlist.php?group_id=36234 the gretl team]\n| released               = {{Start date and age|2000|01|31|df=yes}}\n| latest release version = 2019b\n| latest release date    = {{Start date and age|2019|05|21|df=yes}}\n| latest preview version = Through [[git (software)|git]]\n| latest preview date    = \n| programming language   = [[C (programming language)|C]]\n| operating system       = [[Cross-platform]]\n| language               = [[Multilingual]] (11)\n| genre                  = [[Statistical software]]\n| license                = [[GNU General Public License|GNU GPLv3]]\n| website                = {{URL|gretl.sourceforge.net}}\n}}\n'''gretl''' is an [[Open-source software|open-source]] [[statistical package]], mainly for [[econometrics]]. The name is an acronym for [[GNU General Public License|''G''nu]] [[Regression analysis|''R''egression]], ''E''conometrics and [[Time series|''T''ime-series]] ''L''ibrary.\n\nIt has both a [[graphical user interface]] (GUI) and a [[command-line interface]]. It is written in [[C (programming language)|C]], uses [[GTK+]] as [[widget toolkit]] for creating its GUI, and calls [[gnuplot]] for generating graphs. The native scripting language of gretl is known as hansl (see below); it can also be used together with [[TRAMO|TRAMO/SEATS]], [[R (programming language)|R]], [[Stata]], [[Python (programming language)|Python]], [[GNU Octave|Octave]], [[Ox (programming language)|Ox]] and [[Julia (programming language)|Julia]].\n\ngretl can output models as [[LaTeX]] files.\n\nBesides [[English language|English]], gretl is also available in [[Albanian language|Albanian]], [[Basque language|Basque]], [[Bulgarian language|Bulgarian]], [[Catalan language|Catalan]], [[Chinese language|Chinese]], [[Czech language|Czech]], [[French language|French]], [[Galician language|Galician]], [[German language|German]], [[Greek language|Greek]], [[Italian language|Italian]], [[Polish language|Polish]], [[Portuguese language|Portuguese]] (both varieties), [[Romanian language|Romanian]], [[Russian language|Russian]], [[Spanish language|Spanish]], [[Turkish language|Turkish]] and [[Ukrainian language|Ukrainian]].\n\nGretl has been reviewed several times in the ''[[Journal of Applied Econometrics]]''<ref>{{cite journal|title=GRETL: Econometric software for the GNU generation | doi=10.1002/jae.704 | volume=18|journal=Journal of Applied Econometrics|pages=105–110|year = 2003|last1 = Baiocchi|first1 = Giovanni| last2=Distaso | first2=Walter | citeseerx=10.1.1.466.7942 }}</ref><ref>{{cite journal|url=http://www3.interscience.wiley.com/cgi-bin/abstract/114277452/ABSTRACT|archive-url=https://archive.today/20121216134245/http://www3.interscience.wiley.com/cgi-bin/abstract/114277452/ABSTRACT|dead-url=yes|archive-date=2012-12-16|title=GRETL: 1.6.0 and its numerical accuracy}}</ref><ref>{{cite journal|title=Teaching undergraduate econometrics with GRETL | doi=10.1002/jae.927 | volume=21| issue=7 |journal=Journal of Applied Econometrics|pages=1103–1107|year = 2006|last1 = Mixon Jr|first1 = J. Wilson| last2=Smith | first2=Ryan J. }}</ref> and, more recently, in the ''[[Australian Economic Review]]''.<ref>{{cite journal|title=Practical Empirical Research Using gretl and hansl|journal=Australian Economic Review|volume=52|issue=2|pages=255–271|doi=10.1111/1467-8462.12324|year=2019|last1=Tarassow|first1=Artur}}</ref>\n\nA review also appeared in the ''[[Journal of Statistical Software]]''<ref>{{cite journal|title=gretl 1.7.3|journal=Journal of Statistical Software|volume=25|issue=1|pages=1–14|doi=10.18637/jss.v025.s01|year=2008|last1=Rosenblad|first1=Andreas}}</ref> in 2008. Since then, the journal has featured several articles in which gretl is used to implement various statistical techniques.\n\n== Supported data formats ==\ngretl offers its own fully documented, [[XML]]-based data format.\n\nIt can also import [[ASCII]], [[CSV (file format)|CSV]], [[Databank format|databank]], [[EViews]], [[Microsoft Excel|Excel]], [[Gnumeric]], [[GNU Octave]], [[JMulTi]], [[OpenDocument]] spreadsheets, [[PcGive]], [[Regression Analysis of Time Series|RATS 4]], [[SAS (software)|SAS]] xport, [[SPSS]], and [[Stata]] files. It can export to [[Stata]], [[GNU Octave]], [[R (programming language)|R]], [[CSV (file format)|CSV]], [[JMulTi]], and [[PcGive]] file formats.\n\n== hansl ==\nGretl has its own [[scripting language]], called ''hansl'' (which is a [[recursive acronym]] for Hansl's A Neat Scripting Language). \n\nHansl is a Turing-complete, interpreted programming language, featuring loops, conditionals, user-defined functions and complex data structures.<ref>{{cite web|url=http://ricardo.ecn.wfu.edu/pub/gretl/manual/PDF/hansl-primer-a4.pdf|title=A Hansl Primer}}</ref> It can be considered a [[domain-specific language]] for econometrics.<ref>{{cite book|chapter-url=https://dl.acm.org/citation.cfm?id=3039896|title=Hansl: a DSL for econometrics|pages=1–10|doi=10.1145/3039895.3039896|chapter=Hansl|year=2017|last1=Cottrell|first1=Allin|isbn=9781450348454}}</ref> Like other scientifically oriented programming languages, such as [[MATLAB]] and [[Julia (programming language)|Julia]], matrices are supported natively as a primitive variable type. The gretl add-ons known as ''function packages'' are written in hansl.<ref>{{cite web|url=http://ricardo.ecn.wfu.edu/gretl/cgi-bin/gretldata.cgi?opt=SHOW_FUNCS|title=gretl function packages}}</ref>\n\nHere's a simple example of hansl\n{{pre|1=\nmatrix A = {1, 2 ; 3, 4}\nmatrix B = [[Invertible matrix|inv]](A)\nmatrix C = A*B\n\nprint A B C\n\nloop i=-3..3\n    [[printf]] \"[[Phi]](%d) = %7.3f\\n\", i, [[Cumulative distribution function|cdf]](N, i)\nendloop\n}}\n\nRunning the above code produces \n<pre>\nA (2 x 2)\n\n  1   2 \n  3   4\n\nB (2 x 2)\n\n    -2      1 \n   1.5   -0.5\n\nC (2 x 2)\n\n      1.0000       0.0000 \n  8.8818e-16       1.0000\n\nPhi(-3) =   0.001\nPhi(-2) =   0.023\nPhi(-1) =   0.159\nPhi( 0) =   0.500\nPhi( 1) =   0.841\nPhi( 2) =   0.977\nPhi( 3) =   0.999\n</pre><br />\n\n== Random Number Generation ==\nRandom Number Generation (RNG) in ''gretl'' has been examined and tested in Yalta & Schreiber (2012)<ref>{{Cite journal|last=Yalta|first=A. Talha|last2=Schreiber|first2=Sven|date=2012|title=Random Number Generation ingretl|journal=Journal of Statistical Software|volume=50|issue=Code Snippet 1|doi=10.18637/jss.v050.c01|issn=1548-7660}}</ref> . The authors conclude  \"Our results show that the RNG related procedures in ''gretl'' are implemented soundly and perform well in the three crush test suites of the TestU01\".\n<br />\n== Gretl as a teaching tool ==\n\nDue to its libre nature and the breadth of econometric techniques it contains, gretl is widely used for teaching econometrics, from the undergraduate level onwards. Datasets in gretl format are available for several popular textbooks.\n\nThe following is a list of textbooks that use gretl as their software of choice:\n \n* Dougherty, Christopher ''Introduction to Econometrics'' (Oxford University Press)\n* Kufel, Tadeusz ''Ekonometria'' (Wydawnictwo Naukowe PWN); in Polish (Russian version also available)\n\nIn addition, a free supplement<ref>{{cite web|url=http://www.learneconometrics.com/gretl/index.html|title=Lee Adkins' book}}</ref> to Hill, Griffiths and Lim ''Principles of Econometrics'' (Wiley) is available.\n\n== See also ==\n{{Portal|Free and open-source software}}\n* [[Comparison of statistical packages]]\n* [[MATLAB]]\n* [[Scilab]]\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* {{sourceforge|gretl}}\n* [http://gretlwiki.econ.univpm.it/wiki/index.php/Main_Page Gretl wiki]\n* [http://www.learneconometrics.com/gretl.html GretlWeb]\n* [http://ricardo.ecn.wfu.edu/pub//gretl/manual/en/gretl-guide.pdf ''Gretl User's Guide'']\n* [http://www.learneconometrics.com/gretl/index.html ''Lee Adkins's Using gretl for Principles of Econometrics'']\n* [http://ricardo.ecn.wfu.edu/pub//gretl/manual/en/gretl-ref.pdf ''Gretl Command Reference'']\n* [http://www.gretlconference.org Gretl Conference. Berlin, 2015]\n\n{{GNU}}\n{{Numerical analysis software}}\n{{Statistical software}}\n\n[[Category:Cross-platform free software]]\n[[Category:Cross-platform software]]\n[[Category:Econometrics software]]\n[[Category:Free econometrics software]]\n[[Category:Free multilingual software]]\n[[Category:Free plotting software]]\n[[Category:Free software programmed in C]]\n[[Category:Free statistical software]]\n[[Category:GNU Project software]]\n[[Category:Numerical analysis software for Linux]]\n[[Category:Numerical analysis software for MacOS]]\n[[Category:Numerical analysis software for Windows]]\n[[Category:Numerical programming languages]]\n[[Category:Numerical software]]\n[[Category:Plotting software]]\n[[Category:Regression and curve fitting software]]\n[[Category:Science software that uses GTK]]\n[[Category:Statistical programming languages]]\n[[Category:Time series software]]"
    },
    {
      "title": "HippoDraw",
      "url": "https://en.wikipedia.org/wiki/HippoDraw",
      "text": "{{Unreferenced|date=May 2013}}\n{{Infobox Software \n| name = HippoDraw\n| logo = \n| screenshot = hippodraw-screen.png\n| caption = HippoDraw canvas window\n| developer = [[Paul Kunz|Paul F. Kunz]]\n| latest_release_version = 1.21.3\n| latest_release_date = {{Start date and age|2007|10}}\n| operating_system = [[Cross-platform]]\n| programming language   = [[C++]]\n| genre = [[Data analysis]]\n| license = [[GNU General Public License|GPLv2]]+\n| website = {{URL|www.slac.stanford.edu/grp/ek/hippodraw/}}\n}}\n'''HippoDraw''' is a powerful [[Object-oriented programming|object-oriented]] statistical [[data analysis]] [[Software package (installation)|package]] written in [[C++]], with user interaction via a [[Qt (toolkit)|Qt]]-based [[Graphical user interface|GUI]] and a [[Python (programming language)|Python]]-scriptable interface. It is being developed by [[Paul Kunz]] at [[SLAC]], primarily for the analysis and presentation of [[particle physics]] and [[astrophysics]] data, but can be equally well used in other fields where data handling is important.\n\nHippoDraw can read and write files in an XML-based format, astrophysics [[FITS]] files, data objects produced by [[ROOT]] (optional), and through the Python bindings, anything that can be read/written by Python ([[Hierarchical Data Format|HDF5]], for instance, with PyTables).\n\nHippoDraw can be used as a Python extension module, allowing users to use HippoDraw data objects with the full power of the Python language. This includes other scientific Python extension modules such Numeric and numarray, whose use with HippoDraw can lead to a large increase in processing speed, even for ROOT objects.<!--\nis this still true?\n\nBoth HippoDraw and [[Java Analysis Studio|JAS]] implement the [[AIDA (computing)|AIDA]] interface specifications, and as such can interoperate well. \n-->\n\n== See also ==\n{{Portal|Free and open-source software}}\n* [[Java Analysis Studio]] (JAS)\n* [[ROOT]]\n* [[AIDA (computing)|AIDA]]\n\n== External links ==\n* {{Official website|http://www.slac.stanford.edu/grp/ek/hippodraw/}}\n* [https://www-glast.stanford.edu/cgi-bin/viewvc/LICENSE?revision=1.2&root=HIPPO&view=markup License]\n\n[[Category:Data analysis software]]\n[[Category:Free plotting software]]\n[[Category:Free science software]]\n[[Category:Free software programmed in C++]]\n[[Category:Free software projects]]\n[[Category:Free statistical software]]\n[[Category:Numerical software]]\n[[Category:Physics software]]\n[[Category:Science software for Linux]]\n[[Category:Science software for MacOS]]\n[[Category:Science software for Windows]]\n[[Category:Science software that uses Qt]]\n\n\n{{free-software-stub}}"
    },
    {
      "title": "HOBBIES (electromagnetic solver)",
      "url": "https://en.wikipedia.org/wiki/HOBBIES_%28electromagnetic_solver%29",
      "text": "{{multiple issues|\n{{Technical|date=August 2018}}\n{{Orphan|date=February 2013}}\n}}\n\n'''HOBBIES''' is a general purpose [[Electromagnetic field solver|electromagnetic solver]] for various [[Computer application|applications]].<ref>Y. Zhang, T. K. Sarkar, X. Zhao, D. Garcia-Donoro, W. Zhao, M. Salazar, and S. Ting, Higher Order Basis Based Integral Equation Solver (HOBBIES), John Wiley, Hoboken, NJ, 2012. (has the code in the book) {{ISBN|9781118140659}}.</ref> The name is an acronym for '''H'''igher '''O'''rder '''B'''asis '''B'''ased '''I'''ntegral '''E'''quation '''S'''olver. The [[software]] is based on the [[Method of moments (statistics)|Method of Moments]] (MoM),<ref>R. F. Harrington (1968). Field Computation by Moment Methods. Latest printing by IEEE Press in 1993, {{ISBN|0780310144}}.</ref> and it employs [[Higher-order function|higher order]] [[polynomial]]s as the [[basis function]]s for the [[frequency domain]] [[integral equation]] solver. The higher-order basis functions can significantly reduce the number of [[Unknown (math)|unknowns]] compared with the traditional piece-wise basis functions, e.g., Rao-Wiltion-Glisson triangular patch basis functions (RWGs).\n\nHOBBIES can be used to solve various types of [[electromagnetic field]] problems including antenna design, antenna placement, [[scattering]] analysis, [[Electromagnetic induction|EMI]]/[[Electromagnetic compatibility|EMC]] analysis, etc. The software pioneered the commercial implementation of [[Parallel computing|parallel computation]] for solving extremely electrically large problems using modest [[computational resource]]s.<ref>Y. Zhang and T. K.Sarkar, Parallel Solution of Integral Equation Based EM Problems in the Frequency Domain, Wiley-IEEE Press, 2009, {{ISBN|9780470405451}}.</ref>\n\n==References==\n{{reflist}}\n\n==External links==\n* {{Official website|www.em-hobbies.com}}\n\n[[Category:Numerical software]]\n[[Category:Electronic design automation software]]\n[[Category:Electromagnetic simulation software]]\n\n\n{{Software-stub}}"
    },
    {
      "title": "HSL (Fortran library)",
      "url": "https://en.wikipedia.org/wiki/HSL_%28Fortran_library%29",
      "text": "'''HSL''', originally the '''Harwell Subroutine Library''', is a collection of [[Fortran 77]] and 95 codes that address core problems in [[numerical analysis]]. It is primarily developed by the Numerical Analysis Group at the [[Rutherford Appleton Laboratory]] with contributions from other experts in the field.\n\nHSL codes are easily recognizable by the format of their names, consisting of two letters followed by two numbers, dating back to early versions of Fortran's limited [[subroutine]] name length. The letters denote a broad classification of the problem they solve, and the numbers serve to distinguish different codes. For example, the well known sparse LU code ''MA28'' (superseded by ''MA48'') is a Matrix Algebra code number 28. Fortran 95 codes are differentiated from Fortran 77 codes by the prefix ''HSL_''.\n\n== History ==\n\n=== Early history ===\nOriginal development of the Harwell Subroutine Library began in 1963 by Mike Powell and Mike Hopper for internal use on an IBM mainframe at [[Atomic Energy Research Establishment|AERE Harwell]]. Early contributors also included Alan Curtis. With a spreading reputation, the Library was distributed externally for the first time in 1964 upon request.  The first library catalog (AERE Report M-1748) was released in 1966.\n\n=== Recent history ===\nOver the intervening years, HSL has striven to maintain a high standard of reliability and has garnered a worldwide reputation as a prime source of numerical software. It has undergone a number of changes to reflect newly available features of the Fortran language, completing in 1990 the conversion to Fortran 77, and more recently, the entire Library has been made [[thread safe]]. Many newer codes are written in Fortran 95.\n\nNew packages continue to be developed, with a new release issued every two to three years. Many older codes have now been superseded and are available in the HSL Archive.\n\n== Licensing ==\nThe current version, ''HSL 2007'' is a commercial product sold by [[Aspen Technology|AspenTech]], but is also available without charge to individual academics direct from STFC for teaching and their own academic research purposes.<ref>{{cite web | url =http://www.hsl.rl.ac.uk/| title = The HSL Mathematical Software Library | work=STFC | accessdate=2010-07-05}}</ref>\n\nObsolete routines are stored in the ''HSL archive'' and are available for personal non-commercial use by anyone following registration with HSL. Commercial use and distribution of these routines still requires a purchased licence.<ref>{{cite web | url =http://www.hsl.rl.ac.uk/archive/hslarchive.html | title = HSL archive | work=STFC | accessdate=2010-07-05}}</ref>\n\n== References ==\n* J.K.Reid and J.A.Scott (Dec 2006, Sep 2007), ''Guidelines for the development of HSL software'', Technical Report [ftp://ftp.numerical.rl.ac.uk/pub/reports/rsRAL2006031.pdf RAL-TR-2006-031]\n* M.J.D.Powell ''25 years of Theoretical Physics 1954-1979: Chapter XVIII: Numerical Analysis''. A special publication by Harwell Research Laboratory of UKAEA\n\n== Footnotes ==\n<references/>\n\n== External links ==\n* [http://www.hsl.rl.ac.uk/ HSL home page at STFC]\n* [http://www.aspentech.com/hsl HSL home page at AspenTech]\n* [http://hsl.rl.ac.uk/archive/hslarchive.html HSL Archive]\n\n[[Category:Fortran libraries]]\n[[Category:Numerical software]]\n[[Category:Science and Technology Facilities Council]]\n\n\n{{compu-library-stub}}"
    },
    {
      "title": "JAMA (numerical linear algebra library)",
      "url": "https://en.wikipedia.org/wiki/JAMA_%28numerical_linear_algebra_library%29",
      "text": "{{Infobox software\n| author                 = [[NIST]]\n| operating system       = [[Cross-platform]]\n| released = 1998\n| latest release version = 1.0.3\n| latest release date    = {{release date|2012|11|09}}\n| genre                  = Library\n| license                = [[Public domain software]]\n| website                = {{url|http://math.nist.gov/javanumerics/jama/}}\n}}\n'''JAMA''' is a [[software library]] for performing numerical [[linear algebra]] tasks created at [[National Institute of Standards and Technology]] in 1998 similar in functionality to [[LAPACK]]. \n\n== Functionality ==\nThe main capabilities provided by JAMA are:\n* [[Eigensystem]] solving\n* [[LU decomposition]]\n* [[Singular value decomposition]]\n* [[QR decomposition]]\n* [[Cholesky decomposition]] \n\nVersions exist for both [[C++]] and the [[Java programming language]]. The C++ version uses the [[Template Numerical Toolkit]] for lower-level operations.  The Java version provides the lower-level operations itself.  \n\n==History==\nAs work of US governmental organization the algorithm and [[source code]] have been released to the [[public domain]] around 1998.<ref>[http://math.nist.gov/javanumerics/jama/ JAMA : A Java Matrix Package] on math.nist.gov</ref> JAMA has had little development since the year 2000,<ref name=\"ChangeLog\"/> with only the occasional bug fix being released. The project's webpage contains the following statement, ''\"(JAMA) is no longer actively developed to keep track of evolving usage patterns in the Java language, nor to further improve the API. We will, however, fix outright errors in the code.\"'' <ref name=\"ProjectPage\"/> The last bug fix was released November 2012, with the previous one being released in 2005.\n\n== Usage Example ==\nExample of Singular Value Decomposition (SVD):\n<source lang=\"java\">\nSingularValueDecomposition s = matA.svd();\n\nMatrix U = s.getU();\nMatrix S = s.getS();\nMatrix V = s.getV();\n</source>\n\nExample of matrix multiplication:\n<source lang=\"java\">\nMatrix result = A.times(B);\n</source>\n\n==See also==\n* [[List of numerical libraries]]\n\n== References ==\n{{reflist|30em|refs=\n<ref name=ChangeLog>{{cite web |url=http://math.nist.gov/javanumerics/jama/ChangeLog |title=JAMA Change Log |date=November 8, 2012 |work=JAMA|publisher=NIST|accessdate=November 30, 2012}}</ref>\n<ref name=ProjectPage>{{cite web |url=http://math.nist.gov/javanumerics/jama/ |title=JAMA Project Page |work=JAMA|publisher=NIST|accessdate=November 30, 2012}}</ref>\n}}\n\n== External links ==\n* [http://math.nist.gov/tnt/download.html JAMA/C++ download and documentation page] at NIST\n* [http://math.nist.gov/javanumerics/jama/ JAMA/Java homepage] at NIST\n\n[[Category:Numerical software]]\n[[Category:Public-domain software with source code]]\n[[Category:C++ numerical libraries]]"
    },
    {
      "title": "Java Analysis Studio",
      "url": "https://en.wikipedia.org/wiki/Java_Analysis_Studio",
      "text": "{{merge|AIDA (computing)|date=June 2018}}\n{{More citations needed|date=March 2016}}\n{{Infobox software\n| name = Java Analysis Studio\n| logo = \n| screenshot = \n| caption = \n| developer = FreeHEP\n| latest_release_version = JAS3, 3.0.3\n| latest_release_date = {{Start date and age|2013|06|10}}\n| operating_system = [[Cross-platform]]\n| genre = Data recording\n| license = [[LGPL]]\n| website = {{URL|jas.freehep.org/jas3/}}\n}}\n\n'''Java Analysis Studio''' (JAS) is an [[Object oriented programming|object oriented]]  [[data analysis]] [[Software package (installation)|package]] developed for the analysis of [[particle physics]] data. The latest major version is JAS3.\n\nJAS3 is particularly notable for being a fully [[AIDA (computing)|AIDA]]-compliant data analysis system. It is popular for data analysis in areas of particle physics which are familiar with the [[Java (programming language)|Java programming language]].\n\nThe Studio uses many other libraries from the [[FreeHEP]] project.\n\n== External links ==\n{{Portal|Java (programming language)}} \n* [http://jas.freehep.org/jas3/ Java Analysis Studio 3] website\n* [http://aida.freehep.org/ AIDA: Abstract Interfaces for Data Analysis] &mdash; open interfaces and formats for particle physics data processing\n\n[[Category:Data analysis software]]\n[[Category:Experimental particle physics]]\n[[Category:Free software programmed in Java (programming language)]]\n[[Category:Free statistical software]]\n[[Category:Numerical software]]\n[[Category:Physics software]]"
    },
    {
      "title": "LabVIEW",
      "url": "https://en.wikipedia.org/wiki/LabVIEW",
      "text": "{{third-party|date=May 2015}}\n{{Infobox Software\n| name = LabVIEW\n| logo = [[Image:LabVIEW_Logo.jpg|right|300px|LabVIEW logo.]]\n| screenshot =\n| caption =\n| developer = [[National Instruments]]\n| released               = {{Start date and age|1986|df=yes}}\n| latest release version = LabVIEW NXG 3.1\nLabVIEW 2019\n| latest release date = {{Start date and age|2019|05}}\n| operating system = [[Cross-platform]]: [[Microsoft Windows|Windows]], [[macOS]], [[Linux]]\n| genre = [[Data acquisition]], [[instrument control]], [[test automation]], analysis and [[signal processing]], [[Industrial control system|industrial control]], [[embedded system]] design\n| license = [[Proprietary software|Proprietary]]\n| website = {{URL|https://www.ni.com/labview}}\n|programming language=C, C++, .NET}}\n'''Laboratory Virtual Instrument Engineering Workbench''' ('''LabVIEW''')<ref name=kring2006>{{Cite book|url=https://www.worldcat.org/oclc/67361308|title=LabVIEW for everyone : graphical programming made easy and fun.|last=Jeffrey.|first=Travis,|date=2006|publisher=Prentice Hall|others=Kring, Jim.|isbn=0131856723|edition=3rd|location=Upper Saddle River, NJ|oclc=67361308}}</ref>{{rp|3}} is a system-design platform and development environment for a [[visual programming language]] from [[National Instruments]].\n\nThe graphical language is named \"G\"; not to be confused with [[G-code]]. Originally released for the Apple [[Macintosh]] in 1986, LabVIEW is commonly used for [[data acquisition]], [[instrument control]], and industrial [[automation]] on a variety of [[operating system]]s (OSs), including [[Microsoft Windows]], various versions of [[Unix]], [[Linux]], and [[macOS]].\n\nThe latest versions of LabVIEW are LabVIEW 2019 and LabVIEW NXG 3.1, released in May 2019.<ref>{{cite web|url=http://www.ni.com/pdf/manuals/376808f.html|title=LabVIEW NXG: Version 3.1 Readme|last1=|first1=|date=|website=Manuals|publisher=National Instruments|archive-url=|archive-date=|dead-url=|accessdate=|df=}}</ref>\n\n== Dataflow programming ==\nThe programming paradigm used in LabVIEW, sometimes called G, is based on data availability. If there is enough data available to a subVI or function, that subVI or function will execute. Execution flow is determined by the structure of a graphical block diagram (the LabVIEW-source code) on which the programmer connects different function-nodes by drawing wires. These wires propagate variables and any node can execute as soon as all its input data become available. Since this might be the case for multiple nodes simultaneously, LabVIEW can execute inherently in parallel.<ref name=bress2013>{{cite book|last1=Bress|first1=Thomas J.|title=Effective LabVIEW Programming|date=2013|publisher=NTS Press|location=[S.l.]|isbn=1-934891-08-8}}</ref>{{rp|1–2}} [[Multi-processing]] and [[Thread (computer science)|multi-threading]] hardware is exploited automatically by the built-in scheduler, which [[multiplexing|multiplexes]] multiple OS threads over the nodes ready for execution.\n\n== Graphical programming ==\n\nLabVIEW integrates the creation of user interfaces (termed front panels) into the development cycle. LabVIEW programs-subroutines are termed virtual instruments (VIs). Each VI has three components: a block diagram, a front panel, and a connector pane. The last is used to represent the VI in the block diagrams of other, calling VIs. The front panel is built using controls and indicators. Controls are inputs: they allow a user to supply information to the VI. Indicators are outputs: they indicate, or display, the results based on the inputs given to the VI. The back panel, which is a block diagram, contains the graphical source code. All of the objects placed on the front panel will appear on the back panel as terminals. The back panel also contains structures and functions which perform operations on controls and supply data to indicators. The structures and functions are found on the Functions palette and can be placed on the back panel. Collectively controls, indicators, structures, and functions are referred to as nodes. Nodes are connected to one another using wires, e.g., two controls and an indicator can be wired to the addition function so that the indicator displays the sum of the two controls. Thus a virtual instrument can be run as either a program, with the front panel serving as a user interface, or, when dropped as a node onto the block diagram, the front panel defines the inputs and outputs for the  node through the connector pane. This implies each VI can be easily tested before being embedded as a subroutine into a larger program.\n\nThe graphical approach also allows nonprogrammers to build programs by dragging and dropping virtual representations of lab equipment with which they are already familiar. The LabVIEW programming environment, with the included examples and documentation, makes it simple to create small applications. This is a benefit on one side, but there is also a certain danger of underestimating the expertise needed for high-quality G programming. For complex algorithms or large-scale code, it is important that a programmer possess an extensive knowledge of the special LabVIEW syntax and the topology of its memory management. The most advanced LabVIEW development systems offer the ability to build stand-alone applications. Furthermore, it is possible to create distributed applications, which communicate by a [[client–server model]], and are thus easier to implement due to the inherently parallel nature of G.\n\n=== Widely-accepted design patterns ===\nApplications in LabVIEW are usually designed using well-known architectures, known as [[design pattern]]s. The most common design patterns for graphical LabVIEW applications are listed in the table below.\n{| class=\"wikitable\"\n|+Common design patterns for LabVIEW applications\n!Design pattern\n!Purpose\n!Implementation details\n!Use cases\n!Limitations\n|-\n|Functional Global Variable\n|Exchange information without using global variables\n|A shift register of a [[while loop]] is used to store the data and the while loop runs only one iteration in a \"non-reentrant\" VI\n|\n* Exchange information with less wiring\n|\n* All owning VIs are kept in memory\n|-\n|[[State machine (LabVIEW programming)|State machine]]<ref>{{cite web |url=http://www.ni.com/white-paper/3024/en/ |title=Application Design Patterns: State Machines |author=<!--Not stated--> |date=8 September 2011 |website=National Instruments whitepapers |publisher= |access-date=21 September 2017 |quote= |deadurl=no |archiveurl=https://web.archive.org/web/20170922002635/http://www.ni.com/white-paper/3024/en/ |archivedate=22 September 2017 |df= }}</ref>\n|Controlled execution that depends on past events\n|[[Switch statement|Case structure]] inside a while loop pass an [[Enumerated type|enumerated variable]] to a shift register, representing the next state; complex state machines can be designed using the Statechart module\n|\n* User interfaces\n* Complex logic\n* Communication protocols\n|\n* All possible states must be known in advance\n|-\n|Event-driven user interface\n|Lossless processing of user actions\n|GUI events are captured by an event structure queue, inside a while loop; the while loop is suspended by the event structure and resumes only when the desired events are captured\n|\n* Graphical user interface\n|\n* Only one event structure in a loop\n|-\n|Master-slave<ref>{{cite web |url=http://www.ni.com/white-paper/3022/en/ |title=Application Design Patterns: Master/Slave |author=<!--Not stated--> |date=7 October 2015 |website=National Instruments whitepapers |publisher= |access-date=21 September 2017 |quote= |deadurl=no |archiveurl=https://web.archive.org/web/20170922002540/http://www.ni.com/white-paper/3022/en/ |archivedate=22 September 2017 |df= }}</ref>\n|Run independent processes simultaneously\n|Several parallel while loops, out of which one functions as the \"master\", controlling the \"slave\" loops\n|\n* Simple GUI for data acquisition and visualization\n|\n* Attention to and prevention of [[race condition]]s is required\n|-\n|Producer-consumer<ref>{{cite web |url=http://www.ni.com/white-paper/3023/en/ |title=Application Design Patterns: Producer/Consumer |author=<!--Not stated--> |date=24 August 2016 |website=National Instruments whitepapers |publisher= |access-date=21 September 2017 |quote= |deadurl=no |archiveurl=https://web.archive.org/web/20170922002635/http://www.ni.com/white-paper/3023/en/ |archivedate=22 September 2017 |df= }}</ref>\n|Asynchronous of multithreaded execution of loops\n|A master loop controls the execution of two slave loops, that communicate using notifiers, queues and semaphores; data-independent loops are automatically executed in separate threads\n|\n* Data sampling and visualization\n|\n* Order of execution is not obvious to control\n|-\n|Queued state machine with event-driven producer-consumer\n|Highly responsive user-interface for multithreaded applications\n|An event-driven user interface is placed inside the producer loop and a state machine is placed inside the consumer loop, communicating using queues between themselves and other parallel VIs\n|\n* Complex applications\n|\n|}\n\n==  Benefits ==\n\n=== Interfacing to devices ===\nLabVIEW includes extensive support for interfacing to devices, instruments, camera, and other devices. Users interface to hardware by either writing direct bus commands (USB, GPIB, Serial) or using high-level, device-specific, drivers that provide native LabVIEW function nodes for controlling the device.\n\nLabVIEW includes built-in support for NI hardware platforms such as [[CompactDAQ]] and [[CompactRIO]], with a large number of device-specific blocks for such hardware, the ''Measurement and Automation eXplorer'' (MAX) and ''Virtual Instrument Software Architecture'' (VISA) toolsets.\n\nNational Instruments makes thousands of device drivers available for download on the NI Instrument Driver Network (IDNet).<ref>{{cite web|url=http://www.ni.com/downloads/instrument-drivers/|title=3rd Party Instrument Drivers - National Instruments|author=|date=|website=www.ni.com|deadurl=no|archiveurl=https://web.archive.org/web/20141128134304/http://www.ni.com/downloads/instrument-drivers/|archivedate=2014-11-28|df=}}</ref>\n\n=== Code compiling ===\nLabVIEW includes a [[compiler]] that produces native code for the CPU  platform. This aids performance. The graphical code is translated into executable machine code by a compiler. The LabVIEW syntax is strictly enforced during the editing process and compiled into the executable machine code when requested to run or upon saving. In the latter case, the executable and the source code are merged into a single file. The executable runs with the help of the LabVIEW [[Run-time system|run-time]] engine, which contains some pre-compiled code to perform common tasks that are defined by the G language. The run-time engine reduces compiling time and provides a consistent interface to various operating systems, graphic systems, hardware components, etc. The run-time environment makes the code portable across platforms. Generally, LabVIEW code can be slower than equivalent compiled C code, although the differences often lie more with program optimization than inherent execution speed.{{Citation needed|date=June 2013}}\n\n=== Large libraries ===\nMany [[Library (computing)|libraries]] with a large number of functions for data acquisition, signal generation, mathematics, statistics, signal conditioning, analysis, etc., along with numerous for functions such as integration, filters, and other specialized abilities usually associated with data capture from hardware sensors is enormous. In addition, LabVIEW includes a text-based programming component named MathScript with added functions for signal processing, analysis, and mathematics. MathScript can be integrated with graphical programming using ''script nodes'' and uses a syntax that is compatible generally with [[MATLAB]].<ref>{{cite web|title=LabVIEW MathScript RT Module|url=http://www.ni.com/labview/mathscript/|website=www.ni.com|deadurl=no|archiveurl=https://web.archive.org/web/20160805161725/http://www.ni.com/labview/mathscript/|archivedate=2016-08-05|df=}}</ref>\n\n=== Parallel programming ===\nLabVIEW is an inherently [[concurrent language]], so it is very easy to program multiple tasks that are performed in parallel via multithreading. For example, this is done easily by drawing two or more parallel while loops and connecting them to two separate nodes. This is a great benefit for test system automation, where it is common practice to run processes like test sequencing, data recording, and hardware interfacing in parallel.\n\n=== Ecosystem ===\nDue to the longevity and popularity of the LabVIEW language, and the ability for users to extend its functions, a large ecosystem of third party add-ons has developed via contributions from the community.  This ecosystem is available on the LabVIEW Tools Network, which is a marketplace for both free and paid LabVIEW add-ons.\n\n=== User community ===\nThere is a low-cost LabVIEW Student Edition aimed at educational institutions for learning purposes. There is also an active community of LabVIEW users who communicate through several [[electronic mailing list]]s (email groups) and [[Internet forum]]s.\n\n===Home Bundle Edition===\n[[National Instruments]] provides a low cost LabVIEW Home Bundle Edition.<ref>{{cite web|url=http://sine.ni.com/nips/cds/view/p/lang/en/nid/213095|title=LabVIEW Home Bundle for Windows - National Instruments|author=|date=|website=sine.ni.com|deadurl=no|archiveurl=https://web.archive.org/web/20160704163634/http://sine.ni.com/nips/cds/view/p/lang/en/nid/213095|archivedate=2016-07-04|df=}}</ref>\n\n== Criticism ==\nLabVIEW is a [[proprietary software|proprietary]] product of [[National Instruments]]. Unlike common programming languages such as [[C Programming Language|C]] or [[Fortran]], LabVIEW is not managed or specified by a third party standards committee such as [[American National Standards Institute]] (ANSI), [[Institute of Electrical and Electronics Engineers]] (IEEE), [[International Organization for Standardization]] (ISO), etc. Many users have criticised it for its tendency to freeze or crash during simple tasks, often requiring the software to be shut down and restarted.{{Citation needed|date=July 2014}}\n\n=== Slow ===\n\nVery small applications still have to start the run-time environment which is a large and slow task.  This tends to restrict LabVIEW to monolithic applications. Examples of this might be tiny programs\nto grab a single value from some hardware that can be used in a scripting language - the overhead of\nthe run-time environment render this approach impractical with LabVIEW.{{Citation needed|date=July 2014}}\n\n=== Non-textual ===\nG language being non-textual, software tools such as versioning, side-by-side (or diff) comparison, and version code change tracking cannot be applied in the same manner as for textual programming languages. There are some additional tools to make comparison and merging of code with source code control (versioning) tools such as subversion, CVS and Perforce.\n<ref>{{cite web |url=http://thinkinging.com/2007/06/17/top-5-bad-excuses-for-not-using-source-code-control/ |title=Archived copy |accessdate=2016-10-28 |deadurl=no |archiveurl=https://web.archive.org/web/20161028215453/http://thinkinging.com/2007/06/17/top-5-bad-excuses-for-not-using-source-code-control/ |archivedate=2016-10-28 |df= }}</ref><ref>{{cite web|url=http://www.ni.com/white-paper/4114/en/|title=Software Configuration Management and LabVIEW - National Instruments|author=|date=|website=www.ni.com|deadurl=no|archiveurl=https://web.archive.org/web/20161029043345/http://www.ni.com/white-paper/4114/en/|archivedate=2016-10-29|df=}}</ref><ref>{{cite web|url=http://www.ni.com/tutorial/14304/en/|title=Configuring LabVIEW Source Code Control (SCC) for use with Team Foundation Server (TFS) - National Instruments|author=|date=|website=www.ni.com|deadurl=no|archiveurl=https://web.archive.org/web/20161028215645/http://www.ni.com/tutorial/14304/en/|archivedate=2016-10-28|df=}}</ref>\n\n=== No zoom function ===\nThere was no ability to zoom in to (or enlarge) a VI which will be hard to see on a large, high-resolution monitor.\n\n== Release history ==\nIn 2005, starting with LabVIEW 8.0, major versions are released around the first week of August, to coincide with the annual National Instruments conference NI Week, and followed by a bug-fix release the following February.\n\nIn 2009, National Instruments began naming releases after the year in which they are released. A bug-fix is termed a Service Pack, for example, the 2009 service pack 1 was released in February 2010.\n\nIn 2017, National Instruments moved the annual conference to May and released LabVIEW 2017 alongside a completely redesigned LabVIEW NXG 1.0 built on Windows Presentation Foundation (WPF).\n{| class=\"wikitable\" style=\"font-size: 90%; text-align: left; \"\n|-\n! Name-version !! Build number !! Date\n|-\n| LabVIEW project begins\n| \n| April 1983\n|-\n| LabVIEW 1.0 (for Macintosh)\n| ??\n| October 1986\n|-\n| LabVIEW 2.0\n| ??\n| January 1990\n|-\n| LabVIEW 2.5 (first release for Sun & Windows)\n| ??\n| August 1992\n|-\n| LabVIEW 3.0 (Multiplatform)\n| ??\n| July 1993\n|-\n| LabVIEW 3.0.1 (first release for Windows NT)\n| ??\n| 1994\n|-\n| LabVIEW 3.1\n| ??\n| 1994\n|-\n| LabVIEW 3.1.1 (first release with \"application builder\" ability)\n| ??\n| 1995\n|-\n| LabVIEW 4.0\n| ??\n| April 1996\n|-\n| LabVIEW 4.1\n| ??\n| 1997\n|-\n| LabVIEW 5.0\n| ??\n| February 1998\n|-\n| LabVIEW RT (Real Time)\n| ??\n| May 1999\n|-\n| LabVIEW 6.0 (6i)\n| 6.0.0.4005\n| 26 July 2000\n|-\n| LabVIEW 6.1\n| 6.1.0.4004\n| 12 April 2001\n|-\n| LabVIEW 7.0 (Express)\n| 7.0.0.4000\n| April 2003\n|-\n| LabVIEW PDA module first released\n| ??\n| May 2003\n|-\n| LabVIEW FPGA module first released\n| ??\n| June 2003\n|-\n| LabVIEW 7.1\n| 7.1.0.4000\n| 2004\n|-\n| LabVIEW Embedded module first released\n| ??\n| May 2005\n|-\n| LabVIEW 8.0 \n| 8.0.0.4005\n| September 2005\n|-\n| LabVIEW 8.20 (native Object Oriented Programming)\n| ??\n| August 2006\n|-\n| LabVIEW 8.2.1\n| 8.2.1.4002\n| 21 February 2007\n|-\n| LabVIEW 8.5\n| 8.5.0.4002\n| 2007\n|-\n| LabVIEW 8.6\n| 8.6.0.4001\n| 24 July 2008\n|-\n| LabVIEW 8.6.1\n| 8.6.0.4001\n| 10 December 2008\n|-\n| LabVIEW 2009 (32 and 64-bit)\n| 9.0.0.4022\n| 4 August 2009\n|-\n| LabVIEW 2009 SP1\n| 9.0.1.4011\n| 8 January 2010\n|-\n| LabVIEW 2010\n| 10.0.0.4032\n| 4 August 2010\n|-\n| LabVIEW 2010 f2\n| 10.0.0.4033\n| 16 September 2010\n|-\n| LabVIEW 2010 SP1\n| 10.0.1.4004\n| 17 May 2011\n|-\n| LabVIEW for LEGO MINDSTORMS (2010 SP1 with some modules)\n| \n| August 2011\n|-\n| LabVIEW 2011\n| 11.0.0.4029\n| 22 June 2011\n|-\n| LabVIEW 2011 SP1\n| 11.0.1.4015\n| 1 March 2012\n|-\n| LabVIEW 2012\n| 12.0.0.4029\n| August 2012\n|-\n| LabVIEW 2012 SP1\n| 12.0.1.4013\n| December 2012\n|-\n| LabVIEW 2013\n| 13.0.0.4047\n| August 2013\n|-\n| LabVIEW 2013 SP1\n| 13.0.1.4017\n| March 2014<ref>{{cite web|url=http://www.ni.com/white-paper/5920/en/|title=What's New in NI Developer Suite - National Instruments|author=|date=|website=www.ni.com|deadurl=no|archiveurl=https://web.archive.org/web/20140331131311/http://www.ni.com/white-paper/5920/en/|archivedate=2014-03-31|df=}}</ref>\n|-\n| LabVIEW 2014\n| 14.0\n| August 2014\n|-\n| LabVIEW 2014 SP1\n| 14.0.1.4008\n| March 2015\n|-\n|LabVIEW 2015\n|15.0f2\n|August 2015\n|-\n|LabVIEW 2015 SP1\n|15.0.1f1\n|March 2016\n|-\n|LabVIEW 2016\n|16.0.0\n|August 2016\n|-\n|LabVIEW 2017\n|17.0f1\n|May 2017\n|-\n|LabVIEW 2017 SP1\n|17.0.1f1\n|Jan 2018 <ref>{{Cite web|url=http://www.ni.com/product-documentation/54421/en/|title=LabVIEW 2017 SP1 Patch Details - National Instruments|website=www.ni.com|access-date=2018-05-28}}</ref>\n|-\n|LabVIEW 2018\n|18.0\n|May 2018\n|-\n|LabVIEW 2018 SP1\n|18.0.1\n|Dec 2018\n|-\n|LabVIEW 2019\n|19.0\n|May 2019\n|}\n\n==Repositories and libraries==\n[[OpenG]], as well as LAVA Code Repository (LAVAcr), serve as repositories for a wide range of Open Source LabVIEW applications and [[Library (computing)|libraries]]. [[SourceForge]] has LabVIEW listed as one of the possible languages in which code can be written.\n\nVI Package Manager has become the standard [[package manager]] for LabVIEW libraries. It is very similar in purpose to Ruby's [[RubyGems]] and Perl's [[CPAN]], although it provides a graphical user interface similar to the [[Synaptic Package Manager]]. VI Package Manager provides access to a repository of the OpenG (and other) libraries for LabVIEW.\n\nTools exist to convert [[MathML]] into G code.<ref>{{cite web|url=https://decibel.ni.com/content/docs/DOC-13859|title=Math Node - A new way to do math in LabVIEW|author=|date=25 October 2010|website=ni.com|deadurl=no|archiveurl=https://web.archive.org/web/20110225172619/http://decibel.ni.com/content/docs/DOC-13859|archivedate=25 February 2011|df=}}</ref>\n\n==Related software==\nNational Instruments also offers a product named [[Measurement Studio]], which offers many of the test, measurement, and control abilities of LabVIEW, as a set of classes for use with [[Microsoft]] [[Visual Studio]]. This allows developers to harness some of LabVIEW's strengths within the text-based [[.NET Framework]]. National Instruments also offers [[LabWindows/CVI]] as an alternative for ANSI C programmers.\n\nWhen applications need sequencing, users often use LabVIEW with TestStand test management software, also from National Instruments.\n\nThe [[Ch interpreter]] is a [[C (programming language)|C]]/[[C++]] interpreter that can be embedded in LabVIEW for scripting.<ref name=\"chlabview\">{{cite web|url=http://iel.ucdavis.edu/projects/chlabview/|title=Embedding a C/C++ Interpreter Ch into LabVIEW for Scripting|author=|date=|website=iel.ucdavis.edu|deadurl=no|archiveurl=https://web.archive.org/web/20110515065700/http://iel.ucdavis.edu/projects/chlabview/|archivedate=2011-05-15|df=}}</ref>\n\nThe TRIL Centre Ireland BioMobius platform and DSP Robotics' FlowStone DSP also use a form of graphical programming similar to LabVIEW, but are limited to the biomedical and robotics industries respectively.\n\nLabVIEW has a direct node with [[modeFRONTIER]], a multidisciplinary and multi-objective optimization and design environment, written to allow coupling to almost any [[computer-aided engineering]] tool. Both can be part of the same process workflow description and can be virtually driven by the optimization technologies available in modeFRONTIER.\n\n== See also ==\n* [[20-sim]]\n* [[Comparison of numerical analysis software]]\n* [[Dataflow programming]]\n* [[DRAKON]]\n* [[Fourth-generation programming language]]\n* [[Graphical programming]]\n* [[Graphical system design]]\n* [[LabWindows/CVI]]\n* [[Lego Mindstorms NXT]], whose programming environment, NXT-G is based on LabVIEW, and can be programmed within LabVIEW.\n* [[MATLAB]]/[[Simulink]]\n* [[Virtual instrumentation]]\n* [[CompactDAQ]]\n* [[CompactRIO]]\n\n== References ==\n{{Reflist}}\n\n== Further reading ==\n* {{cite book|last1=Bress|first1=Thomas J.|title=Effective LabVIEW Programming|date=2013|publisher=NTS Press|location=[S.l.]|isbn=1-934891-08-8}}\n* {{cite book|last1=Blume|first1=Peter A.|title=The LabVIEW Style Book|date=2007|publisher=Prentice Hall|location=Upper Saddle River, NJ|isbn=0-13-145835-3}}\n* {{cite book|last1=Travis|first1=Jeffrey|last2=Kring|first2=Jim|title=LabVIEW for Everyone : Graphical Programming Made Easy and Fun.|date=2006|publisher=Prentice Hall|location=Upper Saddle River, NJ|isbn=0-13-185672-3|edition=3rd}}\n* {{cite book|last1=Conway|first1=Jon|last2=Watts|first2=Steve|title=A Software Engineering Approach to LabVIEW|date=2003|publisher=Prentice Hall PTR|location=Upper Saddle River, NJ|isbn=0-13-009365-3}}\n* {{cite book|last1=Olansen|first1=Jon B.|last2=Rosow|first2=Eric|title=Virtual Bio-Instrumentation : Biomedical, Clinical, and Healthcare Applications in LabVIEW|date=2002|publisher=Prentice Hall PTR|location=Upper Saddle River, NJ|isbn=0-13-065216-4}}\n* {{cite book|last1=Beyon|first1=Jeffrey Y.|title=LabVIEW Programming, Data Acquisition and Analysis|date=2001|publisher=Prentice Hall PTR|location=Upper Saddle River, NJ|isbn=0-13-030367-4}}\n* {{cite book|last1=Travis|first1=Jeffrey|title=Internet Applications In LabVIEW|date=2000|publisher=Prentice Hall PTR|location=Upper Saddle River, NJ|isbn=0-13-014144-5}}\n* {{cite book|last1=Essick|first1=John|title=Advanced LabVIEW Labs|date=1999|publisher=Prentice Hall|location=Upper Saddle River, NJ|isbn=0-13-833949-X}}\n\n=== Articles on specific uses ===\n*{{cite journal\n  | last = Desnica V, Schreiner M\n  | first =\n  | authorlink =\n  | coauthors =\n  | title = A LabVIEW-controlled portable x-ray fluorescence spectrometer for the analysis of art objects\n  | journal = X-Ray Spectrometry\n  | volume = 35\n  | issue = 5\n  | pages = 280–286\n  | publisher =\n  |date=October 2006\n  | url = http://www3.interscience.wiley.com/cgi-bin/abstract/112748693/ABSTRACT\n  | archive-url = https://archive.today/20100818064910/http://www3.interscience.wiley.com/cgi-bin/abstract/112748693/ABSTRACT\n  | dead-url = yes\n  | archive-date = 2010-08-18\n  | doi = 10.1002/xrs.906\n  | id =\n  | accessdate =\n  | first1 = Vladan\n  | last2 = Schreiner\n  | first2 = Manfred}}\n*{{cite journal\n  | last = Keleshis C, Ionita C, Rudin S\n  | first =\n  | authorlink =\n  | coauthors =\n  | title = Labview <nowiki>[sic]</nowiki> graphical user interface for micro angio-fluoroscopic high resolution detector\n  | journal = Medical Physics\n  | volume = 33\n  | issue = 6\n  | pages = 2007\n  | publisher =\n  |date=June 2006\n  | url = http://scitation.aip.org/getabs/servlet/GetabsServlet?prog=normal&id=MPHYA6000033000006002007000001&idtype=cvips&gifs=Yes\n  | doi = 10.1118/1.2240285\n  | id =\n  | accessdate =\n  | first1 = C.\n  | last2 = Ionita\n  | first2 = C.\n  | last3 = Rudin\n  | first3 = S.}}\n*{{cite journal\n  | last = Fedak W., Bord D., Smith C., Gawrych D., Lindeman K.\n  | first =\n  | authorlink =\n  | coauthors =\n  | title = Automation of the Franck-Hertz experiment and the Tel-X-Ometer x-ray machine using LABVIEW\n  | journal = American Journal of Physics\n  | volume = 71\n  | issue = 5\n  | pages = 501–506\n  | publisher = AAPT\n  |date=May 2003\n  | url = http://scitation.aip.org/getabs/servlet/GetabsServlet?prog=normal&id=AJPIAS000071000005000501000001&idtype=cvips&gifs=Yes\n  | doi = 10.1119/1.1527949\n  | id =\n  | accessdate =\n  | first1 = W.\n  | last2 = Bord\n  | first2 = D.\n  | last3 = Smith\n  | first3 = C.\n  | last4 = Gawrych\n  | first4 = D.\n  | last5 = Lindeman\n  | first5 = K.}}\n\n===Articles on education uses===\n*{{cite journal\n  | last = Belletti A., Borromei R., Ingletto G.\n  | first =\n  | authorlink =\n  | coauthors =\n  | last2 = Borromei\n  | first2 = R.\n  | last3 = Ingletto\n  | first3 = G.| title = Teaching physical chemistry experiments with a computer simulation by LabVIEW\n  | journal = Journal of Chemical Education\n  | volume = 83\n  | issue = 9\n  | pages = 1353–1355\n  | publisher = ACS\n  |date=September 2006\n  | url = http://jchemed.chem.wisc.edu/Journal/Issues/2006/Sep/abs1353.html\n  | doi = 10.1021/ed083p1353\n  | id =\n  | accessdate =\n  | first1 = A.}}\n*{{cite journal\n  | last = Moriarty P.J., Gallagher B.L., Mellor C.J., Baines R.R.\n  | first =\n  | authorlink =\n  | coauthors =\n  | title = Graphical computing in the undergraduate laboratory: Teaching and interfacing with LabVIEW\n  | journal = American Journal of Physics\n  | volume = 71\n  | issue = 10\n  | pages = 1062–1074\n  | publisher = AAPT\n  |date=October 2003\n  | url = http://scitation.aip.org/getabs/servlet/GetabsServlet?prog=normal&id=AJPIAS000071000010001062000001&idtype=cvips&gifs=Yes\n  | doi = 10.1119/1.1582189\n  | id =\n  | accessdate =\n  | first1 = P. J.\n  | last2 = Gallagher\n  | first2 = B. L.\n  | last3 = Mellor\n  | first3 = C. J.\n  | last4 = Baines\n  | first4 = R. R.}}\n*{{cite journal\n  | last = Lauterburg\n  | first = Urs\n  | authorlink =\n  | coauthors =\n  | title = LabVIEW in Physics Education\n  | journal = A white paper about using LabVIEW in physics demonstration and laboratory experiments and simulations.\n  | volume =\n  | issue =\n  | pages =\n  | publisher =\n  |date=June 2001\n  | url = http://www.lauterburg.ch/UrsLPublications/LV-PhysicsWPPrint.pdf\n  | doi =\n  | id =\n  | accessdate =  |format=PDF}}\n*{{cite journal\n  | last = Drew SM\n  | first =\n  | authorlink =\n  | coauthors =\n  | title = Integration of National Instruments' LabVIEW software into the chemistry curriculum\n  | journal = Journal of Chemical Education\n  | volume = 73\n  | issue = 12\n  | pages = 1107–1111\n  | publisher = ACS\n  |date=December 1996\n  | url = http://jchemed.chem.wisc.edu/Journal/Issues/1996/Dec/abs1107.html\n  | doi = 10.1021/ed073p1107\n  | id =\n  | accessdate =\n  | first1 = Steven M.}}\n*{{cite journal\n  | last = Muyskens MA, Glass SV, Wietsma TW, Gray TM\n  | first =\n  | authorlink =\n  | coauthors =\n  | last2 = Glass\n  | first2 = Samuel V.\n  | last3 = Wietsma\n  | first3 = Thomas W.\n  | last4 = Gray\n  | first4 = Terry M.| title = Data acquisition in the chemistry laboratory using LabVIEW software\n  | journal = Journal of Chemical Education\n  | volume = 73\n  | issue = 12\n  | pages = 1112–1114\n  | publisher = ACS\n  |date=December 1996\n  | url = http://jchemed.chem.wisc.edu/Journal/Issues/1996/Dec/abs1112.html\n  | doi = 10.1021/ed073p1112\n  | id =\n  | accessdate =\n  | first1 = Mark A.}}\n*{{cite journal\n  | last = Ogren PJ, Jones TP\n  | first =\n  | authorlink =\n  | coauthors =\n  | last2 = Jones\n  | first2 = Thomas P.| title = Laboratory interfacing using the LabVIEW software package\n  | journal = Journal of Chemical Education\n  | volume = 73\n  | issue = 12\n  | pages = 1115–1116\n  | publisher = ACS\n  |date=December 1996\n  | url = http://jchemed.chem.wisc.edu/Journal/Issues/1996/Dec/abs1115.html\n  | doi = 10.1021/ed073p1115\n  | id =\n  | accessdate =\n  | first1 = Paul J.}}\n*{{cite journal\n  | last = Trevelyan\n  | first = J.P.\n  | authorlink =\n  | coauthors =\n  | title = 10 Years Experience with Remote Laboratories\n  | journal = International Conference on Engineering Education Research\n  | volume =\n  | issue =\n  | pages =\n  | publisher = ACS\n  |date=June 2004\n  | url = http://telerobot.mech.uwa.edu.au/Information/Trevelyan-INEER-2004.pdf\n  |format=PDF| doi =\n  | id =\n  | accessdate = }}\n\n==External links ==\n* {{Official website|www.ni.com/labview}}, National Instruments\n\n{{Prone to spam|date=November 2018}}\n{{Z148}}<!--     {{No more links}}\n\n       Please be cautious adding more external links.\n\nWikipedia is not a collection of links and should not be used for advertising.\n\n     Excessive or inappropriate links will be removed.\n\n See [[Wikipedia:External links]] and [[Wikipedia:Spam]] for details.\n\nIf there are already suitable links, propose additions or replacements on\nthe article's talk page.\n\n-->\n\n\n{{National Instruments}}\n{{Numerical analysis software}}\n\n{{DEFAULTSORT:Labview}}\n[[Category:Numerical software]]\n[[Category:Visual programming languages]]\n[[Category:Numerical programming languages]]\n[[Category:Numerical analysis software for Linux]]\n[[Category:Numerical analysis software for MacOS]]\n[[Category:Numerical analysis software for Windows]]\n[[Category:Cross-platform software]]\n[[Category:Pedagogic integrated development environments]]\n[[Category:Synchronous programming languages]]\n[[Category:Software modeling language]]"
    },
    {
      "title": "LabWindows/CVI",
      "url": "https://en.wikipedia.org/wiki/LabWindows%2FCVI",
      "text": "{{Other uses|CVI (disambiguation)}}\n{{Infobox Software\n| name = LabWindows/CVI\n| logo = \n| screenshot = \n| caption = \n| developer = [[National Instruments]]\n| released = {{Start date and age|1989|01}}\n| latest_release_version = 2019\n| latest_release_date = {{Start date and age|2019|05}}\n| operating_system = Windows 10/Windows 8/Windows 7/Vista/XP with Linux run-time support and Pharlap real-time run-time support\n| genre = [[Data acquisition]], [[instrument control]], [[test automation]], [[Signal processing|analysis and signal processing]]\n| website = {{URL|ni.com/cvi}}\n}}\n'''LabWindows/CVI''' (CVI is short for [[C (programming language)|C]] for [[Virtual instrumentation|Virtual Instrumentation]]) is an [[ANSI]] C programming environment for test and measurement developed by [[National Instruments]]. The program was originally released as LabWindows for [[DOS]] in 1987, but was soon revisioned (and renamed) for the [[Microsoft Windows]] platform. The current version of LabWindows/CVI (commonly referred to as CVI) is 2019.\n\nLabWindows/CVI uses the same libraries and data-acquisition modules as the better known National Instrument product [[LabVIEW]] and is thus highly compatible with it.\n\nLabVIEW is targeted more at domain experts and scientists, and CVI more towards software engineers that are more comfortable with text-based linear languages such as [[C (programming language)|C]].\n\n== Release history ==\nStarting with LabVIEW 8.0, major versions are released around the first week of August, to coincide with the annual National Instruments conference NI Week, and followed by a bug-fix release the following February.\n\nIn 2009 National Instruments started to name the releases after the year in which they are released. The bug-fix is called a Service Pack (for instance, the 2009 service pack 1 is released in February 2010). <ref>[http://www.ni.com/product-documentation/6609/en/ LabWindows™/CVI™ Release Information]</ref>\n\n{| class=\"wikitable\" style=\"font-size: 90%; text-align: left; \"\n|-\n! Name/version !! Build number !! Date\n|-\n| LabWindows/CVI project begins\n| \n| 1987\n|-\n| LabWindows/CVI 1.0 (for DOS)\n| \n| Jan 1989\n|-\n| LabWindows/CVI 2.0 (for DOS; GUI Tools and Memory Extender)\n|  \n| Apr 1991\n|-\n| LabWindows/CVI 3.0 (for DOS/Windows 3.1/Solaris)\n| \n| Mar 1994\n|-\n| LabWindows/CVI 3.1 (generate codes automatically)\n| \n| Jul 1995\n|-\n| LabVIEW 3.1.1 (first release with \"application builder\" capability)\n| ??\n| 1995\n|-\n| LabWindows/CVI 4.0 (External C/C++ compiler compatibility)\n| \n| May 1996\n|-\n| LabWindows/CVI 4.0.1\n|  \n| Aug 1996 \n|-\n| LabWindows/CVI 5.0 (support VXI and IVI)\n| \n| Feb 1998\n|-\n| LabWindows/CVI 5.5 (Multithreaded libraries, debugging)\n| \n| Feb 2000\n|-\n| LabWindows/CVI 6.0 (support ActiveX, improved presentation)\n|  \n| Oct 2001\n|-\n| LabWindows/CVI 7.0 (use Workspace) \n|  \n| Jul 2003 \n|-\n| LabWindows/CVI 7.1 (completion automatically)\n|  \n| Sep 2004 \n|-\n| LabWindows/CVI 8.0 (support .NET assemblies)\n|  \n| Oct 2005\n|-\n| LabWindows/CVI 8.0.1\n|  \n|  \n|-\n| LabWindows/CVI 8.1\n|  \n| 2006 \n|-\n| LabWindows/CVI 8.1.1\n|  \n|  \n|-\n| LabWindows/CVI 8.5\n|  \n| 2007 \n|-\n| LabWindows/CVI 8.5.1\n|  \n|  \n|-\n| LabWindows/CVI 9.0 (support ANSI C99)\n|  \n| 2008 \n|-\n| LabWindows/CVI 9.0.1\n|  \n| \n|-\n| LabWindows/CVI 2009 (create 64-bit applications)\n| 9.1 \n| 2009  \n|-\n| LabWindows/CVI 2009 SP1\n| \n|  \n|-\n| LabWindows/CVI 2010\n| 10.0 \n| 2010 \n|-\n| LabWindows/CVI 2010 SP1\n|  \n|  \n|-\n| LabWindows/CVI 2012\n| 12.0 \n| 2012 \n|-\n| LabWindows/CVI 2012 SP1\n|  \n|  \n|-\n| LabWindows/CVI 2013 \n| 13.0\n| 2013 \n|-\n| LabWindows/CVI 2013 SP1\n|  \n|  \n|-\n| LabWindows/CVI 2013 SP2\n| \n|  \n|-\n| LabWindows/CVI 2015 (upgrade to Clang 3.3)\n| 15.0\n| 2015\n|-\n| LabWindows/CVI 2015 SP1\n| 15.1\n| 2016\n|-\n| LabWindows/CVI 2017 (tracepoints, word/semantic highlighting, thread-specific breakpoints, comment/uncomment)\n| 17.0\n| 2017\n|-\n| LabWindows/CVI 2019\n| 19.0\n| May 2019\n|}\n\n==See also==\n*[[National Instruments]]\n\n==References==\n{{reflist}}\n\n{{National Instruments}}\n\n{{DEFAULTSORT:LabWindows CVI}}\n[[Category:Integrated development environments]]\n[[Category:Domain-specific programming languages]]\n[[Category:C compilers]]\n[[Category:Data analysis software]]\n[[Category:Numerical software]]\n[[Category:Cross-platform software]]"
    },
    {
      "title": "LEMON (C++ library)",
      "url": "https://en.wikipedia.org/wiki/LEMON_%28C%2B%2B_library%29",
      "text": "{{third-party|date=February 2013}}\n\n{{Infobox Software\n | logo = <!-- [[File:lemon-logo.png|The LEMON logo.]] -->\n | released = {{start date|2004|9|30}}\n | latest release version = 1.3.1\n | latest release date = {{start date|2014|7|7}}\n | programming language = C++\n | operating system = [[Cross-platform]]\n | platform = [[GNU Compiler Collection|gcc]], [[Intel C++ Compiler|icc]], [[Microsoft Visual Studio|Visual Studio]], [[XL C/C++|xlC]]\n | genre = [[Graph (discrete mathematics)|Graph]] and [[Network Optimization]] [[Library (computing)|Library]]\n | license = [[Free software]] ([[Boost Software License|Boost license]])\n | website = [http://lemon.cs.elte.hu http://lemon.cs.elte.hu]\n}}\n\n'''LEMON''' is an [[Open-source software|open source]] [[Graph (discrete mathematics)|graph]] [[Library (computing)|library]] written in the [[C++]] language providing implementations of common data structures and algorithms with focus on combinatorial optimization tasks connected mainly with graphs and networks. The library is part of the [[COIN-OR]] project.\n\n'''LEMON''' is an abbreviation of '''''L'''ibrary for '''E'''fficient '''M'''odeling and '''O'''ptimization in '''N'''etworks''.\n\n== Design ==\nLEMON employs [[genericity]] in C++ by using [[Template (programming)|templates]]. The tools of the library are designed to be versatile, convenient and highly efficient. They can be combined easily to solve complex real-life optimization problems. For example, LEMON’s graphs can differ in many ways (depending on the representation and other specialities), but all have to satisfy one or more graph concepts, which are standardized interfaces to work with the rest of the library.\n\n== Features ==\nLEMON provides\n* [[Graph (discrete mathematics)|Graph structures]] and related tools\n* [[Graph search algorithm]]s\n* [[Shortest path]] algorithms\n* [[Maximum flow problem|Maximum flow]] algorithms\n* [[Minimum cost flow problem|Minimum cost flow]] algorithms\n* [[Minimum cut]] algorithms\n* [[Graph connectivity|Connectivity]] and other graph properties\n* Maximum cardinality and minimum cost [[perfect matching]] algorithms\n* [[Minimum cost spanning tree]] algorithms\n* [[Approximation algorithm]]s\n* Auxiliary algorithms\n\nLEMON also contains some [[metaheuristic]] optimization tools and provides a general high-level interface for several [[Linear program|LP]] and [[Linear_program#Integer_unknowns|MIP]] solvers, such as [[GLPK]], [[CPLEX|ILOG CPLEX]], [[COIN-OR#CLP|CLP]], [[COIN-OR#CLP|CBC]], [[SoPlex]].\n\nLEMON has its own graph storing format, the so called ''Lemon Graph Format'' and includes general [[Encapsulated PostScript|EPS]] drawing methods and special graph exporting tools.\n\nLEMON also includes several miscellaneous tools. For example, it provides simple tools for measuring the performance of algorithms, which can be used to compare different implementations of the same problem.\n\n== External links ==\nLEMON webpage:\n* [http://lemon.cs.elte.hu Lemon site]\n\n[[Category:C++ libraries]]\n[[Category:Numerical software]]\n[[Category:Software_using_the_Boost_license]]"
    },
    {
      "title": "Libfixmath",
      "url": "https://en.wikipedia.org/wiki/Libfixmath",
      "text": "{{lowercase|title=libfixmath}}\n{{Infobox software\n| name = libfixmath\n| developer = Ben Brewer (aka flatmush)\n| latest_release_version = r64\n| latest_release_date = {{Start date and age|2012|02|02}}\n| programming language   = [[C (programming language)|C99]]\n| operating_system = [[Cross-platform]]\n| genre = [[fixed-point arithmetic|fixed point math library]]\n| license = [[MIT License|MIT]]\n| website = https://github.com/PetteriAimonen/libfixmath\n| status = active\n}}\n\n'''libfixmath''' is a [[platform independent|platform-independent]] [[fixed-point arithmetic|fixed point]] maths library aimed at developers wanting to perform fast non-integer maths on platforms lacking a (or with a low performance) [[Floating Point Unit|FPU]]. It offers developers a similar interface to the standard [[math.h]] functions for use on [[Q (number format)|Q16.16]] [[fixed-point arithmetic|fixed point]] numbers.\nlibfixmath has no external dependencies other than [[stdint.h]] and a compiler which supports [[64-bit]] integer arithmetic (such as [[GNU Compiler Collection|GCC]]).<ref>{{cite web|url=https://github.com/PetteriAimonen/libfixmath|title=libfixmath Project Page}}</ref>\nConditional compilation options exist to remove the requirement for a [[64-bit]] capable [[compiler]] as many compilers for [[microcontroller]]s and [[Digital signal processor|DSPs]] do not support [[64-bit]] arithmetic.<ref>{{cite web|url=https://code.google.com/p/libfixmath/issues/detail?id=4|title=64-bit Compiler Support Issues}}</ref>\n\n==History==\n'''libfixmath''' was originally developed by Ben Brewer (aka flatmush) and first released publicly as part of the [[Dingoo SDK]].<ref>{{cite web|url=https://code.google.com/p/dingoo-sdk|title=Dingoo SDK Project Page}}</ref> It has since been used to implement a software [[3D graphics]] library called FGL.<ref>{{cite web|url=https://code.google.com/p/fgl|title=FGL Flatmush/Fixed-Point Graphics Library Project Page}}</ref>\n\n==Q16.16 Functions==\n{|class=\"wikitable\"\n|-\n! Name !! Description\n|-\n|<code>fix16_acos</code> || [[arccosine|inverse cosine]]\n|-\n|<code>fix16_asin</code> || [[arcsine|inverse sine]]\n|-\n|<code>fix16_atan</code> || [[arctangent|one-parameter inverse tangent]]\n|-\n|<code>fix16_atan2</code> || [[atan2|two-parameter inverse tangent]]\n|-\n|<code>fix16_cos</code> || [[cosine]]\n|-\n|<code>fix16_exp</code> || [[exponential function]]\n|-\n|<code>fix16_sin</code> || [[sine]]\n|-\n|<code>fix16_sqrt</code> || [[square root]]\n|-\n|<code>fix16_tan</code> || [[tangent (trigonometric function)|tangent]]\n|-\n|<code>fix16_mul</code> || [[multiplication]]\n|-\n|<code>fix16_div</code> || [[Division (mathematics)|division]]\n|-\n|<code>fix16_sadd</code> || [[Saturation arithmetic|saturated]] [[addition]]\n|-\n|<code>fix16_smul</code> || [[Saturation arithmetic|saturated]] [[multiplication]]\n|-\n|<code>fix16_sdiv</code> || [[Saturation arithmetic|saturated]] [[Division (mathematics)|division]]\n|}\n\n==Other Functions==\n{|class=\"wikitable\"\n|-\n! Name !! Description\n|-\n|<code>fix16_to_dbl</code> || Convert [[Q (number format)|Q16.16]] to a [[Double precision|double]]\n|-\n|<code>fix16_to_float</code> || Convert [[Q (number format)|Q16.16]] to a [[Single precision|float]]\n|-\n|<code>fix16_to_int</code> || Convert [[Q (number format)|Q16.16]] to an [[Integer (computer science)|integer]]\n|-\n|<code>fix16_from_dbl</code> || Convert [[Double precision|double]] to a [[Q (number format)|Q16.16]]\n|-\n|<code>fix16_from_float</code> || Convert [[Single precision|float]] to a [[Q (number format)|Q16.16]]\n|-\n|<code>fix16_from_int</code> || Convert [[Integer (computer science)|integer]] to a [[Q (number format)|Q16.16]]\n|}\n\n==Performance==\nFor the most intensive function ([[atan2]]) benchmark results show the following results:\n\n{|class=\"wikitable\"\n|-\n! Name !! Time Compared to Float\n|-\n|[[ARM Cortex-M0]] || 26.3%\n|-\n|[[Marvell Technology Group|Marvell]] [[XScale|PXA270]] ([[ARM architecture|ARM]]) @ 312&nbsp;MHz || 58.45%\n|-\n|[[List of Intel Core 2 microprocessors|Intel T5500]] || 120%\n|-\n|[[Intel atom|Intel Atom N280]] || 141%\n|}\nNote: These results were calculated using fixtest with caching optimizations turned off.<ref>{{cite web|url=https://code.google.com/p/libfixmath/downloads/detail?name=fixtest_r16.zip|title=fixtest Download Page}}</ref>\n\n==Licensing==\n'''libfixmath''' is released under the [[MIT License]], a [[permissive free software licence]], and is [[free software]].\n\n==See also==\n{{Portal|Free Software}}\n* [[Binary scaling]]\n* [[Fixed-point arithmetic]]\n* [[Floating-point arithmetic]]\n* [[Q (number format)]]\n\n==References==\n{{Reflist}}\n\n==External links==\n* [https://github.com/PetteriAimonen/libfixmath Project Page]\n* [http://groups.google.com/group/libfixmath Group Page/Mailing List]\n\n[[Category:Numerical software]]\n[[Category:C libraries]]\n[[Category:Free computer libraries]]\n[[Category:Free software programmed in C]]"
    },
    {
      "title": "List of C++ multiple precision arithmetic libraries",
      "url": "https://en.wikipedia.org/wiki/List_of_C%2B%2B_multiple_precision_arithmetic_libraries",
      "text": "{{refimprove|date=November 2013}}\n\nThe following is an incomplete list of some [[arbitrary-precision arithmetic]] [[Library (computing)|libraries]] for [[C++]].\n\n* [[GNU Multiple Precision Arithmetic Library|GMP]]<ref>[http://gmplib.org/ The GNU Multiple Precision Arithmetic Library]</ref>{{refn|group=\"nb\"|C++ support must be enabled with <code>--enable-cxx</code> during the building of GMP<ref>[http://gmplib.org/gmp-man-5.1.2.pdf GNU MP], p. 6</ref>}}\n* [[MPFR]]<ref>[http://www.mpfr.org/ The GNU MPFR Library]</ref>\n* [[MPIR (mathematics software)|MPIR]]<ref>[http://www.mpir.org/ MPIR Multiple Precision Integers and Rationals]</ref>\n* [[TTMath]]<ref>[http://www.ttmath.org/ Bignum C++ library]</ref>\n* [[Arbitrary Precision Math C++ Package]]<ref>[http://www.hvks.com/Numerical/arbitrary_precision.html Arbitrary precision package]</ref>\n* [[Class Library for Numbers]]\n* [[Number Theory Library]]\n* [[Apfloat]]<ref>[http://www.apfloat.org/ Apfloat Home Page]</ref>\n* [[C++ Big Integer Library]]<ref>[https://mattmccutchen.net/bigint/ C++ Big Integer Library]</ref>\n* [[MAPM]]<ref>[http://www.tc.umn.edu/~ringx004/mapm-main.html MAPM Home Page] {{webarchive|url=https://web.archive.org/web/20140909153659/http://www.tc.umn.edu/~ringx004/mapm-main.html |date=2014-09-09 }}</ref>\n* [[ARPREC]]<ref>[http://crd-legacy.lbl.gov/~dhbailey/mpdist/ High-Precision Software Directory]</ref>\n* [[InfInt]]<ref>[https://sercantutar.github.io/infint/ InfInt]</ref>\n\n==Footnotes==\n{{Reflist|group=nb}}\n\n==References==\n{{Reflist}}\n\n[[Category:C++ libraries]]\n[[Category:Numerical software]]"
    },
    {
      "title": "MADNESS",
      "url": "https://en.wikipedia.org/wiki/MADNESS",
      "text": "{{Infobox software\n| name                   = MADNESS\n| author                 = [[George Fann]], [[Robert J. Harrison]]\n| developer              = [[Oak Ridge National Laboratory]], [[Stony Brook University]], [[Virginia Tech]], [[Argonne National Laboratory]]\n| released               = Forthcoming\n| genre                  = Scientific simulation software\n| license                = [[GNU GPL]] v2\n| website                = {{URL|https://github.com/m-a-d-n-e-s-s/madness}}\n}}\n\n'''MADNESS''' ('''Multiresolution Adaptive Numerical Environment for Scientific Simulation''')\nis a high-level software environment for the solution of [[integral equations|integral]] and [[differential equations]] in many dimensions using adaptive and fast harmonic analysis methods with guaranteed precision based on [[multiresolution analysis]]\n<ref>{{cite journal|last1 = Beylkin|first1 = Gregory|last2=Fann|first2=George|last3 = Harrison|first3 = Robert J.|last4 = Kurcz|first4 = Christopher|last5=Monzón|first5=Lucas|title = Multiresolution representation of operators with boundary conditions on simple domains|journal = Applied and Computational Harmonic Analysis|volume = 33|issue = 1|pages = 109–139|year = 2012|doi = 10.1016/j.acha.2011.10.001}}</ref>\n<ref>{{cite journal|last1 = Fann|first1 = George|last2 = Beylkin|first2 = Gregory|last3 = Harrison|first3 = Robert J.|last4 = Jordan|first4 = Kirk E.|title = Singular operators in multiwavelet bases|journal = IBM Journal of Research and Development|volume = 48|issue = 2|pages = 161–171|year = 2004|doi = 10.1147/rd.482.0161}}</ref>\nand separated representations\n.<ref>{{cite journal|last1 = Beylkin|first1 = Gregory|last4 = Harrison|first4 = Robert J.|last2 = Cramer|first2 = Robert|last3=Fann|first3=George|title = Multiresolution separated representations of singular and weakly singular operators|journal = Applied and Computational Harmonic Analysis|volume = 23|issue = 2|pages = 235–253|year = 2007|doi = 10.1016/j.acha.2007.01.001}}</ref>\n\nThere are three main components to MADNESS. At the lowest level is a [[petascale]] [[parallel programming]] environment\n<ref>{{cite journal|last1 = Thornton|first1 = W. Scott|last2 = Vence|first2 = Nicholas|last3 = Harrison|first3 = Robert E.|title = Introducing the MADNESS numerical framework for petascale computing|journal = Proceedings of the Cray User Group Conference|year = 2009|url = http://www.cug.org/5-publications/proceedings_attendee_lists/CUG09CD/S09_Proceedings/pages/authors/01-5Monday/4A-Thornton/Thornton-paper.pdf}}</ref>\nthat aims to increases programmer productivity and code performance/scalability while maintaining backward compatibility with current programming tools such as the [[Message Passing Interface|message-passing interface]] and [[Global Arrays]]. The numerical capabilities built upon the parallel tools provide a high-level environment for composing and solving [[Numerical analysis|numerical problems]] in many (1-6+) dimensions. Finally, built upon the numerical tools are new applications with initial focus upon chemistry, <ref>{{cite journal|last1=Fosso-Tande|first1=Jacob|last2=Harrison|first2=Robert\n|title= Implicit solvation models in a multiresolution multiwavelet basis\n|journal=Chemical Physics Letters|volume=561–562|pages=179–184|year=2013\n|doi=10.1016/j.cplett.2013.01.065|bibcode=2013CPL...561..179F}}\n</ref>\n<ref>{{cite journal|last1=Fosso-Tande|first1=Jacob|last2=Harrison|first2=Robert\n|title= Confinement effects of solvation on a molecule physisorbed on a polarizable continuum particle\n|journal=Computational and Theoretical Chemistry|volume=1017|pages=22–30|year=2013\n|doi=10.1016/j.comptc.2013.05.006}}\n</ref>\n, atomic and molecular physics, <ref>{{cite journal|last1=Vence|first1=Nicholas|last2=Harrison|first2=Robert|last3=Krstic|first3=Predrag\n|title= Attosecond electron dynamics: A multiresolution approach\n|journal=Physical Review A|volume=85|issue=3|page=0303403|year=2012\n|doi=10.1103/PhysRevA.85.033403|bibcode=2012PhRvA..85c3403V}}\n</ref>\nmaterial science, and nuclear structure.  It is [[open-source software|open-source]], has an [[object-oriented]] design, and is designed to be a [[Parallel computing|parallel processing]] program for computers with up to millions of cores running already on the [[Cray XT5]] at [[Oak Ridge National Laboratory]] and the [[IBM Blue Gene]] at [[Argonne National Laboratory]].  The small [[matrix multiplication]] (relative to large, [[BLAS]]-optimized matrices) is the primary computational kernel in MADNESS; thus, an efficient implement on modern [[CPUs]] is an ongoing research effort.\n<ref>{{cite book|last5 = Harrison|first5 = Robert J.|last1=Stock|first1=Kevin|last2=Henretty|first2=Thomas|last3= Murugandi |first3=I.|last4 = Sadayappan|first4=P.|title = Model-Driven SIMD Code Generation for a Multi-resolution Tensor Kernel|journal = Proceedings of the IEEE International Parallel Distributed Processing Symposium (IPDPS)|pages = 1058–1067|year = 2011|doi = 10.1109/IPDPS.2011.101|isbn = 978-1-61284-372-8}}</ref>\n.<ref>{{cite journal|first1=Jaewook|last1=Shin|first2=Mary W.|last2=Hall|first3=Jacqueline|last3=Chame|first4=Chun|last4=Chen|first5=Paul D.|last5=Hovland|title=Autotuning and specialization: Speeding up matrix multiply for small matrices with compiler technology|journal=Proceedings of the Fourth International Workshop on Automatic Performance Tuning|year=2009|url=http://www-unix.mcs.anl.gov/~jaewook/papers/iwapt09.pdf}}{{dead link|date=January 2018 |bot=InternetArchiveBot |fix-attempted=yes }}</ref>\nAdapting the irregular computation in MADNESS to heterogeneous platforms is nontrivial due to the size of the kernel, which is too small to be offloaded via compiler directives (e.g. [[OpenACC]]), but has been demonstrated for [[CPU]]&ndash;[[GPU]] systems\n.<ref>{{cite book|last1=Slavici|first1=Vlad|last2=Varier|first2=Raghu|last3=Cooperman|first3=Gene|last4=Harrison|first4=Robert J.|title = Adapting Irregular Computations to Large CPU-GPU Clusters in the MADNESS Framework|journal = Proceedings of the IEEE International Conference on Cluster Computing (CLUSTER)|date=September 2012|pages = 1–9|doi = 10.1109/CLUSTER.2012.42|url = http://www.ccs.neu.edu/home/gene/papers/cluster12a.pdf|isbn=978-0-7695-4807-4}}</ref>\nIntel has publicly stated that MADNESS is one of the codes running on the [[Intel MIC]] architecture\n<ref>\n{{cite web\n|url=http://software.intel.com/en-us/blogs/2011/11/17/mic-architecture-support-by-software-tools-sc11-wrap-up\n|title= Intel Xeon Phi coprocessor support by software tools\n|author=James Reinders\n|date=20 September 2012}}\n</ref>\n<ref>\n{{cite web\n|url=https://www.theregister.co.uk/2011/11/16/intel_mic_xeon_e5_performance/\n|title= Hot Intel teraflops MIC coprocessor action in a hotel\n|author=Timothy Prickett Morgan\n|date=16 November 2011}}\n</ref>\nbut no performance data has been published yet.\n\nMADNESS' chemistry capability includes [[Hartree&ndash;Fock]] and [[density functional theory]] in chemistry\n<ref>{{cite journal|last1 = Harrison|first1 = Robert J.|last2 = Fann|first2 = George I.|last3 = Yanai|first3 = Takeshi|last4 = Gan|first4 = Zhengting|last5 = Beylkin|first5 = Gregory|title = Multiresolution quantum chemistry: Basic theory and initial applications|journal = The Journal of Chemical Physics|volume = 121|issue = 23|pages = 11587–11598|year = 2004|doi = 10.1063/1.1791051|url = http://link.aip.org/link/?JCP/121/11587/1|pmid = 15634124|bibcode = 2004JChPh.12111587H|access-date = 2019-05-15|archive-url = https://archive.is/20130223162802/http://link.aip.org/link/?JCP/121/11587/1|archive-date = 2013-02-23|dead-url = yes|df = }}</ref>\n<ref>{{cite journal|last4 = Harrison|first4 = Robert J.|last2 = George I.|first2 = Fann|last1 = Yanai|first1 = Takeshi|last3 = Gan|first3 = Zhengting|last5 = Beylkin|first5 = Gregory|title = Multiresolution quantum chemistry: Hartree-Fock exchange|journal = The Journal of Chemical Physics|volume = 121|issue = 14|pages = 6680–6688|year = 2004|doi = 10.1063/1.1790931|pmid = 15473723|url = http://link.aip.org/link/?JCP/121/6680/1|bibcode = 2004JChPh.121.6680Y|access-date = 2019-05-15|archive-url = https://archive.is/20130224003543/http://link.aip.org/link/?JCP/121/6680/1|archive-date = 2013-02-24|dead-url = yes|df = }}</ref>\n(including analytic derivatives, <ref>{{cite journal|last4 = Harrison|first4 = Robert J.|last2 = George I.|first2 = Fann|last1 = Yanai|first1 = Takeshi|last3 = Gan|first3 = Zhengting|last5 = Beylkin|first5 = Gregory|title = Multiresolution quantum chemistry: Analytic derivatives for Hartree--Fock and density functional theory|journal = The Journal of Chemical Physics|volume = 121|issue = 7|pages = 2866–2876|year = 2004|doi = 10.1063/1.1768161|pmid = 15291596|url = http://link.aip.org/link/?JCP/121/2866/1|bibcode = 2004JChPh.121.2866Y|access-date = 2019-05-15|archive-url = https://archive.is/20130223084131/http://link.aip.org/link/?JCP/121/2866/1|archive-date = 2013-02-23|dead-url = yes|df = }}</ref> response properties\n<ref>{{cite journal|last1 = Sekino|first1 = Hideo|last2 = Maeda|first2 = Yasuyuki|last3 = Yanai|first3 = Takeshi|last4 = Harrison|first4 = Robert J.|title = Basis set limit Hartree--Fock and density functional theory response property evaluation by multiresolution multiwavelet basis|journal = The Journal of Chemical Physics|volume = 129|issue = 3|pages = 034111–034117|year = 2008|doi = 10.1063/1.2955730|url = http://link.aip.org/link/?JCP/129/034111/1|pmid = 18647020|bibcode = 2008JChPh.129c4111S|access-date = 2019-05-15|archive-url = https://archive.is/20130223111053/http://link.aip.org/link/?JCP/129/034111/1|archive-date = 2013-02-23|dead-url = yes|df = }}</ref>\nand [[TDDFT|time-dependent density functional theory]] with asymptotically corrected potentials\n<ref>{{cite journal|last1=Yanai|first1=Takeshi|last2 = Harrison|first2 = Robert J.|last3=Handy|first3=Nicholas C.|title = Multiresolution quantum chemistry in multiwavelet bases: time-dependent density functional theory with asymptotically corrected potentials in local density and generalized gradient approximations|journal = Molecular Physics|volume = 103|issue = 2–3|pages = 413–424|year = 2005|doi = 10.1080/00268970412331319236|bibcode=2005MolPh.103..413Y|url=https://zenodo.org/record/1234395}}</ref>)\nas well as [[Density functional theory|nuclear density functional theory]]<ref>{{cite web |url=http://www.unedf.org/ |title=UNEDF SciDAC Collaboration Universal Nuclear Energy Density Functional |access-date=2012-11-19 |archive-url=https://web.archive.org/web/20130403054104/http://unedf.org/ |archive-date=2013-04-03 |dead-url=yes |df= }}</ref>\nand\n[[Hartree–Fock method|Hartree–Fock]]&ndash;[[Bogoliubov transformation|Bogoliubov]] theory.\n<ref>{{cite journal |first1=J.C.|last1=Pei|first2=G.I.|last2=Fann|first3=R.J.|last3=Harrison|first4=W.|last4=Nazarewicz|first5=J.|last5=Hill|first6=D.|last6=Galindo|first7=J.|last7=Jia |arxiv=1204.5254 |title=Coordinate-Space Hartree-Fock-Bogoliubov Solvers for Superfluid Fermi Systems in Large Boxes|year=2012 |doi=10.1088/1742-6596/402/1/012035 |volume=402 |journal=Journal of Physics: Conference Series |page=012035}}</ref><ref>{{cite journal\n|last1=Pei|first1=J. C.|last2=Stoitsov|first2=M. V.|last3=Fann|first3=G. I.\n|last4=Nazarewicz|first4=W.|last5=Schunck|first5=N.|last6=Xu|first6=F. R.\n|title = Deformed coordinate-space Hartree-Fock-Bogoliubov approach to weakly bound nuclei and large deformations\n|journal = Physical Review C\n|volume = 78\n|issue = 6\n|date=December 2008\n|pages = 064306–064317\n|doi = 10.1103/PhysRevC.78.064306\n|arxiv=0807.3036|bibcode=2008PhRvC..78f4306P}}\n</ref>\nMADNESS and [[BigDFT]] are the two most widely known codes that perform [[Density functional theory|DFT]] and [[TDDFT]] using wavelets\n.<ref>{{cite journal|first1=Bhaarathi|last1=Natarajan|first2=Luigi|last2=Genovese|first3=Mark E.|last3=Casida|first4=Thierry|last4=Deutsch|first5=Olga N.|last5=Burchak|first6=Christian|last6=Philouze|first7=Maxim Y.|last7=Balakirev|title = Wavelet-based linear-response time-dependent density-functional theory |journal = Chemical Physics|volume = 402 |pages = 29–40|year = 2012|doi = 10.1016/j.chemphys.2012.03.024 |arxiv=1108.3475|bibcode=2012CP....402...29N}}</ref>\nMany-body wavefunctions requiring six-dimensional spatial representations are also implemented\n(e.g. MP2<ref>{{cite journal|last1 = Bischoff|first1 = Florian A.|last2 = Harrison|first2 = Robert J.|last3 = Valeev|first3 = Edward F.|title = Computing many-body wave functions with guaranteed precision: The first-order Moller-Plesset wave function for the ground state of helium atom|journal = The Journal of Chemical Physics|volume = 137|issue = 10|pages = 104103–104112|year = 2012|doi = 10.1063/1.4747538|pmid = 22979846|url = http://link.aip.org/link/?JCP/137/104103/1|bibcode = 2012JChPh.137j4103B|access-date = 2019-05-15|archive-url = https://archive.is/20130223065104/http://link.aip.org/link/?JCP/137/104103/1|archive-date = 2013-02-23|dead-url = yes|df = }}</ref>).\nThe parallel runtime inside of MADNESS has been used to implement a wide variety of features, including graph optimization\n.<ref>{{cite techreport|last1 = Sullivan|first1=Blair D.|last2=Weerapurage|first2=Dinesh P.|last3=Groer|first3=Christopher S.|title = Parallel Algorithms for Graph Optimization using Tree Decompositions |year = 2012|doi = 10.2172/1042920|url = https://dx.doi.org/10.2172/1042920}}</ref>\nFrom a mathematical perspective, MADNESS emphasizes rigorous numerical precision without loss of computational performance\n.<ref>{{cite journal|last2 = Fann|first2 = George I.|last1 = Harrison|first1 = Robert J.|title = SPEED and PRECISION in QUANTUM CHEMISTRY|journal = SciDAC Review|volume = 1|issue = 3|pages = 54–65|year = 2007|url = http://www.scidacreview.org/0701/html/chemistry.html|access-date = 2012-11-19|archive-url = https://web.archive.org/web/20120803155207/http://www.scidacreview.org/0701/html/chemistry.html|archive-date = 2012-08-03|dead-url = yes|df = }}</ref>  This is useful not only in quantum chemistry and nuclear physics, but also the modeling of [[partial differential equations]]\n.<ref>{{cite journal|last1 = Reuter|first1 = Matthew G.|last3 = Harrison|first3 = Robert J.|last2 = Hill|first2 = Judith C.|title = Solving PDEs in irregular geometries with multiresolution methods I: Embedded Dirichlet boundary conditions|journal = Computer Physics Communications|volume = 183|issue = 1|pages = 1–7|year = 2012|doi = 10.1016/j.cpc.2011.07.001|bibcode = 2012CoPhC.183....1R|url = https://zenodo.org/record/1258871}}</ref>\n\nMADNESS was recognized by the [[R&D 100 Awards]] in 2011.<ref name=\"rdmag 2011\">{{cite news | url=http://www.rdmag.com/award-winners/2011/08/free-framework-scientific-simulation | title=Free framework for scientific simulation | work=[[R&D Magazine]] | date=14 August 2011 | accessdate=November 26, 2012}}</ref><ref>{{cite web|url=http://www.olcf.ornl.gov/2011/06/29/madness-named-rd-100-winner/ |title=MADNESS Named R&D 100 Winner}}</ref>  It is an important code to [[United States Department of Energy|Department of Energy]] supercomputing sites and is being used by both the leadership computing facilities at [[Argonne National Laboratory]]<ref>{{cite web|url=https://www.alcf.anl.gov/projects/accurate-numerical-simulations-chemical-phenomena-involved-energy-production-and-storage/ |title=Accurate Numerical Simulations Of Chemical Phenomena Involved in Energy Production and Storage with MADNESS and MPQC}}</ref>\nand [[Oak Ridge National Laboratory]]<ref>{{cite web|url=http://www.olcf.ornl.gov/wp-content/uploads/2012/05/CrayTechOct2012Bronson2.pdf |title=Application Readiness at ORNL}}</ref> to evaluate the stability and performance of their latest supercomputers.  It has users around the world, including the United States and Japan\n.<ref>{{cite web\n |url         = http://www.jics.tennessee.edu/yukina\n |title       = Far from home - Japanese graduate student journeys to UT to study computational chemistry\n |deadurl     = yes\n |archiveurl  = https://archive.is/20121215010848/http://www.jics.tennessee.edu/yukina\n |archivedate = 2012-12-15\n |df          = \n}}</ref>\nMADNESS has been a workhorse code for computational chemistry in the DOE [[INCITE]] program\n<ref>{{cite web\n |url         = http://www.hpcwire.com/hpcwire/2011-06-01/chemistry_and_materials_simulations_speed_clean_energy_production_and_storage.html\n |title       = Chemistry and Materials Simulations Speed Clean Energy Production and Storage\n |date        = 1 June 2011\n |deadurl     = yes\n |archiveurl  = https://web.archive.org/web/20110806073031/http://www.hpcwire.com/hpcwire/2011-06-01/chemistry_and_materials_simulations_speed_clean_energy_production_and_storage.html\n |archivedate = 6 August 2011\n |df          = \n}}</ref>\nat the [[Oak Ridge Leadership Computing Facility]]\n<ref>{{cite journal|first1=A.|last1=Bland|first2=R.|last2=Kendall|first3=D.|last3=Kothe|first4=J.|last4=Rogers|first5=G.|last5=Shipman|title=Jaguar: The world's most powerful computer|journal=Proceedings of the Cray User Group Conference|year=2010|url=http://www.nccs.gov/wp-content/uploads/2010/01/Bland-Jaguar-Paper.pdf|deadurl=yes|archiveurl=https://web.archive.org/web/20121224203858/http://www.nccs.gov/wp-content/uploads/2010/01/Bland-Jaguar-Paper.pdf|archivedate=2012-12-24|df=}}</ref>\nand is noted as one of the important codes to run on the [[Cray]] Cascade architecture.<ref>{{cite web\n|url=http://www.greencarcongress.com/2012/11/cascade-20121108.html\n|title= Cray unveils 100 petaflop XC30 supercomputer\n|date= 8 November 2012}}</ref>\n\n==See also==\n{{Portal|Free and open-source software}}\n\n* [[List of numerical analysis software]]\n* [[List of quantum chemistry and solid state physics software]]\n\n==References==\n{{reflist|colwidth=30em}}\n\n==External links==\n* [http://code.google.com/p/m-a-d-n-e-s-s/ MADNESS Homepage] on [[Google Code]]\n\n{{Chemistry software}}\n\n[[Category:Computational chemistry software]]\n[[Category:Free mathematics software]]\n[[Category:Mathematical software]]\n[[Category:Numerical software]]\n[[Category:Parallel computing]]"
    },
    {
      "title": "Math Kernel Library",
      "url": "https://en.wikipedia.org/wiki/Math_Kernel_Library",
      "text": "{{Infobox software\n| name                   = Math Kernel Library\n| logo                   =\n| screenshot             =\n| caption                =\n| collapsible            =\n| author                 =\n| developer              = [[Intel]]\n| released               = {{Start date and age|2003|05|09}}\n| latest release version = 2018 Update 1\n| latest release date    = November 11, 2017<ref>{{cite web |title=Intel Math Kernel Library 2018 Release Notes |url=https://software.intel.com/en-us/articles/intel-math-kernel-library-intel-mkl-2018-release-notes}}</ref>\n| latest preview version =\n| latest preview date    =\n| frequently updated     =\n| programming language   = [[C (programming language)|C]]/[[C++]], [[Fortran]]\n| operating system       = [[Microsoft Windows]], [[Linux]], [[macOS]]\n| platform               = [[Intel Xeon Phi]], [[Intel Xeon]], [[Intel Core]], [[Intel Atom]]<ref name=\"official\">[https://software.intel.com/en-us/intel-mkl Intel® Math Kernel Library (Intel® MKL) | Intel® Software]</ref>\n| size                   =\n| language               =\n| status                 =\n| genre                  = [[Library (computing)|Library]] and [[Software framework|framework]]\n| license                = Freeware<ref name=\"license\">{{cite web |title=Intel Simplified Software License |url=https://software.intel.com/en-us/license/intel-simplified-software-license}}</ref>\n}}\n\n[[Intel]] '''Math Kernel Library''' (Intel '''MKL''') is a [[Library (computer science)|library]] of optimized math routines for science, engineering, and financial applications. Core math functions include [[BLAS]], [[LAPACK]], [[ScaLAPACK]], sparse solvers, [[fast Fourier transform]]s, and vector math.<ref name=\"cambridge\">{{cite web |title=Intel Math Kernel Library|url=http://www.its.hku.hk/services/research/hpc/software/mkl}}</ref> The routines in MKL are hand-optimized specifically for Intel processors.<ref name=\"NSC\">{{cite web |title=Intel Math Kernel Library (MKL)|url=https://www.nsc.liu.se/software/math-libraries/}}</ref><ref name=\"colfax\">{{cite web |title=Webinar 3 – Practical usage of Intel Math Kernel Library: performance tuning tips and usage with coprocessors|url=http://colfaxresearch.com/hot-16-03/#3}}</ref>\n\nThe library supports Intel processors<ref name=\"official\"/> and is available for [[Windows]], [[Linux]] and [[macOS]] [[operating system]]s.<ref name=\"OhioState\">{{cite web |title=MKL - Intel Math Kernel Library|url=https://www.osc.edu/documentation/software-list/mkl-intel-math-kernel-library}}</ref><ref name=\"cambridge\"/><ref name=\"NSC\"/><ref name=\"cambridge\"/>\n\n==History==\nIntel launched the Math Kernel Library on May 9, 2003, and called it blas.lib.<ref name=Telecomworldwire/> The project's development teams are located in Russia and the United States. MKL is bundled with Intel Parallel Studio XE, Intel Cluster Studio XE, Intel C++, Fortran Studio XE products as well as canopy. Standalone versions have not been sold for years to new customers but are now available for free.<ref name=\"licensing-faq\" />\n\n==License==\nThe library is available free of charge under the terms of Intel Simplified Software License<ref name=\"license\" /> which allow redistribution.<ref name=\"licensing-faq\">{{cite web|title=Intel Math Kernel Library Licensing FAQ|url=https://software.intel.com/en-us/mkl/license-faq}}</ref> Commercial support is available when purchased as a standalone software or as part of [[Intel Parallel Studio]] XE or Intel System Studio.\n\n==Details==\n\n===Functional categories===\nIntel MKL has the following functional categories:<ref>[https://software.intel.com/en-us/mkl-developer-reference-c Developer Reference for Intel® Math Kernel Library 2018 - C]</ref>\n*'''[[Linear algebra]]:''' BLAS routines are vector-vector (Level 1), matrix-vector (Level 2) and matrix matrix (Level 3) operations for real and complex single and double precision data. LAPACK consists of tuned LU, Cholesky and QR factorizations, eigenvalue and least squares solvers. MKL also includes Sparse BLAS, [[ScaLAPACK]], Sparse Solver, [[Eigenvalue algorithm|Extended Eigensolver]], [[PBLAS]] and BLACS. \n*:Since MKL uses standard interfaces for BLAS and LAPACK, the application which uses other implementations can get better performance on Intel and compatible processors by re-linking with MKL libraries.\n*MKL includes a variety of '''[[Fast Fourier transform|Fast Fourier Transforms]] (FFTs)''' from 1D to multidimensional, complex to complex, real to complex, and real to real transforms of arbitrary lengths. Applications written with the open source [[FFTW]] can be easily ported to MKL by linking with interface wrapper libraries provided as part of MKL for easy migration.\n*:Cluster versions of LAPACK and FFTs are also available as part of MKL to take advantage of MPI parallelism in addition to single node parallelism from multithreading.\n*'''Vector math''' functions include computationally intensive core mathematical operations for single and double precision real and complex data types. These are similar to libm functions from compiler libraries but operate on vectors rather than scalars to provide better performance. There are various controls for setting accuracy, error mode and denormalized number handling to customize the behavior of the routines.\n*'''[[Statistics]]''' functions include random number generators and probability distributions. optimized for multicore processors. Also included are compute-intensive in and out-of-core routines to compute basic statistics, estimation of dependencies etc.\n*'''Data fitting''' functions include splines (linear, quadratic, cubic, look-up, stepwise constant) for 1-dimensional interpolation that can be used in data analytics, geometric modeling and surface approximation applications.\n*'''[[Deep learning|Deep Neural Network]]'''\n*'''[[Partial differential equation|Partial Differential Equations]]'''\n*'''[[Nonlinear programming|Nonlinear Optimization Problem Solvers]]'''\n\n==See also==\n* [[List of numerical libraries]]\n* [[Automatically Tuned Linear Algebra Software]] (ATLAS)\n* [[GotoBLAS|GotoBLAS and OpenBLAS]]\n\n==References==\n{{Reflist|refs=\n<ref name=\"Telecomworldwire\">{{cite news|url=http://www.accessmylibrary.com/article-1G1-101517042/intel-launches-library-tool.html|title=Intel launches library tool for developers. (Intel Math Kernel Library 6.0) (Brief Article)|date=May 9, 2003|work=Telecomworldwire|accessdate=November 29, 2009}}</ref>\n}}\n\n==External links==\n*{{Official website}}\n*[https://software.intel.com/en-us/mkl/documentation Documentation]\n*[http://software.intel.com/en-us/forums/intel-math-kernel-library MKL User Forum]\n*[http://premier.intel.com/ MKL Support Channel]\n\n{{Intel software}}\n{{Numerical linear algebra}}\n\n[[Category:C++ numerical libraries]]\n[[Category:Intel software]]\n[[Category:Numerical software]]"
    },
    {
      "title": "Mathcad",
      "url": "https://en.wikipedia.org/wiki/Mathcad",
      "text": "{{Infobox Software\n|name                   = Mathcad\n|logo                   = Ptc mathcad logo standard color.png\n|screenshot             = top right Prime screengrab.png\n|caption                = Mathcad Prime 1.0\n|developer              = [[Mathsoft]], [[Parametric Technology Corporation|PTC]]\n|released               = {{Start date|1986}}\n|discontinued           = \n|latest release version = 15.0 M045 / {{Start date|2015|11}}<ref>{{cite web |title=READ THIS FIRST PTC® Mathcad® 15.0 M045 |url=https://www.ptcusercommunity.com/servlet/JiveServlet/download/431701-109230/PTC%20Mathcad%2015.0%20M045%20Read%20This%20First.pdf |date=November 2015 |deadurl=no |archiveurl=https://web.archive.org/web/20170510065949/https://www.ptcusercommunity.com/servlet/JiveServlet/download/431701-109230/PTC%20Mathcad%2015.0%20M045%20Read%20This%20First.pdf |archivedate=2017-05-10 |df= }}</ref> <br /> Prime 5.0 / {{Start date and age|2018|08|14}}\n|latest release date    = \n|latest preview version = \n|latest preview date    = \n|programming language   = \n|operating system       = [[Microsoft Windows]]\n|language               = 10 languages<ref>{{cite web|url=http://www.ptc.com/products/mathcad/language-support/|title=Mathcad Language Support - PTC.com|publisher=|deadurl=no|archiveurl=https://web.archive.org/web/20100910123516/http://www.ptc.com/products/mathcad/language-support/|archivedate=2010-09-10|df=}}</ref>\n|status                 = Active\n|genre                  = [[Computer algebra system]]\n|license                = [[Proprietary software|Proprietary]]\n|website                = {{URL|www.ptc.com/products/mathcad/}}\n}}\n\n'''Mathcad''' is computer software primarily intended for the verification, validation, documentation and re-use of engineering calculations.<ref>{{cite web|url=http://ptc.com/products/mathcad/|title=PTC Mathcad - PTC|publisher=|deadurl=no|archiveurl=https://web.archive.org/web/20091011031346/http://www.ptc.com/products/mathcad/|archivedate=2009-10-11|df=}}</ref> First introduced in 1986 on [[DOS]], it was the first to introduce live editing of typeset mathematical notation, combined with its automatic computations.\n\n==Overview==\nMathcad, [[Parametric Technology Corporation]]'s engineering calculation solution, is used by engineers and scientists in various disciplines – most often those of mechanical, chemical, electrical, and civil engineering. Originally conceived and written by [[Allen Razdow]] (of [[Massachusetts Institute of Technology|MIT]], co-founder of [[Mathsoft]]), Mathcad is now owned by [[Parametric Technology Corporation|PTC]] and is generally accepted as the first computer application to automatically compute and check consistency of engineering units such as the [[International System of Units]] (SI), throughout the entire set of calculations. Mathcad today includes some of the capabilities of a [[computer algebra system]], but remains oriented towards ease of use and simultaneous documentation of numerical engineering applications.\n\nMathcad is oriented around a [[worksheet]], in which equations and expressions are created and manipulated in the same graphical format in which they are presented ([[WYSIWYG]]) - as opposed to authoring in plain text, an approach later adopted by other systems such as [[Mathematica]] and [[Maple (software)|Maple]].\n\nMathcad is part of a broader product development system developed by PTC, and often utilized for the many analytical touch points within the systems engineering processes. It integrates with PTC’s other solutions that aid product development, including [[Creo Elements/Pro]], [[Windchill (software)|Windchill]], and [[Creo Elements/View]]. Its live feature-level integration with Creo Elements/Pro enables Mathcad analytical models to be directly used in driving [[Computer-aided design|CAD]] geometry, and its structural awareness within Windchill allows live calculations to be re-used and re-applied toward multiple design models.\n\n==Summary of capabilities==\nThe Mathcad interface allows users to combine a variety of different elements (mathematics, descriptive text, and supporting imagery) into the form of a worksheet, which is naturally readable. Because the mathematics are core to the program, the math is inherently live, dynamically recalculating as upstream values are altered. This allows for simple manipulation of input variables, assumptions, and expressions, which in turn update in real-time. The examples below serve to outline the scope of Mathcad’s capabilities, rather than to give specific details on the individual product functionality.\n\n* Utilize numerous numeric [[function (mathematics)|functions]], across examples such as statistics, data analysis, image processing, and signal processing\n* Automatically manage [[units of measurement|units]] throughout the worksheet, preventing improper operations and performing automatic unit-reduction\n* Solve [[systems of equations]], such as [[Ordinary differential equation|ODEs]] and [[Partial differential equation|PDEs]] through the use of several methods\n* Find roots of [[polynomials]] and functions\n* Calculate and manipulate expressions [[Symbolic math|symbolically]], including within systems of equations\n* Create [[Parametric plot|parametric]] 2D and 3D [[plot (graphics)|plot]] types, as well as discrete data plots\n* Leverage standard, readable [[Expression (mathematics)|mathematical expressions]] within embedded program constructs\n* Perform [[Vector (geometric)|vector]] and [[Matrix (mathematics)|matrix]] operations, including [[eigenvalue]]s and [[eigenvector]]s\n* Perform [[curve fitting]] and [[regression analysis]] on experimental datasets\n* Utilize statistical and [[Design of experiments|Design of Experiments]] functions and plot types, and evaluate probability distributions\n* Import from, and export to, other applications and file types, such as [[Microsoft excel|Microsoft Excel]] and [[MathML]].<ref>{{cite web |url=http://www.jmcsweeney.co.uk/webdesign/mathcad.php |title=Archived copy |accessdate=2011-09-09 |deadurl=yes |archiveurl=https://web.archive.org/web/20110916053405/http://www.jmcsweeney.co.uk/webdesign/mathcad.php |archivedate=2011-09-16 |df= }}</ref>\n* Include references to other Mathcad worksheets to re-use common engineering methods\n* Integrate with other engineering applications, such as [[Computer-aided design|CAD]], [[Finite Element Method|FEM]], [[Building Information Modeling|BIM]], and [[Simulation]] tools, to aid in product design, like [[Autocad]], [[Ansys]], [[Revit]]\n\nAlthough Mathcad is mostly oriented to non-programming users, it is also used in more complex projects to visualize results of mathematical modeling by using distributed computing and coupling with programs written using more traditional languages such as C++.\n\n== Computational Errors ==\n{{Original Research|date=November 2018}}\nMathCAD 11.0, 12.0, 13.0, and 14.0 had faulty implementations of the '''rref''' algorithm that frequently miscalculated the reduced row echelon form of complex matrices.<ref>{{Cite news|url=https://community.ptc.com/t5/PTC-Mathcad-Questions/rref-and-complex-arithmetic/m-p/157103/highlight/false#M61657|title=rref and complex arithmetic|date=2006-11-13|access-date=2018-05-29|language=en}}</ref>  MathCAD 13.0 and 14.0 (and probably the earlier versions) had an error with symbolic vector cross product.<ref name=\":0\">Personal Experience</ref>  Specifically, in a MathCAD document, the expression\n\n:<math>(a+b)\\times c</math>\n\nwould be incorrectly \"simplified\" to the expression\n:<math>a+b\\times c</math>.  \n\nThat is, to the user, the parentheses merely \"disappeared\".  If the vectors <math>a, b, c </math> were not previously provided values in the form of three-tuples of numbers, then this amounted to a vector algebra error, failing to properly apply distributivity of vector cross product over vector addition.  On the other hand, if the vectors had been assigned values, then both of the above expressions would reduce to the same value, as long as the second expression had been copied and pasted from the \"simplified\" result of the former expression, but if the user typed in the second expression, then its value as a specific three-tuple would be computed correctly.<ref name=\":0\" />  MathCAD 15.0 erroneously computes some integrals.<ref name=\":0\" /> See the image at right for an example.\n\n== Current releases ==\n\nAs of August 2018, there are two actively maintained Mathcad releases and a free express version available to consumers:\n* Mathcad 15.0 was originally released in June, 2010. Its first maintenance release was released in November, 2010. Mathcad 15.0 is the next progressive release of the traditional product line, sharing the same worksheet file structure and extension as its predecessor, Mathcad 14.0.\n* Mathcad Prime 5.0.0.0, PTC’s latest generation product, was introduced in August 2018 <ref name=\":1\">{{Cite news|url=https://www.ptc.com/en/products/mathcad/new-release|title=PTC Mathcad Prime 5.0 Introduces New 2D Plotting Capabilities {{!}} PTC|access-date=2018-08-28}}</ref>. This is PTC's latest release.\n* PTC Mathcad Express Free-for-Life Engineering Calculations Software - a Mathcad Prime 30-day trial, but when the 30-day trial period has ended, it is possible to continue using PTC Mathcad Express for an unlimited time. This [[Freemium]] pilot marks a new marketing approach for PTC. Review and markup of engineering notes can now be done directly by team members without them all requiring a full Mathcad Prime license.<ref>{{cite web|url=http://www.engineering.com/DesignSoftware/DesignSoftwareArticles/ArticleID/4897/Psst-PTC-is-Giving-Away-Mathcad-Express-Pass-it-on.aspx|title=Psst... PTC is Giving Away Mathcad Express. Pass it on. > ENGINEERING.com|first=|last=ENGINEERING.com|publisher=|deadurl=no|archiveurl=https://web.archive.org/web/20131012015718/http://www.engineering.com/DesignSoftware/DesignSoftwareArticles/ArticleID/4897/Psst-PTC-is-Giving-Away-Mathcad-Express-Pass-it-on.aspx|archivedate=2013-10-12|df=}}</ref>\n\n==Computer operating system platforms==\nMathcad is currently a Windows-only application. Current releases of Mathcad 15.0 and Mathcad Prime 4.0 are supported on 32-bit and 64-bit versions of [[Windows XP]], [[Windows Vista]], and [[Windows 7]]. Currently releases are 32-bit applications only, however. While users do utilize emulation to establish other platform operability, Mathcad’s last officially supported, natively installed Mac OS release was on January 8, 1998.<ref>{{cite web |url=http://www.versiontracker.com/dyn/moreinfo/macos/2770 |title=Archived copy |accessdate=2009-10-23 |deadurl=yes |archiveurl=https://web.archive.org/web/20090611154512/http://www.versiontracker.com/dyn/moreinfo/macos/2770 |archivedate=2009-06-11 |df= }}</ref> This version can still be run on Macintosh computers that support the [[Classic Environment|Mac OS X Classic Environment]] or [[SheepShaver]].\n==Support==\nAfter [[Parametric Technology Corporation|PTC]]’s purchase of Mathcad in 2006,<ref>{{cite web|url=http://library.corporate-ir.net/library/11/116/116312/items/194510/PTC_Slides.pdf|title=PTC to Acquire Mathsoft Engineering & Education (April 26, 2006 slide show),|author=|date=|website=corporate-ir.net|accessdate=24 April 2018|deadurl=no|archiveurl=https://web.archive.org/web/20160303182216/http://library.corporate-ir.net/library/11/116/116312/items/194510/PTC_Slides.pdf|archivedate=3 March 2016|df=}}</ref> changes were made to the Mathcad support policy. That change specified non-maintenance bearing licenses were no longer able to receive updates, including bug fixes, without purchasing a maintenance contract. Faculty at universities that owned enterprise subscriptions for the purpose of teaching or research applications who needed to report bugs were redirected to report such computational errors to the client license officers, putting at least three layers of extra bureaucracy between knowledgeable users and programmers, so that bug fixes would never be implemented, even in future major new version upgrades.<ref name=\":0\" /> Even such faculty who purchased an enterprise license for their own business, and therefore became their own license representative, were not allowed to report to programmers, but only to untrained support personnel at PTC, resulting in extremely long times between bug reporting and even admission by programmers at PTC and support personnel that there was a bug worth fixing.<ref name=\":0\" /> Though disagreeable to some long-time, pre-PTC-acquisition customers, this is PTC’s standard policy for all its other products.<ref>{{cite web|url=http://www.ptc.com/support/maintenance/gold.htm|title=Global Maintenance Support - Packages - PTC.com|publisher=|deadurl=no|archiveurl=https://web.archive.org/web/20080513213500/http://www.ptc.com/support/maintenance/gold.htm|archivedate=2008-05-13|df=}}</ref> The price of a Mathcad maintenance contract for an individual is roughly half the cost of a single-user license upgrade at the time of a major Mathcad release, and grants the user the right to receive major releases in addition to bug fixes, access to engineering content, technical support, self-service license transferring tools, and more.\n\nThe Mathcad Business Unit within PTC recently updated their support policy. For Mathcad 15.0 and future versions of Mathcad, the first year of maintenance entitlements and support will be included in the purchase or upgrade price.\n\nAs of September 2014, due to the rapidly changing technology landscape and [[Microsoft's]] retiring of the [[Windows XP]] operating system, [[PTC (software company)|PTC]] will discontinue PTC Mathcad support of the Windows Vista and XP operating systems with the release of PTC Mathcad Prime 3.1 and PTC Mathcad 15.0 M040.<ref>{{cite web|title=PTC Mathcad Support Announcement|website=http://blogs.ptc.com/2014/09/25/ptc-mathcad-operating-systems-support-announcement/}}</ref>\n\n== Release history ==\n{| class=\"wikitable\"\n|-\n! Name\n! Version\n! Release Date\n! Notes\n|-\n| Mathcad 0.3\n| 0.3\n| \n|beta on 5 1/4 floppy\n|-\n| Mathcad 2.5.2\n| 2.5.2\n| \n|Added document interface, last DOS version\n|-\n| Mathcad 3.1\n| 3.1\n| \n| Windows version\n|-\n| Mathcad 4.0\n| 4.0\n| \n| Windows version\n|-\n|-\n| Mathcad 5.0\n| 5.0\n|\n| Added Maple based CAS features\n|-\n| Mathcad 5.5\n| 5.5\n| \n| Windows version\n|-\n| Mathcad 6.0<ref>{{cite web |title=Mathcad Features Comparison Chart |url=http://www.mathsoft.com/60dir/chart.htm |archive-url=https://web.archive.org/web/19961110063011/http://www.mathsoft.com/60dir/chart.htm |archive-date=1996-11-10 |access-date=2016-06-12 }}</ref>\n| 6.0\n|-\n| Mathcad 7<ref>{{cite web |title=Mathcad PLUS 6.0 and Mathcad 7 Professional Comparison |url=http://www.mathsoft.com/mathcad/70dir/prof/mcd6to7.htm |archive-url=https://web.archive.org/web/19980220072847/http://www.mathsoft.com/mathcad/70dir/prof/mcd6to7.htm |archive-date=1998-02-20 |access-date=2016-06-12 }}</ref>\n| 7.0\n| 1997\n|-\n| Mathcad 8<ref>{{cite web |title=Mathcad Features: Version by Version Comparison |url=http://www.mathsoft.com/mathcad/80dir/comparison.htm |archive-url=https://web.archive.org/web/20000929212107/http://www.mathsoft.com/mathcad/80dir/comparison.htm |archive-date=2000-09-29 |access-date=2016-06-12 }}</ref>\n|-\n| Mathcad 2000<ref>{{cite web |title=Mathcad Version Comparison Chart |url=http://www.mathsoft.com/mathcad/2000dir/prof/comparechart.asp |archive-url=https://web.archive.org/web/19991128020553/http://www.mathsoft.com/mathcad/2000dir/prof/comparechart.asp |archive-date=1999-11-28 |access-date=2016-06-12 }}</ref>\n|-\n| Mathcad 2001i<ref>{{cite web |title=What's New in Mathcad 2001i |url=http://www.mathcad.com/upgrade/new_features.asp |archive-url=https://web.archive.org/web/20010215043224/http://www.mathcad.com/upgrade/new_features.asp |archive-date=2001-02-15 |access-date=2016-06-11 }}</ref>\n|-\n| Mathcad 11<ref>{{cite web |title=What's New in Mathcad 11 |url=http://www.mathcad.com/upgrade/new_features.asp |archive-url=https://web.archive.org/web/20021111110824/http://www.mathcad.com/upgrade/new_features.asp |archive-date=2002-11-11 |access-date=2016-06-11 }}</ref>\n|-\n| Mathcad 12<ref>{{cite web |title=Untitled |url=http://www.mathcad.com/products/mc12_brochure.pdf |archive-url=https://web.archive.org/web/20040916163113/http://www.mathcad.com/products/mc12_brochure.pdf |archive-date=2004-09-16 |access-date=2016-06-12 }}</ref>\n|-\n| Mathcad 13.0<ref name=\"mathcad_relnotes_14\" />\n| 13.0 || {{dts|2005|09|15}}<ref name=\"mathcad15RTF2\" />\n|-\n| Mathcad 13.1<ref name=\"mathcad_relnotes_14\" />\n| 13.1\n|-\n| Mathcad 14.0<ref name=\"mathcad_relnotes_14\">{{cite web |title=Release Notes for Mathcad 14 |url=http://support.ptc.com/WCMS/files/45358/en/MC14_RELNOTES.htm |access-date=2016-06-12 |deadurl=no |archiveurl=https://web.archive.org/web/20170510131058/http://support.ptc.com/WCMS/files/45358/en/MC14_RELNOTES.htm |archivedate=2017-05-10 |df= }}</ref>\n| 14.0 || {{dts|2007|02|12}}<ref name=\"mathcad15RTF2\" />\n|-\n| Mathcad 15.0<ref name=\"mathcad15RTF\">{{cite web |title=Latest Release of MC15 (M0??) |url=https://www.ptcusercommunity.com/thread/35657 |access-date=2016-06-12 |deadurl=no |archiveurl=https://web.archive.org/web/20160616194930/https://www.ptcusercommunity.com/thread/35657 |archivedate=2016-06-16 |df= }}</ref>\n| 15.0 F000 || {{dts|2010|06|25}}<ref name=\"mathcad15RTF2\" />\n|-\n| Mathcad 15.0 M010<ref name=\"mathcad15RTF\" />\n| 15.0 M010 || {{dts|2011|06|29}}<ref name=\"mathcad15RTF2\" />\n|-\n| Mathcad 15.0 M040<ref name=\"mathcad15RTF2\">{{cite web |title=When to release Mathcad 15 M040? |url=https://www.ptcusercommunity.com/thread/85444 |access-date=2016-06-12 |deadurl=no |archiveurl=https://web.archive.org/web/20160616203441/https://www.ptcusercommunity.com/thread/85444 |archivedate=2016-06-16 |df= }}</ref>\n| 15.0 M040\n|-\n| Mathcad 15.0 M045<ref name=\"mathcad15m045RTF\">{{cite web |title=Mathcad 15 M045 release notes (\"Read This First\" guide) |url=https://www.ptcusercommunity.com/docs/DOC-8181 |access-date=2016-06-12 |deadurl=no |archiveurl=https://web.archive.org/web/20160616193504/https://www.ptcusercommunity.com/docs/DOC-8181 |archivedate=2016-06-16 |df= }}</ref>\n| 15.0 M045 || {{dts|2015|11}}<ref name=\"mathcad15m045RTF\" />\n|-\n| Mathcad Prime 1.0<ref>{{cite web |title=Choose the Mathcad® product that’s right for you |url=http://www.ptc.com/WCMS/files/121440/en/6017_Mathcad_Comparison_TS_EN.PDF |archive-url=https://web.archive.org/web/20110810032128/http://www.ptc.com/WCMS/files/121440/en/6017_Mathcad_Comparison_TS_EN.PDF |archive-date=2011-08-10 |access-date=2016-06-12 }}</ref>\n| || {{dts|2011|01|10}}<ref name=\"mathcad15RTF2\" />\n|-\n| Mathcad Prime 2.0\n| || {{dts|2012|02|29}}<ref name=\"mathcad15RTF2\" />\n|-\n| Mathcad Prime 3.0\n| || {{dts|2013|10|02}}<ref name=\"mathcad15RTF2\" />\n|-\n| Mathcad Prime 3.1\n| || {{dts|2015|03|02}}<ref name=\"mathcad15RTF2\" />\n|-\n| Mathcad Prime 4.0\n| || {{dts|2017|03|06}}<ref name=\"mathcad15RTF2\" />\n|-\n| Mathcad Prime 5.0.0.0\n| || {{dts|2018|08|14}}<ref name=\":1\" />\n|}\n\n==Screen captures of previous Mathcad versions==\n<gallery>\nImage:Mathcad 252 screenshot.png|Mathcad 2.52 (1989)\nImage:Mathcad31.png|Mathcad 3.1 (1992)\nImage:Mathcad 6 screenshot.png|Mathcad PLUS 6.0 (1995)\nImage:historical_MC13.png|Mathcad 13.0\nImage:historical_MC15.jpg|Mathcad 15.0\nImage:historical_MCPrime1.png|Mathcad Prime 1.0\nImage:top_right_Prime_screengrab.png|Mathcad Prime 1.0 working session\n</gallery>\n\n==See also==\n* [[Comparison of computer algebra systems]]\n* [[Comparison of numerical analysis software]]\n* [[TK Solver]]\n* [[Creo (design software)|PTC:Creo]]\n* [[Windchill (software)|PTC:Windchill]]\n* [[SMath Studio]], a freeware similar to MathCad\n\n==References==\n{{reflist}}\n\n==External links==\n* {{Official website|www.ptc.com/products/mathcad/}}\n* [http://blogs.ptc.com/product/mathcad/ Mathcad Engineering Blog]\n* [http://www.ptc.com/product/mathcad/free-trial Free trial of Mathcad Prime – Mathcad Express] \n\n{{Computer algebra systems}}\n{{Numerical analysis software}}\n{{Statistical software}}\n{{PTC (software company)}}\n\n[[Category:Computer-related introductions in 1986]]\n[[Category:Computer algebra systems]]\n[[Category:Numerical software]]\n[[Category:Proprietary software]]\n[[Category:Windows-only software]]"
    },
    {
      "title": "Math.NET Numerics",
      "url": "https://en.wikipedia.org/wiki/Math.NET_Numerics",
      "text": "{{ infobox software\n| name                   = Math.NET Numerics\n| latest release version = 4.7.0\n| latest release date    = {{Start date and age|2018|11|11|df=yes}}\n| programming language   = [[C Sharp (programming language)|C#]], [[F Sharp (programming language)|F#]],  [[Common Language Runtime|.NET CLR]]\n| operating system       = [[Cross-platform]]\n| developer              = C. Rüegg, M. Cuda, et al.\n| genre                  = [[List of numerical analysis software|Numerical library]]\n| license                = [[MIT License|MIT/X11]]\n| website                = {{URL|http://numerics.mathdotnet.com/}}\n}}\n'''Math.NET Numerics''' is an [[open-source software|open-source]] numerical library for [[Microsoft .NET|.NET]] and [[Mono (software)|Mono]], written in [[C Sharp (programming language)|C#]] and [[F Sharp (programming language)|F#]]. It features functionality similar to [[Basic Linear Algebra Subprograms|BLAS]] and [[LAPACK]].\n\n== History ==\nMath.NET Numerics started 2009 by merging code and teams of [[dnAnalytics]] with Math.NET Iridium. It is influenced by [[ALGLIB]], [[JAMA (numerical linear algebra library)|JAMA]] and [[Boost (C++ libraries)|Boost]], among others, and has accepted numerous code contributions.<ref>{{cite web|url=https://github.com/mathnet/mathnet-numerics/blob/master/README.md |title=Math.NET Numerics ReadMe |publisher=GitHub.com |accessdate=2013-05-08}}</ref><ref>{{cite web|url=https://github.com/mathnet/mathnet-numerics/blob/master/CONTRIBUTORS.md |title=Math.NET Numerics Contributors |publisher=GitHub.com |accessdate=2013-05-08}}</ref> It is part of the Math.NET initiative to build and maintain open mathematical toolkits for the .NET platform since 2002.\n\nMath.NET is used by several open source libraries and research projects, like MyMediaLite,<ref>{{cite web|url=http://www.ismll.uni-hildesheim.de/mymedialite/ |title=MyMediaLite Recommender System Library}}</ref> FermiSim<ref>{{cite web|url=https://launchpad.net/fermisim |title=FermiSim, studying potential solutions to the Fermi paradox via computational simulation of models for space colonisation}}</ref> and LightField Retrieval,<ref>{{cite web|url=https://code.google.com/p/lightfieldretrieval/ |title=Three-Dimensional Model Shape Description and Retrieval Based on LightField Descriptors}}</ref> and various theses<ref>{{cite thesis |type=M.Sc |title=Detecting falls and poses in image silhouettes |last=Schräder |first=Niklas |year=2011 |publisher=Chalmers University of Technology, Gothenburg, Sweden |issn=1652-8557}}</ref><ref>{{cite thesis |type=M.Sc |title=Elastic Properties of Growing 2D Foam |last=Schindlberger |first=Michael |year=2011 |publisher=University of Zurich}}</ref><ref>{{cite thesis |type=M.Sc |title=SoundLog: Make More Noise |last=Ferreira |first=André Filipe Mateus |publisher=Universidade Técnica de Lisboa}}</ref><ref>{{cite thesis |type=B.Sc |title=Design of a Wireless Acquisition System for a Digital Stethoscope |last=Miller |first=Justin |year=2010 |publisher=University of Southern Queensland}}</ref> and papers.<ref>{{cite journal |last1=LÍŠKA |first1=Ondrej |last2=ŽIDEK |first2=Kamil |year=2010 |title=Accelerometers usability for danger tilt off-highway vehicles and signal filtration with kalman filter |journal=Journal of applied science in the thermodynamics and fluid mechanics |volume=4 |issue=2 |pages=1–6 |issn=1802-9388}}</ref><ref>{{cite journal |last1=Krejcar |first1=Ondrej |last2=Jirka |first2=Jakub |last3=Janckulik |first3=Dalibor |year=2011 |title=Use of Mobile Phones as Intelligent Sensors for Sound Input Analysis and Sleep State Detection |journal=Sensors |issue=11 |pages=6037–6055 |doi=10.3390/s110606037 |issn=1424-8220}}</ref>\n\n== Features ==\nThe software library provides facilities for:\n* Probability distributions: discrete, continuous and multivariate.\n* Pseudo-random number generation, including [[Mersenne twister|Mersenne Twister MT19937]].\n* Real and complex linear algebra types and solvers with support for sparse matrices and vectors.\n* [[LU decomposition|LU]], [[QR decomposition|QR]], [[Singular value decomposition|SVD]], [[Eigendecomposition of a matrix|EVD]], and [[Cholesky decomposition|Cholesky]] decompositions.\n* Matrix IO classes that read and write matrices from/to Matlab and delimited files.\n* Complex number arithmetic and trigonometry.\n* “Special” routines including the [[Gamma function|Gamma]], [[Beta function|Beta]], [[Error function|Erf]], modified [[Bessel function|Bessel]] and [[Struve function|Struve]] functions.\n* Interpolation routines, including [[Lagrange polynomial#Barycentric interpolation|Barycentric]], Floater-Hormann.\n* Linear Regression/Curve Fitting routines.\n* Numerical Quadrature/Integration.\n* Root finding methods, including Brent, Robust Newton-Raphson and Broyden.\n* Descriptive Statistics, Order Statistics, Histogram, and [[Pearson product-moment correlation coefficient|Pearson Correlation Coefficient]].\n* [[Markov chain Monte Carlo]] sampling.\n* Basic financial statistics.\n* Fourier and Hartley transforms (FFT).\n* Overloaded mathematical operators to simplify complex expressions.\n* Runs under Microsoft Windows and platforms that support Mono.\n* Optional support for Intel Math Kernel Library (Microsoft Windows and Linux)\n* Optional F# extensions for more idiomatic usage.\n\n== See also ==\n* [[List of numerical analysis software]]\n* [[List of numerical libraries]]\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [http://numerics.mathdotnet.com Math.NET Numerics Website]\n* [http://www.mathdotnet.com Math.NET Initiative]\n\n{{DEFAULTSORT:Mathnetnumerics}}\n[[Category:C Sharp libraries]]\n[[Category:Numerical software]]"
    },
    {
      "title": "Matrix Toolkit Java",
      "url": "https://en.wikipedia.org/wiki/Matrix_Toolkit_Java",
      "text": "{{multiple issues|\n{{Notability|Products|date=November 2012}}\n{{one source|date=November 2012}}\n{{primary sources|date=November 2012}}\n}}\n{{Infobox software\n| author                 = Bjørn-Ove Heimsund\n| released               = {{Start date and age|2003}}\n| latest preview version = 0.9.14\n| latest preview date    = {{release date|2011|11|22}}\n| repo                   = {{url|https://github.com/hwinkler/matrix-toolkits-java}}\n| programming language   = [[Java (programming language)|Java]]\n| genre                  = [[Application programming interface]] (API)\n| license                = [[LGPL]]\n}}\n'''Matrix Toolkit Java''' (MTJ) is an [[open-source software|open-source]] [[Java (programming language)|Java]] [[software library]] for performing numerical [[linear algebra]].  The library contains a full set of standard linear algebra operations for [[Density matrix|dense matrices]] based on [[BLAS]] and [[LAPACK]] code.  Partial set of [[Sparse matrix|sparse]] operations is provided through the Templates project.  The library can be configured to run as a pure Java library or use BLAS machine-optimized code through the [[Java Native Interface]].\n\nMTJ was originally developed by Bjørn-Ove Heimsund, who has taken a step back due to other commitments.  The project webpage states that \"(The new maintainers are) primarily concerned with keeping the library maintained, and fixing bugs as they are discovered. There is no road plan for future releases\".<ref name=\"ProjectPage\"/>\n\nSeveral citations for MTJ can be found in scientific literature, including <ref name=\"McMahan2005\"/> which uses its LU preconditioner.  Performance of MTJ has been compared to other libraries, which can be found at Java Matrix Benchmark's site.<ref name=\"JMatBench\"/>\n\n==Capabilities==\nThe following is an overview of MTJ's capabilities, as listed on the project's website:\n\n* Datastructures for dense and structured sparse matrices in the following formats:\n** Dense, column major.\n** Banded matrices, which store only a few diagonals.\n** Packed matrices, storing only half the matrices (for triangular or symmetric matrices).\n** Tridiagonal and symmetric tridiagonal matrices. \n* Transparent support for symmetric and triangular storage.\n* Datastructures for unstructured sparse matrices in these formats:\n** Compressed row or column storage (CRS/CCS).\n** Flexible CRS/CCS, using growable sparse vectors.\n** Compressed diagonal storage (CDS). \n* The dense and structured sparse matrices are built on top of BLAS and LAPACK, and include the following intrinsic operations:\n** Matrix/vector multiplication.\n** Matrix/matrix multiplication.\n** Rank updates by matrices or vectors.\n** Direct matrix solvers. \n* The unstructured sparse matrices supports the same operations as the structured ones, except they do not have direct solvers. However, their matrix/vector multiplication methods are optimised for use in iterative solvers.\n* Matrix decompositions of dense and structured sparse matrices:\n** LU and Cholesky.\n** Eigenvalue decompositions for unsymmetrical dense matrices.\n** Singular value decompositions for unsymmetrical dense matrices.\n** Eigenvalue decompositions for symmetrical matrices (tridiagonal, banded, packed and dense).\n** [[Orthogonal matrix]] decompositions for dense matrices (QR, RQ, LQ, and QL). \n* Iterative solvers for unstructured sparse matrices from the Templates project:\n** BiConjugate gradients.\n** BiConjugate gradients stabilized.\n** Conjugate gradients.\n** Conjugate gradients squared.\n** Chebyshev iteration.\n** Generalized minimal residual (GMRES).\n** Iterative refinement (Richardson's method).\n** Quasi-minimal residual. \n* A selection of algebraic preconditioners:\n** Diagonal preconditioning.\n** Symmetrical successive overrelaxation.\n** Incomplete Cholesky.\n** Incomplete LU.\n** Incomplete LU with fill-in using thresholding.\n** Algebraic multigrid by smoothed aggregation.\n\n==Usage Examples==\nExample of Singular Value Decomposition (SVD):\n<source lang=\"java\">\nSVD svd = new SVD(matA.numRows(),matA.numColumns());\nSVD s = svd.factor(matA);\nDenseMatrix U = s.getU();\nDenseMatrix S = s.getS();\nDenseMatrix Vt = s.getVt();\n</source>\n\nExample of matrix multiplication:\n<source lang=\"java\">\nDenseMatrix result = new DenseMatrix(matA.numRows(),matB.numColumns());\nmatA.mult(matB,result);\n</source>\n\n==See also==\n* [[List of numerical analysis software]]\n\n==References==\n{{reflist|refs=\n<ref name=ProjectPage>{{cite web |url=http://code.google.com/p/matrix-toolkits-java/ |title=MTJ Project Page |work=MTJ|accessdate=November 30, 2012}}</ref>\n<ref name=McMahan2005>{{cite conference\n | author = McMahan, H.B.\n |author2=Gordon, G.J.\n | year = 2005\n | title = Fast exact planning in Markov decision processes\n | booktitle = Proc. of the 15th International Conference on Automated Planning and Scheduling (ICAPS-05)\n | url = http://www.cs.cmu.edu/~ggordon/mcmahan-gordon.fast-mdp.pdf\n}}</ref>\n<ref name=JMatBench>{{cite web |url=https://code.google.com/p/java-matrix-benchmark/ |title=Java Matrix Benchmark | accessdate=December 6, 2012}}</ref>\n}}\n\n[[Category:Numerical software]]"
    },
    {
      "title": "Meta.Numerics",
      "url": "https://en.wikipedia.org/wiki/Meta.Numerics",
      "text": "{{Multiple issues|{{notability|1=Products|date=February 2017}}{{Third-party|date=February 2017}}}}\n\n{{ infobox software\n| name                   = Meta.Numerics\n| latest_release_version = 3.1\n| latest_release_date    = 26 Dec 2016\n| programming language   = [[C Sharp (programming language)|C#]]\n| operating system       = [[.NET Framework]]\n| genre                  = [[List of numerical analysis software|Numerical library]]\n| license                = [[Microsoft Public License]]\n| website                = http://www.meta-numerics.net/\n}}\n\n'''Meta.Numerics''' is an [[open-source software|open-source]] library for advanced scientific computing on the [[Microsoft .NET|.NET]] platform. It provides an object-oriented [[Application Programming Interface|API]] supporting advanced functions, matrix algebra, statistics, optimization, and other numerical algorithms.<ref>{{cite web|url=http://www.meta-numerics.net/Pages/Specs.aspx |title=Meta.Numerics Features |publisher=meta-numerics.net |accessdate=2017-02-10}}</ref>\n\n== History ==\nVersion 1.0 was released in April 2009.<ref>{{cite web|url=http://metanumerics.codeplex.com/releases/view/25535 |title=Meta.Numerics Version 1.0 |publisher=CodePlex.com |accessdate=2017-02-10}}</ref> The current version 3.1 was released in December 2016.<ref>{{cite web|url=http://metanumerics.codeplex.com/releases/view/630182 |title=Meta.Numerics Version 3.1 |publisher=CodePlex.com |accessdate=2017-02-10}}</ref> It has been used in academic research and software development.<ref>Savio, et al., \"Computation of Femoral Canine Morphometric Parameters in Three-Dimensional Geometrical Models\", Veterinary Surgery 45 (2016) 987</ref><ref>Story, D. and Kacira, M. (2014). AUTOMATED MACHINE VISION GUIDED PLANT MONITORING SYSTEM FOR GREENHOUSE CROP DIAGNOSTICS. Acta Hortic. 1037, 635-641 DOI: 10.17660/ActaHortic.2014.1037.81</ref>  It is listed in the software index of the [[Digital Library of Mathematical Functions]].<ref>{{cite web|url=http://dlmf.nist.gov/software/ |title=Software Index |publisher=National Institute of Standards and Technology |accessdate=2017-02-12}}</ref>\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [http://www.meta-numerics.net/ Meta.Numerics Website]\n* [http://metanumerics.codeplex.com/ Meta.Numerics Project on CodePlex]\n\n[[Category:Numerical software]]\n[[Category:C Sharp libraries]]"
    },
    {
      "title": "MINPACK",
      "url": "https://en.wikipedia.org/wiki/MINPACK",
      "text": "'''MINPACK''' is a library of [[FORTRAN]] subroutines for the solving of systems of nonlinear equations, or the [[least squares]] minimization of the residual of a set of [[System of linear equations|linear]] or nonlinear equations.\n\nMINPACK, along with other similar libraries such as [[LINPACK]] and [[EISPACK]] originated from the Mathematics and Computer Science Division Software (MCS) of [[Argonne National Laboratory]] Written by [[Jorge J. Moré|Jorge Moré]], [[Burt Garbow]], and [[Ken Hillstrom]] MINPACK is free and designed to be highly [[Software portability|portable]], robust and reliable. The quality of its implementation of [[Levenberg-Marquardt]] is attested by Dennis and Schnabel.<ref>{{cite book |first1=J. E. |last1=Dennis |first2=Robert B. |last2=Schnabel |year=1996 |title=Numerical Methods for Unconstrained Optimization and Nonlinear Equations |publisher=[[Society for Industrial and Applied Mathematics]] |pages=228, 233, 265|url=https://books.google.com/books?hl=en&lr=&id=ksvJTtJCx9cC&oi=fnd&pg=PR1&dq=%22Numerical+Methods+for+Unconstrained+Optimization+and+Nonlinear+Equations%22&ots=BJoJEL3Kzq&sig=NOD-_n06CfJGqICGeR81IHP1NWE}}</ref>\n\nFive algorithmic paths each include a core subroutine and a driver routine. The algorithms proceed either from an analytic specification of the [[Jacobian matrix]] or directly from the problem functions. The paths include facilities for systems of equations with a [[band matrix|banded]] Jacobian matrix, for least squares problems with a large amount of data, and for checking the consistency of the Jacobian matrix with the functions. \n\n==References==\n* J. J. Moré, B. S. Garbow, and K. E. Hillstrom, [http://cds.cern.ch/record/126569/files/CM-P00068642.pdf User Guide for MINPACK-1], Argonne National Laboratory Report ANL-80-74, Argonne, Ill., 1980. \n* J. J. Moré, D. C. Sorensen, K. E. Hillstrom, and B. S. Garbow, The MINPACK Project, in Sources and Development of Mathematical Software, W. J. Cowell, ed., Prentice-Hall,pages 88-111, 1984.\n\n{{reflist}}\n\n==External links==\n* [http://www.netlib.org/minpack/ Netlib download site].\n* [http://www.mcs.anl.gov/~more/ANL8074a.pdf User Guide for MINPACK-1, Chapters 1 to 3], from J. J. Moré website\n* [http://www.mcs.anl.gov/~more/ANL8074b.pdf User Guide for MINPACK-1, Chapter 4], from J. J. Moré website\n\n[[Category:Fortran libraries]]\n[[Category:Numerical software]]"
    },
    {
      "title": "MINUIT",
      "url": "https://en.wikipedia.org/wiki/MINUIT",
      "text": "'''MINUIT''', now '''MINUIT2''', is a [[numerical analysis|numerical]] [[Optimization (mathematics)|minimization]] [[computer program]] originally written in the [[FORTRAN]] [[programming language]]<ref>{{cite web|url=http://wwwasdoc.web.cern.ch/wwwasdoc/minuit/minmain.html |title=Archived copy |accessdate=2008-08-03 |deadurl=yes |archiveurl=https://web.archive.org/web/20080526130742/http://wwwasdoc.web.cern.ch/wwwasdoc/minuit/minmain.html |archivedate=2008-05-26 |df= }}</ref> by [[CERN]] staff [[physicist]] Fred James in the 1970s. The program searches for minima in a user-defined [[Function (computer science)|function]] with respect to one or more [[parameter]]s using several different methods as specified by the user. The original FORTRAN code was later ported to [[C++]] by the [[ROOT]] project; both the FORTRAN and C++ versions are in use {{As of|2009|alt=today}}. The program is very widely used in [[particle physics]], and hundreds of published papers cite use of MINUIT.<ref>http://inspirehep.net/search?ln=en&p=refersto%3Arecid%3A101965</ref> In the early 2000s Fred James started a project to implement MINUIT in C++ using [[object-oriented programming]]. The new MINUIT is an optional package (minuit2) in the ROOT release. {{As of|2014|10}} the latest version is 5.34.14, released on 24 January 2014.<ref>[http://lcgapp.cern.ch/project/cls/work-packages/mathlibs/minuit/index.html Home page for new C++ MINUIT]</ref> There is also a [[Java (programming language)|Java]] port<ref>[http://java.freehep.org/freehep-jminuit/project-summary.html Java port of MINUIT project]</ref> as well as several [[Python (programming language)|Python]] ports.<ref>[http://code.google.com/p/pyminuit/ PyMinuit - Python port of MINUIT project]</ref><ref>[https://github.com/iminuit/iminuit IMinuit - Interactive IPython Friendly Mimizer based on SEAL Minuit]</ref>\n\nMINUIT is not a program that can be distributed as an [[executable]] binary to be run by a relatively unskilled user: the user must write and [[Compiler|compile]] a subroutine defining the function to be optimized, and oversee the optimization process.\n\n==References==\n{{Reflist}}\n\n==External links==\n* [http://inspirehep.net/record/101965 MINUIT paper]\n\n[[Category:Fortran software]]\n[[Category:Numerical software]]"
    },
    {
      "title": "MPrime",
      "url": "https://en.wikipedia.org/wiki/MPrime",
      "text": "#REDIRECT [[Prime95#Linux version]]\n\n{{Redirect category shell|1=\n{{R from merge}}\n}}\n\n{{DEFAULTSORT:Mprime}}\n[[Category:Distributed computing projects]]\n[[Category:Primality tests]]\n[[Category:Science software for Linux]]\n[[Category:Numerical software]]\n\n[[de:MPrime]]"
    },
    {
      "title": "MPSolve",
      "url": "https://en.wikipedia.org/wiki/MPSolve",
      "text": "{{Infobox Software\n| name                   = MPSolve\n| logo                   = \n| screenshot             = \n| caption                = \n| collapsible            = \n| author                 = Dario Bini, Giuseppe Fiorentino, and Leonardo Robol\n| developer              = \n| released               = \n| latest release version = Version 3.1.5\n| latest release date    = April 2017\n| latest preview version = \n| latest preview date    = \n| frequently updated     = \n| programming language   = [[C (programming language)|C]]\n| operating system       = Linux, Windows, Mac OS X\n| platform               = PC\n| size                   = \n| language               = English\n| status                 = \n| genre                  = mathematical software\n| license                = [[GPLv3]]  \n| website                = [http://numpi.dm.unipi.it/software/mpsolve  home page]\n}}\n\n'''MPSolve''' ('''Multiprecision Polynomial Solver''') is a package  for the [[Root-finding algorithm|approximation of the roots]] of a [[polynomial|univariate polynomial]]. It uses the [[Aberth method]].<ref>\"Design, Analysis, and Implementation of a Multiprecision Polynomial Rootfinder\" by D. A. Bini and G. Fiorentino published in Numerical Algorithms, Volume 23 (2000), pages 127-173</ref> together with a careful use of multiprecision.<ref>\"Solving secular and polynomial equations: A multiprecision algorithm\" by D. A. Bini and L. Robol published in Journal of Computational and Applied Mathematics, Volume 272 (2015)</ref>\n\n\"Mpsolve takes advantage of [[Sparse matrix|sparsity]], and has special [[hooking|hooks]] for polynomials that can be evaluated efficiently by [[straight-line program]]s\"<ref>{{Cite web |url=http://cm.bell-labs.com/who/sjf/eigensolveperformance.html |title=Comparison of performance of MPSolve and Eigensolve by Steven Fortune |access-date=2008-04-05 |archive-url=https://web.archive.org/web/20070815232226/http://cm.bell-labs.com/who/sjf/eigensolveperformance.html |archive-date=2007-08-15 |dead-url=yes |df= }}</ref>\n\n==Implementation==\nThe program is written mostly in [[C (programming language)|ANSI C]] and  makes use of the [[GNU Multi-Precision Library]]. It uses a [[command line interface]] (CLI) and, \nstarting from version 3.1.0 has also a [[Graphical user interface|GUI]] and interfaces for [[MATLAB]] and [[GNU Octave|GNU/Octave]].\n\n==Usage==\nThe executable program of the package is called '''mpsolve'''. It can be [[execution (computers)|run]] from command line in [[terminal emulator|console]]. The executable file for the graphical user interface is called '''xmpsolve''', and the MATLAB and Octave functions are called '''mps_roots'''. They behave similarly to the function '''roots''' that is already included in these software packages.\n\n==Output ==\nTypically output will be on the screen.  It may also be saved as a text file (with res [[Filename extension|extension]]) and plotted in [[gnuplot]]. Direct plotting in [[gnuplot]] is also supported on Unix systems.\n\n[[Image:mcenter10.jpg|This file shows centers of hyperbolic components of [[mandelbrot set]] for period 10 ( and its [[divisor]]s). It is made with [[gnuplot]]. Centers are computed with MPSolve.]]\n\n==References==\n{{Reflist}}\n\n==External links==\n{{Wikibooks|MPSolve}}\n* [http://numpi.dm.unipi.it/software/mpsolve Home page]\n\n[[Category:C software]]\n[[Category:Numerical software]]"
    },
    {
      "title": "MUMPS (software)",
      "url": "https://en.wikipedia.org/wiki/MUMPS_%28software%29",
      "text": "{{about||the programming language of the same name|MUMPS|other uses of the word MUMPS|Mumps (disambiguation)}}\n{{Infobox software\n| name                       = MUMPS\n| logo                       =\n| screenshot                 =\n| caption                    =\n| collapsible                =\n| author                     =\n| developer                  =\n| released                   =\n| latest release version     = 5.2.0\n| latest release date        = {{release date|2019|04}}\n| latest preview version     =\n| latest preview date        =\n| programming language       = [[C (programming language)|C]], [[Fortran 90]]\n| operating system           = [[Unix-like]] and [[Windows]] (through [http://sourceforge.net/projects/winmumps/ WinMUMPS])\n| size                       =\n| status                     =\n| genre                      =\n| license                    = [[CeCILL-C]]\n| website                    = {{URL|mumps.enseeiht.fr}}\n}}\n\n'''MUMPS''' ('''MU'''ltifrontal '''M'''assively '''P'''arallel sparse direct '''S'''olver) is a software application for the solution of large [[sparse matrix|sparse]] systems of [[linear]] [[algebra]]ic [[equation]]s on [[distributed memory]] [[parallel computer]]s. It was developed in European project PARASOL (1996–1999) by [[CERFACS]], [[IRIT]]-[[ENSEEIHT]] and [[Rutherford Appleton Laboratory|RAL]]. The software implements the [[multifrontal method]], which is a version of [[Gaussian elimination]] for large sparse systems of equations, especially those arising from the [[finite element method]]. It is written in [[Fortran 90]] with parallelism by [[Message Passing Interface|MPI]] and it uses [[BLAS]] and [[ScaLAPACK]] kernels for [[dense matrix]] computations.\nSince 1999, MUMPS has been supported by CERFACS, [[IRIT]]-[[ENSEEIHT]], and [[INRIA]].\n\nThe importance of MUMPS lies in the fact that it is a supported [[free software|free]] implementation of the multifrontal method.\n\n== References ==\n* {{cite journal|doi=10.1016/S0045-7825(99)00242-X|id= [http://graal.ens-lyon.fr/MUMPS/index.php?page=bibtex#MUMPS:1 BibteX entry]|title= Multifrontal parallel distributed symmetric and unsymmetric solvers|journal= Computer Methods in Applied Mechanics and Engineering|volume= 184|issue= 2–4|pages= 501–520|year= 2000|last1= Amestoy|first1= P.R.|last2= Duff|first2= I.S.|last3= l'Excellent|first3= J.-Y.|bibcode= 2000CMAME.184..501A|citeseerx= 10.1.1.56.5118}}\n* {{Cite journal|doi=10.1137/S0895479899358194 |id= [http://graal.ens-lyon.fr/MUMPS/index.php?page=bibtex#MUMPS:2 BibteX entry]|title=A fully asynchronous multifrontal solver using distributed dynamic scheduling|journal= SIAM Journal on Matrix Analysis and Applications|volume= 23|issue=1|pages=15–41 |year=2001|last1= Amestoy|first1= Patrick R.|last2= Duff|first2= Iain S.|last3= l'Excellent|first3= Jean-Yves|last4= Koster|first4= Jacko|citeseerx= 10.1.1.40.4181}}\n* {{Cite journal|doi=10.1016/j.parco.2005.07.004|id=[http://graal.ens-lyon.fr/MUMPS/index.php?page=bibtex#MUMPS:3 BibteX entry]|title=Hybrid scheduling for the parallel solution of linear systems|journal=Parallel Computing|volume=32|issue=2|pages=136–156|year=2006|last1=Amestoy|first1=Patrick R.|last2=Guermouche|first2=Abdou|last3=l’Excellent|first3=Jean-Yves|last4=Pralet|first4=Stéphane|citeseerx=10.1.1.332.1751}}\n{{refend}}\n\n== External links ==\n* {{Official website|http://mumps.enseeiht.fr/}}\n* [http://sourceforge.net/projects/winmumps/ WinMUMPS, files for compiling MUMPS on Windows]\n\n[[Category:Free software programmed in Fortran]]\n[[Category:Numerical software]]\n[[Category:Public-domain software with source code]]\n\n\n{{free-software-stub}}"
    },
    {
      "title": "Netlib",
      "url": "https://en.wikipedia.org/wiki/Netlib",
      "text": "'''Netlib''' is a repository of software for [[scientific computing]] maintained by [[AT&T Inc.|AT&T]], [[Bell Laboratories]], the [[University of Tennessee]] and [[Oak Ridge National Laboratory]].<ref>[http://www.netlib.org/misc/faq.html#2.1 Netlib FAQ<!-- Bot generated title -->]</ref> Netlib comprises a large number of separate programs and [[library (computing)|libraries]]. Most of the code is written in [[C (programming language)|C]] and [[Fortran]], with some programs in other languages.\n\n==History==\n\nThe project began with [[email]] distribution on [[UUCP]], [[ARPANET]] and [[CSNET]] in the 1980s.<ref>{{Cite journal |title= Distribution of mathematical software via electronic mail |author1= [[Jack J. Dongarra]] |author2= Eric Grosse |journal=  Communications |date= May 1987 |volume= 30 |issue= 5 |pages= 403 |doi= 10.1145/22899.22904 |citeseerx= 10.1.1.73.7679 }}</ref>\n\nThe code base of Netlib was written at a time when computer software was not yet considered merchandise. Therefore, no license terms or terms of use are stated for many programs. Before the [[Berne Convention for the Protection of Literary and Artistic Works|Berne Convention]] Implementation Act of 1988 (and the earlier [[Copyright Act of 1976]]) works without an explicit [[copyright notice]] were [[public domain software]]. Also, most of the Netlib code is [[Copyright status of work by the U.S. government|work of US government employees]] and therefore in the [[public domain]].<ref>{{cite journal|url=http://eprints.maths.manchester.ac.uk/857/01/covered/MIMS_ep2007_123.pdf|title=Netlib and NA-Net: building a scientific computing community |journal=IEEE Annals of the History of Computing |volume=30 |issue=2 |pages=30–41 |authors=Jack Dongarra, Gene Golub, Eric Grosse, Cleve Moler, Keith Moore |quote=''The Netlib software repository was created in 1984 to facilitate quick distribution of public domain software routines for use in scientific computation.'' |doi=10.1109/MAHC.2008.29|accessdate=2018-05-13|year=2008 |citeseerx=10.1.1.102.7201 }}</ref><ref>[https://books.google.de/books?id=2GPNBQAAQBAJ&pg=PA109&lpg=PA109&dq=netlib+public+domain&source=bl&ots=d6Uw8D5JMv&sig=Dw3wXTAQThFKLVrL7pm_qnCaq9s&hl=de&sa=X&ved=0ahUKEwjO05Xxt_XKAhXCwxQKHUBlC5sQ6AEIXjAI#v=onepage&q=netlib%20public%20domain&f=false Classical Fortran: Programming for Engineering and Scientific Applications] ''\"The best known repository of public-domain scientific numerical software is netlib\"'' (2009)</ref> While several packages therefore don't have explicit [[waiver]]/[[anti-copyright]] statements, for instance the [[SLATEC]] package has an explicit statement.\n\n== Contents ==\nSome well-known packages maintained in Netlib are:\n*[[AMPL|AMPL Solver Library]] (ASL)\n*[[Basic Linear Algebra Subprograms]] (BLAS)\n*[[EISPACK]]\n*[[LAPACK]]\n*[[LINPACK]]\n*[[MINPACK]]\n*[[QUADPACK]]\n\nThe [[SLATEC]] package is special in that it comprises a number of other packages like BLAS and LINPACK.\n\n==Other projects==\n*[[GNU Scientific Library]] (GSL), written in C and distributed under the [[GNU General Public License]]\n\n==References==\n{{reflist|30em}}\n\n==External links==\n* [http://www.netlib.org www.netlib.org]\n\n[[Category:Numerical software]]\n[[Category:Public-domain software with source code]]"
    },
    {
      "title": "NetworkX",
      "url": "https://en.wikipedia.org/wiki/NetworkX",
      "text": "{{Infobox software\n| name = NetworkX\n| screenshot = Watts strogatz.svg<!-- Image name is enough. -->\n| screenshot alt = A graph created with NetworkX\n| caption = A graph created with NetworkX\n| author = [[Aric Hagberg]]<br />Pieter Swart<br />Dan Schult\n| developer = Many others\n| released = {{Start date and age|2005|04|11|df=yes}}<ref>[https://mail.python.org/pipermail/python-announce-list/2005-April/003924.html NetworkX first public release (NX-0.2)], From: Aric Hagberg, Date: 12 April 2005, Python-announce-list mailing list</ref><ref>[https://web.archive.org/web/20050429205335/http://sourceforge.net:80/projects/networkx/ NetworkX initial release, NX-0.2], hagberg – 2005-04-11, Project Info – NetworkX, Registered: 2004-10-21, SourceForge.net</ref>\n| latest release version = 2.2<ref>https://networkx.github.io/documentation/stable/release/release_2.2.html</ref>\n| latest release date = {{Start date and age|2018|09|19|df=yes}}\n| operating system = [[Cross-platform]]\n| programming language = [[Python (programming language)|Python]]\n| genre = [[Software library]]\n| license = [[BSD-new|BSD-new license]]\n| website = {{URL|networkx.github.io}}\n}}\n'''NetworkX''' is a [[Python (programming language)|Python]] library for studying [[Graph (discrete mathematics)|graphs]] and [[Network theory|networks]]. NetworkX is [[free software]] released under the [[BSD-new|BSD-new license]].\n\n== Features ==\n* [[Class (computer programming)|Classes]] for graphs and [[Directed graph|digraphs]].\n* Conversion of graphs to and from several formats.\n* Ability to construct [[random graph]]s or construct them incrementally.\n* Ability to find [[Glossary of graph theory#subgraph|subgraphs]], [[Clique (graph theory)|cliques]], [[Degeneracy (graph theory)|k-cores]].\n* Explore [[Glossary of graph theory terms#adjacent|adjacency]], [[Degree (graph theory)|degree]], [[Glossary of graph theory#diameter|diameter]], [[Glossary of graph theory#radius|radius]], [[Glossary of graph theory#center|center]], [[betweenness]], etc.\n* Draw networks in 2D and 3D.\n\n== Suitability ==\nNetworkX is suitable for operation on large real-world graphs: e.g., graphs in excess of 10 million [[Vertex (graph theory)|nodes]] and 100 million [[Glossary of graph theory terms#edge|edges]].<ref>Aric Hagberg, Drew Conway, [https://github.com/drewconway/NetworkX_Intro_Materials/blob/master/2-Why_Do_SNA_with_NX/module_II.pdf \"Hacking social networks using the Python programming language (Module II – Why do SNA in NetworkX)\"], ''Sunbelt 2010: International Network for Social Network Analysis''.</ref> Due to its dependence on a pure-Python \"dictionary of dictionary\" data structure, NetworkX is a reasonably efficient, very [[Scalability|scalable]], highly portable framework for network and [[social network analysis]].<ref>Aric A. Hagberg, Daniel A. Schult, Pieter J. Swart, [http://conference.scipy.org/proceedings/SciPy2008/paper_2/ Exploring Network Structure, Dynamics, and Function using NetworkX], ''Proceedings of the 7th Python in Science conference (SciPy 2008)'', G. Varoquaux, T. Vaught, J. Millman (Eds.), pp. 11–15.</ref>\n\n== Integration ==\nNetworkX is integrated into [[SageMath]].<ref>http://sagemath.org/links-components.html</ref>\n\n== See also ==\n{{Portal|Free and open-source software}}\n* [[Social network analysis software]]\n* [[JGraph]]\n\n== References ==\n{{Reflist}}\n\n==External links==\n* Official website: {{URL|networkx.github.io}}\n* [https://groups.google.com/forum/#!forum/networkx-discuss NetworkX discussion group]\n* [https://wiki.sagemath.org/graph_survey Survey of existing graph theory software]\n* [https://stackoverflow.com/questions/tagged/networkx NetworkX on StackOverflow]\n* {{github|networkx/networkx}}\n\n{{Graph Analysis Software}}\n\n{{DEFAULTSORT:Networkx}}\n[[Category:Free mathematics software]]\n[[Category:Free software programmed in Python]]\n[[Category:Graph drawing software]]\n[[Category:Numerical software]]\n[[Category:Software using the BSD license]]"
    },
    {
      "title": "NewPGen",
      "url": "https://en.wikipedia.org/wiki/NewPGen",
      "text": "{{refimprove|date=September 2014}}\n\n'''NewPGen''' is a program used by researchers looking for large [[prime numbers]]. It is a program that is used to rapidly presieve a set of candidate numbers, removing those that are definitely composite numbers. It does this by removing those numbers divisible by 3, 5, etc., but since it works with a large set of numbers (and uses some clever [[algorithms]] and fast implementations), it is a lot better than performing trial division on each number in the set. Those numbers which get through this sieve would then be tested with a primality proving program to find the primes. \n\nNewPGen has been used to make many discoveries in the realm of prime numbers, for example the largest known [[twin prime]]s, and nearly all of the known [[cunningham chain]]s. It is written and maintained by Paul Jobling.\n\n==External links==\n*[http://primes.utm.edu/programs/NewPGen/ The Prime Pages: ''NewPGen'']\n\n==See also==\n*[[Sieve of Eratosthenes]]\n\n[[Category:Numerical software]]\n[[Category:Primality tests]]"
    },
    {
      "title": "NSMB (mathematics)",
      "url": "https://en.wikipedia.org/wiki/NSMB_%28mathematics%29",
      "text": "'''NSMB''' is a computer system for solving [[Navier–Stokes equations]] using the [[finite volume method]]. It supports meshes built of several blocks (multi-blocks) and supports [[parallel computing|parallelisation]]. The name stands for \"Navier–Stokes multi-block\". It was developed by a consortium of European scientific institutions and companies, between 1992 and 2003.<ref>{{citation|doi=10.2514/6.1998-225|contribution=Recent advances in aerodynamics inside the NSMB (Navier Stokes Multi Block) consortium|first1=J.|last1=Vos|first2=A.|last2=Rizzi|first3=A.|last3=Corjon|first4=E.|last4=Chaput|first5=E.|last5=Soinne|title=36th AIAA Aerospace Sciences Meeting and Exhibit|year=1998}}.</ref><ref>{{citation|doi=10.2514/6.2010-4366|contribution=Virtual Aircraft Design of TransCruiser - Computing Break Points in Pitch Moment Curve|first1=Peter|last1=Eliasson|first2=Jan|last2=Vos|first3=Andrea|last3=Da Ronch|first4=Mengmeng|last4=Zhang|first5=Arthur|last5=Rizzi|title=28th AIAA Applied Aerodynamics Conference|year=2010}}.</ref>\n\n==References==\n{{reflist}}\n\n[[Category:Numerical software]]\n\n\n{{applied-math-stub}}"
    },
    {
      "title": "Numerical Recipes",
      "url": "https://en.wikipedia.org/wiki/Numerical_Recipes",
      "text": "{{italic title}}\n{{Infobox book series\n| name             = Numerical Recipes: The Art of Scientific Computing\n| image            =  NumericalRecipes3rdEdCover.jpg\n| image_caption    = Cover of the third (C++) edition\n| books            = \n| author           = [[William H. Press]], [[Saul A. Teukolsky]], [[William T. Vetterling]] and [[Brian P. Flannery]]\n| editors          =\n| title_orig       = \n| translator       = \n| illustrator      = \n| cover_artist     = \n| country          = \n| language         = English\n| discipline       = [[Numerical analysis]]\n| publisher        = [[Cambridge University Press]]\n| pub_date         = \n| english_pub_date = \n| media_type       = \n| number_of_books  = \n| list_books       = \n| oclc             = \n| preceded by      = \n| followed by      = \n| website          = {{URL|http://numerical.recipes/}}\n}}\n'''''Numerical Recipes''''' is the generic title of a series of books on [[algorithm]]s and [[numerical analysis]] by [[William H. Press]], [[Saul A. Teukolsky]], William T. Vetterling and [[Brian P. Flannery]]. In various editions, the books have been in print since 1986.  The most recent edition was published in 2007.  In 2015 Numerical Recipes sold its historic two-letter domain name nr.com<ref>{{cite web|url=http://domaingang.com/domain-news/two-letter-domain-nr-com-sold-rebrands-to-numerical-recipes/|title=Two letter domain NR.com sold : Rebrands to Numerical.Recipes|date=14 October 2015|publisher=}}</ref> and became <code>numerical.recipes</code> instead.\n\n==Overview==\nThe ''Numerical Recipes'' books cover a range of topics that include both classical [[numerical analysis]] ([[interpolation]], [[Numerical integration|integration]], [[linear algebra]], [[differential equations]], and so on), signal processing ([[Fast Fourier transform|Fourier methods]], [[Digital filter|filtering]]), statistical treatment of data, and a few topics in machine learning ([[hidden Markov model]], [[support vector machines]]).  The writing style is accessible and has an informal tone.  The emphasis is on understanding the underlying basics of techniques, not on the refinements that may, in practice, be needed to achieve optimal performance and reliability.  Few results are proved with any degree of rigor, although the ideas behind proofs are often sketched, and references are given. Importantly, virtually all methods that are discussed are also implemented in a [[programming language]], with the code printed in the book. Each version is keyed to a specific language.\n\nAccording to the publisher, [[Cambridge University Press]], the ''Numerical Recipes'' books are historically the all-time best-selling books on scientific programming methods.  In recent years, ''Numerical Recipes'' books have been cited in the scientific literature more than 3000 times per year according to [[ISI Web of Knowledge]] (e.g., 3962 times in the year 2008).<ref>[[Thomson Reuters]], [http://www.isiwebofknowledge.com/ Web of Knowledge], Cited Reference Search.</ref> And as of the end of 2017, the book had over 44000 citations on [[Google Scholar]].<ref>[https://scholar.google.co.uk/scholar?cites=9526883588545811444&as_sdt=2005&sciodt=0,5&hl=en], Google Scholar</ref>\n\n==History==\nThe first publication was in 1986 with the title,”Numerical Recipes, The Art of Scientific Computing,” containing code in both Fortran and Pascal; an accompanying book, “Numerical Recipes Example Book (Pascal) was first published in 1985. (A preface note in “Examples\" mentions that the main book was also published in 1985, but the official note in that book says 1986.) Supplemental editions followed with code in Pascal, BASIC, and C. ''Numerical Recipes'' took, from the start, an opinionated editorial position at odds with the conventional wisdom of the numerical analysis community:\n\n{{cquote|If there is a single dominant theme in this book, it is that practical methods of numerical computation can be simultaneously efficient, clever, and — important — clear.  The alternative viewpoint, that efficient computational methods must necessarily be so arcane and complex as to be useful only in \"black box\" form, we firmly reject.<ref>{{Cite book \n|last1=Press |first1=William H. \n|last2=Teukolsky |first2=Saul A. \n|last3=Vetterling |first3=William T. \n|last4=Flannery |first4=Brian P. \n|year=1986 |title=Numerical Recipes: The Art of Scientific Computing \n|publisher=Cambridge University Press \n|publication-place=New York \n|isbn=0-521-30811-9 |chapter=Preface |page=xi }}</ref>}}\n\nHowever, as it turned out, the 1980s were fertile years for the \"black box\" side, yielding important libraries such as [[BLAS]] and [[LAPACK]], and integrated environments like [[MATLAB]] and [[Mathematica]]. By the early 1990s, when Second Edition versions of ''Numerical Recipes'' (with code in C, Fortran-77, and Fortran-90) were published, it was clear that the constituency for ''Numerical Recipes'' was by no means the majority of scientists doing computation, but only that slice that lived ''between'' the more mathematical numerical analysts and the larger community using integrated environments. The Second Edition versions occupied a stable role in this niche environment.<ref name=\"cip\">Press, William H.; and Teukolsky, Saul A.; \"Numerical Recipes: Does This Paradigm Have a Future?,\" Computers in Physics, 11, 416 (1997). [http://numerical.recipes/whp/CIPso97.ps Preprint.]</ref>\n\nBy the mid-2000s, the practice of scientific computing had been radically altered by the mature Internet and Web. Recognizing that their ''Numerical Recipes'' books were increasingly valued more for their explanatory text than for their code examples, the authors significantly expanded the scope of the book, and significantly rewrote a large part of the text. They continued to include code, still printed in the book, now in C++, for every method discussed.<ref>{{Cite book \n|last1=Press |first1=William H. \n|last2=Teukolsky |first2=Saul A. \n|last3=Vetterling |first3=William T. \n|last4=Flannery |first4=Brian P. \n|year=2007 |title=Numerical Recipes: The Art of Scientific Computing |edition=3rd |publisher=Cambridge University Press |publication-place=New York |isbn=978-0-521-88068-8 |chapter=Preface to the Third Edition |page=xi }}</ref> The Third Edition was also released as an electronic book,<ref name=\"ThirdEdition\">{{Cite book \n|last1=Press |first1=William H. \n|last2=Teukolsky |first2=Saul A. \n|last3=Vetterling |first3=William T. \n|last4=Flannery |first4=Brian P. \n|year=2007 |title=Numerical Recipes: The Art of Scientific Computing |edition=3rd |publisher=Cambridge University Press |publication-place=New York |isbn=978-0-521-88068-8 |url=http://apps.nrbook.com/empanel/index.html }}</ref> eventually made available on the Web for free (with limited page views) or by paid or institutional subscription (with unlimited page views).\n\n==Criticism==\nNumerical Recipes as a single volume that covers very broad range of algorithms. Unfortunately that format skewed the choice of algorithms towards simpler and shorter early algorithms which were not as acurate, efficient or stable as later more complex algorithms<ref>{{cite web |last1=Harrop |first1=Jon |title=The Flying Frog Blog: Alternatives to Numerical Recipes |url=http://flyingfrogblog.blogspot.com/2011/03/alternatives-to-numerical-recipes.html |website=The Flying Frog Blog |accessdate=28 January 2019 |date=13 March 2011}}</ref><ref>{{cite web |title=Reviews: Numerical Recipes |url=http://www.quut.com/c/num-recipes-in-c.html |website=www.quut.com |accessdate=28 January 2019}}[https://www.lysator.liu.se/c/num-recipes-in-c.html clone URL]</ref><ref>{{cite web |last1=Van Snyder |first1=W. |title=Why not use Numerical Recipes? |url=http://www.uwyo.edu/buerkle/misc/wnotnr.html |website=www.uwyo.edu |accessdate=28 January 2019}}[https://www.stat.uchicago.edu/~lekheng/courses/302/wnnr/nr.html clone URL]</ref><ref>{{cite web| url=http://math.stanford.edu/~lekheng/courses/302/wnnr/nr-alt.html |title=Alternatives to Numerical Recipes |archive-url=https://web.archive.org/web/20160118122451/http://www.stat.uchicago.edu/~lekheng/courses/302/wnnr/nr-alt.html |archive-date=18 Jan 2016}}</ref>. First edition had also some minor bugs, which were fixed in the latter editions; however according to the authors for years they were encountering on the internet rumors that Numerical Recipes is ''\"full of bugs\"''. They attributed this to people using outdated versions of the code, bugs in other parts of the code and misuse of routines which require some understanding to use correctly.<ref name=nrdr>{{cite web |title=Numerical Recipes Distressing Rumors |url=http://numerical.recipes/bug-rebutt.html |website=numerical.recipes |accessdate=28 January 2019}}</ref>. \n\nThe code listings are copyrighted and commercially licensed by the ''Numerical Recipes'' authors.<ref>Numerical Recipes Web site, [http://numerical.recipes/aboutNR3code.html Numerical Recipes Code]</ref> However, ''Numerical Recipes'' includes the following statement regarding copyrights on computer programs:<blockquote>Copyright does not protect ideas, but only the expression of those ideas in a particular form. In the case of a computer program, the ideas consist of the program's methodology and algorithm, including the necessary sequence of steps adopted by the programmer. The expression of those ideas is the program source code{{nbsp}}... If you analyze the ideas contained in a program, and then express those ideas in your own completely different implementation, then that new program implementation belongs to you.<ref name=\"ThirdEdition\" /></blockquote> \n\nOne early motivation for the [[GNU Scientific Library]] was that a [[free software|free]] library was needed as a substitute for ''Numerical Recipes''.<ref name=gsl>{{cite web |last1=Galassi |first1=Mark |last2=Theiler |first2=James |last3=Gough |first3=Brian |title=GNU Scientific Library -- Design document |url=https://www.gnu.org/software/gsl/design/gsl-design.html |website=GNU Operating System |publisher=GNU.org |accessdate=January 5, 2019}}</ref>\n\nAnother line of criticism centers on the coding style of the books, which strike some modern readers as \"Fortran-ish\", though written in contemporary, object-oriented C++<ref name=gsl/>. The authors have defended their very terse coding style as necessary to the format of the book because of space limitations and for readability.<ref name=\"cip\" />\n\n==Titles in the series (partial list)==\nThe books differ by edition (1st, 2nd, and 3rd) and by the computer language in which the code is given.\n\n* ''Numerical Recipes. The Art of Scientific Computing, 1st Edition'', 1986, {{isbn|0-521-30811-9}}. (FORTRAN and Pascal)\n* ''Numerical Recipes. The Art of Scientific Computing, 3rd Edition'', 2007, {{isbn|0-521-88068-8}}. (C++ code)\n* ''Numerical Recipes in [[C (programming language)|C]]. The Art of Scientific Computing, 2nd Edition'', 1992, {{isbn|0-521-43108-5}}.\n* ''Numerical Recipes in [[Fortran]]. The Art of Scientific Computing, 2nd Edition'', 1992, {{isbn|0-521-43064-X}}.\n* ''Numerical Recipes in Fortran 90. The Art of Parallel Scientific Computing, 2nd Edition'', 1996, {{isbn|0-521-57439-0}}.\n* ''Numerical Recipes in [[Pascal programming language|Pascal]]. The Art of Scientific Computing, 1st Edition'', 1989, {{isbn|0-521-37516-9}}.\n\nThe books are published by [[Cambridge University Press]].\n\n==References==\n{{reflist}}\n\n== External links ==\n* {{official website|http://numerical.recipes/ }}\n* [http://apps.nrbook.com/empanel/index.html Current electronic edition of Numerical Recipes] (limited free page views).\n* {{google books |id=1aAOdzK3FegC}}\n* [http://numerical.recipes/oldverswitcher.html Older versions of Numerical Recipes available electronically] (links to C, Fortran 77, and Fortran 90 versions in various formats, plus other hosted books)\n* [http://www.lysator.liu.se/c/num-recipes-in-c.html Reviews of Numerical Recipes]\n* Norman Gray, [http://www.astro.gla.ac.uk/users/norman/star/sc13/sc13.htx/N-a2b3c1.html A balanced discussion of NR and its critics]\n* {{cite web| url=http://math.stanford.edu/~lekheng/courses/302/wnnr/nr-alt.html |title=Alternatives to Numerical Recipes |archive-url=https://web.archive.org/web/20160118122451/http://www.stat.uchicago.edu/~lekheng/courses/302/wnnr/nr-alt.html |archive-date=18 Jan 2016}}\n\n[[Category:Computer science books]]\n[[Category:Engineering textbooks]]\n[[Category:Mathematics books]]\n[[Category:Numerical software]]"
    },
    {
      "title": "Orange (software)",
      "url": "https://en.wikipedia.org/wiki/Orange_%28software%29",
      "text": "{{Infobox software\n| name = Orange\n| title = Orange\n| logo = Orange-software-logo.png\n| logo caption = \n| logo_size = 200px\n| logo_alt = \n| screenshot = <!-- Image name is enough -->\n| caption = <!-- Orange visual programming, showing explorative data analysis on Iris [[University of California, Irvine|UCI]] data set -->\n| screenshot_size = \n| screenshot_alt = \n| collapsible = \n| author = \n| developer = [[University of Ljubljana]]\n| released = {{Start date and age|1996|10|10|df=yes}}<ref>{{Cite web|url=https://github.com/biolab/orange3/blob/master/CHANGELOG.md#01---1996-10-10|title=orange3/CHANGELOG.md at master . biolab/orange3 . GitHub|last=|first=|date=|website=|archive-url=|archive-date=|dead-url=|access-date=}}</ref>\n| discontinued = \n| latest release version = 3.17.0\n| latest release date = {{Start date and age|2018|10|26}}<ref>{{cite web |title=Orange Change Log |url=https://github.com/biolab/orange3/blob/master/CHANGELOG.md\n}}</ref>\n| latest preview version = \n| latest preview date = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| frequently updated = <!-- DO NOT include this parameter unless you know what it does -->\n| status = Active\n| repo = https://github.com/biolab/orange3\n| programming language = [[Python (programming language)|Python]], [[Cython]], [[C++]], [[C (programming language)|C]]\n| operating system = [[Cross-platform]]\n| platform = \n| size = \n| language = \n| language count = <!-- DO NOT include this parameter unless you know what it does -->\n| language footnote = \n| genre = [[Machine learning]], [[Data mining]], [[Data visualization]], [[Data analysis]]\n| license = [[GPL|GPLv3 or later]]<ref>{{cite web |title=Orange - License|url=https://orange.biolab.si/license/\n}}</ref><ref>{{cite web |title=orange3/LICENSE at master . biolab/orange3 . GitHub|url=https://github.com/biolab/orange3/blob/master/LICENSE\n}}</ref>\n| alexa = \n| website = {{URL|https://orange.biolab.si}}\n| standard = \n| AsOf = \n}}\n\n'''Orange''' is an [[open-source software|open-source]] [[data visualization]], [[machine learning]] and [[data mining]] toolkit. It features a [[visual programming]] front-end for explorative [[data analysis]] and interactive data [[Information visualization|visualization]], and can also be used as a Python library.\n\n[[File:Workflow-Orange3.png|thumb|A typical workflow in Orange 3.]]\n\n== Description ==\n\nOrange is a component-based [[visual programming]] software package for [[data visualization]], [[machine learning]], data mining, and [[data analysis]].\n\nOrange components are called widgets and they range from simple data visualization, subset selection, and preprocessing, to empirical evaluation of learning [[algorithms]]\nand [[predictive modeling]].\n\nVisual programming is implemented through an interface in which workflows are created by linking predefined or user-designed [[Software widget|widget]]s, while advanced users can use Orange as a Python library for data manipulation and widget alteration.<ref>{{cite journal |author1=Janez Demšar |author2=Tomaž Curk |author3=Aleš Erjavec |author4=Črt Gorup |author5=Tomaž Hočevar |author6=Mitar Milutinovič |author7=Martin Možina |author8=Matija Polajnar |author9=Marko Toplak |author10=Anže Starič |author11=Miha Stajdohar |author12=Lan Umek |author13=Lan Žagar |author14=Jure Žbontar |author15=Marinka Žitnik |author16=Blaž Zupan |title=Orange: data mining toolbox in Python |journal=[[Journal of Machine Learning Research|JMLR]] |volume=14 |issue=1 |year=2013 |pages=2349–2353 |url=http://eprints.fri.uni-lj.si/2267/1/2013-Demsar-Orange-JMLR.pdf}}</ref>\n\n== Software ==\n\nOrange is an open-source software package released under [[GPL]]. Versions up to 3.0 include core components in [[C++]] with [[Wrapper function|wrappers]] in Python are available on [[GitHub]]. From version 3.0 onwards, Orange uses common Python open-source libraries for scientific computing, such as [[numpy]], [[scipy]] and [[scikit-learn]], while its graphical user interface operates within the [[cross-platform]] [[Qt (software)|Qt]] framework. Orange3 has its own separate github.\n\nThe default installation includes a number of machine learning, preprocessing and data visualization algorithms in 6 widget sets (data, visualize, classify, regression, evaluate and unsupervised). Additional functionalities are available as add-ons (bioinformatics, data fusion and text-mining).\n\nOrange is supported on [[macOS]], [[Microsoft Windows|Windows]] and [[Linux]] and can also be installed from the Python Package Index repository (''pip install Orange3'').\n\nAs of May 2018 the stable version is 3.13 and runs with Python 3, while the legacy version 2.7 that runs with Python 2.7 is still available.<ref>{{cite web|url=https://orange.biolab.si/download/|title=Orange - Download|publisher=Orange|accessdate=28 May 2018}}</ref>\n\n== Features ==\n\nOrange consists of a canvas [[interface (computing)|interface]] onto which the user places widgets and creates a data analysis workflow. Widgets offer basic functionalities such as reading the data, showing a data table, selecting features, training predictors, comparing learning algorithms, visualizing data elements, etc. The user can interactively explore visualizations or feed the selected subset into other widgets.\n\n[[File:Classification Tree widget in Orange 3.0.PNG|thumb|Classification Tree widget in Orange 3.0]]\n\n*'''Canvas''': graphical front-end for data analysis\n*'''Widgets''':\n**'''Data''': widgets for data input, data filtering, sampling, imputation, feature manipulation and feature selection\n**'''Visualize''': widgets for common visualization (box plot, histograms, scatter plot) and multivariate visualization (mosaic display, sieve diagram).\n**'''Classify''': a set of [[supervised learning|supervised machine learning]] algorithms for classification\n**'''Regression''': a set of supervised machine learning algorithms for regression\n**'''Evaluate''': cross-validation, sampling-based procedures, reliability estimation and scoring of prediction methods\n**'''Unsupervised''': [[unsupervised learning]] algorithms for [[cluster analysis|clustering]] (k-means, hierarchical clustering) and data projection techniques (multidimensional scaling, principal component analysis, correspondence analysis).\n**'''Add-ons''':\n***'''Associate''': widgets for mining frequent itemsets and [[association rule learning]]\n***'''Bioinformatics''': widgets for gene set analysis, [[gene set enrichment|enrichment]], and access to pathway libraries\n***'''Data fusion''': widgets for [[Data fusion|fusing different data sets]], collective matrix factorization, and exploration of latent factors\n***'''Educational''': widgets for teaching machine learning concepts, such as [[k-means clustering]], [[polynomial regression]], [[stochastic gradient descent]], ...\n***'''Geo''': widgets for working with [[Geospatial analysis|geospatial data]]\n***'''Image analytics''': widgets for working with images and [[ImageNet]] [[word embedding|embeddings]]\n***'''Network''': widgets for graph and [[network theory|network analysis]]\n***'''Text mining''': widgets for [[natural language processing]] and [[text mining]]\n***'''Time series''': widgets for [[time series]] analysis and modeling\n***'''Spectroscopy''': widgets for analyzing and visualization of [[Hyperspectral imaging|(hyper)spectral]] datasets <ref>M. Toplak, G. Birarda, S. Read, C. Sandt, S. Rosendahl, L. Vaccari, J. Demšar, F. Borondics, Synchrotron Radiation News 30, 40–45 (2017). https://doi.org/10.1080/08940886.2017.1338424</ref>\n\n[[File:Paint Data - example workflow.png|thumb|Paint Data widget in combination with [[hierarchical clustering]] and k-Means.]]\n\n== Objectives ==\n\nThe program provides a platform for experiment selection, [[recommendation systems]], and predictive modeling and is used in [[biomedicine]], [[bioinformatics]], [[genomics|genomic research]], and teaching. In science, it is used as a platform for testing new machine learning algorithms and for implementing new techniques in [[genetics]] and bioinformatics. In education, it was used for teaching machine learning and data mining methods to students of biology, biomedicine, and informatics.\n\n== Extensions ==\nVarious projects build on Orange either by extending the core components with add-ons or using only the Orange Canvas to exploit the implemented visual programming features and GUI.\n\n* [https://www.aps.anl.gov/Science/Scientific-Software/OASYS OASYS] the ORange SYnchrotron Suite <ref>L. Rebuffi, M. Sanchez del Rio, Proc. SPIE 10388, 103880S (2017). https://doi.org/10.1117/12.2274263</ref>\n* [https://singlecell.biolab.si/ scOrange] for single cell biostatistics\n* [https://quasar.codes/ Quasar] for data analysis in natural sciences\n\n== History ==\n* In 1996, the [[University of Ljubljana]] and [[Jožef Stefan Institute]] started development of ML*, a machine learning framework in [[C++]].\n* In 1997, [[Python (programming language)|Python]] bindings were developed for ML*, which together with emerging Python modules formed a joint framework called Orange.\n* During the following years most major algorithms for data mining and machine learning have been developed either in C++ (Orange's core) or in Python modules.\n* In 2002, first prototypes to create a flexible graphical user interface were designed, using [[PWM (window manager)|Pmw Python megawidgets]].\n* In 2003, graphical user interface was redesigned and re-developed for [[Qt (software)|Qt]] framework using [[PyQt]] Python bindings. The visual programming framework was defined, and development of widgets (graphical components of data analysis pipeline) has begun.\n* In 2005, extensions for data analysis in [[bioinformatics]] was created.\n* In 2008, [[macOS|Mac OS X]] DMG and [[Fink (software)|Fink]]-based installation packages were developed.\n* In 2009, over 100 widgets were created and maintained.\n* From 2009, Orange is in 2.0 beta and web site offers installation packages based on daily compiling cycle.\n* In 2012, new object hierarchy was imposed, replacing the old module-based structure.\n* In 2013, a major GUI redesign.\n* In 2015, Orange 3.0 is released.\n* In 2016, Orange is in version 3.3. The development uses monthly stable release cycle.\n\n==References==\n{{reflist}}\n\n== Further reading ==\n* Demšar, Janez and Blaž Zupan, ''[http://ailab.ijs.si/dunja/TuringSLAIS-2012/Papers/Demsar_Orange.pdf Orange: Data Mining Fruitful and Fun - A Historical Perspective]'', Informatica 37, pgs. 55-60, (2013).\n\n== External links ==\n* {{Official website|https://orange.biolab.si}}\n*[https://www.aps.anl.gov/Science/Scientific-Software/OASYS OASYS website]\n*[https://singlecell.biolab.si/ scOrange website]\n*[https://quasar.codes/ Quasar website]\n\n<!-- Avoid [[WP:Overcategorization]]! -->\n[[Category:Artificial intelligence applications]]\n[[Category:Cross-platform free software]]\n[[Category:Data mining and machine learning software]]\n[[Category:Data visualization software]]\n[[Category:Free plotting software]]\n[[Category:Free science software]]\n[[Category:Free software programmed in Python]]\n[[Category:Numerical software]]\n[[Category:Science software that uses Qt]]\n[[Category:Software using the GPL license]]\n[[Category:Time series software]]"
    },
    {
      "title": "Pipeline Pilot",
      "url": "https://en.wikipedia.org/wiki/Pipeline_Pilot",
      "text": "{{multiple issues|\n{{COI|date=February 2013}}\n{{advert|date=February 2013}}\n{{original research|date=February 2013}}\n}}\n{{Infobox software\n| name                   = Pipeline Pilot\n| title                  = \n| logo                   = <!-- [[File: ]] -->\n| logo caption           = \n| screenshot             = <!-- [[File: ]] -->\n| caption                = \n| collapsible            = \n| author                 = \n| developer              = [[Accelrys]]\n| released               = {{Start date|1999|||df=yes/no}}\n| discontinued           = \n| latest release version = 18.1\n| latest release date    = {{Start date|2018|05||df=yes/no}}\n| latest preview version = \n| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->\n| frequently updated     = <!-- DO NOT include this parameter unless you know what it does -->\n| status                 = \n| programming language   = [[C++]]\n| operating system       = [[Windows]] and [[Linux]]\n| platform               = \n| size                   = \n| language               = \n| language count         = <!-- DO NOT include this parameter unless you know what it does -->\n| language footnote      = \n| genre                  = [[visual programming|Visual]] and [[dataflow programming|dataflow]] programming language\n| license                = [[Proprietary software|Proprietary]]\n| alexa                  = \n| website                = {{URL|http://accelrys.com/products/collaborative-science/biovia-pipeline-pilot/}}\n| standard               = \n| AsOf                   = \n}}\n\n'''Pipeline Pilot''' is a desktop software program developed by [[The Accelrys Enterprise Platform|Accelrys Enterprise Platform]] to process and analyze data. Originally used in the natural sciences, the product's basic ETL ([[Extract, transform, load]]) and analytics capabilities have been broadened. The product is now used for [[data science]], ETL, reporting, prediction and analytics in a number of sectors. The main feature of the product is the ability to design data workflows using a graphical user interface. The program is an example of [[visual programming|visual]] and [[dataflow programming|dataflow]] programming. It has use in a variety of settings, such as  cheminformatics and QSAR,<ref name=\"HassanBrown2007\">{{cite journal\n |last1=Hassan|first1=Moises\n |last2=Brown|first2=Robert D.\n |last3=Varma-O'Brien|first3=Shikha\n |last4=Rogers|first4=David\n |title=Cheminformatics Analysis and Learning in a Data Pipelining Environment\n |journal=ChemInform|volume=38|issue=12|year=2007\n |issn=0931-7597|doi=10.1002/chin.200712278\n}}</ref><ref name=\"HuLounkine2009\">{{cite journal\n |last1=Hu|first1=Ye\n |last2=Lounkine|first2=Eugen\n |last3=Bajorath|first3=Jürgen\n |title=Improving the Search Performance of Extended Connectivity Fingerprints through Activity-Oriented Feature Filtering and Application of a Bit-Density-Dependent Similarity Function\n |journal=ChemMedChem|volume=4|issue=4|year=2009|pages=540–548\n |issn=1860-7179|doi=10.1002/cmdc.200800408\n}}</ref><ref name=\"Warr2012\">{{cite journal\n |last1=Warr\n |first1=Wendy A.\n |title=Scientific workflow systems: Pipeline Pilot and KNIME\n |journal=Journal of Computer-Aided Molecular Design\n |volume=26\n |issue=7\n |year=2012\n |pages=801–804\n |issn=0920-654X\n |doi=10.1007/s10822-012-9577-7\n |pmid=22644661\n |pmc=3414708\n}}</ref> Next Generation Sequencing,<ref>{{cite web\n |url        = http://www.businesswire.com/news/home/20110223005584/en/Accelrys-Enters-Generation-Sequencing-Market-NGS-Collection\n |title      = Accelrys Enters Next Generation Sequencing Market with NGS Collection for Pipeline Pilot\n |publisher  = Business Wire\n |accessdate = 15 February 2013\n}}</ref> image analysis,<ref name=\"RabalLink2010\">{{cite journal\n |last1=Rabal|first1=Obdulia\n |last2=Link|first2=Wolfgang\n |last3=G. Serelde|first3=Beatriz\n |last4=Bischoff|first4=James R.\n |last5=Oyarzabal|first5=Julen\n |title=An integrated one-step system to extract, analyze and annotate all relevant information from image-based cell screening of chemical libraries\n |journal=Molecular BioSystems\n |volume=6\n |issue=4\n |year=2010\n |pages=711–20\n |issn=1742-206X\n |doi=10.1039/b919830j\n |pmid=20237649\n}}</ref><ref name=\"SimeonovPaveley2012\">{{cite journal\n |last1=Paveley|first1=Ross A.\n |last2=Mansour|first2=Nuha R.\n |last3=Hallyburton|first3=Irene\n |last4=Bleicher|first4=Leo S.\n |last5=Benn|first5=Alex E.\n |last6=Mikic|first6=Ivana\n |last7=Guidi|first7=Alessandra\n |last8=Gilbert|first8=Ian H.\n |last9=Hopkins|first9=Andrew L.\n |last10=Bickle|first10=Quentin D.\n |title=Whole Organism High-Content Screening by Label-Free, Image-Based Bayesian Classification for Parasitic Diseases\n |journal=PLoS Neglected Tropical Diseases\n |volume=6\n |issue=7\n |year=2012\n |pages=e1762\n |issn=1935-2735\n |doi=10.1371/journal.pntd.0001762\n}}</ref> and text analytics.<ref>{{cite journal\n |last1=Vellay|first1=SG\n |last2=Latimer|first2=NE\n |last3=Paillard|first3=G\n |title=Interactive text mining with Pipeline Pilot: a bibliographic web-based tool for PubMed\n |journal=Infectious disorders drug targets\n |volume=9\n |issue=3\n |year=2009\n |pages=366–74\n |pmid=19519489\n |doi=10.2174/1871526510909030366\n}}</ref>\n\n== History ==\n\nThe product was created by [[SciTegic]]. [[BIOVIA]] subsequently acquired SciTegic and Pipeline Pilot in 2004. BIOVIA was itself purchased by [[Dassault Systèmes]] in 2014. The product expanded from an initial focus on chemistry to include general extract, transform and load (ETL) capabilities. Beyond the base product, Dassault has added analytical and data processing collections for report generation, data visualization and a number of scientific and engineering sectors. Currently, the product is used for ETL, analytics and machine learning in the chemical, energy, consumer packaged goods, aerospace, automotive and electronics manufacturing industries.\n\n== Overview ==\n\nPipeline Pilot is part of a class of software products that provide user interfaces for manipulating and analyzing data. Pipeline Pilot and similar products allow users with limited or no coding abilities to transform and manipulate datasets. Usually, this is a precursor to conducting analysis of the data. Like other graphical ETL products, it enables users to pull from different data sources, such as CSV files, text files and databases.\n\n=== Components, pipelines, protocols and data records ===\n\nThe [[graphical user interface]], called the Pipeline Pilot Professional Client, allows users to drag and drop discrete data processing units called \"components\". Components can load, filter, join or manipulate data. Components can also perform much more advanced data manipulations, such as building regression models, training neural networks or processing datasets into PDF reports.\n\nPipeline Pilot implements a [[Component-based software engineering|Components]] paradigm. Components are represented as nodes in a workflow. In a mathematical sense, components are modeled as nodes in a [[directed graph]]: \"pipes\" (graph edges) connect components and move data along the from node to node where operations are performed on the data. Users have the choice to use predefined components, or to develop their own. To help in industry-specific applications, such as Next Generation Sequencing (see [[DNA sequencing|High-throughput sequencing (HTS) methods]]), BIOVIA has developed components that greatly reduce the amount of time users need to do common industry-specific tasks.\n\nUsers can choose from components that come pre-installed or create their own components in workflows called \"protocols\". Protocols are sets of linked components. Protocols can be saved, reused and shared. Users can mix and match components that are provided with the software from BIOVIA with their own custom components. Connections between two components are called \"pipes\", and are visualized in the software as two components connected by a pipe. End users design their workflows/protocols, then execute them by running the protocol. Data flows from left to right along the pipes.\n\nModern data analysis and processing can involve a very large number of manipulations and transformations. One major feature of Pipeline Pilot is the ability to visually condense a lengthy series of data manipulations that involve many components. A workflow of any length can be visually condensed into a component that is used in a high level workflow. This means that a protocol can be saved and used as a component in another protocol. In the terminology used in Pipeline Pilot, protocols that are used as components in other protocols are called \"subprotocols\". This allows users to add layers of complexity to their data processing and manipulation workflows, then hide that complexity so they can design the workflow at a higher level of abstraction.\n\n=== Component collections ===\n\nPipeline Pilot features a number of add-ons called \"collections\". Collections are groups of specialized functions like processing genetic information or analyzing polymers offered to end users for an additional licensing fee. Currently, there are a number of these collections.<ref name=\"Collections\">{{cite web|url=http://accelrys.com/products/pipeline-pilot/component-collections.html |title=Pipeline Pilot Component Collections |publisher=Accelrys |accessdate=26 January 2013 |deadurl=yes |archiveurl=https://web.archive.org/web/20130115123007/http://accelrys.com/products/pipeline-pilot/component-collections.html |archivedate=January 15, 2013 }}</ref>\n\n{| class=\"wikitable\"\n|-\n! Group !! Domain !! Component collection\n|-\n| rowspan=\"9\"|Science specific || rowspan=\"3\"|Chemistry || Chemistry \n|-\n| ADMET \n|-\n| Cheminformatics \n|-\n| rowspan=\"4\"|Biology || Gene Expression \n|-\n| Sequence Analysis \n|-\n| Mass Spectrometry for Proteomics \n|-\n| Next Generation Sequencing \n|-\n| rowspan=\"2\"|Materials Modeling & Simulation || Materials Studio \n|-\n| Polymer Properties (Synthia)\n|-\n| rowspan=\"10\"|Generic|| Reporting & Visualization || Reporting \n|-\n| Database & Application Integration || Integration \n|-\n| Imaging || Imaging \n|-\n| rowspan=\"3\"|Analysis & Statistics || Data Modeling \n|-\n| Advanced Data Modeling \n|-\n| R Statistics\n|-\n| rowspan=\"2\"|Document Search & Analysis || Chemical Text Mining\n|-\n| Text Analytics \n|-\n| rowspan=\"2\"|Laboratory|| Plate Data Analytics \n|-\n| Analytical Instrumentation \n|}\n\nGiven the number of different add-ons now offered by BIOVIA, Pipeline Pilot's use cases are very broad and difficult to summarize succinctly. The product has been used in:\n* Predictive maintenance\n* Image analysis, for example the determination of the inhibitory action of a substance on biological processes ([[IC50]]) by calculating the [[dose–response relationship]] directly from information extracted from [[high-content screening]] assay images, associated with dilution in the [[microtitre plate|plate]] layout and chemistry information about the tested compounds (Imaging, Chemistry, Plate Data Analytics)\n* A [[recommender system]] for scientific literature based on a Bayesian model built using [[fingerprint (computing)|fingerprint]] and user's reading list or papers ranking\n* Access to experiment methods and results from [[electronic laboratory notebook]] or [[laboratory information management system]], with resulting reports for resource [[capacity planning]]\n\n=== PilotScript and custom scripts ===\n\nAs with other ETL and analytics solutions, Pipeline Pilot is often used when one or more large (1TB+) and/or complex datasets is processed. In these situations, end users may want to utilize programming scripts that they have written. Early in its development, Pipeline Pilot created a simplified, pared-down scripting language called PilotScript that enabled end users to easily write basic programming scripts that could be incorporated into a Pipeline Pilot protocol. Later releases extended support for a variety of programming languages, including [[Python (programming language)|Python]], [[.NET Framework|.NET]], [[Matlab]], [[Perl]], [[SQL]], [[Java (programming language)|Java]], [[VBScript]] and [[R (programming language)|R]].<ref name=\"Integration\">{{cite web\n |url        = http://accelrys.com/products/datasheets/integration-collection.pdf\n |title      = Pipeline Pilot Integration Component Collection Datasheet\n |publisher  = Accelrys\n |accessdate = 8 February 2013\n}}</ref>\n\nThe syntax for PilotScript is based on [[PLSQL]]. It can be used in components such as the ''Custom Manipulator (PilotScript)'' or the ''Custom Filter (PilotScript)''. As an example, the following script can be used to add a property named \"Hello\" to each record passing through a custom scripting component in a Pipeline Pilot protocol. The value of the property is the string \"Hello World!\".\n\n<source lang=\"plsql\">\nHello := \"Hello World!\";\n</source>\n\nCurrently, the product supports a number of APIs for different programming languages that can be executed without the program's graphical user interface.\n\n== References ==\n\n{{Reflist}}\n\n[[Category:Science software]]\n[[Category:Enterprise application integration]]\n[[Category:Extract, transform, load tools]]\n[[Category:Bioinformatics software]]\n[[Category:Computational chemistry software]]\n[[Category:Computer vision software]]\n[[Category:Data analysis software]]\n[[Category:Data mining and machine learning software]]\n[[Category:Data visualization software]]\n[[Category:Laboratory software]]\n[[Category:Mass spectrometry software]]\n[[Category:Natural language processing software]]\n[[Category:Numerical software]]\n[[Category:Plotting software]]\n[[Category:Proprietary software]]\n[[Category:Visual programming languages]]"
    },
    {
      "title": "PottersWheel",
      "url": "https://en.wikipedia.org/wiki/PottersWheel",
      "text": "{{notability|products|date=September 2011}}\n{{Infobox Software \n| name                   = PottersWheel\n| status                 = Active\n| developer              = TIKANIS GmbH, Freiburg, Germany\n| released               = {{initial-release|October 6, 2006}}\n| latest release version = 4.1.1\n| latest release date    = {{release date and age|mf=yes|2017|05|20}} \n| programming language   = [[MATLAB]], [[C (programming language)|C]] \n| operating system       = [[Microsoft Windows]], [[Mac OS X]], [[Linux]]\n| size                   = 9&nbsp;[[Megabyte|MB]] (250.000 lines)\n| genre                  = [[Mathematical modeling]] \n| license                = Free trial license\n| website                = [http://www.potterswheel.de/ www.potterswheel.de]\n}}\n\n'''PottersWheel''' is a [[MATLAB]] toolbox for mathematical modeling of time-dependent [[dynamical system]]s that can be expressed as [[chemical reaction network]]s or [[ordinary differential equations]] (ODEs).<ref>T. Maiwald and J. Timmer (2008) \"Dynamical Modeling and Multi-Experiment Fitting with PottersWheel\", ''Bioinformatics'' 24(18):2037–2043</ref> It allows the automatic calibration of model parameters by fitting the model to experimental measurements. CPU-intensive functions are written or – in case of model dependent functions – dynamically generated in C. Modeling can be done interactively using graphical user interfaces or based on MATLAB scripts using the PottersWheel function library. The software is intended to support the work of a mathematical modeler as a real [[potter's wheel]] eases the modeling of pottery.\n\n== Seven modeling phases ==\n\nThe basic use of PottersWheel covers seven phases from model creation to the prediction of new\nexperiments.\n\n=== Model creation ===\n\n[[File:Formalization-of-Jak-Stat-Pathway.png]]\n\nThe dynamical system is formalized into a set of reactions or differential equations using a visual model designer or a text editor. The model is stored as a MATLAB *.m ASCII file. Modifications can therefore be tracked using a version control system like [[subversion]] or [[Git (software)|git]]. Model import and export is supported for [[SBML]]. Custom import-templates may be used to import custom model structures. [[Rule-based modeling]] is also supported, where a pattern represents a set of automatically generated reactions.\n\nExample for a simple model definition file for a reaction network A → B → C → A with observed species A and C:\n\n<source lang=matlab>\nfunction m = getModel();\n\n% Starting with an empty model\nm = pwGetEmtptyModel();\n\n% Adding reactions\nm = pwAddR(m, 'A', 'B');\nm = pwAddR(m, 'B', 'C');\nm = pwAddR(m, 'C', 'A');\n\n% Adding observables\nm = pwAddY(m, 'A');\nm = pwAddY(m, 'C');\n</source>\n\n=== Data import ===\n\nExternal data saved in *.xls or *.txt files can be added to a model creating a ''model-data-couple''. A mapping dialog allows to connect data column names to observed species names. Meta information in the data files comprise information about the experimental setting. Measurement errors are either stored in the data files, will be calculated using an error model, or are estimated automatically.\n\n=== Parameter calibration ===\n\nTo fit a model to one or more data sets, the corresponding model-data-couples are combined into a ''fitting-assembly''. Parameters like initial values, rate constants, and scaling factors can be fitted in an experiment-wise or global fashion. The user may select from several numerical integrators, optimization algorithms, and calibration strategies like fitting in normal or logarithmic parameter space.\n\n=== Interpretation of the goodness-of-fit ===\n\n[[File:Fitting-data-with-PottersWheel.png]]\n\nThe quality of a fit is characterized by its ''chi-squared'' value.  As a rule of thumb, for\n''N'' fitted data points and ''p'' calibrated parameters, the chi-squared value should have a similar value\nas ''N''&nbsp;&minus;&nbsp;''p'' or at least&nbsp;''N''. Statistically, this is expressed using a [[chi-squared test]] resulting in a [[p-value]] above a significance threshold of e.g.&nbsp;0.05. For lower p-values, the model is\n\n* either not able to explain the data and has to be refined,\n* the standard deviation of the data points is actually larger than specified,\n* or the used fitting strategy was not successful and the fit was trapped in a local minimum.\n\nApart from further chi-squared based characteristics like [[Akaike information criterion|AIC]] and [[Bayesian information criterion|BIC]], data-model-residual analyses exist, e.g. to investigate whether the [[errors and residuals in statistics|residuals]] follow a [[Gaussian distribution]]. Finally, parameter [[confidence intervals]] may be estimated using either the [[Fisher information matrix]] approximation or based on the [[profile-likelihood function]], if parameters are not unambiguously identifiable.\n\nIf the fit is not acceptable, the model has to be refined and the procedure continues with step 2. Else, the dynamic model properties can be examined and predictions calculated.\n\n=== Model refinement ===\n\nIf the model structure is not able to explain the experimental measurements, a set of physiologically reasonable alternative models should be created. In order to avoid redundant model paragraphs and copy-and-paste errors, this can be done using a common core-model which is the same for all variants.  Then, ''daughter''-models are created and fitted to the data, preferably using batch processing strategies based on MATLAB scripts. As a starting point to envision suitable model variants, the PottersWheel ''equalizer'' may be used to understand the dynamic behavior of the original system.\n\n=== Model analysis and prediction ===\n\nA mathematical model may serve to display the concentration time-profile of unobserved species, to determine sensitive parameters representing potential targets within a clinical setting, or to calculate model characteristics like the half-life of a species.\n\nEach analysis step may be stored into a modeling report, which may be exported as a Latex-based PDF.\n\n=== Experimental design ===\n\nAn experimental setting corresponds to specific characteristics of ''driving input functions'' and\ninitial concentrations. In a signal transduction pathway model the concentration of a ligand\nlike EGF may be controlled experimentally.  The driving input designer allows investigating the effect of a continuous, ramp, or pulse stimulation in combination with varying initial concentrations using the equalizer.  In order to discriminate competing model hypotheses, the designed experiment should have as different observable time-profiles as possible.\n\n== Parameter identifiability ==\n\nMany dynamical systems can only be observed partially, i.e. not all system species are accessible experimentally. For biological applications the amount and quality of experimental data is often limited. In this setting parameters can be structurally or practically non-identifiable. Then, parameters may compensate each other and fitted parameter values strongly depend on initial guesses. In PottersWheel non-identifiability can be detected using the [[Profile Likelihood Approach]].<ref>''Structural and practical identifiability analysis of partially observed dynamical models by exploiting the profile likelihood'', A. Raue, C. Kreutz, T. Maiwald, J. Bachmann, M. Schilling, U. Klingmüller and J. Timmer, Bioinformatics 2009\n</ref> For characterizing functional relationships between the non-identifiable parameters PottersWheel applies random and systematic fit sequences.<ref>''Data-based identifiability analysis of non-linear dynamical models'', S. Hengl, C. Kreutz, J. Timmer and T. Maiwald, Bioinformatics 2007 23(19):2612–2618</ref>\n\n== References ==\n\n{{reflist}}\n\n== External links ==\n\n* {{Official website|http://www.potterswheel.de/}}\n* [https://web.archive.org/web/20111128010157/http://www.fdmold.uni-freiburg.de/~araue/Main/Software Profile Likelihood Approach]\n\n{{Numerical analysis software}}\n\n[[Category:Applied mathematics]]\n[[Category:Cross-platform software]]\n[[Category:Mathematical modeling]]\n[[Category:Numerical software]]\n[[Category:Pharmacokinetics]]\n[[Category:Visual programming languages]]\n[[Category:Statistical software]]\n[[Category:Simulation software]]"
    }
  ]
}