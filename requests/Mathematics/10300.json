{
  "pages": [
    {
      "title": "Bochner identity",
      "url": "https://en.wikipedia.org/wiki/Bochner_identity",
      "text": "In [[mathematics]] &mdash; specifically, [[differential geometry]] &mdash; the '''Bochner identity''' is an [[Identity (mathematics)|identity]] concerning [[harmonic map]]s between [[Riemannian manifold]]s. The identity is named after the [[United States|American]] [[mathematician]] [[Salomon Bochner]].\n\n==Statement of the result==\nLet ''M'' and ''N'' be [[Riemannian manifold]]s and let ''u''&nbsp;:&nbsp;''M''&nbsp;→&nbsp;''N'' be a harmonic map. Let d''u'' denote the derivative (pushforward) of ''u'', ∇ the [[gradient]], Δ the [[Laplace–Beltrami operator]], Riem<sub>''N''</sub> the [[Riemann curvature tensor]] on ''N'' and Ric<sub>''M''</sub> the [[Ricci curvature tensor]] on ''M''. Then\n\n:<math>\\frac12 \\Delta \\big( | \\nabla u |^{2} \\big) = \\big| \\nabla ( \\mathrm{d} u ) \\big|^{2} + \\big\\langle \\mathrm{Ric}_{M} \\nabla u, \\nabla u \\big\\rangle - \\big\\langle \\mathrm{Riem}_{N} (u) (\\nabla u, \\nabla u) \\nabla u, \\nabla u \\big\\rangle.</math>\n\n==See also==\n*[[Bochner's formula]]\n\n==References==\n* {{cite journal\n| last = Eells\n| first = J\n|author2=Lemaire, L.\n| title = A report on harmonic maps\n| journal = Bull. London Math. Soc.\n| volume = 10\n| year = 1978\n| issue = 1\n| pages = 1&ndash;68\n| doi = 10.1112/blms/10.1.1\n| mr = 495450\n}}\n\n==External links==\n* {{MathWorld|urlname=BochnerIdentity|title=Bochner identity}}\n\n[[Category:Differential geometry]]\n[[Category:Mathematical identities]]\n\n\n{{differential-geometry-stub}}"
    },
    {
      "title": "Bochner–Kodaira–Nakano identity",
      "url": "https://en.wikipedia.org/wiki/Bochner%E2%80%93Kodaira%E2%80%93Nakano_identity",
      "text": "In [[mathematics]], the '''Bochner–Kodaira–Nakano identity''' is an analogue of the [[Weitzenböck identity]] for [[hermitian manifold]]s, giving an expression for the [[Antiholomorphic function|antiholomorphic]] Laplacian of a [[vector bundle]] over a hermitian manifold in terms of its complex conjugate and the curvature of the bundle and the torsion of the metric of the manifold. It is named after [[Salomon Bochner]], [[Kunihiko Kodaira]], and [[Shigeo Nakano]].\n\n==References==\n\n*{{Citation | author1-link=Jean-Pierre Demailly | last1=Demailly | first1=Jean-Pierre | title=Séminaire d'analyse P. Lelong-P. Dolbeault-H. Skoda, années 1983/1984 | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Lecture Notes in Math. | doi=10.1007/BFb0077045 |mr=874763 | year=1986 | volume=1198 | chapter=Sur l'identité de Bochner-Kodaira-Nakano en géométrie hermitienne | pages=88–97| isbn=978-3-540-16762-4 }}\n*{{Citation | author1-link=Jean-Pierre Demailly | last1=Demailly | first1=Jean-Pierre | title=  Complex Analytic and Differential Geometry  | url=http://www-fourier.ujf-grenoble.fr/~demailly/manuscripts/agbook.pdf | year=2012}}\n*{{Citation | last1=Kodaira | first1=Kunihiko | authorlink = Kunihiko Kodaira | title=On a differential-geometric method in the theory of analytic stacks |mr=0066693 | year=1953 | journal=[[Proceedings of the National Academy of Sciences|Proceedings of the National Academy of Sciences of the United States of America]] | issn=0027-8424 | volume=39 | issue=12 | pages=1268–1273 | doi=10.1073/pnas.39.12.1268 | pmid=16589409 | jstor=89226 | pmc=1063947}}\n\n{{DEFAULTSORT:Bochner-Kodaira-Nakano identity}}\n[[Category:Theorems in differential geometry]]\n[[Category:Vector bundles]]\n[[Category:Mathematical identities]]\n\n\n{{differential-geometry-stub}}"
    },
    {
      "title": "Cyclotomic identity",
      "url": "https://en.wikipedia.org/wiki/Cyclotomic_identity",
      "text": "In [[mathematics]], the '''cyclotomic identity''' states that\n\n:<math>{1 \\over 1-\\alpha z}=\\prod_{j=1}^\\infty\\left({1 \\over 1-z^j}\\right)^{M(\\alpha,j)}</math>\n\nwhere ''M'' is [[Moreau's necklace-counting function]],\n\n:<math>M(\\alpha,n)={1\\over n}\\sum_{d\\,|\\,n}\\mu\\left({n \\over d}\\right)\\alpha^d,</math>\n\nand ''μ'' is the classic [[Möbius function]] of [[number theory]].  \n\nThe name comes from the denominator, 1&nbsp;&minus;&nbsp;''z''<sup>&nbsp;''j''</sup>, which is the product of [[cyclotomic polynomial]]s.\n\nThe left hand side of the cyclotomic identity is the [[generating function]] for the free associative algebra on α generators, and the right hand side is the generating function for the [[universal enveloping algebra]] of the [[free Lie algebra]] on α generators. The cyclotomic identity witnesses the fact that these two algebras are isomorphic. \n\nThere is also a symmetric generalization of the cyclotomic identity found by Strehl:\n:<math>\\prod_{j=1}^\\infty\\left({1 \\over 1-\\alpha z^j}\\right)^{M(\\beta,j)}=\\prod_{j=1}^\\infty\\left({1 \\over 1-\\beta z^j}\\right)^{M(\\alpha,j)}</math>\n\n==References==\n\n*{{Citation | last1=Metropolis | first1=N. | last2=Rota | first2=Gian-Carlo | author2-link=Gian-Carlo Rota | editor1-last=Greene | editor1-first=Curtis | editor1-link = Curtis Greene | title=Combinatorics and algebra (Boulder, Colo., 1983). Proceedings of the AMS-IMS-SIAM joint summer research conference held at the University of Colorado, Boulder, Colo., June 5–11, 1983. | url=https://books.google.com/books?id=2axt00oBDEwC&pg=PA19 | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Contemp. Math. | isbn=978-0-8218-5029-9  | mr=777692 | year=1984 | volume=34 | chapter=The cyclotomic identity | pages=19–27}}\n\n[[Category:Mathematical identities]]"
    },
    {
      "title": "Degen's eight-square identity",
      "url": "https://en.wikipedia.org/wiki/Degen%27s_eight-square_identity",
      "text": "In [[mathematics]], '''Degen's eight-square identity''' establishes that the product of two numbers, each of which is a sum of eight squares, is itself the sum of eight squares.\nNamely:\n:<math>(a_1^2+a_2^2+a_3^2+a_4^2+a_5^2+a_6^2+a_7^2+a_8^2)(b_1^2+b_2^2+b_3^2+b_4^2+b_5^2+b_6^2+b_7^2+b_8^2)=</math>\n\n::<math>(a_1b_1 - a_2b_2 - a_3b_3 - a_4b_4 - a_5b_5 - a_6b_6 - a_7b_7 - a_8b_8)^2+</math>\n::<math>(a_1b_2 + a_2b_1 + a_3b_4 - a_4b_3 + a_5b_6 - a_6b_5 - a_7b_8 + a_8b_7)^2+</math>\n::<math>(a_1b_3 - a_2b_4 + a_3b_1 + a_4b_2 + a_5b_7 + a_6b_8 - a_7b_5 - a_8b_6)^2+</math>\n::<math>(a_1b_4 + a_2b_3 - a_3b_2 + a_4b_1 + a_5b_8 - a_6b_7 + a_7b_6 - a_8b_5)^2+</math>\n::<math>(a_1b_5 - a_2b_6 - a_3b_7 - a_4b_8 + a_5b_1 + a_6b_2 + a_7b_3 + a_8b_4)^2+</math>\n::<math>(a_1b_6 + a_2b_5 - a_3b_8 + a_4b_7 - a_5b_2 + a_6b_1 - a_7b_4 + a_8b_3)^2+</math>\n::<math>(a_1b_7 + a_2b_8 + a_3b_5 - a_4b_6 - a_5b_3 + a_6b_4 + a_7b_1 - a_8b_2)^2+</math>\n::<math>(a_1b_8 - a_2b_7 + a_3b_6 + a_4b_5 - a_5b_4 - a_6b_3 + a_7b_2 + a_8b_1)^2</math>\n\nFirst discovered by [[Carl Ferdinand Degen]] around 1818, the identity was independently rediscovered by [[John T. Graves|John Thomas Graves]] (1843) and [[Arthur Cayley]] (1845). The latter two derived it while working on an extension of [[quaternion]]s called [[octonion]]s. In [[normed division algebra|algebraic terms]] the identity means that the [[norm (mathematics)|norm]] of product of two octonions equals the product of their norms: <math>\\|ab\\| = \\|a\\|\\|b\\|</math>. Similar statements are true for quaternions ([[Euler's four-square identity]]), complex numbers (the [[Brahmagupta–Fibonacci identity|Brahmagupta–Fibonacci two-square identity]]) and real numbers.  In 1898 [[Adolf Hurwitz]] proved that there is no similar [[bilinear map|bilinear]] identity for 16 squares ([[sedenions]]) or any other number of squares except for 1,2,4, and 8.  However, in the 1960s, H. Zassenhaus, W. Eichhorn, and A. Pfister (independently) showed there can be a non-bilinear identity for [[Pfister's sixteen-square identity|16 squares]]. \n\nNote that each quadrant reduces to a version of [[Euler's four-square identity]]:\n:<math>(a_1^2+a_2^2+a_3^2+a_4^2)(b_1^2+b_2^2+b_3^2+b_4^2)=</math>\n\n::<math>(a_1b_1 - a_2b_2 - a_3b_3 - a_4b_4)^2+</math>\n::<math>(a_1b_2 + a_2b_1 + a_3b_4 - a_4b_3)^2+</math>\n::<math>(a_1b_3 - a_2b_4 + a_3b_1 + a_4b_2)^2+</math>\n::<math>(a_1b_4 + a_2b_3 - a_3b_2 + a_4b_1)^2</math>\n\nand similarly for the other three quadrants. By [[Pfister's theorem]], a different sort of eight-square identity can be given where the <math>z_i</math>, introduced below, are non-bilinear and merely [[rational functions]] of the <math>x_i, y_i</math>. Thus,\n\n:<math>(x_1^2+x_2^2+x_3^2+x_4^2+x_5^2+x_6^2+x_7^2+x_8^2)(y_1^2+y_2^2+y_3^2+y_4^2+y_5^2+y_6^2+y_7^2+y_8^2) = z_1^2+z_2^2+z_3^2+z_4^2+z_5^2+z_6^2+z_7^2+z_8^2</math>\n\nwhere,\n\n::<math>z_1 = x_1 y_1 - x_2 y_2 - x_3 y_3 - x_4 y_4 + u_1 y_5 - u_2 y_6 - u_3 y_7 - u_4 y_8</math>\n::<math>z_2 = x_2 y_1 + x_1 y_2 + x_4 y_3 - x_3 y_4 + u_2 y_5 + u_1 y_6 + u_4 y_7 - u_3 y_8</math>\n::<math>z_3 = x_3 y_1 - x_4 y_2 + x_1 y_3 + x_2 y_4 + u_3 y_5 - u_4 y_6 + u_1 y_7 + u_2 y_8</math>\n::<math>z_4 = x_4 y_1 + x_3 y_2 - x_2 y_3 + x_1 y_4 + u_4 y_5 + u_3 y_6 - u_2 y_7 + u_1 y_8</math>\n::<math>z_5 = x_5 y_1 - x_6 y_2 - x_7 y_3 - x_8 y_4 + x_1 y_5 - x_2 y_6 - x_3 y_7 - x_4 y_8</math>\n::<math>z_6 = x_6 y_1 + x_5 y_2 + x_8 y_3 - x_7 y_4 + x_2 y_5 + x_1 y_6 + x_4 y_7 - x_3 y_8</math>\n::<math>z_7 = x_7 y_1 - x_8 y_2 + x_5 y_3 + x_6 y_4 + x_3 y_5 - x_4 y_6 + x_1 y_7 + x_2 y_8</math>\n::<math>z_8 = x_8 y_1 + x_7 y_2 - x_6 y_3 + x_5 y_4 + x_4 y_5 + x_3 y_6 - x_2 y_7 + x_1 y_8</math>\n\nand,\n\n::<math>u_1 = \\frac{(ax_1^2+x_2^2+x_3^2+x_4^2)x_5 - 2x_1(bx_1 x_5 + x_2 x_6+ x_3 x_7+ x_4 x_8)}{c}</math>\n::<math>u_2 = \\frac{(x_1^2+ax_2^2+x_3^2+x_4^2)x_6 - 2x_2(x_1 x_5 + bx_2 x_6+ x_3 x_7+ x_4 x_8)}{c}</math>\n::<math>u_3 = \\frac{(x_1^2+x_2^2+ax_3^2+x_4^2)x_7 - 2x_3(x_1 x_5 + x_2 x_6+ bx_3 x_7+ x_4 x_8)}{c}</math>\n::<math>u_4 = \\frac{(x_1^2+x_2^2+x_3^2+ax_4^2)x_8 - 2x_4(x_1 x_5 + x_2 x_6+ x_3 x_7+ bx_4 x_8)}{c}</math>\n\nwith,\n\n::<math>a=-1,\\;\\; b=0,\\;\\; c=x_1^2+x_2^2+x_3^2+x_4^2</math>\n\nIncidentally, the <math>u_i</math> obey the identity,\n\n::<math>u_1^2+u_2^2+u_3^2+u_4^2 = x_5^2+x_6^2+x_7^2+x_8^2</math>\n\n== See also ==\n*[[Pfister's sixteen-square identity]]\n*[[Cayley–Dickson construction]]\n*[[Hypercomplex number]]\n*[[Latin square]]\n\n==External links==\n*[http://mathworld.wolfram.com/DegensEight-SquareIdentity.html Degen's eight-square identity] on [[MathWorld]]\n*[https://www.webcitation.org/query?url=http://www.geocities.com/titus_piezas/ramanujan_page8.html&date=2009-10-25+23:07:36 The Degen–Graves–Cayley Eight-Square Identity]\n*[http://sites.google.com/site/tpiezas/0021c Pfister's 16-Square Identity]\n\n[[Category:Analytic number theory]]\n[[Category:Mathematical identities]]\n[[Category:Squares in number theory]]"
    },
    {
      "title": "Difference of two squares",
      "url": "https://en.wikipedia.org/wiki/Difference_of_two_squares",
      "text": "In [[mathematics]], the '''difference of two squares''' is a [[Square (algebra)|squared]] (multiplied by itself) number subtracted from another squared number. Every difference of squares may be factored according to the [[identity (mathematics)|identity]]\n\n:<math>a^2-b^2 = (a+b)(a-b)</math>\n\nin [[elementary algebra]].\n\n==Proof== \nThe [[mathematical proof|proof]] of the factorization identity is straightforward. Starting from the [[Sides of an equation|left-hand side]], apply the [[distributive law]] to get \n:<math>(a+b)(a-b) = a^2+ba-ab-b^2</math>\nBy the [[commutative law]], the middle two terms cancel:\n:<math>ba - ab = 0</math>\nleaving\n:<math>(a+b)(a-b) = a^2-b^2</math>\nThe resulting identity is one of the most commonly used in mathematics. Among many uses, it gives a simple proof of the [[AM–GM inequality]] in two variables.\n\nThe proof just given indicates the scope of the identity in [[abstract algebra]]: it will hold in any [[commutative ring]] ''R''.\n\nConversely, if this identity holds in a [[ring (mathematics)|ring]] ''R'' for all pairs of elements ''a'' and ''b'' of the ring, then ''R'' is commutative.   To see this, apply the distributive law to the right-hand side of the original equation and get\n:<math>a^2 + ba - ab - b^2</math>\n\nand for this to be equal to <math>a^2 - b^2</math>, we must have\n\n:<math>ba - ab = 0</math>\n\nfor all pairs ''a'', ''b'' of elements of ''R'', so the ring ''R'' is commutative.\n\n==Geometrical demonstrations==\n[[Image:Difference of two squares.svg|right|170px]]\n\nThe difference of two squares can also be illustrated geometrically as the difference of two square areas in a [[Plane (mathematics)|plane]]. In the diagram, the shaded part represents the difference between the areas of the two squares, i.e. <math>a^2 - b^2</math>.  The area of the shaded part can be found by adding the areas of the two rectangles; <math>a(a-b) + b(a-b)</math>, which can be factorized to <math>(a+b)(a-b)</math>.  Therefore <math>a^2 - b^2 = (a+b)(a-b)</math>\n\nAnother geometric proof proceeds as follows: We start with the figure shown in the first diagram below, a large square with a smaller square removed from it. The side of the entire square is a, and the side of the small removed square is b. The area of the shaded region is <math>a^2-b^2</math>. A cut is made, splitting the region into two rectangular pieces, as shown in the second diagram. The larger piece, at the top, has width a and height a-b. The smaller piece, at the bottom, has width a-b and height b. Now the smaller piece can be detached, rotated, and placed to the right of the larger piece. In this new arrangement, shown in the last diagram below, the two pieces together form a rectangle, whose width is <math>a+b</math> and whose height is <math>a-b</math>. This rectangle's area is <math>(a+b)(a-b)</math>. Since this rectangle came from rearranging the original figure, it must have the same area as the original figure. Therefore, <math>a^2-b^2 = (a+b)(a-b)</math>.\n[[Image:Difference of two squares geometric proof.png]]\n\n==Uses==\n===Factorization of polynomials and simplification of expressions===\nThe formula for the difference of two squares can be used for factoring [[polynomial]]s that contain the square of a first quantity minus the square of a second quantity. For example, the polynomial <math>x^4 - 1</math> can be factored as follows:\n\n:<math>x^4 - 1 = (x^2 + 1)(x^2 - 1) = (x^2 + 1)(x + 1)(x - 1)</math>\n\nAs a second example, the first two terms of <math>x^2 - y^2 + x - y</math> can be factored as <math>(x + y)(x - y)</math>, so we have:\n\n:<math>x^2 - y^2 + x - y = (x + y)(x - y) + x - y = (x - y)(x + y + 1)</math>\n\nMoreover, this formula can also be used for simplifying expressions:\n\n:<math>(a+b)^2-(a-b)^2=(a+b+a-b)(a+b-a+b)=(2a)(2b)=4ab</math>\n\n===Complex number case: sum of two squares===\nThe difference of two squares is used to find the [[linear factors]] of the ''sum'' of two squares, using [[complex number]] coefficients.\n\nFor example, the complex roots of <math>z^2 + 4</math> can be found using difference of two squares:\n\n:<math>z^2 + 4</math>\n:<math> = z^2 - i^2 \\cdot 4</math>    (since <math>i^2 = -1</math>)\n:<math> = z^2 - (2 i)^2</math>\n:<math> = (z + 2 i)(z - 2 i)</math>\n\nTherefore the linear factors are <math>(z + 2 i)</math> and <math>(z - 2 i)</math>.\n\nSince the two factors found by this method are [[complex conjugate]]s, we can use this in reverse as a method of multiplying a complex number to get a real number. This is used to get real denominators in complex fractions.<ref>[http://www.themathpage.com/alg/complex-numbers.htm#conjugates Complex or imaginary numbers] TheMathPage.com, retrieved 22 December 2011</ref>\n\n===Rationalising denominators===\nThe difference of two squares can also be used in the [[Rationalisation (mathematics)|rationalising]] of [[irrational number|irrational]] [[denominator]]s.<ref>[http://www.themathpage.com/alg/multiply-radicals.htm Multiplying Radicals] TheMathPage.com, retrieved 22 December 2011</ref> This is a method for removing [[Nth root|surds]] from expressions (or at least moving them), applying to division by some combinations involving [[square root]]s.\n\nFor example:\nThe denominator of <math>\\dfrac{5}{\\sqrt{3} + 4}</math> can be rationalised as follows:\n\n:<math>\\dfrac{5}{\\sqrt{3} + 4}</math>\n\n:<math> = \\dfrac{5}{\\sqrt{3} + 4} \\times \\dfrac{\\sqrt{3} - 4}{\\sqrt{3} - 4}</math>\n\n:<math> = \\dfrac{5(\\sqrt{3} - 4)}{(\\sqrt{3} + 4)(\\sqrt{3} - 4)}</math>\n\n:<math> = \\dfrac{5(\\sqrt{3} - 4)}{\\sqrt{3}^2 - 4^2}</math>\n\n:<math> = \\dfrac{5(\\sqrt{3} - 4)}{3 - 16}</math>\n\n:<math> = -\\dfrac{5(\\sqrt{3} - 4)}{13}.</math>\n\nHere, the irrational denominator <math>\\sqrt{3} + 4</math> has been rationalised to <math>13</math>.\n\n===Mental arithmetic===\n{{Main|Multiplication algorithm#Quarter square multiplication}}\nThe difference of two squares can also be used as an arithmetical short cut.  If you are multiplying two numbers whose average is a number which is easily squared the difference of two squares can be used to give you the product of the original two numbers.\n\nFor example: \n:<math> 27 \\times 33 = (30 - 3)(30 + 3)</math>\n\nWhich means  using the difference of two squares <math>27 \\times 33</math> can be restated as\n \n:<math>a^2 - b^2</math> which is <math>30^2 - 3^2 = 891</math>.\n\n===Difference of two consecutive perfect squares===\n\nThe difference of two consecutive [[square number|perfect square]]s is the sum of the two [[base (exponentiation)|base]]s ''n'' and ''n''+1. This can be seen as follows:\n\n:<math>\n\\begin{array}{lcl}\n (n+1)^2 - n^2 & = & ((n+1)+n)((n+1)-n) \\\\\n           & = & 2n+1\n\\end{array}\n</math>\n\nTherefore the difference of two consecutive perfect squares is an odd number. Similarly, the difference of two arbitrary perfect squares is calculated as follows:\n\n:<math>\n\\begin{array}{lcl}\n (n+k)^2 - n^2 & = & ((n+k)+n)((n+k)-n) \\\\\n           & = & k(2n+k)\n\\end{array}\n</math>\n\nTherefore the difference of two even perfect squares is a multiple of 4 and the difference of two odd perfect squares is a multiple of 8.\n\n===Factorization of integers===\n\nSeveral algorithms in number theory and cryptography use differences of squares to find factors of integers and detect composite numbers. A simple example is the [[Fermat factorization method]], which considers the sequence of numbers <math>x_i:=a_i^2-N</math>, for <math>a_i:=\\left\\lceil \\sqrt{N}\\right\\rceil+i</math>. If one of the <math>x_i</math> equals a perfect square <math>b^2</math>, then <math>N=a_i^2-b^2=(a_i+b)(a_i-b)</math> is a (potentially non-trivial) factorization of <math>N</math>.\n\nThis trick can be generalized as follows. If <math>a^2\\equiv b^2</math> mod <math>N</math> and <math>a\\not\\equiv \\pm b</math> mod <math>N</math>, then <math>N</math> is composite with non-trivial factors <math>\\gcd(a-b,N)</math> and <math>\\gcd(a+b,N)</math>. This forms the basis of several factorization algorithms (such as the [[quadratic sieve]]) and can be combined with the [[Fermat primality test]] to give the stronger [[Miller–Rabin primality test]].\n\n==Generalizations==\n[[Image:Rhombus understood analytically.svg|thumb|right|Vectors {{math|'''a'''}}&nbsp;(purple), {{math|'''b'''}}&nbsp;(cyan) and {{math|'''a''' + '''b'''}}&nbsp;(blue) are shown with [[arrow (symbol)|arrows]]]]\nThe identity also holds in [[inner product space]]s over the [[field (mathematics)|field]] of [[real numbers]], such as for [[dot product]] of [[Euclidean vector]]s:\n:<math>{\\mathbf a}\\cdot{\\mathbf a} - {\\mathbf b}\\cdot{\\mathbf b} = ({\\mathbf a}+{\\mathbf b})\\cdot({\\mathbf a}-{\\mathbf b})</math>\nThe proof is identical. By the way, assuming that {{math|'''a'''}} and {{math|'''b'''}} have equal [[normed vector space|norms]] (which means that their dot squares are equal), it demonstrates [[analytic geometry|analytically]] the fact that two diagonals of a [[rhombus]] are [[right angle|perpendicular]].\n===Difference of two nth powers===\nIf ''a'' and ''b'' are two elements of a commutative ring ''R'', then <math>a^n-b^n=\\left(a-b\\right)\\left(\\sum_{k=0}^{n-1} a^{n-1-k}b^k\\right)</math>. Note that binomial coefficients do ''not'' appear in the second factor, and the summation stops at ''n''-1, ''not'' ''n''.\n\n==See also==\n*[[Congruum]], the shared difference of three squares in arithmetic progression\n*[[Conjugate (algebra)]]\n*[[Factorization]]\n\n==Notes==\n<references/>\n\n==References==\n*{{cite book |first=James Stuart |last=Stanton |title=Encyclopedia of Mathematics |location= |publisher=Infobase Publishing |year=2005 |isbn=0-8160-5124-0 |page=131 |url=https://books.google.com/books?id=MfKKMSuthacC&pg=PA131 }}\n*{{cite book |first=Alan S. |last=Tussy |first2=Roy David |last2=Gustafson |title=Elementary Algebra |edition=5th |publisher=Cengage Learning |year=2011 |isbn=978-1-111-56766-8 |pages=467–469 |url=https://books.google.com/books?id=xwOrtVKSVpoC&pg=PA467 }}\n\n==External links==\n*[http://www.themathpage.com/alg/difference-two-squares.htm difference of two squares] at mathpages.com\n\n\n{{DEFAULTSORT:Difference Of Two Squares}}\n[[Category:Elementary algebra]]\n[[Category:Commutative algebra]]\n[[Category:Mathematical identities]]\n[[Category:Articles containing proofs]]\n[[Category:Subtraction]]"
    },
    {
      "title": "Differentiation rules",
      "url": "https://en.wikipedia.org/wiki/Differentiation_rules",
      "text": "{{Calculus |Differential}}\n\nThis is a summary of '''differentiation rules''', that is, rules for computing the [[derivative]] of a [[function (mathematics)|function]] in [[calculus]].\n\n== Elementary rules of differentiation ==\n\nUnless otherwise stated, all functions are functions of [[real number|real numbers ('''R''')]] that return real values; although more generally, the formulae below apply wherever they are [[well defined]]<ref>''Calculus (5th edition)'', F. Ayres, E. Mendelson, Schaum's Outline Series, 2009, {{ISBN|978-0-07-150861-2}}.</ref><ref>''Advanced Calculus (3rd edition)'', R. Wrede, M.R. Spiegel, Schaum's Outline Series, 2010, {{ISBN|978-0-07-162366-7}}.</ref>—including [[complex number|complex numbers ('''C''')]].<ref>''Complex Variables'', M.R. Speigel, S. Lipschutz, J.J. Schiller, D. Spellman, Schaum's Outlines Series, McGraw Hill (USA), 2009, {{ISBN|978-0-07-161569-3}}</ref>\n\n===Differentiation is linear===\n \n{{main|Linearity of differentiation}}\n\nFor any functions <math>f</math> and <math>g</math> and any real numbers <math>a</math> and <math>b</math> the derivative of the function <math>h(x) = af(x) + bg(x)</math> with respect to <math>x</math> is\n\n:<math> h'(x) = a f'(x) + b g'(x).</math>\n\nIn [[Leibniz's notation]] this is written as:\n\n:<math> \\frac{d(af+bg)}{dx}  = a\\frac{df}{dx} +b\\frac{dg}{dx}.</math>\n\nSpecial cases include:\n\n* ''The [[Constant factor rule in differentiation|constant factor\n rule]]''\n:<math>(af)' = af' </math>\n* ''The [[Sum rule in differentiation|sum rule]]''\n:<math>(f + g)' = f' + g'</math>\n* ''The subtraction rule''\n:<math>(f - g)' = f' - g'.</math>\n\n===The product rule===\n\n{{main|Product rule}}\n\nFor the functions ''f'' and ''g'', the derivative of the function ''h''(''x'') = ''f''(''x'') ''g''(''x'') with respect to ''x'' is\n:<math> h'(x) = (fg)'(x) = f'(x) g(x) + f(x) g'(x).</math>\nIn Leibniz's notation this is written\n:<math>\\frac{d(fg)}{dx} = \\frac{df}{dx} g + f \\frac{dg}{dx}.</math>\n\n===The chain rule===\n{{main|Chain rule}}\n\nThe derivative of the function <math>h(x) = f(g(x))</math> is\n\n:<math> h'(x) = f'(g(x))\\cdot g'(x).</math>\n\nIn Leibniz's notation, this is written as:\n\n:<math>\\frac{d}{dx}h(x) = \\frac{d}{dz}f(z)|_{z=g(x)}\\cdot \\frac{d}{dx}g(x),</math> \n\noften abridged to\n\n:<math>\\frac{dh(x)}{dx} = \\frac{df(g(x))}{dg(x)}\\cdot \\frac{dg(x)}{dx}.</math>\n\nFocusing on the notion of maps, and the differential being a map <math>\\text{D}</math>, this is written in a more concise way as:\n\n:<math> [\\text{D} (h\\circ g)]_x = [\\text{D} h]_{g(x)} \\cdot [\\text{D}g]_x\\,.</math>\n\n===The inverse function rule===\n\n{{main|inverse functions and differentiation}}\n\nIf the function ''f'' has an [[inverse function]] ''g'', meaning that {{nowrap|1=''g''(''f''(''x'')) = ''x''}} and {{nowrap|1=''f''(''g''(''y'')) = ''y''}}, then\n:<math>g' = \\frac{1}{f'\\circ g}.</math>\n\nIn Leibniz notation, this is written as\n:<math> \\frac{dx}{dy} = \\frac{1}{\\frac{dy}{dx}}.</math>\n\n==Power laws, polynomials, quotients, and reciprocals==\n===The polynomial or elementary power rule===\n\n{{main|Power rule}}\n\nIf <math>f(x) = x^r</math>, for any real number <math>r \\neq 0,</math> then \n:<math>f'(x) = rx^{r-1}.</math>\n\nWhen <math>r = 1,</math> this becomes the special case that if <math>f(x) = x,</math> then <math>f'(x) = 1.</math>\n\nCombining the power rule with the sum and constant multiple rules permits the computation of the derivative of any polynomial.\n\n===The reciprocal rule===\n\n{{main|Reciprocal rule}}\nThe derivative of ''h''(''x'') = 1/''f''(''x'') for any (nonvanishing) function ''f'' is:\n\n:<math> h'(x) = -\\frac{f'(x)}{(f(x))^2}.</math>\n\nIn Leibniz's notation, this is written\n\n:<math> \\frac{d(1/f)}{dx} = -\\frac{1}{f^2}\\frac{df}{dx}.</math>\n\nThe reciprocal rule can be derived from the quotient rule.\n\n===The quotient rule===\n{{main|Quotient rule}}\nIf ''f'' and ''g'' are functions, then:\n:<math>\\left(\\frac{f}{g}\\right)' = \\frac{f'g - g'f}{g^2}\\quad</math> wherever ''g'' is nonzero.\nThis can be derived from the product rule.\n\n===Generalized power rule===\n\n{{main|Power rule}}\n\nThe elementary power rule generalizes considerably. The most general power rule is the '''functional power rule''': for any functions ''f'' and ''g'',\n:<math>(f^g)' = \\left(e^{g\\ln f}\\right)' = f^g\\left(f'{g \\over f} + g'\\ln f\\right),\\quad</math>\nwherever both sides are well defined.\n\nSpecial cases:\n* If ''f''(''x'') = ''x''<sup>''a''</sup>, ''f′''(''x'') = ''ax''<sup>''a'' − 1</sup> when ''a'' is any non-zero real number and ''x'' is positive.\n* The reciprocal rule may be derived as the special case where ''g''(''x'') = −1.\n\n== Derivatives of exponential and logarithmic functions ==\n\n:<math> \\frac{d}{dx}\\left(c^{ax}\\right) = {ac^{ax} \\ln c } ,\\qquad c > 0</math>\nnote that the equation above is true for all ''c'', but the derivative for c < 0 yields a complex number.\n\n:<math> \\frac{d}{dx}\\left(e^{ax}\\right) = ae^{ax}</math>\n\n:<math> \\frac{d}{dx}\\left( \\log_c x\\right) = {1 \\over x \\ln c} , \\qquad c > 0, c \\ne 1</math>\nthe equation above is also true for all ''c'' but yields a complex number if c<0.\n\n:<math> \\frac{d}{dx}\\left( \\ln x\\right)  = {1 \\over x} ,\\qquad x > 0.</math>\n\n:<math> \\frac{d}{dx}\\left( \\ln |x|\\right) = {1 \\over x}.</math>\n\n:<math> \\frac{d}{dx}\\left( x^x \\right) = x^x(1+\\ln x).</math>\n\n:<math> \\frac{d}{dx}\\left( f(x)^{ g(x) } \\right ) = g(x)f(x)^{g(x)-1} \\frac{df}{dx} + f(x)^{g(x)}\\ln{( f(x) )}\\frac{dg}{dx}, \\qquad \\text{if }f(x) > 0, \\text{ and if } \\frac{df}{dx} \\text{ and } \\frac{dg}{dx} \\text{ exist.}</math>\n\n:<math> \\frac{d}{dx}\\left( f_{1}(x)^{f_{2}(x)^{\\left ( ... \\right )^{f_{n}(x)}}} \\right ) = \\left [\\sum\\limits_{k=1}^{n} \\frac{\\partial }{\\partial x_{k}} \\left( f_{1}(x_1)^{f_{2}(x_2)^{\\left ( ... \\right )^{f_{n}(x_n)}}} \\right ) \\right ] \\biggr\\vert_{x_1 = x_2 = ... =x_n = x}, \\text{ if } f_{i<n}(x) > 0 \\text{ and }</math> <math> \\frac{df_{i}}{dx} \\text{ exists. }</math>\n===Logarithmic derivatives===\n\nThe [[logarithmic derivative]] is another way of stating the rule for differentiating the [[logarithm]] of a function (using the chain rule):\n:<math> (\\ln f)'= \\frac{f'}{f} \\quad</math> wherever ''f'' is positive.\n\n[[Logarithmic differentiation]] is a technique which uses logarithms and its differentiation rules to simplify certain expressions before actually applying the derivative.\nLogarithms can be used to remove exponents, convert products into sums, and convert division into subtraction, each of which may lead to a simplified expression for taking derivatives.\n\n== Derivatives of trigonometric functions ==\n{{main|Differentiation of trigonometric functions}}\n\n{| style=\"width:100%; background:transparent; margin-left:2em;\"\n|width=50%|<math> (\\sin x)' = \\cos x </math>\n|width=50%|<math> (\\arcsin x)' = { 1 \\over \\sqrt{1 - x^2}} </math>\n|-\n|<math> (\\cos x)' = -\\sin x </math>\n|<math> (\\arccos x)' = -{1 \\over \\sqrt{1 - x^2}} </math>\n|-\n|<math> (\\tan x)' = \\sec^2 x = { 1 \\over \\cos^2 x} = 1 + \\tan^2 x </math>\n|<math> (\\arctan x)' = { 1 \\over 1 + x^2} </math>\n|-\n|<math> (\\sec x)' = \\sec x \\tan x </math>\n|<math> (\\operatorname{arcsec} x)' = { 1 \\over |x|\\sqrt{x^2 - 1}} </math>\n|-\n|<math> (\\csc x)' = -\\csc x \\cot x </math>\n|<math> (\\operatorname{arccsc} x)' = -{1 \\over |x|\\sqrt{x^2 - 1}} </math>\n|-\n|<math> (\\cot x)' = -\\csc^2 x = { -1 \\over \\sin^2 x} = -(1 + \\cot^2 x)</math>\n|<math> (\\operatorname{arccot} x)' = -{1 \\over 1 + x^2} </math>\n|}\nIt is common to additionally define an inverse tangent function with two arguments, <math>\\arctan(y,x)</math>.  Its value lies in the range <math>[-\\pi,\\pi]</math> and reflects the quadrant of the point <math>(x,y)</math>.  For the first and fourth quadrant (i.e. <math>x > 0</math>) one has <math>\\arctan(y, x>0) = \\arctan(y/x)</math>.  Its partial derivatives are\n{| style=\"width:100%; background:transparent; margin-left:2em;\"\n|width=100%|<math> \\frac{\\partial \\arctan(y,x)}{\\partial y} = \\frac{x}{x^2 + y^2}</math>, and <math> \\frac{\\partial \\arctan(y,x)}{\\partial x} = \\frac{-y}{x^2 + y^2}.</math>\n|}\n\n==Derivatives of hyperbolic functions==\n{| style=\"width:100%; background:transparent; margin-left:2em;\"\n|width=50%|<math>( \\sinh x )'= \\cosh x = \\frac{e^x +\n e^{-x}}{2}</math>\n|width=50%|<math>(\\operatorname{arsinh}\\,x)' = { 1 \\over \\sqrt{x^2 + 1}}</math>\n|-\n|<math>(\\cosh x )'= \\sinh x = \\frac{e^x - e^{-x}}{2}</math>\n|<math>(\\operatorname{arcosh}\\,x)' = {\\frac {1}{\\sqrt{x^2-1}}}</math>\n|-\n|<math>(\\tanh x )'= {\\operatorname{sech}^2\\,x}</math>\n|<math>(\\operatorname{artanh}\\,x)' = { 1 \\over 1 - x^2}</math>\n|-\n|<math>(\\operatorname{sech}\\,x)' = - \\tanh x\\,\\operatorname{sech}\\,x</math>\n|<math>(\\operatorname{arsech}\\,x)' = -{1 \\over x\\sqrt{1 - x^2}}</math>\n|-\n|<math>(\\operatorname{csch}\\,x)' = -\\,\\operatorname{coth}\\,x\\,\\operatorname{csch}\\,x</math>\n|<math>(\\operatorname{arcsch}\\,x)' = -{1 \\over |x|\\sqrt{1 + x^2}}</math>\n|-\n|<math>(\\operatorname{coth}\\,x )' =\n\n -\\,\\operatorname{csch}^2\\,x</math>\n|<math>(\\operatorname{arcoth}\\,x)' = { 1 \\over 1 - x^2}</math>\n|}\n\n==Derivatives of special functions==\n{| style=\"width:100%; background:transparent; margin-left:2em;\"\n|width=80%|\n;[[Gamma function]] <math>\\quad \\Gamma(x) = \\int_0^\\infty  t^{x-1} e^{-t}\\, dt</math>\n:<math>\\Gamma'(x) = \\int_0^\\infty t^{x-1} e^{-t} \\ln t\\,dt</math>\n::: <math>\\, = \\Gamma(x) \\left(\\sum_{n=1}^\\infty \\left(\\ln\\left(1 + \\dfrac{1}{n}\\right) - \\dfrac{1}{x + n}\\right) - \\dfrac{1}{x}\\right)</math>\n::: <math>\\, = \\Gamma(x) \\psi(x)</math>\nwith <math>\\psi(x)</math> being the [[digamma function]], expressed by the parenthesized expression to the right of <math>\\Gamma(x)</math> in the line above.\n|width=50%|\n|}\n{| style=\"width:100%; background:transparent; margin-left:2em;\"\n|width=50%|\n;[[Riemann Zeta function]]<math>\\quad\\zeta(x) =\\sum_{n=1}^\\infty\\frac{1}{n^x}</math>\n:<math>\\zeta'(x) = -\\sum_{n=1}^\\infty \\frac{\\ln n}{n^x}\n=-\\frac{\\ln 2}{2^x} - \\frac{\\ln 3}{3^x} - \\frac{\\ln 4}{4^x} - \\cdots\n</math>\n\n::: <math>\\, = -\\sum_{p \\text{ prime}} \\frac{p^{-x} \\ln p}{(1-p^{-x})^2}\\prod_{q \\text{ prime}, q \\neq p} \\frac{1}{1-q^{-x}} </math>\n|}\n\n==Derivatives of integrals==\n\n{{main|Differentiation under the integral sign}}\n\nSuppose that it is required to differentiate with respect to ''x'' the function\n\n:<math>F(x)=\\int_{a(x)}^{b(x)}f(x,t)\\,dt,</math>\n\nwhere the functions <math>f(x,t)</math> and <math>\\frac{\\partial}{\\partial x}\\,f(x,t)</math> are both continuous in both <math>t</math> and <math>x</math> in some region of the <math>(t,x)</math> plane, including <math>a(x)\\leq t\\leq b(x),</math> <math>x_0\\leq x\\leq x_1</math>, and the functions <math>a(x)</math> and <math>b(x)</math> are both continuous and both have continuous derivatives for <math>x_0\\leq x\\leq x_1</math>.  Then for <math>\\,x_0\\leq x\\leq x_1</math>:\n\n:<math> F'(x) = f(x,b(x))\\,b'(x) - f(x,a(x))\\,a'(x) + \\int_{a(x)}^{b(x)} \\frac{\\partial}{\\partial x}\\, f(x,t)\\; dt\\,. </math>\n\nThis formula is the general form of the [[Leibniz integral rule]] and can be derived using the \n[[fundamental theorem of calculus]].\n\n==Derivatives to ''n''th order==\nSome rules exist for computing the ''n''th derivative of functions, where ''n'' is a positive integer.  These include:\n\n===Faà di Bruno's formula===\n{{main|Faà di Bruno's formula}}\nIf ''f'' and ''g'' are ''n'' times differentiable, then\n\n:<math>  \\frac{d^n}{d x^n} [f(g(x))]= n! \\sum_{\\{k_m\\}}^{} f^{(r)}(g(x)) \\prod_{m=1}^n \\frac{1}{k_m!} \\left(g^{(m)}(x) \\right)^{k_m}</math>\n\nwhere <math> r = \\sum_{m=1}^{n-1} k_m</math> and the set <math> \\{k_m\\}</math> consists of all non-negative integer solutions of the Diophantine equation <math> \\sum_{m=1}^{n} m k_m = n</math>.\n\n===General Leibniz rule===\n{{main|General Leibniz rule}}\nIf ''f'' and ''g'' are ''n'' times differentiable, then\n\n:<math> \\frac{d^n}{dx^n}[f(x)g(x)] = \\sum_{k=0}^{n} \\binom{n}{k} \\frac{d^{n-k}}{d x^{n-k}} f(x) \\frac{d^k}{d x^k} g(x)</math>\n\n==See also==\n\n*[[Vector calculus identities]]\n*[[Differentiable function]]\n*[[Differential of a function]]\n*[[List of mathematical functions]]\n*[[Trigonometric functions]]\n*[[Inverse trigonometric functions]]\n*[[Hyperbolic functions]]\n*[[Inverse hyperbolic functions]]\n*[[Matrix calculus]]\n*[[Differentiation under the integral sign]]\n\n==References==\n{{reflist}}\n\n==Sources and further reading==\nThese rules are given in many books, both on elementary and advanced calculus, in pure and applied mathematics. Those in this article (in addition to the above references) can be found in:\n*''Mathematical Handbook of Formulas and Tables (3rd edition)'', S. Lipschutz, M.R. Spiegel, J. Liu, Schaum's Outline Series, 2009, {{ISBN|978-0-07-154855-7}}.\n*''The Cambridge Handbook of Physics Formulas'', G. Woan, Cambridge University Press, 2010, {{ISBN|978-0-521-57507-2}}.\n*''Mathematical methods for physics and engineering'', K.F. Riley, M.P. Hobson, S.J. Bence, Cambridge University Press, 2010, {{ISBN|978-0-521-86153-3}}\n*''NIST Handbook of Mathematical Functions'', F. W. J. Olver, D. W. Lozier, R. F. Boisvert, C. W. Clark, Cambridge University Press, 2010, {{ISBN|978-0-521-19225-5}}.\n\n==External links==\n{{Library resources box \n|by=no \n|onlinebooks=no \n|others=no \n|about=yes \n|label=Differentiation rules}}\n\n* [http://www.planetcalc.com/675/ Derivative calculator with formula simplification]\n\n[[Category:Differential calculus|*]]\n[[Category:Differentiation rules| ]]\n[[Category:Mathematics-related lists|Derivatives]]\n[[Category:Mathematical tables|Derivatives]]\n[[Category:Mathematical identities]]\n\n[[ca:Taula de derivades]]\n[[es:Tabla de derivadas]]\n[[fr:Dérivées usuelles]]\n[[he:נגזרת]]\n[[pl:Pochodna_funkcji#Pochodne_funkcji_elementarnych]]\n[[sq:Tabela e derivateve]]"
    },
    {
      "title": "Dyson conjecture",
      "url": "https://en.wikipedia.org/wiki/Dyson_conjecture",
      "text": "[[File:Freeman Dyson.jpg|thumb|Freeman Dyson in 2005]]\n\nIn mathematics, the '''Dyson conjecture''' {{harvs |first=Freeman |last=Dyson |year=1962 |authorlink=Freeman Dyson}} is a conjecture about the constant term of certain [[Laurent polynomial]]s, proved by [[Kenneth G. Wilson|Wilson]] and Gunson. [[George Andrews (mathematician)|Andrews]] generalized it to the '''q-Dyson conjecture''', proved by [[Doron Zeilberger|Zeilberger]] and [[David Bressoud|Bressoud]] and sometimes called the '''Zeilberger–Bressoud theorem'''. [[Ian G. Macdonald|Macdonald]] generalized it further to more general [[root system]]s with the '''Macdonald constant term conjecture''', proved by [[Ivan Cherednik|Cherednik]].\n\n==Dyson conjecture==\n\nThe Dyson conjecture states that the [[Laurent polynomial]]\n\n:<math>\\prod _{1\\le i\\ne j\\le n}(1-t_i/t_j)^{a_i}</math>\n\nhas constant term\n\n:<math>\\frac{(a_1+a_2+\\cdots+a_n)!}{a_1!a_2!\\cdots a_n!}.</math>\n\nThe conjecture was first proved independently by {{harvtxt|Wilson|1962}} and {{harvtxt|Gunson|1962}}. {{harvtxt|Good|1970}} later found a short proof, by observing that the Laurent polynomials, and therefore their constant terms, satisfy the recursion relations\n\n:<math>F(a_1,\\dots,a_n) = \\sum_{i=1}^nF(a_1,\\dots,a_i-1,\\dots,a_n).</math>\n\nThe case  ''n''&nbsp;=&nbsp;3 of Dyson's conjecture follows from the [[Dixon identity]].\n\n{{harvtxt|Sills|Zeilberger|2006}} and {{harv|Sills|2006}} used a computer to find expressions for non-constant coefficients of\nDyson's Laurent polynomial.\n\n==Dyson integral==\nWhen all the values ''a''<sub>''i''</sub>  are equal to β/2, the constant term in Dyson's conjecture is the value of '''Dyson's integral'''\n\n:<math>\\frac{1}{(2\\pi)^n}\\int_0^{2\\pi}\\cdots\\int_0^{2\\pi}\\prod_{1\\le j<k\\le n}|e^{i\\theta_j}-e^{i\\theta_k}|^\\beta \\, d\\theta_1\\cdots d\\theta_n.</math>\n\nDyson's integral is a special case of [[Selberg's integral]] after a change of variable and has value\n\n:<math>\\frac{\\Gamma(1+\\beta n/2)}{\\Gamma(1+\\beta/2)^n}</math>\n\nwhich gives another proof of Dyson's conjecture in this special case.\n\n==''q''-Dyson conjecture==\n{{harvtxt|Andrews|1975}} found a [[q-analog]] of Dyson's conjecture, stating that the constant term of \n:<math>\\prod_{1\\le i<j\\le n}\\left(\\frac{x_i}{x_j};q\\right)_{a_i}\\left(\\frac{qx_j}{x_i};q\\right)_{a_j}</math>\nis \n:<math>\\frac{(q;q)_{a_1+\\cdots+a_n}}{(q;q)_{a_1}\\cdots(q;q)_{a_n}}.</math>\nHere (''a'';''q'')<sub>''n''</sub> is the [[q-Pochhammer symbol]].\nThis conjecture reduces to Dyson's conjecture for ''q''=1, and was proved by {{harvtxt|Zeilberger|Bressoud|1985}}, using a combinatorial approach inspired by\nprevious work of [[Ira Gessel]] and [[Dominique Foata]]. A shorter proof, using formal Laurent series, was given in 2004 by Ira Gessel and Guoce Xin, and\nan even shorter proof, using a quantitative form, due to Karasev and Petrov, and independently to Lason, of Noga Alon's Combinatorial Nullstellensatz,\nwas given in 2012 by Gyula Karolyi and Zoltan Lorant Nagy.\nThe latter method was extended, in 2013, by Shalosh B. Ekhad and Doron Zeilberger to derive explicit expressions of any specific coefficient, not just the\nconstant term, see http://www.math.rutgers.edu/~zeilberg/mamarim/mamarimhtml/qdyson.html, for detailed references.\n\n==Macdonald conjectures==\n{{harvtxt|Macdonald|1982}} extended the conjecture to arbitrary finite or affine [[root system]]s, with Dyson's original conjecture corresponding to \nthe case of the ''A''<sub>''n''&minus;1</sub> root system and Andrews's conjecture corresponding to the affine ''A''<sub>''n''&minus;1</sub> root system. Macdonald reformulated these conjectures as conjectures about the norms of [[Macdonald polynomial]]s. Macdonald's conjectures were proved by {{harv|Cherednik|1995}} using doubly affine Hecke algebras.\n\n[[Ian G. Macdonald|Macdonald]]'s form of Dyson's conjecture for root systems of type BC is closely related to [[Selberg's integral]].\n\n==References==\n*{{Citation | authorlink=George Andrews (mathematician) | last1=Andrews | first1=George E. | title=Theory and application of special functions (Proc. Advanced Sem., Math. Res. Center, Univ. Wisconsin, Madison, Wis., 1975) | publisher=[[Academic Press]] | location=Boston, MA | mr=0399528 | year=1975 | chapter=Problems and prospects for basic hypergeometric functions | pages=191–224}}\n*{{citation |jstor=2118632 |pages=191–216 |last1=Cherednik |first1=I. |title=Double Affine Hecke Algebras and Macdonald's Conjectures |volume=141 |issue=1 |journal=The Annals of Mathematics |year=1995 |doi=10.2307/2118632}}\n*{{Citation | last1=Dyson | first1=Freeman J. | title=Statistical theory of the energy levels of complex systems. I | doi=10.1063/1.1703773 | mr=0143556 | year=1962 | journal=[[Journal of Mathematical Physics]] | issn=0022-2488 | volume=3 | pages=140–156}}\n*{{Citation | last1=Good | first1=I. J. | author1-link=I. J. Good | title=Short proof of a conjecture by Dyson | doi=10.1063/1.1665339 | mr=0258644 | year=1970 | journal=[[Journal of Mathematical Physics]] | issn=0022-2488 | volume=11 | pages=1884 | issue=6}}\n*{{Citation | last1=Gunson | first1=J. | title=Proof of a conjecture by Dyson in the statistical theory of energy levels | doi=10.1063/1.1724277 | mr=0148401 | year=1962 | journal=[[Journal of Mathematical Physics]] | issn=0022-2488 | volume=3 | pages=752–753 | issue=4}}\n*{{Citation | last1=Macdonald | first1=I. G. | title=Some conjectures for root systems | doi=10.1137/0513070 | mr=674768 | year=1982 | journal=SIAM Journal on Mathematical Analysis | issn=0036-1410 | volume=13 | issue=6 | pages=988–1007}}\n*{{Citation | last1=Sills | first1=Andrew V. | title=Disturbing the Dyson conjecture, in a generally GOOD way | doi=10.1016/j.jcta.2005.12.005 | mr=2259066 | year=2006 | journal=Journal of Combinatorial Theory, Series A | issn=1096-0899 | volume=113 | issue=7 | pages=1368–1380}}\n*{{Citation | last1=Sills | first1=Andrew V. | last2=Zeilberger | first2=Doron | author2-link=Doron Zeilberger | title=Disturbing the Dyson conjecture (in a GOOD way) | url=http://projecteuclid.org/euclid.em/1175789739 | mr=2253005 | year=2006 | journal=Experimental Mathematics | issn=1058-6458 | volume=15 | issue=2 | pages=187–191 | doi=10.1080/10586458.2006.10128959}}\n*{{Citation | last1=Wilson | first1=Kenneth G. | author1-link = Kenneth G. Wilson | title=Proof of a conjecture by Dyson | doi=10.1063/1.1724291 | mr=0144627 | year=1962 | journal=[[Journal of Mathematical Physics]] | issn=0022-2488 | volume=3 | pages=1040–1043 | issue=5}}\n*{{Citation | last1=Zeilberger | first1=Doron | author1-link=Doron Zeilberger | last2=Bressoud | first2=David M. |author2-link=David Bressoud | title=A proof of Andrews' q-Dyson conjecture | doi=10.1016/0012-365X(85)90081-0 | mr=791661 | year=1985 | journal=[[Discrete Mathematics (journal)|Discrete Mathematics]] | issn=0012-365X | volume=54 | issue=2 | pages=201–224}}\n\n[[Category:Enumerative combinatorics]]\n[[Category:Algebraic combinatorics]]\n[[Category:Factorial and binomial topics]]\n[[Category:Mathematical identities]]\n[[Category:Freeman Dyson]]\n[[Category:Conjectures that have been proved]]"
    },
    {
      "title": "Enumerator polynomial",
      "url": "https://en.wikipedia.org/wiki/Enumerator_polynomial",
      "text": "In [[coding theory]], the '''weight enumerator polynomial''' of a binary [[linear code]] specifies the number of words of each possible  [[Hamming weight]].\n\nLet <math>C \\subset \\mathbb{F}_2^n</math> be a binary linear code length <math>n</math>.  The '''weight distribution''' is the sequence of numbers\n\n:<math> A_t = \\#\\{c \\in C \\mid w(c) = t \\} </math>\n\ngiving the number of [[codeword]]s ''c'' in ''C'' having weight ''t'' as ''t'' ranges from 0 to ''n''.  The '''weight enumerator''' is the bivariate [[polynomial]]\n\n:<math> W(C;x,y) = \\sum_{w=0}^n A_w x^w y^{n-w}.</math>\n\n==Basic properties==\n#<math> W(C;0,1) = A_{0}=1 </math>\n#<math> W(C;1,1) = \\sum_{w=0}^{n}A_{w}=|C| </math>\n#<math> W(C;1,0) = A_{n}= 1 \\mbox{ if } (1,\\ldots,1)\\in C\\ \\mbox{ and } 0 \\mbox{ otherwise} </math>\n#<math> W(C;1,-1) = \\sum_{w=0}^{n}A_{w}(-1)^{n-w} = A_{n}+(-1)^{1}A_{n-1}+\\ldots+(-1)^{n-1}A_{1}+(-1)^{n}A_{0} </math>\n\n==MacWilliams identity==\nDenote the [[dual code]] of <math>C \\subset \\mathbb{F}_2^n</math> by\n\n:<math>C^\\perp = \\{x \\in \\mathbb{F}_2^n \\,\\mid\\, \\langle x,c\\rangle = 0 \\mbox{  }\\forall c \\in C \\} </math>\n\n(where <math>\\langle\\ ,\\ \\rangle</math> denotes the vector [[dot product]] and which is taken over <math>\\mathbb{F}_2</math>).\n\nThe '''MacWilliams identity''' states that\n\n:<math>W(C^\\perp;x,y) = \\frac{1}{\\mid C \\mid} W(C;y-x,y+x). </math>\n\nThe identity is named after [[Jessie MacWilliams]].\n\n==Distance enumerator==\nThe '''distance distribution''' or '''inner distribution''' of a code ''C'' of size ''M'' and length ''n'' is the sequence of numbers\n\n:<math> A_i = \\frac{1}{M} \\# \\left\\lbrace (c_1,c_2) \\in C \\times C \\mid d(c_1,c_2) = i \\right\\rbrace </math>\n\nwhere ''i'' ranges from 0 to ''n''.  The '''distance enumerator polynomial''' is\n\n:<math> A(C;x,y) = \\sum_{i=0}^n A_i x^i y^{n-i} </math>\n\nand when ''C'' is linear this is equal to the weight enumerator.\n\nThe '''outer distribution''' of ''C'' is the 2<sup>''n''</sup>-by-''n''+1 matrix ''B'' with rows indexed by elements of GF(2)<sup>''n''</sup> and columns indexed by integers 0...''n'', and entries\n\n:<math> B_{x,i} = \\# \\left\\lbrace c \\in C \\mid d(c,x) = i \\right\\rbrace . </math>\n\nThe sum of the rows of ''B'' is ''M'' times the inner distribution vector (''A''<sub>0</sub>,...,''A''<sub>''n''</sub>).\n\nA code ''C'' is '''regular''' if the rows of ''B'' corresponding to the codewords of ''C'' are all equal.\n\n==References==\n* {{cite book | last=Hill | first=Raymond | title=A first course in coding theory | publisher=[[Oxford University Press]] | series=Oxford Applied Mathematics and Computing Science Series | date=1986 | isbn=0-19-853803-0 | pages=165–173 }}\n* {{cite book | last = Pless | first = Vera | authorlink=Vera Pless | title = Introduction to the theory of error-correcting codes | publisher = [[John Wiley & Sons]]|series = Wiley-Interscience Series in Discrete Mathematics | date = 1982| isbn = 0-471-08684-3 | pages=103–119 }}\n* {{cite book | author=J.H. van Lint | title=Introduction to Coding Theory | edition=2nd | publisher=[[Springer-Verlag]] | series=[[Graduate Texts in Mathematics|GTM]] | volume=86 | date=1992 | isbn=3-540-54894-7 }} Chapters 3.5 and 4.3.\n\n[[Category:Coding theory]]\n[[Category:Error detection and correction]]\n[[Category:Mathematical identities]]"
    },
    {
      "title": "Euler's four-square identity",
      "url": "https://en.wikipedia.org/wiki/Euler%27s_four-square_identity",
      "text": "In [[mathematics]], '''Euler's four-square identity''' says that the product of two numbers, each of which is a sum of four [[square (algebra)|square]]s, is itself a sum of four squares.\n\n==Algebraic identity==\nFor any pair of quadruples from a [[commutative ring]], the following expressions are equal:\n\n::<math>(a_1^2+a_2^2+a_3^2+a_4^2)(b_1^2+b_2^2+b_3^2+b_4^2)=</math>\n\n::<math>(a_1 b_1 - a_2 b_2 - a_3 b_3 - a_4 b_4)^2 +</math>\n\n::<math>(a_1 b_2 + a_2 b_1 + a_3 b_4 - a_4 b_3)^2 +</math>\n\n::<math>(a_1 b_3 - a_2 b_4 + a_3 b_1 + a_4 b_2)^2 +</math>\n\n::<math>(a_1 b_4 + a_2 b_3 - a_3 b_2 + a_4 b_1)^2.</math>\n\n[[Leonhard Euler|Euler]] wrote about this identity in a letter dated May 4, 1748 to [[Christian Goldbach|Goldbach]]<ref>''Leonhard Euler: Life, Work and Legacy'', R.E. Bradley and C.E. Sandifer (eds), Elsevier, 2007, p. 193</ref><ref>''Mathematical Evolutions'', A. Shenitzer and J. Stillwell (eds), Math. Assoc. America, 2002, p. 174</ref> (but he used a different sign convention from the above). It can be proven with [[elementary algebra]]. \n\nThe identity was used by [[Joseph Louis Lagrange|Lagrange]] to prove his [[Lagrange's four-square theorem|four square theorem]]. More specifically, it implies that it is sufficient to prove the theorem for [[prime numbers]], after which the more general theorem follows. The sign convention used above corresponds to the signs obtained by multiplying two quaternions. Other sign conventions can be obtained by changing any <math>a_k</math> to <math>-a_k</math>, and/or any <math>b_k</math> to <math>-b_k</math>.\n\nIf the <math>a_k</math> and <math>b_k</math> are [[real number]]s, the identity expresses the fact that the absolute value of the product of two [[quaternion]]s is equal to the product of their absolute values, in the same way that the [[Brahmagupta–Fibonacci identity|Brahmagupta–Fibonacci two-square identity]] does for [[complex numbers]]. This property is the definitive feature of [[composition algebra]]s.\n\n[[Hurwitz's theorem (normed division algebras)|Hurwitz's theorem]] states that an identity of form,\n\n:<math>(a_1^2+a_2^2+a_3^2+...+a_n^2)(b_1^2+b_2^2+b_3^2+...+b_n^2) = c_1^2+c_2^2+c_3^2+...+c_n^2</math>\n\nwhere the <math>c_i</math> are [[bilinear map|bilinear]] functions of the <math>a_i</math> and <math>b_i</math> is possible only for ''n'' = 1, 2, 4, or 8.\n\n===Proof of the identity using quaternions===\nLet \n: <math>\\alpha = a_1 + a_2 i + a_3 j + a_4 k</math> \nand \n: <math>\\beta = b_1 + b_2 i + b_3 j + b_4 k</math> \nbe a pair of quaternions. Their quaternion conjugates are\n:<math> \\alpha^* = a_1 - a_2 i - a_3 j - a_4 k </math>\nand \n:<math> \\beta^* = b_1 - b_2 i - b_3 j - b_4 k</math>.\nThen\n:<math> A = \\alpha \\alpha^* = a_1^2 + a_2^2 + a_3^2 + a_4^2</math>\nand\n:<math> B = \\beta \\beta^* = b_1^2 + b_2^2 + b_3^2 + b_4^2</math>.\n\nThe product of these two is\n:<math>A B = \\alpha \\alpha^* \\beta \\beta^* </math>\nwhere <math>\\beta \\beta^*</math> is a real number, so it can commute with the quaternion <math>\\alpha^*</math> yielding\n:<math>A B = \\alpha \\beta \\beta^* \\alpha^*</math>.\n(No round brackets are used because quaternions are associative.) The conjugate of a product is equal to the commuted product of the conjugates (of the factors), so\n:<math>A B = \\alpha \\beta (\\alpha \\beta)^* = \\gamma \\gamma^*</math>\nwhere <math>\\gamma</math> is the Hamilton product of <math>\\alpha</math> and <math>\\beta</math>:\n:<math>\\gamma = ( a_1 + \\langle a_2, a_3, a_4 \\rangle) (b_1 + \\langle b_2, b_3, b_4 \\rangle) </math>\n:<math> \\qquad = a_1 b_1 + a_1 \\langle b_2, \\ b_3, \\ b_4\\rangle + \\langle a_2, \\ a_3, \\ a_4\\rangle b_1 + \\langle a_2, \\ a_3, \\ a_4\\rangle \\langle b_2, \\ b_3, \\ b_4\\rangle</math>\n:<math> \\qquad = a_1 b_1 + \\langle a_1 b_2, \\ a_1 b_3, \\ a_1 b_4\\rangle + \\langle a_2 b_1, \\ a_3 b_1, \\ a_4 b_1\\rangle - \\langle a_2,\\  a_3, \\ a_4\\rangle \\cdot \\langle b_2, \\ b_3, \\ b_4\\rangle + \\langle a_2, \\ a_3, \\ a_4\\rangle \\times \\langle b_2, \\ b_3, \\  b_4\\rangle</math>\n:<math> \\qquad = a_1 b_1 + \\langle a_1 b_2 + a_2 b_1, \\ a_1 b_3 + a_3 b_1, \\ a_1 b_4 + a_4 b_1\\rangle - a_2 b_2 - a_3 b_3 - a_4 b_4 + \\langle a_3 b_4 - a_4 b_3, \\ a_4 b_2 - a_2 b_4, \\ a_2 b_3 - a_3 b_2\\rangle</math>\n:<math>\\qquad = (a_1 b_1 - a_2 b_2 - a_3 b_3 - a_4 b_4) + \\langle a_1 b_2 + a_2 b_1 + a_3 b_4 - a_4 b_3, \\ a_1 b_3 + a_3 b_1 + a_4 b_2 - a_2 b_4, \\ a_1 b_4 + a_4 b_1 + a_2 b_3 - a_3 b_2\\rangle</math>\n:<math>\\gamma = (a_1 b_1 - a_2 b_2 - a_3 b_3 - a_4 b_4) + (a_1 b_2 + a_2 b_1 + a_3 b_4 - a_4 b_3) i + (a_1 b_3 + a_3 b_1 + a_4 b_2 - a_2 b_4) j + (a_1 b_4 + a_4 b_1 + a_2 b_3 - a_3 b_2) k </math>\nThen\n:<math>\\gamma^* = (a_1 b_1 - a_2 b_2 - a_3 b_3 - a_4 b_4) - (a_1 b_2 + a_2 b_1 + a_3 b_4 - a_4 b_3) i - (a_1 b_3 + a_3 b_1 + a_4 b_2 - a_2 b_4) j - (a_1 b_4 + a_4 b_1 + a_2 b_3 - a_3 b_2) k </math>\nand\n:<math> A B = \\gamma \\gamma^* = (a_1 b_1 - a_2 b_2 - a_3 b_3 - a_4 b_4)^2 + (a_1 b_2 + a_2 b_1 + a_3 b_4 - a_4 b_3)^2 + (a_1 b_3 + a_3 b_1 + a_4 b_2 - a_2 b_4)^2 + (a_1 b_4 + a_4 b_1 + a_2 b_3 - a_3 b_2)^2</math>.\n(If <math>\\gamma = r + \\vec u</math> where ''r'' is the scalar part and <math>\\vec u = \\langle u_1, u_2, u_3\\rangle</math> is the vector part, then <math>\\gamma^* = r - \\vec u</math> so <math>\\gamma \\gamma^* = (r + \\vec u) (r - \\vec u) = r^2 - r \\vec u + r \\vec u - \\vec u \\vec u = r^2 + \\vec u \\cdot \\vec u - \\vec u \\times \\vec u = r^2 + \\vec u \\cdot \\vec u = r^2 + u_1^2 + u_2^2 + u_3^2.</math>)\n\n==Pfister's identity==\nPfister found another square identity for any even power:<ref>Keith Conrad [http://www.math.uconn.edu/~kconrad/blurbs/linmultialg/pfister.pdf Pfister's Theorem on Sums of Squares] from [[University of Connecticut]]</ref> \n\nIf the <math>c_i</math> are just [[rational functions]] of one set of variables, hence has a [[denominator]], then it is possible for all <math>n = 2^m</math>.\n\nThus, a different kind of four-square identity can be given as,\n:<math>(a_1^2+a_2^2+a_3^2+a_4^2)(b_1^2+b_2^2+b_3^2+b_4^2)=</math>\n\n::<math>(a_1 b_4 + a_2 b_3 + a_3 b_2 + a_4 b_1)^2 +</math>\n\n::<math>(a_1 b_3 - a_2 b_4 + a_3 b_1 - a_4 b_2)^2 +</math>\n\n::<math>\\left(a_1 b_2 + a_2 b_1 + \\frac{a_3 u_1}{b_1^2+b_2^2} - \\frac{a_4 u_2}{b_1^2+b_2^2}\\right)^2+</math>\n\n::<math>\\left(a_1 b_1 - a_2 b_2 - \\frac{a_4 u_1}{b_1^2+b_2^2} - \\frac{a_3 u_2}{b_1^2+b_2^2}\\right)^2</math>\n\nwhere,\n\n:<math>u_1 = b_1^2b_4-2b_1b_2b_3-b_2^2b_4</math>\n\n:<math>u_2 = b_1^2b_3+2b_1b_2b_4-b_2^2b_3</math>\n\nNote also the incidental fact that,\n\n:<math>u_1^2+u_2^2 = (b_1^2+b_2^2)^2(b_3^2+b_4^2)</math>\n\n==See also==\n*[[Brahmagupta–Fibonacci identity]] (sums of two squares)\n*[[Degen's eight-square identity]]\n*[[Pfister's sixteen-square identity]]\n*[[Latin square]]\n\n==References==\n<references/>\n\n==External links==\n*[http://sites.google.com/site/tpiezas/005b/ A Collection of Algebraic Identities]\n*[http://math.dartmouth.edu/~euler/correspondence/letters/OO0841.pdf] Lettre CXV from Euler to Goldbach\n\n{{DEFAULTSORT:Euler's Four-Square Identity}}\n[[Category:Elementary algebra]]\n[[Category:Elementary number theory]]\n[[Category:Mathematical identities]]\n[[Category:Squares in number theory]]\n[[Category:Leonhard Euler]]"
    },
    {
      "title": "Exterior calculus identities",
      "url": "https://en.wikipedia.org/wiki/Exterior_calculus_identities",
      "text": "This article summarizes important [[Identity (mathematics)|identities]] in [[exterior calculus]].<ref>{{Cite book |last1=Crane |first1=Keenan |last2=de Goes |first2=Fernando |last3=Desbrun |first3=Mathieu |last4=Schröder |first4=Peter |title=Digital geometry processing with discrete exterior calculus |journal=Proceeding SIGGRAPH '13 ACM SIGGRAPH 2013 Courses |pages=1–126 |date=21 July 2013 |doi=10.1145/2504435.2504442|isbn=9781450323390 }}</ref><ref>{{cite book |last1=Schwarz |first1=Günter |title=Hodge Decomposition – A Method for Solving Boundary Value Problems |date=1995 |publisher=Springer |isbn=978-3-540-49403-4}}</ref><ref>{{cite book |last1=Cartan |first1=Henri |title=Differential forms |publisher=Dover Publications |isbn=978-0486450100 |edition=Dover}}</ref><ref>{{cite book |last1=Bott |first1=Raoul |last2=Tu |first2=Loring W. |title=Differential forms in algebraic topology |publisher=Springer |isbn=978-0387906133}}</ref><ref>{{cite book |last1=Abraham |first1=Ralph |last2=J.E. |first2=Marsden |last3=Ratiu |first3=Tudor |title=Manifolds, tensor analysis, and applications |publisher=Springer-Verlag |isbn=978-1-4612-1029-0 |edition=2nd}}</ref>\n<!-- Comment \nvector valued differential forms?\n\nprojection rejection and tangen and normals\n\nsources : albert phd thesis, crane lecture notes, günter schwarz hodge decomp\n-->\n<!-- Comment -->\n\n== Notation ==\nThe following summarizes short definitions and notations that are used in this article.\n\n=== Manifold ===\n<math>M</math>, <math>N</math> are <math>n</math>-dimensional smooth manifolds, where <math> n\\in \\mathbb{N} </math>. That is, [[differentiable manifold]]s that can be differentiated enough times for the purposes on this page.\n\n<math> p \\in M </math>, <math> q \\in N </math> denote two points on the manifolds.\n\n<math>TM</math> is the [[tangent bundle]] of the smooth manifold <math>M</math>.\n\n<math> T_p M </math>, <math> T_q N </math> denote the [[tangent space]]s of <math>M</math>, <math>N</math> at the points <math>p</math>, <math>q</math>, respectively.\n\n[[Section (fiber bundle)|Sections]] of the tangent bundles, also known as [[vector field]]s, are typically denoted as <math>X, Y, Z \\in \\Gamma(TM)</math> such that at a point <math> p \\in M </math> we have <math> X|_p, Y|_p, Z|_p \\in T_p M </math>.\n\nGiven an [[Inner product space|inner product]] <math> g( \\cdot , \\cdot  )_p </math> on each <math> T_p M </math>, the manifold becomes a [[Riemannian manifold]].\n\nThe boundary of a [[manifold]] <math> M </math> is a manifold <math> \\partial M </math>, which has dimension <math> n - 1 </math>.  An orientation on <math> M </math> induces an orientation on <math> \\partial M </math>.\n\nWe usually denote a [[submanifold]] by <math>\\Sigma \\subset M</math>.\n\n=== ''k''-forms ===\n\n<math>k</math>-forms are [[differential form]]s defined on <math>TM</math>. We denote the set of all <math>k</math>-forms as <math>\\Omega^k(M)</math>. For <math> 0\\leq k,\\ l,\\ m\\leq n </math> we usually write <math>\\alpha\\in\\Omega^k(M)</math>, <math>\\beta\\in\\Omega^l(M)</math>, <math>\\gamma\\in\\Omega^m(M)</math>.\n\n<math>0</math>-forms <math>f\\in\\Omega^0(M)</math> are just scalar functions <math>C^{\\infty}(M)</math> on <math>M</math>. <math>\\mathbf{1}\\in\\Omega^0(M)</math> denotes the constant <math>0</math>-form equal to <math>1</math> everywhere.\n\n=== Omitted elements of a sequence ===\n\nWhen we are given <math>(k+1)</math> inputs <math>X_0,\\ldots,X_k</math> and a <math>k</math>-form <math>\\alpha\\in\\Omega^k(M)</math> we omit the <math>i</math>th entry by writing\n\n:<math>\\alpha(X_0,\\ldots,\\hat{X}_i,\\ldots,X_k):=\\alpha(X_0,\\ldots,X_{i-1},X_{i+1},\\ldots,X_k) .</math>\n\n=== Exterior product ===\n\nThe [[exterior product]] is also known as the ''wedge product''. It is denoted by <math> \\wedge : \\Omega^k(M) \\times \\Omega^l(M) \\rightarrow \\Omega^{k+l}(M)</math>. The exterior product of a <math>k</math>-form <math>\\alpha\\in\\Omega^k(M)</math> and an <math>l</math>-form <math>\\beta\\in\\Omega^l(M)</math> produce a <math>(k+l)</math>-form <math>\\alpha\\wedge\\beta \\in\\Omega^{k+l}(M)</math>. It can be written using the set <math>S(k,k+l)</math> of all permutations <math>\\sigma</math> of <math>\\{1,\\ldots,n\\}</math> such that <math>\\sigma(1)<\\ldots <\\sigma(k), \\ \\sigma(k+1)<\\ldots <\\sigma(k+l) </math> as\n\n:<math>(\\alpha\\wedge\\beta)(X_1,\\ldots,X_{k+l})=\\sum_{\\sigma\\in S(k,k+l)}\\text{sign}(\\sigma)\\alpha(X_{\\sigma(1)},\\ldots,X_{\\sigma(k)})\\beta(X_{\\sigma(k+1)},\\ldots,X_{\\sigma(k+l)}) .</math>\n\n=== Lie bracket ===\n\nThe [[Lie bracket of vector fields | Lie bracket]] of sections <math>X,Y \\in \\Gamma(TM)</math> is defined as the unique section <math>[X,Y] \\in \\Gamma(TM)</math> that satisfies\n\n:<math>\n\\forall f\\in\\Omega^0(M) \\Rightarrow [X,Y]f = XYf-YXf .\n</math>\n\n=== Exterior derivative ===\n\nThe [[exterior derivative]] <math>d_k : \\Omega^k(M) \\rightarrow \\Omega^{k+1}(M) </math> is defined for all <math> 0 \\leq k\\leq n</math>.  We generally omit the subscript when it is clear from the context.\n<!--Its defining integral property is that for any <math>k+1</math> dimensional submanifold <math>\\Sigma\\subset M</math> and <math>k</math>-form <math>\\alpha\\in\\Omega^k(M)</math> \n:<math> \\int_{\\Sigma} d\\alpha = \\int_{\\partial\\Sigma} \\alpha </math> -->\n\nFor a <math>0</math>-form <math>f\\in\\Omega^k(M)</math> we have <math>d_0f\\in\\Omega^1(M)</math> as the directional derivative <math>1</math>-form. i.e. in the direction <math>X\\in T_pM</math> we have <math>(d_0f)(X)=Xf</math>.{{citation needed|reason=The notation Xf is only defined in words, and its use here is confusing.|date=November 2018}}\n\nFor <math> 0 < k\\leq n</math>,\n:<math> (d_k\\omega)(X_0,\\ldots,X_k)=\\sum_{0\\leq j\\leq k}(-1)^jd_{k-1}(\\omega(X_0,\\ldots,\\hat{X}_j,\\ldots,X_k))(X_j) + \\sum_{0\\leq i < j\\leq k}(-1)^{i+j}\\omega([X_i,X_j],X_0,\\ldots,\\hat{X}_i,\\ldots,\\hat{X}_j,\\ldots,X_k) .</math>\n\n=== Tangent maps ===\n\nIf <math> \\phi : M \\rightarrow N </math> is a smooth map, then <math>(d\\phi)_p:T_pM\\rightarrow T_{\\phi(p)}N</math> defines a tangent map from <math>M</math> to <math>N</math>. It is defined through curves <math>\\gamma</math> on <math>M</math> with derivative <math>\\gamma'(0)=X\\in T_pM</math> such that\n\n:<math>d\\phi(X):=(\\phi\\circ\\gamma)' .</math>\n\nNote that <math>\\phi</math> is a <math>0</math>-form with values in <math>N</math>.\n\n=== Pull-back ===\n\nIf <math> \\phi : M \\rightarrow N </math> is a smooth map, then the [[Pullback_(differential_geometry) | pull-back]] of a <math>k</math>-form <math> \\alpha\\in \\Omega^k(N) </math> is defined such that for any <math>k</math> dimensional submanifold <math>\\Sigma\\subset M</math>\n\n:<math> \\int_{\\Sigma} \\phi^*\\alpha = \\int_{\\phi(\\Sigma)} \\alpha .</math>\n\nThe pull-back can also be expressed as\n\n:<math>(\\phi^*\\alpha)(X_1,\\ldots,X_k)=\\alpha(d\\phi(X_1),\\ldots,d\\phi(X_k)) .</math>\n\n=== Musical isomorphisms ===\n\nGiven a section <math> A \\in \\Gamma(TM)</math> there exists a <math>1</math>-form <math>A^{\\flat}\\in\\Omega^1(M)</math> such that on each <math>p \\in M</math>\n\n:<math>\\forall X \\in T_p M \\Rightarrow A^{\\flat}(X) = g(A,X)_p .</math>\n\nWe call this mapping the flat operator <math>\\flat</math>.\n\nGiven a <math>1</math>-form <math>\\alpha\\in\\Omega^1(M)</math> there exists a section <math> \\alpha^{\\sharp}\\in \\Gamma(TM)</math> such that on each <math>p\\in M</math>\n\n:<math>\\forall X \\in T_pM \\Rightarrow g(\\alpha|_p^{\\sharp},X)_p = \\alpha|_p(X) .</math>\n\nWe call this mapping the sharp operator <math>\\sharp</math>.  <math>\\sharp</math> and <math>\\flat</math> constitute the [[musical isomorphisms]].\n\n=== Interior product ===\n\nAlso known as the interior derivative, the [[interior product]] given a section <math> Y\\in \\Gamma(TM) </math> is a map <math>\\iota_Y:\\Omega^{k+1}(M) \\rightarrow \\Omega^k(M)</math> that effectively substitutes the first input of a <math>(k+1)</math>-form with <math>Y</math>. If <math>\\alpha\\in\\Omega^{k+1}(M)</math> and <math>X_i\\in \\Gamma(TM)</math> then\n\n:<math> (\\iota_Y\\alpha)(X_1,\\ldots,X_k) = \\alpha(Y,X_1,\\ldots,X_k) .</math>\n\n=== Hodge star ===\n\nThe [[Hodge star operator]] <math>{\\star}:\\Omega^k(M)\\rightarrow\\Omega^{n-k}(M)</math> is defined as such that it maps <math>k</math>-forms <math>\\alpha \\in \\Omega^k(M)</math> to their dual <math>(n-k)</math>-form <math>({\\star}\\alpha) \\ \\Omega^{n-k}(M)</math>.\n\nFor example, if <math>(X_1,\\ldots,X_n)</math> is a positively oriented frame for <math>TM</math> according to the given metric <math>g</math>, then\n\n:<math>\n({\\star}\\alpha)(X_1,\\ldots,X_{n-k})=\\alpha(X_{n-k+1},\\ldots,X_n) .\n</math>\n\nWe omit to write the dimension <math>k</math> or inversion symbol <math>-1</math> with the Hodge star operator as it is evident in the context. <!--Note that <math>{\\star}^{(-1)} = (-1)^{k(n-k)}</math>.-->\n\nWe call <math>s=\\text{sign}(g)</math> the [[Hodge_star_operator#Duality | signature]] of the metric <math>g</math>. For example in [[Minkowski_space | Minkowski space]] <math>s=-1</math> and in Riemannian manifolds <math>s=1</math>.\n\n=== Co-differential operator ===\n\nThe [[Hodge_star_operator#Codifferential|co-differential operator]] <math>\\delta:\\Omega^k(M)\\rightarrow\\Omega^{k-1}(M)</math> on an <math>n</math> dimensional manifold <math>M</math> is defined by\n\n:<math>\\delta:=(-1)^{nk+n+1}{\\star} d {\\star} .</math>\n\n=== Oriented manifold ===\n\nAn <math>n</math>-dimensional [[orientable manifold]] <math>M</math> is a manifold that can be equipped with a choice of a non-zero <math>n</math>-form <math>\\mu\\in\\Omega^n(M)</math>.\n\n=== Volume form ===\n\nOn a orientable manifold <math>M</math> the canonical choice of a [[ Volume_form | volume form ]] given a metric <math>g</math> is <math>\\mathbf{det}:=\\sqrt{\\det g}\\;dX_1^{\\flat}\\wedge\\ldots\\wedge dX_n^{\\flat}</math> for any [[Orientation_(vector_space)#Multilinear_algebra | positively oriented]] basis <math>dX_1,\\ldots, dX_n</math>.\n\n=== Area form ===\n\nGiven a volume form <math>\\mathbf{det}</math> and a unit normal vector <math>N</math> we can also define an area form <math>\\sigma:=\\iota_N\\textbf{det}</math> on the {{nowrap|boundary <math>\\partial M.</math>}}\n\n=== Inner product for ''k''-forms ===\n\nThe [[inner product]] between two <math>k</math>-forms <math>\\alpha,\\beta\\in\\Omega^k(M)</math> is defined [[pointwise]] on <math>M</math> by\n\n:<math>\n\\langle\\alpha,\\beta\\rangle|_p := {\\star}(\\alpha\\wedge {\\star}\\beta )|_p .\n</math>\n\nThe <math>L^2</math>-inner product for the space of <math>k</math>-forms <math>\\Omega^k(M)</math> is defined by\n\n:<math>\n\\langle\\!\\langle\\alpha,\\beta\\rangle\\!\\rangle:= \\int_M\\alpha\\wedge {\\star}\\beta .\n</math>\n\n=== Lie derivative ===\n\nWe define the [[Lie derivative]] <math>\\mathcal{L}:\\Omega^k(M)\\rightarrow\\Omega^k(M)</math> through [[Cartan's magic formula]] for a given section <math>X\\in \\Gamma(TM)</math> as\n\n:<math>\n\\mathcal{L}_X = d \\circ \\iota_X + \\iota_X \\circ d .\n</math>\n\nIt describes the change of a <math>k</math>-form along a flow map <math>\\phi_t</math> associated to the section <math>X</math>.\n<!--\n==== Levi-Civita connection ====\n\nThe [[Levi-Civita connection]] is the unique map <math>\\nabla : \\Gamma(TM)\\times\\Gamma(TM) \\rightarrow \\Gamma(TM) </math> that is ''affine'', ''compatible with the given metric'' and ''torsion free''.\n\nWe write <math>\\nabla(X,Y)=\\nabla_XY</math>.\n\n==== Covariant derivative ====\n\nThe generalization of the Levi-Civita connection to be applied on <math>k</math>-forms is called the [[covariant derivative]]. Typically the same symbol <math>\\nabla</math> is used since it acts on forms in <math>\\Omega^k(M)</math> and not on sections in <math>\\Gamma(TM)</math>. It is defined as a map\n\n<math>\n\\nabla: \\Gamma(TM)\\times\\Omega^k(M)\\rightarrow\\Omega^k(M)\n</math>\n\n<math>\n(\\nabla_Y\\alpha):= d(\\alpha(X_1,\\ldots,X_k))(Y) - \\sum_{i=1}^k\\alpha(X_1,\\ldots,\\nabla_YX_i,\\ldots,X_k) .\n</math>\n\nNote <math>\\alpha(X_1,\\ldots,X_k)</math> is a <math>0</math>-form (scalar function) as it is evaluated on the input and that <math>d(\\alpha(X_1,\\ldots,X_k))(Y)</math> is a directional derivative of the scalar function <math>\\alpha(X_1,\\ldots,X_k)</math> along <math>Y</math>.-->\n\n=== Laplace–Beltrami operator ===\n\nThe [[Laplace operator | Laplacian]] <math>\\Delta:\\Omega^k(M) \\rightarrow \\Omega^k(M)</math> is defined as <math>\\Delta = -(d\\delta + \\delta d)</math>.\n\n== Important Definitions ==\n\n=== Definitions on Ω<sup>''k''</sup>(''M'') ===\n\n<math>\\alpha\\in\\Omega^k(M)</math> is called...\n\n* ''closed'' if <math>d\\alpha=0</math>\n* ''exact'' if <math>\\exists \\beta\\in\\Omega^{k-1}: \\ \\alpha = d\\beta</math>\n* ''coclosed'' if <math>\\delta\\alpha=0</math>\n* ''coexact'' if <math>\\exists \\beta\\in\\Omega^{k+1}: \\ \\alpha = \\delta\\beta</math>\n* ''harmonic'' if ''closed'' and ''coclosed''\n\n=== Cohomology ===\n\nThe <math>k</math>-th [[cohomology]] of a manifold <math>M</math> and its exterior derivative operators <math>d_0,\\ldots,d_{n-1}</math> is given by\n\n:<math>\nH^k(M):=\\tfrac{\\text{ker}(d_{k})}{\\text{im}(d_{k-1})}\n</math>\n\nTwo closed <math>k</math>-forms <math>\\alpha,\\beta\\in\\Omega^k(M)</math> are in the same cohomology class if their difference is an exact form i.e.\n\n:<math>\n[\\alpha]=[\\beta] \\Leftrightarrow \\exist \\ \\eta\\in\\Omega^{k-1}(M) \\ : \\ \\alpha -\\beta = d\\eta\n</math>\n\nA closed surface of genus <math>g</math> will have <math>2g</math> generators which are harmonic.\n\n=== [[Dirichlet energy]] ===\n\nGiven <math>\\alpha\\in\\Omega^k(M)</math>\n\n:<math>\n\\mathcal{E}_D(\\alpha):= \\dfrac{1}{2}\\langle\\!\\langle d\\alpha,d\\alpha\\rangle\\!\\rangle + \\dfrac{1}{2}\\langle\\!\\langle \\delta\\alpha,\\delta\\alpha\\rangle\\!\\rangle\n</math>\n<!--\n==== Definitions on metric ''g'' ====\n\nGiven Riemannian manifold <math>M</math> with metric <math>g</math> and immersion <math>f</math>. We call <math>g</math>...\n\n* isometric if <math>g(df(X),df(X)) =  g(X,X)</math>\n* conformal if <math>g(df(X),df(Y)) = \\lambda g(X,Y)</math>\n-->\n<!--\n==== Lagrangian derivative ====\n\nGiven a flow map <math>\\phi_t</math> from a section <math>X\\in\\Gamma(TM)</math> on <math>M</math>, the Lagrangian derivative of a time dependent <math>\\alpha\\in\\Omega^k(M)</math> is defined as\n\n:<math>\nD_X(\\alpha) := \\left. \\frac{\\partial}{\\partial t}\\right|_{t=0}(\\phi^*\\alpha) = \\left. \\frac{\\partial}{\\partial t}\\right|_{t=0}\\alpha + \\mathcal{L}_X\\alpha\n</math>\n\nIt is not the same as the [[material derivative]].-->\n\n== Properties ==\n\n=== Exterior derivative properties ===\n\n:<math>\n\\int_{\\Sigma} d\\alpha = \\int_{\\partial\\Sigma} \\alpha </math> ( ''[[Stokes' theorem]]'' )\n\n:<math>\nd \\circ d  = 0\n</math> ( ''Nilpotent'' )\n\n:<math>\nd(\\alpha \\wedge \\beta ) = d\\alpha\\wedge \\beta +(-1)^k\\alpha\\wedge d\\beta\n</math> if <math> \\alpha\\in\\Omega^k(M), \\ \\beta\\in\\Omega^l(M) </math> ( ''Leibniz rule'' )\n\n:<math>\ndf(X) = Xf\n</math> if <math> f\\in\\Omega^0(M), \\ X\\in \\Gamma(TM) </math> ( ''Directional derivative'' )\n\n:<math>\nd\\alpha = 0\n</math> if <math>\\alpha \\in \\Omega^n(M), \\ \\text{dim}(M)=n </math>\n\n=== Exterior product properties ===\n\n:<math>\n\\alpha \\wedge \\beta = (-1)^{kl}\\beta \\wedge \\alpha\n</math> if <math> \\alpha\\in\\Omega^k(M), \\ \\beta\\in\\Omega^l(M) </math> ( ''Anticommutative'' )\n\n:<math>\n(\\alpha \\wedge \\beta)\\wedge\\gamma = \\alpha \\wedge (\\beta\\wedge\\gamma)\n</math> ( ''Associativity'' )\n\n:<math>\n(\\lambda\\alpha) \\wedge \\beta = \\lambda (\\alpha \\wedge \\beta)\n</math> for <math>\\lambda\\in\\mathbb{R}</math> ( ''Distributivity of scalar multiplication'' )\n\n:<math>\n\\alpha \\wedge ( \\beta_1 + \\beta_2 ) = \\alpha \\wedge \\beta_1 + \\alpha \\wedge \\beta_2\n</math> ( ''Distributivity over addition'' )\n\n:<math>\n\\alpha \\wedge \\alpha = 0\n</math> when <math>\\operatorname{rank} \\alpha \\le 1</math>.  The [[Exterior algebra#Rank of a k-vector|rank of a <math>k</math>-form]] <math>\\alpha</math> is defined as the minimum number of terms, each consisting of the exterior product of <math>k</math> <math>1</math>-forms, that can be summed to produce <math>\\alpha</math>.  <!-- this result generalizes to form of higher rank; it would be nice to put the general result here. For 2-forms, the rank can be determined as the largest number of copies wedged together that produces a non-zero result, and this might apply unchanged to other forms. -->\n\n=== Pull-back properties ===\n\n:<math>\nd(\\phi^*\\alpha) = \\phi^*(d\\alpha)\n</math> ( ''Commutative with <math>d</math>'' )\n\n:<math>\n\\phi^*(\\alpha\\wedge\\beta) = (\\phi^*\\alpha)\\wedge(\\phi^*\\beta)\n</math> ( ''Distributes over <math>\\wedge</math>'' )\n\n:<math>\n(\\phi_1\\circ\\phi_2)^* = \\phi_2^*\\phi_1^*\n</math> ( ''Contravariant'' )\n\n:<math>\n\\phi^*f=f\\circ\\phi\n</math> for <math>f\\in\\Omega^0(N)</math> ( ''Function composition'' )\n\n=== Musical isomorphism properties ===\n\n:<math>\n(X^{\\flat})^{\\sharp}=X\n</math>\n\n:<math>\n(\\alpha^{\\sharp})^{\\flat}=\\alpha\n</math>\n\n=== Inner product properties ===\n\n:<math>\n\\iota_X \\circ \\iota_X = 0\n</math> ( ''Nilpotent'' )\n\n:<math>\n\\iota_X \\circ \\iota_Y = - \\iota_Y \\circ \\iota_X\n</math>\n\n:<math>\n\\iota_X (\\alpha \\wedge \\beta ) = (\\iota_X\\alpha)\\wedge\\beta + (-1)^k\\alpha\\wedge(\\iota_X \\beta ) = 0 \n</math> for <math>\\alpha\\in\\Omega^k(M), \\ \\beta\\in\\Omega^l(M)</math> ( ''Leibniz rule'' )\n\n:<math>\n\\iota_X\\alpha = \\alpha(X)\n</math> for <math>\\alpha\\in\\Omega^1(M)</math>\n\n:<math>\n\\iota_X f = 0\n</math> for <math>f \\in \\Omega^0(M)</math>\n\n:<math>\n\\iota_X(f\\alpha) = f \\iota_X\\alpha\n</math> for <math>f \\in \\Omega^0(M)</math>\n\n=== Hodge star properties ===\n\n:<math>\n{\\star}(\\lambda_1\\alpha + \\lambda_2\\beta) = \\lambda_1({\\star}\\alpha) + \\lambda_2({\\star}\\beta)\n</math> for <math>\\lambda_1,\\lambda_2\\in\\mathbb{R}</math> ( ''Linearity'' )\n\n:<math>\n{\\star}{\\star}\\alpha = s(-1)^{k(n-k)}\\alpha\n</math>\n\n:<math>\n{\\star}^{(-1)} = s(-1)^{k(n-k)}{\\star}\n</math> ( ''Inversion'' )\n\n:<math>\n{\\star}(f\\alpha)=f({\\star}\\alpha)\n</math> for <math>f\\in\\Omega^0(M)</math> ( ''Commutative with <math>0</math>-forms'' )\n\n:<math>\n\\langle\\!\\langle\\alpha,\\alpha\\rangle\\!\\rangle = \\langle\\!\\langle{\\star}\\alpha,{\\star}\\alpha\\rangle\\!\\rangle\n</math> for <math>\\alpha\\in\\Omega^1(M)</math> ( ''Hodge star preserves <math>1</math>-form norm '' )\n\n:<math>\n{\\star} \\mathbf{1} = \\mathbf{det}\n</math> ( ''The Hodge dual of the constant function 1 is the volume form'' )\n\n=== Co-differential operator properties ===\n\n:<math>\n\\delta\\circ\\delta = 0\n</math> ( ''Nilpotent'' )\n\n:<math>\n{\\star}\\delta=(-1)^kd{\\star} \\text{ and } {\\star} d = (-1)^{k+1}\\delta{\\star}\n</math> ( ''Hodge adjoint to <math>d</math>'' )\n\n:<math>\n\\langle\\!\\langle d\\alpha,\\beta\\rangle\\!\\rangle = \\langle\\!\\langle \\alpha,\\delta\\beta\\rangle\\!\\rangle\n</math> if <math>\\partial M=0</math> ( ''<math>\\delta</math> adjoint to <math>d</math>'' )\n\n:<math>\n\\delta f = 0\n</math> if <math>f \\in \\Omega^0(M)</math>\n\n=== Lie derivative properties ===\n\n:<math>\nd\\circ\\mathcal{L}_X = \\mathcal{L}_X\\circ d\n</math> ( ''Commutative with <math>d</math>'' )\n\n:<math>\n\\iota_X \\circ\\mathcal{L}_X = \\mathcal{L}_X\\circ \\iota_X\n</math> ( ''Commutative with <math>\\iota_X</math>'' )\n\n:<math>\n\\mathcal{L}_X(\\iota_Y\\alpha) = \\iota_{[X,Y]}\\alpha + \\iota_Y\\mathcal{L}_X\\alpha\n</math>\n\n:<math>\n\\mathcal{L}_X(\\alpha\\wedge\\beta) = (\\mathcal{L}_X\\alpha)\\wedge\\beta + \\alpha\\wedge(\\mathcal{L}_X\\beta)\n</math> ( ''Leibniz rule'' )\n<!--\n==== Levi-Civita connection properties ====\n\n:<math>\nf\\in\\Omega^0(M) \\Rightarrow \\nabla_{fX}Y=f\\nabla_XY, \\ \\ \\ \\nabla_{X}(fY) = (Xf)Y + f\\nabla_XY\n</math> ( ''Affine'' )\n\n:<math>\nXg(Y,Z) = g(\\nabla_X Y,Z) + g(Y,\\nabla_XZ)\n</math> ( ''Compatible with metric'' )\n\n:<math>\n\\nabla_XY-\\nabla_YX = [X,Y]\n</math> ( ''Torsion free'' )\n\n==== Covariant derivative ====\n\n:<math>\n\\nabla_Y\\langle\\alpha,\\beta\\rangle = \\langle\\nabla_Y\\alpha,\\beta\\rangle + \\langle\\alpha,\\nabla_Y\\beta\\rangle\n</math> ( ''Compatibile with metric <math>\\langle\\alpha,\\beta\\rangle={\\star}(\\alpha\\wedge{\\star}\\beta)</math>'' )\n\n:<math>\n\\nabla_Y({\\star}\\alpha)={\\star}(\\nabla_Y\\alpha)\n</math>-->\n\n== Exterior calculus identities ==\n\n:<math>\n\\iota_X({\\star}\\mathbf{1}) = {\\star} X^{\\flat}\n</math> if <math>f \\in \\Omega^0(M)</math>\n\n:<math>\n\\iota_X({\\star}\\alpha) = (-1)^k{\\star}(X^{\\flat}\\wedge\\alpha)\n</math> if <math>\\alpha\\in\\Omega^k(M)</math>\n\n:<math>\n\\iota_X(\\phi^*\\alpha)=\\phi^*(\\iota_{d\\phi(X)}\\alpha)\n</math>\n\n:<math>\n\\nu,\\mu\\in\\Omega^n(M), \\mu \\text{ non-zero } \\ \\Rightarrow \\ \\exist \\ f\\in\\Omega^0(M): \\ \\nu=f\\mu\n</math>\n\n:<math>\nX^{\\flat}\\wedge{\\star} Y^{\\flat} = g(X,Y)( {\\star} \\mathbf{1})\n</math> ( ''Inner product'' )\n\n:<math>\n[X,[Y,Z]]+[Y,[Z,X]]+[Z,[X,Y]] = 0\n</math> ([[Jacobi identity | ''Jacobi identity'' ]])\n\n=== Dimensions ===\nIf <math>n=\\dim M</math>\n\n:<math>\n\\dim\\Omega^k(M) = \\binom{n}{k}\n</math> for <math>0\\leq k\\leq n</math>\n\n:<math>\n\\dim\\Omega^k(M) = 0\n</math> for <math>k < 0, \\  k > n</math>\n\nIf <math>X_1,\\ldots,X_n\\in \\Gamma(TM)</math> is a basis, then a basis of <math>\\Omega^k(M)</math> is\n\n:<math>\n\\{X_{\\sigma(1)}^{\\flat}\\wedge\\ldots\\wedge X_{\\sigma(k)}^{\\flat} \\ : \\ \\sigma\\in S(k,n)\\}\n</math>\n\n=== Exterior products ===\n\n:<math>\n\\alpha(X) = \\det\n\\begin{bmatrix}\n    \\alpha(X) \\\\\n  \\end{bmatrix}\n</math>\n\n:<math>\n(\\alpha\\wedge\\beta)(X,Y) = \\det\n\\begin{bmatrix}\n    \\alpha(X) & \\alpha(Y) \\\\\n    \\beta(X)  & \\beta(Y) \\\\\n  \\end{bmatrix}\n</math>\n\n:<math>\n(\\alpha\\wedge\\beta\\wedge\\gamma)(X,Y,Z) = \\det\n\\begin{bmatrix}\n    \\alpha(X) & \\alpha(Y) & \\alpha(Z) \\\\\n    \\beta(X)  & \\beta(Y)  & \\beta(Z) \\\\\n    \\gamma(X) & \\gamma(Y) & \\gamma(Z)\n  \\end{bmatrix}\n</math>\n\n:<math>\n(\\alpha_1\\wedge\\ldots\\wedge\\alpha_l)(X_1,\\ldots,X_l) = \\det\n\\begin{bmatrix}\n    \\alpha_1(X_1) & \\alpha_1(X_2) & \\dots & \\alpha_1(X_l) \\\\\n    \\alpha_2(X_1) & \\alpha_2(X_2) & \\dots & \\alpha_2(X_l) \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\alpha_l(X_1) & \\alpha_l(X_2) & \\dots & \\alpha_l(X_l) \n  \\end{bmatrix}\n</math>\n\n=== Projection and rejection ===\n\n:<math>\n(-1)^k\\iota_X{\\star}\\alpha = {\\star}(X^{\\flat}\\wedge\\alpha)\n</math> ( ''Interior product <math>\\iota_X{\\star}</math> dual to wedge <math>X^{\\flat}\\wedge</math>'' )\n\n:<math>\n(\\iota_X\\alpha)\\wedge{\\star}\\beta =\\alpha\\wedge{\\star}(X^{\\flat}\\wedge\\beta)\n</math> for <math>\\alpha\\in\\Omega^{k+1}(M),\\beta\\in\\Omega^k(M)</math>\n\nIf <math>|X|=1, \\ \\alpha\\in\\Omega^k(M)</math>, then\n\n*<math>\\iota_X\\circ (X^{\\flat}\\wedge ):\\Omega^k(M)\\rightarrow\\Omega^k(M)</math> is the ''projection'' of <math>\\alpha</math> onto the orthogonal complement of <math>X</math>.\n\n*<math>(X^{\\flat}\\wedge )\\circ \\iota_X:\\Omega^k(M)\\rightarrow\\Omega^k(M)</math> is the ''rejection'' of <math>\\alpha</math>, the remainder of the projection.\n\n* thus <math> \\iota_X \\circ (X^{\\flat}\\wedge ) + (X^{\\flat}\\wedge)\\circ\\iota_X = \\text{id} </math> ( ''Projection rejection decomposition'' )\n\nGiven the boundary <math>\\partial M</math> with unit normal vector <math>N</math>\n\n*<math>\\mathbf{t}:=\\iota_N\\circ (N^{\\flat}\\wedge )</math> extracts the ''tangential component'' of the boundary.\n\n*<math>\\mathbf{n}:=(\\text{id}-\\mathbf{t})</math> extracts the ''normal component'' of the boundary.\n\n=== Sum expressions ===\n\n:<math>\n(d\\alpha)(X_0,\\ldots,X_k)=\\sum_{0\\leq j\\leq k}(-1)^jd(\\alpha(X_0,\\ldots,\\hat{X}_j,\\ldots,X_k))(X_j) + \\sum_{0\\leq i < j\\leq k}(-1)^{i+j}\\alpha([X_i,X_j],X_0,\\ldots,\\hat{X}_i,\\ldots,\\hat{X}_j,\\ldots,X_k)\n</math>\n\n:<math>\n(d\\alpha)(X_1,\\ldots,X_k) =\\sum_{i=1}^k(-1)^{i+1}(\\nabla_{X_i}\\alpha)(X_1,\\ldots,\\hat{X}_i,\\ldots,X_k)\n</math> <!-- ( ''Anti-symmetrization of tensor <math>\\nabla\\alpha</math>'' ) -->\n\n:<math>\n(\\delta\\alpha)(X_1,\\ldots,X_{k-1})=-\\sum_{i=1}^n(\\iota_{E_i}(\\nabla_{E_i}\\alpha))(X_1,\\ldots,\\hat{X}_i,\\ldots,X_k)\n</math> given a positively oriented orthonormal frame <math>E_1,\\ldots,E_n</math>.\n\n:<math>\n(\\mathcal{L}_Y\\alpha)(X_1,\\ldots,X_k) =(\\nabla_Y\\alpha)(X_1,\\ldots,X_k) - \\sum_{i=1}^k\\alpha(X_1,\\ldots,\\nabla_{X_i}Y,\\ldots,X_k)\n</math>\n\n=== [[Hodge decomposition]] ===\n\nIf <math>\\partial M =\\empty</math>, <math>\\omega\\in\\Omega^k(M) \\Rightarrow \\exists \\alpha\\in\\Omega^{k-1}, \\ \\beta\\in\\Omega^{k+1}, \\ \\gamma\\in\\Omega^k(M), \\ d\\gamma=0, \\ \\delta\\gamma = 0</math> such that\n\n:<math>\n\\omega = d\\alpha + \\delta\\beta + \\gamma\n</math>\n\n=== [[Poincaré lemma]] ===\n\nIf <math>M</math> has only one cohomology class <math>H^k(M)=\\{0\\}</math> and no boundary <math>\\partial M=\\empty</math>, then for any closed <math>\\omega\\in\\Omega^k(M) \\Rightarrow \\exists \\ \\alpha\\in\\Omega^{k-1}</math> such that\n\n:<math>\n\\omega = d\\alpha\n</math>\n\n== Relations to vector calculus ==\n{{See also|Vector calculus identities}}\n\n=== Identities in Euclidean 3-space ===\n\nLet [[Euclidean metric]] <math>g(X,Y):=\\langle X,Y\\rangle = X\\cdot Y</math>.\n\nWe use <math>\n\\nabla = \\left( {\\partial \\over \\partial x}, {\\partial \\over \\partial y}, {\\partial \\over \\partial z} \\right)\n</math> [[Del | differential operator]] <math>\\mathbb{R}^3</math>\n\n:<math>\n\\iota_X\\alpha = g(X,\\alpha^{\\sharp}) = X\\cdot \\alpha^{\\sharp}\n</math> for <math>\\alpha\\in\\Omega^1(M)</math>.\n\n:<math>\n\\text{det}(X,Y,Z)=\\langle X,Y\\times Z\\rangle = \\langle X\\times Y,Z\\rangle\n</math> ( ''[[Cross product]]'' )\n<!--\n* <math>\nX\\times Y = ({\\star}(X^{\\flat}\\wedge Y^{\\flat}))^{\\sharp}\n</math> \n\n* <math>\n\\gamma = {\\star}(\\alpha\\wedge\\beta) \\Rightarrow  \\gamma(X\\times Y)=(\\alpha\\wedge\\beta)(X,Y)\n</math>-->\n\n* <math>\n{\\star}(\\alpha\\wedge\\beta) = \\alpha^{\\sharp}\\times\\beta^{\\sharp}\n</math>\n<!--\n* <math>\nX^{\\flat}\\wedge\\alpha = {\\star}(X\\times\\alpha^{\\sharp})^{\\flat}\n</math>-->\n\n* <math>\n\\iota_X\\alpha=-(X\\times A)^{\\flat}\n</math> if <math>\\alpha\\in\\Omega^2(M),\\ A=({\\star}\\alpha)^{\\sharp}</math>\n\n:<math>\nX\\cdot Y = {\\star}(X^{\\flat}\\wedge {\\star} Y^{\\flat})\n</math> ( ''[[Dot product]]'' )\n\n:<math>\n\\nabla f=(df)^{\\sharp}\n</math> ( ''[[Gradient]] <math>1</math>-form'' )\n\n:<math>\nX\\cdot\\nabla f=df(X)\n</math> ( ''[[Directional derivative]]'' )\n\n:<math>\n\\nabla\\cdot X = {\\star} d {\\star} X^{\\flat} = \\delta X^{\\flat}\n</math> ( ''[[Divergence]]'' )\n\n:<math>\n\\nabla\\times X = ({\\star} d X^{\\flat})^{\\sharp}\n</math> ( ''[[Curl_(mathematics) | Curl]]'' )\n\n:<math>\n\\langle X,N\\rangle\\sigma = {\\star} X^\\flat\n</math> where <math>N</math> is the unit normal vector of <math>\\partial M</math> and <math>\\sigma=\\iota_{N}\\mathbf{det}</math> is the area form on <math>\\partial M</math>.\n\n:<math>\n\\int_{\\Sigma} d{\\star} X^{\\flat} = \\int_{\\partial\\Sigma}{\\star} X^{\\flat} = \\int_{\\partial\\Sigma}\\langle X,N\\rangle\\sigma\n</math> ( ''[[Divergence theorem]]'' )\n\n=== Lie derivatives ===\n\n:<math>\n\\mathcal{L}_X f =X\\cdot \\nabla f\n</math> ( ''<math>0</math>-forms'' )\n\n:<math>\n\\mathcal{L}_X \\alpha = (\\nabla_X\\alpha^{\\sharp})^{\\flat} +g(\\alpha^{\\sharp},\\nabla X)\n</math> ( ''<math>1</math>-forms'' )\n\n:<math>\n{\\star}\\mathcal{L}_X\\beta = \\left( \\nabla_XB - \\nabla_BX + (\\text{div}X)B \\right)^{\\flat}\n</math> if <math>B=({\\star}\\beta)^{\\sharp}</math> ( ''<math>2</math>-forms on <math>3</math>-manifolds'' )\n\n:<math>\n{\\star}\\mathcal{L}_X\\rho = dq(X)+(\\text{div}X)q\n</math> if <math>\\rho={\\star} q \\in \\Omega^0(M)</math> ( ''<math>n</math>-forms'' )\n\n:<math>\n\\mathcal{L}_X(\\mathbf{det})=(\\text{div}(X))\\mathbf{det}\n</math>\n\n<!--\n==== Laplacian ====\nDefine the matrix <math>g:=\\left(g(X_i,X_j)\\right)_{ij}\\in\\mathbb{R}^{n\\times n}</math> for a basis <math>X_1,\\ldots,X_n</math>. Then\n\n:<math>\n\\Delta f = \\frac{1}{\\sqrt{\\det(g)}}\\sum_{i,j=1}^{n}\\frac{\\partial}{\\partial X_i} \\left( \\sqrt{\\text{det}(g)}(g^{-1})_{ij}\\frac{\\partial}{\\partial X_j} f \\right)\n</math> ( ''Coordinate Laplacian'' )\n-->\n<!-- ==== Material derivatives ''D''<sub>''X''</sub> ==== -->\n\n== References ==\n{{Reflist}}\n\n[[Category:Calculus]]\n[[Category:Mathematical identities]]\n[[Category:Mathematics-related lists]]\n\n{{improve categories|date=November 2018}}"
    },
    {
      "title": "Green's identities",
      "url": "https://en.wikipedia.org/wiki/Green%27s_identities",
      "text": "In [[mathematics]], '''Green's identities''' are a set of three identities in [[vector calculus]] relating the bulk with the boundary of a region on which differential operators act. They are named after the mathematician [[George Green (mathematician)|George Green]], who discovered [[Green's theorem]].\n\n==Green's first identity==\nThis identity is derived from the [[divergence theorem]] applied to the vector field {{math|'''F''' {{=}} ''ψ''&nbsp;∇''φ''}} and using the identity that {{math|∇ &middot;(''φ'' '''X''' ) {{=}} ∇''φ'' &middot;'''X''' + ''φ'' ∇&middot;'''X'''}}: Let {{mvar|φ}} and {{mvar|ψ}} be scalar functions defined on some region {{math|''U'' ⊂ '''R'''<sup>''d''</sup>}}, and suppose that {{mvar|φ}} is twice [[continuously differentiable]], and {{mvar|ψ}} is once continuously differentiable. Then<ref name=\"strauss\">{{cite book|last=Strauss|first=Walter|title=Partial Differential Equations: An Introduction|publisher=Wiley}}</ref>\n: <math>\\int_U \\left( \\psi \\, \\Delta \\varphi + \\nabla \\psi \\cdot \\nabla \\varphi \\right)\\, dV  = \\oint_{\\partial U} \\psi \\left( \\nabla \\varphi \\cdot \\mathbf{n} \\right)\\, dS=\\oint_{\\partial U}\\psi\\,\\nabla\\varphi\\cdot d\\mathbf{S} </math>\nwhere {{math|∆ ≡ ∇<sup>2</sup>}} is the [[Laplace operator]], {{math|∂''U''}} is the boundary of region {{mvar|U}}, {{math|'''n'''}} is the outward pointing unit normal of surface element {{math|''dS''}} and {{mvar|d}}{{math|'''S'''}} is the oriented surface element.\n\nThis theorem is a special case of the [[divergence theorem]], and is essentially the higher dimensional equivalent of [[integration by parts]] with {{mvar|ψ}} and the gradient of {{mvar|φ}} replacing {{mvar|u}} and {{mvar|v}}.\n\nNote that Green's first identity above is a special case of the more general identity derived from the [[divergence theorem]] by substituting {{math|'''F''' {{=}} ''ψ'''''Γ'''}},\n: <math>\\int_U \\left( \\psi \\, \\nabla \\cdot \\mathbf{\\Gamma} + \\mathbf{\\Gamma} \\cdot \\nabla \\psi\\right)\\, dV  = \\oint_{\\partial U} \\psi \\left( \\mathbf{\\Gamma} \\cdot \\mathbf{n} \\right)\\, dS=\\oint_{\\partial U}\\psi\\mathbf{\\Gamma}\\cdot d\\mathbf{S} ~. </math>\n\n==Green's second identity==\nIf {{mvar|φ}} and {{mvar|ψ}} are both twice continuously differentiable on {{math|''U'' ⊂ '''R'''<sup>3</sup>}}, and {{mvar|ε}} is once continuously differentiable,  one may choose {{math|'''F''' {{=}} ''ψε''&nbsp;∇''φ'' − ''φε''&nbsp;∇''ψ''}} to obtain\n: <math> \\int_U \\left[ \\psi \\, \\nabla \\cdot \\left( \\varepsilon \\, \\nabla \\varphi \\right) - \\varphi \\, \\nabla \\cdot \\left( \\varepsilon \\, \\nabla \\psi \\right) \\right]\\, dV = \\oint_{\\partial U} \\varepsilon \\left( \\psi {\\partial \\varphi \\over \\partial \\mathbf{n}} - \\varphi {\\partial \\psi \\over \\partial \\mathbf{n}}\\right)\\, dS ~. </math>\n\nFor the special case of {{math|''ε'' {{=}} 1}} all across {{math|''U'' ⊂ '''R'''<sup>3</sup>}}, then,\n: <math> \\int_U \\left( \\psi \\, \\Delta \\varphi - \\varphi \\, \\Delta \\psi\\right)\\, dV = \\oint_{\\partial U} \\left( \\psi {\\partial \\varphi \\over \\partial \\mathbf{n}} - \\varphi {\\partial \\psi \\over \\partial \\mathbf{n}}\\right)\\, dS. </math>\n\nIn the equation above, {{math|∂''φ''/∂'''n'''}} is the [[directional derivative]] of {{mvar|φ}} in the direction of the outward pointing normal {{math|'''n'''}} to the surface element {{math|''dS''}},\n: <math> {\\partial \\varphi \\over \\partial \\mathbf{n}} = \\nabla \\varphi \\cdot \\mathbf{n}=\\nabla_\\mathbf{n}\\varphi.</math>\n\nIn particular, this demonstrates that the Laplacian is [[Self-adjoint operator|self-adjoint]] in the {{mvar|L}}<sup>2</sup> inner product for functions vanishing on the boundary.\n\n==Green's third identity==\nGreen's third identity derives from the second identity by choosing {{math|''φ'' {{=}} ''G''}}, where the [[Green's function]] {{mvar|G}} is taken to be a [[fundamental solution]] of the [[Laplace operator]], ∆.  This means that:\n\n:<math> \\Delta G(\\mathbf{x},\\boldsymbol{\\eta}) = \\delta(\\mathbf{x} - \\boldsymbol{\\eta})    ~.</math>\n\nFor example, in {{math|'''R'''<sup>3</sup>}}, a solution has the form\n:<math>G(\\mathbf{x},\\boldsymbol{\\eta})= \\frac{-1}{4 \\pi \\|\\mathbf{x} - \\boldsymbol{\\eta} \\|}         ~.</math>\n\nGreen's third identity states that if {{mvar|ψ}} is a function that is twice continuously differentiable on {{mvar|U}}, then\n: <math> \\int_U \\left[ G(\\mathbf{y},\\boldsymbol{\\eta}) \\, \\Delta \\psi(\\mathbf{y}) \\right] \\, dV_\\mathbf{y} - \\psi(\\boldsymbol{\\eta})=  \\oint_{\\partial U} \\left[ G(\\mathbf{y},\\boldsymbol{\\eta}) {\\partial \\psi \\over \\partial \\mathbf{n}} (\\mathbf{y}) - \\psi(\\mathbf{y}) {\\partial G(\\mathbf{y},\\boldsymbol{\\eta}) \\over \\partial \\mathbf{n}} \\right]\\, dS_\\mathbf{y}.</math>\n\nA simplification arises if {{mvar|ψ}} is itself a [[harmonic function]], i.e. a solution to the [[Laplace equation]].  Then {{math|∇<sup>2</sup>''ψ'' {{=}} 0}} and the identity simplifies to\n:<math>\\psi(\\boldsymbol{\\eta})= \\oint_{\\partial U} \\left[\\psi(\\mathbf{y}) \\frac{\\partial G(\\mathbf{y},\\boldsymbol{\\eta})}{\\partial \\mathbf{n}} - G(\\mathbf{y},\\boldsymbol{\\eta}) \\frac{\\partial \\psi}{\\partial \\mathbf{n}} (\\mathbf{y}) \\right]\\, dS_\\mathbf{y}.</math>\n\nThe second term in the integral above can be eliminated if  {{mvar|G}} is chosen to be the [[Green's function]] for the boundary of the region {{mvar|U}} where the problem is posed ([[Dirichlet boundary condition]]),\n:<math>\\psi(\\boldsymbol{\\eta}) = \\oint_{\\partial U} \\psi(\\mathbf{y}) \\frac{\\partial G(\\mathbf{y},\\boldsymbol{\\eta})}{\\partial \\mathbf{n}} \\, dS_\\mathbf{y}   ~.</math>\n\nThis form is used to construct solutions to Dirichlet boundary condition problems. To find solutions for [[Neumann boundary condition]] problems, the Green's function with vanishing normal gradient on the boundary is used instead.\n\nIt can be further verified that the above identity also applies when {{mvar|ψ}} is a solution to the [[Helmholtz equation]] or [[wave equation]] and {{mvar|G}} is the appropriate Green's function. In such a context, this identity is the mathematical expression of the [[Huygens principle]].\n\n==On manifolds==\nGreen's identities hold on a Riemannian manifold. In this setting, the first two are\n\n:<math>\\begin{align}\n\\int_M u \\,\\Delta v\\, dV + \\int_M \\langle\\nabla u, \\nabla v\\rangle\\, dV &= \\int_{\\partial M} u N v \\, d\\widetilde{V} \\\\\n\\int_M \\left (u \\, \\Delta v - v \\, \\Delta u \\right )\\, dV &= \\int_{\\partial M}(u N v - v N u) \\, d \\widetilde{V}\n\\end{align}</math>\n\nwhere {{mvar|u}} and {{mvar|v}} are smooth real-valued functions on {{mvar|M}}, {{mvar|dV}} is the volume form compatible with the metric, <math>d\\widetilde{V}</math> is the induced volume form on the boundary of {{mvar|M}}, {{mvar|N}} is the outward oriented unit vector field normal to the boundary, and {{math|Δ''u'' {{=}} div(grad ''u'')}} is the Laplacian.\n\n==Green's vector identity==\nGreen’s second identity establishes a relationship between second and (the divergence of) first order derivatives of two scalar functions. In differential form\n:<math>p_m \\, \\Delta q_m-q_m \\, \\Delta p_m = \\nabla\\cdot\\left(p_m\\nabla q_m-q_m \\, \\nabla p_m\\right),</math>\nwhere {{math|''p<sub>m</sub>''}} and {{math|''q<sub>m</sub>''}} are two arbitrary twice continuously differentiable scalar fields. This identity is of great importance in physics because continuity equations can thus be established for scalar fields such as mass or energy.<ref>M. Fernández-Guasti. Complementary fields conservation equation derived from the scalar wave equation. ''J. Phys. A: Math. Gen.'', 37:4107–4121, 2004.</ref>\n\nIn vector diffraction theory, two versions of Green’s second identity are introduced.\n\nOne variant invokes the divergence of a cross product <ref>[[Augustus Edward Hough Love|A. E. H. Love]]. The Integration of the Equations of Propagation of Electric Waves. ''Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character'', 197:pp. 1–45, 1901.</ref><ref>J. A. Stratton and L. J. Chu. Diffraction Theory of Electromagnetic Waves. ''Phys. Rev.'', 56(1):99–107, Jul 1939.</ref><ref>N. C. Bruce. Double scatter vector-wave Kirchhoff scattering from perfectly conducting surfaces with infinite slopes. ''Journal of Optics'', 12(8):085701, 2010.</ref> and states a relationship in terms of the curl-curl of the field\n:<math>\\mathbf{P}\\cdot\\left(\\nabla\\times\\nabla\\times\\mathbf{Q}\\right)-\\mathbf{Q}\\cdot\\left(\\nabla\\times\\nabla\\times\\mathbf{P}\\right)=\\nabla\\cdot\\left(\\mathbf{Q}\\times\\left(\\nabla\\times\\mathbf{P}\\right)-\\mathbf{P}\\times\\left(\\nabla\\times\\mathbf{Q}\\right)\\right).</math>\n\nThis equation can be written in terms of the Laplacians,\n\n:<math>\\mathbf{P}\\cdot\\Delta \\mathbf{Q}-\\mathbf{Q}\\cdot\\Delta \\mathbf{P} + \\mathbf{Q} \\cdot \\left[\\nabla\\left(\\nabla\\cdot\\mathbf{P}\\right)\\right]-\\mathbf{P} \\cdot \\left[ \\nabla \\left(\\nabla \\cdot \\mathbf{Q}\\right)\\right] = \\nabla \\cdot \\left( \\mathbf{P}\\times \\left(\\nabla\\times\\mathbf{Q}\\right) - \\mathbf{Q}\\times\\left(\\nabla\\times\\mathbf{P}\\right)\\right).</math>\n\nHowever, the terms\n\n: <math>\\mathbf{Q}\\cdot\\left[\\nabla\\left(\\nabla\\cdot\\mathbf{P}\\right)\\right]-\\mathbf{P} \\cdot \\left[\\nabla\\left(\\nabla\\cdot\\mathbf{Q}\\right)\\right],</math>\n\ncould not be readily written in terms of a divergence.\n\nThe other approach introduces bi-vectors, this formulation requires a dyadic Green function.<ref>W. Franz, On the Theory of Diffraction. ''Proceedings of the Physical Society. Section A'', 63(9):925, 1950.</ref><ref>Chen-To Tai. Kirchhoff theory: Scalar, vector, or dyadic? ''Antennas and Propagation, IEEE Transactions on'', 20(1):114–115, jan 1972.</ref> The derivation presented here avoids these problems.<ref>M. Fernández-Guasti. Green's second identity for vector fields. ISRN Mathematical Physics, 2012:7, 2012. Article ID: 973968. [http://dx.doi.org/10.5402/2012/973968]</ref>\n\nConsider that the scalar fields in Green's second identity are the Cartesian components of vector fields, i.e.\n\n: <math>\\mathbf{P}=\\sum_m p_{m}\\hat{\\mathbf{e}}_m, \\qquad \\mathbf{Q}=\\sum_m q_m \\hat{\\mathbf{e}}_m.</math>\n\nSumming up the equation for each component, we obtain\n\n: <math>\\sum_m \\left[p_m\\Delta q_m-q_m\\Delta p_m\\right]=\\sum_m \\nabla \\cdot \\left( p_m \\nabla q_m-q_m\\nabla p_m \\right).</math>\n\nThe LHS according to the definition of the dot product may be written in vector form as\n\n:<math>\\sum_m \\left[p_m \\, \\Delta q_m-q_m \\, \\Delta p_m\\right] = \\mathbf{P} \\cdot \\Delta\\mathbf{Q}-\\mathbf{Q}\\cdot\\Delta\\mathbf{P}.</math>\n\nThe RHS is a bit more awkward to express in terms of vector operators. Due to the distributivity of the divergence operator over addition, the sum of the divergence is equal to the divergence of the sum, i.e.\n\n: <math>\\sum_m \\nabla\\cdot\\left(p_m \\nabla q_m-q_m\\nabla p_m\\right)= \\nabla \\cdot \\left(\\sum_m p_m \\nabla q_m-\\sum_m q_m \\nabla p_m \\right).</math>\n\nRecall the vector identity for the gradient of a dot product,\n: <math>\\nabla \\left(\\mathbf{P} \\cdot \\mathbf{Q} \\right) = \\left( \\mathbf{P} \\cdot \\nabla \\right) \\mathbf{Q} + \\left( \\mathbf{Q} \\cdot \\nabla \\right) \\mathbf{P} + \\mathbf{P}\\times \\left(\\nabla\\times\\mathbf{Q}\\right)+\\mathbf{Q}\\times \\left(\\nabla\\times\\mathbf{P}\\right),</math>\nwhich, written out in vector components is given by\n\n: <math>\\nabla\\left(\\mathbf{P}\\cdot\\mathbf{Q}\\right)=\\nabla\\sum_m p_m q_m = \\sum_m p_m \\nabla q_m + \\sum_m q_m \\nabla p_m.</math>\n\nThis result is similar to what we wish to evince in vector terms 'except' for the minus sign. Since the differential operators in each term act either over one vector (say <math>p_m</math>’s) or the other (<math>q_m</math>’s), the contribution to each term must be\n\n: <math>\\sum_m p_m \\nabla q_m = \\left(\\mathbf{P} \\cdot \\nabla\\right) \\mathbf{Q} +  \\mathbf{P}\\times \\left(\\nabla\\times\\mathbf{Q}\\right),</math>\n: <math>\\sum_m q_m \\nabla p_m = \\left( \\mathbf{Q} \\cdot \\nabla\\right) \\mathbf{P} +  \\mathbf{Q}\\times \\left(\\nabla\\times\\mathbf{P}\\right).</math>\n\nThese results can be rigorously proven to be correct through [http://luz.izt.uam.mx/wiki/index.php/Green%27s_vector_identity evaluation of the vector components]. Therefore, the RHS can be written in vector form as\n\n: <math> \\sum_m p_m \\nabla q_m - \\sum_m q_m \\nabla p_m = \\left(\\mathbf{P} \\cdot \\nabla\\right) \\mathbf{Q} +  \\mathbf{P}\\times \\left(\\nabla\\times\\mathbf{Q}\\right)-\\left( \\mathbf{Q} \\cdot \\nabla\\right) \\mathbf{P} -  \\mathbf{Q}\\times \\left(\\nabla\\times\\mathbf{P}\\right).</math>\n\nPutting together these two results, a result analogous to Green’s theorem for scalar fields is obtained,\n:'''Theorem for vector fields.'''\n:: <math>\\color{OliveGreen}\\mathbf{P} \\cdot \\Delta \\mathbf{Q} - \\mathbf{Q} \\cdot \\Delta \\mathbf{P} = \\nabla \\cdot \\left[ \\left(\\mathbf{P} \\cdot \\nabla\\right) \\mathbf{Q} +  \\mathbf{P}\\times \\left(\\nabla\\times\\mathbf{Q}\\right)-\\left( \\mathbf{Q} \\cdot \\nabla\\right) \\mathbf{P} -  \\mathbf{Q}\\times \\left(\\nabla\\times\\mathbf{P}\\right)\\right].</math>\n\nThe curl of a cross product can be written as\n:<math>\\nabla\\times\\left(\\mathbf{P}\\times\\mathbf{Q}\\right)=\\left(\\mathbf{Q}\\cdot\\nabla\\right)\\mathbf{P}-\\left(\\mathbf{P}\\cdot\\nabla\\right)\\mathbf{Q}+\\mathbf{P}\\left(\\nabla\\cdot\\mathbf{Q}\\right)-\\mathbf{Q}\\left(\\nabla\\cdot\\mathbf{P}\\right);</math>\n\nGreen’s vector identity can then be rewritten as\n: <math>\\mathbf{P}\\cdot\\Delta \\mathbf{Q}-\\mathbf{Q}\\cdot\\Delta \\mathbf{P}= \\nabla \\cdot \\left[\\mathbf{P} \\left(\\nabla\\cdot\\mathbf{Q}\\right)-\\mathbf{Q} \\left( \\nabla \\cdot \\mathbf{P}\\right)-\\nabla \\times \\left( \\mathbf{P} \\times \\mathbf{Q} \\right) +\\mathbf{P}\\times\\left(\\nabla\\times\\mathbf{Q}\\right) - \\mathbf{Q}\\times \\left(\\nabla\\times\\mathbf{P}\\right)\\right].</math>\n\nSince the divergence of a curl is zero, the third term vanishes  to yield\n:'''Green's vector identity.'''\n::<math>\\color{OliveGreen}\\mathbf{P}\\cdot\\Delta\\mathbf{Q}-\\mathbf{Q} \\cdot \\Delta \\mathbf{P} =\\nabla\\cdot\\left[\\mathbf{P}\\left(\\nabla\\cdot\\mathbf{Q}\\right)-\\mathbf{Q} \\left( \\nabla \\cdot \\mathbf{P} \\right) + \\mathbf{P}\\times \\left(\\nabla\\times\\mathbf{Q}\\right) - \\mathbf{Q}\\times\\left(\\nabla\\times\\mathbf{P}\\right)\\right].</math>\n\nWith a similar procedure, the Laplacian of the dot product can be expressed in terms of the Laplacians of the factors\n\n: <math> \\Delta \\left( \\mathbf{P} \\cdot \\mathbf{Q} \\right) = \\mathbf{P} \\cdot \\Delta \\mathbf{Q}-\\mathbf{Q}\\cdot\\Delta \\mathbf{P} + 2\\nabla \\cdot \\left[ \\left( \\mathbf{Q} \\cdot \\nabla \\right) \\mathbf{P} + \\mathbf{Q} \\times \\nabla \\times \\mathbf{P} \\right].</math>\n\nAs a corollary, the awkward terms can now be written in terms of a divergence by comparison with the vector Green equation,\n\n: <math>\\mathbf{P}\\cdot \\left[ \\nabla \\left(\\nabla \\cdot \\mathbf{Q} \\right) \\right] - \\mathbf{Q} \\cdot \\left[ \\nabla \\left( \\nabla \\cdot \\mathbf{P} \\right) \\right] = \\nabla \\cdot\\left[\\mathbf{P}\\left(\\nabla\\cdot\\mathbf{Q}\\right)-\\mathbf{Q} \\left( \\nabla \\cdot \\mathbf{P} \\right) \\right].</math>\n\nThis result can be verified by expanding the divergence of a scalar times a vector on the RHS.\n\n==See also==\n* [[Green's function]]\n* [[Kirchhoff integral theorem]]\n\n==References==\n{{reflist}}\n\n==External links==\n* {{springer|title=Green formulas|id=p/g045080}}\n*[http://mathworld.wolfram.com/GreensIdentities.html] Green's Identities at Wolfram MathWorld\n\n{{DEFAULTSORT:Green's Identities}}\n[[Category:Vector calculus]]\n[[Category:Mathematical identities]]"
    },
    {
      "title": "Hermite's identity",
      "url": "https://en.wikipedia.org/wiki/Hermite%27s_identity",
      "text": "{{dablink|This article is not about [[Hermite's cotangent identity]].}}\nIn [[mathematics]], '''Hermite's identity''', named after [[Charles Hermite]], gives the value of a [[summation]] involving the [[floor function]]. It states that for every [[real number]] ''x'' and for every positive [[integer]] ''n'' the following [[Identity (mathematics)|identity]] holds:<ref>{{citation|title=Mathematical Miniatures|volume=43|series=New Mathematical Library|first1=Svetoslav|last1=Savchev|first2=Titu|last2=Andreescu|publisher=[[Mathematical Association of America]]|year=2003|isbn=9780883856451|chapter=12 Hermite's Identity|pages=41–44}}.</ref><ref>{{citation\n | last = Matsuoka | first = Yoshio\n | doi = 10.2307/2311413\n | issue = 10\n | journal = [[The American Mathematical Monthly]]\n | mr = 1533020\n | page = 1115\n | title = Classroom Notes: On a Proof of Hermite's Identity\n | volume = 71\n | year = 1964}}.</ref>\n\n: <math>\\sum_{k=0}^{n-1}\\left\\lfloor x+\\frac{k}{n}\\right\\rfloor=\\lfloor nx\\rfloor .</math>\n\n==Proof==\nSplit <math>x</math> into its [[integer part]] and [[fractional part]], <math>x=\\lfloor x\\rfloor+\\{x\\}</math>. There is exactly one <math>k'\\in\\{1,\\ldots,n\\}</math> with \n:<math>\\lfloor x\\rfloor=\\left\\lfloor x+\\frac{k'-1}{n}\\right\\rfloor\\le x<\\left\\lfloor x+\\frac{k'}{n}\\right\\rfloor=\\lfloor x\\rfloor+1.</math>\nBy subtracting the same integer <math>\\lfloor x\\rfloor</math> from inside the floor operations on the left and right sides of this inequality, it may be rewritten as\n\n:<math>0=\\left\\lfloor \\{x\\}+\\frac{k'-1}{n}\\right\\rfloor\\le \\{x\\}<\\left\\lfloor \\{x\\}+\\frac{k'}{n}\\right\\rfloor=1.</math>\n\nTherefore,\n\n:<math>1-\\frac{k'}{n}\\le \\{x\\}<1-\\frac{k'-1}{n} ,</math>\n\nand multiplying both sides by <math>n</math> gives\n\n:<math>n-k'\\le n\\, \\{x\\}<n-k'+1.</math>\n\nNow if  the summation from Hermite's identity is split into two parts at index <math>k'</math>, it becomes\n\n: <math>\n\\begin{align}\n\\sum_{k=0}^{n-1}\\left\\lfloor x+\\frac{k}{n}\\right\\rfloor\n& =\\sum_{k=0}^{k'-1} \\lfloor x\\rfloor+\\sum_{k=k'}^{n-1} (\\lfloor x\\rfloor+1)=n\\, \\lfloor x\\rfloor+n-k' \\\\[8pt]\n& =n\\, \\lfloor x\\rfloor+\\lfloor n\\,\\{x\\}\\rfloor=\\left\\lfloor n\\, \\lfloor x\\rfloor+n\\, \\{x\\} \\right\\rfloor=\\lfloor nx\\rfloor.\n\\end{align}\n</math>\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Hermite's Identity}}\n[[Category:Mathematical identities]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "Hockey-stick identity",
      "url": "https://en.wikipedia.org/wiki/Hockey-stick_identity",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|Recurrence relations of binomial coefficients in Pascal's triangle}}\n[[File:Pascal triangle small.png|thumb|right|300px|Pascal's triangle, rows 0 through 7. The hockey stick identity confirms, for example: for ''n''=5, ''r''=2: 1+3+6+10=20; for ''n''=6, ''r''=3: 1+4+10+20=35. ]]\n\nIn [[combinatorics|combinatorial]] mathematics, the identity\n\n: <math>\\sum^n_{i=r}{i\\choose r}={n+1\\choose r+1} \\qquad \\text{ for } n,r\\in\\mathbb{N}, \\quad n>r</math>\n\nis known as the '''hockey-stick'''<ref>CH Jones (1996) ''Generalized Hockey Stick Identities and N-Dimensional Block Walking.'' [[Fibonacci Quarterly]] '''34'''(3), 280-288.</ref> or '''Christmas stocking identity'''.<ref>{{Cite web|url=http://mathworld.wolfram.com/ChristmasStockingTheorem.html|title=Christmas Stocking Theorem|last=W.|first=Weisstein, Eric|website=mathworld.wolfram.com|language=en|access-date=2016-11-01}}</ref> That name stems from the graphical representation of the identity on [[Pascal's triangle]]: when the addends represented in the summation and the sum itself are highlighted, the shape revealed is vaguely reminiscent of those objects.\n\n==Proofs==\n\nThe inductive and algebraic proofs both make use of [[Pascal's identity]]:\n\n:<math>{n \\choose k}={n-1\\choose k-1}+{n-1\\choose k}.</math>\n\n===Inductive proof===\n\nThis identity can be proven by induction on <math>n</math>.\n\n<u>Base case</u>\nLet <math>n=r</math>;\n\n:<math>\\sum^n_{i=r} {i\\choose r} = \\sum^r_{i=r}{i\\choose r}={r\\choose r} = 1 = {r+1\\choose r+1} = {n+1\\choose r+1}. </math>\n\n<u>Inductive step</u>\nSuppose, for some <math>k\\in\\mathbb{N}, k \\geqslant r</math>,\n\n:<math>\\sum^k_{i=r}{i\\choose r}={k+1\\choose r+1}</math>\n\nThen\n\n:<math>\\sum^{k+1}_{i=r} {i\\choose r} = \\left(\\sum^k_{i=r} {i\\choose r} \\right) + {k+1\\choose r}={k+1\\choose r+1}+{k+1\\choose r}={k+2\\choose r+1}.</math>\n\n===Algebraic proof===\n\nWe use a [[Telescoping series|telescoping]] argument to simplify the computation of the sum:\n\n:<math>\n\\begin{align}\n\\sum_{t=\\color{blue}0}^n \\binom{t}{k} \n=\\sum_{t=\\color{blue}k}^n\\binom tk\n&= \\sum_{t=k}^n\\left[ \\binom {t+1}{k+1}-\\binom {t}{k+1}\\right]\\\\\n&=\\sum_{t=\\color{green}k}^{\\color{green}n}\\binom {\\color{green}{t+1}}{k+1} - \\sum_{t=k}^n \\binom t{k+1}\\\\\n&=\\sum_{t=\\color{green}{k+1}}^{\\color{green}{n+1}}\\binom {\\color{green}{t}}{k+1} - \\sum_{t=k}^n \\binom t{k+1}\\\\\n&=\\binom{n+1}{k+1}-\\underbrace{\\binom k{k+1}}_0&&\\text{by telescoping}\\\\\n&=\\binom{n+1}{k+1}.\n\\end{align}\n</math>\n\n===A combinatorial proof===\n\nImagine that we are distributing <math>n</math> indistinguishable candies to <math>k</math> distinguishable children. By a direct application of [[Stars and bars (combinatorics)|the stars and bars method]], there are\n\n:<math>\\binom{n+k-1}{ k-1}</math>\n\nways to do this. Alternatively, we can first give <math>0\\leqslant i\\leqslant n</math> candies to the oldest child so that we are essentially giving <math>n-i</math> candies to <math>k-1</math> kids and again, with stars and bars and [[Double counting (proof technique)|double counting]], we have\n\n:<math>\\binom{n+k-1}{ k-1}=\\sum_{i=0}^n\\binom{n+k-2-i}{k-2},</math>\n\nwhich simplifies to the desired result by taking <math>n' = n+k-2</math> and <math>r=k-2</math>, and noticing that <math>n'-n = k-2=r</math>:\n\n:<math>\\binom{n'+1}{ r+1}=\\sum_{i=0}^n \\binom {n'-i}r = \\sum_{i=r}^{n'} \\binom {i}r .</math>\n\n===Another combinatorial proof===\n\nWe can form a committee of size <math>k+1</math> from a group of <math>n+1</math> people in\n\n:<math> \\binom{n+1}{k+1}</math>\n\nways. Now we hand out the numbers <math>1,2,3,\\dots,n-k+1</math> to <math>n-k+1</math> of the <math>n+1</math> people.  We can divide this into <math>n-k+1</math> disjoint cases.  In general, in case <math>x</math>, <math>1\\leqslant x\\leqslant n-k+1</math>, person <math>x</math> is on the committee and persons <math>1,2,3,\\dots, x-1</math> are not on the committee.  This can be done in\n\n:<math>\\binom{n-x+1}{k}</math>\n\nways.  Now we can sum the values of these <math>n-k+1</math> disjoint cases, getting\n\n:<math> \\binom{n+1}{k+1} = \\binom n k + \\binom {n-1} k + \\binom{n-2} k + \\cdots + \\binom{k+1} k+ \\binom k k.</math>\n\n==See also==\n* [[Pascal's identity]] \n* [[Pascal's triangle]]\n* [[Vandermonde's identity]]\n\n==References==\n{{reflist}}\n\n==External links==\n* [http://artofproblemsolving.com/wiki/index.php?title=Combinatorial_identity#Hockey-Stick_Identity On AOPS]\n* [http://math.stackexchange.com/questions/1490794/proof-of-the-hockey-stick-identity-sum-t-0n-binom-tk-binomn1k1 On StackExchange, Mathematics]\n\n\n[[Category:Theorems]]\n[[Category:Combinatorics]]\n[[Category:Mathematical identities]]\n[[Category:Articles containing proofs]]\n[[Category:Factorial and binomial topics]]"
    },
    {
      "title": "Lagrange's identity",
      "url": "https://en.wikipedia.org/wiki/Lagrange%27s_identity",
      "text": "{{other uses|Lagrange's identity (disambiguation)}}\n\nIn [[algebra]], '''Lagrange's identity''', named after [[Joseph Louis Lagrange]], is:<ref name=Weisstein>\n{{cite book |title=CRC concise encyclopedia of mathematics |author=Eric W. Weisstein |year= 2003|url=https://books.google.com/books?id=8LmCzWQYh_UC&pg=PA228 |isbn=1-58488-347-2 |edition=2nd |publisher=CRC Press }}\n\n</ref><ref name= Greene>\n\n{{cite book |title=Function theory of one complex variable |author1=Robert E Greene|author1-link= Robert Everist Greene |author2=Steven G Krantz |url=https://www.amazon.com/Function-Complex-Variable-Graduate-Mathematics/dp/082182905X/ref=sr_1_1?ie=UTF8&s=books&qid=1271907834&sr=1-1#reader_082182905X |page=22 |chapter=Exercise 16   |isbn=0-8218-3962-4 |year=2006 |edition=3rd |publisher=American Mathematical Society}}\n\n</ref>\n\n:<math>\n\\begin{align}\n\\biggl( \\sum_{k=1}^n a_k^2\\biggr) \\biggl(\\sum_{k=1}^n b_k^2\\biggr) - \\biggl(\\sum_{k=1}^n a_k b_k\\biggr)^2 & = \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n (a_i b_j - a_j b_i)^2 \\\\\n& \\biggl(= \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1,j\\neq i}^n (a_i b_j - a_j b_i)^2\\biggr),\n\\end{align}\n</math>\n\nwhich applies to any two sets {''a''<sub>1</sub>, ''a''<sub>2</sub>, .&nbsp;.&nbsp;., ''a<sub>n</sub>''} and {''b''<sub>1</sub>, ''b''<sub>2</sub>, .&nbsp;.&nbsp;., ''b<sub>n</sub>''} of [[real number|real]] or [[complex number]]s (or more generally, elements of a [[commutative ring]]). This identity is a generalisation of the [[Brahmagupta–Fibonacci identity]] and a special form of the [[Binet–Cauchy identity]].\n\nIn a more compact vector notation, Lagrange's identity is expressed as:<ref name=Boichenko>\n\n{{cite book |title=Dimension theory for ordinary differential equations |author1=Vladimir A. Boichenko |author2=Gennadiĭ Alekseevich Leonov |author3=Volker Reitmann |url=https://books.google.com/books?id=9bN1-b_dSYsC&pg=PA26 |page=26 |isbn=3-519-00437-2 |year=2005 |publisher=Vieweg+Teubner Verlag}}\n\n</ref>\n\n:<math>\\| \\mathbf a \\|^2 \\ \\| \\mathbf b \\|^2 - (\\mathbf {a \\cdot b } )^2 = \\sum_{1 \\le i < j \\le n} \\left(a_ib_j-a_jb_i \\right)^2 \\ , </math>\n\nwhere '''a''' and '''b''' are ''n''-dimensional vectors with components that are real numbers. The extension to complex numbers requires the interpretation of the [[dot product]] as an [[inner product]] or Hermitian dot product. Explicitly, for complex numbers, Lagrange's identity can be written in the form:<ref name=Steele>\n\n{{cite book |title=The Cauchy-Schwarz master class: an introduction to the art of mathematical inequalities |author=J. Michael Steele |pages=68–69 |chapter=Exercise 4.4: Lagrange's identity for complex numbers |url=https://books.google.com/books?id=bvgBdZKEYAEC&pg=PA68  |year=2004 |isbn=0-521-54677-X |publisher=Cambridge University Press}}\n\n</ref>\n\n:<math>\\biggl( \\sum_{k=1}^n |a_k|^2\\biggr) \\biggl(\\sum_{k=1}^n |b_k|^2\\biggr) - \\biggl|\\sum_{k=1}^n a_k b_k\\biggr|^2 = \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n |a_i \\overline{b}_j - a_j \\overline{b}_i|^2</math>\n\ninvolving the [[Absolute value#Complex numbers|absolute value]].<ref>\n{{Cite book | last1=Greene | first1=Robert E. | last2=Krantz | first2=Steven G. | title=Function Theory of One Complex Variable | publisher=[[American Mathematical Society]] | location=Providence, R.I. | isbn=978-0-8218-2905-9 | year=2002 | page=22, Exercise 16 | postscript=<!--None-->}};<br>\n{{Cite book | last1=Palka | first1=Bruce P. | title=An Introduction to Complex Function Theory | publisher=[[Springer-Verlag]] | location=Berlin, New York | isbn=978-0-387-97427-9 | year=1991 | page=27, Exercise 4.22 | postscript=<!--None-->}}.\n</ref>\n\nSince the right-hand side of the identity is clearly non-negative, it implies [[Cauchy–Schwarz inequality|Cauchy's inequality]] in the [[Dimension (vector space)|finite-dimensional]] [[real coordinate space]] ℝ<sup>''n''</sup> and its complex counterpart ℂ<sup>''n''</sup>.\n\nGeometrically, the identity asserts that the square of the volume of the parallelepiped spanned by a set of vectors is the [[Gramian|Gram determinant]] of the vectors.\n\n==Lagrange's identity and exterior algebra==\n\nIn terms of the [[wedge product]], Lagrange's identity can be written\n\n:<math>(a \\cdot a)(b \\cdot b) - (a \\cdot b)^2 = (a \\wedge b) \\cdot (a \\wedge b).</math>\n\nHence, it can be seen as a formula which gives the length of the wedge product of two vectors, which is the area of the parallelogram they define, in terms of the dot products of the two vectors, as\n\n:<math>\\|a \\wedge b\\| = \\sqrt{(\\|a\\|\\ \\|b\\|)^2 - \\|a \\cdot b\\|^2}.</math>\n\n==Lagrange's identity and vector calculus==\n\nIn three dimensions, Lagrange's identity asserts that if '''a''' and '''b''' are vectors in ℝ<sup>3</sup> with lengths |'''a'''| and |'''b'''|, then Lagrange's identity can be written in terms of the [[cross product]] and [[dot product]]:<ref name=Anton>\n\n{{cite book |title=Elementary Linear Algebra: Applications Version |author1=Howard Anton |author2=Chris Rorres |url=https://books.google.com/books?id=1PJ-WHepeBsC&pg=PA162&dq=%22cross+product%22+%22Lagrange%27s+identity%22&cd=6#v=onepage&q=%22cross%20product%22%20%22Lagrange%27s%20identity%22 |page=162 |chapter=Relationships between dot and cross products\n|isbn=0-470-43205-5 |year=2010 |publisher=John Wiley and Sons |edition=10th}}\n\n</ref><ref name=Lounesto1>\n\n{{cite book |title=Clifford algebras and spinors |author=Pertti Lounesto |url=https://books.google.com/books?id=kOsybQWDK4oC&pg=PA94&dq=%22which+in+coordinate+form+means+Lagrange%27s+identity%22&cd=1#v=onepage&q=%22which%20in%20coordinate%20form%20means%20Lagrange%27s%20identity%22 |isbn=0-521-00551-5 |year=2001 |page=94 |edition=2nd |publisher=Cambridge University Press}}\n\n</ref>\n\n:<math> |\\mathbf{a}|^2 |\\mathbf{b}|^2 - (\\mathbf {a \\cdot b})^2 = |\\mathbf {a \\times b}|^2</math>\n\nUsing the definition of angle based upon the [[Dot product#Geometric interpretation|dot product]] (see also [[Cauchy–Schwarz inequality#Use|Cauchy–Schwarz inequality]]), the left-hand side is\n:<math>|\\mathbf{a}|^2|\\mathbf{b}|^2(1-\\cos^2\\theta) = |\\mathbf{a}|^2|\\mathbf{b}|^2\\sin^2\\theta</math>\nwhere θ is the angle formed by the vectors '''a''' and '''b'''.  The area of a parallelogram with sides |'''a'''| and |'''b'''| and angle θ is known in elementary geometry to be\n:<math>|\\mathbf{a}|\\,|\\mathbf{b}|\\,|\\sin\\theta|,</math>\nso the left-hand side of Lagrange's identity is the squared area of the parallelogram.  The cross product appearing on the right-hand side is defined by\n:<math>\\mathbf{a}\\times\\mathbf{b} = (a_2b_3-a_3b_2)\\mathbf{i} + (a_3b_1-a_1b_3)\\mathbf{j} + (a_1b_2-a_2b_1)\\mathbf{k}</math>\nwhich is a vector whose components are equal in magnitude to the areas of the projections of the parallelogram onto the ''yz'', ''zx'', and ''xy'' planes, respectively.\n\n===Seven dimensions===\n{{main|Seven-dimensional cross product}} \nFor '''a''' and '''b''' as vectors in ℝ<sup>7</sup>, Lagrange's identity takes on the same form as in the case of ℝ<sup>3</sup> <ref name=Lounesto>{{cite book |title=Clifford algebras and spinors |author=Door Pertti Lounesto |isbn=0-521-00551-5 |year=2001 |edition =2nd  |url=https://books.google.com/books?id=kOsybQWDK4oC&printsec=frontcover&q=Pythagorean |publisher=Cambridge University Press}} See particularly [https://books.google.com/books?id=kOsybQWDK4oC&pg=PA96#v=onepage&q&f=false § 7.4 Cross products in ℝ<sup>7</sup>], p. 96.\n\n</ref>\n\n:<math>|\\mathbf{a}|^2 |\\mathbf{b}|^2 -|\\mathbf{a} \\cdot \\mathbf{b}|^2 = |\\mathbf{a} \\times \\mathbf{b}|^2 \\ ,</math>\n\nHowever, the cross product in 7 dimensions does not share all the properties of the cross product in 3 dimensions. For example, the direction of '''a × b''' in 7-dimensions may be the same as '''c × d''' even though '''c''' and '''d''' are linearly independent of '''a''' and '''b'''. Also the [[seven-dimensional cross product]] is not compatible with the [[Jacobi identity]].<ref name=Lounesto/>\n\n===Quaternions===\n\nA [[quaternion]] ''p'' is defined as the sum of a scalar ''t'' and a vector '''v''':\n\n:<math>p = t + \\mathbf v = t + x \\ \\mathbf  i +y \\ \\mathbf j + z\\  \\mathbf k.</math>\n\nThe product of two quaternions {{nowrap|1=''p'' = ''t'' + '''v'''}} and {{nowrap|1=''q'' = ''s'' + '''w'''}} is defined by\n\n:<math>pq = (st - \\mathbf{v}\\cdot\\mathbf{w}) + s \\ \\mathbf{v} + t \\ \\mathbf{w}  + \\mathbf{v}\\times\\mathbf{w}.</math>\n\nThe quaternionic conjugate of ''q'' is defined by\n\n:<math>\\overline{q} = t - \\mathbf{v},</math>\n\nand the norm squared is\n\n:<math>|q|^2 = q\\overline{q} = t^2 \\ + \\ x ^2   + \\ y^2 \\ +\\  z^2.</math>\n\nThe multiplicativity of the norm in the quaternion algebra provides, for quaternions ''p'' and ''q'':<ref name=Kuipers>\n\n{{cite book |title=Quaternions and rotation sequences: a primer with applications to orbits |author=Jack B. Kuipers |url=https://books.google.com/books?id=_2sS4mC0p-EC&pg=PA111 |page=111 |chapter=§5.6 The norm |isbn=0-691-10298-8 |year=2002 |publisher=Princeton University Press}}\n\n</ref>\n\n:<math>|pq| = |p| |q|.</math>\n\nThe quaternions ''p'' and ''q'' are called imaginary if their scalar part is zero; equivalently, if\n\n:<math>p = \\mathbf{v},\\quad q=\\mathbf{w}.</math>\n\nLagrange's identity is just the multiplicativity of the norm of imaginary quaternions,\n\n:<math>|\\mathbf{v}\\mathbf{w}|^2 = |\\mathbf{v}|^2|\\mathbf{w}|^2,</math>\n\nsince, by definition,\n:<math>\n|\\mathbf{v}\\mathbf{w}|^2 = (\\mathbf{v}\\cdot\\mathbf{w})^2 + |\\mathbf{v}\\times\\mathbf{w}|^2.\n</math>\n\n== Proof of algebraic form ==\n\nThe vector form follows from the Binet-Cauchy identity by setting ''c<sub>i</sub>''&nbsp;=&nbsp;''a<sub>i</sub>'' and ''d<sub>i</sub>''&nbsp;=&nbsp;''b<sub>i</sub>''. The second version follows by letting ''c<sub>i</sub>'' and ''d<sub>i</sub>'' denote the [[complex conjugate]]s of ''a<sub>i</sub>'' and ''b<sub>i</sub>'', respectively,\n\nHere is also a direct proof.<ref name=Jones>\n\nSee, for example, [https://docs.google.com/viewer?a=v&q=cache:rDnOA-ZKljkJ:www.owlnet.rice.edu/~fjones/chap7.pdf+lagrange%27s+identity+in+the+seven+dimensional+cross+product&hl=en&gl=ph&sig=AHIEtbQQtdVGhgbYhz78SQQb2biLxRi4kA Frank Jones, Rice University], page 4 in Chapter 7 of a [http://www.owlnet.rice.edu/~fjones/ book still to be published].\n\n</ref> The expansion of the first term on the left side is:\n\n:({{EquationRef|1}}) &ensp;&ensp;<math> \\left( \\sum_{k=1}^n a_k^2\\right) \\left(\\sum_{k=1}^n b_k^2\\right) = \n\\sum_{i=1}^n \\sum_{j=1}^n a_i^2 b_j^2 \n= \\sum_{k=1}^n a_k^2 b_k^2 \n+ \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n a_i^2 b_j^2 \n+ \\sum_{j=1}^{n-1} \\sum_{i=j+1}^n a_i^2 b_j^2 \\ ,</math>\n\nwhich means that the product of a column of ''a''<small>s</small> and a row of ''b''<small>s</small> yields (a sum of elements of) a square of ''ab''<small>s</small>, which can be broken up into a diagonal and a pair of triangles on either side of the diagonal.\n\nThe second term on the left side of Lagrange's identity can be expanded as:\n\n:({{EquationRef|2}}) &ensp;&ensp;<math> \\left(\\sum_{k=1}^n a_k b_k\\right)^2 = \n\\sum_{k=1}^n a_k^2 b_k^2 + 2\\sum_{i=1}^{n-1} \\sum_{j=i+1}^n a_i b_i a_j b_j \\ ,</math>\n\nwhich means that a symmetric square can be broken up into its diagonal and a pair of equal triangles on either side of the diagonal.\n\nTo expand the summation on the right side of Lagrange's identity, first expand the square within the summation:\n\n:<math> \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n (a_i b_j - a_j b_i)^2 = \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n (a_i^2 b_j^2 + a_j^2 b_i^2 - 2 a_i b_j a_j b_i). </math>\n\nDistribute the summation on the right side,\n\n:<math> \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n (a_i b_j - a_j b_i)^2 = \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n a_i^2 b_j^2 + \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n a_j^2 b_i^2 - 2 \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n a_i b_j a_j b_i .</math>\n\nNow exchange the indices ''i'' and ''j'' of the second term on the right side, and permute the ''b'' factors of the third term, yielding:\n\n:({{EquationRef|3}}) &ensp;&ensp;<math> \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n (a_i b_j - a_j b_i)^2 = \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n a_i^2 b_j^2 + \\sum_{j=1}^{n-1} \\sum_{i=j+1}^n a_i^2 b_j^2 - 2 \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n a_i b_i a_j b_j \\ .</math>\n\nBack to the left side of Lagrange's identity: it has two terms, given in expanded form by Equations '''('''{{EquationNote|1}}''')''' and '''('''{{EquationNote|2}}''')'''.  The first term on the right side of Equation '''('''{{EquationNote|2}}''')''' ends up canceling out the first term on the right side of Equation '''('''{{EquationNote|1}}''')''', yielding\n:'''('''{{EquationNote|1}}''')''' - '''('''{{EquationNote|2}}''')''' = <math>\\sum_{i=1}^{n-1} \\sum_{j=i+1}^n a_i^2 b_j^2 \n+ \\sum_{j=1}^{n-1} \\sum_{i=j+1}^n a_i^2 b_j^2 - 2\\sum_{i=1}^{n-1} \\sum_{j=i+1}^n a_i b_i a_j b_j </math>\nwhich is the same as Equation '''('''{{EquationNote|3}}''')''', so Lagrange's identity is indeed an identity, ''[[Q.E.D.]]''.\n\n== Proof of Lagrange's identity for complex numbers ==\nNormed division algebras require that the norm of the product is equal\nto the product of the norms. Lagrange's identity exhibits this equality.\nThe product identity used as a starting point here, is a consequence of the norm of the product equality with the product of the norm for scator algebras. This proposal, originally presented in the context of a deformed Lorentz metric, is based on a transformation stemming from the product operation and magnitude definition in hyperbolic scator algebra.<ref>M. Fernández-Guasti, ''Alternative realization for the composition of relativistic velocities'', Optics and Photonics 2011, vol. 8121 of The nature of light: What are photons? IV, pp. 812108–1–11. SPIE, 2011.</ref>\nLagrange's identity can be proved in a variety of ways.<ref name=\"Steele\"/>\nMost derivations use the identity as a starting point and prove in one way or another that the equality is true. In the present approach, Lagrange's identity is actually derived without assuming it ''a priori''. An extended version of these results are available in an open source journal.<ref>M. Fernández-Guasti. Lagrange's identity obtained from product identity, Int. Math. Forum, 70(52):2555-2559, 2012. [http://www.m-hikari.com/imf/imf-2012/49-52-2012/fernandezguastiIMF49-52-2012.pdf]</ref>\n\nLet <math>a_{i},b_{i}\\in\\mathbb{C}</math> be complex numbers and the overbar\nrepresents complex conjugate.\n\nThe product identity <math>\\prod_{i=1}^{n}\\left(1-a_{i}\\bar{a}_{i}-b_{i}\\bar{b}_{i}+a_{i}\\bar{a}_{i}b_{i}\\bar{b}_{i}\\right)=\\prod_{i=1}^{n}\\left(1-a_{i}\\bar{a}_{i}\\right)\\prod_{i=1}^{n}\\left(1-b_{i}\\bar{b}_{i}\\right)</math>\nreduces to the complex Lagrange's identity when fourth order terms, in a series expansion, are considered.\n\nIn order to prove it, expand the product on the LHS of the product identity in terms of\nseries up to fourth order. To this end, recall that products of the form <math>\\left(1+x_{i}\\right)</math> can be expanded\nin terms of sums as\n<math>\n\\prod_{i=1}^{n}\\left(1+x_{i}\\right)=1+\\sum_{i=1}^{n}x_{i}+\\sum_{i<j}^{n}x_{i}x_{j}+\\mathcal{O}^{3+}(x),\n</math>\nwhere <math>\\mathcal{O}^{3+}(x)</math> means terms with order three or higher\nin <math>x</math>.\n\n<center><math>\n\\prod_{i=1}^{n}\\left(1-a_{i}\\bar{a}_{i}-b_{i}\\bar{b}_{i}+a_{i}\\bar{a}_{i}b_{i}\\bar{b}_{i}\\right)=1-\\sum_{i=1}^{n}\\left(a_{i}\\bar{a}_{i}+b_{i}\\bar{b}_{i}\\right)+\\sum_{i=1}^{n}a_{i}\\bar{a}_{i}b_{i}\\bar{b}_{i}\n+\\sum_{i<j}^{n}\\left(a_{i}\\bar{a}_{i}a_{j}\\bar{a}_{j}+b_{i}\\bar{b}_{i}b_{j}\\bar{b}_{j}\\right)+\\sum_{i<j}^{n}\\left(a_{i}\\bar{a}_{i}b_{j}\\bar{b}_{j}+a_{j}\\bar{a}_{j}b_{i}\\bar{b}_{i}\\right)+\\mathcal{O}^{5+}.\n</math></center>\n\nThe two factors on the RHS are also written in terms of series\n<math>\n\\prod_{i=1}^{n}\\left(1-a_{i}\\bar{a}_{i}\\right)\\prod_{i=1}^{n}\\left(1-b_{i}\\bar{b}_{i}\\right)=\\left(1-\\sum_{i=1}^{n}a_{i}\\bar{a}_{i}+\\sum_{i<j}^{n}a_{i}\\bar{a}_{i}a_{j}\\bar{a}_{j}+\\mathcal{O}^{5+}\\right)\n\\left(1-\\sum_{i=1}^{n}b_{i}\\bar{b}_{i}+\\sum_{i<j}^{n}b_{i}\\bar{b}_{i}b_{j}\\bar{b}_{j}+\\mathcal{O}^{5+}\\right).\n</math>\n\nThe product of this expression up to fourth order is \n<center><math>\n\\prod_{i=1}^{n}\\left(1-a_{i}\\bar{a}_{i}\\right)\\prod_{i=1}^{n}\\left(1-b_{i}\\bar{b}_{i}\\right)=1-\\sum_{i=1}^{n}\\left(a_{i}\\bar{a}_{i}+b_{i}\\bar{b}_{i}\\right)\n+\\left(\\sum_{i=1}^{n}a_{i}\\bar{a}_{i}\\right)\\left(\\sum_{i=1}^{n}b_{i}\\bar{b}_{i}\\right)+\\sum_{i<j}^{n}\\left(a_{i}\\bar{a}_{i}a_{j}\\bar{a}_{j}+b_{i}\\bar{b}_{i}b_{j}\\bar{b}_{j}\\right)+\\mathcal{O}^{5+}.\n</math></center>\nSubstitution of these two results in the product identity give \n<center><math>\n\\sum_{i=1}^{n}a_{i}\\bar{a}_{i}b_{i}\\bar{b}_{i}+\\sum_{i<j}^{n}\\left(a_{i}\\bar{a}_{i}b_{j}\\bar{b}_{j}+a_{j}\\bar{a}_{j}b_{i}\\bar{b}_{i}\\right)=\\left(\\sum_{i=1}^{n}a_{i}\\bar{a}_{i}\\right)\\left(\\sum_{i=1}^{n}b_{i}\\bar{b}_{i}\\right).\n</math></center>\n\nThe product of two conjugates series can be expressed as series involving the product of conjugate terms. The conjugate series product is <math>\\left(\\sum_{i=1}^{n}x_{i}\\right)\\left(\\sum_{i=1}^{n}\\bar{x}_{i}\\right)=\\sum_{i=1}^{n}x_{i}\\bar{x}_{i}+\\sum_{i<j}^{n}\\left(x_{i}\\bar{x}_{j}+\\bar{x}_{i}x_{j}\\right)</math>, thus\n\n<math>\n\\left(\\sum_{i=1}^{n}a_{i}b_{i}\\right)\\left(\\sum_{i=1}^{n}\\overline{a_{i}b_{i}}\\right)-\\sum_{i<j}^{n}\\left(a_{i}b_{i}\\bar{a}_{j}\\bar{b}_{j}+\\bar{a}_{i}\\bar{b}_{i}a_{j}b_{j}\\right)+\\sum_{i<j}^{n}\\left(a_{i}\\bar{a}_{i}b_{j}\\bar{b}_{j}+a_{j}\\bar{a}_{j}b_{i}\\bar{b}_{i}\\right)\n=\\left(\\sum_{i=1}^{n}a_{i}\\bar{a}_{i}\\right)\\left(\\sum_{i=1}^{n}b_{i}\\bar{b}_{i}\\right).\n</math>\n\nThe terms of the last two series on the LHS are grouped as \n<math>\na_{i}\\bar{a}_{i}b_{j}\\bar{b}_{j}+a_{j}\\bar{a}_{j}b_{i}\\bar{b}_{i}-a_{i}b_{i}\\bar{a}_{j}\\bar{b}_{j}-\\bar{a}_{i}\\bar{b}_{i}a_{j}b_{j}=\\left(a_{i}\\bar{b}_{j}-a_{j}\\bar{b}_{i}\\right)\\left(\\bar{a}_{i}b_{j}-\\bar{a}_{j}b_{i}\\right),\n</math>\nin order to obtain the complex Lagrange's identity:\n\n<center><math>\n\\left(\\sum_{i=1}^{n}a_{i}b_{i}\\right)\\left(\\sum_{i=1}^{n}\\overline{a_{i}b_{i}}\\right)+\\sum_{i<j}^{n}\\left(a_{i}\\bar{b}_{j}-a_{j}\\bar{b}_{i}\\right)\\left(\\overline{a_{i}\\bar{b}_{j}-a_{j}\\bar{b}_{i}}\\right)=\\left(\\sum_{i=1}^{n}a_{i}\\bar{a}_{i}\\right)\\left(\\sum_{i=1}^{n}b_{i}\\bar{b}_{i}\\right).\n</math></center>\n\nIn terms of the modulii,\n<center><math>\n\\left|\\sum_{i=1}^{n}a_{i}b_{i}\\right|^{2}+\\sum_{i<j}^{n}\\left|a_{i}\\bar{b}_{j}-a_{j}\\bar{b}_{i}\\right|^{2}=\\left(\\sum_{i=1}^{n}\\left|a_{i}\\right|^{2}\\right)\\left(\\sum_{i=1}^{n}\\left|b_{i}\\right|^{2}\\right).\n</math></center>\n\nLagrange's identity for complex numbers has been obtained from a straightforward\nproduct identity. A derivation for the reals is obviously even more succinct. Since the Cauchy–Schwarz inequality is a particular case of Lagrange's identity,<ref name=\"Steele\"/> this\nproof is yet another way to obtain the CS inequality. Higher order terms in the series produce novel identities.\n\n== See also ==\n*[[Brahmagupta&ndash;Fibonacci identity]]\n*[[Lagrange's identity (boundary value problem)]]\n*[[Binet–Cauchy identity]]\n\n== References ==\n\n<references/>\n\n== External links ==\n*{{MathWorld|LagrangesIdentity|Lagrange's Identity}}\n\n[[Category:Mathematical identities]]\n[[Category:Multilinear algebra]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "Lagrange's identity (boundary value problem)",
      "url": "https://en.wikipedia.org/wiki/Lagrange%27s_identity_%28boundary_value_problem%29",
      "text": "{{other uses|Lagrange's identity (disambiguation)}}\n\nIn the study of [[ordinary differential equation]]s and their associated [[boundary value problem]]s, '''Lagrange's identity''', named after [[Joseph Louis Lagrange]], gives the boundary terms arising from [[integration by parts]] of a self-adjoint linear [[differential operator]].  Lagrange's identity is fundamental in [[Sturm&ndash;Liouville theory]].  In more than one independent variable, Lagrange's identity is generalized by [[Green's second identity]].\n\n==Statement==\nIn general terms, Lagrange's identity for any pair of functions ''u'' and ''v'' &ensp;in [[Smooth functions#The space of Ck functions|function space ''C''<sup>2</sup>]] (that is, twice differentiable) in ''n'' dimensions is:<ref name=Schaum>\n\n{{cite book |title= Schaum's outline of theory and problems of partial differential equations |author=Paul DuChateau, David W. Zachmann |page=103 |url=https://books.google.com/books?id=h9kRkvGQCgkC&pg=PA103 |chapter=§8.3 Elliptic boundary value problems |isbn=0-07-017897-6 |publisher=McGraw-Hill Professional |year=1986}}\n</ref>\n\n:<math>vL[u]-uL^*[v]=\\nabla \\cdot \\boldsymbol M, </math>\n\nwhere:\n\n:<math>M_i = \\sum_{j=1}^n a_{ij}\\left( \nv \\frac{\\partial u}{\\partial x_j} -u \\frac{\\partial v}{\\partial x_j} \n\\right ) + uv \\left( \nb_i - \\sum_{j=1}^{n} \\frac{\\partial a_{ij}}{\\partial x_j} \\right ), </math>\n\nand\n\n:<math>\\nabla \\cdot \\boldsymbol M  = \\sum_{i=1}^n \\frac{\\partial}{\\partial x_i} M_i, </math>\n\nThe operator ''L'' and its [[adjoint operator]] ''L''<sup>*</sup> are given by:\n\n:<math>L[u] = \\sum_{i,\\ j =1}^n a_{i,j}  \\frac {\\partial ^2 u }{\\partial x_i \\partial x_j} + \\sum_{i=1}^n b_i \\frac {\\partial u}{\\partial x_i} +c u </math>\n\nand\n\n:<math>L^*[v] = \\sum_{i,\\ j =1}^n  \\frac {\\partial ^2 (a_{i,j} v) }{\\partial x_i \\partial x_j} - \\sum_{i=1}^n \\frac {\\partial (b_i  v)}{\\partial x_i} + cv. </math>\n\nIf Lagrange's identity is integrated over a bounded region, then the [[divergence theorem]] can be used to form [[Green's second identity]] in the form:\n\n:<math>\\int_\\Omega v L[u]\\  d\\Omega = \\int_{\\Omega} u L^*[v]\\  d\\Omega +\\int_S \\boldsymbol{M \\cdot n } \\, dS, </math>\n\nwhere ''S'' is the surface bounding the volume ''Ω'' and '''''n''''' is the unit outward normal to the surface ''S''.\n\n===Ordinary differential equations=== \nAny second order [[ordinary differential equation]] of the form:\n\n:<math>a(x)\\frac{d^2y}{dx^2} + b(x)\\frac {dy}{dx} +c(x)y +\\lambda w(x) y =0, </math>\n\ncan be put in the form:<ref name= Richards>\n\n{{cite book |title=Advanced mathematical methods with Maple |author=Derek Richards |page=354 |url=https://books.google.com/books?id=AKsLy0rVGuwC&pg=PA354 |isbn=0-521-77981-2 |chapter=§10.4 Sturm&ndash;Liouville systems |year=2002 |publisher=Cambridge University Press }}\n\n</ref>\n\n:<math>\\frac {d}{dx} \\left( p(x) \\frac {dy}{dx} \\right )  +\\left( q(x)+ \\lambda w(x) \\right) y(x) = 0. </math>\n\nThis general form motivates introduction of the [[Sturm–Liouville theory|Sturm–Liouville operator]] ''L'', defined as an operation upon a function ''f&ensp;'' such that:\n\n:<math>L f = \\frac {d}{dx} \\left( p(x) \\frac {df}{dx} \\right) + q(x) f. </math>\n\nIt can be shown that for any ''u'' and ''v''&ensp;for which the various derivatives exist, '''Lagrange's identity''' for ordinary differential equations holds:<ref name= Richards/>\n\n:<math> uLv - vLu = - \\frac{d}{dx} \\left[ p(x) \\left(v\\frac{du}{dx} -u \\frac{dv}{dx} \\right ) \\right]. </math>\n\nFor ordinary differential equations defined in the interval [0, 1], Lagrange's identity can be integrated to obtain an integral form (also known as Green's formula):<ref name=Loney>\n\n{{cite book |title=Applied mathematical methods for chemical engineers |author=Norman W. Loney |url=https://books.google.com/books?id=QD6hOiRrAjIC&pg=PA218 |page=218 |chapter=Equation 6.73 |edition=2nd |isbn=0-8493-9778-2 |date=2007 |publisher=CRC Press}}\n\n</ref><ref name=Al_Gwaiz>\n{{cite book |title=Sturm&ndash;Liouville theory and its applications |page=66 |author=M. A. Al-Gwaiz |url=https://books.google.com/books?id=myy5WxCtBygC&pg=PA66 |isbn=1-84628-971-8 |date=2008 |chapter=Exercise 2.16 |publisher=Springer}}\n\n</ref><ref name=\"Boyce 2001\">{{cite book|author=William E. Boyce and Richard C. DiPrima |title=Elementary Differential Equations and Boundary Value Problems |edition=7th|date=2001|publisher=John Wiley & Sons|location=New York|isbn=0-471-31999-6|oclc=64431691|doi= |id= |chapter=Boundary Value Problems and Sturm–Liouville Theory |chapterurl= |quote= |page=630 }}</ref><ref>{{cite book|author=[[Gerald Teschl]]| title = Ordinary Differential Equations and Dynamical Systems| publisher=[[American Mathematical Society]]| place = [[Providence, Rhode Island|Providence]]| date = 2012| isbn= 978-0-8218-8328-0| url = http://www.mat.univie.ac.at/~gerald/ftp/book-ode/}}\n</ref>\n:<math>\\int_0^1 \\ dx \\ ( uLv-vLu) = \\left[p(x)\\left(u \\frac {dv}{dx}- v  \\frac {du}{dx} \\right)\\right]_0^1, </math>\n\nwhere <math>\\ p=P(x)</math>, <math>\\ q=Q(x)</math>, <math>\\ u=U(x)</math> and <math>\\ v=V(x)</math> are functions of <math>\\ x</math>. <math>\\ u</math> and <math>\\ v</math> having continuous second derivatives on the {{nowrap|interval <math>\\ [0,1] </math>.}}\n\n=== Proof of form for ordinary differential equations ===\nWe have:\n:<math>uLv = u \\left[\\frac {d}{dx} \\left( p(x) \\frac {dv}{dx} \\right) + q(x) v \\right], </math>\n\nand\n\n:<math>vLu = v \\left[\\frac {d}{dx} \\left( p(x) \\frac {du}{dx} \\right) + q(x) u \\right].  </math>\n\nSubtracting:\n\n:<math>uLv-vLu = u \\frac {d}{dx} \\left( p(x) \\frac {dv}{dx} \\right)-v \\frac {d}{dx} \\left( p(x) \\frac {du}{dx} \\right). </math>\n\nThe leading multiplied ''u'' and ''v'' can be moved ''inside'' the differentiation, because the extra differentiated terms in ''u'' and ''v'' are the same in the two subtracted terms and simply cancel each other. Thus,\n\n:<math>uLv-vLu = \\frac {d}{dx} \\left( p(x)u \\frac {dv}{dx} \\right)-\\frac {d}{dx}  \\left( v p(x) \\frac {du}{dx} \\right), </math>\n::<math>=\\frac {d}{dx}\\left[p(x)\\left(u \\frac {dv}{dx}- v  \\frac {du}{dx} \\right)\\right], </math>\n\nwhich is Lagrange's identity. Integrating from zero to one:\n\n:<math>\\int_0^1 \\ dx \\ ( uLv-vLu) = \\left[p(x)\\left(u \\frac {dv}{dx}- v  \\frac {du}{dx} \\right)\\right]_0^1, </math>\nas was to be shown.\n\n==References==\n{{reflist|2}}\n\n[[Category:Ordinary differential equations]]\n[[Category:Mathematical identities]]"
    },
    {
      "title": "Liouville's formula",
      "url": "https://en.wikipedia.org/wiki/Liouville%27s_formula",
      "text": "In [[mathematics]], '''Liouville's formula''', also known as the Abel-Jacobi-Liouville Identity, is an equation that expresses the [[determinant]] of a [[matrix (mathematics)#Square matrices|square-matrix]] solution of a first-order system of homogeneous [[linear differential equation]]s in terms of the sum of the diagonal coefficients of the system. The formula is named after the [[France|French]] [[mathematician]] [[Joseph Liouville]].  [[Jacobi's formula]] provides another representation of the same mathematical relationship.\n\nLiouville's formula is a generalization of [[Abel's identity]] and can be used to prove it. Since Liouville's formula relates the different [[linear independence|linearly independent]] solutions of the system of differential equations, it can help to find one solution from the other(s), see the example application below.\n\n==Statement of Liouville's formula==\nConsider the {{math|''n''}}-dimensional first-order homogeneous linear differential equation\n\n:<math>y'=A(x)y</math>\n\non an [[interval (mathematics)|interval]] {{math|''I''}} of the [[real line]], where {{math|''A''(''x'')}} for {{math|''x'' ∈ ''I''}} denotes a square matrix of dimension {{math|''n''}} with [[real number|real]] or [[complex number|complex]] entries. Let {{math|Φ}} denote a matrix-valued solution on {{math|''I''}}, meaning that each {{math|Φ(''x'')}} is a square matrix of dimension {{math|''n''}} with real or complex entries and the [[matrix calculus|derivative]] satisfies\n\n:<math>\\Phi'(x)=A(x)\\Phi(x),\\qquad x\\in I.</math>\n\nLet\n\n:<math>\\mathrm{tr}\\,A(\\xi)=\\sum_{i=1}^n a_{i,i}(\\xi),\\qquad \\xi\\in I,</math>\n\ndenote the [[trace (linear algebra)|trace]] of {{math|''A''(''ξ'') {{=}} (''a''<sub>''i'',&thinsp;''j''&thinsp;</sub>(''ξ''))<sub>''i'',&thinsp;''j''&thinsp;∈&thinsp;{1,...,''n''}</sub>}}, the sum of its diagonal entries. If the trace of {{math|''A''}} is a [[continuous function]], then the determinant of {{math|Φ}} satisfies\n\n:<math>\\det\\Phi(x)=\\det\\Phi(x_0)\\,\\exp\\biggl(\\int_{x_0}^x \\mathrm{tr}\\,A(\\xi) \\,\\textrm{d}\\xi\\biggr)</math>\n\nfor all {{math|''x''}} and {{math|''x''<sub>0</sub>}} in {{math|''I''}}.\n\n==Example application==\nThis example illustrates how Liouville's formula can help to find the general solution of a first-order system of homogeneous linear differential equations. Consider\n\n:<math>y'=\\underbrace{\\begin{pmatrix}1&-1/x\\\\1+x&-1\\end{pmatrix}}_{=\\,A(x)}y</math>\n\non the open interval {{math|''I'' {{=}} }}{{open-open|0,&thinsp;∞}}. Assume that the easy solution\n\n:<math>y(x)=\\begin{pmatrix}1\\\\x\\end{pmatrix},\\qquad x\\in I,</math>\n\nis already found. Let\n\n:<math>y(x)=\\begin{pmatrix}y_1(x)\\\\y_2(x)\\end{pmatrix}</math>\n\ndenote another solution, then\n\n:<math>\\Phi(x)=\\begin{pmatrix}y_1(x)&1\\\\y_2(x)&x\\end{pmatrix},\\qquad x\\in I,</math>\n\nis a square-matrix-valued solution of the above differential equation. Since the trace of {{math|''A''(''x'')}} is zero for all {{math|''x'' ∈ ''I''}}, Liouville's formula implies that the determinant\n\n{{NumBlk|:|<math>c_1:=\\det\\Phi(x)=x\\,y_1(x)-y_2(x),\\qquad x\\in I,</math>|{{EquationRef|1}}}}\n\nis actually a constant independent of {{math|''x''}}. Writing down the first component of the differential equation for {{math|''y''}}, we obtain using  ({{EquationNote|1}}) that\n\n:<math>y'_1(x)=y_1(x)-\\frac{y_2(x)}x=\\frac{x\\,y_1(x)-y_2(x)}x=\\frac{c_1}x,\\qquad x\\in I.</math>\n\nTherefore, by integration, we see that\n\n:<math>y_1(x)=c_1\\ln x+c_2,\\qquad x\\in I,</math>\n\ninvolving the [[natural logarithm]] and the [[constant of integration]] {{math|''c''<sub>2</sub>}}. Solving equation  ({{EquationNote|1}}) for {{math|''y''<sub>2</sub>(''x'')}} and substituting for {{math|''y''<sub>1</sub>(''x'')}} gives\n\n:<math>y_2(x)=x\\,y_1(x)-c_1=\\,c_1x\\ln x+c_2x-c_1,\\qquad x\\in I,</math>\n\nwhich is the general solution for {{math|''y''}}. With the special choice {{math|''c''<sub>1</sub> {{=}} 0}} and {{math|''c''<sub>2</sub> {{=}} 1}} we recover the easy solution we started with, the choice {{math|''c''<sub>1</sub> {{=}} 1}} and {{math|''c''<sub>2</sub> {{=}} 0}} yields a linearly independent solution. Therefore,\n\n:<math>\\Phi(x)=\\begin{pmatrix}\\ln x&1\\\\x\\ln x-1&x\\end{pmatrix},\\qquad x\\in I,</math>\n\nis a so-called fundamental solution of the system.\n\n==Proof of Liouville's formula==\nWe omit the argument {{math|''x''}} for brevity. By the [[Leibniz formula for determinants]], the derivative of the determinant of {{math|Φ {{=}} (Φ<sub>''i'',&thinsp;''j''&thinsp;</sub>)<sub>''i'',&thinsp;''j''&thinsp;∈&thinsp;{0,...,''n''}</sub>}} can be calculated by differentiating one row at a time and taking the sum, i.e.\n\n{{NumBlk|:|<math>(\\det\\Phi)'=\\sum_{i=1}^n\\det\\begin{pmatrix}\n\\Phi_{1,1}&\\Phi_{1,2}&\\cdots&\\Phi_{1,n}\\\\\n\\vdots&\\vdots&&\\vdots\\\\\n\\Phi'_{i,1}&\\Phi'_{i,2}&\\cdots&\\Phi'_{i,n}\\\\\n\\vdots&\\vdots&&\\vdots\\\\\n\\Phi_{n,1}&\\Phi_{n,2}&\\cdots&\\Phi_{n,n}\n\\end{pmatrix}.</math>|{{EquationRef|2}}}}\n\nSince the matrix-valued solution {{math|Φ}} satisfies the equation {{math|Φ' {{=}} ''A''Φ}}, we have for every entry of the matrix {{math|Φ'}}\n\n:<math>\\Phi'_{i,k}=\\sum_{j=1}^n a_{i,j}\\Phi_{j,k}\\,,\\qquad i,k\\in\\{1,\\ldots,n\\},</math>\n\nor for the entire row\n\n:<math>(\\Phi'_{i,1},\\dots,\\Phi'_{i,n})\n=\\sum_{j=1}^n a_{i,j}(\\Phi_{j,1},\\ldots,\\Phi_{j,n}), \\qquad i\\in\\{1,\\ldots,n\\}.</math>\n\nWhen we subtract from the {{math|''i''}}<sup>&thinsp;th</sup> row the linear combination\n\n:<math>\\sum_{\\scriptstyle j=1\\atop\\scriptstyle j\\not=i}^n a_{i,j}(\\Phi_{j,1},\\ldots,\\Phi_{j,n}),</math>\n\nof all the other rows, then the value of the determinant remains unchanged, hence\n\n:<math>\\det\\begin{pmatrix}\n\\Phi_{1,1}&\\Phi_{1,2}&\\cdots&\\Phi_{1,n}\\\\\n\\vdots&\\vdots&&\\vdots\\\\\n\\Phi'_{i,1}&\\Phi'_{i,2}&\\cdots&\\Phi'_{i,n}\\\\\n\\vdots&\\vdots&&\\vdots\\\\\n\\Phi_{n,1}&\\Phi_{n,2}&\\cdots&\\Phi_{n,n}\n\\end{pmatrix}\n=\\det\\begin{pmatrix}\n\\Phi_{1,1}&\\Phi_{1,2}&\\cdots&\\Phi_{1,n}\\\\\n\\vdots&\\vdots&&\\vdots\\\\\na_{i,i}\\Phi_{i,1}&a_{i,i}\\Phi_{i,2}&\\cdots&a_{i,i}\\Phi_{i,n}\\\\\n\\vdots&\\vdots&&\\vdots\\\\\n\\Phi_{n,1}&\\Phi_{n,2}&\\cdots&\\Phi_{n,n}\n\\end{pmatrix}\n=a_{i,i}\\det\\Phi</math>\n\nfor every {{math|''i'' ∈ {1, . . . , ''n''}}} by the linearity of the determinant with respect to every row. Hence\n\n{{NumBlk|:|<math>(\\det\\Phi)'=\\sum_{i=1}^n a_{i,i}\\det\\Phi=\\mathrm{tr}\\,A\\,\\det\\Phi</math>|{{EquationRef|3}}}}\n\nby  ({{EquationNote|2}}) and the definition of the trace. It remains to show that this representation of the derivative implies Liouville's formula.\n\nFix {{math|''x''<sub>0</sub> ∈ ''I''}}. Since the trace of {{math|''A''}} is assumed to be continuous function on {{math|''I''}}, it is bounded on every closed and bounded subinterval of {{math|''I''}} and therefore integrable, hence\n\n:<math>g(x):=\\det\\Phi(x) \\exp\\left(-\\int_{x_0}^x \\mathrm{tr}\\,A(\\xi) \\,\\textrm{d}\\xi\\right), \\qquad x\\in I,</math>\n\nis a well defined function. Differentiating both sides, using the product rule, the [[chain rule]], the derivative of the [[exponential function]] and the [[fundamental theorem of calculus]], we obtain\n\n:<math>g'(x)=\\bigl((\\det\\Phi(x))'-\\det\\Phi(x)\\,\\mathrm{tr}\\,A(x)\\bigr)\\exp\\biggl(-\\int_{x_0}^x \\mathrm{tr}\\,A(\\xi) \\,\\textrm{d}\\xi\\biggr)=0,\\qquad x\\in I,</math>\n\ndue to the derivative in  ({{EquationNote|3}}). Therefore, {{math|''g''}} has to be constant on {{math|''I''}}, because otherwise we would obtain a contradiction to the [[mean value theorem]] (applied separately to the real and imaginary part in the complex-valued case). Since {{math|''g''(''x''<sub>0</sub>) {{=}} det Φ(''x''<sub>0</sub>)}}, Liouville's formula follows by solving the definition of {{math|''g''}} for {{math|det Φ(''x'')}}.\n\n==References==\n* {{Citation\n  | first = Carmen \n  | last = Chicone\n  | title = Ordinary Differential Equations with Applications\n  | place = New York\n  | publisher = Springer-Verlag\n  | year = 2006\n  | edition = 2\n  | pages = 152–153\n  | isbn = 978-0-387-30769-5\n  | mr = 2224508\n  | zbl = 1120.34001}}\n* {{Citation\n | last = Teschl\n | first = Gerald\n | authorlink=Gerald Teschl\n | title = Ordinary Differential Equations and Dynamical Systems\n | publisher=[[American Mathematical Society]]\n | place = [[Providence, Rhode Island|Providence]]\n | year = 2012\n | url = http://www.mat.univie.ac.at/~gerald/ftp/book-ode/\n | mr = 2961944\n | zbl = 1263.34002}}\n\n[[Category:Mathematical identities]]\n[[Category:Ordinary differential equations]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "List of vector calculus identities",
      "url": "https://en.wikipedia.org/wiki/List_of_vector_calculus_identities",
      "text": "#REDIRECT [[Vector calculus identities]]\n\n[[Category:Mathematical identities]]"
    },
    {
      "title": "Maximum-minimums identity",
      "url": "https://en.wikipedia.org/wiki/Maximum-minimums_identity",
      "text": "In mathematics, the '''maximum-minimums identity''' is a relation between the maximum element of a set ''S'' of ''n'' numbers and the minima of the 2<sup>''n''</sup>&nbsp;&minus;&nbsp;1 nonempty subsets of ''S''.\n\nLet ''S'' = {''x''<sub>1</sub>, ''x''<sub>2</sub>, ..., ''x''<sub>''n''</sub>}.  The [[identity (mathematics)|identity]] states that \n\n:<math>\\begin{align}\n\\max\\{x_1,x_2,\\ldots,x_{n}\\} \n& = \\sum_{i=1}^n x_i - \\sum_{i<j}\\min\\{x_i,x_j\\} +\\sum_{i<j<k}\\min\\{x_i,x_j,x_k\\} - \\cdots \\\\\n& \\qquad \\cdots + \\left(-1\\right)^{n+1}\\min\\{x_1,x_2,\\ldots,x_n\\},\\end{align}</math>\nor conversely\n\n:<math>\\begin{align}\n\\min\\{x_1,x_2,\\ldots,x_{n}\\} \n& = \\sum_{i=1}^n x_i - \\sum_{i<j}\\max\\{x_i,x_j\\} +\\sum_{i<j<k}\\max\\{x_i,x_j,x_k\\} - \\cdots \\\\\n& \\qquad \\cdots + \\left(-1\\right)^{n+1}\\max\\{x_1,x_2,\\ldots,x_n\\}.\n\\end{align}\n</math>\n\n<!--\nOK, this didn't go as planned\n==Inductive proof==\nClearly the identity holds for ''n'' = 1. Assume it holds for ''n'' and let \n:<math>\nM_n = \\max(x_1,x_2,\\ldots,x_n)\n</math>\nThen for arbitrary ''x''<sub>''n''+1</sub>,\n:<math>\nM_{n+1} = \\max(x_{n+1}, M_n)\n</math>\nand\n:<math>\nM_{n+1} = x_{n+1} + M_n - \\min(x_{n+1}, M_n)\n</math>\nBy the inductive hypothesis,\n:<math>\nM_{n+1} = x_{n+1} + \\sum_{i=1}^n x_i - \\sum_{i<j}\\min(x_i,x_j) +\\sum_{i<j<k}\\min(x_i,x_j,x_k) +\\cdots+\\left(-1\\right)^{n+1}\\min(x_1,x_2,\\ldots,x_n) - \\min(x_{n+1}, \\sum_{i=1}^n x_i - \\sum_{i<j}\\min(x_i,x_j) +\\sum_{i<j<k}\\min(x_i,x_j,x_k) +\\cdots+\\left(-1\\right)^{n}\\min(x_1,x_2,\\ldots,x_n))\n</math>\ncollecting terms and applying the fact that the minimum operator is [[distributive property|distributive]],\n:<math>\nM_{n+1} = \\sum_{i=1}^{n+1} x_i - \\sum_{i<j}\\min(x_i,x_j) +\\sum_{i<j<k}\\min(x_i,x_j,x_k) +\\cdots+\\left(-1\\right)^{n+1}\\min(x_1,x_2,\\ldots,x_n) - \\min(x_{n+1}, \\sum_{i=1}^n x_i) + \\min(x_{n+1}, \\sum_{i<j}\\min(x_i,x_j)) - \\min(x_{n+1},\\sum_{i<j<k}\\min(x_i,x_j,x_k) )+\\cdots+\\min(x_{n+1}, \\left(-1\\right)^{n+1}\\min(x_1,x_2,\\ldots,x_n))\n</math>\n-->\nFor a probabilistic proof, see the reference.\n\n== See also ==\n\n* [[Inclusion–exclusion principle]]\n* [[Zaskulnikov's identity]]\n\n==References==\n* {{cite book | last = Ross | first = Sheldon | title = A First Course in Probability | publisher = Prentice Hall | location = Englewood Cliffs | year = 2002 | isbn = 0-13-033851-6 }}\n\n[[Category:Mathematical identities]]"
    },
    {
      "title": "Mingarelli identity",
      "url": "https://en.wikipedia.org/wiki/Mingarelli_identity",
      "text": "In the field of [[ordinary differential equation]]s, the '''Mingarelli identity'''<ref name=\"Hartman\">The locution was coined by [[Philip Hartman]], according to {{harvtxt|Clark D.N., G. Pecelli, and R. Sacksteder|1981}}</ref> is a theorem that provides criteria for the [[oscillation theory|oscillation]] and [[oscillation theory|non-oscillation]] of solutions of some [[linear differential equation]]s in the real domain. It extends the [[Picone identity]] from two to three or more differential equations of the second order.\n\n== The identity ==\nConsider the {{mvar|n}} solutions of the following (uncoupled) system of second order linear differential equations over the {{mvar|t}}–interval {{math|[''a'',&nbsp;''b'']}}:\n:<math>(p_i(t) x_i^\\prime)^\\prime + q_i(t) x_i = 0, \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, x_i(a)=1,\\,\\, x_i^\\prime(a)=R_i</math> where <math>i=1,2, \\ldots, n</math>. \nLet <math>\\Delta</math> denote the forward difference operator, i.e.\n:<math>\\Delta x_i = x_{i+1}-x_i.</math> \nThe second order difference operator is found by iterating the first order operator as in \n:<math>\\Delta^2 (x_i) = \\Delta(\\Delta x_i) = x_{i+2}-2x_{i+1}+x_{i},</math>, \nwith a similar definition for the higher iterates. Leaving out the independent variable {{mvar|t}} for convenience, and assuming the {{math|''x{{sub|i}}''(''t'')&nbsp;≠&nbsp;0}} on {{math|(''a'',&nbsp;''b'']}}, there holds the identity,<ref name=\"Mingarelli.223\">{{harv|Mingarelli|1979|p=223}}.</ref>\n: <math>\n\\begin{align}\nx_{n-1}^2\\Delta^{n-1}(p_1r_1)]_a^b & = \\int_a^b (x^\\prime_{n-1})^2 \\Delta^{n-1}(p_1) - \\int_a^b x_{n-1}^2 \\Delta^{n-1}(q_1) \n- \\sum_{k=0}^{n-1} C(n-1,k)(-1)^{n-k-1}\\int_a^b p_{k+1} W^2(x_{k+1},x_{n-1})/x_{k+1}^2,\n\\end{align}\n</math>\n\nwhere \n*<math>r_i = x^\\prime_i/x_i</math> is the [[logarithmic derivative]], \n*<math>W(x_i, x_j) = x^\\prime_ix_j - x_ix^\\prime_j</math>, is the [[Wronskian determinant]], \n*<math>C(n-1,k)</math> are [[binomial coefficients]]. \nWhen {{math|''n''&nbsp;{{=}}&nbsp;2}} this equality reduces to the [[Picone identity]].\n\n==An application==\nThe above identity leads quickly to the following comparison theorem for three linear differential equations,<ref name=\"Mingarelli.th2\">{{harv|Mingarelli|1979|loc=Theorem 2}}.</ref> which extends the classical [[Sturm–Picone comparison theorem]].\n\nLet {{mvar|p{{sub|i}}}}, {{mvar|q{{sub|i}}}} {{math|''i''&nbsp;{{=}}&nbsp;1,&nbsp;2,&nbsp;3}}, be real-valued continuous functions on the interval {{math|[''a'',&nbsp;''b'']}} and let\n#<math>(p_1(t) x_1^\\prime)^\\prime + q_1(t) x_1 = 0, \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, x_1(a)=1,\\,\\, x_1^\\prime(a)=R_1</math> \n#<math>(p_2(t) x_2^\\prime)^\\prime + q_2(t) x_2 = 0, \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, x_2(a)=1,\\,\\,  x_2^\\prime(a)=R_2</math>\n#<math>(p_3(t) x_3^\\prime)^\\prime + q_3(t) x_3 = 0, \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, x_3(a)=1,\\,\\,  x_3^\\prime(a)=R_3</math>\nbe three homogeneous linear second order differential equations in [[self-adjoint form]], where \n*{{math|''p{{sub|i}}''(''t'')&nbsp;>&nbsp;0}}  for each {{mvar|i}} and for all {{mvar|t}} in {{math|[''a'',&nbsp;''b'']}} , and \n*the {{mvar|R{{sub|i}}}} are arbitrary real numbers.\n\nAssume that for all {{math|''t''}} in {{math|[''a'',&nbsp;''b'']}} we have,\n:<math>\\Delta^2(q_1) \\ge 0 </math>,\n:<math>\\Delta^2(p_1) \\le 0 </math>,\n:<math>\\Delta^2(p_1(a)R_1) \\le 0 </math>.\nThen, if {{math|''x''{{sub|1}}(''t'')&nbsp;>&nbsp;0}} on {{math|[''a'',&nbsp;''b'']}} and {{math|''x''{{sub|2}}(''b'')&nbsp;{{=}}&nbsp;0}}, then any solution {{math|''x''{{sub|3}}(''t'')}} has at least one zero in {{math|[''a'',&nbsp;''b'']}}.\n\n==Notes==\n{{reflist}}\n\n==References==\n*{{cite book\n|author1=Clark D.N.\n|author2=G. Pecelli\n|author3=R. Sacksteder\n|last-author-amp=yes\n|year=1981\n|author2-link= G. Pecelli\n|author3-link=R. Sacksteder\n|language=\n|title=Contributions to Analysis and Geometry\n|series=\n|volume=\n|pages=ix+357\n|location= Baltimore, USA\n|publisher= Johns Hopkins University Press\n|isbn=0-80182-779-5\n|url=https://books.google.com/books?id=SsJPAQAAIAAJ&hl\n|ref=harv}}\n*{{cite journal\n|last=Mingarelli\n|first= Angelo B.\n|year=1979\n|language=\n|title= Some extensions of the Sturm–Picone theorem\n|journal= [[Mathematical Reports of the Academy of Science|Comptes Rendus Mathématique]]\n|series=\n|volume=1 \n| issue = 4 \n|pages=223–226\n|location= Toronto, Ontario, Canada\n|publisher= The Royal Society of Canada\n|url=\n|ref=harv}}\n\n[[Category:Ordinary differential equations]]\n[[Category:Mathematical identities]]"
    },
    {
      "title": "Pascal's rule",
      "url": "https://en.wikipedia.org/wiki/Pascal%27s_rule",
      "text": "{{distinguish|Pascal's law}}\nIn [[mathematics]], '''Pascal's rule''' (or '''Pascal's formula''') is a [[combinatorics|combinatorial]] [[identity (mathematics)|identity]] about [[binomial coefficient]]s. It states that for positive [[natural number|natural numbers]] ''n'' and ''k'', \n:<math>{n-1\\choose k} + {n-1\\choose k-1} = {n\\choose k},</math> \nwhere <math>{n\\choose k}</math> is a binomial coefficient; one interpretation of which is the coefficient of the {{math|''x''<sup>''k''</sup>}} term in the [[polynomial expansion|expansion]] of {{math|(1 + ''x'')<sup>''n''</sup>}}. There is no restriction on the relative sizes of {{mvar|n}} and {{mvar|k}},<ref>{{citation|first=David R.|last=Mazur|title=Combinatorics / A Guided Tour|year=2010|publisher=Mathematical Association of America|page=60|isbn=978-0-88385-762-5}}</ref> since, if {{math|''n'' < ''k''}} the value of the binomial coefficient is zero and the identity remains valid.\n\nPascal's rule can also be generalized to apply to [[multinomial coefficient]]s.\n\n==Combinatorial proof==\n[[Blaise Pascal|Pascal's]] rule has an intuitive combinatorial meaning, that is clearly expressed in this counting proof.<ref>{{citation|first=Richard A.|last=Brualdi|title=Introductory Combinatorics|edition=5th|year=2010|publisher=Prentice-Hall|isbn=978-0-13-602040-0|page=44}}</ref> \n\n''Proof''. Recall that <math>{n\\choose k}</math> equals the number of [[subset|subsets]] with ''k'' elements from a set with ''n'' elements. Suppose one particular element is uniquely labeled ''X'' in a set with ''n'' elements. \n\nTo construct a subset of ''k'' elements containing ''X'', choose ''X'' and ''k-1'' elements from the remaining ''n-1'' elements in the set. There are <math>{n-1\\choose k-1}</math> such subsets.\n\nTo construct a subset of ''k'' elements ''not'' containing ''X'', choose ''k'' elements from the remaining ''n-1'' elements in the set. There are <math>{n-1\\choose k}</math> such subsets.\n\nEvery subset of ''k'' elements either contains ''X'' or not. The total number of subsets with ''k'' elements in a set of ''n'' elements is the sum of the number of subsets containing ''X'' and the number of subsets that do not contain ''X'', <math>{n-1\\choose k-1} + {n-1\\choose k}</math>.\n\nThis equals <math>{n\\choose k}</math>; therefore, <math>{n\\choose k} = {n-1\\choose k-1} + {n-1\\choose k}</math>. \n\n==Algebraic proof==\n\nAlternatively, the algebraic derivation of the binomial case follows.\n\n<math>\\begin{align}\n{ n-1 \\choose k } + { n-1 \\choose k-1} & = \\frac{(n-1)!}{k! (n - 1 - k)!} + \\frac{(n-1)!}{(k - 1)!(n - k)!} \\\\\n& = (n-1)! \\left[ \\frac{n - k}{k!(n - k)!} + \\frac{k}{k!(n - k)!}\\right] \\\\\n& = (n-1)! \\frac{n}{k!(n - k)!} \\\\\n& = \\frac{n!}{k!(n - k)!} \\\\\n& = \\binom{n}{k}.\n\\end{align}\n</math>\n\n==Generalization==\nPascal's rule can be generalized to multinomial coefficients.<ref name=Brualdi>{{citation|first=Richard A.|last=Brualdi|title=Introductory Combinatorics|edition=5th|year=2010|publisher=Prentice-Hall|isbn=978-0-13-602040-0|page=144}}</ref> For any integer ''p'' such that <math>p \\ge 2</math>, <math>k_1, k_2, k_3,\\dots, k_p \\in \\mathbb{N}^* </math>, and <math>n=k_1+k_2+k_3+ \\cdots +k_p \\ge 1</math>,\n:<math>{n-1\\choose k_1-1,k_2,k_3, \\dots, k_p}+{n-1\\choose k_1,k_2-1,k_3,\\dots, k_p}+\\cdots+{n-1\\choose k_1,k_2,k_3,\\dots,k_p-1} = {n\\choose k_1, k_2, k_3, \\dots , k_p}</math>\nwhere <math>{n\\choose k_1, k_2, k_3, \\dots , k_p}</math> is the coefficient of the <math>x_1^{k_1}x_2^{k_2}\\dots x_p^{k_p}</math> term in the expansion of <math>(x_1+x_2+\\dots+x_p)^{n}</math>.\n\nThe algebraic derivation for this general case is as follows.<ref name=Brualdi /> Let ''p'' be an integer such that <math>p \\ge 2</math>, <math>k_1, k_2, k_3,\\dots, k_p \\in \\mathbb{N}^* </math>, and <math>n=k_1+k_2+k_3+ \\cdots +k_p \\ge 1</math>. Then\n: <math>\n\\begin{align}\n& {} \\quad {n-1\\choose k_1-1,k_2,k_3, \\dots, k_p}+{n-1\\choose k_1,k_2-1,k_3,\\dots, k_p}+\\cdots+{n-1\\choose k_1,k_2,k_3,\\dots,k_p-1} \\\\\n& = \\frac{(n-1)!}{(k_1-1)!k_2!k_3! \\cdots k_p!} + \\frac{(n-1)!}{k_1!(k_2-1)!k_3!\\cdots k_p!} + \\cdots + \\frac{(n-1)!}{k_1!k_2!k_3! \\cdots (k_p-1)!} \\\\\n& = \\frac{k_1(n-1)!}{k_1!k_2!k_3! \\cdots k_p!} + \\frac{k_2(n-1)!}{k_1!k_2!k_3! \\cdots k_p!} + \\cdots + \\frac{k_p(n-1)!}{k_1!k_2!k_3! \\cdots k_p!} = \\frac{(k_1+k_2+\\cdots+k_p) (n-1)!}{k_1!k_2!k_3!\\cdots k_p!}  \\\\\n& = \\frac{n(n-1)!}{k_1!k_2!k_3! \\cdots k_p!} = \\frac{n!}{k_1!k_2!k_3! \\cdots k_p!}\n= {n\\choose k_1, k_2, k_3, \\dots , k_p}.\n\\end{align}\n</math>\n\n==See also==\n* [[Pascal's triangle]]\n\n==References==\n{{reflist}}\n\n==Sources==\n*{{PlanetMath attribution|id=246|title=Pascal's rule}}\n*{{PlanetMath attribution|id=259|title= Pascal's rule proof}}\n*Merris, Russell. [http://media.wiley.com/product_data/excerpt/6X/04712629/047126296X.pdf''Combinatorics'']. John Wiley & Sons. 2003 {{ISBN|978-0-471-26296-1}}\n\n==External links==\n* {{planetmath reference|id=5936|title=Central binomial coefficient}}\n* {{planetmath reference|id=273|title=Binomial coefficient}}\n* {{planetmath reference|id=4248|title=Pascal's triangle}}\n\n{{DEFAULTSORT:Pascal's Rule}}\n[[Category:Combinatorics]]\n[[Category:Mathematical identities]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "Pfister's sixteen-square identity",
      "url": "https://en.wikipedia.org/wiki/Pfister%27s_sixteen-square_identity",
      "text": "In [[algebra]], '''Pfister's sixteen-square identity''' is a non-[[bilinear map|bilinear]] identity of form\n\n:<math>\n(x_1^2+x_2^2+x_3^2+\\cdots+x_{16}^2)\\,(y_1^2+y_2^2+y_3^2+\\cdots+y_{16}^2) = z_1^2+z_2^2+z_3^2+\\cdots+z_{16}^2\n</math>\n\nIt was first proven to exist by [[Hans Zassenhaus|H. Zassenhaus]] and W. Eichhorn in the 1960s,<ref>H. Zassenhaus and W. Eichhorn, \"Herleitung von Acht- und Sechzehn-Quadrate-Identitäten mit Hilfe von Eigenschaften der verallgemeinerten Quaternionen und der Cayley-Dicksonchen Zahlen,\" Arch. Math. 17 (1966), 492-496</ref> and independently by Pfister<ref>A. Pfister, Zur Darstellung von -1 als Summe von Quadraten in einem Körper,\" J. London Math. Soc. 40 (1965), 159-165</ref> around the same time.  There are several versions, a concise one of which is\n\n:<math>\\,^{z_1 = {\\color{blue}{x_1 y_1 - x_2 y_2 - x_3 y_3 - x_4 y_4 - x_5 y_5 - x_6 y_6 - x_7 y_7 - x_8 y_8}} + u_1 y_9 - u_2 y_{10} - u_3 y_{11} - u_4 y_{12} - u_5 y_{13} - u_6 y_{14} - u_7 y_{15} - u_8 y_{16}}</math>\n:<math>\\,^{z_2 = {\\color{blue}{x_2 y_1 + x_1 y_2 + x_4 y_3 - x_3 y_4 + x_6 y_5 - x_5 y_6 - x_8 y_7 + x_7 y_8}} + u_2 y_9 + u_1 y_{10} + u_4 y_{11} - u_3 y_{12} + u_6 y_{13} - u_5 y_{14} - u_8 y_{15} + u_7 y_{16}}</math>\n:<math>\\,^{z_3 = {\\color{blue}{x_3 y_1 - x_4 y_2 + x_1 y_3 + x_2 y_4 + x_7 y_5 + x_8 y_6 - x_5 y_7 - x_6 y_8}} + u_3 y_9 - u_4 y_{10} + u_1 y_{11} + u_2 y_{12} + u_7 y_{13} + u_8 y_{14} - u_5 y_{15} - u_6 y_{16}}</math>\n:<math>\\,^{z_4 = {\\color{blue}{x_4 y_1 + x_3 y_2 - x_2 y_3 + x_1 y_4 + x_8 y_5 - x_7 y_6 + x_6 y_7 - x_5 y_8}} + u_4 y_9 + u_3 y_{10} - u_2 y_{11} + u_1 y_{12} + u_8 y_{13} - u_7 y_{14} + u_6 y_{15} - u_5 y_{16}}</math>\n:<math>\\,^{z_5 = {\\color{blue}{x_5 y_1 - x_6 y_2 - x_7 y_3 - x_8 y_4 + x_1 y_5 + x_2 y_6 + x_3 y_7 + x_4 y_8}} + u_5 y_9 - u_6 y_{10} - u_7 y_{11} - u_8 y_{12} + u_1 y_{13} + u_2 y_{14} + u_3 y_{15} + u_4 y_{16}}</math>\n:<math>\\,^{z_6 = {\\color{blue}{x_6 y_1 + x_5 y_2 - x_8 y_3 + x_7 y_4 - x_2 y_5 + x_1 y_6 - x_4 y_7 + x_3 y_8}} + u_6 y_9 + u_5 y_{10} - u_8 y_{11} + u_7 y_{12} - u_2 y_{13} + u_1 y_{14} - u_4 y_{15} + u_3 y_{16}}</math>\n:<math>\\,^{z_7 = {\\color{blue}{x_7 y_1 + x_8 y_2 + x_5 y_3 - x_6 y_4 - x_3 y_5 + x_4 y_6 + x_1 y_7 - x_2 y_8}} + u_7 y_9 + u_8 y_{10} + u_5 y_{11} - u_6 y_{12} - u_3 y_{13} + u_4 y_{14} + u_1 y_{15} - u_2 y_{16}}</math>\n:<math>\\,^{z_8 = {\\color{blue}{x_8 y_1 - x_7 y_2 + x_6 y_3 + x_5 y_4 - x_4 y_5 - x_3 y_6 + x_2 y_7 + x_1 y_8}} + u_8 y_9 - u_7 y_{10} + u_6 y_{11} + u_5 y_{12} - u_4 y_{13} - u_3 y_{14} + u_2 y_{15} + u_1 y_{16}}</math>\n:<math>\\,^{z_9  =  x_9 y_1 - x_{10} y_2 - x_{11} y_3 - x_{12} y_4 - x_{13} y_5 - x_{14} y_6 - x_{15} y_7 - x_{16} y_8 + x_1 y_9 - x_2 y_{10} - x_3 y_{11} - x_4 y_{12} - x_5 y_{13} - x_6 y_{14} - x_7 y_{15} - x_8 y_{16}}</math>\n:<math>\\,^{z_{10} = x_{10} y_1 + x_9 y_2 + x_{12} y_3 - x_{11} y_4 + x_{14} y_5 - x_{13} y_6 - x_{16} y_7 + x_{15} y_8 + x_2 y_9 + x_1 y_{10} + x_4 y_{11} - x_3 y_{12} + x_6 y_{13} - x_5 y_{14} - x_8 y_{15} + x_7 y_{16}}</math>\n:<math>\\,^{z_{11} = x_{11} y_1 - x_{12} y_2 + x_9 y_3 + x_{10} y_4 + x_{15} y_5 + x_{16} y_6 - x_{13} y_7 - x_{14} y_8 + x_3 y_9 - x_4 y_{10} + x_1 y_{11} + x_2 y_{12} + x_7 y_{13} + x_8 y_{14} - x_5 y_{15} - x_6 y_{16}}</math>\n:<math>\\,^{z_{12} = x_{12} y_1 + x_{11} y_2 - x_{10} y_3 + x_9 y_4 + x_{16} y_5 - x_{15} y_6 + x_{14} y_7 - x_{13} y_8 + x_4 y_9 + x_3 y_{10} - x_2 y_{11} + x_1 y_{12} + x_8 y_{13} - x_7 y_{14} + x_6 y_{15} - x_5 y_{16}}</math>\n:<math>\\,^{z_{13} = x_{13} y_1 - x_{14} y_2 - x_{15} y_3 - x_{16} y_4 + x_9 y_5 + x_{10} y_6 + x_{11} y_7 + x_{12} y_8 + x_5 y_9 - x_6 y_{10} - x_7 y_{11} - x_8 y_{12} + x_1 y_{13} + x_2 y_{14} + x_3 y_{15} + x_4 y_{16}}</math>\n:<math>\\,^{z_{14} = x_{14} y_1 + x_{13} y_2 - x_{16} y_3 + x_{15} y_4 - x_{10} y_5 + x_9 y_6 - x_{12} y_7 + x_{11} y_8 + x_6 y_9 + x_5 y_{10} - x_8 y_{11} + x_7 y_{12} - x_2 y_{13} + x_1 y_{14} - x_4 y_{15} + x_3 y_{16}}</math>\n:<math>\\,^{z_{15} = x_{15} y_1 + x_{16} y_2 + x_{13} y_3 - x_{14} y_4 - x_{11} y_5 + x_{12} y_6 + x_9 y_7 - x_{10} y_8 + x_7 y_9 + x_8 y_{10} + x_5 y_{11} - x_6 y_{12} - x_3 y_{13} + x_4 y_{14} + x_1 y_{15} - x_2 y_{16}}</math>\n:<math>\\,^{z_{16} = x_{16} y_1 - x_{15} y_2 + x_{14} y_3 + x_{13} y_4 - x_{12} y_5 - x_{11} y_6 + x_{10} y_7 + x_9 y_8 + x_8 y_9 - x_7 y_{10} + x_6 y_{11} + x_5 y_{12} - x_4 y_{13} - x_3 y_{14} + x_2 y_{15} + x_1 y_{16}}</math>\n\nIf all <math>x_i</math> and <math>y_i</math> with <math>i>8</math> are set equal to zero, then it reduces to [[Degen's eight-square identity]] (in blue). The <math>u_i</math> are\n\n:<math>u_1 = \\tfrac{(ax_1^2+x_2^2+x_3^2+x_4^2+x_5^2+x_6^2+x_7^2+x_8^2)x_9 - 2x_1(bx_1 x_9 +x_2 x_{10} +x_3 x_{11} +x_4 x_{12} +x_5 x_{13} +x_6 x_{14} +x_7 x_{15} +x_8 x_{16})}{c}</math>\n:<math>u_2 = \\tfrac{(x_1^2+ax_2^2+x_3^2+x_4^2+x_5^2+x_6^2+x_7^2+x_8^2)x_{10} - 2x_2(x_1 x_9 +bx_2 x_{10} +x_3 x_{11} +x_4 x_{12} +x_5 x_{13} +x_6 x_{14} +x_7 x_{15} +x_8 x_{16})}{c}</math>\n:<math>u_3 = \\tfrac{(x_1^2+x_2^2+ax_3^2+x_4^2+x_5^2+x_6^2+x_7^2+x_8^2)x_{11} - 2x_3(x_1 x_9 +x_2 x_{10} +bx_3 x_{11} +x_4 x_{12} +x_5 x_{13} +x_6 x_{14} +x_7 x_{15} +x_8 x_{16})}{c}</math>\n:<math>u_4 = \\tfrac{(x_1^2+x_2^2+x_3^2+ax_4^2+x_5^2+x_6^2+x_7^2+x_8^2)x_{12} - 2x_4(x_1 x_9 +x_2 x_{10} +x_3 x_{11} +bx_4 x_{12} +x_5 x_{13} +x_6 x_{14} +x_7 x_{15} +x_8 x_{16})}{c}</math>\n:<math>u_5 = \\tfrac{(x_1^2+x_2^2+x_3^2+x_4^2+ax_5^2+x_6^2+x_7^2+x_8^2)x_{13} - 2x_5(x_1 x_9 +x_2 x_{10} +x_3 x_{11} +x_4 x_{12} +bx_5 x_{13} +x_6 x_{14} +x_7 x_{15} +x_8 x_{16})}{c}</math>\n:<math>u_6 = \\tfrac{(x_1^2+x_2^2+x_3^2+x_4^2+x_5^2+ax_6^2+x_7^2+x_8^2)x_{14} - 2x_6(x_1 x_9 +x_2 x_{10} +x_3 x_{11} +x_4 x_{12} +x_5 x_{13} +bx_6 x_{14} +x_7 x_{15} +x_8 x_{16})}{c}</math>\n:<math>u_7 = \\tfrac{(x_1^2+x_2^2+x_3^2+x_4^2+x_5^2+x_6^2+ax_7^2+x_8^2)x_{15} - 2x_7(x_1 x_9 +x_2 x_{10} +x_3 x_{11} +x_4 x_{12} +x_5 x_{13} +x_6 x_{14} +bx_7 x_{15} +x_8 x_{16})}{c}</math>\n:<math>u_8 = \\tfrac{(x_1^2+x_2^2+x_3^2+x_4^2+x_5^2+x_6^2+x_7^2+ax_8^2)x_{16} - 2x_8(x_1 x_9 +x_2 x_{10} +x_3 x_{11} +x_4 x_{12} +x_5 x_{13} +x_6 x_{14} +x_7 x_{15} +bx_8 x_{16})}{c}</math>\n\nand,\n\n:<math>a=-1,\\;\\;b=0,\\;\\;c=x_1^2+x_2^2+x_3^2+x_4^2+x_5^2+x_6^2+x_7^2+x_8^2\\,.</math>\n\nThe identity shows that, in general, the product of two sums of sixteen squares is the sum of sixteen [[rational number|rational]] squares. Incidentally, the <math>u_i</math> also obey,\n\n:<math>u_1^2+u_2^2+u_3^2+u_4^2+u_5^2+u_6^2+u_7^2+u_8^2 = x_{9}^2+x_{10}^2+x_{11}^2+x_{12}^2+x_{13}^2+x_{14}^2+x_{15}^2+x_{16}^2</math>\n\nNo sixteen-square identity exists involving only bilinear functions since [[Hurwitz's theorem (normed division algebras)|Hurwitz's theorem]] states an identity of the form\n\n:<math>(x_1^2+x_2^2+x_3^2+\\cdots+x_n^2)(y_1^2+y_2^2+y_3^2+\\cdots+y_n^2) = z_1^2+z_2^2+z_3^2+\\cdots+z_n^2</math>\n\nwith the <math>z_i</math> [[bilinear map|bilinear]] functions of the <math>x_i</math> and <math>y_i</math> is possible only for ''n'' &isin; {1, 2, 4, 8} . However, the more general [[Pfister's theorem]] (1965)  shows that if the <math>z_i</math> are [[rational functions]] of one set of variables, hence has a [[denominator]], then it is possible for all <math>n = 2^m</math>.<ref>Pfister's Theorem on Sums of Squares, Keith Conrad, http://www.math.uconn.edu/~kconrad/blurbs/linmultialg/pfister.pdf</ref> There are also non-bilinear versions of [[Euler's four-square identity|Euler's four-square]] and [[Degen's eight-square identity|Degen's eight-square]] identities.\n\n==See also==\n* [[Brahmagupta–Fibonacci identity]]\n* [[Euler's four-square identity]]\n* [[Degen's eight-square identity]]\n* [[Sedenions]]\n\n==References==\n<references/>\n\n==External links==\n*[http://sites.google.com/site/tpiezas/0021c Pfister's 16-Square Identity]\n\n[[Category:Analytic number theory]]\n[[Category:Mathematical identities]]"
    },
    {
      "title": "Picone identity",
      "url": "https://en.wikipedia.org/wiki/Picone_identity",
      "text": "In the field of [[ordinary differential equation]]s, the '''Picone identity''', named after [[Mauro Picone]],<ref name=\"Picone\">{{harvnb|Picone|1910}}</ref> is a classical result about [[homogeneous differential equation|homogeneous]] linear second order differential equations. Since its inception in 1910 it has been used with tremendous success in association with an almost immediate proof of the [[Sturm comparison theorem]], a theorem whose proof took up many pages in Sturm's original memoir of 1836. It is also useful in studying the [[oscillation (differential equation)|oscillation]] of such equations and has been generalized to other type of [[differential equation]]s and [[difference equation]]s.\n\nThe Picone identity is used to prove the [[Sturm–Picone comparison theorem]].\n\n== Picone identity ==\nSuppose that ''u'' and ''v'' are solutions of the two homogeneous linear second order differential equations in [[self-adjoint form]]\n:<math>(p_1(x) u')' + q_1(x) u = 0 </math>\nand\n:<math>(p_2(x) v')' + q_2(x) v = 0. </math>\nThen, for all ''x'' with ''v''(''x'') ≠ 0, the following identity holds\n:<math>\\left(\\frac{u}{v}(p_1 u' v - p_2 u v')\\right)' = \\left(q_2 - q_1\\right) u^2 + \\left(p_1 - p_2\\right)u'^2 + p_2\\left(u'-v'\\frac{u}{v}\\right)^2.</math>\n\n=== Proof ===\n\n<math>\\left(\\frac{u}{v}(p_1 u' v - p_2 u v')\\right)' \n= p_1u'^2-p_2\\frac {u'} v u v'-p_1\\frac {u v' u'}{v}+p_2\\frac {u^2 v'^2}{v^2} \n+ \\frac u v (p_1u')'v+p_1\\frac{u u' v '}v-\\frac u v (p_2 v')'u - p_2\\frac{u v' u'} v =</math>\n\n<math>= p_1u'^2-p_2u'^2+p_2u'^2-2p_2\\frac u v u' v'+p_2\\frac{u^2}{v^2}v'^2-\\frac u v (q_1u)v+\\frac u v (q_2 v)u \n= \\left(p_1 - p_2\\right)u'^2 + p_2\\left(u'-v'\\frac{u}{v}\\right)^2 + \\left(q_2 - q_1\\right) u^2</math>\n\n==Notes==\n\n* {{cite journal\n  | last = Picone\n  | first = Mauro\n  | authorlink = Mauro Picone\n  | title = Sui valori eccezionali di un parametro da cui dipende un’equazione differenziale lineare del secondo ordine\n  | journal = Ann. Scuola Norm. Sup. Pisa\n  | volume = 11\n  | pages = 1–141\n  | year = 1910\n  | ref=harv}}\n* {{cite journal\n  | last = Swanson\n  | first = Charles A.\n  | title = Picone's Identity\n  | journal = Rendiconti di Matematica\n  | volume = 8\n  | issue = 2\n  | pages = 373–397\n  | year = 1975 \n  | ref=harv}}\n\n== References ==\n{{Reflist}}\n\n[[Category:Ordinary differential equations]]\n[[Category:Mathematical identities]]"
    },
    {
      "title": "Rothe–Hagen identity",
      "url": "https://en.wikipedia.org/wiki/Rothe%E2%80%93Hagen_identity",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|Mathematical theorem}}\nIn [[mathematics]], the '''Rothe–Hagen identity''' is a [[mathematical identity]] valid for all [[complex number]]s (<math>x, y, z</math>) except where its denominators [[Zero of a function|vanish]]:\n\n:<math>\\sum_{k=0}^n\\frac{x}{x+kz}{x+kz \\choose k}\\frac{y}{y+(n-k)z}{y+(n-k)z \\choose n-k}=\\frac{x+y}{x+y+nz}{x+y+nz \\choose n}.</math>\n\nIt is a generalization of [[Vandermonde's identity]], and is named after [[Heinrich August Rothe]] and [[Johann Georg Hagen]].\n\n==References==\n*{{citation\n | last = Chu | first = Wenchang\n | issue = 1\n | journal = [[Electronic Journal of Combinatorics]]\n | at = N24\n | title = Elementary proofs for convolution identities of Abel and Hagen-Rothe\n | url = http://www.combinatorics.org/ojs/index.php/eljc/article/view/v17i1n24\n | volume = 17\n | year = 2010}}.\n*{{citation\n | last = Gould | first = H. W.\n | journal = [[The American Mathematical Monthly]]\n | jstor = 2306429\n | mr = 0075170\n | pages = 84–91\n | title = Some generalizations of Vandermonde's convolution\n | volume = 63\n | year = 1956}}. See especially pp.&nbsp;89–91.\n*{{citation\n | last = Hagen | first = Johann G. | authorlink = Johann Georg Hagen\n | title = Synopsis Der Hoeheren Mathematik\n | at = formula 17, pp. 64–68, vol. I\n | location = Berlin\n | year = 1891}}. As cited by {{harvtxt|Gould|1956}}.\n*{{citation\n | last = Ma | first = Xinrong\n | doi = 10.1016/j.jcta.2010.12.012\n | issue = 4\n | journal = [[Journal of Combinatorial Theory]] | series = Series A\n | mr = 2763069\n | pages = 1475–1493\n | title = Two matrix inversions associated with the Hagen-Rothe formula, their ''q''-analogues and applications\n | volume = 118\n | year = 2011}}.\n*{{citation\n | last = Rothe | first = Heinrich August\n | title = Formulae De Serierum Reversione Demonstratio Universalis Signis Localibus Combinatorio-Analyticorum Vicariis Exhibita: Dissertatio Academica\n | url = https://books.google.com/books/about/Formulae_De_Serierum_Reversione_Demonstr.html\n | location = Leipzig\n | year = 1793}}. As cited by {{harvtxt|Gould|1956}}.\n\n{{DEFAULTSORT:Rothe-Hagen identity}}\n[[Category:Factorial and binomial topics]]\n[[Category:Mathematical identities]]\n[[Category:Complex analysis]]\n\n\n{{mathapplied-stub}}"
    },
    {
      "title": "Siegel identity",
      "url": "https://en.wikipedia.org/wiki/Siegel_identity",
      "text": "In [[mathematics]], '''Siegel's identity''' refers to one of two formulae that are used in the resolution of [[Diophantine equation]]s.\n\n==Statement==\nThe first formula is\n\n:<math> \\frac{x_3 - x_1}{x_2 - x_1} + \\frac{x_2 - x_3}{x_2 - x_1} = 1 . </math>\n\nThe second is\n\n:<math> \\frac{x_3 - x_1}{x_2 - x_1} \\cdot\\frac{t - x_2}{t - x_3} + \\frac{x_2 - x_3}{x_2 - x_1} \\cdot \\frac{t - x_1}{t - x_3} = 1 . </math>\n\n==Application==\nThe identities are used in translating Diophantine problems connected with integral points on [[hyperelliptic curve]]s into [[S-unit equation]]s.\n\n==See also==\n* [[Siegel formula]]\n\n==References==\n* {{cite book | first=Alan | last=Baker | authorlink=Alan Baker (mathematician) | title=Transcendental Number Theory | publisher=[[Cambridge University Press]] | year=1975 | isbn=0-521-20461-5 | zbl=0297.10013 | page=40 }}\n* {{cite book | first1=Alan | last1=Baker | authorlink1=Alan Baker (mathematician)| first2=Gisbert | last2= Wüstholz | authorlink2=Gisbert Wüstholz | title=Logarithmic Forms and Diophantine Geometry | series=New Mathematical Monographs | volume=9 | publisher=[[Cambridge University Press]] | year=2007 | isbn=978-0-521-88268-2 | zbl=1145.11004 | page=53 }}\n* {{cite book | first1=Daniel S. | last1=Kubert | authorlink1=Daniel Kubert | first2=Serge | last2=Lang | authorlink2=Serge Lang | title=Modular Units | series= Grundlehren der Mathematischen Wissenschaften | volume=244 | year=1981 | isbn=0-387-90517-0 }}\n* {{cite book | first=Serge | last=Lang | authorlink=Serge Lang | title=Elliptic Curves: Diophantine Analysis | volume=231 | series=Grundlehren der mathematischen Wissenschaften | publisher=[[Springer-Verlag]] | year=1978 | isbn=0-387-08489-4 }}\n* {{cite book | title=The Algorithmic Resolution of Diophantine Equations | volume=41 | series=London Mathematical Society Student Texts | first=N. P. | last=Smart | authorlink=Nigel Smart (cryptographer) | publisher=[[Cambridge University Press]] | year=1998 | isbn=0-521-64633-2 | pages=36–37 }}\n\n[[Category:Mathematical identities]]\n[[Category:Diophantine equations]]"
    },
    {
      "title": "Sommerfeld identity",
      "url": "https://en.wikipedia.org/wiki/Sommerfeld_identity",
      "text": "The '''Sommerfeld identity''' is a mathematical identity, due [[Arnold Sommerfeld]], used in the theory of propagation of waves, \n\n:<math>\n\\frac{{e^{ik R} }}\n{R} = \\int\\limits_0^\\infty I_0(\\lambda r) e^{ - \\mu \\left| z \\right| } \\frac{{\\lambda d \\lambda}}{{\\mu}}\n</math>\n\nwhere\n:<math>\n\\mu = \n\\sqrt {\\lambda ^2  - k^2 } \n</math>\nis to be taken with positive real part, to ensure the convergence of the integral and its vanishing in the limit <math> z  \\rightarrow \\pm \\infty </math> and\n:<math>\nR^2=r^2+z^2\n</math>.\nHere, <math>R</math> is the distance from the origin while <math>r</math> is the distance from the central axis of a cylinder as in the <math>(r,\\phi,z)</math> [[cylindrical coordinate system]]. Here the notation for Bessel functions follows the German convention, to be consistent with the original notation used by Sommerfeld. The function <math>I_0(z)</math> is the zeroth-order [[Bessel function]] of the first kind, better known by the notation <math>I_0(z)=J_0(iz)</math> in English literature.\nThis identity is known as the '''Sommerfeld Identity''' [Ref.1,Pg.242]. \n\nIn alternative notation, the Sommerfeld identity can be more easily seen as an expansion of a spherical wave in terms of cylindrically-symmetric waves,\n:<math>\n\\frac{{e^{ik_0 r} }}\n{r} = i\\int\\limits_0^\\infty  {dk_\\rho  \\frac{{k_\\rho  }}\n{{k_z }}J_0 (k_\\rho  \\rho )e^{ik_z \\left| z \\right|} } \n</math>\nWhere\n:<math>\nk_z=(k_0^2-k_\\rho^2)^{1/2}\n</math>\n[Ref.2,Pg.66]. The notation used here is different form that above: <math>r</math> is now the distance from the origin and <math>\\rho</math> is the radial distance in a [[cylindrical coordinate system]] defined as <math>(\\rho,\\phi,z)</math>. The physical interpretation is that a spherical wave can be expanded into a summation of cylindrical waves in <math>\\rho</math> direction, multiplied by a two-sided [[plane wave]] in the <math>z</math> direction; see the [[Jacobi-Anger expansion]]. The summation has to be taken over all the wavenumbers <math>k_\\rho</math>.\n\nThe Sommerfeld identity is closely related to the two-dimensional [[Fourier transform]] with cylindrical symmetry, i.e., the [[Hankel transform]]. It is found by transforming the spherical wave along the in-plane coordinates (<math>x</math>,<math>y</math>, or <math>\\rho</math>, <math>\\phi</math>) but not transforming along the height coordinate <math>z</math>.\n\n== References ==\n# Sommerfeld, A.,''Partial Differential Equations in Physics'',Academic Press,New York,1964\n# Chew, W.C.,''Waves and Fields in Inhomogeneous Media'',Van Nostrand Reinhold,New York,1990\n<br>\n\n[[Category:Mathematical identities]]\n{{physics-stub}}"
    },
    {
      "title": "Sun's curious identity",
      "url": "https://en.wikipedia.org/wiki/Sun%27s_curious_identity",
      "text": "In [[combinatorics]], '''Sun's curious identity''' is the following [[Identity (mathematics)|identity]] involving [[binomial coefficient]]s, first established by [[Zhi-Wei Sun]] in 2002:\n\n:<math>\n(x+m+1)\\sum_{i=0}^m(-1)^i\\dbinom{x+y+i}{m-i}\\dbinom{y+2i}{i}\n-\\sum_{i=0}^{m}\\dbinom{x+i}{m-i}(-4)^i=(x-m)\\dbinom{x}{m}.\n</math>\n\n== Proofs ==\nAfter Sun's publication of this identity, five other proofs were obtained by various mathematicians: \n\n* Panholzer and Prodinger's proof via [[generating functions]]; \n* Merlini and Sprugnoli's proof using [[Riordan arrays]]; \n* Ekhad and Mohammed's proof by the [[WZ theory|WZ]] method; \n* Chu and Claudio's proof with the help of [[Johan Jensen (mathematician)|Jensen]]'s formula; \n* Callan's [[combinatorial proof]] involving [[dominos]] and colorings.\n\n==References==\n\n*{{citation\n | last = Callan | first = D.\n | arxiv = math.CO/0401216 \n | journal = Integers: Electronic Journal of Combinatorial Number Theory\n | page = A05\n | title = A combinatorial proof of Sun's 'curious' identity\n | url = http://www.emis.de/journals/INTEGERS/papers/e5/e5.pdf\n | volume = 4\n | year = 2004}}. \n\n*{{citation\n | last1 = Chu | first1 = W.\n | last2 = Claudio | first2 = L.V.D.\n | journal = Integers: Electronic Journal of Combinatorial Number Theory\n | page = A20\n | title = Jensen proof of a curious binomial identity\n | url = http://www.emis.de/journals/INTEGERS/papers/d20/d20.pdf\n | volume = 3\n | year = 2003}}.\n\n*{{citation\n | last1 = Ekhad | first1 = S. B. | author1-link = Doron Zeilberger\n | last2 = Mohammed | first2 = M.\n | journal = Integers: Electronic Journal of Combinatorial Number Theory\n | page = A06\n | title = A WZ proof of a 'curious' identity\n | url = http://www.emis.de/journals/INTEGERS/papers/d6/d6.pdf\n | volume = 3\n | year = 2003}}.\n\n*{{citation\n | last1 = Merlini | first1 = D.\n | last2 = Sprugnoli | first2 = R.\n | journal = Integers: Electronic Journal of Combinatorial Number Theory\n | page = A08\n | title = A Riordan array proof of a curious identity\n | url = http://www.emis.de/journals/INTEGERS/papers/c8/c8.pdf\n | volume = 2\n | year = 2002}}.\n\n*{{citation\n | last1 = Panholzer | first1 = A.\n | last2 = Prodinger | first2 = H.\n | journal = Integers: Electronic Journal of Combinatorial Number Theory\n | page = A06\n | title = A generating functions proof of a curious identity\n | url = http://www.emis.de/journals/INTEGERS/papers/c6/c6.pdf\n | volume = 2\n | year = 2002}}.\n\n*{{citation\n | last = Sun | first = Zhi-Wei\n | journal = Integers: Electronic Journal of Combinatorial Number Theory\n | page = A04\n | title = A curious identity involving binomial coefficients\n | url = http://www.emis.de/journals/INTEGERS/papers/c4/c4.pdf\n | volume = 2\n | year = 2002}}.\n\n*{{citation\n | last = Sun | first = Zhi-Wei\n | doi = 10.1016/j.disc.2007.08.046\n | arxiv = math.NT/0404385 \n | issue = 18\n | journal = [[Discrete Mathematics (journal)|Discrete Mathematics]]\n | pages = 4231–4245\n | title = On sums of binomial coefficients and their applications\n | volume = 308\n | year = 2008}}.\n\n[[Category:Factorial and binomial topics]]\n[[Category:Mathematical identities]]"
    },
    {
      "title": "Vandermonde's identity",
      "url": "https://en.wikipedia.org/wiki/Vandermonde%27s_identity",
      "text": "{{Short description|Mathematical theorem of binomial coefficients}}\n{{for|the expression for a special determinant|Vandermonde matrix}}\nIn [[combinatorics]], '''Vandermonde's identity''' (or '''Vandermonde's convolution''') is the following identity for [[binomial coefficient]]s:\n\n:<math>{m+n \\choose r}=\\sum_{k=0}^r{m \\choose k}{n \\choose r-k}</math>\n\nfor any nonnegative [[integer]]s ''r'', ''m'', ''n''.  The identity is named after [[Alexandre-Théophile Vandermonde]] (1772), although it was already known in 1303 by the [[Chinese mathematics|Chinese mathematician]] [[Zhu Shijie]] (Chu Shi-Chieh). See [[#Askey1975|Askey 1975, pp. 59&ndash;60]] for the history.\n\nThere is a [[q-analog|''q''-analog]] to this theorem called the [[q-Vandermonde identity|''q''-Vandermonde identity]].\n\nVandermonde's identity can be generalized in numerous ways, including to the identity\n\n: <math>\n{ n_1+\\dots +n_p \\choose m }= \\sum_{k_1+\\cdots +k_p = m} {n_1\\choose k_1} {n_2\\choose k_2} \\cdots {n_p\\choose k_p}. \n</math>\n\n== Proofs ==\n=== Algebraic proof ===\nIn general, the product of two [[polynomial]]s with degrees ''m'' and ''n'', respectively, is given by\n\n:<math>\\biggl(\\sum_{i=0}^m a_ix^i\\biggr) \\biggl(\\sum_{j=0}^n b_jx^j\\biggr)\n= \\sum_{r=0}^{m+n}\\biggl(\\sum_{k=0}^r a_k b_{r-k}\\biggr) x^r,</math>\n\nwhere we use the convention that ''a<sub>i</sub>''&nbsp;=&nbsp;0 for all integers ''i''&nbsp;>&nbsp;''m'' and ''b<sub>j</sub>''&nbsp;=&nbsp;0 for all integers ''j''&nbsp;>&nbsp;''n''. By the [[binomial theorem]],\n\n:<math>(1+x)^{m+n} = \\sum_{r=0}^{m+n} {m+n \\choose r}x^r. </math>\n\nUsing the binomial theorem also for the exponents ''m'' and ''n'', and then the above formula for the product of polynomials, we obtain\n\n:<math>\\begin{align}\n\\sum_{r=0}^{m+n} {m+n \\choose r}x^r\n&= (1+x)^{m+n}\\\\\n&= (1+x)^m (1+x)^n \\\\\n&= \\biggl(\\sum_{i=0}^m {m\\choose i}x^i\\biggr)\n   \\biggl(\\sum_{j=0}^n {n\\choose j}x^j\\biggr)\\\\\n&=\\sum_{r=0}^{m+n}\\biggl(\\sum_{k=0}^r {m\\choose k} {n\\choose r-k}\\biggr) x^r,\n\\end{align}\n</math>\n\nwhere the above convention for the coefficients of the polynomials agrees with the definition of the binomial coefficients, because both give zero for all ''i''&nbsp;>&nbsp;''m'' and ''j''&nbsp;>&nbsp;''n'', respectively.\n\nBy comparing coefficients of ''x<sup>r</sup>'', Vandermonde's identity follows for all integers ''r'' with 0&nbsp;≤&nbsp;''r''&nbsp;≤&nbsp;''m''&nbsp;+&nbsp;''n''. For larger integers ''r'', both sides of Vandermonde's identity are zero due to the definition of binomial coefficients.\n\n=== Combinatorial proof ===\nVandermonde's identity also admits a combinatorial [[double counting (proof technique)|double counting proof]], as follows.  Suppose a committee consists of ''m'' men and ''n'' women.  In how many ways can a subcommittee of ''r'' members be formed?  The answer is\n\n:<math>{m+n \\choose r}.</math>\n\nThe answer is also the sum over all possible values of ''k'', of the number of subcommittees consisting of ''k'' men and ''r''&nbsp;&minus;&nbsp;''k'' women:\n\n:<math>\\sum_{k=0}^r{m \\choose k}{n \\choose r-k}.</math>\n\n=== Geometrical proof ===\nTake a rectangular grid of ''r x (m+n-r)'' squares. There are\n\n: <math>\\binom{r+(m+n-r)}{r}=\\binom{m+n}{r}</math>\n\npaths that start on the bottom left vertex and, moving only upwards or rightwards, end at the top right vertex (this is because ''r'' right moves and ''m+n-r'' up moves must be made (or vice versa) in any order, and the total path length is ''m+n''). Call the bottom left vertex ''(0,0)''.\n\nThere are <math> \\binom{m}{k} </math> paths starting at ''(0,0)'' that end at ''(k,m-k)'', as ''k'' right moves and ''m-k'' upward moves must be made (and the path length is ''m''). Similarly, there are <math> \\binom{n}{r-k} </math> paths starting at ''(k,m-k)'' that end at ''(r,m+n-r)'', as a total of ''r-k'' right moves and ''(m+n-r)-(m-k)'' upward moves must be made and the path length must be ''r-k + (m+n-r)-(m-k) = n''. Thus there are\n\n: <math> \\binom{m}{k}\\binom{n}{r-k} </math>\n\npaths that start at ''(0,0)'', end at ''(r,m+n-r)'', and go through ''(k,m-k)''. This is a subset of all paths that start at ''(0,0)'' and end at ''(r,m+n-r)'', so sum from ''k=0'' to ''k=r'' (as the point ''(k,m-k)'' is confined to be within the square) to obtain the total number of paths that start at ''(0,0)'' and end at ''(r,m+n-r)''.\n\n== Generalizations ==\n\n=== Generalized Vandermonde's identity ===\n\nOne can generalize Vandermonde's identity as follows:\n\n: <math>\n\\sum_{k_1+\\cdots +k_p = m} {n_1\\choose k_1} {n_2\\choose k_2} \\cdots {n_p\\choose k_p} = { n_1+\\dots +n_p \\choose m }.\n</math>\n\nThis identity can be obtained through the algebraic derivation above when more than two polynomials are used, or through a simple [[Double counting (proof technique)|double counting]] argument.\n\nOn the one hand, one chooses <math>\\textstyle k_1</math> elements out of a first set of <math>\\textstyle n_1</math> elements; then <math>\\textstyle k_2</math> out of another set, and so on, through <math>\\textstyle p</math> such sets, until a total of <math>\\textstyle m</math> elements have been chosen from the <math>\\textstyle p</math> sets.  One therefore chooses <math>\\textstyle m</math> elements out of <math>\\textstyle n_1+\\dots +n_p</math> in the left-hand side, which is also exactly what is done in the right-hand side.\n\n===Chu&ndash;Vandermonde identity===\nThe identity generalizes to non-integer arguments.  In this case, it is known as the '''Chu&ndash;Vandermonde identity''' (see [[#Askey1975|Askey 1975, pp. 59&ndash;60]]) and takes the form\n\n:<math>{s+t \\choose n}=\\sum_{k=0}^n {s \\choose k}{t \\choose n-k}</math>\n\nfor general [[complex number|complex-valued]] ''s'' and ''t'' and any non-negative integer ''n''.  It can be proved along the lines of the algebraic proof above by [[Cauchy product|multiplying]] the [[binomial series]] for <math>(1+x)^s</math> and <math>(1+x)^t</math> and comparing terms with the binomial series for <math>(1+x)^{s+t}</math>.\n\nThis identity may be re-written in terms of the falling [[Pochhammer symbol]]s as\n\n:<math>(s+t)_n = \\sum_{k=0}^n {n \\choose k} (s)_k (t)_{n-k}</math>\n\nin which form it is clearly recognizable as an [[umbral calculus|umbral]] variant of the [[binomial theorem]] (for more on umbral variants of the binomial theorem, see [[binomial type]]).  The Chu&ndash;Vandermonde identity can also be seen to be a special case of [[Gauss's hypergeometric theorem]], which states that\n\n:<math>\\;_2F_1(a,b;c;1) = \\frac{\\Gamma(c)\\Gamma(c-a-b)}{\\Gamma(c-a)\\Gamma(c-b)}</math>\n\nwhere <math>\\;_2F_1</math> is the [[hypergeometric function]] and <math>\\Gamma(n+1)=n!</math> is the [[gamma function]]. One regains the Chu&ndash;Vandermonde identity by taking ''a''&nbsp;=&nbsp;&minus;''n'' and applying the identity\n\n:<math>{n\\choose k} = (-1)^k {k-n-1 \\choose k}</math>\n\nliberally.\n\nThe [[Rothe–Hagen identity]] is a further generalization of this identity.\n\n==The hypergeometric probability distribution==\nWhen both sides have been divided by the expression on the left, so that the sum is 1, then the terms of the sum may be interpreted as probabilities.  The resulting [[probability distribution]] is the [[hypergeometric distribution]].  That is the probability distribution of the number of red marbles in ''r'' draws ''without replacement'' from an urn containing ''n'' red and ''m'' blue marbles.\n\n==See also==\n* [[Pascal's identity]] \n* [[Hockey-stick identity]]\n* [[Rothe–Hagen identity]]\n\n==References==\n*<cite id=Askey1975>{{Citation | last=Askey | first=Richard | authorlink=Richard Askey | title = [[Orthogonal polynomials]] and [[special function]]s | location=Philadelphia, PA | series=Regional Conference Series in Applied Mathematics | volume=21 | publisher=SIAM | year=1975 | pages=viii+110}}\n\n[[Category:Factorial and binomial topics]]\n[[Category:Mathematical identities]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "Vector algebra relations",
      "url": "https://en.wikipedia.org/wiki/Vector_algebra_relations",
      "text": "{{See also|Vector calculus identities}}\nThe relations below apply to [[Euclidean vector|vectors]] in a three-dimensional [[Euclidean space]].<ref name=Albright>See, for example, {{cite book |title=Albright's chemical engineering handbook |author=Lyle Frederick Albright |url=https://books.google.com/books?id=HYB3Udjx_FYC&pg=PA68 |page=68 |isbn=0-8247-5362-3 |publisher=CRC Press |chapter=§2.5.1 Vector algebra |year=2008}}\n</ref> Some, but not all of them, extend to vectors of higher dimensions. In particular, the cross product of vectors is defined only in three dimensions (but see [[Seven-dimensional cross product]]).\n==Magnitudes==\n\nThe magnitude of a vector '''A''' is determined by its three components along three orthogonal directions using [[Pythagoras' theorem]]:\n\n:<math>\\|\\mathbf A \\|^2 = A_1^2 + A_2^2 +A_3^2 </math>\n\nThe magnitude also can be expressed using the [[dot product]]:\n\n:<math>\\|\\mathbf A \\|^2 = \\mathbf {A \\cdot A} </math>\n\n==Inequalities==\n\n:<math>\\frac{ \\mathbf{A \\cdot B}}{\\|\\mathbf A \\| \\|\\mathbf B \\|} \\le 1 </math>;  [[Cauchy–Schwarz inequality]]  in three dimensions\n:<math>\\|\\mathbf{A + B}\\| \\le \\| \\mathbf{A}\\| + \\|\\mathbf{B}\\| </math>; the [[triangle inequality]] in three dimensions\n:<math>\\|\\mathbf{A - B}\\| \\ge \\| \\mathbf{A}\\| - \\|\\mathbf{B}\\| </math>; the [[Triangle_inequality#Reverse_triangle_inequality |reverse triangle inequality]]\nHere the notation ('''A&thinsp;·&thinsp;B''') denotes the [[dot product]] of vectors '''A''' and '''B'''.\n\n==Angles==\n\nThe vector product and the scalar product of two vectors define the angle between them, say θ:<ref name=Albright/><ref name=Hildebrand>\n\n{{cite book |title=Methods of applied mathematics |author=Francis Begnaud Hildebrand |page=24 |url=https://books.google.com/books?id=17EZkWPz_eQC&pg=PA24|isbn=0-486-67002-3 |edition=Reprint of Prentice-Hall 1965 2nd|publisher=Courier Dover Publications |year=1992}}\n</ref>\n\n:<math>\\sin \\theta =\\frac{\\|\\mathbf{A \\times B}\\|}{\\|\\mathbf A \\| \\|\\mathbf B \\|} \\ \\ ( -\\pi < \\theta \\le \\pi ) </math> \nTo satisfy the [[right-hand rule]], for positive θ, vector '''B''' is counter-clockwise from '''A''', and for negative θ it is clockwise.\n:<math>\\cos \\theta = \\frac{ \\mathbf{A \\cdot B}}{\\|\\mathbf A \\| \\|\\mathbf B \\|} \\ \\ ( -\\pi < \\theta \\le \\pi )</math>\nHere the notation '''A&thinsp;×&thinsp;B''' denotes the vector [[cross product]] of vectors '''A''' and '''B'''.\nThe [[Pythagorean trigonometric identity]] then provides:\n\n:<math>  \\|\\mathbf{A \\times B}\\|^2 +(\\mathbf{A \\cdot B})^2 = \\|\\mathbf A \\|^2   \\|\\mathbf B \\|^2 </math>\n\nIf a vector '''A''' = (''A<sub>x</sub>, A<sub>y</sub>, A<sub>z</sub>'') makes angles α, β, γ with an orthogonal set of ''x-'', ''y-'' and ''z-''axes, then:\n\n:<math> \\cos \\alpha = \\frac{ A_x }{ \\sqrt {A_x^2 +A_y^2 +A_z^2} }  = \\frac {A_x} {\\| \\mathbf A \\|} \\ , </math>\nand analogously for angles β, γ. Consequently:\n:<math>\\mathbf A = \\|\\mathbf A \\|\\left( \\cos \\alpha \\  \\hat{\\mathbf  i}  +  \\cos \\beta\\  \\hat{\\mathbf  j} +  \\cos \\gamma \\ \\hat{\\mathbf  k}  \\right) \\ ,</math>\nwith <math>\\hat{\\mathbf  i}, \\ \\hat{\\mathbf  j}, \\ \\hat{\\mathbf  k}</math> unit vectors along the axis directions.\n\n==Areas and volumes==\n\nThe area Σ of a [[parallelogram]] with sides ''A'' and ''B'' containing the angle θ is:\n:<math> \\Sigma = AB \\ \\sin \\theta \\ , </math>\nwhich will be recognized as the magnitude of the vector cross product of the vectors '''A''' and '''B''' lying along the sides of the parallelogram. That is:\n:<math>\\Sigma = \\|\\mathbf { A \\times B } \\| = \\sqrt{ \\|\\mathbf A\\|^2 \\|\\mathbf B\\|^2 -(\\mathbf{A \\cdot B} )^2} \\ . </math>\n\n(If '''A''', '''B''' are two-dimensional vectors, this is equal to the determinant of the 2 &times; 2 matrix with rows '''A''', '''B'''.) The square of this expression is:<ref name=Courant>\n\n{{cite book |title=Introduction to calculus and analysis, Volume II |author=Richard Courant, Fritz John |url=https://books.google.com/books?id=ngkQxS4eicgC&pg=PA191 |pages=190–195 |chapter=Areas of parallelograms and volumes of parallelepipeds in higher dimensions  |isbn=3-540-66569-2 |year=2000 |publisher=Springer |edition=Reprint of original 1974 Interscience}}\n\n</ref>\n:<math>\\Sigma^2 = (\\mathbf{A \\cdot A })(\\mathbf{B \\cdot B })-(\\mathbf{A \\cdot B })(\\mathbf{B \\cdot A })=\\Gamma(\\mathbf A,\\ \\mathbf B ) \\ , </math>\nwhere Γ('''A''', '''B''') is the [[Gram determinant]] of '''A''' and '''B''' defined by:\n\n:<math>\\Gamma(\\mathbf A,\\ \\mathbf B )=\\begin{vmatrix} \\mathbf{A\\cdot A} & \\mathbf{A\\cdot B} \\\\\n \\mathbf{B\\cdot A} & \\mathbf{B\\cdot B}  \\end{vmatrix} \\ . </math>\n\nIn a similar fashion, the squared volume ''V'' of a [[parallelepiped]] spanned by the three vectors '''A''', '''B''', '''C''' is given by the Gram determinant of the three vectors:<ref name=Courant/>\n:<math>V^2 =\\Gamma ( \\mathbf A ,\\ \\mathbf B ,\\  \\mathbf C ) = \\begin{vmatrix} \\mathbf{A\\cdot A} & \\mathbf{A\\cdot B} & \\mathbf{A\\cdot C} \\\\\\mathbf{B\\cdot A} & \\mathbf{B\\cdot B} & \\mathbf{B\\cdot C}\\\\\n \\mathbf{C\\cdot A} & \\mathbf{C\\cdot B} & \\mathbf{C\\cdot C}  \\end{vmatrix} \n\\ , </math>\n\nSince '''A''', '''B, C''' are three-dimensional vectors, this is equal to the square of the [[scalar triple product]] <math>\\mathrm{det}[\\mathbf{A},\\mathbf{B},\\mathbf{C}]=|\\mathbf{A},\\mathbf{B},\\mathbf{C}|</math> below.\n\nThis process can be extended to ''n''-dimensions.\n\n==Addition and multiplication of vectors==\n\nSome of the following algebraic relations refer to the [[dot product]] and the [[cross product]] of vectors. These relations can be found in a variety of sources, for example, see Albright.<ref name=Albright/>\n*<math> c (\\mathbf{A}+\\mathbf{B})=c\\mathbf{A}+c\\mathbf{B} </math>; distributivity of multiplication by a scalar and addition\n*<math> \\mathbf{A}+\\mathbf{B}=\\mathbf{B}+\\mathbf{A} </math>; commutativity of addition\n*<math> \\mathbf{A}+(\\mathbf{B}+\\mathbf{C})=(\\mathbf{A}+\\mathbf{B})+\\mathbf{C} </math>; associativity of addition\n*<math> \\mathbf{A}\\cdot\\mathbf{B}=\\mathbf{B}\\cdot\\mathbf{A} </math>; commutativity of scalar (dot) product\n*<math> \\mathbf{A}\\times\\mathbf{B}=\\mathbf{-B}\\times\\mathbf{A} </math>; anticommutativity of vector cross product \n*<math> \\left(\\mathbf{A}+\\mathbf{B}\\right)\\cdot\\mathbf{C}=\\mathbf{A}\\cdot\\mathbf{C}+\\mathbf{B}\\cdot\\mathbf{C} </math>; distributivity of addition wrt scalar product\n*<math> \\left(\\mathbf{A}+\\mathbf{B}\\right)\\times\\mathbf{C}=\\mathbf{A}\\times\\mathbf{C}+\\mathbf{B}\\times\\mathbf{C} </math>; distributivity of addition wrt vector cross product\n*<math> \\mathbf{A}\\cdot\\left(\\mathbf{B}\\times\\mathbf{C}\\right)\\ =\\ \\mathbf{B}\\cdot\\left(\\mathbf{C}\\times\\mathbf{A}\\right)\\ =\\ \\mathbf{C}\\cdot\\left(\\mathbf{A}\\times\\mathbf{B}\\right)</math><math>\\ =\\ \\left|\\begin{array}{ccc}\nA_{x} & B_{x} & C_{x}\\\\\nA_{y} & B_{y} & C_{y}\\\\\nA_{z} & B_{z} & C_{z}\\end{array}\\right| \\ =\\  |\\mathbf{A}, \\mathbf{B},\\mathbf{C}| </math> ; [[scalar triple product]]\n\n*<math> \\mathbf{A\\times}\\left(\\mathbf{B}\\times\\mathbf{C}\\right)=\\left(\\mathbf{A}\\cdot\\mathbf{C}\\right)\\mathbf{B}-\\left(\\mathbf{A}\\cdot\\mathbf{B}\\right)\\mathbf{C} </math>; [[vector triple product]]\n*<math> \\mathbf{\\left(A\\times B\\right)\\cdot}\\left(\\mathbf{C}\\times\\mathbf{D}\\right)=\\left(\\mathbf{A}\\cdot\\mathbf{C}\\right)\\left(\\mathbf{B}\\cdot\\mathbf{D}\\right)-\\left(\\mathbf{B}\\cdot\\mathbf{C}\\right)\\left(\\mathbf{A}\\cdot\\mathbf{D}\\right) </math>; [[Binet–Cauchy identity]] in three dimensions\n:In particular, when '''A''' = '''C'''  and '''B''' = '''D''', the above reduces to:\n::<math>(\\mathbf{A} \\times \\mathbf{B}) \\cdot (\\mathbf{A} \\times \\mathbf{B}) = |\\mathbf{A} \\times \\mathbf{B}|^2  =   (\\mathbf{A} \\cdot \\mathbf{A}) (\\mathbf{B} \\cdot \\mathbf{B})-(\\mathbf{A} \\cdot \\mathbf{B})^2</math>; [[Lagrange's identity]] in three dimensions  \n*<math>|\\mathbf{A},\\mathbf{B},\\mathbf{C}|\\,\\mathbf{D}\\ =\\ \\left(\\mathbf{A}\\cdot\\mathbf{D}\\right)\\left(\\mathbf{B}\\times\\mathbf{C}\\right)\n\\,+\\,\\left(\\mathbf{B}\\cdot\\mathbf{D}\\right)\\left(\\mathbf{C}\\times\\mathbf{A}\\right)\\,+\\,\\left(\\mathbf{C}\\cdot\\mathbf{D}\\right)\\left(\\mathbf{A}\\times\\mathbf{B}\\right)</math>\n*A vector quadruple product, which is also a vector, can be defined, which satisfies the following identities:<ref name=Soni>\n\n{{cite book |title=Mechanics and relativity |author=Vidwan Singh Soni |url=https://books.google.com/books?id=-3H5V0LGBOgC&pg=PA11 |pages=11–12 |chapter=§1.10.2 Vector quadruple product |publisher=PHI Learning Pvt. Ltd. |isbn=81-203-3713-1 |year=2009}}\n\n</ref><ref name=Gibbs>This formula is applied to spherical trigonometry by \n\n{{cite book |title=Vector analysis: a text-book for the use of students of mathematics |author=Edwin Bidwell Wilson, Josiah Willard Gibbs |url=https://books.google.com/books?id=RC8PAAAAIAAJ&pg=PA77 |chapter=§42 in ''Direct and skew products of vectors'' |publisher=Scribner |year=1901 |pages=77 ''ff''}}\n\n</ref>\n:<math>(\\mathbf{A} \\times \\mathbf{B}) \\times (\\mathbf{C} \\times \\mathbf{D}) \\ =\\  |\\mathbf{A},\\mathbf{B}, \\mathbf{D}|\\,\\mathbf{C}\\,-\\,|\\mathbf{A},\\mathbf{B}, \\mathbf{C}|\\,\\mathbf{D}\\ =\\ \n|\\mathbf{A},\\mathbf{C}, \\mathbf{D}|\\,\\mathbf{B}\\,-\\,|\\mathbf{B}, \\mathbf{C},\\mathbf{D}|\\,\\mathbf{A}</math>\n:where |'''A''', '''B''', '''C|''' is the scalar [[triple product]] '''A''' · ('''B''' × '''C''') , the [[determinant]] of the [[matrix (mathematics)|matrix]] ['''A''', '''B''', '''C'''] with these vectors as rows .\n*In 3 dimensions, given any three non-coplanar vectors '''A''', '''B''', '''C''', any other vector '''D''' can be expressed in terms of these as:<ref name=Coffin>\n\n{{cite book |title=Vector analysis: an introduction to vector-methods and their various applications to physics and mathematics |author=Joseph George Coffin |url=https://books.google.com/books?id=9mgGAQAAIAAJ&pg=PA56 |page=56 |year=1911 |publisher=Wiley |edition=2nd}}\n\n</ref>\n:<math>\\mathbf D = \\frac{\\mathbf{D} \\cdot (\\mathbf{B} \\times \\mathbf{C})}{[\\mathbf{A},\\ \\mathbf{B}, \\ \\mathbf{C}]}\\ \\mathbf A +\\frac{\\mathbf{D} \\cdot (\\mathbf{C} \\times \\mathbf{A})}{[\\mathbf{A},\\ \\mathbf{B}, \\ \\mathbf{C}]}\\ \\mathbf B + \\frac{\\mathbf{D} \\cdot (\\mathbf{A} \\times \\mathbf{B})}{[\\mathbf{A},\\ \\mathbf{B}, \\ \\mathbf{C}]}\\ \\mathbf C \\ .</math>\n\n==See also==\n*[[Vector space]]\n*[[Geometric algebra]]\n\n==References==\n{{Reflist}}\n\n[[Category:Vectors (mathematics and physics)]]\n[[Category:Mathematical identities]]\n[[Category:Mathematics-related lists]]"
    },
    {
      "title": "Vector calculus identities",
      "url": "https://en.wikipedia.org/wiki/Vector_calculus_identities",
      "text": "{{See also|Vector algebra relations}}\n{{More footnotes|date=August 2017}}\n{{Calculus |Vector}}\nThe following [[Identity (mathematics)|identities]] are important in [[vector calculus]]:\n\n==Operator notations==\n\n===Gradient===\n{{main|Gradient}}\n\nFor a function <math>f(x,y,z)</math>in three-dimensional [[Cartesian coordinate system|Cartesian coordinate]] variables, the gradient is the vector field:\n\n:<math>\\operatorname{grad}(f) \\ =\\ \\nabla f \\ =\\  \\tfrac{\\partial f}{\\partial x} \\mathbf{i} + \\tfrac{\\partial f}{\\partial y}  \\mathbf{j} + \\tfrac{\\partial f}{\\partial z} \\mathbf{k}\n\\ =\\ \n(\\tfrac{\\partial f}{\\partial x}, \\tfrac{\\partial f}{\\partial y}, \\tfrac{\\partial f}{\\partial z})\n</math>\n\nwhere '''i''', '''j''', '''k''' are the [[standard basis|standard]] [[unit vector]]s for the ''x'', ''y'', ''z''-axes. More generally, for a function of ''n'' variables <math>\\psi(x_1,\\ldots,x_n)</math>, also called a [[scalar (mathematics)|scalar]] field, the gradient is the [[vector field]]: <blockquote><math>\\nabla\\psi = (\\tfrac{\\partial \\psi}{\\partial x_1},\\ldots,\\tfrac{\\partial \\psi}{\\partial x_n}).</math></blockquote>For a vector field <math>\\mathbf{A}= (A_1,\\ldots,A_n)</math> written as a 1 × ''n'' row vector, also called a tensor field of order 1, the gradient or [[covariant derivative]] is the ''n × n'' [[Jacobian matrix and determinant|Jacobian matrix]]:<blockquote><math>\\nabla \\!\\mathbf{A}=\\mathbf{J}_{\\mathbf{A}} = \\left(\\frac{\\partial A_i}{\\partial x_j}\\right)_{\\!ij}.</math></blockquote>For a [[tensor field]] <math>\\mathbf{A}</math> of any order ''k,'' the gradient <math>\\operatorname{grad}(\\mathbf{A}) = \\nabla\\!\\mathbf{A} </math>is a tensor field of order ''k +'' 1. \n\n===Divergence===\n{{main|Divergence}}\n\nIn Cartesian coordinates, the divergence of a [[continuously differentiable]] [[vector field]] <math>\\mathbf{F} = F_x\\mathbf{i} + F_y\\mathbf{j} + F_z\\mathbf{k}</math> is the scalar-valued function:\n\n:<math>\\operatorname{div} \\mathbf{F} \\ =\\  \\nabla\\cdot\\mathbf{F} \\ =\\ ( \\tfrac{\\partial}{\\partial x}, \\tfrac{\\partial}{\\partial y}, \\tfrac{\\partial}{\\partial z} ) \\cdot (F_x,F_y,F_z) \\ =\\  \\tfrac{\\partial F_x}{\\partial x}+\\tfrac{\\partial F_y}{\\partial y}+\\tfrac{\\partial F_z}{\\partial z}.</math>\n\nThe divergence of a [[tensor field]] <math>\\mathbf{A}</math>of non-zero order ''k'', is written as <math>\\operatorname{div}(\\mathbf{A}) = \\nabla \\cdot \\mathbf{A}</math>, a [[Tensor contraction|contraction]] to a tensor field of order ''k'' - 1. Specifically, the divergence of a vector is a scalar. The divergence of a higher order tensor field may be found by decomposing the tensor field into a sum of outer products and using the identity,\n\n:<math>\\nabla \\cdot (\\mathbf{B} \\otimes \\hat{\\mathbf{A}}) = \\hat{\\mathbf{A}}(\\nabla \\cdot \\mathbf{B})+(\\mathbf{B}\\cdot \\nabla) \\hat{\\mathbf{A}}</math>\n\nwhere <math> \\mathbf{B}\\cdot\\nabla </math> is the [[directional derivative]] in the direction of <math> \\mathbf{B} </math> multiplied by its magnitude. Specifically, for the outer product of two vectors,\n\n:<math>\\nabla \\cdot \\left(\\mathbf{b} \\mathbf{a}^\\mathrm{T}\\right) = \\mathbf{a}(\\nabla \\cdot \\mathbf{b})+(\\mathbf{b}\\cdot \\nabla) \\mathbf{a}.</math>\n\n===Curl===\n{{main|Curl (mathematics)}}\n\nIn Cartesian coordinates, for <math>\\mathbf{F} = F_x\\mathbf{i} + F_y\\mathbf{j} + F_z\\mathbf{k}</math> the curl is the vector field:\n\n:<math>\\operatorname{curl}\\mathbf F\\ =\\ \\nabla \\times \\mathbf{F} \\ =\\  \\left|\\begin{matrix} \\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\ \\frac{\\partial}{\\partial x} & \\frac{\\partial}{\\partial y} & \\frac{\\partial}{\\partial z} \\\\ F_x & F_y & F_z \\end{matrix}\\right|\n\\ =\\ \n\\left(\\tfrac{\\partial F_z}{\\partial y}{-} \\tfrac{\\partial F_y}{\\partial z}\\right)\\! \\mathbf{i} + \\left(\\tfrac{\\partial F_x}{\\partial z} {-} \\tfrac{\\partial F_z}{\\partial x}\\right)\\! \\mathbf{j} + \\left(\\tfrac{\\partial F_y}{\\partial x}{-}\\tfrac{\\partial F_x}{\\partial y}\\right)\\!\\mathbf{k}</math>\n\nwhere '''i''', '''j''', and '''k''' are the [[unit vector]]s for the ''x''-, ''y''-, and ''z''-axes, respectively. In [[Einstein notation]], the vector field <math>\\mathbf{F}=(F_1,F_2,F_3)</math>has curl given by:\n\n:<math> \\nabla\\times\\mathbf{F} \\ =\\ \\varepsilon^{ijk} \\frac {\\partial F_k}{\\partial x^j}</math>\n\nwhere <math>\\varepsilon=\\plusmn 1</math> or '''0''' is the [[Levi-Civita symbol|Levi-Civita parity symbol]].\n\n===Laplacian===\n{{main|Laplace operator}}\n\nIn '''[[Cartesian coordinates]]''', the Laplacian of a function <math>f(x,y,z)</math> is\n\n:<math>\\Delta f = \\nabla^2 f = (\\nabla \\cdot \\nabla) f = \\frac{\\partial^2 f}{\\partial x^2} + \\frac{\\partial^2 f}{\\partial y^2} + \\frac{\\partial^2 f}{\\partial z^2}.</math>\n\nFor a [[tensor field]], <math> \\mathbf{A} </math>, the laplacian is generally written as:\n\n:<math>\\Delta\\mathbf{A} = \\nabla^2 \\mathbf{A} = (\\nabla \\cdot \\nabla) \\mathbf{A}</math>\n\nand is a tensor field of the same order.\n\n===Special notations===\nIn ''Feynman subscript notation'',\n\n:<math> \\nabla_\\mathbf{B}\\! \\left( \\mathbf{A {\\cdot} B} \\right)\\ =\\ \\mathbf{A} {\\times}\\! \\left( \\nabla {\\times} \\mathbf{B} \\right) \\,+\\, \\left( \\mathbf{A} {\\cdot} \\nabla \\right) \\mathbf{B} </math>\n\nwhere the notation ∇<sub>'''B'''</sub>  means the subscripted gradient operates on only the factor '''B'''.<ref name=Feynman>{{cite book |first1=R. P. |last1=Feynman |first2=R. B. |last2=Leighton |first3=M. |last3=Sands |title=The Feynman Lectures on Physics |publisher = Addison-Wesley |year=1964 |isbn=0-8053-9049-9 |nopp= true |pages= Vol II, p. 27–4}}</ref><ref name=Missevitch>{{cite arXiv |eprint=physics/0504223 |first1=A. L. |last1=Kholmetskii |first2=O. V. |last2=Missevitch |title=The Faraday induction law in relativity theory |year=2005 |url=https://arxiv.org/ftp/physics/papers/0504/0504223.pdf |page=4 }}</ref>\n\nLess general but similar is the ''Hestenes'' ''overdot notation'' in [[geometric algebra]].<ref name=Doran>{{cite book |first1=C. |last1=Doran |authorlink1=Chris J. L. Doran |first2=A. |last2=Lasenby |title=Geometric algebra for physicists |year=2003 |publisher=Cambridge University Press |page=169 |isbn=978-0-521-71595-9}}</ref> The above identity is then expressed as:\n\n:<math> \\dot{\\nabla} \\left( \\mathbf{A} {\\cdot} \\dot{\\mathbf{B}} \\right)\\ =\\ \\mathbf{A} {\\times}\\! \\left( \\nabla {\\times} \\mathbf{B} \\right)\\,+\\, \\left( \\mathbf{A} {\\cdot} \\nabla \\right) \\mathbf{B} </math>\n\nwhere overdots define the scope of the vector derivative. The dotted vector, in this case '''B''', is differentiated, while the (undotted) '''A''' is held constant.\n\nFor the remainder of this article, Feynman subscript notation will be used where appropriate.\n\n==Properties==\nFor scalar fields <math>\\psi</math>, <math>\\phi</math> and vector fields <math>\\mathbf{A}</math>, <math>\\mathbf{B}</math>, we have the following derivative identities.\n\n===Distributive properties===\n\n:<math> \\nabla ( \\psi + \\phi ) \\ =\\ \\nabla \\psi + \\nabla \\phi </math>\n:<math> \\nabla( \\mathbf{A} + \\mathbf{B} ) \\ =\\ \\nabla \\!\\mathbf{A} + \\nabla \\mathbf{B} </math>\n:<math> \\nabla \\,{\\cdot}\\, ( \\mathbf{A} + \\mathbf{B} ) \\ =\\ \\nabla {\\cdot} \\mathbf{A} + \\nabla {\\cdot} \\mathbf{B} </math>\n:<math> \\nabla {\\times} ( \\mathbf{A} + \\mathbf{B} )\\ =\\ \\nabla {\\times} \\mathbf{A} + \\nabla {\\times} \\mathbf{B} </math>\n\n===Product rule ===\nWe have the following generalizations of the [[product rule]] in single variable [[calculus]].\n\n:<math> \\nabla (\\psi \\phi)\\ =\\ \\phi\\, \\nabla \\psi  \\,+\\, \\psi\\, \\nabla \\phi </math>\n:<math> \\nabla ( \\psi \\mathbf{A} )\n\\ =\\ (\\nabla \\psi)^{\\mathbf{T}} \\mathbf{A} \\,+\\, \\psi\\, \\nabla\\! \\mathbf{A}\n\\ =\\ \\nabla \\psi \\otimes \\mathbf{A} \\,+\\, \\psi\\, \\nabla\\!\\mathbf{A}\n </math>   \n:<math> \\nabla \\,{\\cdot}\\, ( \\psi \\mathbf{A} ) \\ =\\ \\psi\\, \\nabla {\\cdot} \\mathbf{A} \\,+\\,  (\\nabla \\psi){\\cdot}\\mathbf{A} </math>\n:<math> \\nabla {\\times} ( \\psi \\mathbf{A} ) \\ =\\ \\psi\\,( \\nabla {\\times} \\mathbf{A}) \\,+\\, ( \\nabla \\psi ) {\\times} \\mathbf{A} </math>\n\nIn the second formula, the transposed gradient <math>(\\nabla \\psi)^{\\mathbf{T}}</math> is an ''n'' × 1 column vector, <math>\\mathbf{A}</math> is a 1 × ''n'' row vector, and their product is an  ''n × n''  matrix: this may also be considered as the [[tensor product]] of two vectors, or of a covector and a vector''.'' \n\n===Quotient rule===\n\n:<math> \\nabla\\!\\left(\\frac{\\psi}{\\phi}\\right)\\ =\\  \\frac{\\phi\\,\\nabla \\psi - (\\nabla\\!\\phi)\\psi}{\\phi^2}</math>\n:<math> \\nabla \\,{\\cdot}\\! \\left(\\frac{\\mathbf{A}}{\\phi}\\right) \\ =\\ \\frac{\\phi\\, \\nabla{\\cdot}\\mathbf{A} - (\\nabla\\! \\phi){\\cdot}\\mathbf{A}}{\\phi^2}</math>\n:<math> \\nabla \\,{\\times}\\! \\left(\\frac{\\mathbf{A}}{\\phi}\\right) \\ =\\ \\frac{\\phi\\, \\nabla{\\times}\\mathbf{A} - (\\nabla\\! \\phi){\\times} \\mathbf{A}}{\\phi^2}</math>\n\n===Chain rule===\nLet <math>f(x)\n</math> be a one-variable function from scalars to scalars,  <math>\\mathbf{r}(t) = (r_1(t),\\ldots,r_n(t))\n</math> a [[Parametrization (geometry)|parametrized]] curve, and <math>F:\\mathbb{R}^n\\to\\mathbb{R}\n</math> a function from vectors to scalars. We have the following special cases of the multi-variable [[chain rule]].\n\n:<math>\\nabla(f \\circ F) \\,=\\, (f'\\! \\circ F)\\, \\nabla\\! F</math>\n:<math>(F\\circ \\mathbf{r})'  = (\\nabla\\!F\\circ \\mathbf{r})\\cdot \\mathbf{r}'\n</math>\n:<math>\\nabla(F \\circ \\mathbf{A}) \\ =\\ (\\nabla\\! F \\circ \\mathbf{A})\\, \\nabla\\! \\mathbf{A}</math>\n\nFor a [[Coordinate system|coordinate parametrization]] <math>\\Phi:\\mathbb{R}^n \\to \\mathbb{R}^n\n</math> we have:\n\n:<math>\\nabla \\cdot (\\mathbf{A} \\circ \\Phi) = \\mathrm{tr}\\!\\left(\\,(\\nabla\\!\\mathbf{A} \\circ \\Phi) \\, \\mathbf{J}_\\Phi\\,\\right)</math>\n\nHere we take the [[Trace (linear algebra)|trace]] of the product of two ''n × n'' matrices: the gradient of '''A''' and the Jacobian of Φ.\n\n===Dot product rule===\n\n:<math>\\begin{align}\n\\nabla(\\mathbf{A} \\cdot \\mathbf{B}) & \\ =\\ (\\mathbf{A} {\\cdot} \\nabla)\\mathbf{B} \\,+\\, (\\mathbf{B}\\, {\\cdot} \\nabla)\\mathbf{A} \\,+\\, \\mathbf{A} {\\times} (\\nabla {\\times} \\mathbf{B}) \\,+\\, \\mathbf{B} {\\times} (\\nabla {\\times} \\mathbf{A}) \\\\\n&\\ =\\   \\mathbf{A}\\,\\mathbf{J}_\\mathbf{B} + \\mathbf{B}\\,\\mathbf{J}_\\mathbf{A}   \n\\ =\\ \\mathbf{A}\\,\\nabla\\mathbf{B}  + \\mathbf{B}\\,\\nabla\\!\\mathbf{A}\n\\end{align}</math>\n\nwhere <math>\\mathbf{J}_{\\mathbf{A}} = \\nabla \\!\\mathbf{A}=(\\partial A_i/\\partial x_j)_{ij}</math> denotes the [[Jacobian matrix and determinant|Jacobian matrix]] of the vector field <math>\\mathbf{A} = (A_1,\\ldots,A_n)</math>.\n\nAlternatively, using Feynman subscript notation,\n\n:<math> \\nabla(\\mathbf{A} \\cdot \\mathbf{B}) = \\nabla_\\mathbf{A}(\\mathbf{A}  \\cdot \\mathbf{B}) +  \\nabla_\\mathbf{B} (\\mathbf{A} \\cdot \\mathbf{B}) \\ . </math>\n\nSee these notes.<ref>{{cite book |last1=Kelly |first1=P. |year=2013 |title=Mechanics Lecture Notes Part III: Foundations of Continuum Mechanics |url=http://homepages.engineering.auckland.ac.nz/~pkel015/SolidMechanicsBooks/Part_III/ |publisher=University of Auckland |chapter=Chapter 1.14 Tensor Calculus 1: Tensor Fields |chapter-url=http://homepages.engineering.auckland.ac.nz/~pkel015/SolidMechanicsBooks/Part_III/Chapter_1_Vectors_Tensors/Vectors_Tensors_14_Tensor_Calculus.pdf |accessdate=7 December 2017}}</ref> \n\nAs a special case, when {{math|'''A''' {{=}} '''B'''}},\n\n:<math>\\tfrac{1}{2} \\nabla \\left( \\mathbf{A}\\cdot\\mathbf{A} \\right)\n\\ =\\ (\\mathbf{A} \\cdot \\nabla) \\mathbf{A} + \\mathbf{A} \\times (\\nabla \\times \\mathbf{A}) \n\\ =\\  \\mathbf A \\,\\mathbf{J}_\\mathbf{A}\n\\ =\\ \\mathbf{A} \\,\\nabla\\! \\mathbf{A}  . </math>\n\nThe generalization of the dot product formula to Riemannian manifolds is a defining property of a [[Riemannian connection]], which differentiates a vector field to give a vector-valued [[Differential form|1-form]].\n===Cross product rule===\n\n:<math> \\nabla \\cdot (\\mathbf{A} \\times \\mathbf{B}) = (\\nabla \\times \\mathbf{A}) \\cdot \\mathbf{B} - \\mathbf{A} \\cdot (\\nabla \\times \\mathbf{B}) </math>\n\n:<math> \\begin{align}\n\\nabla \\times (\\mathbf{A} \\times \\mathbf{B}) &= \\mathbf{A}(\\nabla \\cdot \\mathbf{B}) - \\mathbf{B}(\\nabla \\cdot \\mathbf{A}) + (\\mathbf{B} \\cdot \\nabla) \\mathbf{A} - (\\mathbf{A} \\cdot \\nabla) \\mathbf{B} \\\\\n&= (\\nabla \\cdot \\mathbf{B}  + \\mathbf{B} \\cdot \\nabla)\\mathbf{A} -(\\nabla \\cdot \\mathbf{A} + \\mathbf{A} \\cdot \\nabla )\\mathbf{B} \\\\\n&= \\nabla \\cdot (\\mathbf{B} \\mathbf{A}^\\mathrm{T}) - \\nabla \\cdot (\\mathbf{A} \\mathbf{B}^\\mathrm{T}) \\\\\n&= \\nabla \\cdot (\\mathbf{B} \\mathbf{A}^\\mathrm{T} - \\mathbf{A} \\mathbf{B}^\\mathrm{T}) \n\\end{align} </math>\n\n==Second derivatives==\n\n===Curl of gradient is zero===\n\nThe [[Curl (mathematics)|curl]] of the [[gradient]] of ''any'' continuously twice-differentiable [[scalar field]] <math>\\ \\phi </math> is always the [[zero vector]]:\n\n:<math>\\nabla \\times ( \\nabla \\phi )  = \\mathbf{0}</math>\n\n===Divergence of curl is zero===\nThe [[divergence]] of the curl of ''any'' [[vector field]] '''A''' is always zero:\n:<math>\\nabla \\cdot ( \\nabla \\times \\mathbf{A} ) = 0 </math>\n\nThe above two vanishing properties are a special case of the vanishing of the square of the [[exterior derivative]] in the [[De Rham cohomology|De Rham]] [[chain complex]].\n\n===Divergence of gradient===\nThe [[Laplacian]] of a scalar field is the divergence of its gradient:\n:<math> \\nabla^2 \\psi = \\nabla \\cdot (\\nabla \\psi) </math>\nThe result is a scalar quantity.\n\n===Curl of curl===\n:<math> \\nabla \\times \\left( \\nabla \\times \\mathbf{A} \\right) = \\nabla(\\nabla \\cdot \\mathbf{A}) - \\nabla^{2}\\mathbf{A}</math>\n\nHere ∇<sup>2</sup> is the [[vector Laplacian]] operating on the vector field '''A'''.\n\n==Summary of important identities==\n\n===Addition and multiplication===\n\n*<math>\\mathbf{A}+\\mathbf{B}=\\mathbf{B}+\\mathbf{A} </math>\n*<math>\\mathbf{A}\\cdot\\mathbf{B}=\\mathbf{B}\\cdot\\mathbf{A} </math>\n*<math>\\mathbf{A}\\times\\mathbf{B}=\\mathbf{-B}\\times\\mathbf{A} </math>\n*<math>(\\mathbf{A}+\\mathbf{B})\\cdot\\mathbf{C}=\\mathbf{A}\\cdot\\mathbf{C}+\\mathbf{B}\\cdot\\mathbf{C} </math>\n*<math>(\\mathbf{A}+\\mathbf{B})\\times\\mathbf{C}=\\mathbf{A}\\times\\mathbf{C}+\\mathbf{B}\\times\\mathbf{C} </math>\n*<math>\\mathbf{A}\\cdot (\\mathbf{B}\\times\\mathbf{C})=\\mathbf{B}\\cdot (\\mathbf{C}\\times\\mathbf{A})=\\mathbf{C}\\cdot (\\mathbf{A}\\times\\mathbf{B})</math> ([[scalar triple product]])\n*<math>\\mathbf{A}\\times (\\mathbf{B}\\times\\mathbf{C})= (\\mathbf{A}\\cdot\\mathbf{C} )\\mathbf{B}- (\\mathbf{A}\\cdot\\mathbf{B})\\mathbf{C} </math> ([[vector triple product]])\n*<math>(\\mathbf{A}\\times\\mathbf{B})\\times\\mathbf{C}= (\\mathbf{A}\\cdot\\mathbf{C} )\\mathbf{B}- (\\mathbf{B}\\cdot\\mathbf{C})\\mathbf{A} </math> ([[vector triple product]])\n*<math>\\mathbf{A}\\times (\\mathbf{B}\\times\\mathbf{C}) = (\\mathbf{A}\\times\\mathbf{B} )\\times\\mathbf{C} + \\mathbf{B}\\times (\\mathbf{A}\\times\\mathbf{C} ) </math> ([[Jacobi identity]])\n*<math>\\mathbf{A}\\times (\\mathbf{B}\\times\\mathbf{C} )+\\mathbf{C}\\times (\\mathbf{A}\\times\\mathbf{B} )+ \\mathbf{B}\\times (\\mathbf{C}\\times\\mathbf{A} )= 0 </math>  ([[Jacobi identity]])\n*<math> (\\mathbf{A}\\times\\mathbf{B})\\cdot (\\mathbf{C}\\times\\mathbf{D} )= \\left(\\mathbf{A}\\cdot\\mathbf{C}\\right)\\left(\\mathbf{B}\\cdot\\mathbf{D}\\right)-\\left(\\mathbf{B}\\cdot\\mathbf{C}\\right)\\left(\\mathbf{A}\\cdot\\mathbf{D}\\right) </math>\t \n*<math>\n\\!(\\mathbf{A}\\cdot(\\mathbf{B}\\times\\mathbf{C}))\\,\\mathbf{D}= (\\mathbf{A}\\cdot\\mathbf{D} )\\left(\\mathbf{B}\\times\\mathbf{C}\\right)+\\left(\\mathbf{B}\\cdot\\mathbf{D}\\right)\\left(\\mathbf{C}\\times\\mathbf{A}\\right)+\\left(\\mathbf{C}\\cdot\\mathbf{D}\\right)\\left(\\mathbf{A}\\times\\mathbf{B}\\right) </math>\n*<math>\\!(\\mathbf{A}\\times\\mathbf{B} )\\times (\\mathbf{C}\\times\\mathbf{D} )=\\left (\\mathbf{A}\\cdot (\\mathbf{B}\\times\\mathbf{D})\\right)\\mathbf{C}-\\left(\\mathbf{A}\\cdot\\left(\\mathbf{B}\\times\\mathbf{C}\\right)\\right)\\mathbf{D}</math>\n\n===Differentiation===\n\n====Gradient====\n\n*<math>\\nabla(\\psi+\\phi)=\\nabla\\psi+\\nabla\\phi </math>\n*<math>\\nabla(\\psi \\phi) = \\phi\\nabla \\psi  + \\psi \\nabla \\phi </math>\n*<math>\\nabla(\\psi \\mathbf{A} ) = \\nabla \\psi \\otimes \\mathbf{A} + \\psi \\nabla \\mathbf{A}</math>\n*<math>\\nabla(\\mathbf{A}\\cdot\\mathbf{B})= (\\mathbf{A}\\cdot\\nabla)\\mathbf{B} + (\\mathbf{B}\\cdot\\nabla)\\mathbf{A}+ \\mathbf{A} \\times (\\nabla \\times \\mathbf{B} )+ \\mathbf{B}\\times\\left(\\nabla\\times\\mathbf{A}\\right)</math>\n\n====Divergence====\n\n*<math> \\nabla\\cdot(\\mathbf{A}+\\mathbf{B})= \\nabla\\cdot\\mathbf{A}+\\nabla\\cdot\\mathbf{B} </math>\n*<math> \\nabla\\cdot\\left(\\psi\\mathbf{A}\\right)= \\psi\\nabla\\cdot\\mathbf{A}+\\mathbf{A}\\cdot\\nabla \\psi</math>\n*<math> \\nabla\\cdot\\left(\\mathbf{A}\\times\\mathbf{B}\\right)= (\\nabla\\times\\mathbf{A})\\cdot \\mathbf{B}-(\\nabla\\times\\mathbf{B})\\cdot \\mathbf{A}</math>\n\n====Curl====\n\n*<math>\\nabla\\times(\\mathbf{A}+\\mathbf{B})=\\nabla\\times\\mathbf{A}+\\nabla\\times\\mathbf{B} </math>\n*<math>\\nabla\\times\\left(\\psi\\mathbf{A}\\right)=\\psi\\,(\\nabla\\times\\mathbf{A})+\\nabla\\psi\\times\\mathbf{A}</math>\n*<math>\\nabla\\times\\left(\\psi\\nabla\\phi\\right)= \\nabla \\psi \\times \\nabla \\phi</math>\n*<math>\\nabla\\times\\left(\\mathbf{A}\\times\\mathbf{B}\\right)= \\mathbf{A}\\left(\\nabla\\cdot\\mathbf{B}\\right)-\\mathbf{B} \\left( \\nabla\\cdot\\mathbf{A}\\right)+\\left(\\mathbf{B}\\cdot\\nabla\\right)\\mathbf{A}- \\left(\\mathbf{A}\\cdot\\nabla\\right)\\mathbf{B} </math>\n\n====Second derivatives====\n[[File:DCG chart.svg|right|thumb|300px|DCG chart:\n\nA simple chart depicting all rules pertaining to second derivatives.\nD, C, G, L and CC stand for divergence, curl, gradient, Laplacian and curl of curl, respectively.\n\nArrows indicate existence of second derivatives. Blue circle in the middle represents curl of curl, whereas the other two red circles(dashed) mean that DD and GG do not exist.\n]]\n\n*<math>\\nabla\\cdot(\\nabla\\times\\mathbf{A})=0 </math>\n*<math>\\nabla\\times(\\nabla\\psi)= \\mathbf{0} </math>\n*<math>\\nabla\\cdot(\\nabla\\psi)=\\nabla^{2}\\psi </math>        ([[Laplace operator|scalar Laplacian]])\n*<math>\\nabla\\left(\\nabla\\cdot\\mathbf{A}\\right)-\\nabla\\times\\left(\\nabla\\times\\mathbf{A}\\right)=\\nabla^{2}\\mathbf{A}</math>   ([[vector Laplacian]])\n*<math>\\nabla\\cdot(\\phi\\nabla\\psi)=\\phi\\nabla^{2}\\psi + \\nabla\\phi\\cdot\\nabla\\psi </math> \n*<math>\\psi\\nabla^2\\phi-\\phi\\nabla^2\\psi= \\nabla\\cdot\\left(\\psi\\nabla\\phi-\\phi\\nabla\\psi\\right)</math>\n*<math>\\nabla^2(\\phi\\psi)=\\phi\\nabla^2\\psi + 2(\\nabla\\phi)\\cdot(\\nabla\\psi) + (\\nabla^2\\phi)\\psi</math>\n*<math>\\nabla^2(\\psi\\mathbf{A})=\\mathbf{A}\\nabla^2\\psi+2(\\nabla\\psi\\cdot\\nabla)\\mathbf{A}+\\psi\\nabla^2\\mathbf{A}</math>\n*<math>\\nabla^2(\\mathbf{A}\\cdot\\mathbf{B})\\ =\\ \\mathbf{A}{\\cdot}\\nabla^2\\mathbf{B} \\,-\\, \\mathbf{B}{\\cdot}\\nabla^2\\!\\mathbf{A}\\, +\\, 2\\nabla {\\cdot}\\, ((\\mathbf{B}\\cdot\\nabla)\\mathbf{A} \\,+\\, \\mathbf{B}{\\times}(\\nabla{\\times}\\mathbf{A}))</math> ([[Green's identities|Green's vector identity]])\n\n====Third derivatives====\n\n*<math>\\nabla^{2}(\\nabla\\psi) = \\nabla(\\nabla\\cdot(\\nabla\\psi)) = \\nabla(\\nabla^{2}\\psi)</math>\n*<math>\\nabla^{2}(\\nabla\\cdot\\mathbf{A}) = \\nabla\\cdot(\\nabla(\\nabla\\cdot\\mathbf{A})) =\\nabla\\cdot(\\nabla^{2}\\mathbf{A})</math>\n*<math>\\nabla^{2}(\\nabla\\times\\mathbf{A}) = -\\nabla\\times(\\nabla\\times(\\nabla\\times\\mathbf{A})) = \\nabla\\times(\\nabla^{2}\\mathbf{A})</math>\n\n===Integration===\nBelow, the curly symbol ∂ means \"[[boundary (topology)|boundary of]]\".\n\n====Surface–volume integrals====\nIn the following surface–volume integral theorems, ''V'' denotes a three-dimensional volume with a corresponding two-dimensional [[Boundary (topology)|boundary]] ''S'' = ∂''V'' (a [[closed surface]]):\n\n*{{oiint|intsubscpt=<math>\\scriptstyle \\partial V</math>|integrand=<math>\\mathbf{A}\\cdot d\\mathbf{S}\\ =\\ \\iiint_V \\left(\\nabla \\cdot \\mathbf{A}\\right)dV </math>}} ([[Divergence theorem]])\n*{{oiint|intsubscpt=<math>\\scriptstyle \\partial V</math>|integrand=<math>\\psi\\, d \\mathbf{S} \\ =\\ \\iiint_V \\nabla \\psi\\, dV</math>}}\n*{{oiint|intsubscpt=<math>\\scriptstyle \\partial V</math>|integrand=<math>\\left(\\hat{\\mathbf{n}}\\times\\mathbf{A}\\right)dS=\\iiint _{V}\\left(\\nabla\\times\\mathbf{A}\\right)dV</math>}}\n*{{oiint|intsubscpt=<math>\\scriptstyle \\partial V</math>|integrand=<math>\\psi\\left(\\nabla\\varphi\\cdot\\hat{\\mathbf{n}}\\right)dS = \\iiint _{V}\\left(\\psi\\nabla^{2}\\varphi+\\nabla\\varphi\\cdot\\nabla\\psi\\right)dV</math>}} ([[Green's first identity]])\n*{{oiint|intsubscpt=<math>\\scriptstyle \\partial V</math>|integrand=<math>\\left[\\left(\\psi\\nabla\\varphi-\\varphi\\nabla\\psi\\right)\\cdot\\hat{\\mathbf{n}}\\right]dS=</math>{{oiint|intsubscpt=<math>\\scriptstyle \\partial V</math>|integrand=<math>\\left[\\psi\\frac{\\partial\\varphi}{\\partial n}-\\varphi\\frac{\\partial\\psi}{\\partial n}\\right]dS</math>}} <math>\\displaystyle=\\iiint_{V}\\left(\\psi\\nabla^{2}\\varphi-\\varphi\\nabla^{2}\\psi\\right)dV</math>}} ([[Green's second identity]])\n*{{Oiint|preintegral=<math>\\iiint_V \\left( \\mathbf{A} \\cdot \\nabla \\psi \\right) dV = </math>|intsubscpt=<math>\\scriptstyle \\partial V</math>|integrand=<math>\\left( \\psi \\mathbf{A}\\right) \\cdot \\hat{ \\mathbf{n} } \\,dS - \\iiint_V \\left( \\psi\\, \\nabla {\\cdot} \\mathbf{A} \\right) dV</math>}} ([[Integration by parts]])\n\n====Curve–surface integrals====\nIn the following curve–surface integral theorems, ''S'' denotes a 2d open surface with a corresponding 1d boundary ''C'' = ∂''S'' (a [[closed curve]]):\n\n*<math> \\oint_{\\partial S}\\mathbf{A}\\cdot d\\boldsymbol{\\ell}=\\iint_{S}\\left(\\nabla\\times\\mathbf{A}\\right)\\cdot d\\mathbf{S} </math>  ([[Stokes' theorem]])\n*<math> \\oint_{\\partial S}\\psi d\\boldsymbol{\\ell}=\\iint_{S}\\left(\\hat{\\mathbf{n}}\\times\\nabla\\psi\\right)dS </math>\n\nIntegration around a closed curve in the [[clockwise]] sense is the negative of the same line integral in the counterclockwise sense (analogous to interchanging the limits in a [[definite integral]]):\n\n:{{intorient|\n| preintegral = {{intorient|\n| preintegral =\n|symbol=oint\n| intsubscpt = <math>{\\scriptstyle \\partial S}</math>\n| integrand = <math>\\mathbf{A}\\cdot{\\rm d}\\boldsymbol{\\ell}=-</math>\n}}\n|symbol=ointctr\n| intsubscpt = <math>{\\scriptstyle \\partial S}</math>\n| integrand = <math>\\mathbf{A}\\cdot{\\rm d}\\boldsymbol{\\ell}.</math>\n}}\n\n==See also==\n* [[Exterior calculus identities]]\n* [[Exterior derivative]]\n* [[Vector calculus]]\n* [[Del in cylindrical and spherical coordinates]]\n* [[Comparison of vector algebra and geometric algebra]]\n\n==References==\n{{reflist}}\n\n==Further reading==\n{{Refbegin}}\n* {{cite book | title = Advanced Engineering Electromagnetics | first = Constantine A. | last = Balanis | isbn = 0-471-62194-3 }}\n* {{cite book | first = H. M. | last = Schey | title = Div Grad Curl and all that:  An informal text on vector calculus | publisher=W. W. Norton & Company | year= 1997 | isbn = 0-393-96997-5 }}\n* {{cite book | first = David J. | last = Griffiths | title = Introduction to Electrodynamics | publisher=Prentice Hall| year=1999 |isbn= 0-13-805326-X}}\n{{Refend}}\n\n<!--List of vector identities, oldid=343957081-->\n\n[[Category:Vector calculus]]\n[[Category:Mathematical identities]]\n[[Category:Mathematics-related lists]]\n\n[[bs:Spisak vektorskih identiteta]]\n[[eo:Vektoraj identoj]]\n[[zh:向量恆等式列表]]"
    },
    {
      "title": "Weitzenböck identity",
      "url": "https://en.wikipedia.org/wiki/Weitzenb%C3%B6ck_identity",
      "text": "{{distinguish|Weitzenböck's inequality}}\n\nIn [[mathematics]], in particular in [[differential geometry]], [[mathematical physics]], and [[representation theory]] a '''Weitzenböck identity''', named after [[Roland Weitzenböck]], expresses a relationship between two second-order [[elliptic operator]]s on a [[manifold (mathematics)|manifold]] with the same leading symbol.  (The origins of this terminology seem doubtful, however, as there does not seem to be any evidence that such identities ever appeared in Weitzenböck's work.) Usually Weitzenböck formulae are implemented for ''G''-invariant self-adjoint operators between vector bundles associated to some [[principal bundle|principal ''G''-bundle]], although the precise conditions under which such a formula exists are difficult to formulate.  This article focuses on three examples of Weitzenböck identities:  from Riemannian geometry, spin geometry, and complex analysis.\n\n==Riemannian geometry==\nIn [[Riemannian geometry]] there are two notions of the [[Laplacian]] on [[differential forms]] over an oriented compact Riemannian manifold ''M''.  The first definition uses the [[divergence|divergence operator]] ''δ'' defined as the formal adjoint of the de Rham operator ''d'':\n::<math>\\int_M \\langle \\alpha,\\delta\\beta\\rangle := \\int_M\\langle d\\alpha,\\beta\\rangle</math>\nwhere ''α'' is any ''p''-form and ''β'' is any ({{nowrap|''p'' + 1}})-form, and <math>\\langle -,-\\rangle</math> is the metric induced on the bundle of ({{nowrap|''p'' + 1}})-forms.  The usual '''form Laplacian''' is then given by\n\n::<math>\\Delta = d\\delta +\\delta d.</math>\n\nOn the other hand, the [[Levi-Civita connection]] supplies a differential operator\n\n::<math>\\nabla:\\Omega^pM\\rightarrow T^*M\\otimes\\Omega^pM ,</math>\n\nwhere Ω<sup>''p''</sup>''M'' is the bundle of ''p''-forms and ''T''<sup>∗</sup>''M'' is the [[cotangent bundle]] of ''M''.  The '''Bochner Laplacian''' is given by\n\n::<math>\\Delta'=\\nabla^*\\nabla</math>\n\nwhere <math>\\nabla^*</math> is the adjoint of <math>\\nabla</math>.\n\nThe Weitzenböck formula then asserts that\n\n::<math>\\Delta' - \\Delta = A</math>\n\nwhere ''A'' is a linear operator of order zero involving only the curvature.\n\nThe precise form of ''A'' is given, up to an overall sign depending on curvature conventions, by\n\n::<math>A=\\frac{1}{2}\\langle R(\\theta,\\theta)\\#,\\#\\rangle + \\operatorname{Ric}(\\theta,\\#) ,</math>\n\nwhere\n\n:*''R'' is the Riemann curvature tensor,\n:* Ric is the Ricci tensor,\n:* <math>\\theta:T^*M\\otimes\\Omega^pM\\rightarrow\\Omega^{p+1}M</math> is the map that takes the wedge product of a 1-form and ''p''-form and gives a (''p''+1)-form,\n:* <math>\\#:\\Omega^{p+1}M\\rightarrow T^*M\\otimes\\Omega^pM</math> is the universal derivation inverse to ''θ'' on 1-forms.\n\n==Spin geometry==\nIf ''M'' is an oriented [[spin manifold]] with [[Dirac operator]] ð, then one may form the spin Laplacian Δ = ð<sup>2</sup> on the spin bundle.  On the other hand, the Levi-Civita connection extends to the spin bundle to yield a differential operator\n:<math>\\nabla:SM\\rightarrow T^*M\\otimes SM.</math>\nAs in the case of Riemannian manifolds, let <math>\\Delta'=\\nabla^*\\nabla</math>.  This is another self-adjoint operator and, moreover, has the same leading symbol as the spin Laplacian.  The Weitzenböck formula yields:\n\n:<math>\\Delta'-\\Delta=-\\frac{1}{4}Sc</math>\n\nwhere ''Sc'' is the scalar curvature.  This result is also known as the [[Lichnerowicz formula]].\n\n==Complex differential geometry==\nIf ''M'' is a compact [[Kähler manifold]], there is a Weitzenböck formula relating the <math>\\bar{\\partial}</math>-Laplacian (see [[Dolbeault complex]]) and the Euclidean Laplacian on [[complex differential form|(''p'',''q'')-forms]].  Specifically, let\n\n: <math>\\Delta=\\bar{\\partial}^*\\bar{\\partial}+\\bar{\\partial}\\bar{\\partial}^*</math>, and\n: <math>\\Delta'=-\\sum_k\\nabla_k\\nabla_{\\bar{k}}</math> in a unitary frame at each point.\n\nAccording to the Weitzenböck formula, if α&nbsp;ε&nbsp;Ω<sup>(''p'',''q'')</sup>''M'', then\n\n: &Delta;'&alpha;&nbsp;&minus;&nbsp;&Delta;&alpha; = ''A''(&alpha;)\n\nwhere ''A'' is an operator of order zero involving the curvature.  Specifically,\n::if <math>\\alpha=\\alpha_{i_1i_2\\dots i_p\\bar{j}_1\\bar{j}_2\\dots\\bar{j}_q}</math> in a unitary frame, then\n::<math>A(\\alpha)=-\\sum_{k,j_s} \\operatorname{Ric}_{\\bar{j}_\\alpha}^{\\bar{k}}\\alpha_{i_1i_2\\dots i_p\\bar{j}_1\\bar{j}_2\\dots\\bar{k}\\dots\\bar{j}_q}</math> with ''k'' in the ''s''-th place.\n\n==Other Weitzenböck identities==\n*In [[conformal geometry]] there is a Weitzenböck formula relating a particular pair of differential operators defined on the [[tractor bundle]].  See Branson, T. and Gover, A.R., \"Conformally Invariant Operators, Differential Forms, Cohomology and a Generalisation of Q-Curvature\", ''Communications in Partial Differential Equations'', '''30''' (2005) 1611–1669.\n\n==See also==\n*[[Bochner identity]]\n*[[Bochner–Kodaira–Nakano identity]]\n*[[Laplacian operators in differential geometry]]\n\n==References==\n*{{citation|title=Principles of algebraic geometry|first1=Philip|last1=Griffiths|first2=Joe|last2=Harris|authorlink1=Philip A. Griffiths|authorlink2=Joe Harris (mathematician)|publisher=Wiley-Interscience|publication-date=1994|isbn=978-0-471-05059-9|year=1978}}\n\n{{DEFAULTSORT:Weitzenbock identity}}\n[[Category:Mathematical identities]]\n[[Category:Differential operators]]\n[[Category:Differential geometry]]"
    },
    {
      "title": "Multiple comparisons problem",
      "url": "https://en.wikipedia.org/wiki/Multiple_comparisons_problem",
      "text": "[[File:Spurious correlations - spelling bee spiders.svg|thumb|An example of data produced by [[data dredging]], showing a correlation between the number of letters in a spelling bee's winning word and the number of people in the United States killed by venomous spiders. The clear similarity in trends is a coincidence. If many data series are compared, similarly convincing but coincidental data may be obtained.]]\nIn [[statistics]], the '''multiple comparisons''', '''multiplicity''' or '''multiple testing problem''' occurs when one considers a set of [[statistical inference]]s simultaneously<ref>{{cite book | last=Miller | first=R.G. | year=1981 | title=Simultaneous Statistical Inference 2nd Ed | publisher=Springer Verlag New York | isbn=978-0-387-90548-8}}</ref> or infers a subset of parameters selected based on the observed values.<ref>{{cite journal | journal=Biometrical Journal | title=Simultaneous and selective inference: Current successes and future challenges | year=2010 | volume=52  | last=Benjamini | first=Y. | pages=708–721 | doi=10.1002/bimj.200900299 | issue=6 | pmid=21154895}}</ref> In certain fields it is known as the [[look-elsewhere effect]].\n\nThe more inferences are made, the more likely erroneous inferences are to occur.  Several statistical techniques have been developed to prevent this from happening, allowing significance levels for single and multiple comparisons to be directly compared.  These techniques generally require a stricter significance threshold for individual comparisons, so as to compensate for the number of inferences being made.\n\n==History==\n\nThe interest in the problem of multiple comparisons began in the 1950s with the work of [[Tukey]] and [[Scheffé]]. Other methods, such as the [[closed testing procedure]] (Marcus et al., 1976) and the [[Holm–Bonferroni method]] (1979), later emerged. In 1995, work on the [[false discovery rate]] began. In 1996, the first conference on multiple comparisons took place in [[Israel]]. This was followed by conferences around the world, usually taking place about every two years.<ref>[http://www.mcp-conference.org]</ref>\n\n==Definition==\nMultiple comparisons arise when a statistical analysis involves multiple simultaneous statistical tests, each of which has a potential to produce a \"discovery.\" A stated confidence level generally applies only to each test considered individually, but often it is desirable to have a confidence level for the whole family of simultaneous tests.<ref>{{cite book |last1=Kutner |first1=Michael |last2=Nachtsheim |first2=Christopher |last3=Neter |first3=John |authorlink3=John Neter |last4=Li |first4=William |date=2005 |title=Applied Linear Statistical Models |pages=744–745}}</ref>  Failure to compensate for multiple comparisons can have important real-world consequences, as illustrated by the following examples:\n\n* Suppose the treatment is a new way of teaching writing to students, and the control is the standard way of teaching writing.  Students in the two groups can be compared in terms of grammar, spelling, organization, content, and so on.  As more attributes are compared, it becomes increasingly likely that the treatment and control groups will appear to differ on at least one attribute due to random [[sampling error]] alone.\n* Suppose we consider the efficacy of a [[Pharmacology|drug]] in terms of the reduction of any one of a number of disease symptoms.  As more symptoms are considered, it becomes increasingly likely that the drug will appear to be an improvement over existing drugs in terms of at least one symptom.\n\nIn both examples, as the number of comparisons increases, it becomes more likely that the groups being compared will appear to differ in terms of at least one attribute. Our confidence that a result will generalize to independent data should generally be weaker if it is observed as part of an analysis that involves multiple comparisons, rather than an analysis that involves only a single comparison.\n\nFor example, if one test is performed at the 5% level and the corresponding null hypothesis is true, there is only a 5% chance of incorrectly rejecting the null hypothesis.  However, if 100 tests are conducted and all corresponding null hypotheses are true, the [[expected number]] of incorrect rejections (also known as [[false positive]]s or [[Type I error]]s) is 5.  If the tests are statistically independent from each other, the probability of at least one incorrect rejection is 99.4%.\n\nThe multiple comparisons problem also applies to [[confidence intervals]]. A single confidence interval with a 95% [[coverage probability]] level will contain the population parameter in 95% of experiments.  However, if one considers 100 confidence intervals simultaneously, each with 95% coverage probability, the expected number of non-covering intervals is 5. If the intervals are statistically independent from each other, the probability that at least one interval does not contain the population parameter is 99.4%.\n\nTechniques have been developed to prevent the inflation of false positive rates and non-coverage rates that occur with multiple statistical tests.\n\n===Classification of multiple hypothesis tests{{anchor|Classification of ''m'' hypothesis tests}}===\n<!-- [[Template:Classification of multiple hypothesis tests]]: -->\n{{Classification of multiple hypothesis tests}}\n\n==Controlling procedures==\n{{further information|Family-wise error rate#Controlling procedures}}\n{{see also|False coverage rate#Controlling procedures|False discovery rate#Controlling procedures}}\n\nIf ''m'' independent comparisons are performed, the ''[[family-wise error rate]]'' (FWER), is given by\n\n:<math> \\bar{\\alpha} = 1-\\left( 1-\\alpha_{\\{\\text{per comparison}\\}} \\right)^m.</math>\n\nHence, unless the tests are perfectly positively dependent (i.e., identical), <math>\\bar{\\alpha}</math> increases as the number of comparisons increases.\nIf we do not assume that the comparisons are independent, then we can still say:\n\n:<math> \\bar{\\alpha} \\le m \\cdot \\alpha_{\\{\\text{per comparison}\\}},</math>\n\nwhich follows from [[Boole's inequality]]. Example: <math> 0.2649=1-(1-.05)^6  \\le .05 \\times 6 = 0.3</math>\n\nThere are different ways to assure that the family-wise error rate is at most <math>\\bar{\\alpha}</math>. The most conservative method, which is free of dependence and distributional assumptions, is the [[Bonferroni correction]] <math> \\alpha_\\mathrm{\\{per\\ comparison\\}}={\\alpha}/m</math>.\n\nA marginally less conservative correction can be obtained by solving the equation for the family-wise error rate of <math>m</math> independent comparisons for <math>\\alpha_\\mathrm{\\{per\\ comparison\\}}</math>. This yields <math>\\alpha_{\\{\\text{per comparison}\\}} = 1-{(1-{\\alpha})}^{1/m}</math>, which is known as the [[Šidák correction]]. Another procedure is the [[Holm–Bonferroni method]], which uniformly delivers more power than the simple Bonferroni correction, by testing only the lowest p-value (<math>i=1</math>) against the strictest criterion, and the higher p-values (<math>i>1</math>) against progressively less strict criteria.<ref>{{cite journal | last1 = Aickin | first1 = M | last2 = Gensler | first2 = H  | title = Adjusting for multiple testing when reporting research results: the Bonferroni vs Holm methods | url = | journal = Am J Public Health | volume = 86| pages = 726–728 | doi=10.2105/ajph.86.5.726 | pmid=8629727 | date=May 1996 | pmc=1380484 | issue=5}}</ref>\n<math> \\alpha_\\mathrm{\\{per\\ comparison\\}}={\\alpha}/(m-i+1)</math>.\n\n{{anchor|Correction}}\n{{cleanup merge|section=y|Multiple testing correction|date=April 2016}}\n'''Multiple testing correction''' refers to re-calculating probabilities obtained from a statistical test which was repeated multiple times. In order to retain a prescribed family-wise error rate α in an analysis involving more than one comparison, the error rate for each comparison must be more stringent than&nbsp;''α''.  Boole's inequality implies that if each of ''m'' tests is performed to have type I error rate&nbsp;''α''/''m'', the total error rate will not exceed&nbsp;''α''.  This is called the [[Bonferroni correction]], and is one of the most commonly used approaches for multiple comparisons.\n\nIn some situations, the Bonferroni correction is substantially conservative, i.e., the actual family-wise error rate is much less than the prescribed level&nbsp;''α''.  This occurs when the test statistics are highly dependent (in the extreme case where the tests are perfectly dependent, the family-wise error rate with no multiple comparisons adjustment and the per-test error rates are identical).  For example, in fMRI analysis,<ref>{{Cite journal | last1 = Logan | first1 = B. R. | last2 = Rowe | first2 = D. B. | title = An evaluation of thresholding techniques in fMRI analysis | journal = NeuroImage | volume = 22 | issue = 1 | pages = 95–108 | year = 2004 | pmid = 15110000 | doi = 10.1016/j.neuroimage.2003.12.047| citeseerx = 10.1.1.10.421 }}</ref><ref>{{Cite journal | last1 = Logan | first1 = B. R. | last2 = Geliazkova | first2 = M. P. | last3 = Rowe | first3 = D. B. | doi = 10.1002/hbm.20471 | title = An evaluation of spatial thresholding techniques in fMRI analysis | journal = Human Brain Mapping | volume = 29 | issue = 12 | pages = 1379–1389 | year = 2008 | pmid =  18064589| pmc = }}</ref> tests are done on over 100,000 [[voxel]]s in the brain.  The Bonferroni method would require p-values to be smaller than .05/100000 to declare significance.  Since adjacent voxels tend to be highly correlated, this threshold is generally too stringent.\n\nBecause simple techniques such as the Bonferroni method can be conservative, there has been a great deal of attention paid to developing better techniques, such that the overall rate of false positives can be maintained without excessively inflating the rate of false negatives. Such methods can be divided into general categories:\n*Methods where total alpha can be proved to never exceed 0.05 (or some other chosen value) under any conditions. These methods provide \"strong\" control against Type I error, in all conditions including a partially correct null hypothesis.\n*Methods where total alpha can be proved not to exceed 0.05 except under certain defined conditions.\n*Methods which rely on an [[omnibus test]] before proceeding to multiple comparisons. Typically these methods require a significant [[ANOVA]], [[MANOVA]], or [[Tukey's range test]]. These methods generally provide only \"weak\" control of Type I error, except for certain numbers of hypotheses.\n*Empirical methods, which control the proportion of Type I errors adaptively, utilizing correlation and distribution characteristics of the observed data.\n\nThe advent of computerized [[resampling (statistics)|resampling]] methods, such as [[bootstrapping (statistics)|bootstrapping]] and [[Monte Carlo simulation]]s, has given rise to many techniques in the latter category. In some cases where exhaustive permutation resampling is performed, these tests provide exact, strong control of Type I error rates; in other cases, such as bootstrap sampling, they provide only approximate control.\n\n==Large-scale multiple testing==\nTraditional methods for multiple comparisons adjustments focus on correcting for modest numbers of comparisons, often in an [[analysis of variance]].  A different set of techniques have been developed for \"large-scale multiple testing\", in which thousands or even greater numbers of tests are performed. For example, in [[genomics]], when using technologies such as [[DNA microarray|microarray]]s, expression levels of tens of thousands of genes can be measured, and genotypes for millions of genetic markers can be measured.  Particularly in the field of [[genetic association]] studies, there has been a serious problem with non-replication — a result being strongly statistically significant in one study but failing to be replicated in a follow-up study.  Such non-replication can have many causes, but it is widely considered that failure to fully account for the consequences of making multiple comparisons is one of the causes.<ref>{{Cite journal|last=Qu|first=Hui-Qi|last2=Tien|first2=Matthew|last3=Polychronakos|first3=Constantin|date=2010-10-01|title=Statistical significance in genetic association studies|journal=Clinical and Investigative Medicine. Medecine Clinique et Experimentale|volume=33|issue=5|pages=E266–E270|issn=0147-958X|pmc=3270946|pmid=20926032}}</ref>\n\nIn different branches of science, multiple testing is handled in different ways.  It has been argued that if statistical tests are only performed when there is a strong basis for expecting the result to be true, multiple comparisons adjustments are not necessary.<ref>{{cite journal | doi=10.1097/00001648-199001000-00010 | last=Rothman | first=Kenneth J. | journal=Epidemiology | volume=1 | pages=43–46 | year=1990 | title=No Adjustments Are Needed for Multiple Comparisons | issue=1 | pmid=2081237 | jstor=20065622}}</ref>  It has also been argued that use of multiple testing corrections is an inefficient way to perform [[empirical research]], since multiple testing adjustments control false positives at the potential expense of many more [[Type I and type II errors|false negatives]].  On the other hand, it has been argued that advances in [[measurement]] and [[information technology]] have made it far easier to generate large datasets for [[exploratory data analysis|exploratory analysis]], often leading to the testing of large numbers of hypotheses with no prior basis for expecting many of the hypotheses to be true.  In this situation, very high [[false positive rate]]s are expected unless multiple comparisons adjustments are made.\n\nFor large-scale testing problems where the goal is to provide definitive results, the [[familywise error rate]] remains the most accepted parameter for ascribing significance levels to statistical tests.  Alternatively, if a study is viewed as exploratory, or if significant results can be easily re-tested in an independent study, control of the [[false discovery rate]] (FDR)<ref>{{cite journal | last=Benjamini | first=Yoav |author2=Hochberg, Yosef | year=1995 | title=Controlling the false discovery rate: a practical and powerful approach to multiple testing | journal=[[Journal of the Royal Statistical Society, Series B]] | volume=57 | pages=125–133 | issue=1 | jstor=2346101}}</ref><ref>{{cite journal | last=Storey | first=JD |author2=Tibshirani, Robert | year=2003 | title=Statistical significance for genome-wide studies | journal=PNAS | volume=100 | pages=9440–9445 | doi=10.1073/pnas.1530509100 | pmid=12883005 | issue=16 | pmc=170937 | jstor=3144228| bibcode=2003PNAS..100.9440S }}</ref><ref>{{cite journal | last=Efron | first=Bradley |author2=Tibshirani, Robert |author3=Storey, John D. |author4= Tusher, Virginia  | journal=[[Journal of the American Statistical Association]] | volume=96 | issue=456 | year=2001 | pages=1151–1160 | title=Empirical Bayes analysis of a microarray experiment | doi=10.1198/016214501753382129 | jstor=3085878}}</ref> is often preferred.  The FDR, loosely defined as the expected proportion of false positives among all significant tests, allows researchers to identify a set of \"candidate positives\" that can be more rigorously evaluated in a follow-up study.<ref>{{Cite journal|last=Noble|first=William S.|date=2009-12-01|title=How does multiple testing correction work?|journal=Nature Biotechnology|language=en|volume=27|issue=12|pages=1135–1137|doi=10.1038/nbt1209-1135|issn=1087-0156|pmc=2907892|pmid=20010596}}</ref>\n\nThe practice of trying many unadjusted comparisons in the hope of finding a significant one is a known problem, whether applied unintentionally or deliberately, is sometimes called \"p-hacking.\"<ref name=\"Deming\">{{Cite journal\n|author = Young, S. S., Karr, A.\n|title = Deming, data and observational studies\n|journal = Significance\n|volume = 8\n|issue = 3\n|pages = 116–120\n|year = 2011\n|url = http://www.niss.org/sites/default/files/Young%20Karr%20Obs%20Study%20Problem.pdf|doi = 10.1111/j.1740-9713.2011.00506.x\n}}\n</ref><ref name=\"bmj02\">\n{{Cite journal\n|author = Smith, G. D., Shah, E.\n|title = Data dredging, bias, or confounding\n|journal = BMJ\n|volume = 325\n|year = 2002\n|pmc = 1124898\n|doi = 10.1136/bmj.325.7378.1437\n|pmid=12493654\n|issue=7378\n|pages=1437–1438}}\n</ref>\n\n===Assessing whether any alternative hypotheses are true===\n[[Image:quantile meta test.svg|thumb|325px|A [[Q-Q plot|normal quantile plot]] for a simulated set of test statistics that have been standardized to be [[standard score|Z-scores]] under the null hypothesis. The departure of the upper tail of the distribution from the expected trend along the diagonal is due to the presence of substantially more large test statistic values than would be expected if all null hypotheses were true.  The red point corresponds to the fourth largest observed test statistic, which is 3.13, versus an expected value of 2.06.  The blue point corresponds to the fifth smallest test statistic, which is -1.75, versus an expected value of -1.96.  The graph suggests that it is unlikely that all the null hypotheses are true, and that most or all instances of a true alternative hypothesis result from deviations in the positive direction.]]\n\nA basic question faced at the outset of analyzing a large set of testing results is whether there is evidence that any of the alternative hypotheses are true.{{citation needed|date=June 2016}}  One simple meta-test that can be applied when it is assumed that the tests are independent of each other is to use the [[Poisson distribution]] as a model for the number of significant results at a given level α that would be found when all null hypotheses are true.{{citation needed|date=June 2016}}  If the observed number of positives is substantially greater than what should be expected, this suggests that there are likely to be some true positives among the significant results.{{citation needed|date=June 2016}}  For example, if 1000 independent tests are performed, each at level α&nbsp;=&nbsp;0.05, we expect 0.05 × 1000 = 50 significant tests to occur when all null hypotheses are true.  Based on the Poisson distribution with mean 50, the probability of observing more than 61 significant tests is less than 0.05, so if more than 61 significant results are observed, it is very likely that some of them correspond to situations where the alternative hypothesis holds.{{citation needed|date=June 2016}}  A drawback of this approach is that it over-states the evidence that some of the alternative hypotheses are true when the [[test statistic]]s are positively correlated, which commonly occurs in practice.  {{citation needed|date=August 2012}}. On the other hand, the approach remains valid even in the presence of correlation among the test statistics, as long as the Poisson distribution can be shown to provide a good approximation for the number of significant results. This scenario arises, for instance, when mining significant frequent itemsets from transactional datasets. Furthermore, a careful two stage analysis can bound the FDR at a pre-specified level.<ref>{{cite journal | last1 = Kirsch | first1 = A | last2 = Mitzenmacher | first2 = M | author2-link = Michael Mitzenmacher | last3 = Pietracaprina | first3 = A | last4 = Pucci | first4 = G |  last5 = Upfal | first5 = E | author5-link = Eli Upfal | last6 = Vandin | first6 = F | title = An Efficient Rigorous Approach for Identifying Statistically Significant Frequent Itemsets | url = | journal = Journal of the ACM | volume = 59 | issue = 3 | pages = 12:1–12:22 | doi=10.1145/2220357.2220359  | date=June 2012| arxiv = 1002.1104 }}</ref>\n\nAnother common approach that can be used in situations where the [[test statistic]]s can be standardized to [[standard score|Z-scores]] is to make a [[Q-Q plot|normal quantile plot]] of the test statistics.  If the observed quantiles are markedly more [[statistical dispersion|dispersed]] than the normal quantiles, this suggests that some of the significant results may be true positives.{{citation needed|date=January 2012}}\n\n==See also==\n;Key concepts\n*[[Familywise error rate]]\n*[[False positive rate]]\n*[[False discovery rate]] (FDR)\n*[[False coverage rate]] (FCR)\n*[[Interval estimation]]\n*[[Post-hoc analysis]]\n*[[Experimentwise error rate]]\n\n;General methods of alpha adjustment for multiple comparisons\n*[[Closed testing procedure]]\n*[[Bonferroni correction]]\n*Boole–[[Bonferroni bound]]\n*[[Holm–Bonferroni method]]\n*[[Harmonic mean p-value]] procedure\n\n;Related concepts\n*[[Testing hypotheses suggested by the data]]\n*[[Texas sharpshooter fallacy]]\n*[[Model selection]]\n*[[Look-elsewhere effect]]\n\n==References==\n{{Reflist|30em}}\n\n==Further reading==\n* F. Betz, T. Hothorn, P. Westfall (2010), ''Multiple Comparisons Using R'', CRC Press\n* [[Sandrine Dudoit|S. Dudoit]] and M. J. van der Laan (2008), ''Multiple Testing Procedures with Application to Genomics'', Springer\n* B. Phipson and G. K. Smyth (2010), \"Permutation P-values Should Never Be Zero: Calculating Exact P-values when Permutations are Randomly Drawn\", ''Statistical Applications in Genetics and Molecular Biology'' Vol.. 9 Iss. 1, Article 39, {{doi|10.2202/1544-6155.1585}}\n* P. H. Westfall and S. S. Young (1993), ''Resampling-based Multiple Testing: Examples and Methods for p-Value Adjustment'', Wiley\n* P. Westfall, R. Tobias, R. Wolfinger (2011) ''Multiple comparisons and multiple testing using SAS'', 2nd edn, SAS Institute\n* [http://www.tylervigen.com/spurious-correlations A gallery of examples of implausible correlations sourced by data dredging]\n{{Experimental design}}\n{{Statistics}}\n\n[[Category:Statistical hypothesis testing]]\n[[Category:Multiple comparisons| ]]"
    },
    {
      "title": "Bonferroni correction",
      "url": "https://en.wikipedia.org/wiki/Bonferroni_correction",
      "text": "In [[statistics]], the '''Bonferroni correction''' is one of several methods used to counteract the problem of [[Multiple comparisons problem|multiple comparisons]].\n\n==Background==\nThe Bonferroni correction is named after Italian [[mathematician]] [[Carlo Emilio Bonferroni]] for its use of [[Bonferroni inequalities]].<ref>Bonferroni, C. E., Teoria statistica delle classi e calcolo delle probabilità, Pubblicazioni del R Istituto Superiore di Scienze Economiche e Commerciali di Firenze 1936</ref> Its development is often credited to [[Olive Jean Dunn]], who described the procedure's application to [[confidence intervals]].<ref name=Dunn1958>{{cite journal |first=Olive Jean |last=Dunn |title=Estimation of the Means for Dependent Variables |journal=[[Annals of Mathematical Statistics]] |volume=29 |issue=4 |pages=1095–1111 |year=1958 |jstor=2237135 |doi=10.1214/aoms/1177706374}}</ref><ref name=Dunn1961>{{cite journal |first=Olive Jean |last=Dunn |title=Multiple Comparisons Among Means |journal=[[Journal of the American Statistical Association]] |volume=56 |issue=293 |pages=52–64 |year=1961 |url=http://sci2s.ugr.es/keel/pdf/algorithm/articulo/1961-Bonferroni_Dunn-JASA.pdf |doi=10.1080/01621459.1961.10482090 |citeseerx=10.1.1.309.1277 }}</ref>\n\n[[Statistical hypothesis testing]] is based on rejecting the [[null hypothesis]] if the likelihood of the observed data under the null hypotheses is low. If multiple hypotheses are tested, the chance of a rare event increases, and therefore, the likelihood of incorrectly rejecting a null hypothesis (i.e., making a [[Type I error]]) increases.<ref>{{cite book |first=Ron C. |last=Mittelhammer |first2=George G. |last2=Judge |first3=Douglas J. |last3=Miller |title=Econometric Foundations |location= |publisher=Cambridge University Press |year=2000 |isbn=978-0-521-62394-0 |pages=73–74 |url=https://books.google.com/books?id=fycmsfkK6RQC&pg=PA73 }}</ref>\n\nThe Bonferroni correction compensates for that increase by testing each individual hypothesis at a significance level of <math>\\alpha/m</math>, where <math>\\alpha</math> is the desired overall alpha level and <math>m</math> is the number of hypotheses.<ref>{{cite book |first=Rupert G. |last=Miller |title=Simultaneous Statistical Inference |publisher=Springer |year=1966|url=https://books.google.com/books?id=4C7VBwAAQBAJ&printsec=frontcover#v=onepage&q=Bonferroni&f=false|isbn=9781461381228 }}</ref> For example, if a trial is testing <math>m = 20</math> hypotheses with a desired <math>\\alpha = 0.05</math>, then the Bonferroni correction would test each individual hypothesis at <math>\\alpha = 0.05/20 = 0.0025</math>.\n\n==Definition==\nLet <math>H_1,\\ldots,H_m</math> be a family of hypotheses and <math>p_1,\\ldots,p_m</math> their corresponding [[p-value]]s. Let <math>m</math> be the total number of null hypotheses and <math>m_0</math> the number of true null hypotheses. The [[familywise error rate]] (FWER) is the probability of rejecting at least one true <math>H_{i}</math>, that is, of making at least one [[type I error]]. The Bonferroni correction rejects the null hypothesis for each <math>p_i\\leq\\frac \\alpha m</math>, thereby controlling the [[familywise error rate|FWER]] at <math>\\leq \\alpha</math>. Proof of this control follows from [[Boole's inequality]], as follows:\n\n: <math> \\text{FWER} = P\\left\\{ \\bigcup_{i=1}^{m_0}\\left(p_i\\leq\\frac \\alpha m \\right) \\right\\} \\leq\\sum_{i=1}^{m_0}\\left\\{P\\left(p_i\\leq\\frac \\alpha m\\right)\\right\\} = m_0 \\frac \\alpha m\\leq m \\frac \\alpha m = \\alpha.</math>\n\nThis control does not require any assumptions about dependence among the p-values or about how many of the null hypotheses are true.<ref>{{cite journal |last1=Goeman |first1=Jelle J. |last2=Solari |first2=Aldo |title=Multiple Hypothesis Testing in Genomics |journal=[[Statistics in Medicine (journal)|Statistics in Medicine]] |date=2014 |volume=33 |issue=11 |pages= 1946–1978|doi=10.1002/sim.6082 |pmid=24399688 }}</ref>\n\n==Extensions==\n\n===Generalization===\nRather than testing each hypothesis at the <math>\\alpha/m</math> level, the hypotheses may be tested at any other combination of levels that add up to <math>\\alpha</math>, provided that the level of each test is determined before looking at the data.<ref name=\"pmid8014990\">{{cite journal |last=Neuwald |first=AF |last2=Green |first2=P |title=Detecting patterns in protein sequences |journal=J. Mol. Biol. |volume=239 |issue=5 |pages=698–712 |year=1994 |pmid=8014990 |doi=10.1006/jmbi.1994.1407 |url=}}</ref> For example, for two hypothesis tests, an overall <math>\\alpha</math> of .05 could be maintained by conducting one test at .04 and the other at .01.\n\n===Confidence intervals===\nThe Bonferroni correction can be used to adjust [[confidence intervals]]. If one establishes <math>m</math> confidence intervals, and wishes to have an overall confidence level of <math>1-\\alpha</math>, each individual confidence interval can be adjusted to the level of <math>1-\\frac{\\alpha}{m}</math>.<ref name=Dunn1958 /><ref name=Dunn1961 />\n\n==Alternatives==\n{{main|Familywise error rate#Controlling procedures}}\nThere are alternative ways to control the [[familywise error rate]].\nFor example, the [[Holm–Bonferroni method]] and the [[Šidák correction]] are universally more powerful procedures than the Bonferroni correction, meaning that they are always at least as powerful. Unlike the Bonferroni procedure, these methods do not control the [[expected number]] of Type I errors per family (the per-family Type I error rate).<ref>{{cite journal|last1=Frane|first1=Andrew|title=Are per-family Type I error rates relevant in social and behavioral science?| journal=Journal of Modern Applied Statistical Methods| date=2015| volume=14| issue=1| pages=12–23| url=http://digitalcommons.wayne.edu/jmasm/vol14/iss1/5}}</ref>\n\n==Criticism==\nWith respect to [[familywise error rate|FWER]] control, the Bonferroni correction can be conservative if there are a large number of tests and/or the test statistics are positively correlated.\n\nThe correction comes at the cost of increasing the probability of producing [[Type I and type II errors#Type II error|false negatives]], i.e., reducing [[statistical power]].<ref name=Nakagawa2004>{{cite journal |first=Shinichi |last=Nakagawa |title=A farewell to Bonferroni: the problems of low statistical power and publication bias |journal=[[Behavioral Ecology]] |volume=15 |issue=6 |pages=1044–1045 |year=2004 |url=https://academic.oup.com/beheco/article/15/6/1044/206216 |doi=10.1093/beheco/arh107}}</ref>\n\nThere is not a definitive consensus on how to define a family in all cases, and adjusted test results may vary depending on the number of tests included in the [[Familywise error rate#The concept of a family|family]] of hypotheses.{{citation needed|date=June 2016}}\n\nNote that these criticisms apply to [[familywise error rate|FWER]] control in general, and are not specific to the Bonferroni correction.\n\n==References==\n{{Reflist}}\n\n==Further reading==\n{{Refbegin|60em}}\n*{{cite journal |last=Dunnett |first=C. W. |year=1955 |title=A multiple comparisons procedure for comparing several treatments with a control |journal=Journal of the American Statistical Association |volume=50 |issue=272 |pages=1096–1121 |doi=10.1080/01621459.1955.10501294 }}\n*{{cite journal |last=Dunnett |first=C. W. |year=1964 |title=New tables for multiple comparisons with a control |journal=[[Biometrics (journal)|Biometrics]] |volume=20 |issue=3 |pages=482–491 |jstor=2528490 |doi=10.2307/2528490}}\n*{{cite journal |last=Shaffer |first=J. P. |year=1995 |title=Multiple Hypothesis Testing |journal=[[Annual Review of Psychology]] |volume=46 |issue= |pages=561–584 |doi=10.1146/annurev.ps.46.020195.003021 }}\n*{{cite journal |last=Strassburger |first=K. |last2=Bretz |first2=Frank |year=2008 |title=Compatible simultaneous lower confidence bounds for the Holm procedure and other Bonferroni-based closed tests |journal=[[Statistics in Medicine (journal)|Statistics in Medicine]] |volume=27 |issue=24 |pages=4914–4927 |doi=10.1002/sim.3338 |pmid=18618415 }}\n*{{cite journal |last=Šidák |first=Z. |year=1967 |title=Rectangular confidence regions for the means of multivariate normal distributions |journal=Journal of the American Statistical Association |volume=62 |issue=318 |pages=626–633 |doi=10.1080/01621459.1967.10482935 }}\n*{{cite journal | last1 = Hochberg | first1= Yosef | year = 1988 | title = A Sharper Bonferroni Procedure for Multiple Tests of Significance | journal = [[Biometrika]] | volume = 75 | issue = 4 | pages = 800–802 | url = http://www-stat.wharton.upenn.edu/~steele/Courses/956/Resource/MultipleComparision/Hochberg88.pdf | doi=10.1093/biomet/75.4.800}}\n{{Refend}}\n\n==External links==\n*[http://www.quantitativeskills.com/sisa/calculations/bonfer.htm Bonferroni, Sidak online calculator]\n*[http://nebc.nerc.ac.uk/courses/GeneSpring/GS_Mar2006/Multiple%20testing%20corrections.pdf Multiple Testing Corrections in GeneSpring and Gene Expression data]\n\n{{DEFAULTSORT:Bonferroni Correction}}\n[[Category:Multiple comparisons]]\n[[Category:Statistical hypothesis testing]]"
    },
    {
      "title": "Closed testing procedure",
      "url": "https://en.wikipedia.org/wiki/Closed_testing_procedure",
      "text": "{{primary source|date=June 2012}}\nIn [[statistics]], the '''closed testing procedure'''<ref>{{cite journal | doi = 10.1093/biomet/63.3.655 | last1 = Marcus | first1 = R | last2 = Peritz | first2 = E | last3 = Gabriel | first3 = KR | author3-link=K. Ruben Gabriel | year = 1976 | title = On closed testing procedures with special reference to ordered analysis of variance | url = | journal = Biometrika | volume = 63 | issue = | pages = 655–660 |jstor=2335748}}</ref> is a general method for performing more than one [[Statistical hypothesis testing|hypothesis test]] simultaneously.\n\n==The closed testing principle==\nSuppose there are ''k'' hypotheses ''H''<sub>1</sub>,..., ''H''<sub>''k''</sub> to be tested and the overall type I error rate is α.  The closed testing principle allows the rejection of any one of these elementary hypotheses, say ''H''<sub>''i''</sub>, if all possible intersection hypotheses involving ''H''<sub>''i''</sub> can be rejected by using valid local level α tests. It controls the [[familywise error rate]] for all the ''k'' hypotheses at level α in the strong sense.\n\n==Example==\nSuppose there are three hypotheses ''H''<sub>1</sub>,''H''<sub>2</sub>, and ''H''<sub>3</sub> to be tested and the overall type I error rate is 0.05. Then ''H''<sub>1</sub> can be rejected at level α if ''H''<sub>1</sub> ∩ ''H''<sub>2</sub> ∩ ''H''<sub>3</sub>,  ''H''<sub>1</sub> ∩ ''H''<sub>2</sub>,  ''H''<sub>1</sub> ∩ ''H''<sub>3</sub> and  ''H''<sub>1</sub> can all be rejected using valid tests with level 0.05.\n\n==Special cases==\nThe [[Holm–Bonferroni method]] is a special case of a closed test procedure for which each intersection null hypothesis is tested using the simple Bonferroni test.  As such, it controls the [[familywise error rate]] for all the ''k'' hypotheses at level α in the strong sense.\n\nMultiple test procedures developed using the graphical approach for constructing and illustrating multiple test procedures<ref>{{cite journal | doi = 10.1002/sim.3495 | last1 = Bretz | first1 = F | last2 = Maurer | first2 = W | last3 = Brannath | first3 = W | last4 = Posch | first4 = M | year = 2009 | title = A graphical approach to sequentially rejective multiple test procedures | url = | journal = [[Stat Med]] | volume = 28 | issue = 4 | pages = 586-604 }}</ref> are a subclass of closed testing procedures.\n\n==See also==\n* [[Multiple comparisons]]\n* [[Holm–Bonferroni method]]\n* [[Bonferroni correction]]\n\n==References==\n{{Reflist}}\n\n[[Category:Statistical hypothesis testing]]\n[[Category:Statistical tests]]\n[[Category:Multiple comparisons]]"
    },
    {
      "title": "Contrast (statistics)",
      "url": "https://en.wikipedia.org/wiki/Contrast_%28statistics%29",
      "text": "In [[statistics]], particularly in [[analysis of variance]] and [[linear regression]], a '''contrast''' is a [[linear combination]] of variables ([[Statistical parameter|parameters]] or [[statistic]]s) whose coefficients add up to zero, allowing comparison of different treatments.<ref name=\"CasellaBerger2001\">{{cite book |last=Casella |first=George |last2=Berger |first2=Roger L |year=2001 |title=Statistical inference |url=http://www.cengage.com/search/productOverview.do?N=16%204294945501%204294965305 |publisher=Cengage Learning |isbn=9780534243128}}</ref><ref name=\"casella2008\">{{cite book |author=[[George Casella]] |year=2008 |title=Statistical design |url=https://www.springer.com/statistics/statistical+theory+and+methods/book/978-0-387-75964-7 |publisher=[[Springer Science+Business Media|Springer]] |isbn=978-0-387-75965-4 }}</ref>\n\n==Definitions==\nLet <math>\\theta_1,\\ldots,\\theta_t</math> be a set of variables, either [[Statistical parameter|parameters]] or [[statistic]]s, and <math>a_1,\\ldots,a_t</math> be known constants. The quantity <math>\\sum_{i=1}^t a_i \\theta_i</math> is a linear combination. It is called a '''contrast''' if {{nowrap|<math>\\sum_{i=1}^t a_i = 0</math>.}}<ref>Casella a Berger 2001, p. 526.</ref><ref name=\"Casella 2008, p. 11\">Casella 2008, p. 11.</ref> Furthermore, two contrasts, <math>\\sum_{i=1}^t a_i \\theta_i</math> and <math>\\sum_{i=1}^t b_i \\theta_i</math>, are '''orthogonal''' if {{nowrap|<math>\\sum_{i=1}^t a_i b_i = 0</math>.}}<ref>Casella 2008, p. 12.</ref>\n\n==Examples==\nLet us imagine that we are comparing four means, <math>\\mu_1,\\mu_2,\\mu_3,\\mu_4</math>. The following table describes three possible contrasts:\n{| class=\"wikitable\" style=\"text-align:center\"\n! <math>\\mu_1</math> !! <math>\\mu_2</math> !! <math>\\mu_3</math> !! <math>\\mu_4</math>\n|-\n| 1 || -1 || 0 || 0\n|-\n| 0 || 0 || 1 || -1\n|-\n| 1 || 1 || -1 || -1\n|}\nThe first contrast allows comparison of the first mean with the second, the second contrast allows comparison of the third mean with the fourth, and the third contrast allows comparison of the average of the first two means with the average of the last two.<ref name=\"Casella 2008, p. 11\"/>\n\nIn a balanced [[one-way analysis of variance]], using orthogonal contrasts has the advantage of completely partitioning the treatment sum of squares into non-overlapping additive components that represent the variation due to each contrast.<ref>Casella 2008, p. 13.</ref> Consider the numbers above: each of the rows sums up to zero (hence they are contrasts). If we multiply each element of the first and second row and add those up, this again results in zero, thus the first and second contrast are orthogonal and so on.\n\n==Sets of contrast==\n*'''Orthogonal contrasts''' are a set of contrasts in which, for any distinct pair, the sum of the cross-products of the coefficients is zero (assume sample sizes are equal).<ref name=EV>Everitt, B.S. (2002) ''The Cambridge Dictionary of Statistics'', CUP. {{isbn|0-521-81099-X}} (entry for \"Orthogonal contrasts\")</ref> Although there are potentially infinite sets of orthogonal contrasts, within any given set there will always be a maximum of exactly ''k''&nbsp;–&nbsp;1 possible orthogonal contrasts (where ''k'' is the number of group means available).<ref name=\"Howell\"/>\n*'''Polynomial contrasts''' are a special set of orthogonal contrasts that test polynomial patterns in data with more than two means (e.g., linear, quadratic, cubic, quartic, etc.).<ref>{{cite web |last=Kim |first=Jong Sung |title=Orthogonal Polynomial Contrasts |url=http://www.mth.pdx.edu/~jkim/Teaching/S4566/S2012/Handout/Hd6_1%28Polycnst_RCBD%29.pdf |accessdate=27 April 2012}}</ref>\n*'''Orthonormal contrasts''' are orthogonal contrasts which satisfy the additional condition that, for each contrast, the sum squares of the coefficients add up to one.<ref name=EV/>\n\n==Background==\nA contrast is defined as the sum of each group mean multiplied by a coefficient for each group (i.e., a signed number, c<sub>j</sub>).<ref name=\"Clark\">{{cite book |last=Clark |first=James M. |title=Intermediate Data Analysis: Multiple Regression and Analysis of Variance |year=2007 |location=University of Winnipeg }}</ref> In equation form, \n<math>L = c_1 \\bar X_1 + c_2 \\bar X_2 + \\cdots + c_k \\bar X_k  \\equiv \\sum_j c_j \\bar X_j</math>, where L is the weighted sum of group means, the c<sub>j</sub> coefficients represent the assigned weights of the means (these must sum to 0 for orthogonal contrasts), and <math>\\bar X</math><sub>j</sub> represents the group means.<ref name=\"Howell\">{{cite book |last=Howell |first=David C. |title=Statistical methods for psychology |year=2010 |publisher=Thomson Wadsworth |location=Belmont, CA |isbn=978-0-495-59784-1 |edition=7th}}</ref> Coefficients can be positive or negative, and fractions or whole numbers, depending on the comparison of interest.  Linear contrasts are very useful and can be used to test complex hypotheses when used in conjunction with ANOVA or multiple regression. In essence, each contrast defines and tests for a particular pattern of differences among the means.<ref name=\"Clark\" />\n\nContrasts should be constructed \"to answer specific research questions\", and do not necessarily have to be orthogonal.<ref name=Kuehl>{{cite book |last=Kuehl |first=Robert O. |title=Design of experiments: statistical principles of research design and analysis |year=2000 |publisher=Duxbury/Thomson Learning |location=Pacific Grove, CA |isbn=0534368344 |edition=2nd}}</ref>\n\nA simple (not necessarily orthogonal) contrast is the difference between two means. A more complex contrast can test differences among several means (ex. with four means, assigning coefficients of –3, –1, +1, and +3), or test the difference between a single mean and the combined mean of several groups (e.g., if you have four means assign coefficients of –3, +1, +1, and +1) or test the difference between the combined mean of several groups and the combined mean of several other groups (i.e., with four means, assign coefficients of –1, –1, +1, and +1).<ref name=\"Howell\" /> The coefficients for the means to be combined (or averaged) must be the same in magnitude and direction, that is, equally weighted. When means are assigned different coefficients (either in magnitude or direction, or both), the contrast is testing for a difference between those means. A ''contrast'' may be any of: the set of coefficients used to specify a comparison; the specific value of the linear combination obtained for a given study or experiment; the [[random variable|random quantity]] defined by applying the linear combination to treatment effects when these are themselves considered as random variables. In the last context, the term '''contrast variable''' is sometimes used.\n\nContrasts are sometimes used to compare [[mixed model|mixed effects]]. A common example is the difference between two test scores &mdash; one at the beginning of the semester and one at its end. Note that we are not interested in one of these scores by itself, but only in the contrast (in this case &mdash; the difference). Since this is a linear combination of independent variables, its variance equals the weighted sum of the summands' variances; in this case both weights are one. This \"blending\" of two variables into one might be useful in many cases such as [[ANOVA]], [[Regression analysis|regression]], or even as descriptive statistics in its own right.\n\nAn example of a complex contrast would be comparing 5 standard treatments to a new treatment, hence giving each old treatment mean a weight of 1/5, and the new sixth treatment mean a weight of &minus;1 (using the equation above). If this new linear combination has a mean zero, this will mean that there is no evidence that the old treatments are different from the new treatment on average. If the sum of the new linear combination is positive, there is some evidence (the strength of the evidence is often associated with the p-value computed on that linear combination) that the combined mean of the 5 standard treatments is higher than the new treatment mean. Analogous conclusions obtain when the linear combination is negative.<ref name=\"Clark\" />  However, the sum of the linear combination is not a significance test, see testing significance (below) to learn how to determine if the contrast computed from the sample is significant.\n\nThe usual results for linear combinations of [[Statistical independence|independent random variables]] mean that the variance of a contrast is equal to the weighted sum of the variances.<ref name=\"nist\">[http://www.itl.nist.gov/div898/handbook/prc/section4/prc426.htm NIST/SEMATECH e-Handbook of Statistical Methods]</ref> If two contrasts are [[Orthogonality|orthogonal]], estimates created by using such contrasts will be [[uncorrelated]]. If orthogonal contrasts are available, it is possible to summarize the results of a statistical analysis in the form of a simple analysis of variance table, in such a way that it contains the results for different test statistics relating to different contrasts, each of which are statistically independent. Linear contrasts can be easily converted into [[partition of sums of squares|sums of squares]].  SS<sub>contrast</sub> = <math>\\tfrac{n(\\sum c_j \\bar X_j)^2 }{\\sum c_j^2} </math>, with 1 [[Degrees of freedom (statistics)|degree of freedom]], where ''n'' represents the number of observations per group. If the contrasts are orthogonal, the sum of the SS<sub>contrasts</sub> = SS<sub>treatment</sub>. Testing the significance of a contrast requires the computation of SS<sub>contrast</sub>.<ref name=\"Howell\" /> A recent development in statistical analysis is the [[standardized mean of a contrast variable]]. This makes a comparison between the size of the differences between groups, as measured by a contrast and the accuracy with which that contrast can be measured by a given study or experiment.<ref name=ZhangBook2011>{{cite book \n|author= Zhang XHD\n|year=2011\n|title= Optimal High-Throughput Screening: Practical Experimental Design and Data Analysis for Genome-scale RNAi Research \n|publisher =Cambridge University Press\n|url= \n|isbn=978-0-521-73444-8}}</ref>\n\n==Testing significance==\nSS<sub>contrast</sub> also happens to be a mean square because all contrasts have 1 degree of freedom.  Dividing <math>MS_{contrast}</math> by <math>MS_{error}</math> produces an [[F-test|F-statistic]] with one and <math>df_{error}</math> degrees of freedom, the [[statistical significance]] of ''F''<sub>contrast</sub> can be determined by comparing the obtained F statistic with a critical value of ''F'' with the same degrees of freedom.<ref name=\"Howell\" />\n\n==References==\n* {{cite book |last=Casella |first=George |last2=Berger |first2=Roger L\n|year=2001\n|title=Statistical inference\n|url=http://www.cengage.com/search/productOverview.do?N=16%204294945501%204294965305\n|publisher=Cengage Learning\n|isbn=9780534243128}}\n* {{cite book |author=[[George Casella]]\n|year=2008\n|title=Statistical design\n|url=https://www.springer.com/statistics/statistical+theory+and+methods/book/978-0-387-75964-7\n|publisher=[[Springer Science+Business Media|Springer]]\n|isbn=978-0-387-75965-4}}\n* {{cite book |last=Everitt |first=B S |last2=Skrondal |first2=A\n|year=2010\n|title=Cambridge dictionary of statistics\n|edition=4th\n|url=http://www.cambridge.org/us/academic/subjects/statistics-probability/statistics-and-probability-general-interest/cambridge-dictionary-statistics-4th-edition\n|publisher=[[Cambridge University Press]]\n|isbn=9780521766999}}\n* {{cite book |last=Dean |first=Angela M. | author1-link= Angela Dean |last2=Voss |first2=Daniel\n|year=1999\n|title=Design and analysis of experiments\n|url=https://www.springer.com/mathematics/probability/book/978-0-387-98561-9\n|publisher=[[Springer Science+Business Media|Springer]]\n|isbn=9780387985619}}\n\n==External links==\n* [http://www.soton.ac.uk/~cpd/anovas/datasets/Orthogonal%20contrasts.htm  Examples of orthogonal contrasts for analysis of variance]\n* [http://www.utdallas.edu/~herve/abdi-contrasts2010-pretty.pdf Contrast Analysis (Abdi & Williams, 2010)]\n\n==Notes==\n{{reflist}}\n\n[[Category:Analysis of variance]]\n[[Category:Regression analysis]]\n[[Category:Multiple comparisons]]"
    },
    {
      "title": "Duncan's new multiple range test",
      "url": "https://en.wikipedia.org/wiki/Duncan%27s_new_multiple_range_test",
      "text": "In [[statistics]], '''Duncan's new multiple range test (MRT)''' is a [[multiple comparisons|multiple comparison]] procedure developed by [[David B. Duncan]] in 1955. Duncan's MRT belongs to the general class of multiple comparison procedures that use the [[studentized range]] statistic ''q''<sub>''r''</sub> to compare sets of means.\n\nDavid B. Duncan developed this test as a modification of the [[Student–Newman–Keuls method]] that would have greater power. Duncan's MRT is especially protective against [[Type I and type II errors|false negative (Type II) error]] at the expense of having a greater risk of making [[Type I and type II errors|false positive (Type I) errors]]. Duncan's test is commonly used in [[agronomy]] and other agricultural research.\n\nThe result of the test is a set of subsets of means, where in each subset means have been found not to be significantly different from one another.\n\n== Definition ==\nAssumptions: <br />\n1.A sample of observed means  <math> m_{1},m_{2},...,m_{n} </math> , which have been drawn independently from n normal populations with \"true\" means, <math> \\mu_{1},\\mu_{2},...,\\mu_{n} </math>  respectively.\n<br />\n2.A common [[standard error]] <math>\\sigma </math>. This [[standard error]] is unknown, but there is available the usual estimate <math>s_{m}</math> , which is independent of the observed means and is based on a number of [[degrees of freedom]], denoted by <math>n_{2}</math> . (More precisely, <math>S_{m}</math>, has the property that <math> \\frac{n_{2}\\cdot S_{m}^2}{\\sigma^2_{m}} </math>  is distributed as <math>\\chi^2</math> with <math>n_2</math> degrees of freedom, independently of sample means).\n\nThe exact definition of the test is:\n\nThe difference between any two means in a set of n means is significant provided the range of each and every subset which contains the given means is significant according to an <math>\\alpha_{p}</math> level range test where <math>\\alpha_p=1-\\gamma_p</math> , <math>\\gamma_p =(1-\\alpha)^{(p-1)}</math> and <math>p</math> is the number of means in the subset concerned.\n\nException: The sole exception to this rule is that no difference\nbetween two means can be declared significant if the two means concerned\nare both contained in a subset of the means which has a non-significant\nrange.\n\n=== Procedure ===\nThe procedure consists of a series of [[pairwise comparisons]] between means. Each comparison is performed at a significance level <math> \\alpha_{p}</math> , defined by the number of means separating the two means compared (<math>\\alpha_p</math> for <math>p-2</math> separating means). The test are performed sequentially, where the result of a test determines which test is performed next.\n\nThe tests are performed in the following order: the largest minus the smallest, the largest minus the second smallest, up to the largest minus the second largest; then the second largest minus the smallest, the second largest minus the second smallest, and so on, finishing with the second smallest minus the smallest.\n\nWith only one exception, given below, each difference is significant if it exceeds the corresponding shortest significant range; otherwise it is not significant. Where the shortest significant range is the significant [[studentized range]], multiplied by the standard error.\nThe shortest significant range will be designated as <math> R_{(p,\\alpha)} </math>, where <math>p</math> is the number means in the subset.\nThe sole exception to this rule is that no difference between two means can be declared significant if the two means concerned are both contained in a subset of the means which has a non-significant range.\n\nAn algorithm for performing the test is as follows:\n\n        1.Rank the sample means, largest to smallest.\n        2. For each <math>m_{i}</math> sample mean, largest to smallest, do the following:\n        2.1 for each sample mean, (denoted <math>m_{j}</math>), for smallest up to <math> m_{(i-1)} </math>.\n        2.1.1 compare <math> m_i -m_j </math> to critical value <math> \\sigma_m \\cdot R_{(p,\\alpha)} </math>,<math> P=i-j, \\alpha=\\alpha_p </math>\n        2.1.2 if <math>m_i-m_j</math> does not exceed the critical value, '''the subset <math> (m_j , m_{j+1},...,m_{I}) </math> is declared not siginificantlly different''':\n                2.1.2.1 Go to next iteration of loop 2.\n        2.1.3 Otherwise, keep going with loop 2.1\n\n=== Critical values ===\nDuncan's multiple range test makes use of the [[studentized range distribution]] in order to determine critical values for comparisons between means. Note that different comparisons between means may differ by their significance levels- since the significance level is subject to the size of the subset of means in question.\n\nLet us denote <math>Q_{(p,\\nu,\\gamma_{(p,\\alpha)})}</math> as the <math>\\gamma_{\\alpha}</math> quantile of the [[studentized range distribution]], with p observations, and <math>\\nu</math> degrees of freedom for the second sample (see studentized range for more information).\nLet us denote <math>r_{(p,\\nu,\\alpha)}</math> as the standardized critical value, given by the rule:\n\nIf p=2 <br />\n<math>r_{(p,\\nu,\\alpha)}= Q_{(p,\\nu,\\gamma_{(p,\\alpha)})}</math><br />\nElse<br />\n<math>r_{(p,\\nu,\\alpha)}= max(   Q_{(p,\\nu,\\gamma_{(p,\\alpha)})}, r_{(p-1,\\nu,\\alpha)}    )</math>\n\nThe shortest critical range,  (the actual critical value of the test) is computed as :\n<math>R_(p,\\nu,\\alpha)=\\sigma_{m} \\cdot r_{(p,\\nu,\\alpha)}</math>.\nFor <math>\\nu</math>->∞, a tabulation exists for an exact value of Q (see link).\nA word of caution is needed here: notations for Q and R are not the same throughout literature, where Q is sometimes denoted as the shortest significant interval, and R as the significant [[quantile]] for [[studentized range distribution]] (Duncan's 1955 paper uses both notations in different parts).\n\n== Numeric example ==\nLet us look at the example of 5 treatment means:\n<br />\n{| class=\"wikitable\"\n|-\n! Treatments !! T1 !! T2 !! T3 !! T4 !! T5\n|-\n| Treatment Means || 9.8 || 15.4 || 17.6 || 21.6 || 10.8\n|-\n| Rank || 5 || 3 || 2 || 1 || 4\n|}\n<br />\nWith a standard error of <math>s_m =1.796</math>, and <math>\\nu=20</math> (degrees of freedom for estimating the standard error).\nUsing a known tabulation for Q, one reaches the values of <math>r_{(p,\\nu,\\alpha)}</math>:\n\n<math>r_{(2,20,0.05)}=2.95</math><br />\n<math>r_{(3,20,0.05)}=3.10</math><br />\n<math>r_{(4,20,0.05)}=3.18</math><br />\n<math>r_{(5,20,0.05)}=3.25</math>\n\nNow we may obtain the values of the shortest significant range, by the formula:<br />\n<math>R_{(p,\\nu,\\alpha)}=\\sigma_{m}* r_{(p,\\nu,\\alpha)}</math>\n\nReaching:\n\n<math>R_{(2,20,0.05)}=3.75</math><br />\n<math>R_{(3,20,0.05)}=3.94</math><br />\n<math>R_{(4,20,0.05)}=4.04</math><br />\n<math>R_{(5,20,0.05)}=4.13</math>\n\nThen, the observed differences between means are tested, beginning with the largest versus smallest, which would be compared with the least significant range <math> R_{(5,20,0.05)}=4.13. </math> Next, the difference of the largest and the second smallest is computed and compared with the least significant difference <math>R_{(4,20,0.05)}=4.04</math>.<br />\n<br />\nIf an observed difference is greater than the corresponding shortest significant range, then we conclude that the pair of means in question is significantly different.\nIf an observed difference is smaller than the corresponding shortest significant range, all differences sharing the same upper mean are considered insignificant, in order to prevent contradictions (differences  sharing the same upper mean are shorter by construction).<br />\n <br />\nFor our case, the comparison will yield:\n\n<math>4 vs. 1: 21.6-9.8=11.8 >4.13 (R_5)</math><br />\n<math>4 vs. 5: 21.6-10.8=10.8>4.04 (R_4)</math><br />\n<math>4 vs. 2: 21.6-15.4=6.2>3.94 (R_3)</math><br />\n<math>4 vs. 3: 21.6-17.6=4.0>3.75 (R_2)</math><br />\n<math>3 vs. 1:17.6-9.8=7.8>4.04 (R_4)</math><br />\n<math>3 vs. 5:17.6-10.8=6.8>3.94 (R_3)</math><br />\n<math>3 vs. 2: 17.6-15.4=2.2<3.75 (R_2)</math><br />\n<math>2 vs. 1:15.4-9.8=5.6>3.94 (R_3)</math><br />\n<math>2 vs. 5:15.4-10.8=4.6>3.75 (R_2)</math><br />\n<math>5 vs.1 :10.8-9.8=1.0<3.75 (R_2) </math><br />\n<br />\nWe see that there are significant differences between all pairs of treatments except (T3,T2) and (T5,T1). A graph underlining those means that are not significantly different is shown below:\n<br />\n<u>T1       T5</u>                                             <u> T2              T3 </u>                                        T4\n\n== Protection and significance levels based on degrees of freedom ==\n\nThe new multiple range test proposed by Duncan makes use of special protection levels based upon [[degrees of freedom]]. Let   <math> \\gamma_{2,\\alpha} = {1-\\alpha} </math> be the protection level for testing the significance of a difference between two means; that is, the [[probability]] that a significant difference between two means will not be found if the population means are equal. Duncan reasons that one has p-1 [[degrees of freedom]] for testing p ranked mean, and hence one may conduct p-1 independent tests, each with protection level  <math> \\gamma_{2,\\alpha} = {1-\\alpha} </math> . Hence, the joint protection level is:\n\n<math> \\gamma_{p,\\alpha} = \\gamma_{2,\\alpha}^{p-1} = (1-\\alpha)^{p-1}</math> where <math> \\alpha _p = 1-\\gamma_p</math>\n\nthat is, the probability that one finds no significant differences in making p-1 independent tests, each at protection level <math> \\gamma_{2,\\alpha} = {1-\\alpha} </math>, is  <math>  \\gamma_{2,\\alpha}^{p-1} </math>, under the hypothesis that all p population means are equal.\nIn general: the difference between any two means in a set of n means is significant provided the range of each and every subset, which contains the given means, is significant according to an <math>\\alpha_p</math> –level range test, where p is the number of means in the subset concerned.\n\nFor <math> \\alpha = 0.05 </math> , the protection level can be tabulated for various value of r as follows:\n\n{| class=\"wikitable\"\n|-\n!  !! Protection level <math> : \\gamma_{p,\\alpha} </math> !! probability of falsely rejecting <math>H_0 : \\alpha_p</math>\n|-\n| p=2 || 0.95 || 0.05\n|-\n| p=3|| 0.903|| 0.097\n|-\n| p=4 || 0.857 || 0.143\n|-\n| p=5 || 0.815 || 0.185\n|-\n| p=6 || 0.774 || 0.226\n|-\n| p=7 || 0.735 || 0.265\n|-\n\n|}\n\nNote that although this procedure makes use of the [[Studentized range]], his error rate is neither on an experiment-wise basis (as with Tukey's) nor on a per- comparisons basis. Duncan's multiple range test does not control the [[familywise error rate]]. See Criticism Section for further details.\n\n== Duncan Bayesian multiple comparison procedure ==\nDuncan (1965) also gave the first Bayesian multiple comparison procedure, for the [[pairwise comparisons]] among the means in a one-way layout.\nThis multiple comparison procedure is different for the one discussed above.\n\nDuncan's Bayesian MCP discusses the differences between ordered group means, where the statistics in question are [[pairwise comparison]] (no equivalent is defined for the property of a subset having 'significantly different' property).\n\nDuncan modeled the consequences of two or more means being equal using additive [[loss functions]] within and across the [[pairwise comparisons]]. If one assumes the same [[loss function]] across the pairwise comparisons, one needs to specify only one constant K, and this indicates the relative seriousness of type I to type II errors in each pairwise comparison.\n\nA study, which performed by Juliet Popper Shaffer (1998), has shown that the method proposed by Duncan, modified to provide weak control of FWE and using an empirical estimate of the [[variance]] of the population means, has good properties both from the Bayesian point of view, as a minimum- risk method, and from the frequentist point of view, with good average power.\n\nIn addition, results indicate considerable similarity in both risk and average [[Statistical power|power]] between Duncan's modified procedure and the [[Yoav Benjamini|Benjamini]] and [[Yosi Hochberg|Hochberg]] (1995) [[False discovery rate]] -controlling procedure, with the same weak familywise error control.\n\n== Criticism ==\nDuncan's test has been criticised as being too liberal by many statisticians including [[Henry Scheffé]], and [[John W. Tukey]].\nDuncan argued that a more liberal procedure was appropriate because in real world practice the global null hypothesis H<sub>0</sub> = \"All means are equal\" is often false and thus traditional statisticians overprotect a probably false null hypothesis against type I errors. According to Duncan, one should adjust the protection levels for different p-mean comparisons according to the problem discussed. The example discussed by Duncan in his 1955 paper is of a comparison of many means (i.e. 100),when  one is interested only in two-mean and three-mean comparisons, and general p-mean comparisons (deciding whether there is some difference between p-means) are of no special interest (if p is 15 or more for example).\nDuncan's multiple range test  is very “liberal” in terms of Type I errors. The following example will illustrate why:\n\nLet us assume one is truly interested, as Duncan suggested, only with the correct ranking of subsets of size 4 or below. Let us also assume that one performs the simple pairwise comparison with a protection level <math> \\gamma_2 =0.95 </math>. Given an overall set of 100 means, let us look at the null hypotheses of the test:\n\nThere are <math> 100\\choose2 </math>  null hypotheses for the correct ranking of each 2 means. The significance level of each hypothesis is <math>  1-0.95 = 0.05 </math>\n\nThere are <math> 100\\choose3</math>   null hypotheses for the correct ranking of each 3 means. The significance level of each hypothesis is <math> 1- (0.95)^2=0.097 </math>\n\nThere are <math> 100\\choose4</math>   null hypotheses for the correct ranking of each 4 means. The significance level of each hypothesis is <math> 1- (0.95)^3=0.143 </math>\n\nAs we can see, the test has two main problems, regarding the type I errors:\n# Duncan’s tests is based on the [[Newman–Keuls procedure]], which does not protect the [[familywise error rate]] (though protecting the per-comparison alpha level)\n# Duncan’s test intentionally raises the alpha levels ([[Type I error rate]]) in each step of the [[Newman–Keuls procedure]] (significance levels of <math> \\alpha_p\\geq\\alpha </math>).\n\nTherefore, it is advised not to use the procedure discussed.\n\nDuncan later developed the Duncan–Waller test which is based on Bayesian principles. It uses the obtained value of F to estimate the prior probability of the [[null hypothesis]] being true.\n\n=== Different approaches to the problem ===\nIf one still wishes to address the problem of finding similar subsets of group means, other solutions are found in literature.\n\n[[Tukey's range test]] is commonly used to compare pairs of means, this procedure controls the [[familywise error rate]] in the strong sense.\n\nAnother solution is to perform [[Student's t-test]] of all pairs of means, and then to use FDR Controlling procedure (to control the expected proportion of incorrectly rejected [[null hypotheses]]).\n\nOther possible solutions, which do not include hypothesis testing, but result in a partition of subsets include [[Cluster analysis|Clustering]] & [[Hierarchical Clustering]]. These solutions differ from the approach presented in this method:\n* By being distance/density based, and not distribution based.\n* Needing a larger group of means, in order to produce significant results or working with the entire data set.\n\n== References ==\n{{Reflist}}\n* {{cite journal |last=Duncan |first=D. B. |title=Multiple range and multiple F tests |journal=[[Biometrics (journal)|Biometrics]] |volume=11 |issue= |pages=1–42 |year=1955 |doi=10.2307/3001478}}\n* {{cite journal |first=Juliet Popper |last=Shaffer |title=A semi-Bayesian study of Duncan's Bayesian multiple comparison procedure |journal=Journal of Statistical Planning and Inference |volume=82 |year=1999 |issue=1–2 |pages=197–213 |doi=10.1016/S0378-3758(99)00042-7 }}\n* {{cite journal |first=Donald A. |last=Berry |first2=Yosef |last2=Hochberg |title=Bayesian perspectives on multiple comparisons |journal=Journal of Statistical Planning and Inference |volume=82 |year=1999 |issue=1–2 |pages=215–227 |doi=10.1016/S0378-3758(99)00044-0 }}\n* {{cite paper |first=Rajender |last=Parsad |title=Multiple comparison Procedures |publisher=I.A.S.R.I, Library Avenue, New Delhi 110012 }}\n\n;Tables for the Use of Range and Studentized Range in Tests of Hypotheses\n* H. Leon Harter, Champaigne, IL; N. Balakrishnan, McMaster University, Hamilton, Ontario, Canada; Hardback  - Published Oct 27, 1997\n\n== External links ==\n* [http://www.ime.usp.br/~abe/lista/pdfepXJ7Z5yxl.pdf Critical values for Duncan's multiple range tests]\n\n[[Category:Statistical tests]]\n[[Category:Multiple comparisons]]"
    },
    {
      "title": "Dunnett's test",
      "url": "https://en.wikipedia.org/wiki/Dunnett%27s_test",
      "text": "In [[statistics]], '''Dunnett's test''' is a [[multiple comparisons|multiple comparison]] procedure<ref>Upton G. & Cook I. (2006.) ''A Dictionary of Statistics'', 2e, Oxford University Press, Oxford, United Kingdom.</ref> developed by Canadian statistician [[Charles Dunnett]]<ref>{{cite book|url=https://books.google.com/books?id=4m7jFmUwcoIC&pg=PA186&lpg=PA186&dq=dunnett%27s+test+developed+by&source=bl&ots=6uQpWlwo5h&sig=nL722TTBV-WpqNbG7P1hXkAFgDQ&hl=en&sa=X&ei=bTIMUKK_M-6TiAfk2OG0DQ&ved=0CGYQ6AEwBg |title=Statistics II for Dummies|first= Deborah|last= Rumsey |authorlink= Deborah J. Rumsey |date=2009-08-19 |accessdate=2012-08-22}}</ref> to compare each of a number of treatments with a single control.<ref>Everett B. S. & Shrondal A. (2010.) ''The Cambridge Dictionary of Statistics'', 4e, Cambridge University Press, Cambridge, United Kingdom.</ref><ref>{{cite web|url=http://www.uky.edu/ComputingCenter/SSTARS/www/documentation/MultipleComparisons_3.htm |title=Statistical Software &#124; University of Kentucky Information Technology |publisher=Uky.edu |date= |accessdate=2012-08-22}}</ref> Multiple comparisons to a control are also referred to as many-to-one comparisons.\n\n==History==\nDunnett's test was developed in 1955;<ref name=\"original_article\">{{cite journal|author=Dunnett C. W.|year=1955|title=A multiple comparison procedure for comparing several treatments with a control|journal=Journal of the American Statistical Association|volume=50|page=1096{{Ndash}}1121|url=https://amstat.tandfonline.com/doi/abs/10.1080/01621459.1955.10501294?journalCode=uasa20#.XN7PxVJKjIU|doi=10.1080/01621459.1955.10501294}}</ref> an updated table of critical values was published in 1964.<ref name=\"Dunnett C. W. 1964\">Dunnett C. W. (1964.) \"New tables for multiple comparisons with a control\", ''Biometrics'', '''20''':482{{Ndash}}491.</ref>\n\n===Multiple Comparisons Problem===\n{{main|Multiple comparisons problem|l1 = Multiple comparisons problem}}\nThe multiple comparisons, multiplicity or multiple testing problem occurs when one considers a set of statistical inferences simultaneously or infers a subset of parameters selected based on the observed values. The major issue in any discussion of multiple-comparison procedures is the question of the probability of Type I errors. Most differences among alternative techniques result from different approaches to the question of how to control these errors. The problem is in part technical; but it is really much more a subjective question of how you want to define the error rate and how large you are willing to let the maximum possible error rate be.<ref name=\"howell\">David C. Howell, \"Statistical Methods for Psychology\",8th ed.</ref> \nDunnett's test are well known and widely used in multiple comparison procedure for simultaneously comparing, by interval estimation or hypothesis testing, all active treatments with a control when sampling from a distribution where the normality assumption is reasonable.\nDunnett's test is designed to hold the [[familywise error rate]] at or below <math>\\alpha</math> when performing multiple comparisons of treatment group with control.<ref name=\"howell\" />\n\n===Uses of Dunnett’s test===\nThe original work on Multiple Comparisons problem was made by [[John Tukey|Tukey]] and [[Henry Scheffé|Scheffé]]. Their method was a general one, which considered all kinds of pairwise comparisons.<ref name=\"howell\"/> Tukey's and Scheffé's methods allow any number of comparisons among a set of sample means. On the other hand, Dunnett's test only compares one group with the others, addressing a special case of multiple comparisons problem — pairwise comparisons of multiple treatment groups with a single control group. In the general case, where we compare each of the pairs, we make <math>k(k-1)\\big/2</math> comparisons (where k is the number of groups), but in the treatment vs. controls case we will make only <math>(k-1)</math> comparisons. If in the case of treatment and control groups we were to use the more general Tukey's and Scheffé's methods, they can result in unnecessarily wide confidence intervals. Dunnett's test takes into consideration the special structure of comparing treatment against control, yielding in narrower confidence intervals.<ref name=\"original_article\"/><br>\nIt is very common to use Dunnett's test in medical experiments, for example comparing blood count measurements on three groups of animals, one of which served as a control while the other two were treated with two different drugs. Another common use of this method is among agronomists: agronomists may want to study the effect of certain chemicals added to the soil on crop yield, so they will leave some plots untreated (control plots) and compare them to the plots where chemicals were added to the soil (treatment plots).\n\n==Formal description of Dunnett's test==\nDunnett's test is performed by computing a [[Student's t-statistic]] for each experimental, or treatment, group where the statistic compares the treatment group to a single control group.<ref>[http://davidmlane.com/hyperstat/B112114.html Dunnett's test], HyperStat Online: An Introductory Statistics Textbook and Online Tutorial for Help in Statistics Courses</ref><ref>[http://www.anselm.edu/homepage/jpitocch/anova/multcomp.html Mechanics of Different Tests - Biostatistics BI 345], [[Saint Anselm College]]</ref> Since each comparison has the same control in common, the procedure incorporates the dependencies between these comparisons. In particular, the t-statistics are all derived from the same estimate of the error variance which is obtained by pooling the sums of squares for error across all (treatment and control) groups. The formal test statistic for Dunnett's test is either the largest in absolute value of these t-statistics (if a two-tailed test is required), or the most negative or most positive of the t-statistics (if a one-tailed test is required).\n\nIn Dunnett's test we can use a common table of critical values, but more flexible options are nowadays readily available in many statistics packages such as [[R (programming language)|R]]. The critical values for any given percentage point depend on: whether a one- or- two-tailed test is performed; the number of groups being compared; the overall number of trials.\n\n===Assumptions===\nThe analysis considers the case where the results of the experiment are numerical, and the experiment is performed to compare p treatments with a control group. The results can be summarized as a set of <math>(p+1)</math> calculated means of the sets of observations, <math>(\\bar{X_{0}},...,\\bar{X_{p}})</math>, while <math>(\\bar{X_{1}},...,\\bar{X_{p}})</math> are referring to the treatment and <math>\\bar{X_{0}}</math> is referring to the control set of observations, and <math>s</math> is an independent estimate of the common standard deviation of all <math>p+1</math> sets of observations. All <math>\\bar{X_{i}}</math> of the <math>p+1</math> sets of observations are assumed to be independently and normally distributed with a common [[variance]] <math>\\sigma^2</math> and means <math>\\mu_{i}</math>. There is also an assumption that there is an available estimate <math>s^2</math> for <math>\\sigma^2</math>.\n\n===Calculation===\nDunnett's test's calculation is a procedure that is based on calculating confidence statements about the true or the expected values of the <math>p</math> differences <math>\\bar{X_{i}}-\\bar{X_{0}}</math>, thus the differences between treatment groups' mean and control group's mean. This procedure ensures that the probability of all <math>p</math> statements <math>\\bar{X_{i}}-\\bar{X_{0}}</math> being simultaneously correct is equal to a specified value,<math>P</math>. When calculating one sided upper (or lower) [[Confidence interval]] for the true value of the difference between the mean of the treatment and the [[control group]], <math>P</math> constitutes the probability that this actual value will be less than the upper (or greater than the lower) limit of that interval. When calculating two-sided [[confidence interval]], <math>P</math> constitutes the probability that the true value will be between the upper and the lower limits.\n\nFirst, we will denote the available N observations by <math>X_{ij}</math> when <math>i=1...p</math> and <math>j=1...N_{i}</math> and estimate the common [[variance]] by, for example:\n<math> s^2= \\frac{\\sum_{i=0}^{p}\\sum_{j=1}^{N_{i}} (X_{ij}-\\bar{X_{i}})^2}{n}</math> \nwhen <math>\\bar{X_{i}}</math> is the mean of group <math>i</math> and <math>N_{i}</math> is the number of observations in group <math>i</math>, and <math>n=\\sum_{i=0}^{p} N_{i}-(p+1)</math> degrees of freedom. As mentioned before, we would like to obtain separate confidence limits for each of the differences <math>m_{i}-m_{0}, (i=1...p)</math> such that the probability that all <math>p</math> confidence intervals will contain the corresponding <math>m_{i}-m_{0}</math> is equal to <math>P</math>.\n\nWe will consider the general case where there are <math>p</math> treatment groups and one control group. We will write:\n\n<math>z_{i}= \n\\cfrac{\\bar{X_{i}}-\\bar{X_{0}}-(m_{i}-m_{0})}{\\sqrt{\n\\cfrac{1}{N_{i}}+\n\\cfrac{1}{N_{0}}}}</math>\n\n<math>D_{i}= \n\\cfrac{\\bar{X_{i}}-\\bar{X_{0}}-(m_{i}-m_{0})}{s\\sqrt{\n\\cfrac{1}{N_{i}}+\n\\cfrac{1}{N_{0}}}}</math>\n\nwe will also write: <math>D_{i}=\\frac{z_{i}}{s}</math>, which follows the [[Student's t-statistic]] distribution with n [[degrees of freedom]]. The lower confidence limits with joint confidence coefficient <math>P</math> for the <math>p</math> treatment effects <math>m_{i}-m_{0}, (i=1...p)</math> will be given by:\n\n<math>\\bar{X_{i}}-\\bar{X_{0}}-d_{i}'s\\sqrt{\n\\frac{1}{N_{i}}+\\frac{1}{N_{0}}} , i=1...p</math>\n\nand the <math>p</math> constants <math>d_{i}'</math> are chosen so that <math>Prob(t_{1}<d_{1}',...,t_{p}<d_{p}') = P</math>.\nSimilarly, the upper limits will be given by:\n\n<math>\\bar{X_{i}}-\\bar{X_{0}}+d_{i}'s\\sqrt{\n\\frac{1}{N_{i}}+\\frac{1}{N_{0}}} , i=1...p</math>\n\nFor bounding <math>m_{i}-m_{0}</math> in both directions, the following interval might be taken:\n\n<math>\\bar{X_{i}}-\\bar{X_{0}}\\pm d_{i}'s\\sqrt{\n\\frac{1}{N_{i}}+\\frac{1}{N_{0}}} , i=1...p</math>\n\nwhen <math>d_{i}''</math> are chosen to satisfy <math>Prob(|t_{1}|<d_{1}',...,|t_{p}|<d_{p}') = P</math>.\nThe solution to those particular values of <math>d_{i}''</math> for two sided test and <math>d_{i}'</math> for one sided test is given in the tables.<ref name=\"original_article\"/> An updated table of critical values was published in 1964.<ref name=\"Dunnett C. W. 1964\"/>\n\n==Examples==\n\n===Breaking strength of fabric<ref name=\"original_article\"/>===\nThe following example was adapted from one given by Villars[6]. The data represent measurements on the breaking strength of fabric treated by three different chemical process compared with a standard method of manufacture.\n\n{| class=\"wikitable\"\n|+breaking strength(lbs.)\n|-\n! !! standard !! process 1 !! process 2 !! process 3\n|-\n| || 55|| 55 || 55 || 50\n|-\n| || 47 ||64 || 49 || 44\n|-\n| || 48 || 64 || 52 || 41\n|-\n!Means\n| 50 || 61 || 52 || 45\n|-\n!Variance\n| 19 || 27 || 9 || 21\n|}\n\nHere, p=3 and N=3. The average variance is <math>s^2=19</math>,\nwhich is an estimate of the common variance of the four sets with (p+1)(N-1)=8 \ndegrees of freedom.\nThis can be calculated as follows:\n\n<math> \\frac {55^2+47^2+48^2+55^2+...+41^2-3(50^2+61^2+52^2+45^2)}{8}=\\frac{152}{8}=19</math>.\n\nThe standard deviation is <math> s=\\sqrt{19}=4.36 </math> and the estimated standard error of a difference between two means is \n<math> s\\sqrt{\\frac{2}{N}}=4.36\\sqrt{\\frac{2}{N}}=3.56 </math>.\n\nThe quantity which must be added to and/or subtracted from the observed differences between the means to give their confidence limits has been called by Tukey an \"allowance\" and is given by <math>A=t s\\sqrt{\\frac{2}{N}}</math>,\nwhere t is obtained from Dunnett's Table 1 if one side limits are desired or from\nDunnett's Table 2 if two-sided limits are wanted.\nFor p=3 and d.f.=8, t=2.42 for one side limits and t=2.88 for two-sided limits for p=95%. Analogous values of t can be determined from the tables if p=99% confidence is required.\nFor one-sided limits, the allowance is A=(2.42)(3.56)=9 and the experimenter can conclude that:\n* The breaking strength using process 1 exceeds the standard by at least <math>61-50-9=2lbs.</math>\n* The breaking strength using process 2 exceeds the standard by at least <math>52-50-9=-7lbs</math>.\n* The breaking strength using process 3 exceeds the standard by at least <math>45-50-9=-14lbs</math>.\nThe joint statement consisting of the above three conclusions has a confidence coefficient of 95%, i.e., in the long run, 95% of such joint statements will actually be correct. Upper limits for the three differences could be obtained in an analogous manner.\nFor two-sided limits, the allowance is A=(2.94)(3.56)=11 and the experimenter can conclude that:\n* The breaking strength using process 1 exceeds the standard by an amount between\n<math>61-50-11=0lbs.</math> and <math>61-50+11=22lbs.</math>\n* The breaking strength using process 2 exceeds the standard by an amount between\n<math> 52-50-11=-9lbs</math> and <math> 52-50+11=13lbs</math>.\n* The breaking strength using process 3 exceeds the standard by an amount between\n<math> 45-50-11=-16lbs</math> and <math> 45-50+11=6lbs</math>.\nThe joint confidence coefficient for these three statement is greater than 95%.\n(Due to an approximation made in computing Tables 2a and 2b, the tabulated values of t are somewhat larger than necessary so that the actual p's attained are slightly greater than 95 and 99%.No such approximation was made in computing Tables 1a and 1b).\n\n==References==\n<references/>\n\n[[Category:Statistical tests]]\n[[Category:Multiple comparisons]]"
    },
    {
      "title": "Extensions of Fisher's method",
      "url": "https://en.wikipedia.org/wiki/Extensions_of_Fisher%27s_method",
      "text": "In [[statistics]], '''extensions of Fisher's method''' are a group of approaches that allow approximately valid [[statistical inference]]s to be made when the assumptions required for the direct application of [[Fisher's method]] are not valid. Fisher's method is a way of combining the information in the [[p-values]] from different [[statistical hypothesis testing|statistical tests]] so as to form a single overall test: this method requires that the individual test statistics (or, more immediately, their resulting p-values) should be [[statistical independence|statistically independent]].\n\n==Dependent statistics==\n\nA principle limitation of [[Fisher's method]] is its exclusive design to combine independent p-values, which renders it an unreliable technique to combine dependent p-values. To overcome this limitation, a number of methods were developed to extend its utility.\n\n===Known covariance===\n====Brown's method====\n\nFisher's method showed that the log-sum of ''k'' independent [[p-value]]s follow a [[chi-squared distribution|''χ''<sup>2</sup>-distribution]] with 2''k'' degrees of freedom:<ref>{{cite journal | last1 = Brown  | first1 = M. | title =  A method for combining non-independent, one-sided tests of significance | journal = Biometrics | volume = 31 | pages = 987–992 | year = 1975 |doi=10.2307/2529826 }}</ref><ref name=\":2\">{{cite journal | last1 = Kost  | first1 = J. | last2 = McDermott | first2 = M. | title =  Combining dependent P-values | journal = Statistics & Probability Letters | volume = 60 | pages = 183–190 | year = 2002 |doi=10.1016/S0167-7152(02)00310-3 }}</ref>\n\n: <math>X = -2\\sum_{i=1}^k \\log_e(p_i) \\sim \\chi^2(2k) .</math>\n\nIn the case that these p-values are not independent, Brown proposed the idea of approximating ''X'' using a scaled ''χ''<sup>2</sup>-distribution, ''cχ''<sup>2</sup>(''k’''), with ''k’'' degrees of freedom.\n\nThe mean and variance of this scaled ''χ''<sup>2</sup> variable are:\n\n: <math>\\operatorname{E}[c\\chi^2(k')] = ck' ,</math>\n: <math>\\operatorname{Var}[c\\chi^2(k')] = 2c^2k' .</math>\n\nwhere <math>c=\\operatorname{Var}(X)/(2\\operatorname{E}[X])</math> and <math>k'=2(\\operatorname{E}[X])^2/\\operatorname{Var}(X)</math>. This approximation is shown to be accurate up to two moments.\n\n===Unknown covariance===\n\n==== Harmonic mean ''p-''value ====\n{{Main|harmonic mean p-value}}\nThe [[Harmonic mean p-value|harmonic mean ''p''-value]] offers an alternative to Fisher's method for combining ''p''-values when the dependency structure is unknown but the tests cannot be assumed to be independent.<ref name=\":0\">{{cite journal|vauthors=Good, I J|date=1958|title=Significance tests in parallel and in series|journal=Journal of the American Statistical Association|volume=53|issue=284|pages=799–813|doi=10.1080/01621459.1958.10501480|jstor=2281953}}</ref><ref name=\":1\">{{cite journal|vauthors=Wilson, D J|date=2019|title=The harmonic mean ''p''-value for combining dependent tests|journal=Proceedings of the National Academy of Sciences USA|volume=116|issue=4|pages=1195–1200|doi=10.1073/pnas.1814092116}}</ref>\n\n====Kost's method: [[Student's t-distribution|''t'' approximation]] ====\n\nThis method requires the test statistics' covariance structure to be known up to a scalar multiplicative constant. See reference.<ref name=\":2\" />\n\n==References==\n{{Reflist}}\n\n[[Category:Multiple comparisons]]\n\n\n{{Statistics-stub}}"
    },
    {
      "title": "False coverage rate",
      "url": "https://en.wikipedia.org/wiki/False_coverage_rate",
      "text": "In [[statistics]], a '''false coverage rate (FCR)''' is the average rate of false [[coverage (statistics)|coverage]], i.e. not covering the true parameters, among the selected intervals.\n\nThe FCR gives a simultaneous coverage at a (1&nbsp;&minus;&nbsp;''α'')&times;100% level for all of the parameters considered in the problem. The FCR has a strong connection to the [[false discovery rate]] (FDR). Both methods address the [[problem of multiple comparisons]], FCR from [[confidence interval]]s (CIs) and FDR from P-value's point of view.\n\nFCR was needed because of dangers caused by selective inference. Researchers and scientists tend to report or highlight only the portion of data that is considered significant without clearly indicating the various hypothesis that were considered. It is therefore necessary to understand how the data is falsely covered. There are many FCR procedures which can be used depending on the length of the CI – Bonferroni-selected–Bonferroni-adjusted, {{citation needed|date=August 2012}} Adjusted BH-Selected CIs (Benjamini and Yekutieli 2005<ref name=BenjaminiYekutieli2005> {{cite journal|last1= Benjamini|first1=Yoav|last2=Yekutieli|first2=Daniel|journal=Journal of the American Statistical Association|date=March 2005|volume=100|issue=469|pages=71–93|title= False Discovery Rate–Adjusted Multiple Confidence Intervals for Selected Parameters|url=http://www.math.tau.ac.il/~ybenja/MyPapers/benjamini_yekutieli_JASA2005.pdf|format=pdf|doi= 10.1198/016214504000001907}}</ref>). The incentive of choosing one procedure over another is to ensure that the CI is as narrow as possible and to keep the FCR. For [[microarray]] experiments and other modern applications, there are a huge number of [[Statistical parameter|parameters]], often tens of thousands or more and it is very important to choose the most powerful procedure.\n\nThe FCR was first introduced by [[Daniel Yekutieli]] in his PhD thesis in 2001<ref>[http://primage.tau.ac.il/libraries/theses/exeng/free/2979859.pdf Theoretical Results Needed for Applying the False Discovery Rate in Statistical Problems]. April, 2001 (Section 3.2, Page 51)</ref>.\n\n \n\n==Definitions==\n\nNot keeping the FCR means <math>\\text{FCR}>q</math> when <math>q= \\frac{V}{R} = \\frac{\\alpha m_0}{R} </math>, where <math>m_0 </math> is the number of true null hypotheses, <math>R </math> is the number of rejected hypothesis, <math>V</math> is the number of false positives, and <math>\\alpha</math> is the significance level. Intervals with simultaneous coverage probability <math>1-q </math> can control the FCR to be bounded by <math>q</math>.\n\n===Classification of multiple hypothesis tests===\n{{Main|Classification of multiple hypothesis tests}}\n<!-- [[Template:Classification of multiple hypothesis tests]]: -->\n{{Classification of multiple hypothesis tests}}\n\n==The problems addressed by FCR==\n\n===Selection===\n[[Selection bias|Selection]] causes reduced average coverage. Selection can be presented as conditioning on an event defined by the data and may affect the coverage probability of a CI for a single [[Statistical parameter|parameter]]. Equivalently, the problem of selection changes the basic sense of [[P-values]]. FCR procedures consider that the goal of conditional coverage following any selection rule for any set of (unknown) values for the parameters is impossible to achieve. A weaker property when it comes to selective CIs is possible and will avoid false coverage statements. FCR is a measure of interval coverage following selection. Therefore, even though a 1&nbsp;−&nbsp;''α'' CI does not offer selective ([[Conditional probability|conditional]]) coverage, the probability of constructing a no covering CI is at most ''α'', where\n\n:<math> \\Pr[\\theta \\not\\in \\mathrm{CI},\\  \\text{CI constructed}] \\leq \\Pr[\\theta \\not\\in \\mathrm{CI}] \\leq \\alpha</math>\n\n===Selection and multiplicity===\nWhen facing both multiplicity (inference about multiple parameters) and [[Selection bias|selection]], not only is the expected proportion of coverage over selected parameters at 1−α not equivalent to the expected proportion of no coverage at α, but also the latter can no longer be ensured by constructing marginal CIs for each selected parameter. FCR procedures solve this by taking the expected proportion of parameters not covered by their CIs among the selected parameters, where the proportion is 0 if no parameter is selected. This false coverage-statement rate (FCR) is a property of any procedure that is defined by the way in which parameters are selected and the way in which the multiple intervals are constructed.\n\n==Controlling procedures==\n{{broader|Multiple testing correction}}\n{{see also|False discovery rate#Controlling procedures|Familywise error rate#Controlling procedures}}\n\n===Bonferroni procedure (Bonferroni-selected–Bonferroni-adjusted) for simultaneous CI===\nSimultaneous CIs with Bonferroni procedure when we have m parameters, each marginal CI constructed at the 1 − α/m level. Without selection, these CIs offer simultaneous coverage, in the sense that the probability that all CIs cover their respective parameters is at least 1 − α. unfortunately, even such a strong property does not ensure the conditional confidence property following selection.\n\n===FCR for Bonferroni-selected–Bonferroni-adjusted simultaneous CI===\nThe Bonferroni–Bonferroni procedure cannot offer conditional coverage, however it does control the FCR at <α In fact it does so too well, in the sense that the FCR is much too close to 0 for large values of θ Intervals selection is based on Bonferroni testing, and Bonferroni CIs are then constructed. The FCR is estimated as, the proportion of intervals failing to cover their respective parameters among the constructed CIs is calculated (setting the proportion to 0 when none are selected). Where selection is based on unadjusted individual testing and unadjusted CIs are constructed.\n\n===FCR-adjusted BH-selected CIs===\nIn BH procedure for FDR after sorting the ''p'' values ''P''(1) ≤&nbsp;• • •&nbsp;≤ ''P''(''m'') and calculating ''R'' = max{ ''j'' : ''P''( ''j'') ≤ ''j'' • ''q''/''m''}, the ''R'' null hypotheses for which ''P''(''i'') ≤ ''R'' • ''q''/''m'' are rejected. If testing is done using the Bonferroni procedure, then the lower bound of the FCR may drop well below the desired level ''q'', implying that the intervals are too long. In contrast, applying the following procedure, which combines the general procedure with the FDR controlling testing in the BH procedure, also yields a lower bound for the FCR, ''q''/2 ≤ FCR. This procedure is sharp in the sense that for some configurations, the FCR approaches ''q''.\n\n1. Sort the p values used for testing the m hypotheses regarding the parameters, ''P''(1) ≤ • • • ≤''P''(''m'').\n\n2. Calculate ''R'' = max{''i'' : ''P''(''i'') ≤ ''i'' • ''q''/''m''}.\n\n3. Select the ''R'' parameters for which ''P''(''i'') ≤ ''R'' • ''q''/''m'', corresponding to the rejected hypotheses.\n\n4. Construct a 1&nbsp;&minus;&nbsp;''R'' • ''q''/''m'' CI for each parameter selected.\n\n==See also==\n*[[False positive rate]]\n*[[Post-hoc analysis]]\n\n== References ==\n'''Footnotes'''\n{{Reflist}}\n\n'''Other Sources'''\n* {{cite journal|last1=Zhao |first1=Zhigen |last2=Hwang |first2=J. T. Gene |journal=Journal of the Royal Statistical Society, Series B |title=Empirical Bayes false coverage rate controlling confidence intervals |year=2012 |url=http://astro.temple.edu/~zhaozhg/FCR_JRSS.pdf |format=pdf |doi=10.1111/j.1467-9868.2012.01033.x }}{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}\n\n[[Category:Statistical hypothesis testing]]\n[[Category:Multiple comparisons]]"
    },
    {
      "title": "False discovery rate",
      "url": "https://en.wikipedia.org/wiki/False_discovery_rate",
      "text": "The '''false discovery rate''' ('''FDR''') is a method of conceptualizing the rate of [[Type I and type II errors|type I errors]] in [[null hypothesis]] testing when conducting [[multiple comparisons]]. FDR-controlling procedures are designed to control the [[Expected value|expected]] proportion of \"discoveries\" (rejected [[null hypothesis|null hypotheses]]) that are false (incorrect rejections).<ref name=BenjaminiHochberg1995>{{cite journal | last1 = Benjamini | first1= Yoav | last2= Hochberg | first2= Yosef | year = 1995 | title = Controlling the false discovery rate: a practical and powerful approach to multiple testing | journal = [[Journal of the Royal Statistical Society, Series B]] | volume = 57 | issue = 1 | pages = 289–300 | mr = 1325392  | url = http://www.math.tau.ac.il/~ybenja/MyPapers/benjamini_hochberg1995.pdf}}</ref> FDR-controlling procedures provide less stringent control of Type I errors compared to [[familywise error rate]] (FWER) controlling procedures (such as the [[Bonferroni correction]]), which control the probability of ''at least one'' Type I error. Thus, FDR-controlling procedures have greater [[Statistical power|power]], at the cost of increased numbers of Type I errors.<ref>Shaffer J.P. (1995) Multiple hypothesis testing, Annual Review of Psychology 46:561-584, [https://archive.is/20120712123608/http://dx.doi.org/10.1146/annurev.ps.46.020195.003021 Annual Reviews]</ref>\n\n==History==\n\n===Technological motivations===\n\nThe modern widespread use of the FDR is believed to stem from, and be motivated by, the development in technologies that allowed the collection and analysis of a large number of distinct variables in several individuals (e.g., the expression level of each of 10,000 different genes in 100 different persons).<ref name=Benjamini2010 /> By the late 1980s and 1990s, the development of \"high-throughput\" sciences, such as [[genomics]], allowed for rapid data acquisition.  This, coupled with the growth in computing power, made it possible to seamlessly perform hundreds and thousands of [[Statistical hypothesis testing|statistical tests]] on a given data set. The technology of [[microarray]]s was a prototypical example, as it enabled thousands of genes to be tested simultaneously for differential expression between two biological conditions.<ref name=StoreyTibs2003>{{cite journal | last1 = Storey | first1 = John D. | last2 = Tibshirani | first2 = Robert | year = 2003 | title = Statistical significance for genome-wide studies | journal = [[Proceedings of the National Academy of Sciences]] | volume = 100 | issue = 16 | pages = 9440–9445 | doi = 10.1073/pnas.1530509100 | pmid=12883005 | pmc=170937|bibcode = 2003PNAS..100.9440S }}</ref>\n\nAs high-throughput technologies became common, technological and/or financial constraints led researchers to collect datasets with relatively small sample sizes (e.g. few individuals being tested) and large numbers of variables being measured per sample (e.g. thousands of gene expression levels). In these datasets, too few of the measured variables showed statistical significance after classic correction for multiple tests with standard [[Multiple comparisons|multiple comparison procedures]].  This created a need within many scientific communities to abandon [[Familywise error rate|FWER]] and unadjusted multiple hypothesis testing for other ways to highlight and rank in publications those variables showing marked effects across individuals or treatments that would otherwise be dismissed as non-significant after standard correction for multiple tests.  In response to this, a variety of error rates have been proposed—and become commonly used in publications—that are less conservative than [[Familywise error rate|FWER]] in flagging possibly noteworthy observations.\n\n===Literature===\n\nThe FDR concept was formally described by [[Yoav Benjamini]] and [[Yosef Hochberg]] in 1995<ref name=BenjaminiHochberg1995 /> ([[#BH procedure|BH procedure]]) as a less conservative and arguably more appropriate approach for identifying the important few from the trivial many effects tested. The FDR has been particularly influential, as it was the first alternative to the FWER to gain broad acceptance in many scientific fields (especially in the life sciences, from genetics to biochemistry, oncology and plant sciences).<ref name=Benjamini2010 /> In 2005, the Benjamini and Hochberg paper from 1995 was identified as one of the 25 most-cited statistical papers.<ref name=RYANA2005>{{Cite journal | last1 = Ryan | first1 = T. P. | last2 = Woodall | first2 = W. H. | doi = 10.1080/02664760500079373 | title = The most-cited statistical papers | journal = Journal of Applied Statistics | volume = 32 | issue = 5 | pages = 461 | year = 2005 | pmid =  | pmc = }}</ref>\n\nPrior to the 1995 introduction of the FDR concept, various precursor ideas had been considered in the statistics literature. In 1979, Holm proposed the [[Holm–Bonferroni method|Holm procedure]],<ref>{{cite journal\n |last=Holm |first=S.\n |year=1979\n |title=A simple sequentially rejective multiple test procedure\n |journal=Scandinavian Journal of Statistics\n |volume=6 |issue=2 |pages=65–70\n |mr=538597 | jstor = 4615733\n}}</ref> a stepwise algorithm for controlling the FWER that is at least as powerful as the well-known [[Bonferroni adjustment]]. This stepwise algorithm sorts the [[p-value|''p''-values]] and sequentially rejects the hypotheses starting from the smallest ''p''-values.\n\nBenjamini (2010)<ref name=Benjamini2010>{{Cite journal | last1 = Benjamini | first1 = Y. | title = Discovering the false discovery rate | doi = 10.1111/j.1467-9868.2010.00746.x | journal = Journal of the Royal Statistical Society, Series B | volume = 72 | issue = 4 | pages = 405–416 | year = 2010 | pmid =  | pmc = }}</ref> said that the false discovery rate, and the paper Benjamini and Hochberg (1995), had its origins in two papers concerned with multiple testing:\n\n* The first paper is by Schweder and Spjotvoll (1982)<ref name=SCHWEDER1982>{{Cite journal | last1 = Schweder | first1 = T. | last2 = Spjøtvoll | first2 = E. | doi = 10.1093/biomet/69.3.493 | title = Plots of P-values to evaluate many tests simultaneously | journal = Biometrika | volume = 69 | issue = 3 | pages = 493 | year = 1982 | pmid =  | pmc = }}</ref> who suggested plotting the ranked ''p''-values and assessing the number of true null hypotheses (<math>m_0</math>) via an eye-fitted line starting from the largest ''p''-values. The ''p''-values that deviate from this straight line then should correspond to the false null hypotheses. This idea was later developed into an algorithm and incorporated the estimation of <math>m_0</math> into procedures such as Bonferroni, Holm or Hochberg.<ref name=Hochberg1990>{{Cite journal | last1 = Hochberg | first1 = Y. | last2 = Benjamini | first2 = Y. | doi = 10.1002/sim.4780090710 | title = More powerful procedures for multiple significance testing | journal = Statistics in Medicine | volume = 9 | issue = 7 | pages = 811–818 | year = 1990 | pmid =  2218183| pmc = }}</ref> This idea is closely related to the graphical interpretation of the BH procedure.\n* The second paper is by Branko Soric (1989)<ref name=Soric1989>{{Cite journal\n |last=Soric |first=Branko\n |title=Statistical \"Discoveries\" and Effect-Size Estimation\n |journal=Journal of the American Statistical Association\n |volume=84 |date=June 1989 |pages=608–610\n |jstor=2289950 |issue=406 |doi=10.1080/01621459.1989.10478811\n}}</ref> which introduced the terminology of \"discovery\" in the multiple hypothesis testing context. Soric used the expected number of false discoveries divided by the number of discoveries <math>\\left (E[V]/R \\right )</math> as a warning that \"a large part of statistical discoveries may be wrong\". This led Benjamini and Hochberg to the idea that a similar error rate, rather than being merely a warning, can serve as a worthy goal to control.\n\nThe BH procedure was proven to control the FDR for independent tests in 1995 by Benjamini and Hochberg.<ref name=BenjaminiHochberg1995 />  In 1986, R. J. Simes offered the same procedure as the \"[[Simes procedure]]\", in order to control the FWER in the weak sense (under the intersection null hypothesis) when the statistics are independent.<ref name=Simes1986>{{Cite journal | last1 = Simes | first1 = R. J. | title = An improved Bonferroni procedure for multiple tests of significance | doi = 10.1093/biomet/73.3.751 | journal = Biometrika | volume = 73 | issue = 3 | pages = 751–754 | year = 1986 | pmid =  | pmc = }}</ref>\n\n==Definitions==\n\nBased on definitions below we can define {{mvar|Q}} as the proportion of false discoveries among the discoveries (rejections of the null hypothesis):\n\n:<math>Q = V/R = V/(V+S)</math>.\n\nThe '''false discovery rate''' ('''FDR''') is then simply:<ref name=BenjaminiHochberg1995 />\n\n: <math>\\mathrm{FDR} = Q_e =  \\mathrm{E}\\!\\left [Q \\right ], </math>\n\nwhere <math>Q</math> is defined to be 0 when <math> R = 0 </math>.\nOne wants to keep FDR below a threshold ''q''. To include the case when <math> R = 0 </math>, formally <math>\\mathrm{FDR} =  \\mathrm{E}\\!\\left [V/R | R>0 \\right ] \\cdot \\mathrm{P}\\!\\left (R>0 \\right) </math>.<ref name=\"BenjaminiHochberg1995\" />\n\n===Classification of multiple hypothesis tests===\n{{Main article|Classification of multiple hypothesis tests}}\n<!-- [[Template:Classification of multiple hypothesis tests]]: -->\n{{Classification of multiple hypothesis tests}}\n\n==Controlling procedures==\n{{broader|Multiple testing correction}}\n{{see also|False coverage rate#Controlling procedures|Familywise error rate#Controlling procedures}}\n\nThe settings for many procedures is such that we have <math>H_1 \\ldots H_m</math> null hypotheses tested and <math>P_1 \\ldots P_m</math> their corresponding [[p-value|''p''-values]]. We list these ''p''-values in ascending order and denote them by <math>P_{(1)} \\ldots P_{(m)}</math>.  A procedure that goes from a small ''p''-value to a large one will be called a step-up procedure.  In a similar way, in a \"step-down\" procedure we move from a large corresponding test statistic to a smaller one.\n\n===Benjamini–Hochberg procedure{{anchor|BH procedure}}===\n\nThe ''Benjamini–Hochberg procedure'' (BH step-up procedure) controls the FDR at level <math>\\alpha</math>.<ref name=BenjaminiHochberg1995 /> It works as follows:\n\n# For a given <math>\\alpha</math>, find the largest {{mvar|k}} such that <math>P_{(k)} \\leq \\frac{k}{m} \\alpha.</math>\n# Reject the null hypothesis (i.e., declare discoveries) for all <math>H_{(i)}</math> for <math>i = 1, \\ldots, k</math>.\n\nGeometrically, this corresponds to plotting <math>P_{(k)} </math> vs.  {{mvar|k}} (on the {{mvar|y}} and {{mvar|x}} axes respectively), drawing the line through the origin with slope <math>\\frac\n\\alpha{m}</math> , and declaring discoveries for all points on the left up to and including the last point that is below the line.\n\nThe BH procedure is valid when the {{mvar|m}} tests are [[Statistical independence|independent]], and also in various scenarios of dependence, but is not universally valid.<ref name=BenjaminiYekutieli2001 />  It also satisfies the inequality:\n\n: <math>E(Q) \\leq \\frac{m_0}{m}\\alpha \\leq \\alpha</math>\n\nIf an estimator of <math>m_0</math> is inserted into the BH procedure, it is no longer guaranteed to achieve FDR control at the desired level.<ref name=Benjamini2010 /> Adjustments may be needed in the estimator and several modifications have been proposed.<ref name=Storey2004>{{Cite journal | last1 = Storey | first1 = J. D. | last2 = Taylor | first2 = J. E. | last3 = Siegmund | first3 = D. | title = Strong control, conservative point estimation and simultaneous conservative consistency of false discovery rates: A unified approach | doi = 10.1111/j.1467-9868.2004.00439.x | journal = Journal of the Royal Statistical Society, Series B | volume = 66 | pages = 187–205 | year = 2004 | pmid =  | pmc = }}</ref><ref name=Benjamini2006>{{Cite journal | last1 = Benjamini | first1 = Y. | last2 = Krieger | first2 = A. M. | last3 = Yekutieli | first3 = D. | doi = 10.1093/biomet/93.3.491 | title = Adaptive linear step-up procedures that control the false discovery rate | journal = Biometrika | volume = 93 | issue = 3 | pages = 491 | year = 2006 | pmid =  | pmc = }}</ref><ref name=Gavrilov2009>{{Cite journal | last1 = Gavrilov | first1 = Y. | last2 = Benjamini | first2 = Y. | last3 = Sarkar | first3 = S. K. | doi = 10.1214/07-AOS586 | title = An adaptive step-down procedure with proven FDR control under independence | journal = The Annals of Statistics | volume = 37 | issue = 2 | pages = 619 | year = 2009 | pmid =  | pmc = | arxiv = 0903.5373 }}</ref><ref name=Blanchard2009>{{Cite journal | last1 = Blanchard | first1 = G. | last2 = Roquain | first2 = E. | doi = 10.1214/08-EJS180 | title = Two simple sufficient conditions for FDR control | journal = Electronic Journal of Statistics | volume = 2 | pages = 963–992 | year = 2008 | pmid =  | pmc = | arxiv = 0802.1406 }}</ref>\n\nNote that the mean <math>\\alpha</math> for these {{mvar|m}} tests is <math>\\frac{\\alpha(m+1)}{2m}</math>, the Mean(FDR <math>\\alpha</math>) or MFDR, <math>\\alpha</math> adjusted for {{mvar|m}} independent or positively correlated tests (see AFDR below). The MFDR expression here is for a single recomputed value of <math>\\alpha</math> and is not part of the Benjamini and Hochberg method.\n\n===Benjamini–Yekutieli procedure===\n<!-- Can someone confirm this? One would think positively correlated tests would need greater reduction in alpha than negatively correlated tests. -->\n\nThe ''Benjamini–Yekutieli'' procedure controls the false discovery rate under arbitrary dependence assumptions.<ref name=BenjaminiYekutieli2001>\n{{cite journal\n |author1=Benjamini, Yoav |author2=Yekutieli, Daniel | year = 2001\n | title = The control of the false discovery rate in multiple testing under dependency\n | journal = Annals of Statistics\n | volume = 29\n | issue = 4\n | pages = 1165–1188\n | url = http://www.math.tau.ac.il/~ybenja/MyPapers/benjamini_yekutieli_ANNSTAT2001.pdf\n | mr = 1869245\n | doi = 10.1214/aos/1013699998}}\n</ref> This refinement modifies the threshold and finds the largest {{mvar|k}} such that:\n\n:<math>P_{(k)} \\leq \\frac{k}{m \\cdot c(m)} \\alpha </math>\n\n* If the tests are independent or positively correlated (as in Benjamini–Hochberg procedure): <math>c(m)=1</math>\n* Under arbitrary dependence: <math>c(m) = \\sum _{i=1} ^m \\frac{1}{i}</math>\n\nIn the case of negative correlation, <math>c(m)</math> can be approximated by using the [[Euler–Mascheroni constant]].\n\n:<math>\\sum _{i=1} ^m \\frac{1}{i} \\approx \\ln(m) + \\gamma + \\frac{1}{2m}.</math>\n\nUsing MFDR and formulas above, an adjusted MFDR, or AFDR, is the min(mean&nbsp;<math>\\alpha</math>) for {{mvar|m}}&nbsp;dependent tests <math>= \\frac\\mathrm{MFDR}{c(m)}</math>.\n\nThe other way to address dependence is by bootstrapping and rerandomization.<ref name=StoreyTibs2003 /><ref name=YekutieliBenjamini1999>{{Cite journal |vauthors=Yekutieli D, Benjamini Y |year=1999 |title=Resampling based False Discovery Rate controlling procedure for dependent test statistics |journal=J. Statist. Planng Inf. |volume=82 |issue=1–2 |pages=171–196 |doi=10.1016/S0378-3758(99)00041-5}}</ref><ref name=VanDudoit2007>{{Cite book |author=van der Laan, M. J. |author2=Dudoit, S. |author2-link=Sandrine Dudoit|year=2007 |title=Multiple Testing Procedures with Applications to Genomics |location=New York |publisher=Springer}}</ref>\n\n==Properties==\n\n===Adaptive and scalable===\n\nUsing a multiplicity procedure that controls the FDR criterion is [[adaptive]] and [[scalable]]. Meaning that controlling the FDR can be very permissive (if the data justify it), or conservative (acting close to control of FWER for sparse problem) - all depending on the number of hypotheses tested and the level of significance.<ref name=Benjamini2010 />\n\nThe FDR criterion ''adapts'' so that the same number of false discoveries (V) will have different implications, depending on the total number of discoveries (R).  This contrasts with the [[family wise error rate]] criterion. For example, if inspecting 100 hypotheses (say, 100 genetic mutations or [[Single-nucleotide polymorphism|SNPs]] for association with some phenotype in some population):\n* If we make 4 discoveries (R), having 2 of them be false discoveries (V) is often very costly.  Whereas,\n* If we make 50 discoveries (R), having 2 of them be false discoveries (V) is often not very costly.\n\nThe FDR criterion is ''scalable'' in that the same proportion of false discoveries out of the total number of discoveries (Q), remains sensible for different number of total discoveries (R). For example:\n* If we make 100 discoveries (R), having 5 of them be false discoveries (<math>q=5\\%</math>) may not be very costly.\n* Similarly, if we make 1000 discoveries (R), having 50 of them be false discoveries (as before, <math>q=5\\%</math>) may still not be very costly.\n\n===Dependency among the test statistics===\nControlling the FDR using the linear step-up BH procedure, at level q, has several properties related to the dependency structure between the test statistics of the {{mvar|m}} null hypotheses that are being corrected for.  If the test statistics are:\n*Independent:<ref name=BenjaminiYekutieli2001 /> <math>\\mathrm{FDR} \\le \\frac{m_0}{m}q</math>\n*Independent and continuous:<ref name=BenjaminiHochberg1995 /> <math>\\mathrm{FDR} = \\frac{m_0}{m}q</math>\n*Positive dependent:<ref name=BenjaminiYekutieli2001 /> <math>\\mathrm{FDR} \\le \\frac{m_0}{m}q</math>\n*In the general case:<ref name=BenjaminiYekutieli2001 /> <math>\\mathrm{FDR} \\le \\frac{m_0}{m} q / \\left( 1 + \\frac{1}{2} + \\frac{1}{3} + \\cdots + \\frac{1}{m} \\right) \\approx \\frac{m_0}{m}q / (\\ln (m) + \\gamma + \\frac{1}{2m})</math> , where <math>\\gamma</math> is the [[Euler–Mascheroni constant]].\n\n===Proportion of true hypotheses===\nIf all of the null hypotheses are true (<math>m_0=m</math>), then controlling the FDR at level {{mvar|q}} guarantees control over the [[FWER]] (this is also called [[Familywise error rate#FWER definition|\"weak control of the FWER\"]]):  <math>\\mathrm{FWER}=P\\left( V \\ge 1 \\right) = E\\left( \\frac{V}{R} \\right) = \\mathrm{FDR} \\le q</math>, simply because the event of rejecting at least one true null hypothesis <math> \\{V \\ge 1\\} </math> is exactly the event <math> \\{V/R = 1\\} </math>, and the event <math> \\{V = 0\\} </math> is exactly the event <math> \\{V/R = 0\\} </math> (when <math> V = R = 0 </math>, <math> V/R = 0 </math> by definition).<ref name=BenjaminiHochberg1995 />  But if there are some true discoveries to be made (<math>m_0<m</math>) then {{math|FWER &ge; FDR}}.  In that case there will be room for improving detection power.  It also means that any procedure that controls the FWER will also control the FDR.\n\n==Related concepts==\n\n===Related error rates===\n\nThe discovery of the FDR was preceded and followed by many other types of error rates.  These include:\n\n* {{math|PCER}} ([[per-comparison error rate]]) is defined as: <math>\\mathrm{PCER} = E \\left[ \\frac{V}{m} \\right] </math>.  Testing individually each hypothesis at level {{mvar|&alpha;}} guarantees that <math>\\mathrm{PCER} \\le \\alpha </math> (this is testing without any correction for multiplicity)\n* {{math|FWER}} (the [[family wise error rate]]) is defined as: <math>\\mathrm{FWER} = P(V \\ge 1) </math>.  There are [[Familywise error rate#Controlling procedures|numerous procedures that control the FWER]].\n* <math>k\\text{-FWER}</math> (The tail probability of the False Discovery Proportion), suggested by Lehmann and Romano, van der Laan at al, {{citation needed|date=August 2012}} is defined as: <math>k\\text{-FWER} = P(V \\ge k) \\le q</math>.\n* <math>k\\text{-FDR}</math> (also called the ''generalized FDR'' by Sarkar in 2007<ref>Sarkar, Sanat K. \"Stepup procedures controlling generalized FWER and generalized FDR.\" The Annals of Statistics (2007): 2405-2420.</ref><ref>Sarkar, Sanat K., and Wenge Guo. \"On a generalized false discovery rate.\" The Annals of Statistics (2009): 1545-1565.</ref>) is defined as: <math>k\\text{-FDR} = E \\left( \\frac{V}{R}I_{(V>k)}  \\right) \\le q</math>.\n* <math>Q'</math> is the proportion of false discoveries among the discoveries\", suggested by Soric in 1989,<ref name=Soric1989 /> and is defined as: <math>Q' = \\frac{E[V]}{R} </math>. This is a mixture of expectations and realizations, and has the problem of control for <math>m_0=m</math>.<ref name=BenjaminiHochberg1995 />\n* <math>\\mathrm{FDR}_{-1}</math>(or Fdr) was used by Benjamini and Hochberg,<ref name=Benjamini2010 /> and later called \"Fdr\" by Efron (2008) and earlier.<ref name=Efron2008/>  It is defined as: <math>\\mathrm{FDR}_{-1} = Fdr = \\frac{E[V]}{E[R]} </math>. This error rate cannot be strictly controlled because it is 1 when <math>m = m_0</math>.\n* <math>\\mathrm{FDR}_{+1}</math> was used by Benjamini and Hochberg,<ref name=Benjamini2010 /> and later called \"pFDR\" by Storey (2002).<ref name=Storey2002>{{cite journal | last1 = Storey | first1= John D. | year = 2002 | title = A direct approach to false discovery rates | journal = [[Journal of the Royal Statistical Society, Series B]] | volume = 64 | issue = 3 | pages = 479–498 | url = http://genomics.princeton.edu/storeylab/papers/directfdr.pdf | doi = 10.1111/1467-9868.00346| citeseerx= 10.1.1.320.7131 }}</ref>  It is defined as: <math>\\mathrm{FDR}_{+1} = pFDR = E \\left[ \\left. {\\frac{V}{R}} \\right| R>0 \\right] </math>.  This error rate cannot be strictly controlled because it is 1 when <math>m = m_0</math>.\n* False exceedance rate (the tail probability of FDP), defined as:<ref name=Benj2010second>{{Cite journal | last1 = Benjamini | first1 = Y. | title = Simultaneous and selective inference: Current successes and future challenges | doi = 10.1002/bimj.200900299 | journal = Biometrical Journal | volume = 52 | issue = 6 | pages = 708–721 | year = 2010 | pmid =  21154895| pmc = }}</ref> <math>\\mathrm{P} \\left( \\frac{V}{R} > q \\right) </math>\n* <math>W\\text{-FDR}</math> (Weighted FDR).  Associated with each hypothesis i is a weight <math>w_i \\ge 0</math>, the weights capture importance/price.  The W-FDR is defined as: <math>W\\text{-FDR} = E\\left( \\frac{\\sum w_i V_i }{\\sum w_i R_i } \\right)</math>.\n* {{math|FDCR}} (False Discovery Cost Rate).  Stemming from [[statistical process control]]: associated with each hypothesis i is a cost <math>\\mathrm{c}_i</math> and with the intersection hypothesis <math>H_{00}</math> a cost <math>c_0</math>.  The motivation is that stopping a production process may incur a fixed cost.  It is defined as: <math>\\mathrm{FDCR} = E\\left( c_0 V_0 + \\frac{\\sum c_i V_i }{c_0 R_0 + \\sum c_i R_i } \\right)</math>\n* {{math|PFER}} (per-family error rate) is defined as: <math>\\mathrm{PFER} = E(V)</math>.\n* {{math|FNR}} (False non-discovery rates) by Sarkar; Genovese and Wasserman {{citation needed|date=August 2012}} is defined as: <math>\\mathrm{FNR} = E\\left( \\frac{T}{m - R} \\right) = E\\left( \\frac{m - m_0 - (R - V)}{m - R} \\right) </math>\n* <math>\\mathrm{FDR}(z)</math> is defined as: <math>\\mathrm{FDR}(z) = \\frac{p_0 F_0 (z)}{F(z)} </math>\n* <math>\\mathrm{FDR}</math> The local fdr is defined as: <math>\\mathrm{FDR} = \\frac{p_0 f_0 (z)}{f(z)} </math>\n\n===False coverage rate===\n{{main article|False coverage rate}}\n\nThe [[false coverage rate]] (FCR) is, in a sense, the FDR analog to the [[confidence interval]]. FCR indicates the average rate of false coverage, namely, not covering the true parameters, among the selected intervals. The FCR gives a simultaneous coverage at a <math>1-\\alpha</math> level for all of the parameters considered in the problem. Intervals with simultaneous coverage probability 1−q can control the FCR to be bounded by ''q''. There are many FCR procedures such as: Bonferroni-Selected–Bonferroni-Adjusted,{{citation needed|date=August 2012}} Adjusted BH-Selected CIs (Benjamini and Yekutieli (2005)),<ref name=BenjYekut2005 /> Bayes FCR (Yekutieli (2008)),{{citation needed|date=August 2012}} and other Bayes methods.<ref name=Zhao2012>{{Cite journal | last1 = Zhao | first1 = Z. | last2 = Gene Hwang | first2 = J. T. | doi = 10.1111/j.1467-9868.2012.01033.x | title = Empirical Bayes false coverage rate controlling confidence intervals | journal = Journal of the Royal Statistical Society, Series B | volume = 74 | issue = 5 | pages = 871–891 | year = 2012 | pmid =  | pmc = }}</ref>\n\n===Bayesian approaches===\nConnections have been made between the FDR and Bayesian approaches (including empirical Bayes methods),<ref name=Efron2008>{{Cite journal |author=Efron B |year=2008 |title=Microarrays, empirical Bayes and the two groups model |journal=Statistical Science |volume=23 |pages=1–22 |doi=10.1214/07-STS236|arxiv=0808.0603 }}</ref><ref name=Storey2003>{{cite journal | last1 = Storey | first1= John D. | year = 2003 | title = The positive false discovery rate: A Bayesian interpretation and the q-value | journal = [[Annals of Statistics]] | volume = 31 | issue = 6 | pages = 2013–2035 | url = http://genomics.princeton.edu/storeylab/papers/Storey_Annals_2003.pdf | doi = 10.1214/aos/1074290335}}</ref><ref name=Efron2010>{{cite book | last = Efron | first = Bradley | title = Large-Scale Inference  | publisher = [[Cambridge University Press]] | year = 2010 | isbn = 978-0-521-19249-1}}</ref> thresholding wavelets coefficients and [[model selection]],<ref name=Abramovichetel2006>{{Cite journal |vauthors=Abramovich F, Benjamini Y, Donoho D, Johnstone IM|year=2006 |title=Adapting to unknown sparsity by controlling the false discovery rate |journal=Annals of Statistics |volume=34 |pages=584–653 |bibcode=2005math......5374A|arxiv=math/0505374 |doi=10.1214/009053606000000074 |issue=2}}</ref><ref name=DonohoJin2006>{{Cite journal |vauthors=Donoho D, Jin J|year=2006 |title=Asymptotic minimaxity of false discovery rate thresholding for sparse exponential data |journal=Annals of Statistics |volume=34 |pages=2980–3018 |bibcode=2006math......2311D |arxiv=math/0602311 |doi=10.1214/009053606000000920 |issue=6}}</ref><ref name=BenjaminiGavrilov2009>{{Cite journal |vauthors=Benjamini Y, Gavrilov Y|year=2009 |title=A simple forward selection procedure based on false discovery rate control |journal=Annals of Applied Statistics |volume=3 |pages=179–198 |bibcode=2009arXiv0905.2819B |arxiv=0905.2819 |doi=10.1214/08-AOAS194 |issue=1}}</ref><ref name=DonohoJin2004>{{Cite journal |vauthors=Donoho D, Jin JS|year=2004 |title=Higher criticism for detecting sparse heterogeneous mixtures |journal=Annals of Statistics |volume=32 |pages=962–994 |bibcode=2004math.....10072D |arxiv=math/0410072 |doi=10.1214/009053604000000265 |issue=3}}</ref> and generalizing the [[confidence interval]] into the false coverage statement rate (FCR).<ref name=BenjYekut2005>{{Cite journal |vauthors=Benjamini Y, Yekutieli Y |year=2005 |title=False discovery rate controlling confidence intervals for selected parameters |journal=Journal of the American Statistical Association |volume=100 |pages=71–80 |doi=10.1198/016214504000001907 |issue=469}}</ref>\n\n===False positive rates in single tests of significance===\nColquhoun (2014)<ref name=DC2015>{{cite journal|last1=Colquhoun|first1=David|title=An investigation of the false discovery rate and the misinterpretation of ''p''-values|journal=Royal Society Open Science|date=2015|volume=1|issue=3|page=140216|doi=10.1098/rsos.140216|pmid=26064558|pmc=4448847|arxiv=1407.5296|bibcode=2014RSOS....140216C}}</ref> used the term \"false discovery rate\" to mean the probability that a statistically significant result was a false positive.  This was part of an investigation of the question \"how should one interpret the P value found in a single unbiased test of significance\".  In subsequent work,<ref name=\"DC2016\">{{cite web|last1=Colquhoun|first1=David|title=The problem with p-values|url=https://aeon.co/essays/it-s-time-for-science-to-abandon-the-term-statistically-significant|website=Aeon|publisher=Aeon Magazine|accessdate=11 December 2016}}</ref><ref name=\"DC2017\">{{cite journal|last1=Colquhoun|first1=David|title=The reproducibility of research and the misinterpretation of p-values|journal=Royal Society Open Science|volume=4|issue=12|pages=171085|date=2017|doi=10.1098/rsos.171085|pmid=29308247|pmc=5750014|url=http://rsos.royalsocietypublishing.org/content/4/12/171085#abstract-1}}</ref> Colquhoun called the same thing the false positive risk, rather than the false discovery rate in order to avoid confusion with the use of the latter term in connection with the problem of multiple comparisons.  The methods for dealing with multiple comparisons described above aim to control the type 1 error rate. The result of applying them is to produce a (corrected) P value. The result is, therefore, subject to the same misinterpretations as any other P value.\n\n==See also==\n* [[Positive predictive value]]\n\n==References==\n{{reflist|30em}}\n\n==External links==\n*[http://strimmerlab.org/notes/fdr.html False Discovery Rate Analysis in R] – Lists links with popular [[R (programming language)|R]] packages\n*[https://github.com/puolival/multipy False Discovery Rate Analysis in Python] – Python implementations of false discovery rate procedures\n*[https://eranraviv.com/understanding-false-discovery-rate/ Understanding False Discovery Rate] - blog post\n* {{YouTube|K8LQSvtjcEo|StatQuest: FDR and the Benjamini-Hochberg Method clearly explained}}\n{{Statistics|state=expanded}}\n\n[[Category:Statistical hypothesis testing]]\n[[Category:Summary statistics for contingency tables]]\n[[Category:Multiple comparisons]]"
    },
    {
      "title": "False positive rate",
      "url": "https://en.wikipedia.org/wiki/False_positive_rate",
      "text": "{{one source|date=July 2016}}\nIn [[statistics]], when performing [[multiple comparisons]], a '''false positive ratio''' (or '''false alarm ratio)''' is the [[probability]] of falsely rejecting the [[null hypothesis]] for a particular [[hypothesis test|test]]. The false positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positives) and the total number of actual negative events (regardless of classification).\n\nThe false positive '''rate''' (or \"false alarm rate\") usually refers to the '''expectancy''' of the false positive '''ratio'''.\n\n== Definition ==\n\nThe false positive rate is <math>\\frac{\\mathrm{FP}}{N}=\\frac{\\mathrm{FP}}{\\mathrm{FP} + \\mathrm{TN}}</math>\n\nwhere <math> \\mathrm{FP} </math> is the number of false positives, <math> \\mathrm{TN} </math> is the number of true negatives and <math> N=\\mathrm{FP}+\\mathrm{TN} </math> is the total number of negatives.\n\nThe level of significance that is used to test each hypothesis is set based on the form of inference ([[Familywise error rate#Simultaneous inference vs. selective inference|simultaneous inference vs. selective inference]]) and its supporting criteria (for example [[Familywise error rate|FWER]] or [[False discovery rate|FDR]]), that were pre-determined by the researcher.\n\nWhen performing [[multiple comparisons]] in a [[statistics|statistical]] framework such as above, the '''false positive ratio''' (also known as the '''false alarm ratio''', as opposed to false positive '''rate''' / false alarm '''rate''' ) usually refers to the probability of falsely rejecting the [[null hypothesis]] for a particular [[hypothesis test|test]]. Using the terminology suggested here, it is simply <math>V/m_0</math>.\n\nSince ''V'' is a random variable and ''<math>m_0</math>'' is a constant (<math> V \\leq m_0 </math>), the false positive '''ratio''' is also a random variable, ranging between 0-1.\n<br />\nThe '''false positive rate''' (or \"false alarm rate\") usually refers to the '''expectancy of the false positive ratio''', expressed by <math>E(V/m_0)</math>.\n\nIt is worth noticing that the two definitions (\"false positive ratio\" / \"false positive rate\") are somewhat interchangeable. For example, in the referenced article<ref name=Burke.at.all1988>{{cite journal | last1 = Burke | first1= Donald | first2= John | last2= Brundage | last3= Redfield | first3= Robert | year = 1988 | title = Measurement of the False Positive Rate in a Screening Program for Human Immunodeficiency Virus Infections | journal = [[The New England Journal of Medicine]] | volume = 319 | issue= 15 | pages = 961–964 | doi=10.1056/NEJM198810133191501 | pmid=3419477}}</ref> <math>V/m_0</math> serves as the false positive \"rate\" rather than as its \"ratio\".\n\n=== Classification of multiple hypothesis tests ===\n{{Main|Classification of multiple hypothesis tests}}\n<!-- [[Template:Classification of multiple hypothesis tests]]: -->\n{{Classification of multiple hypothesis tests}}\n\n== Difference from \"type I error rate\" and other close terms ==\n{{original research section|date=February 2013}}\nWhile the false positive rate is mathematically equal to the [[type I error]] rate, it is viewed as a separate term for the following reasons:{{citation needed|date=February 2013}}\n\n* The [[type I error]] rate is often associated with the '''a-priori''' setting of the [[significance level]] by the researcher: the significance level represents an acceptable error rate '''considering that all null hypotheses are true''' (the \"global null\" hypothesis). the choice of a significance level may thus be somewhat arbitrary (i.e. setting 10% (0.1), 5% (0.05), 1% (0.01) etc.)\n\n: As opposed to that, the false positive rate is associated with a '''post-prior''' result, which is the expected number of false positives divided by the total number of hypotheses under the '''real''' combination of true and non-true null hypotheses (disregarding the \"global null\" hypothesis). Since the false positive rate is a parameter that is not controlled by the researcher, it cannot be identified with the significance level.\n\n* Moreover, false positive rate is usually used regarding a medical test or diagnostic device (i.e. \"the false positive rate of a certain diagnostic device is 1%\"), while type I error is a term associated with statistical tests, where the meaning of the word \"positive\" is not as clear (i.e. \"the type I error of a test is 1%\").\n\nThe false positive rate should also not be confused with the [[familywise error rate]], which is defined as <math> \\mathrm{FWER} = \\Pr(V \\ge 1)\\,</math>. As the number of tests grows, the familywise error rate usually converges to 1 while the false positive rate remains fixed.\n\nLastly, it is important to note the profound difference between the false positive rate and the [[false discovery rate]]: while the first is defined as <math>E(V/m_0)</math>, the second is defined as <math>E(V/R)</math>.\n\n== See also ==\n* [[False coverage rate]]\n* [[False discovery rate]]\n\n== References ==\n\n<references />\n\n[[Category:Multiple comparisons]]\n[[Category:Statistical tests]]\n[[Category:Analysis of variance]]\n[[Category:Statistical ratios]]"
    },
    {
      "title": "Family-wise error rate",
      "url": "https://en.wikipedia.org/wiki/Family-wise_error_rate",
      "text": "{{more citations needed|date=June 2016}}\nIn [[statistics]], '''family-wise error rate''' ('''FWER''') is the [[probability]] of making one or more false discoveries, or [[Type I and type II errors|type I error]]s when performing [[multiple comparisons|multiple hypotheses tests]].\n\n==History==\n[[John Tukey|Tukey]] coined the terms experimentwise error rate and '''\"error rate per-experiment\"''' to indicate error rates that the researcher could use as a control level in a multiple hypothesis experiment.{{citation needed|date=June 2016}}\n\n==Background==\nWithin the statistical framework, there are several definitions for the term \"family\":\n\n* Hochberg & Tamhane defined \"family\" in 1987 as \"any collection of inferences for which it is meaningful to take into account some combined measure of error\".<ref>{{Cite book |last=Hochberg |first=Y. |last2=Tamhane |first2=A. C. |year=1987 |title=Multiple Comparison Procedures |location=New York |publisher=Wiley |page=5 |isbn=978-0-471-82222-6 }}</ref>\n* According to Cox in 1982, a set of inferences should be regarded a family:{{citation needed|date=June 2016}}\n# To take into account the selection effect due to [[data dredging]]\n# To ensure simultaneous correctness of a set of inferences as to guarantee a correct overall decision\n\nTo summarize, a family could best be defined by the potential selective inference that is being faced: A family is the smallest set of items of inference in an analysis, interchangeable about their meaning for the goal of research, from which selection of results for action, presentation or highlighting could be made ([[Yoav Benjamini]]).{{citation needed|date=June 2016}}\n\n===Classification of multiple hypothesis tests===\n{{Main|Classification of multiple hypothesis tests}}\n<!-- [[Template:Classification of multiple hypothesis tests]]: -->\n{{Classification of multiple hypothesis tests}}\n\n==Definition==\nThe FWER is the probability of making at least one [[Type I and type II errors|type I error]] in the family,\n\n:<math> \\mathrm{FWER} = \\Pr(V \\ge 1), \\,</math>\n\nor equivalently,\n\n:<math> \\mathrm{FWER} = 1 -\\Pr(V = 0).</math>\n\nThus, by assuring <math> \\mathrm{FWER} \\le \\alpha\\,\\! \\,</math>, the probability of making one or more [[Type I and type II errors|type I errors]] in the family is controlled at level <math>\\alpha\\,\\!</math>.\n\nA procedure controls the FWER ''in the weak sense'' if the FWER control at level <math>\\alpha\\,\\!</math> is guaranteed ''only'' when all null hypotheses are true (i.e. when <math>m_0 = m</math>, meaning the \"global null hypothesis\" is true).<ref>{{cite book |last1=Dmitrienko |first1=Alex |last2=Tamhane |first2=Ajit |last3=Bretz |first3=Frank |title=Multiple Testing Problems in Pharmaceutical Statistics |date=2009 |publisher=CRC Press |isbn=9781584889847 |page=37 |edition=1}}</ref>\n\nA procedure controls the FWER ''in the strong sense'' if the FWER control at level <math>\\alpha\\,\\!</math> is guaranteed for ''any'' configuration of true and non-true null hypotheses (whether the global null hypothesis is true or not).<ref>{{cite book |last1=Dmitrienko |first1=Alex |last2=Tamhane |first2=Ajit |last3=Bretz |first3=Frank |title=Multiple Testing Problems in Pharmaceutical Statistics |date=2009 |publisher=CRC Press |isbn=9781584889847 |page=37 |edition=1}}</ref>\n\n==Controlling procedures==\n{{broader|Multiple testing correction}}\n{{see also|False coverage rate#Controlling procedures|False discovery rate#Controlling procedures}}\n{{further|Post hoc analysis#Tests{{!}}List of ''post hoc'' tests}}\n\nSome classical solutions that ensure strong level <math>\\alpha</math> FWER control, and some newer solutions exist.\n\n===The Bonferroni procedure===\n{{main|Bonferroni correction}}\n* Denote by <math>p_{i}</math> the ''p''-value for testing <math>H_{i}</math>\n* reject <math>H_{i}</math> if <math> p_{i} \\leq \\frac{\\alpha}{m} </math>\n\n===The Šidák procedure===\n{{main|Šidák correction}}\n* Testing each hypothesis at level <math> \\alpha_{SID} = 1-(1-\\alpha)^\\frac{1}{m} </math> is Sidak's multiple testing procedure.\n* This procedure is more powerful than Bonferroni but the gain is small.\n* This procedure can fail to control the FWER when the tests are negatively dependent.\n\n===Tukey's procedure===\n{{main|Tukey's range test}}\n* Tukey's procedure is only applicable for [[pairwise comparison]]s.\n* It assumes independence of the observations being tested, as well as equal variation across observations ([[homoscedasticity]]).\n* The procedure calculates for each pair the [[studentized range]] statistic: <math> \\frac {Y_{A}-Y_{B}} {SE} </math> where <math>Y_{A}</math> is the larger of the two means being compared, <math>Y_{B}</math> is the smaller, and <math>SE</math> is the standard error of the data in question.{{citation needed|date=June 2016}}\n* Tukey's test is essentially a [[Student's t-test]], except that it corrects for '''family-wise error-rate'''.{{citation needed|date=June 2016}}\n\n===Holm's  step-down procedure (1979)===\n{{main|Holm–Bonferroni method}}\n\n* Start by ordering the ''p''-values (from lowest to highest) <math>P_{(1)} \\ldots P_{(m)}</math> and let the associated hypotheses be  <math>H_{(1)} \\ldots H_{(m)}</math>\n* Let <math>k</math> be the minimal index such that <math>P_{(k)} > \\frac{\\alpha}{m+1-k}</math>\n* Reject the null hypotheses <math>H_{(1)} \\ldots H_{(k-1)}</math>. If <math>k = 1</math> then none of the hypotheses are rejected.{{citation needed|date=June 2016}}\nThis procedure is uniformly more powerful than the Bonferroni procedure.<ref name=\"Aickin1996\">{{cite journal | last1 = Aickin | first1= M | last2 = Gensler | first2 = H | year = 1996 | title = Adjusting for multiple testing when reporting research results: the Bonferroni vs Holm methods | journal = [[American Journal of Public Health]] | volume = 86 | issue = 5 | pages = 726–728 | pmc=1380484 | pmid=8629727|doi=10.2105/ajph.86.5.726 }}</ref>  \nThe reason why this procedure controls the family-wise error rate for all the m hypotheses at level α in the strong sense is, because it is a [[closed testing procedure]]. As such, each intersection is tested using the simple Bonferroni test.{{citation needed|date=June 2016}}\n\n===Hochberg's step-up procedure===\nHochberg's step-up procedure (1988) is performed using the following steps:<ref name=Hochberg1988>{{cite journal | last1 = Hochberg | first1= Yosef | year = 1988 | title = A Sharper Bonferroni Procedure for Multiple Tests of Significance | journal = [[Biometrika]] | volume = 75 | issue = 4 | pages = 800–802 | url = http://www-stat.wharton.upenn.edu/~steele/Courses/956/Resource/MultipleComparision/Hochberg88.pdf | doi=10.1093/biomet/75.4.800}}</ref>\n\n* Start by ordering the ''p''-values (from lowest to highest) <math>P_{(1)} \\ldots P_{(m)}</math> and let the associated hypotheses be  <math>H_{(1)} \\ldots H_{(m)}</math>\n* For a given <math>\\alpha</math>, let <math>R</math> be the largest <math>k</math> such that <math>P_{(k)} \\leq \\frac{\\alpha}{m-k+1}</math>\n* Reject the null hypotheses <math>H_{(1)} \\ldots H_{(R)}</math>\n\nHochberg's procedure is more powerful than Holms'.  Nevertheless, while Holm’s is a closed testing procedure (and thus, like Bonferroni, has no restriction on the joint distribution of the test statistics), Hochberg’s is based on the Simes test, so it holds only under non-negative dependence.{{citation needed|date=June 2016}}\n\n===Dunnett's correction===\n{{main|Dunnett's test}}\n[[Charles Dunnett]] (1955, 1966) described an alternative alpha error adjustment when ''k'' groups are compared to the same control group. Now known as Dunnett's test, this method is less conservative than the Bonferroni adjustment.{{citation needed|date=June 2016}}\n\n===Scheffé's method===\n{{main|Scheffé's method}}\n{{empty section|date=February 2013}}\n\n===Resampling procedures===\n\nThe procedures of Bonferroni and Holm control the FWER under any dependence structure of the ''p''-values (or equivalently the individual test statistics). Essentially, this is achieved by accommodating a `worst-case' dependence structure (which is close to independence for most practical purposes). But such an approach is conservative if dependence is actually positive. To give an extreme example, under perfect positive dependence, there is effectively only one test and thus, the FWER is uninflated.\n\nAccounting for the dependence structure of the ''p''-values (or of the individual test statistics) produces more powerful procedures. This can be achieved by applying resampling methods, such as bootstrapping and permutations methods. The procedure of Westfall and Young (1993) requires a certain condition that does not always hold in practice (namely, subset pivotality).<ref>{{Cite book |last=Westfall |first=P. H. |last2=Young |first2=S. S. |year=1993 |title=Resampling-Based Multiple Testing: Examples and Methods for p-Value Adjustment |location=New York |publisher=John Wiley |isbn=978-0-471-55761-6 }}</ref> The procedures of Romano and Wolf (2005a,b) dispense with this condition and are thus more generally valid.<ref name=\"Romano and Wolf 2005a\">{{cite journal | last1 = Romano | first1= J.P. | last2 = Wolf | first2 = M. | year = 2005a | title = Exact and approximate stepdown methods for multiple hypothesis testing | journal = [[Journal of the American Statistical Association]] | volume = 100 | issue= 469 | pages = 94–108 | doi=10.1198/016214504000000539}}</ref><ref name=\"Romano and Wolf 2005b\">{{cite journal | last1 = Romano | first1= J.P. | last2 = Wolf | first2 = M. | year = 2005b | title = Stepwise multiple testing as formalized data snooping | journal = [[Econometrica]] | volume = 73 | issue= 4 | pages = 1237–1282 | doi=10.1111/j.1468-0262.2005.00615.x| citeseerx = 10.1.1.198.2473 }}</ref>\n\n=== Harmonic mean ''p''-value procedure ===\n{{Main|Harmonic mean p-value}}\nThe harmonic mean ''p''-value (HMP) procedure<ref>{{cite journal |vauthors=Good, I J |date=1958 |title=Significance tests in parallel and in series |journal=Journal of the American Statistical Association |volume=53 |issue=284 |pages=799–813|jstor=2281953 |doi=10.1080/01621459.1958.10501480 }}</ref><ref>{{cite journal |vauthors=Wilson, D J |date=2019 |title=The harmonic mean ''p''-value for combining dependent tests|journal=Proceedings of the National Academy of Sciences USA |volume=116 |issue=4 |pages=1195–1200|doi=10.1073/pnas.1814092116 }}</ref> provides a multilevel test that improves on the power of Bonferroni correction by assessing the significance of ''groups'' of hypotheses while controlling the strong-sense family-wise error rate. The significance of any subset <math display=\"inline\">\\mathcal{R}</math> of the <math display=\"inline\">m</math> tests is assessed by calculating the HMP for the subset,<math display=\"block\">\n\\overset{\\circ}{p}_\\mathcal{R} = \\frac{\\sum_{i\\in\\mathcal{R}} w_{i}}{\\sum_{i\\in\\mathcal{R}} w_{i}/p_{i}},\n</math>where <math display=\"inline\">w_1,\\dots,w_m</math> are weights that sum to one (i.e. <math display=\"inline\">\\sum_{i=1}^m w_i=1</math>). An approximate procedure that controls the strong-sense family-wise error rate at level approximately <math display=\"inline\">\\alpha</math> rejects the null hypothesis that none of the ''p''-values in subset <math display=\"inline\">\\mathcal{R}</math> are significant when <math display=\"inline\">\\overset{\\circ}{p}_\\mathcal{R}\\leq\\alpha\\,w_\\mathcal{R}</math> (where <math display=\"inline\">w_\\mathcal{R}=\\sum_{i\\in\\mathcal{R}}w_i</math>). This approximation is reasonable for small <math display=\"inline\">\\alpha</math> (e.g. <math display=\"inline\">\\alpha<0.05</math>) and becomes arbitrarily good as <math display=\"inline\">\\alpha</math> approaches zero. An asymptotically exact test is also available (see [[Harmonic mean p-value|main article]]).\n\n==Alternative approaches==\n{{further|False discovery rate}}\n\nFWER control exerts a more stringent control over false discovery compared to false discovery rate (FDR) procedures. FWER control limits the probability of ''at least one'' false discovery, whereas FDR control limits (in a loose sense) the expected proportion of false discoveries. Thus, FDR procedures have greater [[Statistical power|power]] at the cost of increased rates of [[Type I and type II errors|type I]] errors, i.e., rejecting null hypotheses that are actually true.<ref>{{cite journal |last=Shaffer |first=J. P. |year=1995 |title=Multiple hypothesis testing |journal=[[Annual Review of Psychology]] |volume=46 |issue= |pages=561–584 |doi=10.1146/annurev.ps.46.020195.003021 }}</ref>\n\nOn the other hand, FWER control is less stringent than per-family error rate control, which limits the expected number of errors per family. Because FWER control is concerned with ''at least one'' false discovery, unlike per-family error rate control it does not treat multiple simultaneous false discoveries as any worse than one false discovery. The [[Bonferroni correction]] is often considered as merely controlling the FWER, but in fact also controls the per-family error rate.<ref>{{cite journal|last1=Frane|first1=Andrew|title=Are per-family Type I error rates relevant in social and behavioral science?|journal=Journal of Modern Applied Statistical Methods|date=2015|volume=14|issue=1|pages=12–23|url=http://digitalcommons.wayne.edu/jmasm/vol14/iss1/5|doi=10.22237/jmasm/1430453040}}</ref>\n\n==References==\n{{Reflist}}\n\n== External links ==\n\n* [https://riffyn.com/riffyn-blog/2017/10/29/family-wise-error-rate Understanding Family Wise Error Rate] - blog post including its utility relative to False Discovery Rate\n\n{{DEFAULTSORT:Familywise Error Rate}}\n[[Category:Statistical hypothesis testing]]\n[[Category:Multiple comparisons]]\n[[Category:Rates]]"
    },
    {
      "title": "Harmonic mean p-value",
      "url": "https://en.wikipedia.org/wiki/Harmonic_mean_p-value",
      "text": "{{Short description|Statistical method for multiple testing}}\n\nThe '''harmonic mean ''p''-value'''<ref name=\":0\">{{cite journal |vauthors=Good, I J |date=1958 |title=Significance tests in parallel and in series |journal=Journal of the American Statistical Association |volume=53 |issue=284 |pages=799–813|jstor=2281953 |doi=10.1080/01621459.1958.10501480 }}</ref><ref name=\":1\">{{cite journal |vauthors=Wilson, D J |date=2019 |title=The harmonic mean ''p''-value for combining dependent tests|journal=Proceedings of the National Academy of Sciences USA |volume=116 |issue=4 |pages=1195–1200|doi=10.1073/pnas.1814092116 }}</ref><ref name=\":2\">{{cite web |url=http://alrw.net/articles/21.pdf |title= Combining p-values via averaging |last=Vovk |first=Vladimir |last2=Wang |first2=Ruodu  |date=April 25, 2019 |website=Algorithmic Learning in a Random World}}</ref> '''(HMP)''' is a statistical technique for addressing the [[multiple comparisons problem]] that controls the [[Family-wise error rate#Definition|strong-sense family-wise error rate]].<ref name=\":1\" /> It improves on the [[Power (statistics)|power]] of [[Bonferroni correction]] by performing combined tests, i.e. by testing whether ''groups'' of [[P-value|''p''-values]] are statistically significant, like [[Fisher's method]].<ref>{{cite book|title=Statistical Methods for Research Workers|last=Fisher|first=R A|date=1934|publisher=Oliver and Boyd|edition=5th|location=Edinburgh, UK}}</ref> However, it avoids the restrictive assumption that the ''p''-values are [[Independence (probability theory)|independent]], unlike Fisher's method.<ref name=\":1\" /><ref name=\":2\" /> Consequently, it controls the [[Type I and type II errors|false positive rate]] when tests are dependent, at the expense of less power (i.e. a higher [[Type I and type II errors|false negative rate]]) when tests are independent.<ref name=\":1\" /> Besides providing an alternative to approaches such as [[Bonferroni correction]] that controls the stringent [[Family-wise error rate#Definition|family-wise error rate]], it also provides an alternative to the widely-used [[False discovery rate#Benjamini–Hochberg procedure|Benjamini-Hochberg procedure]] (BH) for controlling the less-stringent [[false discovery rate]].<ref>{{cite journal|vauthors=Benjamini Y, Hochberg Y|date=1995|title=Controlling the false discovery rate: A practical and powerful approach to multiple testing|journal=Journal of the Royal Statistical Society Series B|volume=57|issue=1|pages=289–300|jstor=2346101}}</ref> This is because the power of the HMP to detect significant ''groups'' of hypotheses is greater than the power of BH to detect significant ''individual'' hypotheses.<ref name=\":1\" />\n\nThere are two versions of the technique: (i) [[#Direct interpretation of the harmonic mean p-value|direct interpretation of the HMP]] as an approximate ''p''-value and (ii) a procedure for transforming the HMP into an [[#Asymptotically exact harmonic mean p-value procedure|asymptotically exact ''p''-value]]. The approach provides a [[#Multiple testing via the multilevel test procedure|multilevel test procedure]] in which the smallest groups of ''p''-values that are statistically significant may be sought.\n\n== Direct interpretation of the harmonic mean ''p''-value ==\nThe [[Harmonic mean#Weighted harmonic mean|weighted harmonic mean]] of ''p''-values <math display=\"inline\">p_1, \\dots, p_L</math> is defined as <math display=\"block\">\n\\overset{\\circ}{p} = \\frac{\\sum_{i=1}^L w_{i}}{\\sum_{i=1}^L w_{i}/p_{i}}, \n</math> where <math display=\"inline\">w_1, \\dots, w_L</math> are weights that must sum to one, i.e. <math display=\"inline\">\\sum_{i=1}^L w_i=1</math>. Equal weights may be chosen, in which case <math display=\"inline\">w_i=1/L</math>.\n\nIn general, interpreting the HMP directly as a ''p''-value is anti-conservative, meaning that the [[Type I and type II errors|false positive rate]] is higher than expected. However, as the HMP becomes smaller, the discrepancy decreases, so that direct interpretation of significance achieves a false positive rate close to that implied for sufficiently small values (e.g. <math>\\overset{\\circ}{p}<0.05</math>).<ref name=\":1\" />\n\nThe HMP is never anti-conservative by more than a factor of <math display=\"inline\">e\\,\\log L</math> for small <math display=\"inline\">L</math>, or <math display=\"inline\">\\log L</math> for large <math display=\"inline\">L</math>.<ref name=\":2\" /> However, these bounds represent worst case scenarios that are necessarily conservative, becoming increasingly conservative for smaller values of the HMP since  <math>\\overset{\\circ}{p}</math> approaches the true false positive rate. Rather than applying these bounds, asymptotically exact ''p''-values can be produced by transforming the HMP.\n\n== Asymptotically exact harmonic mean ''p''-value procedure ==\n[[Stable distribution#A generalized central limit theorem|Generalized central limit theorem]] shows that an asymptotically exact ''p''-value, <math display=\"inline\">p_{\\overset{\\circ}{p}}</math>, can be computed from the HMP,  <math>\\overset{\\circ}{p}</math>, using the formula<ref name=\":1\" /> <math display=\"block\">p_{\\overset{\\circ}{p}} = \\int_{1/\\overset{\\circ}{p}}^\\infty f_\\textrm{Landau}\\left(x\\,|\\,\\log L+0.874,\\frac{\\pi}{2}\\right) \\mathrm{d} x. </math>This transformed ''p''-value becomes exact as the number of tests, <math display=\"inline\">L</math>, becomes large. The computation uses the [[Landau distribution]], whose density function can be written<math display=\"block\">f_\\textrm{Landau}(x\\,|\\,\\mu,\\sigma) = \\frac{1}{\\pi\\sigma}\\int_0^\\infty \\textrm{e}^{\n-t\\frac{(x-\\mu)}{\\sigma} -\\frac{2}{\\pi}t \\log t\n}\\,\\sin(2t)\\,\\textrm{d}t.</math>The test is implemented by the <code>p.hmp</code> command of the <code>harmonicmeanp</code> [https://cran.r-project.org/package=harmonicmeanp R package]; a [http://www.danielwilson.me.uk/harmonicmeanp/hmpTutorial.html tutorial] is available online.\n\nEquivalently, one can compare the HMP to a table of critical values (Table 1). The table illustrates that the smaller the false positive rate, and the smaller the number of tests, the closer the critical value is to the false positive rate. \n{| class=\"wikitable\"\n|+\nTable 1. Critical values for the HMP <math display=\"inline\">\\overset{\\circ}{p}</math> for varying numbers of tests <math display=\"inline\">L</math> and false positive rates <math display=\"inline\">\\alpha</math>.<ref name=\":1\" />\n!<math display=\"inline\">L</math>\n!<math display=\"inline\">\\alpha=0.05</math>\n!<math display=\"inline\">\\alpha=0.01</math>\n!<math display=\"inline\">\\alpha=0.001</math>\n|-\n|10\n|0.040\n|0.0094\n|0.00099\n|-\n|100\n|0.036\n|0.0092\n|0.00099\n|-\n|1,000\n|0.034\n|0.0090\n|0.00099\n|-\n|10,000\n|0.031\n|0.0088\n|0.00098\n|-\n|100,000\n|0.029\n|0.0086\n|0.00098\n|-\n|1,000,000\n|0.027\n|0.0084\n|0.00098\n|-\n|10,000,000\n|0.026\n|0.0083\n|0.00098\n|-\n|100,000,000\n|0.024\n|0.0081\n|0.00098\n|-\n|1,000,000,000\n|0.023\n|0.0080\n|0.00097\n|}\n\n== Multiple testing via the multilevel test procedure ==\nIf the HMP is significant at some level <math display=\"inline\">\\alpha</math> for a group of <math display=\"inline\">L</math> ''p''-values, one may search all subsets of the <math display=\"inline\">L</math> ''p''-values for the smallest significant group, while maintaining the strong-sense family-wise error rate.<ref name=\":1\" /> Formally, this constitutes a [[Closed testing procedure|closed-testing procedure]].<ref>{{cite journal |vauthors=Marcus R, Eric P, Gabriel KR |date=1976 |title=On closed testing procedures with special reference to ordered analysis of variance. |journal=Biometrika |volume=63 |issue=3 |pages=655–660|jstor=2335748 |doi=10.1093/biomet/63.3.655 }}</ref>\n\nWhen <math display=\"inline\">\\alpha</math> is small (e.g. <math display=\"inline\">\\alpha<0.05</math>), the following multilevel test based on direct interpretation of the HMP controls the strong-sense family-wise error rate at level approximately <math display=\"inline\">\\alpha:</math>\n\n# Define the HMP of any subset <math display=\"inline\">\\mathcal{R}</math> of the  <math display=\"inline\">L</math> ''p''-values to be<math display=\"block\">\n\\overset{\\circ}{p}_\\mathcal{R} = \\frac{\\sum_{i\\in\\mathcal{R}} w_{i}}{\\sum_{i\\in\\mathcal{R}} w_{i}/p_{i}}.\n</math> \n# Reject the null hypothesis that none of the ''p''-values in subset <math display=\"inline\">\\mathcal{R}</math> are significant if <math display=\"inline\">\\overset{\\circ}{p}_\\mathcal{R}\\leq\\alpha\\,w_\\mathcal{R}</math>, where <math display=\"inline\">w_\\mathcal{R}=\\sum_{i\\in\\mathcal{R}}w_i</math>. (Recall that, by definition, <math display=\"inline\">\\sum_{i=1}^L w_i=1</math>.)\n\nAn asymptotically exact version of the above replaces <math display=\"inline\">\\overset{\\circ}{p}_\\mathcal{R}</math>in step 2 with <math display=\"block\">p_{\\overset{\\circ}{p}_\\mathcal{R}} = \\int_{1/\\overset{\\circ}{p}_\\mathcal{R}}^\\infty f_\\textrm{Landau}\\left(x\\,|\\,\\log |\\mathcal{R}|+0.874,\\frac{\\pi}{2}\\right) \\mathrm{d} x, </math>where <math display=\"inline\">1\\leq|\\mathcal{R}|\\leq L</math> gives the number of ''p''-values in subset <math display=\"inline\">\\mathcal{R}</math>.\n\nSince direct interpretation of the HMP is faster, a two-pass procedure may be used to identify subsets of ''p''-values that are likely to be significant using direct interpretation, subject to confirmation using the asymptotically exact formula.\n\n== Properties of the HMP ==\nThe HMP has a range of properties that arise from generalized central limit theorem.<ref name=\":1\" /> It is:\n\n* Robust to positive dependency between the ''p''-values. \n* Insensitive to the exact number of tests, ''L''. \n* Robust to the distribution of weights, ''w''. \n* Most influenced by the smallest ''p''-values.\n\nWhen the HMP is not significant, neither is any subset of the constituent tests. Conversely, when the multilevel test deems a subset of ''p''-values to be significant, the HMP for all the ''p''-values combined is likely to be significant; this is certain when the HMP is interpreted directly. When the goal is to assess the significance of ''individual'' ''p''-values, so that combined tests concerning ''groups'' of ''p''-values are of no interest, the HMP is equivalent to the [[Bonferroni correction|Bonferroni]] procedure.\n\nThe HMP assumes the individual ''p''-values have (not necessarily independent) [[Uniform distribution (continuous)#Standard uniform|standard uniform]] distributions when their null hypotheses are true. Large numbers of underpowered tests can therefore harm the power of the HMP.\n\nWhile the choice of weights is unimportant for the validity of the HMP under the null hypothesis, the weights influence the power of the procedure. Supplementary Methods §5C of <ref name=\":1\" /> and an online [http://www.danielwilson.me.uk/harmonicmeanp/hmpTutorial.html tutorial] consider the issue in more detail.\n\n== Bayesian interpretations of the HMP ==\nThe HMP was conceived by analogy to Bayesian model averaging and can be interpreted as inversely proportional to a model-averaged [[Bayes factor]] when combining ''p''-values from [[Likelihood-ratio test|likelihood ratio tests]].<ref name=\":0\" /><ref name=\":1\" />\n\n=== The harmonic mean rule-of-thumb ===\n[[I. J. Good]] reported an empirical relationship between the Bayes factor and the ''p''-value from a likelihood ratio test.<ref name=\":0\" /> For a null hypothesis <math display=\"inline\">H_0</math> nested in a more general alternative hypothesis <math display=\"inline\">H_A,</math> he observed that often,<math display=\"block\">\\textrm{BF}_i\\approx \\frac{1}{\\gamma\\,p_i},\\quad3\\frac{1}{3}<\\gamma<30,</math>where <math display=\"inline\">\\textrm{BF}_i</math> denotes the Bayes factor in favour of <math display=\"inline\">H_A</math> versus <math>H_0.</math> Extrapolating, he proposed a rule of thumb in which the HMP is taken to be inversely proportional to the model-averaged Bayes factor for a collection of <math display=\"inline\">L</math> tests with common null hypothesis:<math display=\"block\">\\overline{\\textrm{BF}}=\\sum_{i=1}^L w_i\\,\\textrm{BF}_i \\approx \\sum_{i=1}^L \\frac{w_i}{\\gamma\\,p_i} = \\frac{1}{\\gamma\\,\\overset{\\circ}{p}}.</math>For Good, his rule-of-thumb supported an interchangeability between [[Bayesian inference|Bayesian]] and [[Frequentist inference|classical]] approaches to hypothesis testing.<ref>{{cite journal|vauthors=Good, I J|date=1984|title=C192. One tail versus two-tails, and the harmonic-mean rule of thumb.|journal=Journal of Statistical Computation and Simulation|volume=19|issue=2|pages=174–176|doi=10.1080/00949658408810727}}</ref><ref>{{cite journal|vauthors=Good, I J|date=1984|title=C193. Paired versus unpaired comparisons and the harmonic-mean rule of thumb.|journal=Journal of Statistical Computation and Simulation|volume=19|issue=2|pages=176–177|doi=10.1080/00949658408810728}}</ref><ref>{{cite journal|vauthors=Good, I J|date=1984|title=C213. A sharpening of the harmonic-mean rule of thumb for combining tests \"in parallel\".|journal=Journal of Statistical Computation and Simulation|volume=20|issue=2|pages=173–176|doi=10.1080/00949658408810770}}</ref><ref>{{cite journal|vauthors=Good, I J|date=1984|title=C214. The harmonic-mean rule of thumb: Some classes of applications.|journal=Journal of Statistical Computation and Simulation|volume=20|issue=2|pages=176–179|doi=10.1080/00949658408810771}}</ref><ref>{{Cite book|url=http://worldcat.org/oclc/319491702|title=Good thinking : the foundations of probability and its applications|last=Good, Irving John.|date=2009|publisher=Dover Publications|isbn=9780486474380|oclc=319491702}}</ref>\n\n=== Bayesian calibration of ''p''-values ===\nIf the distributions of the ''p''-values under the alternative hypotheses follow [[Beta distribution]]s with parameters <math>\\left(0<\\xi_i<1, 1\\right)</math>, a form considered by Sellke, Bayarri and Berger,<ref>{{Cite journal|last=Sellke|first=Thomas|last2=Bayarri|first2=M. J|last3=Berger|first3=James O|date=2001|title=Calibration of p Values for Testing Precise Null Hypotheses|url=http://dx.doi.org/10.1198/000313001300339950|journal=The American Statistician|volume=55|issue=1|pages=62–71|doi=10.1198/000313001300339950|issn=0003-1305|via=}}</ref> then the inverse proportionality between the model-averaged Bayes factor and the HMP can be formalized as<ref name=\":1\" /><ref name=\":3\">{{cite journal|vauthors=Wilson, D J|date=2019|title=Reply to Held: When is a harmonic mean ''p''-value a Bayes factor?|url=http://www.danielwilson.me.uk/preprints/wilson_2019b.pdf|journal=Proceedings of the National Academy of Sciences USA|volume=116|issue=13|pages=5857–5858|doi=10.1073/pnas.1902157116}}</ref><math display=\"block\">\\overline{\\textrm{BF}}=\\sum_{i=1}^L \\mu_i\\,\\textrm{BF}_i=\\sum_{i=1}^L \\mu_i\\,\\xi_i\\,p_i^{\\xi_i-1}\\approx\\bar\\xi\\sum_{i=1}^L w_i\\,p_i^{-1}=\\frac{\\bar\\xi}{\\overset{\\circ}{p}},</math>where\n\n*<math display=\"inline\">\\mu_i</math> is the prior probability of alternative hypothesis <math display=\"inline\">i,</math> such that <math display=\"inline\">\\sum_{i=1}^L\\mu_i=1,</math>\n*<math display=\"inline\">\\xi_i/(1+\\xi_i)</math> is the expected value of <math display=\"inline\">p_i</math> under alternative hypothesis <math display=\"inline\">i,</math>\n*<math display=\"inline\">w_i=u_i/\\bar\\xi</math> is the weight attributed to ''p''-value <math display=\"inline\">i,</math>\n*<math display=\"inline\">u_i = \\left(\\mu_i\\,\\xi_i\\right)^{1/(1-\\xi_i)}</math> incorporates the prior model probabilities and powers into the weights, and\n*<math display=\"inline\">\\bar\\xi = \\sum_{i=1}^L u_i</math> normalizes the weights.\n\nThe approximation works best for well-powered tests (<math>\\xi_i\\ll 1</math>).\n\n=== The harmonic mean ''p''-value as a bound on the Bayes factor ===\nFor likelihood ratio tests with exactly two degrees of freedom, [[Wilks' theorem]] implies that <math display=\"inline\">p_i=1/R_i</math>, where <math display=\"inline\">R_i</math> is the maximized likelihood ratio in favour of alternative hypothesis <math display=\"inline\">i,</math> and therefore <math display=\"inline\">\\overset{\\circ}{p}=1/\\bar{R}</math>, where <math display=\"inline\">\\bar{R}</math> is the weighted mean maximized likelihood ratio, using weights <math display=\"inline\">w_1,\\dots,w_L.</math> Since <math display=\"inline\">R_i</math> is an upper bound on the Bayes factor, <math display=\"inline\">\\textrm{BF}_i</math>, then <math display=\"inline\">1/\\overset{\\circ}{p}</math> is an upper bound on the model-averaged Bayes factor:<math display=\"block\">\\overline{\\textrm{BF}}\\leq\\frac{1}{\\overset{\\circ}{p}}.</math>While the equivalence holds only for two degrees of freedom, the relationship between <math display=\"inline\">\\overset{\\circ}{p}</math> and <math display=\"inline\">\\bar{R},</math> and therefore <math display=\"inline\">\\overline{\\textrm{BF}},</math> behaves similarly for other degrees of freedom.<ref name=\":1\" />\n\nUnder the assumption that the distributions of the ''p''-values under the alternative hypotheses follow [[Beta distribution]]s with parameters <math>\\left(1, \\kappa_i>1\\right),</math> and that the weights <math>w_i=\\mu_i,</math> the HMP provides a tighter upper bound on the model-averaged Bayes factor:<math display=\"block\">\\overline{\\textrm{BF}}\\leq \\frac{1}{e\\,\\overset{\\circ}{p}},</math>a result that again reproduces the inverse proportionality of Good's empirical relationship.<ref>{{cite journal|vauthors=Held, L|date=2019|title=On the Bayesian interpretation of the harmonic mean ''p''-value|journal=Proceedings of the National Academy of Sciences USA|volume=116|issue=13|pages=5855–5856|doi=10.1073/pnas.1900671116}}</ref>\n\n== References ==\n<references />\n\n\n[[Category:Multiple comparisons]]\n[[Category:Statistical hypothesis testing]]"
    },
    {
      "title": "Holm–Bonferroni method",
      "url": "https://en.wikipedia.org/wiki/Holm%E2%80%93Bonferroni_method",
      "text": "In [[statistics]], the '''Holm&ndash;Bonferroni method'''<ref>{{cite journal\n|last=Holm |first=S.\n|year=1979\n|title=A simple sequentially rejective multiple test procedure\n|journal=Scandinavian Journal of Statistics\n|volume=6 |issue=2 |pages=65&ndash;70\n|mr=538597 | jstor = 4615733\n}}</ref> (also called the '''Holm method''' or '''Bonferroni-Holm method''') is used to counteract the problem of [[multiple comparisons]]. It is intended to control the [[family-wise error rate]] and offers a simple test [[uniformly more powerful]] than the [[Bonferroni correction]]. It is named after Sture Holm, who codified the method, and [[Carlo Emilio Bonferroni]].\n\n==Motivation==\nWhen considering several hypotheses, the problem of [[Multiple comparisons#The problem|multiplicity]] arises: the more hypotheses we check, the higher the probability of a Type I error (false positive). The Holm–Bonferroni method is one of many approaches that control the [[family-wise error rate]] (the probability that one or more Type I errors will occur) by adjusting the rejection criteria of each of the individual hypotheses.\n\n==Formulation==\nThe method is as follows:\n* Let <math>H_{1},...,H_{m}</math> be a family of <math>m</math> null hypotheses and <math>P_{1},...,P_{m}</math> the corresponding p-values.\n* Start by ordering the p-values (from lowest to highest) <math>P_{(1)} \\ldots P_{(m)}</math> and let the associated hypotheses be  <math>H_{(1)} \\ldots H_{(m)}</math>\n* For a given [[significance level]] <math>\\alpha</math>, let <math>k</math> be the minimal index such that <math>P_{(k)} > \\frac{\\alpha}{m+1-k}</math>\n* Reject the null hypotheses <math>H_{(1)} \\ldots H_{(k-1)}</math> and do not reject <math>H_{(k)} \\ldots H_{(m)}</math>\n* If <math>k=1</math> then do not reject any of the null hypotheses and if no such <math>k</math> exist then reject all of the null hypotheses.\n\nThis method ensures that <math>FWER\\leq\\alpha</math>, where <math>FWER</math> is the [[family-wise error rate]].\n\n===Rationale===\nThe simple [[Bonferroni correction]] rejects only null hypotheses with ''p''-value less than <math>\\frac{\\alpha}{m}</math>, in order to ensure that the risk of rejecting one or more true null hypotheses (i.e., of committing one or more type I errors) is at most <math>\\alpha</math>. The cost of this protection against type I errors is an increased risk of accepting one or more false null hypotheses (i.e., of committing one or more type II errors).\n\nThe Holm-Bonferroni method also controls the maximum family-wise error rate at <math>\\alpha</math>, but with a lower increase of type II error risk than the classical Bonferroni method. The Holm-Bonferroni method sorts the ''p''-values from lowest to highest and compares them to nominal alpha levels of <math>\\frac{\\alpha}{m}</math> to <math>\\alpha</math> (respectively), namely the values <math>\\frac{\\alpha}{m},  \\frac{\\alpha}{m-1},  ...  ,  \\frac{\\alpha}{2},  \\frac{\\alpha}{1}</math>.\n* The index <math>k</math> identifies the first ''p''-value that is <i>not</i> low enough to validate rejection. Therefore, the null hypotheses <math>H_{(1)}, ... , H_{(k-1)}</math> are rejected, while the null hypotheses <math> H_{(k)}, ... , H_{(m)}</math> are accepted (not rejected). \n* If <math>k = 1</math> then no ''p''-values were low enough for rejection, therefore no null hypotheses are rejected (i.e., all null hypotheses are accepted). \n* If no such index <math>k</math> could be found then all ''p''-values were low enough for rejection, therefore all null hypotheses are rejected (none are accepted).\n\n===Proof===\nHolm-Bonferroni controls the FWER as follows.\nLet <math>H_{(1)}\\ldots H_{(m)}</math> be a family of hypotheses, and <math>P_{(1)}\\leq P_{(2)}\\leq\\ldots\\leq P_{(m)}</math>  be the sorted p-values. Let <math>I_{0}</math> be the set of indices corresponding to the (unknown) true null hypotheses, having <math>m_{0}</math> members.\n\nLet us assume that we wrongly reject a true hypothesis. We have to prove that the probability of this event is at most <math>\\alpha</math>. Let <math>h</math> be the first rejected true hypothesis (first in the ordering given by the Bonferroni–Holm test). Then <math>H_{(1)}\\ldots H_{(h-1)}</math>are all rejected false hypotheses and <math>h-1 \\leq m -m_0</math>. From there, we get <math>\\frac{1}{m-h+1}\\leq \\frac{1}{m_0}</math> (1). Since <math>h</math> is rejected we have <math>P_{(h)} \\leq \\frac{\\alpha}{m-h+1}</math> by definition of the test. Using (1), the right hand side is at most <math>\\frac{\\alpha}{m_0}</math>. Thus, if we wrongly reject a true hypothesis, there has to be a true hypothesis with P-value at most <math>\\frac{\\alpha}{m_0}</math>.\n\nSo let us define the random variable <math>A=\\left\\{ P_i \\leq \\frac{\\alpha}{m_{0}} \\text{ for } i\\in I_{0}\\right\\}</math>. Whatever the (unknown) set of true hypotheses <math>I_0</math> is, we have <math>\\Pr(A)\\leq \\alpha</math> (by the [[Bonferroni inequalities#Bonferroni inequalities|Bonferroni inequalities]]). Therefore, the probability to reject a true hypothesis is at most <math>\\alpha</math>.\n\n===Alternative proof===\nThe Holm–Bonferroni method can be viewed as [[closed testing procedure]],<ref>{{cite journal |last=Marcus |first=R. |last2=Peritz |first2=E. |last3=Gabriel |first3=K. R. |year=1976 |title=On closed testing procedures with special reference to ordered analysis of variance |journal=[[Biometrika]] |volume=63 |issue=3 |pages=655–660 |doi=10.1093/biomet/63.3.655 }}</ref> with Bonferroni method applied locally on each of the intersections of null hypotheses.\nAs such, it controls the [[family-wise error rate]] for all the ''k'' hypotheses at level α in the strong sense. Each intersection is tested using the simple Bonferroni test.\n\nIt is a ''shortcut procedure'' since practically the number of comparisons to be made equal to <math>m</math> or less, while the number of all intersections of null hypotheses to be tested is of order <math>2^m</math>.\n\nThe closure principle states that a hypothesis <math>H_i</math> in a family of hypotheses <math>H_1,...,H_m</math> is rejected - while controlling the family-wise error rate of <math>\\alpha</math> - if and only if all the sub-families of the intersections with <math>H_i</math> are controlled at level of family-wise error rate of <math>\\alpha</math>.\n\nIn Holm-Bonferroni procedure, we first test <math>H_{(1)}</math>. If it is not rejected then the intersection of all null hypotheses <math>\\bigcap\\nolimits_{i = 1}^m {{H_i}}</math> is not rejected too, such that there exist at least one intersection hypothesis for each of elementary hypotheses <math>H_1,...,H_m</math> that is not rejected, thus we reject none of the elementary hypotheses.\n\nIf <math>H_{(1)}</math> is rejected at level <math>\\alpha/m</math> then all the intersection sub-families that contain it are rejected too, thus <math>H_{(1)}</math> is rejected.\nThis is because <math>P_{(1)}</math> is the smallest in each one of the intersection sub-families and the size of the sub-families is the most <math>m</math>, such that the Bonferroni threshold larger than <math>\\alpha/m</math>.\n\nThe same rationale applies for <math>H_{(2)}</math>. However, since <math>H_{(1)}</math> already rejected, it sufficient to reject all the intersection sub-families of <math>H_{(2)}</math> without <math>H_{(1)}</math>. Once <math>P_{(2)}\\leq\\alpha/(m-1)</math> holds all the intersections that contains <math>H_{(2)}</math> are rejected.\n\nThe same applies for each <math>1\\leq i \\leq m</math>.\n\n== Example ==\nConsider four null hypotheses <math>H_1,...,H_4</math> with unadjusted p-values <math>p_1=0.01</math>, <math>p_2=0.04</math>, <math>p_3=0.03</math> and <math>p_4=0.005</math>, to be tested at significance level <math>\\alpha=0.05</math>. Since the procedure is step-down, we first test <math>H_4=H_{(1)}</math>, which has the smallest p-value <math>p_4=p_{(1)}=0.005</math>. The p-value is compared to <math>\\alpha/4=0.0125</math>, the null hypothesis is rejected and we continue to the next one. Since <math>p_1=p_{(2)}=0.01<0.0167=\\alpha/3</math> we reject <math>H_1=H_{(2)}</math> as well and continue. The next hypothesis <math>H_3</math> is not rejected since <math>p_3=p_{(3)}=0.03 > 0.025=\\alpha/2</math>. We stop testing and conclude that <math>H_1</math> and <math>H_4</math> are rejected and  <math>H_2</math> and <math>H_3</math> are not rejected while controlling the family-wise error rate at level <math>\\alpha=0.05</math>. Note that even though <math>p_2=p_{(4)}=0.04 < 0.05=\\alpha</math> applies, <math>H_2</math> is '''not''' rejected. This is because the testing procedure stops once a failure to reject occurs.\n\n==Extensions==\n\n===Holm–Šidák method===\n{{further|Šidák correction}}\nWhen the hypothesis tests are not negatively dependent, it is possible to replace <math>\\frac{\\alpha}{m},\\frac{\\alpha}{m-1},...,\\frac{\\alpha}{1}</math> with:\n: <math>1-(1-\\alpha)^{1/m},1-(1-\\alpha)^{1/(m-1)},...,1-(1-\\alpha)^{1}</math>\nresulting in a slightly more powerful test.\n\n===Weighted version===\nLet <math>P_{(1)},...,P_{(m)}</math> be the ordered unadjusted p-values. Let <math>H_{(i)}</math>, <math>0\\leq w_{(i)}</math> correspond to <math>P_{(i)}</math>. Reject <math>H_{(i)}</math> as long as\n\n: <math>P_{(j)}\\leq\\frac{w_{(j)}}{\\sum^m_{k=j}{w_{(k)}}}\\alpha,\\quad j=1,...,i</math>\n\n===Adjusted ''p''-values===\nThe adjusted [[P-values|''p''-values]] for Holm–Bonferroni method are:\n: <math>\\widetilde{p}_{(i)}=\\max_{j\\leq i}\\left\\{ (m-j+1)p_{(j)}\\right\\} _{1}</math>, where <math>\\{x\\}_{1}\\equiv \\min(x,1)</math>.\n\nIn the earlier example, the adjusted ''p''-values are <math>\\widetilde{p}_1 = 0.03</math>, <math>\\widetilde{p}_2 = 0.06</math>, <math>\\widetilde{p}_3 = 0.06</math> and <math>\\widetilde{p}_4 = 0.02</math>. Only hypotheses <math>H_1</math> and <math>H_4</math> are rejected at level <math>\\alpha=0.05</math>.\n\nThe weighted adjusted ''p''-values are:{{citation needed|date=June 2012}}\n:<math>\\widetilde{p}_{(i)}=\\max_{j\\leq i}\\left\\{\\frac{\\sum^m_{k=j}{w_{(k)}}}{w_{(j)}} p_{(j)}\\right\\} _{1}</math>, where <math>\\{x\\}_{1}\\equiv \\min(x,1)</math>.\nA hypothesis is rejected at level α if and only if its adjusted ''p''-value is less than α. In the earlier example using equal weights, the adjusted ''p''-values are 0.03, 0.06, 0.06, and 0.02. This is another way to see that using α = 0.05, only hypotheses one and four are rejected by this procedure.\n\n==Alternatives and usage==\n{{main|Family-wise error rate#Controlling procedures}}\nThe Holm–Bonferroni method is \"uniformly\" more powerful than the classic [[Bonferroni correction]], meaning that it is always at least as powerful.\n\nThere are other methods for controlling the family-wise error rate that are more powerful than Holm-Bonferroni. For instance, in the [[Family-wise error rate#Hochberg.27s step-up procedure .281988.29|Hochberg procedure]], rejection of <math>H_{(1)} \\ldots H_{(k)}</math> is made after finding the ''maximal'' index <math>k</math> such that <math>P_{(k)} \\leq \\frac{\\alpha}{m+1-k}</math>. Thus, The Hochberg procedure is uniformly more powerful than the Holm procedure. However, the Hochberg procedure requires the hypotheses to be [[Independence (probability theory)|independent]] or under certain forms of positive dependence, whereas Holm-Bonferroni can be applied without such assumptions. A similar step-up procedure is the Hommel procedure, which is uniformly more powerful than the Hochberg procedure.<ref name=\"Hommel1988\">{{cite journal|last1=Hommel|first1=G.|title=A stagewise rejective multiple test procedure based on a modified Bonferroni test|journal=Biometrika|volume=75|issue=2|year=1988|pages=383–386|issn=0006-3444|doi=10.1093/biomet/75.2.383}}</ref>\n\n==Naming==\nCarlo Emilio Bonferroni did not take part in inventing the method described here. Holm originally called the method the \"sequentially rejective Bonferroni test\", and it became known as Holm-Bonferroni only after some time. Holm's motives for naming his method after Bonferroni are explained in the original paper:\n''\"The use of the Boole inequality within multiple inference theory is usually called the Bonferroni technique, and for this reason we will call our test the sequentially rejective Bonferroni test.\"''\n\n==References==\n<references/>\n\n{{DEFAULTSORT:Holm-Bonferroni Method}}\n[[Category:Statistical hypothesis testing]]\n[[Category:Statistical tests]]\n[[Category:Multiple comparisons]]"
    },
    {
      "title": "Look-elsewhere effect",
      "url": "https://en.wikipedia.org/wiki/Look-elsewhere_effect",
      "text": "The '''look-elsewhere effect''' is a [[phenomenon]] in the statistical analysis of [[scientific experiment]]s where an apparently [[statistically significant]] observation may have actually arisen by chance because of the sheer size of the [[parameter space]] to be searched.<ref>{{Cite journal | last1 = Lyons | first1 = L. | title = Open statistical issues in Particle Physics | doi = 10.1214/08-AOAS163 | journal = The Annals of Applied Statistics | volume = 2 | issue = 3 | pages = 887 | year = 2008 | pmid =  | pmc = | arxiv = 0811.1663 }}</ref><ref>{{cite web|url=http://physics.aps.org/synopsis-for/10.1103/PhysRevLett.107.101801|title=Synopsis: Controlling for the \"look-elsewhere effect\"|publisher=American Physical Society|date=2011}}</ref><ref>{{cite web|url=https://news.slac.stanford.edu/features/word-week-look-elsewhere-effect|title=Word of the Week: Look Elsewhere Effect|publisher=Stanford National Accelerator Laboratory|date=August 12, 2011|author=Lori Ann White|deadurl=yes|archiveurl=https://web.archive.org/web/20120419212455/https://news.slac.stanford.edu/features/word-week-look-elsewhere-effect|archivedate=April 19, 2012|df=}}</ref><ref>{{cite web|url=http://www.science20.com/quantum_diaries_survivor/supernatural_coincidences_and_lookelsewhere_effect|title=Supernatural Coincidences And The Look-Elsewhere Effect|date=2009-10-16|first=Tommaso|last=Dorigo|accessdate=2012-10-17}}</ref><ref>{{cite web|url=http://cms.web.cern.ch/news/should-you-get-excited-your-data-let-look-elsewhere-effect-decide|title=Should you get excited by your data? Let the Look-Elsewhere Effect decide|publisher=CMS Collaboration|date=2011-08-19|first=Tommaso|last=Dorigo}}</ref>\n\nOnce the possibility of look-elsewhere error in an analysis is acknowledged, it can be compensated for by careful application of standard mathematical techniques.<ref>{{Cite journal | last1 = Gross | first1 = E. | last2 = Vitells | first2 = O. | doi = 10.1140/epjc/s10052-010-1470-8 | title = Trial factors for the look elsewhere effect in high energy physics | journal = The European Physical Journal C | volume = 70 | pages = 525 | year = 2010 | pmid =  | pmc = |arxiv = 1005.1891 |bibcode = 2010EPJC...70..525G }}</ref>\n\nMore generally known in statistics as the [[problem of multiple comparisons]], the term gained some media attention in 2011, in the context of the search for the [[Higgs boson]] at the [[Large Hadron Collider]].<ref>{{cite web|url=http://blogs.telegraph.co.uk/news/tomchiversscience/100123873/an-unconfirmed-sighting-of-the-elusive-higgs-boson/|title=An unconfirmed sighting of the elusive Higgs boson|author=Tom Chivers|date=2011-12-13|publisher=Daily Telegraph}}</ref>\n\n==Use==\n{{Main|Bonferroni correction}}\nMany statistical tests deliver a [[p-value]], the probability that a given result could be obtained, assuming random coincidence. When asking \"does ''X'' affect ''Y''?\", it is common to vary ''X'' and see if there is significant variation in ''Y'' as a result. If this p-value is less than some predetermined [[statistical significance]] threshold ''α'', one considers the result \"significant\".\n\nHowever, if one is performing multiple tests (\"looking elsewhere\" if the first test fails) then a ''p'' value of 1/''n'' is expected to occur after about ''n'' tests.  For example, when there is no real effect, an event with ''p''&nbsp;&lt;&nbsp;0.05 will on average still be seen after 20 tests. In order to compensate for this, you could divide your threshold ''α'' by the number of tests ''n'', so a result is significant when ''p'' &lt; ''α''/''n''.  Or, equivalently, multiply the observed ''p'' value by the number of tests (significant when ''np'' &lt; ''α'').\n\nThis is a simplified case; the number ''n'' is actually the number of [[degrees of freedom (statistics)|degrees of freedom]] in the tests, or the number of effectively independent tests.  If they are not fully independent, the number may be lower than the number of tests.\n\nThe look-elsewhere effect is a frequent cause of \"significance inflation\" when the number of independent tests ''n'' is underestimated because failed tests are not published.  One paper may fail to mention alternative hypotheses considered, or a paper producing no result may simply not be published at all, leading to journals dominated by statistical outliers.\n\n== Examples ==\n* A Swedish study in 1992 tried to determine whether or not power lines caused some kind of poor health effects. The researchers surveyed everyone living within 300 meters of high-voltage power lines over a 25-year period and looked for statistically significant increases in rates of over 800 ailments. The study found that the incidence of childhood leukemia was four times higher among those that lived closest to the power lines, and it spurred calls to action by the Swedish government. The problem with the conclusion, however, was that they failed to compensate for the look-elsewhere effect; in any collection of 800 random samples, it is likely that at least one will be at least 3 standard deviations above the expected value, by chance alone.  Subsequent studies failed to show any links between power lines and childhood leukemia, neither in causation nor even in correlation.<ref>{{Citation |url=https://www.pbs.org/wgbh/pages/frontline/programs/transcripts/1319.html |title=Currents of fear |date=1995-06-13 |first=Jon |last=Palfreman |authorlink=Jon Palfreman|journal=[[Frontline (U.S. TV series)|Frontline]] |publisher=[[PBS]] |accessdate=2012-07-01}}</ref>\n* The [[Bible Code]] phenomenon purports to find atypical significant groupings of words predicting future events hidden in text of the [[Hebrew Bible]] taken as a raw sequence of unspaced letters and arranged into various grids of different proportions.  However, as an article in ''[[Skeptical Inquirer]]'' demonstrated,<ref>{{Citation |url=http://www.csicop.org/si/show/hidden_messages_and_the_bible_code |title=Hidden Messages and The Bible Code |date=1997-11-01 |first=Dave |last=Thomas |journal=[[Skeptical Inquirer]] |publisher=[[CSICOP]] |accessdate=2015-04-19}}</ref> this amounts to generating vast numbers of grids to examine for patterns or groupings by dividing the full text string into widths of from a few to hundreds of thousands of letters wide, repeating the width for subsequent rows.  Each one of those many grids can then in turn be searched further for a wide range of words of interest by skipping in intervals, forward or backward, of an arbitrary x letters in the text (or x+1, x+2, etc.), in a massive [[cross product]] of [[parameter]]ized possibilities, and an associated coincident word of interest can be any nearby string in an arbitrary skip of x+k or y+k letters, forward or backward, such that the permutational volumes become enormous.  Thus, setting aside related questions like [[confirmation bias]], even if no groupings of interest or significance were found in the first grid, the next iteration can be tried by computer and so on en masse until \"miraculous\" or \"improbable\" groupings are finally arrived at.  This is tantamount in effect to, upon dealing oneself an uninteresting [[poker]] hand, continuing to do so in whatever great quantities necessary until one obtains a [[straight flush]], [[Royal flush (poker hand)|royal flush]], or even many such events in sequence, and calling the deck inspired for enabling such a result.  The Skeptical Inquirer author was thus able to achieve identical effects simply by applying the same search algorithms both to the English language [[King James Bible]] text in place of the allegedly divinely inspired Hebrew version, and then just as effectively to the mundane and arbitrary example text of the 1987 [[United States Supreme Court]] decision ''[[Edwards v. Aguillard]]''.\n\n==See also==\n*[[Bonferroni correction]]\n*[[Data dredging]]\n*[[Law of truly large numbers]]: with a sample size large enough, any outrageous thing is likely to happen\n*[[Littlewood's law]]: any individual can expect a \"miracle\" to happen to them at the rate of about one per month\n*[[Texas sharpshooter fallacy]]\n*[[Multiple comparisons problem]]\n\n==References==\n{{reflist|30em}}\n\n==External links==\n* [http://xkcd.com/882/ XKCD comic illustrating the Look-Elsewhere effect]\n\n[[Category:Multiple comparisons]]"
    },
    {
      "title": "Newman–Keuls method",
      "url": "https://en.wikipedia.org/wiki/Newman%E2%80%93Keuls_method",
      "text": "The '''Newman–Keuls''' or '''Student–Newman–Keuls (SNK)''' method is a stepwise [[multiple comparisons]] procedure used to identify [[Sample (statistics)|sample]] [[Arithmetic mean|means]] that are [[Statistical significance|significantly]] different from each other.<ref name=Muth>{{cite book |last1 = De Muth |first1 = James E. |title = Basic Statistics and Pharmaceutical Statistical Applications |edition=2nd |publisher = Chapman and Hall/CRC |location = Boca Raton, FL |year = 2006 |isbn = 978-0-8493-3799-4 |pages=229–259}}</ref> It was named after [[William Sealy Gosset|Student]] (1927),<ref name=Student>{{cite journal|author=Student|title=Errors of routine analysis|journal=Biometrika|volume=19|issue=1/2|year=1927|pages=151–164|doi=10.2307/2332181|url=http://biomet.oxfordjournals.org/content/19/1-2/151.full.pdf+html|jstor=2332181}}</ref> D. Newman,<ref name=Newman>{{cite journal |last=Newman |first=D. |title=The distribution of range in samples from a normal population, expressed in terms of an independent estimate of standard deviation|journal=Biometrika|volume=31|issue=1|year=1939|pages=20–30|doi=10.1093/biomet/31.1-2.20|url=http://biomet.oxfordjournals.org/content/31/1-2/20.full.pdf+html}}</ref> and M. Keuls.<ref name=Keuls>{{cite journal|last=Keuls |first=M. |title=The use of the \"studentized range\" in connection with an analysis of variance|journal=Euphytica|volume=1|issue=2 |year=1952|pages=112–122|doi=10.1007/bf01908269|url=http://www.wias-berlin.de/people/dickhaus/downloads/MultipleTests-SoSe-2010/keuls1952.pdf|deadurl=yes|archiveurl=https://web.archive.org/web/20141104043438/http://www.wias-berlin.de/people/dickhaus/downloads/MultipleTests-SoSe-2010/keuls1952.pdf|archivedate=2014-11-04|df=}}</ref> This procedure is often used as a [[post-hoc analysis|post-hoc test]] whenever a significant difference between three or more sample means has been revealed by an [[analysis of variance|analysis of variance (ANOVA)]].<ref name=Muth /> The Newman–Keuls method is similar to [[Tukey's range test]] as both procedures use [[studentized range|studentized range statistics]].<ref name=Broota>{{cite book |last1 = Broota |first1 = K. D. |title = Experimental Design in Behavioural Research |edition=1st |publisher = New Age International (P) Ltd. |location = New Delhi, India |year = 1989 |isbn = 978-81-224-0215-5 |pages=81–96}}</ref><ref name=Sheskin>{{cite book |last1 = Sheskin |first1 = David J. |title = Handbook of Parametric and Nonparametric Statistical Procedures |edition=3rd |publisher = CRC Press |location = Boca Raton, FL |year = 1989 |isbn = 978-1-58488-440-8 |pages=665–756}}</ref> Unlike Tukey's range test, the Newman–Keuls method uses different [[Critical value#Statistics|critical value]]s for different pairs of mean comparisons. Thus, the procedure is more likely to reveal significant differences between group means and to commit [[type I errors]] by incorrectly rejecting a null hypothesis when it is true. In other words, the Neuman-Keuls procedure is more [[Statistical power|powerful]] but less conservative than Tukey's range test.<ref name=\"Sheskin\"/><ref name=\"Roberts and Russo\">{{cite book |last1 = Roberts |first1 = Maxwell | last2 = Russo | first2 = Riccardo | chapter = Following up a one-factor between-subjects ANOVA | title = A Student's Guide to Analysis of Variance | year = 1999 | edition= |publisher = J&L Composition Ltd. |location = Filey, United Kingdom | isbn = 978-0-415-16564-8 |pages=82–109}}</ref>\n\n==History==\n\nThe Newman–Keuls method was introduced by Newman in 1939 and developed further by Keuls in 1952. This before [[John Tukey|Tukey]] presented the concept of different types of multiple error rates (1952a,<ref name=1952a>{{cite journal|author=Tukey, J. W. |title=Reminder sheets for Allowances for various types of error rates. Unpublished manuscript |journal=Brown, 1984|year=1952a}}</ref> 1952b,<ref name=1952b>{{cite journal|author=Tukey, J. W. |title=Reminder sheets for Multiple comparisons. Unpublished manuscript |journal=Brown, 1984|year=1952b}}</ref> 1953<ref name=1953a>{{cite journal|author=Tukey, J. W. |title=The problem of multiple comparisons. Unpublished manuscript |journal=Brown, 1984|year=1953}}</ref>).\nThe Newman–Keuls method was popular during 1950s and 1960s{{citation needed|date=October 2014}}. But when the control of [[familywise error rate]] (FWER) became an accepted criterion in multiple comparison testing, the procedure became less popular{{citation needed|date=October 2014}} as it does not control FWER (except for the special case of exactly three groups<ref name=groups>{{cite journal|author1=M. A. Seaman |author2=J. R. Levin  |author3=R. C. Serlin |lastauthoramp=yes |title=New Developments in pairwise multiple comparisons: Some powerful and practicable procedures |journal=Psychological Bulletin |year=1991|pages=577–586|url=http://psycnet.apa.org/journals/bul/110/3/577.pdf|doi=10.1037/0033-2909.110.3.577 |volume=110 |issue=3}}</ref>).\nIn 1995 Benjamini and Hochberg presented a new, more liberal and more powerful criterion for those types of problems: [[False discovery rate]] (FDR) control.<ref name=control>{{cite journal|author=Benjamini, Y., Hochberg, Y. |title=Controlling the false discovery rate: a new and powerful approach to multiple testing |url=http://engr.case.edu/ray_soumya/mlrg/controlling_fdr_benjamini95.pdf|journal=JRSS, Series B, Methodological 57 |year=1995|pages=289–300}}</ref> In 2006, Shaffer showed (by extensive simulation) that the Newman–Keuls method controls the FDR with some constrains.<ref name=constrains>{{cite journal|author=Shaffer, Juliet P|title=Controlling the false discovery rate with constraints: The Newman–Keuls test revisited |journal=Biometrical Journal |volume=49|issue=1 |year=2007|pages=136–143|pmid=17342955|doi=10.1002/bimj.200610297}}</ref>\n\n==Required assumptions==\n\nThe assumptions of the Newman–Keuls test are essentially the same as for an independent groups [[t-test]]: normality, homogeneity of variance, and independent observations. The test is quite robust to violations of normality. Violating homogeneity of variance can be more problematic than in the two-sample case since the MSE is based on data from all groups. The assumption of independence of observations is important and should not be violated.\n\n==Procedures==\n\nThe Newman–Keuls method employs a stepwise approach when comparing sample means.<ref name=Toothaker>{{cite book |last1 = Toothaker |first1 = Larry E. |title = Multiple Comparison Procedures (Quantitative Applications in the Social Sciences) |edition=2nd |publisher = Chapman and Hall/CRC |location = Newburry Park, CA |year = 1993 |isbn = 978-0-8039-4177-9 |pages=27–45}}</ref> Prior to any mean comparison, all sample means are rank-ordered in ascending or descending order, thereby producing an ordered range (''p'') of sample means.<ref name=Muth /><ref name=Toothaker /> A comparison is then made between the largest and smallest sample means within the largest range.<ref name=Toothaker /> Assuming that the largest range is four means (or ''p'' = 4), a significant difference between the largest and smallest means as revealed by the Newman–Keuls method would result in a rejection of the [[null hypothesis]] for that specific range of means. The next largest comparison of two sample means would then be made within a smaller range of three means (or ''p'' = 3). Unless there is no significant differences between two sample means within any given range, this stepwise comparison of sample means will continue until a final comparison is made with the smallest range of just two means. If there is no significant difference between the two sample means, then all the null hypotheses within that range would be retained and no further comparisons within smaller ranges are necessary.\n\n{| class=\"wikitable\"\n|+ Range of sample means\n|-\n!\n! <math>\\bar{X}_1</math>\n! <math>\\bar{X}_2</math>\n! <math>\\bar{X}_3</math>\n! <math>\\bar{X}_4</math>\n|-\n! Mean values || 2 || 4 || 6 || 8\n|-\n!<math>\\bar{X}_1 =</math> 2\n|\n| 2\n| 4\n| 6\n|-\n!<math>\\bar{X}_2 =</math> 4\n| \n|\n| 2\n| 4\n|-\n!<math>\\bar{X}_3 =</math> 6\n|\n|\n|\n| 2\n|-\n|}\nTo determine if there is a significant difference between two means with equal sample sizes, the Newman–Keuls method uses a formula that is identical to the one used in [[Tukey's range test#The test statistic|Tukey's range test]], which calculates the ''q'' value by taking the difference between two sample means and dividing it by the standard error:\n\n:<math> q = \\frac{\\bar{X}_A - \\bar{X}_B}\\sqrt{\\frac{MSE}{n}}, </math>\n\nwhere <math>q</math> represents the [[studentized range]] value, <math>\\bar{X}_A</math> and <math>\\bar{X}_B</math> are the largest and smallest sample means within a range, <math>MSE</math> is the error variance taken from the ANOVA table, and <math>n</math> is the sample size (number of observations within a sample). If comparisons are made with means of unequal sample sizes (<math>{n_A}\\neq{n_B}</math>), then the Newman–Keuls formula would be adjusted as follows:\n\n:<math> q = \\frac{\\bar{X}_A - \\bar{X}_B}\\sqrt{\\frac{MSE}{2}(\\frac{1}{n_A} + \\frac{1}{n_B})}, </math>\n\nwhere <math>n_A</math> and <math>n_B</math> represent the sample sizes of the two sample means.  On both cases, [[Mean squared error|MSE]] (Mean squared error) is taken from the ANOVA conducted in the first stage of the analysis.\n\nOnce calculated, the computed ''q'' value can be compared to a ''q'' critical value (or <math>q_\\alpha\\,_\\nu\\,_p</math>), which can be found in a ''q'' distribution table based on the [[Statistical significance|significance level]] (<math>\\alpha</math>), the error [[Degrees of freedom (statistics)|degrees of freedom]] (<math>\\nu</math>) from the ANOVA table, and the range (<math>p</math>) of sample means to be tested.<ref name=Zar /> If the computed ''q'' value is equal to or greater than the ''q'' critical value, then the null hypothesis (''H''<sub>0</sub>: ''μ''<sub>A</sub> = ''μ''<sub>B</sub>) for that specific range of means can be rejected.<ref name=Zar>{{cite book |last1 = Zar |first1 = Jerrold H. |title = Biostatistical Analysis |edition=4th |publisher = Prentice Hall |location = Newburry Park, CA |year = 1999 |isbn = 978-0-13-081542-2 |pages=208–230}}</ref> Because the number of means within a range changes with each successive pairwise comparison, the critical value of the ''q'' statistic also changes with each comparison, which makes the Neuman-Keuls method more lenient and hence more powerful than Tukey's range test. Thus, if a pairwise comparison was found to be significantly different using the Newman–Keuls method, it may not necessarily be significantly different when analyzed with Tukey's range test.<ref name=\"Roberts and Russo\" /><ref name=Zar /> Conversely, if the pairwise comparison was found not to be significantly different using the Newman–Keuls method, it cannot in any way be significantly different when tested with Tukey's range test.<ref name=\"Roberts and Russo\" />\n\n==Limitations==\n\nThe Newman–Keuls procedure cannot produce a confidence interval for each mean difference, or for multiplicity adjusted exact p-values due to its sequential nature.{{citation needed|date=October 2014}} Results are somewhat difficult to interpret since it is difficult to articulate what are the null hypotheses that were tested.{{citation needed|date=October 2014}}\n\n==See also==\n*[[Multiple comparisons]]\n*[[Post-hoc analysis]]\n*[[Tukey's range test]]\n\n==References==\n{{reflist|30em}}\n\n{{DEFAULTSORT:Newman-Keuls method}}\n[[Category:Multiple comparisons]]"
    },
    {
      "title": "Post hoc analysis",
      "url": "https://en.wikipedia.org/wiki/Post_hoc_analysis",
      "text": "{{Short description|Statistical analyses that were not specified before the data were seen}}\n{{Distinguish|Post hoc theorizing}}\nIn a scientific study, '''post hoc analysis''' (from [[Latin language|Latin]] ''[[post hoc (disambiguation)|post hoc]]'', \"after this\") consists of [[statistics|statistical analyses]] that were not specified before the data was seen. This typically creates a [[multiple testing]] problem because each potential analysis is effectively a [[Statistical hypothesis testing|statistical test]]. Multiple testing procedures are sometimes used to compensate, but that is often difficult or impossible to do precisely. Post hoc analysis that is conducted and interpreted without adequate consideration of this problem is sometimes called ''[[data dredging]]'' by critics because the statistical associations that it finds are often spurious. \n\n== Causes ==\nSometimes the temptation to engage in post hoc analysis is motivated by a desire to produce positive results or see a project as successful.  In the case of pharmaceutical research, there may be significant financial consequences to a failed trial, although the US [[Food and Drug Administration]] does not accept post hoc analysis.<ref name=\":0\">{{Cite news|url=https://www.nytimes.com/2017/11/28/magazine/a-failure-to-heal.html|title=A Failure to Heal|last=Mukherjee|first=Siddhartha|date=2017-11-28|work=The New York Times|access-date=2017-12-02|language=en-US|issn=0362-4331}}</ref>  \n\nIn some cases, additional [[subgroup analysis]] may be requested by scientific peers or the editors of academic journals.  In one such incident, journal editors demanded that the statistician [[Richard Peto]] provide a post hoc analysis of subgroups for the use of aspirin as [[secondary prevention]] for people who had experienced heart attacks.  He refused the request as being statistically unsound and likely to lead to nonsensical results.  When they persisted, he provided the editors with a subgroup analysis that evaluated the supposed response based upon the patients' [[Astrological sign|astrological signs]].<ref name=\":0\" /><ref>[[Richard Peto]], \"Current misconception 3: that subgroup-specific trial mortality results often provide a good basis for individualising patient care\", ''[[Br J Cancer]]'', 104(7), pages 1057-1058 (2011). {{doi|10.1038/bjc.2011.79}}</ref>\n\n== See also ==\n*[[Testing hypotheses suggested by the data]]\n\n==References==\n<references />\n\n[[Category:Data analysis]]\n[[Category:Multiple comparisons]]\n[[Category:Clinical research]]\n[[Category:Medical statistics]]"
    },
    {
      "title": "Q-value (statistics)",
      "url": "https://en.wikipedia.org/wiki/Q-value_%28statistics%29",
      "text": "{{short description|Statistical hypothesis testing measure}}\nIn [[Statistical hypothesis testing|statistical hypothesis testing]], specifically [[Multiple comparisons problem|multiple hypothesis testing]], the '''''q''-value''' provides a means to control the [[False discovery rate#Related error rates|positive false discovery rate]] (pFDR).<ref name=\":0\">{{Cite journal|last=Storey|first=John D.|date=2003|title=The positive false discovery rate: a Bayesian interpretation and the q-value|journal=The Annals of Statistics|volume=31|issue=6|pages=2013–2035|doi=10.1214/aos/1074290335}}</ref> Just as the [[P-value|''p''-value]] gives the expected [[False positive rate|false positive rate]] obtained by rejecting the [[null hypothesis]] for any result with an equal or smaller ''p''-value, the ''q''-value gives the expected pFDR obtained by rejecting the null hypothesis for any result with an equal or smaller ''q''-value.\n\n== History ==\nIn statistics, testing multiple hypotheses simultaneously using methods appropriate for testing single hypotheses tends to yield many false positives: the so-called [[Multiple comparisons problem|multiple comparisons problem]].<ref name=\":2\" /> For example, assume that one were to test 1,000 null hypotheses, all of which are true, and (as is conventional in single hypothesis testing) to reject null hypotheses with a [[Significance level|significance level]] of 0.05; due to random chance, one would expect 5% of the results to appear significant (''[[P-value|P]]'' < 0.05), yielding 50 false positives (rejections of the null hypothesis).<ref>{{Cite news|url=https://www.nature.com/news/scientific-method-statistical-errors-1.14700|title=Scientific method: Statistical errors|last=Nuzzo|first=Regina|date=2014|work=Nature|access-date=5 March 2019}}</ref> Since the 1950s, statisticians had been developing methods for multiple comparisons that reduced the number of false positives, such as controlling the [[Family-wise error rate|family-wise error rate]] (FWER) using the [[Bonferroni correction]], but these methods also increased the number of false negatives (i.e. reduced the [[Statistical power|statistical power]]).<ref name=\":2\" /> In 1995, [[Yoav Benjamini]] and Yosef Hochberg proposed controlling the [[false discovery rate]] (FDR) as a more statistically powerful alternative to controlling the FWER in multiple hypothesis testing.<ref name=\":2\">{{Cite journal|last=Benjamini|first=Yoav|last2=Hochberg|first2=Yosef|date=1995|title=Controlling the false discovery rate: a practical and powerful approach tomultiple testing|journal=Journal of the Royal Statistical Society. Series B (Methodological)|volume=57|pages=289–300|doi=10.1111/j.2517-6161.1995.tb02031.x}}</ref> The pFDR and the ''q-''value were introduced by John D. Storey in 2002 in order to improve upon a limitation of the FDR, namely that the FDR is not defined when there are no positive results.<ref name=\":0\" /><ref>{{Cite journal|last=Storey|first=John D.|date=2002|title=A direct approach to false discovery rates|journal=Journal of the Royal Statistical Society: Series B (Statistical Methodology)|volume=64|issue=3|pages=479–498|doi=10.1111/1467-9868.00346|citeseerx=10.1.1.320.7131}}</ref>\n\n== Definition ==\nLet there be a null hypothesis <math>H_0</math> and an [[Alternative hypothesis|alternative hypothesis]] <math>H_1</math>. Perform <math>m</math> hypothesis tests; let the [[Test statistic|test statistics]] be [[Independent and identically distributed random variables|i.i.d. random variables]] <math>T_1, \\ldots, T_m</math> such that <math>T_i \\mid H_i \\sim (1 - H_i) \\cdot F_0 + H_i \\cdot F_1</math>. That is, if <math>H_0</math> is true for test <math>i</math> (<math>H_i = 0</math>), then <math>T_i</math> follows the [[null distribution]]  <math>F_0</math>; while if <math>H_1</math> is true (<math>H_i = 1</math>), then <math>T_i</math> follows the alternative distribution <math>F_1</math>. Let <math>H_i \\sim \\operatorname{Bernoulli}(\\pi_1)</math>, that is, for each test, <math>H_1</math> is true with probability <math>\\pi_1</math> and <math>H_0</math> is true with probability <math>\\pi_0 = 1 - \\pi_1</math>. Denote the [[Rejection region|critical region]] (the values of <math>T_i</math> for which <math>H_0</math> is rejected) at [[significance level]] <math>\\alpha</math> by <math>\\Gamma_\\alpha</math>. Let an experiment yield a value <math>t</math> for the test statistic. The ''q''-value of <math>t</math> is formally defined as\n\n: <math>\\inf_{\\{\\Gamma_\\alpha : t \\in \\Gamma_\\alpha\\}} \\operatorname{pFDR}(\\Gamma_\\alpha)</math>\n\nThat is, the ''q''-value is the [[Infimum and supremum|infimum]] of the pFDR if <math>H_0</math>is rejected for test statistics with values <math>\\ge t</math>. Equivalently, the ''q''-value equals\n\n: <math>\\inf_{\\{\\Gamma_\\alpha : t \\in \\Gamma_\\alpha\\}}\\Pr(H = 0 \\mid T \\in \\Gamma_\\alpha)</math>\n\nwhich is the infimum of the probability that <math>H_0</math> is true given that <math>H_0</math> is rejected (the [[False discovery rate|false discovery rate]]).<ref name=\":0\" />\n\n== Relationship to the ''p''-value ==\nThe ''p''-value is defined as\n\n: <math>\\inf_{\\{\\Gamma_\\alpha : t \\in \\Gamma_\\alpha\\}} \\Pr(T \\in \\Gamma_\\alpha \\mid H = 0)</math>\n\nthe infimum of the probability that <math>H_0</math> is rejected given that <math>H_0</math> is true (the [[false positive rate]]). Comparing the definitions of the ''p''- and ''q''-values, it can be seen that the ''q''-value is the minimum [[posterior probability]] that <math>H_0</math> is true.<ref name=\":0\" />\n\n== Interpretation ==\nThe ''q''-value can be interpreted as the false discovery rate (FDR): the proportion of false positives among all positive results. Given a set of test statistics and their associated ''q''-values, rejecting the null hypothesis for all tests whose ''q''-value is less than or equal to some threshold <math>\\alpha</math> ensures that the expected value of the false discovery rate is <math>\\alpha</math>.<ref name=\":1\">{{Cite journal|last=Storey|first=John D.|last2=Tibshirani|first2=Robert|date=2003|title=Statistical significance for genomewide studies|journal=PNAS|volume=100|issue=16|pages=9440–9445|doi=10.1073/pnas.1530509100|pmid=12883005|pmc=170937|bibcode=2003PNAS..100.9440S}}</ref>\n\n== Applications ==\n\n=== Biology ===\n\n==== Gene expression ====\n[[Gene expression profiling|Genome-wide analyses of differential gene expression]] involve simultaneously testing the [[Gene expression|expression]] of thousands of genes. Controlling the FWER (usually to 0.05) avoids excessive false positives (i.e. detecting differential expression in a gene that is not differentially expressed) but imposes a strict threshold for the ''p''-value that results in many false negatives (many differentially expressed genes are overlooked). However, controlling the pFDR by selecting genes with significant ''q''-values lowers the number of false negatives (increases the statistical power) while ensuring that the expected value of the proportion of false positives among all positive results is low (e.g. 5%).<ref name=\":1\" />\n\nFor example, suppose that among 10,000 genes tested, 1,000 are actually differentially expressed and 9,000 are not:\n\n* If we consider every gene with a ''p''-value of less than 0.05 to be differentially expressed, we expect that 450 (5%) of the 9,000 genes that are not differentially expressed will appear to be differentially expressed (450 false positives).\n* If we control the FWER to 0.05, there is only a 5% probability of obtaining at least one false positive. However, this very strict criterion will reduce the power such that few of the 1,000 genes that are actually differentially expressed will appear to be differentially expressed (many false negatives).\n* If we control the pFDR to 0.05 by considering all genes with a ''q''-value of less than 0.05 to be differentially expressed, then we expect 5% of the positive results to be false positives (e.g. 900 true positives, 45 false positives, 100 false negatives, 8,955 true negatives). This strategy enables one to obtain relatively low numbers of both false positives and false negatives.\n\n== Implementations ==\nNote: the following is an incomplete list.\n\n=== R ===\n\n* The [https://www.bioconductor.org/packages/release/bioc/html/qvalue.html qvalue] package in [[R (programming language)|R]] estimates ''q''-values from a list of ''p''-values.<ref>{{Cite web|url=https://www.bioconductor.org/packages/release/bioc/html/qvalue.html|title=qvalue: Q-value estimation for false discovery rate control|last=Storey|first=John D.|last2=Bass|first2=Andrew J.|date=2019|website=Bioconductor|archive-url=|archive-date=|dead-url=|access-date=|last3=Dabney|first3=Alan|last4=Robinson|first4=David|last5=Warnes|first5=Gregory}}</ref>\n\n== References ==\n{{reflist}}\n\n[[Category:Multiple comparisons]]\n[[Category:Statistical hypothesis testing]]"
    },
    {
      "title": "Scheffé's method",
      "url": "https://en.wikipedia.org/wiki/Scheff%C3%A9%27s_method",
      "text": "In [[statistics]], '''Scheffé's method''', named after the [[United States|American]] [[statistician]] [[Henry Scheffé]], is a method for adjusting [[statistical significance|significance levels]] in a [[linear regression]] analysis to account for [[multiple comparisons]].  It is particularly useful in [[analysis of variance]] (a special case of regression analysis), and in constructing simultaneous [[confidence band]]s for regressions involving [[basis functions]].\n\nScheffé's method is a single-step multiple comparison procedure which applies to the set of estimates of all possible [[contrast (statistics)|contrast]]s among the factor level means, not just the pairwise differences considered by the [[Tukey–Kramer method]]. It works on similar principles as the [[Working–Hotelling procedure]] for estimating mean responses in regression, which applies to the set of all possible factor levels.\n\n==The method==\n\nLet ''μ''<sub>1</sub>,&nbsp;...,&nbsp;''μ''<sub>''r''</sub> be the [[mean]]s of some variable in ''r'' disjoint populations.\n\nAn arbitrary contrast is defined by\n\n:<math>C = \\sum_{i=1}^r c_i\\mu_i</math>\n\nwhere\n\n:<math>\\sum_{i=1}^r c_i = 0.</math>\n\nIf ''μ''<sub>1</sub>,&nbsp;...,&nbsp;''μ''<sub>''r''</sub> are all equal to each other, then all contrasts among them are&nbsp;0.  Otherwise, some contrasts differ from&nbsp;0.\n\nTechnically there are infinitely many contrasts. The simultaneous confidence coefficient is exactly 1&nbsp;−&nbsp;α, whether the factor level sample sizes are equal or unequal. (Usually only a finite number of comparisons are of interest. In this case, Scheffé's method is typically quite conservative, and the [[family-wise error rate]] <nowiki/>(experimental error rate) will generally be much smaller than&nbsp;α.)<ref name=\"MaxwellDelaney\">{{cite book |first=Scott E. |last=Maxwell |first2=Harold D. |last2=Delaney |title=Designing Experiments and Analyzing Data: A Model Comparison |publisher=Lawrence Erlbaum Associates |year=2004 |isbn=0-8058-3718-3 |pages=217–218 }}</ref><ref name=\"MillikenJohnson\">{{cite book |first=George A. |last=Milliken |first2=Dallas E. |last2=Johnson |title=Analysis of Messy Data |publisher=CRC Press |year=1993 |isbn=0-412-99081-4 |pages=35–36 }}</ref>\n\nWe estimate ''C'' by\n:<math>\\hat{C} = \\sum_{i=1}^r c_i\\bar{Y}_i</math>\n\nfor which the estimated variance is\n:<math>s_{\\hat{C}}^2 = \\hat{\\sigma}_e^2\\sum_{i=1}^r \\frac{c_i^2}{n_i},</math>\n\nwhere\n\n* ''n''<sub>''i''</sub> is the size of the sample taken from the ''i''th population (the one whose mean is&nbsp;''μ''<sub>''i''</sub>), and\n* <math>\\hat{\\sigma}_e^2</math> is the estimated variance of the [[errors and residuals in statistics|errors]].\n\nIt can be shown that the probability is 1&nbsp;−&nbsp;α that all confidence limits of the type\n\n:<math>\\hat{C}\\pm\\,s_\\hat{C}\\sqrt{\\left(r-1\\right)F_{\\alpha;r-1;N-r}}  </math>\n\nare simultaneously correct, where as usual N is the size of the whole population. Draper and Smith, in their 'Applied Regression Analysis' (see references), indicate that 'r' should be in the equation in place of 'r-1'. The slip with 'r-1' is a result of failing to allow for the additional effect of the constant term in many regressions. That the result based on 'r-1' is wrong is readily seen by considering r = 2, as in a standard simple linear regression. That formula would then reduce to one with the usual t distribution, which is appropriate for predicting/estimating for a single value of the independent variable, not for constructing a confidence band for a range of values of the independent value. Also note that the formula is for dealing with the mean values for a range of independent values, not for comparing with individual values such as individual observed data values.<ref>{{Cite book|title=Applied Regression Analysis|last=Draper|first=Norman R|last2=Smith|first2=Harry|publisher=John Wiley and Sons, Inc.|year=|isbn=9780471170822|edition=2nd|location=|page=93}}</ref>\n\n==Denoting Scheffé significance in a table==\n\nFrequently, superscript letters are used to indicate which values are significantly different using the Scheffé method. For example, when mean values of variables that have been analyzed using an [[ANOVA]] are presented in a table, they are assigned a different letter superscript based on a Scheffé contrast. Values that are not significantly different based on the post-hoc Scheffé contrast will have the same superscript and values that are significantly different will have different superscripts (i.e. 15a, 17a, 34b would mean that the first and second variables both differ from the third variable but not each other because they are both assigned the superscript \"a\").{{citation needed|date=August 2012}}\n\n==Comparison with the Tukey–Kramer method==\nIf only a fixed number of pairwise comparisons are to be made, the [[Tukey–Kramer method]] will result in a more precise confidence interval. In the general case when many or all contrasts might be of interest, the Scheffé method is more appropriate and will give narrower confidence intervals in the case of a large number of comparisons.\n\n==References==\n{{reflist}}\n\n* {{cite journal |last=Bohrer |first=Robert |year=1967 |title=On Sharpening Scheffé Bounds |journal=[[Journal of the Royal Statistical Society]] |series=Series B |volume=29 |issue=1 |pages=110–114 |jstor=2984571 }}\n* {{cite book |last=Scheffé |first=H. |origyear=1959 |title=The Analysis of Variance |publisher=Wiley |location=New York |year=1999 |isbn=0-471-34505-9 }}\n\n==External links==\n*[http://www.itl.nist.gov/div898/handbook/prc/section4/prc472.htm Scheffé's method]\n\n{{NIST-PD}}\n\n{{DEFAULTSORT:Scheffe's method}}\n[[Category:Multiple comparisons]]"
    },
    {
      "title": "Šidák correction",
      "url": "https://en.wikipedia.org/wiki/%C5%A0id%C3%A1k_correction",
      "text": "In [[statistics]], the '''Šidák correction''', or '''Dunn–Šidák correction''', is a method used to counteract the problem of [[multiple comparisons]]. It is a simple method to control the [[familywise error rate]].  When all null hypotheses are true, the method provides familywise error control that is exact for tests that are stochastically independent, is conservative for tests that are positively dependent, and is liberal for tests that are negatively dependent.  It is credited to a 1967 paper <ref>{{Cite journal | last1 = Šidák | first1 = Z. K. | title = Rectangular Confidence Regions for the Means of Multivariate Normal Distributions | doi = 10.1080/01621459.1967.10482935 | journal = Journal of the American Statistical Association | volume = 62 | issue = 318 | pages = 626–633 | year = 1967 | pmid =  | pmc = }}</ref> by the [[statistician]] and [[Mathematician|probabilist]] [[Zbyněk Šidák]].<ref>{{Cite journal | last1 = Seidler | first1 = J. | last2 = Vondráček | first2 = J. Í. | last3 = Saxl | first3 = I. | journal = Applications of Mathematics | volume = 45 | issue = 5 | pages = 321 | year = 2000 | doi = 10.1023/A:1022238410461 | pmid =  | pmc = |title=The life and work of Zbyněk Šidák (1933–1999)}}</ref>\n\n==Usage==\n\n* Given ''m'' different null hypotheses and a familywise alpha level of <math>\\alpha</math>, each null hypotheses is rejected that has a p-value lower than <math> \\alpha_{SID} = 1-(1-\\alpha)^\\frac{1}{m} </math>.\n* This test produces a familywise Type I error rate of exactly <math> \\alpha </math> when the tests are independent from each other and all null hypotheses are true. It is less stringent than the Bonferroni correction, but only slightly.  For example,  for <math> \\alpha </math> = 0.05 and ''m'' = 10, the Bonferroni-adjusted level is 0.005 and the Šidák-adjusted level is approximately 0.005116.\n* One can also compute [[confidence intervals]] matching the test decision using the Šidák correction by using 100(1&nbsp;−&nbsp;α)<sup>1/''m''</sup>% confidence intervals.\n\n==Proof==\n{{Expand section|date=September 2013}}\n\nThe Šidák correction is derived by assuming that the individual tests are [[Independence (probability theory)|independent]]. Let the significance threshold for each test be <math>\\alpha_1</math>; then the probability that at least one of the tests is significant under this threshold is (1 - the probability that none of them are significant). Since it is assumed that they are independent, the probability that all of them are not significant is the product of the probabilities that each of them are not significant, or <math>1 - (1 - \\alpha_1)^m</math>. Our intention is for this probability to equal <math>\\alpha</math>, the significance level for the entire series of tests. By solving for <math>\\alpha_1</math>, we obtain <math>\\alpha_1 = 1 - (1 - \\alpha)^{1/m}.</math>\n\n==Šidák correction for t-test==\n{{main|Šidák correction for t-test}}\n\n==See also ==\n* [[Multiple comparisons]]\n* [[Bonferroni correction]]\n* [[Familywise error rate]]\n* [[Closed testing procedure]]\n\n==References==\n\n{{reflist}}\n\n==External links==\n*[http://www.utdallas.edu/~herve/Abdi-Bonferroni2007-pretty.pdf The Bonferonni and Šidák Corrections for Multiple Comparisons]\n\n{{DEFAULTSORT:Sidak correction}}\n[[Category:Multiple comparisons]]"
    },
    {
      "title": "Šidák correction for t-test",
      "url": "https://en.wikipedia.org/wiki/%C5%A0id%C3%A1k_correction_for_t-test",
      "text": "One of the application of [[Student's t-test]] is to test the location of one sequence of [[independent and identically distributed random variables]]. If we want to test the locations of multiple sequences of such variables, [[Šidák correction]] should be applied in order to calibrate the level of the Student's t-test. Moreover, if we want to test the locations of nearly infinitely many sequences of variables, then Šidák correction should be used, but with caution. More specifically, the validity of Šidák correction depends on how fast the number of sequences goes to infinity.\n\n==Introduction of Šidák correction==\n\nSuppose we are interested in {{mvar|m}} different hypotheses, <math>  H_{1},...,H_{m} </math>, and would like to check if all of them are true. Now the hypothesis test scheme becomes\n\n: <math> H_{null} </math>: all of <math> H_{i} </math> are true;\n\n: <math> H_{alternative}</math>: at least one of <math> H_{i} </math> is false.\n\nLet <math> \\alpha </math> be the level of this test (the type-I error), that is, the probability that we falsely reject <math>  H_{null} </math> when it is true. \n\nWe aim to design a test with certain level <math> \\alpha </math>. \n\nSuppose when testing each hypothesis <math>  H_{i}</math>, the test statistic we use is <math> t_{i}</math>. \n\nIf these <math> t_{i}</math>'s are independent, then a test for <math>  H_{null} </math> can be developed by the following procedure, known as Šidák correction.\n\n:Step 1, we test each of {{mvar|m}} null hypotheses at level <math> 1-(1-\\alpha)^\\frac{1}{m} </math>.\n\n:Step 2, if any of these {{mvar|m}} null hypotheses is rejected, we reject <math>  H_{null} </math>.\n\n==Šidák correction for finitely many t-test==\n\nSuppose <math> Y_{ij}=\\mu_{i}+\\epsilon_{ij}, i=1,...,N, j=1,...,n, </math> where for each {{mvar|i}}, <math> \\epsilon_{i1},...,\\epsilon_{in} </math> are independently and identically distributed, for each {{mvar|j}} <math>  \\epsilon_{1j},...,\\epsilon_{Nj} </math> are independent but not necessarily identically distributed, and <math> \\epsilon_{ij} </math> has finite fourth moment.\n\nOur goal is to design a test for <math> H_{null}: \\mu_{i}=0, \\forall i=1,...,N </math> with level {{mvar|&alpha;}}. This test can be based on the [[t-statistic]] of each sequences, that is,\n\n: <math> t_{i}=\\frac{\\bar{Y}_{i}}{S_{i}/\\sqrt{n}},</math>\n\nwhere:\n\n: <math> \\bar{Y}_{i}=\\frac{1}{n}\\sum_{j=1}^{n}Y_{ij}, \\qquad S_{i}^{2}=\\frac{1}{n}\\sum_{j=1}^{n}(Y_{ij}-\\bar{Y}_{i})^{2}.</math>\n\nUsing Šidák correction, we reject <math> H_{null} </math> if any of  the t-tests based on the t-statistics above reject at level <math> 1-(1-\\alpha)^{\\frac{1}{N}}.</math> More specifically, we reject <math> H_{null} </math> when\n\n: <math> \\exists i \\in \\{1,\\ldots,N\\} : |t_{i}|> \\zeta_{\\alpha,N},</math>\n\nwhere\n\n: <math> P(|Z|>\\zeta_{\\alpha,N})=1-(1-\\alpha)^{\\frac{1}{N}}, \\qquad Z\\sim N(0,1)</math>\n\nThe test defined above has asymptotic level {{mvar|&alpha;}}, because\n\n: <math>\\begin{align}\n\\text{level} &= P_{null} \\left (\\text{reject } H_{null} \\right) \\\\\n&= P_{null} \\left(\\exists i \\in \\{1,\\ldots,N\\} : |t_{i}|>\\zeta_{\\alpha,N} \\right ) \\\\\n&= 1-P_{null} \\left (\\forall i \\in \\{1,\\ldots,N\\} : |t_{i}|\\leq\\zeta_{\\alpha,N} \\right ) \\\\\n&=1-\\prod_{i=1}^{N}P_{null} \\left (|t_{i}|\\leq\\zeta_{\\alpha,N} \\right ) \\\\\n&\\to 1-\\prod_{i=1}^{N}P \\left (|Z_{i}|\\leq\\zeta_{\\alpha,N} \\right ) && Z_{i}\\sim N(0,1) \\\\\n&=\\alpha \n\\end{align}</math>\n\n==Šidák correction for infinitely many t-test==\n\nIn some cases, the number of sequences, <math> N </math>, increase as the data size of each sequences, <math> n </math>, increase. In particular, suppose <math> N(n)\\rightarrow \\infty \\text{ as } n \\rightarrow \\infty </math>. If this is true, then we will need to test a null including infinitely many hypotheses, that is\n\n:<math>  H_{null}: \\text{ all of } H_{i}  \\text{ are true, } i=1,2,....</math>\n\nTo design a test, [[Šidák correction]] may be applied, as in the case of finitely many t-test. However, when <math>N(n)\\rightarrow \\infty \\text{ as } n\\rightarrow \\infty</math>, the Šidák correction for t-test may not achieve the level we want, that is, the true level of the test may not converges to the nominal level <math> \\alpha </math> as n goes to infinity. This result is related to [[high-dimensional statistics]] and is proven by Fan, Hall and Yao (2007).<ref>{{cite journal| last = Fan |first=Jianqing| last2 = Hall |first2 =Peter| last3 = Yao |first3 =Qiwei| year = 2007| title = [[To How Many Simultaneous Hypothesis Tests Can Normal, Student's t or Bootstrap Calibration Be Applied]]| journal = Journal of the American Statistical Association| pages = 1282–1288| ref = harv| volume = 102| issue = 480 | doi=10.1198/016214507000000969| arxiv =math/0701003}}</ref> Specifically, if we want the true level of the test converges to the nominal level <math> \\alpha </math>, then we need a restraint on how fast <math> N(n)\\rightarrow \\infty </math>. Indeed,\n\n* When all of <math> \\epsilon_{ij} </math> have distribution symmetric about zero, then it is sufficient to require <math> \\log N = o (n^{1/3}) </math> to guarantee the true level converges to <math> \\alpha </math>.\n* When the distributions of <math> \\epsilon_{ij} </math> are asymmetric, then it is necessary to impose <math> \\log N = o(n^{1/2})</math> to ensure the true level converges to <math> \\alpha </math>.\n* Actually, if we apply [[bootstrapping]] method to the calibration of level, then we will only need <math> \\log N = o (n^{1/3}) </math> even if <math> \\epsilon_{ij} </math> has asymmetric distribution.\n\nThe results above are based on [[Central Limit Theorem]]. According to Central Limit Theorem, each of our t-statistics <math> t_{i} </math> possesses asymptotic standard normal distribution, and so the difference between the distribution of each <math> t_{i} </math> and the standard normal distribution is asymptotically negligible. The question is, if we aggregate all the differences between the distribution of each <math> t_{i} </math> and the standard normal distribution, is this aggregation of differences still asymptotically ignorable?\n\nWhen we have finitely many <math> t_{i} </math>, the answer is yes. But when we have infinitely many <math> t_{i} </math>, the answer some time becomes no. This is because in the latter case we are summing up infinitely many infinitesimal terms. If the number of the terms goes to infinity too fast, that is, <math> N(n) \\rightarrow \\infty </math> too fast, then the sum may not be zero, the distribution of the t-statistics can not be approximated by the standard normal distribution, the true level does not converges to the nominal level <math> \\alpha </math>, and then the Šidák correction fails.\n\n==See also ==\n* [[Šidák correction]]\n* [[Multiple comparisons]]\n* [[Bonferroni correction]]\n* [[Familywise error rate]]\n* [[Closed testing procedure]]\n\n==Notes==\n{{Reflist}}\n\n==References==\n* {{cite journal\n  | last = Fan |first=Jianqing\n  | last2 = Hall |first2 =Peter\n  | last3 = Yao |first3 =Qiwei\n  | year = 2007\n  | title = [[To How Many Simultaneous Hypothesis Tests Can Normal, Student's t or Bootstrap Calibration Be Applied]]\n  | journal = Journal of the American Statistical Association\n  | pages = 1282–1288\n  | ref = harv\n  | volume = 102\n  | issue = 480\n  | doi=10.1198/016214507000000969\n  | arxiv =math/0701003\n  }}\n\n{{DEFAULTSORT:Sidak correction for t-test}}\n[[Category:Multiple comparisons]]"
    },
    {
      "title": "Studentized range",
      "url": "https://en.wikipedia.org/wiki/Studentized_range",
      "text": "In [[statistics]], the '''studentized range''' is the difference between the largest and smallest data in a [[sample (statistics)|sample]] measured in units of [[standard deviation|sample standard deviation]]s.\n\nThe '''studentized range''', ''q'', is named for  [[William Sealy Gosset]] (who wrote under the pseudonym \"''Student''\"), and was introduced by him (1927).<ref>{{cite journal |author=Student |title= Errors of routine analysis|journal= Biometrika|volume=19 |issue= 1/2|pages=151–164 |year=1927 | doi = 10.2307/2332181 |jstor= 2332181}}</ref> The concept was later presented by a number of actual ''[[student]]s'', Newman (1939)<ref>{{cite journal |author= Newman D. |title= The Distribution of Range in Samples from a Normal Population Expressed in Terms of an Independent Estimate of Standard Deviation |journal= Biometrika |volume=31 |issue= 1–2 |pages=20–30 |year=1939 |doi=10.1093/biomet/31.1-2.20}}</ref> and Keuls (1952)<ref>{{cite journal |author= Keuls M.|title= The Use of the \"Studentized Range\" in Connection with an Analysis of Variance |journal= Euphytica |volume=1 |issue= 2 |pages=112–122 |year=1952 |doi=10.1007/bf01908269}}</ref>  and [[John Tukey]] in some unpublished notes. ''q''&nbsp;is the basic statistic for the '''[[studentized range distribution]]''', which is used for [[Multiple comparisons problem|multiple comparison]] procedures, such as the single step procedure [[Tukey's range test]], the [[Newman–Keuls method]], and the Duncan's step down procedure, and establishing [[confidence interval]]s that are still valid after [[data snooping]] has occurred.<ref>{{cite journal |author=John A. Rafter |title=Multiple Comparison Methods for Means |journal=SIAM Review|volume=44 |issue=2 |pages=259–278 |year=2002 |doi=10.1137/s0036144501357233|citeseerx=10.1.1.132.2976 |bibcode=2002SIAMR..44..259R }}</ref>\n\n== Description ==\nThe value of the '''studentized range''' is most often represented by the variable ''q''.\n\nThe '''studentized range''' can be defined based on a random sample ''x''<sub>1</sub>,&nbsp;...,&nbsp;''x''<sub>''n''</sub> from the ''N''(0, 1) distribution of numbers, and another random variable ''s'' that is independent of all the ''x<sub>i</sub>'', and ''νs''<sup>2</sup> has a ''χ''<sup>2</sup> distribution with ''ν'' degrees of freedom. Then\n\n: <math>\nq _{n,\\nu}= \\frac{\\max\\{\\,x_1,\\ \\dots, \\ x_n\\,\\} - \\min\\{\\,x_1,\\ \\dots,\\ x_n\\}}{s} = \\max_{i,j=1, \\dots, n} \\left\\{\\frac{x_i - x_j}{s}\\right\\}</math>\n\nhas the Studentized range distribution for ''n'' groups and ''ν'' degrees of freedom. In applications, the ''x<sub>i</sub>'' are typically the means of samples each of size ''m'', ''s''<sup>2</sup> is the [[pooled variance]], and the degrees of freedom are&nbsp;''ν''&nbsp;=&nbsp;''n''(''m''&nbsp;−&nbsp;1).\n\nThe critical value of ''q'' is based on three factors:\n#''α'' (the probability of rejecting a true [[null hypothesis]])\n#''n'' (the number of observations or groups) \n#''ν'' (the degrees of freedom used to estimate the [[sample variance]])\n\n== Distribution (normal data) and applications ==\nIf ''X''<sub>1</sub>, ..., ''X''<sub>''n''</sub> are [[independent identically distributed]] [[random variable]]s that are [[normal distribution|normally distributed]], the probability distribution of their studentized range is what is usually called the '''[[studentized range distribution]]'''. Note that the definition of ''q'' does not depend on the [[expected value]] or the [[standard deviation]] of the distribution from which the sample is drawn, and therefore its probability distribution is the same regardless of those parameters. tables of the distribution quantiles are available [http://www.watpon.com/table/studen_range.pdf here].\n\nThe Studentized range distribution has applications to [[hypothesis testing]] and [[multiple comparisons]] procedures. For example, [[Tukey's range test]] and [[Duncan's new multiple range test]] (MRT), in  which the sample ''x''<sub>1</sub>,&nbsp;...,&nbsp;''x''<sub>''n''</sub> is a sample of [[sample mean|means]] and '''q''' is the basic test-statistic, can be used as [[post-hoc analysis]] to test between which two groups means there is a significant difference (pairwise comparisons) after rejecting the [[null hypothesis]] that all groups are from the same population (i.e. all means are equal) by the standard [[analysis of variance]].<ref>Pearson & Hartley (1970, Section 14.2)</ref>\n\nWhen only the equality of the '''two''' groups means is in question (i.e. whether ''μ''<sub>1</sub> = ''μ''<sub>2</sub>), the '''[[studentized range distribution]]''' is similar to the [[Student's t distribution]], differing only in that the first  takes into account the number of means under consideration, and the critical value is adjusted accordingly. The more means under consideration, the larger the critical value is. This makes sense since the more means there are, the greater the probability that at least some differences between pairs of means will be significantly large due to chance alone.\n\n== ''Studentized'' data ==\nGenerally, the term ''[[Studentization|studentized]]'' means that the variable's scale was adjusted by dividing by an [[estimation theory|estimate]] of a population [[standard deviation]] (see also [[studentized residual]]). The fact that the standard deviation is a ''sample'' standard deviation rather than the ''population'' standard deviation, and thus something that differs from one random sample to the next, is essential to the definition and the distribution of the ''Studentized'' data. The variability in the value of the ''sample'' standard deviation contributes additional uncertainty into the values calculated. This complicates the problem of finding the probability distribution of any statistic that is ''studentized''.\n\n==See also==\n*[[Studentized range distribution]]\n*[[Tukey's range test]]\n\n==Notes==\n{{reflist}}\n{{More footnotes|date=November 2010}}\n\n==References==\n* Pearson, E.S.; Hartley, H.O. (1970) ''Biometrika Tables for Statisticians, Volume 1, 3rd Edition'', Cambridge University Press. {{ISBN|0-521-05920-8}}\n\n== Further reading ==\n\n* John Neter, Michael H. Kutner, Christopher J. Nachtsheim, William Wasserman (1996) ''Applied Linear Statistical Models'', fourth edition, McGraw-Hill, page 726.\n* John A. Rice (1995) ''Mathematical Statistics and Data Analysis'', second edition, Duxbury Press, pages 451&ndash;452.\n* Douglas C. Montgomery (2013) \"Design and Analysis of Experiments\", eighth edition, Wiley, page 98.\n\n{{DEFAULTSORT:Studentized Range}}\n[[Category:Summary statistics]]\n[[Category:Multiple comparisons]]\n[[Category:Statistical ratios]]"
    },
    {
      "title": "Testing hypotheses suggested by the data",
      "url": "https://en.wikipedia.org/wiki/Testing_hypotheses_suggested_by_the_data",
      "text": "{{Distinguish|Post hoc analysis}}\n{{expert needed|1=statistics|date=February 2019}}\n{{Refimprove|date=January 2008}}\n\nIn statistics, '''hypotheses suggested by a given dataset''', when tested with the same dataset that suggested them, are likely to be accepted even when they are not true.  This is because circular reasoning (double dipping) would be involved: something seems true in the limited data set, therefore we hypothesize that it is true in general, therefore we (wrongly) test it on the same limited data set, which seems to confirm that it is true. Generating hypotheses based on data already observed, in the absence of testing them on new data, is referred to as '''post hoc theorizing''' (from [[Latin language|Latin]] ''[[post hoc analysis|post hoc]]'', \"after this\").\n\nThe correct procedure is to test any hypothesis on a data set that was not used to generate the hypothesis.\n\n==Example of fallacious acceptance of a hypothesis==\n\nSuppose fifty different researchers run clinical trials to test whether Vitamin X is efficacious in treating cancer. The vast majority of them find no significant differences between measurements done on patients who have taken Vitamin X and those who have taken a [[placebo]]. However, due to [[statistical noise]], one study finds a significant correlation between taking Vitamin X and being cured from cancer.\n\nTaking into account all 50 studies as a whole, the only conclusion that could be made with great certainty is that there remains no evidence that Vitamin X has any effect on treating cancer. However, someone trying to achieve greater publicity for the one outlier study could try to create a hypothesis suggested by the data, by finding some aspect unique to that one study, and claiming that this aspect is the key to its differing results. Suppose, for instance, that this study was the only one conducted in Denmark. It could be claimed that this set of 50 studies shows that Vitamin X is more efficacious in Denmark than elsewhere. However, while the data do not contradict this hypothesis, they do not strongly support it either. Only one or more additional studies could bolster this additional hypothesis.\n\n==The general problem==\n\nTesting a hypothesis suggested by the data can very easily result in false positives ([[type I error]]s). If one looks long enough and in enough different places, eventually data can be found to support any hypothesis. Yet, these positive data do not by themselves constitute [[scientific evidence|evidence]] that the hypothesis is correct. The negative test data that were thrown out are just as important, because they give one an idea of how common the positive results are compared to chance. Running an experiment, seeing a pattern in the data, proposing a hypothesis from that pattern, then using the ''same'' experimental data as evidence for the new hypothesis is extremely suspect, because data from all other experiments, completed or potential, has essentially been \"thrown out\" by choosing to look only at the experiments that suggested the new hypothesis in the first place.\n\nA large set of tests as described above greatly inflates the [[probability]] of [[type I error]] as all but the data most favorable to the [[hypothesis]] is discarded. This is a risk, not only in [[statistical hypothesis testing|hypothesis testing]] but in all [[statistical inference]] as it is often problematic to accurately describe the process that has been followed in searching and discarding [[data]]. In other words, one wants to keep all data (regardless of whether they tend to support or refute the hypothesis) from \"good tests\", but it is sometimes difficult to figure out what a \"good test\" is. It is a particular problem in [[statistical model]]ling, where many different models are rejected by [[trial and error]] before publishing a result (see also [[overfitting]], [[publication bias]]).\n\nThe error is particularly prevalent in [[data mining]] and [[machine learning]]. It also commonly occurs in [[academic publishing]] where only reports of positive, rather than negative, results tend to be accepted, resulting in the effect known as [[publication bias]].\n\n==Correct procedures==\n\nAll strategies for sound testing of hypotheses suggested by the data involve including a wider range of tests in an attempt to validate or refute the new hypothesis. These include:\n*Collecting [[confirmation sample]]s\n*[[Cross-validation (statistics)|Cross-validation]]\n*Methods of compensation for [[multiple comparisons]]\n*Simulation studies including adequate representation of the multiple-testing actually involved\n\n[[Scheffé test|Henry Scheffé's simultaneous test]] of all contrasts in [[multiple comparisons|multiple comparison]] problems is the most{{Citation needed|date=February 2011}} well-known remedy in the case of [[analysis of variance]].<ref>[[Henry Scheffé]], \"A Method for Judging All Contrasts in the Analysis of Variance\", ''[[Biometrika]]'', 40, pages 87–104 (1953). {{doi|10.1093/biomet/40.1-2.87}}</ref> It is a method designed for testing hypotheses suggested by the data while avoiding the fallacy described above.\n\n==See also==\n*[[Bonferroni correction]]\n*[[Data analysis]]\n*[[Data dredging]]\n*[[Exploratory data analysis]]\n*[[Post-hoc analysis]]\n*[[Predictive analytics]]\n*[[Texas sharpshooter fallacy]]\n*[[Type I and type II errors]]\n*[[Uncomfortable science]]\n\n== Notes and references ==\n\n{{reflist}}\n\n[[Category:Statistical hypothesis testing]]\n[[Category:Misuse of statistics]]\n[[Category:Multiple comparisons]]"
    },
    {
      "title": "Tukey's range test",
      "url": "https://en.wikipedia.org/wiki/Tukey%27s_range_test",
      "text": "'''Tukey's range test''', also known as the '''Tukey's test''', '''Tukey method''', '''Tukey's honest significance test''', or '''Tukey's HSD (honestly significant difference) test''',<ref name=Vassar>{{cite web |last=Lowry |first=Richard |url=http://faculty.vassar.edu/lowry/ch14pt2.html |title=One Way ANOVA – Independent Samples |work=Vassar.edu |accessdate=December 4, 2008 |deadurl=yes |archiveurl=https://web.archive.org/web/20081017161620/http://faculty.vassar.edu/lowry/ch14pt2.html |archivedate=October 17, 2008 |df= }}  Also occasionally as \"honestly,\" see e.g. {{cite journal |last=Morrison |first=S. |last2=Sosnoff |first2=J. J. |last3=Heffernan |first3=K. S. |last4=Jae |first4=S. Y. |last5=Fernhall |first5=B. |title=Aging, hypertension and physiological tremor: The contribution of the cardioballistic impulse to tremorgenesis in older adults |journal=Journal of the Neurological Sciences |year=2013 |volume=326 |issue=1–2 |pages=68–74 |doi=10.1016/j.jns.2013.01.016 }}</ref> is a single-step [[multiple comparison]] procedure and [[statistical test]]. It can be used on raw data or in conjunction with an [[ANOVA]] to find means that are significantly different from each other. \n\nNamed after [[John Tukey]],<ref>{{cite journal |last=Tukey |first=John |jstor=3001913 |title=Comparing Individual Means in the Analysis of Variance |journal=[[Biometrics (journal)|Biometrics]] |volume=5 |issue=2 |year=1949 |pages=99–114 }}</ref> it compares all possible pairs of [[sample mean|means]], and is based on a [[studentized range distribution]] (''q'') (this distribution is similar to the distribution of ''t'' from the [[t-test|''t''-test]]. See below).<ref name=\"Calgary\">Linton, L.R., Harder, L.D. (2007) Biology 315 – Quantitative Biology Lecture Notes. University of Calgary, Calgary, AB</ref> The Tukey HSD tests should not be confused with the Tukey Mean Difference tests (also known as the [[Bland–Altman plot|Bland–Altman diagram]]).\n\nTukey's test compares the means of every treatment to the means of every other treatment; that is, it applies simultaneously to the set of all pairwise comparisons\n\n:<math>\\mu_i-\\mu_j \\, </math>\n\nand identifies any difference between two means that is greater than the expected [[standard error]]. The [[confidence coefficient]] for the [[Set (mathematics)|set]], when all sample sizes are equal, is exactly  <math>1 -\n \\alpha</math> for any <math>0 \\le \\alpha \\le 1</math>. For unequal sample sizes, the confidence coefficient is greater than 1&nbsp;−&nbsp;α. In other words, the Tukey method is conservative when there are [[Sample size determination|unequal sample sizes]].\n\n==Assumptions==\n#The observations being tested are [[statistical independence|independent]] within and among the groups.\n#The groups associated with each mean in the test are [[normal distribution|normally distributed]].\n#There is equal within-group variance across the groups associated with each mean in the test ([[Homoscedasticity|homogeneity of variance]]).\n\n==The test statistic==\nTukey's test is based on a formula very similar to that of the t-test. In fact, Tukey's test is essentially a t-test, except that it corrects for [[family-wise error rate]].\n\nThe formula for Tukey's test is:\n\n:<math> q_s = \\frac{Y_A - Y_B}{SE}, </math>\n\nwhere ''Y''<sub>A</sub> is the larger of the two means being compared, ''Y''<sub>B</sub> is the smaller of the two means being compared, and SE is the [[Standard error (statistics)|standard error]] of the sum of the means.\n\nThis ''q''<sub>''s''</sub> value can then be compared to a ''q'' value from the [[studentized range distribution]]. If the ''q''<sub>''s''</sub> value is ''larger'' than the critical value <math>q_{\\alpha} </math> obtained from the distribution, the two means are said to be significantly different at level <math>\\alpha, 0 \\le \\alpha \\le 1</math>.<ref name=\"Calgary\" />\n\nSince the [[null hypothesis]] for Tukey's test states that all means being compared are from the same population (i.e. ''μ''<sub>1</sub> = ''μ''<sub>2</sub> = ''μ''<sub>3</sub> = ... = ''μ''<sub>''k''</sub>), the means should be normally distributed (according to the [[central limit theorem]]). This gives rise to the normality assumption of Tukey's test.\n\n==The studentized range (''q'') distribution==\nThe Tukey method uses the [[studentized range]] distribution. Suppose that we take a sample of size ''n'' from each of ''k'' populations with the same [[normal distribution]] ''N''(''μ'', ''σ''<sup>2</sup>) and suppose that <math>\\bar{y}</math><sub>min</sub> is the smallest of these sample means and ''<math>\\bar{y}</math>''<sub>max</sub> is the largest of these sample means, and suppose ''S''<sup>2</sup> is the pooled sample variance from these samples. Then the following random variable has a Studentized range distribution.\n\n:<math>q = \\frac{\\overline{y}_{\\max} - \\overline{y}_{\\min}}{S\\sqrt{2/n}}</math>\n\nThis value of ''q'' is the basis of the critical value of ''q'', based on three factors:\n# α (the [[Type I error]] rate, or the probability of rejecting a true null hypothesis)\n# ''k'' (the number of populations)\n# ''df'' (the number of degrees of freedom (''n''&nbsp;–&nbsp;''k'') where ''n'' is the total number of observations)\n\nThe distribution of ''q'' has been tabulated and appears in many textbooks on statistics. In some tables the distribution of ''q'' has been tabulated without the <math>\\sqrt{2}</math> factor. To understand  which table it is, we can compute the result for ''k''&nbsp;=&nbsp;2 and compare it to the result of the [[Student's t-distribution]] with the same degrees of freedom and the same&nbsp;''α''.\nIn addition, [[R (programming language)|R]] offers a [[cumulative distribution function]] (<code>ptukey</code>) and a [[quantile function]] (<code>qtukey</code>) for&nbsp;''q''.\n\n==Confidence limits==\nThe Tukey [[Confidence interval|confidence limits]] for all pairwise comparisons with confidence coefficient of at least 1&nbsp;−&nbsp;α are\n\n:<math>\\bar{y}_{i\\bullet}-\\bar{y}_{j\\bullet} \\pm \\frac{q_{\\alpha;k;N-k}}{\\sqrt{2}}\\widehat{\\sigma}_\\varepsilon \\sqrt{\\frac{2}{n}} \\qquad i,j=1,\\ldots,k\\quad i\\neq j.</math>\n\nNotice that the point estimator and the estimated variance are the same as those for a single pairwise comparison. The only difference between the confidence limits for simultaneous comparisons and those for a single comparison is the multiple of the estimated standard deviation.\n\nAlso note that the sample sizes must be equal when using the studentized range approach. <math>\\widehat{\\sigma}_\\varepsilon</math> is the standard deviation of the entire design, not just that of the two groups being compared. It is possible to work with unequal sample sizes. In this case, one has to calculate the estimated standard deviation for each pairwise comparison as formalized by [[Clyde Kramer]] in 1956, so the procedure for unequal sample sizes is sometimes referred to as the '''Tukey–Kramer method''' which is as follows:\n\n:<math>\\bar{y}_{i\\bullet}-\\bar{y}_{j\\bullet} \\pm \\frac{q_{\\alpha;k;N-k}}{\\sqrt{2}}\\widehat{\\sigma}_\\varepsilon \\sqrt{\\frac{1}{n}_{i} + \\frac{1}{n}_{j}} \\qquad </math>\n\nwhere ''n''<sub>&nbsp;''i''</sub> and ''n''<sub>&nbsp;''j''</sub> are the sizes of groups ''i'' and ''j'' respectively.  The degrees of freedom for the whole design is also applied.\n\n==See also==\n*[[Familywise error rate]]\n*[[Newman–Keuls method]]\n\n==Notes==\n{{reflist}}\n\n==Further reading==\n* {{cite book |first=Douglas C. |last=Montgomery |year=2013 |title=Design and Analysis of Experiments |edition=Eighth |publisher=Wiley }} Section 3.5.7.\n\n==External links==\n* [http://www.itl.nist.gov/div898/handbook/prc/section4/prc471.htm NIST/SEMATECH e-Handbook of Statistical Methods: Tukey's method]\n\n[[Category:Analysis of variance]]\n[[Category:Statistical tests]]\n[[Category:Multiple comparisons]]"
    },
    {
      "title": "Working–Hotelling procedure",
      "url": "https://en.wikipedia.org/wiki/Working%E2%80%93Hotelling_procedure",
      "text": "{{regression bar}}\n\nIn [[statistics]], particularly [[regression analysis]], the '''Working–Hotelling procedure''', named after [[Holbrook Working]] and [[Harold Hotelling]], is a method of simultaneous estimation in [[linear regression]] models. One of the first developments in [[simultaneous inference]], it was devised by Working and Hotelling for the [[simple linear regression]] model in 1929.<ref>Miller (1966), p. 1</ref> It provides a [[confidence region]] for multiple mean responses, that is, it gives the upper and lower bounds of more than one value of a [[dependent variable]] at several levels of the [[independent variable]]s at a certain [[confidence level]]. The resulting [[confidence band]]s are known as the '''Working–Hotelling–Scheffé confidence bands'''.\n\nLike the closely related [[Scheffé's method]] in the [[analysis of variance]], which considers all possible [[contrast (statistics)|contrast]]s, the Working–Hotelling procedure considers all possible values of the independent variables; that is, in a particular regression model, the probability that all the Working–Hotelling confidence intervals cover the true value of the mean response is the [[confidence coefficient]]. As such, when only a small subset of the possible values of the independent variable is considered, it is more conservative and yields wider intervals than competitors like the [[Bonferroni correction]] at the same level of confidence. It outperforms the Bonferroni correction as more values are considered.\n\n== Statement ==\n\n=== Simple linear regression ===\nConsider a [[simple linear regression]] model <math>Y = \\beta_0 + \\beta_1 X + \\varepsilon</math>, where <math>Y</math> is the response variable and <math>X</math> the explanatory variable, and let <math>b_0</math> and <math>b_1</math> be the [[ordinary least squares|least-squares]] estimates of <math>\\beta_0</math> and <math>\\beta_1</math> respectively. Then the least-squares estimate of the mean response <math>E(Y_i)</math> at the level <math>X = x_i</math> is <math>\\hat{Y_i} = b_0 + b_1 x_i </math>. It can then be [[Simple linear regression#Normality assumption|shown]], assuming that the errors independently and identically follow the [[normal distribution]], that an <math>1 - \\alpha</math> confidence interval of the mean response at a certain level of <math>X</math> is as follows:\n\n: <math>\\hat{y}_i \\in \\left[ b_0 + b_1 x_i \\pm t_{\\alpha/2,\\text{df}=n - 2} \\sqrt{ \\left(\\frac{1}{n - 2} \\sum_{j=1}^n e_i^{\\,2} \\right) \\cdot \\left(\\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^n (x_j - \\bar{x})^2}\\right)}\\right],</math>\n\nwhere <math>\\left(\\frac{1}{n - 2} \\sum_{j=1}^n e_ji^{\\,2} \\right)</math> is the [[mean squared error]] and <math>t_{\\alpha/2,\\text{df}=n - 2}</math> denotes the upper <math>\\frac{\\alpha}{2}^\\text{th}</math> [[percentile]] of [[Student's t-distribution]] with <math>n-2</math> [[degrees of freedom (statistics)|degrees of freedom]].\n\nHowever, as multiple mean responses are estimated, the confidence level declines rapidly. To fix the confidence coefficient at <math>1 - \\alpha</math>, the Working–Hotelling approach employs an F-statistic:<ref name=\":1\">Miller (2014)</ref><ref name=\":0\">Neter, Wasserman and Kutner, pp. 163–165</ref>\n\n: <math>\\hat{y}_i \\in \\left[ b_0 + b_1 x_i \\pm W \\sqrt{ \\left(\\frac{1}{n - 2} \\sum_{j=1}^n e_j^{\\,2} \\right) \\cdot \\left(\\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^n(x_j - \\bar{x})^2}\\right)}\\right],</math>\n\nwhere <math>W^2 = 2F_{\\alpha,\\text{df}=(2,n-2)}</math> and <math>F</math> denotes the upper <math>\\alpha^\\text{th}</math> percentile of the [[F-distribution]] with <math>(2, n-2)</math> degrees of freedom. The confidence level of is <math>1 - \\alpha</math> over ''all'' values of <math>X</math>, i.e. <math>x_i \\in \\mathbb{R}</math>.\n\n=== Multiple linear regression ===\nThe Working–Hotelling confidence bands can be easily generalised to multiple linear regression. Consider a general linear model as defined in the [[Linear regression#Introduction|linear regressions]] article, that is,\n\n: <math>\n \\mathbf{Y} = \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\varepsilon, \\,\n </math>\nwhere\n: <math>\n \\mathbf{Y} = \\begin{pmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{pmatrix}, \\quad\n \\mathbf{X}  = \\begin{pmatrix} \\mathbf{x}^{\\rm T}_1 \\\\ \\mathbf{x}^{\\rm T}_2 \\\\ \\vdots \\\\ \\mathbf{x}^{\\rm T}_n \\end{pmatrix} = \\begin{pmatrix} x_{11} & \\cdots & x_{1p}  \\\\\n x_{21} & \\cdots & x_{2p} \\\\\n \\vdots & \\ddots & \\vdots \\\\\n x_{n1} & \\cdots & x_{np}\n \\end{pmatrix}, \n \\boldsymbol\\beta = \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_p \\end{pmatrix}, \\quad\n \\boldsymbol\\varepsilon = \\begin{pmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{pmatrix}.\n </math>\n\nAgain, it can be shown that the least-squares estimate of the mean response <math>E(Y_i) = \\mathbf{x}^{\\rm T}_i \\boldsymbol\\beta</math> is <math>\\hat{Y}_i = \\mathbf{x}^{\\rm T}_i \\mathbf{b}</math>, where <math>\\mathbf{b}</math> consists of least-square estimates of the entries in <math>\\boldsymbol\\beta</math>, i.e. <math>\\mathbf{b} = (\\mathbf{X}^{\\rm T} \\mathbf{X})^{-1} \\mathbf{X}^{\\rm T}\\mathbf{Y}</math>. Likewise, it can be shown that a <math>1 - \\alpha</math> confidence interval for a single mean response estimate is as follows:<ref name=\"tb2\" />\n\n: <math>\\hat{y}_i \\in \\left[ \\mathbf{x}^{\\rm T}_i \\mathbf{b} \\pm t_{\\alpha/2,\\text{df}=n - p} \\sqrt{\\operatorname{MSE}(\\mathbf{x}^{\\rm T}_i (\\mathbf{X}^{\\rm T}\\mathbf{X})^{-1} \\mathbf{x}_i})\\right],</math>\n\nwhere <MATH>\\operatorname{MSE}</math> is the observed value of the mean squared error <math>(Y^{\\rm T} Y - \\mathbf{b}^{\\rm T} X^{\\rm T} Y)</math>.\n\nThe Working–Hotelling approach to multiple estimations is similar to that of simple linear regression, with only a change in the degrees of freedom:<ref name=\":0\" />\n\n: <math>\\hat{y}_i \\in \\left[ \\mathbf{x}^{\\rm T}_i \\mathbf{b} \\pm W \\sqrt{\\operatorname{MSE}(\\mathbf{x}^{\\rm T}_i (\\mathbf{X}^{\\rm T}\\mathbf{X})^{-1} \\mathbf{x}_i})\\right],</math>\n\nwhere <math>W^2 = 2F_{\\alpha,\\text{df}=(p,n-p)}</math>.\n\n== Graphical representation ==\nIn the simple linear regression case, Working–Hotelling–Scheffé [[confidence band]]s, drawn by connecting the upper and lower limits of the mean response at every level, take the shape of [[hyperbola]]s. In drawing, they are sometimes approximated by the Graybill–Bowden confidence bands, which are linear and hence easier to graph:<ref name=\":1\" />\n\n: <math>\\beta_0 + \\beta_1(x_i-\\bar{x}) \\in \\left[ b_0 + b_1(x_i-\\bar{x}) \\pm m_{\\alpha, 2, \\text{df}=n-2} \\cdot \\left(\\frac{1}{\\sqrt n}\n+ \\frac{|x_i-\\bar x|}{\\sqrt{\\sum_{j=1}^n (x_j-\\bar x)}} \\right) \\right]</math>\n\nwhere <math>m_{\\alpha, 2, \\text{df}=n-2}</math>denotes the upper <math>\\alpha^\\text{th}</math> percentile of the Studentized maximum modulus distribution with two means and <math>n - 2</math> degrees of freedom.\n\n[[File:Heightmass-wh.svg|The simple linear regression model with a Working–Hotelling confidence band.|thumb|left|300px]]\n\n== Numerical example ==\nThe same data in [[ordinary least squares]] are utilised in this example:\n\n:{|class=\"wikitable\" style=\"text-align:right;\"\n|-\n! style=\"text-align:left;\" | Height (m)\n| 1.47 || 1.50 || 1.52 || 1.55 || 1.57 || 1.60 || 1.63 || 1.65 || 1.68 || 1.70 || 1.73 || 1.75 || 1.78 || 1.80 || 1.83\n|-\n! style=\"text-align:left;\" | Weight (kg)\n| 52.21 || 53.12 || 54.48 || 55.84 || 57.20 || 58.57 || 59.93 || 61.29 || 63.11 || 64.47 || 66.28 || 68.10 || 69.92 || 72.19 || 74.46\n|}\n\nA simple linear regression model is fit to this data. The values of <math>b_0</math> and <math>b_1</math> have been found to be −39.06 and 61.27 respectively. The goal is to estimate the mean mass of women given their heights at the 95% confidence level. The value of <math>W^2</math> was found to be <math>F_{0.95, \\text{df}=(2,15-2)} = 2.758828</math>. It was also found that <math>\\bar{x} = 1.651</math>, <math>\\sum_{j=1}^n e_j^{\\,2}= 7.490558</math>, <math>\\operatorname{MSE} = 0.5761968</math> and <math>\\sum_{j=1}^n (x_j - \\bar{x})^2 = 693.3726</math>. Then, to predict the mean mass of all women of a particular height, the following Working–Hotelling–Scheffé band has been derived:\n\n: <math>\\hat{y}_i \\in \\left[ -39.06 + 61.27 x_i \\pm \\sqrt{ 2.758828 \\cdot 0.5761968 \\cdot \\left(\\frac{1}{15} + \\frac{(x_i - 1.651)^2}{693.3726} \\right)}\\right],</math>\n\nwhich results in the graph on the left.\n\n== Comparison with other methods ==\n[[File:Heightmass-bonf (1).svg|thumb|right|300px|Bonferroni bands for the same linear regression model, based on estimating the response variable given the observed values of X. The confidence bands are noticeably tighter.]]\nThe Working–Hotelling approach may give tighter or looser confidence limits compared to the [[Bonferroni correction]]. In general, for small families of statements, the Bonferroni bounds may be tighter, but when the number of estimated values increases, the Working–Hotelling procedure will yield narrower limits. This is because the confidence level of Working–Hotelling–Scheffé bounds is exactly <math>1 - \\alpha</math> when ''all'' values of the independent variables, i.e. <math>x_i \\in \\mathbb{R}</math>, are considered. Alternatively, from an algebraic perspective, the critical value <math>\\pm \\sqrt{W}</math> remains constant as the number estimates of increases, whereas the corresponding values in Bonferonni estimates, <math>\\pm t_{1-\\alpha/g, \\text{df}=n-p}</math>, will be increasingly divergent as the number <math>g</math> of estimates increases. Therefore, the Working–Hotelling method is more suited for large-scale comparisons, whereas Bonferroni is preferred if only a few mean responses are to be estimated. In practice, both methods are usually used first and the narrower interval chosen.<ref name=\"tb2\">Neter, Wasserman and Kutner, pp. 244–245</ref>\n\nAnother alternative to the Working–Hotelling–Scheffé band is the Gavarian band, which is used when a confidence band is needed that maintains equal widths at all levels.<ref name=\":2\">Miller (1966), pp. 123–127</ref>\n\nThe Working–Hotelling procedure is based on the same principles as [[Scheffé's method]], which gives family confidence intervals for all possible [[Contrast (statistics)|contrasts]].<ref name=\":3\">Westfall, Tobias and Wolfinger, pp. 277–280</ref> Their proofs are almost identical.<ref name=\":2\" /> This is because both methods estimate linear combinations of mean response at all factor levels. However, the Working–Hotelling procedure does not deal with contrasts but with different levels of the independent variable, so there is no requirement that the coefficients of the parameters sum up to zero. Therefore, it has one more degree of freedom.<ref name=\":3\" />\n\n== See also ==\n* [[Multiple comparisons]]\n\n== Footnotes ==\n{{reflist|30em}}\n\n== Bibliography ==\n* {{Cite journal|last=Graybill|first=Franklin A.|last2=Bowden|first2=David C.|date=1967-06-01|title=Linear Segment Confidence Bands for Simple Linear Models|journal=Journal of the American Statistical Association|volume=62|issue=318|pages=403–408|doi=10.1080/01621459.1967.10482917|issn=0162-1459}}\n*{{Cite book|title=Simultaneous Statistical Inference|last=Miller|first=Rupert G.|publisher=Springer-Verlag|year=1966|isbn=978-1-4613-8124-2|location=New York|pages=|quote=|via=}}\n*{{Cite book|first=R.|last=Miller|chapter-url=http://onlinelibrary.wiley.com/book/10.1002/0471667196;jsessionid=52A4434648CAAFAC4B5CF6F525F38E57.f04t04|chapter=Multiple Comparisons I|title=Encyclopedia of Statistical Sciences|isbn=9780471667193|doi=10.1002/0471667196|year=2014}}\n*{{Cite book|title=Applied Linear Statistical Models|last1=Neter|last2=Wasserman|last3=Kutner|first1=John|first2=William|first3=Michael|publisher=Richard D Irwin, Inc.|year=1990|isbn=978-0-256-08338-5|location=Tokyo|pages=|quote=|via=}}\n*{{Cite book|title=Multiple comparisons and multiple tests using SAS|last1=Westfall|last2=Tobias|last3=Wolfinger|first1=Peter H|first2=R D|first3=Russell Dean|publisher=SAS Pub.|year=2011|isbn=9781607648857|location=Cary, N.C.|pages=|quote=|via=}}\n*{{Cite journal|last=Working|first=Holbrook|last2=Hotelling|first2=Harold|date=1929-03-01|title=Applications of the Theory of Error to the Interpretation of Trends|journal=Journal of the American Statistical Association|volume=24|issue=165A|pages=73–85|doi=10.1080/01621459.1929.10506274|issn=0162-1459}}\n\n{{Least squares and regression analysis}}\n\n{{DEFAULTSORT:Working-Hotelling procedure}}\n[[Category:Multiple comparisons]]\n[[Category:Regression analysis]]"
    },
    {
      "title": "Glossary of order theory",
      "url": "https://en.wikipedia.org/wiki/Glossary_of_order_theory",
      "text": "This is a glossary of some terms used in various branches of [[mathematics]] that are related to the fields of [[order theory|order]], [[lattice (order)|lattice]], and [[domain theory]]. Note that there is a structured [[list of order topics]] available as well. Other helpful resources might be the following overview articles:\n\n* [[completeness (order theory)|completeness properties]] of partial orders\n* [[distributivity (order theory)|distributivity laws]] of order theory\n* [[limit preserving (order theory)|preservation properties]] of functions between posets.\n\nIn the following, partial orders will usually just be denoted by their carrier sets. As long as the intended meaning is clear from the context, ≤ will suffice to denote the corresponding relational symbol, even without prior introduction. Furthermore, < will denote the [[strict order]] induced by ≤.\n{{compact ToC|side=yes|top=yes|num=yes}}\n__NOTOC__\n== A ==\n* '''Acyclic'''.  A [[binary relation]] is acyclic if it contains no \"cycles\": equivalently, its [[transitive closure]] is [[Antisymmetric relation|antisymmetric]].<ref name=BosSuz/>\n* '''Adjoint'''. See ''Galois connection''.\n* '''[[Alexandrov topology]]'''. For a preordered set ''P'', any upper set ''O'' is '''Alexandrov-open'''. Inversely, a topology is Alexandrov if any intersection of open sets is open.\n* '''[[Algebraic poset]]'''. A poset is algebraic if it has a base of compact elements.\n* '''[[Antichain]]'''. An antichain is a poset in which no two elements are comparable, i.e., there are no two distinct elements ''x'' and ''y'' such that ''x'' ≤ ''y''. In other words, the order relation of an antichain is just the identity relation.\n* '''Approximates relation'''. See ''way-below relation''.\n* A [[relation (mathematics)|relation]] ''R'' on a set ''X'' is '''[[antisymmetric relation|antisymmetric]]''', if ''x R y'' and ''y R x'' implies ''x = y'', for all elements ''x'', ''y'' in ''X''.\n* An '''[[antitone]]''' function ''f'' between posets ''P'' and ''Q'' is a function for which, for all elements ''x'', ''y'' of ''P'', ''x'' ≤ ''y'' (in ''P'') implies ''f''(''y'') ≤ ''f''(''x'') (in ''Q''). Another name for this property is ''order-reversing''. In [[Mathematical analysis|analysis]], in the presence of [[total order]]s, such functions are often called '''monotonically decreasing''', but this is not a very convenient description when dealing with non-total orders. The dual notion is called ''monotone'' or ''order-preserving''.\n* '''[[Asymmetric relation|Asymmetric]]'''. A [[Relation (mathematics)|relation]] ''R'' on a set ''X'' is asymmetric, if ''x R y'' implies ''not y R x'', for all elements ''x'', ''y'' in ''X''.\n* An '''atom''' in a poset ''P'' with least element 0, is an element that is minimal among all elements that are unequal to 0.\n* A '''atomic''' poset ''P'' with least element 0 is one in which, for  every non-zero element ''x'' of ''P'', there is an atom ''a'' of ''P'' with ''a'' ≤ ''x''.\n\n== B ==\n\n* '''Base'''. See ''continuous poset''.\n* A '''[[Boolean algebra (structure)|Boolean algebra]]''' is a distributive lattice with least element 0 and greatest element 1, in which every element ''x'' has a complement ¬''x'', such that ''x'' &and; ¬''x'' = 0 and ''x'' &or; ¬''x'' = 1.\n* A '''[[bounded poset|bounded]]''' poset is one that has a least element and a greatest element.\n* A poset is '''[[bounded complete]]''' if every of its subsets with some upper bound also has a least such upper bound. The dual notion is not common.\n\n== C ==\n\n* '''[[Total order#Chains|Chain]]'''. A chain is a totally ordered set or a totally ordered subset of a poset. See also ''total order''.\n* '''[[Chain complete]]'''.  A [[partially ordered set]] in which every [[Total order#Chains|chain]] has a [[least upper bound]].\n* '''[[Closure operator]]'''. A closure operator on the poset ''P'' is a function ''C'' : ''P'' → ''P'' that is monotone, [[idempotent]], and satisfies ''C''(''x'') ≥ ''x'' for all ''x'' in ''P''.\n* '''[[compact element|Compact]]'''. An element ''x'' of a poset is compact if it is ''[[way-below relation|way below]]'' itself, i.e. ''x''<<''x''. One also says that such an ''x'' is ''finite''.\n* '''Comparable'''. Two elements ''x'' and ''y'' of a poset ''P'' are comparable if either ''x'' ≤ ''y'' or ''y'' ≤ ''x''.\n* '''[[Comparability graph]]'''.  The comparability graph of a poset (''P'', ≤) is the [[Graph (discrete mathematics)|graph]] with vertex set ''P'' in which the edges are those pairs of distinct elements of ''P'' that are comparable under ≤ (and, in particular, under its reflexive reduction <).\n* '''[[Complete Boolean algebra]]'''.  A [[Boolean algebra (structure)|Boolean algebra]] that is a complete lattice.\n* '''[[Complete Heyting algebra]]'''. A [[Heyting algebra]] that is a complete lattice is called a complete Heyting algebra. This notion coincides with the concepts ''frame'' and ''locale''.\n* '''[[Complete lattice]]'''. A complete [[lattice (order)|lattice]] is a poset in which arbitrary (possibly infinite) joins (suprema) and meets (infima) exist.\n* '''[[Complete partial order]]'''. A complete partial order, or '''cpo''', is a [[directed complete partial order]] (q.v.) with least element.\n* '''Complete relation'''.  Synonym for ''[[Total relation]]''.\n* '''Complete semilattice'''. The notion of a ''complete semilattice'' is defined in different ways. As explained in the article on [[completeness (order theory)]], any poset for which either all suprema or all infima exist is already a complete lattice. Hence the notion of a complete semilattice is sometimes used to coincide with the one of a complete lattice. In other cases, complete (meet-) semilattices are defined to be [[bounded complete]] [[complete partial order|cpos]], which is arguably the most complete class of posets that are not already complete lattices.\n* '''[[Completely distributive lattice]]'''. A complete lattice is completely distributive if arbitrary joins distribute over arbitrary meets.\n* '''Completion'''. A completion of a poset is an [[order-embedding]] of the poset in a complete lattice.\n* '''[[Continuous poset]]'''. A poset is continuous if it has a '''base''', i.e. a subset ''B'' of ''P'' such that every element ''x'' of ''P'' is the supremum of a directed set contained in {''y'' in ''B'' | ''y''<<''x''}.\n* '''Continuous function'''. See ''Scott-continuous''.\n* '''Converse'''.  The converse <° of an order < is that in which x <° y whenever y < x.\n* '''Cover'''. An element ''y'' of a poset ''P'' is said to cover an element ''x'' of ''P'' (and is called a cover of ''x'') if ''x'' < ''y'' and there is no element ''z'' of ''P'' such that ''x'' < ''z'' < ''y''.\n* '''[[Complete partial order|cpo]]'''. See ''complete partial order''.\n\n== D ==\n\n* '''dcpo'''. See ''[[directed complete partial order]]''.\n* A '''[[dense order|dense]]''' poset ''P'' is one in which, for all elements ''x'' and ''y'' in ''P'' with ''x'' < ''y'', there is an element ''z'' in ''P'', such that ''x'' < ''z'' < ''y''.  A subset ''Q'' of ''P'' is '''dense in''' ''P'' if for any elements ''x'' < ''y'' in ''P'', there is an element ''z'' in ''Q'' such that ''x'' < ''z'' < ''y''.\n* '''[[directed set|Directed]]'''. A [[non-empty]] subset ''X'' of a poset ''P'' is called directed, if, for all elements ''x'' and ''y'' of ''X'', there is an element ''z'' of ''X'' such that ''x'' ≤ ''z'' and ''y'' ≤ ''z''. The dual notion is called ''filtered''.\n* '''[[Directed complete partial order]]'''. A poset ''D'' is said to be a directed complete poset, or '''dcpo''', if every directed subset of ''D'' has a supremum.\n* '''[[distributivity (order theory)|Distributive]]'''. A lattice ''L'' is called distributive if, for all ''x'', ''y'', and ''z'' in ''L'', we find that ''x'' &and; (''y'' &or; ''z'') = (''x'' &and; ''y'') &or; (''x'' &and; ''z''). This condition is known to be equivalent to its order dual. A meet-[[semilattice]] is distributive if for all elements ''a'', ''b'' and ''x'', ''a'' &and; ''b'' ≤ ''x'' implies the existence of elements ''a' '' ≥ ''a'' and ''b' '' ≥ ''b'' such that ''a' '' &and; ''b' '' = ''x''. See also ''completely distributive''.\n* '''[[domain theory|Domain]]'''. Domain is a general term for objects like those that are studied in [[domain theory]]. If used, it requires further definition.\n* '''Down-set'''. See ''lower set''.\n* '''[[duality (order theory)|Dual]]'''. For a poset (''P'', ≤), the dual order ''P''<sup>''d''</sup> = (''P'', ≥) is defined by setting ''x ≥ y'' [[if and only if]] ''y ≤ x''. The dual order of ''P'' is sometimes denoted by ''P''<sup>op</sup>, and is also called ''opposite'' or ''converse'' order. Any order theoretic notion induces a dual notion, defined by applying the original statement to the order dual of a given set. This exchanges ≤ and ≥, meets and joins, zero and unit.\n\n== E ==\n\n* '''Extension'''.  For partial orders ≤ and ≤′ on a set ''X'', ≤′ is an extension of ≤ provided that for all elements ''x'' and ''y'' of ''X'', ''x'' ≤ ''y'' implies that ''x'' ≤′ ''y''.\n\n== F ==\n\n* '''[[filter (mathematics)|Filter]]'''. A subset ''X'' of a poset ''P'' is called a filter if it is a filtered upper set. The dual notion is called ''ideal''.\n* '''Filtered'''. A [[non-empty]] subset ''X'' of a poset ''P'' is called filtered, if, for all elements ''x'' and ''y'' of ''X'', there is an element ''z'' of ''X'' such that ''z'' ≤ ''x'' and ''z'' ≤ ''y''. The dual notion is called ''directed''.\n* '''Finite element'''. See ''compact''.\n* '''[[complete Heyting algebra|Frame]]'''. A frame ''F'' is a complete lattice, in which, for every ''x'' in ''F'' and every subset ''Y'' of ''F'', the infinite distributive law ''x'' &and; <math>\\bigvee</math>''Y'' = <math>\\bigvee</math>{''x'' &and; ''y'' | ''y'' in ''Y''} holds. Frames are also known as ''locales'' and as complete [[Heyting algebra]]s.\n\n== G ==\n\n* '''[[Galois connection]]'''. Given two posets ''P'' and ''Q'', a pair of monotone functions ''F'':''P'' → ''Q'' and ''G'':''Q'' → ''P'' is called a Galois connection, if ''F''(''x'') ≤ ''y'' is equivalent to ''x'' ≤ ''G''(''y''), for all ''x'' in ''P'' and ''y'' in ''Q''. ''F'' is called the '''lower adjoint''' of ''G'' and ''G'' is called the '''upper adjoint''' of ''F''.\n* '''[[Greatest element]]'''. For a subset ''X'' of a poset ''P'', an element ''a'' of ''X'' is called the greatest element of ''X'', if ''x'' ≤ ''a'' for every element ''x'' in ''X''. The dual notion is called ''least element''.\n* '''Ground set'''. The ground set of a poset (''X'', ≤) is the set ''X'' on which the partial order ≤ is defined.\n\n== H ==\n\n* '''[[Heyting algebra]]'''. A Heyting algebra ''H'' is a bounded lattice in which the function ''f''<sub>''a''</sub>: ''H'' → ''H'', given by ''f''<sub>''a''</sub>(''x'') = ''a'' &and; ''x'' is the lower adjoint of a [[Galois connection]], for every element ''a'' of ''H''. The upper adjoint of ''f''<sub>''a''</sub> is then denoted by ''g''<sub>''a''</sub>, with ''g''<sub>''a''</sub>(''x'') = ''a'' ⇒; ''x''. Every [[Boolean algebra (structure)|Boolean algebra]] is a Heyting algebra.\n* '''[[Hasse diagram]]'''. A Hasse diagram is a type of mathematical diagram used to represent a finite partially ordered set, in the form of a drawing of its [[transitive reduction]].\n\n== I ==\n\n* An '''[[ideal (order theory)|ideal]]''' is a subset ''X'' of a poset ''P'' that is a directed lower set. The dual notion is called ''filter''.\n* The '''[[incidence algebra]]''' of a poset is the [[associative algebra]] of all scalar-valued functions on intervals, with addition and scalar multiplication defined pointwise, and multiplication defined as a certain convolution; see [[incidence algebra]] for the details.\n* '''[[Infimum]]'''. For a poset ''P'' and a subset ''X'' of ''P'', the greatest element in the set of lower bounds of ''X'' (if it exists, which it may not) is called the '''infimum''', '''meet''', or '''greatest lower bound''' of ''X''. It is denoted by inf ''X'' or <math>\\bigwedge</math>''X''. The infimum of two elements may be written as inf{''x'',''y''} or ''x'' &and; ''y''. If the set ''X'' is finite, one speaks of a '''finite infimum'''.  The dual notion is called ''supremum''.\n* '''[[Interval (mathematics)|Interval]]'''. For two elements ''a'', ''b'' of a partially ordered set ''P'', the ''interval'' [''a'',''b''] is the subset {''x'' in ''P'' | ''a'' ≤ ''x'' ≤ ''b''} of ''P''. If ''a'' ≤ ''b'' does not hold the interval will be empty.\n*<span id=\"interval finite poset\"></span>'''Interval finite poset'''. A partially ordered set ''P'' is '''interval finite''' if every interval of the form {x in P | x ≤ a} is a finite set.<ref>{{harvnb|Deng|2008|loc=p. 22}}</ref>\n* '''Inverse'''.  See ''converse''.\n* '''[[Irreflexive]]'''. A [[Relation (mathematics)|relation]] ''R'' on a set ''X'' is irreflexive, if there is no element ''x'' in ''X'' such that ''x R x''.\n* '''Isotone'''.  See ''monotone''.\n\n== J ==\n\n* '''Join'''. See ''supremum''.\n\n== L ==\n\n* '''[[Lattice (order)|Lattice]]'''. A lattice is a poset in which all non-empty finite joins (suprema) and meets (infima) exist.\n* '''[[Least element]]'''. For a subset ''X'' of a poset ''P'', an element ''a'' of ''X'' is called the least element of ''X'', if ''a'' ≤ ''x'' for every element ''x'' in ''X''. The dual notion is called ''greatest element''.\n* The '''length''' of a chain is the number of elements less one.  A chain with 1 element has length 0, one with 2 elements has length 1, etc.\n* '''Linear'''. See ''total order''.\n* '''[[Linear extension]]'''. A linear extension of a partial order is an extension that is a linear order, or total order.\n* '''[[complete Heyting algebra|Locale]]'''. A locale is a ''complete Heyting algebra''. Locales are also called ''frames'' and appear in [[Stone duality]] and [[pointless topology]].\n* '''[[Locally finite poset]]'''. A partially ordered set ''P'' is ''locally finite'' if every interval [''a'', ''b''] = {''x'' in ''P'' | ''a'' ≤ ''x'' ≤ ''b''} is a finite set.\n* '''[[Lower bound]]'''. A lower bound of a subset ''X'' of a poset ''P'' is an element ''b'' of ''P'', such that ''b'' ≤ ''x'', for all ''x'' in ''X''. The dual notion is called ''upper bound''.\n* '''[[Lower set]]'''. A subset ''X'' of a poset ''P'' is called a lower set if, for all elements ''x'' in ''X'' and ''p'' in ''P'', ''p'' ≤ ''x'' implies that ''p'' is contained in ''X''. The dual notion is called ''upper set''.\n\n== M ==\n\n* '''Maximal chain'''. A [[Total order#Chains|chain]] in a poset to which no element can be added without losing the property of being totally ordered. This is stronger than being a saturated chain, as it also excludes the existence of elements either less than all elements of the chain or greater than all its elements. A finite saturated chain is maximal if and only if it contains both a minimal and a maximal element of the poset.\n* '''[[Maximal element]]'''. A maximal element of a subset ''X'' of a poset ''P'' is an element ''m'' of ''X'', such that ''m'' ≤ ''x'' implies ''m'' = ''x'', for all ''x'' in ''X''. The dual notion is called ''minimal element''.\n* '''[[greatest element|Maximum element]]'''. Synonym of greatest element. For a subset ''X'' of a poset ''P'', an element ''a'' of ''X'' is called the maximum element of ''X'' if ''x'' ≤ ''a'' for every element ''x'' in ''X''. A maxim''um'' element is necessarily maxim''al'', but the converse need not hold.\n* '''Meet'''. See ''infimum''.\n* '''[[Minimal element]]'''. A minimal element of a subset ''X'' of a poset ''P'' is an element ''m'' of ''X'', such that ''x'' ≤ ''m'' implies ''m'' = ''x'', for all ''x'' in ''X''. The dual notion is called ''maximal element''.\n* '''[[least element|Minimum element]]'''. Synonym of least element. For a subset ''X'' of a poset ''P'', an element ''a'' of ''X'' is called the minimum element of ''X'' if ''x'' ≥ ''a'' for every element ''x'' in ''X''. A minim''um'' element is necessarily minim''al'', but the converse need not hold.\n* '''[[monotone function|Monotone]]'''. A function ''f'' between posets ''P'' and ''Q'' is monotone if, for all elements ''x'', ''y'' of ''P'', ''x'' ≤ ''y'' (in ''P'') implies ''f''(''x'') ≤ ''f''(''y'') (in ''Q''). Other names for this property are ''isotone'' and ''order-preserving''. In [[Mathematical analysis|analysis]], in the presence of [[total order]]s, such functions are often called '''monotonically increasing''', but this is not a very convenient description when dealing with non-total orders. The dual notion is called ''antitone'' or ''order reversing''.\n\n== O ==\n\n* '''Order-dual'''.  The order dual of a partially ordered set is the same set with the partial order relation replaced by its converse.\n* '''[[Order-embedding]]'''. A function ''f'' between posets ''P'' and ''Q'' is an order-embedding if, for all elements ''x'', ''y'' of ''P'', ''x'' ≤ ''y'' (in ''P'') is equivalent to ''f''(''x'') ≤ ''f''(''y'') (in ''Q'').\n* '''[[Order isomorphism]]'''. A mapping ''f'': ''P'' → ''Q'' between two posets ''P'' and ''Q'' is called an order isomorphism, if it is [[bijective]] and both ''f'' and ''f''<sup>−1</sup> are [[monotone function|monotone]]. Equivalently, an order isomorphism is a surjective ''order embedding''.\n* '''[[Order-preserving]]'''. See ''monotone''.\n* '''[[Order-reversing]]'''. See ''antitone''.\n\n== P ==\n\n* '''[[Partial order]]'''. A partial order is a [[binary relation]] that is [[reflexive relation|reflexive]], [[antisymmetric relation|antisymmetric]], and [[transitive relation|transitive]]. In a slight abuse of terminology, the term is sometimes also used to refer not to such a relation, but to its corresponding partially ordered set.\n* [[Partially ordered set]]. A partially ordered set (''P'', ≤), or ''poset'' for short, is a set ''P'' together with a partial order ≤ on ''P''.\n* '''Poset'''. A partially ordered set.\n* '''[[Preorder]]'''. A preorder is a [[binary relation]] that is [[reflexive relation|reflexive]] and [[transitive relation|transitive]]. Such orders may also be called ''quasiorders''. The term ''preorder'' is also used to denote an [[#A|acyclic]] [[binary relation]] (also called an ''acyclic digraph'').\n* '''[[limit-preserving function (order theory)|Preserving]]'''. A function ''f'' between posets ''P'' and ''Q'' is said to preserve suprema (joins), if, for all subsets ''X'' of ''P'' that have a supremum sup ''X'' in ''P'', we find that sup{''f''(''x''): ''x'' in ''X''} exists and is equal to ''f''(sup ''X''). Such a function is also called '''join-preserving'''. Analogously, one says that ''f'' preserves finite, non-empty, directed, or arbitrary joins (or meets). The converse property is called ''join-reflecting''.\n* '''[[order ideal|Prime]]'''. An ''ideal'' ''I'' in a lattice ''L'' is said to be prime, if, for all elements ''x'' and ''y'' in ''L'', ''x'' &and; ''y'' in ''I'' implies ''x'' in ''I'' or ''y'' in ''I''. The dual notion is called a ''prime filter''. Equivalently, a set is a prime filter [[if and only if]] its complement is a prime ideal.\n* '''[[order ideal|Principal]]'''. A filter is called ''principal filter'' if it has a least element. Dually, a ''principal ideal'' is an ideal with a greatest element. The least or greatest elements may also be called ''principal elements'' in these situations.\n* '''Projection (operator)'''. A self-map on a [[poset|partially ordered set]] that is [[monotonic function|monotone]] and [[idempotent]] under [[function composition]]. Projections play an important role in [[domain theory]].\n* '''Pseudo-complement'''. In a [[Heyting algebra]], the element ''x'' ⇒; ''0'' is called the pseudo-complement of ''x''. It is also given by sup{''y'' :  ''y'' &and; ''x'' = 0}, i.e. as the least upper bound of all elements ''y'' with ''y'' &and; ''x'' = 0.\n\n== Q ==\n\n* '''Quasiorder'''. See ''preorder''.\n* '''[[Quasitransitive relation|Quasitransitive]]'''.  A relation is quasitransitive if the relation on distinct elements is transitive.  Transitive implies quasitransitive and quasitransitive implies acyclic.<ref name=BosSuz>{{cite book | title=Consistency, choice and rationality | first1=Walter | last1=Bossert | first2=Kōtarō | last2=Suzumura | publisher=Harvard University Press | year=2010 | isbn=0674052994 }}</ref>\n\n== R ==\n\n* '''[[limit preserving (order theory)|Reflecting]]'''. A function ''f'' between posets ''P'' and ''Q'' is said to reflect suprema (joins), if, for all subsets ''X'' of ''P'' for which the supremum sup{''f''(''x''): ''x'' in ''X''} exists and is of the form ''f''(''s'') for some ''s'' in ''P'', then we find that sup ''X'' exists and that sup ''X'' = ''s'' . Analogously, one says that ''f'' reflects finite, non-empty, directed, or arbitrary joins (or meets). The converse property is called ''join-preserving''.\n* '''[[reflexive relation|Reflexive]]'''. A [[binary relation]] ''R'' on a set ''X'' is reflexive, if ''x R x'' holds for every element ''x'' in ''X''.\n* '''Residual'''.  A dual map attached to a [[residuated mapping]].\n* '''[[Residuated mapping]]'''.  A monotone map for which the preimage of a principal down-set is again principal.  Equivalently, one component of a Galois connection.\n\n== S ==\n\n* '''Saturated chain'''. A [[Total order#Chains|chain]] such that no element can be added ''between two of its elements'' without losing the property of being totally ordered. If the chain is finite, this means that in every pair of successive elements the larger one covers the smaller one. See also maximal chain.\n* '''[[scattered order|Scattered]]'''. A total order is scattered if it has no densely ordered subset.\n* '''[[Scott-continuous]]'''. A monotone function ''f'' : ''P'' → ''Q'' between posets ''P'' and ''Q'' is Scott-continuous if, for every directed set ''D'' that has a supremum sup ''D'' in ''P'', the set {''fx'' | ''x'' in ''D''} has the supremum ''f''(sup ''D'') in ''Q''. Stated differently, a Scott-continuous function is one that [[limit preserving function (order theory)|preserves]] all directed suprema. This is in fact equivalent to being [[continuity (topology)|continuous]] with respect to the [[Scott topology]] on the respective posets.\n* '''[[Scott domain]]'''. A Scott domain is a partially ordered set which is a [[bounded complete]] [[algebraic poset|algebraic]] [[complete partial order|cpo]].\n* '''Scott open'''. See ''Scott topology''.\n* '''Scott topology'''. For a poset ''P'', a subset ''O'' is '''Scott-open''' if it is an [[upper set]] and all directed sets ''D'' that have a supremum in ''O'' have non-empty intersection with ''O''. The set of all Scott-open sets forms a [[topology]], the '''Scott topology'''.\n* '''[[Semilattice]]'''. A semilattice is a poset in which either all finite non-empty joins (suprema) or all finite non-empty meets (infima) exist. Accordingly, one speaks of a '''join-semilattice''' or '''meet-semilattice'''.\n* '''Smallest element'''. See ''least element''.\n* [[Sperner property of a partially ordered set]]\n* [[Sperner poset]]\n* [[Strictly Sperner poset]]\n* [[Strongly Sperner poset]]\n* '''[[Strict order]]'''. A strict order is a [[binary relation]] that is [[antisymmetric relation|antisymmetric]], [[transitive relation|transitive]], and [[irreflexive]].\n* '''[[Supremum]]'''. For a poset ''P'' and a subset ''X'' of ''P'', the [[least element]] in the set of [[upper bound]]s of ''X'' (if it exists, which it may not) is called the '''supremum''', '''join''', or '''least upper bound''' of ''X''. It is denoted by sup ''X'' or <math>\\bigvee</math>''X''. The supremum of two elements may be written as sup{''x'',''y''} or ''x'' &or; ''y''. If the set ''X'' is finite, one speaks of a '''finite supremum'''. The dual notion is called ''infimum''.\n* '''Suzumura consistency'''.  A binary relation R is Suzumura consistent if ''x'' R<sup>&lowast;</sup> ''y'' implies that ''x'' R ''y'' or not ''y'' R ''x''.<ref name=BosSuz/>\n* '''[[Symmetric relation|Symmetric]]'''. A [[Relation (mathematics)|relation]] ''R'' on a set ''X'' is symmetric, if ''x R y'' implies ''y R x'', for all elements ''x'', ''y'' in ''X''.\n\n== T ==\n\n* '''Top'''. See ''unit''.\n* '''[[Total order]]'''. A total order ''T'' is a partial order in which, for each ''x'' and ''y'' in ''T'', we have ''x'' ≤ ''y'' or ''y'' ≤ ''x''. Total orders are also called ''linear orders'' or ''chains''.\n* '''[[Total relation]]'''.  A total or complete relation ''R'' on a set ''X'' has the property that for all elements ''x'', ''y'' of ''X'', at least one of ''x R y'' or ''y R x'' holds.\n* '''[[transitive relation|Transitive]]'''. A [[Relation (mathematics)|relation]] ''R'' on a set ''X'' is transitive, if ''x R y'' and ''y R z'' imply ''x R z'', for all elements ''x'', ''y'', ''z'' in ''X''.\n* '''[[Transitive closure]]'''.  The transitive closure R<sup>&lowast;</sup> of a relation R consists of all pairs ''x'',''y'' for which there cists a finite chain ''x'' R ''a'', ''a'' R ''b'', ..., ''z'' R ''y''.<ref name=BosSuz/>\n\n== U ==\n\n* '''Unit'''. The [[greatest element]] of a poset ''P'' can be called ''unit'' or just ''1'' (if it exists). Another common term for this element is '''top'''. It is the infimum of the empty set and the supremum of ''P''. The dual notion is called ''zero''.\n* '''Up-set'''. See ''upper set''.\n* '''[[Upper bound]]'''. An upper bound of a subset ''X'' of a poset ''P'' is an element ''b'' of ''P'', such that ''x'' ≤ ''b'', for all ''x'' in ''X''. The dual notion is called ''lower bound''.\n* '''[[Upper set]]'''. A subset ''X'' of a poset ''P'' is called an upper set if, for all elements ''x'' in ''X'' and ''p'' in ''P'', ''x'' ≤ ''p'' implies that ''p'' is contained in ''X''. The dual notion is called ''lower set''.\n\n== V ==\n\n* '''Valuation'''.  Given a lattice <math>X</math>, a valuation <math>\\nu : X \\to [0,1]</math> is strict (i.e., <math>\\nu(\\emptyset)=0</math>), monotone, modular (i.e., <math>\\nu(U) + \\nu(V) = \\nu(U \\cup V) + \\nu(U \\cap V)</math>) and positive.  Continuous valuations are a generalization of measures.\n\n== W ==\n\n* '''[[Way-below relation]]'''. In a poset ''P'', some element ''x'' is ''way below'' ''y'', written ''x''<<''y'', if for all directed subsets ''D'' of ''P'' which have a supremum, ''y'' ≤ ''sup D'' implies ''x'' ≤ ''d'' for some ''d'' in ''D''. One also says that ''x'' '''approximates''' ''y''. See also [[domain theory]].\n* '''[[Weak order]]'''.  A partial order ≤ on a set ''X'' is a weak order provided that the poset (X, ≤) is [[isomorphic]] to a countable collection of sets ordered by comparison of [[cardinality]].\n\n== Z ==\n\n* '''Zero'''. The [[least element]] of a poset ''P'' can be called ''zero'' or just ''0'' (if it exists). Another common term for this element is '''bottom'''. Zero is the supremum of the empty set and the infimum of ''P''. The dual notion is called ''unit''.\n\n==Notes==\n{{reflist}}\n\n==References==\n\nThe definitions given here are consistent with those that can be found in the following standard reference books:\n\n* B. A. Davey and H. A. Priestley, ''Introduction to Lattices and Order'', 2nd Edition, Cambridge University Press, 2002.\n* G. Gierz, K. H. Hofmann, K. Keimel, J. D. Lawson, M. Mislove and D. S. Scott, ''Continuous Lattices and Domains'', In ''Encyclopedia of Mathematics and its Applications'', Vol. 93, Cambridge University Press, 2003.\n\nSpecific definitions:\n\n*{{Citation\n| last=Deng\n| first=Bangming\n| title=Finite dimensional algebras and quantum groups\n| year=2008\n| publisher=American Mathematical Society\n| isbn=978-0-8218-4186-0\n| series=Mathematical surveys and monographs\n| volume=150\n}}\n\n[[Category:Glossaries of mathematics|Order theory]]\n[[Category:Order theory| ]]"
    },
    {
      "title": "List of order theory topics",
      "url": "https://en.wikipedia.org/wiki/List_of_order_theory_topics",
      "text": "'''[[Order theory]]''' is a branch of [[mathematics]] that studies various kinds of objects (often [[binary relation]]s) that capture the intuitive notion of ordering, providing a framework for saying when one thing is \"less than\" or \"precedes\" another.\n\nAn alphabetical list of many notions of order theory can be found in the [[order theory glossary]]. See also [[inequality (mathematics)|inequality]], [[extreme value]] and [[mathematical optimization]].\n\n==Overview==\n*[[Partially ordered set]]\n*[[Preorder]]\n*[[Totally ordered set]]\n**[[Total preorder]]\n**[[Chain (order theory)|Chain]]\n**[[Trichotomy (mathematics)|Trichotomy]]\n**[[Extended real number line]]\n*[[Antichain]]\n*[[Strict order]]\n*[[Hasse diagram]]\n**[[Directed acyclic graph]]\n*[[Duality (order theory)]]\n*[[Product order]]\n\n==Distinguished elements of partial orders==\n\n*[[Greatest element]] (maximum, top, unit), [[Least element]] (minimum, bottom, zero)\n*[[Maximal element]], [[minimal element]]\n*[[Upper bound]]\n**[[Least upper bound]] (supremum, join)\n**[[Greatest lower bound]] (infimum, meet)\n**[[Limit superior and limit inferior]]\n*[[Irreducible element (order theory)|Irreducible element]]\n*[[Prime element (order theory)|Prime element]]\n*[[Compact element]]\n\n==Subsets of partial orders==\n\n*[[Cofinal (mathematics)|Cofinal]] and [[Cofinal (mathematics)|coinitial]] set, sometimes also called [[Cofinal (mathematics)|dense]]\n*[[Meet-dense set]] and [[join-dense set]]\n*[[Linked set]] (upwards and downwards)\n*[[Directed set]] (upwards and downwards)\n*[[Centered set|centered]] and [[centered set|σ-centered set]]\n*[[Net (mathematics)]]\n*[[Upper set]] and lower set\n*[[ideal (order theory)|Ideal]] and [[Filter (mathematics)|filter]]\n**[[Ultrafilter]]\n\n==Special types of partial orders==\n\n*[[Completeness (order theory)]]\n*[[Dense order]]\n*[[Distributivity (order theory)]]\n**[[modular lattice]]\n**[[distributive lattice]]\n**[[completely distributive lattice]]\n*[[Ascending chain condition]]\n**[[Infinite descending chain]]\n*[[Countable chain condition]], often abbreviated as ''ccc''\n*[[Knaster's condition]], sometimes denoted ''property (K)''\n\n=== [[Well-order]]s ===\n*[[Well-founded relation]]\n*[[Ordinal number]]\n*[[Well-quasi-ordering]]\n\n===[[completeness (order theory)|Completeness properties]]===\n* [[Semilattice]]\n* [[lattice (order)|Lattice]]\n* (Directed) [[complete partial order]], (d)cpo\n* [[Bounded complete]]\n* [[Complete lattice]]\n**[[Knaster&ndash;Tarski theorem]]\n* [[Infinite divisibility]]\n\n===Orders with further [[abstract algebra|algebraic]] operations===\n*[[Heyting algebra]]\n**[[Relatively complemented lattice]]\n*[[Complete Heyting algebra]]\n**[[Pointless topology]]\n*[[MV-algebra]]\n*[[Ockham algebra]]s:\n**[[Stone algebra]]\n**[[De Morgan algebra]]\n***[[Kleene algebra (with involution)]]\n***[[Łukasiewicz–Moisil algebra]]\n**[[Boolean algebra (structure)]]\n***[[Boolean ring]]\n***[[Complete Boolean algebra]]\n*[[Orthocomplemented lattice]]\n*[[Quantale]]\n\n===Orders in [[algebra]]===\n*[[Partially ordered monoid]]\n*[[Ordered group]]\n**[[Archimedean property]]\n*[[Ordered ring]]\n*[[Ordered field]]\n*[[Artinian ring]]\n*[[Noetherian]]\n*[[Linearly ordered group]]\n*[[Monomial order]]\n*[[Weak order of permutations]]\n*[[Bruhat order]] on a Coxeter group\n*[[Incidence algebra]]\n\n==[[Function (mathematics)|Functions]] between partial orders==\n*[[Monotonic]]\n*[[Pointwise order]] of functions\n*[[Galois connection]]\n*[[Order embedding]]\n*[[Order isomorphism]]\n*[[Closure operator]]\n*Functions that [[limit-preserving function (order theory)|preserve]] suprema/infima\n\n==[[Completion (order theory)|Completions]] and [[free object|free constructions]]==\n*[[Dedekind completion]]\n*[[Ideal completion]]\n\n==Domain theory==\n{{Main|Domain theory}}\n\n*[[Way-below relation]]\n*[[Continuous poset]]\n**[[Continuous lattice]]\n*[[Algebraic poset]]\n**[[Scott domain]]\n**[[Algebraic lattice]]\n*[[Scott information system]]\n*[[Powerdomain]]\n*[[Scott topology]]\n*[[Scott continuity]]\n\n==Orders in [[mathematical logic]]==\n*[[Lindenbaum algebra]]\n*[[Zorn's lemma]]\n**[[Hausdorff maximality theorem]]\n*[[Boolean prime ideal theorem]]\n*[[Ultrafilter]]\n*[[Ultrafilter lemma]]\n*[[Tree (set theory)]]\n*[[Tree (descriptive set theory)]]\n*[[Suslin's problem]]\n*[[Absorption law]]\n*[[Prewellordering]]\n\n==Orders in [[topology]]==\n*[[Stone duality]]\n**[[Stone's representation theorem for Boolean algebras]]\n*[[Specialization (pre)order]]\n*[[Order topology]] of a total order (open interval topology)\n*[[Alexandrov topology]]\n*[[Upper topology]]\n*[[Scott topology]]\n**[[Scott continuity]]\n*[[Lawson topology]]\n*[[Finer topology]]\n\n{{Outline footer}}\n\n[[Category:Mathematics-related lists|Order]]\n[[Category:Order theory| ]]\n[[Category:Wikipedia outlines|Order theory]]"
    },
    {
      "title": "Order theory",
      "url": "https://en.wikipedia.org/wiki/Order_theory",
      "text": "{{Use American English|date = March 2019}}\n{{Short description|Branch of mathematics}}\n{{Outline|Outline of order theory}}\n{{More footnotes|date=December 2015}}\n'''Order theory''' is a branch of [[mathematics]] which investigates the intuitive notion of order using [[binary relation]]s. It provides a formal framework for describing statements such as \"this is less than that\" or \"this precedes that\". This article introduces the field and provides basic definitions. A list of order-theoretic terms can be found in the [[order theory glossary]].\n\n== Background and motivation ==\nOrders are everywhere in mathematics and related fields like [[computer science]]. The first order often discussed in [[primary school]] is the standard order on the [[natural numbers]] e.g. \"2 is less than 3\", \"10 is greater than 5\", or \"Does Tom have fewer cookies than Sally?\". This intuitive concept can be extended to orders on other sets of [[number]]s, such as the [[integer]]s and the [[real number|reals]]. The idea of being greater than or less than another number is one of the basic intuitions of [[number systems]] (compare with [[numeral systems]]) in general (although one usually is also interested in the actual [[Subtraction|difference]] of two numbers, which is not given by the order). Other familiar examples of orderings are the [[alphabetical order]] of words in a dictionary and the [[genealogy|genealogical]] property of [[lineal descent]] within a group of people.\n\nThe notion of order is very general, extending beyond contexts that have an immediate, intuitive feel of sequence or relative quantity. In other contexts orders may capture notions of containment or specialization. Abstractly, this type of order amounts to the [[subset|subset relation]], e.g., \"[[Pediatricians]] are [[physicians]],\" and \"[[Circles]] are merely special-case [[ellipse]]s.\"\n\nSome orders, like \"less-than\" on the natural numbers and alphabetical order on words, have a special property: each element can be ''compared'' to any other element, i.e. it is smaller (earlier) than, larger (later) than, or identical to. However, many other orders do not. Consider for example the subset order on a collection of [[Set (mathematics)|sets]]: though the set of birds and the set of dogs are both subsets of the set of animals, neither the birds nor the dogs constitutes a subset of the other. Those orders like the \"subset-of\" relation for which there exist ''incomparable'' elements are called ''[[partial order]]s''; orders for which every pair of elements is comparable are ''[[total order]]s''.\n\nOrder theory captures the intuition of orders that arises from such examples in a general setting. This is achieved by specifying properties that a relation ≤ must have to be a mathematical order. This more abstract approach makes much sense, because one can derive numerous theorems in the general setting, without focusing on the details of any particular order. These insights can then be readily transferred to many less abstract applications.\n\nDriven by the wide practical usage of orders, numerous special kinds of ordered sets have been defined, some of which have grown into mathematical fields of their own. In addition, order theory does not restrict itself to the various classes of ordering relations, but also considers appropriate [[function (mathematics)|functions]] between them. A simple example of an order theoretic property for functions comes from [[Functional analysis|analysis]] where [[Monotonic function|monotone]] functions are frequently found.\n\n== Basic definitions ==\nThis section introduces ordered sets by building upon the concepts of [[set theory]], [[arithmetic]], and [[binary relation]]s.\n\n=== Partially ordered sets ===<!-- This section is linked from [[Indifference curve]] -->\nOrders are special binary relations. Suppose that ''P'' is a set and that ≤ is a relation on ''P''. Then ≤ is a '''partial order''' if it is [[reflexive relation|reflexive]], [[antisymmetric relation|antisymmetric]], and [[transitive relation|transitive]], i.e., for all ''a'', ''b'' and ''c'' in ''P'', we have that:\n\n:''a'' ≤ ''a'' (reflexivity)\n: if ''a'' ≤ ''b'' and ''b'' ≤ ''a'' then ''a'' = ''b'' (antisymmetry)\n: if ''a'' ≤ ''b'' and ''b'' ≤ ''c'' then ''a'' ≤ ''c'' (transitivity).\n\nA set with a [[partially ordered set|partial order]] on it is called a '''partially ordered set''', '''poset''', or just an '''ordered set''' if the intended meaning is clear. By checking these properties, one immediately sees that the well-known orders on [[natural number]]s, [[integer]]s, [[rational number]]s and [[real number|reals]] are all orders in the above sense. However, they have the additional property of being [[connex relation|connex]], i.e., for all ''a'' and ''b'' in ''P'', we have that:\n\n:''a'' ≤ ''b'' or ''b'' ≤ ''a'' (connexity).\n\nA connex partial order is called a [[total order]]. These orders can also be called '''linear orders''' or '''chains'''. While many classical orders are linear, the [[subset]] order on sets provides an example where this is not the case. Another example is given by the divisibility (or \"is-a-[[divisor|factor]]-of\") relation |. For two natural numbers ''n'' and ''m'', we write ''n''|''m'' if ''n'' [[division (mathematics)|divides]] ''m'' without remainder. One easily sees that this yields a partial order.\nThe identity relation = on any set is also a partial order in which every two distinct elements are incomparable. It is also the only relation that is both a partial order and an [[equivalence relation]]. Many advanced properties of posets are interesting mainly for non-linear orders.\n\n=== Visualizing a poset ===\n[[File:Lattice of the divisibility of 60.svg|thumb|right|250px|[[Hasse diagram]] of the set of all divisors of 60, partially ordered by divisibility]]\n[[Hasse diagram]]s can visually represent the elements and relations of a partial ordering. These are [[graph drawing]]s where the [[vertex (graph theory)|vertices]] are the elements of the poset and the ordering relation is indicated by both the [[graph theory|edges]] and the relative positioning of the vertices. Orders are drawn bottom-up: if an element ''x'' is smaller than (precedes) ''y'' then there exists a path from ''x'' to ''y'' that is directed upwards. It is often necessary for the edges connecting elements to cross each other, but elements must never be located within an edge. An instructive exercise is to draw the Hasse diagram for the set of natural numbers that are smaller than or equal to 13, ordered by | (the ''[[Divisor|divides]]'' relation).\n\nEven some infinite sets can be diagrammed by superimposing an [[ellipsis]] (...) on a finite sub-order. This works well for the natural numbers, but it fails for the reals, where there is no immediate successor above 0; however, quite often one can obtain an intuition related to diagrams of a similar kind{{vague|date=January 2017}}.\n\n=== Special elements within an order ===\nIn a partially ordered set there may be some elements that play a special role. The most basic example is given by the '''least element''' of a [[poset]]. For example, 1 is the [[least element]] of the positive integers and the [[empty set]] is the least set under the subset order. Formally, an element ''m'' is a least element if:\n\n:  ''m'' ≤ ''a'', for all elements ''a'' of the order.\n\nThe notation 0 is frequently found for the least element, even when no numbers are concerned. However, in orders on sets of numbers, this notation might be inappropriate or ambiguous, since the number 0 is not always least. An example is given by the above divisibility order |, where 1 is the least element since it divides all other numbers. In contrast, 0 is the number that is divided by all other numbers. Hence it is the '''greatest element''' of the order.  Other frequent terms for the least and greatest elements is '''bottom''' and '''top''' or '''zero''' and '''unit'''.\n\nLeast and [[greatest element]]s may fail to exist, as the example of the real numbers shows. But if they exist, they are always unique. In contrast, consider the divisibility relation | on the set {2,3,4,5,6}. Although this set has neither top nor bottom, the elements 2, 3, and 5 have no elements below them, while 4, 5 and 6 have none above. Such elements are called '''minimal''' and '''maximal''', respectively. Formally, an element ''m'' is [[minimal element|minimal]] if:\n\n: ''a'' ≤ ''m'' implies ''a'' = ''m'', for all elements ''a'' of the order.\n\nExchanging ≤ with ≥ yields the definition of [[maximal element|maximality]]. As the example shows, there can be many maximal elements and some elements may be both maximal and minimal (e.g. 5 above). However, if there is a least element, then it is the only minimal element of the order. Again, in infinite posets maximal elements do not always exist - the set of all ''finite'' subsets of a given infinite set, ordered by subset inclusion, provides one of many counterexamples. An important tool to ensure the existence of maximal elements under certain conditions is '''[[Zorn's Lemma]]'''.\n\nSubsets of partially ordered sets inherit the order. We already applied this by considering the subset {2,3,4,5,6} of the natural numbers with the induced divisibility ordering. Now there are also elements of a poset that are special with respect to some subset of the order. This leads to the definition of '''[[upper bound]]s'''. Given a subset ''S'' of some poset ''P'', an upper bound of ''S'' is an element ''b'' of ''P'' that is above all elements of ''S''. Formally, this means that\n\n: ''s'' ≤ ''b'', for all ''s'' in ''S''.\n\nLower bounds again are defined by inverting the order. For example, -5 is a lower bound of the natural numbers as a subset of the integers. Given a set of sets, an upper bound for these sets under the subset ordering is given by their [[union (set theory)|union]]. In fact, this upper bound is quite special: it is the smallest set that contains all of the sets. Hence, we have found the '''[[least upper bound]]''' of a set of sets. This concept is also called '''supremum''' or '''join''', and for a set ''S'' one writes sup(''S'') or <math>\\bigvee S</math> for its least upper bound. Conversely, the '''[[greatest lower bound]]''' is known as '''[[infimum]]''' or '''meet''' and denoted inf(''S'') or <math>\\bigwedge S</math>. These concepts play an important role in many applications of order theory. For two elements ''x'' and ''y'', one also writes <math>x\\vee y</math> and <math>x\\wedge y</math> for sup({''x'',''y''}) and inf({''x'',''y''}), respectively.\n\nFor example, 1 is the infimum of the positive integers as a subset of integers.\n\nFor another example, consider again the relation | on natural numbers. The least upper bound of two numbers is the smallest number that is divided by both of them, i.e. the [[least common multiple]] of the numbers. Greatest lower bounds in turn are given by the [[greatest common divisor]].\n\n=== Duality ===\nIn the previous definitions, we often noted that a concept can be defined by just inverting the ordering in a former definition. This is the case for \"least\" and \"greatest\", for \"minimal\" and \"maximal\", for \"upper bound\" and \"lower bound\", and so on. This is a general situation in order theory: A given order can be inverted by just exchanging its direction, pictorially flipping the Hasse diagram top-down. This yields the so-called '''dual''', '''inverse''', or '''opposite order'''.\n\nEvery order theoretic definition has its dual: it is the notion one obtains by applying the definition to the inverse order. Since all concepts are symmetric, this operation preserves the theorems of partial orders. For a given mathematical result, one can just invert the order and replace all definitions by their duals and one obtains another valid theorem. This is important and useful, since one obtains two theorems for the price of one. Some more details and examples can be found in the article on [[duality (order theory)|duality in order theory]].\n\n=== Constructing new orders ===\nThere are many ways to construct orders out of given orders. The dual order is one example. Another important construction is the [[cartesian product]] of two partially ordered sets, taken together with the [[product order]] on pairs of elements. The ordering is defined by (''a'', ''x'') ≤ (''b'', ''y'') if (and only if) ''a'' ≤ ''b'' and ''x'' ≤ ''y''. (Notice carefully that there are three distinct meanings for the relation symbol ≤ in this definition.) The [[disjoint union]] of two posets is another typical example of order construction, where the order is just the (disjoint) union of the original orders.\n\nEvery partial order ≤ gives rise to a so-called [[strict order]] <, by defining ''a'' < ''b'' if ''a'' ≤ ''b'' and not ''b'' ≤ ''a''. This transformation can be inverted by setting ''a'' ≤ ''b'' if ''a'' < ''b'' or ''a'' = ''b''. The two concepts are equivalent although in some circumstances one can be more convenient to work with than the other.\n\n== Functions between orders ==\nIt is reasonable to consider functions between partially ordered sets having certain additional properties that are related to the ordering relations of the two sets. The most fundamental condition that occurs in this context is [[monotonic function|monotonicity]]. A function ''f'' from a poset ''P'' to a poset ''Q'' is '''monotone''', or '''order-preserving''', if ''a'' ≤ ''b'' in ''P'' implies ''f''(''a'') ≤ ''f''(''b'') in ''Q'' (Noting that, strictly, the two relations here are different since they apply to different sets.). The converse of this implication leads to functions that are '''order-reflecting''', i.e. functions ''f'' as above for which ''f''(''a'') ≤ ''f''(''b'') implies ''a'' ≤ ''b''. On the other hand, a function may also be '''order-reversing''' or '''antitone''', if ''a'' ≤ ''b'' implies ''f''(''a'') ≥ ''f''(''b'').\n\nAn '''[[order-embedding]]''' is a function ''f'' between orders that is both order-preserving and order-reflecting. Examples for these definitions are found easily. For instance, the function that maps a natural number to its successor is clearly monotone with respect to the natural order. Any function from a discrete order, i.e. from a set ordered by the identity order \"=\", is also monotone. Mapping each natural number to the corresponding real number gives an example for an order embedding. The [[complement (set theory)|set complement]] on a [[powerset]] is an example of an antitone function.\n\nAn important question is when two orders are \"essentially equal\", i.e. when they are the same up to renaming of elements. '''[[Order isomorphism]]s''' are functions that define such a renaming. An order-isomorphism is a monotone [[bijective]] function that has a monotone inverse. This is equivalent to being a [[surjective]] order-embedding. Hence, the image ''f''(''P'') of an order-embedding is always isomorphic to ''P'', which justifies the term \"embedding\".\n\nA more elaborate type of functions is given by so-called '''[[Galois connection]]s'''. Monotone Galois connections can be viewed as a generalization of order-isomorphisms, since they constitute of a pair of two functions in converse directions, which are \"not quite\" inverse to each other, but that still have close relationships.\n\nAnother special type of self-maps on a poset are '''[[closure operator#Closure operators on partially ordered sets|closure operator]]s''', which are not only monotonic, but also [[idempotent]], i.e. ''f''(''x'') = ''f''(''f''(''x'')), and '''[[Closure operator|extensive]]''' (or ''inflationary''), i.e. ''x'' ≤ ''f''(''x''). These have many applications in all kinds of \"closures\" that appear in mathematics.\n\nBesides being compatible with the mere order relations, functions between posets may also behave well with respect to special elements and constructions. For example, when talking about posets with least element, it may seem reasonable to consider only monotonic functions that preserve this element, i.e. which map least elements to least elements. If binary infima ∧ exist, then a reasonable property might be to require that ''f''(''x'' ∧ ''y'') = ''f''(''x'') ∧ ''f''(''y''), for all ''x'' and ''y''. All of these properties, and indeed many more, may be compiled under the label of [[limit-preserving function (order theory)|limit-preserving functions]].\n\nFinally, one can invert the view, switching from ''functions of orders'' to ''orders of functions''. Indeed, the functions between two posets ''P'' and ''Q'' can be ordered via the [[pointwise order]]. For two functions ''f'' and ''g'', we have ''f'' ≤ ''g'' if ''f''(''x'') ≤ ''g''(''x'') for all elements ''x'' of ''P''. This occurs for example in [[domain theory]], where [[function space]]s play an important role.\n\n== Special types of orders ==\nMany of the structures that are studied in order theory employ order relations with further properties. In fact, even some relations that are not partial orders are of special interest. Mainly the concept of a [[preorder]] has to be mentioned. A preorder is a relation that is reflexive and transitive, but not necessarily antisymmetric. Each preorder induces an [[equivalence relation]] between elements, where ''a'' is equivalent to ''b'', if ''a'' ≤ ''b'' and ''b'' ≤ ''a''. Preorders can be turned into orders by identifying all elements that are equivalent with respect to this relation.\n\nSeveral types of orders can be defined from numerical data on the items of the order: a [[total order]] results from attaching distinct real numbers to each item and using the numerical comparisons to order the items; instead, if distinct items are allowed to have equal numerical scores, one obtains a [[strict weak ordering]]. Requiring two scores to be separated by a fixed threshold before they may be compared leads to the concept of a [[semiorder]], while allowing the threshold to vary on a per-item basis produces an [[interval order]].\n\nAn additional simple but useful property leads to so-called '''[[Well-founded relation|well-founded]]''', for which all non-empty subsets have a minimal element.  Generalizing well-orders from linear to partial orders, a set is '''[[well partial order|well partially ordered]]''' if all its non-empty subsets have a finite number of minimal elements.\n\nMany other types of orders arise when the existence of [[infimum|infima]] and [[supremum|suprema]] of certain sets is guaranteed. Focusing on this aspect, usually referred to as [[completeness (order theory)|completeness]] of orders, one obtains:\n\n* [[Bounded poset]]s, i.e. posets with a [[least element|least]] and [[greatest element]] (which are just the supremum and infimum of the [[empty subset]]),\n* [[lattice (order)|Lattices]], in which every non-empty finite set has a supremum and infimum,\n* [[Complete lattice]]s, where every set has a supremum and infimum, and\n* [[Directed complete partial order]]s (dcpos), that guarantee the existence of suprema of all [[directed set|directed subsets]] and that are studied in [[domain theory]].\n* [[Partial orders with complements]], or ''poc sets'',<ref>{{citation |first=Martin A. |last=Roller |title=Poc sets, median algebras and group actions. An extended study of Dunwoody's construction and Sageev's theorem |date=1998 |publisher=Southampton Preprint Archive |url=http://www.personal.soton.ac.uk/gan/Roller.pdf |format=PDF}}</ref> are posets ''S'' having a unique bottom element ''0∈S'', along with an order-reversing involution, such that <math>a\\leq a^{*} \\Rightarrow a=0</math>.\n\nHowever, one can go even further: if all finite non-empty infima exist, then ∧ can be viewed as a total binary operation in the sense of [[universal algebra]]. Hence, in a lattice, two operations ∧ and ∨ are available, and one can define new properties by giving identities, such as\n\n: ''x''&nbsp;∧&nbsp;(''y''&nbsp;∨&nbsp;''z'') &nbsp;=&nbsp; (''x''&nbsp;∧&nbsp;''y'')&nbsp;∨&nbsp;(''x''&nbsp;∧&nbsp;''z''), for all ''x'', ''y'', and ''z''.\n\nThis condition is called '''distributivity''' and gives rise to [[distributive lattice]]s. There are some other important distributivity laws which are discussed in the article on [[distributivity (order theory)|distributivity in order theory]]. Some additional order structures that are often specified via algebraic operations and defining identities are\n\n* [[Heyting algebra]]s and\n* [[Boolean algebra (structure)|Boolean algebra]]s,\n\nwhich both introduce a new operation ~ called '''negation'''. Both structures play a role in [[mathematical logic]] and especially Boolean algebras have major applications in [[computer science]].\nFinally, various structures in mathematics combine orders with even more algebraic operations, as in the case of [[quantale]]s, that allow for the definition of an addition operation.\n\nMany other important properties of posets exist. For example, a poset is '''locally finite''' if every closed [[interval (mathematics)|interval]] [''a'', ''b''] in it is [[finite set|finite]]. Locally finite posets give rise to [[incidence algebra]]s which in turn can be used to define the [[Euler characteristic#Generalizations|Euler characteristic]] of finite bounded posets.\n\n== Subsets of ordered sets ==\nIn an ordered set, one can define many types of special subsets based on the given order. A simple example are '''upper sets'''; i.e. sets that contain all elements that are above them in the order. Formally, the '''upper closure''' of a set ''S'' in a poset ''P'' is given by the set {''x'' in ''P'' | there is some ''y'' in ''S'' with ''y'' ≤ ''x''}. A set that is equal to its upper closure is called an upper set. '''Lower sets''' are defined dually.\n\nMore complicated lower subsets are [[ideal (order theory)|ideals]], which have the additional property that each two of their elements have an upper bound within the ideal. Their duals are given by [[filter (mathematics)|filters]]. A related concept is that of a [[directed set|directed subset]], which like an ideal contains upper bounds of finite subsets, but does not have to be a lower set. Furthermore, it is often generalized to preordered sets.\n\nA subset which is - as a sub-poset - linearly ordered, is called a [[total order#Chains|chain]]. The opposite notion, the [[antichain]], is a subset that contains no two comparable elements; i.e. that is a discrete order.\n\n== Related mathematical areas ==\nAlthough most mathematical areas ''use'' orders in one or the other way, there are also a few theories that have relationships which go far beyond mere application. Together with their major points of contact with order theory, some of these are to be presented below.\n\n=== Universal algebra ===\nAs already mentioned, the methods and formalisms of [[universal algebra]] are an important tool for many order theoretic considerations. Beside formalizing orders in terms of algebraic structures that satisfy certain identities, one can also establish other connections to algebra. An example is given by the correspondence between [[Boolean algebra (structure)|Boolean algebra]]s and [[Boolean ring]]s. Other issues are concerned with the existence of free constructions, such as ''free lattices'' based on a given set of generators. Furthermore, closure operators are important in the study of universal algebra.\n\n=== Topology ===\nIn [[topology]], orders play a very prominent role. In fact, the set of [[open set]]s provides a classical example of a complete lattice, more precisely a complete [[Heyting algebra]] (or \"'''frame'''\" or \"'''locale'''\"). [[filter (mathematics)|Filters]] and [[net (mathematics)|nets]] are notions closely related to order theory and the [[closed set|closure operator of sets]] can be used to define topology. Beyond these relations, topology can be looked at solely in terms of the open set lattices, which leads to the study of [[pointless topology]]. Furthermore, a natural preorder of elements of the underlying set of a topology is given by the so-called [[specialization order]], that is actually a partial order if the topology is [[T0 space|T<sub>0</sub>]].\n\nConversely, in order theory, one often makes use of topological results. There are various ways to define subsets of an order which can be considered as open sets of a topology. Considering topologies on a poset (''X'', ≤) that in turn induce ≤ as their specialization order, the ''finest'' such topology is the [[Alexandrov topology]], given by taking all upper sets as opens. Conversely, the ''coarsest'' topology that induces the specialization order is the [[upper topology]], having the complements of [[ideal (order theory)|principal ideals]] (i.e. sets of the form {''y'' in ''X'' | ''y'' ≤ ''x''} for some ''x'') as a [[subbase]]. Additionally, a topology with specialization order ≤ may be [[order consistent]], meaning that their open sets are \"inaccessible by directed suprema\" (with respect to ≤). The finest order consistent topology is the [[Scott topology]], which is coarser than the Alexandrov topology. A third important topology in this spirit is the [[Lawson topology]]. There are close connections between these topologies and the concepts of order theory. For example, a function preserves directed suprema [[iff]] it is [[continuous function (topology)|continuous]] with respect to the Scott topology (for this reason this order theoretic property is also called [[Scott-continuous|Scott-continuity]]).\n\n=== Category theory ===\nThe visualization of orders with [[Hasse diagram]]s has a straightforward generalization: instead of displaying lesser elements ''below'' greater ones, the direction of the order can also be depicted by giving directions to the edges of a graph. In this way, each order is seen to be equivalent to a [[directed acyclic graph]], where the nodes are the elements of the poset and there is a directed path from ''a'' to ''b'' if and only if ''a'' ≤ ''b''. Dropping the requirement of being acyclic, one can also obtain all preorders.\n\nWhen equipped with all transitive edges, these graphs in turn are just special [[category theory|categories]], where elements are objects and each set of morphisms between two elements is at most singleton. Functions between orders become functors between categories. Many ideas of order theory are just concepts of category theory in small. For example, an infimum is just a [[product (category theory)|categorical product]]. More generally, one can capture infima and suprema under the abstract notion of a [[limit (category theory)|categorical limit]] (or ''colimit'', respectively). Another place where categorical ideas occur is the concept of a (monotone) [[Galois connection]], which is just the same as a pair of [[adjoint functor]]s.\n\nBut category theory also has its impact on order theory on a larger scale. Classes of posets with appropriate functions as discussed above form interesting categories. Often one can also state constructions of orders, like the [[product order]], in terms of categories. Further insights result when categories of orders are found [[equivalence of categories|categorically equivalent]] to other categories, for example of topological spaces. This line of research leads to various ''representation theorems'', often collected under the label of [[Stone duality]].\n\n==History==\nAs explained before, orders are ubiquitous in mathematics. However, earliest explicit mentionings of partial orders are probably to be found not before the 19th century. In this context the works of [[George Boole]] are of great importance. Moreover, works of [[Charles Sanders Peirce]], [[Richard Dedekind]], and [[Ernst Schröder]] also consider concepts of order theory. Certainly, there are others to be named in this context and surely there exists more detailed material on the history of order theory. <!-- ''Please contribute if any further knowledge is available to you.'' -->\n\nThe term ''poset'' as an abbreviation for partially ordered set was coined by [[Garrett Birkhoff]] in the second edition of his influential book ''Lattice Theory''.<ref>Birkhoff 1948, p.1</ref><ref>[http://jeff560.tripod.com/p.html Earliest Known Uses of Some of the Words of Mathematics]</ref>\n\n==See also==\n* [[Cyclic order]]\n* [[Hierarchy]]\n* [[Incidence algebra]]\n* [[Causal Sets]]\n\n== Notes ==\n{{reflist}}\n\n==References==\n\n* {{cite book|first=Garrett|last=Birkhoff|author-link=Garrett Birkhoff|title=Lattice Theory|volume=25|publisher=American Mathematical Society|edition=3rd Revised|year=1940|isbn=978-0-8218-1025-5}}\n* {{cite book|first1=S. N.|last1=Burris|first2=H. P.|last2=Sankappanavar|year=1981|url=http://www.thoralf.uwaterloo.ca/htdocs/ualg.html|title=A Course in Universal Algebra|publisher= Springer|isbn=978-0-387-90578-5}}\n* {{cite book|first1=B. A.|last1=Davey|first2=H. A.|last2=Priestley|year=2002|author2-link=Hilary Priestley| title = Introduction to Lattices and Order|edition=2nd|publisher=Cambridge University Press|isbn=0-521-78451-4}}\n* {{cite book|first1=G.|last1=Gierz|first2=K. H.|last2=Hofmann|first3=K.|last3=Keimel|first4=M.|last4=Mislove|first5=D. S.|last5=Scott|year = 2003|title=Continuous Lattices and Domains|publisher=Cambridge University Press|isbn=978-0-521-80338-0|series=Encyclopedia of Mathematics and its Applications|volume=93}}\n\n== External links ==\n{{Wiktionary|ordering}}\n* [http://www.apronus.com/provenmath/orders.htm Orders at ProvenMath] partial order, linear order, well order, initial segment; formal definitions and proofs within the axioms of set theory.\n* Nagel, Felix (2013). [http://www.felixnagel.org Set Theory and Topology. An Introduction to the Foundations of Analysis]\n\n{{Authority control}}\n{{Areas of mathematics | state=collapsed}}\n\n[[Category:Order theory|*]]"
    },
    {
      "title": "1/3–2/3 conjecture",
      "url": "https://en.wikipedia.org/wiki/1%2F3%E2%80%932%2F3_conjecture",
      "text": "[[File:Aigner poset.svg|thumb|A partial order formed by the [[Series-parallel partial order|series composition]] of one-element and three-element partial orders. Among its 27 linear extensions, the bottom left element occurs prior to the bottom right element in 9 out of 27. Partial orders with this structure are the only known extreme cases for the 1/3–2/3 conjecture.]]\nIn [[order theory]], a branch of mathematics, the '''1/3–2/3 conjecture''' states that, if one is [[comparison sort]]ing a set of items then, no matter what comparisons may have already been performed, it is always possible to choose the next comparison in such a way that it will reduce the number of possible sorted orders by a factor of 2/3 or better. Equivalently, in every finite [[partially ordered set]] that is not [[total order|totally ordered]], there exists a pair of elements ''x'' and ''y'' with the property that at least 1/3 and at most 2/3 of the [[linear extension]]s of the partial order place ''x'' earlier than ''y''.\n\n==Example==\nThe partial order formed by three elements ''a'', ''b'', and ''c'' with a single comparability relationship, {{nowrap|''a'' ≤ ''b'',}} has three linear extensions, {{nowrap|''a'' ≤ ''b'' ≤ ''c'',}} {{nowrap|''a'' ≤ ''c'' ≤ ''b'',}} and {{nowrap|''c'' ≤ ''a'' ≤ ''b''.}} In all three of these extensions, ''a'' is earlier than ''b''. However, ''a'' is earlier than ''c'' in only two of them, and later than ''c'' in the third. Therefore, the pair of ''a'' and ''c'' have the desired property, showing that this partial order obeys the 1/3–2/3 conjecture.\n\nThis example shows that the constants 1/3 and 2/3 in the conjecture are tight; if ''q'' is any fraction strictly between 1/3 and 2/3, then there would not exist a pair ''x'', ''y'' in which ''x'' is earlier than ''y'' in a number of partial orderings that is between ''q'' and {{nowrap|1 &minus; ''q''}} times the total number of partial orderings.<ref>{{harvtxt|Kahn|Saks|1984}}; {{harvtxt|Brightwell|Felsner|Trotter|1995}}.</ref>\n\nMore generally, let ''P'' be any [[Series-parallel partial order|series composition]] of three-element partial orders and of one-element partial orders, such as the one in the figure. Then ''P'' forms an extreme case for the 1/3–2/3 conjecture in the sense that, for each pair ''x'', ''y'' of elements, one of the two elements occurs earlier than the other in at most 1/3 of the linear extensions of ''P''. Partial orders with this structure are necessarily [[Series-parallel partial order|series-parallel]] [[semiorder]]s; they are the only known extreme cases for the conjecture and can be proven to be the only extreme cases with width two.<ref>{{harvtxt|Aigner|1985}}.</ref>\n\n==Definitions==\nA partially ordered set is a set ''X'' together with a [[binary relation]] ≤ that is [[Reflexive relation|reflexive]], [[Antisymmetric relation|antisymmetric]], and [[Transitive relation|transitive]]. A total order is a partial order in which every pair of elements is comparable. A linear extension of a finite partial order is a sequential ordering of the elements of ''X'', with the property that if ''x'' ≤ ''y'' in the partial order, then ''x'' must come before ''y'' in the linear extension. In other words, it is a total order compatible with the partial order. If a finite partially ordered set is totally ordered, then it has only one linear extension, but otherwise it will have more than one. The 1/3–2/3 conjecture states that one can choose two elements ''x'' and ''y'' such that, among this set of possible linear extensions, between 1/3 and 2/3 of them place ''x'' earlier than ''y'', and symmetrically between 1/3 and 2/3 of them place ''y'' earlier {{nowrap|than ''x''.<ref name=\"bft95\"/>}}\n\nThere is an alternative and equivalent statement of the 1/3–2/3 conjecture in the language of [[probability theory]].\nOne may define a [[Uniform distribution (discrete)|uniform probability distribution]] on the linear extensions in which each possible linear extension is equally likely to be chosen. The 1/3–2/3 conjecture states that, under this probability distribution, there exists a pair of elements ''x'' and ''y'' such that the probability that ''x'' is earlier than ''y'' in a random linear extension is between 1/3 and 2/3.<ref name=\"bft95\"/>\n\n{{harvtxt|Kahn|Saks|1984}} define δ(''P''), for any partially ordered set ''P'', to be the largest real number δ such that ''P'' has a pair ''x'', ''y'' with ''x'' earlier than ''y'' in a number of linear extensions that is between δ and {{nowrap|1 &minus; δ}} of the total number of linear extensions. In this notation, the 1/3–2/3 conjecture states that every finite partial order that is not total has {{nowrap|δ(''P'') ≥ 1/3}}.\n\n==History==\nThe 1/3–2/3 conjecture was formulated by {{harvtxt|Kislitsyn|1968}}, and later made independently by [[Michael Fredman]]<ref>However, despite the close connection of {{harvtxt|Fredman|1976}} to the problem of sorting partially ordered data and hence to the 1/3–2/3 conjecture, it is not mentioned in that paper.</ref> and by {{harvs|first=Nati|last=Linial|authorlink=Nati Linial|year=1984|txt}}.<ref name=\"bft95\">{{harvtxt|Brightwell|Felsner|Trotter|1995}}.</ref> It was listed as a featured unsolved problem at the founding of the journal ''[[Order (journal)|Order]]'', and remains unsolved;<ref name=\"bft95\"/> {{harvtxt|Brightwell|Felsner|Trotter|1995}} call it \"one of the most intriguing problems in the combinatorial theory of posets.\"\n\nA survey of the conjecture is given by {{harvtxt|Brightwell|1999}}.\n\n==Partial results==\nThe 1/3–2/3 conjecture is known to be true for certain special classes of partial orders, including partial orders of [[Antichain|width]] two,<ref>{{harvtxt|Linial|1984}}, Theorem 2.</ref> partial orders of height two,<ref name=\"tgf92\">{{harvtxt|Trotter|Gehrlein|Fishburn|1992}}.</ref> partial orders with at most 11 elements,<ref name=\"p06\">{{harvtxt|Peczarski|2006}}.</ref> partial orders in which each element is incomparable to at most six others,<ref name=\"p08\">{{harvtxt|Peczarski|2008}}.</ref> [[series-parallel partial order]]s,<ref>{{harvtxt|Zaguia|2012}}</ref> and [[semiorder]]s.<ref>{{harvtxt|Brightwell|1989}}.</ref> In the limit as ''n'' goes to infinity, the proportion of ''n''-element partial orders that obey the 1/3–2/3 conjecture approaches 100%.<ref name=\"p06\"/>\n\n{{harvtxt|Brightwell|Felsner|Trotter|1995}} proved that, for any finite partial order ''P'' that is not total, {{nowrap|δ(''P'') ≥ 1/2 &minus; {{radic|5}}/10 ≈ 0.276.}} Their results improve previous weaker bounds of the same type.<ref>{{harvtxt|Kahn|Saks|1984}}; {{harvtxt|Khachiyan|1989}}; {{harvtxt|Kahn|Linial|1991}}; {{harvtxt|Felsner|Trotter|1993}}.</ref> They use the probabilistic interpretation of δ(''P'') to extend its definition to certain infinite partial orders; in that context, they show that their bounds are optimal, in that there exist infinite partial orders with {{nowrap|1=δ(''P'') = 1/2 &minus; {{radic|5}}/10.}}\n\n==Applications==\n{{harvtxt|Kahn|Saks|1984}} proposed the following application for the problem:\nsuppose one wishes to [[comparison sort]] a totally ordered set ''X'', for which some partial order information is already known in the form of a partial order on ''X''. In the worst case, each additional comparison between a pair ''x'' and ''y'' of elements may yield as little information as possible, by resolving the comparison in a way that leaves as many linear extensions as possible compatible with the comparison result. The 1/3–2/3 conjecture states that, at each step, one may choose a comparison to perform that reduces the remaining number of linear extensions by a factor of 2/3; therefore, if there are ''E'' linear extensions of the partial order given by the initial information, the sorting problem can be completed in at most log<sub>3/2</sub>''E'' additional comparisons.\n\nHowever, this analysis neglects the complexity of selecting the optimal pair ''x'' and ''y'' to compare. Additionally, it may be possible to sort a partial order using a number of comparisons that is better than this analysis would suggest, because it may not be possible for this worst-case behavior to occur at each step of a sorting algorithm. In this direction, it has been conjectured that log<sub>φ</sub>''E'' comparisons may suffice, where φ denotes the [[golden ratio]].<ref name=\"p06\"/>\n\nA closely related class of comparison sorting problems is considered by {{harvtxt|Fredman|1976}}, among them the problem of comparison sorting a set ''X'' when the sorted order of ''X'' is known to lie in some set ''S'' of permutations of ''X''. Here ''S'' is not necessarily generated as the set of linear extensions of a partial order. Despite this added generality, Fredman shows that ''X'' can be sorted using log<sub>2</sub>|''S''|&nbsp;+&nbsp;O(|''X''|) comparisons. This same bound applies as well to the case of partial orders and shows that log<sub>2</sub>''E''&nbsp;+&nbsp;O(''n'') comparisons suffice.\n\n==Generalizations and related results==\n{{harvtxt|Kahn|Saks|1984}} conjectured that, in the limit as ''w'' tends to infinity, the value of δ(''P'') for partially ordered sets of width ''w'' should tend to 1/2. In particular, they expect that only partially ordered sets of width two can achieve the worst case value {{nowrap|1=δ(''P'') = 1/3,}} and {{harvtxt|Aigner|1985}} stated this explicitly as a conjecture. The smallest known value of δ(''P'') for posets of width three is 14/39,<ref>{{harvtxt|Saks|1985}}.</ref> and computer searches have shown that no smaller value is possible for width-3 posets with nine or fewer elements.<ref name=\"tgf92\"/>\n\nMarcin Peczarski<ref name=\"p06\"/><ref name=\"p08\"/> has formulated a \"gold partition conjecture\" stating that in each partial order that is not a total order one can find two consecutive comparisons such that, if ''t''<sub>''i''</sub> denotes the number of linear extensions remaining after ''i'' of the comparisons have been made, then (in each of the four possible outcomes of the comparisons) {{nowrap|''t''<sub>0</sub> ≥ ''t''<sub>1</sub> + ''t''<sub>2</sub>.}} If this conjecture is true, it would imply the 1/3–2/3 conjecture: the first of the two comparisons must be between a pair that splits the remaining comparisons by at worst a 1/3–2/3 ratio. The gold partition conjecture would also imply that a partial order with ''E'' linear extensions can be sorted in at most log<sub>φ</sub>''E'' comparisons; the name of the conjecture is derived from this connection with the golden ratio.\n\nIt is [[Sharp-P-complete|#P-complete]], given a finite partial order  ''P'' and a pair of elements ''x'' and ''y'', to calculate the proportion of the linear extensions of ''P'' that place ''x'' earlier than ''y''.<ref>{{harvtxt|Brightwell|Winkler|1991}}.</ref>\n\n==Notes==\n{{reflist|35em}}\n\n==References==\n*{{citation\n | last = Aigner | first = Martin | authorlink = Martin Aigner\n | doi = 10.1007/BF00333131\n | issue = 3\n | journal = Order\n | pages = 257–264\n | title = A note on merging\n | volume = 2\n | year = 1985| doi-broken-date = 2019-03-06 }}.\n*{{citation\n | last = Brightwell | first = Graham R.\n | doi = 10.1007/BF00353656\n | issue = 4\n | journal = [[Order (journal)|Order]]\n | pages = 369–380\n | title = Semiorders and the 1/3–2/3 conjecture\n | volume = 5\n | year = 1989}}.\n*{{citation\n | last = Brightwell | first = Graham R.\n | doi = 10.1016/S0012-365X(98)00311-2\n | issue = 1–3\n | journal = [[Discrete Mathematics (journal)|Discrete Mathematics]]\n | pages = 25–52\n | title = Balanced pairs in partial orders\n | volume = 201\n | year = 1999}}.\n*{{citation\n | last1 = Brightwell | first1 = Graham R.\n | last2 = Felsner | first2 = Stefan\n | last3 = Trotter | first3 = William T.\n | doi = 10.1007/BF01110378\n | mr = 1368815\n | issue = 4\n | journal = [[Order (journal)|Order]]\n | pages = 327–349\n | title = Balancing pairs and the cross product conjecture\n | volume = 12\n | year = 1995| citeseerx = 10.1.1.38.7841\n }}.\n*{{citation\n | last1 = Brightwell | first1 = Graham R.\n | last2 = Winkler | first2 = Peter | author2-link = Peter Winkler\n | doi = 10.1007/BF00383444\n | issue = 3\n | journal = [[Order (journal)|Order]]\n | pages = 225–242\n | title = Counting linear extensions\n | volume = 8\n | year = 1991}}.\n*{{citation\n | last1 = Felsner | first1 = Stefan\n | last2 = Trotter | first2 = William T.\n | contribution = Balancing pairs in partially ordered sets\n | mr = 1249709\n | location = Budapest\n | pages = 145–157\n | publisher = [[János Bolyai Mathematical Society]]\n | series = Bolyai Society Mathematical Studies\n | title = Combinatorics, Paul Erdős is eighty\n | volume = 1\n | year = 1993}}.\n*{{citation\n | last = Fredman | first = M. L. | authorlink = Michael Fredman\n | doi = 10.1016/0304-3975(76)90078-5\n | issue = 4\n | journal = Theoretical Computer Science\n | pages = 355–361\n | title = How good is the information theory bound in sorting?\n | volume = 1\n | year = 1976}}\n*{{citation\n | last1 = Kahn | first1 = Jeff\n | last2 = Linial | first2 = Nati | author2-link = Nati Linial\n | doi = 10.1007/BF01275670\n | issue = 4\n | journal = [[Combinatorica]]\n | pages = 363–368\n | title = Balancing extensions via Brunn-Minkowski\n | volume = 11\n | year = 1991}}.\n*{{citation\n | last1 = Kahn | first1 = Jeff\n | last2 = Saks | first2 = Michael | author2-link = Michael Saks (mathematician)\n | doi = 10.1007/BF00565647\n | issue = 2\n | journal = [[Order (journal)|Order]]\n | pages = 113–126\n | title = Balancing poset extensions\n | volume = 1\n | year = 1984}}.\n*{{citation\n | last = Khachiyan | first = Leonid | authorlink = Leonid Khachiyan\n | contribution = Optimal algorithms in convex programming decomposition and sorting\n | editor-last = Jaravlev | editor-first = J.\n | language = Russian\n | location = Moscow\n | pages = 161–205\n | publisher = Nauka\n | title = Computers and Decision Problems\n | year = 1989}}. As cited by {{harvtxt|Brightwell|Felsner|Trotter|1995}}.\n*{{citation\n | last = Kislitsyn | first = S. S.\n | doi = 10.1007/BF01111312\n | issue = 5\n | journal = [[Mathematical Notes]]\n | pages = 798–801\n | title = A finite partially ordered set and its corresponding set of permutations\n | volume = 4\n | year = 1968}}. \n*{{citation\n | last = Linial | first = Nati | authorlink = Nati Linial\n | doi = 10.1137/0213049\n | issue = 4\n | journal = [[SIAM Journal on Computing]]\n | pages = 795–801\n | title = The information-theoretic bound is good for merging\n | volume = 13\n | year = 1984}}.\n*{{citation\n | last = Peczarski | first = Marcin\n | doi = 10.1007/s11083-006-9033-1\n | issue = 1\n | journal = [[Order (journal)|Order]]\n | pages = 89–95\n | title = The gold partition conjecture\n | volume = 23\n | year = 2006}}.\n*{{citation\n | last = Peczarski | first = Marcin\n | doi = 10.1007/s11083-008-9081-9\n | issue = 2\n | journal = [[Order (journal)|Order]]\n | pages = 91–103\n | title = The gold partition conjecture for 6-thin posets\n | volume = 25\n | year = 2008}}.\n*{{citation\n | last = Saks | first = Michael | authorlink = Michael Saks (mathematician)\n | title = Balancing linear extensions of ordered sets | department = Unsolved problems\n | doi = 10.1007/BF00333138| journal = [[Order (journal)|Order]]\n | pages = 327–330\n | volume = 2\n | year = 1985| doi-broken-date = 2019-03-06 }}\n*{{citation\n | last1 = Trotter | first1 = William T.\n | last2 = Gehrlein | first2 = William V.\n | last3 = Fishburn | first3 = Peter C. | author3-link = Peter C. Fishburn\n | doi = 10.1007/BF00419038\n | issue = 1\n | journal = [[Order (journal)|Order]]\n | pages = 43–53\n | title = Balance theorems for height-2 posets\n | volume = 9\n | year = 1992}}.\n*{{citation\n | last1 = Zaguia| first1 = Imed\n | issue = 2\n | journal = [[Electronic Journal of Combinatorics]]\n | pages = P29\n | title = The 1/3-2/3 Conjecture for ''N''-free ordered sets\n | url = http://www.combinatorics.org/ojs/index.php/eljc/article/view/v19i2p29\n | volume = 19\n | year = 2012}}.\n\n{{DEFAULTSORT:1 3-2 3 conjecture}}\n[[Category:Order theory]]\n[[Category:Conjectures]]"
    },
    {
      "title": "Alexandrov topology",
      "url": "https://en.wikipedia.org/wiki/Alexandrov_topology",
      "text": "In [[topology]], an '''Alexandrov topology''' is a [[topological space|topology]] in which the [[intersection (set theory)|intersection]] of any family of [[open set]]s is open. It is an axiom of topology that the intersection of any ''finite'' family of open sets is open; in Alexandrov topologies the finite restriction is dropped.\n\nA set together with an Alexandrov topology is known as an '''Alexandrov-discrete space''' or '''finitely generated space'''.\n\nAlexandrov topologies are uniquely determined by their [[specialization preorder]]s. Indeed, given any [[preorder]] ≤ on a [[Set (mathematics)|set]] ''X'', there is a unique Alexandrov topology on ''X'' for which the specialization preorder is ≤. The open sets are just the [[upper set]]s with respect to ≤. Thus, Alexandrov topologies on ''X'' are in [[one-to-one correspondence]] with preorders on ''X''.\n\nAlexandrov-discrete spaces are also called '''finitely generated spaces''' since their topology is uniquely [[coherent topology|determined by]] the family of all finite subspaces. Alexandrov-discrete spaces can thus be viewed as a generalization of [[finite topological space]]s.\n\nDue to the fact that [[Image_(mathematics)|inverse images]] commute with arbitrary unions and intersections, the property of being an Alexandrov-discrete space is preserved under [[Quotient_space_(topology)|quotients]].\n\nAlexandrov-discrete spaces are named after the Russian topologist [[P. S. Alexandrov|Pavel Alexandrov]]. They should not be confused with the more geometrical [[Alexandrov space|Alexandrov spaces]] introduced by the Russian mathematician [[Aleksandr Danilovich Aleksandrov]].\n\n== Characterizations of Alexandrov topologies ==\n\nAlexandrov topologies have numerous characterizations. Let '''''X''''' = <''X'', ''T''> be a topological space. Then the following are equivalent:\n \n*'''Open and closed set characterizations:'''\n** '''Open set.''' An arbitrary intersection of open sets in '''''X''''' is open.\n** '''Closed set.''' An arbitrary union of closed sets in '''''X''''' is closed.\n*'''Neighbourhood characterizations:'''\n** '''Smallest neighbourhood.''' Every point of '''''X''''' has a smallest [[neighbourhood (topology)|neighbourhood]].\n** '''Neighbourhood filter.''' The [[neighbourhood filter]] of every point in '''''X''''' is closed under arbitrary intersections.\n*'''Interior and closure algebraic characterizations:'''\n** '''Interior operator.''' The [[interior operator]] of '''''X''''' distributes over arbitrary intersections of subsets.\n** '''Closure operator.''' The [[closure operator]] of '''''X''''' distributes over arbitrary unions of subsets.\n*'''Preorder characterizations:'''\n** '''Specialization preorder.''' ''T'' is the [[finest topology]] consistent with the [[specialization preorder]] of '''''X''''' i.e. the finest topology giving the [[preorder]] ≤ satisfying ''x'' ≤ ''y'' if and only if ''x'' is in the closure of {''y''} in '''''X'''''.\n** '''Open up-set.'''  There is a preorder ≤ such that the open sets of '''''X''''' are precisely those that are [[upper set|upwardly closed]] i.e. if ''x'' is in the set and ''x'' ≤ ''y'' then ''y'' is in the set.  (This preorder will be precisely the specialization preorder.)\n** '''Closed down-set.'''  There is a preorder ≤ such that the closed sets of '''''X''''' are precisely those that are downwardly closed i.e. if ''x'' is in the set and ''y'' ≤ ''x'' then ''y'' is in the set.  (This preorder will be precisely the specialization preorder.)\n** '''Upward interior.''' A point ''x'' lies in the interior of a subset ''S'' of '''''X''''' if and only if there is a point ''y'' in ''S'' such that ''y'' ≤ ''x'' where ≤ is the specialization preorder i.e. ''y'' lies in the closure of {''x''}.\n** '''Downward closure.''' A point ''x'' lies in the closure of a subset ''S'' of '''''X''''' if and only if there is a point ''y'' in ''S'' such that ''x'' ≤ ''y'' where ≤ is the specialization preorder i.e. ''x'' lies in the closure of {''y''}.\n*'''Finite generation and category theoretic characterizations:'''\n** '''Finite closure.''' A point ''x'' lies within the closure of a subset ''S'' of '''''X''''' if and only if there is a finite subset ''F'' of ''S'' such that ''x'' lies in the closure of ''F''. (This finite subset can always be chosen to be a singleton.)\n** '''Finite subspace.''' ''T'' is [[coherent topology|coherent]] with the finite subspaces of '''''X'''''.\n** '''Finite inclusion map.''' The inclusion maps ''f''<sub>''i''</sub> : '''''X'''''<sub>''i''</sub> → '''''X''''' of the finite subspaces of '''''X''''' form a [[final sink]].\n** '''Finite generation.''' '''''X''''' is finitely generated i.e. it is in the [[final hull]] of the finite spaces. (This means that there is a final sink ''f''<sub>''i''</sub> : '''''X'''''<sub>''i''</sub> → '''''X''''' where each '''''X'''''<sub>''i''</sub> is a finite topological space.)\n\nTopological spaces satisfying the above equivalent characterizations are called '''finitely generated spaces''' or '''Alexandrov-discrete spaces''' and their topology ''T'' is called an '''Alexandrov topology'''.\n\n== Duality with preordered sets ==\n\n=== The Alexandrov topology on a preordered set ===\n\nGiven a [[preordered set]] <math> \\mathbf{X} = \\langle X, \\le\\rangle</math> we can define an Alexandrov topology <math>\\tau</math> on ''X'' by choosing the open sets to be the [[upper set]]s:\n\n:<math>\\tau = \\{\\, G \\subseteq X :  \\forall x,y\\in X\\ \\ (x\\in G\\ \\land\\ x\\le y)\\ \\rightarrow\\ y \\in G\\,\\}</math>\n\nWe thus obtain a topological space <math>\\mathbf{T}(\\mathbf{X}) = \\langle X, \\tau\\rangle</math>.\n\nThe corresponding closed sets are the [[lower set]]s:\n::<math>\\{\\, S \\subseteq X :  \\forall x,y\\in X\\ \\ (x\\in S\\ \\land\\ y\\le x)\\ \\rightarrow\\ y \\in S\\,\\}</math>\n\n=== The specialization preorder on a topological space ===\n\nGiven a topological space '''''X''''' = <''X'', ''T''> the [[specialization preorder]] on ''X'' is defined by:\n\n: ''x''≤''y'' if and only if ''x'' is in the closure of {''y''}.\n\nWe thus obtain a preordered set '''''W'''''('''''X''''') = <''X'', ≤>.\n\n=== Equivalence between preorders and Alexandrov topologies ===\n\nFor every preordered set '''''X''''' = <''X'', ≤> we always have '''''W'''''('''''T'''''('''''X''''')) = '''''X''''', i.e. the preorder of '''''X''''' is recovered from the topological space '''''T'''''('''''X''''') as the specialization preorder.\nMoreover for every ''Alexandrov-discrete space'' '''''X''''', we have '''''T'''''('''''W'''''('''''X''''')) = '''''X''''', i.e. the Alexandrov topology of '''''X''''' is recovered as the topology induced by the specialization preorder.\n\nHowever for a topological space in general we do '''not''' have '''''T'''''('''''W'''''('''''X''''')) = '''''X'''''. Rather '''''T'''''('''''W'''''('''''X''''')) will be the set ''X'' with a finer topology than that of '''''X''''' (i.e. it will have more open sets).\n\n=== Equivalence between monotonicity and continuity ===\n\nGiven a [[monotone function]]\n\n:''f''&nbsp;:&nbsp;'''''X'''''→'''''Y'''''\n\nbetween two preordered sets (i.e. a function\n\n:''f''&nbsp;:&nbsp;''X''→''Y''\n\nbetween the underlying sets such that ''x''≤''y'' in '''''X''''' implies ''f''(''x'')≤''f''(''y'') in '''''Y'''''), let\n\n:'''''T'''''(''f'')&nbsp;:&nbsp;'''''T'''''('''''X''''')→'''''T'''''('''''Y''''')\n\nbe the same map as ''f'' considered as a map between the corresponding Alexandrov spaces. Then '''''T'''''(''f'') is a [[continuous map (topology)|continuous map]].\n\nConversely given a continuous map\n\n:''g'':&nbsp;'''''X'''''→'''''Y'''''\n\nbetween two topological spaces, let\n\n:'''''W'''''(g)&nbsp;:&nbsp;'''''W'''''('''''X''''')→'''''W'''''('''''Y''''')\n\nbe the same map as ''f'' considered as a map between the corresponding preordered sets. Then '''''W'''''(g) is a monotone function.\n\nThus a map between two preordered sets is monotone if and only if it is a continuous map between the corresponding Alexandrov-discrete spaces. Conversely a map between two Alexandrov-discrete spaces is continuous if and only if it is a monotone function between the corresponding preordered sets.\n\nNotice however that in the case of topologies other than the Alexandrov topology, we can have a map between two topological spaces that is not continuous but which is nevertheless still a monotone function between the corresponding preordered sets. (To see this consider a non-Alexandrov-discrete space '''''X''''' and consider the [[identity function|identity map]] ''i''&nbsp;:&nbsp;'''''X'''''→'''''T'''''('''''W'''''('''''X''''')).)\n\n===Category theoretic description of the duality===\n\nLet '''Set''' denote the [[category of sets]] and [[map (mathematics)|maps]]. Let '''Top''' denote the [[category of topological spaces]] and [[continuity (topology)|continuous maps]]; and let '''Pro''' denote the category of [[preorder|preordered sets]] and [[monotone function]]s. Then\n\n:'''''T'''''&nbsp;:&nbsp;'''Pro'''→'''Top''' and\n:'''''W'''''&nbsp;:&nbsp;'''Top'''→'''Pro'''\n\nare [[concrete functor]]s over '''Set''' which are [[adjoint functors|left and right adjoints]] respectively.\n\nLet '''Alx''' denote the [[full subcategory]] of '''Top''' consisting of the Alexandrov-discrete spaces. Then the restrictions\n\n:'''''T'''''&nbsp;:&nbsp;'''Pro'''→'''Alx''' and\n:'''''W'''''&nbsp;:&nbsp;'''Alx'''→'''Pro'''\n\nare inverse [[concrete functor|concrete isomorphisms]] over '''Set'''.\n\n'''Alx''' is in fact a [[coreflective subcategory|bico-reflective subcategory]] of '''Top''' with bico-reflector '''''T'''''◦'''''W'''''&nbsp;:&nbsp;'''Top'''→'''Alx'''. This means that given a topological space '''''X''''', the identity map\n\n:''i''&nbsp;:&nbsp;'''''T'''''('''''W'''''('''''X'''''))→'''''X'''''\nis continuous and for every continuous map\n\n:''f''&nbsp;:&nbsp;'''''Y'''''→'''''X'''''\n\nwhere '''''Y''''' is an Alexandrov-discrete space, the composition\n\n:''i''&nbsp;<sup>−1</sup>◦''f''&nbsp;:&nbsp;'''''Y'''''→'''''T'''''('''''W'''''('''''X'''''))\n\nis continuous.\n\n=== Relationship to the construction of modal algebras from modal frames ===\n\nGiven a preordered set '''''X''''', the [[interior operator]] and [[closure operator]] of '''''T'''''('''''X''''') are given by:\n\n:'''Int'''(''S'') = { ''x''&nbsp;∈&nbsp;X : for all ''y''&nbsp;∈&nbsp;X, ''x''≤''y'' implies ''y''&nbsp;∈&nbsp;S }, and\n:'''Cl'''(''S'') = { ''x''&nbsp;∈&nbsp;X : there exists a ''y''&nbsp;∈&nbsp;S with ''x''≤''y'' } \n\nfor all ''S''⊆&nbsp;''X.''\n\nConsidering the interior operator and closure operator to be modal operators on the [[power set]] [[Boolean algebra (structure)|Boolean algebra]] of ''X'', this construction is a special case of the construction of a [[modal algebra]] from a [[Kripke semantics|modal frame]] i.e. from a set with a single [[binary relation]]. (The latter construction is itself a special case of a more general construction of a [[complex algebra]] from a [[relational structure]] i.e. a set with relations defined on it.) The class of modal algebras that we obtain in the case of a preordered set is the class of [[interior algebra]]s&mdash;the algebraic abstractions of topological spaces.\n\n== History ==\n\nAlexandrov spaces were first introduced in 1937 by [[P. S. Alexandrov]] under the name '''discrete spaces''', where he provided the characterizations in terms of sets and neighbourhoods.<ref name=\"Ale37\">{{cite journal |last=Alexandroff |first=P. |title=Diskrete Räume |journal=Mat. Sb. (N.S.) |volume=2 |issue= |year=1937 |pages=501–518 |doi= |url=http://mi.mathnet.ru/rus/msb/v44/i3/p501 |language=German }}</ref> The name [[discrete space]]s later came to be used for topological spaces in which every subset is open and the original concept lay forgotten. With the advancement of [[categorical topology]] in the 1980s, Alexandrov spaces were rediscovered when the concept of [[Finitely generated object|finite generation]] was applied to general topology and the name '''finitely generated spaces''' was adopted for them. Alexandrov spaces were also rediscovered around the same time in the context of topologies resulting from [[denotational semantics]] and [[domain theory]] in [[computer science]].\n\nIn 1966 Michael C. McCord and A. K. Steiner each independently observed a duality between [[partially ordered set]]s and spaces which were precisely the [[Kolomogorov space|T<sub>0</sub>]] versions of the spaces that Alexandrov had introduced.<ref name=\"McC66\">{{cite journal |last=McCord |first=M. C. |title=Singular homology and homotopy groups of finite topological spaces |journal=[[Duke Mathematical Journal]] |volume=33 |issue=3 |year=1966 |pages=465–474 |doi=10.1215/S0012-7094-66-03352-7 }}</ref><ref name=\"Ste66\">{{cite journal |last=Steiner |first=A. K. |title=The Lattice of Topologies: Structure and Complementation |journal=[[Transactions of the American Mathematical Society]] |volume=122 |issue=2 |year=1966 |pages=379–398 |doi=10.2307/1994555 | issn=0002-9947 |jstor=1994555 }}</ref> P. Johnstone referred to such topologies as '''Alexandrov topologies'''.<ref name=\"Joh82\">{{cite book |last=Johnstone |first=P. T. |title=Stone spaces |location=New York |publisher=Cambridge University Press |year=1986 |edition=1st paperback |isbn=978-0-521-33779-3 }}</ref> F. G. Arenas independently proposed this name for the general version of these topologies.<ref name=\"Are99\">{{cite journal |last=Arenas |first=F. G. |title=Alexandroff spaces |journal=Acta Math. Univ. Comenianae |volume=68 |issue=1 |year=1999 |pages=17–25 |doi= |url=http://www.emis.ams.org/journals/AMUC/_vol-68/_no_1/_arenas/arenas.pdf }}</ref>  McCord also showed that these spaces are [[weak homotopy equivalence|weak homotopy equivalent]] to the [[order complex]] of the corresponding partially ordered set.  Steiner demonstrated that the duality is a [[Covariance and contravariance of functors|contravariant]] [[Lattice (order)|lattice]] isomorphism preserving [[Complete lattice|arbitrary meets and joins]] as well as complementation.\n\nIt was also a well known result in the field of [[modal logic]] that a duality exists between finite topological spaces and preorders on finite sets (the finite [[modal frame]]s for the modal logic ''S4''). [[Andrzej Grzegorczyk|A. Grzegorczyk]] observed that this extended to a duality between what he referred to as ''totally distributive spaces'' and preorders. C. Naturman observed that these spaces were the Alexandrov-discrete spaces and extended the result to a category theoretic duality between the category of Alexandrov-discrete spaces and (open) continuous maps, and the category of preorders and (bounded) monotone maps, providing the preorder characterizations as well as the [[interior algebra|interior and closure algebraic]] characterizations.<ref name=\"Nat91\">{{cite book |last=Naturman |first=C. A. |title=Interior Algebras and Topology |publisher=Ph.D. thesis, University of Cape Town Department of Mathematics |year=1991 }}</ref>\n \nA systematic investigation of these spaces from the point of view of general topology which had been neglected since the original paper by Alexandrov, was taken up by F.G. Arenas.<ref name=\"Are99\" />\n\n== See also ==\n* [[P-space|''P''-space]], a space satisfying the weaker condition that countable intersections of open sets are open\n\n== References ==\n{{Reflist}}\n\n{{DEFAULTSORT:Alexandrov Topology}}\n[[Category:Properties of topological spaces]]\n[[Category:Order theory]]\n[[Category:Closure operators]]"
    },
    {
      "title": "Amoeba order",
      "url": "https://en.wikipedia.org/wiki/Amoeba_order",
      "text": "{{Context|date=October 2014}}\n\nIn mathematics, the '''amoeba order''' is the [[partial order]] of open subsets of 2<sup>''ω''</sup> of measure less than 1/2, ordered by reverse inclusion. Amoeba forcing is [[Forcing (mathematics)|forcing]] with the amoeba order; it adds a measure 1 set of random reals. \n\nThere are several variations, where 2<sup>''ω''</sup> is replaced by the real numbers or a real vector space or the unit interval, and the number 1/2 is replaced by some positive number&nbsp;''ε''.\n\nThe name \"amoeba order\" come from the fact that a subset in the amoeba order can \"engulf\" a measure zero set by extending a \"[[pseudopod]]\" to form a larger subset in the order containing this measure zero set, which is analogous to the way an [[amoeba]] eats food.\n\nThe amoeba order satisfies the [[countable chain condition]].\n\n==References==\n\n*{{citation|mr=2905394 | zbl=1262.03001 \n|last=Kunen|first= Kenneth\n|title=Set theory\n|series=Studies in Logic |volume=34|publisher= College Publications|place= London|year= 2011|isbn= 978-1-84890-050-9 \n}}\n\n[[Category:Order theory]]\n[[Category:Forcing (mathematics)]]"
    },
    {
      "title": "Aronszajn line",
      "url": "https://en.wikipedia.org/wiki/Aronszajn_line",
      "text": "In mathematical [[set theory]], an '''Aronszajn line''' (named after [[Nachman Aronszajn]]) is a [[Total order|linear ordering]] of cardinality <math>\\aleph_1</math>\nwhich contains no subset [[order isomorphism|order-isomorphic]] to\n* <math>\\omega_1</math> with the usual ordering\n* the reverse of <math>\\omega_1</math> \n* an [[Uncountable set|uncountable]] subset of the [[Real number]]s with the usual ordering.\n\nUnlike [[Suslin's problem|Suslin lines]], the existence of Aronszajn lines is provable using the standard axioms of set theory. A linear ordering is an Aronszajn line if and only if it is the lexicographical ordering of some [[Aronszajn tree]].<ref>{{cite journal\n|title=Lexicographically ordered trees\n|last1=Funk | first1=Will\n|last2=Lutzer | first2=David J.\n|journal=Topology and its Applications\n|volume=152\n|date=2005\n|issue=3\n|pages=275–300\n|doi=10.1016/j.topol.2004.10.011\n|zbl=1071.03032\n}}</ref>\n\n==References==\n<references />\n\n[[Category:Order theory]]\n\n{{settheory-stub}}"
    },
    {
      "title": "Ascending chain condition",
      "url": "https://en.wikipedia.org/wiki/Ascending_chain_condition",
      "text": "In mathematics, the '''ascending chain condition''' ('''ACC''') and '''descending chain condition''' ('''DCC''') are finiteness properties satisfied by some [[algebraic structure]]s, most importantly [[Ideal (ring theory)|ideal]]s in certain [[commutative ring]]s.<ref>Hazewinkel, Gubareni & Kirichenko (2004), p.6, Prop. 1.1.4.</ref><ref>Fraleigh & Katz (1967), p. 366, Lemma 7.1</ref><ref>Jacobson (2009), p. 142 and 147</ref> These conditions played an important role in the development of the structure theory of commutative rings in the works of [[David Hilbert]], [[Emmy Noether]], and [[Emil Artin]].\nThe conditions themselves can be stated in an abstract form, so that they make sense for any [[partially ordered set]]. This point of view is useful in abstract algebraic dimension theory due to Gabriel and Rentschler.\n\n== Definition ==\n{{unref section|date=May 2019}}\nA [[partially ordered set]] (poset) ''P'' is said to satisfy the '''ascending chain condition''' (ACC) if every strictly [[ascending sequence]] of elements [[Eventually (mathematics)|eventually]] terminates.{{clarify|resason=meaning of “terminate”, please|date=May 2019}}  Equivalently, given any [[weakly ascending sequence]]\n:<math>a_1 \\leq a_2 \\leq a_3 \\leq \\cdots,</math>\nthere exists a positive [[integer]] ''n'' such that\n:<math>a_n = a_{n+1} = a_{n+2} = \\cdots.</math>\nSimilarly, ''P'' is said to satisfy the '''descending chain condition''' (DCC) if every strictly descending sequence of elements of ''P'' eventually terminates, that is, there is no [[infinite descending chain]]. Equivalently, every weakly descending sequence\n:<math>a_1 \\geq a_2 \\geq a_3 \\geq \\cdots</math>\nof elements of ''P'' eventually stabilizes.\n\n=== Comments ===\n* The descending chain condition on ''P'' is equivalent to ''P'' being [[well-founded]]: every nonempty subset of ''P'' has a minimal element (also called the '''minimal condition''' or '''minimum condition''').{{clarify|reason=According to the 4th item, and to the article 'Infinite descending chain', equivalence requires the axiom of choice.|date=May 2018}}\n* Similarly, the ascending chain condition is equivalent to ''P'' being converse well-founded: every nonempty subset of ''P'' has a maximal element (the '''maximal condition''' or '''maximum condition''').\n* Trivially every finite poset satisfies both ACC and DCC.\n* A [[total order|totally ordered set]] that satisfies the descending chain condition is a [[well-order|well-ordered set]] (assuming the [[axiom of dependent choice]]).\n\n== See also ==\n* [[Artinian (disambiguation)|Artinian]]\n* [[Ascending chain condition for principal ideals]]\n* [[Krull dimension]]\n* [[Maximal condition on congruences]]\n* [[Noetherian]]\n\n== Notes ==\n<references/>\n\n== References ==\n* [[M. F. Atiyah|Atiyah, M. F.]], and I. G. MacDonald, ''[[Introduction to Commutative Algebra]]'', Perseus Books, 1969, {{isbn|0-201-00361-9}}\n* [[Michiel Hazewinkel]], Nadiya Gubareni, V. V. Kirichenko. ''Algebras, rings and modules''. [[Kluwer Academic Publishers]], 2004. {{isbn|1-4020-2690-0}}\n* John B. Fraleigh, Victor J. Katz. ''A first course in abstract algebra''. Addison-Wesley Publishing Company. 5 ed., 1967. {{isbn|0-201-53467-3}}\n* [[Nathan Jacobson]]. Basic Algebra I. Dover, 2009. {{isbn|978-0-486-47189-1}}\n\n{{DEFAULTSORT:Ascending Chain Condition}}\n[[Category:Commutative algebra]]\n[[Category:Order theory]]\n[[Category:Wellfoundedness]]"
    },
    {
      "title": "Atom (order theory)",
      "url": "https://en.wikipedia.org/wiki/Atom_%28order_theory%29",
      "text": "In the mathematical field of [[order theory]], an element ''a'' of a [[partially ordered set]] with [[least element]] '''0''' is an '''atom''' if '''0''' < ''a'' and there is no ''x'' such that '''0''' < ''x'' < ''a''. \n\nEquivalently, one may define an atom to be an element that is [[minimal element|minimal]] among the non-zero elements, or alternatively an element that [[covering relation|covers]] the least element '''0'''.\n\n==Atomic orderings==\n{| style=\"float:right\"\n| [[File:Lattice T 4.svg|thumb|500x150px|'''Fig. 2''': The [[lattice (order)|lattice]] of divisors of 4, with the ordering \"''is [[divisor]] of''\", is atomic, with 2 being the only atom. It is not atomistic, since 4 cannot be obtained as [[least common multiple]] of atoms.]]\n|}\n{| style=\"float:right\"\n| [[File:Hasse diagram of powerset of 3.svg|thumb|x150px|'''Fig. 1''': The [[power set]] of the set {''x'', ''y'', ''z''} with the ordering \"''is [[subset]] of''\" is an atomistic partially ordered set: each member set can be obtained as the [[union (set theory)|union]] of all [[Singleton (mathematics)|singleton]] sets below it.]]\n|}\nLet <: denote the cover relation in a partially ordered set.\n\nA partially ordered set with a least element '''0''' is '''atomic''' if every element ''b''&nbsp;>&nbsp;'''0''' has an atom ''a'' below it, that is, there is some ''a'' such that ''b''&nbsp;≥&nbsp;''a''&nbsp;:>&nbsp;''0''.  Every finite partially ordered set with '''0''' is atomic, but the set of nonnegative [[real number]]s (ordered in the usual way) is not atomic (and in fact has no atoms).\n\nA partially ordered set is '''relatively atomic''' (or ''strongly atomic'') if for all ''a''&nbsp;<&nbsp;''b'' there is an element ''c'' such that ''a''&nbsp;<:&nbsp;''c''&nbsp;≤&nbsp;''b'' or, equivalently, if every interval [''a'',&nbsp;''b''] is atomic. Every relatively atomic partially ordered set with a least element is atomic. Every finite poset is relatively atomic,\n\nA partially ordered set with least element '''0''' is called '''atomistic''' if every element is the [[least upper bound]] of a set of atoms. The linear order with three elements is not atomistic (see Fig.2).\n\nAtoms in partially ordered sets are abstract generalizations of [[Singleton (mathematics)|singleton]]s in [[set theory]] (see Fig.1). Atomicity (the property of being atomic) provides an abstract generalization in the context of [[order theory]] of the ability to select an element from a non-empty set.\n\n==Coatoms==\nThe terms ''coatom'', ''coatomic'', and ''coatomistic'' are defined dually.  Thus, in a partially ordered set with [[greatest element]] '''1''', one says that\n* a '''coatom''' is an element covered by '''1''',\n* the set is '''coatomic''' if every ''b''&nbsp;<&nbsp;'''1''' has a coatom ''c'' above it, and\n* the set is '''coatomistic''' if every element is the [[greatest lower bound]] of a set of coatoms.\n\n==References==\n* {{Citation | last1=Davey | first1=B.A. | last2=Priestley | first2=H. A. | title=Introduction to Lattices and Order | publisher=[[Cambridge University Press]] | isbn=978-0-521-78451-1 | year=2002}}\n\n==External links==\n* {{planetmath reference|id=7153|title=Atom}}\n* {{planetmath reference|id=132|title=Poset}}\n\n[[Category:Order theory]]"
    },
    {
      "title": "Banach lattice",
      "url": "https://en.wikipedia.org/wiki/Banach_lattice",
      "text": "{{inline|date=November 2018}}{{onesource|date=November 2018}}{{notability|date=June 2017}}{{Buzzword|date=July 2017}}\nIn [[mathematics]], specifically in [[functional analysis]] and [[order theory]], a '''Banach lattice''' <math>(X, \\| \\cdot \\|)</math> is a [[Riesz space]] with a norm <math>\\| \\cdot \\|</math> such that <math>(X, \\| \\cdot\\|)</math> is a Banach space and for all <math>x, y \\in X</math> the implication <math>|x| \\le |y| \\Rightarrow \\|x\\| \\le \\|y\\|</math> holds, where as usual <math>|x| := x \\vee -x</math>.\n\n== Examples and constructions ==\n\n* <math>\\mathbb R</math>, together with its absolute value as a norm, is a Banach lattice.\n* Let <math>X</math> be a topological space, <math>Y</math> a Banach lattice and <math>\\mathcal C(X, Y)</math> the space of bounded, continuous functions from <math>X</math> to <math>Y</math> with norm <math>\\|f\\|_\\infty := \\sup_{x \\in X} \\|f(x)\\|_Y</math>. <math>\\mathcal C(X, Y)</math> becomes a Banach lattice with the pointwise order <math>f \\le g :\\Leftrightarrow \\forall x \\in X: f(x) \\le g(x)</math>.\n\n==See also==\n* [[Banach space]]\n* [[Riesz space]]\n* [[Lattice (order)]]\n\n== References ==\n\n* {{cite book\n | last = Abramovich\n | first = Yuri A.\n | author2 = Aliprantis, C. D.\n | year = 2002\n | title = An Invitation to Operator Theory\n | series= Graduate Studies in Mathematics\n | volume= 50\n | publisher = American Mathematical Society\n | location =\n | isbn = 0-8218-2146-6\n}}\n\n{{Functional Analysis}}\n\n[[Category:Functional analysis]]\n[[Category:Order theory]]\n\n\n{{mathanalysis-stub}}"
    },
    {
      "title": "Betweenness",
      "url": "https://en.wikipedia.org/wiki/Betweenness",
      "text": "{{about|ordering items with constraints|centrality in social networks|betweenness centrality|the betweenness relation in geometry|ordered geometry}}\n'''Betweenness''' is an [[algorithmic problem]] in [[order theory]] about ordering a collection of items subject to constraints that some items must be placed between others.<ref name=\"cs98\">{{citation\n | last1 = Chor | first1 = Benny\n | last2 = Sudan | first2 = Madhu | author2-link = Madhu Sudan\n | doi = 10.1137/S0895480195296221\n | issue = 4\n | journal = [[SIAM Journal on Discrete Mathematics]]\n | mr = 1640920\n | pages = 511–523 (electronic)\n | title = A geometric approach to betweenness\n | volume = 11\n | year = 1998}}.</ref> It has applications in [[bioinformatics]]<ref name=\"sksl97\">{{citation\n | last1 = Slonim | first1 = Donna\n | last2 = Kruglyak | first2 = Leonid\n | last3 = Stein | first3 = Lincoln\n | last4 = Lander | first4 = Eric | author4-link = Eric Lander\n | doi = 10.1089/cmb.1997.4.487\n | issue = 4\n | journal = Journal of Computational Biology\n | pages = 487–504\n | title = Building human genome maps with radiation hybrids\n | volume = 4\n | year = 1997}}.</ref> and was shown to be [[NP-complete]] by {{harvtxt|Opatrný|1979}}.<ref name=\"o79\"/>\n\n==Problem statement==\nThe input to a betweenness problem is a collection of [[ordered triple]]s of items. The items listed in these triples should be placed into a [[total order]], with the property that for each of the given triples, the middle item in the triple appears in the output somewhere between the other two items. The items of each triple are not required to be consecutive in the output.<ref name=\"cs98\"/><ref name=\"o79\"/>\n\n==Examples==\nAs an example, the collection of input triples\n:(2,1,3), (3,4,5), (1,4,5), (2,4,1), (5,2,3)\nis satisfied by the output ordering\n:3, 1, 4, 2, 5\nbut not by\n:3, 1, 2, 4, 5.\nIn the first of these output orderings, for all five of the input triples, the middle item of the triple appears between the other two items \nHowever, for the second output ordering, item 4 is not between items 1 and 2, contradicting the requirement given by the triple (2,4,1).\n\nIf an input contains two triples like (1,2,3) and (2,3,1) with the same three items but a different choice of the middle item, then there is no valid solution. However, there are more complicated ways of forming a set of triples with no valid solution, that do not contain such a pair of contradictory triples.\n\n==Complexity==\n{{harvtxt|Opatrný|1979}} showed that the [[Decision problem|decision version]] of the betweenness problem (in which an algorithm must decide whether or not there exists a valid solution) is [[NP-complete]] in two ways, by a [[Reduction (complexity)|reduction]] from [[3-satisfiability]] and also by a different reduction from [[hypergraph]] [[graph coloring|2-coloring]].<ref name=\"o79\">{{citation\n | last = Opatrný | first = J.\n | doi = 10.1137/0208008\n | issue = 1\n | journal = [[SIAM Journal on Computing]]\n | mr = 522973\n | pages = 111–114\n | title = Total ordering problem\n | volume = 8\n | year = 1979}}.</ref> However, it can easily be solved when all unordered triples of items are represented by an ordered triple of the input, by choosing one of the two items that are not between any others to be the start of the ordering and then using the triples involving this item to compare the relative positions of each pair of remaining items.\n\nThe related problem of finding an ordering that maximizes the number of satisfied triples is [[SNP (complexity)|MAXSNP-hard]], implying that it is impossible to achieve an [[approximation ratio]] arbitrarily close to&nbsp;1 in [[polynomial time]] unless [[P = NP]].<ref name=\"cs98\"/> It remains hard to solve or approximate even for dense instances that include an ordered triple for each possible unordered triple of items.<ref>{{citation\n | last1 = Ailon | first1 = Nir\n | last2 = Alon | first2 = Noga | author2-link = Noga Alon\n | doi = 10.1016/j.ic.2007.02.006\n | issue = 8\n | journal = [[Information and Computation]]\n | mr = 2340896\n | pages = 1117–1129\n | title = Hardness of fully dense problems\n | volume = 205\n | year = 2007}}.</ref> \nThe minimum version of the problem restricted to the tournaments was proven to have polynomial time approximation schemes (PTAS).<ref>\n{{citation\n | last1 = Karpinski | first1=Marek\n | last2 = Schudy    | first2=Warren\n | title=Approximation schemes for the betweenness problem in tournaments and related ranking problems\n | journal = LLNCS. Proc. Approx. Random \n | year=2011\n | pages=277-288\n | volume=68\n}}\n</ref>One can achieve an approximation ratio of 1/3 (in expectation) by ordering the items randomly, and this simple strategy gives the best possible polynomial-time approximation if the [[unique games conjecture]] is true.<ref>{{citation\n | last1 = Charikar | first1 = Moses | author1-link = Moses Charikar\n | last2 = Guruswami | first2 = Venkatesan | author2-link = Venkatesan Guruswami\n | last3 = Manokaran | first3 = Rajsekar\n | contribution = Every permutation CSP of arity 3 is approximation resistant\n | doi = 10.1109/CCC.2009.29\n | mr = 2932455\n | pages = 62–73\n | title = 24th Annual IEEE Conference on Computational Complexity\n | year = 2009}}.</ref> It is also possible to use [[semidefinite programming]] or combinatorial methods to find an ordering that satisfies at least half of the triples of any satisfiable instance, in polynomial time.<ref name=\"cs98\"/><ref>{{citation\n | last = Makarychev | first = Yury\n | doi = 10.1016/j.orl.2012.08.008\n | issue = 6\n | journal = Operations Research Letters\n | mr = 2998680\n | pages = 450–452\n | title = Simple linear time approximation algorithm for betweenness\n | volume = 40\n | year = 2012}}.</ref>\n\nIn [[parameterized complexity]], the problem of satisfying as many constraints as possible from a set ''C'' of constraints is [[fixed-parameter tractable]] when parameterized by the difference ''q''&nbsp;&minus;&nbsp;|''C''|/3 between the solution quality ''q'' found by the parameterized algorithm and the |''C''|/3 quality guaranteed in expectation by a random ordering.<ref>{{citation\n | last1 = Gutin | first1 = Gregory\n | last2 = Kim | first2 = Eun Jung\n | last3 = Mnich | first3 = Matthias\n | last4 = Yeo | first4 = Anders\n | doi = 10.1016/j.jcss.2010.05.001\n | issue = 8\n | journal = [[Journal of Computer and System Sciences]]\n | mr = 2722353\n | pages = 872–878\n | title = Betweenness parameterized above tight lower bound\n | volume = 76\n | year = 2010| arxiv = 0907.5427\n }}.</ref>\n\nAlthough not guaranteed to succeed, a [[greedy heuristic]] can find solutions to many instances of the betweenness problem arising in practice.<ref name=\"sksl97\"/>\n\n==Applications==\nOne application of betweenness arises in [[bioinformatics]], as part of the process of [[gene mapping]]. Certain types of genetic experiments can be used to determine the ordering of triples of genetic markers, but do not distinguish a genetic sequence from its reversal, so the information yielded from such an experiment determines only which one out of three markers is the middle one. The betweenness problem is an abstraction of the problem of assembling a collection of markers into a single sequence given experimental data of this type.<ref name=\"cs98\"/><ref name=\"sksl97\"/>\n\nThe betweenness problem has also been used to model theories of [[probability]], [[causality]], and [[time]].<ref>{{citation\n | last1 = Chvátal | first1 = Vašek | author1-link = Václav Chvátal\n | last2 = Wu | first2 = Baoyindureng\n | doi = 10.1007/s10670-011-9321-z\n | issue = 1\n | journal = Erkenntnis\n | pages = 41–48\n | title = On Reichenbach's causal betweenness\n | volume = 76\n | year = 2011| arxiv = 0902.1763}}.</ref>\n\n==References==\n{{reflist}}\n\n[[Category:NP-complete problems]]\n[[Category:Order theory]]"
    },
    {
      "title": "Bound graph",
      "url": "https://en.wikipedia.org/wiki/Bound_graph",
      "text": "{{Orphan|date=July 2016}}\n\nIn [[graph theory]], a '''bound graph''' expresses which pairs of elements of some [[partially ordered set]] have an [[upper bound]].  Rigorously, any [[Graph (discrete mathematics)|graph]] ''G'' is a bound graph if there exists a partial order ≤ on the [[vertex (graph theory)|vertices]] of ''G'' with the property that for any vertices ''u'' and ''v'' of ''G'', ''uv'' is an [[edge (graph theory)|edge]] of ''G'' if and only if ''u'' ≠ ''v'' and there is a vertex ''w'' such that ''u'' ≤ ''w'' and ''v'' ≤ ''w''.\n\nBound graphs are sometimes referred to as ''upper bound graphs'', but the analogously defined '''lower bound graphs''' comprise exactly the same class—any lower bound for ≤ is easily seen to be an upper bound for the [[duality (order theory)|dual]] partial order ≥.\n\n== References ==\n\n*{{cite journal\n |author1=McMorris, F.R. |author2=Zaslavsky, T. | title = Bound graphs of a partially ordered set\n | journal = Journal of Combinatorics, Information & System Sciences\n | volume = 7\n | year = 1982\n | pages = 134–138}}\n\n*{{cite journal\n |author1=Lundgren, J.R. |author2=Maybee, J.S. | title = A characterization of upper bound graphs\n | journal = Congressus Numerantium\n | volume = 40\n | year = 1983\n | pages = 189–193}}\n\n*{{cite journal\n |author1=Bergstrand, D.J. |author2=Jones, K.F. | title = On upper bound graphs of partially ordered sets\n | journal = Congressus Numerantium\n | volume = 66\n | year = 1988\n | pages = 185–193}}\n\n*{{cite journal\n | author = Tanenbaum, P.J.\n | title = Bound graph polysemy\n | journal = Electronic Journal of Combinatorics\n | volume = 7\n | year = 2000\n | pages = #R43\n | url = http://www.combinatorics.org/Volume_7/PDF/v7i1r43.pdf}}\n\n[[Category:Graph families]]\n[[Category:Order theory]]"
    },
    {
      "title": "Bounded complete poset",
      "url": "https://en.wikipedia.org/wiki/Bounded_complete_poset",
      "text": "In the [[mathematics|mathematical]] field of [[order theory]], a [[partially ordered set]] is '''bounded complete''' if all of its [[subset]]s that have some [[upper bound]] also have a [[least upper bound]]. Such a partial order can also be called '''consistently''' or '''coherently complete''' ([[#Visser2004|Visser 2004, p. 182]]), since any upper bound of a set can be interpreted as some [[consistent]] (non-contradictory) piece of information that extends all the information present in the set. Hence the presence of some upper bound in a way guarantees the consistency of a set. Bounded completeness then yields the existence of a least upper bound of any \"consistent\" subset, which can be regarded as the most general piece of information that captures all the knowledge present within this subset. This view closely relates to the idea of information ordering that one typically finds in [[domain theory]]. \n\nFormally, a partially ordered set (''P'', ≤) is ''bounded complete'' if the following holds for any subset ''S'' of ''P'':\n\n: If ''S'' has some upper bound, then it also has a least upper bound.\n\nBounded completeness has various relationships to other completeness properties, which are detailed in the article on [[completeness (order theory)|completeness in order theory]]. Note also that the term ''bounded poset'' is sometimes used to refer to a partially ordered set that has both a [[least element|least]] and a [[greatest element]]. Hence it is important to distinguish between a bounded-complete poset and a bounded [[complete partial order]] (cpo).\n\nFor a typical example of a bounded-complete poset, consider the set of all finite decimal numbers starting with \"0.\" (like 0.1, 0.234, 0.122) together with all infinite such numbers (like the decimal representation 0.1111... of 1/9). Now these elements can be ordered based on the [[prefix order]] of words: a decimal number ''n'' is below some other number ''m'' if there is some string of digits w such that ''nw'' = ''m''. For example, 0.2 is below 0.234, since one can obtain the latter by appending the string \"34\" to 0.2. The infinite decimal numbers are the [[maximal element]]s within this order. In general, subsets of this order do not have least upper bounds: just consider the set {0.1, 0.3}. Looking back at the above intuition, one might say that it is not consistent to assume that some number starts both with 0.1 and with 0.3. However, the order is still bounded complete. In fact, it is even an example of a more specialized class of structures, the [[Scott domain]]s, which provide many other examples for bounded-complete posets.\n\n==References==\n* <cite id=Visser2004>Visser, A. (2004) ‘Semantics and the Liar Paradox’ in: D.M. Gabbay and F. Günther (ed.) Handbook of Philosophical Logic, 2nd Edition, Volume 11, pp.&nbsp;149 – 240</cite>\n\n[[Category:Order theory]]"
    },
    {
      "title": "Bounded set",
      "url": "https://en.wikipedia.org/wiki/Bounded_set",
      "text": "[[Image:Bounded unbounded.svg|right|thumb|An [[artist's impression]] of a bounded set (top) and of an unbounded set (bottom). The set at the bottom continues forever towards the right.]]\n:''\"Bounded\" and \"boundary\" are distinct concepts; for the latter see [[boundary (topology)]]. A [[circle]] in isolation is a boundaryless bounded set, while the [[half plane]] is unbounded yet has a boundary.\nIn [[mathematical analysis]] and related areas of [[mathematics]], a [[Set (mathematics)|set]] is called '''bounded''', if it is, in a certain sense, of finite size. Conversely, a set which is not bounded is called '''unbounded'''. The word bounded makes no sense in a general topological space without a corresponding [[Metric_(mathematics)|metric]].\n\n== Definition ==\n[[File:Illustration of supremum.svg|thumb|upright=1.6|A real set with upper bounds and its [[supremum]].]]\nA set ''S'' of [[real number]]s is called ''bounded from above'' if there exists some real number ''k'' (not necessarily in ''S'') such that ''k'' ≥ '' s'' for all ''s'' in ''S''. The number ''k'' is called an '''upper bound''' of ''S''. The terms ''bounded from below'' and '''lower bound''' are similarly defined.\n\nA set ''S'' is '''bounded''' if it has both upper and lower bounds. Therefore, a set of real numbers is bounded if it is contained in a [[interval (mathematics)|finite interval]].\n\n== Metric space ==\n\nA [[subset]] ''S'' of a [[metric space]] (''M'', ''d'') is '''bounded''' if it is contained in a [[ball (mathematics)|ball]] of finite radius, i.e. if there exists ''x'' in ''M'' and ''r'' > 0 such that for all ''s'' in ''S'', we have d(''x'', ''s'') < ''r''. (''M'', ''d'') is a ''bounded'' metric space (or ''d'' is a ''bounded'' metric) if ''M'' is bounded as a subset of itself.\n\n*[[Total boundedness]] implies boundedness. For subsets of '''R'''<sup>''n''</sup> the two are equivalent.\n*A metric space is [[compact space|compact]] if and only if it is [[Complete metric space|complete]] and totally bounded.\n*A subset of [[Euclidean space]] '''R'''<sup>''n''</sup> is compact if and only if it is [[closed set|closed]] and bounded.\n\n== Boundedness in topological vector spaces ==\n{{main|Bounded set (topological vector space)}}\nIn [[topological vector space]]s, a different definition for bounded sets exists which is sometimes called [[von Neumann bounded]]ness. If the topology of the topological vector space is induced by a [[metric (mathematics)|metric]] which is [[homogeneous metric|homogeneous]], as in the case of a metric induced by the [[norm (mathematics)|norm]] of [[normed vector spaces]], then the two definitions coincide.\n\n==Boundedness in order theory==\n\nA set of real numbers is bounded if and only if it has an upper and lower bound. This definition is extendable to subsets of any [[partially ordered set]]. Note that this more general concept of boundedness does not correspond to a notion of \"size\". \n\nA subset ''S'' of a partially ordered set ''P'' is called '''bounded above''' if there is an element ''k'' in ''P'' such that ''k'' ≥ ''s'' for all ''s'' in ''S''. The element ''k'' is called an '''upper bound''' of ''S''. The concepts of '''bounded below''' and '''lower bound''' are defined similarly.  (See also [[upper and lower bounds]].)\n\nA subset ''S'' of a partially ordered set ''P'' is called '''bounded''' if it has both an upper and a lower bound, or equivalently, if it is contained in an [[Interval (mathematics)#Intervals in order theory|interval]]. Note that this is not just a property of the set ''S'' but also one of the set ''S'' as subset of ''P''.\n\nA '''bounded poset''' ''P'' (that is, by itself, not as subset) is one that has a least element and a [[greatest element]]. Note that this concept of boundedness has nothing to do with finite size, and that a subset ''S'' of a bounded poset ''P'' with as order the [[Binary_relation#Restriction|restriction]] of the order on ''P'' is not necessarily a bounded poset.\n\nA subset ''S'' of '''R'''<sup>''n''</sup> is bounded with respect to the [[Euclidean distance]] if and only if it bounded as subset of '''R'''<sup>''n''</sup> with the [[product order]]. However, ''S'' may be bounded as subset of '''R'''<sup>''n''</sup> with the [[lexicographical order]], but not with respect to the Euclidean distance.\n\nA class of [[ordinal number]]s is said to be unbounded, or [[Cofinal (mathematics)|cofinal]], when given any ordinal, there is always some element of the class greater than it. Thus in this case \"unbounded\" does not mean unbounded by itself but unbounded as a subclass of the class of all ordinal numbers.\n\n== See also ==\n*[[Bounded function]]\n*[[Local boundedness]]\n*[[Order theory]]\n*[[Totally bounded]]\n\n==References==\n*{{cite book |first=Robert G. |last=Bartle |authorlink=Robert G. Bartle |first2=Donald R. |last2=Sherbert |title=Introduction to Real Analysis |location=New York |publisher=John Wiley & Sons |year=1982 |isbn=0-471-05944-7 }}\n*{{cite book |first=Robert D. |last=Richtmyer |authorlink=Robert D. Richtmyer |title=Principles of Advanced Mathematical Physics |publisher=Springer |location=New York |year=1978 |isbn=0-387-08873-3 }}\n\n[[Category:Mathematical analysis]]\n[[Category:Functional analysis]]\n[[Category:Order theory]]"
    },
    {
      "title": "Bourbaki–Witt theorem",
      "url": "https://en.wikipedia.org/wiki/Bourbaki%E2%80%93Witt_theorem",
      "text": "In [[mathematics]], the '''Bourbaki–Witt theorem''' in [[order theory]], named after [[Nicolas Bourbaki]] and [[Ernst Witt]], is a basic [[fixed point theorem]] for [[partially ordered set]]s. It states that if ''X'' is a non-empty [[chain complete]] [[poset]], and \n\n: <math>f : X \\to X</math> \n\nsuch that \n\n: <math>f (x) \\geq x</math> for all <math>x,</math>\n\nthen ''f'' has a [[fixed point (mathematics)|fixed point]]. Such a function ''f'' is called ''inflationary'' or ''progressive''.\n\n== Special case of a finite poset ==\n\nIf the poset ''X'' is finite then the statement of the theorem has a clear interpretation that leads to the proof. The sequence of successive iterates, \n\n: <math> x_{n+1}=f(x_n), n=0,1,2,\\ldots, </math>\n\nwhere ''x''<sub>0</sub> is any element of ''X'', is monotone increasing. By the finiteness of ''X'', it stabilizes:\n\n: <math> x_n=x_{\\infty},</math> for ''n'' sufficiently large. \n\nIt follows that ''x''<sub>∞</sub> is a fixed point of ''f''.\n\n== Proof of the theorem ==\n\nPick some <math>y \\in X</math>. Define a function ''K'' recursively on the ordinals as follows:\n\n:<math>\\,K(0) = y</math>\n\n:<math>\\,K( \\alpha+1 ) = f( K( \\alpha ) ).</math>\n\nIf <math> \\beta </math> is a [[limit ordinal]], then by construction \n\n:<math>\\{ K( \\alpha ) \\ : \\ \\alpha < \\beta \\}</math> \n\nis a chain in ''X''. Define \n\n:<math>K( \\beta ) = \\sup \\{ K( \\alpha ) \\ : \\ \\alpha < \\beta \\}.</math>\n\nThis is now an increasing function from the ordinals into ''X''. It cannot be strictly increasing, as if it were we would have an [[injective function]] from the ordinals into a set, violating [[Hartogs number|Hartogs' lemma]]. Therefore the function must be eventually constant, so for some \n\n:<math> \\alpha , \\ \\ K( \\alpha+1 ) = K ( \\alpha ); </math>\n\nthat is,\n\n:<math>\\,f( K( \\alpha ) ) = K ( \\alpha ).</math>\n\nSo letting \n\n:<math>\\,x = K ( \\alpha ),</math> \n\nwe have our desired fixed point. [[Q.E.D.]]\n\n==Applications==\nThe Bourbaki–Witt theorem has various important applications. One of the most common is in the proof that the [[axiom of choice]] implies [[Zorn's lemma]]. We first prove it for the case where ''X'' is chain complete and has no maximal element. Let ''g'' be a choice function on \n\n:<math>P(X) - \\{ \\varnothing \\}.</math>\n\nDefine a function \n\n:<math>f : X \\to X</math> \n\nby \n\n:<math>f(x) = g( \\{ y \\ : \\ y > x \\} ).</math>\n\nThis is allowed as, by assumption, the set is non-empty. Then ''f''(''x'') > ''x'', so ''f'' is an inflationary function with no fixed point, contradicting the theorem.\n\nThis special case of Zorn's lemma is then used to prove the [[Hausdorff maximality principle]], that every poset has a maximal chain, which is easily seen to be equivalent to Zorn's Lemma.\n\nBourbaki–Witt has other applications.  In particular in [[computer science]], it is used in the theory of [[computable function]]s.\nIt is also used to define recursive data types, e.g. linked lists, in [[domain theory]].\n\n==References==\n*{{cite journal | author=Nicolas Bourbaki | authorlink=Nicolas Bourbaki| title=Sur le théorème de Zorn | journal = [[Archiv der Mathematik]] | volume=2 | year=1949 | pages=434&ndash;437 | doi=10.1007/bf02036949 }}\n*{{cite journal | author=Ernst Witt |authorlink=Ernst Witt| title=Beweisstudien zum Satz von M. Zorn | journal = [[Mathematische Nachrichten]] | volume=4 | year=1951 | pages=434–438 | doi=10.1002/mana.3210040138}}\n\n{{DEFAULTSORT:Bourbaki-Witt theorem}}\n[[Category:Order theory]]\n[[Category:Fixed-point theorems]]\n[[Category:Theorems in the foundations of mathematics]]\n[[Category:Articles containing proofs]]"
    },
    {
      "title": "Bruhat order",
      "url": "https://en.wikipedia.org/wiki/Bruhat_order",
      "text": "In mathematics, the '''Bruhat order''' (also called '''strong order''' or '''strong Bruhat order''' or '''Chevalley order''' or '''Bruhat–Chevalley order''' or '''Chevalley–Bruhat order''') is a partial order on the elements of a [[Coxeter group]], that corresponds to the inclusion order on [[Schubert varieties]].\n\n==History==\n\nThe Bruhat order on the Schubert varieties of a flag manifold or Grassmannian was first studied by {{harvtxt|Ehresmann|1934}}, and the analogue for more general semisimple algebraic groups was studied by {{harvtxt|Chevalley|1958}}. {{harvtxt|Verma|1968}} started the combinatorial study of the Bruhat order on the Weyl group, and introduced the name \"Bruhat order\" because of the relation to the [[Bruhat decomposition]] introduced by [[François Bruhat]].\n\nThe left and right weak Bruhat orderings were studied by {{harvs|txt|last=Björner|year1=1984}}.\n\n==Definition==\n\nIf (''W'',''S'') is a [[Coxeter system]] with generators ''S'', then the Bruhat order is a partial order on the group ''W''. Recall that a reduced word for an element ''w'' of ''W'' is a minimal length expression of ''w'' as a product of elements of ''S'', and the length ''l''(''w'') of ''w'' is the length of a reduced word.\n\n*The (strong) Bruhat order is defined by ''u''≤''v'' if some substring of some (or every) reduced word for ''v'' is a reduced word for ''u''.  \n(Note that here a substring is not necessarily a consecutive substring.)\n\n*The weak left (Bruhat) order is defined by  ''u''≤<sub>''L''</sub>''v''  if some final substring of some  reduced word for ''v'' is a reduced word for ''u''.\n*The weak right (Bruhat) order is defined by  ''u''≤<sub>''R''</sub>''v''  if some initial substring of some  reduced word for ''v'' is a reduced word for ''u''.\n\nFor more on the weak orders, see the article [[weak order of permutations]].\n\n==Bruhat graph==\n\nThe Bruhat graph is a directed graph related to the (strong) Bruhat order. The vertex set is the set of elements of the Coxeter group and the edge set consists of directed edges (''u'', ''v'') whenever ''u'' = ''t'' ''v'' for some reflection ''t'' and ''l''(''u'') < ''l''(''v'').  One may view the graph as an edge-labeled directed graph with edge labels coming from the set of reflections.  (One could also define the Bruhat graph using multiplication on the right; as graphs, the resulting objects are isomorphic, but the edge labelings are different.)\n\nThe strong Bruhat order on the symmetric group (permutations) has Möbius function given by <math>\\mu(\\pi,\\sigma)=(-1)^{\\ell(\\sigma)-\\ell(\\pi)}</math>,\nand thus this poset is Eulerian, meaning its Möbius function is produced by the rank function on the poset.\n\n==References==\n\n*{{Citation | last1=Björner | first1=Anders | editor1-last=Greene | editor1-first=Curtis | editor1-link = Curtis Greene | title=Combinatorics and algebra (Boulder, Colo., 1983) | url=https://books.google.com/books?id=2axt00oBDEwC | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Contemp. Math. | isbn=978-0-8218-5029-9  | mr=777701 | year=1984 | volume=34 | chapter=Orderings of Coxeter groups | chapterurl=https://books.google.com/books?id=2axt00oBDEwC&pg=175 | pages=175–195}}\n*{{Citation | last1=Björner | first1=Anders | last2=Brenti | first2=Francesco | title=Combinatorics of Coxeter groups | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Graduate Texts in Mathematics | isbn=978-3-540-44238-7 | doi=10.1007/3-540-27596-7 | mr=2133266 | year=2005 | volume=231}}\n*{{Citation | last1=Chevalley | first1=C. | editor1-last=Haboush | editor1-first=William J. | editor2-last=Parshall | editor2-first=Brian J. | title=Algebraic groups and their generalizations: classical methods (University Park, PA, 1991) | url=https://books.google.com/books?id=-fTjI8adsNQC | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Proc. Sympos. Pure Math. | isbn=978-0-8218-1540-3  | mr=1278698 | year=1958 | volume=56 | chapter=Sur les décompositions cellulaires des espaces G/B | pages=1–23}}\n*{{Citation | last1=Ehresmann | first1=Charles | title=Sur la Topologie de Certains Espaces Homogènes | jstor=1968440 | publisher=Annals of Mathematics | language=French | series=Second Series | jfm=60.1223.05 | year=1934 | journal=[[Annals of Mathematics]] | issn=0003-486X | volume=35 | issue=2 | pages=396–443 | doi = 10.2307/1968440}}\n*{{Citation | last1=Verma | first1=Daya-Nand | title=Structure of certain induced representations of complex semisimple Lie algebras | doi=10.1090/S0002-9904-1968-11921-4  | mr=0218417 | year=1968 | journal=[[Bulletin of the American Mathematical Society]] | issn=0002-9904 | volume=74 | pages=160–166}}\n\n[[Category:Coxeter groups]]\n[[Category:Order theory]]"
    },
    {
      "title": "Cantor–Bernstein theorem",
      "url": "https://en.wikipedia.org/wiki/Cantor%E2%80%93Bernstein_theorem",
      "text": "{{For|the theorem that injections from A to B and from B to A imply a bijection between A and B|Schröder–Bernstein theorem}}\nIn [[set theory]] and [[order theory]], the '''Cantor–Bernstein theorem''' states that the [[cardinality]] of the second type class, the class of [[Countable set|countable]] [[order type]]s, equals the [[cardinality of the continuum]]. It was used by [[Felix Hausdorff]] and named by him after [[Georg Cantor]] and [[Felix Bernstein (mathematician)|Felix Bernstein]]. Cantor constructed a family of countable order types with the cardinality of the continuum, and in his 1901 inaugural dissertation Bernstein proved that such a family can have no higher cardinality.<ref name=\"plotkin\"/>\n\nBecause the second type class contains the countable [[ordinal number]]s, which have cardinality <math>\\aleph_1</math>, this result proves (by an inclusion of naturally defined sets) that <math>\\aleph_1\\le 2^{\\aleph_0}</math>, a relation between these two [[aleph number]]s that (without assuming the [[axiom of choice]]) was not previously known.<ref name=\"plotkin\">{{cite book|title=Hausdorff on Ordered Sets|volume=25|series=History of Mathematics|editor-first=J. M.|editor-last=Plotkin|publisher=American Mathematical Society|isbn=9780821890516|year=2005|page=3|url=https://books.google.com/books?id=M_skkA3r-QAC&pg=PA3}}.</ref>\n\n== References ==\n{{reflist}}\n\n{{DEFAULTSORT:Cantor-Bernstein theorem}}\n[[Category:Order theory]]"
    },
    {
      "title": "Causal sets",
      "url": "https://en.wikipedia.org/wiki/Causal_sets",
      "text": "{{Beyond the Standard Model|expanded=[[Quantum gravity]]}}\nThe '''causal sets''' program is an approach to [[quantum gravity]]. Its founding principles are that [[spacetime]] is fundamentally discrete (a collection of discrete spacetime points, called the elements of the causal set) and that spacetime events are related by a [[partial order]]. This partial order has the physical meaning of the [[causality relation]]s between spacetime events.\n\nThe program is based on a theorem<ref name=\"Malament\">{{cite journal|last1=Malament|first1=David B.|title=The class of continuous timelike curves determines the topology of spacetime|journal=Journal of Mathematical Physics|date=July 1977|volume=18|issue=7|pages=1399–1404|doi=10.1063/1.523436|bibcode=1977JMP....18.1399M}}</ref> by [[David Malament]] that states that if there is a [[bijection|bijective]] map between two [[past and future distinguishing]] space times that preserves their [[causal structure]] then the map is a [[Conformal map|conformal isomorphism]]. The conformal factor that is left undetermined is related to the volume of regions in the spacetime. This volume factor can be recovered by specifying a volume element for each space time point. The volume of a space time region could then be found by counting the number of points in that region.\n\nCausal sets was initiated by [[Rafael Sorkin]] who continues to be the main proponent of the program. He has coined the slogan \"Order + Number = Geometry\" to characterize the above argument. The program provides a theory in which space time is fundamentally discrete while retaining local [[Lorentz invariance]].\n\n== Definition ==\nA '''causal set''' (or '''causet''') is a set <math>C</math> with a [[partial order]] [[binary relation|relation]] <math>\\preceq</math> that is\n* [[Reflexive relation|Reflexive]]: For all <math>x \\in C</math>, we have <math> x \\preceq x </math>.\n* [[Antisymmetric relation|Antisymmetric]]: For all <math>x, y \\in C</math>, we have <math> x \\preceq y</math> and <math>y \\preceq x</math> implies <math>x = y</math>.\n* [[Transitive relation|Transitive]]: For all <math>x, y, z \\in C</math>, we have <math> x \\preceq y</math> and <math>y \\preceq z </math> implies <math> x \\preceq z </math>.\n* [[Locally finite poset|Locally finite]]: For all <math>x, z \\in C</math>, we have <math>|\\{y \\in C | x \\preceq y \\preceq z\\}| < \\aleph_0</math>.\n\nWe'll write <math>x \\prec y</math> if <math>x \\preceq y </math> and <math>x \\neq y</math>.\n\nThe set <math>C</math> represents the set of [[Event (relativity)|spacetime event]]s and the order relation <math>\\preceq</math> represents the causal relationship between events (see [[causal structure]] for the analogous idea in a [[pseudo-Riemannian manifold#Lorentzian manifold|Lorentzian manifold]]).\n\nAlthough this definition uses the reflexive convention we could have chosen the irreflexive convention in which the order relation is [[irreflexive relation|irreflexive]]. \n\nThe [[causal relation]] of a [[Lorentzian manifold]] (without closed [[causal curve]]s) satisfies the first three conditions. It is the local finiteness condition that introduces spacetime discreteness.\n\n== Comparison to the continuum ==\n\nGiven a causal set we may ask whether it can be [[embedding|embedded]] into a [[Lorentzian manifold]]. An embedding would be a map taking elements of the causal set into points in the manifold such that the order relation of the causal set matches the causal ordering of the manifold. A further criterion is needed however before the embedding is suitable. If, on average, the number of causal set elements mapped into a region of the manifold is proportional to the volume of the region then the embedding is said to be ''faithful''. In this case we can consider the causal set to be 'manifold-like'\n\nA central conjecture to the causal set program is that the same causal set cannot be faithfully embedded into two spacetimes that are not similar on large scales. This is called the ''[[Hauptvermutung]]'', meaning 'fundamental conjecture'. It is difficult to define this conjecture precisely because it is difficult to decide when two spacetimes are 'similar on large scales'.\n\nModelling spacetime as a causal set would require us to restrict attention to those causal sets that are 'manifold-like'. Given a causal set this is a difficult property to determine.\n\n=== Sprinkling ===\n\n[[File:CausalSet(1000Points).png|right|thumb|A plot of 1000 sprinkled points in 1+1 dimensions]]\nThe difficulty of determining whether a causal set can be embedded into a manifold can be approached from the other direction. We can create a causal set by sprinkling points into a Lorentzian manifold. By sprinkling points in proportion to the volume of the spacetime regions and using the causal order relations in the manifold to induce order relations between the sprinkled points, we can produce a causal set that (by construction) can be faithfully embedded into the manifold.\n\nTo maintain Lorentz invariance this sprinkling of points must be done randomly using a [[Poisson process]]. Thus the probability of sprinkling <math>n</math> points into a region of volume <math>V</math> is\n\n<math>P(n) = \\frac{(\\rho V)^n e^{-\\rho V}}{n!}</math>\n\nwhere <math> \\rho </math> is the density of the sprinkling.\n\nSprinkling points as a regular lattice would not keep the number of points proportional to the region volume.\n\n== Geometry ==\n\nSome geometrical constructions in manifolds carry over to causal sets. When defining these we must remember to rely only on the causal set itself, not on any background spacetime into which it might be embedded. For an overview of these constructions, see.<ref name=Brightwell>{{cite journal |last1=Brightwell |first1=Graham |last2=Gregory |first2=Ruth |title=Structure of random discrete spacetime |journal=Physical Review Letters |date=21 January 1991 |volume=66 |issue=3 |pages=260–263 |doi=10.1103/PhysRevLett.66.260|pmid=10043761 |bibcode=1991PhRvL..66..260B }}</ref>\n\n=== Geodesics ===\n\n[[File:CausalSet180Geodesic.png|right|thumb|A plot of geodesics between two points in a 180-point causal set made by sprinkling into 1+1 dimensions]]\n\nA ''link'' in a causal set is a pair of elements <math>x, y \\in C</math> such that <math>x \\prec y</math> but with no <math>z \\in C</math> such that <math>x \\prec z \\prec y</math>.\n\nA ''chain'' is a sequence of elements <math>x_0,x_1,\\ldots,x_n</math> such that <math>x_i \\prec x_{i+1}</math> for <math>i=0,\\ldots,n-1</math>. The length of a chain is <math>n</math>.\nIf every <math>x_i, x_{i+1}</math> in the chain form a link, then the chain is called a ''path''.\n\nWe can use this to define the notion of a [[geodesic]] between two causal set elements, provided they are order comparable, that is, causally connected (physically, this means they are time-like).  A geodesic between two elements <math>x \\preceq y \\in C</math> is a chain consisting only of links such that\n# <math>x_0 = x</math> and <math>x_n = y</math>\n# The length of the chain, <math>n</math>, is maximal over all chains from <math>x</math> to <math>y</math>.\nIn general there can be more than one geodesic between two comparable elements.\n\nMyrheim<ref name='Myrhiem'>J. Myrheim, [http://doc.cern.ch//archive/electronic/kek-scan//197808143.pdf CERN preprint] TH-2538 (1978)</ref> first suggested that the length of such a geodesic should be directly proportional to the proper time along a timelike geodesic joining the two spacetime points. Tests of this conjecture have been made using causal sets generated from sprinklings into flat spacetimes. The proportionality has been shown to hold and is conjectured to hold for sprinklings in curved spacetimes too.\n\n=== Dimension estimators ===\n\nMuch work has been done in estimating the manifold [[dimension]] of a causal set. This involves algorithms using the causal set aiming to give the dimension of the manifold into which it can be faithfully embedded. The algorithms developed so far are based on finding the dimension of a [[Minkowski spacetime]] into which the causal set can be faithfully embedded.\n\n*'''Myrheim-Meyer dimension'''\nThis approach relies on estimating the number of <math>k</math>-length chains present in a sprinkling into <math>d</math>-dimensional Minkowski spacetime. Counting the number of <math>k</math>-length chains in the causal set then allows an estimate for <math>d</math> to be made.\n\n*'''Midpoint-scaling dimension'''\nThis approach relies on the relationship between the proper time between two points in Minkowski spacetime and the volume of the [[Spacetime#Spacetime intervals|spacetime interval]] between them. By computing the maximal chain length (to estimate the proper time) between two points <math>x</math> and <math>y</math> and counting the number of elements <math>z</math> such that <math>x \\prec z \\prec y</math> (to estimate the volume of the spacetime interval) the dimension of the spacetime can be calculated.\n\nThese estimators should give the correct dimension for causal sets generated by high-density sprinklings into <math>d</math>-dimensional Minkowski spacetime. Tests in conformally-flat spacetimes<ref name=Reid>{{cite journal |last1=Reid |first1=David D. |title=Manifold dimension of a causal set: Tests in conformally flat spacetimes |journal=Physical Review D |date=30 January 2003 |volume=67 |issue=2 |pages=024034 |arxiv=gr-qc/0207103 |bibcode= 2003PhRvD..67b4034R|doi=10.1103/PhysRevD.67.024034}}</ref> have shown these two methods to be accurate.\n\n== Dynamics ==\n\nAn ongoing task is to develop the correct [[Dynamics (physics)|dynamics]] for causal sets. These would provide a set of rules that determine which causal sets correspond to physically realistic [[spacetime]]s. The most popular approach to developing causal set dynamics is based on the ''[[sum-over-histories]]'' version of [[quantum mechanics]]. This approach would perform a \"sum-over-causal sets\" by ''growing'' a causal set one element at a time. Elements would be added according to quantum mechanical rules and [[Interference (wave propagation)|interference]] would ensure a large manifold-like spacetime would dominate the contributions. The best model for dynamics at the moment is a classical model in which elements are added according to probabilities. This model, due to David Rideout and [[Rafael Sorkin]], is known as ''classical sequential growth'' (CSG) dynamics.<ref>{{cite journal |last1=Rideout |first1=D. P. |last2=Sorkin |first2=R. D. |title=Classical sequential growth dynamics for causal sets |journal=Physical Review D |year=2000 |volume=61 |issue=2 |pages=024002 |arxiv=gr-qc/9904062 |doi=10.1103/PhysRevD.61.024002|bibcode=2000PhRvD..61b4002R }}</ref> The classical sequential growth model is a way to generate causal sets by adding new elements one after another. Rules for how new elements are added are specified and, depending on the parameters in the model, different causal sets result.\n\nIn analogy to the [[path integral formulation]] of quantum mechanics, one approach to developing a quantum dynamics for causal sets has been to apply an [[action principle]] in the sum-over-causal sets approach. Sorkin has proposed a discrete analogue for the [[d'Alembertian]], which can in turn be used to define the [[Ricci curvature scalar]] and thereby the ''Benincasa-Dowker action'' on a causal set.<ref>{{cite arxiv |last=Sorkin |first=D. P. |title=Does Locality Fail at Intermediate Length-Scales  |date=20 March 2007 |eprint=gr-qc/0703099}}</ref><ref>{{cite journal |last1=Benincasa |first1=D. M. T. |last2=Dowker |first2=F. |title=The Scalar Curvature of a Causal Set |journal=Phys. Rev. Lett. |date=May 2010 |volume=104 |issue=18 |pages=181301 |doi=10.1103/PhysRevLett.104.181301|pmid=20482164 |bibcode=2010PhRvL.104r1301B |arxiv=1001.2725 }}</ref> Monte-Carlo simulations have provided evidence for a continuum phase in 2D using the Benincasa-Dowker Action.<ref>{{cite journal |last=Surya |first=S. |title=Evidence for the continuum in 2D causal set quantum gravity |journal=Classical and Quantum Gravity |date=July 2012 |volume=29 |issue=13 |pages=132001 |doi=10.1088/0264-9381/29/13/132001|bibcode=2012CQGra..29m2001S |arxiv=1110.6244 }}</ref>\n\n== See also ==\n\n* [[Causal dynamical triangulation|Causal dynamical triangulation (CDT)]]\n* [[Causal structure]]\n* [[General relativity]]\n* [[Order theory]]\n\n== References ==\n{{reflist}}\n\n== Further reading ==\n{{refbegin|2}}\n;Introduction and reviews\n*L. Bombelli.  ''[http://www.phy.olemiss.edu/~luca/Topics/st/causal_sets.html Causal Set reference page]'' (Overview)\n*L. Bombelli. ''[http://www.gravity.psu.edu/events/conferences/Quantum_GravityIII/proceedings.shtml Causal Sets: Overview and Status]'',  Talk given at Quantum Gravity in the Americas III, August 24–26, 2006;  (Introduction, Overview)\n*[[Fay Dowker|F. Dowker]],  ''Causal sets and the deep structure of spacetime'', [[arXiv:gr-qc/0508109]]; (Introduction)\n*[[Fay Dowker|F. Dowker]], ''[https://dx.doi.org/10.1080/17445760500356833 Causal sets as discrete spacetime]'', Contemporary Physics, vol. 47, Issue 1, p.&nbsp;1-9; (Overview, Introduction)\n*F. Dowker, ''Introduction to causal sets and their phenomenology'', Gen Relativ Gravit (2013) 45:1651–1667 doi:10.1007/s10714-013-1569-y (Overview of recent research)\n*J. Henson, ''The causal set approach to quantum gravity'', [[arXiv:gr-qc/0601121]]; (Introduction, Overview)\n*D.D. Reid; ''Introduction to causal sets: an alternate view of spacetime structure''; Canadian Journal of Physics 79, 1-16 (2001); [[arXiv:gr-qc/9909075]]; (General);\n*[[Rafael Sorkin|R.D. Sorkin]]; ''[http://www.maths.qmw.ac.uk/~pjc/csgnotes/ Causal set glossary and bibliography]'' (20 November 2001); (Glossary and bibliography);\n*[[Rafael Sorkin|R.D. Sorkin]], ''Causal Sets: Discrete Gravity (Notes for the Valdivia Summer School)'', In Proceedings of the Valdivia Summer School, edited by A. Gomberoff and D. Marolf; [[arXiv:gr-qc/0309009]]; (Introduction, Glossary)\n\n;Foundations\n*L. Bombelli, J. Lee, D. Meyer, [[Rafael Sorkin|R.D. Sorkin]], ''[http://prola.aps.org/abstract/PRL/v59/i5/p521_1 Spacetime as a causal set]'', Phys. Rev. Lett. 59:521-524 (1987) ; (Introduction, Foundations)\n*C. Moore, ''[http://link.aps.org/abstract/PRL/v60/p655 Comment on \"Space-time as a causal set\"]'',   Phys. Rev. Lett. 60, 655 (1988); (Foundations)\n*L. Bombelli, J. Lee, D. Meyer, [[Rafael Sorkin|R.D. Sorkin]], ''[http://link.aps.org/abstract/PRL/v60/p656 Bombelli et al. Reply]'',  Phys. Rev. Lett. 60, 656 (1988); (Foundations)\n*[[Albert Einstein|A. Einstein]],  ''Letter to H.S. Joachim'', August 14, 1954; Item 13-453 cited in J. Stachel,“Einstein and the Quantum: Fifty Years of Struggle”, in From Quarks to Quasars, Philosophical Problems of Modern Physics, edited by R.G. Colodny (U. Pittsburgh Press, 1986), pages 380-381; (Historical)\n*[[David Finkelstein|D. Finkelstein]], ''[http://prola.aps.org/abstract/PR/v184/i5/p1261_1 Space-time code]'', Phys. Rev. 184:1261 (1969); (Foundations)\n*[[David Finkelstein|D. Finkelstein]], ''[https://dx.doi.org/10.1007/BF00669395 \"Superconducting\" Causal Nets]'',  Int. J. Th. Phys 27:473(1988);  (Foundations)\n*G. Hemion, ''[https://dx.doi.org/10.1007/BF00708682 A quantum theory of space and time]''; Found. Phys. 10 (1980), p.&nbsp;819 (Similar proposal)\n*J. Myrheim, ''[http://doc.cern.ch//archive/electronic/kek-scan//197808143.pdf Statistical geometry]'', CERN preprint TH-2538 (1978); (Foundations, Historical)\n*[[Bernhard Riemann|B. Riemann]], ''[https://web.archive.org/web/20160318034045/http://www.maths.tcd.ie/pub/HistMath/People/Riemann/Geom/ Über die Hypothesen, welche der Geometrie zu Grunde liegen]'', The Collected Works of B. Riemann (Dover NY 1953); ; (Historical)\n*[[Rafael Sorkin|R.D. Sorkin]]; ''[https://dx.doi.org/10.1007/BF00673986 A Finitary Substitute for Continuous Topology]'',  Int. J. Theor. Phys. 30 7: 923-947 (1991); (Foundational)\n*[[Rafael Sorkin|R.D. Sorkin]], ''Does a Discrete Order underly Spacetime and its Metric?'',  Proceedings of the Third Canadian Conference on General Relativity and Relativistic Astrophysics, (Victoria, Canada, May, 1989), edited by A. Coley, F. Cooperstock, B.Tupper, pp.&nbsp;82–86, (World Scientific, 1990); (Introduction)\n*[[Rafael Sorkin|R.D. Sorkin]], ''[http://physics.syr.edu/~sorkin/some.papers/ First Steps with Causal Sets]'',  General Relativity and Gravitational Physics, (Proceedings of the Ninth Italian Conference of the same name, held Capri, Italy, September, 1990), 68-90, (World Scientific, Singapore), (1991), R. Cianci, R. de Ritis, M. Francaviglia, G. Marmo, C. Rubano, P. Scudellaro (eds.);  (Introduction)\n*[[Rafael Sorkin|R.D. Sorkin]], ''[http://physics.syr.edu/~sorkin/some.papers/ Spacetime and Causal Sets]'',  Relativity and Gravitation: Classical and Quantum, (Proceedings of the SILARG VII Conference, held Cocoyoc, Mexico, December, 1990), pages 150-173, (World Scientific, Singapore, 1991), J.C. D’Olivo, E. Nahmad-Achar, M.Rosenbaum, M.P. Ryan, L.F. Urrutia and F. Zertuche (eds.);  (Introduction)\n*[[Rafael Sorkin|R.D. Sorkin]], ''Forks in the Road, on the Way to Quantum Gravity'',  Talk given at the conference entitled “Directions in General Relativity”, held at College Park, Maryland, May, 1993, Int. J. Th. Phys. 36: 2759–2781 (1997); [[arXiv:gr-qc/9706002]]; (Philosophical, Introduction)\n*[[Gerardus 't Hooft|G.'t Hooft]], ''Quantum gravity: a fundamental problem and some radical ideas'',  Recent Developments in Gravitation (Proceedings of the 1978 Cargese Summer Institute) edited by M. Levy and S. Deser (Plenum, 1979); (Introduction, Foundations, Historical)\n*[[Erik Christopher Zeeman|E.C. Zeeman]], ''[https://archive.is/20130224064640/http://link.aip.org/link/?JMAPAQ/5/490/1 Causality Implies the Lorentz Group]'',  J. Math. Phys. 5: 490-493; (Historical, Foundations)\n\n;PhD theses\n*L. Bombelli, ''[https://web.archive.org/web/20110718134535/http://www.phy.olemiss.edu/~luca/Papers/PhD.pdf Space-time as a Causal Set]'', PhD thesis ([[Syracuse University]], 1987); (Introduction, Kinematics)\n*A.R. Daughton; ''The Recovery of Locality for Causal Sets and Related Topics''; PhD thesis ([[Syracuse University]], 1993); (Locality)\n*D. Dou, ''Causal Sets, a Possible Interpretation for the Black Hole Entropy, and Related Topics'';  PhD thesis ([[SISSA]], Trieste, 1999); [[arXiv:gr-qc/0106024]] (Black hole entropy)\n*S. Johnston, ''Quantum Fields on Causal Sets'', PhD Thesis ([[Imperial College London]], 2010) [[arXiv:1010.5514]] (Quantum Field Theory)\n*D.A. Meyer, ''[http://hdl.handle.net/1721.1/14328 The Dimension of Causal Sets]'',  PhD thesis ([[M.I.T.]], 1988);  (Dimension theory)\n*L. Philpott, ''Causal Set Phenomenology'', PhD Thesis ([[Imperial College London]], 2010); [[arXiv:1009.1593]] (Swerves, Phenomenology)\n*D.P. Rideout; ''Dynamics of Causal Sets''; PhD Thesis ([[Syracuse University]] 2001); [[arXiv:gr-qc/0212064]]; (Cosmology, Dynamics)\n*R.B. Salgado; ''[https://web.archive.org/web/20120227062902/http://physics.syr.edu/~salgado/thesis/Salgado-dissertation-proquest.pdf Toward a Quantum Dynamics for Causal Sets]''; PhD Thesis ([[Syracuse University]] 2008); (Scalar field theory, Quantum Measure Theory)\n*R. Sverdlov; ''Quantum Field Theory and Gravity in Causal Sets''; PhD Thesis ([[University of Michigan]] 2009); [[arXiv: 0905.2263]] (Quantum Field Theory and Gravity)\n\n;Talks\n*Joe Henson, ''[http://pirsa.org/10090092/ An Invitation to Causal Sets]''; Talk given at [[Perimeter Institute]], 14 September 2010, Waterloo ON (Introduction)\n*[[Fay Dowker|F. Dowker]], ''[http://loops05.aei.mpg.de/index_files/abstract_dowker.html Causal Set Phenomenology]''; Talk given at Loops 05, 10–14 October 2005, Potsdam, [[Max Planck Institute for Gravitational Physics]] (Swerves)\n*S. Johnston; ''[http://pirsa.org/08040043/ Particle Propagators from Discrete Spacetime]''; Talk given at [[Perimeter Institute]] 14 April 2008 (Quantum field theory)\n*D.A. Meyer; Talk given at the 1997 Santa Fe workshop: ''[https://web.archive.org/web/20090114214257/http://t8web.lanl.gov/people/emil/Slides/sf97talks.html Causal Sets and Feynman diagrams]''; Presented at \"New Directions in Simplicial Quantum Gravity\" July 28 - August 8, 1997; (Feynman diagrams, Quantum Dynamics)\n*D.P. Rideout; ''[http://loops05.aei.mpg.de/index_files/abstract_rideout.html Spatial Hypersurfaces in Causal Set Cosmology]''; Talk given at  Loops 05, 10–14 October 2005, Potsdam, [[Max Planck Institute for Gravitational Physics]] (Spatial hyper-surfaces, Dynamics)\n*J. Scargle, ''[http://scipp.ucsc.edu/seminars/experimental/archive_sq07/scargle_4-24-07.ppt Testing Quantum Gravity Theories with GLAST]''; Talk given at Santa Cruz Institute for Particle Physics, April 24, 2007. (Lorentz invariance, Phenomenology)\n*[[Rafael Sorkin|R.D. Sorkin]]; Two Talks given at the 1997 Santa Fe workshop: ''[https://web.archive.org/web/20090114214257/http://t8web.lanl.gov/people/emil/Slides/sf97talks.html A Review of the Causal Set Approach to Quantum Gravity]'' and ''[https://web.archive.org/web/20090114214257/http://t8web.lanl.gov/people/emil/Slides/sf97talks.html A Growth Dynamics for Causal Sets]''; Presented at ”New Directions in Simplicial Quantum Gravity” July 28 - August 8, 1997; ;;\n*[[Rafael Sorkin|R.D. Sorkin]]; ''[http://pirsa.org/07010001 Does quantum gravity give rise to an observable nonlocality?]''; Talk given at [[Perimeter Institute]] 17/01/2007 (d'Alembertian, Locality)\n*[[Rafael Sorkin|R.D. Sorkin]], ''[http://loops05.aei.mpg.de/index_files/abstract_sorkin.html Some Insights for Quantum Gravity Derived from Work on Causal Sets]''; Talk given at  Loops 05, 10–14 October 2005, Potsdam, [[Max Planck Institute for Gravitational Physics]] (Overview)\n*[[Rafael Sorkin|R.D. Sorkin]] ''[http://pirsa.org/05090001 Is a past-finite causal order the inner basis of spacetime?]'' Talk given at [[Perimeter Institute]] 07/09/2005\n*S. Surya, ''[http://loops05.aei.mpg.de/index_files/abstract_surya.html Recovering spacetime topology from a causet]''; Talk given at  Loops 05, 10–14 October 2005, Potsdam, [[Max Planck Institute for Gravitational Physics]] (Topology)\n*R. Sverdlov; ''[http://pirsa.org/08020043/ Introduction of bosonic fields into causal set theory]''; Talk given at [[Perimeter Institute]] 19/02/2008 (Quantum field theory)\n\n;Manifoldness\n*L. Bombelli, D.A. Meyer; ''[https://dx.doi.org/10.1016/0375-9601(89)90474-X The origin of Lorentzian geometry]''; Phys. Lett. A 141:226-228 (1989);  (Manifoldness)\n*L. Bombelli, [[Rafael Sorkin|R.D. Sorkin]], ''When are Two Lorentzian Metrics close?'', General Relativity and Gravitation, proceedings of the 12th International Conference on General Relativity and Gravitation, held July 2–8, 1989, in Boulder, Colorado, USA, under the auspices of the International Society on General Relativity and Gravitation, 1989, p.&nbsp;220; (Closeness of Lorentzian manifolds)\n*L. Bombelli, ''Causal sets and the closeness of Lorentzian manifolds'', Relativity in General: proceedings of the Relativity Meeting \"93, held September 7–10, 1993, in Salas, Asturias, Spain. Edited by J. Diaz Alonso, M. Lorente Paramo. {{ISBN|2-86332-168-4}}. Published by Editions Frontieres, 91192 Gif-sur-Yvette Cedex, France, 1994, p.&nbsp;249; (Closeness of Lorentzian manifolds)\n*L. Bombelli, ''Statistical Lorentzian geometry and the closeness of Lorentzian manifolds'', J. Math. Phys.41:6944-6958 (2000); [[arXiv:gr-qc/0002053]]  (Closeness of Lorentzian manifolds, Manifoldness)\n*A.R. Daughton, ''[http://www.iop.org/EJ/abstract/0264-9381/15/11/009 An investigation of the symmetric case of when causal sets can embed into manifolds]'',  Class. Quantum Grav.15(11):3427-3434 (Nov., 1998) (Manifoldness)\n*J. Henson, ''Constructing an interval of Minkowski space from a causal set'',  Class. Quantum Grav. 23 (2006) L29-L35; [[arXiv:gr-qc/0601069]]; (Continuum limit, Sprinkling)\n*S. Major, D.P. Rideout, S. Surya, ''On Recovering Continuum Topology from a Causal Set'', J.Math.Phys.48:032501,2007; [[arXiv:gr-qc/0604124]] (Continuum Topology)\n*S. Major, D.P. Rideout, S. Surya; ''Spatial Hypersurfaces in Causal Set Cosmology''; Class. Quantum Grav. 23 (2006) 4743-4752; [[arXiv:gr-qc/0506133v2]]; (Observables, Continuum topology)\n*S. Major, D.P. Rideout, S. Surya, ''Stable Homology as an Indicator of Manifoldlikeness in Causal Set Theory'', [[arXiv:0902.0434]] (Continuum topology and homology)\n*D.A. Meyer, ''The Dimension of Causal Sets I: Minkowski dimension'', Syracuse University preprint (1988); (Dimension theory)\n*D.A. Meyer, ''The Dimension of Causal Sets II: Hausdorff dimension'', Syracuse University preprint (1988); (Dimension theory)\n*D.A. Meyer, ''[https://dx.doi.org/10.1007/BF01110544 Spherical containment and the Minkowski dimension of partial orders]'',  [[Order (journal)|Order]] 10: 227-237 (1993); (Dimension theory)\n*J. Noldus, ''[https://dx.doi.org/10.1088/0264-9381/19/23/313 A new topology on the space of Lorentzian metrics on a fixed manifold]'',   Class. Quant. Grav 19: 6075-6107 (2002); (Closeness of Lorentzian manifolds)\n*J. Noldus, ''[https://dx.doi.org/10.1088/0264-9381/21/4/007 A Lorentzian Gromov–Hausdorff notion of distance]'', Class. Quantum Grav. 21, 839-850, (2004);  (Closeness of Lorentzian manifolds)\n*D.D. Reid, ''Manifold dimension of a causal set: Tests in conformally flat spacetimes'',  Phys. Rev. D67 (2003) 024034; [[arXiv:gr-qc/0207103v2]] (Dimension theory)\n*S. Surya, ''Causal Set Topology''; [[arXiv:0712.1648]]\n\n;Geometry\n*E. Bachmat; ''Discrete spacetime and its applications''; [[arXiv:gr-qc/0702140]]; (Geodesics, Antichains)\n*G. Brightwell, R. Gregory; ''[http://link.aps.org/abstract/PRL/v66/p260 The Structure of Random Discrete Spacetime]''; Phys. Rev. Lett. 66:260-263 (1991);  (Geodesic Length)\n*[[Gary Gibbons|G. W. Gibbons]], S. N. Solodukhin; ''The Geometry of Small Causal Diamonds'' [[arXiv:hep-th/0703098]] (Causal intervals)\n*[[Stephen Hawking|S.W. Hawking]], A.R. King, P.J. McCarthy; ''[http://resolver.caltech.edu/CaltechAUTHORS:HAWjmp76 A new topology for curved space–time which incorporates the causal, differential, and conformal structures]''; J. Math. Phys. 17 2:174-181 (1976);  (Geometry, [[Causal Structure]])\n*S. He, D.P. Rideout; ''A Causal Set Black Hole''; [[arXiv:0811.4235]] (Causal structure of Schwarzschild spacetime, Sprinklings)\n*R. Ilie, G.B. Thompson, D.D. Reid; ''[https://dx.doi.org/10.1088/0264-9381/23/10/002 A numerical study of the correspondence between paths in a causal set and geodesics in the continuum]''; 2006 Class. Quantum Grav. 23 3275-3285 [[arXiv:gr-qc/0512073]](Geodesic length)\n*A.V. Levichev; ''Prescribing the conformal geometry of a lorentz manifold by means of its causal structure''; Soviet Math. Dokl. 35:452-455, (1987); (Geometry, [[Causal Structure]])\n*{{cite journal|last1=Malament|first1=David B.|title=The class of continuous timelike curves determines the topology of spacetime|journal=Journal of Mathematical Physics|date=July 1977|volume=18|issue=7|pages=1399–1404|doi=10.1063/1.523436|bibcode=1977JMP....18.1399M}}\n*D.P. Rideout, P. Wallden; ''Spacelike distance from discrete causal order''; [[arXiv:0810.1768]] (Spatial distances)\n\n;Cosmological constant prediction\n*M. Ahmed, S. Dodelson, P.B. Greene, [[Rafael Sorkin|R.D. Sorkin]],  ''Everpresent lambda'';  Phys. Rev. D69, 103523, (2004) [[arXiv:astro-ph/0209274v1]] ;   (Cosmological Constant)\n*Y. Jack Ng and H. van Dam, ''A small but nonzero cosmological constant'';  Int. J. Mod. Phys D. 10 : 49 (2001) [[arXiv:hep-th/9911102v3]]; (PreObservation Cosmological Constant)\n*Y. Kuznetsov, ''On cosmological constant in Causal Set theory''; [[arXiv:0706.0041]]\n*[[Rafael Sorkin|R.D. Sorkin]], ''A Modified Sum-Over-Histories for Gravity''; reported in Highlights in gravitation and cosmology: Proceedings of the International Conference on Gravitation and Cosmology, Goa, India, 14–19 December 1987, edited by B. R. Iyer, Ajit Kembhavi, [[Jayant Narlikar|Jayant V. Narlikar]], and [[C. V. Vishveshwara]], see pages 184-186 in the article by D. Brill and L. Smolin: “Workshop on quantum gravity and new directions”, pp  183–191 (Cambridge University Press, Cambridge, 1988); (PreObservation Cosmological Constant)\n*[[Rafael Sorkin|R.D. Sorkin]]; ''[https://dx.doi.org/10.1007/BF00670514 On the Role of Time in the Sum-over-histories Framework for Gravity]'', paper presented to the conference on The History of Modern Gauge Theories, held Logan, Utah, July 1987; Int. J. Theor. Phys. 33 : 523-534 (1994); (PreObservation Cosmological Constant)\n*[[Rafael Sorkin|R.D. Sorkin]], ''[http://physics.syr.edu/~sorkin/some.papers/ First Steps with Causal Sets]'', in R. Cianci, R. de Ritis, M. Francaviglia, G. Marmo, C. Rubano, P. Scudellaro (eds.), General Relativity and Gravitational Physics (Proceedings of the Ninth Italian Conference of the same name, held Capri, Italy, September, 1990), pp.&nbsp;68–90 (World Scientific, Singapore, 1991); (PreObservation Cosmological Constant)\n*[[Rafael Sorkin|R.D. Sorkin]]; ''Forks in the Road, on the Way to Quantum Gravity'', talk given at the conference entitled “Directions in General Relativity”, held at College Park, Maryland,  May, 1993; Int. J. Th. Phys. 36 : 2759–2781 (1997) [[arXiv:gr-qc/9706002]] ; (PreObservation Cosmological Constant)\n*[[Rafael Sorkin|R.D. Sorkin]], ''Discrete Gravity'';  a series of lectures to the First Workshop on Mathematical Physics and Gravitation, held Oaxtepec, Mexico, Dec. 1995 (unpublished); (PreObservation Cosmological Constant)\n*[[Rafael Sorkin|R.D. Sorkin]], ''Big extra dimensions make Lambda too small''; [[arXiv:gr-qc/0503057v1]];  (Cosmological Constant)\n*[[Rafael Sorkin|R.D. Sorkin]],  ''Is the cosmological \"constant\" a nonlocal quantum residue of discreteness of the causal set type?''; Proceedings of the PASCOS-07 Conference, July 2007, Imperial College London; [[arXiv:0710.1675]]; (Cosmological Constant)\n*J. Zuntz, ''The CMB in a Causal Set Universe'', [[arXiv:0711.2904]] (CMB)\n\n;Lorentz and Poincaré invariance, phenomenology\n*L. Bombelli, J. Henson, [[Rafael Sorkin|R.D. Sorkin]]; ''Discreteness without symmetry breaking: a theorem'';   [[arXiv:gr-qc/0605006v1]]; (Lorentz invariance, Sprinkling)\n*[[Fay Dowker|F. Dowker]], J. Henson, [[Rafael Sorkin|R.D. Sorkin]], ''Quantum gravity phenomenology, Lorentz invariance and discreteness''; Mod. Phys. Lett. A19, 1829–1840, (2004) [[arXiv:gr-qc/0311055v3]]; (Lorentz invariance, Phenomenology, Swerves)\n*[[Fay Dowker|F. Dowker]], J. Henson, [[Rafael Sorkin|R.D. Sorkin]], ''Discreteness and the transmission of light from distant sources'';  [[arXiv:1009.3058]] (Coherence of light, Phenomenology)\n*J. Henson, ''Macroscopic observables and Lorentz violation in discrete quantum gravity''; [[arXiv:gr-qc/0604040v1]]; (Lorentz invariance, Phenomenology)\n*N. Kaloper, D. Mattingly, ''Low energy bounds on Poincaré violation in causal set theory''; Phys. Rev. D 74, 106001 (2006) [[arXiv:astro-ph/0607485]] (Poincaré invariance, Phenomenology)\n*D. Mattingly, ''Causal sets and conservation laws in tests of Lorentz symmetry''; Phys. Rev. D 77, 125021 (2008) [[arXiv:0709.0539]] (Lorentz invariance, Phenomenology)\n*L. Philpott, [[Fay Dowker|F. Dowker]], [[Rafael Sorkin|R.D. Sorkin]], ''Energy-momentum diffusion from spacetime discreteness''; [[arXiv:0810.5591]] (Phenomenology, Swerves)\n\n;Black hole entropy in causal set theory\n*D. Dou, ''Black Hole Entropy as Causal Links'';  Fnd. of Phys, 33 2:279-296(18) (2003); [[arXiv:gr-qc/0302009v1]]  (Black hole entropy)\n*D.P. Rideout, S. Zohren, ''Counting entropy in causal set quantum gravity'' ; [[arXiv:gr-qc/0612074v1]]; (Black hole entropy)\n*D.P. Rideout, S. Zohren, ''Evidence for an entropy bound from fundamentally discrete gravity'';  Class. Quantum Grav. 23 (2006) 6195-6213; [[arXiv:gr-qc/0606065v2]] (Black hole entropy)\n\n;Locality and quantum field theory\n*G. Hemion, ''[https://dx.doi.org/10.1007/BF00670680 A discrete geometry: speculations on a new framework for classical electrodynamics]''; Int. J. Theor. Phys. 27 (1988), p.&nbsp;1145 (Classical electrodynamics)\n*S. Johnston; ''Particle propagators on discrete spacetime''; 2008 Class. Quantum Grav. 25 202001; [[arXiv:0806.3083]] (Quantum Field Theory)\n*S. Johnston; ''The Feynman propagator for a Free Scalar Field on a Causal Set''; Phys. Rev. Lett. 103, 180401 (2009); [[arXiv:0909.0944]] (Quantum Field Theory)\n*[[Rafael Sorkin|R.D. Sorkin]]; ''Does Locality Fail at Intermediate Length-Scales''; Towards Quantum Gravity, Daniele Oriti (ed.) (Cambridge University Press, 2007); [[arXiv:gr-qc/0703099v1]]; (d'Alembertian, Locality)\n*R. Sverdlov, L. Bombelli; ''Gravity and Matter in Causal Set Theory''; [[arXiv:0801.0240]]\n*R. Sverdlov; ''A Geometrical Description of Spinor Fields''; [[arXiv:0802.1914]]\n*R. Sverdlov; ''Bosonic Fields in Causal Set Theory''; [[arXiv:0807.4709]]\n*R. Sverdlov; ''Gauge Fields in Causal Set Theory''; [[arXiv:0807.2066]]\n*R. Sverdlov; ''Spinor fields in Causal Set Theory''; [[arXiv:0808.2956]]\n\n;Causal set dynamics\n*M. Ahmed, D. Rideout, ''Indications of de Sitter Spacetime from Classical Sequential Growth Dynamics of Causal Sets''; [[arXiv:0909.4771]]\n*A.Ash, P. McDonald, ''Moment Problems and the Causal Set Approach to Quantum Gravity''; J.Math.Phys. 44 (2003) 1666-1678; [[arXiv:gr-qc/0209020]]\n*A.Ash, P. McDonald, ''[https://archive.is/20130223083234/http://link.aip.org/link/?JMAPAQ/46/062502/1 Random partial orders, posts, and the causal set approach to discrete quantum gravity]''; J.Math.Phys. 46 (2005) 062502 (Analysis of number of posts in growth processes)\n*D.M.T. Benincasa, [[Fay Dowker|F. Dowker]], ''The Scalar Curvature of a Causal Set''; [[arXiv:1001.2725]]; (Scalar curvature, actions)\n*G. Brightwell; M. Luczak; ''Order-invariant Measures on Causal Sets''; [[arXiv:0901.0240]]; (Measures on causal sets)\n*G. Brightwell; M. Luczak; ''Order-invariant Measures on Fixed Causal Sets''; [[arXiv:0901.0242]]; (Measures on causal sets)\n*G. Brightwell, [[Fay Dowker|H.F. Dowker]], R.S. Garcia, J. Henson, [[Rafael Sorkin|R.D. Sorkin]]; ''General covariance and the \"problem of time\" in a discrete cosmology''; In ed. K. Bowden, Correlations:Proceedings of the ANPA 23 conference, August 16–21, 2001, Cambridge, England, pp.&nbsp;1–17. Alternative Natural Philosophy Association, (2002).;[[arXiv:gr-qc/0202097]]; (Cosmology, Dynamics, Observables)\n*G. Brightwell, [[Fay Dowker|H.F. Dowker]], R.S. Garcia, J. Henson, [[Rafael Sorkin|R.D. Sorkin]]; ''\"Observables\" in causal set cosmology''; Phys. Rev. D67, 084031, (2003); [[arXiv:gr-qc/0210061]];  (Cosmology, Dynamics, Observables)\n*G. Brightwell, J. Henson, S. Surya; ''A 2D model of Causal Set Quantum Gravity: The emergence of the continuum''; [[arXiv:0706.0375]]; (Quantum Dynamics, Toy Model)\n*G.Brightwell, N. Georgiou; ''[https://web.archive.org/web/20110312102804/http://www.maths.bris.ac.uk/~maxng/contlim.pdf Continuum limits for classical sequential growth models]'' [[University of Bristol]] preprint. (Dynamics)\n*A. Criscuolo, H. Waelbroeck; ''Causal Set Dynamics: A Toy Model''; Class. Quantum Grav.16:1817-1832 (1999); [[arXiv:gr-qc/9811088]]; (Quantum Dynamics, Toy Model)\n*[[Fay Dowker|F. Dowker]], S. Surya; ''Observables in extended percolation models of causal set cosmology'';Class. Quantum Grav. 23, 1381-1390 (2006); [[arXiv:gr-qc/0504069v1]]; (Cosmology, Dynamics,  Observables)\n*M. Droste, ''Universal homogeneous causal sets'', J. Math. Phys. 46, 122503 (2005); [[arXiv:gr-qc/0510118]]; (Past-finite causal sets)\n*J. Henson, D. Rideout, [[Rafael Sorkin|R.D. Sorkin]], S. Surya; ''Onset of the Asymptotic Regime for (Uniformly Random) Finite Orders''; Experimental Mathematics 26, 3:253-266 (2017); (Cosmology, Dynamics)\n*A.L. Krugly; ''[https://dx.doi.org/10.1023/A:1013273214769 Causal Set Dynamics and Elementary Particles]''; Int. J. Theo. Phys 41 1:1-37(2004);; (Quantum Dynamics)\n*X. Martin, D. O'Connor, D.P. Rideout, [[Rafael Sorkin|R.D. Sorkin]]; ''On the “renormalization” transformations induced by cycles of expansion and contraction in causal set cosmology''; Phys. Rev. D 63, 084026 (2001); [[arXiv:gr-qc/0009063]] (Cosmology, Dynamics)\n*D.A. Meyer; ''Spacetime Ising models''; (UCSD preprint May 1993); (Quantum Dynamics)\n*D.A. Meyer; ''[https://dx.doi.org/10.1007/BF00759191 Why do clocks tick?]''; General Relativity and Gravitation 25 9:893-900;; (Quantum Dynamics)\n*I. Raptis; ''Quantum Space-Time as a Quantum Causal Set'', [[arXiv:gr-qc/0201004v8]]\n*D.P. Rideout, [[Rafael Sorkin|R.D. Sorkin]]; ''A classical sequential growth dynamics for causal sets'', Phys. Rev. D, 6, 024002 (2000);[[arXiv:gr-qc/9904062]] (Cosmology, Dynamics)\n*D.P. Rideout, [[Rafael Sorkin|R.D. Sorkin]]; ''Evidence for a continuum limit in causal set dynamics''  Phys. Rev. D 63:104011,2001; [[arXiv:gr-qc/0003117]](Cosmology, Dynamics)\n*[[Rafael Sorkin|R.D. Sorkin]]; ''Indications of causal set cosmology''; Int. J. Theor. Ph. 39(7):1731-1736 (2000); [[arXiv:gr-qc/0003043]]; (Cosmology, Dynamics)\n*[[Rafael Sorkin|R.D. Sorkin]]; ''Relativity theory does not imply that the future already exists: a counterexample''; Relativity and the Dimensionality of the World, Vesselin Petkov (ed.) (Springer 2007, in press); [[arXiv:gr-qc/0703098v1]]; (Dynamics, Philosophy)\n*M. Varadarajan, D.P. Rideout; ''A general solution for classical sequential growth dynamics of Causal Sets''; Phys. Rev. D 73 (2006) 104021; [[arXiv:gr-qc/0504066v3]]; (Cosmology, Dynamics)\n* {{cite journal | last1 = M.R.| first1 = Khoshbin-e-Khoshnazar | year = 2013 | title = Binding Energy of the Very Early Universe: Abandoning Einstein for a Discretized Three–Torus Poset.A Proposal on the Origin of Dark Energy | url = | journal = Gravitation and Cosmology | volume = 19 | issue = 2| pages = 106–113 | doi=10.1134/s0202289313020059|bibcode = 2013GrCo...19..106K }};(Dynamics, Poset)\n{{refend}}\n{{refend}}\n\n== External links ==\n* [https://arxiv.org/abs/gr-qc/0601121 The causal set approach to quantum gravity] a review article by Joe Henson on causal sets\n* [http://link.aps.org/abstract/PRL/v59/p521 Space-time as a causal set] - one of the first papers by Luca Bombelli, Joohan Lee, David Meyer, and Rafael D. Sorkin\n* [http://www.einstein-online.info/en/spotlights/causal_sets/index.html Geometry from order: causal sets] - non-technical article by Rafael D. Sorkin on [https://web.archive.org/web/20070611231234/http://www.einstein-online.info/en/ Einstein Online]\n\n{{Theories of gravitation}}\n{{quantum gravity}}\n\n{{DEFAULTSORT:Causal Sets}}\n[[Category:Theoretical physics]]\n[[Category:Quantum gravity]]\n[[Category:Order theory]]"
    },
    {
      "title": "Centered set",
      "url": "https://en.wikipedia.org/wiki/Centered_set",
      "text": "In [[mathematics]], in the area of order theory, an '''upwards centered set''' ''S'' is a [[subset]] of a [[partially ordered set]], ''P'', such that any finite subset of ''S'' has an [[upper bound]] in ''P''. Similarly, any finite subset of a '''downwards centered set''' has a lower bound. An upwards centered set can also be called a '''consistent set'''. Note that any [[directed set]] is necessarily centered, and any centered set is [[linked set|linked]].\n\nA subset ''B'' of a partial order is said to be '''σ-centered''' if it is a [[countable]] [[Union (set theory)|union]] of centered sets.\n\n== References ==\n*{{cite book | last=Fremlin | first=David H. | title=Consequences of Martin's axiom | publisher=[[Cambridge University Press]]| location=Cambridge | year=1984 | isbn=0-521-25091-9 | others=Cambridge tracts in mathematics, no. 84}}\n*{{citation | title=Introduction to Lattices and Order | first1=B. A. | last1=Davey | first2=Hilary A. | last2=Priestley | edition=2nd | publisher=[[Cambridge University Press]] | year=2002 | isbn=978-0-521-78451-1 | url=https://books.google.com/books?id=vVVTxeuiyvQC&pg=PA201 | page=201| contribution=9.1 | zbl=1002.06001 }}.\n\n[[Category:Order theory]]\n\n\n{{mathlogic-stub}}"
    },
    {
      "title": "Chain-complete partial order",
      "url": "https://en.wikipedia.org/wiki/Chain-complete_partial_order",
      "text": "In [[mathematics]], specifically [[order theory]], a [[partially ordered set]] is '''chain-complete''' if every [[chain (order theory)|chain]] in it has a [[least upper bound]]. It is '''ω-complete''' when every increasing sequence of elements (a type of [[countability|countable]] chain) has a least upper bound; the same notion can be extended to other cardinalities of chains.<ref name=\"m76\">{{citation\n | last = Markowsky | first = George\n | issue = 1\n | journal = Algebra Universalis\n | mr = 0398913\n | pages = 53–68\n | title = Chain-complete posets and directed sets with applications\n | volume = 6\n | year = 1976\n | doi=10.1007/bf02485815}}.</ref>\n\n==Examples==\nEvery [[complete lattice]] is chain-complete. Unlike complete lattices, chain-complete posets are relatively common. Examples include:\n\n* The set of all [[linearly independent]] subsets of a [[vector space]] ''V'', ordered by [[inclusion (set theory)|inclusion]]. \n* The set of all [[partial function]]s on a set, ordered by [[Restriction (mathematics)|restriction]].\n* The set of all partial [[choice function]]s on a collection of non-empty sets, ordered by restriction.\n* The set of all [[prime ideals]] of a [[ring (mathematics)|ring]], ordered by inclusion.\n* The set of all [[consistent]] theories of a [[first-order logic|first-order language]].\n\n==Properties==\nA poset is chain-complete if and only if it is a [[complete partial order|pointed dcpo]]<ref name=\"m76\"/>. However, this equivalence requires the [[axiom of choice]].\n\n[[Zorn's lemma]] states that, if a poset has an upper bound for every chain, then it has a [[maximal element]]. Thus, it applies to chain-complete posets, but is more general in that it allows chains that have upper bounds but do not have least upper bounds.\n\nChain-complete posets also obey the [[Bourbaki–Witt theorem]], a [[fixed point theorem]] stating that, if ''f'' is a function from a chain complete poset to itself with the property that, for all ''x'', ''f''(''x'')&nbsp;≥&nbsp;''x'', then ''f'' has a fixed point. This theorem, in turn, can be used to prove that Zorn's lemma is a consequence of the [[axiom of choice]].<ref>{{citation\n | last = Bourbaki | first = Nicolas | authorlink = Nicolas Bourbaki\n | journal = Archiv der Mathematik\n | mr = 0047739\n | pages = 434–437 (1951)\n | title = Sur le théorème de Zorn\n | volume = 2\n | year = 1949\n | doi=10.1007/bf02036949}}.</ref><ref>{{citation\n | last = Witt | first = Ernst | authorlink = Ernst Witt\n | journal = [[Mathematische Nachrichten]]\n | mr = 0039776\n | pages = 434–438\n | title = Beweisstudien zum Satz von M. Zorn\n | volume = 4\n | year = 1951\n | doi=10.1002/mana.3210040138}}.</ref>\n\nBy analogy with the [[Dedekind–MacNeille completion]] of a partially ordered set, every partially ordered set can be extended uniquely to a minimal chain-complete poset.<ref name=\"m76\"/>\n\n==See also==\n*[[Completeness (order theory)]]\n\n==References==\n{{reflist}}\n\n{{DEFAULTSORT:Chain Complete}}\n[[Category:Order theory]]"
    },
    {
      "title": "Closure operator",
      "url": "https://en.wikipedia.org/wiki/Closure_operator",
      "text": "In [[mathematics]], a '''closure operator''' on a [[Set (mathematics)|set]] ''S'' is a [[Function (mathematics)|function]] <math>\\operatorname{cl}: \\mathcal{P}(S)\\rightarrow \\mathcal{P}(S)</math> from the [[power set]] of ''S'' to itself which satisfies the following conditions for all sets <math>X,Y\\subseteq S</math>\n:{| border=\"0\"\n|-\n| <math>X \\subseteq \\operatorname{cl}(X)</math>\n| {{nb5}}(cl is ''extensive''),\n|-\n| <math>X\\subseteq Y \\Rightarrow \\operatorname{cl}(X) \\subseteq \\operatorname{cl}(Y)</math>\n| {{nb5}}(cl is ''increasing''),\n|-\n| <math> \\operatorname{cl}(\\operatorname{cl}(X))=\\operatorname{cl}(X)</math>\n| {{nb5}}(cl is ''idempotent'').\n|}\n\nClosure operators are determined by their '''closed sets''', i.e., by the sets of the form cl(''X''), since the '''closure''' cl(''X'') of a set ''X'' is the smallest closed set containing ''X''. Such families of \"closed sets\" are sometimes called \"'''Moore families'''\", in honor of [[E. H. Moore]] who studied closure operators in his 1910 ''Introduction to a form of general analysis'', whereas the concept of the closure of a subset originated in the work of [[Frigyes Riesz]] in connection with topological spaces.<ref>Blyth p.11</ref>\n\nClosure operators are also called \"'''hull operators'''\", which prevents confusion with the \"closure operators\" studied in [[point-set topology|topology]]. A set together with a closure operator on it is sometimes called a '''closure space'''.\n\n== Applications ==\n\nClosure operators have many applications: \n\nIn topology, the closure operators are [[Kuratowski closure axioms|''topological'' closure operator]]s, which must satisfy\n\n: <math>\\operatorname{cl}(X_1 \\cup\\dots\\cup X_n) = \\operatorname{cl}(X_1)\\cup\\dots\\cup \\operatorname{cl}(X_n)</math>\n\nfor all <math>n\\in\\N</math> (Note that for <math>n=0</math> this gives <math>\\operatorname{cl}(\\varnothing)=\\varnothing</math>). \n\nIn [[algebra]] and [[logic]], many closure operators are '''finitary closure operators''', i.e. they satisfy\n\n: <math>\\operatorname{cl}(X) = \\bigcup\\left\\{\\operatorname{cl}(Y) : Y\\subseteq X \\text{ and } Y \\text{ finite} \\right\\}.</math>\n\nIn [[universal logic]], closure operators are also known as '''consequence operators'''. \n\nIn the theory of [[partially ordered set]]s, which are important in [[theoretical computer science]], closure operators have an alternative definition.\n\n== Closure operators in topology ==\n{{main|Kuratowski closure axioms}}\nThe [[topological closure]] of a subset ''X'' of a [[topological space]] consists of all points ''y'' of the space, such that every [[neighbourhood (mathematics)|neighbourhood]] of ''y'' contains a point of ''X''. The function that associates to every subset ''X'' its closure is a topological closure operator. Conversely, every topological closure operator on a set gives rise to a topological space whose closed sets are exactly the closed sets with respect to the closure operator.\n\n== Closure operators in algebra ==\nFinitary closure operators play a relatively prominent role in [[universal algebra]], and in this context they are traditionally called ''algebraic closure operators''. Every subset of an [[structure (mathematical logic)|algebra]] ''generates'' a [[substructure (mathematics)|subalgebra]]: the smallest subalgebra containing the set. This gives rise to a finitary closure operator.\n\nPerhaps the best known example for this is the function that associates to every subset of a given [[vector space]] its [[linear span]]. Similarly, the function that associates to every subset of a given [[group (mathematics)|group]] the [[subgroup]] generated by it, and similarly for [[field (mathematics)|field]]s and all other types of [[algebraic structure]]s.\n\nThe linear span in a vector space and the similar algebraic closure in a field both satisfy the ''exchange property:'' If ''x'' is in the closure of the union of ''A'' and {''y''} but not in the closure of ''A'', then ''y'' is in the closure of the union of ''A'' and {''x''}. A finitary closure operator with this property is called a [[matroid]]. The [[dimension (vector space)|dimension]] of a vector space, or the [[transcendence degree]] of a field (over its [[prime field]]) is exactly the rank of the corresponding matroid.\n\nThe function that maps every subset of a given [[field (mathematics)|field]] to its [[algebraic closure]] is also a finitary closure operator, and in general it is different from the operator mentioned before. Finitary closure operators that generalize these two operators are studied in [[model theory]] as dcl (for ''definable closure'') and acl (for ''algebraic closure'').\n\nThe [[convex hull]] in ''n''-dimensional [[Euclidean space]] is another example of a finitary closure operator. It satisfies the ''anti-exchange property:'' If ''x'' is not contained in the union of ''A'' and {''y''}, but in its closure, then ''y'' is not contained in the closure of the union of ''A'' and {''x''}. Finitary closure operators with this property give rise to [[antimatroid]]s.\n\n== Closure operators in logic ==\nSuppose you have some [[mathematical logic|logical formalism]] that contains certain rules allowing you to derive new formulas from given ones. Consider the set ''F'' of all possible formulas, and let ''P'' be the [[power set]] of ''F'', ordered by ⊆. For a set ''X'' of formulas, let cl(''X'') be the set of all formulas that can be derived from ''X''. Then cl is a closure operator on ''P''. More precisely, we can obtain cl as follows. Call \"continuous\" an operator ''J'' such that, for every directed class ''T'',\n\t\n:''J''(''lim T'')= ''lim J''(''T'').\t\n\nThis continuity condition is on the basis of a fixed point theorem for ''J''. Consider the one-step operator ''J'' of a monotone logic. This is the operator associating any set ''X'' of formulas with the set ''J(X)'' of formulas which are either logical axioms or are obtained by an inference rule from formulas in ''X'' or are in ''X''. Then such an operator is continuous and we can define cl(''X'') as the least fixed point for ''J'' greater or equal to ''X''. In accordance with such a point of view, Tarski, Brown, Suszko and other authors proposed a general approach to logic based on closure operator theory. Also, such an idea is proposed in programming logic (see Lloyd 1987) and in fuzzy logic (see Gerla 2000).\n\n=== Consequence operators ===\nAround 1930, [[Alfred Tarski]] developed an abstract theory of logical deductions which models some properties of logical calculi. Mathematically, what he described is just a finitary closure operator on a set (the set of ''sentences'').  In [[universal logic]] and [[Abstract algebraic logic]], finitary closure operators are still studied under the name ''consequence operator'', which was coined by Tarski. The set ''S'' represents a set of sentences, a subset ''T'' of ''S'' a theory, and cl(''T'') is the set of all sentences that follow from the theory. Nowadays the term can refer to closure operators which need not be finitary; finitary closure operators are then sometimes called '''finite consequence operators'''.\n\n== Closed and pseudo-closed sets ==\nThe closed sets with respect to a closure operator on ''S'' form a subset ''C'' of the power set '''''P'''''(''S''). Any intersection of sets in ''C'' is again in ''C''. In other words, ''C'' is a complete meet-subsemilattice of '''''P'''''(''S''). Conversely, if ''C'' ⊆ '''''P'''''(''S'') is closed under arbitrary intersections, then the function that associates to every subset ''X'' of ''S'' the smallest set ''Y'' ∈ ''C'' such that ''X'' ⊆ ''Y'' is a closure operator.\n\nThere is a simple and fast algorithm for generating all closed sets of a given closure operator<ref> Ganter, Algorithm 1</ref>.\n\nA closure operator on a set is topological if and only if the set of closed sets is closed under finite unions, i.e., ''C'' is a meet-complete sublattice of '''''P'''''(''S''). Even for non-topological closure operators, ''C'' can be seen as having the structure of a lattice. (The join of two sets ''X'',''Y'' ⊆ '''''P'''''(''S'') being cl(''X'' <math>\\cup</math> ''Y'').) But then ''C'' is not a [[sublattice]] of the lattice '''''P'''''(''S'').\n\nGiven a finitary closure operator on a set, the closures of finite sets are exactly the [[compact element]]s of the set ''C'' of closed sets. It follows that ''C'' is an [[algebraic poset]].\nSince ''C'' is also a lattice, it is often referred to as an algebraic lattice in this context. Conversely, if ''C'' is an algebraic poset, then the closure operator is finitary.\n\nEach closure operator on a finite set ''S'' is uniquely determined by its images of its ''pseudo-closed'' sets<ref> Ganter, Section 3.2</ref>.\nThese are recursively defined: A set is '''pseudo-closed''' if it is not closed and contains the closure of each of its pseudo-closed proper subsets. Formally: ''P''&sube;''S'' is pseudo-closed if and only if\n* ''P''&ne;cl(''P'') and \n* if ''Q''&sub;''P'' is pseudo-closed, then cl(''Q'')&sube;''P''.\n\n== Closure operators on partially ordered sets ==\nA [[partially ordered set]] (poset) is a set together with a ''partial order'' ≤, i.e. a binary relation which is reflexive ({{nowrap|''a'' ≤ ''a''}}), transitive ({{nowrap|''a'' ≤ ''b'' ≤ ''c''}} implies {{nowrap|''a'' ≤ ''c''}}) and antisymmetric ({{nowrap|''a'' ≤ ''b'' ≤ ''a''}} implies ''a''&nbsp;=&nbsp;''b''). Every [[power set]] '''P'''(''S'') together with inclusion ⊆ is a partially ordered set.\n\nA function cl: ''P'' → ''P'' from a partial order ''P'' to itself is called a closure operator if it satisfies the following axioms for all elements ''x'', ''y'' in ''P''.\n:{| border=\"0\"\n|-\n| ''x'' ≤ cl(''x'')\n| (cl is ''extensive'')\n|-\n| ''x'' ≤ ''y'' implies cl(''x'') ≤ cl(''y'')&nbsp;&nbsp;\n| (cl is [[increasing]])\n|-\n| cl(cl(''x'')) = cl(''x'')\n| (cl is [[idempotent]])\n|}\n\nMore succinct alternatives are available: the definition above is equivalent to the single axiom\n\n:''x'' ≤ cl(''y'') if and only if cl(''x'') ≤ cl(''y'')\n\nfor all ''x'', ''y'' in ''P''.\n\nUsing the [[pointwise order]] on functions between posets, one may alternatively write the extensiveness property as id<sub>''P''</sub> ≤ cl, where id is the [[identity function]]. A self-map ''k'' that is increasing and idempotent, but satisfies the [[Duality (order theory)|dual]] of the extensiveness property, i.e. ''k'' ≤ id<sub>''P''</sub> is called a '''kernel operator''',<ref>Giertz, p. 26</ref> '''interior operator''',<ref>Erné, p. 2, uses closure (resp. interior) operation</ref> or '''dual closure'''.<ref>Blyth, p. 10</ref> As examples, if ''A'' is a subset of a set ''B'', then the self-map on the powerset of ''B'' given by ''μ<sub>A</sub>''(''X'') = ''A'' ∪ ''X'' is a closure operator, whereas ''λ<sub>A</sub>''(''X'') = ''A'' ∩ ''X'' is a kernel operator. The [[ceiling function]] from the [[real number]]s to the real numbers, which assigns to every real ''x'' the smallest [[integer]] not smaller than ''x'', is another example of a closure operator.\n\nA [[fixpoint]] of the function cl, i.e. an element ''c'' of ''P'' that satisfies cl(''c'')&nbsp;=&nbsp;''c'', is called a '''closed element'''. A closure operator on a partially ordered set is determined by its closed elements. If ''c'' is a closed element, then ''x'' ≤ ''c'' and cl(''x'') ≤ ''c'' are equivalent conditions.\n\nEvery [[Galois connection]] (or [[residuated mapping]]) gives rise to a closure operator (as is explained in that article). In fact, ''every'' closure operator arises in this way from a suitable Galois connection.<ref>Blyth, p. 10</ref> The Galois connection is not uniquely determined by the closure operator. One Galois connection that gives rise to the closure operator cl can be described as follows: if ''A'' is the set of closed elements with respect to cl, then cl: ''P'' → ''A'' is the lower adjoint of a Galois connection between ''P'' and ''A'', with the upper adjoint being the embedding of ''A'' into ''P''. Furthermore, every lower adjoint of an embedding of some subset into ''P'' is a closure operator. \"Closure operators are lower adjoints of embeddings.\" Note however that not every embedding has a lower adjoint.\n\nAny partially ordered set ''P'' can be viewed as a [[category theory|category]], with a single morphism from ''x'' to ''y'' if and only if ''x'' ≤ ''y''. The closure operators on the partially ordered set ''P'' are then nothing but the [[monad (category theory)|monad]]s on the category ''P''. Equivalently, a closure operator can be viewed as an endofunctor on the category of partially ordered sets that has the additional ''idempotent'' and ''extensive'' properties.\n\nIf ''P'' is a [[complete lattice]], then a subset ''A'' of ''P'' is the set of closed elements for some closure operator on ''P'' if and only if ''A'' is a '''Moore family''' on ''P'', i.e. the largest element of ''P'' is in ''A'', and the [[infimum]] (meet) of any non-empty subset of ''A'' is again in ''A''. Any such set ''A'' is itself a complete lattice with the order inherited from ''P'' (but the [[supremum]] (join) operation might differ from that of ''P''). When ''P'' is the [[powerset]] Boolean algebra of a set ''X'', then a Moore family on ''P'' is called a '''closure system''' on ''X''.\n\nThe closure operators on ''P'' form themselves a complete lattice; the order on closure operators is defined by cl<sub>1</sub> ≤ cl<sub>2</sub> [[iff]] cl<sub>1</sub>(''x'') ≤ cl<sub>2</sub>(''x'') for all ''x'' in ''P''.\n\n==See also==\n*[[Čech closure operator]]\n*[[Galois connection]]\n*[[Interior algebra]]\n*[[Kuratowski closure axioms]]\n*[[Closure (topology)]] and [[Interior (topology)]]\n\n== Notes ==\n{{reflist|3}}\n\n== References ==\n* [[Garrett Birkhoff]]. 1967 (1940). ''Lattice Theory, 3rd ed''. American Mathematical Society.\n* Burris, Stanley N., and H.P. Sankappanavar (1981) [http://www.thoralf.uwaterloo.ca/htdocs/ualg.html A Course in Universal Algebra]  Springer-Verlag. {{ISBN|3-540-90578-2}} ''Free online edition''.\n* Brown, D.J. and Suszko, R. (1973) \"Abstract Logics,\" [[Dissertationes Mathematicae]] 102- 9-42.\n* Castellini, G. (2003) ''Categorical closure operators''. Boston MA: Birkhaeuser.\n* Edelman, Paul H. (1980) ''Meet-distributive lattices and the anti-exchange closure,'' [[Algebra Universalis]] 10: 290-299.\n* Ganter, Bernhard and Obiedkov, Sergei (2016) ''Conceptual Exploration''. Springer, {{ISBN|978-3-662-49290-1}}.\n* Gerla, G. (2000) ''Fuzzy Logic: Mathematical Tools for Approximate Reasoning''. [[Kluwer Academic Publishers]].\n* Lloyd, J.W. (1987) ''Foundations of Logic Programming''. [[Springer-Verlag]].\n* [[Alfred Tarski|Tarski, Alfred]] (1983) \"Fundamental concepts of the methodology of deductive sciences\" in ''Logic, Semantics, Metamathematics''. Hackett (1956 ed., [[Oxford University Press]]).\n* [[Alfred Tarski]] (1956) ''Logic, semantics and metamathematics''. [[Oxford University Press]].\n* [[Morgan Ward|Ward, Morgan]] (1942) \"The closure operators of a lattice,\" [[Annals of Mathematics]] 43: 191-96.\n* G. Gierz, K. H. Hofmann, K. Keimel, J. D. Lawson, M. Mislove, D. S. Scott: ''Continuous Lattices and Domains'', Cambridge University Press, 2003\n* T.S. Blyth, ''Lattices and Ordered Algebraic Structures'', Springer, 2005, {{ISBN|1-85233-905-5}}.\n* M. Erné, J. Koslowski, A. Melton, G. E. Strecker, ''A primer on Galois connections'', in: Proceedings of the 1991 Summer Conference on General Topology and Applications in Honor of Mary Ellen Rudin and Her Work, Annals of the New York Academy of Sciences, Vol. 704, 1993, pp.&nbsp;103–125. Available online in various file formats: [https://web.archive.org/web/20060108063506/http://www.iti.cs.tu-bs.de/TI-INFO/koslowj/RESEARCH/gal_bw.ps.gz PS.GZ] [http://www.math.ksu.edu/~strecker/primer.ps PS]\n\n==External links==\n*[[Stanford Encyclopedia of Philosophy]]: \"[http://plato.stanford.edu/entries/consequence-algebraic/ Propositional Consequence Relations and Algebraic Logic]\" -- by Ramon Jansana.\n\n{{DEFAULTSORT:Closure Operator}}\n[[Category:Closure operators|*]]\n[[Category:Universal algebra]]\n[[Category:Order theory]]\n\n[[pl:Operator konsekwencji]]"
    },
    {
      "title": "Cofinal (mathematics)",
      "url": "https://en.wikipedia.org/wiki/Cofinal_%28mathematics%29",
      "text": "In [[mathematics]], let ''A'' be a set and let ≤ be a [[binary relation]] on ''A''.  Then a [[subset]] ''B'' of ''A'' is said to be '''cofinal''' if it satisfies the following condition:\n:For every ''a''&nbsp;&isin;&nbsp;''A'', there exists some ''b''&nbsp;&isin;&nbsp;''B'' such that ''a''&nbsp;&le;&nbsp;''b''.\nThis definition is most commonly applied when ''A'' is a [[partially ordered set]] or [[directed set]] under the relation ≤.\n\nCofinal subsets are very important in the theory of directed sets and [[net (mathematics)|nets]], where “[[subnet (mathematics)|cofinal subnet]]” is the appropriate generalization of “[[subsequence]]”.  They are also important in [[order theory]], including the theory of [[cardinal numbers]], where the minimum possible [[cardinality]] of a cofinal subset of ''A'' is referred to as the [[cofinality]] of ''A''.\n\nA subset ''B'' of ''A'' is said to be '''coinitial''' (or '''dense''' in the sense of [[Forcing (mathematics)|forcing]]) if it satisfies the following condition:\n:For every ''a''&nbsp;&isin;&nbsp;''A'', there exists some ''b''&nbsp;&isin;&nbsp;''B'' such that ''b''&nbsp;&le;&nbsp;''a''.\nThis is the [[Duality (order theory)|order-theoretic dual]] to the notion of cofinal subset.\n\nNote that cofinal and coinitial subsets are both dense in the sense of appropriate (right- or left-) [[order topology]].\n\n== Properties ==\n\nThe cofinal relation over partially ordered sets (\"[[Partially ordered set|poset]]\") is [[reflexive relation|reflexive]]: every poset is cofinal in itself. It is also [[transitive relation|transitive]]: if ''B'' is a cofinal subset of a poset ''A'', and ''C'' is a cofinal subset of ''B'' (with the partial ordering of ''A'' applied to ''B''), then ''C'' is also a cofinal subset of ''A''.\n\nFor a partially ordered set with [[maximal element]]s, every cofinal subset must contain all [[maximal element]]s, otherwise a maximal element which is not in the subset would fail to be ''less than'' any element of the subset, violating the definition of cofinal. For a partially ordered set with a [[greatest element]], a subset is cofinal if and only if it contains that greatest element (this follows, since a greatest element is necessarily a maximal element). Partially ordered sets without greatest element or maximal elements admit disjoint cofinal subsets. For example, the even and odd [[natural number]]s form disjoint cofinal subsets of the set of all natural numbers.\n\nIf a partially ordered set ''A'' admits a [[totally ordered]] cofinal subset, then we can find a subset ''B'' which is [[well-ordered]] and cofinal in ''A''.\n\n== Cofinal set of subsets ==\n\nA particular but important case is given if ''A'' is a subset of the [[power set]] ''P''(''E'') of some set ''E'', ordered by reverse inclusion (⊃). Given this ordering of ''A'', a subset ''B'' of ''A'' is cofinal in ''A'' if for every ''a''&nbsp;∈&nbsp;''A'' there is a ''b''&nbsp;∈&nbsp;''B'' such that ''a''&nbsp;⊃&nbsp;''b''.\n\nFor example, let ''E'' be a group and let ''A'' be the set of [[normal subgroup]]s of finite [[index of a subgroup|index]]. The [[profinite group|profinite completion]] of ''E'' is defined to be the [[inverse limit]] of the [[inverse system]] of finite quotients of ''E'' (which are parametrized by the set ''A''). \nIn this situation, every cofinal subset of ''A'' is sufficient to construct and describe the profinite completion of ''E''.\n\n== Related Notions ==\n\nA [[function (mathematics)|map]] f:&nbsp;''X''&nbsp;→&nbsp;''A'' between two directed sets is said to be '''final''' <ref name=bredon93>{{cite book|last=Bredon|first=Glen|author-link=Glen Bredon|title=Topology and Geometry|year=1993|publisher=Springer|page=16}}</ref> if the [[range (mathematics)|range]] f(''X'') of f is a cofinal subset of ''A''.\n\n== See also ==\n* [[cofinite]]\n* [[cofinality]]\n* [[Upper set]] – a subset ''U'' of a partially ordered set (''P'',≤) that contains every element ''y'' of ''P'' for which there is an ''x'' in ''U'' with ''x'' ≤ ''y''\n\n==References==\n\n{{reflist}}\n\n* {{Lang Algebra|edition=3}}\n\n[[Category:Order theory]]"
    },
    {
      "title": "Cofinality",
      "url": "https://en.wikipedia.org/wiki/Cofinality",
      "text": "{{Distinguish|cofiniteness}}\nIn [[mathematics]], especially in [[order theory]], the '''cofinality''' cf(''A'') of a [[partially ordered set]] ''A'' is the least of the [[cardinality|cardinalities]] of the [[cofinal (mathematics)|cofinal]] subsets of ''A''.\n\nThis definition of cofinality relies on the [[axiom of choice]], as it uses the fact that every non-empty set of [[cardinal number]]s has a least member. The cofinality of a partially ordered set ''A'' can alternatively be defined as the least [[ordinal number|ordinal]] ''x'' such that there is a function from ''x'' to ''A'' with cofinal [[Image (mathematics)|image]]. This second definition makes sense without the axiom of choice. If the axiom of choice is assumed, as will be the case in the rest of this article, then the two definitions are equivalent.\n\nCofinality can be similarly defined for a [[directed set]] and is used to generalize the notion of a [[subsequence]] in a [[Net (mathematics)|net]].\n\n== Examples ==\n*  The cofinality of a partially ordered set with [[greatest element]] is 1 as the set consisting only of the greatest element is cofinal (and must be contained in every other cofinal subset).\n** In particular, the cofinality of any nonzero finite ordinal, or indeed any finite directed set, is 1, since such sets have a greatest element.\n* Every cofinal subset of a partially ordered set must contain all [[maximal element]]s of that set. Thus the cofinality of a finite partially ordered set is equal to the number of its maximal elements.\n** In particular, let ''A'' be a set of size ''n'', and consider the set of subsets of ''A'' containing no more than ''m'' elements.  This is partially ordered under inclusion and the subsets with ''m'' elements are maximal.  Thus the cofinality of this poset is ''n'' [[Binomial coefficient|choose]] ''m''.\n* A subset of the natural numbers '''N''' is cofinal in '''N''' if and only if it is infinite, and therefore the cofinality of &alefsym;<sub>0</sub> is &alefsym;<sub>0</sub>. Thus &alefsym;<sub>0</sub> is a [[regular cardinal]].\n* The cofinality of the [[real number]]s with their usual ordering is &alefsym;<sub>0</sub>, since '''N''' is cofinal in '''R'''.  The usual ordering of '''R''' is not [[order isomorphic]] to ''c'', the [[cardinality of the continuum|cardinality of the real numbers]], which has cofinality strictly greater than &alefsym;<sub>0</sub>.  This demonstrates that the cofinality depends on the order; different orders on the same set may have different cofinality.\n\n==Properties==\nIf ''A'' admits a [[total order|totally ordered]] cofinal subset, then we can find a subset ''B'' which is well-ordered and cofinal in ''A''. Any subset of ''B'' is also well-ordered. If two cofinal subsets of ''B'' have minimal cardinality (i.e. their cardinality is the cofinality of ''B''), then they are order isomorphic to each other.\n\n== Cofinality of ordinals and other well-ordered sets ==\nThe '''cofinality of an ordinal''' &alpha; is the smallest ordinal &delta; which is the [[order type]] of a [[cofinal subset]] of &alpha;.  The cofinality of a set of ordinals or any other [[well-ordered set]] is the cofinality of the order type of that set.\n\nThus for a [[limit ordinal]] &alpha;, there exists a &delta;-indexed strictly increasing sequence with limit &alpha;.  For example, the cofinality of ω² is ω, because the sequence ω·''m'' (where ''m'' ranges over the natural numbers) tends to ω²; but, more generally, any countable limit ordinal has cofinality ω.  An uncountable limit ordinal may have either cofinality ω as does &omega;<sub>&omega;</sub> or an uncountable cofinality.\n\nThe cofinality of 0 is 0. The cofinality of any [[successor ordinal]] is 1.  The cofinality of any nonzero limit ordinal is an infinite regular cardinal.\n\n== Regular and singular ordinals ==\n{{Main|Regular cardinal}}\nA '''regular ordinal''' is an ordinal which is equal to its cofinality. A '''singular ordinal''' is any ordinal which is not regular.\n\nEvery regular ordinal is the [[initial ordinal]] of a cardinal.  Any limit of regular ordinals is a limit of initial ordinals and thus is also initial but need not be regular. Assuming the axiom of choice, <math>\\omega_{\\alpha+1}</math> is regular for each α.  In this case, the ordinals 0, 1, <math>\\omega</math>, <math>\\omega_1</math>, and <math>\\omega_2</math> are regular, whereas 2, 3, <math>\\omega_\\omega</math>, and ω<sub>ω·2</sub> are initial ordinals which are not regular.\n\nThe cofinality of any ordinal ''α'' is a regular ordinal, i.e. the cofinality of the cofinality of ''α'' is the same as the cofinality of ''α''.  So the cofinality operation is [[idempotent]].\n\n==Cofinality of cardinals==\nIf κ is an infinite cardinal number, then cf(κ) is the least cardinal such that there is an [[bounded (set theory)|unbounded]] function from cf(κ) to κ; cf(κ) is also the cardinality of the smallest set of strictly smaller cardinals whose sum is κ; more precisely\n\n:<math>\\mathrm{cf}(\\kappa) = \\min \\left\\{ \\mathrm{card}(I)\\ |\\ \\kappa = \\sum_{i \\in I} \\lambda_i\\ \\mathrm{and}\\ (\\forall i)(\\lambda_i < \\kappa)\\right\\}</math>\n\nThat the set above is nonempty comes from the fact that\n\n:<math>\\kappa = \\bigcup_{i \\in \\kappa} \\{i\\}</math>\n\ni.e. the [[disjoint union]] of κ singleton sets. This implies immediately that cf(κ) ≤ κ.\nThe cofinality of any totally ordered set is regular, so one has cf(κ) = cf(cf(κ)).\n\nUsing [[König's theorem (set theory)|König's theorem]], one can prove κ < κ<sup>cf(κ)</sup> and κ < cf(2<sup>κ</sup>) for any infinite cardinal κ.\n\nThe last inequality implies that the cofinality of the cardinality of the continuum must be uncountable. On the other hand,\n\n:<math> \\aleph_\\omega = \\bigcup_{n < \\omega} \\aleph_n </math>.\n\nthe ordinal number ω being the first infinite ordinal, so that the cofinality of <math>\\aleph_\\omega</math> is card(ω) = <math>\\aleph_0</math>.  (In particular, <math>\\aleph_\\omega</math> is singular.) Therefore,\n\n:<math>2^{\\aleph_0}\\neq\\aleph_\\omega.</math>\n\n(Compare to the [[continuum hypothesis]], which states <math>2^{\\aleph_0}= \\aleph_1</math>.)\n\nGeneralizing this argument, one can prove that for a limit ordinal δ\n\n:<math>\\mathrm{cf} (\\aleph_\\delta) = \\mathrm{cf} (\\delta) </math>.\n\nOn the other hand, if the [[axiom of choice]] holds, then for a successor or zero ordinal δ\n\n:<math>\\mathrm{cf} (\\aleph_\\delta) = \\aleph_\\delta </math>.\n\n==See also==\n*[[Initial ordinal]]\n\n==References==\n*Jech, Thomas, 2003. ''Set Theory: The Third Millennium Edition, Revised and Expanded''.  Springer.  {{ISBN|3-540-44085-2}}.\n*Kunen, Kenneth, 1980. ''Set Theory: An Introduction to Independence Proofs''. Elsevier.  {{ISBN|0-444-86839-9}}.\n\n[[Category:Order theory]]\n[[Category:Set theory]]\n[[Category:Ordinal numbers]]\n[[Category:Cardinal numbers]]"
    },
    {
      "title": "Compact element",
      "url": "https://en.wikipedia.org/wiki/Compact_element",
      "text": "{{Unreferenced|date=December 2008}}\n\nIn the [[mathematics|mathematical]] area of [[order theory]], the '''compact''' or '''finite elements''' of a [[partially ordered set]] are those elements that  cannot be subsumed by a [[supremum]] of any [[non-empty]] [[directed set]] that does not already contain members above the compact element.\n\nNote that there are other notions of [[compactness]] in mathematics; also, the term \"[[wiktionary:finite|finite]]\" in its normal [[set theory|set theoretic]] meaning does not coincide with the order-theoretic notion of a \"finite element\".\n\n== Formal definition ==\n\nIn a partially ordered set (''P'',≤) an element ''c'' is called ''compact'' (or ''finite'') if it satisfies one of the following equivalent conditions:\n* For every [[directed set|directed subset]] ''D'' of ''P'', if ''D'' has a supremum sup ''D'' and ''c'' ≤ sup ''D'' then ''c'' ≤ ''d'' for some element ''d'' of ''D''.\n* For every [[ideal (order theory)|ideal]] ''I'' of ''P'', if ''I'' has a supremum sup ''I'' and ''c'' ≤ sup ''I'' then ''c'' is an element of ''I''.\n\nIf the poset ''P'' additionally is a [[semilattice|join-semilattice]] (i.e., if it has binary suprema) then these conditions are equivalent to the following statement:\n* For every nonempty subset ''S'' of ''P'', if ''S'' has a supremum sup ''S'' and ''c'' ≤ sup ''S'', then ''c'' ≤ sup ''T'' for some finite subset ''T'' of ''S''.\nIn particular, if ''c'' = sup ''S'', then ''c'' is the supremum of a finite subset of ''S''.\n\nThese equivalences are easily verified from the definitions of the concepts involved. For the case of a join-semilattice note that any set can be turned into a directed set with the same supremum by closing under finite (non-empty) suprema.\n\nWhen considering [[directed complete partial order]]s or [[complete lattice]]s the additional requirements that the specified suprema exist can of course be dropped. Note also that a join-semilattice which is directed complete is almost a complete lattice (possibly lacking a [[least element]])—see [[completeness (order theory)]] for details.\n\nIf it exists, the least element of a poset is always compact. It may be that this is the only compact element, as the example of the [[real number|real]] unit interval [0,1] shows.\n\n== Examples ==\n\n* The most basic example is obtained by considering the [[power set]] of some set, ordered by [[subset|subset inclusion]]. Within this complete lattice, the compact elements are exactly the [[finite set]]s. This justifies the name \"finite element\".\n* The term \"compact\" is explained by considering the complete lattices of [[open set]]s of some [[topological space]], also ordered by [[subset|subset inclusion]]. Within this order, the compact elements are just the [[compact set]]s. Indeed, the condition for compactness in join-semilattices translates immediately to the corresponding definition.\n\n==Algebraic posets==\n\nA poset in which every element is the supremum of the compact elements below it is called an ''algebraic poset''. Such posets which are [[Directed complete partial order|dcpo]]s are much used in [[domain theory]].\n\nAs an important special case, an ''algebraic lattice'' is a  [[complete lattice]] ''L'', such that every element ''x'' of ''L'' is the supremum of the compact elements below ''x''.\n\nA typical example (which served as the motivation for the name \"algebraic\") is the following:\n\nFor any algebra ''A'' (for example, a group, a ring, a field, a lattice, etc.; or even a mere set without any operations), let Sub(''A'') be the set of all substructures of ''A'', i.e., of all subsets of ''A'' which are closed under all operations of ''A'' (group addition, ring addition and multiplication, etc.).  Here the notion of substructure includes the empty substructure in case the algebra ''A'' has no nullary operations.\n\nThen:\n* The set Sub(''A''), ordered by set inclusion, is a lattice. \n* The greatest element of Sub(''A'') is the set ''A'' itself. \n* For any ''S'', ''T'' in Sub(''A''), the greatest lower bound of ''S'' and ''T'' is the set theoretic intersection of ''S'' and ''T''; the smallest upper bound is the subalgebra generated by the union of ''S'' and ''T''.\n* The set Sub(''A'') is even a complete lattice. The greatest lower bound of any family of substructures is their intersection. \n* The compact elements of Sub(''A'') are exactly the finitely generated substructures of ''A''. \n* Every substructure is the union of its finitely generated substructures; hence Sub(''A'') is an algebraic lattice.\n\nAlso, a kind of converse holds:  Every algebraic lattice is isomorphic to Sub(''A'') for some algebra ''A''.\n\nThere is another algebraic lattice which plays an important role in [[universal algebra]]:  For every algebra ''A'' \nwe let Con(''A'') be the set of all [[congruence relation]]s on ''A''.   Each congruence on ''A'' is a subalgebra of the product algebra ''A''x''A'', so Con(''A'') ⊆ Sub(''A''x''A'').   Again we have\n* Con(''A''), ordered by set inclusion, is a lattice. \n* The greatest element of Con(''A'') is the set ''A''x''A'', which is the congruence corresponding to the constant homomorphism. The smallest congruence is the diagonal of ''A''x''A'', corresponding to isomorphisms.\n* Con(''A'') is a complete lattice. \n* The compact elements of Con(''A'') are exactly the finitely generated congruences. \n* Con(''A'') is an algebraic lattice.\n\nAgain there is a converse:  By a theorem of [[George Grätzer]] and E. T. Schmidt, every algebraic lattice is isomorphic to Con(''A'') for some algebra ''A''.\n\n== Applications ==\n\nCompact elements are important in [[computer science]] in the semantic approach called [[domain theory]], where they are considered as a kind of primitive element: the information represented by compact elements cannot be obtained by any approximation that does not already contain this knowledge. Compact elements cannot be approximated by elements strictly below them. On the other hand, it may happen that all non-compact elements can be obtained as directed suprema of compact elements. This is a desirable situation, since the set of compact elements is often smaller than the original poset &ndash; the examples above illustrate this.\n\n== Literature ==\n\nSee the literature given for [[order theory]] and [[domain theory]].\n\n[[Category:Order theory]]"
    },
    {
      "title": "Comparability graph",
      "url": "https://en.wikipedia.org/wiki/Comparability_graph",
      "text": "In [[graph theory]], a '''comparability graph''' is an [[undirected graph]] that connects pairs of elements that are [[comparability|comparable]] to each other in a [[partial order]]. Comparability graphs have also been called '''transitively orientable graphs''', '''partially orderable graphs''', '''containment graphs''',<ref>{{harvtxt|Golumbic|1980}}, p. 105; {{harvtxt|Brandstädt|Le|Spinrad|1999}}, p. 94.</ref> and '''divisor graphs'''.{{sfnp|Chartrand|Muntean|Saenpholphat|Zhang|2001}}\nAn '''incomparability graph''' is an [[undirected graph]] that connects pairs of elements that are not [[comparability|comparable]] to each other in a [[partial order]].\n\n==Definitions and characterization==\n[[File:Poset et graphe de comparabilité.svg|thumb|300px|Hasse diagram of a poset (left) and its comparability graph (right)]]\n[[Image:Forbidden interval subgraph.svg|thumb|One of the forbidden induced subgraphs of a comparability graph. The generalized cycle \n''a''–''b''–''d''–''f''–''d''–''c''–''e''–''c''–''b''–''a'' in this graph has odd length (nine) but has no triangular chords.]]\nFor any [[strict partial order|strict partially ordered set]] (''S'',&lt;), the '''comparability graph''' of (''S'', &lt;) is the graph (''S'', ⊥) of which the vertices are the elements of ''S'' and the edges are those pairs {''u'', ''v''} of elements such that ''u'' &lt; ''v''.  That is, for a partially ordered set, take the [[directed acyclic graph]], apply [[transitive closure]], and remove orientation. \n\nEquivalently, a comparability graph is a graph that has a '''transitive orientation''',<ref>{{harvtxt|Ghouila-Houri|1962}}; see {{harvtxt|Brandstädt|Le|Spinrad|1999}}, theorem 1.4.1, p. 12. Although the orientations coming from partial orders are [[directed acyclic graph|acyclic]], it is not necessary to include acyclicity as a condition of this characterization.</ref> an assignment of directions to the edges of the graph (i.e. an [[Orientation (graph theory)|orientation]] of the graph) such that the [[adjacency relation]] of the resulting [[directed graph]] is [[transitive relation|transitive]]: whenever there exist directed edges (''x'',''y'') and (''y'',''z''), there must exist an edge (''x'',''z'').\n\nOne can represent any finite partial order as a family of sets, such that ''x'' &lt; ''y'' in the partial order whenever the set corresponding to ''x'' is a subset of the set corresponding to ''y''. In this way, comparability graphs can be shown to be equivalent to containment graphs of set families; that is, a graph with a vertex for each set in the family and an edge between two sets whenever one is a subset of the other.<ref>{{harvtxt|Urrutia|1989}}; {{harvtxt|Trotter|1992}}; {{harvtxt|Brandstädt|Le|Spinrad|1999}}, section 6.3, pp. 94–96.</ref>\nAlternatively, one can represent the partial order by a family of [[integer]]s, such that ''x'' &lt; ''y'' whenever the integer corresponding to ''x'' is a [[divisor]] of the integer corresponding to ''y''. Because of this construction, comparability graphs have also been called divisor graphs.{{sfnp|Chartrand|Muntean|Saenpholphat|Zhang|2001}}\n\nComparability graphs can be characterized as the graphs such that, for every ''generalized cycle'' of odd length, one can find an edge (''x'',''y'') connecting two vertices that are at distance two in the cycle. Such an edge is called a ''triangular chord''. In this context, a generalized cycle is defined to be a [[Glossary of graph theory#Walks|closed walk]] that uses each edge of the graph at most once in each direction.<ref>{{harvtxt|Ghouila-Houri|1962}} and {{harvtxt|Gilmore|Hoffman|1964}}. See also {{harvtxt|Brandstädt|Le|Spinrad|1999}}, theorem 6.1.1, p. 91.</ref> Comparability graphs can also be characterized by a list of [[forbidden induced subgraph]]s.<ref>{{harvtxt|Gallai|1967}}; {{harvtxt|Trotter|1992}}; {{harvtxt|Brandstädt|Le|Spinrad|1999}}, p. 91 and p. 112.</ref>\n\n==Relation to other graph families==\nEvery [[complete graph]] is a comparability graph, the comparability graph of a [[total order]]. All acyclic orientations of a complete graph are transitive. Every [[bipartite graph]] is also a comparability graph. Orienting the edges of a bipartite graph from one side of the bipartition to the other results in a transitive orientation, corresponding to a partial order of height two. As {{harvtxt|Seymour|2006}} observes, every comparability graph that is neither complete nor bipartite has a [[skew partition]].\n\nThe [[complement (graph theory)|complement]] of any [[interval graph]] is a comparability graph. The comparability relation is called an [[interval order]]. Interval graphs are exactly the graphs that are chordal and that have comparability graph complements.<ref>Transitive orientability of interval graph complements was proven by {{harvtxt|Ghouila-Houri|1962}}; the characterization of interval graphs is due to {{harvtxt|Gilmore|Hoffman|1964}}. See also {{harvtxt|Golumbic|1980}}, prop. 1.3, pp. 15–16.</ref>\n\nA [[permutation graph]] is a containment graph on a set of intervals.<ref>{{harvtxt|Dushnik|Miller|1941}}. {{harvtxt|Brandstädt|Le|Spinrad|1999}}, theorem 6.3.1, p. 95.</ref> Therefore, permutation graphs are another subclass of comparability graphs.\n\nThe [[trivially perfect graph]]s are the comparability graphs of [[rooted tree]]s.<ref>{{harvtxt|Brandstädt|Le|Spinrad|1999}}, theorem 6.6.1, p. 99.</ref>\n[[Cograph]]s can be characterized as the comparability graphs of [[series-parallel partial order]]s; thus, cographs are also comparability graphs.<ref>{{harvtxt|Brandstädt|Le|Spinrad|1999}}, corollary 6.4.1, p. 96; {{harvtxt|Jung|1978}}.</ref>\n\n[[Threshold graph]]s are another special kind of comparability graph.\n\nEvery comparability graph is [[perfect graph|perfect]]. The perfection of comparability graphs is [[Mirsky's theorem]], and the perfection of their complements is [[Dilworth's theorem]]; these facts, together with the [[perfect graph theorem]] can be used to prove Dilworth's theorem from Mirsky's theorem or vice versa.<ref>{{harvtxt|Golumbic|1980}}, theorems 5.34 and 5.35, p. 133.</ref> More specifically, comparability graphs are [[perfectly orderable graph]]s, a subclass of perfect graphs: a [[greedy coloring]] algorithm for a [[topological ordering]] of a transitive orientation of the graph will optimally color them.<ref>{{harvtxt|Maffray|2003}}.</ref>\n\nThe [[complement graph|complement]] of every comparability graph is a [[string graph]].<ref>{{harvtxt|Golumbic|Rotem|Urrutia|1983}} and {{harvtxt|Lovász|1983}}. See also {{harvtxt|Fox|Pach|2012}}.</ref>\n\n==Algorithms==\nA transitive orientation of a graph, if it exists, can be found in linear time.<ref>{{harvtxt|McConnell|Spinrad|1997}}; see {{harvtxt|Brandstädt|Le|Spinrad|1999}}, p. 91.</ref> However, the algorithm for doing so will assign orientations to the edges of any graph, so to complete the task of testing whether a graph is a comparability graph, one must test whether the resulting orientation is transitive, a problem provably equivalent in complexity to [[matrix multiplication]].\n\nBecause comparability graphs are perfect, many problems that are hard on more general classes of graphs, including [[graph coloring]] and the [[independent set problem]], can be computed in polynomial time for comparability graphs.\n\n==Notes==\n{{reflist|30em}}\n\n== References ==\n{{refbegin|30em}}\n*{{citation\n | last1 = Brandstädt | first1 = Andreas | author1-link = Andreas Brandstädt\n | last2 = Le | first2 = Van Bang | last3 = Spinrad | first3 = Jeremy\n | title = Graph Classes: A Survey\n | publisher = SIAM Monographs on Discrete Mathematics and Applications\n | year = 1999\n | isbn = 0-89871-432-X}}.\n*{{citation\n | last1 = Chartrand | first1 = Gary | author1-link = Gary Chartrand\n | last2 = Muntean | first2 = Raluca\n | last3 = Saenpholphat | first3 = Varaporn\n | last4 = Zhang | first4 = Ping | author4-link = Ping Zhang (graph theorist)\n | department = Proceedings of the Thirty-second Southeastern International Conference on Combinatorics, Graph Theory and Computing (Baton Rouge, LA, 2001)\n | journal = Congressus Numerantium\n | mr = 1887439\n | pages = 189–200\n | title = Which graphs are divisor graphs?\n | volume = 151\n | year = 2001}}\n*{{citation\n | last1 = Dushnik | first1 = Ben | last2 = Miller | first2 = E. W.\n | title = Partially ordered sets\n | journal = American Journal of Mathematics\n | volume = 63\n | year = 1941\n | pages = 600–610\n | mr = 0004862\n | doi = 10.2307/2371374\n | issue = 3\n | publisher = The Johns Hopkins University Press\n | jstor = 2371374}}.\n*{{citation\n | first1 = J. | last1 = Fox | first2 = J. | last2 = Pach | author2-link = János Pach\n | title = String graphs and incomparability graphs\n | journal = Advances in Mathematics\n | volume = 230\n | issue = 3\n | year = 2012\n | doi = 10.1016/j.aim.2012.03.011\n | url = http://www.renyi.hu/~pach/publications/stringpartial071709.pdf\n | pages = 1381–1401\n }}.\n*{{citation\n | last = Gallai | first = Tibor\n | authorlink=Tibor Gallai\n | title = Transitiv orientierbare Graphen\n | journal = Acta Math. Acad. Sci. Hung.\n | volume = 18\n | year = 1967\n | pages = 25–66\n | mr = 0221974\n | doi = 10.1007/BF02020961}}.\n*{{citation\n | last = Ghouila-Houri | first = Alain\n | title = Caractérisation des graphes non orientés dont on peut orienter les arrêtes de manière à obtenir le graphe d'une relation d'ordre\n | journal = [[Les Comptes rendus de l'Académie des sciences]]\n | volume = 254\n | year = 1962\n | pages = 1370–1371\n | mr =0172275}}.\n*{{citation\n | last1 = Gilmore | first1 = P. C. | last2 = Hoffman | first2 = A. J.\n | title = A characterization of comparability graphs and of interval graphs\n | journal = Canadian Journal of Mathematics\n | volume = 16\n | year = 1964\n | pages = 539–548\n | mr = 0175811\n | doi = 10.4153/CJM-1964-055-5}}.\n*{{citation\n | last = Golumbic | first = Martin Charles | authorlink = Martin Charles Golumbic\n | title = Algorithmic Graph Theory and Perfect Graphs\n | publisher = Academic Press\n | year = 1980\n | isbn = 0-12-289260-7}}.\n*{{citation\n | first1 = M. | last1 = Golumbic | first2 = D. | last2 = Rotem | first3 = J. | last3 = Urrutia | author3-link = Jorge Urrutia Galicia\n | title = Comparability graphs and intersection graphs\n | journal = Discrete Mathematics\n | volume = 43\n | year = 1983\n | pages = 37–46\n | issue = 1\n | doi = 10.1016/0012-365X(83)90019-5}}.\n*{{citation\n | last = Jung | first = H. A.\n | title = On a class of posets and the corresponding comparability graphs\n | journal = Journal of Combinatorial Theory, Series B\n | volume = 24\n | year = 1978\n | issue = 2\n | pages = 125–133\n | mr = 0491356\n | doi = 10.1016/0095-8956(78)90013-8}}.\n*{{citation\n | first = L. | last = Lovász | authorlink = László Lovász\n | contribution = Perfect graphs\n | title = Selected Topics in Graph Theory\n | volume = 2\n | publisher = Academic Press\n | location = London\n | year = 1983\n | pages = 55–87}}.\n*{{citation\n | last = Maffray | first = Frédéric\n | contribution = On the coloration of perfect graphs\n | doi = 10.1007/0-387-22444-0_3\n | editor1-last = Reed | editor1-first = Bruce A. | editor1-link = Bruce Reed (mathematician)\n | editor2-last = Sales | editor2-first = Cláudia L.\n | pages = 65–84\n | publisher = Springer-Verlag\n | series = CMS Books in Mathematics\n | title = Recent Advances in Algorithms and Combinatorics\n | volume = 11\n | year = 2003}}.\n*{{citation\n | last1 = McConnell | first1 = R. M. | last2 = Spinrad | first2 = J.\n | contribution = Linear-time transitive orientation\n | title = 8th ACM-SIAM Symposium on Discrete Algorithms\n | year = 1997\n | pages = 19–25}}.\n*{{citation\n | last = Seymour | first = Paul | author-link = Paul Seymour (mathematician)\n | issue = 109\n | journal = Gazette des Mathématiciens\n | mr = 2245898\n | pages = 69–83\n | title = How the proof of the strong perfect graph conjecture was found\n | url = http://users.encs.concordia.ca/~chvatal/perfect/pds.pdf\n | year = 2006}}.\n*{{citation\n | last = Trotter | first = William T.\n | title = Combinatorics and Partially Ordered Sets — Dimension Theory\n | publisher = Johns Hopkins University Press\n | year = 1992}}.\n*{{citation\n | last = Urrutia | first = Jorge | authorlink = Jorge Urrutia Galicia\n | title = Partial orders and Euclidean geometry\n | booktitle = Algorithms and Order\n | editor = [[Ivan Rival|Rival, I.]]\n | year = 1989\n | publisher = Kluwer Academic Publishers\n | pages = 327–436}}.\n{{refend}}\n\n[[Category:Order theory]]\n[[Category:Graph families]]\n[[Category:Perfect graphs]]"
    },
    {
      "title": "Complete partial order",
      "url": "https://en.wikipedia.org/wiki/Complete_partial_order",
      "text": "In [[mathematics]], the phrase '''complete partial order''' is variously used to refer to at least three similar, but distinct, classes of [[partially ordered set]]s, characterized by particular [[completeness (order theory)|completeness properties]]. Complete partial orders play a central role in [[theoretical computer science]], in [[denotational semantics]] and [[domain theory]].\n\n== Definitions ==\n\nA '''complete partial order''' abbreviated '''cpo''' can, depending on context, refer to any of the following concepts. \n\n* A partially ordered set is a '''directed-complete partial order''' ('''dcpo''') if each of its [[directed set|directed subsets]] has a [[supremum]]. A subset of a partial order is directed if it is non-empty and every pair of elements has an upper bound in the subset. In the literature, dcpos sometimes also appear under the label '''up-complete poset'''.\n\n* A partially ordered set is a '''pointed directed-complete partial order''' if it is a dcpo with a least element. They are sometimes abbreviated '''cppo'''s.\n\n* A partially ordered set is a '''ω-complete partial order''' ('''ω-cpo''')  if it is a poset in which every ω-chain (''x''<sub>1</sub>≤''x''<sub>2</sub>≤''x''<sub>3</sub>≤''x''<sub>4</sub>≤...) has a supremum that belongs to the underlying set of the poset. Every dcpo is an ω-cpo, since every ω-chain is a directed set, but the converse is not true. However, every ω-cpo with a [[domain theory#Bases of domains|basis]] is also a dcpo (with the same basis).<ref>{{Cite book|title=Handbook of Logic in Computer Science, volume 3|vauthors=Abramsky S, Gabbay DM, Maibaum TS|collaboration=|publisher=Clarendon Press|year=1994|isbn=9780198537625|location=Oxford|at=Prop 2.2.14, pp. 20|quote=|via=}}</ref> An ω-cpo (dcpo) with a basis is also called a '''continuous''' ω-cpo (continuous dcpo).\n\nNote that ''complete partial order'' is never used to mean a poset in which ''all'' subsets have suprema; the terminology [[complete lattice]] is used for this concept.\n\nRequiring the existence of directed suprema can be motivated by viewing directed sets as generalized approximation sequences and suprema as ''limits'' of the respective (approximative) computations. This intuition, in the context of denotational semantics, was the motivation behind the development of [[domain theory]].\n\nThe [[duality (order theory)|dual]] notion of a directed complete poset is called a '''filtered complete partial order'''. However, this concept occurs far less frequently in practice, since one usually can work on the dual order explicitly.\n\n== Examples ==\n\n* Every finite poset is directed complete.\n* All [[complete lattice]]s are also directed complete.\n* For any poset, the set of all [[non-empty]] [[filter (mathematics)|filters]], ordered by subset inclusion, is a dcpo. Together with the empty filter it is also pointed. If the order has binary [[join and meet|meets]], then this construction (including the empty filter) actually yields a [[complete lattice]].\n* The set of all [[partial function]]s on some given set ''S'' can be ordered by defining ''f''&nbsp;≤&nbsp;''g'' for functions ''f'' and ''g'' if and only if ''g'' extends ''f'', i.e. if the domain of ''f'' is a subset of the domain of ''g'' and the values of ''f'' and ''g'' agree on all inputs for which both functions are defined. (Equivalently, ''f''&nbsp;≤&nbsp;''g'' if and only if ''f''&nbsp;⊆&nbsp;''g'' where ''f'' and ''g'' are identified with their respective [[graph of a function|graphs]].) This order is a pointed dcpo, where the least element is the nowhere-defined function (with empty domain). In fact, ≤ is also [[bounded complete]]. This example also demonstrates why it is not always natural to have a greatest element.\n* The [[specialization order]] of any [[sober space]] is a dcpo.\n* Let us use the term “[[deductive system]]” as a set of sentences closed under consequence (for defining notion of consequence, let us use e.g. Tarski's algebraic approach<ref name=Tar-BizIg>Tarski, Alfred: Bizonyítás és igazság / Válogatott tanulmányok. Gondolat, Budapest, 1990. (Title means: Proof and truth / Selected papers.)</ref><ref name=BurSan-UnivAlg>[http://www.math.uwaterloo.ca/~snburris/index.html Stanley N. Burris] and H.P. Sankappanavar: [http://www.math.uwaterloo.ca/~snburris/htdocs/ualg.html A Course in Universal Algebra]</ref>). There are interesting theorems which concern a set of deductive systems being a directed-complete partial ordering.<ref name=seqdcpo>See online in p. 24 exercises 5–6 of §5 in [[#_note-BurSan-UnivAlg|BurSan:UnivAlg]]. Or, on paper, see [[#_note-Tar-BizIg|Tar:BizIg]].</ref> Also, a set of deductive systems can be chosen to have a least element in a natural way (so that it can be also a pointed dcpo), because the set of all consequences of the empty set (i.e. “the set of the logically provable / logically valid sentences”) is (1) a deductive system (2) contained by all deductive systems.\n\n== Properties ==\n\nAn ordered set ''P'' is a pointed dcpo if and only if every [[chain (order theory)|chain]] has a supremum in ''P'', i.e., ''P'' is [[chain-complete partial order|chain-complete]]<ref>{{citation\n | last = Markowsky | first = George\n | issue = 1\n | journal = Algebra Universalis\n | mr = 0398913\n | pages = 53–68\n | title = Chain-complete posets and directed sets with applications\n | volume = 6\n | year = 1976\n | doi=10.1007/bf02485815}}.</ref>. Alternatively, an ordered set ''P'' is a pointed dcpo if and only if every [[order-preserving]] self-map of ''P'' has a least [[fixpoint]]. Every set ''S'' can be turned into a pointed dcpo by adding a least element ⊥ and introducing a flat order with ⊥&nbsp;≤&nbsp;''s'' and s&nbsp;≤&nbsp;''s'' for every ''s''&nbsp;∈&nbsp;''S'' and no other order relations.\n\n== Continuous functions and fixpoints ==\n\nA function ''f'' between two dcpos ''P'' and ''Q'' is called '''[[Scott continuity|(Scott) continuous]]''' if it maps directed sets to directed sets while preserving their suprema:\n* <math>f(D) \\subseteq Q</math> is directed for every directed <math>D \\subseteq P</math>.\n* <math>f(\\sup D) = \\sup f(D)</math> for every directed <math>D \\subseteq P</math>.\nNote that every continuous function between dcpos is a [[Monotone function#Monotonicity in order theory|monotone function]]. \nThis notion of continuity is equivalent to the topological continuity induced by the [[Scott topology]].\n\nThe set of all continuous functions between two dcpos ''P'' and ''Q'' is denoted <nowiki>[</nowiki>''P''&nbsp;→&nbsp;''Q''<nowiki>]</nowiki>. Equipped with the pointwise order, this is again a dcpo, and a cpo whenever ''Q'' is a cpo.\nThus the complete partial orders with Scott-continuous maps form a [[cartesian closed category]].<ref name=\"barendregt1984\">[[Henk Barendregt|Barendregt, Henk]], [http://www.elsevier.com/wps/find/bookdescription.cws_home/501727/description#description ''The lambda calculus, its syntax and semantics''], [[North-Holland]] (1984)</ref>\n\nEvery order-preserving self-map ''f'' of a cpo (''P'', ⊥) has a least fixpoint.<ref>See [[Knaster–Tarski theorem]]; The foundations of program verification, 2nd edition, Jacques Loeckx and Kurt Sieber, John Wiley & Sons, {{ISBN|0-471-91282-4}}, Chapter 4; the Knaster-Tarski theorem, formulated over cpo's, is given to prove as exercise 4.3-5 on page 90.</ref> If ''f'' is continuous then this fixpoint is equal to the supremum of the [[Iterated function|iterates]] (⊥, ''f''(⊥), ''f''(''f''(⊥)), … ''f''<sup>''n''</sup>(⊥), …) of ⊥ (see also the [[Kleene fixpoint theorem]]).\n\n== See also ==\n\nDirected completeness relates in various ways to other [[completeness (order theory)|completeness]] notions such as [[chain complete]]ness. Directed completeness alone is quite a basic property that occurs often in other order-theoretic investigations, using for instance [[algebraic poset]]s and the [[Scott topology]].\n\n== Notes ==\n\n<references/>\n\n== References ==\n* {{cite book\n |author1=Davey, B.A. |author2=Priestley, H. A.\n | year = 2002\n | title = '''Introduction to Lattices and Order'''\n | edition = Second\n | publisher = Cambridge University Press\n | isbn = 0-521-78451-4\n}}\n\n{{DEFAULTSORT:Complete Partial Order}}\n[[Category:Order theory]]\n\n[[ru:Частично упорядоченное множество#Полное частично упорядоченное множество]]"
    },
    {
      "title": "Completely distributive lattice",
      "url": "https://en.wikipedia.org/wiki/Completely_distributive_lattice",
      "text": "In the mathematical area of [[order theory]], a '''completely distributive lattice''' is a [[complete lattice]] in which arbitrary [[join (lattice theory)|join]]s [[distributivity (order theory)|distribute]] over arbitrary [[meet (lattice theory)|meet]]s.\n\nFormally, a complete lattice ''L'' is said to be '''completely distributive''' if, for any doubly indexed family \n{''x''<sub>''j'',''k''</sub> | ''j'' in ''J'', ''k'' in ''K''<sub>''j''</sub>} of ''L'', we have\n: <math>\\bigwedge_{j\\in J}\\bigvee_{k\\in K_j} x_{j,k} = \n         \\bigvee_{f\\in F}\\bigwedge_{j\\in J} x_{j,f(j)}</math>\nwhere ''F'' is the set of [[choice function]]s ''f'' choosing for each index ''j'' of ''J'' some index ''f''(''j'') in ''K''<sub>''j''</sub>.<ref name=\"DaveyPriestley\">B. A. Davey and H. A. Priestey, ''Introduction to Lattices and Order'' 2nd Edition, Cambridge University Press, 2002, {{ISBN|0-521-78451-4}}</ref>\n\nComplete distributivity is a self-dual property, i.e. [[Duality (order theory)|dualizing]] the above statement yields the same class of complete lattices.<ref name=\"DaveyPriestley\"/>\n\nWithout the axiom of choice, no complete lattice with more than one element can ever satisfy the above property, as one can just let ''x''<sub>''j'',''k''</sub> equal the top element of ''L'' for all indices ''j'' and ''k'' with all of the sets ''K''<sub>''j''</sub> being nonempty but having no choice function.{{Citation needed|date=March 2015}}\n\n==Alternative characterizations==\n\nVarious different characterizations exist. For example, the following is an equivalent law that avoids the use of choice functions{{Citation needed|date=February 2007}}. For any set ''S'' of sets, we define the set ''S''<sup>#</sup> to be the set of all subsets ''X'' of the complete lattice that have non-empty intersection with all members of ''S''. We then can define complete distributivity via the statement\n\n: <math>\\begin{align}\\bigwedge \\{ \\bigvee Y \\mid Y\\in S\\} = \\bigvee\\{ \\bigwedge Z \\mid Z\\in S^\\# \\}\\end{align}</math>\n\nThe operator ( )<sup>#</sup> might be called the '''crosscut operator'''. This version of complete distributivity only implies the original notion when admitting the [[Axiom of Choice]].\n\n<!-- This isn't valid. See talk.\nHowever, the latter version is always equivalent to the statement:\n\n: <math>\\begin{align}\\bigwedge \\{ \\bigvee Y \\mid Y\\in S\\} = \\bigvee\\bigcap S\\end{align}</math>\n\nfor all sets ''S'' of subsets of a complete lattice.\n-->\n\n==Properties==\nIn addition, it is known that the following statements are equivalent for any complete lattice ''L'':<ref name=\"Raney53\">G. N. Raney, ''A subdirect-union representation for completely distributive complete lattices'', Proceedings of the [[American Mathematical Society]], 4: 518 - 522, 1953.</ref>\n\n* ''L'' is completely distributive.\n* ''L'' can be embedded into a direct product of chains [0,1] by an [[order embedding]] that preserves arbitrary meets and joins.\n* Both ''L'' and its dual order ''L''<sup>op</sup> are [[continuous poset]]s.{{Citation needed|date=January 2016}}\n\nDirect products of [0,1], i.e. sets of all functions from some set ''X'' to [0,1] ordered [[pointwise order|pointwise]], are also called ''cubes''.\n\n==Free completely distributive lattices==<!-- This section is linked from [[Completely distributive lattice]]. See [[WP:MOS#Section management]] -->\nEvery [[partially ordered set|poset]] ''C'' can be [[Complete lattice#Completion|completed]] in a completely distributive lattice.\n\nA completely distributive lattice ''L'' is called the '''free completely distributive lattice over a poset ''C''''' if and only if there is an [[order embedding]] <math>\\phi:C\\rightarrow L</math> such that for every completely distributive lattice ''M'' and [[monotonic function]] <math>f:C\\rightarrow M</math>, there is a unique [[Complete lattice#Morphisms of complete lattices|complete homomorphism]] <math>f^*_\\phi:L\\rightarrow M</math> satisfying <math>f=f^*_\\phi\\circ\\phi</math>. For every poset ''C'', the free completely distributive lattice over a poset ''C'' exists and is unique up to isomorphism.<ref name=\"Morris04\">Joseph M. Morris, ''[http://www.springerlink.com/content/nfqh0l29f3unrlwh Augmenting Types with Unbounded Demonic and Angelic Nondeterminacy]'', Mathematics of Program Construction, LNCS 3125, 274-288, 2004</ref>\n\nThis is an instance of the concept of [[free object]]. Since a set ''X'' can be considered as a poset with the discrete order, the above result guarantees the existence of the free completely distributive lattice over the set ''X''.\n\n==Examples==\n* The [[unit interval]] [0,1], ordered in the natural way, is a completely distributive lattice.<ref name=\"Raney52\">G. N. Raney, ''Completely distributive complete lattices'', Proceedings of the [[American Mathematical Society]], 3: 677 - 680, 1952.</ref>\n**More generally, any [[Total order#Completeness|complete chain]] is a completely distributive lattice.<ref name=\"hopenwasser90\">Alan Hopenwasser, ''Complete Distributivity'', Proceedings of Symposia in Pure Mathematics, 51(1), 285 - 305, 1990.</ref>\n* The [[power set]] lattice <math>(\\mathcal{P}(X),\\subseteq)</math> for any set ''X'' is a completely distributive lattice.<ref name=\"DaveyPriestley\"/>\n* For every poset ''C'', there is a ''free completely distributive lattice over C''.<ref name=\"Morris04\"/> See the section on [[Completely distributive lattice#Free completely distributive lattices|Free completely distributive lattices]] above.\n\n==See also==\n* [[Glossary of order theory]]\n* [[Distributive lattice]]\n\n==References==\n\n<references/>\n\n[[Category:Order theory]]"
    },
    {
      "title": "Completeness (order theory)",
      "url": "https://en.wikipedia.org/wiki/Completeness_%28order_theory%29",
      "text": "In the [[mathematics|mathematical]] area of [[order theory]], '''completeness properties''' assert the existence of certain [[infimum|infima]] or [[supremum|suprema]] of a given [[partially ordered set]] (poset). The most familiar example is the [[completeness of the real numbers]]. A special use of the term refers to [[complete partial order]]s or [[complete lattice]]s. However, many other interesting notions of completeness exist.\n\nThe motivation for considering completeness properties derives from the great importance of [[suprema]] (least upper bounds, [[Join (mathematics)|joins]], \"<math>\\vee</math>\") and [[infima]] (greatest lower bounds, [[meet (mathematics)|meets]], \"<math>\\wedge</math>\") to the theory of partial orders. Finding a supremum means to single out one distinguished least element from the set of upper bounds. On the one hand, these special elements often embody certain concrete properties that are interesting for the given application (such as being the [[least common multiple]] of a set of numbers or the [[union (set theory)|union]] of a collection of sets). On the other hand, the knowledge that certain types of subsets are guaranteed to have suprema or infima enables us to consider the computation of these elements as ''total operations'' on a partially ordered set. For this reason, [[poset]]s with certain completeness properties can often be described as [[algebraic structure]]s of a certain kind. In addition, studying the properties of the newly obtained operations yields further interesting subjects.\n\n==Types of completeness properties==\n\nAll completeness properties are described along a similar scheme: one describes a certain class of subsets of a partially ordered set that are required to have a supremum or infimum. Hence every completeness property has its [[duality (order theory)|dual]], obtained by inverting the order-dependent definitions in the given statement. Some of the notions are usually not dualized while others may be self-dual (i.e. equivalent to their dual statements).\n\n===Least and greatest elements===\n\nThe easiest example of a supremum is the empty one, i.e. the supremum of the [[empty set]]. By definition, this is the least element among all elements that are greater than each member of the empty set. But this is just the [[least element]] of the whole poset, if it has one, since the empty subset of a poset P is conventionally considered to be both bounded from above and from below, with every element of P being both an upper and lower bound of the empty subset. Other common names for the least element are bottom and zero (0). The dual notion, the empty lower bound, is the [[greatest element]], top, or unit (1).\n\nPosets that have a bottom are sometimes called pointed, while posets with a top are called unital or topped. An order that has both a least and a greatest element is bounded. However, this should not be confused with the notion of ''bounded completeness'' given below.\n\n===Finite completeness===\n\nFurther simple completeness conditions arise from the consideration of all non-empty finite sets. An order in which all non-empty finite sets have both a supremum and an infimum is called a [[lattice (order)|lattice]]. It suffices to require that all suprema and infima of ''two'' elements exist to obtain all non-empty finite ones. A straightforward [[mathematical induction|induction]] shows that every finite non-empty supremum/infimum can be decomposed into a finite number of binary suprema/infima. Thus the central operations of lattices are binary suprema <math>\\vee</math> and infima {{nobreak|<math>\\wedge</math>.}} It is in this context that the terms meet for <math>\\wedge</math> and join for <math>\\vee</math> are most common.\n\nA poset in which only non-empty finite suprema are known to exist is therefore called a [[semilattice|join-semilattice]]. The dual notion is [[semilattice|meet-semilattice]].\n\n===Further completeness conditions===\n\nThe strongest form of completeness is the existence of all suprema and all infima. The posets with this property are the [[complete lattice]]s. However, using the given order, one can restrict to further classes of (possibly infinite) subsets, that do not yield this strong completeness at once.\n\nIf all [[directed set|directed subsets]] of a poset have a supremum, then the order is a [[directed complete partial order|directed-complete partial order]] (dcpo). These are especially important in [[domain theory]]. The seldom-considered dual notion of a dcpo is the filtered-complete poset. Dcpos with a least element (\"pointed dcpos\") are one of the possible meanings of the phrase [[complete partial order]] (cpo).\n\nIf every subset that has ''some'' upper bounds has also a least upper bound, then the respective poset is called [[bounded complete]]. The term is used widely with this definition that focuses on suprema and there is no common name for the dual property. However, bounded completeness can be expressed in terms of other completeness conditions that are easily dualized (see below). Although concepts with the names \"complete\" and \"bounded\" were already defined, confusion is unlikely to occur since one would rarely speak of a \"bounded complete poset\" when meaning a \"bounded cpo\" (which is just a \"cpo with greatest element\"). Likewise, \"bounded complete lattice\" is almost unambiguous, since one would not state the boundedness property for complete lattices, where it is implied anyway. Also note that the empty set usually has upper bounds (if the poset is non-empty) and thus a bounded-complete poset has a least element.\n\nOne may also consider the subsets of a poset which are [[total order|totally ordered]], i.e. the [[Total order#Chains|chains]]. If all chains have a supremum, the order is called [[chain complete]]. Again, this concept is rarely needed in the dual form.\n\n==Relationships between completeness properties==\n\nIt was already observed that binary meets/joins yield all non-empty finite meets/joins. Likewise, many other (combinations) of the above conditions are equivalent.\n\n* The best-known example is the existence of all suprema, which is in fact equivalent to the existence of all infima. Indeed, for any subset ''X'' of a poset, one can consider its set of lower bounds ''B''. The supremum of ''B'' is then equal to the infimum of ''X'': since each element of ''X'' is an upper bound of ''B'', sup ''B'' is smaller than all elements of ''X'', i.e. sup ''B'' is in ''B''. It is the greatest element of ''B'' and hence the infimum of ''X''. In a dual way, the existence of all infima implies the existence of all suprema.\n* Bounded completeness can also be characterized differently. By an argument similar to the above, one finds that the supremum of a set with upper bounds is the infimum of the set of upper bounds. Consequently, bounded completeness is equivalent to the existence of all non-empty lower-bounded infima.\n* A poset is a complete lattice [[if and only if]] it is a cpo and a join-semilattice. Indeed, for any subset ''X'', the set of all finite suprema (joins) of ''X'' is directed and the supremum of this set (which exists by directed completeness) is equal to the supremum of ''X''. Thus every set has a supremum and by the above observation we have a complete lattice. The other direction of the proof is trivial.\n* The following equivalence requires the [[Axiom of Choice]]:\n:: A poset is chain-complete if and only if it is a dcpo.\n\n==Completeness in terms of universal algebra==\n\nAs explained above, the presence of certain completeness conditions allows to regard the formation of certain suprema and infima as total operations of an ordered set. It turns out that in many cases it is possible to characterize completeness solely by considering appropriate [[algebraic structure]]s in the sense of [[universal algebra]], which are equipped with operations like <math>\\vee</math> or <math>\\wedge</math>. By imposing additional conditions (in form of suitable [[Identity (mathematics)|identities]]) on these operations, one can then indeed derive the underlying partial order exclusively from such algebraic structures. Details on this characterization can be found in the articles on the \"lattice-like\" structures for which this is typically considered: see [[semilattice]], [[lattice (order)|lattice]], [[Heyting algebra]], and [[Boolean algebra (structure)|Boolean algebra]]. Note that the latter two structures extend the application of these principles beyond mere completeness requirements by introducing an additional operation of ''negation''.\n\n==Completeness in terms of adjunctions==\n\nAnother interesting way to characterize completeness properties is provided through the concept of (monotone) [[Galois connection]]s, i.e. adjunctions between partial orders. In fact this approach offers additional insights both in the nature of many completeness properties and in the importance of Galois connections for order theory. The general observation on which this reformulation of completeness is based is that the construction of certain suprema or infima provides left or right adjoint parts of suitable Galois connections.\n\nConsider a partially ordered set (''X'', ≤). As a first simple example, let 1 = {*} be a specified one-element set with the only possible partial ordering. There is an obvious mapping ''j'': ''X'' → 1 with ''j''(''x'') = * for all ''x'' in ''X''. ''X'' has a least element [[if and only if]] the function ''j'' has a lower adjoint ''j''<sup>*</sup>: 1 → ''X''. Indeed the definition for Galois connections yields that in this case ''j''<sup>*</sup>(*) ≤ ''x'' if and only if * ≤ ''j''(''x''), where the right hand side obviously holds for any ''x''. Dually, the existence of an upper adjoint for ''j'' is equivalent to ''X'' having a greatest element.\n\nAnother simple mapping is the function ''q'': ''X'' → (''X'' x ''X'') given by ''q''(''x'') = (''x'', ''x''). Naturally, the intended ordering relation for (''X'' x ''X'') is just the usual [[product order]]. ''q'' has a lower adjoint ''q''<sup>*</sup> if and only if all binary joins in ''X'' exist. Conversely, the join operation <math>\\vee</math>: (''X'' x ''X'') → ''X'' can always provide the (necessarily unique) lower adjoint for ''q''. Dually, ''q'' allows for an upper adjoint if and only if ''X'' has all binary meets. Thus the meet operation <math>\\wedge</math>, if it exists, always is an upper adjoint. If both <math>\\vee</math> and <math>\\wedge</math> exist and, in addition, <math>\\wedge</math> is also a lower adjoint, then the poset ''X'' is a [[Heyting algebra]]—another important special class of partial orders.\n\nFurther completeness statements can be obtained by exploiting suitable [[completion (order theory)|completion]] procedures. For example, it is well known that the collection of all [[lower set]]s of a poset ''X'', ordered by [[subset|subset inclusion]], yields a complete lattice '''D'''(''X'') (the downset-lattice). Furthermore, there is an obvious embedding ''e'': ''X'' → '''D'''(''X'') that maps each element ''x'' of ''X'' to its [[ideal (order theory)|principal ideal]] {''y'' in ''X'' | ''y'' ≤ ''x''}. A little reflection now shows that ''e'' has a lower adjoint if and only if ''X'' is a complete lattice. In fact, this lower adjoint will map any lower set of ''X'' to its supremum in ''X''. Composing this with the function that maps any subset of ''X'' to its [[lower set|lower closure]] (again an adjunction for the inclusion of lower sets in the [[powerset]]), one obtains the usual supremum map from the powerset 2<sup>''X''</sup> to ''X''. As before, another important situation occurs whenever this supremum map is also an upper adjoint: in this case the complete lattice ''X'' is ''constructively completely distributive''. See also the articles on [[completely distributive lattice|complete distributivity]] and [[distributivity (order theory)]].\n\nThe considerations in this section suggest a reformulation of (parts of) order theory in terms of [[category theory]], where properties are usually expressed by referring to the relationships ([[morphism]]s, more specifically: adjunctions) between objects, instead of considering their internal structure. For more detailed considerations of this relationship see the article on the [[categorical formulation of order theory]].\n\n==See also==\n*[[Limit-preserving function (order theory)|Limit-preserving function]] on the ''preservation'' of existing suprema/infima.\n* [[Total order]]\n\n==Notes==\n<references />\n\n==References==\n* G. Markowsky and B.K. Rosen.  ''Bases for chain-complete posets'' IBM Journal of Research and Development.  March 1976.\n* Stephen Bloom. ''Varieties of ordered algebras'' Journal of Computer and System Sciences.  October 1976.\n* Michael Smyth.  ''Power domains''  Journal of Computer and System Sciences.  1978.\n* Daniel Lehmann.  ''On the algebra of order'' Journal of Computer and System Sciences.  August 1980.\n\n[[Category:Order theory]]"
    },
    {
      "title": "Containment order",
      "url": "https://en.wikipedia.org/wiki/Containment_order",
      "text": "In the [[mathematics|mathematical]] field of [[order theory]], a '''containment order''' is the [[partial order]] that arises as the [[subset]]-containment relation on some collection of objects.  In a simple way, every [[partially ordered set|poset]] ''P'' = (''X'',≤) is ([[isomorphism|isomorphic]] to) a containment order (just as every group is isomorphic to a permutation group - see [[Cayley's theorem]]).  To see this, associate to each element ''x'' of ''X'' the set\n\n:<math> X_\\leq(x) = \\{ y \\in X | y \\leq x\\} ; </math>\n\nthen the transitivity of ≤ ensures that for all ''a'' and ''b'' in ''X'', we have\n\n:<math> X_\\leq(a) \\subseteq X_\\leq(b) \\mbox{ precisely when } a \\leq b . </math>\n\nThere can be sets <math>S</math> of [[cardinal number|cardinality]] less than <math>|X|</math> such that ''P'' is [[isomorphism|isomorphic]] to the containment order on S. The size of the smallest possible ''S'' is called the [[Order dimension#k-dimension and 2-dimension|2-dimension]] of ''S''.\n\nSeveral important classes of poset arise as containment orders for some natural collections, like the [[Boolean lattice]] ''Q<sup>n</sup>'', which is the collection of all 2<sup>''n''</sup> subsets of an ''n''-element set, the '''interval-containment orders''', which are precisely the orders of [[order dimension]] at most two, and the dimension-''n'' orders, which are the containment orders on collections of ''n''-boxes anchored at the [[Origin (mathematics)|origin]].  Other containment orders that are interesting in their own right include the '''circle orders''', which arise from disks in the plane, and the '''angle orders'''.\n\n== See also ==\n*[[Birkhoff's representation theorem]]\n*[[Intersection graph]]\n*[[Interval order]]\n\n== References ==\n\n*{{cite journal\n |author1=Fishburn, P.C.  |author2=Trotter, W.T.\n | title = Geometric containment orders: a survey\n | journal = [[Order (journal)|Order]]\n | volume = 15\n | year = 1998\n | pages = 167–182\n | doi = 10.1023/A:1006110326269\n | issue = 2}}\n*{{cite journal\n | author = Santoro, N., Sidney, J.B., Sidney, S.J., and [[Jorge Urrutia Galicia|Urrutia, J.]]\n | title = Geometric containment and partial orders\n | journal = [[SIAM Journal on Discrete Mathematics]]\n | volume = 2\n | year = 1989\n | pages = 245–254\n | doi = 10.1137/0402021\n | issue = 2| citeseerx = 10.1.1.65.1927\n }}\n\n[[Category:Order theory]]\n\n\n{{algebra-stub}}"
    },
    {
      "title": "Countable chain condition",
      "url": "https://en.wikipedia.org/wiki/Countable_chain_condition",
      "text": "In [[order theory]], a [[partially ordered set]] ''X'' is said to satisfy the '''countable chain condition''', or to be '''ccc''', if every [[strong antichain]] in ''X'' is [[countable]]. \n\n==Overview==\nThere are really two conditions: the ''upwards'' and ''downwards'' countable chain conditions. These are not equivalent. The countable chain condition means the downwards countable chain condition, in other words no two elements have a common lower bound.\n\nThis is called the \"countable chain condition\" rather than the more logical term \"countable antichain condition\" for historical reasons related to certain chains of open sets in topological spaces and chains in complete Boolean algebras, where chain conditions sometimes happen to be equivalent to antichain conditions. For example, if &kappa; is a cardinal, then in a complete Boolean algebra every antichain has size less than &kappa; if and only if there is no descending &kappa;-sequence of elements, so chain conditions are equivalent to antichain conditions.\n\nPartial orders and spaces satisfying the ccc are used in the statement of [[Martin's axiom]].\n\nIn the theory of [[forcing (set theory)|forcing]], ccc partial orders are used because forcing with any generic set over such an order preserves cardinals and cofinalities.  Furthermore, the ccc property is preserved by finite support iterations (see [[iterated forcing]]). For more information on ccc in the context of forcing, see {{format link|Forcing (set theory)#The countable chain condition}}.\n\nMore generally, if &kappa; is a cardinal then a poset is said to satisfy the '''&kappa;-chain condition''' if every antichain has size less than &kappa;. The countable chain condition is the &alefsym;<sub>1</sub>-chain condition.\n\n==Examples and properties in topology==\nA [[topological space]] is said to satisfy the countable chain condition, or '''Suslin's Condition''', if the partially ordered set of non-empty [[open subset]]s of ''X'' satisfies the countable chain condition, ''i.e.'' every [[pairwise disjoint]] collection of non-empty open subsets of ''X'' is countable. The name originates from  [[Suslin's problem|Suslin's Problem]].\n\n* Every [[separable topological space]] is ccc. Furthermore, the [[Product topology|product space]] of at most [[Cardinality of the continuum|<math>\\mathfrak{c}=2^{\\aleph_{0}}</math>]] separable spaces is a separable space and, thus, ccc.\n* Every [[metric space]] is ccc if and only if it's separable.\n* In general, a ccc topological space need not be separable. For example, <math>\\{ 0, 1 \\}^{2^{2^{\\aleph_{0}}}}</math> with the [[product topology]] is ccc, though ''not'' separable.\n* Paracompact ccc spaces are [[Lindelöf space|Lindelöf]].\n\n==References==\n*{{Citation | last1=Jech | first1=Thomas | author1-link=Thomas Jech | title=Set Theory: Millennium Edition | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Springer Monographs in Mathematics | isbn=978-3-540-44085-7 | year=2003}}\n*Products of Separable Spaces, K. A. Ross, and A. H. Stone. The American Mathematical Monthly 71(4):pp.&nbsp;398–403 (1964)\n\n[[Category:Order theory]]\n[[Category:Forcing (mathematics)]]"
    },
    {
      "title": "Countryman line",
      "url": "https://en.wikipedia.org/wiki/Countryman_line",
      "text": "In mathematics, a '''Countryman line''' is an [[Uncountable set|uncountable]] [[Total order|linear ordering]] whose square is the union of [[Countable set|countably]] many [[Total order#Chains|chains]]. The existence of Countryman lines was first proven by [[Saharon Shelah|Shelah]]. Shelah also conjectured that, assuming [[Proper Forcing Axiom|PFA]], every [[Aronszajn line]] contains a Countryman line. This conjecture, which remained open for three decades, was proven by [[Justin T. Moore|Justin Moore]].\n\n==References==\n* {{cite journal|last=Shelah|first=Saharon|title=Decomposing uncountable squares to countably many chains|journal=[[Journal of Combinatorial Theory, Series A]]|volume=21|issue=1|year=1976|pages=110–114|authorlink=Saharon Shelah|doi=10.1016/0097-3165(76)90053-4}}\n* {{cite journal|last=Moore|first=Justin|title=A five element basis for the uncountable linear orders|journal=Annals of Mathematics |series=Second Series|volume=163|issue=2|year=2006|pages=669–688|authorlink=Justin T. Moore|doi=10.4007/annals.2006.163.669|arxiv=math/0501525}}\n* Roger S. Countryman, Jr. Spaces having a <math>\\sigma</math>-monotone base. Preprint, 1970.\n\n[[Category:Order theory]]\n\n\n{{mathlogic-stub}}"
    },
    {
      "title": "Critical pair (order theory)",
      "url": "https://en.wikipedia.org/wiki/Critical_pair_%28order_theory%29",
      "text": "[[File:Critical pair (order theory).svg|thumb|100px|Hasse diagram of a partial order with a critical pair ⟨''b'',''c''⟩. Adding the {{color|#c0c0c0|grey}} line would make ''b''<''c'' without requiring any other changes. Conversely, ⟨''c'',''b''⟩ is not a critical pair, since ''d''<''c'', but not ''d''<''b''.]]\nIn [[order theory]], a discipline within mathematics, a '''critical pair''' is a pair of elements in a [[partially ordered set]] that are [[comparability|incomparable]] but that could be made comparable without requiring any other changes to the partial order.\n\nFormally, let {{math|1=''P'' = (''S'', ≤)}} be a partially ordered set. Then a critical pair is an ordered pair {{math|(''x'', ''y'')}} of elements of {{mvar|S}} with the following three properties:\n*{{mvar|x}} and {{mvar|y}} are incomparable in {{mvar|P}},\n*for every {{mvar|z}} in {{mvar|S}}, if {{math|''z'' < ''x''}} then {{math|''z'' < ''y''}}, and\n*for every {{mvar|z}} in {{mvar|S}}, if {{math|''y'' < ''z''}} then {{math|''x'' < ''z''}}.\n\nIf {{math|(''x'', ''y'')}} is a critical pair, then the binary relation obtained from {{mvar|P}} by adding the single relationship {{math|''x'' ≤ ''y''}} is also a partial order. The properties required of critical pairs ensure that, when the relationship {{math|''x'' ≤ ''y''}} is added, the addition does not cause any violations of the [[transitive property]].\n\nA set {{mvar|R}} of [[linear extension]]s of {{mvar|P}} is said to ''reverse'' a critical pair {{math|(''x'', ''y'')}} in {{mvar|P}} if there exists a linear extension in {{mvar|R}} for which {{mvar|y}} occurs earlier than&nbsp;{{mvar|x}}. This property may be used to characterize [[order dimension|realizer]]s of finite partial orders: A nonempty set {{mvar|R}} of linear extensions is a realizer if and only if it reverses every critical pair.\n\n==References==\n*{{citation|first=W. T.|last=Trotter|title=Combinatorics and partially ordered sets:  Dimension theory|series=Johns Hopkins Series in Mathematical Sciences|publisher=Johns Hopkins Univ. Press|location=Baltimore|year=1992}}.\n\n[[Category:Order theory]]"
    },
    {
      "title": "Cyclic order",
      "url": "https://en.wikipedia.org/wiki/Cyclic_order",
      "text": "[[Image:DC8.png|right]]\n\nIn [[mathematics]], a '''cyclic order''' is a way to arrange a set of objects in a [[circle]].{{ref|cyclic order|[nb]}} Unlike most structures in [[order theory]], a cyclic order is not modeled as a [[binary relation]], such as \"{{math|''a'' < ''b''}}\". One does not say that east is \"more clockwise\" than west. Instead, a cyclic order is defined as a [[ternary relation]] {{math|[''a'', ''b'', ''c'']}}, meaning \"after {{mvar|a}}, one reaches {{mvar|b}} before {{mvar|c}}\". For example, [June, October, February]. A ternary relation is called a cyclic order if it is [[#The ternary relation|cyclic, asymmetric, transitive, and total]]. Dropping the \"total\" requirement results in a [[partial cyclic order]].\n\nA [[set (mathematics)|set]] with a cyclic order is called a '''cyclically ordered set''' or simply a '''cycle'''.{{ref|cycle|[nb]}} Some familiar cycles are discrete, having only a [[Finite set|finite number]] of [[element (mathematics)|element]]s: there are seven [[days of the week]], four [[cardinal direction]]s, twelve notes in the [[chromatic scale]], and three plays in [[rock-paper-scissors]]. In a finite cycle, each element has a \"next element\" and a \"previous element\". There are also continuously variable cycles with infinitely many elements, such as the oriented [[unit circle]] in the plane.\n\nCyclic orders are closely related to the more familiar [[linear order]]s, which arrange objects in a [[line (geometry)|line]]. Any linear order can be bent into a circle, and any cyclic order can be cut at a point, resulting in a line. These operations, along with the related constructions of intervals and covering maps, mean that questions about cyclic orders can often be transformed into questions about linear orders. Cycles have more symmetries than linear orders, and they often naturally occur as residues of linear structures, as in the [[finite cyclic group]]s or the [[real projective line]].\n\n==Finite cycles==\n[[Image:Orientovaná kružnice.svg|thumb|A 5-element cycle]]\nA cyclic order on a set {{mvar|X}} with {{mvar|n}} elements is like an arrangement of {{mvar|X}} on a clock face, for an {{mvar|n}}-hour clock. Each element {{mvar|x}} in {{mvar|X}} has a \"next element\" and a \"previous element\", and taking either successors or predecessors cycles exactly once through the elements as {{math|''x''(1), ''x''(2), ..., ''x''(''n'')}}.\n\nThere are a few equivalent ways to state this definition. A cyclic order on {{mvar|X}} is the same as a [[permutation]] that makes all of {{mvar|X}} into a single [[Cycles and fixed points|cycle]]. A cycle with {{mvar|n}} elements is also a {{math|'''Z'''<sub>''n''</sub>}}-[[torsor]]: a set with a free transitive [[Group action (mathematics)|action]] by a [[finite cyclic group]].{{sfn|Brown|1987|p=52}} Another formulation is to make {{mvar|X}} into the standard [[directed cycle graph]] on {{mvar|n}} vertices, by some matching of elements to vertices.\n\nIt can be instinctive to use cyclic orders for [[symmetric function]]s, for example as in\n\n:{{math|''xy'' + ''yz'' + ''zx''}}\n\nwhere writing the final [[monomial]] as {{math|''xz''}} would distract from the pattern.\n\nA substantial use of cyclic orders is in the determination of the [[conjugacy class]]es of [[free group]]s. Two elements {{mvar|g}} and {{mvar|h}} of the free group {{mvar|F}} on a set {{mvar|Y}} are conjugate if and only if, when they are written as products of elements {{mvar|y}} and {{math|''y<sup>−1</sup>''}} with {{mvar|y}} in {{mvar|Y}}, and then those products are put in cyclic order, the cyclic orders are equivalent under the [[rewriting]] rules that allow one to remove or add adjacent {{mvar|y}} and {{math|''y''<sup>&minus;1</sup>}}.\n\nA cyclic order on a set {{mvar|X}} can be determined by a linear order on {{mvar|X}}, but not in a unique way. Choosing a linear order is equivalent to choosing a first element, so there are exactly {{mvar|n}} linear orders that induce a given cyclic order. Since there are {{math|''n''!}} possible linear orders, there are {{math|(''n'' &minus; 1)!}} possible cyclic orders.\n\n==Definitions==\nAn [[infinite set]] can also be ordered cyclically. Important examples of infinite cycles include the [[unit circle]], {{math|''S''<sup>1</sup>}}, and the [[rational number]]s, {{math|'''Q'''}}. The basic idea is the same: we arrange elements of the set around a circle. However, in the infinite case we cannot rely upon an immediate successor relation, because points may not have successors. For example, given a point on the unit circle, there is no \"next point\". Nor can we rely upon a binary relation to determine which of two points comes \"first\". Traveling clockwise on a circle, neither east or west comes first, but each follows the other.\n\nInstead, we use a ternary relation denoting that elements {{mvar|a}}, {{mvar|b}}, {{mvar|c}} occur after each other (not necessarily immediately) as we go around the circle. For example, in clockwise order, [east, south, west]. By [[currying]] the arguments of the ternary relation {{math|[''a'', ''b'', ''c'']}}, one can think of a cyclic order as a one-parameter family of binary order relations, called ''cuts'', or as a two-parameter family of subsets of {{mvar|K}}, called ''intervals''.\n\n===The ternary relation===\nThe general definition is as follows: a cyclic order on a set {{mvar|X}} is a relation {{math|''C'' ⊂ ''X''<sup>3</sup>}}, written {{math|[''a'', ''b'', ''c'']}}, that satisfies the following axioms:{{ref|ternary relation|[nb]}}\n#Cyclicity: If {{math|[''a'', ''b'', ''c'']}} then {{math|[''b'', ''c'', ''a'']}}\n#Asymmetry: If {{math|[''a'', ''b'', ''c'']}} then not {{math|[''c'', ''b'', ''a'']}}\n#Transitivity:  If {{math|[''a'', ''b'', ''c'']}} and {{math|[''a'', ''c'', ''d'']}} then {{math|[''a'', ''b'', ''d'']}}\n#Totality: If {{mvar|a}}, {{mvar|b}}, and {{mvar|c}} are distinct, then either {{math|[''a'', ''b'', ''c'']}} or {{math|[''c'', ''b'', ''a'']}}\n\nThe axioms are named by analogy with the [[asymmetric relation|asymmetry]], [[transitive relation|transitivity]], and [[total relation|totality]] axioms for a binary relation, which together define a [[strict linear order]]. {{Harvs|first=Edward|last=Huntington|authorlink=Edward Vermilye Huntington|year=1916|year2=1924|txt}} considered other possible lists of axioms, including one list that was meant to emphasize the similarity between a cyclic order and a [[betweenness relation]]. A ternary relation that satisfies the first three axioms, but not necessarily the axiom of totality, is a [[partial cyclic order]].\n\n===Rolling and cuts===\nGiven a linear order {{math|<}} on a set {{mvar|X}}, the cyclic order on {{mvar|X}} induced by {{math|<}} is defined as follows:<ref>{{Harvnb|Huntington|1935|p=6}}; {{Harvnb|Čech|1936|p=25}}.</ref>\n:{{math|[''a'', ''b'', ''c'']}} if and only if {{math|''a'' < ''b'' < ''c''}} or {{math|''b'' < ''c'' < ''a''}} or {{math|''c'' < ''a'' < ''b''}}\n\nTwo linear orders induce the same cyclic order if they can be transformed into each other by a cyclic rearrangement, as in\n[[cut (cards)|cutting a deck of cards]].{{sfn|Calegari|2004|p=439}} One may define a cyclic order relation as a ternary relation that is induced by a strict linear order as above.{{sfn|Courcelle|2003}}\n\nCutting a single point out of a cyclic order leaves a linear order behind. More precisely, given a cyclically ordered set ({{math|''K'', [ ])}}, each element {{math|''a'' ∈ ''K''}} defines a natural linear order {{math|<<sub>''a''</sub>}} on the remainer of the set, {{math|''K'' ∖ ''a''}}, by the following rule:<ref>{{Harvnb|Huntington|1935|p=7}}; {{Harvnb|Čech|1936|p=24}}.</ref>\n:{{math|''x'' <<sub>''a''</sub> ''y''}} if and only if {{math|[''a'', ''x'', ''y'']}}.\nMoreover, {{math|<<sub>''a''</sub>}} can be extended by adjoining {{mvar|a}} as a least element; the resulting linear order on {{mvar|K}} is called the principal cut with least element {{mvar|a}}. Likewise, adjoining {{mvar|a}} as a greatest element results in a cut {{math|<<sup>''a''</sup>}}.{{sfn|Novák|1984|p=323}}\n\n===Intervals===\nGiven two elements {{math|''a'' ≠ ''b'' ∈ ''K''}}, the [[open interval]] from {{mvar|a}} to {{mvar|b}}, written {{math|(''a'', ''b'')}}, is the set of all {{math|''x'' ∈ ''K''}} such that {{math|[''a'', ''x'', ''b'']}}. The system of open intervals completely defines the cyclic order and can be used as an alternate definition of a cyclic order relation.{{sfn|McMullen|2009|p=10}}\n\nAn interval {{math|(''a'', ''b'')}} has a natural linear order given by {{math|<<sub>''a''</sub>}}. One can define half-closed and closed intervals {{math|[''a'', ''b'')}}, {{math|(''a'', ''b'']}}, and {{math|[''a'', ''b'']}} by adjoining {{mvar|a}} as a [[least element]] and/or {{mvar|b}} as a [[greatest element]].{{sfn|Giraudet|Holland|2002|p=2}} As a special case, the open interval {{math|(''a'', ''a'')}} is defined as the cut {{math|''K'' ∖ ''a''}}.\n\nMore generally, a proper subset ''S'' of ''K'' is called [[convex set|convex]] if it contains an interval between every pair of points: for {{math|''a'' ≠ ''b'' ∈ ''S''}}, either {{math|(''a'', ''b'')}} or {{math|(''b'', ''a'')}} must also be in ''S''.{{sfn|Kulpeshov|2009}} A convex set is linearly ordered by the cut {{math|<<sub>''x''</sub>}} for any {{mvar|x}} not in the set; this ordering is independent of the choice of {{mvar|x}}.\n\n===Automorphisms===\nAs a circle has a [[clockwise]] order and a counterclockwise order, any set with a cyclic order has two '''senses'''. A [[bijection]] of the set that preserves the order is called an '''ordered correspondence'''. If the sense is maintained as before, it is a '''direct correspondence''', otherwise it is called an '''opposite correspondence'''.<ref>Coxeter 1949 page 25</ref> Coxeter uses a [[separation relation]] to describe cyclic order, and this relation is strong enough to distinguish the two senses of cyclic order. The [[automorphism]]s of a cyclically ordered set may be identified with C<sub>2</sub>, the two-element group, of direct and opposite correspondences.\n\n==Monotone functions==\nThe \"cyclic order = arranging in a circle\" idea works because any [[subset]] of a cycle is itself a cycle. In order to use this idea to impose cyclic orders on sets that are not actually subsets of the unit circle in the plane, it is necessary to consider [[function (mathematics)|function]]s between sets.\n\nA function between two cyclically ordered sets, {{math|''f'' : ''X'' → ''Y''}}, is called a ''[[monotonic function]]'' or a ''[[morphism|homomorphism]]'' if it pulls back the ordering on {{mvar|Y}}: whenever {{math|[''f''(''a''), ''f''(''b''), ''f''(''c'')]}}, one has {{math|[''a'', ''b'', ''c'']}}. Equivalently, {{mvar|f}} is monotone if whenever {{math|[''a'', ''b'', ''c'']}} and {{math|''f''(''a''), ''f''(''b'')}}, and {{math|''f''(''c'')}} are all distinct, then {{math|[''f''(''a''), ''f''(''b''), ''f''(''c'')]}}. A typical example of a monotone function is the following function on the cycle with 6 elements:\n:{{math|1=''f''(0) = ''f''(1) = 4,}}\n:{{math|1=''f''(2) = ''f''(3) = 0,}}\n:{{math|1=''f''(4) = ''f''(5) = 1.}}\n\nA function is called an ''[[embedding]]'' if it is both monotone and [[injective function|injective]].{{ref|embedding|[nb]}} Equivalently, an embedding is a function that pushes forward the ordering on {{mvar|X}}: whenever {{math|[''a'', ''b'', ''c'']}}, one has {{math|[''f''(''a''), ''f''(''b''), ''f''(''c'')]}}. As an important example, if {{mvar|X}} is a subset of a cyclically ordered set {{mvar|Y}}, and {{mvar|X}} is given its natural ordering, then the [[inclusion map]] {{math|''i'' : ''X'' → ''Y''}} is an embedding.\n\nGenerally, an injective function {{mvar|f}} from an unordered set {{mvar|X}} to a cycle {{mvar|Y}} induces a unique cyclic order on {{mvar|X}} that makes {{mvar|f}} an embedding.\n\n===Functions on finite sets===\nA cyclic order on a finite set {{mvar|X}} can be determined by an injection into the unit circle, {{math|''X'' → ''S''<sup>1</sup>}}. There are many possible functions that induce the same cyclic order—in fact, infinitely many. In order to quantify this redundancy, it takes a more complex combinatorial object than a simple number. Examining the [[Configuration space (physics)|configuration space]] of all such maps leads to the definition of an {{nowrap|{{math|(''n'' − 1)}}-dimensional}} [[polytope]] known as a [[cyclohedron]].  Cyclohedra were first applied to the study of [[knot invariant]]s;{{sfn|Stasheff|1997|p=58}} they have more recently been applied to the experimental detection of [[periodically expressed gene|periodically expressed]] [[gene]]s in the study of [[circadian rhythm|biological clock]]s.{{sfn|Morton|Pachter|Shiu|Sturmfels|2007}}\n\nThe category of homomorphisms of the standard finite cycles is called the [[cyclic category]]; it may be used to construct [[Alain Connes]]' [[cyclic homology]].\n\nOne may define a degree of a function between cycles, analogous to the [[degree of a continuous mapping]]. For example, the natural map from the [[circle of fifths]] to the [[chromatic circle]] is a map of degree 7. One may also define a [[rotation number]].\n\n===Completion===\n*A cut with both a least element and a greatest element is called a ''jump''. For example, every cut of a finite cycle {{math|'''Z'''<sub>''n''</sub>}} is a jump. A cycle with no jumps is called ''[[dense order|dense]]''.{{sfn|Novák|1984|p=325}}{{sfn|Novák|Novotný|1987|p=409–410}}\n*A cut with neither a least element nor a greatest element is called a ''gap''. For example, the rational numbers {{math|'''Q'''}} have a gap at every irrational number. They also have a gap at infinity, i.e. the usual ordering. A cycle with no gaps is called ''[[completeness (order theory)|complete]]''.{{sfn|Novák|1984|pp=325, 331}}{{sfn|Novák|Novotný|1987|p=409–410}}\n*A cut with exactly one endpoint is called a ''principal'' or ''Dedekind'' cut. For example, every cut of the circle {{math|''S''<sup>1</sup>}} is a principal cut. A cycle where every cut is principal, being both dense and complete, is called ''continuous''.{{sfn|Novák|1984|p=333}}{{sfn|Novák|Novotný|1987|p=409–410}}\n\n[[Image:CyclicOrderingOfCuts.svg|thumb|{{math|[<<sub>1</sub>, <<sub>2</sub>, <<sub>3</sub>]}} and {{math|[''x'', ''y'', ''z'']}}]]\n\nThe set of all cuts is cyclically ordered by the following relation: {{math|[<<sub>1</sub>, <<sub>2</sub>, <<sub>3</sub>]}} if and only if there exist {{math|''x'', ''y'', ''z''}} such that:{{sfn|Novák|1984|p=330}}\n:{{math|''x'' <<sub>1</sub> ''y'' <<sub>1</sub> ''z''}},\n:<span style=\"color:transparent\">{{math|''x'' <<sub>1</sub> }}</span>{{math|''y'' <<sub>2</sub> ''z'' <<sub>2</sub> ''x''}}, and\n:<span style=\"color:transparent\">{{math|''x'' <<sub>1</sub> ''y'' <<sub>1</sub> }}</span>{{math|''z'' <<sub>3</sub> ''x'' <<sub>3</sub> ''y''}}.\n\nA certain subset of this cycle of cuts is the [[Dedekind completion]] of the original cycle.\n\n==Further constructions==\n===Unrolling and covers===\nStarting from a cyclically ordered set {{mvar|K}}, one may form a linear order by unrolling it along an infinite line. This captures the intuitive notion of keeping track of how many times one goes around the circle. Formally, one defines a linear order on the [[Cartesian product]] {{math|'''Z''' × ''K''}}, where {{math|'''Z'''}} is the set of [[integer]]s, by fixing an element {{mvar|a}} and requiring that for all {{mvar|i}}:<ref>{{harvnb|Roll|1993|p=469}}; {{harvnb|Freudenthal|Bauer|1974|p=10}}</ref>\n:If {{math|[''a'', ''x'', ''y'']}}, then {{math|''a''<sub>''i''</sub> < ''x''<sub>''i''</sub> < ''y''<sub>''i''</sub> < ''a''<sub>''i'' + 1</sub>}}.\nFor example, the months {{MONTHNAME|1}} {{CURRENTYEAR}}, {{MONTHNAME|5}} {{CURRENTYEAR}}, {{MONTHNAME|9}} {{CURRENTYEAR}}, and {{MONTHNAME|1}} {{#expr:{{CURRENTYEAR}} + 1}} occur in that order.\n\nThis ordering of {{math|'''Z''' × ''K''}} is called the [[universal cover]] of {{mvar|K}}.{{ref|universal cover|[nb]}} Its [[order type]] is independent of the choice of {{mvar|a}}, but the notation is not, since the integer coordinate \"rolls over\" at {{mvar|a}}. For example, although the cyclic order of [[pitch class]]es is compatible with the A-to-G alphabetical order, C is chosen to be the first note in each octave, so in [[note-octave]] notation, B<sub>3</sub> is followed by C<sub>4</sub>.\n\nThe inverse construction starts with a linearly ordered set and coils it up into a cyclically ordered set. Given a linearly ordered set {{mvar|L}} and an order-preserving [[bijection]] {{math|''T'' : ''L'' → ''L''}} with unbounded orbits, the [[orbit space]] {{math|''L'' / ''T''}} is cyclically ordered by the requirement:{{sfn|McMullen|2009|p=10}}{{ref|orbit space|[nb]}}\n:If {{math|''a'' < ''b'' < ''c'' < ''T''(''a'')}}, then {{math|{{bracket|[''a''], [''b''], [''c'']}}}}.\nIn particular, one can recover {{mvar|K}} by defining {{math|1=''T''(''x''<sub>''i''</sub>) = ''x''<sub>''i'' + 1</sub>}} on {{math|'''Z''' × ''K''}}.\n\nThere are also {{mvar|n}}-fold coverings for finite {{mvar|n}}; in this case, one cyclically ordered set covers another cyclically ordered set. For example, the {{nowrap|[[24-hour clock]]}} is a double cover of the {{nowrap|[[12-hour clock]]}}. In geometry, the [[pencil (mathematics)|pencil]] of [[ray (mathematics)|ray]]s emanating from a point in the oriented plane is a double cover of the pencil of unoriented [[line (geometry)|line]]s passing through the same point.<ref>{{harvnb|Freudenthal|1973|p=475}}; {{harvnb|Freudenthal|Bauer|1974|p=10}}</ref> These covering maps can be characterized by lifting them to the universal cover.{{sfn|McMullen|2009|p=10}}\n\n===Products and retracts===\n[[Image:CyclicLinearProductLabels.svg|400px|right]]\n\nGiven a cyclically ordered set {{math|(''K'', [ ])}} and a linearly ordered set {{math|(''L'', <)}}, the (total) lexicographic product is a cyclic order on the [[Cartesian product|product set]] {{math|''K'' × ''L''}}, defined by {{math|[(''a'', ''x''), (''b'', ''y''), (''c'', ''z'')]}} if one of the following holds:{{sfn|Świerczkowski|1959a|p=161}}\n*{{math|[''a'', ''b'', ''c'']}}\n*{{math|1=''a'' = ''b'' ≠ ''c''}} and {{math|''x'' < ''y''}}\n*{{math|1=''b'' = ''c'' ≠ ''a''}} and {{math|''y'' < ''z''}}\n*{{math|1=''c'' = ''a'' ≠ ''b''}} and {{math|''z'' < ''x''}}\n*{{math|1=''a'' = ''b'' = ''c''}} and {{math|[''x'', ''y'', ''z'']}}\n\nThe lexicographic product {{math|''K'' × ''L''}} globally looks like {{mvar|K}} and locally looks like {{mvar|L}}; it can be thought of as {{mvar|K}} copies of {{mvar|L}}. This construction is sometimes used to characterize cyclically ordered groups.{{sfn|Świerczkowski|1959a}}\n\nOne can also glue together different linearly ordered sets to form a circularly ordered set. For example, given two linearly ordered sets {{math|''L''<sub>1</sub>}} and {{math|''L''<sub>2</sub>}}, one may form a circle by joining them together at positive and negative infinity. A circular order on the disjoint union {{math|''L''<sub>1</sub> ∪ ''L''<sub>2</sub> ∪ {–∞, ∞}}} is defined by {{math|∞ < ''L''<sub>1</sub> < –∞ < ''L''<sub>2</sub> < ∞}}, where the induced ordering on {{math|''L''<sub>1</sub>}} is the opposite of its original ordering. For example, the set of all [[longitude]]s is circularly ordered by joining together all points west and all points east, along with the [[prime meridian]] and the [[180th meridian]]. {{Harvtxt|Kuhlmann|Marshall|Osiak|2011}} use this construction while characterizing the spaces of orderings and [[real place]]s of double [[formal Laurent series]] over a [[real closed field]].{{sfn|Kuhlmann|Marshall|Osiak|2011|p=8}}\n\n==Topology==\nThe open intervals form a [[base (topology)|base]] for a natural [[topological space|topology]], the cyclic [[order topology]]. The [[open set]]s in this topology are exactly those sets which are open in ''every'' compatible linear order.{{sfn|Viro|Ivanov|Netsvetaev|Kharlamov|2008| p=44}} To illustrate the difference, in the set [0, 1), the subset [0, 1/2) is a neighborhood of 0 in the linear order but not in the cyclic order.\n\nInteresting examples of cyclically ordered spaces include the conformal boundary of a [[simply connected]] [[Lorentz surface]]{{sfn|Weinstein|1996|pp=80–81}} and the [[leaf space]] of a lifted [[essential lamination]] of certain 3-manifolds.{{sfn|Calegari|Dunfield|2003|pp=12–13}} [[Discrete dynamical system]]s on cyclically ordered spaces have also been studied.{{sfn|Bass|Otero-Espinar|Rockmore|Tresser|1996| p=19}}\n\nThe interval topology forgets the original orientation of the cyclic order. This orientation can be restored by enriching the intervals with their induced linear orders; then one has a set covered with an atlas of linear orders that are compatible where they overlap. In other words, a cyclically ordered set can be thought of as a locally linearly ordered space: an object like a [[manifold]], but with order relations instead of coordinate charts. This viewpoint makes it easier to be precise about such concepts as covering maps. The generalization to a locally partially ordered space is studied in {{Harvtxt|Roll|1993}}; see also ''[[Directed topology]]''.\n\n==Related structures==\n===Groups===\n{{Main|Cyclically ordered group}}\nA [[cyclically ordered group]] is a set with both a [[group (mathematics)|group structure]] and a cyclic order, such that left and right multiplication both preserve the cyclic order. Cyclically ordered groups were first studied in depth by [[Ladislav Rieger]] in 1947.{{sfn|Pecinová-Kozáková|2005|p=194}} They are a generalization of [[cyclic group]]s: the [[infinite cyclic group]] {{math|'''Z'''}} and the [[finite cyclic group]]s {{math|'''Z'''/''n''}}. Since a linear order induces a cyclic order, cyclically ordered groups are also a generalization of [[linearly ordered group]]s: the [[rational number]]s {{math|'''Q'''}}, the real numbers {{math|'''R'''}}, and so on. Some of the most important cyclically ordered groups fall into neither previous category: the [[circle group]] {{math|'''T'''}} and its subgroups, such as the [[group of rational points on the unit circle|subgroup of rational points]].\n\nEvery cyclically ordered group can be expressed as a quotient {{math|''L'' / ''Z''}}, where {{mvar|L}} is a linearly ordered group and {{mvar|Z}} is a cyclic cofinal subgroup of {{mvar|L}}. Every cyclically ordered group can also be expressed as a subgroup of a product {{math|'''T''' × ''L''}}, where {{mvar|L}} is a linearly ordered group. If a cyclically ordered group is Archimedean or compact, it can be embedded in {{math|'''T'''}} itself.{{sfn|Świerczkowski|1959a|pp=161–162}}\n\n===Modified axioms===\nA [[partial cyclic order]] is a ternary relation that generalizes a (total) cyclic order in the same way that a [[partial order]] generalizes a [[total order]]. It is cyclic, asymmetric, and transitive, but it need not be total. An [[order variety]] is a partial cyclic order that satisfies an additional ''spreading'' axiom {{Citation needed|date=March 2018}}. Replacing the asymmetry axiom with a complementary version results in the definition of a ''co-cyclic order''. Appropriately total co-cyclic orders are related to cyclic orders in the same way that {{math|≤}} is related to {{math|<}}.\n\nA cyclic order obeys a relatively strong 4-point transitivity axiom. One structure that weakens this axiom is a [[CC system]]: a ternary relation that is cyclic, asymmetric, and total, but generally not transitive. Instead, a CC system must obey a 5-point transitivity axiom and a new ''interiority'' axiom, which constrains the 4-point configurations that violate cyclic transitivity.{{sfn|Knuth|1992|p=4}}\n\nA cyclic order is required to be symmetric under cyclic permutation, {{math|[''a'', ''b'', ''c''] ⇒ [''b'', ''c'', ''a'']}}, and asymmetric under reversal: {{math|[''a'', ''b'', ''c''] ⇒ ¬[''c'', ''b'', ''a'']}}. A ternary relation that is ''asymmetric'' under cyclic permutation and ''symmetric'' under reversal, together with appropriate versions of the transitivity and totality axioms, is called a [[betweenness relation]]. A [[separation relation]] is a [[quaternary relation]] that can be thought of as a cyclic order without an orientation. The relationship between a circular order and a [[separation relation]] is analogous to the relationship between a linear order and a betweenness relation.{{sfn|Huntington|1935}}\n\n==Symmetries and model theory==\n{{Harvtxt|Evans|Macpherson|Ivanov|1997}} provide a model-theoretic description of the covering maps of cycles.\n\n{{Harvs|txt|last=Tararin|year=2001|year2=2001}} studies groups of automorphisms of cycles with various [[transitive action|transitivity]] properties. {{Harvtxt|Giraudet|Holland|2002}} characterize cycles whose full automorphism groups act [[free transitive action|freely and transitively]]. {{Harvtxt|Campero-Arena|Truss|2009}} characterize [[countable set|countable]] [[colored set|colored]] cycles whose automorphism groups act transitively. {{Harvtxt|Truss|2009}} studies the automorphism group of the unique (up to isomorphism) countable dense cycle.\n\n{{Harvtxt|Kulpeshov|Macpherson|2005}} study [[minimality]] conditions on circularly ordered [[structure (mathematical logic)|structure]]s, i.e. models of first-order languages that include a cyclic order relation. These conditions are analogues of [[o-minimal theory|o-minimality]] and [[Weakly o-minimal structure|weak o-minimality]] for the case of linearly ordered structures. {{Harvs|txt|last=Kulpeshov|year=2006|year2=2009}} continues with some characterizations of [[omega-categorical theory|ω-categorical]] structures.{{sfn|Macpherson|2011}}\n\n==Cognition==\n[[Hans Freudenthal]] has emphasized the role of cyclic orders in cognitive development, as a contrast to [[Jean Piaget]] who addresses only linear orders. Some experiments have been performed to investigate the mental representations of cyclically ordered sets, such as the months of the year.\n\n==Notes on usage==\n{{note|cyclic order|cyclic order}}The relation may be called a ''cyclic order'' {{Harv|Huntington|1916|p=630}}, a ''circular order'' {{Harv|Huntington|1916|p=630}}, a ''cyclic ordering'' {{Harv|Kok|1973|p=6}}, or a ''circular ordering'' {{Harv|Mosher|1996|p=109}}. Some authors call such an ordering a ''total cyclic order'' {{Harv|Isli|Cohn|1998|p=643}}, a ''complete cyclic order'' {{Harv|Novák|1982|p=462}}, a ''linear cyclic order'' {{Harv|Novák|1984|p=323}}, or an ''l-cyclic order'' or ℓ-''cyclic order'' {{Harv|Černák|2001|p=32}}, to distinguish from the broader class of [[partial cyclic order]]s, which they call simply ''cyclic orders''. Finally, some authors may take ''cyclic order'' to mean an unoriented quaternary [[separation relation]] {{Harv|Bowditch|1998|p=155}}.\n\n{{note|cycle|cycle}}A set with a cyclic order may be called a ''cycle'' {{Harv|Novák|1982|p=462}} or a ''circle'' {{Harv|Giraudet|Holland|2002|p=1}}. The above variations also appear in adjective form: ''cyclically ordered set'' (''cyklicky uspořádané množiny'', {{Harvnb|Čech|1936|p=23}}), ''circularly ordered set'', ''total cyclically ordered set'', ''complete cyclically ordered set'', ''linearly cyclically ordered set'', ''l-cyclically ordered set'', ℓ-''cyclically ordered set''. All authors agree that a cycle is totally ordered.\n\n{{note|ternary relation|ternary relation}}There are a few different symbols in use for a cyclic relation. {{Harvtxt|Huntington|1916|p=630}} uses concatenation: {{math|''ABC''}}. {{Harvtxt|Čech|1936|p=23}} and {{Harv|Novák|1982|p=462}} use ordered triples and the set membership symbol: {{math|(''a'', ''b'', ''c'') ∈ ''C''}}. {{Harvtxt|Megiddo|1976|p=274}} uses concatenation and set membership: {{math|''abc'' ∈ ''C''}}, understanding {{math|''abc''}} as a cyclically ordered triple. The literature on groups, such as {{Harvtxt|Świerczkowski|1959a|p=162}} and {{Harvtxt|Černák|Jakubík|1987|p=157}}, tend to use square brackets: {{math|[''a'', ''b'', ''c'']}}. {{Harvtxt|Giraudet|Holland|2002|p=1}} use round parentheses: {{math|(''a'', ''b'', ''c'')}}, reserving square brackets for a betweenness relation. {{Harvtxt|Campero-Arena|Truss|2009|p=1}} use a function-style notation: {{math|''R''(''a'', ''b'', ''c'')}}. [[#CITEREFRieger1947|Rieger (1947)]], cited after {{Harvnb|Pecinová|2008|p=82}}) uses a \"less-than\" symbol as a delimiter: {{math|< ''x'', ''y'', ''z'' <}}. Some authors use infix notation: {{math|''a'' < ''b'' < ''c''}}, with the understanding that this does not carry the usual meaning of {{math|''a'' < ''b''}} and {{math|''b'' < ''c''}} for some binary relation < {{Harv|Černy|1978|p=262}}. {{Harvtxt|Weinstein|1996|p=81}} emphasizes the cyclic nature by repeating an element: {{math|''p'' ↪ ''r'' ↪ ''q'' ↪ ''p''}}.\n\n{{note|embedding|embedding}}{{Harvtxt|Novák|1984|p=332}} calls an embedding an \"isomorphic embedding\".\n\n{{note|roll|roll}}In this case, {{Harvtxt|Giraudet|Holland|2002|p=2}} write that {{mvar|K}} is {{mvar|L}} \"rolled up\".\n\n{{note|orbit space|orbit space}}The map ''T'' is called ''archimedean'' by {{Harvtxt|Bowditch|2004|p=33}}, ''coterminal'' by {{Harvtxt|Campero-Arena|Truss|2009|p=582}}, and a ''translation'' by {{Harvtxt|McMullen|2009|p=10}}.\n\n{{note|universal cover|universal cover}}{{Harvtxt|McMullen|2009|p=10}} calls {{math|'''Z''' × ''K''}} the \"universal cover\" of {{mvar|K}}. {{Harvtxt|Giraudet|Holland|2002|p=3}} write that {{mvar|K}} is {{math|'''Z''' × ''K''}} \"coiled\". {{Harvtxt|Freudenthal|Bauer|1974|p=10}} call {{math|'''Z''' × ''K''}} the \"∞-times covering\" of {{mvar|K}}. Often this construction is written as the anti-lexicographic order on {{math|''K'' × '''Z'''}}.\n\n==References==\n;Citations\n{{Reflist|22em}}\n\n;Bibliography\n{{Refbegin}}\n*{{Citation |last=Bass |first=Hyman |authorlink=Hyman Bass |last2=Otero-Espinar |first2=Maria Victoria |last3=Rockmore |first3=Daniel |last4=Tresser |first4=Charles |year=1996 |title=Cyclic renormallzatlon and automorphism groups of rooted trees |series=Lecture Notes in Mathematics |volume=1621 |publisher=Springer |isbn=978-3-540-60595-9 |doi=10.1007/BFb0096321}}\n*{{Citation |last=Bowditch |first=Brian H. |authorlink=Brian Bowditch |date=September 1998 |title=Cut points and canonical splittings of hyperbolic groups |journal=[[Acta Mathematica]] |volume=180 |issue=2 |pages=145–186 |doi=10.1007/BF02392898 |url=http://www.kryakin.com/files/Acta_Mat_(2_55)/acta197_151/180/180_1.pdf |accessdate=25 April 2011 |deadurl=yes |archiveurl=https://web.archive.org/web/20120322145318/http://www.kryakin.com/files/Acta_Mat_%282_55%29/acta197_151/180/180_1.pdf |archivedate=22 March 2012 |df= }}\n*{{Citation |last=Bowditch |first=Brian H. |date=November 2004 |title=Planar groups and the Seifert conjecture |journal=Journal für die Reine und Angewandte Mathematik |volume=576 |issue=576 |pages=11–62 |doi=10.1515/crll.2004.084 |url=http://www.warwick.ac.uk/~masgak/abstracts/pla.html |accessdate=31 May 2011}}\n*{{Citation |last=Brown |first=Kenneth S. |date=February 1987 |title=Finiteness properties of groups |journal=Journal of Pure and Applied Algebra |volume=44 |issue=1–3 |pages=45–75 |doi=10.1016/0022-4049(87)90015-6 |url=http://www.math.cornell.edu/~kbrown/scan/1987.0044.0045.pdf |accessdate=21 May 2011}}\n*{{Citation |last=Calegari |first=Danny |authorlink=Danny Calegari |date=13 December 2004 |title=Circular groups, planar groups, and the Euler class |journal=Geometry & Topology Monographs |volume=7 |pages=431–491 |doi=10.2140/gtm.2004.7.431 |arxiv=math/0403311 |url=http://emis.math.ca/journals/UW/gt/ftp/main/m7/m7-15.pdf |accessdate=30 April 2011|series=Geometry and Topology Monographs |citeseerx=10.1.1.235.122 }}\n*{{Citation |last=Calegari |first=Danny |last2=Dunfield |first2=Nathan M. |date=April 2003 |title=Laminations and groups of homeomorphisms of the circle |journal=Inventiones Mathematicae |volume=152 |issue=1 |pages=149–204 |doi=10.1007/s00222-002-0271-6 |arxiv=math/0203192|bibcode=2003InMat.152..149D }}\n*{{Citation |last=Campero-Arena |first=G. |last2=Truss |first2=John K. |date=April 2009 |title=1-transitive cyclic orderings |journal=[[Journal of Combinatorial Theory, Series A]] |volume=116 |issue=3 |pages=581–594 |doi=10.1016/j.jcta.2008.08.006 |url=http://www.maths.leeds.ac.uk/pure/staff/truss/g.pdf |accessdate=25 April 2011}}\n*{{Citation |last=Čech |first=Eduard |authorlink=Eduard Čech |year=1936 |title=Bodové množiny |language=Czech |location=Prague |publisher=Jednota Československých matematiků a fysiků |url=http://dml.cz/handle/10338.dmlcz/400435 |accessdate=9 May 2011 |hdl=10338.dmlcz/400435}}\n*{{Citation |last=Černák |first=Štefan |year=2001 |title=Cantor extension of a half linearly cyclically ordered group |journal=Discussiones Mathematicae - General Algebra and Applications |volume=21 |issue=1 |pages=31–46 |url=http://lord.uz.zgora.pl:7777/bib/bibwww.pdf?nIdA=4493 |accessdate=22 May 2011 |doi=10.7151/dmgaa.1025 }}{{dead link|date=August 2017 |bot=InternetArchiveBot |fix-attempted=yes }}\n*{{Citation |last=Černák |first=Štefan |last2=Jakubík |first2=Ján |year=1987 |title=Completion of a cyclically ordered group |journal=Czechoslovak Mathematical Journal |volume=37 |issue=1 |pages=157–174 |mr=875137 |zbl=0624.06021 |hdl=10338.dmlcz/102144 |url=http://dspace.dml.cz/bitstream/handle/10338.dmlcz/102144/CzechMathJ_37-1987-1_16.pdf |accessdate=25 April 2011 |archive-url=https://web.archive.org/web/20110815071654/http://dspace.dml.cz/bitstream/handle/10338.dmlcz/102144/CzechMathJ_37-1987-1_16.pdf |archive-date=15 August 2011 |dead-url=yes |df=dmy-all }}\n*{{Citation |last=Černy |first=Ilja |year=1978 |title=Cuts in simple connected regions and the cyclic ordering of the system of all boundary elements |journal=Časopis Pro Pěstování Matematiky |volume=103 |issue=3 |pages=259–281 |hdl=10338.dmlcz/117983 |url=http://dml.cz/bitstream/handle/10338.dmlcz/117983/CasPestMat_103-1978-3_6.pdf |accessdate=11 May 2011}}\n*{{Citation |last=Courcelle |first=Bruno |authorlink=Bruno Courcelle |date=21 August 2003 |chapter=2.3 Circular order |editor-first=Dietmar |editor-last=Berwanger |editor2-first=Erich |editor2-last=Grädel |title=Problems in Finite Model Theory |page=12 |chapter-url=http://www-mgi.informatik.rwth-aachen.de/FMT/problems.pdf |accessdate=15 May 2011 |archive-url=https://web.archive.org/web/20110527152858/http://www-mgi.informatik.rwth-aachen.de/FMT/problems.pdf |archive-date=27 May 2011 |dead-url=yes |df=dmy-all }}\n*[[H. S. M. Coxeter]] (1949) ''The Real Projective Plane'', chapter 3: Order and continuity.\n*{{Citation |last=Evans |first=David M. |last2=Macpherson |first2=Dugald |last3=Ivanov |first3=Alexandre A. |year=1997 |chapter=Finite Covers |pages=1–72 |editor-last=Evans |editor-first=David M. |title=Model theory of groups and automorphism groups: Blaubeuren, August 1995 |series=London Mathematical Society Lecture Note Series |volume=244 |publisher=Cambridge University Press |isbn=978-0-521-58955-0 |chapter-url=http://www.amsta.leeds.ac.uk/Pure/preprints/hdm/hdm5.ps |accessdate=5 May 2011}}\n*{{Citation |last=Freudenthal |first=Hans |authorlink=Hans Freudenthal |title=Mathematics as an educational task |year=1973 |publisher=D. Reidel |isbn=978-90-277-0235-7}}\n*{{Citation |last=Freudenthal |first=Hans |first2=A. |last2=Bauer |chapter=Geometry—A Phenomenological Discussion |year=1974 |editor-last=Behnke |editor-first=Heinrich |editor2-first=S. H. |editor2-last=Gould |title=Fundamentals of mathematics |volume=2 |pages=3–28 |publisher=MIT Press |isbn=978-0-262-02069-5}}\n*{{Citation |last=Freudenthal |first=Hans |year=1983 |title=Didactical phenomenology of mathematical structures |publisher=D. Reidel |isbn=978-90-277-1535-7}}\n*{{Citation |last=Giraudet |first=Michèle |last2=Holland |first2=W. Charles |date=September 2002 |title=Ohkuma Structures |journal=[[Order (journal)|Order]] |volume=19 |issue=3 |pages=223–237 |doi=10.1023/A:1021249901409 |url=http://web.mac.com/chollan1/iWeb/Site/publications_files/55%20Ohkuma%20structures.pdf |accessdate=28 April 2011 }}{{dead link|date=September 2017 |bot=InternetArchiveBot |fix-attempted=yes }}\n*{{Citation |last=Huntington |first=Edward V. |authorlink=Edward Vermilye Huntington |date=1 November 1916 |title=A Set of Independent Postulates for Cyclic Order |journal=Proceedings of the National Academy of Sciences of the United States of America |volume=2 |issue=11 |pages=630–631 |doi=10.1073/pnas.2.11.630|pmid=16576195 |pmc=1091120 |bibcode=1916PNAS....2..630H }}\n*{{Citation |last=Huntington |first=Edward V. |date=15 February 1924 |title=Sets of Completely Independent Postulates for Cyclic Order |journal=Proceedings of the National Academy of Sciences of the United States of America |volume=10 |issue=2 |pages=74–78 |doi=10.1073/pnas.10.2.74|pmid=16576785 |bibcode=1924PNAS...10...74H |pmc=1085517 }}\n*{{Citation |last=Huntington |first=Edward V. |date=July 1935 |title=Inter-Relations Among the Four Principal Types of Order |journal=Transactions of the American Mathematical Society |volume=38 |issue=1 |pages=1–9 |doi=10.1090/S0002-9947-1935-1501800-1 |url=http://www.ams.org/journals/tran/1935-038-01/S0002-9947-1935-1501800-1/S0002-9947-1935-1501800-1.pdf |accessdate=8 May 2011}}\n*{{Citation |last=Isli |first=Amar |last2=Cohn |first2=Anthony G. |year=1998 |chapter=An algebra for cyclic ordering of 2D orientations |title=AAAI '98/IAAI '98 Proceedings of the fifteenth national/tenth conference on Artificial intelligence/Innovative applications of artificial intelligence |isbn=978-0-262-51098-1 |chapter-url=https://www.aaai.org/Papers/AAAI/1998/AAAI98-091.pdf |accessdate=23 May 2011}}\n*{{Citation |last=Knuth |first=Donald E. |authorlink=Donald Knuth |year=1992 |title=Axioms and Hulls |series=Lecture Notes in Computer Science |volume=606 |location=Heidelberg |publisher=Springer-Verlag |pages=ix+109 |isbn=978-3-540-55611-4 |url=http://www-cs-faculty.stanford.edu/~uno/aah.html |accessdate=5 May 2011 |doi=10.1007/3-540-55611-7}}\n*{{Citation |last=Kok |first=H. |year=1973 |title=Connected orderable spaces |location=Amsterdam |publisher=[[Mathematisch Centrum]] |isbn=978-90-6196-088-1}}\n*{{Citation |last=Kuhlmann |first=Salma |last2=Marshall |first2=Murray |last3=Osiak |first3=Katarzyna |date=1 June 2011 |title=Cyclic 2-structures and spaces of orderings of power series fields in two variables |journal=Journal of Algebra |volume=335 |issue=1 |pages=36–48 |doi=10.1016/j.jalgebra.2011.02.026 |url=http://math.usask.ca/~marshall/r%5B%5Bx,y%5D%5D_20,11.pdf |accessdate=11 May 2011}}\n*{{Citation |last=Kulpeshov |first=Beibut Sh. |date=December 2006 |title=On ℵ<sub>0</sub>-categorical weakly circularly minimal structures |journal=Mathematical Logic Quarterly |volume=52 |issue=6 |pages=555–574 |doi=10.1002/malq.200610014}}\n*{{Citation |last=Kulpeshov |first=Beibut Sh. |date=March 2009 |title=Definable functions in the ℵ<sub>0</sub>-categorical weakly circularly minimal structures |journal=Siberian Mathematical Journal |volume=50 |issue=2 |pages=282–301 |doi=10.1007/s11202-009-0034-3}} Translation of {{Citation |last=Kulpeshov |year=2009 |title=Определимые функции в ℵ<sub>0</sub>-категоричных слабо циклически минимальных структурах |journal=Sibirskiĭ Matematicheskiĭ Zhurnal |volume=50 |issue=2 |pages=356–379 |url=http://mi.mathnet.ru/eng/smj1965 |accessdate=24 May 2011}}\n*{{Citation |last=Kulpeshov |first=Beibut Sh. |last2=Macpherson |first2=H. Dugald |date=July 2005 |title=Minimality conditions on circularly ordered structures |journal=Mathematical Logic Quarterly |volume=51 |issue=4 |pages=377–399 |doi=10.1002/malq.200410040 |mr=2150368}}\n*{{Citation |last=Macpherson |first=H. Dugald |year=2011 |title=A survey of homogeneous structures |journal=Discrete Mathematics |doi=10.1016/j.disc.2011.01.024 |url=http://www.amsta.leeds.ac.uk/pure/staff/macpherson/homog_final2.pdf |accessdate=28 April 2011 |volume=311 |issue=15 |pages=1599–1634}}\n*{{Citation |last=McMullen |first=Curtis T. |year=2009 |title=Ribbon R-trees and holomorphic dynamics on the unit disk |journal=Journal of Topology |volume=2 |issue=1 |pages=23–76 |doi=10.1112/jtopol/jtn032 |url=http://www.math.harvard.edu/~ctm/papers/home/text/papers/rtrees/rtrees.pdf |accessdate=15 May 2011|citeseerx=10.1.1.139.8850 }}\n*{{Citation |last=Megiddo |first=Nimrod |date=March 1976 |title=Partial and complete cyclic orders |journal=[[Bulletin of the American Mathematical Society]] |volume=82 |issue=2 |pages=274–276 |doi=10.1090/S0002-9904-1976-14020-7 |url=http://www.ams.org/journals/bull/1976-82-02/S0002-9904-1976-14020-7/S0002-9904-1976-14020-7.pdf |accessdate=30 April 2011}}\n*{{Citation |last=Morton |first=James |last2=Pachter |first2=Lior|author2-link= Lior Pachter |last3=Shiu |first3=Anne |last4=Sturmfels |first4=Bernd|author4-link=Bernd Sturmfels|date=January 2007 |title=The Cyclohedron Test for Finding Periodic Genes in Time Course Expression Studies |journal=Statistical Applications in Genetics and Molecular Biology |volume=6 |issue=1 |doi=10.2202/1544-6115.1286 |pmid=17764440 |arxiv=q-bio/0702049}}\n*{{Citation |last=Mosher |first=Lee |year=1996 |chapter=A user's guide to the mapping class group: once-punctured surfaces |editor-last=Baumslag |editor-first=Gilbert |title=Geometric and computational perspectives on infinite groups |series=DIMACS |volume=25 |publisher=AMS Bookstore |pages=101–174 |isbn=978-0-8218-0449-0 |arxiv=math/9409209 |bibcode=1994math......9209M }}\n*{{Citation |last=Novák |first=Vítězslav |year=1982 |title=Cyclically ordered sets |journal=Czechoslovak Mathematical Journal |volume=32 |issue=3 |pages=460–473 |hdl=10338.dmlcz/101821 |url=http://dml.cz/bitstream/handle/10338.dmlcz/101821/CzechMathJ_32-1982-3_12.pdf |accessdate=30 April 2011}}\n*{{Citation |last=Novák |first=Vítězslav |year=1984 |title=Cuts in cyclically ordered sets |journal=Czechoslovak Mathematical Journal |volume=34 |issue=2 |pages=322–333 |hdl=10338.dmlcz/101955 |url=http://dml.cz/bitstream/handle/10338.dmlcz/101955/CzechMathJ_34-1984-2_17.pdf |accessdate=30 April 2011}}\n*{{Citation |last=Novák |first=Vítězslav |last2=Novotný |first2=Miroslav |year=1987 |title=On completion of cyclically ordered sets |journal=Czechoslovak Mathematical Journal |volume=37 |issue=3 |pages=407–414 |hdl=10338.dmlcz/102168 |url=http://dspace.dml.cz/bitstream/handle/10338.dmlcz/102168/CzechMathJ_37-1987-3_6.pdf |accessdate=25 April 2011 |archive-url=https://web.archive.org/web/20110815071743/http://dspace.dml.cz/bitstream/handle/10338.dmlcz/102168/CzechMathJ_37-1987-3_6.pdf |archive-date=15 August 2011 |dead-url=yes |df=dmy-all }}\n*{{Citation |last=Pecinová-Kozáková |first=Eliška |year=2005 |chapter=Ladislav Svante Rieger and His Algebraic Work |editor-last=Safrankova |editor-first=Jana |title=WDS 2005 - Proceedings of Contributed Papers, Part I |location=Prague |publisher=[[Matfyzpress]] |isbn=978-80-86732-59-6 |pages=190–197 |citeseerx=10.1.1.90.2398 }}\n*{{Citation |last=Pecinová |first=Eliška |year=2008 |title=Ladislav Svante Rieger (1916–1963) |language=Czech |series=Dějiny matematiky |volume=36 |location=Prague |publisher=Matfyzpress |isbn=978-80-7378-047-0 |url=http://dml.cz/handle/10338.dmlcz/400757 |accessdate=9 May 2011 |hdl=10338.dmlcz/400757}}\n*{{Citation |last=Rieger |first=L. S. |authorlink=Ladislav Rieger |year=1947 |title=О uspořádaných a cyklicky uspořádaných grupách II (On ordered and cyclically ordered groups II) |language=Czech |journal=Věstník Královské české Spolecnosti Nauk, Třída Mathematicko-přírodovědná (Journal of the Royal Czech Society of Sciences, Mathematics and Natural History) |issue=1 |pages=1–33}}\n*{{Citation |last=Roll |first=J. Blair |year=1993 |title=Locally partially ordered groups |journal=Czechoslovak Mathematical Journal |volume=43 |issue=3 |pages=467–481 |hdl=10338.dmlcz/128411 |url=http://dml.cz/bitstream/handle/10338.dmlcz/128411/CzechMathJ_43-1993-3_8.pdf |accessdate=30 April 2011}}\n*{{Citation |last=Stasheff |first=Jim |authorlink=Jim Stasheff |year=1997 |chapter=From operads to 'physically' inspired theories |editor-last=Loday |editor-first=Jean-Louis |editor2-last=Stasheff |editor2-first=James D. |editor3-last=Voronov |editor3-first=Alexander A. |title=Operads: Proceedings of Reneassance Conferences |series=Contemporary Mathematics |volume=202 |pages=53–82 |publisher=AMS Bookstore |isbn=978-0-8218-0513-8 |chapter-url=http://www.math.unc.edu/Faculty/jds/operadchik.ps |accessdate=1 May 2011 |deadurl=yes |archiveurl=https://web.archive.org/web/19970523172846/http://www.math.unc.edu/Faculty/jds/operadchik.ps |archivedate=23 May 1997 |df= }}\n*{{Citation |last=Świerczkowski |first=S. |year=1959a |title=On cyclically ordered groups |journal=Fundamenta Mathematicae |volume=47 |issue=2 |pages=161–166 |url=http://matwbn.icm.edu.pl/ksiazki/fm/fm47/fm4718.pdf |accessdate=2 May 2011|doi=10.4064/fm-47-2-161-166 }}\n*{{Citation |last=Tararin |first=Valeri Mikhailovich |year=2001 |title=On Automorphism Groups of Cyclically Ordered Sets |journal=Siberian Mathematical Journal |volume=42 |issue=1 |pages=190–204 |doi=10.1023/A:1004866131580}}. Translation of {{Citation |last=Tamarin |year=2001 |script-title=ru:О группах автоморфизмов циклически упорядоченных множеств |language=Russian |journal=Sibirskii Matematicheskii Zhurnal |volume=42 |issue=1 |pages=212–230 |url=http://mi.mathnet.ru/eng/smj1484 |accessdate=30 April 2011}}\n*{{Citation |last=Tararin |first=Valeri Mikhailovich |year=2002 |title=On c-3-Transitive Automorphism Groups of Cyclically Ordered Sets |journal=Mathematical Notes |volume=71 |issue=1 |pages=110–117 |doi=10.1023/A:1013934509265}}. Translation of {{Citation |last=Tamarin |year=2002 |title=О c-3-транзитивных группах автоморфизмов циклически упорядоченных множеств |journal=Matematicheskie Zametki |volume=71 |issue=1 |pages=122–129 |doi=10.4213/mzm333}}\n*{{Citation |last=Truss |first=John K. |year=2009 |title=On the automorphism group of the countable dense circular order |journal=[[Fundamenta Mathematicae]] |volume=204 |issue=2 |pages=97–111 |doi=10.4064/fm204-2-1 |url=http://www.maths.leeds.ac.uk/Pure/staff/truss/mnew.pdf |accessdate=25 April 2011}}\n*{{Citation |last=Viro |first=Oleg |authorlink=Oleg Viro |last2=Ivanov |first2=Oleg |last3=Netsvetaev |first3=Nikita |last4=Kharlamov |first4=Viatcheslav |year=2008 |title=Elementary topology: problem textbook |edition=1st English |publisher=[[AMS Bookstore]] |isbn=978-0-8218-4506-6 |chapter-url=http://www.ams.org/bookstore/pspdf/mbk-54-prev.pdf |accessdate=25 April 2011 |chapter=8. Cyclic Orders |pages=42–44}}\n*{{Citation |last=Weinstein |first=Tilla |date=July 1996 |title=An introduction to Lorentz surfaces |series=De Gruyter Expositions in Mathematics |volume=22 |publisher=Walter de Gruyter |isbn=978-3-11-014333-1}}\n{{Refend}}\n\n==Further reading==\n{{Refbegin}}\n*{{Citation |last=Bhattacharjee |first=Meenaxi |last2=Macpherson |first2=Dugald |last3=Möller |first3=Rögnvaldur G. |last4=Neumann |first4=Peter M. |year=1998 |title=Notes on Infinite Permutation Groups |series=Lecture Notes in Mathematics |volume=1698 |publisher=Springer |doi=10.1007/BFb0092550 |pages=108–109|isbn=978-3-540-64965-6 }}\n*{{Citation |last=Bodirsky |first=Manuel |last2=Pinsker |first2=Michael |date=to appear |chapter=Reducts of Ramsey Structures |title=Model Theoretic Methods in Finite Combinatorics |publisher=AMS |series=Contemporary Mathematics |arxiv=1105.6073|bibcode=2011arXiv1105.6073B }}\n*{{Citation |last=Cameron |first=Peter J. |date=June 1976 |title=Transitivity of permutation groups on unordered sets |journal=Mathematische Zeitschrift |volume=148 |issue=2 |pages=127–139 |doi=10.1007/BF01214702}}\n*{{Citation |last=Cameron |first=Peter J. |date=June 1977 |title=Cohomological aspects of two-graphs |journal=Mathematische Zeitschrift |volume=157 |issue=2 |pages=101–119 |doi=10.1007/BF01215145}}\n*{{Citation |last=Cameron |first=Peter J. |year=1997 |chapter=The algebra of an age |pages=126–133 |editor-last=Evans |editor-first=David M. |title=Model theory of groups and automorphism groups: Blaubeuren, August 1995 |series=London Mathematical Society Lecture Note Series |volume=244 |publisher=Cambridge University Press |isbn=978-0-521-58955-0 |citeseerx = 10.1.1.39.2321 }}\n*{{Citation |last=Courcelle |first=Bruno |last2=Engelfriet |first2=Joost |date=April 2011 |title=Graph Structure and Monadic Second-Order Logic, a Language Theoretic Approach |publisher=Cambridge University Press |url=http://www.labri.fr/perso/courcell/Book/TheBook.pdf |accessdate=17 May 2011}}\n*{{Citation |last=Droste |first=M. |last2=Giraudet |first2=M. |last3=Macpherson |first3=D. |date=March 1995 |title=Periodic Ordered Permutation Groups and Cyclic Orderings |journal=Journal of Combinatorial Theory, Series B |volume=63 |issue=2 |pages=310–321 |doi=10.1006/jctb.1995.1022}}\n*{{Citation |last=Droste |first=M. |last2=Giraudet |first2=M. |last3=Macpherson |first3=D. |date=March 1997 |title=Set-Homogeneous Graphs and Embeddings of Total Orders |journal=Order |volume=14 |issue=1 |pages=9–20 |doi=10.1023/A:1005880810385 |citeseerx = 10.1.1.22.9135 }}<!--|accessdate=17 May 2011-->\n*{{Citation |last=Evans |first=David M. |date=17 November 1997 |title=Finite covers with finite kernels |journal=Annals of Pure and Applied Logic |volume=88 |issue=2–3 |pages=109–147 |doi=10.1016/S0168-0072(97)00018-3 |citeseerx = 10.1.1.57.5323 }}<!--|accessdate=5 May 2011-->\n*{{Citation |last=Ivanov |first=A. A. |date=January 1999 |title=Finite Covers, Cohomology and Homogeneous Structures |journal=Proceedings of the London Mathematical Society |volume=78 |issue=1 |pages=1–28 |doi=10.1112/S002461159900163X}}\n*{{Citation |last=Jakubík |first=Ján |year=2006 |title=On monotone permutations of ℓ-cyclically ordered sets |journal=Czechoslovak Mathematical Journal |volume=45 |issue=2 |pages=403–415 |hdl=10338.dmlcz/128075 |url=http://dml.cz/bitstream/handle/10338.dmlcz/128075/CzechMathJ_56-2006-2_10.pdf |accessdate=30 April 2011|doi=10.1007/s10587-006-0026-4 }}\n*{{Citation |last=Kennedy |first=Christine Cowan |date=August 1955 |title=On a cyclic ternary relation ... (M.A. Thesis) |publisher=Tulane University |oclc=16508645}}\n*{{Citation |last=Kónya |first=Eszter Herendine |year=2006 |title=A mathematical and didactical analysis of the concept of orientation |journal=Teaching Mathematics and Computer Science |volume=4 |issue=1 |pages=111–130 |url=http://tmcs.math.klte.hu/Contents/2006-Vol-IV-Issue-I/konya-abstract.pdf |archive-url=https://web.archive.org/web/20110726051043/http://tmcs.math.klte.hu/Contents/2006-Vol-IV-Issue-I/konya-abstract.pdf |dead-url=yes |archive-date=26 July 2011 |accessdate=17 May 2011 |doi=10.5485/TMCS.2006.0108 }}\n*{{Citation |last=Kónya |first=Eszter Herendine |year=2008 |chapter=Geometrical transformations and the concept of cyclic ordering |pages=102–108 |editor-last=Maj |editor-first=Bożena |editor2-last=Pytlak |editor2-first=Marta |editor3-last=Swoboda |editor3-first=Ewa |title=Supporting Independent Thinking Through Mathematical Education |publisher=Rzeszów University Press |isbn=978-83-7338-420-0 |chapter-url=http://www.cme.rzeszow.pl/pdf/part_2_3.pdf |accessdate=17 May 2011}}\n*{{Citation |last=Leloup |first=Gérard |date=February 2011 |title=Existentially equivalent cyclic ultrametric spaces and cyclically valued groups |journal=Logic Journal of the IGPL |volume=19 |issue=1 |pages=144–173 |doi=10.1093/jigpal/jzq024 |url=http://math.usask.ca/fvk/leloup3.pdf |accessdate=30 April 2011|citeseerx=10.1.1.152.7462 }}\n*{{Citation |last=Marongiu |first=Gabriele |year=1985 |title=Some remarks on the ℵ<sub>0</sub>-categoricity of circular orderings |language=Italian |journal=Unione Matematica Italiana. Bollettino. B. Serie VI |volume=4 |issue=3 |pages=883–900 |mr=0831297}}\n*{{Citation |last=McCleary |first=Stephen |last2=Rubin |first2=Matatyahu |date=6 October 2005 |title=Locally Moving Groups and the Reconstruction Problem for Chains and Circles |arxiv=math/0510122|bibcode=2005math.....10122M }}\n*{{Citation |last=Müller |first=G. |year=1974 |title=Lineare und zyklische Ordnung |journal=Praxis der Mathematik |volume=16 |pages=261–269 |mr=0429660}}\n*{{Citation |last=Rubin |first=M. |year=1996 |chapter=Locally moving groups and reconstruction problems |editor-last=Holland |editor-first=W. Charles |title=Ordered groups and infinite permutation groups |pages=121–157 |series=Mathematics and Its Applications |volume=354 |publisher=Kluwer |isbn=978-0-7923-3853-6}}\n*{{Citation |last=Świerczkowski |first=S. |year=1956 |title=On cyclic ordering relations |journal=Bulletin de l'Académie Polonaise des Sciences, Classe III |volume=4 |pages=585–586}}\n*{{Citation |last=Świerczkowski |first=S. |year=1959b |title=On cyclically ordered intervals of integers |journal=Fundamenta Mathematicae |volume=47 |issue=2 |pages=167–172 |url=http://matwbn.icm.edu.pl/ksiazki/fm/fm47/fm4719.pdf |accessdate=2 May 2011|doi=10.4064/fm-47-2-167-172 }}\n*{{Citation |last=Truss |first=J.K. |date=July 1992 |volume=s3-65 |issue=1 |title=Generic Automorphisms of Homogeneous Structures |journal=Proceedings of the London Mathematical Society |pages=121–141 |doi=10.1112/plms/s3-65.1.121}}\n{{Refend}}\n\n==External links==\n*{{nlab|id=cyclic+order|title=cyclic order}}\n*{{commons category inline|Cyclic order (mathematics)}}\n\n{{DEFAULTSORT:Cyclic Order}}\n[[Category:Order theory]]\n[[Category:Circles]]\n[[Category:Combinatorics]]"
    },
    {
      "title": "Dedekind cut",
      "url": "https://en.wikipedia.org/wiki/Dedekind_cut",
      "text": "{{For|the American record producer known professionally as Dedekind Cut|Fred Warmsley}}\n{{Refimprove|date=March 2011}}\n[[File:Dedekind cut- square root of two.png| thumb| right| 350px| Dedekind used his cut to construct the [[irrational number|irrational]], [[real number]]s.]]\n\nIn [[mathematics]], '''Dedekind cuts''', named after German mathematician [[Richard Dedekind]] but previously considered by [[Joseph Bertrand]]<ref>{{cite book|last=Bertrand|first=Joseph|title=Trait'e d'Arithmetique |url = https://gallica.bnf.fr/ark:/12148/bpt6k77735p/f209.image.r=%22joseph%20bertrand%22 |year=1849|at=page 203|quote=An incommensurable number can be defined only by indicating how the magnitude it expresses can be formed by means of unity. In what follows, we suppose that this definition consists of indicating which are the commensurable numbers smaller or larger than it ....}}</ref><ref>{{cite book |last=Spalt |first=Detlef |title=Eine kurze Geschichte der Analysis|year=2019|publisher=Springer|doi=10.1007/978-3-662-57816-2}}</ref>, are а method of [[construction of the real numbers]] from the [[rational number]]s. A Dedekind cut is a [[partition of a set|partition]] of the rational numbers into two non-empty [[Set (mathematics) | sets]] ''A'' and ''B'', such that all elements of ''A'' are less than all elements of ''B'', and ''A'' contains no [[greatest element]]. The set ''B'' may or may not have a smallest element among the rationals. If ''B'' has a smallest element among the rationals, the cut corresponds to that rational. Otherwise, that cut defines a unique irrational number which, loosely speaking, fills the \"gap\" between ''A'' and&nbsp;''B''.<ref>{{cite book|last=Dedekind |first=Richard |title=Continuity and Irrational Numbers |url=http://www.math.ubc.ca/~cass/courses/m446-05b/dedekind-book.pdf#page=15 |year=1872|at=Section IV |quote=Whenever, then, we have to do with a cut produced by no rational number, we create a new ''irrational'' number, which we regard as completely defined by this cut ... . From now on, therefore, to every definite cut there corresponds a definite rational or irrational number ....}}</ref> In other words, ''A'' contains every rational number less than the cut, and ''B'' contains every rational number greater than or equal to the cut. An irrational cut is equated to an irrational number which is in neither set. Every real number, rational or not, is equated to one and only one cut of rationals.\n\nDedekind cuts can be generalized from the rational numbers to any [[totally ordered set]] by defining a Dedekind cut as a partition of a totally ordered set into two non-empty parts ''A'' and ''B'', such that ''A'' is closed downwards (meaning that for all ''a'' in ''A'', ''x'' ≤ ''a'' implies that ''x'' is in ''A'' as well) and ''B'' is closed upwards, and ''A'' contains no greatest element. See also [[completeness (order theory)]].\n\nIt is straightforward to show that a Dedekind cut among the real numbers is uniquely defined by the corresponding cut among the rational numbers. Similarly, every cut of reals is identical to the cut produced by a specific real number (which can be identified as the smallest element of the ''B'' set). In other words, the [[number line]] where every [[real number]] is defined as a Dedekind cut of rationals is a [[Complete metric space|complete]] [[linear continuum|continuum]] without any further gaps.\n\nA similar construction to that used by Dedekind cuts was used in [[Euclid's Elements]] (book V, definition 5) to define proportional segments.\n\n== Definition ==\n\nA Dedekind cut is a partition of the rationals <math>\\mathbb{Q}</math> into two subsets ''A'' and ''B'' such that\n\n# <math>A</math> is nonempty.\n# <math>A \\neq \\mathbb{Q}</math>.\n# If <math>x, y \\in \\mathbb{Q}</math>, <math> x < y </math>, and <math> y \\in A </math>, then <math> x \\in A </math>. (<math>A</math> is \"closed downwards\".)\n# If <math> x \\in A </math>, then there exists a <math> y \\in A </math> such that <math> y > x </math>. (<math>A</math> does not contain a greatest element.)\n\nBy relaxing the first two requirements, we formally obtain the [[extended real number line]].\n\n== Representations ==\n\nIt is more symmetrical to use the (''A'',''B'') notation for Dedekind cuts, but each of ''A'' and ''B'' does determine the other. It can be a simplification, in terms of notation if nothing more, to concentrate on one \"half\" — say, the lower one — and call any downward closed set ''A'' without greatest element a \"Dedekind cut\".\n\nIf the ordered set ''S'' is complete, then, for every Dedekind cut (''A'', ''B'') of ''S'', the set ''B'' must have a minimal element ''b'', \nhence we must have that  ''A'' is the [[interval (mathematics)|interval]] (&minus;∞, ''b''), and ''B'' the interval [''b'', +∞).\nIn this case, we say that ''b'' ''is represented by'' the cut (''A'',''B'').\n\nThe important purpose of the Dedekind cut is to work with number sets that are ''not'' complete. The cut itself can represent a number not in the original collection of numbers (most often [[rational number]]s). The cut can represent a number ''b'', even though the numbers contained in the two sets ''A'' and ''B'' do not actually include the number ''b'' that their cut represents.\n\nFor example if ''A'' and ''B'' only contain [[rational numbers]], they can still be cut at {{radic|2}} by putting every negative rational number in ''A'', along with every non-negative number whose square is less than 2; similarly ''B'' would contain every positive rational number whose square is greater than or equal to 2. Even though there is no rational value for {{sqrt|2}}, if the rational numbers are partitioned into ''A'' and ''B'' this way, the partition itself represents an [[irrational number]].\n\n==Ordering of cuts==\nRegard one Dedekind cut (''A'', ''B'') as ''less than'' another Dedekind cut (''C'', ''D'') (of the same superset) if ''A'' is a proper subset of ''C''. Equivalently, if ''D'' is a proper subset of ''B'', the cut (''A'', ''B'') is again ''less than'' (''C'', ''D''). In this way, set inclusion can be used to represent the ordering of numbers, and all other relations (''greater than'', ''less than or equal to'', ''equal to'', and so on) can be similarly created from set relations.\n\nThe set of all Dedekind cuts is itself a linearly ordered set (of sets). Moreover, the set of Dedekind cuts has the [[least-upper-bound property]], i.e., every nonempty subset of it that has any upper bound has a ''least'' upper bound.  Thus, constructing the set of Dedekind cuts serves the purpose of embedding the original ordered set ''S'', which might not have had the least-upper-bound property, within a (usually larger) linearly ordered set that does have this useful property.\n\n==Construction of the real numbers==\n{{Cleanup|reason=Contains information outside the scope of the article|date=June 2015}}\n{{See also|Construction of the real numbers#Construction by Dedekind cuts}}\nA typical Dedekind cut of the [[rational number]]s <math>\\Q</math> is given by the partition <math>(A,B)</math> with\n\n:<math>A = \\{ a\\in\\mathbb{Q} : a^2 < 2 \\text{ or } a < 0 \\},</math>\n:<math>B = \\{ b\\in\\mathbb{Q} : b^2 \\ge 2 \\text{ and } b \\ge 0 \\}.</math><ref>In the second line, <math>\\ge</math> may be replaced by <math>></math> without any difference as there is no solution for <math>x^2 = 2</math> in <math>\\Q</math> and <math>b=0</math> is already forbidden by the first condition. This results in the equivalent expression\n:<math>B = \\{ b\\in\\mathbb{Q} : b^2 > 2 \\text{ and } b > 0 \\}.</math></ref>  \n\nThis cut represents the [[irrational number]] {{sqrt|2}} in Dedekind's construction. To establish this truly, one must show that this really is a cut and that it is the square root of two. However, neither claim is immediate. Showing that it is a cut requires showing that for any positive rational ''x'' with {{math|''x''<sup>2</sup>&nbsp;<&nbsp;2}}, there is a rational ''y'' with {{math|''x''&nbsp;<&nbsp;''y''}} and {{math|''y''<sup>2</sup>&nbsp;<&nbsp;2}}. The choice <math>y=\\frac{2x+2}{x+2}</math> works. Then we have a cut and it has a square no larger than 2, but to show equality requires showing that if ''r'' is any rational number less than 2, then there is positive ''x'' in ''A'' with {{math|''r''&nbsp;<&nbsp;''x''<sup>2</sup>}}.\n\nNote that the equality {{math|1=''b''<sup>2</sup>&nbsp;=&nbsp;2}} cannot hold since [[Square root of 2#Proofs of irrationality|{{sqrt|2}} is not rational]].\n\n==Generalizations==\nA construction similar to Dedekind cuts is used for the construction of [[surreal number]]s.\n\n===Partially ordered sets===\n{{Main|Dedekind–MacNeille completion}}\nMore generally, if ''S'' is a [[partially ordered set]], a ''completion'' of ''S'' means a [[complete lattice]] ''L'' with an order-embedding of ''S'' into ''L''. The notion of ''complete lattice'' generalizes the least-upper-bound property of the reals.\n\nOne completion of ''S'' is the set of its ''downwardly closed'' subsets, ordered by [[subset|inclusion]]. A related completion that preserves all existing sups and infs of ''S'' is obtained by the following construction: For each subset ''A'' of ''S'', let ''A''<sup>u</sup> denote the set of upper bounds of ''A'', and let ''A''<sup>l</sup> denote the set of lower bounds of ''A''. (These operators form a [[Galois connection]].) Then the [[Dedekind–MacNeille completion]] of ''S'' consists of all subsets ''A'' for which (''A''<sup>u</sup>)<sup>l</sup> = ''A''; it is ordered by inclusion. The Dedekind-MacNeille completion is the smallest complete lattice with ''S'' embedded in it.\n\n==Notes==\n{{reflist}}\n\n==References==\n*Dedekind, Richard, ''Essays on the Theory of Numbers'', \"Continuity and Irrational Numbers,\" Dover: New York, {{ISBN|0-486-21010-3}}. Also [http://www.gutenberg.org/etext/21016 available] at Project Gutenberg.\n\n==External links==\n* {{springer|title=Dedekind cut|id=p/d030530}}\n\n\n{{Rational numbers}}\n \n[[Category:Order theory]]\n[[Category:Rational numbers]]"
    },
    {
      "title": "Dedekind–MacNeille completion",
      "url": "https://en.wikipedia.org/wiki/Dedekind%E2%80%93MacNeille_completion",
      "text": "{{redirect|Dedekind completion|some other related concepts|Dedekind completeness}}\n\n[[File:Dedekind-Macneille completion.svg|thumb|240px|The [[Hasse diagram]] of a partially ordered set (left) and its Dedekind–MacNeille completion (right).]]\nIn [[order theory|order-theoretic mathematics]], the '''Dedekind–MacNeille completion''' of a [[partially ordered set]] (also called the '''completion by cuts''' or '''normal completion''')<ref>{{harvtxt|Davey| Priestley|2002|p=166}}; {{harvtxt|Siegfried|Schröder|2003|p=119}}.</ref> is the smallest [[complete lattice]] that contains the given partial order. It is named after [[Holbrook Mann MacNeille]] whose 1937 paper first defined and constructed it, and after [[Richard Dedekind]] because its construction generalizes the [[Dedekind cut]]s used by Dedekind to construct the [[real number]]s from the [[rational number]]s.\n\n==Order embeddings and lattice completions==\nA [[partially ordered set]] consists of a set of elements together with a [[binary relation]] {{math|''x'' ≤ ''y''}} on pairs of elements that is reflexive ({{math|''x'' ≤ ''x''}} for every ''x''), transitive (if {{math|''x'' ≤ ''y''}} and {{math|''y'' ≤ ''z''}} then {{math|''x'' ≤ ''z''}}), and antisymmetric (if both {{math|''x'' ≤ ''y''}} and {{math|''y'' ≤ ''x''}} hold, then {{math|1=''x'' = ''y''}}). The usual numeric orderings on the integers or real numbers satisfy these properties; however, unlike the orderings on the numbers, a partial order may have two elements that are ''incomparable'': neither {{math|''x'' ≤ ''y''}} nor {{math|''y'' ≤ ''x''}} holds. Another familiar example of a partial ordering is the inclusion ordering ⊆ on pairs of sets.\n\nIf {{mvar|S}} is a partially ordered set, a ''completion'' of {{mvar|S}} means a [[complete lattice]] {{mvar|L}} with an [[order-embedding]] of {{mvar|S}} into {{mvar|L}}.<ref>{{harvtxt|Siegfried|Schröder|2003}}, definition 5.3.1, p.&nbsp;119.</ref> The notion of a complete lattice means that every subset of elements of {{mvar|L}} has [[infimum and supremum]]; this generalizes the analogous properties of the [[real number]]s. The notion of an order-embedding enforces the requirements that distinct elements of {{mvar|S}} must be mapped to distinct elements of {{mvar|L}}, and that each pair of elements in {{mvar|S}} has the same ordering in {{mvar|L}} as they do in {{mvar|S}}. The [[extended real number line]] (real numbers together with +∞ and &minus;∞) is a completion in this sense of the rational numbers: the set of rational numbers {3, 3.1, 3.14, 3.141, 3.1415, 3.14159, ...} does not have a rational least upper bound, but in the real numbers it has the least upper bound {{pi}}.\n\nA given partially ordered set may have several different completions. For instance, one completion of any partially ordered set {{mvar|S}} is the set of its [[lower set|downwardly closed subsets]] ordered by [[subset|inclusion]]. {{mvar|S}} is embedded in this (complete) lattice by mapping each element {{mvar|x}} to the lower set of elements that are less than or equal to {{mvar|x}}. The result is a [[distributive lattice]] and is used in [[Birkhoff's representation theorem]]. However, it may have many more elements than are needed to form a completion of {{mvar|S}}.<ref>{{citation|title=Concept data analysis: theory and applications|first1=Claudio|last1=Carpineto|first2=Giovanni|last2=Romano|publisher=John Wiley and Sons|year=2004|isbn=978-0-470-85055-8|page=10}}.</ref> Among all possible lattice completions, the Dedekind–MacNeille completion is the smallest complete lattice with {{mvar|S}} embedded in it.<ref name=\"smallest-completion\"/>\n\n==Definition==\nFor each subset {{mvar|A}} of a partially ordered set {{mvar|S}}, let {{math|''A''<sup>u</sup>}} denote the set of upper bounds of {{mvar|A}}; that is, an element {{mvar|x}} of {{mvar|S}} belongs to {{math|''A''<sup>u</sup>}} whenever {{mvar|x}} is greater than or equal to every element in {{mvar|A}}. Symmetrically, let {{math|''A''<sup>l</sup>}} denote the set of lower bounds of {{mvar|A}}, the elements that are less than or equal to every element in {{mvar|A}}. Then the Dedekind–MacNeille completion of {{mvar|S}} consists of all subsets {{mvar|A}} for which \n:{{math|1=(''A''<sup>u</sup>)<sup>l</sup> = ''A''}};\nit is ordered by inclusion: {{math|''A'' ≤ ''B''}} in the completion if and only if {{math|''A'' &sube; ''B''}} as sets.\n\nAn element {{mvar|x}} of {{mvar|S}} embeds into the completion as its [[Ideal (order theory)|principal ideal]], the set {{math|↓<sub>''x''</sub>}} of elements less than or equal to {{mvar|x}}. Then {{math|(↓<sub>''x''</sub>)<sup>u</sup>}} is the set of elements greater than or equal to {{mvar|x}}, and {{math|1=((↓<sub>''x''</sub>)<sup>u</sup>)<sup>l</sup> = ↓<sub>''x''</sub>}}, showing that {{math|↓<sub>''x''</sub>}} is indeed a member of the completion.<ref>{{harvtxt|MacNeille|1937}}, Lemma 11.8, p.&nbsp; 444; {{harvtxt|Davey| Priestley|2002}}, Lemma 3.9(i), p.&nbsp;166.</ref> It is straightforward to verify that the mapping from {{mvar|x}} to {{math|↓<sub>''x''</sub>}} is an order-embedding.\n\nAn alternative definition of the Dedekind–MacNeille completion that more closely resembles the definition of a Dedekind cut is sometimes used.<ref>This is the definition originally used by {{harvtxt|MacNeille|1937}}, for instance.</ref> In a partially ordered set {{mvar|S}}, define a ''cut'' to be a pair of sets {{math|(''A'',''B'')}} for which {{math|1=''A''<sup>u</sup> = ''B''}} and {{math|1=''A'' = ''B''<sup>l</sup>}}. If {{math|(''A'',''B'')}} is a cut then ''A'' satisfies the equation {{math|1=(''A''<sup>u</sup>)<sup>l</sup> = ''A''}}, and conversely if {{math|1=(''A''<sup>u</sup>)<sup>l</sup> = ''A''}} then {{math|(''A'',''A''<sup>u</sup>)}} is a cut. Therefore, the set of cuts, partially ordered by inclusion on the lower set of the cut (or the reverse of the inclusion relation on the upper set), gives an equivalent definition of the Dedekind–MacNeille completion.\n\nWith the alternative definition, both the join and the meet operations of the complete lattice have symmetric descriptions: if {{math|(''A<sub>i</sub>'',''B<sub>i</sub>'')}} are the cuts in any family of cuts, then the meet of these cuts is the cut {{math|(''L'',''L''<sup>u</sup>)}} where {{math|1=''L'' = ∩<sub>''i''</sub>''A<sub>i</sub>''}}, and the join is the cut {{math|(''U''<sup>l</sup>,''U'')}} where {{math|1=''U'' = ∩<sub>''i''</sub>''B<sub>i</sub>''}}.\n\n==Examples==\nIf '''Q''' is the set of [[rational number]]s, viewed as a totally ordered set with the usual numerical order, then each element of the Dedekind–MacNeille completion of '''Q''' may be viewed as a [[Dedekind cut]], and the Dedekind–MacNeille completion of '''Q''' is the total ordering on the [[real number]]s, together with the two additional values ±∞.<ref>{{harvtxt|Davey| Priestley|2002}}, Example 7.44(1), p.&nbsp;168; {{harvtxt|Siegfried|Schröder|2003}}, Example 5.3.3(2), p.&nbsp;120.</ref> The construction of the real numbers from the rational numbers is an example of the Dedekind completion of a [[totally ordered set]], and the Dedekind–MacNeille completion generalizes this concept from total orders to partial orders.\n\nIf {{mvar|S}} is an [[antichain]] (a set of elements no two of which are comparable) then the Dedekind–MacNeille completion of {{mvar|S}} consists of {{mvar|S}} itself together with two additional elements, a bottom element that is below every element in {{mvar|S}} and a top element that is above every element in {{mvar|S}}.<ref>{{harvtxt|Davey| Priestley|2002}}, Example 7.44(2), p.&nbsp;168.</ref>\n\nIf {{mvar|O}} is any finite set of objects, and {{mvar|A}} is any finite set of unary attributes for the objects in {{mvar|O}}, then one may form a partial order of height two in which the elements of the partial order are the objects and attributes, and in which {{math|''x'' ≤ ''y''}} when {{mvar|x}} is an object that has attribute&nbsp;{{mvar|y}}. For a partial order defined in this way, the Dedekind–MacNeille completion of {{mvar|S}} is known as a [[concept lattice]], and it plays a central role in the field of [[formal concept analysis]].\n\n==Properties==\nThe Dedekind–MacNeille completion is the smallest complete lattice with {{mvar|S}} embedded in it, in the sense that, if {{mvar|L}} is any lattice completion of {{mvar|S}}, then the Dedekind–MacNeille completion is a partially ordered subset of {{mvar|L}}.<ref name=\"smallest-completion\">{{harvtxt|Bishop|1978}}; {{harvtxt|Siegfried|Schröder|2003}}, Theorem 5.3.8, p.&nbsp;121.</ref> When {{mvar|S}} is finite, its completion is also finite, and has the smallest number of elements among all finite complete lattices containing {{mvar|S}}.\n\nThe partially ordered set {{mvar|S}} is join-dense and meet-dense in the Dedekind–MacNeille completion; that is, every element of the completion is a join of some set of elements of {{mvar|S}}, and is also the meet of some set of elements in {{mvar|S}}.<ref>{{harvtxt|Siegfried|Schröder|2003}}, Proposition 5.3.7, p.&nbsp;121.</ref> The Dedekind–MacNeille completion is characterized among completions of {{mvar|S}} by this property.\n\nThe Dedekind–MacNeille completion of a [[Boolean algebra (structure)|Boolean algebra]] is a [[complete Boolean algebra]]; this result is known as the '''Glivenko–Stone theorem''', after [[Valery Ivanovich Glivenko]] and [[Marshall Stone]].<ref>{{harvtxt|Birkhoff|1995}}, Theorem 27, p.&nbsp;130.</ref> Similarly, the Dedekind–MacNeille completion of a [[residuated lattice]] is a complete residuated lattice.<ref>{{harvtxt|Gabbay|Shehtman|Skvortsov|2009}}.</ref> However, the completion of a [[distributive lattice]] need not itself be distributive, and the completion of a [[modular lattice]] may not remain modular.<ref>{{harvtxt|Cotlar|1944}}; {{harvtxt|Funayama|1944}}.</ref>\n\nThe Dedekind–MacNeille completion is self-dual: the completion of the [[Duality (order theory)|dual]] of a partial order is the same as the dual of the completion.<ref>{{harvtxt|Birkhoff|1995}}.</ref>\n\nThe Dedekind–MacNeille completion of {{mvar|S}} has the same [[order dimension]] as does {{mvar|S}} itself.<ref>This result is frequently attributed to an unpublished 1961 Harvard University honors thesis by K. A. Baker, \"Dimension, join-independence and breadth in partially ordered sets\". It was published by {{harvtxt|Novák|1969}}.</ref>\n\nIn the [[Category (mathematics)|category]] of partially ordered sets and monotonic functions between partially ordered sets, the complete lattices form the [[injective object]]s for [[order-embedding]]s, and the Dedekind–MacNeille completion of {{mvar|S}} is the injective hull of&nbsp;{{mvar|S}}.<ref>{{harvtxt|Banaschewski|Bruns|1967}}.</ref>\n\n==Algorithms==\nSeveral researchers have investigated algorithms for constructing the Dedekind–MacNeille completion of a finite partially ordered set. Because the Dedekind–MacNeille completion may be exponentially larger than the partial order it comes from,<ref>{{harvtxt|Ganter|Kuznetsov|1998}}.</ref> it is necessary to measure the time bounds for such algorithms both in terms of the number {{mvar|n}} of elements of the input partial order, but also in terms of the number {{mvar|c}} of elements of its completion, and sometimes also in terms of additional measures of the complexity of the input and output. The format in which the output lattice is represented may also affect the running time of its construction algorithms; for instance, if it is represented as a [[logical matrix]] specifying the result of a comparison between each pair of lattice elements, the output size is {{math|&Theta;(''c''<sup>2</sup>)}} and this will be a lower bound on the time for a construction algorithm.\n\n===Constructing the set of cuts===\n{{harvtxt|Ganter|Kuznetsov|1998}} describe an incremental algorithm, in which the input partial order is built up by adding one element at a time; at each step, the completion of the smaller partial order is expanded to form the completion of the larger partial order. In their method, the completion is represented by an explicit list of cuts. Each cut of the augmented partial order, except for the one whose two sets intersect in the new element, is either a cut from the previous partial order or is formed by adding the new element to one or the other side of a cut from the previous partial order, so their algorithm need only test pairs of sets of this form to determine which ones are cuts. The time for using their method to add a single element to the completion of a partial order is {{math|''O''(''cnw'')}} where {{mvar|w}} is the width of the partial order, that is, the size of its largest [[antichain]]. Therefore, the time to compute the completion of a given partial order is {{math|1=''O''(''cn''<sup>2</sup>''w'') = O(''cn''<sup>3</sup>)}}.\n\nAs {{harvtxt|Jourdan|Rampon|Jard|1994}} observe, the problem of listing all cuts in a partially ordered set can be formulated as a special case of a simpler problem, of listing all maximal [[antichain]]s in a different partially ordered set. If {{mvar|P}} is any partially ordered set, let {{mvar|Q}} be a partial order whose elements contain two copies of {{mvar|P}}: for each element {{mvar|x}} of {{mvar|P}}, {{mvar|Q}} contains two elements {{math|''x''<sub>0</sub>}} and {{math|''x''<sub>1</sub>}}, with {{math|''x<sub>i</sub>'' < ''y<sub>j</sub>''}} if and only if {{math|''x'' < ''y''}} and {{math|''i'' < ''j''}}. Then the cuts in {{mvar|P}} correspond one-for-one with the maximal antichains in {{mvar|Q}}: the elements in the lower set of a cut correspond to the elements with subscript 0 in an antichain, and the elements in the upper set of a cut correspond to the elements with subscript 1 in an antichain. Jourdan et al. describe an algorithm for finding maximal antichains that, when applied to the problem of listing all cuts in {{mvar|P}}, takes time {{math|1=''O''(''c''(''nw'' + ''w''<sup>3</sup>))}}, an improvement on the algorithm of {{harvtxt|Ganter|Kuznetsov|1998}} when the width {{mvar|w}} is small. Alternatively, a maximal antichain in {{mvar|Q}} is the same as a [[maximal independent set]] in the [[comparability graph]] of {{mvar|Q}}, or a [[maximal clique]] in the complement of the comparability graph, so algorithms for the [[clique problem]] or the independent set problem can also be applied to this version of the Dedekind–MacNeille completion problem.\n\n===Constructing the covering graph===\nThe [[transitive reduction]] or covering graph of the Dedekind–MacNeille completion describes the order relation between its elements in a concise way:\neach [[neighborhood (graph theory)|neighbor]] of a cut must remove an element of the original partial order from either the upper or lower set of the cut, so each vertex has at most {{mvar|n}} neighbors. Thus, the covering graph has {{mvar|c}} vertices and at most {{mvar|''cn''/2}} neighbors, a number that may be much smaller than the {{mvar|''c''<sup>2</sup>}} entries in a matrix that specifies all pairwise comparisons between elements. {{harvtxt|Nourine|Raynoud}} show how to compute this covering graph efficiently; more generally, if {{mvar|B}} is any family of sets, they show how to compute the covering graph of the lattice of unions of subsets of {{mvar|B}}. In the case of the Dedekind–MacNeille lattice, {{mvar|B}} may be taken as the family of [[complement set]]s of principal ideals, and the unions of subsets of {{mvar|B}} are complements of the lower sets of cuts. The main idea for their algorithm is to generate unions of subsets of {{mvar|B}} incrementally (for each set in {{mvar|B}}, forming its union with all previously generated unions), represent the resulting family of sets in a [[trie]], and use the trie representation to test certain candidate pairs of sets for adjacency in the covering relation; it takes time {{math|''O''(''cn''<sup>2</sup>)}}. In later work, the same authors showed that the algorithm could be made fully incremental (capable of adding elements to the partial order one at a time) with the same total time bound.<ref>{{harvtxt|Nourine|Raynaud|2002}}.</ref>\n\n==Notes==\n{{reflist|colwidth=30em}}\n\n==References==\n{{refbegin|colwidth=30em}}\n*{{citation\n | last1 = Banaschewski | first1 = B.\n | last2 = Bruns | first2 = G.\n | doi = 10.1007/BF01898828\n | mr = 0221984\n | journal = Archiv der Mathematik\n | pages = 369–377\n | title = Categorical characterization of the MacNeille completion\n | volume = 18\n | year = 1967}}.\n*{{citation|title=Lattice Theory|first=Garrett|last=Birkhoff|authorlink=Garrett Birkhoff|edition=3rd|publisher=American Mathematical Society|series=Colloquium Publications|volume=25|year=1995|isbn=978-0-8218-1025-5|contribution=VI.9 Completion by Cuts|pages=126–128|url=https://books.google.com/books?id=o4bu3ex9BdkC&pg=PA126}}.\n*{{citation\n | last = Bishop | first = Alan A.\n | mr = 0469839\n | issue = 3\n | journal = Algebra Universalis\n | pages = 349–353\n | title = A universal mapping characterization of the completion by cuts\n | volume = 8\n | year = 1978\n | doi=10.1007/bf02485405}}.\n*{{citation\n | last = Cotlar | first = Mischa\n | mr = 0014062\n | journal = Univ. Nac. Tucumán. Revista A.\n | pages = 105–157\n | title = A method of construction of structures and its application to topological spaces and abstract arithmetic\n | volume = 4\n | year = 1944}}.\n*{{citation | title=Introduction to Lattices and Order | first1=B. A. | last1=Davey | first2=Hilary A. | last2=Priestley | edition=2nd | publisher=[[Cambridge University Press]] | year=2002 | isbn=978-0-521-78451-1 | url=https://books.google.com/books?id=vVVTxeuiyvQC&pg=PA166 | page=166 | contribution=7.38 The Dedekind–MacNeille completion | zbl=1002.06001 }}.\n*{{citation\n | last = Funayama | first = Nenosuke\n | doi = 10.3792/pia/1195573210\n | mr = 0014063 | zbl=0063.01484 \n | journal = Proc. Imp. Acad. Tokyo\n | pages = 1–2\n | title = On the completion by cuts of distributive lattices\n | volume = 20\n | year = 1944}}.\n*{{citation | title=Quantification in Nonclassical Logic, Volume 1 | volume=153 | series=Studies in logic and the foundations of mathematics | first1=Dov M. | last1=Gabbay | author1-link=Dov Gabbay | first2=Valentin | last2=Shehtman | first3=Dimitrij | last3=Skvortsov | publisher=Elsevier | year=2009 | isbn=978-0-444-52012-8 | zbl=1211.03002 | contribution=3.4.12 The Dedekind–MacNeille completion of a residuated lattice | url=https://books.google.com/books?id=Gy46SP7px7gC&pg=PA177 | pages=177–178}}.\n*{{citation\n | last1 = Ganter | first1 = Bernhard\n | last2 = Kuznetsov | first2 = Sergei O.\n | contribution = Stepwise construction of the Dedekind-MacNeille completion\n | doi = 10.1007/BFb0054922\n | mr = 1673860 | zbl=0928.06004 \n | pages = 295–302\n | publisher = Springer-Verlag\n | series = Lecture Notes in Computer Science\n | title = Proc. 6th Int. Conf. Conceptual Structures: Theory, Tools and Applications (ICCS98)\n | volume = 1453\n | year = 1998}}.\n*{{citation\n | last1 = Jourdan | first1 = Guy-Vincent\n | last2 = Rampon | first2 = Jean-Xavier\n | last3 = Jard | first3 = Claude\n | doi = 10.1007/BF02115811\n | mr = 1308475 | zbl=0814.06004 \n | issue = 3\n | journal = Order\n | pages = 197–210\n | title = Computing on-line the lattice of maximal antichains of posets\n | volume = 11\n | year = 1994}}.\n*{{citation\n | last = MacNeille | first = H. M. | authorlink = Holbrook Mann MacNeille\n | doi = 10.2307/1989739\n | mr = 1501929 | zbl=0017.33904  | jfm=63.0833.04 \n | issue = 3\n | journal = Trans. Amer. Math. Soc.\n | pages = 416–460\n | title = Partially ordered sets\n | volume = 42\n | year = 1937}}.\n*{{citation\n | last1 = Nourine | first1 = Lhouari\n | last2 = Raynaud | first2 = Olivier\n | doi = 10.1016/S0020-0190(99)00108-8\n | mr = 1726978 | zbl=0998.06005 \n | issue = 5-6\n | journal = Information Processing Letters\n | pages = 199–204\n | title = A fast algorithm for building lattices\n | volume = 71\n | year = 1999}}.\n*{{citation\n | last1 = Nourine | first1 = Lhouari\n | last2 = Raynaud | first2 = Olivier\n | doi = 10.1080/09528130210164152\n | zbl=1022.68027 \n | issue = 2\n | journal = [[Journal of Experimental and Theoretical Artificial Intelligence]]\n | pages = 217–227\n | title = A fast incremental algorithm for building lattices\n | volume = 14\n | year = 2002}}.\n*{{citation\n | last = Novák | first = Vítězslav\n | doi = 10.1007/BF01350778\n | mr = 0240010\n | journal = Math. Ann.\n | pages = 337–342\n | title = Über eine Eigenschaft der Dedekind-MacNeilleschen Hülle\n | volume = 179\n | year = 1969}}.\n*{{citation|title=Ordered Sets: An Introduction|first1=Bernd|last1=Siegfried|first2=Walter|last2=Schröder|publisher=Birkhäuser|year=2003|isbn=978-0-8176-4128-3|contribution=5.3 Embeddings/The Dedekind/MacNeille Completion|url=https://books.google.com/books?id=2esoXnolEWgC&pg=PA119|pages=119–122}}.\n{{refend}}\n\n==External links==\n*[http://planetmath.org/encyclopedia/MacNeilleCompletion.html MacNeille completion] in [[PlanetMath]]\n*{{nlab|id=MacNeille+completion|title=MacNeille completion}}\n\n{{DEFAULTSORT:Dedekind-MacNeille completion}}\n[[Category:Order theory]]"
    },
    {
      "title": "Dehornoy order",
      "url": "https://en.wikipedia.org/wiki/Dehornoy_order",
      "text": "In the [[mathematics|mathematical]] area of [[braid theory]], the '''Dehornoy order''' is a left-invariant [[total order]] on the [[braid group]], found by {{harvs|txt|authorlink=Patrick Dehornoy|first=Patrick|last=Dehornoy|year1=1994|year2=1995}}.\n\nDehornoy's original discovery of the order on the braid group used [[huge cardinal]]s, but there are now several more elementary constructions of it.\n\n==Definition==\n\nSuppose that ''&sigma;''<sub>1</sub>,&nbsp;...,&nbsp;''&sigma;''<sub>''n''&minus;1</sub> are the usual generators of the braid group ''B''<sub>''n''</sub> on ''n'' strings.  \nThe set ''P'' of  positive elements in the Dehornoy order is defined to be the elements that  can be written as  word in the elements ''&sigma;''<sub>1</sub>,&nbsp;...,&nbsp;''&sigma;''<sub>''n''&minus;1</sub> and their inverses, such that for some ''i'' the word contains σ<sub>''i''</sub> but does not contain \n''&sigma;''{{su|b=''j''|''p'' = &minus;1}}<sup>±1</sup> for ''j''&nbsp;<&nbsp;''i'' \nnor ''&sigma;''{{su|b=''i''|''p'' = &minus;1}}<sup>−1</sup>.\n\nThe set ''P'' has the properties ''PP''&nbsp;⊆&nbsp;''P'', and the braid group is a disjoint union of ''P'', 1, and ''P''<sup>&minus;1</sup>. \nThese properties imply that if we define ''a''&nbsp;<&nbsp;''b'' to mean ''a''<sup>&minus;1</sup>''b''&nbsp;∈&nbsp;''P'' then we get a left-invariant total order on the braid group.\n\n==Properties==\n\nThe Dehornoy order is a well-ordering when restricted to the monoid generated by ''&sigma;''<sub>1</sub>,&nbsp;...,&nbsp;''&sigma;''<sub>''n''&minus;1</sub>.\n\n== References ==\n\n*{{Citation | last1=Dehornoy | first1=Patrick | title=Braid groups and left distributive operations | doi=10.2307/2154598 | mr=1214782 | year=1994 | journal=[[Transactions of the American Mathematical Society]] | issn=0002-9947 | volume=345 | issue=1 | pages=115–150| url=http://www.ams.org/tran/1994-345-01/S0002-9947-1994-1214782-4/S0002-9947-1994-1214782-4.pdf }}\n*{{Citation | last1=Dehornoy | first1=Patrick | title=From large cardinals to braids via distributive algebra | doi=10.1142/S0218216595000041 | mr=1321290 | year=1995 | journal=Journal of Knot Theory and its Ramifications | issn=0218-2165 | volume=4 | issue=1 | pages=33–79}}\n\n==Further reading==\n*{{Citation | last1=Dehornoy | first1=Patrick | last2=Dynnikov | first2=Ivan | last3=Rolfsen | first3=Dale | last4=Wiest | first4=Bert | title=Why are braids orderable? | url=http://www.math.unicaen.fr/~dehornoy/Books/Why/DgrIntro.pdf | publisher=[[Société Mathématique de France]] | location=Paris | series=Panoramas et Synthèses  | isbn=978-2-85629-135-1 | mr=1988550 | year=2002 | volume=14}}\n*{{Citation | last1=Dehornoy | first1=Patrick | last2=Dynnikov | first2=Ivan | last3=Rolfsen | first3=Dale | last4=Wiest | first4=Bert | title=Ordering braids | url=https://books.google.com/books?id=St68wblwRlEC | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Mathematical Surveys and Monographs | isbn=978-0-8218-4431-1 | mr=2463428 | year=2008 | volume=148}}\n\n[[Category:Knot theory]]\n[[Category:Braid groups]]\n[[Category:Order theory]]"
    },
    {
      "title": "Deviation of a poset",
      "url": "https://en.wikipedia.org/wiki/Deviation_of_a_poset",
      "text": "In [[order theory|order-theoretic mathematics]], the '''deviation of a poset''' is an [[ordinal number]] measuring the complexity of a [[partially ordered set]].\n\nThe deviation of a poset is used to define the [[Krull dimension]] of a [[Module (mathematics)|module over a ring]] as the deviation of its poset of submodules.\n\n==Definition==\nA trivial poset (one in which no two elements are comparable) is declared to have deviation <math>-\\infty</math>. A nontrivial poset satisfying the descending chain condition is said to have deviation 0. Then, inductively, a poset is said to have deviation at most α (for an ordinal α) if for every descending chain of elements ''a''<sub>0</sub> > ''a''<sub>1</sub> >... all but a finite number of the posets of elements between ''a''<sub>''n''</sub> and ''a''<sub>''n''+1</sub> have deviation less than α. The deviation (if it exists) is the minimum value of α for which this is true.\n\nNot every poset has a deviation. The following conditions on a poset are equivalent:\n*The poset has a deviation\n*The [[Duality (order theory)|opposite poset]] has a deviation\n*The poset does not contain a subset [[order isomorphism|order-isomorphic]] to the [[rational number]]s (with their standard numerical ordering)\n\n==Examples==\nThe poset of positive integers has deviation 0: every descending chain is finite, so the defining condition for deviation is [[vacuous truth|vacuously true]].\nHowever, its opposite poset has deviation 1.\n\nLet ''k'' be an algebraically closed field and consider the poset of ideals of the polynomial ring ''k[x]'' in one variable. Since the deviation of this poset is the Krull dimension of the ring, we know that it should be 1. This corresponds to the fact that ''k[x]'' does not have the descending chain condition (so the deviation is greater than zero), but in any descending chain, consecutive elements are 'close together'. For instance, take the descending chain of ideals <math>(x)\\supset (x^2)\\supset (x^3)\\supset...</math> - this is an infinite descending chain, but for any two consecutive terms, say <math>(x^n)</math> and <math> (x^{n+1})</math>, there is no infinite descending chain of ideals of ''k[x]'' contained between these terms.\n\nExtending this example further, consider the polynomial ring in two variables, ''k[x,y]'', which has Krull dimension 2. Take the descending chain <math> (x)\\supset (x^2)\\supset(x^3)\\supset...</math>. Given any two adjacent terms in this chain, <math>(x^n)</math> and <math>(x^{n+1})</math>, there is an infinite descending chain <math> (x^ny,x^{n+1})\\supset(x^ny^2,x^{n+1})\\supset(x^ny^3,x^{n+1})\\supset...</math>. So we can find a descending chain such that between any two adjacent terms there is a further infinite descending chain - we can 'nest' descending chains two layers deep. Extending this, it is easy to see that in the polynomial ring in ''n'' variables, it is possible to nest descending chains ''n'' layers deep and no more. This is essentially what it means for the poset of ideals to have deviation ''n''.\n\n==References==\n\n*{{Citation | last1=McConnell | first1=J. C. | last2=Robson | first2=J. C. | title=Noncommutative Noetherian rings | url=https://books.google.com/books?id=c3fWk8LoSvgC | publisher=[[American Mathematical Society]] | location=Providence, R.I. | edition=Revised | series=[[Graduate Studies in Mathematics]] | isbn=978-0-8218-2169-5 |mr=1811901 | year=2001 | volume=30}}\n\n[[Category:Order theory]]\n\n{{settheory-stub}}"
    },
    {
      "title": "Dilworth's theorem",
      "url": "https://en.wikipedia.org/wiki/Dilworth%27s_theorem",
      "text": "{{Redirects|Chain decomposition|the path decomposition|Heavy path decomposition}}\nIn [[mathematics]], in the areas of [[order theory]] and [[combinatorics]], '''Dilworth's theorem''' characterizes the '''width''' of any finite [[partially ordered set]] in terms of a [[Partition of a set|partition]] of the order into a minimum number of chains. It is named for the mathematician {{harvs|first=Robert P.|last=Dilworth|authorlink=Robert P. Dilworth|year=1950|txt}}.\n\nAn [[antichain]] in a partially ordered set is a set of elements no two of which are comparable to each other, and a chain is a set of elements every two of which are comparable. A chain decomposition is a partition of the elements of the order into [[disjoint set|disjoint]] chains.  Dilworth's theorem states that, in any finite partially ordered set, the largest antichain has the same size as the smallest chain decomposition. Here, the size of the antichain is its number of elements, and the size of the chain decomposition is its number of chains. The width of the partial order is defined as the common size of the antichain and chain decomposition.\n\nA version of the theorem for infinite partially ordered sets states that, when there exists a decomposition into finitely many chains, or when there exists a finite upper bound on the size of an antichain, the sizes of the largest antichain and of the smallest chain decomposition are again equal.\n\n== Inductive proof ==\n\nThe following proof by induction on the size of the partially ordered set <math>P</math> is based on that of {{harvs|last=Galvin|authorlink=Fred Galvin|txt|year=1994}}.\n\nLet <math>P</math> be a finite partially ordered set. The theorem holds trivially if <math>P</math> is empty. So, assume that <math>P</math> has at least one element, and let <math>a</math> be a maximal element of <math>P</math>.\n\nBy induction, we assume that for some integer <math>k</math> the partially ordered set <math>P':=P\\setminus\\{a\\}</math> can be covered by <math>k</math> disjoint chains <math>C_1,\\dots,C_k</math> and has at least one antichain <math>A_0</math> of size <math>k</math>. Clearly, <math>A_0\\cap C_i\\ne\\emptyset</math> for <math>i=1,2,\\dots,k</math>. For <math>i=1,2,\\dots,k</math>, let <math>x_i</math> be the maximal element in <math>C_i</math> that belongs to an antichain of size <math>k</math> in <math>P'</math>, and set <math>A:=\\{x_1,x_2,\\dots,x_k\\}</math>. \nWe claim that <math>A</math> is an antichain. \nLet <math>A_i</math> be an antichain of size <math>k</math> that contains <math>x_i</math>. Fix arbitrary distinct indices <math>i</math> and <math>j</math>. Then <math>A_i\\cap C_j\\ne\\emptyset</math>. Let <math>y\\in A_i\\cap C_j</math>. Then <math>y\\le x_j</math>, by the definition of <math>x_j</math>. This implies that <math>x_i\\not \\ge x_j</math>, since <math>x_i\\not\\ge y</math>. By interchanging the roles of <math>i</math> and <math>j</math> in this argument we also have <math>x_j\\not\\ge x_i</math>. This verifies that <math>A</math> is an antichain.\n\nWe now return to <math>P</math>. Suppose first that <math>a\\ge x_i</math> for some <math>i\\in\\{1,2,\\dots,k\\}</math>. Let <math>K</math> be the chain <math>\\{a\\}\\cup\\{z\\in C_i:z\\le x_i\\}</math>. Then by the choice of <math>x_i</math>, <math>P\\setminus K</math> does not have an antichain of size <math>k</math>. Induction then implies that <math>P\\setminus K</math> can be covered by <math>k-1</math> disjoint chains since <math>A \\setminus \\{x_i \\}</math> is an antichain of size <math>k - 1</math> in <math>P \\setminus K</math>. \nThus, <math>P</math> can be covered by <math>k</math> disjoint chains, as required. Next, if <math>a\\not\\ge x_i</math> for each <math>i\\in\\{1,2,\\dots,k\\}</math>, then <math>A\\cup\\{a\\}</math> is an antichain of size <math>k+1</math> in <math>P</math> (since <math>a</math> is maximal in <math>P</math>). Now <math>P</math> can be covered by the <math>k+1</math> chains <math>\\{a\\},C_1,C_2,\\dots,C_k</math>, completing the proof.\n\n==Proof via Kőnig's theorem==\n[[Image:Dilworth-via-König.svg|thumb|upright=1.8|Proof of Dilworth's theorem via Kőnig's theorem: constructing a bipartite graph from a partial order, and partitioning into chains according to a matching]]\n\nLike a number of other results in combinatorics, Dilworth's theorem is equivalent to [[Kőnig's theorem (graph theory)|Kőnig's theorem]] on [[bipartite graph]] matching and several other related theorems including [[Hall's marriage theorem]] {{harv|Fulkerson|1956}}.\n\nTo prove Dilworth's theorem for a partial order ''S'' with ''n'' elements, using Kőnig's theorem, define a bipartite graph ''G'' = (''U'',''V'',''E'') where ''U'' = ''V'' = ''S'' and where (''u'',''v'') is an edge in ''G'' when ''u'' < ''v'' in ''S''.  By Kőnig's theorem, there exists a matching ''M'' in ''G'', and a set of vertices ''C'' in ''G'', such that each edge in the graph contains at least one vertex in ''C'' and such that ''M'' and ''C'' have the same cardinality ''m''.  Let ''A'' be the set of elements of ''S'' that do not correspond to any vertex in ''C''; then ''A'' has at least ''n'' - ''m'' elements (possibly more if ''C'' contains vertices corresponding to the same element on both sides of the bipartition). Let ''P'' be a family of chains formed by including ''x'' and ''y'' in the same chain whenever there is an edge (''x'',''y'') in ''M''; then ''P'' has ''n'' - ''m'' chains. Therefore, we have constructed an antichain and a partition into chains with the same cardinality.\n\nTo prove Kőnig's theorem from Dilworth's theorem, for a bipartite graph ''G'' = (''U'',''V'',''E''), form a partial order on the vertices of ''G'' in which ''u'' < ''v'' exactly when ''u'' is in ''U'', ''v'' is in ''V'', and there exists an edge in ''E'' from ''u'' to ''v''. By Dilworth's theorem, there exists an antichain ''A'' and a partition into chains ''P'' both of which have the same size. But the only nontrivial chains in the partial order are pairs of elements corresponding to the edges in the graph, so the nontrivial chains in ''P'' form a matching in the graph. The complement of ''A'' forms a vertex cover in ''G'' with the same cardinality as this matching.\n\nThis connection to bipartite matching allows the width of any partial order to be computed in [[polynomial time]]. More precisely, ''n''-element partial orders of width ''k'' can be recognized in time ''O''(''kn''<sup>2</sup>) {{harv|Felsner|Raghavan|Spinrad|2003}}.\n\n==Extension to infinite partially ordered sets==\nDilworth's theorem for infinite partially ordered sets states that a partially ordered set has finite width ''w'' if and only if it may be partitioned into ''w'' chains.  For, suppose that an infinite partial order ''P'' has width ''w'', meaning that there are at most a finite number ''w'' of elements in any antichain. For any subset ''S'' of ''P'', a decomposition into ''w'' chains (if it exists) may be described as a [[graph coloring|coloring]] of the [[comparability graph|incomparability graph]] of ''S'' (a graph that has the elements of ''S'' as vertices, with an edge between every two incomparable elements) using ''w'' colors; every color class in a proper coloring of the incomparability graph must be a chain. By the assumption that ''P'' has width ''w'', and by the finite version of Dilworth's theorem, every finite subset ''S'' of ''P'' has a ''w''-colorable incomparability graph. Therefore, by the [[De Bruijn–Erdős theorem (graph theory)|De Bruijn–Erdős theorem]], ''P'' itself also has a ''w''-colorable incomparability graph, and thus has the desired partition into chains {{harv|Harzheim|2005}}.\n\nHowever, the theorem does not extend so simply to partially ordered sets in which the width, and not just the cardinality of the set, is infinite. In this case the size of the largest antichain and the minimum number of chains needed to cover the partial order may be very different from each other. In particular, for every infinite cardinal number κ there is an infinite partially ordered set of width [[Aleph-naught|ℵ<sub>0</sub>]] whose partition into the fewest chains has κ chains {{harv|Harzheim|2005}}.\n\n{{harvtxt|Perles|1963}} discusses analogues of Dilworth's theorem in the infinite setting.\n\n==Dual of Dilworth's theorem (Mirsky's theorem)==\n{{main article|Mirsky's theorem}}\nA dual of Dilworth's theorem states that the size of the largest chain in a partial order (if finite) equals the smallest number of antichains into which the order may be partitioned {{harv|Mirsky|1971}}.  The proof of this is much simpler than the proof of Dilworth's theorem itself: for any element ''x'', consider the chains that have ''x'' as their largest element, and let ''N''(''x'') denote the size of the largest of these ''x''-maximal chains. Then each set ''N''<sup>−1</sup>(''i''), consisting of elements that have equal values of ''N'', is an antichain, and these antichains partition the partial order into a number of antichains equal to the size of the largest chain.\n\n==Perfection of comparability graphs==\nA [[comparability graph]] is an [[undirected graph]] formed from a partial order by creating a vertex per element of the order, and an edge connecting any two comparable elements. Thus, a [[Clique (graph theory)|clique]] in a comparability graph corresponds to a chain, and an [[Independent set (graph theory)|independent set]] in a comparability graph corresponds to an antichain. Any [[induced subgraph]] of a comparability graph is itself a comparability graph, formed from the restriction of the partial order to a subset of its elements.\n\nAn undirected graph is [[Perfect graph|perfect]] if, in every induced subgraph, the [[chromatic number]] equals the size of the largest clique. Every comparability graph is perfect: this is essentially just Mirsky's theorem, restated in graph-theoretic terms {{harv|Berge|Chvátal|1984}}. By the [[perfect graph theorem]] of {{harvtxt|Lovász|1972}}, the [[Complement graph|complement]] of any perfect graph is also perfect. Therefore, the complement of any comparability graph is perfect; this is essentially just Dilworth's theorem itself, restated in graph-theoretic terms {{harv|Berge|Chvátal|1984}}. Thus, the complementation property of perfect graphs can provide an alternative proof of Dilworth's theorem.\n\n==Width of special partial orders==\nThe [[Boolean lattice]] ''B''<sub>''n''</sub> is the [[power set]] of an ''n''-element set ''X''—essentially {1, 2, …, ''n''}—ordered by [[inclusion (set theory)|inclusion]] or, notationally, (2<sup>[''n'']</sup>, ⊆). [[Sperner's theorem]] states that a maximum antichain of ''B''<sub>''n''</sub> has size at most\n:<math>\\mbox{width}(B_n) = {n \\choose \\lfloor{n/2}\\rfloor}.</math>\nIn other words, a largest family of incomparable subsets of ''X'' is obtained by selecting the subsets of ''X'' that have median size. The [[Lubell–Yamamoto–Meshalkin inequality]] also concerns antichains in a power set and can be used to prove Sperner's theorem.\n\nIf we order the integers in the interval [1,&nbsp;2''n''] by [[divisibility]], the subinterval [''n''&nbsp;+&nbsp;1,&nbsp;2''n''] forms an antichain with cardinality ''n''. A partition of this partial order into ''n'' chains is easy to achieve: for each odd integer ''m'' in [1,2''n''], form a chain of the numbers of the form ''m''2<sup>''i''</sup>. Therefore, by Dilworth's theorem, the width of this partial order is ''n''.\n\nThe [[Erdős–Szekeres theorem]] on monotone subsequences can be interpreted as an application of Dilworth's theorem to partial orders of [[order dimension]] two {{harv|Steele|1995}}.\n\nThe \"convex dimension\" of an [[antimatroid]] is defined as the minimum number of chains needed to define the antimatroid, and Dilworth's theorem can be used to show that it equals the width of an associated partial order; this connection leads to a polynomial time algorithm for convex dimension {{harv|Edelman|Saks|1988}}.\n\n==References==\n*{{citation\n | last1 = Berge | first1 = Claude | author1-link = Claude Berge\n | last2 = Chvátal | first2 = Václav | author2-link = Václav Chvátal\n | isbn = 978-0-444-86587-8\n | page = viii\n | publisher = Elsevier\n | series = Annals of Discrete Mathematics\n | title = Topics on Perfect Graphs\n | volume = 21\n | year = 1984}}\n*{{citation\n | last = Dilworth | first = Robert P.\n | authorlink = Robert P. Dilworth\n | title = A Decomposition Theorem for Partially Ordered Sets\n | jstor = 1969503\n | journal = [[Annals of Mathematics]]\n | volume = 51\n | issue = 1 | pages = 161–166 | year = 1950\n | doi = 10.2307/1969503}}.\n*{{citation\n | last1 = Edelman | first1 = Paul H.\n | last2 = Saks | first2 = Michael E. | author2-link = Michael Saks (mathematician)\n | doi = 10.1007/BF00143895\n | issue = 1\n | journal = [[Order (journal)|Order]]\n | pages = 23–32\n | title = Combinatorial representation and convex dimension of convex geometries\n | volume = 5\n | year = 1988}}.\n*{{citation\n | last1 = Felsner | first1 = Stefan\n | last2 = Raghavan | first2 = Vijay\n | last3 = Spinrad | first3 = Jeremy\n | doi = 10.1023/B:ORDE.0000034609.99940.fb\n | issue = 4\n | journal = [[Order (journal)|Order]]\n | mr = 2079151\n | pages = 351–364 (2004)\n | title = Recognition algorithms for orders of small width and graphs of small Dilworth number\n | volume = 20\n | year = 2003}}.\n*{{citation\n | last = Fulkerson | first = D. R. \n | authorlink = D. R. Fulkerson\n | title = Note on Dilworth’s decomposition theorem for partially ordered sets\n | journal = [[Proceedings of the American Mathematical Society]]\n | volume = 7\n | issue = 4 | pages = 701–702 | year = 1956\n | jstor = 2033375 | doi=10.2307/2033375}}.\n*{{citation\n | last = Galvin | first = Fred | authorlink = Fred Galvin\n | title = A proof of Dilworth's chain decomposition theorem\n | mr = 1270960 \n | jstor = 2975628 \n | journal = [[American Mathematical Monthly|The American Mathematical Monthly]]\n | year = 1994 | volume = 101 | issue = 4 | pages = 352–353\n | doi = 10.2307/2975628}}.\n*{{citation\n | last = Harzheim | first = Egbert\n | isbn = 0-387-24219-8\n | location = New York\n | mr = 2127991\n | at = Theorem&nbsp;5.6, p.&nbsp;60\n | publisher = Springer\n | series = Advances in Mathematics (Springer)\n | title = Ordered sets\n | url = https://books.google.com/books?id=FYV6tGm3NzgC&pg=PA59\n | volume = 7\n | year = 2005}}.\n*{{citation\n | last = Lovász | first = László | author-link = László Lovász\n | doi = 10.1016/0012-365X(72)90006-4\n | journal = [[Discrete Mathematics (journal)|Discrete Mathematics]]\n | pages = 253–267\n | title = Normal hypergraphs and the perfect graph conjecture\n | volume = 2\n | year = 1972\n | issue = 3}}.\n*{{citation\n | authorlink = Leon Mirsky\n | last = Mirsky | first = Leon\n | title = A dual of Dilworth's decomposition theorem\n | jstor = 2316481\n | journal = [[American Mathematical Monthly]]\n | volume = 78\n | issue = 8 | year = 1971 | pages = 876–877\n | doi = 10.2307/2316481}}.\n*{{citation\n | last1 = Nešetřil | first1 = Jaroslav | author1-link = Jaroslav Nešetřil\n | last2 = Ossona de Mendez | first2 = Patrice | author2-link = Patrice Ossona de Mendez\n | contribution = Theorem 3.13\n | doi = 10.1007/978-3-642-27875-4\n | isbn = 978-3-642-27874-7\n | location = Heidelberg\n | mr = 2920058\n | page = 42\n | publisher = Springer\n | series = Algorithms and Combinatorics\n | title = Sparsity: Graphs, Structures, and Algorithms\n | volume = 28\n | year = 2012}}.\n*{{citation\n | last = Perles | first = Micha A. | authorlink = Micha Perles\n | title = On Dilworth's theorem in the infinite case\n | mr = 0168497  \n | journal=Israel Journal of Mathematics\n | year = 1963  | volume = 1 | pages = 108–109 \n | doi=10.1007/BF02759806\n | issue = 2}}.\n*{{citation\n | last = Steele | first = J. Michael | authorlink = J. Michael Steele\n | contribution = Variations on the monotone subsequence theme of Erdős and Szekeres\n | editor1-last = Aldous | editor1-first = David | editor1-link = David Aldous\n | editor2-last = Diaconis | editor2-first = Persi | editor2-link = Persi Diaconis\n | editor3-last = Spencer | editor3-first = Joel | editor3-link = Joel Spencer\n |display-editors = 3 | editor4-last = Steele | editor4-first = J. Michael | editor4-link = J. Michael Steele\n | pages = 111–131\n | publisher = Springer-Verlag\n | series = IMA Volumes in Mathematics and its Applications\n | title = Discrete Probability and Algorithms\n | url = http://www-stat.wharton.upenn.edu/~steele/Publications/PDF/VOTMSTOEAS.pdf\n | volume = 72\n | year = 1995}}.\n\n== External links ==\n* [http://robertborgersen.info/Presentations/GS-05R-1.pdf Equivalence of seven major theorems in combinatorics]\n* {{Citation\n |url=http://planetmath.org/encyclopedia/DualOfDilworthsTheorem.html \n |title=Dual of Dilworth's Theorem \n |work=[[PlanetMath]] \n |postscript= \n |deadurl=yes \n |archiveurl=https://web.archive.org/web/20070714201213/http://planetmath.org/encyclopedia/DualOfDilworthsTheorem.html \n |archivedate=2007-07-14 \n |df= \n}}\n* {{Citation\n |author=Babai, László \n |authorlink=László Babai \n |url=http://www.classes.cs.uchicago.edu/archive/2005/spring/37200-1/notes/10.pdf \n |title=Lecture Notes in Combinatorics and Probability, Lecture 10: Perfect Graphs \n |year=2005 \n |postscript= \n |deadurl=yes \n |archiveurl=https://web.archive.org/web/20110720134637/http://www.classes.cs.uchicago.edu/archive/2005/spring/37200-1/notes/10.pdf \n |archivedate=2011-07-20 \n |df= \n}}\n* {{Citation|author1=Felsner, S. |author2=Raghavan, V. |author3= Spinrad, J. |last-author-amp=yes | year = 1999\n  | title = Recognition Algorithms for Orders of Small Width and Graphs of Small Dilworth Number\n  | url = http://www.inf.fu-berlin.de/inst/pubs/tr-b-99-05.abstract.html| postscript = \n  }}\n* {{mathworld | title = Dilworth's Lemma | urlname = DilworthsLemma}}\n\n[[Category:Order theory]]\n[[Category:Theorems in combinatorics]]\n[[Category:Articles containing proofs]]\n[[Category:Perfect graphs]]"
    },
    {
      "title": "Disjunction property of Wallman",
      "url": "https://en.wikipedia.org/wiki/Disjunction_property_of_Wallman",
      "text": "In [[mathematics]], especially in [[order theory]], a [[partially ordered set]] with a unique [[minimal element]] 0 has the '''disjunction property of Wallman''' when for every pair (''a'', ''b'') of elements of the poset, either ''b'' ≤ ''a'' or there exists an element ''c'' ≤ ''b'' such that ''c'' ≠ 0 and ''c'' has no nontrivial common predecessor with ''a''. That is, in the latter case, the only ''x'' with ''x'' ≤ ''a'' and ''x'' ≤ ''c'' is ''x'' = 0.\n\nA version of this property for [[lattice (order)|lattices]] was introduced by {{harvtxt|Wallman|1938}}, in a paper showing that the [[homology theory]] of a [[topological space]] could be defined in terms of its [[distributive lattice]] of [[closed set]]s. He observed that the inclusion order on the closed sets of a [[T1 space]] has the disjunction property. The generalization to partial orders was introduced by {{harvtxt|Wolk|1956}}.\n\n==References==\n*{{citation | last = Wallman | first = Henry | authorlink = Henry Wallman\n | title = Lattices and topological spaces | journal = [[Annals of Mathematics]]\n | volume = 39 | issue = 1 | pages = 112–126 | year = 1938 | doi = 10.2307/1968717 | jstor = 0003486}}.\n*{{citation | last = Wolk | first = E. S. | title = Some Representation Theorems for Partially Ordered Sets\n | journal = [[Proceedings of the American Mathematical Society]]\n | volume = 7 | issue = 4 | year = 1956 | pages = 589–594 | doi = 10.2307/2033355 | jstor = 00029939}}.\n\n[[Category:Order theory]]\n\n\n{{algebra-stub}}\n{{combin-stub}}"
    },
    {
      "title": "Distributivity (order theory)",
      "url": "https://en.wikipedia.org/wiki/Distributivity_%28order_theory%29",
      "text": "{{Refimprove|date=May 2014}}\nIn the [[mathematics|mathematical]] area of [[order theory]], there are various notions of the common concept of [[distributivity]], applied to the formation of [[supremum|suprema]] and [[infimum|infima]]. Most of these apply to [[partially ordered set]]s that are at least [[lattice (order)|lattices]], but the concept can in fact reasonably be generalized to [[semilattice]]s as well.\n\n==Distributive lattices==\nProbably the most common type of distributivity is the one defined for [[lattice (order)|lattices]], where the formation of binary suprema and infima provide the total operations of join (<math>\\vee</math>) and meet (<math>\\wedge</math>). Distributivity of these two operations is then expressed by requiring that the identity\n\n: <math>x \\wedge (y \\vee z) = (x \\wedge y) \\vee (x \\wedge z)</math>\n\nhold for all elements ''x'', ''y'', and ''z''. This distributivity law defines the class of '''[[distributive lattice]]s'''. Note that this requirement can be rephrased by saying that binary meets [[limit preserving (order theory)|preserve]] binary joins. The above statement is known to be equivalent to its [[duality (order theory)|order dual]]\n\n: <math>x \\vee (y \\wedge z) = (x \\vee y) \\wedge (x \\vee z)</math>\n\nsuch that one of these properties suffices to define distributivity for lattices. Typical examples of distributive lattice are [[totally ordered set]]s, [[Boolean algebra (structure)|Boolean algebra]]s, and [[Heyting algebra]]s. Every finite distributive lattice is [[Order isomorphism|isomorphic]] to a lattice of sets, ordered by inclusion ([[Birkhoff's representation theorem]]).\n\n==Distributivity for semilattices==\n[[File:DistrSemilattice.svg|thumb|Hasse diagram for the definition of distributivity for a meet-semilattice.]]\nA [[semilattice]] is [[partially ordered set]] with only one of the two lattice operations, either a '''meet-''' or a '''join-semilattice'''. Given that there is only one binary operation, distributivity obviously cannot be defined in the standard way. Nevertheless, because of the interaction of the single operation with the given order, the following definition of distributivity remains possible. A '''meet-semilattice''' is '''distributive''', if for all ''a'', ''b'', and ''x'':\n\n: If ''a'' &and; ''b'' &le; ''x'' then there exist ''a' '' and ''b' '' such that ''a'' &le; ''a' '', ''b'' &le; ''b' '' and ''x'' = ''a' '' &and; ''b' ''.\n\nDistributive join-semilattices are defined [[duality (order theory)|dually]]: a '''join-semilattice''' is '''distributive''', if for all ''a'', ''b'', and ''x'':\n\n: If ''x'' &le; ''a'' &or; ''b'' then there exist ''a' '' and ''b' '' such that ''a' '' &le; ''a'', ''b' '' &le; ''b'' and ''x'' = ''a' '' &or; ''b' ''.\n\nIn either case, a' and b' need not be unique.\nThese definitions are justified by the fact that given any lattice ''L'', the following statements are all equivalent:\n\n* ''L'' is distributive as a meet-semilattice\n* ''L'' is distributive as a join-semilattice\n* ''L'' is a distributive lattice.\n\nThus any distributive meet-semilattice in which binary joins exist is a distributive lattice. \nA join-semilattice is distributive if and only if the lattice of its [[ideal (order theory)|ideals]] (under inclusion) is distributive.\n<ref>{{cite book| author=G. Grätzer| title=Lattice Theory: Foundation| year=2011| publisher=Springer/Birkhäuser}}; here: Sect. II.5.1, p.167</ref>\n\nThis definition of distributivity allows generalizing some statements about distributive lattices to distributive semilattices.\n\n==Distributivity laws for complete lattices==\nFor a [[completeness (order theory)|complete]] lattice, arbitrary subsets have both infima and suprema and thus infinitary meet and join operations are available. Several extended notions of distributivity can thus be described. For example, for the '''infinite distributive law''', finite meets may distribute over arbitrary joins, i.e.\n\n: <math>x \\wedge \\bigvee S = \\bigvee \\{ x \\wedge s \\mid s \\in S \\}</math>\n\nmay hold for all elements ''x'' and all subsets ''S'' of the lattice. Complete lattices with this property are called '''frames''', '''locales''' or '''[[complete Heyting algebra]]s'''. They arise in connection with [[pointless topology]] and [[Stone duality]]. This distributive law ''is not equivalent'' to its dual statement\n\n: <math>x \\vee \\bigwedge S = \\bigwedge \\{ x \\vee s \\mid s \\in S \\}</math>\n\nwhich defines the class of dual frames or complete co-Heyting algebras.\n\nNow one can go even further and define orders where arbitrary joins distribute over arbitrary meets. Such structures are called [[completely distributive lattice]]s. However, expressing this requires formulations that are a little more technical. Consider a doubly indexed family {''x''<sub>''j'',''k''</sub> | ''j'' in ''J'', ''k'' in ''K''(''j'')} of elements of a complete lattice, and let ''F'' be the set of choice functions ''f'' choosing for each index ''j'' of ''J'' some index ''f''(''j'') in ''K''(''j''). A complete lattice is '''completely distributive''' if for all such data the following statement holds:\n\n: <math> \\bigwedge_{j\\in J}\\bigvee_{k\\in K(j)} x_{j,k} = \n         \\bigvee_{f\\in F}\\bigwedge_{j\\in J} x_{j,f(j)}\n </math>\n\nComplete distributivity is again a self-dual property, i.e. dualizing the above statement yields the same class of complete lattices. Completely distributive complete lattices (also called ''completely distributive lattices'' for short) are indeed highly special structures. See the article on [[completely distributive lattice]]s.\n\n==Literature==\n''Distributivity is a basic concept that is treated in any textbook on lattice and order theory. See the literature given for the articles on [[order theory]] and [[lattice theory]]. More specific literature includes:''\n\n* G. N. Raney, ''Completely distributive complete lattices'', Proceedings of the [[American Mathematical Society]], 3: 677 - 680, 1952.\n\n{{reflist}}\n\n{{DEFAULTSORT:Distributivity (Order Theory)}}\n[[Category:Order theory]]"
    },
    {
      "title": "Duality (order theory)",
      "url": "https://en.wikipedia.org/wiki/Duality_%28order_theory%29",
      "text": "In the [[mathematics|mathematical]] area of [[order theory]], every [[partially ordered set]] ''P'' gives rise to a '''dual''' (or '''opposite''') partially ordered set which is often denoted by ''P''<sup>op</sup> or ''P''<sup>''d''</sup>. This dual order ''P''<sup>op</sup> is defined to be the set with the '''inverse order''', i.e. ''x'' ≤ ''y'' holds in ''P''<sup>op</sup> [[if and only if]] ''y'' ≤ ''x'' holds in ''P''. It is easy to see that this construction, which can be depicted by flipping the [[Hasse diagram]] for ''P'' upside down, will indeed yield a partially ordered set. In a broader sense, two [[poset]]s are also said to be duals if they are '''dually isomorphic''', i.e. if one poset is [[order isomorphism|order isomorphic]] to the dual of the other.\n\nThe importance of this simple definition stems from the fact that every definition and theorem of order theory can readily be transferred to the dual order. Formally, this is captured by the '''Duality Principle''' for ordered sets:\n\n: If a given statement is valid for all partially ordered sets, then its dual statement, obtained by inverting the direction of all order relations and by dualizing all order theoretic definitions involved, is also valid for all partially ordered sets.\n\nIf a statement or definition is equivalent to its dual then it is said to be '''self-dual'''. Note that the consideration of dual orders is so fundamental that it often occurs implicitly when writing ≥ for the dual order of ≤ without giving any prior definition of this \"new\" symbol.\n\n== Examples ==\n[[File:Duale Verbaende.svg|thumb|A bounded distributive lattice, and its dual]]\nNaturally, there are a great number of examples for concepts that are dual:\n* [[Greatest element|Greatest elements and least elements]]\n* [[Maximal element|Maximal elements and minimal elements]]\n* [[Least upper bound]]s (suprema, ∨) and [[greatest lower bound]]s (infima, ∧)\n* [[Upper set|Upper sets and lower sets]]\n* [[ideal (order theory)|Ideals]] and [[filter (mathematics)|filters]]\n* [[Closure operator]]s and [[kernel operator]]s.\n\nExamples of notions which are self-dual include:\n* Being a ([[complete lattice|complete]]) [[lattice (order)|lattice]]\n* [[monotonic function|Monotonicity]] of functions\n* [[distributive lattice|Distributivity of lattices]], i.e. the lattices for which ∀''x'',''y'',''z'': ''x'' ∧ (''y'' ∨ ''z'') = (''x'' ∧ ''y'') ∨ (''x'' ∧ ''z'') holds are exactly those for which the dual statement ∀''x'',''y'',''z'': ''x'' ∨ (''y'' ∧ ''z'') = (''x'' ∨ ''y'') ∧ (''x'' ∨ ''z'') holds<ref>The quantifiers are essential: for individual elements ''x'', ''y'', ''z'', e.g. the first equation may be violated, but the second may hold; see the [[modular lattice|N<sub>5</sub> lattice]] for an example.</ref>\n* Being a [[Boolean algebra (structure)|Boolean algebra]]\n* Being an [[order isomorphism]].\n\nSince partial orders are [[Antisymmetric relation|antisymmetric]], the only ones that are self-dual are the [[equivalence relations]].\n\n==See also==\n* [[Converse relation]]\n* [[List of Boolean algebra topics]]\n* [[Transpose graph]]\n*[[Dual (category theory)|Duality in category theory]], of which duality in order theory is a special case\n\n==References==\n{{Reflist}}\n* {{Citation | last1=Davey | first1=B.A. | last2=Priestley | first2=H. A. | title=Introduction to Lattices and Order | edition = 2nd | publisher=[[Cambridge University Press]] | isbn=978-0-521-78451-1 | year=2002}}\n\n[[Category:Order theory]]\n[[Category:Duality theories|Order theory]]"
    },
    {
      "title": "Encompassment ordering",
      "url": "https://en.wikipedia.org/wiki/Encompassment_ordering",
      "text": "[[File:Encompassment ordering on terms s,t_svg.svg|thumb|Triangle diagram of two terms ''s''&nbsp;≤&nbsp;''t'' related by the encompassment preorder.]]\nIn [[theoretical computer science]], in particular in [[automated theorem proving]] and [[term rewriting]],\nthe '''containment''',<ref>{{cite journal| author=Gerard Huet| title=A Complete Proof of Correctness of the Knuth–Bendix Completion Algorithm| journal=J. Comput. Syst. Sci.| year=1981| volume=23| number=1| pages=11–21| doi=10.1016/0022-0000(81)90002-7}}</ref> or '''encompassment''', [[preorder]] (≤) on the set of [[term (logic)|term]]s, is defined by<ref>{{cite book| author=N. Dershowitz, [[J.-P. Jouannaud]]| title=Rewrite Systems| year=1990| volume=B| pages=243–320| publisher=Elsevier| editor=[[Jan van Leeuwen]]| series=Handbook of Theoretical Computer Science}} Here:sect.2.1, p.&nbsp;250</ref>\n:''s''&nbsp;≤&nbsp;''t'' if a [[Term (logic)#Operations with terms|subterm]] of ''t'' is a [[substitution instance]] of ''s''.\nIt is used e.g. in the [[Knuth–Bendix completion algorithm]].\n\n==Properties==\n\n* Encompassment is a [[preorder]], i.e. [[reflexive relation|reflexive]] and [[transitive relation|transitive]], but not [[anti-symmetric relation|anti-symmetric]],<ref group=note>since both ''f''(''x'')&nbsp;≤&nbsp;''f''(''y'') and ''f''(''y'')&nbsp;≤&nbsp;''f''(''x'') for [[Term (logic)#Formal definition|variable symbols]] ''x'', ''y'' and a [[Term (logic)#Formal definition|function symbol]] ''f''</ref> nor [[total order|total]]<ref group=note>since neither ''a''&nbsp;≤&nbsp;''b'' nor ''b''&nbsp;≤&nbsp;''a'' for distinct [[Term (logic)#Formal definition|constant symbols]] ''a'', ''b''</ref>\n* The [[preorder#Constructions|corresponding equivalence relation]], defined by ''s''&nbsp;~&nbsp;''t'' if ''s''&nbsp;≤&nbsp;''t''&nbsp;≤&nbsp;''s'', is [[Term (logic)#Structural equality|equality modulo renaming]].\n* ''s''&nbsp;≤&nbsp;''t'' whenever ''s'' is a [[Term (logic)#Operations with terms|subterm]] of ''t''.\n* ''s''&nbsp;≤&nbsp;''t'' whenever ''t'' is a [[substitution instance]] of ''s''.\n* The union of any well-founded [[rewrite order]] ''R''<ref group=note>i.e. irreflexive, transitive, and well-founded binary relation ''R'' such that ''sRt'' implies ''u''[[term (logic)#Operations with terms|[]]''s''[[Substitution (logic)#First-order logic|σ]]]<sub>''p''</sub> R ''u''[''t''σ]<sub>''p''</sub> for all terms ''s'', ''t'', ''u'', each path ''p'' of ''u'', and each [[Substitution (logic)#First-order logic|substitution]]&nbsp;''σ''</ref> with (<) is [[well-founded]], where (<) denotes the [[irreflexive kernel]] of (≤).<ref>Dershowitz, Jouannaud (1990), sect.5.4, p.&nbsp;278; somewhat sloppy, ''R'' is required to be a \"terminating rewrite relation\" there.</ref> In particular, (<) itself is well-founded.\n\n==Notes==\n{{reflist|group=note}}\n\n==References==\n{{reflist}}\n\n[[Category:Rewriting systems]]\n[[Category:Order theory]]\n\n\n{{comp-sci-stub}}"
    },
    {
      "title": "Η set",
      "url": "https://en.wikipedia.org/wiki/%CE%97_set",
      "text": "{{lowercase}}\nIn mathematics, an '''[[Eta|η]] set''' is a type of [[totally ordered set]] introduced by {{harvs|txt|last=Hausdorff|year1=1907|loc1=p.126|year2=1914|loc2=chapter 6 section 8}} that generalizes the order type η of the rational numbers.\n\n==Definition==\n\nIf α is an ordinal then a η<sub>α</sub> set is a totally ordered set such that if ''X'' and ''Y'' are two subsets of cardinality less than [[Aleph number|ℵ]]<sub>α</sub> such that every element of ''X'' is less than every element of ''Y'' then there is some element greater than all elements of ''X'' and less than all elements of ''Y''.\n\n==Examples==\n\nThe only non-empty countable η<sub>0</sub> set (up to isomorphism) is the ordered set of rational numbers.\n\nSuppose that κ=ℵ<sub>α</sub> is a regular cardinal and let ''X'' be the set of all functions ''f'' from κ to {−1,0,1} such that if ''f''(α) = 0 then ''f''(β) = 0 for all β>α, ordered lexicographically. Then ''X'' is a η<sub>α</sub> set. The union of all these sets is the class of [[surreal number]]s.\n\nA dense totally ordered set without endpoints is a η<sub>α</sub> set if and only if it is [[saturated model|ℵ<sub>α</sub> saturated]].\n\n==Properties==\n\nAny η<sub>α</sub> set ''X'' is universal for totally ordered sets of cardinality at most ℵ<sub>α</sub>, meaning that any such set can be embedded into ''X''.\n\nFor any given ordinal α, any two η<sub>α</sub> sets of cardinality  ℵ<sub>α</sub> are isomorphic (as ordered sets). An η<sub>α</sub> set of cardinality  ℵ<sub>α</sub> exists if ℵ<sub>α</sub> is regular and ∑<sub>β<α</sub> 2<sup>ℵ<sub>β</sub></sup> ≤ ℵ<sub>α</sub>.\n\n==References==\n\n*{{citation|title=On the existence of real-closed fields that are  η<sub>α</sub>-sets of power ℵ<sub>α</sub>.\n|first= Norman L.|last= Alling \n|journal= Trans. Amer. Math. Soc.|volume= 103 |year=1962|pages= 341-352 \n|mr= 0146089|doi=10.1090/S0002-9947-1962-0146089-X}}\n*{{Cite book | last1=[[Chen Chung Chang|Chang]] | first1=Chen Chung | last2=Keisler | first2=H. Jerome | author2-link=Howard Jerome Keisler | title=Model Theory | origyear=1973 | publisher=Elsevier | edition=3rd | series=Studies in Logic and the Foundations of Mathematics | isbn=978-0-444-88054-3 | year=1990 | postscript=<!--None-->}}\n*{{citation|url=http://hausdorff-edition.de/media/pdf/Eta_Alpha.pdf|first=U. |last=Felgner|chapter=Die Hausdorffsche Theorie der ηα-Mengen und ihre Wirkungsgeschichte|title=Hausdorff Gesammelte Werke|volume=II|publisher= Springer-Verlag|place= Berlin, Heidelberg |year= 2002|pages= 645–674}}\n*{{citation|last=Hausdorff|title=Untersuchungen über Ordnungstypen  V|journal= Ber. über die Verhandlungen der Königl. Sächs. Ges. der Wiss. zu Leipzig. Math.-phys. Klasse|volume= 59 |year=1907|pages=105–159}} English translation in {{harvtxt|Hausdorff|2005}}\n*{{citation|title=Grundzüge der Mengenlehre|publisher= Veit & Co|place= Leipzig|last=Hausdorff|first=F.|year=1914 |url=https://archive.org/details/grundzgedermen00hausuoft}}\n*{{citation|mr=2187098 \n|last=Hausdorff|first= Felix\n|title=Hausdorff on ordered sets\n|editor-first= J. M.|editor-last= Plotkin|series= History of Mathematics|volume= 25|publisher= American Mathematical Society|place=Providence, RI|year= 2005|isbn= 0-8218-3788-5 |url=https://books.google.com/books?id=M_skkA3r-QAC}}\n\n{{DEFAULTSORT:Eta set}}\n[[Category:Order theory]]"
    },
    {
      "title": "Fence (mathematics)",
      "url": "https://en.wikipedia.org/wiki/Fence_%28mathematics%29",
      "text": "[[File:Zigzag poset.svg|thumb|The [[Hasse diagram]] of a six-element fence.]]\nIn [[mathematics]], a '''fence''', also called a '''zigzag poset''', is a [[partially ordered set]] in which the order relations form a [[path graph|path]] with alternating orientations:\n:''a'' &lt; ''b'' &gt; ''c'' &lt; ''d'' &gt; ''e'' &lt; ''f'' &gt; ''h'' &lt; ''i'' ...\nor\n:''a'' &gt; ''b'' &lt; ''c'' &gt; ''d'' &lt; ''e'' &gt; ''f'' &lt; ''h'' &gt; ''i'' ...\nA fence may be [[Finite set|finite]], or it may be formed by an infinite alternating sequence extending in both directions. The [[incidence poset]]s of [[path graph]]s form examples of fences.\n\nA [[linear extension]] of a fence is called an [[alternating permutation]]; [[André's problem]] of counting the number of different linear extensions has been studied since the 19th century.<ref>{{harvtxt|André|1881}}.</ref> The solutions to this counting problem, the so-called Euler zigzag numbers or up/down numbers, are\n:1, 1, 2, 4, 10, 32, 122, 544, 2770, 15872, 101042 {{OEIS|A001250}}.\n\nThe number of [[antichain]]s in a fence is a [[Fibonacci number]]; the [[distributive lattice]] with this many elements, generated from a fence via [[Birkhoff's representation theorem]], has as its graph the [[Fibonacci cube]].<ref>{{harvtxt|Gansner|1982}} calls the fact that this lattice has a Fibonacci number of elements a “well known fact,” while {{harvtxt|Stanley|1986}} asks for a description of it in an exercise. See also {{harvtxt|Höft|Höft|1985}}, {{harvtxt|Beck|1990}}, and {{harvtxt|Salvi|Salvi|2008}}.</ref>\n\nA partially ordered set is [[series-parallel partial order|series-parallel]] if and only if it does not have four elements forming a fence.<ref>{{harvtxt|Valdes|Tarjan|Lawler|1982}}.</ref>\n\nSeveral authors have also investigated the number of order-preserving maps from fences to themselves, or to fences of other sizes.<ref>{{harvtxt|Currie|Visentin|1991}}; {{harvtxt|Duffus|Rödl|Sands|Woodrow|1992}}; {{harvtxt|Rutkowski|1992a}}; {{harvtxt|Rutkowski|1992b}}; {{harvtxt|Farley|1995}}.</ref>\n\nAn '''up-down poset''' ''Q''(''a'',''b'') is a generalization of a zigzag poset in which there are ''a'' downward orientations for every upward one and ''b'' total elements.<ref>{{harvtxt|Gansner|1982}}.</ref> For instance, ''Q''(2,9) has the elements and relations\n:''a'' &gt; ''b'' &gt; ''c'' &lt; ''d'' &gt; ''e'' &gt; ''f'' &lt; ''g'' &gt; ''h'' &gt; ''i''.\nIn this notation, a fence is a partially ordered set of the form ''Q''(1,''n'').\n\n==Equivalent conditions==\nThe following conditions are equivalent for a poset ''P'':{{Citation needed|date=March 2015}}\n#''P'' is a disjoint union of zigzag posets.\n#If ''a'' ≤ ''b'' ≤ ''c'' in ''P'', either ''a'' = ''b'' or ''b'' = ''c''.\n#< <math>\\circ</math> < = <math>\\emptyset</math>, i.e. it is never the case that ''a'' < ''b'' and ''b'' < ''c'', so that < is vacuously transitive.\n#''P'' has dimension at most one (defined analogously to the [[Krull dimension]] of a [[commutative ring]]).\n#Every element of ''P'' is either [[maximal element|maximal]] or [[minimal element|minimal]].\n#The slice category '''Pos'''/''P'' is [[cartesian closed category|cartesian closed]].\n\nThe [[prime ideal]]s of a commutative ring ''R'', ordered by inclusion, satisfy the equivalent conditions above if and only if ''R'' has Krull dimension at most one.{{Citation needed|date=March 2015}}\n\n==Notes==\n{{reflist}}\n\n==References==\n{{refbegin|2}}\n*{{citation\n | last = André | first = Désiré\n | journal = J. Math. Pures Appl. \n | series = (Ser. 3)\n | pages = 167–184\n | title = Sur les permutations alternées\n | volume = 7\n | year = 1881}}.\n*{{citation\n | last = Beck | first = István\n | mr = 1051291\n | issue = 2\n | journal = Fibonacci Quarterly\n | pages = 172–174\n | title = Partial orders and the Fibonacci numbers\n | volume = 28\n | year = 1990}}.\n*{{citation\n | last1 = Currie | first1 = J. D.\n | last2 = Visentin | first2 = T. I.\n | doi = 10.1007/BF00383399\n | mr = 1137906\n | issue = 2\n | journal = [[Order (journal)|Order]]\n | pages = 133–142\n | title = The number of order-preserving maps of fences and crowns\n | volume = 8\n | year = 1991}}.\n*{{citation\n | last1 = Duffus | first1 = Dwight | author1-link = Dwight Duffus\n | last2 = Rödl | first2 = Vojtěch\n | last3 = Sands | first3 = Bill\n | last4 = Woodrow | first4 = Robert\n | doi = 10.1007/BF00419036\n | mr = 1194849\n | issue = 1\n | journal = [[Order (journal)|Order]]\n | pages = 15–29\n | title = Enumeration of order preserving maps\n | volume = 9\n | year = 1992}}.\n*{{citation\n | last = Farley | first = Jonathan David\n | doi = 10.1007/BF01108588\n | mr = 1336535\n | issue = 1\n | journal = [[Order (journal)|Order]]\n | pages = 5–44\n | title = The number of order-preserving maps between fences and crowns\n | volume = 12\n | year = 1995}}.\n*{{citation\n | last = Gansner | first = Emden R.\n | doi = 10.1016/0012-365X(82)90134-0\n | mr = 675856\n | issue = 2\n | journal = [[Discrete Mathematics (journal)|Discrete Mathematics]]\n | pages = 113–122\n | title = On the lattice of order ideals of an up-down poset\n | volume = 39\n | year = 1982}}.\n*{{citation\n | last1 = Höft | first1 = Hartmut\n | last2 = Höft | first2 = Margret\n | mr = 806293\n | issue = 3\n | journal = Fibonacci Quarterly\n | pages = 232–237\n | title = A Fibonacci sequence of distributive lattices\n | volume = 23\n | year = 1985}}.\n*{{citation\n | last1 = Kelly | first1 = David\n | last2 = Rival | first2 = Ivan | author2-link = Ivan Rival\n | mr = 0417003\n | journal = Canadian Journal of Mathematics\n | pages = 1257–1271\n | title = Crowns, fences, and dismantlable lattices\n | volume = 26\n | year = 1974}}.\n*{{citation\n | last = Rutkowski | first = Aleksander\n | doi = 10.1007/BF00419037\n | mr = 1194850\n | issue = 1\n | journal = [[Order (journal)|Order]]\n | pages = 31–42\n | title = The number of strictly increasing mappings of fences\n | volume = 9\n | year = 1992a}}.\n*{{citation\n | last = Rutkowski | first = Aleksander\n | doi = 10.1007/BF00814405\n | mr = 1199291\n | issue = 2\n | journal = [[Order (journal)|Order]]\n | pages = 127–137\n | title = The formula for the number of order-preserving self-mappings of a fence\n | volume = 9\n | year = 1992b}}.\n*{{citation\n | last1 = Salvi | first1 = Rodolfo\n | last2 = Salvi | first2 = Norma Zagaglia\n | mr = 2414008\n | journal = Ars Combinatoria\n | pages = 105–117\n | title = Alternating unimodal sequences of Whitney numbers\n | volume = 87\n | year = 2008}}.\n*{{citation\n | last = Stanley | first = Richard P. | authorlink = Richard P. Stanley\n | title = Enumerative Combinatorics\n | year = 1986\n | publisher = Wadsworth, Inc.}} Exercise 3.23a, page 157.\n*{{citation\n | last1 = Valdes | first1 = Jacobo\n | last2 = Tarjan | first2 = Robert E. | author2-link = Robert Tarjan\n | last3 = Lawler | first3 = Eugene L. | author3-link = Eugene Lawler\n | doi = 10.1137/0211023\n | issue = 2\n | journal = [[SIAM Journal on Computing]]\n | pages = 298–313\n | title = The Recognition of Series Parallel Digraphs\n | volume = 11\n | year = 1982}}.\n{{refend}}\n\n==External links==\n*{{mathworld|title=Fence Poset|urlname=FencePoset}}\n\n[[Category:Order theory]]\n[[Category:Enumerative combinatorics]]"
    },
    {
      "title": "Filter (mathematics)",
      "url": "https://en.wikipedia.org/wiki/Filter_%28mathematics%29",
      "text": "{{more footnotes|date=June 2017}}\n[[File:Filter vs ultrafilter.svg|thumb|The powerset lattice of the set {1,2,3,4}, with the [[upper set]] ↑{1,4} colored yellow. It is a ''filter'', and even a ''principal filter''. It is not an ''ultrafilter'', as it can be extended to the larger nontrivial filter ↑{1}, by including also the light green elements. Since ↑{1} cannot be extended any further, it is an ultrafilter.]] \nIn [[mathematics]], a '''filter''' is a special [[subset]] of a [[partially ordered set]]. Filters appear in [[order theory|order]] and [[lattice theory]], but can also be found in [[topology]] from where they originate. The [[duality (order theory)|dual]] notion of a filter is an [[ideal (order theory)|ideal]].\n\nFilters were introduced by [[Henri Cartan]] in 1937<ref>H. Cartan, [http://gallica.bnf.fr/ark:/12148/bpt6k3157c/f594.image \"Théorie des filtres\"], ''CR Acad. Paris'', '''205''', (1937) 595–598.</ref><ref>H. Cartan,  [http://gallica.bnf.fr/ark:/12148/bpt6k3157c/f776.image \"Filtres et ultrafiltres\"], ''CR Acad. Paris'', '''205''', (1937) 777–779.</ref> and subsequently used by [[Nicolas Bourbaki|Bourbaki]] in their book ''[[Topologie Générale]]'' as an alternative to the similar notion of a [[net (topology)|net]] developed in 1922 by [[E. H. Moore]] and [[H. L. Smith]].\n\n== Motivation ==\nIntuitively, a filter in a partially ordered set (''poset''), X, is a subset of X that includes as members those elements that are large enough to satisfy some criterion. For example, if ''x'' is an element of the poset, then the set of elements that are above ''x'' is a filter, called the '''principal filter''' at ''x''. (Notice that if ''x'' and ''y'' are incomparable elements of the poset, then neither of the principal filters at ''x'' and ''y'' is contained in the other one, and conversely.)\n\nSimilarly, a filter on a set contains those subsets that are sufficiently large to contain ''something''. For example, if the set is the [[real line]] and ''x'' is one of its points, then the family of sets that include ''x'' in their interior is a filter, called the '''filter of neighbourhoods''' of ''x''. (Notice that the ''thing'' in this case is slightly larger than ''x'', but it still doesn't contain any other specific point of the line.)\n\nThe above interpretations do not really, without elaboration, explain the condition 2. of the general definition of filter (see below). For, why should two \"large enough\" things contain a ''common'' \"large enough\" thing? (Note, however, that they do explain conditions 1 and 3: Clearly the empty set is not \"large enough\", and clearly the collection of \"large enough\" things should be \"upward closed\".)\n\nAlternatively, a filter can be viewed as a \"locating scheme\": Suppose we try to locate something (a point or a subset) in the space X. Call a filter the ''collection of subsets of X that might contain \"what we are looking for\".'' Then this \"filter\" should possess the following natural structure: 1. The empty set cannot contain anything so it will not belong to our filter. 2. If two subsets, E and F, both might contain \"what we are looking for\", then so might their intersection. Thus our filter should be closed with respect to finite intersection. 3. If a set E might contain \"what we are looking for\", so might any superset of it. Thus our filter is upward closed.\n\nAn '''ultrafilter''' can be viewed as a \"perfect locating scheme\" where ''each'' subset E of the space X can be used in deciding whether \"what we are looking for\" might lie in E.\n\nFrom this interpretation, '''compactness''' (see the mathematical characterization below) can be viewed as the property that ''no location scheme can end up with nothing'', or, to put it another way, ''we will always find something''.\n\nThe mathematical notion of '''filter''' provides a precise language to treat these situations in a rigorous and general way, which is useful in analysis, [[general topology]] and logic.\n\n== {{Anchor|PROPER}}General definition ==\nA subset ''F'' of a partially ordered set (''P'',≤) is a '''filter''' if the following conditions hold:\n\n# ''F'' is nonempty.\n# For every ''x'', ''y'' in ''F'', there is some element ''z'' in ''F'' such that ''z''&nbsp;≤&nbsp;''x'' and ''z''&nbsp;≤&nbsp;''y''. (''F'' is a '''filter base''' (see below), or downward [[directed set|directed]])\n# For every ''x'' in ''F'' and ''y'' in ''P'', ''x''&nbsp;≤&nbsp;''y'' implies that ''y'' is in ''F''. (''F'' is an ''[[upper set]]'', or upward closed)\n\nA filter is '''proper''' if it is not equal to the whole set ''P''. This condition is sometimes added to the definition of a filter.\n\nWhile the above definition is the most general way to define a filter for arbitrary [[Partially ordered set|posets]], it was originally defined for [[lattice (order)|lattice]]s only. In this case, the above definition can be characterized by the following equivalent statement:\nA subset ''F'' of a lattice (''P'',≤) is a filter, [[if and only if]] it is a nonempty upper set that is closed under finite intersection ([[infimum|infima]] or [[Meet (mathematics)|meet]]), i.e., for all ''x'', ''y'' in ''F'', then ''x'' ∧ ''y'' is also in ''F''.<ref>{{cite book | author=B.A. Davey and H.A. Priestley | title=Introduction to Lattices and Order | publisher=Cambridge University Press | series=Cambridge Mathematical Textbooks | year=1990 }}</ref>{{rp|184}}\n\nThe smallest filter that contains a given element ''p'' is a '''principal filter''' and ''p'' is a '''principal element''' in this situation. The principal filter for ''p'' is just given by the set <math>\\{x \\in P\\ |\\ p \\leq x\\}</math> and is denoted by prefixing ''p'' with an upward arrow: {{nobreak|<math>\\uparrow p</math>.}}\n\nThe [[Duality (mathematics)|dual notion]] of a filter, i.e. the concept obtained by reversing all ≤ and exchanging ∧ with ∨, is '''ideal'''. Because of this duality, the discussion of filters usually boils down to the discussion of ideals. Hence, most additional information on this topic (including the definition of '''maximal filters''' and '''prime filters''') is to be found in the article on [[ideal (order theory)|ideals]]. There is a separate article on [[ultrafilter]]s.\n\n== Filter on a set ==\nA special case of a filter is a filter defined on a set. Given a set ''S'', a partial ordering ⊆ can be defined on the [[powerset]] '''P'''(''S'') by subset inclusion, turning ('''P'''(''S''),⊆) into a lattice. Define a '''filter''' ''F'' on ''S'' as a nonempty subset of '''P'''(''S'') with the following properties:\n\n# ''S'' is in ''F'', and if ''A'' and ''B'' are in ''F'', then so is their intersection. (''F is closed under finite intersection'')\n# If ''A'' is in ''F'' and ''A'' is a subset of ''B'', then ''B'' is in ''F'', for all subsets ''B'' of ''S''. (''F is upward closed'')\n\nIf the empty set is not in ''F'', we say ''F'' is a proper filter. <ref>{{cite book|last1=Goldblatt|first1=R|title=Lectures on the Hyperreals: an Introduction to Nonstandard Analysis|page=32|url=https://archive.org/stream/springer_10.1007-978-1-4612-0615-6/10.1007-978-1-4612-0615-6#page/n31/mode/2up/search/proper+filter}}</ref>\n\nThe first two properties imply that a '''filter on a set''' has the [[finite intersection property]]. With this definition, a filter on a set is indeed a filter. The only nonproper filter on ''S'' is '''P'''(''S'').\n\nA '''filter base''' (or '''filter basis''') is a subset ''B'' of '''P'''(''S'') with the following properties:\n# ''B'' is non-empty and the intersection of any two members of ''B'' contains a member of ''B'' (''B is downward directed'').\n# The empty set is not a member of ''B'' (''B is a proper filter base'').\n\nGiven a filter base ''B'', the filter generated or spanned by ''B'' is defined as the minimum filter containing ''B''. It is the family of all the subsets of ''S'' which contain a member of ''B''. Every filter is also a filter base, so the process of passing from filter base to filter may be viewed as a sort of completion.\n\nIf ''B'' and ''C'' are two filter bases on ''S'', one says ''C'' is '''finer''' than ''B'' (or that ''C'' is a '''refinement''' of ''B'') if for each ''B''<sub>0</sub> ∈ ''B'', there is a ''C''<sub>0</sub> ∈ ''C'' such that ''C''<sub>0</sub> ⊆ ''B''<sub>0</sub>. If also ''B'' is finer than ''C'', one says that they are '''equivalent filter bases'''.\n* If ''B'' and ''C'' are filter bases, then ''C'' is finer than ''B'' if and only if the filter spanned by ''C'' contains the filter spanned by ''B''. Therefore, ''B'' and ''C'' are equivalent filter bases if and only if they generate the same filter.\n* For filter bases ''A'', ''B'', and ''C'', if ''A'' is finer than ''B'' and ''B'' is finer than ''C'' then ''A'' is finer than ''C''.  Thus the refinement relation is a [[preorder]] on the set of filter bases, and the passage from filter base to filter is an instance of passing from a preordering to the associated partial ordering.\n\nFor any subset ''T'' of '''P'''(''S'') there is a smallest (possibly nonproper) filter ''F'' containing ''T'', called the filter generated or spanned by ''T''. It is constructed by taking all finite intersections of ''T'', which then form a filter base for ''F''. This filter is proper if and only if any finite intersection of elements of ''T'' is non-empty, and in that case we say that ''T'' is a '''filter subbase'''.\n\n=== Examples ===\n* Let ''S'' be a nonempty set and ''C'' be a nonempty subset of ''S''.  Then <math>\\{ C \\}</math> is a filter base.  The filter it generates (i.e., the collection of all subsets containing ''C'') is called the '''principal filter''' generated by ''C''.\n* A filter is said to be a '''free filter''' if the intersection of all of its members is empty.  A principal filter is not free.  Since the intersection of any finite number of members of a filter is also a member, no filter on a finite set is free, and indeed is the principal filter generated by the common intersection of all of its members.  A nonprincipal filter on an infinite set is not necessarily free.\n* The [[Fréchet filter]] on an infinite set ''S'' is the set of all subsets of ''S'' that have finite complement.  A filter on ''S'' is free if and only if it contains the Fréchet filter.\n* Every [[uniform structure]] on a set ''X'' is a filter on ''X''&times;''X''.\n* A filter in a [[poset]] can be created using the [[Rasiowa-Sikorski lemma]], often used in [[forcing (mathematics)|forcing]].\n* The set <math>\\{ \\{ N, N+1, N+2, \\dots \\} : N \\in \\{1,2,3,\\dots\\} \\}</math> is called a ''filter base of tails'' of the sequence of natural numbers <math>(1,2,3,\\dots)</math>. A filter base of tails can be made of any [[net (mathematics)|net]] <math>(x_\\alpha)_{\\alpha \\in A}</math> using the construction <math>\\{ \\{ x_\\alpha : \\alpha \\in A, \\alpha_0 \\leq \\alpha \\} : \\alpha_0 \\in A \\}</math> where the filter that this filter base generates is called the net's ''eventuality filter.'' Therefore, all nets generate a filter base (and therefore a filter). Since all sequences are nets, this holds for sequences as well.\n\n=== Filters in model theory ===\nFor any filter ''F'' on a set ''S'', the set function defined by\n:<math>\nm(A)=\n\\begin{cases}\n1 & \\text{if }A\\in F \\\\\n0 & \\text{if }S\\setminus A\\in F \\\\\n\\text{undefined} & \\text{otherwise}\n\\end{cases}\n</math>\nis finitely additive — a \"[[measure (mathematics)|measure]]\" if that term is construed rather loosely.  Therefore the statement\n\n:<math>\\left\\{\\,x\\in S: \\varphi(x)\\,\\right\\}\\in F</math>\n\ncan be considered somewhat analogous to the statement that φ holds \"almost everywhere\".  That interpretation of membership in a filter is used (for motivation, although it is not needed for actual ''proofs'') in the theory of [[ultraproduct]]s in [[model theory]], a branch of [[mathematical logic]].\n\n=== Filters in topology ===\nIn [[topology]] and analysis, filters are used to define convergence in a manner similar to the role of [[sequence]]s in a [[metric space]].\n\nIn topology and related areas of mathematics, a filter is a generalization of a [[net (mathematics)|net]]. Both nets and filters provide very general contexts to unify the various notions of [[Limit (mathematics)|limit]] to arbitrary [[topological space]]s.\n\nA [[sequence]] is usually indexed by the [[natural numbers]], which are a [[totally ordered set]]. Thus, limits in [[first-countable space]]s can be described by sequences. However, if the space is not first-countable, nets or filters must be used. Nets generalize the notion of a sequence by requiring the index set simply be a [[directed set]]. Filters can be thought of as sets built from multiple nets. Therefore, both the limit of a filter and the limit of a net are conceptually the same as the limit of a sequence.<!--An advantage to using filters is that many results can be shown without using the [[axiom of choice]].\n\nThis sentence is very misleading. Whenever invoking the ultrafilter lemma, you are essentially using the axiom of choice (if my understanding is correct.) Besides, regardless of the use of filter, you can't avoid ac to prove, say, Tychonoff's theorem. TakuyaMurata -->\n\n====Neighbourhood bases====\n\nLet ''X'' be a topological space and ''x'' a point of ''X''.\n\n* Take ''N''<sub>''x''</sub> to be the '''[[Neighbourhood system|neighbourhood filter]]''' at point ''x'' for ''X''. This means that ''N''<sub>''x''</sub> is the set of all topological [[neighbourhood (mathematics)|neighbourhood]]s of  the point ''x''. It can be verified that ''N''<sub>''x''</sub> is a filter. A '''neighbourhood system''' is another name for a '''neighbourhood filter'''.\n* To say that ''N'' is a '''neighbourhood base''' at ''x'' for ''X'' means that each subset ''V''<sub>0</sub> of X is a neighbourhood of ''x'' if and only if there exists ''N''<sub>0</sub> ∈ ''N'' such that ''N''<sub>0</sub> ⊆ ''V''<sub>0</sub>. Every neighbourhood base at ''x'' is a filter base that generates the neighbourhood filter at ''x''.\n\n====Convergent filter bases====\n\nLet ''X'' be a topological space and ''x'' a point of ''X''.\n\n* To say that a filter base ''B'' '''converges''' to ''x'', denoted ''B'' → ''x'', means that for every neighbourhood ''U'' of ''x'', there is a ''B''<sub>0</sub> ∈ ''B'' such that ''B''<sub>0</sub> ⊆ ''U''. In this case, ''x'' is called a [[Limit (mathematics)|limit]] of ''B'' and ''B'' is called a '''convergent filter base'''.\n* Every neighbourhood base ''N'' of ''x'' converges to ''x''.\n** If ''N'' is a neighbourhood base at ''x'' and ''C'' is a filter base on ''X'', then ''C'' → ''x'' [[if and only if]] ''C'' is finer than ''N''.\n** If ''Y'' ⊆ ''X'', a point ''p ∈ X'' is called a '''limit point''' of ''Y'' in ''X'' if and only if each neighborhood ''U'' of ''p'' in ''X'' intersects ''Y''. This happens if and only if there is a filter base of subsets of ''Y'' that converges to ''p'' in ''X''.\n* For ''Y'' ⊆ ''X'', the following are equivalent:\n** (i) There exists a filter base ''F'' whose elements are all contained in ''Y'' such that ''F'' → ''x''.\n** (ii) There exists a filter ''F'' such that ''Y'' is an element of ''F'' and ''F'' → ''x''.\n** (iii) The point ''x'' lies in the closure of ''Y''.\n\nIndeed:\n\n(i) implies (ii): if ''F'' is a filter base satisfying the properties of (i), then the filter associated to ''F'' satisfies the properties of (ii).\n\n(ii) implies (iii): if ''U'' is any open neighborhood of ''x'' then by the definition of convergence ''U'' contains an element of ''F''; since also ''Y'' is an element of ''F'', \n''U'' and ''Y'' have nonempty intersection.\n\n(iii) implies (i): Define <math> F = \\{ U \\cap Y \\ | \\ U \\in N_x \\}</math>.  Then ''F'' is a filter base satisfying the properties of (i).\n\n====Clustering====\n\nLet ''X'' be a topological space and ''x'' a point of ''X''.\n\n* A filter base ''B'' on ''X'' is said to '''cluster''' at ''x'' (or have ''x'' as a [[cluster point]]) if and only if each element of ''B'' has nonempty intersection with each neighbourhood of ''x''.\n** If a filter base ''B'' clusters at ''x'' and is finer than a filter base ''C'', then ''C'' clusters at ''x'' too.\n** Every limit of a filter base is also a cluster point of the base.\n** A filter base ''B'' that has ''x'' as a cluster point may not converge to ''x''. But there is a finer filter base that does. For example the filter base of finite intersections of sets of the subbase <math>B\\cup N_x</math>.\n** For a filter base ''B'', the set ∩{cl(''B''<sub>0</sub>) : ''B''<sub>0</sub>∈''B''} is the set of all cluster points of ''B'' (note: cl(''B''<sub>0</sub>) is the [[closure (topology)|closure]] of ''B''<sub>0</sub>). Assume that ''X'' is a [[complete lattice]].\n*** The [[limit inferior]] of ''B'' is the [[infimum]] of the set of all cluster points of ''B''.\n*** The [[limit superior]] of ''B'' is the [[supremum]] of the set of all cluster points of ''B''.\n*** ''B'' is a convergent filter base [[if and only if]] its limit inferior and limit superior agree; in this case, the value on which they agree is the limit of the filter base.\n\n====Properties of a topological space====\n\nLet ''X'' be a topological space.\n\n* ''X'' is a [[Hausdorff space]] [[if and only if]] every filter base on ''X'' has at most one limit.\n* ''X'' is [[Compact space|compact]] if and only if every filter base on ''X'' clusters or has a cluster point.\n* ''X'' is compact if and only if every filter base on ''X'' is a subset of a convergent filter base.\n* ''X'' is compact if and only if every [[ultrafilter]] on ''X'' converges.\n\n====Functions on topological spaces====\n\nLet <math>X</math>, <math>Y</math> be topological spaces. Let <math>A</math> be a filter base on <math>X</math> and <math>f\\colon X \\to Y</math> be a function.  The [[Image (mathematics)|image]] of <math>A</math> under <math>f</math> is defined as the set <math>\\{ f(a) : a \\in A \\}</math>. The image is denoted <math>f[A]</math> and forms a filter base on <math>Y</math>. \n* <math>f</math> is [[Continuous function (topology)|continuous]] at <math>x</math> if and only if <math>A \\to x</math> implies <math>f[A] \\to f(x)</math>.\n\n==== Cauchy filters ====\n\nLet <math>(X,d)</math> be a [[metric space]].\n* To say that a filter base ''B'' on ''X'' is '''Cauchy''' means that for each [[real number]] ε>0, there is a ''B''<sub>0</sub> ∈ ''B'' such that the metric [[diameter]] of ''B''<sub>0</sub> is less than ε.\n* Take (''x<sub>n</sub>'') to be a [[sequence]] in metric space ''X''. (''x<sub>n</sub>'') is a [[Cauchy sequence]] if and only if the filter base <nowiki>{{</nowiki>''x''<sub>''N''</sub>, ''x''<sub>''N''  +1</sub>, ...} : ''N'' ∈ {1,2,3,...} } is Cauchy.\n\nMore generally, given a [[uniform space]] ''X'', a filter ''F'' on ''X'' is called '''Cauchy filter''' if for every  [[entourage (topology)|entourage]] ''U'' there is an ''A'' ∈ ''F'' with (''x'', ''y'') ∈ ''U'' for all ''x'', ''y'' ∈ ''A''. In a metric space this agrees with the previous definition. ''X'' is said to be complete if every Cauchy filter converges. Conversely, on a uniform space every convergent filter is a Cauchy filter. Moreover, every cluster point of a Cauchy filter is a limit point.\n\nA compact uniform space is complete: on a compact space each filter has a cluster point, and if the filter is Cauchy, such a cluster point is a limit point. Further, a uniformity is compact if and only if it is complete and [[totally bounded]].\n\nMost generally, a [[Cauchy space]] is a set equipped with a class of filters declared to be Cauchy. These are required to have the following properties:\n# for each ''x'' in ''X'', the [[ultrafilter]] at ''x'', ''U''(''x''), is Cauchy.\n# if ''F'' is a Cauchy filter, and ''F'' is a subset of a filter ''G'', then ''G'' is Cauchy.\n# if ''F'' and ''G'' are Cauchy filters and each member of ''F'' intersects each member of ''G'', then ''F'' ∩ ''G'' is Cauchy.\nThe Cauchy filters on a uniform space have these properties, so every uniform space (hence every metric space) defines a Cauchy space.\n\n== See also ==\n* [[Ultrafilter]]\n* [[Filtration (mathematics)]]\n* [[Filtration (probability theory)]]\n* [[Filtration (abstract algebra)]]\n* [[Net (mathematics)]]\n* [[Generic filter]]\n* [[Ideal (set theory)]]\n\n== Notes ==\n{{reflist}}\n\n== References ==\n*[[Nicolas Bourbaki]], <cite>General Topology</cite> (<cite>Topologie Générale</cite>), {{ISBN|0-387-19374-X}} (Ch. 1-4): Provides a good reference for filters in general topology (Chapter I) and for Cauchy filters in uniform spaces (Chapter II)\n* Stephen Willard, ''General Topology'', (1970) Addison-Wesley Publishing Company, Reading Massachusetts. ''(Provides an introductory review of filters in topology.)''\n*David MacIver, ''[http://www.efnet-math.org/~david/mathematics/filters.pdf Filters in Analysis and Topology]'' (2004) ''(Provides an introductory review of filters in topology and in metric spaces.)''\n* Burris, Stanley N., and H.P. Sankappanavar, H. P., 1981. ''[http://www.thoralf.uwaterloo.ca/htdocs/ualg.html A Course in Universal Algebra.]''  Springer-Verlag. {{ISBN|3-540-90578-2}}.\n\n[[Category:Order theory]]\n[[Category:General topology]]"
    },
    {
      "title": "Fréchet filter",
      "url": "https://en.wikipedia.org/wiki/Fr%C3%A9chet_filter",
      "text": "In [[mathematics]], the '''Fréchet filter''', also called the '''cofinite filter''', on a [[Set (mathematics)|set]] is a special subset of the set's [[power set]]. A member of this power set is in the Fréchet filter if and only if its [[Complement (set theory)|complement]] in the set is finite. This is of interest in [[topology]], where filters originated, and relates to [[Order theory|order]] and  [[Lattice (order)|lattice theory]] because a set's power set is a [[partially ordered set]] (and more specifically, a lattice) under [[Subset|set inclusion]].\n\nThe Fréchet filter is named after the French mathematician [[Maurice Fréchet]] (1878-1973), who worked in topology. It is alternatively called a ''cofinite filter'' because its members are exactly the cofinite sets in a power set.\n\n== Definition ==\nThe Fréchet filter ''F'' on ''X'' is the set of all subsets ''A'' of ''X'' such that the complement of ''A'' in ''X'' is finite. That is, \n\n::{{math|big=1|''F'' {{=}} {''A'' &sube; ''X'' : ''X'' &minus; ''A'' is finite}.}}\n\nThis makes ''F'' a [[Filter (mathematics)|filter]] on the lattice ('''P'''(''X''), &sube;), the power set of ''X'' with set inclusion, since\n# Intersection condition: if two sets are finitely complemented in ''X'', then so is their intersection (since {{math|(''A'' &cap; ''B'')<sup>C</sup> {{=}} ''A''<sup>C</sup> &cup; ''B''<sup>C</sup>)}}, where ''S''<sup>C</sup> denotes the complement of a set ''S'', and \n# Upper-set condition: if a set is finitely complemented in ''X'', then so are its supersets in ''X''.\n\n== Properties ==\n\nIf the base set ''X'' is finite, then ''F'' = '''P'''(''X'') since every subset of ''X'', and in particular every complement, is then finite. This case is sometimes excluded by definition or else called the '''improper filter''' on ''X''.<ref>Hodges, Wilfrid. ''Model Theory''. Encyclopedia of Mathematics and its Applications. Cambridge University Press, 2008, p. 265. {{ISBN|978-0-521-06636-5}}</ref> Allowing ''X'' to be finite creates a single exception to the Fréchet filter's being [[Filter (mathematics)#Examples|free]] and [[Filter (mathematics)#General definition|non-principal]] since a filter on a finite set cannot be free and a non-principal filter cannot contain any singletons as members.\n\nIf ''X'' is infinite, then every member of ''F'' is infinite since it is simply ''X'' minus finitely many of its members. Additionally, ''F'' is infinite since one of its subsets is the set of all {''x''}<sup>C</sup>, where ''x'' &isin; ''X''.\n\nThe Fréchet filter is both free and non-principal, excepting the finite case mentioned above, and is included in every free filter. It is also the [[Duality (mathematics)|dual]] filter of the [[Ideal (order theory)|ideal]] of all finite subsets of (infinite) ''X''.\n\nThe Fréchet filter is ''not'' necessarily an [[ultrafilter]] (or maximal proper filter). Consider '''P'''('''N'''). The set of even numbers is the complement of the set of odd numbers. Since neither of these sets is finite, neither set is in the Fréchet filter on '''N'''. However, an ultrafilter is free if and only if it includes the Fréchet filter. The existence of free ultrafilters was established by Tarski in 1930, relying on a theorem equivalent to the axiom of choice and is used in the construction of the [[Hyperreal number|hyperreals]] in [[Non-standard analysis|nonstandard analysis]].<ref>Pinto, J. Sousa and Hoskins, R. F. ''Infinitesimal methods for mathematical analysis''. Mathematics and Applications Series. Horwood Publishing, 2004, p. 53. {{ISBN|978-1-898563-99-0}}</ref>\n\n== Examples ==\n{{Expand section|date=January 2012}}\nOn the set '''N''' of [[natural number]]s, the set ''B'' = { (''n'',∞) : ''n'' &isin; '''N'''} is a Fréchet [[filter base]], i.e., the Fréchet filter on '''N''' consists of all supersets of elements of ''B''.\n\n== See also ==\n* [[Ultrafilter]]\n* [[Filter (mathematics)]]\n* [[Boolean prime ideal theorem]]\n\n== Notes ==\n{{reflist}}\n\n== External links ==\n* {{Mathworld|urlname=CofiniteFilter |title=Cofinite Filter}}\n* J.B. Nation, [http://www.math.hawaii.edu/~jb/books.html ''Notes on Lattice Theory''], unpublished course notes available as two PDF files.\n\n\n{{Areas of mathematics |collapsed}}\n\n{{DEFAULTSORT:Frechet Filter}}\n[[Category:Order theory]]\n[[Category:Topology]]"
    }
  ]
}